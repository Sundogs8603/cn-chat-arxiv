<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27969;&#24418;&#25490;&#21517;&#32858;&#21512;&#26469;&#36873;&#25321;&#21644;&#34701;&#21512;&#19981;&#21516;&#40065;&#26834;&#30340;&#20154;&#21592;&#20877;&#35782;&#21035;&#25490;&#21517;&#22120;&#65292;&#20197;&#22635;&#34917;&#20351;&#29992;&#26631;&#35760;&#25968;&#25454;&#22256;&#38590;&#30340;&#31354;&#30333;&#12290;</title><link>http://arxiv.org/abs/2304.14321</link><description>&lt;p&gt;
&#36890;&#36807;&#26080;&#30417;&#30563;&#36229;&#22270;&#25490;&#21517;&#36873;&#25321;&#21644;&#34701;&#21512;&#36827;&#34892;&#20154;&#21592;&#20877;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Person Re-ID through Unsupervised Hypergraph Rank Selection and Fusion. (arXiv:2304.14321v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14321
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27969;&#24418;&#25490;&#21517;&#32858;&#21512;&#26469;&#36873;&#25321;&#21644;&#34701;&#21512;&#19981;&#21516;&#40065;&#26834;&#30340;&#20154;&#21592;&#20877;&#35782;&#21035;&#25490;&#21517;&#22120;&#65292;&#20197;&#22635;&#34917;&#20351;&#29992;&#26631;&#35760;&#25968;&#25454;&#22256;&#38590;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#21592;&#20877;&#35782;&#21035;&#24050;&#32463;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#65292;&#29616;&#22312;&#22312;&#35768;&#22810;&#30417;&#25511;&#24212;&#29992;&#20013;&#20855;&#26377;&#22522;&#26412;&#37325;&#35201;&#24615;&#12290;&#35813;&#20219;&#21153;&#21253;&#25324;&#22312;&#27809;&#26377;&#37325;&#21472;&#35270;&#22270;&#30340;&#22810;&#20010;&#25668;&#20687;&#22836;&#20043;&#38388;&#35782;&#21035;&#20010;&#20154;&#12290;&#22823;&#22810;&#25968;&#26041;&#27861;&#37117;&#38656;&#35201;&#26631;&#35760;&#25968;&#25454;&#65292;&#20294;&#30001;&#20110;&#38656;&#27714;&#37327;&#24222;&#22823;&#19988;&#25163;&#21160;&#20026;&#27599;&#20010;&#20154;&#25351;&#23450;&#31867;&#21035;&#30340;&#38590;&#24230;&#36739;&#22823;&#65292;&#26631;&#35760;&#25968;&#25454;&#24182;&#19981;&#24635;&#26159;&#21487;&#29992;&#30340;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#37325;&#26032;&#25490;&#21517;&#26041;&#27861;&#33021;&#22815;&#23454;&#29616;&#26174;&#33879;&#30340;&#25910;&#30410;&#65292;&#29305;&#21035;&#26159;&#22312;&#27809;&#26377;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#12290;&#27492;&#22806;&#65292;&#29305;&#24449;&#25552;&#21462;&#22120;&#30340;&#34701;&#21512;&#21644;&#22810;&#28304;&#35757;&#32451;&#20063;&#26159;&#21478;&#19968;&#20010;&#38750;&#24120;&#26377;&#21069;&#26223;&#30340;&#30740;&#31350;&#26041;&#21521;&#65292;&#20294;&#24182;&#26410;&#34987;&#24191;&#27867;&#21033;&#29992;&#12290;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#19968;&#31181;&#27969;&#24418;&#25490;&#21517;&#32858;&#21512;&#26041;&#27861;&#65292;&#21033;&#29992;&#19981;&#21516;&#30340;&#20154;&#21592;&#20877;&#35782;&#21035;&#25490;&#21517;&#22120;&#30340;&#20114;&#34917;&#24615;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;&#20174;&#22810;&#20010;&#21644;&#22810;&#26679;&#21270;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#33719;&#24471;&#30340;&#19981;&#21516;&#25490;&#21517;&#21015;&#34920;&#36827;&#34892;&#20102;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#36873;&#25321;&#21644;&#34701;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Person Re-ID has been gaining a lot of attention and nowadays is of fundamental importance in many camera surveillance applications. The task consists of identifying individuals across multiple cameras that have no overlapping views. Most of the approaches require labeled data, which is not always available, given the huge amount of demanded data and the difficulty of manually assigning a class for each individual. Recently, studies have shown that re-ranking methods are capable of achieving significant gains, especially in the absence of labeled data. Besides that, the fusion of feature extractors and multiple-source training is another promising research direction not extensively exploited. We aim to fill this gap through a manifold rank aggregation approach capable of exploiting the complementarity of different person Re-ID rankers. In this work, we perform a completely unsupervised selection and fusion of diverse ranked lists obtained from multiple and diverse feature extractors. A
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#38646;-shot&#22330;&#26223;&#19979;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#22823;&#35268;&#27169;&#26816;&#32034;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#26597;&#35810;&#21644;&#26597;&#35810;&#30340;&#20505;&#36873;&#31572;&#26696;&#30340;&#32452;&#21512;&#20316;&#20026;&#25552;&#31034;&#65292;&#20351;LLM&#29983;&#25104;&#26356;&#31934;&#30830;&#30340;&#31572;&#26696;&#12290;&#30001;&#20110;&#33258;&#30417;&#30563;&#26816;&#32034;&#22120;&#22312;&#38646;-shot&#22330;&#26223;&#20013;&#24615;&#33021;&#36739;&#24046;&#65292;&#22240;&#27492;LameR&#20248;&#20110;&#33258;&#30417;&#30563;&#26816;&#32034;&#22120;&#12290;</title><link>http://arxiv.org/abs/2304.14233</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;-shot&#26816;&#32034;&#20013;&#20855;&#26377;&#36739;&#24378;&#30340;&#34920;&#29616;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are Strong Zero-Shot Retriever. (arXiv:2304.14233v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14233
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#38646;-shot&#22330;&#26223;&#19979;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#22823;&#35268;&#27169;&#26816;&#32034;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#26597;&#35810;&#21644;&#26597;&#35810;&#30340;&#20505;&#36873;&#31572;&#26696;&#30340;&#32452;&#21512;&#20316;&#20026;&#25552;&#31034;&#65292;&#20351;LLM&#29983;&#25104;&#26356;&#31934;&#30830;&#30340;&#31572;&#26696;&#12290;&#30001;&#20110;&#33258;&#30417;&#30563;&#26816;&#32034;&#22120;&#22312;&#38646;-shot&#22330;&#26223;&#20013;&#24615;&#33021;&#36739;&#24046;&#65292;&#22240;&#27492;LameR&#20248;&#20110;&#33258;&#30417;&#30563;&#26816;&#32034;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#22312;&#38646;-shot&#22330;&#26223;&#19979;&#24212;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#22823;&#35268;&#27169;&#26816;&#32034;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;Language Model&#20316;&#20026;&#26816;&#32034;&#22120;&#65288;LameR&#65289;&#20165;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#32780;&#19981;&#26159;&#20854;&#20182;&#31070;&#32463;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;LLM&#19982;&#26816;&#32034;&#22120;&#30340;&#26292;&#21147;&#32452;&#21512;&#36827;&#34892;&#20998;&#35299;&#65292;&#23558;&#38646;-shot&#26816;&#32034;&#30340;&#24615;&#33021;&#25552;&#39640;&#21040;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#24456;&#24378;&#30340;&#31454;&#20105;&#21147;&#12290;&#26412;&#25991;&#20027;&#35201;&#25552;&#20986;&#36890;&#36807;&#20351;&#29992;&#26597;&#35810;&#21644;&#26597;&#35810;&#30340;&#20505;&#36873;&#31572;&#26696;&#30340;&#32452;&#21512;&#20316;&#20026;&#25552;&#31034;&#65292;&#20351;LLM&#29983;&#25104;&#26356;&#31934;&#30830;&#30340;&#31572;&#26696;&#12290;&#26080;&#35770;&#20505;&#36873;&#31572;&#26696;&#26159;&#21542;&#27491;&#30830;&#65292;&#37117;&#21487;&#20197;&#36890;&#36807;&#27169;&#24335;&#27169;&#20223;&#25110;&#20505;&#36873;&#25688;&#35201;&#26469;&#24110;&#21161;LLM&#20135;&#29983;&#26356;&#31934;&#30830;&#30340;&#31572;&#26696;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#33258;&#30417;&#30563;&#26816;&#32034;&#22120;&#22312;&#38646;-shot&#22330;&#26223;&#20013;&#24615;&#33021;&#36739;&#24046;&#65292;&#22240;&#27492;&#36890;&#36807;&#21033;&#29992;LLM&#23545;&#25991;&#26412;&#27169;&#24335;&#30340;&#24378;&#22823;&#34920;&#29616;&#33021;&#21147;&#65292;LameR&#21487;&#20197;&#20248;&#20110;&#33258;&#30417;&#30563;&#26816;&#32034;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we propose a simple method that applies a large language model (LLM) to large-scale retrieval in zero-shot scenarios. Our method, Language language model as Retriever (LameR) is built upon no other neural models but an LLM, while breaking up brute-force combinations of retrievers with LLMs and lifting the performance of zero-shot retrieval to be very competitive on benchmark datasets. Essentially, we propose to augment a query with its potential answers by prompting LLMs with a composition of the query and the query's in-domain candidates. The candidates, regardless of correct or wrong, are obtained by a vanilla retrieval procedure on the target collection. Such candidates, as a part of prompts, are likely to help LLM generate more precise answers by pattern imitation or candidate summarization. Even if all the candidates are wrong, the prompts at least make LLM aware of in-collection patterns and genres. Moreover, due to the low performance of a self-supervised retriever
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#32467;&#21512;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;Transformer&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Deeply-Coupled Convolution-Transformer&#30340;&#26032;&#22411;&#31354;&#26102;&#20114;&#34917;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#39640;&#24615;&#33021;&#30340;&#22522;&#20110;&#35270;&#39057;&#30340;&#20154;&#21592;&#20877;&#35782;&#21035;&#65292;&#24182;&#36890;&#36807;&#20114;&#34917;&#20869;&#23481;&#27880;&#24847;&#21644;&#20998;&#23618;&#26102;&#38388;&#32858;&#21512;&#65292;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.14122</link><description>&lt;p&gt;
&#24102;&#26377;&#31354;&#26102;&#20114;&#34917;&#23398;&#20064;&#30340;&#28145;&#24230;&#32806;&#21512;&#21367;&#31215;Transformer&#29992;&#20110;&#22522;&#20110;&#35270;&#39057;&#30340;&#20154;&#21592;&#20877;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Deeply-Coupled Convolution-Transformer with Spatial-temporal Complementary Learning for Video-based Person Re-identification. (arXiv:2304.14122v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14122
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#32467;&#21512;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;Transformer&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Deeply-Coupled Convolution-Transformer&#30340;&#26032;&#22411;&#31354;&#26102;&#20114;&#34917;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#39640;&#24615;&#33021;&#30340;&#22522;&#20110;&#35270;&#39057;&#30340;&#20154;&#21592;&#20877;&#35782;&#21035;&#65292;&#24182;&#36890;&#36807;&#20114;&#34917;&#20869;&#23481;&#27880;&#24847;&#21644;&#20998;&#23618;&#26102;&#38388;&#32858;&#21512;&#65292;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#36827;&#30340;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#22522;&#20110;&#35270;&#39057;&#30340;&#20154;&#21592;&#20877;&#35782;&#21035;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#36890;&#24120;&#19987;&#27880;&#20110;&#20154;&#21592;&#30340;&#26368;&#26126;&#26174;&#30340;&#21306;&#22495;&#65292;&#20855;&#26377;&#26377;&#38480;&#30340;&#20840;&#23616;&#34920;&#31034;&#33021;&#21147;&#12290;&#26368;&#36817;&#65292;Transformer&#25506;&#32034;&#20102;&#20840;&#23616;&#35266;&#23519;&#19979;&#30340;&#34917;&#19969;&#38388;&#20851;&#31995;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32467;&#21512;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;Transformer&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Deeply-Coupled Convolution-Transformer (DCCT)&#30340;&#26032;&#22411;&#31354;&#26102;&#20114;&#34917;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#39640;&#24615;&#33021;&#30340;&#22522;&#20110;&#35270;&#39057;&#30340;&#20154;&#21592;&#20877;&#35782;&#21035;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;CNN&#21644;Transformer&#32452;&#21512;&#36215;&#26469;&#25552;&#21462;&#20004;&#31181;&#35270;&#35273;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#23427;&#20204;&#30340;&#20114;&#34917;&#24615;&#12290;&#36827;&#19968;&#27493;&#22312;&#31354;&#38388;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20114;&#34917;&#20869;&#23481;&#27880;&#24847;(CCA)&#26469;&#21033;&#29992;&#32806;&#21512;&#32467;&#26500;&#65292;&#24182;&#25351;&#23548;&#29420;&#31435;&#29305;&#24449;&#36827;&#34892;&#31354;&#38388;&#20114;&#34917;&#23398;&#20064;&#12290;&#22312;&#26102;&#38388;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20998;&#23618;&#26102;&#38388;&#32858;&#21512;(HTA)&#26469;&#36880;&#27493;&#25429;&#25417;&#24103;&#38388;&#30340;&#20381;&#36182;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Advanced deep Convolutional Neural Networks (CNNs) have shown great success in video-based person Re-Identification (Re-ID). However, they usually focus on the most obvious regions of persons with a limited global representation ability. Recently, it witnesses that Transformers explore the inter-patch relations with global observations for performance improvements. In this work, we take both sides and propose a novel spatial-temporal complementary learning framework named Deeply-Coupled Convolution-Transformer (DCCT) for high-performance video-based person Re-ID. Firstly, we couple CNNs and Transformers to extract two kinds of visual features and experimentally verify their complementarity. Further, in spatial, we propose a Complementary Content Attention (CCA) to take advantages of the coupled structure and guide independent features for spatial complementary learning. In temporal, a Hierarchical Temporal Aggregation (HTA) is proposed to progressively capture the inter-frame dependenc
&lt;/p&gt;</description></item><item><title>&#19968;&#31181;&#29992;&#20110;&#39034;&#24207;&#25512;&#33616;&#30340;&#24402;&#32435;&#39044;&#27979;&#20462;&#27491;&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#25311;&#24402;&#32435;&#25512;&#29702;&#26469;&#26657;&#27491;&#39044;&#27979;&#65292;&#20174;&#32780;&#25552;&#39640;&#25512;&#33616;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.14050</link><description>&lt;p&gt;
&#39044;&#27979;&#20877;&#20462;&#27491;&#65306;&#19968;&#31181;&#29992;&#20110;&#39034;&#24207;&#25512;&#33616;&#30340;&#24402;&#32435;&#39044;&#27979;&#20462;&#27491;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Prediction then Correction: An Abductive Prediction Correction Method for Sequential Recommendation. (arXiv:2304.14050v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14050
&lt;/p&gt;
&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#39034;&#24207;&#25512;&#33616;&#30340;&#24402;&#32435;&#39044;&#27979;&#20462;&#27491;&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#25311;&#24402;&#32435;&#25512;&#29702;&#26469;&#26657;&#27491;&#39044;&#27979;&#65292;&#20174;&#32780;&#25552;&#39640;&#25512;&#33616;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39034;&#24207;&#25512;&#33616;&#27169;&#22411;&#36890;&#24120;&#20250;&#22312;&#27979;&#35797;&#36807;&#31243;&#20013;&#19968;&#27493;&#29983;&#25104;&#39044;&#27979;&#65292;&#32780;&#19981;&#32771;&#34385;&#39069;&#22806;&#30340;&#39044;&#27979;&#20462;&#27491;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;&#36825;&#24433;&#21709;&#20102;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;Abductive Prediction Correction&#8221;&#65288;APC&#65289;&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#24402;&#32435;&#25512;&#29702;&#26657;&#27491;&#39044;&#27979;&#65292;&#20174;&#32780;&#25552;&#39640;&#25512;&#33616;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequential recommender models typically generate predictions in a single step during testing, without considering additional prediction correction to enhance performance as humans would. To improve the accuracy of these models, some researchers have attempted to simulate human analogical reasoning to correct predictions for testing data by drawing analogies with the prediction errors of similar training data. However, there are inherent gaps between testing and training data, which can make this approach unreliable. To address this issue, we propose an \textit{Abductive Prediction Correction} (APC) framework for sequential recommendation. Our approach simulates abductive reasoning to correct predictions. Specifically, we design an abductive reasoning task that infers the most probable historical interactions from the future interactions predicted by a recommender, and minimizes the discrepancy between the inferred and true historical interactions to adjust the predictions.We perform th
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32534;&#30721;&#26041;&#24335;&#25915;&#20987;&#25628;&#32034;&#24341;&#25806;&#65292;&#20197;&#24494;&#19981;&#21487;&#35265;&#30340;&#26041;&#24335;&#25197;&#26354;&#25991;&#26412;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#25511;&#21046;&#25628;&#32034;&#32467;&#26524;&#12290;&#35813;&#25915;&#20987;&#25104;&#21151;&#22320;&#24433;&#21709;&#20102;Google&#12289;Bing&#21644;Elasticsearch&#31561;&#22810;&#20010;&#25628;&#32034;&#24341;&#25806;&#12290;&#27492;&#22806;&#65292;&#36824;&#21487;&#20197;&#23558;&#35813;&#25915;&#20987;&#38024;&#23545;&#25628;&#32034;&#30456;&#20851;&#30340;&#20219;&#21153;&#22914;&#25991;&#26412;&#25688;&#35201;&#21644;&#25220;&#34989;&#26816;&#27979;&#27169;&#22411;&#12290;&#38656;&#35201;&#25552;&#20379;&#19968;&#22871;&#26377;&#25928;&#30340;&#38450;&#24481;&#25514;&#26045;&#26469;&#24212;&#23545;&#36825;&#20123;&#25216;&#26415;&#24102;&#26469;&#30340;&#28508;&#22312;&#23041;&#32961;&#12290;</title><link>http://arxiv.org/abs/2304.14031</link><description>&lt;p&gt;
&#25552;&#21319;&#32769;&#22823;&#21733;&#65306;&#37319;&#29992;&#32534;&#30721;&#26041;&#24335;&#25915;&#20987;&#25628;&#32034;&#24341;&#25806;
&lt;/p&gt;
&lt;p&gt;
Boosting Big Brother: Attacking Search Engines with Encodings. (arXiv:2304.14031v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14031
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32534;&#30721;&#26041;&#24335;&#25915;&#20987;&#25628;&#32034;&#24341;&#25806;&#65292;&#20197;&#24494;&#19981;&#21487;&#35265;&#30340;&#26041;&#24335;&#25197;&#26354;&#25991;&#26412;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#25511;&#21046;&#25628;&#32034;&#32467;&#26524;&#12290;&#35813;&#25915;&#20987;&#25104;&#21151;&#22320;&#24433;&#21709;&#20102;Google&#12289;Bing&#21644;Elasticsearch&#31561;&#22810;&#20010;&#25628;&#32034;&#24341;&#25806;&#12290;&#27492;&#22806;&#65292;&#36824;&#21487;&#20197;&#23558;&#35813;&#25915;&#20987;&#38024;&#23545;&#25628;&#32034;&#30456;&#20851;&#30340;&#20219;&#21153;&#22914;&#25991;&#26412;&#25688;&#35201;&#21644;&#25220;&#34989;&#26816;&#27979;&#27169;&#22411;&#12290;&#38656;&#35201;&#25552;&#20379;&#19968;&#22871;&#26377;&#25928;&#30340;&#38450;&#24481;&#25514;&#26045;&#26469;&#24212;&#23545;&#36825;&#20123;&#25216;&#26415;&#24102;&#26469;&#30340;&#28508;&#22312;&#23041;&#32961;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25628;&#32034;&#24341;&#25806;&#23545;&#20110;&#25991;&#26412;&#32534;&#30721;&#25805;&#32437;&#30340;&#32034;&#24341;&#21644;&#25628;&#32034;&#23384;&#22312;&#28431;&#27934;&#12290;&#36890;&#36807;&#20197;&#19981;&#24120;&#35265;&#30340;&#32534;&#30721;&#34920;&#31034;&#24418;&#24335;&#24494;&#19981;&#21487;&#35265;&#22320;&#25197;&#26354;&#25991;&#26412;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#25511;&#21046;&#29305;&#23450;&#25628;&#32034;&#26597;&#35810;&#22312;&#22810;&#20010;&#25628;&#32034;&#24341;&#25806;&#19978;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;&#36825;&#31181;&#25915;&#20987;&#25104;&#21151;&#22320;&#38024;&#23545;&#20102;&#20004;&#20010;&#20027;&#35201;&#30340;&#21830;&#19994;&#25628;&#32034;&#24341;&#25806;&#8212;&#8212;Google&#21644;Bing&#8212;&#8212;&#20197;&#21450;&#19968;&#20010;&#24320;&#28304;&#25628;&#32034;&#24341;&#25806;&#8212;&#8212;Elasticsearch&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#36825;&#31181;&#25915;&#20987;&#25104;&#21151;&#22320;&#38024;&#23545;&#20102;&#21253;&#25324;Bing&#30340;GPT-4&#32842;&#22825;&#26426;&#22120;&#20154;&#21644;Google&#30340;Bard&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#20869;&#30340;LLM&#32842;&#22825;&#25628;&#32034;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21464;&#20307;&#25915;&#20987;&#65292;&#38024;&#23545;&#19982;&#25628;&#32034;&#23494;&#20999;&#30456;&#20851;&#30340;&#20004;&#20010;ML&#20219;&#21153;&#8212;&#8212;&#25991;&#26412;&#25688;&#35201;&#21644;&#25220;&#34989;&#26816;&#27979;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#22871;&#38024;&#23545;&#36825;&#20123;&#25216;&#26415;&#30340;&#38450;&#24481;&#25514;&#26045;&#65292;&#24182;&#35686;&#21578;&#25915;&#20987;&#32773;&#21487;&#20197;&#21033;&#29992;&#36825;&#20123;&#25915;&#20987;&#21551;&#21160;&#21453;&#20449;&#24687;&#20105;&#22842;&#25112;&#12290;&#36825;&#20419;&#20351;&#25628;&#32034;&#24341;&#25806;&#32500;&#25252;&#20154;&#21592;&#20462;&#34917;&#24050;&#37096;&#32626;&#30340;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Search engines are vulnerable to attacks against indexing and searching via text encoding manipulation. By imperceptibly perturbing text using uncommon encoded representations, adversaries can control results across search engines for specific search queries. We demonstrate that this attack is successful against two major commercial search engines - Google and Bing - and one open source search engine - Elasticsearch. We further demonstrate that this attack is successful against LLM chat search including Bing's GPT-4 chatbot and Google's Bard chatbot. We also present a variant of the attack targeting text summarization and plagiarism detection models, two ML tasks closely tied to search. We provide a set of defenses against these techniques and warn that adversaries can leverage these attacks to launch disinformation campaigns against unsuspecting users, motivating the need for search engine maintainers to patch deployed systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21697;&#21619;&#32858;&#31867;&#23398;&#20064;&#23454;&#29616;&#21487;&#35299;&#37322;&#24615;&#21327;&#21516;&#36807;&#28388;&#30340;&#27169;&#22411;&#65292;&#22312;&#20445;&#35777;&#39640;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#20026;&#29992;&#25143;&#21644;&#39033;&#30446;&#25552;&#20379;&#21487;&#35299;&#37322;&#30340;&#32858;&#31867;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2304.13937</link><description>&lt;p&gt;
&#29992;&#21697;&#21619;&#32858;&#31867;&#23398;&#20064;&#23454;&#29616;&#21487;&#35299;&#37322;&#24615;&#21327;&#21516;&#36807;&#28388;
&lt;/p&gt;
&lt;p&gt;
Towards Explainable Collaborative Filtering with Taste Clusters Learning. (arXiv:2304.13937v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13937
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21697;&#21619;&#32858;&#31867;&#23398;&#20064;&#23454;&#29616;&#21487;&#35299;&#37322;&#24615;&#21327;&#21516;&#36807;&#28388;&#30340;&#27169;&#22411;&#65292;&#22312;&#20445;&#35777;&#39640;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#20026;&#29992;&#25143;&#21644;&#39033;&#30446;&#25552;&#20379;&#21487;&#35299;&#37322;&#30340;&#32858;&#31867;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21327;&#21516;&#36807;&#28388;&#26159;&#25512;&#33616;&#31995;&#32479;&#20013;&#24191;&#27867;&#20351;&#29992;&#19988;&#26377;&#25928;&#30340;&#25216;&#26415;&#12290;&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#28508;&#22312;&#23884;&#20837;&#30340;&#21327;&#21516;&#36807;&#28388;&#26041;&#27861;&#65288;&#22914;&#30697;&#38453;&#20998;&#35299;&#12289;&#31070;&#32463;&#21327;&#21516;&#36807;&#28388;&#21644;LightGCN&#65289;&#24050;&#32463;&#26377;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;&#20294;&#26159;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#32473;&#25512;&#33616;&#27169;&#22411;&#28155;&#21152;&#35299;&#37322;&#24615;&#65292;&#19981;&#20165;&#21487;&#20197;&#22686;&#21152;&#20154;&#20204;&#23545;&#20915;&#31574;&#36807;&#31243;&#30340;&#20449;&#20219;&#65292;&#32780;&#19988;&#36824;&#26377;&#22810;&#20010;&#22909;&#22788;&#65292;&#22914;&#20026;&#39033;&#30446;&#25512;&#33616;&#25552;&#20379;&#26377;&#35828;&#26381;&#21147;&#30340;&#35299;&#37322;&#12289;&#20026;&#29992;&#25143;&#21644;&#39033;&#30446;&#21019;&#24314;&#26126;&#30830;&#30340;&#25991;&#20214;&#12289;&#20026;&#39033;&#30446;&#21046;&#36896;&#21830;&#25552;&#20379;&#35774;&#35745;&#25913;&#36827;&#30340;&#21327;&#21161;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28165;&#26224;&#26377;&#25928;&#30340;&#21487;&#35299;&#37322;&#24615;&#21327;&#21516;&#36807;&#28388;&#27169;&#22411;&#65292;&#21033;&#29992;&#21487;&#35299;&#37322;&#30340;&#32858;&#31867;&#23398;&#20064;&#26469;&#23454;&#29616;&#20004;&#20010;&#26368;&#33499;&#21051;&#30340;&#30446;&#26631;&#65306;&#65288;1&#65289;&#31934;&#30830;&#8212;&#8212;&#27169;&#22411;&#22312;&#36861;&#27714;&#21487;&#35299;&#37322;&#24615;&#26102;&#19981;&#24212;&#22949;&#21327;&#20934;&#30830;&#24615;&#65307;&#65288;2&#65289;&#33258;&#25105;&#35299;&#37322;&#8212;&#8212;&#27169;&#22411;&#30340;&#35299;&#37322;&#24212;&#26131;&#20110;&#20154;&#20204;&#29702;&#35299;&#12290;&#24341;&#20837;&#21697;&#21619;&#32858;&#31867;&#23398;&#20064;&#26469;&#26500;&#25104;&#29992;&#25143;&#21644;&#39033;&#30446;&#30340;&#35299;&#37322;&#65292;&#24182;&#22312;&#22235;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#32467;&#26524;&#35777;&#23454;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#25552;&#20379;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#35299;&#37322;&#30340;&#21516;&#26102;&#20445;&#35777;&#20102;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Collaborative Filtering (CF) is a widely used and effective technique for recommender systems. In recent decades, there have been significant advancements in latent embedding-based CF methods for improved accuracy, such as matrix factorization, neural collaborative filtering, and LightGCN. However, the explainability of these models has not been fully explored. Adding explainability to recommendation models can not only increase trust in the decisionmaking process, but also have multiple benefits such as providing persuasive explanations for item recommendations, creating explicit profiles for users and items, and assisting item producers in design improvements.  In this paper, we propose a neat and effective Explainable Collaborative Filtering (ECF) model that leverages interpretable cluster learning to achieve the two most demanding objectives: (1) Precise - the model should not compromise accuracy in the pursuit of explainability; and (2) Self-explainable - the model's explanations 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#19977;&#31181;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65288;T5&#12289;CatSeq-Transformer&#12289;ExHiRD&#65289;&#22312;&#20851;&#38190;&#35789;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#21644;&#34892;&#20026;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35780;&#20272;&#26694;&#26550;SoftKeyScore&#26469;&#34913;&#37327;&#20004;&#32452;&#20851;&#38190;&#35789;&#30340;&#30456;&#20284;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.13883</link><description>&lt;p&gt;
&#31070;&#32463;&#20851;&#38190;&#35789;&#29983;&#25104;&#65306;&#20998;&#26512;&#19982;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Neural Keyphrase Generation: Analysis and Evaluation. (arXiv:2304.13883v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13883
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#19977;&#31181;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65288;T5&#12289;CatSeq-Transformer&#12289;ExHiRD&#65289;&#22312;&#20851;&#38190;&#35789;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#21644;&#34892;&#20026;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35780;&#20272;&#26694;&#26550;SoftKeyScore&#26469;&#34913;&#37327;&#20004;&#32452;&#20851;&#38190;&#35789;&#30340;&#30456;&#20284;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#38190;&#35789;&#29983;&#25104;&#26088;&#22312;&#36890;&#36807;&#20174;&#21407;&#22987;&#25991;&#26412;&#20013;&#22797;&#21046;&#65288;&#29616;&#26377;&#20851;&#38190;&#35789;&#65289;&#25110;&#29983;&#25104;&#25429;&#25417;&#25991;&#26412;&#35821;&#20041;&#24847;&#20041;&#30340;&#26032;&#20851;&#38190;&#35789;&#65288;&#32570;&#22833;&#20851;&#38190;&#35789;&#65289;&#26469;&#29983;&#25104;&#35805;&#39064;&#30701;&#35821;&#12290;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#22312;&#27492;&#20219;&#21153;&#20013;&#26368;&#24191;&#27867;&#20351;&#29992;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#29983;&#25104;&#32570;&#22833;&#20851;&#38190;&#35789;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#20960;&#20046;&#27809;&#26377;&#23545;&#27492;&#31867;&#27169;&#22411;&#22312;&#20851;&#38190;&#35789;&#29983;&#25104;&#20013;&#30340;&#24615;&#33021;&#21644;&#34892;&#20026;&#36827;&#34892;&#20998;&#26512;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19977;&#31181;&#24378;&#21170;&#27169;&#22411; T5&#65288;&#22522;&#20110;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#65289;&#12289;CatSeq-Transformer&#65288;&#38750;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#65289;&#21644; ExHiRD&#65288;&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65289;&#25152;&#23637;&#31034;&#30340;&#21508;&#31181;&#36235;&#21183;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#39044;&#27979;&#32622;&#20449;&#24230;&#20998;&#25968;&#12289;&#27169;&#22411;&#26657;&#20934;&#20197;&#21450;&#35789;&#20803;&#20301;&#32622;&#23545;&#20851;&#38190;&#35789;&#29983;&#25104;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#25512;&#21160;&#19968;&#20010;&#26032;&#30340;&#24230;&#37327;&#26694;&#26550;SoftKeyScore&#65292;&#36890;&#36807;&#20351;&#29992; softscores &#26469;&#35745;&#31639;&#37096;&#20998;&#21305;&#37197;&#30340;&#30456;&#20284;&#24230;&#26469;&#35780;&#20272;&#20004;&#32452;&#20851;&#38190;&#35789;&#30340;&#30456;&#20284;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Keyphrase generation aims at generating topical phrases from a given text either by copying from the original text (present keyphrases) or by producing new keyphrases (absent keyphrases) that capture the semantic meaning of the text. Encoder-decoder models are most widely used for this task because of their capabilities for absent keyphrase generation. However, there has been little to no analysis on the performance and behavior of such models for keyphrase generation. In this paper, we study various tendencies exhibited by three strong models: T5 (based on a pre-trained transformer), CatSeq-Transformer (a non-pretrained Transformer), and ExHiRD (based on a recurrent neural network). We analyze prediction confidence scores, model calibration, and the effect of token position on keyphrases generation. Moreover, we motivate and propose a novel metric framework, SoftKeyScore, to evaluate the similarity between two sets of keyphrases by using softscores to account for partial matching and 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#19968;&#20010;&#21517;&#20026;ConvSim&#30340;&#29992;&#25143;&#27169;&#25311;&#22120;&#26469;&#35780;&#20272;&#29992;&#25143;&#21453;&#39304;&#65292;&#20174;&#32780;&#25552;&#39640;&#20250;&#35805;&#24335;&#25628;&#32034;&#30340;&#24615;&#33021;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#26377;&#25928;&#21033;&#29992;&#29992;&#25143;&#21453;&#39304;&#21487;&#20197;&#22823;&#24133;&#25552;&#39640;&#26816;&#32034;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.13874</link><description>&lt;p&gt;
&#21033;&#29992;&#27169;&#25311;&#29992;&#25143;&#21453;&#39304;&#30340;&#26041;&#24335;&#26469;&#20248;&#21270;&#20250;&#35805;&#24335;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Exploiting Simulated User Feedback for Conversational Search: Ranking, Rewriting, and Beyond. (arXiv:2304.13874v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13874
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#19968;&#20010;&#21517;&#20026;ConvSim&#30340;&#29992;&#25143;&#27169;&#25311;&#22120;&#26469;&#35780;&#20272;&#29992;&#25143;&#21453;&#39304;&#65292;&#20174;&#32780;&#25552;&#39640;&#20250;&#35805;&#24335;&#25628;&#32034;&#30340;&#24615;&#33021;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#26377;&#25928;&#21033;&#29992;&#29992;&#25143;&#21453;&#39304;&#21487;&#20197;&#22823;&#24133;&#25552;&#39640;&#26816;&#32034;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;&#35780;&#20272;&#29992;&#25143;&#21453;&#39304;&#22312;&#28151;&#21512;&#20513;&#35758;&#30340;&#20250;&#35805;&#24335;&#25628;&#32034;&#31995;&#32479;&#20013;&#30340;&#21508;&#31181;&#26041;&#27861;&#12290;&#34429;&#28982;&#20250;&#35805;&#24335;&#25628;&#32034;&#31995;&#32479;&#22312;&#22810;&#20010;&#26041;&#38754;&#37117;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#26368;&#36817;&#30340;&#30740;&#31350;&#26410;&#33021;&#25104;&#21151;&#22320;&#23558;&#29992;&#25143;&#21453;&#39304;&#32435;&#20837;&#31995;&#32479;&#20013;&#12290;&#20854;&#20013;&#19968;&#20010;&#20027;&#35201;&#21407;&#22240;&#26159;&#32570;&#20047;&#31995;&#32479;-&#29992;&#25143;&#23545;&#35805;&#20132;&#20114;&#25968;&#25454;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29992;&#25143;&#27169;&#25311;&#22120;&#30340;&#26694;&#26550;&#65292;&#21487;&#29992;&#20110;&#19982;&#21508;&#31181;&#28151;&#21512;&#20513;&#35758;&#30340;&#20250;&#35805;&#24335;&#25628;&#32034;&#31995;&#32479;&#36827;&#34892;&#22810;&#36718;&#20132;&#20114;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;ConvSim&#30340;&#29992;&#25143;&#27169;&#25311;&#22120;&#65292;&#19968;&#26086;&#21021;&#22987;&#21270;&#20102;&#20449;&#24687;&#38656;&#27714;&#25551;&#36848;&#65292;&#23601;&#33021;&#22815;&#23545;&#31995;&#32479;&#30340;&#21709;&#24212;&#25552;&#20379;&#21453;&#39304;&#65292;&#24182;&#22238;&#31572;&#28508;&#22312;&#30340;&#28548;&#28165;&#38382;&#39064;&#12290;&#25105;&#20204;&#23545;&#21508;&#31181;&#26368;&#20808;&#36827;&#30340;&#27573;&#33853;&#26816;&#32034;&#21644;&#31070;&#32463;&#37325;&#26032;&#25490;&#24207;&#27169;&#22411;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#26377;&#25928;&#21033;&#29992;&#29992;&#25143;&#21453;&#39304;&#21487;&#20197;&#23548;&#33268;&#22312;nDCG@3&#26041;&#38754;16%&#30340;&#26816;&#32034;&#24615;&#33021;&#25552;&#39640;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#38543;&#30528;n&#30340;&#22686;&#21152;&#65292;&#19968;&#33268;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
This research aims to explore various methods for assessing user feedback in mixed-initiative conversational search (CS) systems. While CS systems enjoy profuse advancements across multiple aspects, recent research fails to successfully incorporate feedback from the users. One of the main reasons for that is the lack of system-user conversational interaction data. To this end, we propose a user simulator-based framework for multi-turn interactions with a variety of mixed-initiative CS systems. Specifically, we develop a user simulator, dubbed ConvSim, that, once initialized with an information need description, is capable of providing feedback to a system's responses, as well as answering potential clarifying questions. Our experiments on a wide variety of state-of-the-art passage retrieval and neural re-ranking models show that effective utilization of user feedback can lead to 16% retrieval performance increase in terms of nDCG@3. Moreover, we observe consistent improvements as the n
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;GPT-3&#35821;&#35328;&#27169;&#22411;&#20174;&#31185;&#23398;&#25991;&#29486;&#20013;&#33258;&#21160;&#21270;&#22320;&#25552;&#21462;&#37329;&#32435;&#31859;&#26834;&#21512;&#25104;&#20449;&#24687;&#30340;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#39640;&#36890;&#37327;&#30340;&#25506;&#32034;&#37329;&#32435;&#31859;&#26834;&#30340;&#31181;&#23376;&#20171;&#23548;&#29983;&#38271;&#36807;&#31243;&#20197;&#21450;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.13846</link><description>&lt;p&gt;
&#20174;&#25991;&#29486;&#20013;&#25552;&#21462;&#32467;&#26500;&#21270;&#30340;&#31181;&#23376;&#20171;&#23548;&#37329;&#32435;&#31859;&#26834;&#29983;&#38271;&#26041;&#27861;&#65306;&#22522;&#20110;GPT-3&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Extracting Structured Seed-Mediated Gold Nanorod Growth Procedures from Literature with GPT-3. (arXiv:2304.13846v1 [physics.app-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13846
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;GPT-3&#35821;&#35328;&#27169;&#22411;&#20174;&#31185;&#23398;&#25991;&#29486;&#20013;&#33258;&#21160;&#21270;&#22320;&#25552;&#21462;&#37329;&#32435;&#31859;&#26834;&#21512;&#25104;&#20449;&#24687;&#30340;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#39640;&#36890;&#37327;&#30340;&#25506;&#32034;&#37329;&#32435;&#31859;&#26834;&#30340;&#31181;&#23376;&#20171;&#23548;&#29983;&#38271;&#36807;&#31243;&#20197;&#21450;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#37329;&#32435;&#31859;&#26834;&#24050;&#32463;&#25104;&#20026;&#30740;&#31350;&#30340;&#28909;&#28857;&#65292;&#20294;&#25511;&#21046;&#23427;&#20204;&#30340;&#24418;&#29366;&#65292;&#20174;&#32780;&#25511;&#21046;&#23427;&#20204;&#30340;&#20809;&#23398;&#29305;&#24615;&#30340;&#36884;&#24452;&#20173;&#28982;&#24456;&#22823;&#31243;&#24230;&#19978;&#26159;&#22522;&#20110;&#32463;&#39564;&#30340;&#12290;&#23613;&#31649;&#21512;&#25104;&#36807;&#31243;&#20013;&#19981;&#21516;&#35797;&#21058;&#29289;&#20043;&#38388;&#30340;&#20849;&#23384;&#21644;&#30456;&#20114;&#20316;&#29992;&#25511;&#21046;&#30528;&#36825;&#20123;&#29305;&#24615;&#65292;&#20294;&#22312;&#23454;&#36341;&#20013;&#65292;&#25506;&#32034;&#21512;&#25104;&#31354;&#38388;&#30340;&#35745;&#31639;&#21644;&#23454;&#39564;&#26041;&#27861;&#21487;&#33021;&#20250;&#26497;&#20854;&#32321;&#29712;&#25110;&#32791;&#36153;&#36807;&#22810;&#26102;&#38388;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#31185;&#23398;&#25991;&#29486;&#20013;&#24050;&#32463;&#21253;&#21547;&#30340;&#22823;&#37327;&#21512;&#25104;&#20449;&#24687;&#26469;&#33258;&#21160;&#21270;&#25552;&#21462;&#30456;&#20851;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#20197;&#39640;&#36890;&#37327;&#26041;&#24335;&#25506;&#23547;&#37329;&#32435;&#31859;&#26834;&#31181;&#23376;&#20171;&#23548;&#29983;&#38271;&#36807;&#31243;&#20197;&#21450;&#32467;&#26524;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#24378;&#22823;&#30340;GPT-3&#35821;&#35328;&#27169;&#22411;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#26469;&#20174;&#38750;&#32467;&#26500;&#21270;&#31185;&#23398;&#25991;&#26412;&#20013;&#25552;&#21462;&#37329;&#32435;&#31859;&#26834;&#30340;&#32467;&#26500;&#21270;&#22810;&#27493;&#31181;&#23376;&#20171;&#23548;&#29983;&#38271;&#36807;&#31243;&#21644;&#32467;&#26524;&#12290;&#23558;GPT-3&#30340;&#25552;&#31034;&#23436;&#25104;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#39044;&#27979;JSON&#25991;&#26723;&#24418;&#24335;&#30340;&#21512;&#25104;&#27169;&#26495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although gold nanorods have been the subject of much research, the pathways for controlling their shape and thereby their optical properties remain largely heuristically understood. Although it is apparent that the simultaneous presence of and interaction between various reagents during synthesis control these properties, computational and experimental approaches for exploring the synthesis space can be either intractable or too time-consuming in practice. This motivates an alternative approach leveraging the wealth of synthesis information already embedded in the body of scientific literature by developing tools to extract relevant structured data in an automated, high-throughput manner. To that end, we present an approach using the powerful GPT-3 language model to extract structured multi-step seed-mediated growth procedures and outcomes for gold nanorods from unstructured scientific text. GPT-3 prompt completions are fine-tuned to predict synthesis templates in the form of JSON docu
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#20004;&#37096;&#20998;&#20869;&#23481;&#12290;&#39318;&#20808;&#65292;&#20182;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20110;&#19977;&#20803;&#32452;&#25439;&#22833;&#30340;&#31616;&#21333;&#27169;&#22411;&#65292;&#24615;&#33021;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#27700;&#24179;&#65292;&#20294;&#27809;&#26377;&#22797;&#26434;&#27169;&#22411;&#30340;&#32553;&#25918;&#38382;&#39064;&#12290;&#20854;&#27425;&#65292;&#20182;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21518;&#22788;&#29702;&#26041;&#27861;STIR&#65292;&#21487;&#22312;&#21333;&#20010;&#21069;&#21521;&#20256;&#36882;&#20013;&#37325;&#26032;&#25490;&#21015;&#22810;&#20010;&#39030;&#37096;&#36755;&#20986;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;&#20840;&#23616;/&#23616;&#37096;&#29305;&#24449;&#25552;&#21462;&#12290;</title><link>http://arxiv.org/abs/2304.13393</link><description>&lt;p&gt;
STIR&#65306;&#29992;&#20110;&#22270;&#20687;&#26816;&#32034;&#21518;&#22788;&#29702;&#30340;Siamese Transformer&#65288;arXiv&#65306;2304.13393v1 [cs.IR]&#65289;
&lt;/p&gt;
&lt;p&gt;
STIR: Siamese Transformer for Image Retrieval Postprocessing. (arXiv:2304.13393v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13393
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#20004;&#37096;&#20998;&#20869;&#23481;&#12290;&#39318;&#20808;&#65292;&#20182;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20110;&#19977;&#20803;&#32452;&#25439;&#22833;&#30340;&#31616;&#21333;&#27169;&#22411;&#65292;&#24615;&#33021;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#27700;&#24179;&#65292;&#20294;&#27809;&#26377;&#22797;&#26434;&#27169;&#22411;&#30340;&#32553;&#25918;&#38382;&#39064;&#12290;&#20854;&#27425;&#65292;&#20182;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21518;&#22788;&#29702;&#26041;&#27861;STIR&#65292;&#21487;&#22312;&#21333;&#20010;&#21069;&#21521;&#20256;&#36882;&#20013;&#37325;&#26032;&#25490;&#21015;&#22810;&#20010;&#39030;&#37096;&#36755;&#20986;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;&#20840;&#23616;/&#23616;&#37096;&#29305;&#24449;&#25552;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#65292;&#22270;&#20687;&#26816;&#32034;&#30340;&#24230;&#37327;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#22522;&#20110;&#23398;&#20064;&#20855;&#26377;&#20449;&#24687;&#30340;&#28508;&#22312;&#34920;&#31034;&#31354;&#38388;&#65292;&#20854;&#20013;&#31616;&#21333;&#30340;&#26041;&#27861;&#22914;&#20313;&#24358;&#36317;&#31163;&#23558;&#34920;&#29616;&#33391;&#22909;&#12290;&#26368;&#36817;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65288;&#22914;HypViT&#65289;&#36716;&#21521;&#26356;&#22797;&#26434;&#30340;&#23884;&#20837;&#31354;&#38388;&#65292;&#21487;&#33021;&#20250;&#20135;&#29983;&#26356;&#22909;&#30340;&#32467;&#26524;&#65292;&#20294;&#26356;&#38590;&#20197;&#25193;&#23637;&#21040;&#29983;&#20135;&#29615;&#22659;&#20013;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20110;&#19977;&#20803;&#32452;&#25439;&#22833;&#30340;&#31616;&#21333;&#27169;&#22411;&#65292;&#20855;&#26377;&#30828;&#36127;&#20363;&#25366;&#25496;&#65292;&#24615;&#33021;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#27700;&#24179;&#65292;&#20294;&#27809;&#26377;&#36825;&#20123;&#32570;&#28857;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#20687;&#26816;&#32034;&#21518;&#22788;&#29702;&#26041;&#27861;&#65292;&#31216;&#20026;&#29992;&#20110;&#22270;&#20687;&#26816;&#32034;&#30340;Siamese Transformer&#65288;STIR&#65289;&#65292;&#21487;&#22312;&#21333;&#20010;&#21069;&#21521;&#20256;&#36882;&#20013;&#37325;&#26032;&#25490;&#21015;&#22810;&#20010;&#39030;&#37096;&#36755;&#20986;&#12290;&#19982;&#20808;&#21069;&#25552;&#20986;&#30340;&#37325;&#25490;&#21464;&#21387;&#22120;&#19981;&#21516;&#65292;STIR&#19981;&#20381;&#36182;&#20110;&#20840;&#23616;/&#23616;&#37096;&#29305;&#24449;&#25552;&#21462;&#65292;&#32780;&#26159;&#20511;&#21161;&#27880;&#24847;&#26426;&#21046;&#30452;&#25509;&#22312;&#20687;&#32032;&#32423;&#21035;&#27604;&#36739;&#26597;&#35810;&#22270;&#20687;&#21644;&#26816;&#32034;&#21040;&#30340;&#20505;&#36873;&#22270;&#20687;&#12290;&#30001;&#27492;&#24471;&#20986;&#30340;&#26041;&#27861;&#23450;&#20041;&#20102;&#19968;&#20010;&#26032;&#30340;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current metric learning approaches for image retrieval are usually based on learning a space of informative latent representations where simple approaches such as the cosine distance will work well. Recent state of the art methods such as HypViT move to more complex embedding spaces that may yield better results but are harder to scale to production environments. In this work, we first construct a simpler model based on triplet loss with hard negatives mining that performs at the state of the art level but does not have these drawbacks. Second, we introduce a novel approach for image retrieval postprocessing called Siamese Transformer for Image Retrieval (STIR) that reranks several top outputs in a single forward pass. Unlike previously proposed Reranking Transformers, STIR does not rely on global/local feature extraction and directly compares a query image and a retrieved candidate on pixel level with the usage of attention mechanism. The resulting approach defines a new state of the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;Transformer&#27169;&#22411;&#65288;XBERT&#65289;&#36827;&#34892;&#26497;&#31471;&#22810;&#26631;&#31614;&#20998;&#31867;&#65292;&#36890;&#36807;&#23558;KG&#31867;&#22411;&#22522;&#20110;&#38382;&#39064;&#25991;&#26412;&#20351;&#29992;&#32467;&#26500;&#21644;&#35821;&#20041;&#29305;&#24449;&#36827;&#34892;&#32858;&#31867;&#65292;&#20197;&#25552;&#39640;&#38382;&#39064;&#22238;&#31572;&#65288;QA&#65289;&#31995;&#32479;&#20013;&#35821;&#20041;&#31572;&#26696;&#31867;&#22411;&#39044;&#27979;&#65288;SMART&#65289;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#24182;&#33719;&#24471;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.12395</link><description>&lt;p&gt;
&#38382;&#39064;&#22238;&#31572;&#20013;&#30340;&#31572;&#26696;&#31867;&#22411;&#39044;&#27979;&#30340;&#26497;&#38480;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Extreme Classification for Answer Type Prediction in Question Answering. (arXiv:2304.12395v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12395
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;Transformer&#27169;&#22411;&#65288;XBERT&#65289;&#36827;&#34892;&#26497;&#31471;&#22810;&#26631;&#31614;&#20998;&#31867;&#65292;&#36890;&#36807;&#23558;KG&#31867;&#22411;&#22522;&#20110;&#38382;&#39064;&#25991;&#26412;&#20351;&#29992;&#32467;&#26500;&#21644;&#35821;&#20041;&#29305;&#24449;&#36827;&#34892;&#32858;&#31867;&#65292;&#20197;&#25552;&#39640;&#38382;&#39064;&#22238;&#31572;&#65288;QA&#65289;&#31995;&#32479;&#20013;&#35821;&#20041;&#31572;&#26696;&#31867;&#22411;&#39044;&#27979;&#65288;SMART&#65289;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#24182;&#33719;&#24471;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#31572;&#26696;&#31867;&#22411;&#39044;&#27979;&#65288;SMART&#65289;&#24050;&#34987;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#38382;&#39064;&#22238;&#31572;&#65288;QA&#65289;&#31995;&#32479;&#30340;&#26377;&#29992;&#27493;&#39588;&#12290; SMART&#20219;&#21153;&#28041;&#21450;&#39044;&#27979;&#32473;&#23450;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#30340;&#21069;k&#20010;&#30693;&#35782;&#22270;&#65288;KG&#65289;&#31867;&#22411;&#12290;&#30001;&#20110;KG&#20013;&#23384;&#22312;&#22823;&#37327;&#31867;&#22411;&#65292;&#36825;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;Transformer&#27169;&#22411;&#65288;XBERT&#65289;&#36827;&#34892;&#26497;&#31471;&#22810;&#26631;&#31614;&#20998;&#31867;&#65292;&#36890;&#36807;&#23558;KG&#31867;&#22411;&#22522;&#20110;&#38382;&#39064;&#25991;&#26412;&#20351;&#29992;&#32467;&#26500;&#21644;&#35821;&#20041;&#29305;&#24449;&#36827;&#34892;&#32858;&#31867;&#12290;&#25105;&#20204;&#20855;&#20307;&#22320;&#25913;&#21892;&#20102;XBERT&#27969;&#31243;&#30340;&#32858;&#31867;&#38454;&#27573;&#65292;&#21033;&#29992;&#20174;KG&#20013;&#27966;&#29983;&#30340;&#25991;&#26412;&#21644;&#32467;&#26500;&#29305;&#24449;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#36825;&#20123;&#29305;&#24449;&#21487;&#20197;&#25552;&#39640;SMART&#20219;&#21153;&#30340;&#31471;&#21040;&#31471;&#24615;&#33021;&#65292;&#24182;&#20135;&#29983;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic answer type prediction (SMART) is known to be a useful step towards effective question answering (QA) systems. The SMART task involves predicting the top-$k$ knowledge graph (KG) types for a given natural language question. This is challenging due to the large number of types in KGs. In this paper, we propose use of extreme multi-label classification using Transformer models (XBERT) by clustering KG types using structural and semantic features based on question text. We specifically improve the clustering stage of the XBERT pipeline using textual and structural features derived from KGs. We show that these features can improve end-to-end performance for the SMART task, and yield state-of-the-art results.
&lt;/p&gt;</description></item><item><title>LongEval-Retrieval&#26159;&#19968;&#20010;&#38754;&#21521;&#25345;&#32493;Web&#25628;&#32034;&#35780;&#20272;&#30340;&#21160;&#24577;&#27979;&#35797;&#38598;&#21512;&#65292;&#26088;&#22312;&#30740;&#31350;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#30340;&#26102;&#38388;&#25345;&#20037;&#24615;&#12290;&#27599;&#20010;&#23376;&#38598;&#21512;&#21253;&#21547;&#19968;&#32452;&#26597;&#35810;&#12289;&#25991;&#26723;&#21644;&#22522;&#20110;&#28857;&#20987;&#27169;&#22411;&#26500;&#24314;&#30340;&#36719;&#20851;&#32852;&#24615;&#35780;&#20272;&#65292;&#25968;&#25454;&#26469;&#33258;Qwant&#65292;&#19968;&#20010;&#38544;&#31169;&#20445;&#25252;&#30340;Web&#25628;&#32034;&#24341;&#25806;&#12290;</title><link>http://arxiv.org/abs/2303.03229</link><description>&lt;p&gt;
LongEval-Retrieval: &#38754;&#21521;&#25345;&#32493;Web&#25628;&#32034;&#35780;&#20272;&#30340;&#27861;&#33521;&#21160;&#24577;&#27979;&#35797;&#38598;&#21512;
&lt;/p&gt;
&lt;p&gt;
LongEval-Retrieval: French-English Dynamic Test Collection for Continuous Web Search Evaluation. (arXiv:2303.03229v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.03229
&lt;/p&gt;
&lt;p&gt;
LongEval-Retrieval&#26159;&#19968;&#20010;&#38754;&#21521;&#25345;&#32493;Web&#25628;&#32034;&#35780;&#20272;&#30340;&#21160;&#24577;&#27979;&#35797;&#38598;&#21512;&#65292;&#26088;&#22312;&#30740;&#31350;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#30340;&#26102;&#38388;&#25345;&#20037;&#24615;&#12290;&#27599;&#20010;&#23376;&#38598;&#21512;&#21253;&#21547;&#19968;&#32452;&#26597;&#35810;&#12289;&#25991;&#26723;&#21644;&#22522;&#20110;&#28857;&#20987;&#27169;&#22411;&#26500;&#24314;&#30340;&#36719;&#20851;&#32852;&#24615;&#35780;&#20272;&#65292;&#25968;&#25454;&#26469;&#33258;Qwant&#65292;&#19968;&#20010;&#38544;&#31169;&#20445;&#25252;&#30340;Web&#25628;&#32034;&#24341;&#25806;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LongEval-Retrieval&#26159;&#19968;&#20010;Web&#25991;&#26723;&#26816;&#32034;&#22522;&#20934;&#65292;&#19987;&#27880;&#20110;&#25345;&#32493;&#26816;&#32034;&#35780;&#20272;&#12290;&#35813;&#27979;&#35797;&#38598;&#21512;&#26088;&#22312;&#29992;&#20110;&#30740;&#31350;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#30340;&#26102;&#38388;&#25345;&#20037;&#24615;&#65292;&#24182;&#23558;&#29992;&#20316;CLEF 2023&#30340;Longitudinal Evaluation of Model Performance Track (LongEval)&#30340;&#27979;&#35797;&#38598;&#21512;&#12290;&#35813;&#22522;&#20934;&#27169;&#25311;&#20102;&#19968;&#20010;&#19981;&#26029;&#28436;&#21464;&#30340;&#20449;&#24687;&#31995;&#32479;&#29615;&#22659;&#65292;&#20363;&#22914;Web&#25628;&#32034;&#24341;&#25806;&#25152;&#22788;&#30340;&#29615;&#22659;&#65292;&#22312;&#36981;&#24490;&#31163;&#32447;&#35780;&#20272;&#30340;Cranfield&#33539;&#20363;&#30340;&#21516;&#26102;&#65292;&#25991;&#26723;&#38598;&#21512;&#12289;&#26597;&#35810;&#20998;&#24067;&#21644;&#30456;&#20851;&#24615;&#37117;&#22312;&#19981;&#26029;&#31227;&#21160;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21160;&#24577;&#27979;&#35797;&#38598;&#21512;&#30340;&#27010;&#24565;&#65292;&#30001;&#36830;&#32493;&#30340;&#23376;&#38598;&#21512;&#32452;&#25104;&#65292;&#27599;&#20010;&#23376;&#38598;&#21512;&#34920;&#31034;&#20449;&#24687;&#31995;&#32479;&#22312;&#32473;&#23450;&#26102;&#38388;&#27493;&#39588;&#30340;&#29366;&#24577;&#12290;&#22312;LongEval-Retrieval&#20013;&#65292;&#27599;&#20010;&#23376;&#38598;&#21512;&#21253;&#21547;&#19968;&#32452;&#26597;&#35810;&#12289;&#25991;&#26723;&#21644;&#22522;&#20110;&#28857;&#20987;&#27169;&#22411;&#26500;&#24314;&#30340;&#36719;&#20851;&#32852;&#24615;&#35780;&#20272;&#12290;&#36825;&#20123;&#25968;&#25454;&#26469;&#33258;Qwant&#65292;&#19968;&#20010;&#38544;&#31169;&#20445;&#25252;&#30340;Web&#25628;&#32034;&#24341;&#25806;&#12290;
&lt;/p&gt;
&lt;p&gt;
LongEval-Retrieval is a Web document retrieval benchmark that focuses on continuous retrieval evaluation. This test collection is intended to be used to study the temporal persistence of Information Retrieval systems and will be used as the test collection in the Longitudinal Evaluation of Model Performance Track (LongEval) at CLEF 2023. This benchmark simulates an evolving information system environment - such as the one a Web search engine operates in - where the document collection, the query distribution, and relevance all move continuously, while following the Cranfield paradigm for offline evaluation. To do that, we introduce the concept of a dynamic test collection that is composed of successive sub-collections each representing the state of an information system at a given time step. In LongEval-Retrieval, each sub-collection contains a set of queries, documents, and soft relevance assessments built from click models. The data comes from Qwant, a privacy-preserving Web search e
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#25512;&#33616;&#31995;&#32479;&#30340;&#26412;&#22320;&#31574;&#30053;&#25913;&#36827;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#29616;&#22330;&#26657;&#27491;&#65292;&#26131;&#20110;&#20174;&#25968;&#25454;&#20013;&#20272;&#35745;&#65292;&#36866;&#29992;&#20110;&#20197;&#21069;&#30340;&#31574;&#30053;&#36136;&#37327;&#36739;&#39640;&#20294;&#25968;&#37327;&#36739;&#23569;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2212.11431</link><description>&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#30340;&#26412;&#22320;&#31574;&#30053;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Local Policy Improvement for Recommender Systems. (arXiv:2212.11431v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.11431
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#25512;&#33616;&#31995;&#32479;&#30340;&#26412;&#22320;&#31574;&#30053;&#25913;&#36827;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#29616;&#22330;&#26657;&#27491;&#65292;&#26131;&#20110;&#20174;&#25968;&#25454;&#20013;&#20272;&#35745;&#65292;&#36866;&#29992;&#20110;&#20197;&#21069;&#30340;&#31574;&#30053;&#36136;&#37327;&#36739;&#39640;&#20294;&#25968;&#37327;&#36739;&#23569;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#22522;&#20110;&#29992;&#25143;&#36807;&#21435;&#30340;&#20114;&#21160;&#34892;&#20026;&#32780;&#39044;&#27979;&#20182;&#20204;&#21487;&#33021;&#20250;&#19982;&#21738;&#20123;&#39033;&#30446;&#20132;&#20114;&#12290;&#35299;&#20915;&#35813;&#38382;&#39064;&#30340;&#24120;&#29992;&#26041;&#27861;&#26159;&#36890;&#36807;&#30417;&#30563;&#23398;&#20064;&#65292;&#20294;&#26368;&#36817;&#30340;&#36827;&#23637;&#36716;&#21521;&#20102;&#22522;&#20110;&#22870;&#21169;&#65288;&#20363;&#22914;&#29992;&#25143;&#21442;&#19982;&#24230;&#65289;&#30340;&#31574;&#30053;&#20248;&#21270;&#12290;&#21518;&#32773;&#38754;&#20020;&#30340;&#25361;&#25112;&#20043;&#19968;&#26159;&#31574;&#30053;&#19981;&#21305;&#37197;&#65306;&#25105;&#20204;&#21482;&#33021;&#22522;&#20110;&#20197;&#21069;&#37096;&#32626;&#31574;&#30053;&#25910;&#38598;&#21040;&#30340;&#25968;&#25454;&#26469;&#35757;&#32451;&#26032;&#30340;&#31574;&#30053;&#12290;&#20256;&#32479;&#30340;&#26041;&#27861;&#26159;&#36890;&#36807;&#37325;&#35201;&#24615;&#37319;&#26679;&#26657;&#27491;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#23384;&#22312;&#23454;&#38469;&#38480;&#21046;&#12290;&#25105;&#20204;&#24314;&#35758;&#19968;&#31181;&#19981;&#38656;&#35201;&#29616;&#22330;&#26657;&#27491;&#30340;&#26412;&#22320;&#31574;&#30053;&#25913;&#36827;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#35745;&#31639;&#21644;&#20248;&#21270;&#30446;&#26631;&#31574;&#30053;&#39044;&#26399;&#22870;&#21169;&#30340;&#19979;&#38480;&#65292;&#36825;&#26131;&#20110;&#20174;&#25968;&#25454;&#20013;&#20272;&#35745;&#24182;&#19988;&#19981;&#28041;&#21450;&#23494;&#24230;&#27604;&#65288;&#20363;&#22914;&#22312;&#37325;&#35201;&#24615;&#37319;&#26679;&#26657;&#27491;&#20013;&#20986;&#29616;&#30340;&#27604;&#29575;&#65289;&#12290;&#36825;&#31181;&#26412;&#22320;&#31574;&#30053;&#25913;&#36827;&#33539;&#20363;&#38750;&#24120;&#36866;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#65292;&#22240;&#20026;&#20197;&#21069;&#30340;&#31574;&#30053;&#36890;&#24120;&#36136;&#37327;&#36739;&#39640;&#65292;&#31574;&#30053;&#30340;&#25968;&#37327;&#20063;&#24456;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommender systems predict what items a user will interact with next, based on their past interactions. The problem is often approached through supervised learning, but recent advancements have shifted towards policy optimization of rewards (e.g., user engagement). One challenge with the latter is policy mismatch: we are only able to train a new policy given data collected from a previously-deployed policy. The conventional way to address this problem is through importance sampling correction, but this comes with practical limitations. We suggest an alternative approach of local policy improvement without off-policy correction. Our method computes and optimizes a lower bound of expected reward of the target policy, which is easy to estimate from data and does not involve density ratios (such as those appearing in importance sampling correction). This local policy improvement paradigm is ideal for recommender systems, as previous policies are typically of decent quality and policies ar
&lt;/p&gt;</description></item></channel></rss>