<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>LightLM&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#22522;&#20110;Transformer&#30340;&#29983;&#25104;&#25512;&#33616;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#36731;&#37327;&#32423;&#28145;&#31364;Transformer&#26550;&#26500;&#26469;&#23454;&#29616;&#30452;&#25509;&#29983;&#25104;&#25512;&#33616;&#39033;&#12290;</title><link>http://arxiv.org/abs/2310.17488</link><description>&lt;p&gt;
LightLM: &#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#22522;&#20110;Transformer&#30340;&#29983;&#25104;&#25512;&#33616;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
LightLM: A Lightweight Deep and Narrow Language Model for Generative Recommendation. (arXiv:2310.17488v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17488
&lt;/p&gt;
&lt;p&gt;
LightLM&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#22522;&#20110;Transformer&#30340;&#29983;&#25104;&#25512;&#33616;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#36731;&#37327;&#32423;&#28145;&#31364;Transformer&#26550;&#26500;&#26469;&#23454;&#29616;&#30452;&#25509;&#29983;&#25104;&#25512;&#33616;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;LightLM&#65292;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#22522;&#20110;Transformer&#30340;&#29983;&#25104;&#25512;&#33616;&#27169;&#22411;&#12290;&#22312;NLP&#21644;&#35270;&#35273;&#31561;&#21508;&#20010;&#20154;&#24037;&#26234;&#33021;&#23376;&#39046;&#22495;&#20013;&#65292;&#22522;&#20110;Transformer&#30340;&#29983;&#25104;&#24314;&#27169;&#24050;&#32463;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#32780;&#29983;&#25104;&#25512;&#33616;&#30001;&#20110;&#20854;&#23545;&#20010;&#24615;&#21270;&#29983;&#25104;&#24314;&#27169;&#30340;&#29420;&#29305;&#38656;&#27714;&#65292;&#20173;&#22788;&#20110;&#21021;&#32423;&#38454;&#27573;&#12290;&#29616;&#26377;&#30340;&#29983;&#25104;&#25512;&#33616;&#26041;&#27861;&#36890;&#24120;&#20351;&#29992;&#38754;&#21521;NLP&#30340;Transformer&#26550;&#26500;&#65292;&#22914;T5&#12289;GPT&#12289;LLaMA&#21644;M6&#65292;&#36825;&#20123;&#27169;&#22411;&#27604;&#36739;&#24222;&#22823;&#65292;&#19988;&#24182;&#27809;&#26377;&#19987;&#38376;&#38024;&#23545;&#25512;&#33616;&#20219;&#21153;&#36827;&#34892;&#35774;&#35745;&#12290;LightLM&#36890;&#36807;&#24341;&#20837;&#36731;&#37327;&#32423;&#28145;&#31364;Transformer&#26550;&#26500;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#35813;&#26550;&#26500;&#29305;&#21035;&#36866;&#29992;&#20110;&#30452;&#25509;&#29983;&#25104;&#25512;&#33616;&#39033;&#12290;&#36825;&#31181;&#32467;&#26500;&#23545;&#20110;&#30452;&#25509;&#30340;&#29983;&#25104;&#25512;&#33616;&#38750;&#24120;&#21512;&#36866;&#65292;&#22240;&#20026;&#36755;&#20837;&#20027;&#35201;&#30001;&#36866;&#21512;&#27169;&#22411;&#23481;&#37327;&#30340;&#30701;&#26631;&#35760;&#32452;&#25104;&#65292;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#19981;&#38656;&#35201;&#22826;&#23485;&#30340;&#32467;&#26500;&#12290;&#25105;&#20204;&#36824;...
&lt;/p&gt;
&lt;p&gt;
This paper presents LightLM, a lightweight Transformer-based language model for generative recommendation. While Transformer-based generative modeling has gained importance in various AI sub-fields such as NLP and vision, generative recommendation is still in its infancy due to its unique demand on personalized generative modeling. Existing works on generative recommendation often use NLP-oriented Transformer architectures such as T5, GPT, LLaMA and M6, which are heavy-weight and are not specifically designed for recommendation tasks. LightLM tackles the issue by introducing a light-weight deep and narrow Transformer architecture, which is specifically tailored for direct generation of recommendation items. This structure is especially apt for straightforward generative recommendation and stems from the observation that language model does not have to be too wide for this task, as the input predominantly consists of short tokens that are well-suited for the model's capacity. We also sh
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FMMRec&#30340;&#20844;&#24179;&#24863;&#30693;&#22810;&#27169;&#24577;&#25512;&#33616;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#27169;&#24577;&#34920;&#31034;&#20013;&#20998;&#31163;&#25935;&#24863;&#21644;&#38750;&#25935;&#24863;&#20449;&#24687;&#65292;&#23454;&#29616;&#26356;&#20844;&#24179;&#30340;&#34920;&#31034;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2310.17373</link><description>&lt;p&gt;
FMMRec: &#20844;&#24179;&#24863;&#30693;&#30340;&#22810;&#27169;&#24577;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
FMMRec: Fairness-aware Multimodal Recommendation. (arXiv:2310.17373v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17373
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FMMRec&#30340;&#20844;&#24179;&#24863;&#30693;&#22810;&#27169;&#24577;&#25512;&#33616;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#27169;&#24577;&#34920;&#31034;&#20013;&#20998;&#31163;&#25935;&#24863;&#21644;&#38750;&#25935;&#24863;&#20449;&#24687;&#65292;&#23454;&#29616;&#26356;&#20844;&#24179;&#30340;&#34920;&#31034;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22810;&#27169;&#24577;&#25512;&#33616;&#22240;&#20026;&#21487;&#20197;&#26377;&#25928;&#35299;&#20915;&#25968;&#25454;&#31232;&#30095;&#38382;&#39064;&#24182;&#32467;&#21512;&#21508;&#31181;&#27169;&#24577;&#30340;&#34920;&#31034;&#32780;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#23613;&#31649;&#22810;&#27169;&#24577;&#25512;&#33616;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#24341;&#20837;&#19981;&#21516;&#30340;&#27169;&#24577;&#65288;&#20363;&#22914;&#22270;&#20687;&#12289;&#25991;&#26412;&#21644;&#38899;&#39057;&#65289;&#21487;&#33021;&#20250;&#23558;&#26356;&#22810;&#29992;&#25143;&#30340;&#25935;&#24863;&#20449;&#24687;&#65288;&#20363;&#22914;&#24615;&#21035;&#21644;&#24180;&#40836;&#65289;&#26292;&#38706;&#32473;&#25512;&#33616;&#31995;&#32479;&#65292;&#20174;&#32780;&#23548;&#33268;&#26356;&#20005;&#37325;&#30340;&#19981;&#20844;&#24179;&#38382;&#39064;&#12290;&#23613;&#31649;&#24050;&#32463;&#26377;&#24456;&#22810;&#20851;&#20110;&#20844;&#24179;&#24615;&#30340;&#21162;&#21147;&#65292;&#20294;&#29616;&#26377;&#30340;&#20844;&#24179;&#24615;&#26041;&#27861;&#35201;&#20040;&#19982;&#22810;&#27169;&#24577;&#24773;&#22659;&#19981;&#20860;&#23481;&#65292;&#35201;&#20040;&#30001;&#20110;&#24573;&#35270;&#22810;&#27169;&#24577;&#20869;&#23481;&#30340;&#25935;&#24863;&#20449;&#24687;&#32780;&#23548;&#33268;&#20844;&#24179;&#24615;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#20102;&#22312;&#22810;&#27169;&#24577;&#25512;&#33616;&#20013;&#23454;&#29616;&#21453;&#20107;&#23454;&#20844;&#24179;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20844;&#24179;&#24863;&#30693;&#22810;&#27169;&#24577;&#25512;&#33616;&#26041;&#27861;&#65288;&#31216;&#20026;FMMRec&#65289;&#65292;&#36890;&#36807;&#20174;&#27169;&#24577;&#34920;&#31034;&#20013;&#20998;&#31163;&#25935;&#24863;&#21644;&#38750;&#25935;&#24863;&#20449;&#24687;&#65292;&#24182;&#21033;&#29992;&#20998;&#31163;&#21518;&#30340;&#27169;&#24577;&#34920;&#31034;&#26469;&#25351;&#23548;&#26356;&#20844;&#24179;&#30340;&#34920;&#31034;&#23398;&#20064;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, multimodal recommendations have gained increasing attention for effectively addressing the data sparsity problem by incorporating modality-based representations. Although multimodal recommendations excel in accuracy, the introduction of different modalities (e.g., images, text, and audio) may expose more users' sensitive information (e.g., gender and age) to recommender systems, resulting in potentially more serious unfairness issues. Despite many efforts on fairness, existing fairness-aware methods are either incompatible with multimodal scenarios, or lead to suboptimal fairness performance due to neglecting sensitive information of multimodal content. To achieve counterfactual fairness in multimodal recommendations, we propose a novel fairness-aware multimodal recommendation approach (dubbed as FMMRec) to disentangle the sensitive and non-sensitive information from modal representations and leverage the disentangled modal representations to guide fairer representation learn
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#22312;&#19975;&#32500;&#32593;&#39046;&#22495;&#30340;&#28508;&#21147;&#65292;&#29305;&#21035;&#20851;&#27880;&#22270;&#20687;&#29983;&#25104;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;WebDiffusion&#24037;&#20855;&#26469;&#27169;&#25311;&#22522;&#20110;&#31283;&#23450;&#25193;&#25955;&#30340;&#19975;&#32500;&#32593;&#65292;&#24182;&#35780;&#20272;&#20102;&#29983;&#25104;&#22270;&#20687;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.17370</link><description>&lt;p&gt;
&#25506;&#32034;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#22312;&#19975;&#32500;&#32593;&#39046;&#22495;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
Exploring the Potential of Generative AI for the World Wide Web. (arXiv:2310.17370v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17370
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#22312;&#19975;&#32500;&#32593;&#39046;&#22495;&#30340;&#28508;&#21147;&#65292;&#29305;&#21035;&#20851;&#27880;&#22270;&#20687;&#29983;&#25104;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;WebDiffusion&#24037;&#20855;&#26469;&#27169;&#25311;&#22522;&#20110;&#31283;&#23450;&#25193;&#25955;&#30340;&#19975;&#32500;&#32593;&#65292;&#24182;&#35780;&#20272;&#20102;&#29983;&#25104;&#22270;&#20687;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#26159;&#19968;&#31181;&#20808;&#36827;&#30340;&#25216;&#26415;&#65292;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#21644;&#29992;&#25143;&#25552;&#31034;&#33021;&#22815;&#29983;&#25104;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#21508;&#31181;&#23186;&#20307;&#20869;&#23481;&#12290;&#22312;2022&#24180;&#21040;2023&#24180;&#26399;&#38388;&#65292;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#22312;AI&#30005;&#24433;&#21040;&#32842;&#22825;&#26426;&#22120;&#20154;&#31561;&#20247;&#22810;&#24212;&#29992;&#39046;&#22495;&#36805;&#36895;&#22686;&#38271;&#12290;&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#22312;&#19975;&#32500;&#32593;&#39046;&#22495;&#30340;&#28508;&#21147;&#65292;&#29305;&#21035;&#20851;&#27880;&#22270;&#20687;&#29983;&#25104;&#12290;&#32593;&#32476;&#24320;&#21457;&#20154;&#21592;&#24050;&#32463;&#21033;&#29992;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#26469;&#36741;&#21161;&#32534;&#20889;&#25991;&#26412;&#21644;&#22270;&#20687;&#65292;&#32780;&#32593;&#32476;&#27983;&#35272;&#22120;&#26410;&#26469;&#21487;&#33021;&#20250;&#20351;&#29992;&#23427;&#26469;&#26412;&#22320;&#29983;&#25104;&#22270;&#20687;&#65292;&#20197;&#20462;&#22797;&#25439;&#22351;&#30340;&#32593;&#39029;&#12289;&#33410;&#30465;&#24102;&#23485;&#21644;&#22686;&#24378;&#38544;&#31169;&#20445;&#25252;&#12290;&#20026;&#20102;&#25506;&#32034;&#36825;&#19968;&#30740;&#31350;&#39046;&#22495;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#27454;&#21517;&#20026;WebDiffusion&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#20174;&#23458;&#25143;&#31471;&#21644;&#26381;&#21153;&#22120;&#30340;&#35282;&#24230;&#27169;&#25311;&#30001;&#31283;&#23450;&#25193;&#25955;&#65288;&#19968;&#31181;&#27969;&#34892;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#65289;&#39537;&#21160;&#30340;&#19975;&#32500;&#32593;&#12290;WebDiffusion&#36824;&#25903;&#25345;&#29992;&#25143;&#24847;&#35265;&#30340;&#20247;&#21253;&#65292;&#25105;&#20204;&#21033;&#29992;&#36825;&#19968;&#21151;&#33021;&#35780;&#20272;&#20102;&#29983;&#25104;&#22270;&#20687;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Artificial Intelligence (AI) is a cutting-edge technology capable of producing text, images, and various media content leveraging generative models and user prompts. Between 2022 and 2023, generative AI surged in popularity with a plethora of applications spanning from AI-powered movies to chatbots. In this paper, we delve into the potential of generative AI within the realm of the World Wide Web, specifically focusing on image generation. Web developers already harness generative AI to help crafting text and images, while Web browsers might use it in the future to locally generate images for tasks like repairing broken webpages, conserving bandwidth, and enhancing privacy. To explore this research area, we have developed WebDiffusion, a tool that allows to simulate a Web powered by stable diffusion, a popular text-to-image model, from both a client and server perspective. WebDiffusion further supports crowdsourcing of user opinions, which we use to evaluate the quality and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#34920;&#26126;&#65292;&#23545;&#20110;&#19981;&#21516;&#30340;&#19979;&#28216;&#35821;&#35328;&#20219;&#21153;&#65292;&#21482;&#23545;&#35821;&#35328;&#32534;&#30721;&#22120;&#30340;&#37096;&#20998;&#23618;&#36827;&#34892;&#32454;&#35843;&#21363;&#21487;&#33719;&#24471;&#25509;&#36817;&#29978;&#33267;&#20248;&#20110;&#32454;&#35843;&#25152;&#26377;&#23618;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#39640;&#25928;&#24230;&#37327;&#26041;&#27861;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#21487;&#20197;&#36873;&#25321;&#24615;&#24494;&#35843;&#23548;&#33268;&#24378;&#22823;&#19979;&#28216;&#24615;&#33021;&#30340;&#23618;&#12290;&#30740;&#31350;&#31361;&#20986;&#34920;&#26126;&#65292;&#20219;&#21153;&#29305;&#23450;&#20449;&#24687;&#36890;&#24120;&#23616;&#37096;&#21270;&#22312;&#23569;&#25968;&#23618;&#20869;&#65292;&#21482;&#35843;&#25972;&#36825;&#20123;&#23618;&#23601;&#36275;&#22815;&#20102;&#12290;</title><link>http://arxiv.org/abs/2310.17041</link><description>&lt;p&gt;
&#20851;&#20110;&#35821;&#35328;&#32534;&#30721;&#22120;&#30340;&#25163;&#26415;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
On Surgical Fine-tuning for Language Encoders. (arXiv:2310.17041v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17041
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#34920;&#26126;&#65292;&#23545;&#20110;&#19981;&#21516;&#30340;&#19979;&#28216;&#35821;&#35328;&#20219;&#21153;&#65292;&#21482;&#23545;&#35821;&#35328;&#32534;&#30721;&#22120;&#30340;&#37096;&#20998;&#23618;&#36827;&#34892;&#32454;&#35843;&#21363;&#21487;&#33719;&#24471;&#25509;&#36817;&#29978;&#33267;&#20248;&#20110;&#32454;&#35843;&#25152;&#26377;&#23618;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#39640;&#25928;&#24230;&#37327;&#26041;&#27861;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#21487;&#20197;&#36873;&#25321;&#24615;&#24494;&#35843;&#23548;&#33268;&#24378;&#22823;&#19979;&#28216;&#24615;&#33021;&#30340;&#23618;&#12290;&#30740;&#31350;&#31361;&#20986;&#34920;&#26126;&#65292;&#20219;&#21153;&#29305;&#23450;&#20449;&#24687;&#36890;&#24120;&#23616;&#37096;&#21270;&#22312;&#23569;&#25968;&#23618;&#20869;&#65292;&#21482;&#35843;&#25972;&#36825;&#20123;&#23618;&#23601;&#36275;&#22815;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32454;&#35843;&#39044;&#35757;&#32451;&#30340;&#31070;&#32463;&#35821;&#35328;&#32534;&#30721;&#22120;&#30340;&#25152;&#26377;&#23618;&#65288;&#20351;&#29992;&#25152;&#26377;&#21442;&#25968;&#25110;&#20351;&#29992;&#21442;&#25968;&#39640;&#25928;&#30340;&#26041;&#27861;&#65289;&#24448;&#24448;&#26159;&#23558;&#20854;&#36866;&#24212;&#20110;&#26032;&#20219;&#21153;&#30340;&#40664;&#35748;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35777;&#25454;&#65292;&#23545;&#20110;&#19981;&#21516;&#30340;&#19979;&#28216;&#35821;&#35328;&#20219;&#21153;&#65292;&#20165;&#32454;&#35843;&#37096;&#20998;&#23618;&#21363;&#21487;&#33719;&#24471;&#25509;&#36817;&#29978;&#33267;&#20248;&#20110;&#32454;&#35843;&#35821;&#35328;&#32534;&#30721;&#22120;&#30340;&#25152;&#26377;&#23618;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Fisher&#20449;&#24687;&#30697;&#38453;&#30340;&#23545;&#35282;&#32447;&#65288;FIM&#35780;&#20998;&#65289;&#30340;&#39640;&#25928;&#24230;&#37327;&#26041;&#27861;&#65292;&#29992;&#20110;&#36873;&#25321;&#29992;&#20110;&#36873;&#25321;&#24615;&#24494;&#35843;&#30340;&#20505;&#36873;&#23618;&#12290;&#25105;&#20204;&#22312;GLUE&#21644;SuperGLUE&#20219;&#21153;&#20197;&#21450;&#19981;&#21516;&#30340;&#35821;&#35328;&#32534;&#30721;&#22120;&#19978;&#32463;&#39564;&#24615;&#22320;&#23637;&#31034;&#20102;&#65292;&#36825;&#20010;&#24230;&#37327;&#21487;&#20197;&#26377;&#25928;&#36873;&#25321;&#23548;&#33268;&#24378;&#22823;&#19979;&#28216;&#24615;&#33021;&#30340;&#23618;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#31361;&#20986;&#20102;&#19982;&#32473;&#23450;&#30340;&#19979;&#28216;&#20219;&#21153;&#23545;&#24212;&#30340;&#20219;&#21153;&#29305;&#23450;&#20449;&#24687;&#36890;&#24120;&#23616;&#37096;&#21270;&#22312;&#23569;&#25968;&#23618;&#20869;&#65292;&#21482;&#35843;&#25972;&#36825;&#20123;&#23618;&#23545;&#20110;&#24378;&#22823;&#30340;&#24615;&#33021;&#23601;&#36275;&#22815;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning all the layers of a pre-trained neural language encoder (either using all the parameters or using parameter-efficient methods) is often the de-facto way of adapting it to a new task. We show evidence that for different downstream language tasks, fine-tuning only a subset of layers is sufficient to obtain performance that is close to and often better than fine-tuning all the layers in the language encoder. We propose an efficient metric based on the diagonal of the Fisher information matrix (FIM score), to select the candidate layers for selective fine-tuning. We show, empirically on GLUE and SuperGLUE tasks and across distinct language encoders, that this metric can effectively select layers leading to a strong downstream performance. Our work highlights that task-specific information corresponding to a given downstream task is often localized within a few layers, and tuning only those is sufficient for strong performance. Additionally, we demonstrate the robustness of the 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Word2vec&#22270;&#27169;&#22411;&#30340;&#25991;&#26723;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#25429;&#25417;&#25991;&#26723;&#30340;&#19978;&#19979;&#25991;&#21644;&#39118;&#26684;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#30340;&#20316;&#32773;&#37492;&#23450;&#21644;&#20307;&#35009;&#26816;&#27979;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2310.16972</link><description>&lt;p&gt;
Word2vec&#22270;&#27169;&#22411;&#22312;&#25991;&#23398;&#20998;&#26512;&#20013;&#30340;&#20316;&#32773;&#37492;&#23450;&#21644;&#20307;&#35009;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
The Word2vec Graph Model for Author Attribution and Genre Detection in Literary Analysis. (arXiv:2310.16972v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16972
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Word2vec&#22270;&#27169;&#22411;&#30340;&#25991;&#26723;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#25429;&#25417;&#25991;&#26723;&#30340;&#19978;&#19979;&#25991;&#21644;&#39118;&#26684;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#30340;&#20316;&#32773;&#37492;&#23450;&#21644;&#20307;&#35009;&#26816;&#27979;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#26512;&#20316;&#32773;&#21644;&#25991;&#31456;&#30340;&#20889;&#20316;&#39118;&#26684;&#23545;&#25903;&#25345;&#21508;&#31181;&#25991;&#23398;&#20998;&#26512;&#65292;&#22914;&#20316;&#32773;&#37492;&#23450;&#21644;&#20307;&#35009;&#26816;&#27979;&#33267;&#20851;&#37325;&#35201;&#12290;&#22810;&#24180;&#26469;&#65292;&#21253;&#25324;&#25991;&#20307;&#23398;&#12289;&#35789;&#34955;&#27169;&#22411;&#21644;n-gram&#31561;&#20016;&#23500;&#30340;&#29305;&#24449;&#38598;&#34987;&#24191;&#27867;&#29992;&#20110;&#36827;&#34892;&#27492;&#31867;&#20998;&#26512;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#29305;&#24449;&#30340;&#26377;&#25928;&#24615;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#29305;&#23450;&#35821;&#35328;&#30340;&#35821;&#35328;&#29305;&#24449;&#21644;&#25968;&#25454;&#38598;&#30340;&#29305;&#28857;&#12290;&#22240;&#27492;&#65292;&#22522;&#20110;&#36825;&#20123;&#29305;&#24449;&#38598;&#30340;&#25216;&#26415;&#26080;&#27861;&#22312;&#19981;&#21516;&#39046;&#22495;&#33719;&#24471;&#26399;&#26395;&#30340;&#32467;&#26524;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Word2vec&#22270;&#27169;&#22411;&#30340;&#25991;&#26723;&#24314;&#27169;&#26041;&#27861;&#65292;&#21487;&#20197;&#20934;&#30830;&#25429;&#25417;&#25991;&#26723;&#30340;&#19978;&#19979;&#25991;&#21644;&#39118;&#26684;&#12290;&#36890;&#36807;&#20351;&#29992;&#36825;&#20123;&#22522;&#20110;Word2vec&#22270;&#30340;&#29305;&#24449;&#65292;&#25105;&#20204;&#36827;&#34892;&#20998;&#31867;&#20174;&#32780;&#36827;&#34892;&#20316;&#32773;&#37492;&#23450;&#21644;&#20307;&#35009;&#26816;&#27979;&#20219;&#21153;&#12290;&#25105;&#20204;&#36890;&#36807;&#35814;&#32454;&#30340;&#23454;&#39564;&#30740;&#31350;&#23545;&#24191;&#27867;&#30340;&#25991;&#23398;&#20316;&#21697;&#36827;&#34892;&#20102;&#39564;&#35777;&#65292;&#32467;&#26524;&#34920;&#26126;&#19982;&#20256;&#32479;&#22522;&#20110;&#29305;&#24449;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#36825;&#31181;&#26041;&#27861;&#26356;&#20026;&#26377;&#25928;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#26159;&#20844;&#24320;&#21487;&#29992;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Analyzing the writing styles of authors and articles is a key to supporting various literary analyses such as author attribution and genre detection. Over the years, rich sets of features that include stylometry, bag-of-words, n-grams have been widely used to perform such analysis. However, the effectiveness of these features largely depends on the linguistic aspects of a particular language and datasets specific characteristics. Consequently, techniques based on these feature sets cannot give desired results across domains. In this paper, we propose a novel Word2vec graph based modeling of a document that can rightly capture both context and style of the document. By using these Word2vec graph based features, we perform classification to perform author attribution and genre detection tasks. Our detailed experimental study with a comprehensive set of literary writings shows the effectiveness of this method over traditional feature based approaches. Our code and data are publicly availa
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;Web-DRO&#65292;&#23427;&#21033;&#29992;&#32593;&#32476;&#32467;&#26500;&#36827;&#34892;&#32858;&#31867;&#24182;&#37325;&#26032;&#21152;&#26435;&#65292;&#22312;&#26080;&#30417;&#30563;&#22330;&#26223;&#20013;&#26174;&#33879;&#25552;&#39640;&#20102;&#26816;&#32034;&#25928;&#26524;&#12290;&#32676;&#32452;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#26041;&#27861;&#25351;&#23548;&#27169;&#22411;&#23545;&#39640;&#23545;&#27604;&#25439;&#22833;&#30340;&#32676;&#32452;&#20998;&#37197;&#26356;&#22810;&#26435;&#37325;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#26356;&#21152;&#20851;&#27880;&#26368;&#22351;&#24773;&#20917;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#32467;&#21512;URL&#20449;&#24687;&#30340;&#32593;&#32476;&#22270;&#35757;&#32451;&#33021;&#36798;&#21040;&#26368;&#20339;&#30340;&#32858;&#31867;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.16605</link><description>&lt;p&gt;
&#22522;&#20110;&#32593;&#32476;&#22270;&#30340;&#20998;&#24067;&#40065;&#26834;&#26080;&#30417;&#30563;&#23494;&#38598;&#26816;&#32034;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Distributionally Robust Unsupervised Dense Retrieval Training on Web Graphs. (arXiv:2310.16605v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16605
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;Web-DRO&#65292;&#23427;&#21033;&#29992;&#32593;&#32476;&#32467;&#26500;&#36827;&#34892;&#32858;&#31867;&#24182;&#37325;&#26032;&#21152;&#26435;&#65292;&#22312;&#26080;&#30417;&#30563;&#22330;&#26223;&#20013;&#26174;&#33879;&#25552;&#39640;&#20102;&#26816;&#32034;&#25928;&#26524;&#12290;&#32676;&#32452;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#26041;&#27861;&#25351;&#23548;&#27169;&#22411;&#23545;&#39640;&#23545;&#27604;&#25439;&#22833;&#30340;&#32676;&#32452;&#20998;&#37197;&#26356;&#22810;&#26435;&#37325;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#26356;&#21152;&#20851;&#27880;&#26368;&#22351;&#24773;&#20917;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#32467;&#21512;URL&#20449;&#24687;&#30340;&#32593;&#32476;&#22270;&#35757;&#32451;&#33021;&#36798;&#21040;&#26368;&#20339;&#30340;&#32858;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Web-DRO&#65292;&#19968;&#31181;&#22522;&#20110;&#32593;&#32476;&#32467;&#26500;&#36827;&#34892;&#32858;&#31867;&#24182;&#22312;&#23545;&#27604;&#35757;&#32451;&#26399;&#38388;&#37325;&#26032;&#21152;&#26435;&#30340;&#26080;&#30417;&#30563;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#21033;&#29992;&#32593;&#32476;&#22270;&#38142;&#25509;&#24182;&#23545;&#38170;&#28857;-&#25991;&#26723;&#23545;&#36827;&#34892;&#23545;&#27604;&#35757;&#32451;&#65292;&#35757;&#32451;&#19968;&#20010;&#23884;&#20837;&#27169;&#22411;&#29992;&#20110;&#32858;&#31867;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#32676;&#32452;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#26041;&#27861;&#26469;&#37325;&#26032;&#21152;&#26435;&#19981;&#21516;&#30340;&#38170;&#28857;-&#25991;&#26723;&#23545;&#32676;&#32452;&#65292;&#36825;&#25351;&#23548;&#27169;&#22411;&#23558;&#26356;&#22810;&#26435;&#37325;&#20998;&#37197;&#32473;&#23545;&#27604;&#25439;&#22833;&#26356;&#39640;&#30340;&#32676;&#32452;&#65292;&#24182;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#26356;&#21152;&#20851;&#27880;&#26368;&#22351;&#24773;&#20917;&#12290;&#22312;MS MARCO&#21644;BEIR&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;Web-DRO&#22312;&#26080;&#30417;&#30563;&#22330;&#26223;&#20013;&#26174;&#33879;&#25552;&#39640;&#20102;&#26816;&#32034;&#25928;&#26524;&#12290;&#23545;&#32858;&#31867;&#25216;&#26415;&#30340;&#27604;&#36739;&#34920;&#26126;&#65292;&#32467;&#21512;URL&#20449;&#24687;&#30340;&#32593;&#32476;&#22270;&#35757;&#32451;&#33021;&#36798;&#21040;&#26368;&#20339;&#30340;&#32858;&#31867;&#24615;&#33021;&#12290;&#36827;&#19968;&#27493;&#20998;&#26512;&#35777;&#23454;&#20102;&#32676;&#32452;&#26435;&#37325;&#30340;&#31283;&#23450;&#24615;&#21644;&#26377;&#25928;&#24615;&#65292;&#34920;&#26126;&#20102;&#19968;&#33268;&#30340;&#27169;&#22411;&#20559;&#22909;&#20197;&#21450;&#23545;&#26377;&#20215;&#20540;&#25991;&#26723;&#30340;&#26377;&#25928;&#21152;&#26435;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces Web-DRO, an unsupervised dense retrieval model, which clusters documents based on web structures and reweights the groups during contrastive training. Specifically, we first leverage web graph links and contrastively train an embedding model for clustering anchor-document pairs. Then we use Group Distributional Robust Optimization to reweight different clusters of anchor-document pairs, which guides the model to assign more weights to the group with higher contrastive loss and pay more attention to the worst case during training. Our experiments on MS MARCO and BEIR show that our model, Web-DRO, significantly improves the retrieval effectiveness in unsupervised scenarios. A comparison of clustering techniques shows that training on the web graph combining URL information reaches optimal performance on clustering. Further analysis confirms that group weights are stable and valid, indicating consistent model preferences as well as effective up-weighting of valuable 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#36719;&#20214;&#31038;&#21306;&#38382;&#31572;&#20013;&#30340;&#38382;&#39064;&#37325;&#22797;&#26816;&#32034;&#21644;&#30830;&#35748;&#26102;&#38388;&#39044;&#27979;&#30340;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#22823;&#22411;&#36719;&#20214;&#31995;&#32479;&#30340;CQA&#20013;&#65292;&#24110;&#21161;&#24537;&#30860;&#30340;&#19987;&#23478;&#31649;&#29702;&#21592;&#26356;&#39640;&#25928;&#22320;&#22788;&#29702;&#37325;&#22797;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.05035</link><description>&lt;p&gt;
&#36719;&#20214;&#31038;&#21306;&#20013;&#30340;&#37325;&#22797;&#38382;&#39064;&#26816;&#32034;&#21644;&#30830;&#35748;&#26102;&#38388;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Duplicate Question Retrieval and Confirmation Time Prediction in Software Communities. (arXiv:2309.05035v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05035
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#36719;&#20214;&#31038;&#21306;&#38382;&#31572;&#20013;&#30340;&#38382;&#39064;&#37325;&#22797;&#26816;&#32034;&#21644;&#30830;&#35748;&#26102;&#38388;&#39044;&#27979;&#30340;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#22823;&#22411;&#36719;&#20214;&#31995;&#32479;&#30340;CQA&#20013;&#65292;&#24110;&#21161;&#24537;&#30860;&#30340;&#19987;&#23478;&#31649;&#29702;&#21592;&#26356;&#39640;&#25928;&#22320;&#22788;&#29702;&#37325;&#22797;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#23384;&#22312;&#22810;&#20010;&#24179;&#21488;&#21644;&#29992;&#25143;&#20043;&#38388;&#30340;&#22823;&#35268;&#27169;&#20849;&#20139;&#20449;&#24687;&#65292;&#19981;&#21516;&#39046;&#22495;&#30340;&#31038;&#21306;&#38382;&#31572;&#65288;CQA&#65289;&#27491;&#22312;&#24555;&#36895;&#22686;&#38271;&#12290;&#38543;&#30528;&#36825;&#20123;&#22312;&#32447;&#24179;&#21488;&#30340;&#24555;&#36895;&#22686;&#38271;&#65292;&#22823;&#37327;&#30340;&#23384;&#26723;&#25968;&#25454;&#20351;&#24471;&#31649;&#29702;&#21592;&#38590;&#20197;&#26816;&#32034;&#26032;&#38382;&#39064;&#30340;&#21487;&#33021;&#37325;&#22797;&#65292;&#24182;&#22312;&#36866;&#24403;&#30340;&#26102;&#38388;&#35782;&#21035;&#21644;&#30830;&#35748;&#29616;&#26377;&#38382;&#39064;&#23545;&#20316;&#20026;&#37325;&#22797;&#38382;&#39064;&#12290;&#36825;&#20010;&#38382;&#39064;&#22312;&#31867;&#20284;askubuntu&#36825;&#26679;&#30340;&#22823;&#22411;&#36719;&#20214;&#31995;&#32479;&#23545;&#24212;&#30340;CQA&#20013;&#23588;&#20026;&#37325;&#35201;&#65292;&#31649;&#29702;&#21592;&#38656;&#35201;&#26159;&#19987;&#23478;&#25165;&#33021;&#29702;&#35299;&#38382;&#39064;&#26159;&#21542;&#20026;&#37325;&#22797;&#12290;&#38656;&#35201;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;&#36825;&#26679;&#30340;CQA&#24179;&#21488;&#19978;&#65292;&#20027;&#35201;&#25361;&#25112;&#22312;&#20110;&#31649;&#29702;&#21592;&#26412;&#36523;&#23601;&#26159;&#19987;&#23478;&#65292;&#22240;&#27492;&#36890;&#24120;&#26102;&#38388;&#38750;&#24120;&#23453;&#36149;&#65292;&#38750;&#24120;&#24537;&#30860;&#12290;&#20026;&#20102;&#24110;&#21161;&#31649;&#29702;&#21592;&#23436;&#25104;&#20219;&#21153;&#65292;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;askubuntu CQA&#24179;&#21488;&#19978;&#30340;&#20004;&#20010;&#37325;&#22823;&#38382;&#39064;&#65306;&#65288;1&#65289;&#32473;&#23450;&#19968;&#20010;&#26032;&#38382;&#39064;&#65292;&#26816;&#32034;&#37325;&#22797;&#38382;&#39064;&#65307;&#65288;2&#65289;&#37325;&#22797;&#38382;&#39064;&#30830;&#35748;&#30340;&#26102;&#38388;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Community Question Answering (CQA) in different domains is growing at a large scale because of the availability of several platforms and huge shareable information among users. With the rapid growth of such online platforms, a massive amount of archived data makes it difficult for moderators to retrieve possible duplicates for a new question and identify and confirm existing question pairs as duplicates at the right time. This problem is even more critical in CQAs corresponding to large software systems like askubuntu where moderators need to be experts to comprehend something as a duplicate. Note that the prime challenge in such CQA platforms is that the moderators are themselves experts and are therefore usually extremely busy with their time being extraordinarily expensive. To facilitate the task of the moderators, in this work, we have tackled two significant issues for the askubuntu CQA platform: (1) retrieval of duplicate questions given a new question and (2) duplicate question 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;DocumentNet&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;Web&#19978;&#25910;&#38598;&#22823;&#35268;&#27169;&#21644;&#24369;&#26631;&#27880;&#30340;&#25968;&#25454;&#65292;&#24357;&#21512;&#20102;&#25991;&#26723;&#39044;&#35757;&#32451;&#20013;&#30340;&#25968;&#25454;&#24046;&#36317;&#65292;&#24182;&#22312;&#21508;&#31867;VDER&#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2306.08937</link><description>&lt;p&gt;
DocumentNet: &#22312;&#25991;&#26723;&#39044;&#35757;&#32451;&#20013;&#24357;&#21512;&#25968;&#25454;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
DocumentNet: Bridging the Data Gap in Document Pre-Training. (arXiv:2306.08937v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08937
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;DocumentNet&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;Web&#19978;&#25910;&#38598;&#22823;&#35268;&#27169;&#21644;&#24369;&#26631;&#27880;&#30340;&#25968;&#25454;&#65292;&#24357;&#21512;&#20102;&#25991;&#26723;&#39044;&#35757;&#32451;&#20013;&#30340;&#25968;&#25454;&#24046;&#36317;&#65292;&#24182;&#22312;&#21508;&#31867;VDER&#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#25991;&#26723;&#29702;&#35299;&#20219;&#21153;&#65292;&#29305;&#21035;&#26159;&#23500;&#26377;&#35270;&#35273;&#20803;&#32032;&#30340;&#25991;&#26723;&#23454;&#20307;&#26816;&#32034;&#65288;VDER&#65289;&#65292;&#30001;&#20110;&#22312;&#20225;&#19994;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#21463;&#21040;&#20102;&#26497;&#22823;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20005;&#26684;&#30340;&#38544;&#31169;&#32422;&#26463;&#21644;&#39640;&#26114;&#30340;&#26631;&#27880;&#25104;&#26412;&#65292;&#36825;&#20123;&#20219;&#21153;&#30340;&#20844;&#24320;&#21487;&#29992;&#25968;&#25454;&#38750;&#24120;&#26377;&#38480;&#12290;&#26356;&#31967;&#31957;&#30340;&#26159;&#65292;&#26469;&#33258;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;&#19981;&#37325;&#21472;&#23454;&#20307;&#31354;&#38388;&#22952;&#30861;&#20102;&#25991;&#26723;&#31867;&#22411;&#20043;&#38388;&#30340;&#30693;&#35782;&#36716;&#31227;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;Web&#25910;&#38598;&#22823;&#35268;&#27169;&#21644;&#24369;&#26631;&#27880;&#30340;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#20197;&#21033;&#20110;VDER&#27169;&#22411;&#30340;&#35757;&#32451;&#12290;&#25152;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#21517;&#20026;DocumentNet&#65292;&#19981;&#20381;&#36182;&#20110;&#29305;&#23450;&#30340;&#25991;&#26723;&#31867;&#22411;&#25110;&#23454;&#20307;&#38598;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#25152;&#26377;&#30340;VDER&#20219;&#21153;&#12290;&#30446;&#21069;&#30340;DocumentNet&#21253;&#21547;&#20102;30M&#20010;&#25991;&#26723;&#65292;&#28085;&#30422;&#20102;&#36817;400&#20010;&#25991;&#26723;&#31867;&#22411;&#65292;&#32452;&#32455;&#25104;&#20102;&#19968;&#20010;&#22235;&#32423;&#26412;&#20307;&#32467;&#26500;&#12290;&#22312;&#19968;&#31995;&#21015;&#24191;&#27867;&#37319;&#29992;&#30340;VDER&#20219;&#21153;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#24403;&#23558;DocumentNet&#32435;&#20837;&#39044;&#35757;&#32451;&#36807;&#31243;&#26102;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Document understanding tasks, in particular, Visually-rich Document Entity Retrieval (VDER), have gained significant attention in recent years thanks to their broad applications in enterprise AI. However, publicly available data have been scarce for these tasks due to strict privacy constraints and high annotation costs. To make things worse, the non-overlapping entity spaces from different datasets hinder the knowledge transfer between document types. In this paper, we propose a method to collect massive-scale and weakly labeled data from the web to benefit the training of VDER models. The collected dataset, named DocumentNet, does not depend on specific document types or entity sets, making it universally applicable to all VDER tasks. The current DocumentNet consists of 30M documents spanning nearly 400 document types organized in a four-level ontology. Experiments on a set of broadly adopted VDER tasks show significant improvements when DocumentNet is incorporated into the pre-train
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#31890;&#24230;&#36229;&#22270;&#20852;&#36259;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#21382;&#21490;&#23545;&#35805;&#25968;&#25454;&#20016;&#23500;&#24403;&#21069;&#23545;&#35805;&#30340;&#19978;&#19979;&#25991;&#65292;&#20174;&#19981;&#21516;&#35282;&#24230;&#25429;&#25417;&#29992;&#25143;&#20852;&#36259;&#12290;&#37319;&#29992;&#36229;&#22270;&#32467;&#26500;&#34920;&#31034;&#22797;&#26434;&#30340;&#35821;&#20041;&#20851;&#31995;&#65292;&#24314;&#27169;&#29992;&#25143;&#30340;&#21382;&#21490;&#23545;&#35805;&#20250;&#35805;&#65292;&#25429;&#25417;&#31895;&#31890;&#24230;&#30340;&#20250;&#35805;&#32423;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2305.04798</link><description>&lt;p&gt;
&#22810;&#31890;&#24230;&#36229;&#22270;&#20852;&#36259;&#24314;&#27169;&#23545;&#35805;&#24335;&#25512;&#33616;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Multi-grained Hypergraph Interest Modeling for Conversational Recommendation. (arXiv:2305.04798v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04798
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#31890;&#24230;&#36229;&#22270;&#20852;&#36259;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#21382;&#21490;&#23545;&#35805;&#25968;&#25454;&#20016;&#23500;&#24403;&#21069;&#23545;&#35805;&#30340;&#19978;&#19979;&#25991;&#65292;&#20174;&#19981;&#21516;&#35282;&#24230;&#25429;&#25417;&#29992;&#25143;&#20852;&#36259;&#12290;&#37319;&#29992;&#36229;&#22270;&#32467;&#26500;&#34920;&#31034;&#22797;&#26434;&#30340;&#35821;&#20041;&#20851;&#31995;&#65292;&#24314;&#27169;&#29992;&#25143;&#30340;&#21382;&#21490;&#23545;&#35805;&#20250;&#35805;&#65292;&#25429;&#25417;&#31895;&#31890;&#24230;&#30340;&#20250;&#35805;&#32423;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#30340;&#22810;&#36718;&#23545;&#35805;&#19982;&#29992;&#25143;&#36827;&#34892;&#20132;&#20114;&#65292;&#26088;&#22312;&#20026;&#29992;&#25143;&#30340;&#21363;&#26102;&#20449;&#24687;&#38656;&#27714;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#25512;&#33616;&#12290;&#23613;&#31649;&#24050;&#32463;&#20570;&#20986;&#20102;&#24456;&#22810;&#26377;&#25928;&#30340;&#23545;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;&#65292;&#20294;&#22823;&#22810;&#25968;&#20173;&#28982;&#38598;&#20013;&#22312;&#24403;&#21069;&#23545;&#35805;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#19978;&#65292;&#36890;&#24120;&#20250;&#36935;&#21040;&#25968;&#25454;&#31232;&#32570;&#30340;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#32771;&#34385;&#21033;&#29992;&#21382;&#21490;&#23545;&#35805;&#25968;&#25454;&#26469;&#20016;&#23500;&#24403;&#21069;&#23545;&#35805;&#30340;&#26377;&#38480;&#19978;&#19979;&#25991;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#31890;&#24230;&#36229;&#22270;&#20852;&#36259;&#24314;&#27169;&#26041;&#27861;&#65292;&#20197;&#20174;&#19981;&#21516;&#30340;&#35282;&#24230;&#25429;&#25417;&#22797;&#26434;&#21382;&#21490;&#25968;&#25454;&#19979;&#30340;&#29992;&#25143;&#20852;&#36259;&#12290;&#20316;&#20026;&#26680;&#24515;&#24605;&#24819;&#65292;&#25105;&#20204;&#20351;&#29992;&#36229;&#22270;&#26469;&#34920;&#31034;&#21382;&#21490;&#23545;&#35805;&#20013;&#22797;&#26434;&#30340;&#35821;&#20041;&#20851;&#31995;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#36229;&#22270;&#32467;&#26500;&#26469;&#24314;&#27169;&#29992;&#25143;&#30340;&#21382;&#21490;&#23545;&#35805;&#20250;&#35805;&#65292;&#24182;&#24418;&#25104;&#19968;&#20010;&#22522;&#20110;&#20250;&#35805;&#30340;&#36229;&#22270;&#65292;&#35813;&#36229;&#22270;&#25429;&#25417;&#20102;&#31895;&#31890;&#24230;&#30340;&#20250;&#35805;&#32423;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conversational recommender system (CRS) interacts with users through multi-turn dialogues in natural language, which aims to provide high-quality recommendations for user's instant information need. Although great efforts have been made to develop effective CRS, most of them still focus on the contextual information from the current dialogue, usually suffering from the data scarcity issue. Therefore, we consider leveraging historical dialogue data to enrich the limited contexts of the current dialogue session.  In this paper, we propose a novel multi-grained hypergraph interest modeling approach to capture user interest beneath intricate historical data from different perspectives. As the core idea, we employ hypergraph to represent complicated semantic relations underlying historical dialogues. In our approach, we first employ the hypergraph structure to model users' historical dialogue sessions and form a session-based hypergraph, which captures coarse-grained, session-level relation
&lt;/p&gt;</description></item></channel></rss>