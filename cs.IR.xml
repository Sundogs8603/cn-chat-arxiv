<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#26412;&#25991;&#38024;&#23545;&#32676;&#20307;&#29992;&#25143;&#25512;&#33616;&#21830;&#21697;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#20123;&#27169;&#22411;&#30456;&#27604;&#29616;&#26377;&#27169;&#22411;&#30340;&#25913;&#36827;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.09447</link><description>&lt;p&gt;
&#20026;&#32676;&#20307;&#29992;&#25143;&#25512;&#33616;&#21830;&#21697;&#30340;&#28145;&#24230;&#31070;&#32463;&#32858;&#21512;
&lt;/p&gt;
&lt;p&gt;
Deep Neural Aggregation for Recommending Items to Group of Users. (arXiv:2307.09447v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09447
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#32676;&#20307;&#29992;&#25143;&#25512;&#33616;&#21830;&#21697;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#20123;&#27169;&#22411;&#30456;&#27604;&#29616;&#26377;&#27169;&#22411;&#30340;&#25913;&#36827;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#31038;&#20250;&#33457;&#36153;&#20102;&#22823;&#37327;&#26102;&#38388;&#22312;&#25968;&#23383;&#20132;&#20114;&#19978;&#65292;&#25105;&#20204;&#30340;&#26085;&#24120;&#34892;&#20026;&#24456;&#22810;&#37117;&#36890;&#36807;&#25968;&#23383;&#25163;&#27573;&#23436;&#25104;&#12290;&#36825;&#23548;&#33268;&#20102;&#35768;&#22810;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#30340;&#20986;&#29616;&#65292;&#24110;&#21161;&#25105;&#20204;&#22312;&#29983;&#27963;&#30340;&#21508;&#20010;&#26041;&#38754;&#36827;&#34892;&#36741;&#21161;&#12290;&#23545;&#20110;&#25968;&#23383;&#31038;&#20250;&#26469;&#35828;&#65292;&#19968;&#20010;&#20851;&#38190;&#30340;&#24037;&#20855;&#26159;&#25512;&#33616;&#31995;&#32479;&#65292;&#23427;&#26159;&#26234;&#33021;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#23398;&#20064;&#25105;&#20204;&#30340;&#36807;&#21435;&#34892;&#20026;&#65292;&#25552;&#20986;&#19982;&#25105;&#20204;&#20852;&#36259;&#30456;&#31526;&#30340;&#26032;&#34892;&#20026;&#24314;&#35758;&#12290;&#20854;&#20013;&#19968;&#20123;&#31995;&#32479;&#19987;&#38376;&#20174;&#29992;&#25143;&#32676;&#20307;&#30340;&#34892;&#20026;&#20013;&#23398;&#20064;&#65292;&#21521;&#24076;&#26395;&#20849;&#21516;&#23436;&#25104;&#26576;&#20010;&#20219;&#21153;&#30340;&#20010;&#20307;&#32676;&#20307;&#25552;&#20986;&#24314;&#35758;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#32676;&#20307;&#25512;&#33616;&#31995;&#32479;&#30340;&#29616;&#29366;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#20351;&#29992;&#26032;&#20852;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#30340;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20351;&#29992;&#22235;&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;&#26368;&#26032;&#27169;&#22411;&#30456;&#27604;&#65292;&#37319;&#29992;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#21487;&#20197;&#21462;&#24471;&#25913;&#36827;&#12290;&#35813;&#27169;&#22411;&#21450;&#25152;&#26377;&#23454;&#39564;&#30340;&#28304;&#20195;&#30721;&#37117;&#21487;&#20379;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern society devotes a significant amount of time to digital interaction. Many of our daily actions are carried out through digital means. This has led to the emergence of numerous Artificial Intelligence tools that assist us in various aspects of our lives. One key tool for the digital society is Recommender Systems, intelligent systems that learn from our past actions to propose new ones that align with our interests. Some of these systems have specialized in learning from the behavior of user groups to make recommendations to a group of individuals who want to perform a joint task. In this article, we analyze the current state of Group Recommender Systems and propose two new models that use emerging Deep Learning architectures. Experimental results demonstrate the improvement achieved by employing the proposed models compared to the state-of-the-art models using four different datasets. The source code of the models, as well as that of all the experiments conducted, is available i
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#26597;&#35810;&#37325;&#26500;&#65288;ZeQR&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#20219;&#21153;&#30340;&#35821;&#35328;&#27169;&#22411;&#26469;&#35299;&#20915;&#23545;&#35805;&#25628;&#32034;&#20013;&#30340;&#25968;&#25454;&#31232;&#30095;&#24615;&#12289;&#35299;&#37322;&#24615;&#19981;&#36275;&#21644;&#27495;&#20041;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.09384</link><description>&lt;p&gt;
&#38646;&#26679;&#26412;&#23545;&#35805;&#25628;&#32034;&#20013;&#30340;&#26597;&#35810;&#37325;&#26500;
&lt;/p&gt;
&lt;p&gt;
Zero-shot Query Reformulation for Conversational Search. (arXiv:2307.09384v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09384
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#26597;&#35810;&#37325;&#26500;&#65288;ZeQR&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#20219;&#21153;&#30340;&#35821;&#35328;&#27169;&#22411;&#26469;&#35299;&#20915;&#23545;&#35805;&#25628;&#32034;&#20013;&#30340;&#25968;&#25454;&#31232;&#30095;&#24615;&#12289;&#35299;&#37322;&#24615;&#19981;&#36275;&#21644;&#27495;&#20041;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35821;&#38899;&#21161;&#25163;&#30340;&#26222;&#21450;&#65292;&#23545;&#35805;&#25628;&#32034;&#22312;&#20449;&#24687;&#26816;&#32034;&#39046;&#22495;&#24341;&#36215;&#20102;&#26356;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#23545;&#35805;&#25628;&#32034;&#20013;&#30340;&#25968;&#25454;&#31232;&#30095;&#24615;&#38382;&#39064;&#20005;&#37325;&#38459;&#30861;&#20102;&#30417;&#30563;&#24335;&#23545;&#35805;&#25628;&#32034;&#26041;&#27861;&#30340;&#36827;&#23637;&#12290;&#22240;&#27492;&#65292;&#30740;&#31350;&#20154;&#21592;&#26356;&#21152;&#20851;&#27880;&#38646;&#26679;&#26412;&#23545;&#35805;&#25628;&#32034;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#38646;&#26679;&#26412;&#26041;&#27861;&#23384;&#22312;&#19977;&#20010;&#20027;&#35201;&#38480;&#21046;&#65306;&#23427;&#20204;&#19981;&#36866;&#29992;&#20110;&#25152;&#26377;&#30340;&#26816;&#32034;&#22120;&#65292;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#32570;&#20047;&#36275;&#22815;&#30340;&#35299;&#37322;&#24615;&#65292;&#24182;&#19988;&#20182;&#20204;&#26080;&#27861;&#35299;&#20915;&#22240;&#30465;&#30053;&#32780;&#23548;&#33268;&#30340;&#24120;&#35265;&#23545;&#35805;&#27495;&#20041;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38646;&#26679;&#26412;&#26597;&#35810;&#37325;&#26500;&#65288;ZeQR&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#26681;&#25454;&#20808;&#21069;&#30340;&#23545;&#35805;&#19978;&#19979;&#25991;&#37325;&#26500;&#26597;&#35810;&#65292;&#32780;&#26080;&#38656;&#23545;&#35805;&#25628;&#32034;&#25968;&#25454;&#30340;&#30417;&#30563;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21033;&#29992;&#20102;&#35774;&#35745;&#29992;&#20110;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#20219;&#21153;&#30340;&#35821;&#35328;&#27169;&#22411;&#26469;&#26126;&#30830;&#35299;&#20915;&#20004;&#20010;&#24120;&#35265;&#30340;&#27495;&#20041;&#65306;&#21327;&#35843;&#21644;&#30465;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the popularity of voice assistants continues to surge, conversational search has gained increased attention in Information Retrieval. However, data sparsity issues in conversational search significantly hinder the progress of supervised conversational search methods. Consequently, researchers are focusing more on zero-shot conversational search approaches. Nevertheless, existing zero-shot methods face three primary limitations: they are not universally applicable to all retrievers, their effectiveness lacks sufficient explainability, and they struggle to resolve common conversational ambiguities caused by omission. To address these limitations, we introduce a novel Zero-shot Query Reformulation (ZeQR) framework that reformulates queries based on previous dialogue contexts without requiring supervision from conversational search data. Specifically, our framework utilizes language models designed for machine reading comprehension tasks to explicitly resolve two common ambiguities: cor
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;ESMC&#27169;&#22411;&#65292;&#36890;&#36807;&#21442;&#25968;&#32422;&#26463;&#26469;&#35299;&#20915;&#22823;&#35268;&#27169;&#22312;&#32447;&#25512;&#33616;&#31995;&#32479;&#20013;&#28857;&#20987;&#21518;&#36716;&#21270;&#29575;&#20272;&#35745;&#20013;&#30340;&#26679;&#26412;&#36873;&#25321;&#20559;&#24046;&#21644;&#25968;&#25454;&#31232;&#30095;&#24615;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#25193;&#23637;&#20915;&#31574;&#36335;&#24452;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#29992;&#25143;&#30340;&#20915;&#31574;&#24847;&#22270;&#65292;&#24182;&#25552;&#39640;&#25512;&#33616;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.09193</link><description>&lt;p&gt;
ESMC:&#25972;&#20010;&#31354;&#38388;&#22810;&#20219;&#21153;&#27169;&#22411;&#36890;&#36807;&#21442;&#25968;&#32422;&#26463;&#29992;&#20110;&#28857;&#20987;&#21518;&#36716;&#21270;&#29575;
&lt;/p&gt;
&lt;p&gt;
ESMC: Entire Space Multi-Task Model for Post-Click Conversion Rate via Parameter Constraint. (arXiv:2307.09193v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09193
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;ESMC&#27169;&#22411;&#65292;&#36890;&#36807;&#21442;&#25968;&#32422;&#26463;&#26469;&#35299;&#20915;&#22823;&#35268;&#27169;&#22312;&#32447;&#25512;&#33616;&#31995;&#32479;&#20013;&#28857;&#20987;&#21518;&#36716;&#21270;&#29575;&#20272;&#35745;&#20013;&#30340;&#26679;&#26412;&#36873;&#25321;&#20559;&#24046;&#21644;&#25968;&#25454;&#31232;&#30095;&#24615;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#25193;&#23637;&#20915;&#31574;&#36335;&#24452;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#29992;&#25143;&#30340;&#20915;&#31574;&#24847;&#22270;&#65292;&#24182;&#25552;&#39640;&#25512;&#33616;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#22312;&#32447;&#25512;&#33616;&#31995;&#32479;&#22312;&#20114;&#32852;&#32593;&#19978;&#24191;&#27867;&#20351;&#29992;&#65292;&#36127;&#36131;&#28857;&#20987;&#29575;&#65288;CTR&#65289;&#21644;&#28857;&#20987;&#21518;&#36716;&#21270;&#29575;&#65288;CVR&#65289;&#30340;&#20272;&#35745;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;CVR&#20272;&#35745;&#22120;&#23384;&#22312;&#30528;&#26679;&#26412;&#36873;&#25321;&#20559;&#24046;&#21644;&#25968;&#25454;&#31232;&#30095;&#24615;&#38382;&#39064;&#12290;&#36890;&#36807;&#36861;&#36394;&#8220;&#26333;&#20809;_&#28857;&#20987;_&#36141;&#20080;&#8221;&#36825;&#20010;&#20915;&#31574;&#36335;&#24452;&#65292;&#25552;&#20986;&#20102;&#25972;&#20010;&#31354;&#38388;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#19968;&#20123;&#30740;&#31350;&#32773;&#35266;&#23519;&#21040;&#22312;&#28857;&#20987;&#21644;&#36141;&#20080;&#20043;&#38388;&#23384;&#22312;&#36141;&#20080;&#30456;&#20851;&#34892;&#20026;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#20102;&#35299;&#29992;&#25143;&#30340;&#20915;&#31574;&#24847;&#22270;&#24182;&#25552;&#39640;&#25512;&#33616;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#20915;&#31574;&#36335;&#24452;&#24050;&#25193;&#23637;&#20026;&#8220;&#26333;&#20809;_&#28857;&#20987;_&#24215;&#20869;&#21160;&#20316;_&#36141;&#20080;&#8221;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#26465;&#20214;&#27010;&#29575;&#26041;&#27861;&#24314;&#27169;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#26465;&#20214;&#27010;&#29575;&#30340;&#38142;&#24335;&#27861;&#21017;&#24182;&#19981;&#24635;&#26159;&#25104;&#31435;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;&#27010;&#29575;&#31354;&#38388;&#28151;&#28102;&#65288;PSC&#65289;&#38382;&#39064;&#65292;&#24182;&#25512;&#23548;&#20102;&#22320;&#38754;&#23454;&#20917;&#19982;&#20272;&#35745;&#25968;&#23398;&#38388;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale online recommender system spreads all over the Internet being in charge of two basic tasks: Click-Through Rate (CTR) and Post-Click Conversion Rate (CVR) estimations. However, traditional CVR estimators suffer from well-known Sample Selection Bias and Data Sparsity issues. Entire space models were proposed to address the two issues via tracing the decision-making path of "exposure_click_purchase". Further, some researchers observed that there are purchase-related behaviors between click and purchase, which can better draw the user's decision-making intention and improve the recommendation performance. Thus, the decision-making path has been extended to "exposure_click_in-shop action_purchase" and can be modeled with conditional probability approach. Nevertheless, we observe that the chain rule of conditional probability does not always hold. We report Probability Space Confusion (PSC) issue and give a derivation of difference between ground-truth and estimation mathematical
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#35757;&#32451;&#30340;&#30456;&#20851;&#27169;&#22411;&#30340;&#31227;&#21160;&#21151;&#33021;&#26816;&#32034;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#21487;&#20197;&#25509;&#21463;&#30452;&#35266;&#21644;&#19978;&#19979;&#25991;&#30340;&#25628;&#32034;&#26597;&#35810;&#65292;&#24182;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#36827;&#34892;&#27169;&#22411;&#21387;&#32553;&#65292;&#20197;&#22312;&#35774;&#22791;&#19978;&#39640;&#25928;&#36816;&#34892;&#65292;&#24182;&#36827;&#34892;&#20102;&#19982;&#24403;&#21069;&#37096;&#32626;&#30340;&#25628;&#32034;&#22522;&#20934;&#30340;&#27604;&#36739;&#23454;&#39564;&#12290;</title><link>http://arxiv.org/abs/2307.09177</link><description>&lt;p&gt;
&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#35757;&#32451;&#30340;&#30456;&#20851;&#27169;&#22411;&#23454;&#29616;&#23545;&#26234;&#33021;&#25163;&#26426;&#35774;&#32622;&#30340;&#30452;&#35266;&#35775;&#38382;
&lt;/p&gt;
&lt;p&gt;
Intuitive Access to Smartphone Settings Using Relevance Model Trained by Contrastive Learning. (arXiv:2307.09177v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09177
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#35757;&#32451;&#30340;&#30456;&#20851;&#27169;&#22411;&#30340;&#31227;&#21160;&#21151;&#33021;&#26816;&#32034;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#21487;&#20197;&#25509;&#21463;&#30452;&#35266;&#21644;&#19978;&#19979;&#25991;&#30340;&#25628;&#32034;&#26597;&#35810;&#65292;&#24182;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#36827;&#34892;&#27169;&#22411;&#21387;&#32553;&#65292;&#20197;&#22312;&#35774;&#22791;&#19978;&#39640;&#25928;&#36816;&#34892;&#65292;&#24182;&#36827;&#34892;&#20102;&#19982;&#24403;&#21069;&#37096;&#32626;&#30340;&#25628;&#32034;&#22522;&#20934;&#30340;&#27604;&#36739;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26234;&#33021;&#25163;&#26426;&#22686;&#21152;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#26032;&#21151;&#33021;&#65292;&#29992;&#25143;&#36234;&#26469;&#36234;&#38590;&#20197;&#25214;&#21040;&#23427;&#20204;&#65292;&#22240;&#20026;&#36825;&#20123;&#21151;&#33021;&#30340;&#21517;&#31216;&#36890;&#24120;&#24456;&#30701;&#65292;&#35760;&#19981;&#20303;&#22826;&#22810;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#29992;&#25143;&#21487;&#33021;&#24076;&#26395;&#25552;&#20986;&#25551;&#36848;&#20182;&#20204;&#35201;&#23547;&#25214;&#30340;&#21151;&#33021;&#30340;&#19978;&#19979;&#25991;&#26597;&#35810;&#65292;&#20294;&#26631;&#20934;&#30340;&#22522;&#20110;&#35789;&#39057;&#30340;&#25628;&#32034;&#26080;&#27861;&#22788;&#29702;&#23427;&#20204;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31227;&#21160;&#21151;&#33021;&#26816;&#32034;&#31995;&#32479;&#65292;&#21487;&#20197;&#25509;&#21463;&#30452;&#35266;&#21644;&#19978;&#19979;&#25991;&#30340;&#25628;&#32034;&#26597;&#35810;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#20174;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#35757;&#32451;&#20102;&#19968;&#20010;&#30456;&#20851;&#27169;&#22411;&#65292;&#20197;&#24863;&#30693;&#26597;&#35810;&#23884;&#20837;&#21644;&#32034;&#24341;&#30340;&#31227;&#21160;&#21151;&#33021;&#20043;&#38388;&#30340;&#19978;&#19979;&#25991;&#30456;&#20851;&#24615;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#22312;&#35774;&#22791;&#19978;&#20351;&#29992;&#26368;&#20302;&#36164;&#28304;&#39640;&#25928;&#36816;&#34892;&#65292;&#25105;&#20204;&#24212;&#29992;&#20102;&#30693;&#35782;&#33976;&#39311;&#26469;&#21387;&#32553;&#27169;&#22411;&#32780;&#19981;&#38477;&#20302;&#22826;&#22810;&#24615;&#33021;&#12290;&#20026;&#20102;&#39564;&#35777;&#25105;&#20204;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#27979;&#35797;&#26597;&#35810;&#65292;&#24182;&#19982;&#24403;&#21069;&#37096;&#32626;&#30340;&#25628;&#32034;&#22522;&#20934;&#36827;&#34892;&#20102;&#27604;&#36739;&#23454;&#39564;&#12290;&#32467;&#26524;&#26174;&#31034;
&lt;/p&gt;
&lt;p&gt;
The more new features that are being added to smartphones, the harder it becomes for users to find them. This is because the feature names are usually short, and there are just too many to remember. In such a case, the users may want to ask contextual queries that describe the features they are looking for, but the standard term frequency-based search cannot process them. This paper presents a novel retrieval system for mobile features that accepts intuitive and contextual search queries. We trained a relevance model via contrastive learning from a pre-trained language model to perceive the contextual relevance between query embeddings and indexed mobile features. Also, to make it run efficiently on-device using minimal resources, we applied knowledge distillation to compress the model without degrading much performance. To verify the feasibility of our method, we collected test queries and conducted comparative experiments with the currently deployed search baselines. The results show
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#29992;&#20110;&#35770;&#25454;&#22270;&#20687;&#26816;&#32034;&#30340;&#22270;&#20687;&#29983;&#25104;&#12289;&#31435;&#22330;&#26816;&#27979;&#21644;&#29305;&#24449;&#21305;&#37197;&#30340;&#19981;&#21516;&#27969;&#31243;&#65292;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#27969;&#31243;&#19982;&#22522;&#20934;&#30456;&#20284;&#12290;</title><link>http://arxiv.org/abs/2307.09172</link><description>&lt;p&gt;
2023&#24180;&#65292;&#35753;&#183;&#21525;&#20811;&#183;&#30382;&#21345;&#23572;&#22312;Touch'e&#30340;&#27604;&#36739;&#65306;&#29992;&#20110;&#35770;&#25454;&#22270;&#20687;&#26816;&#32034;&#30340;&#22270;&#20687;&#29983;&#25104;&#12289;&#31435;&#22330;&#26816;&#27979;&#21644;&#29305;&#24449;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Jean-Luc Picard at Touch\'e 2023: Comparing Image Generation, Stance Detection and Feature Matching for Image Retrieval for Arguments. (arXiv:2307.09172v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09172
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#29992;&#20110;&#35770;&#25454;&#22270;&#20687;&#26816;&#32034;&#30340;&#22270;&#20687;&#29983;&#25104;&#12289;&#31435;&#22330;&#26816;&#27979;&#21644;&#29305;&#24449;&#21305;&#37197;&#30340;&#19981;&#21516;&#27969;&#31243;&#65292;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#27969;&#31243;&#19982;&#22522;&#20934;&#30456;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21442;&#19982;&#8220;&#35770;&#25454;&#22270;&#20687;&#26816;&#32034;&#8221;&#30340;&#20849;&#20139;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#21253;&#21547;&#22270;&#20687;&#29983;&#25104;&#12289;&#31435;&#22330;&#26816;&#27979;&#12289;&#39044;&#36873;&#21644;&#29305;&#24449;&#21305;&#37197;&#30340;&#19981;&#21516;&#27969;&#31243;&#29992;&#20110;&#22270;&#20687;&#26816;&#32034;&#12290;&#25105;&#20204;&#25552;&#20132;&#20102;&#22235;&#20010;&#19981;&#21516;&#30340;&#36816;&#34892;&#32467;&#26524;&#65292;&#24182;&#23558;&#20854;&#19982;&#32473;&#23450;&#30340;&#22522;&#20934;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#27969;&#31243;&#19982;&#22522;&#20934;&#30456;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;
Participating in the shared task "Image Retrieval for arguments", we used different pipelines for image retrieval containing Image Generation, Stance Detection, Preselection and Feature Matching. We submitted four different runs with different pipeline layout and compare them to given baseline. Our pipelines perform similarly to the baseline.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22810;&#20219;&#21153;&#26694;&#26550;&#65292;&#21033;&#29992;&#29992;&#25143;&#34892;&#20026;&#30340;&#39034;&#24207;&#26469;&#39044;&#27979;&#29992;&#25143;&#28857;&#20987;&#21518;&#30340;&#36716;&#21270;&#12290;&#36890;&#36807;&#32858;&#21512;&#31639;&#23376;&#21644;&#21487;&#24494;&#25490;&#24207;&#26041;&#27861;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#27169;&#22411;&#26469;&#35299;&#20915;&#28857;&#20987;&#25968;&#25454;&#20013;&#30340;&#36873;&#25321;&#20559;&#24046;&#21644;&#25968;&#25454;&#31232;&#30095;&#24615;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#27169;&#22411;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.09089</link><description>&lt;p&gt;
&#36890;&#36807;&#21487;&#24494;&#30340;&#25490;&#24207;&#27169;&#25311;&#29992;&#25143;&#34892;&#20026;&#39034;&#24207;&#65306;&#19968;&#20010;&#29992;&#20110;&#39044;&#27979;&#29992;&#25143;&#28857;&#20987;&#21518;&#36716;&#21270;&#30340;&#22810;&#20219;&#21153;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Modeling Orders of User Behaviors via Differentiable Sorting: A Multi-task Framework to Predicting User Post-click Conversion. (arXiv:2307.09089v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09089
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22810;&#20219;&#21153;&#26694;&#26550;&#65292;&#21033;&#29992;&#29992;&#25143;&#34892;&#20026;&#30340;&#39034;&#24207;&#26469;&#39044;&#27979;&#29992;&#25143;&#28857;&#20987;&#21518;&#30340;&#36716;&#21270;&#12290;&#36890;&#36807;&#32858;&#21512;&#31639;&#23376;&#21644;&#21487;&#24494;&#25490;&#24207;&#26041;&#27861;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#27169;&#22411;&#26469;&#35299;&#20915;&#28857;&#20987;&#25968;&#25454;&#20013;&#30340;&#36873;&#25321;&#20559;&#24046;&#21644;&#25968;&#25454;&#31232;&#30095;&#24615;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#27169;&#22411;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#25143;&#28857;&#20987;&#21518;&#30340;&#36716;&#21270;&#39044;&#27979;&#23545;&#30740;&#31350;&#20154;&#21592;&#21644;&#24320;&#21457;&#32773;&#26469;&#35828;&#38750;&#24120;&#37325;&#35201;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#37319;&#29992;&#22810;&#20219;&#21153;&#23398;&#20064;&#26469;&#35299;&#20915;&#28857;&#20987;&#25968;&#25454;&#20013;&#30340;&#36873;&#25321;&#20559;&#24046;&#21644;&#25968;&#25454;&#31232;&#30095;&#24615;&#38382;&#39064;&#65292;&#20294;&#26159;&#20043;&#21069;&#30340;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#28857;&#23545;&#28857;&#23398;&#20064;&#19978;&#65292;&#26631;&#31614;&#30340;&#39034;&#24207;&#65288;&#21363;&#28857;&#20987;&#21644;&#28857;&#20987;&#21518;&#65289;&#24182;&#27809;&#26377;&#24471;&#21040;&#24456;&#22909;&#30340;&#25506;&#32034;&#65292;&#36825;&#33258;&#28982;&#20135;&#29983;&#20102;&#19968;&#20010;&#21015;&#34920;&#23398;&#20064;&#38382;&#39064;&#12290;&#21463;&#21040;&#26368;&#36817;&#22312;&#21487;&#24494;&#25490;&#24207;&#26041;&#38754;&#30340;&#36827;&#23637;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22810;&#20219;&#21153;&#26694;&#26550;&#65292;&#21033;&#29992;&#29992;&#25143;&#34892;&#20026;&#30340;&#39034;&#24207;&#26469;&#39044;&#27979;&#29992;&#25143;&#28857;&#20987;&#21518;&#30340;&#36716;&#21270;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#32858;&#21512;&#31639;&#23376;&#26469;&#23558;&#19981;&#21516;&#20219;&#21153;&#30340;&#39044;&#27979;&#36755;&#20986;&#21512;&#24182;&#20026;&#19968;&#20010;&#32479;&#19968;&#30340;&#20998;&#25968;&#65292;&#28982;&#21518;&#25105;&#20204;&#20351;&#29992;&#35745;&#31639;&#24471;&#21040;&#30340;&#20998;&#25968;&#36890;&#36807;&#21487;&#24494;&#25490;&#24207;&#26469;&#24314;&#27169;&#26631;&#31614;&#30340;&#20851;&#31995;&#12290;&#22312;&#20844;&#20849;&#21644;&#24037;&#19994;&#25968;&#25454;&#38598;&#19978;&#30340;&#24191;&#27867;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#30456;&#23545;&#31454;&#20105;&#23545;&#25163;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
User post-click conversion prediction is of high interest to researchers and developers. Recent studies employ multi-task learning to tackle the selection bias and data sparsity problem, two severe challenges in post-click behavior prediction, by incorporating click data. However, prior works mainly focused on pointwise learning and the orders of labels (i.e., click and post-click) are not well explored, which naturally poses a listwise learning problem. Inspired by recent advances on differentiable sorting, in this paper, we propose a novel multi-task framework that leverages orders of user behaviors to predict user post-click conversion in an end-to-end approach. Specifically, we define an aggregation operator to combine predicted outputs of different tasks to a unified score, then we use the computed scores to model the label relations via differentiable sorting. Extensive experiments on public and industrial datasets show the superiority of our proposed model against competitive ba
&lt;/p&gt;</description></item><item><title>GraphCL-DTA&#26159;&#19968;&#31181;&#20351;&#29992;&#20998;&#23376;&#35821;&#20041;&#36827;&#34892;&#33647;&#29289;&#38774;&#26631;&#32467;&#21512;&#20146;&#21644;&#21147;&#39044;&#27979;&#30340;&#22270;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#12290;&#36890;&#36807;&#35774;&#35745;&#22270;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#23398;&#20064;&#21040;&#26356;&#26412;&#36136;&#21644;&#26377;&#25928;&#30340;&#33647;&#29289;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2307.08989</link><description>&lt;p&gt;
GraphCL-DTA: &#19968;&#31181;&#22522;&#20110;&#20998;&#23376;&#35821;&#20041;&#30340;&#22270;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#33647;&#29289;&#38774;&#26631;&#32467;&#21512;&#20146;&#21644;&#21147;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
GraphCL-DTA: a graph contrastive learning with molecular semantics for drug-target binding affinity prediction. (arXiv:2307.08989v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08989
&lt;/p&gt;
&lt;p&gt;
GraphCL-DTA&#26159;&#19968;&#31181;&#20351;&#29992;&#20998;&#23376;&#35821;&#20041;&#36827;&#34892;&#33647;&#29289;&#38774;&#26631;&#32467;&#21512;&#20146;&#21644;&#21147;&#39044;&#27979;&#30340;&#22270;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#12290;&#36890;&#36807;&#35774;&#35745;&#22270;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#23398;&#20064;&#21040;&#26356;&#26412;&#36136;&#21644;&#26377;&#25928;&#30340;&#33647;&#29289;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33647;&#29289;&#38774;&#26631;&#32467;&#21512;&#20146;&#21644;&#21147;&#39044;&#27979;&#22312;&#33647;&#29289;&#30740;&#21457;&#30340;&#26089;&#26399;&#38454;&#27573;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#21487;&#20197;&#25512;&#26029;&#26032;&#33647;&#21644;&#26032;&#38774;&#26631;&#20043;&#38388;&#30456;&#20114;&#20316;&#29992;&#30340;&#24378;&#24230;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#35745;&#31639;&#27169;&#22411;&#22312;&#20197;&#19979;&#26041;&#38754;&#23384;&#22312;&#38480;&#21046;&#65306;&#33647;&#29289;&#34920;&#31034;&#30340;&#23398;&#20064;&#20165;&#20381;&#36182;&#20110;&#26377;&#30417;&#30563;&#25968;&#25454;&#65292;&#27809;&#26377;&#32771;&#34385;&#20998;&#23376;&#22270;&#20013;&#25152;&#21253;&#21547;&#30340;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#30740;&#31350;&#20542;&#21521;&#20110;&#35774;&#35745;&#22797;&#26434;&#30340;&#34920;&#31034;&#23398;&#20064;&#27169;&#22359;&#65292;&#24573;&#30053;&#20102;&#29992;&#20110;&#34913;&#37327;&#34920;&#31034;&#36136;&#37327;&#30340;&#19968;&#33268;&#24615;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GraphCL-DTA&#30340;&#22270;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#33647;&#29289;&#38774;&#26631;&#32467;&#21512;&#20146;&#21644;&#21147;&#39044;&#27979;&#12290;&#22312;GraphCL-DTA&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22270;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#33647;&#29289;&#34920;&#31034;&#65292;&#20197;&#20445;&#30041;&#20998;&#23376;&#22270;&#30340;&#35821;&#20041;&#12290;&#36890;&#36807;&#36825;&#20010;&#22270;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#24471;&#21040;&#26356;&#26412;&#36136;&#21644;&#26377;&#25928;&#30340;&#33647;&#29289;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Drug-target binding affinity prediction plays an important role in the early stages of drug discovery, which can infer the strength of interactions between new drugs and new targets. However, the performance of previous computational models is limited by the following drawbacks. The learning of drug representation relies only on supervised data, without taking into account the information contained in the molecular graph itself. Moreover, most previous studies tended to design complicated representation learning module, while uniformity, which is used to measure representation quality, is ignored. In this study, we propose GraphCL-DTA, a graph contrastive learning with molecular semantics for drug-target binding affinity prediction. In GraphCL-DTA, we design a graph contrastive learning framework for molecular graphs to learn drug representations, so that the semantics of molecular graphs are preserved. Through this graph contrastive framework, a more essential and effective drug repre
&lt;/p&gt;</description></item><item><title>&#38160;&#24230;&#24863;&#30693;&#30340;&#22270;&#21327;&#21516;&#36807;&#28388;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;gSAM&#30340;&#35757;&#32451;&#26041;&#26696;&#65292;&#36890;&#36807;&#27491;&#21017;&#21270;&#26435;&#37325;&#25439;&#22833;&#20989;&#25968;&#30340;&#24179;&#22374;&#31243;&#24230;&#65292;&#36873;&#25321;&#24179;&#22374;&#26368;&#23567;&#20540;&#20197;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#65292;&#22312;&#21327;&#21516;&#36807;&#28388;&#20013;&#21462;&#24471;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.08910</link><description>&lt;p&gt;
&#38160;&#24230;&#24863;&#30693;&#30340;&#22270;&#21327;&#21516;&#36807;&#28388;
&lt;/p&gt;
&lt;p&gt;
Sharpness-Aware Graph Collaborative Filtering. (arXiv:2307.08910v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08910
&lt;/p&gt;
&lt;p&gt;
&#38160;&#24230;&#24863;&#30693;&#30340;&#22270;&#21327;&#21516;&#36807;&#28388;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;gSAM&#30340;&#35757;&#32451;&#26041;&#26696;&#65292;&#36890;&#36807;&#27491;&#21017;&#21270;&#26435;&#37325;&#25439;&#22833;&#20989;&#25968;&#30340;&#24179;&#22374;&#31243;&#24230;&#65292;&#36873;&#25321;&#24179;&#22374;&#26368;&#23567;&#20540;&#20197;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#65292;&#22312;&#21327;&#21516;&#36807;&#28388;&#20013;&#21462;&#24471;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#22312;&#21327;&#21516;&#36807;&#28388;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#24403;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#30340;&#20998;&#24067;&#19981;&#22826;&#19968;&#33268;&#26102;&#65292;GNNs&#24448;&#24448;&#20250;&#20135;&#29983;&#36739;&#24046;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#35757;&#32451;GNNs&#38656;&#35201;&#20248;&#21270;&#38750;&#20984;&#31070;&#32463;&#32593;&#32476;&#65292;&#26377;&#22823;&#37327;&#23616;&#37096;&#26368;&#23567;&#20540;&#21644;&#20840;&#23616;&#26368;&#23567;&#20540;&#65292;&#36825;&#20123;&#22312;&#27979;&#35797;&#26102;&#21487;&#33021;&#24615;&#33021;&#24046;&#24322;&#24456;&#22823;&#12290;&#22240;&#27492;&#65292;&#36873;&#25321;&#26368;&#23567;&#20540;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#35757;&#32451;&#26041;&#26696;&#65292;&#31216;&#20026;{gSAM}&#65292;&#26681;&#25454;&#8220;&#24179;&#22374;&#8221;&#26368;&#23567;&#20540;&#27604;&#8220;&#38160;&#21033;&#8221;&#26368;&#23567;&#20540;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#30340;&#21407;&#21017;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;gSAM&#36890;&#36807;&#24418;&#25104;&#21452;&#23618;&#20248;&#21270;&#26469;&#27491;&#21017;&#21270;&#26435;&#37325;&#25439;&#22833;&#20989;&#25968;&#30340;&#24179;&#22374;&#31243;&#24230;&#65306;&#22806;&#37096;&#38382;&#39064;&#36827;&#34892;&#26631;&#20934;&#27169;&#22411;&#35757;&#32451;&#65292;&#32780;&#20869;&#37096;&#38382;&#39064;&#21017;&#24110;&#21161;&#27169;&#22411;&#36339;&#20986;&#38160;&#21033;&#30340;&#26368;&#23567;&#20540;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20102;&#25105;&#20204;&#30340;gSAM&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have achieved impressive performance in collaborative filtering. However, GNNs tend to yield inferior performance when the distributions of training and test data are not aligned well. Also, training GNNs requires optimizing non-convex neural networks with an abundance of local and global minima, which may differ widely in their performance at test time. Thus, it is essential to choose the minima carefully. Here we propose an effective training schema, called {gSAM}, under the principle that the \textit{flatter} minima has a better generalization ability than the \textit{sharper} ones. To achieve this goal, gSAM regularizes the flatness of the weight loss landscape by forming a bi-level optimization: the outer problem conducts the standard model training while the inner problem helps the model jump out of the sharp minima. Experimental results show the superiority of our gSAM.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#31227;&#21160;&#19968;&#33268;&#24615;&#32422;&#26463;&#35299;&#20915;&#20102;&#30697;&#38453;/&#24352;&#37327;&#23436;&#25104;&#38382;&#39064;&#65292;&#24182;&#28385;&#36275;&#21487;&#25509;&#21463;&#24615;&#20934;&#21017;&#21644;&#20844;&#24179;&#24615;&#23450;&#20041;&#65292;&#21516;&#26102;&#20855;&#26377;&#40065;&#26834;&#24615;&#21644;&#21487;&#35777;&#26126;&#30340;&#24615;&#33021;&#29305;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.08857</link><description>&lt;p&gt;
&#19968;&#31181;&#36866;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#30340;&#21487;&#25509;&#21463;&#30340;&#31227;&#21160;&#19968;&#33268;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Admissible Shift-Consistent Method for Recommender Systems. (arXiv:2307.08857v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08857
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#31227;&#21160;&#19968;&#33268;&#24615;&#32422;&#26463;&#35299;&#20915;&#20102;&#30697;&#38453;/&#24352;&#37327;&#23436;&#25104;&#38382;&#39064;&#65292;&#24182;&#28385;&#36275;&#21487;&#25509;&#21463;&#24615;&#20934;&#21017;&#21644;&#20844;&#24179;&#24615;&#23450;&#20041;&#65292;&#21516;&#26102;&#20855;&#26377;&#40065;&#26834;&#24615;&#21644;&#21487;&#35777;&#26126;&#30340;&#24615;&#33021;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32422;&#26463;&#65292;&#31216;&#20026;&#31227;&#21160;&#19968;&#33268;&#24615;&#65292;&#29992;&#20110;&#35299;&#20915;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#30697;&#38453;/&#24352;&#37327;&#23436;&#25104;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#35777;&#26126;&#20445;&#35777;&#19968;&#20123;&#20851;&#38190;&#30340;&#25968;&#23398;&#23646;&#24615;&#65306;&#65288;1&#65289;&#28385;&#36275;&#26368;&#36817;&#24314;&#31435;&#30340;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#30340;&#21487;&#25509;&#21463;&#24615;&#20934;&#21017;&#65307;&#65288;2&#65289;&#28385;&#36275;&#19968;&#20010;&#23450;&#20041;&#20844;&#24179;&#24615;&#30340;&#26465;&#20214;&#65292;&#28040;&#38500;&#20102;&#29992;&#25143;&#23545;&#31995;&#32479;&#25512;&#33616;&#30340;&#24694;&#24847;&#24433;&#21709;&#30340;&#28508;&#22312;&#26426;&#20250;&#65307;&#65288;3&#65289;&#36890;&#36807;&#21033;&#29992;&#21487;&#35777;&#26126;&#30340;&#32570;&#22833;&#20540;&#25554;&#34917;&#30340;&#21807;&#19968;&#24615;&#65292;&#25552;&#20379;&#20102;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#35813;&#26041;&#27861;&#30340;&#20005;&#26684;&#25968;&#23398;&#25551;&#36848;&#65292;&#21253;&#25324;&#20174;&#30697;&#38453;&#21040;&#24352;&#37327;&#24418;&#24335;&#30340;&#27867;&#21270;&#65292;&#20197;&#20415;&#34920;&#31034;&#21644;&#21033;&#29992;&#29992;&#25143;&#21644;&#20135;&#21697;&#23646;&#24615;&#38598;&#20043;&#38388;&#30340;&#22797;&#26434;&#32467;&#26500;&#20851;&#31995;&#12290;&#25105;&#20204;&#35748;&#20026;&#25105;&#20204;&#30340;&#20998;&#26512;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#23450;&#20041;&#28508;&#22312;&#31354;&#38388;&#25237;&#24433;&#65292;&#20197;&#20415;&#20026;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#24314;&#31435;&#21487;&#35777;&#26126;&#30340;&#24615;&#33021;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a new constraint, called shift-consistency, for solving matrix/tensor completion problems in the context of recommender systems. Our method provably guarantees several key mathematical properties: (1) satisfies a recently established admissibility criterion for recommender systems; (2) satisfies a definition of fairness that eliminates a specific class of potential opportunities for users to maliciously influence system recommendations; and (3) offers robustness by exploiting provable uniqueness of missing-value imputation. We provide a rigorous mathematical description of the method, including its generalization from matrix to tensor form to permit representation and exploitation of complex structural relationships among sets of user and product attributes. We argue that our analysis suggests a structured means for defining latent-space projections that can permit provable performance properties to be established for machine learning methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#35805;&#24335;&#27573;&#33853;&#26816;&#32034;&#20013;&#28151;&#21512;&#20027;&#21160;&#26597;&#35810;&#37325;&#26500;&#30340;&#25506;&#32034;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#28151;&#21512;&#20027;&#21160;&#26597;&#35810;&#37325;&#26500;&#27169;&#22359;&#65292;&#35813;&#27169;&#22359;&#33021;&#22815;&#22522;&#20110;&#29992;&#25143;&#19982;&#31995;&#32479;&#20043;&#38388;&#30340;&#28151;&#21512;&#20027;&#21160;&#20132;&#20114;&#23545;&#21407;&#22987;&#26597;&#35810;&#36827;&#34892;&#37325;&#26500;&#65292;&#20197;&#25552;&#39640;&#26816;&#32034;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.08803</link><description>&lt;p&gt;
&#12298;&#28151;&#21512;&#20027;&#21160;&#26597;&#35810;&#37325;&#26500;&#22312;&#23545;&#35805;&#24335;&#27573;&#33853;&#26816;&#32034;&#20013;&#30340;&#25506;&#32034;&#30740;&#31350;&#12299;&#30340;&#30740;&#31350;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
An Exploration Study of Mixed-initiative Query Reformulation in Conversational Passage Retrieval. (arXiv:2307.08803v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08803
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#35805;&#24335;&#27573;&#33853;&#26816;&#32034;&#20013;&#28151;&#21512;&#20027;&#21160;&#26597;&#35810;&#37325;&#26500;&#30340;&#25506;&#32034;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#28151;&#21512;&#20027;&#21160;&#26597;&#35810;&#37325;&#26500;&#27169;&#22359;&#65292;&#35813;&#27169;&#22359;&#33021;&#22815;&#22522;&#20110;&#29992;&#25143;&#19982;&#31995;&#32479;&#20043;&#38388;&#30340;&#28151;&#21512;&#20027;&#21160;&#20132;&#20114;&#23545;&#21407;&#22987;&#26597;&#35810;&#36827;&#34892;&#37325;&#26500;&#65292;&#20197;&#25552;&#39640;&#26816;&#32034;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25253;&#21578;&#20102;&#25105;&#20204;&#22312;TREC Conversational Assistance Track (CAsT) 2022&#20013;&#30340;&#26041;&#27861;&#21644;&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#22797;&#29616;&#22810;&#38454;&#27573;&#30340;&#26816;&#32034;&#31649;&#32447;&#65292;&#24182;&#25506;&#32034;&#22312;&#23545;&#35805;&#24335;&#27573;&#33853;&#26816;&#32034;&#22330;&#26223;&#20013;&#28041;&#21450;&#28151;&#21512;&#20027;&#21160;&#20132;&#20114;&#30340;&#28508;&#22312;&#22909;&#22788;&#20043;&#19968;&#65306;&#23545;&#21407;&#22987;&#26597;&#35810;&#36827;&#34892;&#37325;&#26500;&#12290;&#22312;&#22810;&#38454;&#27573;&#26816;&#32034;&#31649;&#32447;&#30340;&#31532;&#19968;&#20010;&#25490;&#21517;&#38454;&#27573;&#20043;&#21069;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#28151;&#21512;&#20027;&#21160;&#26597;&#35810;&#37325;&#26500;&#27169;&#22359;&#65292;&#23427;&#36890;&#36807;&#29992;&#25143;&#19982;&#31995;&#32479;&#20043;&#38388;&#30340;&#28151;&#21512;&#20027;&#21160;&#20132;&#20114;&#23454;&#29616;&#26597;&#35810;&#37325;&#26500;&#65292;&#20316;&#20026;&#31070;&#32463;&#37325;&#26500;&#26041;&#27861;&#30340;&#26367;&#20195;&#21697;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31639;&#27861;&#26469;&#29983;&#25104;&#19982;&#21407;&#22987;&#26597;&#35810;&#20013;&#30340;&#27495;&#20041;&#30456;&#20851;&#30340;&#36866;&#24403;&#38382;&#39064;&#65292;&#20197;&#21450;&#21478;&#19968;&#20010;&#31639;&#27861;&#26469;&#35299;&#26512;&#29992;&#25143;&#30340;&#21453;&#39304;&#24182;&#23558;&#20854;&#34701;&#20837;&#21040;&#21407;&#22987;&#26597;&#35810;&#20013;&#20197;&#36827;&#34892;&#26597;&#35810;&#37325;&#26500;&#12290;&#23545;&#20110;&#25105;&#20204;&#30340;&#22810;&#38454;&#27573;&#31649;&#32447;&#30340;&#31532;&#19968;&#20010;&#25490;&#21517;&#38454;&#27573;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#20010;&#31232;&#30095;&#25490;&#21517;&#20989;&#25968;&#65306;BM25&#21644;&#19968;&#20010;&#23494;&#38598;&#26816;&#32034;&#26041;&#27861;&#65306;TCT-ColBERT&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we report our methods and experiments for the TREC Conversational Assistance Track (CAsT) 2022. In this work, we aim to reproduce multi-stage retrieval pipelines and explore one of the potential benefits of involving mixed-initiative interaction in conversational passage retrieval scenarios: reformulating raw queries. Before the first ranking stage of a multi-stage retrieval pipeline, we propose a mixed-initiative query reformulation module, which achieves query reformulation based on the mixed-initiative interaction between the users and the system, as the replacement for the neural reformulation method. Specifically, we design an algorithm to generate appropriate questions related to the ambiguities in raw queries, and another algorithm to reformulate raw queries by parsing users' feedback and incorporating it into the raw query. For the first ranking stage of our multi-stage pipelines, we adopt a sparse ranking function: BM25, and a dense retrieval method: TCT-ColBERT
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#22312;&#40657;&#30418;&#31995;&#32479;&#20013;&#26045;&#21152;&#19968;&#33268;&#24615;&#23646;&#24615;&#30340;&#26041;&#27861;&#65292;&#24182;&#20197;&#22522;&#20110;SVD&#30340;&#25512;&#33616;&#31995;&#32479;&#20026;&#20363;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#23454;&#35777;&#32467;&#26524;&#26174;&#31034;&#65292;&#21333;&#20301;&#19968;&#33268;&#24615;&#21644;&#24179;&#31227;&#19968;&#33268;&#24615;&#30340;&#24378;&#21046;&#25191;&#34892;&#19981;&#20165;&#19982;&#25512;&#33616;&#31995;&#32479;&#30340;&#40065;&#26834;&#24615;&#21644;&#20844;&#24179;&#24615;&#26377;&#20851;&#65292;&#32780;&#19988;&#36824;&#25913;&#21892;&#20102;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.08760</link><description>&lt;p&gt;
&#23545;&#40657;&#30418;&#31995;&#32479;&#26045;&#21152;&#19968;&#33268;&#24615;&#23646;&#24615;&#65292;&#24182;&#24212;&#29992;&#20110;&#22522;&#20110;SVD&#30340;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Imposing Consistency Properties on Blackbox Systems with Applications to SVD-Based Recommender Systems. (arXiv:2307.08760v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08760
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#22312;&#40657;&#30418;&#31995;&#32479;&#20013;&#26045;&#21152;&#19968;&#33268;&#24615;&#23646;&#24615;&#30340;&#26041;&#27861;&#65292;&#24182;&#20197;&#22522;&#20110;SVD&#30340;&#25512;&#33616;&#31995;&#32479;&#20026;&#20363;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#23454;&#35777;&#32467;&#26524;&#26174;&#31034;&#65292;&#21333;&#20301;&#19968;&#33268;&#24615;&#21644;&#24179;&#31227;&#19968;&#33268;&#24615;&#30340;&#24378;&#21046;&#25191;&#34892;&#19981;&#20165;&#19982;&#25512;&#33616;&#31995;&#32479;&#30340;&#40065;&#26834;&#24615;&#21644;&#20844;&#24179;&#24615;&#26377;&#20851;&#65292;&#32780;&#19988;&#36824;&#25913;&#21892;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#22312;&#40657;&#30418;&#31995;&#32479;&#65288;&#20363;&#22914;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#31995;&#32479;&#65289;&#20013;&#35825;&#23548;&#25152;&#38656;&#30340;&#19968;&#33268;&#24615;&#21644;/&#25110;&#19981;&#21464;&#24615;&#23646;&#24615;&#30340;&#39044;&#22788;&#29702;&#21644;&#21518;&#22788;&#29702;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#24120;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#24212;&#29992;&#20013;&#30340;&#40657;&#30418;&#22522;&#20110;SVD&#30340;&#30697;&#38453;&#34917;&#20840;&#26041;&#27861;&#30340;&#32972;&#26223;&#19979;&#28436;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#23454;&#35777;&#32467;&#26524;&#65292;&#26174;&#31034;&#20102;&#21333;&#20301;&#19968;&#33268;&#24615;&#21644;&#24179;&#31227;&#19968;&#33268;&#24615;&#30340;&#24378;&#21046;&#25191;&#34892;&#65292;&#36825;&#20123;&#23646;&#24615;&#19982;&#25512;&#33616;&#31995;&#32479;&#30340;&#40065;&#26834;&#24615;&#21644;&#20844;&#24179;&#24615;&#26377;&#20851;&#65292;&#21516;&#26102;&#20063;&#36890;&#36807;&#36890;&#29992;&#30340;RMSE&#21644;MAE&#24615;&#33021;&#25351;&#26631;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#19981;&#21463;&#21021;&#22987;&#36229;&#21442;&#25968;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we discuss pre- and post-processing methods to induce desired consistency and/or invariance properties in blackbox systems, e.g., AI-based. We demonstrate our approach in the context of blackbox SVD-based matrix-completion methods commonly used in recommender system (RS) applications. We provide empirical results showing that enforcement of unit-consistency and shift-consistency, which have provable RS-relevant properties relating to robustness and fairness, also lead to improved performance according to generic RMSE and MAE performance metrics, irrespective of the initial chosen hyperparameter.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#22312;SuggestBot&#19978;&#36827;&#34892;&#31163;&#32447;&#20998;&#26512;&#21644;&#20026;&#26399;&#19977;&#20010;&#26376;&#30340;&#23545;&#29031;&#23454;&#39564;&#65292;&#21457;&#29616;&#25512;&#33616;&#34987;&#20302;&#20272;&#20027;&#39064;&#30340;&#25991;&#31456;&#21487;&#20197;&#22686;&#21152;&#22312;&#36825;&#20123;&#25991;&#31456;&#19978;&#30340;&#32534;&#36753;&#24037;&#20316;&#37327;&#65292;&#21516;&#26102;&#19981;&#20250;&#26126;&#26174;&#38477;&#20302;&#23545;&#25512;&#33616;&#20869;&#23481;&#30340;&#25509;&#21463;&#31243;&#24230;&#12290;&#36825;&#19968;&#21457;&#29616;&#23545;&#35299;&#20915;&#23545;&#31561;&#29983;&#20135;&#24179;&#21488;&#19978;&#30340;&#20869;&#23481;&#32570;&#21475;&#38382;&#39064;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2307.08669</link><description>&lt;p&gt;
&#21033;&#29992;&#25512;&#33616;&#31995;&#32479;&#32553;&#23567;&#23545;&#31561;&#29983;&#20135;&#24179;&#21488;&#19978;&#30340;&#20869;&#23481;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Leveraging Recommender Systems to Reduce Content Gaps on Peer Production Platforms. (arXiv:2307.08669v2 [cs.CY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08669
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#22312;SuggestBot&#19978;&#36827;&#34892;&#31163;&#32447;&#20998;&#26512;&#21644;&#20026;&#26399;&#19977;&#20010;&#26376;&#30340;&#23545;&#29031;&#23454;&#39564;&#65292;&#21457;&#29616;&#25512;&#33616;&#34987;&#20302;&#20272;&#20027;&#39064;&#30340;&#25991;&#31456;&#21487;&#20197;&#22686;&#21152;&#22312;&#36825;&#20123;&#25991;&#31456;&#19978;&#30340;&#32534;&#36753;&#24037;&#20316;&#37327;&#65292;&#21516;&#26102;&#19981;&#20250;&#26126;&#26174;&#38477;&#20302;&#23545;&#25512;&#33616;&#20869;&#23481;&#30340;&#25509;&#21463;&#31243;&#24230;&#12290;&#36825;&#19968;&#21457;&#29616;&#23545;&#35299;&#20915;&#23545;&#31561;&#29983;&#20135;&#24179;&#21488;&#19978;&#30340;&#20869;&#23481;&#32570;&#21475;&#38382;&#39064;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32500;&#22522;&#30334;&#31185;&#31561;&#23545;&#31561;&#29983;&#20135;&#24179;&#21488;&#24120;&#24120;&#23384;&#22312;&#20869;&#23481;&#32570;&#21475;&#12290;&#20808;&#21069;&#30740;&#31350;&#34920;&#26126;&#65292;&#25512;&#33616;&#31995;&#32479;&#21487;&#20197;&#24110;&#21161;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#36890;&#36807;&#24341;&#23548;&#32534;&#36753;&#20154;&#21592;&#20851;&#27880;&#34987;&#20302;&#20272;&#30340;&#20027;&#39064;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#36825;&#31181;&#26041;&#27861;&#26159;&#21542;&#20250;&#23548;&#33268;&#19981;&#22826;&#30456;&#20851;&#30340;&#25512;&#33616;&#65292;&#20174;&#32780;&#38477;&#20302;&#23545;&#25512;&#33616;&#20869;&#23481;&#30340;&#25972;&#20307;&#21442;&#19982;&#24230;&#12290;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#22312;SuggestBot&#19978;&#36827;&#34892;&#20102;&#31163;&#32447;&#20998;&#26512;&#65288;&#30740;&#31350;1&#65289;&#65292;&#28982;&#21518;&#36827;&#34892;&#20102;&#20026;&#26399;&#19977;&#20010;&#26376;&#30340;&#23545;&#29031;&#23454;&#39564;&#65288;&#30740;&#31350;2&#65289;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#21521;&#29992;&#25143;&#23637;&#31034;&#26469;&#33258;&#34987;&#20302;&#20272;&#20027;&#39064;&#30340;&#25991;&#31456;&#21487;&#20197;&#22686;&#21152;&#22312;&#36825;&#20123;&#25991;&#31456;&#19978;&#30340;&#24037;&#20316;&#37327;&#65292;&#32780;&#19981;&#20250;&#26126;&#26174;&#38477;&#20302;&#23545;&#25512;&#33616;&#20869;&#23481;&#30340;&#25509;&#21463;&#31243;&#24230;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#32467;&#26524;&#30340;&#24847;&#20041;&#65292;&#21253;&#25324;&#22914;&#20309;&#24573;&#35270;&#25991;&#31456;&#21457;&#29616;&#36807;&#31243;&#21487;&#33021;&#20250;&#20154;&#20026;&#22320;&#38480;&#21046;&#25512;&#33616;&#12290;&#25105;&#20204;&#20197;"&#36807;&#28388;&#27668;&#27873;"&#30340;&#24120;&#35265;&#38382;&#39064;&#26469;&#23637;&#31034;&#36825;&#19968;&#29616;&#35937;&#65292;&#24182;&#23545;&#20219;&#20309;&#24179;&#21488;&#37117;&#23384;&#22312;&#30340;&#31867;&#20284;&#38382;&#39064;&#36827;&#34892;&#20102;&#31867;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
Peer production platforms like Wikipedia commonly suffer from content gaps. Prior research suggests recommender systems can help solve this problem, by guiding editors towards underrepresented topics. However, it remains unclear whether this approach would result in less relevant recommendations, leading to reduced overall engagement with recommended items. To answer this question, we first conducted offline analyses (Study 1) on SuggestBot, a task-routing recommender system for Wikipedia, then did a three-month controlled experiment (Study 2). Our results show that presenting users with articles from underrepresented topics increased the proportion of work done on those articles without significantly reducing overall recommendation uptake. We discuss the implications of our results, including how ignoring the article discovery process can artificially narrow recommendations. We draw parallels between this phenomenon and the common issue of "filter bubbles" to show how any platform tha
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#28857;&#20987;&#27169;&#22411;&#19981;&#21487;&#30693;&#30340;&#32479;&#19968;&#38750;&#21516;&#31574;&#30053;&#23398;&#20064;&#25490;&#24207;&#65288;CUOLR&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30452;&#25509;&#23398;&#20064;&#26368;&#20248;&#25490;&#21517;&#65292;&#21487;&#20197;&#36731;&#26494;&#22320;&#24212;&#29992;&#20110;&#21508;&#31181;&#28857;&#20987;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.07528</link><description>&lt;p&gt;
&#32479;&#19968;&#30340;&#38750;&#21516;&#31574;&#30053;&#23398;&#20064;&#25490;&#24207;&#65306;&#24378;&#21270;&#23398;&#20064;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Unified Off-Policy Learning to Rank: a Reinforcement Learning Perspective. (arXiv:2306.07528v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07528
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#28857;&#20987;&#27169;&#22411;&#19981;&#21487;&#30693;&#30340;&#32479;&#19968;&#38750;&#21516;&#31574;&#30053;&#23398;&#20064;&#25490;&#24207;&#65288;CUOLR&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30452;&#25509;&#23398;&#20064;&#26368;&#20248;&#25490;&#21517;&#65292;&#21487;&#20197;&#36731;&#26494;&#22320;&#24212;&#29992;&#20110;&#21508;&#31181;&#28857;&#20987;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#21516;&#31574;&#30053;&#23398;&#20064;&#25490;&#24207;&#65288;LTR&#65289;&#26088;&#22312;&#36890;&#36807;&#24050;&#37096;&#32626;&#30340;&#35760;&#24405;&#31574;&#30053;&#25910;&#38598;&#30340;&#25968;&#25454;&#20248;&#21270;&#25490;&#21517;&#22120;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#38750;&#21516;&#31574;&#30053;&#23398;&#20064;&#25490;&#24207;&#26041;&#27861;&#32463;&#24120;&#23545;&#29992;&#25143;&#22914;&#20309;&#29983;&#25104;&#28857;&#20987;&#25968;&#25454;&#21363;&#28857;&#20987;&#27169;&#22411;&#36827;&#34892;&#20551;&#35774;&#65292;&#22240;&#27492;&#38656;&#35201;&#26681;&#25454;&#19981;&#21516;&#30340;&#28857;&#20987;&#27169;&#22411;&#19987;&#38376;&#35843;&#25972;&#20182;&#20204;&#30340;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#25490;&#21517;&#36807;&#31243;&#22312;&#19968;&#33324;&#38543;&#26426;&#28857;&#20987;&#27169;&#22411;&#19979;&#32479;&#19968;&#20026;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#65292;&#36890;&#36807;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#65292;&#21487;&#20197;&#30452;&#25509;&#23398;&#20064;&#26368;&#20248;&#25490;&#21517;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#21033;&#29992;&#31163;&#32447;RL&#25216;&#26415;&#36827;&#34892;&#38750;&#21516;&#31574;&#30053;LTR&#65292;&#24182;&#25552;&#20986;&#28857;&#20987;&#27169;&#22411;&#19981;&#21487;&#30693;&#30340;&#32479;&#19968;&#38750;&#21516;&#31574;&#30053;&#23398;&#20064;&#25490;&#24207;&#65288;CUOLR&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#36731;&#26494;&#22320;&#24212;&#29992;&#20110;&#21508;&#31181;&#28857;&#20987;&#27169;&#22411;&#12290;&#36890;&#36807;&#23545;MDP&#30340;&#19987;&#38376;&#21046;&#23450;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#31163;&#32447;RL&#31639;&#27861;&#21487;&#20197;&#36866;&#24212;&#21508;&#31181;&#28857;&#20987;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#22797;&#26434;&#30340;&#21435;&#20559;&#20506;&#25216;&#26415;&#21644;&#20808;&#39564;&#30693;&#35782;&#12290;&#22312;&#21508;&#31181;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#37117;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Off-policy Learning to Rank (LTR) aims to optimize a ranker from data collected by a deployed logging policy. However, existing off-policy learning to rank methods often make strong assumptions about how users generate the click data, i.e., the click model, and hence need to tailor their methods specifically under different click models. In this paper, we unified the ranking process under general stochastic click models as a Markov Decision Process (MDP), and the optimal ranking could be learned with offline reinforcement learning (RL) directly. Building upon this, we leverage offline RL techniques for off-policy LTR and propose the Click Model-Agnostic Unified Off-policy Learning to Rank (CUOLR) method, which could be easily applied to a wide range of click models. Through a dedicated formulation of the MDP, we show that offline RL algorithms can adapt to various click models without complex debiasing techniques and prior knowledge of the model. Results on various large-scale datasets
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#32858;&#31867;&#26041;&#27861;&#65288;CLC&#65289;&#65292;&#23427;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#30452;&#25509;&#23398;&#20064;&#32858;&#31867;&#20998;&#37197;&#65292;&#24182;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#32858;&#31867;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.05439</link><description>&lt;p&gt;
CLC: &#22522;&#20110;&#23545;&#27604;&#34920;&#31034;&#23398;&#20064;&#30340;&#32858;&#31867;&#20998;&#37197;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
CLC: Cluster Assignment via Contrastive Representation Learning. (arXiv:2306.05439v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05439
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#32858;&#31867;&#26041;&#27861;&#65288;CLC&#65289;&#65292;&#23427;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#30452;&#25509;&#23398;&#20064;&#32858;&#31867;&#20998;&#37197;&#65292;&#24182;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#32858;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32858;&#31867;&#26159;&#19968;&#39033;&#37325;&#35201;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#26088;&#22312;&#23558;&#26679;&#26412;&#20998;&#32452;&#65292;&#32780;&#19981;&#38656;&#35201;&#25163;&#21160;&#27880;&#37322;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#36890;&#36807;&#23545;&#33258;&#30417;&#30563;&#23398;&#20064;&#24471;&#21040;&#30340;&#29305;&#24449;&#34920;&#31034;&#36827;&#34892;&#32858;&#31867;&#65292;&#22312;&#23567;&#22411;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#21253;&#21547;&#22823;&#37327;&#32858;&#31867;&#30340;&#25968;&#25454;&#38598;&#65292;&#22914;ImageNet&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#20173;&#28982;&#26080;&#27861;&#23454;&#29616;&#39640;&#32858;&#31867;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#32858;&#31867;&#26041;&#27861;&#65288;CLC&#65289;&#65292;&#23427;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#30452;&#25509;&#23398;&#20064;&#32858;&#31867;&#20998;&#37197;&#12290;&#25105;&#20204;&#23558;&#34920;&#31034;&#20998;&#35299;&#20026;&#20004;&#37096;&#20998;&#65306;&#19968;&#37096;&#20998;&#23545;&#31867;&#21035;&#20449;&#24687;&#36827;&#34892;&#32534;&#30721;&#65292;&#24182;&#37319;&#29992;&#31561;&#20998;&#32422;&#26463;&#65292;&#21478;&#19968;&#37096;&#20998;&#25429;&#25417;&#23454;&#20363;&#22240;&#32032;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#27604;&#25439;&#22833;&#65292;&#20351;&#29992;&#34920;&#31034;&#30340;&#20004;&#20010;&#37096;&#20998;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#20998;&#26512;&#20102;&#25152;&#25552;&#20986;&#30340;&#23545;&#27604;&#25439;&#22833;&#65292;&#24182;&#25581;&#31034;&#20102;CLC&#22312;&#23398;&#20064;&#32858;&#31867;&#20998;&#37197;&#26102;&#20026;&#36127;&#26679;&#26412;&#35774;&#32622;&#19981;&#21516;&#30340;&#26435;&#37325;&#12290;&#36827;&#19968;&#27493;&#30340;&#26799;&#24230;&#20998;&#26512;&#34920;&#26126;&#65292;&#24403;&#20351;&#29992;CLC&#26102;&#65292;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#32858;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clustering remains an important and challenging task of grouping samples into clusters without manual annotations. Recent works have achieved excellent results on small datasets by performing clustering on feature representations learned from self-supervised learning. However, for datasets with a large number of clusters, such as ImageNet, current methods still can not achieve high clustering performance. In this paper, we propose Contrastive Learning-based Clustering (CLC), which uses contrastive learning to directly learn cluster assignment. We decompose the representation into two parts: one encodes the categorical information under an equipartition constraint, and the other captures the instance-wise factors. We propose a contrastive loss using both parts of the representation. We theoretically analyze the proposed contrastive loss and reveal that CLC sets different weights for the negative samples while learning cluster assignments. Further gradient analysis shows that the larger 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;CMR&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#22810;&#30446;&#26631;&#20877;&#25490;&#24207;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;&#31574;&#30053;&#36229;&#32593;&#32476;&#65292;&#20351;&#24471;&#20559;&#22909;&#26435;&#37325;&#21487;&#20197;&#22312;&#32447;&#20248;&#21270;&#65292;&#32780;&#19981;&#29992;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.05118</link><description>&lt;p&gt;
&#29992;&#31574;&#30053;&#36229;&#32593;&#32476;&#30340;&#21487;&#25511;&#22810;&#30446;&#26631;&#20877;&#25490;&#24207;
&lt;/p&gt;
&lt;p&gt;
Controllable Multi-Objective Re-ranking with Policy Hypernetworks. (arXiv:2306.05118v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05118
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;CMR&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#22810;&#30446;&#26631;&#20877;&#25490;&#24207;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;&#31574;&#30053;&#36229;&#32593;&#32476;&#65292;&#20351;&#24471;&#20559;&#22909;&#26435;&#37325;&#21487;&#20197;&#22312;&#32447;&#20248;&#21270;&#65292;&#32780;&#19981;&#29992;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#38454;&#27573;&#25490;&#21517;&#31649;&#36947;&#24050;&#25104;&#20026;&#29616;&#20195;&#25512;&#33616;&#31995;&#32479;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#31574;&#30053;&#65292;&#20854;&#20013;&#26368;&#32456;&#38454;&#27573;&#26088;&#22312;&#36820;&#22238;&#19968;&#20010;&#25490;&#21517;&#21015;&#34920;&#65292;&#20197;&#24179;&#34913;&#29992;&#25143;&#20559;&#22909;&#12289;&#22810;&#26679;&#24615;&#12289;&#26032;&#39062;&#24615;&#31561;&#22810;&#20010;&#35201;&#27714;&#12290;&#32447;&#24615;&#26631;&#37327;&#21270;&#26159;&#23558;&#22810;&#20010;&#35201;&#27714;&#21512;&#24182;&#20026;&#19968;&#20010;&#20248;&#21270;&#30446;&#26631;&#26368;&#24191;&#27867;&#20351;&#29992;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#23450;&#30340;&#20559;&#22909;&#26435;&#37325;&#26469;&#24635;&#32467;&#36825;&#20123;&#35201;&#27714;&#12290;&#29616;&#26377;&#30340;&#26368;&#32456;&#38454;&#27573;&#25490;&#21517;&#26041;&#27861;&#36890;&#24120;&#37319;&#29992;&#38745;&#24577;&#27169;&#22411;&#65292;&#20854;&#20013;&#20559;&#22909;&#26435;&#37325;&#22312;&#31163;&#32447;&#35757;&#32451;&#26399;&#38388;&#30830;&#23450;&#65292;&#24182;&#22312;&#22312;&#32447;&#26381;&#21153;&#26399;&#38388;&#20445;&#25345;&#19981;&#21464;&#12290;&#27599;&#24403;&#38656;&#35201;&#20462;&#25913;&#20559;&#22909;&#26435;&#37325;&#26102;&#65292;&#27169;&#22411;&#24517;&#39035;&#37325;&#26032;&#35757;&#32451;&#65292;&#36825;&#26159;&#26102;&#38388;&#21644;&#36164;&#28304;&#19978;&#30340;&#28010;&#36153;&#12290;&#21516;&#26102;&#65292;&#19981;&#21516;&#29992;&#25143;&#32676;&#20307;&#25110;&#19981;&#21516;&#26102;&#38388;&#27573;&#65288;&#20363;&#22914;&#65292;&#22312;&#33410;&#26085;&#20419;&#38144;&#26399;&#38388;&#65289;&#30340;&#26368;&#21512;&#36866;&#26435;&#37325;&#21487;&#33021;&#20250;&#26377;&#24456;&#22823;&#30340;&#24046;&#24322;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#21487;&#25511;&#22810;&#30446;&#26631;&#20877;&#25490;&#24207;&#65288;CMR&#65289;&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20351;&#29992;&#31574;&#30053;&#36229;&#32593;&#32476;&#65292;&#20197;&#20351;&#20559;&#22909;&#26435;&#37325;&#22312;&#32447;&#20248;&#21270;&#65292;&#32780;&#19981;&#24517;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#20855;&#26377;&#28789;&#27963;&#24615;&#21644;&#21487;&#25511;&#24615;&#65292;&#20026;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#22810;&#30446;&#26631;&#20877;&#25490;&#24207;&#38382;&#39064;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-stage ranking pipelines have become widely used strategies in modern recommender systems, where the final stage aims to return a ranked list of items that balances a number of requirements such as user preference, diversity, novelty etc. Linear scalarization is arguably the most widely used technique to merge multiple requirements into one optimization objective, by summing up the requirements with certain preference weights. Existing final-stage ranking methods often adopt a static model where the preference weights are determined during offline training and kept unchanged during online serving. Whenever a modification of the preference weights is needed, the model has to be re-trained, which is time and resources inefficient. Meanwhile, the most appropriate weights may vary greatly for different groups of targeting users or at different time periods (e.g., during holiday promotions). In this paper, we propose a framework called controllable multi-objective re-ranking (CMR) whic
&lt;/p&gt;</description></item><item><title>&#8220;Life of PII&#8221;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#28151;&#28102;&#21464;&#25442;&#22120;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;PII&#36716;&#21270;&#20026;&#20154;&#36896;PII&#21516;&#26102;&#23613;&#21487;&#33021;&#22320;&#20445;&#30041;&#21407;&#22987;&#20449;&#24687;&#12289;&#24847;&#22270;&#21644;&#19978;&#19979;&#25991;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#26377;&#36873;&#25321;&#22320;&#28151;&#28102;&#25991;&#26723;&#20013;&#30340;&#25935;&#24863;&#20449;&#24687;&#65292;&#21516;&#26102;&#20445;&#30041;&#25991;&#26723;&#30340;&#32479;&#35745;&#21644;&#35821;&#20041;&#29305;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.09550</link><description>&lt;p&gt;
PII&#30340;&#29983;&#21629;--&#19968;&#31181;PII&#28151;&#28102;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Life of PII -- A PII Obfuscation Transformer. (arXiv:2305.09550v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09550
&lt;/p&gt;
&lt;p&gt;
&#8220;Life of PII&#8221;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#28151;&#28102;&#21464;&#25442;&#22120;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;PII&#36716;&#21270;&#20026;&#20154;&#36896;PII&#21516;&#26102;&#23613;&#21487;&#33021;&#22320;&#20445;&#30041;&#21407;&#22987;&#20449;&#24687;&#12289;&#24847;&#22270;&#21644;&#19978;&#19979;&#25991;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#26377;&#36873;&#25321;&#22320;&#28151;&#28102;&#25991;&#26723;&#20013;&#30340;&#25935;&#24863;&#20449;&#24687;&#65292;&#21516;&#26102;&#20445;&#30041;&#25991;&#26723;&#30340;&#32479;&#35745;&#21644;&#35821;&#20041;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#20170;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#25968;&#25454;&#39537;&#21160;&#26381;&#21153;&#30340;&#19990;&#30028;&#20013;&#65292;&#20445;&#25252;&#25935;&#24863;&#20449;&#24687;&#33267;&#20851;&#37325;&#35201;&#12290;&#19968;&#31181;&#24120;&#35265;&#30340;&#26041;&#27861;&#26159;&#20351;&#29992;&#25968;&#25454;&#25200;&#21160;&#25216;&#26415;&#26469;&#20943;&#23569;(&#25935;&#24863;)&#20010;&#20154;&#36523;&#20221;&#35782;&#21035;&#20449;&#24687;(PII)&#25968;&#25454;&#30340;&#36807;&#24230;&#23454;&#29992;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#32479;&#35745;&#21644;&#35821;&#20041;&#29305;&#24615;&#12290;&#25968;&#25454;&#25200;&#21160;&#26041;&#27861;&#32463;&#24120;&#23548;&#33268;&#26174;&#30528;&#30340;&#20449;&#24687;&#25439;&#22833;&#65292;&#20351;&#23427;&#20204;&#38590;&#20197;&#20351;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;PII&#30340;&#29983;&#21629;&#8221;--&#19968;&#31181;&#26032;&#39062;&#30340;&#28151;&#28102;&#21464;&#25442;&#22120;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;PII&#36716;&#21270;&#20026;&#20154;&#36896;PII&#21516;&#26102;&#23613;&#21487;&#33021;&#22320;&#20445;&#30041;&#21407;&#22987;&#20449;&#24687;&#12289;&#24847;&#22270;&#21644;&#19978;&#19979;&#25991;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#19968;&#20010;API&#26469;&#19982;&#32473;&#23450;&#30340;&#25991;&#26723;&#36827;&#34892;&#25509;&#21475;&#65292;&#19968;&#20010;&#22522;&#20110;&#37197;&#32622;&#30340;&#28151;&#28102;&#22120;&#21644;&#19968;&#20010;&#22522;&#20110;Transformer&#26550;&#26500;&#30340;&#27169;&#22411;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#21644;LLMs&#20013;&#34920;&#29616;&#20986;&#39640;&#30340;&#19978;&#19979;&#25991;&#20445;&#23384;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#22522;&#20110;Transformer&#30340;&#26041;&#27861;&#23398;&#20064;&#20102;&#21407;&#22987;PII&#21644;&#20854;&#36716;&#25442;&#21518;&#30340;&#20154;&#36896;PII&#23545;&#24212;&#30340;&#26144;&#23556;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#26377;&#36873;&#25321;&#22320;&#28151;&#28102;&#25991;&#26723;&#20013;&#30340;&#25935;&#24863;&#20449;&#24687;&#65292;&#21516;&#26102;&#20445;&#30041;&#25991;&#26723;&#30340;&#32479;&#35745;&#21644;&#35821;&#20041;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Protecting sensitive information is crucial in today's world of Large Language Models (LLMs) and data-driven services. One common method used to preserve privacy is by using data perturbation techniques to reduce overreaching utility of (sensitive) Personal Identifiable Information (PII) data while maintaining its statistical and semantic properties. Data perturbation methods often result in significant information loss, making them impractical for use. In this paper, we propose 'Life of PII', a novel Obfuscation Transformer framework for transforming PII into faux-PII while preserving the original information, intent, and context as much as possible. Our approach includes an API to interface with the given document, a configuration-based obfuscator, and a model based on the Transformer architecture, which has shown high context preservation and performance in natural language processing tasks and LLMs.  Our Transformer-based approach learns mapping between the original PII and its tra
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986; EXPLAIGNN &#26041;&#27861;&#65292;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#26500;&#24314;&#24322;&#26500;&#22270;&#65292;&#20174;&#22810;&#20010;&#25968;&#25454;&#28304;&#20013;&#25552;&#21462;&#20449;&#24687;&#65292;&#36798;&#21040;&#26356;&#20840;&#38754;&#20934;&#30830;&#30340;&#38382;&#31572;&#25928;&#26524;&#65292;&#24182;&#25552;&#20379;&#20102;&#29992;&#25143;&#21487;&#29702;&#35299;&#30340;&#31572;&#26696;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2305.01548</link><description>&lt;p&gt;
&#36845;&#20195;&#22270;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#24322;&#26500;&#25968;&#25454;&#21487;&#35299;&#37322;&#23545;&#35805;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Explainable Conversational Question Answering over Heterogeneous Sources via Iterative Graph Neural Networks. (arXiv:2305.01548v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01548
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986; EXPLAIGNN &#26041;&#27861;&#65292;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#26500;&#24314;&#24322;&#26500;&#22270;&#65292;&#20174;&#22810;&#20010;&#25968;&#25454;&#28304;&#20013;&#25552;&#21462;&#20449;&#24687;&#65292;&#36798;&#21040;&#26356;&#20840;&#38754;&#20934;&#30830;&#30340;&#38382;&#31572;&#25928;&#26524;&#65292;&#24182;&#25552;&#20379;&#20102;&#29992;&#25143;&#21487;&#29702;&#35299;&#30340;&#31572;&#26696;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23545;&#35805;&#38382;&#31572;&#20013;&#65292;&#29992;&#25143;&#36890;&#36807;&#19968;&#31995;&#21015;&#19978;&#19979;&#25991;&#19981;&#23436;&#25972;&#30340;&#34920;&#36798;&#26469;&#34920;&#36798;&#20182;&#20204;&#30340;&#20449;&#24687;&#38656;&#27714;&#12290;&#20856;&#22411;&#30340; ConvQA &#26041;&#27861;&#20381;&#36182;&#20110;&#21333;&#19968;&#25968;&#25454;&#28304;(&#30693;&#35782;&#24211;&#12289;&#25991;&#26412;&#35821;&#26009;&#24211;&#25110;&#19968;&#32452;&#34920;&#26684;)&#65292;&#22240;&#27492;&#26080;&#27861;&#20174;&#22810;&#20010;&#25968;&#25454;&#28304;&#30340;&#35206;&#30422;&#33539;&#22260;&#21644;&#20887;&#20313;&#24615;&#20013;&#33719;&#30410;&#12290;&#25105;&#20204;&#30340; EXPLAIGNN &#26041;&#27861;&#36890;&#36807;&#28151;&#21512;&#25968;&#25454;&#28304;&#24182;&#25552;&#20379;&#29992;&#25143;&#21487;&#29702;&#35299;&#30340;&#31572;&#26696;&#35299;&#37322;&#26469;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#12290;&#23427;&#20174;&#30693;&#35782;&#24211;&#12289;&#25991;&#26412;&#35821;&#26009;&#24211;&#12289;&#32593;&#32476;&#34920;&#26684;&#21644;&#20449;&#24687;&#26694;&#20013;&#26816;&#32034;&#23454;&#20307;&#21644;&#35777;&#25454;&#29255;&#27573;&#65292;&#24182;&#26500;&#24314;&#19968;&#20010;&#24322;&#26500;&#22270;&#12290;&#28982;&#21518;&#36890;&#36807;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#36845;&#20195;&#32553;&#20943;&#65292;&#30452;&#21040;&#26368;&#20339;&#31572;&#26696;&#21450;&#20854;&#35299;&#37322;&#34987;&#25552;&#28860;&#20986;&#26469;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;EXPLAIGNN &#25552;&#39640;&#20102;&#29616;&#26377;&#22522;&#32447;&#26041;&#24335;&#30340;&#24615;&#33021;&#12290;&#29992;&#25143;&#30740;&#31350;&#34920;&#26126;&#65292;&#23548;&#20986;&#30340;&#31572;&#26696;&#34987;&#32456;&#31471;&#29992;&#25143;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
In conversational question answering, users express their information needs through a series of utterances with incomplete context. Typical ConvQA methods rely on a single source (a knowledge base (KB), or a text corpus, or a set of tables), thus being unable to benefit from increased answer coverage and redundancy of multiple sources. Our method EXPLAIGNN overcomes these limitations by integrating information from a mixture of sources with user-comprehensible explanations for answers. It constructs a heterogeneous graph from entities and evidence snippets retrieved from a KB, a text corpus, web tables, and infoboxes. This large graph is then iteratively reduced via graph neural networks that incorporate question-level attention, until the best answers and their explanations are distilled. Experiments show that EXPLAIGNN improves performance over state-of-the-art baselines. A user study demonstrates that derived answers are understandable by end users.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#22810;&#27169;&#24577;&#33258;&#30417;&#30563;&#23398;&#20064;&#8221;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26377;&#25928;&#22320;&#23398;&#20064;&#27169;&#24577;&#24863;&#30693;&#29992;&#25143;&#20559;&#22909;&#21644;&#36328;&#27169;&#24577;&#20381;&#36182;&#20851;&#31995;&#30340;&#33258;&#25105;&#30417;&#25511;&#20449;&#21495;&#65292;&#20197;&#25552;&#39640;&#25512;&#33616;&#31995;&#32479;&#30340;&#24615;&#33021;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#25928;&#26524;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#21644;&#22810;&#27169;&#24577;&#25512;&#33616;&#12290;</title><link>http://arxiv.org/abs/2302.10632</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#33258;&#30417;&#30563;&#23398;&#20064;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Multi-Modal Self-Supervised Learning for Recommendation. (arXiv:2302.10632v4 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10632
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#22810;&#27169;&#24577;&#33258;&#30417;&#30563;&#23398;&#20064;&#8221;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26377;&#25928;&#22320;&#23398;&#20064;&#27169;&#24577;&#24863;&#30693;&#29992;&#25143;&#20559;&#22909;&#21644;&#36328;&#27169;&#24577;&#20381;&#36182;&#20851;&#31995;&#30340;&#33258;&#25105;&#30417;&#25511;&#20449;&#21495;&#65292;&#20197;&#25552;&#39640;&#25512;&#33616;&#31995;&#32479;&#30340;&#24615;&#33021;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#25928;&#26524;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#21644;&#22810;&#27169;&#24577;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#20998;&#20139;&#24179;&#21488;&#65288;&#20363;&#22914; TikTok&#12289;YouTube&#65289;&#30340;&#23835;&#36215;&#20351;&#24471;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#33021;&#22815;&#23558;&#21508;&#31181;&#27169;&#24335;&#65288;&#20363;&#22914;&#35270;&#35273;&#12289;&#25991;&#26412;&#21644;&#22768;&#38899;&#65289;&#32435;&#20837;&#28508;&#22312;&#29992;&#25143;&#34920;&#31034;&#27861;&#12290;&#34429;&#28982;&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;&#25512;&#33616;&#24037;&#20316;&#21033;&#29992;&#22810;&#23186;&#20307;&#20869;&#23481;&#29305;&#24449;&#22686;&#24378;&#29289;&#21697;&#23884;&#20837;&#65292;&#20294;&#23427;&#20204;&#30340;&#27169;&#22411;&#34920;&#31034;&#33021;&#21147;&#21463;&#21040;&#37325;&#26631;&#31614;&#20381;&#36182;&#24615;&#21644;&#31232;&#30095;&#29992;&#25143;&#34892;&#20026;&#25968;&#25454;&#30340;&#24433;&#21709;&#12290;&#21463;&#33258;&#30417;&#25511;&#23398;&#20064;&#22312;&#20943;&#36731;&#26631;&#31614;&#31232;&#32570;&#24615;&#38382;&#39064;&#26041;&#38754;&#30340;&#26368;&#26032;&#36827;&#23637;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25506;&#32034;&#26377;&#25928;&#22320;&#23398;&#20064;&#27169;&#24577;&#24863;&#30693;&#29992;&#25143;&#20559;&#22909;&#21644;&#36328;&#27169;&#24577;&#20381;&#36182;&#20851;&#31995;&#30340;&#33258;&#25105;&#30417;&#25511;&#20449;&#21495;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#27169;&#24577;&#33258;&#30417;&#25511;&#23398;&#20064;&#65288;MMSSL&#65289;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#20004;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20026;&#20102;&#34920;&#24449;&#29992;&#25143;-&#29289;&#21697;&#21327;&#21516;&#35270;&#22270;&#21644;&#29289;&#21697;&#22810;&#27169;&#24577;&#35821;&#20041;&#35270;&#22270;&#20043;&#38388;&#30340;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#27169;&#24577;&#24863;&#30693;&#30340;&#20132;&#20114;&#32467;&#26500;&#23398;&#20064;&#32452;&#20214;&#65307;&#20026;&#20102;&#22312;&#27169;&#24577;&#30456;&#20851;&#30340;&#25512;&#33616;&#20219;&#21153;&#20013;&#36827;&#19968;&#27493;&#21033;&#29992;&#33258;&#25105;&#30417;&#25511;&#20449;&#21495;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#23398;&#20064;&#29992;&#25143;&#20869;&#37096;&#27169;&#24577;&#20998;&#24067;&#30340;&#27169;&#24577;&#30456;&#20851;&#30340;&#39044;&#25991;&#26412;&#20219;&#21153;&#12290;&#23545;&#21508;&#31181;&#30495;&#23454;&#19990;&#30028;&#25512;&#33616;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;MMSSL&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#25512;&#33616;&#22522;&#32447;&#21644;&#22810;&#27169;&#24577;&#23545;&#24212;&#29289;&#65292;&#29305;&#21035;&#26159;&#22312;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#31232;&#32570;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
The online emergence of multi-modal sharing platforms (eg, TikTok, Youtube) is powering personalized recommender systems to incorporate various modalities (eg, visual, textual and acoustic) into the latent user representations. While existing works on multi-modal recommendation exploit multimedia content features in enhancing item embeddings, their model representation capability is limited by heavy label reliance and weak robustness on sparse user behavior data. Inspired by the recent progress of self-supervised learning in alleviating label scarcity issue, we explore deriving self-supervision signals with effectively learning of modality-aware user preference and cross-modal dependencies. To this end, we propose a new Multi-Modal Self-Supervised Learning (MMSSL) method which tackles two key challenges. Specifically, to characterize the inter-dependency between the user-item collaborative view and item multi-modal semantic view, we design a modality-aware interactive structure learnin
&lt;/p&gt;</description></item></channel></rss>