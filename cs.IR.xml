<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LFG&#30340;&#29983;&#25104;&#32593;&#32476;&#65292;&#29992;&#20110;&#23454;&#29616;&#23454;&#26102;&#30340;&#25512;&#33616;&#31995;&#32479;&#12290;&#35813;&#32593;&#32476;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21160;&#24577;&#29983;&#25104;&#29992;&#25143;&#30340;&#28508;&#22312;&#22240;&#23376;&#65292;&#26080;&#38656;&#37325;&#26032;&#20998;&#35299;&#25110;&#37325;&#26032;&#35757;&#32451;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#32593;&#32476;&#22312;&#25552;&#39640;&#25512;&#33616;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#23454;&#26102;&#25512;&#33616;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2310.20189</link><description>&lt;p&gt;
LFG&#65306;&#19968;&#31181;&#29992;&#20110;&#23454;&#26102;&#25512;&#33616;&#30340;&#29983;&#25104;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
LFG: A Generative Network for Real-Time Recommendation. (arXiv:2310.20189v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20189
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LFG&#30340;&#29983;&#25104;&#32593;&#32476;&#65292;&#29992;&#20110;&#23454;&#29616;&#23454;&#26102;&#30340;&#25512;&#33616;&#31995;&#32479;&#12290;&#35813;&#32593;&#32476;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21160;&#24577;&#29983;&#25104;&#29992;&#25143;&#30340;&#28508;&#22312;&#22240;&#23376;&#65292;&#26080;&#38656;&#37325;&#26032;&#20998;&#35299;&#25110;&#37325;&#26032;&#35757;&#32451;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#32593;&#32476;&#22312;&#25552;&#39640;&#25512;&#33616;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#23454;&#26102;&#25512;&#33616;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#26159;&#24403;&#20170;&#37325;&#35201;&#30340;&#20449;&#24687;&#25216;&#26415;&#65292;&#32467;&#21512;&#28145;&#24230;&#23398;&#20064;&#30340;&#25512;&#33616;&#31639;&#27861;&#24050;&#25104;&#20026;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#28909;&#28857;&#12290;&#36890;&#36807;&#30697;&#38453;&#20998;&#35299;&#21644;&#26799;&#24230;&#19979;&#38477;&#25429;&#25417;&#28508;&#22312;&#29305;&#24449;&#20197;&#36866;&#24212;&#29992;&#25143;&#20559;&#22909;&#30340;&#28508;&#22312;&#22240;&#23376;&#27169;&#22411;&#65288;LFM&#65289;&#25512;&#21160;&#20102;&#21508;&#31181;&#25913;&#36827;&#25512;&#33616;&#20934;&#30830;&#24615;&#30340;&#25512;&#33616;&#31639;&#27861;&#30340;&#20986;&#29616;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;LFM&#30340;&#21327;&#21516;&#36807;&#28388;&#25512;&#33616;&#27169;&#22411;&#32570;&#20047;&#28789;&#27963;&#24615;&#65292;&#24182;&#19988;&#22312;&#23454;&#26102;&#25512;&#33616;&#26041;&#38754;&#23384;&#22312;&#19968;&#20123;&#32570;&#28857;&#65292;&#22240;&#20026;&#24403;&#26377;&#26032;&#29992;&#25143;&#21040;&#36798;&#26102;&#38656;&#35201;&#37325;&#26032;&#36827;&#34892;&#30697;&#38453;&#20998;&#35299;&#21644;&#37325;&#26032;&#35757;&#32451;&#12290;&#38024;&#23545;&#36825;&#19968;&#38382;&#39064;&#65292;&#26412;&#25991;&#21019;&#26032;&#24615;&#22320;&#25552;&#20986;&#20102;&#19968;&#31181;Latent Factor Generator (LFG)&#32593;&#32476;&#65292;&#24182;&#23558;&#30005;&#24433;&#25512;&#33616;&#20316;&#20026;&#30740;&#31350;&#20027;&#39064;&#12290;LFG&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21160;&#24577;&#29983;&#25104;&#29992;&#25143;&#30340;&#28508;&#22312;&#22240;&#23376;&#65292;&#26080;&#38656;&#37325;&#26032;&#20998;&#35299;&#25110;&#37325;&#26032;&#35757;&#32451;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#25552;&#39640;&#25512;&#33616;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#23454;&#26102;&#25512;&#33616;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommender systems are essential information technologies today, and recommendation algorithms combined with deep learning have become a research hotspot in this field. The recommendation model known as LFM (Latent Factor Model), which captures latent features through matrix factorization and gradient descent to fit user preferences, has given rise to various recommendation algorithms that bring new improvements in recommendation accuracy. However, collaborative filtering recommendation models based on LFM lack flexibility and has shortcomings for real-time recommendations, as they need to redo the matrix factorization and retrain using gradient descent when new users arrive. In response to this, this paper innovatively proposes a Latent Factor Generator (LFG) network, and set the movie recommendation as research theme. The LFG dynamically generates user latent factors through deep neural networks without the need for re-factorization or retrain. Experimental results indicate that the
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ICSRec&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#24314;&#27169;&#29992;&#25143;&#30340;&#28508;&#22312;&#24847;&#22270;&#65292;&#20197;&#25552;&#39640;&#39034;&#24207;&#25512;&#33616;&#30340;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#21644;&#20132;&#21449;&#23376;&#24207;&#21015;&#26469;&#20934;&#30830;&#25429;&#25417;&#29992;&#25143;&#30340;&#24847;&#22270;&#12290;</title><link>http://arxiv.org/abs/2310.14318</link><description>&lt;p&gt;
&#29992;&#20132;&#21449;&#23376;&#24207;&#21015;&#36827;&#34892;&#24847;&#22270;&#23545;&#27604;&#23398;&#20064;&#30340;&#39034;&#24207;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Intent Contrastive Learning with Cross Subsequences for Sequential Recommendation. (arXiv:2310.14318v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14318
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ICSRec&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#24314;&#27169;&#29992;&#25143;&#30340;&#28508;&#22312;&#24847;&#22270;&#65292;&#20197;&#25552;&#39640;&#39034;&#24207;&#25512;&#33616;&#30340;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#21644;&#20132;&#21449;&#23376;&#24207;&#21015;&#26469;&#20934;&#30830;&#25429;&#25417;&#29992;&#25143;&#30340;&#24847;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#25143;&#30340;&#36141;&#20080;&#34892;&#20026;&#20027;&#35201;&#21463;&#21040;&#20182;&#20204;&#30340;&#24847;&#22270;&#24433;&#21709;&#65288;&#20363;&#22914;&#65292;&#36141;&#20080;&#35013;&#39280;&#29992;&#30340;&#34915;&#26381;&#65292;&#36141;&#20080;&#30011;&#30011;&#29992;&#30340;&#30011;&#31508;&#31561;&#65289;&#12290;&#24314;&#27169;&#29992;&#25143;&#30340;&#28508;&#22312;&#24847;&#22270;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#25512;&#33616;&#30340;&#24615;&#33021;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#36890;&#36807;&#32771;&#34385;&#36741;&#21161;&#20449;&#24687;&#20013;&#30340;&#39044;&#23450;&#20041;&#26631;&#31614;&#25110;&#24341;&#20837;&#38543;&#26426;&#25968;&#25454;&#22686;&#24378;&#26469;&#24314;&#27169;&#29992;&#25143;&#30340;&#24847;&#22270;&#12290;&#28982;&#32780;&#65292;&#36741;&#21161;&#20449;&#24687;&#26159;&#31232;&#30095;&#30340;&#65292;&#24182;&#19988;&#23545;&#20110;&#25512;&#33616;&#31995;&#32479;&#24182;&#19981;&#24635;&#26159;&#21487;&#29992;&#30340;&#65292;&#24341;&#20837;&#38543;&#26426;&#25968;&#25454;&#22686;&#24378;&#21487;&#33021;&#20250;&#24341;&#20837;&#22122;&#22768;&#65292;&#20174;&#32780;&#25913;&#21464;&#24207;&#21015;&#20013;&#38544;&#34255;&#30340;&#24847;&#22270;&#12290;&#22240;&#27492;&#65292;&#21033;&#29992;&#29992;&#25143;&#30340;&#24847;&#22270;&#36827;&#34892;&#39034;&#24207;&#25512;&#33616;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#32463;&#24120;&#21464;&#21270;&#19988;&#19981;&#21487;&#35266;&#23519;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#29992;&#20110;&#39034;&#24207;&#25512;&#33616;&#30340;&#24847;&#22270;&#23545;&#27604;&#23398;&#20064;&#19982;&#20132;&#21449;&#23376;&#24207;&#21015;&#65288;ICSRec&#65289;&#65292;&#29992;&#20110;&#24314;&#27169;&#29992;&#25143;&#30340;&#28508;&#22312;&#24847;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
The user purchase behaviors are mainly influenced by their intentions (e.g., buying clothes for decoration, buying brushes for painting, etc.). Modeling a user's latent intention can significantly improve the performance of recommendations. Previous works model users' intentions by considering the predefined label in auxiliary information or introducing stochastic data augmentation to learn purposes in the latent space. However, the auxiliary information is sparse and not always available for recommender systems, and introducing stochastic data augmentation may introduce noise and thus change the intentions hidden in the sequence. Therefore, leveraging user intentions for sequential recommendation (SR) can be challenging because they are frequently varied and unobserved. In this paper, Intent contrastive learning with Cross Subsequences for sequential Recommendation (ICSRec) is proposed to model users' latent intentions. Specifically, ICSRec first segments a user's sequential behaviors
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32479;&#19968;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#39034;&#24207;&#25512;&#33616;&#26041;&#27861;&#65288;UPSR&#65289;&#65292;&#26088;&#22312;&#26500;&#24314;&#19968;&#20010;&#32479;&#19968;&#30340;&#39044;&#35757;&#32451;&#25512;&#33616;&#27169;&#22411;&#29992;&#20110;&#22810;&#39046;&#22495;&#25512;&#33616;&#20219;&#21153;&#12290;&#30740;&#31350;&#32773;&#35774;&#35745;&#20102;&#20116;&#20010;&#20851;&#38190;&#25351;&#26631;&#26469;&#25351;&#23548;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#25991;&#26412;-&gt;&#29289;&#21697;&#36866;&#24212;&#21644;&#34892;&#20026;&#24207;&#21015;-&gt;&#25991;&#26412;&#24207;&#21015;&#36866;&#24212;&#12290;</title><link>http://arxiv.org/abs/2310.13540</link><description>&lt;p&gt;
&#20840;&#38754;&#23558;&#22810;&#39046;&#22495;&#39044;&#35757;&#32451;&#25512;&#33616;&#24314;&#27169;&#20026;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
Thoroughly Modeling Multi-domain Pre-trained Recommendation as Language. (arXiv:2310.13540v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13540
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32479;&#19968;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#39034;&#24207;&#25512;&#33616;&#26041;&#27861;&#65288;UPSR&#65289;&#65292;&#26088;&#22312;&#26500;&#24314;&#19968;&#20010;&#32479;&#19968;&#30340;&#39044;&#35757;&#32451;&#25512;&#33616;&#27169;&#22411;&#29992;&#20110;&#22810;&#39046;&#22495;&#25512;&#33616;&#20219;&#21153;&#12290;&#30740;&#31350;&#32773;&#35774;&#35745;&#20102;&#20116;&#20010;&#20851;&#38190;&#25351;&#26631;&#26469;&#25351;&#23548;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#25991;&#26412;-&gt;&#29289;&#21697;&#36866;&#24212;&#21644;&#34892;&#20026;&#24207;&#21015;-&gt;&#25991;&#26412;&#24207;&#21015;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#20808;&#39537;&#24615;&#24037;&#20316;&#35797;&#22270;&#25506;&#32034;&#23558;PLM&#20013;&#30340;&#36890;&#29992;&#25991;&#26412;&#20449;&#24687;&#19982;&#29992;&#25143;&#21382;&#21490;&#34892;&#20026;&#24207;&#21015;&#20013;&#30340;&#20010;&#24615;&#21270;&#34892;&#20026;&#20449;&#24687;&#30456;&#32467;&#21512;&#65292;&#20197;&#22686;&#24378;&#39034;&#24207;&#25512;&#33616;&#65288;SR&#65289;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#36755;&#20837;&#26684;&#24335;&#21644;&#20219;&#21153;&#30446;&#26631;&#23384;&#22312;&#20849;&#24615;&#65292;&#34892;&#20026;&#21644;&#25991;&#26412;&#20449;&#24687;&#20043;&#38388;&#23384;&#22312;&#24040;&#22823;&#24046;&#36317;&#65292;&#36825;&#38459;&#30861;&#20102;&#23558;SR&#20316;&#20026;&#35821;&#35328;&#24314;&#27169;&#23436;&#20840;&#24314;&#27169;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32479;&#19968;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#39034;&#24207;&#25512;&#33616;&#65288;UPSR&#65289;&#26041;&#27861;&#65292;&#26088;&#22312;&#26500;&#24314;&#19968;&#20010;&#32479;&#19968;&#30340;&#39044;&#35757;&#32451;&#25512;&#33616;&#27169;&#22411;&#29992;&#20110;&#22810;&#39046;&#22495;&#25512;&#33616;&#20219;&#21153;&#12290;&#25105;&#20204;&#27491;&#24335;&#35774;&#35745;&#20102;&#33258;&#28982;&#24615;&#12289;&#39046;&#22495;&#19968;&#33268;&#24615;&#12289;&#20449;&#24687;&#24615;&#12289;&#22122;&#22768;&#21644;&#27169;&#31946;&#24615;&#20197;&#21450;&#25991;&#26412;&#38271;&#24230;&#31561;&#20116;&#20010;&#20851;&#38190;&#25351;&#26631;&#65292;&#20998;&#21035;&#29992;&#20110;&#25351;&#23548;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#25991;&#26412;-&gt;&#29289;&#21697;&#36866;&#24212;&#21644;&#34892;&#20026;&#24207;&#21015;-&gt;&#25991;&#26412;&#24207;&#21015;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the thriving of pre-trained language model (PLM) widely verified in various of NLP tasks, pioneer efforts attempt to explore the possible cooperation of the general textual information in PLM with the personalized behavioral information in user historical behavior sequences to enhance sequential recommendation (SR). However, despite the commonalities of input format and task goal, there are huge gaps between the behavioral and textual information, which obstruct thoroughly modeling SR as language modeling via PLM. To bridge the gap, we propose a novel Unified pre-trained language model enhanced sequential recommendation (UPSR), aiming to build a unified pre-trained recommendation model for multi-domain recommendation tasks. We formally design five key indicators, namely naturalness, domain consistency, informativeness, noise &amp; ambiguity, and text length, to guide the text-&gt;item adaptation and behavior sequence-&gt;text sequence adaptation differently for pre-training and fine-tuning 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#22686;&#24378;&#30340;&#20302;&#36164;&#28304;&#25991;&#26412;&#20998;&#31867;&#27169;&#22411;G2P2&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#21644;&#25552;&#31034;&#30340;&#26041;&#24335;&#65292;&#21033;&#29992;&#22270;&#32467;&#26500;&#30340;&#35821;&#20041;&#20851;&#31995;&#26469;&#25552;&#21319;&#20302;&#36164;&#28304;&#25991;&#26412;&#20998;&#31867;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.10230</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#22686;&#24378;&#30340;&#20302;&#36164;&#28304;&#25991;&#26412;&#20998;&#31867;&#30340;Prompt&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
Prompt Tuning on Graph-augmented Low-resource Text Classification. (arXiv:2307.10230v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10230
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#22686;&#24378;&#30340;&#20302;&#36164;&#28304;&#25991;&#26412;&#20998;&#31867;&#27169;&#22411;G2P2&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#21644;&#25552;&#31034;&#30340;&#26041;&#24335;&#65292;&#21033;&#29992;&#22270;&#32467;&#26500;&#30340;&#35821;&#20041;&#20851;&#31995;&#26469;&#25552;&#21319;&#20302;&#36164;&#28304;&#25991;&#26412;&#20998;&#31867;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#20998;&#31867;&#26159;&#20449;&#24687;&#26816;&#32034;&#20013;&#30340;&#19968;&#20010;&#22522;&#30784;&#38382;&#39064;&#65292;&#26377;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#65292;&#20363;&#22914;&#39044;&#27979;&#22312;&#32447;&#25991;&#31456;&#30340;&#20027;&#39064;&#21644;&#30005;&#23376;&#21830;&#21153;&#20135;&#21697;&#25551;&#36848;&#30340;&#31867;&#21035;&#12290;&#28982;&#32780;&#65292;&#20302;&#36164;&#28304;&#25991;&#26412;&#20998;&#31867;&#65292;&#21363;&#27809;&#26377;&#25110;&#21482;&#26377;&#24456;&#23569;&#26631;&#27880;&#26679;&#26412;&#30340;&#24773;&#20917;&#65292;&#23545;&#30417;&#30563;&#23398;&#20064;&#26500;&#25104;&#20102;&#20005;&#37325;&#38382;&#39064;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#35768;&#22810;&#25991;&#26412;&#25968;&#25454;&#26412;&#36136;&#19978;&#37117;&#24314;&#31435;&#22312;&#32593;&#32476;&#32467;&#26500;&#19978;&#65292;&#20363;&#22914;&#22312;&#32447;&#25991;&#31456;&#30340;&#36229;&#38142;&#25509;/&#24341;&#29992;&#32593;&#32476;&#21644;&#30005;&#23376;&#21830;&#21153;&#20135;&#21697;&#30340;&#29992;&#25143;-&#29289;&#21697;&#36141;&#20080;&#32593;&#32476;&#12290;&#36825;&#20123;&#22270;&#32467;&#26500;&#25429;&#25417;&#20102;&#20016;&#23500;&#30340;&#35821;&#20041;&#20851;&#31995;&#65292;&#26377;&#21161;&#20110;&#22686;&#24378;&#20302;&#36164;&#28304;&#25991;&#26412;&#20998;&#31867;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Graph-Grounded Pre-training and Prompting (G2P2)&#30340;&#26032;&#27169;&#22411;&#65292;&#20197;&#20004;&#26041;&#38754;&#26041;&#27861;&#35299;&#20915;&#20302;&#36164;&#28304;&#25991;&#26412;&#20998;&#31867;&#38382;&#39064;&#12290;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#22522;&#20110;&#22270;&#20132;&#20114;&#30340;&#23545;&#27604;&#31574;&#30053;&#65292;&#20849;&#21516;&#39044;&#35757;&#32451;&#22270;&#25991;&#27169;&#22411;&#65307;&#22312;&#19979;&#28216;&#20998;&#31867;&#38454;&#27573;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#25163;&#24037;&#35774;&#35745;&#30340;&#25552;&#31034;&#20449;&#24687;&#23545;&#27169;&#22411;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text classification is a fundamental problem in information retrieval with many real-world applications, such as predicting the topics of online articles and the categories of e-commerce product descriptions. However, low-resource text classification, with no or few labeled samples, presents a serious concern for supervised learning. Meanwhile, many text data are inherently grounded on a network structure, such as a hyperlink/citation network for online articles, and a user-item purchase network for e-commerce products. These graph structures capture rich semantic relationships, which can potentially augment low-resource text classification. In this paper, we propose a novel model called Graph-Grounded Pre-training and Prompting (G2P2) to address low-resource text classification in a two-pronged approach. During pre-training, we propose three graph interaction-based contrastive strategies to jointly pre-train a graph-text model; during downstream classification, we explore handcrafted 
&lt;/p&gt;</description></item><item><title>Text2Cohort&#26159;&#19968;&#20010;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#24037;&#20855;&#31665;&#65292;&#21487;&#20197;&#23558;&#29992;&#25143;&#36755;&#20837;&#36716;&#21270;&#20026;IDC&#25968;&#25454;&#24211;&#26597;&#35810;&#65292;&#20419;&#36827;&#33258;&#28982;&#35821;&#35328;&#38431;&#21015;&#21457;&#29616;&#65292;&#20943;&#23569;&#30740;&#31350;&#20154;&#21592;&#26597;&#35810;IDC&#25968;&#25454;&#24211;&#30340;&#23398;&#20064;&#26354;&#32447;&#65292;&#23454;&#29616;&#20102;&#30284;&#30151;&#25104;&#20687;&#25968;&#25454;&#30340;&#27665;&#20027;&#21270;&#12290;</title><link>http://arxiv.org/abs/2305.07637</link><description>&lt;p&gt;
Text2Cohort: &#33258;&#28982;&#35821;&#35328;&#38431;&#21015;&#21457;&#29616;&#23545;&#30284;&#30151;&#24433;&#20687;&#25968;&#25454;&#20849;&#20139;&#24179;&#21488;&#30340;&#27665;&#20027;&#21270;
&lt;/p&gt;
&lt;p&gt;
Text2Cohort: Democratizing the NCI Imaging Data Commons with Natural Language Cohort Discovery. (arXiv:2305.07637v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07637
&lt;/p&gt;
&lt;p&gt;
Text2Cohort&#26159;&#19968;&#20010;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#24037;&#20855;&#31665;&#65292;&#21487;&#20197;&#23558;&#29992;&#25143;&#36755;&#20837;&#36716;&#21270;&#20026;IDC&#25968;&#25454;&#24211;&#26597;&#35810;&#65292;&#20419;&#36827;&#33258;&#28982;&#35821;&#35328;&#38431;&#21015;&#21457;&#29616;&#65292;&#20943;&#23569;&#30740;&#31350;&#20154;&#21592;&#26597;&#35810;IDC&#25968;&#25454;&#24211;&#30340;&#23398;&#20064;&#26354;&#32447;&#65292;&#23454;&#29616;&#20102;&#30284;&#30151;&#25104;&#20687;&#25968;&#25454;&#30340;&#27665;&#20027;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24433;&#20687;&#25968;&#25454;&#20849;&#20139;&#24179;&#21488;(IDC)&#26159;&#19968;&#20010;&#22522;&#20110;&#20113;&#30340;&#25968;&#25454;&#24211;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#24320;&#25918;&#33719;&#21462;&#30340;&#30284;&#30151;&#25104;&#20687;&#25968;&#25454;&#21644;&#20998;&#26512;&#24037;&#20855;&#65292;&#26088;&#22312;&#20419;&#36827;&#21307;&#23398;&#25104;&#20687;&#30740;&#31350;&#20013;&#30340;&#21327;&#20316;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#22797;&#26434;&#21644;&#25216;&#26415;&#24615;&#36136;&#65292;&#26597;&#35810;IDC&#25968;&#25454;&#24211;&#20197;&#36827;&#34892;&#38431;&#21015;&#21457;&#29616;&#21644;&#35775;&#38382;&#25104;&#20687;&#25968;&#25454;&#23545;&#30740;&#31350;&#20154;&#21592;&#26469;&#35828;&#20855;&#26377;&#26174;&#33879;&#30340;&#23398;&#20064;&#26354;&#32447;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;Text2Cohort&#24037;&#20855;&#31665;&#65292;&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#23558;&#29992;&#25143;&#36755;&#20837;&#36716;&#21270;&#20026;IDC&#25968;&#25454;&#24211;&#26597;&#35810;&#65292;&#24182;&#23558;&#26597;&#35810;&#30340;&#21709;&#24212;&#36820;&#22238;&#32473;&#29992;&#25143;&#65292;&#20197;&#20419;&#36827;&#33258;&#28982;&#35821;&#35328;&#38431;&#21015;&#21457;&#29616;&#12290;&#27492;&#22806;&#65292;&#23454;&#29616;&#20102;&#33258;&#21160;&#26657;&#27491;&#20197;&#35299;&#20915;&#26597;&#35810;&#20013;&#30340;&#35821;&#27861;&#21644;&#35821;&#20041;&#38169;&#35823;&#65292;&#36890;&#36807;&#23558;&#38169;&#35823;&#20256;&#22238;&#27169;&#22411;&#36827;&#34892;&#35299;&#37322;&#21644;&#26657;&#27491;&#12290;&#25105;&#20204;&#23545;50&#20010;&#33258;&#28982;&#35821;&#35328;&#29992;&#25143;&#36755;&#20837;&#36827;&#34892;&#20102;Text2Cohort&#35780;&#20272;&#65292;&#33539;&#22260;&#20174;&#20449;&#24687;&#25552;&#21462;&#21040;&#38431;&#21015;&#21457;&#29616;&#12290;&#32467;&#26524;&#26597;&#35810;&#21644;&#36755;&#20986;&#30001;&#20004;&#20301;&#35745;&#31639;&#26426;&#31185;&#23398;&#23478;&#36827;&#34892;&#20102;&#30830;&#35748;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Imaging Data Commons (IDC) is a cloud-based database that provides researchers with open access to cancer imaging data and tools for analysis, with the goal of facilitating collaboration in medical imaging research. However, querying the IDC database for cohort discovery and access to imaging data has a significant learning curve for researchers due to its complex and technical nature. We developed Text2Cohort, a large language model (LLM) based toolkit to facilitate natural language cohort discovery by translating user input into IDC database queries through prompt engineering and returning the query's response to the user. Furthermore, autocorrection is implemented to resolve syntax and semantic errors in queries by passing the errors back to the model for interpretation and correction. We evaluate Text2Cohort on 50 natural language user inputs ranging from information extraction to cohort discovery. The resulting queries and outputs were verified by two computer scientists to me
&lt;/p&gt;</description></item></channel></rss>