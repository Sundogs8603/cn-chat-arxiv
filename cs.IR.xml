<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MeKB-Rec&#30340;&#36328;&#39046;&#22495;&#25512;&#33616;&#26041;&#27861;&#65292;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#35299;&#20915;&#20102;&#20919;&#21551;&#21160;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20010;&#20154;&#30693;&#35782;&#22270;&#35889;&#20316;&#20026;&#39046;&#22495;&#19981;&#21464;&#30340;&#29992;&#25143;&#20852;&#36259;&#34920;&#31034;&#65292;&#36890;&#36807;&#23398;&#20064;&#35821;&#20041;&#34920;&#31034;&#21644;&#27880;&#20837;&#19990;&#30028;&#30693;&#35782;&#65292;&#23454;&#29616;&#20102;&#23545;&#26032;&#29992;&#25143;&#30340;&#38646;-shot&#25512;&#33616;&#12290;</title><link>http://arxiv.org/abs/2310.11088</link><description>&lt;p&gt;
MeKB-Rec&#65306;&#20010;&#20154;&#30693;&#35782;&#22270;&#35889;&#23398;&#20064;&#29992;&#20110;&#36328;&#39046;&#22495;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
MeKB-Rec: Personal Knowledge Graph Learning for Cross-Domain Recommendation. (arXiv:2310.11088v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11088
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MeKB-Rec&#30340;&#36328;&#39046;&#22495;&#25512;&#33616;&#26041;&#27861;&#65292;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#35299;&#20915;&#20102;&#20919;&#21551;&#21160;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20010;&#20154;&#30693;&#35782;&#22270;&#35889;&#20316;&#20026;&#39046;&#22495;&#19981;&#21464;&#30340;&#29992;&#25143;&#20852;&#36259;&#34920;&#31034;&#65292;&#36890;&#36807;&#23398;&#20064;&#35821;&#20041;&#34920;&#31034;&#21644;&#27880;&#20837;&#19990;&#30028;&#30693;&#35782;&#65292;&#23454;&#29616;&#20102;&#23545;&#26032;&#29992;&#25143;&#30340;&#38646;-shot&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#20195;&#25512;&#33616;&#31995;&#32479;&#20013;&#65292;&#22914;&#20309;&#38024;&#23545;&#26032;&#29992;&#25143;&#26377;&#25928;&#22320;&#36827;&#34892;&#25512;&#33616;&#65292;&#21363;&#20919;&#21551;&#21160;&#38382;&#39064;&#65292;&#19968;&#30452;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20010;&#20154;&#30693;&#35782;&#22270;&#35889;&#65288;PKG&#65289;&#20316;&#20026;&#19968;&#20010;&#39046;&#22495;&#19981;&#21464;&#30340;&#20852;&#36259;&#34920;&#31034;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MeKB-Rec&#30340;&#26032;&#22411;&#36328;&#39046;&#22495;&#25512;&#33616;&#33539;&#24335;&#12290;&#25105;&#20204;&#39318;&#20808;&#23558;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#29992;&#25143;&#21644;&#23454;&#20307;&#36827;&#34892;&#20851;&#32852;&#65292;&#26500;&#24314;&#20102;&#29992;&#25143;&#20852;&#36259;&#30340;PKG&#65292;&#21363;MeKB&#12290;&#28982;&#21518;&#25105;&#20204;&#23398;&#20064;&#20102;MeKB&#30340;&#35821;&#20041;&#34920;&#31034;&#65292;&#29992;&#20110;&#36328;&#39046;&#22495;&#25512;&#33616;&#12290;&#20026;&#20102;&#39640;&#25928;&#21033;&#29992;CDR&#20013;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;MeKB-Rec&#37319;&#29992;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#23558;&#19990;&#30028;&#30693;&#35782;&#27880;&#20837;&#21040;&#23545;&#29992;&#25143;&#20852;&#36259;&#30340;&#29702;&#35299;&#20013;&#12290;&#19982;&#22823;&#22810;&#25968;&#29616;&#26377;&#31995;&#32479;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#39046;&#22495;&#20043;&#38388;&#24314;&#31435;&#20102;&#35821;&#20041;&#26144;&#23556;&#65292;&#28040;&#38500;&#20102;&#23545;&#39046;&#22495;&#20869;&#29992;&#25143;&#34892;&#20026;&#30340;&#35201;&#27714;&#65292;&#23454;&#29616;&#20102;&#23545;&#26032;&#29992;&#25143;&#30340;&#38646;-shot&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is a long-standing challenge in modern recommender systems to effectively make recommendations for new users, namely the cold-start problem. Cross-Domain Recommendation (CDR) has been proposed to address this challenge, but current ways to represent users' interests across systems are still severely limited. We introduce Personal Knowledge Graph (PKG) as a domain-invariant interest representation, and propose a novel CDR paradigm named MeKB-Rec. We first link users and entities in a knowledge base to construct a PKG of users' interests, named MeKB. Then we learn a semantic representation of MeKB for the cross-domain recommendation. To efficiently utilize limited training data in CDR, MeKB-Rec employs Pretrained Language Models to inject world knowledge into understanding users' interests. Beyond most existing systems, our approach builds a semantic mapping across domains which breaks the requirement for in-domain user behaviors, enabling zero-shot recommendations for new users in a 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#22312;SemEval-2023&#27861;&#24459;&#35780;&#20272;&#20219;&#21153;6&#19978;&#30340;&#25552;&#20132;&#65292;&#20027;&#35201;&#38598;&#20013;&#22312;&#27861;&#24459;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12289;&#27861;&#24459;&#21028;&#20915;&#39044;&#27979;&#21644;&#24102;&#35299;&#37322;&#30340;&#27861;&#38498;&#21028;&#20915;&#39044;&#27979;&#31561;&#23376;&#20219;&#21153;&#19978;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22810;&#20010;&#23454;&#39564;&#65292;&#24182;&#21462;&#24471;&#20102;&#22312;&#21508;&#20010;&#23376;&#20219;&#21153;&#20013;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#25490;&#21517;&#12290;</title><link>http://arxiv.org/abs/2310.11049</link><description>&lt;p&gt;
SemEval-2023&#20219;&#21153;6&#20013;&#30340;&#38750;&#32435;&#20219;&#21153;:&#27861;&#24459;&#35780;&#20272;&#26041;&#27861;&#35770;&#12290;(arXiv:2310.11049v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
Nonet at SemEval-2023 Task 6: Methodologies for Legal Evaluation. (arXiv:2310.11049v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11049
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#22312;SemEval-2023&#27861;&#24459;&#35780;&#20272;&#20219;&#21153;6&#19978;&#30340;&#25552;&#20132;&#65292;&#20027;&#35201;&#38598;&#20013;&#22312;&#27861;&#24459;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12289;&#27861;&#24459;&#21028;&#20915;&#39044;&#27979;&#21644;&#24102;&#35299;&#37322;&#30340;&#27861;&#38498;&#21028;&#20915;&#39044;&#27979;&#31561;&#23376;&#20219;&#21153;&#19978;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22810;&#20010;&#23454;&#39564;&#65292;&#24182;&#21462;&#24471;&#20102;&#22312;&#21508;&#20010;&#23376;&#20219;&#21153;&#20013;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#25105;&#20204;&#22312;SemEval-2023&#27861;&#24459;&#35780;&#20272;&#20219;&#21153;6&#19978;&#30340;&#25552;&#20132;&#12290;&#25105;&#20204;&#30340;&#25552;&#20132;&#20027;&#35201;&#38598;&#20013;&#22312;&#19977;&#20010;&#23376;&#20219;&#21153;&#19978;&#65306;&#20219;&#21153;B&#30340;&#27861;&#24459;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;(L-NER)&#65292;&#20219;&#21153;C1&#30340;&#27861;&#24459;&#21028;&#20915;&#39044;&#27979;(LJP)&#21644;&#20219;&#21153;C2&#30340;&#24102;&#35299;&#37322;&#30340;&#27861;&#38498;&#21028;&#20915;&#39044;&#27979;(CJPE)&#12290;&#25105;&#20204;&#23545;&#36825;&#20123;&#23376;&#20219;&#21153;&#36827;&#34892;&#20102;&#21508;&#31181;&#23454;&#39564;&#65292;&#24182;&#35814;&#32454;&#21576;&#29616;&#20102;&#32467;&#26524;&#65292;&#21253;&#25324;&#25968;&#25454;&#32479;&#35745;&#21644;&#26041;&#27861;&#35770;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#20687;&#26412;&#30740;&#31350;&#20013;&#25152;&#28041;&#21450;&#30340;&#27861;&#24459;&#20219;&#21153;&#27491;&#22312;&#22240;&#33258;&#21160;&#21270;&#27861;&#24459;&#20998;&#26512;&#21644;&#25903;&#25345;&#30340;&#38656;&#27714;&#22686;&#21152;&#32780;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#25105;&#20204;&#30340;&#22242;&#38431;&#22312;&#25490;&#34892;&#27036;&#19978;&#25253;&#21578;&#30340;&#20219;&#21153;B&#12289;&#20219;&#21153;C1&#21644;&#20219;&#21153;C2&#20013;&#20998;&#21035;&#33719;&#24471;&#20102;15th&#12289;11th&#21644;1st&#30340;&#31454;&#20105;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper describes our submission to the SemEval-2023 for Task 6 on LegalEval: Understanding Legal Texts. Our submission concentrated on three subtasks: Legal Named Entity Recognition (L-NER) for Task-B, Legal Judgment Prediction (LJP) for Task-C1, and Court Judgment Prediction with Explanation (CJPE) for Task-C2. We conducted various experiments on these subtasks and presented the results in detail, including data statistics and methodology. It is worth noting that legal tasks, such as those tackled in this research, have been gaining importance due to the increasing need to automate legal analysis and support. Our team obtained competitive rankings of 15$^{th}$, 11$^{th}$, and 1$^{st}$ in Task-B, Task-C1, and Task-C2, respectively, as reported on the leaderboard.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21382;&#21490;&#30740;&#31350;&#36741;&#21161;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#23558;&#39640;&#24230;&#19987;&#19994;&#21270;&#30340;&#23398;&#26415;&#36164;&#28304;&#23884;&#20837;&#21040;&#27169;&#22411;&#20013;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#23545;&#35805;&#24418;&#24335;&#30340;&#30740;&#31350;&#26041;&#27861;&#65292;&#21487;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#26816;&#32034;&#19981;&#21516;&#31867;&#22411;&#30340;&#21382;&#21490;&#25991;&#29486;&#65292;&#24182;&#22312;&#38382;&#31572;&#21644;&#25968;&#25454;&#25552;&#21462;&#31561;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2310.10808</link><description>&lt;p&gt;
&#22914;&#26524;&#36164;&#28304;&#33021;&#22815;&#35828;&#35805;&#65306;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21382;&#21490;&#30740;&#31350;&#36741;&#21161;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
If the Sources Could Talk: Evaluating Large Language Models for Research Assistance in History. (arXiv:2310.10808v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10808
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21382;&#21490;&#30740;&#31350;&#36741;&#21161;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#23558;&#39640;&#24230;&#19987;&#19994;&#21270;&#30340;&#23398;&#26415;&#36164;&#28304;&#23884;&#20837;&#21040;&#27169;&#22411;&#20013;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#23545;&#35805;&#24418;&#24335;&#30340;&#30740;&#31350;&#26041;&#27861;&#65292;&#21487;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#26816;&#32034;&#19981;&#21516;&#31867;&#22411;&#30340;&#21382;&#21490;&#25991;&#29486;&#65292;&#24182;&#22312;&#38382;&#31572;&#21644;&#25968;&#25454;&#25552;&#21462;&#31561;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#22823;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#20986;&#29616;&#20026;&#21382;&#21490;&#35760;&#24518;&#30340;&#23545;&#35805;&#24418;&#24335;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#30740;&#31350;&#36884;&#24452;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#39640;&#24230;&#19987;&#19994;&#21270;&#23398;&#26415;&#36164;&#28304;&#30340;&#21521;&#37327;&#23884;&#20837;&#24341;&#20837;&#21040;LLM&#20013;&#65292;&#20351;&#24471;&#23545;&#35805;&#26041;&#27861;&#21487;&#20197;&#34987;&#21382;&#21490;&#23398;&#23478;&#21644;&#20854;&#20182;&#20154;&#25991;&#23398;&#31185;&#30740;&#31350;&#20154;&#21592;&#20351;&#29992;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#35780;&#20272;&#21644;&#23637;&#31034;&#20102;LLM&#22312;&#30740;&#31350;&#20154;&#21592;&#26816;&#26597;&#19981;&#21516;&#31867;&#22411;&#25991;&#26723;&#30340;&#23450;&#21046;&#35821;&#26009;&#24211;&#26102;&#30340;&#36741;&#21161;&#33021;&#21147;&#65292;&#21253;&#25324;&#20294;&#19981;&#38480;&#20110;&#65306;(1).&#19968;&#25163;&#36164;&#26009;&#65292;(2).&#30001;&#19987;&#23478;&#25776;&#20889;&#30340;&#20108;&#25163;&#36164;&#26009;&#65292;&#20197;&#21450;(3).&#20004;&#32773;&#30340;&#32467;&#21512;&#12290;&#19982;&#20256;&#32479;&#30340;&#25968;&#23383;&#30446;&#24405;&#25628;&#32034;&#30028;&#38754;&#65288;&#22914;&#20803;&#25968;&#25454;&#21644;&#20840;&#25991;&#25628;&#32034;&#65289;&#30456;&#27604;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;LLM&#30340;&#26356;&#20016;&#23500;&#30340;&#23545;&#35805;&#39118;&#26684;&#23545;&#20004;&#31181;&#20027;&#35201;&#20219;&#21153;&#30340;&#34920;&#29616;&#65306;(1).&#38382;&#31572;&#65292;&#20197;&#21450;(2).&#25968;&#25454;&#30340;&#25552;&#21462;&#21644;&#32452;&#32455;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;LLM&#30340;&#35821;&#20041;&#26816;&#32034;&#21644;&#25512;&#29702;&#33021;&#21147;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent advent of powerful Large-Language Models (LLM) provides a new conversational form of inquiry into historical memory (or, training data, in this case). We show that by augmenting such LLMs with vector embeddings from highly specialized academic sources, a conversational methodology can be made accessible to historians and other researchers in the Humanities. Concretely, we evaluate and demonstrate how LLMs have the ability of assisting researchers while they examine a customized corpora of different types of documents, including, but not exclusive to: (1). primary sources, (2). secondary sources written by experts, and (3). the combination of these two. Compared to established search interfaces for digital catalogues, such as metadata and full-text search, we evaluate the richer conversational style of LLMs on the performance of two main types of tasks: (1). question-answering, and (2). extraction and organization of data. We demonstrate that LLMs semantic retrieval and reaso
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#27169;&#22411;&#65292;&#26088;&#22312;&#21516;&#26102;&#27169;&#25311;&#35821;&#20041;&#21644;&#21327;&#21516;&#30693;&#35782;&#65292;&#20197;&#23454;&#29616;&#20934;&#30830;&#30340;CTR&#20272;&#35745;&#65292;&#24182;&#35299;&#20915;&#25512;&#29702;&#25928;&#29575;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.09234</link><description>&lt;p&gt;
ClickPrompt: CTR&#27169;&#22411;&#26159;&#23558;&#35821;&#35328;&#27169;&#22411;&#36866;&#24212;&#20026;CTR&#39044;&#27979;&#30340;&#24378;&#22823;&#25552;&#31034;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
ClickPrompt: CTR Models are Strong Prompt Generators for Adapting Language Models to CTR Prediction. (arXiv:2310.09234v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09234
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#27169;&#22411;&#65292;&#26088;&#22312;&#21516;&#26102;&#27169;&#25311;&#35821;&#20041;&#21644;&#21327;&#21516;&#30693;&#35782;&#65292;&#20197;&#23454;&#29616;&#20934;&#30830;&#30340;CTR&#20272;&#35745;&#65292;&#24182;&#35299;&#20915;&#25512;&#29702;&#25928;&#29575;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28857;&#20987;&#29575;&#65288;CTR&#65289;&#39044;&#27979;&#24050;&#32463;&#25104;&#20026;&#21508;&#31181;&#20114;&#32852;&#32593;&#24212;&#29992;&#31243;&#24207;&#20013;&#36234;&#26469;&#36234;&#19981;&#21487;&#25110;&#32570;&#30340;&#12290;&#20256;&#32479;&#30340;CTR&#27169;&#22411;&#36890;&#36807;&#29420;&#28909;&#32534;&#30721;&#23558;&#22810;&#23383;&#27573;&#20998;&#31867;&#25968;&#25454;&#36716;&#25442;&#20026;ID&#29305;&#24449;&#65292;&#24182;&#25552;&#21462;&#29305;&#24449;&#20043;&#38388;&#30340;&#21327;&#21516;&#20449;&#21495;&#12290;&#36825;&#31181;&#33539;&#24335;&#30340;&#38382;&#39064;&#22312;&#20110;&#35821;&#20041;&#20449;&#24687;&#30340;&#20002;&#22833;&#12290;&#21478;&#19968;&#26041;&#38754;&#30340;&#30740;&#31350;&#36890;&#36807;&#23558;&#36755;&#20837;&#25968;&#25454;&#36716;&#25442;&#20026;&#25991;&#26412;&#21477;&#23376;&#26469;&#25506;&#32034;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#22312;CTR&#39044;&#27979;&#20013;&#30340;&#28508;&#21147;&#12290;&#34429;&#28982;&#35821;&#20041;&#20449;&#21495;&#24471;&#21040;&#20102;&#20445;&#30041;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#26080;&#27861;&#25429;&#25417;&#21040;&#21327;&#21516;&#20449;&#24687;&#65288;&#22914;&#29305;&#24449;&#20132;&#20114;&#12289;&#32431;ID&#29305;&#24449;&#65289;&#65292;&#26356;&#19981;&#29992;&#35828;&#30001;&#24222;&#22823;&#30340;&#27169;&#22411;&#22823;&#23567;&#24102;&#26469;&#30340;&#26080;&#27861;&#25509;&#21463;&#30340;&#25512;&#29702;&#24320;&#38144;&#20102;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#20026;&#20934;&#30830;&#30340;CTR&#20272;&#35745;&#24314;&#31435;&#35821;&#20041;&#30693;&#35782;&#21644;&#21327;&#21516;&#30693;&#35782;&#65292;&#24182;&#35299;&#20915;&#25512;&#29702;&#25928;&#29575;&#38382;&#39064;&#12290;&#20026;&#20102;&#20174;&#20004;&#20010;&#39046;&#22495;&#20013;&#21463;&#30410;&#24182;&#24357;&#21512;&#23427;&#20204;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;-&#12290;
&lt;/p&gt;
&lt;p&gt;
Click-through rate (CTR) prediction has become increasingly indispensable for various Internet applications. Traditional CTR models convert the multi-field categorical data into ID features via one-hot encoding, and extract the collaborative signals among features. Such a paradigm suffers from the problem of semantic information loss. Another line of research explores the potential of pretrained language models (PLMs) for CTR prediction by converting input data into textual sentences through hard prompt templates. Although semantic signals are preserved, they generally fail to capture the collaborative information (e.g., feature interactions, pure ID features), not to mention the unacceptable inference overhead brought by the huge model size. In this paper, we aim to model both the semantic knowledge and collaborative knowledge for accurate CTR estimation, and meanwhile address the inference inefficiency issue. To benefit from both worlds and close their gaps, we propose a novel model-
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#22686;&#24378;&#22806;&#37096;&#27880;&#24847;&#21147;&#30340;&#22270;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#28040;&#38500;&#36328;&#39046;&#22495;&#23494;&#24230;&#20559;&#24046;&#24182;&#31283;&#23450;&#22320;&#25429;&#25417;&#29992;&#25143;&#30340;&#34892;&#20026;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2310.04633</link><description>&lt;p&gt;
&#26080;&#20559;&#21644;&#40065;&#26834;&#24615;&#65306;&#22686;&#24378;&#22806;&#37096;&#27880;&#24847;&#21147;&#30340;&#36328;&#39046;&#22495;&#24207;&#21015;&#25512;&#33616;&#20013;&#30340;&#22270;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Unbiased and Robust: External Attention-enhanced Graph Contrastive Learning for Cross-domain Sequential Recommendation. (arXiv:2310.04633v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04633
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#22686;&#24378;&#22806;&#37096;&#27880;&#24847;&#21147;&#30340;&#22270;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#28040;&#38500;&#36328;&#39046;&#22495;&#23494;&#24230;&#20559;&#24046;&#24182;&#31283;&#23450;&#22320;&#25429;&#25417;&#29992;&#25143;&#30340;&#34892;&#20026;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#39046;&#22495;&#24207;&#21015;&#25512;&#33616;&#22120;&#65288;CSRs&#65289;&#22240;&#33021;&#22815;&#21033;&#29992;&#22810;&#20010;&#39046;&#22495;&#30340;&#36741;&#21161;&#20449;&#24687;&#25429;&#25417;&#29992;&#25143;&#30340;&#24207;&#21015;&#20559;&#22909;&#32780;&#24341;&#36215;&#20102;&#30456;&#24403;&#22823;&#30340;&#30740;&#31350;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30740;&#31350;&#36890;&#24120;&#36981;&#24490;&#19968;&#20010;&#29702;&#24819;&#30340;&#35774;&#32622;&#65292;&#21363;&#19981;&#21516;&#30340;&#39046;&#22495;&#36981;&#23432;&#30456;&#20284;&#30340;&#25968;&#25454;&#20998;&#24067;&#65292;&#24573;&#35270;&#20102;&#30001;&#19981;&#23545;&#31216;&#20132;&#20114;&#23494;&#24230;&#24102;&#26469;&#30340;&#20559;&#24046;&#65288;&#21363;&#36328;&#39046;&#22495;&#23494;&#24230;&#20559;&#24046;&#65289;&#12290;&#27492;&#22806;&#65292;&#24207;&#21015;&#32534;&#30721;&#22120;&#20013;&#32463;&#24120;&#37319;&#29992;&#30340;&#26426;&#21046;&#65288;&#22914;&#33258;&#27880;&#24847;&#32593;&#32476;&#65289;&#21482;&#20851;&#27880;&#23616;&#37096;&#35270;&#22270;&#20869;&#30340;&#20132;&#20114;&#65292;&#24573;&#35270;&#20102;&#19981;&#21516;&#35757;&#32451;&#25209;&#27425;&#20043;&#38388;&#30340;&#20840;&#23616;&#30456;&#20851;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#22806;&#37096;&#27880;&#24847;&#21147;&#30340;&#22270;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#21363;EA-GCL&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20026;&#20102;&#28040;&#38500;&#36328;&#39046;&#22495;&#23494;&#24230;&#20559;&#24046;&#30340;&#24433;&#21709;&#65292;&#22312;&#20256;&#32479;&#22270;&#32534;&#30721;&#22120;&#19979;&#38468;&#21152;&#20102;&#19968;&#20010;&#36741;&#21161;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#20219;&#21153;&#65292;&#37319;&#29992;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#24335;&#12290;&#20026;&#20102;&#31283;&#23450;&#22320;&#25429;&#25417;&#29992;&#25143;&#30340;&#34892;&#20026;&#27169;&#24335;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;...
&lt;/p&gt;
&lt;p&gt;
Cross-domain sequential recommenders (CSRs) are gaining considerable research attention as they can capture user sequential preference by leveraging side information from multiple domains. However, these works typically follow an ideal setup, i.e., different domains obey similar data distribution, which ignores the bias brought by asymmetric interaction densities (a.k.a. the inter-domain density bias). Besides, the frequently adopted mechanism (e.g., the self-attention network) in sequence encoder only focuses on the interactions within a local view, which overlooks the global correlations between different training batches. To this end, we propose an External Attention-enhanced Graph Contrastive Learning framework, namely EA-GCL. Specifically, to remove the impact of the inter-domain density bias, an auxiliary Self-Supervised Learning (SSL) task is attached to the traditional graph encoder under a multi-task learning manner. To robustly capture users' behavioral patterns, we develop a
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#20154;&#31867;&#20013;&#24515;&#30340;&#25512;&#33616;&#26694;&#26550;RAH&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#21161;&#25163;&#65292;&#23454;&#29616;&#29992;&#25143;&#28385;&#24847;&#24230;&#21644;&#20010;&#24615;&#21270;&#21453;&#39304;&#65292;&#24182;&#25104;&#21151;&#24212;&#29992;&#20110;&#23398;&#20064;&#29992;&#25143;&#20010;&#24615;&#21644;&#35843;&#25972;&#25512;&#33616;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2308.09904</link><description>&lt;p&gt;
RAH&#65281;RecSys-Assistant-Human&#65306;&#19968;&#20010;&#20855;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20154;&#31867;&#20013;&#24515;&#25512;&#33616;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
RAH! RecSys-Assistant-Human: A Human-Central Recommendation Framework with Large Language Models. (arXiv:2308.09904v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09904
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#20154;&#31867;&#20013;&#24515;&#30340;&#25512;&#33616;&#26694;&#26550;RAH&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#21161;&#25163;&#65292;&#23454;&#29616;&#29992;&#25143;&#28385;&#24847;&#24230;&#21644;&#20010;&#24615;&#21270;&#21453;&#39304;&#65292;&#24182;&#25104;&#21151;&#24212;&#29992;&#20110;&#23398;&#20064;&#29992;&#25143;&#20010;&#24615;&#21644;&#35843;&#25972;&#25512;&#33616;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#29983;&#24577;&#31995;&#32479;&#28041;&#21450;&#21040;&#25512;&#33616;&#31995;&#32479;&#65288;&#35745;&#31639;&#26426;&#65289;&#21644;&#29992;&#25143;&#65288;&#20154;&#31867;&#65289;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;&#19982;&#25512;&#33616;&#31995;&#32479;&#30340;&#35282;&#24230;&#19981;&#21516;&#65292;&#25105;&#20204;&#23581;&#35797;&#20174;&#29992;&#25143;&#30340;&#35282;&#24230;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#24182;&#25552;&#20986;&#19968;&#20010;&#26356;&#21152;&#20154;&#31867;&#20013;&#24515;&#30340;&#25512;&#33616;&#26694;&#26550;&#65292;&#21629;&#21517;&#20026;RAH&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#25512;&#33616;&#31995;&#32479;&#12289;&#21161;&#25163;&#21644;&#20154;&#31867;&#12290;&#21161;&#25163;&#26159;&#19968;&#20010;&#22522;&#20110;LLMs&#30340;&#20010;&#20154;&#20195;&#29702;&#65292;&#29992;&#20110;&#23454;&#29616;&#29992;&#25143;&#28385;&#24847;&#24230;&#12290;&#21161;&#25163;&#25198;&#28436;&#38750;&#20405;&#20837;&#24615;&#30340;&#35282;&#33394;&#65292;RAH&#26694;&#26550;&#21487;&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#25512;&#33616;&#31995;&#32479;&#21644;&#29992;&#25143;&#32676;&#20307;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#23454;&#29616;&#24182;&#35780;&#20272;&#20102;RAH&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#29992;&#25143;&#20010;&#24615;&#21644;&#20195;&#29702;&#20154;&#31867;&#21453;&#39304;&#12290;&#23454;&#39564;&#34920;&#26126;&#65306;&#65288;1&#65289;&#20351;&#29992;&#23398;&#20064;-&#34892;&#21160;-&#35780;&#35770;&#23478;&#21644;&#21453;&#24605;&#26426;&#21046;&#21487;&#20197;&#23548;&#33268;&#26356;&#21152;&#19968;&#33268;&#30340;&#20010;&#24615;&#65292;&#65288;2&#65289;&#25105;&#20204;&#30340;&#21161;&#25163;&#21487;&#20197;&#26377;&#25928;&#22320;&#20195;&#29702;&#20154;&#31867;&#21453;&#39304;&#24182;&#24110;&#21161;&#35843;&#25972;&#25512;&#33616;&#31995;&#32479;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#22312;RAH&#26694;&#26550;&#20013;&#36827;&#19968;&#27493;&#35299;&#20915;&#20154;&#31867;&#20013;&#24515;&#38382;&#39064;&#30340;&#31574;&#30053;&#65292;&#21253;&#25324;&#29992;&#25143;``&#22842;&#26435;''&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recommendation ecosystem involves interactions between recommender systems(Computer) and users(Human). Orthogonal to the perspective of recommender systems, we attempt to utilize LLMs from the perspective of users and propose a more human-central recommendation framework named RAH, which consists of Recommender system, Assistant and Human. The assistant is a LLM-based and personal proxy for a human to achieve user satisfaction. The assistant plays a non-invasion role and the RAH framework can adapt to different recommender systems and user groups. Subsequently, we implement and evaluate the RAH framework for learning user personalities and proxy human feedback. The experiment shows that (1) using learn-action-critic and reflection mechanisms can lead more aligned personality and (2) our assistant can effectively proxy human feedback and help adjust recommender systems. Finally, we discuss further strategies in the RAH framework to address human-central concerns including user contr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#39640;&#20998;&#36776;&#29575;&#27169;&#26495;&#21305;&#37197;&#26041;&#27861;&#65292;&#36890;&#36807;&#21521;&#37327;&#37327;&#21270;&#21644;&#28388;&#27874;&#26469;&#20943;&#23569;&#35745;&#31639;&#37327;&#21644;&#32771;&#34385;&#21464;&#24418;&#65292;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.15010</link><description>&lt;p&gt;
&#39640;&#20998;&#36776;&#29575;&#27169;&#26495;&#21305;&#37197;&#20013;&#30340;&#39640;&#25928;&#21521;&#37327;&#37327;&#21270;&#26368;&#36817;&#37051;&#22330;
&lt;/p&gt;
&lt;p&gt;
Efficient High-Resolution Template Matching with Vector Quantized Nearest Neighbour Fields. (arXiv:2306.15010v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15010
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#39640;&#20998;&#36776;&#29575;&#27169;&#26495;&#21305;&#37197;&#26041;&#27861;&#65292;&#36890;&#36807;&#21521;&#37327;&#37327;&#21270;&#21644;&#28388;&#27874;&#26469;&#20943;&#23569;&#35745;&#31639;&#37327;&#21644;&#32771;&#34385;&#21464;&#24418;&#65292;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#26495;&#21305;&#37197;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#22522;&#30784;&#38382;&#39064;&#65292;&#24182;&#22312;&#29289;&#20307;&#26816;&#27979;&#12289;&#22270;&#20687;&#37197;&#20934;&#21644;&#29289;&#20307;&#36319;&#36394;&#31561;&#39046;&#22495;&#26377;&#24212;&#29992;&#12290;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#26159;&#20381;&#36182;&#20110;&#26368;&#36817;&#37051;&#65288;NN&#65289;&#21305;&#37197;&#65292;&#22312;&#35813;&#26041;&#27861;&#20013;&#65292;&#23558;&#26597;&#35810;&#29305;&#24449;&#31354;&#38388;&#36716;&#25442;&#20026;NN&#31354;&#38388;&#65292;&#20854;&#20013;&#27599;&#20010;&#26597;&#35810;&#20687;&#32032;&#29992;&#27169;&#26495;&#20687;&#32032;&#20013;&#30340;&#26368;&#36817;&#37051;&#34920;&#31034;&#12290;NN&#21305;&#37197;&#22312;&#36974;&#25377;&#12289;&#22806;&#35266;&#21464;&#21270;&#12289;&#20809;&#29031;&#21464;&#21270;&#21644;&#38750;&#21018;&#24615;&#21464;&#25442;&#31561;&#26041;&#38754;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;NN&#21305;&#37197;&#22312;&#39640;&#20998;&#36776;&#29575;&#25968;&#25454;&#21644;&#39640;&#32500;&#29305;&#24449;&#26041;&#38754;&#30340;&#25193;&#23637;&#24615;&#36739;&#24046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;NN&#30340;&#27169;&#26495;&#21305;&#37197;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26377;&#25928;&#22320;&#20943;&#23569;&#20102;NN&#35745;&#31639;&#37327;&#65292;&#24182;&#22312;NN&#22330;&#20013;&#24341;&#20837;&#28388;&#27874;&#20197;&#32771;&#34385;&#21464;&#24418;&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#21521;&#37327;&#37327;&#21270;&#23558;&#27169;&#26495;&#34920;&#31034;&#20026;k&#20010;&#29305;&#24449;&#65292;&#28982;&#21518;&#36890;&#36807;&#28388;&#27874;&#27604;&#36739;&#27169;&#26495;&#21644;&#26597;&#35810;&#22312;k&#20010;&#29305;&#24449;&#19978;&#30340;&#20998;&#24067;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Template matching is a fundamental problem in computer vision and has applications in various fields, such as object detection, image registration, and object tracking. The current state-of-the-art methods rely on nearest-neighbour (NN) matching in which the query feature space is converted to NN space by representing each query pixel with its NN in the template pixels. The NN-based methods have been shown to perform better in occlusions, changes in appearance, illumination variations, and non-rigid transformations. However, NN matching scales poorly with high-resolution data and high feature dimensions. In this work, we present an NN-based template-matching method which efficiently reduces the NN computations and introduces filtering in the NN fields to consider deformations. A vector quantization step first represents the template with $k$ features, then filtering compares the template and query distributions over the $k$ features. We show that state-of-the-art performance was achiev
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25512;&#33616;&#33539;&#24335;&#8212;&#8212;&#36890;&#36807;LLM&#36827;&#34892;&#25512;&#33616;&#65292;&#20294;&#30001;&#20110;LLMs&#21487;&#33021;&#23384;&#22312;&#31038;&#20250;&#20559;&#35265;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#35843;&#26597;RecLLM&#25152;&#20570;&#25512;&#33616;&#30340;&#20844;&#27491;&#24615;&#12290;&#20026;&#27492;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20844;&#24179;&#24615;&#22522;&#20934;&#8212;&#8212;FaiRLLM&#65292;&#24182;&#38024;&#23545;&#38899;&#20048;&#21644;&#30005;&#24433;&#25512;&#33616;&#22330;&#26223;&#20013;&#30340;&#20843;&#20010;&#25935;&#24863;&#23646;&#24615;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2305.07609</link><description>&lt;p&gt;
ChatGPT&#26159;&#21542;&#20844;&#24179;&#21487;&#38752;&#65311;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#33616;&#20013;&#30340;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Is ChatGPT Fair for Recommendation? Evaluating Fairness in Large Language Model Recommendation. (arXiv:2305.07609v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07609
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25512;&#33616;&#33539;&#24335;&#8212;&#8212;&#36890;&#36807;LLM&#36827;&#34892;&#25512;&#33616;&#65292;&#20294;&#30001;&#20110;LLMs&#21487;&#33021;&#23384;&#22312;&#31038;&#20250;&#20559;&#35265;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#35843;&#26597;RecLLM&#25152;&#20570;&#25512;&#33616;&#30340;&#20844;&#27491;&#24615;&#12290;&#20026;&#27492;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20844;&#24179;&#24615;&#22522;&#20934;&#8212;&#8212;FaiRLLM&#65292;&#24182;&#38024;&#23545;&#38899;&#20048;&#21644;&#30005;&#24433;&#25512;&#33616;&#22330;&#26223;&#20013;&#30340;&#20843;&#20010;&#25935;&#24863;&#23646;&#24615;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26174;&#30528;&#25104;&#23601;&#23548;&#33268;&#19968;&#31181;&#26032;&#30340;&#25512;&#33616;&#33539;&#24335;&#8212;&#8212;&#36890;&#36807;LLM&#36827;&#34892;&#25512;&#33616;&#65288;RecLLM&#65289;&#12290;&#28982;&#32780;&#65292;&#38656;&#35201;&#27880;&#24847;LLMs&#21487;&#33021;&#21253;&#21547;&#31038;&#20250;&#20559;&#35265;&#65292;&#22240;&#27492;&#38656;&#35201;&#36827;&#19968;&#27493;&#35843;&#26597;RecLLM&#25152;&#20570;&#25512;&#33616;&#30340;&#20844;&#27491;&#24615;&#12290;&#20026;&#20102;&#36991;&#20813;RecLLM&#30340;&#28508;&#22312;&#39118;&#38505;&#65292;&#26377;&#24517;&#35201;&#20174;&#29992;&#25143;&#30340;&#21508;&#31181;&#25935;&#24863;&#23646;&#24615;&#35282;&#24230;&#35780;&#20272;RecLLM&#30340;&#20844;&#24179;&#24615;&#12290;&#30001;&#20110;RecLLM&#33539;&#24335;&#19982;&#20256;&#32479;&#25512;&#33616;&#33539;&#24335;&#20043;&#38388;&#23384;&#22312;&#24046;&#24322;&#65292;&#22240;&#27492;&#30452;&#25509;&#20351;&#29992;&#20256;&#32479;&#25512;&#33616;&#30340;&#20844;&#24179;&#24615;&#22522;&#20934;&#26159;&#26377;&#38382;&#39064;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#22256;&#22659;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#31216;&#20026;&#8220;&#36890;&#36807;LLM&#30340;&#25512;&#33616;&#30340;&#20844;&#24179;&#24615;&#8221;&#65288;FaiRLLM&#65289;&#12290;&#35813;&#22522;&#20934;&#21253;&#25324;&#31934;&#24515;&#35774;&#35745;&#30340;&#25351;&#26631;&#21644;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20004;&#20010;&#25512;&#33616;&#22330;&#26223;&#20013;&#30340;&#20843;&#20010;&#25935;&#24863;&#23646;&#24615;&#65306;&#38899;&#20048;&#21644;&#30005;&#24433;&#12290;&#36890;&#36807;&#21033;&#29992;&#25105;&#20204;&#30340;FaiRLLM&#22522;&#20934;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
The remarkable achievements of Large Language Models (LLMs) have led to the emergence of a novel recommendation paradigm -- Recommendation via LLM (RecLLM). Nevertheless, it is important to note that LLMs may contain social prejudices, and therefore, the fairness of recommendations made by RecLLM requires further investigation. To avoid the potential risks of RecLLM, it is imperative to evaluate the fairness of RecLLM with respect to various sensitive attributes on the user side. Due to the differences between the RecLLM paradigm and the traditional recommendation paradigm, it is problematic to directly use the fairness benchmark of traditional recommendation. To address the dilemma, we propose a novel benchmark called Fairness of Recommendation via LLM (FaiRLLM). This benchmark comprises carefully crafted metrics and a dataset that accounts for eight sensitive attributes1 in two recommendation scenarios: music and movies. By utilizing our FaiRLLM benchmark, we conducted an evaluation 
&lt;/p&gt;</description></item><item><title>TALLRec&#26159;&#23545;LLMs&#36827;&#34892;&#35843;&#25972;&#30340;&#19968;&#31181;&#39640;&#25928;&#19988;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;LLMs&#19982;&#25512;&#33616;&#31995;&#32479;&#23545;&#40784;&#65292;&#20174;&#32780;&#22686;&#24378;LLMs&#22312;&#25512;&#33616;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.00447</link><description>&lt;p&gt;
TALLRec: &#19968;&#31181;&#19982;&#25512;&#33616;&#31995;&#32479;&#23545;&#40784;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26377;&#25928;&#19988;&#39640;&#25928;&#30340;&#35843;&#25972;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
TALLRec: An Effective and Efficient Tuning Framework to Align Large Language Model with Recommendation. (arXiv:2305.00447v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00447
&lt;/p&gt;
&lt;p&gt;
TALLRec&#26159;&#23545;LLMs&#36827;&#34892;&#35843;&#25972;&#30340;&#19968;&#31181;&#39640;&#25928;&#19988;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;LLMs&#19982;&#25512;&#33616;&#31995;&#32479;&#23545;&#40784;&#65292;&#20174;&#32780;&#22686;&#24378;LLMs&#22312;&#25512;&#33616;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#23637;&#29616;&#20102;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#26174;&#33879;&#24615;&#33021;&#65292;&#22240;&#27492;&#30740;&#31350;&#20154;&#21592;&#24320;&#22987;&#25506;&#32034;&#23427;&#20204;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#28508;&#21147;&#12290;&#34429;&#28982;&#21021;&#22987;&#30340;&#23581;&#35797;&#24050;&#32463;&#21033;&#29992;&#20102;LLMs&#30340;&#20248;&#24322;&#33021;&#21147;&#65292;&#27604;&#22914;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#25552;&#31034;&#35789;&#26469;&#20016;&#23500;&#30693;&#35782;&#24182;&#36827;&#34892;&#24378;&#21270;&#27867;&#21270;&#65292;&#20294;&#26159;&#30001;&#20110;LLMs&#30340;&#35757;&#32451;&#20219;&#21153;&#19982;&#25512;&#33616;&#20219;&#21153;&#20043;&#38388;&#30340;&#24040;&#22823;&#24046;&#24322;&#20197;&#21450;&#39044;&#35757;&#32451;&#26399;&#38388;&#30340;&#19981;&#36275;&#30340;&#25512;&#33616;&#25968;&#25454;&#65292;LLMs&#22312;&#25512;&#33616;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#20173;&#28982;&#19981;&#29702;&#24819;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#32771;&#34385;&#20351;&#29992;&#25512;&#33616;&#25968;&#25454;&#23545;LLMs&#36827;&#34892;&#35843;&#25972;&#26469;&#26500;&#24314;&#22823;&#22411;&#25512;&#33616;&#35821;&#35328;&#27169;&#22411;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TALLRec&#30340;&#39640;&#25928;&#19988;&#26377;&#25928;&#30340;&#35843;&#25972;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;LLMs&#19982;&#25512;&#33616;&#31995;&#32479;&#23545;&#40784;&#12290;&#25105;&#20204;&#24050;&#32463;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;TALLRec&#26694;&#26550;&#21487;&#20197;&#26174;&#33879;&#22686;&#24378;LLMs&#22312;&#25512;&#33616;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated remarkable performance across diverse domains, thereby prompting researchers to explore their potential for use in recommendation systems. Initial attempts have leveraged the exceptional capabilities of LLMs, such as rich knowledge and strong generalization through In-context Learning, which involves phrasing the recommendation task as prompts. Nevertheless, the performance of LLMs in recommendation tasks remains suboptimal due to a substantial disparity between the training tasks for LLMs and recommendation tasks, as well as inadequate recommendation data during pre-training. To bridge the gap, we consider building a Large Recommendation Language Model by tunning LLMs with recommendation data. To this end, we propose an efficient and effective Tuning framework for Aligning LLMs with Recommendation, namely TALLRec. We have demonstrated that the proposed TALLRec framework can significantly enhance the recommendation capabilities of LLMs in 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27010;&#24565;&#20540;&#29942;&#39048;&#27169;&#22411;LACE&#65292;&#29992;&#20110;&#21487;&#25511;&#25991;&#26412;&#25512;&#33616;&#12290;&#35813;&#27169;&#22411;&#22522;&#20110;&#29992;&#25143;&#25991;&#26723;&#23398;&#20064;&#20010;&#24615;&#21270;&#30340;&#27010;&#24565;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#22810;&#31181;&#20132;&#20114;&#26041;&#24335;&#20026;&#29992;&#25143;&#25552;&#20379;&#20102;&#25511;&#21046;&#25512;&#33616;&#30340;&#26426;&#21046;&#65292;&#39564;&#35777;&#20102;&#22312;&#31163;&#32447;&#21644;&#22312;&#32447;&#23454;&#39564;&#20013;&#35813;&#27169;&#22411;&#30340;&#25512;&#33616;&#36136;&#37327;&#21644;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.04250</link><description>&lt;p&gt;
&#21487;&#32534;&#36753;&#29992;&#25143;&#26723;&#26696;&#30340;&#21487;&#25511;&#25991;&#26412;&#25512;&#33616;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Editable User Profiles for Controllable Text Recommendation. (arXiv:2304.04250v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04250
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27010;&#24565;&#20540;&#29942;&#39048;&#27169;&#22411;LACE&#65292;&#29992;&#20110;&#21487;&#25511;&#25991;&#26412;&#25512;&#33616;&#12290;&#35813;&#27169;&#22411;&#22522;&#20110;&#29992;&#25143;&#25991;&#26723;&#23398;&#20064;&#20010;&#24615;&#21270;&#30340;&#27010;&#24565;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#22810;&#31181;&#20132;&#20114;&#26041;&#24335;&#20026;&#29992;&#25143;&#25552;&#20379;&#20102;&#25511;&#21046;&#25512;&#33616;&#30340;&#26426;&#21046;&#65292;&#39564;&#35777;&#20102;&#22312;&#31163;&#32447;&#21644;&#22312;&#32447;&#23454;&#39564;&#20013;&#35813;&#27169;&#22411;&#30340;&#25512;&#33616;&#36136;&#37327;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#29616;&#39640;&#36136;&#37327;&#25512;&#33616;&#30340;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#20174;&#20132;&#20114;&#25968;&#25454;&#20013;&#23398;&#20064;&#28508;&#22312;&#34920;&#31034;&#12290;&#28982;&#32780;&#36825;&#20123;&#26041;&#27861;&#27809;&#26377;&#25552;&#20379;&#32473;&#29992;&#25143;&#25511;&#21046;&#25152;&#25509;&#25910;&#30340;&#25512;&#33616;&#30340;&#26426;&#21046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;LACE&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#27010;&#24565;&#20540;&#29942;&#39048;&#27169;&#22411;&#65292;&#29992;&#20110;&#21487;&#25511;&#25991;&#26412;&#25512;&#33616;&#12290;LACE&#22522;&#20110;&#29992;&#25143;&#20132;&#20114;&#30340;&#25991;&#26723;&#26816;&#32034;&#65292;&#23558;&#27599;&#20010;&#29992;&#25143;&#34920;&#31034;&#20026;&#31616;&#27905;&#30340;&#21487;&#35835;&#30340;&#27010;&#24565;&#38598;&#65292;&#24182;&#22522;&#20110;&#29992;&#25143;&#25991;&#26723;&#23398;&#20064;&#27010;&#24565;&#30340;&#20010;&#24615;&#21270;&#34920;&#31034;&#12290;&#35813;&#22522;&#20110;&#27010;&#24565;&#30340;&#29992;&#25143;&#26723;&#26696;&#34987;&#21033;&#29992;&#26469;&#20570;&#20986;&#25512;&#33616;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#35774;&#35745;&#36890;&#36807;&#36879;&#26126;&#30340;&#29992;&#25143;&#26723;&#26696;&#65292;&#25552;&#20379;&#20102;&#25511;&#21046;&#25512;&#33616;&#30340;&#22810;&#31181;&#30452;&#35266;&#20132;&#20114;&#26041;&#24335;&#12290;&#25105;&#20204;&#39318;&#20808;&#22312;&#19977;&#20010;&#25512;&#33616;&#20219;&#21153;&#65288;&#28201;&#21551;&#21160;&#12289;&#20919;&#21551;&#21160;&#21644;&#38646;&#26679;&#26412;&#65289;&#30340;&#20845;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#31163;&#32447;&#35780;&#20272;&#65292;&#39564;&#35777;&#20102;&#20174;LACE&#33719;&#24471;&#30340;&#25512;&#33616;&#36136;&#37327;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#22312;&#22312;&#32447;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;LACE&#30340;&#26377;&#25928;&#24615;&#21644;&#29992;&#25143;&#25511;&#21046;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Methods for making high-quality recommendations often rely on learning latent representations from interaction data. These methods, while performant, do not provide ready mechanisms for users to control the recommendation they receive. Our work tackles this problem by proposing LACE, a novel concept value bottleneck model for controllable text recommendations. LACE represents each user with a succinct set of human-readable concepts through retrieval given user-interacted documents and learns personalized representations of the concepts based on user documents. This concept based user profile is then leveraged to make recommendations. The design of our model affords control over the recommendations through a number of intuitive interactions with a transparent user profile. We first establish the quality of recommendations obtained from LACE in an offline evaluation on three recommendation tasks spanning six datasets in warm-start, cold-start, and zero-shot setups. Next, we validate the 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#20498;&#25490;&#32034;&#24341;(HI$^2$)&#29992;&#20110;&#21152;&#36895;&#31264;&#23494;&#26816;&#32034;&#65292;&#36890;&#36807;&#23884;&#20837;&#32858;&#31867;&#21644;&#26174;&#33879;&#35789;&#27719;&#30340;&#21327;&#21516;&#20316;&#29992;&#65292;&#26500;&#24314;&#32039;&#20945;&#30340;&#20498;&#25490;&#21015;&#34920;&#24182;&#25552;&#39640;&#26816;&#32034;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2210.05521</link><description>&lt;p&gt;
&#28151;&#21512;&#20498;&#25490;&#32034;&#24341;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#31264;&#23494;&#26816;&#32034;&#21152;&#36895;&#22120;
&lt;/p&gt;
&lt;p&gt;
Hybrid Inverted Index Is a Robust Accelerator for Dense Retrieval. (arXiv:2210.05521v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.05521
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#20498;&#25490;&#32034;&#24341;(HI$^2$)&#29992;&#20110;&#21152;&#36895;&#31264;&#23494;&#26816;&#32034;&#65292;&#36890;&#36807;&#23884;&#20837;&#32858;&#31867;&#21644;&#26174;&#33879;&#35789;&#27719;&#30340;&#21327;&#21516;&#20316;&#29992;&#65292;&#26500;&#24314;&#32039;&#20945;&#30340;&#20498;&#25490;&#21015;&#34920;&#24182;&#25552;&#39640;&#26816;&#32034;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20498;&#25490;&#25991;&#20214;&#32467;&#26500;&#26159;&#19968;&#31181;&#24120;&#29992;&#30340;&#21152;&#36895;&#31264;&#23494;&#26816;&#32034;&#30340;&#25216;&#26415;&#12290;&#23427;&#26681;&#25454;&#23884;&#20837;&#23558;&#25991;&#26723;&#32858;&#31867;&#65307;&#22312;&#25628;&#32034;&#36807;&#31243;&#20013;&#65292;&#26681;&#25454;&#36755;&#20837;&#26597;&#35810;&#25506;&#27979;&#38468;&#36817;&#30340;&#32858;&#31867;&#65292;&#24182;&#19988;&#20165;&#23545;&#20854;&#20013;&#30340;&#25991;&#26723;&#36827;&#34892;&#21518;&#32493;&#30340;&#35299;&#30721;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#31351;&#20030;&#36941;&#21382;&#30340;&#26114;&#36149;&#20195;&#20215;&#12290;&#28982;&#32780;&#65292;&#32858;&#31867;&#36807;&#31243;&#24635;&#26159;&#26377;&#25439;&#30340;&#65292;&#36825;&#23548;&#33268;&#25506;&#27979;&#21040;&#30340;&#32858;&#31867;&#20013;&#32570;&#22833;&#20102;&#30456;&#20851;&#30340;&#25991;&#26723;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#26816;&#32034;&#36136;&#37327;&#12290;&#30456;&#21453;&#65292;&#35789;&#27719;&#21305;&#37197;&#65292;&#22914;&#26174;&#33879;&#35789;&#27719;&#30340;&#37325;&#21472;&#65292;&#26356;&#23481;&#26131;&#35782;&#21035;&#30456;&#20851;&#25991;&#26723;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#28151;&#21512;&#20498;&#25490;&#32034;&#24341; (HI$^2$)&#65292;&#20854;&#20013;&#23884;&#20837;&#32858;&#31867;&#21644;&#26174;&#33879;&#35789;&#27719;&#20849;&#21516;&#21152;&#36895;&#31264;&#23494;&#26816;&#32034;&#12290;&#20026;&#20102;&#20860;&#39038;&#25928;&#26524;&#21644;&#25928;&#29575;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#32858;&#31867;&#36873;&#25321;&#22120;&#21644;&#19968;&#20010;&#35789;&#27719;&#36873;&#25321;&#22120;&#65292;&#29992;&#20110;&#26500;&#24314;&#32039;&#20945;&#30340;&#20498;&#25490;&#21015;&#34920;&#24182;&#24555;&#36895;&#25628;&#32034;&#23427;&#20204;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#31616;&#21333;&#30340;&#26080;&#30417;&#30563;&#31639;&#27861;&#21644;&#31471;&#21040;&#31471;&#23398;&#20064;&#26469;&#25552;&#39640;&#32034;&#24341;&#36136;&#37327;.
&lt;/p&gt;
&lt;p&gt;
Inverted file structure is a common technique for accelerating dense retrieval. It clusters documents based on their embeddings; during searching, it probes nearby clusters w.r.t. an input query and only evaluates documents within them by subsequent codecs, thus avoiding the expensive cost of exhaustive traversal. However, the clustering is always lossy, which results in the miss of relevant documents in the probed clusters and hence degrades retrieval quality. In contrast, lexical matching, such as overlaps of salient terms, tends to be strong feature for identifying relevant documents. In this work, we present the Hybrid Inverted Index (HI$^2$), where the embedding clusters and salient terms work collaboratively to accelerate dense retrieval. To make best of both effectiveness and efficiency, we devise a cluster selector and a term selector, to construct compact inverted lists and efficiently searching through them. Moreover, we leverage simple unsupervised algorithms as well as end-
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20851;&#27880;&#25490;&#21517;&#26381;&#21153;&#20013;&#30340;&#26333;&#20809;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#29616;&#26377;&#20844;&#24179;&#24615;&#20248;&#21270;&#26041;&#27861;&#22312;&#20844;&#24179;&#24615;&#19982;&#30456;&#20851;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;Vertic&#26469;&#35299;&#20915;&#35813;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2204.03046</link><description>&lt;p&gt;
&#22522;&#20110;&#22402;&#30452;&#20998;&#37197;&#30340;&#25490;&#21517;&#20013;&#20844;&#24179;&#26333;&#20809;&#25674;&#38144;
&lt;/p&gt;
&lt;p&gt;
Vertical Allocation-based Fair Exposure Amortizing in Ranking. (arXiv:2204.03046v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.03046
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20851;&#27880;&#25490;&#21517;&#26381;&#21153;&#20013;&#30340;&#26333;&#20809;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#29616;&#26377;&#20844;&#24179;&#24615;&#20248;&#21270;&#26041;&#27861;&#22312;&#20844;&#24179;&#24615;&#19982;&#30456;&#20851;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;Vertic&#26469;&#35299;&#20915;&#35813;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#26524;&#25490;&#21517;&#32463;&#24120;&#24433;&#21709;&#28040;&#36153;&#32773;&#28385;&#24847;&#24230;&#20197;&#21450;&#25490;&#21517;&#26381;&#21153;&#20013;&#27599;&#20010;&#39033;&#30446;&#30340;&#26333;&#20809;&#37327;&#12290;&#20165;&#26681;&#25454;&#30456;&#20851;&#24615;&#23545;&#39033;&#30446;&#36827;&#34892;&#25490;&#21517;&#20250;&#23548;&#33268;&#39033;&#30446;&#26333;&#20809;&#20998;&#37197;&#19981;&#20844;&#24179;&#65292;&#20174;&#32780;&#20026;&#39033;&#30446;&#29983;&#20135;&#32773;/&#25552;&#20379;&#32773;&#24102;&#26469;&#19981;&#20844;&#24179;&#30340;&#26426;&#20250;&#21644;&#32463;&#27982;&#25910;&#30410;&#12290;&#36825;&#31181;&#19981;&#20844;&#24179;&#20250;&#23548;&#33268;&#25552;&#20379;&#32773;&#31163;&#24320;&#31995;&#32479;&#65292;&#24182;&#38459;&#27490;&#26032;&#30340;&#25552;&#20379;&#32773;&#21152;&#20837;&#12290;&#26368;&#32456;&#65292;&#28040;&#36153;&#32773;&#20250;&#21097;&#19979;&#26356;&#23569;&#30340;&#36141;&#20080;&#36873;&#39033;&#65292;&#28040;&#36153;&#32773;&#21644;&#25552;&#20379;&#32773;&#30340;&#25928;&#29992;&#37117;&#20250;&#21463;&#21040;&#25439;&#23475;&#12290;&#22240;&#27492;&#65292;&#23545;&#20110;&#21452;&#26041;&#26469;&#35828;&#65292;&#20445;&#25345;&#25490;&#21517;&#30456;&#20851;&#24615;&#21644;&#20844;&#24179;&#20043;&#38388;&#30340;&#24179;&#34913;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#32858;&#28966;&#20110;&#25490;&#21517;&#26381;&#21153;&#20013;&#30340;&#26333;&#20809;&#20844;&#24179;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#29616;&#26377;&#30340;&#20844;&#24179;&#24615;&#20248;&#21270;&#26041;&#27861;&#22312;&#20844;&#24179;&#24615;&#21644;&#30456;&#20851;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#26041;&#38754;&#21487;&#33021;&#19981;&#26159;&#26368;&#20248;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#28040;&#36153;&#32773;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Vertic&#30340;&#26032;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Result ranking often affects consumer satisfaction as well as the amount of exposure each item receives in the ranking services. Myopically maximizing customer satisfaction by ranking items only according to relevance will lead to unfair distribution of exposure for items, followed by unfair opportunities and economic gains for item producers/providers. Such unfairness will force providers to leave the system and discourage new providers from coming in. Eventually, fewer purchase options would be left for consumers, and the utilities of both consumers and providers would be harmed. Thus, to maintain a balance between ranking relevance and fairness is crucial for both parties. In this paper, we focus on the exposure fairness in ranking services. We demonstrate that existing methods for amortized fairness optimization could be suboptimal in terms of fairness-relevance tradeoff because they fail to utilize the prior knowledge of consumers. We further propose a novel algorithm named Vertic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#19968;&#33268;&#24615;&#26041;&#27861;&#26469;&#35299;&#20915;&#30697;&#38453;&#21644;&#24352;&#37327;&#34917;&#20840;&#38382;&#39064;&#65292;&#22312;&#25512;&#33616;&#31995;&#32479;&#24212;&#29992;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36890;&#36807;&#20445;&#30041;&#21333;&#20301;&#27604;&#20363;&#21644;&#19968;&#33268;&#24615;&#20004;&#20010;&#32422;&#26463;&#26465;&#20214;&#21487;&#20197;&#23454;&#29616;&#35299;&#30340;&#23384;&#22312;&#24615;&#19982;&#21807;&#19968;&#24615;&#12290;</title><link>http://arxiv.org/abs/2204.01815</link><description>&lt;p&gt;
&#20855;&#26377;&#21487;&#35777;&#26126;&#30340;&#19968;&#33268;&#24615;&#21644;&#20844;&#24179;&#20445;&#35777;&#30340;&#25512;&#33616;&#31995;&#32479;&#24352;&#37327;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
Tensor Completion with Provable Consistency and Fairness Guarantees for Recommender Systems. (arXiv:2204.01815v3 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.01815
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#19968;&#33268;&#24615;&#26041;&#27861;&#26469;&#35299;&#20915;&#30697;&#38453;&#21644;&#24352;&#37327;&#34917;&#20840;&#38382;&#39064;&#65292;&#22312;&#25512;&#33616;&#31995;&#32479;&#24212;&#29992;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36890;&#36807;&#20445;&#30041;&#21333;&#20301;&#27604;&#20363;&#21644;&#19968;&#33268;&#24615;&#20004;&#20010;&#32422;&#26463;&#26465;&#20214;&#21487;&#20197;&#23454;&#29616;&#35299;&#30340;&#23384;&#22312;&#24615;&#19982;&#21807;&#19968;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#19968;&#33268;&#24615;&#30340;&#26041;&#27861;&#26469;&#23450;&#20041;&#21644;&#35299;&#20915;&#38750;&#36127;/&#27491;&#30697;&#38453;&#21644;&#24352;&#37327;&#34917;&#20840;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#30340;&#26032;&#39062;&#20043;&#22788;&#22312;&#20110;&#65292;&#25105;&#20204;&#19981;&#26159;&#20154;&#20026;&#22320;&#23558;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#20219;&#24847;&#20248;&#21270;&#38382;&#39064;&#65292;&#20363;&#22914;&#65292;&#26368;&#23567;&#21270;&#19968;&#20010;&#32467;&#26500;&#37327;&#65292;&#22914;&#31209;&#25110;&#33539;&#25968;&#65292;&#32780;&#26159;&#23637;&#31034;&#20102;&#19968;&#20010;&#21333;&#19968;&#30340;&#23646;&#24615;/&#32422;&#26463;&#65306;&#20445;&#30041;&#21333;&#20301;&#27604;&#20363;&#19968;&#33268;&#24615;&#65292;&#20445;&#35777;&#20102;&#35299;&#30340;&#23384;&#22312;&#65292;&#24182;&#22312;&#30456;&#23545;&#36739;&#24369;&#30340;&#25903;&#25345;&#20551;&#35774;&#19979;&#20445;&#35777;&#20102;&#35299;&#30340;&#21807;&#19968;&#24615;&#12290;&#35813;&#26694;&#26550;&#21644;&#35299;&#31639;&#27861;&#20063;&#30452;&#25509;&#25512;&#24191;&#21040;&#20219;&#24847;&#32500;&#24230;&#30340;&#24352;&#37327;&#20013;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#22266;&#23450;&#32500;&#24230; d &#30340;&#38382;&#39064;&#35268;&#27169;&#30340;&#32447;&#24615;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;&#22312;&#25512;&#33616;&#31995;&#32479;&#24212;&#29992;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20004;&#20010;&#21512;&#29702;&#30340;&#24615;&#36136;&#65292;&#36825;&#20123;&#24615;&#36136;&#24212;&#35813;&#36866;&#29992;&#20110;&#20219;&#20309; RS &#38382;&#39064;&#30340;&#35299;&#65292;&#36275;&#20197;&#20801;&#35768;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20869;&#24314;&#31435;&#21807;&#19968;&#24615;&#20445;&#35777;&#12290;&#20851;&#38190;&#29702;&#35770;&#36129;&#29486;&#26159;&#23637;&#31034;&#20102;&#36825;&#20123;&#32422;&#26463;&#19979;&#35299;&#30340;&#23384;&#22312;&#24615;&#19982;&#21807;&#19968;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a new consistency-based approach for defining and solving nonnegative/positive matrix and tensor completion problems. The novelty of the framework is that instead of artificially making the problem well-posed in the form of an application-arbitrary optimization problem, e.g., minimizing a bulk structural measure such as rank or norm, we show that a single property/constraint: preserving unit-scale consistency, guarantees the existence of both a solution and, under relatively weak support assumptions, uniqueness. The framework and solution algorithms also generalize directly to tensors of arbitrary dimensions while maintaining computational complexity that is linear in problem size for fixed dimension d. In the context of recommender system (RS) applications, we prove that two reasonable properties that should be expected to hold for any solution to the RS problem are sufficient to permit uniqueness guarantees to be established within our framework. Key theoretical contribu
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#25512;&#29305;&#19978;&#26816;&#27979;&#40657;&#20154;&#29983;&#21629;&#33267;&#20851;&#37325;&#35201;&#35805;&#39064;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;BLM-17m&#65292;&#28085;&#30422;&#20102;&#20052;&#27835;&#183;&#24343;&#27931;&#20234;&#24503;&#20107;&#20214;&#26399;&#38388;&#30340;17&#30334;&#19975;&#25512;&#25991;&#12290;&#20316;&#32773;&#25552;&#20379;&#20102;&#20004;&#20010;&#22522;&#32447;&#27169;&#22411;TF-IDF&#21644;LDA&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2105.01331</link><description>&lt;p&gt;
BLM-17m: &#19968;&#20010;&#29992;&#20110;&#25512;&#29305;&#19978;&#40657;&#20154;&#29983;&#21629;&#33267;&#20851;&#37325;&#35201;&#35805;&#39064;&#26816;&#27979;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
BLM-17m: A Large-Scale Dataset for Black Lives Matter Topic Detection on Twitter. (arXiv:2105.01331v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2105.01331
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#25512;&#29305;&#19978;&#26816;&#27979;&#40657;&#20154;&#29983;&#21629;&#33267;&#20851;&#37325;&#35201;&#35805;&#39064;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;BLM-17m&#65292;&#28085;&#30422;&#20102;&#20052;&#27835;&#183;&#24343;&#27931;&#20234;&#24503;&#20107;&#20214;&#26399;&#38388;&#30340;17&#30334;&#19975;&#25512;&#25991;&#12290;&#20316;&#32773;&#25552;&#20379;&#20102;&#20004;&#20010;&#22522;&#32447;&#27169;&#22411;TF-IDF&#21644;LDA&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#26435;&#20445;&#25252;&#26159;&#19990;&#30028;&#19978;&#26368;&#37325;&#35201;&#30340;&#38382;&#39064;&#20043;&#19968;&#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#20379;&#19968;&#20010;&#28085;&#30422;&#26368;&#36817;&#20960;&#20010;&#26376;&#20840;&#29699;&#24433;&#21709;&#28145;&#36828;&#30340;&#20154;&#26435;&#30683;&#30462;&#20043;&#19968;&#8212;&#8212;&#20052;&#27835;&#183;&#24343;&#27931;&#20234;&#24503;&#20107;&#20214;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24102;&#26377;17&#30334;&#19975;&#25512;&#25991;&#30340;&#20027;&#39064;&#26816;&#27979;&#26631;&#35760;&#25968;&#25454;&#38598;&#12290;&#36825;&#20123;&#25512;&#25991;&#26159;&#20174;2020&#24180;5&#26376;25&#26085;&#33267;2020&#24180;8&#26376;21&#26085;&#25910;&#38598;&#30340;&#65292;&#28085;&#30422;&#20102;&#36825;&#19968;&#20107;&#20214;&#24320;&#22987;&#21518;&#30340;89&#22825;&#12290;&#25105;&#20204;&#36890;&#36807;&#30417;&#27979;&#20840;&#29699;&#21644;&#26412;&#22320;&#25253;&#32440;&#30340;&#26368;&#28909;&#38376;&#26032;&#38395;&#20027;&#39064;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#26631;&#35760;&#12290;&#38500;&#27492;&#20043;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#20004;&#20010;&#22522;&#32447;&#27169;&#22411;&#65292;TF-IDF&#21644;LDA&#12290;&#25105;&#20204;&#20351;&#29992;&#19977;&#20010;&#19981;&#21516;&#30340;k&#20540;&#23545;&#36825;&#20004;&#31181;&#26041;&#27861;&#30340;&#31934;&#30830;&#24230;&#12289;&#21484;&#22238;&#29575;&#21644;F1&#20998;&#25968;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#25910;&#38598;&#21040;&#30340;&#25968;&#25454;&#38598;&#21487;&#20197;&#22312;https://github.com/MeysamAsgariC/BLMT &#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Protection of human rights is one of the most important problems of our world. In this paper, our aim is to provide a dataset which covers one of the most significant human rights contradiction in recent months affected the whole world, George Floyd incident. We propose a labeled dataset for topic detection that contains 17 million tweets. These Tweets are collected from 25 May 2020 to 21 August 2020 that covers 89 days from start of this incident. We labeled the dataset by monitoring most trending news topics from global and local newspapers. Apart from that, we present two baselines, TF-IDF and LDA. We evaluated the results of these two methods with three different k values for metrics of precision, recall and f1-score. The collected dataset is available at https://github.com/MeysamAsgariC/BLMT.
&lt;/p&gt;</description></item></channel></rss>