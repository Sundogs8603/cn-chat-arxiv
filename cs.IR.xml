<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#35774;&#35745;&#20102;&#26032;&#39062;&#30340;&#25552;&#31034;&#31574;&#30053;&#21644;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#20197;&#25506;&#32034;LLMs&#29992;&#20110;&#19979;&#19968;&#20010;POI&#25512;&#33616;&#30340;&#38646;&#26679;&#26412;&#27867;&#21270;&#33021;&#21147;</title><link>https://arxiv.org/abs/2404.01855</link><description>&lt;p&gt;
&#19979;&#19968;&#20010;&#21435;&#21738;&#37324;&#65306;&#22522;&#20110;&#38646;&#26679;&#26412;&#27867;&#21270;&#30340;LLMs&#29992;&#20110;&#19979;&#19968;&#20010;POI&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Where to Move Next: Zero-shot Generalization of LLMs for Next POI Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01855
&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#20102;&#26032;&#39062;&#30340;&#25552;&#31034;&#31574;&#30053;&#21644;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#20197;&#25506;&#32034;LLMs&#29992;&#20110;&#19979;&#19968;&#20010;POI&#25512;&#33616;&#30340;&#38646;&#26679;&#26412;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19979;&#19968;&#20010;&#20852;&#36259;&#28857;&#65288;POI&#65289;&#25512;&#33616;&#20026;&#29992;&#25143;&#25552;&#20379;&#20102;&#25506;&#32034;&#21608;&#36793;&#29615;&#22659;&#30340;&#23453;&#36149;&#24314;&#35758;&#12290;&#29616;&#26377;&#30740;&#31350;&#20381;&#36182;&#20110;&#20174;&#22823;&#35268;&#27169;&#29992;&#25143;&#31614;&#21040;&#25968;&#25454;&#26500;&#24314;&#25512;&#33616;&#27169;&#22411;&#65292;&#36825;&#26159;&#20219;&#21153;&#29305;&#23450;&#30340;&#65292;&#24182;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;&#26368;&#36817;&#65292;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;NLP&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#24182;&#19988;&#24050;&#32463;&#34987;&#30740;&#31350;&#29992;&#20110;&#25512;&#33616;&#22330;&#26223;&#12290;&#28982;&#32780;&#65292;LLMs&#30340;&#27867;&#21270;&#33021;&#21147;&#22312;&#35299;&#20915;&#19979;&#19968;&#20010;POI&#25512;&#33616;&#38382;&#39064;&#26102;&#20173;&#26410;&#34987;&#25506;&#32034;&#65292;&#20854;&#20013;&#24212;&#25552;&#21462;&#29992;&#25143;&#30340;&#22320;&#29702;&#31227;&#21160;&#27169;&#24335;&#12290;&#34429;&#28982;&#26377;&#30740;&#31350;&#21033;&#29992;LLMs&#36827;&#34892;&#19979;&#19968;&#20010;&#39033;&#30446;&#25512;&#33616;&#65292;&#20294;&#23427;&#20204;&#26410;&#33021;&#32771;&#34385;&#22320;&#29702;&#24433;&#21709;&#21644;&#39034;&#24207;&#36716;&#25442;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#26080;&#27861;&#26377;&#25928;&#35299;&#20915;&#19979;&#19968;&#20010;POI&#25512;&#33616;&#20219;&#21153;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#26032;&#39062;&#30340;&#25552;&#31034;&#31574;&#30053;&#65292;&#24182;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#20197;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01855v1 Announce Type: cross  Abstract: Next Point-of-interest (POI) recommendation provides valuable suggestions for users to explore their surrounding environment. Existing studies rely on building recommendation models from large-scale users' check-in data, which is task-specific and needs extensive computational resources. Recently, the pretrained large language models (LLMs) have achieved significant advancements in various NLP tasks and have also been investigated for recommendation scenarios. However, the generalization abilities of LLMs still are unexplored to address the next POI recommendations, where users' geographical movement patterns should be extracted. Although there are studies that leverage LLMs for next-item recommendations, they fail to consider the geographical influence and sequential transitions. Hence, they cannot effectively solve the next POI recommendation task. To this end, we design novel prompting strategies and conduct empirical studies to ass
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#38598;&#25104;&#20102;&#22788;&#29702;&#27169;&#22359;&#21644;&#20998;&#26512;&#27169;&#22359;&#30340;&#25968;&#25454;&#22788;&#29702;&#26694;&#26550;&#65292;&#26088;&#22312;&#25913;&#21892;&#25968;&#25454;&#36136;&#37327;&#24182;&#23637;&#31034;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.16358</link><description>&lt;p&gt;
&#19968;&#20010;&#25972;&#21512;&#30340;&#25968;&#25454;&#22788;&#29702;&#26694;&#26550;&#29992;&#20110;&#39044;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
An Integrated Data Processing Framework for Pretraining Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16358
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#38598;&#25104;&#20102;&#22788;&#29702;&#27169;&#22359;&#21644;&#20998;&#26512;&#27169;&#22359;&#30340;&#25968;&#25454;&#22788;&#29702;&#26694;&#26550;&#65292;&#26088;&#22312;&#25913;&#21892;&#25968;&#25454;&#36136;&#37327;&#24182;&#23637;&#31034;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#30340;&#33021;&#21147;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#22823;&#35268;&#27169;&#12289;&#22810;&#26679;&#21270;&#21644;&#39640;&#36136;&#37327;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#12290;&#20026;&#20102;&#25552;&#39640;&#25968;&#25454;&#36136;&#37327;&#65292;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#32463;&#24120;&#38656;&#35201;&#25163;&#21160;&#20174;&#19981;&#21516;&#26469;&#28304;&#31574;&#21010;&#25968;&#25454;&#38598;&#65292;&#24182;&#20026;&#27599;&#20010;&#25968;&#25454;&#23384;&#20648;&#24211;&#24320;&#21457;&#19987;&#38376;&#30340;&#25968;&#25454;&#28165;&#27927;&#27969;&#31243;&#12290;&#32570;&#20047;&#32479;&#19968;&#30340;&#25968;&#25454;&#22788;&#29702;&#26694;&#26550;&#65292;&#36825;&#19968;&#36807;&#31243;&#37325;&#22797;&#32780;&#32321;&#29712;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38598;&#25104;&#20102;&#22788;&#29702;&#27169;&#22359;&#21644;&#20998;&#26512;&#27169;&#22359;&#30340;&#25968;&#25454;&#22788;&#29702;&#26694;&#26550;&#65292;&#22788;&#29702;&#27169;&#22359;&#21253;&#25324;&#19968;&#31995;&#21015;&#19981;&#21516;&#31890;&#24230;&#27700;&#24179;&#30340;&#25805;&#20316;&#31526;&#65292;&#32780;&#20998;&#26512;&#27169;&#22359;&#25903;&#25345;&#23545;&#31934;&#28860;&#25968;&#25454;&#36827;&#34892;&#25506;&#26597;&#21644;&#35780;&#20272;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#26131;&#20110;&#20351;&#29992;&#19988;&#39640;&#24230;&#28789;&#27963;&#12290;&#22312;&#36825;&#31687;&#28436;&#31034;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#22914;&#20309;&#20351;&#29992;&#36825;&#20010;&#26694;&#26550;&#24182;&#23637;&#31034;&#23427;&#22312;&#25913;&#21892;&#25968;&#25454;&#36136;&#37327;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#36890;&#36807;&#19982;ChatGPT&#30340;&#33258;&#21160;&#35780;&#20272;&#21644;&#31471;&#21040;&#31471;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16358v1 Announce Type: cross  Abstract: The ability of the foundation models heavily relies on large-scale, diverse, and high-quality pretraining data. In order to improve data quality, researchers and practitioners often have to manually curate datasets from difference sources and develop dedicated data cleansing pipeline for each data repository. Lacking a unified data processing framework, this process is repetitive and cumbersome. To mitigate this issue, we propose a data processing framework that integrates a Processing Module which consists of a series of operators at different granularity levels, and an Analyzing Module which supports probing and evaluation of the refined data. The proposed framework is easy to use and highly flexible. In this demo paper, we first introduce how to use this framework with some example use cases and then demonstrate its effectiveness in improving the data quality with an automated evaluation with ChatGPT and an end-to-end evaluation in 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#37096;&#20998;&#30456;&#20851;&#24615;&#22686;&#24378;&#65292;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#25913;&#36827;&#35270;&#39057;&#35821;&#26009;&#24211;&#26102;&#21051;&#26816;&#32034;&#30340;&#26041;&#27861;&#65292;&#20197;&#25429;&#25417;&#20851;&#38190;&#20869;&#23481;&#21644;&#22788;&#29702;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#24046;&#24322;&#12290;</title><link>https://arxiv.org/abs/2402.13576</link><description>&lt;p&gt;
&#36890;&#36807;&#37096;&#20998;&#30456;&#20851;&#24615;&#22686;&#24378;&#25913;&#36827;&#35270;&#39057;&#35821;&#26009;&#24211;&#26102;&#21051;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Improving Video Corpus Moment Retrieval with Partial Relevance Enhancement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13576
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#37096;&#20998;&#30456;&#20851;&#24615;&#22686;&#24378;&#65292;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#25913;&#36827;&#35270;&#39057;&#35821;&#26009;&#24211;&#26102;&#21051;&#26816;&#32034;&#30340;&#26041;&#27861;&#65292;&#20197;&#25429;&#25417;&#20851;&#38190;&#20869;&#23481;&#21644;&#22788;&#29702;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#35821;&#26009;&#24211;&#26102;&#21051;&#26816;&#32034;&#65288;VCMR&#65289;&#26159;&#19968;&#20010;&#26032;&#30340;&#35270;&#39057;&#26816;&#32034;&#20219;&#21153;&#65292;&#26088;&#22312;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#20316;&#20026;&#26597;&#35810;&#65292;&#20174;&#22823;&#37327;&#26410;&#32463;&#20462;&#21098;&#30340;&#35270;&#39057;&#35821;&#26009;&#24211;&#20013;&#26816;&#32034;&#30456;&#20851;&#26102;&#21051;&#12290;&#35270;&#39057;&#19982;&#26597;&#35810;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#26159;&#37096;&#20998;&#30340;&#65292;&#20027;&#35201;&#20307;&#29616;&#22312;&#20004;&#20010;&#26041;&#38754;&#65306;&#65288;1&#65289;&#33539;&#22260;&#65306;&#26410;&#32463;&#20462;&#21098;&#30340;&#35270;&#39057;&#21253;&#21547;&#20449;&#24687;&#20016;&#23500;&#30340;&#24103;&#65292;&#32780;&#24182;&#38750;&#25152;&#26377;&#24103;&#37117;&#19982;&#26597;&#35810;&#30456;&#20851;&#12290;&#24378;&#30456;&#20851;&#24615;&#36890;&#24120;&#20165;&#22312;&#30456;&#20851;&#26102;&#21051;&#20869;&#35266;&#23519;&#21040;&#65292;&#24378;&#35843;&#25429;&#25417;&#20851;&#38190;&#20869;&#23481;&#30340;&#37325;&#35201;&#24615;&#12290;&#65288;2&#65289;&#27169;&#24577;&#65306;&#26597;&#35810;&#19982;&#19981;&#21516;&#27169;&#24577;&#30340;&#30456;&#20851;&#24615;&#19981;&#21516;&#65307;&#21160;&#20316;&#25551;&#36848;&#26356;&#20506;&#36182;&#20110;&#35270;&#35273;&#20803;&#32032;&#65292;&#32780;&#35282;&#33394;&#23545;&#35805;&#19982;&#25991;&#26412;&#20449;&#24687;&#26356;&#30456;&#20851;&#12290;&#35782;&#21035;&#21644;&#35299;&#20915;&#36825;&#20123;&#27169;&#24577;&#29305;&#23450;&#30340;&#32454;&#24494;&#24046;&#21035;&#23545;&#20110;&#22312;VCMR&#20013;&#36827;&#34892;&#26377;&#25928;&#26816;&#32034;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#23558;&#25152;&#26377;&#35270;&#39057;&#20869;&#23481;&#24179;&#31561;&#23545;&#24453;&#65292;&#23548;&#33268;&#23376;&#20248;&#26102;&#21051;&#26816;&#32034;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#26377;&#25928;&#25429;&#25417;p
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13576v1 Announce Type: cross  Abstract: Video corpus moment retrieval~(VCMR) is a new video retrieval task aimed at retrieving a relevant moment from a large corpus of untrimmed videos using a natural language text as query. The relevance between the video and query is partial, mainly evident in two aspects: (1) Scope: The untrimmed video contains information-rich frames, and not all are relevant to the query. Strong correlation is typically observed only within the relevant moment, emphasizing the importance of capturing key content. (2) Modality: The relevance of query to different modalities varies; action descriptions align more with the visual elements, while character conversations are more related to textual information. Recognizing and addressing these modality-specific nuances is crucial for effective retrieval in VCMR. However, existing methods often treat all video contents equally, leading to sub-optimal moment retrieval. We argue that effectively capturing the p
&lt;/p&gt;</description></item></channel></rss>