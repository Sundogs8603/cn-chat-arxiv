<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32479;&#19968;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#39034;&#24207;&#25512;&#33616;&#26041;&#27861;&#65288;UPSR&#65289;&#65292;&#26088;&#22312;&#26500;&#24314;&#19968;&#20010;&#32479;&#19968;&#30340;&#39044;&#35757;&#32451;&#25512;&#33616;&#27169;&#22411;&#29992;&#20110;&#22810;&#39046;&#22495;&#25512;&#33616;&#20219;&#21153;&#12290;&#30740;&#31350;&#32773;&#35774;&#35745;&#20102;&#20116;&#20010;&#20851;&#38190;&#25351;&#26631;&#26469;&#25351;&#23548;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#25991;&#26412;-&gt;&#29289;&#21697;&#36866;&#24212;&#21644;&#34892;&#20026;&#24207;&#21015;-&gt;&#25991;&#26412;&#24207;&#21015;&#36866;&#24212;&#12290;</title><link>http://arxiv.org/abs/2310.13540</link><description>&lt;p&gt;
&#20840;&#38754;&#23558;&#22810;&#39046;&#22495;&#39044;&#35757;&#32451;&#25512;&#33616;&#24314;&#27169;&#20026;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
Thoroughly Modeling Multi-domain Pre-trained Recommendation as Language. (arXiv:2310.13540v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13540
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32479;&#19968;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#39034;&#24207;&#25512;&#33616;&#26041;&#27861;&#65288;UPSR&#65289;&#65292;&#26088;&#22312;&#26500;&#24314;&#19968;&#20010;&#32479;&#19968;&#30340;&#39044;&#35757;&#32451;&#25512;&#33616;&#27169;&#22411;&#29992;&#20110;&#22810;&#39046;&#22495;&#25512;&#33616;&#20219;&#21153;&#12290;&#30740;&#31350;&#32773;&#35774;&#35745;&#20102;&#20116;&#20010;&#20851;&#38190;&#25351;&#26631;&#26469;&#25351;&#23548;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#25991;&#26412;-&gt;&#29289;&#21697;&#36866;&#24212;&#21644;&#34892;&#20026;&#24207;&#21015;-&gt;&#25991;&#26412;&#24207;&#21015;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#20808;&#39537;&#24615;&#24037;&#20316;&#35797;&#22270;&#25506;&#32034;&#23558;PLM&#20013;&#30340;&#36890;&#29992;&#25991;&#26412;&#20449;&#24687;&#19982;&#29992;&#25143;&#21382;&#21490;&#34892;&#20026;&#24207;&#21015;&#20013;&#30340;&#20010;&#24615;&#21270;&#34892;&#20026;&#20449;&#24687;&#30456;&#32467;&#21512;&#65292;&#20197;&#22686;&#24378;&#39034;&#24207;&#25512;&#33616;&#65288;SR&#65289;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#36755;&#20837;&#26684;&#24335;&#21644;&#20219;&#21153;&#30446;&#26631;&#23384;&#22312;&#20849;&#24615;&#65292;&#34892;&#20026;&#21644;&#25991;&#26412;&#20449;&#24687;&#20043;&#38388;&#23384;&#22312;&#24040;&#22823;&#24046;&#36317;&#65292;&#36825;&#38459;&#30861;&#20102;&#23558;SR&#20316;&#20026;&#35821;&#35328;&#24314;&#27169;&#23436;&#20840;&#24314;&#27169;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32479;&#19968;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#39034;&#24207;&#25512;&#33616;&#65288;UPSR&#65289;&#26041;&#27861;&#65292;&#26088;&#22312;&#26500;&#24314;&#19968;&#20010;&#32479;&#19968;&#30340;&#39044;&#35757;&#32451;&#25512;&#33616;&#27169;&#22411;&#29992;&#20110;&#22810;&#39046;&#22495;&#25512;&#33616;&#20219;&#21153;&#12290;&#25105;&#20204;&#27491;&#24335;&#35774;&#35745;&#20102;&#33258;&#28982;&#24615;&#12289;&#39046;&#22495;&#19968;&#33268;&#24615;&#12289;&#20449;&#24687;&#24615;&#12289;&#22122;&#22768;&#21644;&#27169;&#31946;&#24615;&#20197;&#21450;&#25991;&#26412;&#38271;&#24230;&#31561;&#20116;&#20010;&#20851;&#38190;&#25351;&#26631;&#65292;&#20998;&#21035;&#29992;&#20110;&#25351;&#23548;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#25991;&#26412;-&gt;&#29289;&#21697;&#36866;&#24212;&#21644;&#34892;&#20026;&#24207;&#21015;-&gt;&#25991;&#26412;&#24207;&#21015;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the thriving of pre-trained language model (PLM) widely verified in various of NLP tasks, pioneer efforts attempt to explore the possible cooperation of the general textual information in PLM with the personalized behavioral information in user historical behavior sequences to enhance sequential recommendation (SR). However, despite the commonalities of input format and task goal, there are huge gaps between the behavioral and textual information, which obstruct thoroughly modeling SR as language modeling via PLM. To bridge the gap, we propose a novel Unified pre-trained language model enhanced sequential recommendation (UPSR), aiming to build a unified pre-trained recommendation model for multi-domain recommendation tasks. We formally design five key indicators, namely naturalness, domain consistency, informativeness, noise &amp; ambiguity, and text length, to guide the text-&gt;item adaptation and behavior sequence-&gt;text sequence adaptation differently for pre-training and fine-tuning 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;REIGN&#65292;&#36890;&#36807;&#29983;&#25104;&#35757;&#32451;&#38382;&#39064;&#30340;&#25913;&#20889;&#65292;&#24182;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26469;&#25351;&#23548;&#23545;&#35805;&#38382;&#31572;&#27169;&#22411;&#65292;&#22686;&#21152;&#27169;&#22411;&#23545;&#34920;&#38754;&#24418;&#24335;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#22312;&#19981;&#21516;&#30340;&#22522;&#20934;&#19978;&#36827;&#34892;&#38646;-shot&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2310.13505</link><description>&lt;p&gt;
&#20855;&#26377;&#24378;&#21270;&#25913;&#20889;&#29983;&#25104;&#30340;&#23545;&#35805;&#38382;&#31572;&#27169;&#22411;&#30340;&#40065;&#26834;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Robust Training for Conversational Question Answering Models with Reinforced Reformulation Generation. (arXiv:2310.13505v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13505
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;REIGN&#65292;&#36890;&#36807;&#29983;&#25104;&#35757;&#32451;&#38382;&#39064;&#30340;&#25913;&#20889;&#65292;&#24182;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26469;&#25351;&#23548;&#23545;&#35805;&#38382;&#31572;&#27169;&#22411;&#65292;&#22686;&#21152;&#27169;&#22411;&#23545;&#34920;&#38754;&#24418;&#24335;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#22312;&#19981;&#21516;&#30340;&#22522;&#20934;&#19978;&#36827;&#34892;&#38646;-shot&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#19978;&#30340;&#23545;&#35805;&#38382;&#31572;&#65288;ConvQA&#65289;&#27169;&#22411;&#36890;&#24120;&#22312;&#40644;&#37329;QA&#23545;&#30340;&#22522;&#20934;&#19978;&#36827;&#34892;&#35757;&#32451;&#21644;&#27979;&#35797;&#12290;&#36825;&#24847;&#21619;&#30528;&#35757;&#32451;&#20165;&#38480;&#20110;&#22312;&#30456;&#24212;&#25968;&#25454;&#38598;&#20013;&#35265;&#21040;&#30340;&#34920;&#38754;&#24418;&#24335;&#65292;&#35780;&#20272;&#20165;&#38024;&#23545;&#19968;&#23567;&#37096;&#20998;&#38382;&#39064;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#25552;&#20986;&#30340;&#26694;&#26550;REIGN&#65292;&#25105;&#20204;&#37319;&#21462;&#20102;&#20960;&#20010;&#27493;&#39588;&#26469;&#35299;&#20915;&#36825;&#20010;&#21463;&#38480;&#30340;&#23398;&#20064;&#35774;&#32622;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#29983;&#25104;&#35757;&#32451;&#38382;&#39064;&#30340;&#25913;&#20889;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#23545;&#34920;&#38754;&#24418;&#24335;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#12290;&#36825;&#26159;&#19968;&#20010;&#29305;&#21035;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#36825;&#20123;&#38382;&#39064;&#30340;&#19981;&#23436;&#25972;&#24615;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#23558;ConvQA&#27169;&#22411;&#24341;&#23548;&#21040;&#26356;&#39640;&#30340;&#24615;&#33021;&#65292;&#21482;&#25552;&#20379;&#37027;&#20123;&#26377;&#21161;&#20110;&#25552;&#39640;&#22238;&#31572;&#36136;&#37327;&#30340;&#25913;&#20889;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#19968;&#20010;&#22522;&#20934;&#19978;&#35757;&#32451;&#20027;&#35201;&#27169;&#22411;&#32452;&#20214;&#24182;&#23558;&#20854;&#38646;-shot&#24212;&#29992;&#20110;&#21478;&#19968;&#20010;&#30340;&#21487;&#34892;&#24615;&#12290;&#26368;&#21518;&#65292;&#20026;&#20102;&#23545;&#35757;&#32451;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#36827;&#34892;&#20005;&#26684;&#35780;&#20272;&#65292;&#25105;&#20204;&#20351;&#29992;&#21644;&#37325;&#26032;&#37197;&#32622;&#21021;&#22987;&#30340;&#25913;&#20889;&#12289;&#27979;&#35797;&#35821;&#26009;&#12290;
&lt;/p&gt;
&lt;p&gt;
Models for conversational question answering (ConvQA) over knowledge graphs (KGs) are usually trained and tested on benchmarks of gold QA pairs. This implies that training is limited to surface forms seen in the respective datasets, and evaluation is on a small set of held-out questions. Through our proposed framework REIGN, we take several steps to remedy this restricted learning setup. First, we systematically generate reformulations of training questions to increase robustness of models to surface form variations. This is a particularly challenging problem, given the incomplete nature of such questions. Second, we guide ConvQA models towards higher performance by feeding it only those reformulations that help improve their answering quality, using deep reinforcement learning. Third, we demonstrate the viability of training major model components on one benchmark and applying them zero-shot to another. Finally, for a rigorous evaluation of robustness for trained models, we use and re
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35838;&#31243;&#23398;&#20064;&#30340;&#20004;&#38454;&#27573;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21306;&#20998;&#21322;&#30828;&#21644;&#30828;&#19977;&#20803;&#32452;&#65292;&#25552;&#39640;&#20102;&#36328;&#27169;&#24577;&#26816;&#32034;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.13451</link><description>&lt;p&gt;
&#20004;&#38454;&#27573;&#19977;&#20803;&#32452;&#25439;&#22833;&#35757;&#32451;&#19982;&#35838;&#31243;&#22686;&#24378;&#22312;&#38899;&#35270;&#39057;&#26816;&#32034;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Two-Stage Triplet Loss Training with Curriculum Augmentation for Audio-Visual Retrieval. (arXiv:2310.13451v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13451
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35838;&#31243;&#23398;&#20064;&#30340;&#20004;&#38454;&#27573;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21306;&#20998;&#21322;&#30828;&#21644;&#30828;&#19977;&#20803;&#32452;&#65292;&#25552;&#39640;&#20102;&#36328;&#27169;&#24577;&#26816;&#32034;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#27169;&#24577;&#26816;&#32034;&#27169;&#22411;&#21033;&#29992;&#19977;&#20803;&#32452;&#25439;&#22833;&#20248;&#21270;&#26469;&#23398;&#20064;&#40065;&#26834;&#30340;&#23884;&#20837;&#31354;&#38388;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#22312;&#21333;&#27425;&#35757;&#32451;&#20013;&#23545;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#24573;&#35270;&#20102;&#20248;&#21270;&#36807;&#31243;&#20013;&#21322;&#30828;&#21644;&#30828;&#19977;&#20803;&#32452;&#20043;&#38388;&#30340;&#21306;&#21035;&#12290;&#27809;&#26377;&#21306;&#20998;&#21322;&#30828;&#21644;&#30828;&#19977;&#20803;&#32452;&#30340;&#30095;&#24573;&#23548;&#33268;&#20102;&#27169;&#22411;&#24615;&#33021;&#30340;&#27425;&#20248;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#35838;&#31243;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#35757;&#32451;&#33539;&#24335;&#65292;&#24341;&#23548;&#27169;&#22411;&#30340;&#23398;&#20064;&#36807;&#31243;&#20174;&#21322;&#30828;&#19977;&#20803;&#32452;&#21040;&#30828;&#19977;&#20803;&#32452;&#12290;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;&#27169;&#22411;&#20174;&#20302;&#25439;&#22833;&#22522;&#30784;&#24320;&#22987;&#65292;&#20351;&#29992;&#19968;&#32452;&#21322;&#30828;&#19977;&#20803;&#32452;&#36827;&#34892;&#35757;&#32451;&#12290;&#38543;&#21518;&#65292;&#22312;&#31532;&#20108;&#38454;&#27573;&#65292;&#25105;&#20204;&#20351;&#29992;&#25554;&#20540;&#25216;&#26415;&#23545;&#23884;&#20837;&#36827;&#34892;&#22686;&#24378;&#12290;&#36825;&#20010;&#36807;&#31243;&#21487;&#20197;&#35782;&#21035;&#28508;&#22312;&#30340;&#22256;&#38590;&#36127;&#26679;&#26412;&#65292;&#32531;&#35299;&#30001;&#20110;&#22256;&#38590;&#19977;&#20803;&#32452;&#31232;&#32570;&#23548;&#33268;&#30340;&#39640;&#25439;&#22833;&#20989;&#25968;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#22312;&#22686;&#24378;&#21518;&#30340;&#23884;&#20837;&#20013;&#24212;&#29992;&#20102;&#30828;&#19977;&#20803;&#32452;&#25366;&#25496;&#12290;
&lt;/p&gt;
&lt;p&gt;
The cross-modal retrieval model leverages the potential of triple loss optimization to learn robust embedding spaces. However, existing methods often train these models in a singular pass, overlooking the distinction between semi-hard and hard triples in the optimization process. The oversight of not distinguishing between semi-hard and hard triples leads to suboptimal model performance. In this paper, we introduce a novel approach rooted in curriculum learning to address this problem. We propose a two-stage training paradigm that guides the model's learning process from semi-hard to hard triplets. In the first stage, the model is trained with a set of semi-hard triplets, starting from a low-loss base. Subsequently, in the second stage, we augment the embeddings using an interpolation technique. This process identifies potential hard negatives, alleviating issues arising from high-loss functions due to a scarcity of hard triples. Our approach then applies hard triplet mining in the aug
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#38899;&#39057;&#22686;&#24378;&#27969;&#27700;&#32447;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20026;&#22024;&#26434;&#29615;&#22659;&#19979;&#30340;&#38899;&#39057;&#25351;&#32441;&#35782;&#21035;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#65292;&#25552;&#39640;&#20102;&#35782;&#21035;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.13388</link><description>&lt;p&gt;
&#38899;&#39057;&#25351;&#32441;&#35782;&#21035;&#20013;&#30340;&#38899;&#20048;&#22686;&#24378;&#21644;&#38477;&#22122;
&lt;/p&gt;
&lt;p&gt;
Music Augmentation and Denoising For Peak-Based Audio Fingerprinting. (arXiv:2310.13388v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13388
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#38899;&#39057;&#22686;&#24378;&#27969;&#27700;&#32447;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20026;&#22024;&#26434;&#29615;&#22659;&#19979;&#30340;&#38899;&#39057;&#25351;&#32441;&#35782;&#21035;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#65292;&#25552;&#39640;&#20102;&#35782;&#21035;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#39057;&#25351;&#32441;&#35782;&#21035;&#26159;&#19968;&#31181;&#20174;&#31616;&#30701;&#24405;&#38899;&#29255;&#27573;&#20013;&#35782;&#21035;&#27468;&#26354;&#30340;&#25104;&#29087;&#35299;&#20915;&#26041;&#26696;&#12290;&#27969;&#34892;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#25277;&#21462;&#31232;&#30095;&#34920;&#31034;&#65292;&#36890;&#24120;&#26159;&#39057;&#35889;&#23792;&#20540;&#65292;&#24182;&#19988;&#24050;&#32463;&#35777;&#26126;&#20934;&#30830;&#12289;&#24555;&#36895;&#65292;&#24182;&#21487;&#25193;&#23637;&#21040;&#22823;&#22411;&#38598;&#21512;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#38899;&#39057;&#35782;&#21035;&#30340;&#24212;&#29992;&#24448;&#24448;&#21457;&#29983;&#22312;&#22024;&#26434;&#30340;&#29615;&#22659;&#20013;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#36825;&#20123;&#31995;&#32479;&#22833;&#25928;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#21644;&#21457;&#24067;&#19968;&#20010;&#26032;&#30340;&#38899;&#39057;&#22686;&#24378;&#27969;&#27700;&#32447;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#35813;&#27969;&#27700;&#32447;&#20197;&#19968;&#31181;&#36924;&#30495;&#30340;&#26041;&#24335;&#21521;&#38899;&#20048;&#29255;&#27573;&#28155;&#21152;&#22122;&#22768;&#65292;&#36890;&#36807;&#38543;&#26426;&#27169;&#25311;&#29616;&#23454;&#19990;&#30028;&#30340;&#22330;&#26223;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#21457;&#24067;&#20102;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20174;&#35889;&#22270;&#20013;&#21435;&#38500;&#22122;&#22768;&#25104;&#20998;&#65292;&#20197;&#25552;&#39640;&#22522;&#20110;&#23792;&#20540;&#30340;&#25351;&#32441;&#35782;&#21035;&#31995;&#32479;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#30340;&#28155;&#21152;&#25913;&#36827;&#20102;&#24120;&#29992;&#38899;&#39057;&#25351;&#32441;&#35782;&#21035;&#31995;&#32479;&#30340;&#35782;&#21035;&#24615;&#33021;&#65292;&#21363;&#20351;&#22312;&#22024;&#26434;&#30340;&#26465;&#20214;&#19979;&#20063;&#26159;&#22914;&#27492;&#12290;
&lt;/p&gt;
&lt;p&gt;
Audio fingerprinting is a well-established solution for song identification from short recording excerpts. Popular methods rely on the extraction of sparse representations, generally spectral peaks, and have proven to be accurate, fast, and scalable to large collections. However, real-world applications of audio identification often happen in noisy environments, which can cause these systems to fail. In this work, we tackle this problem by introducing and releasing a new audio augmentation pipeline that adds noise to music snippets in a realistic way, by stochastically mimicking real-world scenarios. We then propose and release a deep learning model that removes noisy components from spectrograms in order to improve peak-based fingerprinting systems' accuracy. We show that the addition of our model improves the identification performance of commonly used audio fingerprinting systems, even under noisy conditions.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#23376;&#20250;&#35805;&#22810;&#36718;&#20250;&#35805;&#25512;&#33616;&#30340;&#26032;&#39062;&#22330;&#26223;&#65292;&#36890;&#36807;&#22810;&#36718;&#20132;&#20114;&#23545;&#35805;&#33719;&#21462;&#29992;&#25143;&#23545;&#25152;&#38656;&#29289;&#21697;&#30340;&#21160;&#24577;&#20559;&#22909;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#22330;&#26223;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#25512;&#33616;&#27169;&#22359;&#32508;&#21512;&#24314;&#27169;&#29992;&#25143;&#20852;&#36259;&#12290;</title><link>http://arxiv.org/abs/2310.13365</link><description>&lt;p&gt;
&#36808;&#21521;&#22810;&#23376;&#20250;&#35805;&#20250;&#35805;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Towards Multi-Subsession Conversational Recommendation. (arXiv:2310.13365v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13365
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#23376;&#20250;&#35805;&#22810;&#36718;&#20250;&#35805;&#25512;&#33616;&#30340;&#26032;&#39062;&#22330;&#26223;&#65292;&#36890;&#36807;&#22810;&#36718;&#20132;&#20114;&#23545;&#35805;&#33719;&#21462;&#29992;&#25143;&#23545;&#25152;&#38656;&#29289;&#21697;&#30340;&#21160;&#24577;&#20559;&#22909;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#22330;&#26223;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#25512;&#33616;&#27169;&#22359;&#32508;&#21512;&#24314;&#27169;&#29992;&#25143;&#20852;&#36259;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20250;&#35805;&#25512;&#33616;&#31995;&#32479;(CRS)&#21487;&#20197;&#36890;&#36807;&#22810;&#36718;&#20132;&#20114;&#23545;&#35805;&#33719;&#21462;&#29992;&#25143;&#23545;&#25152;&#38656;&#29289;&#21697;&#30340;&#21160;&#24577;&#20559;&#22909;&#12290;&#20197;&#24448;&#30340;CRS&#20027;&#35201;&#20851;&#27880;&#29992;&#25143;&#22312;&#25104;&#21151;&#25512;&#33616;&#21518;&#36864;&#20986;&#30340;&#21333;&#27425;&#23545;&#35805;(&#23376;&#20250;&#35805;)&#65292;&#24573;&#35270;&#20102;&#29992;&#25143;&#22312;&#30701;&#26102;&#38388;&#20869;&#36827;&#34892;&#22810;&#27425;&#23545;&#35805;(&#22810;&#23376;&#20250;&#35805;)&#30340;&#24120;&#35265;&#22330;&#26223;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20250;&#35805;&#25512;&#33616;&#22330;&#26223;&#65292;&#21517;&#20026;&#22810;&#23376;&#20250;&#35805;&#22810;&#36718;&#20250;&#35805;&#25512;&#33616;(MSMCR)&#65292;&#29992;&#25143;&#22312;&#22810;&#20010;&#23376;&#20250;&#35805;&#20043;&#21518;&#20173;&#28982;&#20250;&#20351;&#29992;CRS&#65292;&#24182;&#21487;&#33021;&#20445;&#25345;&#27169;&#31946;&#30340;&#20852;&#36259;&#65292;&#31995;&#32479;&#20250;&#20027;&#21160;&#35810;&#38382;&#23646;&#24615;&#20197;&#28608;&#27963;&#24403;&#21069;&#23376;&#20250;&#35805;&#20013;&#30340;&#29992;&#25143;&#20852;&#36259;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#26032;&#30340;CRS&#22330;&#26223;&#20013;&#30340;&#31354;&#30333;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#31216;&#20026;&#24102;&#26377;&#28608;&#27963;&#23646;&#24615;&#30340;&#22810;&#23376;&#20250;&#35805;&#20250;&#35805;&#25512;&#33616;&#22120;&#30340;&#26032;&#26694;&#26550;(MSCAA)&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#24320;&#21457;&#20102;&#19968;&#20010;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#25512;&#33616;&#27169;&#22359;&#65292;&#20840;&#38754;&#22320;&#23545;&#29992;&#25143;&#20852;&#36259;&#36827;&#34892;&#24314;&#27169;&#65292;
&lt;/p&gt;
&lt;p&gt;
Conversational recommendation systems (CRS) could acquire dynamic user preferences towards desired items through multi-round interactive dialogue. Previous CRS mainly focuses on the single conversation (subsession) that user quits after a successful recommendation, neglecting the common scenario where user has multiple conversations (multi-subsession) over a short period. Therefore, we propose a novel conversational recommendation scenario named Multi-Subsession Multi-round Conversational Recommendation (MSMCR), where user would still resort to CRS after several subsessions and might preserve vague interests, and system would proactively ask attributes to activate user interests in the current subsession. To fill the gap in this new CRS scenario, we devise a novel framework called Multi-Subsession Conversational Recommender with Activation Attributes (MSCAA). Specifically, we first develop a context-aware recommendation module, comprehensively modeling user interests from historical in
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#24335;&#30340;&#25552;&#31034;&#23398;&#20064;&#26694;&#26550;MOP&#65292;&#35813;&#26694;&#26550;&#24341;&#20837;&#20102;&#22522;&#20110;&#27169;&#24335;&#30340;&#20849;&#20139;&#23884;&#20837;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;CDR&#27169;&#22411;&#22312;&#36328;&#39046;&#22495;&#25512;&#33616;&#20013;&#38754;&#20020;&#30340;&#36866;&#24212;&#24615;&#21644;&#36127;&#38754;&#36716;&#31227;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.13303</link><description>&lt;p&gt;
&#22522;&#20110;&#27169;&#24335;&#30340;&#25552;&#31034;&#23398;&#20064;&#29992;&#20110;&#36890;&#29992;&#36328;&#39046;&#22495;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Motif-Based Prompt Learning for Universal Cross-Domain Recommendation. (arXiv:2310.13303v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13303
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#24335;&#30340;&#25552;&#31034;&#23398;&#20064;&#26694;&#26550;MOP&#65292;&#35813;&#26694;&#26550;&#24341;&#20837;&#20102;&#22522;&#20110;&#27169;&#24335;&#30340;&#20849;&#20139;&#23884;&#20837;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;CDR&#27169;&#22411;&#22312;&#36328;&#39046;&#22495;&#25512;&#33616;&#20013;&#38754;&#20020;&#30340;&#36866;&#24212;&#24615;&#21644;&#36127;&#38754;&#36716;&#31227;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#39046;&#22495;&#25512;&#33616;&#65288;CDR&#65289;&#26159;&#19968;&#39033;&#20851;&#38190;&#25216;&#26415;&#65292;&#36890;&#36807;&#23558;&#28304;&#39046;&#22495;&#30340;&#36890;&#29992;&#30693;&#35782;&#36716;&#31227;&#33267;&#30446;&#26631;&#39046;&#22495;&#65292;&#35299;&#20915;&#25968;&#25454;&#31232;&#30095;&#21644;&#20919;&#21551;&#21160;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;CDR&#27169;&#22411;&#30001;&#20110;&#22266;&#26377;&#30340;&#22797;&#26434;&#24615;&#65292;&#20854;&#22312;&#19981;&#21516;&#22330;&#26223;&#20013;&#30340;&#36866;&#24212;&#24615;&#21463;&#21040;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;&#22522;&#20110;&#20849;&#20139;&#23884;&#20837;&#30340;&#36890;&#29992;CDR&#27169;&#22411;&#65292;&#36890;&#36807;"&#22810;&#20219;&#21153;&#23398;&#20064;"&#25110;"&#39044;&#35757;&#32451;&#65292;&#24494;&#35843;"&#33539;&#24335;&#26469;&#25429;&#25417;&#36328;&#39046;&#22495;&#36890;&#29992;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#24573;&#35270;&#36328;&#39046;&#22495;&#20043;&#38388;&#30340;&#24191;&#27867;&#32467;&#26500;&#25299;&#25169;&#65292;&#24182;&#19988;&#22312;&#35757;&#32451;&#30446;&#26631;&#19978;&#19981;&#33021;&#23436;&#20840;&#23545;&#40784;&#65292;&#21487;&#33021;&#23548;&#33268;&#36127;&#38754;&#30340;&#36716;&#31227;&#25928;&#26524;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#27169;&#24335;&#30340;&#25552;&#31034;&#23398;&#20064;&#26694;&#26550;MOP&#65292;&#24341;&#20837;&#20102;&#22522;&#20110;&#27169;&#24335;&#30340;&#20849;&#20139;&#23884;&#20837;&#65292;&#20197;&#21253;&#21547;&#27867;&#21270;&#30340;&#39046;&#22495;&#30693;&#35782;&#65292;&#21516;&#26102;&#36866;&#24212;&#39046;&#22495;&#20869;&#21644;&#39046;&#22495;&#38388;&#30340;CDR&#20219;&#21153;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19977;&#31181;&#20856;&#22411;&#30340;&#27169;&#24335;&#65306;&#34676;&#34678;...
&lt;/p&gt;
&lt;p&gt;
Cross-Domain Recommendation (CDR) stands as a pivotal technology addressing issues of data sparsity and cold start by transferring general knowledge from the source to the target domain. However, existing CDR models suffer limitations in adaptability across various scenarios due to their inherent complexity. To tackle this challenge, recent advancements introduce universal CDR models that leverage shared embeddings to capture general knowledge across domains and transfer it through "Multi-task Learning" or "Pre-train, Fine-tune" paradigms. However, these models often overlook the broader structural topology that spans domains and fail to align training objectives, potentially leading to negative transfer. To address these issues, we propose a motif-based prompt learning framework, MOP, which introduces motif-based shared embeddings to encapsulate generalized domain knowledge, catering to both intra-domain and inter-domain CDR tasks. Specifically, we devise three typical motifs: butterf
&lt;/p&gt;</description></item><item><title>VR PreM+&#26159;&#19968;&#20010;&#27785;&#28024;&#24335;&#30340;&#21338;&#29289;&#39302;&#23548;&#35272;&#39044;&#23398;&#20064;&#20998;&#25903;&#21487;&#35270;&#21270;&#31995;&#32479;&#65292;&#36890;&#36807;&#20851;&#38190;&#23383;&#20449;&#24687;&#26816;&#32034;&#22312;&#21160;&#24577;&#30340;3D&#31354;&#38388;&#20013;&#31649;&#29702;&#21644;&#36830;&#25509;&#21508;&#31181;&#20869;&#23481;&#26469;&#28304;&#65292;&#25552;&#39640;&#20102;&#27807;&#36890;&#21644;&#25968;&#25454;&#27604;&#36739;&#30340;&#33021;&#21147;&#65292;&#26377;&#26395;&#24212;&#29992;&#20110;&#30740;&#31350;&#21644;&#25945;&#32946;&#31561;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2310.13294</link><description>&lt;p&gt;
VR PreM+&#65306;&#19968;&#20010;&#27785;&#28024;&#24335;&#30340;&#21338;&#29289;&#39302;&#23548;&#35272;&#39044;&#23398;&#20064;&#20998;&#25903;&#21487;&#35270;&#21270;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
VR PreM+: An Immersive Pre-learning Branching Visualization System for Museum Tours. (arXiv:2310.13294v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13294
&lt;/p&gt;
&lt;p&gt;
VR PreM+&#26159;&#19968;&#20010;&#27785;&#28024;&#24335;&#30340;&#21338;&#29289;&#39302;&#23548;&#35272;&#39044;&#23398;&#20064;&#20998;&#25903;&#21487;&#35270;&#21270;&#31995;&#32479;&#65292;&#36890;&#36807;&#20851;&#38190;&#23383;&#20449;&#24687;&#26816;&#32034;&#22312;&#21160;&#24577;&#30340;3D&#31354;&#38388;&#20013;&#31649;&#29702;&#21644;&#36830;&#25509;&#21508;&#31181;&#20869;&#23481;&#26469;&#28304;&#65292;&#25552;&#39640;&#20102;&#27807;&#36890;&#21644;&#25968;&#25454;&#27604;&#36739;&#30340;&#33021;&#21147;&#65292;&#26377;&#26395;&#24212;&#29992;&#20110;&#30740;&#31350;&#21644;&#25945;&#32946;&#31561;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;VR PreM+&#65292;&#19968;&#20010;&#21019;&#26032;&#30340;VR&#31995;&#32479;&#65292;&#26088;&#22312;&#22686;&#24378;&#20256;&#32479;&#30005;&#33041;&#23631;&#24149;&#20197;&#22806;&#30340;&#32593;&#32476;&#25506;&#32034;&#12290;&#19982;&#38745;&#24577;&#30340;2D&#26174;&#31034;&#19981;&#21516;&#65292;VR PreM+&#21033;&#29992;3D&#29615;&#22659;&#21019;&#24314;&#20102;&#27785;&#28024;&#24335;&#30340;&#39044;&#23398;&#20064;&#20307;&#39564;&#12290;&#36890;&#36807;&#20851;&#38190;&#23383;&#20449;&#24687;&#26816;&#32034;&#65292;&#29992;&#25143;&#21487;&#20197;&#22312;&#21160;&#24577;&#30340;3D&#31354;&#38388;&#20013;&#31649;&#29702;&#21644;&#36830;&#25509;&#21508;&#31181;&#20869;&#23481;&#26469;&#28304;&#65292;&#25552;&#39640;&#27807;&#36890;&#21644;&#25968;&#25454;&#27604;&#36739;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#21021;&#27493;&#21644;&#29992;&#25143;&#30740;&#31350;&#65292;&#35777;&#26126;&#20102;&#39640;&#25928;&#30340;&#20449;&#24687;&#26816;&#32034;&#12289;&#22686;&#21152;&#20102;&#29992;&#25143;&#21442;&#19982;&#24230;&#21644;&#26356;&#24378;&#30340;&#23384;&#22312;&#24863;&#12290;&#36825;&#20123;&#21457;&#29616;&#20026;&#26410;&#26469;&#30340;VR&#20449;&#24687;&#31995;&#32479;&#25552;&#20379;&#20102;&#19977;&#20010;&#35774;&#35745;&#25351;&#23548;&#21407;&#21017;&#65306;&#26174;&#31034;&#12289;&#20132;&#20114;&#21644;&#20197;&#29992;&#25143;&#20026;&#20013;&#24515;&#30340;&#35774;&#35745;&#12290;VR PreM+&#24357;&#34917;&#20102;&#20256;&#32479;&#30340;&#32593;&#32476;&#27983;&#35272;&#21644;&#27785;&#28024;&#24335;VR&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#20132;&#20114;&#24335;&#21644;&#20840;&#38754;&#30340;&#20449;&#24687;&#33719;&#21462;&#26041;&#27861;&#12290;&#23427;&#22312;&#30740;&#31350;&#12289;&#25945;&#32946;&#31561;&#39046;&#22495;&#20855;&#26377;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present VR PreM+, an innovative VR system designed to enhance web exploration beyond traditional computer screens. Unlike static 2D displays, VR PreM+ leverages 3D environments to create an immersive pre-learning experience. Using keyword-based information retrieval allows users to manage and connect various content sources in a dynamic 3D space, improving communication and data comparison. We conducted preliminary and user studies that demonstrated efficient information retrieval, increased user engagement, and a greater sense of presence. These findings yielded three design guidelines for future VR information systems: display, interaction, and user-centric design. VR PreM+ bridges the gap between traditional web browsing and immersive VR, offering an interactive and comprehensive approach to information acquisition. It holds promise for research, education, and beyond.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"&#36890;&#36807;&#20219;&#21153;&#36229;&#22270;&#30340;&#32479;&#19968;&#39044;&#35757;&#32451;&#25512;&#33616;"&#30340;&#22810;&#20219;&#21153;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#20351;&#29992;&#20219;&#21153;&#36229;&#22270;&#23558;&#20808;&#21069;&#20219;&#21153;&#25512;&#24191;&#20026;&#36229;&#36793;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#36807;&#28193;&#24615;&#27880;&#24847;&#21147;&#23618;&#23398;&#20064;&#27599;&#20010;&#20808;&#21069;&#20219;&#21153;&#19982;&#25512;&#33616;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#22522;&#20934;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.13286</link><description>&lt;p&gt;
&#36890;&#36807;&#20219;&#21153;&#36229;&#22270;&#23454;&#29616;&#25512;&#33616;&#30340;&#32479;&#19968;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Unified Pretraining for Recommendation via Task Hypergraphs. (arXiv:2310.13286v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13286
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"&#36890;&#36807;&#20219;&#21153;&#36229;&#22270;&#30340;&#32479;&#19968;&#39044;&#35757;&#32451;&#25512;&#33616;"&#30340;&#22810;&#20219;&#21153;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#20351;&#29992;&#20219;&#21153;&#36229;&#22270;&#23558;&#20808;&#21069;&#20219;&#21153;&#25512;&#24191;&#20026;&#36229;&#36793;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#36807;&#28193;&#24615;&#27880;&#24847;&#21147;&#23618;&#23398;&#20064;&#27599;&#20010;&#20808;&#21069;&#20219;&#21153;&#19982;&#25512;&#33616;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#22522;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#39044;&#35757;&#32451;&#22312;&#26368;&#36817;&#20960;&#24180;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#21644;&#27969;&#34892;&#65292;&#20294;&#23427;&#22312;&#22522;&#20110;&#22270;&#30340;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#30456;&#23545;&#26377;&#38480;&#12290;&#22312;&#24191;&#27867;&#20351;&#29992;&#30340;ID&#20381;&#36182;&#25968;&#25454;&#38598;&#20013;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#20808;&#21069;&#30693;&#35782;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#19968;&#26041;&#38754;&#65292;&#19968;&#20010;&#25968;&#25454;&#38598;&#20013;&#30340;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#21382;&#21490;&#24456;&#38590;&#36890;&#36807;&#39044;&#35757;&#32451;&#36716;&#31227;&#21040;&#20854;&#20182;&#25968;&#25454;&#38598;&#20013;&#65292;&#22240;&#20026;ID&#19981;&#21516;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22312;&#21516;&#19968;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#20250;&#23548;&#33268;&#36807;&#25311;&#21512;&#30340;&#39640;&#39118;&#38505;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#36890;&#36807;&#20219;&#21153;&#36229;&#22270;&#30340;&#32479;&#19968;&#25512;&#33616;&#39044;&#35757;&#32451;&#30340;&#26032;&#22411;&#22810;&#20219;&#21153;&#39044;&#35757;&#32451;&#26694;&#26550;&#12290;&#20026;&#20102;&#22788;&#29702;&#21508;&#31181;&#20808;&#21069;&#20219;&#21153;&#30340;&#19981;&#21516;&#35201;&#27714;&#21644;&#32454;&#24494;&#24046;&#21035;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20219;&#21153;&#36229;&#22270;&#23558;&#20808;&#21069;&#20219;&#21153;&#25512;&#24191;&#20026;&#36229;&#36793;&#39044;&#27979;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#36807;&#28193;&#24615;&#27880;&#24847;&#21147;&#23618;&#26469;&#26377;&#24046;&#24322;&#22320;&#23398;&#20064;&#27599;&#20010;&#20808;&#21069;&#20219;&#21153;&#19982;&#25512;&#33616;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#22312;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#20854;&#20182;&#22522;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although pretraining has garnered significant attention and popularity in recent years, its application in graph-based recommender systems is relatively limited. It is challenging to exploit prior knowledge by pretraining in widely used ID-dependent datasets. On one hand, user-item interaction history in one dataset can hardly be transferred to other datasets through pretraining, where IDs are different. On the other hand, pretraining and finetuning on the same dataset leads to a high risk of overfitting. In this paper, we propose a novel multitask pretraining framework named Unified Pretraining for Recommendation via Task Hypergraphs. For a unified learning pattern to handle diverse requirements and nuances of various pretext tasks, we design task hypergraphs to generalize pretext tasks to hyperedge prediction. A novel transitional attention layer is devised to discriminatively learn the relevance between each pretext task and recommendation. Experimental results on three benchmark da
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#23398;&#20064;&#25490;&#24207;&#20013;&#20351;&#29992;&#27169;&#25311;&#36864;&#28779;&#36827;&#34892;&#29305;&#24449;&#36873;&#25321;&#30340;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#36827;&#23637;&#21442;&#25968;&#26469;&#26377;&#25928;&#36941;&#21382;&#25628;&#32034;&#31354;&#38388;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.13269</link><description>&lt;p&gt;
&#29992;&#20110;&#23398;&#20064;&#25490;&#24207;&#20013;&#29305;&#24449;&#36873;&#25321;&#30340;&#27169;&#25311;&#36864;&#28779;&#25506;&#32034;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Exploratory Study on Simulated Annealing for Feature Selection in Learning-to-Rank. (arXiv:2310.13269v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13269
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#23398;&#20064;&#25490;&#24207;&#20013;&#20351;&#29992;&#27169;&#25311;&#36864;&#28779;&#36827;&#34892;&#29305;&#24449;&#36873;&#25321;&#30340;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#36827;&#23637;&#21442;&#25968;&#26469;&#26377;&#25928;&#36941;&#21382;&#25628;&#32034;&#31354;&#38388;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#25490;&#24207;&#26159;&#19968;&#31181;&#24212;&#29992;&#20110;&#30417;&#30563;&#24335;&#26426;&#22120;&#23398;&#20064;&#30340;&#39046;&#22495;&#12290;&#30001;&#20110;&#29305;&#24449;&#36873;&#25321;&#24050;&#34987;&#21457;&#29616;&#21487;&#20197;&#26377;&#25928;&#25552;&#39640;&#23398;&#20064;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#22240;&#27492;&#30740;&#31350;&#23558;&#35813;&#36807;&#31243;&#24212;&#29992;&#20110;&#23398;&#20064;&#25490;&#24207;&#39046;&#22495;&#26159;&#38750;&#24120;&#26377;&#36259;&#30340;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#19968;&#31181;&#31216;&#20026;&#27169;&#25311;&#36864;&#28779;&#30340;&#27969;&#34892;&#20803;&#21551;&#21457;&#24335;&#26041;&#27861;&#22312;&#35813;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;&#22312;&#27169;&#25311;&#36864;&#28779;&#30340;&#19968;&#33324;&#26694;&#26550;&#19979;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#21508;&#31181;&#37051;&#22495;&#36873;&#25321;&#31574;&#30053;&#21644;&#28201;&#24230;&#20919;&#21364;&#26041;&#26696;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#36229;&#21442;&#25968;&#65292;&#31216;&#20026;&#36827;&#23637;&#21442;&#25968;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#29992;&#20110;&#36941;&#21382;&#25628;&#32034;&#31354;&#38388;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#20116;&#20010;&#20844;&#24320;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#39564;&#35777;&#65292;&#25105;&#20204;&#36824;&#23558;&#22522;&#20110;&#27169;&#25311;&#36864;&#28779;&#30340;&#29305;&#24449;&#36873;&#25321;&#31639;&#27861;&#19982;&#21478;&#19968;&#31181;&#26377;&#25928;&#30340;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#21363;&#23616;&#37096;&#27874;&#26463;&#25628;&#32034;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning-to-rank is an applied domain of supervised machine learning. As feature selection has been found to be effective for improving the accuracy of learning models in general, it is intriguing to investigate this process for learning-to-rank domain. In this study, we investigate the use of a popular meta-heuristic approach called simulated annealing for this task. Under the general framework of simulated annealing, we explore various neighborhood selection strategies and temperature cooling schemes. We further introduce a new hyper-parameter called the progress parameter that can effectively be used to traverse the search space. Our algorithms are evaluated on five publicly benchmark datasets of learning-to-rank. For a better validation, we also compare the simulated annealing-based feature selection algorithm with another effective meta-heuristic algorithm, namely local beam search. Extensive experimental results shows the efficacy of our proposed models.
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20197;&#25968;&#25454;&#20026;&#20013;&#24515;&#30340;&#22810;&#30446;&#26631;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#26500;&#24314;&#36127;&#36131;&#20219;&#30340;&#25512;&#33616;&#31995;&#32479;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#35299;&#20915;&#24403;&#21069;&#26041;&#27861;&#22312;&#22788;&#29702;&#22810;&#20010;&#30446;&#26631;&#26102;&#36935;&#21040;&#30340;&#22256;&#38590;&#65292;&#24182;&#19988;&#25552;&#20379;&#20102;&#23545;&#30446;&#26631;&#20248;&#20808;&#32423;&#30340;&#21487;&#25511;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.13260</link><description>&lt;p&gt;
&#19968;&#20010;&#20197;&#25968;&#25454;&#20026;&#20013;&#24515;&#30340;&#22810;&#30446;&#26631;&#23398;&#20064;&#26694;&#26550;&#29992;&#20110;&#36127;&#36131;&#20219;&#30340;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
A Data-Centric Multi-Objective Learning Framework for Responsible Recommendation Systems. (arXiv:2310.13260v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13260
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20197;&#25968;&#25454;&#20026;&#20013;&#24515;&#30340;&#22810;&#30446;&#26631;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#26500;&#24314;&#36127;&#36131;&#20219;&#30340;&#25512;&#33616;&#31995;&#32479;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#35299;&#20915;&#24403;&#21069;&#26041;&#27861;&#22312;&#22788;&#29702;&#22810;&#20010;&#30446;&#26631;&#26102;&#36935;&#21040;&#30340;&#22256;&#38590;&#65292;&#24182;&#19988;&#25552;&#20379;&#20102;&#23545;&#30446;&#26631;&#20248;&#20808;&#32423;&#30340;&#21487;&#25511;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#33021;&#22815;&#26377;&#25928;&#22320;&#24341;&#23548;&#29992;&#25143;&#22312;&#24191;&#27867;&#30340;&#20869;&#23481;&#24211;&#20013;&#25214;&#21040;&#20182;&#20204;&#38656;&#35201;&#30340;&#20449;&#24687;&#12290;&#19968;&#33324;&#26469;&#35828;&#65292;&#25512;&#33616;&#27169;&#22411;&#30340;&#20248;&#21270;&#26159;&#20174;&#29992;&#25143;&#25928;&#29992;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#27604;&#22914;&#28857;&#20987;&#29575;&#25110;&#21305;&#37197;&#30456;&#20851;&#24615;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#36127;&#36131;&#20219;&#30340;&#24037;&#19994;&#25512;&#33616;&#31995;&#32479;&#19981;&#20165;&#38656;&#35201;&#32771;&#34385;&#29992;&#25143;&#25928;&#29992;&#65288;&#23545;&#29992;&#25143;&#30340;&#36131;&#20219;&#65289;&#65292;&#36824;&#38656;&#35201;&#32771;&#34385;&#20854;&#20182;&#30446;&#26631;&#65292;&#21253;&#25324;&#22686;&#21152;&#24179;&#21488;&#25910;&#20837;&#65288;&#23545;&#24179;&#21488;&#30340;&#36131;&#20219;&#65289;&#65292;&#30830;&#20445;&#20844;&#24179;&#24615;&#65288;&#23545;&#20869;&#23481;&#21019;&#20316;&#32773;&#30340;&#36131;&#20219;&#65289;&#21644;&#20445;&#25345;&#26080;&#20559;&#24615;&#65288;&#23545;&#38271;&#26399;&#20581;&#24247;&#21457;&#23637;&#30340;&#36131;&#20219;&#65289;&#12290;&#22810;&#30446;&#26631;&#23398;&#20064;&#26159;&#23454;&#29616;&#36127;&#36131;&#20219;&#25512;&#33616;&#31995;&#32479;&#30340;&#19968;&#31181;&#26377;&#21147;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#26041;&#27861;&#38754;&#20020;&#20004;&#20010;&#25361;&#25112;&#65306;&#22312;&#32479;&#19968;&#26694;&#26550;&#20869;&#25193;&#23637;&#21040;&#24322;&#26500;&#30446;&#26631;&#30340;&#22256;&#38590;&#65292;&#20197;&#21450;&#20248;&#21270;&#36807;&#31243;&#20013;&#23545;&#30446;&#26631;&#20248;&#20808;&#32423;&#30340;&#19981;&#20805;&#20998;&#21487;&#25511;&#24615;&#65292;&#23548;&#33268;&#26080;&#27861;&#25511;&#21046;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommendation systems effectively guide users in locating their desired information within extensive content repositories. Generally, a recommendation model is optimized to enhance accuracy metrics from a user utility standpoint, such as click-through rate or matching relevance. However, a responsible industrial recommendation system must address not only user utility (responsibility to users) but also other objectives, including increasing platform revenue (responsibility to platforms), ensuring fairness (responsibility to content creators), and maintaining unbiasedness (responsibility to long-term healthy development). Multi-objective learning is a potent approach for achieving responsible recommendation systems. Nevertheless, current methods encounter two challenges: difficulty in scaling to heterogeneous objectives within a unified framework, and inadequate controllability over objective priority during optimization, leading to uncontrollable solutions.  In this paper, we present 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#22312;&#30693;&#35782;&#22270;&#35889;&#32972;&#26223;&#19979;&#25506;&#32034;&#22810;&#26679;&#21270;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#24341;&#20837;&#21019;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#21644;&#35780;&#20998;&#20989;&#25968;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#30693;&#35782;&#22270;&#35889;&#25512;&#33616;&#31639;&#27861;&#30340;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.13253</link><description>&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#22686;&#24378;&#30340;&#22810;&#26679;&#21270;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graph Context-Enhanced Diversified Recommendation. (arXiv:2310.13253v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13253
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#22312;&#30693;&#35782;&#22270;&#35889;&#32972;&#26223;&#19979;&#25506;&#32034;&#22810;&#26679;&#21270;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#24341;&#20837;&#21019;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#21644;&#35780;&#20998;&#20989;&#25968;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#30693;&#35782;&#22270;&#35889;&#25512;&#33616;&#31639;&#27861;&#30340;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#39046;&#22495;&#19968;&#30452;&#33268;&#21147;&#20110;&#36890;&#36807;&#21033;&#29992;&#29992;&#25143;&#30340;&#21382;&#21490;&#20132;&#20114;&#26469;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#36861;&#27714;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#24448;&#24448;&#23548;&#33268;&#20102;&#22810;&#26679;&#24615;&#30340;&#38477;&#20302;&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;&#20247;&#25152;&#21608;&#30693;&#30340;&#8220;&#22238;&#22768;&#23460;&#8221;&#29616;&#35937;&#12290;&#22810;&#26679;&#21270;&#25512;&#33616;&#31995;&#32479;&#20316;&#20026;&#19968;&#31181;&#23545;&#31574;&#24212;&#36816;&#32780;&#29983;&#65292;&#23558;&#22810;&#26679;&#24615;&#19982;&#20934;&#30830;&#24615;&#21516;&#31561;&#30475;&#24453;&#65292;&#24182;&#22312;&#23398;&#26415;&#30028;&#21644;&#34892;&#19994;&#23454;&#36341;&#32773;&#20013;&#33719;&#24471;&#20102;&#26174;&#33879;&#30340;&#20851;&#27880;&#12290;&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22810;&#26679;&#21270;&#25512;&#33616;&#31995;&#32479;&#22312;&#22797;&#26434;&#30340;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#32972;&#26223;&#19979;&#30340;&#24212;&#29992;&#12290;&#36825;&#20123;&#30693;&#35782;&#22270;&#35889;&#26159;&#36830;&#25509;&#23454;&#20307;&#21644;&#39033;&#30446;&#30340;&#20449;&#24687;&#24211;&#65292;&#36890;&#36807;&#21152;&#20837;&#28145;&#20837;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#25552;&#20379;&#20102;&#22686;&#21152;&#25512;&#33616;&#22810;&#26679;&#24615;&#30340;&#26377;&#21033;&#36884;&#24452;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#21253;&#25324;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#23454;&#20307;&#35206;&#30422;&#21644;&#20851;&#31995;&#35206;&#30422;&#65292;&#26377;&#25928;&#22320;&#37327;&#21270;&#20102;&#30693;&#35782;&#22270;&#35889;&#39046;&#22495;&#30340;&#22810;&#26679;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#22810;&#26679;&#21270;&#35780;&#20998;&#20989;&#25968;&#65292;&#35813;&#20989;&#25968;&#36890;&#36807;&#32508;&#21512;&#21033;&#29992;&#23454;&#20307;&#35206;&#30422;&#21644;&#20851;&#31995;&#35206;&#30422;&#26469;&#25552;&#39640;&#25512;&#33616;&#31639;&#27861;&#30340;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The field of Recommender Systems (RecSys) has been extensively studied to enhance accuracy by leveraging users' historical interactions. Nonetheless, this persistent pursuit of accuracy frequently engenders diminished diversity, culminating in the well-recognized "echo chamber" phenomenon. Diversified RecSys has emerged as a countermeasure, placing diversity on par with accuracy and garnering noteworthy attention from academic circles and industry practitioners. This research explores the realm of diversified RecSys within the intricate context of knowledge graphs (KG). These KGs act as repositories of interconnected information concerning entities and items, offering a propitious avenue to amplify recommendation diversity through the incorporation of insightful contextual information. Our contributions include introducing an innovative metric, Entity Coverage, and Relation Coverage, which effectively quantifies diversity within the KG domain. Additionally, we introduce the Diversified
&lt;/p&gt;</description></item><item><title>TempGNN&#26159;&#19968;&#31181;&#29992;&#20110;&#21160;&#24577;&#20250;&#35805;&#25512;&#33616;&#30340;&#26102;&#38388;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#25429;&#25417;&#22797;&#26434;&#29289;&#21697;&#36716;&#25442;&#30340;&#32467;&#26500;&#21644;&#26102;&#38388;&#21160;&#24577;&#65292;&#36890;&#36807;&#23545;&#21160;&#24577;&#20250;&#35805;&#22270;&#20013;&#30340;&#33410;&#28857;&#21644;&#36793;&#36827;&#34892;&#26102;&#38388;&#23884;&#20837;&#25805;&#20316;&#65292;&#26377;&#25928;&#22320;&#39044;&#27979;&#19979;&#19968;&#27493;&#21160;&#20316;&#12290;</title><link>http://arxiv.org/abs/2310.13249</link><description>&lt;p&gt;
TempGNN: &#29992;&#20110;&#21160;&#24577;&#20250;&#35805;&#25512;&#33616;&#30340;&#26102;&#38388;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
TempGNN: Temporal Graph Neural Networks for Dynamic Session-Based Recommendations. (arXiv:2310.13249v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13249
&lt;/p&gt;
&lt;p&gt;
TempGNN&#26159;&#19968;&#31181;&#29992;&#20110;&#21160;&#24577;&#20250;&#35805;&#25512;&#33616;&#30340;&#26102;&#38388;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#25429;&#25417;&#22797;&#26434;&#29289;&#21697;&#36716;&#25442;&#30340;&#32467;&#26500;&#21644;&#26102;&#38388;&#21160;&#24577;&#65292;&#36890;&#36807;&#23545;&#21160;&#24577;&#20250;&#35805;&#22270;&#20013;&#30340;&#33410;&#28857;&#21644;&#36793;&#36827;&#34892;&#26102;&#38388;&#23884;&#20837;&#25805;&#20316;&#65292;&#26377;&#25928;&#22320;&#39044;&#27979;&#19979;&#19968;&#27493;&#21160;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#20250;&#35805;&#30340;&#25512;&#33616;&#31995;&#32479;&#36890;&#36807;&#29702;&#35299;&#29992;&#25143;&#22312;&#30701;&#26399;&#20250;&#35805;&#20013;&#19982;&#29289;&#21697;&#30340;&#20132;&#20114;&#34892;&#20026;&#26469;&#39044;&#27979;&#19979;&#19968;&#27493;&#21160;&#20316;&#30340;&#26041;&#27861;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#20197;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#36890;&#36807;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#12289;&#33258;&#27880;&#24847;&#27169;&#22411;&#21644;&#26368;&#36817;&#30340;&#22823;&#22810;&#25968;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#25429;&#25417;&#20250;&#35805;&#20013;&#22797;&#26434;&#29289;&#21697;&#36716;&#25442;&#30340;&#39034;&#24207;&#20381;&#36182;&#24615;&#12290;&#23613;&#31649;&#26377;&#24456;&#22810;&#26681;&#25454;&#20250;&#35805;&#20013;&#29289;&#21697;&#39034;&#24207;&#30340;&#19981;&#21516;&#27169;&#22411;&#65292;&#20294;&#24456;&#23569;&#26377;&#26041;&#27861;&#26469;&#26356;&#22909;&#22320;&#22788;&#29702;&#20132;&#20114;&#20043;&#38388;&#30340;&#26102;&#38388;&#24847;&#20041;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Temporal Graph Neural Networks (TempGNN)&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#21033;&#29992;&#33410;&#28857;&#21644;&#36793;&#19978;&#30340;&#26102;&#38388;&#23884;&#20837;&#25805;&#20316;&#26469;&#25429;&#25417;&#22797;&#26434;&#29289;&#21697;&#36716;&#25442;&#30340;&#32467;&#26500;&#21644;&#26102;&#38388;&#21160;&#24577;&#65292;&#23558;&#20854;&#34920;&#31034;&#20026;&#23450;&#26102;&#20107;&#20214;&#24207;&#21015;&#30340;&#21160;&#24577;&#20250;&#35805;&#22270;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20102;&#25152;&#25552;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Session-based recommendations which predict the next action by understanding a user's interaction behavior with items within a relatively short ongoing session have recently gained increasing popularity. Previous research has focused on capturing the dynamics of sequential dependencies from complicated item transitions in a session by means of recurrent neural networks, self-attention models, and recently, mostly graph neural networks. Despite the plethora of different models relying on the order of items in a session, few approaches have been proposed for dealing better with the temporal implications between interactions. We present Temporal Graph Neural Networks (TempGNN), a generic framework for capturing the structural and temporal dynamics in complex item transitions utilizing temporal embedding operators on nodes and edges on dynamic session graphs, represented as sequences of timed events. Extensive experimental results show the effectiveness and adaptability of the proposed met
&lt;/p&gt;</description></item><item><title>&#26368;&#26032;&#30340;&#24320;&#28304;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#23637;&#29616;&#20986;&#20102;&#24378;&#22823;&#30340;&#38646;-shot&#26597;&#35810;&#20284;&#28982;&#27169;&#22411;&#30340;&#25490;&#21517;&#33021;&#21147;&#65292;&#30740;&#31350;&#26174;&#31034;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#27809;&#26377;&#30417;&#30563;&#25351;&#23548;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#20381;&#28982;&#33021;&#22815;&#36827;&#34892;&#26377;&#25928;&#30340;&#25490;&#24207;&#65292;&#32780;&#19988;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26368;&#20808;&#36827;&#30340;&#25490;&#24207;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#23558;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#26597;&#35810;&#20284;&#28982;&#27169;&#22411;&#19982;&#38646;-shot&#26816;&#32034;&#22120;&#38598;&#25104;&#65292;&#19981;&#20165;&#22312;&#38646;-shot&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#33394;&#65292;&#32780;&#19988;&#22312;&#23569;-shot&#22330;&#26223;&#20013;&#20063;&#23637;&#29616;&#20102;&#21331;&#36234;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.13243</link><description>&lt;p&gt;
&#24320;&#28304;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#26159;&#29992;&#20110;&#25991;&#26723;&#25490;&#24207;&#30340;&#38646;-shot&#26597;&#35810;&#20284;&#28982;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Open-source Large Language Models are Strong Zero-shot Query Likelihood Models for Document Ranking. (arXiv:2310.13243v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13243
&lt;/p&gt;
&lt;p&gt;
&#26368;&#26032;&#30340;&#24320;&#28304;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#23637;&#29616;&#20986;&#20102;&#24378;&#22823;&#30340;&#38646;-shot&#26597;&#35810;&#20284;&#28982;&#27169;&#22411;&#30340;&#25490;&#21517;&#33021;&#21147;&#65292;&#30740;&#31350;&#26174;&#31034;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#27809;&#26377;&#30417;&#30563;&#25351;&#23548;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#20381;&#28982;&#33021;&#22815;&#36827;&#34892;&#26377;&#25928;&#30340;&#25490;&#24207;&#65292;&#32780;&#19988;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26368;&#20808;&#36827;&#30340;&#25490;&#24207;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#23558;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#26597;&#35810;&#20284;&#28982;&#27169;&#22411;&#19982;&#38646;-shot&#26816;&#32034;&#22120;&#38598;&#25104;&#65292;&#19981;&#20165;&#22312;&#38646;-shot&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#33394;&#65292;&#32780;&#19988;&#22312;&#23569;-shot&#22330;&#26223;&#20013;&#20063;&#23637;&#29616;&#20102;&#21331;&#36234;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20449;&#24687;&#26816;&#32034;&#39046;&#22495;&#20013;&#65292;&#26597;&#35810;&#20284;&#28982;&#27169;&#22411;&#65288;QLMs&#65289;&#26681;&#25454;&#29983;&#25104;&#26597;&#35810;&#30340;&#27010;&#29575;&#26469;&#23545;&#25991;&#26723;&#36827;&#34892;&#25490;&#24207;&#12290;&#26368;&#36817;&#65292;&#20808;&#36827;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#25104;&#20026;&#26377;&#25928;&#30340;QLMs&#65292;&#23637;&#31034;&#20102;&#26377;&#21069;&#36884;&#30340;&#25490;&#24207;&#33021;&#21147;&#12290;&#26412;&#25991;&#37325;&#28857;&#30740;&#31350;&#20102;&#26368;&#36817;LLMs&#30340;&#30495;&#23454;&#38646;-shot&#25490;&#24207;&#25928;&#26524;&#65292;&#36825;&#20123;&#27169;&#22411;&#20165;&#22312;&#26080;&#32467;&#26500;&#25991;&#26412;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#39044;&#35757;&#32451;&#65292;&#27809;&#26377;&#36827;&#34892;&#30417;&#30563;&#25351;&#23548;&#24494;&#35843;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#25581;&#31034;&#20102;&#36825;&#20123;LLMs&#30340;&#24378;&#22823;&#38646;-shot&#25490;&#24207;&#33021;&#21147;&#65292;&#21516;&#26102;&#24378;&#35843;&#38500;&#38750;&#24494;&#35843;&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#38382;&#31572;&#29983;&#25104;&#20219;&#21153;&#65292;&#21542;&#21017;&#39069;&#22806;&#30340;&#25351;&#23548;&#24494;&#35843;&#21487;&#33021;&#20250;&#38477;&#20302;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26368;&#20808;&#36827;&#25490;&#24207;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#23558;&#22522;&#20110;LLM&#30340;QLMs&#19982;&#28151;&#21512;&#38646;-shot&#26816;&#32034;&#22120;&#38598;&#25104;&#65292;&#23637;&#31034;&#20102;&#22312;&#38646;-shot&#21644;&#23569;-shot&#22330;&#26223;&#20013;&#30340;&#20986;&#33394;&#25928;&#26524;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#20195;&#30721;&#24211;&#20844;&#24320;&#22312;https://github.com/ielab/llm-qlm&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of information retrieval, Query Likelihood Models (QLMs) rank documents based on the probability of generating the query given the content of a document. Recently, advanced large language models (LLMs) have emerged as effective QLMs, showcasing promising ranking capabilities. This paper focuses on investigating the genuine zero-shot ranking effectiveness of recent LLMs, which are solely pre-trained on unstructured text data without supervised instruction fine-tuning. Our findings reveal the robust zero-shot ranking ability of such LLMs, highlighting that additional instruction fine-tuning may hinder effectiveness unless a question generation task is present in the fine-tuning dataset. Furthermore, we introduce a novel state-of-the-art ranking system that integrates LLM-based QLMs with a hybrid zero-shot retriever, demonstrating exceptional effectiveness in both zero-shot and few-shot scenarios. We make our codebase publicly available at https://github.com/ielab/llm-qlm.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#20108;&#36827;&#21046;&#20195;&#30721;&#35780;&#35770;&#36136;&#37327;&#20998;&#31867;&#27169;&#22411;&#20013;&#25552;&#21319;&#24615;&#33021;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#29983;&#25104;&#25968;&#25454;&#38598;&#65292;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#24471;&#21040;&#20102;&#26174;&#33879;&#25913;&#21892;&#65292;&#25903;&#25345;&#21521;&#37327;&#26426;&#27169;&#22411;&#30340;&#31934;&#30830;&#24230;&#25552;&#39640;&#20102;6%&#65292;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#21484;&#22238;&#29575;&#25552;&#39640;&#20102;1.5%&#12290;</title><link>http://arxiv.org/abs/2310.13006</link><description>&lt;p&gt;
&#22522;&#20110;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#36719;&#20214;&#20803;&#25968;&#25454;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Software Metadata Classification based on Generative Artificial Intelligence. (arXiv:2310.13006v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13006
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#20108;&#36827;&#21046;&#20195;&#30721;&#35780;&#35770;&#36136;&#37327;&#20998;&#31867;&#27169;&#22411;&#20013;&#25552;&#21319;&#24615;&#33021;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#29983;&#25104;&#25968;&#25454;&#38598;&#65292;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#24471;&#21040;&#20102;&#26174;&#33879;&#25913;&#21892;&#65292;&#25903;&#25345;&#21521;&#37327;&#26426;&#27169;&#22411;&#30340;&#31934;&#30830;&#24230;&#25552;&#39640;&#20102;6%&#65292;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#21484;&#22238;&#29575;&#25552;&#39640;&#20102;1.5%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24212;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;(AI)&#26469;&#25552;&#39640;&#20108;&#36827;&#21046;&#20195;&#30721;&#35780;&#35770;&#36136;&#37327;&#20998;&#31867;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#21033;&#29992;OpenAI API&#65292;&#23558;&#20174;&#21508;&#31181;GitHub&#20179;&#24211;&#21644;&#24320;&#28304;&#39033;&#30446;&#20013;&#25552;&#21462;&#30340;1239&#20010;&#26032;&#29983;&#25104;&#30340;&#20195;&#30721;-&#35780;&#35770;&#23545;&#25968;&#25454;&#38598;&#26631;&#35760;&#20026;&#8220;&#26377;&#29992;&#8221;&#25110;&#8220;&#26080;&#29992;&#8221;&#65292;&#24182;&#19982;&#29616;&#26377;&#30340;9048&#20010;C&#32534;&#31243;&#35821;&#35328;&#23545;&#25968;&#25454;&#38598;&#38598;&#25104;&#12290;&#21033;&#29992;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26550;&#26500;&#65292;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#22312;&#27169;&#22411;&#20934;&#30830;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#26126;&#26174;&#30340;&#25913;&#21892;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#24403;&#38598;&#25104;&#21040;&#25903;&#25345;&#21521;&#37327;&#26426;(SVM)&#27169;&#22411;&#20013;&#26102;&#65292;&#31934;&#30830;&#24230;&#25552;&#39640;&#20102;6%&#65292;&#20174;0.79&#22686;&#21152;&#21040;0.85&#12290;&#27492;&#22806;&#65292;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;(ANN)&#27169;&#22411;&#30340;&#21484;&#22238;&#29575;&#25552;&#39640;&#20102;1.5%&#65292;&#20174;0.731&#22686;&#21152;&#21040;0.746&#12290;&#26412;&#25991;&#25581;&#31034;&#20102;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#22686;&#24378;&#20195;&#30721;&#35780;&#35770;&#36136;&#37327;&#20998;&#31867;&#27169;&#22411;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a novel approach to enhance the performance of binary code comment quality classification models through the application of Generative Artificial Intelligence (AI). By leveraging the OpenAI API, a dataset comprising 1239 newly generated code-comment pairs, extracted from various GitHub repositories and open-source projects, has been labelled as "Useful" or "Not Useful", and integrated into the existing corpus of 9048 pairs in the C programming language. Employing a cutting-edge Large Language Model Architecture, the generated dataset demonstrates notable improvements in model accuracy. Specifically, when incorporated into the Support Vector Machine (SVM) model, a 6% increase in precision is observed, rising from 0.79 to 0.85. Additionally, the Artificial Neural Network (ANN) model exhibits a 1.5% increase in recall, climbing from 0.731 to 0.746. This paper sheds light on the potential of Generative AI in augmenting code comment quality classification models. The res
&lt;/p&gt;</description></item><item><title>ConFIRM&#26159;&#19968;&#31181;&#20250;&#35805;&#24335;&#37329;&#34701;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#65292;&#36890;&#36807;&#21512;&#25104;&#37329;&#34701;&#39046;&#22495;&#29305;&#23450;&#38382;&#31572;&#23545;&#21644;&#35780;&#20272;&#21442;&#25968;&#24494;&#35843;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#36229;&#36807;90%&#30340;&#20934;&#30830;&#24615;&#65292;&#20026;&#37329;&#34701;&#23545;&#35805;&#31995;&#32479;&#25552;&#20379;&#20102;&#25968;&#25454;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2310.13001</link><description>&lt;p&gt;
&#20250;&#35805;&#24335;&#37329;&#34701;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#65288;ConFIRM&#65289;
&lt;/p&gt;
&lt;p&gt;
Conversational Financial Information Retrieval Model (ConFIRM). (arXiv:2310.13001v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13001
&lt;/p&gt;
&lt;p&gt;
ConFIRM&#26159;&#19968;&#31181;&#20250;&#35805;&#24335;&#37329;&#34701;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#65292;&#36890;&#36807;&#21512;&#25104;&#37329;&#34701;&#39046;&#22495;&#29305;&#23450;&#38382;&#31572;&#23545;&#21644;&#35780;&#20272;&#21442;&#25968;&#24494;&#35843;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#36229;&#36807;90%&#30340;&#20934;&#30830;&#24615;&#65292;&#20026;&#37329;&#34701;&#23545;&#35805;&#31995;&#32479;&#25552;&#20379;&#20102;&#25968;&#25454;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#21033;&#29992;&#23427;&#20204;&#22312;&#37329;&#34701;&#31561;&#19987;&#38376;&#39046;&#22495;&#30340;&#26032;&#20852;&#29305;&#24615;&#20855;&#26377;&#25506;&#32034;&#30340;&#20215;&#20540;&#12290;&#28982;&#32780;&#65292;&#37329;&#34701;&#31561;&#21463;&#30417;&#31649;&#39046;&#22495;&#20855;&#26377;&#29420;&#29305;&#30340;&#32422;&#26463;&#26465;&#20214;&#65292;&#38656;&#35201;&#20855;&#22791;&#38024;&#23545;&#35813;&#39046;&#22495;&#30340;&#20248;&#21270;&#26694;&#26550;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;ConFIRM&#65292;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#20250;&#35805;&#24335;&#37329;&#34701;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#65292;&#29992;&#20110;&#26597;&#35810;&#24847;&#22270;&#20998;&#31867;&#21644;&#30693;&#35782;&#24211;&#26631;&#35760;&#12290;ConFIRM&#21253;&#25324;&#20004;&#20010;&#27169;&#22359;&#65306;1&#65289;&#19968;&#31181;&#21512;&#25104;&#37329;&#34701;&#39046;&#22495;&#29305;&#23450;&#38382;&#31572;&#23545;&#30340;&#26041;&#27861;&#65292;&#20197;&#21450;2&#65289;&#35780;&#20272;&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#26469;&#36827;&#34892;&#26597;&#35810;&#20998;&#31867;&#20219;&#21153;&#12290;&#25105;&#20204;&#29983;&#25104;&#20102;&#19968;&#20010;&#21253;&#21547;4000&#22810;&#20010;&#26679;&#26412;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;&#21333;&#29420;&#30340;&#27979;&#35797;&#38598;&#19978;&#35780;&#20272;&#20102;&#20934;&#30830;&#24615;&#12290;ConFIRM&#23454;&#29616;&#20102;&#36229;&#36807;90%&#30340;&#20934;&#30830;&#24615;&#65292;&#36825;&#23545;&#20110;&#31526;&#21512;&#30417;&#31649;&#35201;&#27714;&#33267;&#20851;&#37325;&#35201;&#12290;ConFIRM&#25552;&#20379;&#20102;&#19968;&#31181;&#25968;&#25454;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#25552;&#21462;&#37329;&#34701;&#23545;&#35805;&#31995;&#32479;&#30340;&#31934;&#30830;&#26597;&#35810;&#24847;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the exponential growth in large language models (LLMs), leveraging their emergent properties for specialized domains like finance merits exploration. However, regulated fields such as finance pose unique constraints, requiring domain-optimized frameworks. We present ConFIRM, an LLM-based conversational financial information retrieval model tailored for query intent classification and knowledge base labeling.  ConFIRM comprises two modules:  1) a method to synthesize finance domain-specific question-answer pairs, and  2) evaluation of parameter efficient fine-tuning approaches for the query classification task. We generate a dataset of over 4000 samples, assessing accuracy on a separate test set.  ConFIRM achieved over 90% accuracy, essential for regulatory compliance. ConFIRM provides a data-efficient solution to extract precise query intent for financial dialog systems.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#31995;&#22270;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#25991;&#26723;&#32423;&#20851;&#31995;&#25277;&#21462;&#20013;&#24573;&#35270;&#20851;&#31995;&#30456;&#20851;&#24615;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#26500;&#24314;&#20851;&#31995;&#22270;&#12289;&#21152;&#26435;&#22788;&#29702;&#21644;&#21033;&#29992;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25429;&#25417;&#25991;&#26723;&#20013;&#20851;&#31995;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#20316;&#20026;&#29616;&#26377;&#27169;&#22411;&#30340;&#27169;&#22359;&#38598;&#25104;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.13000</link><description>&lt;p&gt;
&#20851;&#31995;&#30456;&#20851;&#24615;&#22686;&#24378;&#30340;&#25991;&#26723;&#32423;&#20851;&#31995;&#25277;&#21462;
&lt;/p&gt;
&lt;p&gt;
Document-Level Relation Extraction with Relation Correlation Enhancement. (arXiv:2310.13000v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13000
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#31995;&#22270;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#25991;&#26723;&#32423;&#20851;&#31995;&#25277;&#21462;&#20013;&#24573;&#35270;&#20851;&#31995;&#30456;&#20851;&#24615;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#26500;&#24314;&#20851;&#31995;&#22270;&#12289;&#21152;&#26435;&#22788;&#29702;&#21644;&#21033;&#29992;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25429;&#25417;&#25991;&#26723;&#20013;&#20851;&#31995;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#20316;&#20026;&#29616;&#26377;&#27169;&#22411;&#30340;&#27169;&#22359;&#38598;&#25104;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26723;&#32423;&#20851;&#31995;&#25277;&#21462;&#26159;&#19968;&#39033;&#33268;&#21147;&#20110;&#35782;&#21035;&#25991;&#26723;&#20869;&#23454;&#20307;&#38388;&#20851;&#31995;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25991;&#26723;&#32423;&#20851;&#31995;&#25277;&#21462;&#27169;&#22411;&#24448;&#24448;&#24573;&#35270;&#20102;&#20851;&#31995;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#32570;&#20047;&#23545;&#20851;&#31995;&#30456;&#20851;&#24615;&#30340;&#23450;&#37327;&#20998;&#26512;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#24182;&#26377;&#25928;&#25429;&#25417;&#25991;&#26723;&#32423;&#20851;&#31995;&#25277;&#21462;&#20013;&#30340;&#20851;&#31995;&#30456;&#20851;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#31995;&#22270;&#26041;&#27861;&#65292;&#26088;&#22312;&#26126;&#30830;&#21033;&#29992;&#20851;&#31995;&#20043;&#38388;&#30340;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#26500;&#24314;&#19968;&#20010;&#20851;&#31995;&#22270;&#65292;&#21033;&#29992;&#20808;&#21069;&#30340;&#20851;&#31995;&#30693;&#35782;&#23548;&#20986;&#30340;&#32479;&#35745;&#20849;&#29616;&#20449;&#24687;&#26469;&#24314;&#27169;&#20851;&#31995;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#37319;&#29992;&#37325;&#26032;&#21152;&#26435;&#30340;&#26041;&#27861;&#21019;&#24314;&#19968;&#20010;&#26377;&#25928;&#30340;&#20851;&#31995;&#30456;&#20851;&#24615;&#30697;&#38453;&#65292;&#20197;&#24341;&#23548;&#20851;&#31995;&#20449;&#24687;&#30340;&#20256;&#25773;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#26469;&#32858;&#21512;&#20851;&#31995;&#23884;&#20837;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#26080;&#32541;&#22320;&#20316;&#20026;&#19968;&#20010;&#21363;&#25554;&#21363;&#29992;&#30340;&#27169;&#22359;&#38598;&#25104;&#21040;&#29616;&#26377;&#30340;&#27169;&#22411;&#20013;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22686;&#24378;&#20851;&#31995;&#25277;&#21462;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Document-level relation extraction (DocRE) is a task that focuses on identifying relations between entities within a document. However, existing DocRE models often overlook the correlation between relations and lack a quantitative analysis of relation correlations. To address this limitation and effectively capture relation correlations in DocRE, we propose a relation graph method, which aims to explicitly exploit the interdependency among relations. Firstly, we construct a relation graph that models relation correlations using statistical co-occurrence information derived from prior relation knowledge. Secondly, we employ a re-weighting scheme to create an effective relation correlation matrix to guide the propagation of relation information. Furthermore, we leverage graph attention networks to aggregate relation embeddings. Importantly, our method can be seamlessly integrated as a plug-and-play module into existing models. Experimental results demonstrate that our approach can enhanc
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21453;&#39304;&#30340;&#35777;&#25454;&#26816;&#32034;&#22120;(FER)&#65292;&#36890;&#36807;&#25972;&#21512;&#22768;&#26126;&#39564;&#35777;&#32773;&#30340;&#21453;&#39304;&#26469;&#20248;&#21270;&#20107;&#23454;&#39564;&#35777;&#20013;&#30340;&#35777;&#25454;&#26816;&#32034;&#36807;&#31243;&#12290;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;FER&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20934;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.11675</link><description>&lt;p&gt;
&#20174;&#30456;&#20851;&#24615;&#21040;&#23454;&#29992;&#24615;: &#22522;&#20110;&#21453;&#39304;&#30340;&#35777;&#25454;&#26816;&#32034;&#22312;&#20107;&#23454;&#39564;&#35777;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
From Relevance to Utility: Evidence Retrieval with Feedback for Fact Verification. (arXiv:2310.11675v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11675
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21453;&#39304;&#30340;&#35777;&#25454;&#26816;&#32034;&#22120;(FER)&#65292;&#36890;&#36807;&#25972;&#21512;&#22768;&#26126;&#39564;&#35777;&#32773;&#30340;&#21453;&#39304;&#26469;&#20248;&#21270;&#20107;&#23454;&#39564;&#35777;&#20013;&#30340;&#35777;&#25454;&#26816;&#32034;&#36807;&#31243;&#12290;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;FER&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20107;&#23454;&#39564;&#35777;&#20013;&#65292;&#26816;&#32034;&#22686;&#24378;&#26041;&#27861;&#24050;&#25104;&#20026;&#20027;&#35201;&#30340;&#26041;&#27861;&#20043;&#19968;&#65307;&#23427;&#38656;&#35201;&#23545;&#22810;&#20010;&#26816;&#32034;&#21040;&#30340;&#35777;&#25454;&#36827;&#34892;&#25512;&#29702;&#65292;&#20197;&#39564;&#35777;&#22768;&#26126;&#30340;&#30495;&#23454;&#24615;&#12290;&#20026;&#20102;&#26816;&#32034;&#35777;&#25454;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#24120;&#20351;&#29992;&#22522;&#20110;&#27010;&#29575;&#25490;&#24207;&#21407;&#21017;&#35774;&#35745;&#30340;&#29616;&#25104;&#26816;&#32034;&#27169;&#22411;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#22312;&#20107;&#23454;&#39564;&#35777;&#20013;&#65292;&#25105;&#20204;&#38656;&#35201;&#20851;&#27880;&#30340;&#26159;&#22768;&#26126;&#39564;&#35777;&#32773;&#20174;&#26816;&#32034;&#21040;&#30340;&#35777;&#25454;&#20013;&#33719;&#24471;&#30340;&#23454;&#29992;&#24615;&#65292;&#32780;&#19981;&#26159;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#21453;&#39304;&#30340;&#35777;&#25454;&#26816;&#32034;&#22120;&#65288;FER&#65289;&#65292;&#36890;&#36807;&#25972;&#21512;&#22768;&#26126;&#39564;&#35777;&#32773;&#30340;&#21453;&#39304;&#26469;&#20248;&#21270;&#35777;&#25454;&#26816;&#32034;&#36807;&#31243;&#12290;&#20316;&#20026;&#21453;&#39304;&#20449;&#21495;&#65292;&#25105;&#20204;&#20351;&#29992;&#39564;&#35777;&#32773;&#26377;&#25928;&#21033;&#29992;&#26816;&#32034;&#21040;&#30340;&#35777;&#25454;&#21644;&#22522;&#20934;&#35777;&#25454;&#20043;&#38388;&#23454;&#29992;&#24615;&#30340;&#24046;&#24322;&#26469;&#20135;&#29983;&#26368;&#32456;&#30340;&#22768;&#26126;&#26631;&#31614;&#12290;&#23454;&#35777;&#30740;&#31350;&#35777;&#26126;FER&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Retrieval-enhanced methods have become a primary approach in fact verification (FV); it requires reasoning over multiple retrieved pieces of evidence to verify the integrity of a claim. To retrieve evidence, existing work often employs off-the-shelf retrieval models whose design is based on the probability ranking principle. We argue that, rather than relevance, for FV we need to focus on the utility that a claim verifier derives from the retrieved evidence. We introduce the feedback-based evidence retriever(FER) that optimizes the evidence retrieval process by incorporating feedback from the claim verifier. As a feedback signal we use the divergence in utility between how effectively the verifier utilizes the retrieved evidence and the ground-truth evidence to produce the final claim label. Empirical studies demonstrate the superiority of FER over prevailing baselines.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#33258;&#36866;&#24212;&#30340;NLP&#27169;&#22411;&#36873;&#25321;&#21644;&#20020;&#24202;&#19987;&#23478;&#35268;&#21017;&#30340;&#20998;&#31867;&#22120;&#65292;&#35813;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#25913;&#36827;VTE&#35782;&#21035;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#25918;&#23556;&#23398;&#25253;&#21578;&#20013;&#20934;&#30830;&#35782;&#21035;VTE&#20107;&#20214;&#30340;&#20934;&#30830;&#24615;&#24471;&#21040;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2309.12273</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#36866;&#24212;&#30340;NLP&#27169;&#22411;&#36873;&#25321;&#21644;&#22522;&#20110;&#20020;&#24202;&#19987;&#23478;&#35268;&#21017;&#30340;&#20998;&#31867;&#22120;&#25913;&#36827;VTE&#30340;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Improving VTE Identification through Adaptive NLP Model Selection and Clinical Expert Rule-based Classifier from Radiology Reports. (arXiv:2309.12273v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12273
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#33258;&#36866;&#24212;&#30340;NLP&#27169;&#22411;&#36873;&#25321;&#21644;&#20020;&#24202;&#19987;&#23478;&#35268;&#21017;&#30340;&#20998;&#31867;&#22120;&#65292;&#35813;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#25913;&#36827;VTE&#35782;&#21035;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#25918;&#23556;&#23398;&#25253;&#21578;&#20013;&#20934;&#30830;&#35782;&#21035;VTE&#20107;&#20214;&#30340;&#20934;&#30830;&#24615;&#24471;&#21040;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24555;&#36895;&#20934;&#30830;&#22320;&#35782;&#21035;&#38745;&#33033;&#34880;&#26643;&#26643;&#22622;&#65288;VTE&#65289;&#65292;&#21253;&#25324;&#28145;&#38745;&#33033;&#34880;&#26643;&#65288;DVT&#65289;&#21644;&#32954;&#26643;&#22622;&#65288;PE&#65289;&#65292;&#23545;&#20110;&#26377;&#25928;&#27835;&#30103;&#38750;&#24120;&#37325;&#35201;&#12290;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#22312;&#25918;&#23556;&#23398;&#25253;&#21578;&#20013;&#65292;&#33258;&#21160;&#21270;&#26041;&#27861;&#24050;&#32463;&#22312;&#20174;&#22238;&#39038;&#24615;&#25968;&#25454;&#38598;&#20013;&#35782;&#21035;VTE&#20107;&#20214;&#25110;&#24110;&#21161;&#20020;&#24202;&#19987;&#23478;&#35782;&#21035;&#25918;&#23556;&#23398;&#25253;&#21578;&#20013;&#30340;VTE&#20107;&#20214;&#26041;&#38754;&#23637;&#31034;&#20102;&#26377;&#24076;&#26395;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26631;&#35760;&#26377;&#38480;&#30340;&#21307;&#23398;&#25991;&#26412;&#25968;&#25454;&#12289;&#25918;&#23556;&#23398;&#25253;&#21578;&#30340;&#22797;&#26434;&#24615;&#21644;&#24322;&#36136;&#24615;&#20197;&#21450;&#25968;&#25454;&#19981;&#24179;&#34913;&#65292;&#26377;&#25928;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#21644;NLP&#27169;&#22411;&#23384;&#22312;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;DL&#26041;&#27861;&#30340;&#26032;&#30340;&#32452;&#21512;&#26041;&#27861;&#65292;&#32467;&#21512;&#25968;&#25454;&#22686;&#24378;&#12289;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;&#30340;NLP&#27169;&#22411;&#36873;&#25321;&#21644;&#20020;&#24202;&#19987;&#23478;NLP&#22522;&#20110;&#35268;&#21017;&#30340;&#20998;&#31867;&#22120;&#65292;&#20197;&#25552;&#39640;&#38750;&#32467;&#26500;&#21270;&#65288;&#33258;&#30001;&#25991;&#26412;&#65289;&#25918;&#23556;&#23398;&#25253;&#21578;&#20013;VTE&#35782;&#21035;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rapid and accurate identification of Venous thromboembolism (VTE), a severe cardiovascular condition including deep vein thrombosis (DVT) and pulmonary embolism (PE), is important for effective treatment. Leveraging Natural Language Processing (NLP) on radiology reports, automated methods have shown promising advancements in identifying VTE events from retrospective data cohorts or aiding clinical experts in identifying VTE events from radiology reports. However, effectively training Deep Learning (DL) and the NLP models is challenging due to limited labeled medical text data, the complexity and heterogeneity of radiology reports, and data imbalance. This study proposes novel method combinations of DL methods, along with data augmentation, adaptive pre-trained NLP model selection, and a clinical expert NLP rule-based classifier, to improve the accuracy of VTE identification in unstructured (free-text) radiology reports. Our experimental results demonstrate the model's efficacy, achievi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedDCSR&#30340;&#32852;&#37030;&#36328;&#39046;&#22495;&#39034;&#24207;&#25512;&#33616;&#26694;&#26550;&#65292;&#36890;&#36807;&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;&#26469;&#22788;&#29702;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#24207;&#21015;&#29305;&#24449;&#24322;&#36136;&#24615;&#65292;&#24182;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;</title><link>http://arxiv.org/abs/2309.08420</link><description>&lt;p&gt;
FedDCSR: &#36890;&#36807;&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;&#23454;&#29616;&#32852;&#37030;&#36328;&#39046;&#22495;&#39034;&#24207;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
FedDCSR: Federated Cross-domain Sequential Recommendation via Disentangled Representation Learning. (arXiv:2309.08420v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08420
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedDCSR&#30340;&#32852;&#37030;&#36328;&#39046;&#22495;&#39034;&#24207;&#25512;&#33616;&#26694;&#26550;&#65292;&#36890;&#36807;&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;&#26469;&#22788;&#29702;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#24207;&#21015;&#29305;&#24449;&#24322;&#36136;&#24615;&#65292;&#24182;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#21033;&#29992;&#26469;&#33258;&#22810;&#20010;&#39046;&#22495;&#30340;&#29992;&#25143;&#24207;&#21015;&#25968;&#25454;&#30340;&#36328;&#39046;&#22495;&#39034;&#24207;&#25512;&#33616;(CSR)&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;CSR&#26041;&#27861;&#38656;&#35201;&#22312;&#39046;&#22495;&#20043;&#38388;&#20849;&#20139;&#21407;&#22987;&#29992;&#25143;&#25968;&#25454;&#65292;&#36825;&#36829;&#21453;&#20102;&#12298;&#36890;&#29992;&#25968;&#25454;&#20445;&#25252;&#26465;&#20363;&#12299;(GDPR)&#12290;&#22240;&#27492;&#65292;&#26377;&#24517;&#35201;&#23558;&#32852;&#37030;&#23398;&#20064;(FL)&#21644;CSR&#30456;&#32467;&#21512;&#65292;&#20805;&#20998;&#21033;&#29992;&#19981;&#21516;&#39046;&#22495;&#30340;&#30693;&#35782;&#65292;&#21516;&#26102;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#24207;&#21015;&#29305;&#24449;&#24322;&#36136;&#24615;&#23545;FL&#30340;&#25972;&#20307;&#24615;&#33021;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FedDCSR&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#36807;&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;&#30340;&#26032;&#22411;&#32852;&#37030;&#36328;&#39046;&#22495;&#39034;&#24207;&#25512;&#33616;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20026;&#20102;&#35299;&#20915;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#24207;&#21015;&#29305;&#24449;&#24322;&#36136;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#39046;&#22495;&#20869;-&#39046;&#22495;&#38388;&#24207;&#21015;&#34920;&#31034;&#35299;&#32544;(SRD)&#30340;&#26041;&#27861;&#65292;&#23558;&#29992;&#25143;&#24207;&#21015;&#29305;&#24449;&#35299;&#32544;&#25104;&#39046;&#22495;&#20849;&#20139;&#21644;&#39046;&#22495;&#19987;&#23646;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-domain Sequential Recommendation (CSR) which leverages user sequence data from multiple domains has received extensive attention in recent years. However, the existing CSR methods require sharing origin user data across domains, which violates the General Data Protection Regulation (GDPR). Thus, it is necessary to combine federated learning (FL) and CSR to fully utilize knowledge from different domains while preserving data privacy. Nonetheless, the sequence feature heterogeneity across different domains significantly impacts the overall performance of FL. In this paper, we propose FedDCSR, a novel federated cross-domain sequential recommendation framework via disentangled representation learning. Specifically, to address the sequence feature heterogeneity across domains, we introduce an approach called inter-intra domain sequence representation disentanglement (SRD) to disentangle the user sequence features into domain-shared and domain-exclusive features. In addition, we design
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22810;&#20851;&#31995;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65288;RCL&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#20013;&#21333;&#19968;&#34892;&#20026;&#23398;&#20064;&#30340;&#38480;&#21046;&#12290;RCL&#27169;&#22411;&#36890;&#36807;&#22810;&#20851;&#31995;&#22270;&#32534;&#30721;&#22120;&#25429;&#25417;&#30701;&#26399;&#20559;&#22909;&#30340;&#24322;&#36136;&#24615;&#65292;&#24182;&#20351;&#29992;&#21160;&#24577;&#36328;&#20851;&#31995;&#35760;&#24518;&#32593;&#32476;&#26469;&#25429;&#25417;&#29992;&#25143;&#30340;&#38271;&#26399;&#22810;&#34892;&#20026;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2309.01103</link><description>&lt;p&gt;
&#22810;&#20851;&#31995;&#23545;&#27604;&#23398;&#20064;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Multi-Relational Contrastive Learning for Recommendation. (arXiv:2309.01103v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01103
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22810;&#20851;&#31995;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65288;RCL&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#20013;&#21333;&#19968;&#34892;&#20026;&#23398;&#20064;&#30340;&#38480;&#21046;&#12290;RCL&#27169;&#22411;&#36890;&#36807;&#22810;&#20851;&#31995;&#22270;&#32534;&#30721;&#22120;&#25429;&#25417;&#30701;&#26399;&#20559;&#22909;&#30340;&#24322;&#36136;&#24615;&#65292;&#24182;&#20351;&#29992;&#21160;&#24577;&#36328;&#20851;&#31995;&#35760;&#24518;&#32593;&#32476;&#26469;&#25429;&#25417;&#29992;&#25143;&#30340;&#38271;&#26399;&#22810;&#34892;&#20026;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#22312;&#25429;&#25417;&#29992;&#25143;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#20559;&#22909;&#12289;&#25552;&#20379;&#20934;&#30830;&#26377;&#25928;&#30340;&#25512;&#33616;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#25512;&#33616;&#27169;&#22411;&#21482;&#20381;&#36182;&#20110;&#19968;&#31181;&#34892;&#20026;&#23398;&#20064;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#34920;&#31034;&#29992;&#25143;&#21644;&#29289;&#21697;&#20043;&#38388;&#22797;&#26434;&#20851;&#31995;&#30340;&#33021;&#21147;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#29992;&#25143;&#20197;&#22810;&#31181;&#26041;&#24335;&#19982;&#29289;&#21697;&#20114;&#21160;&#65292;&#21253;&#25324;&#28857;&#20987;&#12289;&#26631;&#35760;&#20026;&#21916;&#29233;&#12289;&#35780;&#35770;&#21644;&#36141;&#20080;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20851;&#31995;&#24863;&#30693;&#23545;&#27604;&#23398;&#20064;&#65288;RCL&#65289;&#26694;&#26550;&#65292;&#23427;&#26377;&#25928;&#22320;&#24314;&#27169;&#21160;&#24577;&#20132;&#20114;&#24322;&#36136;&#24615;&#12290;RCL&#27169;&#22411;&#21253;&#25324;&#19968;&#20010;&#22810;&#20851;&#31995;&#22270;&#32534;&#30721;&#22120;&#65292;&#25429;&#25417;&#30701;&#26399;&#20559;&#22909;&#30340;&#24322;&#36136;&#24615;&#65292;&#21516;&#26102;&#20445;&#30041;&#19981;&#21516;&#31867;&#22411;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#30340;&#19987;&#29992;&#20851;&#31995;&#35821;&#20041;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21160;&#24577;&#36328;&#20851;&#31995;&#35760;&#24518;&#32593;&#32476;&#65292;&#20351;RCL&#27169;&#22411;&#33021;&#22815;&#25429;&#25417;&#29992;&#25143;&#30340;&#38271;&#26399;&#22810;&#34892;&#20026;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Personalized recommender systems play a crucial role in capturing users' evolving preferences over time to provide accurate and effective recommendations on various online platforms. However, many recommendation models rely on a single type of behavior learning, which limits their ability to represent the complex relationships between users and items in real-life scenarios. In such situations, users interact with items in multiple ways, including clicking, tagging as favorite, reviewing, and purchasing. To address this issue, we propose the Relation-aware Contrastive Learning (RCL) framework, which effectively models dynamic interaction heterogeneity. The RCL model incorporates a multi-relational graph encoder that captures short-term preference heterogeneity while preserving the dedicated relation semantics for different types of user-item interactions. Moreover, we design a dynamic cross-relational memory network that enables the RCL model to capture users' long-term multi-behavior p
&lt;/p&gt;</description></item><item><title>Jina Embeddings&#26159;&#19968;&#32452;&#39640;&#24615;&#33021;&#30340;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#65292;&#33021;&#22815;&#25429;&#25417;&#25991;&#26412;&#30340;&#35821;&#20041;&#26412;&#36136;&#12290;&#35813;&#35770;&#25991;&#35814;&#32454;&#20171;&#32461;&#20102;Jina Embeddings&#30340;&#24320;&#21457;&#36807;&#31243;&#65292;&#24182;&#36890;&#36807;&#24615;&#33021;&#35780;&#20272;&#39564;&#35777;&#20102;&#20854;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.11224</link><description>&lt;p&gt;
Jina Embeddings:&#19968;&#31181;&#26032;&#39062;&#30340;&#39640;&#24615;&#33021;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Jina Embeddings: A Novel Set of High-Performance Sentence Embedding Models. (arXiv:2307.11224v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11224
&lt;/p&gt;
&lt;p&gt;
Jina Embeddings&#26159;&#19968;&#32452;&#39640;&#24615;&#33021;&#30340;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#65292;&#33021;&#22815;&#25429;&#25417;&#25991;&#26412;&#30340;&#35821;&#20041;&#26412;&#36136;&#12290;&#35813;&#35770;&#25991;&#35814;&#32454;&#20171;&#32461;&#20102;Jina Embeddings&#30340;&#24320;&#21457;&#36807;&#31243;&#65292;&#24182;&#36890;&#36807;&#24615;&#33021;&#35780;&#20272;&#39564;&#35777;&#20102;&#20854;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Jina Embeddings&#30001;&#19968;&#32452;&#39640;&#24615;&#33021;&#30340;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#32452;&#25104;&#65292;&#33021;&#22815;&#23558;&#21508;&#31181;&#25991;&#26412;&#36755;&#20837;&#36716;&#21270;&#20026;&#25968;&#20540;&#34920;&#31034;&#65292;&#20174;&#32780;&#25429;&#25417;&#25991;&#26412;&#30340;&#35821;&#20041;&#26412;&#36136;&#12290;&#34429;&#28982;&#36825;&#20123;&#27169;&#22411;&#24182;&#38750;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#25991;&#26412;&#29983;&#25104;&#65292;&#20294;&#22312;&#23494;&#38598;&#26816;&#32034;&#21644;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#31561;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#26412;&#25991;&#35814;&#32454;&#20171;&#32461;&#20102;Jina Embeddings&#30340;&#24320;&#21457;&#36807;&#31243;&#65292;&#20174;&#21019;&#24314;&#39640;&#36136;&#37327;&#30340;&#25104;&#23545;&#21644;&#19977;&#20803;&#25968;&#25454;&#38598;&#24320;&#22987;&#12290;&#23427;&#24378;&#35843;&#20102;&#25968;&#25454;&#28165;&#29702;&#22312;&#25968;&#25454;&#38598;&#20934;&#22791;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#65292;&#24182;&#23545;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#36827;&#34892;&#20102;&#28145;&#20837;&#25506;&#35752;&#65292;&#26368;&#21518;&#21033;&#29992;Massive Textual Embedding Benchmark&#65288;MTEB&#65289;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#24615;&#33021;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Jina Embeddings constitutes a set of high-performance sentence embedding models adept at translating various textual inputs into numerical representations, thereby capturing the semantic essence of the text. While these models are not exclusively designed for text generation, they excel in applications such as dense retrieval and semantic textual similarity. This paper details the development of Jina Embeddings, starting with the creation of a high-quality pairwise and triplet dataset. It underlines the crucial role of data cleaning in dataset preparation, gives in-depth insights into the model training process, and concludes with a comprehensive performance evaluation using the Massive Textual Embedding Benchmark (MTEB).
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#32467;&#26500;&#21270;&#34920;&#26684;&#25968;&#25454;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#19981;&#21516;&#30340;&#36755;&#20837;&#36873;&#25321;&#20250;&#23545;&#24615;&#33021;&#20135;&#29983;&#24433;&#21709;&#12290;&#22312;&#22522;&#20934;&#27979;&#35797;&#30340;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#8220;&#33258;&#25105;&#22686;&#24378;&#8221;&#25216;&#26415;&#20197;&#25913;&#21892;&#29702;&#35299;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.13062</link><description>&lt;p&gt;
GPT4Table&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#29702;&#35299;&#32467;&#26500;&#21270;&#34920;&#26684;&#25968;&#25454;&#21527;&#65311;&#19968;&#39033;&#22522;&#20934;&#27979;&#35797;&#21644;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
GPT4Table: Can Large Language Models Understand Structured Table Data? A Benchmark and Empirical Study. (arXiv:2305.13062v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13062
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#32467;&#26500;&#21270;&#34920;&#26684;&#25968;&#25454;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#19981;&#21516;&#30340;&#36755;&#20837;&#36873;&#25321;&#20250;&#23545;&#24615;&#33021;&#20135;&#29983;&#24433;&#21709;&#12290;&#22312;&#22522;&#20934;&#27979;&#35797;&#30340;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#8220;&#33258;&#25105;&#22686;&#24378;&#8221;&#25216;&#26415;&#20197;&#25913;&#21892;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#23569;&#26679;&#26412;&#25512;&#29702;&#22120;&#26469;&#35299;&#20915;&#19982;&#33258;&#28982;&#35821;&#35328;&#30456;&#20851;&#30340;&#20219;&#21153;&#36234;&#26469;&#36234;&#20855;&#21560;&#24341;&#21147;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;LLMs&#23545;&#32467;&#26500;&#21270;&#25968;&#25454;&#65288;&#20363;&#22914;&#34920;&#26684;&#65289;&#30340;&#29702;&#35299;&#31243;&#24230;&#36824;&#26377;&#24456;&#22810;&#38656;&#35201;&#23398;&#20064;&#30340;&#22320;&#26041;&#12290;&#23613;&#31649;&#21487;&#20197;&#20351;&#29992;&#34920;&#26684;&#24207;&#21015;&#21270;&#20316;&#20026;LLMs&#30340;&#36755;&#20837;&#65292;&#20294;&#30446;&#21069;&#36824;&#32570;&#20047;&#23545;LLMs&#26159;&#21542;&#30495;&#27491;&#33021;&#22815;&#29702;&#35299;&#36825;&#31867;&#25968;&#25454;&#30340;&#20840;&#38754;&#30740;&#31350;&#12290;&#26412;&#25991;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#26469;&#35780;&#20272;LLMs&#30340;&#32467;&#26500;&#29702;&#35299;&#33021;&#21147;&#65288;SUC&#65289;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#21019;&#24314;&#30340;&#22522;&#20934;&#27979;&#35797;&#21253;&#25324;&#19971;&#20010;&#20219;&#21153;&#65292;&#27599;&#20010;&#20219;&#21153;&#37117;&#26377;&#20854;&#29420;&#29305;&#30340;&#25361;&#25112;&#65292;&#20363;&#22914;&#21333;&#20803;&#26684;&#26597;&#25214;&#12289;&#34892;&#26816;&#32034;&#21644;&#22823;&#23567;&#26816;&#27979;&#12290;&#25105;&#20204;&#23545;GPT-3.5&#21644;GPT-4&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#35780;&#20272;&#12290;&#25105;&#20204;&#21457;&#29616;&#24615;&#33021;&#22240;&#22810;&#31181;&#36755;&#20837;&#36873;&#25321;&#32780;&#24322;&#65292;&#21253;&#25324;&#34920;&#26684;&#36755;&#20837;&#26684;&#24335;&#12289;&#20869;&#23481;&#39034;&#24207;&#12289;&#35282;&#33394;&#25552;&#31034;&#21644;&#20998;&#21306;&#26631;&#35760;&#31561;&#12290;&#26681;&#25454;&#22522;&#20934;&#27979;&#35797;&#35780;&#20272;&#25152;&#24471;&#30340;&#35265;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#33258;&#25105;&#22686;&#24378;&#8221;&#25216;&#26415;&#20197;&#25913;&#21892;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are becoming attractive as few-shot reasoners to solve Natural Language (NL)-related tasks. However, there is still much to learn about how well LLMs understand structured data, such as tables. While it is true that tables can be used as inputs to LLMs with serialization, there lack of comprehensive studies examining whether LLMs can truly comprehend such data. In this paper, we try to understand this by designing a benchmark to evaluate the structural understanding capabilities (SUC) of LLMs. The benchmark we create includes seven tasks, each with its own unique challenges, \eg, cell lookup, row retrieval, and size detection. We run a series of evaluations on GPT-3.5 and GPT-4. We discover that the performance varied depending on a number of input choices, including table input format, content order, role prompting, and partition marks. Drawing from the insights gained through the benchmark evaluations, we then propose \textit{self-augmentation} for effect
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25552;&#31034;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25991;&#26412;&#25628;&#32034;&#24847;&#22270;&#35299;&#37322;&#22120;&#26469;&#24110;&#21161;&#20250;&#35805;&#25628;&#32034;&#12290;&#36890;&#36807;&#25506;&#32034;&#19977;&#31181;&#25552;&#31034;&#26041;&#27861;&#65292;&#20174;&#32780;&#29983;&#25104;&#22810;&#20010;&#26597;&#35810;&#37325;&#20889;&#21644;&#20551;&#35774;&#24615;&#22238;&#22797;&#65292;&#24182;&#23558;&#23427;&#20204;&#32858;&#21512;&#25104;&#19968;&#20010;&#38598;&#25104;&#34920;&#31034;&#20197;&#34920;&#31034;&#29992;&#25143;&#30340;&#30495;&#23454;&#19978;&#19979;&#25991;&#25628;&#32034;&#24847;&#22270;&#12290;</title><link>http://arxiv.org/abs/2303.06573</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20102;&#35299;&#24744;&#30340;&#19978;&#19979;&#25991;&#25628;&#32034;&#24847;&#22270;&#65306;&#29992;&#20110;&#20250;&#35805;&#25628;&#32034;&#30340;&#25552;&#31034;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Large Language Models Know Your Contextual Search Intent: A Prompting Framework for Conversational Search. (arXiv:2303.06573v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06573
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25552;&#31034;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25991;&#26412;&#25628;&#32034;&#24847;&#22270;&#35299;&#37322;&#22120;&#26469;&#24110;&#21161;&#20250;&#35805;&#25628;&#32034;&#12290;&#36890;&#36807;&#25506;&#32034;&#19977;&#31181;&#25552;&#31034;&#26041;&#27861;&#65292;&#20174;&#32780;&#29983;&#25104;&#22810;&#20010;&#26597;&#35810;&#37325;&#20889;&#21644;&#20551;&#35774;&#24615;&#22238;&#22797;&#65292;&#24182;&#23558;&#23427;&#20204;&#32858;&#21512;&#25104;&#19968;&#20010;&#38598;&#25104;&#34920;&#31034;&#20197;&#34920;&#31034;&#29992;&#25143;&#30340;&#30495;&#23454;&#19978;&#19979;&#25991;&#25628;&#32034;&#24847;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#29702;&#35299;&#29992;&#25143;&#30340;&#19978;&#19979;&#25991;&#25628;&#32034;&#24847;&#22270;&#23545;&#20110;&#20250;&#35805;&#25628;&#32034;&#32780;&#35328;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#12290;&#30001;&#20110;&#23545;&#35805;&#25628;&#32034;&#20250;&#35805;&#30340;&#22810;&#26679;&#24615;&#21644;&#38271;&#23614;&#24615;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#26377;&#38480;&#25968;&#25454;&#35757;&#32451;&#30340;&#26041;&#27861;&#22312;&#22788;&#29702;&#30495;&#23454;&#30340;&#20250;&#35805;&#25628;&#32034;&#22330;&#26223;&#26102;&#20173;&#28982;&#26174;&#31034;&#20986;&#19981;&#20196;&#20154;&#28385;&#24847;&#30340;&#25928;&#26524;&#21644;&#40065;&#26834;&#24615;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23637;&#31034;&#20102;&#20196;&#20154;&#24778;&#21497;&#30340;&#25991;&#26412;&#29983;&#25104;&#21644;&#23545;&#35805;&#29702;&#35299;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#25552;&#31034;&#26694;&#26550;&#65292;&#21517;&#20026;LLM4CS&#65292;&#21033;&#29992;LLM&#20316;&#20026;&#22522;&#20110;&#25991;&#26412;&#30340;&#25628;&#32034;&#24847;&#22270;&#35299;&#37322;&#22120;&#26469;&#24110;&#21161;&#20250;&#35805;&#25628;&#32034;&#12290;&#22312;&#35813;&#26694;&#26550;&#19979;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19977;&#31181;&#25552;&#31034;&#26041;&#27861;&#26469;&#29983;&#25104;&#22810;&#20010;&#26597;&#35810;&#37325;&#20889;&#21644;&#20551;&#35774;&#24615;&#22238;&#22797;&#65292;&#24182;&#25552;&#35758;&#23558;&#23427;&#20204;&#32858;&#21512;&#21040;&#19968;&#20010;&#38598;&#25104;&#34920;&#31034;&#20013;&#65292;&#36825;&#21487;&#20197;&#31283;&#20581;&#22320;&#34920;&#31034;&#29992;&#25143;&#30340;&#30495;&#23454;&#19978;&#19979;&#25991;&#25628;&#32034;&#24847;&#22270;&#12290;&#22312;&#19977;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#20250;&#35805;&#25628;&#32034;&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#33258;&#21160;&#35780;&#20272;&#21644;&#20154;&#24037;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Precisely understanding users' contextual search intent has been an important challenge for conversational search. As conversational search sessions are much more diverse and long-tailed, existing methods trained on limited data still show unsatisfactory effectiveness and robustness to handle real conversational search scenarios. Recently, large language models (LLMs) have demonstrated amazing capabilities for text generation and conversation understanding. In this work, we present a simple yet effective prompting framework, called LLM4CS, to leverage LLMs as a text-based search intent interpreter to help conversational search. Under this framework, we explore three prompting methods to generate multiple query rewrites and hypothetical responses, and propose to aggregate them into an integrated representation that can robustly represent the user's real contextual search intent. Extensive automatic evaluations and human evaluations on three widely used conversational search benchmarks, 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20110;&#25361;&#25112;&#30340;&#30005;&#23376;&#25307;&#32856;&#25512;&#33616;&#31995;&#32479;&#35843;&#26597;&#65292;&#35813;&#35843;&#26597;&#20197;&#23454;&#29992;&#24615;&#20026;&#23548;&#21521;&#65292;&#36866;&#29992;&#20110;&#38754;&#20020;&#20855;&#20307;&#25307;&#32856;&#35774;&#35745;&#20219;&#21153;&#21644;&#25361;&#25112;&#30340;&#24320;&#21457;&#20154;&#21592;&#12290;</title><link>http://arxiv.org/abs/2209.05112</link><description>&lt;p&gt;
&#19968;&#20010;&#22522;&#20110;&#25361;&#25112;&#30340;&#30005;&#23376;&#25307;&#32856;&#25512;&#33616;&#31995;&#32479;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A challenge-based survey of e-recruitment recommendation systems. (arXiv:2209.05112v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.05112
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20110;&#25361;&#25112;&#30340;&#30005;&#23376;&#25307;&#32856;&#25512;&#33616;&#31995;&#32479;&#35843;&#26597;&#65292;&#35813;&#35843;&#26597;&#20197;&#23454;&#29992;&#24615;&#20026;&#23548;&#21521;&#65292;&#36866;&#29992;&#20110;&#38754;&#20020;&#20855;&#20307;&#25307;&#32856;&#35774;&#35745;&#20219;&#21153;&#21644;&#25361;&#25112;&#30340;&#24320;&#21457;&#20154;&#21592;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#25307;&#32856;&#25512;&#33616;&#31995;&#32479;&#20250;&#20026;&#27714;&#32844;&#32773;&#25512;&#33616;&#24037;&#20316;&#65292;&#20063;&#20250;&#20026;&#25307;&#32856;&#32773;&#25512;&#33616;&#27714;&#32844;&#32773;&#12290;&#25512;&#33616;&#26159;&#26681;&#25454;&#27714;&#32844;&#32773;&#36866;&#21512;&#32844;&#20301;&#20197;&#21450;&#27714;&#32844;&#32773;&#21644;&#25307;&#32856;&#32773;&#30340;&#20559;&#22909;&#29983;&#25104;&#30340;&#12290;&#22240;&#27492;&#65292;&#30005;&#23376;&#25307;&#32856;&#25512;&#33616;&#31995;&#32479;&#23545;&#27714;&#32844;&#32773;&#30340;&#32844;&#19994;&#21457;&#23637;&#26377;&#30528;&#37325;&#35201;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#24433;&#21709;&#20844;&#21496;&#30340;&#25307;&#32856;&#27969;&#31243;&#65292;&#30005;&#23376;&#25307;&#32856;&#25512;&#33616;&#31995;&#32479;&#22312;&#22609;&#36896;&#20844;&#21496;&#22312;&#24066;&#22330;&#30340;&#31454;&#20105;&#20248;&#21183;&#26041;&#38754;&#25198;&#28436;&#30528;&#37325;&#35201;&#35282;&#33394;&#12290;&#22240;&#27492;&#65292;&#30005;&#23376;&#25307;&#32856;&#25512;&#33616;&#39046;&#22495;&#20540;&#24471;&#29305;&#21035;&#20851;&#27880;&#12290;&#29616;&#26377;&#30340;&#35843;&#26597;&#24448;&#24448;&#20174;&#31639;&#27861;&#30340;&#35282;&#24230;&#35752;&#35770;&#36807;&#21435;&#30340;&#30740;&#31350;&#65292;&#20363;&#22914;&#23558;&#23427;&#20204;&#20998;&#20026;&#21327;&#21516;&#36807;&#28388;&#12289;&#22522;&#20110;&#20869;&#23481;&#21644;&#28151;&#21512;&#26041;&#27861;&#12290;&#32780;&#36825;&#39033;&#35843;&#26597;&#21017;&#37319;&#21462;&#20102;&#19968;&#20010;&#20114;&#34917;&#30340;&#12289;&#22522;&#20110;&#25361;&#25112;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#35748;&#20026;&#36825;&#23545;&#20110;&#38754;&#20020;&#20855;&#20307;&#25307;&#32856;&#35774;&#35745;&#20219;&#21153;&#21644;&#19968;&#31995;&#21015;&#20855;&#20307;&#25361;&#25112;&#30340;&#24320;&#21457;&#20154;&#21592;&#26356;&#21152;&#23454;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
E-recruitment recommendation systems recommend jobs to job seekers and job seekers to recruiters. The recommendations are generated based on the suitability of the job seekers for the positions as well as the job seekers' and the recruiters' preferences. Therefore, e-recruitment recommendation systems could greatly impact job seekers' careers. Moreover, by affecting the hiring processes of the companies, e-recruitment recommendation systems play an important role in shaping the companies' competitive edge in the market. Hence, the domain of e-recruitment recommendation deserves specific attention. Existing surveys on this topic tend to discuss past studies from the algorithmic perspective, e.g., by categorizing them into collaborative filtering, content based, and hybrid methods. This survey, instead, takes a complementary, challenge-based approach, which we believe might be more practical to developers facing a concrete e-recruitment design task with a specific set of challenges, as w
&lt;/p&gt;</description></item></channel></rss>