<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;CVaR&#30340;&#26041;&#27861;&#26469;&#25913;&#36827;&#38271;&#23614;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#24182;&#20855;&#22791;&#22362;&#23454;&#30340;&#29702;&#35770;&#22522;&#30784;&#12290;&#31532;&#19968;&#31181;&#26041;&#27861;&#26159;&#24341;&#20837;&#20102;&#19968;&#31181;&#26631;&#31614;&#24863;&#30693;&#26377;&#30028;CVaR&#65288;LAB-CVaR&#65289;&#25439;&#22833;&#20989;&#25968;&#65292;&#35299;&#20915;&#20102;&#21407;&#22987;CVaR&#30340;&#24754;&#35266;&#32467;&#26524;&#65292;&#36890;&#36807;&#35774;&#35745;&#26368;&#20248;&#26435;&#37325;&#19978;&#19979;&#30028;&#36827;&#34892;&#25913;&#36827;&#65307;&#31532;&#20108;&#31181;&#26041;&#27861;&#26159;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;logit&#35843;&#25972;&#30340;LAB-CVaR&#65288;LAB-CVaR-logit&#65289;&#25439;&#22833;&#20989;&#25968;&#65292;&#20197;&#31283;&#23450;&#20248;&#21270;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2308.15405</link><description>&lt;p&gt;
&#36890;&#36807;&#26631;&#31614;&#24863;&#30693;&#26377;&#30028;CVaR&#23454;&#29616;&#40065;&#26834;&#30340;&#38271;&#23614;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Robust Long-Tailed Learning via Label-Aware Bounded CVaR. (arXiv:2308.15405v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15405
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;CVaR&#30340;&#26041;&#27861;&#26469;&#25913;&#36827;&#38271;&#23614;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#24182;&#20855;&#22791;&#22362;&#23454;&#30340;&#29702;&#35770;&#22522;&#30784;&#12290;&#31532;&#19968;&#31181;&#26041;&#27861;&#26159;&#24341;&#20837;&#20102;&#19968;&#31181;&#26631;&#31614;&#24863;&#30693;&#26377;&#30028;CVaR&#65288;LAB-CVaR&#65289;&#25439;&#22833;&#20989;&#25968;&#65292;&#35299;&#20915;&#20102;&#21407;&#22987;CVaR&#30340;&#24754;&#35266;&#32467;&#26524;&#65292;&#36890;&#36807;&#35774;&#35745;&#26368;&#20248;&#26435;&#37325;&#19978;&#19979;&#30028;&#36827;&#34892;&#25913;&#36827;&#65307;&#31532;&#20108;&#31181;&#26041;&#27861;&#26159;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;logit&#35843;&#25972;&#30340;LAB-CVaR&#65288;LAB-CVaR-logit&#65289;&#25439;&#22833;&#20989;&#25968;&#65292;&#20197;&#31283;&#23450;&#20248;&#21270;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#20998;&#31867;&#38382;&#39064;&#20013;&#65292;&#25968;&#25454;&#24448;&#24448;&#26159;&#19981;&#24179;&#34913;&#25110;&#38271;&#23614;&#20998;&#24067;&#30340;&#65292;&#20854;&#20013;&#22823;&#22810;&#25968;&#31867;&#21035;&#25317;&#26377;&#22823;&#37096;&#20998;&#26679;&#26412;&#65292;&#24182;&#20027;&#23548;&#27169;&#22411;&#35757;&#32451;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#26222;&#36890;&#27169;&#22411;&#24448;&#24448;&#22312;&#23569;&#25968;&#31867;&#19978;&#34920;&#29616;&#36739;&#24046;&#12290;&#20043;&#21069;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#25439;&#22833;&#20989;&#25968;&#20462;&#27491;&#26041;&#27861;&#26469;&#35299;&#20915;&#38271;&#23614;&#23398;&#20064;&#38382;&#39064;&#65292;&#28982;&#32780;&#36825;&#20123;&#26041;&#27861;&#35201;&#20040;&#23545;&#21516;&#19968;&#31867;&#21035;&#30340;&#26679;&#26412;&#28448;&#19981;&#20851;&#24515;&#65292;&#35201;&#20040;&#32570;&#20047;&#29702;&#35770;&#20445;&#35777;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;CVaR&#65288;&#26465;&#20214;&#20215;&#20540;-at-Risk&#65289;&#30340;&#26032;&#26041;&#27861;&#26469;&#25552;&#39640;&#38271;&#23614;&#23398;&#20064;&#30340;&#24615;&#33021;&#24182;&#20855;&#22791;&#22362;&#23454;&#30340;&#29702;&#35770;&#22522;&#30784;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#19968;&#31181;&#26631;&#31614;&#24863;&#30693;&#26377;&#30028;CVaR&#65288;LAB-CVaR&#65289;&#25439;&#22833;&#20989;&#25968;&#26469;&#20811;&#26381;&#21407;&#22987;CVaR&#24754;&#35266;&#30340;&#32467;&#26524;&#65292;&#24182;&#22312;&#29702;&#35770;&#19978;&#35774;&#35745;&#20102;LAB-CVaR&#30340;&#26368;&#20248;&#26435;&#37325;&#19978;&#19979;&#30028;&#12290;&#22522;&#20110;LAB-CVaR&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;logit&#35843;&#25972;&#30340;LAB-CVaR&#65288;LAB-CVaR-logit&#65289;&#25439;&#22833;&#20989;&#25968;&#65292;&#20197;&#31283;&#23450;&#20248;&#21270;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data in the real-world classification problems are always imbalanced or long-tailed, wherein the majority classes have the most of the samples that dominate the model training. In such setting, the naive model tends to have poor performance on the minority classes. Previously, a variety of loss modifications have been proposed to address the long-tailed leaning problem, while these methods either treat the samples in the same class indiscriminatingly or lack a theoretical guarantee. In this paper, we propose two novel approaches based on CVaR (Conditional Value at Risk) to improve the performance of long-tailed learning with a solid theoretical ground. Specifically, we firstly introduce a Label-Aware Bounded CVaR (LAB-CVaR) loss to overcome the pessimistic result of the original CVaR, and further design the optimal weight bounds for LAB-CVaR theoretically. Based on LAB-CVaR, we additionally propose a LAB-CVaR with logit adjustment (LAB-CVaR-logit) loss to stabilize the optimization pro
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#35282;&#24230;&#23398;&#20064;&#25490;&#24207;&#26041;&#27861;&#65292;&#33021;&#22815;&#36890;&#36807;&#20998;&#26512;URL&#12289;&#29255;&#27573;&#21644;&#39029;&#38754;&#26631;&#39064;&#65292;&#20248;&#20808;&#32771;&#34385;&#39640;&#25945;&#32946;&#19968;&#33268;&#24615;&#12289;&#36866;&#24403;&#24615;&#21644;&#21487;&#35835;&#24615;&#30340;Web&#36164;&#28304;&#65292;&#20174;&#32780;&#25913;&#36827;&#20799;&#31461;&#20449;&#24687;&#25628;&#32034;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.15265</link><description>&lt;p&gt;
&#19968;&#31181;&#22810;&#35282;&#24230;&#23398;&#20064;&#25490;&#24207;&#26041;&#27861;&#65292;&#25903;&#25345;&#25945;&#23460;&#20013;&#20799;&#31461;&#30340;&#20449;&#24687;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
A Multi-Perspective Learning to Rank Approach to Support Children's Information Seeking in the Classroom. (arXiv:2308.15265v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15265
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#35282;&#24230;&#23398;&#20064;&#25490;&#24207;&#26041;&#27861;&#65292;&#33021;&#22815;&#36890;&#36807;&#20998;&#26512;URL&#12289;&#29255;&#27573;&#21644;&#39029;&#38754;&#26631;&#39064;&#65292;&#20248;&#20808;&#32771;&#34385;&#39640;&#25945;&#32946;&#19968;&#33268;&#24615;&#12289;&#36866;&#24403;&#24615;&#21644;&#21487;&#35835;&#24615;&#30340;Web&#36164;&#28304;&#65292;&#20174;&#32780;&#25913;&#36827;&#20799;&#31461;&#20449;&#24687;&#25628;&#32034;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#37325;&#26032;&#25490;&#24207;&#27169;&#22411;&#65292;&#26088;&#22312;&#22686;&#24378;&#26631;&#20934;&#25628;&#32034;&#24341;&#25806;&#30340;&#21151;&#33021;&#65292;&#20197;&#25903;&#25345;6&#33267;11&#23681;&#20799;&#31461;&#30340;&#35838;&#22530;&#25628;&#32034;&#27963;&#21160;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#24179;&#34913;&#39118;&#38505;&#21644;&#22238;&#25253;&#25193;&#23637;&#20102;&#24050;&#30693;&#30340;&#25972;&#20307;&#25490;&#24207;&#23398;&#20064;&#26694;&#26550;&#12290;&#36890;&#36807;&#20998;&#26512;&#32473;&#23450;&#20027;&#27969;&#25628;&#32034;&#24341;&#25806;&#26816;&#32034;&#21040;&#30340;Web&#36164;&#28304;&#30340;URL&#12289;&#29255;&#27573;&#21644;&#39029;&#38754;&#26631;&#39064;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#20248;&#20808;&#32771;&#34385;&#39640;&#25945;&#32946;&#19968;&#33268;&#24615;&#12289;&#36866;&#24403;&#24615;&#21644;&#21487;&#35835;&#24615;&#30340;Web&#36164;&#28304;&#12290;&#23454;&#39564;&#32467;&#26524;&#21253;&#25324;&#28040;&#34701;&#30740;&#31350;&#21644;&#19982;&#29616;&#26377;&#22522;&#32447;&#30340;&#27604;&#36739;&#65292;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#27169;&#22411;&#30340;&#27491;&#30830;&#24615;&#12290;&#36825;&#39033;&#24037;&#20316;&#30340;&#32467;&#26524;&#35777;&#26126;&#20102;&#22312;&#35774;&#35745;&#33021;&#22815;&#26356;&#22909;&#22320;&#25903;&#25345;&#20799;&#31461;&#20449;&#24687;&#21457;&#29616;&#30340;&#31639;&#27861;&#26102;&#65292;&#32771;&#34385;&#21040;&#25945;&#23460;&#29615;&#22659;&#20013;&#30340;&#22810;&#20010;&#35270;&#35282;&#30340;&#20215;&#20540;&#65292;&#20363;&#22914;&#25945;&#32946;&#19968;&#33268;&#24615;&#12289;&#21487;&#35835;&#24615;&#21644;&#21453;&#23545;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a novel re-ranking model that aims to augment the functionality of standard search engines to support classroom search activities for children (ages 6 to 11). This model extends the known listwise learning-to-rank framework by balancing risk and reward. Doing so enables the model to prioritize Web resources of high educational alignment, appropriateness, and adequate readability by analyzing the URLs, snippets, and page titles of Web resources retrieved by a given mainstream search engine. Experimental results, including an ablation study and comparisons with existing baselines, showcase the correctness of the proposed model. The outcomes of this work demonstrate the value of considering multiple perspectives inherent to the classroom setting, e.g., educational alignment, readability, and objectionability, when applied to the design of algorithms that can better support children's information discovery.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#30340;&#22810;&#37325;&#33258;&#36866;&#24212;&#31354;&#38388;&#34701;&#21512;&#25512;&#33616;&#26041;&#27861;&#65292;&#36890;&#36807;&#32479;&#19968;&#30340;&#31354;&#38388;&#26469;&#34701;&#21512;&#21452;&#26354;&#12289;&#27431;&#20960;&#37324;&#24471;&#21644;&#29699;&#38754;&#31354;&#38388;&#65292;&#24182;&#21033;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#25552;&#39640;&#20102;&#30693;&#35782;&#20256;&#25773;&#30340;&#23884;&#20837;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2308.15244</link><description>&lt;p&gt;
&#22522;&#20110;&#30693;&#35782;&#30340;&#22810;&#37325;&#33258;&#36866;&#24212;&#31354;&#38388;&#34701;&#21512;&#25512;&#33616;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Knowledge-based Multiple Adaptive Spaces Fusion for Recommendation. (arXiv:2308.15244v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15244
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#30340;&#22810;&#37325;&#33258;&#36866;&#24212;&#31354;&#38388;&#34701;&#21512;&#25512;&#33616;&#26041;&#27861;&#65292;&#36890;&#36807;&#32479;&#19968;&#30340;&#31354;&#38388;&#26469;&#34701;&#21512;&#21452;&#26354;&#12289;&#27431;&#20960;&#37324;&#24471;&#21644;&#29699;&#38754;&#31354;&#38388;&#65292;&#24182;&#21033;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#25552;&#39640;&#20102;&#30693;&#35782;&#20256;&#25773;&#30340;&#23884;&#20837;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#30693;&#35782;&#22270;&#35889;&#21253;&#21547;&#20016;&#23500;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#36817;&#24180;&#26469;&#19968;&#20123;KG-enhanced&#25512;&#33616;&#26041;&#27861;&#23618;&#20986;&#19981;&#31351;&#12290;&#29616;&#26377;&#26041;&#27861;&#22823;&#22810;&#26159;&#22522;&#20110;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#35774;&#35745;&#30340;&#65292;&#27809;&#26377;&#32771;&#34385;&#26354;&#29575;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#24040;&#22823;&#30340;&#22270;&#32467;&#26500;&#25968;&#25454;&#34920;&#29616;&#20986;&#39640;&#24230;&#38750;&#27431;&#20960;&#37324;&#24471;&#30340;&#29305;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21463;&#21040;&#36825;&#20123;&#35266;&#23519;&#30340;&#21551;&#21457;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#30340;&#22810;&#37325;&#33258;&#36866;&#24212;&#31354;&#38388;&#34701;&#21512;&#25512;&#33616;&#26041;&#27861;&#65292;&#31216;&#20026;MCKG&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#20165;&#37319;&#29992;&#29305;&#23450;&#27969;&#24418;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19982;&#21452;&#26354;&#12289;&#27431;&#20960;&#37324;&#24471;&#21644;&#29699;&#38754;&#31354;&#38388;&#20860;&#23481;&#30340;&#32479;&#19968;&#31354;&#38388;&#12290;&#27492;&#22806;&#65292;&#22312;&#27880;&#24847;&#21147;&#26041;&#24335;&#19979;&#65292;&#25105;&#20204;&#34701;&#21512;&#20102;&#22810;&#20010;&#32479;&#19968;&#31354;&#38388;&#65292;&#20197;&#33719;&#21462;&#26356;&#22909;&#30340;&#30693;&#35782;&#20256;&#25773;&#30340;&#39640;&#36136;&#37327;&#23884;&#20837;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20960;&#20309;&#24863;&#30693;&#20248;&#21270;&#31574;&#30053;&#65292;&#20351;&#24471;&#25289;&#21644;&#25512;&#36807;&#31243;&#21487;&#20197;&#20805;&#20998;&#21033;&#29992;&#21452;&#26354;&#21644;&#29699;&#38754;&#31354;&#38388;&#30340;&#20248;&#21183;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#21452;&#26354;&#31354;&#38388;&#20013;&#65292;
&lt;/p&gt;
&lt;p&gt;
Since Knowledge Graphs (KGs) contain rich semantic information, recently there has been an influx of KG-enhanced recommendation methods. Most of existing methods are entirely designed based on euclidean space without considering curvature. However, recent studies have revealed that a tremendous graph-structured data exhibits highly non-euclidean properties. Motivated by these observations, in this work, we propose a knowledge-based multiple adaptive spaces fusion method for recommendation, namely MCKG. Unlike existing methods that solely adopt a specific manifold, we introduce the unified space that is compatible with hyperbolic, euclidean and spherical spaces. Furthermore, we fuse the multiple unified spaces in an attention manner to obtain the high-quality embeddings for better knowledge propagation. In addition, we propose a geometry-aware optimization strategy which enables the pull and push processes benefited from both hyperbolic and spherical spaces. Specifically, in hyperbolic 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#21487;&#35299;&#37322;&#24615;&#20998;&#26512;&#30340;&#20998;&#31867;&#24863;&#30693;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#65292;&#29992;&#20110;&#20914;&#31361;&#20998;&#31867;&#21644;&#20027;&#39064;&#21457;&#29616;&#12290;&#35813;&#27169;&#22411;&#25552;&#20379;&#20102;&#21487;&#38752;&#30340;&#20998;&#31867;&#32467;&#26524;&#21644;&#21457;&#29616;&#30340;&#20027;&#39064;&#30340;&#35299;&#37322;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#27169;&#22411;&#30340;&#22797;&#26434;&#24230;&#26469;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.15232</link><description>&lt;p&gt;
&#32467;&#21512;&#21487;&#35299;&#37322;&#24615;&#20998;&#26512;&#30340;&#20998;&#31867;&#24863;&#30693;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#8212;&#8212;&#29992;&#20110;&#20914;&#31361;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Classification-Aware Neural Topic Model Combined With Interpretable Analysis -- For Conflict Classification. (arXiv:2308.15232v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15232
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#21487;&#35299;&#37322;&#24615;&#20998;&#26512;&#30340;&#20998;&#31867;&#24863;&#30693;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#65292;&#29992;&#20110;&#20914;&#31361;&#20998;&#31867;&#21644;&#20027;&#39064;&#21457;&#29616;&#12290;&#35813;&#27169;&#22411;&#25552;&#20379;&#20102;&#21487;&#38752;&#30340;&#20998;&#31867;&#32467;&#26524;&#21644;&#21457;&#29616;&#30340;&#20027;&#39064;&#30340;&#35299;&#37322;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#27169;&#22411;&#30340;&#22797;&#26434;&#24230;&#26469;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19990;&#30028;&#19978;&#26377;&#22823;&#37327;&#30340;&#20914;&#31361;&#20107;&#20214;&#19968;&#30452;&#22312;&#24433;&#21709;&#30528;&#25105;&#20204;&#12290;&#20026;&#20102;&#26377;&#25928;&#20998;&#26512;&#36825;&#20123;&#20914;&#31361;&#20107;&#20214;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20914;&#31361;&#20449;&#24687;&#20998;&#31867;&#21644;&#20027;&#39064;&#21457;&#29616;&#30340;&#20998;&#31867;&#24863;&#30693;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#65288;CANTM-IA&#65289;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#24341;&#20837;&#21487;&#35299;&#37322;&#24615;&#20998;&#26512;&#26469;&#25552;&#20379;&#21487;&#38752;&#30340;&#20998;&#31867;&#32467;&#26524;&#21644;&#21457;&#29616;&#30340;&#20027;&#39064;&#30340;&#35299;&#37322;&#12290;&#21516;&#26102;&#65292;&#23558;&#35299;&#37322;&#24615;&#24341;&#20837;&#27169;&#22411;&#26550;&#26500;&#20013;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#20998;&#31867;&#24615;&#33021;&#65292;&#24182;&#20351;&#35299;&#37322;&#36827;&#19968;&#27493;&#20851;&#27880;&#25968;&#25454;&#30340;&#32454;&#33410;&#12290;&#26368;&#21518;&#65292;&#23545;&#27169;&#22411;&#26550;&#26500;&#36827;&#34892;&#20248;&#21270;&#65292;&#20197;&#38477;&#20302;&#27169;&#22411;&#30340;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
A large number of conflict events are affecting the world all the time. In order to analyse such conflict events effectively, this paper presents a Classification-Aware Neural Topic Model (CANTM-IA) for Conflict Information Classification and Topic Discovery. The model provides a reliable interpretation of classification results and discovered topics by introducing interpretability analysis. At the same time, interpretation is introduced into the model architecture to improve the classification performance of the model and to allow interpretation to focus further on the details of the data. Finally, the model architecture is optimised to reduce the complexity of the model.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#38480;&#21046;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#30340;&#32534;&#30721;&#26469;&#20943;&#23569;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#27495;&#35270;&#65292;&#20174;&#32780;&#20026;&#20197;&#21069;&#26410;&#20986;&#29616;&#30340;&#29992;&#25143;&#25552;&#20379;&#20844;&#24179;&#25512;&#33616;&#12290;</title><link>http://arxiv.org/abs/2308.15230</link><description>&lt;p&gt;
&#20351;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#20026;&#20197;&#21069;&#26410;&#20986;&#29616;&#30340;&#29992;&#25143;&#25552;&#20379;&#20844;&#24179;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Providing Previously Unseen Users Fair Recommendations Using Variational Autoencoders. (arXiv:2308.15230v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15230
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#38480;&#21046;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#30340;&#32534;&#30721;&#26469;&#20943;&#23569;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#27495;&#35270;&#65292;&#20174;&#32780;&#20026;&#20197;&#21069;&#26410;&#20986;&#29616;&#30340;&#29992;&#25143;&#25552;&#20379;&#20844;&#24179;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#20851;&#20110;&#20844;&#24179;&#24615;&#30340;&#26032;&#23450;&#20041;&#35201;&#27714;&#27169;&#22411;&#23545;&#29992;&#25143;&#30340;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#19981;&#21487;&#35265;&#65292;&#20363;&#22914;&#65292;&#29992;&#25143;&#30340;&#24615;&#21035;&#25110;&#24180;&#40836;&#19981;&#24212;&#24433;&#21709;&#27169;&#22411;&#12290;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#29305;&#21035;&#23481;&#26131;&#36890;&#36807;&#20854;&#26174;&#24335;&#30340;&#29992;&#25143;&#20851;&#27880;&#21644;&#29992;&#25143;&#24314;&#27169;&#26469;&#36829;&#21453;&#36825;&#20010;&#23450;&#20041;&#12290;&#26174;&#24335;&#30340;&#29992;&#25143;&#24314;&#27169;&#20063;&#26159;&#35768;&#22810;&#25512;&#33616;&#31995;&#32479;&#26080;&#27861;&#20026;&#20197;&#21069;&#26410;&#20986;&#29616;&#30340;&#29992;&#25143;&#25552;&#20379;&#25512;&#33616;&#30340;&#21407;&#22240;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38480;&#21046;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#32534;&#30721;&#30340;&#26032;&#26041;&#27861;&#26469;&#20943;&#23569;&#22522;&#20110;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#27495;&#35270;&#12290;&#36825;&#20123;&#26041;&#27861;&#33021;&#22815;&#22312;&#35780;&#20272;&#20013;&#20026;&#26410;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#20986;&#29616;&#30340;&#29992;&#25143;&#25552;&#20379;&#20844;&#24179;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;
An emerging definition of fairness in machine learning requires that models are oblivious to demographic user information, e.g., a user's gender or age should not influence the model. Personalized recommender systems are particularly prone to violating this definition through their explicit user focus and user modelling. Explicit user modelling is also an aspect that makes many recommender systems incapable of providing hitherto unseen users with recommendations. We propose novel approaches for mitigating discrimination in Variational Autoencoder-based recommender systems by limiting the encoding of demographic information. The approaches are capable of, and evaluated on, providing users that are not represented in the training data with fair recommendations.
&lt;/p&gt;</description></item><item><title>CAGRA&#26159;&#19968;&#31181;&#38754;&#21521;GPU&#30340;&#39640;&#24230;&#24182;&#34892;&#22270;&#26500;&#24314;&#21644;&#36817;&#20284;&#26368;&#36817;&#37051;&#25628;&#32034;&#26041;&#27861;&#65292;&#22312;&#36817;&#20284;&#26368;&#36817;&#37051;&#25628;&#32034;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25928;&#29575;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2308.15136</link><description>&lt;p&gt;
CAGRA&#65306;&#38754;&#21521;GPU&#30340;&#39640;&#24230;&#24182;&#34892;&#22270;&#26500;&#24314;&#21644;&#36817;&#20284;&#26368;&#36817;&#37051;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
CAGRA: Highly Parallel Graph Construction and Approximate Nearest Neighbor Search for GPUs. (arXiv:2308.15136v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15136
&lt;/p&gt;
&lt;p&gt;
CAGRA&#26159;&#19968;&#31181;&#38754;&#21521;GPU&#30340;&#39640;&#24230;&#24182;&#34892;&#22270;&#26500;&#24314;&#21644;&#36817;&#20284;&#26368;&#36817;&#37051;&#25628;&#32034;&#26041;&#27861;&#65292;&#22312;&#36817;&#20284;&#26368;&#36817;&#37051;&#25628;&#32034;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25928;&#29575;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#20284;&#26368;&#36817;&#37051;&#25628;&#32034;&#65288;ANNS&#65289;&#22312;&#25968;&#25454;&#25366;&#25496;&#21644;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#28085;&#30422;&#20102;&#20449;&#24687;&#26816;&#32034;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#25512;&#33616;&#31995;&#32479;&#31561;&#21508;&#20010;&#23398;&#31185;&#12290;&#36817;&#24180;&#26469;&#65292;&#25968;&#25454;&#37327;&#24613;&#21095;&#22686;&#21152;&#65292;&#31351;&#20030;&#31934;&#30830;&#26368;&#36817;&#37051;&#25628;&#32034;&#30340;&#35745;&#31639;&#25104;&#26412;&#24448;&#24448;&#26159;&#31105;&#27490;&#24615;&#30340;&#65292;&#24517;&#39035;&#37319;&#29992;&#36817;&#20284;&#25216;&#26415;&#12290;&#23613;&#31649;&#22270;&#24418;&#21270;&#26041;&#27861;&#30340;&#24179;&#34913;&#24615;&#33021;&#21644;&#21484;&#22238;&#29575;&#22312;ANNS&#31639;&#27861;&#20013;&#26368;&#36817;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#20294;&#21482;&#26377;&#23569;&#25968;&#30740;&#31350;&#25506;&#32034;&#20102;&#21033;&#29992;GPU&#21644;&#22810;&#26680;&#22788;&#29702;&#22120;&#30340;&#24378;&#22823;&#35745;&#31639;&#33021;&#21147;&#65292;&#23613;&#31649;&#24191;&#27867;&#20351;&#29992;&#20102;&#22823;&#35268;&#27169;&#24182;&#34892;&#21644;&#36890;&#29992;&#35745;&#31639;&#33021;&#21147;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#24182;&#34892;&#35745;&#31639;&#30828;&#20214;&#30340;&#26032;&#39062;&#25509;&#36817;&#22270;&#21644;&#25628;&#32034;&#31639;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;&#29616;&#20195;&#30828;&#20214;&#30340;&#39640;&#24615;&#33021;&#33021;&#21147;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#25928;&#29575;&#25552;&#21319;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#22270;&#26500;&#24314;&#21644;&#36817;&#20284;&#26368;&#36817;&#37051;&#25628;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Approximate Nearest Neighbor Search (ANNS) plays a critical role in various disciplines spanning data mining and artificial intelligence, from information retrieval and computer vision to natural language processing and recommender systems. Data volumes have soared in recent years and the computational cost of an exhaustive exact nearest neighbor search is often prohibitive, necessitating the adoption of approximate techniques. The balanced performance and recall of graph-based approaches have more recently garnered significant attention in ANNS algorithms, however, only a few studies have explored harnessing the power of GPUs and multi-core processors despite the widespread use of massively parallel and general-purpose computing. To bridge this gap, we introduce a novel parallel computing hardware-based proximity graph and search algorithm. By leveraging the high-performance capabilities of modern hardware, our approach achieves remarkable efficiency gains. In particular, our method s
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#38899;&#39057;&#23383;&#24149;&#31995;&#32479;&#21644;&#38899;&#39057;&#25991;&#26412;&#26816;&#32034;&#31995;&#32479;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#36890;&#36807;&#25506;&#32034;&#26410;&#32463;&#20462;&#25913;&#30340;&#38899;&#39057;&#23383;&#24149;&#31995;&#32479;&#23545;&#38899;&#39057;&#25991;&#26412;&#26816;&#32034;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#21363;&#20351;&#26410;&#36827;&#34892;&#24494;&#35843;&#65292;&#38899;&#39057;&#23383;&#24149;&#31995;&#32479;&#22312;&#38899;&#39057;&#25991;&#26412;&#26816;&#32034;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20102;&#19968;&#23450;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.15090</link><description>&lt;p&gt;
&#19968;&#30707;&#20108;&#40479;&#65306;&#38899;&#39057;&#23383;&#24149;&#31995;&#32479;&#26159;&#21542;&#33021;&#29992;&#20110;&#38899;&#39057;&#25991;&#26412;&#26816;&#32034;&#65311;
&lt;/p&gt;
&lt;p&gt;
Killing two birds with one stone: Can an audio captioning system also be used for audio-text retrieval?. (arXiv:2308.15090v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15090
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#38899;&#39057;&#23383;&#24149;&#31995;&#32479;&#21644;&#38899;&#39057;&#25991;&#26412;&#26816;&#32034;&#31995;&#32479;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#36890;&#36807;&#25506;&#32034;&#26410;&#32463;&#20462;&#25913;&#30340;&#38899;&#39057;&#23383;&#24149;&#31995;&#32479;&#23545;&#38899;&#39057;&#25991;&#26412;&#26816;&#32034;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#21363;&#20351;&#26410;&#36827;&#34892;&#24494;&#35843;&#65292;&#38899;&#39057;&#23383;&#24149;&#31995;&#32479;&#22312;&#38899;&#39057;&#25991;&#26412;&#26816;&#32034;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20102;&#19968;&#23450;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#38899;&#39057;&#23383;&#24149;&#31995;&#32479;&#26088;&#22312;&#24320;&#21457;&#33021;&#22815;&#29992;&#25991;&#26412;&#21477;&#23376;&#25551;&#36848;&#38899;&#39057;&#24405;&#38899;&#30340;&#31995;&#32479;&#12290;&#19982;&#27492;&#30456;&#21453;&#65292;&#38899;&#39057;&#25991;&#26412;&#26816;&#32034;&#31995;&#32479;&#26088;&#22312;&#20026;&#32473;&#23450;&#30340;&#25991;&#26412;&#26597;&#35810;&#65288;&#25991;&#26412;&#21040;&#38899;&#39057;&#65289;&#25110;&#21453;&#20043;&#65288;&#38899;&#39057;&#21040;&#25991;&#26412;&#65289;&#25214;&#21040;&#26368;&#20339;&#21305;&#37197;&#30340;&#38899;&#39057;&#24405;&#38899;&#12290;&#36825;&#20123;&#20219;&#21153;&#38656;&#35201;&#19981;&#21516;&#31867;&#22411;&#30340;&#31995;&#32479;&#65306;&#38899;&#39057;&#23383;&#24149;&#31995;&#32479;&#37319;&#29992;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#65292;&#32780;&#38899;&#39057;&#25991;&#26412;&#26816;&#32034;&#31995;&#32479;&#21033;&#29992;&#22312;&#20849;&#20139;&#25237;&#23556;&#23376;&#31354;&#38388;&#20869;&#27604;&#36739;&#38899;&#39057;&#21644;&#25991;&#26412;&#34920;&#31034;&#30340;&#25490;&#24207;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#26412;&#30740;&#31350;&#36890;&#36807;&#25506;&#32034;&#26410;&#32463;&#20462;&#25913;&#30340;&#38899;&#39057;&#23383;&#24149;&#31995;&#32479;&#65288;&#26080;&#38656;&#38024;&#23545;&#26032;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#65289;&#30340;&#38899;&#39057;&#25991;&#26412;&#26816;&#32034;&#33021;&#21147;&#65292;&#30740;&#31350;&#20102;&#38899;&#39057;&#23383;&#24149;&#31995;&#32479;&#19982;&#38899;&#39057;&#25991;&#26412;&#26816;&#32034;&#31995;&#32479;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#38899;&#39057;&#23383;&#24149;&#31995;&#32479;&#21253;&#25324;&#19968;&#20010;&#22312;&#38899;&#39057;&#26631;&#35760;&#19978;&#36890;&#36807;AudioSet&#36827;&#34892;&#35757;&#32451;&#30340;&#38899;&#39057;&#32534;&#30721;&#22120;&#65288;ConvNeXt-Tiny&#65289;&#65292;&#20197;&#21450;&#19968;&#20010;&#36127;&#36131;&#29983;&#25104;&#21477;&#23376;&#30340;&#21464;&#21387;&#22120;&#35299;&#30721;&#22120;&#12290;&#23545;&#20110;&#38899;&#39057;&#23383;&#24149;&#31995;&#32479;&#65292;&#23427;&#22312;Clotho&#19978;&#30340;SPIDEr-FL&#24471;&#20998;&#24179;&#22343;&#20026;0.298&#65292;&#22312;AudioCaps&#19978;&#30340;&#24471;&#20998;&#24179;&#22343;&#20026;0.472&#12290;&#23545;&#20110;&#38899;&#39057;&#25991;&#26412;&#26816;&#32034;&#31995;&#32479;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#26631;&#20934;&#30340;&#20132;&#21449;&#29109;&#25439;&#22833;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated Audio Captioning (AAC) aims to develop systems capable of describing an audio recording using a textual sentence. In contrast, Audio-Text Retrieval (ATR) systems seek to find the best matching audio recording(s) for a given textual query (Text-to-Audio) or vice versa (Audio-to-Text). These tasks require different types of systems: AAC employs a sequence-to-sequence model, while ATR utilizes a ranking model that compares audio and text representations within a shared projection subspace. However, this work investigates the relationship between AAC and ATR by exploring the ATR capabilities of an unmodified AAC system, without fine-tuning for the new task. Our AAC system consists of an audio encoder (ConvNeXt-Tiny) trained on AudioSet for audio tagging, and a transformer decoder responsible for generating sentences. For AAC, it achieves a high SPIDEr-FL score of 0.298 on Clotho and 0.472 on AudioCaps on average. For ATR, we propose using the standard Cross-Entropy loss values ob
&lt;/p&gt;</description></item><item><title>STEC&#26159;&#19968;&#31181;&#22522;&#20110;&#36879;&#26126;Transformer&#32534;&#30721;&#22120;&#30340;CTR&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#22810;&#31181;&#20132;&#20114;&#23398;&#20064;&#26041;&#27861;&#21644;&#27531;&#24046;&#36830;&#25509;&#65292;&#25552;&#21319;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.15033</link><description>&lt;p&gt;
STEC: &#22522;&#20110;&#36879;&#26126;Transformer&#32534;&#30721;&#22120;&#30340;CTR&#39044;&#27979;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
STEC: See-Through Transformer-based Encoder for CTR Prediction. (arXiv:2308.15033v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15033
&lt;/p&gt;
&lt;p&gt;
STEC&#26159;&#19968;&#31181;&#22522;&#20110;&#36879;&#26126;Transformer&#32534;&#30721;&#22120;&#30340;CTR&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#22810;&#31181;&#20132;&#20114;&#23398;&#20064;&#26041;&#27861;&#21644;&#27531;&#24046;&#36830;&#25509;&#65292;&#25552;&#21319;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28857;&#20987;&#29575;&#65288;CTR&#65289;&#39044;&#27979;&#22312;&#22312;&#32447;&#24191;&#21578;&#21644;&#25512;&#33616;&#31995;&#32479;&#20013;&#25198;&#28436;&#30528;&#20851;&#38190;&#35282;&#33394;&#65292;&#22240;&#20026;CTR&#39044;&#27979;&#24615;&#33021;&#30452;&#25509;&#24433;&#21709;&#29992;&#25143;&#30340;&#25972;&#20307;&#28385;&#24847;&#24230;&#21644;&#20844;&#21496;&#30340;&#25910;&#20837;&#12290;&#28982;&#32780;&#65292;CTR&#39044;&#27979;&#20173;&#28982;&#26159;&#19968;&#20010;&#27963;&#36291;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#22240;&#20026;&#23427;&#28041;&#21450;&#22522;&#20110;&#31232;&#30095;&#21644;&#39640;&#32500;&#29305;&#24449;&#20934;&#30830;&#24314;&#27169;&#29992;&#25143;&#30340;&#20559;&#22909;&#65292;&#22810;&#20010;&#29305;&#24449;&#30340;&#39640;&#38454;&#20132;&#20114;&#21487;&#33021;&#23548;&#33268;&#19981;&#21516;&#30340;&#32467;&#26524;&#12290;&#22823;&#22810;&#25968;CTR&#39044;&#27979;&#27169;&#22411;&#20381;&#36182;&#20110;&#21333;&#19968;&#30340;&#34701;&#21512;&#21644;&#20132;&#20114;&#23398;&#20064;&#31574;&#30053;&#12290;&#23569;&#25968;CTR&#39044;&#27979;&#27169;&#22411;&#21033;&#29992;&#22810;&#20010;&#20132;&#20114;&#24314;&#27169;&#31574;&#30053;&#65292;&#20294;&#23558;&#27599;&#20010;&#20132;&#20114;&#35270;&#20026;&#29420;&#31435;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;STEC&#30340;&#26032;&#27169;&#22411;&#65292;&#23427;&#22312;&#21333;&#19968;&#32479;&#19968;&#30340;&#32467;&#26500;&#20013;&#20805;&#20998;&#21033;&#29992;&#20102;&#22810;&#31181;&#20132;&#20114;&#23398;&#20064;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#24341;&#20837;&#20102;&#26469;&#33258;&#19981;&#21516;&#20132;&#20114;&#38454;&#30340;&#27531;&#24046;&#36830;&#25509;&#65292;&#20174;&#32780;&#25552;&#21319;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Click-Through Rate (CTR) prediction holds a pivotal place in online advertising and recommender systems since CTR prediction performance directly influences the overall satisfaction of the users and the revenue generated by companies. Even so, CTR prediction is still an active area of research since it involves accurately modelling the preferences of users based on sparse and high-dimensional features where the higher-order interactions of multiple features can lead to different outcomes. Most CTR prediction models have relied on a single fusion and interaction learning strategy. The few CTR prediction models that have utilized multiple interaction modelling strategies have treated each interaction to be self-contained. In this paper, we propose a novel model named STEC that reaps the benefits of multiple interaction learning approaches in a single unified architecture. Additionally, our model introduces residual connections from different orders of interactions which boosts the perfor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#20302;&#25104;&#26412;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20256;&#32479;&#30340;TF-IDF&#21644;&#27973;&#23618;&#23884;&#20837;&#27169;&#22411;&#32467;&#21512;&#20351;&#29992;&#65292;&#21487;&#20197;&#19982;&#22522;&#20110;&#22823;&#22411;Transformer&#27169;&#22411;&#30340;&#31070;&#32463;&#25490;&#21517;&#27169;&#22411;&#31454;&#20105;&#65292;&#24182;&#21487;&#20197;&#25552;&#39640;&#36825;&#20123;&#27169;&#22411;&#22312;&#22823;&#35268;&#27169;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.15027</link><description>&lt;p&gt;
&#29992;&#20256;&#32479;IR&#26041;&#27861;&#25552;&#39640;&#31070;&#32463;&#25490;&#21517;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Improving Neural Ranking Models with Traditional IR Methods. (arXiv:2308.15027v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15027
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#20302;&#25104;&#26412;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20256;&#32479;&#30340;TF-IDF&#21644;&#27973;&#23618;&#23884;&#20837;&#27169;&#22411;&#32467;&#21512;&#20351;&#29992;&#65292;&#21487;&#20197;&#19982;&#22522;&#20110;&#22823;&#22411;Transformer&#27169;&#22411;&#30340;&#31070;&#32463;&#25490;&#21517;&#27169;&#22411;&#31454;&#20105;&#65292;&#24182;&#21487;&#20197;&#25552;&#39640;&#36825;&#20123;&#27169;&#22411;&#22312;&#22823;&#35268;&#27169;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;Transformer&#27169;&#22411;&#30340;&#31070;&#32463;&#25490;&#21517;&#26041;&#27861;&#36817;&#24180;&#26469;&#22312;&#20449;&#24687;&#26816;&#32034;&#39046;&#22495;&#24341;&#36215;&#20102;&#26497;&#22823;&#20851;&#27880;&#65292;&#24182;&#34987;&#20027;&#35201;&#21830;&#19994;&#35299;&#20915;&#26041;&#26696;&#37319;&#29992;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#21019;&#24314;&#36807;&#31243;&#20013;&#35745;&#31639;&#25104;&#26412;&#39640;&#26114;&#65292;&#24182;&#38656;&#35201;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#26469;&#36866;&#24212;&#29305;&#23450;&#30340;&#35821;&#26009;&#24211;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20302;&#36164;&#28304;&#26367;&#20195;&#26041;&#26696;&#65292;&#21363;&#22522;&#20110;&#23884;&#20837;&#27169;&#22411;&#30340;&#25991;&#26723;&#26816;&#32034;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#23427;&#22312;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#19978;&#19982;&#32454;&#35843;&#30340;&#22823;&#22411;Transformer&#27169;&#22411;&#30456;&#31454;&#20105;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;&#20256;&#32479;&#20851;&#38190;&#23383;&#21305;&#37197;&#26041;&#27861;TF-IDF&#19982;&#27973;&#23618;&#23884;&#20837;&#27169;&#22411;&#31616;&#21333;&#32467;&#21512;&#21487;&#20197;&#20197;&#20302;&#25104;&#26412;&#36861;&#36214;&#21040;&#22797;&#26434;&#31070;&#32463;&#25490;&#21517;&#27169;&#22411;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#22312;&#36825;&#20123;&#20219;&#21153;&#19978;&#28155;&#21152;TF-IDF&#24230;&#37327;&#21487;&#20197;&#25552;&#39640;&#22823;&#35268;&#27169;&#32454;&#35843;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural ranking methods based on large transformer models have recently gained significant attention in the information retrieval community, and have been adopted by major commercial solutions. Nevertheless, they are computationally expensive to create, and require a great deal of labeled data for specialized corpora. In this paper, we explore a low resource alternative which is a bag-of-embedding model for document retrieval and find that it is competitive with large transformer models fine tuned on information retrieval tasks. Our results show that a simple combination of TF-IDF, a traditional keyword matching method, with a shallow embedding model provides a low cost path to compete well with the performance of complex neural ranking models on 3 datasets. Furthermore, adding TF-IDF measures improves the performance of large-scale fine tuned models on these tasks.
&lt;/p&gt;</description></item><item><title>CAPS&#26159;&#19968;&#31181;&#36890;&#36807;&#31354;&#38388;&#20998;&#21306;&#23454;&#29616;&#30340;&#29992;&#20110;&#24102;&#26377;&#36807;&#28388;&#30340;&#30456;&#20284;&#24615;&#25628;&#32034;&#30340;&#32034;&#24341;&#65292;&#23427;&#22312;&#21484;&#22238;-&#24310;&#36831;&#26435;&#34913;&#26041;&#38754;&#20248;&#20110;&#22522;&#20110;&#22270;&#30340;&#32422;&#26463;&#25628;&#32034;&#25216;&#26415;&#65292;&#24182;&#19988;&#21482;&#38656;&#35201;&#36739;&#23567;&#30340;&#32034;&#24341;&#22823;&#23567;&#12290;</title><link>http://arxiv.org/abs/2308.15014</link><description>&lt;p&gt;
CAPS&#65306;&#29992;&#20110;&#24102;&#26377;&#36807;&#28388;&#30340;&#30456;&#20284;&#24615;&#25628;&#32034;&#30340;&#23454;&#29992;&#20998;&#21306;&#32034;&#24341;
&lt;/p&gt;
&lt;p&gt;
CAPS: A Practical Partition Index for Filtered Similarity Search. (arXiv:2308.15014v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15014
&lt;/p&gt;
&lt;p&gt;
CAPS&#26159;&#19968;&#31181;&#36890;&#36807;&#31354;&#38388;&#20998;&#21306;&#23454;&#29616;&#30340;&#29992;&#20110;&#24102;&#26377;&#36807;&#28388;&#30340;&#30456;&#20284;&#24615;&#25628;&#32034;&#30340;&#32034;&#24341;&#65292;&#23427;&#22312;&#21484;&#22238;-&#24310;&#36831;&#26435;&#34913;&#26041;&#38754;&#20248;&#20110;&#22522;&#20110;&#22270;&#30340;&#32422;&#26463;&#25628;&#32034;&#25216;&#26415;&#65292;&#24182;&#19988;&#21482;&#38656;&#35201;&#36739;&#23567;&#30340;&#32034;&#24341;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31070;&#32463;&#34920;&#31034;&#23398;&#20064;&#30340;&#36827;&#27493;&#65292;&#36817;&#20284;&#36817;&#37051;&#25628;&#32034;&#65288;ANNS&#65289;&#30340;&#27969;&#34892;&#24230;&#36234;&#26469;&#36234;&#39640;&#65292;&#21516;&#26102;&#20063;&#21152;&#24378;&#20102;&#23545;&#33021;&#22815;&#22312;&#26597;&#35810;&#20013;&#38468;&#24102;&#19968;&#32452;&#32422;&#26463;&#30340;&#33021;&#21147;&#30340;&#20851;&#27880;&#12290;&#23613;&#31649;&#31038;&#21306;&#26368;&#36817;&#25552;&#20986;&#20102;&#20960;&#31181;&#29992;&#20110;&#32422;&#26463;ANNS&#30340;&#31639;&#27861;&#65292;&#20294;&#20960;&#20046;&#25152;&#26377;&#36825;&#20123;&#26041;&#27861;&#37117;&#38598;&#20013;&#22312;&#19982;&#22522;&#20110;&#22270;&#30340;&#32034;&#24341;&#30340;&#38598;&#25104;&#19978;&#65292;&#36825;&#26159;&#19968;&#31181;&#33021;&#22815;&#22312;&#24310;&#36831;-&#21484;&#22238;&#26435;&#34913;&#20013;&#23454;&#29616;&#26368;&#20808;&#36827;&#24615;&#33021;&#30340;&#20027;&#35201;&#31639;&#27861;&#31867;&#21035;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#19987;&#27880;&#20110;&#36890;&#36807;&#31354;&#38388;&#20998;&#21306;&#32780;&#19981;&#26159;&#22270;&#26469;&#24320;&#21457;&#32422;&#26463;ANNS&#31639;&#27861;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#32422;&#26463;&#36817;&#20284;&#20998;&#21306;&#25628;&#32034;&#65288;CAPS&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#36807;&#31354;&#38388;&#20998;&#21306;&#36827;&#34892;&#24102;&#26377;&#36807;&#28388;&#30340;ANNS&#32034;&#24341;&#65292;&#19981;&#20165;&#20445;&#30041;&#20102;&#22522;&#20110;&#20998;&#21306;&#30340;&#31639;&#27861;&#30340;&#20248;&#21183;&#65292;&#32780;&#19988;&#22312;&#21484;&#22238;-&#24310;&#36831;&#26435;&#34913;&#26041;&#38754;&#32988;&#36807;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#22270;&#30340;&#32422;&#26463;&#25628;&#32034;&#25216;&#26415;&#65292;&#32780;&#19988;&#21482;&#38656;&#35201;10%&#30340;&#32034;&#24341;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the surging popularity of approximate near-neighbor search (ANNS), driven by advances in neural representation learning, the ability to serve queries accompanied by a set of constraints has become an area of intense interest. While the community has recently proposed several algorithms for constrained ANNS, almost all of these methods focus on integration with graph-based indexes, the predominant class of algorithms achieving state-of-the-art performance in latency-recall tradeoffs. In this work, we take a different approach and focus on developing a constrained ANNS algorithm via space partitioning as opposed to graphs. To that end, we introduce Constrained Approximate Partitioned Search (CAPS), an index for ANNS with filters via space partitions that not only retains the benefits of a partition-based algorithm but also outperforms state-of-the-art graph-based constrained search techniques in recall-latency tradeoffs, with only 10% of the index size.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35299;&#20915;&#20102;&#29983;&#25104;&#24615;&#26816;&#32034;&#22312;&#21160;&#24577;&#35821;&#26009;&#24211;&#20013;&#30340;&#25345;&#32493;&#23398;&#20064;&#38382;&#39064;&#65292;&#36890;&#36807;&#24341;&#20837;CLEVER&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#22686;&#37327;&#20135;&#21697;&#37327;&#21270;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#20302;&#35745;&#31639;&#25104;&#26412;&#30340;&#26032;&#25991;&#26723;&#32534;&#30721;&#12290;</title><link>http://arxiv.org/abs/2308.14968</link><description>&lt;p&gt;
&#21160;&#24577;&#35821;&#26009;&#24211;&#19978;&#30340;&#29983;&#25104;&#24615;&#26816;&#32034;&#30340;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Continual Learning for Generative Retrieval over Dynamic Corpora. (arXiv:2308.14968v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14968
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35299;&#20915;&#20102;&#29983;&#25104;&#24615;&#26816;&#32034;&#22312;&#21160;&#24577;&#35821;&#26009;&#24211;&#20013;&#30340;&#25345;&#32493;&#23398;&#20064;&#38382;&#39064;&#65292;&#36890;&#36807;&#24341;&#20837;CLEVER&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#22686;&#37327;&#20135;&#21697;&#37327;&#21270;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#20302;&#35745;&#31639;&#25104;&#26412;&#30340;&#26032;&#25991;&#26723;&#32534;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24615;&#26816;&#32034;&#65288;GR&#65289;&#22522;&#20110;&#21442;&#25968;&#27169;&#22411;&#30452;&#25509;&#39044;&#27979;&#30456;&#20851;&#25991;&#26723;&#30340;&#26631;&#35782;&#31526;&#65288;&#21363;docids&#65289;&#65292;&#22312;&#35768;&#22810;&#24212;&#29992;&#26816;&#32034;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#36825;&#20123;&#20219;&#21153;&#37117;&#20551;&#35774;&#20102;&#38745;&#24577;&#25991;&#26723;&#38598;&#21512;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#25991;&#26723;&#38598;&#21512;&#26159;&#21160;&#24577;&#30340;&#65292;&#21363;&#25345;&#32493;&#19981;&#26029;&#22320;&#28155;&#21152;&#26032;&#25991;&#26723;&#12290;&#33021;&#22815;&#22686;&#37327;&#32034;&#24341;&#26032;&#25991;&#26723;&#21516;&#26102;&#20445;&#30041;&#20197;&#21069;&#21644;&#26032;&#32034;&#24341;&#30340;&#30456;&#20851;&#25991;&#26723;&#22238;&#31572;&#26597;&#35810;&#30340;&#33021;&#21147;&#65292;&#23545;&#24212;&#29992;GR&#27169;&#22411;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;GR&#30340;&#23454;&#38469;&#25345;&#32493;&#23398;&#20064;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25345;&#32493;&#23398;&#20064;&#27169;&#22411;CLEVER&#65288;Continual-LEarner for generatiVE Retrieval&#65289;&#65292;&#22312;GR&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#38754;&#20570;&#20986;&#20102;&#20004;&#20010;&#37325;&#22823;&#36129;&#29486;&#65306;&#65288;i&#65289;&#20026;&#20102;&#20197;&#20302;&#35745;&#31639;&#25104;&#26412;&#23558;&#26032;&#25991;&#26723;&#32534;&#30721;&#20026;docids&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22686;&#37327;&#20135;&#21697;&#37327;&#21270;&#65288;Incremental Product Quantization&#65289;&#65292;&#26681;&#25454;&#20004;&#20010;&#33258;&#36866;&#24212;&#38408;&#20540;&#26356;&#26032;&#37096;&#20998;&#37327;&#21270;&#30721;&#26412;&#65307;&#21644;&#65288;ii&#65289;&#20026;&#20102;
&lt;/p&gt;
&lt;p&gt;
Generative retrieval (GR) directly predicts the identifiers of relevant documents (i.e., docids) based on a parametric model. It has achieved solid performance on many ad-hoc retrieval tasks. So far, these tasks have assumed a static document collection. In many practical scenarios, however, document collections are dynamic, where new documents are continuously added to the corpus. The ability to incrementally index new documents while preserving the ability to answer queries with both previously and newly indexed relevant documents is vital to applying GR models. In this paper, we address this practical continual learning problem for GR. We put forward a novel Continual-LEarner for generatiVE Retrieval (CLEVER) model and make two major contributions to continual learning for GR: (i) To encode new documents into docids with low computational cost, we present Incremental Product Quantization, which updates a partial quantization codebook according to two adaptive thresholds; and (ii) To
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#20351;&#29992;Lucene&#36827;&#34892;&#21521;&#37327;&#25628;&#32034;&#30340;&#21487;&#22797;&#29616;&#28436;&#31034;&#65292;&#25361;&#25112;&#20102;&#38656;&#35201;&#19987;&#29992;&#21521;&#37327;&#23384;&#20648;&#30340;&#35266;&#28857;&#65292;&#24182;&#23637;&#31034;&#20102;Lucene&#20013;&#30340;HNSW&#32034;&#24341;&#36275;&#20197;&#25552;&#20379;&#21521;&#37327;&#25628;&#32034;&#21151;&#33021;&#12290;&#36825;&#34920;&#26126;&#22312;&#29616;&#20195;"AI&#22534;&#26632;"&#20013;&#36827;&#34892;&#25628;&#32034;&#24182;&#19981;&#38656;&#35201;&#24341;&#20837;&#19987;&#29992;&#21521;&#37327;&#23384;&#20648;&#12290;</title><link>http://arxiv.org/abs/2308.14963</link><description>&lt;p&gt;
&#20351;&#29992;OpenAI&#23884;&#20837;&#21644;Lucene&#30340;&#21521;&#37327;&#25628;&#32034;&#65306;Lucene&#21363;&#20026;&#25152;&#38656;
&lt;/p&gt;
&lt;p&gt;
Vector Search with OpenAI Embeddings: Lucene Is All You Need. (arXiv:2308.14963v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14963
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#20351;&#29992;Lucene&#36827;&#34892;&#21521;&#37327;&#25628;&#32034;&#30340;&#21487;&#22797;&#29616;&#28436;&#31034;&#65292;&#25361;&#25112;&#20102;&#38656;&#35201;&#19987;&#29992;&#21521;&#37327;&#23384;&#20648;&#30340;&#35266;&#28857;&#65292;&#24182;&#23637;&#31034;&#20102;Lucene&#20013;&#30340;HNSW&#32034;&#24341;&#36275;&#20197;&#25552;&#20379;&#21521;&#37327;&#25628;&#32034;&#21151;&#33021;&#12290;&#36825;&#34920;&#26126;&#22312;&#29616;&#20195;"AI&#22534;&#26632;"&#20013;&#36827;&#34892;&#25628;&#32034;&#24182;&#19981;&#38656;&#35201;&#24341;&#20837;&#19987;&#29992;&#21521;&#37327;&#23384;&#20648;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#22797;&#29616;&#30340;&#12289;&#31471;&#21040;&#31471;&#30340;&#21521;&#37327;&#25628;&#32034;&#28436;&#31034;&#65292;&#20351;&#29992;Lucene&#22312;&#27969;&#34892;&#30340;MS MARCO&#27573;&#33853;&#25490;&#21517;&#27979;&#35797;&#38598;&#19978;&#36827;&#34892;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#25361;&#25112;&#19968;&#31181;&#26222;&#36941;&#30340;&#35266;&#28857;&#65292;&#21363;&#21033;&#29992;&#26368;&#36817;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#25628;&#32034;&#20013;&#30340;&#24212;&#29992;&#20013;&#21462;&#24471;&#30340;&#36827;&#23637;&#38656;&#35201;&#19968;&#20010;&#19987;&#29992;&#30340;&#21521;&#37327;&#23384;&#20648;&#12290;&#24688;&#24688;&#30456;&#21453;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;Lucene&#20013;&#30340;&#23618;&#27425;&#21487;&#23548;&#33322;&#23567;&#19990;&#30028;&#32593;&#32476;&#65288;HNSW&#65289;&#32034;&#24341;&#36275;&#20197;&#22312;&#26631;&#20934;&#30340;&#21452;&#32534;&#30721;&#22120;&#26550;&#26500;&#20013;&#25552;&#20379;&#21521;&#37327;&#25628;&#32034;&#21151;&#33021;&#12290;&#36825;&#34920;&#26126;&#65292;&#20174;&#31616;&#21333;&#30340;&#25104;&#26412;&#25928;&#30410;&#20998;&#26512;&#26469;&#30475;&#65292;&#23558;&#19968;&#20010;&#19987;&#38376;&#30340;&#21521;&#37327;&#23384;&#20648;&#24341;&#20837;&#29616;&#20195;&#30340;&#8220;AI&#22534;&#26632;&#8221;&#20013;&#36827;&#34892;&#25628;&#32034;&#20284;&#20046;&#27809;&#26377;&#24378;&#26377;&#21147;&#30340;&#29702;&#30001;&#65292;&#22240;&#20026;&#36825;&#20123;&#24212;&#29992;&#24050;&#32463;&#22312;&#29616;&#26377;&#30340;&#24191;&#27867;&#37096;&#32626;&#30340;&#22522;&#30784;&#35774;&#26045;&#20013;&#24471;&#21040;&#20102;&#22823;&#37327;&#30340;&#25237;&#36164;&#12290;
&lt;/p&gt;
&lt;p&gt;
We provide a reproducible, end-to-end demonstration of vector search with OpenAI embeddings using Lucene on the popular MS MARCO passage ranking test collection. The main goal of our work is to challenge the prevailing narrative that a dedicated vector store is necessary to take advantage of recent advances in deep neural networks as applied to search. Quite the contrary, we show that hierarchical navigable small-world network (HNSW) indexes in Lucene are adequate to provide vector search capabilities in a standard bi-encoder architecture. This suggests that, from a simple cost-benefit analysis, there does not appear to be a compelling reason to introduce a dedicated vector store into a modern "AI stack" for search, since such applications have already received substantial investments in existing, widely deployed infrastructure.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31639;&#27861;&#24615;&#34917;&#25937;&#26694;&#26550;&#65292;&#29992;&#20110;&#24110;&#21161;&#29702;&#35299;&#25512;&#33616;&#31995;&#32479;&#30340;&#27169;&#22411;&#24182;&#20462;&#25913;&#25512;&#33616;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.14916</link><description>&lt;p&gt;
RecRec: &#25512;&#33616;&#31995;&#32479;&#30340;&#31639;&#27861;&#24615;&#34917;&#25937;&#25514;&#26045;
&lt;/p&gt;
&lt;p&gt;
RecRec: Algorithmic Recourse for Recommender Systems. (arXiv:2308.14916v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14916
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31639;&#27861;&#24615;&#34917;&#25937;&#26694;&#26550;&#65292;&#29992;&#20110;&#24110;&#21161;&#29702;&#35299;&#25512;&#33616;&#31995;&#32479;&#30340;&#27169;&#22411;&#24182;&#20462;&#25913;&#25512;&#33616;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#22312;&#23089;&#20048;&#12289;&#36141;&#29289;&#12289;&#39135;&#29289;&#12289;&#26032;&#38395;&#12289;&#23601;&#19994;&#21644;&#25945;&#32946;&#31561;&#39046;&#22495;&#30340;&#20915;&#31574;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#36825;&#20123;&#25512;&#33616;&#31995;&#32479;&#32972;&#21518;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23545;&#20110;&#29992;&#25143;&#12289;&#20869;&#23481;&#25552;&#20379;&#32773;&#21644;&#31995;&#32479;&#24320;&#21457;&#32773;&#26469;&#35828;&#36890;&#24120;&#37117;&#26159;&#24040;&#22823;&#19988;&#19981;&#36879;&#26126;&#30340;&#12290;&#23545;&#20110;&#25152;&#26377;&#21033;&#30410;&#30456;&#20851;&#32773;&#26469;&#35828;&#65292;&#29702;&#35299;&#27169;&#22411;&#22312;&#36827;&#34892;&#26576;&#20123;&#39044;&#27979;&#21644;&#25512;&#33616;&#26102;&#30340;&#21407;&#29702;&#33267;&#20851;&#37325;&#35201;&#12290;&#23545;&#20110;&#37027;&#20123;&#29983;&#35745;&#20381;&#36182;&#20110;&#25512;&#33616;&#31995;&#32479;&#30340;&#20869;&#23481;&#25552;&#20379;&#32773;&#26469;&#35828;&#23588;&#20854;&#22914;&#27492;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20174;&#20174;&#23454;&#38469;&#38656;&#27714;&#20986;&#21457;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#38754;&#21521;&#20869;&#23481;&#25552;&#20379;&#32773;&#30340;&#25512;&#33616;&#31995;&#32479;&#34917;&#25937;&#26694;&#26550;&#12290;&#25512;&#33616;&#35774;&#32622;&#20013;&#30340;&#31639;&#27861;&#24615;&#34917;&#25937;&#26159;&#19968;&#32452;&#25805;&#20316;&#65292;&#22914;&#26524;&#25191;&#34892;&#65292;&#23558;&#20197;&#26399;&#26395;&#30340;&#26041;&#24335;&#20462;&#25913;&#39033;&#30446;&#30340;&#25512;&#33616;&#65288;&#25110;&#25490;&#24207;&#65289;&#12290;&#34917;&#25937;&#25514;&#26045;&#25552;&#20379;&#30340;&#25805;&#20316;&#24418;&#24335;&#20026;&#65306;&#8220;&#22914;&#26524;&#19968;&#20010;&#29305;&#24449;&#20174;X&#21464;&#20026;Y&#65292;&#37027;&#20040;&#35813;&#39033;&#30446;&#30340;&#25490;&#21517;&#20063;&#20250;&#30456;&#24212;&#21464;&#21270;&#12290;&#8221;
&lt;/p&gt;
&lt;p&gt;
Recommender systems play an essential role in the choices people make in domains such as entertainment, shopping, food, news, employment, and education. The machine learning models underlying these recommender systems are often enormously large and black-box in nature for users, content providers, and system developers alike. It is often crucial for all stakeholders to understand the model's rationale behind making certain predictions and recommendations. This is especially true for the content providers whose livelihoods depend on the recommender system. Drawing motivation from the practitioners' need, in this work, we propose a recourse framework for recommender systems, targeted towards the content providers. Algorithmic recourse in the recommendation setting is a set of actions that, if executed, would modify the recommendations (or ranking) of an item in the desired manner. A recourse suggests actions of the form: "if a feature changes X to Y, then the ranking of that item for a s
&lt;/p&gt;</description></item><item><title>Ad-Rec&#26159;&#19968;&#20010;&#21033;&#29992;&#39640;&#32423;&#29305;&#24449;&#20132;&#20114;&#25216;&#26415;&#35299;&#20915;&#25512;&#33616;&#32593;&#32476;&#20013;&#21327;&#21464;&#37327;&#28418;&#31227;&#38382;&#39064;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#25513;&#30721;&#36716;&#25442;&#22120;&#23454;&#29616;&#39640;&#38454;&#20132;&#21449;&#29305;&#24449;&#30340;&#23398;&#20064;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#36136;&#37327;&#12289;&#21152;&#36895;&#20102;&#25910;&#25947;&#36895;&#24230;&#24182;&#20943;&#23569;&#20102;&#35757;&#32451;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2308.14902</link><description>&lt;p&gt;
Ad-Rec: &#39640;&#32423;&#29305;&#24449;&#20132;&#20114;&#26469;&#35299;&#20915;&#25512;&#33616;&#32593;&#32476;&#20013;&#30340;&#21327;&#21464;&#37327;&#28418;&#31227;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Ad-Rec: Advanced Feature Interactions to Address Covariate-Shifts in Recommendation Networks. (arXiv:2308.14902v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14902
&lt;/p&gt;
&lt;p&gt;
Ad-Rec&#26159;&#19968;&#20010;&#21033;&#29992;&#39640;&#32423;&#29305;&#24449;&#20132;&#20114;&#25216;&#26415;&#35299;&#20915;&#25512;&#33616;&#32593;&#32476;&#20013;&#21327;&#21464;&#37327;&#28418;&#31227;&#38382;&#39064;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#25513;&#30721;&#36716;&#25442;&#22120;&#23454;&#29616;&#39640;&#38454;&#20132;&#21449;&#29305;&#24449;&#30340;&#23398;&#20064;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#36136;&#37327;&#12289;&#21152;&#36895;&#20102;&#25910;&#25947;&#36895;&#24230;&#24182;&#20943;&#23569;&#20102;&#35757;&#32451;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#27169;&#22411;&#36890;&#36807;&#21033;&#29992;&#22810;&#20010;&#36755;&#20837;&#29305;&#24449;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#22312;&#25552;&#20379;&#20010;&#24615;&#21270;&#29992;&#25143;&#20307;&#39564;&#26041;&#38754;&#36215;&#21040;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#25512;&#33616;&#27169;&#22411;&#32463;&#24120;&#38754;&#20020;&#29992;&#25143;&#34892;&#20026;&#21644;&#29289;&#21697;&#29305;&#24449;&#19981;&#26029;&#21464;&#21270;&#23548;&#33268;&#30340;&#21327;&#21464;&#37327;&#28418;&#31227;&#38382;&#39064;&#12290;&#22312;&#22788;&#29702;&#25968;&#25454;&#20998;&#24067;&#28418;&#31227;&#21644;&#36866;&#24212;&#29992;&#25143;&#34892;&#20026;&#21464;&#21270;&#26041;&#38754;&#65292;&#26377;&#25928;&#30340;&#29305;&#24449;&#20132;&#20114;&#23398;&#20064;&#33267;&#20851;&#37325;&#35201;&#12290;&#20256;&#32479;&#30340;&#29305;&#24449;&#20132;&#20114;&#25216;&#26415;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#23384;&#22312;&#19968;&#20123;&#23616;&#38480;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;Ad-Rec&#65292;&#19968;&#20010;&#21033;&#29992;&#29305;&#24449;&#20132;&#20114;&#25216;&#26415;&#26469;&#35299;&#20915;&#21327;&#21464;&#37327;&#28418;&#31227;&#38382;&#39064;&#30340;&#39640;&#32423;&#32593;&#32476;&#12290;&#36825;&#26377;&#21161;&#20110;&#28040;&#38500;&#25512;&#33616;&#20219;&#21153;&#20013;&#30340;&#26080;&#20851;&#20132;&#20114;&#12290;Ad-Rec&#21033;&#29992;&#25513;&#30721;&#36716;&#25442;&#22120;&#26469;&#23454;&#29616;&#39640;&#38454;&#20132;&#21449;&#29305;&#24449;&#30340;&#23398;&#20064;&#65292;&#21516;&#26102;&#20943;&#36731;&#25968;&#25454;&#20998;&#24067;&#28418;&#31227;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;Area Under Curve&#65288;AUC&#65289;&#25351;&#26631;&#34913;&#37327;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#36136;&#37327;&#65292;&#21152;&#24555;&#20102;&#25910;&#25947;&#36895;&#24230;&#65292;&#20943;&#23569;&#20102;&#35757;&#32451;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommendation models are vital in delivering personalized user experiences by leveraging the correlation between multiple input features. However, deep learning-based recommendation models often face challenges due to evolving user behaviour and item features, leading to covariate shifts. Effective cross-feature learning is crucial to handle data distribution drift and adapting to changing user behaviour. Traditional feature interaction techniques have limitations in achieving optimal performance in this context.  This work introduces Ad-Rec, an advanced network that leverages feature interaction techniques to address covariate shifts. This helps eliminate irrelevant interactions in recommendation tasks. Ad-Rec leverages masked transformers to enable the learning of higher-order cross-features while mitigating the impact of data distribution drift. Our approach improves model quality, accelerates convergence, and reduces training time, as measured by the Area Under Curve (AUC) metric.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25552;&#20986;&#21517;&#20026;Excalibur&#30340;&#38646;&#26679;&#26412;&#36328;&#27169;&#24577;&#22270;&#20687;&#26816;&#32034;&#31995;&#32479;&#65292;&#25506;&#32034;&#20102;&#20132;&#20114;&#24335;&#23398;&#20064;&#22312;&#25552;&#39640;&#25191;&#27861;&#23398;&#20013;&#22270;&#20687;&#26816;&#32034;&#24615;&#33021;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#36890;&#36807;&#27169;&#25311;&#21644;&#29992;&#25143;&#30740;&#31350;&#35777;&#26126;&#20102;&#20854;&#26174;&#33879;&#30340;&#25913;&#36827;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.14786</link><description>&lt;p&gt;
&#22312;&#25191;&#27861;&#23398;&#20013;&#36890;&#36807;&#20132;&#20114;&#24335;&#23398;&#20064;&#25193;&#23637;&#36328;&#27169;&#24577;&#26816;&#32034;&#20197;&#25552;&#39640;&#22270;&#20687;&#26816;&#32034;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Extending Cross-Modal Retrieval with Interactive Learning to Improve Image Retrieval Performance in Forensics. (arXiv:2308.14786v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14786
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25552;&#20986;&#21517;&#20026;Excalibur&#30340;&#38646;&#26679;&#26412;&#36328;&#27169;&#24577;&#22270;&#20687;&#26816;&#32034;&#31995;&#32479;&#65292;&#25506;&#32034;&#20102;&#20132;&#20114;&#24335;&#23398;&#20064;&#22312;&#25552;&#39640;&#25191;&#27861;&#23398;&#20013;&#22270;&#20687;&#26816;&#32034;&#24615;&#33021;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#36890;&#36807;&#27169;&#25311;&#21644;&#29992;&#25143;&#30740;&#31350;&#35777;&#26126;&#20102;&#20854;&#26174;&#33879;&#30340;&#25913;&#36827;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;&#22312;&#25191;&#27861;&#23398;&#20013;&#38754;&#20020;&#30340;&#20851;&#38190;&#25361;&#25112;&#20043;&#19968;&#26159;&#20998;&#26512;&#22823;&#37327;&#30340;&#38750;&#32467;&#26500;&#21270;&#25968;&#23383;&#35777;&#25454;&#65292;&#20363;&#22914;&#22270;&#20687;&#12290;&#24448;&#24448;&#65292;&#38750;&#32467;&#26500;&#21270;&#25968;&#23383;&#35777;&#25454;&#21253;&#21547;&#20102;&#25191;&#27861;&#35843;&#26597;&#20013;&#23453;&#36149;&#30340;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#19968;&#20010;&#33021;&#22815;&#26377;&#25928;&#35782;&#21035;&#19982;&#25191;&#27861;&#30456;&#20851;&#22270;&#20687;&#30340;&#26816;&#32034;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#21517;&#20026;Excalibur&#30340;&#38646;&#26679;&#26412;&#36328;&#27169;&#24577;&#22270;&#20687;&#26816;&#32034;&#31995;&#32479;&#65292;&#25506;&#32034;&#20102;&#20132;&#20114;&#24335;&#23398;&#20064;&#22312;&#25552;&#39640;&#25191;&#27861;&#23398;&#20013;&#22270;&#20687;&#26816;&#32034;&#24615;&#33021;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;Excalibur&#36890;&#36807;&#27169;&#25311;&#21644;&#29992;&#25143;&#30740;&#31350;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#27169;&#25311;&#32467;&#26524;&#34920;&#26126;&#65292;&#20132;&#20114;&#24335;&#23398;&#20064;&#22312;&#25191;&#27861;&#23398;&#20013;&#25552;&#39640;&#26816;&#32034;&#24615;&#33021;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#12290;&#27492;&#22806;&#65292;&#29992;&#25143;&#30740;&#31350;&#21442;&#19982;&#32773;&#33021;&#22815;&#26377;&#25928;&#22320;&#21033;&#29992;&#20132;&#20114;&#24335;&#23398;&#20064;&#30340;&#21147;&#37327;&#12290;&#26368;&#21518;&#65292;&#20182;&#20204;&#35748;&#20026;Excalibur&#26131;&#20110;&#20351;&#29992;&#19988;&#26377;&#25928;&#65292;&#24182;&#23545;&#23558;&#20854;&#24212;&#29992;&#20110;&#26085;&#24120;&#23454;&#36341;&#34920;&#36798;&#20102;&#20852;&#36259;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nowadays, one of the critical challenges in forensics is analyzing the enormous amounts of unstructured digital evidence, such as images. Often, unstructured digital evidence contains precious information for forensic investigations. Therefore, a retrieval system that can effectively identify forensically relevant images is paramount. In this work, we explored the effectiveness of interactive learning in improving image retrieval performance in the forensic domain by proposing Excalibur - a zero-shot cross-modal image retrieval system extended with interactive learning. Excalibur was evaluated using both simulations and a user study. The simulations reveal that interactive learning is highly effective in improving retrieval performance in the forensic domain. Furthermore, user study participants could effectively leverage the power of interactive learning. Finally, they considered Excalibur effective and straightforward to use and expressed interest in using it in their daily practice.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20351;&#29992;CNN-LSTM&#27169;&#22411;&#23545;&#27874;&#26031;&#25512;&#29305;&#30340;&#25919;&#27835;&#24773;&#24863;&#36827;&#34892;&#20998;&#26512;&#65292;&#20351;&#29992;ParsBERT&#36827;&#34892;&#35789;&#27719;&#34920;&#31034;&#65292;&#24182;&#27604;&#36739;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#25928;&#26524;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#65292;&#20854;&#20013;CNN-LSTM&#27169;&#22411;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#20998;&#21035;&#36798;&#21040;&#20102;89%&#21644;71%&#30340;&#20998;&#31867;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2307.07740</link><description>&lt;p&gt;
&#20351;&#29992;CNN-LSTM&#27169;&#22411;&#23545;&#27874;&#26031;&#25512;&#29305;&#30340;&#25919;&#27835;&#24773;&#24863;&#36827;&#34892;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Political Sentiment Analysis of Persian Tweets Using CNN-LSTM Model. (arXiv:2307.07740v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07740
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20351;&#29992;CNN-LSTM&#27169;&#22411;&#23545;&#27874;&#26031;&#25512;&#29305;&#30340;&#25919;&#27835;&#24773;&#24863;&#36827;&#34892;&#20998;&#26512;&#65292;&#20351;&#29992;ParsBERT&#36827;&#34892;&#35789;&#27719;&#34920;&#31034;&#65292;&#24182;&#27604;&#36739;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#25928;&#26524;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#65292;&#20854;&#20013;CNN-LSTM&#27169;&#22411;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#20998;&#21035;&#36798;&#21040;&#20102;89%&#21644;71%&#30340;&#20998;&#31867;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#24863;&#20998;&#26512;&#26159;&#35782;&#21035;&#21644;&#20998;&#31867;&#20154;&#20204;&#23545;&#21508;&#31181;&#35805;&#39064;&#30340;&#24773;&#24863;&#25110;&#35266;&#28857;&#30340;&#36807;&#31243;&#12290;&#36817;&#24180;&#26469;&#65292;&#23545;Twitter&#24773;&#24863;&#30340;&#20998;&#26512;&#25104;&#20026;&#19968;&#20010;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#30340;&#35805;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#31181;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#20998;&#26512;&#27874;&#26031;&#25919;&#27835;&#25512;&#29305;&#30340;&#24773;&#24863;&#12290;&#25105;&#20204;&#20351;&#29992;&#35789;&#34955;&#27169;&#22411;&#21644;ParsBERT&#36827;&#34892;&#35789;&#27719;&#34920;&#31034;&#30340;&#20998;&#26512;&#12290;&#25105;&#20204;&#24212;&#29992;&#20102;&#39640;&#26031;&#26420;&#32032;&#36125;&#21494;&#26031;&#12289;&#26799;&#24230;&#25552;&#21319;&#12289;&#36923;&#36753;&#22238;&#24402;&#12289;&#20915;&#31574;&#26641;&#12289;&#38543;&#26426;&#26862;&#26519;&#20197;&#21450;CNN&#21644;LSTM&#30340;&#32452;&#21512;&#26469;&#20998;&#31867;&#25512;&#29305;&#30340;&#26497;&#24615;&#12290;&#26412;&#30740;&#31350;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;ParsBERT&#23884;&#20837;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#27604;&#26426;&#22120;&#23398;&#20064;&#34920;&#29616;&#26356;&#22909;&#12290;CNN-LSTM&#27169;&#22411;&#22312;&#31532;&#19968;&#20010;&#26377;&#19977;&#31181;&#31867;&#21035;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#20998;&#31867;&#20934;&#30830;&#29575;&#20026;89&#65285;&#65292;&#22312;&#31532;&#20108;&#20010;&#26377;&#19971;&#31181;&#31867;&#21035;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#20998;&#31867;&#20934;&#30830;&#29575;&#20026;71&#65285;&#12290;&#30001;&#20110;&#27874;&#26031;&#35821;&#30340;&#22797;&#26434;&#24615;&#65292;&#36798;&#21040;&#36825;&#19968;&#25928;&#29575;&#27700;&#24179;&#26159;&#19968;&#39033;&#22256;&#38590;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sentiment analysis is the process of identifying and categorizing people's emotions or opinions regarding various topics. The analysis of Twitter sentiment has become an increasingly popular topic in recent years. In this paper, we present several machine learning and a deep learning model to analysis sentiment of Persian political tweets. Our analysis was conducted using Bag of Words and ParsBERT for word representation. We applied Gaussian Naive Bayes, Gradient Boosting, Logistic Regression, Decision Trees, Random Forests, as well as a combination of CNN and LSTM to classify the polarities of tweets. The results of this study indicate that deep learning with ParsBERT embedding performs better than machine learning. The CNN-LSTM model had the highest classification accuracy with 89 percent on the first dataset with three classes and 71 percent on the second dataset with seven classes. Due to the complexity of Persian, it was a difficult task to achieve this level of efficiency.
&lt;/p&gt;</description></item><item><title>Mol-Instructions&#26159;&#19968;&#20010;&#19987;&#38376;&#20026;&#29983;&#29289;&#20998;&#23376;&#39046;&#22495;&#35774;&#35745;&#30340;&#32508;&#21512;&#25351;&#20196;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#29289;&#39046;&#22495;&#20013;&#30340;&#36866;&#24212;&#33021;&#21147;&#21644;&#35748;&#30693;&#25935;&#38160;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.08018</link><description>&lt;p&gt;
Mol-Instructions: &#19968;&#20010;&#22823;&#35268;&#27169;&#29983;&#29289;&#20998;&#23376;&#25351;&#20196;&#25968;&#25454;&#38598;&#65292;&#20026;&#22823;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#25903;&#25345;
&lt;/p&gt;
&lt;p&gt;
Mol-Instructions: A Large-Scale Biomolecular Instruction Dataset for Large Language Models. (arXiv:2306.08018v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08018
&lt;/p&gt;
&lt;p&gt;
Mol-Instructions&#26159;&#19968;&#20010;&#19987;&#38376;&#20026;&#29983;&#29289;&#20998;&#23376;&#39046;&#22495;&#35774;&#35745;&#30340;&#32508;&#21512;&#25351;&#20196;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#29289;&#39046;&#22495;&#20013;&#30340;&#36866;&#24212;&#33021;&#21147;&#21644;&#35748;&#30693;&#25935;&#38160;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20197;&#20854;&#21331;&#36234;&#30340;&#20219;&#21153;&#22788;&#29702;&#33021;&#21147;&#21644;&#21019;&#26032;&#30340;&#36755;&#20986;&#65292;&#22312;&#35768;&#22810;&#39046;&#22495;&#25512;&#21160;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#29983;&#29289;&#20998;&#23376;&#30740;&#31350;&#31561;&#19987;&#19994;&#39046;&#22495;&#30340;&#29087;&#32451;&#24212;&#29992;&#36824;&#21463;&#21040;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Mol-Instructions&#65292;&#36825;&#26159;&#19968;&#20010;&#32463;&#36807;&#31934;&#24515;&#31574;&#21010;&#12289;&#19987;&#38376;&#38024;&#23545;&#29983;&#29289;&#20998;&#23376;&#39046;&#22495;&#35774;&#35745;&#30340;&#32508;&#21512;&#25351;&#20196;&#25968;&#25454;&#38598;&#12290;Mol-Instructions&#30001;&#19977;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#32452;&#25104;&#65306;&#20998;&#23376;&#23548;&#21521;&#25351;&#20196;&#12289;&#34507;&#30333;&#36136;&#23548;&#21521;&#25351;&#20196;&#21644;&#29983;&#29289;&#20998;&#23376;&#25991;&#26412;&#25351;&#20196;&#65292;&#27599;&#20010;&#37096;&#20998;&#37117;&#34987;&#31574;&#21010;&#29992;&#20110;&#22686;&#24378;LLM&#23545;&#29983;&#29289;&#20998;&#23376;&#29305;&#24615;&#21644;&#34892;&#20026;&#30340;&#29702;&#35299;&#21644;&#39044;&#27979;&#33021;&#21147;&#12290;&#36890;&#36807;&#23545;&#20195;&#34920;&#24615;LLM&#30340;&#24191;&#27867;&#25351;&#20196;&#35843;&#25972;&#23454;&#39564;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;Mol-Instructions&#22312;&#22686;&#24378;&#22823;&#27169;&#22411;&#22312;&#29983;&#29289;&#20998;&#23376;&#30740;&#31350;&#22797;&#26434;&#39046;&#22495;&#20869;&#30340;&#36866;&#24212;&#33021;&#21147;&#21644;&#35748;&#30693;&#25935;&#38160;&#24230;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#20174;&#32780;&#20419;&#36827;&#29983;&#29289;&#20998;&#23376;&#39046;&#22495;&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), with their remarkable task-handling capabilities and innovative outputs, have catalyzed significant advancements across a spectrum of fields. However, their proficiency within specialized domains such as biomolecular studies remains limited. To address this challenge, we introduce Mol-Instructions, a meticulously curated, comprehensive instruction dataset expressly designed for the biomolecular realm. Mol-Instructions is composed of three pivotal components: molecule-oriented instructions, protein-oriented instructions, and biomolecular text instructions, each curated to enhance the understanding and prediction capabilities of LLMs concerning biomolecular features and behaviors. Through extensive instruction tuning experiments on the representative LLM, we underscore the potency of Mol-Instructions to enhance the adaptability and cognitive acuity of large models within the complex sphere of biomolecular studies, thereby promoting advancements in the biomol
&lt;/p&gt;</description></item></channel></rss>