<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;MTAMCR&#30340;&#23545;&#35805;&#25512;&#33616;&#38382;&#39064;&#35774;&#23450;&#65292;&#36890;&#36807;&#27599;&#36718;&#35810;&#38382;&#28085;&#30422;&#22810;&#20010;&#23646;&#24615;&#31867;&#22411;&#30340;&#22810;&#36873;&#39064;&#65292;&#25552;&#39640;&#20102;&#20114;&#21160;&#25928;&#29575;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;Chain-of-Choice&#23618;&#27425;&#21270;&#31574;&#30053;&#23398;&#20064;&#26694;&#26550;&#65292;&#25552;&#39640;&#20102;&#23545;&#35805;&#25512;&#33616;&#31995;&#32479;&#30340;&#35810;&#38382;&#25928;&#29575;&#21644;&#25512;&#33616;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.17922</link><description>&lt;p&gt;
Chain-of-Choice&#23618;&#27425;&#21270;&#31574;&#30053;&#23398;&#20064;&#29992;&#20110;&#23545;&#35805;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Chain-of-Choice Hierarchical Policy Learning for Conversational Recommendation. (arXiv:2310.17922v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17922
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;MTAMCR&#30340;&#23545;&#35805;&#25512;&#33616;&#38382;&#39064;&#35774;&#23450;&#65292;&#36890;&#36807;&#27599;&#36718;&#35810;&#38382;&#28085;&#30422;&#22810;&#20010;&#23646;&#24615;&#31867;&#22411;&#30340;&#22810;&#36873;&#39064;&#65292;&#25552;&#39640;&#20102;&#20114;&#21160;&#25928;&#29575;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;Chain-of-Choice&#23618;&#27425;&#21270;&#31574;&#30053;&#23398;&#20064;&#26694;&#26550;&#65292;&#25552;&#39640;&#20102;&#23545;&#35805;&#25512;&#33616;&#31995;&#32479;&#30340;&#35810;&#38382;&#25928;&#29575;&#21644;&#25512;&#33616;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#25512;&#33616;&#31995;&#32479;&#36890;&#36807;&#22810;&#36718;&#20114;&#21160;&#23545;&#35805;&#26469;&#25581;&#31034;&#29992;&#25143;&#20559;&#22909;&#65292;&#26368;&#32456;&#23548;&#21521;&#31934;&#30830;&#21644;&#28385;&#24847;&#30340;&#25512;&#33616;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#23545;&#35805;&#25512;&#33616;&#31995;&#32479;&#20165;&#38480;&#20110;&#26681;&#25454;&#27599;&#36718;&#21333;&#20010;&#23646;&#24615;&#31867;&#22411;&#65288;&#22914;&#39068;&#33394;&#65289;&#35810;&#38382;&#20108;&#36827;&#21046;&#25110;&#22810;&#36873;&#39064;&#65292;&#23548;&#33268;&#20114;&#21160;&#36718;&#25968;&#36807;&#22810;&#65292;&#38477;&#20302;&#20102;&#29992;&#25143;&#20307;&#39564;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#29616;&#23454;&#21644;&#39640;&#25928;&#30340;&#23545;&#35805;&#25512;&#33616;&#38382;&#39064;&#35774;&#23450;&#65292;&#31216;&#20026;&#22810;&#31867;&#22411;&#23646;&#24615;&#22810;&#36718;&#23545;&#35805;&#25512;&#33616;&#65288;MTAMCR&#65289;&#65292;&#35813;&#38382;&#39064;&#35774;&#23450;&#20351;&#24471;&#23545;&#35805;&#25512;&#33616;&#31995;&#32479;&#33021;&#22815;&#22312;&#27599;&#36718;&#20013;&#35810;&#38382;&#28085;&#30422;&#22810;&#20010;&#23646;&#24615;&#31867;&#22411;&#30340;&#22810;&#36873;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#20114;&#21160;&#25928;&#29575;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#23558;MTAMCR&#23450;&#20041;&#20026;&#19968;&#39033;&#23618;&#27425;&#21270;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;Chain-of-Choice&#23618;&#27425;&#21270;&#31574;&#30053;&#23398;&#20064;&#65288;CoCHPL&#65289;&#26694;&#26550;&#26469;&#25552;&#39640;MTAMCR&#20013;&#30340;&#35810;&#38382;&#25928;&#29575;&#21644;&#25512;&#33616;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conversational Recommender Systems (CRS) illuminate user preferences via multi-round interactive dialogues, ultimately navigating towards precise and satisfactory recommendations. However, contemporary CRS are limited to inquiring binary or multi-choice questions based on a single attribute type (e.g., color) per round, which causes excessive rounds of interaction and diminishes the user's experience. To address this, we propose a more realistic and efficient conversational recommendation problem setting, called Multi-Type-Attribute Multi-round Conversational Recommendation (MTAMCR), which enables CRS to inquire about multi-choice questions covering multiple types of attributes in each round, thereby improving interactive efficiency. Moreover, by formulating MTAMCR as a hierarchical reinforcement learning task, we propose a Chain-of-Choice Hierarchical Policy Learning (CoCHPL) framework to enhance both the questioning efficiency and recommendation effectiveness in MTAMCR. Specifically,
&lt;/p&gt;</description></item><item><title>&#24102;&#26377;&#27133;&#32422;&#26463;&#30340;&#25490;&#21517;&#38382;&#39064;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25490;&#21517;&#31639;&#27861;MatchRank&#65292;&#23427;&#22312;&#20505;&#36873;&#20154;&#25353;&#25490;&#21517;&#39034;&#24207;&#34987;&#20154;&#31867;&#20915;&#31574;&#32773;&#35780;&#20272;&#26102;&#65292;&#20135;&#29983;&#26368;&#22823;&#21270;&#22635;&#20805;&#27133;&#20301;&#30340;&#25490;&#21517;&#12290;&#31639;&#27861;&#22312;&#29702;&#35770;&#19978;&#20855;&#26377;&#24378;&#22823;&#30340;&#36924;&#36817;&#20445;&#35777;&#65292;&#24182;&#19988;&#21487;&#20197;&#39640;&#25928;&#23454;&#29616;&#12290; (arXiv:2310.17870v1 [cs.IR])</title><link>http://arxiv.org/abs/2310.17870</link><description>&lt;p&gt;
&#24102;&#26377;&#27133;&#32422;&#26463;&#30340;&#25490;&#21517;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Ranking with Slot Constraints. (arXiv:2310.17870v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17870
&lt;/p&gt;
&lt;p&gt;
&#24102;&#26377;&#27133;&#32422;&#26463;&#30340;&#25490;&#21517;&#38382;&#39064;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25490;&#21517;&#31639;&#27861;MatchRank&#65292;&#23427;&#22312;&#20505;&#36873;&#20154;&#25353;&#25490;&#21517;&#39034;&#24207;&#34987;&#20154;&#31867;&#20915;&#31574;&#32773;&#35780;&#20272;&#26102;&#65292;&#20135;&#29983;&#26368;&#22823;&#21270;&#22635;&#20805;&#27133;&#20301;&#30340;&#25490;&#21517;&#12290;&#31639;&#27861;&#22312;&#29702;&#35770;&#19978;&#20855;&#26377;&#24378;&#22823;&#30340;&#36924;&#36817;&#20445;&#35777;&#65292;&#24182;&#19988;&#21487;&#20197;&#39640;&#25928;&#23454;&#29616;&#12290; (arXiv:2310.17870v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#24102;&#26377;&#27133;&#32422;&#26463;&#30340;&#25490;&#21517;&#38382;&#39064;&#65292;&#36825;&#21487;&#20197;&#29992;&#26469;&#24314;&#27169;&#21508;&#31181;&#24212;&#29992;&#38382;&#39064; - &#20174;&#20855;&#26377;&#19981;&#21516;&#19987;&#19994;&#38480;&#21046;&#27133;&#20301;&#30340;&#22823;&#23398;&#24405;&#21462;&#65292;&#21040;&#22312;&#21307;&#23398;&#35797;&#39564;&#20013;&#26500;&#24314;&#31526;&#21512;&#26465;&#20214;&#30340;&#21442;&#19982;&#32773;&#20998;&#23618;&#38431;&#21015;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20256;&#32479;&#30340;&#27010;&#29575;&#25490;&#21517;&#21407;&#21017;&#65288;PRP&#65289;&#22312;&#24102;&#26377;&#27133;&#32422;&#26463;&#30340;&#25490;&#21517;&#38382;&#39064;&#20013;&#21487;&#33021;&#20250;&#38750;&#24120;&#27425;&#20248;&#65292;&#22240;&#27492;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25490;&#21517;&#31639;&#27861;&#65292;&#31216;&#20026;MatchRank&#12290;MatchRank&#30340;&#30446;&#26631;&#26159;&#22312;&#20505;&#36873;&#20154;&#25353;&#25490;&#21517;&#39034;&#24207;&#30001;&#20154;&#31867;&#20915;&#31574;&#32773;&#36827;&#34892;&#35780;&#20272;&#26102;&#65292;&#20135;&#29983;&#26368;&#22823;&#21270;&#22635;&#20805;&#27133;&#20301;&#30340;&#25490;&#21517;&#12290;&#36825;&#26679;&#65292;MatchRank&#22312;&#24191;&#20041;&#19978;&#26159;PRP&#30340;&#25512;&#24191;&#65292;&#24403;&#27809;&#26377;&#27133;&#32422;&#26463;&#26102;&#65292;&#23427;&#26159;PRP&#30340;&#29305;&#20363;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;MatchRank&#20855;&#26377;&#24378;&#22823;&#30340;&#36924;&#36817;&#20445;&#35777;&#65292;&#27809;&#26377;&#20219;&#20309;&#27133;&#20301;&#25110;&#20505;&#36873;&#20154;&#20043;&#38388;&#30340;&#29420;&#31435;&#24615;&#20551;&#35774;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#39640;&#25928;&#22320;&#23454;&#29616;MatchRank&#12290;&#38500;&#20102;&#29702;&#35770;&#20445;&#35777;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;MatchRank&#30340;&#23454;&#39564;&#32467;&#26524;&#22312;&#19981;&#21516;&#24212;&#29992;&#39046;&#22495;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce the problem of ranking with slot constraints, which can be used to model a wide range of application problems -- from college admission with limited slots for different majors, to composing a stratified cohort of eligible participants in a medical trial. We show that the conventional Probability Ranking Principle (PRP) can be highly sub-optimal for slot-constrained ranking problems, and we devise a new ranking algorithm, called MatchRank. The goal of MatchRank is to produce rankings that maximize the number of filled slots if candidates are evaluated by a human decision maker in the order of the ranking. In this way, MatchRank generalizes the PRP, and it subsumes the PRP as a special case when there are no slot constraints. Our theoretical analysis shows that MatchRank has a strong approximation guarantee without any independence assumptions between slots or candidates. Furthermore, we show how MatchRank can be implemented efficiently. Beyond the theoretical guarantees, em
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;GNN-GMVO&#30340;&#26032;&#22411;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#29992;&#20110;&#20248;&#21270;&#30005;&#23376;&#21830;&#21153;&#20013;&#30456;&#20284;&#21830;&#21697;&#25512;&#33616;&#30340;&#21830;&#21697;&#24635;&#20132;&#26131;&#20215;&#20540;&#65288;GMV&#65289;&#12290;&#23427;&#35299;&#20915;&#20102;&#20256;&#32479;GNN&#26550;&#26500;&#22312;&#20248;&#21270;&#25910;&#20837;&#30456;&#20851;&#30446;&#26631;&#26041;&#38754;&#30340;&#19981;&#36275;&#65292;&#24182;&#36890;&#36807;&#30452;&#25509;&#20248;&#21270;GMV&#26469;&#20445;&#35777;&#25512;&#33616;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.17732</link><description>&lt;p&gt;
GNN-GMVO: &#29992;&#20110;&#20248;&#21270;&#30456;&#20284;&#21830;&#21697;&#25512;&#33616;&#20013;&#30340;&#21830;&#21697;&#24635;&#20132;&#26131;&#20215;&#20540;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
GNN-GMVO: Graph Neural Networks for Optimizing Gross Merchandise Value in Similar Item Recommendation. (arXiv:2310.17732v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17732
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;GNN-GMVO&#30340;&#26032;&#22411;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#29992;&#20110;&#20248;&#21270;&#30005;&#23376;&#21830;&#21153;&#20013;&#30456;&#20284;&#21830;&#21697;&#25512;&#33616;&#30340;&#21830;&#21697;&#24635;&#20132;&#26131;&#20215;&#20540;&#65288;GMV&#65289;&#12290;&#23427;&#35299;&#20915;&#20102;&#20256;&#32479;GNN&#26550;&#26500;&#22312;&#20248;&#21270;&#25910;&#20837;&#30456;&#20851;&#30446;&#26631;&#26041;&#38754;&#30340;&#19981;&#36275;&#65292;&#24182;&#36890;&#36807;&#30452;&#25509;&#20248;&#21270;GMV&#26469;&#20445;&#35777;&#25512;&#33616;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#20284;&#21830;&#21697;&#25512;&#33616;&#26159;&#30005;&#23376;&#21830;&#21153;&#34892;&#19994;&#20013;&#30340;&#20851;&#38190;&#20219;&#21153;&#65292;&#23427;&#24110;&#21161;&#23458;&#25143;&#22522;&#20110;&#20182;&#20204;&#24863;&#20852;&#36259;&#30340;&#20135;&#21697;&#25506;&#32034;&#30456;&#20284;&#21644;&#30456;&#20851;&#30340;&#26367;&#20195;&#21697;&#12290;&#23613;&#31649;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#21487;&#20197;&#29702;&#35299;&#20135;&#21697;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#65292;&#22914;&#30456;&#20284;&#24615;&#12290;&#28982;&#32780;&#65292;&#19982;&#23427;&#20204;&#22312;&#26816;&#32034;&#20219;&#21153;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#21644;&#20248;&#21270;&#30456;&#20851;&#24615;&#30340;&#37325;&#28857;&#30456;&#21453;&#65292;&#24403;&#21069;&#30340;GNN&#26550;&#26500;&#24182;&#26410;&#38024;&#23545;&#26368;&#22823;&#21270;&#19982;&#25910;&#20837;&#30456;&#20851;&#30340;&#30446;&#26631;&#65288;&#22914;&#21830;&#21697;&#24635;&#20132;&#26131;&#20215;&#20540;&#65288;GMV&#65289;&#65289;&#36827;&#34892;&#35774;&#35745;&#65292;&#32780;GMV&#26159;&#30005;&#23376;&#21830;&#21153;&#20844;&#21496;&#30340;&#20027;&#35201;&#19994;&#21153;&#25351;&#26631;&#20043;&#19968;&#12290;&#27492;&#22806;&#65292;&#22312;&#22823;&#35268;&#27169;&#30005;&#23376;&#21830;&#21153;&#31995;&#32479;&#20013;&#23450;&#20041;&#20934;&#30830;&#30340;&#36793;&#20851;&#31995;&#23545;&#20110;GNN&#26469;&#35828;&#26159;&#38750;&#24120;&#22797;&#26434;&#30340;&#65292;&#22240;&#20026;&#21830;&#21697;&#20043;&#38388;&#30340;&#20851;&#31995;&#20855;&#26377;&#24322;&#36136;&#24615;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#35774;&#35745;&#19968;&#31181;&#31216;&#20026;GNN-GMVO&#30340;&#26032;&#22411;GNN&#26550;&#26500;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#35813;&#27169;&#22411;&#30452;&#25509;&#20248;&#21270;GMV&#65292;&#21516;&#26102;&#20445;&#35777;&#25512;&#33616;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Similar item recommendation is a critical task in the e-Commerce industry, which helps customers explore similar and relevant alternatives based on their interested products. Despite the traditional machine learning models, Graph Neural Networks (GNNs), by design, can understand complex relations like similarity between products. However, in contrast to their wide usage in retrieval tasks and their focus on optimizing the relevance, the current GNN architectures are not tailored toward maximizing revenue-related objectives such as Gross Merchandise Value (GMV), which is one of the major business metrics for e-Commerce companies. In addition, defining accurate edge relations in GNNs is non-trivial in large-scale e-Commerce systems, due to the heterogeneity nature of the item-item relationships. This work aims to address these issues by designing a new GNN architecture called GNN-GMVO (Graph Neural Network - Gross Merchandise Value Optimizer). This model directly optimizes GMV while cons
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38899;&#39057;&#25351;&#32441;&#30340;&#38899;&#20048;&#25512;&#33616;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#19981;&#21516;&#30340;&#38899;&#39057;&#29305;&#24449;&#65292;&#36890;&#36807;PCA&#38477;&#32500;&#24182;&#35745;&#31639;&#25351;&#32441;&#38388;&#30340;&#30456;&#20284;&#30697;&#38453;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;89%&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.17655</link><description>&lt;p&gt;
&#22522;&#20110;&#38899;&#39057;&#25351;&#32441;&#30340;&#38899;&#20048;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Music Recommendation Based on Audio Fingerprint. (arXiv:2310.17655v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17655
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38899;&#39057;&#25351;&#32441;&#30340;&#38899;&#20048;&#25512;&#33616;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#19981;&#21516;&#30340;&#38899;&#39057;&#29305;&#24449;&#65292;&#36890;&#36807;PCA&#38477;&#32500;&#24182;&#35745;&#31639;&#25351;&#32441;&#38388;&#30340;&#30456;&#20284;&#30697;&#38453;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;89%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#32467;&#21512;&#20102;&#19981;&#21516;&#30340;&#38899;&#39057;&#29305;&#24449;&#65292;&#20197;&#33719;&#24471;&#26356;&#31283;&#20581;&#30340;&#25351;&#32441;&#26469;&#29992;&#20110;&#38899;&#20048;&#25512;&#33616;&#30340;&#36807;&#31243;&#20013;&#12290;&#36825;&#20123;&#26041;&#27861;&#30340;&#32452;&#21512;&#23548;&#33268;&#20102;&#19968;&#20010;&#39640;&#32500;&#21521;&#37327;&#12290;&#20026;&#20102;&#20943;&#23569;&#20540;&#30340;&#25968;&#37327;&#65292;&#23545;&#24471;&#21040;&#30340;&#25351;&#32441;&#38598;&#21512;&#24212;&#29992;&#20102;&#20027;&#25104;&#20998;&#20998;&#26512;(PCA)&#65292;&#36873;&#25321;&#19982;&#35299;&#37322;&#26041;&#24046;&#20026;95%&#30456;&#23545;&#24212;&#30340;&#20027;&#25104;&#20998;&#30340;&#25968;&#37327;&#12290;&#26368;&#21518;&#65292;&#20351;&#29992;&#36825;&#20123;PCA&#25351;&#32441;&#35745;&#31639;&#20102;&#27599;&#20010;&#25351;&#32441;&#19982;&#25972;&#20010;&#25968;&#25454;&#38598;&#30340;&#30456;&#20284;&#30697;&#38453;&#12290;&#35813;&#36807;&#31243;&#34987;&#24212;&#29992;&#20110;&#20010;&#20154;&#38899;&#20048;&#24211;&#20013;&#30340;200&#39318;&#27468;&#26354;&#65292;&#36825;&#20123;&#27468;&#26354;&#34987;&#26631;&#35760;&#20102;&#33402;&#26415;&#23478;&#23545;&#24212;&#30340;&#27969;&#27966;&#12290;&#22914;&#26524;&#25512;&#33616;&#30340;&#27468;&#26354;&#30340;&#27969;&#27966;&#19982;&#30446;&#26631;&#27468;&#26354;&#30340;&#27969;&#27966;&#21305;&#37197;&#65292;&#21017;&#34987;&#35780;&#20026;&#25104;&#21151;&#30340;&#25512;&#33616;(&#26681;&#25454;&#25351;&#32441;&#30340;&#30456;&#20284;&#24230;)&#12290;&#36890;&#36807;&#36825;&#20010;&#36807;&#31243;&#65292;&#21487;&#20197;&#33719;&#24471;89%&#30340;&#20934;&#30830;&#29575;(&#25104;&#21151;&#30340;&#25512;&#33616;&#21344;&#24635;&#20849;&#30340;&#25512;&#33616;&#35831;&#27714;)&#12290;
&lt;/p&gt;
&lt;p&gt;
This work combined different audio features to obtain a more robust fingerprint to be used in a music recommendation process. The combination of these methods resulted in a high-dimensional vector. To reduce the number of values, PCA was applied to the set of resulting fingerprints, selecting the number of principal components that corresponded to an explained variance of $95\%$. Finally, with these PCA-fingerprints, the similarity matrix of each fingerprint with the entire data set was calculated. The process was applied to 200 songs from a personal music library; the songs were tagged with the artists' corresponding genres. The recommendations (fingerprints of songs with the closest similarity) were rated successful if the recommended songs' genre matched the target songs' genre. With this procedure, it was possible to obtain an accuracy of $89\%$ (successful recommendations out of total recommendation requests).
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22797;&#26434;&#32593;&#32476;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#24314;&#27169;&#21644;&#25366;&#25496;&#24739;&#32773;&#36335;&#24452;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#36335;&#24452;&#27169;&#22411;&#12289;&#26032;&#30340;&#30456;&#20284;&#24230;&#27979;&#37327;&#26041;&#27861;&#21644;&#22522;&#20110;&#20256;&#32479;&#20013;&#24515;&#24230;&#30340;&#25366;&#25496;&#26041;&#27861;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#35813;&#26694;&#26550;&#21487;&#26377;&#25928;&#24212;&#29992;&#20110;&#23454;&#38469;&#21307;&#30103;&#25968;&#25454;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2309.14208</link><description>&lt;p&gt;
&#22522;&#20110;&#22797;&#26434;&#32593;&#32476;&#30340;&#26694;&#26550;&#29992;&#20110;&#24314;&#27169;&#21644;&#25366;&#25496;&#24739;&#32773;&#36335;&#24452;
&lt;/p&gt;
&lt;p&gt;
Framework based on complex networks to model and mine patient pathways. (arXiv:2309.14208v2 [cs.CY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14208
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22797;&#26434;&#32593;&#32476;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#24314;&#27169;&#21644;&#25366;&#25496;&#24739;&#32773;&#36335;&#24452;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#36335;&#24452;&#27169;&#22411;&#12289;&#26032;&#30340;&#30456;&#20284;&#24230;&#27979;&#37327;&#26041;&#27861;&#21644;&#22522;&#20110;&#20256;&#32479;&#20013;&#24515;&#24230;&#30340;&#25366;&#25496;&#26041;&#27861;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#35813;&#26694;&#26550;&#21487;&#26377;&#25928;&#24212;&#29992;&#20110;&#23454;&#38469;&#21307;&#30103;&#25968;&#25454;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21457;&#29616;&#29992;&#20110;&#34920;&#31034;&#19968;&#32452;&#24739;&#32773;&#19982;&#21307;&#30103;&#31995;&#32479;&#30340;&#25509;&#35302;&#21382;&#21490;&#30340;&#27169;&#22411;&#65292;&#21363;&#25152;&#35859;&#30340;&#8220;&#24739;&#32773;&#36335;&#24452;&#8221;&#65292;&#26159;&#19968;&#39033;&#26032;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#23427;&#25903;&#25345;&#20020;&#24202;&#21644;&#32452;&#32455;&#20915;&#31574;&#65292;&#20197;&#25552;&#39640;&#25552;&#20379;&#30340;&#27835;&#30103;&#36136;&#37327;&#21644;&#25928;&#29575;&#12290;&#24930;&#24615;&#30149;&#24739;&#32773;&#30340;&#36335;&#24452;&#24448;&#24448;&#22240;&#20154;&#32780;&#24322;&#65292;&#26377;&#37325;&#22797;&#30340;&#20219;&#21153;&#65292;&#24182;&#38656;&#35201;&#20998;&#26512;&#22810;&#20010;&#26041;&#38754;&#65288;&#24178;&#39044;&#12289;&#35786;&#26029;&#12289;&#21307;&#30103;&#19987;&#19994;&#31561;&#65289;&#65292;&#24433;&#21709;&#32467;&#26524;&#12290;&#22240;&#27492;&#65292;&#24314;&#27169;&#21644;&#25366;&#25496;&#36825;&#20123;&#36335;&#24452;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21253;&#25324;&#65306;&#65288;i&#65289;&#22522;&#20110;&#22810;&#26041;&#38754;&#22270;&#30340;&#36335;&#24452;&#27169;&#22411;&#65292;&#65288;ii&#65289;&#19968;&#31181;&#26032;&#30340;&#30456;&#20284;&#24230;&#27979;&#37327;&#26041;&#27861;&#65292;&#32771;&#34385;&#20102;&#32791;&#26102;&#65292;&#29992;&#20110;&#27604;&#36739;&#36335;&#24452;&#65292;&#24182;&#19988;&#65288;iii&#65289;&#22522;&#20110;&#20256;&#32479;&#20013;&#24515;&#24230;&#27979;&#37327;&#26041;&#27861;&#30340;&#25366;&#25496;&#26041;&#27861;&#65292;&#29992;&#20110;&#21457;&#29616;&#36335;&#24452;&#20013;&#26368;&#30456;&#20851;&#30340;&#27493;&#39588;&#12290;&#25105;&#20204;&#20351;&#29992;&#23454;&#38469;&#21307;&#30103;&#25968;&#25454;&#35780;&#20272;&#20102;&#36825;&#20010;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
The automatic discovery of a model to represent the history of encounters of a group of patients with the healthcare system -- the so-called "pathway of patients" -- is a new field of research that supports clinical and organisational decisions to improve the quality and efficiency of the treatment provided. The pathways of patients with chronic conditions tend to vary significantly from one person to another, have repetitive tasks, and demand the analysis of multiple perspectives (interventions, diagnoses, medical specialities, among others) influencing the results. Therefore, modelling and mining those pathways is still a challenging task. In this work, we propose a framework comprising: (i) a pathway model based on a multi-aspect graph, (ii) a novel dissimilarity measurement to compare pathways taking the elapsed time into account, and (iii) a mining method based on traditional centrality measures to discover the most relevant steps of the pathways. We evaluated the framework using 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31687;&#20851;&#20110;&#36328;&#27169;&#24577;&#26816;&#32034;&#30340;&#26041;&#27861;&#21644;&#26410;&#26469;&#26041;&#21521;&#30340;&#31995;&#32479;&#32508;&#36848;&#65292;&#20174;&#27973;&#23618;&#32479;&#35745;&#20998;&#26512;&#21040;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#28145;&#20837;&#25506;&#35752;&#20102;&#29616;&#26377;&#36328;&#27169;&#24577;&#26816;&#32034;&#26041;&#27861;&#30340;&#21407;&#29702;&#21644;&#26550;&#26500;&#12290;</title><link>http://arxiv.org/abs/2308.14263</link><description>&lt;p&gt;
&#36328;&#27169;&#24577;&#26816;&#32034;&#65306;&#26041;&#27861;&#21644;&#26410;&#26469;&#26041;&#21521;&#30340;&#31995;&#32479;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Cross-Modal Retrieval: A Systematic Review of Methods and Future Directions. (arXiv:2308.14263v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14263
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31687;&#20851;&#20110;&#36328;&#27169;&#24577;&#26816;&#32034;&#30340;&#26041;&#27861;&#21644;&#26410;&#26469;&#26041;&#21521;&#30340;&#31995;&#32479;&#32508;&#36848;&#65292;&#20174;&#27973;&#23618;&#32479;&#35745;&#20998;&#26512;&#21040;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#28145;&#20837;&#25506;&#35752;&#20102;&#29616;&#26377;&#36328;&#27169;&#24577;&#26816;&#32034;&#26041;&#27861;&#30340;&#21407;&#29702;&#21644;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22810;&#26679;&#21270;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#29190;&#28856;&#24615;&#22686;&#38271;&#65292;&#20256;&#32479;&#30340;&#21333;&#27169;&#24577;&#26816;&#32034;&#26041;&#27861;&#38590;&#20197;&#28385;&#36275;&#29992;&#25143;&#23545;&#19981;&#21516;&#27169;&#24577;&#25968;&#25454;&#35775;&#38382;&#30340;&#38656;&#27714;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#36328;&#27169;&#24577;&#26816;&#32034;&#24212;&#36816;&#32780;&#29983;&#65292;&#23427;&#33021;&#22815;&#23454;&#29616;&#36328;&#27169;&#24577;&#20132;&#20114;&#65292;&#20419;&#36827;&#35821;&#20041;&#21305;&#37197;&#65292;&#24182;&#21033;&#29992;&#19981;&#21516;&#27169;&#24577;&#25968;&#25454;&#20043;&#38388;&#30340;&#20114;&#34917;&#24615;&#21644;&#19968;&#33268;&#24615;&#12290;&#23613;&#31649;&#20197;&#24448;&#30340;&#25991;&#29486;&#23545;&#36328;&#27169;&#24577;&#26816;&#32034;&#39046;&#22495;&#36827;&#34892;&#20102;&#32508;&#36848;&#65292;&#20294;&#23384;&#22312;&#30528;&#20851;&#20110;&#21450;&#26102;&#24615;&#12289;&#20998;&#31867;&#20307;&#31995;&#21644;&#20840;&#38754;&#24615;&#31561;&#26041;&#38754;&#30340;&#32570;&#38519;&#12290;&#26412;&#25991;&#23545;&#36328;&#27169;&#24577;&#26816;&#32034;&#30340;&#21457;&#23637;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#32508;&#36848;&#65292;&#28085;&#30422;&#20102;&#20174;&#27973;&#23618;&#32479;&#35745;&#20998;&#26512;&#25216;&#26415;&#21040;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#28436;&#36827;&#12290;&#25991;&#31456;&#39318;&#20808;&#20174;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#12289;&#26426;&#21046;&#21644;&#27169;&#22411;&#30340;&#35282;&#24230;&#26500;&#24314;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#20998;&#31867;&#20307;&#31995;&#65292;&#28982;&#21518;&#28145;&#20837;&#25506;&#35752;&#20102;&#29616;&#26377;&#36328;&#27169;&#24577;&#26816;&#32034;&#26041;&#27861;&#30340;&#21407;&#29702;&#21644;&#26550;&#26500;&#12290;&#27492;&#22806;&#65292;&#25991;&#31456;&#36824;&#27010;&#36848;&#20102;&#24403;&#21069;&#24191;&#27867;&#20351;&#29992;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#12289;&#24615;&#33021;&#35780;&#20215;&#25351;&#26631;&#21644;&#24120;&#35265;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the exponential surge in diverse multi-modal data, traditional uni-modal retrieval methods struggle to meet the needs of users demanding access to data from various modalities. To address this, cross-modal retrieval has emerged, enabling interaction across modalities, facilitating semantic matching, and leveraging complementarity and consistency between different modal data. Although prior literature undertook a review of the cross-modal retrieval field, it exhibits numerous deficiencies pertaining to timeliness, taxonomy, and comprehensiveness. This paper conducts a comprehensive review of cross-modal retrieval's evolution, spanning from shallow statistical analysis techniques to vision-language pre-training models. Commencing with a comprehensive taxonomy grounded in machine learning paradigms, mechanisms, and models, the paper then delves deeply into the principles and architectures underpinning existing cross-modal retrieval methods. Furthermore, it offers an overview of widel
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#35821;&#20041;&#30693;&#35782;&#22270;&#33258;&#21160;&#21019;&#24314;&#25919;&#31574;&#36777;&#35770;&#26696;&#20363;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20105;&#35770;&#30340;&#35821;&#20041;&#30693;&#35782;&#22270;&#19978;&#36827;&#34892;&#38480;&#21046;&#26368;&#30701;&#36335;&#24452;&#36941;&#21382;&#65292;&#26377;&#25928;&#26500;&#24314;&#39640;&#36136;&#37327;&#30340;&#36777;&#35770;&#26696;&#20363;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#32654;&#22269;&#31454;&#36187;&#36777;&#35770;&#20013;&#65292;&#21033;&#29992;&#36825;&#31181;&#26041;&#27861;&#26174;&#33879;&#25913;&#36827;&#20102;&#24050;&#26377;&#25968;&#25454;&#38598;DebateSum&#65292;&#24182;&#36129;&#29486;&#20102;&#26032;&#30340;&#20363;&#23376;&#21644;&#26377;&#29992;&#30340;&#20803;&#25968;&#25454;&#12290;&#36890;&#36807;&#20351;&#29992;txtai&#35821;&#20041;&#25628;&#32034;&#21644;&#30693;&#35782;&#22270;&#24037;&#20855;&#38142;&#65292;&#21019;&#24314;&#21644;&#36129;&#29486;&#20102;9&#20010;&#35821;&#20041;&#30693;&#35782;&#22270;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#35780;&#20272;&#26041;&#27861;&#26469;&#30830;&#23450;&#21738;&#20010;&#30693;&#35782;&#22270;&#26356;&#36866;&#21512;&#25919;&#31574;&#36777;&#35770;&#26696;&#20363;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2307.04090</link><description>&lt;p&gt;
DebateKG: &#29992;&#35821;&#20041;&#30693;&#35782;&#22270;&#33258;&#21160;&#21019;&#24314;&#25919;&#31574;&#36777;&#35770;&#26696;&#20363;
&lt;/p&gt;
&lt;p&gt;
DebateKG: Automatic Policy Debate Case Creation with Semantic Knowledge Graphs. (arXiv:2307.04090v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04090
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#35821;&#20041;&#30693;&#35782;&#22270;&#33258;&#21160;&#21019;&#24314;&#25919;&#31574;&#36777;&#35770;&#26696;&#20363;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20105;&#35770;&#30340;&#35821;&#20041;&#30693;&#35782;&#22270;&#19978;&#36827;&#34892;&#38480;&#21046;&#26368;&#30701;&#36335;&#24452;&#36941;&#21382;&#65292;&#26377;&#25928;&#26500;&#24314;&#39640;&#36136;&#37327;&#30340;&#36777;&#35770;&#26696;&#20363;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#32654;&#22269;&#31454;&#36187;&#36777;&#35770;&#20013;&#65292;&#21033;&#29992;&#36825;&#31181;&#26041;&#27861;&#26174;&#33879;&#25913;&#36827;&#20102;&#24050;&#26377;&#25968;&#25454;&#38598;DebateSum&#65292;&#24182;&#36129;&#29486;&#20102;&#26032;&#30340;&#20363;&#23376;&#21644;&#26377;&#29992;&#30340;&#20803;&#25968;&#25454;&#12290;&#36890;&#36807;&#20351;&#29992;txtai&#35821;&#20041;&#25628;&#32034;&#21644;&#30693;&#35782;&#22270;&#24037;&#20855;&#38142;&#65292;&#21019;&#24314;&#21644;&#36129;&#29486;&#20102;9&#20010;&#35821;&#20041;&#30693;&#35782;&#22270;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#35780;&#20272;&#26041;&#27861;&#26469;&#30830;&#23450;&#21738;&#20010;&#30693;&#35782;&#22270;&#26356;&#36866;&#21512;&#25919;&#31574;&#36777;&#35770;&#26696;&#20363;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30456;&#20851;&#24037;&#20316;&#34920;&#26126;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#22312;&#35299;&#20915;&#31454;&#36187;&#36777;&#35770;&#20013;&#30340;&#38382;&#39064;&#26041;&#38754;&#20855;&#26377;&#24212;&#29992;&#24615;&#12290;&#31454;&#36187;&#36777;&#35770;&#20013;&#26368;&#37325;&#35201;&#30340;&#20219;&#21153;&#20043;&#19968;&#26159;&#36777;&#25163;&#21019;&#24314;&#39640;&#36136;&#37327;&#30340;&#36777;&#35770;&#26696;&#20363;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#38480;&#21046;&#26368;&#30701;&#36335;&#24452;&#36941;&#21382;&#22312;&#20105;&#35770;&#30340;&#35821;&#20041;&#30693;&#35782;&#22270;&#19978;&#26500;&#24314;&#26377;&#25928;&#30340;&#36777;&#35770;&#26696;&#20363;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#21517;&#20026;DebateSum&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#30740;&#31350;&#20102;&#36825;&#31181;&#28508;&#21147;&#65292;&#35813;&#25968;&#25454;&#38598;&#38024;&#23545;&#30340;&#26159;&#19968;&#31181;&#21517;&#20026;&#25919;&#31574;&#36777;&#35770;&#30340;&#32654;&#22269;&#31454;&#36187;&#36777;&#35770;&#31867;&#22411;&#12290;&#25105;&#20204;&#36890;&#36807;&#21521;&#25968;&#25454;&#38598;&#20013;&#24341;&#20837;53180&#20010;&#26032;&#30340;&#20363;&#23376;&#65292;&#24182;&#20026;&#27599;&#20010;&#20363;&#23376;&#25552;&#20379;&#36827;&#19968;&#27493;&#26377;&#29992;&#30340;&#20803;&#25968;&#25454;&#65292;&#26174;&#33879;&#25913;&#36827;&#20102;DebateSum&#12290;&#25105;&#20204;&#21033;&#29992;txtai&#35821;&#20041;&#25628;&#32034;&#21644;&#30693;&#35782;&#22270;&#24037;&#20855;&#38142;&#22522;&#20110;&#36825;&#20010;&#25968;&#25454;&#38598;&#20135;&#29983;&#24182;&#36129;&#29486;&#20102;9&#20010;&#35821;&#20041;&#30693;&#35782;&#22270;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#20197;&#30830;&#23450;&#22312;&#25919;&#31574;&#36777;&#35770;&#26696;&#20363;&#29983;&#25104;&#30340;&#32972;&#26223;&#19979;&#21738;&#20010;&#30693;&#35782;&#22270;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work within the Argument Mining community has shown the applicability of Natural Language Processing systems for solving problems found within competitive debate. One of the most important tasks within competitive debate is for debaters to create high quality debate cases. We show that effective debate cases can be constructed using constrained shortest path traversals on Argumentative Semantic Knowledge Graphs. We study this potential in the context of a type of American Competitive Debate, called Policy Debate, which already has a large scale dataset targeting it called DebateSum. We significantly improve upon DebateSum by introducing 53180 new examples, as well as further useful metadata for every example, to the dataset. We leverage the txtai semantic search and knowledge graph toolchain to produce and contribute 9 semantic knowledge graphs built on this dataset. We create a unique method for evaluating which knowledge graphs are better in the context of producing policy deb
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#21487;&#35270;&#20016;&#23500;&#25991;&#20214;&#22270;&#20687;&#20013;&#36827;&#34892;&#34920;&#26684;&#26816;&#27979;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;IoU&#20998;&#35299;&#20026;&#30495;&#23454;&#35206;&#30422;&#39033;&#21644;&#39044;&#27979;&#35206;&#30422;&#39033;&#26469;&#34913;&#37327;&#39044;&#27979;&#32467;&#26524;&#30340;&#20449;&#24687;&#20002;&#22833;&#31243;&#24230;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#20351;&#29992;&#39640;&#26031;&#22122;&#22768;&#22686;&#24378;&#30340;&#22270;&#20687;&#22823;&#23567;&#21306;&#22495;&#25552;&#26696;&#21644;&#22810;&#23545;&#19968;&#26631;&#31614;&#20998;&#37197;&#65292;&#36827;&#19968;&#27493;&#25913;&#36827;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.19181</link><description>&lt;p&gt;
&#21487;&#35270;&#20016;&#23500;&#25991;&#20214;&#22270;&#20687;&#30340;&#34920;&#26684;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Table Detection for Visually Rich Document Images. (arXiv:2305.19181v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19181
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#21487;&#35270;&#20016;&#23500;&#25991;&#20214;&#22270;&#20687;&#20013;&#36827;&#34892;&#34920;&#26684;&#26816;&#27979;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;IoU&#20998;&#35299;&#20026;&#30495;&#23454;&#35206;&#30422;&#39033;&#21644;&#39044;&#27979;&#35206;&#30422;&#39033;&#26469;&#34913;&#37327;&#39044;&#27979;&#32467;&#26524;&#30340;&#20449;&#24687;&#20002;&#22833;&#31243;&#24230;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#20351;&#29992;&#39640;&#26031;&#22122;&#22768;&#22686;&#24378;&#30340;&#22270;&#20687;&#22823;&#23567;&#21306;&#22495;&#25552;&#26696;&#21644;&#22810;&#23545;&#19968;&#26631;&#31614;&#20998;&#37197;&#65292;&#36827;&#19968;&#27493;&#25913;&#36827;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#26684;&#26816;&#27979;&#26159;&#23454;&#29616;&#21487;&#35270;&#20016;&#23500;&#25991;&#20214;&#29702;&#35299;&#30340;&#22522;&#26412;&#20219;&#21153;&#65292;&#38656;&#35201;&#27169;&#22411;&#22312;&#25552;&#21462;&#20449;&#24687;&#26102;&#36991;&#20813;&#20449;&#24687;&#20002;&#22833;&#12290;&#28982;&#32780;&#65292;&#24120;&#29992;&#30340;&#20132;&#21449;&#32852;&#21512;&#65288;IoU&#65289;&#35780;&#20272;&#25351;&#26631;&#21644;&#22522;&#20110;IoU&#30340;&#26816;&#27979;&#27169;&#22411;&#25439;&#22833;&#20989;&#25968;&#26080;&#27861;&#30452;&#25509;&#34920;&#31034;&#39044;&#27979;&#32467;&#26524;&#30340;&#20449;&#24687;&#20002;&#22833;&#31243;&#24230;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;IoU&#20998;&#35299;&#20026;&#30495;&#23454;&#35206;&#30422;&#39033;&#21644;&#39044;&#27979;&#35206;&#30422;&#39033;&#65292;&#20854;&#20013;&#21069;&#32773;&#21487;&#20197;&#29992;&#20110;&#34913;&#37327;&#39044;&#27979;&#32467;&#26524;&#30340;&#20449;&#24687;&#20002;&#22833;&#12290;&#27492;&#22806;&#65292;&#32771;&#34385;&#21040;&#25991;&#26723;&#22270;&#20687;&#20013;&#34920;&#26684;&#30340;&#31232;&#30095;&#20998;&#24067;&#65292;&#25105;&#20204;&#20351;&#29992;SparseR-CNN&#20316;&#20026;&#22522;&#30784;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#39640;&#26031;&#22122;&#22768;&#22686;&#24378;&#30340;&#22270;&#20687;&#22823;&#23567;&#21306;&#22495;&#25552;&#26696;&#21644;&#22810;&#23545;&#19968;&#26631;&#31614;&#20998;&#37197;&#26469;&#36827;&#19968;&#27493;&#25913;&#36827;&#27169;&#22411;&#12290;&#20840;&#38754;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#19981;&#21516;IoU&#25351;&#26631;&#19979;&#30340;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#22987;&#32456;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Table Detection (TD) is a fundamental task to enable visually rich document understanding, which requires the model to extract information without information loss. However, popular Intersection over Union (IoU) based evaluation metrics and IoU-based loss functions for the detection models cannot directly represent the degree of information loss for the prediction results. Therefore, we propose to decouple IoU into a ground truth coverage term and a prediction coverage term, in which the former can be used to measure the information loss of the prediction results. Besides, considering the sparse distribution of tables in document images, we use SparseR-CNN as the base model and further improve the model by using Gaussian Noise Augmented Image Size region proposals and many-to-one label assignments. Results under comprehensive experiments show that the proposed method can consistently outperform state-of-the-art methods with different IoU-based metrics under various datasets and demonst
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26696;&#20363;&#25512;&#29702;&#30340;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#26041;&#27861;&#65288;CBR-MRC&#65289;&#65292;&#36890;&#36807;&#20174;&#23384;&#20648;&#22120;&#20013;&#26816;&#32034;&#30456;&#20284;&#26696;&#20363;&#24182;&#36873;&#25321;&#26368;&#31867;&#20284;&#30340;&#19978;&#19979;&#25991;&#26469;&#39044;&#27979;&#31572;&#26696;&#65292;&#20197;&#36798;&#21040;&#39640;&#20934;&#30830;&#24615;&#12290;&#22312;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#21644;&#26032;&#38395;&#38382;&#31572;&#20013;&#65292;CBR-MRC&#30340;&#20934;&#30830;&#24615;&#36229;&#36807;&#22522;&#20934;&#65292;&#24182;&#19988;&#33021;&#22815;&#35782;&#21035;&#19982;&#20854;&#20182;&#35780;&#20272;&#21592;&#19981;&#21516;&#30340;&#31572;&#26696;&#12290;</title><link>http://arxiv.org/abs/2305.14815</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#26696;&#20363;&#25512;&#29702;&#30340;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Machine Reading Comprehension using Case-based Reasoning. (arXiv:2305.14815v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14815
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26696;&#20363;&#25512;&#29702;&#30340;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#26041;&#27861;&#65288;CBR-MRC&#65289;&#65292;&#36890;&#36807;&#20174;&#23384;&#20648;&#22120;&#20013;&#26816;&#32034;&#30456;&#20284;&#26696;&#20363;&#24182;&#36873;&#25321;&#26368;&#31867;&#20284;&#30340;&#19978;&#19979;&#25991;&#26469;&#39044;&#27979;&#31572;&#26696;&#65292;&#20197;&#36798;&#21040;&#39640;&#20934;&#30830;&#24615;&#12290;&#22312;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#21644;&#26032;&#38395;&#38382;&#31572;&#20013;&#65292;CBR-MRC&#30340;&#20934;&#30830;&#24615;&#36229;&#36807;&#22522;&#20934;&#65292;&#24182;&#19988;&#33021;&#22815;&#35782;&#21035;&#19982;&#20854;&#20182;&#35780;&#20272;&#21592;&#19981;&#21516;&#30340;&#31572;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20934;&#30830;&#19988;&#21487;&#35299;&#37322;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#20013;&#30340;&#31572;&#26696;&#25552;&#21462;&#65292;&#35813;&#26041;&#27861;&#31867;&#20284;&#20110;&#32463;&#20856;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#22522;&#20110;&#26696;&#20363;&#25512;&#29702;&#65288;CBR&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#65288;CBR-MRC&#65289;&#22522;&#20110;&#19968;&#20010;&#20551;&#35774;&#65292;&#21363;&#30456;&#20284;&#38382;&#39064;&#30340;&#19978;&#19979;&#25991;&#21270;&#31572;&#26696;&#24444;&#27492;&#20043;&#38388;&#20855;&#26377;&#35821;&#20041;&#30456;&#20284;&#24615;&#12290;&#32473;&#23450;&#19968;&#20010;&#27979;&#35797;&#38382;&#39064;&#65292;CBR-MRC&#39318;&#20808;&#20174;&#38750;&#21442;&#25968;&#21270;&#23384;&#20648;&#22120;&#20013;&#26816;&#32034;&#19968;&#32452;&#30456;&#20284;&#30340;&#26696;&#20363;&#65292;&#28982;&#21518;&#36890;&#36807;&#36873;&#25321;&#27979;&#35797;&#19978;&#19979;&#25991;&#20013;&#26368;&#31867;&#20284;&#20110;&#26816;&#32034;&#21040;&#30340;&#26696;&#20363;&#20013;&#19978;&#19979;&#25991;&#21270;&#31572;&#26696;&#34920;&#31034;&#30340;&#33539;&#22260;&#26469;&#39044;&#27979;&#31572;&#26696;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21322;&#21442;&#25968;&#21270;&#30340;&#29305;&#24615;&#20351;&#20854;&#33021;&#22815;&#23558;&#39044;&#27979;&#24402;&#22240;&#20110;&#29305;&#23450;&#30340;&#35777;&#25454;&#26696;&#20363;&#38598;&#65292;&#22240;&#27492;&#22312;&#26500;&#24314;&#21487;&#38752;&#19988;&#21487;&#35843;&#35797;&#30340;&#38382;&#31572;&#31995;&#32479;&#26102;&#26159;&#19968;&#20010;&#29702;&#24819;&#30340;&#36873;&#25321;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;CBR-MRC&#22312;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#65288;NaturalQuestions&#65289;&#21644;&#26032;&#38395;&#38382;&#31572;&#65288;NewsQA&#65289;&#19978;&#27604;&#22823;&#22411;&#35835;&#32773;&#27169;&#22411;&#25552;&#20379;&#20102;&#39640;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#20248;&#20110;&#22522;&#20934;&#20998;&#21035;&#25552;&#21319;&#20102;11.5&#21644;8.4 EM&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;CBR-MRC&#22312;&#35782;&#21035;&#19982;&#20182;&#20154;&#35780;&#20272;&#21592;&#19981;&#21516;&#30340;&#31572;&#26696;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present an accurate and interpretable method for answer extraction in machine reading comprehension that is reminiscent of case-based reasoning (CBR) from classical AI. Our method (CBR-MRC) builds upon the hypothesis that contextualized answers to similar questions share semantic similarities with each other. Given a test question, CBR-MRC first retrieves a set of similar cases from a non-parametric memory and then predicts an answer by selecting the span in the test context that is most similar to the contextualized representations of answers in the retrieved cases. The semi-parametric nature of our approach allows it to attribute a prediction to the specific set of evidence cases, making it a desirable choice for building reliable and debuggable QA systems. We show that CBR-MRC provides high accuracy comparable with large reader models and outperforms baselines by 11.5 and 8.4 EM on NaturalQuestions and NewsQA, respectively. Further, we demonstrate the ability of CBR-MRC in identi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#25512;&#33616;&#39046;&#22495;&#24191;&#27867;&#20351;&#29992;&#30340;ChatGPT&#30340;&#28508;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#21363;&#20351;&#27809;&#26377;&#24494;&#35843;&#65292;ChatGPT&#22312;&#20116;&#20010;&#25512;&#33616;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20855;&#26377;&#24456;&#22909;&#30340;&#25512;&#33616;&#31934;&#24230;&#21644;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.10149</link><description>&lt;p&gt;
ChatGPT&#26159;&#19968;&#20010;&#22909;&#30340;&#25512;&#33616;&#31639;&#27861;&#21527;&#65311;&#21021;&#27493;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Is ChatGPT a Good Recommender? A Preliminary Study. (arXiv:2304.10149v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10149
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#25512;&#33616;&#39046;&#22495;&#24191;&#27867;&#20351;&#29992;&#30340;ChatGPT&#30340;&#28508;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#21363;&#20351;&#27809;&#26377;&#24494;&#35843;&#65292;ChatGPT&#22312;&#20116;&#20010;&#25512;&#33616;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20855;&#26377;&#24456;&#22909;&#30340;&#25512;&#33616;&#31934;&#24230;&#21644;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#22312;&#36807;&#21435;&#20960;&#21313;&#24180;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#24182;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20256;&#32479;&#25512;&#33616;&#26041;&#27861;&#37117;&#26159;&#29305;&#23450;&#20219;&#21153;&#30340;&#65292;&#22240;&#27492;&#32570;&#20047;&#26377;&#25928;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#26368;&#36817;&#65292;ChatGPT&#30340;&#20986;&#29616;&#36890;&#36807;&#22686;&#24378;&#23545;&#35805;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#26174;&#33879;&#25512;&#36827;&#20102;NLP&#20219;&#21153;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;ChatGPT&#22312;&#25512;&#33616;&#39046;&#22495;&#30340;&#24212;&#29992;&#36824;&#27809;&#26377;&#24471;&#21040;&#20805;&#20998;&#30340;&#30740;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;ChatGPT&#20316;&#20026;&#36890;&#29992;&#25512;&#33616;&#27169;&#22411;&#65292;&#25506;&#35752;&#23427;&#23558;&#20174;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;&#20013;&#33719;&#24471;&#30340;&#24191;&#27867;&#35821;&#35328;&#21644;&#19990;&#30028;&#30693;&#35782;&#36716;&#31227;&#21040;&#25512;&#33616;&#22330;&#26223;&#20013;&#30340;&#28508;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#32452;&#25552;&#31034;&#65292;&#24182;&#35780;&#20272;ChatGPT&#22312;&#20116;&#20010;&#25512;&#33616;&#22330;&#26223;&#20013;&#30340;&#34920;&#29616;&#12290;&#19982;&#20256;&#32479;&#30340;&#25512;&#33616;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#22312;&#25972;&#20010;&#35780;&#20272;&#36807;&#31243;&#20013;&#25105;&#20204;&#19981;&#24494;&#35843;ChatGPT&#65292;&#20165;&#20381;&#38752;&#25552;&#31034;&#33258;&#36523;&#23558;&#25512;&#33616;&#20219;&#21153;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommendation systems have witnessed significant advancements and have been widely used over the past decades. However, most traditional recommendation methods are task-specific and therefore lack efficient generalization ability. Recently, the emergence of ChatGPT has significantly advanced NLP tasks by enhancing the capabilities of conversational models. Nonetheless, the application of ChatGPT in the recommendation domain has not been thoroughly investigated. In this paper, we employ ChatGPT as a general-purpose recommendation model to explore its potential for transferring extensive linguistic and world knowledge acquired from large-scale corpora to recommendation scenarios. Specifically, we design a set of prompts and evaluate ChatGPT's performance on five recommendation scenarios. Unlike traditional recommendation methods, we do not fine-tune ChatGPT during the entire evaluation process, relying only on the prompts themselves to convert recommendation tasks into natural language 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#29983;&#25104;&#24615;LLMs&#65292;&#22914;ChatGPT&#21644;GPT-4&#22312;&#20449;&#24687;&#26816;&#32034;&#20013;&#30340;&#30456;&#20851;&#24615;&#25490;&#21517;&#33021;&#21147;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#27169;&#22411;&#32463;&#36866;&#24403;&#25351;&#23548;&#21518;&#34920;&#29616;&#20248;&#24322;&#65292;&#26377;&#26102;&#29978;&#33267;&#20248;&#20110;&#20256;&#32479;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;&#23558;ChatGPT&#30340;&#25490;&#21517;&#33021;&#21147;&#25552;&#28860;&#20026;&#19987;&#38376;&#27169;&#22411;&#22312;BEIR&#19978;&#30340;&#25928;&#26524;&#26356;&#20248;&#12290;</title><link>http://arxiv.org/abs/2304.09542</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20449;&#24687;&#26816;&#32034;&#20013;&#30340;&#25490;&#21517;&#33021;&#21147;&#30740;&#31350;&#8212;&#8212;&#20197;ChatGPT&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agent. (arXiv:2304.09542v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09542
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#29983;&#25104;&#24615;LLMs&#65292;&#22914;ChatGPT&#21644;GPT-4&#22312;&#20449;&#24687;&#26816;&#32034;&#20013;&#30340;&#30456;&#20851;&#24615;&#25490;&#21517;&#33021;&#21147;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#27169;&#22411;&#32463;&#36866;&#24403;&#25351;&#23548;&#21518;&#34920;&#29616;&#20248;&#24322;&#65292;&#26377;&#26102;&#29978;&#33267;&#20248;&#20110;&#20256;&#32479;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;&#23558;ChatGPT&#30340;&#25490;&#21517;&#33021;&#21147;&#25552;&#28860;&#20026;&#19987;&#38376;&#27169;&#22411;&#22312;BEIR&#19978;&#30340;&#25928;&#26524;&#26356;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#35777;&#26126;&#20855;&#26377;remarkable&#33021;&#21147;&#65292;&#33021;&#22815;&#23558;&#19968;&#20123;&#38646;&#26679;&#26412;&#35821;&#35328;&#20219;&#21153;&#25512;&#24191;&#33267;&#20854;&#20182;&#39046;&#22495;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;ChatGPT&#21644;GPT-4&#31561;&#29983;&#25104;&#24615;LLMs&#30340;&#30456;&#20851;&#24615;&#25490;&#21517;&#22312;&#20449;&#24687;&#26816;&#32034;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#32463;&#36807;&#36866;&#24403;&#30340;&#25351;&#23548;&#65292;ChatGPT&#21644;GPT-4&#21487;&#20197;&#22312;&#27969;&#34892;&#30340;&#20449;&#24687;&#26816;&#32034;&#22522;&#20934;&#19978;&#21462;&#24471;&#31454;&#20105;&#20248;&#21183;&#65292;&#29978;&#33267;&#26377;&#26102;&#20248;&#20110;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;&#29305;&#21035;&#22320;&#65292;GPT-4&#22312;TREC&#25968;&#25454;&#38598;&#19978;&#30340;&#24179;&#22343;nDCG&#19978;&#34920;&#29616;&#20248;&#20110;&#23436;&#20840;&#24494;&#35843;&#30340;monoT5-3B&#65292;BEIR&#25968;&#25454;&#38598;&#19978;&#30340;&#24179;&#22343;nDCG&#19978;&#20248;&#20110;monoT5-3B 2.3&#20010;&#28857;&#65292;&#20302;&#36164;&#28304;&#35821;&#35328;Mr.TyDi&#19978;&#30340;&#24179;&#22343;nDCG&#19978;&#20248;&#20110;monoT5-3B 2.7&#20010;&#28857;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#23558;ChatGPT&#30340;&#25490;&#21517;&#33021;&#21147;&#25552;&#28860;&#20026;&#19987;&#38376;&#30340;&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#35757;&#32451;&#30340;&#23567;&#22411;&#19987;&#38376;&#27169;&#22411;&#65288;&#35757;&#32451;&#20110;10K&#20010;ChatGPT&#29983;&#25104;&#30340;&#25968;&#25454;&#65289;&#22312;BEIR&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#22312;400K&#20010;MS MARCO&#27880;&#37322;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;monoT5&#12290;&#20195;&#30721;&#21487;&#22312;www.github.com/sunnwe&#19978;&#22797;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated a remarkable ability to generalize zero-shot to various language-related tasks. This paper focuses on the study of exploring generative LLMs such as ChatGPT and GPT-4 for relevance ranking in Information Retrieval (IR). Surprisingly, our experiments reveal that properly instructed ChatGPT and GPT-4 can deliver competitive, even superior results than supervised methods on popular IR benchmarks. Notably, GPT-4 outperforms the fully fine-tuned monoT5-3B on MS MARCO by an average of 2.7 nDCG on TREC datasets, an average of 2.3 nDCG on eight BEIR datasets, and an average of 2.7 nDCG on ten low-resource languages Mr.TyDi. Subsequently, we delve into the potential for distilling the ranking capabilities of ChatGPT into a specialized model. Our small specialized model that trained on 10K ChatGPT generated data outperforms monoT5 trained on 400K annotated MS MARCO data on BEIR. The code to reproduce our results is available at www.github.com/sunnwe
&lt;/p&gt;</description></item></channel></rss>