<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;AMR&#30456;&#20284;&#24230;&#24230;&#37327;rematch&#20197;&#25552;&#39640;&#32467;&#26500;&#21644;&#35821;&#20041;&#30456;&#20284;&#24615;&#65292;&#32467;&#26500;&#30456;&#20284;&#24615;&#25490;&#21517;&#31532;&#20108;&#65292;&#35821;&#20041;&#30456;&#20284;&#24615;&#26368;&#20248;&#65292;&#24555;&#20116;&#20493;&#20110;&#19979;&#19968;&#20010;&#26368;&#39640;&#25928;&#24230;&#37327;&#12290;</title><link>https://arxiv.org/abs/2404.02126</link><description>&lt;p&gt;
&#37325;&#26032;&#27604;&#36187;&#65306;&#25913;&#36827;&#26412;&#22320;&#30693;&#35782;&#22270;&#30340;&#40065;&#26834;&#21644;&#39640;&#25928;&#21305;&#37197;&#20197;&#25552;&#39640;&#32467;&#26500;&#21644;&#35821;&#20041;&#30456;&#20284;&#24615;
&lt;/p&gt;
&lt;p&gt;
Rematch: Robust and Efficient Matching of Local Knowledge Graphs to Improve Structural and Semantic Similarity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02126
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;AMR&#30456;&#20284;&#24230;&#24230;&#37327;rematch&#20197;&#25552;&#39640;&#32467;&#26500;&#21644;&#35821;&#20041;&#30456;&#20284;&#24615;&#65292;&#32467;&#26500;&#30456;&#20284;&#24615;&#25490;&#21517;&#31532;&#20108;&#65292;&#35821;&#20041;&#30456;&#20284;&#24615;&#26368;&#20248;&#65292;&#24555;&#20116;&#20493;&#20110;&#19979;&#19968;&#20010;&#26368;&#39640;&#25928;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#20363;&#22914;&#38382;&#31572;&#21644;&#20107;&#23454;&#26680;&#26597;&#12290;&#25277;&#35937;&#24847;&#20041;&#34920;&#31034;&#65288;AMR&#65289;&#23558;&#25991;&#26412;&#34920;&#31034;&#20026;&#30693;&#35782;&#22270;&#12290;&#35780;&#20272;&#36825;&#20123;&#22270;&#30340;&#36136;&#37327;&#28041;&#21450;&#23558;&#23427;&#20204;&#32467;&#26500;&#21270;&#21305;&#37197;&#21040;&#24444;&#27492;&#21644;&#35821;&#20041;&#21305;&#37197;&#21040;&#28304;&#25991;&#26412;&#12290;&#29616;&#26377;&#30340;AMR&#24230;&#37327;&#25928;&#29575;&#20302;&#65292;&#38590;&#20197;&#25429;&#25417;&#35821;&#20041;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#20063;&#32570;&#20047;&#19968;&#20010;&#31995;&#32479;&#24615;&#30340;&#35780;&#20272;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;AMR&#22270;&#20043;&#38388;&#30340;&#32467;&#26500;&#30456;&#20284;&#24615;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;AMR&#30456;&#20284;&#24230;&#24230;&#37327;rematch&#65292;&#20197;&#21450;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#32467;&#26500;&#30456;&#20284;&#24615;&#30340;&#26032;&#35780;&#20272;&#26041;&#27861;RARE&#12290;&#22312;&#26368;&#20808;&#36827;&#30340;&#24230;&#37327;&#20013;&#65292;rematch&#22312;&#32467;&#26500;&#30456;&#20284;&#24615;&#20013;&#25490;&#21517;&#31532;&#20108;&#65307;&#24182;&#19988;&#22312;STS-B&#21644;SICK-R&#22522;&#20934;&#19978;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#26368;&#39640;&#65292;&#19982;&#19979;&#19968;&#20010;&#26368;&#39640;&#25928;&#24230;&#37327;&#30456;&#27604;&#24555;&#20116;&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02126v1 Announce Type: new  Abstract: Knowledge graphs play a pivotal role in various applications, such as question-answering and fact-checking. Abstract Meaning Representation (AMR) represents text as knowledge graphs. Evaluating the quality of these graphs involves matching them structurally to each other and semantically to the source text. Existing AMR metrics are inefficient and struggle to capture semantic similarity. We also lack a systematic evaluation benchmark for assessing structural similarity between AMR graphs. To overcome these limitations, we introduce a novel AMR similarity metric, rematch, alongside a new evaluation for structural similarity called RARE. Among state-of-the-art metrics, rematch ranks second in structural similarity; and first in semantic similarity by 1--5 percentage points on the STS-B and SICK-R benchmarks. Rematch is also five times faster than the next most efficient metric.
&lt;/p&gt;</description></item><item><title>IISAN&#26159;&#19968;&#31181;&#31616;&#21333;&#30340;&#25554;&#25300;&#26550;&#26500;&#65292;&#37319;&#29992;&#35299;&#32806;PEFT&#32467;&#26500;&#65292;&#24182;&#21033;&#29992;&#20869;&#37096;&#21644;&#36328;&#27169;&#24577;&#36866;&#24212;&#65292;&#19982;&#20840;&#24494;&#35843;&#21644;&#26368;&#20808;&#36827;&#30340;PEFT&#24615;&#33021;&#21305;&#37197;&#65292;&#26174;&#33879;&#20943;&#23569;GPU&#20869;&#23384;&#20351;&#29992;&#37327;&#65292;&#24182;&#21152;&#36895;&#20102;&#35757;&#32451;&#26102;&#38388;&#12290;</title><link>https://arxiv.org/abs/2404.02059</link><description>&lt;p&gt;
IISAN&#65306;&#20351;&#29992;&#35299;&#32806;PEFT&#26377;&#25928;&#22320;&#35843;&#25972;&#22810;&#27169;&#24577;&#34920;&#31034;&#20197;&#39034;&#24207;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
IISAN: Efficiently Adapting Multimodal Representation for Sequential Recommendation with Decoupled PEFT
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02059
&lt;/p&gt;
&lt;p&gt;
IISAN&#26159;&#19968;&#31181;&#31616;&#21333;&#30340;&#25554;&#25300;&#26550;&#26500;&#65292;&#37319;&#29992;&#35299;&#32806;PEFT&#32467;&#26500;&#65292;&#24182;&#21033;&#29992;&#20869;&#37096;&#21644;&#36328;&#27169;&#24577;&#36866;&#24212;&#65292;&#19982;&#20840;&#24494;&#35843;&#21644;&#26368;&#20808;&#36827;&#30340;PEFT&#24615;&#33021;&#21305;&#37197;&#65292;&#26174;&#33879;&#20943;&#23569;GPU&#20869;&#23384;&#20351;&#29992;&#37327;&#65292;&#24182;&#21152;&#36895;&#20102;&#35757;&#32451;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#22312;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#20013;&#20855;&#26377;&#36716;&#21464;&#24615;&#65292;&#21033;&#29992;&#24378;&#22823;&#30340;&#34920;&#31034;&#23398;&#20064;&#33021;&#21147;&#12290;&#34429;&#28982;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#36890;&#24120;&#29992;&#20110;&#35843;&#25972;&#22522;&#30784;&#27169;&#22411;&#20197;&#36827;&#34892;&#25512;&#33616;&#20219;&#21153;&#65292;&#20294;&#22823;&#22810;&#25968;&#30740;&#31350;&#20248;&#20808;&#32771;&#34385;&#21442;&#25968;&#25928;&#29575;&#65292;&#36890;&#24120;&#24573;&#30053;GPU&#20869;&#23384;&#25928;&#29575;&#21644;&#35757;&#32451;&#36895;&#24230;&#31561;&#20851;&#38190;&#22240;&#32032;&#12290;&#38024;&#23545;&#36825;&#19968;&#24046;&#36317;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;IISAN&#65288;&#22810;&#27169;&#24577;&#34920;&#31034;&#30340;&#20869;&#37096;&#21644;&#36328;&#27169;&#24577;&#20391;&#38754;&#36866;&#24212;&#32593;&#32476;&#65289;&#65292;&#19968;&#20010;&#20351;&#29992;&#35299;&#32806;PEFT&#32467;&#26500;&#24182;&#21033;&#29992;&#20869;&#37096;&#21644;&#36328;&#27169;&#24577;&#36866;&#24212;&#30340;&#31616;&#21333;&#21363;&#25554;&#21363;&#29992;&#26550;&#26500;&#12290;IISAN&#19982;&#20840;&#24494;&#35843;&#65288;FFT&#65289;&#21644;&#26368;&#20808;&#36827;&#30340;PEFT&#30340;&#24615;&#33021;&#30456;&#21305;&#37197;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#23427;&#26174;&#33879;&#20943;&#23569;&#20102;GPU&#20869;&#23384;&#20351;&#29992;&#37327; - &#23545;&#20110;&#22810;&#27169;&#24577;&#39034;&#24207;&#25512;&#33616;&#20219;&#21153;&#65292;&#20174;47GB&#38477;&#20302;&#21040;&#20165;3GB&#12290;&#27492;&#22806;&#65292;&#19982;FFT&#30456;&#27604;&#65292;&#23427;&#23558;&#27599;&#20010;&#26102;&#20195;&#30340;&#35757;&#32451;&#26102;&#38388;&#20174;443&#31186;&#21152;&#36895;&#21040;22&#31186;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02059v1 Announce Type: new  Abstract: Multimodal foundation models are transformative in sequential recommender systems, leveraging powerful representation learning capabilities. While Parameter-efficient Fine-tuning (PEFT) is commonly used to adapt foundation models for recommendation tasks, most research prioritizes parameter efficiency, often overlooking critical factors like GPU memory efficiency and training speed. Addressing this gap, our paper introduces IISAN (Intra- and Inter-modal Side Adapted Network for Multimodal Representation), a simple plug-and-play architecture using a Decoupled PEFT structure and exploiting both intra- and inter-modal adaptation.   IISAN matches the performance of full fine-tuning (FFT) and state-of-the-art PEFT. More importantly, it significantly reduces GPU memory usage - from 47GB to just 3GB for multimodal sequential recommendation tasks. Additionally, it accelerates training time per epoch from 443s to 22s compared to FFT. This is also
&lt;/p&gt;</description></item><item><title>&#35774;&#35745;&#20102;&#26032;&#39062;&#30340;&#25552;&#31034;&#31574;&#30053;&#21644;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#20197;&#25506;&#32034;LLMs&#29992;&#20110;&#19979;&#19968;&#20010;POI&#25512;&#33616;&#30340;&#38646;&#26679;&#26412;&#27867;&#21270;&#33021;&#21147;</title><link>https://arxiv.org/abs/2404.01855</link><description>&lt;p&gt;
&#19979;&#19968;&#20010;&#21435;&#21738;&#37324;&#65306;&#22522;&#20110;&#38646;&#26679;&#26412;&#27867;&#21270;&#30340;LLMs&#29992;&#20110;&#19979;&#19968;&#20010;POI&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Where to Move Next: Zero-shot Generalization of LLMs for Next POI Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01855
&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#20102;&#26032;&#39062;&#30340;&#25552;&#31034;&#31574;&#30053;&#21644;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#20197;&#25506;&#32034;LLMs&#29992;&#20110;&#19979;&#19968;&#20010;POI&#25512;&#33616;&#30340;&#38646;&#26679;&#26412;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19979;&#19968;&#20010;&#20852;&#36259;&#28857;&#65288;POI&#65289;&#25512;&#33616;&#20026;&#29992;&#25143;&#25552;&#20379;&#20102;&#25506;&#32034;&#21608;&#36793;&#29615;&#22659;&#30340;&#23453;&#36149;&#24314;&#35758;&#12290;&#29616;&#26377;&#30740;&#31350;&#20381;&#36182;&#20110;&#20174;&#22823;&#35268;&#27169;&#29992;&#25143;&#31614;&#21040;&#25968;&#25454;&#26500;&#24314;&#25512;&#33616;&#27169;&#22411;&#65292;&#36825;&#26159;&#20219;&#21153;&#29305;&#23450;&#30340;&#65292;&#24182;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;&#26368;&#36817;&#65292;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;NLP&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#24182;&#19988;&#24050;&#32463;&#34987;&#30740;&#31350;&#29992;&#20110;&#25512;&#33616;&#22330;&#26223;&#12290;&#28982;&#32780;&#65292;LLMs&#30340;&#27867;&#21270;&#33021;&#21147;&#22312;&#35299;&#20915;&#19979;&#19968;&#20010;POI&#25512;&#33616;&#38382;&#39064;&#26102;&#20173;&#26410;&#34987;&#25506;&#32034;&#65292;&#20854;&#20013;&#24212;&#25552;&#21462;&#29992;&#25143;&#30340;&#22320;&#29702;&#31227;&#21160;&#27169;&#24335;&#12290;&#34429;&#28982;&#26377;&#30740;&#31350;&#21033;&#29992;LLMs&#36827;&#34892;&#19979;&#19968;&#20010;&#39033;&#30446;&#25512;&#33616;&#65292;&#20294;&#23427;&#20204;&#26410;&#33021;&#32771;&#34385;&#22320;&#29702;&#24433;&#21709;&#21644;&#39034;&#24207;&#36716;&#25442;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#26080;&#27861;&#26377;&#25928;&#35299;&#20915;&#19979;&#19968;&#20010;POI&#25512;&#33616;&#20219;&#21153;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#26032;&#39062;&#30340;&#25552;&#31034;&#31574;&#30053;&#65292;&#24182;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#20197;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01855v1 Announce Type: cross  Abstract: Next Point-of-interest (POI) recommendation provides valuable suggestions for users to explore their surrounding environment. Existing studies rely on building recommendation models from large-scale users' check-in data, which is task-specific and needs extensive computational resources. Recently, the pretrained large language models (LLMs) have achieved significant advancements in various NLP tasks and have also been investigated for recommendation scenarios. However, the generalization abilities of LLMs still are unexplored to address the next POI recommendations, where users' geographical movement patterns should be extracted. Although there are studies that leverage LLMs for next-item recommendations, they fail to consider the geographical influence and sequential transitions. Hence, they cannot effectively solve the next POI recommendation task. To this end, we design novel prompting strategies and conduct empirical studies to ass
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CIRP&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20135;&#21697;&#25414;&#32465;&#20013;&#30340;&#39033;&#30446;&#34920;&#31034;&#23398;&#20064;&#65292;&#26088;&#22312;&#35299;&#20915;&#20808;&#21069;&#26041;&#27861;&#20013;&#23545;&#36328;&#39033;&#30446;&#20851;&#31995;&#30340;&#19981;&#36275;&#12290;</title><link>https://arxiv.org/abs/2404.01735</link><description>&lt;p&gt;
CIRP: &#36328;&#39033;&#30446;&#20851;&#31995;&#39044;&#35757;&#32451;&#29992;&#20110;&#22810;&#27169;&#24577;&#20135;&#21697;&#25414;&#32465;
&lt;/p&gt;
&lt;p&gt;
CIRP: Cross-Item Relational Pre-training for Multimodal Product Bundling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01735
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CIRP&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20135;&#21697;&#25414;&#32465;&#20013;&#30340;&#39033;&#30446;&#34920;&#31034;&#23398;&#20064;&#65292;&#26088;&#22312;&#35299;&#20915;&#20808;&#21069;&#26041;&#27861;&#20013;&#23545;&#36328;&#39033;&#30446;&#20851;&#31995;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20135;&#21697;&#25414;&#32465;&#19968;&#30452;&#26159;&#19968;&#31181;&#30427;&#34892;&#30340;&#33829;&#38144;&#31574;&#30053;&#65292;&#23545;&#22312;&#32447;&#36141;&#29289;&#22330;&#26223;&#26377;&#30410;&#12290;&#26377;&#25928;&#30340;&#20135;&#21697;&#25414;&#32465;&#26041;&#27861;&#21462;&#20915;&#20110;&#25429;&#33719;&#20010;&#20307;&#39033;&#30446;&#35821;&#20041;&#21644;&#36328;&#39033;&#30446;&#20851;&#31995;&#30340;&#39640;&#36136;&#37327;&#39033;&#30446;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#39033;&#30446;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#26080;&#35770;&#26159;&#29305;&#24449;&#34701;&#21512;&#36824;&#26159;&#22270;&#24418;&#23398;&#20064;&#65292;&#37117;&#23384;&#22312;&#36328;&#27169;&#24577;&#23545;&#40784;&#19981;&#36275;&#24182;&#19988;&#38590;&#20197;&#25429;&#25417;&#20919;&#21551;&#21160;&#39033;&#30446;&#30340;&#36328;&#39033;&#30446;&#20851;&#31995;&#12290;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#33021;&#26159;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#21508;&#31181;&#22810;&#27169;&#24577;&#19979;&#28216;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#30340;&#36328;&#39033;&#30446;&#20851;&#31995;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#31616;&#21333;&#30340;&#26694;&#26550;Cross-Item Relational Pre-training&#65288;CIRP&#65289;&#65292;&#29992;&#20110;&#20135;&#21697;&#25414;&#32465;&#20013;&#30340;&#39033;&#30446;&#34920;&#31034;&#23398;&#20064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#37319;&#29992;&#22810;&#27169;&#24577;&#32534;&#30721;&#22120;&#29983;&#25104;&#22270;&#20687;&#21644;&#25991;&#23383;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01735v1 Announce Type: new  Abstract: Product bundling has been a prevailing marketing strategy that is beneficial in the online shopping scenario. Effective product bundling methods depend on high-quality item representations, which need to capture both the individual items' semantics and cross-item relations. However, previous item representation learning methods, either feature fusion or graph learning, suffer from inadequate cross-modal alignment and struggle to capture the cross-item relations for cold-start items. Multimodal pre-train models could be the potential solutions given their promising performance on various multimodal downstream tasks. However, the cross-item relations have been under-explored in the current multimodal pre-train models. To bridge this gap, we propose a novel and simple framework Cross-Item Relational Pre-training (CIRP) for item representation learning in product bundling. Specifically, we employ a multimodal encoder to generate image and te
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#34701;&#21512;&#23454;&#20307;&#25551;&#36848;&#36827;&#34892;&#23454;&#20307;&#28040;&#27495;&#30340;&#32534;&#30721;-&#35299;&#30721;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2404.01626</link><description>&lt;p&gt;
&#36890;&#36807;&#34701;&#21512;&#23454;&#20307;&#35299;&#30721;&#36827;&#34892;&#23454;&#20307;&#28040;&#27495;
&lt;/p&gt;
&lt;p&gt;
Entity Disambiguation via Fusion Entity Decoding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01626
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#34701;&#21512;&#23454;&#20307;&#25551;&#36848;&#36827;&#34892;&#23454;&#20307;&#28040;&#27495;&#30340;&#32534;&#30721;-&#35299;&#30721;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#20307;&#28040;&#27495;&#65288;ED&#65289;&#26159;&#23558;&#27169;&#31946;&#23454;&#20307;&#30340;&#25552;&#21450;&#38142;&#25509;&#21040;&#30693;&#35782;&#24211;&#20013;&#30340;&#25351;&#20195;&#23454;&#20307;&#30340;&#36807;&#31243;&#65292;&#22312;&#23454;&#20307;&#38142;&#25509;&#65288;EL&#65289;&#20013;&#36215;&#30528;&#26680;&#24515;&#20316;&#29992;&#12290;&#29616;&#26377;&#30340;&#29983;&#25104;&#24335;&#26041;&#27861;&#22312;&#26631;&#20934;&#21270;&#30340;ZELDA&#22522;&#20934;&#19979;&#23637;&#31034;&#20986;&#27604;&#20998;&#31867;&#26041;&#27861;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#24335;&#26041;&#27861;&#38656;&#35201;&#22823;&#35268;&#27169;&#30340;&#39044;&#35757;&#32451;&#19988;&#29983;&#25104;&#25928;&#29575;&#20302;&#19979;&#12290;&#26368;&#37325;&#35201;&#30340;&#26159;&#65292;&#23454;&#20307;&#25551;&#36848;&#32463;&#24120;&#34987;&#24573;&#35270;&#65292;&#32780;&#36825;&#20123;&#25551;&#36848;&#21487;&#33021;&#21253;&#21547;&#21306;&#20998;&#30456;&#20284;&#23454;&#20307;&#30340;&#20851;&#38190;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32534;&#30721;-&#35299;&#30721;&#27169;&#22411;&#65292;&#20197;&#26356;&#35814;&#32454;&#30340;&#23454;&#20307;&#25551;&#36848;&#26469;&#36827;&#34892;&#23454;&#20307;&#28040;&#27495;&#12290;&#32473;&#23450;&#25991;&#26412;&#21644;&#20505;&#36873;&#23454;&#20307;&#65292;&#32534;&#30721;&#22120;&#23398;&#20064;&#25991;&#26412;&#19982;&#27599;&#20010;&#20505;&#36873;&#23454;&#20307;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#20026;&#27599;&#20010;&#23454;&#20307;&#20505;&#36873;&#20135;&#29983;&#34920;&#31034;&#12290;&#35299;&#30721;&#22120;&#38543;&#21518;&#23558;&#23454;&#20307;&#20505;&#36873;&#30340;&#34920;&#31034;&#34701;&#21512;&#22312;&#19968;&#36215;&#65292;&#24182;&#36873;&#25321;&#27491;&#30830;&#30340;&#23454;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01626v1 Announce Type: new  Abstract: Entity disambiguation (ED), which links the mentions of ambiguous entities to their referent entities in a knowledge base, serves as a core component in entity linking (EL). Existing generative approaches demonstrate improved accuracy compared to classification approaches under the standardized ZELDA benchmark. Nevertheless, generative approaches suffer from the need for large-scale pre-training and inefficient generation. Most importantly, entity descriptions, which could contain crucial information to distinguish similar entities from each other, are often overlooked. We propose an encoder-decoder model to disambiguate entities with more detailed entity descriptions. Given text and candidate entities, the encoder learns interactions between the text and each candidate entity, producing representations for each entity candidate. The decoder then fuses the representations of entity candidates together and selects the correct entity. Our 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20351;&#29992;LLMs&#21021;&#22987;&#21270;&#22810;&#27169;&#24577;DE&#26816;&#32034;&#31995;&#32479;&#65292;&#23454;&#29616;&#22312;102&#31181;&#35821;&#35328;&#20013;&#21305;&#37197;&#35821;&#38899;&#21644;&#25991;&#26412;&#30340;&#33021;&#21147;&#65292;&#26080;&#38656;&#22312;LLM&#39044;&#35757;&#32451;&#26399;&#38388;&#20351;&#29992;&#35821;&#38899;&#25968;&#25454;&#65292;&#19988;&#30456;&#27604;&#20808;&#21069;&#31995;&#32479;&#21462;&#24471;10%&#30340;Recall@1&#32477;&#23545;&#25913;&#36827;</title><link>https://arxiv.org/abs/2404.01616</link><description>&lt;p&gt;
&#23558;LLMs&#36716;&#21270;&#20026;&#36328;&#27169;&#24577;&#21644;&#36328;&#35821;&#35328;&#26816;&#32034;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Transforming LLMs into Cross-modal and Cross-lingual RetrievalSystems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01616
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20351;&#29992;LLMs&#21021;&#22987;&#21270;&#22810;&#27169;&#24577;DE&#26816;&#32034;&#31995;&#32479;&#65292;&#23454;&#29616;&#22312;102&#31181;&#35821;&#35328;&#20013;&#21305;&#37197;&#35821;&#38899;&#21644;&#25991;&#26412;&#30340;&#33021;&#21147;&#65292;&#26080;&#38656;&#22312;LLM&#39044;&#35757;&#32451;&#26399;&#38388;&#20351;&#29992;&#35821;&#38899;&#25968;&#25454;&#65292;&#19988;&#30456;&#27604;&#20808;&#21069;&#31995;&#32479;&#21462;&#24471;10%&#30340;Recall@1&#32477;&#23545;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#22312;&#20165;&#22522;&#20110;&#25991;&#26412;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#36825;&#36229;&#20986;&#20102;&#20855;&#26377;&#37197;&#23545;&#35821;&#38899;&#21644;&#25991;&#26412;&#25968;&#25454;&#30340;&#35821;&#35328;&#33539;&#22260;&#12290;&#21516;&#26102;&#65292;&#22522;&#20110;&#21452;&#32534;&#30721;&#22120;&#65288;DE&#65289;&#30340;&#26816;&#32034;&#31995;&#32479;&#23558;&#26597;&#35810;&#21644;&#25991;&#26723;&#25237;&#24433;&#21040;&#30456;&#21516;&#30340;&#23884;&#20837;&#31354;&#38388;&#20013;&#65292;&#24182;&#22312;&#26816;&#32034;&#21644;&#21452;&#35821;&#25991;&#26412;&#25366;&#25496;&#20013;&#23637;&#31034;&#20102;&#25104;&#21151;&#12290;&#20026;&#20102;&#22312;&#35768;&#22810;&#35821;&#35328;&#20013;&#21305;&#37197;&#35821;&#38899;&#21644;&#25991;&#26412;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;LLMs&#21021;&#22987;&#21270;&#22810;&#27169;&#24577;DE&#26816;&#32034;&#31995;&#32479;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#22312;LLM&#39044;&#35757;&#32451;&#26399;&#38388;&#19981;&#38656;&#35201;&#35821;&#38899;&#25968;&#25454;&#65292;&#24182;&#19988;&#21487;&#20197;&#21033;&#29992;LLM&#30340;&#22810;&#35821;&#35328;&#25991;&#26412;&#29702;&#35299;&#33021;&#21147;&#26469;&#21305;&#37197;&#26816;&#32034;&#35757;&#32451;&#26399;&#38388;&#30475;&#19981;&#35265;&#30340;&#35821;&#35328;&#20013;&#30340;&#35821;&#38899;&#21644;&#25991;&#26412;&#12290;&#25105;&#20204;&#30340;&#22810;&#27169;&#24577;LLM-based&#26816;&#32034;&#31995;&#32479;&#33021;&#22815;&#22312;102&#31181;&#35821;&#35328;&#20013;&#21305;&#37197;&#35821;&#38899;&#21644;&#25991;&#26412;&#65292;&#23613;&#31649;&#21482;&#22312;21&#31181;&#35821;&#35328;&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#20248;&#20110;&#20808;&#21069;&#19987;&#38376;&#22312;&#25152;&#26377;102&#31181;&#35821;&#35328;&#19978;&#35757;&#32451;&#30340;&#31995;&#32479;&#12290;&#22312;&#36825;&#20123;&#35821;&#35328;&#20013;&#65292;&#25105;&#20204;&#22312;Recall@1&#19978;&#23454;&#29616;&#20102;10&#65285;&#30340;&#32477;&#23545;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01616v1 Announce Type: new  Abstract: Large language models (LLMs) are trained on text-only data that go far beyond the languages with paired speech and text data. At the same time, Dual Encoder (DE) based retrieval systems project queries and documents into the same embedding space and have demonstrated their success in retrieval and bi-text mining. To match speech and text in many languages, we propose using LLMs to initialize multi-modal DE retrieval systems. Unlike traditional methods, our system doesn't require speech data during LLM pre-training and can exploit LLM's multilingual text understanding capabilities to match speech and text in languages unseen during retrieval training. Our multi-modal LLM-based retrieval system is capable of matching speech and text in 102 languages despite only training on 21 languages. Our system outperforms previous systems trained explicitly on all 102 languages. We achieve a 10% absolute improvement in Recall@1 averaged across these l
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GPT-3.5&#30340;&#25220;&#34989;&#25991;&#26412;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#21644;&#19968;&#31181;&#22522;&#20110;Faiss&#21644;BERT&#30340;&#39640;&#25928;&#39640;&#20934;&#30830;&#24615;&#30340;&#25220;&#34989;&#35782;&#21035;&#26041;&#27861;&#65292;&#22635;&#34917;&#20102;&#39640;&#27700;&#24179;&#25220;&#34989;&#26816;&#27979;&#30740;&#31350;&#25968;&#25454;&#38598;&#32570;&#22833;&#30340;&#31354;&#30333;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#27169;&#22411;&#22312;&#22810;&#20010;&#25351;&#26631;&#19978;&#34920;&#29616;&#20248;&#24322;</title><link>https://arxiv.org/abs/2404.01582</link><description>&lt;p&gt;
&#22522;&#20110;BERT&#22686;&#24378;&#30340;&#20316;&#19994;&#25220;&#34989;&#26816;&#27979;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
BERT-Enhanced Retrieval Tool for Homework Plagiarism Detection System
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01582
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GPT-3.5&#30340;&#25220;&#34989;&#25991;&#26412;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#21644;&#19968;&#31181;&#22522;&#20110;Faiss&#21644;BERT&#30340;&#39640;&#25928;&#39640;&#20934;&#30830;&#24615;&#30340;&#25220;&#34989;&#35782;&#21035;&#26041;&#27861;&#65292;&#22635;&#34917;&#20102;&#39640;&#27700;&#24179;&#25220;&#34989;&#26816;&#27979;&#30740;&#31350;&#25968;&#25454;&#38598;&#32570;&#22833;&#30340;&#31354;&#30333;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#27169;&#22411;&#22312;&#22810;&#20010;&#25351;&#26631;&#19978;&#34920;&#29616;&#20248;&#24322;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#25220;&#34989;&#26816;&#27979;&#20219;&#21153;&#26159;&#19968;&#39033;&#24120;&#35265;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65292;&#26088;&#22312;&#26816;&#27979;&#32473;&#23450;&#25991;&#26412;&#26159;&#21542;&#21253;&#21547;&#20174;&#20854;&#20182;&#25991;&#26412;&#20013;&#25220;&#34989;&#25110;&#22797;&#21046;&#30340;&#20869;&#23481;&#12290;&#22312;&#29616;&#26377;&#30740;&#31350;&#20013;&#65292;&#30001;&#20110;&#32570;&#20047;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#65292;&#26816;&#27979;&#39640;&#27700;&#24179;&#30340;&#25220;&#34989;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GPT-3.5&#30340;&#25220;&#34989;&#25991;&#26412;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#65292;&#20135;&#29983;&#20102;32,927&#23545;&#25991;&#26412;&#25220;&#34989;&#26816;&#27979;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;&#21508;&#31181;&#25220;&#34989;&#26041;&#27861;&#65292;&#22635;&#34917;&#20102;&#36825;&#19968;&#30740;&#31350;&#39046;&#22495;&#30340;&#31354;&#30333;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Faiss&#21644;BERT&#30340;&#39640;&#25928;&#39640;&#20934;&#30830;&#24615;&#30340;&#25220;&#34989;&#35782;&#21035;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#27169;&#22411;&#22312;&#20934;&#30830;&#29575;&#12289;&#31934;&#30830;&#29575;&#12289;&#21484;&#22238;&#29575;&#21644;F1&#20998;&#25968;&#31561;&#22810;&#20010;&#25351;&#26631;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#65292;&#20998;&#21035;&#36798;&#21040;&#20102;98.86&#65285;&#12289;98.90&#65285;&#12289;98.86&#65285;&#21644;0.9888&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#29992;&#25143;&#21451;&#22909;&#30340;&#28436;&#31034;&#24179;&#21488;&#65292;&#20801;&#35768;&#29992;&#25143;&#19978;&#20256;&#25991;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01582v1 Announce Type: cross  Abstract: Text plagiarism detection task is a common natural language processing task that aims to detect whether a given text contains plagiarism or copying from other texts. In existing research, detection of high level plagiarism is still a challenge due to the lack of high quality datasets. In this paper, we propose a plagiarized text data generation method based on GPT-3.5, which produces 32,927 pairs of text plagiarism detection datasets covering a wide range of plagiarism methods, bridging the gap in this part of research. Meanwhile, we propose a plagiarism identification method based on Faiss with BERT with high efficiency and high accuracy. Our experiments show that the performance of this model outperforms other models in several metrics, including 98.86\%, 98.90%, 98.86%, and 0.9888 for Accuracy, Precision, Recall, and F1 Score, respectively. At the end, we also provide a user-friendly demo platform that allows users to upload a text 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#32858;&#28966;&#20110;&#21033;&#29992;&#22810;&#31890;&#24230;&#25200;&#21160;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#36890;&#36807;&#36716;&#21270;&#20026;&#39034;&#24207;&#20915;&#31574;&#36807;&#31243;&#26469;&#35299;&#20915;&#32452;&#21512;&#29190;&#28856;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2404.01574</link><description>&lt;p&gt;
&#30446;&#26631;&#40657;&#30418;&#31070;&#32463;&#25490;&#24207;&#27169;&#22411;&#30340;&#22810;&#31890;&#24230;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Multi-granular Adversarial Attacks against Black-box Neural Ranking Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01574
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#32858;&#28966;&#20110;&#21033;&#29992;&#22810;&#31890;&#24230;&#25200;&#21160;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#36890;&#36807;&#36716;&#21270;&#20026;&#39034;&#24207;&#20915;&#31574;&#36807;&#31243;&#26469;&#35299;&#20915;&#32452;&#21512;&#29190;&#28856;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#25490;&#24207;&#25915;&#20987;&#30001;&#20110;&#22312;&#21457;&#29616;&#31070;&#32463;&#25490;&#24207;&#27169;&#22411;&#30340;&#33030;&#24369;&#24615;&#24182;&#22686;&#24378;&#20854;&#40065;&#26834;&#24615;&#26041;&#38754;&#21462;&#24471;&#25104;&#21151;&#32780;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#20256;&#32479;&#30340;&#25915;&#20987;&#26041;&#27861;&#20165;&#22312;&#21333;&#19968;&#31890;&#24230;&#19978;&#36827;&#34892;&#25200;&#21160;&#65292;&#20363;&#22914;&#21333;&#35789;&#32423;&#25110;&#21477;&#23376;&#32423;&#65292;&#23545;&#30446;&#26631;&#25991;&#26723;&#36827;&#34892;&#25915;&#20987;&#12290;&#28982;&#32780;&#65292;&#23558;&#25200;&#21160;&#38480;&#21046;&#22312;&#21333;&#19968;&#31890;&#24230;&#19978;&#21487;&#33021;&#20250;&#20943;&#23569;&#21019;&#36896;&#23545;&#25239;&#24615;&#31034;&#20363;&#30340;&#28789;&#27963;&#24615;&#65292;&#20174;&#32780;&#38477;&#20302;&#25915;&#20987;&#30340;&#28508;&#22312;&#23041;&#32961;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#36890;&#36807;&#32467;&#21512;&#19981;&#21516;&#31890;&#24230;&#30340;&#25200;&#21160;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#23545;&#25239;&#24615;&#31034;&#20363;&#12290;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#28041;&#21450;&#35299;&#20915;&#32452;&#21512;&#29190;&#28856;&#38382;&#39064;&#65292;&#38656;&#35201;&#35782;&#21035;&#20986;&#36328;&#25152;&#26377;&#21487;&#33021;&#30340;&#31890;&#24230;&#12289;&#20301;&#32622;&#21644;&#25991;&#26412;&#29255;&#27573;&#30340;&#26368;&#20339;&#32452;&#21512;&#25200;&#21160;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#23558;&#22810;&#31890;&#24230;&#23545;&#25239;&#25915;&#20987;&#36716;&#21270;&#20026;&#19968;&#20010;&#39034;&#24207;&#20915;&#31574;&#36807;&#31243;&#65292;&#20854;&#20013;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01574v1 Announce Type: cross  Abstract: Adversarial ranking attacks have gained increasing attention due to their success in probing vulnerabilities, and, hence, enhancing the robustness, of neural ranking models. Conventional attack methods employ perturbations at a single granularity, e.g., word-level or sentence-level, to a target document. However, limiting perturbations to a single level of granularity may reduce the flexibility of creating adversarial examples, thereby diminishing the potential threat of the attack. Therefore, we focus on generating high-quality adversarial examples by incorporating multi-granular perturbations. Achieving this objective involves tackling a combinatorial explosion problem, which requires identifying an optimal combination of perturbations across all possible levels of granularity, positions, and textual pieces. To address this challenge, we transform the multi-granular adversarial attack into a sequential decision-making process, where 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#38024;&#23545;&#33258;&#21160;&#22238;&#24402;&#20107;&#20214;&#26102;&#38388;&#22270;&#29983;&#25104;&#30340;&#26465;&#20214;&#38598;&#21512;&#29983;&#25104;&#38382;&#39064;&#30340;&#38598;&#21512;&#23545;&#40784;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#32447;&#24615;&#21270;&#22270;&#21644;&#35821;&#35328;&#27169;&#22411;&#22788;&#29702;&#24207;&#21015;&#19981;&#21305;&#37197;&#30340;&#25361;&#25112;</title><link>https://arxiv.org/abs/2404.01532</link><description>&lt;p&gt;
&#33258;&#21160;&#22238;&#24402;&#20107;&#20214;&#26102;&#38388;&#22270;&#29983;&#25104;&#30340;&#38598;&#21512;&#23545;&#40784;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Set-Aligning Framework for Auto-Regressive Event Temporal Graph Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01532
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#38024;&#23545;&#33258;&#21160;&#22238;&#24402;&#20107;&#20214;&#26102;&#38388;&#22270;&#29983;&#25104;&#30340;&#26465;&#20214;&#38598;&#21512;&#29983;&#25104;&#38382;&#39064;&#30340;&#38598;&#21512;&#23545;&#40784;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#32447;&#24615;&#21270;&#22270;&#21644;&#35821;&#35328;&#27169;&#22411;&#22788;&#29702;&#24207;&#21015;&#19981;&#21305;&#37197;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#33258;&#22238;&#24402;&#29983;&#25104;&#32447;&#24615;&#21270;&#22270;&#65292;&#29992;&#20110;&#26500;&#24314;&#20107;&#20214;&#26102;&#38388;&#22270;&#65292;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#23548;&#33268;&#27425;&#20248;&#22270;&#29983;&#25104;&#65292;&#22240;&#20026;&#32447;&#24615;&#21270;&#22270;&#34920;&#29616;&#20986;&#38598;&#21512;&#29305;&#24449;&#65292;&#32780;&#35821;&#35328;&#27169;&#22411;&#21017;&#25353;&#39034;&#24207;&#22788;&#29702;&#36825;&#20123;&#29305;&#24449;&#12290;&#25105;&#20204;&#37325;&#26032;&#26500;&#24605;&#20102;&#20219;&#21153;&#65292;&#23558;&#20854;&#20316;&#20026;&#26465;&#20214;&#38598;&#21512;&#29983;&#25104;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38598;&#21512;&#23545;&#40784;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01532v1 Announce Type: new  Abstract: Event temporal graphs have been shown as convenient and effective representations of complex temporal relations between events in text. Recent studies, which employ pre-trained language models to auto-regressively generate linearised graphs for constructing event temporal graphs, have shown promising results. However, these methods have often led to suboptimal graph generation as the linearised graphs exhibit set characteristics which are instead treated sequentially by language models. This discrepancy stems from the conventional text generation objectives, leading to erroneous penalisation of correct predictions caused by the misalignment of elements in target sequences. To address these challenges, we reframe the task as a conditional set generation problem, proposing a Set-aligning Framework tailored for the effective utilisation of Large Language Models (LLMs). The framework incorporates data augmentations and set-property regularis
&lt;/p&gt;</description></item><item><title>OpenChemIE&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20174;&#21270;&#23398;&#25991;&#29486;&#20013;&#25552;&#21462;&#21453;&#24212;&#25968;&#25454;&#30340;&#24037;&#20855;&#21253;&#65292;&#36890;&#36807;&#25972;&#21512;&#25991;&#26412;&#12289;&#34920;&#26684;&#21644;&#22270;&#20687;&#20449;&#24687;&#20197;&#21450;&#20351;&#29992;&#19987;&#38376;&#31070;&#32463;&#27169;&#22411;&#21644;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#25991;&#26723;&#32423;&#21035;&#30340;&#21453;&#24212;&#25968;&#25454;&#25552;&#21462;&#12290;</title><link>https://arxiv.org/abs/2404.01462</link><description>&lt;p&gt;
OpenChemIE&#65306;&#29992;&#20110;&#21270;&#23398;&#25991;&#29486;&#20449;&#24687;&#25552;&#21462;&#30340;&#24037;&#20855;&#21253;
&lt;/p&gt;
&lt;p&gt;
OpenChemIE: An Information Extraction Toolkit For Chemistry Literature
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01462
&lt;/p&gt;
&lt;p&gt;
OpenChemIE&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20174;&#21270;&#23398;&#25991;&#29486;&#20013;&#25552;&#21462;&#21453;&#24212;&#25968;&#25454;&#30340;&#24037;&#20855;&#21253;&#65292;&#36890;&#36807;&#25972;&#21512;&#25991;&#26412;&#12289;&#34920;&#26684;&#21644;&#22270;&#20687;&#20449;&#24687;&#20197;&#21450;&#20351;&#29992;&#19987;&#38376;&#31070;&#32463;&#27169;&#22411;&#21644;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#25991;&#26723;&#32423;&#21035;&#30340;&#21453;&#24212;&#25968;&#25454;&#25552;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01462v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449;  &#25688;&#35201;&#65306;&#20174;&#21270;&#23398;&#25991;&#29486;&#20013;&#25552;&#21462;&#20449;&#24687;&#23545;&#20110;&#26500;&#24314;&#25968;&#25454;&#39537;&#21160;&#21270;&#23398;&#30340;&#26368;&#26032;&#21453;&#24212;&#25968;&#25454;&#24211;&#33267;&#20851;&#37325;&#35201;&#12290;&#23436;&#25972;&#30340;&#20449;&#24687;&#25552;&#21462;&#38656;&#35201;&#32467;&#21512;&#25991;&#26412;&#12289;&#34920;&#26684;&#21644;&#22270;&#20687;&#20013;&#30340;&#20449;&#24687;&#65292;&#32780;&#20197;&#24448;&#30340;&#24037;&#20316;&#20027;&#35201;&#30740;&#31350;&#20174;&#21333;&#19968;&#26041;&#24335;&#25552;&#21462;&#21453;&#24212;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;OpenChemIE&#26469;&#35299;&#20915;&#36825;&#19968;&#22797;&#26434;&#25361;&#25112;&#65292;&#23454;&#29616;&#22312;&#25991;&#26723;&#32423;&#21035;&#25552;&#21462;&#21453;&#24212;&#25968;&#25454;&#12290;OpenChemIE&#20998;&#20004;&#27493;&#35299;&#20915;&#38382;&#39064;&#65306;&#20174;&#21508;&#20010;&#26041;&#24335;&#20013;&#25552;&#21462;&#30456;&#20851;&#20449;&#24687;&#65292;&#28982;&#21518;&#25972;&#21512;&#32467;&#26524;&#24471;&#21040;&#26368;&#32456;&#30340;&#21453;&#24212;&#21015;&#34920;&#12290;&#23545;&#20110;&#31532;&#19968;&#27493;&#65292;&#25105;&#20204;&#37319;&#29992;&#19987;&#38376;&#30340;&#31070;&#32463;&#27169;&#22411;&#65292;&#27599;&#20010;&#27169;&#22411;&#22788;&#29702;&#21270;&#23398;&#20449;&#24687;&#25552;&#21462;&#30340;&#29305;&#23450;&#20219;&#21153;&#65292;&#27604;&#22914;&#20174;&#25991;&#26412;&#25110;&#22270;&#20687;&#20013;&#35299;&#26512;&#20998;&#23376;&#25110;&#21453;&#24212;&#12290;&#28982;&#21518;&#25105;&#20204;&#20351;&#29992;&#21270;&#23398;&#30456;&#20851;&#30340;&#31639;&#27861;&#25972;&#21512;&#36825;&#20123;&#27169;&#22359;&#30340;&#20449;&#24687;&#65292;&#23454;&#29616;&#31934;&#32454;&#21270;&#21453;&#24212;&#25552;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01462v1 Announce Type: cross  Abstract: Information extraction from chemistry literature is vital for constructing up-to-date reaction databases for data-driven chemistry. Complete extraction requires combining information across text, tables, and figures, whereas prior work has mainly investigated extracting reactions from single modalities. In this paper, we present OpenChemIE to address this complex challenge and enable the extraction of reaction data at the document level. OpenChemIE approaches the problem in two steps: extracting relevant information from individual modalities and then integrating the results to obtain a final list of reactions. For the first step, we employ specialized neural models that each address a specific task for chemistry information extraction, such as parsing molecules or reactions from text or figures. We then integrate the information from these modules using chemistry-informed algorithms, allowing for the extraction of fine-grained reactio
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#30340;&#31038;&#20132;&#23186;&#20307;&#20998;&#26512;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#25968;&#23383;&#20581;&#24247;&#26041;&#27861;&#65292;&#25104;&#21151;&#26816;&#27979;&#20986;&#19982;GLP-1&#21463;&#20307;&#28608;&#21160;&#21058;&#30456;&#20851;&#30340;21&#31181;&#28508;&#22312;&#19981;&#33391;&#21103;&#20316;&#29992;&#65292;&#21253;&#25324;&#26131;&#24594;&#21644;&#40635;&#26408;&#24863;&#65292;&#20174;&#32780;&#38761;&#26032;&#20102;&#23545;&#26032;&#37096;&#32626;&#33647;&#29289;&#26410;&#25253;&#21578;ASEs&#30340;&#26816;&#27979;&#12290;</title><link>https://arxiv.org/abs/2404.01358</link><description>&lt;p&gt;
&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#21644;&#31038;&#20132;&#23186;&#20307;&#20998;&#26512;&#21457;&#29616;GLP-1&#21463;&#20307;&#28608;&#21160;&#21058;&#30340;&#19981;&#33391;&#21103;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Utilizing AI and Social Media Analytics to Discover Adverse Side Effects of GLP-1 Receptor Agonists
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01358
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#30340;&#31038;&#20132;&#23186;&#20307;&#20998;&#26512;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#25968;&#23383;&#20581;&#24247;&#26041;&#27861;&#65292;&#25104;&#21151;&#26816;&#27979;&#20986;&#19982;GLP-1&#21463;&#20307;&#28608;&#21160;&#21058;&#30456;&#20851;&#30340;21&#31181;&#28508;&#22312;&#19981;&#33391;&#21103;&#20316;&#29992;&#65292;&#21253;&#25324;&#26131;&#24594;&#21644;&#40635;&#26408;&#24863;&#65292;&#20174;&#32780;&#38761;&#26032;&#20102;&#23545;&#26032;&#37096;&#32626;&#33647;&#29289;&#26410;&#25253;&#21578;ASEs&#30340;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33647;&#29289;&#30340;&#19981;&#33391;&#21103;&#20316;&#29992;&#65288;ASEs&#65289;&#22312;FDA&#25209;&#20934;&#21518;&#34987;&#21457;&#29616;&#65292;&#23545;&#24739;&#32773;&#23433;&#20840;&#26500;&#25104;&#23041;&#32961;&#12290;&#20026;&#20102;&#21450;&#26102;&#21457;&#29616;&#34987;&#24573;&#35270;&#30340;ASEs&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#25968;&#23383;&#20581;&#24247;&#26041;&#27861;&#65292;&#33021;&#22815;&#20998;&#26512;&#26469;&#33258;&#31038;&#20132;&#23186;&#20307;&#12289;&#24050;&#21457;&#34920;&#30340;&#20020;&#24202;&#30740;&#31350;&#12289;&#21046;&#36896;&#21830;&#25253;&#21578;&#21644;ChatGPT&#31561;&#22823;&#37327;&#20844;&#24320;&#25968;&#25454;&#12290;&#25105;&#20204;&#21457;&#29616;&#20102;&#19982;&#32925;&#32032;&#26679;&#32957;1&#21463;&#20307;&#28608;&#21160;&#21058;&#65288;GLP-1 RA&#65289;&#30456;&#20851;&#30340;ASEs&#65292;&#36825;&#19968;&#24066;&#22330;&#39044;&#35745;&#21040;2030&#24180;&#23558;&#21576;&#25351;&#25968;&#22686;&#38271;&#33267;1335&#20159;&#32654;&#20803;&#12290;&#21033;&#29992;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#27169;&#22411;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25104;&#21151;&#26816;&#27979;&#20986;FDA&#25209;&#20934;&#26102;&#34987;&#24573;&#35270;&#30340;21&#31181;&#28508;&#22312;ASEs&#65292;&#21253;&#25324;&#26131;&#24594;&#21644;&#40635;&#26408;&#24863;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#20998;&#26512;&#26041;&#27861;&#24443;&#24213;&#25913;&#21464;&#20102;&#23545;&#26032;&#37096;&#32626;&#33647;&#29289;&#30456;&#20851;&#26410;&#25253;&#21578;&#30340;ASEs&#30340;&#26816;&#27979;&#65292;&#21033;&#29992;&#21069;&#27839;&#30340;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#31038;&#20132;&#23186;&#20307;&#20998;&#26512;&#12290;&#23427;&#21487;&#20197;&#36890;&#36807;&#37322;&#25918;&#31038;&#20132;&#23186;&#20307;&#30340;&#21147;&#37327;&#26469;&#25903;&#25345;&#30417;&#31649;&#26426;&#26500;&#21644;&#21046;&#36896;&#21830;&#22312;&#24066;&#22330;&#19978;&#22686;&#21152;&#26032;&#33647;&#30340;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01358v1 Announce Type: cross  Abstract: Adverse side effects (ASEs) of drugs, revealed after FDA approval, pose a threat to patient safety. To promptly detect overlooked ASEs, we developed a digital health methodology capable of analyzing massive public data from social media, published clinical research, manufacturers' reports, and ChatGPT. We uncovered ASEs associated with the glucagon-like peptide 1 receptor agonists (GLP-1 RA), a market expected to grow exponentially to $133.5 billion USD by 2030. Using a Named Entity Recognition (NER) model, our method successfully detected 21 potential ASEs overlooked upon FDA approval, including irritability and numbness. Our data-analytic approach revolutionizes the detection of unreported ASEs associated with newly deployed drugs, leveraging cutting-edge AI-driven social media analytics. It can increase the safety of new drugs in the marketplace by unlocking the power of social media to support regulators and manufacturers in the ra
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#65292;&#36890;&#36807;Latent Dirichlet Allocation (LDA)&#36827;&#34892;&#30456;&#20851;&#30340;&#20027;&#39064;&#24314;&#27169;&#65292;&#24110;&#21161;&#25237;&#36164;&#32773;&#20174;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#28304;&#20013;&#26816;&#27979;&#36130;&#32463;&#20107;&#20214;&#20013;&#30340;&#30456;&#20851;&#20449;&#24687;&#12289;&#39044;&#27979;&#21644;&#39044;&#27979;</title><link>https://arxiv.org/abs/2404.01338</link><description>&lt;p&gt;
&#36890;&#36807;Latent Dirichlet Allocation&#20027;&#39064;&#24314;&#27169;&#33258;&#21160;&#26816;&#27979;&#36130;&#32463;&#26032;&#38395;&#20013;&#30340;&#30456;&#20851;&#20449;&#24687;&#12289;&#39044;&#27979;&#21644;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Automatic detection of relevant information, predictions and forecasts in financial news through topic modelling with Latent Dirichlet Allocation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01338
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#65292;&#36890;&#36807;Latent Dirichlet Allocation (LDA)&#36827;&#34892;&#30456;&#20851;&#30340;&#20027;&#39064;&#24314;&#27169;&#65292;&#24110;&#21161;&#25237;&#36164;&#32773;&#20174;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#28304;&#20013;&#26816;&#27979;&#36130;&#32463;&#20107;&#20214;&#20013;&#30340;&#30456;&#20851;&#20449;&#24687;&#12289;&#39044;&#27979;&#21644;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01338v1&#36890;&#21578;&#31867;&#22411;:&#26032;&#25688;&#35201;:&#37329;&#34701;&#26032;&#38395;&#26159;&#19968;&#31181;&#38750;&#32467;&#26500;&#21270;&#30340;&#20449;&#24687;&#28304;&#65292;&#21487;&#20197;&#24320;&#37319;&#20197;&#20174;&#20013;&#25552;&#21462;&#30693;&#35782;&#65292;&#29992;&#20110;&#24066;&#22330;&#31579;&#36873;&#24212;&#29992;&#12290;&#20174;&#25345;&#32493;&#30340;&#37329;&#34701;&#26032;&#38395;&#27969;&#20013;&#25163;&#21160;&#25552;&#21462;&#30456;&#20851;&#20449;&#24687;&#26159;&#32321;&#29712;&#30340;&#65292;&#36229;&#20986;&#20102;&#35768;&#22810;&#25237;&#36164;&#32773;&#30340;&#25216;&#33021;&#33539;&#22260;&#65292;&#20182;&#20204;&#26368;&#22810;&#21482;&#33021;&#20851;&#27880;&#20960;&#20010;&#26469;&#28304;&#21644;&#20316;&#32773;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#23545;&#37329;&#34701;&#26032;&#38395;&#30340;&#20998;&#26512;&#65292;&#20197;&#35782;&#21035;&#30456;&#20851;&#25991;&#26412;&#65292;&#24182;&#22312;&#35813;&#25991;&#26412;&#20013;&#36827;&#34892;&#39044;&#27979;&#21644;&#39044;&#27979;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#31995;&#32479;&#65292;&#24110;&#21161;&#25237;&#36164;&#32773;&#36890;&#36807;&#32771;&#34385;&#35805;&#35821;&#23618;&#38754;&#19978;&#30340;&#30456;&#20851;&#24615;&#21644;&#26102;&#24577;&#24615;&#65292;&#20174;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#28304;&#20013;&#26816;&#27979;&#30456;&#20851;&#30340;&#36130;&#32463;&#20107;&#20214;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;&#25991;&#26412;&#20998;&#21106;&#20197;&#23558;&#30456;&#20851;&#25991;&#26412;&#24402;&#20026;&#19968;&#32452;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24212;&#29992;&#20849;&#25351;&#35299;&#26512;&#26469;&#21457;&#29616;&#27573;&#33853;&#20869;&#37096;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;Latent Dirichlet Allocation (LDA)&#36827;&#34892;&#30456;&#20851;&#20027;&#39064;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01338v1 Announce Type: new  Abstract: Financial news items are unstructured sources of information that can be mined to extract knowledge for market screening applications. Manual extraction of relevant information from the continuous stream of finance-related news is cumbersome and beyond the skills of many investors, who, at most, can follow a few sources and authors. Accordingly, we focus on the analysis of financial news to identify relevant text and, within that text, forecasts and predictions. We propose a novel Natural Language Processing (NLP) system to assist investors in the detection of relevant financial events in unstructured textual sources by considering both relevance and temporality at the discursive level. Firstly, we segment the text to group together closely related text. Secondly, we apply co-reference resolution to discover internal dependencies within segments. Finally, we perform relevant topic modelling with Latent Dirichlet Allocation (LDA) to separ
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32467;&#21512;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31995;&#32479;&#65292;&#26088;&#22312;&#22312;&#37329;&#34701;&#26032;&#38395;&#20013;&#26816;&#27979;&#31687;&#31456;&#32423;&#21035;&#30340;&#20851;&#38190;&#22768;&#26126;&#30340;&#26102;&#38388;&#24615;&#65292;&#20197;&#20998;&#26512;&#21477;&#27861;&#21644;&#35821;&#20041;&#20381;&#36182;&#20851;&#31995;&#65292;&#21306;&#20998;&#19978;&#19979;&#25991;&#20449;&#24687;&#21644;&#26377;&#20215;&#20540;&#30340;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2404.01337</link><description>&lt;p&gt;
&#32467;&#21512;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#22312;&#37329;&#34701;&#26032;&#38395;&#20013;&#26816;&#27979;&#31687;&#31456;&#32423;&#21035;&#30340;&#26102;&#38388;&#24615;
&lt;/p&gt;
&lt;p&gt;
Detection of Temporality at Discourse Level on Financial News by Combining Natural Language Processing and Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01337
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32467;&#21512;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31995;&#32479;&#65292;&#26088;&#22312;&#22312;&#37329;&#34701;&#26032;&#38395;&#20013;&#26816;&#27979;&#31687;&#31456;&#32423;&#21035;&#30340;&#20851;&#38190;&#22768;&#26126;&#30340;&#26102;&#38388;&#24615;&#65292;&#20197;&#20998;&#26512;&#21477;&#27861;&#21644;&#35821;&#20041;&#20381;&#36182;&#20851;&#31995;&#65292;&#21306;&#20998;&#19978;&#19979;&#25991;&#20449;&#24687;&#21644;&#26377;&#20215;&#20540;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Finance-related news, such as Bloomberg News, CNN Business, and Forbes, provide valuable real data for market screening systems. Experts in these news articles not only provide technical analyses but also share opinions considering political, sociological, and cultural factors. We propose a novel system that utilizes Natural Language Processing and Machine Learning techniques to detect the temporality of key statements in finance-related news at the discourse level, aiming to differentiate between context information and valuable predictions by analyzing syntactic and semantic dependencies.
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01337v1 Announce Type: new  Abstract: Finance-related news such as Bloomberg News, CNN Business and Forbes are valuable sources of real data for market screening systems. In news, an expert shares opinions beyond plain technical analyses that include context such as political, sociological and cultural factors. In the same text, the expert often discusses the performance of different assets. Some key statements are mere descriptions of past events while others are predictions. Therefore, understanding the temporality of the key statements in a text is essential to separate context information from valuable predictions. We propose a novel system to detect the temporality of finance-related news at discourse level that combines Natural Language Processing and Machine Learning techniques, and exploits sophisticated features such as syntactic and semantic dependencies. More specifically, we seek to extract the dominant tenses of the main statements, which may be either explicit 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#8220;&#35268;&#21010;&#19982;&#26816;&#32034;&#8221;&#21644;&#8220;&#32534;&#36753;&#19982;&#30830;&#35748;&#8221;&#33539;&#24335;&#65292;&#36890;&#36807;&#31070;&#32463;&#26816;&#32034;&#27169;&#22359;&#21644;LLM-based&#26597;&#35810;&#35268;&#21010;&#22120;&#25552;&#39640;&#20102;&#24037;&#20855;&#21033;&#29992;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2404.00450</link><description>&lt;p&gt;
&#35268;&#21010;&#21644;&#32534;&#36753;&#26816;&#32034;&#20197;&#22686;&#24378;&#24037;&#20855;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Planning and Editing What You Retrieve for Enhanced Tool Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00450
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#8220;&#35268;&#21010;&#19982;&#26816;&#32034;&#8221;&#21644;&#8220;&#32534;&#36753;&#19982;&#30830;&#35748;&#8221;&#33539;&#24335;&#65292;&#36890;&#36807;&#31070;&#32463;&#26816;&#32034;&#27169;&#22359;&#21644;LLM-based&#26597;&#35810;&#35268;&#21010;&#22120;&#25552;&#39640;&#20102;&#24037;&#20855;&#21033;&#29992;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#23558;&#22806;&#37096;&#24037;&#20855;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#38598;&#25104;&#26041;&#38754;&#21462;&#24471;&#30340;&#36827;&#23637;&#25171;&#24320;&#20102;&#26032;&#30340;&#39046;&#22495;&#65292;&#24212;&#29992;&#33539;&#22260;&#28085;&#30422;&#25968;&#23398;&#25512;&#29702;&#12289;&#20195;&#30721;&#29983;&#25104;&#22120;&#21644;&#26234;&#33021;&#21161;&#25163;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#31616;&#21333;&#30340;&#19968;&#27425;&#24615;&#26816;&#32034;&#31574;&#30053;&#65292;&#26080;&#27861;&#26377;&#25928;&#20934;&#30830;&#22320;&#31579;&#36873;&#30456;&#20851;&#24037;&#20855;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#8220;&#35268;&#21010;&#19982;&#26816;&#32034;&#65288;P&amp;R&#65289;&#8221;&#21644;&#8220;&#32534;&#36753;&#19982;&#30830;&#35748;&#65288;E&amp;G&#65289;&#8221;&#33539;&#24335;&#30340;&#27169;&#22411;&#65292;&#21253;&#25324;&#20102;&#31070;&#32463;&#26816;&#32034;&#27169;&#22359;&#21644;&#22522;&#20110;LLM&#30340;&#26597;&#35810;&#35268;&#21010;&#22120;&#65292;&#20197;&#22686;&#24378;&#24037;&#20855;&#21033;&#29992;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00450v1 Announce Type: new  Abstract: Recent advancements in integrating external tools with Large Language Models (LLMs) have opened new frontiers, with applications in mathematical reasoning, code generators, and smart assistants. However, existing methods, relying on simple one-time retrieval strategies, fall short on effectively and accurately shortlisting relevant tools. This paper introduces a novel \modelname (\modelmeaning) approach, encompassing ``Plan-and-Retrieve (P\&amp;R)'' and ``Edit-and-Ground (E\&amp;G)'' paradigms. The P\&amp;R paradigm consists of a neural retrieval module for shortlisting relevant tools and an LLM-based query planner that decomposes complex queries into actionable tasks, enhancing the effectiveness of tool utilization. The E\&amp;G paradigm utilizes LLMs to enrich tool descriptions based on user scenarios, bridging the gap between user queries and tool functionalities. Experiment results demonstrate that these paradigms significantly improve the recall an
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25506;&#35752;&#20102;&#20154;&#20204;&#26085;&#30410;&#20381;&#36182;&#30340;&#22522;&#20110;LLM&#30340;&#20889;&#20316;&#21161;&#25163;&#23545;&#21019;&#36896;&#21147;&#21644;&#20010;&#24615;&#21487;&#33021;&#36896;&#25104;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#26088;&#22312;&#25913;&#36827;&#20154;&#26426;&#20132;&#20114;&#31995;&#32479;&#21644;&#25552;&#21319;&#20889;&#20316;&#21161;&#25163;&#30340;&#20010;&#24615;&#21270;&#21644;&#20010;&#24615;&#21270;&#21151;&#33021;&#12290;</title><link>https://arxiv.org/abs/2404.00026</link><description>&lt;p&gt;
&#22696;&#27700;&#19982;&#20010;&#24615;&#65306;&#22312;LLMs&#26102;&#20195;&#22609;&#36896;&#20010;&#24615;&#21270;&#21465;&#20107;
&lt;/p&gt;
&lt;p&gt;
Ink and Individuality: Crafting a Personalised Narrative in the Age of LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00026
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25506;&#35752;&#20102;&#20154;&#20204;&#26085;&#30410;&#20381;&#36182;&#30340;&#22522;&#20110;LLM&#30340;&#20889;&#20316;&#21161;&#25163;&#23545;&#21019;&#36896;&#21147;&#21644;&#20010;&#24615;&#21487;&#33021;&#36896;&#25104;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#26088;&#22312;&#25913;&#36827;&#20154;&#26426;&#20132;&#20114;&#31995;&#32479;&#21644;&#25552;&#21319;&#20889;&#20316;&#21161;&#25163;&#30340;&#20010;&#24615;&#21270;&#21644;&#20010;&#24615;&#21270;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21644;&#20010;&#24615;&#21270;&#26500;&#25104;&#20102;&#20351;&#27599;&#20010;&#20316;&#23478;&#29420;&#29305;&#24182;&#24433;&#21709;&#20854;&#25991;&#23383;&#20197;&#26377;&#25928;&#21560;&#24341;&#35835;&#32773;&#21516;&#26102;&#20256;&#36798;&#30495;&#23454;&#24615;&#30340;&#29420;&#29305;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#26085;&#30410;&#20381;&#36182;&#22522;&#20110;LLM&#30340;&#20889;&#20316;&#21161;&#25163;&#21487;&#33021;&#20250;&#21361;&#21450;&#25105;&#20204;&#30340;&#21019;&#36896;&#21147;&#21644;&#20010;&#24615;&#12290;&#25105;&#20204;&#32463;&#24120;&#24573;&#35270;&#36825;&#19968;&#36235;&#21183;&#23545;&#25105;&#20204;&#30340;&#21019;&#36896;&#21147;&#21644;&#29420;&#29305;&#24615;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#23613;&#31649;&#21487;&#33021;&#20250;&#36896;&#25104;&#21518;&#26524;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#36827;&#34892;&#31616;&#35201;&#35843;&#26597;&#25506;&#32034;&#19981;&#21516;&#30340;&#35266;&#28857;&#21644;&#27010;&#24565;&#65292;&#20197;&#21450;&#23581;&#35797;&#29702;&#35299;&#20154;&#20204;&#30340;&#35266;&#28857;&#65292;&#32467;&#21512;&#20197;&#24448;&#22312;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#26469;&#30740;&#31350;&#36825;&#20123;&#38382;&#39064;&#12290;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#23545;&#20110;&#25913;&#36827;&#20154;&#26426;&#20132;&#20114;&#31995;&#32479;&#21644;&#22686;&#24378;&#20010;&#24615;&#21270;&#21644;&#20010;&#24615;&#21270;&#20889;&#20316;&#21161;&#25163;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00026v1 Announce Type: cross  Abstract: Individuality and personalization comprise the distinctive characteristics that make each writer unique and influence their words in order to effectively engage readers while conveying authenticity. However, our growing reliance on LLM-based writing assistants risks compromising our creativity and individuality over time. We often overlook the negative impacts of this trend on our creativity and uniqueness, despite the possible consequences. This study investigates these concerns by performing a brief survey to explore different perspectives and concepts, as well as trying to understand people's viewpoints, in conjunction with past studies in the area. Addressing these issues is essential for improving human-computer interaction systems and enhancing writing assistants for personalization and individuality.
&lt;/p&gt;</description></item><item><title>FeatUp&#26159;&#19968;&#20010;&#20219;&#21153;&#21644;&#27169;&#22411;&#26080;&#20851;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#28145;&#24230;&#29305;&#24449;&#20013;&#24674;&#22797;&#20002;&#22833;&#30340;&#31354;&#38388;&#20449;&#24687;&#65292;&#20174;&#32780;&#20351;&#29305;&#24449;&#21487;&#20197;&#20197;&#20219;&#20309;&#20998;&#36776;&#29575;&#37325;&#24314;&#65292;&#22312;&#29616;&#26377;&#24212;&#29992;&#20013;&#21462;&#24471;&#20998;&#36776;&#29575;&#21644;&#24615;&#33021;&#30340;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2403.10516</link><description>&lt;p&gt;
FeatUp: &#19968;&#20010;&#19982;&#27169;&#22411;&#26080;&#20851;&#30340;&#29305;&#24449;&#20219;&#24847;&#20998;&#36776;&#29575;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
FeatUp: A Model-Agnostic Framework for Features at Any Resolution
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10516
&lt;/p&gt;
&lt;p&gt;
FeatUp&#26159;&#19968;&#20010;&#20219;&#21153;&#21644;&#27169;&#22411;&#26080;&#20851;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#28145;&#24230;&#29305;&#24449;&#20013;&#24674;&#22797;&#20002;&#22833;&#30340;&#31354;&#38388;&#20449;&#24687;&#65292;&#20174;&#32780;&#20351;&#29305;&#24449;&#21487;&#20197;&#20197;&#20219;&#20309;&#20998;&#36776;&#29575;&#37325;&#24314;&#65292;&#22312;&#29616;&#26377;&#24212;&#29992;&#20013;&#21462;&#24471;&#20998;&#36776;&#29575;&#21644;&#24615;&#33021;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#29305;&#24449;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#30740;&#31350;&#30340;&#22522;&#30707;&#65292;&#25429;&#25417;&#22270;&#20687;&#35821;&#20041;&#24182;&#20351;&#31038;&#21306;&#33021;&#22815;&#35299;&#20915;&#19979;&#28216;&#20219;&#21153;&#65292;&#21363;&#20351;&#22312;&#38646;&#25110;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#20063;&#33021;&#20570;&#21040;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#29305;&#24449;&#36890;&#24120;&#32570;&#20047;&#31354;&#38388;&#20998;&#36776;&#29575;&#65292;&#26080;&#27861;&#30452;&#25509;&#25191;&#34892;&#20687;&#20998;&#21106;&#21644;&#28145;&#24230;&#39044;&#27979;&#36825;&#26679;&#30340;&#31264;&#23494;&#39044;&#27979;&#20219;&#21153;&#65292;&#22240;&#20026;&#27169;&#22411;&#20250;&#36807;&#20110;&#32858;&#21512;&#22823;&#33539;&#22260;&#30340;&#20449;&#24687;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;FeatUp&#65292;&#19968;&#20010;&#20219;&#21153;&#21644;&#27169;&#22411;&#26080;&#20851;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#24674;&#22797;&#28145;&#24230;&#29305;&#24449;&#20013;&#20002;&#22833;&#30340;&#31354;&#38388;&#20449;&#24687;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;FeatUp&#30340;&#20004;&#20010;&#21464;&#20307;&#65306;&#19968;&#20010;&#22312;&#21333;&#27425;&#21069;&#21521;&#20256;&#36882;&#20013;&#24341;&#23548;&#20855;&#26377;&#39640;&#20998;&#36776;&#29575;&#20449;&#21495;&#30340;&#29305;&#24449;&#65292;&#21478;&#19968;&#20010;&#36866;&#24212;&#21333;&#20010;&#22270;&#20687;&#24182;&#20197;&#20219;&#20309;&#20998;&#36776;&#29575;&#37325;&#26500;&#29305;&#24449;&#30340;&#38544;&#24335;&#27169;&#22411;&#12290;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#20351;&#29992;&#20102;&#19968;&#20010;&#20855;&#26377;&#19982; NeRF &#31867;&#20284;&#30340;&#28145;&#24230;&#31867;&#27604;&#30340;&#22810;&#35270;&#22270;&#19968;&#33268;&#24615;&#25439;&#22833;&#12290;&#25105;&#20204;&#30340;&#29305;&#24449;&#20445;&#30041;&#20854;&#21407;&#22987;&#35821;&#20041;&#65292;&#24182;&#21487;&#20197;&#26367;&#25442;&#29616;&#26377;&#24212;&#29992;&#31243;&#24207;&#65292;&#21363;&#20351;&#19981;&#37325;&#26032;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10516v1 Announce Type: cross  Abstract: Deep features are a cornerstone of computer vision research, capturing image semantics and enabling the community to solve downstream tasks even in the zero- or few-shot regime. However, these features often lack the spatial resolution to directly perform dense prediction tasks like segmentation and depth prediction because models aggressively pool information over large areas. In this work, we introduce FeatUp, a task- and model-agnostic framework to restore lost spatial information in deep features. We introduce two variants of FeatUp: one that guides features with high-resolution signal in a single forward pass, and one that fits an implicit model to a single image to reconstruct features at any resolution. Both approaches use a multi-view consistency loss with deep analogies to NeRFs. Our features retain their original semantics and can be swapped into existing applications to yield resolution and performance gains even without re-
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#29616;&#23454;&#32422;&#26463;&#30340;&#36731;&#37327;&#32423;&#24323;&#26435;&#26426;&#21046;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#20877;&#25490;&#24207;&#38454;&#27573;&#65292;&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#36798;&#21040;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#24320;&#28304;&#20195;&#30721;&#20197;&#20419;&#36827;&#20854;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.12997</link><description>&lt;p&gt;
&#26397;&#30528;&#21487;&#20449;&#30340;&#20877;&#25490;&#24207;&#65306;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#24323;&#26435;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Towards Trustworthy Reranking: A Simple yet Effective Abstention Mechanism
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12997
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#29616;&#23454;&#32422;&#26463;&#30340;&#36731;&#37327;&#32423;&#24323;&#26435;&#26426;&#21046;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#20877;&#25490;&#24207;&#38454;&#27573;&#65292;&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#36798;&#21040;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#24320;&#28304;&#20195;&#30721;&#20197;&#20419;&#36827;&#20854;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#20449;&#24687;&#26816;&#32034;&#65288;NIR&#65289;&#24050;&#32463;&#26174;&#33879;&#25913;&#36827;&#20102;&#22522;&#20110;&#21551;&#21457;&#24335;&#30340;IR&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#22833;&#36133;&#20173;&#28982;&#39057;&#32321;&#21457;&#29983;&#65292;&#36890;&#24120;&#25152;&#20351;&#29992;&#30340;&#27169;&#22411;&#26080;&#27861;&#26816;&#32034;&#19982;&#29992;&#25143;&#26597;&#35810;&#30456;&#20851;&#30340;&#25991;&#26723;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#36866;&#29992;&#20110;&#29616;&#23454;&#32422;&#26463;&#30340;&#36731;&#37327;&#32423;&#24323;&#26435;&#26426;&#21046;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#29305;&#21035;&#24378;&#35843;&#20877;&#25490;&#24207;&#38454;&#27573;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#21327;&#35758;&#65292;&#29992;&#20110;&#22312;&#40657;&#21283;&#23376;&#22330;&#26223;&#20013;&#35780;&#20272;&#24323;&#26435;&#31574;&#30053;&#30340;&#25928;&#26524;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#25968;&#25454;&#39537;&#21160;&#26426;&#21046;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#23454;&#39564;&#22797;&#21046;&#21644;&#24323;&#26435;&#23454;&#26045;&#30340;&#24320;&#28304;&#20195;&#30721;&#65292;&#20419;&#36827;&#20854;&#22312;&#19981;&#21516;&#29615;&#22659;&#20013;&#26356;&#24191;&#27867;&#30340;&#37319;&#29992;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12997v1 Announce Type: cross  Abstract: Neural Information Retrieval (NIR) has significantly improved upon heuristic-based IR systems. Yet, failures remain frequent, the models used often being unable to retrieve documents relevant to the user's query. We address this challenge by proposing a lightweight abstention mechanism tailored for real-world constraints, with particular emphasis placed on the reranking phase. We introduce a protocol for evaluating abstention strategies in a black-box scenario, demonstrating their efficacy, and propose a simple yet effective data-driven mechanism. We provide open-source code for experiment replication and abstention implementation, fostering wider adoption and application in diverse contexts.
&lt;/p&gt;</description></item><item><title>&#25628;&#32034;&#24341;&#25806;&#32467;&#26524;&#39029;&#38754;&#21487;&#20197;&#20316;&#20026;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#20294;&#23384;&#22312;&#23545;&#27969;&#34892;&#24086;&#23376;&#20559;&#35265;&#36739;&#39640;&#12289;&#24773;&#24863;&#26356;&#31215;&#26497;&#20197;&#21450;&#24573;&#35270;&#25919;&#27835;&#12289;&#33394;&#24773;&#21644;&#31895;&#20439;&#24086;&#23376;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2401.15479</link><description>&lt;p&gt;
&#24212;&#23545;&#21518;API&#22256;&#22659;&#65306;&#25628;&#32034;&#24341;&#25806;&#32467;&#26524;&#39029;&#38754;&#21576;&#29616;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#30340;&#20559;&#35265;&#35266;
&lt;/p&gt;
&lt;p&gt;
Navigating the Post-API Dilemma Search Engine Results Pages Present a Biased View of Social Media Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.15479
&lt;/p&gt;
&lt;p&gt;
&#25628;&#32034;&#24341;&#25806;&#32467;&#26524;&#39029;&#38754;&#21487;&#20197;&#20316;&#20026;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#20294;&#23384;&#22312;&#23545;&#27969;&#34892;&#24086;&#23376;&#20559;&#35265;&#36739;&#39640;&#12289;&#24773;&#24863;&#26356;&#31215;&#26497;&#20197;&#21450;&#24573;&#35270;&#25919;&#27835;&#12289;&#33394;&#24773;&#21644;&#31895;&#20439;&#24086;&#23376;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20572;&#27490;&#35775;&#38382;&#31038;&#20132;&#23186;&#20307;API&#30340;&#20915;&#23450;&#23545;&#20114;&#32852;&#32593;&#30740;&#31350;&#21644;&#25972;&#20010;&#35745;&#31639;&#31038;&#20250;&#31185;&#23398;&#39046;&#22495;&#20135;&#29983;&#20102;&#19981;&#21033;&#24433;&#21709;&#12290;&#36825;&#31181;&#23545;&#25968;&#25454;&#30340;&#35775;&#38382;&#32570;&#20047;&#24050;&#34987;&#31216;&#20026;&#20114;&#32852;&#32593;&#30740;&#31350;&#30340;&#21518;API&#26102;&#20195;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;&#27969;&#34892;&#30340;&#25628;&#32034;&#24341;&#25806;&#26377;&#33021;&#21147;&#29228;&#21462;&#12289;&#25429;&#33719;&#21644;&#23637;&#31034;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#22312;&#20854;&#25628;&#32034;&#24341;&#25806;&#32467;&#26524;&#39029;&#38754;(SERP)&#19978;&#65292;&#22914;&#26524;&#25552;&#20379;&#36866;&#24403;&#30340;&#25628;&#32034;&#26597;&#35810;&#65292;&#21487;&#33021;&#20250;&#20026;&#36825;&#19968;&#22256;&#22659;&#25552;&#20379;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#24403;&#21069;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#38382;&#65306;SERP&#26159;&#21542;&#25552;&#20379;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#30340;&#23436;&#25972;&#21644;&#26080;&#20559;&#35265;&#26679;&#26412;&#65311; SERP&#26159;&#21542;&#26159;&#30452;&#25509;API&#35775;&#38382;&#30340;&#21487;&#34892;&#26367;&#20195;&#26041;&#26696;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#23545;&#65288;Google&#65289;SERP&#32467;&#26524;&#21644;&#26469;&#33258;Reddit&#21644;Twitter/X&#30340;&#38750;&#21462;&#26679;&#25968;&#25454;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;SERP&#32467;&#26524;&#22312;&#25903;&#25345;&#27969;&#34892;&#24086;&#23376;&#26041;&#38754;&#23384;&#22312;&#39640;&#24230;&#20559;&#35265;&#65307;&#21453;&#23545;&#25919;&#27835;&#12289;&#33394;&#24773;&#21644;&#31895;&#20439;&#24086;&#23376;&#65307;&#22312;&#24773;&#24863;&#19978;&#26356;&#20026;&#31215;&#26497;&#65307;&#24182;&#26377;&#22823;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.15479v2 Announce Type: replace-cross  Abstract: Recent decisions to discontinue access to social media APIs are having detrimental effects on Internet research and the field of computational social science as a whole. This lack of access to data has been dubbed the Post-API era of Internet research. Fortunately, popular search engines have the means to crawl, capture, and surface social media data on their Search Engine Results Pages (SERP) if provided the proper search query, and may provide a solution to this dilemma. In the present work we ask: does SERP provide a complete and unbiased sample of social media data? Is SERP a viable alternative to direct API-access? To answer these questions, we perform a comparative analysis between (Google) SERP results and nonsampled data from Reddit and Twitter/X. We find that SERP results are highly biased in favor of popular posts; against political, pornographic, and vulgar posts; are more positive in their sentiment; and have large 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#31163;&#21644;&#23398;&#20064;&#28508;&#22312;&#28151;&#28102;&#22240;&#32032;&#65292;&#25552;&#39640;&#20102;&#29992;&#25143;&#20559;&#22909;&#24314;&#27169;&#30340;&#20934;&#30830;&#24615;</title><link>https://arxiv.org/abs/2311.03381</link><description>&lt;p&gt;
&#20998;&#31163;&#21644;&#23398;&#20064;&#28508;&#22312;&#28151;&#28102;&#22240;&#32032;&#20197;&#22686;&#24378;&#29992;&#25143;&#20559;&#22909;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Separating and Learning Latent Confounders to Enhancing User Preferences Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.03381
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#31163;&#21644;&#23398;&#20064;&#28508;&#22312;&#28151;&#28102;&#22240;&#32032;&#65292;&#25552;&#39640;&#20102;&#29992;&#25143;&#20559;&#22909;&#24314;&#27169;&#30340;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#27169;&#22411;&#26088;&#22312;&#20174;&#21382;&#21490;&#21453;&#39304;&#20013;&#25429;&#33719;&#29992;&#25143;&#20559;&#22909;&#65292;&#28982;&#21518;&#39044;&#27979;&#29992;&#25143;&#23545;&#20505;&#36873;&#39033;&#30446;&#30340;&#29305;&#23450;&#21453;&#39304;&#12290;&#28982;&#32780;&#65292;&#21508;&#31181;&#26410;&#27979;&#37327;&#30340;&#28151;&#28102;&#22240;&#32032;&#23548;&#33268;&#21382;&#21490;&#21453;&#39304;&#20013;&#30340;&#29992;&#25143;&#20559;&#22909;&#19982;&#30495;&#23454;&#20559;&#22909;&#20043;&#38388;&#23384;&#22312;&#20559;&#24046;&#65292;&#36827;&#32780;&#23548;&#33268;&#27169;&#22411;&#26410;&#36798;&#21040;&#39044;&#26399;&#30340;&#24615;&#33021;&#12290;&#29616;&#26377;&#30340;&#21435;&#20559;&#27169;&#22411;&#35201;&#20040;&#29305;&#23450;&#20110;&#35299;&#20915;&#29305;&#23450;&#20559;&#24046;&#65292;&#35201;&#20040;&#30452;&#25509;&#20174;&#29992;&#25143;&#21382;&#21490;&#21453;&#39304;&#20013;&#33719;&#21462;&#36741;&#21161;&#20449;&#24687;&#65292;&#36825;&#26080;&#27861;&#30830;&#23450;&#25152;&#23398;&#20559;&#22909;&#26159;&#30495;&#23454;&#29992;&#25143;&#20559;&#22909;&#36824;&#26159;&#28151;&#20837;&#26410;&#27979;&#37327;&#30340;&#28151;&#28102;&#22240;&#32032;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#20197;&#21069;&#30340;&#25512;&#33616;&#31995;&#32479;&#19981;&#20165;&#26159;&#26410;&#27979;&#37327;&#30340;&#28151;&#28102;&#22240;&#32032;&#30340;&#21518;&#32487;&#32773;&#65292;&#36824;&#20250;&#20316;&#20026;&#24433;&#21709;&#29992;&#25143;&#20559;&#22909;&#24314;&#27169;&#30340;&#26410;&#27979;&#37327;&#28151;&#28102;&#22240;&#32032;&#65292;&#36825;&#22312;&#20808;&#21069;&#30340;&#30740;&#31350;&#20013;&#19968;&#30452;&#34987;&#24573;&#35270;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23558;&#21069;&#36848;&#25512;&#33616;&#31995;&#32479;&#30340;&#24433;&#21709;&#32435;&#20837;&#32771;&#34385;&#65292;&#24182;&#23558;&#20854;&#35270;&#20026;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.03381v2 Announce Type: replace-cross  Abstract: Recommender models aim to capture user preferences from historical feedback and then predict user-specific feedback on candidate items. However, the presence of various unmeasured confounders causes deviations between the user preferences in the historical feedback and the true preferences, resulting in models not meeting their expected performance. Existing debias models either (1) specific to solving one particular bias or (2) directly obtain auxiliary information from user historical feedback, which cannot identify whether the learned preferences are true user preferences or mixed with unmeasured confounders. Moreover, we find that the former recommender system is not only a successor to unmeasured confounders but also acts as an unmeasured confounder affecting user preference modeling, which has always been neglected in previous studies. To this end, we incorporate the effect of the former recommender system and treat it as
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#32454;&#31890;&#24230;&#30456;&#20851;&#24615;&#26631;&#31614;&#32435;&#20837;LLM&#25490;&#24207;&#22120;&#25552;&#31034;&#20013;&#65292;&#21487;&#20197;&#26174;&#33879;&#25913;&#36827;&#38646;&#26679;&#26412;LLM&#25490;&#24207;&#22120;&#30340;&#24615;&#33021;</title><link>https://arxiv.org/abs/2310.14122</link><description>&lt;p&gt;
&#36229;&#36234;&#26159;&#19982;&#21542;&#65306;&#36890;&#36807;&#24471;&#20998;&#32454;&#31890;&#24230;&#30456;&#20851;&#24615;&#26631;&#31614;&#26469;&#25913;&#36827;&#38646;&#26679;&#26412;LLM&#25490;&#24207;&#22120;
&lt;/p&gt;
&lt;p&gt;
Beyond Yes and No: Improving Zero-Shot LLM Rankers via Scoring Fine-Grained Relevance Labels
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.14122
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#32454;&#31890;&#24230;&#30456;&#20851;&#24615;&#26631;&#31614;&#32435;&#20837;LLM&#25490;&#24207;&#22120;&#25552;&#31034;&#20013;&#65292;&#21487;&#20197;&#26174;&#33879;&#25913;&#36827;&#38646;&#26679;&#26412;LLM&#25490;&#24207;&#22120;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;LLM&#25216;&#26415;&#39537;&#21160;&#30340;&#38646;&#26679;&#26412;&#25991;&#26412;&#25490;&#24207;&#22120;&#36890;&#36807;&#31616;&#21333;&#30340;&#25552;&#31034;&#33719;&#24471;&#20102;&#26174;&#33879;&#30340;&#25490;&#24207;&#24615;&#33021;&#12290; &#29616;&#26377;&#30340;&#36880;&#28857;LLM&#25490;&#24207;&#22120;&#25552;&#31034;&#22823;&#22810;&#35201;&#27714;&#27169;&#22411;&#20174;&#8220;&#26159;&#8221;&#21644;&#8220;&#21542;&#8221;&#31561;&#20108;&#20803;&#30456;&#20851;&#24615;&#26631;&#31614;&#20013;&#36827;&#34892;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;&#32570;&#23569;&#20013;&#38388;&#30456;&#20851;&#24615;&#26631;&#31614;&#36873;&#39033;&#21487;&#33021;&#20250;&#23548;&#33268;LLM&#22312;&#37096;&#20998;&#30456;&#20851;&#20110;&#26597;&#35810;&#30340;&#25991;&#26723;&#19978;&#25552;&#20379;&#22024;&#26434;&#25110;&#26377;&#20559;&#35265;&#30340;&#31572;&#26696;&#12290;&#25105;&#20204;&#25552;&#20986;&#23558;&#32454;&#31890;&#24230;&#30456;&#20851;&#24615;&#26631;&#31614;&#32435;&#20837;LLM&#25490;&#24207;&#22120;&#30340;&#25552;&#31034;&#20013;&#65292;&#20351;&#20854;&#33021;&#22815;&#26356;&#22909;&#22320;&#21306;&#20998;&#20855;&#26377;&#19981;&#21516;&#30456;&#20851;&#24615;&#27700;&#24179;&#30340;&#25991;&#26723;&#65292;&#20174;&#32780;&#24471;&#20986;&#26356;&#20934;&#30830;&#30340;&#25490;&#24207;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#31181;&#21464;&#20307;&#30340;&#25552;&#31034;&#27169;&#26495;&#65292;&#37197;&#21512;&#19981;&#21516;&#25968;&#37327;&#30340;&#30456;&#20851;&#24615;&#32423;&#21035;&#12290;&#25105;&#20204;&#22312;8&#20010;BEIR&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#28155;&#21152;&#32454;&#31890;&#24230;&#30456;&#20851;&#24615;&#26631;&#31614;&#26174;&#33879;&#25552;&#39640;&#20102;LLM&#25490;&#24207;&#22120;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.14122v3 Announce Type: replace  Abstract: Zero-shot text rankers powered by recent LLMs achieve remarkable ranking performance by simply prompting. Existing prompts for pointwise LLM rankers mostly ask the model to choose from binary relevance labels like "Yes" and "No". However, the lack of intermediate relevance label options may cause the LLM to provide noisy or biased answers for documents that are partially relevant to the query. We propose to incorporate fine-grained relevance labels into the prompt for LLM rankers, enabling them to better differentiate among documents with different levels of relevance to the query and thus derive a more accurate ranking. We study two variants of the prompt template, coupled with different numbers of relevance levels. Our experiments on 8 BEIR data sets show that adding fine-grained relevance labels significantly improves the performance of LLM rankers.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#26469;&#33258;&#21487;&#27604;&#29992;&#25143;&#29983;&#25104;&#20869;&#23481;&#30340;&#35789;&#23884;&#20837;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#36328;&#35821;&#35328;&#33258;&#21160;&#26415;&#35821;&#35782;&#21035;&#26694;&#26550;&#65292;&#23558;&#33521;&#35821;&#28040;&#36153;&#32773;&#20581;&#24247;&#35789;&#27719;&#25193;&#23637;&#20026;&#36328;&#35821;&#35328;&#35789;&#27719;&#12290;</title><link>https://arxiv.org/abs/2206.11612</link><description>&lt;p&gt;
&#20351;&#29992;&#26469;&#33258;&#21487;&#27604;&#29992;&#25143;&#29983;&#25104;&#20869;&#23481;&#30340;&#35789;&#23884;&#20837;&#26500;&#24314;&#36328;&#35821;&#35328;&#28040;&#36153;&#32773;&#20581;&#24247;&#35789;&#27719;
&lt;/p&gt;
&lt;p&gt;
Constructing Cross-lingual Consumer Health Vocabulary with Word-Embedding from Comparable User Generated Content
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2206.11612
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#26469;&#33258;&#21487;&#27604;&#29992;&#25143;&#29983;&#25104;&#20869;&#23481;&#30340;&#35789;&#23884;&#20837;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#36328;&#35821;&#35328;&#33258;&#21160;&#26415;&#35821;&#35782;&#21035;&#26694;&#26550;&#65292;&#23558;&#33521;&#35821;&#28040;&#36153;&#32773;&#20581;&#24247;&#35789;&#27719;&#25193;&#23637;&#20026;&#36328;&#35821;&#35328;&#35789;&#27719;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#20581;&#24247;&#31038;&#21306;&#65288;OHC&#65289;&#26159;&#26222;&#36890;&#20154;&#20998;&#20139;&#20581;&#24247;&#20449;&#24687;&#30340;&#20027;&#35201;&#28192;&#36947;&#12290;&#20026;&#20102;&#20998;&#26512;OHC&#20013;&#28040;&#36153;&#32773;&#29983;&#25104;&#20869;&#23481;&#65288;HCGC&#65289;&#65292;&#35782;&#21035;&#26222;&#36890;&#20154;&#20351;&#29992;&#30340;&#21475;&#22836;&#21307;&#23398;&#34920;&#36798;&#26159;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#24320;&#25918;&#33719;&#21462;&#21644;&#21327;&#20316;&#30340;&#28040;&#36153;&#32773;&#20581;&#24247;&#35789;&#27719;&#65288;OAC CHV&#65289;&#26159;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#30340;&#21463;&#25511;&#35789;&#27719;&#12290;&#28982;&#32780;&#65292;OAC CHV&#20165;&#22312;&#33521;&#35821;&#20013;&#21487;&#29992;&#65292;&#38480;&#21046;&#20102;&#20854;&#22312;&#20854;&#20182;&#35821;&#35328;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#36328;&#35821;&#35328;&#33258;&#21160;&#26415;&#35821;&#35782;&#21035;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#33521;&#35821;CHV&#25193;&#23637;&#20026;&#36328;&#35821;&#35328;CHV&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#38656;&#35201;&#19968;&#20010;&#33521;&#35821;HCGC&#35821;&#26009;&#24211;&#21644;&#19968;&#20010;&#38750;&#33521;&#35821;&#65288;&#26412;&#30740;&#31350;&#20013;&#20026;&#20013;&#25991;&#65289;HCGC&#35821;&#26009;&#24211;&#20316;&#20026;&#36755;&#20837;&#12290;&#20351;&#29992;skip-gram&#31639;&#27861;&#30830;&#23450;&#20102;&#20004;&#20010;&#21333;&#35821;&#35789;&#21521;&#37327;&#31354;&#38388;&#65292;&#20351;&#24471;&#27599;&#20010;&#31354;&#38388;&#32534;&#30721;&#20102;&#35821;&#35328;&#20869;&#26222;&#36890;&#20154;&#20043;&#38388;&#30340;&#24120;&#35265;&#35789;&#20851;&#32852;&#12290;&#26681;&#25454;&#31561;&#36317;&#20551;&#35774;&#65292;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#20004;&#31181;&#35821;&#35328;&#20043;&#38388;&#26144;&#23556;&#20851;&#31995;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2206.11612v2 Announce Type: replace  Abstract: The online health community (OHC) is the primary channel for laypeople to share health information. To analyze the health consumer-generated content (HCGC) from the OHCs, identifying the colloquial medical expressions used by laypeople is a critical challenge. The open-access and collaborative consumer health vocabulary (OAC CHV) is the controlled vocabulary for addressing such a challenge. Nevertheless, OAC CHV is only available in English, limiting its applicability to other languages. This research proposes a cross-lingual automatic term recognition framework for extending the English CHV into a cross-lingual one. Our framework requires an English HCGC corpus and a non-English (i.e., Chinese in this study) HCGC corpus as inputs. Two monolingual word vector spaces are determined using the skip-gram algorithm so that each space encodes common word associations from laypeople within a language. Based on the isometry assumption, the f
&lt;/p&gt;</description></item><item><title>&#36328;&#39046;&#22495;&#24207;&#21015;&#25512;&#33616;&#36890;&#36807;&#38598;&#25104;&#21644;&#23398;&#20064;&#22810;&#20010;&#39046;&#22495;&#30340;&#20132;&#20114;&#20449;&#24687;&#65292;&#23558;&#29992;&#25143;&#20559;&#22909;&#24314;&#27169;&#20174;&#24179;&#38754;&#36716;&#21521;&#31435;&#20307;&#12290;&#25991;&#31456;&#23545;CDSR&#38382;&#39064;&#36827;&#34892;&#20102;&#23450;&#20041;&#21644;&#20998;&#26512;&#65292;&#25552;&#20379;&#20102;&#20174;&#23439;&#35266;&#21644;&#24494;&#35266;&#20004;&#20010;&#35270;&#35282;&#30340;&#31995;&#32479;&#27010;&#36848;&#12290;&#23545;&#20110;&#19981;&#21516;&#39046;&#22495;&#38388;&#30340;&#27169;&#22411;&#65292;&#24635;&#32467;&#20102;&#22810;&#23618;&#34701;&#21512;&#32467;&#26500;&#21644;&#34701;&#21512;&#26725;&#26753;&#12290;&#23545;&#20110;&#29616;&#26377;&#27169;&#22411;&#65292;&#35752;&#35770;&#20102;&#22522;&#30784;&#25216;&#26415;&#21644;&#36741;&#21161;&#23398;&#20064;&#25216;&#26415;&#12290;&#23637;&#31034;&#20102;&#20844;&#24320;&#25968;&#25454;&#38598;&#21644;&#23454;&#39564;&#32467;&#26524;&#65292;&#24182;&#32473;&#20986;&#20102;&#26410;&#26469;&#21457;&#23637;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2401.04971</link><description>&lt;p&gt;
&#36328;&#39046;&#22495;&#24207;&#21015;&#25512;&#33616;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Cross-Domain Sequential Recommendation. (arXiv:2401.04971v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04971
&lt;/p&gt;
&lt;p&gt;
&#36328;&#39046;&#22495;&#24207;&#21015;&#25512;&#33616;&#36890;&#36807;&#38598;&#25104;&#21644;&#23398;&#20064;&#22810;&#20010;&#39046;&#22495;&#30340;&#20132;&#20114;&#20449;&#24687;&#65292;&#23558;&#29992;&#25143;&#20559;&#22909;&#24314;&#27169;&#20174;&#24179;&#38754;&#36716;&#21521;&#31435;&#20307;&#12290;&#25991;&#31456;&#23545;CDSR&#38382;&#39064;&#36827;&#34892;&#20102;&#23450;&#20041;&#21644;&#20998;&#26512;&#65292;&#25552;&#20379;&#20102;&#20174;&#23439;&#35266;&#21644;&#24494;&#35266;&#20004;&#20010;&#35270;&#35282;&#30340;&#31995;&#32479;&#27010;&#36848;&#12290;&#23545;&#20110;&#19981;&#21516;&#39046;&#22495;&#38388;&#30340;&#27169;&#22411;&#65292;&#24635;&#32467;&#20102;&#22810;&#23618;&#34701;&#21512;&#32467;&#26500;&#21644;&#34701;&#21512;&#26725;&#26753;&#12290;&#23545;&#20110;&#29616;&#26377;&#27169;&#22411;&#65292;&#35752;&#35770;&#20102;&#22522;&#30784;&#25216;&#26415;&#21644;&#36741;&#21161;&#23398;&#20064;&#25216;&#26415;&#12290;&#23637;&#31034;&#20102;&#20844;&#24320;&#25968;&#25454;&#38598;&#21644;&#23454;&#39564;&#32467;&#26524;&#65292;&#24182;&#32473;&#20986;&#20102;&#26410;&#26469;&#21457;&#23637;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#39046;&#22495;&#24207;&#21015;&#25512;&#33616;&#65288;CDSR&#65289;&#36890;&#36807;&#22312;&#19981;&#21516;&#31890;&#24230;&#65288;&#20174;&#24207;&#21015;&#38388;&#21040;&#24207;&#21015;&#20869;&#65292;&#20174;&#21333;&#39046;&#22495;&#21040;&#36328;&#39046;&#22495;&#65289;&#19978;&#38598;&#25104;&#21644;&#23398;&#20064;&#26469;&#33258;&#22810;&#20010;&#39046;&#22495;&#30340;&#20132;&#20114;&#20449;&#24687;&#65292;&#23558;&#29992;&#25143;&#20559;&#22909;&#24314;&#27169;&#20174;&#24179;&#38754;&#36716;&#21521;&#20102;&#31435;&#20307;&#12290;&#26412;&#32508;&#36848;&#25991;&#31456;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#22235;&#32500;&#24352;&#37327;&#23450;&#20041;&#20102;CDSR&#38382;&#39064;&#65292;&#24182;&#20998;&#26512;&#20102;&#20854;&#22312;&#22810;&#32500;&#24230;&#38477;&#32500;&#19979;&#30340;&#22810;&#31867;&#22411;&#36755;&#20837;&#34920;&#31034;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#20174;&#25972;&#20307;&#21644;&#32454;&#33410;&#20004;&#20010;&#35270;&#35282;&#25552;&#20379;&#20102;&#31995;&#32479;&#30340;&#27010;&#36848;&#12290;&#20174;&#25972;&#20307;&#35270;&#35282;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#21508;&#20010;&#27169;&#22411;&#22312;&#19981;&#21516;&#39046;&#22495;&#38388;&#30340;&#22810;&#23618;&#34701;&#21512;&#32467;&#26500;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#20204;&#30340;&#34701;&#21512;&#26725;&#26753;&#12290;&#20174;&#32454;&#33410;&#35270;&#35282;&#65292;&#25105;&#20204;&#30528;&#37325;&#35752;&#35770;&#20102;&#29616;&#26377;&#27169;&#22411;&#30340;&#22522;&#30784;&#25216;&#26415;&#65292;&#24182;&#35299;&#37322;&#20102;&#36741;&#21161;&#23398;&#20064;&#25216;&#26415;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21487;&#29992;&#30340;&#20844;&#24320;&#25968;&#25454;&#38598;&#21644;&#20195;&#34920;&#24615;&#30340;&#23454;&#39564;&#32467;&#26524;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#26410;&#26469;&#21457;&#23637;&#30340;&#19968;&#20123;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-domain sequential recommendation (CDSR) shifts the modeling of user preferences from flat to stereoscopic by integrating and learning interaction information from multiple domains at different granularities (ranging from inter-sequence to intra-sequence and from single-domain to cross-domain).In this survey, we initially define the CDSR problem using a four-dimensional tensor and then analyze its multi-type input representations under multidirectional dimensionality reductions. Following that, we provide a systematic overview from both macro and micro views. From a macro view, we abstract the multi-level fusion structures of various models across domains and discuss their bridges for fusion. From a micro view, focusing on the existing models, we specifically discuss the basic technologies and then explain the auxiliary learning technologies. Finally, we exhibit the available public datasets and the representative experimental results as well as provide some insights into future d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20010;&#24615;&#21270;&#25512;&#33616;&#30340;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#22235;&#31181;&#19981;&#21516;&#30340;&#24341;&#23548;&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#20123;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#12290;&#36825;&#19968;&#21457;&#29616;&#24378;&#35843;&#20102;&#22312;&#20010;&#24615;&#21270;&#20869;&#23481;&#25512;&#33616;&#20013;&#65292;&#37319;&#29992;&#22810;&#26679;&#30340;&#24341;&#23548;&#21644;&#36755;&#20837;&#22686;&#24378;&#25216;&#26415;&#21487;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#33616;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.15780</link><description>&lt;p&gt;
LLM-Rec: &#36890;&#36807;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20010;&#24615;&#21270;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
LLM-Rec: Personalized Recommendation via Prompting Large Language Models. (arXiv:2307.15780v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15780
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20010;&#24615;&#21270;&#25512;&#33616;&#30340;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#22235;&#31181;&#19981;&#21516;&#30340;&#24341;&#23548;&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#20123;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#12290;&#36825;&#19968;&#21457;&#29616;&#24378;&#35843;&#20102;&#22312;&#20010;&#24615;&#21270;&#20869;&#23481;&#25512;&#33616;&#20013;&#65292;&#37319;&#29992;&#22810;&#26679;&#30340;&#24341;&#23548;&#21644;&#36755;&#20837;&#22686;&#24378;&#25216;&#26415;&#21487;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#33616;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#36755;&#20837;&#22686;&#24378;&#25216;&#26415;&#65292;&#30740;&#31350;&#20102;&#22810;&#31181;&#19981;&#21516;&#30340;&#24341;&#23548;&#31574;&#30053;&#65292;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#20010;&#24615;&#21270;&#20869;&#23481;&#25512;&#33616;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21517;&#20026;LLM-Rec&#65292;&#21253;&#25324;&#22235;&#31181;&#19981;&#21516;&#30340;&#24341;&#23548;&#31574;&#30053;&#65306;&#65288;1&#65289;&#22522;&#30784;&#24341;&#23548;&#65292;&#65288;2&#65289;&#25512;&#33616;&#39537;&#21160;&#24341;&#23548;&#65292;&#65288;3&#65289;&#21442;&#19982;&#24341;&#23548;&#24341;&#23548;&#65292;&#21644;&#65288;4&#65289;&#25512;&#33616;&#39537;&#21160;+&#21442;&#19982;&#24341;&#23548;&#24341;&#23548;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#23558;&#21407;&#22987;&#20869;&#23481;&#25551;&#36848;&#19982;LLM&#29983;&#25104;&#30340;&#22686;&#24378;&#36755;&#20837;&#25991;&#26412;&#32467;&#21512;&#36215;&#26469;&#65292;&#37319;&#29992;&#36825;&#20123;&#24341;&#23548;&#31574;&#30053;&#21487;&#20197;&#25552;&#39640;&#25512;&#33616;&#24615;&#33021;&#12290;&#36825;&#19968;&#21457;&#29616;&#24378;&#35843;&#20102;&#22312;&#20010;&#24615;&#21270;&#20869;&#23481;&#25512;&#33616;&#20013;&#65292;&#36890;&#36807;&#24341;&#20837;&#22810;&#26679;&#30340;&#24341;&#23548;&#21644;&#36755;&#20837;&#22686;&#24378;&#25216;&#26415;&#26469;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#33616;&#33021;&#21147;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate various prompting strategies for enhancing personalized content recommendation performance with large language models (LLMs) through input augmentation. Our proposed approach, termed LLM-Rec, encompasses four distinct prompting strategies: (1) basic prompting, (2) recommendation-driven prompting, (3) engagement-guided prompting, and (4) recommendation-driven + engagement-guided prompting. Our empirical experiments show that combining the original content description with the augmented input text generated by LLM using these prompting strategies leads to improved recommendation performance. This finding highlights the importance of incorporating diverse prompts and input augmentation techniques to enhance the recommendation capabilities with large language models for personalized content recommendation.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#22312;SuggestBot&#19978;&#36827;&#34892;&#31163;&#32447;&#20998;&#26512;&#21644;&#20026;&#26399;&#19977;&#20010;&#26376;&#30340;&#23545;&#29031;&#23454;&#39564;&#65292;&#21457;&#29616;&#25512;&#33616;&#34987;&#20302;&#20272;&#20027;&#39064;&#30340;&#25991;&#31456;&#21487;&#20197;&#22686;&#21152;&#22312;&#36825;&#20123;&#25991;&#31456;&#19978;&#30340;&#32534;&#36753;&#24037;&#20316;&#37327;&#65292;&#21516;&#26102;&#19981;&#20250;&#26126;&#26174;&#38477;&#20302;&#23545;&#25512;&#33616;&#20869;&#23481;&#30340;&#25509;&#21463;&#31243;&#24230;&#12290;&#36825;&#19968;&#21457;&#29616;&#23545;&#35299;&#20915;&#23545;&#31561;&#29983;&#20135;&#24179;&#21488;&#19978;&#30340;&#20869;&#23481;&#32570;&#21475;&#38382;&#39064;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2307.08669</link><description>&lt;p&gt;
&#21033;&#29992;&#25512;&#33616;&#31995;&#32479;&#32553;&#23567;&#23545;&#31561;&#29983;&#20135;&#24179;&#21488;&#19978;&#30340;&#20869;&#23481;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Leveraging Recommender Systems to Reduce Content Gaps on Peer Production Platforms. (arXiv:2307.08669v2 [cs.CY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08669
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#22312;SuggestBot&#19978;&#36827;&#34892;&#31163;&#32447;&#20998;&#26512;&#21644;&#20026;&#26399;&#19977;&#20010;&#26376;&#30340;&#23545;&#29031;&#23454;&#39564;&#65292;&#21457;&#29616;&#25512;&#33616;&#34987;&#20302;&#20272;&#20027;&#39064;&#30340;&#25991;&#31456;&#21487;&#20197;&#22686;&#21152;&#22312;&#36825;&#20123;&#25991;&#31456;&#19978;&#30340;&#32534;&#36753;&#24037;&#20316;&#37327;&#65292;&#21516;&#26102;&#19981;&#20250;&#26126;&#26174;&#38477;&#20302;&#23545;&#25512;&#33616;&#20869;&#23481;&#30340;&#25509;&#21463;&#31243;&#24230;&#12290;&#36825;&#19968;&#21457;&#29616;&#23545;&#35299;&#20915;&#23545;&#31561;&#29983;&#20135;&#24179;&#21488;&#19978;&#30340;&#20869;&#23481;&#32570;&#21475;&#38382;&#39064;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32500;&#22522;&#30334;&#31185;&#31561;&#23545;&#31561;&#29983;&#20135;&#24179;&#21488;&#24120;&#24120;&#23384;&#22312;&#20869;&#23481;&#32570;&#21475;&#12290;&#20808;&#21069;&#30740;&#31350;&#34920;&#26126;&#65292;&#25512;&#33616;&#31995;&#32479;&#21487;&#20197;&#24110;&#21161;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#36890;&#36807;&#24341;&#23548;&#32534;&#36753;&#20154;&#21592;&#20851;&#27880;&#34987;&#20302;&#20272;&#30340;&#20027;&#39064;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#36825;&#31181;&#26041;&#27861;&#26159;&#21542;&#20250;&#23548;&#33268;&#19981;&#22826;&#30456;&#20851;&#30340;&#25512;&#33616;&#65292;&#20174;&#32780;&#38477;&#20302;&#23545;&#25512;&#33616;&#20869;&#23481;&#30340;&#25972;&#20307;&#21442;&#19982;&#24230;&#12290;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#22312;SuggestBot&#19978;&#36827;&#34892;&#20102;&#31163;&#32447;&#20998;&#26512;&#65288;&#30740;&#31350;1&#65289;&#65292;&#28982;&#21518;&#36827;&#34892;&#20102;&#20026;&#26399;&#19977;&#20010;&#26376;&#30340;&#23545;&#29031;&#23454;&#39564;&#65288;&#30740;&#31350;2&#65289;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#21521;&#29992;&#25143;&#23637;&#31034;&#26469;&#33258;&#34987;&#20302;&#20272;&#20027;&#39064;&#30340;&#25991;&#31456;&#21487;&#20197;&#22686;&#21152;&#22312;&#36825;&#20123;&#25991;&#31456;&#19978;&#30340;&#24037;&#20316;&#37327;&#65292;&#32780;&#19981;&#20250;&#26126;&#26174;&#38477;&#20302;&#23545;&#25512;&#33616;&#20869;&#23481;&#30340;&#25509;&#21463;&#31243;&#24230;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#32467;&#26524;&#30340;&#24847;&#20041;&#65292;&#21253;&#25324;&#22914;&#20309;&#24573;&#35270;&#25991;&#31456;&#21457;&#29616;&#36807;&#31243;&#21487;&#33021;&#20250;&#20154;&#20026;&#22320;&#38480;&#21046;&#25512;&#33616;&#12290;&#25105;&#20204;&#20197;"&#36807;&#28388;&#27668;&#27873;"&#30340;&#24120;&#35265;&#38382;&#39064;&#26469;&#23637;&#31034;&#36825;&#19968;&#29616;&#35937;&#65292;&#24182;&#23545;&#20219;&#20309;&#24179;&#21488;&#37117;&#23384;&#22312;&#30340;&#31867;&#20284;&#38382;&#39064;&#36827;&#34892;&#20102;&#31867;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
Peer production platforms like Wikipedia commonly suffer from content gaps. Prior research suggests recommender systems can help solve this problem, by guiding editors towards underrepresented topics. However, it remains unclear whether this approach would result in less relevant recommendations, leading to reduced overall engagement with recommended items. To answer this question, we first conducted offline analyses (Study 1) on SuggestBot, a task-routing recommender system for Wikipedia, then did a three-month controlled experiment (Study 2). Our results show that presenting users with articles from underrepresented topics increased the proportion of work done on those articles without significantly reducing overall recommendation uptake. We discuss the implications of our results, including how ignoring the article discovery process can artificially narrow recommendations. We draw parallels between this phenomenon and the common issue of "filter bubbles" to show how any platform tha
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#8212;&#8212;&#23569;&#26679;&#26412;N-&#20803;&#20107;&#23454;&#38142;&#25509;&#39044;&#27979;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;FLEN&#30340;&#27169;&#22411;&#26469;&#23454;&#29616;&#12290;FLEN&#30001;&#19977;&#20010;&#27169;&#22359;&#32452;&#25104;&#65292;&#21487;&#20197;&#20174;&#26377;&#38480;&#30340;&#26631;&#35760;&#23454;&#20363;&#20013;&#39044;&#27979;N-&#20803;&#20107;&#23454;&#20013;&#30340;&#32570;&#22833;&#23454;&#20307;&#12290;</title><link>http://arxiv.org/abs/2305.06104</link><description>&lt;p&gt;
N-&#20803;&#20107;&#23454;&#30340;&#23569;&#26679;&#26412;&#38142;&#25509;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Few-shot Link Prediction on N-ary Facts. (arXiv:2305.06104v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06104
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#8212;&#8212;&#23569;&#26679;&#26412;N-&#20803;&#20107;&#23454;&#38142;&#25509;&#39044;&#27979;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;FLEN&#30340;&#27169;&#22411;&#26469;&#23454;&#29616;&#12290;FLEN&#30001;&#19977;&#20010;&#27169;&#22359;&#32452;&#25104;&#65292;&#21487;&#20197;&#20174;&#26377;&#38480;&#30340;&#26631;&#35760;&#23454;&#20363;&#20013;&#39044;&#27979;N-&#20803;&#20107;&#23454;&#20013;&#30340;&#32570;&#22833;&#23454;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
N-&#20803;&#20107;&#23454;&#30001;&#20027;&#35201;&#19977;&#20803;&#32452;&#65288;&#22836;&#23454;&#20307;&#12289;&#20851;&#31995;&#12289;&#23614;&#23454;&#20307;&#65289;&#21644;&#20219;&#24847;&#25968;&#37327;&#30340;&#36741;&#21161;&#23646;&#24615;&#20540;&#23545;&#32452;&#25104;&#65292;&#36825;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#30693;&#35782;&#22270;&#35889;&#20013;&#24456;&#24120;&#35265;&#12290;&#23545;&#20110;N-&#20803;&#20107;&#23454;&#30340;&#38142;&#25509;&#39044;&#27979;&#26159;&#39044;&#27979;&#20854;&#20013;&#19968;&#20010;&#20803;&#32032;&#30340;&#32570;&#22833;&#65292;&#22635;&#34917;&#32570;&#22833;&#20803;&#32032;&#26377;&#21161;&#20110;&#20016;&#23500;&#30693;&#35782;&#22270;&#35889;&#24182;&#20419;&#36827;&#35768;&#22810;&#19979;&#28216;&#24212;&#29992;&#31243;&#24207;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#26469;&#29702;&#35299;N-&#20803;&#20107;&#23454;&#20013;&#30340;&#20803;&#32032;&#65292;&#20294;&#36825;&#20123;&#30740;&#31350;&#24573;&#35270;&#20102;&#23569;&#26679;&#26412;&#20851;&#31995;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#22330;&#26223;&#20013;&#21364;&#24456;&#24120;&#35265;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#24341;&#20837;&#19968;&#20010;&#26032;&#20219;&#21153;&#8212;&#8212;&#23569;&#26679;&#26412;N-&#20803;&#20107;&#23454;&#38142;&#25509;&#39044;&#27979;&#65292;&#26088;&#22312;&#20351;&#29992;&#26377;&#38480;&#30340;&#26631;&#35760;&#23454;&#20363;&#26469;&#39044;&#27979;N-&#20803;&#20107;&#23454;&#20013;&#30340;&#32570;&#22833;&#23454;&#20307;&#12290;&#25105;&#20204;&#20063;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;N-&#20803;&#20107;&#23454;&#30340;&#23569;&#26679;&#26412;&#38142;&#25509;&#39044;&#27979;&#27169;&#22411;FLEN&#65292;&#23427;&#30001;&#19977;&#20010;&#27169;&#22359;&#32452;&#25104;&#65306;&#20851;&#31995;&#23398;&#20064;&#27169;&#22359;&#12289;&#25903;&#25345;&#29305;&#23450;&#35843;&#25972;&#27169;&#22359;&#21644;&#26597;&#35810;&#25512;&#29702;&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;
N-ary facts composed of a primary triple (head entity, relation, tail entity) and an arbitrary number of auxiliary attribute-value pairs, are prevalent in real-world knowledge graphs (KGs). Link prediction on n-ary facts is to predict a missing element in an n-ary fact. This helps populate and enrich KGs and further promotes numerous downstream applications. Previous studies usually require a substantial amount of high-quality data to understand the elements in n-ary facts. However, these studies overlook few-shot relations, which have limited labeled instances, yet are common in real-world scenarios. Thus, this paper introduces a new task, few-shot link prediction on n-ary facts. It aims to predict a missing entity in an n-ary fact with limited labeled instances. We further propose a model for Few-shot Link prEdict on N-ary facts, thus called FLEN, which consists of three modules: the relation learning, support-specific adjusting, and query inference modules. FLEN captures relation me
&lt;/p&gt;</description></item></channel></rss>