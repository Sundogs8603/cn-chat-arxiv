<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21360;&#35937;&#24863;&#30693;&#30340;&#22810;&#34892;&#20026;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;&#27880;&#24847;&#26426;&#21046;&#20174;&#34892;&#20026;&#38388;&#21644;&#34892;&#20026;&#20869;&#37096;&#33719;&#21462;&#20449;&#24687;&#65292;&#24182;&#37319;&#29992;&#22810;&#23618;&#32423;&#22270;&#27880;&#24847;&#21147;&#26041;&#27861;&#65292;&#26469;&#35299;&#20915;&#25512;&#33616;&#31995;&#32479;&#22312;&#22788;&#29702;&#22810;&#20010;&#34892;&#20026;&#20043;&#38388;&#20114;&#21160;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.03169</link><description>&lt;p&gt;
&#22522;&#20110;&#21360;&#35937;&#24863;&#30693;&#30340;&#22810;&#34892;&#20026;&#25512;&#33616;&#31995;&#32479;&#65306;&#19968;&#31181;&#23618;&#27425;&#22270;&#27880;&#24847;&#21147;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Impression-Informed Multi-Behavior Recommender System: A Hierarchical Graph Attention Approach. (arXiv:2309.03169v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03169
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21360;&#35937;&#24863;&#30693;&#30340;&#22810;&#34892;&#20026;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;&#27880;&#24847;&#26426;&#21046;&#20174;&#34892;&#20026;&#38388;&#21644;&#34892;&#20026;&#20869;&#37096;&#33719;&#21462;&#20449;&#24687;&#65292;&#24182;&#37319;&#29992;&#22810;&#23618;&#32423;&#22270;&#27880;&#24847;&#21147;&#26041;&#27861;&#65292;&#26469;&#35299;&#20915;&#25512;&#33616;&#31995;&#32479;&#22312;&#22788;&#29702;&#22810;&#20010;&#34892;&#20026;&#20043;&#38388;&#20114;&#21160;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#25512;&#33616;&#31995;&#32479;&#20174;&#38544;&#24335;&#21453;&#39304;&#20013;&#33719;&#30410;&#33391;&#22810;&#65292;&#20294;&#24448;&#24448;&#20250;&#24573;&#30053;&#29992;&#25143;&#19982;&#29289;&#21697;&#20043;&#38388;&#30340;&#22810;&#34892;&#20026;&#20114;&#21160;&#30340;&#32454;&#24494;&#24046;&#21035;&#12290;&#21382;&#21490;&#19978;&#65292;&#36825;&#20123;&#31995;&#32479;&#35201;&#20040;&#23558;&#25152;&#26377;&#34892;&#20026;&#65292;&#22914;&#8220;&#21360;&#35937;&#8221;&#65288;&#20197;&#21069;&#31216;&#20026;&#8220;&#27983;&#35272;&#8221;&#65289;&#12289;&#8220;&#28155;&#21152;&#21040;&#36141;&#29289;&#36710;&#8221;&#21644;&#8220;&#36141;&#20080;&#8221;&#65292;&#24402;&#24182;&#20026;&#19968;&#20010;&#32479;&#19968;&#30340;&#8220;&#20114;&#21160;&#8221;&#26631;&#31614;&#65292;&#35201;&#20040;&#20165;&#20248;&#20808;&#32771;&#34385;&#30446;&#26631;&#34892;&#20026;&#65292;&#36890;&#24120;&#26159;&#8220;&#36141;&#20080;&#8221;&#34892;&#20026;&#65292;&#24182;&#20002;&#24323;&#26377;&#20215;&#20540;&#30340;&#36741;&#21161;&#20449;&#21495;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;&#36827;&#23637;&#35797;&#22270;&#35299;&#20915;&#36825;&#31181;&#31616;&#21270;&#65292;&#20294;&#23427;&#20204;&#20027;&#35201;&#38598;&#20013;&#20110;&#20248;&#21270;&#30446;&#26631;&#34892;&#20026;&#65292;&#19982;&#25968;&#25454;&#31232;&#32570;&#20316;&#26007;&#20105;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#24448;&#24448;&#32469;&#36807;&#20102;&#19982;&#34892;&#20026;&#20869;&#22312;&#23618;&#27425;&#32467;&#26500;&#26377;&#20851;&#30340;&#24494;&#22937;&#24046;&#24322;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#20123;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#8220;H&#8221;ierarchical &#8220;M&#8221;ulti-behavior &#8220;G&#8221;raph Attention &#8220;N&#8221;etwork&#65288;HMGN&#65289;&#12290;&#36825;&#20010;&#24320;&#21019;&#24615;&#30340;&#26694;&#26550;&#21033;&#29992;&#27880;&#24847;&#26426;&#21046;&#20174;&#34892;&#20026;&#38388;&#21644;&#34892;&#20026;&#20869;&#37096;&#33719;&#21462;&#20449;&#24687;&#65292;&#21516;&#26102;&#37319;&#29992;&#22810;
&lt;/p&gt;
&lt;p&gt;
While recommender systems have significantly benefited from implicit feedback, they have often missed the nuances of multi-behavior interactions between users and items. Historically, these systems either amalgamated all behaviors, such as \textit{impression} (formerly \textit{view}), \textit{add-to-cart}, and \textit{buy}, under a singular 'interaction' label, or prioritized only the target behavior, often the \textit{buy} action, discarding valuable auxiliary signals. Although recent advancements tried addressing this simplification, they primarily gravitated towards optimizing the target behavior alone, battling with data scarcity. Additionally, they tended to bypass the nuanced hierarchy intrinsic to behaviors. To bridge these gaps, we introduce the \textbf{H}ierarchical \textbf{M}ulti-behavior \textbf{G}raph Attention \textbf{N}etwork (HMGN). This pioneering framework leverages attention mechanisms to discern information from both inter and intra-behaviors while employing a multi-
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#22312;&#32447;&#20581;&#24247;&#31038;&#21306;&#20013;&#22686;&#24378;&#31038;&#20250;&#25903;&#25345;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#20197;&#35299;&#20915;&#38382;&#39064;&#25552;&#20986;&#32773;&#25214;&#21040;&#36866;&#24403;&#38382;&#39064;&#35299;&#31572;&#32773;&#30340;&#38382;&#39064;&#12290;&#19982;&#20256;&#32479;&#30340;&#25512;&#33616;&#31995;&#32479;&#19981;&#21516;&#30340;&#26159;&#65292;&#35813;&#31995;&#32479;&#32771;&#34385;&#21040;&#20102;&#21327;&#21161;&#32773;-&#38382;&#39064;&#25552;&#20986;&#32773;&#38142;&#25509;&#30340;&#22797;&#26434;&#24615;&#65292;&#24182;&#35299;&#20915;&#20102;&#21382;&#21490;&#34892;&#20026;&#21644;&#36129;&#29486;&#23545;&#31038;&#20250;&#25903;&#25345;&#30340;&#24433;&#21709;&#30340;&#21306;&#20998;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2309.02978</link><description>&lt;p&gt;
&#22312;&#32447;&#20581;&#24247;&#31038;&#21306;&#20013;&#20855;&#26377;&#36164;&#21382;&#25511;&#21046;&#30340;&#21327;&#21161;&#32773;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Helper Recommendation with seniority control in Online Health Community. (arXiv:2309.02978v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02978
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#22312;&#32447;&#20581;&#24247;&#31038;&#21306;&#20013;&#22686;&#24378;&#31038;&#20250;&#25903;&#25345;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#20197;&#35299;&#20915;&#38382;&#39064;&#25552;&#20986;&#32773;&#25214;&#21040;&#36866;&#24403;&#38382;&#39064;&#35299;&#31572;&#32773;&#30340;&#38382;&#39064;&#12290;&#19982;&#20256;&#32479;&#30340;&#25512;&#33616;&#31995;&#32479;&#19981;&#21516;&#30340;&#26159;&#65292;&#35813;&#31995;&#32479;&#32771;&#34385;&#21040;&#20102;&#21327;&#21161;&#32773;-&#38382;&#39064;&#25552;&#20986;&#32773;&#38142;&#25509;&#30340;&#22797;&#26434;&#24615;&#65292;&#24182;&#35299;&#20915;&#20102;&#21382;&#21490;&#34892;&#20026;&#21644;&#36129;&#29486;&#23545;&#31038;&#20250;&#25903;&#25345;&#30340;&#24433;&#21709;&#30340;&#21306;&#20998;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#20581;&#24247;&#31038;&#21306;&#26159;&#30149;&#20154;&#20132;&#27969;&#32463;&#39564;&#24182;&#25552;&#20379;&#24515;&#29702;&#25903;&#25345;&#30340;&#35770;&#22363;&#12290;&#22312;&#22312;&#32447;&#20581;&#24247;&#31038;&#21306;&#20013;&#65292;&#31038;&#20250;&#25903;&#25345;&#23545;&#20110;&#24110;&#21161;&#21644;&#24247;&#22797;&#30149;&#20154;&#36215;&#21040;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32447;&#19978;&#20581;&#24247;&#31038;&#21306;&#20013;&#30340;&#38382;&#39064;&#20247;&#22810;&#19988;&#30149;&#20154;&#35775;&#38382;&#20855;&#26377;&#38543;&#26426;&#24615;&#65292;&#24456;&#22810;&#30149;&#20154;&#30340;&#26102;&#25928;&#24615;&#38382;&#39064;&#24448;&#24448;&#27809;&#26377;&#24471;&#21040;&#22238;&#31572;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#36843;&#20999;&#38656;&#35201;&#25552;&#20986;&#19968;&#20010;&#25512;&#33616;&#31995;&#32479;&#65292;&#24110;&#21161;&#38382;&#39064;&#25552;&#20986;&#32773;&#25214;&#21040;&#21512;&#36866;&#30340;&#38382;&#39064;&#35299;&#31572;&#32773;&#12290;&#28982;&#32780;&#65292;&#22312;&#32447;&#20581;&#24247;&#31038;&#21306;&#20013;&#24320;&#21457;&#19968;&#20010;&#22686;&#24378;&#31038;&#20250;&#25903;&#25345;&#30340;&#25512;&#33616;&#31639;&#27861;&#20173;&#26159;&#19968;&#20010;&#26410;&#34987;&#20805;&#20998;&#30740;&#31350;&#30340;&#39046;&#22495;&#12290;&#20256;&#32479;&#30340;&#25512;&#33616;&#31995;&#32479;&#19981;&#33021;&#30452;&#25509;&#36866;&#29992;&#20110;&#36825;&#20010;&#38382;&#39064;&#65292;&#22240;&#20026;&#31532;&#19968;&#65292;&#19982;&#20256;&#32479;&#30340;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#29992;&#25143;-&#29289;&#21697;&#38142;&#25509;&#19981;&#21516;&#65292;&#22312;&#32447;&#20581;&#24247;&#31038;&#21306;&#20013;&#30340;&#21327;&#21161;&#32773;-&#38382;&#39064;&#25552;&#20986;&#32773;&#38142;&#25509;&#24456;&#38590;&#24314;&#27169;&#65292;&#22240;&#20026;&#23427;&#20204;&#26159;&#22522;&#20110;&#21508;&#31181;&#24322;&#36136;&#30340;&#21407;&#22240;&#24418;&#25104;&#30340;&#12290;&#31532;&#20108;&#65292;&#24456;&#38590;&#21306;&#20998;&#21382;&#21490;&#34892;&#20026;&#21644;&#36129;&#29486;&#23545;&#31038;&#20250;&#25903;&#25345;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online health communities (OHCs) are forums where patients with similar conditions communicate their experiences and provide moral support. Social support in OHCs plays a crucial role in easing and rehabilitating patients. However, many time-sensitive questions from patients often remain unanswered due to the multitude of threads and the random nature of patient visits in OHCs. To address this issue, it is imperative to propose a recommender system that assists solution seekers in finding appropriate problem helpers. Nevertheless, developing a recommendation algorithm to enhance social support in OHCs remains an under-explored area. Traditional recommender systems cannot be directly adapted due to the following obstacles. First, unlike user-item links in traditional recommender systems, it is hard to model the social support behind helper-seeker links in OHCs since they are formed based on various heterogeneous reasons. Second, it is difficult to distinguish the impact of historical ac
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26377;&#25928;&#30340;&#36755;&#20837;&#37325;&#26500;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#27861;&#24459;&#26696;&#20214;&#26816;&#32034;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#27861;&#24459;&#29305;&#24449;&#23545;&#40784;&#21644;&#27861;&#24459;&#19978;&#19979;&#25991;&#20445;&#30041;&#20004;&#20010;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.02962</link><description>&lt;p&gt;
&#22522;&#20110;&#25552;&#31034;&#30340;&#27861;&#24459;&#26696;&#20214;&#26816;&#32034;&#30340;&#26377;&#25928;&#36755;&#20837;&#37325;&#26500;
&lt;/p&gt;
&lt;p&gt;
Prompt-based Effective Input Reformulation for Legal Case Retrieval. (arXiv:2309.02962v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02962
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26377;&#25928;&#30340;&#36755;&#20837;&#37325;&#26500;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#27861;&#24459;&#26696;&#20214;&#26816;&#32034;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#27861;&#24459;&#29305;&#24449;&#23545;&#40784;&#21644;&#27861;&#24459;&#19978;&#19979;&#25991;&#20445;&#30041;&#20004;&#20010;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27861;&#24459;&#26696;&#20363;&#26816;&#32034;&#22312;&#27861;&#24459;&#20174;&#19994;&#32773;&#26377;&#25928;&#26816;&#32034;&#30456;&#20851;&#26696;&#20363;&#26102;&#36215;&#21040;&#37325;&#35201;&#20316;&#29992;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#31070;&#32463;&#27861;&#24459;&#26696;&#20363;&#26816;&#32034;&#27169;&#22411;&#30452;&#25509;&#23545;&#26696;&#20363;&#30340;&#25972;&#20010;&#27861;&#24459;&#25991;&#26412;&#36827;&#34892;&#32534;&#30721;&#20197;&#29983;&#25104;&#26696;&#20363;&#34920;&#31034;&#65292;&#24182;&#21033;&#29992;&#35813;&#34920;&#31034;&#36827;&#34892;&#26368;&#36817;&#37051;&#25628;&#32034;&#20197;&#36827;&#34892;&#26816;&#32034;&#12290;&#23613;&#31649;&#36825;&#20123;&#30452;&#25509;&#30340;&#26041;&#27861;&#22312;&#26816;&#32034;&#20934;&#30830;&#24615;&#26041;&#38754;&#27604;&#20256;&#32479;&#32479;&#35745;&#26041;&#27861;&#21462;&#24471;&#20102;&#25913;&#36827;&#65292;&#20294;&#26412;&#25991;&#25351;&#20986;&#20102;&#20004;&#20010;&#37325;&#35201;&#25361;&#25112;&#65306;&#65288;1&#65289;&#27861;&#24459;&#29305;&#24449;&#23545;&#40784;&#65306;&#20351;&#29992;&#25972;&#20010;&#26696;&#20363;&#25991;&#26412;&#20316;&#20026;&#36755;&#20837;&#36890;&#24120;&#20250;&#21253;&#21547;&#20887;&#20313;&#21644;&#22122;&#38899;&#20449;&#24687;&#65292;&#22240;&#20026;&#20174;&#27861;&#24459;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#30456;&#20851;&#26696;&#20363;&#30340;&#20915;&#23450;&#22240;&#32032;&#26159;&#20851;&#38190;&#27861;&#24459;&#29305;&#24449;&#30340;&#23545;&#40784;&#65292;&#32780;&#19981;&#26159;&#25972;&#20010;&#25991;&#26412;&#21305;&#37197;&#65307;&#65288;2&#65289;&#27861;&#24459;&#19978;&#19979;&#25991;&#20445;&#30041;&#65306;&#27492;&#22806;&#65292;&#30001;&#20110;&#29616;&#26377;&#30340;&#25991;&#26412;&#32534;&#30721;&#27169;&#22411;&#36890;&#24120;&#26377;&#27604;&#26696;&#20363;&#26356;&#30701;&#30340;&#36755;&#20837;&#38271;&#24230;&#38480;&#21046;&#65292;&#22240;&#27492;&#38656;&#35201;&#25130;&#26029;&#25110;&#20998;&#21106;&#25972;&#20010;&#26696;&#20363;&#25991;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Legal case retrieval plays an important role for legal practitioners to effectively retrieve relevant cases given a query case. Most existing neural legal case retrieval models directly encode the whole legal text of a case to generate a case representation, which is then utilised to conduct a nearest neighbour search for retrieval. Although these straightforward methods have achieved improvement over conventional statistical methods in retrieval accuracy, two significant challenges are identified in this paper: (1) Legal feature alignment: the usage of the whole case text as the input will generally incorporate redundant and noisy information because, from the legal perspective, the determining factor of relevant cases is the alignment of key legal features instead of whole text matching; (2) Legal context preservation: furthermore, since the existing text encoding models usually have an input length limit shorter than the case, the whole case text needs to be truncated or divided int
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#20250;&#35805;&#22411;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#20559;&#35265;&#38382;&#39064;&#65292;&#24182;&#32771;&#34385;&#20102;&#36825;&#20123;&#20559;&#35265;&#22312;CRS&#20013;&#30340;&#24433;&#21709;&#20197;&#21450;&#19982;&#22797;&#26434;&#30340;&#27169;&#22411;&#32467;&#21512;&#26102;&#30340;&#25361;&#25112;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#30740;&#31350;&#35843;&#26597;&#20102;&#32463;&#20856;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#20559;&#35265;&#65292;&#24182;&#35752;&#35770;&#20102;&#19982;&#23545;&#35805;&#31995;&#32479;&#21644;&#35821;&#35328;&#27169;&#22411;&#30456;&#20851;&#30340;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2309.02550</link><description>&lt;p&gt;
&#28165;&#29702;&#20250;&#35805;&#22411;&#25512;&#33616;&#31995;&#32479;&#30340;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Tidying Up the Conversational Recommender Systems' Biases. (arXiv:2309.02550v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02550
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#20250;&#35805;&#22411;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#20559;&#35265;&#38382;&#39064;&#65292;&#24182;&#32771;&#34385;&#20102;&#36825;&#20123;&#20559;&#35265;&#22312;CRS&#20013;&#30340;&#24433;&#21709;&#20197;&#21450;&#19982;&#22797;&#26434;&#30340;&#27169;&#22411;&#32467;&#21512;&#26102;&#30340;&#25361;&#25112;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#30740;&#31350;&#35843;&#26597;&#20102;&#32463;&#20856;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#20559;&#35265;&#65292;&#24182;&#35752;&#35770;&#20102;&#19982;&#23545;&#35805;&#31995;&#32479;&#21644;&#35821;&#35328;&#27169;&#22411;&#30456;&#20851;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#26085;&#30410;&#27969;&#34892;&#24341;&#36215;&#20102;&#23545;&#20250;&#35805;&#22411;&#25512;&#33616;&#31995;&#32479;&#65288;CRS&#65289;&#30340;&#20852;&#36259;&#65292;&#22312;&#24037;&#19994;&#30028;&#21644;&#30740;&#31350;&#30028;&#37117;&#22791;&#21463;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31995;&#32479;&#20013;&#30340;&#20559;&#35265;&#38382;&#39064;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#25285;&#24551;&#12290;&#34429;&#28982;CRS&#30340;&#20010;&#21035;&#32452;&#20214;&#24050;&#32463;&#25509;&#21463;&#20102;&#23545;&#20559;&#35265;&#30340;&#30740;&#31350;&#65292;&#20294;&#30446;&#21069;&#22312;&#29702;&#35299;CRS&#20013;&#29305;&#23450;&#20559;&#35265;&#20197;&#21450;&#24403;&#20854;&#38598;&#25104;&#21040;&#22797;&#26434;&#30340;CRS&#27169;&#22411;&#20013;&#26102;&#36825;&#20123;&#20559;&#35265;&#22914;&#20309;&#34987;&#25918;&#22823;&#25110;&#20943;&#24369;&#26041;&#38754;&#23384;&#22312;&#25991;&#29486;&#31354;&#32570;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#26368;&#36817;&#30340;&#25991;&#29486;&#36827;&#34892;&#35843;&#26597;&#65292;&#25552;&#20379;&#20102;CRS&#20013;&#20559;&#35265;&#30340;&#31616;&#26126;&#32508;&#36848;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20559;&#35265;&#22312;&#31995;&#32479;&#27969;&#31243;&#20013;&#30340;&#23384;&#22312;&#65292;&#24182;&#32771;&#34385;&#20102;&#23558;&#22810;&#20010;&#27169;&#22411;&#32467;&#21512;&#22312;&#19968;&#36215;&#26102;&#20135;&#29983;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#35843;&#26597;&#20102;&#32463;&#20856;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#20559;&#35265;&#21450;&#20854;&#19982;CRS&#30340;&#30456;&#20851;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;CRS&#20013;&#30340;&#29305;&#23450;&#20559;&#35265;&#65292;&#32771;&#34385;&#20102;&#24102;&#26377;&#21644;&#19981;&#24102;&#26377;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#30340;&#21464;&#20307;&#65292;&#20197;&#21450;&#19982;&#23545;&#35805;&#31995;&#32479;&#21644;&#35821;&#35328;&#27169;&#22411;&#30456;&#20851;&#30340;&#20559;&#35265;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#65292;&#25105;&#20204;&#31361;&#20986;&#20102;&#19968;&#20123;&#20851;&#38190;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
The growing popularity of language models has sparked interest in conversational recommender systems (CRS) within both industry and research circles. However, concerns regarding biases in these systems have emerged. While individual components of CRS have been subject to bias studies, a literature gap remains in understanding specific biases unique to CRS and how these biases may be amplified or reduced when integrated into complex CRS models. In this paper, we provide a concise review of biases in CRS by surveying recent literature. We examine the presence of biases throughout the system's pipeline and consider the challenges that arise from combining multiple models. Our study investigates biases in classic recommender systems and their relevance to CRS. Moreover, we address specific biases in CRS, considering variations with and without natural language understanding capabilities, along with biases related to dialogue systems and language models. Through our findings, we highlight t
&lt;/p&gt;</description></item><item><title>MvFS&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#35270;&#22270;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#22810;&#20010;&#23376;&#32593;&#32476;&#27979;&#37327;&#20855;&#26377;&#19981;&#21516;&#29305;&#24449;&#27169;&#24335;&#30340;&#25968;&#25454;&#30340;&#29305;&#24449;&#37325;&#35201;&#24615;&#65292;&#20174;&#32780;&#26356;&#26377;&#25928;&#22320;&#36873;&#25321;&#27599;&#20010;&#23454;&#20363;&#30340;&#20449;&#24687;&#20016;&#23500;&#30340;&#29305;&#24449;&#65292;&#24182;&#35299;&#20915;&#20102;&#24403;&#21069;&#26041;&#27861;&#23545;&#39057;&#32321;&#20986;&#29616;&#29305;&#24449;&#30340;&#20559;&#21521;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.02064</link><description>&lt;p&gt;
MvFS: &#22810;&#35270;&#22270;&#29305;&#24449;&#36873;&#25321;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
MvFS: Multi-view Feature Selection for Recommender System. (arXiv:2309.02064v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02064
&lt;/p&gt;
&lt;p&gt;
MvFS&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#35270;&#22270;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#22810;&#20010;&#23376;&#32593;&#32476;&#27979;&#37327;&#20855;&#26377;&#19981;&#21516;&#29305;&#24449;&#27169;&#24335;&#30340;&#25968;&#25454;&#30340;&#29305;&#24449;&#37325;&#35201;&#24615;&#65292;&#20174;&#32780;&#26356;&#26377;&#25928;&#22320;&#36873;&#25321;&#27599;&#20010;&#23454;&#20363;&#30340;&#20449;&#24687;&#20016;&#23500;&#30340;&#29305;&#24449;&#65292;&#24182;&#35299;&#20915;&#20102;&#24403;&#21069;&#26041;&#27861;&#23545;&#39057;&#32321;&#20986;&#29616;&#29305;&#24449;&#30340;&#20559;&#21521;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#36873;&#25321;&#26159;&#25512;&#33616;&#31995;&#32479;&#20013;&#36873;&#25321;&#20851;&#38190;&#29305;&#24449;&#30340;&#25216;&#26415;&#65292;&#36817;&#24180;&#26469;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#20851;&#27880;&#12290;&#26368;&#36817;&#65292;&#33258;&#36866;&#24212;&#29305;&#24449;&#36873;&#25321;&#65288;AdaFS&#65289;&#36890;&#36807;&#38024;&#23545;&#27599;&#20010;&#25968;&#25454;&#23454;&#20363;&#33258;&#36866;&#24212;&#22320;&#36873;&#25321;&#29305;&#24449;&#65292;&#32771;&#34385;&#21040;&#32473;&#23450;&#29305;&#24449;&#23383;&#27573;&#30340;&#37325;&#35201;&#24615;&#22312;&#25968;&#25454;&#20013;&#21487;&#20197;&#26377;&#26174;&#33879;&#24046;&#24322;&#65292;&#26174;&#31034;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#36873;&#25321;&#36807;&#31243;&#20013;&#20173;&#28982;&#23384;&#22312;&#23481;&#26131;&#20559;&#21521;&#39057;&#32321;&#20986;&#29616;&#30340;&#20027;&#35201;&#29305;&#24449;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#35270;&#22270;&#29305;&#24449;&#36873;&#25321;&#65288;MvFS&#65289;&#65292;&#23427;&#26356;&#26377;&#25928;&#22320;&#20026;&#27599;&#20010;&#23454;&#20363;&#36873;&#25321;&#20449;&#24687;&#20016;&#23500;&#30340;&#29305;&#24449;&#12290;&#26368;&#37325;&#35201;&#30340;&#26159;&#65292;MvFS&#37319;&#29992;&#20102;&#19968;&#20010;&#22810;&#35270;&#22270;&#32593;&#32476;&#65292;&#30001;&#22810;&#20010;&#23376;&#32593;&#32476;&#32452;&#25104;&#65292;&#27599;&#20010;&#23376;&#32593;&#32476;&#23398;&#20064;&#22914;&#20309;&#27979;&#37327;&#20855;&#26377;&#19981;&#21516;&#29305;&#24449;&#27169;&#24335;&#30340;&#25968;&#25454;&#30340;&#29305;&#24449;&#37325;&#35201;&#24615;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;MvFS&#32531;&#35299;&#20102;&#26397;&#21521;&#20027;&#23548;&#27169;&#24335;&#30340;&#20559;&#35265;&#38382;&#39064;&#65292;&#24182;&#20419;&#36827;&#20102;&#26356;&#24179;&#34913;&#30340;&#29305;&#24449;&#36873;&#25321;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;MvFS&#37319;&#29992;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;i
&lt;/p&gt;
&lt;p&gt;
Feature selection, which is a technique to select key features in recommender systems, has received increasing research attention. Recently, Adaptive Feature Selection (AdaFS) has shown remarkable performance by adaptively selecting features for each data instance, considering that the importance of a given feature field can vary significantly across data. However, this method still has limitations in that its selection process could be easily biased to major features that frequently occur. To address these problems, we propose Multi-view Feature Selection (MvFS), which selects informative features for each instance more effectively. Most importantly, MvFS employs a multi-view network consisting of multiple sub-networks, each of which learns to measure the feature importance of a part of data with different feature patterns. By doing so, MvFS mitigates the bias problem towards dominant patterns and promotes a more balanced feature selection process. Moreover, MvFS adopts an effective i
&lt;/p&gt;</description></item><item><title>DynED&#26159;&#19968;&#31181;&#21160;&#24577;&#38598;&#25104;&#22810;&#26679;&#21270;&#26041;&#27861;&#65292;&#22522;&#20110;MRR&#32467;&#21512;&#20102;&#32452;&#20214;&#30340;&#22810;&#26679;&#24615;&#21644;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#22312;&#25968;&#25454;&#27969;&#29615;&#22659;&#20013;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.10807</link><description>&lt;p&gt;
DynED: &#25968;&#25454;&#27969;&#20998;&#31867;&#20013;&#30340;&#21160;&#24577;&#38598;&#25104;&#22810;&#26679;&#21270;
&lt;/p&gt;
&lt;p&gt;
DynED: Dynamic Ensemble Diversification in Data Stream Classification. (arXiv:2308.10807v1 [cs.LG] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10807
&lt;/p&gt;
&lt;p&gt;
DynED&#26159;&#19968;&#31181;&#21160;&#24577;&#38598;&#25104;&#22810;&#26679;&#21270;&#26041;&#27861;&#65292;&#22522;&#20110;MRR&#32467;&#21512;&#20102;&#32452;&#20214;&#30340;&#22810;&#26679;&#24615;&#21644;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#22312;&#25968;&#25454;&#27969;&#29615;&#22659;&#20013;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#25968;&#25454;&#20998;&#24067;&#30340;&#31361;&#21464;&#24615;&#21464;&#21270;&#65292;&#20063;&#31216;&#20026;&#27010;&#24565;&#28418;&#31227;&#65292;&#22312;&#25968;&#25454;&#27969;&#29615;&#22659;&#20013;&#23454;&#29616;&#39640;&#20934;&#30830;&#24230;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#38598;&#21512;&#26041;&#27861;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#20998;&#31867;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290; &#22312;&#38598;&#21512;&#20869;&#37096;&#30340;&#26356;&#22823;&#22810;&#26679;&#24615;&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#23613;&#31649;&#38598;&#21512;&#20869;&#32452;&#20214;&#30340;&#22810;&#26679;&#24615;&#24456;&#39640;&#65292;&#20294;&#24182;&#19981;&#26159;&#25152;&#26377;&#32452;&#20214;&#37117;&#20687;&#39044;&#26399;&#30340;&#37027;&#26679;&#23545;&#25972;&#20307;&#24615;&#33021;&#26377;&#25152;&#36129;&#29486;&#12290;&#36825;&#38656;&#35201;&#19968;&#31181;&#26041;&#27861;&#26469;&#36873;&#25321;&#23637;&#29616;&#20986;&#39640;&#24615;&#33021;&#21644;&#22810;&#26679;&#24615;&#30340;&#32452;&#20214;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;MMR&#65288;&#26368;&#22823;&#36793;&#38469;&#30456;&#20851;&#24615;&#65289;&#30340;&#26032;&#22411;&#38598;&#21512;&#26500;&#24314;&#21644;&#32500;&#25252;&#26041;&#27861;&#65292;&#22312;&#32452;&#21512;&#38598;&#21512;&#30340;&#36807;&#31243;&#20013;&#21160;&#24577;&#22320;&#32467;&#21512;&#20102;&#32452;&#20214;&#30340;&#22810;&#26679;&#24615;&#21644;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#22312;&#22235;&#20010;&#30495;&#23454;&#21644;11&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#65288;DynED&#65289;&#30456;&#27604;&#20110;&#20116;&#31181;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#26041;&#27861;&#25552;&#20379;&#20102;&#26356;&#39640;&#30340;&#24179;&#22343;&#20934;&#30830;&#29575;
&lt;/p&gt;
&lt;p&gt;
Ensemble methods are commonly used in classification due to their remarkable performance. Achieving high accuracy in a data stream environment is a challenging task considering disruptive changes in the data distribution, also known as concept drift. A greater diversity of ensemble components is known to enhance prediction accuracy in such settings. Despite the diversity of components within an ensemble, not all contribute as expected to its overall performance. This necessitates a method for selecting components that exhibit high performance and diversity. We present a novel ensemble construction and maintenance approach based on MMR (Maximal Marginal Relevance) that dynamically combines the diversity and prediction accuracy of components during the process of structuring an ensemble. The experimental results on both four real and 11 synthetic datasets demonstrate that the proposed approach (DynED) provides a higher average mean accuracy compared to the five state-of-the-art baselines
&lt;/p&gt;</description></item><item><title>Ducho&#26159;&#19968;&#20010;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#20013;&#22810;&#27169;&#24577;&#29305;&#24449;&#25552;&#21462;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#36890;&#36807;&#38598;&#25104;&#19977;&#20010;&#24191;&#27867;&#37319;&#29992;&#30340;&#28145;&#24230;&#23398;&#20064;&#24211;&#20316;&#20026;&#21518;&#31471;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#20849;&#20139;&#30028;&#38754;&#26469;&#25552;&#21462;&#21644;&#22788;&#29702;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2306.17125</link><description>&lt;p&gt;
Ducho: &#19968;&#31181;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#20013;&#22810;&#27169;&#24577;&#29305;&#24449;&#25552;&#21462;&#30340;&#32479;&#19968;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Ducho: A Unified Framework for the Extraction of Multimodal Features in Recommendation. (arXiv:2306.17125v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17125
&lt;/p&gt;
&lt;p&gt;
Ducho&#26159;&#19968;&#20010;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#20013;&#22810;&#27169;&#24577;&#29305;&#24449;&#25552;&#21462;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#36890;&#36807;&#38598;&#25104;&#19977;&#20010;&#24191;&#27867;&#37319;&#29992;&#30340;&#28145;&#24230;&#23398;&#20064;&#24211;&#20316;&#20026;&#21518;&#31471;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#20849;&#20139;&#30028;&#38754;&#26469;&#25552;&#21462;&#21644;&#22788;&#29702;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#27169;&#24577;&#24863;&#30693;&#30340;&#25512;&#33616;&#31995;&#32479;&#20013;&#65292;&#26377;&#24847;&#20041;&#30340;&#22810;&#27169;&#24577;&#29305;&#24449;&#30340;&#25552;&#21462;&#26159;&#23454;&#29616;&#39640;&#36136;&#37327;&#25512;&#33616;&#30340;&#22522;&#30784;&#12290;&#36890;&#24120;&#65292;&#27599;&#20010;&#25512;&#33616;&#26694;&#26550;&#37117;&#20250;&#20351;&#29992;&#29305;&#23450;&#30340;&#31574;&#30053;&#21644;&#24037;&#20855;&#26469;&#23454;&#29616;&#20854;&#22810;&#27169;&#24577;&#29305;&#24449;&#30340;&#25552;&#21462;&#36807;&#31243;&#12290;&#36825;&#31181;&#38480;&#21046;&#26377;&#20004;&#20010;&#21407;&#22240;&#65306;&#65288;&#19968;&#65289;&#19981;&#21516;&#30340;&#29305;&#24449;&#25552;&#21462;&#31574;&#30053;&#19981;&#21033;&#20110;&#22810;&#27169;&#24577;&#25512;&#33616;&#26694;&#26550;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#32852;&#65292;&#22240;&#27492;&#26080;&#27861;&#36827;&#34892;&#26377;&#25928;&#21644;&#20844;&#24179;&#30340;&#27604;&#36739;&#65307;&#65288;&#20108;&#65289;&#30001;&#20110;&#19981;&#21516;&#30340;&#24320;&#28304;&#24037;&#20855;&#25552;&#20379;&#20102;&#22823;&#37327;&#32463;&#36807;&#39044;&#35757;&#32451;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#27169;&#22411;&#35774;&#35745;&#32773;&#26080;&#27861;&#35775;&#38382;&#20849;&#20139;&#30028;&#38754;&#26469;&#25552;&#21462;&#29305;&#24449;&#12290;&#22312;&#19978;&#36848;&#38382;&#39064;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Ducho&#65292;&#19968;&#31181;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#20013;&#22810;&#27169;&#24577;&#29305;&#24449;&#25552;&#21462;&#30340;&#32479;&#19968;&#26694;&#26550;&#12290;&#36890;&#36807;&#38598;&#25104;&#19977;&#20010;&#24191;&#27867;&#37319;&#29992;&#30340;&#28145;&#24230;&#23398;&#20064;&#24211;&#20316;&#20026;&#21518;&#31471;&#65292;&#21363;TensorFlow&#12289;PyTorch&#21644;Transformers&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#20849;&#20139;&#30028;&#38754;&#65292;&#29992;&#20110;&#25552;&#21462;&#21644;&#22788;&#29702;&#29305;&#24449;&#65292;&#27599;&#20010;&#21518;&#31471;&#37117;&#26377;&#33258;&#24049;&#30340;&#29305;&#23450;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In multimodal-aware recommendation, the extraction of meaningful multimodal features is at the basis of high-quality recommendations. Generally, each recommendation framework implements its multimodal extraction procedures with specific strategies and tools. This is limiting for two reasons: (i) different extraction strategies do not ease the interdependence among multimodal recommendation frameworks; thus, they cannot be efficiently and fairly compared; (ii) given the large plethora of pre-trained deep learning models made available by different open source tools, model designers do not have access to shared interfaces to extract features. Motivated by the outlined aspects, we propose Ducho, a unified framework for the extraction of multimodal features in recommendation. By integrating three widely-adopted deep learning libraries as backends, namely, TensorFlow, PyTorch, and Transformers, we provide a shared interface to extract and process features where each backend's specific metho
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22810;&#27169;&#24577;&#21307;&#23398;&#25968;&#25454;&#34701;&#21512;&#22312;&#26234;&#24935;&#21307;&#30103;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#31526;&#21512;DIKW&#26426;&#21046;&#30340;&#36890;&#29992;&#34701;&#21512;&#26694;&#26550;&#65292;&#25506;&#35752;&#20102;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2306.11963</link><description>&lt;p&gt;
&#26234;&#24935;&#21307;&#30103;&#20013;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;:&#20174;&#25968;&#25454;&#12289;&#20449;&#24687;&#12289;&#30693;&#35782;&#21040;&#26234;&#24935;&#20043;&#26053;
&lt;/p&gt;
&lt;p&gt;
Multimodality Fusion for Smart Healthcare: a Journey from Data, Information, Knowledge to Wisdom. (arXiv:2306.11963v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11963
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22810;&#27169;&#24577;&#21307;&#23398;&#25968;&#25454;&#34701;&#21512;&#22312;&#26234;&#24935;&#21307;&#30103;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#31526;&#21512;DIKW&#26426;&#21046;&#30340;&#36890;&#29992;&#34701;&#21512;&#26694;&#26550;&#65292;&#25506;&#35752;&#20102;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#21307;&#23398;&#25968;&#25454;&#34701;&#21512;&#24050;&#25104;&#20026;&#26234;&#24935;&#21307;&#30103;&#20013;&#30340;&#19968;&#31181;&#38761;&#26032;&#24615;&#26041;&#27861;&#65292;&#33021;&#22815;&#20840;&#38754;&#20102;&#35299;&#24739;&#32773;&#20581;&#24247;&#29366;&#20917;&#21644;&#20010;&#24615;&#21270;&#27835;&#30103;&#26041;&#26696;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22810;&#27169;&#24577;&#34701;&#21512;&#20026;&#26234;&#24935;&#21307;&#30103;&#24102;&#26469;&#30340;&#20174;&#25968;&#25454;&#12289;&#20449;&#24687;&#21644;&#30693;&#35782;&#21040;&#26234;&#24935;&#65288;DIKW&#65289;&#20043;&#26053;&#12290;&#20840;&#38754;&#22238;&#39038;&#20102;&#22810;&#27169;&#24577;&#21307;&#23398;&#25968;&#25454;&#34701;&#21512;&#30340;&#30740;&#31350;&#29616;&#29366;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#19981;&#21516;&#25968;&#25454;&#27169;&#24577;&#30340;&#38598;&#25104;&#26041;&#24335;&#12290;&#25991;&#31456;&#25506;&#35752;&#20102;&#29305;&#24449;&#36873;&#25321;&#12289;&#22522;&#20110;&#35268;&#21017;&#30340;&#31995;&#32479;&#12289;&#26426;&#22120;&#23398;&#20064;&#12289;&#28145;&#24230;&#23398;&#20064;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#19981;&#21516;&#26041;&#27861;&#65292;&#29992;&#20110;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#34701;&#21512;&#21644;&#20998;&#26512;&#12290;&#21516;&#26102;&#65292;&#25991;&#31456;&#20063;&#30528;&#37325;&#35752;&#35770;&#20102;&#22810;&#27169;&#24577;&#34701;&#21512;&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#32508;&#21512;&#35780;&#36848;&#30340;&#26694;&#26550;&#21644;&#35265;&#35299;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#31526;&#21512;DIKW&#26426;&#21046;&#30340;&#36890;&#29992;&#22810;&#27169;&#24577;&#21307;&#30103;&#25968;&#25454;&#34701;&#21512;&#26694;&#26550;&#12290;&#27492;&#22806;&#65292;&#25991;&#31456;&#36824;&#25506;&#35752;&#20102;&#26410;&#26469;&#19982;&#39044;&#27979;&#12289;&#39044;&#38450;&#12289;&#20010;&#24615;&#21270;&#21644;&#27835;&#30103;&#26377;&#20851;&#30340;&#21307;&#30103;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal medical data fusion has emerged as a transformative approach in smart healthcare, enabling a comprehensive understanding of patient health and personalized treatment plans. In this paper, a journey from data, information, and knowledge to wisdom (DIKW) is explored through multimodal fusion for smart healthcare. A comprehensive review of multimodal medical data fusion focuses on the integration of various data modalities are presented. It explores different approaches such as Feature selection, Rule-based systems, Machine learning, Deep learning, and Natural Language Processing for fusing and analyzing multimodal data. The paper also highlights the challenges associated with multimodal fusion in healthcare. By synthesizing the reviewed frameworks and insights, a generic framework for multimodal medical data fusion is proposed while aligning with the DIKW mechanism. Moreover, it discusses future directions aligned with the four pillars of healthcare: Predictive, Preventive, Pe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#31639;&#27861;&#37319;&#26679;&#29992;&#25143;&#31354;&#38388;&#20559;&#22909;&#30340;POI&#25512;&#33616;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#21482;&#22522;&#20110;&#29992;&#25143;&#20808;&#21069;&#35775;&#38382;&#20301;&#32622;&#32858;&#21512;&#30340;&#32570;&#28857;&#65292;&#36866;&#29992;&#20110;&#25512;&#33616;&#26032;&#39062;&#21306;&#22495;&#30340;POI&#12290;</title><link>http://arxiv.org/abs/2304.07041</link><description>&lt;p&gt;
&#19968;&#31181;POI&#25512;&#33616;&#30340;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Diffusion model for POI recommendation. (arXiv:2304.07041v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07041
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#31639;&#27861;&#37319;&#26679;&#29992;&#25143;&#31354;&#38388;&#20559;&#22909;&#30340;POI&#25512;&#33616;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#21482;&#22522;&#20110;&#29992;&#25143;&#20808;&#21069;&#35775;&#38382;&#20301;&#32622;&#32858;&#21512;&#30340;&#32570;&#28857;&#65292;&#36866;&#29992;&#20110;&#25512;&#33616;&#26032;&#39062;&#21306;&#22495;&#30340;POI&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19979;&#19968;&#20010;&#20852;&#36259;&#28857;&#65288;POI&#65289;&#30340;&#25512;&#33616;&#26159;&#23450;&#20301;&#26381;&#21153;&#20013;&#30340;&#20851;&#38190;&#20219;&#21153;&#65292;&#26088;&#22312;&#20026;&#29992;&#25143;&#30340;&#19979;&#19968;&#20010;&#30446;&#30340;&#22320;&#25552;&#20379;&#20010;&#24615;&#21270;&#24314;&#35758;&#12290;&#20808;&#21069;&#20851;&#20110;POI&#25512;&#33616;&#30340;&#24037;&#20316;&#20391;&#37325;&#20110;&#23545;&#29992;&#25143;&#31354;&#38388;&#20559;&#22909;&#30340;&#24314;&#27169;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#21033;&#29992;&#31354;&#38388;&#20449;&#24687;&#30340;&#26041;&#27861;&#20165;&#22522;&#20110;&#29992;&#25143;&#20808;&#21069;&#35775;&#38382;&#20301;&#32622;&#30340;&#32858;&#21512;&#65292;&#36825;&#20250;&#20351;&#27169;&#22411;&#19981;&#20250;&#25512;&#33616;&#26032;&#39062;&#21306;&#22495;&#30340;POI&#65292;&#20174;&#32780;&#25439;&#23475;&#20854;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#23558;&#26102;&#38388;&#39034;&#24207;&#20449;&#24687;&#34701;&#20837;&#29992;&#25143;&#30340;&#31354;&#38388;&#20559;&#22909;&#20173;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Diff-POI&#65306;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#37319;&#26679;&#29992;&#25143;&#30340;&#31354;&#38388;&#20559;&#22909;&#65292;&#20197;&#36827;&#34892;&#19979;&#19968;&#27493;POI&#25512;&#33616;&#12290;&#22312;&#25193;&#25955;&#31639;&#27861;&#22312;&#20174;&#20998;&#24067;&#20013;&#36827;&#34892;&#37319;&#26679;&#26041;&#38754;&#30340;&#24191;&#27867;&#24212;&#29992;&#30340;&#21551;&#21457;&#19979;&#65292;Diff-POI&#20351;&#29992;&#20004;&#20010;&#37327;&#36523;&#23450;&#21046;&#30340;&#22270;&#32534;&#30721;&#27169;&#22359;&#23545;&#29992;&#25143;&#30340;&#35775;&#38382;&#24207;&#21015;&#21644;&#31354;&#38388;&#29305;&#24615;&#36827;&#34892;&#32534;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Next Point-of-Interest (POI) recommendation is a critical task in location-based services that aim to provide personalized suggestions for the user's next destination. Previous works on POI recommendation have laid focused on modeling the user's spatial preference. However, existing works that leverage spatial information are only based on the aggregation of users' previous visited positions, which discourages the model from recommending POIs in novel areas. This trait of position-based methods will harm the model's performance in many situations. Additionally, incorporating sequential information into the user's spatial preference remains a challenge. In this paper, we propose Diff-POI: a Diffusion-based model that samples the user's spatial preference for the next POI recommendation. Inspired by the wide application of diffusion algorithm in sampling from distributions, Diff-POI encodes the user's visiting sequence and spatial character with two tailor-designed graph encoding modules
&lt;/p&gt;</description></item><item><title>META-CODE&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25506;&#32034;&#23398;&#20064;&#21644;&#26131;&#20110;&#25910;&#38598;&#30340;&#33410;&#28857;&#20803;&#25968;&#25454;&#65292;&#22312;&#26410;&#30693;&#25299;&#25169;&#32593;&#32476;&#20013;&#26816;&#27979;&#37325;&#21472;&#31038;&#21306;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;META-CODE&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.04497</link><description>&lt;p&gt;
&#26410;&#30693;&#25299;&#25169;&#32593;&#32476;&#20013;&#30340;&#25506;&#32034;&#23398;&#20064;&#36741;&#21161;&#31038;&#21306;&#26816;&#27979;&#30340;&#32479;&#19968;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Unified Framework for Exploratory Learning-Aided Community Detection in Networks with Unknown Topology. (arXiv:2304.04497v2 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04497
&lt;/p&gt;
&lt;p&gt;
META-CODE&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25506;&#32034;&#23398;&#20064;&#21644;&#26131;&#20110;&#25910;&#38598;&#30340;&#33410;&#28857;&#20803;&#25968;&#25454;&#65292;&#22312;&#26410;&#30693;&#25299;&#25169;&#32593;&#32476;&#20013;&#26816;&#27979;&#37325;&#21472;&#31038;&#21306;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;META-CODE&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31038;&#20132;&#32593;&#32476;&#20013;&#65292;&#21457;&#29616;&#31038;&#21306;&#32467;&#26500;&#20316;&#20026;&#21508;&#31181;&#32593;&#32476;&#20998;&#26512;&#20219;&#21153;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38544;&#31169;&#38382;&#39064;&#25110;&#35775;&#38382;&#38480;&#21046;&#65292;&#32593;&#32476;&#32467;&#26500;&#36890;&#24120;&#26159;&#26410;&#30693;&#30340;&#65292;&#36825;&#20351;&#24471;&#29616;&#26377;&#30340;&#31038;&#21306;&#26816;&#27979;&#26041;&#27861;&#22312;&#27809;&#26377;&#26114;&#36149;&#30340;&#32593;&#32476;&#25299;&#25169;&#33719;&#21462;&#30340;&#24773;&#20917;&#19979;&#26080;&#25928;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; META-CODE&#65292;&#36825;&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25506;&#32034;&#23398;&#20064;&#36741;&#21161;&#26131;&#20110;&#25910;&#38598;&#30340;&#33410;&#28857;&#20803;&#25968;&#25454;&#65292;&#22312;&#26410;&#30693;&#25299;&#25169;&#32593;&#32476;&#20013;&#26816;&#27979;&#37325;&#21472;&#31038;&#21306;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;META-CODE &#38500;&#20102;&#21021;&#22987;&#30340;&#32593;&#32476;&#25512;&#29702;&#27493;&#39588;&#22806;&#65292;&#36824;&#21253;&#25324;&#19977;&#20010;&#36845;&#20195;&#27493;&#39588;&#65306;1) &#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30340;&#33410;&#28857;&#32423;&#31038;&#21306;&#24402;&#23646;&#23884;&#20837;&#65292;&#36890;&#36807;&#25105;&#20204;&#30340;&#26032;&#37325;&#26500;&#25439;&#22833;&#36827;&#34892;&#35757;&#32451;&#65292;2) &#22522;&#20110;&#31038;&#21306;&#24402;&#23646;&#30340;&#33410;&#28857;&#26597;&#35810;&#36827;&#34892;&#32593;&#32476;&#25506;&#32034;&#65292;3) &#20351;&#29992;&#25506;&#32034;&#32593;&#32476;&#20013;&#30340;&#22522;&#20110;&#36793;&#36830;&#25509;&#30340;&#36830;&#20307;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#36827;&#34892;&#32593;&#32476;&#25512;&#29702;&#12290;&#36890;&#36807;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102; META-CODE &#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In social networks, the discovery of community structures has received considerable attention as a fundamental problem in various network analysis tasks. However, due to privacy concerns or access restrictions, the network structure is often unknown, thereby rendering established community detection approaches ineffective without costly network topology acquisition. To tackle this challenge, we present META-CODE, a unified framework for detecting overlapping communities in networks with unknown topology via exploratory learning aided by easy-to-collect node metadata. Specifically, META-CODE consists of three iterative steps in addition to the initial network inference step: 1) node-level community-affiliation embeddings based on graph neural networks (GNNs) trained by our new reconstruction loss, 2) network exploration via community-affiliation-based node queries, and 3) network inference using an edge connectivity-based Siamese neural network model from the explored network. Through e
&lt;/p&gt;</description></item></channel></rss>