<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>DotHash&#26159;&#19968;&#31181;&#29992;&#20110;&#38598;&#21512;&#30456;&#20284;&#24230;&#24230;&#37327;&#30340;&#26080;&#20559;&#20272;&#35745;&#22120;&#65292;&#21487;&#29992;&#20110;&#38142;&#25509;&#39044;&#27979;&#21644;&#25991;&#26723;&#21435;&#37325;&#31561;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.17310</link><description>&lt;p&gt;
DotHash&#65306;&#29992;&#20110;&#38142;&#25509;&#39044;&#27979;&#21644;&#25991;&#26723;&#21435;&#37325;&#30340;&#38598;&#21512;&#30456;&#20284;&#24230;&#24230;&#37327;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
DotHash: Estimating Set Similarity Metrics for Link Prediction and Document Deduplication. (arXiv:2305.17310v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17310
&lt;/p&gt;
&lt;p&gt;
DotHash&#26159;&#19968;&#31181;&#29992;&#20110;&#38598;&#21512;&#30456;&#20284;&#24230;&#24230;&#37327;&#30340;&#26080;&#20559;&#20272;&#35745;&#22120;&#65292;&#21487;&#29992;&#20110;&#38142;&#25509;&#39044;&#27979;&#21644;&#25991;&#26723;&#21435;&#37325;&#31561;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38598;&#21512;&#30456;&#20284;&#24230;&#30340;&#24230;&#37327;&#26159;&#20960;&#31181;&#25968;&#25454;&#25366;&#25496;&#20219;&#21153;&#30340;&#26680;&#24515;&#26041;&#38754;&#12290;&#20363;&#22914;&#65292;&#22312;&#32593;&#39029;&#25628;&#32034;&#20013;&#21024;&#38500;&#37325;&#22797;&#32467;&#26524;&#30340;&#24120;&#35265;&#26041;&#27861;&#26159;&#26597;&#30475;&#25152;&#26377;&#39029;&#38754;&#23545;&#20043;&#38388;&#30340;Jaccard&#25351;&#25968;&#12290;&#22312;&#31038;&#20132;&#32593;&#32476;&#20998;&#26512;&#20013;&#65292;&#19968;&#31181;&#22791;&#21463;&#36190;&#36175;&#30340;&#24230;&#37327;&#26159;Adamic-Adar&#25351;&#25968;&#65292;&#24191;&#27867;&#29992;&#20110;&#27604;&#36739;&#39044;&#27979;&#38142;&#25509;&#20013;&#30340;&#33410;&#28857;&#37051;&#22495;&#38598;&#21512;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#35201;&#22788;&#29702;&#30340;&#25968;&#25454;&#37327;&#22686;&#21152;&#65292;&#35745;&#31639;&#25152;&#26377;&#25104;&#23545;&#20043;&#38388;&#30340;&#30830;&#20999;&#30456;&#20284;&#24230;&#21487;&#33021;&#26159;&#26840;&#25163;&#30340;&#12290;&#36825;&#31181;&#35268;&#27169;&#19978;&#30340;&#25361;&#25112;&#24050;&#32463;&#28608;&#21457;&#20102;&#23545;&#20110;&#38598;&#21512;&#30456;&#20284;&#24230;&#24230;&#37327;&#30340;&#26377;&#25928;&#20272;&#35745;&#22120;&#30340;&#30740;&#31350;&#12290;&#26368;&#27969;&#34892;&#30340;&#20004;&#31181;&#20272;&#35745;&#22120;&#65292;MinHash&#21644;SimHash&#65292;&#30340;&#30830;&#34987;&#29992;&#20110;&#38656;&#35201;&#22788;&#29702;&#22823;&#37327;&#25968;&#25454;&#30340;&#24212;&#29992;&#31243;&#24207;&#65292;&#22914;&#25991;&#26723;&#21435;&#37325;&#21644;&#25512;&#33616;&#31995;&#32479;&#12290;&#32771;&#34385;&#21040;&#36825;&#20123;&#20219;&#21153;&#30340;&#37325;&#35201;&#24615;&#65292;&#25512;&#36827;&#20272;&#35745;&#22120;&#30340;&#38656;&#27714;&#26159;&#26174;&#28982;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;DotHash&#65292;&#36825;&#26159;&#20004;&#20010;&#38598;&#21512;&#20132;&#38598;&#22823;&#23567;&#30340;&#26080;&#20559;&#20272;&#35745;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Metrics for set similarity are a core aspect of several data mining tasks. To remove duplicate results in a Web search, for example, a common approach looks at the Jaccard index between all pairs of pages. In social network analysis, a much-celebrated metric is the Adamic-Adar index, widely used to compare node neighborhood sets in the important problem of predicting links. However, with the increasing amount of data to be processed, calculating the exact similarity between all pairs can be intractable. The challenge of working at this scale has motivated research into efficient estimators for set similarity metrics. The two most popular estimators, MinHash and SimHash, are indeed used in applications such as document deduplication and recommender systems where large volumes of data need to be processed. Given the importance of these tasks, the demand for advancing estimators is evident. We propose DotHash, an unbiased estimator for the intersection size of two sets. DotHash can be use
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Recformer&#30340;&#26694;&#26550;&#65292;&#23427;&#23558;&#29992;&#25143;&#21916;&#22909;&#21644;&#39033;&#30446;&#29305;&#24449;&#24314;&#27169;&#20026;&#21487;&#20197;&#25512;&#24191;&#21040;&#26032;&#39033;&#30446;&#21644;&#25968;&#25454;&#38598;&#30340;&#35821;&#35328;&#34920;&#31034;&#65292;&#24182;&#21033;&#29992;&#21452;&#21521;Transformer&#26469;&#25429;&#25417;&#38271;&#26399;&#20381;&#36182;&#20851;&#31995;&#65292;&#29992;&#20110;&#24207;&#21015;&#25512;&#33616;&#65292;&#27604;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2305.13731</link><description>&lt;p&gt;
&#25991;&#26412;&#26159;&#21807;&#19968;&#38656;&#35201;&#30340;&#65306;&#29992;&#20110;&#24207;&#21015;&#25512;&#33616;&#30340;&#35821;&#35328;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Text Is All You Need: Learning Language Representations for Sequential Recommendation. (arXiv:2305.13731v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13731
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Recformer&#30340;&#26694;&#26550;&#65292;&#23427;&#23558;&#29992;&#25143;&#21916;&#22909;&#21644;&#39033;&#30446;&#29305;&#24449;&#24314;&#27169;&#20026;&#21487;&#20197;&#25512;&#24191;&#21040;&#26032;&#39033;&#30446;&#21644;&#25968;&#25454;&#38598;&#30340;&#35821;&#35328;&#34920;&#31034;&#65292;&#24182;&#21033;&#29992;&#21452;&#21521;Transformer&#26469;&#25429;&#25417;&#38271;&#26399;&#20381;&#36182;&#20851;&#31995;&#65292;&#29992;&#20110;&#24207;&#21015;&#25512;&#33616;&#65292;&#27604;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24207;&#21015;&#25512;&#33616;&#26088;&#22312;&#20174;&#21382;&#21490;&#20132;&#20114;&#20013;&#24314;&#27169;&#21160;&#24577;&#29992;&#25143;&#34892;&#20026;&#12290;&#29616;&#26377;&#26041;&#27861;&#20381;&#38752;&#26126;&#30830;&#30340;&#39033;&#30446;ID&#25110;&#19968;&#33324;&#25991;&#26412;&#29305;&#24449;&#36827;&#34892;&#24207;&#21015;&#24314;&#27169;&#65292;&#20197;&#29702;&#35299;&#29992;&#25143;&#21916;&#22909;&#12290;&#23613;&#31649;&#24456;&#26377;&#21069;&#36884;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#20173;&#28982;&#38590;&#20197;&#24314;&#27169;&#20919;&#21551;&#21160;&#39033;&#30446;&#25110;&#23558;&#30693;&#35782;&#36716;&#31227;&#33267;&#26032;&#25968;&#25454;&#38598;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24314;&#35758;&#23558;&#29992;&#25143;&#21916;&#22909;&#21644;&#39033;&#30446;&#29305;&#24449;&#24314;&#27169;&#20026;&#21487;&#20197;&#25512;&#24191;&#21040;&#26032;&#39033;&#30446;&#21644;&#25968;&#25454;&#38598;&#30340;&#35821;&#35328;&#34920;&#31034;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Recformer&#30340;&#26032;&#26694;&#26550;&#65292;&#23427;&#26377;&#25928;&#22320;&#23398;&#20064;&#24207;&#21015;&#25512;&#33616;&#30340;&#35821;&#35328;&#34920;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24314;&#35758;&#36890;&#36807;&#23637;&#24179;&#30001;&#25991;&#26412;&#25551;&#36848;&#30340;&#39033;&#30446;&#38190;&#20540;&#23646;&#24615;&#65292;&#23558;&#39033;&#30446;&#20316;&#20026;&#8220;&#21477;&#23376;&#8221;&#65288;&#21333;&#35789;&#24207;&#21015;&#65289;&#26469;&#32534;&#20889;&#65292;&#20197;&#20415;&#29992;&#25143;&#30340;&#39033;&#30446;&#24207;&#21015;&#25104;&#20026;&#21477;&#23376;&#24207;&#21015;&#12290;&#20026;&#25512;&#33616;&#65292;Recformer&#34987;&#35757;&#32451;&#20197;&#29702;&#35299;&#8220;&#21477;&#23376;&#8221;&#24207;&#21015;&#24182;&#26816;&#32034;&#19979;&#19968;&#20010;&#8220;&#21477;&#23376;&#8221;&#12290;&#20026;&#20102;&#32534;&#30721;&#39033;&#30446;&#24207;&#21015;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21452;&#21521;Transformer&#65292;&#21033;&#29992;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#26469;&#25429;&#25417;&#38271;&#26399;&#20381;&#36182;&#20851;&#31995;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#20855;&#26377;&#19981;&#21516;&#29305;&#24449;&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#24182;&#23637;&#31034;&#20102;Recformer&#22987;&#32456;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequential recommendation aims to model dynamic user behavior from historical interactions. Existing methods rely on either explicit item IDs or general textual features for sequence modeling to understand user preferences. While promising, these approaches still struggle to model cold-start items or transfer knowledge to new datasets. In this paper, we propose to model user preferences and item features as language representations that can be generalized to new items and datasets. To this end, we present a novel framework, named Recformer, which effectively learns language representations for sequential recommendation. Specifically, we propose to formulate an item as a "sentence" (word sequence) by flattening item key-value attributes described by text so that an item sequence for a user becomes a sequence of sentences. For recommendation, Recformer is trained to understand the "sentence" sequence and retrieve the next "sentence". To encode item sequences, we design a bi-directional T
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986; InPars-v2&#65292;&#20351;&#29992;&#24320;&#28304; LLMs &#21644;&#24378;&#22823;&#20877;&#25490;&#24207;&#22120;&#29983;&#25104;&#29992;&#20110;&#20449;&#24687;&#26816;&#32034;&#20013;&#35757;&#32451;&#30340;&#21512;&#25104;&#26597;&#35810;-&#25991;&#26723;&#23545;&#65292;&#21487;&#22312; BEIR &#22522;&#20934;&#27979;&#35797;&#20013;&#36798;&#21040;&#26368;&#26032;&#30340;&#26368;&#22909;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2301.01820</link><description>&lt;p&gt;
InPars-v2: &#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20449;&#24687;&#26816;&#32034;&#39640;&#25928;&#25968;&#25454;&#38598;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
InPars-v2: Large Language Models as Efficient Dataset Generators for Information Retrieval. (arXiv:2301.01820v4 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.01820
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986; InPars-v2&#65292;&#20351;&#29992;&#24320;&#28304; LLMs &#21644;&#24378;&#22823;&#20877;&#25490;&#24207;&#22120;&#29983;&#25104;&#29992;&#20110;&#20449;&#24687;&#26816;&#32034;&#20013;&#35757;&#32451;&#30340;&#21512;&#25104;&#26597;&#35810;-&#25991;&#26723;&#23545;&#65292;&#21487;&#22312; BEIR &#22522;&#20934;&#27979;&#35797;&#20013;&#36798;&#21040;&#26368;&#26032;&#30340;&#26368;&#22909;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26469;&#65292;InPars &#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#20013;&#39640;&#25928;&#29983;&#25104;&#30456;&#20851;&#26597;&#35810;&#30340;&#26041;&#27861;&#65306;&#36890;&#36807;&#23569;&#37327;&#26679;&#26412;&#65292;&#35825;&#23548; LLM &#29983;&#25104;&#19982;&#25991;&#26723;&#30456;&#20851;&#30340;&#26597;&#35810;&#65292;&#22312;&#27492;&#22522;&#30784;&#19978;&#29983;&#25104;&#21512;&#25104;&#30340;&#26597;&#35810;-&#25991;&#26723;&#23545;&#65292;&#29992;&#20110;&#35757;&#32451;&#26816;&#32034;&#22120;&#12290;&#28982;&#32780;&#65292;InPars &#21644; Promptagator &#31561;&#26041;&#27861;&#20381;&#36182;&#20110; GPT-3 &#21644; FLAN &#31561;&#19987;&#26377; LLMs &#29983;&#25104;&#36825;&#20123;&#25968;&#25454;&#38598;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102; InPars-v2&#65292;&#35813;&#25968;&#25454;&#38598;&#29983;&#25104;&#22120;&#20351;&#29992;&#24320;&#25918;&#28304;&#20195;&#30721;&#30340; LLM &#21644;&#29616;&#26377;&#30340;&#24378;&#22823;&#20877;&#25490;&#24207;&#22120;&#26469;&#36873;&#25321;&#29992;&#20110;&#35757;&#32451;&#30340;&#21512;&#25104;&#26597;&#35810;-&#25991;&#26723;&#23545;&#12290;&#19968;&#20010;&#31616;&#21333;&#30340; BM25 &#26816;&#32034;&#31649;&#36947;&#65292;&#22312;&#32463;&#36807;&#30001; InPars-v2 &#25968;&#25454;&#24494;&#35843;&#30340; monoT5 &#20877;&#25490;&#24207;&#22120;&#20043;&#21518;&#65292;&#20415;&#21487;&#22312; BEIR &#22522;&#20934;&#27979;&#35797;&#20013;&#36798;&#21040;&#26368;&#26032;&#30340;&#26368;&#22909;&#32467;&#26524;&#12290;&#20026;&#20102;&#35753;&#30740;&#31350;&#20154;&#21592;&#36827;&#19968;&#27493;&#25552;&#39640;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#24320;&#28304;&#20102;&#20195;&#30721;&#12289;&#25968;&#25454;&#21644;&#24494;&#35843;&#27169;&#22411;&#65306;https://github.com/zetaalphavector/inPars/tree/master/tpu&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, InPars introduced a method to efficiently use large language models (LLMs) in information retrieval tasks: via few-shot examples, an LLM is induced to generate relevant queries for documents. These synthetic query-document pairs can then be used to train a retriever. However, InPars and, more recently, Promptagator, rely on proprietary LLMs such as GPT-3 and FLAN to generate such datasets. In this work we introduce InPars-v2, a dataset generator that uses open-source LLMs and existing powerful rerankers to select synthetic query-document pairs for training. A simple BM25 retrieval pipeline followed by a monoT5 reranker finetuned on InPars-v2 data achieves new state-of-the-art results on the BEIR benchmark. To allow researchers to further improve our method, we open source the code, synthetic data, and finetuned models: https://github.com/zetaalphavector/inPars/tree/master/tpu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#36827;&#34892;&#25512;&#29702;&#30340;&#21069;&#27839;&#30740;&#31350;&#32508;&#21512;&#35843;&#26597;&#12290;&#35752;&#35770;&#20102;&#26032;&#20852;&#25512;&#29702;&#33021;&#21147;&#20986;&#29616;&#30340;&#28508;&#22312;&#21407;&#22240;&#65292;&#24182;&#25552;&#20379;&#31995;&#32479;&#36164;&#28304;&#24110;&#21161;&#21021;&#23398;&#32773;&#12290;</title><link>http://arxiv.org/abs/2212.09597</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#36827;&#34892;&#25512;&#29702;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Reasoning with Language Model Prompting: A Survey. (arXiv:2212.09597v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09597
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#36827;&#34892;&#25512;&#29702;&#30340;&#21069;&#27839;&#30740;&#31350;&#32508;&#21512;&#35843;&#26597;&#12290;&#35752;&#35770;&#20102;&#26032;&#20852;&#25512;&#29702;&#33021;&#21147;&#20986;&#29616;&#30340;&#28508;&#22312;&#21407;&#22240;&#65292;&#24182;&#25552;&#20379;&#31995;&#32479;&#36164;&#28304;&#24110;&#21161;&#21021;&#23398;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#29702;&#20316;&#20026;&#22797;&#26434;&#38382;&#39064;&#35299;&#20915;&#30340;&#37325;&#35201;&#33021;&#21147;&#65292;&#21487;&#20197;&#20026;&#21307;&#30103;&#35786;&#26029;&#12289;&#35848;&#21028;&#31561;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#25552;&#20379;&#21518;&#31471;&#25903;&#25345;&#12290;&#26412;&#25991;&#23545;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#36827;&#34892;&#25512;&#29702;&#30340;&#21069;&#27839;&#30740;&#31350;&#36827;&#34892;&#20102;&#32508;&#21512;&#35843;&#26597;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#30740;&#31350;&#25104;&#26524;&#30340;&#27604;&#36739;&#21644;&#24635;&#32467;&#65292;&#24182;&#25552;&#20379;&#20102;&#31995;&#32479;&#36164;&#28304;&#20197;&#24110;&#21161;&#21021;&#23398;&#32773;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#26032;&#20852;&#25512;&#29702;&#33021;&#21147;&#20986;&#29616;&#30340;&#28508;&#22312;&#21407;&#22240;&#65292;&#24182;&#31361;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#36164;&#28304;&#21487;&#22312; https://github.com/zjunlp/Prompt4ReasoningPapers &#19978;&#33719;&#21462;&#65288;&#23450;&#26399;&#26356;&#26032;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reasoning, as an essential ability for complex problem-solving, can provide back-end support for various real-world applications, such as medical diagnosis, negotiation, etc. This paper provides a comprehensive survey of cutting-edge research on reasoning with language model prompting. We introduce research works with comparisons and summaries and provide systematic resources to help beginners. We also discuss the potential reasons for emerging such reasoning abilities and highlight future research directions. Resources are available at https://github.com/zjunlp/Prompt4ReasoningPapers (updated periodically).
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#21442;&#25968;&#21270;&#35299;&#30721;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#19978;&#19979;&#25991;&#21270;&#35789;&#27719;&#23884;&#20837;&#65292;&#35299;&#20915;&#20102;&#29983;&#25104;&#24335;&#26816;&#32034;&#27169;&#22411;&#20449;&#24687;&#23481;&#37327;&#21463;&#38480;&#30340;&#38382;&#39064;&#65292;&#22312;&#25991;&#26723;&#26816;&#32034;&#20219;&#21153;&#20013;&#20855;&#26377;&#39640;&#25928;&#24615;&#21644;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2210.02068</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;&#26816;&#32034;&#30340;&#38750;&#21442;&#25968;&#21270;&#35299;&#30721;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Nonparametric Decoding for Generative Retrieval. (arXiv:2210.02068v3 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.02068
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#21442;&#25968;&#21270;&#35299;&#30721;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#19978;&#19979;&#25991;&#21270;&#35789;&#27719;&#23884;&#20837;&#65292;&#35299;&#20915;&#20102;&#29983;&#25104;&#24335;&#26816;&#32034;&#27169;&#22411;&#20449;&#24687;&#23481;&#37327;&#21463;&#38480;&#30340;&#38382;&#39064;&#65292;&#22312;&#25991;&#26723;&#26816;&#32034;&#20219;&#21153;&#20013;&#20855;&#26377;&#39640;&#25928;&#24615;&#21644;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#26816;&#32034;&#27169;&#22411;&#20165;&#20381;&#36182;&#20110;&#20854;&#27169;&#22411;&#21442;&#25968;&#20013;&#32534;&#30721;&#30340;&#20449;&#24687;&#65292;&#27809;&#26377;&#22806;&#37096;&#23384;&#20648;&#22120;&#65292;&#20854;&#20449;&#24687;&#23481;&#37327;&#21463;&#21040;&#38480;&#21046;&#24182;&#19988;&#26159;&#22266;&#23450;&#30340;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#21442;&#25968;&#21270;&#35299;&#30721;&#26041;&#27861;&#65288;Np Decoding&#65289;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#29616;&#26377;&#30340;&#29983;&#25104;&#24335;&#26816;&#32034;&#27169;&#22411;&#20013;&#12290;Np Decoding&#20351;&#29992;&#38750;&#21442;&#25968;&#21270;&#30340;&#19978;&#19979;&#25991;&#21270;&#35789;&#27719;&#23884;&#20837;&#65288;&#22806;&#37096;&#23384;&#20648;&#22120;&#65289;&#65292;&#32780;&#19981;&#26159;&#20316;&#20026;&#35299;&#30721;&#22120;&#35789;&#27719;&#23884;&#20837;&#30340;&#24120;&#35268;&#35789;&#27719;&#23884;&#20837;&#12290;&#36890;&#36807;&#21033;&#29992;&#19978;&#19979;&#25991;&#21270;&#35789;&#27719;&#23884;&#20837;&#65292;&#29983;&#25104;&#24335;&#26816;&#32034;&#27169;&#22411;&#33021;&#22815;&#21516;&#26102;&#21033;&#29992;&#21442;&#25968;&#31354;&#38388;&#21644;&#38750;&#21442;&#25968;&#31354;&#38388;&#12290;&#22312;9&#20010;&#25968;&#25454;&#38598;&#65288;8&#20010;&#21333;&#36339;&#21644;1&#20010;&#22810;&#36339;&#65289;&#30340;&#25991;&#26723;&#26816;&#32034;&#20219;&#21153;&#20013;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#23558;Np Decoding&#24212;&#29992;&#20110;&#29983;&#25104;&#24335;&#26816;&#32034;&#27169;&#22411;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#34920;&#26126;&#65292;Np Decoding&#20855;&#26377;&#25968;&#25454;&#21644;&#21442;&#25968;&#25928;&#29575;&#65292;&#24182;&#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#20013;&#34920;&#29616;&#20986;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The generative retrieval model depends solely on the information encoded in its model parameters without external memory, its information capacity is limited and fixed. To overcome the limitation, we propose Nonparametric Decoding (Np Decoding) which can be applied to existing generative retrieval models. Np Decoding uses nonparametric contextualized vocab embeddings (external memory) rather than vanilla vocab embeddings as decoder vocab embeddings. By leveraging the contextualized vocab embeddings, the generative retrieval model is able to utilize both the parametric and nonparametric space. Evaluation over 9 datasets (8 single-hop and 1 multi-hop) in the document retrieval task shows that applying Np Decoding to generative retrieval models significantly improves the performance. We also show that Np Decoding is dataand parameter-efficient, and shows high performance in the zero-shot setting.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32852;&#21512;&#20248;&#21270;&#25490;&#21517;&#21644;&#26657;&#20934;&#33021;&#21147;&#30340;&#26041;&#27861;JRC&#65292;&#36890;&#36807;&#23545;&#27604;&#36755;&#20986;logit&#20540;&#26469;&#25552;&#39640;&#25490;&#21517;&#33021;&#21147;&#21644;&#26657;&#20934;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2208.06164</link><description>&lt;p&gt;
&#34701;&#21512;&#32972;&#26223;&#30340;&#28151;&#21512;&#27169;&#22411;&#19979;&#30340;&#25490;&#21517;&#21644;&#26657;&#20934;&#32852;&#21512;&#20248;&#21270;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Joint Optimization of Ranking and Calibration with Contextualized Hybrid Model. (arXiv:2208.06164v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.06164
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32852;&#21512;&#20248;&#21270;&#25490;&#21517;&#21644;&#26657;&#20934;&#33021;&#21147;&#30340;&#26041;&#27861;JRC&#65292;&#36890;&#36807;&#23545;&#27604;&#36755;&#20986;logit&#20540;&#26469;&#25552;&#39640;&#25490;&#21517;&#33021;&#21147;&#21644;&#26657;&#20934;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#25490;&#21517;&#20248;&#21270;&#25216;&#26415;&#24471;&#21040;&#20102;&#21457;&#23637;&#65292;&#28857;&#23545;&#28857;&#25439;&#22833;&#20173;&#28982;&#26159;&#28857;&#20987;&#29575;&#39044;&#27979;&#30340;&#20027;&#35201;&#26041;&#27861;&#12290;&#36825;&#21487;&#20197;&#24402;&#22240;&#20110;&#28857;&#23545;&#28857;&#25439;&#22833;&#30340;&#26657;&#20934;&#33021;&#21147;&#65292;&#22240;&#20026;&#39044;&#27979;&#21487;&#20197;&#34987;&#35270;&#20026;&#28857;&#20987;&#27010;&#29575;&#12290;&#23454;&#38469;&#19978;&#65292;CTR&#39044;&#27979;&#27169;&#22411;&#20063;&#36890;&#24120;&#36890;&#36807;&#25490;&#21517;&#33021;&#21147;&#36827;&#34892;&#35780;&#20272;&#12290;&#20026;&#20102;&#20248;&#21270;&#25490;&#21517;&#33021;&#21147;&#65292;&#21487;&#20197;&#37319;&#29992;&#25490;&#21517;&#25439;&#22833;&#65288;&#20363;&#22914;&#25104;&#23545;&#25110;&#21015;&#34920;&#25439;&#22833;&#65289;&#65292;&#22240;&#20026;&#23427;&#20204;&#36890;&#24120;&#27604;&#28857;&#23545;&#28857;&#25439;&#22833;&#23454;&#29616;&#26356;&#22909;&#30340;&#25490;&#21517;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#23581;&#35797;&#30452;&#25509;&#23558;&#20004;&#31181;&#25439;&#22833;&#32452;&#21512;&#36215;&#26469;&#20197;&#33719;&#24471;&#20004;&#31181;&#25439;&#22833;&#30340;&#30410;&#22788;&#65292;&#24182;&#35266;&#23519;&#21040;&#20102;&#25913;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20043;&#21069;&#30340;&#30740;&#31350;&#25171;&#30772;&#20102;&#36755;&#20986;logit&#20316;&#20026;&#28857;&#20987;&#29575;&#30340;&#21547;&#20041;&#65292;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#27425;&#20248;&#35299;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#32852;&#21512;&#20248;&#21270;&#25490;&#21517;&#21644;&#26657;&#20934;&#33021;&#21147;&#65288;&#31616;&#31216;JRC&#65289;&#12290;JRC&#36890;&#36807;&#23545;&#36755;&#20986;logit&#20540;&#36827;&#34892;&#23545;&#27604;&#26469;&#25552;&#39640;&#25490;&#21517;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the development of ranking optimization techniques, pointwise loss remains the dominating approach for click-through rate prediction. It can be attributed to the calibration ability of the pointwise loss since the prediction can be viewed as the click probability. In practice, a CTR prediction model is also commonly assessed with the ranking ability. To optimize the ranking ability, ranking loss (e.g., pairwise or listwise loss) can be adopted as they usually achieve better rankings than pointwise loss. Previous studies have experimented with a direct combination of the two losses to obtain the benefit from both losses and observed an improved performance. However, previous studies break the meaning of output logit as the click-through rate, which may lead to sub-optimal solutions. To address this issue, we propose an approach that can Jointly optimize the Ranking and Calibration abilities (JRC for short). JRC improves the ranking ability by contrasting the logit value for the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;TOUR&#31639;&#27861;&#65292;&#23427;&#21033;&#29992;&#20132;&#21449;&#32534;&#30721;&#20877;&#25490;&#24207;&#22120;&#25552;&#20379;&#30340;&#20266;&#26631;&#31614;&#20248;&#21270;&#22522;&#20110;&#23454;&#20363;&#32423;&#21035;&#30340;&#26597;&#35810;&#34920;&#31034;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#31471;&#21040;&#31471;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2205.12680</link><description>&lt;p&gt;
&#20248;&#21270;&#23494;&#38598;&#26816;&#32034;&#30340;&#27979;&#35797;&#26102;&#38388;&#26597;&#35810;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Optimizing Test-Time Query Representations for Dense Retrieval. (arXiv:2205.12680v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.12680
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;TOUR&#31639;&#27861;&#65292;&#23427;&#21033;&#29992;&#20132;&#21449;&#32534;&#30721;&#20877;&#25490;&#24207;&#22120;&#25552;&#20379;&#30340;&#20266;&#26631;&#31614;&#20248;&#21270;&#22522;&#20110;&#23454;&#20363;&#32423;&#21035;&#30340;&#26597;&#35810;&#34920;&#31034;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#31471;&#21040;&#31471;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23494;&#38598;&#26816;&#32034;&#30340;&#21457;&#23637;&#20381;&#36182;&#20110;&#39044;&#35757;&#32451;&#30340;&#26597;&#35810;&#21644;&#19978;&#19979;&#25991;&#32534;&#30721;&#22120;&#25552;&#20379;&#30340;&#36136;&#37327;&#34920;&#31034;&#26597;&#35810;&#21644;&#19978;&#19979;&#25991;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;TOUR&#65288;Test-Time Optimization of Query Representations&#65289;&#65292;&#23427;&#36890;&#36807;&#26469;&#33258;&#27979;&#35797;&#26102;&#38388;&#26816;&#32034;&#32467;&#26524;&#30340;&#20449;&#21495;&#36827;&#19968;&#27493;&#20248;&#21270;&#22522;&#20110;&#23454;&#20363;&#32423;&#21035;&#30340;&#26597;&#35810;&#34920;&#31034;&#12290;&#25105;&#20204;&#21033;&#29992;&#20132;&#21449;&#32534;&#30721;&#22120;&#20877;&#25490;&#24207;&#22120;&#26469;&#20026;&#26816;&#32034;&#32467;&#26524;&#25552;&#20379;&#32454;&#31890;&#24230;&#30340;&#20266;&#26631;&#31614;&#65292;&#24182;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#36845;&#20195;&#22320;&#20248;&#21270;&#26597;&#35810;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;TOUR&#21487;&#20197;&#30475;&#20316;&#26159;&#20266;&#30456;&#20851;&#21453;&#39304;&#30340;&#32463;&#20856;Rocchio&#31639;&#27861;&#30340;&#19968;&#31181;&#25512;&#24191;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#21033;&#29992;&#20266;&#26631;&#31614;&#20316;&#20026;&#30828;&#20108;&#36827;&#21046;&#25110;&#36719;&#36830;&#32493;&#26631;&#31614;&#30340;&#21464;&#20307;&#12290;&#25105;&#20204;&#39318;&#20808;&#23558;TOUR&#24212;&#29992;&#20110;&#30701;&#35821;&#26816;&#32034;&#65292;&#24182;&#20351;&#29992;&#25105;&#20204;&#25552;&#20986;&#30340;&#30701;&#35821;&#20877;&#25490;&#24207;&#22120;&#35780;&#20272;&#20854;&#22312;&#36890;&#36947;&#26816;&#32034;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;TOUR&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#31471;&#21040;&#31471;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent developments of dense retrieval rely on quality representations of queries and contexts from pre-trained query and context encoders. In this paper, we introduce TOUR (Test-Time Optimization of Query Representations), which further optimizes instance-level query representations guided by signals from test-time retrieval results. We leverage a cross-encoder re-ranker to provide fine-grained pseudo labels over retrieval results and iteratively optimize query representations with gradient descent. Our theoretical analysis reveals that TOUR can be viewed as a generalization of the classical Rocchio algorithm for pseudo relevance feedback, and we present two variants that leverage pseudo-labels as hard binary or soft continuous labels. We first apply TOUR on phrase retrieval with our proposed phrase re-ranker, and also evaluate its effectiveness on passage retrieval with an off-the-shelf re-ranker. TOUR greatly improves end-to-end open-domain question answering accuracy, as well as pa
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#24320;&#21457;&#39044;&#27979;&#27169;&#22411;&#30340;&#22810;&#38454;&#27573;&#31995;&#32479;&#65292;&#21487;&#39640;&#31934;&#24230;&#39044;&#27979;&#19977;&#31181;&#30524;&#30142;&#24739;&#32773;&#30340;&#35270;&#21147;&#21464;&#21270;&#65292;&#24182;&#36741;&#21161;&#30524;&#31185;&#21307;&#29983;&#36827;&#34892;&#20020;&#24202;&#20915;&#31574;&#21644;&#24739;&#32773;&#21672;&#35810;&#12290;</title><link>http://arxiv.org/abs/2204.11970</link><description>&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#22810;&#38454;&#27573;&#31995;&#32479;&#23545;&#30495;&#23454;&#24739;&#32773;&#25968;&#25454;&#36827;&#34892;&#35270;&#21147;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Visual Acuity Prediction on Real-Life Patient Data Using a Machine Learning Based Multistage System. (arXiv:2204.11970v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.11970
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#24320;&#21457;&#39044;&#27979;&#27169;&#22411;&#30340;&#22810;&#38454;&#27573;&#31995;&#32479;&#65292;&#21487;&#39640;&#31934;&#24230;&#39044;&#27979;&#19977;&#31181;&#30524;&#30142;&#24739;&#32773;&#30340;&#35270;&#21147;&#21464;&#21270;&#65292;&#24182;&#36741;&#21161;&#30524;&#31185;&#21307;&#29983;&#36827;&#34892;&#20020;&#24202;&#20915;&#31574;&#21644;&#24739;&#32773;&#21672;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#23454;&#29983;&#27963;&#20013;&#65292;&#30524;&#31185;&#23398;&#20013;&#30340;&#29627;&#29827;&#20307;&#25163;&#26415;&#33647;&#29289;&#27835;&#30103;&#26159;&#27835;&#30103;&#24180;&#40836;&#30456;&#20851;&#24615;&#40644;&#26001;&#21464;&#24615;&#65288;AMD&#65289;&#12289;&#31958;&#23615;&#30149;&#24615;&#40644;&#26001;&#27700;&#32959;&#65288;DME&#65289;&#21644;&#35270;&#32593;&#33180;&#38745;&#33033;&#38459;&#22622;&#65288;RVO&#65289;&#30456;&#20851;&#30142;&#30149;&#30340;&#19968;&#31181;&#26222;&#36941;&#27835;&#30103;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#24773;&#20917;&#19979;&#65292;&#30001;&#20110;&#25968;&#25454;&#30340;&#24322;&#36136;&#24615;&#21644;&#19981;&#23436;&#25972;&#24615;&#65292;&#24739;&#32773;&#24448;&#24448;&#20250;&#22312;&#22810;&#24180;&#26102;&#38388;&#20869;&#22833;&#21435;&#35270;&#21147;&#65292;&#23613;&#31649;&#25509;&#21463;&#27835;&#30103;&#12290;&#26412;&#25991;&#37319;&#29992;&#22810;&#31181;IT&#31995;&#32479;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#30740;&#31350;&#30340;&#25968;&#25454;&#38598;&#25104;&#27969;&#31243;&#65292;&#35813;&#27969;&#31243;&#34701;&#21512;&#20102;&#24503;&#22269;&#19968;&#23478;&#26368;&#20339;&#21307;&#30103;&#20445;&#20581;&#21307;&#38498;&#30340;&#30524;&#31185;&#37096;&#38376;&#30340;&#19981;&#21516;IT&#31995;&#32479;&#12290;&#32463;&#36807;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#24320;&#21457;&#39044;&#27979;&#27169;&#22411;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#23545;&#24739;&#32773;&#35270;&#21147;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#21487;&#20197;&#20026;&#19977;&#31181;&#30142;&#30149;&#30340;&#39044;&#27979;&#25552;&#20379;&#39640;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#31995;&#32479;&#21487;&#20197;&#20316;&#20026;&#24037;&#20855;&#65292;&#36741;&#21161;&#30524;&#31185;&#21307;&#29983;&#36827;&#34892;&#20020;&#24202;&#20915;&#31574;&#21644;&#24739;&#32773;&#21672;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;
In ophthalmology, intravitreal operative medication therapy (IVOM) is a widespread treatment for diseases related to the age-related macular degeneration (AMD), the diabetic macular edema (DME), as well as the retinal vein occlusion (RVO). However, in real-world settings, patients often suffer from loss of vision on time scales of years despite therapy, whereas the prediction of the visual acuity (VA) and the earliest possible detection of deterioration under real-life conditions is challenging due to heterogeneous and incomplete data. In this contribution, we present a workflow for the development of a research-compatible data corpus fusing different IT systems of the department of ophthalmology of a German maximum care hospital. The extensive data corpus allows predictive statements of the expected progression of a patient and his or her VA in each of the three diseases. We found out for the disease AMD a significant deterioration of the visual acuity over time. Within our proposed m
&lt;/p&gt;</description></item></channel></rss>