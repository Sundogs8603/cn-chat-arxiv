<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>Croissant&#26159;&#19968;&#31181;&#38754;&#21521;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#38598;&#30340;&#20803;&#25968;&#25454;&#26684;&#24335;&#65292;&#20351;&#25968;&#25454;&#38598;&#26356;&#26131;&#21457;&#29616;&#12289;&#21487;&#31227;&#26893;&#21644;&#20114;&#25805;&#20316;&#65292;&#26377;&#21161;&#20110;&#35299;&#20915;ML&#25968;&#25454;&#31649;&#29702;&#21644;&#36127;&#36131;&#20219;AI&#20013;&#30340;&#37325;&#35201;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.19546</link><description>&lt;p&gt;
Croissant&#65306;&#19968;&#31181;&#38754;&#21521;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#38598;&#30340;&#20803;&#25968;&#25454;&#26684;&#24335;
&lt;/p&gt;
&lt;p&gt;
Croissant: A Metadata Format for ML-Ready Datasets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19546
&lt;/p&gt;
&lt;p&gt;
Croissant&#26159;&#19968;&#31181;&#38754;&#21521;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#38598;&#30340;&#20803;&#25968;&#25454;&#26684;&#24335;&#65292;&#20351;&#25968;&#25454;&#38598;&#26356;&#26131;&#21457;&#29616;&#12289;&#21487;&#31227;&#26893;&#21644;&#20114;&#25805;&#20316;&#65292;&#26377;&#21161;&#20110;&#35299;&#20915;ML&#25968;&#25454;&#31649;&#29702;&#21644;&#36127;&#36131;&#20219;AI&#20013;&#30340;&#37325;&#35201;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#26159;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30340;&#20851;&#38190;&#36164;&#28304;&#65292;&#20294;&#22788;&#29702;&#25968;&#25454;&#20173;&#28982;&#26159;&#19968;&#20010;&#20027;&#35201;&#30340;&#25705;&#25830;&#28857;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;Croissant&#65292;&#19968;&#31181;&#29992;&#20110;&#25968;&#25454;&#38598;&#30340;&#20803;&#25968;&#25454;&#26684;&#24335;&#65292;&#31616;&#21270;&#20102;&#25968;&#25454;&#34987;ML&#24037;&#20855;&#21644;&#26694;&#26550;&#20351;&#29992;&#30340;&#26041;&#24335;&#12290;Croissant&#20351;&#25968;&#25454;&#38598;&#26356;&#26131;&#21457;&#29616;&#12289;&#21487;&#31227;&#26893;&#21644;&#20114;&#25805;&#20316;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;ML&#25968;&#25454;&#31649;&#29702;&#21644;&#36127;&#36131;&#20219;AI&#20013;&#30340;&#37325;&#35201;&#25361;&#25112;&#12290;Croissant&#24050;&#24471;&#21040;&#20960;&#20010;&#27969;&#34892;&#25968;&#25454;&#38598;&#24211;&#30340;&#25903;&#25345;&#65292;&#28085;&#30422;&#25968;&#21313;&#19975;&#20010;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#21152;&#36733;&#21040;&#26368;&#27969;&#34892;&#30340;ML&#26694;&#26550;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19546v1 Announce Type: cross  Abstract: Data is a critical resource for Machine Learning (ML), yet working with data remains a key friction point. This paper introduces Croissant, a metadata format for datasets that simplifies how data is used by ML tools and frameworks. Croissant makes datasets more discoverable, portable and interoperable, thereby addressing significant challenges in ML data management and responsible AI. Croissant is already supported by several popular dataset repositories, spanning hundreds of thousands of datasets, ready to be loaded into the most popular ML frameworks.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#25512;&#29702;&#21152;&#36895;&#26694;&#26550;&#65292;&#29992;&#20110;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#29702;&#36895;&#24230;&#65292;&#24182;&#22312;&#20445;&#25345;&#29983;&#25104;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#38477;&#20302;&#25104;&#26412;&#12290;&#35813;&#26694;&#26550;&#22312;&#25903;&#20184;&#23453;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#31995;&#32479;&#20013;&#24471;&#21040;&#20102;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2312.12728</link><description>&lt;p&gt;
Lookahead:&#19968;&#31181;&#29992;&#20110;&#20855;&#26377;&#26080;&#25439;&#29983;&#25104;&#20934;&#30830;&#24615;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#21152;&#36895;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Lookahead: An Inference Acceleration Framework for Large Language Model with Lossless Generation Accuracy. (arXiv:2312.12728v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.12728
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#25512;&#29702;&#21152;&#36895;&#26694;&#26550;&#65292;&#29992;&#20110;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#29702;&#36895;&#24230;&#65292;&#24182;&#22312;&#20445;&#25345;&#29983;&#25104;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#38477;&#20302;&#25104;&#26412;&#12290;&#35813;&#26694;&#26550;&#22312;&#25903;&#20184;&#23453;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#31995;&#32479;&#20013;&#24471;&#21040;&#20102;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#22914;&#38382;&#31572;&#12289;&#32763;&#35793;&#12289;&#25991;&#26412;&#25688;&#35201;&#21644;&#23545;&#35805;&#31995;&#32479;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#20687;&#25903;&#20184;&#23453;&#36825;&#26679;&#20026;&#25968;&#21313;&#20159;&#29992;&#25143;&#25552;&#20379;&#37325;&#35201;&#37329;&#34701;&#20135;&#21697;&#30340;&#38656;&#35201;&#20934;&#30830;&#20449;&#24687;&#30340;&#24773;&#20917;&#65292;&#20449;&#24687;&#30340;&#20934;&#30830;&#24615;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25903;&#20184;&#23453;&#24320;&#21457;&#20102;&#19968;&#31181;&#31216;&#20026;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#31995;&#32479;&#30340;&#26041;&#27861;&#65292;&#35813;&#31995;&#32479;&#23558;LLMs&#19982;&#26368;&#20934;&#30830;&#21644;&#26368;&#26032;&#30340;&#20449;&#24687;&#30456;&#32467;&#21512;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20026;&#25968;&#30334;&#19975;&#29992;&#25143;&#25552;&#20379;&#26381;&#21153;&#30340;&#30495;&#23454;&#20135;&#21697;&#26469;&#35828;&#65292;LLMs&#30340;&#25512;&#29702;&#36895;&#24230;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#22240;&#32032;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#19968;&#20010;&#23454;&#39564;&#24615;&#30340;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#25512;&#29702;&#21152;&#36895;&#26694;&#26550;&#65292;&#36890;&#36807;&#21152;&#36895;&#25512;&#29702;&#36807;&#31243;&#65292;&#23454;&#29616;&#20102;&#25105;&#20204;&#30340;RAG&#31995;&#32479;&#30340;&#36895;&#24230;&#22823;&#24133;&#25552;&#21319;&#21644;&#25104;&#26412;&#38477;&#20302;&#65292;&#21516;&#26102;&#20445;&#25345;&#30528;&#26080;&#25439;&#30340;&#29983;&#25104;&#20934;&#30830;&#24615;&#12290;&#22312;&#20256;&#32479;&#30340;&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#27599;&#20010;&#20196;&#29260;&#37117;&#30001;LLMs&#25353;&#39034;&#24207;&#29983;&#25104;&#65292;&#23548;&#33268;&#30340;&#26102;&#38388;&#28040;&#32791;&#19982;&#29983;&#25104;&#30340;&#20196;&#29260;&#25968;&#25104;&#27491;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
As Large Language Models (LLMs) have made significant advancements across various tasks, such as question answering, translation, text summarization, and dialogue systems, the need for accuracy in information becomes crucial, especially for serious financial products serving billions of users like Alipay. To address this, Alipay has developed a Retrieval-Augmented Generation (RAG) system that grounds LLMs on the most accurate and up-to-date information. However, for a real-world product serving millions of users, the inference speed of LLMs becomes a critical factor compared to a mere experimental model.  Hence, this paper presents a generic framework for accelerating the inference process, resulting in a substantial increase in speed and cost reduction for our RAG system, with lossless generation accuracy. In the traditional inference process, each token is generated sequentially by the LLM, leading to a time consumption proportional to the number of generated tokens. To enhance this 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#35780;&#20272;&#29616;&#26377;&#30340;&#36880;&#28857;&#12289;&#36880;&#23545;&#21644;&#21015;&#34920;&#25552;&#31034;&#26041;&#27861;&#65292;&#25581;&#31034;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#25490;&#21517;&#20219;&#21153;&#20013;&#30340;&#25928;&#26524;&#21644;&#25928;&#29575;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#21457;&#29616;&#36880;&#28857;&#26041;&#27861;&#30340;&#25928;&#29575;&#39640;&#20294;&#25928;&#26524;&#24046;&#65292;&#36880;&#23545;&#26041;&#27861;&#25928;&#26524;&#22909;&#20294;&#35745;&#31639;&#22797;&#26434;&#12290;&#20026;&#20102;&#25552;&#39640;&#25928;&#29575;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#21512;&#25552;&#31034;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.09497</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#25490;&#21517;&#30340;&#39640;&#25928;&#38598;&#21512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Setwise Approach for Effective and Highly Efficient Zero-shot Ranking with Large Language Models. (arXiv:2310.09497v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09497
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35780;&#20272;&#29616;&#26377;&#30340;&#36880;&#28857;&#12289;&#36880;&#23545;&#21644;&#21015;&#34920;&#25552;&#31034;&#26041;&#27861;&#65292;&#25581;&#31034;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#25490;&#21517;&#20219;&#21153;&#20013;&#30340;&#25928;&#26524;&#21644;&#25928;&#29575;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#21457;&#29616;&#36880;&#28857;&#26041;&#27861;&#30340;&#25928;&#29575;&#39640;&#20294;&#25928;&#26524;&#24046;&#65292;&#36880;&#23545;&#26041;&#27861;&#25928;&#26524;&#22909;&#20294;&#35745;&#31639;&#22797;&#26434;&#12290;&#20026;&#20102;&#25552;&#39640;&#25928;&#29575;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#21512;&#25552;&#31034;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#38646;&#26679;&#26412;&#25991;&#26723;&#25490;&#21517;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#24778;&#20154;&#30340;&#26377;&#25928;&#24615;&#12290;&#38024;&#23545;&#22522;&#20110;LLM&#30340;&#38646;&#26679;&#26412;&#25490;&#21517;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#36880;&#28857;&#65292;&#36880;&#23545;&#21644;&#21015;&#34920;&#25552;&#31034;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#39318;&#20808;&#22312;&#19968;&#20010;&#19968;&#33268;&#30340;&#23454;&#39564;&#26694;&#26550;&#20869;&#36827;&#34892;&#20102;&#23545;&#36825;&#20123;&#29616;&#26377;&#26041;&#27861;&#30340;&#24443;&#24213;&#35780;&#20272;&#65292;&#32771;&#34385;&#20102;&#27169;&#22411;&#22823;&#23567;&#65292;&#26631;&#35760;&#28040;&#32791;&#65292;&#24310;&#36831;&#31561;&#22240;&#32032;&#12290;&#36825;&#31181;&#39318;&#27425;&#30340;&#27604;&#36739;&#35780;&#20272;&#35753;&#25105;&#20204;&#33021;&#22815;&#30830;&#23450;&#27599;&#31181;&#26041;&#27861;&#22312;&#25928;&#26524;&#21644;&#25928;&#29575;&#20043;&#38388;&#22266;&#26377;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36880;&#28857;&#26041;&#27861;&#22312;&#25928;&#29575;&#19978;&#24471;&#20998;&#24456;&#39640;&#65292;&#20294;&#22312;&#26377;&#25928;&#24615;&#19978;&#23384;&#22312;&#38382;&#39064;&#12290;&#30456;&#21453;&#65292;&#36880;&#23545;&#26041;&#27861;&#34920;&#29616;&#20986;&#20248;&#36234;&#30340;&#26377;&#25928;&#24615;&#65292;&#20294;&#35745;&#31639;&#22797;&#26434;&#24230;&#36739;&#39640;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#22522;&#20110;LLM&#30340;&#38646;&#26679;&#26412;&#25490;&#21517;&#30340;&#25928;&#29575;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38598;&#21512;&#25552;&#31034;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20943;&#23569;&#20102;LLM&#25512;&#29702;&#30340;&#27425;&#25968;&#21644;&#25490;&#21517;&#36807;&#31243;&#20013;&#30340;&#25552;&#31034;&#26631;&#35760;&#28040;&#32791;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) demonstrate impressive effectiveness in zero-shot document ranking tasks. Pointwise, Pairwise, and Listwise prompting approaches have been proposed for LLM-based zero-shot ranking. Our study begins by thoroughly evaluating these existing approaches within a consistent experimental framework, considering factors like model size, token consumption, latency, among others. This first-of-its-kind comparative evaluation of these approaches allows us to identify the trade-offs between effectiveness and efficiency inherent in each approach. We find that while Pointwise approaches score high on efficiency, they suffer from poor effectiveness. Conversely, Pairwise approaches demonstrate superior effectiveness but incur high computational overhead. To further enhance the efficiency of LLM-based zero-shot ranking, we propose a novel Setwise prompting approach. Our approach reduces the number of LLM inferences and the amount of prompt token consumption during the rankin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#35821;&#20041;ID&#35299;&#20915;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#29289;&#21697;&#20919;&#21551;&#21160;&#38382;&#39064;&#65292;&#36825;&#20123;ID&#26159;&#20174;&#20869;&#23481;&#23884;&#20837;&#20013;&#23398;&#20064;&#30340;&#65292;&#21487;&#20197;&#25429;&#25417;&#27010;&#24565;&#30340;&#23618;&#27425;&#20851;&#31995;&#65292;&#30456;&#36739;&#20110;&#23436;&#20840;&#28040;&#38500;ID&#29305;&#24449;&#30340;&#26041;&#27861;&#65292;&#35821;&#20041;ID&#33021;&#26356;&#22909;&#22320;&#25552;&#39640;&#25512;&#33616;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2306.08121</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#20041;ID&#36827;&#34892;&#26356;&#22909;&#30340;&#27867;&#21270;&#65306;&#25512;&#33616;&#25490;&#21517;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Better Generalization with Semantic IDs: A case study in Ranking for Recommendations. (arXiv:2306.08121v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08121
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#35821;&#20041;ID&#35299;&#20915;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#29289;&#21697;&#20919;&#21551;&#21160;&#38382;&#39064;&#65292;&#36825;&#20123;ID&#26159;&#20174;&#20869;&#23481;&#23884;&#20837;&#20013;&#23398;&#20064;&#30340;&#65292;&#21487;&#20197;&#25429;&#25417;&#27010;&#24565;&#30340;&#23618;&#27425;&#20851;&#31995;&#65292;&#30456;&#36739;&#20110;&#23436;&#20840;&#28040;&#38500;ID&#29305;&#24449;&#30340;&#26041;&#27861;&#65292;&#35821;&#20041;ID&#33021;&#26356;&#22909;&#22320;&#25552;&#39640;&#25512;&#33616;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25512;&#33616;&#27169;&#22411;&#20013;&#65292;&#35757;&#32451;&#22909;&#30340;&#29289;&#21697;&#34920;&#31034;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#36890;&#24120;&#65292;&#19968;&#39033;&#21830;&#21697;&#20250;&#34987;&#20998;&#37197;&#19968;&#20010;&#21807;&#19968;&#30340;&#38543;&#26426;&#29983;&#25104;&#30340;ID&#65292;&#24182;&#19988;&#36890;&#24120;&#20250;&#36890;&#36807;&#23398;&#20064;&#19982;&#38543;&#26426;ID&#20540;&#30456;&#23545;&#24212;&#30340;&#23884;&#20837;&#26469;&#34920;&#31034;&#12290;&#34429;&#28982;&#36825;&#31181;&#26041;&#27861;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#22312;&#29289;&#21697;&#25968;&#37327;&#22823;&#19988;&#29289;&#21697;&#26381;&#20174;&#24130;&#24459;&#20998;&#24067;&#30340;&#24773;&#20917;&#19979;&#8212;&#8212;&#36825;&#26159;&#30495;&#23454;&#19990;&#30028;&#25512;&#33616;&#31995;&#32479;&#30340;&#20856;&#22411;&#29305;&#24449;&#8212;&#8212;&#20250;&#26377;&#19968;&#23450;&#23616;&#38480;&#24615;&#12290;&#36825;&#20250;&#23548;&#33268;&#29289;&#21697;&#20919;&#21551;&#21160;&#38382;&#39064;&#65292;&#27169;&#22411;&#26080;&#27861;&#23545;&#23614;&#37096;&#21644;&#20197;&#21069;&#26410;&#35265;&#36807;&#30340;&#29289;&#21697;&#36827;&#34892;&#21487;&#38752;&#30340;&#25512;&#33616;&#12290;&#23436;&#20840;&#28040;&#38500;&#36825;&#20123;ID&#29305;&#24449;&#21450;&#20854;&#23398;&#20064;&#30340;&#23884;&#20837;&#20197;&#35299;&#20915;&#20919;&#21551;&#21160;&#38382;&#39064;&#20250;&#20005;&#37325;&#38477;&#20302;&#25512;&#33616;&#36136;&#37327;&#12290;&#22522;&#20110;&#20869;&#23481;&#30340;&#29289;&#21697;&#23884;&#20837;&#26356;&#20026;&#21487;&#38752;&#65292;&#20294;&#23545;&#20110;&#29992;&#25143;&#36807;&#21435;&#30340;&#29289;&#21697;&#20132;&#20114;&#24207;&#21015;&#26469;&#35828;&#65292;&#23427;&#20204;&#25104;&#26412;&#39640;&#19988;&#20351;&#29992;&#22256;&#38590;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#35821;&#20041;ID&#26469;&#34920;&#31034;&#31163;&#25955;&#30340;&#29289;&#21697;&#65292;&#36825;&#20123;ID&#26159;&#36890;&#36807;&#20351;&#29992;RQ-VAE&#20174;&#20869;&#23481;&#23884;&#20837;&#20013;&#23398;&#20064;&#30340;&#65292;&#21487;&#20197;&#25429;&#25417;&#27010;&#24565;&#30340;&#23618;&#27425;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training good representations for items is critical in recommender models. Typically, an item is assigned a unique randomly generated ID, and is commonly represented by learning an embedding corresponding to the value of the random ID. Although widely used, this approach have limitations when the number of items are large and items are power-law distributed -- typical characteristics of real-world recommendation systems. This leads to the item cold-start problem, where the model is unable to make reliable inferences for tail and previously unseen items. Removing these ID features and their learned embeddings altogether to combat cold-start issue severely degrades the recommendation quality. Content-based item embeddings are more reliable, but they are expensive to store and use, particularly for users' past item interaction sequence. In this paper, we use Semantic IDs, a compact discrete item representations learned from content embeddings using RQ-VAE that captures hierarchy of concep
&lt;/p&gt;</description></item></channel></rss>