<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#24322;&#36136;&#22270;&#19978;&#30340;&#38142;&#36335;&#39044;&#27979;&#38754;&#20020;&#23398;&#20064;&#33021;&#21147;&#21644;&#34920;&#36798;&#33021;&#21147;&#26041;&#38754;&#30340;&#25361;&#25112;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#21463;&#29289;&#29702;&#21551;&#21457;&#30340;&#26041;&#27861;&#20197;&#22686;&#24378;&#33410;&#28857;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.14802</link><description>&lt;p&gt;
&#22312;&#24322;&#36136;&#24615;&#19979;&#30340;&#38142;&#36335;&#39044;&#27979;: &#21463;&#29289;&#29702;&#21551;&#21457;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Link Prediction under Heterophily: A Physics-Inspired Graph Neural Network Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14802
&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#24322;&#36136;&#22270;&#19978;&#30340;&#38142;&#36335;&#39044;&#27979;&#38754;&#20020;&#23398;&#20064;&#33021;&#21147;&#21644;&#34920;&#36798;&#33021;&#21147;&#26041;&#38754;&#30340;&#25361;&#25112;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#21463;&#29289;&#29702;&#21551;&#21457;&#30340;&#26041;&#27861;&#20197;&#22686;&#24378;&#33410;&#28857;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#30001;&#20110;&#20854;&#22312;&#23545;&#22270;&#34920;&#31034;&#30340;&#30495;&#23454;&#19990;&#30028;&#29616;&#35937;&#24314;&#27169;&#26041;&#38754;&#30340;&#28789;&#27963;&#24615;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#24050;&#25104;&#20026;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#30340;&#20107;&#23454;&#26631;&#20934;&#12290;&#28982;&#32780;&#65292;GNNs&#30340;&#28040;&#24687;&#20256;&#36882;&#26426;&#21046;&#22312;&#23398;&#20064;&#33021;&#21147;&#21644;&#34920;&#36798;&#33021;&#21147;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#65292;&#36825;&#38480;&#21046;&#20102;&#22312;&#24322;&#36136;&#22270;&#19978;&#23454;&#29616;&#39640;&#24615;&#33021;&#30340;&#33021;&#21147;&#65292;&#20854;&#20013;&#30456;&#37051;&#33410;&#28857;&#32463;&#24120;&#20855;&#26377;&#19981;&#21516;&#30340;&#26631;&#31614;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#20027;&#35201;&#23616;&#38480;&#20110;&#38024;&#23545;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#30340;&#29305;&#23450;&#22522;&#20934;&#12290;&#36825;&#31181;&#29421;&#31364;&#30340;&#28966;&#28857;&#38480;&#21046;&#20102;&#38142;&#36335;&#39044;&#27979;&#22312;&#22810;&#20010;&#24212;&#29992;&#20013;&#30340;&#28508;&#22312;&#24433;&#21709;&#65292;&#21253;&#25324;&#25512;&#33616;&#31995;&#32479;&#12290;&#20363;&#22914;&#65292;&#22312;&#31038;&#20132;&#32593;&#32476;&#20013;&#65292;&#20004;&#20010;&#29992;&#25143;&#21487;&#33021;&#30001;&#20110;&#26576;&#31181;&#28508;&#22312;&#21407;&#22240;&#32780;&#36830;&#25509;&#65292;&#36825;&#20351;&#24471;&#25552;&#21069;&#39044;&#27979;&#36825;&#31181;&#36830;&#25509;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#21463;&#29289;&#29702;&#21551;&#21457;&#30340;GNNs&#65288;&#22914;GRAFF&#65289;&#23545;&#25552;&#39640;&#33410;&#28857;&#20998;&#31867;&#24615;&#33021;&#25552;&#20379;&#20102;&#26174;&#33879;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14802v1 Announce Type: new  Abstract: In the past years, Graph Neural Networks (GNNs) have become the `de facto' standard in various deep learning domains, thanks to their flexibility in modeling real-world phenomena represented as graphs. However, the message-passing mechanism of GNNs faces challenges in learnability and expressivity, hindering high performance on heterophilic graphs, where adjacent nodes frequently have different labels. Most existing solutions addressing these challenges are primarily confined to specific benchmarks focused on node classification tasks. This narrow focus restricts the potential impact that link prediction under heterophily could offer in several applications, including recommender systems. For example, in social networks, two users may be connected for some latent reason, making it challenging to predict such connections in advance. Physics-Inspired GNNs such as GRAFF provided a significant contribution to enhance node classification perf
&lt;/p&gt;</description></item><item><title>&#21457;&#24067;&#20102;IEPile&#65292;&#19968;&#20010;&#21253;&#21547;&#32422;0.32B&#20010;&#26631;&#35760;&#30340;&#32508;&#21512;&#21452;&#35821;IE&#25351;&#20196;&#35821;&#26009;&#24211;&#65292;&#36890;&#36807;&#25910;&#38598;&#21644;&#28165;&#29702;33&#20010;&#29616;&#26377;IE&#25968;&#25454;&#38598;&#24182;&#24341;&#20837;&#22522;&#20110;&#27169;&#24335;&#30340;&#25351;&#20196;&#29983;&#25104;&#65292;&#21487;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20449;&#24687;&#25277;&#21462;&#39046;&#22495;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#38646;&#26679;&#26412;&#27867;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.14710</link><description>&lt;p&gt;
IEPile: &#25366;&#25496;&#22823;&#35268;&#27169;&#22522;&#20110;&#27169;&#24335;&#30340;&#20449;&#24687;&#25277;&#21462;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
IEPile: Unearthing Large-Scale Schema-Based Information Extraction Corpus
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14710
&lt;/p&gt;
&lt;p&gt;
&#21457;&#24067;&#20102;IEPile&#65292;&#19968;&#20010;&#21253;&#21547;&#32422;0.32B&#20010;&#26631;&#35760;&#30340;&#32508;&#21512;&#21452;&#35821;IE&#25351;&#20196;&#35821;&#26009;&#24211;&#65292;&#36890;&#36807;&#25910;&#38598;&#21644;&#28165;&#29702;33&#20010;&#29616;&#26377;IE&#25968;&#25454;&#38598;&#24182;&#24341;&#20837;&#22522;&#20110;&#27169;&#24335;&#30340;&#25351;&#20196;&#29983;&#25104;&#65292;&#21487;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20449;&#24687;&#25277;&#21462;&#39046;&#22495;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#38646;&#26679;&#26412;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#23637;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#28508;&#21147;&#65307;&#28982;&#32780;&#65292;&#22312;&#20449;&#24687;&#25277;&#21462;&#65288;IE&#65289;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#25968;&#25454;&#26159;&#25552;&#21319;LLMs&#29305;&#23450;&#33021;&#21147;&#30340;&#20851;&#38190;&#65292;&#32780;&#24403;&#21069;&#30340;IE&#25968;&#25454;&#38598;&#24448;&#24448;&#35268;&#27169;&#36739;&#23567;&#12289;&#20998;&#25955;&#19988;&#32570;&#20047;&#26631;&#20934;&#21270;&#30340;&#27169;&#24335;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;IEPile&#65292;&#19968;&#20010;&#32508;&#21512;&#30340;&#21452;&#35821;&#65288;&#33521;&#25991;&#21644;&#20013;&#25991;&#65289;IE&#25351;&#20196;&#35821;&#26009;&#24211;&#65292;&#21253;&#21547;&#32422;0.32B&#20010;&#26631;&#35760;&#12290;&#25105;&#20204;&#36890;&#36807;&#25910;&#38598;&#21644;&#28165;&#29702;33&#20010;&#29616;&#26377;IE&#25968;&#25454;&#38598;&#26500;&#24314;IEPile&#65292;&#24182;&#24341;&#20837;&#22522;&#20110;&#27169;&#24335;&#30340;&#25351;&#20196;&#29983;&#25104;&#26469;&#25366;&#25496;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;&#12290;&#22312;LLaMA&#21644;Baichuan&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;IEPile&#21487;&#20197;&#25552;&#39640;LLMs&#22312;IE&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#38646;&#26679;&#26412;&#27867;&#21270;&#12290;&#25105;&#20204;&#24320;&#28304;&#20102;&#36164;&#28304;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24076;&#26395;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31038;&#21306;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14710v1 Announce Type: cross  Abstract: Large Language Models (LLMs) demonstrate remarkable potential across various domains; however, they exhibit a significant performance gap in Information Extraction (IE). Note that high-quality instruction data is the vital key for enhancing the specific capabilities of LLMs, while current IE datasets tend to be small in scale, fragmented, and lack standardized schema. To this end, we introduce IEPile, a comprehensive bilingual (English and Chinese) IE instruction corpus, which contains approximately 0.32B tokens. We construct IEPile by collecting and cleaning 33 existing IE datasets, and introduce schema-based instruction generation to unearth a large-scale corpus. Experimental results on LLaMA and Baichuan demonstrate that using IEPile can enhance the performance of LLMs for IE, especially the zero-shot generalization. We open-source the resource and pre-trained models, hoping to provide valuable support to the NLP community.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#31361;&#20986;&#20102;&#20449;&#24687;&#26816;&#32034;&#24341;&#25806;&#22312;&#31185;&#23398;&#30028;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#32467;&#26500;&#21270;&#35760;&#24405;&#21644;&#20808;&#36827;&#20449;&#24687;&#25216;&#26415;&#24037;&#20855;&#23454;&#29616;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#38761;&#26032;&#30740;&#31350;&#20154;&#21592;&#35775;&#38382;&#21644;&#36807;&#28388;&#25991;&#31456;&#30340;&#26041;&#24335;&#12290;</title><link>https://arxiv.org/abs/2402.14622</link><description>&lt;p&gt;
&#20174;&#20851;&#38190;&#35789;&#21040;&#32467;&#26500;&#21270;&#25688;&#35201;: &#31934;&#31616;&#23398;&#26415;&#30693;&#35782;&#33719;&#21462;
&lt;/p&gt;
&lt;p&gt;
From Keywords to Structured Summaries: Streamlining Scholarly Knowledge Access
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14622
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#31361;&#20986;&#20102;&#20449;&#24687;&#26816;&#32034;&#24341;&#25806;&#22312;&#31185;&#23398;&#30028;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#32467;&#26500;&#21270;&#35760;&#24405;&#21644;&#20808;&#36827;&#20449;&#24687;&#25216;&#26415;&#24037;&#20855;&#23454;&#29616;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#38761;&#26032;&#30740;&#31350;&#20154;&#21592;&#35775;&#38382;&#21644;&#36807;&#28388;&#25991;&#31456;&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#30701;&#25991;&#24378;&#35843;&#20102;&#20449;&#24687;&#26816;&#32034;&#24341;&#25806;&#22312;&#31185;&#23398;&#30028;&#26085;&#30410;&#37325;&#35201;&#65292;&#25351;&#20986;&#20256;&#32479;&#22522;&#20110;&#20851;&#38190;&#35789;&#30340;&#25628;&#32034;&#24341;&#25806;&#30001;&#20110;&#20986;&#29256;&#29289;&#25968;&#37327;&#19981;&#26029;&#22686;&#21152;&#32780;&#25928;&#29575;&#20302;&#19979;&#12290;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#28041;&#21450;&#32467;&#26500;&#21270;&#35760;&#24405;&#65292;&#25903;&#25345;&#20808;&#36827;&#30340;&#20449;&#24687;&#25216;&#26415;&#24037;&#20855;&#65292;&#21253;&#25324;&#21487;&#35270;&#21270;&#20202;&#34920;&#26495;&#65292;&#20197;&#24443;&#24213;&#25913;&#21464;&#30740;&#31350;&#20154;&#21592;&#22914;&#20309;&#35775;&#38382;&#21644;&#36807;&#28388;&#25991;&#31456;&#65292;&#21462;&#20195;&#20256;&#32479;&#30340;&#25991;&#26412;&#23494;&#38598;&#22411;&#26041;&#27861;&#12290;&#36825;&#19968;&#24895;&#26223;&#36890;&#36807;&#19968;&#20010;&#20197;&#8220;&#20256;&#26579;&#30149;&#30340;&#32321;&#27542;&#25968;&#20272;&#35745;&#8221;&#30740;&#31350;&#20027;&#39064;&#20026;&#20013;&#24515;&#30340;&#27010;&#24565;&#39564;&#35777;&#24471;&#20197;&#20307;&#29616;&#65292;&#20351;&#29992;&#32463;&#36807;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#33258;&#21160;&#21019;&#24314;&#32467;&#26500;&#21270;&#35760;&#24405;&#20197;&#22635;&#20805;&#19968;&#20010;&#36229;&#36234;&#20851;&#38190;&#35789;&#30340;&#21518;&#31471;&#25968;&#25454;&#24211;&#12290;&#32467;&#26524;&#26159;&#19968;&#20010;&#19979;&#19968;&#20195;&#20449;&#24687;&#26816;&#32034;&#26041;&#27861;&#65292;&#21487;&#22312;https://orkg.org/usecases/r0-estimates &#19978;&#35775;&#38382;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14622v1 Announce Type: cross  Abstract: This short paper highlights the growing importance of information retrieval (IR) engines in the scientific community, addressing the inefficiency of traditional keyword-based search engines due to the rising volume of publications. The proposed solution involves structured records, underpinning advanced information technology (IT) tools, including visualization dashboards, to revolutionize how researchers access and filter articles, replacing the traditional text-heavy approach. This vision is exemplified through a proof of concept centered on the ``reproductive number estimate of infectious diseases'' research theme, using a fine-tuned large language model (LLM) to automate the creation of structured records to populate a backend database that now goes beyond keywords. The result is a next-generation IR method accessible at https://orkg.org/usecases/r0-estimates.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#29983;&#25104;&#39044;&#35757;&#32451;&#21464;&#25442;&#22120;&#26469;&#33258;&#21160;&#35780;&#20272;&#36741;&#23548;&#21592;&#20351;&#29992;&#31038;&#20132;&#24773;&#24863;&#36741;&#23548;&#31574;&#30053;&#30340;&#33021;&#21147;&#65292;&#20174;&#32780;&#25913;&#36827;&#36741;&#23548;&#23454;&#36341;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2402.14594</link><description>&lt;p&gt;
&#21033;&#29992;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#25913;&#36827;&#36741;&#23548;&#23454;&#36341;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Improving Assessment of Tutoring Practices using Retrieval-Augmented Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14594
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#29983;&#25104;&#39044;&#35757;&#32451;&#21464;&#25442;&#22120;&#26469;&#33258;&#21160;&#35780;&#20272;&#36741;&#23548;&#21592;&#20351;&#29992;&#31038;&#20132;&#24773;&#24863;&#36741;&#23548;&#31574;&#30053;&#30340;&#33021;&#21147;&#65292;&#20174;&#32780;&#25913;&#36827;&#36741;&#23548;&#23454;&#36341;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#23545;&#19968;&#36741;&#23548;&#26159;&#25552;&#39640;&#23398;&#20064;&#25928;&#26524;&#30340;&#26377;&#25928;&#25945;&#23398;&#26041;&#27861;&#65292;&#28982;&#32780;&#20854;&#25928;&#21147;&#21462;&#20915;&#20110;&#36741;&#23548;&#21592;&#30340;&#33021;&#21147;&#12290;&#26032;&#25163;&#25968;&#23398;&#36741;&#23548;&#21592;&#36890;&#24120;&#20248;&#20808;&#32771;&#34385;&#29305;&#23450;&#20869;&#23481;&#30340;&#25351;&#23548;&#65292;&#24573;&#35270;&#31038;&#20132;&#24773;&#24863;&#23398;&#20064;&#31561;&#26041;&#38754;&#12290;&#31038;&#20132;&#24773;&#24863;&#23398;&#20064;&#20419;&#36827;&#20102;&#20844;&#24179;&#21644;&#21253;&#23481;&#24615;&#65292;&#24182;&#22521;&#20859;&#19982;&#23398;&#29983;&#30340;&#20851;&#31995;&#65292;&#36825;&#23545;&#20110;&#23398;&#29983;&#25972;&#20307;&#21457;&#23637;&#33267;&#20851;&#37325;&#35201;&#12290;&#20934;&#30830;&#26377;&#25928;&#22320;&#35780;&#20272;&#36741;&#23548;&#21592;&#30340;&#33021;&#21147;&#21487;&#20197;&#25512;&#21160;&#23450;&#21046;&#30340;&#36741;&#23548;&#21592;&#22521;&#35757;&#35745;&#21010;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#26102;&#36741;&#23548;&#20013;&#35780;&#20272;&#26032;&#25163;&#36741;&#23548;&#21592;&#30340;&#33021;&#21147;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#36890;&#24120;&#38656;&#35201;&#19987;&#23478;&#21442;&#19982;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#21021;&#27493;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#29983;&#25104;&#39044;&#35757;&#32451;&#21464;&#25442;&#22120;&#65288;GPT&#65289;&#65292;&#22914;GPT-3.5&#21644;GPT-4&#27169;&#22411;&#65292;&#33258;&#21160;&#35780;&#20272;&#36741;&#23548;&#21592;&#20351;&#29992;&#31038;&#20132;&#24773;&#24863;&#36741;&#23548;&#31574;&#30053;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#26412;&#30740;&#31350;&#36824;&#25253;&#21578;&#20102;&#36130;&#21153;&#32500;&#24230;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14594v1 Announce Type: cross  Abstract: One-on-one tutoring is an effective instructional method for enhancing learning, yet its efficacy hinges on tutor competencies. Novice math tutors often prioritize content-specific guidance, neglecting aspects such as social-emotional learning. Social-emotional learning promotes equity and inclusion and nurturing relationships with students, which is crucial for holistic student development. Assessing the competencies of tutors accurately and efficiently can drive the development of tailored tutor training programs. However, evaluating novice tutor ability during real-time tutoring remains challenging as it typically requires experts-in-the-loop. To address this challenge, this preliminary study aims to harness Generative Pre-trained Transformers (GPT), such as GPT-3.5 and GPT-4 models, to automatically assess tutors' ability of using social-emotional tutoring strategies. Moreover, this study also reports on the financial dimensions an
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22312;Google&#24191;&#21578;&#20013;&#36827;&#34892;&#20869;&#23481;&#31649;&#29702;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;LLMs&#23457;&#26680;&#20195;&#34920;&#24615;&#24191;&#21578;&#24182;&#23558;&#20915;&#31574;&#20256;&#25773;&#22238;&#20854;&#32676;&#38598;&#65292;&#23558;&#23457;&#26680;&#25968;&#30446;&#20943;&#23569;&#20102;3&#20010;&#25968;&#37327;&#32423;&#20197;&#19978;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;2&#20493;&#30340;&#21484;&#22238;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.14590</link><description>&lt;p&gt;
&#25193;&#23637;LLM&#23457;&#26680;&#20197;&#36827;&#34892;Google&#24191;&#21578;&#20869;&#23481;&#31649;&#29702;
&lt;/p&gt;
&lt;p&gt;
Scaling Up LLM Reviews for Google Ads Content Moderation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14590
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22312;Google&#24191;&#21578;&#20013;&#36827;&#34892;&#20869;&#23481;&#31649;&#29702;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;LLMs&#23457;&#26680;&#20195;&#34920;&#24615;&#24191;&#21578;&#24182;&#23558;&#20915;&#31574;&#20256;&#25773;&#22238;&#20854;&#32676;&#38598;&#65292;&#23558;&#23457;&#26680;&#25968;&#30446;&#20943;&#23569;&#20102;3&#20010;&#25968;&#37327;&#32423;&#20197;&#19978;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;2&#20493;&#30340;&#21484;&#22238;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#20869;&#23481;&#31649;&#29702;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#20294;&#23427;&#20204;&#30340;&#25512;&#29702;&#25104;&#26412;&#21644;&#24310;&#36831;&#20351;&#23427;&#20204;&#22312;&#22823;&#22411;&#25968;&#25454;&#38598;&#65288;&#22914;Google Ads&#23384;&#20648;&#24211;&#65289;&#19978;&#30340;&#20020;&#26102;&#20351;&#29992;&#25104;&#26412;&#36807;&#39640;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#25193;&#23637;LLM&#23457;&#26680;&#20197;&#22312;Google Ads&#20013;&#36827;&#34892;&#20869;&#23481;&#31649;&#29702;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#21551;&#21457;&#24335;&#26041;&#27861;&#36890;&#36807;&#36807;&#28388;&#21644;&#37325;&#22797;&#39033;&#21024;&#38500;&#26469;&#36873;&#25321;&#20505;&#36873;&#39033;&#65292;&#24182;&#20026;&#27492;&#21019;&#24314;&#24191;&#21578;&#32676;&#38598;&#65292;&#25105;&#20204;&#36873;&#25321;&#27599;&#20010;&#32676;&#38598;&#30340;&#19968;&#20010;&#20195;&#34920;&#24615;&#24191;&#21578;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;LLMs&#20165;&#23457;&#26680;&#20195;&#34920;&#24615;&#24191;&#21578;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;&#20195;&#34920;&#24615;&#24191;&#21578;&#30340;LLM&#20915;&#31574;&#20256;&#25773;&#22238;&#23427;&#20204;&#30340;&#32676;&#38598;&#12290;&#35813;&#26041;&#27861;&#23558;&#23457;&#26680;&#25968;&#30446;&#20943;&#23569;&#20102;3&#20010;&#25968;&#37327;&#32423;&#20197;&#19978;&#65292;&#21516;&#26102;&#19982;&#22522;&#32447;&#38750;LLM&#27169;&#22411;&#30456;&#27604;&#23454;&#29616;&#20102;2&#20493;&#30340;&#21484;&#22238;&#29575;&#12290;&#35813;&#26041;&#27861;&#30340;&#25104;&#21151;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#32858;&#31867;&#21644;&#26631;&#31614;&#20256;&#25773;&#20013;&#20351;&#29992;&#30340;&#34920;&#31034;; &#25105;&#20204;&#21457;&#29616;&#20132;&#21449;&#27169;&#24577;&#30456;&#20284;&#24615;&#34920;&#31034;&#20135;&#29983;&#27604;&#21333;&#19968;&#27169;&#24577;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14590v1 Announce Type: cross  Abstract: Large language models (LLMs) are powerful tools for content moderation, but their inference costs and latency make them prohibitive for casual use on large datasets, such as the Google Ads repository. This study proposes a method for scaling up LLM reviews for content moderation in Google Ads. First, we use heuristics to select candidates via filtering and duplicate removal, and create clusters of ads for which we select one representative ad per cluster. We then use LLMs to review only the representative ads. Finally, we propagate the LLM decisions for the representative ads back to their clusters. This method reduces the number of reviews by more than 3 orders of magnitude while achieving a 2x recall compared to a baseline non-LLM model. The success of this approach is a strong function of the representations used in clustering and label propagation; we found that cross-modal similarity representations yield better results than uni-m
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20171;&#32461;&#20102;&#23558;&#22522;&#20110;&#35268;&#33539;&#30340;&#31354;&#38388;&#34920;&#31034;&#36716;&#25442;&#20026;&#22522;&#20110;&#22270;&#30340;&#26102;&#31354;&#27969;&#34892;&#30149;&#23398;&#27169;&#22411;&#30340;&#26032;&#26694;&#26550;&#21644;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.14539</link><description>&lt;p&gt;
&#23558;&#22522;&#20110;&#35268;&#33539;&#30340;&#31354;&#38388;&#34920;&#31034;&#36716;&#25442;&#20026;&#22522;&#20110;&#22270;&#30340;&#26102;&#31354;&#27969;&#34892;&#30149;&#23398;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Transforming Norm-based To Graph-based Spatial Representation for Spatio-Temporal Epidemiological Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14539
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20171;&#32461;&#20102;&#23558;&#22522;&#20110;&#35268;&#33539;&#30340;&#31354;&#38388;&#34920;&#31034;&#36716;&#25442;&#20026;&#22522;&#20110;&#22270;&#30340;&#26102;&#31354;&#27969;&#34892;&#30149;&#23398;&#27169;&#22411;&#30340;&#26032;&#26694;&#26550;&#21644;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30123;&#24773;&#22823;&#27969;&#34892;&#20197;&#20854;&#28145;&#36828;&#30340;&#31038;&#20250;&#21644;&#32463;&#27982;&#24433;&#21709;&#65292;&#23545;&#20840;&#29699;&#20581;&#24247;&#12289;&#27515;&#20129;&#29575;&#12289;&#32463;&#27982;&#31283;&#23450;&#24615;&#21644;&#25919;&#27835;&#26684;&#23616;&#26500;&#25104;&#37325;&#22823;&#23041;&#32961;&#12290;&#20026;&#20102;&#24212;&#23545;&#26032;&#20852;&#21644;&#20877;&#29616;&#30123;&#24773;&#24102;&#26469;&#30340;&#25345;&#32493;&#25361;&#25112;&#65292;&#35768;&#22810;&#30740;&#31350;&#37319;&#29992;&#26102;&#31354;&#27169;&#22411;&#26469;&#22686;&#24378;&#25105;&#20204;&#23545;&#36825;&#20123;&#22797;&#26434;&#29616;&#35937;&#30340;&#29702;&#35299;&#21644;&#31649;&#29702;&#12290;&#36825;&#20123;&#26102;&#31354;&#27169;&#22411;&#21487;&#20197;&#22823;&#33268;&#20998;&#20026;&#20004;&#31181;&#20027;&#35201;&#31354;&#38388;&#31867;&#21035;&#65306;&#22522;&#20110;&#35268;&#33539;&#21644;&#22522;&#20110;&#22270;&#30340;&#27169;&#22411;&#65292;&#22312;&#20934;&#30830;&#24615;&#12289;&#35745;&#31639;&#36127;&#25285;&#21644;&#21487;&#34920;&#31034;&#24615;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#23558;&#36825;&#20123;&#27169;&#22411;&#20174;&#22522;&#20110;&#35268;&#33539;&#36716;&#25442;&#20026;&#22522;&#20110;&#22270;&#30340;&#31354;&#38388;&#34920;&#31034;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#20197;&#21450;&#20351;&#29992;&#24191;&#27867;&#30340;&#21551;&#21457;&#24335;&#20248;&#21270;&#26041;&#27861;&#30340;&#21313;&#20108;&#31181;&#21487;&#33021;&#23454;&#29616;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#36890;&#36807;&#21033;&#29992;&#22522;&#20110;&#20195;&#29702;&#30340;&#27169;&#25311;&#21644;&#21551;&#21457;&#24335;&#31639;&#27861;&#26469;&#23454;&#29616;&#22270;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14539v1 Announce Type: new  Abstract: Pandemics, with their profound societal and economic impacts, pose significant threats to global health, mortality rates, economic stability, and political landscapes. In response to the persistent challenges posed by emerging and reemerging pandemics, numerous studies have employed spatio-temporal models to enhance our understanding and management of these complex phenomena. These spatio-temporal models can be roughly divided into two main spatial categories: norm-based and graph-based trade-offering between accuracy, computational burden, and representational feasibility. In this study, we explore the ability to transform from norm-based to graph-based spatial representation for these models. We introduce a novel framework for this task together with twelve possible implementations using a wide range of heuristic optimization approaches. Our findings show that by leveraging agent-based simulations and heuristic algorithms for the graph
&lt;/p&gt;</description></item><item><title>&#20010;&#24615;&#21270;&#34892;&#20026;&#24863;&#30693;Transformer&#26694;&#26550;&#29992;&#20110;&#22810;&#34892;&#20026;&#39034;&#24207;&#25512;&#33616;&#65292;&#26088;&#22312;&#26356;&#22909;&#22320;&#25506;&#32034;&#29992;&#25143;&#30340;&#28508;&#22312;&#24847;&#22270;&#65292;&#24182;&#35299;&#20915;&#30701;&#24207;&#21015;&#19979;&#25512;&#33616;&#24615;&#33021;&#38477;&#20302;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.14473</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#34892;&#20026;&#24863;&#30693;Transformer&#29992;&#20110;&#22810;&#34892;&#20026;&#39034;&#24207;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Personalized Behavior-Aware Transformer for Multi-Behavior Sequential Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14473
&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#34892;&#20026;&#24863;&#30693;Transformer&#26694;&#26550;&#29992;&#20110;&#22810;&#34892;&#20026;&#39034;&#24207;&#25512;&#33616;&#65292;&#26088;&#22312;&#26356;&#22909;&#22320;&#25506;&#32034;&#29992;&#25143;&#30340;&#28508;&#22312;&#24847;&#22270;&#65292;&#24182;&#35299;&#20915;&#30701;&#24207;&#21015;&#19979;&#25512;&#33616;&#24615;&#33021;&#38477;&#20302;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Sequential Recommendation (SR)&#36890;&#36807;&#24314;&#27169;&#29992;&#25143;&#22312;&#29289;&#21697;&#20043;&#38388;&#36716;&#25442;&#30340;&#26041;&#24335;&#26469;&#25429;&#25417;&#29992;&#25143;&#30340;&#21160;&#24577;&#20559;&#22909;&#12290;&#28982;&#32780;&#65292;&#20165;&#21033;&#29992;&#21333;&#19968;&#31867;&#22411;&#30340;&#34892;&#20026;&#20132;&#20114;&#25968;&#25454;&#30340;SR&#27169;&#22411;&#22312;&#24207;&#21015;&#36739;&#30701;&#26102;&#24615;&#33021;&#20250;&#19979;&#38477;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#20851;&#27880;&#22810;&#34892;&#20026;&#39034;&#24207;&#25512;&#33616;(MBSR)&#65292;&#26088;&#22312;&#21033;&#29992;&#26102;&#21464;&#24322;&#26500;&#34892;&#20026;&#20381;&#36182;&#20851;&#31995;&#26356;&#22909;&#22320;&#25506;&#32034;&#29992;&#25143;&#22312;&#30446;&#26631;&#34892;&#20026;&#19978;&#30340;&#28508;&#22312;&#24847;&#22270;&#12290;&#35299;&#20915;MBSR&#38382;&#39064;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#19968;&#26041;&#38754;&#65292;&#30001;&#20110;&#20010;&#20154;&#29305;&#24449;&#65292;&#29992;&#25143;&#23637;&#29616;&#20986;&#22810;&#26679;&#21270;&#30340;&#22810;&#34892;&#20026;&#27169;&#24335;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#34892;&#20026;&#30456;&#20851;&#24615;&#21644;&#29289;&#21697;&#21327;&#20316;&#20043;&#38388;&#23384;&#22312;&#20840;&#38754;&#30340;&#30456;&#20114;&#24433;&#21709;&#65292;&#20854;&#24378;&#24230;&#28145;&#21463;&#26102;&#38388;&#22240;&#32032;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;MBSR&#38382;&#39064;&#30340;Personalized Behavior-Aware Transformer&#26694;&#26550;(PBAT)&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#24314;&#27169;&#20010;&#24615;&#21270;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14473v1 Announce Type: cross  Abstract: Sequential Recommendation (SR) captures users' dynamic preferences by modeling how users transit among items. However, SR models that utilize only single type of behavior interaction data encounter performance degradation when the sequences are short. To tackle this problem, we focus on Multi-Behavior Sequential Recommendation (MBSR) in this paper, which aims to leverage time-evolving heterogeneous behavioral dependencies for better exploring users' potential intents on the target behavior. Solving MBSR is challenging. On the one hand, users exhibit diverse multi-behavior patterns due to personal characteristics. On the other hand, there exists comprehensive co-influence between behavior correlations and item collaborations, the intensity of which is deeply affected by temporal factors. To tackle these challenges, we propose a Personalized Behavior-Aware Transformer framework (PBAT) for MBSR problem, which models personalized patterns 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#39135;&#21697;&#37197;&#36865;&#25512;&#33616;&#20013;&#30340;&#37325;&#22797;&#21644;&#25506;&#32034;&#28040;&#36153;&#38382;&#39064;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#21457;&#29616;&#29616;&#26377;&#24773;&#22659;&#24863;&#30693;&#26041;&#27861;&#38590;&#20197;&#26377;&#25928;&#35299;&#20915;&#36825;&#20004;&#20010;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2402.14440</link><description>&lt;p&gt;
&#38024;&#23545;&#20854;&#30446;&#30340;&#30340;&#25512;&#33616;&#31995;&#32479;&#65306;&#39135;&#21697;&#37197;&#36865;&#25512;&#33616;&#20013;&#30340;&#37325;&#22797;&#21644;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Recommender for Its Purpose: Repeat and Exploration in Food Delivery Recommendations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14440
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#39135;&#21697;&#37197;&#36865;&#25512;&#33616;&#20013;&#30340;&#37325;&#22797;&#21644;&#25506;&#32034;&#28040;&#36153;&#38382;&#39064;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#21457;&#29616;&#29616;&#26377;&#24773;&#22659;&#24863;&#30693;&#26041;&#27861;&#38590;&#20197;&#26377;&#25928;&#35299;&#20915;&#36825;&#20004;&#20010;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#22330;&#26223;&#65292;&#20363;&#22914;&#30005;&#23376;&#21830;&#21153;&#12289;&#26032;&#38395;&#21644;&#38899;&#20048;&#65292;&#25552;&#20379;&#22312;&#32447;&#20869;&#23481;&#20197;&#24110;&#21161;&#21644;&#20016;&#23500;&#29992;&#25143;&#30340;&#26085;&#24120;&#29983;&#27963;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#39135;&#21697;&#37197;&#36865;&#25512;&#33616;&#65292;&#25581;&#31034;&#35813;&#39046;&#22495;&#30340;&#29420;&#29305;&#29305;&#24449;&#65292;&#29992;&#25143;&#22312;&#32447;&#35746;&#36141;&#39135;&#29289;&#65292;&#24182;&#22312;&#36865;&#36798;&#21518;&#19981;&#20037;&#20139;&#29992;&#39184;&#28857;&#12290;&#25105;&#20204;&#39318;&#20808;&#23545;&#39135;&#21697;&#37197;&#36865;&#25968;&#25454;&#38598;&#36827;&#34892;&#28145;&#20837;&#20998;&#26512;&#12290;&#20998;&#26512;&#26174;&#31034;&#65292;&#29992;&#25143;&#21644;&#21830;&#23478;&#22343;&#23384;&#22312;&#37325;&#22797;&#35746;&#21333;&#65292;&#24182;&#19988;&#24773;&#22659;&#19981;&#21516;&#24433;&#21709;&#30528;&#39135;&#21697;&#37197;&#36865;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#37325;&#22797;&#21644;&#25506;&#32034;&#28040;&#36153;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#29616;&#26377;&#24773;&#22659;&#24863;&#30693;&#26041;&#27861;&#20998;&#21035;&#29992;&#20110;&#37325;&#22797;&#21644;&#25506;&#32034;&#25512;&#33616;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#26080;&#27861;&#26377;&#25928;&#22320;&#21516;&#26102;&#35299;&#20915;&#36825;&#20004;&#20010;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14440v1 Announce Type: new  Abstract: Recommender systems have been widely used for various scenarios, such as e-commerce, news, and music, providing online contents to help and enrich users' daily life. Different scenarios hold distinct and unique characteristics, calling for domain-specific investigations and corresponding designed recommender systems. Therefore, in this paper, we focus on food delivery recommendations to unveil unique features in this domain, where users order food online and enjoy their meals shortly after delivery. We first conduct an in-depth analysis on food delivery datasets. The analysis shows that repeat orders are prevalent for both users and stores, and situations' differently influence repeat and exploration consumption in the food delivery recommender systems. Moreover, we revisit the ability of existing situation-aware methods for repeat and exploration recommendations respectively, and find them unable to effectively solve both tasks simultan
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#30693;&#35782;&#20914;&#31361;&#65292;&#25552;&#20986;&#20102;&#35780;&#20272;&#26694;&#26550;&#65292;&#30740;&#31350;&#20102;RALMs&#23545;&#20869;&#37096;&#35760;&#24518;&#21644;&#22806;&#37096;&#26469;&#28304;&#38388;&#30340;&#20914;&#31361;&#65292;&#21457;&#29616;&#20102;&#23427;&#20204;&#20250;&#20559;&#21521;&#38169;&#35823;&#30340;&#20869;&#37096;&#35760;&#24518;&#12290;</title><link>https://arxiv.org/abs/2402.14409</link><description>&lt;p&gt;
&#30693;&#35782;&#20043;&#38388;&#30340;&#25289;&#38191;&#25112;: &#25506;&#32034;&#21644;&#35299;&#20915;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#30693;&#35782;&#20914;&#31361;
&lt;/p&gt;
&lt;p&gt;
Tug-of-War Between Knowledge: Exploring and Resolving Knowledge Conflicts in Retrieval-Augmented Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14409
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#30693;&#35782;&#20914;&#31361;&#65292;&#25552;&#20986;&#20102;&#35780;&#20272;&#26694;&#26550;&#65292;&#30740;&#31350;&#20102;RALMs&#23545;&#20869;&#37096;&#35760;&#24518;&#21644;&#22806;&#37096;&#26469;&#28304;&#38388;&#30340;&#20914;&#31361;&#65292;&#21457;&#29616;&#20102;&#23427;&#20204;&#20250;&#20559;&#21521;&#38169;&#35823;&#30340;&#20869;&#37096;&#35760;&#24518;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#65288;RALMs&#65289;&#24050;&#32463;&#22312;&#36890;&#36807;&#20174;&#22806;&#37096;&#26469;&#28304;&#26816;&#32034;&#35777;&#25454;&#26469;&#20248;&#21270;&#21644;&#25193;&#23637;&#20854;&#20869;&#37096;&#35760;&#24518;&#26041;&#38754;&#34920;&#29616;&#20986;&#37325;&#35201;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;RALMs&#22312;&#23558;&#20869;&#37096;&#35760;&#24518;&#19982;&#22806;&#37096;&#26469;&#28304;&#25972;&#21512;&#26102;&#24517;&#28982;&#20250;&#36935;&#21040;&#30693;&#35782;&#20914;&#31361;&#12290;&#30693;&#35782;&#20914;&#31361;&#20250;&#20351;RALMs&#38519;&#20837;&#30693;&#35782;&#20043;&#38388;&#30340;&#25289;&#38191;&#25112;&#65292;&#38480;&#21046;&#20854;&#23454;&#38469;&#24212;&#29992;&#12290;&#26412;&#25991;&#30528;&#37325;&#20110;&#25506;&#32034;&#21644;&#35299;&#20915;RALMs&#20013;&#30340;&#30693;&#35782;&#20914;&#31361;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#19981;&#21516;&#32500;&#24230;&#19978;&#30340;&#30693;&#35782;&#20914;&#31361;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20174;&#20197;&#19979;&#20004;&#20010;&#35282;&#24230;&#30740;&#31350;&#20102;RALMs&#30340;&#34892;&#20026;&#21644;&#20559;&#22909;&#65306;&#65288;1&#65289;&#20869;&#37096;&#35760;&#24518;&#19982;&#22806;&#37096;&#26469;&#28304;&#20043;&#38388;&#30340;&#20914;&#31361;&#65306;&#25105;&#20204;&#21457;&#29616;&#65292;&#38543;&#30528;&#37011;&#23425;-&#20811;&#40065;&#26684;&#25928;&#24212;&#30340;&#22686;&#24378;&#65292;&#26356;&#24378;&#22823;&#30340;RALMs&#20250;&#25345;&#32493;&#20559;&#29233;&#20854;&#38169;&#35823;&#30340;&#20869;&#37096;&#35760;&#24518;&#65292;&#21363;&#20351;&#25552;&#20379;&#20102;&#27491;&#30830;&#30340;&#35777;&#25454;&#12290;&#27492;&#22806;&#65292;RALMs&#36824;&#34920;&#29616;&#20986;&#19968;&#31181;&#21487;&#29992;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14409v1 Announce Type: cross  Abstract: Retrieval-augmented language models (RALMs) have demonstrated significant potential in refining and expanding their internal memory by retrieving evidence from external sources. However, RALMs will inevitably encounter knowledge conflicts when integrating their internal memory with external sources. Knowledge conflicts can ensnare RALMs in a tug-of-war between knowledge, limiting their practical applicability. In this paper, we focus on exploring and resolving knowledge conflicts in RALMs. First, we present an evaluation framework for assessing knowledge conflicts across various dimensions. Then, we investigate the behavior and preference of RALMs from the following two perspectives: (1) Conflicts between internal memory and external sources: We find that stronger RALMs emerge with the Dunning-Kruger effect, persistently favoring their faulty internal memory even when correct evidence is provided. Besides, RALMs exhibit an availability
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Sliver&#30340;&#28369;&#21160;&#31383;&#21475;&#25968;&#25454;&#27969;&#35774;&#35745;&#33539;&#24335;&#65292;&#36890;&#36807;&#20943;&#23567;&#31383;&#21475;&#22823;&#23567;&#21644;&#23454;&#29616;&#28369;&#21160;&#31383;&#21475;&#26469;&#35299;&#20915;&#23454;&#26102;&#25512;&#33616;&#31995;&#32479;&#20013;&#26631;&#31614;&#30340;&#21450;&#26102;&#24615;&#21644;&#20934;&#30830;&#24615;&#38382;&#39064;</title><link>https://arxiv.org/abs/2402.14399</link><description>&lt;p&gt;
&#30830;&#20445;&#21450;&#26102;&#24615;&#21644;&#20934;&#30830;&#24615;&#65306;&#19968;&#31181;&#26032;&#30340;&#28369;&#21160;&#31383;&#21475;&#25968;&#25454;&#27969;&#33539;&#24335;&#29992;&#20110;&#23454;&#26102;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Ensure Timeliness and Accuracy: A Novel Sliding Window Data Stream Paradigm for Live Streaming Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14399
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Sliver&#30340;&#28369;&#21160;&#31383;&#21475;&#25968;&#25454;&#27969;&#35774;&#35745;&#33539;&#24335;&#65292;&#36890;&#36807;&#20943;&#23567;&#31383;&#21475;&#22823;&#23567;&#21644;&#23454;&#29616;&#28369;&#21160;&#31383;&#21475;&#26469;&#35299;&#20915;&#23454;&#26102;&#25512;&#33616;&#31995;&#32479;&#20013;&#26631;&#31614;&#30340;&#21450;&#26102;&#24615;&#21644;&#20934;&#30830;&#24615;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Live streaming recommender system&#19987;&#20026;&#21521;&#29992;&#25143;&#25512;&#33616;&#23454;&#26102;&#24863;&#20852;&#36259;&#30340;&#30452;&#25773;&#27969;&#32780;&#35774;&#35745;&#12290;&#30001;&#20110;&#30452;&#25773;&#20869;&#23481;&#21160;&#24577;&#21464;&#21270;&#65292;&#25552;&#39640;&#25512;&#33616;&#31995;&#32479;&#30340;&#21450;&#26102;&#24615;&#26159;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#27969;&#35774;&#35745;&#33539;&#24335;&#65292;&#21517;&#20026;Sliver&#65292;&#36890;&#36807;&#20943;&#23567;&#31383;&#21475;&#22823;&#23567;&#21644;&#30456;&#24212;&#23454;&#29616;&#28369;&#21160;&#31383;&#21475;&#26469;&#35299;&#20915;&#26631;&#31614;&#30340;&#21450;&#26102;&#24615;&#21644;&#20934;&#30830;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14399v1 Announce Type: cross  Abstract: Live streaming recommender system is specifically designed to recommend real-time live streaming of interest to users. Due to the dynamic changes of live content, improving the timeliness of the live streaming recommender system is a critical problem. Intuitively, the timeliness of the data determines the upper bound of the timeliness that models can learn. However, none of the previous works addresses the timeliness problem of the live streaming recommender system from the perspective of data stream design. Employing the conventional fixed window data stream paradigm introduces a trade-off dilemma between labeling accuracy and timeliness. In this paper, we propose a new data stream design paradigm, dubbed Sliver, that addresses the timeliness and accuracy problem of labels by reducing the window size and implementing a sliding window correspondingly. Meanwhile, we propose a time-sensitive re-reco strategy reducing the latency between 
&lt;/p&gt;</description></item><item><title>&#20856;&#22411;&#25512;&#33616;&#26041;&#27861;&#24573;&#35270;&#20102;&#25512;&#33616;&#23545;&#39033;&#30446;&#21644;&#25552;&#20379;&#32773;&#30340;&#24433;&#21709;&#65292;&#32780;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#21644;&#21487;&#35777;&#26126;&#20844;&#24179;&#26292;&#38706;&#25511;&#21046;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#24037;&#19994;&#35268;&#27169;&#25512;&#33616;&#31995;&#32479;&#20013;&#26333;&#20809;&#25511;&#21046;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.14369</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#25512;&#33616;&#31995;&#32479;&#30340;&#21487;&#25193;&#23637;&#21644;&#21487;&#35777;&#26126;&#20844;&#24179;&#26292;&#38706;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Scalable and Provably Fair Exposure Control for Large-Scale Recommender Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14369
&lt;/p&gt;
&lt;p&gt;
&#20856;&#22411;&#25512;&#33616;&#26041;&#27861;&#24573;&#35270;&#20102;&#25512;&#33616;&#23545;&#39033;&#30446;&#21644;&#25552;&#20379;&#32773;&#30340;&#24433;&#21709;&#65292;&#32780;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#21644;&#21487;&#35777;&#26126;&#20844;&#24179;&#26292;&#38706;&#25511;&#21046;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#24037;&#19994;&#35268;&#27169;&#25512;&#33616;&#31995;&#32479;&#20013;&#26333;&#20809;&#25511;&#21046;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20856;&#22411;&#30340;&#25512;&#33616;&#21644;&#25490;&#21517;&#26041;&#27861;&#26088;&#22312;&#20248;&#21270;&#29992;&#25143;&#30340;&#28385;&#24847;&#24230;&#65292;&#20294;&#36890;&#24120;&#24573;&#35270;&#23427;&#20204;&#23545;&#39033;&#30446;&#65288;&#20363;&#22914;&#20135;&#21697;&#12289;&#24037;&#20316;&#12289;&#26032;&#38395;&#12289;&#35270;&#39057;&#65289;&#21450;&#20854;&#25552;&#20379;&#32773;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#20154;&#20204;&#26085;&#30410;&#24847;&#35782;&#21040;&#21518;&#32773;&#23545;&#20110;&#35768;&#22810;&#24212;&#29992;&#31243;&#24207;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#20915;&#23450;&#20102;&#34987;&#25512;&#33616;&#20154;&#30340;&#25928;&#29992;&#12290;&#20808;&#21069;&#30340;&#20844;&#24179;&#24863;&#30693;&#25512;&#33616;&#26041;&#27861;&#20248;&#21270;&#20102;&#19968;&#20010;&#27491;&#21017;&#21270;&#30340;&#30446;&#26631;&#65292;&#20197;&#24179;&#34913;&#29992;&#25143;&#28385;&#24847;&#24230;&#21644;&#22522;&#20110;&#26576;&#20123;&#27010;&#24565;&#65288;&#22914;&#26333;&#20809;&#20844;&#24179;&#24615;&#65289;&#30340;&#39033;&#30446;&#20844;&#24179;&#24615;&#12290;&#36825;&#20123;&#29616;&#26377;&#26041;&#27861;&#24050;&#34987;&#35777;&#26126;&#22312;&#25511;&#21046;&#20844;&#24179;&#24615;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#65292;&#20294;&#22823;&#22810;&#25968;&#26041;&#27861;&#22312;&#35745;&#31639;&#19978;&#25928;&#29575;&#20302;&#19979;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#20165;&#36866;&#29992;&#20110;&#19981;&#20999;&#23454;&#38469;&#30340;&#23567;&#35268;&#27169;&#24773;&#20917;&#12290;&#36825;&#23454;&#38469;&#19978;&#24847;&#21619;&#30528;&#25991;&#29486;&#23578;&#26410;&#25552;&#20379;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#22312;&#24037;&#19994;&#35268;&#27169;&#30340;&#25512;&#33616;&#31995;&#32479;&#20013;&#23454;&#29616;&#23545;&#26333;&#20809;&#30340;&#28789;&#27963;&#25511;&#21046;&#65292;&#20854;&#20013;&#26377;&#25968;&#20197;&#30334;&#19975;&#35745;&#30340;&#29992;&#25143;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14369v1 Announce Type: new  Abstract: Typical recommendation and ranking methods aim to optimize the satisfaction of users, but they are often oblivious to their impact on the items (e.g., products, jobs, news, video) and their providers. However, there has been a growing understanding that the latter is crucial to consider for a wide range of applications, since it determines the utility of those being recommended. Prior approaches to fairness-aware recommendation optimize a regularized objective to balance user satisfaction and item fairness based on some notion such as exposure fairness. These existing methods have been shown to be effective in controlling fairness, however, most of them are computationally inefficient, limiting their applications to only unrealistically small-scale situations. This indeed implies that the literature does not yet provide a solution to enable a flexible control of exposure in the industry-scale recommender systems where millions of users a
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#20351;&#29992;Expohedron&#35299;&#20915;&#24085;&#32047;&#25176;&#26368;&#20248;&#25928;&#29992;-&#20844;&#24179;&#24615;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;Birkhoff-von Neumann&#20998;&#35299;&#30340;&#39640;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.14305</link><description>&lt;p&gt;
&#26377;&#25928;&#23454;&#29616;&#22312;&#37325;&#22797;&#25490;&#21517;&#20013;&#32676;&#20307;&#38388;&#24085;&#32047;&#25176;&#26368;&#20248;&#30340;&#25928;&#29992;&#8212;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Towards Efficient Pareto-optimal Utility-Fairness between Groups in Repeated Rankings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14305
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#20351;&#29992;Expohedron&#35299;&#20915;&#24085;&#32047;&#25176;&#26368;&#20248;&#25928;&#29992;-&#20844;&#24179;&#24615;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;Birkhoff-von Neumann&#20998;&#35299;&#30340;&#39640;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#33268;&#21147;&#20110;&#35299;&#20915;&#35745;&#31639;&#20855;&#26377;&#24085;&#32047;&#25176;&#26368;&#20248;&#24179;&#34913;&#20445;&#35777;&#30340;&#19968;&#31995;&#21015;&#25490;&#21517;&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;&#32771;&#34385;&#20102;&#65288;1&#65289;&#26368;&#22823;&#21270;&#28040;&#36153;&#32773;&#25928;&#29992;&#21644;&#65288;2&#65289;&#26368;&#23567;&#21270;&#29289;&#21697;&#29983;&#20135;&#32773;&#20043;&#38388;&#30340;&#19981;&#20844;&#24179;&#24615;&#12290;&#36825;&#26679;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#36890;&#24120;&#20351;&#29992;&#26631;&#37327;&#21270;&#26041;&#27861;&#21644;&#32447;&#24615;&#35268;&#21010;&#26469;&#35299;&#20915;&#65292;&#22522;&#20110;&#34920;&#31034;&#29289;&#21697;&#21487;&#33021;&#25490;&#21517;&#20998;&#24067;&#30340;&#21452;&#38543;&#26426;&#30697;&#38453;&#12290;&#28982;&#32780;&#65292;&#19978;&#36848;&#26041;&#27861;&#20381;&#36182;&#20110;Birkhoff-von Neumann&#65288;BvN&#65289;&#20998;&#35299;&#65292;&#20854;&#35745;&#31639;&#22797;&#26434;&#24230;&#20026;$\mathcal{O}(n^5)$&#65292;&#20854;&#20013;$n$&#26159;&#29289;&#21697;&#25968;&#37327;&#65292;&#36825;&#20351;&#24471;&#22312;&#22823;&#35268;&#27169;&#31995;&#32479;&#20013;&#21464;&#24471;&#19981;&#20999;&#23454;&#38469;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#32570;&#38519;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;Expohedron&#26469;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064; - &#19968;&#20010;&#34920;&#24449;&#25152;&#26377;&#21487;&#36798;&#21040;&#29289;&#21697;&#26333;&#20809;&#30340;&#25490;&#21015;&#22810;&#38754;&#20307;&#12290;&#22312;Expohedron&#19978;&#65292;&#25105;&#20204;&#32472;&#21046;&#20102;&#24085;&#32047;&#25176;&#26354;&#32447;&#65292;&#25429;&#25417;&#20102;&#22312;&#26368;&#22823;&#21270;&#25928;&#29992;&#21644;&#26368;&#23567;&#21270;&#19981;&#20844;&#24179;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14305v1 Announce Type: cross  Abstract: In this paper, we tackle the problem of computing a sequence of rankings with the guarantee of the Pareto-optimal balance between (1) maximizing the utility of the consumers and (2) minimizing unfairness between producers of the items. Such a multi-objective optimization problem is typically solved using a combination of a scalarization method and linear programming on bi-stochastic matrices, representing the distribution of possible rankings of items. However, the above-mentioned approach relies on Birkhoff-von Neumann (BvN) decomposition, of which the computational complexity is $\mathcal{O}(n^5)$ with $n$ being the number of items, making it impractical for large-scale systems. To address this drawback, we introduce a novel approach to the above problem by using the Expohedron - a permutahedron whose points represent all achievable exposures of items. On the Expohedron, we profile the Pareto curve which captures the trade-off betwee
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;GenSERP&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21160;&#24577;&#25972;&#29702;&#25628;&#32034;&#32467;&#26524;&#24182;&#26681;&#25454;&#29992;&#25143;&#26597;&#35810;&#29983;&#25104;&#36830;&#36143;&#30340;&#25628;&#32034;&#24341;&#25806;&#32467;&#26524;&#39029;&#38754;&#12290;</title><link>https://arxiv.org/abs/2402.14301</link><description>&lt;p&gt;
GenSERP: &#29992;&#20110;&#25972;&#20010;&#39029;&#38754;&#21576;&#29616;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
GenSERP: Large Language Models for Whole Page Presentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14301
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;GenSERP&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21160;&#24577;&#25972;&#29702;&#25628;&#32034;&#32467;&#26524;&#24182;&#26681;&#25454;&#29992;&#25143;&#26597;&#35810;&#29983;&#25104;&#36830;&#36143;&#30340;&#25628;&#32034;&#24341;&#25806;&#32467;&#26524;&#39029;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#20026;&#26368;&#23567;&#21270;&#25628;&#32034;&#24341;&#25806;&#32467;&#26524;&#39029;&#38754;&#65288;SERP&#65289;&#30340;&#32452;&#32455;&#24037;&#20316;&#24102;&#26469;&#20102;&#26426;&#20250;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;GenSERP&#65292;&#36825;&#26159;&#19968;&#20010;&#21033;&#29992;LLMs&#21644;&#35270;&#35273;&#22312;&#23569;&#26679;&#26412;&#35774;&#32622;&#20013;&#21160;&#24577;&#32452;&#32455;&#20013;&#38388;&#25628;&#32034;&#32467;&#26524;&#30340;&#26694;&#26550;&#65292;&#21253;&#25324;&#29983;&#25104;&#30340;&#32842;&#22825;&#31572;&#26696;&#12289;&#32593;&#31449;&#25688;&#35201;&#12289;&#22810;&#23186;&#20307;&#25968;&#25454;&#12289;&#30693;&#35782;&#38754;&#26495;&#31561;&#65292;&#24182;&#26681;&#25454;&#29992;&#25143;&#30340;&#26597;&#35810;&#20197;&#36830;&#36143;&#30340;SERP&#24067;&#23616;&#21576;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14301v1 Announce Type: cross  Abstract: The advent of large language models (LLMs) brings an opportunity to minimize the effort in search engine result page (SERP) organization. In this paper, we propose GenSERP, a framework that leverages LLMs with vision in a few-shot setting to dynamically organize intermediate search results, including generated chat answers, website snippets, multimedia data, knowledge panels into a coherent SERP layout based on a user's query. Our approach has three main stages: (1) An information gathering phase where the LLM continuously orchestrates API tools to retrieve different types of items, and proposes candidate layouts based on the retrieved items, until it's confident enough to generate the final result. (2) An answer generation phase where the LLM populates the layouts with the retrieved content. In this phase, the LLM adaptively optimize the ranking of items and UX configurations of the SERP. Consequently, it assigns a location on the pag
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;MerRec&#65292;&#36825;&#26159;&#39318;&#20010;&#19987;&#38376;&#38024;&#23545;C2C&#25512;&#33616;&#32780;&#25552;&#20986;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#22635;&#34917;&#20102;C2C&#25512;&#33616;&#25968;&#25454;&#38598;&#20013;&#29289;&#21697;&#23646;&#24615;&#12289;&#29992;&#25143;&#22810;&#26679;&#24615;&#21644;&#35268;&#27169;&#31561;&#26041;&#38754;&#30340;&#32570;&#22833;&#12290;</title><link>https://arxiv.org/abs/2402.14230</link><description>&lt;p&gt;
MerRec&#65306;&#29992;&#20110;&#28040;&#36153;&#32773;&#23545;&#28040;&#36153;&#32773;&#25512;&#33616;&#31995;&#32479;&#30340;&#22823;&#35268;&#27169;&#22810;&#21151;&#33021;Mercari&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
MerRec: A Large-scale Multipurpose Mercari Dataset for Consumer-to-Consumer Recommendation Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14230
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;MerRec&#65292;&#36825;&#26159;&#39318;&#20010;&#19987;&#38376;&#38024;&#23545;C2C&#25512;&#33616;&#32780;&#25552;&#20986;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#22635;&#34917;&#20102;C2C&#25512;&#33616;&#25968;&#25454;&#38598;&#20013;&#29289;&#21697;&#23646;&#24615;&#12289;&#29992;&#25143;&#22810;&#26679;&#24615;&#21644;&#35268;&#27169;&#31561;&#26041;&#38754;&#30340;&#32570;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#26029;&#21457;&#23637;&#30340;&#30005;&#23376;&#21830;&#21153;&#39046;&#22495;&#20013;&#65292;&#25512;&#33616;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#22320;&#22609;&#36896;&#20102;&#29992;&#25143;&#20307;&#39564;&#21644;&#21442;&#19982;&#24230;&#12290;&#28040;&#36153;&#32773;&#23545;&#28040;&#36153;&#32773;&#65288;C2C&#65289;&#25512;&#33616;&#31995;&#32479;&#30340;&#23835;&#36215;&#65292;&#20197;&#20854;&#28789;&#27963;&#24615;&#21644;&#20026;&#23458;&#25143;&#20379;&#24212;&#21830;&#25552;&#20379;&#26131;&#20110;&#35775;&#38382;&#30340;&#29305;&#28857;&#65292;&#26631;&#24535;&#30528;&#19968;&#20010;&#37325;&#35201;&#36235;&#21183;&#12290;&#28982;&#32780;&#65292;&#23398;&#26415;&#20851;&#27880;&#20027;&#35201;&#38598;&#20013;&#22312;&#21830;&#23478;&#23545;&#28040;&#36153;&#32773;&#65288;B2C&#65289;&#27169;&#22411;&#19978;&#65292;&#30041;&#19979;&#20102;&#19968;&#20010;&#31354;&#30333;&#65292;&#21363;&#32570;&#20047;&#29289;&#21697;&#23646;&#24615;&#12289;&#29992;&#25143;&#22810;&#26679;&#24615;&#21644;&#35268;&#27169;&#30340;C2C&#25512;&#33616;&#25968;&#25454;&#38598;&#12290;C2C&#25512;&#33616;&#31995;&#32479;&#30340;&#22797;&#26434;&#24615;&#36827;&#19968;&#27493;&#31361;&#20986;&#20102;&#29992;&#25143;&#25198;&#28436;&#21334;&#23478;&#21644;&#20080;&#23478;&#20004;&#31181;&#35282;&#33394;&#30340;&#21452;&#37325;&#24615;&#36136;&#65292;&#24341;&#20837;&#20102;&#19968;&#31995;&#21015;&#19981;&#37027;&#20040;&#32479;&#19968;&#21644;&#22810;&#26679;&#21270;&#30340;&#36755;&#20837;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;MerRec&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;C2C&#25512;&#33616;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#28304;&#33258;Mercari&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#65292;&#35206;&#30422;&#20102;2023&#24180;6&#20010;&#26376;&#20869;&#25968;&#30334;&#19975;&#29992;&#25143;&#21644;&#20135;&#21697;&#12290;MerRec&#19981;&#20165;&#21253;&#25324;&#26631;&#20934;&#29305;&#24449;&#65292;&#22914;user_id&#12289;item_id&#21644;session_id
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14230v1 Announce Type: cross  Abstract: In the evolving e-commerce field, recommendation systems crucially shape user experience and engagement. The rise of Consumer-to-Consumer (C2C) recommendation systems, noted for their flexibility and ease of access for customer vendors, marks a significant trend. However, the academic focus remains largely on Business-to-Consumer (B2C) models, leaving a gap filled by the limited C2C recommendation datasets that lack in item attributes, user diversity, and scale. The intricacy of C2C recommendation systems is further accentuated by the dual roles users assume as both sellers and buyers, introducing a spectrum of less uniform and varied inputs. Addressing this, we introduce MerRec, the first large-scale dataset specifically for C2C recommendations, sourced from the Mercari e-commerce platform, covering millions of users and products over 6 months in 2023. MerRec not only includes standard features such as user_id, item_id, and session_id
&lt;/p&gt;</description></item><item><title>BIRCO&#22522;&#20934;&#35780;&#20272;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#23545;&#22810;&#26041;&#38754;&#29992;&#25143;&#30446;&#26631;&#30340;&#26816;&#32034;&#33021;&#21147;&#65292;&#21457;&#29616;&#26032;&#30340;&#26816;&#32034;&#21327;&#35758;&#21644;&#26356;&#24378;&#22823;&#30340;&#27169;&#22411;&#26159;&#35299;&#20915;&#22797;&#26434;&#29992;&#25143;&#38656;&#27714;&#30340;&#24517;&#35201;&#26465;&#20214;&#12290;</title><link>https://arxiv.org/abs/2402.14151</link><description>&lt;p&gt;
BIRCO&#65306;&#20855;&#26377;&#22797;&#26434;&#30446;&#26631;&#30340;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
BIRCO: A Benchmark of Information Retrieval Tasks with Complex Objectives
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14151
&lt;/p&gt;
&lt;p&gt;
BIRCO&#22522;&#20934;&#35780;&#20272;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#23545;&#22810;&#26041;&#38754;&#29992;&#25143;&#30446;&#26631;&#30340;&#26816;&#32034;&#33021;&#21147;&#65292;&#21457;&#29616;&#26032;&#30340;&#26816;&#32034;&#21327;&#35758;&#21644;&#26356;&#24378;&#22823;&#30340;&#27169;&#22411;&#26159;&#35299;&#20915;&#22797;&#26434;&#29992;&#25143;&#38656;&#27714;&#30340;&#24517;&#35201;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#20855;&#26377;&#22797;&#26434;&#30446;&#26631;&#30340;&#20449;&#24687;&#26816;&#32034;(IR)&#20219;&#21153;&#22522;&#20934;(BIRCO)&#12290; BIRCO&#35780;&#20272;IR&#31995;&#32479;&#26681;&#25454;&#22810;&#26041;&#38754;&#29992;&#25143;&#30446;&#26631;&#26816;&#32034;&#25991;&#26723;&#30340;&#33021;&#21147;&#12290; &#35813;&#22522;&#20934;&#30340;&#22797;&#26434;&#24615;&#21644;&#32039;&#20945;&#22823;&#23567;&#20351;&#20854;&#36866;&#29992;&#20110;&#35780;&#20272;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#12290; &#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22359;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#30740;&#31350;&#21487;&#33021;&#24433;&#21709;LLM&#22312;&#26816;&#32034;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#30340;&#22240;&#32032;&#65292;&#24182;&#30830;&#23450;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#22522;&#32447;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#19982;&#25110;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#21644;&#26356;&#22797;&#26434;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290; &#27809;&#26377;&#19968;&#31181;&#26041;&#27861;&#22312;&#25152;&#26377;&#22522;&#20934;&#20219;&#21153;&#19978;&#22343;&#36798;&#21040;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#65292;&#36825;&#34920;&#26126;&#38656;&#35201;&#26356;&#24378;&#22823;&#30340;&#27169;&#22411;&#21644;&#26032;&#30340;&#26816;&#32034;&#21327;&#35758;&#26469;&#35299;&#20915;&#22797;&#26434;&#30340;&#29992;&#25143;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14151v1 Announce Type: cross  Abstract: We present the Benchmark of Information Retrieval (IR) tasks with Complex Objectives (BIRCO). BIRCO evaluates the ability of IR systems to retrieve documents given multi-faceted user objectives. The benchmark's complexity and compact size make it suitable for evaluating large language model (LLM)-based information retrieval systems. We present a modular framework for investigating factors that may influence LLM performance on retrieval tasks, and identify a simple baseline model which matches or outperforms existing approaches and more complex alternatives. No approach achieves satisfactory performance on all benchmark tasks, suggesting that stronger models and new retrieval protocols are necessary to address complex user needs.
&lt;/p&gt;</description></item><item><title>GraphScholarBERT&#26159;&#19968;&#31181;&#32467;&#21512;&#35821;&#35328;&#21644;&#22270;&#27169;&#22411;&#30340;&#20449;&#24687;&#25277;&#21462;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;Web&#19978;&#21322;&#32467;&#26500;&#21270;&#20449;&#24687;&#20013;&#25552;&#21462;&#30446;&#26631;&#20851;&#31995;&#65292;&#24182;&#22312;&#38646;&#23556;&#39046;&#22495;&#21644;&#38646;&#23556;&#32593;&#31449;&#35774;&#32622;&#20013;&#23558;&#25552;&#21462;F1&#24471;&#20998;&#25552;&#39640;34.8&#65285;&#12290;</title><link>https://arxiv.org/abs/2402.14129</link><description>&lt;p&gt;
&#32467;&#21512;&#35821;&#35328;&#21644;&#22270;&#27169;&#22411;&#36827;&#34892;Web&#19978;&#21322;&#32467;&#26500;&#21270;&#20449;&#24687;&#25277;&#21462;
&lt;/p&gt;
&lt;p&gt;
Combining Language and Graph Models for Semi-structured Information Extraction on the Web
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14129
&lt;/p&gt;
&lt;p&gt;
GraphScholarBERT&#26159;&#19968;&#31181;&#32467;&#21512;&#35821;&#35328;&#21644;&#22270;&#27169;&#22411;&#30340;&#20449;&#24687;&#25277;&#21462;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;Web&#19978;&#21322;&#32467;&#26500;&#21270;&#20449;&#24687;&#20013;&#25552;&#21462;&#30446;&#26631;&#20851;&#31995;&#65292;&#24182;&#22312;&#38646;&#23556;&#39046;&#22495;&#21644;&#38646;&#23556;&#32593;&#31449;&#35774;&#32622;&#20013;&#23558;&#25552;&#21462;F1&#24471;&#20998;&#25552;&#39640;34.8&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#31995;&#25277;&#21462;&#26159;&#22312;&#32593;&#32476;&#19978;&#25366;&#25496;&#20154;&#31867;&#30693;&#35782;&#30340;&#19968;&#31181;&#39640;&#25928;&#26041;&#24335;&#12290;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#20110;&#29305;&#23450;&#39046;&#22495;&#30340;&#35757;&#32451;&#25968;&#25454;&#25110;&#20135;&#29983;&#22024;&#26434;&#30340;&#36755;&#20986;&#12290;&#26412;&#25991;&#30528;&#37325;&#20110;&#20174;&#21322;&#32467;&#26500;&#21270;&#30340;&#32593;&#39029;&#20013;&#25552;&#21462;&#30446;&#26631;&#20851;&#31995;&#65292;&#20165;&#32473;&#20986;&#20851;&#31995;&#30340;&#31616;&#30701;&#25551;&#36848;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;GraphScholarBERT&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#32852;&#21512;&#22270;&#21644;&#35821;&#35328;&#27169;&#22411;&#32467;&#26500;&#30340;&#24320;&#25918;&#39046;&#22495;&#20449;&#24687;&#25552;&#21462;&#26041;&#27861;&#12290;GraphScholarBERT&#33021;&#22815;&#27867;&#21270;&#21040;&#20197;&#21069;&#26410;&#35265;&#36807;&#30340;&#39046;&#22495;&#65292;&#26080;&#38656;&#39069;&#22806;&#25968;&#25454;&#25110;&#35757;&#32451;&#65292;&#24182;&#19988;&#20165;&#20135;&#29983;&#19982;&#25628;&#32034;&#20851;&#38190;&#23383;&#21305;&#37197;&#30340;&#24178;&#20928;&#25552;&#21462;&#32467;&#26524;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#38646;&#23556;&#39046;&#22495;&#21644;&#38646;&#23556;&#32593;&#31449;&#35774;&#32622;&#20013;&#30340;&#20808;&#21069;&#24037;&#20316;&#30456;&#27604;&#65292;GraphScholarBERT&#21487;&#20197;&#23558;&#25552;&#21462;&#30340;F1&#24471;&#20998;&#25552;&#39640;&#22810;&#36798;34.8&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14129v1 Announce Type: cross  Abstract: Relation extraction is an efficient way of mining the extraordinary wealth of human knowledge on the Web. Existing methods rely on domain-specific training data or produce noisy outputs. We focus here on extracting targeted relations from semi-structured web pages given only a short description of the relation. We present GraphScholarBERT, an open-domain information extraction method based on a joint graph and language model structure. GraphScholarBERT can generalize to previously unseen domains without additional data or training and produces only clean extraction results matched to the search keyword. Experiments show that GraphScholarBERT can improve extraction F1 scores by as much as 34.8\% compared to previous work in a zero-shot domain and zero-shot website setting.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#29616;&#23454;&#32422;&#26463;&#30340;&#36731;&#37327;&#32423;&#24323;&#26435;&#26426;&#21046;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#20877;&#25490;&#24207;&#38454;&#27573;&#65292;&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#36798;&#21040;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#24320;&#28304;&#20195;&#30721;&#20197;&#20419;&#36827;&#20854;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.12997</link><description>&lt;p&gt;
&#26397;&#30528;&#21487;&#20449;&#30340;&#20877;&#25490;&#24207;&#65306;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#24323;&#26435;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Towards Trustworthy Reranking: A Simple yet Effective Abstention Mechanism
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12997
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#29616;&#23454;&#32422;&#26463;&#30340;&#36731;&#37327;&#32423;&#24323;&#26435;&#26426;&#21046;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#20877;&#25490;&#24207;&#38454;&#27573;&#65292;&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#36798;&#21040;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#24320;&#28304;&#20195;&#30721;&#20197;&#20419;&#36827;&#20854;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#20449;&#24687;&#26816;&#32034;&#65288;NIR&#65289;&#24050;&#32463;&#26174;&#33879;&#25913;&#36827;&#20102;&#22522;&#20110;&#21551;&#21457;&#24335;&#30340;IR&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#22833;&#36133;&#20173;&#28982;&#39057;&#32321;&#21457;&#29983;&#65292;&#36890;&#24120;&#25152;&#20351;&#29992;&#30340;&#27169;&#22411;&#26080;&#27861;&#26816;&#32034;&#19982;&#29992;&#25143;&#26597;&#35810;&#30456;&#20851;&#30340;&#25991;&#26723;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#36866;&#29992;&#20110;&#29616;&#23454;&#32422;&#26463;&#30340;&#36731;&#37327;&#32423;&#24323;&#26435;&#26426;&#21046;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#29305;&#21035;&#24378;&#35843;&#20877;&#25490;&#24207;&#38454;&#27573;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#21327;&#35758;&#65292;&#29992;&#20110;&#22312;&#40657;&#21283;&#23376;&#22330;&#26223;&#20013;&#35780;&#20272;&#24323;&#26435;&#31574;&#30053;&#30340;&#25928;&#26524;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#25968;&#25454;&#39537;&#21160;&#26426;&#21046;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#23454;&#39564;&#22797;&#21046;&#21644;&#24323;&#26435;&#23454;&#26045;&#30340;&#24320;&#28304;&#20195;&#30721;&#65292;&#20419;&#36827;&#20854;&#22312;&#19981;&#21516;&#29615;&#22659;&#20013;&#26356;&#24191;&#27867;&#30340;&#37319;&#29992;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12997v1 Announce Type: cross  Abstract: Neural Information Retrieval (NIR) has significantly improved upon heuristic-based IR systems. Yet, failures remain frequent, the models used often being unable to retrieve documents relevant to the user's query. We address this challenge by proposing a lightweight abstention mechanism tailored for real-world constraints, with particular emphasis placed on the reranking phase. We introduce a protocol for evaluating abstention strategies in a black-box scenario, demonstrating their efficacy, and propose a simple yet effective data-driven mechanism. We provide open-source code for experiment replication and abstention implementation, fostering wider adoption and application in diverse contexts.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25351;&#20986;&#29616;&#26377;&#30340;&#22522;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#33616;&#26041;&#27861;&#22312;&#20844;&#24179;&#24615;&#21644;&#35780;&#20272;&#19978;&#23384;&#22312;&#38382;&#39064;&#65292;&#38656;&#35201;&#37325;&#26032;&#23457;&#35270;&#20132;&#21449;&#29109;&#25439;&#22833;&#24182;&#26367;&#20195;&#20256;&#32479;&#30340;&#28857;&#23545;/&#28857;&#23545;&#25439;&#22833;&#20989;&#25968;&#12290;&#24182;&#19988;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;LLM&#30340;&#26041;&#27861;&#23545;&#20110;&#39044;&#27979;&#19979;&#19968;&#20010;&#39033;&#30446;&#30340;&#25928;&#26524;&#24182;&#19981;&#20687;&#23459;&#31216;&#30340;&#37027;&#26679;&#26377;&#25928;&#12290;</title><link>https://arxiv.org/abs/2402.06216</link><description>&lt;p&gt;
&#20844;&#24179;&#35780;&#20272;&#22522;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#33616;&#26041;&#27861;&#38656;&#35201;&#37325;&#26032;&#23457;&#35270;&#20132;&#21449;&#29109;&#25439;&#22833;
&lt;/p&gt;
&lt;p&gt;
Fairly Evaluating Large Language Model-based Recommendation Needs Revisit the Cross-Entropy Loss
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06216
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25351;&#20986;&#29616;&#26377;&#30340;&#22522;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#33616;&#26041;&#27861;&#22312;&#20844;&#24179;&#24615;&#21644;&#35780;&#20272;&#19978;&#23384;&#22312;&#38382;&#39064;&#65292;&#38656;&#35201;&#37325;&#26032;&#23457;&#35270;&#20132;&#21449;&#29109;&#25439;&#22833;&#24182;&#26367;&#20195;&#20256;&#32479;&#30340;&#28857;&#23545;/&#28857;&#23545;&#25439;&#22833;&#20989;&#25968;&#12290;&#24182;&#19988;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;LLM&#30340;&#26041;&#27861;&#23545;&#20110;&#39044;&#27979;&#19979;&#19968;&#20010;&#39033;&#30446;&#30340;&#25928;&#26524;&#24182;&#19981;&#20687;&#23459;&#31216;&#30340;&#37027;&#26679;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;(LLM)&#22312;&#25512;&#33616;&#39046;&#22495;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65307;&#19968;&#20123;&#30740;&#31350;&#35266;&#23519;&#21040;&#65292;&#32463;&#36807;&#20840;softmax&#32454;&#35843;&#30340;LLM&#24050;&#32463;&#21487;&#20197;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35266;&#28857;&#26469;&#33258;&#20110;&#20027;&#35266;&#21644;&#19981;&#20844;&#24179;&#30340;&#27604;&#36739;&#12290;&#37492;&#20110;&#29616;&#23454;&#20013;&#30340;&#22823;&#37327;&#29289;&#21697;&#65292;&#20256;&#32479;&#30340;&#25512;&#33616;&#26041;&#27861;&#36890;&#24120;&#37319;&#29992;&#28857;&#23545;/&#28857;&#23545;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26367;&#20195;&#26041;&#27861;&#20250;&#23548;&#33268;&#24615;&#33021;&#20005;&#37325;&#19979;&#38477;&#65292;&#20302;&#20272;&#20256;&#32479;&#26041;&#27861;&#30340;&#25928;&#26524;&#65292;&#24182;&#36807;&#39640;&#35780;&#20272;LLM&#30340;&#25490;&#24207;&#33021;&#21147;&#12290;&#26412;&#25991;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#20132;&#21449;&#29109;&#30340;&#20248;&#36234;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#21487;&#20197;&#29992;&#19968;&#20123;&#22522;&#26412;&#36817;&#20284;&#26041;&#27861;&#36827;&#34892;&#36866;&#24403;&#26367;&#20195;&#30340;&#24517;&#35201;&#20462;&#25913;&#12290;&#22312;&#19977;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#26174;&#33879;&#32467;&#26524;&#35777;&#23454;&#65292;&#21363;&#20351;&#20174;&#23454;&#38469;&#24847;&#20041;&#19978;&#35762;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;LLM&#30340;&#26041;&#27861;&#23545;&#20110;&#39044;&#27979;&#19979;&#19968;&#20010;&#39033;&#30446;&#30340;&#25928;&#26524;&#24182;&#19981;&#20687;&#23459;&#31216;&#30340;&#37027;&#26679;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have gained much attention in the recommendation community; some studies have observed that LLMs, fine-tuned by the cross-entropy loss with a full softmax, could achieve state-of-the-art performance already. However, these claims are drawn from unobjective and unfair comparisons. In view of the substantial quantity of items in reality, conventional recommenders typically adopt a pointwise/pairwise loss function instead for training. This substitute however causes severe performance degradation, leading to under-estimation of conventional methods and over-confidence in the ranking capability of LLMs.   In this work, we theoretically justify the superiority of cross-entropy, and showcase that it can be adequately replaced by some elementary approximations with certain necessary modifications. The remarkable results across three public datasets corroborate that even in a practical sense, existing LLM-based methods are not as effective as claimed for next-item 
&lt;/p&gt;</description></item><item><title>&#30693;&#35782;&#22270;&#35889;&#19982;&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;&#32508;&#36848;&#20171;&#32461;&#20102;KG4MM&#21644;MM4KG&#20004;&#20010;&#20027;&#35201;&#26041;&#38754;&#65292;&#21253;&#25324;&#20219;&#21153;&#23450;&#20041;&#12289;&#26500;&#24314;&#36827;&#23637;&#12289;&#35780;&#20272;&#22522;&#20934;&#20197;&#21450;&#20851;&#38190;&#30740;&#31350;&#36712;&#36857;&#12290;</title><link>https://arxiv.org/abs/2402.05391</link><description>&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#19982;&#22810;&#27169;&#24577;&#23398;&#20064;&#65306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graphs Meet Multi-Modal Learning: A Comprehensive Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05391
&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#19982;&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;&#32508;&#36848;&#20171;&#32461;&#20102;KG4MM&#21644;MM4KG&#20004;&#20010;&#20027;&#35201;&#26041;&#38754;&#65292;&#21253;&#25324;&#20219;&#21153;&#23450;&#20041;&#12289;&#26500;&#24314;&#36827;&#23637;&#12289;&#35780;&#20272;&#22522;&#20934;&#20197;&#21450;&#20851;&#38190;&#30740;&#31350;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#22312;&#25512;&#21160;&#21508;&#31181;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#35821;&#20041;&#32593;&#32476;&#31038;&#21306;&#23545;&#22810;&#27169;&#24577;&#32500;&#24230;&#30340;&#25506;&#32034;&#20026;&#21019;&#26032;&#25171;&#24320;&#20102;&#26032;&#30340;&#36884;&#24452;&#12290;&#22312;&#26412;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#20180;&#32454;&#23457;&#26597;&#20102;300&#22810;&#31687;&#25991;&#31456;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#20004;&#20010;&#20027;&#35201;&#26041;&#38754;&#30340;&#30693;&#35782;&#22270;&#35889;&#24863;&#30693;&#30740;&#31350;&#65306;&#20197;&#30693;&#35782;&#22270;&#35889;&#25903;&#25345;&#22810;&#27169;&#24577;&#20219;&#21153;&#30340;KG&#39537;&#21160;&#22810;&#27169;&#24577;&#65288;KG4MM&#65289;&#23398;&#20064;&#65292;&#23558;&#30693;&#35782;&#22270;&#35889;&#30740;&#31350;&#25193;&#23637;&#21040;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#65288;MM4KG&#65289;&#39046;&#22495;&#12290;&#25105;&#20204;&#20174;&#23450;&#20041;&#30693;&#35782;&#22270;&#35889;&#21644;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#24320;&#22987;&#65292;&#28982;&#21518;&#25506;&#32034;&#23427;&#20204;&#30340;&#26500;&#24314;&#36827;&#23637;&#12290;&#25105;&#20204;&#30340;&#32508;&#36848;&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#20219;&#21153;&#31867;&#21035;&#65306;KG&#24863;&#30693;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#20219;&#21153;&#65292;&#22914;&#22270;&#20687;&#20998;&#31867;&#21644;&#35270;&#35273;&#38382;&#31572;&#65292;&#20197;&#21450;&#20869;&#22312;&#30340;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#20219;&#21153;&#65292;&#22914;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#21644;&#23454;&#20307;&#23545;&#40784;&#65292;&#31361;&#20986;&#20102;&#20855;&#20307;&#30340;&#30740;&#31350;&#36712;&#36857;&#12290;&#23545;&#20110;&#36825;&#20123;&#20219;&#21153;&#20013;&#30340;&#22823;&#37096;&#20998;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#23450;&#20041;&#12289;&#35780;&#20272;&#22522;&#20934;&#65292;&#24182;&#36827;&#19968;&#27493;&#25351;&#20986;&#36827;&#34892;&#30456;&#20851;&#30740;&#31350;&#30340;&#37325;&#35201;&#35265;&#35299;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;cu
&lt;/p&gt;
&lt;p&gt;
Knowledge Graphs (KGs) play a pivotal role in advancing various AI applications, with the semantic web community's exploration into multi-modal dimensions unlocking new avenues for innovation. In this survey, we carefully review over 300 articles, focusing on KG-aware research in two principal aspects: KG-driven Multi-Modal (KG4MM) learning, where KGs support multi-modal tasks, and Multi-Modal Knowledge Graph (MM4KG), which extends KG studies into the MMKG realm. We begin by defining KGs and MMKGs, then explore their construction progress. Our review includes two primary task categories: KG-aware multi-modal learning tasks, such as Image Classification and Visual Question Answering, and intrinsic MMKG tasks like Multi-modal Knowledge Graph Completion and Entity Alignment, highlighting specific research trajectories. For most of these tasks, we provide definitions, evaluation benchmarks, and additionally outline essential insights for conducting relevant research. Finally, we discuss cu
&lt;/p&gt;</description></item><item><title>RAG-Fusion&#26041;&#27861;&#36890;&#36807;&#29983;&#25104;&#22810;&#20010;&#26597;&#35810;&#65292;&#24182;&#32467;&#21512;&#20114;&#24800;&#25490;&#21517;&#34701;&#21512;&#25216;&#26415;&#65292;&#33021;&#22815;&#20174;&#19981;&#21516;&#35282;&#24230;&#19978;&#19979;&#25991;&#21270;&#21407;&#22987;&#26597;&#35810;&#65292;&#25552;&#20379;&#20934;&#30830;&#21644;&#20840;&#38754;&#30340;&#20449;&#24687;&#12290;&#36825;&#39033;&#30740;&#31350;&#22312;&#20154;&#24037;&#26234;&#33021;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#26041;&#38754;&#26377;&#37325;&#35201;&#36827;&#23637;&#65292;&#24182;&#23637;&#31034;&#20102;&#20840;&#29699;&#21644;&#21306;&#22495;&#20043;&#38388;&#30340;&#36716;&#21464;&#12290;</title><link>https://arxiv.org/abs/2402.03367</link><description>&lt;p&gt;
RAG-Fusion: &#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#30340;&#26032;&#36884;&#24452;
&lt;/p&gt;
&lt;p&gt;
RAG-Fusion: a New Take on Retrieval-Augmented Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03367
&lt;/p&gt;
&lt;p&gt;
RAG-Fusion&#26041;&#27861;&#36890;&#36807;&#29983;&#25104;&#22810;&#20010;&#26597;&#35810;&#65292;&#24182;&#32467;&#21512;&#20114;&#24800;&#25490;&#21517;&#34701;&#21512;&#25216;&#26415;&#65292;&#33021;&#22815;&#20174;&#19981;&#21516;&#35282;&#24230;&#19978;&#19979;&#25991;&#21270;&#21407;&#22987;&#26597;&#35810;&#65292;&#25552;&#20379;&#20934;&#30830;&#21644;&#20840;&#38754;&#30340;&#20449;&#24687;&#12290;&#36825;&#39033;&#30740;&#31350;&#22312;&#20154;&#24037;&#26234;&#33021;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#26041;&#38754;&#26377;&#37325;&#35201;&#36827;&#23637;&#65292;&#24182;&#23637;&#31034;&#20102;&#20840;&#29699;&#21644;&#21306;&#22495;&#20043;&#38388;&#30340;&#36716;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Infineon&#24050;&#32463;&#30830;&#23450;&#24037;&#31243;&#24072;&#12289;&#23458;&#25143;&#32463;&#29702;&#21644;&#23458;&#25143;&#36805;&#36895;&#33719;&#21462;&#20135;&#21697;&#20449;&#24687;&#30340;&#38656;&#27714;&#12290;&#20256;&#32479;&#19978;&#65292;&#36825;&#20010;&#38382;&#39064;&#36890;&#36807;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#32842;&#22825;&#26426;&#22120;&#20154;&#26469;&#35299;&#20915;&#65292;&#20294;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#35780;&#20272;&#20102;&#26032;&#36817;&#27969;&#34892;&#30340;RAG-Fusion&#26041;&#27861;&#30340;&#20351;&#29992;&#12290;RAG-Fusion&#23558;RAG&#21644;&#20114;&#24800;&#25490;&#21517;&#34701;&#21512;&#65288;RRF&#65289;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#29983;&#25104;&#22810;&#20010;&#26597;&#35810;&#65292;&#20351;&#29992;&#20114;&#24800;&#20998;&#25968;&#23545;&#20854;&#36827;&#34892;&#20877;&#25490;&#24207;&#65292;&#24182;&#34701;&#21512;&#25991;&#26723;&#21644;&#20998;&#25968;&#12290;&#36890;&#36807;&#23545;&#20934;&#30830;&#24615;&#12289;&#30456;&#20851;&#24615;&#21644;&#20840;&#38754;&#24615;&#36827;&#34892;&#25163;&#21160;&#35780;&#20272;&#65292;&#25105;&#21457;&#29616;RAG-Fusion&#33021;&#22815;&#36890;&#36807;&#20174;&#19981;&#21516;&#30340;&#35282;&#24230;&#23545;&#21407;&#22987;&#26597;&#35810;&#36827;&#34892;&#19978;&#19979;&#25991;&#21270;&#65292;&#25552;&#20379;&#20934;&#30830;&#21644;&#20840;&#38754;&#30340;&#22238;&#31572;&#12290;&#28982;&#32780;&#65292;&#24403;&#29983;&#25104;&#30340;&#26597;&#35810;&#19982;&#21407;&#22987;&#26597;&#35810;&#30340;&#30456;&#20851;&#24615;&#19981;&#36275;&#26102;&#65292;&#26377;&#20123;&#31572;&#26696;&#20559;&#31163;&#20102;&#20027;&#39064;&#12290;&#36825;&#39033;&#30740;&#31350;&#22312;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#24212;&#29992;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#65292;&#24182;&#23637;&#31034;&#20102;&#20840;&#29699;&#21644;&#21306;&#22495;&#20043;&#38388;&#30340;&#21464;&#38761;&#12290;
&lt;/p&gt;
&lt;p&gt;
Infineon has identified a need for engineers, account managers, and customers to rapidly obtain product information. This problem is traditionally addressed with retrieval-augmented generation (RAG) chatbots, but in this study, I evaluated the use of the newly popularized RAG-Fusion method. RAG-Fusion combines RAG and reciprocal rank fusion (RRF) by generating multiple queries, reranking them with reciprocal scores and fusing the documents and scores. Through manually evaluating answers on accuracy, relevance, and comprehensiveness, I found that RAG-Fusion was able to provide accurate and comprehensive answers due to the generated queries contextualizing the original query from various perspectives. However, some answers strayed off topic when the generated queries' relevance to the original query is insufficient. This research marks significant progress in artificial intelligence (AI) and natural language processing (NLP) applications and demonstrates transformations in a global and m
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20840;&#38754;&#35780;&#20272;&#20102;LLMs&#22312;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#21644;&#25512;&#29702;&#39046;&#22495;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;GPT-4&#26356;&#36866;&#21512;&#20316;&#20026;&#25512;&#29702;&#21161;&#25163;&#65292;&#24182;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#36229;&#36234;&#20102;&#31934;&#35843;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2305.13168</link><description>&lt;p&gt;
LLMs&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#21644;&#25512;&#29702;&#65306;&#26368;&#26032;&#21151;&#33021;&#19982;&#26410;&#26469;&#26426;&#36935;
&lt;/p&gt;
&lt;p&gt;
LLMs for Knowledge Graph Construction and Reasoning: Recent Capabilities and Future Opportunities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.13168
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20840;&#38754;&#35780;&#20272;&#20102;LLMs&#22312;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#21644;&#25512;&#29702;&#39046;&#22495;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;GPT-4&#26356;&#36866;&#21512;&#20316;&#20026;&#25512;&#29702;&#21161;&#25163;&#65292;&#24182;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#36229;&#36234;&#20102;&#31934;&#35843;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#26500;&#24314;&#21644;&#25512;&#29702;&#20013;&#30340;&#25968;&#37327;&#21270;&#21644;&#36136;&#21270;&#35780;&#20272;&#36827;&#34892;&#20102;&#35814;&#23613;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#22312;&#20843;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#37325;&#28857;&#20851;&#27880;&#28085;&#30422;&#23454;&#20307;&#21644;&#20851;&#31995;&#25552;&#21462;&#12289;&#20107;&#20214;&#25552;&#21462;&#12289;&#38142;&#25509;&#39044;&#27979;&#21644;&#38382;&#31572;&#22235;&#20010;&#20856;&#22411;&#20219;&#21153;&#65292;&#20174;&#32780;&#20840;&#38754;&#25506;&#32034;&#20102;LLMs&#22312;&#26500;&#24314;&#21644;&#25512;&#29702;&#39046;&#22495;&#30340;&#34920;&#29616;&#12290;&#32463;&#39564;&#24615;&#30740;&#31350;&#21457;&#29616;&#65292;&#20197;GPT-4&#20026;&#20195;&#34920;&#30340;LLMs&#26356;&#36866;&#21512;&#20316;&#20026;&#25512;&#29702;&#21161;&#25163;&#65292;&#32780;&#19981;&#26159;&#23569;&#26679;&#26412;&#20449;&#24687;&#25552;&#21462;&#22120;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#34429;&#28982;GPT-4&#22312;&#19982;KG&#26500;&#24314;&#30456;&#20851;&#30340;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#26356;&#20986;&#33394;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#36229;&#36234;&#20102;&#31934;&#35843;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#35843;&#26597;&#36824;&#25193;&#23637;&#21040;LLMs&#22312;&#20449;&#24687;&#25552;&#21462;&#26041;&#38754;&#30340;&#28508;&#22312;&#27867;&#21270;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#34394;&#25311;&#30693;&#35782;&#25552;&#21462;&#30340;&#26500;&#24819;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2305.13168v2 Announce Type: replace-cross  Abstract: This paper presents an exhaustive quantitative and qualitative evaluation of Large Language Models (LLMs) for Knowledge Graph (KG) construction and reasoning. We engage in experiments across eight diverse datasets, focusing on four representative tasks encompassing entity and relation extraction, event extraction, link prediction, and question-answering, thereby thoroughly exploring LLMs' performance in the domain of construction and inference. Empirically, our findings suggest that LLMs, represented by GPT-4, are more suited as inference assistants rather than few-shot information extractors. Specifically, while GPT-4 exhibits good performance in tasks related to KG construction, it excels further in reasoning tasks, surpassing fine-tuned models in certain cases. Moreover, our investigation extends to the potential generalization ability of LLMs for information extraction, leading to the proposition of a Virtual Knowledge Extr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;E5&#30340;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#36890;&#36807;&#24369;&#30417;&#30563;&#23545;&#27604;&#35757;&#32451;&#26041;&#24335;&#65292;&#22312;&#26410;&#32463;&#36807;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#34920;&#29616;&#21331;&#36234;&#65292;&#26159;&#31532;&#19968;&#20010;&#22312;BEIR&#26816;&#32034;&#22522;&#20934;&#27979;&#35797;&#19978;&#20987;&#36133;BM25&#22522;&#32447;&#30340;&#27169;&#22411;&#65292;&#22312;&#24494;&#35843;&#21518;&#22312;MTEB&#22522;&#20934;&#27979;&#35797;&#19978;&#33719;&#24471;&#26368;&#20339;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2212.03533</link><description>&lt;p&gt;
&#29992;&#24369;&#30417;&#30563;&#23545;&#27604;&#39044;&#35757;&#32451;&#36827;&#34892;&#25991;&#26412;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Text Embeddings by Weakly-Supervised Contrastive Pre-training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2212.03533
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;E5&#30340;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#36890;&#36807;&#24369;&#30417;&#30563;&#23545;&#27604;&#35757;&#32451;&#26041;&#24335;&#65292;&#22312;&#26410;&#32463;&#36807;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#34920;&#29616;&#21331;&#36234;&#65292;&#26159;&#31532;&#19968;&#20010;&#22312;BEIR&#26816;&#32034;&#22522;&#20934;&#27979;&#35797;&#19978;&#20987;&#36133;BM25&#22522;&#32447;&#30340;&#27169;&#22411;&#65292;&#22312;&#24494;&#35843;&#21518;&#22312;MTEB&#22522;&#20934;&#27979;&#35797;&#19978;&#33719;&#24471;&#26368;&#20339;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;E5&#65292;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#21487;&#20197;&#24456;&#22909;&#22320;&#36801;&#31227;&#21040;&#21508;&#31181;&#20219;&#21153;&#20013;&#12290;&#35813;&#27169;&#22411;&#20197;&#23545;&#27604;&#26041;&#24335;&#35757;&#32451;&#65292;&#20351;&#29992;&#25105;&#20204;&#31934;&#24515;&#31574;&#21010;&#30340;&#22823;&#35268;&#27169;&#25991;&#26412;&#37197;&#23545;&#25968;&#25454;&#38598;&#65288;&#21517;&#20026;CCPairs&#65289;&#30340;&#24369;&#30417;&#30563;&#20449;&#21495;&#12290;E5&#21487;&#20197;&#20316;&#20026;&#36890;&#29992;&#23884;&#20837;&#27169;&#22411;&#29992;&#20110;&#20219;&#20309;&#38656;&#35201;&#21333;&#19968;&#25991;&#26412;&#21521;&#37327;&#34920;&#31034;&#30340;&#20219;&#21153;&#65292;&#22914;&#26816;&#32034;&#12289;&#32858;&#31867;&#21644;&#20998;&#31867;&#65292;&#22312;&#38646;-shot&#21644;&#24494;&#35843;&#35774;&#32622;&#19979;&#34920;&#29616;&#20986;&#33394;&#12290;&#25105;&#20204;&#22312;BEIR&#21644;MTEB&#22522;&#20934;&#27979;&#35797;&#30340;56&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#12290;&#22312;&#38646;-shot&#35774;&#32622;&#19979;&#65292;E5&#26159;&#31532;&#19968;&#20010;&#22312;BEIR&#26816;&#32034;&#22522;&#20934;&#27979;&#35797;&#19978;&#20987;&#36133;&#24378;&#22823;&#30340;BM25&#22522;&#32447;&#19988;&#19981;&#20351;&#29992;&#20219;&#20309;&#26631;&#35760;&#25968;&#25454;&#30340;&#27169;&#22411;&#12290;&#22312;&#24494;&#35843;&#21518;&#65292;E5&#22312;MTEB&#22522;&#20934;&#27979;&#35797;&#19978;&#21462;&#24471;&#20102;&#26368;&#20339;&#32467;&#26524;&#65292;&#32988;&#36807;&#20855;&#26377;40&#20493;&#21442;&#25968;&#30340;&#29616;&#26377;&#23884;&#20837;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2212.03533v2 Announce Type: replace  Abstract: This paper presents E5, a family of state-of-the-art text embeddings that transfer well to a wide range of tasks. The model is trained in a contrastive manner with weak supervision signals from our curated large-scale text pair dataset (called CCPairs). E5 can be readily used as a general-purpose embedding model for any tasks requiring a single-vector representation of texts such as retrieval, clustering, and classification, achieving strong performance in both zero-shot and fine-tuned settings. We conduct extensive evaluations on 56 datasets from the BEIR and MTEB benchmarks. For zero-shot settings, E5 is the first model that outperforms the strong BM25 baseline on the BEIR retrieval benchmark without using any labeled data. When fine-tuned, E5 obtains the best results on the MTEB benchmark, beating existing embedding models with 40x more parameters.
&lt;/p&gt;</description></item><item><title>Re4&#26694;&#26550;&#20351;&#29992;&#21453;&#21521;&#27969;&#37325;&#26032;&#23457;&#35270;&#27599;&#20010;&#20852;&#36259;&#23884;&#20837;&#65292;&#21253;&#25324;Re-contrast&#23545;&#20852;&#36259;&#23884;&#20837;&#36827;&#34892;&#23545;&#27604;&#23398;&#20064;&#20351;&#20854;&#24444;&#27492;&#21306;&#20998;&#24320;&#65292;Re-attend&#30830;&#20445;&#20852;&#36259;-&#39033;&#30446;&#30456;&#20851;&#24615;&#20272;&#35745;</title><link>https://arxiv.org/abs/2208.08011</link><description>&lt;p&gt;
Re4: &#23398;&#20064;&#37325;&#26032;&#23545;&#27604;&#12289;&#37325;&#26032;&#20851;&#27880;&#12289;&#37325;&#26032;&#26500;&#24314;&#20197;&#29992;&#20110;&#22810;&#20852;&#36259;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Re4: Learning to Re-contrast, Re-attend, Re-construct for Multi-interest Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2208.08011
&lt;/p&gt;
&lt;p&gt;
Re4&#26694;&#26550;&#20351;&#29992;&#21453;&#21521;&#27969;&#37325;&#26032;&#23457;&#35270;&#27599;&#20010;&#20852;&#36259;&#23884;&#20837;&#65292;&#21253;&#25324;Re-contrast&#23545;&#20852;&#36259;&#23884;&#20837;&#36827;&#34892;&#23545;&#27604;&#23398;&#20064;&#20351;&#20854;&#24444;&#27492;&#21306;&#20998;&#24320;&#65292;Re-attend&#30830;&#20445;&#20852;&#36259;-&#39033;&#30446;&#30456;&#20851;&#24615;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#22320;&#34920;&#31034;&#29992;&#25143;&#26159;&#29616;&#20195;&#25512;&#33616;&#31995;&#32479;&#30340;&#26680;&#24515;&#12290;&#30001;&#20110;&#29992;&#25143;&#30340;&#20852;&#36259;&#33258;&#28982;&#22320;&#23637;&#29616;&#20986;&#22810;&#20010;&#26041;&#38754;&#65292;&#21457;&#23637;&#22810;&#20852;&#36259;&#25512;&#33616;&#26694;&#26550;&#32780;&#19981;&#26159;&#29992;&#25972;&#20307;&#23884;&#20837;&#34920;&#31034;&#27599;&#20010;&#29992;&#25143;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#29616;&#26377;&#26041;&#27861;&#34429;&#28982;&#26377;&#25928;&#65292;&#21364;&#20165;&#21033;&#29992;&#32534;&#30721;&#22120;&#65288;&#21069;&#21521;&#27969;&#65289;&#26469;&#34920;&#31034;&#20852;&#36259;&#30340;&#22810;&#20010;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#26126;&#30830;&#30340;&#27491;&#21017;&#21270;&#65292;&#20852;&#36259;&#23884;&#20837;&#21487;&#33021;&#24444;&#27492;&#19981;&#26126;&#26174;&#21306;&#20998;&#65292;&#20063;&#19981;&#33021;&#35821;&#20041;&#19978;&#21453;&#26144;&#20195;&#34920;&#24615;&#21382;&#21490;&#39033;&#30446;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Re4&#26694;&#26550;&#65292;&#23427;&#21033;&#29992;&#21453;&#21521;&#27969;&#37325;&#26032;&#23457;&#35270;&#27599;&#20010;&#20852;&#36259;&#23884;&#20837;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;Re4&#21253;&#21547;&#19977;&#20010;&#21453;&#21521;&#27969;&#65292;&#21363;1&#65289;Re-contrast&#65292;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#39537;&#20351;&#27599;&#20010;&#20852;&#36259;&#23884;&#20837;&#19982;&#20854;&#20182;&#20852;&#36259;&#21306;&#20998;&#24320;&#26469;&#65307;2&#65289;Re-attend&#65292;&#30830;&#20445;&#20852;&#36259;-&#39033;&#30446;&#30456;&#20851;&#24615;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
arXiv:2208.08011v2 Announce Type: replace  Abstract: Effectively representing users lie at the core of modern recommender systems. Since users' interests naturally exhibit multiple aspects, it is of increasing interest to develop multi-interest frameworks for recommendation, rather than represent each user with an overall embedding. Despite their effectiveness, existing methods solely exploit the encoder (the forward flow) to represent multiple aspects of interests. However, without explicit regularization, the interest embeddings may not be distinct from each other nor semantically reflect representative historical items. Towards this end, we propose the Re4 framework, which leverages the backward flow to reexamine each interest embedding. Specifically, Re4 encapsulates three backward flows, i.e., 1) Re-contrast, which drives each interest embedding to be distinct from other interests using contrastive learning; 2) Re-attend, which ensures the interest-item correlation estimation in t
&lt;/p&gt;</description></item><item><title>&#25512;&#33616;&#31995;&#32479;&#22312;&#20154;&#31867;&#20013;&#24515;&#30340;&#20154;&#24037;&#26234;&#33021;&#20013;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#20294;&#21487;&#33021;&#24102;&#26469;&#20449;&#20219;&#38382;&#39064;&#12289;&#19981;&#20844;&#24179;&#24453;&#36935;&#21644;&#38544;&#31169;&#25285;&#24551;&#31561;&#36127;&#38754;&#24433;&#21709;&#65292;&#22240;&#27492;&#26377;&#24517;&#35201;&#24320;&#21457;&#20540;&#24471;&#20449;&#36182;&#30340;&#25512;&#33616;&#31995;&#32479;&#26469;&#20943;&#36731;&#36825;&#20123;&#39118;&#38505;&#12290;</title><link>https://arxiv.org/abs/2207.12515</link><description>&lt;p&gt;
&#19968;&#39033;&#20851;&#20110;&#20540;&#24471;&#20449;&#36182;&#25512;&#33616;&#31995;&#32479;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey on Trustworthy Recommender Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2207.12515
&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#22312;&#20154;&#31867;&#20013;&#24515;&#30340;&#20154;&#24037;&#26234;&#33021;&#20013;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#20294;&#21487;&#33021;&#24102;&#26469;&#20449;&#20219;&#38382;&#39064;&#12289;&#19981;&#20844;&#24179;&#24453;&#36935;&#21644;&#38544;&#31169;&#25285;&#24551;&#31561;&#36127;&#38754;&#24433;&#21709;&#65292;&#22240;&#27492;&#26377;&#24517;&#35201;&#24320;&#21457;&#20540;&#24471;&#20449;&#36182;&#30340;&#25512;&#33616;&#31995;&#32479;&#26469;&#20943;&#36731;&#36825;&#20123;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#65288;RS&#65289;&#20316;&#20026;AI&#20013;&#20154;&#31867;&#20013;&#24515;&#30340;&#21069;&#27839;&#26381;&#21153;&#65292;&#22312;&#32593;&#32476;&#30340;&#20960;&#20046;&#27599;&#20010;&#35282;&#33853;&#34987;&#24191;&#27867;&#37096;&#32626;&#65292;&#24182;&#20419;&#36827;&#20154;&#31867;&#20915;&#31574;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#20854;&#24040;&#22823;&#30340;&#33021;&#21147;&#21644;&#28508;&#21147;&#65292;RS&#20063;&#21487;&#33021;&#23545;&#29992;&#25143;&#12289;&#29289;&#21697;&#12289;&#29983;&#20135;&#32773;&#12289;&#24179;&#21488;&#29978;&#33267;&#25972;&#20010;&#31038;&#20250;&#20135;&#29983;&#19981;&#33391;&#24433;&#21709;&#65292;&#27604;&#22914;&#30001;&#20110;&#32570;&#20047;&#36879;&#26126;&#24230;&#32780;&#23548;&#33268;&#29992;&#25143;&#20449;&#20219;&#21463;&#25439;&#12289;&#23545;&#19981;&#21516;&#28040;&#36153;&#32773;&#25110;&#29983;&#20135;&#32773;&#30340;&#19981;&#20844;&#24179;&#23545;&#24453;&#12289;&#30001;&#20110;&#24191;&#27867;&#20351;&#29992;&#29992;&#25143;&#31169;&#20154;&#25968;&#25454;&#36827;&#34892;&#20010;&#24615;&#21270;&#32780;&#24341;&#36215;&#30340;&#38544;&#31169;&#38382;&#39064;&#65292;&#31561;&#31561;&#12290;&#25152;&#26377;&#36825;&#20123;&#37117;&#36843;&#20999;&#38656;&#35201;&#20540;&#24471;&#20449;&#36182;&#30340;&#25512;&#33616;&#31995;&#32479;&#65288;TRS&#65289;&#26469;&#20943;&#36731;&#25110;&#36991;&#20813;&#36825;&#20123;&#36127;&#38754;&#24433;&#21709;&#21644;&#39118;&#38505;&#12290;&#22312;&#26412;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#23558;&#20171;&#32461;&#19982;&#20540;&#24471;&#20449;&#36182;&#25512;&#33616;&#30456;&#20851;&#30340;&#25216;&#26415;&#65292;&#21253;&#25324;&#20294;&#19981;&#38480;&#20110;&#21487;&#35299;&#37322;&#25512;&#33616;&#12289;&#20844;&#24179;&#25512;&#33616;&#12289;&#38544;&#31169;&#24863;&#30693;&#25512;&#33616;&#12289;&#25512;&#33616;&#30340;&#40065;&#26834;&#24615;&#12289;&#29992;&#25143;&#21487;&#25511;&#21046;&#30340;&#25512;&#33616;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2207.12515v2 Announce Type: replace  Abstract: Recommender systems (RS), serving at the forefront of Human-centered AI, are widely deployed in almost every corner of the web and facilitate the human decision-making process. However, despite their enormous capabilities and potential, RS may also lead to undesired effects on users, items, producers, platforms, or even the society at large, such as compromised user trust due to non-transparency, unfair treatment of different consumers, or producers, privacy concerns due to extensive use of user's private data for personalization, just to name a few. All of these create an urgent need for Trustworthy Recommender Systems (TRS) so as to mitigate or avoid such adverse impacts and risks. In this survey, we will introduce techniques related to trustworthy recommendation, including but not limited to explainable recommendation, fairness in recommendation, privacy-aware recommendation, robustness in recommendation, user-controllable recomme
&lt;/p&gt;</description></item><item><title>MuGI&#26159;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22810;&#25991;&#26412;&#29983;&#25104;&#38598;&#25104;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21512;&#20316;&#29983;&#25104;&#22810;&#20010;&#20266;&#21442;&#32771;&#25991;&#29486;&#65292;&#24182;&#23558;&#20854;&#19982;&#26597;&#35810;&#38598;&#25104;&#20197;&#25552;&#21319;&#20449;&#24687;&#26816;&#32034;&#24615;&#33021;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;MuGI&#27169;&#22411;&#22312;TREC DL&#25968;&#25454;&#38598;&#19978;&#30340;BM25&#24615;&#33021;&#19978;&#21462;&#24471;&#20102;18%&#20197;&#19978;&#30340;&#22686;&#24378;&#65292;&#24182;&#22312;BEIR&#19978;&#25552;&#39640;&#20102;7.5%&#12290;</title><link>http://arxiv.org/abs/2401.06311</link><description>&lt;p&gt;
MuGI:&#36890;&#36807;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#25991;&#26412;&#29983;&#25104;&#38598;&#25104;&#22686;&#24378;&#20449;&#24687;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
MuGI: Enhancing Information Retrieval through Multi-Text Generation Intergration with Large Language Models. (arXiv:2401.06311v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06311
&lt;/p&gt;
&lt;p&gt;
MuGI&#26159;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22810;&#25991;&#26412;&#29983;&#25104;&#38598;&#25104;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21512;&#20316;&#29983;&#25104;&#22810;&#20010;&#20266;&#21442;&#32771;&#25991;&#29486;&#65292;&#24182;&#23558;&#20854;&#19982;&#26597;&#35810;&#38598;&#25104;&#20197;&#25552;&#21319;&#20449;&#24687;&#26816;&#32034;&#24615;&#33021;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;MuGI&#27169;&#22411;&#22312;TREC DL&#25968;&#25454;&#38598;&#19978;&#30340;BM25&#24615;&#33021;&#19978;&#21462;&#24471;&#20102;18%&#20197;&#19978;&#30340;&#22686;&#24378;&#65292;&#24182;&#22312;BEIR&#19978;&#25552;&#39640;&#20102;7.5%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24050;&#32463;&#25104;&#20026;&#35821;&#35328;&#25216;&#26415;&#39046;&#22495;&#30340;&#19968;&#20010;&#37325;&#35201;&#21147;&#37327;&#12290;&#23427;&#20204;&#24378;&#22823;&#30340;&#25512;&#29702;&#33021;&#21147;&#21644;&#24191;&#27867;&#30340;&#30693;&#35782;&#24211;&#20351;&#20854;&#22312;&#21508;&#20010;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#65292;&#21253;&#25324;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#26041;&#38754;&#20855;&#22791;&#20102;&#20986;&#33394;&#30340;&#38646;-shot&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;LLM&#29983;&#25104;&#30340;&#25991;&#26723;&#22312;IR&#20013;&#30340;&#23454;&#29992;&#24615;&#36827;&#34892;&#20102;&#28145;&#20837;&#30740;&#31350;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;&#21363;&#22810;&#25991;&#26412;&#29983;&#25104;&#38598;&#25104;&#65288;MuGI&#65289;&#65292;&#26469;&#22686;&#24378;&#29616;&#26377;&#30340;IR&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#23548;LLM&#29983;&#25104;&#22810;&#20010;&#20266;&#21442;&#32771;&#25991;&#29486;&#65292;&#24182;&#23558;&#20854;&#19982;&#26597;&#35810;&#36827;&#34892;&#38598;&#25104;&#20197;&#36827;&#34892;&#26816;&#32034;&#12290;&#26080;&#38656;&#35757;&#32451;&#30340;MuGI&#27169;&#22411;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#26597;&#35810;&#25193;&#23637;&#31574;&#30053;&#65292;&#22312;TREC DL&#25968;&#25454;&#38598;&#19978;&#30340;BM25&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26631;&#20934;&#65292;&#24182;&#22312;BEIR&#19978;&#25552;&#39640;&#20102;7.5%&#12290;&#36890;&#36807;MuGI&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#24555;&#36895;&#19988;&#39640;&#20445;&#30495;&#24230;&#30340;&#37325;&#25490;&#24207;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have emerged as a pivotal force in language technology. Their robust reasoning capabilities and expansive knowledge repositories have enabled exceptional zero-shot generalization abilities across various facets of the natural language processing field, including information retrieval (IR). In this paper, we conduct an in-depth investigation into the utility of documents generated by LLMs for IR. We introduce a simple yet effective framework, Multi-Text Generation Integration (MuGI), to augment existing IR methodologies. Specifically, we prompt LLMs to generate multiple pseudo references and integrate with query for retrieval. The training-free MuGI model eclipses existing query expansion strategies, setting a new standard in sparse retrieval. It outstrips supervised counterparts like ANCE and DPR, achieving a notable over 18% enhancement in BM25 on the TREC DL dataset and a 7.5% increase on BEIR. Through MuGI, we have forged a rapid and high-fidelity re-ran
&lt;/p&gt;</description></item><item><title>TREC iKAT 2023&#26159;&#19968;&#20010;&#20132;&#20114;&#24335;&#30340;&#30693;&#35782;&#36741;&#21161;&#20219;&#21153;&#65292;&#26088;&#22312;&#24320;&#21457;&#36866;&#24212;&#29992;&#25143;&#20132;&#20114;&#21644;&#19978;&#19979;&#25991;&#30340;&#20250;&#35805;&#25628;&#32034;&#20195;&#29702;&#12290;&#35813;&#20219;&#21153;&#36824;&#24378;&#35843;&#20915;&#31574;&#25628;&#32034;&#20219;&#21153;&#65292;&#29992;&#25143;&#36890;&#36807;&#31579;&#36873;&#25968;&#25454;&#21644;&#20449;&#24687;&#26469;&#36827;&#34892;&#20915;&#31574;&#21644;&#25191;&#34892;&#21160;&#20316;&#12290;</title><link>http://arxiv.org/abs/2401.01330</link><description>&lt;p&gt;
TREC iKAT 2023: &#20132;&#20114;&#24335;&#30693;&#35782;&#36741;&#21161;&#20219;&#21153;&#27010;&#36848;
&lt;/p&gt;
&lt;p&gt;
TREC iKAT 2023: The Interactive Knowledge Assistance Track Overview. (arXiv:2401.01330v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01330
&lt;/p&gt;
&lt;p&gt;
TREC iKAT 2023&#26159;&#19968;&#20010;&#20132;&#20114;&#24335;&#30340;&#30693;&#35782;&#36741;&#21161;&#20219;&#21153;&#65292;&#26088;&#22312;&#24320;&#21457;&#36866;&#24212;&#29992;&#25143;&#20132;&#20114;&#21644;&#19978;&#19979;&#25991;&#30340;&#20250;&#35805;&#25628;&#32034;&#20195;&#29702;&#12290;&#35813;&#20219;&#21153;&#36824;&#24378;&#35843;&#20915;&#31574;&#25628;&#32034;&#20219;&#21153;&#65292;&#29992;&#25143;&#36890;&#36807;&#31579;&#36873;&#25968;&#25454;&#21644;&#20449;&#24687;&#26469;&#36827;&#34892;&#20915;&#31574;&#21644;&#25191;&#34892;&#21160;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20250;&#35805;&#24335;&#20449;&#24687;&#26597;&#35810;&#26159;&#19968;&#20010;&#20851;&#38190;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#20043;&#21069;&#30340;&#24037;&#20316;&#20063;&#26377;&#24456;&#22823;&#30340;&#36129;&#29486;&#12290;TREC&#20132;&#20114;&#24335;&#30693;&#35782;&#36741;&#21161;&#20219;&#21153;&#65288;iKAT&#65289;&#24314;&#31435;&#22312;TREC&#20250;&#35805;&#36741;&#21161;&#20219;&#21153;&#65288;CAsT&#65289;&#30340;&#22522;&#30784;&#19978;&#12290;&#28982;&#32780;&#65292;iKAT&#30528;&#37325;&#20110;&#21019;&#24314;&#21644;&#30740;&#31350;&#21487;&#20197;&#26681;&#25454;&#29992;&#25143;&#20043;&#21069;&#30340;&#20132;&#20114;&#21644;&#24403;&#21069;&#24773;&#22659;&#33258;&#36866;&#24212;&#21709;&#24212;&#30340;&#20250;&#35805;&#25628;&#32034;&#20195;&#29702;&#12290;&#25361;&#25112;&#22312;&#20110;&#20351;&#20250;&#35805;&#25628;&#32034;&#20195;&#29702;&#33021;&#22815;&#23558;&#20010;&#24615;&#21270;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#34701;&#20837;&#21040;&#30456;&#24212;&#20013;&#65292;&#20197;&#39640;&#25928;&#22320;&#24341;&#23548;&#29992;&#25143;&#33719;&#21462;&#30456;&#20851;&#20449;&#24687;&#12290;iKAT&#36824;&#30528;&#37325;&#20110;&#20915;&#31574;&#25628;&#32034;&#20219;&#21153;&#65292;&#21363;&#29992;&#25143;&#36890;&#36807;&#25968;&#25454;&#21644;&#20449;&#24687;&#31579;&#36873;&#26469;&#34913;&#37327;&#21508;&#31181;&#36873;&#25321;&#65292;&#20197;&#36798;&#21040;&#32467;&#35770;&#25110;&#25191;&#34892;&#21160;&#20316;&#12290;&#36825;&#20123;&#20219;&#21153;&#22312;&#26085;&#24120;&#20449;&#24687;&#25628;&#32034;&#20915;&#31574;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#26080;&#35770;&#26159;&#26053;&#28216;&#12289;&#20581;&#24247;&#36824;&#26159;&#36141;&#29289;&#31561;&#65292;&#36890;&#24120;&#28041;&#21450;&#19968;&#32452;&#39640;&#32423;&#20449;&#24687;&#25805;&#20316;&#31526;&#65292;&#20854;&#20013;&#26597;&#35810;&#25110;&#38382;&#39064;&#21487;&#33021;&#20250;
&lt;/p&gt;
&lt;p&gt;
Conversational Information Seeking stands as a pivotal research area with significant contributions from previous works. The TREC Interactive Knowledge Assistance Track (iKAT) builds on the foundational work of the TREC Conversational Assistance Track (CAsT). However, iKAT distinctively emphasizes the creation and research of conversational search agents that adapt responses based on user's prior interactions and present context. The challenge lies in enabling Conversational Search Agents (CSA) to incorporate this personalized context to efficiency and effectively guide users through the relevant information to them. iKAT also emphasizes decisional search tasks, where users sift through data and information to weigh up options in order to reach a conclusion or perform an action. These tasks, prevalent in everyday information-seeking decisions -- be it related to travel, health, or shopping -- often revolve around a subset of high-level information operators where queries or questions a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;CLLM4Rec&#65292;&#39318;&#20010;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#25512;&#33616;&#31995;&#32479;&#30340; ID &#27169;&#24335;&#32039;&#23494;&#38598;&#25104;&#30340;&#21327;&#21516;&#25512;&#33616;&#31639;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#35821;&#20041;&#24046;&#36317;&#12289;&#34394;&#20551;&#30456;&#20851;&#21644;&#20302;&#25928;&#25512;&#33616;&#31561;&#38382;&#39064;&#12290;&#36890;&#36807;&#25193;&#23637;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#35789;&#27719;&#34920;&#65292;&#24182;&#24341;&#20837;&#36719;&#30828;&#25552;&#31034;&#31574;&#30053;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#20934;&#30830;&#22320;&#27169;&#25311;&#29992;&#25143;&#21644;&#39033;&#30446;&#30340;&#21327;&#21516;&#19982;&#20869;&#23481;&#35821;&#20041;&#12290;</title><link>http://arxiv.org/abs/2311.01343</link><description>&lt;p&gt;
&#21327;&#21516;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Collaborative Large Language Model for Recommender Systems. (arXiv:2311.01343v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01343
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;CLLM4Rec&#65292;&#39318;&#20010;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#25512;&#33616;&#31995;&#32479;&#30340; ID &#27169;&#24335;&#32039;&#23494;&#38598;&#25104;&#30340;&#21327;&#21516;&#25512;&#33616;&#31639;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#35821;&#20041;&#24046;&#36317;&#12289;&#34394;&#20551;&#30456;&#20851;&#21644;&#20302;&#25928;&#25512;&#33616;&#31561;&#38382;&#39064;&#12290;&#36890;&#36807;&#25193;&#23637;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#35789;&#27719;&#34920;&#65292;&#24182;&#24341;&#20837;&#36719;&#30828;&#25552;&#31034;&#31574;&#30053;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#20934;&#30830;&#22320;&#27169;&#25311;&#29992;&#25143;&#21644;&#39033;&#30446;&#30340;&#21327;&#21516;&#19982;&#20869;&#23481;&#35821;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#23545;&#22522;&#20110;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24320;&#21457;&#19979;&#19968;&#20195;&#25512;&#33616;&#31995;&#32479;&#65288;RS&#65289;&#20135;&#29983;&#20102;&#20852;&#36259;&#65292;&#20805;&#20998;&#21033;&#29992;&#20854;&#32534;&#30721;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#33258;&#28982;&#35821;&#35328;&#19982;&#25512;&#33616;&#20219;&#21153;&#20043;&#38388;&#30340;&#35821;&#20041;&#24046;&#36317;&#20173;&#26410;&#24471;&#21040;&#24456;&#22909;&#30340;&#35299;&#20915;&#65292;&#23548;&#33268;&#19968;&#20123;&#38382;&#39064;&#65292;&#22914;&#34394;&#20551;&#30456;&#20851;&#30340;&#29992;&#25143;/&#39033;&#30446;&#25551;&#36848;&#31526;&#12289;&#23545;&#29992;&#25143;/&#39033;&#30446;&#20869;&#23481;&#30340;&#20302;&#25928;&#35821;&#35328;&#24314;&#27169;&#20197;&#21450;&#36890;&#36807;&#33258;&#21160;&#22238;&#24402;&#36827;&#34892;&#20302;&#25928;&#30340;&#25512;&#33616;&#31561;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CLLM4Rec&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#32039;&#23494;&#38598;&#25104;LLM&#33539;&#24335;&#21644;RS&#30340;ID&#33539;&#24335;&#30340;&#29983;&#25104;RS&#65292;&#26088;&#22312;&#21516;&#26102;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#12290;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#29992;&#25143;/&#39033;&#30446;ID&#26631;&#35760;&#25193;&#23637;&#20102;&#39044;&#35757;&#32451;LLM&#30340;&#35789;&#27719;&#34920;&#65292;&#20197;&#24544;&#23454;&#22320;&#27169;&#25311;&#29992;&#25143;/&#39033;&#30446;&#30340;&#21327;&#21516;&#21644;&#20869;&#23481;&#35821;&#20041;&#12290;&#22240;&#27492;&#65292;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36719;&#30828;&#25552;&#31034;&#31574;&#30053;&#65292;&#36890;&#36807;&#35821;&#35328;&#24314;&#27169;&#26377;&#25928;&#22320;&#23398;&#20064;&#29992;&#25143;/&#39033;&#30446;&#30340;&#21327;&#21516;/&#20869;&#23481;&#26631;&#35760;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, there is a growing interest in developing next-generation recommender systems (RSs) based on pretrained large language models (LLMs), fully utilizing their encoded knowledge and reasoning ability. However, the semantic gap between natural language and recommendation tasks is still not well addressed, leading to multiple issues such as spuriously-correlated user/item descriptors, ineffective language modeling on user/item contents, and inefficient recommendations via auto-regression, etc. In this paper, we propose CLLM4Rec, the first generative RS that tightly integrates the LLM paradigm and ID paradigm of RS, aiming to address the above challenges simultaneously. We first extend the vocabulary of pretrained LLMs with user/item ID tokens to faithfully model the user/item collaborative and content semantics. Accordingly, in the pretraining stage, a novel soft+hard prompting strategy is proposed to effectively learn user/item collaborative/content token embeddings via language m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32467;&#26500;&#24863;&#30693;&#23884;&#20837;&#28436;&#21270;(SEvo)&#26426;&#21046;&#65292;&#33021;&#22815;&#20197;&#36739;&#20302;&#30340;&#35745;&#31639;&#24320;&#38144;&#23558;&#22270;&#32467;&#26500;&#20449;&#24687;&#27880;&#20837;&#21040;&#23884;&#20837;&#20013;&#65292;&#20174;&#32780;&#22312;&#29616;&#20195;&#25512;&#33616;&#31995;&#32479;&#20013;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.03032</link><description>&lt;p&gt;
&#22270;&#22686;&#24378;&#20248;&#21270;&#22120;&#29992;&#20110;&#32467;&#26500;&#24863;&#30693;&#25512;&#33616;&#23884;&#20837;&#28436;&#21270;
&lt;/p&gt;
&lt;p&gt;
Graph-enhanced Optimizers for Structure-aware Recommendation Embedding Evolution. (arXiv:2310.03032v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03032
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32467;&#26500;&#24863;&#30693;&#23884;&#20837;&#28436;&#21270;(SEvo)&#26426;&#21046;&#65292;&#33021;&#22815;&#20197;&#36739;&#20302;&#30340;&#35745;&#31639;&#24320;&#38144;&#23558;&#22270;&#32467;&#26500;&#20449;&#24687;&#27880;&#20837;&#21040;&#23884;&#20837;&#20013;&#65292;&#20174;&#32780;&#22312;&#29616;&#20195;&#25512;&#33616;&#31995;&#32479;&#20013;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23884;&#20837;&#22312;&#29616;&#20195;&#25512;&#33616;&#31995;&#32479;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#22240;&#20026;&#23427;&#20204;&#26159;&#30495;&#23454;&#19990;&#30028;&#23454;&#20307;&#30340;&#34394;&#25311;&#34920;&#31034;&#65292;&#24182;&#19988;&#26159;&#21518;&#32493;&#20915;&#31574;&#27169;&#22411;&#30340;&#22522;&#30784;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23884;&#20837;&#26356;&#26032;&#26426;&#21046;&#65292;&#31216;&#20026;&#32467;&#26500;&#24863;&#30693;&#23884;&#20837;&#28436;&#21270;(SEvo)&#65292;&#20197;&#40723;&#21169;&#30456;&#20851;&#33410;&#28857;&#22312;&#27599;&#19968;&#27493;&#20013;&#20197;&#31867;&#20284;&#30340;&#26041;&#24335;&#28436;&#21270;&#12290;&#19982;&#36890;&#24120;&#20316;&#20026;&#20013;&#38388;&#37096;&#20998;&#30340;GNN&#65288;&#22270;&#31070;&#32463;&#32593;&#32476;&#65289;&#19981;&#21516;&#65292;SEvo&#33021;&#22815;&#30452;&#25509;&#23558;&#22270;&#32467;&#26500;&#20449;&#24687;&#27880;&#20837;&#21040;&#23884;&#20837;&#20013;&#65292;&#19988;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#35745;&#31639;&#24320;&#38144;&#21487;&#24573;&#30053;&#12290;&#26412;&#25991;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#39564;&#35777;&#20102;SEvo&#30340;&#25910;&#25947;&#24615;&#36136;&#21450;&#20854;&#21487;&#33021;&#30340;&#25913;&#36827;&#29256;&#26412;&#65292;&#20197;&#35777;&#26126;&#35774;&#35745;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;SEvo&#21487;&#20197;&#26080;&#32541;&#38598;&#25104;&#21040;&#29616;&#26377;&#30340;&#20248;&#21270;&#22120;&#20013;&#65292;&#20197;&#23454;&#29616;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;&#29305;&#21035;&#26159;&#65292;&#22312;&#30697;&#20272;&#35745;&#26657;&#27491;&#30340;SEvo&#22686;&#24378;AdamW&#20013;&#65292;&#35777;&#26126;&#20102;&#19968;&#33268;&#30340;&#25913;&#36827;&#25928;&#26524;&#22312;&#22810;&#31181;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#19978;&#65292;&#20026;&#26377;&#25928;&#25512;&#33616;&#20102;&#19968;&#31181;&#26032;&#30340;&#25216;&#26415;&#36335;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
Embedding plays a critical role in modern recommender systems because they are virtual representations of real-world entities and the foundation for subsequent decision models. In this paper, we propose a novel embedding update mechanism, Structure-aware Embedding Evolution (SEvo for short), to encourage related nodes to evolve similarly at each step. Unlike GNN (Graph Neural Network) that typically serves as an intermediate part, SEvo is able to directly inject the graph structure information into embedding with negligible computational overhead in training. The convergence properties of SEvo as well as its possible variants are theoretically analyzed to justify the validity of the designs. Moreover, SEvo can be seamlessly integrated into existing optimizers for state-of-the-art performance. In particular, SEvo-enhanced AdamW with moment estimate correction demonstrates consistent improvements across a spectrum of models and datasets, suggesting a novel technical route to effectively 
&lt;/p&gt;</description></item><item><title>SilverRetriever&#26159;&#19968;&#20010;&#29305;&#20026;&#27874;&#20848;&#35821;&#38382;&#31572;&#31995;&#32479;&#24320;&#21457;&#30340;&#31070;&#32463;&#26816;&#32034;&#22120;&#65292;&#36890;&#36807;&#35757;&#32451;&#22312;&#22810;&#31181;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#25928;&#26524;&#65292;&#24182;&#19988;&#19982;&#26356;&#22823;&#30340;&#22810;&#35821;&#31181;&#27169;&#22411;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.08469</link><description>&lt;p&gt;
SilverRetriever&#65306;&#25552;&#21319;&#27874;&#20848;&#38382;&#31572;&#31995;&#32479;&#30340;&#31070;&#32463;&#36890;&#36947;&#26816;&#32034;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
SilverRetriever: Advancing Neural Passage Retrieval for Polish Question Answering. (arXiv:2309.08469v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08469
&lt;/p&gt;
&lt;p&gt;
SilverRetriever&#26159;&#19968;&#20010;&#29305;&#20026;&#27874;&#20848;&#35821;&#38382;&#31572;&#31995;&#32479;&#24320;&#21457;&#30340;&#31070;&#32463;&#26816;&#32034;&#22120;&#65292;&#36890;&#36807;&#35757;&#32451;&#22312;&#22810;&#31181;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#25928;&#26524;&#65292;&#24182;&#19988;&#19982;&#26356;&#22823;&#30340;&#22810;&#35821;&#31181;&#27169;&#22411;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#24320;&#25918;&#39046;&#22495;&#30340;&#38382;&#31572;&#31995;&#32479;&#36890;&#24120;&#20381;&#36182;&#20110;&#20934;&#30830;&#21644;&#39640;&#25928;&#30340;&#26816;&#32034;&#32452;&#20214;&#26469;&#25214;&#21040;&#21253;&#21547;&#22238;&#31572;&#38382;&#39064;&#25152;&#38656;&#20107;&#23454;&#30340;&#27573;&#33853;&#12290;&#36817;&#24180;&#26469;&#65292;&#30001;&#20110;&#20854;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#31070;&#32463;&#26816;&#32034;&#22120;&#27604;&#35789;&#27719;&#26367;&#20195;&#26041;&#24335;&#26356;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#22823;&#37096;&#20998;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#27969;&#34892;&#35821;&#35328;&#22914;&#33521;&#35821;&#25110;&#20013;&#25991;&#19978;&#65292;&#23545;&#20110;&#20854;&#20182;&#35821;&#35328;&#22914;&#27874;&#20848;&#35821;&#65292;&#21487;&#29992;&#30340;&#27169;&#22411;&#24456;&#23569;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;SilverRetriever&#65292;&#19968;&#20010;&#22522;&#20110;&#22810;&#31181;&#25163;&#21160;&#26631;&#35760;&#25110;&#24369;&#26631;&#35760;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#27874;&#20848;&#35821;&#31070;&#32463;&#26816;&#32034;&#22120;&#12290;SilverRetriever&#22312;&#27874;&#20848;&#35821;&#27169;&#22411;&#20013;&#21462;&#24471;&#20102;&#27604;&#20854;&#20182;&#27169;&#22411;&#26356;&#22909;&#30340;&#32467;&#26524;&#65292;&#24182;&#19982;&#26356;&#22823;&#30340;&#22810;&#35821;&#31181;&#27169;&#22411;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;&#19982;&#35813;&#27169;&#22411;&#19968;&#36215;&#65292;&#25105;&#20204;&#36824;&#24320;&#28304;&#20102;&#20116;&#20010;&#26032;&#30340;&#27573;&#33853;&#26816;&#32034;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern open-domain question answering systems often rely on accurate and efficient retrieval components to find passages containing the facts necessary to answer the question. Recently, neural retrievers have gained popularity over lexical alternatives due to their superior performance. However, most of the work concerns popular languages such as English or Chinese. For others, such as Polish, few models are available. In this work, we present SilverRetriever, a neural retriever for Polish trained on a diverse collection of manually or weakly labeled datasets. SilverRetriever achieves much better results than other Polish models and is competitive with larger multilingual models. Together with the model, we open-source five new passage retrieval datasets.
&lt;/p&gt;</description></item></channel></rss>