<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27604;&#36739;&#30340;&#24555;&#36895;&#20132;&#20114;&#24335;&#25628;&#32034;&#31639;&#27861;&#65292;&#20854;&#20013;&#20351;&#29992;&#20102;&#31216;&#20026; $\gamma$-CKL &#30340;&#21051;&#24230;&#33258;&#30001;&#27010;&#29575;&#39044;&#35328;&#27169;&#22411;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#20855;&#26377;&#25351;&#25968;&#25910;&#25947;&#36895;&#24230;&#30340;&#25628;&#32034;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.01814</link><description>&lt;p&gt;
&#22522;&#20110;&#27604;&#36739;&#30340;&#24555;&#36895;&#20132;&#20114;&#24335;&#25628;&#32034;&#31639;&#27861;&#21450;&#20854;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Fast Interactive Search with a Scale-Free Comparison Oracle. (arXiv:2306.01814v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01814
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27604;&#36739;&#30340;&#24555;&#36895;&#20132;&#20114;&#24335;&#25628;&#32034;&#31639;&#27861;&#65292;&#20854;&#20013;&#20351;&#29992;&#20102;&#31216;&#20026; $\gamma$-CKL &#30340;&#21051;&#24230;&#33258;&#30001;&#27010;&#29575;&#39044;&#35328;&#27169;&#22411;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#20855;&#26377;&#25351;&#25968;&#25910;&#25947;&#36895;&#24230;&#30340;&#25628;&#32034;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27604;&#36739;&#30340;&#25628;&#32034;&#31639;&#27861;&#21487;&#20197;&#35753;&#29992;&#25143;&#36890;&#36807;&#22238;&#31572;&#8220;&#39033; $i$ &#21644; $j$ &#21738;&#19968;&#20010;&#26356;&#25509;&#36817; $t$&#65311;&#8221;&#30340;&#26597;&#35810;&#26469;&#22312;&#25968;&#25454;&#24211;&#20013;&#25214;&#21040;&#30446;&#26631;&#39033; $t$ &#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026; $\gamma$-CKL &#30340;&#21051;&#24230;&#33258;&#30001;&#27010;&#29575;&#39044;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#34920;&#31034;&#36825;&#31181;&#30456;&#20284;&#24615;&#19977;&#20803;&#32452; $(i,j;t)$&#12290;&#36825;&#31181;&#27169;&#22411;&#22312;&#25511;&#21046;&#39044;&#35328;&#30340;&#21306;&#20998;&#33021;&#21147;&#21644;&#21253;&#21547;&#39033;&#30340;&#29305;&#24449;&#31354;&#38388;&#32500;&#24230;&#26041;&#38754;&#20855;&#26377;&#29420;&#31435;&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#25628;&#32034;&#31639;&#27861;&#65292;&#35777;&#26126;&#20102;&#22312; $\gamma$-CKL &#27169;&#22411;&#19979;&#20855;&#26377;&#25351;&#25968;&#25910;&#25947;&#36895;&#24230;&#12290;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;&#31639;&#27861;&#22312;&#20960;&#20010;&#30495;&#23454;&#19977;&#20803;&#32452;&#25968;&#25454;&#24211;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
A comparison-based search algorithm lets a user find a target item $t$ in a database by answering queries of the form, ``Which of items $i$ and $j$ is closer to $t$?'' Instead of formulating an explicit query (such as one or several keywords), the user navigates towards the target via a sequence of such (typically noisy) queries.  We propose a scale-free probabilistic oracle model called $\gamma$-CKL for such similarity triplets $(i,j;t)$, which generalizes the CKL triplet model proposed in the literature. The generalization affords independent control over the discriminating power of the oracle and the dimension of the feature space containing the items.  We develop a search algorithm with provably exponential rate of convergence under the $\gamma$-CKL oracle, thanks to a backtracking strategy that deals with the unavoidable errors in updating the belief region around the target.  We evaluate the performance of the algorithm both over the posited oracle and over several real-world tri
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#29983;&#25104;AI&#26041;&#27861;&#26469;&#25193;&#23637;&#24403;&#21069;&#30340;&#39135;&#21697;&#35745;&#31639;&#27169;&#22411;&#65292;&#23558;&#28921;&#39274;&#34892;&#20026;&#32435;&#20837;&#32771;&#34385;&#65292;&#20197;&#23454;&#29616;&#26356;&#20840;&#38754;&#30340;&#20581;&#24247;&#39135;&#35889;&#25512;&#33616;&#12290;</title><link>http://arxiv.org/abs/2306.01805</link><description>&lt;p&gt;
Cook-Gen&#65306;&#20174;&#39135;&#35889;&#20013;&#29983;&#25104;&#20581;&#24247;&#28921;&#39274;&#21160;&#20316;&#30340;&#40065;&#26834;&#24615;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Cook-Gen: Robust Generative Modeling of Cooking Actions from Recipes. (arXiv:2306.01805v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01805
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#29983;&#25104;AI&#26041;&#27861;&#26469;&#25193;&#23637;&#24403;&#21069;&#30340;&#39135;&#21697;&#35745;&#31639;&#27169;&#22411;&#65292;&#23558;&#28921;&#39274;&#34892;&#20026;&#32435;&#20837;&#32771;&#34385;&#65292;&#20197;&#23454;&#29616;&#26356;&#20840;&#38754;&#30340;&#20581;&#24247;&#39135;&#35889;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#20204;&#36234;&#26469;&#36234;&#20851;&#27880;&#33258;&#24049;&#30340;&#39278;&#39135;&#36873;&#25321;&#65292;&#39135;&#21697;&#35745;&#31639;&#27169;&#22411;&#22312;&#24110;&#21161;&#20154;&#20204;&#20445;&#25345;&#20581;&#24247;&#39278;&#39135;&#20064;&#24815;&#26041;&#38754;&#21464;&#24471;&#36234;&#26469;&#36234;&#27969;&#34892;&#12290;&#20363;&#22914;&#65292;&#39135;&#21697;&#25512;&#33616;&#31995;&#32479;&#20998;&#26512;&#39135;&#35889;&#25351;&#20196;&#20197;&#35780;&#20272;&#33829;&#20859;&#25104;&#20998;&#24182;&#25552;&#20379;&#39135;&#35889;&#25512;&#33616;&#12290;&#32780;&#29983;&#25104;AI&#26041;&#27861;&#65288;&#22914;&#33258;&#22238;&#24402;&#22823;&#35821;&#35328;&#27169;&#22411;&#65289;&#30340;&#25104;&#21151;&#24212;&#29992;&#21487;&#20197;&#35753;&#25105;&#20204;&#26356;&#20840;&#38754;&#22320;&#29702;&#35299;&#39135;&#35889;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#20026;&#20581;&#24247;&#20840;&#38754;&#30340;&#39135;&#35889;&#25512;&#33616;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20351;&#29992;&#29983;&#25104;AI&#26041;&#27861;&#26469;&#25193;&#23637;&#24403;&#21069;&#30340;&#39135;&#21697;&#35745;&#31639;&#27169;&#22411;&#65292;&#20174;&#32780;&#23558;&#28921;&#39274;&#34892;&#20026;&#65288;&#20363;&#22914;&#21152;&#30416;&#12289;&#29006;&#32905;&#12289;&#29038;&#34092;&#33756;&#31561;&#65289;&#32435;&#20837;&#32771;&#34385;&#12290;&#28921;&#39274;&#34892;&#20026;&#30001;&#20110;&#20854;&#19981;&#35268;&#21017;&#30340;&#25968;&#25454;&#27169;&#24335;&#32780;&#38590;&#20197;&#20351;&#29992;&#32479;&#35745;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
As people become more aware of their food choices, food computation models have become increasingly popular in assisting people in maintaining healthy eating habits. For example, food recommendation systems analyze recipe instructions to assess nutritional contents and provide recipe recommendations. The recent and remarkable successes of generative AI methods, such as auto-regressive large language models, can lead to robust methods for a more comprehensive understanding of recipes for healthy food recommendations beyond surface-level nutrition content assessments. In this study, we explore the use of generative AI methods to extend current food computation models, primarily involving the analysis of nutrition and ingredients, to also incorporate cooking actions (e.g., add salt, fry the meat, boil the vegetables, etc.). Cooking actions are notoriously hard to model using statistical learning methods due to irregular data patterns - significantly varying natural language descriptions f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21152;&#26435;&#25490;&#21517;&#25439;&#22833;&#20989;&#25968;&#26469;&#35757;&#32451;CTR&#27169;&#22411;&#65292;&#20197;&#23454;&#29616;&#24191;&#21578;&#25293;&#21334;&#20013;&#30340;&#31119;&#21033;&#26368;&#22823;&#21270;&#65292;&#24182;&#19988;&#27809;&#26377;&#20551;&#35774;eCPM&#30340;&#20808;&#39564;&#20998;&#24067;&#65292;&#21516;&#26102;&#36991;&#20813;&#20102;&#26420;&#32032;&#22320;&#24212;&#29992;&#29616;&#26377;&#23398;&#20064;&#25490;&#21517;&#26041;&#27861;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.01799</link><description>&lt;p&gt;
&#22312;&#24191;&#21578;&#25293;&#21334;&#20013;&#36890;&#36807;&#28857;&#20987;&#29575;&#39044;&#27979;&#26469;&#23454;&#29616;&#31119;&#21033;&#26368;&#22823;&#21270;&#30340;&#25104;&#23545;&#25490;&#21517;&#25439;&#22833;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Pairwise Ranking Losses of Click-Through Rates Prediction for Welfare Maximization in Ad Auctions. (arXiv:2306.01799v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01799
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21152;&#26435;&#25490;&#21517;&#25439;&#22833;&#20989;&#25968;&#26469;&#35757;&#32451;CTR&#27169;&#22411;&#65292;&#20197;&#23454;&#29616;&#24191;&#21578;&#25293;&#21334;&#20013;&#30340;&#31119;&#21033;&#26368;&#22823;&#21270;&#65292;&#24182;&#19988;&#27809;&#26377;&#20551;&#35774;eCPM&#30340;&#20808;&#39564;&#20998;&#24067;&#65292;&#21516;&#26102;&#36991;&#20813;&#20102;&#26420;&#32032;&#22320;&#24212;&#29992;&#29616;&#26377;&#23398;&#20064;&#25490;&#21517;&#26041;&#27861;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35774;&#35745;&#28857;&#20987;&#29575;&#65288;CTR&#65289;&#25439;&#22833;&#20989;&#25968;&#20197;&#22312;&#24191;&#21578;&#25293;&#21334;&#20013;&#20248;&#21270;&#65288;&#31038;&#20250;&#65289;&#31119;&#21033;&#12290;&#29616;&#26377;&#30340;&#30740;&#31350;&#35201;&#20040;&#21482;&#20851;&#27880;&#20110;CTR&#39044;&#27979;&#65292;&#32780;&#27809;&#26377;&#32771;&#34385;&#24191;&#21578;&#25293;&#21334;&#20013;&#30340;&#19994;&#21153;&#30446;&#26631;&#65288;&#20363;&#22914;&#31119;&#21033;&#65289;&#65292;&#35201;&#20040;&#20551;&#35774;&#21442;&#19982;&#32773;&#26399;&#26395;&#27599;&#27425;&#23637;&#31034;&#36153;&#29992;&#65288;eCPM&#65289;&#30340;&#20998;&#24067;&#26159;&#20808;&#39564;&#24050;&#30693;&#30340;&#65292;&#28982;&#21518;&#20351;&#29992;&#21508;&#31181;&#38468;&#21152;&#20551;&#35774;&#26469;&#25512;&#23548;&#29992;&#20110;&#39044;&#27979;CTR&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#24191;&#21578;&#25293;&#21334;&#30340;&#31119;&#21033;&#30446;&#26631;&#24102;&#22238;CTR&#39044;&#27979;&#20013;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#21152;&#26435;&#25490;&#21517;&#25439;&#22833;&#20989;&#25968;&#26469;&#35757;&#32451;CTR&#27169;&#22411;&#12290;&#19982;&#29616;&#26377;&#25991;&#29486;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19981;&#20551;&#35774;eCPM&#20998;&#24067;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#20102;&#23545;&#31119;&#21033;&#30340;&#21487;&#35777;&#26126;&#20445;&#35777;&#65292;&#21516;&#26102;&#36991;&#20813;&#20102;&#26420;&#32032;&#22320;&#24212;&#29992;&#29616;&#26377;&#23398;&#20064;&#25490;&#21517;&#26041;&#27861;&#26102;&#30340;&#26840;&#25163;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#8220;&#32769;&#24072;&#32593;&#32476;&#8221;&#29983;&#25104;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#26657;&#20934;&#25439;&#22833;&#30340;&#29702;&#35770;&#21487;&#35777;&#26126;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the design of loss functions for click-through rates (CTR) to optimize (social) welfare in advertising auctions. Existing works either only focus on CTR predictions without consideration of business objectives (e.g., welfare) in auctions or assume that the distribution over the participants' expected cost-per-impression (eCPM) is known a priori, then use various additional assumptions on the parametric form of the distribution to derive loss functions for predicting CTRs. In this work, we bring back the welfare objectives of ad auctions into CTR predictions and propose a novel weighted rankloss to train the CTR model. Compared to existing literature, our approach provides a provable guarantee on welfare but without assumptions on the eCPMs' distribution while also avoiding the intractability of naively applying existing learning-to-rank methods. Further, we propose a theoretically justifiable technique for calibrating the losses using labels generated from a teacher network, o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25345;&#32493;&#29992;&#25143;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;TERACON&#65292;&#23427;&#33021;&#22815;&#23398;&#20064;&#36890;&#29992;&#30340;&#29992;&#25143;&#34920;&#31034;&#65292;&#32780;&#19981;&#26159;&#20026;&#27599;&#20010;&#20219;&#21153;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#30340;&#29992;&#25143;&#34920;&#31034;&#65292;&#20855;&#26377;&#24456;&#24378;&#30340;&#23454;&#29992;&#24615;&#21644;&#23398;&#20064;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.01792</link><description>&lt;p&gt;
&#20219;&#21153;&#20851;&#31995;&#24863;&#30693;&#30340;&#25345;&#32493;&#29992;&#25143;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Task Relation-aware Continual User Representation Learning. (arXiv:2306.01792v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01792
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25345;&#32493;&#29992;&#25143;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;TERACON&#65292;&#23427;&#33021;&#22815;&#23398;&#20064;&#36890;&#29992;&#30340;&#29992;&#25143;&#34920;&#31034;&#65292;&#32780;&#19981;&#26159;&#20026;&#27599;&#20010;&#20219;&#21153;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#30340;&#29992;&#25143;&#34920;&#31034;&#65292;&#20855;&#26377;&#24456;&#24378;&#30340;&#23454;&#29992;&#24615;&#21644;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#25143;&#24314;&#27169;&#26159;&#22522;&#20110;&#20854;&#36807;&#21435;&#34892;&#20026;&#23398;&#20064;&#23558;&#29992;&#25143;&#34920;&#31034;&#20026;&#20302;&#32500;&#34920;&#31034;&#31354;&#38388;&#30340;&#26041;&#27861;&#65292;&#23427;&#21463;&#21040;&#20102;&#24037;&#19994;&#30028;&#25552;&#20379;&#20010;&#24615;&#21270;&#26381;&#21153;&#30340;&#20852;&#36259;&#28608;&#22686;&#12290;&#20197;&#24448;&#30340;&#29992;&#25143;&#24314;&#27169;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#23398;&#20064;&#20026;&#21333;&#19968;&#20219;&#21153;&#32780;&#35774;&#35745;&#30340;&#20219;&#21153;&#29305;&#23450;&#29992;&#25143;&#34920;&#31034;&#19978;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20026;&#27599;&#20010;&#20219;&#21153;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#29992;&#25143;&#34920;&#31034;&#26159;&#19981;&#21487;&#34892;&#30340;&#65292;&#22240;&#27492;&#26368;&#36817;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;&#36890;&#29992;&#29992;&#25143;&#34920;&#31034;&#30340;&#27010;&#24565;&#65292;&#21363;&#19982;&#22810;&#31181;&#20219;&#21153;&#30456;&#20851;&#30340;&#26356;&#24191;&#20041;&#29992;&#25143;&#34920;&#31034;&#12290;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#30001;&#20110;&#25968;&#25454;&#38656;&#27714;&#12289;&#28798;&#38590;&#24615;&#36951;&#24536;&#20197;&#21450;&#20026;&#25345;&#32493;&#28155;&#21152;&#30340;&#20219;&#21153;&#25552;&#20379;&#26377;&#38480;&#30340;&#23398;&#20064;&#33021;&#21147;&#65292;&#29616;&#26377;&#30340;&#23398;&#20064;&#36890;&#29992;&#29992;&#25143;&#34920;&#31034;&#30340;&#26041;&#27861;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25345;&#32493;&#29992;&#25143;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;TERACON&#65292;&#20854;&#23398;&#20064;&#33021;&#21147;&#19981;&#21463;&#20219;&#21153;&#25968;&#37327;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
User modeling, which learns to represent users into a low-dimensional representation space based on their past behaviors, got a surge of interest from the industry for providing personalized services to users. Previous efforts in user modeling mainly focus on learning a task-specific user representation that is designed for a single task. However, since learning task-specific user representations for every task is infeasible, recent studies introduce the concept of universal user representation, which is a more generalized representation of a user that is relevant to a variety of tasks. Despite their effectiveness, existing approaches for learning universal user representations are impractical in real-world applications due to the data requirement, catastrophic forgetting and the limited learning capability for continually added tasks. In this paper, we propose a novel continual user representation learning method, called TERACON, whose learning capability is not limited as the number 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;SERP&#29305;&#24449;&#23545;&#28857;&#20987;&#29575;&#30340;&#24433;&#21709;&#65292;&#25581;&#31034;SERP&#29305;&#24449;&#19981;&#20165;&#26159;&#32654;&#23398;&#25104;&#20998;&#65292;&#32780;&#19988;&#24378;&#28872;&#24433;&#21709;&#28857;&#20987;&#29575;&#21644;&#20114;&#32852;&#32593;&#29992;&#25143;&#30340;&#30456;&#20851;&#34892;&#20026;&#65292;&#33021;&#22815;&#26174;&#30528;&#35843;&#33410;&#32593;&#32476;&#27969;&#37327;&#12290;</title><link>http://arxiv.org/abs/2306.01785</link><description>&lt;p&gt;
&#36229;&#36234;&#25490;&#21517;&#65306;&#25506;&#32034;SERP&#29305;&#24449;&#23545;&#26377;&#26426;&#28857;&#20987;&#29575;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Beyond Rankings: Exploring the Impact of SERP Features on Organic Click-through Rates. (arXiv:2306.01785v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01785
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;SERP&#29305;&#24449;&#23545;&#28857;&#20987;&#29575;&#30340;&#24433;&#21709;&#65292;&#25581;&#31034;SERP&#29305;&#24449;&#19981;&#20165;&#26159;&#32654;&#23398;&#25104;&#20998;&#65292;&#32780;&#19988;&#24378;&#28872;&#24433;&#21709;&#28857;&#20987;&#29575;&#21644;&#20114;&#32852;&#32593;&#29992;&#25143;&#30340;&#30456;&#20851;&#34892;&#20026;&#65292;&#33021;&#22815;&#26174;&#30528;&#35843;&#33410;&#32593;&#32476;&#27969;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25628;&#32034;&#24341;&#25806;&#32467;&#26524;&#39029;&#38754;&#65288;SERP&#65289;&#20316;&#20026;&#36890;&#24448;&#24191;&#38420;&#20114;&#32852;&#32593;&#19990;&#30028;&#30340;&#25968;&#23383;&#38376;&#25143;&#12290;&#36807;&#21435;&#20960;&#21313;&#24180;&#20197;&#32593;&#31449;&#25490;&#21517;&#20026;&#20013;&#24515;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#36825;&#20123;&#39029;&#38754;&#19978;&#65292;&#20197;&#30830;&#23450;&#28857;&#20987;&#29575;&#65288;CTR&#65289;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#27573;&#26102;&#38388;&#20869;&#65292;SERP&#30340;&#26223;&#35266;&#21457;&#29983;&#20102;&#25103;&#21095;&#24615;&#30340;&#28436;&#21464;&#65306;SERP&#29305;&#24449;&#65292;&#21253;&#25324;&#30693;&#35782;&#38754;&#26495;&#12289;&#23186;&#20307;&#30011;&#24266;&#12289;&#24120;&#35265;&#38382;&#39064;&#35299;&#31572;&#31561;&#20803;&#32032;&#65292;&#24050;&#32463;&#25104;&#20026;&#36825;&#20123;&#32467;&#26524;&#39029;&#38754;&#20013;&#36234;&#26469;&#36234;&#31361;&#20986;&#30340;&#26041;&#38754;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#30740;&#31350;&#20102;&#36825;&#20123;&#29305;&#24449;&#30340;&#20851;&#38190;&#20316;&#29992;&#65292;&#25581;&#31034;&#23427;&#20204;&#19981;&#20165;&#26159;&#32654;&#23398;&#25104;&#20998;&#65292;&#32780;&#19988;&#24378;&#28872;&#24433;&#21709;&#28857;&#20987;&#29575;&#21644;&#20114;&#32852;&#32593;&#29992;&#25143;&#30340;&#30456;&#20851;&#34892;&#20026;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#29305;&#24449;&#22914;&#20309;&#26174;&#30528;&#35843;&#33410;&#32593;&#32476;&#27969;&#37327;&#65292;&#26080;&#35770;&#26159;&#25918;&#22823;&#36824;&#26159;&#20943;&#24369;&#23427;&#12290;&#25105;&#20204;&#20351;&#29992;&#28085;&#30422;40&#20010;&#19981;&#21516;&#32654;&#22269;&#30005;&#23376;&#21830;&#21153;&#39046;&#22495;&#30340;67,000&#20010;&#20851;&#38190;&#23383;&#21450;&#20854;&#30456;&#24212;&#30340;Google SERPs&#30340;&#29420;&#29305;&#25968;&#25454;&#38598;&#65292;&#20998;&#26512;&#20102;&#36825;&#20123;&#22797;&#26434;&#30340;&#20132;&#20114;&#20316;&#29992;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Search Engine Result Pages (SERPs) serve as the digital gateways to the vast expanse of the internet. Past decades have witnessed a surge in research primarily centered on the influence of website ranking on these pages, to determine the click-through rate (CTR). However, during this period, the landscape of SERPs has undergone a dramatic evolution: SERP features, encompassing elements such as knowledge panels, media galleries, FAQs, and more, have emerged as an increasingly prominent facet of these result pages. Our study examines the crucial role of these features, revealing them to be not merely aesthetic components, but strongly influence CTR and the associated behavior of internet users. We demonstrate how these features can significantly modulate web traffic, either amplifying or attenuating it. We dissect these intricate interaction effects leveraging a unique dataset of 67,000 keywords and their respective Google SERPs, spanning over 40 distinct US-based e-commerce domains, gen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26032;&#38395;&#25991;&#31456;&#23545;&#24191;&#21578;&#28040;&#36153;&#30340;&#24433;&#21709;&#65292;&#24182;&#23637;&#31034;&#20102;&#39640;&#36136;&#37327;&#25991;&#31456;&#19982;&#24191;&#21578;&#28040;&#36153;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.01781</link><description>&lt;p&gt;
&#26032;&#38395;&#25991;&#31456;&#36136;&#37327;&#23545;&#24191;&#21578;&#28040;&#36153;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
The Effect of News Article Quality on Ad Consumption. (arXiv:2306.01781v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01781
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26032;&#38395;&#25991;&#31456;&#23545;&#24191;&#21578;&#28040;&#36153;&#30340;&#24433;&#21709;&#65292;&#24182;&#23637;&#31034;&#20102;&#39640;&#36136;&#37327;&#25991;&#31456;&#19982;&#24191;&#21578;&#28040;&#36153;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#29992;&#30340;&#26032;&#38395;&#24179;&#21488;&#20250;&#29983;&#25104;&#19968;&#31995;&#21015;&#26032;&#38395;&#25991;&#31456;&#21644;&#24191;&#21578;&#20869;&#23481;&#65292;&#20294;&#24448;&#24448;&#29420;&#31435;&#22320;&#20248;&#21270;&#25991;&#31456;&#21644;&#24191;&#21578;&#30340;&#20301;&#32622;&#12290;&#28982;&#32780;&#65292;&#22914;&#25105;&#20204;&#25152;&#31034;&#65292;&#22312;&#32771;&#34385;&#24444;&#27492;&#24433;&#21709;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#25490;&#29256;&#38750;&#24120;&#37325;&#35201;&#65292;&#22240;&#20026;&#26032;&#38395;&#25991;&#31456;&#20250;&#26497;&#22823;&#22320;&#24433;&#21709;&#29992;&#25143;&#23545;&#24191;&#21578;&#30340;&#34892;&#20026;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#26032;&#38395;&#25991;&#31456;&#23545;&#29992;&#25143;&#24191;&#21578;&#28040;&#36153;&#30340;&#24433;&#21709;&#65292;&#24182;&#23637;&#31034;&#20102;&#26032;&#38395;&#19982;&#24191;&#21578;&#25928;&#26524;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#26381;&#21153;&#26085;&#24535;&#20998;&#26512;&#65292;&#24182;&#23637;&#31034;&#20102;&#19982;&#39640;&#36136;&#37327;&#26032;&#38395;&#25991;&#31456;&#25509;&#35302;&#30340;&#20250;&#35805;&#27604;&#20302;&#36136;&#37327;&#26032;&#38395;&#25991;&#31456;&#25509;&#35302;&#30340;&#20250;&#35805;&#25317;&#26377;&#26356;&#22810;&#30340;&#24191;&#21578;&#28040;&#36153;&#12290;&#22522;&#20110;&#27492;&#32467;&#26524;&#65292;&#25105;&#20204;&#20551;&#35774;&#25509;&#35302;&#39640;&#36136;&#37327;&#25991;&#31456;&#23558;&#23548;&#33268;&#26356;&#39640;&#30340;&#24191;&#21578;&#28040;&#36153;&#29575;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#21315;&#19975;&#32423;&#21035;&#30340;A/B&#27979;&#35797;&#65292;&#20197;&#30740;&#31350;&#39640;&#36136;&#37327;&#25991;&#31456;&#23545;&#24191;&#21578;&#28040;&#36153;&#30340;&#24433;&#21709;&#65292;&#20854;&#20013;&#25105;&#20204;&#20248;&#20808;&#32771;&#34385;&#39640;&#36136;&#37327;&#25991;&#31456;&#12290;
&lt;/p&gt;
&lt;p&gt;
Practical news feed platforms generate a hybrid list of news articles and advertising items (e.g., products, services, or information) and many platforms optimize the position of news articles and advertisements independently. However, they should be arranged with careful consideration of each other, as we show in this study, since user behaviors toward advertisements are significantly affected by the news articles. This paper investigates the effect of news articles on users' ad consumption and shows the dependency between news and ad effectiveness. We conducted a service log analysis and showed that sessions with high-quality news article exposure had more ad consumption than those with low-quality news article exposure. Based on this result, we hypothesized that exposure to high-quality articles will lead to a high ad consumption rate. Thus, we conducted million-scale A/B testing to investigate the effect of high-quality articles on ad consumption, in which we prioritized high-quali
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#26021;&#21147;-&#21560;&#24341;&#21147;&#25439;&#22833;&#20989;&#25968;&#65292;&#35813;&#20989;&#25968;&#20301;&#20110;&#24230;&#37327;&#23398;&#20064;&#33539;&#24335;&#20013;&#65292;&#21487;&#20197;&#30452;&#25509;&#20248;&#21270;L2&#24230;&#37327;&#65292;&#26080;&#38656;&#29983;&#25104;&#25104;&#23545;&#65292;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#26816;&#32034;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2306.00630</link><description>&lt;p&gt;
&#22522;&#20110;&#20869;&#23481;&#30340;&#22270;&#20687;&#26816;&#32034;&#30340;&#31867;&#38170;&#28857;&#36793;&#36317;&#25439;&#22833;
&lt;/p&gt;
&lt;p&gt;
Class Anchor Margin Loss for Content-Based Image Retrieval. (arXiv:2306.00630v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00630
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#26021;&#21147;-&#21560;&#24341;&#21147;&#25439;&#22833;&#20989;&#25968;&#65292;&#35813;&#20989;&#25968;&#20301;&#20110;&#24230;&#37327;&#23398;&#20064;&#33539;&#24335;&#20013;&#65292;&#21487;&#20197;&#30452;&#25509;&#20248;&#21270;L2&#24230;&#37327;&#65292;&#26080;&#38656;&#29983;&#25104;&#25104;&#23545;&#65292;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#26816;&#32034;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#22312;&#20869;&#23481;&#20026;&#22522;&#30784;&#30340;&#22270;&#20687;&#26816;&#32034;&#65288;CBIR&#65289;&#20013;&#30340;&#24615;&#33021;&#21463;&#25152;&#36873;&#30340;&#25439;&#22833;&#65288;&#30446;&#26631;&#65289;&#20989;&#25968;&#30340;&#24433;&#21709;&#24456;&#22823;&#12290;&#31070;&#32463;&#27169;&#22411;&#30340;&#22823;&#22810;&#25968;&#30446;&#26631;&#20989;&#25968;&#21487;&#20197;&#20998;&#20026;&#24230;&#37327;&#23398;&#20064;&#21644;&#32479;&#35745;&#23398;&#20064;&#20004;&#31867;&#12290;&#24230;&#37327;&#23398;&#20064;&#26041;&#27861;&#38656;&#35201;&#25104;&#23545;&#25366;&#25496;&#31574;&#30053;&#65292;&#36825;&#24448;&#24448;&#32570;&#20047;&#25928;&#29575;&#65292;&#32780;&#32479;&#35745;&#23398;&#20064;&#26041;&#27861;&#30001;&#20110;&#20854;&#38388;&#25509;&#29305;&#24449;&#20248;&#21270;&#32780;&#26080;&#27861;&#29983;&#25104;&#39640;&#24230;&#21387;&#32553;&#30340;&#29305;&#24449;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26021;&#21147;-&#21560;&#24341;&#21147;&#25439;&#22833;&#20989;&#25968;&#65292;&#20301;&#20110;&#24230;&#37327;&#23398;&#20064;&#33539;&#24335;&#20013;&#65292;&#21364;&#21487;&#20197;&#30452;&#25509;&#20248;&#21270;L2&#24230;&#37327;&#65292;&#26080;&#38656;&#29983;&#25104;&#25104;&#23545;&#12290;&#25105;&#20204;&#30340;&#25439;&#22833;&#30001;&#19977;&#20010;&#32452;&#25104;&#37096;&#20998;&#32452;&#25104;&#12290;&#19968;&#20010;&#20027;&#35201;&#30446;&#26631;&#30830;&#20445;&#23398;&#20064;&#21040;&#30340;&#29305;&#24449;&#34987;&#21560;&#24341;&#21040;&#21508;&#33258;&#25351;&#23450;&#30340;&#21487;&#23398;&#20064;&#31867;&#38170;&#28857;&#12290;&#31532;&#20108;&#20010;&#25439;&#22833;&#32452;&#20998;&#23545;&#38170;&#28857;&#36827;&#34892;&#35843;&#33410;&#65292;&#24378;&#21046;&#23427;&#20204;&#30456;&#20114;&#20043;&#38388;&#26377;&#19968;&#23450;&#38388;&#38548;&#65292;&#32780;&#31532;&#19977;&#20010;&#30446;&#26631;&#30830;&#20445;&#38170;&#28857;&#19981;&#20250;&#23849;&#28291;&#20026;&#38646;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26356;&#39640;&#25928;&#30340;&#21464;&#20307;&#65292;&#23427;&#19981;&#38656;&#35201;&#35745;&#31639;&#23436;&#25972;&#30340;&#25104;&#23545;&#36317;&#31163;&#30697;&#38453;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#25439;&#22833;&#22312;&#26816;&#32034;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
The performance of neural networks in content-based image retrieval (CBIR) is highly influenced by the chosen loss (objective) function. The majority of objective functions for neural models can be divided into metric learning and statistical learning. Metric learning approaches require a pair mining strategy that often lacks efficiency, while statistical learning approaches are not generating highly compact features due to their indirect feature optimization. To this end, we propose a novel repeller-attractor loss that falls in the metric learning paradigm, yet directly optimizes for the L2 metric without the need of generating pairs. Our loss is formed of three components. One leading objective ensures that the learned features are attracted to each designated learnable class anchor. The second loss component regulates the anchors and forces them to be separable by a margin, while the third objective ensures that the anchors do not collapse to zero. Furthermore, we develop a more eff
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#22810;&#20934;&#21017;&#25512;&#33616;&#30340;&#26631;&#20934;&#20559;&#22909;&#24863;&#30693;&#36731;&#37327;&#22270;&#21367;&#31215;&#32593;&#32476;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;MC&#25193;&#23637;&#22270;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#25429;&#25417;&#29992;&#25143;&#30340;&#26631;&#20934;&#20559;&#22909;&#65292;&#24182;&#36827;&#19968;&#27493;&#23558;&#29992;&#25143;&#23545;&#21508;&#20010;&#26631;&#20934;&#30340;&#20559;&#22909;&#21512;&#24182;&#21040;&#26368;&#32456;&#30340;&#25512;&#33616;&#21015;&#34920;&#20013;&#12290;</title><link>http://arxiv.org/abs/2305.18885</link><description>&lt;p&gt;
&#26631;&#20934;&#27604;&#35780;&#20998;&#26356;&#37325;&#35201;&#65306;&#38754;&#21521;&#22810;&#20934;&#21017;&#25512;&#33616;&#30340;&#26631;&#20934;&#20559;&#22909;&#24863;&#30693;&#36731;&#37327;&#22270;&#21367;&#31215;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Criteria Tell You More than Ratings: Criteria Preference-Aware Light Graph Convolution for Effective Multi-Criteria Recommendation. (arXiv:2305.18885v2 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18885
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#22810;&#20934;&#21017;&#25512;&#33616;&#30340;&#26631;&#20934;&#20559;&#22909;&#24863;&#30693;&#36731;&#37327;&#22270;&#21367;&#31215;&#32593;&#32476;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;MC&#25193;&#23637;&#22270;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#25429;&#25417;&#29992;&#25143;&#30340;&#26631;&#20934;&#20559;&#22909;&#65292;&#24182;&#36827;&#19968;&#27493;&#23558;&#29992;&#25143;&#23545;&#21508;&#20010;&#26631;&#20934;&#30340;&#20559;&#22909;&#21512;&#24182;&#21040;&#26368;&#32456;&#30340;&#25512;&#33616;&#21015;&#34920;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20934;&#21017;&#25512;&#33616;&#31995;&#32479;&#29616;&#22312;&#22312;&#24191;&#27867;&#30340;&#30005;&#23376;&#21830;&#21153;&#39046;&#22495;&#20013;&#21033;&#29992;&#22810;&#20934;&#21017; (MC) &#35780;&#20998;&#20449;&#24687;&#65292;&#32780;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476; (GNN) &#24050;&#32463;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#25512;&#33616;&#31995;&#32479;&#30340;&#24320;&#21457;&#20013;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#26412;&#25991;&#39318;&#27425;&#23581;&#35797;&#20351;&#29992;GNN&#36741;&#21161;&#35774;&#35745;MC&#25512;&#33616;&#31995;&#32479;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26631;&#20934;&#20559;&#22909;&#24863;&#30693;&#36731;&#37327;&#22270;&#21367;&#31215;&#26041;&#27861;(CPA-LGC),&#21487;&#20197;&#20934;&#30830;&#25429;&#25417;&#29992;&#25143;&#30340;&#26631;&#20934;&#20559;&#22909;&#20197;&#21450;&#22797;&#26434;&#39640;&#38454;&#36830;&#25509;&#20013;&#30340;&#21327;&#20316;&#20449;&#21495;&#12290;&#26412;&#25991;&#22312;MC&#25193;&#23637;&#22270;&#19978;&#26500;&#24314;&#20102;&#19968;&#20010;&#33021;&#22815;&#23558;&#29992;&#25143;-&#29289;&#21697;MC&#35780;&#20998;&#36716;&#25442;&#20026;&#25193;&#23637;&#20108;&#20998;&#22270;&#30340;MC&#25193;&#23637;&#22270;&#65292;&#20877;&#36827;&#19968;&#27493;&#23558;&#26631;&#20934;&#37325;&#35201;&#24615;&#32534;&#30721;&#21040;&#22270;&#21367;&#31215;&#36807;&#31243;&#20013;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26631;&#20934;&#20559;&#22909;&#24863;&#30693;&#32858;&#21512;&#26041;&#27861;&#26469;&#23558;&#29992;&#25143;&#23545;&#19981;&#21516;&#26631;&#20934;&#30340;&#20559;&#22909;&#21512;&#24182;&#21040;&#26368;&#32456;&#30340;&#25512;&#33616;&#21015;&#34920;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
The multi-criteria (MC) recommender system, which leverages MC rating information in a wide range of e-commerce areas, is ubiquitous nowadays. Surprisingly, although graph neural networks (GNNs) have been widely applied to develop various recommender systems due to GNN's high expressive capability in learning graph representations, it has been still unexplored how to design MC recommender systems with GNNs. In light of this, we make the first attempt towards designing a GNN-aided MC recommender system. Specifically, rather than straightforwardly adopting existing GNN-based recommendation methods, we devise a novel criteria preference-aware light graph convolution CPA-LGC method, which is capable of precisely capturing the criteria preference of users as well as the collaborative signal in complex high-order connectivities. To this end, we first construct an MC expansion graph that transforms user--item MC ratings into an expanded bipartite graph to potentially learn from the collaborat
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20004;&#38454;&#27573;&#27169;&#24335;&#32467;&#21512;&#39044;&#20808;&#35745;&#31639;&#22270;&#20687;&#23884;&#20837;&#21644;&#21442;&#32771;&#25991;&#26412;-&#20505;&#36873;&#39033;&#19977;&#20803;&#32452;&#20132;&#20114;&#36873;&#25321;&#30340;&#26041;&#24335;&#36827;&#34892;&#32452;&#21512;&#22270;&#20687;&#26816;&#32034;&#20505;&#36873;&#38598;&#37325;&#25490;&#24207;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.16304</link><description>&lt;p&gt;
&#20855;&#26377;&#21452;&#22810;&#27169;&#24577;&#32534;&#30721;&#22120;&#30340;&#32452;&#21512;&#22270;&#20687;&#26816;&#32034;&#20505;&#36873;&#38598;&#37325;&#25490;&#24207;
&lt;/p&gt;
&lt;p&gt;
Candidate Set Re-ranking for Composed Image Retrieval with Dual Multi-modal Encoder. (arXiv:2305.16304v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16304
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20004;&#38454;&#27573;&#27169;&#24335;&#32467;&#21512;&#39044;&#20808;&#35745;&#31639;&#22270;&#20687;&#23884;&#20837;&#21644;&#21442;&#32771;&#25991;&#26412;-&#20505;&#36873;&#39033;&#19977;&#20803;&#32452;&#20132;&#20114;&#36873;&#25321;&#30340;&#26041;&#24335;&#36827;&#34892;&#32452;&#21512;&#22270;&#20687;&#26816;&#32034;&#20505;&#36873;&#38598;&#37325;&#25490;&#24207;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32452;&#21512;&#22270;&#20687;&#26816;&#32034;&#26088;&#22312;&#25214;&#21040;&#26368;&#21305;&#37197;&#32473;&#23450;&#22810;&#27169;&#24577;&#29992;&#25143;&#26597;&#35810;(&#21253;&#25324;&#21442;&#32771;&#22270;&#20687;&#21644;&#25991;&#26412;&#23545;)&#30340;&#22270;&#20687;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#39044;&#20808;&#35745;&#31639;&#25972;&#20010;&#35821;&#26009;&#24211;&#30340;&#22270;&#20687;&#23884;&#20837;&#65292;&#24182;&#22312;&#27979;&#35797;&#26102;&#23558;&#36825;&#20123;&#23884;&#20837;&#19982;&#32463;&#36807;&#26597;&#35810;&#25991;&#26412;&#20462;&#25913;&#30340;&#21442;&#32771;&#22270;&#20687;&#23884;&#20837;&#36827;&#34892;&#27604;&#36739;&#12290;&#28982;&#32780;&#65292;&#20165;&#36890;&#36807;&#30701;&#25991;&#26412;&#25551;&#36848;&#24341;&#23548;&#20462;&#25913;&#21442;&#32771;&#22270;&#20687;&#23884;&#20837;&#21487;&#33021;&#24456;&#22256;&#38590;&#65292;&#29305;&#21035;&#26159;&#29420;&#31435;&#20110;&#28508;&#22312;&#30340;&#20505;&#36873;&#39033;&#12290;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#26159;&#20801;&#35768;&#26597;&#35810;&#21644;&#27599;&#20010;&#21487;&#33021;&#30340;&#20505;&#36873;&#39033;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#21363;&#21442;&#32771;&#25991;&#26412;-&#20505;&#36873;&#39033;&#19977;&#20803;&#32452;&#65292;&#24182;&#20174;&#25972;&#20010;&#38598;&#21512;&#20013;&#36873;&#25321;&#26368;&#20339;&#21305;&#37197;&#12290;&#34429;&#28982;&#36825;&#31181;&#26041;&#27861;&#26356;&#20855;&#26377;&#21028;&#21035;&#24615;&#65292;&#20294;&#23545;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#30001;&#20110;&#19981;&#33021;&#39044;&#20808;&#35745;&#31639;&#20505;&#36873;&#23884;&#20837;&#65292;&#22240;&#27492;&#35745;&#31639;&#25104;&#26412;&#26159;&#31105;&#27490;&#24615;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#20004;&#38454;&#27573;&#27169;&#24335;&#32467;&#21512;&#36825;&#20004;&#20010;&#26041;&#26696;&#30340;&#20248;&#28857;
&lt;/p&gt;
&lt;p&gt;
Composed image retrieval aims to find an image that best matches a given multi-modal user query consisting of a reference image and text pair. Existing methods commonly pre-compute image embeddings over the entire corpus and compare these to a reference image embedding modified by the query text at test time. Such a pipeline is very efficient at test time since fast vector distances can be used to evaluate candidates, but modifying the reference image embedding guided only by a short textual description can be difficult, especially independent of potential candidates. An alternative approach is to allow interactions between the query and every possible candidate, i.e., reference-text-candidate triplets, and pick the best from the entire set. Though this approach is more discriminative, for large-scale datasets the computational cost is prohibitive since pre-computation of candidate embeddings is no longer possible. We propose to combine the merits of both schemes using a two-stage mode
&lt;/p&gt;</description></item><item><title>&#19968;&#31181;&#21327;&#21516;&#36807;&#28388;&#26032;&#26041;&#27861;&#29992;&#20110;&#31283;&#20581;&#23545;&#35805;&#29702;&#35299;&#65292;&#22312;&#21382;&#21490;&#29992;&#25143;-&#23454;&#20307;&#20132;&#20114;&#30340;&#22522;&#30784;&#19978;&#65292;&#21033;&#29992;&#22810;&#36339;&#23458;&#25143;&#20146;&#21644;&#21147;&#20016;&#23500;&#27599;&#20010;&#29992;&#25143;&#30340;&#32034;&#24341;&#65292;&#24182;&#20351;&#29992;&#26377;&#38480;&#20869;&#23384;BFGS&#31639;&#27861;&#35843;&#25972;&#27599;&#20010;&#32034;&#24341;&#30340;&#26435;&#37325;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20854;&#26126;&#26174;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#20010;&#24615;&#21270;&#26597;&#35810;&#37325;&#20889;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.14449</link><description>&lt;p&gt;
&#22270;&#35889;&#36935;&#35265;LLM&#65306;&#19968;&#31181;&#29992;&#20110;&#31283;&#20581;&#23545;&#35805;&#29702;&#35299;&#30340;&#21327;&#21516;&#36807;&#28388;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Graph Meets LLM: A Novel Approach to Collaborative Filtering for Robust Conversational Understanding. (arXiv:2305.14449v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14449
&lt;/p&gt;
&lt;p&gt;
&#19968;&#31181;&#21327;&#21516;&#36807;&#28388;&#26032;&#26041;&#27861;&#29992;&#20110;&#31283;&#20581;&#23545;&#35805;&#29702;&#35299;&#65292;&#22312;&#21382;&#21490;&#29992;&#25143;-&#23454;&#20307;&#20132;&#20114;&#30340;&#22522;&#30784;&#19978;&#65292;&#21033;&#29992;&#22810;&#36339;&#23458;&#25143;&#20146;&#21644;&#21147;&#20016;&#23500;&#27599;&#20010;&#29992;&#25143;&#30340;&#32034;&#24341;&#65292;&#24182;&#20351;&#29992;&#26377;&#38480;&#20869;&#23384;BFGS&#31639;&#27861;&#35843;&#25972;&#27599;&#20010;&#32034;&#24341;&#30340;&#26435;&#37325;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20854;&#26126;&#26174;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#20010;&#24615;&#21270;&#26597;&#35810;&#37325;&#20889;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20250;&#35805;&#24335;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65288;&#20363;&#22914;Alexa&#65292;Siri&#65292;Google Assistant&#31561;&#65289;&#38656;&#35201;&#29702;&#35299;&#23384;&#22312;&#32570;&#38519;&#30340;&#26597;&#35810;&#20197;&#30830;&#20445;&#31283;&#20581;&#30340;&#20250;&#35805;&#29702;&#35299;&#24182;&#20943;&#23569;&#29992;&#25143;&#25705;&#25830;&#12290;&#36825;&#20123;&#26377;&#32570;&#38519;&#30340;&#26597;&#35810;&#36890;&#24120;&#26159;&#30001;&#29992;&#25143;&#30340;&#27495;&#20041;&#21644;&#38169;&#35823;&#65292;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#21644;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65288;NLU&#65289;&#20013;&#30340;&#38169;&#35823;&#24341;&#36215;&#30340;&#12290;&#20010;&#24615;&#21270;&#26597;&#35810;&#37325;&#20889;&#65288;&#20010;&#24615;&#21270;QR&#65289;&#26088;&#22312;&#20943;&#23569;&#36523;&#20307;&#21644;&#23614;&#37096;&#29992;&#25143;&#26597;&#35810;&#27969;&#37327;&#20013;&#30340;&#32570;&#38519;&#65292;&#36890;&#24120;&#20381;&#36182;&#20110;&#19982;&#23545;&#35805;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#36807;&#21435;&#25104;&#21151;&#30340;&#29992;&#25143;&#20132;&#20114;&#30340;&#32034;&#24341;&#12290;&#26412;&#25991;&#25552;&#20986;&#25105;&#20204;&#30340;&#8220;&#21327;&#21516;&#26597;&#35810;&#37325;&#20889;&#8221;&#26041;&#27861;&#65292;&#19987;&#27880;&#20110;&#37325;&#20889;&#29992;&#25143;&#21382;&#21490;&#20013;&#27809;&#26377;&#20986;&#29616;&#36807;&#30340;&#26032;&#22411;&#29992;&#25143;&#20132;&#20114;&#12290;&#35813;&#26041;&#27861;&#26500;&#24314;&#20102;&#19968;&#20010;&#8220;&#29992;&#25143;&#21453;&#39304;&#20132;&#20114;&#22270;&#8221;&#65288;FIG&#65289;&#65292;&#30001;&#21382;&#21490;&#29992;&#25143;-&#23454;&#20307;&#20132;&#20114;&#32452;&#25104;&#65292;&#24182;&#21033;&#29992;&#22810;&#36339;&#23458;&#25143;&#20146;&#21644;&#21147;&#26469;&#20016;&#23500;&#27599;&#20010;&#29992;&#25143;&#30340;&#32034;&#24341;&#65288;&#21363;&#21327;&#21516;&#29992;&#25143;&#32034;&#24341;&#65289;&#65292;&#20174;&#32780;&#24110;&#21161;&#35206;&#30422;&#26410;&#26469;&#26410;&#26366;&#35265;&#36807;&#30340;&#23384;&#22312;&#32570;&#38519;&#30340;&#26597;&#35810;&#12290;&#20026;&#20102;&#38450;&#27490;&#36825;&#20123;&#26032;&#30340;&#20016;&#23500;&#32034;&#24341;&#34987;&#22122;&#22768;&#21453;&#39304;&#20132;&#20114;&#25152;&#25903;&#37197;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#26377;&#38480;&#20869;&#23384;BFGS&#65288;LLM&#65289;&#31639;&#27861;&#21644;&#22238;&#36864;&#26041;&#26696;&#26469;&#35843;&#25972;&#27599;&#20010;&#32034;&#24341;&#30340;&#26435;&#37325;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#20010;&#24615;&#21270;QR&#26041;&#27861;&#65292;&#24182;&#22312;&#26410;&#30475;&#21040;&#30340;&#29992;&#25143;&#20132;&#20114;&#19978;&#21462;&#24471;&#20102;&#36817;&#20046;&#23436;&#32654;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conversational AI systems (e.g. Alexa, Siri, Google Assistant, etc.) need to understand queries with defects to ensure robust conversational understanding and reduce user frictions. The defective queries are often induced by user ambiguities and mistakes, or errors in the automatic speech recognition (ASR) and natural language understanding (NLU).  Personalized query rewriting (personalized QR) targets reducing defects in the torso and tail user query traffic, and it typically relies on an index of past successful user interactions with the conversational AI. This paper presents our "Collaborative Query Rewriting" approach that focuses on rewriting novel user interactions unseen in the user history. This approach builds a "user Feedback Interaction Graph" (FIG) consisting of historical user-entity interactions, and leverages multi-hop customer affinity to enrich each user's index (i.e. the Collaborative User Index) that would help cover future unseen defective queries. To counteract th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;LLM&#39537;&#21160;&#30340;&#29983;&#25104;&#24335;&#26032;&#38395;&#25512;&#33616;&#26694;&#26550;GENRE&#65292;&#23427;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#20041;&#30693;&#35782;&#20016;&#23500;&#26032;&#38395;&#25968;&#25454;&#65292;&#36890;&#36807;&#20174;&#27169;&#22411;&#35774;&#35745;&#36716;&#31227;&#21040;&#25552;&#31034;&#35774;&#35745;&#25552;&#20379;&#28789;&#27963;&#32780;&#32479;&#19968;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#20010;&#24615;&#21270;&#26032;&#38395;&#29983;&#25104;&#12289;&#29992;&#25143;&#30011;&#20687;&#21644;&#26032;&#38395;&#25688;&#35201;&#12290;</title><link>http://arxiv.org/abs/2305.06566</link><description>&lt;p&gt;
LLM&#39537;&#21160;&#30340;&#29983;&#25104;&#24335;&#26032;&#38395;&#25512;&#33616;&#21021;&#25506;
&lt;/p&gt;
&lt;p&gt;
A First Look at LLM-Powered Generative News Recommendation. (arXiv:2305.06566v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06566
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;LLM&#39537;&#21160;&#30340;&#29983;&#25104;&#24335;&#26032;&#38395;&#25512;&#33616;&#26694;&#26550;GENRE&#65292;&#23427;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#20041;&#30693;&#35782;&#20016;&#23500;&#26032;&#38395;&#25968;&#25454;&#65292;&#36890;&#36807;&#20174;&#27169;&#22411;&#35774;&#35745;&#36716;&#31227;&#21040;&#25552;&#31034;&#35774;&#35745;&#25552;&#20379;&#28789;&#27963;&#32780;&#32479;&#19968;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#20010;&#24615;&#21270;&#26032;&#38395;&#29983;&#25104;&#12289;&#29992;&#25143;&#30011;&#20687;&#21644;&#26032;&#38395;&#25688;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#30340;&#26032;&#38395;&#25512;&#33616;&#31995;&#32479;&#24050;&#25104;&#20026;&#29992;&#25143;&#27983;&#35272;&#28023;&#37327;&#22312;&#32447;&#26032;&#38395;&#20869;&#23481;&#25152;&#24517;&#38656;&#30340;&#24037;&#20855;&#65292;&#28982;&#32780;&#29616;&#26377;&#30340;&#26032;&#38395;&#25512;&#33616;&#31995;&#32479;&#38754;&#20020;&#30528;&#20919;&#21551;&#21160;&#38382;&#39064;&#12289;&#29992;&#25143;&#30011;&#20687;&#24314;&#27169;&#21644;&#26032;&#38395;&#20869;&#23481;&#29702;&#35299;&#31561;&#37325;&#22823;&#25361;&#25112;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#36890;&#24120;&#36890;&#36807;&#27169;&#22411;&#35774;&#35745;&#36981;&#24490;&#19968;&#31181;&#19981;&#28789;&#27963;&#30340;&#20363;&#34892;&#31243;&#24207;&#26469;&#35299;&#20915;&#29305;&#23450;&#30340;&#25361;&#25112;&#65292;&#20294;&#22312;&#29702;&#35299;&#26032;&#38395;&#20869;&#23481;&#21644;&#25429;&#25417;&#29992;&#25143;&#20852;&#36259;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;GENRE&#65292;&#19968;&#31181;LLM&#39537;&#21160;&#30340;&#29983;&#25104;&#24335;&#26032;&#38395;&#25512;&#33616;&#26694;&#26550;&#65292;&#23427;&#21033;&#29992;&#26469;&#33258;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#35821;&#20041;&#30693;&#35782;&#26469;&#20016;&#23500;&#26032;&#38395;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#20174;&#27169;&#22411;&#35774;&#35745;&#36716;&#31227;&#21040;&#25552;&#31034;&#35774;&#35745;&#26469;&#25552;&#20379;&#19968;&#31181;&#28789;&#27963;&#32780;&#32479;&#19968;&#30340;&#26032;&#38395;&#25512;&#33616;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;GENRE&#22312;&#20010;&#24615;&#21270;&#26032;&#38395;&#29983;&#25104;&#12289;&#29992;&#25143;&#30011;&#20687;&#21644;&#26032;&#38395;&#25688;&#35201;&#20013;&#30340;&#24212;&#29992;&#12290;&#20351;&#29992;&#21508;&#31181;&#27969;&#34892;&#30340;&#25512;&#33616;&#27169;&#22411;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;GENRE&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Personalized news recommendation systems have become essential tools for users to navigate the vast amount of online news content, yet existing news recommenders face significant challenges such as the cold-start problem, user profile modeling, and news content understanding. Previous works have typically followed an inflexible routine to address a particular challenge through model design, but are limited in their ability to understand news content and capture user interests. In this paper, we introduce GENRE, an LLM-powered generative news recommendation framework, which leverages pretrained semantic knowledge from large language models to enrich news data. Our aim is to provide a flexible and unified solution for news recommendation by moving from model design to prompt design. We showcase the use of GENRE for personalized news generation, user profiling, and news summarization. Extensive experiments with various popular recommendation models demonstrate the effectiveness of GENRE. 
&lt;/p&gt;</description></item><item><title>AdaTT&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#30340;&#33258;&#36866;&#24212;&#20219;&#21153;&#34701;&#21512;&#28145;&#24230;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#27531;&#24046;&#26426;&#21046;&#21644;&#38376;&#25511;&#26426;&#21046;&#23454;&#29616;&#20219;&#21153;&#20043;&#38388;&#30340;&#34701;&#21512;&#65292;&#33258;&#36866;&#24212;&#22320;&#23398;&#20064;&#20849;&#20139;&#30693;&#35782;&#21644;&#20219;&#21153;&#29305;&#23450;&#30693;&#35782;&#65292;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#21487;&#20197;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#22522;&#32447;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2304.04959</link><description>&lt;p&gt;
AdaTT: &#33258;&#36866;&#24212;&#20219;&#21153;&#34701;&#21512;&#32593;&#32476;&#29992;&#20110;&#22810;&#20219;&#21153;&#23398;&#20064;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
AdaTT: Adaptive Task-to-Task Fusion Network for Multitask Learning in Recommendations. (arXiv:2304.04959v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04959
&lt;/p&gt;
&lt;p&gt;
AdaTT&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#30340;&#33258;&#36866;&#24212;&#20219;&#21153;&#34701;&#21512;&#28145;&#24230;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#27531;&#24046;&#26426;&#21046;&#21644;&#38376;&#25511;&#26426;&#21046;&#23454;&#29616;&#20219;&#21153;&#20043;&#38388;&#30340;&#34701;&#21512;&#65292;&#33258;&#36866;&#24212;&#22320;&#23398;&#20064;&#20849;&#20139;&#30693;&#35782;&#21644;&#20219;&#21153;&#29305;&#23450;&#30693;&#35782;&#65292;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#21487;&#20197;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20219;&#21153;&#23398;&#20064;&#26088;&#22312;&#36890;&#36807;&#21516;&#26102;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#25552;&#39640;&#24615;&#33021;&#21644;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#22810;&#20219;&#21153;&#23398;&#20064;&#38754;&#20020;&#20004;&#20010;&#25361;&#25112;&#65306;1&#65289;&#23545;&#20219;&#21153;&#20043;&#38388;&#30340;&#20851;&#31995;&#36827;&#34892;&#24314;&#27169;&#65292;&#20197;&#26377;&#25928;&#22320;&#20849;&#20139;&#30693;&#35782;&#65307;2&#65289;&#32852;&#21512;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#21644;&#20849;&#20139;&#30693;&#35782;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#33258;&#36866;&#24212;&#20219;&#21153;&#34701;&#21512;&#32593;&#32476;&#65288;AdaTT&#65289;&#26469;&#35299;&#20915;&#36825;&#20004;&#20010;&#25361;&#25112;&#12290;AdaTT&#26159;&#19968;&#20010;&#28145;&#24230;&#34701;&#21512;&#32593;&#32476;&#65292;&#22312;&#22810;&#20010;&#32423;&#21035;&#19978;&#20351;&#29992;&#20219;&#21153;&#29305;&#23450;&#21644;&#21487;&#36873;&#20849;&#20139;&#34701;&#21512;&#21333;&#20803;&#26500;&#24314;&#12290;&#36890;&#36807;&#21033;&#29992;&#27531;&#24046;&#26426;&#21046;&#21644;&#38376;&#25511;&#26426;&#21046;&#23454;&#29616;&#20219;&#21153;&#20043;&#38388;&#30340;&#34701;&#21512;&#65292;&#36825;&#20123;&#21333;&#20803;&#33258;&#36866;&#24212;&#22320;&#23398;&#20064;&#20849;&#20139;&#30693;&#35782;&#21644;&#20219;&#21153;&#29305;&#23450;&#30693;&#35782;&#12290;&#20026;&#20102;&#35780;&#20272;AdaTT&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#22312;&#20844;&#20849;&#22522;&#20934;&#27979;&#35797;&#38598;&#21644;&#24037;&#19994;&#32423;&#25512;&#33616;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#19981;&#21516;&#30340;&#20219;&#21153;&#32452;&#36827;&#34892;&#23454;&#39564;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;AdaTT&#21487;&#20197;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-task learning (MTL) aims at enhancing the performance and efficiency of machine learning models by training them on multiple tasks simultaneously. However, MTL research faces two challenges: 1) modeling the relationships between tasks to effectively share knowledge between them, and 2) jointly learning task-specific and shared knowledge. In this paper, we present a novel model Adaptive Task-to-Task Fusion Network (AdaTT) to address both challenges. AdaTT is a deep fusion network built with task specific and optional shared fusion units at multiple levels. By leveraging a residual mechanism and gating mechanism for task-to-task fusion, these units adaptively learn shared knowledge and task specific knowledge. To evaluate the performance of AdaTT, we conduct experiments on a public benchmark and an industrial recommendation dataset using various task groups. Results demonstrate AdaTT can significantly outperform existing state-of-the-art baselines.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;CONTINUOUS&#65292;&#21487;&#20197;&#23545;&#28508;&#22312;&#22240;&#23376;&#27169;&#22411;&#36827;&#34892;&#36830;&#32493;&#23884;&#20837;&#22823;&#23567;&#25628;&#32034;&#65292;&#23427;&#36890;&#36807;&#23558;&#23884;&#20837;&#22823;&#23567;&#36873;&#25321;&#24314;&#27169;&#20026;&#36830;&#32493;&#21464;&#37327;&#35299;&#20915;&#20102;&#20808;&#21069;&#24037;&#20316;&#20013;&#30340;&#25361;&#25112;&#65292;&#24182;&#22312;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#20013;&#35777;&#23454;&#20102;&#23427;&#30340;&#26377;&#25928;&#24615;&#21644;&#39640;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.03501</link><description>&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#30340;&#36830;&#32493;&#36755;&#20837;&#23884;&#20837;&#22823;&#23567;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Continuous Input Embedding Size Search For Recommender Systems. (arXiv:2304.03501v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03501
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;CONTINUOUS&#65292;&#21487;&#20197;&#23545;&#28508;&#22312;&#22240;&#23376;&#27169;&#22411;&#36827;&#34892;&#36830;&#32493;&#23884;&#20837;&#22823;&#23567;&#25628;&#32034;&#65292;&#23427;&#36890;&#36807;&#23558;&#23884;&#20837;&#22823;&#23567;&#36873;&#25321;&#24314;&#27169;&#20026;&#36830;&#32493;&#21464;&#37327;&#35299;&#20915;&#20102;&#20808;&#21069;&#24037;&#20316;&#20013;&#30340;&#25361;&#25112;&#65292;&#24182;&#22312;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#20013;&#35777;&#23454;&#20102;&#23427;&#30340;&#26377;&#25928;&#24615;&#21644;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28508;&#22312;&#22240;&#23376;&#27169;&#22411;&#26159;&#29616;&#20170;&#25512;&#33616;&#31995;&#32479;&#26368;&#27969;&#34892;&#30340;&#22522;&#30784;&#65292;&#20854;&#24615;&#33021;&#21331;&#36234;&#12290;&#28508;&#22312;&#22240;&#23376;&#27169;&#22411;&#36890;&#36807;&#23545;&#29992;&#25143;&#21644;&#39033;&#30446;&#36827;&#34892;&#34920;&#31034;&#65292;&#29992;&#20110;&#23545;&#25104;&#23545;&#30456;&#20284;&#24230;&#30340;&#35745;&#31639;&#12290;&#25152;&#26377;&#23884;&#20837;&#21521;&#37327;&#20256;&#32479;&#19978;&#37117;&#34987;&#38480;&#21046;&#22312;&#19968;&#20010;&#30456;&#23545;&#36739;&#22823;&#30340;&#32479;&#19968;&#22823;&#23567;&#65288;&#20363;&#22914;256&#32500;&#65289;&#12290;&#38543;&#30528;&#24403;&#20195;&#30005;&#23376;&#21830;&#21153;&#20013;&#29992;&#25143;&#21644;&#39033;&#30446;&#30446;&#24405;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#36825;&#31181;&#35774;&#35745;&#26174;&#28982;&#21464;&#24471;&#25928;&#29575;&#20302;&#19979;&#12290;&#20026;&#20102;&#20419;&#36827;&#36731;&#37327;&#32423;&#25512;&#33616;&#65292;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26368;&#36817;&#24320;&#36767;&#20102;&#19968;&#20123;&#26426;&#20250;&#65292;&#29992;&#20110;&#35782;&#21035;&#19981;&#21516;&#29992;&#25143;/&#39033;&#30446;&#30340;&#19981;&#21516;&#23884;&#20837;&#22823;&#23567;&#12290;&#28982;&#32780;&#65292;&#21463;&#21040;&#25628;&#32034;&#25928;&#29575;&#21644;&#23398;&#20064;&#26368;&#20248;RL&#31574;&#30053;&#30340;&#38480;&#21046;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;RL&#30340;&#26041;&#27861;&#34987;&#38480;&#21046;&#20026;&#39640;&#24230;&#31163;&#25955;&#30340;&#39044;&#23450;&#20041;&#23884;&#20837;&#22823;&#23567;&#36873;&#39033;&#12290;&#36825;&#23548;&#33268;&#20102;&#19968;&#20010;&#34987;&#24191;&#27867;&#24573;&#35270;&#30340;&#28508;&#21147;&#65292;&#21487;&#20197;&#22312;&#32473;&#23450;&#35745;&#31639;&#39044;&#31639;&#19979;&#24341;&#20837;&#26356;&#32454;&#30340;&#31890;&#24230;&#26469;&#33719;&#24471;&#26356;&#22909;&#30340;&#25512;&#33616;&#25928;&#26524;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#31216;&#20026;CONTINUOUS&#65292;&#21487;&#20197;&#23545;&#28508;&#22312;&#22240;&#23376;&#27169;&#22411;&#36827;&#34892;&#36830;&#32493;&#23884;&#20837;&#22823;&#23567;&#25628;&#32034;&#12290;CONTINUOUS&#36890;&#36807;&#23558;&#23884;&#20837;&#22823;&#23567;&#36873;&#25321;&#24314;&#27169;&#20026;&#36830;&#32493;&#21464;&#37327;&#21644;&#21046;&#23450;&#21487;&#24494;&#20248;&#21270;&#38382;&#39064;&#30340;&#24418;&#24335;&#26469;&#35299;&#20915;&#20043;&#21069;&#24037;&#20316;&#30340;&#25361;&#25112;&#12290;&#22312;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#23454;&#20102;CONTINUOUS&#20248;&#20110;&#22522;&#32447;&#30340;&#20248;&#36234;&#24615;&#65292;&#39564;&#35777;&#20102;&#21160;&#24577;&#20248;&#21270;&#23884;&#20837;&#22823;&#23567;&#30340;&#26377;&#25928;&#24615;&#21644;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Latent factor models are the most popular backbones for today's recommender systems owing to their prominent performance. Latent factor models represent users and items as real-valued embedding vectors for pairwise similarity computation, and all embeddings are traditionally restricted to a uniform size that is relatively large (e.g., 256-dimensional). With the exponentially expanding user base and item catalog in contemporary e-commerce, this design is admittedly becoming memory-inefficient. To facilitate lightweight recommendation, reinforcement learning (RL) has recently opened up opportunities for identifying varying embedding sizes for different users/items. However, challenged by search efficiency and learning an optimal RL policy, existing RL-based methods are restricted to highly discrete, predefined embedding size choices. This leads to a largely overlooked potential of introducing finer granularity into embedding sizes to obtain better recommendation effectiveness under a giv
&lt;/p&gt;</description></item><item><title>ANTM&#26159;&#19968;&#31181;&#23545;&#40784;&#30340;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#65292;&#23427;&#21033;&#29992;&#37325;&#21472;&#28369;&#21160;&#31383;&#21475;&#31639;&#27861;&#26469;&#32500;&#25252;&#28436;&#21464;&#20027;&#39064;&#30340;&#26102;&#38388;&#36830;&#32493;&#24615;&#65292;&#24182;&#36890;&#36807;&#23545;&#35821;&#20041;&#30456;&#20284;&#30340;&#25991;&#26723;&#36827;&#34892;&#23545;&#40784;&#26469;&#25429;&#25417;&#20986;&#29616;&#21644;&#28040;&#36864;&#30340;&#36235;&#21183;&#12290;&#23454;&#39564;&#35777;&#26126;ANTM&#22312;&#20027;&#39064;&#36830;&#36143;&#24615;&#21644;&#22810;&#26679;&#24615;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#21160;&#24577;&#20027;&#39064;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2302.01501</link><description>&lt;p&gt;
ANTM: &#19968;&#31181;&#23545;&#40784;&#30340;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#65292;&#29992;&#20110;&#25506;&#32034;&#28436;&#21464;&#30340;&#20027;&#39064;
&lt;/p&gt;
&lt;p&gt;
ANTM: An Aligned Neural Topic Model for Exploring Evolving Topics. (arXiv:2302.01501v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01501
&lt;/p&gt;
&lt;p&gt;
ANTM&#26159;&#19968;&#31181;&#23545;&#40784;&#30340;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#65292;&#23427;&#21033;&#29992;&#37325;&#21472;&#28369;&#21160;&#31383;&#21475;&#31639;&#27861;&#26469;&#32500;&#25252;&#28436;&#21464;&#20027;&#39064;&#30340;&#26102;&#38388;&#36830;&#32493;&#24615;&#65292;&#24182;&#36890;&#36807;&#23545;&#35821;&#20041;&#30456;&#20284;&#30340;&#25991;&#26723;&#36827;&#34892;&#23545;&#40784;&#26469;&#25429;&#25417;&#20986;&#29616;&#21644;&#28040;&#36864;&#30340;&#36235;&#21183;&#12290;&#23454;&#39564;&#35777;&#26126;ANTM&#22312;&#20027;&#39064;&#36830;&#36143;&#24615;&#21644;&#22810;&#26679;&#24615;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#21160;&#24577;&#20027;&#39064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#23545;&#40784;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#65288;ANTM&#65289;&#30340;&#21160;&#24577;&#20027;&#39064;&#27169;&#22411;&#31639;&#27861;&#23478;&#26063;&#65292;&#23427;&#32467;&#21512;&#20102;&#26032;&#39062;&#30340;&#25968;&#25454;&#25366;&#25496;&#31639;&#27861;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#27169;&#22359;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#21457;&#29616;&#28436;&#21464;&#30340;&#20027;&#39064;&#12290;ANTM&#21033;&#29992;&#20808;&#36827;&#30340;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#25991;&#26723;&#20013;&#25552;&#21462;&#26102;&#38388;&#24863;&#30693;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#37325;&#21472;&#28369;&#21160;&#31383;&#21475;&#31639;&#27861;&#36827;&#34892;&#39034;&#24207;&#25991;&#26723;&#32858;&#31867;&#65292;&#20174;&#32780;&#32500;&#25252;&#20102;&#28436;&#21464;&#20027;&#39064;&#30340;&#26102;&#38388;&#36830;&#32493;&#24615;&#12290;&#36825;&#31181;&#37325;&#21472;&#28369;&#21160;&#31383;&#21475;&#31639;&#27861;&#22312;&#27599;&#20010;&#26102;&#38388;&#26694;&#26550;&#20869;&#26631;&#35782;&#19981;&#21516;&#25968;&#37327;&#30340;&#20027;&#39064;&#65292;&#24182;&#22312;&#26102;&#38388;&#27573;&#20869;&#23545;&#35821;&#20041;&#30456;&#20284;&#30340;&#25991;&#26723;&#32858;&#31867;&#36827;&#34892;&#23545;&#40784;&#12290;&#36825;&#20010;&#36807;&#31243;&#25429;&#25417;&#20102;&#19981;&#21516;&#26102;&#26399;&#20986;&#29616;&#21644;&#28040;&#36864;&#30340;&#36235;&#21183;&#65292;&#24182;&#20801;&#35768;&#26356;&#20855;&#21487;&#35299;&#37322;&#24615;&#30340;&#28436;&#21464;&#20027;&#39064;&#34920;&#31034;&#12290;&#38024;&#23545;&#22235;&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;ANTM&#22312;&#20027;&#39064;&#36830;&#36143;&#24615;&#21644;&#22810;&#26679;&#24615;&#25351;&#26631;&#26041;&#38754;&#20248;&#20110;&#27010;&#29575;&#21160;&#24577;&#20027;&#39064;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#23427;&#25913;&#21892;&#20102;&#21160;&#24577;&#20027;&#39064;&#24314;&#27169;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents an algorithmic family of dynamic topic models called Aligned Neural Topic Models (ANTM), which combine novel data mining algorithms to provide a modular framework for discovering evolving topics. ANTM maintains the temporal continuity of evolving topics by extracting time-aware features from documents using advanced pre-trained Large Language Models (LLMs) and employing an overlapping sliding window algorithm for sequential document clustering. This overlapping sliding window algorithm identifies a different number of topics within each time frame and aligns semantically similar document clusters across time periods. This process captures emerging and fading trends across different periods and allows for a more interpretable representation of evolving topics. Experiments on four distinct datasets show that ANTM outperforms probabilistic dynamic topic models in terms of topic coherence and diversity metrics. Moreover, it improves the scalability and flexibility of dy
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;ExaRanker&#30340;&#35299;&#37322;&#22686;&#24378;&#22411;&#31070;&#32463;&#25490;&#24207;&#27169;&#22411;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#26816;&#32034;&#25968;&#25454;&#38598;&#65292;&#21487;&#22312;&#36755;&#20986;&#30456;&#20851;&#24230;&#26631;&#31614;&#19982;&#35299;&#37322;&#26102;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.10521</link><description>&lt;p&gt;
ExaRanker: &#35299;&#37322;&#22686;&#24378;&#22411;&#31070;&#32463;&#25490;&#24207;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ExaRanker: Explanation-Augmented Neural Ranker. (arXiv:2301.10521v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10521
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;ExaRanker&#30340;&#35299;&#37322;&#22686;&#24378;&#22411;&#31070;&#32463;&#25490;&#24207;&#27169;&#22411;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#26816;&#32034;&#25968;&#25454;&#38598;&#65292;&#21487;&#22312;&#36755;&#20986;&#30456;&#20851;&#24230;&#26631;&#31614;&#19982;&#35299;&#37322;&#26102;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#35753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36755;&#20986;&#31572;&#26696;&#21069;&#29983;&#25104;&#35299;&#37322;&#26159;&#25552;&#39640;&#21508;&#31181;&#25512;&#29702;&#20219;&#21153;&#24615;&#33021;&#30340;&#26377;&#25928;&#31574;&#30053;&#12290;&#26412;&#25991;&#25552;&#20986;&#31070;&#32463;&#25490;&#24207;&#27169;&#22411;&#20063;&#21463;&#30410;&#20110;&#35299;&#37322;&#12290;&#25105;&#20204;&#20351;&#29992;GPT-3.5&#31561;&#35821;&#35328;&#27169;&#22411;&#26469;&#22686;&#24378;&#20855;&#26377;&#35299;&#37322;&#30340;&#26816;&#32034;&#25968;&#25454;&#38598;&#65292;&#24182;&#35757;&#32451;&#19968;&#20010;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#25490;&#24207;&#27169;&#22411;&#65292;&#20197;&#36755;&#20986;&#32473;&#23450;&#26597;&#35810;-&#25991;&#26723;&#23545;&#30340;&#30456;&#20851;&#24230;&#26631;&#31614;&#21644;&#35299;&#37322;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#34987;&#31216;&#20026;ExaRanker&#65292;&#22312;&#20351;&#29992;&#21512;&#25104;&#35299;&#37322;&#30340;&#20960;&#21315;&#20010;&#26679;&#26412;&#36827;&#34892;&#24494;&#35843;&#21518;&#65292;&#24615;&#33021;&#19982;&#26080;&#35299;&#37322;&#30340;3&#20493;&#26679;&#26412;&#24494;&#35843;&#30340;&#27169;&#22411;&#30456;&#24403;&#12290;&#27492;&#22806;&#65292;ExaRanker&#27169;&#22411;&#22312;&#25490;&#24207;&#36807;&#31243;&#20013;&#27809;&#26377;&#39069;&#22806;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#24182;&#20801;&#35768;&#26681;&#25454;&#38656;&#35201;&#35831;&#27714;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work has shown that inducing a large language model (LLM) to generate explanations prior to outputting an answer is an effective strategy to improve performance on a wide range of reasoning tasks. In this work, we show that neural rankers also benefit from explanations. We use LLMs such as GPT-3.5 to augment retrieval datasets with explanations and train a sequence-to-sequence ranking model to output a relevance label and an explanation for a given query-document pair. Our model, dubbed ExaRanker, finetuned on a few thousand examples with synthetic explanations performs on par with models finetuned on 3x more examples without explanations. Furthermore, the ExaRanker model incurs no additional computational cost during ranking and allows explanations to be requested on demand.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25551;&#36848;&#20102;&#22312;&#26080;&#20559;&#23398;&#20064;&#25490;&#24207;&#20013;&#35299;&#31163;&#20851;&#32852;&#24615;&#21644;&#20559;&#24046;&#24615;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#19977;&#31181;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2212.13937</link><description>&lt;p&gt;
&#22312;&#26080;&#20559;&#23398;&#20064;&#25490;&#24207;&#20013;&#35299;&#31163;&#20851;&#32852;&#24615;&#21644;&#20559;&#24046;&#24615;&#30340;&#36861;&#27714;
&lt;/p&gt;
&lt;p&gt;
Towards Disentangling Relevance and Bias in Unbiased Learning to Rank. (arXiv:2212.13937v4 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.13937
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25551;&#36848;&#20102;&#22312;&#26080;&#20559;&#23398;&#20064;&#25490;&#24207;&#20013;&#35299;&#31163;&#20851;&#32852;&#24615;&#21644;&#20559;&#24046;&#24615;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#19977;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#20559;&#23398;&#20064;&#25490;&#24207;(ULTR)&#30740;&#31350;&#30340;&#38382;&#39064;&#22312;&#20110;&#20174;&#38544;&#21547;&#30340;&#29992;&#25143;&#21453;&#39304;&#25968;&#25454;&#65288;&#22914;&#28857;&#20987;&#65289;&#20013;&#20943;&#36731;&#21508;&#31181;&#20559;&#24046;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#19988;&#26368;&#36817;&#21463;&#21040;&#20102;&#30456;&#24403;&#22823;&#30340;&#20851;&#27880;&#12290;&#19968;&#31181;&#29992;&#20110;&#23454;&#38469;&#24212;&#29992;&#30340;&#27969;&#34892;ULTR&#26041;&#27861;&#26159;&#20351;&#29992;&#21452;&#22612;&#32467;&#26500;&#65292;&#20854;&#20013;&#23558;&#28857;&#20987;&#24314;&#27169;&#20998;&#35299;&#20026;&#19968;&#20010;&#20855;&#26377;&#24120;&#35268;&#36755;&#20837;&#29305;&#24449;&#30340;&#20851;&#32852;&#22612;&#21644;&#19968;&#20010;&#20855;&#26377;&#20559;&#24046;&#30456;&#20851;&#36755;&#20837;&#65288;&#22914;&#25991;&#20214;&#20301;&#32622;&#65289;&#30340;&#20559;&#24046;&#22612;&#12290;&#25104;&#21151;&#30340;&#20998;&#35299;&#23558;&#20801;&#35768;&#20851;&#32852;&#22612;&#20813;&#21463;&#20559;&#24046;&#30340;&#24433;&#21709;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#29616;&#26377;ULTR&#26041;&#27861;&#24573;&#30053;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#8212;&#8212;&#36890;&#36807;&#24213;&#23618;&#30495;&#23454;&#20851;&#32852;&#24615;&#65292;&#20559;&#24046;&#22612;&#21487;&#33021;&#20250;&#19982;&#20851;&#32852;&#22612;&#28151;&#28102;&#12290;&#29305;&#21035;&#26159;&#65292;&#20301;&#32622;&#26159;&#30001;&#35760;&#24405;&#31574;&#30053;&#65288;&#21363;&#20197;&#21069;&#30340;&#29983;&#20135;&#27169;&#22411;&#65289;&#30830;&#23450;&#30340;&#65292;&#23427;&#23558;&#20855;&#26377;&#20851;&#32852;&#20449;&#24687;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#35777;&#32467;&#26524;&#65292;&#20197;&#23637;&#31034;&#30001;&#20110;&#36825;&#31181;&#30456;&#20851;&#24615;&#23545;&#20110;&#20851;&#32852;&#22612;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unbiased learning to rank (ULTR) studies the problem of mitigating various biases from implicit user feedback data such as clicks, and has been receiving considerable attention recently. A popular ULTR approach for real-world applications uses a two-tower architecture, where click modeling is factorized into a relevance tower with regular input features, and a bias tower with bias-relevant inputs such as the position of a document. A successful factorization will allow the relevance tower to be exempt from biases. In this work, we identify a critical issue that existing ULTR methods ignored - the bias tower can be confounded with the relevance tower via the underlying true relevance. In particular, the positions were determined by the logging policy, i.e., the previous production model, which would possess relevance information. We give both theoretical analysis and empirical results to show the negative effects on relevance tower due to such a correlation. We then propose three method
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;MR.COD&#30340;&#22810;&#36339;&#35777;&#25454;&#26816;&#32034;&#26041;&#27861;&#65292;&#29992;&#20110;&#25903;&#25345;&#36328;&#25991;&#26723;&#20851;&#31995;&#25277;&#21462;&#12290;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#26377;&#25928;&#22320;&#33719;&#21462;&#20102;&#36328;&#25991;&#26723;&#35777;&#25454;&#65292;&#24182;&#25552;&#21319;&#20102;&#23553;&#38381;&#21644;&#24320;&#25918;&#35774;&#32622;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2212.10786</link><description>&lt;p&gt;
&#36328;&#25991;&#26723;&#20851;&#31995;&#25277;&#21462;&#30340;&#22810;&#36339;&#35777;&#25454;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Multi-hop Evidence Retrieval for Cross-document Relation Extraction. (arXiv:2212.10786v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10786
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;MR.COD&#30340;&#22810;&#36339;&#35777;&#25454;&#26816;&#32034;&#26041;&#27861;&#65292;&#29992;&#20110;&#25903;&#25345;&#36328;&#25991;&#26723;&#20851;&#31995;&#25277;&#21462;&#12290;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#26377;&#25928;&#22320;&#33719;&#21462;&#20102;&#36328;&#25991;&#26723;&#35777;&#25454;&#65292;&#24182;&#25552;&#21319;&#20102;&#23553;&#38381;&#21644;&#24320;&#25918;&#35774;&#32622;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#31995;&#25277;&#21462;(RE)&#24050;&#32463;&#25193;&#23637;&#21040;&#36328;&#25991;&#26723;&#22330;&#26223;&#20013;&#65292;&#22240;&#20026;&#35768;&#22810;&#20851;&#31995;&#19981;&#20165;&#20165;&#22312;&#19968;&#20010;&#25991;&#26723;&#20013;&#25551;&#36848;&#12290;&#36825;&#19981;&#21487;&#36991;&#20813;&#22320;&#24102;&#26469;&#20102;&#26377;&#25928;&#30340;&#24320;&#25918;&#31354;&#38388;&#35777;&#25454;&#26816;&#32034;&#30340;&#25361;&#25112;&#65292;&#20197;&#25903;&#25345;&#36328;&#25991;&#26723;&#20851;&#31995;&#30340;&#25512;&#26029;&#65292;&#21516;&#26102;&#20063;&#24102;&#26469;&#20102;&#22810;&#36339;&#25512;&#29702;&#30340;&#25361;&#25112;&#65292;&#20197;&#22788;&#29702;&#25955;&#24067;&#22312;&#24320;&#25918;&#24335;&#25991;&#26723;&#38598;&#20013;&#30340;&#23454;&#20307;&#21644;&#35777;&#25454;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MR.COD(&#36328;&#25991;&#26723;&#20851;&#31995;&#25277;&#21462;&#30340;&#22810;&#36339;&#35777;&#25454;&#26816;&#32034;)&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#35777;&#25454;&#36335;&#24452;&#25366;&#25496;&#21644;&#25490;&#24207;&#30340;&#22810;&#36339;&#35777;&#25454;&#26816;&#32034;&#26041;&#27861;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#22810;&#20010;&#26816;&#32034;&#22120;&#30340;&#21464;&#20307;&#65292;&#20197;&#26174;&#31034;&#35777;&#25454;&#26816;&#32034;&#22312;&#36328;&#25991;&#26723;RE&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#36824;&#20026;&#27492;&#35774;&#32622;&#25552;&#20986;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#23494;&#38598;&#30340;&#26816;&#32034;&#22120;&#12290;&#22312;CodRED&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;MR.COD&#30340;&#35777;&#25454;&#26816;&#32034;&#26377;&#25928;&#22320;&#33719;&#21462;&#20102;&#36328;&#25991;&#26723;&#35777;&#25454;&#65292;&#24182;&#25552;&#21319;&#20102;&#23553;&#38381;&#21644;&#24320;&#25918;&#35774;&#32622;&#20013;&#30340;&#31471;&#21040;&#31471;RE&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Relation Extraction (RE) has been extended to cross-document scenarios because many relations are not simply described in a single document. This inevitably brings the challenge of efficient open-space evidence retrieval to support the inference of cross-document relations, along with the challenge of multi-hop reasoning on top of entities and evidence scattered in an open set of documents. To combat these challenges, we propose MR.COD (Multi-hop evidence retrieval for Cross-document relation extraction), which is a multi-hop evidence retrieval method based on evidence path mining and ranking. We explore multiple variants of retrievers to show evidence retrieval is essential in cross-document RE. We also propose a contextual dense retriever for this setting. Experiments on CodRED show that evidence retrieval with MR.COD effectively acquires crossdocument evidence and boosts end-to-end RE performance in both closed and open settings.
&lt;/p&gt;</description></item><item><title>&#8220;Tenrec&#8221;&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#22810;&#29992;&#36884;&#25512;&#33616;&#31995;&#32479;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#32422;500&#19975;&#29992;&#25143;&#21644;1.4&#20159;&#27425;&#20132;&#20114;&#65292;&#19981;&#20165;&#20855;&#26377;&#31215;&#26497;&#30340;&#29992;&#25143;&#21453;&#39304;&#65292;&#36824;&#26377;&#30495;&#27491;&#30340;&#36127;&#21453;&#39304;&#65292;&#21253;&#21547;&#20102;&#21508;&#31181;&#31867;&#22411;&#30340;&#29992;&#25143;&#31215;&#26497;&#21453;&#39304;&#65292;&#26159;&#19968;&#20010;&#32508;&#21512;&#24615;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#21487;&#29992;&#20110;&#35780;&#20272;&#25512;&#33616;&#31995;&#32479;&#27169;&#22411;&#30340;&#22810;&#20010;&#26041;&#38754;&#12290;</title><link>http://arxiv.org/abs/2210.10629</link><description>&lt;p&gt;
Tenrec: &#19968;&#31181;&#22823;&#35268;&#27169;&#22810;&#29992;&#36884;&#25512;&#33616;&#31995;&#32479;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Tenrec: A Large-scale Multipurpose Benchmark Dataset for Recommender Systems. (arXiv:2210.10629v3 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.10629
&lt;/p&gt;
&lt;p&gt;
&#8220;Tenrec&#8221;&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#22810;&#29992;&#36884;&#25512;&#33616;&#31995;&#32479;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#32422;500&#19975;&#29992;&#25143;&#21644;1.4&#20159;&#27425;&#20132;&#20114;&#65292;&#19981;&#20165;&#20855;&#26377;&#31215;&#26497;&#30340;&#29992;&#25143;&#21453;&#39304;&#65292;&#36824;&#26377;&#30495;&#27491;&#30340;&#36127;&#21453;&#39304;&#65292;&#21253;&#21547;&#20102;&#21508;&#31181;&#31867;&#22411;&#30340;&#29992;&#25143;&#31215;&#26497;&#21453;&#39304;&#65292;&#26159;&#19968;&#20010;&#32508;&#21512;&#24615;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#21487;&#29992;&#20110;&#35780;&#20272;&#25512;&#33616;&#31995;&#32479;&#27169;&#22411;&#30340;&#22810;&#20010;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#25512;&#33616;&#31995;&#32479;&#22522;&#20934;&#25968;&#25454;&#38598;&#35201;&#20040;&#35268;&#27169;&#36739;&#23567;&#65292;&#35201;&#20040;&#21482;&#28041;&#21450;&#38750;&#24120;&#26377;&#38480;&#24418;&#24335;&#30340;&#29992;&#25143;&#21453;&#39304;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#20840;&#26032;&#30340;&#12289;&#20844;&#24320;&#21487;&#29992;&#30340;&#25512;&#33616;&#31995;&#32479;&#25968;&#25454;&#38598;&#8220;Tenrec&#8221;&#65292;&#35760;&#24405;&#20102;&#26469;&#33258;&#22235;&#31181;&#19981;&#21516;&#25512;&#33616;&#22330;&#26223;&#30340;&#21508;&#31181;&#29992;&#25143;&#21453;&#39304;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;Tenrec &#20855;&#26377;&#20197;&#19979;&#20116;&#20010;&#29305;&#28857;&#65306;(1)&#23427;&#26159;&#22823;&#35268;&#27169;&#30340;&#65292;&#21253;&#21547;&#32422; 500 &#19975;&#29992;&#25143;&#21644; 1.4 &#20159;&#27425;&#20132;&#20114;;(2)&#23427;&#19981;&#20165;&#20855;&#26377;&#31215;&#26497;&#30340;&#29992;&#25143;&#21453;&#39304;&#65292;&#36824;&#26377;&#30495;&#27491;&#30340;&#36127;&#21453;&#39304;;(3)&#23427;&#21253;&#21547;&#20102;&#22235;&#20010;&#19981;&#21516;&#22330;&#26223;&#20013;&#30340;&#37325;&#21472;&#29992;&#25143;&#21644;&#39033;&#30446;;(4)&#23427;&#21253;&#21547;&#20102;&#21508;&#31181;&#31867;&#22411;&#30340;&#29992;&#25143;&#31215;&#26497;&#21453;&#39304;&#65292;&#20197;&#28857;&#20987;&#12289;&#21916;&#27426;&#12289;&#20998;&#20139;&#21644;&#20851;&#27880;&#31561;&#24418;&#24335;&#23637;&#29616;;(5)&#23427;&#36824;&#21253;&#21547;&#20102;&#29992;&#25143; ID &#21644;&#39033;&#30446; ID &#20197;&#22806;&#30340;&#20854;&#20182;&#29305;&#24449;&#12290;&#25105;&#20204;&#36816;&#34892;&#20102;&#22810;&#31181;&#26368;&#20808;&#36827;&#30340;&#25512;&#33616;&#31995;&#32479;&#27169;&#22411;&#26469;&#39564;&#35777; Tenrec&#65292;&#32467;&#26524;&#23637;&#31034;&#20102; Tenrec &#23545;&#25512;&#33616;&#31995;&#32479;&#27169;&#22411;&#30340;&#25361;&#25112;&#65292;&#20197;&#21450;&#20854;&#23454;&#38469;&#28508;&#21147;&#12290;Tenrec &#21487;&#20197;&#20316;&#20026;&#19968;&#20010;&#32508;&#21512;&#24615;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#35780;&#20272;&#25512;&#33616;&#31995;&#32479;&#27169;&#22411;&#30340;&#21508;&#20010;&#26041;&#38754;&#65292;&#22914;&#25512;&#33616;&#36136;&#37327;&#12289;&#21453;&#39304;&#39044;&#27979;&#12289;&#25968;&#25454;&#31232;&#30095;&#24615;&#12289;&#36801;&#31227;&#23398;&#20064;&#21644;&#22810;&#26679;&#24615;&#20419;&#36827;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing benchmark datasets for recommender systems (RS) either are created at a small scale or involve very limited forms of user feedback. RS models evaluated on such datasets often lack practical values for large-scale real-world applications. In this paper, we describe Tenrec, a novel and publicly available data collection for RS that records various user feedback from four different recommendation scenarios. To be specific, Tenrec has the following five characteristics: (1) it is large-scale, containing around 5 million users and 140 million interactions; (2) it has not only positive user feedback, but also true negative feedback (vs. one-class recommendation); (3) it contains overlapped users and items across four different scenarios; (4) it contains various types of user positive feedback, in forms of clicks, likes, shares, and follows, etc; (5) it contains additional features beyond the user IDs and item IDs. We verify Tenrec on ten diverse recommendation tasks by running sever
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#22312;&#32447;&#26597;&#35810;&#37325;&#20889;&#30340;&#33539;&#24335;&#65306;&#29983;&#25104;&#24335;&#65288;NLG&#65289;&#21644;&#23494;&#38598;&#26816;&#32034;&#65288;DR&#65289;&#26041;&#27861;&#12290;&#36890;&#36807;&#27604;&#36739;&#36825;&#20004;&#31181;&#26041;&#27861;&#24182;&#32467;&#21512;&#20248;&#21183;&#65292;&#25552;&#20986;&#20102;CLOVER-Unity&#27169;&#22411;&#65292;&#20854;NLG&#21644;DR&#32452;&#20214;&#22987;&#32456;&#20248;&#20110;&#21333;&#29420;&#35757;&#32451;&#30340;&#32452;&#20214;&#20197;&#21450;&#20854;&#20182;&#22522;&#32447;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2209.05861</link><description>&lt;p&gt;
&#32852;&#21512;&#29983;&#25104;&#19982;&#23494;&#38598;&#26816;&#32034;&#29992;&#20110;&#36190;&#21161;&#25628;&#32034;&#30340;&#26597;&#35810;&#37325;&#20889;
&lt;/p&gt;
&lt;p&gt;
Unified Generative &amp; Dense Retrieval for Query Rewriting in Sponsored Search. (arXiv:2209.05861v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.05861
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#22312;&#32447;&#26597;&#35810;&#37325;&#20889;&#30340;&#33539;&#24335;&#65306;&#29983;&#25104;&#24335;&#65288;NLG&#65289;&#21644;&#23494;&#38598;&#26816;&#32034;&#65288;DR&#65289;&#26041;&#27861;&#12290;&#36890;&#36807;&#27604;&#36739;&#36825;&#20004;&#31181;&#26041;&#27861;&#24182;&#32467;&#21512;&#20248;&#21183;&#65292;&#25552;&#20986;&#20102;CLOVER-Unity&#27169;&#22411;&#65292;&#20854;NLG&#21644;DR&#32452;&#20214;&#22987;&#32456;&#20248;&#20110;&#21333;&#29420;&#35757;&#32451;&#30340;&#32452;&#20214;&#20197;&#21450;&#20854;&#20182;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36190;&#21161;&#25628;&#32034;&#26159;&#25628;&#32034;&#24341;&#25806;&#30340;&#19968;&#20010;&#20851;&#38190;&#25910;&#20837;&#26469;&#28304;&#65292;&#24191;&#21578;&#20027;&#36890;&#36807;&#31454;&#20215;&#26041;&#24335;&#38024;&#23545;&#29992;&#25143;&#25110;&#24863;&#20852;&#36259;&#30340;&#25628;&#32034;&#26597;&#35810;&#31454;&#26631;&#20851;&#38190;&#23383;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20851;&#38190;&#23383;&#31354;&#38388;&#24222;&#22823;&#19988;&#21160;&#24577;&#21464;&#21270;&#12289;&#27169;&#31946;&#30340;&#29992;&#25143;/&#24191;&#21578;&#20027;&#24847;&#22270;&#20197;&#21450;&#21508;&#31181;&#21487;&#33021;&#30340;&#20027;&#39064;&#21644;&#35821;&#35328;&#65292;&#25214;&#21040;&#19982;&#32473;&#23450;&#26597;&#35810;&#30456;&#20851;&#30340;&#20851;&#38190;&#23383;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;&#32447;&#26597;&#35810;&#37325;&#20889;&#30340;&#20004;&#31181;&#33539;&#24335;&#65306;&#29983;&#25104;&#24335;&#65288;NLG&#65289;&#21644;&#23494;&#38598;&#26816;&#32034;&#65288;DR&#65289;&#26041;&#27861;&#20043;&#38388;&#30340;&#20840;&#38754;&#27604;&#36739;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#20004;&#31181;&#26041;&#27861;&#37117;&#25552;&#20379;&#20102;&#20114;&#34917;&#30340;&#19988;&#21487;&#30456;&#21152;&#30340;&#20248;&#21183;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#30001;&#36825;&#20004;&#31181;&#26041;&#27861;&#26816;&#32034;&#21040;&#30340;&#39640;&#36136;&#37327;&#20851;&#38190;&#23383;&#20013;&#26377;&#32422;40%&#26159;&#29420;&#29305;&#30340;&#65292;&#32780;&#21478;&#19968;&#31181;&#26041;&#27861;&#27809;&#26377;&#25214;&#21040;&#12290;&#20026;&#20102;&#21457;&#25381;&#20004;&#31181;&#26041;&#27861;&#30340;&#20248;&#21183;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CLOVER-Unity&#65292;&#36825;&#26159;&#19968;&#31181;&#23558;&#29983;&#25104;&#24335;&#19982;&#23494;&#38598;&#26816;&#32034;&#26041;&#27861;&#32467;&#21512;&#22312;&#19968;&#20010;&#27169;&#22411;&#20013;&#30340;&#26032;&#26041;&#27861;&#12290;&#36890;&#36807;&#31163;&#32447;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;CLOVER-Unity&#30340;NLG&#21644;DR&#32452;&#20214;&#22987;&#32456;&#20248;&#20110;&#21333;&#29420;&#35757;&#32451;&#30340;NLG&#21644;DR&#32452;&#20214;&#20197;&#21450;&#20854;&#20182;&#24378;&#22522;&#32447;&#27169;&#22411;&#65292;&#20855;&#26377;&#26816;&#32034;&#21040;&#20851;&#38190;&#23383;&#30340;&#30456;&#20851;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sponsored search is a key revenue source for search engines, where advertisers bid on keywords to target users or search queries of interest. However, finding relevant keywords for a given query is challenging due to the large and dynamic keyword space, ambiguous user/advertiser intents, and diverse possible topics and languages. In this work, we present a comprehensive comparison between two paradigms for online query rewriting: Generative (NLG) and Dense Retrieval (DR) methods. We observe that both methods offer complementary benefits that are additive. As a result, we show that around 40% of the high-quality keywords retrieved by the two approaches are unique and not retrieved by the other. To leverage the strengths of both methods, we propose CLOVER-Unity, a novel approach that unifies generative and dense retrieval methods in one single model. Through offline experiments, we show that the NLG and DR components of CLOVER-Unity consistently outperform individually trained NLG and DR
&lt;/p&gt;</description></item><item><title>LexMAE&#26159;&#19968;&#20010;&#26032;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#37325;&#35201;&#24615;&#24863;&#30693;&#30340;&#35789;&#27719;&#34920;&#31034;&#65292;&#22312;&#22823;&#35268;&#27169;&#20449;&#24687;&#26816;&#32034;&#20013;&#34920;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2208.14754</link><description>&lt;p&gt;
LexMAE&#65306;&#29992;&#20110;&#22823;&#35268;&#27169;&#26816;&#32034;&#30340;&#35789;&#27719;&#38480;&#21046;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
LexMAE: Lexicon-Bottlenecked Pretraining for Large-Scale Retrieval. (arXiv:2208.14754v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.14754
&lt;/p&gt;
&lt;p&gt;
LexMAE&#26159;&#19968;&#20010;&#26032;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#37325;&#35201;&#24615;&#24863;&#30693;&#30340;&#35789;&#27719;&#34920;&#31034;&#65292;&#22312;&#22823;&#35268;&#27169;&#20449;&#24687;&#26816;&#32034;&#20013;&#34920;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#35268;&#27169;&#26816;&#32034;&#20013;&#65292;&#21033;&#29992;&#35789;&#27719;&#26435;&#37325;&#33539;&#20363;&#65292;&#23398;&#20064;&#22312;&#35789;&#27719;&#31354;&#38388;&#20013;&#30340;&#21152;&#26435;&#31232;&#30095;&#34920;&#31034;&#65292;&#26174;&#31034;&#20986;&#20102;&#39640;&#36136;&#37327;&#21644;&#20302;&#24310;&#36831;&#30340;&#26377;&#21069;&#36884;&#30340;&#32467;&#26524;&#12290;&#20026;&#20102;&#22635;&#34917;&#35821;&#35328;&#24314;&#27169;&#21644;&#35789;&#27719;&#21152;&#26435;&#26816;&#32034;&#20043;&#38388;&#30340;&#24040;&#22823;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;LexMAE&#65292;&#29992;&#20110;&#23398;&#20064;&#37325;&#35201;&#24615;&#24863;&#30693;&#30340;&#35789;&#27719;&#34920;&#31034;&#12290;&#36825;&#19968;&#26694;&#26550;&#36890;&#36807;&#21152;&#20837;&#19968;&#20010;&#35789;&#27719;&#29942;&#39048;&#27169;&#22359;&#65292;&#23558;&#26222;&#36890;&#30340;&#35821;&#35328;&#27169;&#22411;&#32534;&#30721;&#22120;&#21644;&#24369;&#21270;&#30340;&#35299;&#30721;&#22120;&#20043;&#38388;&#21152;&#20837;&#20102;&#19968;&#20010;&#36830;&#32493;&#35789;&#34955;&#29942;&#39048;&#65292;&#20197;&#23454;&#29616;&#26080;&#30417;&#30563;&#23398;&#20064;&#35789;&#27719;&#37325;&#35201;&#24615;&#20998;&#24067;&#12290;&#39044;&#35757;&#32451;&#21518;&#30340;LexMAE&#24050;&#32463;&#34920;&#29616;&#20986;&#20102;&#22312;&#22823;&#35268;&#27169;&#20449;&#24687;&#26816;&#32034;&#20013;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In large-scale retrieval, the lexicon-weighting paradigm, learning weighted sparse representations in vocabulary space, has shown promising results with high quality and low latency. Despite it deeply exploiting the lexicon-representing capability of pre-trained language models, a crucial gap remains between language modeling and lexicon-weighting retrieval -- the former preferring certain or low-entropy words whereas the latter favoring pivot or high-entropy words -- becoming the main barrier to lexicon-weighting performance for large-scale retrieval. To bridge this gap, we propose a brand-new pre-training framework, lexicon-bottlenecked masked autoencoder (LexMAE), to learn importance-aware lexicon representations. Essentially, we present a lexicon-bottlenecked module between a normal language modeling encoder and a weakened decoder, where a continuous bag-of-words bottleneck is constructed to learn a lexicon-importance distribution in an unsupervised fashion. The pre-trained LexMAE 
&lt;/p&gt;</description></item><item><title>UnifieR&#26159;&#19968;&#20010;&#23558;PLM&#30340;&#23494;&#38598;&#21521;&#37327;&#21644;&#22522;&#20110;&#35789;&#27719;&#34920;&#30340;&#26816;&#32034;&#32479;&#19968;&#21040;&#19968;&#20010;&#27169;&#22411;&#20013;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;&#27573;&#33853;&#26816;&#32034;&#22522;&#20934;&#27979;&#35797;&#20013;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2205.11194</link><description>&lt;p&gt;
&#32479;&#19968;&#26816;&#32034;&#22120;&#65306;&#22823;&#35268;&#27169;&#26816;&#32034;&#30340;&#32479;&#19968;&#26816;&#32034;&#22120;
&lt;/p&gt;
&lt;p&gt;
UnifieR: A Unified Retriever for Large-Scale Retrieval. (arXiv:2205.11194v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.11194
&lt;/p&gt;
&lt;p&gt;
UnifieR&#26159;&#19968;&#20010;&#23558;PLM&#30340;&#23494;&#38598;&#21521;&#37327;&#21644;&#22522;&#20110;&#35789;&#27719;&#34920;&#30340;&#26816;&#32034;&#32479;&#19968;&#21040;&#19968;&#20010;&#27169;&#22411;&#20013;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;&#27573;&#33853;&#26816;&#32034;&#22522;&#20934;&#27979;&#35797;&#20013;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#26816;&#32034;&#26159;&#25351;&#22312;&#32473;&#23450;&#26597;&#35810;&#30340;&#24773;&#20917;&#19979;&#20174;&#22823;&#37327;&#25991;&#26723;&#20013;&#21484;&#22238;&#30456;&#20851;&#25991;&#26723;&#12290;&#23427;&#20381;&#36182;&#20110;&#34920;&#24449;&#23398;&#20064;&#65292;&#23558;&#25991;&#26723;&#21644;&#26597;&#35810;&#23884;&#20837;&#21040;&#19968;&#20010;&#20849;&#21516;&#30340;&#35821;&#20041;&#32534;&#30721;&#31354;&#38388;&#20013;&#12290;&#26681;&#25454;&#32534;&#30721;&#31354;&#38388;&#65292;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#30340;&#26368;&#36817;&#26816;&#32034;&#26041;&#27861;&#21487;&#20197;&#31895;&#30053;&#22320;&#20998;&#20026;&#23494;&#38598;&#21521;&#37327;&#25110;&#22522;&#20110;&#35789;&#27719;&#34920;&#30340;&#33539;&#20363;&#12290;&#36825;&#20004;&#31181;&#33539;&#20363;&#22312;&#19981;&#21516;&#31890;&#24230;&#30340;&#20840;&#23616;&#24207;&#21015;&#32423;&#21387;&#32553;&#21644;&#23616;&#37096;&#21333;&#35789;&#32423;&#19978;&#19979;&#25991;&#20013;&#23637;&#29616;&#20102;PLMs&#30340;&#34920;&#24449;&#33021;&#21147;&#12290;&#21463;&#21040;&#23427;&#20204;&#20114;&#34917;&#30340;&#20840;&#23616;&#23616;&#37096;&#19978;&#19979;&#25991;&#21270;&#21644;&#19981;&#21516;&#30340;&#20195;&#34920;&#35270;&#35282;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#32479;&#19968;&#26816;&#32034;&#22120;&#65292;&#23427;&#23558;&#23494;&#38598;&#21521;&#37327;&#21644;&#22522;&#20110;&#35789;&#27719;&#34920;&#30340;&#26816;&#32034;&#32479;&#19968;&#21040;&#19968;&#20010;&#27169;&#22411;&#20013;&#65292;&#20855;&#26377;&#21452;&#37325;&#34920;&#31034;&#33021;&#21147;&#12290;&#23545;&#27573;&#33853;&#26816;&#32034;&#22522;&#20934;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#23427;&#22312;&#20004;&#20010;&#33539;&#20363;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#26356;&#22909;&#30340;&#26816;&#32034;&#36136;&#37327;&#30340;uni-retrieval&#26041;&#26696;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#35813;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale retrieval is to recall relevant documents from a huge collection given a query. It relies on representation learning to embed documents and queries into a common semantic encoding space. According to the encoding space, recent retrieval methods based on pre-trained language models (PLM) can be coarsely categorized into either dense-vector or lexicon-based paradigms. These two paradigms unveil the PLMs' representation capability in different granularities, i.e., global sequence-level compression and local word-level contexts, respectively. Inspired by their complementary global-local contextualization and distinct representing views, we propose a new learning framework, UnifieR which unifies dense-vector and lexicon-based retrieval in one model with a dual-representing capability. Experiments on passage retrieval benchmarks verify its effectiveness in both paradigms. A uni-retrieval scheme is further presented with even better retrieval quality. We lastly evaluate the model 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#19968;&#33268;&#24615;&#26041;&#27861;&#26469;&#35299;&#20915;&#30697;&#38453;&#21644;&#24352;&#37327;&#34917;&#20840;&#38382;&#39064;&#65292;&#22312;&#25512;&#33616;&#31995;&#32479;&#24212;&#29992;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36890;&#36807;&#20445;&#30041;&#21333;&#20301;&#27604;&#20363;&#21644;&#19968;&#33268;&#24615;&#20004;&#20010;&#32422;&#26463;&#26465;&#20214;&#21487;&#20197;&#23454;&#29616;&#35299;&#30340;&#23384;&#22312;&#24615;&#19982;&#21807;&#19968;&#24615;&#12290;</title><link>http://arxiv.org/abs/2204.01815</link><description>&lt;p&gt;
&#20855;&#26377;&#21487;&#35777;&#26126;&#30340;&#19968;&#33268;&#24615;&#21644;&#20844;&#24179;&#20445;&#35777;&#30340;&#25512;&#33616;&#31995;&#32479;&#24352;&#37327;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
Tensor Completion with Provable Consistency and Fairness Guarantees for Recommender Systems. (arXiv:2204.01815v3 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.01815
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#19968;&#33268;&#24615;&#26041;&#27861;&#26469;&#35299;&#20915;&#30697;&#38453;&#21644;&#24352;&#37327;&#34917;&#20840;&#38382;&#39064;&#65292;&#22312;&#25512;&#33616;&#31995;&#32479;&#24212;&#29992;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36890;&#36807;&#20445;&#30041;&#21333;&#20301;&#27604;&#20363;&#21644;&#19968;&#33268;&#24615;&#20004;&#20010;&#32422;&#26463;&#26465;&#20214;&#21487;&#20197;&#23454;&#29616;&#35299;&#30340;&#23384;&#22312;&#24615;&#19982;&#21807;&#19968;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#19968;&#33268;&#24615;&#30340;&#26041;&#27861;&#26469;&#23450;&#20041;&#21644;&#35299;&#20915;&#38750;&#36127;/&#27491;&#30697;&#38453;&#21644;&#24352;&#37327;&#34917;&#20840;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#30340;&#26032;&#39062;&#20043;&#22788;&#22312;&#20110;&#65292;&#25105;&#20204;&#19981;&#26159;&#20154;&#20026;&#22320;&#23558;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#20219;&#24847;&#20248;&#21270;&#38382;&#39064;&#65292;&#20363;&#22914;&#65292;&#26368;&#23567;&#21270;&#19968;&#20010;&#32467;&#26500;&#37327;&#65292;&#22914;&#31209;&#25110;&#33539;&#25968;&#65292;&#32780;&#26159;&#23637;&#31034;&#20102;&#19968;&#20010;&#21333;&#19968;&#30340;&#23646;&#24615;/&#32422;&#26463;&#65306;&#20445;&#30041;&#21333;&#20301;&#27604;&#20363;&#19968;&#33268;&#24615;&#65292;&#20445;&#35777;&#20102;&#35299;&#30340;&#23384;&#22312;&#65292;&#24182;&#22312;&#30456;&#23545;&#36739;&#24369;&#30340;&#25903;&#25345;&#20551;&#35774;&#19979;&#20445;&#35777;&#20102;&#35299;&#30340;&#21807;&#19968;&#24615;&#12290;&#35813;&#26694;&#26550;&#21644;&#35299;&#31639;&#27861;&#20063;&#30452;&#25509;&#25512;&#24191;&#21040;&#20219;&#24847;&#32500;&#24230;&#30340;&#24352;&#37327;&#20013;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#22266;&#23450;&#32500;&#24230; d &#30340;&#38382;&#39064;&#35268;&#27169;&#30340;&#32447;&#24615;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;&#22312;&#25512;&#33616;&#31995;&#32479;&#24212;&#29992;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20004;&#20010;&#21512;&#29702;&#30340;&#24615;&#36136;&#65292;&#36825;&#20123;&#24615;&#36136;&#24212;&#35813;&#36866;&#29992;&#20110;&#20219;&#20309; RS &#38382;&#39064;&#30340;&#35299;&#65292;&#36275;&#20197;&#20801;&#35768;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20869;&#24314;&#31435;&#21807;&#19968;&#24615;&#20445;&#35777;&#12290;&#20851;&#38190;&#29702;&#35770;&#36129;&#29486;&#26159;&#23637;&#31034;&#20102;&#36825;&#20123;&#32422;&#26463;&#19979;&#35299;&#30340;&#23384;&#22312;&#24615;&#19982;&#21807;&#19968;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a new consistency-based approach for defining and solving nonnegative/positive matrix and tensor completion problems. The novelty of the framework is that instead of artificially making the problem well-posed in the form of an application-arbitrary optimization problem, e.g., minimizing a bulk structural measure such as rank or norm, we show that a single property/constraint: preserving unit-scale consistency, guarantees the existence of both a solution and, under relatively weak support assumptions, uniqueness. The framework and solution algorithms also generalize directly to tensors of arbitrary dimensions while maintaining computational complexity that is linear in problem size for fixed dimension d. In the context of recommender system (RS) applications, we prove that two reasonable properties that should be expected to hold for any solution to the RS problem are sufficient to permit uniqueness guarantees to be established within our framework. Key theoretical contribu
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#20351;&#29992;&#27010;&#24565;&#28608;&#27963;&#21521;&#37327;&#26469;&#25226;&#29992;&#25143;&#25551;&#36848;&#21830;&#21697;&#30340;&#23646;&#24615;&#30340;&#35821;&#20041;&#34920;&#36798;&#20986;&#26469;&#65292;&#20197;&#25913;&#36827;&#25512;&#33616;&#31995;&#32479;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2202.02830</link><description>&lt;p&gt;
&#20351;&#29992;&#27010;&#24565;&#28608;&#27963;&#21521;&#37327;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#21457;&#29616;&#36719;&#23646;&#24615;&#30340;&#20010;&#24615;&#21270;&#35821;&#20041;
&lt;/p&gt;
&lt;p&gt;
Discovering Personalized Semantics for Soft Attributes in Recommender Systems using Concept Activation Vectors. (arXiv:2202.02830v3 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.02830
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20351;&#29992;&#27010;&#24565;&#28608;&#27963;&#21521;&#37327;&#26469;&#25226;&#29992;&#25143;&#25551;&#36848;&#21830;&#21697;&#30340;&#23646;&#24615;&#30340;&#35821;&#20041;&#34920;&#36798;&#20986;&#26469;&#65292;&#20197;&#25913;&#36827;&#25512;&#33616;&#31995;&#32479;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#20114;&#24335;&#25512;&#33616;&#31995;&#32479;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#33539;&#20363;&#65292;&#20197;&#20811;&#26381;&#20256;&#32479;&#25512;&#33616;&#31995;&#32479;&#25152;&#20351;&#29992;&#30340;&#21407;&#22987;&#29992;&#25143;&#21453;&#39304;&#30340;&#23616;&#38480;&#24615;&#65288;&#20363;&#22914;&#28857;&#20987;&#12289;&#39033;&#30446;&#28040;&#36153;&#12289;&#35780;&#20998;&#65289;&#12290;&#23427;&#20204;&#20801;&#35768;&#29992;&#25143;&#20197;&#26356;&#20016;&#23500;&#30340;&#26041;&#24335;&#34920;&#36798;&#24847;&#22270;&#12289;&#20559;&#22909;&#12289;&#32422;&#26463;&#21644;&#19978;&#19979;&#25991;&#65292;&#36890;&#24120;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#65288;&#21253;&#25324;&#20998;&#31867;&#25628;&#32034;&#21644;&#23545;&#35805;&#65289;&#12290;&#28982;&#32780;&#65292;&#38656;&#35201;&#26356;&#22810;&#30340;&#30740;&#31350;&#26469;&#25214;&#21040;&#20351;&#29992;&#36825;&#20123;&#21453;&#39304;&#30340;&#26368;&#26377;&#25928;&#26041;&#27861;&#12290;&#19968;&#20010;&#25361;&#25112;&#26159;&#20174;&#32463;&#24120;&#29992;&#20110;&#25551;&#36848;&#25152;&#38656;&#39033;&#30446;&#30340;&#24320;&#25918;&#24335;&#26415;&#35821;&#25110;&#23646;&#24615;&#20013;&#25512;&#26029;&#29992;&#25143;&#30340;&#35821;&#20041;&#24847;&#22270;&#65292;&#24182;&#20351;&#29992;&#23427;&#26469;&#25913;&#36827;&#25512;&#33616;&#32467;&#26524;&#12290;&#21033;&#29992;&#26368;&#36817;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#24320;&#21457;&#30340;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#8212;&#8212;&#27010;&#24565;&#28608;&#27963;&#21521;&#37327;&#65288;CAVs&#65289;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#23398;&#20064;&#19968;&#31181;&#34920;&#31034;&#65292;&#25429;&#25417;&#36825;&#20123;&#23646;&#24615;&#30340;&#35821;&#20041;&#65292;&#24182;&#23558;&#23427;&#20204;&#36830;&#25509;&#21040;&#29992;&#25143;&#30340;&#20559;&#22909;&#21644;&#34892;&#20026;&#20013;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#19968;&#20010;&#26032;&#21151;&#33021;&#26159;&#23427;&#33021;&#22815;&#21306;&#20998;
&lt;/p&gt;
&lt;p&gt;
Interactive recommender systems have emerged as a promising paradigm to overcome the limitations of the primitive user feedback used by traditional recommender systems (e.g., clicks, item consumption, ratings). They allow users to express intent, preferences, constraints, and contexts in a richer fashion, often using natural language (including faceted search and dialogue). Yet more research is needed to find the most effective ways to use this feedback. One challenge is inferring a user's semantic intent from the open-ended terms or attributes often used to describe a desired item, and using it to refine recommendation results. Leveraging concept activation vectors (CAVs) [26], a recently developed approach for model interpretability in machine learning, we develop a framework to learn a representation that captures the semantics of such attributes and connects them to user preferences and behaviors in recommender systems. One novel feature of our approach is its ability to distinguis
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#36229;&#20960;&#20309;&#26816;&#39564;&#22312;&#36873;&#23450;&#30340;&#30495;&#23454;&#25968;&#25454;&#25991;&#26723;&#26816;&#32034;&#21644;&#25688;&#35201;&#20219;&#21153;&#20013;&#34920;&#29616;&#19982;&#24120;&#29992;&#30340;TF-IDF&#21464;&#20307;&#30456;&#24403;&#65292;&#36825;&#25552;&#20379;&#20102;TF-IDF&#38271;&#26399;&#26377;&#25928;&#24615;&#30340;&#32479;&#35745;&#26174;&#33879;&#24615;&#35299;&#37322;&#30340;&#31532;&#19968;&#27493;&#12290;</title><link>http://arxiv.org/abs/2002.11844</link><description>&lt;p&gt;
&#36229;&#20960;&#20309;&#26816;&#39564;&#22312;&#26631;&#20934;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#20013;&#34920;&#29616;&#19982;TF-IDF&#21464;&#20307;&#30456;&#24403;
&lt;/p&gt;
&lt;p&gt;
The hypergeometric test performs comparably to a common TF-IDF variant on standard information retrieval tasks. (arXiv:2002.11844v3 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2002.11844
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#36229;&#20960;&#20309;&#26816;&#39564;&#22312;&#36873;&#23450;&#30340;&#30495;&#23454;&#25968;&#25454;&#25991;&#26723;&#26816;&#32034;&#21644;&#25688;&#35201;&#20219;&#21153;&#20013;&#34920;&#29616;&#19982;&#24120;&#29992;&#30340;TF-IDF&#21464;&#20307;&#30456;&#24403;&#65292;&#36825;&#25552;&#20379;&#20102;TF-IDF&#38271;&#26399;&#26377;&#25928;&#24615;&#30340;&#32479;&#35745;&#26174;&#33879;&#24615;&#35299;&#37322;&#30340;&#31532;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35789;&#39057;-&#36870;&#25991;&#26723;&#39057;&#29575;&#65288;TF-IDF&#65289;&#21450;&#20854;&#35768;&#22810;&#21464;&#20307;&#24418;&#25104;&#20102;&#19968;&#31867;&#24120;&#29992;&#30340;&#26415;&#35821;&#21152;&#26435;&#20989;&#25968;&#65292;&#22312;&#20449;&#24687;&#26816;&#32034;&#24212;&#29992;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;&#34429;&#28982;TF-IDF&#26368;&#21021;&#26159;&#19968;&#31181;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#20294;&#24050;&#32463;&#25552;&#20986;&#20102;&#22522;&#20110;&#20449;&#24687;&#29702;&#35770;&#12289;&#27010;&#29575;&#21644;&#19982;&#38543;&#26426;&#24615;&#32972;&#31163;&#30340;&#33539;&#24335;&#30340;&#29702;&#35770;&#35777;&#26126;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#39033;&#23454;&#35777;&#30740;&#31350;&#65292;&#34920;&#26126;&#22312;&#36873;&#23450;&#30340;&#30495;&#23454;&#25968;&#25454;&#25991;&#26723;&#26816;&#32034;&#21644;&#25688;&#35201;&#20219;&#21153;&#20013;&#65292;&#36229;&#20960;&#20309;&#26816;&#39564;&#30340;&#32479;&#35745;&#26174;&#33879;&#24615;&#19982;&#24120;&#29992;&#30340;TF-IDF&#21464;&#20307;&#38750;&#24120;&#25509;&#36817;&#12290;&#36825;&#20123;&#21457;&#29616;&#34920;&#26126;TF-IDF&#21464;&#20307;&#19982;&#36229;&#20960;&#20309;&#26816;&#39564;P&#20540;&#30340;&#36127;&#23545;&#25968;&#65288;&#21363;&#36229;&#20960;&#20309;&#20998;&#24067;&#23614;&#27010;&#29575;&#65289;&#20043;&#38388;&#23384;&#22312;&#26681;&#26412;&#30340;&#25968;&#23398;&#36830;&#32467;&#26377;&#24453;&#38416;&#26126;&#12290;&#25105;&#20204;&#22312;&#27492;&#25552;&#20379;&#23454;&#35777;&#26696;&#20363;&#30740;&#31350;&#65292;&#20316;&#20026;&#20174;&#32479;&#35745;&#26174;&#33879;&#24615;&#35282;&#24230;&#35299;&#37322;TF-IDF&#38271;&#26399;&#26377;&#25928;&#24615;&#30340;&#31532;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Term frequency-inverse document frequency, or tf-idf for short, and its many variants form a class of term weighting functions the members of which are widely used in information retrieval applications. While tf-idf was originally proposed as a heuristic, theoretical justifications grounded in information theory, probability, and the divergence from randomness paradigm have been advanced. In this work, we present an empirical study showing that the hypergeometric test of statistical significance corresponds very nearly with a common tf-idf variant on selected real-data document retrieval and summarization tasks. These findings suggest that a fundamental mathematical connection between the tf-idf variant and the negative logarithm of the hypergeometric test P-value (i.e., a hypergeometric distribution tail probability) remains to be elucidated. We offer the empirical case study herein as a first step toward explaining the long-standing effectiveness of tf-idf from a statistical signific
&lt;/p&gt;</description></item></channel></rss>