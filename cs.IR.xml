<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;LLaMA&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25928;&#26524;&#20248;&#20110;&#36739;&#23567;&#27169;&#22411;&#65292;&#24182;&#19988;&#33021;&#22815;&#20840;&#38754;&#22320;&#34920;&#31034;&#25972;&#20010;&#25991;&#26723;&#65292;&#28040;&#38500;&#20102;&#20256;&#32479;&#30340;&#20998;&#27573;&#21644;&#27719;&#38598;&#31574;&#30053;&#30340;&#38656;&#27714;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;RepLLaMA-RankLLaMA&#27969;&#27700;&#32447;&#22312;&#38646;-shot&#24773;&#20917;&#19979;&#23637;&#29616;&#20986;&#24378;&#22823;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.08319</link><description>&lt;p&gt;
&#23545;&#22810;&#38454;&#27573;&#25991;&#26412;&#26816;&#32034;&#20013;&#30340;LLaMA&#36827;&#34892;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Fine-Tuning LLaMA for Multi-Stage Text Retrieval. (arXiv:2310.08319v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08319
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;LLaMA&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25928;&#26524;&#20248;&#20110;&#36739;&#23567;&#27169;&#22411;&#65292;&#24182;&#19988;&#33021;&#22815;&#20840;&#38754;&#22320;&#34920;&#31034;&#25972;&#20010;&#25991;&#26723;&#65292;&#28040;&#38500;&#20102;&#20256;&#32479;&#30340;&#20998;&#27573;&#21644;&#27719;&#38598;&#31574;&#30053;&#30340;&#38656;&#27714;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;RepLLaMA-RankLLaMA&#27969;&#27700;&#32447;&#22312;&#38646;-shot&#24773;&#20917;&#19979;&#23637;&#29616;&#20986;&#24378;&#22823;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20986;&#29616;&#20043;&#21069;&#65292;&#22810;&#38454;&#27573;&#25991;&#26412;&#26816;&#32034;&#30340;&#26377;&#25928;&#24615;&#24050;&#32463;&#24471;&#21040;&#20102;&#20805;&#20998;&#30340;&#35777;&#26126;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#30740;&#31350;&#37117;&#20351;&#29992;&#20102;&#36807;&#26102;&#30340;&#27169;&#22411;&#65292;&#27809;&#26377;&#32771;&#34385;&#21040;&#26368;&#26032;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#23637;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;&#26368;&#20808;&#36827;&#30340;LLMs&#21487;&#33021;&#24102;&#26469;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#23545;&#26368;&#26032;&#30340;LLaMA&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#30740;&#31350;&#65292;&#36890;&#36807;&#20351;&#29992;MS MARCO&#25968;&#25454;&#38598;&#65292;&#23558;&#20854;&#20316;&#20026;&#31264;&#23494;&#26816;&#32034;&#22120;&#65288;RepLLaMA&#65289;&#21644;&#28857;&#23545;&#28857;&#20877;&#25490;&#24207;&#22120;&#65288;RankLLaMA&#65289;&#65292;&#29992;&#20110;&#27573;&#33853;&#26816;&#32034;&#21644;&#25991;&#26723;&#26816;&#32034;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#30830;&#23454;&#36229;&#36807;&#20102;&#36739;&#23567;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;LLMs&#21487;&#20197;&#22266;&#26377;&#22320;&#22788;&#29702;&#26356;&#38271;&#30340;&#19978;&#19979;&#25991;&#65292;&#23427;&#20204;&#21487;&#20197;&#20840;&#38754;&#22320;&#34920;&#31034;&#25972;&#20010;&#25991;&#26723;&#65292;&#28040;&#38500;&#20102;&#20256;&#32479;&#30340;&#20998;&#27573;&#21644;&#27719;&#38598;&#31574;&#30053;&#30340;&#38656;&#27714;&#12290;&#27492;&#22806;&#65292;&#23545;BEIR&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;RepLLaMA-RankLLaMA&#27969;&#27700;&#32447;&#23637;&#29616;&#20102;&#24378;&#22823;&#30340;&#38646;-shot&#26377;&#25928;&#24615;&#12290;&#26469;&#33258;&#36825;&#39033;&#30740;&#31350;&#30340;&#27169;&#22411;&#26816;&#26597;&#28857;...
&lt;/p&gt;
&lt;p&gt;
The effectiveness of multi-stage text retrieval has been solidly demonstrated since before the era of pre-trained language models. However, most existing studies utilize models that predate recent advances in large language models (LLMs). This study seeks to explore potential improvements that state-of-the-art LLMs can bring. We conduct a comprehensive study, fine-tuning the latest LLaMA model both as a dense retriever (RepLLaMA) and as a pointwise reranker (RankLLaMA) for both passage retrieval and document retrieval using the MS MARCO datasets. Our findings demonstrate that the effectiveness of large language models indeed surpasses that of smaller models. Additionally, since LLMs can inherently handle longer contexts, they can represent entire documents holistically, obviating the need for traditional segmenting and pooling strategies. Furthermore, evaluations on BEIR demonstrate that our RepLLaMA-RankLLaMA pipeline exhibits strong zero-shot effectiveness. Model checkpoints from thi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#21033;&#29992;GUI&#20132;&#20114;&#25968;&#25454;&#26469;&#25913;&#36827;&#22522;&#20110;&#25991;&#26412;&#26816;&#32034;&#30340;&#38169;&#35823;&#23450;&#20301;&#26041;&#27861;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#23558;&#29992;&#25143;&#30028;&#38754;&#20013;&#30340;&#20449;&#24687;&#19982;&#38169;&#35823;&#25253;&#21578;&#20013;&#30340;&#20449;&#24687;&#30456;&#36830;&#25509;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#20449;&#24687;&#26469;&#26816;&#32034;&#21487;&#33021;&#26377;&#38169;&#35823;&#30340;&#25991;&#20214;&#65292;&#21487;&#20197;&#25552;&#39640;&#38169;&#35823;&#23450;&#20301;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.08083</link><description>&lt;p&gt;
&#21033;&#29992;GUI&#20132;&#20114;&#25968;&#25454;&#26469;&#25913;&#36827;&#22522;&#20110;&#25991;&#26412;&#26816;&#32034;&#30340;&#38169;&#35823;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
On Using GUI Interaction Data to Improve Text Retrieval-based Bug Localization. (arXiv:2310.08083v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08083
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#21033;&#29992;GUI&#20132;&#20114;&#25968;&#25454;&#26469;&#25913;&#36827;&#22522;&#20110;&#25991;&#26412;&#26816;&#32034;&#30340;&#38169;&#35823;&#23450;&#20301;&#26041;&#27861;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#23558;&#29992;&#25143;&#30028;&#38754;&#20013;&#30340;&#20449;&#24687;&#19982;&#38169;&#35823;&#25253;&#21578;&#20013;&#30340;&#20449;&#24687;&#30456;&#36830;&#25509;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#20449;&#24687;&#26469;&#26816;&#32034;&#21487;&#33021;&#26377;&#38169;&#35823;&#30340;&#25991;&#20214;&#65292;&#21487;&#20197;&#25552;&#39640;&#38169;&#35823;&#23450;&#20301;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31649;&#29702;&#38169;&#35823;&#25253;&#21578;&#20013;&#26368;&#37325;&#35201;&#30340;&#20219;&#21153;&#20043;&#19968;&#26159;&#23450;&#20301;&#25925;&#38556;&#65292;&#20197;&#20415;&#21487;&#20197;&#24212;&#29992;&#20462;&#22797;&#12290;&#22240;&#27492;&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#33268;&#21147;&#20110;&#33258;&#21160;&#21270;&#38169;&#35823;&#23450;&#20301;&#20219;&#21153;&#65292;&#23558;&#20854;&#20316;&#20026;&#20449;&#24687;&#26816;&#32034;&#38382;&#39064;&#36827;&#34892;&#24314;&#27169;&#65292;&#36890;&#36807;&#26816;&#32034;&#28508;&#22312;&#30340;&#26377;&#38169;&#35823;&#30340;&#25991;&#20214;&#65292;&#24182;&#26681;&#25454;&#20854;&#19982;&#32473;&#23450;&#38169;&#35823;&#25253;&#21578;&#30340;&#25991;&#26412;&#30456;&#20284;&#24615;&#36827;&#34892;&#25490;&#21517;&#12290;&#28982;&#32780;&#65292;&#38169;&#35823;&#25253;&#21578;&#20013;&#21253;&#21547;&#30340;&#20449;&#24687;&#19982;&#28304;&#20195;&#30721;&#25991;&#20214;&#20013;&#30340;&#26631;&#35782;&#31526;&#25110;&#33258;&#28982;&#35821;&#35328;&#20043;&#38388;&#36890;&#24120;&#23384;&#22312;&#26126;&#26174;&#30340;&#35821;&#20041;&#24046;&#36317;&#12290;&#23545;&#20110;&#38754;&#21521;&#29992;&#25143;&#30340;&#36719;&#20214;&#65292;&#30446;&#21069;&#23384;&#22312;&#19968;&#31181;&#20851;&#38190;&#20449;&#24687;&#26469;&#28304;&#65292;&#21487;&#20197;&#24110;&#21161;&#38169;&#35823;&#23450;&#20301;&#65292;&#20294;&#23578;&#26410;&#24471;&#21040;&#28145;&#20837;&#30740;&#31350;&#65292;&#21363;&#26469;&#33258;GUI&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20197;&#19979;&#20551;&#35774;&#65306;&#23545;&#20110;&#38754;&#21521;&#26368;&#32456;&#29992;&#25143;&#30340;&#24212;&#29992;&#31243;&#24207;&#65292;&#23558;&#38169;&#35823;&#25253;&#21578;&#20013;&#30340;&#20449;&#24687;&#19982;GUI&#20013;&#30340;&#20449;&#24687;&#36830;&#25509;&#36215;&#26469;&#65292;&#24182;&#21033;&#29992;&#36825;&#20123;&#20449;&#24687;&#26469;&#24110;&#21161;&#26816;&#32034;&#21487;&#33021;&#26377;&#38169;&#35823;&#30340;&#25991;&#20214;&#65292;&#21487;&#20197;&#25913;&#36827;&#29616;&#26377;&#30340;&#38169;&#35823;&#23450;&#20301;&#25216;&#26415;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#19968;&#29616;&#35937;&#65292;&#25105;&#20204;...&#65288;&#25688;&#35201;&#26410;&#23436;&#65292;&#30465;&#30053;&#65289;
&lt;/p&gt;
&lt;p&gt;
One of the most important tasks related to managing bug reports is localizing the fault so that a fix can be applied. As such, prior work has aimed to automate this task of bug localization by formulating it as an information retrieval problem, where potentially buggy files are retrieved and ranked according to their textual similarity with a given bug report. However, there is often a notable semantic gap between the information contained in bug reports and identifiers or natural language contained within source code files. For user-facing software, there is currently a key source of information that could aid in bug localization, but has not been thoroughly investigated information from the GUI.  We investigate the hypothesis that, for end user-facing applications, connecting information in a bug report with information from the GUI, and using this to aid in retrieving potentially buggy files, can improve upon existing techniques for bug localization. To examine this phenomenon, we
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;Soft-InfoNCE&#25439;&#22833;&#20989;&#25968;&#65292;&#36890;&#36807;&#22312;InfoNCE&#20013;&#25554;&#20837;&#26435;&#37325;&#39033;&#26469;&#35299;&#20915;&#20195;&#30721;&#25628;&#32034;&#20013;&#36127;&#26679;&#26412;&#30340;&#38382;&#39064;&#65292;&#21253;&#25324;&#22823;&#22411;&#20195;&#30721;&#24211;&#20013;&#30340;&#34394;&#20551;&#36127;&#26679;&#26412;&#21644;&#26410;&#33021;&#21306;&#20998;&#36127;&#26679;&#26412;&#30340;&#28508;&#22312;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.08069</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#20195;&#30721;&#25628;&#32034;&#20013;&#30340;&#36127;&#26679;&#26412;&#23545;
&lt;/p&gt;
&lt;p&gt;
Rethinking Negative Pairs in Code Search. (arXiv:2310.08069v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08069
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;Soft-InfoNCE&#25439;&#22833;&#20989;&#25968;&#65292;&#36890;&#36807;&#22312;InfoNCE&#20013;&#25554;&#20837;&#26435;&#37325;&#39033;&#26469;&#35299;&#20915;&#20195;&#30721;&#25628;&#32034;&#20013;&#36127;&#26679;&#26412;&#30340;&#38382;&#39064;&#65292;&#21253;&#25324;&#22823;&#22411;&#20195;&#30721;&#24211;&#20013;&#30340;&#34394;&#20551;&#36127;&#26679;&#26412;&#21644;&#26410;&#33021;&#21306;&#20998;&#36127;&#26679;&#26412;&#30340;&#28508;&#22312;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23545;&#27604;&#23398;&#20064;&#25104;&#20026;&#32454;&#21270;&#20195;&#30721;&#25628;&#32034;&#27169;&#22411;&#20197;&#25552;&#39640;&#36719;&#20214;&#24320;&#21457;&#25928;&#29575;&#21644;&#25928;&#26524;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#23427;&#23558;&#27491;&#26679;&#26412;&#20195;&#30721;&#29255;&#27573;&#32858;&#38598;&#22312;&#19968;&#36215;&#65292;&#21516;&#26102;&#23558;&#19982;&#25628;&#32034;&#26597;&#35810;&#19981;&#30456;&#20851;&#30340;&#36127;&#26679;&#26412;&#25512;&#24320;&#12290;&#22312;&#23545;&#27604;&#23398;&#20064;&#20013;&#65292;InfoNCE&#26159;&#26368;&#24120;&#29992;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#22240;&#20026;&#23427;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;InfoNCE&#36127;&#26679;&#26412;&#23384;&#22312;&#20197;&#19979;&#38382;&#39064;&#21487;&#33021;&#20250;&#25439;&#23475;&#20854;&#34920;&#31034;&#23398;&#20064;&#30340;&#25928;&#26524;&#65306;1&#65289;&#30001;&#20110;&#37325;&#22797;&#65292;&#22823;&#22411;&#20195;&#30721;&#24211;&#20013;&#23384;&#22312;&#34394;&#20551;&#36127;&#26679;&#26412;&#12290;2&#65289;&#26410;&#33021;&#26126;&#30830;&#21306;&#20998;&#36127;&#26679;&#26412;&#30340;&#28508;&#22312;&#30456;&#20851;&#24615;&#12290;&#20363;&#22914;&#65292;&#23545;&#20110;&#24555;&#36895;&#25490;&#24207;&#31639;&#27861;&#26597;&#35810;&#65292;&#20882;&#27873;&#25490;&#24207;&#31639;&#27861;&#31034;&#20363;&#35201;&#27604;&#25991;&#20214;&#20445;&#23384;&#20989;&#25968;&#8220;&#26356;&#36127;&#38754;&#8221;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;Soft-InfoNCE&#25439;&#22833;&#26469;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#12290;&#22312;&#25105;&#20204;&#25552;&#20986;&#30340;&#25439;&#22833;&#20989;&#25968;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19977;&#31181;&#26041;&#27861;&#26469;&#20272;&#35745;&#26435;&#37325;...
&lt;/p&gt;
&lt;p&gt;
Recently, contrastive learning has become a key component in fine-tuning code search models for software development efficiency and effectiveness. It pulls together positive code snippets while pushing negative samples away given search queries. Among contrastive learning, InfoNCE is the most widely used loss function due to its better performance. However, the following problems in negative samples of InfoNCE may deteriorate its representation learning: 1) The existence of false negative samples in large code corpora due to duplications. 2). The failure to explicitly differentiate between the potential relevance of negative samples. As an example, a bubble sorting algorithm example is less ``negative'' than a file saving function for the quick sorting algorithm query. In this paper, we tackle the above problems by proposing a simple yet effective Soft-InfoNCE loss that inserts weight terms into InfoNCE. In our proposed loss function, we apply three methods to estimate the weights of n
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37325;&#26032;&#24605;&#32771;&#22823;&#35268;&#27169;&#39044;&#25490;&#24207;&#31995;&#32479;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#20307;&#38142;&#36335;&#36328;&#22495;&#27169;&#22411;&#21644;&#32454;&#31890;&#24230;&#31070;&#32463;&#32467;&#26500;&#26469;&#35299;&#20915;&#26679;&#26412;&#36873;&#25321;&#20559;&#24046;&#38382;&#39064;&#65292;&#24182;&#25913;&#36827;&#39044;&#25490;&#24207;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.08039</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#22823;&#35268;&#27169;&#39044;&#25490;&#24207;&#31995;&#32479;&#65306;&#25972;&#20307;&#38142;&#36335;&#36328;&#22495;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Rethinking Large-scale Pre-ranking System: Entire-chain Cross-domain Models. (arXiv:2310.08039v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08039
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37325;&#26032;&#24605;&#32771;&#22823;&#35268;&#27169;&#39044;&#25490;&#24207;&#31995;&#32479;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#20307;&#38142;&#36335;&#36328;&#22495;&#27169;&#22411;&#21644;&#32454;&#31890;&#24230;&#31070;&#32463;&#32467;&#26500;&#26469;&#35299;&#20915;&#26679;&#26412;&#36873;&#25321;&#20559;&#24046;&#38382;&#39064;&#65292;&#24182;&#25913;&#36827;&#39044;&#25490;&#24207;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24037;&#19994;&#31995;&#32479;&#65292;&#22914;&#25512;&#33616;&#31995;&#32479;&#21644;&#22312;&#32447;&#24191;&#21578;&#65292;&#24050;&#24191;&#27867;&#37197;&#22791;&#20102;&#22810;&#38454;&#27573;&#26550;&#26500;&#65292;&#21253;&#25324;&#21305;&#37197;&#12289;&#39044;&#25490;&#24207;&#12289;&#25490;&#24207;&#21644;&#20877;&#25490;&#24207;&#12290;&#20316;&#20026;&#21305;&#37197;&#21644;&#25490;&#24207;&#20043;&#38388;&#30340;&#20851;&#38190;&#26725;&#26753;&#65292;&#29616;&#26377;&#30340;&#39044;&#25490;&#24207;&#26041;&#27861;&#20027;&#35201;&#24573;&#35270;&#20102;&#25972;&#20010;&#38142;&#36335;&#25968;&#25454;&#30340;&#20381;&#36182;&#24615;&#65292;&#23548;&#33268;&#23376;&#20248;&#21270;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#25972;&#20307;&#26679;&#26412;&#31354;&#38388;&#30340;&#35282;&#24230;&#37325;&#26032;&#24605;&#32771;&#39044;&#25490;&#24207;&#31995;&#32479;&#65292;&#24182;&#25552;&#20986;&#20102;&#25972;&#20307;&#38142;&#36335;&#36328;&#22495;&#27169;&#22411;&#65288;ECM&#65289;&#65292;&#21033;&#29992;&#25972;&#20010;&#32423;&#32852;&#38454;&#27573;&#30340;&#26679;&#26412;&#26469;&#26377;&#25928;&#20943;&#36731;&#26679;&#26412;&#36873;&#25321;&#20559;&#24046;&#65288;SSB&#65289;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#32454;&#31890;&#24230;&#31070;&#32463;&#32467;&#26500;&#65292;&#21517;&#20026;ECMM&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#39044;&#25490;&#24207;&#30340;&#20934;&#30830;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36328;&#22495;&#22810;&#22612;&#31070;&#32463;&#32593;&#32476;&#26469;&#32508;&#21512;&#39044;&#27979;&#27599;&#20010;&#38454;&#27573;&#30340;&#32467;&#26524;&#65292;&#24182;&#24341;&#20837;$L0$&#27491;&#21017;&#21270;&#30340;&#23376;&#32593;&#32476;&#36335;&#30001;&#31574;&#30053;&#26469;&#35843;&#25972;&#27599;&#20010;&#38454;&#27573;&#30340;&#36129;&#29486;&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;
Industrial systems such as recommender systems and online advertising, have been widely equipped with multi-stage architectures, which are divided into several cascaded modules, including matching, pre-ranking, ranking and re-ranking. As a critical bridge between matching and ranking, existing pre-ranking approaches mainly endure sample selection bias (SSB) problem owing to ignoring the entire-chain data dependence, resulting in sub-optimal performances. In this paper, we rethink pre-ranking system from the perspective of the entire sample space, and propose Entire-chain Cross-domain Models (ECM), which leverage samples from the whole cascaded stages to effectively alleviate SSB problem. Besides, we design a fine-grained neural structure named ECMM to further improve the pre-ranking accuracy. Specifically, we propose a cross-domain multi-tower neural network to comprehensively predict for each stage result, and introduce the sub-networking routing strategy with $L0$ regularization to r
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Manifold Expansion Replay&#65288;MaER&#65289;&#30340;&#26032;&#22411;&#22238;&#25918;&#31574;&#30053;&#65292;&#36890;&#36807;&#25193;&#23637;&#30693;&#35782;&#34920;&#31034;&#20013;&#30340;&#38544;&#24335;&#27969;&#24418;&#26469;&#20943;&#23569;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#37319;&#29992;&#36138;&#24515;&#31574;&#30053;&#22686;&#21152;&#32531;&#20914;&#21306;&#20013;&#30693;&#35782;&#34920;&#31034;&#30340;&#27969;&#24418;&#30452;&#24452;&#65292;&#24182;&#20351;&#29992;Wasserstein&#36317;&#31163;&#20316;&#20026;&#36317;&#31163;&#24230;&#37327;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#34920;&#36798;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.08038</link><description>&lt;p&gt;
&#22522;&#20110;&#27969;&#24418;&#25193;&#23637;&#22238;&#25918;&#30340;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Continual Learning via Manifold Expansion Replay. (arXiv:2310.08038v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08038
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Manifold Expansion Replay&#65288;MaER&#65289;&#30340;&#26032;&#22411;&#22238;&#25918;&#31574;&#30053;&#65292;&#36890;&#36807;&#25193;&#23637;&#30693;&#35782;&#34920;&#31034;&#20013;&#30340;&#38544;&#24335;&#27969;&#24418;&#26469;&#20943;&#23569;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#37319;&#29992;&#36138;&#24515;&#31574;&#30053;&#22686;&#21152;&#32531;&#20914;&#21306;&#20013;&#30693;&#35782;&#34920;&#31034;&#30340;&#27969;&#24418;&#30452;&#24452;&#65292;&#24182;&#20351;&#29992;Wasserstein&#36317;&#31163;&#20316;&#20026;&#36317;&#31163;&#24230;&#37327;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#34920;&#36798;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25345;&#32493;&#23398;&#20064;&#20013;&#65292;&#23398;&#20064;&#32773;&#25353;&#39034;&#24207;&#23398;&#20064;&#22810;&#20010;&#20219;&#21153;&#65292;&#27599;&#20010;&#20219;&#21153;&#21482;&#33719;&#21462;&#19968;&#27425;&#25968;&#25454;&#12290;&#28798;&#38590;&#24615;&#36951;&#24536;&#26159;&#25345;&#32493;&#23398;&#20064;&#30340;&#20027;&#35201;&#25361;&#25112;&#12290;&#20026;&#20102;&#20943;&#23569;&#36951;&#24536;&#65292;&#19968;&#20123;&#29616;&#26377;&#30340;&#22522;&#20110;&#22238;&#25918;&#30340;&#26041;&#27861;&#20351;&#29992;&#24773;&#22659;&#35760;&#24518;&#26469;&#37325;&#26032;&#25773;&#25918;&#20808;&#21069;&#20219;&#21153;&#30340;&#26679;&#26412;&#12290;&#28982;&#32780;&#65292;&#22312;&#23398;&#20064;&#26032;&#20219;&#21153;&#26102;&#36827;&#34892;&#30693;&#35782;&#25972;&#21512;&#30340;&#36807;&#31243;&#20013;&#65292;&#30001;&#20110;&#26087;&#30693;&#35782;&#21644;&#26032;&#30693;&#35782;&#20043;&#38388;&#30340;&#19981;&#24179;&#34913;&#65292;&#36825;&#31181;&#31574;&#30053;&#20063;&#20250;&#36973;&#21463;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Manifold Expansion Replay&#65288;MaER&#65289;&#30340;&#26032;&#22411;&#22238;&#25918;&#31574;&#30053;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#22312;&#24773;&#22659;&#35760;&#24518;&#20013;&#25193;&#23637;&#30693;&#35782;&#34920;&#31034;&#30340;&#38544;&#24335;&#27969;&#24418;&#26377;&#21161;&#20110;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#34920;&#36798;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36138;&#24515;&#31574;&#30053;&#65292;&#22312;&#20869;&#23384;&#31649;&#29702;&#36807;&#31243;&#20013;&#65292;&#19981;&#26029;&#22686;&#21152;&#30001;&#32531;&#20914;&#21306;&#20013;&#30340;&#30693;&#35782;&#34920;&#31034;&#30340;&#38544;&#24335;&#27969;&#24418;&#30340;&#30452;&#24452;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;Wasserstein&#36317;&#31163;&#24341;&#20837;&#26367;&#20195;&#20132;&#21449;&#29109;&#20316;&#20026;&#36317;&#31163;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
In continual learning, the learner learns multiple tasks in sequence, with data being acquired only once for each task. Catastrophic forgetting is a major challenge to continual learning. To reduce forgetting, some existing rehearsal-based methods use episodic memory to replay samples of previous tasks. However, in the process of knowledge integration when learning a new task, this strategy also suffers from catastrophic forgetting due to an imbalance between old and new knowledge. To address this problem, we propose a novel replay strategy called Manifold Expansion Replay (MaER). We argue that expanding the implicit manifold of the knowledge representation in the episodic memory helps to improve the robustness and expressiveness of the model. To this end, we propose a greedy strategy to keep increasing the diameter of the implicit manifold represented by the knowledge in the buffer during memory management. In addition, we introduce Wasserstein distance instead of cross entropy as dis
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22810;&#35270;&#22270;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#26469;&#22635;&#20805;&#38750;&#30446;&#26631;&#20195;&#35874;&#32452;&#23398;&#20013;&#30340;&#32570;&#22833;&#20540;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#20840;&#22522;&#22240;&#32452;&#27979;&#24207;&#25968;&#25454;&#21644;&#21442;&#32771;&#20195;&#35874;&#29289;&#30340;&#20449;&#24687;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#26681;&#25454;&#22522;&#22240;&#32452;&#20449;&#24687;&#22635;&#20805;&#32570;&#22833;&#30340;&#20195;&#35874;&#32452;&#23398;&#20540;&#12290;</title><link>http://arxiv.org/abs/2310.07990</link><description>&lt;p&gt;
&#22810;&#35270;&#22270;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#22312;&#38750;&#30446;&#26631;&#20195;&#35874;&#32452;&#23398;&#20013;&#32570;&#22833;&#20540;&#22635;&#20805;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Multi-View Variational Autoencoder for Missing Value Imputation in Untargeted Metabolomics. (arXiv:2310.07990v1 [q-bio.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07990
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22810;&#35270;&#22270;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#26469;&#22635;&#20805;&#38750;&#30446;&#26631;&#20195;&#35874;&#32452;&#23398;&#20013;&#30340;&#32570;&#22833;&#20540;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#20840;&#22522;&#22240;&#32452;&#27979;&#24207;&#25968;&#25454;&#21644;&#21442;&#32771;&#20195;&#35874;&#29289;&#30340;&#20449;&#24687;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#26681;&#25454;&#22522;&#22240;&#32452;&#20449;&#24687;&#22635;&#20805;&#32570;&#22833;&#30340;&#20195;&#35874;&#32452;&#23398;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#65306;&#22312;&#22522;&#20110;&#36136;&#35889;&#30340;&#20195;&#35874;&#32452;&#23398;&#20013;&#65292;&#32570;&#22833;&#25968;&#25454;&#26159;&#19968;&#20010;&#24120;&#35265;&#30340;&#25361;&#25112;&#65292;&#21487;&#33021;&#23548;&#33268;&#20559;&#20506;&#21644;&#19981;&#23436;&#25972;&#30340;&#20998;&#26512;&#12290;&#23558;&#20840;&#22522;&#22240;&#32452;&#27979;&#24207;&#65288;WGS&#65289;&#25968;&#25454;&#19982;&#20195;&#35874;&#32452;&#23398;&#25968;&#25454;&#25972;&#21512;&#36215;&#26469;&#65292;&#24050;&#32463;&#25104;&#20026;&#22686;&#24378;&#20195;&#35874;&#32452;&#23398;&#30740;&#31350;&#20013;&#25968;&#25454;&#22635;&#20805;&#20934;&#30830;&#24615;&#30340;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#12290;&#26041;&#27861;&#65306;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#26469;&#33258;WGS&#25968;&#25454;&#21644;&#21442;&#32771;&#20195;&#35874;&#29289;&#30340;&#20449;&#24687;&#26469;&#22635;&#20805;&#26410;&#30693;&#20195;&#35874;&#29289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#22810;&#35270;&#22270;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#20849;&#21516;&#23545;&#36127;&#25285;&#35780;&#20998;&#12289;&#22810;&#22522;&#22240;&#39118;&#38505;&#35780;&#20998;&#65288;PGS&#65289;&#21644;&#36830;&#38145;&#19981;&#24179;&#34913;&#65288;LD&#65289;&#21024;&#20943;&#30340;&#21333;&#26680;&#33527;&#37240;&#22810;&#24577;&#24615;&#65288;SNPs&#65289;&#36827;&#34892;&#29305;&#24449;&#25552;&#21462;&#21644;&#32570;&#22833;&#20195;&#35874;&#32452;&#23398;&#25968;&#25454;&#30340;&#22635;&#20805;&#12290;&#36890;&#36807;&#23398;&#20064;&#20004;&#31181;&#32452;&#23398;&#25968;&#25454;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#26681;&#25454;&#22522;&#22240;&#32452;&#20449;&#24687;&#26377;&#25928;&#22320;&#22635;&#20805;&#32570;&#22833;&#30340;&#20195;&#35874;&#32452;&#23398;&#20540;&#12290;&#32467;&#26524;&#65306;&#25105;&#20204;&#22312;&#20855;&#26377;&#32570;&#22833;&#20540;&#21644;&#19981;&#23436;&#25972;&#25968;&#25454;&#30340;&#23454;&#39564;&#20195;&#35874;&#32452;&#23398;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Background: Missing data is a common challenge in mass spectrometry-based metabolomics, which can lead to biased and incomplete analyses. The integration of whole-genome sequencing (WGS) data with metabolomics data has emerged as a promising approach to enhance the accuracy of data imputation in metabolomics studies. Method: In this study, we propose a novel method that leverages the information from WGS data and reference metabolites to impute unknown metabolites. Our approach utilizes a multi-view variational autoencoder to jointly model the burden score, polygenetic risk score (PGS), and linkage disequilibrium (LD) pruned single nucleotide polymorphisms (SNPs) for feature extraction and missing metabolomics data imputation. By learning the latent representations of both omics data, our method can effectively impute missing metabolomics values based on genomic information. Results: We evaluate the performance of our method on empirical metabolomics datasets with missing values and de
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20027;&#21160;&#23398;&#20064;&#21644;&#26426;&#21046;&#35774;&#35745;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#22312;&#25293;&#21334;&#20013;&#22788;&#29702;&#22823;&#37327;&#21830;&#21697;&#21644;&#31574;&#30053;&#25307;&#26631;&#20154;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;&#20027;&#39064;&#27169;&#22411;&#26469;&#36817;&#20284;&#25237;&#26631;&#20154;&#30340;&#20808;&#39564;&#20998;&#24067;&#65292;&#24182;&#35774;&#35745;&#20986;&#30456;&#24212;&#30340;&#26426;&#21046;&#65292;&#20197;&#25552;&#39640;&#26426;&#21046;&#30340;&#31283;&#23450;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.07874</link><description>&lt;p&gt;
&#36890;&#36807;&#20027;&#21160;&#22238;&#24402;&#23545;&#36817;&#20284;&#32467;&#26500;&#30340;&#20808;&#39564;&#36827;&#34892;&#31934;&#32454;&#26426;&#21046;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Refined Mechanism Design for Approximately Structured Priors via Active Regression. (arXiv:2310.07874v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07874
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20027;&#21160;&#23398;&#20064;&#21644;&#26426;&#21046;&#35774;&#35745;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#22312;&#25293;&#21334;&#20013;&#22788;&#29702;&#22823;&#37327;&#21830;&#21697;&#21644;&#31574;&#30053;&#25307;&#26631;&#20154;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;&#20027;&#39064;&#27169;&#22411;&#26469;&#36817;&#20284;&#25237;&#26631;&#20154;&#30340;&#20808;&#39564;&#20998;&#24067;&#65292;&#24182;&#35774;&#35745;&#20986;&#30456;&#24212;&#30340;&#26426;&#21046;&#65292;&#20197;&#25552;&#39640;&#26426;&#21046;&#30340;&#31283;&#23450;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#26377;&#22823;&#37327;&#21830;&#21697;m&#20986;&#21806;&#32473;n&#20010;&#31574;&#30053;&#25307;&#26631;&#20154;&#30340;&#26368;&#22823;&#21270;&#25910;&#20837;&#21334;&#26041;&#30340;&#38382;&#39064;&#65292;&#20182;&#20204;&#30340;&#20272;&#20540;&#26159;&#20174;&#39640;&#32500;&#26410;&#30693;&#30340;&#20808;&#39564;&#20998;&#24067;&#20013;&#29420;&#31435;&#25277;&#21462;&#30340;&#12290;&#20247;&#25152;&#21608;&#30693;&#65292;&#36825;&#31181;&#24773;&#20917;&#19979;&#30340;&#26368;&#20248;&#29978;&#33267;&#36817;&#20284;&#26368;&#20248;&#30340;&#26426;&#21046;&#24456;&#38590;&#34920;&#36798;&#25110;&#35745;&#31639;&#65292;&#32780;&#19988;&#21363;&#20351;&#25214;&#21040;&#20102;&#65292;&#36890;&#24120;&#20063;&#20855;&#26377;&#21508;&#31181;&#21453;&#30452;&#35273;&#30340;&#29305;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#26681;&#25454;Cai&#21644;Daskalakis&#26368;&#36817;&#25552;&#20986;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#32771;&#34385;&#25237;&#26631;&#20154;&#30340;&#20808;&#39564;&#20998;&#24067;&#21487;&#20197;&#34987;&#19968;&#20010;&#20027;&#39064;&#27169;&#22411;&#24456;&#22909;&#22320;&#36817;&#20284;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#36127;&#36131;&#19982;&#25237;&#26631;&#20154;&#36827;&#34892;&#20132;&#20114;&#24182;&#36755;&#20986;&#20854;&#31867;&#22411;&#30340;&#20302;&#32500;&#36817;&#20284;&#30340;&#20027;&#21160;&#23398;&#20064;&#32452;&#20214;&#65292;&#20197;&#21450;&#19968;&#20010;&#36127;&#36131;&#20026;&#20302;&#32500;&#27169;&#22411;&#35774;&#35745;&#26426;&#21046;&#20197;&#36866;&#24212;&#21069;&#19968;&#32452;&#20214;&#30340;&#36817;&#20284;&#31867;&#22411;&#30340;&#26426;&#21046;&#35774;&#35745;&#32452;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of a revenue-maximizing seller with a large number of items $m$ for sale to $n$ strategic bidders, whose valuations are drawn independently from high-dimensional, unknown prior distributions. It is well-known that optimal and even approximately-optimal mechanisms for this setting are notoriously difficult to characterize or compute, and, even when they can be found, are often rife with various counter-intuitive properties. In this paper, following a model introduced recently by Cai and Daskalakis~\cite{cai2022recommender}, we consider the case that bidders' prior distributions can be well-approximated by a topic model. We design an active learning component, responsible for interacting with the bidders and outputting low-dimensional approximations of their types, and a mechanism design component, responsible for robustifying mechanisms for the low-dimensional model to work for the approximate types of the former component. On the active learning front, we cast o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#29983;&#25104;&#24615;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#35821;&#20041;ID&#30340;&#33258;&#30417;&#30563;&#26694;&#26550;LMINDEXER&#12290;</title><link>http://arxiv.org/abs/2310.07815</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35821;&#20041;&#32034;&#24341;&#22120;
&lt;/p&gt;
&lt;p&gt;
Language Models As Semantic Indexers. (arXiv:2310.07815v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07815
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#29983;&#25104;&#24615;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#35821;&#20041;ID&#30340;&#33258;&#30417;&#30563;&#26694;&#26550;LMINDEXER&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#26631;&#35782;&#31526;&#65288;ID&#65289;&#26159;&#20449;&#24687;&#26816;&#32034;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#27010;&#24565;&#65292;&#26088;&#22312;&#20445;&#30041;&#23545;&#35937;&#65288;&#22914;&#25991;&#26723;&#21644;&#39033;&#65289;&#20869;&#37096;&#30340;&#35821;&#20041;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#36890;&#24120;&#37319;&#29992;&#20004;&#38454;&#27573;&#27969;&#31243;&#26469;&#23398;&#20064;&#35821;&#20041;ID&#65292;&#39318;&#20808;&#20351;&#29992;&#29616;&#25104;&#30340;&#25991;&#26412;&#32534;&#30721;&#22120;&#33719;&#21462;&#23884;&#20837;&#65292;&#24182;&#26681;&#25454;&#23884;&#20837;&#26469;&#25512;&#23548;ID&#12290;&#28982;&#32780;&#65292;&#27599;&#20010;&#27493;&#39588;&#37117;&#20250;&#24341;&#20837;&#28508;&#22312;&#30340;&#20449;&#24687;&#25439;&#22833;&#65292;&#24182;&#19988;&#25991;&#26412;&#32534;&#30721;&#22120;&#29983;&#25104;&#30340;&#28508;&#22312;&#31354;&#38388;&#20869;&#30340;&#23884;&#20837;&#20998;&#24067;&#36890;&#24120;&#19982;&#35821;&#20041;&#32034;&#24341;&#25152;&#38656;&#30340;&#39044;&#26399;&#20998;&#24067;&#23384;&#22312;&#22266;&#26377;&#30340;&#19981;&#21305;&#37197;&#12290;&#28982;&#32780;&#65292;&#35774;&#35745;&#19968;&#20010;&#26082;&#33021;&#23398;&#20064;&#25991;&#26723;&#30340;&#35821;&#20041;&#34920;&#31034;&#21448;&#33021;&#21516;&#26102;&#23398;&#20064;&#20854;&#20998;&#23618;&#32467;&#26500;&#30340;&#26041;&#27861;&#24182;&#19981;&#23481;&#26131;&#65292;&#22240;&#20026;&#35821;&#20041;ID&#26159;&#31163;&#25955;&#21644;&#39034;&#24207;&#32467;&#26500;&#30340;&#65292;&#24182;&#19988;&#35821;&#20041;&#30417;&#30563;&#26159;&#19981;&#20805;&#20998;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;LMINDEXER&#65292;&#23427;&#26159;&#19968;&#20010;&#33258;&#30417;&#30563;&#26694;&#26550;&#65292;&#29992;&#20110;&#20351;&#29992;&#29983;&#25104;&#24615;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#35821;&#20041;ID&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic identifier (ID) is an important concept in information retrieval that aims to preserve the semantics of objects such as documents and items inside their IDs. Previous studies typically adopt a two-stage pipeline to learn semantic IDs by first procuring embeddings using off-the-shelf text encoders and then deriving IDs based on the embeddings. However, each step introduces potential information loss and there is usually an inherent mismatch between the distribution of embeddings within the latent space produced by text encoders and the anticipated distribution required for semantic indexing. Nevertheless, it is non-trivial to design a method that can learn the document's semantic representations and its hierarchical structure simultaneously, given that semantic IDs are discrete and sequentially structured, and the semantic supervision is deficient. In this paper, we introduce LMINDEXER, a self-supervised framework to learn semantic IDs with a generative language model. We tackl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38750;&#31283;&#24577;&#24773;&#22659;&#36172;&#21338;&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;&#21487;&#25193;&#23637;&#30340;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26550;&#26500;&#19982;&#31934;&#24515;&#35774;&#35745;&#30340;&#25506;&#32034;&#26426;&#21046;&#30456;&#32467;&#21512;&#65292;&#22312;&#38750;&#31283;&#24577;&#29615;&#22659;&#20013;&#20248;&#20808;&#25910;&#38598;&#25345;&#20037;&#20215;&#20540;&#20449;&#24687;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.07786</link><description>&lt;p&gt;
&#38750;&#31283;&#24577;&#29615;&#22659;&#19979;&#22522;&#20110;&#31070;&#32463;&#39044;&#27979;&#38598;&#25104;&#25277;&#26679;&#30340;&#24773;&#22659;&#36172;&#21338;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Non-Stationary Contextual Bandit Learning via Neural Predictive Ensemble Sampling. (arXiv:2310.07786v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07786
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38750;&#31283;&#24577;&#24773;&#22659;&#36172;&#21338;&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;&#21487;&#25193;&#23637;&#30340;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26550;&#26500;&#19982;&#31934;&#24515;&#35774;&#35745;&#30340;&#25506;&#32034;&#26426;&#21046;&#30456;&#32467;&#21512;&#65292;&#22312;&#38750;&#31283;&#24577;&#29615;&#22659;&#20013;&#20248;&#20808;&#25910;&#38598;&#25345;&#20037;&#20215;&#20540;&#20449;&#24687;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#38469;&#19990;&#30028;&#20013;&#30340;&#24773;&#22659;&#36172;&#21338;&#24212;&#29992;&#24120;&#24120;&#22240;&#23395;&#33410;&#24615;&#12289;&#20598;&#28982;&#24615;&#21644;&#19981;&#26029;&#21464;&#21270;&#30340;&#31038;&#20132;&#36235;&#21183;&#32780;&#21576;&#38750;&#31283;&#24577;&#12290;&#23613;&#31649;&#25991;&#29486;&#20013;&#24050;&#25552;&#20986;&#20102;&#35768;&#22810;&#38750;&#31283;&#24577;&#24773;&#22659;&#36172;&#21338;&#23398;&#20064;&#31639;&#27861;&#65292;&#20294;&#30001;&#20110;&#32570;&#20047;&#23545;&#25345;&#20037;&#20215;&#20540;&#20449;&#24687;&#30340;&#20248;&#20808;&#32771;&#34385;&#65292;&#36825;&#20123;&#31639;&#27861;&#22312;&#25506;&#32034;&#26102;&#36807;&#24230;&#65292;&#25110;&#32773;&#35774;&#35745;&#26041;&#24335;&#38590;&#20197;&#22312;&#20855;&#26377;&#39640;&#32500;&#29992;&#25143;&#29305;&#23450;&#29305;&#24449;&#21644;&#22823;&#35268;&#27169;&#21160;&#20316;&#38598;&#30340;&#29616;&#20195;&#24212;&#29992;&#20013;&#25193;&#23637;&#65292;&#25110;&#32773;&#20004;&#32773;&#37117;&#26377;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38750;&#31283;&#24577;&#24773;&#22659;&#36172;&#21338;&#31639;&#27861;&#65292;&#23427;&#35299;&#20915;&#20102;&#36825;&#20123;&#38382;&#39064;&#12290;&#23427;&#23558;&#21487;&#25193;&#23637;&#30340;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26550;&#26500;&#19982;&#19968;&#20010;&#31934;&#24515;&#35774;&#35745;&#30340;&#25506;&#32034;&#26426;&#21046;&#30456;&#32467;&#21512;&#65292;&#22312;&#38750;&#31283;&#24577;&#29615;&#22659;&#20013;&#25112;&#30053;&#24615;&#22320;&#20248;&#20808;&#25910;&#38598;&#20855;&#26377;&#26368;&#25345;&#20037;&#20215;&#20540;&#30340;&#20449;&#24687;&#12290;&#36890;&#36807;&#22312;&#23637;&#31034;&#26126;&#26174;&#38750;&#31283;&#24577;&#30340;&#20004;&#20010;&#23454;&#38469;&#25512;&#33616;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#35777;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#32988;&#36807;&#29616;&#26377;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-world applications of contextual bandits often exhibit non-stationarity due to seasonality, serendipity, and evolving social trends. While a number of non-stationary contextual bandit learning algorithms have been proposed in the literature, they excessively explore due to a lack of prioritization for information of enduring value, or are designed in ways that do not scale in modern applications with high-dimensional user-specific features and large action set, or both. In this paper, we introduce a novel non-stationary contextual bandit algorithm that addresses these concerns. It combines a scalable, deep-neural-network-based architecture with a carefully designed exploration mechanism that strategically prioritizes collecting information with the most lasting value in a non-stationary environment. Through empirical evaluations on two real-world recommendation datasets, which exhibit pronounced non-stationarity, we demonstrate that our approach significantly outperforms the state
&lt;/p&gt;</description></item></channel></rss>