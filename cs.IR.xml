<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;PT-Pump-Up&#65292;&#19968;&#22871;&#26088;&#22312;&#25552;&#39640;&#33889;&#33796;&#29273;&#35821;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#36164;&#28304;&#21487;&#35775;&#38382;&#24615;&#30340;&#24037;&#20855;&#65292;&#24182;&#21253;&#25324;&#20102;&#19968;&#20010;Web&#24179;&#21488;&#12289;&#19968;&#20010;&#23458;&#25143;&#31471;Python&#36719;&#20214;&#21253;&#12289;&#19968;&#20010;&#31649;&#29702;&#24179;&#21488;&#30340;&#31649;&#29702;Python&#36719;&#20214;&#21253;&#21644;&#19968;&#20010;&#20844;&#20849;GitHub&#23384;&#20648;&#24211;&#12290;</title><link>http://arxiv.org/abs/2401.15400</link><description>&lt;p&gt;
&#20351;&#29992;PT-Pump-Up&#32034;&#24341;&#33889;&#33796;&#29273;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#36164;&#28304;
&lt;/p&gt;
&lt;p&gt;
Indexing Portuguese NLP Resources with PT-Pump-Up. (arXiv:2401.15400v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15400
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;PT-Pump-Up&#65292;&#19968;&#22871;&#26088;&#22312;&#25552;&#39640;&#33889;&#33796;&#29273;&#35821;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#36164;&#28304;&#21487;&#35775;&#38382;&#24615;&#30340;&#24037;&#20855;&#65292;&#24182;&#21253;&#25324;&#20102;&#19968;&#20010;Web&#24179;&#21488;&#12289;&#19968;&#20010;&#23458;&#25143;&#31471;Python&#36719;&#20214;&#21253;&#12289;&#19968;&#20010;&#31649;&#29702;&#24179;&#21488;&#30340;&#31649;&#29702;Python&#36719;&#20214;&#21253;&#21644;&#19968;&#20010;&#20844;&#20849;GitHub&#23384;&#20648;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#30340;&#26368;&#26032;&#36827;&#23637;&#19982;&#38656;&#35201;&#22823;&#37327;&#35821;&#26009;&#24211;&#30340;&#35757;&#32451;&#36807;&#31243;&#30456;&#20851;&#12290;&#30001;&#20110;&#36164;&#28304;&#20998;&#25955;&#21644;&#38656;&#35201;&#32500;&#25252;&#36825;&#20123;&#22522;&#30784;&#35774;&#26045;&#30340;&#22312;&#32447;&#21644;&#26356;&#26032;&#65292;&#35775;&#38382;&#36825;&#20123;&#25968;&#25454;&#36890;&#24120;&#19981;&#26159;&#19968;&#20010;&#31616;&#21333;&#30340;&#36807;&#31243;&#12290;&#30001;&#20110;&#32570;&#20047;&#25968;&#25454;&#21644;&#36866;&#24403;&#30340;&#36164;&#28304;&#31649;&#29702;&#22522;&#30784;&#35774;&#26045;&#65292;&#20302;&#36164;&#28304;&#21644;&#20013;&#36164;&#28304;&#35821;&#35328;&#65288;&#22914;&#33889;&#33796;&#29273;&#35821;&#65289;&#22312;NLP&#26041;&#38754;&#30340;&#26032;&#21457;&#23637;&#24448;&#24448;&#21463;&#21040;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PT-Pump-Up&#65292;&#19968;&#22871;&#26088;&#22312;&#20943;&#23569;&#36164;&#28304;&#20998;&#25955;&#24182;&#25552;&#39640;&#33889;&#33796;&#29273;&#35821;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#36164;&#28304;&#21487;&#35775;&#38382;&#24615;&#30340;&#24037;&#20855;&#12290;&#25105;&#20204;&#30340;&#25552;&#35758;&#20998;&#20026;&#22235;&#20010;&#36719;&#20214;&#32452;&#20214;&#65306;a&#65289;&#19968;&#20010;&#21015;&#20986;&#21487;&#29992;&#36164;&#28304;&#30340;Web&#24179;&#21488;&#65307;b&#65289;&#19968;&#20010;&#23458;&#25143;&#31471;Python&#36719;&#20214;&#21253;&#65292;&#31616;&#21270;&#33889;&#33796;&#29273;&#35821;NLP&#36164;&#28304;&#30340;&#21152;&#36733;&#65307;c&#65289;&#19968;&#20010;&#31649;&#29702;&#24179;&#21488;&#30340;&#31649;&#29702;Python&#36719;&#20214;&#21253;&#65307;d&#65289;&#19968;&#20010;&#20844;&#20849;GitHub&#23384;&#20648;&#24211;
&lt;/p&gt;
&lt;p&gt;
The recent advances in natural language processing (NLP) are linked to training processes that require vast amounts of corpora. Access to this data is commonly not a trivial process due to resource dispersion and the need to maintain these infrastructures online and up-to-date. New developments in NLP are often compromised due to the scarcity of data or lack of a shared repository that works as an entry point to the community. This is especially true in low and mid-resource languages, such as Portuguese, which lack data and proper resource management infrastructures. In this work, we propose PT-Pump-Up, a set of tools that aim to reduce resource dispersion and improve the accessibility to Portuguese NLP resources. Our proposal is divided into four software components: a) a web platform to list the available resources; b) a client-side Python package to simplify the loading of Portuguese NLP resources; c) an administrative Python package to manage the platform and d) a public GitHub rep
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#31169;&#20445;&#25252;&#30340;&#36328;&#39046;&#22495;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#65288;PriCDSR&#65289;&#65292;&#21487;&#20197;&#22312;&#25552;&#20379;&#25512;&#33616;&#26381;&#21153;&#30340;&#21516;&#26102;&#20445;&#25252;&#29992;&#25143;&#30340;&#38544;&#31169;&#12290;</title><link>http://arxiv.org/abs/2401.15369</link><description>&lt;p&gt;
&#38544;&#31169;&#20445;&#25252;&#30340;&#36328;&#39046;&#22495;&#39034;&#24207;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Privacy-Preserving Cross-Domain Sequential Recommendation. (arXiv:2401.15369v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15369
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#31169;&#20445;&#25252;&#30340;&#36328;&#39046;&#22495;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#65288;PriCDSR&#65289;&#65292;&#21487;&#20197;&#22312;&#25552;&#20379;&#25512;&#33616;&#26381;&#21153;&#30340;&#21516;&#26102;&#20445;&#25252;&#29992;&#25143;&#30340;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#39046;&#22495;&#39034;&#24207;&#25512;&#33616;&#26159;&#25512;&#33616;&#31995;&#32479;&#30340;&#37325;&#35201;&#21457;&#23637;&#26041;&#21521;&#12290;&#23427;&#32467;&#21512;&#20102;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#21644;&#36328;&#39046;&#22495;&#25512;&#33616;&#31995;&#32479;&#30340;&#29305;&#28857;&#65292;&#21487;&#20197;&#25429;&#25417;&#29992;&#25143;&#30340;&#21160;&#24577;&#20559;&#22909;&#24182;&#32531;&#35299;&#20919;&#21551;&#21160;&#29992;&#25143;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36817;&#24180;&#26469;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#20851;&#27880;&#33258;&#24049;&#30340;&#38544;&#31169;&#12290;&#20182;&#20204;&#19981;&#24076;&#26395;&#21035;&#20154;&#30693;&#36947;&#20182;&#20204;&#21018;&#21018;&#36141;&#20080;&#20102;&#20160;&#20040;&#12289;&#35266;&#30475;&#20102;&#21738;&#20123;&#35270;&#39057;&#20197;&#21450;&#20182;&#20204;&#26469;&#33258;&#21738;&#37324;&#12290;&#22914;&#20309;&#20445;&#25252;&#29992;&#25143;&#30340;&#38544;&#31169;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#36843;&#20999;&#38656;&#35201;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38544;&#31169;&#20445;&#25252;&#30340;&#36328;&#39046;&#22495;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#65288;PriCDSR&#65289;&#65292;&#23427;&#21487;&#20197;&#21516;&#26102;&#20026;&#29992;&#25143;&#25552;&#20379;&#25512;&#33616;&#26381;&#21153;&#24182;&#20445;&#25252;&#20182;&#20204;&#30340;&#38544;&#31169;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22312;&#25968;&#25454;&#19978;&#23450;&#20041;&#20102;&#19968;&#31181;&#26032;&#30340;&#24046;&#20998;&#38544;&#31169;&#65292;&#32771;&#34385;&#20102;ID&#20449;&#24687;&#21644;&#39034;&#24207;&#20449;&#24687;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#28385;&#36275;&#36825;&#31181;&#24046;&#20998;&#38544;&#31169;&#30340;&#38543;&#26426;&#26426;&#21046;...
&lt;/p&gt;
&lt;p&gt;
Cross-domain sequential recommendation is an important development direction of recommender systems. It combines the characteristics of sequential recommender systems and cross-domain recommender systems, which can capture the dynamic preferences of users and alleviate the problem of cold-start users. However, in recent years, people pay more and more attention to their privacy. They do not want other people to know what they just bought, what videos they just watched, and where they just came from. How to protect the users' privacy has become an urgent problem to be solved. In this paper, we propose a novel privacy-preserving cross-domain sequential recommender system (PriCDSR), which can provide users with recommendation services while preserving their privacy at the same time. Specifically, we define a new differential privacy on the data, taking into account both the ID information and the order information. Then, we design a random mechanism that satisfies this differential privac
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#35843;&#30740;&#20102;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#30340;&#26041;&#27861;&#12289;&#24212;&#29992;&#21644;&#25361;&#25112;&#65292;&#23545;&#20110;&#30701;&#25991;&#26412;&#21644;&#36328;&#35821;&#35328;&#25991;&#26723;&#31561;&#21508;&#31181;&#22330;&#26223;&#25552;&#20379;&#20102;&#31995;&#32479;&#24615;&#30340;&#32452;&#32455;&#21644;&#20171;&#32461;&#65292;&#24182;&#35752;&#35770;&#20102;&#24191;&#27867;&#24212;&#29992;&#30340;&#19968;&#31995;&#21015;&#28909;&#38376;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.15351</link><description>&lt;p&gt;
&#20851;&#20110;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#30340;&#32508;&#36848;&#65306;&#26041;&#27861;&#12289;&#24212;&#29992;&#21644;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
A Survey on Neural Topic Models: Methods, Applications, and Challenges. (arXiv:2401.15351v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15351
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#35843;&#30740;&#20102;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#30340;&#26041;&#27861;&#12289;&#24212;&#29992;&#21644;&#25361;&#25112;&#65292;&#23545;&#20110;&#30701;&#25991;&#26412;&#21644;&#36328;&#35821;&#35328;&#25991;&#26723;&#31561;&#21508;&#31181;&#22330;&#26223;&#25552;&#20379;&#20102;&#31995;&#32479;&#24615;&#30340;&#32452;&#32455;&#21644;&#20171;&#32461;&#65292;&#24182;&#35752;&#35770;&#20102;&#24191;&#27867;&#24212;&#29992;&#30340;&#19968;&#31995;&#21015;&#28909;&#38376;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#39064;&#27169;&#22411;&#20960;&#21313;&#24180;&#26469;&#19968;&#30452;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#26080;&#30417;&#30563;&#26041;&#24335;&#19979;&#21457;&#29616;&#28508;&#22312;&#20027;&#39064;&#21644;&#25512;&#26029;&#25991;&#26723;&#30340;&#20027;&#39064;&#27604;&#20363;&#12290;&#23427;&#20204;&#22312;&#25991;&#26412;&#20998;&#26512;&#21644;&#19978;&#19979;&#25991;&#25512;&#33616;&#31561;&#21508;&#31181;&#24212;&#29992;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#12290;&#36817;&#24180;&#26469;&#65292;&#31070;&#32463;&#32593;&#32476;&#30340;&#23835;&#36215;&#20419;&#25104;&#20102;&#19968;&#20010;&#26032;&#30340;&#30740;&#31350;&#39046;&#22495;&#8212;&#8212;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;(NTMs)&#30340;&#20986;&#29616;&#12290;&#19982;&#20256;&#32479;&#30340;&#20027;&#39064;&#27169;&#22411;&#19981;&#21516;&#65292;NTMs&#30452;&#25509;&#20248;&#21270;&#21442;&#25968;&#65292;&#32780;&#19981;&#38656;&#35201;&#27169;&#22411;&#29305;&#23450;&#30340;&#25512;&#23548;&#12290;&#36825;&#20351;&#24471;NTMs&#20855;&#26377;&#26356;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#28789;&#27963;&#24615;&#65292;&#21560;&#24341;&#20102;&#22823;&#37327;&#30340;&#30740;&#31350;&#20851;&#27880;&#24182;&#20135;&#29983;&#20102;&#20016;&#23500;&#30340;&#26032;&#26041;&#27861;&#21644;&#24212;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#30340;&#26041;&#27861;&#12289;&#24212;&#29992;&#21644;&#25361;&#25112;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35843;&#30740;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#26681;&#25454;&#32593;&#32476;&#32467;&#26500;&#31995;&#32479;&#22320;&#32452;&#32455;&#20102;&#24403;&#21069;NTM&#26041;&#27861;&#65292;&#24182;&#20171;&#32461;&#20102;&#38024;&#23545;&#30701;&#25991;&#26412;&#21644;&#36328;&#35821;&#35328;&#25991;&#26723;&#31561;&#21508;&#31181;&#22330;&#26223;&#30340;NTMs&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#24191;&#27867;&#24212;&#29992;&#30340;&#19968;&#31995;&#21015;&#28909;&#38376;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Topic models have been prevalent for decades to discover latent topics and infer topic proportions of documents in an unsupervised fashion. They have been widely used in various applications like text analysis and context recommendation. Recently, the rise of neural networks has facilitated the emergence of a new research field -- Neural Topic Models (NTMs). Different from conventional topic models, NTMs directly optimize parameters without requiring model-specific derivations. This endows NTMs with better scalability and flexibility, resulting in significant research attention and plentiful new methods and applications. In this paper, we present a comprehensive survey on neural topic models concerning methods, applications, and challenges. Specifically, we systematically organize current NTM methods according to their network structures and introduce the NTMs for various scenarios like short texts and cross-lingual documents. We also discuss a wide range of popular applications built 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#39046;&#22495;&#23545;&#25239;&#35757;&#32451;(DAT)&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#21892;&#22024;&#26434;&#29615;&#22659;&#20013;&#38899;&#20048;&#33258;&#21160;&#26631;&#35760;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#39069;&#22806;&#30340;&#39044;&#35757;&#32451;&#38454;&#27573;&#21644;&#28155;&#21152;&#21512;&#25104;&#30340;&#22024;&#26434;&#38899;&#20048;&#25968;&#25454;&#65292;&#33719;&#24471;&#20102;&#40065;&#26834;&#30340;&#38899;&#20048;&#34920;&#31034;&#65292;&#24182;&#22312;&#38899;&#20048;&#33258;&#21160;&#26631;&#35760;&#26041;&#38754;&#23637;&#29616;&#20102;&#22686;&#24378;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.15323</link><description>&lt;p&gt;
&#38899;&#20048;&#33258;&#21160;&#26631;&#35760;&#65306;&#36890;&#36807;&#39046;&#22495;&#23545;&#25239;&#35757;&#32451;&#23398;&#20064;&#40065;&#26834;&#30340;&#38899;&#20048;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Music Auto-Tagging with Robust Music Representation Learned via Domain Adversarial Training. (arXiv:2401.15323v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15323
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#39046;&#22495;&#23545;&#25239;&#35757;&#32451;(DAT)&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#21892;&#22024;&#26434;&#29615;&#22659;&#20013;&#38899;&#20048;&#33258;&#21160;&#26631;&#35760;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#39069;&#22806;&#30340;&#39044;&#35757;&#32451;&#38454;&#27573;&#21644;&#28155;&#21152;&#21512;&#25104;&#30340;&#22024;&#26434;&#38899;&#20048;&#25968;&#25454;&#65292;&#33719;&#24471;&#20102;&#40065;&#26834;&#30340;&#38899;&#20048;&#34920;&#31034;&#65292;&#24182;&#22312;&#38899;&#20048;&#33258;&#21160;&#26631;&#35760;&#26041;&#38754;&#23637;&#29616;&#20102;&#22686;&#24378;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#20048;&#33258;&#21160;&#26631;&#35760;&#23545;&#20110;&#22686;&#24378;&#38899;&#20048;&#21457;&#29616;&#21644;&#25512;&#33616;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#38899;&#20048;&#20449;&#24687;&#26816;&#32034;(MIR)&#27169;&#22411;&#22312;&#22810;&#23186;&#20307;&#20869;&#23481;&#20013;&#23384;&#22312;&#30340;&#29615;&#22659;&#22122;&#22768;&#21644;&#35821;&#38899;&#22768;&#38899;&#31561;&#29616;&#23454;&#19990;&#30028;&#22122;&#22768;&#26041;&#38754;&#38754;&#20020;&#22256;&#38590;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#35821;&#38899;&#30456;&#20851;&#20219;&#21153;&#21551;&#21457;&#30340;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#22024;&#26434;&#29615;&#22659;&#20013;&#30340;&#38899;&#20048;&#33258;&#21160;&#26631;&#35760;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#23558;&#39046;&#22495;&#23545;&#25239;&#35757;&#32451;(DAT)&#38598;&#25104;&#21040;&#38899;&#20048;&#39046;&#22495;&#20013;&#65292;&#20351;&#24471;&#40065;&#26834;&#30340;&#38899;&#20048;&#34920;&#31034;&#33021;&#22815;&#25269;&#25239;&#22122;&#22768;&#12290;&#19982;&#20197;&#21069;&#30340;&#30740;&#31350;&#19981;&#21516;&#65292;&#35813;&#26041;&#27861;&#36824;&#28041;&#21450;&#39046;&#22495;&#20998;&#31867;&#22120;&#30340;&#39069;&#22806;&#39044;&#35757;&#32451;&#38454;&#27573;&#65292;&#20197;&#36991;&#20813;&#21518;&#32493;&#38454;&#27573;&#24615;&#33021;&#30340;&#19979;&#38477;&#12290;&#28155;&#21152;&#21508;&#31181;&#21512;&#25104;&#30340;&#22024;&#26434;&#38899;&#20048;&#25968;&#25454;&#25913;&#21892;&#20102;&#35813;&#27169;&#22411;&#22312;&#19981;&#21516;&#22122;&#22768;&#27700;&#24179;&#19979;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25152;&#25552;&#20986;&#30340;&#26550;&#26500;&#36890;&#36807;&#26377;&#25928;&#21033;&#29992;&#26080;&#26631;&#31614;&#30340;&#22024;&#26434;&#38899;&#20048;&#25968;&#25454;&#65292;&#22312;&#38899;&#20048;&#33258;&#21160;&#26631;&#35760;&#26041;&#38754;&#23637;&#29616;&#20102;&#22686;&#24378;&#30340;&#24615;&#33021;&#12290;&#22312;&#34917;&#20805;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#39069;&#22806;&#23454;&#39564;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Music auto-tagging is crucial for enhancing music discovery and recommendation. Existing models in Music Information Retrieval (MIR) struggle with real-world noise such as environmental and speech sounds in multimedia content. This study proposes a method inspired by speech-related tasks to enhance music auto-tagging performance in noisy settings. The approach integrates Domain Adversarial Training (DAT) into the music domain, enabling robust music representations that withstand noise. Unlike previous research, this approach involves an additional pretraining phase for the domain classifier, to avoid performance degradation in the subsequent phase. Adding various synthesized noisy music data improves the model's generalization across different noise levels. The proposed architecture demonstrates enhanced performance in music auto-tagging by effectively utilizing unlabeled noisy music data. Additional experiments with supplementary unlabeled data further improves the model's performance
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Self-BioRAG&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#26816;&#32034;&#21644;&#33258;&#25105;&#21453;&#24605;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#21307;&#30103;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;&#35813;&#26694;&#26550;&#19987;&#27880;&#20110;&#29983;&#25104;&#35299;&#37322;&#12289;&#26816;&#32034;&#39046;&#22495;&#29305;&#23450;&#25991;&#26723;&#20197;&#21450;&#23545;&#29983;&#25104;&#30340;&#21709;&#24212;&#36827;&#34892;&#33258;&#25105;&#21453;&#24605;&#12290;</title><link>http://arxiv.org/abs/2401.15269</link><description>&lt;p&gt;
&#36890;&#36807;&#26816;&#32034;&#21644;&#33258;&#25105;&#21453;&#24605;&#25913;&#21892;&#21307;&#30103;&#25512;&#29702;&#33021;&#21147;&#30340;&#26816;&#32034;&#22686;&#24378;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Improving Medical Reasoning through Retrieval and Self-Reflection with Retrieval-Augmented Large Language Models. (arXiv:2401.15269v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15269
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Self-BioRAG&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#26816;&#32034;&#21644;&#33258;&#25105;&#21453;&#24605;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#21307;&#30103;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;&#35813;&#26694;&#26550;&#19987;&#27880;&#20110;&#29983;&#25104;&#35299;&#37322;&#12289;&#26816;&#32034;&#39046;&#22495;&#29305;&#23450;&#25991;&#26723;&#20197;&#21450;&#23545;&#29983;&#25104;&#30340;&#21709;&#24212;&#36827;&#34892;&#33258;&#25105;&#21453;&#24605;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#19987;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#20363;&#22914;GPT-4&#65292;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20013;&#35299;&#20915;&#20102;&#20174;&#22810;&#39033;&#36873;&#25321;&#39064;&#21040;&#38271;&#31687;&#29983;&#25104;&#31561;&#22810;&#26679;&#21270;&#25361;&#25112;&#30340;&#37324;&#31243;&#30865;&#12290;&#20026;&#20102;&#35299;&#20915;LLMs&#32534;&#30721;&#30693;&#35782;&#26080;&#27861;&#22788;&#29702;&#30340;&#25361;&#25112;&#65292;&#24050;&#32463;&#24320;&#21457;&#20102;&#21508;&#31181;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#30693;&#35782;&#35821;&#26009;&#24211;&#20013;&#25628;&#32034;&#25991;&#26723;&#24182;&#26080;&#26465;&#20214;&#25110;&#26377;&#36873;&#25321;&#22320;&#23558;&#20854;&#38468;&#21152;&#21040;LLMs&#30340;&#36755;&#20837;&#26469;&#36827;&#34892;&#29983;&#25104;&#12290;&#28982;&#32780;&#65292;&#23558;&#29616;&#26377;&#26041;&#27861;&#24212;&#29992;&#20110;&#19981;&#21516;&#39046;&#22495;&#29305;&#23450;&#38382;&#39064;&#26102;&#65292;&#20986;&#29616;&#20102;&#27867;&#21270;&#33021;&#21147;&#24046;&#30340;&#38382;&#39064;&#65292;&#23548;&#33268;&#33719;&#21462;&#19981;&#27491;&#30830;&#30340;&#25991;&#26723;&#25110;&#20570;&#20986;&#19981;&#20934;&#30830;&#30340;&#21028;&#26029;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#38752;&#30340;&#21307;&#23398;&#25991;&#26412;&#26694;&#26550;Self-BioRAG&#65292;&#19987;&#38376;&#29992;&#20110;&#29983;&#25104;&#35299;&#37322;&#12289;&#26816;&#32034;&#39046;&#22495;&#29305;&#23450;&#25991;&#26723;&#21644;&#33258;&#25105;&#21453;&#24605;&#29983;&#25104;&#30340;&#21709;&#24212;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;84k&#20010;&#32463;&#36807;&#36807;&#28388;&#30340;&#29983;&#29289;&#21307;&#23398;&#25351;&#20196;&#38598;&#26469;&#35757;&#32451;Self-BioRAG&#65292;&#23427;&#20855;&#22791;&#35780;&#20272;&#33258;&#24049;&#30340;&#22522;&#22240;
&lt;/p&gt;
&lt;p&gt;
Recent proprietary large language models (LLMs), such as GPT-4, have achieved a milestone in tackling diverse challenges in the biomedical domain, ranging from multiple-choice questions to long-form generations. To address challenges that still cannot be handled with the encoded knowledge of LLMs, various retrieval-augmented generation (RAG) methods have been developed by searching documents from the knowledge corpus and appending them unconditionally or selectively to the input of LLMs for generation. However, when applying existing methods to different domain-specific problems, poor generalization becomes apparent, leading to fetching incorrect documents or making inaccurate judgments. In this paper, we introduce Self-BioRAG, a framework reliable for biomedical text that specializes in generating explanations, retrieving domain-specific documents, and self-reflecting generated responses. We utilize 84k filtered biomedical instruction sets to train Self-BioRAG that can assess its gene
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#21644;&#35780;&#20272;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#31995;&#32479;&#20013;&#30340;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#32452;&#20214;&#65292;&#22635;&#34917;&#20102;&#30446;&#21069;&#30740;&#31350;&#20013;&#24573;&#35270;&#30340;&#39046;&#22495;&#65292;&#22312;&#26377;&#25928;&#30340;RAG&#30340;&#25552;&#31034;&#34920;&#36848;&#20013;&#65292;&#19981;&#30456;&#20851;&#25991;&#26723;&#30340;&#21253;&#21547;&#21487;&#33021;&#20250;&#23545;&#31995;&#32479;&#24615;&#33021;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2401.14887</link><description>&lt;p&gt;
&#22122;&#22768;&#30340;&#21147;&#37327;&#65306;&#37325;&#26032;&#23450;&#20041;RAG&#31995;&#32479;&#30340;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
The Power of Noise: Redefining Retrieval for RAG Systems. (arXiv:2401.14887v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14887
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#21644;&#35780;&#20272;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#31995;&#32479;&#20013;&#30340;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#32452;&#20214;&#65292;&#22635;&#34917;&#20102;&#30446;&#21069;&#30740;&#31350;&#20013;&#24573;&#35270;&#30340;&#39046;&#22495;&#65292;&#22312;&#26377;&#25928;&#30340;RAG&#30340;&#25552;&#31034;&#34920;&#36848;&#20013;&#65292;&#19981;&#30456;&#20851;&#25991;&#26723;&#30340;&#21253;&#21547;&#21487;&#33021;&#20250;&#23545;&#31995;&#32479;&#24615;&#33021;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#31995;&#32479;&#30456;&#23545;&#20110;&#20256;&#32479;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20195;&#34920;&#20102;&#19968;&#20010;&#37325;&#22823;&#36827;&#27493;&#12290;RAG&#31995;&#32479;&#36890;&#36807;&#25972;&#21512;&#36890;&#36807;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#38454;&#27573;&#26816;&#32034;&#30340;&#22806;&#37096;&#25968;&#25454;&#26469;&#22686;&#24378;&#20854;&#29983;&#25104;&#33021;&#21147;&#65292;&#20811;&#26381;&#20102;&#26631;&#20934;LLMs&#30340;&#38480;&#21046;&#65292;&#21518;&#32773;&#20165;&#38480;&#20110;&#20854;&#39044;&#20808;&#35757;&#32451;&#30340;&#30693;&#35782;&#21644;&#26377;&#38480;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#12290;&#36825;&#20010;&#39046;&#22495;&#30340;&#22823;&#37096;&#20998;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;RAG&#31995;&#32479;&#20869;LLMs&#30340;&#29983;&#25104;&#26041;&#38754;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#22635;&#34917;&#20102;&#36825;&#19968;&#31354;&#30333;&#65292;&#36890;&#36807;&#20840;&#38754;&#32780;&#25209;&#21028;&#24615;&#22320;&#20998;&#26512;IR&#32452;&#20214;&#23545;RAG&#31995;&#32479;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#19968;&#20010;&#26816;&#32034;&#22120;&#22312;&#26377;&#25928;&#30340;RAG&#30340;&#25552;&#31034;&#34920;&#36848;&#20013;&#24212;&#35813;&#20855;&#22791;&#30340;&#29305;&#24449;&#65292;&#37325;&#28857;&#20851;&#27880;&#24212;&#35813;&#26816;&#32034;&#21738;&#31181;&#31867;&#22411;&#30340;&#25991;&#26723;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#21508;&#31181;&#22240;&#32032;&#65292;&#22914;&#25991;&#26723;&#19982;&#25552;&#31034;&#30340;&#30456;&#20851;&#24615;&#65292;&#23427;&#20204;&#30340;&#20301;&#32622;&#20197;&#21450;&#19978;&#19979;&#25991;&#20013;&#21253;&#21547;&#30340;&#25968;&#37327;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#25581;&#31034;&#20986;&#65292;&#21253;&#21547;&#19981;&#30456;&#20851;&#30340;&#25991;&#26723;&#21487;&#33021;&#20250;&#8230;
&lt;/p&gt;
&lt;p&gt;
Retrieval-Augmented Generation (RAG) systems represent a significant advancement over traditional Large Language Models (LLMs). RAG systems enhance their generation ability by incorporating external data retrieved through an Information Retrieval (IR) phase, overcoming the limitations of standard LLMs, which are restricted to their pre-trained knowledge and limited context window. Most research in this area has predominantly concentrated on the generative aspect of LLMs within RAG systems. Our study fills this gap by thoroughly and critically analyzing the influence of IR components on RAG systems. This paper analyzes which characteristics a retriever should possess for an effective RAG's prompt formulation, focusing on the type of documents that should be retrieved. We evaluate various elements, such as the relevance of the documents to the prompt, their position, and the number included in the context. Our findings reveal, among other insights, that including irrelevant documents can
&lt;/p&gt;</description></item><item><title>PolyCF&#26159;&#19968;&#20010;&#28789;&#27963;&#30340;&#22270;&#20449;&#21495;&#28388;&#27874;&#22120;&#65292;&#36890;&#36807;&#22810;&#39033;&#24335;&#22270;&#36807;&#28388;&#22120;&#22788;&#29702;&#20132;&#20114;&#20449;&#21495;&#65292;&#33021;&#22815;&#25429;&#25417;&#22810;&#20010;&#29305;&#24449;&#31354;&#38388;&#20013;&#30340;&#35889;&#29305;&#24449;&#65292;&#24182;&#36817;&#20284;&#24674;&#22797;&#20002;&#22833;&#30340;&#20132;&#20114;&#65292;&#26088;&#22312;&#23454;&#29616;&#26368;&#20248;&#30340;&#21327;&#21516;&#36807;&#28388;&#12290;</title><link>http://arxiv.org/abs/2401.12590</link><description>&lt;p&gt;
PolyCF: &#38754;&#21521;&#21327;&#21516;&#36807;&#28388;&#30340;&#26368;&#20248;&#35889;&#22270;&#36807;&#28388;&#22120;
&lt;/p&gt;
&lt;p&gt;
PolyCF: Towards the Optimal Spectral Graph Filters for Collaborative Filtering. (arXiv:2401.12590v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12590
&lt;/p&gt;
&lt;p&gt;
PolyCF&#26159;&#19968;&#20010;&#28789;&#27963;&#30340;&#22270;&#20449;&#21495;&#28388;&#27874;&#22120;&#65292;&#36890;&#36807;&#22810;&#39033;&#24335;&#22270;&#36807;&#28388;&#22120;&#22788;&#29702;&#20132;&#20114;&#20449;&#21495;&#65292;&#33021;&#22815;&#25429;&#25417;&#22810;&#20010;&#29305;&#24449;&#31354;&#38388;&#20013;&#30340;&#35889;&#29305;&#24449;&#65292;&#24182;&#36817;&#20284;&#24674;&#22797;&#20002;&#22833;&#30340;&#20132;&#20114;&#65292;&#26088;&#22312;&#23454;&#29616;&#26368;&#20248;&#30340;&#21327;&#21516;&#36807;&#28388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21327;&#21516;&#36807;&#28388;&#65288;CF&#65289;&#26159;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#30740;&#31350;&#39046;&#22495;&#65292;&#21033;&#29992;&#29992;&#25143;&#19982;&#39033;&#30446;&#20043;&#38388;&#30340;&#21327;&#21516;&#30456;&#20284;&#24615;&#25552;&#20379;&#20010;&#24615;&#21270;&#25512;&#33616;&#12290;&#22312;&#22522;&#20110;&#33410;&#28857;&#23884;&#20837;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#21462;&#24471;&#26174;&#33879;&#25104;&#23601;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#22522;&#20110;&#23884;&#20837;&#26041;&#27861;&#30340;&#34920;&#36798;&#33021;&#21147;&#30340;&#19978;&#38480;&#65292;&#24182;&#36890;&#36807;&#23558;CF&#20219;&#21153;&#37325;&#26032;&#23450;&#20041;&#20026;&#22270;&#20449;&#21495;&#22788;&#29702;&#38382;&#39064;&#26469;&#24212;&#23545;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PolyCF&#65292;&#19968;&#20010;&#28789;&#27963;&#30340;&#22270;&#20449;&#21495;&#28388;&#27874;&#22120;&#65292;&#21033;&#29992;&#22810;&#39033;&#24335;&#22270;&#36807;&#28388;&#22120;&#22788;&#29702;&#20132;&#20114;&#20449;&#21495;&#12290;PolyCF&#36890;&#36807;&#19968;&#31995;&#21015;&#24191;&#20041;&#26684;&#25289;&#22982;&#28388;&#27874;&#22120;&#25429;&#25417;&#22810;&#20010;&#29305;&#24449;&#31354;&#38388;&#20013;&#30340;&#35889;&#29305;&#24449;&#65292;&#24182;&#33021;&#22815;&#36817;&#20284;&#20110;&#24674;&#22797;&#20002;&#22833;&#30340;&#20132;&#20114;&#30340;&#26368;&#20248;&#22810;&#39033;&#24335;&#21709;&#24212;&#20989;&#25968;&#12290;&#22270;&#20248;&#21270;&#30446;&#26631;&#21644;&#25104;&#23545;&#25490;&#21517;&#30446;&#26631;&#20849;&#21516;&#29992;&#20110;&#20248;&#21270;&#21367;&#31215;&#26680;&#30340;&#21442;&#25968;&#12290;&#22312;&#19977;&#20010;&#24191;&#27867;&#37319;&#29992;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
Collaborative Filtering (CF) is a pivotal research area in recommender systems that capitalizes on collaborative similarities between users and items to provide personalized recommendations. With the remarkable achievements of node embedding-based Graph Neural Networks (GNNs), we explore the upper bounds of expressiveness inherent to embedding-based methodologies and tackle the challenges by reframing the CF task as a graph signal processing problem. To this end, we propose PolyCF, a flexible graph signal filter that leverages polynomial graph filters to process interaction signals. PolyCF exhibits the capability to capture spectral features across multiple eigenspaces through a series of Generalized Gram filters and is able to approximate the optimal polynomial response function for recovering missing interactions. A graph optimization objective and a pair-wise ranking objective are jointly used to optimize the parameters of the convolution kernel. Experiments on three widely adopted 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#23884;&#20837;&#26041;&#26696;&#65292;&#25512;&#26029;&#20986;&#20102;&#30693;&#35782;&#20132;&#38169;&#22320;&#22270;&#65292;&#25581;&#31034;&#20102;&#30693;&#35782;&#20043;&#38388;&#38169;&#32508;&#22797;&#26434;&#30340;&#32852;&#31995;&#65292;&#24182;&#23637;&#31034;&#20102;&#22810;&#20010;&#24212;&#29992;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2401.11742</link><description>&lt;p&gt;
&#30693;&#35782;&#23548;&#33322;&#65306;&#20174;&#30740;&#31350;&#36712;&#36857;&#20013;&#25512;&#26029;&#30693;&#35782;&#30340;&#20132;&#38169;&#22320;&#22270;
&lt;/p&gt;
&lt;p&gt;
Knowledge Navigation: Inferring the Interlocking Map of Knowledge from Research Trajectories. (arXiv:2401.11742v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11742
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#23884;&#20837;&#26041;&#26696;&#65292;&#25512;&#26029;&#20986;&#20102;&#30693;&#35782;&#20132;&#38169;&#22320;&#22270;&#65292;&#25581;&#31034;&#20102;&#30693;&#35782;&#20043;&#38388;&#38169;&#32508;&#22797;&#26434;&#30340;&#32852;&#31995;&#65292;&#24182;&#23637;&#31034;&#20102;&#22810;&#20010;&#24212;&#29992;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
"&#22914;&#26524;&#25105;&#30475;&#24471;&#26356;&#36828;&#65292;&#37027;&#26159;&#22240;&#20026;&#25105;&#31449;&#22312;&#24040;&#20154;&#30340;&#32937;&#33152;&#19978;&#12290;"&#33406;&#33832;&#20811;&#183;&#29275;&#39039;&#30340;&#33879;&#21517;&#22768;&#26126;&#26263;&#31034;&#20102;&#26032;&#30693;&#35782;&#24314;&#31435;&#22312;&#29616;&#26377;&#22522;&#30784;&#20043;&#19978;&#30340;&#20107;&#23454;&#65292;&#36825;&#24847;&#21619;&#30528;&#30693;&#35782;&#20043;&#38388;&#23384;&#22312;&#30528;&#30456;&#20114;&#20381;&#36182;&#30340;&#20851;&#31995;&#65292;&#32780;&#36825;&#31181;&#20851;&#31995;&#22312;&#31185;&#23398;&#20307;&#31995;&#30340;&#21382;&#21490;&#21457;&#23637;&#20013;&#19968;&#30452;&#26410;&#34987;&#25581;&#31034;&#12290;&#36890;&#36807;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#65292;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#23884;&#20837;&#26041;&#26696;&#65292;&#26088;&#22312;&#25512;&#26029;&#8220;&#30693;&#35782;&#20132;&#38169;&#22320;&#22270;&#8221;&#12290;&#36825;&#20010;&#22320;&#22270;&#26159;&#20174;&#25968;&#30334;&#19975;&#23398;&#32773;&#30340;&#30740;&#31350;&#36712;&#36857;&#20013;&#25512;&#23548;&#20986;&#26469;&#30340;&#65292;&#25581;&#31034;&#20102;&#30693;&#35782;&#20043;&#38388;&#38169;&#32508;&#22797;&#26434;&#30340;&#32852;&#31995;&#12290;&#25105;&#20204;&#39564;&#35777;&#20102;&#25512;&#26029;&#20986;&#30340;&#22320;&#22270;&#26377;&#25928;&#22320;&#21246;&#30011;&#20102;&#23398;&#31185;&#36793;&#30028;&#65292;&#24182;&#25429;&#25417;&#21040;&#20102;&#19981;&#21516;&#27010;&#24565;&#20043;&#38388;&#22797;&#26434;&#30340;&#20851;&#31995;&#12290;&#20132;&#38169;&#22320;&#22270;&#30340;&#23454;&#29992;&#24615;&#36890;&#36807;&#22810;&#20010;&#24212;&#29992;&#23637;&#31034;&#20986;&#26469;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#30693;&#35782;&#31354;&#38388;&#20013;&#30340;&#22810;&#27493;&#31867;&#27604;&#25512;&#29702;&#21644;&#27010;&#24565;&#20043;&#38388;&#30340;&#21151;&#33021;&#36830;&#25509;&#12290;
&lt;/p&gt;
&lt;p&gt;
"If I have seen further, it is by standing on the shoulders of giants," Isaac Newton's renowned statement hints that new knowledge builds upon existing foundations, which means there exists an interdependent relationship between knowledge, which, yet uncovered, is implied in the historical development of scientific systems for hundreds of years. By leveraging natural language processing techniques, this study introduces an innovative embedding scheme designed to infer the "knowledge interlocking map." This map, derived from the research trajectories of millions of scholars, reveals the intricate connections among knowledge. We validate that the inferred map effectively delineates disciplinary boundaries and captures the intricate relationships between diverse concepts. The utility of the interlocking map is showcased through multiple applications. Firstly, we demonstrated the multi-step analogy inferences within the knowledge space and the functional connectivity between concepts in di
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21307;&#23398;&#20195;&#30721;&#20013;&#24515;&#30340;&#22810;&#27169;&#24577;&#23545;&#27604;EHR&#24314;&#27169;&#39044;&#27979;&#19979;&#27425;&#23601;&#35786;&#35786;&#26029;&#65292;&#24182;&#36890;&#36807;&#20998;&#23618;&#27491;&#21017;&#21270;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.11648</link><description>&lt;p&gt;
&#36890;&#36807;&#20855;&#26377;&#20998;&#23618;&#27491;&#21017;&#21270;&#30340;&#21307;&#23398;&#20195;&#30721;&#20013;&#24515;&#30340;&#22810;&#27169;&#24577;&#23545;&#27604;EHR&#24314;&#27169;&#39044;&#27979;&#19979;&#27425;&#23601;&#35786;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
Next Visit Diagnosis Prediction via Medical Code-Centric Multimodal Contrastive EHR Modelling with Hierarchical Regularisation. (arXiv:2401.11648v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11648
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21307;&#23398;&#20195;&#30721;&#20013;&#24515;&#30340;&#22810;&#27169;&#24577;&#23545;&#27604;EHR&#24314;&#27169;&#39044;&#27979;&#19979;&#27425;&#23601;&#35786;&#35786;&#26029;&#65292;&#24182;&#36890;&#36807;&#20998;&#23618;&#27491;&#21017;&#21270;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#65292;&#21033;&#29992;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#39044;&#27979;&#19979;&#27425;&#23601;&#35786;&#30340;&#35786;&#26029;&#26159;&#19968;&#39033;&#24517;&#35201;&#30340;&#20219;&#21153;&#65292;&#23545;&#20110;&#21046;&#23450;&#21307;&#30103;&#20445;&#20581;&#25552;&#20379;&#32773;&#21644;&#24739;&#32773;&#30340;&#20027;&#21160;&#26410;&#26469;&#35745;&#21010;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20043;&#21069;&#30340;&#35768;&#22810;&#30740;&#31350;&#24182;&#27809;&#26377;&#20805;&#20998;&#35299;&#20915;EHR&#25968;&#25454;&#22266;&#26377;&#30340;&#24322;&#26500;&#21644;&#20998;&#23618;&#29305;&#24449;&#65292;&#24517;&#28982;&#23548;&#33268;&#27425;&#20248;&#30340;&#24615;&#33021;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;NECHO&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#21307;&#23398;&#20195;&#30721;&#20013;&#24515;&#30340;&#22810;&#27169;&#24577;&#23545;&#27604;EHR&#23398;&#20064;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#20998;&#23618;&#27491;&#21017;&#21270;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#23450;&#21046;&#30340;&#32593;&#32476;&#35774;&#35745;&#21644;&#19968;&#23545;&#21452;&#27169;&#24577;&#23545;&#27604;&#25439;&#22833;&#34701;&#21512;&#28085;&#30422;&#21307;&#23398;&#20195;&#30721;&#12289;&#20154;&#21475;&#32479;&#35745;&#25968;&#25454;&#21644;&#20020;&#24202;&#31508;&#35760;&#30340;&#22810;&#26041;&#38754;&#20449;&#24687;&#65292;&#25152;&#26377;&#36825;&#20123;&#37117;&#22260;&#32469;&#30528;&#21307;&#23398;&#20195;&#30721;&#34920;&#29616;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#21307;&#23398;&#26412;&#20307;&#20013;&#30340;&#29238;&#32423;&#20449;&#24687;&#26469;&#35268;&#33539;&#29305;&#23450;&#27169;&#24577;&#30340;&#32534;&#30721;&#22120;&#65292;&#20197;&#23398;&#20064;EHR&#25968;&#25454;&#30340;&#23618;&#27425;&#32467;&#26500;&#12290;&#23545;MIMIC-III&#25968;&#25454;&#36827;&#34892;&#30340;&#19968;&#31995;&#21015;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predicting next visit diagnosis using Electronic Health Records (EHR) is an essential task in healthcare, critical for devising proactive future plans for both healthcare providers and patients. Nonetheless, many preceding studies have not sufficiently addressed the heterogeneous and hierarchical characteristics inherent in EHR data, inevitably leading to sub-optimal performance. To this end, we propose NECHO, a novel medical code-centric multimodal contrastive EHR learning framework with hierarchical regularisation. First, we integrate multifaceted information encompassing medical codes, demographics, and clinical notes using a tailored network design and a pair of bimodal contrastive losses, all of which pivot around a medical code representation. We also regularise modality-specific encoders using a parental level information in medical ontology to learn hierarchical structure of EHR data. A series of experiments on MIMIC-III data demonstrates effectiveness of our approach.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20301;&#32622;&#25935;&#24863;&#23884;&#20837;&#65288;LSE&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20851;&#31995;&#29305;&#23450;&#30340;&#26144;&#23556;&#26469;&#20462;&#25913;&#22836;&#23454;&#20307;&#65292;&#23558;&#20851;&#31995;&#27010;&#24565;&#21270;&#20026;&#32447;&#24615;&#21464;&#25442;&#12290;LSE&#22312;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#39046;&#22495;&#20855;&#26377;&#29702;&#35770;&#22522;&#30784;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#26356;&#39640;&#25928;&#30340;&#21464;&#20307;LSEd&#12290;&#23454;&#39564;&#35777;&#26126;LSEd&#22312;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#19978;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.10893</link><description>&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#30340;&#20301;&#32622;&#25935;&#24863;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Location Sensitive Embedding for Knowledge Graph Embedding. (arXiv:2401.10893v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10893
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20301;&#32622;&#25935;&#24863;&#23884;&#20837;&#65288;LSE&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20851;&#31995;&#29305;&#23450;&#30340;&#26144;&#23556;&#26469;&#20462;&#25913;&#22836;&#23454;&#20307;&#65292;&#23558;&#20851;&#31995;&#27010;&#24565;&#21270;&#20026;&#32447;&#24615;&#21464;&#25442;&#12290;LSE&#22312;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#39046;&#22495;&#20855;&#26377;&#29702;&#35770;&#22522;&#30784;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#26356;&#39640;&#25928;&#30340;&#21464;&#20307;LSEd&#12290;&#23454;&#39564;&#35777;&#26126;LSEd&#22312;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#19978;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#23558;&#30693;&#35782;&#22270;&#35889;&#36716;&#21270;&#20026;&#36830;&#32493;&#30340;&#12289;&#20302;&#32500;&#24230;&#30340;&#31354;&#38388;&#65292;&#26377;&#21161;&#20110;&#25512;&#29702;&#21644;&#34917;&#20840;&#20219;&#21153;&#12290;&#35813;&#39046;&#22495;&#20027;&#35201;&#20998;&#20026;&#20256;&#32479;&#30340;&#36317;&#31163;&#27169;&#22411;&#21644;&#35821;&#20041;&#21305;&#37197;&#27169;&#22411;&#12290;&#20256;&#32479;&#30340;&#36317;&#31163;&#27169;&#22411;&#38754;&#20020;&#30340;&#20851;&#38190;&#25361;&#25112;&#26159;&#26080;&#27861;&#26377;&#25928;&#21306;&#20998;&#22270;&#35889;&#20013;&#30340;&#8220;&#22836;&#23454;&#20307;&#8221;&#21644;&#8220;&#23614;&#23454;&#20307;&#8221;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;&#20301;&#32622;&#25935;&#24863;&#23884;&#20837;&#65288;LSE&#65289;&#26041;&#27861;&#12290;LSE&#36890;&#36807;&#20851;&#31995;&#29305;&#23450;&#30340;&#26144;&#23556;&#20462;&#25913;&#22836;&#23454;&#20307;&#65292;&#23558;&#20851;&#31995;&#27010;&#24565;&#21270;&#20026;&#32447;&#24615;&#21464;&#25442;&#32780;&#19981;&#20165;&#20165;&#26159;&#24179;&#31227;&#12290;LSE&#30340;&#29702;&#35770;&#22522;&#30784;&#65292;&#21253;&#25324;&#20854;&#34920;&#31034;&#33021;&#21147;&#21644;&#19982;&#29616;&#26377;&#27169;&#22411;&#30340;&#32852;&#31995;&#65292;&#37117;&#36827;&#34892;&#20102;&#35814;&#32454;&#30740;&#31350;&#12290;&#19968;&#31181;&#26356;&#31616;&#21270;&#30340;&#21464;&#20307;LSEd&#21033;&#29992;&#23545;&#35282;&#30697;&#38453;&#36827;&#34892;&#21464;&#25442;&#20197;&#25552;&#39640;&#23454;&#29992;&#24615;&#33021;&#12290;&#22312;&#23545;&#22235;&#20010;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#36827;&#34892;&#38142;&#25509;&#39044;&#27979;&#30340;&#27979;&#35797;&#20013;&#65292;LSEd&#35201;&#20040;&#34920;&#29616;&#26356;&#22909;&#65292;&#35201;&#20040;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graph embedding transforms knowledge graphs into a continuous, low-dimensional space, facilitating inference and completion tasks. This field is mainly divided into translational distance models and semantic matching models. A key challenge in translational distance models is their inability to effectively differentiate between 'head' and 'tail' entities in graphs. To address this, the novel location-sensitive embedding (LSE) method has been developed. LSE innovatively modifies the head entity using relation-specific mappings, conceptualizing relations as linear transformations rather than mere translations. The theoretical foundations of LSE, including its representational capabilities and its connections to existing models, have been thoroughly examined. A more streamlined variant, LSEd, employs a diagonal matrix for transformations to enhance practical efficiency. In tests conducted on four large-scale datasets for link prediction, LSEd either outperforms or is competitive
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#25512;&#33616;&#27169;&#22411;KGLN&#65292;&#36890;&#36807;&#21512;&#24182;&#33410;&#28857;&#29305;&#24449;&#12289;&#35843;&#25972;&#32858;&#21512;&#26435;&#37325;&#21644;&#36845;&#20195;&#28436;&#21270;&#65292;&#25552;&#39640;&#20102;&#20010;&#24615;&#21270;&#25512;&#33616;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#26524;&#12290;&#22312;&#23454;&#39564;&#20013;&#30456;&#23545;&#20110;&#24050;&#26377;&#22522;&#20934;&#26041;&#27861;&#65292;KGLN&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;AUC&#25552;&#39640;&#20102;0.3%&#33267;5.9%&#21644;1.1%&#33267;8.2%&#12290;</title><link>http://arxiv.org/abs/2401.10244</link><description>&lt;p&gt;
&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#39537;&#21160;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#25512;&#33616;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Knowledge graph driven recommendation model of graph neural network. (arXiv:2401.10244v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10244
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#25512;&#33616;&#27169;&#22411;KGLN&#65292;&#36890;&#36807;&#21512;&#24182;&#33410;&#28857;&#29305;&#24449;&#12289;&#35843;&#25972;&#32858;&#21512;&#26435;&#37325;&#21644;&#36845;&#20195;&#28436;&#21270;&#65292;&#25552;&#39640;&#20102;&#20010;&#24615;&#21270;&#25512;&#33616;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#26524;&#12290;&#22312;&#23454;&#39564;&#20013;&#30456;&#23545;&#20110;&#24050;&#26377;&#22522;&#20934;&#26041;&#27861;&#65292;KGLN&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;AUC&#25552;&#39640;&#20102;0.3%&#33267;5.9%&#21644;1.1%&#33267;8.2%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25512;&#33616;&#27169;&#22411;KGLN&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#20449;&#24687;&#65292;&#25552;&#39640;&#20102;&#20010;&#24615;&#21270;&#25512;&#33616;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#26524;&#12290;&#35813;&#27169;&#22411;&#39318;&#20808;&#21033;&#29992;&#21333;&#23618;&#31070;&#32463;&#32593;&#32476;&#23558;&#22270;&#20013;&#30340;&#20010;&#20307;&#33410;&#28857;&#29305;&#24449;&#21512;&#24182;&#65292;&#28982;&#21518;&#36890;&#36807;&#32467;&#21512;&#24433;&#21709;&#22240;&#32032;&#35843;&#25972;&#30456;&#37051;&#23454;&#20307;&#30340;&#32858;&#21512;&#26435;&#37325;&#12290;&#36890;&#36807;&#36845;&#20195;&#65292;&#27169;&#22411;&#20174;&#21333;&#23618;&#36880;&#28176;&#28436;&#21464;&#20026;&#22810;&#23618;&#65292;&#20351;&#23454;&#20307;&#33021;&#22815;&#33719;&#21462;&#20016;&#23500;&#30340;&#22810;&#38454;&#20851;&#32852;&#23454;&#20307;&#20449;&#24687;&#12290;&#26368;&#21518;&#65292;&#23558;&#23454;&#20307;&#21644;&#29992;&#25143;&#30340;&#29305;&#24449;&#32467;&#21512;&#36215;&#26469;&#20135;&#29983;&#25512;&#33616;&#20998;&#25968;&#12290;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#32858;&#21512;&#26041;&#27861;&#21644;&#24433;&#21709;&#22240;&#32032;&#30340;&#25928;&#26524;&#65292;&#35780;&#20272;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#22312;&#20351;&#29992;MovieLen-1M&#21644;Book-Crossing&#25968;&#25454;&#38598;&#36827;&#34892;&#27979;&#35797;&#26102;&#65292;KGLN&#30456;&#23545;&#20110;LibFM&#21644;D&#31561;&#24050;&#26377;&#22522;&#20934;&#26041;&#27861;&#65292;AUC&#65288;ROC&#26354;&#32447;&#19979;&#30340;&#38754;&#31215;&#65289;&#25552;&#39640;&#20102;0.3%&#33267;5.9%&#21644;1.1%&#33267;8.2%&#12290;
&lt;/p&gt;
&lt;p&gt;
A new graph neural network-based recommendation model called KGLN, which leverages Knowledge Graph (KG) information, was developed to enhance the accuracy and effectiveness of personalized recommendations. This model begins by using a single-layer neural network to merge individual node features in the graph. It then adjusts the aggregation weights of neighboring entities by incorporating influence factors. The model evolves from a single layer to multiple layers through iteration, enabling entities to access extensive multi-order associated entity information. The final step involves integrating features of entities and users to produce a recommendation score. The model's performance was evaluated by comparing its effects on various aggregation methods and influence factors. In tests using the MovieLen-1M and Book-Crossing datasets, KGLN showed an AUC (Area Under the ROC curve) improvement of 0.3% to 5.9% and 1.1% to 8.2%, respectively, over established benchmark methods like LibFM, D
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21435;&#22122;&#25193;&#25955;&#25512;&#33616;&#27169;&#22411;&#65288;DDRM&#65289;&#65292;&#36890;&#36807;&#22312;&#25512;&#33616;&#27169;&#22411;&#20013;&#27880;&#20837;&#22122;&#22768;&#24182;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#22810;&#27493;&#21435;&#22122;&#36807;&#31243;&#65292;&#22686;&#24378;&#29992;&#25143;&#21644;&#39033;&#30446;&#23884;&#20837;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.06982</link><description>&lt;p&gt;
&#21435;&#22122;&#25193;&#25955;&#25512;&#33616;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Denoising Diffusion Recommender Model. (arXiv:2401.06982v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06982
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21435;&#22122;&#25193;&#25955;&#25512;&#33616;&#27169;&#22411;&#65288;DDRM&#65289;&#65292;&#36890;&#36807;&#22312;&#25512;&#33616;&#27169;&#22411;&#20013;&#27880;&#20837;&#22122;&#22768;&#24182;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#22810;&#27493;&#21435;&#22122;&#36807;&#31243;&#65292;&#22686;&#24378;&#29992;&#25143;&#21644;&#39033;&#30446;&#23884;&#20837;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#36890;&#24120;&#38754;&#20020;&#30528;&#20855;&#26377;&#22122;&#22768;&#30340;&#38544;&#24335;&#21453;&#39304;&#38382;&#39064;&#12290;&#22823;&#22810;&#25968;&#30740;&#31350;&#20174;&#25968;&#25454;&#28165;&#27927;&#30340;&#35282;&#24230;&#32531;&#35299;&#22122;&#22768;&#38382;&#39064;&#65292;&#22914;&#25968;&#25454;&#37325;&#26032;&#37319;&#26679;&#21644;&#37325;&#26032;&#21152;&#26435;&#65292;&#20294;&#23427;&#20204;&#21463;&#21040;&#21551;&#21457;&#24335;&#20551;&#35774;&#30340;&#38480;&#21046;&#12290;&#21478;&#19968;&#31181;&#21435;&#22122;&#26041;&#27861;&#26159;&#20174;&#27169;&#22411;&#30340;&#35282;&#24230;&#65292;&#31215;&#26497;&#22320;&#21521;&#29992;&#25143;-&#39033;&#30446;&#20132;&#20114;&#20013;&#27880;&#20837;&#22122;&#22768;&#65292;&#24182;&#22686;&#24378;&#27169;&#22411;&#30340;&#20869;&#22312;&#21435;&#22122;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#21435;&#22122;&#36807;&#31243;&#23545;&#25512;&#33616;&#27169;&#22411;&#30340;&#34920;&#31034;&#33021;&#21147;&#26469;&#25429;&#25417;&#22122;&#22768;&#27169;&#24335;&#25552;&#20986;&#20102;&#26174;&#33879;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21435;&#22122;&#25193;&#25955;&#25512;&#33616;&#27169;&#22411;&#65288;DDRM&#65289;&#65292;&#23427;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#20351;&#29992;&#22810;&#27493;&#21435;&#22122;&#36807;&#31243;&#26469;&#22686;&#24378;&#26469;&#33258;&#20219;&#20309;&#25512;&#33616;&#27169;&#22411;&#30340;&#29992;&#25143;&#21644;&#39033;&#30446;&#23884;&#20837;&#30340;&#40065;&#26834;&#24615;&#12290;DDRM&#22312;&#21069;&#21521;&#36807;&#31243;&#20013;&#27880;&#20837;&#21463;&#25511;&#39640;&#26031;&#22122;&#22768;&#65292;&#24182;&#22312;&#21453;&#21521;&#21435;&#22122;&#36807;&#31243;&#20013;&#36845;&#20195;&#22320;&#21435;&#38500;&#22122;&#22768;&#65292;&#20174;&#32780;&#25552;&#39640;&#23545;&#22122;&#22768;&#21453;&#39304;&#30340;&#23884;&#20837;&#40065;&#26834;&#24615;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#20851;&#38190;&#22312;&#20110;&#25552;&#20379;&#19968;&#31181;&#33021;&#22815;&#25429;&#25417;&#22122;&#22768;&#27169;&#24335;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#22810;&#27493;&#21435;&#22122;&#36807;&#31243;&#26469;&#25913;&#21892;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommender systems often grapple with noisy implicit feedback. Most studies alleviate the noise issues from data cleaning perspective such as data resampling and reweighting, but they are constrained by heuristic assumptions. Another denoising avenue is from model perspective, which proactively injects noises into user-item interactions and enhance the intrinsic denoising ability of models. However, this kind of denoising process poses significant challenges to the recommender model's representation capacity to capture noise patterns. To address this issue, we propose Denoising Diffusion Recommender Model (DDRM), which leverages multi-step denoising process based on diffusion models to robustify user and item embeddings from any recommender models. DDRM injects controlled Gaussian noises in the forward process and iteratively removes noises in the reverse denoising process, thereby improving embedding robustness against noisy feedback. To achieve this target, the key lies in offering 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#23545;&#22522;&#20110;&#24341;&#29992;&#30340;&#22269;&#23478;&#25490;&#21517;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#21457;&#29616;&#36825;&#20123;&#25490;&#21517;&#21487;&#33021;&#23545;&#26085;&#26412;&#30340;&#31185;&#30740;&#22320;&#20301;&#36827;&#34892;&#38169;&#35823;&#30340;&#20998;&#31867;&#12290;&#30740;&#31350;&#36824;&#21457;&#29616;&#20102;&#24341;&#29992;&#20998;&#24067;&#30340;&#20559;&#31163;&#24773;&#20917;&#65292;&#24182;&#35780;&#20272;&#20102;&#20854;&#23545;&#35780;&#20272;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2312.17560</link><description>&lt;p&gt;
&#19981;&#30830;&#23450;&#24615;&#30340;&#30740;&#31350;&#22269;&#23478;&#25490;&#21517;&#65306;&#25105;&#20204;&#24212;&#35813;&#32487;&#32493;&#21046;&#20316;&#19981;&#30830;&#23450;&#24615;&#25490;&#21517;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Uncertain research country rankings. Should we continue producing uncertain rankings?. (arXiv:2312.17560v2 [cs.DL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.17560
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#23545;&#22522;&#20110;&#24341;&#29992;&#30340;&#22269;&#23478;&#25490;&#21517;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#21457;&#29616;&#36825;&#20123;&#25490;&#21517;&#21487;&#33021;&#23545;&#26085;&#26412;&#30340;&#31185;&#30740;&#22320;&#20301;&#36827;&#34892;&#38169;&#35823;&#30340;&#20998;&#31867;&#12290;&#30740;&#31350;&#36824;&#21457;&#29616;&#20102;&#24341;&#29992;&#20998;&#24067;&#30340;&#20559;&#31163;&#24773;&#20917;&#65292;&#24182;&#35780;&#20272;&#20102;&#20854;&#23545;&#35780;&#20272;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#24341;&#29992;&#30340;&#22269;&#23478;&#25490;&#21517;&#36890;&#24120;&#23558;&#26085;&#26412;&#21010;&#20026;&#21457;&#23637;&#20013;&#22269;&#23478;&#65292;&#29978;&#33267;&#26159;&#22312;&#26368;&#26377;&#22768;&#26395;&#30340;&#26426;&#26500;&#20013;&#20063;&#26159;&#22914;&#27492;&#12290;&#32771;&#34385;&#21040;&#26085;&#26412;&#30340;&#31185;&#30740;&#22320;&#20301;&#25552;&#21319;&#65292;&#36825;&#31181;&#20998;&#31867;&#25361;&#25112;&#20102;&#36825;&#20123;&#25490;&#21517;&#30340;&#21487;&#20449;&#24230;&#12290;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#36825;&#20123;&#25490;&#21517;&#20351;&#29992;&#30334;&#20998;&#20301;&#25351;&#26631;&#65292;&#22914;&#26524;&#22269;&#23478;&#30340;&#24341;&#29992;&#31526;&#21512;&#29702;&#24819;&#30340;&#20998;&#24067;&#27169;&#22411;&#65292;&#37027;&#20040;&#36825;&#20123;&#25490;&#21517;&#26159;&#20934;&#30830;&#30340;&#65292;&#20294;&#22312;&#20559;&#31163;&#30340;&#24773;&#20917;&#19979;&#65292;&#23427;&#20204;&#21487;&#33021;&#20855;&#26377;&#35823;&#23548;&#24615;&#12290;&#29702;&#24819;&#27169;&#22411;&#24847;&#21619;&#30528;&#23545;&#25968;&#27491;&#24577;&#20998;&#24067;&#30340;&#24341;&#29992;&#20998;&#24067;&#21644;&#22522;&#20110;&#24130;&#24459;&#30340;&#24341;&#29992;&#20998;&#24067;&#30340;&#21452;&#37325;&#25490;&#21517;&#65306;&#20840;&#29699;&#21644;&#22269;&#23478;&#25490;&#21517;&#12290;&#26412;&#30740;&#31350;&#23545;&#20559;&#31163;&#29702;&#24819;&#27169;&#22411;&#30340;&#24773;&#20917;&#36827;&#34892;&#20102;&#31995;&#32479;&#24615;&#30340;&#26816;&#26597;&#65292;&#20197;&#21450;&#20854;&#23545;&#35780;&#20272;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#35780;&#20272;&#20102;&#19977;&#20010;&#20855;&#26377;&#31185;&#23398;&#30456;&#20851;&#24615;&#30340;&#20027;&#39064;&#30340;&#20845;&#20010;&#36873;&#23450;&#22269;&#23478;&#65292;&#24182;&#21033;&#29992;&#33713;&#39039;&#25490;&#21517;&#35780;&#20272;&#20102;300&#22810;&#25152;&#22823;&#23398;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#20102;&#23545;&#25968;&#27491;&#24577;&#20998;&#24067;&#30340;&#19977;&#31181;&#20559;&#31163;&#31867;&#22411;&#65306;i &#26497;&#31471;&#19978;&#23614;&#37096;&#30340;&#20559;&#31163;&#65307;ii &#33192;&#32960;&#30340;&#19979;&#23614;&#37096;&#65307;
&lt;/p&gt;
&lt;p&gt;
Citation based country rankings consistently categorize Japan as a developing country, even in those from the most reputed institutions. This categorization challenges the credibility of such rankings, considering Japan elevated scientific standing. In most cases, these rankings use percentile indicators and are accurate if country citations fit an ideal model of distribution, but they can be misleading in cases of deviations. The ideal model implies a lognormal citation distribution and a power law citation based double rank: in the global and country lists. This report conducts a systematic examination of deviations from the ideal model and their consequential impact on evaluations. The study evaluates six selected countries across three scientifically relevant topics and utilizes Leiden Ranking assessments of over 300 universities. The findings reveal three types of deviations from the lognormal citation distribution: i deviations in the extreme upper tail; ii inflated lower tails; 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35843;&#30740;&#30740;&#31350;&#20102;&#36807;&#21435;10&#24180;&#21457;&#34920;&#30340;API&#25512;&#33616;&#30740;&#31350;&#65292;&#36890;&#36807;&#23545;API&#25512;&#33616;&#24037;&#20855;&#30340;&#32467;&#26500;&#12289;&#25968;&#25454;&#26469;&#28304;&#21644;&#25910;&#38598;&#26041;&#27861;&#20197;&#21450;&#24120;&#35265;&#25968;&#25454;&#34920;&#31034;&#31561;&#26041;&#38754;&#30340;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#22235;&#20010;&#20851;&#38190;&#30740;&#31350;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2312.10623</link><description>&lt;p&gt;
&#19968;&#20221;&#20851;&#20110;&#22522;&#20110;&#26597;&#35810;&#30340;API&#25512;&#33616;&#30340;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
A Survey on Query-based API Recommendation. (arXiv:2312.10623v3 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.10623
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#30740;&#30740;&#31350;&#20102;&#36807;&#21435;10&#24180;&#21457;&#34920;&#30340;API&#25512;&#33616;&#30740;&#31350;&#65292;&#36890;&#36807;&#23545;API&#25512;&#33616;&#24037;&#20855;&#30340;&#32467;&#26500;&#12289;&#25968;&#25454;&#26469;&#28304;&#21644;&#25910;&#38598;&#26041;&#27861;&#20197;&#21450;&#24120;&#35265;&#25968;&#25454;&#34920;&#31034;&#31561;&#26041;&#38754;&#30340;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#22235;&#20010;&#20851;&#38190;&#30740;&#31350;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24212;&#29992;&#31243;&#24207;&#25509;&#21475;(APIs)&#26088;&#22312;&#24110;&#21161;&#24320;&#21457;&#20154;&#21592;&#26356;&#26377;&#25928;&#22320;&#26500;&#24314;&#36719;&#20214;&#12290;&#36817;&#24180;&#26469;&#65292;&#20026;&#29305;&#23450;&#20219;&#21153;&#25512;&#33616;&#21512;&#36866;&#30340;APIs&#24050;&#32463;&#24341;&#36215;&#20102;&#30740;&#31350;&#20154;&#21592;&#21644;&#24320;&#21457;&#32773;&#30340;&#36234;&#26469;&#36234;&#22810;&#20851;&#27880;&#12290;&#20026;&#20102;&#20840;&#38754;&#20102;&#35299;&#36825;&#20010;&#30740;&#31350;&#39046;&#22495;&#65292;&#25105;&#20204;&#23545;&#36807;&#21435;10&#24180;&#21457;&#34920;&#30340;API&#25512;&#33616;&#30740;&#31350;&#36827;&#34892;&#20102;&#35843;&#26597;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20174;API&#25512;&#33616;&#24037;&#20855;&#30340;&#32467;&#26500;&#27010;&#36848;&#24320;&#22987;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#20998;&#26512;&#20102;&#20808;&#21069;&#30340;&#30740;&#31350;&#24182;&#25552;&#20986;&#20102;&#22235;&#20010;&#20851;&#38190;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;&#22312;RQ1&#20013;&#65292;&#25105;&#20204;&#26816;&#26597;&#20102;API&#25512;&#33616;&#39046;&#22495;&#20869;&#21457;&#34920;&#30340;&#35770;&#25991;&#25968;&#37327;&#21644;&#21457;&#34920;&#30340;&#20250;&#35758;&#12290;&#22312;RQ2&#20013;&#65292;&#25105;&#20204;&#23545;API&#25512;&#33616;&#30740;&#31350;&#20013;&#24120;&#29992;&#30340;&#25968;&#25454;&#26469;&#28304;&#21644;&#25910;&#38598;&#26041;&#27861;&#36827;&#34892;&#20998;&#31867;&#21644;&#24635;&#32467;&#12290;&#22312;RQ3&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;API&#25512;&#33616;&#26041;&#27861;&#20351;&#29992;&#30340;&#25968;&#25454;&#31867;&#22411;&#21644;&#24120;&#35265;&#30340;&#25968;&#25454;&#34920;&#31034;&#12290;&#25105;&#20204;&#36824;&#35843;&#26597;&#20102;&#20856;&#22411;&#30340;&#25968;&#25454;&#25552;&#21462;&#36807;&#31243;&#21644;&#25910;&#38598;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Application Programming Interfaces (APIs) are designed to help developers build software more effectively. Recommending the right APIs for specific tasks has gained increasing attention among researchers and developers in recent years. To comprehensively understand this research domain, we have surveyed to analyze API recommendation studies published in the last 10 years. Our study begins with an overview of the structure of API recommendation tools. Subsequently, we systematically analyze prior research and pose four key research questions. For RQ1, we examine the volume of published papers and the venues in which these papers appear within the API recommendation field. In RQ2, we categorize and summarize the prevalent data sources and collection methods employed in API recommendation research. In RQ3, we explore the types of data and common data representations utilized by API recommendation approaches. We also investigate the typical data extraction procedures and collection approac
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#23558;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;GCN&#65289;&#19982;&#27880;&#24847;&#21147;&#26426;&#21046;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25216;&#26415;&#26469;&#22686;&#24378;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#65292;&#36890;&#36807;&#26816;&#26597;&#23454;&#20307;&#20043;&#38388;&#21450;&#20854;&#37051;&#23621;&#33410;&#28857;&#20043;&#38388;&#30340;&#20851;&#31995;&#20197;&#21450;&#25972;&#21512;&#23454;&#20307;&#30340;&#23646;&#24615;&#21644;&#30456;&#20114;&#20316;&#29992;&#65292;&#29983;&#25104;&#20016;&#23500;&#30340;&#38544;&#24335;&#29305;&#24449;&#21521;&#37327;&#65292;&#20197;&#25552;&#39640;&#23454;&#20307;&#20998;&#31867;&#21644;&#38142;&#25509;&#39044;&#27979;&#31561;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2312.10049</link><description>&lt;p&gt;
&#22522;&#20110;&#27880;&#24847;&#21147;GCN&#30340;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graph Reasoning Based on Attention GCN. (arXiv:2312.10049v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.10049
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#23558;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;GCN&#65289;&#19982;&#27880;&#24847;&#21147;&#26426;&#21046;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25216;&#26415;&#26469;&#22686;&#24378;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#65292;&#36890;&#36807;&#26816;&#26597;&#23454;&#20307;&#20043;&#38388;&#21450;&#20854;&#37051;&#23621;&#33410;&#28857;&#20043;&#38388;&#30340;&#20851;&#31995;&#20197;&#21450;&#25972;&#21512;&#23454;&#20307;&#30340;&#23646;&#24615;&#21644;&#30456;&#20114;&#20316;&#29992;&#65292;&#29983;&#25104;&#20016;&#23500;&#30340;&#38544;&#24335;&#29305;&#24449;&#21521;&#37327;&#65292;&#20197;&#25552;&#39640;&#23454;&#20307;&#20998;&#31867;&#21644;&#38142;&#25509;&#39044;&#27979;&#31561;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#23558;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;GCN&#65289;&#19982;&#27880;&#24847;&#21147;&#26426;&#21046;&#30456;&#32467;&#21512;&#26469;&#22686;&#24378;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#26816;&#26597;&#23454;&#20307;&#20043;&#38388;&#21450;&#20854;&#37051;&#23621;&#33410;&#28857;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20174;&#32780;&#20026;&#27599;&#20010;&#23454;&#20307;&#24320;&#21457;&#35814;&#32454;&#30340;&#29305;&#24449;&#21521;&#37327;&#12290;GCN&#20351;&#29992;&#20849;&#20139;&#21442;&#25968;&#26377;&#25928;&#22320;&#34920;&#31034;&#30456;&#37051;&#23454;&#20307;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#39318;&#20808;&#23398;&#20064;&#23454;&#20307;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#65292;&#20197;&#36827;&#34892;&#33410;&#28857;&#34920;&#31034;&#23398;&#20064;&#12290;&#36890;&#36807;&#25972;&#21512;&#23454;&#20307;&#30340;&#23646;&#24615;&#21644;&#23427;&#20204;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#35813;&#26041;&#27861;&#20026;&#27599;&#20010;&#23454;&#20307;&#29983;&#25104;&#20102;&#20016;&#23500;&#30340;&#38544;&#24335;&#29305;&#24449;&#21521;&#37327;&#65292;&#25552;&#39640;&#20102;&#23454;&#20307;&#20998;&#31867;&#21644;&#38142;&#25509;&#39044;&#27979;&#31561;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#20248;&#20110;&#20256;&#32479;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#24635;&#20043;&#65292;&#36825;&#39033;&#24037;&#20316;&#20026;&#25628;&#32034;&#24341;&#25806;&#12289;&#38382;&#31572;&#31995;&#32479;&#12289;&#25512;&#33616;&#31995;&#32479;&#21644;&#25968;&#25454;&#25972;&#21512;&#20219;&#21153;&#31561;&#22810;&#20010;&#24212;&#29992;&#39046;&#22495;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#26041;&#27861;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel technique to enhance Knowledge Graph Reasoning by combining Graph Convolution Neural Network (GCN) with the Attention Mechanism. This approach utilizes the Attention Mechanism to examine the relationships between entities and their neighboring nodes, which helps to develop detailed feature vectors for each entity. The GCN uses shared parameters to effectively represent the characteristics of adjacent entities. We first learn the similarity of entities for node representation learning. By integrating the attributes of the entities and their interactions, this method generates extensive implicit feature vectors for each entity, improving performance in tasks including entity classification and link prediction, outperforming traditional neural network models. To conclude, this work provides crucial methodological support for a range of applications, such as search engines, question-answering systems, recommendation systems, and data integration tasks.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25581;&#31034;&#22312;&#26080;&#20559;&#23398;&#20064;&#25490;&#21517;&#20013;&#65292;&#24403;&#28857;&#20987;&#25968;&#25454;&#19981;&#33021;&#23436;&#20840;&#25311;&#21512;&#26102;&#65292;&#26080;&#27861;&#24674;&#22797;&#30495;&#23454;&#30456;&#20851;&#24615;&#65292;&#23548;&#33268;&#25490;&#21517;&#24615;&#33021;&#26174;&#33879;&#38477;&#20302;&#65292;&#25552;&#20986;&#20102;&#21487;&#35782;&#21035;&#24615;&#22270;&#27169;&#22411;&#20316;&#20026;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2309.15560</link><description>&lt;p&gt;
&#35782;&#21035;&#24615;&#24456;&#37325;&#35201;&#65306;&#25581;&#31034;&#26080;&#20559;&#23398;&#20064;&#25490;&#21517;&#20013;&#38544;&#34255;&#30340;&#21487;&#24674;&#22797;&#26465;&#20214;
&lt;/p&gt;
&lt;p&gt;
Identifiability Matters: Revealing the Hidden Recoverable Condition in Unbiased Learning to Rank. (arXiv:2309.15560v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15560
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25581;&#31034;&#22312;&#26080;&#20559;&#23398;&#20064;&#25490;&#21517;&#20013;&#65292;&#24403;&#28857;&#20987;&#25968;&#25454;&#19981;&#33021;&#23436;&#20840;&#25311;&#21512;&#26102;&#65292;&#26080;&#27861;&#24674;&#22797;&#30495;&#23454;&#30456;&#20851;&#24615;&#65292;&#23548;&#33268;&#25490;&#21517;&#24615;&#33021;&#26174;&#33879;&#38477;&#20302;&#65292;&#25552;&#20986;&#20102;&#21487;&#35782;&#21035;&#24615;&#22270;&#27169;&#22411;&#20316;&#20026;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#20559;&#23398;&#20064;&#25490;&#21517;(Unbiased Learning to Rank, ULTR)&#22312;&#20174;&#26377;&#20559;&#28857;&#20987;&#26085;&#24535;&#35757;&#32451;&#26080;&#20559;&#25490;&#21517;&#27169;&#22411;&#30340;&#29616;&#20195;&#31995;&#32479;&#20013;&#34987;&#24191;&#27867;&#24212;&#29992;&#12290;&#20851;&#38190;&#22312;&#20110;&#26126;&#30830;&#22320;&#24314;&#27169;&#29992;&#25143;&#34892;&#20026;&#30340;&#29983;&#25104;&#36807;&#31243;&#65292;&#24182;&#22522;&#20110;&#26816;&#39564;&#20551;&#35774;&#23545;&#28857;&#20987;&#25968;&#25454;&#36827;&#34892;&#25311;&#21512;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#32463;&#39564;&#24615;&#22320;&#21457;&#29616;&#21482;&#35201;&#28857;&#20987;&#23436;&#20840;&#25311;&#21512;&#65292;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#21487;&#20197;&#24674;&#22797;&#20986;&#30495;&#23454;&#28508;&#22312;&#30456;&#20851;&#24615;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35777;&#26126;&#24182;&#38750;&#24635;&#26159;&#33021;&#22815;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#20174;&#32780;&#23548;&#33268;&#25490;&#21517;&#24615;&#33021;&#26174;&#33879;&#38477;&#20302;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#22238;&#31572;&#30495;&#23454;&#30456;&#20851;&#24615;&#26159;&#21542;&#33021;&#22815;&#20174;&#28857;&#20987;&#25968;&#25454;&#24674;&#22797;&#20986;&#26469;&#30340;&#38382;&#39064;&#65292;&#36825;&#26159;ULTR&#39046;&#22495;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#23558;&#19968;&#20010;&#25490;&#21517;&#27169;&#22411;&#23450;&#20041;&#20026;&#21487;&#35782;&#21035;&#30340;&#65292;&#22914;&#26524;&#23427;&#21487;&#20197;&#24674;&#22797;&#20986;&#30495;&#23454;&#30456;&#20851;&#24615;&#65292;&#26368;&#22810;&#21482;&#26377;&#19968;&#20010;&#32553;&#25918;&#21464;&#25442;&#65292;&#36825;&#23545;&#20110;&#25104;&#23545;&#25490;&#21517;&#30446;&#26631;&#26469;&#35828;&#24050;&#36275;&#22815;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19968;&#20010;&#31561;&#20215;&#30340;&#21487;&#35782;&#21035;&#26465;&#20214;&#65292;&#21487;&#20197;&#26032;&#39062;&#22320;&#34920;&#36798;&#20026;&#19968;&#20010;&#22270;&#36830;&#36890;&#24615;&#27979;&#35797;&#38382;&#39064;&#65306;&#24403;&#19988;&#20165;&#24403;&#19968;&#20010;&#22270;&#65288;&#21363;&#21487;&#35782;&#21035;&#24615;&#22270;&#65289;&#36830;&#36890;&#26102;&#65292;&#35813;&#25490;&#21517;&#27169;&#22411;&#26159;&#21487;&#35782;&#21035;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
The application of Unbiased Learning to Rank (ULTR) is widespread in modern systems for training unbiased ranking models from biased click logs. The key is to explicitly model a generation process for user behavior and fit click data based on examination hypothesis. Previous research found empirically that the true latent relevance can be recovered in most cases as long as the clicks are perfectly fitted. However, we demonstrate that this is not always achievable, resulting in a significant reduction in ranking performance. In this work, we aim to answer if or when the true relevance can be recovered from click data, which is a foundation issue for ULTR field. We first define a ranking model as identifiable if it can recover the true relevance up to a scaling transformation, which is enough for pairwise ranking objective. Then we explore an equivalent condition for identifiability that can be novely expressed as a graph connectivity test problem: if and only if a graph (namely identifi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20004;&#38454;&#27573;&#27169;&#24335;&#32467;&#21512;&#39044;&#20808;&#35745;&#31639;&#22270;&#20687;&#23884;&#20837;&#21644;&#21442;&#32771;&#25991;&#26412;-&#20505;&#36873;&#39033;&#19977;&#20803;&#32452;&#20132;&#20114;&#36873;&#25321;&#30340;&#26041;&#24335;&#36827;&#34892;&#32452;&#21512;&#22270;&#20687;&#26816;&#32034;&#20505;&#36873;&#38598;&#37325;&#25490;&#24207;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.16304</link><description>&lt;p&gt;
&#20855;&#26377;&#21452;&#22810;&#27169;&#24577;&#32534;&#30721;&#22120;&#30340;&#32452;&#21512;&#22270;&#20687;&#26816;&#32034;&#20505;&#36873;&#38598;&#37325;&#25490;&#24207;
&lt;/p&gt;
&lt;p&gt;
Candidate Set Re-ranking for Composed Image Retrieval with Dual Multi-modal Encoder. (arXiv:2305.16304v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16304
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20004;&#38454;&#27573;&#27169;&#24335;&#32467;&#21512;&#39044;&#20808;&#35745;&#31639;&#22270;&#20687;&#23884;&#20837;&#21644;&#21442;&#32771;&#25991;&#26412;-&#20505;&#36873;&#39033;&#19977;&#20803;&#32452;&#20132;&#20114;&#36873;&#25321;&#30340;&#26041;&#24335;&#36827;&#34892;&#32452;&#21512;&#22270;&#20687;&#26816;&#32034;&#20505;&#36873;&#38598;&#37325;&#25490;&#24207;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32452;&#21512;&#22270;&#20687;&#26816;&#32034;&#26088;&#22312;&#25214;&#21040;&#26368;&#21305;&#37197;&#32473;&#23450;&#22810;&#27169;&#24577;&#29992;&#25143;&#26597;&#35810;(&#21253;&#25324;&#21442;&#32771;&#22270;&#20687;&#21644;&#25991;&#26412;&#23545;)&#30340;&#22270;&#20687;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#39044;&#20808;&#35745;&#31639;&#25972;&#20010;&#35821;&#26009;&#24211;&#30340;&#22270;&#20687;&#23884;&#20837;&#65292;&#24182;&#22312;&#27979;&#35797;&#26102;&#23558;&#36825;&#20123;&#23884;&#20837;&#19982;&#32463;&#36807;&#26597;&#35810;&#25991;&#26412;&#20462;&#25913;&#30340;&#21442;&#32771;&#22270;&#20687;&#23884;&#20837;&#36827;&#34892;&#27604;&#36739;&#12290;&#28982;&#32780;&#65292;&#20165;&#36890;&#36807;&#30701;&#25991;&#26412;&#25551;&#36848;&#24341;&#23548;&#20462;&#25913;&#21442;&#32771;&#22270;&#20687;&#23884;&#20837;&#21487;&#33021;&#24456;&#22256;&#38590;&#65292;&#29305;&#21035;&#26159;&#29420;&#31435;&#20110;&#28508;&#22312;&#30340;&#20505;&#36873;&#39033;&#12290;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#26159;&#20801;&#35768;&#26597;&#35810;&#21644;&#27599;&#20010;&#21487;&#33021;&#30340;&#20505;&#36873;&#39033;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#21363;&#21442;&#32771;&#25991;&#26412;-&#20505;&#36873;&#39033;&#19977;&#20803;&#32452;&#65292;&#24182;&#20174;&#25972;&#20010;&#38598;&#21512;&#20013;&#36873;&#25321;&#26368;&#20339;&#21305;&#37197;&#12290;&#34429;&#28982;&#36825;&#31181;&#26041;&#27861;&#26356;&#20855;&#26377;&#21028;&#21035;&#24615;&#65292;&#20294;&#23545;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#30001;&#20110;&#19981;&#33021;&#39044;&#20808;&#35745;&#31639;&#20505;&#36873;&#23884;&#20837;&#65292;&#22240;&#27492;&#35745;&#31639;&#25104;&#26412;&#26159;&#31105;&#27490;&#24615;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#20004;&#38454;&#27573;&#27169;&#24335;&#32467;&#21512;&#36825;&#20004;&#20010;&#26041;&#26696;&#30340;&#20248;&#28857;
&lt;/p&gt;
&lt;p&gt;
Composed image retrieval aims to find an image that best matches a given multi-modal user query consisting of a reference image and text pair. Existing methods commonly pre-compute image embeddings over the entire corpus and compare these to a reference image embedding modified by the query text at test time. Such a pipeline is very efficient at test time since fast vector distances can be used to evaluate candidates, but modifying the reference image embedding guided only by a short textual description can be difficult, especially independent of potential candidates. An alternative approach is to allow interactions between the query and every possible candidate, i.e., reference-text-candidate triplets, and pick the best from the entire set. Though this approach is more discriminative, for large-scale datasets the computational cost is prohibitive since pre-computation of candidate embeddings is no longer possible. We propose to combine the merits of both schemes using a two-stage mode
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38754;&#21521;&#20250;&#35805;&#25628;&#32034;&#30340;ConvGQR&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26469;&#37325;&#26032;&#26500;&#36896;&#26597;&#35810;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#22909;&#30340;&#25628;&#32034;&#26597;&#35810;&#12290;</title><link>http://arxiv.org/abs/2305.15645</link><description>&lt;p&gt;
ConvGQR&#65306;&#38754;&#21521;&#20250;&#35805;&#25628;&#32034;&#30340;&#29983;&#25104;&#24335;&#26597;&#35810;&#37325;&#26500;
&lt;/p&gt;
&lt;p&gt;
ConvGQR: Generative Query Reformulation for Conversational Search. (arXiv:2305.15645v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15645
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38754;&#21521;&#20250;&#35805;&#25628;&#32034;&#30340;ConvGQR&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26469;&#37325;&#26032;&#26500;&#36896;&#26597;&#35810;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#22909;&#30340;&#25628;&#32034;&#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20250;&#35805;&#25628;&#32034;&#20013;&#65292;&#29992;&#25143;&#24403;&#21069;&#25628;&#32034;&#24847;&#22270;&#20381;&#36182;&#20110;&#20808;&#21069;&#30340;&#23545;&#35805;&#21382;&#21490;&#12290;&#20174;&#25972;&#20010;&#23545;&#35805;&#19978;&#19979;&#25991;&#20013;&#30830;&#23450;&#19968;&#20010;&#33391;&#22909;&#30340;&#25628;&#32034;&#26597;&#35810;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#36991;&#20813;&#26597;&#35810;&#32534;&#30721;&#22120;&#30340;&#26114;&#36149;&#37325;&#26032;&#35757;&#32451;&#65292;&#22823;&#37096;&#20998;&#29616;&#26377;&#26041;&#27861;&#23581;&#35797;&#23398;&#20064;&#19968;&#20010;&#37325;&#20889;&#27169;&#22411;&#65292;&#36890;&#36807;&#27169;&#20223;&#25163;&#21160;&#26597;&#35810;&#37325;&#20889;&#26469;&#21435;&#38500;&#24403;&#21069;&#26597;&#35810;&#30340;&#19978;&#19979;&#25991;&#12290;&#28982;&#32780;&#65292;&#25163;&#21160;&#37325;&#20889;&#30340;&#26597;&#35810;&#24182;&#19981;&#24635;&#26159;&#26368;&#22909;&#30340;&#25628;&#32034;&#26597;&#35810;&#12290;&#35757;&#32451;&#37325;&#20889;&#27169;&#22411;&#20250;&#38480;&#21046;&#27169;&#22411;&#20135;&#29983;&#33391;&#22909;&#25628;&#32034;&#26597;&#35810;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;ConvGQR&#65292;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#65292;&#19968;&#20010;&#29992;&#20110;&#26597;&#35810;&#37325;&#20889;&#65292;&#21478;&#19968;&#20010;&#29992;&#20110;&#29983;&#25104;&#28508;&#22312;&#31572;&#26696;&#65292;&#20197;&#37325;&#26032;&#26500;&#36896;&#20250;&#35805;&#26597;&#35810;&#12290;&#36890;&#36807;&#32467;&#21512;&#20004;&#32773;&#65292;ConvGQR&#21487;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#25628;&#32034;&#26597;&#35810;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#23558;&#26597;&#35810;&#37325;&#26500;&#19982;&#26816;&#32034;&#24615;&#33021;&#32852;&#31995;&#36215;&#26469;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#36873;&#25321;&#30340;&#30456;&#20284;&#24230;&#20998;&#25968;&#27169;&#22411;&#65292;&#29992;&#20110;&#39564;&#35777;ConvGQR&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In conversational search, the user's real search intent for the current turn is dependent on the previous conversation history. It is challenging to determine a good search query from the whole conversation context. To avoid the expensive re-training of the query encoder, most existing methods try to learn a rewriting model to de-contextualize the current query by mimicking the manual query rewriting. However, manually rewritten queries are not always the best search queries. Training a rewriting model on them would limit the model's ability to produce good search queries. Another useful hint is the potential answer to the question. In this paper, we propose ConvGQR, a new framework to reformulate conversational queries based on generative pre-trained language models (PLMs), one for query rewriting and another for generating potential answers. By combining both, ConvGQR can produce better search queries. In addition, to relate query reformulation to retrieval performance, we propose a 
&lt;/p&gt;</description></item><item><title>SNN&#26159;&#19968;&#31181;&#26032;&#30340;&#22266;&#23450;&#21322;&#24452;&#36817;&#37051;&#25628;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#25490;&#24207;&#21644;&#20351;&#29992;&#39640;&#32423;BLAS&#23454;&#29616;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#26597;&#35810;&#21644;&#32034;&#24341;&#26102;&#38388;&#65292;&#36820;&#22238;&#31934;&#30830;&#32467;&#26524;&#65292;&#24182;&#19988;&#26080;&#38656;&#21442;&#25968;&#35843;&#25972;&#12290;</title><link>http://arxiv.org/abs/2212.07679</link><description>&lt;p&gt;
&#22522;&#20110;&#25490;&#24207;&#30340;&#24555;&#36895;&#20934;&#30830;&#30340;&#22266;&#23450;&#21322;&#24452;&#36817;&#37051;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Fast and exact fixed-radius neighbor search based on sorting. (arXiv:2212.07679v6 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.07679
&lt;/p&gt;
&lt;p&gt;
SNN&#26159;&#19968;&#31181;&#26032;&#30340;&#22266;&#23450;&#21322;&#24452;&#36817;&#37051;&#25628;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#25490;&#24207;&#21644;&#20351;&#29992;&#39640;&#32423;BLAS&#23454;&#29616;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#26597;&#35810;&#21644;&#32034;&#24341;&#26102;&#38388;&#65292;&#36820;&#22238;&#31934;&#30830;&#32467;&#26524;&#65292;&#24182;&#19988;&#26080;&#38656;&#21442;&#25968;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22266;&#23450;&#21322;&#24452;&#36817;&#37051;&#25628;&#32034;&#26159;&#19968;&#31181;&#22522;&#26412;&#30340;&#25968;&#25454;&#25805;&#20316;&#65292;&#29992;&#20110;&#26816;&#32034;&#21040;&#26597;&#35810;&#28857;&#22312;&#29992;&#25143;&#25351;&#23450;&#36317;&#31163;&#20869;&#30340;&#25152;&#26377;&#25968;&#25454;&#28857;&#12290;&#30446;&#21069;&#23384;&#22312;&#19968;&#20123;&#39640;&#25928;&#30340;&#31639;&#27861;&#21487;&#20197;&#25552;&#20379;&#24555;&#36895;&#30340;&#36817;&#20284;&#26597;&#35810;&#21709;&#24212;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#20855;&#26377;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;&#32034;&#24341;&#38454;&#27573;&#65292;&#24182;&#19988;&#38656;&#35201;&#20180;&#32454;&#30340;&#21442;&#25968;&#35843;&#25972;&#12290;&#22240;&#27492;&#65292;&#31934;&#30830;&#30340;&#26292;&#21147;&#25628;&#32034;&#21644;&#22522;&#20110;&#26641;&#30340;&#25628;&#32034;&#26041;&#27861;&#20173;&#28982;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22266;&#23450;&#21322;&#24452;&#36817;&#37051;&#25628;&#32034;&#26041;&#27861;SNN&#65292;&#22312;&#32034;&#24341;&#21644;&#26597;&#35810;&#26102;&#38388;&#26041;&#38754;&#26174;&#33879;&#25913;&#36827;&#20102;&#26292;&#21147;&#25628;&#32034;&#21644;&#22522;&#20110;&#26641;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#20934;&#30830;&#36820;&#22238;&#32467;&#26524;&#65292;&#24182;&#19988;&#26080;&#38656;&#21442;&#25968;&#35843;&#25972;&#12290;SNN&#21033;&#29992;&#25968;&#25454;&#28857;&#26681;&#25454;&#20854;&#31532;&#19968;&#20027;&#25104;&#20998;&#36827;&#34892;&#25490;&#24207;&#65292;&#20174;&#32780;&#20462;&#21098;&#26597;&#35810;&#25628;&#32034;&#31354;&#38388;&#12290;&#36890;&#36807;&#20351;&#29992;&#39640;&#32423;&#30340;&#22522;&#30784;&#32447;&#24615;&#20195;&#25968;&#23376;&#31243;&#24207;&#65288;BLAS&#65289;&#30340;&#39640;&#25928;&#23454;&#29616;&#65292;&#36827;&#19968;&#27493;&#25552;&#36895;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#35813;&#26041;&#27861;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22312;&#29420;&#31435;&#20351;&#29992;&#21644;&#19982;&#20854;&#20182;&#26041;&#27861;&#32467;&#21512;&#20351;&#29992;&#26102;&#30340;&#23454;&#38469;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fixed-radius near neighbor search is a fundamental data operation that retrieves all data points within a user-specified distance to a query point. There are efficient algorithms that can provide fast approximate query responses, but they often have a very compute-intensive indexing phase and require careful parameter tuning. Therefore, exact brute force and tree-based search methods are still widely used. Here we propose a new fixed-radius near neighbor search method, called SNN, that significantly improves over brute force and tree-based methods in terms of index and query time, provably returns exact results, and requires no parameter tuning. SNN exploits a sorting of the data points by their first principal component to prune the query search space. Further speedup is gained from an efficient implementation using high-level Basic Linear Algebra Subprograms (BLAS). We provide theoretical analysis of our method and demonstrate its practical performance when used stand-alone and when 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#22411;&#25361;&#25112;&#20219;&#21153;&#65292;&#21363;&#36890;&#36807;&#36828;&#31243;&#30417;&#30563;&#26041;&#24335;&#20174;&#31185;&#23398;&#25991;&#31456;&#20013;&#30340;&#34920;&#26684;&#20013;&#25552;&#21462;&#26377;&#20851;&#26448;&#26009;&#32452;&#25104;&#30340;&#20449;&#24687;&#12290;&#20026;&#27492;&#65292;&#30740;&#31350;&#32773;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;4408&#20010;&#36828;&#31243;&#30417;&#30563;&#34920;&#26684;&#21644;1475&#20010;&#25163;&#21160;&#27880;&#37322;&#30340;&#24320;&#21457;&#21644;&#27979;&#35797;&#34920;&#26684;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#24378;&#22522;&#32447;&#8212;&#8212;DiSCoMaT&#12290;</title><link>http://arxiv.org/abs/2207.01079</link><description>&lt;p&gt;
DiSCoMaT&#65306;&#26448;&#26009;&#31185;&#23398;&#25991;&#31456;&#20013;&#22522;&#20110;&#36828;&#31243;&#30417;&#30563;&#30340;&#34920;&#26684;&#32452;&#25104;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
DiSCoMaT: Distantly Supervised Composition Extraction from Tables in Materials Science Articles. (arXiv:2207.01079v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.01079
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#22411;&#25361;&#25112;&#20219;&#21153;&#65292;&#21363;&#36890;&#36807;&#36828;&#31243;&#30417;&#30563;&#26041;&#24335;&#20174;&#31185;&#23398;&#25991;&#31456;&#20013;&#30340;&#34920;&#26684;&#20013;&#25552;&#21462;&#26377;&#20851;&#26448;&#26009;&#32452;&#25104;&#30340;&#20449;&#24687;&#12290;&#20026;&#27492;&#65292;&#30740;&#31350;&#32773;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;4408&#20010;&#36828;&#31243;&#30417;&#30563;&#34920;&#26684;&#21644;1475&#20010;&#25163;&#21160;&#27880;&#37322;&#30340;&#24320;&#21457;&#21644;&#27979;&#35797;&#34920;&#26684;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#24378;&#22522;&#32447;&#8212;&#8212;DiSCoMaT&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#31185;&#23398;&#39046;&#22495;&#25991;&#31456;&#20013;&#30340;&#34920;&#26684;&#20013;&#25552;&#21462;&#26377;&#20851;&#26448;&#26009;&#32452;&#25104;&#30340;&#20449;&#24687;&#26159;&#30693;&#35782;&#24211;&#31574;&#21010;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#34920;&#26684;&#25552;&#21462;&#22120;&#20551;&#23450;&#24744;&#24050;&#32463;&#20102;&#35299;&#34920;&#26684;&#32467;&#26500;&#21644;&#26684;&#24335;&#65292;&#32780;&#31185;&#23398;&#34920;&#26684;&#20013;&#21487;&#33021;&#27809;&#26377;&#36825;&#20123;&#20808;&#21069;&#30340;&#30693;&#35782;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#29305;&#23450;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#34920;&#26684;&#25552;&#21462;&#38382;&#39064;&#65306;&#25552;&#21462;&#26448;&#26009;&#65288;&#20363;&#22914;&#29627;&#29827;&#65292;&#21512;&#37329;&#65289;&#30340;&#32452;&#25104;&#12290;&#25105;&#20204;&#39318;&#20808;&#35266;&#23519;&#21040;&#26448;&#26009;&#31185;&#23398;&#30740;&#31350;&#20154;&#21592;&#20351;&#29992;&#21508;&#31181;&#34920;&#26684;&#26679;&#24335;&#32452;&#32455;&#31867;&#20284;&#30340;&#32452;&#25104;&#65292;&#36825;&#38656;&#35201;&#19968;&#20010;&#26234;&#33021;&#27169;&#22411;&#26469;&#29702;&#35299;&#34920;&#26684;&#21644;&#25552;&#21462;&#32452;&#25104;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23558;&#20854;&#23450;&#20041;&#20026;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#26032;&#22411;&#25361;&#25112;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#30001;4408&#20010;&#36828;&#31243;&#30417;&#30563;&#34920;&#26684;&#21644;1475&#20010;&#25163;&#21160;&#27880;&#37322;&#30340;&#24320;&#21457;&#21644;&#27979;&#35797;&#34920;&#26684;&#32452;&#25104;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;DiSCoMaT&#65292;&#23427;&#26159;&#19968;&#20010;&#38024;&#23545;&#35813;&#38382;&#39064;&#30340;&#24378;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
A crucial component in the curation of KB for a scientific domain is information extraction from tables in the domain's published articles -- tables carry important information (often numeric), which must be adequately extracted for a comprehensive machine understanding of an article. Existing table extractors assume prior knowledge of table structure and format, which may not be known in scientific tables. We study a specific and challenging table extraction problem: extracting compositions of materials (e.g., glasses, alloys). We first observe that materials science researchers organize similar compositions in a wide variety of table styles, necessitating an intelligent model for table understanding and composition extraction. Consequently, we define this novel task as a challenge for the ML community and create a training dataset comprising 4,408 distantly supervised tables, along with 1,475 manually annotated dev and test tables. We also present DiSCoMaT, a strong baseline geared t
&lt;/p&gt;</description></item></channel></rss>