<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#36923;&#36753;&#31163;&#25955;&#22270;&#24418;&#27169;&#22411;&#34987;&#35748;&#20026;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26080;&#27861;&#35299;&#20915;&#30340;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#33021;&#22815;&#35299;&#20915;&#24187;&#35273;&#12289;&#22797;&#26434;&#25512;&#29702;&#12289;&#19981;&#30830;&#23450;&#24615;&#19979;&#30340;&#35268;&#21010;&#21644;&#22797;&#26434;&#35745;&#31639;&#31561;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.09599</link><description>&lt;p&gt;
&#36923;&#36753;&#31163;&#25955;&#22270;&#24418;&#27169;&#22411;&#24517;&#39035;&#20026;&#20449;&#24687;&#32508;&#21512;&#34917;&#20805;&#22823;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Logical Discrete Graphical Models Must Supplement Large Language Models for Information Synthesis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09599
&lt;/p&gt;
&lt;p&gt;
&#36923;&#36753;&#31163;&#25955;&#22270;&#24418;&#27169;&#22411;&#34987;&#35748;&#20026;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26080;&#27861;&#35299;&#20915;&#30340;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#33021;&#22815;&#35299;&#20915;&#24187;&#35273;&#12289;&#22797;&#26434;&#25512;&#29702;&#12289;&#19981;&#30830;&#23450;&#24615;&#19979;&#30340;&#35268;&#21010;&#21644;&#22797;&#26434;&#35745;&#31639;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#20852;&#25512;&#29702;&#33021;&#21147;&#65292;&#20449;&#24687;&#26816;&#32034;&#21464;&#24471;&#26356;&#21152;&#22797;&#26434;&#12290;&#29616;&#20195;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#23459;&#31216;&#20182;&#20204;&#21487;&#20197;&#22522;&#20110;&#28508;&#22312;&#30340;&#22810;&#20010;&#19981;&#21516;&#25991;&#26723;&#12289;&#20914;&#31361;&#30340;&#25968;&#25454;&#26469;&#28304;&#21644;&#25512;&#29702;&#26469;&#32508;&#21512;&#29983;&#25104;&#31572;&#26696;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#26816;&#32034;&#25991;&#26723;&#12290;&#25105;&#20204;&#23457;&#26597;&#20102;&#26368;&#36817;&#30340;&#25991;&#29486;&#65292;&#24182;&#35748;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26377;&#20851;&#38190;&#32570;&#38519;&#65292;&#20351;&#20854;&#19981;&#21487;&#33021;&#29420;&#31435;&#26500;&#25104;&#36890;&#29992;&#26234;&#33021;&#65292;&#25110;&#22238;&#31572;&#19968;&#33324;&#20449;&#24687;&#32508;&#21512;&#35831;&#27714;&#12290;&#36825;&#39033;&#23457;&#26597;&#26174;&#31034;&#22823;&#35821;&#35328;&#27169;&#22411;&#23384;&#22312;&#20197;&#19979;&#38382;&#39064;&#65306;&#24187;&#35273;&#12289;&#22797;&#26434;&#25512;&#29702;&#12289;&#19981;&#30830;&#23450;&#24615;&#19979;&#30340;&#35268;&#21010;&#21644;&#22797;&#26434;&#35745;&#31639;&#12290;&#25105;&#20204;&#27010;&#36848;&#20102;&#36923;&#36753;&#31163;&#25955;&#22270;&#24418;&#27169;&#22411;&#22914;&#20309;&#35299;&#20915;&#25152;&#26377;&#36825;&#20123;&#38382;&#39064;&#65292;&#24182;&#27010;&#36848;&#20102;&#20174;&#26080;&#26631;&#31614;&#25991;&#26412;&#35757;&#32451;&#36923;&#36753;&#31163;&#25955;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09599v1 Announce Type: new  Abstract: Given the emergent reasoning abilities of large language models, information retrieval is becoming more complex. Rather than just retrieve a document, modern information retrieval systems advertise that they can synthesize an answer based on potentially many different documents, conflicting data sources, and using reasoning. We review recent literature and argue that the large language model has crucial flaws that prevent it from on its own ever constituting general intelligence, or answering general information synthesis requests. This review shows that the following are problems for large language models: hallucinations, complex reasoning, planning under uncertainty, and complex calculations. We outline how logical discrete graphical models can solve all of these problems, and outline a method of training a logical discrete model from unlabeled text.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#27468;&#21809;&#35821;&#38899;&#35782;&#21035;&#20013;&#30340;&#25361;&#25112;&#21644;&#36827;&#23637;&#65292;&#25506;&#32034;&#20102;&#38899;&#32032;&#35782;&#21035;&#12289;&#27468;&#26354;&#20013;&#30340;&#35821;&#35328;&#35782;&#21035;&#12289;&#20851;&#38190;&#35789;&#35782;&#21035;&#21644;&#23436;&#25972;&#27468;&#35789;&#36716;&#24405;&#31561;&#20851;&#38190;&#39046;&#22495;&#65292;&#24182;&#20171;&#32461;&#20102;&#28145;&#24230;&#23398;&#20064;&#21644;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#22312;&#35813;&#39046;&#22495;&#25512;&#21160;&#36827;&#23637;&#30340;&#26368;&#26032;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2403.09298</link><description>&lt;p&gt;
&#36229;&#36234;&#35328;&#35821;&#65306;&#27468;&#21809;&#35821;&#38899;&#35782;&#21035;&#20013;&#30340;&#36827;&#23637;&#19982;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
More than words: Advancements and challenges in speech recognition for singing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09298
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#27468;&#21809;&#35821;&#38899;&#35782;&#21035;&#20013;&#30340;&#25361;&#25112;&#21644;&#36827;&#23637;&#65292;&#25506;&#32034;&#20102;&#38899;&#32032;&#35782;&#21035;&#12289;&#27468;&#26354;&#20013;&#30340;&#35821;&#35328;&#35782;&#21035;&#12289;&#20851;&#38190;&#35789;&#35782;&#21035;&#21644;&#23436;&#25972;&#27468;&#35789;&#36716;&#24405;&#31561;&#20851;&#38190;&#39046;&#22495;&#65292;&#24182;&#20171;&#32461;&#20102;&#28145;&#24230;&#23398;&#20064;&#21644;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#22312;&#35813;&#39046;&#22495;&#25512;&#21160;&#36827;&#23637;&#30340;&#26368;&#26032;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#27468;&#21809;&#35821;&#38899;&#35782;&#21035;&#20013;&#30340;&#25361;&#25112;&#21644;&#36827;&#23637;&#65292;&#36825;&#26159;&#19968;&#20010;&#19982;&#26631;&#20934;&#35821;&#38899;&#35782;&#21035;&#23436;&#20840;&#19981;&#21516;&#30340;&#39046;&#22495;&#12290;&#27468;&#21809;&#21253;&#21547;&#29420;&#29305;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#24191;&#27867;&#30340;&#38899;&#39640;&#21464;&#21270;&#12289;&#22810;&#26679;&#21270;&#30340;&#22768;&#20048;&#39118;&#26684;&#20197;&#21450;&#32972;&#26223;&#38899;&#20048;&#24178;&#25200;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#35832;&#22914;&#38899;&#32032;&#35782;&#21035;&#12289;&#27468;&#26354;&#20013;&#30340;&#35821;&#35328;&#35782;&#21035;&#12289;&#20851;&#38190;&#35789;&#35782;&#21035;&#21644;&#23436;&#25972;&#27468;&#35789;&#36716;&#24405;&#31561;&#20851;&#38190;&#39046;&#22495;&#12290;&#25105;&#23558;&#25551;&#36848;&#19968;&#20123;&#25105;&#22312;&#36825;&#20123;&#20219;&#21153;&#19978;&#36827;&#34892;&#30740;&#31350;&#26102;&#30340;&#32463;&#21382;&#65292;&#23601;&#22312;&#23427;&#20204;&#24320;&#22987;&#23853;&#38706;&#22836;&#35282;&#30340;&#26102;&#20505;&#65292;&#20294;&#20063;&#20250;&#23637;&#31034;&#28145;&#24230;&#23398;&#20064;&#21644;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#26368;&#26032;&#36827;&#23637;&#22914;&#20309;&#25512;&#21160;&#20102;&#36825;&#19968;&#39046;&#22495;&#30340;&#36827;&#27493;&#12290;&#25105;&#30340;&#30446;&#26631;&#26159;&#38416;&#26126;&#23558;&#35821;&#38899;&#35782;&#21035;&#24212;&#29992;&#20110;&#27468;&#21809;&#26102;&#30340;&#22797;&#26434;&#24615;&#65292;&#35780;&#20272;&#24403;&#21069;&#30340;&#33021;&#21147;&#65292;&#24182;&#27010;&#36848;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09298v1 Announce Type: cross  Abstract: This paper addresses the challenges and advancements in speech recognition for singing, a domain distinctly different from standard speech recognition. Singing encompasses unique challenges, including extensive pitch variations, diverse vocal styles, and background music interference. We explore key areas such as phoneme recognition, language identification in songs, keyword spotting, and full lyrics transcription. I will describe some of my own experiences when performing research on these tasks just as they were starting to gain traction, but will also show how recent developments in deep learning and large-scale datasets have propelled progress in this field. My goal is to illuminate the complexities of applying speech recognition to singing, evaluate current capabilities, and outline future research directions.
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#27604;&#36739;&#20102;&#22522;&#20110;&#31181;&#23376;&#30340;&#30740;&#31350;&#20986;&#29256;&#29289;&#32593;&#32476;&#20449;&#24687;&#26816;&#32034;&#20013;&#30340;&#30452;&#25509;&#24341;&#29992;&#12289;&#25991;&#29486;&#32806;&#21512;&#12289;&#20849;&#21516;&#24341;&#29992;&#21644;PubMed&#30456;&#20851;&#25991;&#31456;&#24471;&#20998;&#31561;&#19981;&#21516;&#26041;&#27861;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#32452;&#21512;&#24341;&#25991;&#26041;&#27861;&#20248;&#20110;&#20165;&#20351;&#29992;&#20849;&#21516;&#24341;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.09295</link><description>&lt;p&gt;
&#22522;&#20110;&#31181;&#23376;&#30340;&#30740;&#31350;&#20986;&#29256;&#29289;&#32593;&#32476;&#20449;&#24687;&#26816;&#32034;&#65306;&#23545;&#30452;&#25509;&#24341;&#29992;&#12289;&#25991;&#29486;&#32806;&#21512;&#12289;&#20849;&#21516;&#24341;&#29992;&#21644;PubMed&#30456;&#20851;&#25991;&#31456;&#24471;&#20998;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Seed-based information retrieval in networks of research publications: Evaluation of direct citations, bibliographic coupling, co-citations and PubMed related article score
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09295
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#27604;&#36739;&#20102;&#22522;&#20110;&#31181;&#23376;&#30340;&#30740;&#31350;&#20986;&#29256;&#29289;&#32593;&#32476;&#20449;&#24687;&#26816;&#32034;&#20013;&#30340;&#30452;&#25509;&#24341;&#29992;&#12289;&#25991;&#29486;&#32806;&#21512;&#12289;&#20849;&#21516;&#24341;&#29992;&#21644;PubMed&#30456;&#20851;&#25991;&#31456;&#24471;&#20998;&#31561;&#19981;&#21516;&#26041;&#27861;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#32452;&#21512;&#24341;&#25991;&#26041;&#27861;&#20248;&#20110;&#20165;&#20351;&#29992;&#20849;&#21516;&#24341;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22522;&#20110;&#31181;&#23376;&#30340;&#30740;&#31350;&#20986;&#29256;&#29289;&#32593;&#32476;&#20449;&#24687;&#26816;&#32034;&#12290;&#20351;&#29992;&#31995;&#32479;&#35780;&#23457;&#20316;&#20026;&#22522;&#20934;&#65292;&#32467;&#21512;NIH&#24320;&#25918;&#24341;&#25991;&#25910;&#38598;&#30340;&#20986;&#29256;&#25968;&#25454;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#19977;&#31181;&#22522;&#20110;&#24341;&#25991;&#30340;&#26041;&#27861;&#8212;&#8212;&#30452;&#25509;&#24341;&#29992;&#12289;&#20849;&#21516;&#24341;&#29992;&#21644;&#25991;&#29486;&#32806;&#21512;&#22312;&#21484;&#22238;&#29575;&#21644;&#31934;&#30830;&#29575;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23558;PubMed&#30456;&#20851;&#25991;&#31456;&#24471;&#20998;&#20197;&#21450;&#32452;&#21512;&#26041;&#27861;&#32435;&#20837;&#27604;&#36739;&#12290;&#25105;&#20204;&#36824;&#23545;&#20808;&#21069;&#20351;&#29992;&#24341;&#25991;&#20851;&#31995;&#36827;&#34892;&#20449;&#24687;&#26816;&#32034;&#30340;&#26089;&#26399;&#30740;&#31350;&#36827;&#34892;&#20102;&#30456;&#24403;&#20840;&#38754;&#30340;&#22238;&#39038;&#12290;&#32467;&#26524;&#26174;&#31034;&#20849;&#21516;&#24341;&#29992;&#20248;&#20110;&#25991;&#29486;&#32806;&#21512;&#21644;&#30452;&#25509;&#24341;&#29992;&#12290;&#28982;&#32780;&#65292;&#22312;&#30740;&#31350;&#20013;&#65292;&#23558;&#36825;&#19977;&#31181;&#26041;&#27861;&#32452;&#21512;&#36215;&#26469;&#32988;&#36807;&#20165;&#20351;&#29992;&#20849;&#21516;&#24341;&#29992;&#12290;&#32467;&#26524;&#36827;&#19968;&#27493;&#34920;&#26126;&#65292;&#19982;&#20808;&#21069;&#30740;&#31350;&#19968;&#33268;&#65292;&#23558;&#22522;&#20110;&#24341;&#25991;&#30340;&#26041;&#27861;&#19982;&#25991;&#26412;&#26041;&#27861;&#30456;&#32467;&#21512;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09295v1 Announce Type: new  Abstract: In this contribution, we deal with seed-based information retrieval in networks of research publications. Using systematic reviews as a baseline, and publication data from the NIH Open Citation Collection, we compare the performance of the three citation-based approaches direct citation, co-citation, and bibliographic coupling with respect to recall and precision measures. In addition, we include the PubMed Related Article score as well as combined approaches in the comparison. We also provide a fairly comprehensive review of earlier research in which citation relations have been used for information retrieval purposes. The results show an advantage for co-citation over bibliographic coupling and direct citation. However, combining the three approaches outperforms the exclusive use of co-citation in the study. The results further indicate, in line with previous research, that combining citation-based approaches with textual approaches en
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#35843;&#26597;&#25628;&#32034;&#28548;&#28165;&#20013;&#22312;&#32447;&#21644;&#31163;&#32447;&#35780;&#20272;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24773;&#20917;&#65292;&#20197;&#29992;&#25143;&#21442;&#19982;&#24230;&#20316;&#20026;&#30495;&#23454;&#24773;&#20917;&#65292;&#25506;&#35752;&#31163;&#32447;&#25490;&#21517;&#21015;&#34920;&#22914;&#20309;&#19982;&#22522;&#20110;&#22312;&#32447;&#29992;&#25143;&#21442;&#19982;&#24230;&#30340;&#29702;&#24819;&#25490;&#21517;&#21015;&#34920;&#30456;&#20284;&#12290;</title><link>https://arxiv.org/abs/2403.09180</link><description>&lt;p&gt;
&#25628;&#32034;&#28548;&#28165;&#20013;&#30340;&#22312;&#32447;&#21644;&#31163;&#32447;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Online and Offline Evaluation in Search Clarification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09180
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#35843;&#26597;&#25628;&#32034;&#28548;&#28165;&#20013;&#22312;&#32447;&#21644;&#31163;&#32447;&#35780;&#20272;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24773;&#20917;&#65292;&#20197;&#29992;&#25143;&#21442;&#19982;&#24230;&#20316;&#20026;&#30495;&#23454;&#24773;&#20917;&#65292;&#25506;&#35752;&#31163;&#32447;&#25490;&#21517;&#21015;&#34920;&#22914;&#20309;&#19982;&#22522;&#20110;&#22312;&#32447;&#29992;&#25143;&#21442;&#19982;&#24230;&#30340;&#29702;&#24819;&#25490;&#21517;&#21015;&#34920;&#30456;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#22312;&#25628;&#32034;&#31995;&#32479;&#20013;&#65292;&#28548;&#28165;&#38382;&#39064;&#27169;&#22411;&#22312;&#21560;&#24341;&#29992;&#25143;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#21463;&#21040;&#38480;&#21046;&#65292;&#23545;&#20854;&#25972;&#20307;&#26377;&#29992;&#24615;&#20135;&#29983;&#20102;&#24576;&#30097;&#12290;&#35201;&#25913;&#21892;&#36825;&#20123;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#20851;&#38190;&#26159;&#37319;&#29992;&#26082;&#21253;&#25324;&#26469;&#33258;&#29992;&#25143;&#30340;&#23454;&#26102;&#21453;&#39304;&#65288;&#22312;&#32447;&#35780;&#20272;&#65289;&#65292;&#21448;&#36890;&#36807;&#20154;&#24037;&#35780;&#20272;&#35780;&#20272;&#28548;&#28165;&#38382;&#39064;&#29305;&#24449;&#30340;&#35780;&#20272;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22312;&#20449;&#24687;&#26816;&#32034;&#39046;&#22495;&#65292;&#20851;&#20110;&#22312;&#32447;&#21644;&#31163;&#32447;&#35780;&#20272;&#20043;&#38388;&#30340;&#20851;&#31995;&#23384;&#22312;&#20105;&#35758;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#35843;&#26597;&#36825;&#31181;&#19981;&#19968;&#33268;&#22312;&#25628;&#32034;&#28548;&#28165;&#20013;&#30340;&#25345;&#32493;&#24773;&#20917;&#12290;&#25105;&#20204;&#20197;&#29992;&#25143;&#21442;&#19982;&#24230;&#20316;&#20026;&#22522;&#26412;&#20107;&#23454;&#65292;&#24182;&#20351;&#29992;&#22810;&#20010;&#31163;&#32447;&#26631;&#31614;&#26469;&#35843;&#26597;&#31163;&#32447;&#25490;&#21517;&#30340;&#28548;&#28165;&#38382;&#39064;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#31867;&#20284;&#20110;&#22522;&#20110;&#22312;&#32447;&#29992;&#25143;&#21442;&#19982;&#24230;&#30340;&#29702;&#24819;&#25490;&#21517;&#21015;&#34920;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09180v1 Announce Type: new  Abstract: The effectiveness of clarification question models in engaging users within search systems is currently constrained, casting doubt on their overall usefulness. To improve the performance of these models, it is crucial to employ assessment approaches that encompass both real-time feedback from users (online evaluation) and the characteristics of clarification questions evaluated through human assessment (offline evaluation). However, the relationship between online and offline evaluations has been debated in information retrieval. This study aims to investigate how this discordance holds in search clarification. We use user engagement as ground truth and employ several offline labels to investigate to what extent the offline ranked lists of clarification resemble the ideal ranked lists based on online user engagement.
&lt;/p&gt;</description></item><item><title>&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26174;&#33879;&#20154;&#29289;&#20013;&#23384;&#22312;&#30340;&#24615;&#21035;&#24046;&#36317;&#65292;&#24182;&#21457;&#29616;GPT-4&#22312;&#24615;&#33021;&#19978;&#26377;&#25152;&#25913;&#36827;&#65292;&#20294;&#38382;&#39064;&#23578;&#26410;&#23436;&#20840;&#35299;&#20915;</title><link>https://arxiv.org/abs/2403.09148</link><description>&lt;p&gt;
&#35780;&#20272;&#29992;&#20110;&#26174;&#33879;&#20154;&#29289;&#30340;LLMs&#20013;&#30340;&#24615;&#21035;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Evaluating LLMs for Gender Disparities in Notable Persons
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09148
&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26174;&#33879;&#20154;&#29289;&#20013;&#23384;&#22312;&#30340;&#24615;&#21035;&#24046;&#36317;&#65292;&#24182;&#21457;&#29616;GPT-4&#22312;&#24615;&#33021;&#19978;&#26377;&#25152;&#25913;&#36827;&#65292;&#20294;&#38382;&#39064;&#23578;&#26410;&#23436;&#20840;&#35299;&#20915;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29992;&#20110;&#26816;&#32034;&#20107;&#23454;&#20449;&#24687;&#30340;&#20351;&#29992;&#65292;&#35299;&#20915;&#20102;&#23427;&#20204;&#20135;&#29983;&#20107;&#23454;&#19981;&#20934;&#30830;&#30340;&#8220;&#24187;&#35273;&#8221;&#22238;&#22797;&#25110;&#23436;&#20840;&#25298;&#32477;&#29978;&#33267;&#22238;&#31572;&#25552;&#31034;&#30340;&#20542;&#21521;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#23427;&#35843;&#26597;&#20102;LLMs&#23545;&#20107;&#23454;&#26597;&#35810;&#30340;&#22238;&#24212;&#20013;&#23384;&#22312;&#30340;&#22522;&#20110;&#24615;&#21035;&#30340;&#20559;&#35265;&#12290;&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#35780;&#20272;GPT&#27169;&#22411;&#22312;&#21484;&#22238;&#12289;&#24187;&#35273;&#21644;&#25298;&#32477;&#31561;&#22810;&#20010;&#32500;&#24230;&#19978;&#30340;&#20844;&#24179;&#24615;&#26469;&#37319;&#29992;&#22810;&#31649;&#40784;&#19979;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;GPT-3.5&#29983;&#25104;&#30340;&#22238;&#24212;&#20013;&#23384;&#22312;&#26126;&#26174;&#30340;&#24615;&#21035;&#24046;&#36317;&#12290;&#34429;&#28982;GPT-4&#30340;&#36827;&#23637;&#25552;&#21319;&#20102;&#24615;&#33021;&#65292;&#20294;&#22312;&#22238;&#24212;&#34987;&#25298;&#32477;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#20123;&#24615;&#21035;&#24046;&#36317;&#24182;&#26410;&#23436;&#20840;&#28040;&#38500;&#12290;&#30740;&#31350;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;&#36825;&#20123;&#24046;&#36317;&#30340;&#36215;&#28304;&#65292;&#36890;&#36807;&#26816;&#26597;&#25552;&#31034;&#20013;&#30340;&#24615;&#21035;&#20851;&#32852;&#21644;&#22238;&#24212;&#20013;&#30340;&#21516;&#36136;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09148v1 Announce Type: new  Abstract: This study examines the use of Large Language Models (LLMs) for retrieving factual information, addressing concerns over their propensity to produce factually incorrect "hallucinated" responses or to altogether decline to even answer prompt at all. Specifically, it investigates the presence of gender-based biases in LLMs' responses to factual inquiries. This paper takes a multi-pronged approach to evaluating GPT models by evaluating fairness across multiple dimensions of recall, hallucinations and declinations. Our findings reveal discernible gender disparities in the responses generated by GPT-3.5. While advancements in GPT-4 have led to improvements in performance, they have not fully eradicated these gender disparities, notably in instances where responses are declined. The study further explores the origins of these disparities by examining the influence of gender associations in prompts and the homogeneity in the responses.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29992;&#25143;&#25628;&#32034;&#34892;&#20026;&#27169;&#25311;&#22120; USimAgent&#65292;&#21487;&#20197;&#27169;&#25311;&#29992;&#25143;&#22312;&#25628;&#32034;&#36807;&#31243;&#20013;&#30340;&#26597;&#35810;&#12289;&#28857;&#20987;&#21644;&#20572;&#27490;&#34892;&#20026;&#65292;&#23454;&#29616;&#29983;&#25104;&#29305;&#23450;&#25628;&#32034;&#30340;&#23436;&#25972;&#25628;&#32034;&#20250;&#35805;&#12290;</title><link>https://arxiv.org/abs/2403.09142</link><description>&lt;p&gt;
USimAgent&#65306;&#29992;&#20110;&#27169;&#25311;&#25628;&#32034;&#29992;&#25143;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
USimAgent: Large Language Models for Simulating Search Users
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09142
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29992;&#25143;&#25628;&#32034;&#34892;&#20026;&#27169;&#25311;&#22120; USimAgent&#65292;&#21487;&#20197;&#27169;&#25311;&#29992;&#25143;&#22312;&#25628;&#32034;&#36807;&#31243;&#20013;&#30340;&#26597;&#35810;&#12289;&#28857;&#20987;&#21644;&#20572;&#27490;&#34892;&#20026;&#65292;&#23454;&#29616;&#29983;&#25104;&#29305;&#23450;&#25628;&#32034;&#30340;&#23436;&#25972;&#25628;&#32034;&#20250;&#35805;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#25104;&#26412;&#25928;&#30410;&#21644;&#21487;&#37325;&#29616;&#24615;&#26041;&#38754;&#30340;&#20248;&#21183;&#65292;&#29992;&#25143;&#27169;&#25311;&#24050;&#25104;&#20026;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#29992;&#25143;&#20026;&#20013;&#24515;&#35780;&#20272;&#30340;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#20934;&#30830;&#27169;&#25311;&#29992;&#25143;&#25628;&#32034;&#34892;&#20026;&#19968;&#30452;&#26159;&#19968;&#39033;&#25361;&#25112;&#65292;&#22240;&#20026;&#29992;&#25143;&#22312;&#25628;&#32034;&#20013;&#30340;&#34892;&#20026;&#38750;&#24120;&#22797;&#26434;&#65292;&#21463;&#21040;&#23398;&#20064;&#12289;&#25512;&#29702;&#21644;&#35268;&#21010;&#31561;&#38169;&#32508;&#22797;&#26434;&#35748;&#30693;&#36807;&#31243;&#30340;&#39537;&#21160;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#23637;&#31034;&#20986;&#22312;&#27169;&#25311;&#20154;&#31867;&#32423;&#26234;&#33021;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#24182;&#24050;&#34987;&#29992;&#20110;&#26500;&#24314;&#21508;&#31181;&#20219;&#21153;&#30340;&#33258;&#20027;&#20195;&#29702;&#12290;&#28982;&#32780;&#65292;&#23578;&#26410;&#20805;&#20998;&#25506;&#32034;&#20351;&#29992;LLM&#27169;&#25311;&#25628;&#32034;&#34892;&#20026;&#30340;&#28508;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#29992;&#25143;&#25628;&#32034;&#34892;&#20026;&#27169;&#25311;&#22120;&#65292;&#21363;USimAgent&#12290;&#25552;&#20986;&#30340;&#27169;&#25311;&#22120;&#21487;&#20197;&#27169;&#25311;&#29992;&#25143;&#22312;&#25628;&#32034;&#36807;&#31243;&#20013;&#30340;&#26597;&#35810;&#12289;&#28857;&#20987;&#21644;&#20572;&#27490;&#34892;&#20026;&#65292;&#22240;&#27492;&#33021;&#22815;&#29983;&#25104;&#29305;&#23450;&#25628;&#32034;&#30340;&#23436;&#25972;&#25628;&#32034;&#20250;&#35805;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09142v1 Announce Type: cross  Abstract: Due to the advantages in the cost-efficiency and reproducibility, user simulation has become a promising solution to the user-centric evaluation of information retrieval systems. Nonetheless, accurately simulating user search behaviors has long been a challenge, because users' actions in search are highly complex and driven by intricate cognitive processes such as learning, reasoning, and planning. Recently, Large Language Models (LLMs) have demonstrated remarked potential in simulating human-level intelligence and have been used in building autonomous agents for various tasks. However, the potential of using LLMs in simulating search behaviors has not yet been fully explored. In this paper, we introduce a LLM-based user search behavior simulator, USimAgent. The proposed simulator can simulate users' querying, clicking, and stopping behaviors during search, and thus, is capable of generating complete search sessions for specific search
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25237;&#24433;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#65288;SHGD&#65289;&#65292;&#36890;&#36807;&#23545;&#31216;&#22240;&#23376;&#20998;&#35299;&#36827;&#34892;&#35889;&#21387;&#32553;&#24863;&#30693;&#65292;&#20943;&#23569;&#20102;&#35745;&#31639;&#21644;&#23384;&#20648;&#25104;&#26412;&#65292;&#24341;&#20837;&#20102;&#26032;&#30340;&#22240;&#23376;&#20998;&#35299;&#27495;&#20041;&#12290;</title><link>https://arxiv.org/abs/2403.09031</link><description>&lt;p&gt;
&#22522;&#20110;&#23545;&#31216; Hankel &#22240;&#23376;&#20998;&#35299;&#30340;&#35889;&#21387;&#32553;&#24863;&#30693;&#30340;&#25237;&#24433;&#26799;&#24230;&#19979;&#38477;
&lt;/p&gt;
&lt;p&gt;
Projected Gradient Descent for Spectral Compressed Sensing via Symmetric Hankel Factorization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09031
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25237;&#24433;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#65288;SHGD&#65289;&#65292;&#36890;&#36807;&#23545;&#31216;&#22240;&#23376;&#20998;&#35299;&#36827;&#34892;&#35889;&#21387;&#32553;&#24863;&#30693;&#65292;&#20943;&#23569;&#20102;&#35745;&#31639;&#21644;&#23384;&#20648;&#25104;&#26412;&#65292;&#24341;&#20837;&#20102;&#26032;&#30340;&#22240;&#23376;&#20998;&#35299;&#27495;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#35889;&#21387;&#32553;&#24863;&#30693;&#26041;&#27861;&#36890;&#36807; Hankel &#30697;&#38453;&#23436;&#25104;&#37319;&#29992;&#23545;&#31216;&#22240;&#23376;&#20998;&#35299;&#26469;&#23637;&#31034; Hankel &#30697;&#38453;&#30340;&#20302;&#31209;&#24615;&#36136;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#38750;&#20984;&#26799;&#24230;&#26041;&#27861;&#21482;&#21033;&#29992;&#19981;&#23545;&#31216;&#22240;&#23376;&#20998;&#35299;&#26469;&#23454;&#29616;&#35889;&#21387;&#32553;&#24863;&#30693;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25237;&#24433;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#31216;&#22240;&#23376;&#20998;&#35299;&#36827;&#34892;&#35889;&#21387;&#32553;&#24863;&#30693;&#65292;&#21517;&#20026;&#23545;&#31216; Hankel &#25237;&#24433;&#26799;&#24230;&#19979;&#38477;&#65288;SHGD&#65289;&#65292;&#23427;&#20165;&#26356;&#26032;&#19968;&#20010;&#30697;&#38453;&#24182;&#36991;&#20813;&#20102;&#24179;&#34913;&#27491;&#21017;&#21270;&#39033;&#12290;&#19982;&#22522;&#20110;&#19981;&#23545;&#31216;&#22240;&#23376;&#20998;&#35299;&#30340;&#20808;&#21069;&#26799;&#24230;&#26041;&#27861;&#30456;&#27604;&#65292;SHGD&#20943;&#23569;&#20102;&#22823;&#32422;&#19968;&#21322;&#30340;&#35745;&#31639;&#21644;&#23384;&#20648;&#25104;&#26412;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24037;&#20316;&#20013;&#20351;&#29992;&#30340;&#23545;&#31216;&#22240;&#23376;&#20998;&#35299;&#19982;&#20808;&#21069;&#30340;&#20302;&#31209;&#20998;&#35299;&#27169;&#22411;&#23436;&#20840;&#19981;&#21516;&#65292;&#24341;&#20837;&#20102;&#22312;&#22797;&#27491;&#20132;&#21464;&#25442;&#19979;&#30340;&#26032;&#22240;&#23376;&#20998;&#35299;&#27495;&#20041;&#12290;&#25105;&#20204;&#20026;&#25105;&#20204;&#30340;&#20998;&#35299;&#35774;&#35745;&#20102;&#26032;&#39062;&#30340;&#36317;&#31163;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09031v1 Announce Type: new  Abstract: Current spectral compressed sensing methods via Hankel matrix completion employ symmetric factorization to demonstrate the low-rank property of the Hankel matrix. However, previous non-convex gradient methods only utilize asymmetric factorization to achieve spectral compressed sensing. In this paper, we propose a novel nonconvex projected gradient descent method for spectral compressed sensing via symmetric factorization named Symmetric Hankel Projected Gradient Descent (SHGD), which updates only one matrix and avoids a balancing regularization term. SHGD reduces about half of the computation and storage costs compared to the prior gradient method based on asymmetric factorization. {Besides, the symmetric factorization employed in our work is completely novel to the prior low-rank factorization model, introducing a new factorization ambiguity under complex orthogonal transformation}. Novel distance metrics are designed for our factorizat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#26597;&#35810;&#29983;&#25104;&#21644;&#33258;&#25105;&#30417;&#30563;&#26041;&#27861;&#30340;&#22495;&#33258;&#36866;&#24212;&#31574;&#30053;&#65292;&#36890;&#36807;&#22312;&#30446;&#26631;&#22495;&#19978;&#33258;&#21160;&#29983;&#25104;&#20266;&#30456;&#20851;&#24615;&#26631;&#31614;&#65292;&#24182;&#24212;&#29992;&#20110;&#23494;&#38598;&#26816;&#32034;&#21644;&#23545;&#35805;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#25512;&#24191;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.08970</link><description>&lt;p&gt;
&#36890;&#36807;&#32454;&#33268;&#30340;&#20266;&#30456;&#20851;&#24615;&#26631;&#35760;&#23454;&#29616;&#33258;&#25105;&#30417;&#30563;&#30340;&#23494;&#38598;&#26816;&#32034;&#21644;&#23545;&#35805;&#23494;&#38598;&#26816;&#32034;&#30340;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Domain Adaptation for Dense Retrieval and Conversational Dense Retrieval through Self-Supervision by Meticulous Pseudo-Relevance Labeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08970
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#26597;&#35810;&#29983;&#25104;&#21644;&#33258;&#25105;&#30417;&#30563;&#26041;&#27861;&#30340;&#22495;&#33258;&#36866;&#24212;&#31574;&#30053;&#65292;&#36890;&#36807;&#22312;&#30446;&#26631;&#22495;&#19978;&#33258;&#21160;&#29983;&#25104;&#20266;&#30456;&#20851;&#24615;&#26631;&#31614;&#65292;&#24182;&#24212;&#29992;&#20110;&#23494;&#38598;&#26816;&#32034;&#21644;&#23545;&#35805;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#25512;&#24191;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#22312;&#23558;&#19981;&#21516;&#20998;&#24067;&#30340;&#30446;&#26631;&#22495;&#25512;&#24191;&#26041;&#38754;&#30340;&#33021;&#21147;&#26159;&#26377;&#38480;&#30340;&#65292;&#36825;&#19982;&#22522;&#20110;&#20132;&#20114;&#30340;&#27169;&#22411;&#30340;&#32467;&#26524;&#24418;&#25104;&#20102;&#23545;&#27604;&#12290;&#20197;&#21069;&#20026;&#20102;&#20943;&#36731;&#36825;&#19968;&#25361;&#25112;&#32780;&#23581;&#35797;&#30340;&#26041;&#27861;&#28041;&#21450;&#21033;&#29992;&#23545;&#25239;&#24615;&#23398;&#20064;&#21644;&#26597;&#35810;&#29983;&#25104;&#26041;&#27861;&#65292;&#20294;&#36825;&#20004;&#31181;&#26041;&#27861;&#20173;&#28982;&#24102;&#26469;&#20102;&#26377;&#38480;&#30340;&#25913;&#36827;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#26597;&#35810;&#29983;&#25104;&#26041;&#27861;&#19982;&#33258;&#25105;&#30417;&#30563;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#20854;&#20013;&#22312;&#30446;&#26631;&#22495;&#19978;&#33258;&#21160;&#29983;&#25104;&#20266;&#30456;&#20851;&#24615;&#26631;&#31614;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#19968;&#20010;T5-3B&#27169;&#22411;&#36827;&#34892;&#20266;&#27491;&#26631;&#35760;&#65292;&#24182;&#36873;&#25321;&#20102;&#32454;&#33268;&#30340;&#30828;&#36127;&#20363;&#12290;&#25105;&#20204;&#36824;&#23558;&#36825;&#31181;&#31574;&#30053;&#24212;&#29992;&#20110;&#29992;&#20110;&#23545;&#35805;&#25628;&#32034;&#30340;&#23545;&#35805;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#12290;&#20351;&#29992;&#31867;&#20284;&#30340;&#20266;&#26631;&#35760;&#26041;&#27861;&#65292;&#20294;&#22686;&#21152;&#20102;&#19968;&#20010;&#26597;&#35810;&#37325;&#20889;&#27169;&#22359;&#26469;&#37325;&#20889;&#23545;&#35805;&#26597;&#35810;&#20197;&#20415;&#21518;&#32493;&#26631;&#35760;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08970v1 Announce Type: new  Abstract: Recent studies have demonstrated that the ability of dense retrieval models to generalize to target domains with different distributions is limited, which contrasts with the results obtained with interaction-based models. Prior attempts to mitigate this challenge involved leveraging adversarial learning and query generation approaches, but both approaches nevertheless resulted in limited improvements. In this paper, we propose to combine the query-generation approach with a self-supervision approach in which pseudo-relevance labels are automatically generated on the target domain. To accomplish this, a T5-3B model is utilized for pseudo-positive labeling, and meticulous hard negatives are chosen. We also apply this strategy on conversational dense retrieval model for conversational search. A similar pseudo-labeling approach is used, but with the addition of a query-rewriting module to rewrite conversational queries for subsequent labelin
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;PAPERCLIP&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22825;&#25991;&#35266;&#27979;&#19982;&#33258;&#28982;&#35821;&#35328;&#20851;&#32852;&#36215;&#26469;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;CLIP&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#23454;&#29616;&#20102;&#35266;&#27979;&#21644;&#33258;&#28982;&#35821;&#35328;&#20043;&#38388;&#30340;&#26377;&#24847;&#20041;&#30340;&#32852;&#21512;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2403.08851</link><description>&lt;p&gt;
PAPERCLIP&#65306;&#20351;&#29992;&#22810;&#27169;&#24577;&#27169;&#22411;&#23558;&#22825;&#25991;&#35266;&#27979;&#21644;&#33258;&#28982;&#35821;&#35328;&#20851;&#32852;&#36215;&#26469;
&lt;/p&gt;
&lt;p&gt;
PAPERCLIP: Associating Astronomical Observations and Natural Language with Multi-Modal Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08851
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;PAPERCLIP&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22825;&#25991;&#35266;&#27979;&#19982;&#33258;&#28982;&#35821;&#35328;&#20851;&#32852;&#36215;&#26469;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;CLIP&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#23454;&#29616;&#20102;&#35266;&#27979;&#21644;&#33258;&#28982;&#35821;&#35328;&#20043;&#38388;&#30340;&#26377;&#24847;&#20041;&#30340;&#32852;&#21512;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;PAPERCLIP&#65288;Proposal Abstracts Provide an Effective Representation for Contrastive Language-Image Pre-training&#65289;&#65292;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#23558;&#30001;&#26395;&#36828;&#38236;&#25104;&#20687;&#30340;&#22825;&#25991;&#35266;&#27979;&#19982;&#33258;&#28982;&#35821;&#35328;&#20851;&#32852;&#36215;&#26469;&#30340;&#26041;&#27861;&#12290;&#35813;&#27169;&#22411;&#26159;&#20174;&#32463;&#36807;&#39044;&#35757;&#32451;&#30340;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;CLIP&#65289;&#27169;&#22411;&#24494;&#35843;&#32780;&#26469;&#65292;&#20351;&#29992;&#25104;&#21151;&#30340;&#35266;&#27979;&#25552;&#26696;&#25688;&#35201;&#21644;&#30456;&#24212;&#30340;&#19979;&#28216;&#35266;&#27979;&#65292;&#20854;&#20013;&#25688;&#35201;&#21487;&#36873;&#25321;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#24341;&#23548;&#29983;&#25104;&#26469;&#36827;&#34892;&#24635;&#32467;&#12290;&#20197;&#21704;&#21187;&#31354;&#38388;&#26395;&#36828;&#38236;&#65288;HST&#65289;&#30340;&#35266;&#27979;&#20026;&#20363;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#24494;&#35843;&#30340;&#27169;&#22411;&#36890;&#36807;&#38024;&#23545;&#22270;&#20687;&#26816;&#32034;&#65288;&#21363;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#25214;&#21040;&#26368;&#30456;&#20851;&#30340;&#35266;&#27979;&#65289;&#21644;&#25551;&#36848;&#26816;&#32034;&#65288;&#21363;&#26597;&#35810;&#19982;&#22825;&#25991;&#29289;&#20307;&#31867;&#21035;&#21644;&#29992;&#20363;&#26368;&#30456;&#20851;&#30340;&#20869;&#23481;&#65289;&#30340;&#27979;&#35797;&#65292;&#20307;&#29616;&#20102;&#35266;&#27979;&#21644;&#33258;&#28982;&#35821;&#35328;&#20043;&#38388;&#30340;&#26377;&#24847;&#20041;&#30340;&#32852;&#21512;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08851v1 Announce Type: cross  Abstract: We present PAPERCLIP (Proposal Abstracts Provide an Effective Representation for Contrastive Language-Image Pre-training), a method which associates astronomical observations imaged by telescopes with natural language using a neural network model. The model is fine-tuned from a pre-trained Contrastive Language-Image Pre-training (CLIP) model using successful observing proposal abstracts and corresponding downstream observations, with the abstracts optionally summarized via guided generation using large language models (LLMs). Using observations from the Hubble Space Telescope (HST) as an example, we show that the fine-tuned model embodies a meaningful joint representation between observations and natural language through tests targeting image retrieval (i.e., finding the most relevant observations using natural language queries) and description retrieval (i.e., querying for astrophysical object classes and use cases most relevant to a 
&lt;/p&gt;</description></item><item><title>AcademiaOS&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#21270;&#23450;&#24615;&#30740;&#31350;&#20013;&#30340;&#29702;&#35770;&#24314;&#26500;&#65292;&#20026;&#23398;&#26415;&#30028;&#25552;&#20379;&#26032;&#39062;&#30340;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.08844</link><description>&lt;p&gt;
AcademiaOS&#65306;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#21270;&#23450;&#24615;&#30740;&#31350;&#20013;&#30340;&#29702;&#35770;&#24314;&#26500;
&lt;/p&gt;
&lt;p&gt;
AcademiaOS: Automating Grounded Theory Development in Qualitative Research with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08844
&lt;/p&gt;
&lt;p&gt;
AcademiaOS&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#21270;&#23450;&#24615;&#30740;&#31350;&#20013;&#30340;&#29702;&#35770;&#24314;&#26500;&#65292;&#20026;&#23398;&#26415;&#30028;&#25552;&#20379;&#26032;&#39062;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AcademiaOS&#26159;&#31532;&#19968;&#20010;&#23581;&#35797;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#21270;&#23450;&#24615;&#30740;&#31350;&#20013;&#30340;&#29702;&#35770;&#24314;&#26500;&#30340;&#31995;&#32479;&#12290;&#21033;&#29992;&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#35328;&#29702;&#35299;&#12289;&#29983;&#25104;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;AcademiaOS&#23545;&#31579;&#36873;&#36807;&#30340;&#23450;&#24615;&#21407;&#22987;&#25968;&#25454;&#65288;&#22914;&#35775;&#35848;&#25991;&#26412;&#65289;&#36827;&#34892;&#32534;&#30721;&#65292;&#24182;&#21457;&#23637;&#20027;&#39064;&#21644;&#32500;&#24230;&#20197;&#36827;&#19968;&#27493;&#26500;&#24314;&#19968;&#20010;&#29702;&#35770;&#27169;&#22411;&#65292;&#25552;&#20379;&#26032;&#39062;&#30340;&#35265;&#35299;&#12290;&#19968;&#39033;&#29992;&#25143;&#30740;&#31350;&#65288;n=19&#65289;&#34920;&#26126;&#65292;&#35813;&#31995;&#32479;&#22312;&#23398;&#26415;&#30028;&#20013;&#24471;&#21040;&#35748;&#21487;&#65292;&#24182;&#20855;&#26377;&#22686;&#24378;&#20154;&#31867;&#36827;&#34892;&#23450;&#24615;&#30740;&#31350;&#30340;&#28508;&#21147;&#12290;AcademiaOS&#24050;&#32463;&#24320;&#28304;&#20379;&#20182;&#20154;&#36827;&#34892;&#26500;&#24314;&#24182;&#36866;&#24212;&#20854;&#29992;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08844v1 Announce Type: cross  Abstract: AcademiaOS is a first attempt to automate grounded theory development in qualitative research with large language models. Using recent large language models' language understanding, generation, and reasoning capabilities, AcademiaOS codes curated qualitative raw data such as interview transcripts and develops themes and dimensions to further develop a grounded theoretical model, affording novel insights. A user study (n=19) suggests that the system finds acceptance in the academic community and exhibits the potential to augment humans in qualitative research. AcademiaOS has been made open-source for others to build upon and adapt to their use cases.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22810;&#37325;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#26377;&#25928;&#22320;&#32467;&#21512;&#20102;&#22522;&#20110;&#24207;&#21015;&#21644;&#22522;&#20110;&#22270;&#30340;&#25512;&#33616;&#26041;&#27861;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#25512;&#33616;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.00895</link><description>&lt;p&gt;
&#31934;&#30830;&#25512;&#33616;&#30340;&#31471;&#21040;&#31471;&#22270;-&#24207;&#21015;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
End-to-end Graph-Sequential Representation Learning for Accurate Recommendations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00895
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22810;&#37325;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#26377;&#25928;&#22320;&#32467;&#21512;&#20102;&#22522;&#20110;&#24207;&#21015;&#21644;&#22522;&#20110;&#22270;&#30340;&#25512;&#33616;&#26041;&#27861;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#25512;&#33616;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#25512;&#33616;&#31995;&#32479;&#30340;&#35768;&#22810;&#26032;&#36827;&#23637;&#38598;&#20013;&#22312;&#24320;&#21457;&#22522;&#20110;&#24207;&#21015;&#21644;&#22522;&#20110;&#22270;&#30340;&#26041;&#27861;&#19978;&#12290;&#36825;&#20004;&#31181;&#26041;&#27861;&#22312;&#24314;&#27169;&#34892;&#20026;&#25968;&#25454;&#20013;&#30340;&#22797;&#26434;&#20851;&#31995;&#26041;&#38754;&#37117;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#65292;&#20174;&#32780;&#22312;&#20010;&#24615;&#21270;&#25490;&#21517;&#21644;&#19979;&#19968;&#20010;&#25512;&#33616;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26377;&#30410;&#30340;&#25104;&#26524;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#33391;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20174;&#25968;&#25454;&#20013;&#25429;&#25417;&#21040;&#30340;&#20449;&#21495;&#25130;&#28982;&#19981;&#21516;&#12290;&#21069;&#32773;&#30452;&#25509;&#36890;&#36807;&#19982;&#26368;&#36817;&#29289;&#21697;&#30340;&#26377;&#24207;&#20132;&#20114;&#26469;&#34920;&#31034;&#29992;&#25143;&#65292;&#32780;&#21518;&#32773;&#26088;&#22312;&#25429;&#25417;&#20132;&#20114;&#22270;&#20013;&#30340;&#38388;&#25509;&#20381;&#36182;&#20851;&#31995;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22810;&#37325;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#36825;&#20004;&#31181;&#33539;&#24335;&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;&#21033;&#29992;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#30456;&#20114;&#35757;&#32451;&#24207;&#21015;&#21644;&#22270;&#32452;&#20214;&#26174;&#33879;&#25913;&#21892;&#20102;&#25512;&#33616;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00895v1 Announce Type: cross  Abstract: Many recent advancements in recommender systems have focused on developing sequence-based and graph-based approaches. Both approaches proved useful in modeling intricate relationships within behavioral data, leading to promising outcomes in personalized ranking and next-item recommendation tasks while maintaining good scalability. However, they capture very different signals from data. While the former approach represents users directly through ordered interactions with recent items, the latter one aims to capture indirect dependencies across the interactions graph. This paper presents a novel multi-representational learning framework that exploits the synergies between these two paradigms. Our empirical evaluation on several datasets demonstrates that mutual training of sequential and graph components with the proposed framework significantly improves recommendations performance.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#22359;&#24335;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#38271;&#25991;&#26723;&#20013;&#20449;&#24687;&#26816;&#32034;&#39046;&#22495;&#30340;&#25361;&#25112;&#65292;&#24182;&#23454;&#29616;&#20102;&#21452;&#21521;&#20132;&#20114;</title><link>https://arxiv.org/abs/2402.13897</link><description>&lt;p&gt;
&#31185;&#23398;&#26816;&#26597;&#32773;&#20877;&#24230;&#21319;&#32423;&#65306;&#36879;&#26126;&#24230;&#21644;&#36923;&#36753;&#25512;&#29702;&#30340;&#21452;&#21521;&#33539;&#24335;
&lt;/p&gt;
&lt;p&gt;
Science Checker Reloaded: A Bidirectional Paradigm for Transparency and Logical Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13897
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#22359;&#24335;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#38271;&#25991;&#26723;&#20013;&#20449;&#24687;&#26816;&#32034;&#39046;&#22495;&#30340;&#25361;&#25112;&#65292;&#24182;&#23454;&#29616;&#20102;&#21452;&#21521;&#20132;&#20114;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#24687;&#26816;&#32034;&#26159;&#19968;&#20010;&#24555;&#36895;&#21457;&#23637;&#30340;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#23427;&#20173;&#28982;&#38754;&#20020;&#30528;&#22312;&#31185;&#23398;&#21644;&#24037;&#19994;&#30340;&#28023;&#37327;&#20449;&#24687;&#20013;&#30340;&#35832;&#22810;&#38480;&#21046;&#65292;&#27604;&#22914;&#35821;&#20041;&#20998;&#27495;&#21644;&#26816;&#32034;&#20013;&#30340;&#35789;&#27719;&#24046;&#36317;&#12289;&#35821;&#20041;&#25628;&#32034;&#20013;&#30340;&#20302;&#31934;&#24230;&#21644;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#65292;&#25110;&#32773;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#21644;&#36807;&#26102;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#22359;&#24335;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#38271;&#25991;&#26723;&#30340;&#36825;&#20123;&#38556;&#30861;&#12290;&#31532;&#19968;&#20010;&#27169;&#22359;&#36890;&#36807;&#26597;&#35810;&#25193;&#23637;&#22686;&#24378;&#20102;&#22312;&#31232;&#30095;&#26816;&#32034;&#20013;&#30340;&#35821;&#35328;&#29702;&#35299;&#65292;&#20197;&#26816;&#32034;&#30456;&#20851;&#25991;&#26723;&#12290;&#31532;&#20108;&#20010;&#27169;&#22359;&#36890;&#36807;&#21482;&#20351;&#29992;&#38271;&#25991;&#26723;&#20013;&#20256;&#25773;&#30340;&#20449;&#24687;&#65292;&#20026;&#22797;&#26434;&#38382;&#39064;&#25552;&#20379;&#20840;&#38754;&#21644;&#20449;&#24687;&#20016;&#23500;&#30340;&#31572;&#26696;&#26469;&#21152;&#28145;&#32467;&#26524;&#65292;&#23454;&#29616;&#21452;&#21521;&#20132;&#20114;&#12290;&#22312;&#31649;&#36947;&#30340;&#21508;&#20010;&#38454;&#27573;&#65292;&#21521;&#29992;&#25143;&#21576;&#29616;&#20013;&#38388;&#32467;&#26524;&#20197;&#20419;&#36827;&#23545;&#31995;&#32479;&#25512;&#29702;&#30340;&#29702;&#35299;&#12290;&#25105;&#20204;&#30456;&#20449;&#36825;&#31181;&#21452;&#21521;&#26041;&#27861;&#24102;&#26469;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13897v1 Announce Type: cross  Abstract: Information retrieval is a rapidly evolving field. However it still faces significant limitations in the scientific and industrial vast amounts of information, such as semantic divergence and vocabulary gaps in sparse retrieval, low precision and lack of interpretability in semantic search, or hallucination and outdated information in generative models. In this paper, we introduce a two-block approach to tackle these hurdles for long documents. The first block enhances language understanding in sparse retrieval by query expansion to retrieve relevant documents. The second block deepens the result by providing comprehensive and informative answers to the complex question using only the information spread in the long document, enabling bidirectional engagement. At various stages of the pipeline, intermediate results are presented to users to facilitate understanding of the system's reasoning. We believe this bidirectional approach brings
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;CARLG&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#19978;&#19979;&#25991;&#32447;&#32034;&#21644;&#35282;&#33394;&#30456;&#20851;&#24615;&#65292;&#25552;&#21319;&#20102;&#25991;&#26723;&#32423;&#20107;&#20214;&#35770;&#35777;&#25552;&#21462;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2310.05116</link><description>&lt;p&gt;
&#21033;&#29992;&#19978;&#19979;&#25991;&#32447;&#32034;&#21644;&#35282;&#33394;&#30456;&#20851;&#24615;&#25552;&#21319;&#25991;&#26723;&#32423;&#20107;&#20214;&#35770;&#35777;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Utilizing Contextual Clues and Role Correlations for Enhancing Document-level Event Argument Extraction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.05116
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;CARLG&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#19978;&#19979;&#25991;&#32447;&#32034;&#21644;&#35282;&#33394;&#30456;&#20851;&#24615;&#65292;&#25552;&#21319;&#20102;&#25991;&#26723;&#32423;&#20107;&#20214;&#35770;&#35777;&#25552;&#21462;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26723;&#32423;&#20107;&#20214;&#35770;&#35777;&#25552;&#21462;&#65288;EAE&#65289;&#26159;&#20449;&#24687;&#25552;&#21462;&#20013;&#33267;&#20851;&#37325;&#35201;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#23376;&#20219;&#21153;&#20043;&#19968;&#12290;&#29616;&#26377;&#26041;&#27861;&#22823;&#22810;&#20851;&#27880;&#35770;&#35777;&#21644;&#20107;&#20214;&#35302;&#21457;&#22120;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#24573;&#35270;&#20102;&#20004;&#20010;&#20851;&#38190;&#28857;&#65306;&#19978;&#19979;&#25991;&#32447;&#32034;&#30340;&#20449;&#24687;&#21644;&#35770;&#35777;&#35282;&#33394;&#20043;&#38388;&#30340;&#35821;&#20041;&#30456;&#20851;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;CARLG&#27169;&#22411;&#65292;&#21253;&#25324;&#20004;&#20010;&#27169;&#22359;&#65306;&#19978;&#19979;&#25991;&#32447;&#32034;&#32858;&#21512;&#65288;CCA&#65289;&#21644;&#22522;&#20110;&#35282;&#33394;&#30340;&#28508;&#22312;&#20449;&#24687;&#24341;&#23548;&#65288;RLIG&#65289;&#65292;&#36890;&#36807;&#26377;&#25928;&#21033;&#29992;&#19978;&#19979;&#25991;&#32447;&#32034;&#21644;&#35282;&#33394;&#30456;&#20851;&#24615;&#26469;&#25552;&#39640;&#25991;&#26723;&#32423;EAE&#12290;CCA&#27169;&#22359;&#36890;&#36807;&#21033;&#29992;&#26469;&#33258;&#39044;&#35757;&#32451;&#32534;&#30721;&#22120;&#30340;&#19978;&#19979;&#25991;&#27880;&#24847;&#26435;&#37325;&#65292;&#33258;&#36866;&#24212;&#22320;&#25429;&#25417;&#21644;&#25972;&#21512;&#19978;&#19979;&#25991;&#32447;&#32034;&#12290;RLIG&#27169;&#22359;&#36890;&#36807;&#35282;&#33394;&#20132;&#20114;&#32534;&#30721;&#25429;&#25417;&#35821;&#20041;&#30456;&#20851;&#24615;&#65292;&#24182;&#36890;&#36807;&#28508;&#22312;&#35282;&#33394;&#34920;&#31034;&#25552;&#20379;&#23453;&#36149;&#30340;&#20449;&#24687;&#24341;&#23548;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;CCA&#21644;RLIG&#27169;&#22359;&#32039;&#20945;&#12289;&#21487;&#31227;&#26893;&#19988;&#39640;&#25928;&#65292;&#24341;&#20837;&#30340;&#26032;&#21442;&#25968;&#19981;&#36229;&#36807;1%&#65292;&#19988;&#26131;&#20110;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Document-level event argument extraction (EAE) is a vital but challenging subtask in information extraction. Most existing approaches focus on the interaction between arguments and event triggers, ignoring two critical points: the information of contextual clues and the semantic correlations among argument roles. In this paper, we propose the CARLG model, which consists of two modules: Contextual Clues Aggregation (CCA) and Role-based Latent Information Guidance (RLIG), effectively leveraging contextual clues and role correlations for improving document-level EAE. The CCA module adaptively captures and integrates contextual clues by utilizing context attention weights from a pre-trained encoder. The RLIG module captures semantic correlations through role-interactive encoding and provides valuable information guidance with latent role representation. Notably, our CCA and RLIG modules are compact, transplantable and efficient, which introduce no more than 1% new parameters and can be eas
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;TRICE&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25191;&#34892;&#21453;&#39304;&#23454;&#29616;&#35821;&#35328;&#27169;&#22411;&#30340;&#24037;&#20855;&#23398;&#20064;&#65292;&#20351;&#20854;&#33021;&#22815;&#23398;&#20250;&#20309;&#26102;&#20197;&#21450;&#22914;&#20309;&#26377;&#25928;&#22320;&#20351;&#29992;&#24037;&#20855;&#12290;</title><link>https://arxiv.org/abs/2305.13068</link><description>&lt;p&gt;
&#36890;&#36807;&#25191;&#34892;&#21453;&#39304;&#20351;&#35821;&#35328;&#27169;&#22411;&#25104;&#20026;&#26356;&#22909;&#30340;&#24037;&#20855;&#23398;&#20064;&#32773;
&lt;/p&gt;
&lt;p&gt;
Making Language Models Better Tool Learners with Execution Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.13068
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;TRICE&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25191;&#34892;&#21453;&#39304;&#23454;&#29616;&#35821;&#35328;&#27169;&#22411;&#30340;&#24037;&#20855;&#23398;&#20064;&#65292;&#20351;&#20854;&#33021;&#22815;&#23398;&#20250;&#20309;&#26102;&#20197;&#21450;&#22914;&#20309;&#26377;&#25928;&#22320;&#20351;&#29992;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24037;&#20855;&#20316;&#20026;&#20851;&#38190;&#30340;&#30028;&#38754;&#65292;&#20351;&#20154;&#31867;&#33021;&#22815;&#29702;&#35299;&#21644;&#25913;&#21464;&#29615;&#22659;&#12290;&#38543;&#30528;&#22522;&#30784;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;AI&#31995;&#32479;&#21487;&#20197;&#21033;&#29992;&#24037;&#20855;&#25193;&#23637;&#20854;&#33021;&#21147;&#24182;&#19982;&#30495;&#23454;&#19990;&#30028;&#20114;&#21160;&#12290;&#29616;&#26377;&#30340;&#24037;&#20855;&#23398;&#20064;&#26041;&#27861;&#21253;&#25324;&#30417;&#30563;&#24494;&#35843;&#21644;&#25552;&#31034;&#24037;&#31243;&#26041;&#27861;&#65292;&#36890;&#24120;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19981;&#21152;&#36873;&#25321;&#22320;&#21033;&#29992;&#24037;&#20855;&#65292;&#22240;&#20026;&#22797;&#26434;&#20219;&#21153;&#24448;&#24448;&#36229;&#20986;&#20102;&#23427;&#20204;&#33258;&#36523;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20026;&#31616;&#21333;&#20219;&#21153;&#24341;&#20837;&#24037;&#20855;&#65288;&#27169;&#22411;&#26412;&#36523;&#21487;&#20197;&#36731;&#26494;&#35299;&#20915;&#30340;&#20219;&#21153;&#65289;&#65292;&#21487;&#33021;&#20250;&#26080;&#24847;&#38388;&#20256;&#25773;&#38169;&#35823;&#32780;&#19981;&#26159;&#25552;&#39640;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#30740;&#31350;&#38382;&#39064;&#26159;&#65306;&#25105;&#20204;&#33021;&#21542;&#25945;&#20250;&#35821;&#35328;&#27169;&#22411;&#20309;&#26102;&#20197;&#21450;&#22914;&#20309;&#20351;&#29992;&#24037;&#20855;&#65311;&#20026;&#28385;&#36275;&#36825;&#20010;&#38656;&#27714;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Tool leaRning wIth exeCution fEedback (TRICE)&#65292;&#36825;&#26159;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#20174;&#24037;&#20855;&#25191;&#34892;&#20013;&#24471;&#21040;&#30340;&#21453;&#39304;&#19981;&#26029;&#23398;&#20064;&#65292;&#20174;&#32780;&#23398;&#20250;&#20309;&#26102;&#20197;&#21450;&#22914;&#20309;&#26377;&#25928;&#22320;&#20351;&#29992;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tools serve as pivotal interfaces that enable humans to understand and reshape the environment. With the advent of foundation models, AI systems can utilize tools to expand their capabilities and interact with the real world. Existing tool learning methodologies, encompassing supervised fine-tuning and prompt engineering approaches, often induce large language models to utilize tools indiscriminately, as complex tasks often exceed their own competencies. However, introducing tools for simple tasks, which the models themselves can readily resolve, can inadvertently propagate errors rather than enhance performance. This leads to the research question: can we teach language models when and how to use tools? To meet this need, we propose Tool leaRning wIth exeCution fEedback (TRICE), a two-stage end-to-end framework that enables the model to continually learn through feedback derived from tool execution, thereby learning when and how to use tools effectively. Experimental results, backed b
&lt;/p&gt;</description></item><item><title>&#36825;&#20221;&#32508;&#36848;&#20840;&#38754;&#35843;&#30740;&#20102;&#22522;&#20110;&#22823;&#25968;&#25454;&#30340;&#29616;&#20195;&#25512;&#33616;&#31995;&#32479;&#30340;&#21457;&#23637;&#21644;&#25361;&#25112;&#65292;&#24635;&#32467;&#20102;&#22235;&#31181;&#20027;&#35201;&#31867;&#22411;&#30340;&#25512;&#33616;&#25216;&#26415;&#65292;&#24182;&#25351;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#22312;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2206.02631</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#25968;&#25454;&#30340;&#29616;&#20195;&#25512;&#33616;&#31995;&#32479;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Modern Recommendation System based on Big Data. (arXiv:2206.02631v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.02631
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20221;&#32508;&#36848;&#20840;&#38754;&#35843;&#30740;&#20102;&#22522;&#20110;&#22823;&#25968;&#25454;&#30340;&#29616;&#20195;&#25512;&#33616;&#31995;&#32479;&#30340;&#21457;&#23637;&#21644;&#25361;&#25112;&#65292;&#24635;&#32467;&#20102;&#22235;&#31181;&#20027;&#35201;&#31867;&#22411;&#30340;&#25512;&#33616;&#25216;&#26415;&#65292;&#24182;&#25351;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#22312;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#20840;&#38754;&#25506;&#32034;&#20102;&#25512;&#33616;&#31995;&#32479;&#30340;&#21457;&#23637;&#21644;&#24403;&#21069;&#29366;&#24577;&#65292;&#36825;&#20123;&#31995;&#32479;&#24050;&#24191;&#27867;&#25972;&#21512;&#21040;&#21508;&#31181;&#32593;&#32476;&#24212;&#29992;&#20013;&#12290;&#23427;&#37325;&#28857;&#20851;&#27880;&#20010;&#24615;&#21270;&#25512;&#33616;&#31574;&#30053;&#22312;&#22312;&#32447;&#20135;&#21697;&#25110;&#26381;&#21153;&#20013;&#30340;&#36827;&#23637;&#12290;&#25105;&#20204;&#23558;&#25512;&#33616;&#25216;&#26415;&#20998;&#20026;&#22235;&#31181;&#20027;&#35201;&#31867;&#22411;&#65306;&#22522;&#20110;&#20869;&#23481;&#30340;&#12289;&#21327;&#21516;&#36807;&#28388;&#30340;&#12289;&#22522;&#20110;&#30693;&#35782;&#30340;&#21644;&#28151;&#21512;&#30340;&#65292;&#27599;&#31181;&#31867;&#22411;&#37117;&#35299;&#20915;&#20102;&#29420;&#29305;&#30340;&#24773;&#26223;&#12290;&#26412;&#32508;&#36848;&#35814;&#32454;&#23457;&#35270;&#20102;&#25512;&#33616;&#31995;&#32479;&#30340;&#21382;&#21490;&#32972;&#26223;&#21644;&#26368;&#26032;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#37027;&#20123;&#20351;&#29992;&#22823;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#26412;&#32508;&#36848;&#36824;&#30830;&#23450;&#24182;&#35752;&#35770;&#20102;&#29616;&#20195;&#25512;&#33616;&#31995;&#32479;&#38754;&#20020;&#30340;&#20851;&#38190;&#25361;&#25112;&#65292;&#22914;&#25968;&#25454;&#31232;&#30095;&#24615;&#12289;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#20197;&#21450;&#23545;&#25512;&#33616;&#30340;&#22810;&#26679;&#24615;&#38656;&#27714;&#12290;&#32508;&#36848;&#26368;&#21518;&#24378;&#35843;&#20102;&#36825;&#20123;&#25361;&#25112;&#20316;&#20026;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#22312;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
This survey provides an exhaustive exploration of the evolution and current state of recommendation systems, which have seen widespread integration in various web applications. It focuses on the advancement of personalized recommendation strategies for online products or services. We categorize recommendation techniques into four primary types: content-based, collaborative filtering-based, knowledge-based, and hybrid-based, each addressing unique scenarios. The survey offers a detailed examination of the historical context and the latest innovative approaches in recommendation systems, particularly those employing big data. Additionally, it identifies and discusses key challenges faced by modern recommendation systems, such as data sparsity, scalability issues, and the need for diversity in recommendations. The survey concludes by highlighting these challenges as potential areas for fruitful future research in the field.
&lt;/p&gt;</description></item></channel></rss>