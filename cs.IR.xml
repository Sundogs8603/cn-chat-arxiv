<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#23545;&#27604;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#23545;&#40784;&#23884;&#20837;&#26469;&#26816;&#32034;&#30456;&#24212;&#30340;&#20505;&#36873;&#25918;&#23556;&#23398;&#25991;&#26412;&#65292;&#24182;&#20351;&#29992;&#36890;&#29992;&#39046;&#22495;&#29983;&#25104;&#27169;&#22411;&#26469;&#29983;&#25104;&#25253;&#21578;&#65292;&#21487;&#25233;&#21046;&#34394;&#26500;&#30340;&#29983;&#25104;&#65292;&#23454;&#29616;&#26356;&#22909;&#30340;&#20020;&#24202;&#25351;&#26631;&#12290;</title><link>http://arxiv.org/abs/2305.03660</link><description>&lt;p&gt;
&#21033;&#29992;OpenAI GPT&#27169;&#22411;&#30340;&#26816;&#32034;&#22686;&#24378;&#30340;&#33016;&#37096;X&#23556;&#32447;&#25253;&#21578;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Retrieval Augmented Chest X-Ray Report Generation using OpenAI GPT models. (arXiv:2305.03660v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03660
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#23545;&#27604;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#23545;&#40784;&#23884;&#20837;&#26469;&#26816;&#32034;&#30456;&#24212;&#30340;&#20505;&#36873;&#25918;&#23556;&#23398;&#25991;&#26412;&#65292;&#24182;&#20351;&#29992;&#36890;&#29992;&#39046;&#22495;&#29983;&#25104;&#27169;&#22411;&#26469;&#29983;&#25104;&#25253;&#21578;&#65292;&#21487;&#25233;&#21046;&#34394;&#26500;&#30340;&#29983;&#25104;&#65292;&#23454;&#29616;&#26356;&#22909;&#30340;&#20020;&#24202;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Retrieval Augmented Generation (RAG) &#30340;&#26041;&#27861;&#26469;&#33258;&#21160;&#29983;&#25104;&#25918;&#23556;&#23398;&#25253;&#21578;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#23545;&#27604;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#23545;&#40784;&#23884;&#20837;&#26469;&#26816;&#32034;&#30456;&#24212;&#30340;&#20505;&#36873;&#25918;&#23556;&#23398;&#25991;&#26412;&#65292;&#24182;&#20351;&#29992;&#20687;OpenAI text-davinci-003&#12289;gpt-3.5-turbo&#21644;gpt-4&#36825;&#26679;&#30340;&#36890;&#29992;&#39046;&#22495;&#29983;&#25104;&#27169;&#22411;&#26469;&#29983;&#25104;&#25253;&#21578;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#25233;&#21046;&#34394;&#26500;&#30340;&#29983;&#25104;&#24182;&#25552;&#20379;&#25351;&#20196;&#36319;&#38543;&#33021;&#21147;&#65292;&#20197;&#25105;&#20204;&#25152;&#38656;&#30340;&#26684;&#24335;&#29983;&#25104;&#25253;&#21578;&#20869;&#23481;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#20020;&#24202;&#25351;&#26631;&#65292;BERTScore&#20026;0.2865&#65288;&#916;+25.88%&#65289;&#65292;Semb Score&#20026;0.4026&#65288;&#916;+6.31%&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#24191;&#27867;&#24212;&#29992;&#20110;&#19981;&#21516;&#30340;&#20020;&#24202;&#35774;&#32622;&#65292;&#22240;&#20026;&#23427;&#20801;&#35768;&#22686;&#24378;&#33258;&#21160;&#29983;&#25104;&#30340;&#25918;&#23556;&#23398;&#25253;&#21578;&#36807;&#31243;&#65292;&#21516;&#26102;&#20855;&#22791;&#36866;&#21512;&#35813;&#35774;&#32622;&#30340;&#30456;&#20851;&#20869;&#23481;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose Retrieval Augmented Generation (RAG) as an approach for automated radiology report writing that leverages multimodally aligned embeddings from a contrastively pretrained vision language model for retrieval of relevant candidate radiology text for an input radiology image and a general domain generative model like OpenAI text-davinci-003, gpt-3.5-turbo and gpt-4 for report generation using the relevant radiology text retrieved. This approach keeps hallucinated generations under check and provides capabilities to generate report content in the format we desire leveraging the instruction following capabilities of these generative models. Our approach achieves better clinical metrics with a BERTScore of 0.2865 ({\Delta}+ 25.88%) and Semb score of 0.4026 ({\Delta}+ 6.31%). Our approach can be broadly relevant for different clinical settings as it allows to augment the automated radiology report generation process with content relevant for that setting while also having the abilit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#26597;&#35810;&#25193;&#23637;&#30340;&#26041;&#27861;&#65292;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#34920;&#29616;&#65292;&#29305;&#21035;&#26159;Chain-of-Thought&#25552;&#31034;&#23545;&#20110;&#26597;&#35810;&#25193;&#23637;&#26377;&#30528;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.03653</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#20419;&#36827;&#26597;&#35810;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
Query Expansion by Prompting Large Language Models. (arXiv:2305.03653v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03653
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#26597;&#35810;&#25193;&#23637;&#30340;&#26041;&#27861;&#65292;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#34920;&#29616;&#65292;&#29305;&#21035;&#26159;Chain-of-Thought&#25552;&#31034;&#23545;&#20110;&#26597;&#35810;&#25193;&#23637;&#26377;&#30528;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26597;&#35810;&#25193;&#23637;&#26159;&#25552;&#39640;&#25628;&#32034;&#31995;&#32479;&#21484;&#22238;&#29575;&#30340;&#24120;&#29992;&#25216;&#26415;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#29983;&#25104;&#33021;&#21147;&#36827;&#34892;&#26597;&#35810;&#25193;&#23637;&#30340;&#26041;&#27861;&#12290;&#19982;&#20256;&#32479;&#30340;&#26597;&#35810;&#25193;&#23637;&#26041;&#27861;&#22914;&#8220;&#20266;&#30456;&#20851;&#21453;&#39304;&#8221;&#65288;PRF&#65289;&#20381;&#36182;&#20110;&#26816;&#32034;&#19968;&#32452;&#22909;&#30340;&#20266;&#30456;&#20851;&#25991;&#26723;&#26469;&#25193;&#23637;&#26597;&#35810;&#30456;&#27604;&#65292;&#25105;&#20204;&#20381;&#36182;LLM&#30340;&#29983;&#25104;&#21644;&#21019;&#36896;&#33021;&#21147;&#65292;&#24182;&#21033;&#29992;&#27169;&#22411;&#22266;&#26377;&#30340;&#30693;&#35782;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#21508;&#31181;&#19981;&#21516;&#30340;&#25552;&#31034;&#65292;&#21253;&#25324;&#38646;-shot&#12289;few-shot&#21644;Chain-of-Thought&#65288;CoT&#65289;&#12290;&#25105;&#20204;&#21457;&#29616;CoT&#25552;&#31034;&#23545;&#20110;&#26597;&#35810;&#25193;&#23637;&#29305;&#21035;&#26377;&#29992;&#65292;&#22240;&#20026;&#36825;&#20123;&#25552;&#31034;&#25351;&#31034;&#27169;&#22411;&#36880;&#27493;&#20998;&#35299;&#26597;&#35810;&#65292;&#24182;&#21487;&#20197;&#25552;&#20379;&#19982;&#21407;&#22987;&#26597;&#35810;&#30456;&#20851;&#30340;&#22823;&#37327;&#26415;&#35821;&#12290;&#22312;MS-MARCO&#21644;BEIR&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LLM&#29983;&#25104;&#30340;&#26597;&#35810;&#25193;&#23637;&#21487;&#20197;&#27604;&#20256;&#32479;&#30340;&#26597;&#35810;&#25193;&#23637;&#26041;&#27861;&#26356;&#20855;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Query expansion is a widely used technique to improve the recall of search systems. In this paper, we propose an approach to query expansion that leverages the generative abilities of Large Language Models (LLMs). Unlike traditional query expansion approaches such as Pseudo-Relevance Feedback (PRF) that relies on retrieving a good set of pseudo-relevant documents to expand queries, we rely on the generative and creative abilities of an LLM and leverage the knowledge inherent in the model. We study a variety of different prompts, including zero-shot, few-shot and Chain-of-Thought (CoT). We find that CoT prompts are especially useful for query expansion as these prompts instruct the model to break queries down step-by-step and can provide a large number of terms related to the original query. Experimental results on MS-MARCO and BEIR demonstrate that query expansions generated by LLMs can be more powerful than traditional query expansion methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Disentangled Incremental Learning (DIL)&#26694;&#26550;&#65292;&#21487;&#20197;&#36890;&#36807;&#20852;&#36259;&#35299;&#32806;&#30340;&#26041;&#24335;&#37325;&#26032;&#35757;&#32451;&#22522;&#20110;&#22270;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.03624</link><description>&lt;p&gt;
&#36890;&#36807;&#20852;&#36259;&#35299;&#32806;&#37325;&#26032;&#35757;&#32451;&#22522;&#20110;&#22270;&#30340;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Retraining A Graph-based Recommender with Interests Disentanglement. (arXiv:2305.03624v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03624
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Disentangled Incremental Learning (DIL)&#26694;&#26550;&#65292;&#21487;&#20197;&#36890;&#36807;&#20852;&#36259;&#35299;&#32806;&#30340;&#26041;&#24335;&#37325;&#26032;&#35757;&#32451;&#22522;&#20110;&#22270;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#30340;&#25512;&#33616;&#31995;&#32479;&#20013;&#65292;&#20250;&#19981;&#26029;&#35266;&#23519;&#21040;&#26032;&#30340;&#20132;&#20114;&#12290;&#19968;&#20123;&#20132;&#20114;&#26159;&#39044;&#26399;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#22823;&#37096;&#20998;&#36981;&#24490;&#29992;&#25143;&#30340;&#38271;&#26399;&#20559;&#22909;&#12290;&#20854;&#20182;&#19968;&#20123;&#20132;&#20114;&#21017;&#34920;&#26126;&#29992;&#25143;&#20559;&#22909;&#30340;&#26368;&#26032;&#36235;&#21183;&#25110;&#26032;&#29289;&#21697;&#30340;&#33829;&#38144;&#31435;&#22330;&#12290;&#22240;&#27492;&#65292;&#25512;&#33616;&#31639;&#27861;&#38656;&#35201;&#21608;&#26399;&#24615;&#22320;&#37325;&#26032;&#35757;&#32451;&#25110;&#26356;&#26032;&#65292;&#20197;&#25429;&#25417;&#26032;&#30340;&#36235;&#21183;&#65292;&#21516;&#26102;&#19981;&#35201;&#24536;&#35760;&#38271;&#26399;&#20559;&#22909;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Disentangled Incremental Learning&#65288;DIL&#65289;&#30340;&#26032;&#22411;&#36890;&#29992;&#37325;&#26032;&#35757;&#32451;&#26694;&#26550;&#65292;&#29992;&#20110;&#22522;&#20110;&#22270;&#30340;&#25512;&#33616;&#31995;&#32479;&#12290;&#20551;&#35774;&#38271;&#26399;&#20559;&#22909;&#24050;&#32463;&#20197;&#23398;&#20064;&#33258;&#36807;&#21435;&#20132;&#20114;&#30340;&#27169;&#22411;&#21442;&#25968;&#30340;&#24418;&#24335;&#22312;&#29616;&#26377;&#27169;&#22411;&#20013;&#24471;&#21040;&#33391;&#22909;&#25429;&#25417;&#12290;&#26032;&#20559;&#22909;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#26032;&#35266;&#23519;&#21040;&#30340;&#20132;&#20114;&#26500;&#24314;&#30340;&#29992;&#25143;-&#29289;&#21697;&#20108;&#20998;&#22270;&#26469;&#23398;&#20064;&#12290;&#22312;Disentangled Incremental Learning&#65288;DIL&#65289;&#20013;&#65292;&#35774;&#35745;&#20102;&#20449;&#24687;&#25552;&#21462;&#27169;&#22359;&#26469;&#20174;&#29616;&#26377;&#27169;&#22411;&#20013;&#25552;&#21462;&#21382;&#21490;&#20559;&#22909;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#22522;&#20110;&#26032;&#35774;&#35745;&#30340;&#27491;&#21017;&#21270;&#39033;&#30340;Disentangled Information Embedding&#65288;&#35299;&#32806;&#20449;&#24687;&#23884;&#20837;&#65289;&#26469;&#28151;&#21512;&#21382;&#21490;&#21644;&#26032;&#20559;&#22909;&#12290;&#35299;&#32806;&#30340;&#23884;&#20837;&#21487;&#20197;&#30452;&#25509;&#29992;&#20110;&#25512;&#33616;&#25110;&#19979;&#28216;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DIL&#22312;&#25512;&#33616;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#22343;&#20248;&#20110;&#22810;&#20010;&#26368;&#26032;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In a practical recommender system, new interactions are continuously observed. Some interactions are expected, because they largely follow users' long-term preferences. Some other interactions are indications of recent trends in user preference changes or marketing positions of new items. Accordingly, the recommender needs to be periodically retrained or updated to capture the new trends, and yet not to forget the long-term preferences. In this paper, we propose a novel and generic retraining framework called Disentangled Incremental Learning (DIL) for graph-based recommenders. We assume that long-term preferences are well captured in the existing model, in the form of model parameters learned from past interactions. New preferences can be learned from the user-item bipartite graph constructed using the newly observed interactions. In DIL, we design an Information Extraction Module to extract historical preferences from the existing model. Then we blend the historical and new preferenc
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220; ReRead&#8221;&#30340;&#20107;&#23454;&#39564;&#35777;&#27169;&#22411;&#65292;&#20854;&#20197;&#20004;&#20010;&#38454;&#27573;&#65288;&#35777;&#25454;&#26816;&#32034;&#21644;&#22768;&#26126;&#39564;&#35777;&#65289;&#23454;&#29616;&#20934;&#30830;&#19988;&#21487;&#35299;&#37322;&#30340;&#20107;&#23454;&#39564;&#35777;&#65292;&#24182;&#22312;FEVER&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.03507</link><description>&lt;p&gt;
&#35835;&#20004;&#36941;&#65306;&#36890;&#36807;&#37325;&#26032;&#23457;&#35270;&#35777;&#25454;&#23454;&#29616;&#20934;&#30830;&#19988;&#21487;&#35299;&#37322;&#30340;&#20107;&#23454;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Read it Twice: Towards Faithfully Interpretable Fact Verification by Revisiting Evidence. (arXiv:2305.03507v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03507
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220; ReRead&#8221;&#30340;&#20107;&#23454;&#39564;&#35777;&#27169;&#22411;&#65292;&#20854;&#20197;&#20004;&#20010;&#38454;&#27573;&#65288;&#35777;&#25454;&#26816;&#32034;&#21644;&#22768;&#26126;&#39564;&#35777;&#65289;&#23454;&#29616;&#20934;&#30830;&#19988;&#21487;&#35299;&#37322;&#30340;&#20107;&#23454;&#39564;&#35777;&#65292;&#24182;&#22312;FEVER&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#20107;&#23454;&#39564;&#35777;&#20219;&#21153;&#26088;&#22312;&#36890;&#36807;&#20174;&#21407;&#22987;&#25991;&#26723;&#20013;&#26816;&#32034;&#35777;&#25454;&#26469;&#39564;&#35777;&#22768;&#26126;&#30340;&#20107;&#23454;&#24615;&#12290; &#26816;&#32034;&#21040;&#30340;&#35777;&#25454;&#30340;&#36136;&#37327;&#22312;&#35813;&#20219;&#21153;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290; &#29702;&#24819;&#24773;&#20917;&#19979;&#65292;&#26816;&#32034;&#21040;&#30340;&#35777;&#25454;&#24212;&#35813;&#26159;&#21487;&#20449;&#30340;&#65288;&#21453;&#26144;&#20102;&#27169;&#22411;&#22312;&#22768;&#26126;&#39564;&#35777;&#20013;&#30340;&#20915;&#31574;&#36807;&#31243;&#65289;&#19988;&#21512;&#29702;&#30340;&#65288;&#23545;&#20154;&#31867;&#26377;&#35828;&#26381;&#21147;&#65289;&#65292;&#24182;&#33021;&#25552;&#39640;&#39564;&#35777;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#12290; &#23613;&#31649;&#29616;&#26377;&#30340;&#26041;&#27861;&#21033;&#29992;&#22768;&#26126;&#21644;&#25991;&#26723;&#20043;&#38388;&#30340;&#35821;&#20041;&#25110;&#34920;&#38754;&#24418;&#24335;&#30340;&#30456;&#20284;&#24615;&#24230;&#37327;&#26469;&#26816;&#32034;&#35777;&#25454;&#65292;&#20294;&#23427;&#20204;&#37117;&#20381;&#36182;&#20110;&#26576;&#20123;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#20250;&#38459;&#27490;&#23427;&#20204;&#28385;&#36275;&#25152;&#26377;&#19977;&#20010;&#35201;&#27714;&#12290; &#37492;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220; ReRead&#8221;&#30340;&#20107;&#23454;&#39564;&#35777;&#27169;&#22411;&#65292;&#20197;&#26816;&#32034;&#35777;&#25454;&#24182;&#39564;&#35777;&#22768;&#26126;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#20197;&#19979;&#20004;&#20010;&#38454;&#27573;&#65306;1&#65289;&#35777;&#25454;&#26816;&#32034;&#38454;&#27573;&#65292;&#35813;&#38454;&#27573;&#36890;&#36807;&#20351;&#29992;&#24544;&#23454;&#19988;&#21512;&#29702;&#30340;&#35777;&#25454;&#21462;&#22238;&#22120;&#26469;&#33719;&#21462;&#21487;&#35299;&#37322;&#30340;&#35777;&#25454;&#65307;2&#65289;&#22768;&#26126;&#39564;&#35777;&#38454;&#27573;&#65292;&#35813;&#38454;&#27573;&#37325;&#26032;&#23457;&#35270;&#26816;&#32034;&#21040;&#30340;&#35777;&#25454;&#20197;&#39564;&#35777;&#22768;&#26126;&#12290; &#25105;&#20204;&#22312;&#24191;&#27867;&#20351;&#29992;&#30340;FEVER&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-world fact verification task aims to verify the factuality of a claim by retrieving evidence from the source document. The quality of the retrieved evidence plays an important role in claim verification. Ideally, the retrieved evidence should be faithful (reflecting the model's decision-making process in claim verification) and plausible (convincing to humans), and can improve the accuracy of verification task. Although existing approaches leverage the similarity measure of semantic or surface form between claims and documents to retrieve evidence, they all rely on certain heuristics that prevent them from satisfying all three requirements. In light of this, we propose a fact verification model named ReRead to retrieve evidence and verify claim that: (1) Train the evidence retriever to obtain interpretable evidence (i.e., faithfulness and plausibility criteria); (2) Train the claim verifier to revisit the evidence retrieved by the optimized evidence retriever to improve the accura
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20851;&#31995;&#25552;&#21462;&#30340;&#29702;&#25454;&#25277;&#21462;&#26694;&#26550;RE2&#65292;&#21033;&#29992;&#20004;&#20010;&#36830;&#32493;&#24615;&#21644;&#31232;&#30095;&#24615;&#22240;&#32032;&#20174;&#21477;&#23376;&#20013;&#33719;&#21462;&#30456;&#20851;&#32780;&#36830;&#36143;&#30340;&#29702;&#25454;&#65292;&#35299;&#20915;&#20102;&#22914;&#20309;&#20445;&#30041;&#30456;&#20851;&#20869;&#23481;&#24182;&#20174;&#21477;&#23376;&#20013;&#21435;&#25481;&#22122;&#38899;&#27573;&#33853;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#30340;&#27169;&#22411;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#24182;&#19988;&#25552;&#21462;&#20986;&#30340;&#29702;&#25454;&#23545;&#20110;&#25512;&#29702;&#21644;&#35299;&#37322;&#26159;&#26377;&#29992;&#30340;&#12290;</title><link>http://arxiv.org/abs/2305.03503</link><description>&lt;p&gt;
&#21512;&#29702;&#30475;&#24453;&#25152;&#30475;&#21040;&#30340;&#65306;&#20851;&#31995;&#25552;&#21462;&#30340;&#36830;&#32493;&#29702;&#25454;&#25277;&#21462;
&lt;/p&gt;
&lt;p&gt;
Think Rationally about What You See: Continuous Rationale Extraction for Relation Extraction. (arXiv:2305.03503v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03503
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20851;&#31995;&#25552;&#21462;&#30340;&#29702;&#25454;&#25277;&#21462;&#26694;&#26550;RE2&#65292;&#21033;&#29992;&#20004;&#20010;&#36830;&#32493;&#24615;&#21644;&#31232;&#30095;&#24615;&#22240;&#32032;&#20174;&#21477;&#23376;&#20013;&#33719;&#21462;&#30456;&#20851;&#32780;&#36830;&#36143;&#30340;&#29702;&#25454;&#65292;&#35299;&#20915;&#20102;&#22914;&#20309;&#20445;&#30041;&#30456;&#20851;&#20869;&#23481;&#24182;&#20174;&#21477;&#23376;&#20013;&#21435;&#25481;&#22122;&#38899;&#27573;&#33853;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#30340;&#27169;&#22411;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#24182;&#19988;&#25552;&#21462;&#20986;&#30340;&#29702;&#25454;&#23545;&#20110;&#25512;&#29702;&#21644;&#35299;&#37322;&#26159;&#26377;&#29992;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#31995;&#25552;&#21462;&#26088;&#22312;&#26681;&#25454;&#20004;&#20010;&#23454;&#20307;&#30340;&#35821;&#22659;&#25552;&#21462;&#28508;&#22312;&#20851;&#31995;&#65292;&#22240;&#27492;&#65292;&#20174;&#21477;&#23376;&#20013;&#25512;&#23548;&#20986;&#21512;&#29702;&#30340;&#35821;&#22659;&#38750;&#24120;&#37325;&#35201;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#35201;&#20040;&#19987;&#27880;&#20110;&#22914;&#20309;&#21033;&#29992;&#23454;&#20307;&#20449;&#24687;&#65288;&#20363;&#22914;&#65292;&#23454;&#20307;&#31867;&#22411;&#65292;&#23454;&#20307;&#29992;&#35821;&#65289;&#26469;&#25512;&#26029;&#20851;&#31995;&#65292;&#20294;&#24573;&#30053;&#20102;&#20197;&#35821;&#22659;&#20026;&#37325;&#28857;&#30340;&#20869;&#23481;&#65292;&#35201;&#20040;&#20351;&#29992;&#21453;&#20107;&#23454;&#24605;&#32500;&#26469;&#28040;&#38500;&#27169;&#22411;&#23545;&#23454;&#20307;&#28508;&#22312;&#20851;&#31995;&#30340;&#20559;&#35265;&#65292;&#20294;&#20851;&#31995;&#25512;&#29702;&#36807;&#31243;&#20173;&#20250;&#21463;&#21040;&#26080;&#20851;&#20869;&#23481;&#30340;&#24178;&#25200;&#12290;&#22240;&#27492;&#65292;&#22914;&#20309;&#20445;&#30041;&#26377;&#20851;&#20869;&#23481;&#24182;&#20174;&#21477;&#23376;&#20013;&#21435;&#25481;&#22122;&#38899;&#27573;&#33853;&#26159;&#19968;&#39033;&#20851;&#38190;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#20445;&#30041;&#30340;&#20869;&#23481;&#38656;&#35201;&#36275;&#22815;&#27969;&#30021;&#65292;&#20197;&#20445;&#25345;&#35821;&#20041;&#30340;&#36830;&#36143;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29702;&#25454;&#25277;&#21462;&#26694;&#26550;RE2&#65292;&#23427;&#21033;&#29992;&#20004;&#20010;&#36830;&#32493;&#24615;&#21644;&#31232;&#30095;&#24615;&#22240;&#32032;&#20174;&#21477;&#23376;&#20013;&#33719;&#21462;&#30456;&#20851;&#32780;&#36830;&#36143;&#30340;&#29702;&#25454;&#12290;&#20026;&#20102;&#35299;&#20915;&#40644;&#37329;&#29702;&#25454;&#26410;&#26631;&#35760;&#30340;&#38382;&#39064;&#65292;RE2&#24212;&#29992;&#19968;&#31181;&#26080;&#30417;&#30563;&#26041;&#27861;&#29983;&#25104;&#20505;&#36873;&#29702;&#25454;&#65292;&#24182;&#36873;&#25321;&#26368;&#30456;&#20851;&#21644;&#36830;&#36143;&#30340;&#29702;&#25454;&#26469;&#25351;&#23548;RE&#27169;&#22411;&#12290;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#24182;&#19988;&#25552;&#21462;&#20986;&#30340;&#29702;&#25454;&#23545;&#20110;&#25512;&#29702;&#21644;&#35299;&#37322;&#26159;&#26377;&#29992;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Relation extraction (RE) aims to extract potential relations according to the context of two entities, thus, deriving rational contexts from sentences plays an important role. Previous works either focus on how to leverage the entity information (e.g., entity types, entity verbalization) to inference relations, but ignore context-focused content, or use counterfactual thinking to remove the model's bias of potential relations in entities, but the relation reasoning process will still be hindered by irrelevant content. Therefore, how to preserve relevant content and remove noisy segments from sentences is a crucial task. In addition, retained content needs to be fluent enough to maintain semantic coherence and interpretability. In this work, we propose a novel rationale extraction framework named RE2, which leverages two continuity and sparsity factors to obtain relevant and coherent rationales from sentences. To solve the problem that the gold rationales are not labeled, RE2 applies an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;G2P2&#30340;&#27169;&#22411;&#65292;&#20351;&#29992;&#22270;&#35889;&#39044;&#35757;&#32451;&#21644;&#25552;&#31034;&#30340;&#26041;&#24335;&#22686;&#24378;&#20302;&#36164;&#28304;&#25991;&#26412;&#20998;&#31867;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#27169;&#22411;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.03324</link><description>&lt;p&gt;
&#29992;&#22270;&#35889;&#39044;&#35757;&#32451;&#21644;&#25552;&#31034;&#22686;&#24378;&#20302;&#36164;&#28304;&#25991;&#26412;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Augmenting Low-Resource Text Classification with Graph-Grounded Pre-training and Prompting. (arXiv:2305.03324v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03324
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;G2P2&#30340;&#27169;&#22411;&#65292;&#20351;&#29992;&#22270;&#35889;&#39044;&#35757;&#32451;&#21644;&#25552;&#31034;&#30340;&#26041;&#24335;&#22686;&#24378;&#20302;&#36164;&#28304;&#25991;&#26412;&#20998;&#31867;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#27169;&#22411;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#20998;&#31867;&#26159;&#20449;&#24687;&#26816;&#32034;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65292;&#20855;&#26377;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#65292;&#20363;&#22914;&#39044;&#27979;&#22312;&#32447;&#25991;&#31456;&#30340;&#20027;&#39064;&#21644;&#30005;&#23376;&#21830;&#21153;&#20135;&#21697;&#25551;&#36848;&#30340;&#31867;&#21035;&#12290;&#28982;&#32780;&#65292;&#20302;&#36164;&#28304;&#25991;&#26412;&#20998;&#31867;&#65292;&#27809;&#26377;&#25110;&#21482;&#26377;&#24456;&#23569;&#26631;&#35760;&#26679;&#26412;&#65292;&#23545;&#20110;&#30417;&#30563;&#23398;&#20064;&#26469;&#35828;&#26159;&#19968;&#20010;&#20005;&#37325;&#30340;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#35768;&#22810;&#25991;&#26412;&#25968;&#25454;&#26412;&#36136;&#19978;&#22522;&#20110;&#32593;&#32476;&#32467;&#26500;&#65292;&#20363;&#22914;&#22312;&#32447;&#25991;&#31456;&#30340;&#36229;&#38142;&#25509;/&#24341;&#29992;&#32593;&#32476;&#21644;&#30005;&#23376;&#21830;&#21153;&#20135;&#21697;&#30340;&#29992;&#25143;-&#39033;&#30446;&#36141;&#20080;&#32593;&#32476;&#12290;&#36825;&#20123;&#22270;&#24418;&#32467;&#26500;&#25429;&#25417;&#20102;&#20016;&#23500;&#30340;&#35821;&#20041;&#20851;&#31995;&#65292;&#21487;&#20197;&#28508;&#22312;&#22320;&#22686;&#24378;&#20302;&#36164;&#28304;&#25991;&#26412;&#20998;&#31867;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#65292;&#31216;&#20026;&#22270;&#24418;&#22522;&#30784;&#39044;&#35757;&#32451;&#21644;&#25552;&#31034;&#65288;G2P2&#65289;&#65292;&#20197;&#20004;&#20010;&#26041;&#38754;&#35299;&#20915;&#20302;&#36164;&#28304;&#25991;&#26412;&#20998;&#31867;&#12290;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#22522;&#20110;&#22270;&#24418;&#20132;&#20114;&#30340;&#23545;&#27604;&#31574;&#30053;&#65292;&#20197;&#32852;&#21512;&#39044;&#35757;&#32451;&#22270;&#24418;-&#25991;&#26412;&#27169;&#22411;&#65307;&#22312;&#19979;&#28216;&#20998;&#31867;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#25552;&#31034;&#36827;&#34892;&#20174;&#39640;&#36164;&#28304;&#21040;&#20302;&#36164;&#28304;&#20219;&#21153;&#30340;&#36801;&#31227;&#23398;&#20064;&#12290;&#22312;&#22235;&#20010;&#20302;&#36164;&#28304;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;G2P2&#26174;&#30528;&#20248;&#20110;&#20808;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#22270;&#24418;&#25509;&#22320;&#21644;&#25552;&#31034;&#31574;&#30053;&#23545;&#20110;&#21033;&#29992;&#36741;&#21161;&#30693;&#35782;&#36827;&#34892;&#20302;&#36164;&#28304;&#25991;&#26412;&#20998;&#31867;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text classification is a fundamental problem in information retrieval with many real-world applications, such as predicting the topics of online articles and the categories of e-commerce product descriptions. However, low-resource text classification, with few or no labeled samples, poses a serious concern for supervised learning. Meanwhile, many text data are inherently grounded on a network structure, such as a hyperlink/citation network for online articles, and a user-item purchase network for e-commerce products. These graph structures capture rich semantic relationships, which can potentially augment low-resource text classification. In this paper, we propose a novel model called Graph-Grounded Pre-training and Prompting (G2P2) to address low-resource text classification in a two-pronged approach. During pre-training, we propose three graph interaction-based contrastive strategies to jointly pre-train a graph-text model; during downstream classification, we explore prompting for t
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25506;&#32034;&#20102;&#19981;&#21516;&#25991;&#26412;&#23884;&#20837;&#23545;&#32858;&#31867;&#31639;&#27861;&#65288;KMeans&#12289;&#21333;&#38142;&#25509;&#32858;&#21512;&#31561;&#32423;&#12289;DBSCAN&#21644;HDBSCAN&#65289;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#24212;&#29992;&#20110;&#35780;&#35770;&#32858;&#31867;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2305.03144</link><description>&lt;p&gt;
&#25991;&#26412;&#23884;&#20837;&#23545;NLP&#32858;&#31867;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Influence of various text embeddings on clustering performance in NLP. (arXiv:2305.03144v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03144
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25506;&#32034;&#20102;&#19981;&#21516;&#25991;&#26412;&#23884;&#20837;&#23545;&#32858;&#31867;&#31639;&#27861;&#65288;KMeans&#12289;&#21333;&#38142;&#25509;&#32858;&#21512;&#31561;&#32423;&#12289;DBSCAN&#21644;HDBSCAN&#65289;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#24212;&#29992;&#20110;&#35780;&#35770;&#32858;&#31867;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#30340;&#20986;&#29616;&#65292;&#35780;&#35770;&#23545;&#20110;&#39038;&#23458;&#35780;&#20272;&#20135;&#21697;&#30340;&#21487;&#20449;&#24230;&#33267;&#20851;&#37325;&#35201;&#12290;&#20294;&#26159;&#65292;&#26143;&#32423;&#35780;&#20998;&#24182;&#19981;&#24635;&#26159;&#19982;&#39038;&#23458;&#32534;&#20889;&#30340;&#35780;&#35770;&#25991;&#26412;&#30456;&#21305;&#37197;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#36873;&#25321;&#19981;&#21516;&#25991;&#26412;&#23884;&#20837;&#26469;&#34920;&#31034;&#36825;&#20123;&#35780;&#35770;&#30340;&#20219;&#21153;&#65292;&#24182;&#25506;&#31350;&#20102;&#23884;&#20837;&#36873;&#25321;&#23545;&#21508;&#31181;&#31867;&#22411;&#32858;&#31867;&#31639;&#27861;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#20351;&#29992;&#19978;&#19979;&#25991;&#65288;BERT&#65289;&#21644;&#38750;&#19978;&#19979;&#25991;&#65288;Word2Vec&#65289;&#25991;&#26412;&#23884;&#20837;&#26469;&#34920;&#31034;&#25991;&#26412;&#65292;&#24182;&#27979;&#37327;&#23427;&#20204;&#23545;&#19977;&#31181;&#32858;&#31867;&#31639;&#27861;&#65288;&#22522;&#20110;&#20998;&#21306;&#30340;KMeans&#12289;&#21333;&#38142;&#25509;&#32858;&#21512;&#31561;&#32423;&#21644;&#23494;&#24230;&#22522;&#30784;&#30340;DBSCAN&#21644;HDBSCAN&#65289;&#22312;&#19981;&#21516;&#23454;&#39564;&#35774;&#32622;&#19979;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the advent of e-commerce platforms, reviews are crucial for customers to assess the credibility of a product. The star ratings do not always match the review text written by the customer. For example, a three star rating (out of five) may be incongruous with the review text, which may be more suitable for a five star review. A clustering approach can be used to relabel the correct star ratings by grouping the text reviews into individual groups. In this work, we explore the task of choosing different text embeddings to represent these reviews and also explore the impact the embedding choice has on the performance of various classes of clustering algorithms. We use contextual (BERT) and non-contextual (Word2Vec) text embeddings to represent the text and measure their impact of three classes on clustering algorithms - partitioning based (KMeans), single linkage agglomerative hierarchical, and density based (DBSCAN and HDBSCAN), each with various experimental settings. We use the sil
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#20010;&#26694;&#26550;&#65292;&#22522;&#20110;&#31038;&#20250;&#20154;&#21475;&#23398;&#30340;&#29305;&#28857;&#65292;&#23545;&#30284;&#30151;&#26816;&#26597;&#30340;&#30701;&#20449;&#25552;&#37266;&#27963;&#21160;&#36827;&#34892;&#20102;&#20248;&#21270;&#65292;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20809;&#22522;&#20110;&#36825;&#20123;&#29305;&#24449;&#36827;&#34892;&#30701;&#20449;&#25552;&#37266;&#27963;&#21160;&#21487;&#20197;&#23558;&#27515;&#20129;&#29575;&#30340;&#32479;&#35745;&#25968;&#37327;&#38477;&#20302;5.8&#65285;&#12290;</title><link>http://arxiv.org/abs/2305.03126</link><description>&lt;p&gt;
&#21033;&#29992;&#31038;&#20250;&#20154;&#21475;&#23398;&#20248;&#21270;&#21069;&#21518;&#30284;&#30151;&#26816;&#26597;&#30340;&#30701;&#20449;&#25552;&#37266;&#27963;&#21160;&#65306;&#23545;&#33152;&#33009;&#30284;&#30340;&#35745;&#31639;&#26426;&#27169;&#25311;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Optimizing SMS Reminder Campaigns for Pre- and Post-Diagnosis Cancer Check-Ups using Socio-Demographics: An In-Silco Investigation Into Bladder Cancer. (arXiv:2305.03126v1 [stat.AP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03126
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#20010;&#26694;&#26550;&#65292;&#22522;&#20110;&#31038;&#20250;&#20154;&#21475;&#23398;&#30340;&#29305;&#28857;&#65292;&#23545;&#30284;&#30151;&#26816;&#26597;&#30340;&#30701;&#20449;&#25552;&#37266;&#27963;&#21160;&#36827;&#34892;&#20102;&#20248;&#21270;&#65292;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20809;&#22522;&#20110;&#36825;&#20123;&#29305;&#24449;&#36827;&#34892;&#30701;&#20449;&#25552;&#37266;&#27963;&#21160;&#21487;&#20197;&#23558;&#27515;&#20129;&#29575;&#30340;&#32479;&#35745;&#25968;&#37327;&#38477;&#20302;5.8&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21450;&#26102;&#36827;&#34892;&#30284;&#30151;&#30340;&#39044;&#21518;&#21644;&#35786;&#26029;&#26816;&#26597;&#23545;&#20110;&#21508;&#31867;&#22411;&#30340;&#30284;&#30151;&#24739;&#32773;&#26469;&#35828;&#37117;&#33267;&#20851;&#37325;&#35201;&#65292;&#36825;&#36890;&#24120;&#20250;&#24102;&#26469;&#26356;&#22909;&#30340;&#27835;&#30103;&#25928;&#26524;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#29616;&#26377;&#30340;&#26816;&#26597;&#25919;&#31574;&#21482;&#32771;&#34385;&#19982;&#30284;&#30151;&#30340;&#20020;&#24202;&#21160;&#21147;&#23398;&#23494;&#20999;&#30456;&#20851;&#30340;&#23646;&#24615;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#21644;&#39640;&#20998;&#36776;&#29575;&#35745;&#31639;&#26426;&#27169;&#25311;&#65292;&#20197;&#35843;&#26597;&#21644;&#20248;&#21270;&#22522;&#20110;&#31038;&#20250;&#20154;&#21475;&#23398;&#30340;&#30284;&#30151;&#26816;&#26597;&#30701;&#20449;&#25552;&#37266;&#27963;&#21160;&#12290;&#25105;&#20204;&#21033;&#29992;&#22823;&#37327;&#30340;&#30495;&#23454;&#25968;&#25454;&#23545;&#33152;&#33009;&#30284;&#36827;&#34892;&#20102;&#26694;&#26550;&#21644;&#27169;&#25311;&#23454;&#20363;&#21270;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20165;&#22522;&#20110;&#31616;&#21333;&#30340;&#31038;&#20250;&#20154;&#21475;&#23398;&#29305;&#24449;&#20248;&#21270;&#30701;&#20449;&#25552;&#37266;&#27963;&#21160;&#21487;&#20197;&#23558;&#27515;&#20129;&#29575;&#30340;&#32479;&#35745;&#26174;&#33879;&#38477;&#20302;5.8&#65285;&#65292;&#19982;&#20854;&#20182;&#27963;&#21160;&#30456;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
Timely pre- and post-diagnosis check-ups are critical for cancer patients, across all cancer types, as these often lead to better outcomes. Several socio-demographic properties have been identified as strongly connected with both cancer's clinical dynamics and (indirectly) with different individual check-up behaviors. Unfortunately, existing check-up policies typically consider only the former association explicitly. In this work, we propose a novel framework, accompanied by a high-resolution computer simulation, to investigate and optimize socio-demographic-based SMS reminder campaigns for cancer check-ups. We instantiate our framework and simulation for the case of bladder cancer, the 10th most prevalent cancer today, using extensive real-world data. Our results indicate that optimizing an SMS reminder campaign based solely on simple socio-demographic features can bring about a statistically significant reduction in mortality rate compared to alternative campaigns by up to 5.8%.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28151;&#21512;&#25628;&#32034;&#30340;&#26041;&#27861;&#65292;&#23558;&#35789;&#27719;&#21644;&#35821;&#20041;&#25628;&#32034;&#34701;&#21512;&#22312;&#19968;&#36215;&#65292;&#22312;&#27492;&#22522;&#30784;&#19978;&#20998;&#26512;&#20102;&#21033;&#29992;&#20984;&#32452;&#21512;&#21644;&#20114;&#24800;&#25490;&#21517;&#34701;&#21512;&#20004;&#31181;&#26041;&#27861;&#30340;&#20248;&#32570;&#28857;&#12290;&#25105;&#20204;&#21457;&#29616;&#20984;&#32452;&#21512;&#27604;&#20114;&#24800;&#25490;&#21517;&#34701;&#21512;&#26356;&#21152;&#20248;&#31168;&#65292;&#20855;&#26377;&#26679;&#26412;&#25928;&#29575;&#65292;&#24182;&#19988;&#21482;&#38656;&#35201;&#23569;&#37327;&#35757;&#32451;&#38598;&#21363;&#21487;&#35843;&#25972;&#21442;&#25968;&#20197;&#36866;&#24212;&#30446;&#26631;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2210.11934</link><description>&lt;p&gt;
&#28151;&#21512;&#26816;&#32034;&#20013;&#30340;&#34701;&#21512;&#20989;&#25968;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
An Analysis of Fusion Functions for Hybrid Retrieval. (arXiv:2210.11934v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.11934
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28151;&#21512;&#25628;&#32034;&#30340;&#26041;&#27861;&#65292;&#23558;&#35789;&#27719;&#21644;&#35821;&#20041;&#25628;&#32034;&#34701;&#21512;&#22312;&#19968;&#36215;&#65292;&#22312;&#27492;&#22522;&#30784;&#19978;&#20998;&#26512;&#20102;&#21033;&#29992;&#20984;&#32452;&#21512;&#21644;&#20114;&#24800;&#25490;&#21517;&#34701;&#21512;&#20004;&#31181;&#26041;&#27861;&#30340;&#20248;&#32570;&#28857;&#12290;&#25105;&#20204;&#21457;&#29616;&#20984;&#32452;&#21512;&#27604;&#20114;&#24800;&#25490;&#21517;&#34701;&#21512;&#26356;&#21152;&#20248;&#31168;&#65292;&#20855;&#26377;&#26679;&#26412;&#25928;&#29575;&#65292;&#24182;&#19988;&#21482;&#38656;&#35201;&#23569;&#37327;&#35757;&#32451;&#38598;&#21363;&#21487;&#35843;&#25972;&#21442;&#25968;&#20197;&#36866;&#24212;&#30446;&#26631;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#25991;&#26412;&#26816;&#32034;&#20013;&#28151;&#21512;&#25628;&#32034;&#30340;&#26041;&#27861;&#65292;&#23558;&#35789;&#27719;&#21644;&#35821;&#20041;&#25628;&#32034;&#34701;&#21512;&#22312;&#19968;&#36215;&#65292;&#22240;&#20026;&#23427;&#20204;&#23545;&#20110;&#27169;&#22411;&#30456;&#20851;&#24615;&#30340;&#24314;&#27169;&#20855;&#26377;&#20114;&#34917;&#24615;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36890;&#36807;&#35789;&#27719;&#21644;&#35821;&#20041;&#35780;&#20998;&#30340;&#20984;&#32452;&#21512;&#65288;CC&#65289;&#36827;&#34892;&#34701;&#21512;&#65292;&#20197;&#21450;&#20114;&#24800;&#25490;&#21517;&#34701;&#21512;&#65288;RRF&#65289;&#26041;&#27861;&#65292;&#24182;&#30830;&#23450;&#23427;&#20204;&#30340;&#20248;&#28857;&#21644;&#28508;&#22312;&#32570;&#38519;&#12290;&#19982;&#29616;&#26377;&#30740;&#31350;&#30456;&#21453;&#65292;&#25105;&#20204;&#21457;&#29616;RRF&#23545;&#20854;&#21442;&#25968;&#24456;&#25935;&#24863;&#65307;CC&#34701;&#21512;&#30340;&#23398;&#20064;&#36890;&#24120;&#19981;&#20851;&#24515;&#35780;&#20998;&#35268;&#33539;&#30340;&#36873;&#25321;&#65307;CC&#22312;&#22495;&#20869;&#21644;&#22495;&#22806;&#35774;&#32622;&#20013;&#20248;&#20110;RRF&#65307;&#26368;&#21518;&#65292;CC&#20855;&#26377;&#26679;&#26412;&#25928;&#29575;&#65292;&#21482;&#38656;&#35201;&#23569;&#37327;&#35757;&#32451;&#31034;&#20363;&#21363;&#21487;&#35843;&#25972;&#20854;&#21807;&#19968;&#21442;&#25968;&#20197;&#36866;&#24212;&#30446;&#26631;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study hybrid search in text retrieval where lexical and semantic search are fused together with the intuition that the two are complementary in how they model relevance. In particular, we examine fusion by a convex combination (CC) of lexical and semantic scores, as well as the Reciprocal Rank Fusion (RRF) method, and identify their advantages and potential pitfalls. Contrary to existing studies, we find RRF to be sensitive to its parameters; that the learning of a CC fusion is generally agnostic to the choice of score normalization; that CC outperforms RRF in in-domain and out-of-domain settings; and finally, that CC is sample efficient, requiring only a small set of training examples to tune its only parameter to a target domain.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23569;&#26679;&#26412;&#30340;&#36923;&#36753;&#30693;&#35782;&#26465;&#20214;&#19979;&#25991;&#26412;&#29983;&#25104;&#30340;&#32479;&#19968;&#26694;&#26550;LOGEN&#65292;&#36890;&#36807;&#33258;&#35757;&#32451;&#21644;&#22522;&#20110;&#20869;&#23481;&#21644;&#32467;&#26500;&#19968;&#33268;&#24615;&#25277;&#26679;&#20266;&#36923;&#36753;&#24418;&#24335;&#65292;&#23454;&#29616;&#20102;&#22312;&#23569;&#37327;&#26679;&#26412;&#19979;&#30340;&#25991;&#26412;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2112.01404</link><description>&lt;p&gt;
LOGEN&#65306;&#22522;&#20110;&#36923;&#36753;&#30693;&#35782;&#26465;&#20214;&#30340;&#33258;&#35757;&#32451;&#25991;&#26412;&#29983;&#25104;&#22312;&#23569;&#26679;&#26412;&#19979;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
LOGEN: Few-shot Logical Knowledge-Conditioned Text Generation with Self-training. (arXiv:2112.01404v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.01404
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23569;&#26679;&#26412;&#30340;&#36923;&#36753;&#30693;&#35782;&#26465;&#20214;&#19979;&#25991;&#26412;&#29983;&#25104;&#30340;&#32479;&#19968;&#26694;&#26550;LOGEN&#65292;&#36890;&#36807;&#33258;&#35757;&#32451;&#21644;&#22522;&#20110;&#20869;&#23481;&#21644;&#32467;&#26500;&#19968;&#33268;&#24615;&#25277;&#26679;&#20266;&#36923;&#36753;&#24418;&#24335;&#65292;&#23454;&#29616;&#20102;&#22312;&#23569;&#37327;&#26679;&#26412;&#19979;&#30340;&#25991;&#26412;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20027;&#35201;&#38598;&#20013;&#22312;&#34920;&#38754;&#23618;&#38754;&#25551;&#36848;&#65292;&#20854;&#23384;&#22312;&#25511;&#21046;&#20869;&#23481;&#36873;&#25321;&#22256;&#38590;&#21644;&#20302;&#20445;&#30495;&#24230;&#30340;&#38382;&#39064;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#21033;&#29992;&#36923;&#36753;&#24418;&#24335;&#26469;&#20419;&#36827;&#36923;&#36753;&#30693;&#35782;&#26465;&#20214;&#19979;&#30340;&#25991;&#26412;&#29983;&#25104;&#12290;&#34429;&#28982;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#26159;&#23427;&#20204;&#23545;&#25968;&#25454;&#30340;&#38656;&#27714;&#37327;&#36739;&#22823;&#65292;&#36825;&#20351;&#24471;&#22312;&#26377;&#38480;&#25968;&#25454;&#24773;&#20917;&#19979;&#24212;&#29992;&#20110;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23569;&#26679;&#26412;&#30340;&#36923;&#36753;&#30693;&#35782;&#26465;&#20214;&#19979;&#25991;&#26412;&#29983;&#25104;&#30340;&#32479;&#19968;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21482;&#20351;&#29992;&#23569;&#37327;&#31181;&#23376;&#36923;&#36753;&#24418;&#24335;&#65288;&#22914;20/100&#31181;&#23376;&#65289; &#65292;&#24182;&#21033;&#29992;&#33258;&#35757;&#32451;&#21644;&#22522;&#20110;&#20869;&#23481;&#21644;&#32467;&#26500;&#19968;&#33268;&#24615;&#25277;&#26679;&#20266;&#36923;&#36753;&#24418;&#24335;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#27604;&#22522;&#20934;&#26041;&#27861;&#33719;&#24471;&#26356;&#22909;&#30340;&#23569;&#26679;&#26412;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural language generation from structured data mainly focuses on surface-level descriptions, suffering from uncontrollable content selection and low fidelity. Previous works leverage logical forms to facilitate logical knowledge-conditioned text generation. Though achieving remarkable progress, they are data-hungry, which makes the adoption for real-world applications challenging with limited data. To this end, this paper proposes a unified framework for logical knowledge-conditioned text generation in the few-shot setting. With only a few seeds logical forms (e.g., 20/100 shot), our approach leverages self-training and samples pseudo logical forms based on content and structure consistency. Experimental results demonstrate that our approach can obtain better few-shot performance than baselines.
&lt;/p&gt;</description></item></channel></rss>