<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#26412;&#30740;&#31350;&#23558;&#31616;&#21382;&#35299;&#26512;&#38382;&#39064;&#20316;&#20026;&#20998;&#23618;&#24207;&#21015;&#26631;&#27880;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#21516;&#26102;&#35299;&#20915;&#34892;&#21644;&#26631;&#35760;&#20004;&#20010;&#20219;&#21153;&#30340;&#27169;&#22411;&#26550;&#26500;&#65292;&#24182;&#26500;&#24314;&#20102;&#22810;&#35821;&#35328;&#30340;&#39640;&#36136;&#37327;&#31616;&#21382;&#35299;&#26512;&#35821;&#26009;&#24211;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#27169;&#22411;&#22312;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#20013;&#20248;&#20110;&#20808;&#21069;&#24037;&#20316;&#20013;&#30340;&#26041;&#27861;&#12290;&#36827;&#19968;&#27493;&#20998;&#26512;&#20102;&#27169;&#22411;&#24615;&#33021;&#21644;&#36164;&#28304;&#25928;&#29575;&#65292;&#24182;&#25551;&#36848;&#20102;&#27169;&#22411;&#22312;&#29983;&#20135;&#29615;&#22659;&#20013;&#30340;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2309.07015</link><description>&lt;p&gt;
&#31616;&#21382;&#35299;&#26512;&#20316;&#20026;&#20998;&#23618;&#24207;&#21015;&#26631;&#27880;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
R\'esum\'e Parsing as Hierarchical Sequence Labeling: An Empirical Study. (arXiv:2309.07015v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07015
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#31616;&#21382;&#35299;&#26512;&#38382;&#39064;&#20316;&#20026;&#20998;&#23618;&#24207;&#21015;&#26631;&#27880;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#21516;&#26102;&#35299;&#20915;&#34892;&#21644;&#26631;&#35760;&#20004;&#20010;&#20219;&#21153;&#30340;&#27169;&#22411;&#26550;&#26500;&#65292;&#24182;&#26500;&#24314;&#20102;&#22810;&#35821;&#35328;&#30340;&#39640;&#36136;&#37327;&#31616;&#21382;&#35299;&#26512;&#35821;&#26009;&#24211;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#27169;&#22411;&#22312;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#20013;&#20248;&#20110;&#20808;&#21069;&#24037;&#20316;&#20013;&#30340;&#26041;&#27861;&#12290;&#36827;&#19968;&#27493;&#20998;&#26512;&#20102;&#27169;&#22411;&#24615;&#33021;&#21644;&#36164;&#28304;&#25928;&#29575;&#65292;&#24182;&#25551;&#36848;&#20102;&#27169;&#22411;&#22312;&#29983;&#20135;&#29615;&#22659;&#20013;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#31616;&#21382;&#20013;&#25552;&#21462;&#20449;&#24687;&#36890;&#24120;&#34987;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#38382;&#39064;&#65292;&#21363;&#39318;&#20808;&#23558;&#25991;&#26723;&#20998;&#27573;&#65292;&#28982;&#21518;&#23545;&#27599;&#20010;&#27573;&#33853;&#36827;&#34892;&#21333;&#29420;&#22788;&#29702;&#20197;&#25552;&#21462;&#30446;&#26631;&#23454;&#20307;&#12290;&#25105;&#20204;&#23558;&#25972;&#20010;&#38382;&#39064;&#20998;&#20026;&#20004;&#20010;&#32423;&#21035;&#30340;&#24207;&#21015;&#26631;&#27880;&#20219;&#21153;&#65292;&#21363;&#34892;&#21644;&#26631;&#35760;&#65292;&#24182;&#30740;&#31350;&#20102;&#21516;&#26102;&#35299;&#20915;&#36825;&#20004;&#20010;&#20219;&#21153;&#30340;&#27169;&#22411;&#26550;&#26500;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#33521;&#35821;&#12289;&#27861;&#35821;&#12289;&#20013;&#25991;&#12289;&#35199;&#29677;&#29273;&#35821;&#12289;&#24503;&#35821;&#12289;&#33889;&#33796;&#29273;&#35821;&#21644;&#29790;&#20856;&#35821;&#30340;&#39640;&#36136;&#37327;&#31616;&#21382;&#35299;&#26512;&#35821;&#26009;&#24211;&#12290;&#22522;&#20110;&#36825;&#20123;&#35821;&#26009;&#24211;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23454;&#39564;&#32467;&#26524;&#65292;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#27169;&#22411;&#22312;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#20248;&#20110;&#20808;&#21069;&#24037;&#20316;&#20013;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23545;&#25152;&#25552;&#20986;&#30340;&#26550;&#26500;&#36827;&#34892;&#20102;&#28040;&#34701;&#30740;&#31350;&#12290;&#25105;&#20204;&#36824;&#20998;&#26512;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#36164;&#28304;&#25928;&#29575;&#65292;&#24182;&#25551;&#36848;&#20102;&#22312;&#29983;&#20135;&#29615;&#22659;&#20013;&#36827;&#34892;&#27169;&#22411;&#37096;&#32626;&#26102;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Extracting information from r\'esum\'es is typically formulated as a two-stage problem, where the document is first segmented into sections and then each section is processed individually to extract the target entities. Instead, we cast the whole problem as sequence labeling in two levels -- lines and tokens -- and study model architectures for solving both tasks simultaneously. We build high-quality r\'esum\'e parsing corpora in English, French, Chinese, Spanish, German, Portuguese, and Swedish. Based on these corpora, we present experimental results that demonstrate the effectiveness of the proposed models for the information extraction task, outperforming approaches introduced in previous work. We conduct an ablation study of the proposed architectures. We also analyze both model performance and resource efficiency, and describe the trade-offs for model deployment in the context of a production environment.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#20351;&#29992;&#35821;&#20041;&#32593;&#25216;&#26415;&#23545;&#20301;&#38169;&#21160;&#21147;&#23398;&#27169;&#25311;&#25968;&#25454;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#36890;&#36807;&#28155;&#21152;&#32570;&#22833;&#30340;&#27010;&#24565;&#21644;&#19982;&#30456;&#20851;&#26412;&#20307;&#23545;&#40784;&#26469;&#25193;&#23637;&#20301;&#38169;&#26412;&#20307;&#12290;</title><link>http://arxiv.org/abs/2309.06930</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#20041;&#32593;&#25216;&#26415;&#23545;&#20301;&#38169;&#21160;&#21147;&#23398;&#25968;&#25454;&#36827;&#34892;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Modeling Dislocation Dynamics Data Using Semantic Web Technologies. (arXiv:2309.06930v1 [cond-mat.mtrl-sci])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06930
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#20351;&#29992;&#35821;&#20041;&#32593;&#25216;&#26415;&#23545;&#20301;&#38169;&#21160;&#21147;&#23398;&#27169;&#25311;&#25968;&#25454;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#36890;&#36807;&#28155;&#21152;&#32570;&#22833;&#30340;&#27010;&#24565;&#21644;&#19982;&#30456;&#20851;&#26412;&#20307;&#23545;&#40784;&#26469;&#25193;&#23637;&#20301;&#38169;&#26412;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26448;&#26009;&#31185;&#23398;&#19982;&#24037;&#31243;&#39046;&#22495;&#30340;&#30740;&#31350;&#30528;&#30524;&#20110;&#26448;&#26009;&#30340;&#35774;&#35745;&#12289;&#21512;&#25104;&#12289;&#24615;&#33021;&#21644;&#24615;&#33021;&#12290;&#34987;&#24191;&#27867;&#30740;&#31350;&#30340;&#19968;&#20010;&#37325;&#35201;&#26448;&#26009;&#31867;&#21035;&#26159;&#26230;&#20307;&#26448;&#26009;&#65292;&#21253;&#25324;&#37329;&#23646;&#21644;&#21322;&#23548;&#20307;&#12290;&#26230;&#20307;&#26448;&#26009;&#36890;&#24120;&#21253;&#21547;&#19968;&#31181;&#31216;&#20026;&#8220;&#20301;&#38169;&#8221;&#30340;&#29305;&#27530;&#32570;&#38519;&#12290;&#36825;&#31181;&#32570;&#38519;&#26174;&#33879;&#24433;&#21709;&#21508;&#31181;&#26448;&#26009;&#24615;&#33021;&#65292;&#21253;&#25324;&#24378;&#24230;&#12289;&#26029;&#35010;&#38887;&#24615;&#21644;&#24310;&#23637;&#24615;&#12290;&#36817;&#24180;&#26469;&#65292;&#30740;&#31350;&#20154;&#21592;&#36890;&#36807;&#23454;&#39564;&#34920;&#24449;&#25216;&#26415;&#21644;&#27169;&#25311;&#65288;&#22914;&#20301;&#38169;&#21160;&#21147;&#23398;&#27169;&#25311;&#65289;&#33268;&#21147;&#20110;&#29702;&#35299;&#20301;&#38169;&#34892;&#20026;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#36890;&#36807;&#20351;&#29992;&#35821;&#20041;&#32593;&#25216;&#26415;&#20197;&#26412;&#20307;&#26041;&#24335;&#23545;&#20301;&#38169;&#21160;&#21147;&#23398;&#27169;&#25311;&#25968;&#25454;&#36827;&#34892;&#24314;&#27169;&#12290;&#25105;&#20204;&#36890;&#36807;&#28155;&#21152;&#32570;&#22833;&#30340;&#27010;&#24565;&#24182;&#23558;&#20854;&#19982;&#20854;&#20182;&#20004;&#20010;&#19982;&#35813;&#39046;&#22495;&#30456;&#20851;&#30340;&#26412;&#20307;&#65288;&#21363;Elementary Multi-perspectiv&#65289;&#36827;&#34892;&#23545;&#40784;&#26469;&#25193;&#23637;&#24050;&#26377;&#30340;&#20301;&#38169;&#26412;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
Research in the field of Materials Science and Engineering focuses on the design, synthesis, properties, and performance of materials. An important class of materials that is widely investigated are crystalline materials, including metals and semiconductors. Crystalline material typically contains a distinct type of defect called "dislocation". This defect significantly affects various material properties, including strength, fracture toughness, and ductility. Researchers have devoted a significant effort in recent years to understanding dislocation behavior through experimental characterization techniques and simulations, e.g., dislocation dynamics simulations. This paper presents how data from dislocation dynamics simulations can be modeled using semantic web technologies through annotating data with ontologies. We extend the already existing Dislocation Ontology by adding missing concepts and aligning it with two other domain-related ontologies (i.e., the Elementary Multi-perspectiv
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;SVD&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22810;&#34892;&#20026;&#25512;&#33616;&#30340;&#27169;&#22411;MB-SVD&#65292;&#36890;&#36807;&#32771;&#34385;&#29992;&#25143;&#22312;&#19981;&#21516;&#34892;&#20026;&#19979;&#30340;&#20559;&#22909;&#65292;&#25913;&#21892;&#20102;&#25512;&#33616;&#25928;&#26524;&#65292;&#21516;&#26102;&#26356;&#22909;&#22320;&#35299;&#20915;&#20102;&#20919;&#21551;&#21160;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.06912</link><description>&lt;p&gt;
&#29992;SVD&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22810;&#34892;&#20026;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Multi-behavior Recommendation with SVD Graph Neural Networks. (arXiv:2309.06912v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06912
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;SVD&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22810;&#34892;&#20026;&#25512;&#33616;&#30340;&#27169;&#22411;MB-SVD&#65292;&#36890;&#36807;&#32771;&#34385;&#29992;&#25143;&#22312;&#19981;&#21516;&#34892;&#20026;&#19979;&#30340;&#20559;&#22909;&#65292;&#25913;&#21892;&#20102;&#25512;&#33616;&#25928;&#26524;&#65292;&#21516;&#26102;&#26356;&#22909;&#22320;&#35299;&#20915;&#20102;&#20919;&#21551;&#21160;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#24191;&#27867;&#24212;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#39046;&#22495;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#20010;&#24615;&#21270;&#25512;&#33616;&#24182;&#21462;&#24471;&#26174;&#33879;&#25104;&#26524;&#12290;&#26368;&#36817;&#65292;&#34701;&#20837;&#23545;&#27604;&#23398;&#20064;&#30340;GNNs&#22312;&#22788;&#29702;&#25512;&#33616;&#31995;&#32479;&#30340;&#31232;&#30095;&#25968;&#25454;&#38382;&#39064;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#22312;&#35299;&#20915;&#20919;&#21551;&#21160;&#38382;&#39064;&#21644;&#25269;&#25239;&#22122;&#22768;&#24178;&#25200;&#26041;&#38754;&#20173;&#28982;&#23384;&#22312;&#38480;&#21046;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#22810;&#34892;&#20026;&#25512;&#33616;&#12290;&#20026;&#20102;&#32531;&#35299;&#19978;&#36848;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GNNs&#30340;&#22810;&#34892;&#20026;&#25512;&#33616;&#27169;&#22411;MB-SVD&#65292;&#21033;&#29992;&#22855;&#24322;&#20540;&#20998;&#35299;(SVD)&#22270;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;MB-SVD&#32771;&#34385;&#20102;&#29992;&#25143;&#22312;&#19981;&#21516;&#34892;&#20026;&#19979;&#30340;&#20559;&#22909;&#65292;&#25913;&#21892;&#20102;&#25512;&#33616;&#25928;&#26524;&#65292;&#21516;&#26102;&#26356;&#22909;&#22320;&#35299;&#20915;&#20102;&#20919;&#21551;&#21160;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#35770;&#65292;&#23558;&#22810;&#34892;&#20026;&#23545;&#27604;&#23398;&#20064;&#33539;&#24335;&#34701;&#20837;&#21040;&#27169;&#22411;&#20013;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) has been extensively employed in the field of recommender systems, offering users personalized recommendations and yielding remarkable outcomes. Recently, GNNs incorporating contrastive learning have demonstrated promising performance in handling sparse data problem of recommendation system. However, existing contrastive learning methods still have limitations in addressing the cold-start problem and resisting noise interference especially for multi-behavior recommendation. To mitigate the aforementioned issues, the present research posits a GNNs based multi-behavior recommendation model MB-SVD that utilizes Singular Value Decomposition (SVD) graphs to enhance model performance. In particular, MB-SVD considers user preferences under different behaviors, improving recommendation effectiveness while better addressing the cold-start problem. Our model introduces an innovative methodology, which subsume multi-behavior contrastive learning paradigm to proficient
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;TopMost&#30340;&#20027;&#39064;&#24314;&#27169;&#31995;&#32479;&#24037;&#20855;&#21253;&#65292;&#36890;&#36807;&#28085;&#30422;&#26356;&#24191;&#27867;&#30340;&#20027;&#39064;&#24314;&#27169;&#22330;&#26223;&#21644;&#20855;&#26377;&#39640;&#24230;&#20957;&#32858;&#21147;&#21644;&#35299;&#32806;&#27169;&#22359;&#21270;&#35774;&#35745;&#30340;&#29305;&#28857;&#65292;&#21487;&#20197;&#20419;&#36827;&#20027;&#39064;&#27169;&#22411;&#30340;&#30740;&#31350;&#21644;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2309.06908</link><description>&lt;p&gt;
&#36208;&#21521;TopMost&#65306;&#19968;&#20010;&#20027;&#39064;&#24314;&#27169;&#31995;&#32479;&#24037;&#20855;&#21253;
&lt;/p&gt;
&lt;p&gt;
Towards the TopMost: A Topic Modeling System Toolkit. (arXiv:2309.06908v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06908
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;TopMost&#30340;&#20027;&#39064;&#24314;&#27169;&#31995;&#32479;&#24037;&#20855;&#21253;&#65292;&#36890;&#36807;&#28085;&#30422;&#26356;&#24191;&#27867;&#30340;&#20027;&#39064;&#24314;&#27169;&#22330;&#26223;&#21644;&#20855;&#26377;&#39640;&#24230;&#20957;&#32858;&#21147;&#21644;&#35299;&#32806;&#27169;&#22359;&#21270;&#35774;&#35745;&#30340;&#29305;&#28857;&#65292;&#21487;&#20197;&#20419;&#36827;&#20027;&#39064;&#27169;&#22411;&#30340;&#30740;&#31350;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#39064;&#27169;&#22411;&#24050;&#32463;&#22312;&#36807;&#21435;&#20960;&#21313;&#24180;&#20013;&#34987;&#25552;&#20986;&#65292;&#24182;&#19988;&#20855;&#26377;&#21508;&#31181;&#24212;&#29992;&#65292;&#22312;&#31070;&#32463;&#21464;&#20998;&#25512;&#26029;&#30340;&#25512;&#21160;&#19979;&#36817;&#26399;&#24471;&#21040;&#20102;&#26356;&#26032;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20027;&#39064;&#27169;&#22411;&#37319;&#29992;&#23436;&#20840;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#12289;&#23454;&#29616;&#21644;&#35780;&#20272;&#35774;&#32622;&#65292;&#36825;&#38459;&#30861;&#20102;&#23427;&#20204;&#30340;&#24555;&#36895;&#21033;&#29992;&#21644;&#20844;&#24179;&#27604;&#36739;&#12290;&#36825;&#20005;&#37325;&#38459;&#30861;&#20102;&#20027;&#39064;&#27169;&#22411;&#30340;&#30740;&#31350;&#36827;&#23637;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20027;&#39064;&#24314;&#27169;&#31995;&#32479;&#24037;&#20855;&#21253;&#65288;TopMost&#65289;&#12290;&#19982;&#29616;&#26377;&#30340;&#24037;&#20855;&#21253;&#30456;&#27604;&#65292;TopMost&#36890;&#36807;&#28085;&#30422;&#26356;&#24191;&#27867;&#30340;&#20027;&#39064;&#24314;&#27169;&#22330;&#26223;&#65292;&#21253;&#25324;&#25968;&#25454;&#38598;&#39044;&#22788;&#29702;&#12289;&#27169;&#22411;&#35757;&#32451;&#12289;&#27979;&#35797;&#21644;&#35780;&#20272;&#30340;&#23436;&#25972;&#29983;&#21629;&#21608;&#26399;&#65292;&#33073;&#39062;&#32780;&#20986;&#12290;TopMost&#30340;&#39640;&#24230;&#20957;&#32858;&#21147;&#21644;&#35299;&#32806;&#27169;&#22359;&#21270;&#35774;&#35745;&#21487;&#20197;&#24555;&#36895;&#21033;&#29992;&#65292;&#20844;&#24179;&#27604;&#36739;&#65292;&#24182;&#28789;&#27963;&#25193;&#23637;&#19981;&#21516;&#30340;&#20027;&#39064;&#27169;&#22411;&#65292;&#36825;&#21487;&#20197;&#20419;&#36827;&#20027;&#39064;&#27169;&#22411;&#30340;&#30740;&#31350;&#21644;&#24212;&#29992;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#12289;&#25945;&#31243;&#21644;&#25991;&#26723;&#21487;&#22312;https://github.com/bobxwu/topmost &#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Topic models have been proposed for decades with various applications and recently refreshed by the neural variational inference. However, these topic models adopt totally distinct dataset, implementation, and evaluation settings, which hinders their quick utilization and fair comparisons. This greatly hinders the research progress of topic models. To address these issues, in this paper we propose a Topic Modeling System Toolkit (TopMost). Compared to existing toolkits, TopMost stands out by covering a wider range of topic modeling scenarios including complete lifecycles with dataset pre-processing, model training, testing, and evaluations. The highly cohesive and decoupled modular design of TopMost enables quick utilization, fair comparisons, and flexible extensions of different topic models. This can facilitate the research and applications of topic models. Our code, tutorials, and documentation are available at https://github.com/bobxwu/topmost.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#20004;&#20010;&#26032;&#30340;&#20135;&#21697;&#26144;&#23556;&#25968;&#25454;&#38598;&#65306;ProMapCz&#21644;ProMapEn&#65292;&#20998;&#21035;&#21253;&#21547;&#25463;&#20811;&#21644;&#33521;&#25991;&#20135;&#21697;&#23545;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#20855;&#26377;&#36739;&#20026;&#23436;&#25972;&#30340;&#20135;&#21697;&#20449;&#24687;&#65292;&#24182;&#35299;&#20915;&#20102;&#30446;&#21069;&#29616;&#26377;&#25968;&#25454;&#38598;&#26080;&#27861;&#21306;&#20998;&#38750;&#24120;&#30456;&#20284;&#20294;&#19981;&#21305;&#37197;&#20135;&#21697;&#23545;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.06882</link><description>&lt;p&gt;
ProMap&#65306;&#30005;&#23376;&#21830;&#21153;&#20135;&#21697;&#26144;&#23556;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
ProMap: Datasets for Product Mapping in E-commerce. (arXiv:2309.06882v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06882
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#20004;&#20010;&#26032;&#30340;&#20135;&#21697;&#26144;&#23556;&#25968;&#25454;&#38598;&#65306;ProMapCz&#21644;ProMapEn&#65292;&#20998;&#21035;&#21253;&#21547;&#25463;&#20811;&#21644;&#33521;&#25991;&#20135;&#21697;&#23545;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#20855;&#26377;&#36739;&#20026;&#23436;&#25972;&#30340;&#20135;&#21697;&#20449;&#24687;&#65292;&#24182;&#35299;&#20915;&#20102;&#30446;&#21069;&#29616;&#26377;&#25968;&#25454;&#38598;&#26080;&#27861;&#21306;&#20998;&#38750;&#24120;&#30456;&#20284;&#20294;&#19981;&#21305;&#37197;&#20135;&#21697;&#23545;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20135;&#21697;&#26144;&#23556;&#30340;&#30446;&#26631;&#26159;&#30830;&#23450;&#20004;&#20010;&#19981;&#21516;&#30005;&#23376;&#21830;&#24215;&#20013;&#30340;&#20004;&#20010;&#21015;&#34920;&#26159;&#21542;&#25551;&#36848;&#30456;&#21516;&#30340;&#20135;&#21697;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#21305;&#37197;&#21644;&#38750;&#21305;&#37197;&#20135;&#21697;&#23545;&#30340;&#25968;&#25454;&#38598;&#32463;&#24120;&#21463;&#21040;&#20135;&#21697;&#20449;&#24687;&#19981;&#23436;&#25972;&#25110;&#32773;&#21482;&#21253;&#21547;&#38750;&#24120;&#36828;&#30340;&#38750;&#21305;&#37197;&#20135;&#21697;&#30340;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#23613;&#31649;&#22312;&#36825;&#20123;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#39044;&#27979;&#27169;&#22411;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#65292;&#20294;&#23454;&#38469;&#19978;&#23427;&#20204;&#26080;&#27861;&#21306;&#20998;&#38750;&#24120;&#30456;&#20284;&#20294;&#19981;&#21305;&#37197;&#30340;&#20135;&#21697;&#23545;&#65292;&#22240;&#27492;&#26080;&#27861;&#20351;&#29992;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#20010;&#26032;&#30340;&#20135;&#21697;&#26144;&#23556;&#25968;&#25454;&#38598;&#65306;ProMapCz&#21253;&#21547;1495&#23545;&#25463;&#20811;&#20135;&#21697;&#65292;ProMapEn&#21253;&#21547;1555&#23545;&#33521;&#25991;&#20135;&#21697;&#65292;&#36825;&#20123;&#20135;&#21697;&#23545;&#26469;&#33258;&#20004;&#20010;&#30005;&#23376;&#21830;&#24215;&#65292;&#21253;&#21547;&#20102;&#20135;&#21697;&#30340;&#22270;&#20687;&#21644;&#25991;&#23383;&#25551;&#36848;&#65292;&#21253;&#25324;&#35268;&#26684;&#65292;&#20351;&#23427;&#20204;&#25104;&#20026;&#26368;&#23436;&#25972;&#30340;&#20135;&#21697;&#26144;&#23556;&#25968;&#25454;&#38598;&#20043;&#19968;&#12290;&#27492;&#22806;&#65292;&#38750;&#21305;&#37197;&#20135;&#21697;&#26159;&#36890;&#36807;&#20004;&#20010;&#38454;&#27573;&#36827;&#34892;&#36873;&#25321;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of product mapping is to decide, whether two listings from two different e-shops describe the same products. Existing datasets of matching and non-matching pairs of products, however, often suffer from incomplete product information or contain only very distant non-matching products. Therefore, while predictive models trained on these datasets achieve good results on them, in practice, they are unusable as they cannot distinguish very similar but non-matching pairs of products. This paper introduces two new datasets for product mapping: ProMapCz consisting of 1,495 Czech product pairs and ProMapEn consisting of 1,555 English product pairs of matching and non-matching products manually scraped from two pairs of e-shops. The datasets contain both images and textual descriptions of the products, including their specifications, making them one of the most complete datasets for product mapping. Additionally, the non-matching products were selected in two phases, creating two types 
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#30740;&#31350;&#25552;&#20986;&#20102;PixelRec&#65292;&#19968;&#20010;&#22823;&#35268;&#27169;&#22270;&#20687;&#25512;&#33616;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;2&#20159;&#20010;&#29992;&#25143;-&#22270;&#20687;&#20132;&#20114;&#12289;30&#19975;&#20010;&#29992;&#25143;&#21644;40&#19975;&#20010;&#39640;&#36136;&#37327;&#23553;&#38754;&#22270;&#20687;&#12290;&#36890;&#36807;&#25552;&#20379;&#21407;&#22987;&#22270;&#20687;&#20687;&#32032;&#30340;&#30452;&#25509;&#35775;&#38382;&#65292;PixelRec&#20351;&#24471;&#25512;&#33616;&#27169;&#22411;&#33021;&#22815;&#30452;&#25509;&#20174;&#22270;&#20687;&#23398;&#20064;&#39033;&#30446;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2309.06789</link><description>&lt;p&gt;
&#20351;&#29992;&#21407;&#22987;&#20687;&#32032;&#20026;&#22522;&#20934;&#30340;&#25512;&#33616;&#31995;&#32479;&#22270;&#20687;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
An Image Dataset for Benchmarking Recommender Systems with Raw Pixels. (arXiv:2309.06789v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06789
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#30740;&#31350;&#25552;&#20986;&#20102;PixelRec&#65292;&#19968;&#20010;&#22823;&#35268;&#27169;&#22270;&#20687;&#25512;&#33616;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;2&#20159;&#20010;&#29992;&#25143;-&#22270;&#20687;&#20132;&#20114;&#12289;30&#19975;&#20010;&#29992;&#25143;&#21644;40&#19975;&#20010;&#39640;&#36136;&#37327;&#23553;&#38754;&#22270;&#20687;&#12290;&#36890;&#36807;&#25552;&#20379;&#21407;&#22987;&#22270;&#20687;&#20687;&#32032;&#30340;&#30452;&#25509;&#35775;&#38382;&#65292;PixelRec&#20351;&#24471;&#25512;&#33616;&#27169;&#22411;&#33021;&#22815;&#30452;&#25509;&#20174;&#22270;&#20687;&#23398;&#20064;&#39033;&#30446;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#65288;RS&#65289;&#36890;&#36807;&#21033;&#29992;&#26126;&#30830;&#30340;&#35782;&#21035;&#65288;ID&#65289;&#29305;&#24449;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#20294;&#26159;&#65292;&#20869;&#23481;&#29305;&#24449;&#65292;&#23588;&#20854;&#26159;&#32431;&#22270;&#20687;&#20687;&#32032;&#29305;&#24449;&#30340;&#20840;&#37096;&#28508;&#21147;&#20173;&#28982;&#30456;&#23545;&#26410;&#34987;&#24320;&#21457;&#12290;&#22823;&#35268;&#27169;&#12289;&#22810;&#26679;&#21270;&#12289;&#20197;&#20869;&#23481;&#20026;&#39537;&#21160;&#30340;&#22270;&#20687;&#25512;&#33616;&#25968;&#25454;&#38598;&#30340;&#26377;&#38480;&#21487;&#29992;&#24615;&#38459;&#30861;&#20102;&#20351;&#29992;&#21407;&#22987;&#22270;&#20687;&#20316;&#20026;&#39033;&#30446;&#34920;&#31034;&#30340;&#33021;&#21147;&#12290;&#22312;&#36825;&#26041;&#38754;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PixelRec&#65292;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#20197;&#22270;&#20687;&#20026;&#20013;&#24515;&#30340;&#25512;&#33616;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#32422;2&#20159;&#20010;&#29992;&#25143;-&#22270;&#20687;&#20132;&#20114;&#12289;3000&#19975;&#20010;&#29992;&#25143;&#21644;40&#19975;&#20010;&#39640;&#36136;&#37327;&#23553;&#38754;&#22270;&#20687;&#12290;&#36890;&#36807;&#30452;&#25509;&#35775;&#38382;&#21407;&#22987;&#22270;&#20687;&#20687;&#32032;&#65292;PixelRec&#20351;&#24471;&#25512;&#33616;&#27169;&#22411;&#33021;&#22815;&#30452;&#25509;&#20174;&#20013;&#23398;&#20064;&#39033;&#30446;&#34920;&#31034;&#12290;&#20026;&#20102;&#35777;&#26126;&#20854;&#25928;&#29992;&#65292;&#25105;&#20204;&#39318;&#20808;&#21576;&#29616;&#20102;&#22312;PixelRec&#19978;&#35757;&#32451;&#30340;&#20960;&#20010;&#32463;&#20856;&#32431;ID&#22522;&#32447;&#27169;&#22411;&#65288;&#31216;&#20026;IDNet&#65289;&#30340;&#32467;&#26524;&#12290;&#28982;&#21518;&#65292;&#20026;&#20102;&#23637;&#31034;&#25968;&#25454;&#38598;&#30340;&#22270;&#20687;&#29305;&#24449;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#23558;&#39033;&#30446;ID&#23884;&#20837;&#65288;&#26469;&#33258;IDNet&#65289;&#19982;&#19968;&#20010;...
&lt;/p&gt;
&lt;p&gt;
Recommender systems (RS) have achieved significant success by leveraging explicit identification (ID) features. However, the full potential of content features, especially the pure image pixel features, remains relatively unexplored. The limited availability of large, diverse, and content-driven image recommendation datasets has hindered the use of raw images as item representations. In this regard, we present PixelRec, a massive image-centric recommendation dataset that includes approximately 200 million user-image interactions, 30 million users, and 400,000 high-quality cover images. By providing direct access to raw image pixels, PixelRec enables recommendation models to learn item representation directly from them. To demonstrate its utility, we begin by presenting the results of several classical pure ID-based baseline models, termed IDNet, trained on PixelRec. Then, to show the effectiveness of the dataset's image features, we substitute the itemID embeddings (from IDNet) with a 
&lt;/p&gt;</description></item><item><title>CONVERSER&#26159;&#19968;&#20010;&#20351;&#29992;&#23569;&#37327;&#23545;&#35805;&#26679;&#26412;&#36827;&#34892;&#35757;&#32451;&#30340;&#23545;&#35805;&#24335;&#23494;&#38598;&#26816;&#32034;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65292;&#33021;&#22815;&#29983;&#25104;&#19982;&#26816;&#32034;&#35821;&#26009;&#24211;&#20013;&#27573;&#33853;&#30456;&#20851;&#30340;&#23545;&#35805;&#26597;&#35810;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#22312;&#23569;&#26679;&#26412;&#23545;&#35805;&#23494;&#38598;&#26816;&#32034;&#20013;&#34920;&#29616;&#20986;&#19982;&#23436;&#20840;&#30417;&#30563;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.06748</link><description>&lt;p&gt;
CONVERSER&#65306;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#30340;&#23569;&#26679;&#26412;&#23545;&#35805;&#23494;&#38598;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
CONVERSER: Few-Shot Conversational Dense Retrieval with Synthetic Data Generation. (arXiv:2309.06748v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06748
&lt;/p&gt;
&lt;p&gt;
CONVERSER&#26159;&#19968;&#20010;&#20351;&#29992;&#23569;&#37327;&#23545;&#35805;&#26679;&#26412;&#36827;&#34892;&#35757;&#32451;&#30340;&#23545;&#35805;&#24335;&#23494;&#38598;&#26816;&#32034;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65292;&#33021;&#22815;&#29983;&#25104;&#19982;&#26816;&#32034;&#35821;&#26009;&#24211;&#20013;&#27573;&#33853;&#30456;&#20851;&#30340;&#23545;&#35805;&#26597;&#35810;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#22312;&#23569;&#26679;&#26412;&#23545;&#35805;&#23494;&#38598;&#26816;&#32034;&#20013;&#34920;&#29616;&#20986;&#19982;&#23436;&#20840;&#30417;&#30563;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#24335;&#25628;&#32034;&#20026;&#20449;&#24687;&#26816;&#32034;&#25552;&#20379;&#20102;&#33258;&#28982;&#30028;&#38754;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#22312;&#23545;&#35805;&#24335;&#20449;&#24687;&#26816;&#32034;&#20013;&#24212;&#29992;&#20102;&#23494;&#38598;&#26816;&#32034;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#23494;&#38598;&#26816;&#32034;&#22120;&#38656;&#35201;&#22823;&#37327;&#30340;&#39046;&#22495;&#30456;&#20851;&#30340;&#37197;&#23545;&#25968;&#25454;&#12290;&#36825;&#38480;&#21046;&#20102;&#23545;&#35805;&#24335;&#23494;&#38598;&#26816;&#32034;&#22120;&#30340;&#21457;&#23637;&#65292;&#22240;&#20026;&#25910;&#38598;&#22823;&#37327;&#39046;&#22495;&#30456;&#20851;&#23545;&#35805;&#26159;&#26114;&#36149;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CONVERSER&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#26368;&#22810;6&#23545;&#39046;&#22495;&#30456;&#20851;&#23545;&#35805;&#36827;&#34892;&#35757;&#32451;&#30340;&#23545;&#35805;&#24335;&#23494;&#38598;&#26816;&#32034;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65292;&#26681;&#25454;&#26816;&#32034;&#35821;&#26009;&#24211;&#20013;&#30340;&#27573;&#33853;&#29983;&#25104;&#23545;&#35805;&#26597;&#35810;&#12290;&#23545;OR-QuAC&#21644;TREC CAsT 19&#31561;&#23545;&#35805;&#26816;&#32034;&#22522;&#20934;&#36827;&#34892;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;CONVERSER&#36798;&#21040;&#20102;&#19982;&#23436;&#20840;&#30417;&#30563;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#23569;&#26679;&#26412;&#23545;&#35805;&#23494;&#38598;&#26816;&#32034;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conversational search provides a natural interface for information retrieval (IR). Recent approaches have demonstrated promising results in applying dense retrieval to conversational IR. However, training dense retrievers requires large amounts of in-domain paired data. This hinders the development of conversational dense retrievers, as abundant in-domain conversations are expensive to collect. In this paper, we propose CONVERSER, a framework for training conversational dense retrievers with at most 6 examples of in-domain dialogues. Specifically, we utilize the in-context learning capability of large language models to generate conversational queries given a passage in the retrieval corpus. Experimental results on conversational retrieval benchmarks OR-QuAC and TREC CAsT 19 show that the proposed CONVERSER achieves comparable performance to fully-supervised models, demonstrating the effectiveness of our proposed framework in few-shot conversational dense retrieval. All source code and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#20250;&#35805;&#25512;&#33616;&#30340;&#23618;&#27425;&#21270;&#22810;&#20219;&#21153;&#23398;&#20064;&#26694;&#26550;HierSRec&#65292;&#36890;&#36807;&#22312;&#39044;&#27979;&#20219;&#21153;&#20043;&#38388;&#35774;&#32622;&#23618;&#27425;&#32467;&#26500;&#65292;&#24182;&#21033;&#29992;&#36741;&#21161;&#20219;&#21153;&#30340;&#36755;&#20986;&#26469;&#25552;&#20379;&#26356;&#20016;&#23500;&#30340;&#36755;&#20837;&#29305;&#24449;&#21644;&#26356;&#39640;&#30340;&#39044;&#27979;&#21487;&#35299;&#37322;&#24615;&#65292;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#21487;&#27867;&#21270;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.06533</link><description>&lt;p&gt;
&#38754;&#21521;&#20250;&#35805;&#25512;&#33616;&#30340;&#23618;&#27425;&#21270;&#22810;&#20219;&#21153;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Multi-Task Learning Framework for Session-based Recommendations. (arXiv:2309.06533v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06533
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#20250;&#35805;&#25512;&#33616;&#30340;&#23618;&#27425;&#21270;&#22810;&#20219;&#21153;&#23398;&#20064;&#26694;&#26550;HierSRec&#65292;&#36890;&#36807;&#22312;&#39044;&#27979;&#20219;&#21153;&#20043;&#38388;&#35774;&#32622;&#23618;&#27425;&#32467;&#26500;&#65292;&#24182;&#21033;&#29992;&#36741;&#21161;&#20219;&#21153;&#30340;&#36755;&#20986;&#26469;&#25552;&#20379;&#26356;&#20016;&#23500;&#30340;&#36755;&#20837;&#29305;&#24449;&#21644;&#26356;&#39640;&#30340;&#39044;&#27979;&#21487;&#35299;&#37322;&#24615;&#65292;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#21487;&#27867;&#21270;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#20250;&#35805;&#25512;&#33616;&#31995;&#32479;&#65288;SBRS&#65289;&#24050;&#32463;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#25512;&#33616;&#24615;&#33021;&#65292;&#20294;&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;MTL&#65289;&#24050;&#32463;&#34987;SBRS&#37319;&#29992;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#20854;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#21487;&#27867;&#21270;&#24615;&#12290;&#23618;&#27425;&#21270;&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;H-MTL&#65289;&#22312;&#39044;&#27979;&#20219;&#21153;&#20043;&#38388;&#35774;&#32622;&#20102;&#23618;&#27425;&#32467;&#26500;&#65292;&#24182;&#23558;&#36741;&#21161;&#20219;&#21153;&#30340;&#36755;&#20986;&#39304;&#36865;&#32473;&#20027;&#20219;&#21153;&#12290;&#19982;&#29616;&#26377;&#30340;MTL&#26694;&#26550;&#30456;&#27604;&#65292;&#36825;&#31181;&#23618;&#27425;&#32467;&#26500;&#20026;&#20027;&#20219;&#21153;&#25552;&#20379;&#20102;&#26356;&#20016;&#23500;&#30340;&#36755;&#20837;&#29305;&#24449;&#21644;&#26356;&#39640;&#30340;&#39044;&#27979;&#21487;&#35299;&#37322;&#24615;&#12290;&#28982;&#32780;&#65292;H-MTL&#26694;&#26550;&#22312;SBRS&#20013;&#23578;&#26410;&#36827;&#34892;&#30740;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HierSRec&#65292;&#23558;H-MTL&#26550;&#26500;&#32435;&#20837;SBRS&#20013;&#12290;HierSRec&#20351;&#29992;&#20803;&#25968;&#25454;&#24863;&#30693;Transformer&#23545;&#32473;&#23450;&#20250;&#35805;&#36827;&#34892;&#32534;&#30721;&#65292;&#24182;&#20351;&#29992;&#20250;&#35805;&#32534;&#30721;&#36827;&#34892;&#19979;&#19968;&#31867;&#21035;&#39044;&#27979;&#65288;&#21363;&#36741;&#21161;&#20219;&#21153;&#65289;&#12290;&#25509;&#19979;&#26469;&#65292;HierSRec&#20351;&#29992;&#31867;&#21035;&#39044;&#27979;&#32467;&#26524;&#21644;&#20250;&#35805;&#32534;&#30721;&#36827;&#34892;&#19979;&#19968;&#20010;&#29289;&#21697;&#39044;&#27979;&#65288;&#21363;&#20027;&#20219;&#21153;&#65289;&#12290;&#20026;&#20102;&#21487;&#25193;&#23637;&#30340;&#25512;&#26029;&#65292;HierSRec&#21019;&#24314;&#20102;&#19968;&#20010;&#32039;&#20945;&#30340;&#20505;&#36873;&#29289;&#21697;&#38598;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
While session-based recommender systems (SBRSs) have shown superior recommendation performance, multi-task learning (MTL) has been adopted by SBRSs to enhance their prediction accuracy and generalizability further. Hierarchical MTL (H-MTL) sets a hierarchical structure between prediction tasks and feeds outputs from auxiliary tasks to main tasks. This hierarchy leads to richer input features for main tasks and higher interpretability of predictions, compared to existing MTL frameworks. However, the H-MTL framework has not been investigated in SBRSs yet. In this paper, we propose HierSRec which incorporates the H-MTL architecture into SBRSs. HierSRec encodes a given session with a metadata-aware Transformer and performs next-category prediction (i.e., auxiliary task) with the session encoding. Next, HierSRec conducts next-item prediction (i.e., main task) with the category prediction result and session encoding. For scalable inference, HierSRec creates a compact set of candidate items (
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38598;&#25104;&#27169;&#22411;&#23558;&#30693;&#35782;&#24211;&#19982;&#26597;&#35810;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23454;&#20307;&#35782;&#21035;&#21644;&#38142;&#25509;&#25361;&#25112;&#12290;&#36890;&#36807;&#25193;&#23637;&#30693;&#35782;&#24211;&#21644;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#65292;&#25552;&#39640;&#20102;&#21484;&#22238;&#29575;&#65292;&#24182;&#20351;&#29992;&#25903;&#25345;&#21521;&#37327;&#22238;&#24402;&#21644;&#22810;&#20803;&#21152;&#24615;&#22238;&#24402;&#26641;&#36807;&#28388;&#32467;&#26524;&#24471;&#21040;&#39640;&#31934;&#24230;&#30340;&#23454;&#20307;&#35782;&#21035;&#21644;&#38142;&#25509;&#12290;&#26368;&#32456;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#35745;&#31639;&#21644;0.535&#30340;F1&#20998;&#25968;&#12290;</title><link>http://arxiv.org/abs/2309.06175</link><description>&lt;p&gt;
AKEM: &#21033;&#29992;&#38598;&#25104;&#27169;&#22411;&#23558;&#30693;&#35782;&#24211;&#19982;&#26597;&#35810;&#23545;&#40784;&#20197;&#36827;&#34892;&#23454;&#20307;&#35782;&#21035;&#21644;&#38142;&#25509;
&lt;/p&gt;
&lt;p&gt;
AKEM: Aligning Knowledge Base to Queries with Ensemble Model for Entity Recognition and Linking. (arXiv:2309.06175v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06175
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38598;&#25104;&#27169;&#22411;&#23558;&#30693;&#35782;&#24211;&#19982;&#26597;&#35810;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23454;&#20307;&#35782;&#21035;&#21644;&#38142;&#25509;&#25361;&#25112;&#12290;&#36890;&#36807;&#25193;&#23637;&#30693;&#35782;&#24211;&#21644;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#65292;&#25552;&#39640;&#20102;&#21484;&#22238;&#29575;&#65292;&#24182;&#20351;&#29992;&#25903;&#25345;&#21521;&#37327;&#22238;&#24402;&#21644;&#22810;&#20803;&#21152;&#24615;&#22238;&#24402;&#26641;&#36807;&#28388;&#32467;&#26524;&#24471;&#21040;&#39640;&#31934;&#24230;&#30340;&#23454;&#20307;&#35782;&#21035;&#21644;&#38142;&#25509;&#12290;&#26368;&#32456;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#35745;&#31639;&#21644;0.535&#30340;F1&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;NLPCC 2015&#20013;&#23454;&#20307;&#35782;&#21035;&#21644;&#38142;&#25509;&#25361;&#25112;&#30340;&#26032;&#26041;&#27861;&#12290;&#35813;&#20219;&#21153;&#21253;&#25324;&#20174;&#30701;&#25628;&#32034;&#26597;&#35810;&#20013;&#25552;&#21462;&#21629;&#21517;&#23454;&#20307;&#30340;&#25552;&#21450;&#65292;&#24182;&#23558;&#20854;&#38142;&#25509;&#21040;&#21442;&#32771;&#20013;&#25991;&#30693;&#35782;&#24211;&#20013;&#30340;&#23454;&#20307;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#25193;&#23637;&#29616;&#26377;&#30693;&#35782;&#24211;&#65292;&#24182;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#35782;&#21035;&#20505;&#36873;&#23454;&#20307;&#65292;&#20174;&#32780;&#25552;&#39640;&#21484;&#22238;&#29575;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#20174;&#20505;&#36873;&#23454;&#20307;&#20013;&#25552;&#21462;&#29305;&#24449;&#65292;&#24182;&#21033;&#29992;&#25903;&#25345;&#21521;&#37327;&#22238;&#24402;&#21644;&#22810;&#20803;&#21152;&#24615;&#22238;&#24402;&#26641;&#20316;&#20026;&#35780;&#20998;&#20989;&#25968;&#26469;&#36807;&#28388;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24212;&#29992;&#35268;&#21017;&#26469;&#36827;&#19968;&#27493;&#32454;&#21270;&#32467;&#26524;&#21644;&#25552;&#39640;&#31934;&#24230;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#35745;&#31639;&#25928;&#29575;&#39640;&#65292;&#36798;&#21040;&#20102;0.535&#30340;F1&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a novel approach to address the Entity Recognition and Linking Challenge at NLPCC 2015. The task involves extracting named entity mentions from short search queries and linking them to entities within a reference Chinese knowledge base. To tackle this problem, we first expand the existing knowledge base and utilize external knowledge to identify candidate entities, thereby improving the recall rate. Next, we extract features from the candidate entities and utilize Support Vector Regression and Multiple Additive Regression Tree as scoring functions to filter the results. Additionally, we apply rules to further refine the results and enhance precision. Our method is computationally efficient and achieves an F1 score of 0.535.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#22914;&#20309;&#20174;&#19981;&#21516;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#21644;&#36716;&#31227;&#30693;&#35782;&#65292;&#25552;&#39640;&#25512;&#33616;&#31995;&#32479;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#31867;&#27861;&#65292;&#20998;&#26512;&#21644;&#24635;&#32467;&#20102;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#33616;&#31995;&#32479;&#30340;&#22521;&#35757;&#31574;&#30053;&#21644;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2302.03735</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;&#12289;&#25552;&#31034;&#21644;&#25512;&#33616;&#65306;&#35821;&#35328;&#27169;&#22411;&#33539;&#24335;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#32508;&#21512;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Pre-train, Prompt and Recommendation: A Comprehensive Survey of Language Modelling Paradigm Adaptations in Recommender Systems. (arXiv:2302.03735v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03735
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#22914;&#20309;&#20174;&#19981;&#21516;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#21644;&#36716;&#31227;&#30693;&#35782;&#65292;&#25552;&#39640;&#25512;&#33616;&#31995;&#32479;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#31867;&#27861;&#65292;&#20998;&#26512;&#21644;&#24635;&#32467;&#20102;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#33616;&#31995;&#32479;&#30340;&#22521;&#35757;&#31574;&#30053;&#21644;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#30340;&#20986;&#29616;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#26041;&#24335;&#22312;&#22823;&#22411;&#35821;&#26009;&#24211;&#19978;&#23398;&#20064;&#36890;&#29992;&#34920;&#31034;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#23398;&#21040;&#30340;&#34920;&#31034;&#21487;&#21463;&#30410;&#20110;&#19968;&#31995;&#21015;&#19979;&#28216;NLP&#20219;&#21153;&#12290;&#36825;&#31181;&#22521;&#35757;&#33539;&#24335;&#26368;&#36817;&#34987;&#36866;&#29992;&#20110;&#25512;&#33616;&#39046;&#22495;&#65292;&#24182;&#34987;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#35748;&#20026;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#22914;&#20309;&#20174;&#19981;&#21516;PLM&#30456;&#20851;&#35757;&#32451;&#33539;&#24335;&#23398;&#20064;&#21040;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#25552;&#21462;&#21644;&#36716;&#31227;&#30693;&#35782;&#65292;&#20174;&#22810;&#20010;&#35282;&#24230;&#65288;&#22914;&#36890;&#29992;&#24615;&#12289;&#31232;&#30095;&#24615;&#12289;&#25928;&#29575;&#21644;&#25928;&#26524;&#65289;&#25552;&#39640;&#25512;&#33616;&#24615;&#33021;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27491;&#20132;&#20998;&#31867;&#27861;&#26469;&#21010;&#20998;&#29616;&#26377;&#30340;&#22522;&#20110;PLM&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#38024;&#23545;&#20854;&#22521;&#35757;&#31574;&#30053;&#21644;&#30446;&#26631;&#36827;&#34892;&#20998;&#26512;&#21644;&#24635;&#32467;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergency of Pre-trained Language Models (PLMs) has achieved tremendous success in the field of Natural Language Processing (NLP) by learning universal representations on large corpora in a self-supervised manner. The pre-trained models and the learned representations can be beneficial to a series of downstream NLP tasks. This training paradigm has recently been adapted to the recommendation domain and is considered a promising approach by both academia and industry. In this paper, we systematically investigate how to extract and transfer knowledge from pre-trained models learned by different PLM-related training paradigms to improve recommendation performance from various perspectives, such as generality, sparsity, efficiency and effectiveness. Specifically, we propose an orthogonal taxonomy to divide existing PLM-based recommender systems w.r.t. their training strategies and objectives. Then, we analyze and summarize the connection between PLM-based training paradigms and differe
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32593;&#26684;&#30340;&#32463;&#27982;&#26368;&#20248;&#25773;&#31181;&#31574;&#30053;&#65292;&#36890;&#36807;&#25968;&#23398;&#27169;&#22411;&#25551;&#36848;&#20102;&#22312;&#26893;&#29289;&#27969;&#34892;&#30149;&#26399;&#38388;&#20892;&#30000;&#20316;&#29289;&#30340;&#32463;&#27982;&#21033;&#28070;&#65292;&#21487;&#20026;&#20892;&#30000;&#20027;&#20154;&#21644;&#20915;&#31574;&#32773;&#25552;&#20379;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2301.02817</link><description>&lt;p&gt;
&#26893;&#29289;&#27969;&#34892;&#30149;&#22312;&#20892;&#30000;&#20013;&#30340;&#25104;&#26412;&#26368;&#20248;&#25773;&#31181;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Cost-optimal Seeding Strategy During a Botanical Pandemic in Domesticated Fields. (arXiv:2301.02817v3 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.02817
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32593;&#26684;&#30340;&#32463;&#27982;&#26368;&#20248;&#25773;&#31181;&#31574;&#30053;&#65292;&#36890;&#36807;&#25968;&#23398;&#27169;&#22411;&#25551;&#36848;&#20102;&#22312;&#26893;&#29289;&#27969;&#34892;&#30149;&#26399;&#38388;&#20892;&#30000;&#20316;&#29289;&#30340;&#32463;&#27982;&#21033;&#28070;&#65292;&#21487;&#20026;&#20892;&#30000;&#20027;&#20154;&#21644;&#20915;&#31574;&#32773;&#25552;&#20379;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#65306;&#26893;&#29289;&#27969;&#34892;&#30149;&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#36896;&#25104;&#20102;&#24040;&#22823;&#30340;&#32463;&#27982;&#25439;&#22833;&#21644;&#31918;&#39135;&#30701;&#32570;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26893;&#29289;&#27969;&#34892;&#30149;&#22312;&#30701;&#20013;&#26399;&#20869;&#23558;&#32487;&#32493;&#23384;&#22312;&#65292;&#20892;&#30000;&#20027;&#20154;&#21487;&#20197;&#26681;&#25454;&#31574;&#30053;&#24615;&#22320;&#22312;&#33258;&#24049;&#30340;&#20892;&#30000;&#20013;&#25773;&#31181;&#65292;&#20197;&#20248;&#21270;&#27599;&#19968;&#27425;&#20316;&#29289;&#29983;&#20135;&#30340;&#32463;&#27982;&#21033;&#28070;&#12290;&#30446;&#26631;&#65306;&#37492;&#20110;&#30149;&#21407;&#20307;&#30340;&#27969;&#34892;&#30149;&#23398;&#29305;&#24615;&#65292;&#25105;&#20204;&#26088;&#22312;&#20026;&#20892;&#30000;&#20027;&#20154;&#21644;&#20915;&#31574;&#32773;&#23547;&#25214;&#19968;&#31181;&#22522;&#20110;&#32593;&#26684;&#30340;&#32463;&#27982;&#26368;&#20248;&#25773;&#31181;&#31574;&#30053;&#12290;&#26041;&#27861;&#65306;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27969;&#34892;&#30149;&#23398;-&#32463;&#27982;&#25968;&#23398;&#27169;&#22411;&#65292;&#25551;&#36848;&#20102;&#22312;&#26893;&#29289;&#27969;&#34892;&#30149;&#26399;&#38388;&#20892;&#30000;&#20316;&#29289;&#30340;&#32463;&#27982;&#21033;&#28070;&#12290;&#25105;&#20204;&#20351;&#29992;&#26102;&#31354;&#25193;&#23637;&#30340;&#26131;&#24863;-&#24863;&#26579;-&#24247;&#22797;&#27969;&#34892;&#30149;&#23398;&#27169;&#22411;&#20197;&#21450;&#38750;&#32447;&#24615;&#36755;&#20986;&#27969;&#34892;&#30149;&#23398;&#27169;&#22411;&#26469;&#25551;&#36848;&#27969;&#34892;&#30149;&#23398;&#21160;&#24577;&#12290;&#32467;&#26524;&#21644;&#32467;&#35770;&#65306;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#29992;&#20110;&#26681;&#25454;&#20892;&#30000;&#21644;&#30149;&#21407;&#20307;&#30340;&#29305;&#24615;&#33719;&#21462;&#26368;&#20248;&#30340;&#32593;&#26684;&#24418;&#25104;&#30340;&#25773;&#31181;&#31574;&#30053;&#65292;&#20197;&#26368;&#22823;&#21270;&#32463;&#27982;&#21033;&#28070;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#22312;&#29616;&#23454;&#24773;&#20917;&#19979;&#23454;&#26045;&#20102;&#25552;&#20986;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Context: Botanical pandemics cause enormous economic damage and food shortage around the globe. However, since botanical pandemics are here to stay in the short-medium term, domesticated field owners can strategically seed their fields to optimize each session's economic profit. Objective: Given the pathogen's epidemiological properties, we aim to find an economically optimal grid-based seeding strategy for field owners and policymakers. Methods: We propose a novel epidemiological-economic mathematical model that describes the economic profit from a field of plants during a botanical pandemic. We describe the epidemiological dynamics using a spatio-temporal extended Susceptible-Infected-Recovered epidemiological model with a non-linear output epidemiological model. Results and Conclusions: We provide an algorithm to obtain an optimal grid-formed seeding strategy to maximize economic profit, given field and pathogen properties. In addition, we implement the proposed model in realistic s
&lt;/p&gt;</description></item></channel></rss>