<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#25903;&#25345;&#36890;&#36807;&#21363;&#26102;&#36890;&#35759;&#36827;&#34892;&#21327;&#20316;&#30340;&#8220;&#22810;&#20010;&#8221;&#25628;&#32034;&#32773;&#65292;&#21457;&#29616;&#22312;&#27492;&#24773;&#20917;&#19979;&#25628;&#32034;&#26426;&#22120;&#20154;&#37319;&#29992;&#20102;&#26356;&#22810;&#30340;&#23545;&#35805;&#27700;&#24179;&#20513;&#35758;&#65292;&#35813;&#30740;&#31350;&#21487;&#20197;&#24110;&#21161;&#26410;&#26469;&#20250;&#35805;&#25628;&#32034;&#31995;&#32479;&#30340;&#35774;&#35745;&#12290;</title><link>http://arxiv.org/abs/2303.13484</link><description>&lt;p&gt;
&#20026;&#20160;&#20040;&#21644;&#20309;&#26102;&#65306;&#29702;&#35299;&#20250;&#35805;&#21327;&#20316;&#25628;&#32034;&#20013;&#30340;&#31995;&#32479;&#20513;&#35758;
&lt;/p&gt;
&lt;p&gt;
Why and When: Understanding System Initiative during Conversational Collaborative Search. (arXiv:2303.13484v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13484
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#25903;&#25345;&#36890;&#36807;&#21363;&#26102;&#36890;&#35759;&#36827;&#34892;&#21327;&#20316;&#30340;&#8220;&#22810;&#20010;&#8221;&#25628;&#32034;&#32773;&#65292;&#21457;&#29616;&#22312;&#27492;&#24773;&#20917;&#19979;&#25628;&#32034;&#26426;&#22120;&#20154;&#37319;&#29992;&#20102;&#26356;&#22810;&#30340;&#23545;&#35805;&#27700;&#24179;&#20513;&#35758;&#65292;&#35813;&#30740;&#31350;&#21487;&#20197;&#24110;&#21161;&#26410;&#26469;&#20250;&#35805;&#25628;&#32034;&#31995;&#32479;&#30340;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#20250;&#35805;&#25628;&#32034;&#24341;&#36215;&#20102;&#30456;&#24403;&#22810;&#30340;&#20851;&#27880;&#12290;&#20294;&#26159;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#33021;&#22815;&#25903;&#25345;&#8220;&#21333;&#20010;&#8221;&#25628;&#32034;&#32773;&#30340;&#31995;&#32479;&#19978;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22914;&#20309;&#25903;&#25345;&#36890;&#36807;&#21363;&#26102;&#36890;&#35759;&#24179;&#21488;&#65288;&#21363; Slack&#65289;&#21327;&#20316;&#30340;&#8220;&#22810;&#20010;&#8221;&#25628;&#32034;&#32773;&#30340;&#31995;&#32479;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#39033;&#8220;&#22885;&#20857;&#24043;&#24072;&#8221;&#30740;&#31350;&#65292;27&#23545;&#21442;&#19982;&#32773;&#22312; Slack &#19978;&#21327;&#20316;&#23436;&#25104;&#20102;&#19977;&#20010;&#20449;&#24687;&#23547;&#27714;&#20219;&#21153;&#12290;&#21442;&#19982;&#32773;&#26080;&#27861;&#29420;&#31435;&#25628;&#32034;&#65292;&#24517;&#39035;&#36890;&#36807;&#19982;&#20174; Slack &#36890;&#36947;&#30452;&#25509;&#20132;&#20114;&#30340;&#8220;&#25628;&#32034;&#26426;&#22120;&#20154;&#8221;&#25910;&#38598;&#20449;&#24687;&#12290;&#25628;&#32034;&#26426;&#22120;&#20154;&#30340;&#35282;&#33394;&#30001;&#21442;&#32771;&#22270;&#20070;&#39302;&#21592;&#25198;&#28436;&#12290;&#20250;&#35805;&#25628;&#32034;&#31995;&#32479;&#24517;&#39035;&#33021;&#22815;&#21442;&#19982;&#8220;&#28151;&#21512;&#20513;&#35758;&#8221;&#20132;&#20114;&#65292;&#36890;&#36807;&#25511;&#21046;&#23545;&#35805;&#24182;&#20132;&#36824;&#25511;&#21046;&#26435;&#20197;&#23454;&#29616;&#19981;&#21516;&#30340;&#30446;&#26631;&#12290;&#35805;&#35821;&#20998;&#26512;&#30740;&#31350;&#34920;&#26126;&#65292;&#20250;&#35805;&#20195;&#29702;&#21487;&#20197;&#37319;&#21462;&#20004;&#31181;&#20513;&#35758;&#27700;&#24179;&#65306;&#23545;&#35805;&#27700;&#24179;&#21644;&#20219;&#21153;&#27700;&#24179;&#20513;&#35758;&#12290;&#20195;&#29702;&#36890;&#36807;&#22312;&#22810;&#20010;&#36873;&#39033;&#20013;&#36873;&#25321;&#25171;&#24320;&#26032;&#30340;&#23376;&#35805;&#39064;&#25110;&#25552;&#31034;&#29992;&#25143;&#36827;&#34892;&#28548;&#28165;&#26469;&#37319;&#21462;&#23545;&#35805;&#27700;&#24179;&#20513;&#35758;&#12290;&#20195;&#29702;&#36890;&#36807;&#20026;&#35813;&#20219;&#21153;&#25552;&#20379;&#26032;&#30340;&#30456;&#20851;&#20449;&#24687;&#26469;&#37319;&#21462;&#20219;&#21153;&#27700;&#24179;&#20513;&#35758;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#19982;&#8220;&#22810;&#20010;&#8221;&#25628;&#32034;&#32773;&#36827;&#34892;&#21327;&#20316;&#25628;&#32034;&#30340;&#24773;&#20917;&#19979;&#65292;&#25628;&#32034;&#26426;&#22120;&#20154;&#37319;&#29992;&#20102;&#26356;&#39640;&#27604;&#20363;&#30340;&#23545;&#35805;&#27700;&#24179;&#20513;&#35758;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#26377;&#20851;&#25628;&#32034;&#26426;&#22120;&#20154;&#22914;&#20309;&#22312;&#23384;&#22312;&#22810;&#20010;&#25628;&#32034;&#32773;&#30340;&#24773;&#20917;&#19979;&#37319;&#21462;&#21644;&#25918;&#24323;&#20513;&#35758;&#20197;&#21709;&#24212;&#30340;&#35265;&#35299;&#65292;&#36825;&#21487;&#20197;&#20026;&#26410;&#26469;&#30340;&#20250;&#35805;&#25628;&#32034;&#31995;&#32479;&#30340;&#35774;&#35745;&#25552;&#20379;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the last decade, conversational search has attracted considerable attention. However, most research has focused on systems that can support a \emph{single} searcher. In this paper, we explore how systems can support \emph{multiple} searchers collaborating over an instant messaging platform (i.e., Slack). We present a ``Wizard of Oz'' study in which 27 participant pairs collaborated on three information-seeking tasks over Slack. Participants were unable to search on their own and had to gather information by interacting with a \emph{searchbot} directly from the Slack channel. The role of the searchbot was played by a reference librarian. Conversational search systems must be capable of engaging in \emph{mixed-initiative} interaction by taking and relinquishing control of the conversation to fulfill different objectives. Discourse analysis research suggests that conversational agents can take \emph{two} levels of initiative: dialog- and task-level initiative. Agents take dialog-level 
&lt;/p&gt;</description></item><item><title>&#35843;&#25972;&#27169;&#22359;&#21270;&#25552;&#31034;&#30340;&#26816;&#32034;&#27169;&#22411;REMOP&#65292;&#36890;&#36807;&#22810;&#20010;&#29616;&#26377;&#26816;&#32034;&#27169;&#22359;&#30340;&#32452;&#21512;&#35299;&#20915;&#26032;&#30340;&#26816;&#32034;&#20219;&#21153;&#65292;&#20855;&#26377;&#27867;&#21270;&#21644;&#35299;&#37322;&#33021;&#21147;&#65292; &#22312;&#38646;-shot&#26816;&#32034;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#19982;&#26368;&#20808;&#36827;&#30340;&#26816;&#32034;&#27169;&#22411;&#30456;&#23218;&#32654;&#12290;</title><link>http://arxiv.org/abs/2303.13419</link><description>&lt;p&gt;
&#27169;&#22359;&#21270;&#26816;&#32034;&#65306;&#27867;&#21270;&#21644;&#35299;&#37322;&#30340;&#20986;&#36335;
&lt;/p&gt;
&lt;p&gt;
Modular Retrieval for Generalization and Interpretation. (arXiv:2303.13419v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13419
&lt;/p&gt;
&lt;p&gt;
&#35843;&#25972;&#27169;&#22359;&#21270;&#25552;&#31034;&#30340;&#26816;&#32034;&#27169;&#22411;REMOP&#65292;&#36890;&#36807;&#22810;&#20010;&#29616;&#26377;&#26816;&#32034;&#27169;&#22359;&#30340;&#32452;&#21512;&#35299;&#20915;&#26032;&#30340;&#26816;&#32034;&#20219;&#21153;&#65292;&#20855;&#26377;&#27867;&#21270;&#21644;&#35299;&#37322;&#33021;&#21147;&#65292; &#22312;&#38646;-shot&#26816;&#32034;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#19982;&#26368;&#20808;&#36827;&#30340;&#26816;&#32034;&#27169;&#22411;&#30456;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26032;&#30340;&#26816;&#32034;&#20219;&#21153;&#19981;&#26029;&#28044;&#29616;&#65292;&#38656;&#35201;&#24320;&#21457;&#26032;&#30340;&#26816;&#32034;&#27169;&#22411;&#12290;&#20294;&#26159;&#65292;&#20026;&#27599;&#20010;&#26032;&#30340;&#26816;&#32034;&#20219;&#21153;&#23454;&#20363;&#21270;&#26816;&#32034;&#27169;&#22411;&#26159;&#32791;&#36153;&#36164;&#28304;&#21644;&#32791;&#26102;&#30340;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#37319;&#29992;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26816;&#32034;&#27169;&#22411;&#32780;&#35328;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#36716;&#21521;&#19968;&#31181;&#26032;&#30340;&#26816;&#32034;&#33539;&#24335;&#65292;&#31216;&#20026;&#27169;&#22359;&#21270;&#26816;&#32034;&#65292;&#23427;&#26088;&#22312;&#36890;&#36807;&#32452;&#21512;&#22810;&#20010;&#29616;&#26377;&#30340;&#26816;&#32034;&#27169;&#22359;&#26469;&#35299;&#20915;&#26032;&#30340;&#26816;&#32034;&#20219;&#21153;&#12290;&#22522;&#20110;&#36825;&#20010;&#33539;&#24335;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#27169;&#22359;&#21270;&#25552;&#31034;&#35843;&#25972;&#30340;&#26816;&#32034;&#27169;&#22411;&#65288;REMOP&#65289;&#12290;&#23427;&#36890;&#36807;&#28145;&#24230;&#25552;&#31034;&#35843;&#25972;&#26500;&#24314;&#19982;&#20219;&#21153;&#23646;&#24615;&#30456;&#20851;&#32852;&#30340;&#26816;&#32034;&#27169;&#22359;&#65292;&#24182;&#36890;&#36807;&#27169;&#22359;&#21270;&#32452;&#21512;&#20135;&#29983;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#26816;&#32034;&#27169;&#22411;&#12290;&#25105;&#20204;&#39564;&#35777;&#65292;REMOP&#22825;&#29983;&#20855;&#26377;&#27169;&#22359;&#24615;&#19981;&#20165;&#22312;&#21021;&#27493;&#25506;&#32034;&#20013;&#20855;&#26377;&#21560;&#24341;&#20154;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#32780;&#19988;&#22312;&#38646;-shot&#26816;&#32034;&#22522;&#20934;&#27979;&#35797;&#20013;&#20855;&#26377;&#19982;&#26368;&#20808;&#36827;&#30340;&#26816;&#32034;&#27169;&#22411;&#30456;&#23218;&#32654;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
New retrieval tasks have always been emerging, thus urging the development of new retrieval models. However, instantiating a retrieval model for each new retrieval task is resource-intensive and time-consuming, especially for a retrieval model that employs a large-scale pre-trained language model. To address this issue, we shift to a novel retrieval paradigm called modular retrieval, which aims to solve new retrieval tasks by instead composing multiple existing retrieval modules. Built upon the paradigm, we propose a retrieval model with modular prompt tuning named REMOP. It constructs retrieval modules subject to task attributes with deep prompt tuning, and yields retrieval models subject to tasks with module composition. We validate that, REMOP inherently with modularity not only has appealing generalizability and interpretability in preliminary explorations, but also achieves comparable performance to state-of-the-art retrieval models on a zero-shot retrieval benchmark.\footnote{Our
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;LSR&#26694;&#26550;&#65292;&#20197;&#32479;&#19968;&#24050;&#26377;&#30340;LSR&#26041;&#27861;&#24182;&#20998;&#26512;&#20102;&#20854;&#20851;&#38190;&#32452;&#20214;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21253;&#25324;&#25991;&#26723;&#26415;&#35821;&#21152;&#26435;&#26102;&#65292;LSR&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#26368;&#20248;&#12290;</title><link>http://arxiv.org/abs/2303.13416</link><description>&lt;p&gt;
&#19968;&#31181;&#23398;&#20064;&#31232;&#30095;&#26816;&#32034;&#30340;&#32479;&#19968;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Unified Framework for Learned Sparse Retrieval. (arXiv:2303.13416v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13416
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;LSR&#26694;&#26550;&#65292;&#20197;&#32479;&#19968;&#24050;&#26377;&#30340;LSR&#26041;&#27861;&#24182;&#20998;&#26512;&#20102;&#20854;&#20851;&#38190;&#32452;&#20214;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21253;&#25324;&#25991;&#26723;&#26415;&#35821;&#21152;&#26435;&#26102;&#65292;LSR&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#26368;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#31232;&#30095;&#26816;&#32034;(Learned sparse retrieval, LSR)&#26159;&#19968;&#32452;&#29992;&#20110;&#29983;&#25104;&#26597;&#35810;&#21644;&#25991;&#26723;&#30340;&#31232;&#30095;&#35789;&#27719;&#34920;&#31034;&#20197;&#29992;&#20110;&#21453;&#21521;&#32034;&#24341;&#30340;&#31532;&#19968;&#38454;&#27573;&#26816;&#32034;&#26041;&#27861;&#12290;&#26368;&#36817;&#24341;&#20837;&#20102;&#35768;&#22810;LSR&#26041;&#27861;&#65292;&#20854;&#20013;Splade&#27169;&#22411;&#22312;MSMarco&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#36825;&#20123;LSR&#26041;&#27861;&#22312;&#27169;&#22411;&#26550;&#26500;&#19978;&#30456;&#20284;&#65292;&#20294;&#22312;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#23384;&#22312;&#23454;&#36136;&#24615;&#24046;&#24322;&#12290;&#20351;&#29992;&#19981;&#21516;&#30340;&#23454;&#39564;&#35774;&#32622;&#21644;&#37197;&#32622;&#20351;&#24471;&#38590;&#20197;&#27604;&#36739;&#36825;&#20123;&#26041;&#27861;&#24182;&#33719;&#24471;&#28145;&#20837;&#30340;&#35265;&#35299;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#29616;&#26377;&#30340;LSR&#26041;&#27861;&#65292;&#30830;&#23450;&#20102;&#20851;&#38190;&#32452;&#20214;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;LSR&#26694;&#26550;&#65292;&#23558;&#25152;&#26377;LSR&#26041;&#27861;&#32479;&#19968;&#21040;&#19968;&#20010;&#26694;&#26550;&#19979;&#12290;&#25105;&#20204;&#20351;&#29992;&#30456;&#21516;&#30340;&#20195;&#30721;&#24211;&#37325;&#29616;&#20102;&#25152;&#26377;&#37325;&#35201;&#30340;&#26041;&#27861;&#65292;&#28982;&#21518;&#22312;&#30456;&#21516;&#30340;&#29615;&#22659;&#20013;&#37325;&#26032;&#35757;&#32451;&#23427;&#20204;&#65292;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#37327;&#21270;&#26694;&#26550;&#30340;&#32452;&#20214;&#22914;&#20309;&#24433;&#21709;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;(1)&#21253;&#25324;&#25991;&#26723;&#26415;&#35821;&#21152;&#26435;&#26368;&#20026;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learned sparse retrieval (LSR) is a family of first-stage retrieval methods that are trained to generate sparse lexical representations of queries and documents for use with an inverted index. Many LSR methods have been recently introduced, with Splade models achieving state-of-the-art performance on MSMarco. Despite similarities in their model architectures, many LSR methods show substantial differences in effectiveness and efficiency. Differences in the experimental setups and configurations used make it difficult to compare the methods and derive insights. In this work, we analyze existing LSR methods and identify key components to establish an LSR framework that unifies all LSR methods under the same perspective. We then reproduce all prominent methods using a common codebase and re-train them in the same environment, which allows us to quantify how components of the framework affect effectiveness and efficiency. We find that (1) including document term weighting is most important 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;GETT-QA&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#20351;&#29992;T5&#23545;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#29983;&#25104;&#31616;&#21270;&#30340;SPARQL&#26597;&#35810;&#65292;&#24182;&#20351;&#29992;&#25130;&#26029;&#30340;KG&#23884;&#20837;&#25552;&#39640;&#20102;&#30693;&#35782;&#22270;&#35889;&#38382;&#31572;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.13284</link><description>&lt;p&gt;
GETT-QA&#65306;&#22522;&#20110;&#22270;&#23884;&#20837;&#30340;&#30693;&#35782;&#22270;&#35889;&#38382;&#31572;&#20013;&#30340;T2T Transformer
&lt;/p&gt;
&lt;p&gt;
GETT-QA: Graph Embedding based T2T Transformer for Knowledge Graph Question Answering. (arXiv:2303.13284v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13284
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;GETT-QA&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#20351;&#29992;T5&#23545;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#29983;&#25104;&#31616;&#21270;&#30340;SPARQL&#26597;&#35810;&#65292;&#24182;&#20351;&#29992;&#25130;&#26029;&#30340;KG&#23884;&#20837;&#25552;&#39640;&#20102;&#30693;&#35782;&#22270;&#35889;&#38382;&#31572;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GETT-QA&#30340;&#31471;&#21040;&#31471;&#30693;&#35782;&#22270;&#35889;&#38382;&#31572;&#31995;&#32479;&#12290;GETT-QA&#20351;&#29992;&#20102;T5&#65292;&#36825;&#26159;&#19968;&#31181;&#28909;&#38376;&#30340;&#25991;&#26412;&#21040;&#25991;&#26412;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#20197;&#33258;&#28982;&#35821;&#35328;&#24418;&#24335;&#30340;&#38382;&#39064;&#20316;&#20026;&#36755;&#20837;&#24182;&#29983;&#25104;&#25152;&#38656;SPARQL&#26597;&#35810;&#30340;&#31616;&#21270;&#24418;&#24335;&#12290;&#22312;&#31616;&#21270;&#24418;&#24335;&#20013;&#65292;&#27169;&#22411;&#19981;&#30452;&#25509;&#29983;&#25104;&#23454;&#20307;&#21644;&#20851;&#31995;ID&#65292;&#32780;&#26159;&#20135;&#29983;&#30456;&#24212;&#30340;&#23454;&#20307;&#21644;&#20851;&#31995;&#26631;&#31614;&#12290;&#26631;&#31614;&#22312;&#38543;&#21518;&#30340;&#27493;&#39588;&#20013;&#19982;KG&#23454;&#20307;&#21644;&#20851;&#31995;ID&#32852;&#31995;&#36215;&#26469;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25913;&#36827;&#32467;&#26524;&#65292;&#25105;&#20204;&#25351;&#23548;&#27169;&#22411;&#20026;&#27599;&#20010;&#23454;&#20307;&#29983;&#25104;KG&#23884;&#20837;&#30340;&#25130;&#26029;&#29256;&#26412;&#12290;&#25130;&#26029;&#30340;KG&#23884;&#20837;&#20351;&#24471;&#26356;&#31934;&#32454;&#30340;&#25628;&#32034;&#20174;&#32780;&#26356;&#26377;&#25928;&#36827;&#34892;&#28040;&#27495;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;T5&#33021;&#22815;&#22312;&#19981;&#25913;&#21464;&#25439;&#22833;&#20989;&#25968;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#25130;&#26029;&#30340;KG&#23884;&#20837;&#65292;&#25552;&#39640;&#20102;KGQA&#30340;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#22312;Wikidata&#30340;LC-QuAD 2.0&#21644;SimpleQuestions-Wikidata&#25968;&#25454;&#38598;&#19978;&#25253;&#21578;&#20102;&#31471;&#21040;&#31471;KGQA&#30340;&#24378;&#22823;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we present an end-to-end Knowledge Graph Question Answering (KGQA) system named GETT-QA. GETT-QA uses T5, a popular text-to-text pre-trained language model. The model takes a question in natural language as input and produces a simpler form of the intended SPARQL query. In the simpler form, the model does not directly produce entity and relation IDs. Instead, it produces corresponding entity and relation labels. The labels are grounded to KG entity and relation IDs in a subsequent step. To further improve the results, we instruct the model to produce a truncated version of the KG embedding for each entity. The truncated KG embedding enables a finer search for disambiguation purposes. We find that T5 is able to learn the truncated KG embeddings without any change of loss function, improving KGQA performance. As a result, we report strong results for LC-QuAD 2.0 and SimpleQuestions-Wikidata datasets on end-to-end KGQA over Wikidata.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20449;&#24687;&#26816;&#32034;&#20013;&#20351;&#29992;&#36866;&#37197;&#22120;&#30340;&#25928;&#26524;&#65292;&#29305;&#21035;&#26159;&#22312;SPLADE&#36825;&#31181;&#31232;&#30095;&#26816;&#32034;&#22120;&#19978;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#36866;&#37197;&#22120;-SPLADE&#21482;&#38656;&#20248;&#21270;2%&#30340;&#35757;&#32451;&#21442;&#25968;&#65292;&#20294;&#22312;&#25928;&#29575;&#21644;&#25928;&#26524;&#26041;&#38754;&#22343;&#20248;&#20110;&#23436;&#20840;&#24494;&#35843;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.13220</link><description>&lt;p&gt;
&#20351;&#29992;&#36866;&#37197;&#22120;&#30340;&#21442;&#25968;&#39640;&#25928;&#31232;&#30095;&#26816;&#32034;&#22120;&#21644;&#37325;&#25490;&#22120;
&lt;/p&gt;
&lt;p&gt;
Parameter-Efficient Sparse Retrievers and Rerankers using Adapters. (arXiv:2303.13220v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13220
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20449;&#24687;&#26816;&#32034;&#20013;&#20351;&#29992;&#36866;&#37197;&#22120;&#30340;&#25928;&#26524;&#65292;&#29305;&#21035;&#26159;&#22312;SPLADE&#36825;&#31181;&#31232;&#30095;&#26816;&#32034;&#22120;&#19978;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#36866;&#37197;&#22120;-SPLADE&#21482;&#38656;&#20248;&#21270;2%&#30340;&#35757;&#32451;&#21442;&#25968;&#65292;&#20294;&#22312;&#25928;&#29575;&#21644;&#25928;&#26524;&#26041;&#38754;&#22343;&#20248;&#20110;&#23436;&#20840;&#24494;&#35843;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#36866;&#37197;&#22120;&#30340;&#21442;&#25968;&#39640;&#25928;&#36801;&#31227;&#23398;&#20064;&#24050;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#30740;&#31350;&#20316;&#20026;&#23436;&#20840;&#24494;&#35843;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#36866;&#37197;&#22120;&#26159;&#20869;&#23384;&#39640;&#25928;&#30340;&#65292;&#24182;&#36890;&#36807;&#22312;&#21464;&#21387;&#22120;&#23618;&#20043;&#38388;&#28155;&#21152;&#23567;&#29942;&#39048;&#23618;&#36827;&#34892;&#35757;&#32451;&#65292;&#21516;&#26102;&#20445;&#25345;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#20923;&#32467;&#26469;&#19982;&#19979;&#28216;&#20219;&#21153;&#33391;&#22909;&#22320;&#36827;&#34892;&#32553;&#25918;&#12290;&#23613;&#31649;&#22312;NLP&#20013;&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#22312;&#20449;&#24687;&#26816;&#32034;&#26041;&#38754;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#26412;&#25991;&#26088;&#22312;&#23436;&#21892;&#36866;&#37197;&#22120;&#22312;IR&#20013;&#30340;&#20351;&#29992;&#24773;&#20917;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36866;&#37197;&#22120;&#23545;&#20110;SPLADE&#65288;&#19968;&#31181;&#31232;&#30095;&#26816;&#32034;&#22120;&#65289;&#30340;&#24212;&#29992;&#65292;&#36866;&#37197;&#22120;&#19981;&#20165;&#20445;&#30041;&#20102;&#36890;&#36807;&#23436;&#20840;&#24494;&#35843;&#23454;&#29616;&#30340;&#25928;&#29575;&#21644;&#25928;&#26524;&#65292;&#32780;&#19988;&#20869;&#23384;&#39640;&#25928;&#65292;&#35757;&#32451;&#36731;&#37327;&#32423;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#36866;&#37197;&#22120;-SPLADE&#20165;&#20248;&#21270;2&#65285;&#30340;&#35757;&#32451;&#21442;&#25968;&#65292;&#20294;&#32988;&#36807;&#23436;&#20840;&#24494;&#35843;&#30340;&#23545;&#24212;&#29289;&#20197;&#21450;&#24050;&#26377;&#30340;&#26368;&#20339;&#31232;&#30095;&#26816;&#32034;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parameter-Efficient transfer learning with Adapters have been studied in Natural Language Processing (NLP) as an alternative to full fine-tuning. Adapters are memory-efficient and scale well with downstream tasks by training small bottle-neck layers added between transformer layers while keeping the large pretrained language model (PLMs) frozen. In spite of showing promising results in NLP, these methods are under-explored in Information Retrieval. While previous studies have only experimented with dense retriever or in a cross lingual retrieval scenario, in this paper we aim to complete the picture on the use of adapters in IR. First, we study adapters for SPLADE, a sparse retriever, for which adapters not only retain the efficiency and effectiveness otherwise achieved by finetuning, but are memory-efficient and orders of magnitude lighter to train. We observe that Adapters-SPLADE not only optimizes just 2\% of training parameters, but outperforms fully fine-tuned counterpart and exis
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;Top-N&#25512;&#33616;&#20013;&#30340;&#21487;&#39044;&#27979;&#24615;&#26497;&#38480;&#65292;&#21457;&#29616;&#29992;&#25143;&#34892;&#20026;&#30340;&#21487;&#39044;&#27979;&#24615;&#38480;&#21046;&#20102;Top-N&#25512;&#33616;&#31639;&#27861;&#30340;&#20934;&#30830;&#24230;&#65292;&#26080;&#35770;&#20351;&#29992;&#20309;&#31181;&#31639;&#27861;&#22343;&#26080;&#27861;&#31361;&#30772;&#20854;&#19978;&#38480;&#12290;</title><link>http://arxiv.org/abs/2303.13091</link><description>&lt;p&gt;
Top-N&#25512;&#33616;&#20013;&#30340;&#21487;&#39044;&#27979;&#24615;&#26497;&#38480;
&lt;/p&gt;
&lt;p&gt;
Limits of Predictability in Top-N Recommendation. (arXiv:2303.13091v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13091
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;Top-N&#25512;&#33616;&#20013;&#30340;&#21487;&#39044;&#27979;&#24615;&#26497;&#38480;&#65292;&#21457;&#29616;&#29992;&#25143;&#34892;&#20026;&#30340;&#21487;&#39044;&#27979;&#24615;&#38480;&#21046;&#20102;Top-N&#25512;&#33616;&#31639;&#27861;&#30340;&#20934;&#30830;&#24230;&#65292;&#26080;&#35770;&#20351;&#29992;&#20309;&#31181;&#31639;&#27861;&#22343;&#26080;&#27861;&#31361;&#30772;&#20854;&#19978;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Top-N&#25512;&#33616;&#26088;&#22312;&#20174;&#22823;&#37327;&#29289;&#21697;&#20013;&#21521;&#27599;&#20010;&#29992;&#25143;&#25512;&#33616;&#19968;&#20010;&#23567;&#30340;N&#20010;&#29289;&#21697;&#38598;&#21512;&#65292;&#20854;&#20934;&#30830;&#24615;&#26159;&#35780;&#20272;&#25512;&#33616;&#31995;&#32479;&#24615;&#33021;&#30340;&#26368;&#24120;&#29992;&#25351;&#26631;&#20043;&#19968;&#12290;&#23613;&#31649;&#26377;&#22823;&#37327;&#30340;&#31639;&#27861;&#25552;&#20986;&#20174;&#29992;&#25143;&#30340;&#21382;&#21490;&#36141;&#20080;&#25968;&#25454;&#20013;&#23398;&#20064;&#29992;&#25143;&#20559;&#22909;&#20197;&#25552;&#39640;Top-N&#20934;&#30830;&#24615;&#65292;&#20294;&#33258;&#28982;&#20250;&#26377;&#19968;&#20010;&#21487;&#39044;&#27979;&#24615;&#38382;&#39064;-&#26159;&#21542;&#23384;&#22312;&#36825;&#26679;&#19968;&#20010;Top-N&#20934;&#30830;&#24615;&#30340;&#19978;&#38480;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#30740;&#31350;&#29305;&#23450;&#29992;&#25143;&#34892;&#20026;&#25968;&#25454;&#38598;&#30340;&#35268;&#21017;&#31243;&#24230;&#26469;&#35843;&#26597;&#36825;&#31181;&#21487;&#39044;&#27979;&#24615;&#12290;&#21516;&#26102;&#37327;&#21270;Top-N&#25512;&#33616;&#30340;&#21487;&#39044;&#27979;&#24615;&#38656;&#35201;&#21516;&#26102;&#37327;&#21270;N&#20010;&#20855;&#26377;&#26368;&#39640;&#27010;&#29575;&#30340;&#34892;&#20026;&#30340;&#20934;&#30830;&#24230;&#30340;&#38480;&#21046;&#65292;&#20174;&#32780;&#22823;&#22823;&#22686;&#21152;&#20102;&#38382;&#39064;&#30340;&#38590;&#24230;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#30340;&#65292;&#25105;&#20204;&#39318;&#20808;&#25366;&#25496;&#20102;N&#20010;&#20855;&#26377;&#26368;&#39640;&#27010;&#29575;&#30340;&#34892;&#20026;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#24182;&#22522;&#20110;&#20449;&#24687;&#35770;&#25551;&#36848;&#20102;&#29992;&#25143;&#34892;&#20026;&#20998;&#24067;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#37319;&#29992;&#29992;&#25143;&#34892;&#20026;&#25968;&#25454;&#30340;&#21487;&#21387;&#32553;&#24615;&#20316;&#20026;&#21487;&#39044;&#27979;&#24615;&#30340;&#24230;&#37327;&#65292;&#24182;&#20998;&#26512;&#20854;&#23545;Top-N&#25512;&#33616;&#20934;&#30830;&#24230;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#29992;&#25143;&#34892;&#20026;&#30340;&#21487;&#39044;&#27979;&#24615;&#38480;&#21046;&#20102;Top-N&#25512;&#33616;&#31639;&#27861;&#30340;&#20934;&#30830;&#24230;&#65292;&#23384;&#22312;&#30528;&#19968;&#31181;&#19978;&#38480;&#30340;&#20934;&#30830;&#24230;&#65292;&#20219;&#20309;&#31639;&#27861;&#37117;&#26080;&#27861;&#36229;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;
Top-N recommendation aims to recommend each consumer a small set of N items from a large collection of items, and its accuracy is one of the most common indexes to evaluate the performance of a recommendation system. While a large number of algorithms are proposed to push the Top-N accuracy by learning the user preference from their history purchase data, a predictability question is naturally raised - whether there is an upper limit of such Top-N accuracy. This work investigates such predictability by studying the degree of regularity from a specific set of user behavior data. Quantifying the predictability of Top-N recommendations requires simultaneously quantifying the limits on the accuracy of the N behaviors with the highest probability. This greatly increases the difficulty of the problem. To achieve this, we firstly excavate the associations among N behaviors with the highest probability and describe the user behavior distribution based on the information theory. Then, we adopt 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22810;&#31181;&#19981;&#30830;&#23450;&#24615;&#26657;&#20934;&#25216;&#26415;&#65292;&#20197;&#25913;&#36827;&#25512;&#33616;&#31995;&#32479;&#20013;&#20542;&#21521;&#24615;&#20272;&#35745;&#30340;&#25928;&#26524;&#12290;&#32463;&#36807;&#23454;&#39564;&#39564;&#35777;&#65292;&#26657;&#20934;&#21518;&#30340;IPS&#20272;&#35745;&#22120;&#22312;Coat&#21644;yahoo&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2303.12973</link><description>&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#20013;&#21453;&#20107;&#23454;&#20542;&#21521;&#20272;&#35745;&#30340;&#19981;&#30830;&#23450;&#24615;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
Uncertainty Calibration for Counterfactual Propensity Estimation in Recommendation. (arXiv:2303.12973v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12973
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22810;&#31181;&#19981;&#30830;&#23450;&#24615;&#26657;&#20934;&#25216;&#26415;&#65292;&#20197;&#25913;&#36827;&#25512;&#33616;&#31995;&#32479;&#20013;&#20542;&#21521;&#24615;&#20272;&#35745;&#30340;&#25928;&#26524;&#12290;&#32463;&#36807;&#23454;&#39564;&#39564;&#35777;&#65292;&#26657;&#20934;&#21518;&#30340;IPS&#20272;&#35745;&#22120;&#22312;Coat&#21644;yahoo&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#65292;&#30001;&#20110;&#36873;&#25321;&#20559;&#24046;&#65292;&#35768;&#22810;&#35780;&#20998;&#20449;&#24687;&#37117;&#20002;&#22833;&#20102;&#65292;&#36825;&#34987;&#31216;&#20026;&#38750;&#38543;&#26426;&#32570;&#22833;&#12290;&#21453;&#20107;&#23454;&#36870;&#20542;&#21521;&#35780;&#20998;&#65288;IPS&#65289;&#34987;&#29992;&#20110;&#34913;&#37327;&#27599;&#20010;&#35266;&#23519;&#21040;&#30340;&#35780;&#20998;&#30340;&#22635;&#20805;&#38169;&#35823;&#12290;&#34429;&#28982;&#22312;&#22810;&#31181;&#24773;&#20917;&#19979;&#26377;&#25928;&#65292;&#20294;&#25105;&#20204;&#35748;&#20026;IPS&#20272;&#35745;&#30340;&#24615;&#33021;&#21463;&#21040;&#20542;&#21521;&#24615;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#30340;&#38480;&#21046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22810;&#31181;&#20195;&#34920;&#24615;&#30340;&#19981;&#30830;&#23450;&#24615;&#26657;&#20934;&#25216;&#26415;&#65292;&#20197;&#25913;&#36827;&#25512;&#33616;&#31995;&#32479;&#20013;&#20542;&#21521;&#24615;&#20272;&#35745;&#30340;&#19981;&#30830;&#23450;&#24615;&#26657;&#20934;&#12290;&#36890;&#36807;&#23545;&#20559;&#35823;&#21644;&#25512;&#24191;&#30028;&#38480;&#30340;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#32463;&#36807;&#26657;&#20934;&#30340;IPS&#20272;&#35745;&#22120;&#20248;&#20110;&#26410;&#26657;&#20934;&#30340;IPS&#20272;&#35745;&#22120;&#12290; Coat&#21644;yahoo&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19981;&#30830;&#23450;&#24615;&#26657;&#20934;&#24471;&#21040;&#25913;&#36827;&#65292;&#20174;&#32780;&#20351;&#25512;&#33616;&#32467;&#26524;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recommendation systems, a large portion of the ratings are missing due to the selection biases, which is known as Missing Not At Random. The counterfactual inverse propensity scoring (IPS) was used to weight the imputation error of every observed rating. Although effective in multiple scenarios, we argue that the performance of IPS estimation is limited due to the uncertainty miscalibration of propensity estimation. In this paper, we propose the uncertainty calibration for the propensity estimation in recommendation systems with multiple representative uncertainty calibration techniques. Theoretical analysis on the bias and generalization bound shows the superiority of the calibrated IPS estimator over the uncalibrated one. Experimental results on the coat and yahoo datasets shows that the uncertainty calibration is improved and hence brings the better recommendation results.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20960;&#20010;AL&#21477;&#23376;&#26597;&#35810;&#35780;&#20272;&#20989;&#25968;&#65292;&#20851;&#27880;&#28508;&#22312;&#27491;&#38754;&#26631;&#35760;&#65292;&#24182;&#20351;&#29992;&#26356;&#22909;&#30340;&#25968;&#25454;&#39537;&#21160;&#30340;&#27491;&#24120;&#21270;&#26041;&#27861;&#65292;&#20197;&#26368;&#23567;&#21270;NER&#27880;&#37322;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2111.03837</link><description>&lt;p&gt;
&#38598;&#20013;&#20851;&#27880;&#28508;&#22312;&#21629;&#21517;&#23454;&#20307;&#30340;&#20027;&#21160;&#26631;&#27880;&#33719;&#21462;
&lt;/p&gt;
&lt;p&gt;
Focusing on Potential Named Entities During Active Label Acquisition. (arXiv:2111.03837v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.03837
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20960;&#20010;AL&#21477;&#23376;&#26597;&#35810;&#35780;&#20272;&#20989;&#25968;&#65292;&#20851;&#27880;&#28508;&#22312;&#27491;&#38754;&#26631;&#35760;&#65292;&#24182;&#20351;&#29992;&#26356;&#22909;&#30340;&#25968;&#25454;&#39537;&#21160;&#30340;&#27491;&#24120;&#21270;&#26041;&#27861;&#65292;&#20197;&#26368;&#23567;&#21270;NER&#27880;&#37322;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;(NER)&#26088;&#22312;&#35782;&#21035;&#32467;&#26500;&#21270;&#25991;&#26412;&#20013;&#21629;&#21517;&#23454;&#20307;&#30340;&#25552;&#21450;&#24182;&#23558;&#20854;&#20998;&#31867;&#21040;&#39044;&#23450;&#20041;&#30340;&#21629;&#21517;&#23454;&#20307;&#31867;&#21035;&#20013;&#12290;&#34429;&#28982;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26377;&#21161;&#20110;&#22312;NER&#20013;&#23454;&#29616;&#33391;&#22909;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#20294;&#35768;&#22810;&#29305;&#23450;&#39046;&#22495;&#30340;NER&#24212;&#29992;&#20173;&#38656;&#35201;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#12290;&#20027;&#21160;&#23398;&#20064;(AL)&#26159;&#35299;&#20915;&#26631;&#31614;&#33719;&#21462;&#38382;&#39064;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#24050;&#29992;&#20110;NER&#20219;&#21153;&#65292;&#20197;&#26368;&#23567;&#21270;&#27880;&#37322;&#25104;&#26412;&#32780;&#19981;&#29306;&#29298;&#27169;&#22411;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#26631;&#35760;&#30340;&#20005;&#37325;&#19981;&#22343;&#21248;&#31867;&#20998;&#24067;&#24341;&#20837;&#20102;&#35774;&#35745;&#26377;&#25928;&#30340;NER&#20027;&#21160;&#23398;&#20064;&#26597;&#35810;&#26041;&#27861;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#20010;AL&#21477;&#23376;&#26597;&#35810;&#35780;&#20272;&#20989;&#25968;&#65292;&#26356;&#22810;&#20851;&#27880;&#28508;&#22312;&#30340;&#27491;&#38754;&#26631;&#35760;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;&#21477;&#23376;&#21644;&#26631;&#35760;&#25104;&#26412;&#35780;&#20272;&#31574;&#30053;&#26469;&#35780;&#20272;&#36825;&#20123;&#25552;&#35758;&#30340;&#20989;&#25968;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#26356;&#22909;&#30340;&#25968;&#25454;&#39537;&#21160;&#30340;&#27491;&#24120;&#21270;&#26041;&#27861;&#65292;&#20197;&#24809;&#32602;&#36807;&#38271;&#25110;&#36807;&#30701;&#30340;&#21477;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
Named entity recognition (NER) aims to identify mentions of named entities in an unstructured text and classify them into predefined named entity classes. While deep learning-based pre-trained language models help to achieve good predictive performances in NER, many domain-specific NER applications still call for a substantial amount of labeled data. Active learning (AL), a general framework for the label acquisition problem, has been used for NER tasks to minimize the annotation cost without sacrificing model performance. However, the heavily imbalanced class distribution of tokens introduces challenges in designing effective AL querying methods for NER. We propose several AL sentence query evaluation functions that pay more attention to potential positive tokens, and evaluate these proposed functions with both sentence-based and token-based cost evaluation strategies. We also propose a better data-driven normalization approach to penalize sentences that are too long or too short. Our
&lt;/p&gt;</description></item></channel></rss>