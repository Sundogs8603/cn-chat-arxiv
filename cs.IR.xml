<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#26412;&#30740;&#31350;&#35752;&#35770;&#20102;&#22312;&#30005;&#24433;&#25512;&#33616;&#31995;&#32479;&#20013;&#21033;&#29992;&#31867;&#22411;&#26631;&#31614;&#36827;&#34892;&#20869;&#23481;&#20803;&#25968;&#25454;&#20998;&#26512;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#31867;&#22411;&#39057;&#35889;&#8221;&#30340;&#26032;&#26041;&#27861;&#26469;&#25429;&#25417;&#26631;&#39064;&#20013;&#30340;&#24494;&#22937;&#31867;&#22411;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#35813;&#30740;&#31350;&#36824;&#35752;&#35770;&#20102;LLMs&#22312;&#22686;&#24378;&#20869;&#23481;&#20803;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2309.08787</link><description>&lt;p&gt;
&#36229;&#36234;&#26631;&#31614;: &#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;LLMs&#36827;&#34892;&#20869;&#23481;&#20803;&#25968;&#25454;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Beyond Labels: Leveraging Deep Learning and LLMs for Content Metadata. (arXiv:2309.08787v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08787
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35752;&#35770;&#20102;&#22312;&#30005;&#24433;&#25512;&#33616;&#31995;&#32479;&#20013;&#21033;&#29992;&#31867;&#22411;&#26631;&#31614;&#36827;&#34892;&#20869;&#23481;&#20803;&#25968;&#25454;&#20998;&#26512;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#31867;&#22411;&#39057;&#35889;&#8221;&#30340;&#26032;&#26041;&#27861;&#26469;&#25429;&#25417;&#26631;&#39064;&#20013;&#30340;&#24494;&#22937;&#31867;&#22411;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#35813;&#30740;&#31350;&#36824;&#35752;&#35770;&#20102;LLMs&#22312;&#22686;&#24378;&#20869;&#23481;&#20803;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20869;&#23481;&#20803;&#25968;&#25454;&#22312;&#30005;&#24433;&#25512;&#33616;&#31995;&#32479;&#20013;&#25198;&#28436;&#30528;&#38750;&#24120;&#37325;&#35201;&#30340;&#35282;&#33394;&#65292;&#23427;&#25552;&#20379;&#20102;&#26377;&#20851;&#30005;&#24433;&#30340;&#21508;&#20010;&#26041;&#38754;&#65288;&#22914;&#31867;&#22411;&#12289;&#28436;&#21592;&#12289;&#21095;&#24773;&#27010;&#35201;&#12289;&#31080;&#25151;&#25688;&#35201;&#31561;&#65289;&#30340;&#26377;&#20215;&#20540;&#20449;&#24687;&#12290;&#20998;&#26512;&#20803;&#25968;&#25454;&#21487;&#20197;&#24110;&#21161;&#29702;&#35299;&#29992;&#25143;&#20559;&#22909;&#65292;&#29983;&#25104;&#20010;&#24615;&#21270;&#25512;&#33616;&#21644;&#35299;&#20915;&#39033;&#30446;&#20919;&#21551;&#21160;&#38382;&#39064;&#12290;&#26412;&#25991;&#23558;&#37325;&#28857;&#20851;&#27880;&#19968;&#31181;&#29305;&#23450;&#31867;&#22411;&#30340;&#20803;&#25968;&#25454;&#8212;&#8212;&#8220;&#31867;&#22411;&#8221;&#26631;&#31614;&#12290;&#30005;&#24433;&#25110;&#30005;&#35270;&#21095;&#30340;&#31867;&#22411;&#26631;&#31614;&#26377;&#21161;&#20110;&#23558;&#19968;&#31995;&#21015;&#26631;&#39064;&#20998;&#31867;&#20026;&#19981;&#21516;&#20027;&#39064;&#65292;&#24182;&#30456;&#24212;&#22320;&#35774;&#32622;&#35266;&#20247;&#26399;&#26395;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#31867;&#22411;&#26631;&#31614;&#20449;&#24687;&#25152;&#28041;&#21450;&#30340;&#19968;&#20123;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#31867;&#22411;&#39057;&#35889;&#8221;&#30340;&#26032;&#26041;&#27861;&#26469;&#30740;&#31350;&#31867;&#22411;&#20449;&#24687;&#12290;&#31867;&#22411;&#39057;&#35889;&#26377;&#21161;&#20110;&#25429;&#25417;&#26631;&#39064;&#20013;&#30340;&#21508;&#31181;&#24494;&#22937;&#31867;&#22411;&#65292;&#25105;&#20204;&#30340;&#31163;&#32447;&#21644;&#22312;&#32447;&#23454;&#39564;&#35777;&#23454;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;LLMs&#22312;&#22686;&#24378;&#20869;&#23481;&#20803;&#25968;&#25454;&#20998;&#26512;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Content metadata plays a very important role in movie recommender systems as it provides valuable information about various aspects of a movie such as genre, cast, plot synopsis, box office summary, etc. Analyzing the metadata can help understand the user preferences to generate personalized recommendations and item cold starting. In this talk, we will focus on one particular type of metadata - \textit{genre} labels. Genre labels associated with a movie or a TV series help categorize a collection of titles into different themes and correspondingly setting up the audience expectation. We present some of the challenges associated with using genre label information and propose a new way of examining the genre information that we call as the \textit{Genre Spectrum}. The Genre Spectrum helps capture the various nuanced genres in a title and our offline and online experiments corroborate the effectiveness of the approach. Furthermore, we also talk about applications of LLMs in augmenting con
&lt;/p&gt;</description></item><item><title>&#22312;&#29983;&#21629;&#31185;&#23398;&#39046;&#22495;&#65292;&#21487;&#22797;&#21046;&#30340;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#22270;&#23545;&#20110;&#30830;&#20445;&#30693;&#35782;&#30340;&#21487;&#20449;&#24615;&#21644;&#21487;&#25345;&#32493;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#21019;&#24314;&#21644;&#32500;&#25252;KG&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#23613;&#31649;&#26377;&#36234;&#26469;&#36234;&#22810;&#30340;&#39046;&#22495;&#29305;&#23450;&#30340;KG&#65292;&#20851;&#20110;&#20854;&#21487;&#22797;&#21046;&#24615;&#30340;&#32508;&#21512;&#20998;&#26512;&#20173;&#28982;&#32570;&#20047;&#12290;</title><link>http://arxiv.org/abs/2309.08754</link><description>&lt;p&gt;
&#22312;&#29983;&#21629;&#31185;&#23398;&#20013;&#30340;&#21487;&#22797;&#21046;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#22270;&#30340;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Reproducible Domain-Specific Knowledge Graphs in the Life Sciences: a Systematic Literature Review. (arXiv:2309.08754v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08754
&lt;/p&gt;
&lt;p&gt;
&#22312;&#29983;&#21629;&#31185;&#23398;&#39046;&#22495;&#65292;&#21487;&#22797;&#21046;&#30340;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#22270;&#23545;&#20110;&#30830;&#20445;&#30693;&#35782;&#30340;&#21487;&#20449;&#24615;&#21644;&#21487;&#25345;&#32493;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#21019;&#24314;&#21644;&#32500;&#25252;KG&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#23613;&#31649;&#26377;&#36234;&#26469;&#36234;&#22810;&#30340;&#39046;&#22495;&#29305;&#23450;&#30340;KG&#65292;&#20851;&#20110;&#20854;&#21487;&#22797;&#21046;&#24615;&#30340;&#32508;&#21512;&#20998;&#26512;&#20173;&#28982;&#32570;&#20047;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#65288;KG&#65289;&#34987;&#24191;&#27867;&#29992;&#20110;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#34920;&#31034;&#21644;&#32452;&#32455;&#32467;&#26500;&#21270;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#21019;&#24314;&#21644;&#32500;&#25252;KG&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#24320;&#21457;KG&#38656;&#35201;&#23545;&#25968;&#25454;&#24314;&#27169;&#12289;&#26412;&#20307;&#35774;&#35745;&#21644;&#25968;&#25454;&#25972;&#29702;&#20855;&#26377;&#24191;&#27867;&#30340;&#19987;&#19994;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;KG&#26159;&#21160;&#24577;&#30340;&#65292;&#38656;&#35201;&#25345;&#32493;&#26356;&#26032;&#21644;&#36136;&#37327;&#25511;&#21046;&#20197;&#30830;&#20445;&#20934;&#30830;&#24615;&#21644;&#30456;&#20851;&#24615;&#12290;&#36825;&#20123;&#22797;&#26434;&#24615;&#23548;&#33268;&#20102;&#23545;&#20854;&#24320;&#21457;&#21644;&#32500;&#25252;&#30340;&#30456;&#24403;&#22823;&#30340;&#21162;&#21147;&#12290;&#20540;&#24471;&#20851;&#27880;&#30340;&#19968;&#39033;&#20851;&#38190;&#32500;&#24230;&#26159;&#21487;&#22797;&#21046;&#24615;&#12290;&#33021;&#22815;&#22797;&#21046;&#21644;&#39564;&#35777;KG&#23545;&#20110;&#30830;&#20445;&#25152;&#34920;&#31034;&#30340;&#30693;&#35782;&#30340;&#21487;&#20449;&#24615;&#21644;&#21487;&#25345;&#32493;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#21487;&#22797;&#21046;&#30340;KG&#19981;&#20165;&#36890;&#36807;&#20801;&#35768;&#20182;&#20154;&#22312;&#29616;&#26377;&#30693;&#35782;&#30340;&#22522;&#30784;&#19978;&#36827;&#34892;&#24314;&#35774;&#26469;&#25903;&#25345;&#24320;&#25918;&#31185;&#23398;&#65292;&#32780;&#19988;&#36824;&#22686;&#24378;&#20102;&#20449;&#24687;&#20256;&#25773;&#30340;&#36879;&#26126;&#24230;&#21644;&#21487;&#38752;&#24615;&#12290;&#23613;&#31649;&#26377;&#36234;&#26469;&#36234;&#22810;&#30340;&#39046;&#22495;&#29305;&#23450;&#30340;KG&#65292;&#20851;&#20110;&#20854;&#21487;&#22797;&#21046;&#24615;&#30340;&#32508;&#21512;&#20998;&#26512;&#20173;&#28982;&#32570;&#20047;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graphs (KGs) are widely used for representing and organizing structured knowledge in diverse domains. However, the creation and upkeep of KGs pose substantial challenges. Developing a KG demands extensive expertise in data modeling, ontology design, and data curation. Furthermore, KGs are dynamic, requiring continuous updates and quality control to ensure accuracy and relevance. These intricacies contribute to the considerable effort required for their development and maintenance. One critical dimension of KGs that warrants attention is reproducibility. The ability to replicate and validate KGs is fundamental for ensuring the trustworthiness and sustainability of the knowledge they represent. Reproducible KGs not only support open science by allowing others to build upon existing knowledge but also enhance transparency and reliability in disseminating information. Despite the growing number of domain-specific KGs, a comprehensive analysis concerning their reproducibility has 
&lt;/p&gt;</description></item><item><title>FedFNN&#26159;&#19968;&#31181;&#22312;&#32852;&#37030;&#25512;&#33616;&#31995;&#32479;&#20013;&#21152;&#36895;&#35757;&#32451;&#30340;&#31639;&#27861;&#12290;&#36890;&#36807;&#39044;&#27979;&#26410;&#25277;&#26679;&#29992;&#25143;&#30340;&#26435;&#37325;&#26356;&#26032;&#65292;&#20351;&#29992;&#24050;&#25277;&#26679;&#38598;&#30340;&#26356;&#26032;&#65292;FedFNN&#23454;&#29616;&#20102;&#27604;&#20854;&#20182;&#26041;&#27861;&#24555;5&#20493;&#30340;&#35757;&#32451;&#36895;&#24230;&#65292;&#21516;&#26102;&#20445;&#25345;&#25110;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.08635</link><description>&lt;p&gt;
FedFNN: &#22312;&#32852;&#37030;&#25512;&#33616;&#31995;&#32479;&#20013;&#36890;&#36807;&#26356;&#26032;&#39044;&#27979;&#23454;&#29616;&#26356;&#24555;&#30340;&#35757;&#32451;&#25910;&#25947;
&lt;/p&gt;
&lt;p&gt;
FedFNN: Faster Training Convergence Through Update Predictions in Federated Recommender Systems. (arXiv:2309.08635v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08635
&lt;/p&gt;
&lt;p&gt;
FedFNN&#26159;&#19968;&#31181;&#22312;&#32852;&#37030;&#25512;&#33616;&#31995;&#32479;&#20013;&#21152;&#36895;&#35757;&#32451;&#30340;&#31639;&#27861;&#12290;&#36890;&#36807;&#39044;&#27979;&#26410;&#25277;&#26679;&#29992;&#25143;&#30340;&#26435;&#37325;&#26356;&#26032;&#65292;&#20351;&#29992;&#24050;&#25277;&#26679;&#38598;&#30340;&#26356;&#26032;&#65292;FedFNN&#23454;&#29616;&#20102;&#27604;&#20854;&#20182;&#26041;&#27861;&#24555;5&#20493;&#30340;&#35757;&#32451;&#36895;&#24230;&#65292;&#21516;&#26102;&#20445;&#25345;&#25110;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#24050;&#25104;&#20026;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#30340;&#20851;&#38190;&#26041;&#27861;&#65292;&#22686;&#24378;&#20102;&#22312;&#32447;&#20010;&#24615;&#21270;&#30340;&#21516;&#26102;&#30830;&#20445;&#29992;&#25143;&#25968;&#25454;&#30340;&#38544;&#31169;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#23558;&#31169;&#26377;&#25968;&#25454;&#21457;&#36865;&#21040;&#20013;&#22830;&#26381;&#21153;&#22120;&#19981;&#21516;&#65292;FL&#23558;&#35745;&#31639;&#20998;&#25955;&#65306;&#35774;&#22791;&#22312;&#26412;&#22320;&#35757;&#32451;&#24182;&#19982;&#20840;&#23616;&#26381;&#21153;&#22120;&#20849;&#20139;&#26356;&#26032;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20027;&#35201;&#25361;&#25112;&#26159;&#23454;&#29616;&#24555;&#36895;&#21644;&#20934;&#30830;&#30340;&#27169;&#22411;&#35757;&#32451;&#65292;&#36825;&#23545;&#20110;&#25512;&#33616;&#31995;&#32479;&#26469;&#35828;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#24310;&#36831;&#21487;&#33021;&#20250;&#25439;&#23475;&#29992;&#25143;&#21442;&#19982;&#24230;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;FedFNN&#65292;&#19968;&#31181;&#21152;&#36895;&#20998;&#25955;&#24335;&#27169;&#22411;&#35757;&#32451;&#30340;&#31639;&#27861;&#12290;&#22312;FL&#20013;&#65292;&#27599;&#20010;&#35757;&#32451;&#21608;&#26399;&#20165;&#28041;&#21450;&#29992;&#25143;&#23376;&#38598;&#12290;FedFNN&#21033;&#29992;&#30417;&#30563;&#23398;&#20064;&#20174;&#26410;&#25277;&#26679;&#30340;&#29992;&#25143;&#20013;&#39044;&#27979;&#26435;&#37325;&#26356;&#26032;&#65292;&#20351;&#29992;&#26469;&#33258;&#25277;&#26679;&#38598;&#30340;&#26356;&#26032;&#12290;&#25105;&#20204;&#20351;&#29992;&#30495;&#23454;&#21644;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#65306;1. FedFNN&#30340;&#35757;&#32451;&#36895;&#24230;&#27604;&#39046;&#20808;&#26041;&#27861;&#24555;5&#20493;&#65292;&#20445;&#25345;&#25110;&#25552;&#39640;&#20934;&#30830;&#24615;&#65307;2. &#35813;&#31639;&#27861;&#30340;&#24615;&#33021;&#19982;&#23458;&#25143;&#31471;&#38598;&#32676;&#30340;&#21464;&#21270;&#26080;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) has emerged as a key approach for distributed machine learning, enhancing online personalization while ensuring user data privacy. Instead of sending private data to a central server as in traditional approaches, FL decentralizes computations: devices train locally and share updates with a global server. A primary challenge in this setting is achieving fast and accurate model training - vital for recommendation systems where delays can compromise user engagement. This paper introduces FedFNN, an algorithm that accelerates decentralized model training. In FL, only a subset of users are involved in each training epoch. FedFNN employs supervised learning to predict weight updates from unsampled users, using updates from the sampled set. Our evaluations, using real and synthetic data, show: 1. FedFNN achieves training speeds 5x faster than leading methods, maintaining or improving accuracy; 2. the algorithm's performance is consistent regardless of client cluster va
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#21160;&#24577;&#20027;&#39064;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20998;&#26512;&#20102;&#27599;&#20010;&#20027;&#39064;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#36136;&#37327;&#21464;&#21270;&#65292;&#24182;&#32467;&#21512;&#20102;&#27169;&#22411;&#30340;&#26102;&#38388;&#19968;&#33268;&#24615;&#12290;&#35813;&#26041;&#27861;&#22312;&#21512;&#25104;&#25968;&#25454;&#21644;&#24050;&#26377;DTMs&#25968;&#25454;&#19978;&#23637;&#31034;&#20102;&#23454;&#29992;&#24615;&#65292;&#24182;&#19982;&#20154;&#31867;&#21028;&#26029;&#20855;&#26377;&#33391;&#22909;&#30340;&#30456;&#20851;&#24615;&#12290;&#36825;&#20123;&#30740;&#31350;&#32467;&#26524;&#23545;&#20110;&#35782;&#21035;&#21464;&#21270;&#30340;&#20027;&#39064;&#12289;&#35780;&#20272;DTMs&#21644;&#25351;&#23548;&#26410;&#26469;&#30740;&#31350;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2309.08627</link><description>&lt;p&gt;
&#35780;&#20272;&#21160;&#24577;&#20027;&#39064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Evaluating Dynamic Topic Models. (arXiv:2309.08627v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08627
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#21160;&#24577;&#20027;&#39064;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20998;&#26512;&#20102;&#27599;&#20010;&#20027;&#39064;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#36136;&#37327;&#21464;&#21270;&#65292;&#24182;&#32467;&#21512;&#20102;&#27169;&#22411;&#30340;&#26102;&#38388;&#19968;&#33268;&#24615;&#12290;&#35813;&#26041;&#27861;&#22312;&#21512;&#25104;&#25968;&#25454;&#21644;&#24050;&#26377;DTMs&#25968;&#25454;&#19978;&#23637;&#31034;&#20102;&#23454;&#29992;&#24615;&#65292;&#24182;&#19982;&#20154;&#31867;&#21028;&#26029;&#20855;&#26377;&#33391;&#22909;&#30340;&#30456;&#20851;&#24615;&#12290;&#36825;&#20123;&#30740;&#31350;&#32467;&#26524;&#23545;&#20110;&#35782;&#21035;&#21464;&#21270;&#30340;&#20027;&#39064;&#12289;&#35780;&#20272;DTMs&#21644;&#25351;&#23548;&#26410;&#26469;&#30740;&#31350;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#20027;&#39064;&#27169;&#22411;(DTMs)&#22312;&#35780;&#20272;&#20027;&#39064;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#36827;&#23637;&#26041;&#38754;&#32570;&#20047;&#23450;&#37327;&#25351;&#26631;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;DTMs&#35780;&#20272;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20998;&#26512;&#20102;&#27599;&#20010;&#20027;&#39064;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#36136;&#37327;&#21464;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#32467;&#21512;&#20027;&#39064;&#36136;&#37327;&#21644;&#27169;&#22411;&#26102;&#38388;&#19968;&#33268;&#24615;&#30340;&#25193;&#23637;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#21512;&#25104;&#25968;&#25454;&#21644;&#24050;&#26377;DTMs&#30340;&#25968;&#25454;&#26469;&#35777;&#26126;&#20854;&#23454;&#29992;&#24615;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#20154;&#24037;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#19982;&#20154;&#31867;&#21028;&#26029;&#20855;&#26377;&#33391;&#22909;&#30340;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26377;&#21161;&#20110;&#35782;&#21035;&#21464;&#21270;&#30340;&#20027;&#39064;&#12289;&#35780;&#20272;&#19981;&#21516;&#30340;DTMs&#20197;&#21450;&#25351;&#23548;&#26410;&#26469;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is a lack of quantitative measures to evaluate the progression of topics through time in dynamic topic models (DTMs). Filling this gap, we propose a novel evaluation measure for DTMs that analyzes the changes in the quality of each topic over time. Additionally, we propose an extension combining topic quality with the model's temporal consistency. We demonstrate the utility of the proposed measure by applying it to synthetic data and data from existing DTMs. We also conducted a human evaluation, which indicates that the proposed measure correlates well with human judgment. Our findings may help in identifying changing topics, evaluating different DTMs, and guiding future research in this area.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#20351;&#29992;&#20302;&#31209;MDP&#23558;&#25512;&#33616;&#38382;&#39064;&#35270;&#20026;&#22312;&#32447;RL&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#25552;&#20986;&#30340;&#31639;&#27861;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#34920;&#31034;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2309.08622</link><description>&lt;p&gt;
&#22522;&#20110;&#20302;&#31209;Slate&#30340;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Representation Learning in Low-rank Slate-based Recommender Systems. (arXiv:2309.08622v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08622
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#20351;&#29992;&#20302;&#31209;MDP&#23558;&#25512;&#33616;&#38382;&#39064;&#35270;&#20026;&#22312;&#32447;RL&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#25552;&#20986;&#30340;&#31639;&#27861;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#34920;&#31034;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#24212;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#21487;&#20197;&#20248;&#21270;&#38271;&#26399;&#29992;&#25143;&#21442;&#19982;&#24230;&#30340;&#25512;&#33616;&#12290;&#28982;&#32780;&#65292;&#35813;&#29615;&#22659;&#36890;&#24120;&#21253;&#21547;&#22823;&#37327;&#30340;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#65292;&#36825;&#20351;&#24471;&#23398;&#20064;&#21644;&#25506;&#32034;&#21464;&#24471;&#22256;&#38590;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26679;&#26412;&#39640;&#25928;&#30340;&#34920;&#31034;&#23398;&#20064;&#31639;&#27861;&#65292;&#20351;&#29992;&#26631;&#20934;&#30340;slate&#25512;&#33616;&#35774;&#32622;&#65292;&#23558;&#20854;&#35270;&#20026;&#20302;&#31209;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#30340;&#22312;&#32447;RL&#38382;&#39064;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#25552;&#20986;&#30340;&#35774;&#32622;&#21644;&#37319;&#26679;&#26041;&#27861;&#26500;&#24314;&#20102;&#25512;&#33616;&#27169;&#25311;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) in recommendation systems offers the potential to optimize recommendations for long-term user engagement. However, the environment often involves large state and action spaces, which makes it hard to efficiently learn and explore. In this work, we propose a sample-efficient representation learning algorithm, using the standard slate recommendation setup, to treat this as an online RL problem with low-rank Markov decision processes (MDPs). We also construct the recommender simulation environment with the proposed setup and sampling method.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#31038;&#20250;&#36873;&#25321;&#26426;&#21046;&#65292;&#25506;&#32034;&#20102;&#22810;&#20010;&#22810;&#26041;&#38754;&#20844;&#24179;&#24212;&#29992;&#20013;&#30340;&#36873;&#25321;&#26426;&#21046;&#36873;&#39033;&#65292;&#32467;&#26524;&#26174;&#31034;&#19981;&#21516;&#30340;&#36873;&#25321;&#21644;&#20998;&#37197;&#26426;&#21046;&#20250;&#20135;&#29983;&#19981;&#21516;&#20294;&#19968;&#33268;&#30340;&#20844;&#24179;&#24615;/&#20934;&#30830;&#24615;&#26435;&#34913;&#32467;&#26524;&#65292;&#24182;&#19988;&#22810;&#26234;&#33021;&#20307;&#30340;&#26500;&#25104;&#20351;&#24471;&#31995;&#32479;&#33021;&#22815;&#36866;&#24212;&#29992;&#25143;&#20154;&#21475;&#30340;&#21160;&#24577;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2309.08621</link><description>&lt;p&gt;
&#22312;SCRUF&#20013;&#25506;&#32034;&#25512;&#33616;&#20844;&#24179;&#24615;&#30340;&#31038;&#20250;&#36873;&#25321;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Exploring Social Choice Mechanisms for Recommendation Fairness in SCRUF. (arXiv:2309.08621v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08621
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#31038;&#20250;&#36873;&#25321;&#26426;&#21046;&#65292;&#25506;&#32034;&#20102;&#22810;&#20010;&#22810;&#26041;&#38754;&#20844;&#24179;&#24212;&#29992;&#20013;&#30340;&#36873;&#25321;&#26426;&#21046;&#36873;&#39033;&#65292;&#32467;&#26524;&#26174;&#31034;&#19981;&#21516;&#30340;&#36873;&#25321;&#21644;&#20998;&#37197;&#26426;&#21046;&#20250;&#20135;&#29983;&#19981;&#21516;&#20294;&#19968;&#33268;&#30340;&#20844;&#24179;&#24615;/&#20934;&#30830;&#24615;&#26435;&#34913;&#32467;&#26524;&#65292;&#24182;&#19988;&#22810;&#26234;&#33021;&#20307;&#30340;&#26500;&#25104;&#20351;&#24471;&#31995;&#32479;&#33021;&#22815;&#36866;&#24212;&#29992;&#25143;&#20154;&#21475;&#30340;&#21160;&#24577;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#24448;&#24448;&#22312;&#23454;&#36341;&#20013;&#20855;&#26377;&#22797;&#26434;&#24615;&#65292;&#32780;&#36825;&#19968;&#28857;&#22312;&#31616;&#21270;&#30340;&#30740;&#31350;&#20844;&#24335;&#20013;&#24182;&#27809;&#26377;&#24471;&#21040;&#20805;&#20998;&#30340;&#20307;&#29616;&#12290;&#22312;&#23545;&#20844;&#24179;&#24615;&#38382;&#39064;&#36827;&#34892;&#31038;&#20250;&#36873;&#25321;&#30340;&#26694;&#26550;&#20013;&#65292;&#21487;&#20197;&#22312;&#22810;&#26234;&#33021;&#20307;&#30340;&#20844;&#24179;&#24615;&#20851;&#27880;&#22522;&#30784;&#19978;&#25552;&#20379;&#19968;&#31181;&#28789;&#27963;&#19988;&#22810;&#26041;&#38754;&#30340;&#20844;&#24179;&#24615;&#24863;&#30693;&#25512;&#33616;&#26041;&#27861;&#12290;&#21033;&#29992;&#31038;&#20250;&#36873;&#25321;&#21487;&#20197;&#22686;&#21152;&#36890;&#29992;&#24615;&#65292;&#24182;&#26377;&#21487;&#33021;&#21033;&#29992;&#32463;&#36807;&#30740;&#31350;&#30340;&#31038;&#20250;&#36873;&#25321;&#31639;&#27861;&#35299;&#20915;&#22810;&#20010;&#31454;&#20105;&#30340;&#20844;&#24179;&#24615;&#20851;&#27880;&#20043;&#38388;&#30340;&#32039;&#24352;&#20851;&#31995;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#22810;&#26041;&#38754;&#20844;&#24179;&#24212;&#29992;&#20013;&#36873;&#25321;&#26426;&#21046;&#30340;&#19968;&#31995;&#21015;&#36873;&#39033;&#65292;&#20351;&#29992;&#30495;&#23454;&#25968;&#25454;&#21644;&#21512;&#25104;&#25968;&#25454;&#65292;&#32467;&#26524;&#26174;&#31034;&#19981;&#21516;&#31867;&#21035;&#30340;&#36873;&#25321;&#21644;&#20998;&#37197;&#26426;&#21046;&#22312;&#20844;&#24179;&#24615;/&#20934;&#30830;&#24615;&#26435;&#34913;&#26041;&#38754;&#20135;&#29983;&#20102;&#19981;&#21516;&#20294;&#19968;&#33268;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#36824;&#34920;&#26126;&#65292;&#22810;&#26234;&#33021;&#20307;&#30340;&#26500;&#25104;&#25552;&#20379;&#20102;&#36866;&#24212;&#29992;&#25143;&#20154;&#21475;&#21160;&#24577;&#30340;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fairness problems in recommender systems often have a complexity in practice that is not adequately captured in simplified research formulations. A social choice formulation of the fairness problem, operating within a multi-agent architecture of fairness concerns, offers a flexible and multi-aspect alternative to fairness-aware recommendation approaches. Leveraging social choice allows for increased generality and the possibility of tapping into well-studied social choice algorithms for resolving the tension between multiple, competing fairness concerns. This paper explores a range of options for choice mechanisms in multi-aspect fairness applications using both real and synthetic data and shows that different classes of choice and allocation mechanisms yield different but consistent fairness / accuracy tradeoffs. We also show that a multi-agent formulation offers flexibility in adapting to user population dynamics.
&lt;/p&gt;</description></item><item><title>Drifter&#26159;&#19968;&#20010;&#39640;&#25928;&#30340;&#22312;&#32447;&#29305;&#24449;&#30417;&#25511;&#31995;&#32479;&#65292;&#36890;&#36807;&#25935;&#25463;&#12289;&#21709;&#24212;&#21644;&#36866;&#24212;&#24615;&#30340;&#25968;&#25454;&#36136;&#37327;&#30417;&#25511;&#65292;&#23454;&#26102;&#20998;&#26512;&#12289;&#26816;&#27979;&#21644;&#35299;&#20915;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#25968;&#25454;&#38382;&#39064;&#65292;&#20351;&#24471;&#23454;&#26102;&#25512;&#33616;&#31995;&#32479;&#30340;&#21487;&#38752;&#24615;&#21644;&#24615;&#33021;&#24471;&#21040;&#26174;&#33879;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2309.08617</link><description>&lt;p&gt;
Drifter: &#22823;&#35268;&#27169;&#25512;&#33616;&#31995;&#32479;&#20013;&#39640;&#25928;&#30340;&#22312;&#32447;&#29305;&#24449;&#30417;&#25511;&#20197;&#25552;&#39640;&#25968;&#25454;&#23436;&#25972;&#24615;
&lt;/p&gt;
&lt;p&gt;
Drifter: Efficient Online Feature Monitoring for Improved Data Integrity in Large-Scale Recommendation Systems. (arXiv:2309.08617v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08617
&lt;/p&gt;
&lt;p&gt;
Drifter&#26159;&#19968;&#20010;&#39640;&#25928;&#30340;&#22312;&#32447;&#29305;&#24449;&#30417;&#25511;&#31995;&#32479;&#65292;&#36890;&#36807;&#25935;&#25463;&#12289;&#21709;&#24212;&#21644;&#36866;&#24212;&#24615;&#30340;&#25968;&#25454;&#36136;&#37327;&#30417;&#25511;&#65292;&#23454;&#26102;&#20998;&#26512;&#12289;&#26816;&#27979;&#21644;&#35299;&#20915;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#25968;&#25454;&#38382;&#39064;&#65292;&#20351;&#24471;&#23454;&#26102;&#25512;&#33616;&#31995;&#32479;&#30340;&#21487;&#38752;&#24615;&#21644;&#24615;&#33021;&#24471;&#21040;&#26174;&#33879;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#38469;&#29983;&#20135;&#31995;&#32479;&#36890;&#24120;&#38754;&#20020;&#22312;&#22823;&#35268;&#27169;&#12289;&#21160;&#24577;&#27969;&#20013;&#32500;&#25252;&#25968;&#25454;&#36136;&#37327;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;Drifter&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#20013;&#22312;&#32447;&#29305;&#24449;&#30417;&#25511;&#21644;&#39564;&#35777;&#30340;&#39640;&#25928;&#19988;&#36731;&#37327;&#32423;&#30340;&#31995;&#32479;&#12290;Drifter&#36890;&#36807;&#25552;&#20379;&#25935;&#25463;&#12289;&#21709;&#24212;&#21644;&#36866;&#24212;&#24615;&#30340;&#25968;&#25454;&#36136;&#37327;&#30417;&#25511;&#65292;&#33021;&#22815;&#23454;&#26102;&#36827;&#34892;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#12289;&#28418;&#31227;&#26816;&#27979;&#20197;&#21450;&#23545;&#26377;&#38382;&#39064;&#30340;&#29983;&#20135;&#20107;&#20214;&#30340;&#27934;&#23519;&#12290;Drifter&#38598;&#25104;&#20102;&#26368;&#20808;&#36827;&#30340;&#31232;&#30095;&#25968;&#25454;&#22312;&#32447;&#29305;&#24449;&#25490;&#21517;&#21644;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#20855;&#26377;&#39640;&#24230;&#21487;&#25193;&#23637;&#24615;&#21644;&#36164;&#28304;&#25928;&#29575;&#24615;&#65292;&#27599;&#20998;&#38047;&#22788;&#29702;&#25968;&#30334;&#19975;&#20010;&#23454;&#20363;&#20165;&#38656;&#35201;&#20004;&#20010;&#32447;&#31243;&#21644;&#23569;&#20110;&#19968;GB&#30340;RAM&#12290;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#35777;&#26126;&#20102;Drifter&#22312;&#35686;&#25253;&#21644;&#32531;&#35299;&#25968;&#25454;&#36136;&#37327;&#38382;&#39064;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#23454;&#26102;&#23454;&#20917;&#25512;&#33616;&#31995;&#32479;&#30340;&#21487;&#38752;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-world production systems often grapple with maintaining data quality in large-scale, dynamic streams. We introduce Drifter, an efficient and lightweight system for online feature monitoring and verification in recommendation use cases. Drifter addresses limitations of existing methods by delivering agile, responsive, and adaptable data quality monitoring, enabling real-time root cause analysis, drift detection and insights into problematic production events. Integrating state-of-the-art online feature ranking for sparse data and anomaly detection ideas, Drifter is highly scalable and resource-efficient, requiring only two threads and less than a gigabyte of RAM per production deployments that handle millions of instances per minute. Evaluation on real-world data sets demonstrates Drifter's effectiveness in alerting and mitigating data quality issues, substantially improving reliability and performance of real-time live recommender systems.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#21307;&#30103;&#39046;&#22495;&#20013;&#21033;&#29992;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#25512;&#33616;&#31995;&#32479;&#36827;&#34892;&#30142;&#30149;&#24182;&#21457;&#30151;&#39044;&#27979;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#20351;&#29992;&#20102;NCF&#21644;DHF&#20004;&#31181;&#26032;&#39062;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#24182;&#21033;&#29992;&#20102;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#39044;&#27979;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;NCF&#27169;&#22411;&#22312;&#20934;&#30830;&#29575;&#21644;&#21629;&#20013;&#29575;&#26041;&#38754;&#34920;&#29616;&#36739;&#24046;&#12290;</title><link>http://arxiv.org/abs/2309.08613</link><description>&lt;p&gt;
&#39044;&#27979;&#30142;&#30149;&#24182;&#21457;&#30151;&#20013;&#30340;&#22810;&#27169;&#24577;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Multimodal Recommender Systems in the Prediction of Disease Comorbidity. (arXiv:2309.08613v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08613
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#21307;&#30103;&#39046;&#22495;&#20013;&#21033;&#29992;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#25512;&#33616;&#31995;&#32479;&#36827;&#34892;&#30142;&#30149;&#24182;&#21457;&#30151;&#39044;&#27979;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#20351;&#29992;&#20102;NCF&#21644;DHF&#20004;&#31181;&#26032;&#39062;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#24182;&#21033;&#29992;&#20102;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#39044;&#27979;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;NCF&#27169;&#22411;&#22312;&#20934;&#30830;&#29575;&#21644;&#21629;&#20013;&#29575;&#26041;&#38754;&#34920;&#29616;&#36739;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21327;&#21516;&#36807;&#28388;&#25512;&#33616;&#31995;&#32479;&#24050;&#32463;&#22312;&#20854;&#20182;&#39046;&#22495;&#30340;&#25512;&#33616;&#20013;&#24471;&#21040;&#26222;&#36941;&#24212;&#29992;&#65292;&#20294;&#22312;&#21307;&#30103;&#39046;&#22495;&#30340;&#24212;&#29992;&#36824;&#24456;&#26377;&#38480;&#12290;&#38500;&#20102;&#24314;&#27169;&#29992;&#25143;-&#39033;&#30446;&#20132;&#20114;&#20043;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#25512;&#33616;&#31995;&#32479;&#21487;&#20197;&#29992;&#20110;&#24314;&#27169;&#20027;&#39064;-&#30142;&#30149;&#30721;&#20132;&#20114;&#12290;&#25105;&#20204;&#21033;&#29992;&#31070;&#32463;&#21327;&#21516;&#36807;&#28388;(NCF)&#21644;&#28145;&#24230;&#28151;&#21512;&#36807;&#28388;(DHF)&#36825;&#20004;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#25512;&#33616;&#31995;&#32479;&#22312;&#30142;&#30149;&#35786;&#26029;&#20013;&#36827;&#34892;&#20102;&#20004;&#31181;&#26032;&#39062;&#30340;&#24212;&#29992;&#65292;&#22522;&#20110;&#24050;&#30693;&#30340;&#36807;&#21435;&#24739;&#32773;&#24182;&#21457;&#30151;&#26469;&#36827;&#34892;&#39044;&#27979;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#20004;&#20010;&#25968;&#25454;&#38598;&#65292;&#19968;&#20010;&#21253;&#21547;MIMIC-III&#25968;&#25454;&#24211;&#20013;&#30340;&#25152;&#26377;&#20027;&#39064;-&#30142;&#30149;&#30721;&#23545;&#65292;&#21478;&#19968;&#20010;&#21253;&#21547;&#21457;&#29983;&#26368;&#24120;&#35265;&#30340;50&#31181;&#30142;&#30149;&#12290;&#20934;&#30830;&#29575;&#21644;Hit Ratio@10&#34987;&#29992;&#20316;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#30340;&#25351;&#26631;&#12290;&#21457;&#29616;&#21033;&#29992;&#20943;&#23569;&#30340;&#8220;top 50&#8221; ICD-9&#30721;&#25968;&#25454;&#38598;&#30340;NCF&#27169;&#22411;&#30340;&#24615;&#33021;&#36739;&#20302;(&#20934;&#30830;&#29575;&#32422;&#20026;80%&#21644;Hit Ratio@10&#20026;...
&lt;/p&gt;
&lt;p&gt;
While deep-learning based recommender systems utilizing collaborative filtering have been commonly used for recommendation in other domains, their application in the medical domain have been limited. In addition to modeling user-item interactions, we show that deep-learning based recommender systems can be used to model subject-disease code interactions. Two novel applications of deep learning-based recommender systems using Neural Collaborative Filtering (NCF) and Deep Hybrid Filtering (DHF) were utilized for disease diagnosis based on known past patient comorbidities. Two datasets, one incorporating all subject-disease code pairs present in the MIMIC-III database, and the other incorporating the top 50 most commonly occurring diseases, were used for prediction. Accuracy and Hit Ratio@10 were utilized as metrics to estimate model performance. The performance of the NCF model making use of the reduced "top 50" ICD-9 code dataset was found to be lower (accuracy of ~80% and hit ratio@10 
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#30740;&#31350;&#25552;&#20986;&#20102;PixelRec&#65292;&#19968;&#20010;&#22823;&#35268;&#27169;&#22270;&#20687;&#25512;&#33616;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;2&#20159;&#20010;&#29992;&#25143;-&#22270;&#20687;&#20132;&#20114;&#12289;30&#19975;&#20010;&#29992;&#25143;&#21644;40&#19975;&#20010;&#39640;&#36136;&#37327;&#23553;&#38754;&#22270;&#20687;&#12290;&#36890;&#36807;&#25552;&#20379;&#21407;&#22987;&#22270;&#20687;&#20687;&#32032;&#30340;&#30452;&#25509;&#35775;&#38382;&#65292;PixelRec&#20351;&#24471;&#25512;&#33616;&#27169;&#22411;&#33021;&#22815;&#30452;&#25509;&#20174;&#22270;&#20687;&#23398;&#20064;&#39033;&#30446;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2309.06789</link><description>&lt;p&gt;
&#20351;&#29992;&#21407;&#22987;&#20687;&#32032;&#20026;&#22522;&#20934;&#30340;&#25512;&#33616;&#31995;&#32479;&#22270;&#20687;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
An Image Dataset for Benchmarking Recommender Systems with Raw Pixels. (arXiv:2309.06789v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06789
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#30740;&#31350;&#25552;&#20986;&#20102;PixelRec&#65292;&#19968;&#20010;&#22823;&#35268;&#27169;&#22270;&#20687;&#25512;&#33616;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;2&#20159;&#20010;&#29992;&#25143;-&#22270;&#20687;&#20132;&#20114;&#12289;30&#19975;&#20010;&#29992;&#25143;&#21644;40&#19975;&#20010;&#39640;&#36136;&#37327;&#23553;&#38754;&#22270;&#20687;&#12290;&#36890;&#36807;&#25552;&#20379;&#21407;&#22987;&#22270;&#20687;&#20687;&#32032;&#30340;&#30452;&#25509;&#35775;&#38382;&#65292;PixelRec&#20351;&#24471;&#25512;&#33616;&#27169;&#22411;&#33021;&#22815;&#30452;&#25509;&#20174;&#22270;&#20687;&#23398;&#20064;&#39033;&#30446;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#65288;RS&#65289;&#36890;&#36807;&#21033;&#29992;&#26126;&#30830;&#30340;&#35782;&#21035;&#65288;ID&#65289;&#29305;&#24449;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#20294;&#26159;&#65292;&#20869;&#23481;&#29305;&#24449;&#65292;&#23588;&#20854;&#26159;&#32431;&#22270;&#20687;&#20687;&#32032;&#29305;&#24449;&#30340;&#20840;&#37096;&#28508;&#21147;&#20173;&#28982;&#30456;&#23545;&#26410;&#34987;&#24320;&#21457;&#12290;&#22823;&#35268;&#27169;&#12289;&#22810;&#26679;&#21270;&#12289;&#20197;&#20869;&#23481;&#20026;&#39537;&#21160;&#30340;&#22270;&#20687;&#25512;&#33616;&#25968;&#25454;&#38598;&#30340;&#26377;&#38480;&#21487;&#29992;&#24615;&#38459;&#30861;&#20102;&#20351;&#29992;&#21407;&#22987;&#22270;&#20687;&#20316;&#20026;&#39033;&#30446;&#34920;&#31034;&#30340;&#33021;&#21147;&#12290;&#22312;&#36825;&#26041;&#38754;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PixelRec&#65292;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#20197;&#22270;&#20687;&#20026;&#20013;&#24515;&#30340;&#25512;&#33616;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#32422;2&#20159;&#20010;&#29992;&#25143;-&#22270;&#20687;&#20132;&#20114;&#12289;3000&#19975;&#20010;&#29992;&#25143;&#21644;40&#19975;&#20010;&#39640;&#36136;&#37327;&#23553;&#38754;&#22270;&#20687;&#12290;&#36890;&#36807;&#30452;&#25509;&#35775;&#38382;&#21407;&#22987;&#22270;&#20687;&#20687;&#32032;&#65292;PixelRec&#20351;&#24471;&#25512;&#33616;&#27169;&#22411;&#33021;&#22815;&#30452;&#25509;&#20174;&#20013;&#23398;&#20064;&#39033;&#30446;&#34920;&#31034;&#12290;&#20026;&#20102;&#35777;&#26126;&#20854;&#25928;&#29992;&#65292;&#25105;&#20204;&#39318;&#20808;&#21576;&#29616;&#20102;&#22312;PixelRec&#19978;&#35757;&#32451;&#30340;&#20960;&#20010;&#32463;&#20856;&#32431;ID&#22522;&#32447;&#27169;&#22411;&#65288;&#31216;&#20026;IDNet&#65289;&#30340;&#32467;&#26524;&#12290;&#28982;&#21518;&#65292;&#20026;&#20102;&#23637;&#31034;&#25968;&#25454;&#38598;&#30340;&#22270;&#20687;&#29305;&#24449;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#23558;&#39033;&#30446;ID&#23884;&#20837;&#65288;&#26469;&#33258;IDNet&#65289;&#19982;&#19968;&#20010;...
&lt;/p&gt;
&lt;p&gt;
Recommender systems (RS) have achieved significant success by leveraging explicit identification (ID) features. However, the full potential of content features, especially the pure image pixel features, remains relatively unexplored. The limited availability of large, diverse, and content-driven image recommendation datasets has hindered the use of raw images as item representations. In this regard, we present PixelRec, a massive image-centric recommendation dataset that includes approximately 200 million user-image interactions, 30 million users, and 400,000 high-quality cover images. By providing direct access to raw image pixels, PixelRec enables recommendation models to learn item representation directly from them. To demonstrate its utility, we begin by presenting the results of several classical pure ID-based baseline models, termed IDNet, trained on PixelRec. Then, to show the effectiveness of the dataset's image features, we substitute the itemID embeddings (from IDNet) with a 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#26032;&#38395;&#25925;&#20107;&#38142;&#30340;&#32858;&#31867;&#65292;&#25913;&#36827;&#21644;&#35780;&#20272;&#20102;&#26032;&#38395;&#25512;&#33616;&#20013;&#20449;&#24687;&#30862;&#29255;&#21270;&#30340;&#26816;&#27979;&#12290;&#30740;&#31350;&#32467;&#26524;&#23545;&#20110;&#34913;&#37327;&#20449;&#24687;&#27969;&#30340;&#23436;&#25972;&#24615;&#21644;&#24433;&#21709;&#27665;&#20027;&#21644;&#20844;&#20849;&#35752;&#35770;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2309.06192</link><description>&lt;p&gt;
&#25552;&#39640;&#21644;&#35780;&#20272;&#26032;&#38395;&#25512;&#33616;&#20013;&#30340;&#20449;&#24687;&#30862;&#29255;&#26816;&#27979;&#19982;&#26032;&#38395;&#25925;&#20107;&#38142;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Improving and Evaluating the Detection of Fragmentation in News Recommendations with the Clustering of News Story Chains. (arXiv:2309.06192v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06192
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#26032;&#38395;&#25925;&#20107;&#38142;&#30340;&#32858;&#31867;&#65292;&#25913;&#36827;&#21644;&#35780;&#20272;&#20102;&#26032;&#38395;&#25512;&#33616;&#20013;&#20449;&#24687;&#30862;&#29255;&#21270;&#30340;&#26816;&#27979;&#12290;&#30740;&#31350;&#32467;&#26524;&#23545;&#20110;&#34913;&#37327;&#20449;&#24687;&#27969;&#30340;&#23436;&#25972;&#24615;&#21644;&#24433;&#21709;&#27665;&#20027;&#21644;&#20844;&#20849;&#35752;&#35770;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#38395;&#25512;&#33616;&#31995;&#32479;&#22312;&#22609;&#36896;&#27665;&#20027;&#31038;&#20250;&#20013;&#30340;&#20449;&#24687;&#33719;&#21462;&#26041;&#38754;&#25198;&#28436;&#30528;&#36234;&#26469;&#36234;&#37325;&#35201;&#30340;&#35282;&#33394;&#12290;&#28982;&#32780;&#65292;&#23558;&#25512;&#33616;&#38024;&#23545;&#29992;&#25143;&#30340;&#20855;&#20307;&#20852;&#36259;&#21487;&#33021;&#23548;&#33268;&#20449;&#24687;&#27969;&#30340;&#20998;&#27495;&#12290;&#20449;&#24687;&#25509;&#35302;&#30340;&#30862;&#29255;&#21270;&#23545;&#20844;&#20849;&#39046;&#22495;&#30340;&#23436;&#25972;&#24615;&#26500;&#25104;&#25361;&#25112;&#65292;&#36827;&#32780;&#24433;&#21709;&#27665;&#20027;&#21644;&#20844;&#20849;&#35752;&#35770;&#12290;&#30862;&#29255;&#21270;&#25351;&#26631;&#37327;&#21270;&#20102;&#26032;&#38395;&#25512;&#33616;&#20013;&#20449;&#24687;&#27969;&#30340;&#30862;&#29255;&#21270;&#31243;&#24230;&#12290;&#20934;&#30830;&#34913;&#37327;&#35813;&#25351;&#26631;&#38656;&#35201;&#23558;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#24212;&#29992;&#20110;&#35782;&#21035;&#19981;&#21516;&#30340;&#26032;&#38395;&#20107;&#20214;&#12289;&#25925;&#20107;&#25110;&#26102;&#38388;&#32447;&#12290;&#26412;&#25991;&#23545;&#22312;&#26032;&#38395;&#25512;&#33616;&#20013;&#37327;&#21270;&#20449;&#24687;&#30862;&#29255;&#21270;&#30340;&#21508;&#31181;&#26041;&#27861;&#36827;&#34892;&#20102;&#24191;&#27867;&#35843;&#26597;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#26032;&#38395;&#25925;&#20107;&#32858;&#31867;&#30340;&#24615;&#33021;&#24230;&#37327;&#21644;&#19981;&#21516;&#27169;&#25311;&#30340;&#26032;&#38395;&#25512;&#33616;&#22330;&#26223;&#19979;&#30340;&#30862;&#29255;&#21270;&#35780;&#20998;&#35780;&#20272;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
News recommender systems play an increasingly influential role in shaping information access within democratic societies. However, tailoring recommendations to users' specific interests can result in the divergence of information streams. Fragmented access to information poses challenges to the integrity of the public sphere, thereby influencing democracy and public discourse. The Fragmentation metric quantifies the degree of fragmentation of information streams in news recommendations. Accurate measurement of this metric requires the application of Natural Language Processing (NLP) to identify distinct news events, stories, or timelines. This paper presents an extensive investigation of various approaches for quantifying Fragmentation in news recommendations. These approaches are evaluated both intrinsically, by measuring performance on news story clustering, and extrinsically, by assessing the Fragmentation scores of different simulated news recommender scenarios. Our findings demons
&lt;/p&gt;</description></item><item><title>CPMR&#26159;&#19968;&#20010;&#22522;&#20110;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#22686;&#37327;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#21019;&#24314;&#38745;&#24577;&#23884;&#20837;&#12289;&#21382;&#21490;&#26102;&#38388;&#29366;&#24577;&#21644;&#19978;&#19979;&#25991;&#26102;&#38388;&#29366;&#24577;&#30340;&#19977;&#20010;&#34920;&#31034;&#65292;&#20934;&#30830;&#22320;&#24314;&#27169;&#20102;&#29992;&#25143;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#34920;&#31034;&#21644;&#20852;&#36259;&#21160;&#24577;&#30340;&#28436;&#21270;&#12290;</title><link>http://arxiv.org/abs/2309.04802</link><description>&lt;p&gt;
CPMR: &#22522;&#20110;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#22686;&#37327;&#39034;&#24207;&#25512;&#33616;&#19982;&#20266;&#22810;&#20219;&#21153;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
CPMR: Context-Aware Incremental Sequential Recommendation with Pseudo-Multi-Task Learning. (arXiv:2309.04802v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04802
&lt;/p&gt;
&lt;p&gt;
CPMR&#26159;&#19968;&#20010;&#22522;&#20110;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#22686;&#37327;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#21019;&#24314;&#38745;&#24577;&#23884;&#20837;&#12289;&#21382;&#21490;&#26102;&#38388;&#29366;&#24577;&#21644;&#19978;&#19979;&#25991;&#26102;&#38388;&#29366;&#24577;&#30340;&#19977;&#20010;&#34920;&#31034;&#65292;&#20934;&#30830;&#22320;&#24314;&#27169;&#20102;&#29992;&#25143;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#34920;&#31034;&#21644;&#20852;&#36259;&#21160;&#24577;&#30340;&#28436;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#25143;&#36827;&#34892;&#20114;&#21160;&#30340;&#21160;&#26426;&#21487;&#20197;&#20998;&#20026;&#38745;&#24577;&#20559;&#22909;&#21644;&#21160;&#24577;&#20852;&#36259;&#12290;&#20026;&#20102;&#20934;&#30830;&#22320;&#24314;&#27169;&#29992;&#25143;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#34920;&#31034;&#65292;&#26368;&#36817;&#30340;&#39034;&#24207;&#25512;&#33616;&#30740;&#31350;&#21033;&#29992;&#20449;&#24687;&#20256;&#25773;&#21644;&#28436;&#21270;&#20174;&#25209;&#37327;&#21040;&#36798;&#30340;&#20114;&#21160;&#20013;&#36827;&#34892;&#25366;&#25496;&#12290;&#28982;&#32780;&#65292;&#20182;&#20204;&#24573;&#30053;&#20102;&#22312;&#19978;&#19979;&#25991;&#22330;&#26223;&#20013;&#20154;&#20204;&#24456;&#23481;&#26131;&#21463;&#21040;&#20854;&#20182;&#29992;&#25143;&#30340;&#26368;&#36817;&#34892;&#20026;&#30340;&#24433;&#21709;&#65292;&#24182;&#19988;&#22312;&#25152;&#26377;&#21382;&#21490;&#20114;&#21160;&#20013;&#24212;&#29992;&#28436;&#21270;&#20250;&#31232;&#37322;&#26368;&#36817;&#20114;&#21160;&#30340;&#37325;&#35201;&#24615;&#65292;&#20174;&#32780;&#26080;&#27861;&#20934;&#30830;&#22320;&#24314;&#27169;&#20852;&#36259;&#21160;&#24577;&#30340;&#28436;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#20266;&#22810;&#20219;&#21153;&#25512;&#33616;&#31995;&#32479;&#65288;CPMR&#65289;&#65292;&#36890;&#36807;&#20026;&#27599;&#20010;&#29992;&#25143;&#21644;&#39033;&#30446;&#21019;&#24314;&#19977;&#20010;&#34920;&#31034;&#65288;&#38745;&#24577;&#23884;&#20837;&#12289;&#21382;&#21490;&#26102;&#38388;&#29366;&#24577;&#21644;&#19978;&#19979;&#25991;&#26102;&#38388;&#29366;&#24577;&#65289;&#65292;&#26469;&#24314;&#27169;&#21382;&#21490;&#21644;&#19978;&#19979;&#25991;&#24773;&#22659;&#20013;&#30340;&#28436;&#21270;&#12290;&#20026;&#20102;&#21516;&#26102;&#25552;&#39640;&#26102;&#38388;&#29366;&#24577;&#28436;&#21270;&#21644;&#22686;&#37327;&#25512;&#33616;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The motivations of users to make interactions can be divided into static preference and dynamic interest. To accurately model user representations over time, recent studies in sequential recommendation utilize information propagation and evolution to mine from batches of arriving interactions. However, they ignore the fact that people are easily influenced by the recent actions of other users in the contextual scenario, and applying evolution across all historical interactions dilutes the importance of recent ones, thus failing to model the evolution of dynamic interest accurately. To address this issue, we propose a Context-Aware Pseudo-Multi-Task Recommender System (CPMR) to model the evolution in both historical and contextual scenarios by creating three representations for each user and item under different dynamics: static embedding, historical temporal states, and contextual temporal states. To dually improve the performance of temporal states evolution and incremental recommenda
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#25512;&#33616;&#20013;&#30340;&#34920;&#36798;&#33021;&#21147;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#21457;&#29616;&#29616;&#26377;&#30340;&#34920;&#36798;&#33021;&#21147;&#24230;&#37327;&#26631;&#20934;&#21487;&#33021;&#26080;&#27861;&#26377;&#25928;&#35780;&#20272;&#27169;&#22411;&#22312;&#25512;&#33616;&#20013;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#29702;&#35770;&#20998;&#26512;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.11127</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#25512;&#33616;&#20013;&#30340;&#34920;&#36798;&#33021;&#21147;&#26377;&#22810;&#24378;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Expressive are Graph Neural Networks in Recommendation?. (arXiv:2308.11127v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11127
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#25512;&#33616;&#20013;&#30340;&#34920;&#36798;&#33021;&#21147;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#21457;&#29616;&#29616;&#26377;&#30340;&#34920;&#36798;&#33021;&#21147;&#24230;&#37327;&#26631;&#20934;&#21487;&#33021;&#26080;&#27861;&#26377;&#25928;&#35780;&#20272;&#27169;&#22411;&#22312;&#25512;&#33616;&#20013;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#29702;&#35770;&#20998;&#26512;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#21508;&#31181;&#22270;&#23398;&#20064;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#21033;&#29992;&#22270;&#20013;&#30340;&#29992;&#25143;-&#29289;&#21697;&#21327;&#20316;&#36807;&#28388;&#20449;&#21495;&#36827;&#34892;&#25512;&#33616;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#23427;&#20204;&#22312;&#26368;&#20808;&#36827;&#30340;&#25512;&#33616;&#27169;&#22411;&#20013;&#30340;&#32463;&#39564;&#26377;&#25928;&#24615;&#65292;&#20294;&#23545;&#20110;&#23427;&#20204;&#30340;&#33021;&#21147;&#30340;&#29702;&#35770;&#34920;&#36848;&#38750;&#24120;&#31232;&#23569;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;GNNs&#30340;&#19968;&#33324;&#34920;&#36798;&#33021;&#21147;&#65292;&#35777;&#26126;&#20102;&#28040;&#24687;&#20256;&#36882;GNNs&#33267;&#22810;&#19982;Weisfeiler-Lehman&#27979;&#35797;&#19968;&#26679;&#24378;&#22823;&#65292;&#24182;&#19988;&#19982;&#38543;&#26426;&#33410;&#28857;&#21021;&#22987;&#21270;&#30456;&#32467;&#21512;&#30340;GNNs&#26159;&#36890;&#29992;&#30340;&#12290;&#28982;&#32780;&#65292;GNNs&#30340;&#8220;&#34920;&#36798;&#33021;&#21147;&#8221;&#27010;&#24565;&#20173;&#28982;&#23450;&#20041;&#27169;&#31946;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#24037;&#20316;&#37319;&#29992;&#22270;&#21516;&#26500;&#27979;&#35797;&#20316;&#20026;&#34920;&#36798;&#33021;&#21147;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#20294;&#36825;&#31181;&#22270;&#32423;&#20219;&#21153;&#21487;&#33021;&#19981;&#33021;&#26377;&#25928;&#35780;&#20272;&#27169;&#22411;&#22312;&#25512;&#33616;&#20013;&#21306;&#20998;&#19981;&#21516;&#25509;&#36817;&#31243;&#24230;&#33410;&#28857;&#30340;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;GNNs&#22312;&#25512;&#33616;&#20013;&#30340;&#34920;&#36798;&#33021;&#21147;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#29702;&#35770;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have demonstrated superior performance on various graph learning tasks, including recommendation, where they leverage user-item collaborative filtering signals in graphs. However, theoretical formulations of their capability are scarce, despite their empirical effectiveness in state-of-the-art recommender models. Recently, research has explored the expressiveness of GNNs in general, demonstrating that message passing GNNs are at most as powerful as the Weisfeiler-Lehman test, and that GNNs combined with random node initialization are universal. Nevertheless, the concept of "expressiveness" for GNNs remains vaguely defined. Most existing works adopt the graph isomorphism test as the metric of expressiveness, but this graph-level task may not effectively assess a model's ability in recommendation, where the objective is to distinguish nodes of different closeness. In this paper, we provide a comprehensive theoretical analysis of the expressiveness of GNNs in 
&lt;/p&gt;</description></item><item><title>&#20174;&#20107;&#29992;&#25143;&#34892;&#20026;&#20998;&#26512;&#30340;&#23398;&#26415;&#30028;&#19968;&#30452;&#38754;&#20020;&#30528;&#25910;&#38598;&#36275;&#22815;&#39640;&#36136;&#37327;&#29992;&#25143;&#34892;&#20026;&#25968;&#25454;&#30340;&#38590;&#39064;&#65292;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#26159;&#33258;&#21160;&#27169;&#25311;&#29992;&#25143;&#34892;&#20026;&#65292;&#36817;&#26399;&#30740;&#31350;&#34920;&#26126;&#65292;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21487;&#38752;&#30340;&#29992;&#25143;&#27169;&#25311;&#26377;&#20102;&#37325;&#35201;&#30340;&#31361;&#30772;&#65292;&#23558;&#36825;&#31181;&#27169;&#22411;&#24212;&#29992;&#21040;&#29992;&#25143;&#34892;&#20026;&#20998;&#26512;&#30740;&#31350;&#20013;&#26377;&#30528;&#24040;&#22823;&#28508;&#21147;&#65292;&#21487;&#33021;&#23545;&#20256;&#32479;&#30740;&#31350;&#33539;&#24335;&#20135;&#29983;&#38761;&#21629;&#24615;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.02552</link><description>&lt;p&gt;
&#24403;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#26234;&#33021;&#20307;&#36935;&#21040;&#29992;&#25143;&#34892;&#20026;&#20998;&#26512;&#65306;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#25143;&#27169;&#25311;&#33539;&#24335;
&lt;/p&gt;
&lt;p&gt;
When Large Language Model based Agent Meets User Behavior Analysis: A Novel User Simulation Paradigm. (arXiv:2306.02552v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02552
&lt;/p&gt;
&lt;p&gt;
&#20174;&#20107;&#29992;&#25143;&#34892;&#20026;&#20998;&#26512;&#30340;&#23398;&#26415;&#30028;&#19968;&#30452;&#38754;&#20020;&#30528;&#25910;&#38598;&#36275;&#22815;&#39640;&#36136;&#37327;&#29992;&#25143;&#34892;&#20026;&#25968;&#25454;&#30340;&#38590;&#39064;&#65292;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#26159;&#33258;&#21160;&#27169;&#25311;&#29992;&#25143;&#34892;&#20026;&#65292;&#36817;&#26399;&#30740;&#31350;&#34920;&#26126;&#65292;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21487;&#38752;&#30340;&#29992;&#25143;&#27169;&#25311;&#26377;&#20102;&#37325;&#35201;&#30340;&#31361;&#30772;&#65292;&#23558;&#36825;&#31181;&#27169;&#22411;&#24212;&#29992;&#21040;&#29992;&#25143;&#34892;&#20026;&#20998;&#26512;&#30740;&#31350;&#20013;&#26377;&#30528;&#24040;&#22823;&#28508;&#21147;&#65292;&#21487;&#33021;&#23545;&#20256;&#32479;&#30740;&#31350;&#33539;&#24335;&#20135;&#29983;&#38761;&#21629;&#24615;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#25143;&#34892;&#20026;&#20998;&#26512;&#22312;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;AI&#24212;&#29992;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#25910;&#38598;&#36275;&#22815;&#21644;&#39640;&#36136;&#37327;&#30340;&#29992;&#25143;&#34892;&#20026;&#25968;&#25454;&#19968;&#30452;&#26159;&#19968;&#20010;&#22522;&#26412;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#33258;&#21160;&#27169;&#25311;&#29992;&#25143;&#34892;&#20026;&#26159;&#19968;&#20010;&#30452;&#35266;&#30340;&#24819;&#27861;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20154;&#31867;&#35748;&#30693;&#36807;&#31243;&#30340;&#20027;&#35266;&#21644;&#22797;&#26434;&#24615;&#36136;&#65292;&#21487;&#38752;&#22320;&#27169;&#25311;&#29992;&#25143;&#34892;&#20026;&#26159;&#22256;&#38590;&#30340;&#12290;&#26368;&#36817;&#65292;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#23637;&#31034;&#20102;&#23454;&#29616;&#31867;&#20284;&#20154;&#31867;&#26234;&#33021;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#20123;&#27169;&#22411;&#20026;&#21487;&#38752;&#30340;&#29992;&#25143;&#27169;&#25311;&#25552;&#20379;&#20102;&#37325;&#35201;&#26426;&#20250;&#65292;&#24182;&#26377;&#21487;&#33021;&#25913;&#21464;&#20256;&#32479;&#30340;&#29992;&#25143;&#34892;&#20026;&#20998;&#26512;&#30740;&#31350;&#33539;&#24335;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20197;&#25512;&#33616;&#31995;&#32479;&#20026;&#20363;&#65292;&#25506;&#32034;&#20351;&#29992;LLM&#36827;&#34892;&#29992;&#25143;&#27169;&#25311;&#30340;&#28508;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#27599;&#20010;&#29992;&#25143;&#35270;&#20026;&#22522;&#20110;LLM&#30340;&#33258;&#27835;&#26234;&#33021;&#20307;&#65292;&#24182;&#35753;&#19981;&#21516;&#26234;&#33021;&#20307;&#22312;&#34394;&#25311;&#29615;&#22659;&#20013;&#33258;&#30001;&#20132;&#27969;&#12289;&#34892;&#20026;&#21644;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
User behavior analysis is crucial in human-centered AI applications. In this field, the collection of sufficient and high-quality user behavior data has always been a fundamental yet challenging problem. An intuitive idea to address this problem is automatically simulating the user behaviors. However, due to the subjective and complex nature of human cognitive processes, reliably simulating the user behavior is difficult. Recently, large language models (LLM) have obtained remarkable successes, showing great potential to achieve human-like intelligence. We argue that these models present significant opportunities for reliable user simulation, and have the potential to revolutionize traditional study paradigms in user behavior analysis. In this paper, we take recommender system as an example to explore the potential of using LLM for user simulation. Specifically, we regard each user as an LLM-based autonomous agent, and let different agents freely communicate, behave and evolve in a vir
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21160;&#24577;&#35821;&#26009;&#24211;&#19978;&#30340;&#29983;&#25104;&#26816;&#32034;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#38745;&#24577;&#35774;&#32622;&#19979;&#65292;&#29983;&#25104;&#26816;&#32034;&#25928;&#26524;&#20248;&#20110;&#21452;&#32534;&#30721;&#22120;&#65292;&#20294;&#22312;&#21160;&#24577;&#35774;&#32622;&#19979;&#24773;&#20917;&#30456;&#21453;&#12290;&#36890;&#36807;&#20351;&#29992;&#21442;&#25968;&#39640;&#25928;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;DynamicGR&#22312;&#26032;&#30340;&#35821;&#26009;&#24211;&#19978;&#23637;&#29616;&#20986;&#20102;&#24847;&#22806;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.18952</link><description>&lt;p&gt;
&#22312;&#21160;&#24577;&#35821;&#26009;&#24211;&#19978;&#25345;&#32493;&#26356;&#26032;&#29983;&#25104;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Continually Updating Generative Retrieval on Dynamic Corpora. (arXiv:2305.18952v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18952
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21160;&#24577;&#35821;&#26009;&#24211;&#19978;&#30340;&#29983;&#25104;&#26816;&#32034;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#38745;&#24577;&#35774;&#32622;&#19979;&#65292;&#29983;&#25104;&#26816;&#32034;&#25928;&#26524;&#20248;&#20110;&#21452;&#32534;&#30721;&#22120;&#65292;&#20294;&#22312;&#21160;&#24577;&#35774;&#32622;&#19979;&#24773;&#20917;&#30456;&#21453;&#12290;&#36890;&#36807;&#20351;&#29992;&#21442;&#25968;&#39640;&#25928;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;DynamicGR&#22312;&#26032;&#30340;&#35821;&#26009;&#24211;&#19978;&#23637;&#29616;&#20986;&#20102;&#24847;&#22806;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#20851;&#20110;&#20449;&#24687;&#26816;&#32034;(IR)&#30340;&#22823;&#22810;&#25968;&#30740;&#31350;&#20551;&#35774;&#35821;&#26009;&#24211;&#26159;&#38745;&#24577;&#30340;&#65292;&#32780;&#23454;&#38469;&#19990;&#30028;&#20013;&#30340;&#25991;&#26723;&#26159;&#19981;&#26029;&#26356;&#26032;&#30340;&#12290;&#26412;&#25991;&#23558;&#30693;&#35782;&#30340;&#21160;&#24577;&#24615;&#24341;&#20837;&#26816;&#32034;&#31995;&#32479;&#20013;&#65292;&#23558;&#26816;&#32034;&#35270;&#20026;&#21160;&#24577;&#30340;&#30693;&#35782;&#24211;&#65292;&#26356;&#31526;&#21512;&#30495;&#23454;&#29615;&#22659;&#12290;&#25105;&#20204;&#23545;&#21452;&#32534;&#30721;&#22120;&#21644;&#29983;&#25104;&#26816;&#32034;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#65292;&#21033;&#29992;StreamingQA&#22522;&#20934;&#27979;&#35797;&#29992;&#20110;&#26102;&#24577;&#30693;&#35782;&#26356;&#26032;&#12290;&#25105;&#20204;&#30340;&#21021;&#27493;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#38745;&#24577;&#35774;&#32622;&#19979;&#65292;&#29983;&#25104;&#26816;&#32034;&#20248;&#20110;&#21452;&#32534;&#30721;&#22120;&#65292;&#20294;&#22312;&#21160;&#24577;&#35774;&#32622;&#19979;&#24773;&#20917;&#30456;&#21453;&#12290;&#28982;&#32780;&#65292;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#24403;&#25105;&#20204;&#21033;&#29992;&#21442;&#25968;&#39640;&#25928;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#22686;&#24378;&#29983;&#25104;&#26816;&#32034;&#23545;&#26032;&#35821;&#26009;&#24211;&#30340;&#36866;&#24212;&#24615;&#26102;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;Dynamic Generative Retrieval (DynamicGR)&#23637;&#29616;&#20986;&#24847;&#22806;&#30340;&#21457;&#29616;&#12290;&#23427;&#33021;&#22815;&#22312;&#20854;&#20869;&#37096;&#32034;&#24341;&#20013;&#39640;&#25928;&#21387;&#32553;&#26032;&#30340;&#30693;&#35782;&#65292;
&lt;/p&gt;
&lt;p&gt;
The majority of prior work on information retrieval (IR) assumes that the corpus is static, whereas in the real world, the documents are continually updated. In this paper, we incorporate often overlooked dynamic nature of knowledge into the retrieval systems. Our work treats retrieval not as static archives but as dynamic knowledge bases better aligned with real-world environments. We conduct a comprehensive evaluation of dual encoders and generative retrieval, utilizing the StreamingQA benchmark designed for the temporal knowledge updates. Our initial results show that while generative retrieval outperforms dual encoders in static settings, the opposite is true in dynamic settings. Surprisingly, however, when we utilize a parameter-efficient pre-training method to enhance adaptability of generative retrieval to new corpora, our resulting model, Dynamic Generative Retrieval (DynamicGR), exhibits unexpected findings. It (1) efficiently compresses new knowledge in their internal index, 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#21327;&#20316;&#22495;&#21069;&#32512;&#35843;&#25972;&#30340;&#36328;&#39046;&#22495;&#23454;&#20307;&#35782;&#21035;&#65292;&#20351;&#29992;&#25991;&#26412;&#21040;&#25991;&#26412;&#29983;&#25104;&#30340;&#25903;&#25745;&#39046;&#22495;&#30456;&#20851;&#25351;&#23548;&#26469;&#23558;&#30693;&#35782;&#36716;&#31227;&#33267;&#26032;&#22495;NER&#20219;&#21153;&#65292;&#36991;&#20813;&#20102;&#20808;&#21069;&#30340;&#20026;&#27599;&#20010;&#39046;&#22495;&#32467;&#26463;&#19968;&#20010;&#20840;&#26032;&#30340;NER&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2301.10410</link><description>&lt;p&gt;
&#36866;&#29992;&#20110;&#25152;&#26377;&#39046;&#22495;&#30340;&#19968;&#20010;&#27169;&#22411;&#65306;&#22522;&#20110;&#21327;&#20316;&#22495;&#21069;&#32512;&#35843;&#25972;&#30340;&#36328;&#39046;&#22495;&#23454;&#20307;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
One Model for All Domains: Collaborative Domain-Prefix Tuning for Cross-Domain NER. (arXiv:2301.10410v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10410
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#21327;&#20316;&#22495;&#21069;&#32512;&#35843;&#25972;&#30340;&#36328;&#39046;&#22495;&#23454;&#20307;&#35782;&#21035;&#65292;&#20351;&#29992;&#25991;&#26412;&#21040;&#25991;&#26412;&#29983;&#25104;&#30340;&#25903;&#25745;&#39046;&#22495;&#30456;&#20851;&#25351;&#23548;&#26469;&#23558;&#30693;&#35782;&#36716;&#31227;&#33267;&#26032;&#22495;NER&#20219;&#21153;&#65292;&#36991;&#20813;&#20102;&#20808;&#21069;&#30340;&#20026;&#27599;&#20010;&#39046;&#22495;&#32467;&#26463;&#19968;&#20010;&#20840;&#26032;&#30340;NER&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#20915;&#23454;&#38469;&#22330;&#26223;&#20013;&#20302;&#36164;&#28304;&#38382;&#39064;&#26159;&#36328;&#39046;&#22495;&#23454;&#20307;&#35782;&#21035;&#30340;&#19968;&#20010;&#25361;&#25112;&#24615;&#20219;&#21153;&#12290;&#20808;&#21069;&#20856;&#22411;&#30340;&#35299;&#20915;&#26041;&#26696;&#20027;&#35201;&#36890;&#36807;&#20351;&#29992;&#26469;&#33258;&#20016;&#23500;&#36164;&#28304;&#39046;&#22495;&#30340;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(PLMs)&#33719;&#24471;NER&#27169;&#22411;&#24182;&#23558;&#20854;&#36866;&#24212;&#20110;&#30446;&#26631;&#39046;&#22495;&#12290;&#30001;&#20110;&#19981;&#21516;&#39046;&#22495;&#23454;&#20307;&#31867;&#22411;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#20808;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#35843;&#25972;&#25152;&#26377;PLMs&#30340;&#21442;&#25968;&#65292;&#20174;&#32780;&#20026;&#27599;&#20010;&#39046;&#22495;&#32467;&#26463;&#19968;&#20010;&#20840;&#26032;&#30340;NER&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#24403;&#21069;&#30340;&#27169;&#22411;&#21482;&#20851;&#27880;&#20110;&#21033;&#29992;&#19968;&#20010;&#26222;&#36890;&#26469;&#28304;&#39046;&#22495;&#20013;&#30340;&#30693;&#35782;&#65292;&#32780;&#26410;&#33021;&#25104;&#21151;&#22320;&#23558;&#26469;&#33258;&#22810;&#20010;&#26469;&#28304;&#39046;&#22495;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#30446;&#26631;&#19978;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#22522;&#20110;&#25991;&#26412;&#21040;&#25991;&#26412;&#29983;&#25104;&#30340;PLM&#24341;&#20837;&#20102;&#21327;&#20316;&#22495;&#21069;&#32512;&#35843;&#25972;&#36328;&#39046;&#22495;NER(CP-NER)&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21576;&#29616;&#20102;&#29992;&#20110;&#25991;&#26412;&#21040;&#25991;&#26412;&#29983;&#25104;&#30340;&#25903;&#25745;&#39046;&#22495;&#30456;&#20851;&#25351;&#23548;&#26469;&#23558;&#30693;&#35782;&#36716;&#31227;&#33267;&#26032;&#22495;NER&#20219;&#21153;&#32780;&#26080;&#38656;&#32467;&#26500;&#20462;&#25913;&#12290;&#25105;&#20204;&#21033;&#29992;&#20923;&#32467;&#30340;PLMs&#24182;&#36827;&#34892;&#21327;&#20316;&#22495;&#21069;&#32512;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-domain NER is a challenging task to address the low-resource problem in practical scenarios. Previous typical solutions mainly obtain a NER model by pre-trained language models (PLMs) with data from a rich-resource domain and adapt it to the target domain. Owing to the mismatch issue among entity types in different domains, previous approaches normally tune all parameters of PLMs, ending up with an entirely new NER model for each domain. Moreover, current models only focus on leveraging knowledge in one general source domain while failing to successfully transfer knowledge from multiple sources to the target. To address these issues, we introduce Collaborative Domain-Prefix Tuning for cross-domain NER (CP-NER) based on text-to-text generative PLMs. Specifically, we present text-to-text generation grounding domain-related instructors to transfer knowledge to new domain NER tasks without structural modifications. We utilize frozen PLMs and conduct collaborative domain-prefix tuning
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#36827;&#34892;&#25512;&#29702;&#30340;&#21069;&#27839;&#30740;&#31350;&#32508;&#21512;&#35843;&#26597;&#12290;&#35752;&#35770;&#20102;&#26032;&#20852;&#25512;&#29702;&#33021;&#21147;&#20986;&#29616;&#30340;&#28508;&#22312;&#21407;&#22240;&#65292;&#24182;&#25552;&#20379;&#31995;&#32479;&#36164;&#28304;&#24110;&#21161;&#21021;&#23398;&#32773;&#12290;</title><link>http://arxiv.org/abs/2212.09597</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#36827;&#34892;&#25512;&#29702;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Reasoning with Language Model Prompting: A Survey. (arXiv:2212.09597v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09597
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#36827;&#34892;&#25512;&#29702;&#30340;&#21069;&#27839;&#30740;&#31350;&#32508;&#21512;&#35843;&#26597;&#12290;&#35752;&#35770;&#20102;&#26032;&#20852;&#25512;&#29702;&#33021;&#21147;&#20986;&#29616;&#30340;&#28508;&#22312;&#21407;&#22240;&#65292;&#24182;&#25552;&#20379;&#31995;&#32479;&#36164;&#28304;&#24110;&#21161;&#21021;&#23398;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#29702;&#20316;&#20026;&#22797;&#26434;&#38382;&#39064;&#35299;&#20915;&#30340;&#37325;&#35201;&#33021;&#21147;&#65292;&#21487;&#20197;&#20026;&#21307;&#30103;&#35786;&#26029;&#12289;&#35848;&#21028;&#31561;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#25552;&#20379;&#21518;&#31471;&#25903;&#25345;&#12290;&#26412;&#25991;&#23545;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#36827;&#34892;&#25512;&#29702;&#30340;&#21069;&#27839;&#30740;&#31350;&#36827;&#34892;&#20102;&#32508;&#21512;&#35843;&#26597;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#30740;&#31350;&#25104;&#26524;&#30340;&#27604;&#36739;&#21644;&#24635;&#32467;&#65292;&#24182;&#25552;&#20379;&#20102;&#31995;&#32479;&#36164;&#28304;&#20197;&#24110;&#21161;&#21021;&#23398;&#32773;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#26032;&#20852;&#25512;&#29702;&#33021;&#21147;&#20986;&#29616;&#30340;&#28508;&#22312;&#21407;&#22240;&#65292;&#24182;&#31361;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#36164;&#28304;&#21487;&#22312; https://github.com/zjunlp/Prompt4ReasoningPapers &#19978;&#33719;&#21462;&#65288;&#23450;&#26399;&#26356;&#26032;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reasoning, as an essential ability for complex problem-solving, can provide back-end support for various real-world applications, such as medical diagnosis, negotiation, etc. This paper provides a comprehensive survey of cutting-edge research on reasoning with language model prompting. We introduce research works with comparisons and summaries and provide systematic resources to help beginners. We also discuss the potential reasons for emerging such reasoning abilities and highlight future research directions. Resources are available at https://github.com/zjunlp/Prompt4ReasoningPapers (updated periodically).
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#29983;&#25104;&#24335;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#21253;&#25324;&#26041;&#27861;&#20998;&#31867;&#21644;&#20248;&#21155;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2210.12714</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Generative Knowledge Graph Construction: A Review. (arXiv:2210.12714v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.12714
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#29983;&#25104;&#24335;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#21253;&#25324;&#26041;&#27861;&#20998;&#31867;&#21644;&#20248;&#21155;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#65288;KGC&#65289;&#26159;&#25351;&#21033;&#29992;&#24207;&#21015;&#21040;&#24207;&#21015;&#26694;&#26550;&#26500;&#24314;&#28789;&#27963;&#19988;&#21487;&#36866;&#29992;&#20110;&#24191;&#27867;&#20219;&#21153;&#30340;&#30693;&#35782;&#22270;&#35889;&#12290;&#26412;&#30740;&#31350;&#24635;&#32467;&#20102;&#29983;&#25104;&#24335;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#39046;&#22495;&#20013;&#36817;&#26399;&#30340;&#37325;&#35201;&#36827;&#23637;&#65292;&#23545;&#19981;&#21516;&#30340;&#29983;&#25104;&#30446;&#26631;&#20174;&#29702;&#35770;&#21644;&#23454;&#35777;&#20998;&#26512;&#35282;&#24230;&#20998;&#21035;&#35752;&#35770;&#20102;&#21508;&#31181;&#26041;&#27861;&#30340;&#20248;&#21183;&#21644;&#19981;&#36275;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#26377;&#28508;&#21147;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#26377;&#19977;&#20010;&#26041;&#38754;&#65306;&#65288;1&#65289;&#25105;&#20204;&#25552;&#20379;&#20102;&#29983;&#25104;&#24335;KGC&#26041;&#27861;&#30340;&#35814;&#32454;&#12289;&#23436;&#25972;&#30340;&#20998;&#31867;&#20307;&#31995;&#65307;&#65288;2&#65289;&#25105;&#20204;&#23545;&#29983;&#25104;&#24335;KGC&#26041;&#27861;&#36827;&#34892;&#20102;&#29702;&#35770;&#21644;&#23454;&#35777;&#20998;&#26512;&#65307;&#65288;3&#65289;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#20010;&#26410;&#26469;&#21487;&#20197;&#21457;&#23637;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Knowledge Graph Construction (KGC) refers to those methods that leverage the sequence-to-sequence framework for building knowledge graphs, which is flexible and can be adapted to widespread tasks. In this study, we summarize the recent compelling progress in generative knowledge graph construction. We present the advantages and weaknesses of each paradigm in terms of different generation targets and provide theoretical insight and empirical analysis. Based on the review, we suggest promising research directions for the future. Our contributions are threefold: (1) We present a detailed, complete taxonomy for the generative KGC methods; (2) We provide a theoretical and empirical analysis of the generative KGC methods; (3) We propose several research directions that can be developed in the future.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#26816;&#32034;&#22686;&#24378;&#30340;&#26550;&#26500;&#24863;&#30693;&#21442;&#32771;&#20316;&#20026;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#21487;&#21160;&#24577;&#21033;&#29992;&#20154;&#31867;&#27880;&#37322;&#21644;&#24369;&#30417;&#30563;&#25968;&#25454;&#25152;&#32487;&#25215;&#30340;&#26550;&#26500;&#21644;&#30693;&#35782;&#65292;&#25351;&#23548;&#29983;&#25104;&#20855;&#26377;&#26356;&#22909;&#35821;&#20041;&#36830;&#36143;&#24615;&#21644;&#19968;&#33268;&#24615;&#30340;&#32467;&#26500;&#21270;&#30693;&#35782;&#65292;&#20174;&#32780;&#22312;&#25968;&#25454;&#25928;&#29575;&#21644;&#30693;&#35782;&#36136;&#37327;&#26041;&#38754;&#20855;&#26377;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.10709</link><description>&lt;p&gt;
&#20197;&#26550;&#26500;&#24863;&#30693;&#21442;&#32771;&#20316;&#20026;&#25552;&#31034;&#25552;&#39640;&#20102;&#25968;&#25454;&#26377;&#25928;&#30340;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;
&lt;/p&gt;
&lt;p&gt;
Schema-aware Reference as Prompt Improves Data-Efficient Knowledge Graph Construction. (arXiv:2210.10709v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.10709
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#26816;&#32034;&#22686;&#24378;&#30340;&#26550;&#26500;&#24863;&#30693;&#21442;&#32771;&#20316;&#20026;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#21487;&#21160;&#24577;&#21033;&#29992;&#20154;&#31867;&#27880;&#37322;&#21644;&#24369;&#30417;&#30563;&#25968;&#25454;&#25152;&#32487;&#25215;&#30340;&#26550;&#26500;&#21644;&#30693;&#35782;&#65292;&#25351;&#23548;&#29983;&#25104;&#20855;&#26377;&#26356;&#22909;&#35821;&#20041;&#36830;&#36143;&#24615;&#21644;&#19968;&#33268;&#24615;&#30340;&#32467;&#26500;&#21270;&#30693;&#35782;&#65292;&#20174;&#32780;&#22312;&#25968;&#25454;&#25928;&#29575;&#21644;&#30693;&#35782;&#36136;&#37327;&#26041;&#38754;&#20855;&#26377;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#65292;&#35768;&#22810;&#22522;&#20110;&#25552;&#31034;&#30340;&#26041;&#27861;&#34987;&#25552;&#20986;&#24182;&#22312;&#25968;&#25454;&#26377;&#25928;&#30340;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#26041;&#27861;&#20173;&#23384;&#22312;&#20960;&#20010;&#28508;&#22312;&#30340;&#38480;&#21046;&#65306;&#65288;i&#65289;&#33258;&#28982;&#35821;&#35328;&#21644;&#39044;&#23450;&#20041;&#27169;&#24335;&#30340;&#36755;&#20986;&#32467;&#26500;&#21270;&#30693;&#35782;&#20043;&#38388;&#30340;&#35821;&#20041;&#24046;&#36317;&#65292;&#36825;&#24847;&#21619;&#30528;&#27169;&#22411;&#26080;&#27861;&#20805;&#20998;&#21033;&#29992;&#21463;&#38480;&#27169;&#26495;&#30340;&#35821;&#20041;&#30693;&#35782;&#65307;&#65288;ii&#65289;&#22522;&#20110;&#23616;&#37096;&#20010;&#20307;&#23454;&#20363;&#30340;&#34920;&#31034;&#23398;&#20064;&#38480;&#21046;&#20102;&#24615;&#33021;&#65292;&#32473;&#23450;&#20102;&#19981;&#20805;&#36275;&#30340;&#29305;&#24449;&#65292;&#36825;&#20123;&#29305;&#24449;&#19981;&#33021;&#37322;&#25918;&#39044;&#20808;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#22312;&#31867;&#27604;&#33021;&#21147;&#12290;&#21463;&#36825;&#20123;&#35266;&#23519;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#26816;&#32034;&#24471;&#21040;&#30340;&#26550;&#26500;&#24863;&#30693;&#21442;&#32771;&#20316;&#20026;&#25552;&#31034;&#65292;&#25552;&#39640;&#20102;&#25968;&#25454;&#26377;&#25928;&#30340;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#30340;&#35821;&#20041;&#36830;&#36143;&#24615;&#21644;&#19968;&#33268;&#24615;&#12290;&#22312;&#20004;&#20010;&#26631;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#27604;&#29616;&#26377;&#30340;&#22522;&#20110;&#25552;&#31034;&#21644;&#38750;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#25968;&#25454;&#25928;&#29575;&#21644;&#30693;&#35782;&#36136;&#37327;&#26041;&#38754;&#20855;&#26377;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the development of pre-trained language models, many prompt-based approaches to data-efficient knowledge graph construction have been proposed and achieved impressive performance. However, existing prompt-based learning methods for knowledge graph construction are still susceptible to several potential limitations: (i) semantic gap between natural language and output structured knowledge with pre-defined schema, which means model cannot fully exploit semantic knowledge with the constrained templates; (ii) representation learning with locally individual instances limits the performance given the insufficient features, which are unable to unleash the potential analogical capability of pre-trained language models. Motivated by these observations, we propose a retrieval-augmented approach, which retrieves schema-aware Reference As Prompt (RAP), for data-efficient knowledge graph construction. It can dynamically leverage schema and knowledge inherited from human-annotated and weak-supe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#30340;&#20851;&#31995;&#25277;&#21462;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#31181;&#26041;&#26696;&#26469;&#25552;&#39640;&#24615;&#33021;&#65292;&#21253;&#25324;&#20351;&#29992;&#25552;&#31034;&#26041;&#27861;&#12289;&#24179;&#34913;&#26041;&#27861;&#21644;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#12290;&#36890;&#36807;&#23545;8&#20010;&#20851;&#31995;&#25277;&#21462;&#25968;&#25454;&#38598;&#30340;&#24191;&#27867;&#27604;&#36739;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#34429;&#28982;&#22522;&#20110;&#25552;&#31034;&#30340;&#35843;&#25972;&#26377;&#30410;&#20110;&#20302;&#36164;&#28304;&#20851;&#31995;&#25277;&#21462;&#65292;&#20294;&#20173;&#26377;&#25913;&#36827;&#31354;&#38388;&#65292;&#23588;&#20854;&#26159;&#36328;&#21477;&#23376;&#19978;&#19979;&#25991;&#20013;&#30340;&#22810;&#20010;&#20851;&#31995;&#19977;&#20803;&#32452;&#30340;&#25277;&#21462;&#12290;</title><link>http://arxiv.org/abs/2210.10678</link><description>&lt;p&gt;
&#23454;&#29616;&#30495;&#23454;&#20302;&#36164;&#28304;&#20851;&#31995;&#25277;&#21462;: &#38024;&#23545;&#20855;&#26377;&#23454;&#35777;&#22522;&#20934;&#30740;&#31350;&#30340;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
Towards Realistic Low-resource Relation Extraction: A Benchmark with Empirical Baseline Study. (arXiv:2210.10678v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.10678
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#30340;&#20851;&#31995;&#25277;&#21462;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#31181;&#26041;&#26696;&#26469;&#25552;&#39640;&#24615;&#33021;&#65292;&#21253;&#25324;&#20351;&#29992;&#25552;&#31034;&#26041;&#27861;&#12289;&#24179;&#34913;&#26041;&#27861;&#21644;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#12290;&#36890;&#36807;&#23545;8&#20010;&#20851;&#31995;&#25277;&#21462;&#25968;&#25454;&#38598;&#30340;&#24191;&#27867;&#27604;&#36739;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#34429;&#28982;&#22522;&#20110;&#25552;&#31034;&#30340;&#35843;&#25972;&#26377;&#30410;&#20110;&#20302;&#36164;&#28304;&#20851;&#31995;&#25277;&#21462;&#65292;&#20294;&#20173;&#26377;&#25913;&#36827;&#31354;&#38388;&#65292;&#23588;&#20854;&#26159;&#36328;&#21477;&#23376;&#19978;&#19979;&#25991;&#20013;&#30340;&#22810;&#20010;&#20851;&#31995;&#19977;&#20803;&#32452;&#30340;&#25277;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#39033;&#38024;&#23545;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#26500;&#24314;&#20851;&#31995;&#25277;&#21462;&#31995;&#32479;&#30340;&#23454;&#35777;&#30740;&#31350;&#12290;&#22522;&#20110;&#26368;&#36817;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#25105;&#20204;&#20840;&#38754;&#35843;&#26597;&#20102;&#19977;&#31181;&#26041;&#26696;&#26469;&#35780;&#20272;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#30340;&#24615;&#33021;&#65306;(i) &#20351;&#29992;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#30340;&#19981;&#21516;&#31867;&#22411;&#30340;&#25552;&#31034;&#26041;&#27861;&#65307; (ii) &#22810;&#26679;&#21270;&#30340;&#24179;&#34913;&#26041;&#27861;&#26469;&#35299;&#20915;&#38271;&#23614;&#20998;&#24067;&#38382;&#39064;&#65307; (iii) &#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#21644;&#33258;&#35757;&#32451;&#26469;&#29983;&#25104;&#26356;&#22810;&#39046;&#22495;&#20869;&#26631;&#35760;&#25968;&#25454;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;8&#20010;&#20851;&#31995;&#25277;&#21462;(RE) &#25968;&#25454;&#38598;&#30340;&#22522;&#20934;&#65292;&#28085;&#30422;&#20102;&#19981;&#21516;&#30340;&#35821;&#35328;&#12289;&#39046;&#22495;&#21644;&#19978;&#19979;&#25991;&#65292;&#24182;&#23545;&#25152;&#25552;&#20986;&#30340;&#26041;&#26696;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65306;(i) &#34429;&#28982;&#22522;&#20110;&#25552;&#31034;&#30340;&#35843;&#25972;&#22312;&#20302;&#36164;&#28304;&#20851;&#31995;&#25277;&#21462;&#20013;&#26159;&#26377;&#30410;&#30340;&#65292;&#20294;&#20173;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#28508;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#25552;&#21462;&#36328;&#21477;&#23376;&#19978;&#19979;&#25991;&#20013;&#30340;&#22810;&#20010;&#20851;&#31995;&#19977;&#20803;&#32452;&#26041;&#38754;&#65307; (ii) &#24179;&#34913;&#26041;&#27861;&#24182;&#19981;&#24635;&#26159;&#26377;&#21161;&#20110;&#38271;&#23614;&#20998;&#24067;&#30340;&#20851;&#31995;&#25277;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents an empirical study to build relation extraction systems in low-resource settings. Based upon recent pre-trained language models, we comprehensively investigate three schemes to evaluate the performance in low-resource settings: (i) different types of prompt-based methods with few-shot labeled data; (ii) diverse balancing methods to address the long-tailed distribution issue; (iii) data augmentation technologies and self-training to generate more labeled in-domain data. We create a benchmark with 8 relation extraction (RE) datasets covering different languages, domains and contexts and perform extensive comparisons over the proposed schemes with combinations. Our experiments illustrate: (i) Though prompt-based tuning is beneficial in low-resource RE, there is still much potential for improvement, especially in extracting relations from cross-sentence contexts with multiple relational triples; (ii) Balancing methods are not always helpful for RE with long-tailed distr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;&#37325;&#23545;&#27604;&#32593;&#32476;&#65288;DCN&#65289;&#65292;&#36890;&#36807;&#20004;&#20010;&#32452;&#20214;&#20805;&#20998;&#21033;&#29992;&#20102;&#29992;&#25143;&#21644;&#29289;&#21697;&#20004;&#20010;&#35270;&#35282;&#65292;&#20197;&#29983;&#25104;&#22320;&#38754;&#30495;&#23454;&#30340;&#33258;&#30417;&#30563;&#20449;&#21495;&#65292;&#35299;&#20915;&#20102;&#38543;&#26426;&#23631;&#34109;&#21382;&#21490;&#29289;&#21697;&#24102;&#26469;&#30340;&#24207;&#21015;&#31232;&#30095;&#24615;&#21644;&#19981;&#21487;&#38752;&#20449;&#21495;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#19977;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#32437;&#21521;&#25512;&#33616;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2209.08446</link><description>&lt;p&gt;
&#32437;&#21521;&#25512;&#33616;&#20013;&#29992;&#25143;&#21644;&#29289;&#21697;&#35270;&#35282;&#30340;&#21452;&#37325;&#23545;&#27604;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Dual Contrastive Network for Sequential Recommendation with User and Item-Centric Perspectives. (arXiv:2209.08446v3 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.08446
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;&#37325;&#23545;&#27604;&#32593;&#32476;&#65288;DCN&#65289;&#65292;&#36890;&#36807;&#20004;&#20010;&#32452;&#20214;&#20805;&#20998;&#21033;&#29992;&#20102;&#29992;&#25143;&#21644;&#29289;&#21697;&#20004;&#20010;&#35270;&#35282;&#65292;&#20197;&#29983;&#25104;&#22320;&#38754;&#30495;&#23454;&#30340;&#33258;&#30417;&#30563;&#20449;&#21495;&#65292;&#35299;&#20915;&#20102;&#38543;&#26426;&#23631;&#34109;&#21382;&#21490;&#29289;&#21697;&#24102;&#26469;&#30340;&#24207;&#21015;&#31232;&#30095;&#24615;&#21644;&#19981;&#21487;&#38752;&#20449;&#21495;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#19977;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#32437;&#21521;&#25512;&#33616;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#27969;&#23186;&#20307;&#25968;&#25454;&#30340;&#29190;&#21457;&#65292;&#32437;&#21521;&#25512;&#33616;&#25104;&#20026;&#23454;&#29616;&#26102;&#38388;&#24863;&#30693;&#20010;&#24615;&#21270;&#24314;&#27169;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;&#37325;&#23545;&#27604;&#32593;&#32476;&#65288;DCN&#65289;&#65292;&#36890;&#36807;&#20004;&#20010;&#32452;&#20214;&#8212;&#8212;&#22522;&#20110;&#29992;&#25143;&#30340;&#23545;&#27604;&#23398;&#20064;&#21644;&#22522;&#20110;&#29289;&#21697;&#30340;&#23545;&#27604;&#23398;&#20064;&#65292;&#20805;&#20998;&#21033;&#29992;&#20102;&#29992;&#25143;&#21644;&#29289;&#21697;&#20004;&#20010;&#35270;&#35282;&#65292;&#20197;&#29983;&#25104;&#22320;&#38754;&#30495;&#23454;&#30340;&#33258;&#30417;&#30563;&#20449;&#21495;&#65292;&#35299;&#20915;&#20102;&#38543;&#26426;&#23631;&#34109;&#21382;&#21490;&#29289;&#21697;&#24102;&#26469;&#30340;&#24207;&#21015;&#31232;&#30095;&#24615;&#21644;&#19981;&#21487;&#38752;&#20449;&#21495;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#19977;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#32437;&#21521;&#25512;&#33616;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the outbreak of today's streaming data, the sequential recommendation is a promising solution to achieve time-aware personalized modeling. It aims to infer the next interacted item of a given user based on the history item sequence. Some recent works tend to improve the sequential recommendation via random masking on the history item so as to generate self-supervised signals. But such approaches will indeed result in sparser item sequence and unreliable signals. Besides, the existing sequential recommendation models are only user-centric, i.e., based on the historical items by chronological order to predict the probability of candidate items, which ignores whether the items from a provider can be successfully recommended. Such user-centric recommendation will make it impossible for the provider to expose their new items and result in popular bias.  In this paper, we propose a novel Dual Contrastive Network (DCN) to generate ground-truth self-supervised signals for sequential recom
&lt;/p&gt;</description></item><item><title>DeepKE&#26159;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#30693;&#35782;&#25552;&#21462;&#24037;&#20855;&#21253;&#65292;&#25903;&#25345;&#22797;&#26434;&#30340;&#20302;&#36164;&#28304;&#12289;&#25991;&#26723;&#32423;&#21644;&#22810;&#27169;&#24577;&#22330;&#26223;&#65292;&#21487;&#29992;&#20110;&#33258;&#23450;&#20041;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#26469;&#20174;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#25552;&#21462;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2201.03335</link><description>&lt;p&gt;
DeepKE: &#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#30693;&#35782;&#25552;&#21462;&#24037;&#20855;&#21253;&#29992;&#20110;&#30693;&#35782;&#24211;&#26500;&#24314;
&lt;/p&gt;
&lt;p&gt;
DeepKE: A Deep Learning Based Knowledge Extraction Toolkit for Knowledge Base Population. (arXiv:2201.03335v6 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.03335
&lt;/p&gt;
&lt;p&gt;
DeepKE&#26159;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#30693;&#35782;&#25552;&#21462;&#24037;&#20855;&#21253;&#65292;&#25903;&#25345;&#22797;&#26434;&#30340;&#20302;&#36164;&#28304;&#12289;&#25991;&#26723;&#32423;&#21644;&#22810;&#27169;&#24577;&#22330;&#26223;&#65292;&#21487;&#29992;&#20110;&#33258;&#23450;&#20041;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#26469;&#20174;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#25552;&#21462;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24320;&#25918;&#28304;&#20195;&#30721;&#21644;&#21487;&#25193;&#23637;&#30340;&#30693;&#35782;&#25552;&#21462;&#24037;&#20855;&#21253;DeepKE&#65292;&#25903;&#25345;&#30693;&#35782;&#24211;&#26500;&#24314;&#20013;&#30340;&#22797;&#26434;&#20302;&#36164;&#28304;&#12289;&#25991;&#26723;&#32423;&#21644;&#22810;&#27169;&#24577;&#22330;&#26223;&#12290;DeepKE&#23454;&#29616;&#20102;&#21508;&#31181;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#65292;&#21253;&#25324;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12289;&#20851;&#31995;&#25552;&#21462;&#21644;&#23646;&#24615;&#25552;&#21462;&#12290;&#36890;&#36807;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;DeepKE&#20801;&#35768;&#24320;&#21457;&#20154;&#21592;&#21644;&#30740;&#31350;&#20154;&#21592;&#26681;&#25454;&#33258;&#24049;&#30340;&#38656;&#27714;&#23450;&#21046;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#65292;&#20174;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#25552;&#21462;&#20449;&#24687;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;DeepKE&#19981;&#20165;&#20026;&#19981;&#21516;&#20219;&#21153;&#21644;&#22330;&#26223;&#25552;&#20379;&#21508;&#31181;&#21151;&#33021;&#27169;&#22359;&#21644;&#27169;&#22411;&#23454;&#29616;&#65292;&#36824;&#36890;&#36807;&#19968;&#33268;&#30340;&#26694;&#26550;&#32452;&#32455;&#25152;&#26377;&#32452;&#20214;&#65292;&#20197;&#20445;&#25345;&#36275;&#22815;&#30340;&#27169;&#22359;&#21270;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;&#25105;&#20204;&#22312;https://github.com/zjunlp/DeepKE&#21457;&#24067;&#20102;&#28304;&#20195;&#30721;&#65292;&#24182;&#25552;&#20379;&#20102;&#36866;&#29992;&#20110;&#21021;&#23398;&#32773;&#30340;Google Colab&#25945;&#31243;&#21644;&#20840;&#38754;&#30340;&#25991;&#26723;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#22312;http URL&#19978;&#25552;&#20379;&#20102;&#19968;&#20010;&#22312;&#32447;&#31995;&#32479;&#65292;&#29992;&#20110;&#23454;&#26102;&#25552;&#21462;&#21508;&#31181;&#20219;&#21153;&#65292;&#24182;&#25552;&#20379;&#20102;&#28436;&#31034;&#35270;&#39057;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present an open-source and extensible knowledge extraction toolkit DeepKE, supporting complicated low-resource, document-level and multimodal scenarios in the knowledge base population. DeepKE implements various information extraction tasks, including named entity recognition, relation extraction and attribute extraction. With a unified framework, DeepKE allows developers and researchers to customize datasets and models to extract information from unstructured data according to their requirements. Specifically, DeepKE not only provides various functional modules and model implementation for different tasks and scenarios but also organizes all components by consistent frameworks to maintain sufficient modularity and extensibility. We release the source code at GitHub in https://github.com/zjunlp/DeepKE with Google Colab tutorials and comprehensive documents for beginners. Besides, we present an online system in this http URL for real-time extraction of various tasks, and a demo video
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;KnowPrompt&#30340;&#30693;&#35782;&#24863;&#30693;&#25552;&#31034;&#35843;&#25972;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20851;&#31995;&#26631;&#31614;&#20013;&#30340;&#28508;&#22312;&#30693;&#35782;&#34701;&#20837;&#21040;&#25552;&#31034;&#26500;&#24314;&#20013;&#65292;&#24182;&#36890;&#36807;&#21327;&#21516;&#20248;&#21270;&#30340;&#26041;&#24335;&#65292;&#25552;&#39640;&#20102;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2104.07650</link><description>&lt;p&gt;
KnowPrompt&#65306;&#20855;&#26377;&#21327;&#21516;&#20248;&#21270;&#30340;&#30693;&#35782;&#24863;&#30693;&#25552;&#31034;&#35843;&#25972;&#22312;&#20851;&#31995;&#25277;&#21462;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
KnowPrompt: Knowledge-aware Prompt-tuning with Synergistic Optimization for Relation Extraction. (arXiv:2104.07650v7 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2104.07650
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;KnowPrompt&#30340;&#30693;&#35782;&#24863;&#30693;&#25552;&#31034;&#35843;&#25972;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20851;&#31995;&#26631;&#31614;&#20013;&#30340;&#28508;&#22312;&#30693;&#35782;&#34701;&#20837;&#21040;&#25552;&#31034;&#26500;&#24314;&#20013;&#65292;&#24182;&#36890;&#36807;&#21327;&#21516;&#20248;&#21270;&#30340;&#26041;&#24335;&#65292;&#25552;&#39640;&#20102;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23545;&#20110;&#29305;&#23450;&#30340;&#23569;&#26679;&#26412;&#20998;&#31867;&#20219;&#21153;&#65292;&#20351;&#29992;&#25552;&#31034;&#35843;&#25972;&#26041;&#27861;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#25552;&#31034;&#35843;&#25972;&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#23558;&#25991;&#26412;&#29255;&#27573;&#65288;&#21363;&#27169;&#26495;&#65289;&#25554;&#20837;&#36755;&#20837;&#65292;&#24182;&#23558;&#20998;&#31867;&#20219;&#21153;&#36716;&#21270;&#20026;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20851;&#31995;&#25277;&#21462;&#65292;&#30830;&#23450;&#19968;&#20010;&#21512;&#36866;&#30340;&#25552;&#31034;&#27169;&#26495;&#38656;&#35201;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#65292;&#33719;&#21462;&#21512;&#36866;&#30340;&#26631;&#31614;&#35789;&#26159;&#32321;&#29712;&#19988;&#32791;&#26102;&#30340;&#12290;&#27492;&#22806;&#65292;&#20851;&#31995;&#26631;&#31614;&#20043;&#38388;&#23384;&#22312;&#20016;&#23500;&#30340;&#35821;&#20041;&#21644;&#20808;&#39564;&#30693;&#35782;&#65292;&#19981;&#23481;&#24573;&#35270;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#30528;&#30524;&#20110;&#23558;&#20851;&#31995;&#26631;&#31614;&#20043;&#38388;&#30340;&#30693;&#35782;&#34701;&#20837;&#21040;&#20851;&#31995;&#25277;&#21462;&#30340;&#25552;&#31034;&#35843;&#25972;&#20013;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#21327;&#21516;&#20248;&#21270;&#30340;&#30693;&#35782;&#24863;&#30693;&#25552;&#31034;&#35843;&#25972;&#26041;&#27861;&#65288;KnowPrompt&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21033;&#29992;&#21487;&#23398;&#20064;&#30340;&#34394;&#25311;&#31867;&#22411;&#35789;&#21644;&#31572;&#26696;&#35789;&#23558;&#20851;&#31995;&#26631;&#31614;&#20013;&#30340;&#28508;&#22312;&#30693;&#35782;&#34701;&#20837;&#21040;&#25552;&#31034;&#26500;&#24314;&#20013;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#32467;&#26500;&#21270;&#32422;&#26463;&#21327;&#21516;&#20248;&#21270;&#23427;&#20204;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, prompt-tuning has achieved promising results for specific few-shot classification tasks. The core idea of prompt-tuning is to insert text pieces (i.e., templates) into the input and transform a classification task into a masked language modeling problem. However, for relation extraction, determining an appropriate prompt template requires domain expertise, and it is cumbersome and time-consuming to obtain a suitable label word. Furthermore, there exists abundant semantic and prior knowledge among the relation labels that cannot be ignored. To this end, we focus on incorporating knowledge among relation labels into prompt-tuning for relation extraction and propose a Knowledge-aware Prompt-tuning approach with synergistic optimization (KnowPrompt). Specifically, we inject latent knowledge contained in relation labels into prompt construction with learnable virtual type words and answer words. Then, we synergistically optimize their representation with structured constraints. Ex
&lt;/p&gt;</description></item></channel></rss>