<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;GOLF&#26694;&#26550;&#65292;&#36890;&#36807;&#30446;&#26631;&#23548;&#21521;&#21644;&#38271;&#26399;&#35268;&#21010;&#22686;&#24378;LLMs&#30340;&#33021;&#21147;&#65292;&#20197;&#21327;&#21161;&#29992;&#25143;&#22788;&#29702;&#37325;&#35201;&#30340;&#29983;&#27963;&#20915;&#31574;&#12290;</title><link>https://arxiv.org/abs/2403.17089</link><description>&lt;p&gt;
GOLF&#65306;&#30446;&#26631;&#23548;&#21521;&#30340;&#38271;&#26399;&#29983;&#27963;&#20219;&#21153;&#65292;&#30001;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#25903;&#25345;
&lt;/p&gt;
&lt;p&gt;
GOLF: Goal-Oriented Long-term liFe tasks supported by human-AI collaboration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17089
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;GOLF&#26694;&#26550;&#65292;&#36890;&#36807;&#30446;&#26631;&#23548;&#21521;&#21644;&#38271;&#26399;&#35268;&#21010;&#22686;&#24378;LLMs&#30340;&#33021;&#21147;&#65292;&#20197;&#21327;&#21161;&#29992;&#25143;&#22788;&#29702;&#37325;&#35201;&#30340;&#29983;&#27963;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#24443;&#24213;&#25913;&#21464;&#20102;&#20154;&#24037;&#26234;&#33021;&#20132;&#20114;&#21644;&#20449;&#24687;&#33719;&#21462;&#36807;&#31243;&#12290;&#21033;&#29992;LLMs&#20316;&#20026;&#25628;&#32034;&#24341;&#25806;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#29992;&#25143;&#29616;&#22312;&#21487;&#20197;&#35775;&#38382;&#26681;&#25454;&#20854;&#26597;&#35810;&#23450;&#21046;&#30340;&#25688;&#35201;&#20449;&#24687;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#22312;&#23548;&#33322;&#22823;&#37327;&#20449;&#24687;&#36164;&#28304;&#26102;&#25152;&#24102;&#26469;&#30340;&#35748;&#30693;&#36127;&#33655;&#12290;&#36825;&#31181;&#36716;&#21464;&#20984;&#26174;&#20102;LLMs&#22312;&#37325;&#26032;&#23450;&#20041;&#20449;&#24687;&#33719;&#21462;&#33539;&#24335;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#22522;&#20110;&#20219;&#21153;&#28966;&#28857;&#20449;&#24687;&#26816;&#32034;&#21644;LLMs&#30340;&#20219;&#21153;&#35268;&#21010;&#33021;&#21147;&#65292;&#26412;&#30740;&#31350;&#23558;LLMs&#30340;&#33021;&#21147;&#33539;&#22260;&#25193;&#23637;&#21040;&#25903;&#25345;&#29992;&#25143;&#23548;&#33322;&#38271;&#26399;&#21644;&#37325;&#35201;&#30340;&#29983;&#27963;&#20219;&#21153;&#12290;&#23427;&#24341;&#20837;&#20102;GOLF&#26694;&#26550;&#65288;&#30446;&#26631;&#23548;&#21521;&#30340;&#38271;&#26399;&#29983;&#27963;&#20219;&#21153;&#65289;&#65292;&#20391;&#37325;&#20110;&#22686;&#24378;LLMs&#36890;&#36807;&#30446;&#26631;&#23450;&#21521;&#21644;&#38271;&#26399;&#35268;&#21010;&#26469;&#21327;&#21161;&#29992;&#25143;&#20570;&#20986;&#37325;&#35201;&#30340;&#29983;&#27963;&#20915;&#31574;&#12290;&#35813;&#26041;&#27861;&#35770;&#21253;&#21547;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#31867;&#27604;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17089v1 Announce Type: cross  Abstract: The advent of ChatGPT and similar large language models (LLMs) has revolutionized the human-AI interaction and information-seeking process. Leveraging LLMs as an alternative to search engines, users can now access summarized information tailored to their queries, significantly reducing the cognitive load associated with navigating vast information resources. This shift underscores the potential of LLMs in redefining information access paradigms. Drawing on the foundation of task-focused information retrieval and LLMs' task planning ability, this research extends the scope of LLM capabilities beyond routine task automation to support users in navigating long-term and significant life tasks. It introduces the GOLF framework (Goal-Oriented Long-term liFe tasks), which focuses on enhancing LLMs' ability to assist in significant life decisions through goal orientation and long-term planning. The methodology encompasses a comprehensive simul
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#20869;&#23481;&#30340;&#21307;&#23398;&#22270;&#20687;&#26816;&#32034;&#20013;&#65292;&#21033;&#29992;&#22522;&#30784;&#27169;&#22411;&#20316;&#20026;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#26080;&#38656;&#24494;&#35843;&#21363;&#21487;&#21462;&#24471;&#19982;&#19987;&#38376;&#27169;&#22411;&#31454;&#20105;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#22312;&#26816;&#32034;&#30149;&#29702;&#29305;&#24449;&#26041;&#38754;&#20855;&#26377;&#36739;&#22823;&#22256;&#38590;&#12290;</title><link>https://arxiv.org/abs/2403.06567</link><description>&lt;p&gt;
&#21033;&#29992;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#25918;&#23556;&#23398;&#20013;&#22522;&#20110;&#20869;&#23481;&#30340;&#21307;&#23398;&#22270;&#20687;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Leveraging Foundation Models for Content-Based Medical Image Retrieval in Radiology
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06567
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20869;&#23481;&#30340;&#21307;&#23398;&#22270;&#20687;&#26816;&#32034;&#20013;&#65292;&#21033;&#29992;&#22522;&#30784;&#27169;&#22411;&#20316;&#20026;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#26080;&#38656;&#24494;&#35843;&#21363;&#21487;&#21462;&#24471;&#19982;&#19987;&#38376;&#27169;&#22411;&#31454;&#20105;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#22312;&#26816;&#32034;&#30149;&#29702;&#29305;&#24449;&#26041;&#38754;&#20855;&#26377;&#36739;&#22823;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Content-based image retrieval&#65288;CBIR&#65289;&#26377;&#26395;&#26174;&#33879;&#25913;&#21892;&#25918;&#23556;&#23398;&#20013;&#30340;&#35786;&#26029;&#36741;&#21161;&#21644;&#21307;&#23398;&#30740;&#31350;&#12290;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#20316;&#20026;&#24378;&#22823;&#19988;&#22810;&#21151;&#33021;&#30340;&#29616;&#25104;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#29992;&#20110;&#22522;&#20110;&#20869;&#23481;&#30340;&#21307;&#23398;&#22270;&#20687;&#26816;&#32034;&#12290;&#36890;&#36807;&#22312;&#28085;&#30422;&#22235;&#31181;&#27169;&#24577;&#21644;161&#31181;&#30149;&#29702;&#23398;&#30340;160&#19975;&#24352;2D&#25918;&#23556;&#22270;&#20687;&#30340;&#20840;&#38754;&#25968;&#25454;&#38598;&#19978;&#23545;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#25105;&#20204;&#21457;&#29616;&#24369;&#30417;&#30563;&#27169;&#22411;&#34920;&#29616;&#20248;&#24322;&#65292;P@1&#21487;&#36798;0.594&#12290;&#36825;&#31181;&#24615;&#33021;&#19981;&#20165;&#19982;&#19987;&#38376;&#21270;&#27169;&#22411;&#31454;&#20105;&#65292;&#32780;&#19988;&#26080;&#38656;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;&#26816;&#32034;&#30149;&#29702;&#23398;&#19982;&#35299;&#21078;&#32467;&#26500;&#30340;&#25361;&#25112;&#65292;&#34920;&#26126;&#20934;&#30830;&#26816;&#32034;&#30149;&#29702;&#29305;&#24449;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06567v1 Announce Type: cross  Abstract: Content-based image retrieval (CBIR) has the potential to significantly improve diagnostic aid and medical research in radiology. Current CBIR systems face limitations due to their specialization to certain pathologies, limiting their utility. In response, we propose using vision foundation models as powerful and versatile off-the-shelf feature extractors for content-based medical image retrieval. By benchmarking these models on a comprehensive dataset of 1.6 million 2D radiological images spanning four modalities and 161 pathologies, we identify weakly-supervised models as superior, achieving a P@1 of up to 0.594. This performance not only competes with a specialized model but does so without the need for fine-tuning. Our analysis further explores the challenges in retrieving pathological versus anatomical structures, indicating that accurate retrieval of pathological features presents greater difficulty. Despite these challenges, our
&lt;/p&gt;</description></item><item><title>JMLR&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#21307;&#23398;&#39046;&#22495;&#25552;&#39640;&#38382;&#39064;&#22238;&#31572;&#31995;&#32479;&#24615;&#33021;&#65292;&#38477;&#20302;&#35745;&#31639;&#36164;&#28304;&#38656;&#27714;&#65292;&#22686;&#24378;&#27169;&#22411;&#21033;&#29992;&#21307;&#30103;&#30693;&#35782;&#36827;&#34892;&#25512;&#29702;&#21644;&#22238;&#31572;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.17887</link><description>&lt;p&gt;
JMLR&#65306;&#32852;&#21512;&#21307;&#30103;LLM&#21644;&#26816;&#32034;&#35757;&#32451;&#20197;&#22686;&#24378;&#25512;&#29702;&#21644;&#19987;&#19994;&#38382;&#39064;&#22238;&#31572;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
JMLR: Joint Medical LLM and Retrieval Training for Enhancing Reasoning and Professional Question Answering Capability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17887
&lt;/p&gt;
&lt;p&gt;
JMLR&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#21307;&#23398;&#39046;&#22495;&#25552;&#39640;&#38382;&#39064;&#22238;&#31572;&#31995;&#32479;&#24615;&#33021;&#65292;&#38477;&#20302;&#35745;&#31639;&#36164;&#28304;&#38656;&#27714;&#65292;&#22686;&#24378;&#27169;&#22411;&#21033;&#29992;&#21307;&#30103;&#30693;&#35782;&#36827;&#34892;&#25512;&#29702;&#21644;&#22238;&#31572;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#21307;&#30103;&#25968;&#25454;&#30340;&#29190;&#28856;&#24615;&#22686;&#38271;&#21644;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#31934;&#20934;&#21307;&#23398;&#24050;&#32463;&#25104;&#20026;&#22686;&#24378;&#21307;&#30103;&#26381;&#21153;&#36136;&#37327;&#21644;&#25928;&#29575;&#30340;&#20851;&#38190;&#12290;&#22312;&#36825;&#31181;&#32972;&#26223;&#19979;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21307;&#30103;&#30693;&#35782;&#33719;&#21462;&#21644;&#38382;&#39064;&#22238;&#31572;&#31995;&#32479;&#20013;&#21457;&#25381;&#36234;&#26469;&#36234;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#36825;&#20123;&#31995;&#32479;&#22312;&#21307;&#23398;&#39046;&#22495;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#22312;&#24494;&#35843;&#38454;&#27573;&#21516;&#26102;&#35757;&#32451;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#31995;&#32479;&#21644;LLM&#12290;&#25105;&#20204;&#31216;&#20043;&#20026;&#32852;&#21512;&#21307;&#30103;LLM&#21644;&#26816;&#32034;&#35757;&#32451;&#65288;JMLR&#65289;&#30340;&#26041;&#27861;&#26088;&#22312;&#20811;&#26381;&#20256;&#32479;&#27169;&#22411;&#22312;&#22788;&#29702;&#21307;&#23398;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#26102;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#37319;&#29992;&#21516;&#27493;&#35757;&#32451;&#26426;&#21046;&#65292;JMLR&#20943;&#23569;&#20102;&#23545;&#35745;&#31639;&#36164;&#28304;&#30340;&#38656;&#27714;&#65292;&#24182;&#22686;&#24378;&#20102;&#27169;&#22411;&#21033;&#29992;&#21307;&#30103;&#30693;&#35782;&#36827;&#34892;&#25512;&#29702;&#21644;&#22238;&#31572;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17887v1 Announce Type: new  Abstract: With the explosive growth of medical data and the rapid development of artificial intelligence technology, precision medicine has emerged as a key to enhancing the quality and efficiency of healthcare services. In this context, Large Language Models (LLMs) play an increasingly vital role in medical knowledge acquisition and question-answering systems. To further improve the performance of these systems in the medical domain, we introduce an innovative method that jointly trains an Information Retrieval (IR) system and an LLM during the fine-tuning phase. This approach, which we call Joint Medical LLM and Retrieval Training (JMLR), is designed to overcome the challenges faced by traditional models in handling medical question-answering tasks. By employing a synchronized training mechanism, JMLR reduces the demand for computational resources and enhances the model's ability to leverage medical knowledge for reasoning and answering question
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;GenSERP&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21160;&#24577;&#25972;&#29702;&#25628;&#32034;&#32467;&#26524;&#24182;&#26681;&#25454;&#29992;&#25143;&#26597;&#35810;&#29983;&#25104;&#36830;&#36143;&#30340;&#25628;&#32034;&#24341;&#25806;&#32467;&#26524;&#39029;&#38754;&#12290;</title><link>https://arxiv.org/abs/2402.14301</link><description>&lt;p&gt;
GenSERP: &#29992;&#20110;&#25972;&#20010;&#39029;&#38754;&#21576;&#29616;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
GenSERP: Large Language Models for Whole Page Presentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14301
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;GenSERP&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21160;&#24577;&#25972;&#29702;&#25628;&#32034;&#32467;&#26524;&#24182;&#26681;&#25454;&#29992;&#25143;&#26597;&#35810;&#29983;&#25104;&#36830;&#36143;&#30340;&#25628;&#32034;&#24341;&#25806;&#32467;&#26524;&#39029;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#20026;&#26368;&#23567;&#21270;&#25628;&#32034;&#24341;&#25806;&#32467;&#26524;&#39029;&#38754;&#65288;SERP&#65289;&#30340;&#32452;&#32455;&#24037;&#20316;&#24102;&#26469;&#20102;&#26426;&#20250;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;GenSERP&#65292;&#36825;&#26159;&#19968;&#20010;&#21033;&#29992;LLMs&#21644;&#35270;&#35273;&#22312;&#23569;&#26679;&#26412;&#35774;&#32622;&#20013;&#21160;&#24577;&#32452;&#32455;&#20013;&#38388;&#25628;&#32034;&#32467;&#26524;&#30340;&#26694;&#26550;&#65292;&#21253;&#25324;&#29983;&#25104;&#30340;&#32842;&#22825;&#31572;&#26696;&#12289;&#32593;&#31449;&#25688;&#35201;&#12289;&#22810;&#23186;&#20307;&#25968;&#25454;&#12289;&#30693;&#35782;&#38754;&#26495;&#31561;&#65292;&#24182;&#26681;&#25454;&#29992;&#25143;&#30340;&#26597;&#35810;&#20197;&#36830;&#36143;&#30340;SERP&#24067;&#23616;&#21576;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14301v1 Announce Type: cross  Abstract: The advent of large language models (LLMs) brings an opportunity to minimize the effort in search engine result page (SERP) organization. In this paper, we propose GenSERP, a framework that leverages LLMs with vision in a few-shot setting to dynamically organize intermediate search results, including generated chat answers, website snippets, multimedia data, knowledge panels into a coherent SERP layout based on a user's query. Our approach has three main stages: (1) An information gathering phase where the LLM continuously orchestrates API tools to retrieve different types of items, and proposes candidate layouts based on the retrieved items, until it's confident enough to generate the final result. (2) An answer generation phase where the LLM populates the layouts with the retrieved content. In this phase, the LLM adaptively optimize the ranking of items and UX configurations of the SERP. Consequently, it assigns a location on the pag
&lt;/p&gt;</description></item><item><title>DPR&#24494;&#35843;&#39044;&#35757;&#32451;&#32593;&#32476;&#20197;&#22686;&#24378;&#26597;&#35810;&#21644;&#30456;&#20851;&#25991;&#26412;&#25968;&#25454;&#20043;&#38388;&#30340;&#23884;&#20837;&#23545;&#40784;&#65292;&#21457;&#29616;&#35757;&#32451;&#20013;&#30693;&#35782;&#21435;&#20013;&#24515;&#21270;&#65292;&#20294;&#20063;&#25581;&#31034;&#20102;&#27169;&#22411;&#20869;&#37096;&#30693;&#35782;&#30340;&#23616;&#38480;&#24615;</title><link>https://arxiv.org/abs/2402.11035</link><description>&lt;p&gt;
&#23494;&#38598;&#36890;&#36947;&#26816;&#32034;&#65306;&#23494;&#38598;&#36890;&#36947;&#26816;&#32034;&#26159;&#21542;&#22312;&#26816;&#32034;&#20013;&#65311;
&lt;/p&gt;
&lt;p&gt;
Retrieval-Augmented Generation: Is Dense Passage Retrieval Retrieving?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11035
&lt;/p&gt;
&lt;p&gt;
DPR&#24494;&#35843;&#39044;&#35757;&#32451;&#32593;&#32476;&#20197;&#22686;&#24378;&#26597;&#35810;&#21644;&#30456;&#20851;&#25991;&#26412;&#25968;&#25454;&#20043;&#38388;&#30340;&#23884;&#20837;&#23545;&#40784;&#65292;&#21457;&#29616;&#35757;&#32451;&#20013;&#30693;&#35782;&#21435;&#20013;&#24515;&#21270;&#65292;&#20294;&#20063;&#25581;&#31034;&#20102;&#27169;&#22411;&#20869;&#37096;&#30693;&#35782;&#30340;&#23616;&#38480;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23494;&#38598;&#36890;&#36947;&#26816;&#32034;&#65288;DPR&#65289;&#26159;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24615;&#33021;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#33539;&#24335;&#20013;&#30340;&#31532;&#19968;&#27493;&#12290; DPR&#24494;&#35843;&#39044;&#35757;&#32451;&#32593;&#32476;&#65292;&#20197;&#22686;&#24378;&#26597;&#35810;&#21644;&#30456;&#20851;&#25991;&#26412;&#25968;&#25454;&#20043;&#38388;&#30340;&#23884;&#20837;&#23545;&#40784;&#12290;&#23545;DPR&#24494;&#35843;&#30340;&#28145;&#20837;&#29702;&#35299;&#23558;&#38656;&#35201;&#20174;&#26681;&#26412;&#19978;&#37322;&#25918;&#35813;&#26041;&#27861;&#30340;&#20840;&#37096;&#28508;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#25506;&#38024;&#12289;&#23618;&#28608;&#27963;&#20998;&#26512;&#21644;&#27169;&#22411;&#32534;&#36753;&#30340;&#32452;&#21512;&#65292;&#26426;&#26800;&#22320;&#25506;&#32034;&#20102;DPR&#35757;&#32451;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;DPR&#35757;&#32451;&#20351;&#32593;&#32476;&#20013;&#23384;&#20648;&#30693;&#35782;&#30340;&#26041;&#24335;&#21435;&#20013;&#24515;&#21270;&#65292;&#21019;&#24314;&#20102;&#35775;&#38382;&#30456;&#21516;&#20449;&#24687;&#30340;&#22810;&#20010;&#36335;&#24452;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#20102;&#36825;&#31181;&#35757;&#32451;&#39118;&#26684;&#30340;&#23616;&#38480;&#24615;&#65306;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20869;&#37096;&#30693;&#35782;&#38480;&#21046;&#20102;&#26816;&#32034;&#27169;&#22411;&#21487;&#20197;&#26816;&#32034;&#30340;&#20869;&#23481;&#12290;&#36825;&#20123;&#21457;&#29616;&#20026;&#23494;&#38598;&#26816;&#32034;&#25552;&#20986;&#20102;&#19968;&#20123;&#21487;&#33021;&#30340;&#26041;&#21521;&#65306;&#65288;1&#65289;&#26292;&#38706;DPR&#35757;&#32451;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11035v1 Announce Type: new  Abstract: Dense passage retrieval (DPR) is the first step in the retrieval augmented generation (RAG) paradigm for improving the performance of large language models (LLM). DPR fine-tunes pre-trained networks to enhance the alignment of the embeddings between queries and relevant textual data. A deeper understanding of DPR fine-tuning will be required to fundamentally unlock the full potential of this approach. In this work, we explore DPR-trained models mechanistically by using a combination of probing, layer activation analysis, and model editing. Our experiments show that DPR training decentralizes how knowledge is stored in the network, creating multiple access pathways to the same information. We also uncover a limitation in this training style: the internal knowledge of the pre-trained model bounds what the retrieval model can retrieve. These findings suggest a few possible directions for dense retrieval: (1) expose the DPR training process 
&lt;/p&gt;</description></item><item><title>UMAIR-FPS&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29992;&#25143;&#24863;&#30693;&#22810;&#27169;&#24577;&#21160;&#30011;&#25554;&#30011;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#34701;&#21512;&#22270;&#20687;&#32472;&#30011;&#39118;&#26684;&#29305;&#24449;&#21644;&#35821;&#20041;&#29305;&#24449;&#26469;&#22686;&#24378;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2402.10381</link><description>&lt;p&gt;
UMAIR-FPS&#65306;&#24102;&#32472;&#30011;&#39118;&#26684;&#30340;&#29992;&#25143;&#24863;&#30693;&#22810;&#27169;&#24577;&#21160;&#30011;&#25554;&#30011;&#25512;&#33616;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
UMAIR-FPS: User-aware Multi-modal Animation Illustration Recommendation Fusion with Painting Style
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10381
&lt;/p&gt;
&lt;p&gt;
UMAIR-FPS&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29992;&#25143;&#24863;&#30693;&#22810;&#27169;&#24577;&#21160;&#30011;&#25554;&#30011;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#34701;&#21512;&#22270;&#20687;&#32472;&#30011;&#39118;&#26684;&#29305;&#24449;&#21644;&#35821;&#20041;&#29305;&#24449;&#26469;&#22686;&#24378;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#36136;&#37327;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#24555;&#36895;&#36827;&#27493;&#20135;&#29983;&#20102;&#22823;&#37327;&#30340;&#21160;&#28459;&#25554;&#30011;&#12290;&#22312;&#28023;&#37327;&#25968;&#25454;&#20013;&#21521;&#29992;&#25143;&#25512;&#33616;&#25554;&#30011;&#24050;&#25104;&#20026;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#21463;&#27426;&#36814;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#21160;&#28459;&#25512;&#33616;&#31995;&#32479;&#20391;&#37325;&#20110;&#25991;&#26412;&#29305;&#24449;&#65292;&#20294;&#20173;&#38656;&#35201;&#25972;&#21512;&#22270;&#20687;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#22810;&#27169;&#24577;&#25512;&#33616;&#30740;&#31350;&#21463;&#21040;&#32039;&#23494;&#32806;&#21512;&#25968;&#25454;&#38598;&#30340;&#38480;&#21046;&#65292;&#38480;&#21046;&#20102;&#20854;&#23545;&#21160;&#28459;&#25554;&#30011;&#30340;&#36866;&#29992;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#24102;&#32472;&#30011;&#39118;&#26684;&#30340;&#29992;&#25143;&#24863;&#30693;&#22810;&#27169;&#24577;&#21160;&#30011;&#25554;&#30011;&#25512;&#33616;&#34701;&#21512;&#65288;UMAIR-FPS&#65289;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#22312;&#29305;&#24449;&#25552;&#21462;&#38454;&#27573;&#65292;&#23545;&#20110;&#22270;&#20687;&#29305;&#24449;&#65292;&#25105;&#20204;&#39318;&#27425;&#32467;&#21512;&#22270;&#20687;&#32472;&#30011;&#39118;&#26684;&#29305;&#24449;&#19982;&#35821;&#20041;&#29305;&#24449;&#26469;&#26500;&#24314;&#21452;&#36755;&#20986;&#22270;&#20687;&#32534;&#30721;&#22120;&#20197;&#22686;&#24378;&#34920;&#31034;&#12290;&#23545;&#20110;&#25991;&#26412;&#29305;&#24449;&#65292;&#25105;&#20204;&#22522;&#20110;Fine-tuning Sentence-Transformers&#33719;&#24471;&#25991;&#26412;&#23884;&#20837;&#65292;&#36890;&#36807;&#25972;&#21512;&#39046;&#22495;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10381v1 Announce Type: cross  Abstract: The rapid advancement of high-quality image generation models based on AI has generated a deluge of anime illustrations. Recommending illustrations to users within massive data has become a challenging and popular task. However, existing anime recommendation systems have focused on text features but still need to integrate image features. In addition, most multi-modal recommendation research is constrained by tightly coupled datasets, limiting its applicability to anime illustrations. We propose the User-aware Multi-modal Animation Illustration Recommendation Fusion with Painting Style (UMAIR-FPS) to tackle these gaps. In the feature extract phase, for image features, we are the first to combine image painting style features with semantic features to construct a dual-output image encoder for enhancing representation. For text features, we obtain text embeddings based on fine-tuning Sentence-Transformers by incorporating domain knowledg
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32771;&#34385;&#22810;&#31181;&#29992;&#25143;&#34892;&#20026;&#26469;&#20943;&#36731;&#27969;&#34892;&#24230;&#20559;&#35265;&#65292;&#22788;&#29702;&#20102;&#22810;&#34892;&#20026;&#25512;&#33616;&#20013;&#30340;&#20844;&#24179;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2209.04589</link><description>&lt;p&gt;
&#22810;&#34892;&#20026;&#25512;&#33616;&#20013;&#30340;&#20844;&#24179;&#22240;&#26524;&#24178;&#39044;
&lt;/p&gt;
&lt;p&gt;
Causal Intervention for Fairness in Multi-behavior Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2209.04589
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32771;&#34385;&#22810;&#31181;&#29992;&#25143;&#34892;&#20026;&#26469;&#20943;&#36731;&#27969;&#34892;&#24230;&#20559;&#35265;&#65292;&#22788;&#29702;&#20102;&#22810;&#34892;&#20026;&#25512;&#33616;&#20013;&#30340;&#20844;&#24179;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#36890;&#24120;&#20174;&#21508;&#31181;&#29992;&#25143;&#34892;&#20026;&#20013;&#23398;&#20064;&#29992;&#25143;&#20852;&#36259;&#65292;&#21253;&#25324;&#28857;&#20987;&#21644;&#28857;&#20987;&#21518;&#30340;&#34892;&#20026;&#65288;&#20363;&#22914;&#65292;&#28857;&#36190;&#21644;&#25910;&#34255;&#65289;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#34892;&#20026;&#19981;&#21487;&#36991;&#20813;&#22320;&#34920;&#29616;&#20986;&#27969;&#34892;&#24230;&#20559;&#24046;&#65292;&#23548;&#33268;&#19968;&#20123;&#19981;&#20844;&#24179;&#38382;&#39064;&#65306;1&#65289;&#23545;&#20110;&#30456;&#20284;&#36136;&#37327;&#30340;&#29289;&#21697;&#65292;&#26356;&#21463;&#27426;&#36814;&#30340;&#29289;&#21697;&#20250;&#33719;&#24471;&#26356;&#22810;&#26333;&#20809;&#65307;2&#65289;&#26356;&#31967;&#31957;&#30340;&#26159;&#65292;&#27969;&#34892;&#24230;&#36739;&#20302;&#30340;&#21463;&#27426;&#36814;&#29289;&#21697;&#21487;&#33021;&#20250;&#33719;&#24471;&#26356;&#22810;&#26333;&#20809;&#12290;&#29616;&#26377;&#24037;&#20316;&#22312;&#20943;&#36731;&#27969;&#34892;&#24230;&#20559;&#24046;&#26041;&#38754;&#30450;&#30446;&#28040;&#38500;&#20559;&#35265;&#65292;&#36890;&#24120;&#24573;&#30053;&#29289;&#21697;&#36136;&#37327;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#19981;&#21516;&#29992;&#25143;&#34892;&#20026;&#20043;&#38388;&#65288;&#20363;&#22914;&#36716;&#21270;&#29575;&#65289;&#30340;&#20851;&#31995;&#23454;&#38469;&#19978;&#21453;&#26144;&#20102;&#29289;&#21697;&#36136;&#37327;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#22788;&#29702;&#19981;&#20844;&#24179;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#32771;&#34385;&#22810;&#31181;&#29992;&#25143;&#34892;&#20026;&#26469;&#20943;&#36731;&#27969;&#34892;&#24230;&#20559;&#35265;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22810;&#34892;&#20026;&#25512;&#33616;&#20013;&#20132;&#20114;&#29983;&#25104;&#36807;&#31243;&#32972;&#21518;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2209.04589v2 Announce Type: replace-cross  Abstract: Recommender systems usually learn user interests from various user behaviors, including clicks and post-click behaviors (e.g., like and favorite). However, these behaviors inevitably exhibit popularity bias, leading to some unfairness issues: 1) for items with similar quality, more popular ones get more exposure; and 2) even worse the popular items with lower popularity might receive more exposure. Existing work on mitigating popularity bias blindly eliminates the bias and usually ignores the effect of item quality. We argue that the relationships between different user behaviors (e.g., conversion rate) actually reflect the item quality. Therefore, to handle the unfairness issues, we propose to mitigate the popularity bias by considering multiple user behaviors.   In this work, we examine causal relationships behind the interaction generation procedure in multi-behavior recommendation. Specifically, we find that: 1) item popula
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33021;&#22815;&#35299;&#20915;&#23454;&#20307;&#23545;&#40784;&#35299;&#30721;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#26368;&#23567;&#21270;&#33021;&#37327;&#26469;&#20248;&#21270;&#35299;&#30721;&#36807;&#31243;&#65292;&#20197;&#23454;&#29616;&#22270;&#21516;&#36136;&#24615;&#65292;&#24182;&#19988;&#20165;&#20381;&#36182;&#20110;&#23454;&#20307;&#23884;&#20837;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#36890;&#29992;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.12798</link><description>&lt;p&gt;
&#33021;&#37327;&#30340;&#26799;&#24230;&#27969;&#65306;&#23454;&#20307;&#23545;&#40784;&#35299;&#30721;&#30340;&#36890;&#29992;&#39640;&#25928;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Gradient Flow of Energy: A General and Efficient Approach for Entity Alignment Decoding. (arXiv:2401.12798v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12798
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33021;&#22815;&#35299;&#20915;&#23454;&#20307;&#23545;&#40784;&#35299;&#30721;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#26368;&#23567;&#21270;&#33021;&#37327;&#26469;&#20248;&#21270;&#35299;&#30721;&#36807;&#31243;&#65292;&#20197;&#23454;&#29616;&#22270;&#21516;&#36136;&#24615;&#65292;&#24182;&#19988;&#20165;&#20381;&#36182;&#20110;&#23454;&#20307;&#23884;&#20837;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#36890;&#29992;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#20307;&#23545;&#40784;&#65288;EA&#65289;&#26159;&#22312;&#38598;&#25104;&#22810;&#28304;&#30693;&#35782;&#22270;&#35889;&#65288;KGs&#65289;&#20013;&#30340;&#20851;&#38190;&#36807;&#31243;&#65292;&#26088;&#22312;&#35782;&#21035;&#36825;&#20123;&#22270;&#35889;&#20013;&#30340;&#31561;&#20215;&#23454;&#20307;&#23545;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#23558;EA&#35270;&#20026;&#22270;&#34920;&#31034;&#23398;&#20064;&#20219;&#21153;&#65292;&#19987;&#27880;&#20110;&#22686;&#24378;&#22270;&#32534;&#30721;&#22120;&#12290;&#28982;&#32780;&#65292;&#22312;EA&#20013;&#35299;&#30721;&#36807;&#31243;-&#23545;&#20110;&#26377;&#25928;&#30340;&#25805;&#20316;&#21644;&#23545;&#40784;&#20934;&#30830;&#24615;&#33267;&#20851;&#37325;&#35201;-&#24471;&#21040;&#20102;&#26377;&#38480;&#30340;&#20851;&#27880;&#65292;&#24182;&#19988;&#20173;&#28982;&#38024;&#23545;&#29305;&#23450;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#26550;&#26500;&#36827;&#34892;&#23450;&#21046;&#65292;&#38656;&#35201;&#23454;&#20307;&#21644;&#39069;&#22806;&#30340;&#26174;&#24335;&#20851;&#31995;&#23884;&#20837;&#12290;&#36825;&#31181;&#29305;&#27530;&#24615;&#38480;&#21046;&#20102;&#23427;&#30340;&#36866;&#29992;&#24615;&#65292;&#23588;&#20854;&#26159;&#22312;&#22522;&#20110;GNN&#30340;&#27169;&#22411;&#20013;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#12289;&#36890;&#29992;&#21644;&#39640;&#25928;&#30340;EA&#35299;&#30721;&#26041;&#27861;&#65292;&#20165;&#20381;&#36182;&#20110;&#23454;&#20307;&#23884;&#20837;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#26368;&#23567;&#21270;&#29380;&#21033;&#20811;&#38647;&#33021;&#37327;&#26469;&#20248;&#21270;&#35299;&#30721;&#36807;&#31243;&#65292;&#22312;&#22270;&#20869;&#24341;&#23548;&#26799;&#24230;&#27969;&#65292;&#20197;&#20419;&#36827;&#22270;&#21516;&#36136;&#24615;&#12290;&#26799;&#24230;&#27969;&#30340;&#31163;&#25955;&#21270;&#20135;&#29983;&#20102;&#19968;&#31181;&#24555;&#36895;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#19977;&#20803;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Entity alignment (EA), a pivotal process in integrating multi-source Knowledge Graphs (KGs), seeks to identify equivalent entity pairs across these graphs. Most existing approaches regard EA as a graph representation learning task, concentrating on enhancing graph encoders. However, the decoding process in EA - essential for effective operation and alignment accuracy - has received limited attention and remains tailored to specific datasets and model architectures, necessitating both entity and additional explicit relation embeddings. This specificity limits its applicability, particularly in GNN-based models. To address this gap, we introduce a novel, generalized, and efficient decoding approach for EA, relying solely on entity embeddings. Our method optimizes the decoding process by minimizing Dirichlet energy, leading to the gradient flow within the graph, to promote graph homophily. The discretization of the gradient flow produces a fast and scalable approach, termed Triple Feature
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#24050;&#32463;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;DNN&#26041;&#27861;&#30340;&#38480;&#21046;&#65292;&#24182;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#35821;&#35328;&#29702;&#35299;&#12289;&#29983;&#25104;&#12289;&#25512;&#29702;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.02046</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#30340;&#25512;&#33616;&#31995;&#32479; (LLMs)
&lt;/p&gt;
&lt;p&gt;
Recommender Systems in the Era of Large Language Models (LLMs). (arXiv:2307.02046v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02046
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#24050;&#32463;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;DNN&#26041;&#27861;&#30340;&#38480;&#21046;&#65292;&#24182;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#35821;&#35328;&#29702;&#35299;&#12289;&#29983;&#25104;&#12289;&#25512;&#29702;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#30005;&#23376;&#21830;&#21153;&#21644;&#32593;&#32476;&#24212;&#29992;&#30340;&#32321;&#33635;&#65292;&#25512;&#33616;&#31995;&#32479;&#65288;RecSys&#65289;&#24050;&#32463;&#25104;&#20026;&#25105;&#20204;&#26085;&#24120;&#29983;&#27963;&#20013;&#37325;&#35201;&#30340;&#32452;&#25104;&#37096;&#20998;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#20010;&#24615;&#21270;&#24314;&#35758;&#20197;&#28385;&#36275;&#20854;&#21916;&#22909;&#12290;&#23613;&#31649;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#36890;&#36807;&#27169;&#25311;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#21644;&#25972;&#21512;&#25991;&#26412;&#20391;&#20449;&#24687;&#22312;&#25552;&#21319;&#25512;&#33616;&#31995;&#32479;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#65292;&#20294;&#26159;DNN&#26041;&#27861;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65292;&#20363;&#22914;&#29702;&#35299;&#29992;&#25143;&#20852;&#36259;&#12289;&#25429;&#25417;&#25991;&#26412;&#20391;&#20449;&#24687;&#30340;&#22256;&#38590;&#65292;&#20197;&#21450;&#22312;&#19981;&#21516;&#25512;&#33616;&#22330;&#26223;&#20013;&#27867;&#21270;&#21644;&#25512;&#29702;&#33021;&#21147;&#30340;&#19981;&#36275;&#31561;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#65288;&#20363;&#22914;ChatGPT&#21644;GPT4&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#39046;&#22495;&#24341;&#36215;&#20102;&#38761;&#21629;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#30340;&#22522;&#26412;&#32844;&#36131;&#19978;&#26377;&#30528;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#20855;&#26377;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#27867;&#21270;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the prosperity of e-commerce and web applications, Recommender Systems (RecSys) have become an important component of our daily life, providing personalized suggestions that cater to user preferences. While Deep Neural Networks (DNNs) have made significant advancements in enhancing recommender systems by modeling user-item interactions and incorporating textual side information, DNN-based methods still face limitations, such as difficulties in understanding users' interests and capturing textual side information, inabilities in generalizing to various recommendation scenarios and reasoning on their predictions, etc. Meanwhile, the emergence of Large Language Models (LLMs), such as ChatGPT and GPT4, has revolutionized the fields of Natural Language Processing (NLP) and Artificial Intelligence (AI), due to their remarkable abilities in fundamental responsibilities of language understanding and generation, as well as impressive generalization and reasoning capabilities. As a result, 
&lt;/p&gt;</description></item></channel></rss>