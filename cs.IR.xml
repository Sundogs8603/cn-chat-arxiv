<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;Pauli&#22122;&#22768;&#19979;&#20174;&#37327;&#23376;&#29366;&#24577;&#20013;&#24674;&#22797;&#20449;&#24687;&#12290;&#36890;&#36807;&#21518;&#22788;&#29702;&#36890;&#36947;&#30340;&#32463;&#20856;&#38452;&#24433;&#65292;&#23398;&#20064;&#26410;&#30693;Pauli&#36890;&#36947;&#30340;&#24517;&#35201;&#20449;&#24687;&#65292;&#24182;&#21482;&#38656;&#35201;&#36890;&#36947;&#30340;&#37096;&#20998;&#20449;&#24687;&#32780;&#19981;&#26159;&#20854;&#23436;&#25972;&#30340;&#32463;&#20856;&#25551;&#36848;&#23601;&#21487;&#20197;&#24674;&#22797;&#29702;&#24819;&#30340;&#20449;&#24687;&#65292;&#20135;&#29983;&#19968;&#20010;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.04148</link><description>&lt;p&gt;
&#36890;&#36807;&#32463;&#20856;shadow&#39640;&#25928;&#24674;&#22797;Pauli&#22122;&#22768;&#20013;&#30340;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Efficient information recovery from Pauli noise via classical shadow. (arXiv:2305.04148v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04148
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;Pauli&#22122;&#22768;&#19979;&#20174;&#37327;&#23376;&#29366;&#24577;&#20013;&#24674;&#22797;&#20449;&#24687;&#12290;&#36890;&#36807;&#21518;&#22788;&#29702;&#36890;&#36947;&#30340;&#32463;&#20856;&#38452;&#24433;&#65292;&#23398;&#20064;&#26410;&#30693;Pauli&#36890;&#36947;&#30340;&#24517;&#35201;&#20449;&#24687;&#65292;&#24182;&#21482;&#38656;&#35201;&#36890;&#36947;&#30340;&#37096;&#20998;&#20449;&#24687;&#32780;&#19981;&#26159;&#20854;&#23436;&#25972;&#30340;&#32463;&#20856;&#25551;&#36848;&#23601;&#21487;&#20197;&#24674;&#22797;&#29702;&#24819;&#30340;&#20449;&#24687;&#65292;&#20135;&#29983;&#19968;&#20010;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#35745;&#31639;&#30340;&#36805;&#29467;&#21457;&#23637;&#23548;&#33268;&#20102;&#23545;&#20174;&#37327;&#23376;&#31995;&#32479;&#20013;&#25552;&#21462;&#32463;&#20856;&#20449;&#24687;&#30340;&#26377;&#25928;&#25216;&#26415;&#30340;&#24191;&#27867;&#38656;&#27714;&#65292;&#23588;&#20854;&#26159;&#22312;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#21644;&#37327;&#23376;&#21270;&#23398;&#31561;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#37327;&#23376;&#31995;&#32479;&#26412;&#36136;&#19978;&#23481;&#26131;&#21463;&#21040;&#22122;&#22768;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#25439;&#22351;&#37327;&#23376;&#31995;&#32479;&#20013;&#32534;&#30721;&#30340;&#20449;&#24687;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;Pauli&#22122;&#22768;&#19979;&#20174;&#37327;&#23376;&#29366;&#24577;&#20013;&#24674;&#22797;&#20449;&#24687;&#12290;&#20854;&#26680;&#24515;&#24605;&#24819;&#26159;&#36890;&#36807;&#21518;&#22788;&#29702;&#36890;&#36947;&#30340;&#32463;&#20856;&#38452;&#24433;&#26469;&#23398;&#20064;&#26410;&#30693;Pauli&#36890;&#36947;&#30340;&#24517;&#35201;&#20449;&#24687;&#12290;&#23545;&#20110;&#23616;&#37096;&#21644;&#26377;&#30028;&#24230;&#25968;&#30340;&#21487;&#35266;&#23519;&#37327;&#65292;&#21482;&#38656;&#35201;&#36890;&#36947;&#30340;&#37096;&#20998;&#20449;&#24687;&#32780;&#19981;&#26159;&#20854;&#23436;&#25972;&#30340;&#32463;&#20856;&#25551;&#36848;&#23601;&#21487;&#20197;&#24674;&#22797;&#29702;&#24819;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#20135;&#29983;&#19968;&#20010;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#12290;&#36825;&#19982;&#24120;&#35268;&#26041;&#27861;&#65288;&#22914;&#27010;&#29575;&#35823;&#24046;&#28040;&#38500;&#65289;&#24418;&#25104;&#23545;&#27604;&#65292;&#21518;&#32773;&#38656;&#35201;&#36890;&#36947;&#30340;&#23436;&#25972;&#20449;&#24687;&#24182;&#23637;&#31034;&#25351;&#25968;&#32423;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid advancement of quantum computing has led to an extensive demand for effective techniques to extract classical information from quantum systems, particularly in fields like quantum machine learning and quantum chemistry. However, quantum systems are inherently susceptible to noises, which adversely corrupt the information encoded in quantum systems. In this work, we introduce an efficient algorithm that can recover information from quantum states under Pauli noise. The core idea is to learn the necessary information of the unknown Pauli channel by post-processing the classical shadows of the channel. For a local and bounded-degree observable, only partial knowledge of the channel is required rather than its complete classical description to recover the ideal information, resulting in a polynomial-time algorithm. This contrasts with conventional methods such as probabilistic error cancellation, which requires the full information of the channel and exhibits exponential scaling 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#20041;&#24863;&#30693;&#23545;&#27604;&#23398;&#20064;&#21644;Transformer&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#20851;&#38190;&#35789;&#22810;&#26679;&#22270;&#20687;&#26816;&#32034;&#65292;&#33021;&#22815;&#24179;&#34913;&#30456;&#20851;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.04072</link><description>&lt;p&gt;
&#22522;&#20110;&#20851;&#38190;&#35789;&#30340;&#22810;&#26679;&#22270;&#20687;&#26816;&#32034;&#65306;&#22522;&#20110;&#35821;&#20041;&#24863;&#30693;&#23545;&#27604;&#23398;&#20064;&#21644;Transformer&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Keyword-Based Diverse Image Retrieval by Semantics-aware Contrastive Learning and Transformer. (arXiv:2305.04072v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04072
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#20041;&#24863;&#30693;&#23545;&#27604;&#23398;&#20064;&#21644;Transformer&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#20851;&#38190;&#35789;&#22810;&#26679;&#22270;&#20687;&#26816;&#32034;&#65292;&#33021;&#22815;&#24179;&#34913;&#30456;&#20851;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38500;&#20102;&#30456;&#20851;&#24615;&#22806;&#65292;&#22810;&#26679;&#24615;&#26159;&#36328;&#27169;&#24577;&#22270;&#20687;&#26816;&#32034;&#31995;&#32479;&#30340;&#21478;&#19968;&#20010;&#37325;&#35201;&#20294;&#36739;&#23569;&#30740;&#31350;&#30340;&#24615;&#33021;&#25351;&#26631;&#65292;&#23545;&#29992;&#25143;&#20307;&#39564;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#35201;&#20040;&#26126;&#30830;&#22320;&#23545;&#26469;&#33258;&#26631;&#20934;&#26816;&#32034;&#31995;&#32479;&#30340;&#21407;&#22987;&#26816;&#32034;&#32467;&#26524;&#36827;&#34892;&#21518;&#22788;&#29702;&#65292;&#35201;&#20040;&#23581;&#35797;&#23398;&#20064;&#22810;&#21521;&#37327;&#34920;&#31034;&#22270;&#20687;&#20197;&#34920;&#31034;&#20854;&#22810;&#26679;&#30340;&#35821;&#20041;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#37117;&#19981;&#33021;&#24456;&#22909;&#22320;&#24179;&#34913;&#30456;&#20851;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In addition to relevance, diversity is an important yet less studied performance metric of cross-modal image retrieval systems, which is critical to user experience. Existing solutions for diversity-aware image retrieval either explicitly post-process the raw retrieval results from standard retrieval systems or try to learn multi-vector representations of images to represent their diverse semantics. However, neither of them is good enough to balance relevance and diversity. On the one hand, standard retrieval systems are usually biased to common semantics and seldom exploit diversity-aware regularization in training, which makes it difficult to promote diversity by post-processing. On the other hand, multi-vector representation methods are not guaranteed to learn robust multiple projections. As a result, irrelevant images and images of rare or unique semantics may be projected inappropriately, which degrades the relevance and diversity of the results generated by some typical algorithm
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21518;&#38376;&#25915;&#20987;&#39044;&#35757;&#32451;&#25512;&#33616;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#25552;&#20379;&#32773;&#21487;&#20197;&#24456;&#23481;&#26131;&#22320;&#23558;&#21518;&#38376;&#25554;&#20837;&#20854;&#20013;&#12290;&#36825;&#21487;&#33021;&#21152;&#21095;&#20102;&#30446;&#26631;&#29992;&#25143;&#32676;&#23545;&#30446;&#26631;&#29289;&#21697;&#30340;&#26333;&#20809;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.03995</link><description>&lt;p&gt;
&#38024;&#23545;&#39044;&#35757;&#32451;&#25512;&#33616;&#27169;&#22411;&#30340;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Attacking Pre-trained Recommendation. (arXiv:2305.03995v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03995
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21518;&#38376;&#25915;&#20987;&#39044;&#35757;&#32451;&#25512;&#33616;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#25552;&#20379;&#32773;&#21487;&#20197;&#24456;&#23481;&#26131;&#22320;&#23558;&#21518;&#38376;&#25554;&#20837;&#20854;&#20013;&#12290;&#36825;&#21487;&#33021;&#21152;&#21095;&#20102;&#30446;&#26631;&#29992;&#25143;&#32676;&#23545;&#30446;&#26631;&#29289;&#21697;&#30340;&#26333;&#20809;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#19968;&#31995;&#21015;&#30740;&#31350;&#34920;&#26126;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#24207;&#21015;&#25512;&#33616;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#20026;&#24314;&#31435;&#36866;&#29992;&#20110;&#19981;&#21516;&#19979;&#28216;&#25512;&#33616;&#20219;&#21153;&#30340;&#26234;&#33021;&#32479;&#19968;&#39044;&#35757;&#32451;&#25512;&#33616;&#27169;&#22411;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36825;&#20123;&#36827;&#23637;&#65292;&#20294;&#26159;&#32463;&#20856;&#25512;&#33616;&#31995;&#32479;&#30340;&#28431;&#27934;&#20063;&#20197;&#19968;&#31181;&#26032;&#24418;&#24335;&#23384;&#22312;&#20110;&#39044;&#35757;&#32451;&#25512;&#33616;&#20013;&#65292;&#21516;&#26102;&#39044;&#35757;&#32451;&#25512;&#33616;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#20173;&#26410;&#34987;&#24320;&#21457;&#65292;&#36825;&#21487;&#33021;&#20250;&#23041;&#32961;&#20854;&#24191;&#27867;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#39044;&#35757;&#32451;&#25512;&#33616;&#30340;&#21518;&#38376;&#25915;&#20987;&#26032;&#26694;&#26550;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#25552;&#20379;&#32773;&#21487;&#20197;&#24456;&#23481;&#26131;&#22320;&#22312;&#39044;&#35757;&#32451;&#20013;&#25554;&#20837;&#21518;&#38376;&#65292;&#20174;&#32780;&#22686;&#21152;&#30446;&#26631;&#29289;&#21697;&#38754;&#21521;&#30446;&#26631;&#29992;&#25143;&#32676;&#20307;&#30340;&#26333;&#20809;&#29575;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#31181;&#26032;&#39062;&#26377;&#25928;&#30340;&#21518;&#38376;&#25915;&#20987;&#65306;&#22522;&#26412;&#26367;&#25442;&#21644;&#25552;&#31034;&#22686;&#24378;&#65292;&#22312;&#21508;&#31181;&#25512;&#33616;&#39044;&#35757;&#32451;&#20351;&#29992;&#24773;&#20917;&#19979;&#12290;&#22312;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;
&lt;/p&gt;
&lt;p&gt;
Recently, a series of pioneer studies have shown the potency of pre-trained models in sequential recommendation, illuminating the path of building an omniscient unified pre-trained recommendation model for different downstream recommendation tasks. Despite these advancements, the vulnerabilities of classical recommender systems also exist in pre-trained recommendation in a new form, while the security of pre-trained recommendation model is still unexplored, which may threaten its widely practical applications. In this study, we propose a novel framework for backdoor attacking in pre-trained recommendation. We demonstrate the provider of the pre-trained model can easily insert a backdoor in pre-training, thereby increasing the exposure rates of target items to target user groups. Specifically, we design two novel and effective backdoor attacks: basic replacement and prompt-enhanced, under various recommendation pre-training usage scenarios. Experimental results on real-world datasets sh
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#20280;&#32553;&#39640;&#25928;&#30340;&#22270;&#20687;&#21040;&#36328;&#27169;&#24577;&#26816;&#32034;&#33539;&#24335;Mixer&#65292;&#35299;&#20915;&#20102;&#39046;&#22495;&#24046;&#36317;&#12289;&#36328;&#27169;&#24577;&#25968;&#25454;&#23545;&#40784;&#21644;&#34701;&#21512;&#12289;&#32321;&#26434;&#30340;&#25968;&#25454;&#35757;&#32451;&#26631;&#31614;&#20197;&#21450;&#28023;&#37327;&#26597;&#35810;&#21644;&#21450;&#26102;&#21709;&#24212;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.03972</link><description>&lt;p&gt;
Mixer: &#24212;&#29992;&#20110;&#24037;&#19994;&#24212;&#29992;&#30340;&#22270;&#20687;&#21040;&#36328;&#27169;&#24577;&#26816;&#32034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Mixer: Image to Multi-Modal Retrieval Learning for Industrial Application. (arXiv:2305.03972v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03972
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#20280;&#32553;&#39640;&#25928;&#30340;&#22270;&#20687;&#21040;&#36328;&#27169;&#24577;&#26816;&#32034;&#33539;&#24335;Mixer&#65292;&#35299;&#20915;&#20102;&#39046;&#22495;&#24046;&#36317;&#12289;&#36328;&#27169;&#24577;&#25968;&#25454;&#23545;&#40784;&#21644;&#34701;&#21512;&#12289;&#32321;&#26434;&#30340;&#25968;&#25454;&#35757;&#32451;&#26631;&#31614;&#20197;&#21450;&#28023;&#37327;&#26597;&#35810;&#21644;&#21450;&#26102;&#21709;&#24212;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#27169;&#24577;&#26816;&#32034;&#19968;&#30452;&#26159;&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#21644;&#20869;&#23481;&#20998;&#20139;&#31038;&#20132;&#23186;&#20307;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#38656;&#27714;&#65292;&#20854;&#20013;&#26597;&#35810;&#26159;&#19968;&#24352;&#22270;&#29255;&#65292;&#25991;&#26723;&#26159;&#20855;&#26377;&#22270;&#29255;&#21644;&#25991;&#26412;&#25551;&#36848;&#30340;&#39033;&#30446;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36825;&#31181;&#26816;&#32034;&#20219;&#21153;&#20173;&#38754;&#20020;&#35832;&#22810;&#25361;&#25112;&#65292;&#21253;&#25324;&#39046;&#22495;&#24046;&#36317;&#12289;&#36328;&#27169;&#24577;&#25968;&#25454;&#23545;&#40784;&#21644;&#34701;&#21512;&#12289;&#32321;&#26434;&#30340;&#25968;&#25454;&#35757;&#32451;&#26631;&#31614;&#20197;&#21450;&#28023;&#37327;&#26597;&#35810;&#21644;&#21450;&#26102;&#21709;&#24212;&#31561;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Mixer&#30340;&#26032;&#22411;&#21487;&#20280;&#32553;&#21644;&#39640;&#25928;&#30340;&#22270;&#20687;&#26597;&#35810;&#21040;&#36328;&#27169;&#24577;&#26816;&#32034;&#23398;&#20064;&#33539;&#24335;&#12290;Mixer&#36890;&#36807;&#33258;&#36866;&#24212;&#22320;&#25972;&#21512;&#22810;&#27169;&#24577;&#25968;&#25454;&#12289;&#26356;&#39640;&#25928;&#22320;&#25366;&#25496;&#20559;&#26012;&#21644;&#22024;&#26434;&#30340;&#25968;&#25454;&#65292;&#24182;&#21487;&#25193;&#23637;&#21040;&#39640;&#36127;&#36733;&#37327;&#65292;&#35299;&#20915;&#20102;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-modal retrieval, where the query is an image and the doc is an item with both image and text description, is ubiquitous in e-commerce platforms and content-sharing social media. However, little research attention has been paid to this important application. This type of retrieval task is challenging due to the facts: 1)~domain gap exists between query and doc. 2)~multi-modality alignment and fusion. 3)~skewed training data and noisy labels collected from user behaviors. 4)~huge number of queries and timely responses while the large-scale candidate docs exist. To this end, we propose a novel scalable and efficient image query to multi-modal retrieval learning paradigm called Mixer, which adaptively integrates multi-modality data, mines skewed and noisy data more efficiently and scalable to high traffic. The Mixer consists of three key ingredients: First, for query and doc image, a shared encoder network followed by separate transformation networks are utilized to account for their
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20013;&#25552;&#20986;&#20102;&#19968;&#20010;&#36328;&#39046;&#22495;&#22686;&#24378;&#32593;&#32476;&#65288;CDAnet&#65289;&#65292;&#33021;&#22815;&#36827;&#34892;&#24322;&#26500;&#36755;&#20837;&#19979;&#30340;&#30693;&#35782;&#36716;&#31227;&#21644;&#29983;&#25104;&#22686;&#24378;&#26679;&#26412;&#20943;&#36731;&#25968;&#25454;&#31232;&#30095;&#38382;&#39064;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#32447;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.03953</link><description>&lt;p&gt;
&#36328;&#39046;&#22495;&#22686;&#24378;&#32593;&#32476;&#29992;&#20110;&#28857;&#20987;&#29575;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Cross-domain Augmentation Networks for Click-Through Rate Prediction. (arXiv:2305.03953v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03953
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20013;&#25552;&#20986;&#20102;&#19968;&#20010;&#36328;&#39046;&#22495;&#22686;&#24378;&#32593;&#32476;&#65288;CDAnet&#65289;&#65292;&#33021;&#22815;&#36827;&#34892;&#24322;&#26500;&#36755;&#20837;&#19979;&#30340;&#30693;&#35782;&#36716;&#31227;&#21644;&#29983;&#25104;&#22686;&#24378;&#26679;&#26412;&#20943;&#36731;&#25968;&#25454;&#31232;&#30095;&#38382;&#39064;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#31232;&#30095;&#24615;&#26159;&#28857;&#20987;&#29575;&#39044;&#27979;&#30340;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#24403;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#22826;&#31232;&#30095;&#20197;&#33267;&#20110;&#26080;&#27861;&#23398;&#20064;&#21487;&#38752;&#30340;&#27169;&#22411;&#26102;&#12290;&#26368;&#36817;&#65292;&#20026;&#20102;&#21033;&#29992;&#30456;&#20851;&#39046;&#22495;&#30340;&#26377;&#24847;&#20041;&#25968;&#25454;&#65292;&#35768;&#22810;&#36328;&#39046;&#22495;&#28857;&#20987;&#29575;&#65288;CDCTR&#65289;&#39044;&#27979;&#24037;&#20316;&#24050;&#32463;&#34987;&#24320;&#21457;&#20986;&#26469;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;CDCTR&#24037;&#20316;&#20855;&#26377;&#19981;&#20999;&#23454;&#38469;&#30340;&#38480;&#21046;&#65292;&#38656;&#35201;&#36328;&#39046;&#22495;&#20043;&#38388;&#20855;&#26377;&#23436;&#20840;&#30456;&#21516;&#30340;&#29305;&#24449;&#36755;&#20837;&#65292;&#32780;&#36328;&#39046;&#22495;&#19982;&#19981;&#21516;&#30340;&#29305;&#24449;&#36755;&#20837;&#30340;CDCTR&#23578;&#26410;&#34987;&#24191;&#27867;&#25506;&#32034;&#65292;&#20294;&#36825;&#26159;&#19968;&#20010;&#32039;&#24613;&#21644;&#37325;&#35201;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36328;&#39046;&#22495;&#22686;&#24378;&#32593;&#32476;&#65288;CDAnet&#65289;&#65292;&#33021;&#22815;&#22312;&#20855;&#26377;&#24322;&#26500;&#36755;&#20837;&#30340;&#20004;&#20010;&#39046;&#22495;&#20043;&#38388;&#25191;&#34892;&#30693;&#35782;&#36716;&#31227;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;CDAnet&#21253;&#21547;&#19968;&#20010;&#35774;&#35745;&#33391;&#22909;&#30340;&#32763;&#35793;&#32593;&#32476;&#21644;&#19968;&#20010;&#22686;&#24378;&#32593;&#32476;&#65292;&#36825;&#20004;&#20010;&#32593;&#32476;&#35757;&#32451;&#36807;&#31243;&#26159;&#20381;&#27425;&#36827;&#34892;&#30340;&#12290;&#32763;&#35793;&#32593;&#32476;&#33021;&#22815;&#35745;&#31639;&#20986;&#20855;&#26377;&#24322;&#26500;&#36755;&#20837;&#30340;&#20004;&#20010;&#39046;&#22495;&#30340;&#29305;&#24449;&#65292;&#22686;&#24378;&#32593;&#32476;&#33021;&#22815;&#29983;&#25104;&#22686;&#24378;&#26679;&#26412;&#20197;&#32531;&#35299;&#25968;&#25454;&#31232;&#30095;&#38382;&#39064;&#12290;&#22312;&#20004;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data sparsity is an important issue for click-through rate (CTR) prediction, particularly when user-item interactions is too sparse to learn a reliable model. Recently, many works on cross-domain CTR (CDCTR) prediction have been developed in an effort to leverage meaningful data from a related domain. However, most existing CDCTR works have an impractical limitation that requires homogeneous inputs (\textit{i.e.} shared feature fields) across domains, and CDCTR with heterogeneous inputs (\textit{i.e.} varying feature fields) across domains has not been widely explored but is an urgent and important research problem. In this work, we propose a cross-domain augmentation network (CDAnet) being able to perform knowledge transfer between two domains with \textit{heterogeneous inputs}. Specifically, CDAnet contains a designed translation network and an augmentation network which are trained sequentially. The translation network is able to compute features from two domains with heterogeneous 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#35821;&#35328;&#31264;&#23494;&#26816;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#36328;&#35821;&#35328;&#26597;&#35810;&#29983;&#25104;&#26041;&#27861;&#26469;&#22686;&#24378;&#36890;&#36947;&#34920;&#31034;&#65292;&#20197;&#24212;&#23545;&#36328;&#35821;&#35328;&#25968;&#25454;&#36739;&#23569;&#30340;&#38382;&#39064;&#12290;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#20351;&#29992;&#29983;&#25104;&#22120;&#19981;&#20250;&#22686;&#21152;&#39069;&#22806;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2305.03950</link><description>&lt;p&gt;
&#36890;&#36807;&#26597;&#35810;&#29983;&#25104;&#22686;&#24378;&#36890;&#36947;&#34920;&#31034;&#20197;&#23454;&#29616;&#22686;&#24378;&#36328;&#35821;&#35328;&#31264;&#23494;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Augmenting Passage Representations with Query Generation for Enhanced Cross-Lingual Dense Retrieval. (arXiv:2305.03950v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03950
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#35821;&#35328;&#31264;&#23494;&#26816;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#36328;&#35821;&#35328;&#26597;&#35810;&#29983;&#25104;&#26041;&#27861;&#26469;&#22686;&#24378;&#36890;&#36947;&#34920;&#31034;&#65292;&#20197;&#24212;&#23545;&#36328;&#35821;&#35328;&#25968;&#25454;&#36739;&#23569;&#30340;&#38382;&#39064;&#12290;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#20351;&#29992;&#29983;&#25104;&#22120;&#19981;&#20250;&#22686;&#21152;&#39069;&#22806;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#30340;&#36328;&#35821;&#35328;&#31264;&#23494;&#26816;&#32034;&#26041;&#27861;&#38656;&#35201;&#20381;&#36182;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(PLMs)&#65292;&#21516;&#26102;&#28085;&#30422;&#30456;&#20851;&#24615;&#21305;&#37197;&#21644;&#36328;&#35821;&#35328;&#23545;&#40784;&#20219;&#21153;&#30340;&#35757;&#32451;&#20063;&#24448;&#24448;&#38754;&#20020;&#25968;&#25454;&#21294;&#20047;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;&#36328;&#35821;&#35328;&#26597;&#35810;&#29983;&#25104;&#26469;&#22686;&#24378;&#36890;&#36947;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#20197;&#24212;&#23545;&#36328;&#35821;&#35328;&#25968;&#25454;&#21294;&#20047;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#21407;&#22987;&#36890;&#36947;&#35821;&#35328;&#20043;&#22806;&#20351;&#29992;&#20854;&#20182;&#35821;&#35328;&#30340;&#26597;&#35810;&#26469;&#22686;&#24378;&#34920;&#31034;&#65292;&#36825;&#20123;&#22686;&#24378;&#34920;&#31034;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#20351;&#29992;&#65292;&#20197;&#20415;&#34920;&#31034;&#21487;&#20197;&#36328;&#36234;&#19981;&#21516;&#30446;&#26631;&#35821;&#35328;&#32534;&#30721;&#26356;&#22810;&#20449;&#24687;&#12290;&#36328;&#35821;&#35328;&#26597;&#35810;&#29983;&#25104;&#22120;&#30340;&#35757;&#32451;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#29983;&#25104;&#22120;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#20063;&#38750;&#24120;&#31867;&#20284;&#20110;&#24494;&#35843;&#20219;&#21153;(&#29983;&#25104;&#26597;&#35810;)&#12290;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#20351;&#29992;&#29983;&#25104;&#22120;&#19981;&#20250;&#22686;&#21152;&#20219;&#20309;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Effective cross-lingual dense retrieval methods that rely on multilingual pre-trained language models (PLMs) need to be trained to encompass both the relevance matching task and the cross-language alignment task. However, cross-lingual data for training is often scarcely available. In this paper, rather than using more cross-lingual data for training, we propose to use cross-lingual query generation to augment passage representations with queries in languages other than the original passage language. These augmented representations are used at inference time so that the representation can encode more information across the different target languages. Training of a cross-lingual query generator does not require additional training data to that used for the dense retriever. The query generator training is also effective because the pre-training task for the generator (T5 text-to-text training) is very similar to the fine-tuning task (generation of a query). The use of the generator does 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#32844;&#19994;&#27169;&#24335;&#21051;&#26495;&#21360;&#35937;&#38382;&#39064;&#65292;&#30740;&#31350;&#20102;&#32593;&#32476;&#25628;&#32034;&#20013;&#30340;&#20559;&#35265;&#21644;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;&#23454;&#39564;&#34920;&#26126;&#24403;&#21069;&#30340;&#22270;&#20687;&#25628;&#32034;&#24341;&#25806;&#23384;&#22312;&#30456;&#24403;&#20005;&#37325;&#30340;&#32844;&#19994;&#27169;&#24335;&#21051;&#26495;&#21360;&#35937;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21435;&#20559;&#35265;&#26041;&#27861;&#20197;&#20943;&#36731;&#27492;&#31867;&#20559;&#35265;&#24182;&#25552;&#39640;&#22270;&#20687;&#25628;&#32034;&#24341;&#25806;&#30340;&#20844;&#24179;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.03881</link><description>&lt;p&gt;
&#22270;&#20687;&#25628;&#32034;&#20013;&#30340;&#20844;&#24179;&#24615;&#65306;&#20851;&#20110;&#20174;&#22270;&#20687;&#26816;&#32034;&#19982;&#21435;&#20559;&#35265;&#35282;&#24230;&#25506;&#31350;&#32844;&#19994;&#27169;&#24335;&#21051;&#26495;&#21360;&#35937;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fairness in Image Search: A Study of Occupational Stereotyping in Image Retrieval and its Debiasing. (arXiv:2305.03881v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03881
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#32844;&#19994;&#27169;&#24335;&#21051;&#26495;&#21360;&#35937;&#38382;&#39064;&#65292;&#30740;&#31350;&#20102;&#32593;&#32476;&#25628;&#32034;&#20013;&#30340;&#20559;&#35265;&#21644;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;&#23454;&#39564;&#34920;&#26126;&#24403;&#21069;&#30340;&#22270;&#20687;&#25628;&#32034;&#24341;&#25806;&#23384;&#22312;&#30456;&#24403;&#20005;&#37325;&#30340;&#32844;&#19994;&#27169;&#24335;&#21051;&#26495;&#21360;&#35937;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21435;&#20559;&#35265;&#26041;&#27861;&#20197;&#20943;&#36731;&#27492;&#31867;&#20559;&#35265;&#24182;&#25552;&#39640;&#22270;&#20687;&#25628;&#32034;&#24341;&#25806;&#30340;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24335;&#25628;&#32034;&#24341;&#25806;&#36817;&#24180;&#26469;&#32463;&#21382;&#20102;&#26174;&#33879;&#30340;&#22686;&#38271;&#21644;&#24191;&#27867;&#30340;&#20351;&#29992;&#65292;&#25104;&#20026;&#32487;&#20449;&#24687;&#26816;&#32034;&#20043;&#21518;&#31532;&#20108;&#24120;&#35265;&#30340;&#20114;&#32852;&#32593;&#20351;&#29992;&#26041;&#24335;&#12290;&#23613;&#31649;&#25628;&#32034;&#24341;&#25806;&#31995;&#32479;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#26381;&#21153;&#65292;&#20294;&#22270;&#20687;&#25628;&#32034;&#39046;&#22495;&#26368;&#36817;&#25104;&#20026;&#20449;&#24687;&#26816;&#32034;&#31038;&#21306;&#30340;&#28966;&#28857;&#65292;&#22240;&#20026;&#24120;&#35328;&#36947;&#8220;&#19968;&#22270;&#32988;&#21315;&#35328;&#8221;&#12290;&#34429;&#28982;&#20687;&#35895;&#27468;&#36825;&#26679;&#30340;&#27969;&#34892;&#25628;&#32034;&#24341;&#25806;&#22312;&#22270;&#20687;&#25628;&#32034;&#31934;&#24230;&#21644;&#25935;&#25463;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20154;&#20204;&#23545;&#23427;&#20204;&#30340;&#25628;&#32034;&#32467;&#26524;&#26159;&#21542;&#20250;&#23384;&#22312;&#24615;&#21035;&#12289;&#35821;&#35328;&#12289;&#20154;&#21475;&#32479;&#35745;&#12289;&#31038;&#20250;&#25991;&#21270;&#26041;&#38754;&#30340;&#20559;&#35265;&#23384;&#22312;&#20105;&#35758;&#12290;&#36825;&#31181;&#28508;&#22312;&#30340;&#20559;&#35265;&#21487;&#33021;&#20250;&#23545;&#20010;&#20154;&#30340;&#35748;&#30693;&#20135;&#29983;&#37325;&#35201;&#24433;&#21709;&#65292;&#24182;&#24433;&#21709;&#20182;&#20204;&#30340;&#35270;&#35282;&#12290;&#26412;&#25991;&#20027;&#35201;&#30740;&#31350;&#32593;&#32476;&#25628;&#32034;&#20013;&#30340;&#20559;&#35265;&#21644;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#37325;&#28857;&#20851;&#27880;&#22522;&#20110;&#20851;&#38190;&#23383;&#30340;&#22270;&#20687;&#25628;&#32034;&#26041;&#38754;&#12290;&#25105;&#20204;&#39318;&#20808;&#35752;&#35770;&#20102;&#25628;&#32034;&#31995;&#32479;&#20013;&#23384;&#22312;&#30340;&#20960;&#31181;&#20559;&#35265;&#31867;&#22411;&#20197;&#21450;&#20026;&#20160;&#20040;&#26377;&#24517;&#35201;&#21152;&#20197;&#32531;&#35299;&#12290;&#25105;&#20204;&#23558;&#30740;&#31350;&#37325;&#28857;&#32553;&#23567;&#21040;&#35780;&#20272;&#21644;&#32531;&#35299;&#22270;&#20687;&#26816;&#32034;&#20013;&#30340;&#32844;&#19994;&#27169;&#24335;&#21051;&#26495;&#21360;&#35937;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#24403;&#21069;&#30340;&#22270;&#20687;&#25628;&#32034;&#24341;&#25806;&#23384;&#22312;&#30456;&#24403;&#20005;&#37325;&#30340;&#32844;&#19994;&#27169;&#24335;&#21051;&#26495;&#21360;&#35937;&#65292;&#36825;&#21487;&#33021;&#23545;&#20010;&#20154;&#21644;&#25972;&#20010;&#31038;&#20250;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21435;&#20559;&#35265;&#26041;&#27861;&#65292;&#20197;&#20943;&#36731;&#27492;&#31867;&#20559;&#35265;&#24182;&#25552;&#39640;&#22270;&#20687;&#25628;&#32034;&#24341;&#25806;&#30340;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-modal search engines have experienced significant growth and widespread use in recent years, making them the second most common internet use. While search engine systems offer a range of services, the image search field has recently become a focal point in the information retrieval community, as the adage goes, "a picture is worth a thousand words". Although popular search engines like Google excel at image search accuracy and agility, there is an ongoing debate over whether their search results can be biased in terms of gender, language, demographics, socio-cultural aspects, and stereotypes. This potential for bias can have a significant impact on individuals' perceptions and influence their perspectives.  In this paper, we present our study on bias and fairness in web search, with a focus on keyword-based image search. We first discuss several kinds of biases that exist in search systems and why it is important to mitigate them. We narrow down our study to assessing and mitigat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#19968;&#20010;&#21517;&#20026;ConvSim&#30340;&#29992;&#25143;&#27169;&#25311;&#22120;&#26469;&#35780;&#20272;&#29992;&#25143;&#21453;&#39304;&#65292;&#20174;&#32780;&#25552;&#39640;&#20250;&#35805;&#24335;&#25628;&#32034;&#30340;&#24615;&#33021;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#26377;&#25928;&#21033;&#29992;&#29992;&#25143;&#21453;&#39304;&#21487;&#20197;&#22823;&#24133;&#25552;&#39640;&#26816;&#32034;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.13874</link><description>&lt;p&gt;
&#21033;&#29992;&#27169;&#25311;&#29992;&#25143;&#21453;&#39304;&#30340;&#26041;&#24335;&#26469;&#20248;&#21270;&#20250;&#35805;&#24335;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Exploiting Simulated User Feedback for Conversational Search: Ranking, Rewriting, and Beyond. (arXiv:2304.13874v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13874
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#19968;&#20010;&#21517;&#20026;ConvSim&#30340;&#29992;&#25143;&#27169;&#25311;&#22120;&#26469;&#35780;&#20272;&#29992;&#25143;&#21453;&#39304;&#65292;&#20174;&#32780;&#25552;&#39640;&#20250;&#35805;&#24335;&#25628;&#32034;&#30340;&#24615;&#33021;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#26377;&#25928;&#21033;&#29992;&#29992;&#25143;&#21453;&#39304;&#21487;&#20197;&#22823;&#24133;&#25552;&#39640;&#26816;&#32034;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;&#35780;&#20272;&#29992;&#25143;&#21453;&#39304;&#22312;&#28151;&#21512;&#20513;&#35758;&#30340;&#20250;&#35805;&#24335;&#25628;&#32034;&#31995;&#32479;&#20013;&#30340;&#21508;&#31181;&#26041;&#27861;&#12290;&#34429;&#28982;&#20250;&#35805;&#24335;&#25628;&#32034;&#31995;&#32479;&#22312;&#22810;&#20010;&#26041;&#38754;&#37117;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#26368;&#36817;&#30340;&#30740;&#31350;&#26410;&#33021;&#25104;&#21151;&#22320;&#23558;&#29992;&#25143;&#21453;&#39304;&#32435;&#20837;&#31995;&#32479;&#20013;&#12290;&#20854;&#20013;&#19968;&#20010;&#20027;&#35201;&#21407;&#22240;&#26159;&#32570;&#20047;&#31995;&#32479;-&#29992;&#25143;&#23545;&#35805;&#20132;&#20114;&#25968;&#25454;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29992;&#25143;&#27169;&#25311;&#22120;&#30340;&#26694;&#26550;&#65292;&#21487;&#29992;&#20110;&#19982;&#21508;&#31181;&#28151;&#21512;&#20513;&#35758;&#30340;&#20250;&#35805;&#24335;&#25628;&#32034;&#31995;&#32479;&#36827;&#34892;&#22810;&#36718;&#20132;&#20114;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;ConvSim&#30340;&#29992;&#25143;&#27169;&#25311;&#22120;&#65292;&#19968;&#26086;&#21021;&#22987;&#21270;&#20102;&#20449;&#24687;&#38656;&#27714;&#25551;&#36848;&#65292;&#23601;&#33021;&#22815;&#23545;&#31995;&#32479;&#30340;&#21709;&#24212;&#25552;&#20379;&#21453;&#39304;&#65292;&#24182;&#22238;&#31572;&#28508;&#22312;&#30340;&#28548;&#28165;&#38382;&#39064;&#12290;&#25105;&#20204;&#23545;&#21508;&#31181;&#26368;&#20808;&#36827;&#30340;&#27573;&#33853;&#26816;&#32034;&#21644;&#31070;&#32463;&#37325;&#26032;&#25490;&#24207;&#27169;&#22411;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#26377;&#25928;&#21033;&#29992;&#29992;&#25143;&#21453;&#39304;&#21487;&#20197;&#23548;&#33268;&#22312;nDCG@3&#26041;&#38754;16%&#30340;&#26816;&#32034;&#24615;&#33021;&#25552;&#39640;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#38543;&#30528;n&#30340;&#22686;&#21152;&#65292;&#19968;&#33268;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
This research aims to explore various methods for assessing user feedback in mixed-initiative conversational search (CS) systems. While CS systems enjoy profuse advancements across multiple aspects, recent research fails to successfully incorporate feedback from the users. One of the main reasons for that is the lack of system-user conversational interaction data. To this end, we propose a user simulator-based framework for multi-turn interactions with a variety of mixed-initiative CS systems. Specifically, we develop a user simulator, dubbed ConvSim, that, once initialized with an information need description, is capable of providing feedback to a system's responses, as well as answering potential clarifying questions. Our experiments on a wide variety of state-of-the-art passage retrieval and neural re-ranking models show that effective utilization of user feedback can lead to 16% retrieval performance increase in terms of nDCG@3. Moreover, we observe consistent improvements as the n
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24207;&#21015;&#25512;&#33616;&#31639;&#27861;&#8212;&#8212;&#39057;&#29575;&#22686;&#24378;&#30340;&#28151;&#21512;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;FEARec&#65289;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#19968;&#20010;&#26012;&#22369;&#32467;&#26500;&#23558;&#21407;&#26377;&#30340;&#33258;&#27880;&#24847;&#21147;&#20174;&#26102;&#38388;&#22495;&#36716;&#25442;&#21040;&#39057;&#29575;&#22495;&#65292;&#20351;&#24471;&#20302;&#39057;&#21644;&#39640;&#39057;&#20449;&#24687;&#21487;&#20197;&#34987;&#26126;&#30830;&#22320;&#23398;&#20064;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#33258;&#30456;&#20851;&#30340;&#35774;&#35745;&#65292;&#35813;&#31639;&#27861;&#36824;&#21487;&#20197;&#25429;&#25417;&#21040;&#29992;&#25143;&#34892;&#20026;&#30340;&#22266;&#26377;&#21608;&#26399;&#24615;&#12290;&#22312;&#22235;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#35813;&#31639;&#27861;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.09184</link><description>&lt;p&gt;
&#39057;&#29575;&#22686;&#24378;&#30340;&#28151;&#21512;&#27880;&#24847;&#21147;&#32593;&#32476;&#29992;&#20110;&#24207;&#21015;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Frequency Enhanced Hybrid Attention Network for Sequential Recommendation. (arXiv:2304.09184v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09184
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24207;&#21015;&#25512;&#33616;&#31639;&#27861;&#8212;&#8212;&#39057;&#29575;&#22686;&#24378;&#30340;&#28151;&#21512;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;FEARec&#65289;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#19968;&#20010;&#26012;&#22369;&#32467;&#26500;&#23558;&#21407;&#26377;&#30340;&#33258;&#27880;&#24847;&#21147;&#20174;&#26102;&#38388;&#22495;&#36716;&#25442;&#21040;&#39057;&#29575;&#22495;&#65292;&#20351;&#24471;&#20302;&#39057;&#21644;&#39640;&#39057;&#20449;&#24687;&#21487;&#20197;&#34987;&#26126;&#30830;&#22320;&#23398;&#20064;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#33258;&#30456;&#20851;&#30340;&#35774;&#35745;&#65292;&#35813;&#31639;&#27861;&#36824;&#21487;&#20197;&#25429;&#25417;&#21040;&#29992;&#25143;&#34892;&#20026;&#30340;&#22266;&#26377;&#21608;&#26399;&#24615;&#12290;&#22312;&#22235;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#35813;&#31639;&#27861;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#27880;&#24847;&#26426;&#21046;&#26159;&#24207;&#21015;&#25512;&#33616;&#39046;&#22495;&#24191;&#27867;&#20351;&#29992;&#30340;&#25216;&#26415;&#20043;&#19968;&#65292;&#20855;&#26377;&#24314;&#27169;&#38271;&#31243;&#20381;&#36182;&#24615;&#30340;&#24378;&#22823;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#30446;&#21069;&#22522;&#20110;&#33258;&#27880;&#24847;&#21147;&#30340;&#27169;&#22411;&#26159;&#20302;&#36890;&#28388;&#27874;&#22120;&#65292;&#19981;&#33021;&#25429;&#25417;&#39640;&#39057;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#29992;&#25143;&#34892;&#20026;&#20013;&#30340;&#39033;&#30446;&#30456;&#20114;&#20132;&#32455;&#65292;&#36825;&#20123;&#27169;&#22411;&#26080;&#27861;&#21306;&#20998;&#26102;&#38388;&#22495;&#20013;&#27169;&#31946;&#30340;&#22266;&#26377;&#21608;&#26399;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#35270;&#35282;&#36716;&#31227;&#21040;&#39057;&#29575;&#22495;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24207;&#21015;&#25512;&#33616;&#31639;&#27861;&#8212;&#8212;&#39057;&#29575;&#22686;&#24378;&#30340;&#28151;&#21512;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#21363;FEARec&#12290;&#22312;&#36825;&#20010;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#19968;&#20010;&#26012;&#22369;&#32467;&#26500;&#23558;&#21407;&#22987;&#26102;&#38388;&#22495;&#33258;&#27880;&#24847;&#21147;&#25913;&#36827;&#21040;&#39057;&#29575;&#22495;&#20013;&#65292;&#20351;&#24471;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#26126;&#30830;&#22320;&#23398;&#20064;&#20302;&#39057;&#21644;&#39640;&#39057;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#33258;&#30456;&#20851;&#35774;&#35745;&#20102;&#31867;&#20284;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#25429;&#25417;&#29992;&#25143;&#34892;&#20026;&#30340;&#22266;&#26377;&#21608;&#26399;&#24615;&#12290;&#22312;&#22235;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#27604;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#34920;&#29616;&#26356;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;
The self-attention mechanism, which equips with a strong capability of modeling long-range dependencies, is one of the extensively used techniques in the sequential recommendation field. However, many recent studies represent that current self-attention based models are low-pass filters and are inadequate to capture high-frequency information. Furthermore, since the items in the user behaviors are intertwined with each other, these models are incomplete to distinguish the inherent periodicity obscured in the time domain. In this work, we shift the perspective to the frequency domain, and propose a novel Frequency Enhanced Hybrid Attention Network for Sequential Recommendation, namely FEARec. In this model, we firstly improve the original time domain self-attention in the frequency domain with a ramp structure to make both low-frequency and high-frequency information could be explicitly learned in our approach. Moreover, we additionally design a similar attention mechanism via auto-corr
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#25910;&#38598;&#29992;&#25143;&#23545;&#39033;&#30446;&#38598;&#20559;&#22909;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26500;&#24314;&#23545;&#35805;&#24335;&#25773;&#25918;&#21015;&#34920;&#26500;&#24314;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#33021;&#22815;&#36229;&#36234;&#21333;&#39033;&#20559;&#22909;&#65292;&#26356;&#21152;&#20840;&#38754;&#26377;&#25928;&#22320;&#20102;&#35299;&#29992;&#25143;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2303.06791</link><description>&lt;p&gt;
&#36229;&#36234;&#21333;&#39033;&#65306;&#20351;&#29992;&#23545;&#35805;&#24335;&#38899;&#20048;&#25773;&#25918;&#21015;&#34920;&#26500;&#24314;&#25968;&#25454;&#25506;&#32034;&#29992;&#25143;&#23545;&#39033;&#30446;&#38598;&#30340;&#20559;&#22909;
&lt;/p&gt;
&lt;p&gt;
Beyond Single Items: Exploring User Preferences in Item Sets with the Conversational Playlist Curation Dataset. (arXiv:2303.06791v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06791
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#25910;&#38598;&#29992;&#25143;&#23545;&#39033;&#30446;&#38598;&#20559;&#22909;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26500;&#24314;&#23545;&#35805;&#24335;&#25773;&#25918;&#21015;&#34920;&#26500;&#24314;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#33021;&#22815;&#36229;&#36234;&#21333;&#39033;&#20559;&#22909;&#65292;&#26356;&#21152;&#20840;&#38754;&#26377;&#25928;&#22320;&#20102;&#35299;&#29992;&#25143;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38899;&#20048;&#31561;&#28040;&#36153;&#39046;&#22495;&#65292;&#29992;&#25143;&#36890;&#24120;&#33021;&#22815;&#26356;&#26377;&#25928;&#22320;&#25552;&#20379;&#23545;&#19968;&#32452;&#39033;&#30446;&#65288;&#20363;&#22914;&#25773;&#25918;&#21015;&#34920;&#25110;&#24191;&#25773;&#65289;&#30340;&#20559;&#22909;&#32780;&#19981;&#26159;&#23545;&#21333;&#20010;&#39033;&#30446;&#65288;&#20363;&#22914;&#27468;&#26354;&#65289;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#26159;&#19968;&#20010;&#26410;&#34987;&#20805;&#20998;&#30740;&#31350;&#30340;&#39046;&#22495;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#25512;&#33616;&#31995;&#32479;&#20165;&#38480;&#20110;&#20102;&#35299;&#21333;&#20010;&#39033;&#30446;&#30340;&#20559;&#22909;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;&#65292;&#36890;&#36807;&#35266;&#23519;&#39033;&#30446;&#32423;&#21644;&#38598;&#21512;&#32423;&#21453;&#39304;&#65292;&#26377;&#25928;&#22320;&#22312;&#20250;&#35805;&#29615;&#22659;&#20013;&#25910;&#38598;&#20851;&#20110;&#39033;&#30446;&#38598;&#30340;&#30495;&#23454;&#20559;&#22909;&#20449;&#24687;&#12290;&#25105;&#20204;&#23558;&#36825;&#39033;&#20219;&#21153;&#31216;&#20026;&#23545;&#35805;&#24335;&#39033;&#30446;&#38598;&#21512;&#20316;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#38899;&#20048;&#25512;&#33616;&#65292;&#26500;&#24314;&#20102;&#23545;&#35805;&#24335;&#25773;&#25918;&#21015;&#34920;&#26500;&#24314;&#25968;&#25454;&#38598;&#65288;CPC&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Users in consumption domains, like music, are often able to more efficiently provide preferences over a set of items (e.g. a playlist or radio) than over single items (e.g. songs). Unfortunately, this is an underexplored area of research, with most existing recommendation systems limited to understanding preferences over single items. Curating an item set exponentiates the search space that recommender systems must consider (all subsets of items!): this motivates conversational approaches-where users explicitly state or refine their preferences and systems elicit preferences in natural language-as an efficient way to understand user needs. We call this task conversational item set curation and present a novel data collection methodology that efficiently collects realistic preferences about item sets in a conversational setting by observing both item-level and set-level feedback. We apply this methodology to music recommendation to build the Conversational Playlist Curation Dataset (CPC
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21453;&#20107;&#23454;&#22240;&#26524;&#25512;&#26029;&#26694;&#26550;&#30340;&#22522;&#20110;&#21327;&#21516;&#20250;&#35805;&#25512;&#33616;&#31995;&#32479;&#65288;COCO-SBRS&#65289;&#65292;&#20197;&#35299;&#20915;&#22806;&#37096;&#20250;&#35805;&#21407;&#22240;&#65288;OSCs&#65289;&#23548;&#33268;&#30340;&#28151;&#28102;&#22240;&#32032;&#21644;&#25968;&#25454;&#30456;&#20851;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2301.13364</link><description>&lt;p&gt;
&#19968;&#31181;&#21453;&#20107;&#23454;&#21327;&#21516;&#20250;&#35805;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
A Counterfactual Collaborative Session-based Recommender System. (arXiv:2301.13364v3 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13364
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21453;&#20107;&#23454;&#22240;&#26524;&#25512;&#26029;&#26694;&#26550;&#30340;&#22522;&#20110;&#21327;&#21516;&#20250;&#35805;&#25512;&#33616;&#31995;&#32479;&#65288;COCO-SBRS&#65289;&#65292;&#20197;&#35299;&#20915;&#22806;&#37096;&#20250;&#35805;&#21407;&#22240;&#65288;OSCs&#65289;&#23548;&#33268;&#30340;&#28151;&#28102;&#22240;&#32032;&#21644;&#25968;&#25454;&#30456;&#20851;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#22522;&#20110;&#20250;&#35805;&#30340;&#25512;&#33616;&#31995;&#32479;&#65288;SBRS&#65289;&#19987;&#27880;&#20110;&#20174;&#29992;&#25143;&#24403;&#21069;&#20250;&#35805;&#20013;&#35266;&#23519;&#21040;&#30340;&#39033;&#30446;&#20013;&#25552;&#21462;&#20449;&#24687;&#20197;&#39044;&#27979;&#19979;&#19968;&#20010;&#39033;&#30446;&#65292;&#24573;&#30053;&#20102;&#24433;&#21709;&#29992;&#25143;&#36873;&#25321;&#39033;&#30446;&#30340;&#20250;&#35805;&#22806;&#21407;&#22240;&#65288;&#31216;&#20026;&#8220;&#22806;&#37096;&#20250;&#35805;&#21407;&#22240;&#8221;&#65292;OSCs&#65289;&#12290; &#28982;&#32780;&#65292;&#36825;&#20123;&#21407;&#22240;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#24191;&#27867;&#23384;&#22312;&#65292;&#24456;&#23569;&#26377;&#30740;&#31350;&#25506;&#35752;&#23427;&#20204;&#22312;SBRS&#20013;&#30340;&#20316;&#29992;&#12290; &#22312;&#27492;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20174;&#22240;&#26524;&#25512;&#26029;&#30340;&#35282;&#24230;&#20998;&#26512;&#20102;SBRS&#20013;OSCs&#30340;&#22240;&#26524;&#20851;&#31995;&#21644;&#30456;&#20851;&#24615;&#12290; &#25105;&#20204;&#21457;&#29616;OSCs&#26412;&#36136;&#19978;&#26159;SBRS&#20013;&#30340;&#28151;&#28102;&#22240;&#32032;&#65292;&#36825;&#23548;&#33268;&#29992;&#20110;&#35757;&#32451;SBRS&#27169;&#22411;&#30340;&#25968;&#25454;&#20013;&#23384;&#22312;&#34394;&#20551;&#30340;&#30456;&#20851;&#24615;&#12290; &#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;SBRS&#26694;&#26550;&#65292;&#31216;&#20026;COCO-SBRS&#65288;COunterfactual COllaborative Session-Based Recommender Systems&#65289;&#65292;&#20197;&#23398;&#20064;SBRS&#20013;OSCs&#21644;&#29992;&#25143;-&#39033;&#30446;&#20132;&#20114;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290; COCO-SBRS&#39318;&#20808;&#37319;&#29992;&#33258;&#25105;&#30417;&#30563;&#26041;&#27861;&#36890;&#36807;&#35774;&#35745;&#27599;&#20010;&#29992;&#25143;&#21407;&#22240;&#30340;&#20266;&#26631;&#31614;&#26469;&#39044;&#35757;&#32451;&#25512;&#33616;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most session-based recommender systems (SBRSs) focus on extracting information from the observed items in the current session of a user to predict a next item, ignoring the causes outside the session (called outer-session causes, OSCs) that influence the user's selection of items. However, these causes widely exist in the real world, and few studies have investigated their role in SBRSs. In this work, we analyze the causalities and correlations of the OSCs in SBRSs from the perspective of causal inference. We find that the OSCs are essentially the confounders in SBRSs, which leads to spurious correlations in the data used to train SBRS models. To address this problem, we propose a novel SBRS framework named COCO-SBRS (COunterfactual COllaborative Session-Based Recommender Systems) to learn the causality between OSCs and user-item interactions in SBRSs. COCO-SBRS first adopts a self-supervised approach to pre-train a recommendation model by designing pseudo-labels of causes for each use
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#40784;&#30340;&#38899;&#20048;&#23383;&#24149;&#29983;&#25104;&#22120;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#26174;&#24335;&#23398;&#20064;&#38899;&#39057;&#21644;&#27468;&#35789;&#30340;&#23545;&#24212;&#20851;&#31995;&#65292;&#24182;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#23383;&#24149;&#65292;&#21462;&#24471;&#20102;&#20004;&#20010;&#38899;&#20048;&#23383;&#24149;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#26032;&#39046;&#20808;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2212.10901</link><description>&lt;p&gt;
ALCAP: &#22522;&#20110;&#23545;&#40784;&#30340;&#38899;&#20048;&#23383;&#24149;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
ALCAP: Alignment-Augmented Music Captioner. (arXiv:2212.10901v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10901
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#40784;&#30340;&#38899;&#20048;&#23383;&#24149;&#29983;&#25104;&#22120;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#26174;&#24335;&#23398;&#20064;&#38899;&#39057;&#21644;&#27468;&#35789;&#30340;&#23545;&#24212;&#20851;&#31995;&#65292;&#24182;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#23383;&#24149;&#65292;&#21462;&#24471;&#20102;&#20004;&#20010;&#38899;&#20048;&#23383;&#24149;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#26032;&#39046;&#20808;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#38899;&#20048;&#27969;&#23186;&#20307;&#24179;&#21488;&#29992;&#20110;&#38899;&#20048;&#25628;&#32034;&#21644;&#25512;&#33616;&#30340;&#26085;&#30410;&#26222;&#21450;&#65292;&#38656;&#35201;&#26032;&#26041;&#27861;&#26469;&#35299;&#37322;&#38899;&#20048;&#65292;&#21516;&#26102;&#32771;&#34385;&#27468;&#35789;&#21644;&#38899;&#39057;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#20808;&#21069;&#30340;&#30740;&#31350;&#20851;&#27880;&#20110;&#31934;&#32454;&#35843;&#25972;&#23558;&#38899;&#20048;&#26144;&#23556;&#21040;&#23383;&#24149;&#35760;&#21495;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#30340;&#21508;&#20010;&#32452;&#20214;&#65292;&#24573;&#30053;&#20102;&#38899;&#39057;&#21644;&#27468;&#35789;&#20043;&#38388;&#23545;&#24212;&#30340;&#28508;&#22312;&#30410;&#22788;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#26469;&#26174;&#24335;&#23398;&#20064;&#22810;&#27169;&#24577;&#23545;&#40784;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23398;&#20064;&#38899;&#39057;-&#27468;&#35789;&#30340;&#23545;&#24212;&#20851;&#31995;&#65292;&#20351;&#27169;&#22411;&#25351;&#23548;&#23398;&#20064;&#26356;&#22909;&#30340;&#36328;&#27169;&#24577;&#19968;&#33268;&#24615;&#65292;&#20174;&#32780;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#23383;&#24149;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#21644;&#32463;&#39564;&#32467;&#26524;&#65292;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#20248;&#21183;&#65292;&#24182;&#22312;&#20004;&#20010;&#38899;&#20048;&#23383;&#24149;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;&#26032;&#30340;&#29366;&#24577;-&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Growing popularity of streaming media platforms for music search and recommendations has led to a need for novel methods for interpreting music that take into account both lyrics and audio. However, many previous works focus on refining individual components of encoder-decoder architecture that maps music to caption tokens, ignoring the potential benefits of correspondence between audio and lyrics. In this paper, we propose to explicitly learn the multimodal alignment through contrastive learning. By learning audio-lyrics correspondence, the model is guided to learn better cross-modal consistency, thus generating high-quality captions. We provide both theoretical and empirical results demonstrating the advantage of the proposed method, and achieve new state-of-the-art on two music captioning datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#36827;&#34892;&#25512;&#29702;&#30340;&#21069;&#27839;&#30740;&#31350;&#32508;&#21512;&#35843;&#26597;&#12290;&#35752;&#35770;&#20102;&#26032;&#20852;&#25512;&#29702;&#33021;&#21147;&#20986;&#29616;&#30340;&#28508;&#22312;&#21407;&#22240;&#65292;&#24182;&#25552;&#20379;&#31995;&#32479;&#36164;&#28304;&#24110;&#21161;&#21021;&#23398;&#32773;&#12290;</title><link>http://arxiv.org/abs/2212.09597</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#36827;&#34892;&#25512;&#29702;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Reasoning with Language Model Prompting: A Survey. (arXiv:2212.09597v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09597
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#36827;&#34892;&#25512;&#29702;&#30340;&#21069;&#27839;&#30740;&#31350;&#32508;&#21512;&#35843;&#26597;&#12290;&#35752;&#35770;&#20102;&#26032;&#20852;&#25512;&#29702;&#33021;&#21147;&#20986;&#29616;&#30340;&#28508;&#22312;&#21407;&#22240;&#65292;&#24182;&#25552;&#20379;&#31995;&#32479;&#36164;&#28304;&#24110;&#21161;&#21021;&#23398;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#29702;&#20316;&#20026;&#22797;&#26434;&#38382;&#39064;&#35299;&#20915;&#30340;&#37325;&#35201;&#33021;&#21147;&#65292;&#21487;&#20197;&#20026;&#21307;&#30103;&#35786;&#26029;&#12289;&#35848;&#21028;&#31561;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#25552;&#20379;&#21518;&#31471;&#25903;&#25345;&#12290;&#26412;&#25991;&#23545;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#36827;&#34892;&#25512;&#29702;&#30340;&#21069;&#27839;&#30740;&#31350;&#36827;&#34892;&#20102;&#32508;&#21512;&#35843;&#26597;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#30740;&#31350;&#25104;&#26524;&#30340;&#27604;&#36739;&#21644;&#24635;&#32467;&#65292;&#24182;&#25552;&#20379;&#20102;&#31995;&#32479;&#36164;&#28304;&#20197;&#24110;&#21161;&#21021;&#23398;&#32773;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#26032;&#20852;&#25512;&#29702;&#33021;&#21147;&#20986;&#29616;&#30340;&#28508;&#22312;&#21407;&#22240;&#65292;&#24182;&#31361;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#36164;&#28304;&#21487;&#22312; https://github.com/zjunlp/Prompt4ReasoningPapers &#19978;&#33719;&#21462;&#65288;&#23450;&#26399;&#26356;&#26032;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reasoning, as an essential ability for complex problem-solving, can provide back-end support for various real-world applications, such as medical diagnosis, negotiation, etc. This paper provides a comprehensive survey of cutting-edge research on reasoning with language model prompting. We introduce research works with comparisons and summaries and provide systematic resources to help beginners. We also discuss the potential reasons for emerging such reasoning abilities and highlight future research directions. Resources are available at https://github.com/zjunlp/Prompt4ReasoningPapers (updated periodically).
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#21457;&#29616;&#65292;&#29992;&#25143;&#21382;&#21490;&#35821;&#35328;&#24314;&#27169;&#21487;&#20197;&#22312;&#19981;&#21516;&#25512;&#33616;&#20219;&#21153;&#20013;&#21462;&#24471;&#20248;&#24322;&#32467;&#26524;&#65292;&#24182;&#19988;&#21033;&#29992;&#20219;&#21153;&#26080;&#20851;&#30340;&#29992;&#25143;&#21382;&#21490;&#36824;&#21487;&#20197;&#25552;&#20379;&#26174;&#33879;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#24191;&#27867;&#30340;&#29616;&#23454;&#19990;&#30028;&#36801;&#31227;&#23398;&#20064;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2212.03760</link><description>&lt;p&gt;
&#35821;&#35328;&#24314;&#27169;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#65306;&#20016;&#23500;&#20219;&#21153;&#29305;&#23450;&#21644;&#20219;&#21153;&#26080;&#20851;&#30340;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Pivotal Role of Language Modeling in Recommender Systems: Enriching Task-specific and Task-agnostic Representation Learning. (arXiv:2212.03760v3 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.03760
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#21457;&#29616;&#65292;&#29992;&#25143;&#21382;&#21490;&#35821;&#35328;&#24314;&#27169;&#21487;&#20197;&#22312;&#19981;&#21516;&#25512;&#33616;&#20219;&#21153;&#20013;&#21462;&#24471;&#20248;&#24322;&#32467;&#26524;&#65292;&#24182;&#19988;&#21033;&#29992;&#20219;&#21153;&#26080;&#20851;&#30340;&#29992;&#25143;&#21382;&#21490;&#36824;&#21487;&#20197;&#25552;&#20379;&#26174;&#33879;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#24191;&#27867;&#30340;&#29616;&#23454;&#19990;&#30028;&#36801;&#31227;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#21033;&#29992;&#26469;&#33258;&#21508;&#31181;&#24212;&#29992;&#31243;&#24207;&#30340;&#29992;&#25143;&#34892;&#20026;&#25968;&#25454;&#30340;&#32479;&#19968;&#29992;&#25143;&#24314;&#27169;&#26694;&#26550;&#12290;&#20854;&#20013;&#35768;&#22810;&#21463;&#30410;&#20110;&#23558;&#29992;&#25143;&#34892;&#20026;&#24207;&#21015;&#20316;&#20026;&#32431;&#25991;&#26412;&#20351;&#29992;&#65292;&#20195;&#34920;&#30528;&#20219;&#20309;&#39046;&#22495;&#25110;&#31995;&#32479;&#20013;&#30340;&#20016;&#23500;&#20449;&#24687;&#32780;&#19981;&#22833;&#36890;&#29992;&#24615;&#12290;&#22240;&#27492;&#65292;&#19968;&#20010;&#38382;&#39064;&#20135;&#29983;&#20102;&#65306;&#29992;&#25143;&#21382;&#21490;&#35821;&#35328;&#24314;&#27169;&#33021;&#21542;&#24110;&#21161;&#25913;&#21892;&#25512;&#33616;&#31995;&#32479;&#65311;&#34429;&#28982;&#35821;&#35328;&#24314;&#27169;&#30340;&#22810;&#21151;&#33021;&#24615;&#24050;&#22312;&#35768;&#22810;&#39046;&#22495;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#20854;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#20173;&#26410;&#28145;&#20837;&#25506;&#35752;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#30452;&#25509;&#24212;&#29992;&#20110;&#20219;&#21153;&#29305;&#23450;&#29992;&#25143;&#21382;&#21490;&#30340;&#35821;&#35328;&#24314;&#27169;&#22312;&#19981;&#21516;&#30340;&#25512;&#33616;&#20219;&#21153;&#19978;&#21487;&#20197;&#21462;&#24471;&#20248;&#24322;&#30340;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#21033;&#29992;&#20219;&#21153;&#26080;&#20851;&#30340;&#29992;&#25143;&#21382;&#21490;&#36824;&#21487;&#20197;&#25552;&#20379;&#26174;&#33879;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20026;&#24191;&#27867;&#30340;&#29616;&#23454;&#19990;&#30028;&#25512;&#33616;&#31995;&#32479;&#25552;&#20379;&#26377;&#21069;&#36884;&#30340;&#36801;&#31227;&#23398;&#20064;&#33021;&#21147;&#65292;&#29978;&#33267;&#22312;&#26410;&#30693;&#22495;&#21644;&#26381;&#21153;&#19978;&#20063;&#21487;&#20197;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies have proposed unified user modeling frameworks that leverage user behavior data from various applications. Many of them benefit from utilizing users' behavior sequences as plain texts, representing rich information in any domain or system without losing generality. Hence, a question arises: Can language modeling for user history corpus help improve recommender systems? While its versatile usability has been widely investigated in many domains, its applications to recommender systems still remain underexplored. We show that language modeling applied directly to task-specific user histories achieves excellent results on diverse recommendation tasks. Also, leveraging additional task-agnostic user histories delivers significant performance benefits. We further demonstrate that our approach can provide promising transfer learning capabilities for a broad spectrum of real-world recommender systems, even on unseen domains and services.
&lt;/p&gt;</description></item><item><title>CPS-MEBR&#26694;&#26550;&#21033;&#29992;&#29992;&#25143;&#30340;&#28857;&#20987;&#25968;&#25454;&#35757;&#32451;&#19968;&#20010;&#25688;&#35201;&#27169;&#22411;&#65292;&#25552;&#21462;&#37027;&#20123;&#34987;&#29992;&#25143;&#39057;&#32321;&#28857;&#20987;&#30340;&#21477;&#23376;&#65292;&#24182;&#20026;&#32593;&#39029;&#29983;&#25104;&#22810;&#20010;&#23884;&#20837;&#20197;&#21305;&#37197;&#19981;&#21516;&#30340;&#28508;&#22312;&#26597;&#35810;&#12290;</title><link>http://arxiv.org/abs/2210.09787</link><description>&lt;p&gt;
CPS-MEBR: &#22522;&#20110;&#28857;&#20987;&#21453;&#39304;&#30340;&#22810;&#23884;&#20837;&#24335;&#26816;&#32034;&#30340;&#32593;&#39029;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
CPS-MEBR: Click Feedback-Aware Web Page Summarization for Multi-Embedding-Based Retrieval. (arXiv:2210.09787v3 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.09787
&lt;/p&gt;
&lt;p&gt;
CPS-MEBR&#26694;&#26550;&#21033;&#29992;&#29992;&#25143;&#30340;&#28857;&#20987;&#25968;&#25454;&#35757;&#32451;&#19968;&#20010;&#25688;&#35201;&#27169;&#22411;&#65292;&#25552;&#21462;&#37027;&#20123;&#34987;&#29992;&#25143;&#39057;&#32321;&#28857;&#20987;&#30340;&#21477;&#23376;&#65292;&#24182;&#20026;&#32593;&#39029;&#29983;&#25104;&#22810;&#20010;&#23884;&#20837;&#20197;&#21305;&#37197;&#19981;&#21516;&#30340;&#28508;&#22312;&#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23884;&#20837;&#24335;&#26816;&#32034;&#26159;&#23558;&#23884;&#20837;&#29992;&#20110;&#34920;&#31034;&#26597;&#35810;&#21644;&#25991;&#26723;&#65292;&#28982;&#21518;&#23558;&#26816;&#32034;&#38382;&#39064;&#36716;&#25442;&#20026;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#26368;&#36817;&#37051;&#25628;&#32034;&#38382;&#39064;&#30340;&#25216;&#26415;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28857;&#20987;&#21453;&#39304;&#30340;&#36866;&#29992;&#20110;&#22810;&#23884;&#20837;&#24335;&#26816;&#32034;&#65288;CPS-MEBR&#65289;&#30340;&#32593;&#39029;&#25688;&#35201;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#22312;&#30495;&#23454;&#32593;&#32476;&#25628;&#32034;&#22330;&#26223;&#20013;&#38590;&#20197;&#23558;&#38271;&#19988;&#32467;&#26500;&#22797;&#26434;&#30340;&#32593;&#39029;&#30340;&#25152;&#26377;&#20449;&#24687;&#34920;&#31034;&#20026;&#21333;&#20010;&#23884;&#20837;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Embedding-based retrieval (EBR) is a technique to use embeddings to represent query and document, and then convert the retrieval problem into a nearest neighbor search problem in the embedding space. Some previous works have mainly focused on representing the web page with a single embedding, but in real web search scenarios, it is difficult to represent all the information of a long and complex structured web page as a single embedding. To address this issue, we design a click feedback-aware web page summarization for multi-embedding-based retrieval (CPS-MEBR) framework which is able to generate multiple embeddings for web pages to match different potential queries. Specifically, we use the click data of users in search logs to train a summary model to extract those sentences in web pages that are frequently clicked by users, which are more likely to answer those potential queries. Meanwhile, we introduce sentence-level semantic interaction to design a multi-embedding-based retrieval 
&lt;/p&gt;</description></item><item><title>Sapling Similarity&#26159;&#19968;&#31181;&#35745;&#31639;&#30456;&#20284;&#24615;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20801;&#35768;&#36127;&#20540;&#65292;&#22522;&#20110;&#29992;&#25143;&#30340;&#30456;&#20114;&#36830;&#25509;&#20449;&#24687;&#26469;&#20272;&#35745;&#27010;&#29575;&#12290;&#36825;&#31181;&#26041;&#27861;&#27604;&#20256;&#32479;&#22522;&#20110;&#20849;&#21516;&#37051;&#23621;&#30340;&#26041;&#27861;&#26356;&#21152;&#21487;&#35299;&#37322;&#21644;&#39640;&#25928;&#12290;</title><link>http://arxiv.org/abs/2210.07039</link><description>&lt;p&gt;
Sapling Similarity: &#19968;&#31181;&#39640;&#24615;&#33021;&#21487;&#35299;&#37322;&#30340;&#22522;&#20110;&#20869;&#23384;&#30340;&#25512;&#33616;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
Sapling Similarity: a performing and interpretable memory-based tool for recommendation. (arXiv:2210.07039v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.07039
&lt;/p&gt;
&lt;p&gt;
Sapling Similarity&#26159;&#19968;&#31181;&#35745;&#31639;&#30456;&#20284;&#24615;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20801;&#35768;&#36127;&#20540;&#65292;&#22522;&#20110;&#29992;&#25143;&#30340;&#30456;&#20114;&#36830;&#25509;&#20449;&#24687;&#26469;&#20272;&#35745;&#27010;&#29575;&#12290;&#36825;&#31181;&#26041;&#27861;&#27604;&#20256;&#32479;&#22522;&#20110;&#20849;&#21516;&#37051;&#23621;&#30340;&#26041;&#27861;&#26356;&#21152;&#21487;&#35299;&#37322;&#21644;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#20108;&#20998;&#32593;&#32476;&#25551;&#36848;&#20102;&#19968;&#20010;&#36793;&#32536;&#34920;&#31034;&#29992;&#25143;&#21644;&#29289;&#21697;&#20043;&#38388;&#30340;&#20851;&#31995;&#30340;&#31995;&#32479;&#12290;&#34913;&#37327;&#29992;&#25143;&#25110;&#29289;&#21697;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#26159;&#22522;&#20110;&#20869;&#23384;&#30340;&#21327;&#21516;&#36807;&#28388;&#30340;&#22522;&#30784;&#65292;&#36825;&#26159;&#19968;&#31181;&#24191;&#27867;&#29992;&#20110;&#26500;&#24314;&#25512;&#33616;&#31995;&#32479;&#30340;&#26041;&#27861;&#65292;&#30446;&#30340;&#26159;&#21521;&#29992;&#25143;&#25512;&#33616;&#29289;&#21697;&#12290;&#24403;&#32593;&#32476;&#30340;&#36793;&#32536;&#26159;&#38750;&#21152;&#26435;&#30340;&#26102;&#65292;&#27969;&#34892;&#30340;&#22522;&#20110;&#20849;&#21516;&#37051;&#23621;&#30340;&#26041;&#27861;&#21482;&#20801;&#35768;&#27491;&#30456;&#20284;&#24230;&#20540;&#65292;&#24573;&#30053;&#20102;&#20004;&#20010;&#29992;&#25143;(&#25110;&#20004;&#20010;&#29289;&#21697;)&#38750;&#24120;&#19981;&#30456;&#20284;&#30340;&#21487;&#33021;&#24615;&#21644;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#22312;&#27169;&#22411;&#20026;&#22522;&#30784;&#30340;(&#26426;&#22120;&#23398;&#20064;)&#26041;&#27861;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#65292;&#23613;&#31649;&#25552;&#20379;&#26356;&#39640;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#21463;&#20915;&#31574;&#26641;&#30340;&#36816;&#34892;&#26041;&#24335;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#30456;&#20284;&#24615;&#30340;&#26041;&#27861;&#65292;&#20063;&#20801;&#35768;&#36127;&#20540;&#65292;&#31216;&#20026; Sapling Similarity&#12290;&#20851;&#38190;&#24605;&#24819;&#26159;&#30475;&#30475;&#36830;&#25509;&#29992;&#25143;&#21644;&#39033;&#30446;&#30340;&#20449;&#24687;&#22914;&#20309;&#24433;&#21709;&#25105;&#20204;&#23545;&#21478;&#19968;&#20010;&#29992;&#25143;&#36830;&#25509;&#21040;&#30456;&#21516;&#39033;&#30446;&#30340;&#27010;&#29575;&#30340;&#20808;&#21069;&#20272;&#35745;&#65306;&#22914;&#26524;&#8230;&#8230;
&lt;/p&gt;
&lt;p&gt;
Many bipartite networks describe systems where an edge represents a relation between a user and an item. Measuring the similarity between either users or items is the basis of memory-based collaborative filtering, a widely used method to build a recommender system with the purpose of proposing items to users. When the edges of the network are unweighted, the popular common neighbors-based approaches, allowing only positive similarity values, neglect the possibility and the effect of two users (or two items) being very dissimilar. Moreover, they underperform with respect to model-based (machine learning) approaches, although providing higher interpretability. Inspired by the functioning of Decision Trees, we propose a method to compute similarity that allows also negative values, the Sapling Similarity. The key idea is to look at how the information that a user is connected to an item influences our prior estimation of the probability that another user is connected to the same item: if 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#21512;&#35780;&#36848;&#20102;&#28145;&#24230;&#22270;&#23398;&#20064;&#65288;DGL&#65289;&#30340;&#26368;&#26032;&#21487;&#38752;&#24615;&#36827;&#23637;&#65292;&#20854;&#20013;&#28085;&#30422;&#20102;&#20869;&#22312;&#22122;&#22768;&#21644;&#20998;&#24067;&#20559;&#31227;&#31561;&#26041;&#38754;&#30340;&#30456;&#20851;&#20869;&#23481;&#65292;&#21516;&#26102;&#25351;&#20986;&#26410;&#26469;&#30740;&#31350;&#38656;&#35201;&#20851;&#27880;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2202.07114</link><description>&lt;p&gt;
&#28145;&#24230;&#22270;&#23398;&#20064;&#30340;&#21487;&#38752;&#24615;&#36817;&#26399;&#36827;&#23637;: &#20869;&#22312;&#22122;&#22768;&#12289;&#20998;&#24067;&#20559;&#31227;&#21644;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Recent Advances in Reliable Deep Graph Learning: Inherent Noise, Distribution Shift, and Adversarial Attack. (arXiv:2202.07114v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.07114
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#21512;&#35780;&#36848;&#20102;&#28145;&#24230;&#22270;&#23398;&#20064;&#65288;DGL&#65289;&#30340;&#26368;&#26032;&#21487;&#38752;&#24615;&#36827;&#23637;&#65292;&#20854;&#20013;&#28085;&#30422;&#20102;&#20869;&#22312;&#22122;&#22768;&#21644;&#20998;&#24067;&#20559;&#31227;&#31561;&#26041;&#38754;&#30340;&#30456;&#20851;&#20869;&#23481;&#65292;&#21516;&#26102;&#25351;&#20986;&#26410;&#26469;&#30740;&#31350;&#38656;&#35201;&#20851;&#27880;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#22270;&#23398;&#20064; (DGL) &#22312;&#37329;&#34701;&#12289;&#30005;&#23376;&#21830;&#21153;&#12289;&#33647;&#29289;&#21644;&#20808;&#36827;&#26448;&#26009;&#21457;&#29616;&#31561;&#21830;&#19994;&#21644;&#31185;&#23398;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#23558; DGL &#24212;&#29992;&#20110;&#23454;&#38469;&#24212;&#29992;&#38754;&#20020;&#19968;&#31995;&#21015;&#21487;&#38752;&#24615;&#23041;&#32961;&#65292;&#21253;&#25324;&#20869;&#22312;&#22122;&#22768;&#12289;&#20998;&#24067;&#20559;&#31227;&#21644;&#23545;&#25239;&#25915;&#20987;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;&#36817;&#26399;&#38024;&#23545;&#19978;&#36848;&#23041;&#32961;&#25913;&#36827; DGL &#31639;&#27861;&#21487;&#38752;&#24615;&#30340;&#20840;&#38754;&#36827;&#23637;&#12290;&#19982;&#20043;&#21069;&#20027;&#35201;&#38598;&#20013;&#22312;&#23545;&#25239;&#25915;&#20987;&#21644;&#38450;&#24481;&#30340;&#30456;&#20851;&#32508;&#36848;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#32508;&#36848;&#28085;&#30422;&#20102;&#26356;&#22810;&#20851;&#20110; DGL &#21487;&#38752;&#24615;&#30456;&#20851;&#30340;&#26041;&#38754;&#65292;&#21363;&#20869;&#22312;&#22122;&#22768;&#21644;&#20998;&#24067;&#20559;&#31227;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#20197;&#19978;&#26041;&#38754;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#24378;&#35843;&#20102;&#26410;&#26469;&#30740;&#31350;&#20013;&#38656;&#35201;&#25506;&#32034;&#30340;&#19968;&#20123;&#37325;&#35201;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep graph learning (DGL) has achieved remarkable progress in both business and scientific areas ranging from finance and e-commerce to drug and advanced material discovery. Despite the progress, applying DGL to real-world applications faces a series of reliability threats including inherent noise, distribution shift, and adversarial attacks. This survey aims to provide a comprehensive review of recent advances for improving the reliability of DGL algorithms against the above threats. In contrast to prior related surveys which mainly focus on adversarial attacks and defense, our survey covers more reliability-related aspects of DGL, i.e., inherent noise and distribution shift. Additionally, we discuss the relationships among above aspects and highlight some important issues to be explored in future research.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20799;&#31461;&#21644;&#26426;&#22120;&#20154;&#20132;&#20114;&#29615;&#22659;&#19979;&#30340;&#20010;&#24615;&#21270;&#22270;&#20070;&#25512;&#33616;&#31995;&#32479;&#65292;&#22312;&#25628;&#32034;&#31639;&#27861;&#12289;&#29992;&#25143;&#20852;&#36259;&#39044;&#27979;&#21644;&#21516;&#20041;&#35789;&#20851;&#32852;&#26041;&#38754;&#36827;&#34892;&#20102;&#21019;&#26032;&#65292;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#36866;&#29992;&#20110;&#23884;&#20837;&#24335;&#28040;&#36153;&#35774;&#22791;&#12290;</title><link>http://arxiv.org/abs/1710.00310</link><description>&lt;p&gt;
&#38754;&#21521;&#20799;&#31461;&#22270;&#20070;&#25512;&#33616;&#30340;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#21450;&#20854;&#19982;&#23454;&#26102;&#20132;&#20114;&#26426;&#22120;&#20154;&#38598;&#25104;
&lt;/p&gt;
&lt;p&gt;
Personalized Recommender System for Children's Book Recommendation with A Realtime Interactive Robot. (arXiv:1710.00310v3 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1710.00310
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20799;&#31461;&#21644;&#26426;&#22120;&#20154;&#20132;&#20114;&#29615;&#22659;&#19979;&#30340;&#20010;&#24615;&#21270;&#22270;&#20070;&#25512;&#33616;&#31995;&#32479;&#65292;&#22312;&#25628;&#32034;&#31639;&#27861;&#12289;&#29992;&#25143;&#20852;&#36259;&#39044;&#27979;&#21644;&#21516;&#20041;&#35789;&#20851;&#32852;&#26041;&#38754;&#36827;&#34892;&#20102;&#21019;&#26032;&#65292;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#36866;&#29992;&#20110;&#23884;&#20837;&#24335;&#28040;&#36153;&#35774;&#22791;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20799;&#31461;&#21644;&#26426;&#22120;&#20154;&#20132;&#20114;&#29615;&#22659;&#20013;&#30340;&#20010;&#24615;&#21270;&#22270;&#20070;&#25512;&#33616;&#31995;&#32479;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25991;&#26412;&#25628;&#32034;&#31639;&#27861;&#65292;&#20351;&#29992;&#21453;&#21521;&#36807;&#28388;&#26426;&#21046;&#26469;&#25552;&#39640;&#25928;&#29575;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#32593;&#32476;&#21644;&#26032;&#22411;&#21453;&#39304;&#26426;&#21046;&#30340;&#29992;&#25143;&#20852;&#36259;&#39044;&#27979;&#26041;&#27861;&#12290;&#26681;&#25454;&#20799;&#31461;&#27169;&#31946;&#30340;&#35821;&#35328;&#36755;&#20837;&#65292;&#35813;&#26041;&#27861;&#32473;&#20986;&#20102;&#39044;&#27979;&#30340;&#20852;&#36259;&#12290;&#31532;&#19977;&#65292;&#22522;&#20110;&#35789;&#21521;&#37327;&#21270;&#65292;&#25552;&#20986;&#20102;&#39046;&#22495;&#29305;&#23450;&#30340;&#21516;&#20041;&#35789;&#20851;&#32852;&#65292;&#20197;&#25552;&#39640;&#29702;&#35299;&#29992;&#25143;&#24847;&#22270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#25512;&#33616;&#31995;&#32479;&#24615;&#33021;&#24471;&#21040;&#20102;&#25913;&#36827;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#26377;&#38480;&#30340;&#35745;&#31639;&#36164;&#28304;&#19979;&#36816;&#34892;&#22312;&#23884;&#20837;&#24335;&#28040;&#36153;&#35774;&#22791;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we study the personalized book recommender system in a child-robot interactive environment. Firstly, we propose a novel text search algorithm using an inverse filtering mechanism that improves the efficiency. Secondly, we propose a user interest prediction method based on the Bayesian network and a novel feedback mechanism. According to children's fuzzy language input, the proposed method gives the predicted interests. Thirdly, the domain specific synonym association is proposed based on word vectorization, in order to improve the understanding of user intention. Experimental results show that the proposed recommender system has an improved performance and it can operate on embedded consumer devices with limited computational resources.
&lt;/p&gt;</description></item></channel></rss>