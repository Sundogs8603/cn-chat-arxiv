<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;GPT-3.5&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#20013;&#65292;&#23454;&#29616;&#22312;&#22235;&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;&#26032;&#30340;&#26368;&#20248;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#25351;&#23548;&#35828;&#26126;&#21644;&#32422;&#26463;&#27169;&#24335;&#19979;&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.01555</link><description>&lt;p&gt;
&#22914;&#20309;&#21457;&#25381;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#20013;&#30340;&#33021;&#21147;&#65311;
&lt;/p&gt;
&lt;p&gt;
How to Unleash the Power of Large Language Models for Few-shot Relation Extraction?. (arXiv:2305.01555v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01555
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;GPT-3.5&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#20013;&#65292;&#23454;&#29616;&#22312;&#22235;&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;&#26032;&#30340;&#26368;&#20248;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#25351;&#23548;&#35828;&#26126;&#21644;&#32422;&#26463;&#27169;&#24335;&#19979;&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#25193;&#23637;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#24191;&#27867;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65292;&#20294;&#26159;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#36824;&#27809;&#26377;&#24471;&#21040;&#20840;&#38754;&#25506;&#32034;&#12290;&#26412;&#25991;&#36890;&#36807;&#35814;&#32454;&#23454;&#39564;&#65292;&#30740;&#31350;&#20102;&#20351;&#29992;GPT-3.5&#36827;&#34892;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#30340;&#22522;&#26412;&#26041;&#27861;&#8212;&#8212;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#25968;&#25454;&#29983;&#25104;&#12290;&#20026;&#20102;&#22686;&#24378;&#23569;&#26679;&#26412;&#24615;&#33021;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#25351;&#23548;&#35828;&#26126;&#21644;&#32422;&#26463;&#27169;&#24335;&#19979;&#30340;&#25968;&#25454;&#29983;&#25104;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#23454;&#29616;&#19982;&#20197;&#21069;&#30340;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#32780;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#25454;&#29983;&#25104;&#21487;&#20197;&#25512;&#21160;&#20197;&#21069;&#30340;&#35299;&#20915;&#26041;&#26696;&#20197;&#22312;&#22235;&#20010;&#24191;&#27867;&#30740;&#31350;&#30340;&#20851;&#31995;&#25277;&#21462;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#26032;&#30340;&#26368;&#20808;&#36827;&#30340;&#23569;&#26679;&#26412;&#32467;&#26524;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#24037;&#20316;&#21487;&#20197;&#28608;&#21457;&#26410;&#26469;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#20013;&#30340;&#33021;&#21147;&#30340;&#30740;&#31350;&#12290;&#20195;&#30721;&#21487;&#20197;&#22312; \url{https://github.com/zjunlp/DeepKE/tree/main/example/llm} &#20013;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scaling language models have revolutionized widespread NLP tasks, yet little comprehensively explored few-shot relation extraction with large language models. In this paper, we investigate principal methodologies, in-context learning and data generation, for few-shot relation extraction via GPT-3.5 through exhaustive experiments. To enhance few-shot performance, we further propose task-related instructions and schema-constrained data generation. We observe that in-context learning can achieve performance on par with previous prompt learning approaches, and data generation with the large language model can boost previous solutions to obtain new state-of-the-art few-shot results on four widely-studied relation extraction datasets. We hope our work can inspire future research for the capabilities of large language models in few-shot relation extraction. Code is available in \url{https://github.com/zjunlp/DeepKE/tree/main/example/llm.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21457;&#29616;&#23478;&#24237;&#25104;&#21592;&#28385;&#24847;&#24230;&#19982;&#23478;&#26063;&#26641;&#22270;&#65292;&#23478;&#24237;&#35268;&#27169;&#65292;&#23401;&#23376;&#26159;&#21542;&#20026;&#21516;&#29238;&#27597;&#65292;&#20197;&#21450;&#23478;&#24237;&#25910;&#20837;&#26377;&#20851;&#12290;</title><link>http://arxiv.org/abs/2305.01552</link><description>&lt;p&gt;
&#23478;&#26063;&#26641;&#22270;&#19982;&#23478;&#24237;&#25104;&#21592;&#28385;&#24847;&#24230;&#20043;&#38388;&#30340;&#20851;&#31995;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
The Family Tree Graph as a Predictor of the Family Members' Satisfaction with One Another. (arXiv:2305.01552v1 [physics.soc-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01552
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21457;&#29616;&#23478;&#24237;&#25104;&#21592;&#28385;&#24847;&#24230;&#19982;&#23478;&#26063;&#26641;&#22270;&#65292;&#23478;&#24237;&#35268;&#27169;&#65292;&#23401;&#23376;&#26159;&#21542;&#20026;&#21516;&#29238;&#27597;&#65292;&#20197;&#21450;&#23478;&#24237;&#25910;&#20837;&#26377;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#20154;&#23545;&#26680;&#24515;&#23478;&#24237;&#21644;&#25193;&#23637;&#23478;&#24237;&#30340;&#28385;&#24847;&#24230;&#22312;&#19968;&#20010;&#20154;&#30340;&#26085;&#24120;&#29983;&#27963;&#20013;&#25198;&#28436;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#35282;&#33394;&#12290;&#22240;&#27492;&#65292;&#26356;&#22909;&#22320;&#20102;&#35299;&#20915;&#23450;&#19968;&#20010;&#20154;&#23545;&#23478;&#24237;&#28385;&#24847;&#24230;&#30340;&#29305;&#24449;&#21487;&#20197;&#20026;&#26356;&#22909;&#30340;&#31038;&#20250;&#25919;&#31574;&#35774;&#35745;&#25171;&#24320;&#22823;&#38376;&#12290;&#20026;&#27492;&#65292;&#35813;&#30740;&#31350;&#32771;&#23519;&#20102;&#23478;&#26063;&#26641;&#22270;&#19982;&#23478;&#24237;&#25104;&#21592;&#23545;&#26680;&#24515;&#23478;&#24237;&#21644;&#25193;&#23637;&#23478;&#24237;&#30340;&#28385;&#24847;&#24230;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;&#26469;&#33258;486&#20010;&#23478;&#24237;&#30340;&#25968;&#25454;&#65292;&#20854;&#20013;&#21253;&#25324;&#23478;&#26063;&#26641;&#22270;&#21644;&#23478;&#24237;&#25104;&#21592;&#20043;&#38388;&#30340;&#28385;&#24847;&#24230;&#12290;&#25105;&#20204;&#24471;&#21040;&#20102;&#19968;&#20010;&#33021;&#22815;&#35299;&#37322;75%&#23478;&#24237;&#25104;&#21592;&#23545;&#24444;&#27492;&#28385;&#24847;&#24230;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#21457;&#29616;&#20102;&#35753;&#23478;&#24237;&#25104;&#21592;&#26356;&#21152;&#28385;&#24847;&#30340;&#19977;&#20010;&#25351;&#26631;&#12290;&#39318;&#20808;&#65292;&#24179;&#22343;&#32780;&#35328;&#65292;&#36739;&#22823;&#30340;&#23478;&#24237;&#26377;&#26356;&#28385;&#24847;&#30340;&#25104;&#21592;&#12290;&#27492;&#22806;&#65292;&#27809;&#26377;&#32487;&#20804;&#24351;&#22992;&#22969;&#30340;&#21516;&#29238;&#27597;&#23478;&#24237;-&#21363;&#25104;&#24180;&#23376;&#22899;&#24050;&#32463;&#25104;&#23478;&#30340;&#23478;&#24237;&#65292;&#22312;&#20804;&#24351;&#22992;&#22969;&#21644;&#29238;&#27597;&#26041;&#38754;&#20063;&#26356;&#28385;&#24847;&#12290;&#26368;&#21518;&#65292;&#23478;&#24237;&#25104;&#21592;&#30340;&#24179;&#22343;&#28385;&#24847;&#24230;&#21644;&#23478;&#24237;&#30340;&#25910;&#20837;&#27700;&#24179;&#24687;&#24687;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
Individuals' satisfaction with their nuclear and extended family plays a critical role in individuals everyday life. Thus, a better understanding of the features that determine one's satisfaction with her family can open the door to the design of better sociological policies. To this end, this study examines the relationship between the family tree graph and family members' satisfaction with their nuclear and extended family. We collected data from 486 families which included a family tree graph and family members' satisfaction with each other. We obtain a model that is able to explain 75\% of the family members' satisfaction with one another. We found three indicators for more satisfied families. First, larger families, on average, have more satisfied members. Moreover, families with kids from the same parents - i.e., without step-siblings also express more satisfaction from both their siblings and parents when the children are already adults. Lastly, the average satisfaction of the f
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986; EXPLAIGNN &#26041;&#27861;&#65292;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#26500;&#24314;&#24322;&#26500;&#22270;&#65292;&#20174;&#22810;&#20010;&#25968;&#25454;&#28304;&#20013;&#25552;&#21462;&#20449;&#24687;&#65292;&#36798;&#21040;&#26356;&#20840;&#38754;&#20934;&#30830;&#30340;&#38382;&#31572;&#25928;&#26524;&#65292;&#24182;&#25552;&#20379;&#20102;&#29992;&#25143;&#21487;&#29702;&#35299;&#30340;&#31572;&#26696;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2305.01548</link><description>&lt;p&gt;
&#36845;&#20195;&#22270;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#24322;&#26500;&#25968;&#25454;&#21487;&#35299;&#37322;&#23545;&#35805;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Explainable Conversational Question Answering over Heterogeneous Sources via Iterative Graph Neural Networks. (arXiv:2305.01548v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01548
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986; EXPLAIGNN &#26041;&#27861;&#65292;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#26500;&#24314;&#24322;&#26500;&#22270;&#65292;&#20174;&#22810;&#20010;&#25968;&#25454;&#28304;&#20013;&#25552;&#21462;&#20449;&#24687;&#65292;&#36798;&#21040;&#26356;&#20840;&#38754;&#20934;&#30830;&#30340;&#38382;&#31572;&#25928;&#26524;&#65292;&#24182;&#25552;&#20379;&#20102;&#29992;&#25143;&#21487;&#29702;&#35299;&#30340;&#31572;&#26696;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23545;&#35805;&#38382;&#31572;&#20013;&#65292;&#29992;&#25143;&#36890;&#36807;&#19968;&#31995;&#21015;&#19978;&#19979;&#25991;&#19981;&#23436;&#25972;&#30340;&#34920;&#36798;&#26469;&#34920;&#36798;&#20182;&#20204;&#30340;&#20449;&#24687;&#38656;&#27714;&#12290;&#20856;&#22411;&#30340; ConvQA &#26041;&#27861;&#20381;&#36182;&#20110;&#21333;&#19968;&#25968;&#25454;&#28304;(&#30693;&#35782;&#24211;&#12289;&#25991;&#26412;&#35821;&#26009;&#24211;&#25110;&#19968;&#32452;&#34920;&#26684;)&#65292;&#22240;&#27492;&#26080;&#27861;&#20174;&#22810;&#20010;&#25968;&#25454;&#28304;&#30340;&#35206;&#30422;&#33539;&#22260;&#21644;&#20887;&#20313;&#24615;&#20013;&#33719;&#30410;&#12290;&#25105;&#20204;&#30340; EXPLAIGNN &#26041;&#27861;&#36890;&#36807;&#28151;&#21512;&#25968;&#25454;&#28304;&#24182;&#25552;&#20379;&#29992;&#25143;&#21487;&#29702;&#35299;&#30340;&#31572;&#26696;&#35299;&#37322;&#26469;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#12290;&#23427;&#20174;&#30693;&#35782;&#24211;&#12289;&#25991;&#26412;&#35821;&#26009;&#24211;&#12289;&#32593;&#32476;&#34920;&#26684;&#21644;&#20449;&#24687;&#26694;&#20013;&#26816;&#32034;&#23454;&#20307;&#21644;&#35777;&#25454;&#29255;&#27573;&#65292;&#24182;&#26500;&#24314;&#19968;&#20010;&#24322;&#26500;&#22270;&#12290;&#28982;&#21518;&#36890;&#36807;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#36845;&#20195;&#32553;&#20943;&#65292;&#30452;&#21040;&#26368;&#20339;&#31572;&#26696;&#21450;&#20854;&#35299;&#37322;&#34987;&#25552;&#28860;&#20986;&#26469;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;EXPLAIGNN &#25552;&#39640;&#20102;&#29616;&#26377;&#22522;&#32447;&#26041;&#24335;&#30340;&#24615;&#33021;&#12290;&#29992;&#25143;&#30740;&#31350;&#34920;&#26126;&#65292;&#23548;&#20986;&#30340;&#31572;&#26696;&#34987;&#32456;&#31471;&#29992;&#25143;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
In conversational question answering, users express their information needs through a series of utterances with incomplete context. Typical ConvQA methods rely on a single source (a knowledge base (KB), or a text corpus, or a set of tables), thus being unable to benefit from increased answer coverage and redundancy of multiple sources. Our method EXPLAIGNN overcomes these limitations by integrating information from a mixture of sources with user-comprehensible explanations for answers. It constructs a heterogeneous graph from entities and evidence snippets retrieved from a KB, a text corpus, web tables, and infoboxes. This large graph is then iteratively reduced via graph neural networks that incorporate question-level attention, until the best answers and their explanations are distilled. Experiments show that EXPLAIGNN improves performance over state-of-the-art baselines. A user study demonstrates that derived answers are understandable by end users.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21453;&#20107;&#23454;&#23398;&#20064;&#25490;&#24207;&#26041;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;&#26333;&#20809;&#30340;&#39118;&#38505;&#27491;&#21017;&#21270;&#23545;IPS&#20272;&#35745;&#36827;&#34892;&#35843;&#25972;&#65292;&#20197;&#30830;&#20445;&#23398;&#20064;&#25490;&#21517;&#27169;&#22411;&#19982;&#32473;&#23450;&#30340;&#23433;&#20840;&#27169;&#22411;&#25509;&#36817;&#65292;&#20174;&#32780;&#22312;&#37096;&#32626;&#36807;&#31243;&#20013;&#38477;&#20302;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2305.01522</link><description>&lt;p&gt;
&#24102;&#26377;&#22522;&#20110;&#26333;&#20809;&#30340;&#39118;&#38505;&#26368;&#23567;&#21270;&#30340;&#21453;&#20107;&#23454;&#23398;&#20064;&#25490;&#24207;&#23433;&#20840;&#37096;&#32626;
&lt;/p&gt;
&lt;p&gt;
Safe Deployment for Counterfactual Learning to Rank with Exposure-Based Risk Minimization. (arXiv:2305.01522v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01522
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21453;&#20107;&#23454;&#23398;&#20064;&#25490;&#24207;&#26041;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;&#26333;&#20809;&#30340;&#39118;&#38505;&#27491;&#21017;&#21270;&#23545;IPS&#20272;&#35745;&#36827;&#34892;&#35843;&#25972;&#65292;&#20197;&#30830;&#20445;&#23398;&#20064;&#25490;&#21517;&#27169;&#22411;&#19982;&#32473;&#23450;&#30340;&#23433;&#20840;&#27169;&#22411;&#25509;&#36817;&#65292;&#20174;&#32780;&#22312;&#37096;&#32626;&#36807;&#31243;&#20013;&#38477;&#20302;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#23398;&#20064;&#25490;&#24207;&#65288;CLTR&#65289;&#20381;&#36182;&#20110;&#22522;&#20110;&#26333;&#20809;&#30340;&#20498;&#25968;&#27010;&#29575;&#35780;&#20998;&#65288;IPS&#65289;&#65292;&#19968;&#31181;LTR&#29305;&#23450;&#30340;&#33258;&#36866;&#24212;IPS&#26469;&#32416;&#27491;&#20301;&#32622;&#20559;&#24046;&#12290;&#34429;&#28982;IPS&#21487;&#20197;&#25552;&#20379;&#26080;&#20559;&#21644;&#19968;&#33268;&#30340;&#20272;&#35745;&#65292;&#20294;&#36890;&#24120;&#20250;&#21463;&#21040;&#39640;&#26041;&#24046;&#30340;&#24433;&#21709;&#12290;&#23588;&#20854;&#26159;&#24403;&#21487;&#29992;&#28857;&#20987;&#25968;&#25454;&#24456;&#23569;&#26102;&#65292;&#36825;&#31181;&#26041;&#24046;&#21487;&#33021;&#20250;&#23548;&#33268;CLTR&#23398;&#20064;&#27425;&#20248;&#30340;&#25490;&#21517;&#34892;&#20026;&#12290;&#22240;&#27492;&#65292;&#29616;&#26377;&#30340;CLTR&#26041;&#27861;&#24102;&#26469;&#20102;&#37325;&#22823;&#39118;&#38505;&#65292;&#22240;&#20026;&#31616;&#21333;&#22320;&#37096;&#32626;&#23427;&#20204;&#30340;&#27169;&#22411;&#21487;&#33021;&#20250;&#23548;&#33268;&#29992;&#25143;&#20307;&#39564;&#38750;&#24120;&#24046;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#39118;&#38505;&#24863;&#30693;CLTR&#26041;&#27861;&#65292;&#20855;&#26377;&#23433;&#20840;&#37096;&#32626;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;&#25105;&#20204;&#23545;LTR&#30340;IPS&#20272;&#35745;&#24212;&#29992;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#26333;&#20809;&#30340;&#39118;&#38505;&#27491;&#21017;&#21270;&#27010;&#24565;&#12290;&#25105;&#20204;&#30340;&#39118;&#38505;&#27491;&#21017;&#21270;&#24809;&#32602;&#23398;&#20064;&#27169;&#22411;&#30340;&#25490;&#21517;&#34892;&#20026;&#19982;&#32473;&#23450;&#23433;&#20840;&#27169;&#22411;&#30340;&#19981;&#21305;&#37197;&#12290;&#22240;&#27492;&#65292;&#22312;IPS&#20272;&#35745;&#23384;&#22312;&#39640;&#24230;&#19981;&#30830;&#23450;&#24615;&#26102;&#65292;&#23427;&#30830;&#20445;&#23398;&#20064;&#25490;&#21517;&#27169;&#22411;&#19982;&#21487;&#20449;&#27169;&#22411;&#20445;&#25345;&#25509;&#36817;&#65292;&#20174;&#32780;&#22823;&#22823;&#38477;&#20302;&#20102;&#37096;&#32626;&#36807;&#31243;&#20013;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterfactual learning to rank (CLTR) relies on exposure-based inverse propensity scoring (IPS), a LTR-specific adaptation of IPS to correct for position bias. While IPS can provide unbiased and consistent estimates, it often suffers from high variance. Especially when little click data is available, this variance can cause CLTR to learn sub-optimal ranking behavior. Consequently, existing CLTR methods bring significant risks with them, as naively deploying their models can result in very negative user experiences. We introduce a novel risk-aware CLTR method with theoretical guarantees for safe deployment. We apply a novel exposure-based concept of risk regularization to IPS estimation for LTR. Our risk regularization penalizes the mismatch between the ranking behavior of a learned model and a given safe model. Thereby, it ensures that learned ranking models stay close to a trusted model, when there is high uncertainty in IPS estimation, which greatly reduces the risks during deployme
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;&#29616;&#23454;&#20013;&#37096;&#32626;&#30340;&#28145;&#24230;&#23398;&#20064;&#25512;&#33616;&#27169;&#22411;&#20013;&#23884;&#20837;&#34920;&#30340;&#24102;&#23485;&#38656;&#27714;&#21644;&#23616;&#37096;&#24615;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#24322;&#26500;&#20869;&#23384;&#25552;&#20986;MTrainS&#26469;&#25552;&#39640;DLRM&#35757;&#32451;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.01515</link><description>&lt;p&gt;
MTrainS: &#20351;&#29992;&#24322;&#26500;&#20869;&#23384;&#25552;&#39640;DLRM&#35757;&#32451;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
MTrainS: Improving DLRM training efficiency using heterogeneous memories. (arXiv:2305.01515v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01515
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;&#29616;&#23454;&#20013;&#37096;&#32626;&#30340;&#28145;&#24230;&#23398;&#20064;&#25512;&#33616;&#27169;&#22411;&#20013;&#23884;&#20837;&#34920;&#30340;&#24102;&#23485;&#38656;&#27714;&#21644;&#23616;&#37096;&#24615;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#24322;&#26500;&#20869;&#23384;&#25552;&#20986;MTrainS&#26469;&#25552;&#39640;DLRM&#35757;&#32451;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#27169;&#22411;&#38750;&#24120;&#24222;&#22823;&#65292;&#22312;&#35757;&#32451;&#26102;&#38656;&#35201;&#20351;&#29992;&#20960;TB&#30340;&#20869;&#23384;&#12290;&#20026;&#20102;&#33719;&#24471;&#26356;&#22909;&#30340;&#36136;&#37327;&#65292;&#27169;&#22411;&#30340;&#22823;&#23567;&#21644;&#22797;&#26434;&#24230;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#32780;&#22686;&#38271;&#65292;&#36825;&#38656;&#35201;&#26356;&#22810;&#30340;&#35757;&#32451;&#25968;&#25454;&#20197;&#36991;&#20813;&#36807;&#25311;&#21512;&#12290;&#36825;&#31181;&#27169;&#22411;&#22686;&#38271;&#35201;&#27714;&#25968;&#25454;&#20013;&#24515;&#22823;&#37327;&#36164;&#28304;&#12290;&#22240;&#27492;&#65292;&#35757;&#32451;&#25928;&#29575;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#20197;&#20445;&#25345;&#25968;&#25454;&#20013;&#24515;&#30340;&#21151;&#29575;&#38656;&#27714;&#21487;&#25511;&#12290;&#22312;&#28145;&#24230;&#23398;&#20064;&#25512;&#33616;&#27169;&#22411;(DLRM)&#20013;&#65292;&#36890;&#36807;&#23884;&#20837;&#34920;&#25429;&#25417;&#20998;&#31867;&#36755;&#20837;&#30340;&#31232;&#30095;&#29305;&#24449;&#26159;&#27169;&#22411;&#22823;&#23567;&#30340;&#20027;&#35201;&#36129;&#29486;&#32773;&#65292;&#24182;&#19988;&#38656;&#35201;&#39640;&#20869;&#23384;&#24102;&#23485;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#29616;&#23454;&#20013;&#37096;&#32626;&#27169;&#22411;&#20013;&#23884;&#20837;&#34920;&#30340;&#24102;&#23485;&#38656;&#27714;&#21644;&#23616;&#37096;&#24615;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#19981;&#21516;&#34920;&#30340;&#24102;&#23485;&#35201;&#27714;&#19981;&#22343;&#21248;&#65292;&#24182;&#19988;&#23884;&#20837;&#34920;&#26174;&#31034;&#20986;&#39640;&#26102;&#24207;&#23616;&#37096;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;MTrainS&#65292;&#21033;&#29992;&#24322;&#26500;&#20869;&#23384;&#65292;&#21253;&#25324;&#23383;&#33410;&#21644;&#22359;&#21487;&#23547;&#22336;&#23384;&#20648;&#31867;&#20869;&#23384;&#65292;&#29992;&#20110;DLRM&#30340;&#20998;&#23618;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommendation models are very large, requiring terabytes (TB) of memory during training. In pursuit of better quality, the model size and complexity grow over time, which requires additional training data to avoid overfitting. This model growth demands a large number of resources in data centers. Hence, training efficiency is becoming considerably more important to keep the data center power demand manageable. In Deep Learning Recommendation Models (DLRM), sparse features capturing categorical inputs through embedding tables are the major contributors to model size and require high memory bandwidth. In this paper, we study the bandwidth requirement and locality of embedding tables in real-world deployed models. We observe that the bandwidth requirement is not uniform across different tables and that embedding tables show high temporal locality. We then design MTrainS, which leverages heterogeneous memory, including byte and block addressable Storage Class Memory for DLRM hierarchicall
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;Touche&#23454;&#39564;&#23460;&#20219;&#21153;&#25552;&#20986;&#20102;&#19968;&#20010;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#20351;&#29992;&#20915;&#31574;&#26641;&#38598;&#25104;&#31639;&#27861;&#21644;&#22823;&#22411;&#19978;&#19979;&#25991;&#21270;&#35821;&#35328;&#27169;&#22411;&#65292;&#33021;&#22815;&#25353;&#29031;&#30456;&#20851;&#24615;&#21644;&#35770;&#35777;&#25903;&#25345;&#25490;&#24207;&#27604;&#36739;&#25991;&#26723;&#12290;&#35813;&#26041;&#27861;&#22312;&#31454;&#36187;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#21487;&#29992;&#20110;&#25913;&#36827;&#20449;&#24687;&#26816;&#32034;&#21644;&#23545;&#35805;&#31995;&#32479;&#20013;&#22788;&#29702;&#27604;&#36739;&#26597;&#35810;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.01513</link><description>&lt;p&gt;
&#21033;&#29992;&#38598;&#25104;&#26041;&#27861;&#21644;&#31070;&#32463;&#20449;&#24687;&#26816;&#32034;&#26816;&#32034;&#27604;&#36739;&#35770;&#28857;
&lt;/p&gt;
&lt;p&gt;
Retrieving Comparative Arguments using Ensemble Methods and Neural Information Retrieval. (arXiv:2305.01513v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01513
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;Touche&#23454;&#39564;&#23460;&#20219;&#21153;&#25552;&#20986;&#20102;&#19968;&#20010;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#20351;&#29992;&#20915;&#31574;&#26641;&#38598;&#25104;&#31639;&#27861;&#21644;&#22823;&#22411;&#19978;&#19979;&#25991;&#21270;&#35821;&#35328;&#27169;&#22411;&#65292;&#33021;&#22815;&#25353;&#29031;&#30456;&#20851;&#24615;&#21644;&#35770;&#35777;&#25903;&#25345;&#25490;&#24207;&#27604;&#36739;&#25991;&#26723;&#12290;&#35813;&#26041;&#27861;&#22312;&#31454;&#36187;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#21487;&#29992;&#20110;&#25913;&#36827;&#20449;&#24687;&#26816;&#32034;&#21644;&#23545;&#35805;&#31995;&#32479;&#20013;&#22788;&#29702;&#27604;&#36739;&#26597;&#35810;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;Touche&#23454;&#39564;&#23460;&#8220;&#27604;&#36739;&#38382;&#39064;&#19979;&#30340;&#35770;&#35777;&#26816;&#32034;&#8221;&#20219;&#21153;&#25552;&#20986;&#20102;&#19968;&#20010;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#22242;&#38431;Katana&#25552;&#20379;&#20102;&#22522;&#20110;&#20915;&#31574;&#26641;&#38598;&#25104;&#31639;&#27861;&#30340;&#22810;&#31181;&#26041;&#27861;&#26469;&#25353;&#29031;&#30456;&#20851;&#24615;&#21644;&#35770;&#35777;&#25903;&#25345;&#25490;&#24207;&#27604;&#36739;&#25991;&#26723;&#12290;&#25105;&#20204;&#20351;&#29992;PyTerrier&#24211;&#23558;&#38598;&#25104;&#27169;&#22411;&#24212;&#29992;&#20110;&#25490;&#24207;&#38382;&#39064;&#65292;&#24182;&#32771;&#34385;&#22522;&#20110;&#32479;&#35745;&#25991;&#26412;&#29305;&#24449;&#21644;&#22522;&#20110;&#27604;&#36739;&#32467;&#26500;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#36824;&#37319;&#29992;&#20102;&#22823;&#22411;&#19978;&#19979;&#25991;&#21270;&#35821;&#35328;&#24314;&#27169;&#25216;&#26415;&#65292;&#20363;&#22914;BERT&#65292;&#26469;&#35299;&#20915;&#25152;&#25552;&#20986;&#30340;&#25490;&#21517;&#20219;&#21153;&#12290;&#20026;&#20102;&#23558;&#36825;&#31181;&#25216;&#26415;&#19982;&#25490;&#24207;&#24314;&#27169;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#31070;&#32463;&#25490;&#21517;&#24211;OpenNIR&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#26126;&#26174;&#20248;&#20110;&#25552;&#35758;&#30340;&#22522;&#20934;&#65292;&#24182;&#22312;&#31454;&#36187;&#30340;&#23448;&#26041;&#25351;&#26631;&#65288;&#23545;&#20110;NDCG@5&#24471;&#20998;&#65289;&#20013;&#22312;&#30456;&#20851;&#24615;&#19978;&#25490;&#21517;&#31532;&#19968;&#65292;&#36136;&#37327;&#19978;&#25490;&#21517;&#31532;&#20108;&#12290;&#25552;&#20986;&#30340;&#27169;&#22411;&#21487;&#20197;&#24110;&#21161;&#25913;&#36827;&#20449;&#24687;&#26816;&#32034;&#21644;&#23545;&#35805;&#31995;&#32479;&#20013;&#22788;&#29702;&#27604;&#36739;&#26597;&#35810;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a submission to the Touche lab's Task 2 on Argument Retrieval for Comparative Questions. Our team Katana supplies several approaches based on decision tree ensembles algorithms to rank comparative documents in accordance with their relevance and argumentative support. We use PyTerrier library to apply ensembles models to a ranking problem, considering statistical text features and features based on comparative structures. We also employ large contextualized language modelling techniques, such as BERT, to solve the proposed ranking task. To merge this technique with ranking modelling, we leverage neural ranking library OpenNIR.  Our systems substantially outperforming the proposed baseline and scored first in relevance and second in quality according to the official metrics of the competition (for measure NDCG@5 score). Presented models could help to improve the performance of processing comparative queries in information retrieval and dialogue systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#25991;&#31456;&#26159;Dagstuhl&#30740;&#35752;&#20250;23031&#8220;&#21069;&#27839;&#20449;&#24687;&#33719;&#21462;&#23454;&#39564;&#30740;&#31350;&#21644;&#25945;&#32946;&#8221;&#30340;&#25253;&#21578;&#65292;&#20027;&#35201;&#35752;&#35770;&#20102;&#36890;&#36807;&#26356;&#36127;&#36131;&#20219;&#30340;&#23454;&#39564;&#23454;&#36341;&#26469;&#25552;&#39640;&#20449;&#24687;&#33719;&#21462;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#21644;&#31185;&#23398;&#25945;&#32946;&#65292;&#20250;&#35758;&#21484;&#38598;&#20102;&#21508;&#39046;&#22495;&#19987;&#23478;&#65292;&#25552;&#20986;&#20102;&#36827;&#19968;&#27493;&#25913;&#21892;&#25105;&#20204;&#30340;&#30740;&#31350;&#26041;&#27861;&#21644;&#30740;&#31350;&#25945;&#32946;&#30340;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2305.01509</link><description>&lt;p&gt;
Dagstuhl&#30740;&#35752;&#20250;23031&#65306;&#8220;&#21069;&#27839;&#20449;&#24687;&#33719;&#21462;&#23454;&#39564;&#30740;&#31350;&#21644;&#25945;&#32946;&#8221;&#12290;&#25253;&#21578;&#12290;
&lt;/p&gt;
&lt;p&gt;
Report from Dagstuhl Seminar 23031: Frontiers of Information Access Experimentation for Research and Education. (arXiv:2305.01509v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01509
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#25991;&#31456;&#26159;Dagstuhl&#30740;&#35752;&#20250;23031&#8220;&#21069;&#27839;&#20449;&#24687;&#33719;&#21462;&#23454;&#39564;&#30740;&#31350;&#21644;&#25945;&#32946;&#8221;&#30340;&#25253;&#21578;&#65292;&#20027;&#35201;&#35752;&#35770;&#20102;&#36890;&#36807;&#26356;&#36127;&#36131;&#20219;&#30340;&#23454;&#39564;&#23454;&#36341;&#26469;&#25552;&#39640;&#20449;&#24687;&#33719;&#21462;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#21644;&#31185;&#23398;&#25945;&#32946;&#65292;&#20250;&#35758;&#21484;&#38598;&#20102;&#21508;&#39046;&#22495;&#19987;&#23478;&#65292;&#25552;&#20986;&#20102;&#36827;&#19968;&#27493;&#25913;&#21892;&#25105;&#20204;&#30340;&#30740;&#31350;&#26041;&#27861;&#21644;&#30740;&#31350;&#25945;&#32946;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35760;&#24405;&#20102;Dagstuhl&#30740;&#35752;&#20250;23031&#8220;&#21069;&#27839;&#20449;&#24687;&#33719;&#21462;&#23454;&#39564;&#30740;&#31350;&#21644;&#25945;&#32946;&#8221;&#30340;&#35745;&#21010;&#21644;&#25104;&#26524;&#65292;&#20250;&#35758;&#27719;&#32858;&#20102;&#26469;&#33258;12&#20010;&#22269;&#23478;&#30340;37&#21517;&#21442;&#19982;&#32773;&#12290;&#30740;&#35752;&#20250;&#28041;&#21450;&#25216;&#26415;&#22686;&#24378;&#22411;&#20449;&#24687;&#35775;&#38382;&#65288;&#20449;&#24687;&#26816;&#32034;&#12289;&#25512;&#33616;&#31995;&#32479;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65289;&#24182;&#29305;&#21035;&#32858;&#28966;&#20110;&#23545;&#20854;&#36827;&#34892;&#26356;&#36127;&#36131;&#20219;&#30340;&#23454;&#39564;&#23454;&#36341;&#20197;&#33719;&#24471;&#26356;&#26377;&#25928;&#30340;&#32467;&#26524;&#65292;&#26082;&#36866;&#29992;&#20110;&#30740;&#31350;&#65292;&#20063;&#36866;&#29992;&#20110;&#31185;&#23398;&#25945;&#32946;&#12290;&#20250;&#35758;&#21484;&#38598;&#20102;&#20449;&#24687;&#35775;&#38382;&#21508;&#23376;&#39046;&#22495;&#30340;&#19987;&#23478;&#65292;&#21363;&#20449;&#24687;&#26816;&#32034;&#12289;&#25512;&#33616;&#31995;&#32479;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#20449;&#24687;&#31185;&#23398;&#21644;&#20154;&#26426;&#20132;&#20114;&#65292;&#20197;&#20849;&#21516;&#29702;&#35299;&#19979;&#19968;&#20195;&#20449;&#24687;&#35775;&#38382;&#31995;&#32479;&#38754;&#20020;&#30340;&#38382;&#39064;&#21644;&#25361;&#25112;&#65292;&#24182;&#20174;&#30740;&#31350;&#21644;&#23454;&#39564;&#30340;&#35282;&#24230;&#35752;&#35770;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#21644;&#38556;&#30861;&#65292;&#24182;&#25552;&#20986;&#26410;&#26469;&#38656;&#35201;&#36861;&#27714;&#30340;&#26041;&#21521;&#65292;&#20197;&#26399;&#25913;&#21892;&#25105;&#20204;&#30340;&#30740;&#31350;&#26041;&#27861;&#21644;&#30740;&#31350;&#25945;&#32946;&#12290;
&lt;/p&gt;
&lt;p&gt;
This report documents the program and the outcomes of Dagstuhl Seminar 23031 ``Frontiers of Information Access Experimentation for Research and Education'', which brought together 37 participants from 12 countries.  The seminar addressed technology-enhanced information access (information retrieval, recommender systems, natural language processing) and specifically focused on developing more responsible experimental practices leading to more valid results, both for research as well as for scientific education.  The seminar brought together experts from various sub-fields of information access, namely IR, RS, NLP, information science, and human-computer interaction to create a joint understanding of the problems and challenges presented by next generation information access systems, from both the research and the experimentation point of views, to discuss existing solutions and impediments, and to propose next steps to be pursued in the area in order to improve not also our research met
&lt;/p&gt;</description></item><item><title>NewsPanda&#26159;&#19968;&#20010;&#21487;&#20197;&#33258;&#21160;&#26816;&#27979;&#21644;&#20998;&#26512;&#19982;&#29615;&#20445;&#21644;&#22522;&#30784;&#24314;&#35774;&#30456;&#20851;&#30340;&#32593;&#19978;&#25991;&#31456;&#30340;&#24037;&#20855;&#12290;&#35813;&#24037;&#20855;&#20351;&#29992;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#21644;&#22122;&#22768;&#26657;&#27491;&#31639;&#27861;&#23545;BERT&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#20197;&#35782;&#21035;&#30456;&#20851;&#25991;&#31456;&#65292;&#24182;&#25552;&#21462;&#20851;&#38190;&#23383;&#21644;&#25214;&#21040;&#30456;&#20851;&#26469;&#28304;&#12290;&#24050;&#34987;&#19990;&#30028;&#33258;&#28982;&#22522;&#37329;&#20250;&#22242;&#38431;&#22312;&#33521;&#22269;&#12289;&#21360;&#24230;&#21644;&#23612;&#27850;&#23572;&#25104;&#21151;&#37096;&#32626;&#12290;</title><link>http://arxiv.org/abs/2305.01503</link><description>&lt;p&gt;
NewsPanda: &#29992;&#20110;&#21450;&#26102;&#20445;&#25252;&#34892;&#21160;&#30340;&#23186;&#20307;&#30417;&#25511;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
NewsPanda: Media Monitoring for Timely Conservation Action. (arXiv:2305.01503v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01503
&lt;/p&gt;
&lt;p&gt;
NewsPanda&#26159;&#19968;&#20010;&#21487;&#20197;&#33258;&#21160;&#26816;&#27979;&#21644;&#20998;&#26512;&#19982;&#29615;&#20445;&#21644;&#22522;&#30784;&#24314;&#35774;&#30456;&#20851;&#30340;&#32593;&#19978;&#25991;&#31456;&#30340;&#24037;&#20855;&#12290;&#35813;&#24037;&#20855;&#20351;&#29992;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#21644;&#22122;&#22768;&#26657;&#27491;&#31639;&#27861;&#23545;BERT&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#20197;&#35782;&#21035;&#30456;&#20851;&#25991;&#31456;&#65292;&#24182;&#25552;&#21462;&#20851;&#38190;&#23383;&#21644;&#25214;&#21040;&#30456;&#20851;&#26469;&#28304;&#12290;&#24050;&#34987;&#19990;&#30028;&#33258;&#28982;&#22522;&#37329;&#20250;&#22242;&#38431;&#22312;&#33521;&#22269;&#12289;&#21360;&#24230;&#21644;&#23612;&#27850;&#23572;&#25104;&#21151;&#37096;&#32626;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29615;&#20445;&#38750;&#25919;&#24220;&#32452;&#32455;&#23545;&#30417;&#27979;&#30456;&#20851;&#23186;&#20307;&#24182;&#21450;&#26102;&#20102;&#35299;&#22522;&#30784;&#24314;&#35774;&#39033;&#30446;&#30340;&#26356;&#26032;&#20855;&#26377;&#37325;&#35201;&#20852;&#36259;&#65292;&#22240;&#20026;&#36825;&#20123;&#39033;&#30446;&#21487;&#33021;&#20250;&#23545;&#37325;&#35201;&#30340;&#20445;&#25252;&#21306;&#22495;&#20135;&#29983;&#24040;&#22823;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#30417;&#27979;&#24456;&#38590;&#19988;&#38656;&#35201;&#32791;&#36153;&#26102;&#38388;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;NewsPanda&#30340;&#24037;&#20855;&#21253;&#65292;&#23427;&#21487;&#20197;&#33258;&#21160;&#26816;&#27979;&#21644;&#20998;&#26512;&#19982;&#29615;&#20445;&#21644;&#22522;&#30784;&#24314;&#35774;&#30456;&#20851;&#30340;&#32593;&#19978;&#25991;&#31456;&#12290;&#25105;&#20204;&#20351;&#29992;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#21644;&#22122;&#22768;&#26657;&#27491;&#31639;&#27861;&#23545;BERT&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#35782;&#21035;&#19982;&#20445;&#25252;&#21644;&#22522;&#30784;&#24314;&#35774;&#30456;&#20851;&#30340;&#25991;&#31456;&#12290;&#23545;&#24050;&#35782;&#21035;&#20986;&#30340;&#25991;&#31456;&#65292;&#25105;&#20204;&#36827;&#34892;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#65292;&#25552;&#21462;&#20851;&#38190;&#23383;&#24182;&#25214;&#21040;&#21487;&#33021;&#30456;&#20851;&#30340;&#26469;&#28304;&#12290;NewsPanda&#33258;2022&#24180;2&#26376;&#20197;&#26469;&#24050;&#34987;&#19990;&#30028;&#33258;&#28982;&#22522;&#37329;&#20250;&#22242;&#38431;&#22312;&#33521;&#22269;&#12289;&#21360;&#24230;&#21644;&#23612;&#27850;&#23572;&#25104;&#21151;&#37096;&#32626;&#12290;&#23427;&#30446;&#21069;&#30417;&#27979;&#20102;&#21360;&#24230;80,000&#20010;&#32593;&#31449;&#21644;1,074&#20010;&#20445;&#25252;&#22320;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Non-governmental organizations for environmental conservation have a significant interest in monitoring conservation-related media and getting timely updates about infrastructure construction projects as they may cause massive impact to key conservation areas. Such monitoring, however, is difficult and time-consuming. We introduce NewsPanda, a toolkit which automatically detects and analyzes online articles related to environmental conservation and infrastructure construction. We fine-tune a BERT-based model using active learning methods and noise correction algorithms to identify articles that are relevant to conservation and infrastructure construction. For the identified articles, we perform further analysis, extracting keywords and finding potentially related sources. NewsPanda has been successfully deployed by the World Wide Fund for Nature teams in the UK, India, and Nepal since February 2022. It currently monitors over 80,000 websites and 1,074 conservation sites across India an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22810;&#27169;&#24577;&#31070;&#32463;&#25968;&#25454;&#24211;&#26694;&#26550;&#65288;MMNDBs&#65289;&#65292;&#21487;&#20197;&#22238;&#31572;&#28041;&#21450;&#25991;&#26412;&#21644;&#22270;&#20687;&#31561;&#19981;&#21516;&#36755;&#20837;&#27169;&#24577;&#30340;&#22797;&#26434;&#26597;&#35810;&#65292;&#20855;&#26377;&#31867;&#20284;&#25968;&#25454;&#24211;&#30340;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.01447</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#31070;&#32463;&#25968;&#25454;&#24211;
&lt;/p&gt;
&lt;p&gt;
Multimodal Neural Databases. (arXiv:2305.01447v1 [cs.MM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01447
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22810;&#27169;&#24577;&#31070;&#32463;&#25968;&#25454;&#24211;&#26694;&#26550;&#65288;MMNDBs&#65289;&#65292;&#21487;&#20197;&#22238;&#31572;&#28041;&#21450;&#25991;&#26412;&#21644;&#22270;&#20687;&#31561;&#19981;&#21516;&#36755;&#20837;&#27169;&#24577;&#30340;&#22797;&#26434;&#26597;&#35810;&#65292;&#20855;&#26377;&#31867;&#20284;&#25968;&#25454;&#24211;&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#20854;&#20182;&#27169;&#24577;&#30340;&#26494;&#25955;&#32467;&#26500;&#25968;&#25454;&#30340;&#22686;&#21152;&#21628;&#21505;&#26032;&#30340;&#26597;&#35810;&#26041;&#27861;&#12290;&#22810;&#23186;&#20307;&#20449;&#24687;&#26816;&#32034;&#22635;&#34917;&#20102;&#36825;&#20010;&#31354;&#30333;&#24182;&#22312;&#26368;&#36817;&#20960;&#24180;&#21462;&#24471;&#20102;&#20196;&#20154;&#20852;&#22859;&#30340;&#36827;&#23637;&#12290;&#26816;&#32034;&#22823;&#35268;&#27169;&#22810;&#23186;&#20307;&#26723;&#26696;&#30340;&#20219;&#21153;&#24050;&#32463;&#32463;&#21382;&#20102;&#24040;&#22823;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#36825;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#26159;&#30001;&#20110;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#36817;&#21457;&#23637;&#25152;&#25512;&#21160;&#30340;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#39046;&#22495;&#30340;&#26041;&#27861;&#22312;&#23427;&#20204;&#25152;&#25903;&#25345;&#30340;&#26597;&#35810;&#31867;&#22411;&#19978;&#20173;&#28982;&#21463;&#21040;&#38480;&#21046;&#65292;&#23588;&#20854;&#26159;&#23427;&#20204;&#26080;&#27861;&#22238;&#31572;&#31867;&#20284;&#25968;&#25454;&#24211;&#30340;&#26597;&#35810;&#12290;&#22240;&#27492;&#65292;&#21463;&#31070;&#32463;&#25968;&#25454;&#24211;&#30340;&#26368;&#26032;&#24037;&#20316;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26694;&#26550;&#65292;&#21629;&#21517;&#20026;&#22810;&#27169;&#24577;&#31070;&#32463;&#25968;&#25454;&#24211;&#65288;MMNDBs&#65289;&#12290;MMNDBs&#21487;&#20197;&#22238;&#31572;&#28041;&#21450;&#19981;&#21516;&#36755;&#20837;&#27169;&#24577;&#65288;&#20363;&#22914;&#25991;&#26412;&#21644;&#22270;&#20687;&#65289;&#30340;&#25512;&#29702;&#30340;&#22797;&#26434;&#31867;&#20284;&#25968;&#25454;&#24211;&#30340;&#26597;&#35810;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#33021;&#22815;&#28385;&#36275;&#36825;&#19968;&#31995;&#21015;&#35201;&#27714;&#30340;&#26550;&#26500;&#65292;&#24182;&#36890;&#36807;&#20960;&#20010;&#22522;&#32447;&#27979;&#35797;&#20102;&#23427;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rise in loosely-structured data available through text, images, and other modalities has called for new ways of querying them. Multimedia Information Retrieval has filled this gap and has witnessed exciting progress in recent years. Tasks such as search and retrieval of extensive multimedia archives have undergone massive performance improvements, driven to a large extent by recent developments in multimodal deep learning. However, methods in this field remain limited in the kinds of queries they support and, in particular, their inability to answer database-like queries. For this reason, inspired by recent work on neural databases, we propose a new framework, which we name Multimodal Neural Databases (MMNDBs). MMNDBs can answer complex database-like queries that involve reasoning over different input modalities, such as text and images, at scale. In this paper, we present the first architecture able to fulfill this set of requirements and test it with several baselines, showing th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#23454;&#39564;&#26694;&#26550;&#65292;&#36890;&#36807;&#27719;&#38598;&#38750;&#19987;&#19994;&#35835;&#32773;&#30340;&#35265;&#35299;&#26469;&#30830;&#23450;&#25991;&#26723;&#30340;&#26377;&#29992;&#20869;&#23481;&#65292;&#20197;&#25552;&#39640;&#38750;&#19987;&#19994;&#35835;&#32773;&#30340;&#20915;&#31574;&#33021;&#21147;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#39135;&#35889;&#20013;&#20026;&#27599;&#20010;&#28921;&#39274;&#27493;&#39588;&#30340;&#26411;&#23614;&#23433;&#25490;&#21103;&#26631;&#39064;&#30340;&#36866;&#24403;&#25104;&#20998;&#25110;&#27880;&#37322;&#21487;&#20197;&#26174;&#33879;&#22686;&#21152;&#39135;&#35889;&#30340;&#21487;&#35835;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.01359</link><description>&lt;p&gt;
&#35774;&#35745;&#29992;&#25143;&#20915;&#31574;&#25991;&#26723;&#32467;&#26500;&#30340;&#23454;&#39564;&#26694;&#26550; - &#20197;&#39135;&#35889;&#20026;&#20363;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An experimental framework for designing document structure for users' decision making - An empirical study of recipes. (arXiv:2305.01359v1 [cs.DL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01359
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#23454;&#39564;&#26694;&#26550;&#65292;&#36890;&#36807;&#27719;&#38598;&#38750;&#19987;&#19994;&#35835;&#32773;&#30340;&#35265;&#35299;&#26469;&#30830;&#23450;&#25991;&#26723;&#30340;&#26377;&#29992;&#20869;&#23481;&#65292;&#20197;&#25552;&#39640;&#38750;&#19987;&#19994;&#35835;&#32773;&#30340;&#20915;&#31574;&#33021;&#21147;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#39135;&#35889;&#20013;&#20026;&#27599;&#20010;&#28921;&#39274;&#27493;&#39588;&#30340;&#26411;&#23614;&#23433;&#25490;&#21103;&#26631;&#39064;&#30340;&#36866;&#24403;&#25104;&#20998;&#25110;&#27880;&#37322;&#21487;&#20197;&#26174;&#33879;&#22686;&#21152;&#39135;&#35889;&#30340;&#21487;&#35835;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36828;&#31243;&#21306;&#22495;&#36827;&#34892;&#26377;&#25928;&#30340;&#24322;&#27493;&#36890;&#20449;&#38656;&#35201;&#20248;&#36136;&#30340;&#25991;&#26412;&#25991;&#26723;&#65292;&#29305;&#21035;&#26159;&#22312;COVID-19&#22823;&#27969;&#34892;&#26399;&#38388;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#25913;&#21892;&#38750;&#19987;&#19994;&#35835;&#32773;&#30340;&#20915;&#31574;&#33021;&#21147;&#32780;&#23450;&#20041;&#39318;&#36873;&#30340;&#25991;&#26723;&#32467;&#26500;&#65288;&#20869;&#23481;&#21644;&#24067;&#23616;&#65289;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22240;&#20026;&#19968;&#26041;&#38754;&#65292;&#26080;&#27861;&#20165;&#36890;&#36807;&#25910;&#38598;&#19987;&#19994;&#30693;&#35782;&#26469;&#30830;&#23450;&#21508;&#31181;&#35835;&#32773;&#26377;&#29992;&#30340;&#20869;&#23481;&#31867;&#22411;&#12290;&#32780;&#21478;&#19968;&#26041;&#38754;&#65292;&#36824;&#27809;&#26377;&#24314;&#31435;&#36215;&#20174;&#29992;&#25143;&#35282;&#24230;&#35780;&#20272;&#25991;&#26723;&#26377;&#29992;&#24615;&#30340;&#26041;&#27861;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#23454;&#39564;&#24615;&#26694;&#26550;&#65292;&#36890;&#36807;&#27719;&#38598;&#38750;&#19987;&#19994;&#35835;&#32773;&#30340;&#35265;&#35299;&#26469;&#30830;&#23450;&#25991;&#26723;&#30340;&#26377;&#29992;&#20869;&#23481;&#12290;&#26412;&#30740;&#31350;&#20197;200&#20010;&#22312;&#32447;&#39135;&#35889;&#20026;&#30740;&#31350;&#23545;&#35937;&#65292;&#25307;&#21215;&#20102;1,340&#21517;&#19994;&#20313;&#21416;&#24072;&#20316;&#20026;&#38750;&#19987;&#19994;&#35835;&#32773;&#12290;&#25552;&#20986;&#30340;&#26694;&#26550;&#30830;&#23450;&#20102;&#20845;&#20010;&#39135;&#35889;&#30340;&#26377;&#29992;&#20869;&#23481;&#12290;&#22810;&#23618;&#27425;&#24314;&#27169;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#30830;&#35748;&#30340;&#20845;&#20010;&#20869;&#23481;&#20013;&#65292;&#27599;&#20010;&#28921;&#39274;&#27493;&#39588;&#30340;&#26411;&#23614;&#23433;&#25490;&#21103;&#26631;&#39064;&#30340;&#36866;&#24403;&#25104;&#20998;&#25110;&#27880;&#37322;&#21487;&#20197;&#26174;&#33879;&#22686;&#21152;&#39135;&#35889;&#30340;&#21487;&#35835;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Textual documents need to be of good quality to ensure effective asynchronous communication in remote areas, especially during the COVID-19 pandemic. However, defining a preferred document structure (content and arrangement) for improving lay readers' decision-making is challenging. First, the types of useful content for various readers cannot be determined simply by gathering expert knowledge. Second, methodologies to evaluate the document's usefulness from the user's perspective have not been established. This study proposed the experimental framework to identify useful contents of documents by aggregating lay readers' insights. This study used 200 online recipes as research subjects and recruited 1,340 amateur cooks as lay readers. The proposed framework identified six useful contents of recipes. Multi-level modeling then showed that among the six identified contents, suitable ingredients or notes arranged with a subheading at the end of each cooking step significantly increased rec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20248;&#21270;BM25&#24341;&#23548;&#30340;&#32034;&#24341;&#36941;&#21382;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#31232;&#30095;&#34920;&#31034;&#36827;&#34892;&#24555;&#36895;&#23398;&#20064;&#31232;&#30095;&#26816;&#32034;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36825;&#31181;&#26041;&#26696;&#27604;&#21407;&#22987;MaxScore&#26041;&#27861;&#24555;&#24471;&#22810;&#65292;&#21516;&#26102;&#20445;&#30041;&#30456;&#20851;&#24615;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.01203</link><description>&lt;p&gt;
&#20248;&#21270;&#24341;&#23548;&#36941;&#21382;&#20197;&#23454;&#29616;&#24555;&#36895;&#23398;&#20064;&#31232;&#30095;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Optimizing Guided Traversal for Fast Learned Sparse Retrieval. (arXiv:2305.01203v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01203
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20248;&#21270;BM25&#24341;&#23548;&#30340;&#32034;&#24341;&#36941;&#21382;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#31232;&#30095;&#34920;&#31034;&#36827;&#34892;&#24555;&#36895;&#23398;&#20064;&#31232;&#30095;&#26816;&#32034;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36825;&#31181;&#26041;&#26696;&#27604;&#21407;&#22987;MaxScore&#26041;&#27861;&#24555;&#24471;&#22810;&#65292;&#21516;&#26102;&#20445;&#30041;&#30456;&#20851;&#24615;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22522;&#20110;&#30001;DeepImpact&#29983;&#25104;&#30340;&#23398;&#20064;&#31232;&#30095;&#34920;&#31034;&#30340;MaxScore&#25991;&#26723;&#26816;&#32034;&#21487;&#20197;&#36890;&#36807;BM25&#39537;&#21160;&#30340;&#21160;&#24577;&#32034;&#24341;&#36339;&#36807;&#22823;&#22823;&#21152;&#24555;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20351;&#29992;&#20854;&#20182;&#27169;&#22411;&#65288;&#22914;SPLADE&#21644;uniCOIL&#65289;&#36827;&#34892;top k&#26816;&#32034;&#26102;&#20351;&#29992;&#36825;&#31181;&#36941;&#21382;&#24341;&#23548;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#21457;&#29616;&#24403;BM25&#27169;&#22411;&#19982;&#23398;&#20064;&#26435;&#37325;&#27169;&#22411;&#19981;&#23436;&#20840;&#23545;&#40784;&#25110;&#26816;&#32034;&#28145;&#24230;k&#24456;&#23567;&#26102;&#65292;&#26080;&#32422;&#26463;&#30340;BM25 driven&#36339;&#36807;&#21487;&#33021;&#20250;&#23548;&#33268;&#26174;&#33879;&#30340;&#30456;&#20851;&#24615;&#38477;&#20302;&#12290;&#26412;&#25991;&#36890;&#36807;&#20108;&#32423;&#21098;&#26525;&#25511;&#21046;&#26041;&#26696;&#21644;&#27169;&#22411;&#23545;&#40784;&#20248;&#21270;&#20102;BM25&#24341;&#23548;&#30340;&#32034;&#24341;&#36941;&#21382;&#65292;&#20197;&#20351;&#29992;&#31232;&#30095;&#34920;&#31034;&#24555;&#36895;&#26816;&#32034;&#12290;&#34429;&#28982;&#21487;&#33021;&#20250;&#22686;&#21152;&#24310;&#36831;&#25104;&#26412;&#65292;&#20294;&#25311;&#35758;&#30340;&#26041;&#26696;&#27604;&#27809;&#26377;BM25&#24341;&#23548;&#30340;&#21407;&#22987;MaxScore&#26041;&#27861;&#24555;&#24471;&#22810;&#65292;&#21516;&#26102;&#20445;&#30041;&#30456;&#20851;&#24615;&#30340;&#26377;&#25928;&#24615;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#36825;&#20010;&#20004;&#32423;&#21098;&#26525;&#26041;&#26696;&#30340;&#31454;&#20105;&#21147;&#65292;&#24182;&#35780;&#20272;&#20102;&#20854;&#22312;&#20960;&#20010;&#29616;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies show that BM25-driven dynamic index skipping can greatly accelerate MaxScore-based document retrieval based on the learned sparse representation derived by DeepImpact. This paper investigates the effectiveness of such a traversal guidance strategy during top k retrieval when using other models such as SPLADE and uniCOIL, and finds that unconstrained BM25-driven skipping could have a visible relevance degradation when the BM25 model is not well aligned with a learned weight model or when retrieval depth k is small. This paper generalizes the previous work and optimizes the BM25 guided index traversal with a two-level pruning control scheme and model alignment for fast retrieval using a sparse representation. Although there can be a cost of increased latency, the proposed scheme is much faster than the original MaxScore method without BM25 guidance while retaining the relevance effectiveness. This paper analyzes the competitiveness of this two-level pruning scheme, and eva
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23433;&#20840;&#30340;&#22312;&#32447;&#23398;&#20064;&#25490;&#24207;&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;&#24403;&#21069;&#25490;&#21517;&#20013;&#30340;&#19968;&#20010;&#39033;&#30446;&#19982;&#25490;&#21517;&#22806;&#30340;&#39033;&#30446;&#39640;&#25928;&#22320;&#20132;&#25442;&#26469;&#25191;&#34892;&#25506;&#32034;&#65292;&#24182;&#22522;&#20110;Kullback-Leibler&#19978;&#32622;&#20449;&#24230;&#30028;&#38480;&#65288;KL-UCB&#65289;&#23545;&#26410;&#25490;&#24207;&#39033;&#30446;&#36827;&#34892;&#20048;&#35266;&#36873;&#25321;&#21644;&#23433;&#20840;&#30340;&#25490;&#24207;&#65292;&#20197;&#25552;&#39640;&#38271;&#26399;&#25910;&#30410;&#12290;</title><link>http://arxiv.org/abs/2305.01202</link><description>&lt;p&gt;
&#23433;&#20840;&#22312;&#32447;&#23398;&#20064;&#20013;&#26410;&#25490;&#24207;&#39033;&#30446;&#30340;&#25506;&#32034;&#19982;&#37325;&#26032;&#25490;&#24207;
&lt;/p&gt;
&lt;p&gt;
Exploration of Unranked Items in Safe Online Learning to Re-Rank. (arXiv:2305.01202v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01202
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23433;&#20840;&#30340;&#22312;&#32447;&#23398;&#20064;&#25490;&#24207;&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;&#24403;&#21069;&#25490;&#21517;&#20013;&#30340;&#19968;&#20010;&#39033;&#30446;&#19982;&#25490;&#21517;&#22806;&#30340;&#39033;&#30446;&#39640;&#25928;&#22320;&#20132;&#25442;&#26469;&#25191;&#34892;&#25506;&#32034;&#65292;&#24182;&#22522;&#20110;Kullback-Leibler&#19978;&#32622;&#20449;&#24230;&#30028;&#38480;&#65288;KL-UCB&#65289;&#23545;&#26410;&#25490;&#24207;&#39033;&#30446;&#36827;&#34892;&#20048;&#35266;&#36873;&#25321;&#21644;&#23433;&#20840;&#30340;&#25490;&#24207;&#65292;&#20197;&#25552;&#39640;&#38271;&#26399;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#23398;&#20064;&#25490;&#24207;&#38382;&#39064;&#30340;&#36125;&#21494;&#26031;&#31639;&#27861;&#32463;&#24120;&#35797;&#22270;&#21033;&#29992;&#29992;&#25143;&#21453;&#39304;&#26368;&#22823;&#21270;&#38271;&#26399;&#25910;&#30410;&#12290;&#28982;&#32780;&#65292;&#20174;&#23454;&#38469;&#35282;&#24230;&#32771;&#34385;&#65292;&#36825;&#31181;&#31639;&#27861;&#30001;&#20110;&#36807;&#20110;&#28608;&#36827;&#30340;&#25506;&#32034;&#32780;&#20855;&#26377;&#25439;&#23475;&#29992;&#25143;&#20307;&#39564;&#30340;&#39640;&#39118;&#38505;&#12290;&#22240;&#27492;&#65292;&#36817;&#24180;&#26469;&#65292;&#23545;&#23433;&#20840;&#25506;&#32034;&#30340;&#38656;&#27714;&#19981;&#26029;&#22686;&#21152;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23433;&#20840;&#30340;&#22312;&#32447;&#23398;&#20064;&#25490;&#24207;&#31639;&#27861;&#65292;&#23427;&#36890;&#36807;&#23558;&#24403;&#21069;&#25490;&#21517;&#20013;&#30340;&#19968;&#20010;&#39033;&#30446;&#19982;&#25490;&#21517;&#22806;&#30340;&#39033;&#30446;&#65288;&#21363;&#26410;&#25490;&#24207;&#39033;&#30446;&#65289;&#39640;&#25928;&#22320;&#20132;&#25442;&#26469;&#25191;&#34892;&#25506;&#32034;&#24182;&#36880;&#27493;&#25552;&#39640;&#21407;&#22987;&#25490;&#21517;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#22522;&#20110;Kullback-Leibler&#19978;&#32622;&#20449;&#24230;&#30028;&#38480;&#65288;KL-UCB&#65289;&#20048;&#35266;&#22320;&#36873;&#25321;&#19968;&#20010;&#26410;&#25490;&#21517;&#39033;&#30446;&#36827;&#34892;&#25506;&#32034;&#65292;&#24182;&#23433;&#20840;&#22320;&#23545;&#21253;&#25324;&#25152;&#36873;&#39033;&#30446;&#22312;&#20869;&#30340;&#39033;&#30446;&#36827;&#34892;&#37325;&#26032;&#25490;&#24207;&#12290;&#36890;&#36807;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#21487;&#20197;&#22312;&#27809;&#26377;&#20219;&#20309;&#23433;&#20840;&#36829;&#35268;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#38271;&#26399;&#24724;&#24680;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bandit algorithms for online learning to rank (OLTR) problems often aim to maximize long-term revenue by utilizing user feedback. From a practical point of view, however, such algorithms have a high risk of hurting user experience due to their aggressive exploration. Thus, there has been a rising demand for safe exploration in recent years. One approach to safe exploration is to gradually enhance the quality of an original ranking that is already guaranteed acceptable quality. In this paper, we propose a safe OLTR algorithm that efficiently exchanges one of the items in the current ranking with an item outside the ranking (i.e., an unranked item) to perform exploration. We select an unranked item optimistically to explore based on Kullback-Leibler upper confidence bounds (KL-UCB) and safely re-rank the items including the selected one. Through experiments, we demonstrate that the proposed algorithm improves long-term regret from baselines without any safety violation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35299;&#32806;&#26041;&#27861;&#65292;&#23558;&#22797;&#26434;&#30340;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#24418;&#24335;&#21270;&#20026;&#19978;&#19979;&#25991;&#30693;&#35782;&#22270;&#25628;&#32034;&#21644;&#25277;&#35937;&#36923;&#36753;&#26597;&#35810;&#25512;&#29702;&#30340;&#32452;&#21512;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#23427;&#22312;&#22810;&#20010;&#36923;&#36753;&#26597;&#35810;&#32467;&#26500;&#30340;&#26631;&#20934;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#37117;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#26356;&#39640;&#22797;&#26434;&#24615;&#30340;&#26597;&#35810;&#20013;&#33719;&#24471;&#20102;&#26174;&#30528;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2305.01157</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#30693;&#35782;&#22270;&#35889;&#19978;&#36827;&#34892;&#22797;&#26434;&#36923;&#36753;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Complex Logical Reasoning over Knowledge Graphs using Large Language Models. (arXiv:2305.01157v1 [cs.LO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01157
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35299;&#32806;&#26041;&#27861;&#65292;&#23558;&#22797;&#26434;&#30340;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#24418;&#24335;&#21270;&#20026;&#19978;&#19979;&#25991;&#30693;&#35782;&#22270;&#25628;&#32034;&#21644;&#25277;&#35937;&#36923;&#36753;&#26597;&#35810;&#25512;&#29702;&#30340;&#32452;&#21512;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#23427;&#22312;&#22810;&#20010;&#36923;&#36753;&#26597;&#35810;&#32467;&#26500;&#30340;&#26631;&#20934;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#37117;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#26356;&#39640;&#22797;&#26434;&#24615;&#30340;&#26597;&#35810;&#20013;&#33719;&#24471;&#20102;&#26174;&#30528;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30693;&#35782;&#22270;&#35889;&#19978;&#36827;&#34892;&#25512;&#29702;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#23427;&#38656;&#35201;&#23545;&#23454;&#20307;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#20197;&#21450;&#23427;&#20204;&#20043;&#38388;&#30340;&#22522;&#30784;&#36923;&#36753;&#36827;&#34892;&#28145;&#20837;&#29702;&#35299;&#12290;&#24403;&#21069;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#23398;&#20064;&#20960;&#20309;&#26469;&#23884;&#20837;&#23454;&#20307;&#30340;&#21521;&#37327;&#31354;&#38388;&#36827;&#34892;&#36923;&#36753;&#26597;&#35810;&#25805;&#20316;&#65292;&#20294;&#26159;&#23427;&#20204;&#22312;&#22797;&#26434;&#26597;&#35810;&#21644;&#29305;&#23450;&#25968;&#25454;&#38598;&#34920;&#31034;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#32806;&#26041;&#27861;&#65292;&#31216;&#20026;&#22522;&#20110;&#35821;&#35328;&#24341;&#23548;&#30340;&#30693;&#35782;&#22270;&#35889;&#25277;&#35937;&#25512;&#29702;&#65288;LARK&#65289;&#65292;&#23558;&#22797;&#26434;&#30340;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#24418;&#24335;&#21270;&#20026;&#19978;&#19979;&#25991;&#30693;&#35782;&#22270;&#25628;&#32034;&#21644;&#25277;&#35937;&#36923;&#36753;&#26597;&#35810;&#25512;&#29702;&#30340;&#32452;&#21512;&#65292;&#20197;&#20998;&#21035;&#21033;&#29992;&#22270;&#24418;&#25552;&#21462;&#31639;&#27861;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#36923;&#36753;&#26597;&#35810;&#32467;&#26500;&#30340;&#26631;&#20934;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#26041;&#27861;&#65292;&#22312;&#26356;&#39640;&#22797;&#26434;&#24615;&#30340;&#26597;&#35810;&#20013;&#33719;&#24471;&#20102;&#26174;&#30528;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reasoning over knowledge graphs (KGs) is a challenging task that requires a deep understanding of the complex relationships between entities and the underlying logic of their relations. Current approaches rely on learning geometries to embed entities in vector space for logical query operations, but they suffer from subpar performance on complex queries and dataset-specific representations. In this paper, we propose a novel decoupled approach, Language-guided Abstract Reasoning over Knowledge graphs (LARK), that formulates complex KG reasoning as a combination of contextual KG search and abstract logical query reasoning, to leverage the strengths of graph extraction algorithms and large language models (LLM), respectively. Our experiments demonstrate that the proposed approach outperforms state-of-the-art KG reasoning methods on standard benchmark datasets across several logical query constructs, with significant performance gain for queries of higher complexity. Furthermore, we show t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;RKGCN&#65292;&#23427;&#33021;&#22815;&#21160;&#24577;&#20998;&#26512;&#29992;&#25143;&#30340;&#20559;&#22909;&#24182;&#25512;&#33616;&#20986;&#21512;&#36866;&#30340;&#29289;&#21697;&#12290;&#35813;&#27169;&#22411;&#22312;&#21253;&#25324;&#30005;&#24433;&#12289;&#20070;&#31821;&#21644;&#38899;&#20048;&#22312;&#20869;&#30340;&#19977;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#19978;&#27604;5&#20010;&#22522;&#20934;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2305.01147</link><description>&lt;p&gt;
&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Ripple Knowledge Graph Convolutional Networks For Recommendation Systems. (arXiv:2305.01147v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01147
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;RKGCN&#65292;&#23427;&#33021;&#22815;&#21160;&#24577;&#20998;&#26512;&#29992;&#25143;&#30340;&#20559;&#22909;&#24182;&#25512;&#33616;&#20986;&#21512;&#36866;&#30340;&#29289;&#21697;&#12290;&#35813;&#27169;&#22411;&#22312;&#21253;&#25324;&#30005;&#24433;&#12289;&#20070;&#31821;&#21644;&#38899;&#20048;&#22312;&#20869;&#30340;&#19977;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#19978;&#27604;5&#20010;&#22522;&#20934;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#24050;&#32463;&#35777;&#26126;&#65292;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#26469;&#36741;&#21161;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#25512;&#33616;&#20915;&#31574;&#33021;&#26377;&#25928;&#25552;&#39640;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#21629;&#21517;&#20026;RKGCN&#65292;&#23427;&#21160;&#24577;&#20998;&#26512;&#27599;&#20010;&#29992;&#25143;&#30340;&#20559;&#22909;&#65292;&#24182;&#25512;&#33616;&#20986;&#21512;&#36866;&#30340;&#29289;&#21697;&#12290;&#23427;&#22312;&#29289;&#21697;&#21644;&#29992;&#25143;&#21452;&#26041;&#38754;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#26469;&#20016;&#23500;&#23427;&#20204;&#30340;&#34920;&#31034;&#65292;&#26368;&#22823;&#21270;&#30693;&#35782;&#22270;&#35889;&#20013;&#20016;&#23500;&#30340;&#20449;&#24687;&#30340;&#21033;&#29992;&#12290; RKGCN&#33021;&#22815;&#22312;&#19977;&#31181;&#19981;&#21516;&#30340;&#22330;&#26223;&#19979;&#25552;&#20379;&#26356;&#20010;&#24615;&#21270;&#21644;&#30456;&#20851;&#30340;&#25512;&#33616;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21253;&#25324;&#30005;&#24433;&#12289;&#20070;&#31821;&#21644;&#38899;&#20048;&#22312;&#20869;&#30340;&#19977;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#27604;5&#20010;&#22522;&#20934;&#27169;&#22411;&#26356;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Using knowledge graphs to assist deep learning models in making recommendation decisions has recently been proven to effectively improve the model's interpretability and accuracy. This paper introduces an end-to-end deep learning model, named RKGCN, which dynamically analyses each user's preferences and makes a recommendation of suitable items. It combines knowledge graphs on both the item side and user side to enrich their representations to maximize the utilization of the abundant information in knowledge graphs. RKGCN is able to offer more personalized and relevant recommendations in three different scenarios. The experimental results show the superior effectiveness of our model over 5 baseline models on three real-world datasets including movies, books, and music.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#20844;&#24179;&#26041;&#27861;&#65292;&#32771;&#34385;&#21040;&#20316;&#32773;&#22810;&#26679;&#24615;&#65292;&#20197;&#35299;&#20915;&#35770;&#25991;&#25512;&#33616;&#20013;&#30340;&#28508;&#22312;&#20559;&#35265;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#21516;&#26102;&#25552;&#20379;&#36328;&#22810;&#20010;&#21463;&#20445;&#25252;&#21464;&#37327;&#30340;&#20844;&#27491;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.01141</link><description>&lt;p&gt;
&#35770;&#25991;&#25512;&#33616;&#20013;&#30340;&#22810;&#32500;&#20844;&#27491;&#24615;
&lt;/p&gt;
&lt;p&gt;
Multidimensional Fairness in Paper Recommendation. (arXiv:2305.01141v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01141
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#20844;&#24179;&#26041;&#27861;&#65292;&#32771;&#34385;&#21040;&#20316;&#32773;&#22810;&#26679;&#24615;&#65292;&#20197;&#35299;&#20915;&#35770;&#25991;&#25512;&#33616;&#20013;&#30340;&#28508;&#22312;&#20559;&#35265;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#21516;&#26102;&#25552;&#20379;&#36328;&#22810;&#20010;&#21463;&#20445;&#25252;&#21464;&#37327;&#30340;&#20844;&#27491;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#38450;&#27490;&#20250;&#35758;&#21644;&#26399;&#21002;&#30340;&#35770;&#25991;&#23457;&#26680;&#21644;&#36873;&#25321;&#36807;&#31243;&#20013;&#20986;&#29616;&#20559;&#35265;&#65292;&#22823;&#22810;&#25968;&#20250;&#37319;&#29992;&#21452;&#30450;&#23457;&#26597;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#30740;&#31350;&#34920;&#26126;&#20559;&#35265;&#20173;&#28982;&#23384;&#22312;&#12290;&#35770;&#25991;&#23457;&#26680;&#30340;&#25512;&#33616;&#31639;&#27861;&#20063;&#21487;&#33021;&#23384;&#22312;&#20869;&#22312;&#30340;&#20559;&#24046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#20844;&#24179;&#26041;&#27861;&#65292;&#29305;&#21035;&#32771;&#34385;&#21040;&#20316;&#32773;&#22810;&#26679;&#24615;&#22312;&#35770;&#25991;&#25512;&#33616;&#20013;&#30340;&#20316;&#29992;&#12290;&#19982;&#20856;&#22411;&#30340;&#20844;&#24179;&#31639;&#27861;&#21482;&#20351;&#29992;&#19968;&#20010;&#20445;&#25252;&#21464;&#37327;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21516;&#26102;&#25552;&#20379;&#36328;&#22810;&#20010;&#21463;&#20445;&#25252;&#21464;&#37327;&#30340;&#20844;&#27491;&#32467;&#26524;&#12290;&#20116;&#20010;&#20154;&#21475;&#32479;&#35745;&#29305;&#24449;-&#24615;&#21035;&#12289;&#31181;&#26063;&#12289;&#32844;&#19994;&#38454;&#27573;&#12289;&#22823;&#23398;&#25490;&#21517;&#21644;&#22320;&#29702;&#20301;&#32622;-&#34987;&#21253;&#25324;&#22312;&#25105;&#20204;&#30340;&#22810;&#32500;&#20316;&#32773;&#36164;&#26009;&#20013;&#12290;&#24635;&#20307;&#22810;&#26679;&#24615;&#26041;&#27861;&#20351;&#29992;&#25972;&#20307;&#22810;&#26679;&#24615;&#24471;&#20998;&#26469;&#25490;&#21517;&#20986;&#29256;&#29289;&#65292;&#36718;&#27969;&#22810;&#26679;&#24615;&#25216;&#26415;&#36873;&#25321;&#20381;&#27425;&#23646;&#20110;&#27599;&#20010;&#21463;&#20445;&#25252;&#22242;&#20307;&#30340;&#20316;&#32773;&#30340;&#35770;&#25991;&#65292;&#32780;&#22810;&#26041;&#38754;&#22810;&#26679;&#24615;&#26041;&#27861;&#36873;&#25321;&#26368;&#21021;&#22635;&#20889;&#26063;&#35028;&#29305;&#24449;&#30340;&#35770;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;
To prevent potential bias in the paper review and selection process for conferences and journals, most include double blind review. Despite this, studies show that bias still exists. Recommendation algorithms for paper review also may have implicit bias. We offer three fair methods that specifically take into account author diversity in paper recommendation to address this. Our methods provide fair outcomes across many protected variables concurrently, in contrast to typical fair algorithms that only use one protected variable. Five demographic characteristics-gender, ethnicity, career stage, university rank, and geolocation-are included in our multidimensional author profiles. The Overall Diversity approach uses a score for overall diversity to rank publications. The Round Robin Diversity technique chooses papers from authors who are members of each protected group in turn, whereas the Multifaceted Diversity method chooses papers that initially fill the demographic feature with the hi
&lt;/p&gt;</description></item><item><title>CHIC&#25552;&#20379;&#20102;&#19968;&#20010;&#20844;&#20849;&#20225;&#19994;&#25991;&#26723;&#38382;&#31572;&#25968;&#25454;&#24211;&#65292;&#20197;&#28385;&#36275;&#19981;&#21516;&#31867;&#22411;&#20225;&#19994;&#25991;&#20214;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2305.01054</link><description>&lt;p&gt;
CHIC: &#20225;&#19994;&#25991;&#26723;&#21487;&#35270;&#21270;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
CHIC: Corporate Document for Visual question Answering. (arXiv:2305.01054v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01054
&lt;/p&gt;
&lt;p&gt;
CHIC&#25552;&#20379;&#20102;&#19968;&#20010;&#20844;&#20849;&#20225;&#19994;&#25991;&#26723;&#38382;&#31572;&#25968;&#25454;&#24211;&#65292;&#20197;&#28385;&#36275;&#19981;&#21516;&#31867;&#22411;&#20225;&#19994;&#25991;&#20214;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#21270;&#25991;&#20214;&#30340;&#24191;&#27867;&#20351;&#29992;&#65292;&#30001;&#20110;&#26080;&#32440;&#21270;&#20513;&#35758;&#30340;&#26174;&#33879;&#36235;&#21183;&#65292;&#36843;&#20351;&#19968;&#20123;&#20844;&#21496;&#23547;&#25214;&#33258;&#21160;&#22788;&#29702;&#27599;&#22825;&#25968;&#20197;&#21315;&#35745;&#25991;&#26723;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#23427;&#20204;&#20351;&#29992;&#33258;&#21160;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#65292;&#20351;&#23427;&#20204;&#33021;&#22815;&#24555;&#36895;&#20174;&#24222;&#22823;&#30340;&#25968;&#25454;&#38598;&#20013;&#25552;&#21462;&#26377;&#29992;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#29616;&#26377;&#25928;&#30340;IR&#26041;&#27861;&#20043;&#21069;&#65292;&#39318;&#20808;&#38656;&#35201;&#26377;&#19968;&#20010;&#36275;&#22815;&#30340;&#25968;&#25454;&#38598;&#12290;&#23613;&#31649;&#20844;&#21496;&#26377;&#36275;&#22815;&#30340;&#25968;&#25454;&#20197;&#28385;&#36275;&#20854;&#38656;&#27714;&#65292;&#20294;&#36824;&#38656;&#35201;&#19968;&#20010;&#20844;&#20849;&#25968;&#25454;&#24211;&#26469;&#27604;&#36739;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;&#25991;&#26723;&#30340;&#20844;&#20849;&#25968;&#25454;&#23384;&#22312;&#20110;DocVQA[2]&#21644;XFUND [10]&#20013;&#65292;&#20294;&#36825;&#20123;&#37117;&#19981;&#33021;&#23436;&#20840;&#28385;&#36275;&#20844;&#21496;&#30340;&#38656;&#27714;&#12290;&#19982;XFUND&#30456;&#27604;&#65292;DocVQA&#26377;&#20960;&#31181;&#31867;&#22411;&#30340;&#25991;&#20214;&#65292;&#20294;&#21482;&#26377;4.5&#65285;&#30340;&#25991;&#20214;&#26159;&#20225;&#19994;&#25991;&#20214;&#65288;&#21363;&#32467;&#26500;&#21270;&#25991;&#20214;&#65292;&#22914;&#34920;&#26684;&#12289;&#21322;&#32467;&#26500;&#21270;&#25991;&#20214;&#65292;&#22914;&#21457;&#31080;&#65292;&#20197;&#21450;&#38750;&#32467;&#26500;&#21270;&#25991;&#20214;&#65292;&#22914;&#30005;&#23376;&#37038;&#20214;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
The massive use of digital documents due to the substantial trend of paperless initiatives confronted some companies to find ways to process thousands of documents per day automatically. To achieve this, they use automatic information retrieval (IR) allowing them to extract useful information from large datasets quickly. In order to have effective IR methods, it is first necessary to have an adequate dataset. Although companies have enough data to take into account their needs, there is also a need for a public database to compare contributions between state-of-the-art methods. Public data on the document exists as DocVQA[2] and XFUND [10], but these do not fully satisfy the needs of companies. XFUND contains only form documents while the company uses several types of documents (i.e. structured documents like forms but also semi-structured as invoices, and unstructured as emails). Compared to XFUND, DocVQA has several types of documents but only 4.5% of them are corporate documents (i.
&lt;/p&gt;</description></item><item><title>LooPy&#26159;&#19968;&#31181;Python&#36719;&#20214;&#21253;&#65292;&#20026;MIR&#25552;&#20379;&#20102;&#19968;&#31181;&#38754;&#21521;&#30005;&#23376;&#33310;&#26354;&#30340;&#22522;&#30784;&#35774;&#26045;&#65292;&#21487;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;EDM&#38899;&#39057;&#65292;&#24182;&#25552;&#20379;&#20102;&#26694;&#26550;&#26469;&#26500;&#24314;&#19987;&#19994;&#32423;&#30340;&#27169;&#26495;&#65292;&#20351;&#29992;&#25143;&#21487;&#20197;&#20174;&#25351;&#23450;&#30340;&#26059;&#24459;&#21644;&#21644;&#24358;&#20013;&#21576;&#29616;&#20986;&#21046;&#20316;&#31934;&#33391;&#30340;&#27468;&#26354;&#36712;&#36947;&#25110;&#20165;&#36890;&#36807;&#20351;&#29992;&#31526;&#21495;&#24615;&#30340;&#26059;&#24459;&#29983;&#25104;&#22120;&#65292;&#25552;&#20379;&#20855;&#26377;&#22810;&#31181;&#39118;&#26684;&#30340;&#38899;&#36712;&#12290;</title><link>http://arxiv.org/abs/2305.01051</link><description>&lt;p&gt;
LooPy: &#19968;&#31181;&#38024;&#23545;&#30005;&#23376;&#33310;&#26354;&#30340;&#30740;&#31350;&#21451;&#22909;&#22411;&#28151;&#21512;&#26694;&#26550;&#30340;&#38899;&#20048;&#20449;&#24687;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
LooPy: A Research-Friendly Mix Framework for Music Information Retrieval on Electronic Dance Music. (arXiv:2305.01051v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01051
&lt;/p&gt;
&lt;p&gt;
LooPy&#26159;&#19968;&#31181;Python&#36719;&#20214;&#21253;&#65292;&#20026;MIR&#25552;&#20379;&#20102;&#19968;&#31181;&#38754;&#21521;&#30005;&#23376;&#33310;&#26354;&#30340;&#22522;&#30784;&#35774;&#26045;&#65292;&#21487;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;EDM&#38899;&#39057;&#65292;&#24182;&#25552;&#20379;&#20102;&#26694;&#26550;&#26469;&#26500;&#24314;&#19987;&#19994;&#32423;&#30340;&#27169;&#26495;&#65292;&#20351;&#29992;&#25143;&#21487;&#20197;&#20174;&#25351;&#23450;&#30340;&#26059;&#24459;&#21644;&#21644;&#24358;&#20013;&#21576;&#29616;&#20986;&#21046;&#20316;&#31934;&#33391;&#30340;&#27468;&#26354;&#36712;&#36947;&#25110;&#20165;&#36890;&#36807;&#20351;&#29992;&#31526;&#21495;&#24615;&#30340;&#26059;&#24459;&#29983;&#25104;&#22120;&#65292;&#25552;&#20379;&#20855;&#26377;&#22810;&#31181;&#39118;&#26684;&#30340;&#38899;&#36712;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#21457;&#23637;&#65292;&#38899;&#20048;&#20449;&#24687;&#26816;&#32034;(MIR)&#24471;&#21040;&#20102;&#29190;&#28856;&#24615;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#35832;&#22914;&#30005;&#23376;&#33310;&#26354;&#31561;&#38899;&#20048;&#31867;&#22411;&#30456;&#23545;&#20110;&#20854;&#20182;&#31867;&#22411;&#19968;&#30452;&#36739;&#23569;&#30740;&#31350;&#12290;&#32771;&#34385;&#21040;&#20854;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;Python&#36719;&#20214;&#21253;&#65292;&#29992;&#20110;&#33258;&#21160;&#21270;EDM&#38899;&#39057;&#29983;&#25104;&#65292;&#24182;&#20316;&#20026;EDM&#27468;&#26354;MIR&#30340;&#22522;&#30784;&#35774;&#26045;&#65292;&#20197;&#32531;&#35299;&#33719;&#21462;&#26631;&#27880;&#25968;&#25454;&#30340;&#38590;&#24230;&#12290;&#23427;&#26159;&#19968;&#20010;&#26041;&#20415;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#36731;&#26494;&#36830;&#25509;&#21040;&#35768;&#22810;&#31526;&#21495;&#38899;&#20048;&#29983;&#25104;&#31649;&#36947;&#30340;&#26411;&#31471;&#12290;&#22312;&#36825;&#20010;&#36719;&#20214;&#21253;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#26500;&#24314;&#19987;&#19994;&#27700;&#24179;&#30340;&#27169;&#26495;&#65292;&#21487;&#20197;&#20174;&#25351;&#23450;&#30340;&#26059;&#24459;&#21644;&#21644;&#24358;&#20013;&#21576;&#29616;&#20986;&#19968;&#20010;&#21046;&#20316;&#31934;&#33391;&#30340;&#36712;&#36947;&#65292;&#25110;&#32773;&#20165;&#36890;&#36807;&#25105;&#20204;&#30340;&#27010;&#29575;&#31526;&#21495;&#26059;&#24459;&#29983;&#25104;&#22120;&#65292;&#25552;&#20379;&#20855;&#26377;&#21508;&#31181;&#39118;&#26684;&#30340;&#38899;&#36712;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#28151;&#38899;&#22312;&#20027;&#35266;&#21644;&#23458;&#35266;&#35780;&#20272;&#26041;&#38754;&#19982;&#30001;&#19990;&#30028;&#33879;&#21517;&#33402;&#26415;&#23478;&#21046;&#20316;&#30340;&#21407;&#22987;&#21442;&#32771;&#27468;&#26354;&#20855;&#26377;&#30456;&#21516;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Music information retrieval (MIR) has gone through an explosive development with the advancement of deep learning in recent years. However, music genres like electronic dance music (EDM) has always been relatively less investigated compared to others. Considering its wide range of applications, we present a Python package for automated EDM audio generation as an infrastructure for MIR for EDM songs, to mitigate the difficulty of acquiring labelled data. It is a convenient tool that could be easily concatenated to the end of many symbolic music generation pipelines. Inside this package, we provide a framework to build professional-level templates that could render a well-produced track from specified melody and chords, or produce massive tracks given only a specific key by our probabilistic symbolic melody generator. Experiments show that our mixes could achieve the same quality of the original reference songs produced by world-famous artists, with respect to both subjective and objecti
&lt;/p&gt;</description></item><item><title>RecD &#26159;&#19968;&#31181;&#20026; DLRM &#35757;&#32451;&#25552;&#20379;&#21435;&#37325;&#21151;&#33021;&#30340;&#31471;&#21040;&#31471;&#22522;&#30784;&#35774;&#26045;&#20248;&#21270;&#65292;&#35299;&#20915;&#20102;&#30001;&#20110;&#29305;&#24449;&#37325;&#22797;&#36896;&#25104;&#30340;&#28023;&#37327;&#23384;&#20648;&#12289;&#39044;&#22788;&#29702;&#21644;&#35757;&#32451;&#24320;&#38144;&#65292;&#24341;&#20837;&#20102;&#26032;&#30340;&#24352;&#37327;&#26684;&#24335; InverseKeyedJaggedTensors (IKJTs) &#26469;&#21435;&#38500;&#29305;&#24449;&#20540;&#30340;&#37325;&#22797;&#65292;&#20351; DLRM &#27169;&#22411;&#26550;&#26500;&#33021;&#22815;&#26356;&#22909;&#22320;&#21033;&#29992;&#25968;&#25454;&#30340;&#37325;&#22797;&#24615;&#25552;&#39640;&#35757;&#32451;&#21534;&#21520;&#37327;&#12290;</title><link>http://arxiv.org/abs/2211.05239</link><description>&lt;p&gt;
RecD&#65306;&#20026;&#31471;&#21040;&#31471;&#28145;&#24230;&#23398;&#20064;&#25512;&#33616;&#27169;&#22411;&#35757;&#32451;&#22522;&#30784;&#35774;&#26045;&#25552;&#20379;&#21435;&#37325;&#21151;&#33021;
&lt;/p&gt;
&lt;p&gt;
RecD: Deduplication for End-to-End Deep Learning Recommendation Model Training Infrastructure. (arXiv:2211.05239v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.05239
&lt;/p&gt;
&lt;p&gt;
RecD &#26159;&#19968;&#31181;&#20026; DLRM &#35757;&#32451;&#25552;&#20379;&#21435;&#37325;&#21151;&#33021;&#30340;&#31471;&#21040;&#31471;&#22522;&#30784;&#35774;&#26045;&#20248;&#21270;&#65292;&#35299;&#20915;&#20102;&#30001;&#20110;&#29305;&#24449;&#37325;&#22797;&#36896;&#25104;&#30340;&#28023;&#37327;&#23384;&#20648;&#12289;&#39044;&#22788;&#29702;&#21644;&#35757;&#32451;&#24320;&#38144;&#65292;&#24341;&#20837;&#20102;&#26032;&#30340;&#24352;&#37327;&#26684;&#24335; InverseKeyedJaggedTensors (IKJTs) &#26469;&#21435;&#38500;&#29305;&#24449;&#20540;&#30340;&#37325;&#22797;&#65292;&#20351; DLRM &#27169;&#22411;&#26550;&#26500;&#33021;&#22815;&#26356;&#22909;&#22320;&#21033;&#29992;&#25968;&#25454;&#30340;&#37325;&#22797;&#24615;&#25552;&#39640;&#35757;&#32451;&#21534;&#21520;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102; RecD&#65288;&#25512;&#33616;&#21435;&#37325;&#65289;&#65292;&#23427;&#26159;&#19968;&#32452;&#38024;&#23545;&#28145;&#24230;&#23398;&#20064;&#25512;&#33616;&#27169;&#22411; (DLRM) &#35757;&#32451;&#27969;&#31243;&#30340;&#31471;&#21040;&#31471;&#22522;&#30784;&#35774;&#26045;&#20248;&#21270;&#12290;RecD&#35299;&#20915;&#20102;&#30001;&#20110;&#29305;&#24449;&#37325;&#22797;&#36896;&#25104;&#30340;&#28023;&#37327;&#23384;&#20648;&#12289;&#39044;&#22788;&#29702;&#21644;&#35757;&#32451;&#24320;&#38144;&#65292;&#36825;&#26159;&#22823;&#35268;&#27169; DLRM &#35757;&#32451;&#25968;&#25454;&#38598;&#20869;&#22312;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026; DLRM &#25968;&#25454;&#38598;&#26159;&#20174;&#20132;&#20114;&#20013;&#29983;&#25104;&#30340;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102; RecD &#22914;&#20309;&#21033;&#29992;&#27492;&#23646;&#24615;&#26469;&#20248;&#21270;&#29983;&#20135;&#25968;&#25454;&#30340;&#27969;&#31243;&#65292;&#20943;&#23569;&#25968;&#25454;&#38598;&#23384;&#20648;&#21644;&#39044;&#22788;&#29702;&#38656;&#27714;&#65292;&#24182;&#26368;&#22823;&#38480;&#24230;&#22320;&#22312;&#35757;&#32451;&#25209;&#27425;&#20013;&#37325;&#22797;&#12290;RecD &#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#24352;&#37327;&#26684;&#24335; InverseKeyedJaggedTensors (IKJTs)&#65292;&#20197;&#22312;&#27599;&#20010;&#25209;&#27425;&#20013;&#21435;&#38500;&#29305;&#24449;&#20540;&#30340;&#37325;&#22797;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102; DLRM &#27169;&#22411;&#26550;&#26500;&#22914;&#20309;&#21033;&#29992; IKJTs &#26469;&#26174;&#33879;&#25552;&#39640;&#35757;&#32451;&#21534;&#21520;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present RecD (Recommendation Deduplication), a suite of end-to-end infrastructure optimizations across the Deep Learning Recommendation Model (DLRM) training pipeline. RecD addresses immense storage, preprocessing, and training overheads caused by feature duplication inherent in industry-scale DLRM training datasets. Feature duplication arises because DLRM datasets are generated from interactions. While each user session can generate multiple training samples, many features' values do not change across these samples. We demonstrate how RecD exploits this property, end-to-end, across a deployed training pipeline. RecD optimizes data generation pipelines to decrease dataset storage and preprocessing resource demands and to maximize duplication within a training batch. RecD introduces a new tensor format, InverseKeyedJaggedTensors (IKJTs), to deduplicate feature values in each batch. We show how DLRM model architectures can leverage IKJTs to drastically increase training throughput. Re
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ELPH&#30340;&#20840;&#22270;GNN&#27169;&#22411;&#65292;&#23427;&#20351;&#29992;&#23376;&#22270;&#33609;&#22270;&#20316;&#20026;&#28040;&#24687;&#20256;&#36882;&#65292;&#20197;&#32531;&#35299;LP&#20219;&#21153;&#20013;&#23376;&#22270;&#20043;&#38388;&#30340;&#20887;&#20313;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2209.15486</link><description>&lt;p&gt;
&#22522;&#20110;&#23376;&#22270;&#33609;&#22270;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#38142;&#36335;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks for Link Prediction with Subgraph Sketching. (arXiv:2209.15486v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.15486
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ELPH&#30340;&#20840;&#22270;GNN&#27169;&#22411;&#65292;&#23427;&#20351;&#29992;&#23376;&#22270;&#33609;&#22270;&#20316;&#20026;&#28040;&#24687;&#20256;&#36882;&#65292;&#20197;&#32531;&#35299;LP&#20219;&#21153;&#20013;&#23376;&#22270;&#20043;&#38388;&#30340;&#20887;&#20313;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#38142;&#36335;&#39044;&#27979;&#20219;&#21153;&#20013;&#34920;&#29616;&#19981;&#20339;&#65292;&#36825;&#26159;&#30001;&#20110;&#34920;&#36798;&#33021;&#21147;&#30340;&#38480;&#21046;&#65292;&#20363;&#22914;&#26080;&#27861;&#35745;&#31639;&#19977;&#35282;&#24418;&#65288;&#22823;&#22810;&#25968;LP&#21551;&#21457;&#24335;&#31639;&#27861;&#30340;&#39592;&#24178;&#65289;&#65292;&#20197;&#21450;&#19981;&#33021;&#21306;&#20998;&#21516;&#26500;&#33410;&#28857;&#65288;&#20855;&#26377;&#30456;&#21516;&#32467;&#26500;&#35282;&#33394;&#30340;&#33410;&#28857;&#65289;&#12290;&#36825;&#20004;&#31181;&#34920;&#36798;&#38382;&#39064;&#21487;&#20197;&#36890;&#36807;&#23398;&#20064;&#38142;&#36335;&#65288;&#32780;&#19981;&#26159;&#33410;&#28857;&#65289;&#34920;&#31034;&#65292;&#24182;&#32467;&#21512;&#19977;&#35282;&#24418;&#35745;&#25968;&#31561;&#32467;&#26500;&#29305;&#24449;&#26469;&#32531;&#35299;&#12290;&#30001;&#20110;&#26174;&#24335;&#38142;&#36335;&#34920;&#31034;&#36890;&#24120;&#20195;&#20215;&#39640;&#26114;&#65292;&#22240;&#27492;&#26368;&#36817;&#30340;&#30740;&#31350;&#37319;&#29992;&#20102;&#22522;&#20110;&#23376;&#22270;&#30340;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;LP&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20294;&#30001;&#20110;&#23376;&#22270;&#20043;&#38388;&#23384;&#22312;&#39640;&#24230;&#20887;&#20313;&#65292;&#25928;&#29575;&#36739;&#20302;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#23376;&#22270;GNN&#65288;SGNN&#65289;&#26041;&#27861;&#30340;&#32452;&#20214;&#65292;&#22522;&#20110;&#25105;&#20204;&#30340;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ELPH&#65288;&#39640;&#25928;&#21704;&#24076;&#38142;&#36335;&#39044;&#27979;&#65289;&#30340;&#26032;&#22411;&#20840;&#22270;GNN&#65292;&#23558;&#23376;&#22270;&#33609;&#22270;&#20316;&#20026;&#28040;&#24687;&#20256;&#36882;&#20197;&#36817;&#20284;&#20840;&#22270;&#19978;&#30340;&#36716;&#25442;&#12290;ELPH&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#38656;&#35201;&#27604;&#29616;&#26377;&#22522;&#20110;&#23376;&#22270;&#30340;&#26041;&#27861;&#26356;&#23569;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many Graph Neural Networks (GNNs) perform poorly compared to simple heuristics on Link Prediction (LP) tasks. This is due to limitations in expressive power such as the inability to count triangles (the backbone of most LP heuristics) and because they can not distinguish automorphic nodes (those having identical structural roles). Both expressiveness issues can be alleviated by learning link (rather than node) representations and incorporating structural features such as triangle counts. Since explicit link representations are often prohibitively expensive, recent works resorted to subgraph-based methods, which have achieved state-of-the-art performance for LP, but suffer from poor efficiency due to high levels of redundancy between subgraphs. We analyze the components of subgraph GNN (SGNN) methods for link prediction. Based on our analysis, we propose a novel full-graph GNN called ELPH (Efficient Link Prediction with Hashing) that passes subgraph sketches as messages to approximate t
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Boosting&#30340;&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#31639;&#27861;&#65292;&#23558;&#22522;&#30784;&#23398;&#20064;&#22120;&#31616;&#21270;&#20026;&#30417;&#30563;&#23398;&#20064;&#65292;&#33719;&#24471;&#20102;&#24191;&#27867;&#30340;&#23454;&#38469;&#25928;&#30410;&#65307;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#24212;&#29992;&#33021;&#21147;&#20248;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#21644;&#31616;&#21333;&#22238;&#24402;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2208.01148</link><description>&lt;p&gt;
&#22522;&#20110;Boosting&#30340;&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Boosted Off-Policy Learning. (arXiv:2208.01148v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.01148
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Boosting&#30340;&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#31639;&#27861;&#65292;&#23558;&#22522;&#30784;&#23398;&#20064;&#22120;&#31616;&#21270;&#20026;&#30417;&#30563;&#23398;&#20064;&#65292;&#33719;&#24471;&#20102;&#24191;&#27867;&#30340;&#23454;&#38469;&#25928;&#30410;&#65307;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#24212;&#29992;&#33021;&#21147;&#20248;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#21644;&#31616;&#21333;&#22238;&#24402;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#26469;&#33258;&#35760;&#24405;&#24335;&#36172;&#21338;&#21453;&#39304;&#30340;&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#30340;Boosting&#31639;&#27861;&#12290;&#19982;&#29616;&#26377;&#30340;&#30417;&#30563;&#23398;&#20064;&#30340;Boosting&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#30452;&#25509;&#20248;&#21270;&#20102;&#31574;&#30053;&#39044;&#26399;&#25910;&#30410;&#30340;&#20272;&#35745;&#12290;&#25105;&#20204;&#23545;&#35813;&#31639;&#27861;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#24182;&#35777;&#26126;&#22914;&#26524;&#22522;&#30784;&#23398;&#20064;&#22120;&#28385;&#36275;&#8220;&#24369;&#8221;&#23398;&#20064;&#26465;&#20214;&#65292;&#37027;&#20040;&#27599;&#19968;&#36718;Boosting&#37117;&#20250;&#20943;&#23567;&#36807;&#37327;&#32463;&#39564;&#39118;&#38505;&#65288;&#21487;&#33021;&#26159;&#25351;&#25968;&#32423;&#65289;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#22522;&#30784;&#23398;&#20064;&#22120;&#31616;&#21270;&#20026;&#30417;&#30563;&#23398;&#20064;&#65292;&#20174;&#32780;&#25171;&#24320;&#20102;&#24191;&#27867;&#30340;&#22522;&#30784;&#23398;&#20064;&#22120;&#28304;&#65292;&#22914;&#20915;&#31574;&#26641;&#31561;&#65292;&#20855;&#26377;&#23454;&#38469;&#30410;&#22788;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#32487;&#25215;&#20102;&#35768;&#22810;&#22522;&#20110;&#20915;&#31574;&#26641;&#30340;Boosting&#31639;&#27861;&#30340;&#20248;&#33391;&#24615;&#36136;&#65288;&#20363;&#22914;&#23545;&#29305;&#24449;&#32553;&#25918;&#21644;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#40065;&#26834;&#24615;&#65289;&#65292;&#24182;&#19988;&#21487;&#20197;&#32988;&#36807;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#21644;&#21482;&#26159;&#22238;&#24402;&#35266;&#23519;&#21040;&#30340;&#22870;&#21169;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose the first boosting algorithm for off-policy learning from logged bandit feedback. Unlike existing boosting methods for supervised learning, our algorithm directly optimizes an estimate of the policy's expected reward. We analyze this algorithm and prove that the excess empirical risk decreases (possibly exponentially fast) with each round of boosting, provided a ''weak'' learning condition is satisfied by the base learner. We further show how to reduce the base learner to supervised learning, which opens up a broad range of readily available base learners with practical benefits, such as decision trees. Experiments indicate that our algorithm inherits many desirable properties of tree-based boosting algorithms (e.g., robustness to feature scaling and hyperparameter tuning), and that it can outperform off-policy learning with deep neural networks as well as methods that simply regress on the observed rewards.
&lt;/p&gt;</description></item></channel></rss>