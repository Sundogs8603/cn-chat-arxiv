<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SUPA&#30340;&#26032;&#22411;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#22823;&#22411;&#21160;&#24577;&#22270;&#20013;&#21363;&#26102;&#23398;&#20064;&#34920;&#31034;&#65292;&#20197;&#25552;&#39640;&#25512;&#33616;&#31995;&#32479;&#30340;&#25928;&#29575;&#21644;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.18622</link><description>&lt;p&gt;
&#24212;&#29992;&#20110;&#22823;&#22411;&#21160;&#24577;&#22270;&#30340;&#21363;&#26102;&#34920;&#31034;&#23398;&#20064;&#25512;&#33616;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Instant Representation Learning for Recommendation over Large Dynamic Graphs. (arXiv:2305.18622v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18622
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SUPA&#30340;&#26032;&#22411;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#22823;&#22411;&#21160;&#24577;&#22270;&#20013;&#21363;&#26102;&#23398;&#20064;&#34920;&#31034;&#65292;&#20197;&#25552;&#39640;&#25512;&#33616;&#31995;&#32479;&#30340;&#25928;&#29575;&#21644;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#36890;&#36807;&#29992;&#25143;&#21644;&#29289;&#21697;&#30340;&#34920;&#31034;&#23398;&#20064;&#26469;&#20102;&#35299;&#29992;&#25143;&#20559;&#22909;&#65292;&#32780;&#29616;&#20195;&#25512;&#33616;&#27169;&#22411;&#24320;&#22987;&#21033;&#29992;&#29992;&#25143;&#34920;&#29616;&#20986;&#30340;&#21508;&#31181;&#34892;&#20026;&#31867;&#22411;&#30340;&#20449;&#24687;&#65292;&#20197;&#25552;&#39640;&#34920;&#31034;&#23398;&#20064;&#25928;&#26524;&#12290;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#24773;&#20917;&#19979;&#65292;&#29992;&#25143;&#34892;&#20026;&#22270;&#19981;&#20165;&#26159;&#22810;&#37325;&#30340;&#65292;&#32780;&#19988;&#26159;&#21160;&#24577;&#30340;&#65292;&#21363;&#22270;&#38543;&#26102;&#38388;&#24555;&#36895;&#28436;&#21464;&#65292;&#28155;&#21152;&#25110;&#21024;&#38500;&#21508;&#31181;&#31867;&#22411;&#30340;&#33410;&#28857;&#21644;&#36793;&#32536;&#65292;&#36825;&#23548;&#33268;&#37051;&#22495;&#25200;&#21160;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#24573;&#30053;&#20102;&#36825;&#31181;&#27969;&#21160;&#21160;&#21147;&#23398;&#65292;&#22240;&#27492;&#19968;&#26086;&#22270;&#24418;&#21457;&#29983;&#26174;&#30528;&#28436;&#21464;&#65292;&#23427;&#20204;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#65292;&#20351;&#23427;&#20204;&#19981;&#36866;&#29992;&#20110;&#22312;&#32447;&#23398;&#20064;&#29615;&#22659;&#12290;&#27492;&#22806;&#65292;&#21160;&#24577;&#22270;&#20013;&#23384;&#22312;&#30340;&#37051;&#22495;&#25200;&#21160;&#20250;&#24694;&#21270;&#22522;&#20110;&#37051;&#23621;&#32858;&#21512;&#30340;&#22270;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SUPA&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#21160;&#24577;&#22810;&#37325;&#24322;&#26500;&#22270;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#12290;&#19982;&#37051;&#23621;&#32858;&#21512;&#20307;&#31995;&#32467;&#26500;&#30456;&#27604;&#65292;SUPA&#24320;&#21457;&#20102;&#19968;&#20010;&#26102;&#38388;&#24863;&#30693;&#30340;&#28040;&#24687;&#20256;&#36882;&#26041;&#26696;&#65292;&#20197;&#33258;&#36866;&#24212;&#22320;&#32858;&#21512;&#26469;&#33258;&#21508;&#31181;&#37051;&#23621;&#30340;&#33410;&#28857;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25972;&#21512;&#20102;&#19968;&#20010;&#21160;&#24577;&#22270;&#32534;&#30721;&#22120;&#26469;&#25429;&#25417;&#22270;&#32423;&#21160;&#24577;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21487;&#20197;&#20197;&#22312;&#32447;&#26041;&#24335;&#39640;&#25928;&#22320;&#29983;&#25104;&#19982;&#20854;&#21382;&#21490;&#34892;&#20026;&#30456;&#20851;&#30340;&#29992;&#25143;&#21644;&#29289;&#21697;&#34920;&#31034;&#12290;&#19977;&#20010;&#20844;&#20849;&#22522;&#20934;&#27979;&#35797;&#30340;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;SUPA&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#27604;&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommender systems are able to learn user preferences based on user and item representations via their historical behaviors. To improve representation learning, recent recommendation models start leveraging information from various behavior types exhibited by users. In real-world scenarios, the user behavioral graph is not only multiplex but also dynamic, i.e., the graph evolves rapidly over time, with various types of nodes and edges added or deleted, which causes the Neighborhood Disturbance. Nevertheless, most existing methods neglect such streaming dynamics and thus need to be retrained once the graph has significantly evolved, making them unsuitable in the online learning environment. Furthermore, the Neighborhood Disturbance existing in dynamic graphs deteriorates the performance of neighbor-aggregation based graph models. To this end, we propose SUPA, a novel graph neural network for dynamic multiplex heterogeneous graphs. Compared to neighbor-aggregation architecture, SUPA dev
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19977;&#31181;&#27169;&#22411;&#65292;&#37319;&#29992;&#21644;&#24494;&#35843;&#26368;&#20808;&#36827;&#30340;&#22810;&#27169;&#24577;Transformer&#36827;&#34892;&#22810;&#27169;&#24577;&#20551;&#26032;&#38395;&#26816;&#27979;&#65292;&#24182;&#25552;&#20986;&#20102;&#35757;&#32451;&#25968;&#25454;&#22686;&#24378;&#26469;&#25552;&#39640;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.18599</link><description>&lt;p&gt;
&#25552;&#39640;&#22810;&#27169;&#24577;&#20551;&#26032;&#38395;&#26816;&#27979;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Improving Generalization for Multimodal Fake News Detection. (arXiv:2305.18599v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18599
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19977;&#31181;&#27169;&#22411;&#65292;&#37319;&#29992;&#21644;&#24494;&#35843;&#26368;&#20808;&#36827;&#30340;&#22810;&#27169;&#24577;Transformer&#36827;&#34892;&#22810;&#27169;&#24577;&#20551;&#26032;&#38395;&#26816;&#27979;&#65292;&#24182;&#25552;&#20986;&#20102;&#35757;&#32451;&#25968;&#25454;&#22686;&#24378;&#26469;&#25552;&#39640;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#22312;&#65292;&#34394;&#20551;&#20449;&#24687;&#30340;&#19981;&#26029;&#20256;&#25773;&#21450;&#20854;&#24778;&#20154;&#30340;&#24433;&#21709;&#24050;&#32463;&#20419;&#20351;&#34892;&#19994;&#21644;&#23398;&#26415;&#30028;&#24320;&#21457;&#20986;&#20551;&#26032;&#38395;&#26816;&#27979;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36890;&#24120;&#20351;&#29992;&#35268;&#27169;&#36739;&#23567;&#25110;&#29305;&#23450;&#20027;&#39064;&#30340;&#26377;&#38480;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#27169;&#22411;&#32570;&#20047;&#27867;&#21270;&#33021;&#21147;&#65292;&#19981;&#33021;&#24212;&#29992;&#20110;&#29616;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#37319;&#29992;&#24182;&#24494;&#35843;&#26368;&#20808;&#36827;&#30340;&#22810;&#27169;&#24577;Transformer&#36827;&#34892;&#22810;&#27169;&#24577;&#20551;&#26032;&#38395;&#26816;&#27979;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#36890;&#36807;&#25805;&#20316;&#36755;&#20837;&#25968;&#25454;&#36827;&#34892;&#28145;&#20837;&#20998;&#26512;&#65292;&#26088;&#22312;&#25506;&#32034;&#36825;&#20123;&#27169;&#22411;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#23454;&#38469;&#20351;&#29992;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36328;&#22810;&#20010;&#27169;&#22411;&#36827;&#34892;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#20123;&#31995;&#32479;&#22312;&#21463;&#21040;&#25805;&#20316;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20250;&#20986;&#29616;&#26174;&#30528;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#20102;&#20943;&#23569;&#20559;&#24046;&#24182;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#25105;&#20204;&#24314;&#35758;&#36827;&#34892;&#35757;&#32451;&#25968;&#25454;&#22686;&#24378;&#65292;&#20197;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#36827;&#34892;&#26356;&#26377;&#24847;&#20041;&#30340;&#20551;&#26032;&#38395;&#26816;&#27979;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing proliferation of misinformation and its alarming impact have motivated both industry and academia to develop approaches for fake news detection. However, state-of-the-art approaches are usually trained on datasets of smaller size or with a limited set of specific topics. As a consequence, these models lack generalization capabilities and are not applicable to real-world data. In this paper, we propose three models that adopt and fine-tune state-of-the-art multimodal transformers for multimodal fake news detection. We conduct an in-depth analysis by manipulating the input data aimed to explore models performance in realistic use cases on social media. Our study across multiple models demonstrates that these systems suffer significant performance drops against manipulated data. To reduce the bias and improve model generalization, we suggest training data augmentation to conduct more meaningful experiments for fake news detection on social media. The proposed data augmentat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#65292;&#21363; ExactSDM &#21644; SoftSDM&#65292;&#23558;&#39034;&#24207;&#20381;&#36182;&#27169;&#22411; (SDM) &#36866;&#24212;&#21040;&#23398;&#20064;&#31232;&#30095;&#26816;&#32034; (LSR) &#20013;&#65292;&#20197;&#35299;&#20915;&#38271;&#31687;&#25991;&#26723;&#26816;&#32034;&#30340;&#38382;&#39064;&#12290;&#22312; MSMARCO &#25991;&#26723;&#21644; TREC Robust04 &#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;ExactSDM &#21644; SoftSDM &#37117;&#20248;&#20110;&#29616;&#26377;&#30340; LSR &#32858;&#21512;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.18494</link><description>&lt;p&gt;
&#23398;&#20064;&#31232;&#30095;&#26816;&#32034;&#22312;&#38271;&#25991;&#26723;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Adapting Learned Sparse Retrieval for Long Documents. (arXiv:2305.18494v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18494
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#65292;&#21363; ExactSDM &#21644; SoftSDM&#65292;&#23558;&#39034;&#24207;&#20381;&#36182;&#27169;&#22411; (SDM) &#36866;&#24212;&#21040;&#23398;&#20064;&#31232;&#30095;&#26816;&#32034; (LSR) &#20013;&#65292;&#20197;&#35299;&#20915;&#38271;&#31687;&#25991;&#26723;&#26816;&#32034;&#30340;&#38382;&#39064;&#12290;&#22312; MSMARCO &#25991;&#26723;&#21644; TREC Robust04 &#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;ExactSDM &#21644; SoftSDM &#37117;&#20248;&#20110;&#29616;&#26377;&#30340; LSR &#32858;&#21512;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#31232;&#30095;&#26816;&#32034; (LSR) &#26159;&#19968;&#31181;&#23558;&#26597;&#35810;&#21644;&#25991;&#26723;&#36716;&#25442;&#25104;&#19982;&#35789;&#27719;&#34920;&#23545;&#40784;&#30340;&#31232;&#30095;&#26435;&#37325;&#21521;&#37327;&#30340;&#31070;&#32463;&#26816;&#32034;&#26041;&#27861;&#12290;&#34429;&#28982;&#20687; Splade &#36825;&#26679;&#30340; LSR &#26041;&#27861;&#22312;&#30701;&#25991;&#27573;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#23427;&#20204;&#22914;&#20309;&#22788;&#29702;&#26356;&#38271;&#30340;&#25991;&#26723;&#36824;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#29992;&#20110;&#36866;&#24212; LSR &#21040;&#38271;&#25991;&#26723;&#30340;&#29616;&#26377;&#32858;&#21512;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#25509;&#36817;&#25171;&#20998;&#23545;&#20110; LSR &#22788;&#29702;&#38271;&#25991;&#26723;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#20026;&#20102;&#21033;&#29992;&#36825;&#20010;&#29305;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#23558;&#39034;&#24207;&#20381;&#36182;&#27169;&#22411; (SDM) &#36866;&#24212;&#21040; LSR &#30340;&#26041;&#27861;&#65306;ExactSDM &#21644; SoftSDM&#12290;ExactSDM &#20551;&#23450;&#21482;&#26377;&#31934;&#30830;&#30340;&#26597;&#35810;&#39033;&#20381;&#36182;&#24615;&#65292;&#32780; SoftSDM &#20351;&#29992;&#28508;&#22312;&#20989;&#25968;&#23545;&#26597;&#35810;&#39033;&#21450;&#20854;&#25193;&#23637;&#39033; (&#21363;&#20351;&#29992;&#36716;&#25442;&#22120;&#30340;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#22836;&#35782;&#21035;&#30340;&#39033;) &#30340;&#20381;&#36182;&#20851;&#31995;&#36827;&#34892;&#24314;&#27169;&#12290;&#22312; MSMARCO &#25991;&#26723;&#21644; TREC Robust04 &#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;ExactSDM &#21644; SoftSDM &#37117;&#20248;&#20110;&#29616;&#26377;&#30340; LSR &#32858;&#21512;&#26041;&#27861;&#65292;&#38024;&#23545;&#19981;&#21516;&#30340;&#25991;&#26723;&#38271;&#24230;&#32422;&#26463;&#34920;&#29616;&#20986;&#26368;&#20339;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learned sparse retrieval (LSR) is a family of neural retrieval methods that transform queries and documents into sparse weight vectors aligned with a vocabulary. While LSR approaches like Splade work well for short passages, it is unclear how well they handle longer documents. We investigate existing aggregation approaches for adapting LSR to longer documents and find that proximal scoring is crucial for LSR to handle long documents. To leverage this property, we proposed two adaptations of the Sequential Dependence Model (SDM) to LSR: ExactSDM and SoftSDM. ExactSDM assumes only exact query term dependence, while SoftSDM uses potential functions that model the dependence of query terms and their expansion terms (i.e., terms identified using a transformer's masked language modeling head).  Experiments on the MSMARCO Document and TREC Robust04 datasets demonstrate that both ExactSDM and SoftSDM outperform existing LSR aggregation approaches for different document length constraints. Surp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#20219;&#21153;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26550;&#26500;Journey Ranker&#65292;&#26469;&#35299;&#20915;Airbnb&#25628;&#32034;&#36807;&#31243;&#20013;&#30340;&#21807;&#19968;&#25361;&#25112;&#65292;&#21363;&#23458;&#25143;&#21644;&#20027;&#26426;&#30340;&#20559;&#22909;&#65292;&#35813;&#27169;&#22411;&#21487;&#24212;&#29992;&#20110;&#22810;&#20010;&#29992;&#20363;&#12290;</title><link>http://arxiv.org/abs/2305.18431</link><description>&lt;p&gt;
&#22810;&#20219;&#21153;&#23398;&#20064;&#20248;&#21270;Airbnb&#25628;&#32034;&#20043;&#26053;
&lt;/p&gt;
&lt;p&gt;
Optimizing Airbnb Search Journey with Multi-task Learning. (arXiv:2305.18431v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18431
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#20219;&#21153;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26550;&#26500;Journey Ranker&#65292;&#26469;&#35299;&#20915;Airbnb&#25628;&#32034;&#36807;&#31243;&#20013;&#30340;&#21807;&#19968;&#25361;&#25112;&#65292;&#21363;&#23458;&#25143;&#21644;&#20027;&#26426;&#30340;&#20559;&#22909;&#65292;&#35813;&#27169;&#22411;&#21487;&#24212;&#29992;&#20110;&#22810;&#20010;&#29992;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Airbnb&#26159;&#19968;&#20010;&#22312;&#32447;&#20303;&#23487;&#21644;&#20307;&#39564;&#24066;&#22330;&#65292;&#23458;&#20154;&#36890;&#24120;&#38656;&#35201;&#33457;&#36153;&#25968;&#21608;&#26469;&#25506;&#32034;&#21644;&#27604;&#36739;&#22810;&#20010;&#29289;&#21697;&#65292;&#24182;&#22312;&#20570;&#20986;&#26368;&#21518;&#30340;&#39044;&#35746;&#35831;&#27714;&#20043;&#21069;&#24179;&#34913;&#23458;&#20154;&#21644;&#20027;&#26426;&#30340;&#20559;&#22909;&#12290;&#25628;&#32034;&#36807;&#31243;&#30340;&#38271;&#26399;&#24615;&#36136;&#20197;&#21450;&#38656;&#35201;&#24179;&#34913;&#23458;&#20154;&#21644;&#20027;&#26426;&#30340;&#20559;&#22909;&#65292;&#36825;&#20123;&#37117;&#20026;Airbnb&#30340;&#25628;&#32034;&#25490;&#21517;&#25552;&#20379;&#20102;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#20219;&#21153;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26550;&#26500;Journey Ranker&#65292;&#20197;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;Journey Ranker&#21033;&#29992;&#20013;&#38388;&#30340;&#23458;&#25143;&#25805;&#20316;&#20316;&#20026;&#37324;&#31243;&#30865;&#65288;&#26080;&#35770;&#26159;&#31215;&#26497;&#30340;&#36824;&#26159;&#28040;&#26497;&#30340;&#65289;&#26469;&#26356;&#22909;&#22320;&#23558;&#23458;&#25143;&#25512;&#21521;&#25104;&#21151;&#30340;&#39044;&#35746;&#12290;&#23427;&#36824;&#20351;&#29992;&#19978;&#19979;&#25991;&#20449;&#24687;&#65288;&#22914;&#23458;&#25143;&#29366;&#24577;&#21644;&#25628;&#32034;&#26597;&#35810;&#65289;&#26469;&#24179;&#34913;&#23458;&#20154;&#21644;&#20027;&#26426;&#30340;&#20559;&#22909;&#12290;&#20854;&#27169;&#22359;&#21270;&#21644;&#21487;&#25193;&#23637;&#30340;&#35774;&#35745;&#21253;&#25324;&#22235;&#20010;&#27169;&#22359;&#65292;&#20998;&#31163;&#26126;&#30830;&#65292;&#21487;&#20197;&#26041;&#20415;&#22320;&#24212;&#29992;&#20110;Airbnb&#25628;&#32034;&#25490;&#21517;&#20197;&#22806;&#30340;&#29992;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
At Airbnb, an online marketplace for stays and experiences, guests often spend weeks exploring and comparing multiple items before making a final reservation request. Each reservation request may then potentially be rejected or cancelled by the host prior to check-in. The long and exploratory nature of the search journey, as well as the need to balance both guest and host preferences, present unique challenges for Airbnb search ranking. In this paper, we present Journey Ranker, a new multi-task deep learning model architecture that addresses these challenges. Journey Ranker leverages intermediate guest actions as milestones, both positive and negative, to better progress the guest towards a successful booking. It also uses contextual information such as guest state and search query to balance guest and host preferences. Its modular and extensible design, consisting of four modules with clear separation of concerns, allows for easy application to use cases beyond the Airbnb search ranki
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Dash&#30340;&#26377;&#25928;&#32780;&#20934;&#30830;&#30340;PARAFAC2&#20998;&#35299;&#26041;&#27861;&#65292;&#20854;&#37319;&#29992;&#20004;&#38454;&#27573;ALS&#31639;&#27861;&#65292;&#22312;&#21452;&#21521;&#27969;&#22788;&#29702;&#38750;&#35268;&#21017;&#24352;&#37327;&#26102;&#39640;&#25928;&#22320;&#22788;&#29702;&#26032;&#34892;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#22312;&#35813;&#24212;&#29992;&#22330;&#26223;&#19979;&#30340;&#24322;&#24120;&#26816;&#27979;&#24230;&#37327;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;Dash&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.18376</link><description>&lt;p&gt;
&#24555;&#36895;&#32780;&#20934;&#30830;&#30340;PARAFAC2&#21452;&#21521;&#27969;&#31639;&#27861;&#21450;&#20854;&#22312;&#38750;&#35268;&#21017;&#24352;&#37327;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Fast and Accurate Dual-Way Streaming PARAFAC2 for Irregular Tensors -- Algorithm and Application. (arXiv:2305.18376v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18376
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Dash&#30340;&#26377;&#25928;&#32780;&#20934;&#30830;&#30340;PARAFAC2&#20998;&#35299;&#26041;&#27861;&#65292;&#20854;&#37319;&#29992;&#20004;&#38454;&#27573;ALS&#31639;&#27861;&#65292;&#22312;&#21452;&#21521;&#27969;&#22788;&#29702;&#38750;&#35268;&#21017;&#24352;&#37327;&#26102;&#39640;&#25928;&#22320;&#22788;&#29702;&#26032;&#34892;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#22312;&#35813;&#24212;&#29992;&#22330;&#26223;&#19979;&#30340;&#24322;&#24120;&#26816;&#27979;&#24230;&#37327;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;Dash&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#19968;&#31181;&#38750;&#35268;&#21017;&#24352;&#37327;&#30340;&#21452;&#21521;&#27969;&#22788;&#29702;&#65292;&#20854;&#20013;&#24352;&#37327;&#30340;&#20004;&#20010;&#32500;&#24230;&#22312;&#26102;&#38388;&#19978;&#22686;&#21152;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#32780;&#20934;&#30830;&#30340;PARAFAC2&#20998;&#35299;&#26041;&#27861;Dash&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#20004;&#38454;&#27573;ALS&#31639;&#27861;&#65292;&#22312;&#22788;&#29702;&#26032;&#34892;&#26102;&#25928;&#29575;&#39640;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#22312;&#21452;&#21521;&#27969;&#19978;&#26816;&#27979;&#24322;&#24120;&#24773;&#20917;&#30340;&#24230;&#37327;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Dash&#26041;&#27861;&#22312;&#24615;&#33021;&#21644;&#26816;&#27979;&#25928;&#26524;&#19978;&#22343;&#20248;&#20110;&#29616;&#26377;&#30340;&#38745;&#24577;&#21644;&#27969;&#24335;PARAFAC2&#20998;&#35299;&#26041;&#27861;&#20197;&#21450;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
How can we efficiently and accurately analyze an irregular tensor in a dual-way streaming setting where the sizes of two dimensions of the tensor increase over time? What types of anomalies are there in the dual-way streaming setting? An irregular tensor is a collection of matrices whose column lengths are the same while their row lengths are different. In a dual-way streaming setting, both new rows of existing matrices and new matrices arrive over time. PARAFAC2 decomposition is a crucial tool for analyzing irregular tensors. Although real-time analysis is necessary in the dual-way streaming, static PARAFAC2 decomposition methods fail to efficiently work in this setting since they perform PARAFAC2 decomposition for accumulated tensors whenever new data arrive. Existing streaming PARAFAC2 decomposition methods work in a limited setting and fail to handle new rows of matrices efficiently. In this paper, we propose Dash, an efficient and accurate PARAFAC2 decomposition method working in 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32431;&#35889;&#22270;&#23884;&#20837;&#26041;&#27861;&#65292;&#22312;Top-N&#25512;&#33616;&#20219;&#21153;&#20013;&#27604;&#29616;&#26377;&#22522;&#20110;&#22270;&#21367;&#31215;&#30340;&#27169;&#22411;&#20855;&#26377;&#26356;&#20248;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.18374</link><description>&lt;p&gt;
&#32431;&#35889;&#22270;&#23884;&#20837;&#65306;&#23558;&#22270;&#21367;&#31215;&#37325;&#26032;&#35299;&#37322;&#20026;Top-N&#25512;&#33616;&#20013;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Pure Spectral Graph Embeddings: Reinterpreting Graph Convolution for Top-N Recommendation. (arXiv:2305.18374v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18374
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32431;&#35889;&#22270;&#23884;&#20837;&#26041;&#27861;&#65292;&#22312;Top-N&#25512;&#33616;&#20219;&#21153;&#20013;&#27604;&#29616;&#26377;&#22522;&#20110;&#22270;&#21367;&#31215;&#30340;&#27169;&#22411;&#20855;&#26377;&#26356;&#20248;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#21327;&#21516;&#36807;&#28388;&#20219;&#21153;&#65288;CF&#65289;&#30340;&#25512;&#33616;&#31995;&#32479;&#31639;&#27861;&#30340;&#24320;&#21457;&#20013;&#65292;&#20351;&#29992;&#22270;&#21367;&#31215;&#24050;&#32463;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#34429;&#28982;&#24050;&#32463;&#35777;&#26126;&#65292;&#22270;&#21367;&#31215;&#25805;&#20316;&#19982;&#22270;&#35889;&#22495;&#19978;&#30340;&#36807;&#28388;&#25805;&#20316;&#26377;&#20851;&#65292;&#20294;&#20026;&#20160;&#20040;&#36825;&#20250;&#23548;&#33268;&#21327;&#21516;&#36807;&#28388;&#38382;&#39064;&#30340;&#26356;&#39640;&#24615;&#33021;&#30340;&#29702;&#35770;&#22522;&#30784;&#20173;&#19981;&#20026;&#20154;&#30693;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#29992;&#25143;&#21644;&#39033;&#30446;&#34920;&#31034;&#23398;&#20064;&#36807;&#31243;&#20013;&#20351;&#29992;&#22270;&#21367;&#31215;&#30340;&#25928;&#26524;&#65292;&#24182;&#23637;&#31034;&#20102;&#23398;&#20064;&#21040;&#30340;&#28508;&#22312;&#29305;&#24449;&#22914;&#20309;&#20174;&#36807;&#28388;&#25805;&#20316;&#25512;&#36827;&#21040;&#30001;&#24402;&#19968;&#21270;&#37051;&#25509;&#30697;&#38453;&#30340;&#26368;&#39640;&#29305;&#24449;&#20540;&#23545;&#24212;&#30340;&#29305;&#24449;&#21521;&#37327;&#24352;&#25104;&#30340;&#23376;&#31354;&#38388;&#20013;&#65292;&#24182;&#19988;&#35813;&#23376;&#31354;&#38388;&#19978;&#30340;&#21521;&#37327;&#26159;&#19982;&#35757;&#32451;&#25968;&#25454;&#19978;&#30340;&#39044;&#27979;&#20989;&#25968;&#30340;&#27714;&#21644;&#30456;&#20851;&#30340;&#30446;&#26631;&#20989;&#25968;&#30340;&#26368;&#20248;&#35299;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#23558;&#22270;&#21367;&#31215;&#25805;&#20316;&#37325;&#26032;&#35299;&#37322;&#20026;&#32431;&#35889;&#23884;&#20837;&#65292;&#23558;&#20854;&#19982;&#35889;&#26041;&#27861;&#25991;&#29486;&#23545;&#40784;&#65292;&#24182;&#31361;&#20986;&#20854;&#19982;Laplacian Eigenmaps&#21644;Common Neighbour Ranking Mechanism&#30340;&#32852;&#31995;&#12290;&#36890;&#36807;&#21033;&#29992;&#22270;Laplacian&#30340;&#35889;&#29305;&#24615;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;Top-N&#25512;&#33616;&#20219;&#21153;&#20013;&#21462;&#24471;&#20248;&#20110;&#29616;&#26377;&#22522;&#20110;&#22270;&#21367;&#31215;&#30340;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The use of graph convolution in the development of recommender system algorithms has recently achieved state-of-the-art results in the collaborative filtering task (CF). While it has been demonstrated that the graph convolution operation is connected to a filtering operation on the graph spectral domain, the theoretical rationale for why this leads to higher performance on the collaborative filtering problem remains unknown. The presented work makes two contributions. First, we investigate the effect of using graph convolution throughout the user and item representation learning processes, demonstrating how the latent features learned are pushed from the filtering operation into the subspace spanned by the eigenvectors associated with the highest eigenvalues of the normalised adjacency matrix, and how vectors lying on this subspace are the optimal solutions for an objective function related to the sum of the prediction function over the training data. Then, we present an approach that 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#34913;&#37327;&#23545;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;&#30340;&#21487;&#35299;&#37322;&#24615;&#30340;&#21313;&#31181;&#35780;&#20272;&#35270;&#35282;&#65292;&#24182;&#36890;&#36807;&#26500;&#24314;&#26032;&#25968;&#25454;&#38598;&#26469;&#25552;&#39640;&#35299;&#37322;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.18363</link><description>&lt;p&gt;
&#36808;&#21521;&#21487;&#35299;&#37322;&#30340;&#23545;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Towards Explainable Conversational Recommender Systems. (arXiv:2305.18363v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18363
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#34913;&#37327;&#23545;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;&#30340;&#21487;&#35299;&#37322;&#24615;&#30340;&#21313;&#31181;&#35780;&#20272;&#35270;&#35282;&#65292;&#24182;&#36890;&#36807;&#26500;&#24314;&#26032;&#25968;&#25454;&#38598;&#26469;&#25552;&#39640;&#35299;&#37322;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#35299;&#37322;&#24050;&#32463;&#35777;&#26126;&#22312;&#24110;&#21161;&#29992;&#25143;&#29702;&#35299;&#25512;&#33616;&#21512;&#29702;&#24615;&#20197;&#21450;&#25552;&#39640;&#31995;&#32479;&#25928;&#29575;&#12289;&#36879;&#26126;&#24230;&#21644;&#21487;&#20449;&#24230;&#26041;&#38754;&#20855;&#26377;&#30410;&#22788;&#12290;&#22312;&#23545;&#35805;&#29615;&#22659;&#20013;&#65292;&#38656;&#35201;&#29983;&#25104;&#22810;&#20010;&#19978;&#19979;&#25991;&#21270;&#35299;&#37322;&#65292;&#36825;&#36827;&#19968;&#27493;&#22686;&#21152;&#20102;&#35299;&#37322;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#34913;&#37327;&#23545;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21313;&#31181;&#35780;&#20272;&#35270;&#35282;&#65292;&#32467;&#21512;&#20256;&#32479;&#25512;&#33616;&#31995;&#32479;&#30340;&#27010;&#24565;&#21644;&#23545;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;&#30340;&#29305;&#28857;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#25351;&#26631;&#35780;&#20272;&#20102;&#20116;&#20010;&#24050;&#26377;&#30340;&#23545;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#24182;&#35266;&#23519;&#21040;&#38656;&#35201;&#25552;&#39640;&#23545;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;&#30340;&#35299;&#37322;&#36136;&#37327;&#30340;&#24517;&#35201;&#24615;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#37319;&#29992;&#25163;&#21160;&#21644;&#33258;&#21160;&#26041;&#27861;&#25193;&#23637;&#20102;&#36825;&#20123;&#23545;&#35805;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#23545;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;&#25968;&#25454;&#38598;&#65292;&#21517;&#20026;&#8220;&#21487;&#35299;&#37322;&#25512;&#33616;&#23545;&#35805;&#8221;&#65288;E-ReDial&#65289;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;756&#20010;&#23545;&#35805;&#65292;&#36229;&#36807;2000&#26465;&#39640;&#36136;&#37327;&#30340;&#37325;&#20889;&#35299;&#37322;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#20004;&#31181;&#22522;&#32447;&#26041;&#27861;&#65292;&#24182;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explanations in conventional recommender systems have demonstrated benefits in helping the user understand the rationality of the recommendations and improving the system's efficiency, transparency, and trustworthiness. In the conversational environment, multiple contextualized explanations need to be generated, which poses further challenges for explanations. To better measure explainability in conversational recommender systems (CRS), we propose ten evaluation perspectives based on concepts from conventional recommender systems together with the characteristics of CRS. We assess five existing CRS benchmark datasets using these metrics and observe the necessity of improving the explanation quality of CRS. To achieve this, we conduct manual and automatic approaches to extend these dialogues and construct a new CRS dataset, namely Explainable Recommendation Dialogues (E-ReDial). It includes 756 dialogues with over 2,000 high-quality rewritten explanations. We compare two baseline approa
&lt;/p&gt;</description></item><item><title>DataChat&#26159;&#19968;&#20010;&#29992;&#20110;&#25968;&#25454;&#38598;&#25628;&#32034;&#21644;&#21487;&#35270;&#21270;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#36890;&#36807;&#22270;&#25968;&#25454;&#24211;&#21644;&#35821;&#35328;&#27169;&#22411;&#20026;&#29992;&#25143;&#25552;&#20379;&#26356;&#21152;&#26234;&#33021;&#21270;&#30340;&#25968;&#25454;&#25628;&#32034;&#20307;&#39564;&#12290;</title><link>http://arxiv.org/abs/2305.18358</link><description>&lt;p&gt;
DataChat&#65306;&#19968;&#20010;&#29992;&#20110;&#25968;&#25454;&#38598;&#25628;&#32034;&#21644;&#21487;&#35270;&#21270;&#30340;&#20250;&#35805;&#20195;&#29702;&#30340;&#21407;&#22411;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
DataChat: Prototyping a Conversational Agent for Dataset Search and Visualization. (arXiv:2305.18358v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18358
&lt;/p&gt;
&lt;p&gt;
DataChat&#26159;&#19968;&#20010;&#29992;&#20110;&#25968;&#25454;&#38598;&#25628;&#32034;&#21644;&#21487;&#35270;&#21270;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#36890;&#36807;&#22270;&#25968;&#25454;&#24211;&#21644;&#35821;&#35328;&#27169;&#22411;&#20026;&#29992;&#25143;&#25552;&#20379;&#26356;&#21152;&#26234;&#33021;&#21270;&#30340;&#25968;&#25454;&#25628;&#32034;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#29992;&#25143;&#38656;&#35201;&#30456;&#20851;&#30340;&#19978;&#19979;&#25991;&#21644;&#30740;&#31350;&#19987;&#19994;&#30693;&#35782;&#20197;&#26377;&#25928;&#22320;&#25628;&#32034;&#21644;&#35782;&#21035;&#30456;&#20851;&#25968;&#25454;&#38598;&#12290;&#20247;&#25152;&#21608;&#30693;&#65292;&#20803;&#25968;&#25454;&#21644;&#25628;&#32034;&#24037;&#20855;&#26159;&#25903;&#25345;&#25968;&#25454;&#25628;&#32034;&#30340;&#20004;&#20010;&#20851;&#38190;&#22240;&#32032;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#25628;&#32034;&#31995;&#32479; DataChat&#65292;&#23427;&#21033;&#29992;&#22270;&#25968;&#25454;&#24211;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;&#29992;&#25143;&#25552;&#20379;&#19982;&#30740;&#31350;&#25968;&#25454;&#20132;&#20114;&#21644;&#25628;&#32034;&#30340;&#26032;&#39062;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data users need relevant context and research expertise to effectively search for and identify relevant datasets. Leading data providers, such as the Inter-university Consortium for Political and Social Research (ICPSR), offer standardized metadata and search tools to support data search. Metadata standards emphasize the machine-readability of data and its documentation. There are opportunities to enhance dataset search by improving users' ability to learn about, and make sense of, information about data. Prior research has shown that context and expertise are two main barriers users face in effectively searching for, evaluating, and deciding whether to reuse data. In this paper, we propose a novel chatbot-based search system, DataChat, that leverages a graph database and a large language model to provide novel ways for users to interact with and search for research data. DataChat complements data archives' and institutional repositories' ongoing efforts to curate, preserve, and share 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#30005;&#23376;&#21830;&#21153;&#20135;&#21697;&#23646;&#24615;&#25366;&#25496;&#30340;&#26032;&#20219;&#21153;&#35774;&#32622;&#65292;&#21487;&#20197;&#21033;&#29992;&#39640;&#36136;&#37327;&#30340;&#31181;&#23376;&#23646;&#24615;&#38598;&#21512;&#36731;&#24230;&#30417;&#30563;&#24182;&#33258;&#21160;&#21457;&#29616;&#26032;&#30340;&#23646;&#24615;&#31867;&#22411;&#12290;&#36890;&#36807;&#33258;&#25105;&#30417;&#30563;&#21551;&#21457;&#24335;&#21644;&#26080;&#30417;&#30563;&#28508;&#22312;&#23646;&#24615;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20197;&#39069;&#22806;&#30340;&#38544;&#21547;&#35821;&#20041;&#20449;&#21495;&#20316;&#20026;&#36741;&#21161;&#30417;&#30563;&#65292;&#23558;&#29616;&#26377;&#31867;&#22411;&#30340;&#23646;&#24615;&#25193;&#23637;&#26368;&#22810;12&#20493;&#65292;&#24182;&#25104;&#21151;&#21457;&#25496;&#20102;39&#65285;&#30340;&#26032;&#23646;&#24615;&#20540;&#12290;</title><link>http://arxiv.org/abs/2305.18350</link><description>&lt;p&gt;
&#23454;&#29616;&#24320;&#25918;&#19990;&#30028;&#20135;&#21697;&#23646;&#24615;&#25366;&#25496;&#65306;&#22522;&#20110;&#36731;&#24230;&#30417;&#30563;&#26041;&#27861;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Open-World Product Attribute Mining: A Lightly-Supervised Approach. (arXiv:2305.18350v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18350
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#30005;&#23376;&#21830;&#21153;&#20135;&#21697;&#23646;&#24615;&#25366;&#25496;&#30340;&#26032;&#20219;&#21153;&#35774;&#32622;&#65292;&#21487;&#20197;&#21033;&#29992;&#39640;&#36136;&#37327;&#30340;&#31181;&#23376;&#23646;&#24615;&#38598;&#21512;&#36731;&#24230;&#30417;&#30563;&#24182;&#33258;&#21160;&#21457;&#29616;&#26032;&#30340;&#23646;&#24615;&#31867;&#22411;&#12290;&#36890;&#36807;&#33258;&#25105;&#30417;&#30563;&#21551;&#21457;&#24335;&#21644;&#26080;&#30417;&#30563;&#28508;&#22312;&#23646;&#24615;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20197;&#39069;&#22806;&#30340;&#38544;&#21547;&#35821;&#20041;&#20449;&#21495;&#20316;&#20026;&#36741;&#21161;&#30417;&#30563;&#65292;&#23558;&#29616;&#26377;&#31867;&#22411;&#30340;&#23646;&#24615;&#25193;&#23637;&#26368;&#22810;12&#20493;&#65292;&#24182;&#25104;&#21151;&#21457;&#25496;&#20102;39&#65285;&#30340;&#26032;&#23646;&#24615;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30005;&#23376;&#21830;&#21153;&#20135;&#21697;&#23646;&#24615;&#25366;&#25496;&#20219;&#21153;&#35774;&#32622;&#65292;&#29992;&#20110;&#25552;&#21462;&#24320;&#25918;&#19990;&#30028;&#23646;&#24615;&#65292;&#21516;&#26102;&#20943;&#23569;&#20154;&#24037;&#24178;&#39044;&#12290;&#25105;&#20204;&#30340;&#30417;&#30563;&#26469;&#33258;&#20110;&#29616;&#26377;&#36164;&#28304;&#20013;&#24341;&#23548;&#30340;&#39640;&#36136;&#37327;&#31181;&#23376;&#23646;&#24615;&#38598;&#21512;&#65292;&#26088;&#22312;&#25193;&#23637;&#29616;&#26377;&#31181;&#23376;&#31867;&#22411;&#30340;&#23646;&#24615;&#35789;&#27719;&#65292;&#24182;&#36890;&#36807;&#33258;&#21160;&#26041;&#24335;&#21457;&#29616;&#20219;&#20309;&#26032;&#30340;&#23646;&#24615;&#31867;&#22411;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#25968;&#25454;&#38598;&#20197;&#25903;&#25345;&#25105;&#20204;&#30340;&#35774;&#32622;&#65292;&#24182;&#25552;&#20986;&#20102;&#29305;&#23450;&#20110;&#21463;&#38480;&#30417;&#30563;&#30340;Amacer&#26041;&#27861;&#12290;&#23588;&#20854;&#26159;&#65292;&#30001;&#20110;&#37027;&#20123;&#26410;&#35265;&#36807;&#30340;&#26032;&#23646;&#24615;&#27809;&#26377;&#30452;&#25509;&#30417;&#30563;&#65292;&#25105;&#20204;&#30340;&#26032;&#39062;&#20844;&#24335;&#21033;&#29992;&#20102;&#33258;&#25105;&#30417;&#30563;&#21551;&#21457;&#24335;&#21644;&#26080;&#30417;&#30563;&#28508;&#22312;&#23646;&#24615;&#65292;&#21033;&#29992;&#20135;&#21697;&#19978;&#19979;&#25991;&#33719;&#24471;&#39069;&#22806;&#30340;&#38544;&#21547;&#35821;&#20041;&#20449;&#21495;&#20316;&#20026;&#36741;&#21161;&#30417;&#30563;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;F1&#20540;&#19978;&#36229;&#36807;&#20102;&#21508;&#31181;&#22522;&#32447;&#26041;&#27861;12&#20010;&#30334;&#20998;&#28857;&#65292;&#20351;&#29616;&#26377;&#31867;&#22411;&#30340;&#23646;&#24615;&#22823;&#22823;&#25193;&#23637;&#20102;&#26368;&#22810;12&#20493;&#65292;&#24182;&#19988;&#21457;&#29616;&#26032;&#23646;&#24615;&#20540;&#30340;&#33021;&#21147;&#36798;&#21040;&#20102;39&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new task setting for attribute mining on e-commerce products, serving as a practical solution to extract open-world attributes without extensive human intervention. Our supervision comes from a high-quality seed attribute set bootstrapped from existing resources, and we aim to expand the attribute vocabulary of existing seed types, and also to discover any new attribute types automatically. A new dataset is created to support our setting, and our approach Amacer is proposed specifically to tackle the limited supervision. Especially, given that no direct supervision is available for those unseen new attributes, our novel formulation exploits self-supervised heuristic and unsupervised latent attributes, which attains implicit semantic signals as additional supervision by leveraging product context. Experiments suggest that our approach surpasses various baselines by 12 F1, expanding attributes of existing types significantly by up to 12 times, and discovering values from 39%
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#29289;&#21697;&#27969;&#34892;&#24230;&#12289;&#36136;&#37327;&#21644;&#20301;&#32622;&#20559;&#24046;&#23545;&#29992;&#25143;&#31119;&#21033;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#36890;&#36807;&#25506;&#32034;&#20943;&#36731;&#27969;&#34892;&#24230;&#20559;&#35265;&#36127;&#38754;&#24433;&#21709;&#30340;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.18333</link><description>&lt;p&gt;
&#20855;&#26377;&#27969;&#34892;&#24230;&#20559;&#35265;&#30340;&#25490;&#21517;&#65306;&#33258;&#22686;&#24378;&#21160;&#24577;&#19979;&#30340;&#29992;&#25143;&#31119;&#21033;
&lt;/p&gt;
&lt;p&gt;
Ranking with Popularity Bias: User Welfare under Self-Amplification Dynamics. (arXiv:2305.18333v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18333
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#29289;&#21697;&#27969;&#34892;&#24230;&#12289;&#36136;&#37327;&#21644;&#20301;&#32622;&#20559;&#24046;&#23545;&#29992;&#25143;&#31119;&#21033;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#36890;&#36807;&#25506;&#32034;&#20943;&#36731;&#27969;&#34892;&#24230;&#20559;&#35265;&#36127;&#38754;&#24433;&#21709;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#24050;&#32463;&#30830;&#35748;&#27969;&#34892;&#24230;&#20559;&#35265;&#22312;&#25512;&#33616;&#65288;&#21644;&#20854;&#20182;&#22522;&#20110;&#25490;&#21517;&#30340;&#65289;&#31995;&#32479;&#20013;&#21457;&#25381;&#20316;&#29992;&#65292;&#20294;&#20854;&#23545;&#29992;&#25143;&#31119;&#21033;&#30340;&#24433;&#21709;&#30340;&#35814;&#32454;&#20998;&#26512;&#20173;&#28982;&#32570;&#20047;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#26426;&#21046;&#65292;&#36890;&#36807;&#23427;&#65292;&#29289;&#21697;&#30340;&#27969;&#34892;&#24230;&#12289;&#36136;&#37327;&#21644;&#20301;&#32622;&#20559;&#24046;&#21487;&#20197;&#24433;&#21709;&#29992;&#25143;&#36873;&#25321;&#65292;&#24182;&#19988;&#21487;&#20197;&#36127;&#38754;&#24433;&#21709;&#21508;&#31181;&#25512;&#33616;&#31574;&#30053;&#30340;&#38598;&#20307;&#29992;&#25143;&#25928;&#29992;&#12290;&#25105;&#20204;&#23558;&#38382;&#39064;&#34920;&#36848;&#20026;&#38750;&#24179;&#31283;&#19978;&#19979;&#25991;&#33073;&#38774;&#26426;&#65292;&#24378;&#35843;&#19981;&#26159;&#20026;&#20102;&#28040;&#38500;&#27969;&#34892;&#24230;&#20559;&#35265;&#32780;&#26159;&#20026;&#20102;&#20943;&#36731;&#20854;&#36127;&#38754;&#24433;&#21709;&#32780;&#36827;&#34892;&#25506;&#32034;&#30340;&#37325;&#35201;&#24615;&#12290;&#39318;&#20808;&#65292;&#26222;&#36890;&#30340;&#26377;&#27969;&#34892;&#24230;&#20559;&#24046;&#30340;&#25512;&#33616;&#31995;&#32479;&#20250;&#36890;&#36807;&#28151;&#28102;&#29289;&#21697;&#36136;&#37327;&#21644;&#27969;&#34892;&#24230;&#32780;&#24341;&#21457;&#32447;&#24615;&#36951;&#25022;&#12290;&#26356;&#19968;&#33324;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21363;&#20351;&#22312;&#32447;&#24615;&#35774;&#32622;&#19979;&#65292;&#30001;&#20110;&#27969;&#34892;&#24230;&#20559;&#35265;&#30340;&#28151;&#28102;&#25928;&#24212;&#65292;&#29289;&#21697;&#36136;&#37327;&#30340;&#21487;&#35782;&#21035;&#24615;&#20063;&#21487;&#33021;&#26080;&#27861;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;&#22312;&#36275;&#22815;&#21464;&#24322;&#30340;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#31867;UCB&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#26377;&#25928;&#30340;&#36951;&#25022;&#20445;&#35777;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#35777;&#23454;&#20102;&#27969;&#34892;&#24230;&#20559;&#35265;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
While popularity bias is recognized to play a role in recommmender (and other ranking-based) systems, detailed analyses of its impact on user welfare have largely been lacking. We propose a general mechanism by which item popularity, item quality, and position bias can impact user choice, and how it can negatively impact the collective user utility of various recommender policies. Formulating the problem as a non-stationary contextual bandit, we highlight the importance of exploration, not to eliminate popularity bias, but to mitigate its negative effects. First, naive popularity-biased recommenders are shown to induce linear regret by conflating item quality and popularity. More generally, we show that, even in linear settings, identifiability of item quality may not be possible due to the confounding effects of popularity bias. However, under sufficient variability assumptions, we develop an efficient UCB-style algorithm and prove efficient regret guarantees. We complement our analys
&lt;/p&gt;</description></item><item><title>#REVAL&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#35821;&#20041;&#35780;&#20272;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#20256;&#32479;&#35780;&#20272;&#26041;&#27861;&#26080;&#27861;&#32771;&#34385;&#25512;&#33616;&#21644;&#23454;&#38469;hashtag&#20043;&#38388;&#35821;&#20041;&#30456;&#20851;&#24615;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;#REVAL&#22312;&#25429;&#25417;&#35821;&#20041;&#30456;&#20851;&#24615;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#12290;</title><link>http://arxiv.org/abs/2305.18330</link><description>&lt;p&gt;
#REVAL&#65306;&#19968;&#31181;&#29992;&#20110;hashtag&#25512;&#33616;&#30340;&#35821;&#20041;&#35780;&#20272;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
#REVAL: a semantic evaluation framework for hashtag recommendation. (arXiv:2305.18330v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18330
&lt;/p&gt;
&lt;p&gt;
#REVAL&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#35821;&#20041;&#35780;&#20272;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#20256;&#32479;&#35780;&#20272;&#26041;&#27861;&#26080;&#27861;&#32771;&#34385;&#25512;&#33616;&#21644;&#23454;&#38469;hashtag&#20043;&#38388;&#35821;&#20041;&#30456;&#20851;&#24615;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;#REVAL&#22312;&#25429;&#25417;&#35821;&#20041;&#30456;&#20851;&#24615;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#22312;&#32447;&#31038;&#20132;&#32593;&#32476;&#31995;&#32479;&#20013;&#65292;&#33258;&#21160;&#35780;&#20272;hashtag&#25512;&#33616;&#27169;&#22411;&#26159;&#19968;&#39033;&#22522;&#26412;&#20219;&#21153;&#12290;&#20256;&#32479;&#30340;&#35780;&#20272;&#26041;&#27861;&#26159;&#39318;&#20808;&#27604;&#36739;&#31639;&#27861;&#25512;&#33616;&#30340;hashtag&#19982;&#23454;&#38469;&#30340;hashtag&#30340;&#31934;&#30830;&#23545;&#24212;&#20851;&#31995;&#65292;&#28982;&#21518;&#20351;&#29992;&#31934;&#30830;&#21305;&#37197;&#30340;&#25968;&#37327;&#35745;&#31639;&#21629;&#20013;&#29575;&#12289;&#21629;&#20013;&#27604;&#29575;&#12289;&#31934;&#30830;&#24230;&#12289;&#21484;&#22238;&#29575;&#25110;F1&#20998;&#25968;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#35780;&#20272;&#26041;&#24335;&#24573;&#30053;&#20102;&#25512;&#33616;&#21644;&#23454;&#38469;hashtag&#20043;&#38388;&#30340;&#35821;&#20041;&#30456;&#20851;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35821;&#20041;&#35780;&#20272;&#26694;&#26550;#REval&#65292;&#23427;&#21253;&#25324;&#19968;&#20010;&#31216;&#20026;BERTag&#30340;&#20869;&#37096;&#27169;&#22359;&#65292;&#21487;&#33258;&#21160;&#23398;&#20064;hashtag&#23884;&#20837;&#12290;&#25105;&#20204;&#20351;&#29992;&#25552;&#20986;&#30340;#REval-hit-ratio&#24230;&#37327;&#26631;&#20934;&#65292;&#30740;&#31350;&#20102;#REval&#26694;&#26550;&#22312;&#19981;&#21516;&#30340;&#35789;&#23884;&#20837;&#26041;&#27861;&#21644;&#25512;&#33616;&#20013;&#30340;&#21516;&#20041;&#35789;&#21644;hashtag&#25968;&#37327;&#19979;&#30340;&#24615;&#33021;&#12290;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#22312;&#25429;&#25417;&#25512;&#33616;&#21644;&#23454;&#38469;hashtag&#20043;&#38388;&#30340;&#35821;&#20041;&#30456;&#20851;&#24615;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic evaluation of hashtag recommendation models is a fundamental task in many online social network systems. In the traditional evaluation method, the recommended hashtags from an algorithm are firstly compared with the ground truth hashtags for exact correspondences. The number of exact matches is then used to calculate the hit rate, hit ratio, precision, recall, or F1-score. This way of evaluating hashtag similarities is inadequate as it ignores the semantic correlation between the recommended and ground truth hashtags. To tackle this problem, we propose a novel semantic evaluation framework for hashtag recommendation, called #REval. This framework includes an internal module referred to as BERTag, which automatically learns the hashtag embeddings. We investigate on how the #REval framework performs under different word embedding methods and different numbers of synonyms and hashtags in the recommendation using our proposed #REval-hit-ratio measure. Our experiments of the propo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31995;&#32479;&#32508;&#36848;&#20102;&#22312;&#20844;&#20849;&#26381;&#21153;&#39046;&#22495;&#20013;&#24212;&#29992;&#25991;&#26412;&#20998;&#26512;&#25216;&#33021;&#20197;&#25903;&#25345;&#20844;&#20849;&#26381;&#21153;&#21327;&#21516;&#21019;&#20316;&#30340;&#30740;&#31350;&#65292;&#21457;&#25496;&#20102;TA&#25216;&#26415;&#21644;&#20844;&#20849;&#26381;&#21153;&#30340;&#23545;&#20844;&#20849;&#20215;&#20540;&#30340;&#24433;&#21709;&#65292;&#20197;&#21450;&#22312;&#20844;&#20849;&#26381;&#21153;&#39046;&#22495;&#20013;TA&#25216;&#26415;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2305.18316</link><description>&lt;p&gt;
&#25991;&#26412;&#20998;&#26512;&#22312;&#20844;&#20849;&#26381;&#21153;&#21327;&#21516;&#21019;&#20316;&#20013;&#30340;&#24212;&#29992;&#65306;&#25991;&#29486;&#32508;&#36848;&#21644;&#30740;&#31350;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Application of Text Analytics in Public Service Co-Creation: Literature Review and Research Framework. (arXiv:2305.18316v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18316
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31995;&#32479;&#32508;&#36848;&#20102;&#22312;&#20844;&#20849;&#26381;&#21153;&#39046;&#22495;&#20013;&#24212;&#29992;&#25991;&#26412;&#20998;&#26512;&#25216;&#33021;&#20197;&#25903;&#25345;&#20844;&#20849;&#26381;&#21153;&#21327;&#21516;&#21019;&#20316;&#30340;&#30740;&#31350;&#65292;&#21457;&#25496;&#20102;TA&#25216;&#26415;&#21644;&#20844;&#20849;&#26381;&#21153;&#30340;&#23545;&#20844;&#20849;&#20215;&#20540;&#30340;&#24433;&#21709;&#65292;&#20197;&#21450;&#22312;&#20844;&#20849;&#26381;&#21153;&#39046;&#22495;&#20013;TA&#25216;&#26415;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20844;&#20849;&#37096;&#38376;&#38754;&#20020;&#30528;&#22810;&#20010;&#25361;&#25112;&#65292;&#20363;&#22914;&#26469;&#33258;&#20869;&#37096;&#22806;&#37096;&#30340;&#38656;&#27714;&#21464;&#21270;&#65292;&#20844;&#27665;&#23545;&#20844;&#20849;&#37096;&#38376;&#32452;&#32455;&#30340;&#19981;&#28385;&#21644;&#25387;&#36133;&#24863;&#65292;&#36825;&#20123;&#38382;&#39064;&#38656;&#35201;&#24471;&#21040;&#35299;&#20915;&#12290;&#20844;&#20849;&#26381;&#21153;&#21327;&#21516;&#21019;&#20316;&#26159;&#20256;&#32479;&#33258;&#19978;&#32780;&#19979;&#30340;&#20844;&#20849;&#26381;&#21153;&#24320;&#21457;&#30340;&#19968;&#31181;&#26367;&#20195;&#26041;&#26696;&#65292;&#23427;&#20419;&#36827;&#20102;&#21033;&#30410;&#30456;&#20851;&#32773;&#20043;&#38388;&#30340;&#21512;&#20316;&#65292;&#26088;&#22312;&#21019;&#24314;&#26356;&#22909;&#30340;&#20844;&#20849;&#26381;&#21153;&#24182;&#23454;&#29616;&#20844;&#20849;&#20215;&#20540;&#12290;&#21516;&#26102;&#65292;&#25991;&#26412;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#25512;&#21160;&#20102;&#25968;&#25454;&#20998;&#26512;&#30340;&#21457;&#23637;&#12290;&#23613;&#31649;&#21327;&#21516;&#21019;&#20316;&#21644;TA&#22312;&#31169;&#33829;&#37096;&#38376;&#20013;&#37117;&#24471;&#21040;&#20102;&#24212;&#29992;&#65292;&#20294;&#26412;&#25991;&#30740;&#31350;&#20102;&#29616;&#26377;&#30340;&#20851;&#20110;&#24212;&#29992;&#25991;&#26412;&#20998;&#26512;&#25216;&#26415;&#25903;&#25345;&#20844;&#20849;&#26381;&#21153;&#21327;&#21516;&#21019;&#20316;&#30340;&#30740;&#31350;&#65292;&#24182;&#23545;975&#31687;&#25991;&#31456;&#36827;&#34892;&#20102;&#31995;&#32479;&#32508;&#36848;&#65292;&#20998;&#26512;&#20102;TA&#25216;&#26415;&#21644;&#20844;&#20849;&#26381;&#21153;&#23545;&#20844;&#20849;&#20215;&#20540;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
The public sector faces several challenges, such as a number of external and internal demands for change, citizens' dissatisfaction and frustration with public sector organizations, that need to be addressed. An alternative to the traditional top-down development of public services is co-creation of public services. Co-creation promotes collaboration between stakeholders with the aim to create better public services and achieve public values. At the same time, data analytics has been fuelled by the availability of immense amounts of textual data. Whilst both co-creation and TA have been used in the private sector, we study existing works on the application of Text Analytics (TA) techniques on text data to support public service co-creation. We systematically review 75 of the 979 papers that focus directly or indirectly on the application of TA in the context of public service development. In our review, we analyze the TA techniques, the public service they support, public value outcome
&lt;/p&gt;</description></item><item><title>CDJUR-BR&#26159;&#19968;&#20221;&#31283;&#20581;&#30340;&#40644;&#37329;&#25910;&#34255;&#65292;&#21253;&#21547;&#24052;&#35199;&#21496;&#27861;&#25991;&#20214;&#20013;&#30340;&#31934;&#32454;&#21629;&#21517;&#23454;&#20307;&#65292;&#35813;&#25910;&#34255;&#28085;&#30422;&#21508;&#31181;&#27861;&#24459;&#31243;&#24207;&#25991;&#20214;&#65292;&#24182;&#26377;&#21161;&#20110;&#35299;&#20915;&#30446;&#21069;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#26080;&#27861;&#36731;&#32780;&#26131;&#20030;&#22320;&#35782;&#21035;&#27861;&#24459;&#23454;&#36341;&#25991;&#26412;&#20013;&#23454;&#20307;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.18315</link><description>&lt;p&gt;
CDJUR-BR -- &#24102;&#26377;&#31934;&#32454;&#21629;&#21517;&#23454;&#20307;&#30340;&#24052;&#35199;&#21496;&#27861;&#25991;&#20214;&#40644;&#37329;&#25910;&#34255;
&lt;/p&gt;
&lt;p&gt;
CDJUR-BR -- A Golden Collection of Legal Document from Brazilian Justice with Fine-Grained Named Entities. (arXiv:2305.18315v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18315
&lt;/p&gt;
&lt;p&gt;
CDJUR-BR&#26159;&#19968;&#20221;&#31283;&#20581;&#30340;&#40644;&#37329;&#25910;&#34255;&#65292;&#21253;&#21547;&#24052;&#35199;&#21496;&#27861;&#25991;&#20214;&#20013;&#30340;&#31934;&#32454;&#21629;&#21517;&#23454;&#20307;&#65292;&#35813;&#25910;&#34255;&#28085;&#30422;&#21508;&#31181;&#27861;&#24459;&#31243;&#24207;&#25991;&#20214;&#65292;&#24182;&#26377;&#21161;&#20110;&#35299;&#20915;&#30446;&#21069;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#26080;&#27861;&#36731;&#32780;&#26131;&#20030;&#22320;&#35782;&#21035;&#27861;&#24459;&#23454;&#36341;&#25991;&#26412;&#20013;&#23454;&#20307;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22823;&#22810;&#25968;&#27861;&#24459;&#20154;&#24037;&#26234;&#33021;&#65288;Legal AI&#65289;&#24212;&#29992;&#31243;&#24207;&#32780;&#35328;&#65292;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#26159;&#19968;&#39033;&#22522;&#26412;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#27861;&#24459;&#23454;&#36341;&#20013;&#20135;&#29983;&#30340;&#25991;&#26412;&#28041;&#21450;&#21040;&#30340;&#23454;&#20307;&#24182;&#38750;&#24403;&#21069;&#21487;&#29992;&#30340;NER&#36731;&#32780;&#26131;&#20030;&#22320;&#35782;&#21035;&#12290;&#32570;&#20047;&#27861;&#35268;&#12289;&#21028;&#20363;&#12289;&#35777;&#25454;&#12289;&#24809;&#32602;&#12289;&#27861;&#24459;&#31243;&#24207;&#20013;&#20154;&#20204;&#30340;&#35282;&#33394;&#65288;&#27861;&#23448;&#12289;&#24459;&#24072;&#12289;&#21463;&#23475;&#32773;&#12289;&#34987;&#21578;&#12289;&#35777;&#20154;&#65289;&#12289;&#20301;&#32622;&#31867;&#22411;&#65288;&#29359;&#32618;&#22320;&#28857;&#12289;&#34987;&#21578;&#22320;&#22336;&#65289;&#31561;&#30340;&#20998;&#31867;&#12290;&#22240;&#27492;&#65292;&#20173;&#38656;&#35201;&#19968;&#20010;&#29992;&#27861;&#24459;&#39046;&#22495;&#30340;&#31934;&#32454;&#23454;&#20307;&#36827;&#34892;&#27880;&#37322;&#30340;&#31283;&#20581;&#30340;&#40644;&#37329;&#25910;&#34255;&#65292;&#28085;&#30422;&#27861;&#24459;&#31243;&#24207;&#30340;&#21508;&#31181;&#25991;&#20214;&#65292;&#20363;&#22914;&#35831;&#24895;&#20070;&#12289;&#35843;&#26597;&#12289;&#25237;&#35785;&#12289;&#20915;&#23450;&#21644;&#21028;&#20915;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#24052;&#35199;&#21496;&#27861;&#40644;&#37329;&#25910;&#34255;&#65288;CDJUR-BR&#65289;&#30340;&#24320;&#21457;&#65292;&#35813;&#25910;&#34255;&#21253;&#21547;&#19968;&#32452;&#30001;&#27861;&#24459;&#25991;&#29486;&#19987;&#23478;&#27880;&#37322;&#30340;&#31934;&#32454;&#21629;&#21517;&#23454;&#20307;&#12290;&#21019;&#24314;CDJUR-BR&#36981;&#24490;&#20102;&#33258;&#24049;&#30340;
&lt;/p&gt;
&lt;p&gt;
A basic task for most Legal Artificial Intelligence (Legal AI) applications is Named Entity Recognition (NER). However, texts produced in the context of legal practice make references to entities that are not trivially recognized by the currently available NERs. There is a lack of categorization of legislation, jurisprudence, evidence, penalties, the roles of people in a legal process (judge, lawyer, victim, defendant, witness), types of locations (crime location, defendant's address), etc. In this sense, there is still a need for a robust golden collection, annotated with fine-grained entities of the legal domain, and which covers various documents of a legal process, such as petitions, inquiries, complaints, decisions and sentences. In this article, we describe the development of the Golden Collection of the Brazilian Judiciary (CDJUR-BR) contemplating a set of fine-grained named entities that have been annotated by experts in legal documents. The creation of CDJUR-BR followed its ow
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#30340;&#39118;&#38505;&#25935;&#24863;&#26597;&#35810;&#36873;&#25321;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#19968;&#32452;&#21487;&#33021;&#30340;&#31995;&#32479;&#37197;&#32622;&#20013;&#36873;&#25321;&#26368;&#20339;&#37197;&#32622;&#65292;&#20174;&#32780;&#25552;&#39640;&#25628;&#32034;&#25928;&#29575;&#21644;&#21033;&#29992;&#31995;&#32479;&#36164;&#28304;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.18311</link><description>&lt;p&gt;
&#38024;&#23545;&#31995;&#32479;&#37197;&#32622;&#30340;&#39118;&#38505;&#25935;&#24863;&#36873;&#25321;&#30340;&#36873;&#25321;&#24615;&#26597;&#35810;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
Selective Query Processing: a Risk-Sensitive Selection of System Configurations. (arXiv:2305.18311v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18311
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#30340;&#39118;&#38505;&#25935;&#24863;&#26597;&#35810;&#36873;&#25321;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#19968;&#32452;&#21487;&#33021;&#30340;&#31995;&#32479;&#37197;&#32622;&#20013;&#36873;&#25321;&#26368;&#20339;&#37197;&#32622;&#65292;&#20174;&#32780;&#25552;&#39640;&#25628;&#32034;&#25928;&#29575;&#21644;&#21033;&#29992;&#31995;&#32479;&#36164;&#28304;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#20013;&#65292;&#25628;&#32034;&#21442;&#25968;&#20250;&#26681;&#25454;&#19968;&#32452;&#36807;&#21435;&#30340;&#25628;&#32034;&#36827;&#34892;&#20248;&#21270;&#20197;&#30830;&#20445;&#39640;&#26377;&#25928;&#24615;&#65292;&#28982;&#21518;&#23558;&#36825;&#20123;&#20248;&#21270;&#21442;&#25968;&#29992;&#20316;&#25152;&#26377;&#21518;&#32493;&#26597;&#35810;&#30340;&#31995;&#32479;&#37197;&#32622;&#12290;&#28982;&#32780;&#65292;&#26356;&#22909;&#30340;&#26041;&#27861;&#26159;&#26681;&#25454;&#23454;&#38469;&#26597;&#35810;&#26469;&#36866;&#24212;&#21442;&#25968;&#12290; &#36873;&#25321;&#24615;&#26597;&#35810;&#25193;&#23637;&#26159;&#36825;&#31181;&#26041;&#27861;&#20043;&#19968;&#65292;&#20854;&#20013;&#31995;&#32479;&#33258;&#21160;&#20915;&#23450;&#26159;&#21542;&#25193;&#23637;&#26597;&#35810;&#65292;&#24418;&#25104;&#20004;&#31181;&#21487;&#33021;&#30340;&#31995;&#32479;&#37197;&#32622;&#12290;&#26368;&#36817;&#65292;&#36825;&#31181;&#26041;&#27861;&#25193;&#23637;&#21040;&#21253;&#25324;&#35768;&#22810;&#20854;&#20182;&#21442;&#25968;&#65292;&#20174;&#32780;&#23548;&#33268;&#35768;&#22810;&#21487;&#33021;&#30340;&#31995;&#32479;&#37197;&#32622;&#65292;&#20854;&#20013;&#31995;&#32479;&#22312;&#27599;&#20010;&#26597;&#35810;&#30340;&#22522;&#30784;&#19978;&#33258;&#21160;&#36873;&#25321;&#26368;&#20339;&#37197;&#32622;&#12290;&#20026;&#20102;&#30830;&#23450;&#22312;&#23454;&#38469;&#31995;&#32479;&#20013;&#22312;&#27599;&#20010;&#26597;&#35810;&#22522;&#30784;&#19978;&#20351;&#29992;&#30340;&#29702;&#24819;&#37197;&#32622;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#20854;&#20013;&#39044;&#20808;&#36873;&#25321;&#20102;&#26377;&#38480;&#25968;&#37327;&#30340;&#21487;&#33021;&#37197;&#32622;&#65292;&#28982;&#21518;&#22312;&#20803;&#25628;&#32034;&#24341;&#25806;&#20013;&#20351;&#29992;&#36825;&#20123;&#37197;&#32622;&#26469;&#30830;&#23450;&#27599;&#20010;&#26597;&#35810;&#30340;&#26368;&#20339;&#25628;&#32034;&#37197;&#32622;&#12290;&#25105;&#20204;&#20026;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#23450;&#20041;&#20102;&#19968;&#20010;&#39118;&#38505;&#25935;&#24863;&#30340;&#26597;&#35810;&#36873;&#25321;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#22522;&#20110;&#39044;&#23450;&#20041;&#30340;&#21487;&#33021;&#37197;&#32622;&#38598;&#30830;&#23450;&#32473;&#23450;&#26597;&#35810;&#30340;&#26368;&#20248;&#31995;&#32479;&#37197;&#32622;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#21033;&#29992;&#31995;&#32479;&#36164;&#28304;&#24182;&#25913;&#21892;&#25628;&#32034;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In information retrieval systems, search parameters are optimized to ensure high effectiveness based on a set of past searches and these optimized parameters are then used as the system configuration for all subsequent queries. A better approach, however, would be to adapt the parameters to fit the query at hand. Selective query expansion is one such an approach, in which the system decides automatically whether or not to expand the query, resulting in two possible system configurations. This approach was extended recently to include many other parameters, leading to many possible system configurations where the system automatically selects the best configuration on a per-query basis. To determine the ideal configurations to use on a per-query basis in real-world systems we developed a method in which a restricted number of possible configurations is pre-selected and then used in a meta-search engine that decides the best search configuration on a per query basis. We define a risk-sens
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22522;&#20110;&#22810;&#35270;&#35282;&#20132;&#20114;&#20027;&#39064;&#22238;&#24402;&#31639;&#27861;&#65288;MV-ICTR&#65289;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#22312;&#19981;&#21516;&#35270;&#35282;&#19979;&#21516;&#26102;&#32435;&#20837;&#35780;&#20998;&#21644;&#19978;&#19979;&#25991;&#20449;&#24687;&#26469;&#24314;&#27169;&#29289;&#21697;&#29305;&#23450;&#21151;&#33021;&#30340;&#30456;&#20851;&#24615;&#21644;&#29992;&#25143;&#30340;&#20010;&#20154;&#20559;&#22909;&#65292;&#37319;&#29992;&#22810;&#33218;&#32769;&#34382;&#26426;&#31574;&#30053;&#36827;&#34892;&#25345;&#32493;&#30340;&#22312;&#32447;&#20010;&#24615;&#21270;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#25968;&#25454;&#38598;&#19978;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.18306</link><description>&lt;p&gt;
&#22810;&#35270;&#35282;&#20132;&#20114;&#24335;&#21327;&#21516;&#36807;&#28388;
&lt;/p&gt;
&lt;p&gt;
Multi-View Interactive Collaborative Filtering. (arXiv:2305.18306v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18306
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22522;&#20110;&#22810;&#35270;&#35282;&#20132;&#20114;&#20027;&#39064;&#22238;&#24402;&#31639;&#27861;&#65288;MV-ICTR&#65289;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#22312;&#19981;&#21516;&#35270;&#35282;&#19979;&#21516;&#26102;&#32435;&#20837;&#35780;&#20998;&#21644;&#19978;&#19979;&#25991;&#20449;&#24687;&#26469;&#24314;&#27169;&#29289;&#21697;&#29305;&#23450;&#21151;&#33021;&#30340;&#30456;&#20851;&#24615;&#21644;&#29992;&#25143;&#30340;&#20010;&#20154;&#20559;&#22909;&#65292;&#37319;&#29992;&#22810;&#33218;&#32769;&#34382;&#26426;&#31574;&#30053;&#36827;&#34892;&#25345;&#32493;&#30340;&#22312;&#32447;&#20010;&#24615;&#21270;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#25968;&#25454;&#38598;&#19978;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#22330;&#26223;&#19979;&#65292;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#29992;&#25143;&#20132;&#20114;&#25968;&#25454;&#65288;&#22914;&#28857;&#20987;&#25110;&#35780;&#20998;&#65289;&#24448;&#24448;&#24456;&#23569;&#65292;&#29289;&#21697;&#30340;&#25442;&#25163;&#29575;&#65288;&#20363;&#22914;&#26032;&#25991;&#31456;&#12289;&#25307;&#32856;&#20449;&#24687;&#65289;&#24456;&#39640;&#12290;&#22240;&#27492;&#65292;&#38500;&#20102;&#29992;&#25143;-&#29289;&#21697;&#35780;&#20998;&#22806;&#65292;&#38598;&#25104;&#19978;&#19979;&#25991;&#8220;&#36793;&#8221;&#20449;&#24687;&#26159;&#38750;&#24120;&#21487;&#21462;&#30340;&#12290;&#34429;&#28982;&#23384;&#22312;&#21487;&#20197;&#21516;&#26102;&#22788;&#29702;&#35780;&#20998;&#21644;&#19978;&#19979;&#25991;&#25968;&#25454;&#30340;&#31639;&#27861;&#65292;&#20294;&#36825;&#20123;&#31639;&#27861;&#36890;&#24120;&#20165;&#33021;&#36827;&#34892;&#26679;&#26412;&#20869;&#25512;&#33616;&#65292;&#21463;&#21040;&#32500;&#24230;&#35781;&#21650;&#30340;&#38480;&#21046;&#65292;&#24182;&#19981;&#37319;&#29992;&#22810;&#33218;&#32769;&#34382;&#26426;&#65288;MAB&#65289;&#31574;&#30053;&#36827;&#34892;&#38271;&#26399;&#32047;&#31215;&#25910;&#30410;&#20248;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#35270;&#35282;&#20132;&#20114;&#20027;&#39064;&#22238;&#24402;&#65288;MV-ICTR&#65289;&#31639;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#37096;&#20998;&#22312;&#32447;&#28508;&#22312;&#22240;&#23376;&#25512;&#33616;&#31639;&#27861;&#65292;&#21516;&#26102;&#32435;&#20837;&#35780;&#20998;&#21644;&#19978;&#19979;&#25991;&#20449;&#24687;&#26469;&#24314;&#27169;&#29289;&#21697;&#29305;&#23450;&#21151;&#33021;&#30340;&#30456;&#20851;&#24615;&#21644;&#29992;&#25143;&#30340;&#20010;&#20154;&#20559;&#22909;&#65292;&#37319;&#29992;&#22810;&#33218;&#32769;&#34382;&#26426;&#31574;&#30053;&#36827;&#34892;&#25345;&#32493;&#22312;&#32447;&#20010;&#24615;&#21270;&#12290;&#35813;&#31639;&#27861;&#22312;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#26174;&#33879;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many scenarios, recommender system user interaction data such as clicks or ratings is sparse, and item turnover rates (e.g., new articles, job postings) high. Given this, the integration of contextual "side" information in addition to user-item ratings is highly desirable. Whilst there are algorithms that can handle both rating and contextual data simultaneously, these algorithms are typically limited to making only in-sample recommendations, suffer from the curse of dimensionality, and do not incorporate multi-armed bandit (MAB) policies for long-term cumulative reward optimization. We propose multi-view interactive topic regression (MV-ICTR) a novel partially online latent factor recommender algorithm that incorporates both rating and contextual information to model item-specific feature dependencies and users' personal preferences simultaneously, with multi-armed bandit policies for continued online personalization. The result is significantly increased performance on datasets wi
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#28508;&#22312;Bandits&#31639;&#27861;&#35299;&#20915;&#29992;&#25143;&#20919;&#21551;&#21160;&#38382;&#39064;&#65292;&#21516;&#26102;&#23454;&#29616;&#26356;&#39640;&#30340;&#20934;&#30830;&#29575;&#21644;&#26356;&#20302;&#30340;&#36951;&#25022;&#12290;</title><link>http://arxiv.org/abs/2305.18305</link><description>&lt;p&gt;
&#20351;&#29992;&#28508;&#22312;Bandits&#30340;&#39640;&#20934;&#30830;&#24230;&#21644;&#20302;&#36951;&#25022;&#29992;&#25143;&#20919;&#21551;&#21160;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
High Accuracy and Low Regret for User-Cold-Start Using Latent Bandits. (arXiv:2305.18305v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18305
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#28508;&#22312;Bandits&#31639;&#27861;&#35299;&#20915;&#29992;&#25143;&#20919;&#21551;&#21160;&#38382;&#39064;&#65292;&#21516;&#26102;&#23454;&#29616;&#26356;&#39640;&#30340;&#20934;&#30830;&#29575;&#21644;&#26356;&#20302;&#30340;&#36951;&#25022;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#28508;&#22312;Bandits&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#26032;&#29992;&#25143;&#21152;&#20837;&#25512;&#33616;&#31995;&#32479;&#26102;&#38754;&#20020;&#30340;&#20919;&#21551;&#21160;&#38382;&#39064;&#12290;&#36825;&#31181;&#26032;&#31639;&#27861;&#22312;&#21516;&#26102;&#23454;&#29616;&#26356;&#39640;&#30340;&#20934;&#30830;&#29575;&#21644;&#26356;&#20302;&#30340;&#36951;&#25022;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop a novel latent-bandit algorithm for tackling the cold-start problem for new users joining a recommender system. This new algorithm significantly outperforms the state of the art, simultaneously achieving both higher accuracy and lower regret.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#25968;&#23383;&#23402;&#29983;&#20307;&#22312;&#20803;&#23431;&#23449;&#20013;&#30340;&#37096;&#32626;&#65292;&#24182;&#20171;&#32461;&#20102;&#36890;&#36807;&#35821;&#20041;&#36890;&#20449;&#23454;&#29616;&#25968;&#23383;&#23402;&#29983;&#20307;&#19982;&#20803;&#23431;&#23449;&#32467;&#21512;&#30340;&#26694;&#26550;&#65292;&#20197;&#21450;&#20854;&#22312;&#26234;&#33021;&#24037;&#19994;&#24212;&#29992;&#20013;&#30340;&#22522;&#26412;&#21407;&#29702;&#21644;&#24615;&#33021;&#20248;&#21270;&#30340;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.18304</link><description>&lt;p&gt;
&#35821;&#20041;&#24863;&#30693;&#25968;&#23383;&#23402;&#29983;&#20307;&#22312;&#20803;&#23431;&#23449;&#20013;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Semantic-aware Digital Twin for Metaverse: A Comprehensive Review. (arXiv:2305.18304v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18304
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#25968;&#23383;&#23402;&#29983;&#20307;&#22312;&#20803;&#23431;&#23449;&#20013;&#30340;&#37096;&#32626;&#65292;&#24182;&#20171;&#32461;&#20102;&#36890;&#36807;&#35821;&#20041;&#36890;&#20449;&#23454;&#29616;&#25968;&#23383;&#23402;&#29983;&#20307;&#19982;&#20803;&#23431;&#23449;&#32467;&#21512;&#30340;&#26694;&#26550;&#65292;&#20197;&#21450;&#20854;&#22312;&#26234;&#33021;&#24037;&#19994;&#24212;&#29992;&#20013;&#30340;&#22522;&#26412;&#21407;&#29702;&#21644;&#24615;&#33021;&#20248;&#21270;&#30340;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22312;&#20803;&#23431;&#23449;&#20013;&#23454;&#29616;&#25968;&#23383;&#23402;&#29983;&#20307;&#30340;&#37096;&#32626;&#65292;&#25552;&#20986;&#20102;&#20855;&#26377;&#35821;&#20041;&#24863;&#30693;&#33021;&#21147;&#30340;&#33539;&#24335;&#65292;&#20197;&#23454;&#29616;&#20934;&#30830;&#30340;&#12289;&#38754;&#21521;&#20219;&#21153;&#30340;&#20449;&#24687;&#25552;&#21462;&#21644;&#20869;&#22312;&#30340;&#26234;&#33021;&#21270;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26694;&#26550;&#38656;&#35201;&#23558;&#20803;&#23431;&#23449;&#29615;&#22659;&#20013;&#30340;&#25152;&#26377;&#35774;&#22791;&#30452;&#25509;&#36830;&#25509;&#21040;&#35821;&#20041;&#27169;&#22411;&#20013;&#65292;&#20197;&#23454;&#29616;&#20449;&#24687;&#30340;&#24544;&#23454;&#35299;&#37322;&#12290;&#30456;&#21453;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#25968;&#23383;&#23402;&#29983;&#20307;&#26694;&#26550;&#65292;&#32771;&#34385;&#20102;&#26234;&#33021;&#24037;&#19994;&#24212;&#29992;&#65292;&#36890;&#36807;&#19982;&#20803;&#23431;&#23449;&#20351;&#33021;&#25216;&#26415;&#32467;&#21512;&#20351;&#29992;&#65292;&#23454;&#29616;&#20102;&#35821;&#20041;&#36890;&#20449;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#35821;&#20041;&#36890;&#20449;&#12289;&#20803;&#23431;&#23449;&#21644;&#25968;&#23383;&#23402;&#29983;&#20307;&#30340;&#22522;&#30784;&#30693;&#35782;&#65292;&#24182;&#22312;&#24037;&#19994;&#36710;&#38388;&#31649;&#29702;&#24212;&#29992;&#29992;&#20363;&#20013;&#23637;&#31034;&#20102;&#35813;&#26694;&#26550;&#30340;&#22522;&#26412;&#21407;&#29702;&#65292;&#20197;&#36890;&#36807;&#35821;&#20041;&#36890;&#20449;&#25913;&#21892;&#20854;&#24615;&#33021;&#12290;&#20171;&#32461;&#20102;&#36825;&#20123;&#25216;&#26415;&#19982;&#22522;&#26412;&#26550;&#26500;&#30340;&#38598;&#25104;&#20197;&#21450;&#23545;&#26410;&#26469;&#24037;&#19994;&#24212;&#29992;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
To facilitate the deployment of digital twins in Metaverse, the paradigm with semantic awareness has been proposed as a means for enabling accurate and task-oriented information extraction with inherent intelligence. However, this framework requires all devices in the Metaverse environment to be directly linked with the semantic model to enable faithful interpretation of messages. In contrast, this article introduces the digital twin framework, considering a smart industrial application, which enables semantic communication in conjugation with the Metaverse enabling technologies. The fundamentals of this framework are demonstrated on an industrial shopfloor management use case with a digital twin so as to improve its performance through semantic communication. An overview of semantic communication, Metaverse, and digital twins is presented. Integration of these technologies with the basic architecture as well as the impact on future industrial applications is presented. In a nutshell, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#30005;&#23376;&#21830;&#21153;&#20013;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#23454;&#29616;&#19968;&#33268;&#30340;&#25991;&#26412;&#20998;&#31867;&#30340;&#26032;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#26088;&#22312;&#25913;&#36827;&#20135;&#21697;&#20998;&#31867;&#27169;&#22411;&#30340;&#19968;&#33268;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#29983;&#20135;&#27700;&#24179;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.05402</link><description>&lt;p&gt;
&#22312;&#30005;&#23376;&#21830;&#21153;&#20013;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#23454;&#29616;&#19968;&#33268;&#30340;&#25991;&#26412;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Consistent Text Categorization using Data Augmentation in e-Commerce. (arXiv:2305.05402v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05402
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#30005;&#23376;&#21830;&#21153;&#20013;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#23454;&#29616;&#19968;&#33268;&#30340;&#25991;&#26412;&#20998;&#31867;&#30340;&#26032;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#26088;&#22312;&#25913;&#36827;&#20135;&#21697;&#20998;&#31867;&#27169;&#22411;&#30340;&#19968;&#33268;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#29983;&#20135;&#27700;&#24179;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#30005;&#23376;&#21830;&#21153;&#25968;&#25454;&#20998;&#31867;&#26159;&#19968;&#39033;&#20851;&#38190;&#30340;&#12289;&#24191;&#27867;&#24212;&#29992;&#20110;&#24037;&#19994;&#39046;&#22495;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#26088;&#22312;&#25913;&#36827;&#19968;&#23478;&#20027;&#35201;&#32593;&#32476;&#20844;&#21496;&#24050;&#32463;&#22312;&#20351;&#29992;&#30340;&#20135;&#21697;&#20998;&#31867;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#29992;&#20110;&#22810;&#31181;&#24212;&#29992;&#12290;&#22312;&#35813;&#27169;&#22411;&#26680;&#24515;&#20013;&#65292;&#20135;&#21697;&#20998;&#31867;&#27169;&#22411;&#26159;&#19968;&#20010;&#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#65292;&#25509;&#21463;&#20135;&#21697;&#26631;&#39064;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#20174;&#25968;&#21315;&#20010;&#21487;&#29992;&#20505;&#36873;&#39033;&#20013;&#36755;&#20986;&#26368;&#21512;&#36866;&#30340;&#31867;&#21035;&#12290;&#32463;&#36807;&#36827;&#19968;&#27493;&#35266;&#23519;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#31867;&#20284;&#29289;&#21697;&#26631;&#31614;&#19978;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;&#20363;&#22914;&#65292;&#26631;&#39064;&#20013;&#20851;&#20110;&#39068;&#33394;&#25110;&#23610;&#23544;&#30340;&#23567;&#21464;&#21270;&#65292;&#20250;&#23545;&#27169;&#22411;&#20135;&#29983;&#36739;&#22823;&#24433;&#21709;&#12290;&#36825;&#31181;&#29616;&#35937;&#21487;&#33021;&#20250;&#23545;&#19979;&#28216;&#30340;&#25512;&#33616;&#25110;&#25628;&#32034;&#24212;&#29992;&#36896;&#25104;&#36127;&#38754;&#24433;&#21709;&#65292;&#23548;&#33268;&#29992;&#25143;&#20307;&#39564;&#19979;&#38477;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#23454;&#29616;&#19968;&#33268;&#30340;&#25991;&#26412;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#25552;&#39640;&#27169;&#22411;&#30340;&#19968;&#33268;&#24615;&#65292;&#24182;&#20445;&#25345;&#20854;&#29983;&#20135;&#27700;&#24179;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The categorization of massive e-Commerce data is a crucial, well-studied task, which is prevalent in industrial settings. In this work, we aim to improve an existing product categorization model that is already in use by a major web company, serving multiple applications. At its core, the product categorization model is a text classification model that takes a product title as an input and outputs the most suitable category out of thousands of available candidates. Upon a closer inspection, we found inconsistencies in the labeling of similar items. For example, minor modifications of the product title pertaining to colors or measurements majorly impacted the model's output. This phenomenon can negatively affect downstream recommendation or search applications, leading to a sub-optimal user experience.  To address this issue, we propose a new framework for consistent text categorization. Our goal is to improve the model's consistency while maintaining its production-level performance. W
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;CTR&#39044;&#27979;&#30340;&#22686;&#24378;&#21452;&#27969;MLP&#27169;&#22411;&#65292;&#32463;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#20165;&#26159;&#31616;&#21333;&#22320;&#32467;&#21512;&#20004;&#20010;MLP&#23601;&#21487;&#20197;&#23454;&#29616;&#20196;&#20154;&#24778;&#35766;&#30340;&#33391;&#22909;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.00902</link><description>&lt;p&gt;
FinalMLP: &#29992;&#20110;CTR&#39044;&#27979;&#30340;&#22686;&#24378;&#21452;&#27969;MLP&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
FinalMLP: An Enhanced Two-Stream MLP Model for CTR Prediction. (arXiv:2304.00902v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00902
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;CTR&#39044;&#27979;&#30340;&#22686;&#24378;&#21452;&#27969;MLP&#27169;&#22411;&#65292;&#32463;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#20165;&#26159;&#31616;&#21333;&#22320;&#32467;&#21512;&#20004;&#20010;MLP&#23601;&#21487;&#20197;&#23454;&#29616;&#20196;&#20154;&#24778;&#35766;&#30340;&#33391;&#22909;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28857;&#20987;&#29575;&#39044;&#27979;&#26159;&#22312;&#32447;&#24191;&#21578;&#21644;&#25512;&#33616;&#20013;&#30340;&#22522;&#26412;&#20219;&#21153;&#20043;&#19968;&#12290;&#34429;&#28982;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#22312;&#35768;&#22810;&#28145;&#24230;CTR&#39044;&#27979;&#27169;&#22411;&#20013;&#20316;&#20026;&#26680;&#24515;&#32452;&#20214;&#65292;&#20294;&#24191;&#20026;&#20154;&#30693;&#30340;&#26159;&#65292;&#20165;&#24212;&#29992;&#19968;&#20010;&#22522;&#26412;MLP&#32593;&#32476;&#22312;&#23398;&#20064;&#20056;&#27861;&#29305;&#24449;&#30456;&#20114;&#20316;&#29992;&#26041;&#38754;&#24182;&#19981;&#39640;&#25928;&#12290;&#22240;&#27492;&#65292;&#35768;&#22810;&#20004;&#20010;&#27969;&#20132;&#20114;&#27169;&#22411;&#65288;&#20363;&#22914;&#65292;DeepFM&#21644;DCN&#65289;&#36890;&#36807;&#23558;MLP&#32593;&#32476;&#19982;&#21478;&#19968;&#20010;&#19987;&#29992;&#32593;&#32476;&#38598;&#25104;&#20197;&#22686;&#24378;CTR&#39044;&#27979;&#12290;&#30001;&#20110;MLP&#27969;&#38544;&#24335;&#22320;&#23398;&#20064;&#29305;&#24449;&#20132;&#20114;&#20316;&#29992;&#65292;&#22240;&#27492;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#20110;&#22686;&#24378;&#34917;&#20805;&#27969;&#20013;&#30340;&#26174;&#24335;&#29305;&#24449;&#20132;&#20114;&#20316;&#29992;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#30340;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#19968;&#20010;&#32463;&#36807;&#33391;&#22909;&#35843;&#25972;&#30340;&#21452;&#27969;MLP&#27169;&#22411;&#65292;&#23427;&#21482;&#26159;&#31616;&#21333;&#22320;&#32467;&#21512;&#20102;&#20004;&#20010;MLP&#65292;&#29978;&#33267;&#21487;&#20197;&#23454;&#29616;&#20196;&#20154;&#24778;&#35766;&#30340;&#33391;&#22909;&#24615;&#33021;&#65292;&#36825;&#22312;&#29616;&#26377;&#30340;&#24037;&#20316;&#20013;&#20174;&#26410;&#34987;&#25253;&#36947;&#36807;&#12290;&#22522;&#20110;&#36825;&#20010;&#35266;&#23519;&#32467;&#26524;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#29305;&#24449;&#36873;&#25321;&#21644;&#20132;&#20114;&#32858;&#21512;&#23618;&#12290;
&lt;/p&gt;
&lt;p&gt;
Click-through rate (CTR) prediction is one of the fundamental tasks for online advertising and recommendation. While multi-layer perceptron (MLP) serves as a core component in many deep CTR prediction models, it has been widely recognized that applying a vanilla MLP network alone is inefficient in learning multiplicative feature interactions. As such, many two-stream interaction models (e.g., DeepFM and DCN) have been proposed by integrating an MLP network with another dedicated network for enhanced CTR prediction. As the MLP stream learns feature interactions implicitly, existing research focuses mainly on enhancing explicit feature interactions in the complementary stream. In contrast, our empirical study shows that a well-tuned two-stream MLP model that simply combines two MLPs can even achieve surprisingly good performance, which has never been reported before by existing work. Based on this observation, we further propose feature selection and interaction aggregation layers that c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;CONE&#65292;&#19968;&#20010;&#39640;&#25928;&#30340;&#31895;-&#32454;&#23545;&#40784;&#26694;&#26550;&#65292;&#21487;&#29992;&#20110;&#38271;&#35270;&#39057;&#26102;&#38388;&#23450;&#20301;&#12290;CONE&#36890;&#36807;&#22522;&#20110;&#26597;&#35810;&#30340;&#31383;&#21475;&#36873;&#25321;&#31574;&#30053;&#21644;&#23545;&#27604;&#23398;&#20064;&#26426;&#21046;&#25552;&#21319;&#20102;&#22810;&#27169;&#24577;&#23545;&#40784;&#65292;&#24182;&#22312;&#20004;&#20010;&#22823;&#35268;&#27169;&#38271;&#35270;&#39057;&#26102;&#38388;&#23450;&#20301;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2209.10918</link><description>&lt;p&gt;
CONE&#65306;&#29992;&#20110;&#38271;&#35270;&#39057;&#26102;&#38388;&#23450;&#20301;&#30340;&#39640;&#25928;&#31895;-&#32454;&#23545;&#40784;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
CONE: An Efficient COarse-to-fiNE Alignment Framework for Long Video Temporal Grounding. (arXiv:2209.10918v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.10918
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;CONE&#65292;&#19968;&#20010;&#39640;&#25928;&#30340;&#31895;-&#32454;&#23545;&#40784;&#26694;&#26550;&#65292;&#21487;&#29992;&#20110;&#38271;&#35270;&#39057;&#26102;&#38388;&#23450;&#20301;&#12290;CONE&#36890;&#36807;&#22522;&#20110;&#26597;&#35810;&#30340;&#31383;&#21475;&#36873;&#25321;&#31574;&#30053;&#21644;&#23545;&#27604;&#23398;&#20064;&#26426;&#21046;&#25552;&#21319;&#20102;&#22810;&#27169;&#24577;&#23545;&#40784;&#65292;&#24182;&#22312;&#20004;&#20010;&#22823;&#35268;&#27169;&#38271;&#35270;&#39057;&#26102;&#38388;&#23450;&#20301;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#19968;&#20010;&#26032;&#20852;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#8212;&#8212;&#38271;&#35270;&#39057;&#26102;&#38388;&#23450;&#20301;&#65288;VTG&#65289;&#65292;&#21363;&#23450;&#20301;&#19982;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#30456;&#20851;&#30340;&#35270;&#39057;&#29255;&#27573;&#12290;&#30456;&#27604;&#20110;&#30701;&#35270;&#39057;&#65292;&#38271;&#35270;&#39057;&#21516;&#26679;&#38750;&#24120;&#21463;&#27426;&#36814;&#65292;&#20294;&#26159;&#25506;&#32034;&#36739;&#23569;&#65292;&#36825;&#24102;&#26469;&#20102;&#22810;&#20010;&#25361;&#25112;&#65292;&#20363;&#22914;&#26356;&#39640;&#30340;&#25512;&#29702;&#35745;&#31639;&#25104;&#26412;&#21644;&#24369;&#30340;&#22810;&#27169;&#24577;&#23545;&#40784;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CONE&#65292;&#19968;&#20010;&#39640;&#25928;&#30340;&#31895;-&#32454;&#23545;&#40784;&#26694;&#26550;&#12290;CONE&#26159;&#19968;&#20010;&#25554;&#25300;&#24335;&#30340;&#26694;&#26550;&#65292;&#21487;&#22312;&#29616;&#26377;&#30340;VTG&#27169;&#22411;&#19978;&#22788;&#29702;&#38271;&#35270;&#39057;&#65292;&#36890;&#36807;&#28369;&#21160;&#31383;&#21475;&#26426;&#21046;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;CONE&#65288;1&#65289;&#24341;&#20837;&#20102;&#22522;&#20110;&#26597;&#35810;&#30340;&#31383;&#21475;&#36873;&#25321;&#31574;&#30053;&#20197;&#21152;&#24555;&#25512;&#29702;&#36895;&#24230;&#65292;&#65288;2&#65289;&#25552;&#35758;&#20102;&#36890;&#36807;&#26032;&#22686;&#23545;&#27604;&#23398;&#20064;&#26469;&#22686;&#24378;&#38271;&#35270;&#39057;&#30340;&#22810;&#27169;&#24577;&#23545;&#40784;&#30340;&#31895;&#32454;&#26426;&#21046;&#12290;&#23545;&#20004;&#20010;&#22823;&#35268;&#27169;&#38271;VTG&#22522;&#20934;&#27979;&#35797;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#22343;&#34920;&#26126;&#65292;CONE&#22312;&#24615;&#33021;&#19978;&#37117;&#26377;&#24456;&#22823;&#25552;&#21319;&#65288;&#20363;&#22914;&#22312;MAD&#19978;&#20174;3.13&#65285;&#21040;6.87&#65285;&#65289;&#65292;&#24182;&#19988;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#20998;&#26512;&#20063;&#35777;&#26126;&#20102;CONE&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper tackles an emerging and challenging problem of long video temporal grounding~(VTG) that localizes video moments related to a natural language (NL) query. Compared with short videos, long videos are also highly demanded but less explored, which brings new challenges in higher inference computation cost and weaker multi-modal alignment. To address these challenges, we propose CONE, an efficient COarse-to-fiNE alignment framework. CONE is a plug-and-play framework on top of existing VTG models to handle long videos through a sliding window mechanism. Specifically, CONE (1) introduces a query-guided window selection strategy to speed up inference, and (2) proposes a coarse-to-fine mechanism via a novel incorporation of contrastive learning to enhance multi-modal alignment for long videos. Extensive experiments on two large-scale long VTG benchmarks consistently show both substantial performance gains (e.g., from 3.13% to 6.87% on MAD) and state-of-the-art results. Analyses also 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;LDA&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#36827;&#34892;&#25991;&#26412;&#25688;&#35201;&#65292;&#38024;&#23545;&#19982;&#22522;&#22240;&#21644;&#30142;&#30149;&#30456;&#20851;&#30340;&#21307;&#23398;&#31185;&#23398;&#26399;&#21002;&#25991;&#31456;&#36827;&#34892;&#30740;&#31350;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#33021;&#22815;&#20445;&#30041;&#20851;&#38190;&#20449;&#24687;&#24182;&#20445;&#25345;&#21407;&#22987;&#24847;&#20041;&#30340;&#21387;&#32553;&#29256;&#26412;&#65292;&#24182;&#20351;&#29992;PyLDAvis&#36827;&#34892;&#20132;&#20114;&#24335;&#21487;&#35270;&#21270;&#12290;</title><link>http://arxiv.org/abs/2207.14687</link><description>&lt;p&gt;
&#22522;&#20110;LDA&#20027;&#39064;&#24314;&#27169;&#30340;&#33258;&#21160;&#25991;&#26412;&#25688;&#35201;&#30340;&#25968;&#25454;&#39537;&#21160;&#28508;&#22312;&#35821;&#24847;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Data-driven Latent Semantic Analysis for Automatic Text Summarization using LDA Topic Modelling. (arXiv:2207.14687v7 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.14687
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;LDA&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#36827;&#34892;&#25991;&#26412;&#25688;&#35201;&#65292;&#38024;&#23545;&#19982;&#22522;&#22240;&#21644;&#30142;&#30149;&#30456;&#20851;&#30340;&#21307;&#23398;&#31185;&#23398;&#26399;&#21002;&#25991;&#31456;&#36827;&#34892;&#30740;&#31350;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#33021;&#22815;&#20445;&#30041;&#20851;&#38190;&#20449;&#24687;&#24182;&#20445;&#25345;&#21407;&#22987;&#24847;&#20041;&#30340;&#21387;&#32553;&#29256;&#26412;&#65292;&#24182;&#20351;&#29992;PyLDAvis&#36827;&#34892;&#20132;&#20114;&#24335;&#21487;&#35270;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29616;&#20195;&#26102;&#20195;&#22823;&#25968;&#25454;&#25366;&#25496;&#21644;&#22823;&#25991;&#26412;&#20998;&#26512;&#30340;&#21040;&#26469;&#21644;&#26222;&#21450;&#65292;&#33258;&#21160;&#25991;&#26412;&#25688;&#35201;&#25104;&#20026;&#20174;&#25991;&#26723;&#20013;&#25552;&#21462;&#21644;&#26816;&#32034;&#37325;&#35201;&#20449;&#24687;&#30340;&#37325;&#35201;&#25163;&#27573;&#12290;&#26412;&#30740;&#31350;&#20174;&#21333;&#20010;&#21644;&#22810;&#20010;&#25991;&#26723;&#30340;&#35282;&#24230;&#25506;&#35752;&#20102;&#33258;&#21160;&#25991;&#26412;&#25688;&#35201;&#30340;&#21508;&#20010;&#26041;&#38754;&#12290;&#25688;&#35201;&#26159;&#23558;&#24222;&#22823;&#30340;&#25991;&#26412;&#25991;&#31456;&#21387;&#32553;&#20026;&#30701;&#23567;&#27719;&#24635;&#29256;&#26412;&#30340;&#20219;&#21153;&#12290;&#30446;&#30340;&#26159;&#32553;&#23567;&#25991;&#26412;&#30340;&#22823;&#23567;&#65292;&#20294;&#35201;&#20445;&#30041;&#20851;&#38190;&#37325;&#35201;&#20449;&#24687;&#24182;&#20445;&#25345;&#21407;&#22987;&#25991;&#26723;&#30340;&#24847;&#20041;&#12290;&#8220;&#28508;&#22312;&#29380;&#21033;&#20811;&#38647;&#20998;&#37197;&#8221;&#65288;LDA&#65289;&#26041;&#27861;&#34987;&#29992;&#20110;&#20174;&#19982;&#22522;&#22240;&#21644;&#30142;&#30149;&#30456;&#20851;&#30340;&#21307;&#23398;&#31185;&#23398;&#26399;&#21002;&#25991;&#31456;&#20013;&#36827;&#34892;&#20027;&#39064;&#24314;&#27169;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#20351;&#29992;PyLDAvis&#30340;&#22522;&#20110;&#32593;&#39029;&#30340;&#20132;&#20114;&#24335;&#21487;&#35270;&#21270;&#24037;&#20855;&#26469;&#21487;&#35270;&#21270;&#25152;&#36873;&#20027;&#39064;&#12290;&#36825;&#31181;&#21487;&#35270;&#21270;&#25552;&#20379;&#20102;&#20027;&#35201;&#20027;&#39064;&#30340;&#24635;&#20307;&#35270;&#22270;&#65292;&#21516;&#26102;&#20801;&#35768;&#28145;&#20837;&#29702;&#35299;&#20010;&#20307;&#30340;&#26222;&#36941;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the advent and popularity of big data mining and huge text analysis in modern times, automated text summarization became prominent for extracting and retrieving important information from documents. This research investigates aspects of automatic text summarization from the perspectives of single and multiple documents. Summarization is a task of condensing huge text articles into short, summarized versions. The text is reduced in size for summarization purpose but preserving key vital information and retaining the meaning of the original document. This study presents the Latent Dirichlet Allocation (LDA) approach used to perform topic modelling from summarised medical science journal articles with topics related to genes and diseases. In this study, PyLDAvis web-based interactive visualization tool was used to visualise the selected topics. The visualisation provides an overarching view of the main topics while allowing and attributing deep meaning to the prevalence individual to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31895;&#21040;&#32454;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;&#20013;&#22810;&#31867;&#22411;&#22806;&#37096;&#25968;&#25454;&#30340;&#25968;&#25454;&#35821;&#20041;&#34701;&#21512;&#65292;&#22312;&#27492;&#22522;&#30784;&#19978;&#25552;&#39640;&#20102;&#25968;&#25454;&#30340;&#22788;&#29702;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2201.02732</link><description>&lt;p&gt;
C2-CRS&#65306;&#38754;&#21521;&#23545;&#35805;&#25512;&#33616;&#31995;&#32479;&#30340;&#31895;&#21040;&#32454;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
C2-CRS: Coarse-to-Fine Contrastive Learning for Conversational Recommender System. (arXiv:2201.02732v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.02732
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31895;&#21040;&#32454;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;&#20013;&#22810;&#31867;&#22411;&#22806;&#37096;&#25968;&#25454;&#30340;&#25968;&#25454;&#35821;&#20041;&#34701;&#21512;&#65292;&#22312;&#27492;&#22522;&#30784;&#19978;&#25552;&#39640;&#20102;&#25968;&#25454;&#30340;&#22788;&#29702;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;&#26088;&#22312;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#20132;&#20114;&#21521;&#29992;&#25143;&#25512;&#33616;&#36866;&#21512;&#30340;&#29289;&#21697;&#12290;&#20026;&#20102;&#24320;&#21457;&#26377;&#25928;&#30340;&#23545;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;&#65292;&#19968;&#20010;&#20027;&#35201;&#30340;&#25216;&#26415;&#38382;&#39064;&#26159;&#22914;&#20309;&#20174;&#38750;&#24120;&#26377;&#38480;&#30340;&#23545;&#35805;&#19978;&#19979;&#25991;&#20013;&#20934;&#30830;&#22320;&#25512;&#26029;&#29992;&#25143;&#20559;&#22909;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#32467;&#21512;&#22806;&#37096;&#25968;&#25454;&#26469;&#20016;&#23500;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#20026;&#19968;&#20123;&#29305;&#23450;&#31867;&#22411;&#30340;&#22806;&#37096;&#25968;&#25454;&#35774;&#35745;&#34701;&#21512;&#27169;&#22411;&#65292;&#36825;&#19981;&#36866;&#29992;&#20110;&#27169;&#22411;&#21644;&#21033;&#29992;&#22810;&#31867;&#22411;&#30340;&#22806;&#37096;&#25968;&#25454;&#12290;&#20026;&#20102;&#26377;&#25928;&#21033;&#29992;&#22810;&#31867;&#22411;&#30340;&#22806;&#37096;&#25968;&#25454;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31895;&#21040;&#32454;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#26469;&#25913;&#21892;&#23545;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;&#30340;&#25968;&#25454;&#35821;&#20041;&#34701;&#21512;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#20174;&#19981;&#21516;&#30340;&#25968;&#25454;&#20449;&#21495;&#20013;&#25552;&#21462;&#21644;&#34920;&#31034;&#22810;&#31181;&#31890;&#24230;&#30340;&#35821;&#20041;&#21333;&#20803;&#65292;&#28982;&#21518;&#20197;&#31895;&#21040;&#32454;&#30340;&#26041;&#24335;&#23545;&#40784;&#30456;&#20851;&#30340;&#22810;&#31181;&#35821;&#20041;&#21333;&#20803;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#26694;&#26550;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#31895;&#31890;&#24230;&#21644;&#32454;&#31890;&#24230;&#30340;&#36807;&#31243;&#26469;&#23545;&#29992;&#25143;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conversational recommender systems (CRS) aim to recommend suitable items to users through natural language conversations. For developing effective CRSs, a major technical issue is how to accurately infer user preference from very limited conversation context. To address issue, a promising solution is to incorporate external data for enriching the context information. However, prior studies mainly focus on designing fusion models tailored for some specific type of external data, which is not general to model and utilize multi-type external data.  To effectively leverage multi-type external data, we propose a novel coarse-to-fine contrastive learning framework to improve data semantic fusion for CRS. In our approach, we first extract and represent multi-grained semantic units from different data signals, and then align the associated multi-type semantic units in a coarse-to-fine way. To implement this framework, we design both coarse-grained and fine-grained procedures for modeling user 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#38544;&#31169;&#20445;&#25252;&#30340;&#26032;&#38395;&#25512;&#33616;&#12290;&#35813;&#26694;&#26550;&#23558;&#26032;&#38395;&#25512;&#33616;&#27169;&#22411;&#20998;&#35299;&#20026;&#22823;&#22411;&#26032;&#38395;&#27169;&#22411;&#21644;&#36731;&#37327;&#32423;&#29992;&#25143;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#26032;&#38395;&#34920;&#31034;&#21644;&#29992;&#25143;&#27169;&#22411;&#22312;&#26381;&#21153;&#22120;&#21644;&#23458;&#25143;&#31471;&#20043;&#38388;&#36827;&#34892;&#36890;&#20449;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#39640;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#30340;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2109.05446</link><description>&lt;p&gt;
&#38754;&#21521;&#38544;&#31169;&#20445;&#25252;&#26032;&#38395;&#25512;&#33616;&#30340;&#39640;&#25928;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Efficient-FedRec: Efficient Federated Learning Framework for Privacy-Preserving News Recommendation. (arXiv:2109.05446v3 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.05446
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#38544;&#31169;&#20445;&#25252;&#30340;&#26032;&#38395;&#25512;&#33616;&#12290;&#35813;&#26694;&#26550;&#23558;&#26032;&#38395;&#25512;&#33616;&#27169;&#22411;&#20998;&#35299;&#20026;&#22823;&#22411;&#26032;&#38395;&#27169;&#22411;&#21644;&#36731;&#37327;&#32423;&#29992;&#25143;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#26032;&#38395;&#34920;&#31034;&#21644;&#29992;&#25143;&#27169;&#22411;&#22312;&#26381;&#21153;&#22120;&#21644;&#23458;&#25143;&#31471;&#20043;&#38388;&#36827;&#34892;&#36890;&#20449;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#39640;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#38395;&#25512;&#33616;&#23545;&#20110;&#20010;&#24615;&#21270;&#26032;&#38395;&#35775;&#38382;&#33267;&#20851;&#37325;&#35201;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#26032;&#38395;&#25512;&#33616;&#26041;&#27861;&#20381;&#36182;&#20110;&#20013;&#24515;&#21270;&#23384;&#20648;&#29992;&#25143;&#21382;&#21490;&#26032;&#38395;&#28857;&#20987;&#34892;&#20026;&#25968;&#25454;&#65292;&#36825;&#21487;&#33021;&#20250;&#24341;&#21457;&#38544;&#31169;&#38382;&#39064;&#21644;&#21361;&#23475;&#12290;&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#38544;&#31169;&#20445;&#25252;&#26694;&#26550;&#65292;&#21487;&#29992;&#20110;&#22810;&#20010;&#23458;&#25143;&#31471;&#20849;&#21516;&#35757;&#32451;&#27169;&#22411;&#32780;&#26080;&#38656;&#20849;&#20139;&#20854;&#31169;&#26377;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#20197;&#32852;&#37030;&#26041;&#24335;&#30452;&#25509;&#23398;&#20064;&#35768;&#22810;&#29616;&#26377;&#30340;&#26032;&#38395;&#25512;&#33616;&#27169;&#22411;&#30340;&#35745;&#31639;&#21644;&#36890;&#20449;&#25104;&#26412;&#23545;&#20110;&#29992;&#25143;&#23458;&#25143;&#31471;&#26159;&#19981;&#21487;&#25509;&#21463;&#30340;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#29992;&#20110;&#38754;&#21521;&#38544;&#31169;&#20445;&#25252;&#30340;&#26032;&#38395;&#25512;&#33616;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;&#26032;&#38395;&#25512;&#33616;&#27169;&#22411;&#20998;&#35299;&#20026;&#26381;&#21153;&#22120;&#32500;&#25252;&#30340;&#22823;&#22411;&#26032;&#38395;&#27169;&#22411;&#21644;&#22312;&#26381;&#21153;&#22120;&#21644;&#23458;&#25143;&#31471;&#20043;&#38388;&#20849;&#20139;&#30340;&#36731;&#37327;&#32423;&#29992;&#25143;&#27169;&#22411;&#65292;&#20854;&#20013;&#26032;&#38395;&#34920;&#31034;&#21644;&#29992;&#25143;&#27169;&#22411;&#22312;&#26381;&#21153;&#22120;&#21644;&#23458;&#25143;&#31471;&#20043;&#38388;&#36827;&#34892;&#36890;&#20449;&#12290;&#28982;&#21518;&#65292;&#23458;&#25143;&#31471;&#35831;&#27714;&#26381;&#21153;&#22120;&#25552;&#20379;&#29992;&#25143;&#27169;&#22411;&#21644;&#26032;&#38395;&#34920;&#31034;&#65292;&#24182;&#22312;&#26412;&#22320;&#20351;&#29992;&#33258;&#24049;&#30340;&#31169;&#26377;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#25509;&#30528;&#65292;&#20182;&#20204;&#23558;&#26356;&#26032;&#21518;&#30340;&#29992;&#25143;&#27169;&#22411;&#19978;&#20256;&#21040;&#26381;&#21153;&#22120;&#65292;&#26381;&#21153;&#22120;&#23545;&#23427;&#20204;&#36827;&#34892;&#32858;&#21512;&#20197;&#26356;&#26032;&#38598;&#20013;&#24335;&#30340;&#26032;&#38395;&#27169;&#22411;&#12290;&#22312;&#20004;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#65292;&#26082;&#20445;&#25345;&#20102;&#29992;&#25143;&#38544;&#31169;&#65292;&#21516;&#26102;&#19982;&#29616;&#26377;&#30340;&#20013;&#24515;&#21270;&#21644;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#34920;&#29616;&#20986;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
News recommendation is critical for personalized news access. Most existing news recommendation methods rely on centralized storage of users' historical news click behavior data, which may lead to privacy concerns and hazards. Federated Learning is a privacy-preserving framework for multiple clients to collaboratively train models without sharing their private data. However, the computation and communication cost of directly learning many existing news recommendation models in a federated way are unacceptable for user clients. In this paper, we propose an efficient federated learning framework for privacy-preserving news recommendation. Instead of training and communicating the whole model, we decompose the news recommendation model into a large news model maintained in the server and a light-weight user model shared on both server and clients, where news representations and user model are communicated between server and clients. More specifically, the clients request the user model an
&lt;/p&gt;</description></item></channel></rss>