<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HASH-CODE&#30340;&#39640;&#39057;&#24863;&#30693;&#20998;&#23618;&#23545;&#27604;&#36873;&#25321;&#32534;&#30721;&#26041;&#27861;&#65292;&#23558;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#21644;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#30456;&#32467;&#21512;&#65292;&#35299;&#20915;&#20102;&#22312;&#25991;&#26412;&#23646;&#24615;&#22270;&#19978;&#33410;&#28857;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.16240</link><description>&lt;p&gt;
&#38024;&#23545;&#25991;&#26412;&#23646;&#24615;&#22270;&#30340;&#39640;&#39057;&#24863;&#30693;&#20998;&#23618;&#23545;&#27604;&#36873;&#25321;&#32534;&#30721;&#29992;&#20110;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
High-Frequency-aware Hierarchical Contrastive Selective Coding for Representation Learning on Text-attributed Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16240
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HASH-CODE&#30340;&#39640;&#39057;&#24863;&#30693;&#20998;&#23618;&#23545;&#27604;&#36873;&#25321;&#32534;&#30721;&#26041;&#27861;&#65292;&#23558;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#21644;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#30456;&#32467;&#21512;&#65292;&#35299;&#20915;&#20102;&#22312;&#25991;&#26412;&#23646;&#24615;&#22270;&#19978;&#33410;&#28857;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#25991;&#26412;&#23646;&#24615;&#22270;&#65288;TAGs&#65289;&#19978;&#30340;&#33410;&#28857;&#34920;&#31034;&#23398;&#20064;&#65292;&#20854;&#20013;&#33410;&#28857;&#20851;&#32852;&#26377;&#25991;&#26412;&#20449;&#24687;&#12290;&#23613;&#31649;&#26368;&#36817;&#20851;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#21644;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#30340;&#30740;&#31350;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#32534;&#30721;&#32593;&#32476;&#21644;&#25991;&#26412;&#20449;&#21495;&#26041;&#38754;&#30340;&#24378;&#22823;&#33021;&#21147;&#65292;&#20294;&#23545;&#20110;&#31934;&#32454;&#22320;&#23558;&#36825;&#20004;&#31181;&#27169;&#22411;&#32806;&#21512;&#22312;TAGs&#19978;&#30340;&#27880;&#24847;&#21147;&#36739;&#23569;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#29616;&#26377;&#30340;GNNs&#24456;&#23569;&#20197;&#19968;&#31181;&#24773;&#22659;&#21270;&#30340;&#26041;&#24335;&#23545;&#27599;&#20010;&#33410;&#28857;&#20013;&#30340;&#25991;&#26412;&#36827;&#34892;&#24314;&#27169;&#65307;&#29616;&#26377;&#30340;PLMs&#30001;&#20110;&#20854;&#24207;&#21015;&#26550;&#26500;&#65292;&#20960;&#20046;&#26080;&#27861;&#24212;&#29992;&#20110;&#34920;&#24449;&#22270;&#32467;&#26500;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HASH-CODE&#65292;&#19968;&#31181;&#39640;&#39057;&#24863;&#30693;&#30340;&#35889;&#20998;&#23618;&#23545;&#27604;&#36873;&#25321;&#32534;&#30721;&#26041;&#27861;&#65292;&#23558;GNNs&#21644;PLMs&#25972;&#21512;&#21040;&#32479;&#19968;&#27169;&#22411;&#20013;&#12290;&#19982;&#20043;&#21069;&#30340;&#8220;&#32423;&#32852;&#26550;&#26500;&#8221;&#19981;&#21516;&#65292;&#30452;&#25509;&#22312;PLM&#20043;&#19978;&#28155;&#21152;GNN&#23618;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;HASH-CODE&#20381;&#38752;&#20116;&#20010;&#33258;&#30417;&#30563;&#20248;&#21270;&#30446;&#26631;&#65292;&#20197;&#20419;&#36827;&#24443;&#24213;&#30340;&#30456;&#20114;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16240v1 Announce Type: new  Abstract: We investigate node representation learning on text-attributed graphs (TAGs), where nodes are associated with text information. Although recent studies on graph neural networks (GNNs) and pretrained language models (PLMs) have exhibited their power in encoding network and text signals, respectively, less attention has been paid to delicately coupling these two types of models on TAGs. Specifically, existing GNNs rarely model text in each node in a contextualized way; existing PLMs can hardly be applied to characterize graph structures due to their sequence architecture. To address these challenges, we propose HASH-CODE, a High-frequency Aware Spectral Hierarchical Contrastive Selective Coding method that integrates GNNs and PLMs into a unified model. Different from previous "cascaded architectures" that directly add GNN layers upon a PLM, our HASH-CODE relies on five self-supervised optimization objectives to facilitate thorough mutual e
&lt;/p&gt;</description></item><item><title>&#20132;&#20114;&#24335;&#38382;&#31572;&#31995;&#32479;&#26159;&#38382;&#31572;&#21644;&#23545;&#35805;&#31995;&#32479;&#30340;&#32467;&#21512;&#65292;&#29992;&#25143;&#21487;&#20197;&#29992;&#33258;&#28982;&#35821;&#35328;&#25552;&#38382;&#24182;&#19982;&#31995;&#32479;&#21160;&#24577;&#20132;&#20114;&#65292;&#33719;&#24471;&#26356;&#31934;&#30830;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2209.01621</link><description>&lt;p&gt;
&#20132;&#20114;&#24335;&#38382;&#31572;&#31995;&#32479;&#65306;&#25991;&#29486;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Interactive Question Answering Systems: Literature Review
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2209.01621
&lt;/p&gt;
&lt;p&gt;
&#20132;&#20114;&#24335;&#38382;&#31572;&#31995;&#32479;&#26159;&#38382;&#31572;&#21644;&#23545;&#35805;&#31995;&#32479;&#30340;&#32467;&#21512;&#65292;&#29992;&#25143;&#21487;&#20197;&#29992;&#33258;&#28982;&#35821;&#35328;&#25552;&#38382;&#24182;&#19982;&#31995;&#32479;&#21160;&#24577;&#20132;&#20114;&#65292;&#33719;&#24471;&#26356;&#31934;&#30830;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2209.01621v2 &#20844;&#21578;&#31867;&#22411;: &#26367;&#25442;-&#36328;  &#25688;&#35201;: &#38382;&#31572;&#31995;&#32479;&#34987;&#20844;&#35748;&#20026;&#22312;&#32593;&#32476;&#19978;&#23547;&#27714;&#20449;&#24687;&#30340;&#27969;&#34892;&#19988;&#26377;&#25928;&#30340;&#25163;&#27573;&#12290;&#22312;&#36825;&#31181;&#31995;&#32479;&#20013;&#65292;&#20449;&#24687;&#23547;&#25214;&#32773;&#21487;&#20197;&#36890;&#36807;&#29992;&#33258;&#28982;&#35821;&#35328;&#25552;&#20986;&#38382;&#39064;&#26469;&#33719;&#24471;&#31616;&#27905;&#30340;&#22238;&#31572;&#12290;&#20132;&#20114;&#24335;&#38382;&#31572;&#26159;&#26368;&#36817;&#25552;&#20986;&#30340;&#24182;&#36234;&#26469;&#36234;&#27969;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20301;&#20110;&#38382;&#31572;&#21644;&#23545;&#35805;&#31995;&#32479;&#30340;&#20132;&#38598;&#22788;&#12290;&#19968;&#26041;&#38754;&#65292;&#29992;&#25143;&#21487;&#20197;&#29992;&#26222;&#36890;&#35821;&#35328;&#25552;&#38382;&#24182;&#25214;&#21040;&#22905;&#38382;&#39064;&#30340;&#23454;&#38469;&#22238;&#31572;&#65307;&#21478;&#19968;&#26041;&#38754;&#65292;&#22914;&#26524;&#21021;&#22987;&#35831;&#27714;&#20013;&#23384;&#22312;&#22810;&#20010;&#21487;&#33021;&#30340;&#22238;&#22797;&#12289;&#24456;&#23569;&#25110;&#27169;&#26865;&#20004;&#21487;&#65292;&#31995;&#32479;&#21487;&#20197;&#23558;&#38382;&#31572;&#20250;&#35805;&#24310;&#38271;&#20026;&#23545;&#35805;&#12290;&#36890;&#36807;&#20801;&#35768;&#29992;&#25143;&#25552;&#20986;&#26356;&#22810;&#38382;&#39064;&#65292;&#20132;&#20114;&#24335;&#38382;&#31572;&#20351;&#29992;&#25143;&#33021;&#22815;&#21160;&#24577;&#22320;&#19982;&#31995;&#32479;&#20132;&#20114;&#24182;&#33719;&#24471;&#26356;&#31934;&#30830;&#30340;&#32467;&#26524;&#12290;&#26412;&#32508;&#36848;&#25552;&#20379;&#20102;&#20132;&#20114;&#24335;&#38382;&#31572;&#31995;&#32479;&#30340;&#35814;&#32454;&#27010;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2209.01621v2 Announce Type: replace-cross  Abstract: Question answering systems are recognized as popular and frequently effective means of information seeking on the web. In such systems, information seekers can receive a concise response to their query by presenting their questions in natural language. Interactive question answering is a recently proposed and increasingly popular solution that resides at the intersection of question answering and dialogue systems. On the one hand, the user can ask questions in normal language and locate the actual response to her inquiry; on the other hand, the system can prolong the question-answering session into a dialogue if there are multiple probable replies, very few, or ambiguities in the initial request. By permitting the user to ask more questions, interactive question answering enables users to dynamically interact with the system and receive more precise results. This survey offers a detailed overview of the interactive question-ans
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Transformer&#21644;&#33258;&#33976;&#39311;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#22823;&#37327;&#26080;&#26631;&#31614;&#30340;3D&#28857;&#38598;&#20013;&#33719;&#21462;&#20934;&#30830;&#19988;&#26059;&#36716;&#19981;&#21464;&#30340;3D&#28857;&#38598;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2308.04725</link><description>&lt;p&gt;
&#20351;&#29992;Transformer&#21644;&#33258;&#33976;&#39311;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26059;&#36716;&#19981;&#21464;&#30340;3D&#28857;&#38598;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Self-supervised Learning of Rotation-invariant 3D Point Set Features using Transformer and its Self-distillation. (arXiv:2308.04725v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04725
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Transformer&#21644;&#33258;&#33976;&#39311;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#22823;&#37327;&#26080;&#26631;&#31614;&#30340;3D&#28857;&#38598;&#20013;&#33719;&#21462;&#20934;&#30830;&#19988;&#26059;&#36716;&#19981;&#21464;&#30340;3D&#28857;&#38598;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20998;&#26512;3D&#28857;&#38598;&#25968;&#25454;&#20013;&#65292;3D&#29289;&#20307;&#30340;&#26059;&#36716;&#19981;&#21464;&#24615;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#23646;&#24615;&#12290;&#20256;&#32479;&#30340;&#20855;&#26377;&#26059;&#36716;&#19981;&#21464;&#24615;&#30340;3D&#28857;&#38598;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36890;&#24120;&#36890;&#36807;&#20351;&#29992;&#26377;&#26631;&#31614;&#30340;3D&#28857;&#38598;&#20316;&#20026;&#35757;&#32451;&#26679;&#26412;&#65292;&#36890;&#36807;&#30417;&#30563;&#23398;&#20064;&#33719;&#21462;&#20934;&#30830;&#30340;3D&#24418;&#29366;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;3D&#28857;&#38598;&#25968;&#25454;&#30340;&#24555;&#36895;&#22686;&#38271;&#21644;&#26631;&#27880;&#30340;&#39640;&#25104;&#26412;&#65292;&#38656;&#35201;&#19968;&#20010;&#20174;&#22823;&#37327;&#26080;&#26631;&#31614;&#30340;3D&#28857;&#38598;&#20013;&#23398;&#20064;&#26059;&#36716;&#19981;&#21464;&#30340;3D&#24418;&#29366;&#29305;&#24449;&#30340;&#26694;&#26550;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#23545;&#35937;&#32423;&#21035;&#33719;&#21462;&#20934;&#30830;&#19988;&#26059;&#36716;&#19981;&#21464;&#30340;3D&#28857;&#38598;&#29305;&#24449;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#36731;&#37327;&#32423;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#23558;&#36755;&#20837;&#30340;3D&#28857;&#38598;&#20998;&#35299;&#20026;&#22810;&#20010;&#20840;&#23616;&#23610;&#24230;&#30340;&#21306;&#22495;&#65288;&#31216;&#20026;tokens&#65289;&#65292;&#36825;&#20123;&#21306;&#22495;&#20445;&#30041;&#20102;&#26500;&#25104;3D&#23545;&#35937;&#30340;&#23616;&#37096;&#24418;&#29366;&#30340;&#31354;&#38388;&#24067;&#23616;&#12290;&#25105;&#20204;&#20351;&#29992;&#33258;&#27880;&#24847;&#26426;&#21046;&#26469;&#25913;&#36827;tokens&#65292;&#24182;&#23558;&#23427;&#20204;&#32858;&#21512;&#25104;&#27599;&#20010;3D&#28857;&#38598;&#30340;&#34920;&#36798;&#24615;&#26059;&#36716;&#19981;&#21464;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36890;&#36807;&#33258;&#33976;&#39311;&#26426;&#21046;&#36827;&#34892;&#26377;&#25928;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Invariance against rotations of 3D objects is an important property in analyzing 3D point set data. Conventional 3D point set DNNs having rotation invariance typically obtain accurate 3D shape features via supervised learning by using labeled 3D point sets as training samples. However, due to the rapid increase in 3D point set data and the high cost of labeling, a framework to learn rotation-invariant 3D shape features from numerous unlabeled 3D point sets is required. This paper proposes a novel self-supervised learning framework for acquiring accurate and rotation-invariant 3D point set features at object-level. Our proposed lightweight DNN architecture decomposes an input 3D point set into multiple global-scale regions, called tokens, that preserve the spatial layout of partial shapes composing the 3D object. We employ a self-attention mechanism to refine the tokens and aggregate them into an expressive rotation-invariant feature per 3D point set. Our DNN is effectively trained by u
&lt;/p&gt;</description></item></channel></rss>