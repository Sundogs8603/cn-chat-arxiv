<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#29992;&#25143;&#21442;&#19982;&#32676;&#32452;&#30340;&#28608;&#21169;&#24847;&#22270;&#65292;&#21253;&#25324;&#31038;&#20132;&#24847;&#22270;&#21644;&#20010;&#20154;&#20852;&#36259;&#24847;&#22270;&#65292;&#24182;&#25552;&#20986;&#20102;&#21452;&#37325;&#24847;&#22270;&#22270;&#27169;&#22411;&#26469;&#36827;&#34892;&#29992;&#25143;&#20013;&#24515;&#30340;&#32676;&#32452;&#21457;&#29616;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2308.05013</link><description>&lt;p&gt;
&#29992;&#25143;&#20013;&#24515;&#30340;&#32676;&#32452;&#21457;&#29616;&#30340;&#21452;&#37325;&#24847;&#22270;&#22270;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Dual Intents Graph Modeling for User-centric Group Discovery. (arXiv:2308.05013v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05013
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#29992;&#25143;&#21442;&#19982;&#32676;&#32452;&#30340;&#28608;&#21169;&#24847;&#22270;&#65292;&#21253;&#25324;&#31038;&#20132;&#24847;&#22270;&#21644;&#20010;&#20154;&#20852;&#36259;&#24847;&#22270;&#65292;&#24182;&#25552;&#20986;&#20102;&#21452;&#37325;&#24847;&#22270;&#22270;&#27169;&#22411;&#26469;&#36827;&#34892;&#29992;&#25143;&#20013;&#24515;&#30340;&#32676;&#32452;&#21457;&#29616;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#32676;&#32452;&#36234;&#26469;&#36234;&#26222;&#36941;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#20102;&#20998;&#20139;&#32463;&#39564;&#21644;&#25506;&#32034;&#20852;&#36259;&#30340;&#31354;&#38388;&#12290;&#22240;&#27492;&#65292;&#29992;&#25143;&#20013;&#24515;&#30340;&#32676;&#32452;&#21457;&#29616;&#20219;&#21153;&#65292;&#21363;&#21521;&#29992;&#25143;&#25512;&#33616;&#32676;&#32452;&#65292;&#21487;&#20197;&#24110;&#21161;&#29992;&#25143;&#30340;&#22312;&#32447;&#20307;&#39564;&#21644;&#24179;&#21488;&#30340;&#38271;&#26399;&#21457;&#23637;&#12290;&#29616;&#26377;&#30340;&#25512;&#33616;&#26041;&#27861;&#19981;&#33021;&#22788;&#29702;&#36825;&#20010;&#20219;&#21153;&#65292;&#22240;&#20026;&#23558;&#29992;&#25143;-&#32676;&#32452;&#21442;&#19982;&#24314;&#27169;&#25104;&#19968;&#20010;&#20108;&#37096;&#22270;&#24573;&#35270;&#20102;&#20182;&#20204;&#30340;&#39033;&#30446;&#20391;&#20852;&#36259;&#12290;&#34429;&#28982;&#26377;&#19968;&#20123;&#20316;&#21697;&#35797;&#22270;&#35299;&#20915;&#36825;&#20010;&#20219;&#21153;&#65292;&#20294;&#20173;&#28982;&#19981;&#36275;&#20197;&#23436;&#20840;&#20445;&#30041;&#31038;&#20132;&#19978;&#19979;&#25991;&#24182;&#30830;&#20445;&#26377;&#25928;&#30340;&#20852;&#36259;&#34920;&#31034;&#23398;&#20064;&#12290;&#26412;&#25991;&#37325;&#28857;&#30740;&#31350;&#28608;&#21169;&#29992;&#25143;&#21442;&#19982;&#32676;&#32452;&#30340;&#24847;&#22270;&#65292;&#36825;&#20123;&#24847;&#22270;&#21487;&#20197;&#20998;&#20026;&#19981;&#21516;&#31867;&#22411;&#65292;&#22914;&#31038;&#20132;&#24847;&#22270;&#21644;&#20010;&#20154;&#20852;&#36259;&#24847;&#22270;&#12290;&#21069;&#32773;&#25351;&#30340;&#26159;&#29992;&#25143;&#21152;&#20837;&#32676;&#32452;&#21463;&#21040;&#20182;&#20204;&#30340;&#31038;&#20132;&#20851;&#31995;&#30340;&#24433;&#21709;&#65292;&#32780;&#21518;&#32773;&#25351;&#30340;&#26159;&#29992;&#25143;&#19982;&#24535;&#21516;&#36947;&#21512;&#30340;&#20154;&#19968;&#36215;&#21152;&#20837;&#32676;&#32452;&#36827;&#34892;&#33258;&#25105;&#20139;&#21463;&#12290;&#20026;&#20102;&#29702;&#35299;&#36825;&#20123;&#24847;&#22270;
&lt;/p&gt;
&lt;p&gt;
Online groups have become increasingly prevalent, providing users with space to share experiences and explore interests. Therefore, user-centric group discovery task, i.e., recommending groups to users can help both users' online experiences and platforms' long-term developments. Existing recommender methods can not deal with this task as modeling user-group participation into a bipartite graph overlooks their item-side interests. Although there exist a few works attempting to address this task, they still fall short in fully preserving the social context and ensuring effective interest representation learning.  In this paper, we focus on exploring the intents that motivate users to participate in groups, which can be categorized into different types, like the social-intent and the personal interest-intent. The former refers to users joining a group affected by their social links, while the latter relates to users joining groups with like-minded people for self-enjoyment. To comprehend
&lt;/p&gt;</description></item><item><title>LLaMA-E&#26159;&#19968;&#31181;&#32479;&#19968;&#19988;&#23450;&#21046;&#30340;&#25351;&#23548;&#35821;&#35328;&#27169;&#22411;&#65292;&#26088;&#22312;&#35299;&#20915;&#30005;&#23376;&#21830;&#21153;&#21019;&#20316;&#36807;&#31243;&#20013;&#36935;&#21040;&#30340;&#21508;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#24191;&#21578;&#29983;&#25104;&#12289;&#26597;&#35810;&#22686;&#24378;&#30340;&#20135;&#21697;&#26631;&#39064;&#25913;&#20889;&#12289;&#20135;&#21697;&#20998;&#31867;&#12289;&#36141;&#20080;&#24847;&#22270;&#25512;&#27979;&#21644;&#24120;&#35268;&#38382;&#31572;&#12290;</title><link>http://arxiv.org/abs/2308.04913</link><description>&lt;p&gt;
LLaMA-E&#65306;&#22810;&#26041;&#38754;&#25351;&#23548;&#19979;&#30340;&#30005;&#23376;&#21830;&#21153;&#21019;&#20316;&#22686;&#24378;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
LLaMA-E: Empowering E-commerce Authoring with Multi-Aspect Instruction Following. (arXiv:2308.04913v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04913
&lt;/p&gt;
&lt;p&gt;
LLaMA-E&#26159;&#19968;&#31181;&#32479;&#19968;&#19988;&#23450;&#21046;&#30340;&#25351;&#23548;&#35821;&#35328;&#27169;&#22411;&#65292;&#26088;&#22312;&#35299;&#20915;&#30005;&#23376;&#21830;&#21153;&#21019;&#20316;&#36807;&#31243;&#20013;&#36935;&#21040;&#30340;&#21508;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#24191;&#21578;&#29983;&#25104;&#12289;&#26597;&#35810;&#22686;&#24378;&#30340;&#20135;&#21697;&#26631;&#39064;&#25913;&#20889;&#12289;&#20135;&#21697;&#20998;&#31867;&#12289;&#36141;&#20080;&#24847;&#22270;&#25512;&#27979;&#21644;&#24120;&#35268;&#38382;&#31572;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#21830;&#21153;&#21019;&#20316;&#28041;&#21450;&#21019;&#24314;&#21560;&#24341;&#20154;&#12289;&#20016;&#23500;&#19988;&#26377;&#38024;&#23545;&#24615;&#30340;&#20419;&#38144;&#20869;&#23481;&#65292;&#20197;&#25512;&#21160;&#20135;&#21697;&#38144;&#21806;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20986;&#29616;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#33539;&#20363;&#65292;&#20026;&#35299;&#20915;&#36825;&#31181;&#24773;&#26223;&#20013;&#30340;&#21508;&#31181;&#21019;&#20316;&#20219;&#21153;&#25552;&#20379;&#20102;&#32479;&#19968;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#36890;&#29992;&#35821;&#26009;&#24211;&#21644;&#24120;&#35782;&#30693;&#35782;&#35757;&#32451;&#30340;&#20027;&#27969;LLM&#22312;&#36866;&#24212;&#30005;&#23376;&#21830;&#21153;&#20135;&#21697;&#21644;&#23458;&#25143;&#29420;&#29305;&#30340;&#22797;&#26434;&#21644;&#20010;&#24615;&#21270;&#29305;&#24449;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#27492;&#22806;&#65292;&#20687;GPT-3.5&#36825;&#26679;&#30340;LLM&#38656;&#35201;&#36827;&#34892;&#36828;&#31243;&#35775;&#38382;&#65292;&#24341;&#21457;&#20102;&#22312;&#20256;&#36755;&#36807;&#31243;&#20013;&#20445;&#25252;&#22823;&#37327;&#23458;&#25143;&#38544;&#31169;&#25968;&#25454;&#30340;&#25285;&#24551;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;LLaMA-E&#65292;&#38024;&#23545;&#22810;&#26679;&#21270;&#30340;&#30005;&#23376;&#21830;&#21153;&#21019;&#20316;&#20219;&#21153;&#30340;&#32479;&#19968;&#19988;&#23450;&#21046;&#30340;&#25351;&#23548;&#35821;&#35328;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#39046;&#22495;&#19987;&#23478;&#20174;&#24191;&#21578;&#29983;&#25104;&#12289;&#26597;&#35810;&#22686;&#24378;&#30340;&#20135;&#21697;&#26631;&#39064;&#25913;&#20889;&#12289;&#20135;&#21697;&#20998;&#31867;&#12289;&#36141;&#20080;&#24847;&#22270;&#25512;&#27979;&#21644;&#24120;&#35268;&#38382;&#31572;&#31561;&#20219;&#21153;&#20013;&#21019;&#24314;&#20102;&#31181;&#23376;&#25351;&#23548;&#38598;&#21512;&#12290;&#36825;&#20123;&#20219;&#21153;&#33021;&#22815;...
&lt;/p&gt;
&lt;p&gt;
E-commerce authoring involves creating attractive, abundant, and targeted promotional content to drive product sales. The emergence of large language models (LLMs) introduces an innovative paradigm, offering a unified solution to address various authoring tasks within this scenario. However, mainstream LLMs trained on general corpora with common sense knowledge reveal limitations in fitting complex and personalized features unique to e-commerce products and customers. Furthermore, LLMs like GPT-3.5 necessitate remote accessibility, raising concerns about safeguarding voluminous customer privacy data during transmission. This paper proposes the LLaMA-E, the unified and customized instruction-following language models focusing on diverse e-commerce authoring tasks. Specifically, the domain experts create the seed instruction set from the tasks of ads generation, query-enhanced product title rewriting, product classification, purchase intent speculation, and general Q&amp;A. These tasks enabl
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#24182;&#34892;&#30693;&#35782;&#22686;&#24378;&#26694;&#26550;&#65288;PKEF&#65289;&#29992;&#20110;&#35299;&#20915;&#22810;&#34892;&#20026;&#25512;&#33616;&#20013;&#30340;&#38382;&#39064;&#65292;&#21253;&#25324;&#35299;&#20915;&#34701;&#21512;&#27493;&#39588;&#20013;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#21644;&#20943;&#23569;&#36127;&#38754;&#20449;&#24687;&#20256;&#36882;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.04807</link><description>&lt;p&gt;
&#24182;&#34892;&#30693;&#35782;&#22686;&#24378;&#24335;&#22810;&#34892;&#20026;&#25512;&#33616;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Parallel Knowledge Enhancement based Framework for Multi-behavior Recommendation. (arXiv:2308.04807v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04807
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#24182;&#34892;&#30693;&#35782;&#22686;&#24378;&#26694;&#26550;&#65288;PKEF&#65289;&#29992;&#20110;&#35299;&#20915;&#22810;&#34892;&#20026;&#25512;&#33616;&#20013;&#30340;&#38382;&#39064;&#65292;&#21253;&#25324;&#35299;&#20915;&#34701;&#21512;&#27493;&#39588;&#20013;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#21644;&#20943;&#23569;&#36127;&#38754;&#20449;&#24687;&#20256;&#36882;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#34892;&#20026;&#25512;&#33616;&#31639;&#27861;&#26088;&#22312;&#21033;&#29992;&#29992;&#25143;&#21644;&#29289;&#21697;&#20043;&#38388;&#30340;&#22810;&#37325;&#20132;&#20114;&#26469;&#23398;&#20064;&#29992;&#25143;&#30340;&#28508;&#22312;&#20559;&#22909;&#12290;&#26368;&#36817;&#30340;&#22810;&#34892;&#20026;&#25512;&#33616;&#26694;&#26550;&#21253;&#25324;&#20004;&#20010;&#27493;&#39588;&#65306;&#34701;&#21512;&#21644;&#39044;&#27979;&#12290;&#22312;&#34701;&#21512;&#27493;&#39588;&#20013;&#65292;&#20808;&#36827;&#30340;&#31070;&#32463;&#32593;&#32476;&#34987;&#29992;&#26469;&#24314;&#27169;&#29992;&#25143;&#34892;&#20026;&#20043;&#38388;&#30340;&#23618;&#32423;&#30456;&#20851;&#24615;&#12290;&#22312;&#39044;&#27979;&#27493;&#39588;&#20013;&#65292;&#22810;&#20010;&#20449;&#21495;&#34987;&#21033;&#29992;&#20197;&#32852;&#21512;&#20248;&#21270;&#27169;&#22411;&#65292;&#37319;&#29992;&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;MTL&#65289;&#33539;&#24335;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#26041;&#27861;&#27809;&#26377;&#35299;&#20915;&#34701;&#21512;&#27493;&#39588;&#20013;&#30001;&#20110;&#25968;&#25454;&#20998;&#24067;&#19981;&#24179;&#34913;&#32780;&#24341;&#36215;&#30340;&#38382;&#39064;&#65292;&#23548;&#33268;&#23398;&#21040;&#30340;&#20851;&#31995;&#34987;&#39640;&#39057;&#34892;&#20026;&#25152;&#20027;&#23548;&#12290;&#22312;&#39044;&#27979;&#27493;&#39588;&#20013;&#65292;&#29616;&#26377;&#26041;&#27861;&#20351;&#29992;&#38376;&#26426;&#21046;&#26469;&#30452;&#25509;&#32858;&#21512;&#30001;&#32806;&#21512;&#36755;&#20837;&#29983;&#25104;&#30340;&#19987;&#23478;&#20449;&#24687;&#65292;&#23548;&#33268;&#36127;&#38754;&#20449;&#24687;&#20256;&#36882;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24182;&#34892;&#30693;&#35782;&#22686;&#24378;&#26694;&#26550;&#65288;PKEF&#65289;&#29992;&#20110;&#22810;&#34892;&#20026;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-behavior recommendation algorithms aim to leverage the multiplex interactions between users and items to learn users' latent preferences. Recent multi-behavior recommendation frameworks contain two steps: fusion and prediction. In the fusion step, advanced neural networks are used to model the hierarchical correlations between user behaviors. In the prediction step, multiple signals are utilized to jointly optimize the model with a multi-task learning (MTL) paradigm. However, recent approaches have not addressed the issue caused by imbalanced data distribution in the fusion step, resulting in the learned relationships being dominated by high-frequency behaviors. In the prediction step, the existing methods use a gate mechanism to directly aggregate expert information generated by coupling input, leading to negative information transfer. To tackle these issues, we propose a Parallel Knowledge Enhancement Framework (PKEF) for multi-behavior recommendation. Specifically, we enhance 
&lt;/p&gt;</description></item><item><title>DiVa&#26159;&#19968;&#20010;&#36845;&#20195;&#26694;&#26550;&#65292;&#36890;&#36807;&#20174;&#29992;&#25143;&#35780;&#35770;&#20013;&#25910;&#38598;&#26356;&#22810;&#22810;&#26679;&#19988;&#26377;&#25928;&#30340;&#26631;&#31614;&#65292;&#20351;&#20998;&#31867;&#22120;&#33021;&#22815;&#20026;&#27468;&#26354;&#24418;&#25104;&#23436;&#25972;&#30340;&#26631;&#31614;&#38598;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.04805</link><description>&lt;p&gt;
DiVa&#65306;&#19968;&#31181;&#20174;&#29992;&#25143;&#35780;&#35770;&#20013;&#25910;&#38598;&#26356;&#22810;&#22810;&#26679;&#19988;&#26377;&#25928;&#30340;&#38899;&#20048;&#26631;&#31614;&#30340;&#36845;&#20195;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
DiVa: An Iterative Framework to Harvest More Diverse and Valid Labels from User Comments for Music. (arXiv:2308.04805v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04805
&lt;/p&gt;
&lt;p&gt;
DiVa&#26159;&#19968;&#20010;&#36845;&#20195;&#26694;&#26550;&#65292;&#36890;&#36807;&#20174;&#29992;&#25143;&#35780;&#35770;&#20013;&#25910;&#38598;&#26356;&#22810;&#22810;&#26679;&#19988;&#26377;&#25928;&#30340;&#26631;&#31614;&#65292;&#20351;&#20998;&#31867;&#22120;&#33021;&#22815;&#20026;&#27468;&#26354;&#24418;&#25104;&#23436;&#25972;&#30340;&#26631;&#31614;&#38598;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#23454;&#29616;&#20805;&#20998;&#30340;&#38899;&#20048;&#25628;&#32034;&#65292;&#24418;&#25104;&#27599;&#39318;&#27468;&#26354;&#30340;&#23436;&#25972;&#26631;&#31614;&#38598;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#35299;&#20915;&#26041;&#26696;&#26080;&#27861;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#20204;&#19981;&#33021;&#20135;&#29983;&#36275;&#22815;&#22810;&#26679;&#21270;&#30340;&#26144;&#23556;&#26469;&#24357;&#34917;&#40644;&#37329;&#26631;&#31614;&#25152;&#36951;&#28431;&#30340;&#20449;&#24687;&#12290;&#22522;&#20110;&#36825;&#26679;&#30340;&#35266;&#23519;&#65292;&#36825;&#20123;&#36951;&#28431;&#30340;&#20449;&#24687;&#21487;&#33021;&#24050;&#32463;&#22312;&#29992;&#25143;&#35780;&#35770;&#20013;&#25552;&#20379;&#65292;&#25105;&#20204;&#25552;&#20986;&#22312;&#19968;&#20010;&#37325;&#35201;&#20294;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#30340;&#22330;&#26223;&#20013;&#30740;&#31350;&#33258;&#21160;&#38899;&#20048;&#26631;&#27880;&#65292;&#21363;&#22312;&#32473;&#23450;&#26377;&#38480;&#40644;&#37329;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#65292;&#27169;&#22411;&#38656;&#35201;&#20174;&#29992;&#25143;&#35780;&#35770;&#20013;&#25910;&#38598;&#26356;&#22810;&#22810;&#26679;&#19988;&#26377;&#25928;&#30340;&#26631;&#31614;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#36845;&#20195;&#26694;&#26550;&#65288;DiVa&#65289;&#65292;&#36890;&#36807;&#20174;&#39044;&#35757;&#32451;&#20998;&#31867;&#22120;&#20013;&#25512;&#26029;&#20986;&#30340;&#20266;&#26631;&#31614;&#21644;&#19968;&#31181;&#26032;&#39062;&#30340;&#32852;&#21512;&#24471;&#20998;&#20989;&#25968;&#65292;&#20351;&#20998;&#31867;&#22120;&#33021;&#22815;&#20026;&#27468;&#26354;&#24418;&#25104;&#23436;&#25972;&#30340;&#26631;&#31614;&#38598;&#12290;&#23545;&#19968;&#20010;&#23494;&#38598;&#27880;&#37322;&#30340;&#27979;&#35797;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;DiVa&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Towards sufficient music searching, it is vital to form a complete set of labels for each song. However, current solutions fail to resolve it as they cannot produce diverse enough mappings to make up for the information missed by the gold labels. Based on the observation that such missing information may already be presented in user comments, we propose to study the automated music labeling in an essential but under-explored setting, where the model is required to harvest more diverse and valid labels from the users' comments given limited gold labels. To this end, we design an iterative framework (DiVa) to harvest more $\underline{\text{Di}}$verse and $\underline{\text{Va}}$lid labels from user comments for music. The framework makes a classifier able to form complete sets of labels for songs via pseudo-labels inferred from pre-trained classifiers and a novel joint score function. The experiment on a densely annotated testing set reveals the superiority of the Diva over state-of-the-a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25972;&#20307;&#31354;&#38388;&#32423;&#32852;&#24310;&#36831;&#21453;&#39304;&#24314;&#27169;&#26041;&#27861;&#26469;&#35299;&#20915;&#26377;&#25928;&#36716;&#21270;&#29575;&#39044;&#27979;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22788;&#29702;&#36716;&#21270;&#21518;&#36864;&#27454;&#29575;&#39044;&#27979;&#20013;&#30340;&#25968;&#25454;&#31232;&#30095;&#24615;&#21644;&#26679;&#26412;&#36873;&#25321;&#20559;&#24046;&#65292;&#24182;&#21033;&#29992;&#32423;&#32852;&#24310;&#36831;&#21453;&#39304;&#30340;&#20449;&#24687;&#25552;&#39640;&#27169;&#22411;&#35757;&#32451;&#30340;&#25968;&#25454;&#26032;&#40092;&#24230;&#12290;</title><link>http://arxiv.org/abs/2308.04768</link><description>&lt;p&gt;
&#26377;&#25928;&#30340;&#36716;&#21270;&#29575;&#39044;&#27979;&#30340;&#25972;&#20307;&#31354;&#38388;&#32423;&#32852;&#24310;&#36831;&#21453;&#39304;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Entire Space Cascade Delayed Feedback Modeling for Effective Conversion Rate Prediction. (arXiv:2308.04768v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04768
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25972;&#20307;&#31354;&#38388;&#32423;&#32852;&#24310;&#36831;&#21453;&#39304;&#24314;&#27169;&#26041;&#27861;&#26469;&#35299;&#20915;&#26377;&#25928;&#36716;&#21270;&#29575;&#39044;&#27979;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22788;&#29702;&#36716;&#21270;&#21518;&#36864;&#27454;&#29575;&#39044;&#27979;&#20013;&#30340;&#25968;&#25454;&#31232;&#30095;&#24615;&#21644;&#26679;&#26412;&#36873;&#25321;&#20559;&#24046;&#65292;&#24182;&#21033;&#29992;&#32423;&#32852;&#24310;&#36831;&#21453;&#39304;&#30340;&#20449;&#24687;&#25552;&#39640;&#27169;&#22411;&#35757;&#32451;&#30340;&#25968;&#25454;&#26032;&#40092;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36716;&#21270;&#29575;&#39044;&#27979;&#26159;&#22823;&#35268;&#27169;&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#30340;&#37325;&#35201;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#22312;&#22312;&#32447;&#36141;&#29289;&#31995;&#32479;&#20013;&#65292;&#32463;&#24120;&#21457;&#29983;&#36716;&#21270;&#21518;&#30340;&#36864;&#27454;&#34892;&#20026;&#65292;&#36825;&#20419;&#20351;&#25105;&#20204;&#20851;&#27880;&#20110;&#26500;&#24314;&#26356;&#20581;&#24247;&#30340;&#36141;&#29289;&#26381;&#21153;&#30340;&#26377;&#25928;&#36716;&#21270;&#29575;&#12290;&#26412;&#25991;&#23558;&#27809;&#26377;&#20219;&#20309;&#21518;&#32493;&#36864;&#27454;&#30340;&#29289;&#21697;&#36141;&#20080;&#30340;&#27010;&#29575;&#23450;&#20041;&#20026;&#26377;&#25928;&#36716;&#21270;&#29575;&#12290;&#26377;&#25928;&#36716;&#21270;&#29575;&#39044;&#27979;&#30340;&#31616;&#21333;&#33539;&#24335;&#26159;&#23558;&#20854;&#20998;&#35299;&#20026;&#20004;&#20010;&#23376;&#20219;&#21153;&#65306;&#36716;&#21270;&#29575;&#39044;&#27979;&#21644;&#36716;&#21270;&#21518;&#36864;&#27454;&#29575;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#36716;&#21270;&#21518;&#36864;&#27454;&#29575;&#39044;&#27979;&#23384;&#22312;&#25968;&#25454;&#31232;&#30095;&#24615;&#21644;&#26679;&#26412;&#36873;&#25321;&#20559;&#24046;&#38382;&#39064;&#65292;&#22240;&#20026;&#36864;&#27454;&#34892;&#20026;&#21482;&#26377;&#22312;&#29992;&#25143;&#36141;&#20080;&#21518;&#25165;&#21487;&#35265;&#12290;&#27492;&#22806;&#65292;&#36716;&#21270;&#21644;&#36864;&#27454;&#20107;&#20214;&#20013;&#23384;&#22312;&#24310;&#36831;&#21453;&#39304;&#65292;&#23427;&#20204;&#26159;&#39034;&#24207;&#20381;&#36182;&#30340;&#65292;&#34987;&#31216;&#20026;&#32423;&#32852;&#24310;&#36831;&#21453;&#39304;&#65292;&#36825;&#23545;&#27169;&#22411;&#35757;&#32451;&#30340;&#25968;&#25454;&#26032;&#40092;&#24230;&#36896;&#25104;&#20102;&#24456;&#22823;&#30340;&#20260;&#23475;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#20110;&#35299;&#20915;&#25968;&#25454;&#31232;&#30095;&#24615;&#12289;&#26679;&#26412;&#36873;&#25321;&#20559;&#24046;&#25110;&#24310;&#36831;&#21453;&#39304;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conversion rate (CVR) prediction is an essential task for large-scale e-commerce platforms. However, refund behaviors frequently occur after conversion in online shopping systems, which drives us to pay attention to effective conversion for building healthier shopping services. This paper defines the probability of item purchasing without any subsequent refund as an effective conversion rate (ECVR). A simple paradigm for ECVR prediction is to decompose it into two sub-tasks: CVR prediction and post-conversion refund rate (RFR) prediction. However, RFR prediction suffers from data sparsity (DS) and sample selection bias (SSB) issues, as the refund behaviors are only available after user purchase. Furthermore, there is delayed feedback in both conversion and refund events and they are sequentially dependent, named cascade delayed feedback (CDF), which significantly harms data freshness for model training. Previous studies mainly focus on tackling DS and SSB or delayed feedback for a sing
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20449;&#24687;&#26816;&#32034;&#27969;&#31243;&#65292;&#36890;&#36807;&#20351;&#29992;&#23454;&#20307;/&#20107;&#20214;&#38142;&#25509;&#27169;&#22411;&#21644;&#26597;&#35810;&#20998;&#35299;&#27169;&#22411;&#65292;&#26356;&#20934;&#30830;&#22320;&#32858;&#28966;&#20110;&#26597;&#35810;&#30340;&#19981;&#21516;&#20449;&#24687;&#21333;&#20803;&#12290;&#36825;&#31181;&#27969;&#31243;&#22312;&#27573;&#33853;&#35206;&#30422;&#21644;&#21629;&#21517;&#20934;&#30830;&#24615;&#26041;&#38754;&#26174;&#33879;&#25913;&#36827;&#65292;&#21516;&#26102;&#20063;&#26356;&#20855;&#35299;&#37322;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.04756</link><description>&lt;p&gt;
&#20026;&#26032;&#30340;&#39046;&#22495;&#26500;&#24314;&#21487;&#35299;&#37322;&#21644;&#21487;&#38752;&#30340;&#24320;&#25918;&#20449;&#24687;&#26816;&#32034;&#22120;
&lt;/p&gt;
&lt;p&gt;
Building Interpretable and Reliable Open Information Retriever for New Domains Overnight. (arXiv:2308.04756v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04756
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20449;&#24687;&#26816;&#32034;&#27969;&#31243;&#65292;&#36890;&#36807;&#20351;&#29992;&#23454;&#20307;/&#20107;&#20214;&#38142;&#25509;&#27169;&#22411;&#21644;&#26597;&#35810;&#20998;&#35299;&#27169;&#22411;&#65292;&#26356;&#20934;&#30830;&#22320;&#32858;&#28966;&#20110;&#26597;&#35810;&#30340;&#19981;&#21516;&#20449;&#24687;&#21333;&#20803;&#12290;&#36825;&#31181;&#27969;&#31243;&#22312;&#27573;&#33853;&#35206;&#30422;&#21644;&#21629;&#21517;&#20934;&#30830;&#24615;&#26041;&#38754;&#26174;&#33879;&#25913;&#36827;&#65292;&#21516;&#26102;&#20063;&#26356;&#20855;&#35299;&#37322;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#25110;&#30693;&#35782;&#26816;&#32034;&#26159;&#35768;&#22810;&#19979;&#28216;&#20219;&#21153;&#65288;&#22914;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#65289;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#23427;&#38754;&#20020;&#35768;&#22810;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#31616;&#27905;&#24615;&#12289;&#23436;&#25972;&#24615;&#21644;&#27491;&#30830;&#24615;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#20013;&#65292;&#36890;&#36807;&#20351;&#29992;&#31264;&#23494;&#21521;&#37327;&#34920;&#31034;&#26597;&#35810;&#21644;&#30693;&#35782;&#27573;&#33853;&#65292;&#24182;&#23398;&#20064;&#35789;&#27719;&#21644;&#35821;&#20041;&#30456;&#20284;&#24615;&#65292;&#31264;&#23494;&#26816;&#32034;&#27169;&#22411;&#22312;&#39046;&#22495;&#20869;IR&#21644;QA&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#21333;&#19968;&#30340;&#31264;&#23494;&#21521;&#37327;&#21644;&#31471;&#21040;&#31471;&#30417;&#30563;&#24182;&#19981;&#24635;&#26159;&#26368;&#20248;&#30340;&#65292;&#22240;&#20026;&#26597;&#35810;&#21487;&#33021;&#38656;&#35201;&#27880;&#24847;&#22810;&#20010;&#26041;&#38754;&#21644;&#38544;&#21547;&#30340;&#30693;&#35782;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20449;&#24687;&#26816;&#32034;&#27969;&#31243;&#65292;&#21033;&#29992;&#23454;&#20307;/&#20107;&#20214;&#38142;&#25509;&#27169;&#22411;&#21644;&#26597;&#35810;&#20998;&#35299;&#27169;&#22411;&#65292;&#26356;&#20934;&#30830;&#22320;&#32858;&#28966;&#20110;&#26597;&#35810;&#30340;&#19981;&#21516;&#20449;&#24687;&#21333;&#20803;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#23613;&#31649;&#26356;&#20855;&#35299;&#37322;&#24615;&#21644;&#21487;&#38752;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#27969;&#31243;&#22312;&#27573;&#33853;&#35206;&#30422;&#21644;&#21629;&#21517;&#20934;&#30830;&#24615;&#26041;&#38754;&#37117;&#26377;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Information retrieval (IR) or knowledge retrieval, is a critical component for many down-stream tasks such as open-domain question answering (QA). It is also very challenging, as it requires succinctness, completeness, and correctness. In recent works, dense retrieval models have achieved state-of-the-art (SOTA) performance on in-domain IR and QA benchmarks by representing queries and knowledge passages with dense vectors and learning the lexical and semantic similarity. However, using single dense vectors and end-to-end supervision are not always optimal because queries may require attention to multiple aspects and event implicit knowledge. In this work, we propose an information retrieval pipeline that uses entity/event linking model and query decomposition model to focus more accurately on different information units of the query. We show that, while being more interpretable and reliable, our proposed pipeline significantly improves passage coverages and denotation accuracies across
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Transformer&#21644;&#33258;&#33976;&#39311;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#22823;&#37327;&#26080;&#26631;&#31614;&#30340;3D&#28857;&#38598;&#20013;&#33719;&#21462;&#20934;&#30830;&#19988;&#26059;&#36716;&#19981;&#21464;&#30340;3D&#28857;&#38598;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2308.04725</link><description>&lt;p&gt;
&#20351;&#29992;Transformer&#21644;&#33258;&#33976;&#39311;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26059;&#36716;&#19981;&#21464;&#30340;3D&#28857;&#38598;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Self-supervised Learning of Rotation-invariant 3D Point Set Features using Transformer and its Self-distillation. (arXiv:2308.04725v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04725
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Transformer&#21644;&#33258;&#33976;&#39311;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#22823;&#37327;&#26080;&#26631;&#31614;&#30340;3D&#28857;&#38598;&#20013;&#33719;&#21462;&#20934;&#30830;&#19988;&#26059;&#36716;&#19981;&#21464;&#30340;3D&#28857;&#38598;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20998;&#26512;3D&#28857;&#38598;&#25968;&#25454;&#20013;&#65292;3D&#29289;&#20307;&#30340;&#26059;&#36716;&#19981;&#21464;&#24615;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#23646;&#24615;&#12290;&#20256;&#32479;&#30340;&#20855;&#26377;&#26059;&#36716;&#19981;&#21464;&#24615;&#30340;3D&#28857;&#38598;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36890;&#24120;&#36890;&#36807;&#20351;&#29992;&#26377;&#26631;&#31614;&#30340;3D&#28857;&#38598;&#20316;&#20026;&#35757;&#32451;&#26679;&#26412;&#65292;&#36890;&#36807;&#30417;&#30563;&#23398;&#20064;&#33719;&#21462;&#20934;&#30830;&#30340;3D&#24418;&#29366;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;3D&#28857;&#38598;&#25968;&#25454;&#30340;&#24555;&#36895;&#22686;&#38271;&#21644;&#26631;&#27880;&#30340;&#39640;&#25104;&#26412;&#65292;&#38656;&#35201;&#19968;&#20010;&#20174;&#22823;&#37327;&#26080;&#26631;&#31614;&#30340;3D&#28857;&#38598;&#20013;&#23398;&#20064;&#26059;&#36716;&#19981;&#21464;&#30340;3D&#24418;&#29366;&#29305;&#24449;&#30340;&#26694;&#26550;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#23545;&#35937;&#32423;&#21035;&#33719;&#21462;&#20934;&#30830;&#19988;&#26059;&#36716;&#19981;&#21464;&#30340;3D&#28857;&#38598;&#29305;&#24449;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#36731;&#37327;&#32423;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#23558;&#36755;&#20837;&#30340;3D&#28857;&#38598;&#20998;&#35299;&#20026;&#22810;&#20010;&#20840;&#23616;&#23610;&#24230;&#30340;&#21306;&#22495;&#65288;&#31216;&#20026;tokens&#65289;&#65292;&#36825;&#20123;&#21306;&#22495;&#20445;&#30041;&#20102;&#26500;&#25104;3D&#23545;&#35937;&#30340;&#23616;&#37096;&#24418;&#29366;&#30340;&#31354;&#38388;&#24067;&#23616;&#12290;&#25105;&#20204;&#20351;&#29992;&#33258;&#27880;&#24847;&#26426;&#21046;&#26469;&#25913;&#36827;tokens&#65292;&#24182;&#23558;&#23427;&#20204;&#32858;&#21512;&#25104;&#27599;&#20010;3D&#28857;&#38598;&#30340;&#34920;&#36798;&#24615;&#26059;&#36716;&#19981;&#21464;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36890;&#36807;&#33258;&#33976;&#39311;&#26426;&#21046;&#36827;&#34892;&#26377;&#25928;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Invariance against rotations of 3D objects is an important property in analyzing 3D point set data. Conventional 3D point set DNNs having rotation invariance typically obtain accurate 3D shape features via supervised learning by using labeled 3D point sets as training samples. However, due to the rapid increase in 3D point set data and the high cost of labeling, a framework to learn rotation-invariant 3D shape features from numerous unlabeled 3D point sets is required. This paper proposes a novel self-supervised learning framework for acquiring accurate and rotation-invariant 3D point set features at object-level. Our proposed lightweight DNN architecture decomposes an input 3D point set into multiple global-scale regions, called tokens, that preserve the spatial layout of partial shapes composing the 3D object. We employ a self-attention mechanism to refine the tokens and aggregate them into an expressive rotation-invariant feature per 3D point set. Our DNN is effectively trained by u
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Pareto Invariant Representation Learning&#65288;PaInvRL&#65289;&#30340;&#26694;&#26550;&#65292;&#24212;&#29992;&#20110;&#22810;&#23186;&#20307;&#25512;&#33616;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#23398;&#20064;&#19981;&#21464;&#34920;&#31034;&#21644;&#21464;&#20307;&#34920;&#31034;&#30340;&#21516;&#26102;&#26469;&#32531;&#35299;&#36890;&#29992;&#34920;&#31034;&#24341;&#20837;&#30340;&#38169;&#35823;&#30456;&#20851;&#24615;&#38382;&#39064;&#12290;&#20174;IID-OOD&#22810;&#30446;&#26631;&#20248;&#21270;&#30340;&#35282;&#24230;&#65292;PaInvRL&#20943;&#23569;&#20102;&#38169;&#35823;&#30456;&#20851;&#24615;&#23545;&#29992;&#25143;&#20559;&#22909;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2308.04706</link><description>&lt;p&gt;
Pareto&#19981;&#21464;&#34920;&#31034;&#23398;&#20064;&#22312;&#22810;&#23186;&#20307;&#25512;&#33616;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Pareto Invariant Representation Learning for Multimedia Recommendation. (arXiv:2308.04706v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04706
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Pareto Invariant Representation Learning&#65288;PaInvRL&#65289;&#30340;&#26694;&#26550;&#65292;&#24212;&#29992;&#20110;&#22810;&#23186;&#20307;&#25512;&#33616;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#23398;&#20064;&#19981;&#21464;&#34920;&#31034;&#21644;&#21464;&#20307;&#34920;&#31034;&#30340;&#21516;&#26102;&#26469;&#32531;&#35299;&#36890;&#29992;&#34920;&#31034;&#24341;&#20837;&#30340;&#38169;&#35823;&#30456;&#20851;&#24615;&#38382;&#39064;&#12290;&#20174;IID-OOD&#22810;&#30446;&#26631;&#20248;&#21270;&#30340;&#35282;&#24230;&#65292;PaInvRL&#20943;&#23569;&#20102;&#38169;&#35823;&#30456;&#20851;&#24615;&#23545;&#29992;&#25143;&#20559;&#22909;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#23186;&#20307;&#25512;&#33616;&#28041;&#21450;&#20010;&#24615;&#21270;&#25490;&#24207;&#20219;&#21153;&#65292;&#36890;&#24120;&#20351;&#29992;&#36890;&#29992;&#32534;&#30721;&#22120;&#34920;&#31034;&#22810;&#23186;&#20307;&#20869;&#23481;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#36890;&#29992;&#34920;&#31034;&#24341;&#20837;&#20102;&#38169;&#35823;&#30340;&#30456;&#20851;&#24615;&#65292;&#26080;&#27861;&#25581;&#31034;&#29992;&#25143;&#30340;&#30495;&#23454;&#20559;&#22909;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#23581;&#35797;&#36890;&#36807;&#23398;&#20064;&#19981;&#21464;&#34920;&#31034;&#26469;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#24573;&#35270;&#20102;&#29420;&#31435;&#21516;&#20998;&#24067;&#65288;IID&#65289;&#21644;&#38750;&#20998;&#24067;&#65288;OOD&#65289;&#24191;&#20041;&#21270;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Pareto Invariant Representation Learning&#65288;PaInvRL&#65289;&#30340;&#26694;&#26550;&#65292;&#20174;IID-OOD&#22810;&#30446;&#26631;&#20248;&#21270;&#30340;&#35282;&#24230;&#20943;&#23569;&#20102;&#38169;&#35823;&#30456;&#20851;&#24615;&#30340;&#24433;&#21709;&#65292;&#21516;&#26102;&#23398;&#20064;&#19981;&#21464;&#34920;&#31034;&#65288;&#21560;&#24341;&#29992;&#25143;&#27880;&#24847;&#30340;&#20869;&#22312;&#22240;&#32032;&#65289;&#21644;&#21464;&#20307;&#34920;&#31034;&#65288;&#20854;&#20182;&#22240;&#32032;&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;PaInvRL&#21253;&#25324;&#19977;&#20010;&#36845;&#20195;&#25191;&#34892;&#30340;&#27169;&#22359;&#65306;&#65288;i&#65289;&#38750;&#21516;&#36136;&#35782;&#21035;&#27169;&#22359;&#65292;&#29992;&#20110;&#35782;&#21035;&#21453;&#26144;&#20998;&#24067;&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;
Multimedia recommendation involves personalized ranking tasks, where multimedia content is usually represented using a generic encoder. However, these generic representations introduce spurious correlations that fail to reveal users' true preferences. Existing works attempt to alleviate this problem by learning invariant representations, but overlook the balance between independent and identically distributed (IID) and out-of-distribution (OOD) generalization. In this paper, we propose a framework called Pareto Invariant Representation Learning (PaInvRL) to mitigate the impact of spurious correlations from an IID-OOD multi-objective optimization perspective, by learning invariant representations (intrinsic factors that attract user attention) and variant representations (other factors) simultaneously. Specifically, PaInvRL includes three iteratively executed modules: (i) heterogeneous identification module, which identifies the heterogeneous environments to reflect distributional shift
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#22312;&#25903;&#25345;&#20195;&#30721;&#26816;&#32034;&#27169;&#22411;&#20013;&#30340;&#25928;&#26524;&#65292;&#24182;&#21457;&#29616;NMT&#22312;&#33258;&#28982;&#35821;&#35328;&#21040;&#20195;&#30721;&#32763;&#35793;&#20219;&#21153;&#20013;&#20934;&#30830;&#24615;&#36739;&#20302;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#25277;&#35937;&#35821;&#27861;&#26641;&#30340;&#23450;&#21046;&#34920;&#31034;&#26041;&#27861;ASTTrans Representation&#12290;</title><link>http://arxiv.org/abs/2308.04693</link><description>&lt;p&gt;
&#35780;&#20272;&#21644;&#20248;&#21270;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#22312;&#25903;&#25345;&#20195;&#30721;&#26816;&#32034;&#27169;&#22411;&#20013;&#30340;&#25928;&#26524;&#65306;&#22522;&#20110;CAT&#22522;&#20934;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Evaluating and Optimizing the Effectiveness of Neural Machine Translation in Supporting Code Retrieval Models: A Study on the CAT Benchmark. (arXiv:2308.04693v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04693
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#22312;&#25903;&#25345;&#20195;&#30721;&#26816;&#32034;&#27169;&#22411;&#20013;&#30340;&#25928;&#26524;&#65292;&#24182;&#21457;&#29616;NMT&#22312;&#33258;&#28982;&#35821;&#35328;&#21040;&#20195;&#30721;&#32763;&#35793;&#20219;&#21153;&#20013;&#20934;&#30830;&#24615;&#36739;&#20302;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#25277;&#35937;&#35821;&#27861;&#26641;&#30340;&#23450;&#21046;&#34920;&#31034;&#26041;&#27861;ASTTrans Representation&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#22312;&#36719;&#20214;&#24037;&#31243;&#20219;&#21153;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#12290;NMT&#22312;&#20195;&#30721;&#26816;&#32034;&#20013;&#30340;&#26377;&#25928;&#24615;&#21462;&#20915;&#20110;&#20174;&#28304;&#35821;&#35328;&#30340;&#20196;&#29260;&#24207;&#21015;&#21040;&#30446;&#26631;&#35821;&#35328;&#30340;&#20196;&#29260;&#24207;&#21015;&#20043;&#38388;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;&#34429;&#28982;NMT&#22312;&#20266;&#20195;&#30721;&#21040;&#20195;&#30721;&#30340;&#32763;&#35793;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;&#23398;&#20064;&#23558;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#32763;&#35793;&#25104;&#26032;&#30340;&#23454;&#38469;&#20195;&#30721;&#25991;&#26723;/&#23454;&#29616;&#25968;&#25454;&#38598;&#20013;&#30340;&#28304;&#20195;&#30721;&#26102;&#21487;&#33021;&#38754;&#20020;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;NMT&#22312;&#26032;&#30340;CAT&#22522;&#20934;&#20013;&#36827;&#34892;&#33258;&#28982;&#35821;&#35328;&#21040;&#20195;&#30721;&#32763;&#35793;&#30340;&#24615;&#33021;&#65292;&#35813;&#22522;&#20934;&#21253;&#25324;&#19977;&#20010;&#32463;&#36807;&#20248;&#21270;&#30340;Java&#25968;&#25454;&#38598;TLCodeSum&#12289;CodeSearchNet&#12289;Funcom&#21644;&#19968;&#20010;Python&#25968;&#25454;&#38598;PCSD&#30340;&#29256;&#26412;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;NMT&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#30340;&#20934;&#30830;&#24615;&#36739;&#20302;&#65292;&#36890;&#36807;CrystalBLEU&#21644;Meteor&#24230;&#37327;&#25152;&#24471;&#12290;&#20026;&#20102;&#20943;&#36731;NMT&#22312;&#23398;&#20064;&#28304;&#20195;&#30721;&#30340;&#22797;&#26434;&#34920;&#31034;&#26041;&#38754;&#30340;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ASTTrans Representation&#65292;&#36825;&#26159;&#19968;&#31181;&#38024;&#23545;&#25277;&#35937;&#35821;&#27861;&#26641;&#30340;&#23450;&#21046;&#34920;&#31034;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Machine Translation (NMT) is widely applied in software engineering tasks. The effectiveness of NMT for code retrieval relies on the ability to learn from the sequence of tokens in the source language to the sequence of tokens in the target language. While NMT performs well in pseudocode-to-code translation, it might have challenges in learning to translate from natural language query to source code in newly curated real-world code documentation/ implementation datasets. In this work, we analyze the performance of NMT in natural language-to-code translation in the newly curated CAT benchmark that includes the optimized versions of three Java datasets TLCodeSum, CodeSearchNet, Funcom, and a Python dataset PCSD. Our evaluation shows that NMT has low accuracy, measured by CrystalBLEU and Meteor metrics in this task. To alleviate the duty of NMT in learning complex representation of source code, we propose ASTTrans Representation, a tailored representation of an Abstract Syntax Tree
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;robot.txt&#38480;&#21046;&#19979;&#30340;&#32593;&#32476;&#29228;&#34411;&#31574;&#30053;&#65292;&#35752;&#35770;&#20102;&#25628;&#32034;&#24341;&#25806;&#22914;&#20309;&#30830;&#23450;&#32593;&#39029;&#25490;&#21517;&#20197;&#21450;&#22914;&#20309;&#33719;&#21462;&#25968;&#25454;&#24211;&#20013;&#30340;&#32593;&#39029;&#12290;&#24182;&#20171;&#32461;&#20102;&#26426;&#22120;&#20154;&#25490;&#38500;&#21327;&#35758;&#35268;&#21017;&#21644;robot.txt&#25991;&#20214;&#30340;&#22522;&#26412;&#26684;&#24335;&#12290;</title><link>http://arxiv.org/abs/2308.04689</link><description>&lt;p&gt;
&#32593;&#32476;&#29228;&#34411;&#22312;robot.txt&#38480;&#21046;&#19979;&#30340;&#31574;&#30053;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
web crawler strategies for web pages under robot.txt restriction. (arXiv:2308.04689v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04689
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;robot.txt&#38480;&#21046;&#19979;&#30340;&#32593;&#32476;&#29228;&#34411;&#31574;&#30053;&#65292;&#35752;&#35770;&#20102;&#25628;&#32034;&#24341;&#25806;&#22914;&#20309;&#30830;&#23450;&#32593;&#39029;&#25490;&#21517;&#20197;&#21450;&#22914;&#20309;&#33719;&#21462;&#25968;&#25454;&#24211;&#20013;&#30340;&#32593;&#39029;&#12290;&#24182;&#20171;&#32461;&#20102;&#26426;&#22120;&#20154;&#25490;&#38500;&#21327;&#35758;&#35268;&#21017;&#21644;robot.txt&#25991;&#20214;&#30340;&#22522;&#26412;&#26684;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20170;&#65292;&#25152;&#26377;&#20154;&#37117;&#20102;&#35299;&#20114;&#32852;&#32593;&#24182;&#27599;&#22825;&#22312;&#20114;&#32852;&#32593;&#19978;&#24037;&#20316;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#20026;&#29992;&#25143;&#36755;&#20837;&#30340;&#20851;&#38190;&#23383;&#36827;&#34892;&#25628;&#32034;&#30340;&#25628;&#32034;&#24341;&#25806;&#12290;&#25628;&#32034;&#24341;&#25806;&#20351;&#29992;&#19981;&#21516;&#30340;&#25628;&#32034;&#31639;&#27861;&#65292;&#20026;&#19978;&#32593;&#32773;&#25552;&#20379;&#26041;&#20415;&#30340;&#32467;&#26524;&#12290;&#19978;&#32593;&#32773;&#36873;&#25321;&#25490;&#21517;&#38752;&#21069;&#30340;&#25628;&#32034;&#32467;&#26524;&#65292;&#20294;&#26159;&#32593;&#39029;&#30340;&#25490;&#21517;&#26159;&#22914;&#20309;&#30001;&#25628;&#32034;&#24341;&#25806;&#30830;&#23450;&#30340;&#65311;&#25628;&#32034;&#24341;&#25806;&#22914;&#20309;&#33719;&#21462;&#25968;&#25454;&#24211;&#20013;&#30340;&#25152;&#26377;&#32593;&#39029;&#65311;&#26412;&#25991;&#32473;&#20986;&#20102;&#25152;&#26377;&#36825;&#20123;&#22522;&#26412;&#38382;&#39064;&#30340;&#31572;&#26696;&#12290;&#26412;&#30740;&#31350;&#35770;&#25991;&#36824;&#35752;&#35770;&#20102;&#20026;&#25628;&#32034;&#24341;&#25806;&#24037;&#20316;&#30340;&#32593;&#32476;&#29228;&#34411;&#21644;&#32593;&#32476;&#29228;&#34411;&#30340;&#26426;&#22120;&#20154;&#25490;&#38500;&#21327;&#35758;&#35268;&#21017;&#12290;&#32593;&#31449;&#31649;&#29702;&#21592;&#20351;&#29992;robot.txt&#25991;&#20214;&#20013;&#30340;&#19981;&#21516;&#38480;&#21046;&#35268;&#21017;&#25351;&#23548;&#32593;&#32476;&#29228;&#34411;&#65292;&#26412;&#25991;&#36824;&#25552;&#21040;&#20102;&#19968;&#20123;&#22522;&#26412;&#30340;robot.txt&#26684;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the present time, all know about World Wide Web and work over the Internet daily. In this paper, we introduce the search engines working for keywords that are entered by users to find something. The search engine uses different search algorithms for convenient results for providing to the net surfer. Net surfers go with the top search results but how did the results of web pages get higher ranks over search engines? how the search engine got that all the web pages in the database? This paper gives the answers to all these kinds of basic questions. Web crawlers working for search engines and robot exclusion protocol rules for web crawlers are also addressed in this research paper. Webmaster uses different restriction facts in robot.txt file to instruct web crawler, some basic formats of robot.txt are also mentioned in this paper.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#22810;&#35270;&#22270;&#32858;&#31867;&#30340;&#32479;&#19968;&#30697;&#38453;&#20998;&#35299;&#26041;&#27861;&#65292;&#23545;&#20110;&#20855;&#26377;&#25968;&#21313;&#20159;&#29992;&#25143;&#21644;&#25968;&#19975;&#20159;&#39033;&#30446;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#21487;&#20197;&#23454;&#29616;&#23454;&#26102;&#24212;&#29992;&#65292;&#24182;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21160;&#24577;&#20002;&#24323;&#19981;&#22909;&#30340;&#32858;&#31867;&#12290;</title><link>http://arxiv.org/abs/2308.04661</link><description>&lt;p&gt;
&#32479;&#19968;&#30697;&#38453;&#20998;&#35299;&#19982;&#21160;&#24577;&#22810;&#35270;&#22270;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Unified Matrix Factorization with Dynamic Multi-view Clustering. (arXiv:2308.04661v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04661
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#22810;&#35270;&#22270;&#32858;&#31867;&#30340;&#32479;&#19968;&#30697;&#38453;&#20998;&#35299;&#26041;&#27861;&#65292;&#23545;&#20110;&#20855;&#26377;&#25968;&#21313;&#20159;&#29992;&#25143;&#21644;&#25968;&#19975;&#20159;&#39033;&#30446;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#21487;&#20197;&#23454;&#29616;&#23454;&#26102;&#24212;&#29992;&#65292;&#24182;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21160;&#24577;&#20002;&#24323;&#19981;&#22909;&#30340;&#32858;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30697;&#38453;&#20998;&#35299;(MF)&#26159;&#19968;&#31181;&#32463;&#20856;&#30340;&#21327;&#21516;&#36807;&#28388;&#31639;&#27861;&#65292;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#12290;&#23427;&#23558;&#29992;&#25143;-&#39033;&#30446;&#20132;&#20114;&#30697;&#38453;&#20998;&#35299;&#20026;&#20302;&#32500;&#29992;&#25143;&#34920;&#31034;&#30697;&#38453;&#21644;&#39033;&#30446;&#34920;&#31034;&#30697;&#38453;&#30340;&#20056;&#31215;&#12290;&#22312;&#20856;&#22411;&#30340;&#25512;&#33616;&#22330;&#26223;&#20013;&#65292;&#29992;&#25143;-&#39033;&#30446;&#20132;&#20114;&#27169;&#24335;&#36890;&#24120;&#26159;&#19968;&#20010;&#20004;&#38454;&#27573;&#36807;&#31243;&#65292;&#24182;&#19988;&#38656;&#35201;&#23545;&#33719;&#24471;&#30340;&#29992;&#25143;&#21644;&#39033;&#30446;&#34920;&#31034;&#36827;&#34892;&#38745;&#24577;&#32858;&#31867;&#20998;&#26512;&#12290;&#28982;&#32780;&#65292;&#19978;&#36848;&#36807;&#31243;&#38656;&#35201;&#22823;&#37327;&#30340;&#26102;&#38388;&#21644;&#35745;&#31639;&#36164;&#28304;&#65292;&#22240;&#27492;&#24456;&#38590;&#24212;&#29992;&#20110;&#20855;&#26377;&#25968;&#21313;&#20159;&#29992;&#25143;&#21644;&#25968;&#19975;&#20159;&#39033;&#30446;&#30340;&#30005;&#23376;&#21830;&#21153;&#25110;&#29289;&#32852;&#32593;&#29615;&#22659;&#20013;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#22810;&#35270;&#22270;&#32858;&#31867;(MFDMC)&#30340;&#32479;&#19968;&#30697;&#38453;&#20998;&#35299;&#26041;&#27861;&#65292;&#37319;&#29992;&#31471;&#21040;&#31471;&#30340;&#35757;&#32451;&#33539;&#24335;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#27599;&#20010;&#35270;&#22270;&#20013;&#65292;&#29992;&#25143;/&#39033;&#30446;&#34920;&#31034;&#34987;&#35270;&#20026;&#25152;&#26377;&#32858;&#31867;&#30340;&#21152;&#26435;&#25237;&#24433;&#12290;&#27599;&#20010;&#32858;&#31867;&#30340;&#34920;&#31034;&#26159;&#21487;&#23398;&#20064;&#30340;&#65292;&#21487;&#20197;&#21160;&#24577;&#20002;&#24323;&#19981;&#22909;&#30340;&#32858;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Matrix factorization (MF) is a classical collaborative filtering algorithm for recommender systems. It decomposes the user-item interaction matrix into a product of low-dimensional user representation matrix and item representation matrix. In typical recommendation scenarios, the user-item interaction paradigm is usually a two-stage process and requires static clustering analysis of the obtained user and item representations. The above process, however, is time and computationally intensive, making it difficult to apply in real-time to e-commerce or Internet of Things environments with billions of users and trillions of items. To address this, we propose a unified matrix factorization method based on dynamic multi-view clustering (MFDMC) that employs an end-to-end training paradigm. Specifically, in each view, a user/item representation is regarded as a weighted projection of all clusters. The representation of each cluster is learnable, enabling the dynamic discarding of bad clusters.
&lt;/p&gt;</description></item><item><title>RECipe&#26159;&#19968;&#20010;&#22810;&#29992;&#36884;&#30340;&#33756;&#35889;&#25512;&#33616;&#26694;&#26550;&#65292;&#20351;&#29992;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#20316;&#20026;&#25903;&#25745;&#65292;&#36890;&#36807;&#22312;&#29992;&#25143;&#20197;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#25110;&#25552;&#20379;&#22270;&#20687;&#26102;&#21521;&#29992;&#25143;&#25512;&#33616;&#33756;&#35889;&#65292;&#36229;&#36234;&#20256;&#32479;&#25512;&#33616;&#31995;&#32479;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.04579</link><description>&lt;p&gt;
RECipe: &#22810;&#27169;&#24577;&#33756;&#35889;&#30693;&#35782;&#22270;&#35889;&#36866;&#29992;&#20110;&#22810;&#29992;&#36884;&#25512;&#33616;&#31995;&#32479;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
RECipe: Does a Multi-Modal Recipe Knowledge Graph Fit a Multi-Purpose Recommendation System?. (arXiv:2308.04579v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04579
&lt;/p&gt;
&lt;p&gt;
RECipe&#26159;&#19968;&#20010;&#22810;&#29992;&#36884;&#30340;&#33756;&#35889;&#25512;&#33616;&#26694;&#26550;&#65292;&#20351;&#29992;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#20316;&#20026;&#25903;&#25745;&#65292;&#36890;&#36807;&#22312;&#29992;&#25143;&#20197;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#25110;&#25552;&#20379;&#22270;&#20687;&#26102;&#21521;&#29992;&#25143;&#25512;&#33616;&#33756;&#35889;&#65292;&#36229;&#36234;&#20256;&#32479;&#25512;&#33616;&#31995;&#32479;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#20004;&#21313;&#24180;&#20013;&#65292;&#25512;&#33616;&#31995;&#32479;&#65288;RS&#65289;&#24050;&#32463;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30340;&#35299;&#20915;&#26041;&#26696;&#20026;&#21830;&#19994;&#25110;&#22312;&#32447;&#24179;&#21488;&#30340;&#23458;&#25143;&#25512;&#33616;&#30005;&#24433;&#12289;&#22270;&#20070;&#21644;&#39184;&#21381;&#31561;&#29289;&#21697;&#12290;&#28982;&#32780;&#65292;&#30456;&#23545;&#20110;&#36825;&#20123;&#24212;&#29992;&#65292;&#33756;&#35889;&#25512;&#33616;&#23578;&#26410;&#24471;&#21040;&#24456;&#22810;&#20851;&#27880;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;RECipe&#20316;&#20026;&#19968;&#20010;&#22810;&#29992;&#36884;&#30340;&#33756;&#35889;&#25512;&#33616;&#26694;&#26550;&#65292;&#20855;&#26377;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#65288;MMKG&#65289;&#30340;&#25903;&#25745;&#12290;RECipe&#30340;&#21160;&#26426;&#26159;&#36890;&#36807;&#22312;&#29992;&#25143;&#20197;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#25110;&#25552;&#20379;&#22270;&#20687;&#26102;&#21521;&#29992;&#25143;&#25512;&#33616;&#33756;&#35889;&#65292;&#36229;&#36234;&#65288;&#28145;&#24230;&#65289;&#31070;&#32463;&#21327;&#21516;&#36807;&#28388;&#65288;NCF&#65289;&#12290;RECipe&#30001;3&#20010;&#23376;&#31995;&#32479;&#32452;&#25104;&#65306;&#22522;&#20110;&#34892;&#20026;&#30340;&#25512;&#33616;&#22120;&#12289;&#22522;&#20110;&#35780;&#35770;&#30340;&#25512;&#33616;&#22120;&#21644;&#22522;&#20110;&#22270;&#20687;&#30340;&#25512;&#33616;&#22120;&#12290;&#27599;&#20010;&#23376;&#31995;&#32479;&#37117;&#20381;&#36182;&#20110;&#22270;&#20013;&#23454;&#20307;&#21644;&#20851;&#31995;&#30340;&#23884;&#20837;&#34920;&#31034;&#12290;&#25105;&#20204;&#39318;&#20808;&#20174;&#24494;&#36719;&#30340;MPNet&#27169;&#22411;&#30340;&#24494;&#35843;&#27169;&#22411;&#20013;&#33719;&#21462;&#25991;&#26412;&#23454;&#20307;&#65288;&#20363;&#22914;&#35780;&#35770;&#25110;&#25104;&#20998;&#65289;&#30340;&#65288;&#39044;&#35757;&#32451;&#65289;&#23884;&#20837;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the past two decades, recommendation systems (RSs) have used machine learning (ML) solutions to recommend items, e.g., movies, books, and restaurants, to clients of a business or an online platform. Recipe recommendation, however, has not yet received much attention compared to those applications. We introduce RECipe as a multi-purpose recipe recommendation framework with a multi-modal knowledge graph (MMKG) backbone. The motivation behind RECipe is to go beyond (deep) neural collaborative filtering (NCF) by recommending recipes to users when they query in natural language or by providing an image. RECipe consists of 3 subsystems: (1) behavior-based recommender, (2) review-based recommender, and (3) image-based recommender. Each subsystem relies on the embedding representations of entities and relations in the graph. We first obtain (pre-trained) embedding representations of textual entities, such as reviews or ingredients, from a fine-tuned model of Microsoft's MPNet. We initiali
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#30340;&#21512;&#20316;&#23545;&#30740;&#31350;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#30340;&#21512;&#20316;&#20986;&#29256;&#29289;&#25968;&#37327;&#26377;&#22686;&#38271;&#36235;&#21183;&#65292;&#24182;&#19988;&#36825;&#20123;&#20986;&#29256;&#29289;&#24448;&#24448;&#27604;&#20165;&#30001;&#23398;&#26415;&#30028;&#20135;&#29983;&#30340;&#20986;&#29256;&#29289;&#20855;&#26377;&#26356;&#39640;&#30340;&#24433;&#21709;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.04524</link><description>&lt;p&gt;
&#25105;&#24212;&#35813;&#21644;&#35841;&#21512;&#20316;? &#20851;&#20110;NLP&#23398;&#26415;&#30028;&#19982;&#24037;&#19994;&#30028;&#21512;&#20316;&#30340;&#27604;&#36739;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Who should I Collaborate with? A Comparative Study of Academia and Industry Research Collaboration in NLP. (arXiv:2308.04524v1 [cs.DL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04524
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#30340;&#21512;&#20316;&#23545;&#30740;&#31350;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#30340;&#21512;&#20316;&#20986;&#29256;&#29289;&#25968;&#37327;&#26377;&#22686;&#38271;&#36235;&#21183;&#65292;&#24182;&#19988;&#36825;&#20123;&#20986;&#29256;&#29289;&#24448;&#24448;&#27604;&#20165;&#30001;&#23398;&#26415;&#30028;&#20135;&#29983;&#30340;&#20986;&#29256;&#29289;&#20855;&#26377;&#26356;&#39640;&#30340;&#24433;&#21709;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#30740;&#31350;&#30446;&#26631;&#26159;&#35843;&#26597;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#30340;&#21512;&#20316;&#23545;&#30740;&#31350;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#20174;NLP&#35770;&#25991;&#20013;&#25552;&#21462;&#26426;&#26500;&#21644;&#24341;&#29992;&#20449;&#24687;&#30340;&#27969;&#31243;&#65292;&#24182;&#23558;&#20854;&#20998;&#20026;&#19977;&#31867;&#65306;&#23398;&#26415;&#30028;&#12289;&#24037;&#19994;&#30028;&#21644;&#28151;&#21512;&#22411;&#65288;&#23398;&#26415;&#30028;&#19982;&#24037;&#19994;&#30028;&#30340;&#21512;&#20316;&#65289;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#20998;&#26512;&#21457;&#29616;&#65292;&#24037;&#19994;&#30028;&#21644;&#23398;&#26415;&#30028;-&#24037;&#19994;&#30028;&#21512;&#20316;&#20986;&#29256;&#29289;&#30340;&#25968;&#37327;&#21576;&#22686;&#38271;&#36235;&#21183;&#65292;&#24182;&#19988;&#36825;&#20123;&#31867;&#22411;&#30340;&#20986;&#29256;&#29289;&#19982;&#20165;&#22312;&#23398;&#26415;&#30028;&#20135;&#29983;&#30340;&#20986;&#29256;&#29289;&#30456;&#27604;&#65292;&#24448;&#24448;&#20855;&#26377;&#26356;&#39640;&#30340;&#24433;&#21709;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of our research was to investigate the effects of collaboration between academia and industry on Natural Language Processing (NLP). To do this, we created a pipeline to extract affiliations and citations from NLP papers and divided them into three categories: academia, industry, and hybrid (collaborations between academia and industry). Our empirical analysis found that there is a trend towards an increase in industry and academia-industry collaboration publications and that these types of publications tend to have a higher impact compared to those produced solely within academia.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#37327;&#21270;&#24503;&#22269;&#25512;&#29305;&#19978;COVID&#30123;&#24773;&#26399;&#38388;&#25919;&#31574;&#20559;&#22909;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24314;&#31435;&#32454;&#31890;&#24230;&#25919;&#27835;&#20559;&#22909;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;&#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#36827;&#34892;&#20998;&#26512;&#65292;&#32467;&#26524;&#26174;&#31034;&#25919;&#27835;&#35266;&#28857;&#22312;&#30123;&#24773;&#26399;&#38388;&#26377;&#25152;&#22686;&#21152;&#65292;&#30740;&#31350;&#36824;&#31361;&#20986;&#20102;&#19981;&#21516;&#25919;&#27835;&#31867;&#21035;&#30340;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2308.04444</link><description>&lt;p&gt;
&#24503;&#22269;&#25512;&#29305;&#19978;COVID&#30123;&#24773;&#26399;&#38388;&#25919;&#31574;&#20559;&#22909;&#30340;&#21464;&#21270;
&lt;/p&gt;
&lt;p&gt;
Changes in Policy Preferences in German Tweets during the COVID Pandemic. (arXiv:2308.04444v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04444
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#37327;&#21270;&#24503;&#22269;&#25512;&#29305;&#19978;COVID&#30123;&#24773;&#26399;&#38388;&#25919;&#31574;&#20559;&#22909;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24314;&#31435;&#32454;&#31890;&#24230;&#25919;&#27835;&#20559;&#22909;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;&#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#36827;&#34892;&#20998;&#26512;&#65292;&#32467;&#26524;&#26174;&#31034;&#25919;&#27835;&#35266;&#28857;&#22312;&#30123;&#24773;&#26399;&#38388;&#26377;&#25152;&#22686;&#21152;&#65292;&#30740;&#31350;&#36824;&#31361;&#20986;&#20102;&#19981;&#21516;&#25919;&#27835;&#31867;&#21035;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#31038;&#20132;&#23186;&#20307;&#24050;&#25104;&#20026;&#20132;&#27969;&#25919;&#27835;&#35266;&#28857;&#30340;&#37325;&#35201;&#35770;&#22363;&#12290;&#38024;&#23545;COVID&#25514;&#26045;&#65292;&#20844;&#27665;&#30452;&#25509;&#22312;&#36825;&#20123;&#24179;&#21488;&#19978;&#34920;&#36798;&#20102;&#33258;&#24049;&#30340;&#25919;&#31574;&#20559;&#22909;&#12290;&#22312;&#22312;&#32447;&#31038;&#20132;&#23186;&#20307;&#20013;&#37327;&#21270;&#25919;&#27835;&#20559;&#22909;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65306;&#22823;&#37327;&#30340;&#20869;&#23481;&#38656;&#35201;&#21487;&#25193;&#23637;&#30340;&#33258;&#21160;&#25552;&#21462;&#25919;&#27835;&#20559;&#22909;--&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#25968;&#25454;&#38598;&#65292;&#20351;&#29992;&#24403;&#21069;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#25216;&#26415;&#36827;&#34892;&#32454;&#31890;&#24230;&#30340;&#25919;&#27835;&#20559;&#22909;&#25552;&#21462;&#26159;&#22256;&#38590;&#30340;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#24102;&#26377;&#32454;&#31890;&#24230;&#25919;&#27835;&#20559;&#22909;&#27880;&#37322;&#30340;&#25512;&#25991;&#25968;&#25454;&#38598;&#12290;&#20351;&#29992;&#35757;&#32451;&#22312;&#36825;&#20010;&#25968;&#25454;&#19978;&#30340;&#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#21462;&#20102;&#20174;2019&#24180;&#21040;2022&#24180;&#30340;&#24503;&#22269;Twitter&#35821;&#26009;&#24211;&#20013;&#30340;&#25919;&#31574;&#20559;&#22909;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23545;&#20110;COVID&#30123;&#24773;&#65292;&#25919;&#27835;&#35266;&#28857;&#30340;&#34920;&#36798;&#22686;&#21152;&#20102;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#25104;&#29087;&#30340;&#25919;&#31574;&#20559;&#22909;&#20998;&#31867;&#27861;&#20998;&#26512;&#20102;&#32454;&#31890;&#24230;&#30340;&#25919;&#27835;&#35266;&#28857;&#65292;&#24182;&#31361;&#20986;&#20102;&#19981;&#21516;&#25919;&#27835;&#31867;&#21035;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online social media have become an important forum for exchanging political opinions. In response to COVID measures citizens expressed their policy preferences directly on these platforms. Quantifying political preferences in online social media remains challenging: The vast amount of content requires scalable automated extraction of political preferences -- however fine grained political preference extraction is difficult with current machine learning (ML) technology, due to the lack of data sets. Here we present a novel data set of tweets with fine grained political preference annotations. A text classification model trained on this data is used to extract policy preferences in a German Twitter corpus ranging from 2019 to 2022. Our results indicate that in response to the COVID pandemic, expression of political opinions increased. Using a well established taxonomy of policy preferences we analyse fine grained political views and highlight changes in distinct political categories. The
&lt;/p&gt;</description></item><item><title>OpinionConv&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#27169;&#25311;&#38144;&#21806;&#23545;&#35805;&#30340;&#23545;&#35805;&#24335;AI&#65292;&#36890;&#36807;&#21033;&#29992;&#20135;&#21697;&#35780;&#35770;&#20316;&#20026;&#35266;&#28857;&#30340;&#20016;&#23500;&#26469;&#28304;&#65292;&#23454;&#29616;&#20102;&#23545;&#35805;&#21644;&#20915;&#31574;&#20013;&#30340;&#30495;&#23454;&#24615;&#21644;&#20449;&#24687;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2308.04226</link><description>&lt;p&gt;
OpinionConv: &#36890;&#36807;&#22522;&#20110;&#30495;&#23454;&#20027;&#35266;&#20307;&#39564;&#30340;&#35266;&#28857;&#23454;&#29616;&#23545;&#35805;&#24335;&#20135;&#21697;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
OpinionConv: Conversational Product Search with Grounded Opinions. (arXiv:2308.04226v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04226
&lt;/p&gt;
&lt;p&gt;
OpinionConv&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#27169;&#25311;&#38144;&#21806;&#23545;&#35805;&#30340;&#23545;&#35805;&#24335;AI&#65292;&#36890;&#36807;&#21033;&#29992;&#20135;&#21697;&#35780;&#35770;&#20316;&#20026;&#35266;&#28857;&#30340;&#20016;&#23500;&#26469;&#28304;&#65292;&#23454;&#29616;&#20102;&#23545;&#35805;&#21644;&#20915;&#31574;&#20013;&#30340;&#30495;&#23454;&#24615;&#21644;&#20449;&#24687;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25628;&#32034;&#20135;&#21697;&#26102;&#65292;&#20182;&#20154;&#30340;&#35266;&#28857;&#22312;&#20570;&#20986;&#26126;&#26234;&#20915;&#31574;&#26041;&#38754;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#23545;&#20135;&#21697;&#30340;&#20027;&#35266;&#20307;&#39564;&#21487;&#20197;&#26159;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#26469;&#28304;&#12290;&#36825;&#22312;&#38144;&#21806;&#23545;&#35805;&#20013;&#20063;&#26159;&#22914;&#27492;&#65292;&#22312;&#36825;&#31181;&#23545;&#35805;&#20013;&#65292;&#23458;&#25143;&#21644;&#38144;&#21806;&#21161;&#25163;&#20132;&#25442;&#26377;&#20851;&#20135;&#21697;&#30340;&#20107;&#23454;&#21644;&#35266;&#28857;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#19968;&#20010;&#29992;&#20110;&#27492;&#31867;&#23545;&#35805;&#30340;AI&#26159;&#22797;&#26434;&#30340;&#65292;&#22240;&#20026;&#35821;&#35328;&#27169;&#22411;&#30001;&#20110;&#32570;&#20047;&#30495;&#23454;&#19990;&#30028;&#30340;&#32463;&#39564;&#27809;&#26377;&#30495;&#23454;&#30340;&#35266;&#28857;&#12290;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#20135;&#21697;&#35780;&#35770;&#20316;&#20026;&#20135;&#21697;&#35266;&#28857;&#30340;&#20016;&#23500;&#26469;&#28304;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20197;&#30495;&#23454;&#20027;&#35266;&#21465;&#36848;&#25903;&#25345;&#23545;&#35805;&#24335;AI&#12290;&#36890;&#36807;OpinionConv&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#31532;&#19968;&#20010;&#27169;&#25311;&#38144;&#21806;&#23545;&#35805;&#30340;&#23545;&#35805;&#24335;AI&#12290;&#20026;&#20102;&#39564;&#35777;&#29983;&#25104;&#30340;&#23545;&#35805;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#22810;&#20010;&#29992;&#25143;&#30740;&#31350;&#65292;&#32467;&#26524;&#26174;&#31034;&#29983;&#25104;&#30340;&#35266;&#28857;&#34987;&#35748;&#20026;&#26159;&#30495;&#23454;&#30340;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#21592;&#20063;&#30830;&#35748;&#20102;&#35266;&#28857;&#23545;&#20110;&#20915;&#31574;&#30340;&#20449;&#24687;&#22522;&#30784;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
When searching for products, the opinions of others play an important role in making informed decisions. Subjective experiences about a product can be a valuable source of information. This is also true in sales conversations, where a customer and a sales assistant exchange facts and opinions about products. However, training an AI for such conversations is complicated by the fact that language models do not possess authentic opinions for their lack of real-world experience. We address this problem by leveraging product reviews as a rich source of product opinions to ground conversational AI in true subjective narratives. With OpinionConv, we develop the first conversational AI for simulating sales conversations. To validate the generated conversations, we conduct several user studies showing that the generated opinions are perceived as realistic. Our assessors also confirm the importance of opinions as an informative basis for decision-making.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25512;&#33616;&#31995;&#32479;&#27169;&#22359;&#65292;&#31216;&#20026;&#31227;&#21160;&#20379;&#24212;&#65292;&#26088;&#22312;&#35299;&#20915;&#20998;&#39029;&#26426;&#21046;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#29992;&#25143;&#35774;&#22791;&#19978;&#37096;&#32626;&#25512;&#33616;&#31639;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#20943;&#23569;&#25968;&#25454;&#20256;&#36755;&#24310;&#36831;&#21644;&#25552;&#21319;&#29992;&#25143;&#30340;&#27785;&#28024;&#24335;&#20307;&#39564;&#12290;</title><link>http://arxiv.org/abs/2308.03855</link><description>&lt;p&gt;
&#31227;&#21160;&#20379;&#24212;&#65306;&#25512;&#33616;&#31995;&#32479;&#30340;&#26368;&#21518;&#19968;&#22359;&#25340;&#22270;
&lt;/p&gt;
&lt;p&gt;
Mobile Supply: The Last Piece of Jigsaw of Recommender System. (arXiv:2308.03855v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03855
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25512;&#33616;&#31995;&#32479;&#27169;&#22359;&#65292;&#31216;&#20026;&#31227;&#21160;&#20379;&#24212;&#65292;&#26088;&#22312;&#35299;&#20915;&#20998;&#39029;&#26426;&#21046;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#29992;&#25143;&#35774;&#22791;&#19978;&#37096;&#32626;&#25512;&#33616;&#31639;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#20943;&#23569;&#25968;&#25454;&#20256;&#36755;&#24310;&#36831;&#21644;&#25552;&#21319;&#29992;&#25143;&#30340;&#27785;&#28024;&#24335;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#26159;&#22312;&#32447;&#24179;&#21488;&#30340;&#22522;&#26412;&#21151;&#33021;&#12290;&#38543;&#30528;&#25163;&#26426;&#35745;&#31639;&#33021;&#21147;&#30340;&#21457;&#23637;&#65292;&#19968;&#20123;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#23558;&#25512;&#33616;&#31639;&#27861;&#37096;&#32626;&#22312;&#29992;&#25143;&#35774;&#22791;&#19978;&#65292;&#20197;&#35299;&#20915;&#25968;&#25454;&#20256;&#36755;&#24310;&#36831;&#21644;&#20998;&#39029;&#26426;&#21046;&#31561;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#36793;&#32536;&#31471;&#31227;&#21160;&#25490;&#21517;&#19981;&#33021;&#23436;&#20840;&#35299;&#20915;&#20998;&#39029;&#26426;&#21046;&#38382;&#39064;&#12290;&#31227;&#21160;&#25490;&#21517;&#21482;&#33021;&#23545;&#24403;&#21069;&#39029;&#38754;&#19978;&#30340;&#39033;&#30446;&#36827;&#34892;&#25490;&#24207;&#65292;&#25152;&#20197;&#22914;&#26524;&#21482;&#35843;&#29992;&#19968;&#20004;&#27425;&#26159;&#19981;&#36215;&#20316;&#29992;&#30340;&#12290;&#27492;&#22806;&#65292;&#22312;&#29992;&#25143;&#26597;&#30475;&#20102;&#24403;&#21069;&#39029;&#38754;&#19978;&#30340;&#24863;&#20852;&#36259;&#30340;&#39033;&#30446;&#21518;&#65292;&#29992;&#25143;&#20250;&#21047;&#26032;&#39029;&#38754;&#33719;&#21462;&#26032;&#30340;&#39033;&#30446;&#12290;&#36825;&#20250;&#20351;&#31227;&#21160;&#25490;&#21517;&#27169;&#22411;&#20570;&#24456;&#22810;&#26080;&#29992;&#21151;&#65292;&#24433;&#21709;&#29992;&#25143;&#30340;&#27785;&#28024;&#24335;&#20307;&#39564;&#12290;&#20026;&#20102;&#35299;&#20915;&#20998;&#39029;&#26426;&#21046;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;&#25512;&#33616;&#31995;&#32479;&#30340;&#27969;&#27700;&#32447;&#20013;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#26032;&#30340;&#27169;&#22359;&#65292;&#31216;&#20026;&#31227;&#21160;&#20379;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommendation system is a fundamental functionality of online platforms. With the development of computing power of mobile phones, some researchers have deployed recommendation algorithms on users' devices to solve the problems of data transmission delay and pagination mechanism. However, the existing edge-side mobile rankings cannot completely solve the problem of pagination mechanism. The mobile rankings can only sort the items on the current page, so it will not work if it is called once or twice. Besides, after the user has viewed the items of interest to the user on the current page, the user refresh to get a new page of items. This will make the mobile ranking model do a lot of useless work and affect the user's immersive experience. In order to solve the pagination mechanism problem, we propose a completely new module in the pipeline of recommender named Mobile Supply. The pipeline of recommender system is extended to "retrival-&gt;pre-ranking-&gt;ranking-&gt;re-ranking-&gt;Mobile Supply-&gt;
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20855;&#26377;&#22823;&#21160;&#20316;&#31354;&#38388;&#30340;&#31163;&#31574;&#30053;&#35780;&#20272;&#30340;&#21452;&#37325;&#31283;&#20581;&#20272;&#35745;&#22120;&#65288;MDR&#65289;&#12290;&#19982;&#29616;&#26377;&#30340;&#22522;&#20934;&#20272;&#35745;&#22120;&#30456;&#27604;&#65292;MDR&#33021;&#22815;&#22312;&#20943;&#23567;&#26041;&#24046;&#30340;&#21516;&#26102;&#20445;&#25345;&#26080;&#20559;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#23454;&#20102;MDR&#30456;&#23545;&#20110;&#29616;&#26377;&#20272;&#35745;&#22120;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.03443</link><description>&lt;p&gt;
&#29992;&#20110;&#20855;&#26377;&#22823;&#21160;&#20316;&#31354;&#38388;&#30340;&#31163;&#31574;&#30053;&#35780;&#20272;&#30340;&#21452;&#37325;&#31283;&#20581;&#20272;&#35745;&#22120;
&lt;/p&gt;
&lt;p&gt;
Doubly Robust Estimator for Off-Policy Evaluation with Large Action Spaces. (arXiv:2308.03443v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03443
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20855;&#26377;&#22823;&#21160;&#20316;&#31354;&#38388;&#30340;&#31163;&#31574;&#30053;&#35780;&#20272;&#30340;&#21452;&#37325;&#31283;&#20581;&#20272;&#35745;&#22120;&#65288;MDR&#65289;&#12290;&#19982;&#29616;&#26377;&#30340;&#22522;&#20934;&#20272;&#35745;&#22120;&#30456;&#27604;&#65292;MDR&#33021;&#22815;&#22312;&#20943;&#23567;&#26041;&#24046;&#30340;&#21516;&#26102;&#20445;&#25345;&#26080;&#20559;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#23454;&#20102;MDR&#30456;&#23545;&#20110;&#29616;&#26377;&#20272;&#35745;&#22120;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20855;&#26377;&#22823;&#21160;&#20316;&#31354;&#38388;&#30340;&#32972;&#26223;&#19979;&#30340;&#31163;&#31574;&#30053;&#35780;&#20272;&#65288;OPE&#65289;&#12290;&#29616;&#26377;&#30340;&#22522;&#20934;&#20272;&#35745;&#22120;&#23384;&#22312;&#20005;&#37325;&#30340;&#20559;&#24046;&#21644;&#26041;&#24046;&#25240;&#34935;&#38382;&#39064;&#12290;&#21442;&#25968;&#21270;&#26041;&#27861;&#30001;&#20110;&#24456;&#38590;&#30830;&#23450;&#27491;&#30830;&#30340;&#27169;&#22411;&#32780;&#23548;&#33268;&#20559;&#24046;&#65292;&#32780;&#37325;&#35201;&#24615;&#21152;&#26435;&#26041;&#27861;&#30001;&#20110;&#26041;&#24046;&#32780;&#20135;&#29983;&#38382;&#39064;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#21028;&#21035;&#24335;&#30340;&#19981;&#33391;&#34892;&#20026;&#25233;&#21046;&#22120;&#65288;MIPS&#65289;&#26469;&#36890;&#36807;&#23545;&#21160;&#20316;&#30340;&#23884;&#20837;&#26469;&#20943;&#23567;&#20272;&#35745;&#22120;&#30340;&#26041;&#24046;&#12290;&#20026;&#20102;&#20351;&#20272;&#35745;&#22120;&#26356;&#20934;&#30830;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MIPS&#30340;&#21452;&#37325;&#31283;&#20581;&#20272;&#35745;&#22120;&#8212;&#8212;&#36793;&#38469;&#21270;&#21452;&#37325;&#31283;&#20581;&#65288;MDR&#65289;&#20272;&#35745;&#22120;&#12290;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#20272;&#35745;&#22120;&#22312;&#27604;MIPS&#26356;&#24369;&#30340;&#20551;&#35774;&#19979;&#26159;&#26080;&#20559;&#30340;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#23545;IPS&#30340;&#26041;&#24046;&#20943;&#23567;&#65292;&#36825;&#26159;MIPS&#30340;&#20027;&#35201;&#20248;&#21183;&#12290;&#32463;&#39564;&#23454;&#39564;&#35777;&#23454;&#20102;MDR&#30456;&#23545;&#20110;&#29616;&#26377;&#20272;&#35745;&#22120;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study Off-Policy Evaluation (OPE) in contextual bandit settings with large action spaces. The benchmark estimators suffer from severe bias and variance tradeoffs. Parametric approaches suffer from bias due to difficulty specifying the correct model, whereas ones with importance weight suffer from variance. To overcome these limitations, Marginalized Inverse Propensity Scoring (MIPS) was proposed to mitigate the estimator's variance via embeddings of an action. To make the estimator more accurate, we propose the doubly robust estimator of MIPS called the Marginalized Doubly Robust (MDR) estimator. Theoretical analysis shows that the proposed estimator is unbiased under weaker assumptions than MIPS while maintaining variance reduction against IPS, which was the main advantage of MIPS. The empirical experiment verifies the supremacy of MDR against existing estimators.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#39034;&#24207;&#25512;&#33616;&#19968;&#33268;&#24615;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#33268;&#24615;&#32422;&#26463;&#65288;C$^2$-Rec&#65289;&#65292;&#21482;&#20351;&#29992;&#20004;&#20010;&#39069;&#22806;&#30340;&#35757;&#32451;&#30446;&#26631;&#65292;&#32780;&#19981;&#38656;&#35201;&#36827;&#34892;&#32467;&#26500;&#20462;&#25913;&#21644;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#65292;&#20174;&#32780;&#29983;&#25104;&#26356;&#22909;&#30340;&#29992;&#25143;&#34920;&#31034;&#24182;&#21521;&#29992;&#25143;&#25512;&#33616;&#26356;&#26377;&#21560;&#24341;&#21147;&#30340;&#29289;&#21697;&#12290;</title><link>http://arxiv.org/abs/2112.06668</link><description>&lt;p&gt;
CT4Rec: &#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#39034;&#24207;&#25512;&#33616;&#19968;&#33268;&#24615;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
CT4Rec: Simple yet Effective Consistency Training for Sequential Recommendation. (arXiv:2112.06668v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.06668
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#39034;&#24207;&#25512;&#33616;&#19968;&#33268;&#24615;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#33268;&#24615;&#32422;&#26463;&#65288;C$^2$-Rec&#65289;&#65292;&#21482;&#20351;&#29992;&#20004;&#20010;&#39069;&#22806;&#30340;&#35757;&#32451;&#30446;&#26631;&#65292;&#32780;&#19981;&#38656;&#35201;&#36827;&#34892;&#32467;&#26500;&#20462;&#25913;&#21644;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#65292;&#20174;&#32780;&#29983;&#25104;&#26356;&#22909;&#30340;&#29992;&#25143;&#34920;&#31034;&#24182;&#21521;&#29992;&#25143;&#25512;&#33616;&#26356;&#26377;&#21560;&#24341;&#21147;&#30340;&#29289;&#21697;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39034;&#24207;&#25512;&#33616;&#26041;&#27861;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#25512;&#33616;&#31995;&#32479;&#20013;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#36825;&#20123;&#31995;&#32479;&#36890;&#36807;&#21033;&#29992;&#21382;&#21490;&#35760;&#24405;&#26469;&#25429;&#25417;&#29992;&#25143;&#20559;&#22909;&#65292;&#28982;&#21518;&#36827;&#34892;&#25512;&#33616;&#12290;&#23545;&#27604;&#23398;&#20064;&#65288;CL&#65289;&#26159;&#19968;&#31181;&#20808;&#36827;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#33719;&#24471;&#26377;&#20449;&#24687;&#37327;&#30340;&#29992;&#25143;&#34920;&#31034;&#65292;&#20294;&#36825;&#20123;&#22522;&#20110;CL&#30340;&#27169;&#22411;&#38656;&#35201;&#31934;&#32454;&#30340;&#36127;&#37319;&#26679;&#31574;&#30053;&#12289;&#32321;&#29712;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#21644;&#22823;&#37327;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#24037;&#20316;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21478;&#19968;&#31181;&#26041;&#27861;&#26469;&#29983;&#25104;&#26356;&#22909;&#30340;&#29992;&#25143;&#34920;&#31034;&#65292;&#24182;&#21521;&#29992;&#25143;&#25512;&#33616;&#26356;&#26377;&#21560;&#24341;&#21147;&#30340;&#29289;&#21697;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#39034;&#24207;&#25512;&#33616;&#20013;&#20351;&#29992;&#30340;&#26377;&#25928;&#30340;&#19968;&#33268;&#24615;&#32422;&#26463;&#65288;C$^2$-Rec&#65289;&#65292;&#20854;&#20013;&#21482;&#20351;&#29992;&#20102;&#20004;&#20010;&#39069;&#22806;&#30340;&#35757;&#32451;&#30446;&#26631;&#65292;&#32780;&#27809;&#26377;&#36827;&#34892;&#20219;&#20309;&#32467;&#26500;&#20462;&#25913;&#21644;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#30495;&#23454;&#24037;&#19994;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequential recommendation methods play an important role in real-world recommender systems. These systems are able to catch user preferences by taking advantage of historical records and then performing recommendations. Contrastive learning(CL) is a cutting-edge technology that can assist us in obtaining informative user representations, but these CL-based models need subtle negative sampling strategies, tedious data augmentation methods, and heavy hyper-parameters tuning work. In this paper, we introduce another way to generate better user representations and recommend more attractive items to users. Particularly, we put forward an effective \textbf{C}onsistency \textbf{C}onstraint for sequential \textbf{Rec}ommendation(C$^2$-Rec) in which only two extra training objectives are used without any structural modifications and data augmentation strategies. Substantial experiments have been conducted on three benchmark datasets and one real industrial dataset, which proves that our propose
&lt;/p&gt;</description></item></channel></rss>