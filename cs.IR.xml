<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#20351;&#29992;&#25991;&#26412;&#25366;&#25496;&#23558;&#20998;&#23618;&#25968;&#25454;&#27169;&#22411;&#36716;&#21270;&#20026;&#30693;&#35782;&#22270;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#35780;&#20272;&#22270;&#36136;&#37327;&#25351;&#26631;&#21644;&#35821;&#20041;&#30456;&#20284;&#24615;&#26469;&#39564;&#35777;&#20854;&#26377;&#25928;&#24615;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25552;&#20379;&#20010;&#24615;&#21270;&#23398;&#20064;&#36335;&#24452;&#25512;&#33616;&#30340;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2401.13609</link><description>&lt;p&gt;
&#20351;&#29992;&#25991;&#26412;&#25366;&#25496;&#21644;&#35821;&#20041;&#22270;&#23436;&#25104;&#26500;&#24314;&#20010;&#24615;&#21270;&#23398;&#20064;&#25512;&#33616;&#30340;&#19978;&#19979;&#25991;&#30693;&#35782;&#22270;&#35889;
&lt;/p&gt;
&lt;p&gt;
Building Contextual Knowledge Graphs for Personalized Learning Recommendations using Text Mining and Semantic Graph Completion. (arXiv:2401.13609v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13609
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#20351;&#29992;&#25991;&#26412;&#25366;&#25496;&#23558;&#20998;&#23618;&#25968;&#25454;&#27169;&#22411;&#36716;&#21270;&#20026;&#30693;&#35782;&#22270;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#35780;&#20272;&#22270;&#36136;&#37327;&#25351;&#26631;&#21644;&#35821;&#20041;&#30456;&#20284;&#24615;&#26469;&#39564;&#35777;&#20854;&#26377;&#25928;&#24615;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25552;&#20379;&#20010;&#24615;&#21270;&#23398;&#20064;&#36335;&#24452;&#25512;&#33616;&#30340;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#25311;&#23398;&#20064;&#23545;&#35937;&#65288;LO&#65289;&#22312;&#20854;&#19978;&#19979;&#25991;&#20013;&#21487;&#20197;&#20351;&#23398;&#20064;&#32773;&#20174;&#22522;&#30784;&#30340;&#35760;&#24518;&#32423;&#23398;&#20064;&#30446;&#26631;&#21457;&#23637;&#21040;&#26356;&#39640;&#32423;&#21035;&#30340;&#24212;&#29992;&#21644;&#20998;&#26512;&#30446;&#26631;&#12290;&#23613;&#31649;&#25968;&#23383;&#23398;&#20064;&#24179;&#21488;&#36890;&#24120;&#20351;&#29992;&#20998;&#23618;&#25968;&#25454;&#27169;&#22411;&#65292;&#20294;&#20351;&#29992;&#22522;&#20110;&#22270;&#30340;&#27169;&#22411;&#21487;&#20197;&#22312;&#36825;&#20123;&#24179;&#21488;&#19978;&#34920;&#31034;LO&#30340;&#19978;&#19979;&#25991;&#12290;&#36825;&#20026;&#20010;&#24615;&#21270;&#23398;&#20064;&#36335;&#24452;&#30340;&#25512;&#33616;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;&#26412;&#25991;&#20171;&#32461;&#21644;&#35780;&#20272;&#20102;&#20351;&#29992;&#25991;&#26412;&#25366;&#25496;&#23558;&#20998;&#23618;&#25968;&#25454;&#27169;&#22411;&#36716;&#25442;&#20026;LO&#30340;&#30693;&#35782;&#22270;&#65288;KG&#65289;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#21033;&#29992;&#33258;&#23450;&#20041;&#30340;&#25991;&#26412;&#25366;&#25496;&#27969;&#31243;&#26469;&#25366;&#25496;&#19987;&#23478;&#31574;&#21010;&#30340;&#20998;&#23618;&#27169;&#22411;&#20803;&#32032;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#12290;&#25105;&#20204;&#20351;&#29992;&#22270;&#36136;&#37327;&#25511;&#21046;&#25351;&#26631;&#21644;&#31639;&#27861;&#35821;&#20041;&#30456;&#20284;&#24615;&#19982;&#19987;&#23478;&#23450;&#20041;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#30340;&#27604;&#36739;&#26469;&#35780;&#20272;KG&#32467;&#26500;&#21644;&#20851;&#31995;&#25552;&#21462;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;KG&#20013;&#30340;&#20851;&#31995;&#22312;&#35821;&#20041;&#19978;&#19982;&#19987;&#23478;&#23450;&#20041;&#30340;&#20851;&#31995;&#26159;&#21487;&#27604;&#36739;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modelling learning objects (LO) within their context enables the learner to advance from a basic, remembering-level, learning objective to a higher-order one, i.e., a level with an application- and analysis objective. While hierarchical data models are commonly used in digital learning platforms, using graph-based models enables representing the context of LOs in those platforms. This leads to a foundation for personalized recommendations of learning paths. In this paper, the transformation of hierarchical data models into knowledge graph (KG) models of LOs using text mining is introduced and evaluated. We utilize custom text mining pipelines to mine semantic relations between elements of an expert-curated hierarchical model. We evaluate the KG structure and relation extraction using graph quality-control metrics and the comparison of algorithmic semantic-similarities to expert-defined ones. The results show that the relations in the KG are semantically comparable to those defined by d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25104;&#26412;&#25935;&#24863;&#26041;&#27861;&#65292;&#26088;&#22312;&#30830;&#20445;&#25512;&#33616;&#31995;&#32479;&#20013;&#25552;&#20379;&#32773;&#30340;&#20844;&#24179;&#26333;&#20809;&#24230;&#12290;&#35813;&#26041;&#27861;&#37327;&#21270;&#21644;&#20943;&#23569;&#20102;&#19981;&#21516;&#25552;&#20379;&#32773;&#32676;&#20307;&#22312;&#25512;&#33616;&#32467;&#26524;&#20013;&#30340;&#24046;&#24322;&#65292;&#20197;&#23454;&#29616;&#20844;&#24179;&#21407;&#21017;&#12290;</title><link>http://arxiv.org/abs/2401.13566</link><description>&lt;p&gt;
&#19968;&#31181;&#20851;&#27880;&#20844;&#24179;&#24615;&#30340;&#25104;&#26412;&#25935;&#24863;&#20803;&#23398;&#20064;&#31574;&#30053;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#25552;&#20379;&#32773;&#26333;&#20809;&#24230;
&lt;/p&gt;
&lt;p&gt;
A Cost-Sensitive Meta-Learning Strategy for Fair Provider Exposure in Recommendation. (arXiv:2401.13566v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13566
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25104;&#26412;&#25935;&#24863;&#26041;&#27861;&#65292;&#26088;&#22312;&#30830;&#20445;&#25512;&#33616;&#31995;&#32479;&#20013;&#25552;&#20379;&#32773;&#30340;&#20844;&#24179;&#26333;&#20809;&#24230;&#12290;&#35813;&#26041;&#27861;&#37327;&#21270;&#21644;&#20943;&#23569;&#20102;&#19981;&#21516;&#25552;&#20379;&#32773;&#32676;&#20307;&#22312;&#25512;&#33616;&#32467;&#26524;&#20013;&#30340;&#24046;&#24322;&#65292;&#20197;&#23454;&#29616;&#20844;&#24179;&#21407;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35774;&#35745;&#25512;&#33616;&#26381;&#21153;&#26102;&#65292;&#32771;&#34385;&#21040;&#25152;&#26377;&#20869;&#23481;&#25552;&#20379;&#32773;&#30340;&#21033;&#30410;&#38750;&#24120;&#37325;&#35201;&#65292;&#36825;&#20123;&#25552;&#20379;&#32773;&#21253;&#25324;&#26032;&#36827;&#20837;&#32773;&#21644;&#23569;&#25968;&#27665;&#26063;&#32676;&#20307;&#12290;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#29305;&#23450;&#30340;&#25552;&#20379;&#32773;&#32676;&#20307;&#22312;&#29289;&#21697;&#30446;&#24405;&#20013;&#30340;&#20195;&#34920;&#24615;&#36739;&#20302;&#65292;&#36825;&#21487;&#33021;&#20250;&#24433;&#21709;&#25512;&#33616;&#32467;&#26524;&#12290;&#22240;&#27492;&#65292;&#24179;&#21488;&#25317;&#26377;&#32773;&#36890;&#24120;&#24076;&#26395;&#35843;&#33410;&#36825;&#20123;&#25552;&#20379;&#32773;&#32676;&#20307;&#22312;&#25512;&#33616;&#21015;&#34920;&#20013;&#30340;&#26333;&#20809;&#24230;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25104;&#26412;&#25935;&#24863;&#26041;&#27861;&#65292;&#26088;&#22312;&#22312;&#25104;&#23545;&#25512;&#33616;&#27169;&#22411;&#20013;&#30830;&#20445;&#36825;&#20123;&#30446;&#26631;&#26333;&#20809;&#27700;&#24179;&#12290;&#36825;&#31181;&#26041;&#27861;&#37327;&#21270;&#24182;&#20943;&#23569;&#20102;&#20998;&#37197;&#32473;&#32676;&#20307;&#30340;&#25512;&#33616;&#37327;&#19982;&#29289;&#21697;&#30446;&#24405;&#20013;&#30340;&#36129;&#29486;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#36981;&#24490;&#20844;&#24179;&#21407;&#21017;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#30830;&#20445;&#32676;&#20307;&#26333;&#20809;&#24230;&#19982;&#20854;&#20998;&#37197;&#27700;&#24179;&#19968;&#33268;&#30340;&#21516;&#26102;&#65292;&#19981;&#20250;&#25439;&#23475;&#21407;&#22987;&#30340;&#25512;&#33616;&#25928;&#26524;&#12290;&#21487;&#20197;&#33719;&#21462;&#28304;&#20195;&#30721;&#21644;&#39044;&#22788;&#29702;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
When devising recommendation services, it is important to account for the interests of all content providers, encompassing not only newcomers but also minority demographic groups. In various instances, certain provider groups find themselves underrepresented in the item catalog, a situation that can influence recommendation results. Hence, platform owners often seek to regulate the exposure of these provider groups in the recommended lists. In this paper, we propose a novel cost-sensitive approach designed to guarantee these target exposure levels in pairwise recommendation models. This approach quantifies, and consequently mitigate, the discrepancies between the volume of recommendations allocated to groups and their contribution in the item catalog, under the principle of equity. Our results show that this approach, while aligning groups exposure with their assigned levels, does not compromise to the original recommendation utility. Source code and pre-processed data can be retrieved
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#22522;&#20110;&#25351;&#20196;&#30340;&#27169;&#22411;&#36827;&#34892;&#32454;&#31890;&#24230;&#30340;&#21512;&#21516;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;NER&#20219;&#21153;&#36716;&#21270;&#20026;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#39640;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;NER&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.13545</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#25351;&#20196;&#30340;&#27169;&#22411;&#36827;&#34892;&#32454;&#31890;&#24230;&#30340;&#21512;&#21516;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Fine-grained Contract NER using instruction based model. (arXiv:2401.13545v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13545
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#22522;&#20110;&#25351;&#20196;&#30340;&#27169;&#22411;&#36827;&#34892;&#32454;&#31890;&#24230;&#30340;&#21512;&#21516;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;NER&#20219;&#21153;&#36716;&#21270;&#20026;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#39640;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;NER&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#25351;&#20196;&#30340;&#25216;&#26415;&#22312;&#25913;&#21892;&#23569;&#26679;&#26412;&#23398;&#20064;&#22330;&#26223;&#19979;&#30340;&#24615;&#33021;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#23427;&#20204;&#36890;&#36807;&#24357;&#21512;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#38024;&#23545;&#29305;&#23450;&#19979;&#28216;&#20219;&#21153;&#30340;&#24494;&#35843;&#20043;&#38388;&#30340;&#24046;&#36317;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36825;&#20123;&#36827;&#23637;&#65292;&#20294;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#31561;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#20013;&#65292;&#20351;&#29992;&#25552;&#31034;&#25110;&#25351;&#20196;&#20173;&#28982;&#19981;&#21450;&#21463;&#30417;&#30563;&#30340;&#22522;&#32447;&#12290;&#36896;&#25104;&#36825;&#31181;&#24615;&#33021;&#24046;&#36317;&#30340;&#21407;&#22240;&#21487;&#20197;&#24402;&#22240;&#20110;NER&#21644;LLMs&#20043;&#38388;&#30340;&#22522;&#26412;&#24046;&#24322;&#12290;NER&#26412;&#36136;&#19978;&#26159;&#19968;&#20010;&#24207;&#21015;&#26631;&#27880;&#20219;&#21153;&#65292;&#27169;&#22411;&#24517;&#39035;&#20026;&#21477;&#23376;&#20013;&#30340;&#21508;&#20010;&#26631;&#35760;&#20998;&#37197;&#23454;&#20307;&#31867;&#22411;&#26631;&#31614;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;LLMs&#34987;&#35774;&#35745;&#20026;&#19968;&#20010;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#12290;&#35821;&#20041;&#26631;&#27880;&#21644;&#25991;&#26412;&#29983;&#25104;&#20043;&#38388;&#30340;&#21306;&#21035;&#23548;&#33268;&#24615;&#33021;&#19981;&#20339;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;NER&#20219;&#21153;&#36716;&#21270;&#20026;&#19968;&#20010;&#21487;&#20197;&#34987;LLMs&#36731;&#26494;&#36866;&#24212;&#30340;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#12290;&#36825;&#28041;&#21450;&#22686;&#24378;&#28304;&#21477;&#23376;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lately, instruction-based techniques have made significant strides in improving performance in few-shot learning scenarios. They achieve this by bridging the gap between pre-trained language models and fine-tuning for specific downstream tasks. Despite these advancements, the performance of Large Language Models (LLMs) in information extraction tasks like Named Entity Recognition (NER), using prompts or instructions, still falls short of supervised baselines. The reason for this performance gap can be attributed to the fundamental disparity between NER and LLMs. NER is inherently a sequence labeling task, where the model must assign entity-type labels to individual tokens within a sentence. In contrast, LLMs are designed as a text generation task. This distinction between semantic labeling and text generation leads to subpar performance. In this paper, we transform the NER task into a text-generation task that can be readily adapted by LLMs. This involves enhancing source sentences wit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#20266;&#30456;&#20851;&#21453;&#39304;&#27169;&#22411;&#65288;TPRF&#65289;&#65292;&#36866;&#29992;&#20110;&#36164;&#28304;&#21463;&#38480;&#30340;&#29615;&#22659;&#12290;TPRF&#30456;&#27604;&#20854;&#20182;&#28145;&#24230;&#35821;&#35328;&#27169;&#22411;&#22312;&#20869;&#23384;&#21344;&#29992;&#21644;&#25512;&#29702;&#26102;&#38388;&#26041;&#38754;&#20855;&#22791;&#26356;&#23567;&#30340;&#24320;&#38144;&#65292;&#24182;&#33021;&#26377;&#25928;&#22320;&#32467;&#21512;&#26469;&#33258;&#31264;&#23494;&#25991;&#20855;&#34920;&#31034;&#30340;&#30456;&#20851;&#21453;&#39304;&#20449;&#21495;&#12290;</title><link>http://arxiv.org/abs/2401.13509</link><description>&lt;p&gt;
TPRF:&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#20266;&#30456;&#20851;&#21453;&#39304;&#27169;&#22411;&#65292;&#29992;&#20110;&#39640;&#25928;&#19988;&#26377;&#25928;&#30340;&#26816;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
TPRF: A Transformer-based Pseudo-Relevance Feedback Model for Efficient and Effective Retrieval. (arXiv:2401.13509v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13509
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#20266;&#30456;&#20851;&#21453;&#39304;&#27169;&#22411;&#65288;TPRF&#65289;&#65292;&#36866;&#29992;&#20110;&#36164;&#28304;&#21463;&#38480;&#30340;&#29615;&#22659;&#12290;TPRF&#30456;&#27604;&#20854;&#20182;&#28145;&#24230;&#35821;&#35328;&#27169;&#22411;&#22312;&#20869;&#23384;&#21344;&#29992;&#21644;&#25512;&#29702;&#26102;&#38388;&#26041;&#38754;&#20855;&#22791;&#26356;&#23567;&#30340;&#24320;&#38144;&#65292;&#24182;&#33021;&#26377;&#25928;&#22320;&#32467;&#21512;&#26469;&#33258;&#31264;&#23494;&#25991;&#20855;&#34920;&#31034;&#30340;&#30456;&#20851;&#21453;&#39304;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#29615;&#22659;&#20013;&#65292;&#22914;&#24265;&#20215;&#20113;&#23454;&#20363;&#25110;&#23884;&#20837;&#24335;&#31995;&#32479;&#65288;&#22914;&#26234;&#33021;&#25163;&#26426;&#21644;&#26234;&#33021;&#25163;&#34920;&#65289;&#20013;&#65292;&#38024;&#23545;&#31264;&#23494;&#26816;&#32034;&#22120;&#30340;&#20266;&#30456;&#20851;&#21453;&#39304;&#65288;PRF&#65289;&#26041;&#27861;&#65292;&#20854;&#20013;&#20869;&#23384;&#21644;CPU&#21463;&#38480;&#65292;&#27809;&#26377;GPU&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;PRF&#26041;&#27861;&#65288;TPRF&#65289;&#65292;&#19982;&#37319;&#29992;PRF&#26426;&#21046;&#30340;&#20854;&#20182;&#28145;&#24230;&#35821;&#35328;&#27169;&#22411;&#30456;&#27604;&#65292;&#20855;&#26377;&#26356;&#23567;&#30340;&#20869;&#23384;&#21344;&#29992;&#21644;&#26356;&#24555;&#30340;&#25512;&#29702;&#26102;&#38388;&#65292;&#36739;&#23567;&#30340;&#25928;&#26524;&#25439;&#22833;&#12290;TPRF&#23398;&#20064;&#22914;&#20309;&#26377;&#25928;&#22320;&#32467;&#21512;&#26469;&#33258;&#31264;&#23494;&#25991;&#20855;&#34920;&#31034;&#30340;&#30456;&#20851;&#21453;&#39304;&#20449;&#21495;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;TPRF&#25552;&#20379;&#20102;&#19968;&#31181;&#24314;&#27169;&#26597;&#35810;&#21644;&#30456;&#20851;&#21453;&#39304;&#20449;&#21495;&#20043;&#38388;&#20851;&#31995;&#21644;&#26435;&#37325;&#30340;&#26426;&#21046;&#12290;&#35813;&#26041;&#27861;&#23545;&#25152;&#20351;&#29992;&#30340;&#20855;&#20307;&#31264;&#23494;&#34920;&#31034;&#19981;&#21152;&#20559;&#35265;&#65292;&#22240;&#27492;&#21487;&#20197;&#24191;&#27867;&#24212;&#29992;&#20110;&#20219;&#20309;&#31264;&#23494;&#26816;&#32034;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper considers Pseudo-Relevance Feedback (PRF) methods for dense retrievers in a resource constrained environment such as that of cheap cloud instances or embedded systems (e.g., smartphones and smartwatches), where memory and CPU are limited and GPUs are not present. For this, we propose a transformer-based PRF method (TPRF), which has a much smaller memory footprint and faster inference time compared to other deep language models that employ PRF mechanisms, with a marginal effectiveness loss. TPRF learns how to effectively combine the relevance feedback signals from dense passage representations. Specifically, TPRF provides a mechanism for modelling relationships and weights between the query and the relevance feedback signals. The method is agnostic to the specific dense representation used and thus can be generally applied to any dense retriever.
&lt;/p&gt;</description></item><item><title>SciMMIR&#26159;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#31185;&#23398;&#39046;&#22495;&#30340;&#22810;&#27169;&#24577;&#20449;&#24687;&#26816;&#32034;&#22522;&#20934;&#65292;&#36890;&#36807;&#24320;&#25918;&#33719;&#21462;&#30340;&#35770;&#25991;&#38598;&#21512;&#25552;&#21462;&#19982;&#31185;&#23398;&#39046;&#22495;&#30456;&#20851;&#30340;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#65292;&#20174;&#32780;&#24357;&#34917;&#20102;&#29616;&#26377;&#22522;&#20934;&#22312;&#27492;&#39046;&#22495;&#20013;&#30340;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2401.13478</link><description>&lt;p&gt;
SciMMIR:&#31185;&#23398;&#22810;&#27169;&#24577;&#20449;&#24687;&#26816;&#32034;&#30340;&#22522;&#20934;&#35780;&#27979;
&lt;/p&gt;
&lt;p&gt;
SciMMIR: Benchmarking Scientific Multi-modal Information Retrieval. (arXiv:2401.13478v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13478
&lt;/p&gt;
&lt;p&gt;
SciMMIR&#26159;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#31185;&#23398;&#39046;&#22495;&#30340;&#22810;&#27169;&#24577;&#20449;&#24687;&#26816;&#32034;&#22522;&#20934;&#65292;&#36890;&#36807;&#24320;&#25918;&#33719;&#21462;&#30340;&#35770;&#25991;&#38598;&#21512;&#25552;&#21462;&#19982;&#31185;&#23398;&#39046;&#22495;&#30456;&#20851;&#30340;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#65292;&#20174;&#32780;&#24357;&#34917;&#20102;&#29616;&#26377;&#22522;&#20934;&#22312;&#27492;&#39046;&#22495;&#20013;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#20449;&#24687;&#26816;&#32034;&#65288;MMIR&#65289;&#26159;&#19968;&#20010;&#24555;&#36895;&#21457;&#23637;&#30340;&#39046;&#22495;&#65292;&#36890;&#36807;&#20808;&#36827;&#30340;&#34920;&#31034;&#23398;&#20064;&#21644;&#36328;&#27169;&#24577;&#23545;&#40784;&#30740;&#31350;&#65292;&#22312;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#22312;&#31185;&#23398;&#39046;&#22495;&#20869;&#35780;&#20272;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#30340;MMIR&#24615;&#33021;&#30340;&#24403;&#21069;&#22522;&#20934;&#23384;&#22312;&#26126;&#26174;&#24046;&#36317;&#65292;&#23398;&#26415;&#35821;&#35328;&#20013;&#25551;&#36848;&#30340;&#22270;&#34920;&#21644;&#34920;&#26684;&#22270;&#20687;&#36890;&#24120;&#19981;&#36215;&#37325;&#35201;&#20316;&#29992;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#21033;&#29992;&#24320;&#25918;&#33719;&#21462;&#30340;&#35770;&#25991;&#38598;&#21512;&#26500;&#24314;&#20102;&#19968;&#20010;&#19987;&#38376;&#30340;&#31185;&#23398;MMIR&#65288;SciMMIR&#65289;&#22522;&#20934;&#65292;&#20197;&#25552;&#21462;&#19982;&#31185;&#23398;&#39046;&#22495;&#30456;&#20851;&#30340;&#25968;&#25454;&#12290;&#35813;&#22522;&#20934;&#21253;&#21547;&#20102;530K&#20010;&#31934;&#24515;&#31574;&#21010;&#30340;&#20174;&#31185;&#23398;&#25991;&#26723;&#20013;&#25552;&#21462;&#30340;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#65292;&#36825;&#20123;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#26469;&#33258;&#20110;&#20855;&#26377;&#35814;&#32454;&#26631;&#39064;&#30340;&#31185;&#23398;&#25991;&#26723;&#20013;&#30340;&#22270;&#34920;&#21644;&#34920;&#26684;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#20004;&#32423;&#23376;&#38598;-&#23376;&#31867;&#21035;&#23618;&#27425;&#27880;&#37322;&#23545;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#36827;&#34892;&#20102;&#27880;&#37322;&#65292;&#20197;&#20419;&#36827;&#23545;&#22522;&#20934;&#27169;&#22411;&#30340;&#26356;&#20840;&#38754;&#35780;&#20272;&#12290;&#25105;&#20204;&#23545;&#38646;&#26679;&#26412;&#21644;&#24494;&#35843;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-modal information retrieval (MMIR) is a rapidly evolving field, where significant progress, particularly in image-text pairing, has been made through advanced representation learning and cross-modality alignment research. However, current benchmarks for evaluating MMIR performance in image-text pairing within the scientific domain show a notable gap, where chart and table images described in scholarly language usually do not play a significant role. To bridge this gap, we develop a specialised scientific MMIR (SciMMIR) benchmark by leveraging open-access paper collections to extract data relevant to the scientific domain. This benchmark comprises 530K meticulously curated image-text pairs, extracted from figures and tables with detailed captions in scientific documents. We further annotate the image-text pairs with two-level subset-subcategory hierarchy annotations to facilitate a more comprehensive evaluation of the baselines. We conducted zero-shot and fine-tuning evaluations o
&lt;/p&gt;</description></item><item><title>SpeechDPR&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#24320;&#25918;&#39046;&#22495;&#21475;&#35821;&#38382;&#31572;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#33021;&#22815;&#20174;&#21475;&#35821;&#23384;&#26723;&#20013;&#26816;&#32034;&#21487;&#33021;&#21253;&#21547;&#31572;&#26696;&#30340;&#27573;&#33853;&#12290;&#36890;&#36807;&#34701;&#21512;&#26080;&#30417;&#30563;ASR&#21644;&#25991;&#26412;&#23494;&#38598;&#26816;&#32034;&#22120;&#30340;&#30693;&#35782;&#65292;SpeechDPR&#33021;&#22815;&#33719;&#24471;&#36739;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;UASR&#24615;&#33021;&#36739;&#24046;&#26102;&#34920;&#29616;&#26356;&#21152;&#40065;&#26834;&#12290;</title><link>http://arxiv.org/abs/2401.13463</link><description>&lt;p&gt;
SpeechDPR: &#24320;&#25918;&#39046;&#22495;&#21475;&#35821;&#38382;&#31572;&#30340;&#31471;&#21040;&#31471;&#21475;&#35821;&#27573;&#33853;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
SpeechDPR: End-to-End Spoken Passage Retrieval for Open-Domain Spoken Question Answering. (arXiv:2401.13463v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13463
&lt;/p&gt;
&lt;p&gt;
SpeechDPR&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#24320;&#25918;&#39046;&#22495;&#21475;&#35821;&#38382;&#31572;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#33021;&#22815;&#20174;&#21475;&#35821;&#23384;&#26723;&#20013;&#26816;&#32034;&#21487;&#33021;&#21253;&#21547;&#31572;&#26696;&#30340;&#27573;&#33853;&#12290;&#36890;&#36807;&#34701;&#21512;&#26080;&#30417;&#30563;ASR&#21644;&#25991;&#26412;&#23494;&#38598;&#26816;&#32034;&#22120;&#30340;&#30693;&#35782;&#65292;SpeechDPR&#33021;&#22815;&#33719;&#24471;&#36739;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;UASR&#24615;&#33021;&#36739;&#24046;&#26102;&#34920;&#29616;&#26356;&#21152;&#40065;&#26834;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21475;&#35821;&#38382;&#31572;(SQA)&#26159;&#26426;&#22120;&#36890;&#36807;&#22312;&#32473;&#23450;&#21475;&#35821;&#27573;&#33853;&#20013;&#25214;&#21040;&#31572;&#26696;&#33539;&#22260;&#26469;&#22238;&#31572;&#29992;&#25143;&#38382;&#39064;&#30340;&#20851;&#38190;&#12290;&#36807;&#21435;&#30340;SQA&#26041;&#27861;&#27809;&#26377;&#20351;&#29992;ASR&#65292;&#20197;&#36991;&#20813;&#35782;&#21035;&#38169;&#35823;&#21644;&#35789;&#27719;&#22806;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#23454;&#38469;&#30340;&#24320;&#25918;&#39046;&#22495;SQA(openSQA)&#38382;&#39064;&#20013;&#65292;&#26426;&#22120;&#38656;&#35201;&#39318;&#20808;&#20174;&#21475;&#35821;&#23384;&#26723;&#20013;&#26816;&#32034;&#21487;&#33021;&#21253;&#21547;&#31572;&#26696;&#30340;&#27573;&#33853;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#24050;&#30693;&#30340;&#29992;&#20110;openSQA&#38382;&#39064;&#26816;&#32034;&#32452;&#20214;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;SpeechDPR&#12290;SpeechDPR&#36890;&#36807;&#20174;&#26080;&#30417;&#30563;ASR(UASR)&#21644;&#25991;&#26412;&#23494;&#38598;&#26816;&#32034;&#22120;(TDR)&#30340;&#32423;&#32852;&#27169;&#22411;&#20013;&#25552;&#28860;&#30693;&#35782;&#65292;&#23398;&#20064;&#21477;&#23376;&#32423;&#35821;&#20041;&#34920;&#31034;&#12290;&#19981;&#38656;&#35201;&#25163;&#21160;&#36716;&#24405;&#30340;&#35821;&#38899;&#25968;&#25454;&#12290;&#21021;&#27493;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#32423;&#32852;&#30340;UASR&#21644;TDR&#27169;&#22411;&#30456;&#27604;&#65292;&#24615;&#33021;&#30456;&#24403;&#65292;&#24182;&#19988;&#22312;UASR&#24615;&#33021;&#36739;&#24046;&#26102;&#26174;&#33879;&#25552;&#39640;&#65292;&#39564;&#35777;&#20102;&#36825;&#31181;&#26041;&#27861;&#26356;&#21152;&#40065;&#26834;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spoken Question Answering (SQA) is essential for machines to reply to user's question by finding the answer span within a given spoken passage. SQA has been previously achieved without ASR to avoid recognition errors and Out-of-Vocabulary (OOV) problems. However, the real-world problem of Open-domain SQA (openSQA), in which the machine needs to first retrieve passages that possibly contain the answer from a spoken archive in addition, was never considered. This paper proposes the first known end-to-end framework, Speech Dense Passage Retriever (SpeechDPR), for the retrieval component of the openSQA problem. SpeechDPR learns a sentence-level semantic representation by distilling knowledge from the cascading model of unsupervised ASR (UASR) and text dense retriever (TDR). No manually transcribed speech data is needed. Initial experiments showed performance comparable to the cascading model of UASR and TDR, and significantly better when UASR was poor, verifying this approach is more robus
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#36866;&#24212;&#24341;&#29992;&#25968;&#25454;&#30340;&#21435;&#20013;&#24515;&#21270;&#21327;&#20316;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35774;&#22791;&#19978;&#30340;&#20852;&#36259;&#28857;&#25512;&#33616;&#65292;&#20197;&#35299;&#20915;&#20351;&#29992;&#21516;&#19968;&#24341;&#29992;&#25968;&#25454;&#23545;&#19981;&#21516;&#29992;&#25143;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.13448</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#36866;&#24212;&#24341;&#29992;&#25968;&#25454;&#30340;&#21435;&#20013;&#24515;&#21270;&#21327;&#20316;&#23398;&#20064;&#36827;&#34892;&#35774;&#22791;&#19978;&#30340;&#20852;&#36259;&#28857;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Decentralized Collaborative Learning with Adaptive Reference Data for On-Device POI Recommendation. (arXiv:2401.13448v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13448
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#36866;&#24212;&#24341;&#29992;&#25968;&#25454;&#30340;&#21435;&#20013;&#24515;&#21270;&#21327;&#20316;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35774;&#22791;&#19978;&#30340;&#20852;&#36259;&#28857;&#25512;&#33616;&#65292;&#20197;&#35299;&#20915;&#20351;&#29992;&#21516;&#19968;&#24341;&#29992;&#25968;&#25454;&#23545;&#19981;&#21516;&#29992;&#25143;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22522;&#20110;&#20301;&#32622;&#30340;&#31038;&#20132;&#32593;&#32476;&#20013;&#65292;&#20852;&#36259;&#28857;&#65288;POI&#65289;&#25512;&#33616;&#24110;&#21161;&#29992;&#25143;&#21457;&#29616;&#26377;&#36259;&#30340;&#22320;&#26041;&#12290;&#20026;&#20102;&#20445;&#25252;&#38544;&#31169;&#21644;&#20943;&#23569;&#26381;&#21153;&#22120;&#20381;&#36182;&#65292;&#20174;&#22522;&#20110;&#20113;&#30340;&#27169;&#22411;&#36716;&#21521;&#35774;&#22791;&#19978;&#30340;&#25512;&#33616;&#26159;&#19968;&#20010;&#36235;&#21183;&#12290;&#30001;&#20110;&#20010;&#21035;&#35774;&#22791;&#19978;&#30340;&#26412;&#22320;&#29992;&#25143;-&#39033;&#30446;&#20132;&#20114;&#25968;&#25454;&#31232;&#32570;&#65292;&#20165;&#20381;&#36182;&#26412;&#22320;&#25968;&#25454;&#26159;&#19981;&#36275;&#22815;&#30340;&#12290;&#21327;&#20316;&#23398;&#20064;&#65288;CL&#65289;&#20852;&#36215;&#65292;&#20419;&#36827;&#29992;&#25143;&#20043;&#38388;&#30340;&#27169;&#22411;&#20849;&#20139;&#65292;&#20854;&#20013;&#24341;&#29992;&#25968;&#25454;&#20316;&#20026;&#20013;&#20171;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#22312;&#19981;&#30452;&#25509;&#20849;&#20139;&#31169;&#26377;&#25968;&#25454;&#25110;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#20132;&#25442;&#20182;&#20204;&#30340;&#36719;&#20915;&#31574;&#65292;&#30830;&#20445;&#38544;&#31169;&#24182;&#20174;&#21327;&#20316;&#20013;&#21463;&#30410;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#21327;&#20316;&#23398;&#20064;&#30340;&#25512;&#33616;&#36890;&#24120;&#20026;&#25152;&#26377;&#29992;&#25143;&#20351;&#29992;&#21516;&#19968;&#20010;&#24341;&#29992;&#25968;&#25454;&#12290;&#23545;&#19968;&#20010;&#29992;&#25143;&#26377;&#20215;&#20540;&#30340;&#24341;&#29992;&#25968;&#25454;&#21487;&#33021;&#23545;&#21478;&#19968;&#20010;&#29992;&#25143;&#26377;&#23475;&#65292;&#37492;&#20110;&#29992;&#25143;&#20559;&#22909;&#30340;&#22810;&#26679;&#24615;&#12290;&#29992;&#25143;&#21487;&#33021;&#19981;&#20250;&#23545;&#20182;&#20204;&#20852;&#36259;&#33539;&#22260;&#20043;&#22806;&#30340;&#39033;&#30446;&#25552;&#20379;&#26377;&#24847;&#20041;&#30340;&#36719;&#20915;&#31574;&#12290;&#22240;&#27492;&#65292;&#20026;&#25152;&#26377;&#21327;&#20316;&#20351;&#29992;&#30456;&#21516;&#30340;&#24341;&#29992;&#25968;&#25454;&#21487;&#33021;&#20250;&#38459;&#30861;&#21327;&#20316;&#25512;&#33616;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Location-based Social Networks, Point-of-Interest (POI) recommendation helps users discover interesting places. There is a trend to move from the cloud-based model to on-device recommendations for privacy protection and reduced server reliance. Due to the scarcity of local user-item interactions on individual devices, solely relying on local instances is not adequate. Collaborative Learning (CL) emerges to promote model sharing among users, where reference data is an intermediary that allows users to exchange their soft decisions without directly sharing their private data or parameters, ensuring privacy and benefiting from collaboration. However, existing CL-based recommendations typically use a single reference for all users. Reference data valuable for one user might be harmful to another, given diverse user preferences. Users may not offer meaningful soft decisions on items outside their interest scope. Consequently, using the same reference data for all collaborations can imped
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#26597;&#35810;&#25490;&#21517;&#20013;&#25991;&#26723;&#32452;&#30340;&#26597;&#35810;&#26333;&#20809;&#39044;&#27979;&#38382;&#39064;&#65292;&#25351;&#20986;&#20102;&#37325;&#26032;&#25490;&#21517;&#27969;&#31243;&#30340;&#25104;&#21151;&#19982;&#31532;&#19968;&#38454;&#27573;&#26816;&#32034;&#30340;&#24615;&#33021;&#23494;&#20999;&#30456;&#20851;&#65292;&#24182;&#35752;&#35770;&#20102;&#29305;&#23450;&#32452;&#25991;&#26723;&#22312;&#26368;&#32456;&#25490;&#21517;&#20013;&#25509;&#21463;&#26333;&#20809;&#37327;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2401.13434</link><description>&lt;p&gt;
&#26597;&#35810;&#25490;&#21517;&#20013;&#30340;&#25991;&#26723;&#32452;&#30340;&#26597;&#35810;&#26333;&#20809;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Query Exposure Prediction for Groups of Documents in Rankings. (arXiv:2401.13434v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13434
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#26597;&#35810;&#25490;&#21517;&#20013;&#25991;&#26723;&#32452;&#30340;&#26597;&#35810;&#26333;&#20809;&#39044;&#27979;&#38382;&#39064;&#65292;&#25351;&#20986;&#20102;&#37325;&#26032;&#25490;&#21517;&#27969;&#31243;&#30340;&#25104;&#21151;&#19982;&#31532;&#19968;&#38454;&#27573;&#26816;&#32034;&#30340;&#24615;&#33021;&#23494;&#20999;&#30456;&#20851;&#65292;&#24182;&#35752;&#35770;&#20102;&#29305;&#23450;&#32452;&#25991;&#26723;&#22312;&#26368;&#32456;&#25490;&#21517;&#20013;&#25509;&#21463;&#26333;&#20809;&#37327;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#21521;&#29992;&#25143;&#25552;&#20379;&#19982;&#29992;&#25143;&#26597;&#35810;&#26368;&#30456;&#20851;&#30340;&#25991;&#26723;&#12290;&#20026;&#20102;&#20570;&#21040;&#36825;&#19968;&#28857;&#65292;&#29616;&#20195;&#30340;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#36890;&#24120;&#20250;&#37096;&#32626;&#19968;&#20010;&#37325;&#26032;&#25490;&#21517;&#30340;&#27969;&#31243;&#65292;&#20854;&#20013;&#19968;&#32452;&#25991;&#26723;&#30001;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#31532;&#19968;&#38454;&#27573;&#26816;&#32034;&#36807;&#31243;&#26816;&#32034;&#20986;&#26469;&#65292;&#28982;&#21518;&#30001;&#19968;&#20010;&#26356;&#26377;&#25928;&#20294;&#20195;&#20215;&#26356;&#39640;&#30340;&#27169;&#22411;&#36827;&#34892;&#37325;&#26032;&#25490;&#21517;&#12290;&#28982;&#32780;&#65292;&#37325;&#26032;&#25490;&#21517;&#27969;&#31243;&#30340;&#25104;&#21151;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#31532;&#19968;&#38454;&#27573;&#26816;&#32034;&#30340;&#24615;&#33021;&#65292;&#22240;&#20026;&#22312;&#37325;&#26032;&#25490;&#21517;&#38454;&#27573;&#36890;&#24120;&#19981;&#20250;&#35782;&#21035;&#20986;&#26032;&#30340;&#25991;&#26723;&#12290;&#27492;&#22806;&#65292;&#36825;&#21487;&#33021;&#20250;&#24433;&#21709;&#29305;&#23450;&#32452;&#25991;&#26723;&#22312;&#26368;&#32456;&#25490;&#21517;&#20013;&#25509;&#21463;&#21040;&#30340;&#26333;&#20809;&#37327;&#12290;&#20363;&#22914;&#65292;&#22914;&#26524;&#31532;&#19968;&#38454;&#27573;&#26816;&#32034;&#36820;&#22238;&#26576;&#20123;&#32452;&#21035;&#30340;&#25991;&#26723;&#22826;&#23569;&#65292;&#20844;&#24179;&#20998;&#37197;&#26333;&#20809;&#21464;&#24471;&#26356;&#21152;&#20855;&#26377;&#25361;&#25112;&#24615;&#25110;&#19981;&#21487;&#33021;&#65292;&#22240;&#20026;&#22312;&#25490;&#21517;&#20013;&#30340;&#32452;&#25991;&#26723;&#25968;&#37327;&#23545;&#26333;&#20809;&#30340;&#24433;&#21709;&#27604;&#25991;&#26723;&#30340;&#20301;&#32622;&#26356;&#22823;&#12290;&#32771;&#34385;&#21040;&#36825;&#19968;&#28857;&#65292;
&lt;/p&gt;
&lt;p&gt;
The main objective of an Information Retrieval system is to provide a user with the most relevant documents to the user's query. To do this, modern IR systems typically deploy a re-ranking pipeline in which a set of documents is retrieved by a lightweight first-stage retrieval process and then re-ranked by a more effective but expensive model. However, the success of a re-ranking pipeline is heavily dependent on the performance of the first stage retrieval, since new documents are not usually identified during the re-ranking stage. Moreover, this can impact the amount of exposure that a particular group of documents, such as documents from a particular demographic group, can receive in the final ranking. For example, the fair allocation of exposure becomes more challenging or impossible if the first stage retrieval returns too few documents from certain groups, since the number of group documents in the ranking affects the exposure more than the documents' positions. With this in mind,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#32852;&#37030;&#22312;&#32447;&#23398;&#20064;&#25490;&#24207;&#20013;&#21024;&#38500;&#23458;&#25143;&#30340;&#36129;&#29486;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#21462;&#28040;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.13410</link><description>&lt;p&gt;
&#22914;&#20309;&#22312;&#32852;&#37030;&#22312;&#32447;&#23398;&#20064;&#25490;&#24207;&#20013;&#24536;&#35760;&#23458;&#25143;&#65311;
&lt;/p&gt;
&lt;p&gt;
How to Forget Clients in Federated Online Learning to Rank?. (arXiv:2401.13410v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13410
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#32852;&#37030;&#22312;&#32447;&#23398;&#20064;&#25490;&#24207;&#20013;&#21024;&#38500;&#23458;&#25143;&#30340;&#36129;&#29486;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#21462;&#28040;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#20445;&#25252;&#27861;&#35268;&#22914;&#27431;&#30431;&#30340;&#12298;&#36890;&#29992;&#25968;&#25454;&#20445;&#25252;&#26465;&#20363;&#12299;&#65288;GDPR&#65289;&#24314;&#31435;&#20102;&#8220;&#34987;&#36951;&#24536;&#30340;&#26435;&#21033;&#8221;&#65306;&#29992;&#25143;&#65288;&#23458;&#25143;&#65289;&#21487;&#20197;&#35201;&#27714;&#23558;&#20351;&#29992;&#20182;&#20204;&#30340;&#25968;&#25454;&#36827;&#34892;&#30340;&#36129;&#29486;&#20174;&#23398;&#20064;&#30340;&#27169;&#22411;&#20013;&#21024;&#38500;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21024;&#38500;&#21442;&#19982;&#32852;&#37030;&#22312;&#32447;&#23398;&#20064;&#25490;&#24207;&#65288;FOLTR&#65289;&#31995;&#32479;&#20013;&#23458;&#25143;&#25152;&#20570;&#30340;&#36129;&#29486;&#12290;&#22312;FOLTR&#31995;&#32479;&#20013;&#65292;&#36890;&#36807;&#32858;&#21512;&#23616;&#37096;&#26356;&#26032;&#21040;&#20840;&#23616;&#25490;&#24207;&#27169;&#22411;&#26469;&#23398;&#20064;&#25490;&#24207;&#22120;&#12290;&#23616;&#37096;&#26356;&#26032;&#26159;&#20197;&#22312;&#32447;&#26041;&#24335;&#22312;&#23458;&#25143;&#32423;&#21035;&#19978;&#20351;&#29992;&#26597;&#35810;&#21644;&#38544;&#24335;&#20132;&#20114;&#26469;&#23398;&#20064;&#30340;&#65292;&#36825;&#20123;&#26597;&#35810;&#21644;&#20132;&#20114;&#21457;&#29983;&#22312;&#29305;&#23450;&#23458;&#25143;&#20869;&#37096;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#27599;&#20010;&#23458;&#25143;&#30340;&#26412;&#22320;&#25968;&#25454;&#19981;&#20250;&#19982;&#20854;&#20182;&#23458;&#25143;&#25110;&#38598;&#20013;&#24335;&#25628;&#32034;&#26381;&#21153;&#20849;&#20139;&#65292;&#21516;&#26102;&#23458;&#25143;&#21487;&#20197;&#20174;&#32852;&#37030;&#20013;&#30340;&#27599;&#20010;&#23458;&#25143;&#30340;&#36129;&#29486;&#20013;&#21463;&#30410;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#21462;&#28040;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#21024;&#38500;&#23458;&#25143;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data protection legislation like the European Union's General Data Protection Regulation (GDPR) establishes the \textit{right to be forgotten}: a user (client) can request contributions made using their data to be removed from learned models. In this paper, we study how to remove the contributions made by a client participating in a Federated Online Learning to Rank (FOLTR) system. In a FOLTR system, a ranker is learned by aggregating local updates to the global ranking model. Local updates are learned in an online manner at a client-level using queries and implicit interactions that have occurred within that specific client. By doing so, each client's local data is not shared with other clients or with a centralised search service, while at the same time clients can benefit from an effective global ranking model learned from contributions of each client in the federation.  In this paper, we study an effective and efficient unlearning method that can remove a client's contribution with
&lt;/p&gt;</description></item><item><title>&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#39044;&#26816;&#32034;&#26597;&#35810;&#39044;&#27979;&#22120;&#26469;&#39044;&#27979;IR&#20010;&#24615;&#21270;&#24615;&#33021;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#31105;&#29992;&#20010;&#24615;&#21270;&#23545;&#20110;&#25552;&#39640;&#24615;&#33021;&#21644;&#29992;&#25143;&#28385;&#24847;&#24230;&#26159;&#26377;&#30410;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20123;&#26032;&#30340;&#39044;&#27979;&#22120;&#24182;&#20351;&#29992;&#20998;&#31867;&#21644;&#22238;&#24402;&#25216;&#26415;&#25913;&#36827;&#20102;&#32467;&#26524;&#65292;&#26368;&#32456;&#36798;&#21040;&#20102;&#30053;&#39640;&#20110;&#26368;&#29702;&#24819;&#24615;&#33021;&#30340;&#19977;&#20998;&#20043;&#19968;&#12290;</title><link>http://arxiv.org/abs/2401.13351</link><description>&lt;p&gt;
&#20351;&#29992;&#39044;&#26816;&#32034;&#26597;&#35810;&#39044;&#27979;&#22120;&#39044;&#27979;IR&#20010;&#24615;&#21270;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Predicting IR Personalization Performance using Pre-retrieval Query Predictors. (arXiv:2401.13351v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13351
&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#39044;&#26816;&#32034;&#26597;&#35810;&#39044;&#27979;&#22120;&#26469;&#39044;&#27979;IR&#20010;&#24615;&#21270;&#24615;&#33021;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#31105;&#29992;&#20010;&#24615;&#21270;&#23545;&#20110;&#25552;&#39640;&#24615;&#33021;&#21644;&#29992;&#25143;&#28385;&#24847;&#24230;&#26159;&#26377;&#30410;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20123;&#26032;&#30340;&#39044;&#27979;&#22120;&#24182;&#20351;&#29992;&#20998;&#31867;&#21644;&#22238;&#24402;&#25216;&#26415;&#25913;&#36827;&#20102;&#32467;&#26524;&#65292;&#26368;&#32456;&#36798;&#21040;&#20102;&#30053;&#39640;&#20110;&#26368;&#29702;&#24819;&#24615;&#33021;&#30340;&#19977;&#20998;&#20043;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#36890;&#24120;&#21487;&#20197;&#25552;&#39640;&#26597;&#35810;&#24615;&#33021;&#65292;&#20294;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#21487;&#33021;&#20250;&#25439;&#23475;&#24615;&#33021;&#12290;&#22914;&#26524;&#25105;&#20204;&#33021;&#22815;&#39044;&#27979;&#24182;&#22240;&#27492;&#22312;&#36825;&#20123;&#24773;&#20917;&#19979;&#31105;&#29992;&#20010;&#24615;&#21270;&#65292;&#25972;&#20307;&#24615;&#33021;&#23558;&#20250;&#26356;&#39640;&#65292;&#24182;&#19988;&#29992;&#25143;&#23545;&#20010;&#24615;&#21270;&#31995;&#32479;&#23558;&#26356;&#28385;&#24847;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20123;&#26368;&#20808;&#36827;&#30340;&#39044;&#26816;&#32034;&#26597;&#35810;&#24615;&#33021;&#39044;&#27979;&#22120;&#65292;&#24182;&#25552;&#20986;&#19968;&#20123;&#26032;&#30340;&#39044;&#27979;&#22120;&#65292;&#21253;&#25324;&#20808;&#21069;&#30446;&#30340;&#30340;&#29992;&#25143;&#37197;&#32622;&#25991;&#20214;&#20449;&#24687;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#20123;&#39044;&#27979;&#22120;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#20197;&#21450;&#20010;&#24615;&#21270;&#21644;&#21407;&#22987;&#26597;&#35810;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#20998;&#31867;&#21644;&#22238;&#24402;&#25216;&#26415;&#26469;&#25913;&#36827;&#32467;&#26524;&#65292;&#24182;&#26368;&#32456;&#36798;&#21040;&#30053;&#39640;&#20110;&#26368;&#29702;&#24819;&#24615;&#33021;&#30340;&#19977;&#20998;&#20043;&#19968;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#26159;&#35813;&#30740;&#31350;&#39046;&#22495;&#19968;&#20010;&#33391;&#22909;&#30340;&#36215;&#28857;&#65292;&#24403;&#28982;&#36824;&#38656;&#35201;&#26356;&#22810;&#30340;&#21162;&#21147;&#21644;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Personalization generally improves the performance of queries but in a few cases it may also harms it. If we are able to predict and therefore to disable personalization for those situations, the overall performance will be higher and users will be more satisfied with personalized systems. We use some state-of-the-art pre-retrieval query performance predictors and propose some others including the user profile information for the previous purpose. We study the correlations among these predictors and the difference between the personalized and the original queries. We also use classification and regression techniques to improve the results and finally reach a bit more than one third of the maximum ideal performance. We think this is a good starting point within this research line, which certainly needs more effort and improvements.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#26089;&#26399;&#35782;&#21035;&#21644;&#20998;&#31867;&#26263;&#32593;&#31449;&#30340;&#22823;&#25968;&#25454;&#26550;&#26500;&#12290;&#35813;&#26550;&#26500;&#21033;&#29992;&#24320;&#28304;&#25216;&#26415;&#23454;&#29616;&#20102;&#20174;&#22810;&#20010;&#26469;&#28304;&#21457;&#29616;&#27915;&#33905;&#22320;&#22336;&#12289;&#19979;&#36733;HTML&#24182;&#36827;&#34892;&#20869;&#23481;&#21435;&#37325;&#21644;&#20998;&#31867;&#30340;&#21151;&#33021;&#65292;&#24182;&#25104;&#21151;&#24212;&#23545;&#20102;Tor&#30340;&#19981;&#31283;&#23450;&#24615;&#25361;&#25112;&#12290;&#22312;93&#22825;&#20869;&#65292;&#31995;&#32479;&#35782;&#21035;&#20102;80,049&#20010;&#27915;&#33905;&#26381;&#21153;&#65292;&#24182;&#23545;90%&#36827;&#34892;&#20102;&#34920;&#24449;&#12290;</title><link>http://arxiv.org/abs/2401.13320</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;&#26089;&#26399;&#35782;&#21035;&#21644;&#20998;&#31867;&#26263;&#32593;&#31449;&#30340;&#22823;&#25968;&#25454;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
A Big Data Architecture for Early Identification and Categorization of Dark Web Sites. (arXiv:2401.13320v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13320
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#26089;&#26399;&#35782;&#21035;&#21644;&#20998;&#31867;&#26263;&#32593;&#31449;&#30340;&#22823;&#25968;&#25454;&#26550;&#26500;&#12290;&#35813;&#26550;&#26500;&#21033;&#29992;&#24320;&#28304;&#25216;&#26415;&#23454;&#29616;&#20102;&#20174;&#22810;&#20010;&#26469;&#28304;&#21457;&#29616;&#27915;&#33905;&#22320;&#22336;&#12289;&#19979;&#36733;HTML&#24182;&#36827;&#34892;&#20869;&#23481;&#21435;&#37325;&#21644;&#20998;&#31867;&#30340;&#21151;&#33021;&#65292;&#24182;&#25104;&#21151;&#24212;&#23545;&#20102;Tor&#30340;&#19981;&#31283;&#23450;&#24615;&#25361;&#25112;&#12290;&#22312;93&#22825;&#20869;&#65292;&#31995;&#32479;&#35782;&#21035;&#20102;80,049&#20010;&#27915;&#33905;&#26381;&#21153;&#65292;&#24182;&#23545;90%&#36827;&#34892;&#20102;&#34920;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26263;&#32593;&#22240;&#20854;&#19982;&#38750;&#27861;&#27963;&#21160;&#30340;&#20851;&#32852;&#32780;&#33261;&#21517;&#26157;&#33879;&#65292;&#38656;&#35201;&#31995;&#32479;&#33258;&#21160;&#30417;&#27979;&#36825;&#19968;&#39046;&#22495;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#21487;&#25193;&#23637;&#30340;&#26550;&#26500;&#65292;&#29992;&#20110;&#26089;&#26399;&#35782;&#21035;&#26032;&#30340;Tor&#32593;&#31449;&#21644;&#23545;&#20854;&#20869;&#23481;&#36827;&#34892;&#26085;&#24120;&#20998;&#26512;&#12290;&#35813;&#35299;&#20915;&#26041;&#26696;&#20351;&#29992;&#24320;&#28304;&#30340;&#22823;&#25968;&#25454;&#25216;&#26415;&#26632;&#65292;&#21253;&#25324;Kubernetes&#12289;Kafka&#12289;Kubeflow&#21644;MinIO&#65292;&#19981;&#26029;&#20174;&#19981;&#21516;&#26469;&#28304;&#65288;&#23041;&#32961;&#24773;&#25253;&#12289;&#20195;&#30721;&#20179;&#24211;&#12289;Web-Tor&#32593;&#20851;&#21644;Tor&#20179;&#24211;&#65289;&#21457;&#29616;&#27915;&#33905;&#22320;&#22336;&#65292;&#20174;Tor&#19979;&#36733;HTML&#65292;&#24182;&#20351;&#29992;MinHash LSH&#36827;&#34892;&#20869;&#23481;&#21435;&#37325;&#65292;&#36890;&#36807;BERTopic&#24314;&#27169;&#65288;SBERT&#23884;&#20837;&#12289;UMAP&#38477;&#32500;&#12289;HDBSCAN&#25991;&#26723;&#32858;&#31867;&#21644;c-TF-IDF&#20027;&#39064;&#20851;&#38190;&#35789;&#65289;&#36827;&#34892;&#20998;&#31867;&#12290;&#22312;93&#22825;&#20869;&#65292;&#35813;&#31995;&#32479;&#35782;&#21035;&#20102;80,049&#20010;&#27915;&#33905;&#26381;&#21153;&#65292;&#24182;&#23545;&#20854;&#20013;90%&#36827;&#34892;&#20102;&#34920;&#24449;&#65292;&#35299;&#20915;&#20102;Tor&#30340;&#19981;&#31283;&#23450;&#24615;&#25361;&#25112;&#12290;&#21457;&#29616;&#20102;&#22823;&#37327;&#37325;&#22797;&#20869;&#23481;&#65292;&#20165;&#21344;6.1%&#12290;
&lt;/p&gt;
&lt;p&gt;
The dark web has become notorious for its association with illicit activities and there is a growing need for systems to automate the monitoring of this space. This paper proposes an end-to-end scalable architecture for the early identification of new Tor sites and the daily analysis of their content. The solution is built using an Open Source Big Data stack for data serving with Kubernetes, Kafka, Kubeflow, and MinIO, continuously discovering onion addresses in different sources (threat intelligence, code repositories, web-Tor gateways, and Tor repositories), downloading the HTML from Tor and deduplicating the content using MinHash LSH, and categorizing with the BERTopic modeling (SBERT embedding, UMAP dimensionality reduction, HDBSCAN document clustering and c-TF-IDF topic keywords). In 93 days, the system identified 80,049 onion services and characterized 90% of them, addressing the challenge of Tor volatility. A disproportionate amount of repeated content is found, with only 6.1% u
&lt;/p&gt;</description></item><item><title>&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24341;&#20837;&#26102;&#38388;&#24615;&#26159;&#20449;&#24687;&#26816;&#32034;&#30340;&#20851;&#38190;&#25361;&#25112;&#65292;&#30446;&#21069;&#30340;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#26080;&#27861;&#24456;&#22909;&#22320;&#22788;&#29702;&#26102;&#38388;&#26597;&#35810;&#12290;</title><link>http://arxiv.org/abs/2401.13222</link><description>&lt;p&gt;
&#20851;&#20110;&#26102;&#38388;&#30340;&#37325;&#35201;&#24615;&#65306;&#22312;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#20013;&#24341;&#20837;&#26102;&#38388;&#24615;
&lt;/p&gt;
&lt;p&gt;
It's About Time: Incorporating Temporality in Retrieval Augmented Language Models. (arXiv:2401.13222v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13222
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24341;&#20837;&#26102;&#38388;&#24615;&#26159;&#20449;&#24687;&#26816;&#32034;&#30340;&#20851;&#38190;&#25361;&#25112;&#65292;&#30446;&#21069;&#30340;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#26080;&#27861;&#24456;&#22909;&#22320;&#22788;&#29702;&#26102;&#38388;&#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#20316;&#20026;&#20840;&#29699;&#30340;&#30693;&#35782;&#23384;&#20648;&#24211;&#65292;&#34987;&#25968;&#21313;&#20159;&#20154;&#29992;&#20110;&#25628;&#32034;&#20449;&#24687;&#12290;&#30830;&#20445;&#29992;&#25143;&#33021;&#22815;&#33719;&#24471;&#26368;&#30456;&#20851;&#21644;&#26368;&#26032;&#30340;&#20449;&#24687;&#26159;&#20449;&#24687;&#26816;&#32034;&#38754;&#20020;&#30340;&#20851;&#38190;&#25361;&#25112;&#65292;&#23588;&#20854;&#26159;&#22312;&#23384;&#22312;&#26469;&#33258;&#19981;&#21516;&#26102;&#38388;&#28857;&#30340;&#22810;&#20010;&#29256;&#26412;&#30340;&#32593;&#32476;&#20869;&#23481;&#30340;&#24773;&#20917;&#19979;&#12290;&#26368;&#36817;&#65292;&#36825;&#20010;&#25361;&#25112;&#21464;&#24471;&#26356;&#21152;&#22797;&#26434;&#65292;&#21407;&#22240;&#26159;&#23545;&#32500;&#22522;&#30334;&#31185;&#25110;&#32593;&#32476;&#20869;&#23481;&#36827;&#34892;&#35757;&#32451;&#30340;&#38382;&#31572;&#24037;&#20855;&#30340;&#22686;&#21152;&#20351;&#29992;&#65292;&#36825;&#20123;&#24037;&#20855;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#39537;&#21160;&#65292;&#32780;&#36825;&#20123;&#27169;&#22411;&#34987;&#21457;&#29616;&#20250;&#34394;&#26500;&#20449;&#24687;&#65292;&#19988;&#22312;&#22788;&#29702;&#26102;&#38388;&#20449;&#24687;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#21363;&#20351;&#26159;&#24341;&#20837;&#25991;&#26723;&#25968;&#25454;&#24211;&#20197;&#20943;&#23569;LLM&#34394;&#26500;&#30340;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#65288;RALM&#65289;&#20063;&#26080;&#27861;&#27491;&#30830;&#22788;&#29702;&#26102;&#38388;&#26597;&#35810;&#12290;&#36825;&#23548;&#33268;RALM&#22312;&#22238;&#31572;&#31867;&#20284;&#8220;&#35841;&#36194;&#24471;&#20102;&#28201;&#32593;&#20896;&#20891;&#65311;&#8221;&#30340;&#26597;&#35810;&#26102;&#65292;&#21482;&#20250;&#26816;&#32034;&#19982;&#28201;&#32593;&#30456;&#20851;&#30340;&#25991;&#26723;&#20869;&#23481;&#65292;&#32780;&#19981;&#23436;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
The web serves as a global repository of knowledge, used by billions of people to search for information. Ensuring that users receive the most relevant and up-to-date information, especially in the presence of multiple versions of web content from different time points remains a critical challenge for information retrieval. This challenge has recently been compounded by the increased use of question answering tools trained on Wikipedia or web content and powered by large language models (LLMs) \citep{chatgpt} which have been found to make up information (or hallucinate), and in addition have been shown to struggle with the temporal dimensions of information. Even Retriever Augmented Language Models (RALMs) which incorporate a document database to reduce LLM hallucination are unable to handle temporal queries correctly. This leads to instances where RALMs respond to queries such as "Who won the Wimbledon Championship?", by retrieving document passages related to Wimbledon but without th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#20174;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#20013;&#30417;&#27979;&#26538;&#25903;&#26292;&#21147;&#20107;&#20214;&#30340;&#21487;&#34892;&#24615;&#12290;&#30740;&#31350;&#22242;&#38431;&#20351;&#29992;&#32463;&#36807;&#24494;&#35843;&#30340;BERT&#27169;&#22411;&#35782;&#21035;&#24052;&#35199;&#30340;&#26538;&#25903;&#26292;&#21147;&#25253;&#21578;&#24182;&#21462;&#24471;&#20102;&#39640;&#20934;&#30830;&#24230;&#12290;&#30740;&#31350;&#32467;&#26524;&#26377;&#21161;&#20110;&#20154;&#26435;&#32452;&#32455;&#25910;&#38598;&#21253;&#21547;&#25152;&#38656;&#25968;&#25454;&#30340;&#20840;&#38754;&#25968;&#25454;&#24211;&#12290;</title><link>http://arxiv.org/abs/2401.12989</link><description>&lt;p&gt;
&#22312;&#20132;&#28779;&#20013;&#65306;&#35780;&#20272;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#20247;&#21253;&#26538;&#25903;&#26292;&#21147;&#25253;&#21578;&#30340;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
Into the crossfire: evaluating the use of a language model to crowdsource gun violence reports. (arXiv:2401.12989v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12989
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#20174;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#20013;&#30417;&#27979;&#26538;&#25903;&#26292;&#21147;&#20107;&#20214;&#30340;&#21487;&#34892;&#24615;&#12290;&#30740;&#31350;&#22242;&#38431;&#20351;&#29992;&#32463;&#36807;&#24494;&#35843;&#30340;BERT&#27169;&#22411;&#35782;&#21035;&#24052;&#35199;&#30340;&#26538;&#25903;&#26292;&#21147;&#25253;&#21578;&#24182;&#21462;&#24471;&#20102;&#39640;&#20934;&#30830;&#24230;&#12290;&#30740;&#31350;&#32467;&#26524;&#26377;&#21161;&#20110;&#20154;&#26435;&#32452;&#32455;&#25910;&#38598;&#21253;&#21547;&#25152;&#38656;&#25968;&#25454;&#30340;&#20840;&#38754;&#25968;&#25454;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26538;&#25903;&#26292;&#21147;&#26159;&#19968;&#20010;&#32039;&#36843;&#19988;&#19981;&#26029;&#22686;&#38271;&#30340;&#20154;&#26435;&#38382;&#39064;&#65292;&#24433;&#21709;&#30528;&#31038;&#20250;&#30340;&#26041;&#26041;&#38754;&#38754;&#65292;&#20174;&#21307;&#30103;&#20445;&#20581;&#21644;&#25945;&#32946;&#21040;&#24515;&#29702;&#23398;&#21644;&#32463;&#27982;&#23398;&#12290;&#21487;&#38752;&#30340;&#26538;&#25903;&#20107;&#20214;&#25968;&#25454;&#23545;&#20110;&#21046;&#23450;&#26356;&#26377;&#25928;&#30340;&#20844;&#20849;&#25919;&#31574;&#21644;&#24212;&#24613;&#21709;&#24212;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#20840;&#38754;&#30340;&#25968;&#25454;&#24211;&#21644;&#38754;&#23545;&#38754;&#35843;&#26597;&#30340;&#39118;&#38505;&#38459;&#27490;&#20102;&#20154;&#26435;&#32452;&#32455;&#22312;&#22823;&#22810;&#25968;&#22269;&#23478;&#25910;&#38598;&#25152;&#38656;&#30340;&#25968;&#25454;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#19982;&#19968;&#23478;&#24052;&#35199;&#20154;&#26435;&#32452;&#32455;&#21512;&#20316;&#65292;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#31995;&#32479;&#35780;&#20272;&#65292;&#20197;&#24110;&#21161;&#30417;&#27979;&#26469;&#33258;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#30340;&#29616;&#23454;&#19990;&#30028;&#26538;&#25903;&#20107;&#20214;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;Twitter&#19978;&#32463;&#36807;&#24494;&#35843;&#30340;&#22522;&#20110;BERT&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#21306;&#20998;&#26538;&#25903;&#26292;&#21147;&#25253;&#21578;&#21644;&#26222;&#36890;&#33889;&#33796;&#29273;&#35821;&#25991;&#26412;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#36798;&#21040;&#20102;&#39640;&#36798;0.97&#30340;AUC&#20998;&#25968;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#27169;&#22411;&#25972;&#21512;&#21040;&#19968;&#20010;Web&#24212;&#29992;&#31243;&#24207;&#20013;&#65292;&#24182;&#22312;&#23454;&#26102;&#24178;&#39044;&#20013;&#23545;&#20854;&#36827;&#34892;&#27979;&#35797;&#12290;&#25105;&#20204;&#30740;&#31350;&#24182;&#37319;&#35775;&#24052;&#35199;&#20998;&#26512;&#24072;&#65292;&#20182;&#20204;&#22312;&#25345;&#32493;&#36827;&#34892;&#31038;&#20132;&#23186;&#20307;&#20107;&#23454;&#26680;&#26597;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gun violence is a pressing and growing human rights issue that affects nearly every dimension of the social fabric, from healthcare and education to psychology and the economy. Reliable data on firearm events is paramount to developing more effective public policy and emergency responses. However, the lack of comprehensive databases and the risks of in-person surveys prevent human rights organizations from collecting needed data in most countries. Here, we partner with a Brazilian human rights organization to conduct a systematic evaluation of language models to assist with monitoring real-world firearm events from social media data. We propose a fine-tuned BERT-based model trained on Twitter (now X) texts to distinguish gun violence reports from ordinary Portuguese texts. Our model achieves a high AUC score of 0.97. We then incorporate our model into a web application and test it in a live intervention. We study and interview Brazilian analysts who continuously fact-check social media
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21307;&#23398;&#20195;&#30721;&#20013;&#24515;&#30340;&#22810;&#27169;&#24577;&#23545;&#27604;EHR&#24314;&#27169;&#39044;&#27979;&#19979;&#27425;&#23601;&#35786;&#35786;&#26029;&#65292;&#24182;&#36890;&#36807;&#20998;&#23618;&#27491;&#21017;&#21270;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.11648</link><description>&lt;p&gt;
&#36890;&#36807;&#20855;&#26377;&#20998;&#23618;&#27491;&#21017;&#21270;&#30340;&#21307;&#23398;&#20195;&#30721;&#20013;&#24515;&#30340;&#22810;&#27169;&#24577;&#23545;&#27604;EHR&#24314;&#27169;&#39044;&#27979;&#19979;&#27425;&#23601;&#35786;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
Next Visit Diagnosis Prediction via Medical Code-Centric Multimodal Contrastive EHR Modelling with Hierarchical Regularisation. (arXiv:2401.11648v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11648
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21307;&#23398;&#20195;&#30721;&#20013;&#24515;&#30340;&#22810;&#27169;&#24577;&#23545;&#27604;EHR&#24314;&#27169;&#39044;&#27979;&#19979;&#27425;&#23601;&#35786;&#35786;&#26029;&#65292;&#24182;&#36890;&#36807;&#20998;&#23618;&#27491;&#21017;&#21270;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#65292;&#21033;&#29992;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#39044;&#27979;&#19979;&#27425;&#23601;&#35786;&#30340;&#35786;&#26029;&#26159;&#19968;&#39033;&#24517;&#35201;&#30340;&#20219;&#21153;&#65292;&#23545;&#20110;&#21046;&#23450;&#21307;&#30103;&#20445;&#20581;&#25552;&#20379;&#32773;&#21644;&#24739;&#32773;&#30340;&#20027;&#21160;&#26410;&#26469;&#35745;&#21010;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20043;&#21069;&#30340;&#35768;&#22810;&#30740;&#31350;&#24182;&#27809;&#26377;&#20805;&#20998;&#35299;&#20915;EHR&#25968;&#25454;&#22266;&#26377;&#30340;&#24322;&#26500;&#21644;&#20998;&#23618;&#29305;&#24449;&#65292;&#24517;&#28982;&#23548;&#33268;&#27425;&#20248;&#30340;&#24615;&#33021;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;NECHO&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#21307;&#23398;&#20195;&#30721;&#20013;&#24515;&#30340;&#22810;&#27169;&#24577;&#23545;&#27604;EHR&#23398;&#20064;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#20998;&#23618;&#27491;&#21017;&#21270;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#23450;&#21046;&#30340;&#32593;&#32476;&#35774;&#35745;&#21644;&#19968;&#23545;&#21452;&#27169;&#24577;&#23545;&#27604;&#25439;&#22833;&#34701;&#21512;&#28085;&#30422;&#21307;&#23398;&#20195;&#30721;&#12289;&#20154;&#21475;&#32479;&#35745;&#25968;&#25454;&#21644;&#20020;&#24202;&#31508;&#35760;&#30340;&#22810;&#26041;&#38754;&#20449;&#24687;&#65292;&#25152;&#26377;&#36825;&#20123;&#37117;&#22260;&#32469;&#30528;&#21307;&#23398;&#20195;&#30721;&#34920;&#29616;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#21307;&#23398;&#26412;&#20307;&#20013;&#30340;&#29238;&#32423;&#20449;&#24687;&#26469;&#35268;&#33539;&#29305;&#23450;&#27169;&#24577;&#30340;&#32534;&#30721;&#22120;&#65292;&#20197;&#23398;&#20064;EHR&#25968;&#25454;&#30340;&#23618;&#27425;&#32467;&#26500;&#12290;&#23545;MIMIC-III&#25968;&#25454;&#36827;&#34892;&#30340;&#19968;&#31995;&#21015;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predicting next visit diagnosis using Electronic Health Records (EHR) is an essential task in healthcare, critical for devising proactive future plans for both healthcare providers and patients. Nonetheless, many preceding studies have not sufficiently addressed the heterogeneous and hierarchical characteristics inherent in EHR data, inevitably leading to sub-optimal performance. To this end, we propose NECHO, a novel medical code-centric multimodal contrastive EHR learning framework with hierarchical regularisation. First, we integrate multifaceted information encompassing medical codes, demographics, and clinical notes using a tailored network design and a pair of bimodal contrastive losses, all of which pivot around a medical code representation. We also regularise modality-specific encoders using a parental level information in medical ontology to learn hierarchical structure of EHR data. A series of experiments on MIMIC-III data demonstrates effectiveness of our approach.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#26816;&#27979;&#26032;&#20986;&#29616;&#30340;&#32534;&#30721;&#24694;&#24847;&#26415;&#35821;&#65292;&#20026;&#26497;&#31471;&#31038;&#20132;&#23186;&#20307;&#20013;&#30340;&#21453;&#29369;&#22826;&#24694;&#24847;&#35328;&#35770;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2401.10841</link><description>&lt;p&gt;
&#20351;&#29992;LLMs&#21457;&#29616;&#26497;&#31471;&#31038;&#20132;&#23186;&#20307;&#20013;&#30340;&#32534;&#30721;&#21453;&#29369;&#22826;&#24694;&#24847;&#35328;&#35770;&#30340;&#20986;&#29616;
&lt;/p&gt;
&lt;p&gt;
Using LLMs to discover emerging coded antisemitic hate-speech emergence in extremist social media. (arXiv:2401.10841v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10841
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#26816;&#27979;&#26032;&#20986;&#29616;&#30340;&#32534;&#30721;&#24694;&#24847;&#26415;&#35821;&#65292;&#20026;&#26497;&#31471;&#31038;&#20132;&#23186;&#20307;&#20013;&#30340;&#21453;&#29369;&#22826;&#24694;&#24847;&#35328;&#35770;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#20167;&#24680;&#35328;&#35770;&#30340;&#34067;&#24310;&#32473;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#24102;&#26469;&#20102;&#19968;&#20010;&#38590;&#39064;&#12290;&#19968;&#20010;&#29305;&#27530;&#30340;&#25361;&#25112;&#19982;&#20351;&#29992;&#32534;&#30721;&#35821;&#35328;&#30340;&#32676;&#20307;&#26377;&#20851;&#65292;&#36825;&#20123;&#32676;&#20307;&#26082;&#24819;&#20026;&#20854;&#29992;&#25143;&#21019;&#36896;&#24402;&#23646;&#24863;&#65292;&#21448;&#24819;&#22238;&#36991;&#26816;&#27979;&#12290;&#32534;&#30721;&#35821;&#35328;&#21457;&#23637;&#36805;&#36895;&#65292;&#24182;&#19988;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#20351;&#29992;&#26041;&#24335;&#19981;&#21516;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#27979;&#26032;&#20986;&#29616;&#30340;&#32534;&#30721;&#24694;&#24847;&#26415;&#35821;&#30340;&#26041;&#27861;&#35770;&#12290;&#35813;&#26041;&#27861;&#22312;&#22312;&#32447;&#21453;&#29369;&#22826;&#35328;&#35770;&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#35813;&#26041;&#27861;&#32771;&#34385;&#20102;&#20174;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#25235;&#21462;&#30340;&#24086;&#23376;&#65292;&#36890;&#24120;&#26159;&#26497;&#31471;&#20027;&#20041;&#29992;&#25143;&#20351;&#29992;&#30340;&#12290;&#24086;&#23376;&#26159;&#20351;&#29992;&#19982;&#20197;&#21069;&#24050;&#30693;&#30340;&#38024;&#23545;&#29369;&#22826;&#20154;&#30340;&#20167;&#24680;&#35328;&#35770;&#30456;&#20851;&#30340;&#31181;&#23376;&#34920;&#36798;&#24335;&#36827;&#34892;&#25235;&#21462;&#30340;&#12290;&#35813;&#26041;&#27861;&#39318;&#20808;&#36890;&#36807;&#35782;&#21035;&#27599;&#20010;&#24086;&#23376;&#26368;&#20855;&#20195;&#34920;&#24615;&#30340;&#34920;&#36798;&#24335;&#65292;&#24182;&#35745;&#31639;&#23427;&#20204;&#22312;&#25972;&#20010;&#35821;&#26009;&#24211;&#20013;&#30340;&#39057;&#29575;&#12290;&#36807;&#28388;&#25481;&#35821;&#27861;&#19981;&#19968;&#33268;&#30340;&#34920;&#36798;&#24335;&#21644;&#20043;&#21069;&#36935;&#21040;&#36807;&#30340;&#34920;&#36798;&#24335;&#65292;&#20197;&#20415;&#20851;&#27880;&#26032;&#20986;&#29616;&#30340;&#33391;&#22909;&#24418;&#24335;&#30340;&#26415;&#35821;&#12290;&#28982;&#21518;&#36827;&#34892;&#20102;&#35821;&#20041;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online hate speech proliferation has created a difficult problem for social media platforms. A particular challenge relates to the use of coded language by groups interested in both creating a sense of belonging for its users and evading detection. Coded language evolves quickly and its use varies over time. This paper proposes a methodology for detecting emerging coded hate-laden terminology. The methodology is tested in the context of online antisemitic discourse. The approach considers posts scraped from social media platforms, often used by extremist users. The posts are scraped using seed expressions related to previously known discourse of hatred towards Jews. The method begins by identifying the expressions most representative of each post and calculating their frequency in the whole corpus. It filters out grammatically incoherent expressions as well as previously encountered ones so as to focus on emergent well-formed terminology. This is followed by an assessment of semantic s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25512;&#33616;&#20013;&#24847;&#22270;&#23398;&#20064;&#30340;&#31471;&#21040;&#31471;&#21487;&#23398;&#20064;&#32858;&#31867;&#26041;&#27861;ELCRec&#65292;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#30340;&#22797;&#26434;&#20248;&#21270;&#38382;&#39064;&#21644;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#32858;&#31867;&#30340;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.05975</link><description>&lt;p&gt;
&#29992;&#20110;&#25512;&#33616;&#20013;&#24847;&#22270;&#23398;&#20064;&#30340;&#31471;&#21040;&#31471;&#21487;&#23398;&#20064;&#32858;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
End-to-end Learnable Clustering for Intent Learning in Recommendation. (arXiv:2401.05975v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05975
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25512;&#33616;&#20013;&#24847;&#22270;&#23398;&#20064;&#30340;&#31471;&#21040;&#31471;&#21487;&#23398;&#20064;&#32858;&#31867;&#26041;&#27861;ELCRec&#65292;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#30340;&#22797;&#26434;&#20248;&#21270;&#38382;&#39064;&#21644;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#32858;&#31867;&#30340;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25366;&#25496;&#29992;&#25143;&#30340;&#24847;&#22270;&#22312;&#24207;&#21015;&#25512;&#33616;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;ICLRec&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#21644;&#32858;&#31867;&#26469;&#25552;&#21462;&#29992;&#25143;&#30340;&#28508;&#22312;&#24847;&#22270;&#12290;&#23613;&#31649;&#23427;&#24050;&#32463;&#26174;&#31034;&#20986;&#26377;&#25928;&#24615;&#65292;&#20294;&#29616;&#26377;&#30340;&#26041;&#27861;&#23384;&#22312;&#22797;&#26434;&#21644;&#32321;&#29712;&#30340;&#20132;&#26367;&#20248;&#21270;&#38382;&#39064;&#65292;&#23548;&#33268;&#20004;&#20010;&#20027;&#35201;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#22312;&#24191;&#20041;&#26399;&#26395;&#26368;&#22823;&#21270;(EM)&#26694;&#26550;&#20013;&#20998;&#31163;&#34920;&#31034;&#23398;&#20064;&#21644;&#32858;&#31867;&#20248;&#21270;&#32463;&#24120;&#23548;&#33268;&#27425;&#20248;&#24615;&#33021;&#12290;&#20854;&#27425;&#65292;&#22312;&#25972;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#32858;&#31867;&#20250;&#24433;&#21709;&#22823;&#35268;&#27169;&#34892;&#19994;&#25968;&#25454;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24847;&#22270;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;ELCRec&#65292;&#23427;&#23558;&#34920;&#31034;&#23398;&#20064;&#38598;&#25104;&#21040;&#19968;&#20010;&#31471;&#21040;&#31471;&#21487;&#23398;&#20064;&#32858;&#31867;&#26694;&#26550;&#20013;&#36827;&#34892;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mining users' intents plays a crucial role in sequential recommendation. The recent approach, ICLRec, was introduced to extract underlying users' intents using contrastive learning and clustering. While it has shown effectiveness, the existing method suffers from complex and cumbersome alternating optimization, leading to two main issues. Firstly, the separation of representation learning and clustering optimization within a generalized expectation maximization (EM) framework often results in sub-optimal performance. Secondly, performing clustering on the entire dataset hampers scalability for large-scale industry data. To address these challenges, we propose a novel intent learning method called \underline{ELCRec}, which integrates representation learning into an \underline{E}nd-to-end \underline{L}earnable \underline{C}lustering framework for \underline{Rec}ommendation. Specifically, we encode users' behavior sequences and initialize the cluster centers as learnable network parameter
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;MARVEL&#30340;&#22810;&#27169;&#24577;&#26816;&#32034;&#27169;&#22411;&#65292;&#36890;&#36807;&#35270;&#35273;&#27169;&#22359;&#25554;&#20214;&#20026;&#23494;&#38598;&#26816;&#32034;&#22120;&#28155;&#21152;&#22270;&#20687;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#19988;&#22312;&#22810;&#27169;&#24577;&#26816;&#32034;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#20248;&#20110;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.14037</link><description>&lt;p&gt;
&#36890;&#36807;&#35270;&#35273;&#27169;&#22359;&#25554;&#20214;&#35299;&#38145;&#23494;&#38598;&#26816;&#32034;&#30340;&#22810;&#27169;&#24577;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Unlock Multi-Modal Capability of Dense Retrieval via Visual Module Plugin. (arXiv:2310.14037v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14037
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;MARVEL&#30340;&#22810;&#27169;&#24577;&#26816;&#32034;&#27169;&#22411;&#65292;&#36890;&#36807;&#35270;&#35273;&#27169;&#22359;&#25554;&#20214;&#20026;&#23494;&#38598;&#26816;&#32034;&#22120;&#28155;&#21152;&#22270;&#20687;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#19988;&#22312;&#22810;&#27169;&#24577;&#26816;&#32034;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#20248;&#20110;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#35270;&#35273;&#27169;&#22359;&#25554;&#20214;&#65288;MARVEL&#65289;&#23398;&#20064;&#26597;&#35810;&#21644;&#22810;&#27169;&#24577;&#25991;&#26723;&#30340;&#23884;&#20837;&#31354;&#38388;&#20197;&#36827;&#34892;&#26816;&#32034;&#30340;&#22810;&#27169;&#24577;&#26816;&#32034;&#27169;&#22411;&#12290;MARVEL&#20351;&#29992;&#32479;&#19968;&#30340;&#32534;&#30721;&#22120;&#27169;&#22411;&#23545;&#26597;&#35810;&#21644;&#22810;&#27169;&#24577;&#25991;&#26723;&#36827;&#34892;&#32534;&#30721;&#65292;&#26377;&#21161;&#20110;&#20943;&#23567;&#22270;&#20687;&#21644;&#25991;&#26412;&#20043;&#38388;&#30340;&#27169;&#24577;&#24046;&#36317;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#35270;&#35273;&#27169;&#22359;&#32534;&#30721;&#30340;&#22270;&#20687;&#29305;&#24449;&#20316;&#20026;&#20854;&#36755;&#20837;&#65292;&#20351;&#24471;&#32463;&#36807;&#35757;&#32451;&#30340;&#23494;&#38598;&#26816;&#32034;&#22120;T5-ANCE&#20855;&#26377;&#22270;&#20687;&#29702;&#35299;&#33021;&#21147;&#12290;&#20026;&#20102;&#20419;&#36827;&#22810;&#27169;&#24577;&#26816;&#32034;&#20219;&#21153;&#65292;&#25105;&#20204;&#22522;&#20110;ClueWeb22&#25968;&#25454;&#38598;&#26500;&#24314;&#20102;ClueWeb22-MM&#25968;&#25454;&#38598;&#65292;&#23558;&#38170;&#25991;&#26412;&#20316;&#20026;&#26597;&#35810;&#65292;&#24182;&#20174;&#38170;&#38142;&#25509;&#30340;&#32593;&#39029;&#20013;&#25552;&#21462;&#30456;&#20851;&#25991;&#26412;&#21644;&#22270;&#20687;&#25991;&#26723;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;MARVEL&#22312;&#22810;&#27169;&#24577;&#26816;&#32034;&#25968;&#25454;&#38598;WebQA&#21644;ClueWeb22-MM&#19978;&#26126;&#26174;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#35270;&#35273;&#27169;&#22359;&#25554;&#20214;&#26041;&#27861;&#20026;&#23454;&#29616;&#22270;&#20687;&#29702;&#35299;&#33021;&#21147;&#37327;&#36523;&#23450;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes Multi-modAl Retrieval model via Visual modulE pLugin (MARVEL) to learn an embedding space for queries and multi-modal documents to conduct retrieval. MARVEL encodes queries and multi-modal documents with a unified encoder model, which helps to alleviate the modality gap between images and texts. Specifically, we enable the image understanding ability of a well-trained dense retriever, T5-ANCE, by incorporating the image features encoded by the visual module as its inputs. To facilitate the multi-modal retrieval tasks, we build the ClueWeb22-MM dataset based on the ClueWeb22 dataset, which regards anchor texts as queries, and exact the related texts and image documents from anchor linked web pages. Our experiments show that MARVEL significantly outperforms the state-of-the-art methods on the multi-modal retrieval dataset WebQA and ClueWeb22-MM. Our further analyses show that the visual module plugin method is tailored to enable the image understanding ability for an 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32431;&#31929;&#30340;&#28040;&#24687;&#20256;&#36882;&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#20849;&#21516;&#37051;&#23621;&#36827;&#34892;&#38142;&#36335;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#36755;&#20837;&#21521;&#37327;&#30340;&#27491;&#20132;&#24615;&#26469;&#25429;&#25417;&#32852;&#21512;&#32467;&#26500;&#29305;&#24449;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38142;&#36335;&#39044;&#27979;&#27169;&#22411;MPLP&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#20934;&#27491;&#20132;&#21521;&#37327;&#20272;&#35745;&#38142;&#36335;&#32423;&#32467;&#26500;&#29305;&#24449;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#33410;&#28857;&#32423;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.00976</link><description>&lt;p&gt;
&#32431;&#31929;&#30340;&#28040;&#24687;&#20256;&#36882;&#21487;&#20197;&#20272;&#35745;&#20849;&#21516;&#37051;&#23621;&#36827;&#34892;&#38142;&#36335;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Pure Message Passing Can Estimate Common Neighbor for Link Prediction. (arXiv:2309.00976v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00976
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32431;&#31929;&#30340;&#28040;&#24687;&#20256;&#36882;&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#20849;&#21516;&#37051;&#23621;&#36827;&#34892;&#38142;&#36335;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#36755;&#20837;&#21521;&#37327;&#30340;&#27491;&#20132;&#24615;&#26469;&#25429;&#25417;&#32852;&#21512;&#32467;&#26500;&#29305;&#24449;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38142;&#36335;&#39044;&#27979;&#27169;&#22411;MPLP&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#20934;&#27491;&#20132;&#21521;&#37327;&#20272;&#35745;&#38142;&#36335;&#32423;&#32467;&#26500;&#29305;&#24449;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#33410;&#28857;&#32423;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65288;MPNN&#65289;&#24050;&#25104;&#20026;&#22270;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;&#20107;&#23454;&#26631;&#20934;&#12290;&#28982;&#32780;&#65292;&#22312;&#38142;&#36335;&#39044;&#27979;&#26041;&#38754;&#65292;&#23427;&#20204;&#24448;&#24448;&#34920;&#29616;&#19981;&#20339;&#65292;&#34987;&#31616;&#21333;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#22914;&#20849;&#21516;&#37051;&#23621;&#65288;CN&#65289;&#25152;&#36229;&#36234;&#12290;&#36825;&#31181;&#24046;&#24322;&#28304;&#20110;&#19968;&#20010;&#26681;&#26412;&#38480;&#21046;&#65306;&#23613;&#31649;MPNN&#22312;&#33410;&#28857;&#32423;&#34920;&#31034;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#32534;&#30721;&#38142;&#36335;&#39044;&#27979;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#32852;&#21512;&#32467;&#26500;&#29305;&#24449;&#65288;&#22914;CN&#65289;&#26041;&#38754;&#21017;&#36935;&#21040;&#22256;&#38590;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#35748;&#20026;&#36890;&#36807;&#21033;&#29992;&#36755;&#20837;&#21521;&#37327;&#30340;&#27491;&#20132;&#24615;&#65292;&#32431;&#31929;&#30340;&#28040;&#24687;&#20256;&#36882;&#30830;&#23454;&#21487;&#20197;&#25429;&#25417;&#21040;&#32852;&#21512;&#32467;&#26500;&#29305;&#24449;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;MPNN&#22312;&#36817;&#20284;CN&#21551;&#21457;&#24335;&#31639;&#27861;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#21457;&#29616;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#38142;&#36335;&#39044;&#27979;&#27169;&#22411;&#8212;&#8212;&#28040;&#24687;&#20256;&#36882;&#38142;&#36335;&#39044;&#27979;&#22120;&#65288;MPLP&#65289;&#12290;MPLP&#21033;&#29992;&#20934;&#27491;&#20132;&#21521;&#37327;&#20272;&#35745;&#38142;&#36335;&#32423;&#32467;&#26500;&#29305;&#24449;&#65292;&#21516;&#26102;&#20445;&#30041;&#33410;&#28857;&#32423;&#22797;&#26434;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#34920;&#26126;&#21033;&#29992;&#28040;&#24687;&#20256;&#36882;&#25429;&#25417;&#32467;&#26500;&#29305;&#24449;&#33021;&#22815;&#25913;&#21892;&#38142;&#36335;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Message Passing Neural Networks (MPNNs) have emerged as the {\em de facto} standard in graph representation learning. However, when it comes to link prediction, they often struggle, surpassed by simple heuristics such as Common Neighbor (CN). This discrepancy stems from a fundamental limitation: while MPNNs excel in node-level representation, they stumble with encoding the joint structural features essential to link prediction, like CN. To bridge this gap, we posit that, by harnessing the orthogonality of input vectors, pure message-passing can indeed capture joint structural features. Specifically, we study the proficiency of MPNNs in approximating CN heuristics. Based on our findings, we introduce the Message Passing Link Predictor (MPLP), a novel link prediction model. MPLP taps into quasi-orthogonal vectors to estimate link-level structural features, all while preserving the node-level complexities. Moreover, our approach demonstrates that leveraging message-passing to capture stru
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#38646;-shot&#25490;&#21517;&#33021;&#21147;&#65292;&#20294;&#22312;&#24863;&#30693;&#21382;&#21490;&#20114;&#21160;&#39034;&#24207;&#21644;&#21463;&#21040;&#20559;&#35265;&#24433;&#21709;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#29305;&#27530;&#35774;&#35745;&#30340;&#25552;&#31034;&#21644;&#24341;&#23548;&#31574;&#30053;&#26469;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.08845</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#38646;-shot&#25512;&#33616;&#31995;&#32479;&#25490;&#21517;&#32773;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are Zero-Shot Rankers for Recommender Systems. (arXiv:2305.08845v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08845
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#38646;-shot&#25490;&#21517;&#33021;&#21147;&#65292;&#20294;&#22312;&#24863;&#30693;&#21382;&#21490;&#20114;&#21160;&#39034;&#24207;&#21644;&#21463;&#21040;&#20559;&#35265;&#24433;&#21709;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#29305;&#27530;&#35774;&#35745;&#30340;&#25552;&#31034;&#21644;&#24341;&#23548;&#31574;&#30053;&#26469;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#20363;&#22914;GPT-4&#65289;&#23637;&#31034;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#36890;&#29992;&#20219;&#21153;&#35299;&#20915;&#33021;&#21147;&#65292;&#21253;&#25324;&#28508;&#21147;&#25509;&#36817;&#25512;&#33616;&#20219;&#21153;&#12290;&#22312;&#36825;&#19968;&#30740;&#31350;&#26041;&#21521;&#19978;&#65292;&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;&#20316;&#20026;&#25512;&#33616;&#31995;&#32479;&#25490;&#21517;&#27169;&#22411;&#30340;LLMs&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, large language models (LLMs) (e.g., GPT-4) have demonstrated impressive general-purpose task-solving abilities, including the potential to approach recommendation tasks. Along this line of research, this work aims to investigate the capacity of LLMs that act as the ranking model for recommender systems. We first formalize the recommendation problem as a conditional ranking task, considering sequential interaction histories as conditions and the items retrieved by other candidate generation models as candidates. To solve the ranking task by LLMs, we carefully design the prompting template and conduct extensive experiments on two widely-used datasets. We show that LLMs have promising zero-shot ranking abilities but (1) struggle to perceive the order of historical interactions, and (2) can be biased by popularity or item positions in the prompts. We demonstrate that these issues can be alleviated using specially designed prompting and bootstrapping strategies. Equipped with thes
&lt;/p&gt;</description></item><item><title>&#22312;&#23545;&#35805;&#24335;&#25628;&#32034;&#20013;&#65292;&#22914;&#20309;&#20934;&#30830;&#22320;&#24314;&#27169;&#29992;&#25143;&#28385;&#24847;&#24230;&#26159;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#29616;&#26377;&#26041;&#27861;&#35201;&#20040;&#24573;&#35270;&#29992;&#25143;&#20449;&#24687;&#38656;&#27714;&#65292;&#35201;&#20040;&#24573;&#35270;&#28151;&#21512;&#20027;&#21160;&#23646;&#24615;&#12290;&#20256;&#32479;&#30340;IR&#30740;&#31350;&#20351;&#29992;Cranfield&#33539;&#24335;&#21644;&#29992;&#25143;&#34892;&#20026;&#24314;&#27169;&#26469;&#20272;&#35745;&#29992;&#25143;&#28385;&#24847;&#24230;&#65292;&#20294;&#23454;&#36341;&#20013;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;</title><link>http://arxiv.org/abs/2204.02659</link><description>&lt;p&gt;
&#22312;&#24320;&#25918;&#39046;&#22495;&#30340;&#23545;&#35805;&#24335;&#25628;&#32034;&#20013;&#26356;&#22909;&#22320;&#29702;&#35299;&#29992;&#25143;&#28385;&#24847;&#24230;
&lt;/p&gt;
&lt;p&gt;
Towards Better Understanding of User Satisfaction in Open-Domain Conversational Search. (arXiv:2204.02659v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.02659
&lt;/p&gt;
&lt;p&gt;
&#22312;&#23545;&#35805;&#24335;&#25628;&#32034;&#20013;&#65292;&#22914;&#20309;&#20934;&#30830;&#22320;&#24314;&#27169;&#29992;&#25143;&#28385;&#24847;&#24230;&#26159;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#29616;&#26377;&#26041;&#27861;&#35201;&#20040;&#24573;&#35270;&#29992;&#25143;&#20449;&#24687;&#38656;&#27714;&#65292;&#35201;&#20040;&#24573;&#35270;&#28151;&#21512;&#20027;&#21160;&#23646;&#24615;&#12290;&#20256;&#32479;&#30340;IR&#30740;&#31350;&#20351;&#29992;Cranfield&#33539;&#24335;&#21644;&#29992;&#25143;&#34892;&#20026;&#24314;&#27169;&#26469;&#20272;&#35745;&#29992;&#25143;&#28385;&#24847;&#24230;&#65292;&#20294;&#23454;&#36341;&#20013;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23545;&#35805;&#24335;&#25628;&#32034;&#30340;&#26222;&#21450;&#65292;&#22914;&#20309;&#35780;&#20272;&#23545;&#35805;&#24335;&#25628;&#32034;&#31995;&#32479;&#30340;&#24615;&#33021;&#25104;&#20026;&#20449;&#24687;&#26816;&#32034;&#39046;&#22495;&#30340;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#29616;&#26377;&#23545;&#35805;&#24335;&#25628;&#32034;&#35780;&#20272;&#30340;&#24037;&#20316;&#20027;&#35201;&#21487;&#20998;&#20026;&#20004;&#31867;&#65306;&#22522;&#20110;&#35821;&#20041;&#30456;&#20284;&#24615;&#26500;&#24314;&#24230;&#37327;&#26631;&#20934;&#65288;&#22914;BLUE&#12289;METEOR&#21644;BERTScore&#65289;&#65292;&#25110;&#30452;&#25509;&#20351;&#29992;&#20256;&#32479;&#25628;&#32034;&#26041;&#27861;&#65288;&#22914;nDCG&#12289;RBP&#21644;nERR&#65289;&#35780;&#20272;&#31995;&#32479;&#30340;&#21709;&#24212;&#25490;&#21517;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#35201;&#20040;&#24573;&#35270;&#29992;&#25143;&#30340;&#20449;&#24687;&#38656;&#27714;&#65292;&#35201;&#20040;&#24573;&#35270;&#23545;&#35805;&#24335;&#25628;&#32034;&#30340;&#28151;&#21512;&#20027;&#21160;&#23646;&#24615;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#22914;&#20309;&#22312;&#23545;&#35805;&#24335;&#25628;&#32034;&#22330;&#26223;&#20013;&#20934;&#30830;&#22320;&#24314;&#27169;&#29992;&#25143;&#28385;&#24847;&#24230;&#12290;&#30001;&#20110;&#26126;&#30830;&#35201;&#27714;&#29992;&#25143;&#25552;&#20379;&#28385;&#24847;&#24230;&#21453;&#39304;&#24456;&#22256;&#38590;&#65292;&#20256;&#32479;&#30340;IR&#30740;&#31350;&#24448;&#24448;&#20381;&#36182;Cranfield&#33539;&#24335;&#65288;&#21363;&#31532;&#19977;&#26041;&#27880;&#37322;&#65289;&#21644;&#29992;&#25143;&#34892;&#20026;&#24314;&#27169;&#26469;&#20272;&#35745;&#25628;&#32034;&#20013;&#30340;&#29992;&#25143;&#28385;&#24847;&#24230;&#12290;&#28982;&#32780;&#65292;&#23454;&#29616;&#36825;&#31181;&#27169;&#22411;&#22312;&#23454;&#36341;&#20013;&#24456;&#26377;&#25361;&#25112;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the increasing popularity of conversational search, how to evaluate the performance of conversational search systems has become an important question in the IR community. Existing works on conversational search evaluation can mainly be categorized into two streams: (1) constructing metrics based on semantic similarity (e.g. BLUE, METEOR and BERTScore), or (2) directly evaluating the response ranking performance of the system using traditional search methods (e.g. nDCG, RBP and nERR). However, these methods either ignore the information need of the user or ignore the mixed-initiative property of conversational search. This raises the question of how to accurately model user satisfaction in conversational search scenarios. Since explicitly asking users to provide satisfaction feedback is difficult, traditional IR studies often rely on the Cranfield paradigm (i.e., third-party annotation) and user behavior modeling to estimate user satisfaction in search. However, the feasibility and
&lt;/p&gt;</description></item></channel></rss>