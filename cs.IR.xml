<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#26597;&#35810;&#20027;&#23548;&#29992;&#25143;&#20852;&#36259;&#32593;&#32476;&#65288;QIN&#65289;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#36807;&#28388;&#21644;&#37325;&#26032;&#26435;&#37325;&#29992;&#25143;&#34892;&#20026;&#23376;&#24207;&#21015;&#26469;&#20805;&#20998;&#21033;&#29992;&#29992;&#25143;&#30340;&#38271;&#26399;&#20852;&#36259;&#65292;&#20197;&#25552;&#39640;&#20010;&#24615;&#21270;&#25628;&#32034;&#25490;&#21517;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.06444</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#25628;&#32034;&#25490;&#21517;&#30340;&#26597;&#35810;&#20027;&#23548;&#29992;&#25143;&#20852;&#36259;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Query-dominant User Interest Network for Large-Scale Search Ranking. (arXiv:2310.06444v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06444
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#26597;&#35810;&#20027;&#23548;&#29992;&#25143;&#20852;&#36259;&#32593;&#32476;&#65288;QIN&#65289;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#36807;&#28388;&#21644;&#37325;&#26032;&#26435;&#37325;&#29992;&#25143;&#34892;&#20026;&#23376;&#24207;&#21015;&#26469;&#20805;&#20998;&#21033;&#29992;&#29992;&#25143;&#30340;&#38271;&#26399;&#20852;&#36259;&#65292;&#20197;&#25552;&#39640;&#20010;&#24615;&#21270;&#25628;&#32034;&#25490;&#21517;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21382;&#21490;&#34892;&#20026;&#22312;&#25512;&#33616;&#21644;&#20449;&#24687;&#26816;&#32034;&#31561;&#21508;&#31181;&#39044;&#27979;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#24040;&#22823;&#30340;&#25928;&#26524;&#21644;&#28508;&#21147;&#12290;&#25972;&#20307;&#21382;&#21490;&#34892;&#20026;&#22810;&#26679;&#20294;&#22122;&#22768;&#24456;&#22823;&#65292;&#32780;&#25628;&#32034;&#34892;&#20026;&#32463;&#24120;&#24456;&#31232;&#30095;&#12290;&#20010;&#24615;&#21270;&#25628;&#32034;&#25490;&#21517;&#20013;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#37319;&#29992;&#31232;&#30095;&#30340;&#25628;&#32034;&#34892;&#20026;&#26469;&#23398;&#20064;&#34920;&#31034;&#65292;&#20294;&#19981;&#20805;&#20998;&#21033;&#29992;&#20851;&#38190;&#30340;&#38271;&#26399;&#20852;&#36259;&#12290;&#20107;&#23454;&#19978;&#65292;&#23545;&#20110;&#21363;&#26102;&#25628;&#32034;&#26469;&#35828;&#65292;&#29992;&#25143;&#30340;&#38271;&#26399;&#20852;&#36259;&#26159;&#22810;&#26679;&#20294;&#22122;&#22768;&#24456;&#22823;&#30340;&#65292;&#22914;&#20309;&#20805;&#20998;&#21033;&#29992;&#23427;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#26597;&#35810;&#20027;&#23548;&#29992;&#25143;&#20852;&#36259;&#32593;&#32476;&#65288;QIN&#65289;&#30340;&#26032;&#27169;&#22411;&#65292;&#21253;&#25324;&#20004;&#20010;&#32423;&#32852;&#21333;&#20803;&#26469;&#36807;&#28388;&#21407;&#22987;&#29992;&#25143;&#34892;&#20026;&#24182;&#37325;&#26032;&#26435;&#37325;&#34892;&#20026;&#23376;&#24207;&#21015;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#30456;&#20851;&#25628;&#32034;&#21333;&#20803;&#65288;RSU&#65289;&#65292;&#20854;&#30446;&#26631;&#26159;&#39318;&#20808;&#25628;&#32034;&#19982;&#26597;&#35810;&#30456;&#20851;&#30340;&#23376;&#24207;&#21015;&#65292;&#28982;&#21518;&#25628;&#32034;&#19982;&#30446;&#26631;&#39033;&#30456;&#20851;&#30340;&#23376;&#23376;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
Historical behaviors have shown great effect and potential in various prediction tasks, including recommendation and information retrieval. The overall historical behaviors are various but noisy while search behaviors are always sparse. Most existing approaches in personalized search ranking adopt the sparse search behaviors to learn representation with bottleneck, which do not sufficiently exploit the crucial long-term interest. In fact, there is no doubt that user long-term interest is various but noisy for instant search, and how to exploit it well still remains an open problem.  To tackle this problem, in this work, we propose a novel model named Query-dominant user Interest Network (QIN), including two cascade units to filter the raw user behaviors and reweigh the behavior subsequences. Specifically, we propose a relevance search unit (RSU), which aims to search a subsequence relevant to the query first and then search the sub-subsequences relevant to the target item. These items 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#34892;&#25919;&#25968;&#25454;&#28165;&#21333;&#65292;&#26412;&#30740;&#31350;&#21019;&#24314;&#20102;&#19968;&#20010;&#36328;&#22269;&#21442;&#32771;&#20316;&#29289;&#31867;&#22411;&#30417;&#27979;&#25968;&#25454;&#24211;&#65292;&#35299;&#20915;&#20102;&#22823;&#35268;&#27169;&#33719;&#21462;&#21487;&#38752;&#39640;&#36136;&#37327;&#21442;&#32771;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.06393</link><description>&lt;p&gt;
&#21033;&#29992;&#34892;&#25919;&#25968;&#25454;&#28165;&#21333;&#21019;&#24314;&#21487;&#38752;&#30340;&#36328;&#22269;&#21442;&#32771;&#20316;&#29289;&#31867;&#22411;&#30417;&#27979;&#25968;&#25454;&#24211;
&lt;/p&gt;
&lt;p&gt;
Harnessing Administrative Data Inventories to Create a Reliable Transnational Reference Database for Crop Type Monitoring. (arXiv:2310.06393v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06393
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#34892;&#25919;&#25968;&#25454;&#28165;&#21333;&#65292;&#26412;&#30740;&#31350;&#21019;&#24314;&#20102;&#19968;&#20010;&#36328;&#22269;&#21442;&#32771;&#20316;&#29289;&#31867;&#22411;&#30417;&#27979;&#25968;&#25454;&#24211;&#65292;&#35299;&#20915;&#20102;&#22823;&#35268;&#27169;&#33719;&#21462;&#21487;&#38752;&#39640;&#36136;&#37327;&#21442;&#32771;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#39134;&#36895;&#21457;&#23637;&#21450;&#20854;&#22312;&#22320;&#29699;&#35266;&#27979;&#25361;&#25112;&#20013;&#30340;&#24212;&#29992;&#65292;&#35813;&#39046;&#22495;&#30340;&#24615;&#33021;&#24471;&#21040;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#25552;&#21319;&#12290;&#34429;&#28982;&#36825;&#20123;&#26041;&#27861;&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#20197;&#21069;&#21463;&#21040;&#20256;&#24863;&#22120;&#25968;&#25454;&#21644;&#35745;&#31639;&#36164;&#28304;&#30340;&#21487;&#29992;&#24615;&#21644;&#25968;&#37327;&#30340;&#38480;&#21046;&#65292;&#20294;&#32570;&#20047;&#36275;&#22815;&#30340;&#21442;&#32771;&#25968;&#25454;&#29616;&#22312;&#26500;&#25104;&#20102;&#26032;&#30340;&#29942;&#39048;&#12290;&#30001;&#20110;&#21019;&#24314;&#36825;&#31181;&#22320;&#38754;&#30495;&#23454;&#20449;&#24687;&#26159;&#19968;&#39033;&#26114;&#36149;&#19988;&#23481;&#26131;&#20986;&#38169;&#30340;&#20219;&#21153;&#65292;&#24517;&#39035;&#24819;&#20986;&#26032;&#30340;&#26041;&#27861;&#22312;&#22823;&#35268;&#27169;&#19978;&#33719;&#21462;&#21487;&#38752;&#39640;&#36136;&#37327;&#30340;&#21442;&#32771;&#25968;&#25454;&#12290;&#20316;&#20026;&#31034;&#20363;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#8220;E URO C ROPS&#8221;&#65292;&#36825;&#26159;&#19968;&#20010;&#20316;&#29289;&#31867;&#22411;&#20998;&#31867;&#30340;&#21442;&#32771;&#25968;&#25454;&#38598;&#65292;&#23427;&#32858;&#21512;&#24182;&#21327;&#35843;&#20102;&#19981;&#21516;&#22269;&#23478;&#35843;&#26597;&#30340;&#34892;&#25919;&#25968;&#25454;&#65292;&#30446;&#30340;&#26159;&#23454;&#29616;&#36328;&#22269;&#20114;&#25805;&#20316;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
With leaps in machine learning techniques and their applicationon Earth observation challenges has unlocked unprecedented performance across the domain. While the further development of these methods was previously limited by the availability and volume of sensor data and computing resources, the lack of adequate reference data is now constituting new bottlenecks. Since creating such ground-truth information is an expensive and error-prone task, new ways must be devised to source reliable, high-quality reference data on large scales. As an example, we showcase E URO C ROPS, a reference dataset for crop type classification that aggregates and harmonizes administrative data surveyed in different countries with the goal of transnational interoperability.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21363;&#25554;&#21363;&#29992;&#30340;&#20010;&#20154;&#35282;&#33394;&#25552;&#31034;&#26041;&#27861;&#65292;&#29992;&#20110;&#20010;&#24615;&#21270;&#22238;&#31572;&#36873;&#25321;&#30340;&#26816;&#32034;&#23545;&#35805;&#26426;&#22120;&#20154;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#19979;&#34920;&#29616;&#33391;&#22909;&#65292;&#20943;&#23569;&#20102;&#23545;&#22522;&#20110;&#20010;&#20154;&#35282;&#33394;&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#20381;&#36182;&#65292;&#24182;&#19988;&#20351;&#24471;&#31995;&#32479;&#26356;&#23481;&#26131;&#25193;&#23637;&#21040;&#20854;&#20182;&#35821;&#35328;&#12290;</title><link>http://arxiv.org/abs/2310.06390</link><description>&lt;p&gt;
P5: &#29992;&#20110;&#20010;&#24615;&#21270;&#22238;&#31572;&#36873;&#25321;&#30340;&#21363;&#25554;&#21363;&#29992;&#20010;&#20154;&#35282;&#33394;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
P5: Plug-and-Play Persona Prompting for Personalized Response Selection. (arXiv:2310.06390v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06390
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21363;&#25554;&#21363;&#29992;&#30340;&#20010;&#20154;&#35282;&#33394;&#25552;&#31034;&#26041;&#27861;&#65292;&#29992;&#20110;&#20010;&#24615;&#21270;&#22238;&#31572;&#36873;&#25321;&#30340;&#26816;&#32034;&#23545;&#35805;&#26426;&#22120;&#20154;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#19979;&#34920;&#29616;&#33391;&#22909;&#65292;&#20943;&#23569;&#20102;&#23545;&#22522;&#20110;&#20010;&#20154;&#35282;&#33394;&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#20381;&#36182;&#65292;&#24182;&#19988;&#20351;&#24471;&#31995;&#32479;&#26356;&#23481;&#26131;&#25193;&#23637;&#21040;&#20854;&#20182;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#20010;&#20154;&#35282;&#33394;&#30340;&#26816;&#32034;&#23545;&#35805;&#26426;&#22120;&#20154;&#23545;&#20110;&#20010;&#24615;&#21270;&#23545;&#35805;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#20063;&#38754;&#20020;&#19968;&#20123;&#25361;&#25112;&#12290;1&#65289;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#25910;&#38598;&#22522;&#20110;&#20010;&#20154;&#35282;&#33394;&#30340;&#35821;&#26009;&#24211;&#38750;&#24120;&#26114;&#36149;&#12290;2&#65289;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#23545;&#35805;&#26426;&#22120;&#20154;&#31995;&#32479;&#24182;&#19981;&#24635;&#26159;&#26681;&#25454;&#20010;&#20154;&#35282;&#33394;&#20570;&#20986;&#22238;&#24212;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21363;&#25554;&#21363;&#29992;&#30340;&#20010;&#20154;&#35282;&#33394;&#25552;&#31034;&#26041;&#27861;&#12290;&#22914;&#26524;&#20010;&#20154;&#35282;&#33394;&#20449;&#24687;&#19981;&#21487;&#29992;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#21487;&#20197;&#20316;&#20026;&#26631;&#20934;&#30340;&#24320;&#25918;&#22495;&#23545;&#35805;&#26426;&#22120;&#20154;&#36816;&#34892;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#19979;&#34920;&#29616;&#33391;&#22909;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#23545;&#22522;&#20110;&#20010;&#20154;&#35282;&#33394;&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#20381;&#36182;&#12290;&#36825;&#20351;&#24471;&#22312;&#26080;&#38656;&#26500;&#24314;&#22522;&#20110;&#20010;&#20154;&#35282;&#33394;&#30340;&#35821;&#26009;&#24211;&#30340;&#24773;&#20917;&#19979;&#65292;&#26356;&#23481;&#26131;&#23558;&#35813;&#31995;&#32479;&#25193;&#23637;&#21040;&#20854;&#20182;&#35821;&#35328;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#36827;&#34892;&#24494;&#35843;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#38646;&#26679;&#26412;&#27169;&#22411;&#22312;&#21407;&#22987;&#20010;&#20154;&#35282;&#33394;&#21644;&#20462;&#35746;&#20010;&#20154;&#35282;&#33394;&#26041;&#38754;&#20998;&#21035;&#25552;&#39640;&#20102;7.71&#21644;1.04&#20010;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
The use of persona-grounded retrieval-based chatbots is crucial for personalized conversations, but there are several challenges that need to be addressed. 1) In general, collecting persona-grounded corpus is very expensive. 2) The chatbot system does not always respond in consideration of persona at real applications. To address these challenges, we propose a plug-and-play persona prompting method. Our system can function as a standard open-domain chatbot if persona information is not available. We demonstrate that this approach performs well in the zero-shot setting, which reduces the dependence on persona-ground training data. This makes it easier to expand the system to other languages without the need to build a persona-grounded corpus. Additionally, our model can be fine-tuned for even better performance. In our experiments, the zero-shot model improved the standard model by 7.71 and 1.04 points in the original persona and revised persona, respectively. The fine-tuned model impro
&lt;/p&gt;</description></item><item><title>MuseChat&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#23545;&#35805;&#24335;&#38899;&#20048;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#27169;&#25311;&#29992;&#25143;&#21644;&#25512;&#33616;&#31995;&#32479;&#20043;&#38388;&#30340;&#23545;&#35805;&#20132;&#20114;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#38899;&#20048;&#26631;&#31614;&#21644;&#33402;&#26415;&#23478;&#20449;&#24687;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#23450;&#21046;&#30340;&#38899;&#20048;&#25512;&#33616;&#65292;&#20351;&#29992;&#25143;&#21487;&#20197;&#20010;&#24615;&#21270;&#36873;&#25321;&#20182;&#20204;&#21916;&#27426;&#30340;&#38899;&#20048;&#12290;</title><link>http://arxiv.org/abs/2310.06282</link><description>&lt;p&gt;
MuseChat:&#19968;&#31181;&#35270;&#39057;&#23545;&#35805;&#38899;&#20048;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
MuseChat: A Conversational Music Recommendation System for Videos. (arXiv:2310.06282v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06282
&lt;/p&gt;
&lt;p&gt;
MuseChat&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#23545;&#35805;&#24335;&#38899;&#20048;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#27169;&#25311;&#29992;&#25143;&#21644;&#25512;&#33616;&#31995;&#32479;&#20043;&#38388;&#30340;&#23545;&#35805;&#20132;&#20114;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#38899;&#20048;&#26631;&#31614;&#21644;&#33402;&#26415;&#23478;&#20449;&#24687;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#23450;&#21046;&#30340;&#38899;&#20048;&#25512;&#33616;&#65292;&#20351;&#29992;&#25143;&#21487;&#20197;&#20010;&#24615;&#21270;&#36873;&#25321;&#20182;&#20204;&#21916;&#27426;&#30340;&#38899;&#20048;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;MuseChat&#65292;&#19968;&#31181;&#21019;&#26032;&#30340;&#22522;&#20110;&#23545;&#35805;&#30340;&#38899;&#20048;&#25512;&#33616;&#31995;&#32479;&#12290;&#36825;&#20010;&#29420;&#29305;&#30340;&#24179;&#21488;&#19981;&#20165;&#25552;&#20379;&#20114;&#21160;&#29992;&#25143;&#21442;&#19982;&#65292;&#36824;&#20026;&#36755;&#20837;&#30340;&#35270;&#39057;&#25552;&#20379;&#20102;&#23450;&#21046;&#30340;&#38899;&#20048;&#25512;&#33616;&#65292;&#20351;&#29992;&#25143;&#21487;&#20197;&#25913;&#36827;&#21644;&#20010;&#24615;&#21270;&#20182;&#20204;&#30340;&#38899;&#20048;&#36873;&#25321;&#12290;&#19982;&#20043;&#30456;&#21453;&#65292;&#20197;&#21069;&#30340;&#31995;&#32479;&#20027;&#35201;&#24378;&#35843;&#20869;&#23481;&#30340;&#20860;&#23481;&#24615;&#65292;&#24448;&#24448;&#24573;&#35270;&#20102;&#29992;&#25143;&#20010;&#20307;&#20559;&#22909;&#30340;&#32454;&#24494;&#24046;&#21035;&#12290;&#20363;&#22914;&#65292;&#25152;&#26377;&#30340;&#25968;&#25454;&#38598;&#37117;&#21482;&#25552;&#20379;&#22522;&#26412;&#30340;&#38899;&#20048;-&#35270;&#39057;&#37197;&#23545;&#65292;&#25110;&#32773;&#24102;&#26377;&#38899;&#20048;&#25551;&#36848;&#30340;&#37197;&#23545;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#19977;&#20010;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#23545;&#35805;&#21512;&#25104;&#26041;&#27861;&#65292;&#27169;&#25311;&#20102;&#29992;&#25143;&#21644;&#25512;&#33616;&#31995;&#32479;&#20043;&#38388;&#30340;&#20004;&#36718;&#20132;&#20114;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#38899;&#20048;&#26631;&#31614;&#21644;&#33402;&#26415;&#23478;&#20449;&#24687;&#12290;&#22312;&#36825;&#20010;&#20132;&#20114;&#20013;&#65292;&#29992;&#25143;&#25552;&#20132;&#19968;&#20010;&#35270;&#39057;&#32473;&#31995;&#32479;&#65292;&#31995;&#32479;&#20250;&#25552;&#20379;&#19968;&#20010;&#21512;&#36866;&#30340;&#38899;&#20048;&#29255;&#27573;&#65292;&#24182;&#38468;&#24102;&#35299;&#37322;&#12290;&#20043;&#21518;&#65292;&#29992;&#25143;&#20250;&#34920;&#36798;&#20182;&#20204;&#23545;&#38899;&#20048;&#30340;&#20559;&#22909;&#65292;&#31995;&#32479;&#20250;&#21576;&#29616;&#19968;&#20010;&#25913;&#36827;&#21518;&#30340;&#38899;&#20048;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
We introduce MuseChat, an innovative dialog-based music recommendation system. This unique platform not only offers interactive user engagement but also suggests music tailored for input videos, so that users can refine and personalize their music selections. In contrast, previous systems predominantly emphasized content compatibility, often overlooking the nuances of users' individual preferences. For example, all the datasets only provide basic music-video pairings or such pairings with textual music descriptions. To address this gap, our research offers three contributions. First, we devise a conversation-synthesis method that simulates a two-turn interaction between a user and a recommendation system, which leverages pre-trained music tags and artist information. In this interaction, users submit a video to the system, which then suggests a suitable music piece with a rationale. Afterwards, users communicate their musical preferences, and the system presents a refined music recomme
&lt;/p&gt;</description></item><item><title>DORIS-MAE&#26159;&#19968;&#20010;&#29992;&#20110;&#31185;&#23398;&#25991;&#26723;&#26816;&#32034;&#30340;&#26032;&#20219;&#21153;&#65292;&#26088;&#22312;&#22788;&#29702;&#22797;&#26434;&#30340;&#22810;&#26041;&#38754;&#26597;&#35810;&#12290;&#30740;&#31350;&#22242;&#38431;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39564;&#35777;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2310.04678</link><description>&lt;p&gt;
DORIS-MAE: &#20351;&#29992;&#22810;&#23618;&#32423;&#22522;&#20110;&#26041;&#38754;&#30340;&#26597;&#35810;&#36827;&#34892;&#31185;&#23398;&#25991;&#26723;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
DORIS-MAE: Scientific Document Retrieval using Multi-level Aspect-based Queries. (arXiv:2310.04678v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04678
&lt;/p&gt;
&lt;p&gt;
DORIS-MAE&#26159;&#19968;&#20010;&#29992;&#20110;&#31185;&#23398;&#25991;&#26723;&#26816;&#32034;&#30340;&#26032;&#20219;&#21153;&#65292;&#26088;&#22312;&#22788;&#29702;&#22797;&#26434;&#30340;&#22810;&#26041;&#38754;&#26597;&#35810;&#12290;&#30740;&#31350;&#22242;&#38431;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39564;&#35777;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31185;&#23398;&#30740;&#31350;&#20013;&#65292;&#26681;&#25454;&#22797;&#26434;&#30340;&#22810;&#26041;&#38754;&#26597;&#35810;&#26377;&#25928;&#26816;&#32034;&#30456;&#20851;&#25991;&#26723;&#30340;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#21463;&#38480;&#20110;&#27880;&#37322;&#22797;&#26434;&#26597;&#35810;&#25152;&#38656;&#30340;&#39640;&#25104;&#26412;&#21644;&#21162;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20219;&#21153;&#65292;&#31185;&#23398;&#25991;&#26723;&#26816;&#32034;&#20013;&#20351;&#29992;&#22810;&#23618;&#32423;&#22522;&#20110;&#26041;&#38754;&#30340;&#26597;&#35810;(DORIS-MAE)&#65292;&#26088;&#22312;&#22788;&#29702;&#31185;&#23398;&#30740;&#31350;&#20013;&#29992;&#25143;&#26597;&#35810;&#30340;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#22312;&#35745;&#31639;&#26426;&#31185;&#23398;&#39046;&#22495;&#20869;&#24314;&#31435;&#20102;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;100&#20010;&#20154;&#24037;&#32534;&#20889;&#30340;&#22797;&#26434;&#26597;&#35810;&#26696;&#20363;&#12290;&#23545;&#20110;&#27599;&#20010;&#22797;&#26434;&#26597;&#35810;&#65292;&#25105;&#20204;&#32452;&#32455;&#20102;100&#20010;&#30456;&#20851;&#25991;&#26723;&#38598;&#21512;&#65292;&#24182;&#20026;&#20854;&#25490;&#21517;&#20135;&#29983;&#20102;&#27880;&#37322;&#30340;&#30456;&#20851;&#24615;&#20998;&#25968;&#12290;&#37492;&#20110;&#19987;&#23478;&#27880;&#37322;&#30340;&#24040;&#22823;&#24037;&#20316;&#37327;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;Anno-GPT&#65292;&#36825;&#26159;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#39564;&#35777;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#19987;&#23478;&#32423;&#25968;&#25454;&#38598;&#27880;&#37322;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In scientific research, the ability to effectively retrieve relevant documents based on complex, multifaceted queries is critical. Existing evaluation datasets for this task are limited, primarily due to the high cost and effort required to annotate resources that effectively represent complex queries. To address this, we propose a novel task, Scientific DOcument Retrieval using Multi-level Aspect-based quEries (DORIS-MAE), which is designed to handle the complex nature of user queries in scientific research. We developed a benchmark dataset within the field of computer science, consisting of 100 human-authored complex query cases. For each complex query, we assembled a collection of 100 relevant documents and produced annotated relevance scores for ranking them. Recognizing the significant labor of expert annotation, we also introduce Anno-GPT, a scalable framework for validating the performance of Large Language Models (LLMs) on expert-level dataset annotation tasks. LLM annotation o
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#22686;&#24378;&#22806;&#37096;&#27880;&#24847;&#21147;&#30340;&#22270;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#28040;&#38500;&#36328;&#39046;&#22495;&#23494;&#24230;&#20559;&#24046;&#24182;&#31283;&#23450;&#22320;&#25429;&#25417;&#29992;&#25143;&#30340;&#34892;&#20026;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2310.04633</link><description>&lt;p&gt;
&#26080;&#20559;&#21644;&#40065;&#26834;&#24615;&#65306;&#22686;&#24378;&#22806;&#37096;&#27880;&#24847;&#21147;&#30340;&#36328;&#39046;&#22495;&#24207;&#21015;&#25512;&#33616;&#20013;&#30340;&#22270;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Unbiased and Robust: External Attention-enhanced Graph Contrastive Learning for Cross-domain Sequential Recommendation. (arXiv:2310.04633v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04633
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#22686;&#24378;&#22806;&#37096;&#27880;&#24847;&#21147;&#30340;&#22270;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#28040;&#38500;&#36328;&#39046;&#22495;&#23494;&#24230;&#20559;&#24046;&#24182;&#31283;&#23450;&#22320;&#25429;&#25417;&#29992;&#25143;&#30340;&#34892;&#20026;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#39046;&#22495;&#24207;&#21015;&#25512;&#33616;&#22120;&#65288;CSRs&#65289;&#22240;&#33021;&#22815;&#21033;&#29992;&#22810;&#20010;&#39046;&#22495;&#30340;&#36741;&#21161;&#20449;&#24687;&#25429;&#25417;&#29992;&#25143;&#30340;&#24207;&#21015;&#20559;&#22909;&#32780;&#24341;&#36215;&#20102;&#30456;&#24403;&#22823;&#30340;&#30740;&#31350;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30740;&#31350;&#36890;&#24120;&#36981;&#24490;&#19968;&#20010;&#29702;&#24819;&#30340;&#35774;&#32622;&#65292;&#21363;&#19981;&#21516;&#30340;&#39046;&#22495;&#36981;&#23432;&#30456;&#20284;&#30340;&#25968;&#25454;&#20998;&#24067;&#65292;&#24573;&#35270;&#20102;&#30001;&#19981;&#23545;&#31216;&#20132;&#20114;&#23494;&#24230;&#24102;&#26469;&#30340;&#20559;&#24046;&#65288;&#21363;&#36328;&#39046;&#22495;&#23494;&#24230;&#20559;&#24046;&#65289;&#12290;&#27492;&#22806;&#65292;&#24207;&#21015;&#32534;&#30721;&#22120;&#20013;&#32463;&#24120;&#37319;&#29992;&#30340;&#26426;&#21046;&#65288;&#22914;&#33258;&#27880;&#24847;&#32593;&#32476;&#65289;&#21482;&#20851;&#27880;&#23616;&#37096;&#35270;&#22270;&#20869;&#30340;&#20132;&#20114;&#65292;&#24573;&#35270;&#20102;&#19981;&#21516;&#35757;&#32451;&#25209;&#27425;&#20043;&#38388;&#30340;&#20840;&#23616;&#30456;&#20851;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#22806;&#37096;&#27880;&#24847;&#21147;&#30340;&#22270;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#21363;EA-GCL&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20026;&#20102;&#28040;&#38500;&#36328;&#39046;&#22495;&#23494;&#24230;&#20559;&#24046;&#30340;&#24433;&#21709;&#65292;&#22312;&#20256;&#32479;&#22270;&#32534;&#30721;&#22120;&#19979;&#38468;&#21152;&#20102;&#19968;&#20010;&#36741;&#21161;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#20219;&#21153;&#65292;&#37319;&#29992;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#24335;&#12290;&#20026;&#20102;&#31283;&#23450;&#22320;&#25429;&#25417;&#29992;&#25143;&#30340;&#34892;&#20026;&#27169;&#24335;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;...
&lt;/p&gt;
&lt;p&gt;
Cross-domain sequential recommenders (CSRs) are gaining considerable research attention as they can capture user sequential preference by leveraging side information from multiple domains. However, these works typically follow an ideal setup, i.e., different domains obey similar data distribution, which ignores the bias brought by asymmetric interaction densities (a.k.a. the inter-domain density bias). Besides, the frequently adopted mechanism (e.g., the self-attention network) in sequence encoder only focuses on the interactions within a local view, which overlooks the global correlations between different training batches. To this end, we propose an External Attention-enhanced Graph Contrastive Learning framework, namely EA-GCL. Specifically, to remove the impact of the inter-domain density bias, an auxiliary Self-Supervised Learning (SSL) task is attached to the traditional graph encoder under a multi-task learning manner. To robustly capture users' behavioral patterns, we develop a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#24191;&#27867;&#39046;&#22495;&#29305;&#23450;&#26631;&#31614;&#31995;&#32479;&#30340;&#22823;&#35268;&#27169;&#21009;&#20107;&#35201;&#32032;&#25552;&#21462;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#20511;&#21161;&#27861;&#24459;&#19987;&#23478;&#22242;&#38431;&#21644;&#27861;&#24459;&#30693;&#35782;&#30340;&#27880;&#37322;&#65292;&#21487;&#20197;&#22686;&#24378;&#27861;&#24459;&#26696;&#20363;&#30340;&#35299;&#37322;&#21644;&#20998;&#26512;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.01271</link><description>&lt;p&gt;
LEEC: &#19968;&#31181;&#20855;&#26377;&#24191;&#27867;&#39046;&#22495;&#29305;&#23450;&#26631;&#31614;&#31995;&#32479;&#30340;&#27861;&#24459;&#35201;&#32032;&#25552;&#21462;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
LEEC: A Legal Element Extraction Dataset with an Extensive Domain-Specific Label System. (arXiv:2310.01271v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01271
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#24191;&#27867;&#39046;&#22495;&#29305;&#23450;&#26631;&#31614;&#31995;&#32479;&#30340;&#22823;&#35268;&#27169;&#21009;&#20107;&#35201;&#32032;&#25552;&#21462;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#20511;&#21161;&#27861;&#24459;&#19987;&#23478;&#22242;&#38431;&#21644;&#27861;&#24459;&#30693;&#35782;&#30340;&#27880;&#37322;&#65292;&#21487;&#20197;&#22686;&#24378;&#27861;&#24459;&#26696;&#20363;&#30340;&#35299;&#37322;&#21644;&#20998;&#26512;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#37325;&#35201;&#20219;&#21153;&#65292;&#35201;&#32032;&#25552;&#21462;&#22312;&#27861;&#24459;&#39046;&#22495;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#20174;&#21496;&#27861;&#25991;&#20214;&#20013;&#25552;&#21462;&#27861;&#24459;&#35201;&#32032;&#26377;&#21161;&#20110;&#22686;&#24378;&#27861;&#24459;&#26696;&#20363;&#30340;&#35299;&#37322;&#21644;&#20998;&#26512;&#33021;&#21147;&#65292;&#20174;&#32780;&#20419;&#36827;&#27861;&#24459;&#39046;&#22495;&#21508;&#20010;&#39046;&#22495;&#30340;&#19979;&#28216;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#35201;&#32032;&#25552;&#21462;&#25968;&#25454;&#38598;&#23384;&#22312;&#23545;&#27861;&#24459;&#30693;&#35782;&#30340;&#21463;&#38480;&#35775;&#38382;&#21644;&#26631;&#31614;&#35206;&#30422;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26356;&#20840;&#38754;&#12289;&#22823;&#35268;&#27169;&#30340;&#21009;&#20107;&#35201;&#32032;&#25552;&#21462;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;15,831&#20010;&#21496;&#27861;&#25991;&#20214;&#21644;159&#20010;&#26631;&#31614;&#12290;&#35813;&#25968;&#25454;&#38598;&#36890;&#36807;&#20004;&#20010;&#20027;&#35201;&#27493;&#39588;&#26500;&#24314;&#65306;&#39318;&#20808;&#65292;&#30001;&#25105;&#20204;&#30340;&#27861;&#24459;&#19987;&#23478;&#22242;&#38431;&#26681;&#25454;&#20808;&#21069;&#30340;&#27861;&#24459;&#30740;&#31350;&#35774;&#35745;&#20102;&#26631;&#31614;&#31995;&#32479;&#65292;&#35813;&#30740;&#31350;&#30830;&#23450;&#20102;&#21009;&#20107;&#26696;&#20214;&#20013;&#24433;&#21709;&#21028;&#20915;&#32467;&#26524;&#30340;&#20851;&#38190;&#22240;&#32032;&#21644;&#29983;&#25104;&#36807;&#31243;&#65307;&#20854;&#27425;&#65292;&#21033;&#29992;&#27861;&#24459;&#30693;&#35782;&#26681;&#25454;&#26631;&#31614;&#31995;&#32479;&#21644;&#27880;&#37322;&#20934;&#21017;&#23545;&#21496;&#27861;&#25991;&#20214;&#36827;&#34892;&#27880;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
As a pivotal task in natural language processing, element extraction has gained significance in the legal domain. Extracting legal elements from judicial documents helps enhance interpretative and analytical capacities of legal cases, and thereby facilitating a wide array of downstream applications in various domains of law. Yet existing element extraction datasets are limited by their restricted access to legal knowledge and insufficient coverage of labels. To address this shortfall, we introduce a more comprehensive, large-scale criminal element extraction dataset, comprising 15,831 judicial documents and 159 labels. This dataset was constructed through two main steps: first, designing the label system by our team of legal experts based on prior legal research which identified critical factors driving and processes generating sentencing outcomes in criminal cases; second, employing the legal knowledge to annotate judicial documents according to the label system and annotation guideli
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32431;&#31929;&#30340;&#28040;&#24687;&#20256;&#36882;&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#20849;&#21516;&#37051;&#23621;&#36827;&#34892;&#38142;&#36335;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#36755;&#20837;&#21521;&#37327;&#30340;&#27491;&#20132;&#24615;&#26469;&#25429;&#25417;&#32852;&#21512;&#32467;&#26500;&#29305;&#24449;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38142;&#36335;&#39044;&#27979;&#27169;&#22411;MPLP&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#20934;&#27491;&#20132;&#21521;&#37327;&#20272;&#35745;&#38142;&#36335;&#32423;&#32467;&#26500;&#29305;&#24449;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#33410;&#28857;&#32423;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.00976</link><description>&lt;p&gt;
&#32431;&#31929;&#30340;&#28040;&#24687;&#20256;&#36882;&#21487;&#20197;&#20272;&#35745;&#20849;&#21516;&#37051;&#23621;&#36827;&#34892;&#38142;&#36335;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Pure Message Passing Can Estimate Common Neighbor for Link Prediction. (arXiv:2309.00976v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00976
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32431;&#31929;&#30340;&#28040;&#24687;&#20256;&#36882;&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#20849;&#21516;&#37051;&#23621;&#36827;&#34892;&#38142;&#36335;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#36755;&#20837;&#21521;&#37327;&#30340;&#27491;&#20132;&#24615;&#26469;&#25429;&#25417;&#32852;&#21512;&#32467;&#26500;&#29305;&#24449;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38142;&#36335;&#39044;&#27979;&#27169;&#22411;MPLP&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#20934;&#27491;&#20132;&#21521;&#37327;&#20272;&#35745;&#38142;&#36335;&#32423;&#32467;&#26500;&#29305;&#24449;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#33410;&#28857;&#32423;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65288;MPNN&#65289;&#24050;&#25104;&#20026;&#22270;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;&#20107;&#23454;&#26631;&#20934;&#12290;&#28982;&#32780;&#65292;&#22312;&#38142;&#36335;&#39044;&#27979;&#26041;&#38754;&#65292;&#23427;&#20204;&#24448;&#24448;&#34920;&#29616;&#19981;&#20339;&#65292;&#34987;&#31616;&#21333;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#22914;&#20849;&#21516;&#37051;&#23621;&#65288;CN&#65289;&#25152;&#36229;&#36234;&#12290;&#36825;&#31181;&#24046;&#24322;&#28304;&#20110;&#19968;&#20010;&#26681;&#26412;&#38480;&#21046;&#65306;&#23613;&#31649;MPNN&#22312;&#33410;&#28857;&#32423;&#34920;&#31034;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#32534;&#30721;&#38142;&#36335;&#39044;&#27979;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#32852;&#21512;&#32467;&#26500;&#29305;&#24449;&#65288;&#22914;CN&#65289;&#26041;&#38754;&#21017;&#36935;&#21040;&#22256;&#38590;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#35748;&#20026;&#36890;&#36807;&#21033;&#29992;&#36755;&#20837;&#21521;&#37327;&#30340;&#27491;&#20132;&#24615;&#65292;&#32431;&#31929;&#30340;&#28040;&#24687;&#20256;&#36882;&#30830;&#23454;&#21487;&#20197;&#25429;&#25417;&#21040;&#32852;&#21512;&#32467;&#26500;&#29305;&#24449;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;MPNN&#22312;&#36817;&#20284;CN&#21551;&#21457;&#24335;&#31639;&#27861;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#21457;&#29616;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#38142;&#36335;&#39044;&#27979;&#27169;&#22411;&#8212;&#8212;&#28040;&#24687;&#20256;&#36882;&#38142;&#36335;&#39044;&#27979;&#22120;&#65288;MPLP&#65289;&#12290;MPLP&#21033;&#29992;&#20934;&#27491;&#20132;&#21521;&#37327;&#20272;&#35745;&#38142;&#36335;&#32423;&#32467;&#26500;&#29305;&#24449;&#65292;&#21516;&#26102;&#20445;&#30041;&#33410;&#28857;&#32423;&#22797;&#26434;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#34920;&#26126;&#21033;&#29992;&#28040;&#24687;&#20256;&#36882;&#25429;&#25417;&#32467;&#26500;&#29305;&#24449;&#33021;&#22815;&#25913;&#21892;&#38142;&#36335;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Message Passing Neural Networks (MPNNs) have emerged as the {\em de facto} standard in graph representation learning. However, when it comes to link prediction, they often struggle, surpassed by simple heuristics such as Common Neighbor (CN). This discrepancy stems from a fundamental limitation: while MPNNs excel in node-level representation, they stumble with encoding the joint structural features essential to link prediction, like CN. To bridge this gap, we posit that, by harnessing the orthogonality of input vectors, pure message-passing can indeed capture joint structural features. Specifically, we study the proficiency of MPNNs in approximating CN heuristics. Based on our findings, we introduce the Message Passing Link Predictor (MPLP), a novel link prediction model. MPLP taps into quasi-orthogonal vectors to estimate link-level structural features, all while preserving the node-level complexities. Moreover, our approach demonstrates that leveraging message-passing to capture stru
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;DocumentNet&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;Web&#19978;&#25910;&#38598;&#22823;&#35268;&#27169;&#21644;&#24369;&#26631;&#27880;&#30340;&#25968;&#25454;&#65292;&#24357;&#21512;&#20102;&#25991;&#26723;&#39044;&#35757;&#32451;&#20013;&#30340;&#25968;&#25454;&#24046;&#36317;&#65292;&#24182;&#22312;&#21508;&#31867;VDER&#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2306.08937</link><description>&lt;p&gt;
DocumentNet: &#22312;&#25991;&#26723;&#39044;&#35757;&#32451;&#20013;&#24357;&#21512;&#25968;&#25454;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
DocumentNet: Bridging the Data Gap in Document Pre-Training. (arXiv:2306.08937v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08937
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;DocumentNet&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;Web&#19978;&#25910;&#38598;&#22823;&#35268;&#27169;&#21644;&#24369;&#26631;&#27880;&#30340;&#25968;&#25454;&#65292;&#24357;&#21512;&#20102;&#25991;&#26723;&#39044;&#35757;&#32451;&#20013;&#30340;&#25968;&#25454;&#24046;&#36317;&#65292;&#24182;&#22312;&#21508;&#31867;VDER&#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#25991;&#26723;&#29702;&#35299;&#20219;&#21153;&#65292;&#29305;&#21035;&#26159;&#23500;&#26377;&#35270;&#35273;&#20803;&#32032;&#30340;&#25991;&#26723;&#23454;&#20307;&#26816;&#32034;&#65288;VDER&#65289;&#65292;&#30001;&#20110;&#22312;&#20225;&#19994;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#21463;&#21040;&#20102;&#26497;&#22823;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20005;&#26684;&#30340;&#38544;&#31169;&#32422;&#26463;&#21644;&#39640;&#26114;&#30340;&#26631;&#27880;&#25104;&#26412;&#65292;&#36825;&#20123;&#20219;&#21153;&#30340;&#20844;&#24320;&#21487;&#29992;&#25968;&#25454;&#38750;&#24120;&#26377;&#38480;&#12290;&#26356;&#31967;&#31957;&#30340;&#26159;&#65292;&#26469;&#33258;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;&#19981;&#37325;&#21472;&#23454;&#20307;&#31354;&#38388;&#22952;&#30861;&#20102;&#25991;&#26723;&#31867;&#22411;&#20043;&#38388;&#30340;&#30693;&#35782;&#36716;&#31227;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;Web&#25910;&#38598;&#22823;&#35268;&#27169;&#21644;&#24369;&#26631;&#27880;&#30340;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#20197;&#21033;&#20110;VDER&#27169;&#22411;&#30340;&#35757;&#32451;&#12290;&#25152;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#21517;&#20026;DocumentNet&#65292;&#19981;&#20381;&#36182;&#20110;&#29305;&#23450;&#30340;&#25991;&#26723;&#31867;&#22411;&#25110;&#23454;&#20307;&#38598;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#25152;&#26377;&#30340;VDER&#20219;&#21153;&#12290;&#30446;&#21069;&#30340;DocumentNet&#21253;&#21547;&#20102;30M&#20010;&#25991;&#26723;&#65292;&#28085;&#30422;&#20102;&#36817;400&#20010;&#25991;&#26723;&#31867;&#22411;&#65292;&#32452;&#32455;&#25104;&#20102;&#19968;&#20010;&#22235;&#32423;&#26412;&#20307;&#32467;&#26500;&#12290;&#22312;&#19968;&#31995;&#21015;&#24191;&#27867;&#37319;&#29992;&#30340;VDER&#20219;&#21153;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#24403;&#23558;DocumentNet&#32435;&#20837;&#39044;&#35757;&#32451;&#36807;&#31243;&#26102;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Document understanding tasks, in particular, Visually-rich Document Entity Retrieval (VDER), have gained significant attention in recent years thanks to their broad applications in enterprise AI. However, publicly available data have been scarce for these tasks due to strict privacy constraints and high annotation costs. To make things worse, the non-overlapping entity spaces from different datasets hinder the knowledge transfer between document types. In this paper, we propose a method to collect massive-scale and weakly labeled data from the web to benefit the training of VDER models. The collected dataset, named DocumentNet, does not depend on specific document types or entity sets, making it universally applicable to all VDER tasks. The current DocumentNet consists of 30M documents spanning nearly 400 document types organized in a four-level ontology. Experiments on a set of broadly adopted VDER tasks show significant improvements when DocumentNet is incorporated into the pre-train
&lt;/p&gt;</description></item></channel></rss>