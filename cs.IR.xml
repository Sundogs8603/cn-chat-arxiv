<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#36890;&#36807;&#25104;&#26412;&#25935;&#24863;&#23398;&#20064;&#20943;&#36731;&#25512;&#33616;&#20013;&#30340;&#20027;&#27969;&#20559;&#35265;&#65292;&#36890;&#36807;&#20351;&#29992;&#25512;&#33616;&#25928;&#29992;&#20316;&#20026;&#34913;&#37327;&#20027;&#27969;&#24615;&#30340;&#20195;&#29702;&#65292;&#24182;&#25552;&#20986;&#20102;&#31616;&#21333;&#30340;&#29992;&#25143;&#21152;&#26435;&#26041;&#27861;&#26469;&#32435;&#20837;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#36825;&#26679;&#33021;&#22815;&#26356;&#22909;&#22320;&#35782;&#21035;&#38750;&#20027;&#27969;&#29992;&#25143;&#24182;&#25552;&#20379;&#26356;&#22909;&#30340;&#26381;&#21153;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.13632</link><description>&lt;p&gt;
&#36890;&#36807;&#25104;&#26412;&#25935;&#24863;&#23398;&#20064;&#20943;&#36731;&#25512;&#33616;&#20013;&#30340;&#20027;&#27969;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Mitigating Mainstream Bias in Recommendation via Cost-sensitive Learning. (arXiv:2307.13632v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13632
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25104;&#26412;&#25935;&#24863;&#23398;&#20064;&#20943;&#36731;&#25512;&#33616;&#20013;&#30340;&#20027;&#27969;&#20559;&#35265;&#65292;&#36890;&#36807;&#20351;&#29992;&#25512;&#33616;&#25928;&#29992;&#20316;&#20026;&#34913;&#37327;&#20027;&#27969;&#24615;&#30340;&#20195;&#29702;&#65292;&#24182;&#25552;&#20986;&#20102;&#31616;&#21333;&#30340;&#29992;&#25143;&#21152;&#26435;&#26041;&#27861;&#26469;&#32435;&#20837;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#36825;&#26679;&#33021;&#22815;&#26356;&#22909;&#22320;&#35782;&#21035;&#38750;&#20027;&#27969;&#29992;&#25143;&#24182;&#25552;&#20379;&#26356;&#22909;&#30340;&#26381;&#21153;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#27969;&#20559;&#35265;&#26159;&#25351;&#19968;&#20123;&#29992;&#25143;&#30001;&#20110;&#20854;&#20559;&#22909;&#19981;&#24120;&#35265;&#25110;&#32773;&#27963;&#36291;&#24230;&#36739;&#20302;&#32780;&#25910;&#21040;&#36739;&#24046;&#30340;&#25512;&#33616;&#65292;&#36825;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#23545;&#20844;&#24179;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#26041;&#27861;&#26410;&#26174;&#24335;&#22320;&#23545;&#36825;&#20123;&#38750;&#20027;&#27969;&#29992;&#25143;&#30340;&#37325;&#35201;&#24615;&#24314;&#27169;&#65292;&#25110;&#32773;&#22312;&#24314;&#27169;&#26102;&#19982;&#25968;&#25454;&#21644;&#25512;&#33616;&#27169;&#22411;&#19981;&#20860;&#23481;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#20351;&#29992;&#25512;&#33616;&#25928;&#29992;&#20316;&#20026;&#26356;&#36890;&#29992;&#21644;&#38544;&#21547;&#30340;&#34913;&#37327;&#20027;&#27969;&#24615;&#30340;&#20195;&#29702;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#29992;&#25143;&#21152;&#26435;&#26041;&#27861;&#65292;&#23558;&#20854;&#32435;&#20837;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#24182;&#32771;&#34385;&#28508;&#22312;&#25512;&#33616;&#38169;&#35823;&#30340;&#25104;&#26412;&#12290;&#25105;&#20204;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#25928;&#29992;&#26469;&#37327;&#21270;&#20027;&#27969;&#24615;&#26356;&#33021;&#20934;&#30830;&#22320;&#35782;&#21035;&#38750;&#20027;&#27969;&#29992;&#25143;&#65292;&#24182;&#19988;&#24403;&#20351;&#29992;&#25104;&#26412;&#25935;&#24863;&#26041;&#24335;&#35757;&#32451;&#27169;&#22411;&#26102;&#65292;&#36825;&#20123;&#29992;&#25143;&#30340;&#26381;&#21153;&#25928;&#26524;&#30830;&#23454;&#26356;&#22909;&#12290;&#36825;&#19968;&#25104;&#26524;&#20960;&#20046;&#27809;&#26377;&#25110;&#27809;&#26377;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mainstream bias, where some users receive poor recommendations because their preferences are uncommon or simply because they are less active, is an important aspect to consider regarding fairness in recommender systems. Existing methods to mitigate mainstream bias do not explicitly model the importance of these non-mainstream users or, when they do, it is in a way that is not necessarily compatible with the data and recommendation model at hand. In contrast, we use the recommendation utility as a more generic and implicit proxy to quantify mainstreamness, and propose a simple user-weighting approach to incorporate it into the training process while taking the cost of potential recommendation errors into account. We provide extensive experimental results showing that quantifying mainstreamness via utility is better able at identifying non-mainstream users, and that they are indeed better served when training the model in a cost-sensitive way. This is achieved with negligible or no loss 
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#22522;&#20110;&#26412;&#20307;&#30340;&#30693;&#35782;&#22270;&#35889;&#65292;&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RenderSelect&#30340;&#26381;&#21153;&#21457;&#29616;&#24341;&#25806;&#65292;&#29992;&#20110;&#21457;&#29616;&#25104;&#26412;&#25928;&#30410;&#39640;&#19988;&#31526;&#21512;&#21151;&#33021;&#35201;&#27714;&#30340;&#20113;&#28210;&#26579;&#20892;&#22330;&#26381;&#21153;&#12290;</title><link>http://arxiv.org/abs/2307.13604</link><description>&lt;p&gt;
&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#22522;&#20110;&#26412;&#20307;&#30340;&#30693;&#35782;&#22270;&#35889;&#21457;&#29616;&#20113;&#28210;&#26579;&#20892;&#22330;&#26381;&#21153;
&lt;/p&gt;
&lt;p&gt;
Cloud Render Farm Services Discovery Using NLP And Ontology Based Knowledge Graph. (arXiv:2307.13604v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13604
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#22522;&#20110;&#26412;&#20307;&#30340;&#30693;&#35782;&#22270;&#35889;&#65292;&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RenderSelect&#30340;&#26381;&#21153;&#21457;&#29616;&#24341;&#25806;&#65292;&#29992;&#20110;&#21457;&#29616;&#25104;&#26412;&#25928;&#30410;&#39640;&#19988;&#31526;&#21512;&#21151;&#33021;&#35201;&#27714;&#30340;&#20113;&#28210;&#26579;&#20892;&#22330;&#26381;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20113;&#28210;&#26579;&#20892;&#22330;&#26381;&#21153;&#26159;&#38024;&#23545;&#21160;&#30011;&#39046;&#22495;&#30340;&#29305;&#23450;&#20113;&#26381;&#21153;&#24179;&#21488;&#65288;PaaS&#65289;&#31867;&#22411;&#30340;&#20113;&#26381;&#21153;&#65292;&#25552;&#20379;&#23436;&#25972;&#30340;&#24179;&#21488;&#26469;&#28210;&#26579;&#21160;&#30011;&#25991;&#20214;&#12290;&#28982;&#32780;&#65292;&#35782;&#21035;&#25104;&#26412;&#25928;&#30410;&#19988;&#31526;&#21512;&#21151;&#33021;&#35201;&#27714;&#65288;&#22914;&#21160;&#30011;&#36719;&#20214;&#12289;&#25152;&#38656;&#25554;&#20214;&#31561;&#65289;&#30340;&#28210;&#26579;&#20892;&#22330;&#26381;&#21153;&#26159;&#19968;&#39033;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26412;&#20307;&#30340;&#26381;&#21153;&#21457;&#29616;&#24341;&#25806;RenderSelect&#65292;&#29992;&#20110;&#20113;&#28210;&#26579;&#20892;&#22330;&#26381;&#21153;&#12290;&#20113;&#28210;&#26579;&#20892;&#22330;&#26412;&#20307;&#35821;&#20041;&#19978;&#23450;&#20041;&#20102;&#20113;&#28210;&#26579;&#20892;&#22330;&#26381;&#21153;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#37319;&#29992;&#22522;&#20110;&#30693;&#35782;&#30340;&#25512;&#29702;&#31639;&#27861;&#65292;&#21253;&#25324;&#27010;&#24565;&#30456;&#20284;&#24615;&#25512;&#29702;&#12289;&#31561;&#20215;&#25512;&#29702;&#21644;&#25968;&#20540;&#30456;&#20284;&#24615;&#25512;&#29702;&#65292;&#26469;&#30830;&#23450;&#20113;&#26381;&#21153;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#35813;&#26381;&#21153;&#21457;&#29616;&#24341;&#25806;&#22312;&#19977;&#31181;&#19981;&#21516;&#30340;&#24773;&#26223;&#19979;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21363;a&#65289;&#21033;&#29992;&#26412;&#20307;&#24110;&#21161;&#65292;b&#65289;
&lt;/p&gt;
&lt;p&gt;
Cloud render farm services are the animation domain specific cloud services Platform-as-a-Service (PaaS) type of cloud services that provides a complete platform to render the animation files. However, identifying the render farm services that is cost effective and also matches the functional requirements that changes for almost every project like the animation software, plug-ins required etc., is a challenge. This research work proposes an ontology-based service discovery engine named RenderSelect for the cloud render farm services. The cloud render farm ontology semantically defines the relationship among the cloud render farm services. The knowledge-based reasoning algorithms namely, the Concept similarity reasoning, Equivalent reasoning and the Numerical similarity reasoning have been applied to determine the similarity among the cloud services. The service discovery engine was evaluated for finding the services under three different scenarios namely a) with help of the ontology, b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20855;&#26377;&#39640;&#26031;&#22270;&#21644;&#20856;&#22411;&#23545;&#27604;&#23398;&#20064;&#65288;GPCL&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#30005;&#23376;&#21830;&#21153;&#22871;&#39184;&#25512;&#33616;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#37319;&#26679;&#20559;&#24046;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.13468</link><description>&lt;p&gt;
&#30005;&#23376;&#21830;&#21153;&#22871;&#39184;&#25512;&#33616;&#20013;&#30340;&#39640;&#26031;&#22270;&#19982;&#20856;&#22411;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Gaussian Graph with Prototypical Contrastive Learning in E-Commerce Bundle Recommendation. (arXiv:2307.13468v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13468
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20855;&#26377;&#39640;&#26031;&#22270;&#21644;&#20856;&#22411;&#23545;&#27604;&#23398;&#20064;&#65288;GPCL&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#30005;&#23376;&#21830;&#21153;&#22871;&#39184;&#25512;&#33616;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#37319;&#26679;&#20559;&#24046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22871;&#39184;&#25512;&#33616;&#26088;&#22312;&#28385;&#36275;&#29992;&#25143;&#22312;&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#19978;&#30340;&#20559;&#22909;&#12290;&#26082;&#26377;&#30340;&#25104;&#21151;&#35299;&#20915;&#26041;&#26696;&#22522;&#20110;&#23545;&#27604;&#22270;&#23398;&#20064;&#33539;&#24335;&#65292;&#20854;&#20013;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#20174;&#29992;&#25143;&#32423;&#21035;&#21644;&#22871;&#39184;&#32423;&#21035;&#30340;&#22270;&#35270;&#22270;&#20013;&#23398;&#20064;&#34920;&#31034;&#65292;&#24182;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#27169;&#22359;&#22686;&#24378;&#19981;&#21516;&#35270;&#22270;&#20043;&#38388;&#30340;&#21512;&#20316;&#20851;&#32852;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#39640;&#24230;&#31232;&#30095;&#24615;&#25110;&#22810;&#26679;&#24615;&#23548;&#33268;&#30340;&#32570;&#20047;&#26377;&#21306;&#21035;&#24615;&#20449;&#24687;&#65292;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#22312;&#23454;&#38469;&#22871;&#39184;&#25512;&#33616;&#22330;&#26223;&#20013;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#65292;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#24573;&#35270;&#20102;&#36825;&#19968;&#38382;&#39064;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25351;&#20986;&#65292;&#23427;&#20204;&#30340;&#36880;&#23454;&#20363;&#23545;&#27604;&#23398;&#20064;&#26080;&#27861;&#21306;&#20998;&#35821;&#20041;&#19978;&#30456;&#20284;&#30340;&#36127;&#26679;&#26412;&#65288;&#21363;&#37319;&#26679;&#20559;&#24046;&#38382;&#39064;&#65289;&#65292;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20855;&#26377;&#39640;&#26031;&#22270;&#21644;&#20856;&#22411;&#23545;&#27604;&#23398;&#20064;&#65288;GPCL&#65289;&#26694;&#26550;&#26469;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bundle recommendation aims to provide a bundle of items to satisfy the user preference on e-commerce platform. Existing successful solutions are based on the contrastive graph learning paradigm where graph neural networks (GNNs) are employed to learn representations from user-level and bundle-level graph views with a contrastive learning module to enhance the cooperative association between different views. Nevertheless, they ignore the uncertainty issue which has a significant impact in real bundle recommendation scenarios due to the lack of discriminative information caused by highly sparsity or diversity. We further suggest that their instancewise contrastive learning fails to distinguish the semantically similar negatives (i.e., sampling bias issue), resulting in performance degradation. In this paper, we propose a novel Gaussian Graph with Prototypical Contrastive Learning (GPCL) framework to overcome these challenges. In particular, GPCL embeds each user/bundle/item as a Gaussian
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#32508;&#36848;&#20102;&#35821;&#20041;&#20449;&#24687;&#26816;&#32034;&#21644;&#26412;&#20307;&#24037;&#31243;&#30340;&#29616;&#29366;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;&#36890;&#36807;&#26412;&#20307;&#25512;&#29702;&#65292;&#21487;&#20197;&#20811;&#26381;&#20256;&#32479;&#31995;&#32479;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20379;&#19968;&#20010;&#24418;&#24335;&#21270;&#12289;&#28789;&#27963;&#21644;&#21487;&#25193;&#23637;&#30340;&#30693;&#35782;&#34920;&#31034;&#12289;&#25512;&#29702;&#21644;&#25512;&#26029;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2307.13427</link><description>&lt;p&gt;
&#35821;&#20041;&#20449;&#24687;&#26816;&#32034;&#21644;&#26412;&#20307;&#24037;&#31243;&#30340;&#32508;&#21512;&#35780;&#36848;
&lt;/p&gt;
&lt;p&gt;
Comprehensive Review on Semantic Information Retrieval and Ontology Engineering. (arXiv:2307.13427v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13427
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#32508;&#36848;&#20102;&#35821;&#20041;&#20449;&#24687;&#26816;&#32034;&#21644;&#26412;&#20307;&#24037;&#31243;&#30340;&#29616;&#29366;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;&#36890;&#36807;&#26412;&#20307;&#25512;&#29702;&#65292;&#21487;&#20197;&#20811;&#26381;&#20256;&#32479;&#31995;&#32479;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20379;&#19968;&#20010;&#24418;&#24335;&#21270;&#12289;&#28789;&#27963;&#21644;&#21487;&#25193;&#23637;&#30340;&#30693;&#35782;&#34920;&#31034;&#12289;&#25512;&#29702;&#21644;&#25512;&#26029;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#26223;&#24847;&#35782;&#26159;&#19968;&#31181;&#37325;&#35201;&#30340;&#35748;&#30693;&#33021;&#21147;&#65292;&#20351;&#20010;&#20307;&#33021;&#22815;&#20934;&#30830;&#22320;&#24863;&#30693;&#12289;&#29702;&#35299;&#21644;&#39044;&#27979;&#29615;&#22659;&#30340;&#24403;&#21069;&#29366;&#24577;&#12290;&#23427;&#28041;&#21450;&#24847;&#35782;&#21040;&#30456;&#20851;&#20449;&#24687;&#65292;&#29702;&#35299;&#20854;&#21547;&#20041;&#65292;&#24182;&#21033;&#29992;&#36825;&#31181;&#29702;&#35299;&#20570;&#20986;&#26126;&#26234;&#30340;&#20915;&#31574;&#12290;&#24847;&#35782;&#31995;&#32479;&#36890;&#24120;&#38656;&#35201;&#25972;&#21512;&#26032;&#30340;&#30693;&#35782;&#24182;&#36866;&#24212;&#19981;&#26029;&#21464;&#21270;&#30340;&#29615;&#22659;&#12290;&#26412;&#20307;&#25512;&#29702;&#20419;&#36827;&#20102;&#30693;&#35782;&#30340;&#25972;&#21512;&#21644;&#28436;&#21270;&#65292;&#23454;&#29616;&#20102;&#26412;&#20307;&#30340;&#26080;&#32541;&#26356;&#26032;&#21644;&#25193;&#23637;&#12290;&#32771;&#34385;&#21040;&#19978;&#36848;&#22240;&#32032;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#35821;&#20041;&#20449;&#24687;&#26816;&#32034;&#21644;&#26412;&#20307;&#24037;&#31243;&#30340;&#24555;&#36895;&#35780;&#36848;&#65292;&#20197;&#20102;&#35299;&#26032;&#20852;&#25361;&#25112;&#21644;&#26410;&#26469;&#30740;&#31350;&#12290;&#22312;&#35780;&#36848;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#26412;&#20307;&#25512;&#29702;&#36890;&#36807;&#25552;&#20379;&#19968;&#20010;&#24418;&#24335;&#21270;&#12289;&#28789;&#27963;&#21644;&#21487;&#25193;&#23637;&#30340;&#30693;&#35782;&#34920;&#31034;&#12289;&#25512;&#29702;&#21644;&#25512;&#26029;&#26694;&#26550;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;&#31995;&#32479;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Situation awareness is a crucial cognitive skill that enables individuals to perceive, comprehend, and project the current state of their environment accurately. It involves being conscious of relevant information, understanding its meaning, and using that understanding to make well-informed decisions. Awareness systems often need to integrate new knowledge and adapt to changing environments. Ontology reasoning facilitates knowledge integration and evolution, allowing for seamless updates and expansions of the ontology. With the consideration of above, we are providing a quick review on semantic information retrieval and ontology engineering to understand the emerging challenges and future research. In the review we have found that the ontology reasoning addresses the limitations of traditional systems by providing a formal, flexible, and scalable framework for knowledge representation, reasoning, and inference.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#20027;&#39064;&#20998;&#21106;&#21644;&#25991;&#26412;&#25688;&#35201;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#25506;&#35752;&#20102;&#22914;&#20309;&#36890;&#36807;&#20026;&#25773;&#23458;&#33410;&#30446;&#25552;&#20379;&#36880;&#20010;&#20027;&#39064;&#30340;&#32454;&#20998;&#21644;&#27599;&#20010;&#20027;&#39064;&#30340;&#31616;&#30701;&#25688;&#35201;&#26469;&#25552;&#39640;&#25773;&#23458;&#33410;&#30446;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;&#30740;&#31350;&#37319;&#26679;&#20102;Spotify&#30340;&#33521;&#35821;&#25773;&#23458;&#25968;&#25454;&#38598;&#20013;&#30340;10&#20010;&#33410;&#30446;&#65292;&#24182;&#20351;&#29992;TextSplit&#31639;&#27861;&#36827;&#34892;&#20102;&#20998;&#21106;&#12290;&#35843;&#26597;&#32467;&#26524;&#34920;&#26126;&#65292;TextSplit&#31639;&#27861;&#22312;&#35780;&#20272;&#25351;&#26631;&#19978;&#36798;&#21040;&#20102;&#26368;&#20302;&#24179;&#22343;&#20540;&#12290;</title><link>http://arxiv.org/abs/2307.13394</link><description>&lt;p&gt;
&#20351;&#29992;&#20027;&#39064;&#20998;&#21106;&#21644;&#25991;&#26412;&#25688;&#35201;&#26041;&#27861;&#30340;&#31471;&#21040;&#31471;&#24037;&#20316;&#27969;&#31243;&#65292;&#20197;&#25552;&#39640;&#25773;&#23458;&#29702;&#35299;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
An End-to-End Workflow using Topic Segmentation and Text Summarisation Methods for Improved Podcast Comprehension. (arXiv:2307.13394v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13394
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#20027;&#39064;&#20998;&#21106;&#21644;&#25991;&#26412;&#25688;&#35201;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#25506;&#35752;&#20102;&#22914;&#20309;&#36890;&#36807;&#20026;&#25773;&#23458;&#33410;&#30446;&#25552;&#20379;&#36880;&#20010;&#20027;&#39064;&#30340;&#32454;&#20998;&#21644;&#27599;&#20010;&#20027;&#39064;&#30340;&#31616;&#30701;&#25688;&#35201;&#26469;&#25552;&#39640;&#25773;&#23458;&#33410;&#30446;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;&#30740;&#31350;&#37319;&#26679;&#20102;Spotify&#30340;&#33521;&#35821;&#25773;&#23458;&#25968;&#25454;&#38598;&#20013;&#30340;10&#20010;&#33410;&#30446;&#65292;&#24182;&#20351;&#29992;TextSplit&#31639;&#27861;&#36827;&#34892;&#20102;&#20998;&#21106;&#12290;&#35843;&#26597;&#32467;&#26524;&#34920;&#26126;&#65292;TextSplit&#31639;&#27861;&#22312;&#35780;&#20272;&#25351;&#26631;&#19978;&#36798;&#21040;&#20102;&#26368;&#20302;&#24179;&#22343;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25773;&#23458;&#23186;&#20307;&#30340;&#28040;&#36153;&#37327;&#27491;&#22312;&#36805;&#36895;&#22686;&#21152;&#12290;&#30001;&#20110;&#25773;&#23458;&#33410;&#30446;&#30340;&#20887;&#38271;&#24615;&#36136;&#65292;&#29992;&#25143;&#24120;&#24120;&#20250;&#20180;&#32454;&#36873;&#25321;&#35201;&#21548;&#30340;&#33410;&#30446;&#12290;&#34429;&#28982;&#33410;&#30446;&#25551;&#36848;&#36890;&#36807;&#25552;&#20379;&#25972;&#20010;&#25773;&#23458;&#30340;&#25688;&#35201;&#26469;&#24110;&#21161;&#29992;&#25143;&#65292;&#20294;&#23427;&#20204;&#24182;&#27809;&#26377;&#25552;&#20379;&#36880;&#20010;&#20027;&#39064;&#30340;&#32454;&#20998;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#21033;&#29992;&#20027;&#39064;&#20998;&#21106;&#21644;&#25991;&#26412;&#25688;&#35201;&#26041;&#27861;&#30456;&#32467;&#21512;&#30340;&#24212;&#29992;&#65292;&#20197;&#30740;&#31350;&#22914;&#20309;&#25552;&#39640;&#25773;&#23458;&#33410;&#30446;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;&#25105;&#20204;&#20174;Spotify&#30340;&#33521;&#35821;&#25773;&#23458;&#25968;&#25454;&#38598;&#20013;&#36873;&#25321;&#20102;10&#20010;&#33410;&#30446;&#65292;&#24182;&#20351;&#29992;TextTiling&#21644;TextSplit&#36827;&#34892;&#20102;&#20998;&#21106;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24212;&#29992;&#20102;&#19977;&#31181;&#25991;&#26412;&#25688;&#35201;&#27169;&#22411;&#65292;&#20998;&#21035;&#26159;T5&#12289;BART&#21644;Pegasus&#65292;&#20026;&#27599;&#20010;&#29255;&#27573;&#25552;&#20379;&#19968;&#20010;&#38750;&#24120;&#31616;&#30701;&#30340;&#26631;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#30340;&#26631;&#27880;&#26679;&#26412;&#20351;&#29992;&#20102;$P_k$&#21644;WindowDiff ($WD$)&#20004;&#20010;&#24230;&#37327;&#26469;&#35780;&#20272;&#20998;&#21106;&#37096;&#20998;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#19968;&#39033;&#35843;&#26597;&#65288;$N=25$&#65289;&#65292;&#20197;&#35780;&#20272;&#29983;&#25104;&#30340;&#25688;&#35201;&#30340;&#36136;&#37327;&#12290;TextSplit&#31639;&#27861;&#22312;&#20004;&#20010;&#35780;&#20272;&#25351;&#26631;&#30340;&#24179;&#22343;&#20540;&#19978;&#37117;&#36798;&#21040;&#20102;&#26368;&#20302;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
The consumption of podcast media has been increasing rapidly. Due to the lengthy nature of podcast episodes, users often carefully select which ones to listen to. Although episode descriptions aid users by providing a summary of the entire podcast, they do not provide a topic-by-topic breakdown. This study explores the combined application of topic segmentation and text summarisation methods to investigate how podcast episode comprehension can be improved. We have sampled 10 episodes from Spotify's English-Language Podcast Dataset and employed TextTiling and TextSplit to segment them. Moreover, three text summarisation models, namely T5, BART, and Pegasus, were applied to provide a very short title for each segment. The segmentation part was evaluated using our annotated sample with the $P_k$ and WindowDiff ($WD$) metrics. A survey was also rolled out ($N=25$) to assess the quality of the generated summaries. The TextSplit algorithm achieved the lowest mean for both evaluation metrics 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;&#22312;&#31185;&#23398;&#35770;&#25991;&#33268;&#35874;&#25991;&#26412;&#20013;&#33258;&#21160;&#25552;&#21462;&#21644;&#20998;&#31867;&#34987;&#33268;&#35874;&#23454;&#20307;&#30340;&#19981;&#21516;&#23884;&#20837;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#22312;&#20351;&#29992;Flair NLP&#26694;&#26550;&#36827;&#34892;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#30340;&#35757;&#32451;&#20013;&#65292;Flair Embeddings&#27169;&#22411;&#22312;&#20013;&#31561;&#35268;&#27169;&#35821;&#26009;&#24211;&#19978;&#36798;&#21040;&#20102;&#26368;&#20339;&#20934;&#30830;&#24230;&#65288;0.79&#65289;&#12290;&#21516;&#26102;&#65292;&#25193;&#22823;&#35757;&#32451;&#35821;&#26009;&#24211;&#30340;&#35268;&#27169;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#25152;&#26377;&#35757;&#32451;&#31639;&#27861;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.13377</link><description>&lt;p&gt;
&#20351;&#29992;&#23884;&#20837;&#27169;&#22411;&#36827;&#34892;&#31185;&#23398;&#33268;&#35874;&#20013;&#21629;&#21517;&#23454;&#20307;&#30340;&#33258;&#21160;&#25552;&#21462;&#21644;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Embedding Models for Supervised Automatic Extraction and Classification of Named Entities in Scientific Acknowledgements. (arXiv:2307.13377v1 [cs.DL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13377
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;&#22312;&#31185;&#23398;&#35770;&#25991;&#33268;&#35874;&#25991;&#26412;&#20013;&#33258;&#21160;&#25552;&#21462;&#21644;&#20998;&#31867;&#34987;&#33268;&#35874;&#23454;&#20307;&#30340;&#19981;&#21516;&#23884;&#20837;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#22312;&#20351;&#29992;Flair NLP&#26694;&#26550;&#36827;&#34892;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#30340;&#35757;&#32451;&#20013;&#65292;Flair Embeddings&#27169;&#22411;&#22312;&#20013;&#31561;&#35268;&#27169;&#35821;&#26009;&#24211;&#19978;&#36798;&#21040;&#20102;&#26368;&#20339;&#20934;&#30830;&#24230;&#65288;0.79&#65289;&#12290;&#21516;&#26102;&#65292;&#25193;&#22823;&#35757;&#32451;&#35821;&#26009;&#24211;&#30340;&#35268;&#27169;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#25152;&#26377;&#35757;&#32451;&#31639;&#27861;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#35770;&#25991;&#20013;&#30340;&#33268;&#35874;&#37096;&#20998;&#21487;&#33021;&#25581;&#31034;&#31185;&#23398;&#31038;&#21306;&#30340;&#26576;&#20123;&#26041;&#38754;&#65292;&#27604;&#22914;&#22870;&#21169;&#20307;&#31995;&#12289;&#21512;&#20316;&#27169;&#24335;&#21644;&#38544;&#34255;&#30340;&#30740;&#31350;&#36235;&#21183;&#12290;&#35813;&#35770;&#25991;&#26088;&#22312;&#35780;&#20272;&#19981;&#21516;&#23884;&#20837;&#27169;&#22411;&#22312;&#31185;&#23398;&#35770;&#25991;&#33268;&#35874;&#25991;&#26412;&#20013;&#33258;&#21160;&#25552;&#21462;&#21644;&#20998;&#31867;&#34987;&#33268;&#35874;&#23454;&#20307;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#20351;&#29992;Flair NLP&#26694;&#26550;&#36827;&#34892;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#20219;&#21153;&#30340;&#35757;&#32451;&#21644;&#23454;&#29616;&#12290;&#35757;&#32451;&#20351;&#29992;&#20102;&#19977;&#20010;&#40664;&#35748;&#30340;Flair NER&#27169;&#22411;&#65292;&#20351;&#29992;&#22235;&#20010;&#19981;&#21516;&#22823;&#23567;&#30340;&#35821;&#26009;&#24211;&#21644;&#19981;&#21516;&#29256;&#26412;&#30340;Flair NLP&#26694;&#26550;&#36827;&#34892;&#12290;&#22312;&#26368;&#26032;&#30340;FLAIR&#29256;&#26412;&#19978;&#65292;&#20351;&#29992;&#20013;&#31561;&#35268;&#27169;&#30340;&#35821;&#26009;&#24211;&#35757;&#32451;&#30340;Flair&#23884;&#20837;&#27169;&#22411;&#26174;&#31034;&#20986;&#20102;&#26368;&#22909;&#30340;&#20934;&#30830;&#24615;&#65292;&#20026;0.79&#12290;&#23558;&#35757;&#32451;&#35821;&#26009;&#24211;&#30340;&#35268;&#27169;&#20174;&#38750;&#24120;&#23567;&#30340;&#25193;&#23637;&#21040;&#20013;&#31561;&#35268;&#27169;&#22823;&#22823;&#25552;&#39640;&#20102;&#25152;&#26377;&#35757;&#32451;&#31639;&#27861;&#30340;&#20934;&#30830;&#24615;&#65292;&#20294;&#36827;&#19968;&#27493;&#25193;&#22823;&#35757;&#32451;&#35821;&#26009;&#24211;&#24182;&#27809;&#26377;&#24102;&#26469;&#36827;&#19968;&#27493;&#30340;&#25913;&#21892;&#12290;&#27492;&#22806;&#65292;&#23884;&#20837;&#27169;&#22411;&#30340;&#24615;&#33021;&#22312;&#20854;&#20182;Embeddings&#36873;&#39033;&#19978;&#27809;&#26377;&#26174;&#33879;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Acknowledgments in scientific papers may give an insight into aspects of the scientific community, such as reward systems, collaboration patterns, and hidden research trends. The aim of the paper is to evaluate the performance of different embedding models for the task of automatic extraction and classification of acknowledged entities from the acknowledgment text in scientific papers. We trained and implemented a named entity recognition (NER) task using the Flair NLP framework. The training was conducted using three default Flair NER models with four differently-sized corpora and different versions of the Flair NLP framework. The Flair Embeddings model trained on the medium corpus with the latest FLAIR version showed the best accuracy of 0.79. Expanding the size of a training corpus from very small to medium size massively increased the accuracy of all training algorithms, but further expansion of the training corpus did not bring further improvement. Moreover, the performance of the
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27861;&#24459;&#26696;&#20214;&#26816;&#32034;&#30340;&#24847;&#22270;&#20998;&#31867;&#27861;&#65292;&#22312;&#26126;&#30830;&#20102;&#27861;&#24459;&#26816;&#32034;&#29992;&#25143;&#30340;&#28508;&#22312;&#25628;&#32034;&#24847;&#22270;&#26356;&#21152;&#22797;&#26434;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#20116;&#31181;&#24847;&#22270;&#31867;&#22411;&#36827;&#34892;&#20998;&#31867;&#12290;&#35813;&#20998;&#31867;&#27861;&#32463;&#36807;&#24191;&#27867;&#30340;&#35780;&#20272;&#65292;&#25581;&#31034;&#20102;&#29992;&#25143;&#34892;&#20026;&#21644;&#28385;&#24847;&#24230;&#26041;&#38754;&#30340;&#26174;&#33879;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2307.13298</link><description>&lt;p&gt;
&#27861;&#24459;&#26696;&#20214;&#26816;&#32034;&#30340;&#24847;&#22270;&#20998;&#31867;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Intent Taxonomy of Legal Case Retrieval. (arXiv:2307.13298v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13298
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27861;&#24459;&#26696;&#20214;&#26816;&#32034;&#30340;&#24847;&#22270;&#20998;&#31867;&#27861;&#65292;&#22312;&#26126;&#30830;&#20102;&#27861;&#24459;&#26816;&#32034;&#29992;&#25143;&#30340;&#28508;&#22312;&#25628;&#32034;&#24847;&#22270;&#26356;&#21152;&#22797;&#26434;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#20116;&#31181;&#24847;&#22270;&#31867;&#22411;&#36827;&#34892;&#20998;&#31867;&#12290;&#35813;&#20998;&#31867;&#27861;&#32463;&#36807;&#24191;&#27867;&#30340;&#35780;&#20272;&#65292;&#25581;&#31034;&#20102;&#29992;&#25143;&#34892;&#20026;&#21644;&#28385;&#24847;&#24230;&#26041;&#38754;&#30340;&#26174;&#33879;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27861;&#24459;&#26696;&#20214;&#26816;&#32034;&#26159;&#19968;&#39033;&#29305;&#27530;&#30340;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#65292;&#20851;&#27880;&#30340;&#26159;&#27861;&#24459;&#26696;&#20214;&#25991;&#20214;&#12290;&#26681;&#25454;&#26816;&#32034;&#21040;&#30340;&#26696;&#20214;&#25991;&#20214;&#30340;&#19979;&#28216;&#20219;&#21153;&#21644;&#29992;&#25143;&#30340;&#20449;&#24687;&#38656;&#27714;&#65292;&#27861;&#24459;&#26696;&#20214;&#26816;&#32034;&#20013;&#30340;&#20449;&#24687;&#38656;&#27714;&#19982;&#32593;&#32476;&#25628;&#32034;&#21644;&#20256;&#32479;&#30340;&#33258;&#36866;&#24212;&#26816;&#32034;&#20219;&#21153;&#21487;&#33021;&#20250;&#26377;&#26174;&#33879;&#30340;&#21306;&#21035;&#12290;&#34429;&#28982;&#26377;&#20960;&#39033;&#30740;&#31350;&#26681;&#25454;&#25991;&#26412;&#30456;&#20284;&#24615;&#26469;&#26816;&#32034;&#27861;&#24459;&#26696;&#20214;&#65292;&#20294;&#20316;&#20026;&#26412;&#25991;&#25152;&#31034;&#65292;&#27861;&#24459;&#26816;&#32034;&#29992;&#25143;&#30340;&#28508;&#22312;&#25628;&#32034;&#24847;&#22270;&#26356;&#21152;&#22797;&#26434;&#65292;&#20294;&#22823;&#37096;&#20998;&#23578;&#26410;&#25506;&#32034;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27861;&#24459;&#26696;&#20214;&#26816;&#32034;&#30340;&#24847;&#22270;&#20998;&#31867;&#27861;&#12290;&#23427;&#30001;&#20116;&#31181;&#24847;&#22270;&#31867;&#22411;&#32452;&#25104;&#65292;&#26681;&#25454;&#19977;&#20010;&#26631;&#20934;&#36827;&#34892;&#20998;&#31867;&#65292;&#21363;&#25628;&#32034;&#29305;&#23450;&#26696;&#20363;&#65292;&#29305;&#24449;&#25551;&#36848;&#65292;&#22788;&#32602;&#65292;&#31243;&#24207;&#21644;&#21033;&#30410;&#12290;&#35813;&#20998;&#31867;&#27861;&#36890;&#36807;&#36879;&#26126;&#30340;&#26500;&#24314;&#21644;&#24191;&#27867;&#30340;&#35780;&#20272;&#65292;&#21253;&#25324;&#35775;&#35848;&#12289;&#32534;&#36753;&#29992;&#25143;&#30740;&#31350;&#21644;&#26597;&#35810;&#26085;&#24535;&#20998;&#26512;&#12290;&#36890;&#36807;&#23454;&#39564;&#23460;&#29992;&#25143;&#30740;&#31350;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#29992;&#25143;&#34892;&#20026;&#21644;&#28385;&#24847;&#24230;&#26041;&#38754;&#30340;&#26174;&#33879;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Legal case retrieval is a special Information Retrieval~(IR) task focusing on legal case documents. Depending on the downstream tasks of the retrieved case documents, users' information needs in legal case retrieval could be significantly different from those in Web search and traditional ad-hoc retrieval tasks. While there are several studies that retrieve legal cases based on text similarity, the underlying search intents of legal retrieval users, as shown in this paper, are more complicated than that yet mostly unexplored. To this end, we present a novel hierarchical intent taxonomy of legal case retrieval. It consists of five intent types categorized by three criteria, i.e., search for Particular Case(s), Characterization, Penalty, Procedure, and Interest. The taxonomy was constructed transparently and evaluated extensively through interviews, editorial user studies, and query log analysis. Through a laboratory user study, we reveal significant differences in user behavior and sati
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#22810;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#21457;&#29616;&#65292;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#20013;&#21024;&#38500;&#24207;&#21015;&#26411;&#23614;&#30340;&#39033;&#30446;&#26174;&#33879;&#38477;&#20302;&#20102;&#24615;&#33021;&#65292;&#32780;&#21024;&#38500;&#24207;&#21015;&#24320;&#22836;&#25110;&#20013;&#38388;&#30340;&#39033;&#30446;&#21017;&#27809;&#26377;&#26126;&#26174;&#24433;&#21709;&#12290;&#36825;&#19968;&#21457;&#29616;&#24378;&#35843;&#20102;&#32771;&#34385;&#35757;&#32451;&#25968;&#25454;&#20013;&#25200;&#21160;&#39033;&#30446;&#20301;&#32622;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#33021;&#25351;&#23548;&#26356;&#20855;&#40065;&#26834;&#24615;&#30340;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#30340;&#35774;&#35745;&#12290;</title><link>http://arxiv.org/abs/2307.13165</link><description>&lt;p&gt;
&#30740;&#31350;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#23545;&#35757;&#32451;&#25968;&#25454;&#25200;&#21160;&#30340;&#40065;&#26834;&#24615;&#65306;&#19968;&#39033;&#32463;&#39564;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Investigating the Robustness of Sequential Recommender Systems Against Training Data Perturbations: an Empirical Study. (arXiv:2307.13165v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13165
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#22810;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#21457;&#29616;&#65292;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#20013;&#21024;&#38500;&#24207;&#21015;&#26411;&#23614;&#30340;&#39033;&#30446;&#26174;&#33879;&#38477;&#20302;&#20102;&#24615;&#33021;&#65292;&#32780;&#21024;&#38500;&#24207;&#21015;&#24320;&#22836;&#25110;&#20013;&#38388;&#30340;&#39033;&#30446;&#21017;&#27809;&#26377;&#26126;&#26174;&#24433;&#21709;&#12290;&#36825;&#19968;&#21457;&#29616;&#24378;&#35843;&#20102;&#32771;&#34385;&#35757;&#32451;&#25968;&#25454;&#20013;&#25200;&#21160;&#39033;&#30446;&#20301;&#32622;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#33021;&#25351;&#23548;&#26356;&#20855;&#40065;&#26834;&#24615;&#30340;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#30340;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#34987;&#24191;&#27867;&#29992;&#20110;&#24314;&#27169;&#29992;&#25143;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#34892;&#20026;&#65292;&#28982;&#32780;&#20854;&#22312;&#38754;&#23545;&#35757;&#32451;&#25968;&#25454;&#25200;&#21160;&#26102;&#30340;&#40065;&#26834;&#24615;&#26159;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#26412;&#25991;&#36827;&#34892;&#20102;&#19968;&#39033;&#32463;&#39564;&#30740;&#31350;&#65292;&#25506;&#31350;&#20102;&#22312;&#26102;&#38388;&#39034;&#24207;&#24207;&#21015;&#20013;&#19981;&#21516;&#20301;&#32622;&#19978;&#21024;&#38500;&#39033;&#30446;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#27169;&#22411;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#65292;&#20351;&#29992;&#24402;&#19968;&#21270;&#25240;&#29616;&#32047;&#31215;&#22686;&#30410;&#65288;NDCG&#65289;&#25351;&#26631;&#21644;&#25490;&#21517;&#25935;&#24863;&#24230;&#21015;&#34920;&#65288;Rank Sensitivity List&#65289;&#25351;&#26631;&#26469;&#34913;&#37327;&#20854;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#21024;&#38500;&#24207;&#21015;&#26411;&#23614;&#30340;&#39033;&#30446;&#26174;&#33879;&#24433;&#21709;&#24615;&#33021;&#65292;NDCG&#19979;&#38477;&#39640;&#36798;60&#65285;&#65292;&#32780;&#21024;&#38500;&#24207;&#21015;&#24320;&#22836;&#25110;&#20013;&#38388;&#30340;&#39033;&#30446;&#27809;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;&#36825;&#20123;&#21457;&#29616;&#20984;&#26174;&#20102;&#32771;&#34385;&#35757;&#32451;&#25968;&#25454;&#20013;&#25200;&#21160;&#39033;&#30446;&#20301;&#32622;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#21487;&#25351;&#23548;&#26356;&#20855;&#40065;&#26834;&#24615;&#30340;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#30340;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequential Recommender Systems (SRSs) have been widely used to model user behavior over time, but their robustness in the face of perturbations to training data is a critical issue. In this paper, we conduct an empirical study to investigate the effects of removing items at different positions within a temporally ordered sequence. We evaluate two different SRS models on multiple datasets, measuring their performance using Normalized Discounted Cumulative Gain (NDCG) and Rank Sensitivity List metrics. Our results demonstrate that removing items at the end of the sequence significantly impacts performance, with NDCG decreasing up to 60\%, while removing items from the beginning or middle has no significant effect. These findings highlight the importance of considering the position of the perturbed items in the training data and shall inform the design of more robust SRSs.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20351;&#29992;&#22810;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#20174;&#33258;&#28982;&#35821;&#35328;&#20013;&#25552;&#21462;&#20998;&#23376;&#23646;&#24615;&#20449;&#24687;&#65292;&#36890;&#36807;&#25913;&#36827;&#25991;&#26412;&#26816;&#32034;&#21644;&#24341;&#20837;&#20998;&#23376;&#22270;&#25193;&#22686;&#31574;&#30053;&#31561;&#26041;&#27861;&#25552;&#39640;&#20102;&#23646;&#24615;&#39044;&#27979;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#30456;&#23545;&#20110;&#20165;&#22312;&#22270;&#27169;&#24577;&#19978;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#21462;&#24471;&#20102;+4.26%&#30340;AUROC&#22686;&#30410;&#21644;+1.54%&#30340;&#22686;&#30410;&#12290;</title><link>http://arxiv.org/abs/2307.12996</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#20174;&#33258;&#28982;&#35821;&#35328;&#20013;&#25552;&#21462;&#20998;&#23376;&#23646;&#24615;
&lt;/p&gt;
&lt;p&gt;
Extracting Molecular Properties from Natural Language with Multimodal Contrastive Learning. (arXiv:2307.12996v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12996
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20351;&#29992;&#22810;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#20174;&#33258;&#28982;&#35821;&#35328;&#20013;&#25552;&#21462;&#20998;&#23376;&#23646;&#24615;&#20449;&#24687;&#65292;&#36890;&#36807;&#25913;&#36827;&#25991;&#26412;&#26816;&#32034;&#21644;&#24341;&#20837;&#20998;&#23376;&#22270;&#25193;&#22686;&#31574;&#30053;&#31561;&#26041;&#27861;&#25552;&#39640;&#20102;&#23646;&#24615;&#39044;&#27979;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#30456;&#23545;&#20110;&#20165;&#22312;&#22270;&#27169;&#24577;&#19978;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#21462;&#24471;&#20102;+4.26%&#30340;AUROC&#22686;&#30410;&#21644;+1.54%&#30340;&#22686;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35745;&#31639;&#29983;&#29289;&#21270;&#23398;&#20013;&#65292;&#28145;&#24230;&#23398;&#20064;&#20256;&#32479;&#19978;&#19987;&#27880;&#20110;&#20998;&#23376;&#22270;&#31070;&#32463;&#34920;&#24449;&#65307;&#28982;&#32780;&#65292;&#26368;&#36817;&#35821;&#35328;&#27169;&#22411;&#30340;&#36827;&#23637;&#31361;&#26174;&#20102;&#25991;&#26412;&#20013;&#25152;&#32534;&#30721;&#30340;&#31185;&#23398;&#30693;&#35782;&#37327;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#20004;&#31181;&#27169;&#24577;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#23558;&#20998;&#23376;&#23646;&#24615;&#20449;&#24687;&#20174;&#33258;&#28982;&#35821;&#35328;&#36716;&#21270;&#20026;&#22270;&#34920;&#24449;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#23558;&#31070;&#32463;&#22270;&#34920;&#24449;&#19982;&#20854;&#29305;&#24449;&#30340;&#25991;&#26412;&#25551;&#36848;&#34920;&#24449;&#23545;&#40784;&#21518;&#65292;&#23646;&#24615;&#39044;&#27979;&#24615;&#33021;&#30340;&#25552;&#21319;&#12290;&#25105;&#20204;&#23454;&#29616;&#20102;&#31070;&#32463;&#30456;&#20851;&#24615;&#35780;&#20998;&#31574;&#30053;&#20197;&#25913;&#36827;&#25991;&#26412;&#26816;&#32034;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#21463;&#26377;&#26426;&#21453;&#24212;&#21551;&#21457;&#30340;&#26032;&#39062;&#21512;&#27861;&#20998;&#23376;&#22270;&#25193;&#22686;&#31574;&#30053;&#65292;&#24182;&#22312;&#19979;&#28216;&#30340;MoleculeNet&#23646;&#24615;&#20998;&#31867;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#24615;&#33021;&#30340;&#25913;&#21892;&#12290;&#19982;&#20165;&#22312;&#22270;&#27169;&#24577;&#19978;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#30456;&#27604;&#65292;&#25105;&#20204;&#21462;&#24471;&#20102;+4.26%&#30340;AUROC&#22686;&#30410;&#65292;&#24182;&#19982;&#26368;&#36817;&#25552;&#20986;&#30340;&#20998;&#23376;&#22270;/&#25991;&#26412;&#23545;&#27604;&#27169;&#22411;&#30456;&#27604;&#65292;&#21462;&#24471;&#20102;+1.54%&#30340;&#22686;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning in computational biochemistry has traditionally focused on molecular graphs neural representations; however, recent advances in language models highlight how much scientific knowledge is encoded in text. To bridge these two modalities, we investigate how molecular property information can be transferred from natural language to graph representations. We study property prediction performance gains after using contrastive learning to align neural graph representations with representations of textual descriptions of their characteristics. We implement neural relevance scoring strategies to improve text retrieval, introduce a novel chemically-valid molecular graph augmentation strategy inspired by organic reactions, and demonstrate improved performance on downstream MoleculeNet property classification tasks. We achieve a +4.26% AUROC gain versus models pre-trained on the graph modality alone, and a +1.54% gain compared to recently proposed molecular graph/text contrastively t
&lt;/p&gt;</description></item><item><title>RRAML&#26159;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#19982;&#29992;&#25143;&#25552;&#20379;&#30340;&#24222;&#22823;&#25968;&#25454;&#24211;&#20013;&#30340;&#25903;&#25345;&#20449;&#24687;&#30456;&#32467;&#21512;&#12290;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#36827;&#23637;&#65292;&#35813;&#26041;&#27861;&#25104;&#21151;&#35299;&#20915;&#20102;&#20960;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2307.12798</link><description>&lt;p&gt;
RRAML: &#24378;&#21270;&#26816;&#32034;&#22686;&#24378;&#30340;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
RRAML: Reinforced Retrieval Augmented Machine Learning. (arXiv:2307.12798v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12798
&lt;/p&gt;
&lt;p&gt;
RRAML&#26159;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#19982;&#29992;&#25143;&#25552;&#20379;&#30340;&#24222;&#22823;&#25968;&#25454;&#24211;&#20013;&#30340;&#25903;&#25345;&#20449;&#24687;&#30456;&#32467;&#21512;&#12290;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#36827;&#23637;&#65292;&#35813;&#26041;&#27861;&#25104;&#21151;&#35299;&#20915;&#20102;&#20960;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#24443;&#24213;&#25913;&#21464;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#30456;&#20851;&#39046;&#22495;&#65292;&#22312;&#29702;&#35299;&#12289;&#29983;&#25104;&#21644;&#25805;&#20316;&#20154;&#31867;&#35821;&#35328;&#26041;&#38754;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#22522;&#20110;API&#30340;&#25991;&#26412;&#25552;&#31034;&#25552;&#20132;&#26469;&#20351;&#29992;&#23427;&#20204;&#20250;&#23384;&#22312;&#19968;&#23450;&#30340;&#38480;&#21046;&#65292;&#21253;&#25324;&#19978;&#19979;&#25991;&#32422;&#26463;&#21644;&#22806;&#37096;&#36164;&#28304;&#30340;&#21487;&#29992;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;&#24378;&#21270;&#26816;&#32034;&#22686;&#24378;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;RRAML&#65289;&#12290;RRAML&#23558;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#19982;&#30001;&#19987;&#29992;&#26816;&#32034;&#22120;&#20174;&#29992;&#25143;&#25552;&#20379;&#30340;&#24222;&#22823;&#25968;&#25454;&#24211;&#20013;&#26816;&#32034;&#21040;&#30340;&#25903;&#25345;&#20449;&#24687;&#30456;&#32467;&#21512;&#12290;&#36890;&#36807;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#20960;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#23427;&#32469;&#36807;&#20102;&#35775;&#38382;LLM&#26799;&#24230;&#30340;&#38656;&#27714;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20943;&#36731;&#20102;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#37325;&#26032;&#35757;&#32451;LLMs&#30340;&#36127;&#25285;&#65292;&#22240;&#20026;&#30001;&#20110;&#23545;&#27169;&#22411;&#21644;&#21512;&#20316;&#30340;&#35775;&#38382;&#21463;&#38480;&#65292;&#36825;&#24448;&#24448;&#26159;&#19981;&#21487;&#34892;&#25110;&#19981;&#21487;&#33021;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of large language models (LLMs) has revolutionized machine learning and related fields, showcasing remarkable abilities in comprehending, generating, and manipulating human language. However, their conventional usage through API-based text prompt submissions imposes certain limitations in terms of context constraints and external source availability. To address these challenges, we propose a novel framework called Reinforced Retrieval Augmented Machine Learning (RRAML). RRAML integrates the reasoning capabilities of LLMs with supporting information retrieved by a purpose-built retriever from a vast user-provided database. By leveraging recent advancements in reinforcement learning, our method effectively addresses several critical challenges. Firstly, it circumvents the need for accessing LLM gradients. Secondly, our method alleviates the burden of retraining LLMs for specific tasks, as it is often impractical or impossible due to restricted access to the model and the co
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36719;&#25552;&#31034;&#35843;&#20248;&#26469;&#22686;&#24378;&#23494;&#38598;&#26816;&#32034;&#30340;&#26041;&#27861;&#65288;SPTAR&#65289;&#12290;&#36890;&#36807;&#20248;&#21270;&#20219;&#21153;&#29305;&#23450;&#30340;&#36719;&#25552;&#31034;&#24182;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;&#26410;&#26631;&#35760;&#30340;&#25991;&#26723;&#29983;&#25104;&#24369;&#26597;&#35810;&#65292;&#21487;&#20197;&#25552;&#39640;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#30340;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.08303</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#23494;&#38598;&#26816;&#32034;&#30340;&#36719;&#25552;&#31034;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
Soft Prompt Tuning for Augmenting Dense Retrieval with Large Language Models. (arXiv:2307.08303v1 [cs.IR] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08303
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36719;&#25552;&#31034;&#35843;&#20248;&#26469;&#22686;&#24378;&#23494;&#38598;&#26816;&#32034;&#30340;&#26041;&#27861;&#65288;SPTAR&#65289;&#12290;&#36890;&#36807;&#20248;&#21270;&#20219;&#21153;&#29305;&#23450;&#30340;&#36719;&#25552;&#31034;&#24182;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;&#26410;&#26631;&#35760;&#30340;&#25991;&#26723;&#29983;&#25104;&#24369;&#26597;&#35810;&#65292;&#21487;&#20197;&#25552;&#39640;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#30340;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23494;&#38598;&#26816;&#32034;&#65288;DR&#65289;&#23558;&#26597;&#35810;&#21644;&#25991;&#26723;&#36716;&#21270;&#20026;&#23494;&#38598;&#21521;&#37327;&#34920;&#31034;&#65292;&#24182;&#22312;&#21521;&#37327;&#31354;&#38388;&#20013;&#27979;&#37327;&#26597;&#35810;&#19982;&#25991;&#26723;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;DR&#30340;&#19968;&#20010;&#25361;&#25112;&#26159;&#32570;&#20047;&#39046;&#22495;&#29305;&#23450;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#34429;&#28982;DR&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#20174;&#22823;&#35268;&#27169;&#20844;&#20849;&#25968;&#25454;&#38598;&#65288;&#22914;MS MARCO&#65289;&#20013;&#23398;&#20064;&#65292;&#20294;&#35777;&#25454;&#34920;&#26126;&#65292;&#24182;&#38750;&#25152;&#26377;DR&#27169;&#22411;&#21644;&#39046;&#22495;&#37117;&#33021;&#21516;&#31561;&#21463;&#30410;&#20110;&#36801;&#31227;&#23398;&#20064;&#12290;&#26368;&#36817;&#65292;&#19968;&#20123;&#30740;&#31350;&#20154;&#21592;&#36716;&#21521;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#25913;&#36827;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#30340;DR&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20013;&#37319;&#29992;&#30340;&#30828;&#25552;&#31034;&#25110;&#20154;&#24037;&#32534;&#20889;&#30340;&#25552;&#31034;&#26080;&#27861;&#20445;&#35777;&#29983;&#25104;&#30340;&#24369;&#26597;&#35810;&#30340;&#36136;&#37327;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#22686;&#24378;DR&#30340;&#36719;&#25552;&#31034;&#35843;&#20248;&#65288;SPTAR&#65289;&#65306;&#23545;&#20110;&#27599;&#20010;&#20219;&#21153;&#65292;&#25105;&#20204;&#21033;&#29992;&#36719;&#25552;&#31034;&#35843;&#20248;&#22312;&#26377;&#38480;&#30340;&#30495;&#23454;&#25968;&#25454;&#19978;&#20248;&#21270;&#20219;&#21153;&#29305;&#23450;&#30340;&#36719;&#25552;&#31034;&#65292;&#28982;&#21518;&#29992;&#36825;&#20123;&#25552;&#31034;&#24341;&#23548;LLMs&#20026;&#26410;&#26631;&#35760;&#30340;&#25991;&#26723;&#26631;&#35760;&#24369;&#26597;&#35810;&#65292;&#20174;&#32780;&#24471;&#21040;&#36275;&#22815;&#30340;&#24369;&#25991;&#26723;-&#26597;&#35810;&#23545;&#26469;&#35757;&#32451;&#20219;&#21153;&#29305;&#23450;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dense retrieval (DR) converts queries and documents into dense embeddings and measures the similarity between queries and documents in vector space. One of the challenges in DR is the lack of domain-specific training data. While DR models can learn from large-scale public datasets like MS MARCO through transfer learning, evidence shows that not all DR models and domains can benefit from transfer learning equally. Recently, some researchers have resorted to large language models (LLMs) to improve the zero-shot and few-shot DR models. However, the hard prompts or human-written prompts utilized in these works cannot guarantee the good quality of generated weak queries. To tackle this, we propose soft prompt tuning for augmenting DR (SPTAR): For each task, we leverage soft prompt-tuning to optimize a task-specific soft prompt on limited ground truth data and then prompt the LLMs to tag unlabeled documents with weak queries, yielding enough weak document-query pairs to train task-specific d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#21521;&#37327;&#21387;&#32553;&#26041;&#27861;&#23616;&#37096;&#33258;&#36866;&#24212;&#37327;&#21270;(LVQ)&#65292;&#24182;&#22312;&#22522;&#20110;&#22270;&#30340;&#32034;&#24341;&#30340;&#20851;&#38190;&#20248;&#21270;&#19979;&#23454;&#29616;&#20943;&#23569;&#26377;&#25928;&#24102;&#23485;&#21516;&#26102;&#21551;&#29992;&#38543;&#26426;&#35775;&#38382;&#21451;&#22909;&#30340;&#24555;&#36895;&#30456;&#20284;&#24615;&#35745;&#31639;&#65292;&#20174;&#32780;&#22312;&#24615;&#33021;&#21644;&#20869;&#23384;&#21344;&#29992;&#26041;&#38754;&#21019;&#36896;&#20102;&#26032;&#30340;&#26368;&#20339;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.04759</link><description>&lt;p&gt;
&#21387;&#32553;&#32034;&#24341;&#23454;&#29616;&#30636;&#38388;&#30456;&#20284;&#24615;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Similarity search in the blink of an eye with compressed indices. (arXiv:2304.04759v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04759
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#21521;&#37327;&#21387;&#32553;&#26041;&#27861;&#23616;&#37096;&#33258;&#36866;&#24212;&#37327;&#21270;(LVQ)&#65292;&#24182;&#22312;&#22522;&#20110;&#22270;&#30340;&#32034;&#24341;&#30340;&#20851;&#38190;&#20248;&#21270;&#19979;&#23454;&#29616;&#20943;&#23569;&#26377;&#25928;&#24102;&#23485;&#21516;&#26102;&#21551;&#29992;&#38543;&#26426;&#35775;&#38382;&#21451;&#22909;&#30340;&#24555;&#36895;&#30456;&#20284;&#24615;&#35745;&#31639;&#65292;&#20174;&#32780;&#22312;&#24615;&#33021;&#21644;&#20869;&#23384;&#21344;&#29992;&#26041;&#38754;&#21019;&#36896;&#20102;&#26032;&#30340;&#26368;&#20339;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;&#25968;&#25454;&#20197;&#21521;&#37327;&#34920;&#31034;&#12290;&#22312;&#28023;&#37327;&#25968;&#25454;&#20013;&#23547;&#25214;&#19982;&#32473;&#23450;&#26597;&#35810;&#30456;&#20284;&#30340;&#21521;&#37327;&#26159;&#19968;&#39033;&#24191;&#27867;&#24212;&#29992;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#21019;&#24314;&#26356;&#24555;&#12289;&#26356;&#23567;&#30340;&#32034;&#24341;&#20197;&#36816;&#34892;&#36825;&#20123;&#25628;&#32034;&#30340;&#26032;&#25216;&#26415;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#21521;&#37327;&#21387;&#32553;&#26041;&#27861;&#65292;&#23616;&#37096;&#33258;&#36866;&#24212;&#37327;&#21270;(LVQ)&#65292;&#23427;&#21516;&#26102;&#20943;&#23569;&#20869;&#23384;&#21344;&#29992;&#21644;&#25913;&#21892;&#25628;&#32034;&#24615;&#33021;&#65292;&#23545;&#25628;&#32034;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#26368;&#23567;&#12290;LVQ&#34987;&#35774;&#35745;&#20026;&#19982;&#22522;&#20110;&#22270;&#30340;&#32034;&#24341;&#19968;&#36215;&#24037;&#20316;&#20197;&#23454;&#29616;&#20943;&#23569;&#26377;&#25928;&#24102;&#23485;&#21516;&#26102;&#21551;&#29992;&#38543;&#26426;&#35775;&#38382;&#21451;&#22909;&#30340;&#24555;&#36895;&#30456;&#20284;&#24615;&#35745;&#31639;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#29616;&#20195;&#25968;&#25454;&#20013;&#24515;&#31995;&#32479;&#20013;&#38024;&#23545;&#22522;&#20110;&#22270;&#30340;&#32034;&#24341;&#36827;&#34892;&#20851;&#38190;&#20248;&#21270;&#21518;&#65292;LVQ&#30340;&#24615;&#33021;&#21644;&#20869;&#23384;&#21344;&#29992;&#26041;&#38754;&#21019;&#36896;&#20102;&#26032;&#30340;&#26368;&#20339;&#34920;&#29616;&#12290;&#22312;&#22788;&#29702;&#25968;&#21313;&#20159;&#20010;&#21521;&#37327;&#26102;&#65292;LVQ&#36229;&#36807;&#31532;&#20108;&#20339;&#26041;&#26696;&#65306;
&lt;/p&gt;
&lt;p&gt;
Nowadays, data is represented by vectors. Retrieving those vectors, among millions and billions, that are similar to a given query is a ubiquitous problem of relevance for a wide range of applications. In this work, we present new techniques for creating faster and smaller indices to run these searches. To this end, we introduce a novel vector compression method, Locally-adaptive Vector Quantization (LVQ), that simultaneously reduces memory footprint and improves search performance, with minimal impact on search accuracy. LVQ is designed to work optimally in conjunction with graph-based indices, reducing their effective bandwidth while enabling random-access-friendly fast similarity computations. Our experimental results show that LVQ, combined with key optimizations for graph-based indices in modern datacenter systems, establishes the new state of the art in terms of performance and memory footprint. For billions of vectors, LVQ outcompetes the second-best alternatives: (1) in the low
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#12289;&#29616;&#20195;&#21644;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#21516;&#26102;&#22788;&#29702;&#22810;&#31181;&#35821;&#35328;&#65292;&#26500;&#24314;&#19968;&#20010;&#21333;&#19968;&#20027;&#39064;&#27169;&#22411;&#65292;&#24182;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#24773;&#24863;&#20998;&#26512;&#12290;&#36890;&#36807;&#23558;&#35813;&#27169;&#22411;&#24212;&#29992;&#20110;&#26377;&#26426;&#39135;&#21697;&#30340;&#25253;&#32440;&#25991;&#31456;&#21644;&#29992;&#25143;&#35780;&#35770;&#65292;&#25105;&#20204;&#21457;&#29616;&#20027;&#39064;&#22312;&#19981;&#21516;&#35821;&#35328;&#20013;&#21305;&#37197;&#65292;&#24182;&#33719;&#24471;&#20102;&#39640;&#27604;&#20363;&#30340;&#31283;&#23450;&#19988;&#19982;&#39046;&#22495;&#30456;&#20851;&#30340;&#20027;&#39064;&#12290;</title><link>http://arxiv.org/abs/2111.02259</link><description>&lt;p&gt;
&#19968;&#20010;&#31616;&#21333;&#30340;&#36328;&#35821;&#35328;&#35266;&#28857;&#25366;&#25496;&#30340;&#26696;&#20363;&#30740;&#31350;&#21644;&#36136;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Case Study and Qualitative Analysis of Simple Cross-Lingual Opinion Mining. (arXiv:2111.02259v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.02259
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#12289;&#29616;&#20195;&#21644;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#21516;&#26102;&#22788;&#29702;&#22810;&#31181;&#35821;&#35328;&#65292;&#26500;&#24314;&#19968;&#20010;&#21333;&#19968;&#20027;&#39064;&#27169;&#22411;&#65292;&#24182;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#24773;&#24863;&#20998;&#26512;&#12290;&#36890;&#36807;&#23558;&#35813;&#27169;&#22411;&#24212;&#29992;&#20110;&#26377;&#26426;&#39135;&#21697;&#30340;&#25253;&#32440;&#25991;&#31456;&#21644;&#29992;&#25143;&#35780;&#35770;&#65292;&#25105;&#20204;&#21457;&#29616;&#20027;&#39064;&#22312;&#19981;&#21516;&#35821;&#35328;&#20013;&#21305;&#37197;&#65292;&#24182;&#33719;&#24471;&#20102;&#39640;&#27604;&#20363;&#30340;&#31283;&#23450;&#19988;&#19982;&#39046;&#22495;&#30456;&#20851;&#30340;&#20027;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#29992;&#25143;&#29983;&#25104;&#20869;&#23481;&#28041;&#21450;&#22810;&#31181;&#35821;&#35328;&#65292;&#36825;&#22312;&#25216;&#26415;&#19978;&#20351;&#24471;&#36328;&#19981;&#21516;&#25991;&#21270;&#21644;&#22320;&#21306;&#27604;&#36739;&#35752;&#35770;&#20027;&#39064;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#23545;&#20110;&#20840;&#29699;&#21270;&#19990;&#30028;&#20013;&#30340;&#39046;&#22495;&#65292;&#22914;&#24066;&#22330;&#30740;&#31350;&#65292;&#20154;&#20204;&#21487;&#33021;&#23545;&#20135;&#21697;&#26377;&#19981;&#21516;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#12289;&#29616;&#20195;&#21644;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#26500;&#24314;&#33021;&#22815;&#21516;&#26102;&#35206;&#30422;&#22810;&#31181;&#35821;&#35328;&#30340;&#21333;&#19968;&#20027;&#39064;&#27169;&#22411;&#65292;&#24182;&#22522;&#20110;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#30340;&#24773;&#24863;&#20998;&#26512;&#12290;&#20026;&#20102;&#35777;&#26126;&#20854;&#21487;&#34892;&#24615;&#65292;&#25105;&#20204;&#23558;&#35813;&#27169;&#22411;&#24212;&#29992;&#20110;&#29305;&#23450;&#39046;&#22495;&#30340;&#25253;&#32440;&#25991;&#31456;&#21644;&#29992;&#25143;&#35780;&#35770;&#65292;&#21363;&#26377;&#26426;&#39135;&#21697;&#20135;&#21697;&#21644;&#30456;&#20851;&#28040;&#36153;&#34892;&#20026;&#12290;&#20027;&#39064;&#22312;&#19981;&#21516;&#35821;&#35328;&#20013;&#21305;&#37197;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#39640;&#27604;&#20363;&#30340;&#31283;&#23450;&#19988;&#30456;&#20851;&#39046;&#22495;&#30340;&#20027;&#39064;&#65292;&#20027;&#39064;&#19982;&#20854;&#30456;&#24212;&#25991;&#26412;&#20869;&#23481;&#20043;&#38388;&#23384;&#22312;&#26377;&#24847;&#20041;&#30340;&#20851;&#31995;&#65292;&#20197;&#21450;&#19968;&#20123;&#26032;&#39062;&#30340;
&lt;/p&gt;
&lt;p&gt;
User-generated content from social media is produced in many languages, making it technically challenging to compare the discussed themes from one domain across different cultures and regions. It is relevant for domains in a globalized world, such as market research, where people from two nations and markets might have different requirements for a product. We propose a simple, modern, and effective method for building a single topic model with sentiment analysis capable of covering multiple languages simultanteously, based on a pre-trained state-of-the-art deep neural network for natural language understanding. To demonstrate its feasibility, we apply the model to newspaper articles and user comments of a specific domain, i.e., organic food products and related consumption behavior. The themes match across languages. Additionally, we obtain an high proportion of stable and domain-relevant topics, a meaningful relation between topics and their respective textual contents, and an interpr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#22797;&#26434;&#30340;&#19987;&#23478;&#27880;&#35299;&#22312;&#31038;&#20132;&#23186;&#20307;&#20013;&#36827;&#34892;&#28040;&#36153;&#32773;&#20449;&#24565;&#38472;&#36848;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#65292;&#27604;&#36739;&#20102;&#32454;&#31890;&#24230;&#21644;&#25277;&#35937;&#31867;&#21035;&#30340;&#26631;&#31614;&#65292;&#24182;&#35828;&#26126;&#22797;&#26434;&#19987;&#23478;&#27880;&#35299;&#22312;&#39640;&#24230;&#29305;&#23450;&#30340;&#24847;&#35265;&#25366;&#25496;&#20013;&#30340;&#28508;&#22312;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2106.15498</link><description>&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#20013;&#28040;&#36153;&#32773;&#20449;&#24565;&#38472;&#36848;&#30340;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Classification of Consumer Belief Statements From Social Media. (arXiv:2106.15498v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.15498
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#22797;&#26434;&#30340;&#19987;&#23478;&#27880;&#35299;&#22312;&#31038;&#20132;&#23186;&#20307;&#20013;&#36827;&#34892;&#28040;&#36153;&#32773;&#20449;&#24565;&#38472;&#36848;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#65292;&#27604;&#36739;&#20102;&#32454;&#31890;&#24230;&#21644;&#25277;&#35937;&#31867;&#21035;&#30340;&#26631;&#31614;&#65292;&#24182;&#35828;&#26126;&#22797;&#26434;&#19987;&#23478;&#27880;&#35299;&#22312;&#39640;&#24230;&#29305;&#23450;&#30340;&#24847;&#35265;&#25366;&#25496;&#20013;&#30340;&#28508;&#22312;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#25552;&#20379;&#20102;&#22823;&#37327;&#20449;&#24687;&#65292;&#21487;&#20197;&#36827;&#34892;&#24066;&#22330;&#35843;&#30740;&#65292;&#20197;&#28385;&#36275;&#23458;&#25143;&#30340;&#38656;&#27714;&#12290;&#30740;&#31350;&#20154;&#21592;&#36890;&#24120;&#36890;&#36807;&#25910;&#38598;&#21644;&#20998;&#31867;&#29992;&#25143;&#29983;&#25104;&#30340;&#20869;&#23481;&#65292;&#26500;&#24314;&#22797;&#26434;&#32454;&#31890;&#24230;&#30340;&#31867;&#21035;&#32467;&#26500;&#26469;&#36827;&#34892;&#24066;&#22330;&#35843;&#30740;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#25968;&#25454;&#37327;&#36739;&#23569;&#19988;&#27880;&#35299;&#22797;&#26434;&#12290;&#22914;&#20309;&#25104;&#21151;&#21033;&#29992;&#36825;&#20123;&#25968;&#25454;&#36827;&#34892;&#20998;&#31867;&#20173;&#19981;&#23436;&#20840;&#28165;&#26970;&#12290;&#26412;&#30740;&#31350;&#32771;&#23519;&#20102;&#24403;&#19987;&#23478;&#27880;&#35299;&#34987;&#24212;&#29992;&#20110;a) &#35768;&#22810;&#32454;&#31890;&#24230;&#31867;&#21035;&#21644;b) &#23569;&#25968;&#25277;&#35937;&#31867;&#21035;&#26102;&#30340;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;&#23545;&#20110;&#22330;&#26223;b)&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#39046;&#22495;&#19987;&#23478;&#32473;&#20986;&#30340;&#25277;&#35937;&#31867;&#21035;&#26631;&#31614;&#65288;&#22522;&#20934;&#65289;&#21644;&#33258;&#21160;&#20998;&#23618;&#32858;&#31867;&#32473;&#20986;&#30340;&#25277;&#35937;&#31867;&#21035;&#26631;&#31614;&#12290;&#25105;&#20204;&#23558;&#20854;&#19982;&#21478;&#19968;&#22522;&#20934;&#36827;&#34892;&#27604;&#36739;&#65292;&#35813;&#22522;&#20934;&#20351;&#29992;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#32858;&#31867;&#26041;&#27861;&#32473;&#20986;&#25972;&#20010;&#31867;&#21035;&#32467;&#26500;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#35813;&#30740;&#31350;&#21487;&#20197;&#20316;&#20026;&#22797;&#26434;&#19987;&#23478;&#27880;&#35299;&#22914;&#20309;&#22312;&#39640;&#24230;&#29305;&#23450;&#30340;&#24847;&#35265;&#25366;&#25496;&#20013;&#21457;&#25381;&#28508;&#22312;&#20248;&#21183;&#65292;&#24182;&#20197;&#26368;&#20248;&#21270;&#30340;&#26041;&#24335;&#21033;&#29992;&#30340;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Social media offer plenty of information to perform market research in order to meet the requirements of customers. One way how this research is conducted is that a domain expert gathers and categorizes user-generated content into a complex and fine-grained class structure. In many of such cases, little data meets complex annotations. It is not yet fully understood how this can be leveraged successfully for classification. We examine the classification accuracy of expert labels when used with a) many fine-grained classes and b) few abstract classes. For scenario b) we compare abstract class labels given by the domain expert as baseline and by automatic hierarchical clustering. We compare this to another baseline where the entire class structure is given by a completely unsupervised clustering approach. By doing so, this work can serve as an example of how complex expert annotations are potentially beneficial and can be utilized in the most optimal way for opinion mining in highly speci
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20934;&#21017;&#30340;&#38750;&#37319;&#26679;&#23398;&#20064;&#26694;&#26550;&#65292;&#21629;&#21517;&#20026;CHCF&#65292;&#29992;&#20110;&#22810;&#34892;&#20026;&#38544;&#24335;&#25512;&#33616;&#30340;&#24322;&#26500;&#21327;&#21516;&#36807;&#28388;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#19978;&#38480;&#21644;&#19979;&#38480;&#38408;&#20540;&#26469;&#25351;&#31034;&#36873;&#25321;&#26631;&#20934;&#65292;&#24182;&#25351;&#23548;&#29992;&#25143;&#20559;&#22909;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2105.11876</link><description>&lt;p&gt;
&#22522;&#20110;&#20934;&#21017;&#30340;&#22810;&#34892;&#20026;&#38544;&#24335;&#25512;&#33616;&#30340;&#24322;&#26500;&#21327;&#21516;&#36807;&#28388;
&lt;/p&gt;
&lt;p&gt;
Criterion-based Heterogeneous Collaborative Filtering for Multi-behavior Implicit Recommendation. (arXiv:2105.11876v3 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2105.11876
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20934;&#21017;&#30340;&#38750;&#37319;&#26679;&#23398;&#20064;&#26694;&#26550;&#65292;&#21629;&#21517;&#20026;CHCF&#65292;&#29992;&#20110;&#22810;&#34892;&#20026;&#38544;&#24335;&#25512;&#33616;&#30340;&#24322;&#26500;&#21327;&#21516;&#36807;&#28388;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#19978;&#38480;&#21644;&#19979;&#38480;&#38408;&#20540;&#26469;&#25351;&#31034;&#36873;&#25321;&#26631;&#20934;&#65292;&#24182;&#25351;&#23548;&#29992;&#25143;&#20559;&#22909;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22810;&#23186;&#20307;&#20449;&#24687;&#31995;&#32479;&#20013;&#30340;&#20114;&#21160;&#34892;&#20026;&#21576;&#29190;&#28856;&#24335;&#22686;&#38271;&#65292;&#21033;&#29992;&#26469;&#33258;&#21508;&#31181;&#36741;&#21161;&#34892;&#20026;&#65288;&#22914;&#25552;&#31034;&#21644;&#25910;&#34255;&#65289;&#30340;&#25968;&#25454;&#65292;&#22810;&#34892;&#20026;&#25512;&#33616;&#31995;&#32479;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#22312;&#21508;&#31181;&#22810;&#34892;&#20026;&#25512;&#33616;&#26041;&#27861;&#20013;&#65292;&#38750;&#37319;&#26679;&#26041;&#27861;&#34920;&#29616;&#20248;&#20110;&#36127;&#37319;&#26679;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#20108;&#36827;&#21046;&#22238;&#24402;&#30340;&#38750;&#37319;&#26679;&#26041;&#27861;&#36890;&#24120;&#24573;&#30053;&#20102;&#20004;&#28857;&#35266;&#23519;&#65306;&#65288;1&#65289;&#29992;&#25143;&#23545;&#19981;&#21516;&#39033;&#30446;&#20855;&#26377;&#19981;&#21516;&#30340;&#20559;&#22909;&#24378;&#24230;&#65292;&#22240;&#27492;&#19981;&#33021;&#31616;&#21333;&#36890;&#36807;&#20108;&#36827;&#21046;&#38544;&#24335;&#25968;&#25454;&#26469;&#34913;&#37327;&#65307;&#65288;2&#65289;&#22810;&#20010;&#34892;&#20026;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#23545;&#19981;&#21516;&#30340;&#29992;&#25143;&#21644;&#39033;&#30446;&#26159;&#19981;&#21516;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38750;&#37319;&#26679;&#23398;&#20064;&#26694;&#26550;&#65292;&#21629;&#21517;&#20026;&#22522;&#20110;&#20934;&#21017;&#30340;&#24322;&#26500;&#21327;&#21516;&#36807;&#28388;&#65288;CHCF&#65289;&#12290;CHCF&#24341;&#20837;&#20102;&#19978;&#38480;&#21644;&#19979;&#38480;&#38408;&#20540;&#26469;&#25351;&#31034;&#36873;&#25321;&#26631;&#20934;&#65292;&#24182;&#25351;&#23548;&#29992;&#25143;&#20559;&#22909;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have witnessed the explosive growth of interaction behaviors in multimedia information systems, where multi-behavior recommender systems have received increasing attention by leveraging data from various auxiliary behaviors such as tip and collect. Among various multi-behavior recommendation methods, non-sampling methods have shown superiority over negative sampling methods. However, two observations are usually ignored in existing state-of-the-art non-sampling methods based on binary regression: (1) users have different preference strengths for different items, so they cannot be measured simply by binary implicit data; (2) the dependency across multiple behaviors varies for different users and items. To tackle the above issue, we propose a novel non-sampling learning framework named Criterion-guided Heterogeneous Collaborative Filtering (CHCF). CHCF introduces both upper and lower thresholds to indicate selection criteria, which will guide user preference learning. Beside
&lt;/p&gt;</description></item></channel></rss>