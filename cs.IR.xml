<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>GMOCAT&#26159;&#19968;&#31181;&#29992;&#20110;&#35745;&#31639;&#26426;&#21270;&#33258;&#36866;&#24212;&#27979;&#35797;&#30340;&#22270;&#22686;&#24378;&#22810;&#30446;&#26631;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#36136;&#37327;&#12289;&#22810;&#26679;&#24615;&#21644;&#26032;&#39062;&#24615;&#19977;&#20010;&#30446;&#26631;&#65292;&#25552;&#39640;&#20102;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#24182;&#32771;&#34385;&#20102;&#27010;&#24565;&#22810;&#26679;&#24615;&#21644;&#38382;&#39064;&#26292;&#38706;&#25511;&#21046;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.07477</link><description>&lt;p&gt;
GMOCAT:&#19968;&#31181;&#29992;&#20110;&#35745;&#31639;&#26426;&#21270;&#33258;&#36866;&#24212;&#27979;&#35797;&#30340;&#22270;&#22686;&#24378;&#22810;&#30446;&#26631;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
GMOCAT: A Graph-Enhanced Multi-Objective Method for Computerized Adaptive Testing. (arXiv:2310.07477v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07477
&lt;/p&gt;
&lt;p&gt;
GMOCAT&#26159;&#19968;&#31181;&#29992;&#20110;&#35745;&#31639;&#26426;&#21270;&#33258;&#36866;&#24212;&#27979;&#35797;&#30340;&#22270;&#22686;&#24378;&#22810;&#30446;&#26631;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#36136;&#37327;&#12289;&#22810;&#26679;&#24615;&#21644;&#26032;&#39062;&#24615;&#19977;&#20010;&#30446;&#26631;&#65292;&#25552;&#39640;&#20102;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#24182;&#32771;&#34385;&#20102;&#27010;&#24565;&#22810;&#26679;&#24615;&#21644;&#38382;&#39064;&#26292;&#38706;&#25511;&#21046;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#26426;&#21270;&#33258;&#36866;&#24212;&#27979;&#35797;&#65288;CAT&#65289;&#26159;&#25351;&#26681;&#25454;&#23398;&#29983;&#30340;&#21382;&#21490;&#20316;&#31572;&#35760;&#24405;&#65292;&#26681;&#25454;&#20182;&#20204;&#30340;&#33021;&#21147;&#26234;&#33021;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#38382;&#39064;&#30340;&#22312;&#32447;&#31995;&#32479;&#12290;&#22823;&#22810;&#25968;CAT&#26041;&#27861;&#21482;&#20851;&#27880;&#20934;&#30830;&#39044;&#27979;&#23398;&#29983;&#33021;&#21147;&#30340;&#36136;&#37327;&#30446;&#26631;&#65292;&#32780;&#24573;&#35270;&#20102;&#27010;&#24565;&#22810;&#26679;&#24615;&#25110;&#38382;&#39064;&#26292;&#38706;&#25511;&#21046;&#65292;&#36825;&#22312;&#30830;&#20445;CAT&#30340;&#24615;&#33021;&#21644;&#26377;&#25928;&#24615;&#26041;&#38754;&#26159;&#37325;&#35201;&#32771;&#34385;&#22240;&#32032;&#12290;&#27492;&#22806;&#65292;&#23398;&#29983;&#30340;&#20316;&#31572;&#35760;&#24405;&#21253;&#21547;&#38382;&#39064;&#21644;&#30693;&#35782;&#27010;&#24565;&#20043;&#38388;&#26377;&#20215;&#20540;&#30340;&#20851;&#31995;&#20449;&#24687;&#12290;&#20043;&#21069;&#30340;&#26041;&#27861;&#24573;&#35270;&#20102;&#36825;&#31181;&#20851;&#31995;&#20449;&#24687;&#65292;&#23548;&#33268;&#36873;&#25321;&#27425;&#20248;&#30340;&#27979;&#35797;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;CAT&#30340;&#22270;&#22686;&#24378;&#22810;&#30446;&#26631;&#26041;&#27861;&#65288;GMOCAT&#65289;&#12290;&#39318;&#20808;&#65292;&#22312;CAT&#30340;&#26631;&#37327;&#21270;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#20013;&#24341;&#20837;&#20102;&#19977;&#20010;&#30446;&#26631;&#65292;&#20998;&#21035;&#23545;&#24212;&#20110;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12289;&#22686;&#21152;&#22810;&#26679;&#24615;&#21644;&#26032;&#39062;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Computerized Adaptive Testing(CAT) refers to an online system that adaptively selects the best-suited question for students with various abilities based on their historical response records. Most CAT methods only focus on the quality objective of predicting the student ability accurately, but neglect concept diversity or question exposure control, which are important considerations in ensuring the performance and validity of CAT. Besides, the students' response records contain valuable relational information between questions and knowledge concepts. The previous methods ignore this relational information, resulting in the selection of sub-optimal test questions. To address these challenges, we propose a Graph-Enhanced Multi-Objective method for CAT (GMOCAT). Firstly, three objectives, namely quality, diversity and novelty, are introduced into the Scalarized Multi-Objective Reinforcement Learning framework of CAT, which respectively correspond to improving the prediction accuracy, incre
&lt;/p&gt;</description></item><item><title>&#39318;&#27425;&#23545;&#24503;&#22269;&#20449;&#24687;&#26816;&#32034;&#31038;&#21306;&#36827;&#34892;&#31185;&#23398;&#35745;&#37327;&#20998;&#26512;&#65292;&#29983;&#25104;&#20102;401&#31687;&#26368;&#36817;&#30340;&#20449;&#24687;&#26816;&#32034;&#30456;&#20851;&#35770;&#25991;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;&#26426;&#26500;&#21644;&#30740;&#31350;&#20154;&#21592;&#32423;&#21035;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2310.07346</link><description>&lt;p&gt;
2020-2023&#24180;&#24503;&#22269;&#20449;&#24687;&#26816;&#32034;&#31038;&#21306;&#30340;&#31185;&#23398;&#35745;&#37327;&#20998;&#26512;&#21021;&#27493;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
Preliminary Results of a Scientometric Analysis of the German Information Retrieval Community 2020-2023. (arXiv:2310.07346v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07346
&lt;/p&gt;
&lt;p&gt;
&#39318;&#27425;&#23545;&#24503;&#22269;&#20449;&#24687;&#26816;&#32034;&#31038;&#21306;&#36827;&#34892;&#31185;&#23398;&#35745;&#37327;&#20998;&#26512;&#65292;&#29983;&#25104;&#20102;401&#31687;&#26368;&#36817;&#30340;&#20449;&#24687;&#26816;&#32034;&#30456;&#20851;&#35770;&#25991;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;&#26426;&#26500;&#21644;&#30740;&#31350;&#20154;&#21592;&#32423;&#21035;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24503;&#22269;&#20449;&#24687;&#26816;&#32034;&#31038;&#21306;&#20998;&#24067;&#22312;&#20449;&#24687;&#31185;&#23398;&#21644;&#35745;&#31639;&#26426;&#31185;&#23398;&#20004;&#20010;&#23376;&#39046;&#22495;&#20013;&#12290;&#30446;&#21069;&#27809;&#26377;&#30740;&#31350;&#23545;&#36825;&#20004;&#20010;&#31038;&#21306;&#36827;&#34892;&#31185;&#23398;&#35745;&#37327;&#20998;&#26512;&#12290;&#29616;&#26377;&#30740;&#31350;&#21482;&#20851;&#27880;&#20449;&#24687;&#31185;&#23398;&#26041;&#38754;&#30340;&#31038;&#21306;&#12290;&#25105;&#20204;&#29983;&#25104;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;401&#31687;&#26368;&#36817;&#30340;&#20449;&#24687;&#26816;&#32034;&#30456;&#20851;&#35770;&#25991;&#65292;&#20174;&#20027;&#35201;&#30001;&#35745;&#31639;&#26426;&#31185;&#23398;&#32972;&#26223;&#30340;&#20845;&#20010;&#26680;&#24515;&#20449;&#24687;&#26816;&#32034;&#20250;&#35758;&#20013;&#25552;&#21462;&#12290;&#25105;&#20204;&#22312;&#26426;&#26500;&#21644;&#30740;&#31350;&#20154;&#21592;&#32423;&#21035;&#23545;&#35813;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;&#35813;&#25968;&#25454;&#38598;&#24050;&#20844;&#24320;&#21457;&#24067;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#19968;&#20010;&#26144;&#23556;&#20351;&#29992;&#26696;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
The German Information Retrieval community is located in two different sub-fields: Information and computer science. There are no current studies that investigate these communities on a scientometric level. Available studies only focus on the information scientific part of the community. We generated a data set of 401 recent IR-related publications extracted from six core IR conferences from a mainly computer scientific background. We analyze this data set at the institutional and researcher level. The data set is publicly released, and we also demonstrate a mapping use case.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23436;&#20840;&#19982;&#26412;&#22320;&#29615;&#22659;&#26080;&#20851;&#30340;&#22522;&#20110;&#20250;&#35805;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;&#35757;&#32451;&#27169;&#22411;&#24182;&#32467;&#21512;&#29305;&#24449;&#36873;&#21462;&#21644;&#37325;&#26032;&#25490;&#24207;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#19981;&#21516;&#30340;&#26412;&#22320;&#29615;&#22659;&#19979;&#34920;&#29616;&#20986;&#19968;&#33268;&#30340;&#33391;&#22909;&#24615;&#33021;&#65292;&#29978;&#33267;&#22312;&#35757;&#32451;&#20013;&#32467;&#21512;&#20854;&#20182;&#26412;&#22320;&#29615;&#22659;&#30340;&#25968;&#25454;&#26102;&#25928;&#26524;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2310.07281</link><description>&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#35757;&#32451;&#27169;&#22411;&#23454;&#29616;&#23436;&#20840;&#19982;&#26412;&#22320;&#29615;&#22659;&#26080;&#20851;&#30340;&#22522;&#20110;&#20250;&#35805;&#30340;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
A Completely Locale-independent Session-based Recommender System by Leveraging Trained Model. (arXiv:2310.07281v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07281
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23436;&#20840;&#19982;&#26412;&#22320;&#29615;&#22659;&#26080;&#20851;&#30340;&#22522;&#20110;&#20250;&#35805;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;&#35757;&#32451;&#27169;&#22411;&#24182;&#32467;&#21512;&#29305;&#24449;&#36873;&#21462;&#21644;&#37325;&#26032;&#25490;&#24207;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#19981;&#21516;&#30340;&#26412;&#22320;&#29615;&#22659;&#19979;&#34920;&#29616;&#20986;&#19968;&#33268;&#30340;&#33391;&#22909;&#24615;&#33021;&#65292;&#29978;&#33267;&#22312;&#35757;&#32451;&#20013;&#32467;&#21512;&#20854;&#20182;&#26412;&#22320;&#29615;&#22659;&#30340;&#25968;&#25454;&#26102;&#25928;&#26524;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#65292;&#22312;KDD Cup 2023&#25361;&#25112;&#20219;&#21153;2&#65288;&#38024;&#23545;&#35821;&#35328;/&#26412;&#22320;&#29615;&#22659;&#25512;&#33616;&#19979;&#19968;&#27454;&#20135;&#21697;&#65289;&#20013;&#33719;&#24471;&#20102;&#31532;10&#21517;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#20004;&#20010;&#27493;&#39588;&#65306;&#65288;i&#65289;&#22522;&#20110;&#20849;&#21516;&#35775;&#38382;&#35782;&#21035;&#20505;&#36873;&#39033;&#38598;&#65292;&#21644;&#65288;ii&#65289;&#20351;&#29992;&#21253;&#25324;&#22522;&#20110;&#20250;&#35805;&#30340;&#29305;&#24449;&#21644;&#20135;&#21697;&#30456;&#20284;&#24615;&#22312;&#20869;&#30340;&#19982;&#26412;&#22320;&#29615;&#22659;&#26080;&#20851;&#30340;&#29305;&#24449;&#65292;&#20351;&#29992;LightGBM&#23545;&#36825;&#20123;&#39033;&#36827;&#34892;&#37325;&#26032;&#25490;&#24207;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#19981;&#21516;&#27979;&#35797;&#26412;&#22320;&#29615;&#22659;&#30456;&#27604;&#65292;&#19982;&#26412;&#22320;&#29615;&#22659;&#26080;&#20851;&#30340;&#27169;&#22411;&#34920;&#29616;&#20986;&#19968;&#33268;&#30340;&#33391;&#22909;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#35757;&#32451;&#20013;&#32467;&#21512;&#20854;&#20182;&#26412;&#22320;&#29615;&#22659;&#30340;&#25968;&#25454;&#26102;&#25928;&#26524;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a solution that won the 10th prize in the KDD Cup 2023 Challenge Task 2 (Next Product Recommendation for Underrepresented Languages/Locales). Our approach involves two steps: (i) Identify candidate item sets based on co-visitation, and (ii) Re-ranking the items using LightGBM with locale-independent features, including session-based features and product similarity. The experiment demonstrated that the locale-independent model performed consistently well across different test locales, and performed even better when incorporating data from other locales into the training.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#25968;&#25454;&#31232;&#32570;&#30340;&#20154;&#22312;&#24490;&#29615;&#29615;&#22659;&#65288;&#22914;&#29983;&#27963;&#23454;&#39564;&#23460;&#65289;&#20013;&#39564;&#35777;&#30001;&#28857;&#20987;&#27169;&#22411;&#20135;&#29983;&#30340;&#21512;&#25104;&#20351;&#29992;&#25968;&#25454;&#30340;&#35780;&#20272;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.07142</link><description>&lt;p&gt;
&#22312;&#29983;&#27963;&#23454;&#39564;&#23460;&#29615;&#22659;&#20013;&#39564;&#35777;&#21512;&#25104;&#20351;&#29992;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Validating Synthetic Usage Data in Living Lab Environments. (arXiv:2310.07142v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07142
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#25968;&#25454;&#31232;&#32570;&#30340;&#20154;&#22312;&#24490;&#29615;&#29615;&#22659;&#65288;&#22914;&#29983;&#27963;&#23454;&#39564;&#23460;&#65289;&#20013;&#39564;&#35777;&#30001;&#28857;&#20987;&#27169;&#22411;&#20135;&#29983;&#30340;&#21512;&#25104;&#20351;&#29992;&#25968;&#25454;&#30340;&#35780;&#20272;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27809;&#26377;&#32534;&#36753;&#30456;&#20851;&#24615;&#21028;&#26029;&#30340;&#24773;&#20917;&#19979;&#35780;&#20272;&#26816;&#32034;&#24615;&#33021;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#20294;&#26159;&#21487;&#20197;&#20351;&#29992;&#29992;&#25143;&#20132;&#20114;&#20316;&#20026;&#30456;&#20851;&#20449;&#21495;&#12290;&#29983;&#27963;&#23454;&#39564;&#23460;&#20026;&#23567;&#35268;&#27169;&#24179;&#21488;&#25552;&#20379;&#20102;&#19968;&#31181;&#20351;&#29992;&#30495;&#23454;&#29992;&#25143;&#39564;&#35777;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#30340;&#26041;&#24335;&#12290;&#22914;&#26524;&#26377;&#36275;&#22815;&#22810;&#30340;&#29992;&#25143;&#20132;&#20114;&#25968;&#25454;&#65292;&#21487;&#20197;&#20174;&#21382;&#21490;&#20250;&#35805;&#20013;&#23545;&#28857;&#20987;&#27169;&#22411;&#36827;&#34892;&#21442;&#25968;&#21270;&#65292;&#20197;&#22312;&#23558;&#29992;&#25143;&#26292;&#38706;&#20110;&#23454;&#39564;&#25490;&#21517;&#20043;&#21069;&#35780;&#20272;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#22312;&#29983;&#27963;&#23454;&#39564;&#23460;&#20013;&#65292;&#20132;&#20114;&#25968;&#25454;&#24456;&#31232;&#30095;&#65292;&#20851;&#20110;&#24403;&#28857;&#20987;&#25968;&#25454;&#25968;&#37327;&#36739;&#23569;&#26102;&#22914;&#20309;&#39564;&#35777;&#21487;&#38752;&#30340;&#29992;&#25143;&#27169;&#25311;&#26041;&#38754;&#30340;&#30740;&#31350;&#24456;&#23569;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#39564;&#35777;&#22312;&#25968;&#25454;&#31232;&#30095;&#30340;&#20154;&#22312;&#24490;&#29615;&#29615;&#22659;&#65288;&#22914;&#29983;&#27963;&#23454;&#39564;&#23460;&#65289;&#20013;&#30001;&#28857;&#20987;&#27169;&#22411;&#20135;&#29983;&#30340;&#21512;&#25104;&#20351;&#29992;&#25968;&#25454;&#30340;&#35780;&#20272;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#28857;&#20987;&#27169;&#22411;&#23545;&#31995;&#32479;&#25490;&#21517;&#19982;&#24050;&#30693;&#30456;&#23545;&#24615;&#33021;&#30340;&#21442;&#32771;&#25490;&#21517;&#20043;&#38388;&#30340;&#20272;&#35745;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#28857;&#20987;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluating retrieval performance without editorial relevance judgments is challenging, but instead, user interactions can be used as relevance signals. Living labs offer a way for small-scale platforms to validate information retrieval systems with real users. If enough user interaction data are available, click models can be parameterized from historical sessions to evaluate systems before exposing users to experimental rankings. However, interaction data are sparse in living labs, and little is studied about how click models can be validated for reliable user simulations when click data are available in moderate amounts.  This work introduces an evaluation approach for validating synthetic usage data generated by click models in data-sparse human-in-the-loop environments like living labs. We ground our methodology on the click model's estimates about a system ranking compared to a reference ranking for which the relative performance is known. Our experiments compare different click m
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#20041;&#21305;&#37197;&#21644;&#36127;&#26631;&#31614;&#37319;&#26679;&#30340;&#22810;&#26631;&#31614;&#20998;&#31867;&#27169;&#22411; AE-smnsMLC&#65292;&#29992;&#20110;&#35299;&#20915;&#20135;&#21697;&#23646;&#24615;&#20540;&#25552;&#21462;&#30340;&#38382;&#39064;&#12290;&#35813;&#27169;&#22411;&#23558;&#23646;&#24615;&#20540;&#25552;&#21462;&#20219;&#21153;&#36716;&#21270;&#20026;&#22810;&#26631;&#31614;&#20998;&#31867;&#20219;&#21153;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#21482;&#26377;&#23646;&#24615;&#20540;&#27880;&#37322;&#30340;&#23454;&#38469;&#22330;&#26223;&#65292;&#32780;&#26080;&#38656;&#23646;&#24615;&#20540;&#30340;&#20301;&#32622;&#20449;&#24687;&#27880;&#37322;&#12290;</title><link>http://arxiv.org/abs/2310.07137</link><description>&lt;p&gt;
AE-smnsMLC&#65306;&#22522;&#20110;&#35821;&#20041;&#21305;&#37197;&#21644;&#36127;&#26631;&#31614;&#37319;&#26679;&#30340;&#20135;&#21697;&#23646;&#24615;&#20540;&#25552;&#21462;&#30340;&#22810;&#26631;&#31614;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
AE-smnsMLC: Multi-Label Classification with Semantic Matching and Negative Label Sampling for Product Attribute Value Extraction. (arXiv:2310.07137v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07137
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#20041;&#21305;&#37197;&#21644;&#36127;&#26631;&#31614;&#37319;&#26679;&#30340;&#22810;&#26631;&#31614;&#20998;&#31867;&#27169;&#22411; AE-smnsMLC&#65292;&#29992;&#20110;&#35299;&#20915;&#20135;&#21697;&#23646;&#24615;&#20540;&#25552;&#21462;&#30340;&#38382;&#39064;&#12290;&#35813;&#27169;&#22411;&#23558;&#23646;&#24615;&#20540;&#25552;&#21462;&#20219;&#21153;&#36716;&#21270;&#20026;&#22810;&#26631;&#31614;&#20998;&#31867;&#20219;&#21153;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#21482;&#26377;&#23646;&#24615;&#20540;&#27880;&#37322;&#30340;&#23454;&#38469;&#22330;&#26223;&#65292;&#32780;&#26080;&#38656;&#23646;&#24615;&#20540;&#30340;&#20301;&#32622;&#20449;&#24687;&#27880;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20135;&#21697;&#23646;&#24615;&#20540;&#25552;&#21462;&#22312;&#30005;&#23376;&#21830;&#21153;&#31561;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#22914;&#20135;&#21697;&#25628;&#32034;&#21644;&#25512;&#33616;&#12290;&#20043;&#21069;&#30340;&#26041;&#27861;&#23558;&#20854;&#35270;&#20026;&#38656;&#35201;&#26356;&#22810;&#27880;&#37322;&#26469;&#26631;&#27880;&#20135;&#21697;&#25991;&#26412;&#20013;&#20540;&#30340;&#24207;&#21015;&#26631;&#35760;&#20219;&#21153;&#12290;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#65292;&#20854;&#20013;&#27599;&#20010;&#20135;&#21697;&#21482;&#26377;&#23646;&#24615;&#20540;&#30340;&#24369;&#26631;&#27880;&#65292;&#32780;&#27809;&#26377;&#23427;&#20204;&#30340;&#20301;&#32622;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26041;&#27861;&#21482;&#20351;&#29992;&#20135;&#21697;&#25991;&#26412;&#65288;&#21363;&#20135;&#21697;&#26631;&#39064;&#21644;&#25551;&#36848;&#65289;&#65292;&#32780;&#19981;&#32771;&#34385;&#32473;&#23450;&#20135;&#21697;&#30340;&#22810;&#20010;&#23646;&#24615;&#20540;&#19982;&#20854;&#25991;&#26412;&#20043;&#38388;&#30340;&#35821;&#20041;&#36830;&#25509;&#65292;&#36825;&#21487;&#20197;&#24110;&#21161;&#23646;&#24615;&#20540;&#25552;&#21462;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#36825;&#20010;&#20219;&#21153;&#37325;&#26032;&#23450;&#20041;&#20026;&#19968;&#20010;&#22810;&#26631;&#31614;&#20998;&#31867;&#20219;&#21153;&#65292;&#21487;&#20197;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#24212;&#29992;&#65292;&#20854;&#20013;&#21482;&#26377;&#23646;&#24615;&#20540;&#30340;&#27880;&#37322;&#21487;&#29992;&#20110;&#35757;&#32451;&#27169;&#22411;&#65288;&#21363;&#27809;&#26377;&#23646;&#24615;&#20540;&#20301;&#32622;&#20449;&#24687;&#30340;&#27880;&#37322;&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#35821;&#20041;&#21305;&#37197;&#21644;&#36127;&#26631;&#31614;&#37319;&#26679;&#30340;&#20998;&#31867;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Product attribute value extraction plays an important role for many real-world applications in e-Commerce such as product search and recommendation. Previous methods treat it as a sequence labeling task that needs more annotation for position of values in the product text. This limits their application to real-world scenario in which only attribute values are weakly-annotated for each product without their position. Moreover, these methods only use product text (i.e., product title and description) and do not consider the semantic connection between the multiple attribute values of a given product and its text, which can help attribute value extraction. In this paper, we reformulate this task as a multi-label classification task that can be applied for real-world scenario in which only annotation of attribute values is available to train models (i.e., annotation of positional information of attribute values is not available). We propose a classification model with semantic matching and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#25991;&#26412;&#38382;&#31572;&#31995;&#32479;&#29983;&#25104;&#30340;&#20505;&#36873;&#31572;&#26696;&#22522;&#20110;&#20854;&#31867;&#22411;&#36827;&#34892;&#36807;&#28388;&#21644;&#37325;&#26032;&#25490;&#24207;&#65292;&#20197;&#35299;&#20915;&#22312;&#30693;&#35782;&#22270;&#35889;&#38382;&#31572;&#20219;&#21153;&#20013;&#65292;&#27169;&#22411;&#23481;&#37327;&#26377;&#38480;&#19988;&#23545;&#20110;&#21547;&#26377;&#19981;&#22826;&#27969;&#34892;&#23454;&#20307;&#30340;&#38382;&#39064;&#36136;&#37327;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.07008</link><description>&lt;p&gt;
&#31572;&#26696;&#20505;&#36873;&#31867;&#22411;&#36873;&#25321;&#65306;&#38381;&#20070;&#38382;&#31572;&#20013;&#30340;&#25991;&#26412;&#21040;&#25991;&#26412;&#35821;&#35328;&#27169;&#22411;&#28385;&#36275;&#30693;&#35782;&#22270;&#35889;
&lt;/p&gt;
&lt;p&gt;
Answer Candidate Type Selection: Text-to-Text Language Model for Closed Book Question Answering Meets Knowledge Graphs. (arXiv:2310.07008v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07008
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#25991;&#26412;&#38382;&#31572;&#31995;&#32479;&#29983;&#25104;&#30340;&#20505;&#36873;&#31572;&#26696;&#22522;&#20110;&#20854;&#31867;&#22411;&#36827;&#34892;&#36807;&#28388;&#21644;&#37325;&#26032;&#25490;&#24207;&#65292;&#20197;&#35299;&#20915;&#22312;&#30693;&#35782;&#22270;&#35889;&#38382;&#31572;&#20219;&#21153;&#20013;&#65292;&#27169;&#22411;&#23481;&#37327;&#26377;&#38480;&#19988;&#23545;&#20110;&#21547;&#26377;&#19981;&#22826;&#27969;&#34892;&#23454;&#20307;&#30340;&#38382;&#39064;&#36136;&#37327;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#25991;&#26412;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;T5&#25110;BART&#65289;&#22312;&#30693;&#35782;&#22270;&#35889;&#38382;&#31572;&#65288;KGQA&#65289;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#27169;&#22411;&#30340;&#23481;&#37327;&#26377;&#38480;&#65292;&#23545;&#20110;&#21253;&#21547;&#19981;&#22826;&#27969;&#34892;&#23454;&#20307;&#30340;&#38382;&#39064;&#65292;&#36136;&#37327;&#19979;&#38477;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#25991;&#26412;&#38382;&#31572;&#31995;&#32479;&#30340;&#22522;&#30784;&#19978;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#26681;&#25454;&#20505;&#36873;&#31572;&#26696;&#30340;&#31867;&#22411;&#65288;&#26469;&#33258;Wikidata&#30340;"instance_of"&#23646;&#24615;&#65289;&#36827;&#34892;&#31579;&#36873;&#21644;&#37325;&#26032;&#25490;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained Text-to-Text Language Models (LMs), such as T5 or BART yield promising results in the Knowledge Graph Question Answering (KGQA) task. However, the capacity of the models is limited and the quality decreases for questions with less popular entities. In this paper, we present a novel approach which works on top of the pre-trained Text-to-Text QA system to address this issue. Our simple yet effective method performs filtering and re-ranking of generated candidates based on their types derived from Wikidata "instance_of" property.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;Transformer&#30340;&#31070;&#32463;&#25991;&#26412;&#34920;&#31034;&#25216;&#26415;&#22312;&#32570;&#38519;&#20998;&#37197;&#20013;&#30340;&#24212;&#29992;&#65292;&#30456;&#27604;&#20043;&#21069;&#30340;&#26041;&#27861;&#65292;&#36825;&#20123;&#26032;&#25216;&#26415;&#33021;&#26356;&#22909;&#22320;&#25429;&#25417;&#24494;&#22937;&#30340;&#25991;&#26412;&#27169;&#24335;&#65292;&#25552;&#39640;&#33258;&#21160;&#21270;&#32570;&#38519;&#20998;&#37197;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.06913</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#31070;&#32463;&#25991;&#26412;&#34920;&#31034;&#25216;&#26415;&#22312;&#32570;&#38519;&#20998;&#37197;&#20013;&#30340;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Comparative Study of Transformer-based Neural Text Representation Techniques on Bug Triaging. (arXiv:2310.06913v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06913
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;Transformer&#30340;&#31070;&#32463;&#25991;&#26412;&#34920;&#31034;&#25216;&#26415;&#22312;&#32570;&#38519;&#20998;&#37197;&#20013;&#30340;&#24212;&#29992;&#65292;&#30456;&#27604;&#20043;&#21069;&#30340;&#26041;&#27861;&#65292;&#36825;&#20123;&#26032;&#25216;&#26415;&#33021;&#26356;&#22909;&#22320;&#25429;&#25417;&#24494;&#22937;&#30340;&#25991;&#26412;&#27169;&#24335;&#65292;&#25552;&#39640;&#33258;&#21160;&#21270;&#32570;&#38519;&#20998;&#37197;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31649;&#29702;&#32570;&#38519;&#25253;&#21578;&#26102;&#65292;&#36890;&#24120;&#31532;&#19968;&#27493;&#26159;&#23558;&#32570;&#38519;&#20998;&#37197;&#32473;&#26368;&#36866;&#21512;&#29702;&#35299;&#12289;&#23450;&#20301;&#21644;&#20462;&#22797;&#30446;&#26631;&#32570;&#38519;&#30340;&#24320;&#21457;&#20154;&#21592;&#12290;&#27492;&#22806;&#65292;&#23558;&#32473;&#23450;&#30340;&#32570;&#38519;&#20998;&#37197;&#32473;&#36719;&#20214;&#39033;&#30446;&#30340;&#29305;&#23450;&#37096;&#20998;&#21487;&#20197;&#21152;&#24555;&#20462;&#22797;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#36825;&#20123;&#27963;&#21160;&#30340;&#37325;&#35201;&#24615;&#65292;&#20294;&#22312;&#25163;&#21160;&#20998;&#37197;&#30340;&#36807;&#31243;&#20013;&#21487;&#33021;&#38656;&#35201;&#33457;&#36153;&#20960;&#22825;&#30340;&#26102;&#38388;&#12290;&#36807;&#21435;&#30340;&#30740;&#31350;&#23581;&#35797;&#21033;&#29992;&#26377;&#38480;&#30340;&#25991;&#26412;&#25968;&#25454;&#35757;&#32451;&#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#26469;&#33258;&#21160;&#21270;&#36825;&#20010;&#36807;&#31243;&#65292;&#20294;&#24471;&#21040;&#30340;&#25104;&#21151;&#31243;&#24230;&#21442;&#24046;&#19981;&#40784;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#24037;&#20316;&#20013;&#20351;&#29992;&#30340;&#25991;&#26412;&#34920;&#31034;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21463;&#21040;&#20854;&#34920;&#36798;&#33021;&#21147;&#30340;&#38480;&#21046;&#65292;&#24448;&#24448;&#26080;&#27861;&#25429;&#25417;&#21040;&#21487;&#20197;&#24110;&#21161;&#32570;&#38519;&#20998;&#37197;&#36807;&#31243;&#30340;&#24494;&#22937;&#25991;&#26412;&#27169;&#24335;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;Transformer&#30340;&#22823;&#22411;&#39044;&#35757;&#32451;&#31070;&#32463;&#25991;&#26412;&#34920;&#31034;&#25216;&#26415;&#65288;&#22914;BERT&#65289;&#22312;&#20960;&#20010;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Often, the first step in managing bug reports is related to triaging a bug to the appropriate developer who is best suited to understand, localize, and fix the target bug. Additionally, assigning a given bug to a particular part of a software project can help to expedite the fixing process. However, despite the importance of these activities, they are quite challenging, where days can be spent on the manual triaging process. Past studies have attempted to leverage the limited textual data of bug reports to train text classification models that automate this process -- to varying degrees of success. However, the textual representations and machine learning models used in prior work are limited by their expressiveness, often failing to capture nuanced textual patterns that might otherwise aid in the triaging process. Recently, large, transformer-based, pre-trained neural text representation techniques such as BERT have achieved greater performance in several natural language processing t
&lt;/p&gt;</description></item><item><title>MuseChat&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#23545;&#35805;&#24335;&#38899;&#20048;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#27169;&#25311;&#29992;&#25143;&#21644;&#25512;&#33616;&#31995;&#32479;&#20043;&#38388;&#30340;&#23545;&#35805;&#20132;&#20114;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#38899;&#20048;&#26631;&#31614;&#21644;&#33402;&#26415;&#23478;&#20449;&#24687;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#23450;&#21046;&#30340;&#38899;&#20048;&#25512;&#33616;&#65292;&#20351;&#29992;&#25143;&#21487;&#20197;&#20010;&#24615;&#21270;&#36873;&#25321;&#20182;&#20204;&#21916;&#27426;&#30340;&#38899;&#20048;&#12290;</title><link>http://arxiv.org/abs/2310.06282</link><description>&lt;p&gt;
MuseChat:&#19968;&#31181;&#35270;&#39057;&#23545;&#35805;&#38899;&#20048;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
MuseChat: A Conversational Music Recommendation System for Videos. (arXiv:2310.06282v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06282
&lt;/p&gt;
&lt;p&gt;
MuseChat&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#23545;&#35805;&#24335;&#38899;&#20048;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#27169;&#25311;&#29992;&#25143;&#21644;&#25512;&#33616;&#31995;&#32479;&#20043;&#38388;&#30340;&#23545;&#35805;&#20132;&#20114;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#38899;&#20048;&#26631;&#31614;&#21644;&#33402;&#26415;&#23478;&#20449;&#24687;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#23450;&#21046;&#30340;&#38899;&#20048;&#25512;&#33616;&#65292;&#20351;&#29992;&#25143;&#21487;&#20197;&#20010;&#24615;&#21270;&#36873;&#25321;&#20182;&#20204;&#21916;&#27426;&#30340;&#38899;&#20048;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;MuseChat&#65292;&#19968;&#31181;&#21019;&#26032;&#30340;&#22522;&#20110;&#23545;&#35805;&#30340;&#38899;&#20048;&#25512;&#33616;&#31995;&#32479;&#12290;&#36825;&#20010;&#29420;&#29305;&#30340;&#24179;&#21488;&#19981;&#20165;&#25552;&#20379;&#20114;&#21160;&#29992;&#25143;&#21442;&#19982;&#65292;&#36824;&#20026;&#36755;&#20837;&#30340;&#35270;&#39057;&#25552;&#20379;&#20102;&#23450;&#21046;&#30340;&#38899;&#20048;&#25512;&#33616;&#65292;&#20351;&#29992;&#25143;&#21487;&#20197;&#25913;&#36827;&#21644;&#20010;&#24615;&#21270;&#20182;&#20204;&#30340;&#38899;&#20048;&#36873;&#25321;&#12290;&#19982;&#20043;&#30456;&#21453;&#65292;&#20197;&#21069;&#30340;&#31995;&#32479;&#20027;&#35201;&#24378;&#35843;&#20869;&#23481;&#30340;&#20860;&#23481;&#24615;&#65292;&#24448;&#24448;&#24573;&#35270;&#20102;&#29992;&#25143;&#20010;&#20307;&#20559;&#22909;&#30340;&#32454;&#24494;&#24046;&#21035;&#12290;&#20363;&#22914;&#65292;&#25152;&#26377;&#30340;&#25968;&#25454;&#38598;&#37117;&#21482;&#25552;&#20379;&#22522;&#26412;&#30340;&#38899;&#20048;-&#35270;&#39057;&#37197;&#23545;&#65292;&#25110;&#32773;&#24102;&#26377;&#38899;&#20048;&#25551;&#36848;&#30340;&#37197;&#23545;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#19977;&#20010;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#23545;&#35805;&#21512;&#25104;&#26041;&#27861;&#65292;&#27169;&#25311;&#20102;&#29992;&#25143;&#21644;&#25512;&#33616;&#31995;&#32479;&#20043;&#38388;&#30340;&#20004;&#36718;&#20132;&#20114;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#38899;&#20048;&#26631;&#31614;&#21644;&#33402;&#26415;&#23478;&#20449;&#24687;&#12290;&#22312;&#36825;&#20010;&#20132;&#20114;&#20013;&#65292;&#29992;&#25143;&#25552;&#20132;&#19968;&#20010;&#35270;&#39057;&#32473;&#31995;&#32479;&#65292;&#31995;&#32479;&#20250;&#25552;&#20379;&#19968;&#20010;&#21512;&#36866;&#30340;&#38899;&#20048;&#29255;&#27573;&#65292;&#24182;&#38468;&#24102;&#35299;&#37322;&#12290;&#20043;&#21518;&#65292;&#29992;&#25143;&#20250;&#34920;&#36798;&#20182;&#20204;&#23545;&#38899;&#20048;&#30340;&#20559;&#22909;&#65292;&#31995;&#32479;&#20250;&#21576;&#29616;&#19968;&#20010;&#25913;&#36827;&#21518;&#30340;&#38899;&#20048;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
We introduce MuseChat, an innovative dialog-based music recommendation system. This unique platform not only offers interactive user engagement but also suggests music tailored for input videos, so that users can refine and personalize their music selections. In contrast, previous systems predominantly emphasized content compatibility, often overlooking the nuances of users' individual preferences. For example, all the datasets only provide basic music-video pairings or such pairings with textual music descriptions. To address this gap, our research offers three contributions. First, we devise a conversation-synthesis method that simulates a two-turn interaction between a user and a recommendation system, which leverages pre-trained music tags and artist information. In this interaction, users submit a video to the system, which then suggests a suitable music piece with a rationale. Afterwards, users communicate their musical preferences, and the system presents a refined music recomme
&lt;/p&gt;</description></item><item><title>DiscoverPath&#26159;&#19968;&#20010;&#22522;&#20110;&#30693;&#35782;&#22270;&#30340;&#29983;&#29289;&#21307;&#23398;&#30740;&#31350;&#35770;&#25991;&#25628;&#32034;&#24341;&#25806;&#65292;&#36890;&#36807;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#35789;&#24615;&#26631;&#27880;&#20174;&#25991;&#31456;&#25688;&#35201;&#20013;&#25552;&#21462;&#26415;&#35821;&#21644;&#20851;&#31995;&#65292;&#24182;&#23637;&#31034;&#32473;&#29992;&#25143;&#19968;&#20010;&#20851;&#27880;&#26597;&#35810;&#23454;&#20307;&#21450;&#20854;&#37051;&#36817;&#33410;&#28857;&#30340;&#23376;&#22270;&#65292;&#20197;&#21450;&#26597;&#35810;&#25512;&#33616;&#31995;&#32479;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#24490;&#24207;&#28176;&#36827;&#22320;&#32454;&#21270;&#26597;&#35810;&#12290;</title><link>http://arxiv.org/abs/2309.01808</link><description>&lt;p&gt;
DiscoverPath&#65306;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#30740;&#31350;&#30340;&#30693;&#35782;&#32454;&#21270;&#21644;&#26816;&#32034;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
DiscoverPath: A Knowledge Refinement and Retrieval System for Interdisciplinarity on Biomedical Research. (arXiv:2309.01808v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01808
&lt;/p&gt;
&lt;p&gt;
DiscoverPath&#26159;&#19968;&#20010;&#22522;&#20110;&#30693;&#35782;&#22270;&#30340;&#29983;&#29289;&#21307;&#23398;&#30740;&#31350;&#35770;&#25991;&#25628;&#32034;&#24341;&#25806;&#65292;&#36890;&#36807;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#35789;&#24615;&#26631;&#27880;&#20174;&#25991;&#31456;&#25688;&#35201;&#20013;&#25552;&#21462;&#26415;&#35821;&#21644;&#20851;&#31995;&#65292;&#24182;&#23637;&#31034;&#32473;&#29992;&#25143;&#19968;&#20010;&#20851;&#27880;&#26597;&#35810;&#23454;&#20307;&#21450;&#20854;&#37051;&#36817;&#33410;&#28857;&#30340;&#23376;&#22270;&#65292;&#20197;&#21450;&#26597;&#35810;&#25512;&#33616;&#31995;&#32479;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#24490;&#24207;&#28176;&#36827;&#22320;&#32454;&#21270;&#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#26415;&#20986;&#29256;&#29289;&#30340;&#25351;&#25968;&#22686;&#38271;&#38656;&#35201;&#39640;&#32423;&#24037;&#20855;&#26469;&#23454;&#29616;&#39640;&#25928;&#30340;&#25991;&#31456;&#26816;&#32034;&#65292;&#23588;&#20854;&#22312;&#36328;&#23398;&#31185;&#39046;&#22495;&#20013;&#65292;&#19981;&#21516;&#30340;&#26415;&#35821;&#34987;&#29992;&#26469;&#25551;&#36848;&#30456;&#20284;&#30340;&#30740;&#31350;&#12290;&#20256;&#32479;&#30340;&#22522;&#20110;&#20851;&#38190;&#35789;&#30340;&#25628;&#32034;&#24341;&#25806;&#24448;&#24448;&#26080;&#27861;&#24110;&#21161;&#37027;&#20123;&#23545;&#29305;&#23450;&#26415;&#35821;&#19981;&#29087;&#24713;&#30340;&#29992;&#25143;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#22270;&#30340;&#29983;&#29289;&#21307;&#23398;&#30740;&#31350;&#35770;&#25991;&#25628;&#32034;&#24341;&#25806;&#65292;&#20197;&#22686;&#24378;&#29992;&#25143;&#22312;&#21457;&#29616;&#30456;&#20851;&#26597;&#35810;&#21644;&#25991;&#31456;&#26041;&#38754;&#30340;&#20307;&#39564;&#12290;&#35813;&#31995;&#32479;&#34987;&#31216;&#20026;DiscoverPath&#65292;&#37319;&#29992;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#21644;&#35789;&#24615;&#26631;&#27880;&#65288;POS&#65289;&#26469;&#20174;&#25991;&#31456;&#25688;&#35201;&#20013;&#25552;&#21462;&#26415;&#35821;&#21644;&#20851;&#31995;&#65292;&#21019;&#24314;&#30693;&#35782;&#22270;&#35889;&#12290;&#20026;&#20102;&#20943;&#23569;&#20449;&#24687;&#36229;&#36733;&#65292;DiscoverPath&#32473;&#29992;&#25143;&#23637;&#31034;&#20102;&#19968;&#20010;&#20851;&#27880;&#26597;&#35810;&#23454;&#20307;&#21450;&#20854;&#37051;&#36817;&#33410;&#28857;&#30340;&#23376;&#22270;&#65292;&#24182;&#19988;&#36824;&#32467;&#21512;&#20102;&#26597;&#35810;&#25512;&#33616;&#31995;&#32479;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#24490;&#24207;&#28176;&#36827;&#22320;&#32454;&#21270;&#26597;&#35810;&#12290;&#35813;&#31995;&#32479;&#37197;&#22791;&#20102;&#19968;&#20010;&#26131;&#20110;&#35775;&#38382;&#30340;&#22270;&#24418;&#29992;&#25143;&#30028;&#38754;&#65288;GUI&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
The exponential growth in scholarly publications necessitates advanced tools for efficient article retrieval, especially in interdisciplinary fields where diverse terminologies are used to describe similar research. Traditional keyword-based search engines often fall short in assisting users who may not be familiar with specific terminologies. To address this, we present a knowledge graph-based paper search engine for biomedical research to enhance the user experience in discovering relevant queries and articles. The system, dubbed DiscoverPath, employs Named Entity Recognition (NER) and part-of-speech (POS) tagging to extract terminologies and relationships from article abstracts to create a KG. To reduce information overload, DiscoverPath presents users with a focused subgraph containing the queried entity and its neighboring nodes and incorporates a query recommendation system, enabling users to iteratively refine their queries. The system is equipped with an accessible Graphical Us
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#32676;&#20307;&#29992;&#25143;&#25512;&#33616;&#21830;&#21697;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#20123;&#27169;&#22411;&#30456;&#27604;&#29616;&#26377;&#27169;&#22411;&#30340;&#25913;&#36827;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.09447</link><description>&lt;p&gt;
&#20026;&#32676;&#20307;&#29992;&#25143;&#25512;&#33616;&#21830;&#21697;&#30340;&#28145;&#24230;&#31070;&#32463;&#32858;&#21512;
&lt;/p&gt;
&lt;p&gt;
Deep Neural Aggregation for Recommending Items to Group of Users. (arXiv:2307.09447v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09447
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#32676;&#20307;&#29992;&#25143;&#25512;&#33616;&#21830;&#21697;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#20123;&#27169;&#22411;&#30456;&#27604;&#29616;&#26377;&#27169;&#22411;&#30340;&#25913;&#36827;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#31038;&#20250;&#33457;&#36153;&#20102;&#22823;&#37327;&#26102;&#38388;&#22312;&#25968;&#23383;&#20132;&#20114;&#19978;&#65292;&#25105;&#20204;&#30340;&#26085;&#24120;&#34892;&#20026;&#24456;&#22810;&#37117;&#36890;&#36807;&#25968;&#23383;&#25163;&#27573;&#23436;&#25104;&#12290;&#36825;&#23548;&#33268;&#20102;&#35768;&#22810;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#30340;&#20986;&#29616;&#65292;&#24110;&#21161;&#25105;&#20204;&#22312;&#29983;&#27963;&#30340;&#21508;&#20010;&#26041;&#38754;&#36827;&#34892;&#36741;&#21161;&#12290;&#23545;&#20110;&#25968;&#23383;&#31038;&#20250;&#26469;&#35828;&#65292;&#19968;&#20010;&#20851;&#38190;&#30340;&#24037;&#20855;&#26159;&#25512;&#33616;&#31995;&#32479;&#65292;&#23427;&#26159;&#26234;&#33021;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#23398;&#20064;&#25105;&#20204;&#30340;&#36807;&#21435;&#34892;&#20026;&#65292;&#25552;&#20986;&#19982;&#25105;&#20204;&#20852;&#36259;&#30456;&#31526;&#30340;&#26032;&#34892;&#20026;&#24314;&#35758;&#12290;&#20854;&#20013;&#19968;&#20123;&#31995;&#32479;&#19987;&#38376;&#20174;&#29992;&#25143;&#32676;&#20307;&#30340;&#34892;&#20026;&#20013;&#23398;&#20064;&#65292;&#21521;&#24076;&#26395;&#20849;&#21516;&#23436;&#25104;&#26576;&#20010;&#20219;&#21153;&#30340;&#20010;&#20307;&#32676;&#20307;&#25552;&#20986;&#24314;&#35758;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#32676;&#20307;&#25512;&#33616;&#31995;&#32479;&#30340;&#29616;&#29366;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#20351;&#29992;&#26032;&#20852;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#30340;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20351;&#29992;&#22235;&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;&#26368;&#26032;&#27169;&#22411;&#30456;&#27604;&#65292;&#37319;&#29992;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#21487;&#20197;&#21462;&#24471;&#25913;&#36827;&#12290;&#35813;&#27169;&#22411;&#21450;&#25152;&#26377;&#23454;&#39564;&#30340;&#28304;&#20195;&#30721;&#37117;&#21487;&#20379;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern society devotes a significant amount of time to digital interaction. Many of our daily actions are carried out through digital means. This has led to the emergence of numerous Artificial Intelligence tools that assist us in various aspects of our lives. One key tool for the digital society is Recommender Systems, intelligent systems that learn from our past actions to propose new ones that align with our interests. Some of these systems have specialized in learning from the behavior of user groups to make recommendations to a group of individuals who want to perform a joint task. In this article, we analyze the current state of Group Recommender Systems and propose two new models that use emerging Deep Learning architectures. Experimental results demonstrate the improvement achieved by employing the proposed models compared to the state-of-the-art models using four different datasets. The source code of the models, as well as that of all the experiments conducted, is available i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#32534;&#36753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38382;&#39064;&#12289;&#26041;&#27861;&#21644;&#26426;&#20250;&#65292;&#25552;&#20379;&#20102;&#20219;&#21153;&#23450;&#20041;&#21644;&#25361;&#25112;&#30340;&#27010;&#36848;&#12289;&#20808;&#36827;&#26041;&#27861;&#30340;&#23454;&#35777;&#20998;&#26512;&#65292;&#20197;&#21450;&#26500;&#24314;&#20102;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#36825;&#20123;&#32467;&#26524;&#26377;&#21161;&#20110;&#25913;&#36827;LLMs&#30340;&#32534;&#36753;&#25216;&#26415;&#65292;&#25552;&#39640;&#20854;&#25928;&#26524;&#21644;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.13172</link><description>&lt;p&gt;
&#32534;&#36753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#38382;&#39064;&#12289;&#26041;&#27861;&#21644;&#26426;&#20250;
&lt;/p&gt;
&lt;p&gt;
Editing Large Language Models: Problems, Methods, and Opportunities. (arXiv:2305.13172v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13172
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#32534;&#36753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38382;&#39064;&#12289;&#26041;&#27861;&#21644;&#26426;&#20250;&#65292;&#25552;&#20379;&#20102;&#20219;&#21153;&#23450;&#20041;&#21644;&#25361;&#25112;&#30340;&#27010;&#36848;&#12289;&#20808;&#36827;&#26041;&#27861;&#30340;&#23454;&#35777;&#20998;&#26512;&#65292;&#20197;&#21450;&#26500;&#24314;&#20102;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#36825;&#20123;&#32467;&#26524;&#26377;&#21161;&#20110;&#25913;&#36827;LLMs&#30340;&#32534;&#36753;&#25216;&#26415;&#65292;&#25552;&#39640;&#20854;&#25928;&#26524;&#21644;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#33021;&#22815;&#35757;&#32451;&#20986;&#34920;&#29616;&#20248;&#31168;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#20294;&#20854;&#20445;&#25345;&#30456;&#20851;&#24615;&#21644;&#32416;&#27491;&#38169;&#35823;&#30340;&#26041;&#27861;&#20173;&#28982;&#38590;&#20197;&#30830;&#23450;&#12290;&#20026;&#27492;&#65292;&#26368;&#36817;&#20960;&#24180;&#20986;&#29616;&#20102;&#35768;&#22810;&#32534;&#36753;LLMs&#30340;&#25216;&#26415;&#65292;&#20854;&#30446;&#26631;&#26159;&#22312;&#29305;&#23450;&#39046;&#22495;&#20869;&#39640;&#25928;&#22320;&#25913;&#21464;LLMs&#30340;&#34892;&#20026;&#65292;&#21516;&#26102;&#19981;&#23545;&#20854;&#20182;&#36755;&#20837;&#30340;&#24615;&#33021;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#19982;LLMs&#27169;&#22411;&#32534;&#36753;&#30456;&#20851;&#30340;&#38382;&#39064;&#12289;&#26041;&#27861;&#21644;&#26426;&#20250;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#27169;&#22411;&#32534;&#36753;&#20219;&#21153;&#23450;&#20041;&#21644;&#30456;&#20851;&#25361;&#25112;&#30340;&#20840;&#38754;&#27010;&#36848;&#65292;&#20197;&#21450;&#23545;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30340;&#28145;&#20837;&#23454;&#35777;&#20998;&#26512;&#12290;&#25105;&#20204;&#36824;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20197;&#20419;&#36827;&#26356;&#24378;&#22823;&#30340;&#35780;&#20272;&#65292;&#24182;&#25351;&#20986;&#29616;&#26377;&#25216;&#26415;&#22266;&#26377;&#30340;&#25345;&#20037;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#20026;&#27599;&#31181;&#32534;&#36753;&#25216;&#26415;&#30340;&#25928;&#26524;&#21644;&#21487;&#34892;&#24615;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#65292;&#20174;&#32780;&#24110;&#21161;&#31038;&#21306;&#22312;LLMs&#30340;&#31649;&#29702;&#20013;&#21462;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the ability to train capable LLMs, the methodology for maintaining their relevancy and rectifying errors remains elusive. To this end, the past few years have witnessed a surge in techniques for editing LLMs, the objective of which is to efficiently alter the behavior of LLMs within a specific domain without negatively impacting performance across other inputs. This paper embarks on a deep exploration of the problems, methods, and opportunities related to model editing for LLMs. In particular, we provide an exhaustive overview of the task definition and challenges associated with model editing, along with an in-depth empirical analysis of the most progressive methods currently at our disposal. We also build a new benchmark dataset to facilitate a more robust evaluation and pinpoint enduring issues intrinsic to existing techniques. Our objective is to provide valuable insights into the effectiveness and feasibility of each editing technique, thereby assisting the community in ma
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#30693;&#35782;&#21453;&#24605;&#30340;&#26032;&#33539;&#24335;&#65292;&#26088;&#22312;&#24110;&#21161;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21033;&#29992;&#24050;&#32463;&#32534;&#30721;&#22312;&#20854;&#39044;&#35757;&#32451;&#21442;&#25968;&#20013;&#30340;&#30456;&#20851;&#28508;&#22312;&#30693;&#35782;&#65292;&#32780;&#19981;&#38656;&#35201;&#20174;&#22806;&#37096;&#35821;&#26009;&#24211;&#20013;&#26816;&#32034;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#22312;&#27169;&#22411;&#20013;&#28155;&#21152;&#25552;&#31034;&#65292;&#24182;&#23558;&#30456;&#20851;&#30693;&#35782;&#27880;&#20837;&#27169;&#22411;&#36827;&#34892;&#25972;&#21512;&#65292;&#21462;&#24471;&#20102;&#22312;&#24120;&#35782;&#25512;&#29702;&#20219;&#21153;&#21644;GLUE&#22522;&#20934;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.08732</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#21453;&#24605;
&lt;/p&gt;
&lt;p&gt;
Knowledge Rumination for Pre-trained Language Models. (arXiv:2305.08732v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08732
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#30693;&#35782;&#21453;&#24605;&#30340;&#26032;&#33539;&#24335;&#65292;&#26088;&#22312;&#24110;&#21161;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21033;&#29992;&#24050;&#32463;&#32534;&#30721;&#22312;&#20854;&#39044;&#35757;&#32451;&#21442;&#25968;&#20013;&#30340;&#30456;&#20851;&#28508;&#22312;&#30693;&#35782;&#65292;&#32780;&#19981;&#38656;&#35201;&#20174;&#22806;&#37096;&#35821;&#26009;&#24211;&#20013;&#26816;&#32034;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#22312;&#27169;&#22411;&#20013;&#28155;&#21152;&#25552;&#31034;&#65292;&#24182;&#23558;&#30456;&#20851;&#30693;&#35782;&#27880;&#20837;&#27169;&#22411;&#36827;&#34892;&#25972;&#21512;&#65292;&#21462;&#24471;&#20102;&#22312;&#24120;&#35782;&#25512;&#29702;&#20219;&#21153;&#21644;GLUE&#22522;&#20934;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#26222;&#36890;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#21333;&#29420;&#22788;&#29702;&#30693;&#35782;&#23494;&#38598;&#22411;NLP&#20219;&#21153;&#30340;&#33021;&#21147;&#19981;&#36275;&#65292;&#22240;&#27492;&#65292;&#19968;&#20123;&#24037;&#20316;&#23581;&#35797;&#23558;&#22806;&#37096;&#30693;&#35782;&#38598;&#25104;&#21040;PLMs&#20013;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#26377;&#30528;&#26377;&#21069;&#36884;&#30340;&#32467;&#26524;&#65292;&#20294;&#25105;&#20204;&#32463;&#39564;&#24615;&#22320;&#35266;&#23519;&#21040;&#65292;PLM&#21487;&#33021;&#24050;&#32463;&#22312;&#20854;&#39044;&#35757;&#32451;&#21442;&#25968;&#20013;&#32534;&#30721;&#20102;&#20016;&#23500;&#30340;&#30693;&#35782;&#65292;&#20294;&#22312;&#24212;&#29992;&#21040;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#26102;&#26410;&#33021;&#20805;&#20998;&#21033;&#29992;&#23427;&#20204;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#30693;&#35782;&#21453;&#24605;&#30340;&#26032;&#33539;&#24335;&#65292;&#20197;&#24110;&#21161;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21033;&#29992;&#30456;&#20851;&#30340;&#28508;&#22312;&#30693;&#35782;&#65292;&#32780;&#19981;&#38656;&#35201;&#20174;&#22806;&#37096;&#35821;&#26009;&#24211;&#20013;&#26816;&#32034;&#23427;&#20204;&#12290;&#36890;&#36807;&#31616;&#21333;&#22320;&#22312;PLMs&#20013;&#28155;&#21152;&#19968;&#20010;&#22914;&#8220;&#25454;&#25105;&#25152;&#30693;&#8221;&#30340;&#25552;&#31034;&#65292;&#25105;&#20204;&#35797;&#22270;&#22238;&#39038;&#30456;&#20851;&#30340;&#28508;&#22312;&#30693;&#35782;&#65292;&#24182;&#23558;&#20854;&#27880;&#20837;&#27169;&#22411;&#20197;&#36827;&#34892;&#30693;&#35782;&#25972;&#21512;&#12290;&#25105;&#20204;&#23558;&#25552;&#20986;&#30340;&#30693;&#35782;&#21453;&#24605;&#24212;&#29992;&#20110;&#21508;&#31181;&#35821;&#35328;&#27169;&#22411;&#65292;&#21253;&#25324;RoBERTa&#12289;DeBERTa&#21644;GPT-3&#12290;&#22312;&#20845;&#20010;&#24120;&#35782;&#25512;&#29702;&#20219;&#21153;&#21644;GLUE&#22522;&#20934;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;.....
&lt;/p&gt;
&lt;p&gt;
Previous studies have revealed that vanilla pre-trained language models (PLMs) lack the capacity to handle knowledge-intensive NLP tasks alone; thus, several works have attempted to integrate external knowledge into PLMs. However, despite the promising outcome, we empirically observe that PLMs may have already encoded rich knowledge in their pre-trained parameters but fail to fully utilize them when applying them to knowledge-intensive tasks. In this paper, we propose a new paradigm dubbed Knowledge Rumination to help the pre-trained language model utilize that related latent knowledge without retrieving it from the external corpus. By simply adding a prompt like "As far as I know" to the PLMs, we try to review related latent knowledge and inject them back into the model for knowledge consolidation. We apply the proposed knowledge rumination to various language models, including RoBERTa, DeBERTa, and GPT-3. Experimental results on six commonsense reasoning tasks and GLUE benchmarks dem
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#35770;&#25991;&#23545;&#22522;&#20110;&#23545;&#27604;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#25512;&#33616;&#26041;&#27861;&#36827;&#34892;&#20102;&#32508;&#21512;&#35780;&#20272;&#21644;&#20998;&#31867;&#65292;&#24182;&#25552;&#20379;&#20102;&#32479;&#19968;&#30340;&#26694;&#26550;&#26469;&#27010;&#36848;&#36825;&#20123;&#26041;&#27861;&#12290;&#23545;&#27604;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#30001;&#20110;&#28789;&#27963;&#24615;&#21644;&#24615;&#33021;&#20248;&#33391;&#32780;&#21560;&#24341;&#20102;&#22823;&#37327;&#20851;&#27880;&#65292;&#24182;&#25104;&#20026;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#25512;&#33616;&#26041;&#27861;&#30340;&#20027;&#23548;&#20998;&#25903;&#12290;</title><link>http://arxiv.org/abs/2303.09902</link><description>&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#20013;&#23545;&#27604;&#33258;&#30417;&#30563;&#23398;&#20064;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Contrastive Self-supervised Learning in Recommender Systems: A Survey. (arXiv:2303.09902v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09902
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#35770;&#25991;&#23545;&#22522;&#20110;&#23545;&#27604;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#25512;&#33616;&#26041;&#27861;&#36827;&#34892;&#20102;&#32508;&#21512;&#35780;&#20272;&#21644;&#20998;&#31867;&#65292;&#24182;&#25552;&#20379;&#20102;&#32479;&#19968;&#30340;&#26694;&#26550;&#26469;&#27010;&#36848;&#36825;&#20123;&#26041;&#27861;&#12290;&#23545;&#27604;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#30001;&#20110;&#28789;&#27963;&#24615;&#21644;&#24615;&#33021;&#20248;&#33391;&#32780;&#21560;&#24341;&#20102;&#22823;&#37327;&#20851;&#27880;&#65292;&#24182;&#25104;&#20026;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#25512;&#33616;&#26041;&#27861;&#30340;&#20027;&#23548;&#20998;&#25903;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#25512;&#33616;&#31995;&#32479;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#20294;&#26159;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#20005;&#37325;&#20381;&#36182;&#20110;&#26377;&#26631;&#31614;&#30340;&#25968;&#25454;&#65288;&#21363;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#65289;&#65292;&#36973;&#21463;&#30528;&#25968;&#25454;&#31232;&#30095;&#21644;&#20919;&#21551;&#21160;&#31561;&#38382;&#39064;&#12290;&#33258;&#30417;&#30563;&#23398;&#20064;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#33539;&#24335;&#65292;&#23427;&#20174;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#20013;&#25552;&#21462;&#20449;&#24687;&#65292;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#23545;&#27604;&#33258;&#30417;&#30563;&#23398;&#20064;&#30001;&#20110;&#20854;&#28789;&#27963;&#24615;&#21644;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#24050;&#32463;&#21560;&#24341;&#20102;&#30456;&#24403;&#22810;&#30340;&#20851;&#27880;&#65292;&#24182;&#26368;&#36817;&#25104;&#20026;&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#25512;&#33616;&#26041;&#27861;&#20013;&#30340;&#20027;&#23548;&#20998;&#25903;&#12290;&#22312;&#26412;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#24403;&#21069;&#22522;&#20110;&#23545;&#27604;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#25512;&#33616;&#26041;&#27861;&#30340;&#26368;&#26032;&#21644;&#20840;&#38754;&#30340;&#35780;&#20272;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#26469;&#27010;&#36848;&#36825;&#20123;&#26041;&#27861;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#26681;&#25454;&#26694;&#26550;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#21253;&#25324;&#35270;&#22270;&#29983;&#25104;&#31574;&#30053;&#12289;&#23545;&#27604;&#20219;&#21153;&#21644;&#23545;&#27604;&#30446;&#26631;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#31867;&#27861;&#12290;&#23545;&#20110;&#27599;&#20010;&#32452;&#25104;&#37096;&#20998;&#65292;
&lt;/p&gt;
&lt;p&gt;
Deep learning-based recommender systems have achieved remarkable success in recent years. However, these methods usually heavily rely on labeled data (i.e., user-item interactions), suffering from problems such as data sparsity and cold-start. Self-supervised learning, an emerging paradigm that extracts information from unlabeled data, provides insights into addressing these problems. Specifically, contrastive self-supervised learning, due to its flexibility and promising performance, has attracted considerable interest and recently become a dominant branch in self-supervised learning-based recommendation methods. In this survey, we provide an up-to-date and comprehensive review of current contrastive self-supervised learning-based recommendation methods. Firstly, we propose a unified framework for these methods. We then introduce a taxonomy based on the key components of the framework, including view generation strategy, contrastive task, and contrastive objective. For each component,
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;query2doc&#30340;&#26597;&#35810;&#25193;&#23637;&#26041;&#27861;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#20266;&#25991;&#26723;&#26469;&#25913;&#21892;&#31232;&#30095;&#21644;&#23494;&#38598;&#26816;&#32034;&#31995;&#32479;&#65292;&#21462;&#24471;&#20102;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#25552;&#39640; BM25 &#24615;&#33021;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.07678</link><description>&lt;p&gt;
Query2doc: &#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26597;&#35810;&#25193;&#23637;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Query2doc: Query Expansion with Large Language Models. (arXiv:2303.07678v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07678
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;query2doc&#30340;&#26597;&#35810;&#25193;&#23637;&#26041;&#27861;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#20266;&#25991;&#26723;&#26469;&#25913;&#21892;&#31232;&#30095;&#21644;&#23494;&#38598;&#26816;&#32034;&#31995;&#32479;&#65292;&#21462;&#24471;&#20102;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#25552;&#39640; BM25 &#24615;&#33021;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26597;&#35810;&#25193;&#23637;&#26041;&#27861;&#65292;&#31216;&#20026;query2doc&#65292;&#21487;&#25913;&#21892;&#31232;&#30095;&#21644;&#23494;&#38598;&#26816;&#32034;&#31995;&#32479;&#12290;&#35813;&#26041;&#27861;&#39318;&#20808;&#21033;&#29992;&#23567;&#25209;&#37327;&#25552;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#20266;&#25991;&#26723;&#65292;&#28982;&#21518;&#20351;&#29992;&#29983;&#25104;&#30340;&#20266;&#25991;&#26723;&#25193;&#23637;&#26597;&#35810;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32463;&#36807;&#35757;&#32451;&#65292;&#33021;&#22815;&#35760;&#24518;&#30693;&#35782;&#65292;&#20174;&#32780;&#29983;&#25104;&#30340;&#20266;&#25991;&#26723;&#36890;&#24120;&#21253;&#21547;&#39640;&#24230;&#30456;&#20851;&#30340;&#20449;&#24687;&#65292;&#26377;&#21161;&#20110;&#26597;&#35810;&#28040;&#23696;&#21644;&#25351;&#23548;&#26816;&#32034;&#22120;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#19981;&#36827;&#34892;&#20219;&#20309;&#27169;&#22411;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#65292;query2doc &#22312; MS-MARCO &#21644; TREC DL &#31561; ad-hoc IR &#25968;&#25454;&#38598;&#19978;&#23558; BM25 &#30340;&#24615;&#33021;&#25552;&#39640;&#20102; 3% &#21040; 15%&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#22312;&#39046;&#22495;&#20869;&#21644;&#39046;&#22495;&#22806;&#30340;&#32467;&#26524;&#26041;&#38754;&#21463;&#30410;&#20110;&#26368;&#20808;&#36827;&#30340;&#23494;&#38598;&#26816;&#32034;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a simple yet effective query expansion approach, denoted as query2doc, to improve both sparse and dense retrieval systems. The proposed method first generates pseudo-documents by few-shot prompting large language models (LLMs), and then expands the query with generated pseudo-documents. LLMs are trained on web-scale text corpora and are adept at knowledge memorization. The pseudo-documents from LLMs often contain highly relevant information that can aid in query disambiguation and guide the retrievers. Experimental results demonstrate that query2doc boosts the performance of BM25 by 3% to 15% on ad-hoc IR datasets, such as MS-MARCO and TREC DL, without any model fine-tuning. Furthermore, our method also benefits state-of-the-art dense retrievers in terms of both in-domain and out-of-domain results.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20851;&#27880;&#22522;&#20110;&#22330;&#26223;&#21644;&#23545;&#35937;&#30340;&#22270;&#20687;-&#25991;&#26412;&#36328;&#27169;&#24577;&#26816;&#32034;&#30340;&#21487;&#22797;&#29616;&#24615;&#65292;&#36890;&#36807;&#36873;&#25321;&#19981;&#21516;&#20307;&#31995;&#32467;&#26500;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#24182;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#25506;&#35752;&#20102;&#20854;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#31867;&#22411;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2301.05174</link><description>&lt;p&gt;
&#22522;&#20110;&#22330;&#26223;&#21644;&#23545;&#35937;&#30340;&#22270;&#20687;-&#25991;&#26412;&#36328;&#27169;&#24577;&#26816;&#32034;&#65306;&#19968;&#39033;&#21487;&#22797;&#29616;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Scene-centric vs. Object-centric Image-Text Cross-modal Retrieval: A Reproducibility Study. (arXiv:2301.05174v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.05174
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20851;&#27880;&#22522;&#20110;&#22330;&#26223;&#21644;&#23545;&#35937;&#30340;&#22270;&#20687;-&#25991;&#26412;&#36328;&#27169;&#24577;&#26816;&#32034;&#30340;&#21487;&#22797;&#29616;&#24615;&#65292;&#36890;&#36807;&#36873;&#25321;&#19981;&#21516;&#20307;&#31995;&#32467;&#26500;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#24182;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#25506;&#35752;&#20102;&#20854;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#31867;&#22411;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#36328;&#27169;&#24577;&#26816;&#32034;&#65288;CMR&#65289;&#26041;&#27861;&#35201;&#20040;&#32858;&#28966;&#20110;&#20197;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#25968;&#25454;&#38598;&#65292;&#21363;&#27599;&#20010;&#25991;&#26723;&#25551;&#32472;&#25110;&#25551;&#36848;&#19968;&#20010;&#21333;&#19968;&#23545;&#35937;&#65292;&#35201;&#20040;&#32858;&#28966;&#20110;&#20197;&#22330;&#26223;&#20026;&#20013;&#24515;&#30340;&#25968;&#25454;&#38598;&#65292;&#21363;&#27599;&#20010;&#22270;&#20687;&#25551;&#32472;&#25110;&#25551;&#36848;&#30456;&#20114;&#20851;&#32852;&#30340;&#22810;&#20010;&#23545;&#35937;&#21644;&#20851;&#31995;&#30340;&#22797;&#26434;&#22330;&#26223;&#12290;&#25105;&#20204;&#35748;&#20026;&#19968;&#20010;&#24378;&#22823;&#30340;CMR&#27169;&#22411;&#24212;&#35813;&#22312;&#20004;&#31181;&#25968;&#25454;&#38598;&#31867;&#22411;&#19978;&#37117;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#23613;&#31649;CMR&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#32467;&#26524;&#30340;&#21487;&#22797;&#29616;&#24615;&#21450;&#20854;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#31867;&#22411;&#19978;&#30340;&#27867;&#21270;&#24615;&#23578;&#26410;&#34987;&#30740;&#31350;&#36807;&#12290;&#25105;&#20204;&#22635;&#34917;&#20102;&#36825;&#20010;&#31354;&#30333;&#65292;&#24182;&#20851;&#27880;&#24403;&#22312;&#20197;&#23545;&#35937;&#20026;&#20013;&#24515;&#21644;&#20197;&#22330;&#26223;&#20026;&#20013;&#24515;&#30340;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#26102;&#65292;&#26368;&#20808;&#36827;&#30340;CMR&#32467;&#26524;&#30340;&#21487;&#22797;&#29616;&#24615;&#12290;&#25105;&#20204;&#36873;&#25321;&#20102;&#20004;&#31181;&#20855;&#26377;&#19981;&#21516;&#20307;&#31995;&#32467;&#26500;&#30340;&#26368;&#20808;&#36827;CMR&#27169;&#22411;&#65306;&#65288;i&#65289;CLIP&#65307;&#20197;&#21450;&#65288;ii&#65289;X-VLM&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36873;&#25321;&#20102;&#20004;&#20010;&#20197;&#22330;&#26223;&#20026;&#20013;&#24515;&#30340;&#25968;&#25454;&#38598;&#21644;&#19977;&#20010;&#20197;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#30830;&#23450;&#20102;&#25152;&#36873;&#27169;&#22411;&#22312;&#36825;&#20123;&#25968;&#25454;&#38598;&#19978;&#30340;&#30456;&#23545;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most approaches to cross-modal retrieval (CMR) focus either on object-centric datasets, meaning that each document depicts or describes a single object, or on scene-centric datasets, meaning that each image depicts or describes a complex scene that involves multiple objects and relations between them. We posit that a robust CMR model should generalize well across both dataset types. Despite recent advances in CMR, the reproducibility of the results and their generalizability across different dataset types has not been studied before. We address this gap and focus on the reproducibility of the state-of-the-art CMR results when evaluated on object-centric and scene-centric datasets. We select two state-of-the-art CMR models with different architectures: (i) CLIP; and (ii) X-VLM. Additionally, we select two scene-centric datasets, and three object-centric datasets, and determine the relative performance of the selected models on these datasets. We focus on reproducibility, replicability, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#38754;&#21521;&#25512;&#33616;&#31995;&#32479;&#35780;&#20272;&#30340;&#21487;&#38752;&#29289;&#21697;&#37319;&#26679;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#37319;&#26679;&#20272;&#35745;&#22120;&#65292;&#20248;&#21270;&#35823;&#24046;&#21644;&#29702;&#35770;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2211.15743</link><description>&lt;p&gt;
&#38754;&#21521;&#21487;&#38752;&#30340;&#25512;&#33616;&#35780;&#20272;&#30340;&#25512;&#33616;&#29289;&#21697;&#37319;&#26679;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Reliable Item Sampling for Recommendation Evaluation. (arXiv:2211.15743v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15743
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#38754;&#21521;&#25512;&#33616;&#31995;&#32479;&#35780;&#20272;&#30340;&#21487;&#38752;&#29289;&#21697;&#37319;&#26679;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#37319;&#26679;&#20272;&#35745;&#22120;&#65292;&#20248;&#21270;&#35823;&#24046;&#21644;&#29702;&#35770;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;Rendle&#21644;Krichene&#35748;&#20026;&#24120;&#29992;&#30340;&#22522;&#20110;&#37319;&#26679;&#30340;&#35780;&#20272;&#26041;&#27861;&#19982;&#20840;&#23616;&#24230;&#37327;&#19981;&#19968;&#33268;&#21518;&#65292;&#24050;&#32463;&#26377;&#19968;&#20123;&#20851;&#20110;&#22522;&#20110;&#37319;&#26679;&#30340;&#25512;&#33616;&#31995;&#32479;&#35780;&#20272;&#30340;&#30740;&#31350;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#35201;&#20040;&#23558;&#22522;&#20110;&#37319;&#26679;&#30340;&#24230;&#37327;&#26144;&#23556;&#21040;&#20840;&#23616;&#24230;&#37327;&#65292;&#35201;&#20040;&#26356;&#19968;&#33324;&#22320;&#23398;&#20064;&#32463;&#39564;&#25490;&#21517;&#20998;&#24067;&#26469;&#20272;&#35745;&#21069;K&#24230;&#37327;&#12290;&#20294;&#26159;&#65292;&#23613;&#31649;&#23384;&#22312;&#35768;&#22810;&#21162;&#21147;&#65292;&#20294;&#20173;&#28982;&#32570;&#20047;&#23545;&#25152;&#25552;&#20986;&#30340;&#24230;&#37327;&#20272;&#35745;&#22120;&#30340;&#20005;&#26684;&#29702;&#35770;&#29702;&#35299;&#65292;&#32780;&#22522;&#26412;&#30340;&#29289;&#21697;&#37319;&#26679;&#20063;&#38754;&#20020;&#8220;&#30450;&#28857;&#8221;&#38382;&#39064;&#65292;&#21363;&#24403;K&#24456;&#23567;&#26102;&#65292;&#20272;&#35745;&#24674;&#22797;&#21069;K&#24230;&#37327;&#30340;&#20934;&#30830;&#24615;&#20173;&#28982;&#21487;&#20197;&#30456;&#24403;&#22823;&#12290;&#26412;&#25991;&#28145;&#20837;&#30740;&#31350;&#36825;&#20123;&#38382;&#39064;&#65292;&#24182;&#20316;&#20986;&#20004;&#39033;&#21019;&#26032;&#24615;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29289;&#21697;&#37319;&#26679;&#20272;&#35745;&#22120;&#65292;&#26174;&#24335;&#22320;&#20248;&#21270;&#20102;&#19982;&#22522;&#30784;&#30495;&#23454;&#20540;&#30340;&#35823;&#24046;&#65292;&#24182;&#22312;&#29702;&#35770;&#19978;&#31361;&#26174;&#20102;&#20854;
&lt;/p&gt;
&lt;p&gt;
Since Rendle and Krichene argued that commonly used sampling-based evaluation metrics are "inconsistent" with respect to the global metrics (even in expectation), there have been a few studies on the sampling-based recommender system evaluation. Existing methods try either mapping the sampling-based metrics to their global counterparts or more generally, learning the empirical rank distribution to estimate the top-$K$ metrics. However, despite existing efforts, there is still a lack of rigorous theoretical understanding of the proposed metric estimators, and the basic item sampling also suffers from the "blind spot" issue, i.e., estimation accuracy to recover the top-$K$ metrics when $K$ is small can still be rather substantial. In this paper, we provide an in-depth investigation into these problems and make two innovative contributions. First, we propose a new item-sampling estimator that explicitly optimizes the error with respect to the ground truth, and theoretically highlight its 
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#27880;&#37325;&#20844;&#27491;&#24615;&#30340;&#33402;&#26415;&#23637;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#23616;&#37096;&#32422;&#26463;&#22270;&#21305;&#37197;&#21644;&#20215;&#20540;&#23548;&#21521;&#30340;&#36164;&#28304;&#20998;&#37197;&#65292;&#23454;&#29616;&#20844;&#20849;&#33402;&#26415;&#23637;&#35272;&#30340;&#31574;&#21010;&#12290;&#35813;&#31995;&#32479;&#37319;&#29992;Schelling&#27169;&#22411;&#26500;&#24314;&#25104;&#26412;&#30697;&#38453;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#35780;&#20998;&#20989;&#25968;&#65292;&#36719;&#20998;&#37197;&#33402;&#26415;&#20316;&#21697;&#21040;&#20844;&#20849;&#31354;&#38388;&#65292;&#20197;&#20943;&#23569;&#20869;&#37096;&#32676;&#20307;&#20559;&#22909;&#12289;&#28385;&#36275;&#26368;&#20302;&#20195;&#34920;&#24615;&#21644;&#26333;&#20809;&#26631;&#20934;&#12290;</title><link>http://arxiv.org/abs/2207.14367</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#23616;&#37096;&#32422;&#26463;&#22270;&#21305;&#37197;&#30340;&#27880;&#37325;&#20844;&#27491;&#24615;&#30340;&#33402;&#26415;&#23637;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
An Equity-Aware Recommender System for Curating Art Exhibits Based on Locally-Constrained Graph Matching. (arXiv:2207.14367v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.14367
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#27880;&#37325;&#20844;&#27491;&#24615;&#30340;&#33402;&#26415;&#23637;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#23616;&#37096;&#32422;&#26463;&#22270;&#21305;&#37197;&#21644;&#20215;&#20540;&#23548;&#21521;&#30340;&#36164;&#28304;&#20998;&#37197;&#65292;&#23454;&#29616;&#20844;&#20849;&#33402;&#26415;&#23637;&#35272;&#30340;&#31574;&#21010;&#12290;&#35813;&#31995;&#32479;&#37319;&#29992;Schelling&#27169;&#22411;&#26500;&#24314;&#25104;&#26412;&#30697;&#38453;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#35780;&#20998;&#20989;&#25968;&#65292;&#36719;&#20998;&#37197;&#33402;&#26415;&#20316;&#21697;&#21040;&#20844;&#20849;&#31354;&#38388;&#65292;&#20197;&#20943;&#23569;&#20869;&#37096;&#32676;&#20307;&#20559;&#22909;&#12289;&#28385;&#36275;&#26368;&#20302;&#20195;&#34920;&#24615;&#21644;&#26333;&#20809;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20844;&#20849;&#33402;&#26415;&#22609;&#36896;&#20102;&#25105;&#20204;&#20849;&#20139;&#30340;&#31354;&#38388;&#12290;&#20844;&#20849;&#33402;&#26415;&#24212;&#35813;&#19982;&#31038;&#21306;&#21644;&#32972;&#26223;&#30456;&#20851;&#65292;&#28982;&#32780;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#35768;&#22810;&#30693;&#21517;&#26426;&#26500;&#30340;&#33402;&#26415;&#20316;&#21697;&#20559;&#21521;&#20110;&#36807;&#26102;&#30340;&#25991;&#21270;&#35268;&#33539;&#21644;&#20256;&#32479;&#31038;&#32676;&#12290;&#37492;&#20110;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#29992;&#20110;&#20197;&#20869;&#32622;&#20844;&#27491;&#30446;&#26631;&#21644;&#23616;&#37096;&#22522;&#20110;&#20215;&#20540;&#30340;&#26377;&#38480;&#36164;&#28304;&#20998;&#37197;&#26469;&#31574;&#21010;&#20844;&#20849;&#33402;&#26415;&#23637;&#35272;&#12290;&#25105;&#20204;&#21033;&#29992;Schelling&#30340;&#31181;&#26063;&#20998;&#31163;&#27169;&#22411;&#26500;&#24314;&#20102;&#25104;&#26412;&#30697;&#38453;&#12290;&#20351;&#29992;&#25104;&#26412;&#30697;&#38453;&#20316;&#20026;&#36755;&#20837;&#65292;&#36890;&#36807;&#25237;&#24433;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#24471;&#21040;&#20102;&#19968;&#20010;&#36719;&#20998;&#37197;&#30697;&#38453;&#30340;&#35780;&#20998;&#20989;&#25968;&#12290;&#25105;&#20204;&#30340;&#20248;&#21270;&#31243;&#24207;&#20197;&#19968;&#31181;&#26041;&#24335;&#23558;&#33402;&#26415;&#20316;&#21697;&#20998;&#37197;&#32473;&#20844;&#20849;&#31354;&#38388;&#65292;&#20197;&#38477;&#20302;&#8220;&#20869;&#37096;&#32676;&#20307;&#8221;&#20559;&#22909;&#65292;&#24182;&#28385;&#36275;&#26368;&#20302;&#20195;&#34920;&#24615;&#21644;&#26333;&#20809;&#26631;&#20934;&#12290;&#25105;&#20204;&#20511;&#37492;&#29616;&#26377;&#30340;&#25991;&#29486;&#20026;&#31639;&#27861;&#36755;&#20986;&#24320;&#21457;&#20102;&#19968;&#20010;&#20844;&#27491;&#24615;&#25351;&#26631;&#65292;&#24182;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#25928;&#26524;&#65292;&#24182;&#20174;&#31574;&#23637;&#21644;&#20844;&#27491;&#24615;&#30340;&#35282;&#24230;&#35752;&#35770;&#20854;&#28508;&#22312;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Public art shapes our shared spaces. Public art should speak to community and context, and yet, recent work has demonstrated numerous instances of art in prominent institutions favoring outdated cultural norms and legacy communities. Motivated by this, we develop a novel recommender system to curate public art exhibits with built-in equity objectives and a local value-based allocation of constrained resources. We develop a cost matrix by drawing on Schelling's model of segregation. Using the cost matrix as an input, the scoring function is optimized via a projected gradient descent to obtain a soft assignment matrix. Our optimization program allocates artwork to public spaces in a way that de-prioritizes "in-group" preferences, by satisfying minimum representation and exposure criteria. We draw on existing literature to develop a fairness metric for our algorithmic output, and we assess the effectiveness of our approach and discuss its potential pitfalls from both a curatorial and equi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#22522;&#20110;&#20250;&#35805;&#30340;&#25512;&#33616;&#20219;&#21153;&#65292;&#36890;&#36807;&#21078;&#26512;&#32463;&#20856;&#30340;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25512;&#33616;&#27169;&#22411;&#65292;&#21457;&#29616;&#19968;&#20123;&#22797;&#26434;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#20256;&#25773;&#37096;&#20998;&#26159;&#22810;&#20313;&#30340;&#12290;&#22522;&#20110;&#27492;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Multi-Level Attention Mixture Network (Atten-Mixer)&#65292;&#23427;&#36890;&#36807;&#31227;&#38500;&#22810;&#20313;&#30340;&#20256;&#25773;&#37096;&#20998;&#65292;&#23454;&#29616;&#20102;&#23545;&#35835;&#20986;&#27169;&#22359;&#30340;&#26356;&#39640;&#25928;&#21033;&#29992;&#12290;</title><link>http://arxiv.org/abs/2206.12781</link><description>&lt;p&gt;
&#36890;&#36807;Atten-Mixer&#32593;&#32476;&#39640;&#25928;&#21033;&#29992;&#22810;&#32423;&#29992;&#25143;&#24847;&#22270;&#36827;&#34892;&#22522;&#20110;&#20250;&#35805;&#30340;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Efficiently Leveraging Multi-level User Intent for Session-based Recommendation via Atten-Mixer Network. (arXiv:2206.12781v3 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.12781
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#22522;&#20110;&#20250;&#35805;&#30340;&#25512;&#33616;&#20219;&#21153;&#65292;&#36890;&#36807;&#21078;&#26512;&#32463;&#20856;&#30340;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25512;&#33616;&#27169;&#22411;&#65292;&#21457;&#29616;&#19968;&#20123;&#22797;&#26434;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#20256;&#25773;&#37096;&#20998;&#26159;&#22810;&#20313;&#30340;&#12290;&#22522;&#20110;&#27492;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Multi-Level Attention Mixture Network (Atten-Mixer)&#65292;&#23427;&#36890;&#36807;&#31227;&#38500;&#22810;&#20313;&#30340;&#20256;&#25773;&#37096;&#20998;&#65292;&#23454;&#29616;&#20102;&#23545;&#35835;&#20986;&#27169;&#22359;&#30340;&#26356;&#39640;&#25928;&#21033;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20250;&#35805;&#30340;&#25512;&#33616;&#26088;&#22312;&#26681;&#25454;&#30701;&#26242;&#19988;&#21160;&#24577;&#30340;&#20250;&#35805;&#39044;&#27979;&#29992;&#25143;&#30340;&#19979;&#19968;&#20010;&#21160;&#20316;&#12290;&#26368;&#36817;&#65292;&#22312;&#21033;&#29992;&#21508;&#31181;&#31934;&#24515;&#35774;&#35745;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#25429;&#25417;&#29289;&#21697;&#20043;&#38388;&#30340;&#25104;&#23545;&#20851;&#31995;&#26041;&#38754;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20852;&#36259;&#65292;&#20284;&#20046;&#34920;&#26126;&#35774;&#35745;&#26356;&#22797;&#26434;&#30340;&#27169;&#22411;&#26159;&#25552;&#39640;&#23454;&#35777;&#24615;&#33021;&#30340;&#19975;&#28789;&#33647;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#27169;&#22411;&#22797;&#26434;&#24230;&#21576;&#25351;&#25968;&#22686;&#38271;&#30340;&#21516;&#26102;&#65292;&#20165;&#21462;&#24471;&#20102;&#30456;&#23545;&#36739;&#23567;&#30340;&#25913;&#36827;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21078;&#26512;&#20102;&#32463;&#20856;&#30340;&#22522;&#20110;GNN&#30340;SBR&#27169;&#22411;&#65292;&#24182;&#22312;&#32463;&#39564;&#19978;&#21457;&#29616;&#65292;&#19968;&#20123;&#22797;&#26434;&#30340;GNN&#20256;&#25773;&#22312;&#32473;&#23450;&#35835;&#20986;&#27169;&#22359;&#22312;GNN&#27169;&#22411;&#20013;&#36215;&#21040;&#37325;&#35201;&#20316;&#29992;&#30340;&#24773;&#20917;&#19979;&#26159;&#22810;&#20313;&#30340;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#65292;&#25105;&#20204;&#30452;&#35266;&#22320;&#25552;&#20986;&#20102;&#31227;&#38500;GNN&#20256;&#25773;&#37096;&#20998;&#30340;&#24819;&#27861;&#65292;&#32780;&#35835;&#20986;&#27169;&#22359;&#23558;&#22312;&#27169;&#22411;&#25512;&#29702;&#36807;&#31243;&#20013;&#25215;&#25285;&#26356;&#22810;&#36131;&#20219;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Multi-Level Attention Mixture Network (Atten-Mixer)&#65292;&#23427;&#21516;&#26102;&#21033;&#29992;&#27010;&#24565;-
&lt;/p&gt;
&lt;p&gt;
Session-based recommendation (SBR) aims to predict the user's next action based on short and dynamic sessions. Recently, there has been an increasing interest in utilizing various elaborately designed graph neural networks (GNNs) to capture the pair-wise relationships among items, seemingly suggesting the design of more complicated models is the panacea for improving the empirical performance. However, these models achieve relatively marginal improvements with exponential growth in model complexity. In this paper, we dissect the classical GNN-based SBR models and empirically find that some sophisticated GNN propagations are redundant, given the readout module plays a significant role in GNN-based models. Based on this observation, we intuitively propose to remove the GNN propagation part, while the readout module will take on more responsibility in the model reasoning process. To this end, we propose the Multi-Level Attention Mixture Network (Atten-Mixer), which leverages both concept-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;UNIQORN&#30340;&#38382;&#31572;&#31995;&#32479;&#65292;&#23427;&#33021;&#22815;&#26080;&#32541;&#22320;&#22788;&#29702;RDF&#25968;&#25454;&#21644;&#25991;&#26412;&#65292;&#20351;&#29992;fine-tuned BERT&#27169;&#22411;&#20026;&#38382;&#39064;&#26500;&#24314;&#19978;&#19979;&#25991;&#22270;&#65292;&#24182;&#20351;&#29992;&#22270;&#31639;&#27861;&#30830;&#23450;&#19982;&#38382;&#39064;&#30456;&#20851;&#30340;&#23376;&#22270;&#26469;&#22238;&#31572;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2108.08614</link><description>&lt;p&gt;
UNIQORN&#65306;&#32479;&#19968;&#30340;RDF&#30693;&#35782;&#22270;&#35889;&#19982;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#38382;&#31572;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
UNIQORN: Unified Question Answering over RDF Knowledge Graphs and Natural Language Text. (arXiv:2108.08614v5 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2108.08614
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;UNIQORN&#30340;&#38382;&#31572;&#31995;&#32479;&#65292;&#23427;&#33021;&#22815;&#26080;&#32541;&#22320;&#22788;&#29702;RDF&#25968;&#25454;&#21644;&#25991;&#26412;&#65292;&#20351;&#29992;fine-tuned BERT&#27169;&#22411;&#20026;&#38382;&#39064;&#26500;&#24314;&#19978;&#19979;&#25991;&#22270;&#65292;&#24182;&#20351;&#29992;&#22270;&#31639;&#27861;&#30830;&#23450;&#19982;&#38382;&#39064;&#30456;&#20851;&#30340;&#23376;&#22270;&#26469;&#22238;&#31572;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38382;&#39064;&#22238;&#31572;&#22312;&#30693;&#35782;&#22270;&#35889;&#21644;&#20854;&#20182;RDF&#25968;&#25454;&#19978;&#24050;&#32463;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#36827;&#23637;&#65292;&#35768;&#22810;&#20248;&#31168;&#30340;&#31995;&#32479;&#21487;&#20197;&#20026;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#25110;&#30005;&#25253;&#26597;&#35810;&#25552;&#20379;&#28165;&#26224;&#30340;&#31572;&#26696;&#12290;&#20854;&#20013;&#19968;&#20123;&#31995;&#32479;&#23558;&#25991;&#26412;&#28304;&#20316;&#20026;&#38468;&#21152;&#35777;&#25454;&#32435;&#20837;&#22238;&#31572;&#36807;&#31243;&#65292;&#20294;&#19981;&#33021;&#35745;&#31639;&#20165;&#23384;&#22312;&#20110;&#25991;&#26412;&#20013;&#30340;&#31572;&#26696;&#12290;&#30456;&#21453;&#65292;IR&#21644;NLP&#31038;&#21306;&#30340;&#31995;&#32479;&#24050;&#32463;&#35299;&#20915;&#20102;&#26377;&#20851;&#25991;&#26412;&#30340;QA&#38382;&#39064;&#65292;&#20294;&#26159;&#36825;&#20123;&#31995;&#32479;&#20960;&#20046;&#19981;&#21033;&#29992;&#35821;&#20041;&#25968;&#25454;&#21644;&#30693;&#35782;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#21487;&#20197;&#26080;&#32541;&#25805;&#20316;&#28151;&#21512;RDF&#25968;&#25454;&#38598;&#21644;&#25991;&#26412;&#35821;&#26009;&#24211;&#25110;&#21333;&#20010;&#26469;&#28304;&#30340;&#22797;&#26434;&#38382;&#39064;&#30340;&#31995;&#32479;&#65292;&#22312;&#32479;&#19968;&#26694;&#26550;&#20013;&#36827;&#34892;&#25805;&#20316;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#31216;&#20026;UNIQORN&#65292;&#36890;&#36807;&#20351;&#29992;&#32463;&#36807;&#31934;&#32454;&#35843;&#25972;&#30340;BERT&#27169;&#22411;&#20174;RDF&#25968;&#25454;&#21644;/&#25110;&#25991;&#26412;&#35821;&#26009;&#24211;&#20013;&#26816;&#32034;&#19982;&#38382;&#39064;&#30456;&#20851;&#30340;&#35777;&#25454;&#26469;&#21160;&#24577;&#26500;&#24314;&#19978;&#19979;&#25991;&#22270;&#12290;&#32467;&#26524;&#22270;&#36890;&#24120;&#38750;&#24120;&#20016;&#23500;&#20294;&#39640;&#24230;&#22024;&#26434;&#12290;UNIQORN&#36890;&#36807;&#29992;&#20110;&#32452;Steiner&#26641;&#30340;&#22270;&#31639;&#27861;&#26469;&#22788;&#29702;&#36825;&#20010;&#36755;&#20837;&#65292;&#20174;&#32780;&#30830;&#23450;&#19982;&#38382;&#39064;&#30456;&#20851;&#30340;&#23376;&#22270;&#65292;&#36827;&#32780;&#22238;&#31572;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Question answering over knowledge graphs and other RDF data has been greatly advanced, with a number of good systems providing crisp answers for natural language questions or telegraphic queries. Some of these systems incorporate textual sources as additional evidence for the answering process, but cannot compute answers that are present in text alone. Conversely, systems from the IR and NLP communities have addressed QA over text, but such systems barely utilize semantic data and knowledge. This paper presents the first system for complex questions that can seamlessly operate over a mixture of RDF datasets and text corpora, or individual sources, in a unified framework. Our method, called UNIQORN, builds a context graph on-the-fly, by retrieving question-relevant evidences from the RDF data and/or a text corpus, using fine-tuned BERT models. The resulting graph is typically rich but highly noisy. UNIQORN copes with this input by a graph algorithm for Group Steiner Trees, that identifi
&lt;/p&gt;</description></item></channel></rss>