<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#33258;&#28982;&#28436;&#32462;&#28436;&#31639;&#26469;&#20998;&#26512;&#38382;&#39064;&#31572;&#26696;&#30340;&#22797;&#26434;&#24230;&#65292;&#21010;&#20998;&#20102;&#21069;&#21521;&#12289;&#26597;&#35810;&#21644;&#35268;&#21010;&#19977;&#20010;&#26126;&#30830;&#30340;&#29255;&#27573;&#65292;&#36825;&#26159;&#19968;&#20010;&#20840;&#26032;&#19988;&#29420;&#29305;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.18566</link><description>&lt;p&gt;
&#20449;&#24687;&#26816;&#32034;&#19982;&#32508;&#21512;&#30340;&#22797;&#26434;&#24230;&#31867;&#21035;&#30340;&#20998;&#31867;&#20351;&#29992;&#33258;&#28982;&#36923;&#36753;
&lt;/p&gt;
&lt;p&gt;
A Categorization of Complexity Classes for Information Retrieval and Synthesis Using Natural Logic
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18566
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#33258;&#28982;&#28436;&#32462;&#28436;&#31639;&#26469;&#20998;&#26512;&#38382;&#39064;&#31572;&#26696;&#30340;&#22797;&#26434;&#24230;&#65292;&#21010;&#20998;&#20102;&#21069;&#21521;&#12289;&#26597;&#35810;&#21644;&#35268;&#21010;&#19977;&#20010;&#26126;&#30830;&#30340;&#29255;&#27573;&#65292;&#36825;&#26159;&#19968;&#20010;&#20840;&#26032;&#19988;&#29420;&#29305;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19981;&#26029;&#22686;&#24378;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#20449;&#24687;&#26816;&#32034;&#21464;&#24471;&#26356;&#21152;&#22797;&#26434;&#12290;&#29616;&#20195;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#19981;&#20165;&#21487;&#20197;&#26816;&#32034;&#25991;&#26723;&#65292;&#36824;&#23459;&#31216;&#21487;&#20197;&#26681;&#25454;&#22810;&#31181;&#19981;&#21516;&#25991;&#26723;&#12289;&#30456;&#20114;&#30683;&#30462;&#30340;&#25968;&#25454;&#28304;&#20197;&#21450;&#25512;&#29702;&#26469;&#32508;&#21512;&#24471;&#20986;&#31572;&#26696;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#29992;&#33258;&#28982;&#28436;&#32462;&#28436;&#31639;&#20013;&#30340;&#35777;&#26126;&#35770;&#20316;&#20026;&#20998;&#26512;&#38382;&#39064;&#31572;&#26696;&#22797;&#26434;&#24230;&#30340;&#22522;&#30784;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#20110;&#65292;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#27809;&#26377;&#20154;&#20197;&#27492;&#36923;&#36753;&#20026;&#22797;&#26434;&#24230;&#31867;&#21035;&#22522;&#30784;&#65292;&#21516;&#26102;&#20063;&#27809;&#26377;&#20219;&#20309;&#24050;&#23384;&#22312;&#30340;&#22797;&#26434;&#24230;&#31867;&#21035;&#29992;&#31867;&#20284;&#26041;&#27861;&#21010;&#20998;&#12290;&#25105;&#20204;&#29305;&#21035;&#35782;&#21035;&#20102;&#19977;&#20010;&#26126;&#30830;&#30340;&#29255;&#27573;&#65292;&#31216;&#20026;&#21069;&#21521;&#29255;&#27573;&#12289;&#26597;&#35810;&#29255;&#27573;&#21644;&#35268;&#21010;&#29255;&#27573;&#65292;&#25105;&#20204;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18566v1 Announce Type: new  Abstract: Given the emergent reasoning abilities of large language models, information retrieval is becoming more complex. Rather than just retrieve a document, modern information retrieval systems advertise that they can synthesize an answer based on potentially many different documents, conflicting data sources, and using reasoning. But, different kinds of questions have different answers, and different answers have different complexities. In this paper, we introduce a novel framework for analyzing the complexity of a question answer based on the natural deduction calculus as presented in Prawitz (1965). Our framework is novel both in that no one to our knowledge has used this logic as a basis for complexity classes, and also in that no other existing complexity classes to these have been delineated using any analogous methods either. We identify three decidable fragments in particular called the forward, query and planning fragments, and we com
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#36827;&#34892;&#39044;&#27979;&#26410;&#26469;&#20107;&#20214;&#30340;&#33021;&#21147;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#22411;LM&#31995;&#32479;&#65292;&#36890;&#36807;&#22312;&#31454;&#20105;&#24615;&#39044;&#27979;&#24179;&#21488;&#25910;&#38598;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;&#30693;&#35782;&#25130;&#27490;&#26085;&#26399;&#21518;&#35780;&#20272;&#31995;&#32479;&#24615;&#33021;&#65292;&#21457;&#29616;&#35813;&#31995;&#32479;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#26410;&#26469;&#20107;&#20214;&#24182;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#36229;&#36234;&#20154;&#31867;&#39044;&#27979;&#32773;&#12290;</title><link>https://arxiv.org/abs/2402.18563</link><description>&lt;p&gt;
&#29992;&#35821;&#35328;&#27169;&#22411;&#25509;&#36817;&#20154;&#31867;&#27700;&#24179;&#30340;&#39044;&#27979;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Approaching Human-Level Forecasting with Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18563
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#36827;&#34892;&#39044;&#27979;&#26410;&#26469;&#20107;&#20214;&#30340;&#33021;&#21147;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#22411;LM&#31995;&#32479;&#65292;&#36890;&#36807;&#22312;&#31454;&#20105;&#24615;&#39044;&#27979;&#24179;&#21488;&#25910;&#38598;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;&#30693;&#35782;&#25130;&#27490;&#26085;&#26399;&#21518;&#35780;&#20272;&#31995;&#32479;&#24615;&#33021;&#65292;&#21457;&#29616;&#35813;&#31995;&#32479;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#26410;&#26469;&#20107;&#20214;&#24182;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#36229;&#36234;&#20154;&#31867;&#39044;&#27979;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#26410;&#26469;&#20107;&#20214;&#23545;&#25919;&#31574;&#21644;&#20915;&#31574;&#21046;&#23450;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#35821;&#35328;&#27169;&#22411;(LMs)&#26159;&#21542;&#33021;&#22815;&#22312;&#31454;&#20105;&#24615;&#20154;&#31867;&#39044;&#27979;&#32773;&#30340;&#27700;&#24179;&#19978;&#36827;&#34892;&#39044;&#27979;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#22411;LM&#31995;&#32479;&#65292;&#26088;&#22312;&#33258;&#21160;&#25628;&#32034;&#30456;&#20851;&#20449;&#24687;&#12289;&#29983;&#25104;&#39044;&#27979;&#21644;&#32858;&#21512;&#39044;&#27979;&#12290;&#20026;&#20102;&#20419;&#36827;&#30740;&#31350;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#26469;&#33258;&#31454;&#20105;&#24615;&#39044;&#27979;&#24179;&#21488;&#30340;&#22823;&#37327;&#38382;&#39064;&#25968;&#25454;&#38598;&#12290;&#22312;LM&#30340;&#30693;&#35782;&#25130;&#27490;&#26085;&#26399;&#20043;&#21518;&#21457;&#24067;&#30340;&#27979;&#35797;&#38598;&#19979;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#25105;&#20204;&#31995;&#32479;&#30340;&#31471;&#21040;&#31471;&#24615;&#33021;&#19982;&#20154;&#31867;&#39044;&#27979;&#30340;&#32858;&#21512;&#20043;&#38388;&#30340;&#27604;&#36739;&#12290;&#24179;&#22343;&#32780;&#35328;&#65292;&#35813;&#31995;&#32479;&#25509;&#36817;&#20110;&#31454;&#20105;&#39044;&#27979;&#32773;&#30340;&#32858;&#21512;&#65292;&#24182;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#36229;&#36234;&#20102;&#23427;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#21033;&#29992;LM&#26469;&#39044;&#27979;&#26410;&#26469;&#21487;&#33021;&#20250;&#25552;&#20379;&#20934;&#30830;&#30340;&#22823;&#35268;&#27169;&#39044;&#27979;&#65292;&#24182;&#26377;&#21161;&#20110;&#20026;&#26426;&#26500;&#20915;&#31574;&#25552;&#20379;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18563v1 Announce Type: cross  Abstract: Forecasting future events is important for policy and decision making. In this work, we study whether language models (LMs) can forecast at the level of competitive human forecasters. Towards this goal, we develop a retrieval-augmented LM system designed to automatically search for relevant information, generate forecasts, and aggregate predictions. To facilitate our study, we collect a large dataset of questions from competitive forecasting platforms. Under a test set published after the knowledge cut-offs of our LMs, we evaluate the end-to-end performance of our system against the aggregates of human forecasts. On average, the system nears the crowd aggregate of competitive forecasters, and in some settings surpasses it. Our work suggests that using LMs to forecast the future could provide accurate predictions at scale and help to inform institutional decision making.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22270;&#27491;&#21017;&#21270;&#32534;&#30721;&#22120;&#35757;&#32451;&#26041;&#27861;&#29992;&#20110;&#26497;&#31471;&#20998;&#31867;&#65292;&#22312;&#23454;&#36341;&#20013;&#21457;&#29616;&#20351;&#29992;&#22270;&#25968;&#25454;&#26469;&#35268;&#33539;&#32534;&#30721;&#22120;&#35757;&#32451;&#27604;&#23454;&#26045; GCN &#25928;&#26524;&#26356;&#22909;&#12290;</title><link>https://arxiv.org/abs/2402.18434</link><description>&lt;p&gt;
&#22270;&#27491;&#21017;&#21270;&#32534;&#30721;&#22120;&#35757;&#32451;&#29992;&#20110;&#26497;&#31471;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Graph Regularized Encoder Training for Extreme Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18434
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22270;&#27491;&#21017;&#21270;&#32534;&#30721;&#22120;&#35757;&#32451;&#26041;&#27861;&#29992;&#20110;&#26497;&#31471;&#20998;&#31867;&#65292;&#22312;&#23454;&#36341;&#20013;&#21457;&#29616;&#20351;&#29992;&#22270;&#25968;&#25454;&#26469;&#35268;&#33539;&#32534;&#30721;&#22120;&#35757;&#32451;&#27604;&#23454;&#26045; GCN &#25928;&#26524;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18434v1 &#36890;&#21578;&#31867;&#22411;: &#26032;&#30340; &#25688;&#35201;: &#28145;&#24230;&#26497;&#31471;&#20998;&#31867;&#65288;XC&#65289;&#26088;&#22312;&#35757;&#32451;&#32534;&#30721;&#22120;&#26550;&#26500;&#21644;&#37197;&#22871;&#30340;&#20998;&#31867;&#22120;&#26550;&#26500;&#65292;&#20197;&#20174;&#19968;&#20010;&#38750;&#24120;&#24222;&#22823;&#30340;&#26631;&#31614;&#38598;&#21512;&#20013;&#20026;&#25968;&#25454;&#28857;&#25171;&#19978;&#26368;&#30456;&#20851;&#30340;&#23376;&#26631;&#31614;&#38598;&#21512;&#12290;&#22312;&#25490;&#21517;&#12289;&#25512;&#33616;&#21644;&#26631;&#35760;&#20013;&#24120;&#35265;&#30340;XC&#24212;&#29992;&#20013;&#65292;&#36890;&#24120;&#20250;&#36935;&#21040;&#35757;&#32451;&#25968;&#25454;&#26497;&#23569;&#30340;&#23614;&#26631;&#31614;&#12290;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#25552;&#20379;&#20102;&#19968;&#20010;&#26041;&#20415;&#20294;&#35745;&#31639;&#20195;&#20215;&#39640;&#26114;&#30340;&#26041;&#27861;&#65292;&#21487;&#21033;&#29992;&#20219;&#21153;&#20803;&#25968;&#25454;&#24182;&#22686;&#24378;&#27169;&#22411;&#22312;&#36825;&#20123;&#35774;&#32622;&#20013;&#30340;&#20934;&#30830;&#24615;&#12290;&#26412;&#25991;&#27491;&#24335;&#30830;&#23450;&#20102;&#22312;&#33509;&#24178;&#29992;&#20363;&#20013;&#65292;&#36890;&#36807;&#29992;&#38750;GCN&#26550;&#26500;&#26367;&#25442;GCNs&#65292;&#23436;&#20840;&#21487;&#20197;&#36991;&#20813;GCNs&#30340;&#24040;&#22823;&#35745;&#31639;&#25104;&#26412;&#12290;&#26412;&#25991;&#25351;&#20986;&#65292;&#22312;&#36825;&#20123;&#35774;&#32622;&#20013;&#65292;&#20351;&#29992;&#22270;&#25968;&#25454;&#26469;&#35268;&#33539;&#32534;&#30721;&#22120;&#35757;&#32451;&#27604;&#23454;&#26045;GCN&#26356;&#21152;&#26377;&#25928;&#12290;&#22522;&#20110;&#36825;&#20123;&#35265;&#35299;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#33539;&#24335;RAMEN&#65292;&#29992;&#20110;&#21033;&#29992;XC&#35774;&#32622;&#20013;&#30340;&#22270;&#20803;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18434v1 Announce Type: new  Abstract: Deep extreme classification (XC) aims to train an encoder architecture and an accompanying classifier architecture to tag a data point with the most relevant subset of labels from a very large universe of labels. XC applications in ranking, recommendation and tagging routinely encounter tail labels for which the amount of training data is exceedingly small. Graph convolutional networks (GCN) present a convenient but computationally expensive way to leverage task metadata and enhance model accuracies in these settings. This paper formally establishes that in several use cases, the steep computational cost of GCNs is entirely avoidable by replacing GCNs with non-GCN architectures. The paper notices that in these settings, it is much more effective to use graph data to regularize encoder training than to implement a GCN. Based on these insights, an alternative paradigm RAMEN is presented to utilize graph metadata in XC settings that offers 
&lt;/p&gt;</description></item><item><title>DynaWarp&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25104;&#21592;&#33609;&#22270;&#32467;&#26500;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#22238;&#31572;&#22810;&#38598;&#22810;&#25104;&#21592;&#26597;&#35810;&#65292;&#30456;&#27604;&#20110;&#20498;&#25490;&#32034;&#24341;&#21644;&#25104;&#21592;&#33609;&#22270;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#23384;&#20648;&#25928;&#29575;&#21644;&#26597;&#35810;&#21534;&#21520;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.18355</link><description>&lt;p&gt;
DynaWarp -- &#39640;&#25928;&#30340;&#22823;&#35268;&#27169;&#26085;&#24535;&#23384;&#20648;&#21644;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
DynaWarp -- Efficient, large-scale log storage and retrieval
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18355
&lt;/p&gt;
&lt;p&gt;
DynaWarp&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25104;&#21592;&#33609;&#22270;&#32467;&#26500;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#22238;&#31572;&#22810;&#38598;&#22810;&#25104;&#21592;&#26597;&#35810;&#65292;&#30456;&#27604;&#20110;&#20498;&#25490;&#32034;&#24341;&#21644;&#25104;&#21592;&#33609;&#22270;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#23384;&#20648;&#25928;&#29575;&#21644;&#26597;&#35810;&#21534;&#21520;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#22823;&#35268;&#27169;&#30417;&#25511;&#31995;&#32479;&#24517;&#39035;&#23454;&#26102;&#22788;&#29702;&#21644;&#23384;&#20648;&#22823;&#37327;&#26085;&#24535;&#25968;&#25454;&#12290;&#22312;&#26597;&#35810;&#26102;&#65292;&#31995;&#32479;&#24517;&#39035;&#22522;&#20110;&#26085;&#24535;&#28040;&#24687;&#30340;&#20869;&#23481;&#25214;&#21040;&#30456;&#20851;&#26085;&#24535;&#65292;&#20351;&#29992;&#33021;&#22815;&#25193;&#23637;&#21040;&#36825;&#20123;&#25968;&#25454;&#37327;&#24182;&#19988;&#20173;&#28982;&#39640;&#25928;&#30340;&#25903;&#25345;&#32467;&#26500;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;DynaWarp&#25104;&#21592;&#33609;&#22270;&#65292;&#33021;&#22815;&#22238;&#31572;&#22810;&#38598;&#22810;&#25104;&#21592;&#26597;&#35810;&#65292;&#21487;&#20316;&#20026;&#27969;&#24335;&#26085;&#24535;&#25968;&#25454;&#30340;&#29616;&#26377;&#32034;&#24341;&#32467;&#26500;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;DynaWarp&#25152;&#38656;&#30340;&#23384;&#20648;&#31354;&#38388;&#27604;&#27979;&#35797;&#30340;&#26368;&#20808;&#36827;&#30340;&#20498;&#25490;&#32034;&#24341;&#23569;&#20102;&#39640;&#36798;93&#65285;&#65292;&#35823;&#25253;&#29575;&#27604;&#27979;&#35797;&#30340;&#26368;&#20808;&#36827;&#30340;&#25104;&#21592;&#33609;&#22270;&#23569;&#20102;&#39640;&#36798;&#22235;&#20010;&#25968;&#37327;&#32423;&#12290;&#27492;&#22806;&#65292;DynaWarp&#30340;&#26597;&#35810;&#21534;&#21520;&#37327;&#27604;&#27979;&#35797;&#30340;&#20498;&#25490;&#32034;&#24341;&#39640;&#20986;&#39640;&#36798;250&#20493;&#65292;&#24182;&#19988;&#27604;&#27979;&#35797;&#30340;&#25104;&#21592;&#33609;&#22270;&#39640;&#20986;&#39640;&#36798;240&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18355v1 Announce Type: new  Abstract: Modern, large scale monitoring systems have to process and store vast amounts of log data in near real-time. At query time the systems have to find relevant logs based on the content of the log message using support structures that can scale to these amounts of data while still being efficient to use. We present our novel DynaWarp membership sketch, capable of answering Multi-Set Multi-Membership-Queries, that can be used as an alternative to existing indexing structures for streamed log data. In our experiments, DynaWarp required up to 93% less storage space than the tested state-of-the-art inverted index and had up to four orders of magnitude less false-positives than the tested state-of-the-art membership sketch. Additionally, DynaWarp achieved up to 250 times higher query throughput than the tested inverted index and up to 240 times higher query throughput than the tested membership sketch.
&lt;/p&gt;</description></item><item><title>&#20998;&#26512;&#20102;&#26469;&#33258;Twitter&#30340;&#22810;&#20010;&#31038;&#20132;&#32593;&#32476;&#34920;&#24449;&#65292;&#35777;&#26126;&#20102;&#36890;&#36807;&#20840;&#23616;&#21644;&#23616;&#37096;&#32593;&#32476;&#24230;&#37327;&#25351;&#26631;&#21487;&#20197;&#26377;&#25928;&#26816;&#27979;&#26377;&#20105;&#35758;&#21644;&#26080;&#20105;&#35758;&#30340;&#21453;&#30123;&#33495;&#26631;&#31614;&#21644;&#20851;&#38190;&#35789;</title><link>https://arxiv.org/abs/2402.18335</link><description>&lt;p&gt;
&#22312;Twitter&#19978;&#21033;&#29992;&#22810;&#20010;&#22522;&#20110;&#28040;&#24687;&#30340;&#32593;&#32476;&#34920;&#31034;&#26816;&#27979;&#21453;&#30123;&#33495;&#20869;&#23481;
&lt;/p&gt;
&lt;p&gt;
Detecting Anti-vaccine Content on Twitter using Multiple Message-Based Network Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18335
&lt;/p&gt;
&lt;p&gt;
&#20998;&#26512;&#20102;&#26469;&#33258;Twitter&#30340;&#22810;&#20010;&#31038;&#20132;&#32593;&#32476;&#34920;&#24449;&#65292;&#35777;&#26126;&#20102;&#36890;&#36807;&#20840;&#23616;&#21644;&#23616;&#37096;&#32593;&#32476;&#24230;&#37327;&#25351;&#26631;&#21487;&#20197;&#26377;&#25928;&#26816;&#27979;&#26377;&#20105;&#35758;&#21644;&#26080;&#20105;&#35758;&#30340;&#21453;&#30123;&#33495;&#26631;&#31614;&#21644;&#20851;&#38190;&#35789;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#22914;Twitter&#22312;&#36890;&#36807;&#36716;&#25512;&#21644;&#22238;&#22797;&#30340;&#27010;&#24565;&#20419;&#36827;&#22312;&#32447;&#35266;&#28857;&#30340;&#20256;&#25773;&#21644;&#35752;&#35770;&#26041;&#38754;&#36215;&#30528;&#22522;&#30784;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#21151;&#33021;&#20063;&#26377;&#21161;&#20110;&#22312;COVID-19&#22823;&#27969;&#34892;&#30340;&#30123;&#33495;&#25512;&#20986;&#26399;&#38388;&#20256;&#25773;&#35823;&#23548;&#20449;&#24687;&#12290;&#25105;&#20204;&#20197;COVID-19&#30123;&#33495;&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;&#20998;&#26512;&#20102;&#26469;&#33258;Twitter&#19978;&#19977;&#31181;&#22522;&#20110;&#28040;&#24687;&#30340;&#20114;&#21160;&#30340;&#22810;&#20010;&#31038;&#20132;&#32593;&#32476;&#34920;&#24449;&#65288;&#24341;&#36848;&#36716;&#25512;&#12289;&#25552;&#21450;&#21644;&#22238;&#22797;&#65289;&#65292;&#22522;&#20110;&#19968;&#32452;&#24050;&#30693;&#30340;&#21453;&#30123;&#33495;&#26631;&#31614;&#21644;&#20851;&#38190;&#35789;&#12290;&#27599;&#20010;&#32593;&#32476;&#20195;&#34920;&#20102;&#19968;&#20010;&#29305;&#23450;&#30340;&#26631;&#31614;&#25110;&#20851;&#38190;&#35789;&#65292;&#26681;&#25454;&#19968;&#23567;&#32452;&#21442;&#19982;&#32773;&#30340;&#26631;&#35760;&#65292;&#36825;&#20123;&#26631;&#31614;&#34987;&#26631;&#35760;&#20026;&#8220;&#26377;&#20105;&#35758;&#30340;&#8221;&#21644;&#8220;&#26080;&#20105;&#35758;&#30340;&#8221;&#12290;&#23545;&#20110;&#27599;&#20010;&#32593;&#32476;&#65292;&#25105;&#20204;&#25552;&#21462;&#20102;&#19968;&#32452;&#20840;&#23616;&#21644;&#23616;&#37096;&#22522;&#20110;&#32593;&#32476;&#30340;&#24230;&#37327;&#65292;&#23558;&#20854;&#29992;&#20316;&#20108;&#36827;&#21046;&#20998;&#31867;&#30340;&#29305;&#24449;&#21521;&#37327;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#21487;&#33021;&#36890;&#36807;&#36825;&#20123;&#29305;&#24449;&#21521;&#37327;&#26469;&#26816;&#27979;&#26377;&#20105;&#35758;&#19982;&#26080;&#20105;&#35758;&#30340;&#26415;&#35821;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18335v1 Announce Type: cross  Abstract: Social media platforms such as Twitter have a fundamental role in facilitating the spread and discussion of ideas online through the concept of retweeting and replying. However, these features also contribute to the spread of mis/disinformation during the vaccine rollout of the COVID-19 pandemic. Using COVID-19 vaccines as a case study, we analyse multiple social network representation derived from three message-based interactions on Twitter (quote retweets, mentions and replies) based upon a set of known anti-vax hashtags and keywords. Each network represents a certain hashtag or keyword which were labelled as "controversial" and "non-controversial" according to a small group of participants. For each network, we extract a combination of global and local network-based metrics which are used as feature vectors for binary classification. Our results suggest that it is possible to detect controversial from non-controversial terms with hi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#24179;&#21488;&#30340;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;Rec4Agentverse&#65292;&#24378;&#35843;&#20195;&#29702;&#39033;&#21644;&#20195;&#29702;&#25512;&#33616;&#22120;&#20043;&#38388;&#30340;&#21512;&#20316;&#65292;&#20419;&#36827;&#20010;&#24615;&#21270;&#20449;&#24687;&#26381;&#21153;&#65292;&#25552;&#21319;&#20449;&#24687;&#20132;&#25442;&#65292;&#24182;&#23637;&#26395;&#20102;&#20854;&#28436;&#36827;&#20026;&#25903;&#25345;&#20114;&#21160;&#21644;&#20449;&#24687;&#20132;&#25442;&#30340;&#19977;&#20010;&#38454;&#27573;</title><link>https://arxiv.org/abs/2402.18240</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20195;&#29702;&#24179;&#21488;&#19978;&#30340;&#21069;&#26223;&#20010;&#24615;&#21270;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Prospect Personalized Recommendation on Large Language Model-based Agent Platform
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18240
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#24179;&#21488;&#30340;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;Rec4Agentverse&#65292;&#24378;&#35843;&#20195;&#29702;&#39033;&#21644;&#20195;&#29702;&#25512;&#33616;&#22120;&#20043;&#38388;&#30340;&#21512;&#20316;&#65292;&#20419;&#36827;&#20010;&#24615;&#21270;&#20449;&#24687;&#26381;&#21153;&#65292;&#25552;&#21319;&#20449;&#24687;&#20132;&#25442;&#65292;&#24182;&#23637;&#26395;&#20102;&#20854;&#28436;&#36827;&#20026;&#25903;&#25345;&#20114;&#21160;&#21644;&#20449;&#24687;&#20132;&#25442;&#30340;&#19977;&#20010;&#38454;&#27573;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#22411;&#20195;&#29702;&#23548;&#21521;&#20449;&#24687;&#31995;&#32479;&#65292;&#20197;GPT&#20026;&#20363;&#65292;&#20419;&#20351;&#25105;&#20204;&#23457;&#35270;&#20449;&#24687;&#31995;&#32479;&#22522;&#30784;&#35774;&#26045;&#65292;&#20197;&#25903;&#25345;&#20195;&#29702;&#32423;&#20449;&#24687;&#22788;&#29702;&#24182;&#36866;&#24212;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20195;&#29702;&#30340;&#29305;&#24449;&#65292;&#22914;&#20114;&#21160;&#24615;&#12290;&#26412;&#30740;&#31350;&#23637;&#26395;&#20102;&#22522;&#20110;LLM&#20195;&#29702;&#24179;&#21488;&#30340;&#25512;&#33616;&#31995;&#32479;&#30340;&#21069;&#26223;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;Rec4Agentverse&#30340;&#26032;&#22411;&#25512;&#33616;&#33539;&#24335;&#65292;&#21253;&#25324;&#20195;&#29702;&#39033;&#21644;&#20195;&#29702;&#25512;&#33616;&#22120;&#12290;Rec4Agentverse&#24378;&#35843;&#20195;&#29702;&#39033;&#21644;&#20195;&#29702;&#25512;&#33616;&#22120;&#20043;&#38388;&#30340;&#21512;&#20316;&#65292;&#20174;&#32780;&#20419;&#36827;&#20010;&#24615;&#21270;&#20449;&#24687;&#26381;&#21153;&#65292;&#24182;&#22686;&#24378;&#20449;&#24687;&#20132;&#25442;&#65292;&#36229;&#36234;&#20256;&#32479;&#30340;&#29992;&#25143;-&#25512;&#33616;&#22120;&#21453;&#39304;&#24490;&#29615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#26395;&#20102;Rec4Agentverse&#30340;&#28436;&#36827;&#65292;&#24182;&#23558;&#20854;&#27010;&#24565;&#21270;&#20026;&#22522;&#20110;&#20195;&#29702;&#39033;&#12289;&#20195;&#29702;&#25512;&#33616;&#22120;&#21644;&#29992;&#25143;&#20043;&#38388;&#20114;&#21160;&#21644;&#20449;&#24687;&#20132;&#25442;&#22686;&#24378;&#30340;&#19977;&#20010;&#38454;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18240v1 Announce Type: cross  Abstract: The new kind of Agent-oriented information system, exemplified by GPTs, urges us to inspect the information system infrastructure to support Agent-level information processing and to adapt to the characteristics of Large Language Model (LLM)-based Agents, such as interactivity. In this work, we envisage the prospect of the recommender system on LLM-based Agent platforms and introduce a novel recommendation paradigm called Rec4Agentverse, comprised of Agent Items and Agent Recommender. Rec4Agentverse emphasizes the collaboration between Agent Items and Agent Recommender, thereby promoting personalized information services and enhancing the exchange of information beyond the traditional user-recommender feedback loop. Additionally, we prospect the evolution of Rec4Agentverse and conceptualize it into three stages based on the enhancement of the interaction and information exchange among Agent Items, Agent Recommender, and the user. A pre
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#36890;&#36807;&#24207;&#21015;&#32423;&#35821;&#20041;&#34701;&#21512;&#30340;&#26041;&#27861;&#65292;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#26356;&#22909;&#22320;&#25972;&#21512;&#25991;&#26412;&#21644;ID&#29305;&#24449;&#65292;&#25552;&#39640;&#25512;&#33616;&#24615;&#33021;</title><link>https://arxiv.org/abs/2402.18166</link><description>&lt;p&gt;
&#24207;&#21015;&#32423;&#35821;&#20041;&#34920;&#31034;&#34701;&#21512;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Sequence-level Semantic Representation Fusion for Recommender Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18166
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#36890;&#36807;&#24207;&#21015;&#32423;&#35821;&#20041;&#34701;&#21512;&#30340;&#26041;&#27861;&#65292;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#26356;&#22909;&#22320;&#25972;&#21512;&#25991;&#26412;&#21644;ID&#29305;&#24449;&#65292;&#25552;&#39640;&#25512;&#33616;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25512;&#33616;&#31995;&#32479;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#21487;&#20197;&#21033;&#29992;&#30340;&#36741;&#21161;&#20449;&#24687;&#36234;&#26469;&#36234;&#22810;&#65292;&#20197;&#25552;&#39640;&#25512;&#33616;&#24615;&#33021;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#20851;&#27880;&#39033;&#30446;&#30340;&#20851;&#32852;\emph{&#25991;&#26412;&#25968;&#25454;}&#30340;&#21033;&#29992;&#65288;&#20363;&#22914;&#20135;&#21697;&#26631;&#39064;&#65289;&#65292;&#24182;&#30740;&#31350;&#25991;&#26412;&#29305;&#24449;&#22914;&#20309;&#19982;ID&#29305;&#24449;&#22312;&#39034;&#24207;&#25512;&#33616;&#20013;&#26377;&#25928;&#34701;&#21512;&#12290;&#28982;&#32780;&#65292;&#36825;&#20004;&#31181;&#39033;&#30446;&#29305;&#24449;&#23384;&#22312;&#26126;&#26174;&#30340;&#25968;&#25454;&#29305;&#24449;&#65292;&#20351;&#24471;&#30452;&#25509;&#34701;&#21512;&#26041;&#27861;&#65288;&#20363;&#22914;&#23558;&#25991;&#26412;&#21644;ID&#23884;&#20837;&#20316;&#20026;&#39033;&#30446;&#34920;&#31034;&#36827;&#34892;&#30456;&#21152;&#65289;&#21464;&#24471;&#19981;&#22826;&#26377;&#25928;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38754;&#21521;&#39034;&#24207;\ul \emph{&#25512;&#33616;}&#30340;{\ul \emph{Rec}}&#30340;{\bf \&#25105;&#20204;}&#30340;{\bf \&#25105;&#20204;}&#30340;{\bf \&#25105;&#20204;}&#30340;{\bf \&#25105;&#20204;}&#30340;{\bf \&#25105;&#20204;}&#30340;{\bf \&#25105;&#20204;}&#30340;{\bf \&#25105;&#20204;}&#30340;{\bf &#25105;&#20204;&#30340;}&#30340;&#21547;&#20041;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#36890;&#36807;&#26356;&#22909;&#22320;&#25972;&#21512;&#20840;&#23616;&#19978;&#19979;&#25991;&#26469;&#36827;&#34892;&#24207;&#21015;&#32423;&#35821;&#20041;&#34701;&#21512;&#26041;&#27861;&#12290;&#20851;&#38190;&#31574;&#30053;&#22312;&#20110;&#25105;&#20204;&#36890;&#36807;&#20613;&#37324;&#21494;&#21464;&#25442;&#23545;&#25991;&#26412;&#23884;&#20837;&#21644;ID&#23884;&#20837;&#36827;&#34892;&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18166v1 Announce Type: new  Abstract: With the rapid development of recommender systems, there is increasing side information that can be employed to improve the recommendation performance. Specially, we focus on the utilization of the associated \emph{textual data} of items (eg product title) and study how text features can be effectively fused with ID features in sequential recommendation. However, there exists distinct data characteristics for the two kinds of item features, making a direct fusion method (eg adding text and ID embeddings as item representation) become less effective. To address this issue, we propose a novel {\ul \emph{Te}}xt-I{\ul \emph{D}} semantic fusion approach for sequential {\ul \emph{Rec}}ommendation, namely \textbf{\our}. The core idea of our approach is to conduct a sequence-level semantic fusion approach by better integrating global contexts. The key strategy lies in that we transform the text embeddings and ID embeddings by Fourier Transform f
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20449;&#24687;&#27969;&#30340;&#35270;&#35282;&#35299;&#37322;&#24182;&#24178;&#39044;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#30693;&#35782;&#20914;&#31361;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Pruning Head via PatH&#30340;&#26041;&#27861;&#26469;&#32531;&#35299;&#20914;&#31361;</title><link>https://arxiv.org/abs/2402.18154</link><description>&lt;p&gt;
&#20999;&#26029;&#22836;&#37096;&#32456;&#32467;&#20914;&#31361;&#65306;&#35299;&#37322;&#21644;&#32531;&#35299;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#30693;&#35782;&#20914;&#31361;&#30340;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Cutting Off the Head Ends the Conflict: A Mechanism for Interpreting and Mitigating Knowledge Conflicts in Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18154
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20449;&#24687;&#27969;&#30340;&#35270;&#35282;&#35299;&#37322;&#24182;&#24178;&#39044;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#30693;&#35782;&#20914;&#31361;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Pruning Head via PatH&#30340;&#26041;&#27861;&#26469;&#32531;&#35299;&#20914;&#31361;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#26816;&#32034;&#22686;&#24378;&#21644;&#24037;&#20855;&#22686;&#24378;&#23637;&#31034;&#20102;&#36890;&#36807;&#25552;&#20379;&#22806;&#37096;&#19978;&#19979;&#25991;&#26469;&#25193;&#23637;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#30340;&#20869;&#37096;&#35760;&#24518;&#36793;&#30028;&#30340;&#26174;&#33879;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20869;&#37096;&#35760;&#24518;&#21644;&#22806;&#37096;&#19978;&#19979;&#25991;&#19981;&#21487;&#36991;&#20813;&#22320;&#21457;&#29983;&#20914;&#31361;&#65292;&#23548;&#33268;LMs&#20869;&#37096;&#20986;&#29616;&#30693;&#35782;&#20914;&#31361;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#20449;&#24687;&#27969;&#30340;&#35270;&#35282;&#35299;&#37322;&#30693;&#35782;&#20914;&#31361;&#30340;&#26426;&#21046;&#65292;&#28982;&#21518;&#36890;&#36807;&#22312;&#20851;&#38190;&#28857;&#36827;&#34892;&#31934;&#30830;&#24178;&#39044;&#26469;&#32531;&#35299;&#20914;&#31361;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#21518;&#32493;&#23618;&#20013;&#26377;&#19968;&#20123;&#20855;&#26377;&#30456;&#21453;&#25928;&#26524;&#30340;&#27880;&#24847;&#21147;&#22836;&#65292;&#20854;&#20013;&#20869;&#23384;&#22836;&#21487;&#20197;&#20174;&#20869;&#37096;&#35760;&#24518;&#20013;&#21484;&#22238;&#30693;&#35782;&#65292;&#32780;&#19978;&#19979;&#25991;&#22836;&#21487;&#20197;&#20174;&#22806;&#37096;&#19978;&#19979;&#25991;&#20013;&#26816;&#32034;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;LMs&#20013;&#30693;&#35782;&#20914;&#31361;&#21457;&#29983;&#30340;&#20851;&#38190;&#28857;&#26159;&#20869;&#23384;&#22836;&#21644;&#19978;&#19979;&#25991;&#22836;&#25972;&#21512;&#19981;&#19968;&#33268;&#20449;&#24687;&#27969;&#30340;&#22320;&#26041;&#12290;&#21463;&#21040;&#36825;&#20123;&#35265;&#35299;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Pruning Head via PatH&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18154v1 Announce Type: cross  Abstract: Recently, retrieval augmentation and tool augmentation have demonstrated a remarkable capability to expand the internal memory boundaries of language models (LMs) by providing external context. However, internal memory and external context inevitably clash, leading to knowledge conflicts within LMs. In this paper, we aim to interpret the mechanism of knowledge conflicts through the lens of information flow, and then mitigate conflicts by precise interventions at the pivotal point. We find there are some attention heads with opposite effects in the later layers, where memory heads can recall knowledge from internal memory, and context heads can retrieve knowledge from external context. Moreover, we reveal that the pivotal point at which knowledge conflicts emerge in LMs is the integration of inconsistent information flows by memory heads and context heads. Inspired by the insights, we propose a novel method called Pruning Head via PatH 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;InFO-RAG&#30340;&#26080;&#30417;&#30563;&#20449;&#24687;&#32454;&#21270;&#35757;&#32451;&#26041;&#27861;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#20013;&#30340;&#35282;&#33394;&#23450;&#20041;&#20026;&#8220;&#20449;&#24687;&#32454;&#21270;&#32773;&#8221;&#65292;&#24110;&#21161;&#27169;&#22411;&#26356;&#22909;&#22320;&#25972;&#21512;&#26816;&#32034;&#20449;&#24687;&#20197;&#29983;&#25104;&#26356;&#21152;&#31616;&#27905;&#12289;&#20934;&#30830;&#21644;&#23436;&#25972;&#30340;&#25991;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.18150</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26080;&#30417;&#30563;&#20449;&#24687;&#32454;&#21270;&#35757;&#32451;&#29992;&#20110;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Information Refinement Training of Large Language Models for Retrieval-Augmented Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18150
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;InFO-RAG&#30340;&#26080;&#30417;&#30563;&#20449;&#24687;&#32454;&#21270;&#35757;&#32451;&#26041;&#27861;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#20013;&#30340;&#35282;&#33394;&#23450;&#20041;&#20026;&#8220;&#20449;&#24687;&#32454;&#21270;&#32773;&#8221;&#65292;&#24110;&#21161;&#27169;&#22411;&#26356;&#22909;&#22320;&#25972;&#21512;&#26816;&#32034;&#20449;&#24687;&#20197;&#29983;&#25104;&#26356;&#21152;&#31616;&#27905;&#12289;&#20934;&#30830;&#21644;&#23436;&#25972;&#30340;&#25991;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#36890;&#36807;&#23558;&#26469;&#33258;&#26816;&#32034;&#30340;&#39069;&#22806;&#20449;&#24687;&#25972;&#21512;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#65292;&#20174;&#32780;&#22686;&#24378;&#20854;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#34920;&#26126;&#65292;LLMs&#22312;&#26377;&#25928;&#21033;&#29992;&#26816;&#32034;&#20449;&#24687;&#26041;&#38754;&#20173;&#28982;&#38754;&#20020;&#25361;&#25112;&#65292;&#26377;&#26102;&#20250;&#24573;&#35270;&#25110;&#34987;&#38169;&#35823;&#24341;&#23548;&#12290;&#20854;&#20851;&#38190;&#21407;&#22240;&#22312;&#20110;LLMs&#30340;&#35757;&#32451;&#27809;&#26377;&#28165;&#26224;&#22320;&#35753;LLMs&#23398;&#20250;&#22914;&#20309;&#21033;&#29992;&#20855;&#26377;&#19981;&#21516;&#36136;&#37327;&#30340;&#26816;&#32034;&#25991;&#26412;&#36755;&#20837;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#35270;&#35282;&#65292;&#23558;LLMs&#22312;RAG&#20013;&#30340;&#35282;&#33394;&#35270;&#20026;&#8220;&#20449;&#24687;&#32454;&#21270;&#32773;&#8221;&#65292;&#36825;&#24847;&#21619;&#30528;&#26080;&#35770;&#26816;&#32034;&#25991;&#26412;&#30340;&#27491;&#30830;&#24615;&#12289;&#23436;&#25972;&#24615;&#25110;&#26377;&#29992;&#24615;&#22914;&#20309;&#65292;LLMs&#37117;&#33021;&#19968;&#33268;&#22320;&#25972;&#21512;&#26816;&#32034;&#25991;&#26412;&#20013;&#30340;&#30693;&#35782;&#21644;&#27169;&#22411;&#21442;&#25968;&#65292;&#29983;&#25104;&#27604;&#26816;&#32034;&#25991;&#26412;&#26356;&#31616;&#27905;&#12289;&#20934;&#30830;&#21644;&#23436;&#25972;&#30340;&#25991;&#26412;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;InFO-RAG&#30340;&#20449;&#24687;&#32454;&#21270;&#35757;&#32451;&#26041;&#27861;&#65292;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#20248;&#21270;LLMs&#29992;&#20110;RAG&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18150v1 Announce Type: cross  Abstract: Retrieval-augmented generation (RAG) enhances large language models (LLMs) by incorporating additional information from retrieval. However, studies have shown that LLMs still face challenges in effectively using the retrieved information, even ignoring it or being misled by it. The key reason is that the training of LLMs does not clearly make LLMs learn how to utilize input retrieved texts with varied quality. In this paper, we propose a novel perspective that considers the role of LLMs in RAG as ``Information Refiner'', which means that regardless of correctness, completeness, or usefulness of retrieved texts, LLMs can consistently integrate knowledge within the retrieved texts and model parameters to generate the texts that are more concise, accurate, and complete than the retrieved texts. To this end, we propose an information refinement training method named InFO-RAG that optimizes LLMs for RAG in an unsupervised manner. InFO-RAG i
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#35821;&#26009;&#24211;&#24341;&#23548;&#30340;&#26597;&#35810;&#25193;&#23637;&#65288;CSQE&#65289;&#65292;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#22686;&#24378;&#25193;&#23637;&#65292;&#25913;&#21892;&#20102;&#26597;&#35810;&#19982;&#30446;&#26631;&#25991;&#26723;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2402.18031</link><description>&lt;p&gt;
&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24341;&#23548;&#30340;&#35821;&#26009;&#24211;&#26597;&#35810;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
Corpus-Steered Query Expansion with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18031
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#35821;&#26009;&#24211;&#24341;&#23548;&#30340;&#26597;&#35810;&#25193;&#23637;&#65288;CSQE&#65289;&#65292;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#22686;&#24378;&#25193;&#23637;&#65292;&#25913;&#21892;&#20102;&#26597;&#35810;&#19982;&#30446;&#26631;&#25991;&#26723;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#30340;&#26597;&#35810;&#25193;&#23637;&#21487;&#20197;&#36890;&#36807;&#29983;&#25104;&#33021;&#22815;&#22238;&#31572;&#26597;&#35810;&#30340;&#20551;&#35774;&#25991;&#26723;&#32780;&#26174;&#30528;&#22686;&#24378;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;LLMs&#30340;&#26377;&#38480;&#20869;&#22312;&#30693;&#35782;&#65292;&#25193;&#23637;&#19982;&#26816;&#32034;&#35821;&#26009;&#24211;&#20043;&#38388;&#30340;&#19981;&#23545;&#40784;&#23548;&#33268;&#38382;&#39064;&#65292;&#22914;&#24187;&#35273;&#21644;&#36807;&#26102;&#20449;&#24687;&#12290;&#21463;&#20266;&#30456;&#20851;&#21453;&#39304;&#65288;PRF&#65289;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#35821;&#26009;&#24211;&#24341;&#23548;&#30340;&#26597;&#35810;&#25193;&#23637;&#65288;CSQE&#65289;&#26469;&#20419;&#36827;&#23884;&#20837;&#22312;&#35821;&#26009;&#24211;&#20013;&#30340;&#30693;&#35782;&#30340;&#25972;&#21512;&#12290;CSQE&#21033;&#29992;LLMs&#30340;&#30456;&#20851;&#24615;&#35780;&#20272;&#33021;&#21147;&#31995;&#32479;&#22320;&#35782;&#21035;&#26368;&#21021;&#26816;&#32034;&#21040;&#30340;&#25991;&#26723;&#20013;&#30340;&#20851;&#38190;&#21477;&#12290;&#36825;&#20123;&#30001;&#35821;&#26009;&#24211;&#20135;&#29983;&#30340;&#25991;&#26412;&#38543;&#21518;&#19982;LLM&#30693;&#35782;&#22686;&#24378;&#25193;&#23637;&#19968;&#36215;&#29992;&#20110;&#25193;&#23637;&#26597;&#35810;&#65292;&#25913;&#21892;&#26597;&#35810;&#19982;&#30446;&#26631;&#25991;&#26723;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#39044;&#27979;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;CSQE&#26126;&#26174;&#25552;&#39640;&#20102;&#20449;&#24687;&#26816;&#32034;&#32467;&#26524;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18031v1 Announce Type: cross  Abstract: Recent studies demonstrate that query expansions generated by large language models (LLMs) can considerably enhance information retrieval systems by generating hypothetical documents that answer the queries as expansions. However, challenges arise from misalignments between the expansions and the retrieval corpus, resulting in issues like hallucinations and outdated information due to the limited intrinsic knowledge of LLMs. Inspired by Pseudo Relevance Feedback (PRF), we introduce Corpus-Steered Query Expansion (CSQE) to promote the incorporation of knowledge embedded within the corpus. CSQE utilizes the relevance assessing capability of LLMs to systematically identify pivotal sentences in the initially-retrieved documents. These corpus-originated texts are subsequently used to expand the query together with LLM-knowledge empowered expansions, improving the relevance prediction between the query and the target documents. Extensive exp
&lt;/p&gt;</description></item><item><title>&#22797;&#21046;&#24182;&#25193;&#23637;&#20102;&#20351;&#29992;&#20010;&#24615;&#21270;&#21830;&#21697;&#39057;&#29575;&#20449;&#24687;&#36827;&#34892;&#19979;&#19968;&#20010;&#36141;&#29289;&#31726;&#25512;&#33616;&#30340;&#35770;&#25991;&#30740;&#31350;&#65292;&#21033;&#29992;&#22810;&#20010;&#25968;&#25454;&#38598;&#39564;&#35777;&#20102;TIFU-KNN&#27169;&#22411;&#30340;&#20248;&#36234;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#26032;&#30340;$\beta$-VAE&#26550;&#26500;&#26469;&#24314;&#27169;NBR&#12290;</title><link>https://arxiv.org/abs/2402.17925</link><description>&lt;p&gt;
[RE] &#23545;&#19979;&#19968;&#20010;&#36141;&#29289;&#31726;&#25512;&#33616;&#24314;&#27169;&#20010;&#24615;&#21270;&#21830;&#21697;&#39057;&#29575;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
[RE] Modeling Personalized Item Frequency Information for Next-basket Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17925
&lt;/p&gt;
&lt;p&gt;
&#22797;&#21046;&#24182;&#25193;&#23637;&#20102;&#20351;&#29992;&#20010;&#24615;&#21270;&#21830;&#21697;&#39057;&#29575;&#20449;&#24687;&#36827;&#34892;&#19979;&#19968;&#20010;&#36141;&#29289;&#31726;&#25512;&#33616;&#30340;&#35770;&#25991;&#30740;&#31350;&#65292;&#21033;&#29992;&#22810;&#20010;&#25968;&#25454;&#38598;&#39564;&#35777;&#20102;TIFU-KNN&#27169;&#22411;&#30340;&#20248;&#36234;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#26032;&#30340;$\beta$-VAE&#26550;&#26500;&#26469;&#24314;&#27169;NBR&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#26088;&#22312;&#22797;&#21046;&#21644;&#25193;&#23637;&#35770;&#25991;&#12298;&#23545;&#19979;&#19968;&#20010;&#36141;&#29289;&#31726;&#25512;&#33616;&#24314;&#27169;&#20010;&#24615;&#21270;&#21830;&#21697;&#39057;&#29575;&#20449;&#24687;&#12299;&#65292;&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;TIFU-KNN&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#21033;&#29992;&#20010;&#24615;&#21270;&#21830;&#21697;&#39057;&#29575; (PIF) &#36827;&#34892;&#19979;&#19968;&#20010;&#36141;&#29289;&#31726;&#25512;&#33616; (NBR)&#12290;&#25105;&#20204;&#21033;&#29992;&#20102;&#21407;&#22987;&#35770;&#25991;&#20013;&#20351;&#29992;&#30340;&#20844;&#24320;&#21487;&#29992;&#30340;&#39135;&#21697;&#26434;&#36135;&#36141;&#29289;&#25968;&#25454;&#38598;&#65292;&#24182;&#32467;&#21512;&#20854;&#20182;&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#30740;&#31350;&#32467;&#26524;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#21508;&#31181;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324; Recall@K&#12289;NDCG@K&#12289;&#20010;&#24615;&#21270;&#21629;&#20013;&#29575; (PHR) &#21644;&#24179;&#22343;&#20498;&#25968;&#25490;&#21517; (MRR) &#31561;&#25351;&#26631;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#29992;&#25143;&#29305;&#24449;&#65292;&#22914;&#24179;&#22343;&#36141;&#29289;&#31726;&#22823;&#23567;&#12289;&#21830;&#21697;&#21463;&#27426;&#36814;&#31243;&#24230;&#21644;&#26032;&#39062;&#24615;&#65292;&#20174;&#32780;&#23545;&#20844;&#24179;&#24615;&#36827;&#34892;&#20102;&#24443;&#24213;&#26816;&#26597;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26032;&#39062;&#30340; $\beta$-VAE &#26550;&#26500;&#26469;&#24314;&#27169; NBR&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22797;&#21046;&#20986;&#30340;&#27169;&#22411; TIFU-KNN &#32988;&#36807;&#22522;&#32447;&#27169;&#22411; Personal Top Frequency&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17925v1 Announce Type: new  Abstract: This paper focuses on reproducing and extending the results of the paper: "Modeling Personalized Item Frequency Information for Next-basket Recommendation" which introduced the TIFU-KNN model and proposed to utilize Personalized Item Frequency (PIF) for Next Basket Recommendation (NBR). We utilized publicly available grocery shopping datasets used in the original paper and incorporated additional datasets to assess the generalizability of the findings. We evaluated the performance of the models using metrics such as Recall@K, NDCG@K, personalized-hit ratio (PHR), and Mean Reciprocal Rank (MRR). Furthermore, we conducted a thorough examination of fairness by considering user characteristics such as average basket size, item popularity, and novelty. Lastly, we introduced novel $\beta$-VAE architecture to model NBR. The experimental results confirmed that the reproduced model, TIFU-KNN, outperforms the baseline model, Personal Top Frequency
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#30340;&#26032;&#27010;&#24565;&#25554;&#20837;&#21040;&#26412;&#20307;&#20013;&#65292;&#22312;&#36793;&#25628;&#32034;&#12289;&#36793;&#24418;&#25104;&#21644;&#22686;&#24378;&#12289;&#36793;&#36873;&#25321;&#19977;&#20010;&#27493;&#39588;&#20013;&#20998;&#21035;&#21033;&#29992;&#31070;&#32463;&#26041;&#27861;&#65292;&#24182;&#22312; SNOMED CT &#26412;&#20307;&#21644; MedMentions &#23454;&#20307;&#38142;&#25509;&#22522;&#20934;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;</title><link>https://arxiv.org/abs/2402.17897</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#26412;&#20307;&#35770;&#20013;&#26032;&#27010;&#24565;&#25918;&#32622;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Language Model based Framework for New Concept Placement in Ontologies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17897
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#30340;&#26032;&#27010;&#24565;&#25554;&#20837;&#21040;&#26412;&#20307;&#20013;&#65292;&#22312;&#36793;&#25628;&#32034;&#12289;&#36793;&#24418;&#25104;&#21644;&#22686;&#24378;&#12289;&#36793;&#36873;&#25321;&#19977;&#20010;&#27493;&#39588;&#20013;&#20998;&#21035;&#21033;&#29992;&#31070;&#32463;&#26041;&#27861;&#65292;&#24182;&#22312; SNOMED CT &#26412;&#20307;&#21644; MedMentions &#23454;&#20307;&#38142;&#25509;&#22522;&#20934;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#23558;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#30340;&#26032;&#27010;&#24565;&#25554;&#20837;&#26412;&#20307;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#20010;&#19977;&#27493;&#26041;&#27861;&#65306;&#36793;&#25628;&#32034;&#65292;&#21363;&#25214;&#21040;&#35201;&#25554;&#20837;&#30340;&#20505;&#36873;&#20301;&#32622;&#38598;&#65288;&#21363;&#27010;&#24565;&#20043;&#38388;&#30340;&#21253;&#21547;&#20851;&#31995;&#65289;&#65292;&#36793;&#24418;&#25104;&#21644;&#22686;&#24378;&#65292;&#21033;&#29992;&#26412;&#20307;&#32467;&#26500;&#29983;&#25104;&#21644;&#22686;&#24378;&#36793;&#20505;&#36873;&#65292;&#20197;&#21450;&#36793;&#36873;&#25321;&#65292;&#26368;&#32456;&#30830;&#23450;&#35201;&#25918;&#32622;&#30340;&#36793;&#12290;&#22312;&#25152;&#26377;&#27493;&#39588;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#31070;&#32463;&#26041;&#27861;&#65292;&#20854;&#20013;&#24212;&#29992;&#22522;&#20110;&#23884;&#20837;&#30340;&#26041;&#27861;&#21644;&#23545;&#27604;&#23398;&#20064;&#65292;&#22914;BERT&#29992;&#20110;&#36793;&#25628;&#32034;&#65292;&#37319;&#29992;&#22522;&#20110;BERT&#24494;&#35843;&#30340;&#22810;&#26631;&#31614;&#36793;&#20132;&#21449;&#32534;&#30721;&#22120;&#65292;&#20197;&#21450;GPT&#31995;&#21015;&#12289;FLAN-T5 &#21644; Llama 2 &#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#29992;&#20110;&#36793;&#36873;&#25321;&#12290;&#25105;&#20204;&#22312;&#20351;&#29992; SNOMED CT &#26412;&#20307;&#21644; MedMentions &#23454;&#20307;&#38142;&#25509;&#22522;&#20934;&#21019;&#24314;&#30340;&#26368;&#26032;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#36825;&#20123;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17897v1 Announce Type: new  Abstract: We investigate the task of inserting new concepts extracted from texts into an ontology using language models. We explore an approach with three steps: edge search which is to find a set of candidate locations to insert (i.e., subsumptions between concepts), edge formation and enrichment which leverages the ontological structure to produce and enhance the edge candidates, and edge selection which eventually locates the edge to be placed into. In all steps, we propose to leverage neural methods, where we apply embedding-based methods and contrastive learning with Pre-trained Language Models (PLMs) such as BERT for edge search, and adapt a BERT fine-tuning-based multi-label Edge-Cross-encoder, and Large Language Models (LLMs) such as GPT series, FLAN-T5, and Llama 2, for edge selection. We evaluate the methods on recent datasets created using the SNOMED CT ontology and the MedMentions entity linking benchmark. The best settings in our fram
&lt;/p&gt;</description></item><item><title>JMLR&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#21307;&#23398;&#39046;&#22495;&#25552;&#39640;&#38382;&#39064;&#22238;&#31572;&#31995;&#32479;&#24615;&#33021;&#65292;&#38477;&#20302;&#35745;&#31639;&#36164;&#28304;&#38656;&#27714;&#65292;&#22686;&#24378;&#27169;&#22411;&#21033;&#29992;&#21307;&#30103;&#30693;&#35782;&#36827;&#34892;&#25512;&#29702;&#21644;&#22238;&#31572;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.17887</link><description>&lt;p&gt;
JMLR&#65306;&#32852;&#21512;&#21307;&#30103;LLM&#21644;&#26816;&#32034;&#35757;&#32451;&#20197;&#22686;&#24378;&#25512;&#29702;&#21644;&#19987;&#19994;&#38382;&#39064;&#22238;&#31572;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
JMLR: Joint Medical LLM and Retrieval Training for Enhancing Reasoning and Professional Question Answering Capability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17887
&lt;/p&gt;
&lt;p&gt;
JMLR&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#21307;&#23398;&#39046;&#22495;&#25552;&#39640;&#38382;&#39064;&#22238;&#31572;&#31995;&#32479;&#24615;&#33021;&#65292;&#38477;&#20302;&#35745;&#31639;&#36164;&#28304;&#38656;&#27714;&#65292;&#22686;&#24378;&#27169;&#22411;&#21033;&#29992;&#21307;&#30103;&#30693;&#35782;&#36827;&#34892;&#25512;&#29702;&#21644;&#22238;&#31572;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#21307;&#30103;&#25968;&#25454;&#30340;&#29190;&#28856;&#24615;&#22686;&#38271;&#21644;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#31934;&#20934;&#21307;&#23398;&#24050;&#32463;&#25104;&#20026;&#22686;&#24378;&#21307;&#30103;&#26381;&#21153;&#36136;&#37327;&#21644;&#25928;&#29575;&#30340;&#20851;&#38190;&#12290;&#22312;&#36825;&#31181;&#32972;&#26223;&#19979;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21307;&#30103;&#30693;&#35782;&#33719;&#21462;&#21644;&#38382;&#39064;&#22238;&#31572;&#31995;&#32479;&#20013;&#21457;&#25381;&#36234;&#26469;&#36234;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#36825;&#20123;&#31995;&#32479;&#22312;&#21307;&#23398;&#39046;&#22495;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#22312;&#24494;&#35843;&#38454;&#27573;&#21516;&#26102;&#35757;&#32451;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#31995;&#32479;&#21644;LLM&#12290;&#25105;&#20204;&#31216;&#20043;&#20026;&#32852;&#21512;&#21307;&#30103;LLM&#21644;&#26816;&#32034;&#35757;&#32451;&#65288;JMLR&#65289;&#30340;&#26041;&#27861;&#26088;&#22312;&#20811;&#26381;&#20256;&#32479;&#27169;&#22411;&#22312;&#22788;&#29702;&#21307;&#23398;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#26102;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#37319;&#29992;&#21516;&#27493;&#35757;&#32451;&#26426;&#21046;&#65292;JMLR&#20943;&#23569;&#20102;&#23545;&#35745;&#31639;&#36164;&#28304;&#30340;&#38656;&#27714;&#65292;&#24182;&#22686;&#24378;&#20102;&#27169;&#22411;&#21033;&#29992;&#21307;&#30103;&#30693;&#35782;&#36827;&#34892;&#25512;&#29702;&#21644;&#22238;&#31572;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17887v1 Announce Type: new  Abstract: With the explosive growth of medical data and the rapid development of artificial intelligence technology, precision medicine has emerged as a key to enhancing the quality and efficiency of healthcare services. In this context, Large Language Models (LLMs) play an increasingly vital role in medical knowledge acquisition and question-answering systems. To further improve the performance of these systems in the medical domain, we introduce an innovative method that jointly trains an Information Retrieval (IR) system and an LLM during the fine-tuning phase. This approach, which we call Joint Medical LLM and Retrieval Training (JMLR), is designed to overcome the challenges faced by traditional models in handling medical question-answering tasks. By employing a synchronized training mechanism, JMLR reduces the demand for computational resources and enhances the model's ability to leverage medical knowledge for reasoning and answering question
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;UniRetriever&#26694;&#26550;&#65292;&#21033;&#29992;&#21452;&#32534;&#30721;&#22120;&#26550;&#26500;&#21644;&#20004;&#20010;&#25439;&#22833;&#32422;&#26463;&#23454;&#29616;&#20102;&#22810;&#20219;&#21153;&#20505;&#36873;&#32773;&#36873;&#25321;&#65292;&#36866;&#29992;&#20110;&#19981;&#21516;&#24773;&#22659;&#19979;&#30340;&#23545;&#35805;&#26816;&#32034;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2402.16261</link><description>&lt;p&gt;
UniRetriever&#65306;&#21508;&#31181;&#24773;&#22659;&#33258;&#36866;&#24212;&#23545;&#35805;&#26816;&#32034;&#30340;&#22810;&#20219;&#21153;&#20505;&#36873;&#32773;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
UniRetriever: Multi-task Candidates Selection for Various Context-Adaptive Conversational Retrieval
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16261
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;UniRetriever&#26694;&#26550;&#65292;&#21033;&#29992;&#21452;&#32534;&#30721;&#22120;&#26550;&#26500;&#21644;&#20004;&#20010;&#25439;&#22833;&#32422;&#26463;&#23454;&#29616;&#20102;&#22810;&#20219;&#21153;&#20505;&#36873;&#32773;&#36873;&#25321;&#65292;&#36866;&#29992;&#20110;&#19981;&#21516;&#24773;&#22659;&#19979;&#30340;&#23545;&#35805;&#26816;&#32034;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#26816;&#32034;&#26159;&#25351;&#20197;&#36845;&#20195;&#21644;&#20132;&#20114;&#26041;&#24335;&#36816;&#34892;&#30340;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#65292;&#38656;&#35201;&#26816;&#32034;&#21508;&#31181;&#22806;&#37096;&#36164;&#28304;&#65288;&#22914;&#20154;&#35774;&#12289;&#30693;&#35782;&#29978;&#33267;&#22238;&#24212;&#65289;&#20197;&#26377;&#25928;&#19982;&#29992;&#25143;&#20132;&#20114;&#24182;&#25104;&#21151;&#23436;&#25104;&#23545;&#35805;&#12290;&#20026;&#20102;&#25552;&#39640;&#25928;&#29575;&#21644;&#24615;&#33021;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#20219;&#21153;&#26694;&#26550;&#65292;&#20316;&#20026;&#19977;&#20010;&#20027;&#35201;&#26816;&#32034;&#20219;&#21153;&#30340;&#36890;&#29992;&#26816;&#32034;&#22120;&#65306;&#20154;&#35774;&#36873;&#25321;&#12289;&#30693;&#35782;&#36873;&#25321;&#21644;&#22238;&#24212;&#36873;&#25321;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21452;&#32534;&#30721;&#22120;&#26550;&#26500;&#65292;&#21253;&#25324;&#19968;&#20010;&#24773;&#22659;&#33258;&#36866;&#24212;&#23545;&#35805;&#32534;&#30721;&#22120;&#21644;&#19968;&#20010;&#20505;&#36873;&#32773;&#32534;&#30721;&#22120;&#65292;&#26088;&#22312;&#36890;&#36807;&#31616;&#21333;&#30340;&#28857;&#31215;&#20851;&#27880;&#38271;&#23545;&#35805;&#20013;&#30340;&#30456;&#20851;&#19978;&#19979;&#25991;&#24182;&#26816;&#32034;&#21512;&#36866;&#30340;&#20505;&#36873;&#32773;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#25439;&#22833;&#32422;&#26463;&#20197;&#25429;&#25417;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16261v1 Announce Type: new  Abstract: Conversational retrieval refers to an information retrieval system that operates in an iterative and interactive manner, requiring the retrieval of various external resources, such as persona, knowledge, and even response, to effectively engage with the user and successfully complete the dialogue. However, most previous work trained independent retrievers for each specific resource, resulting in sub-optimal performance and low efficiency. Thus, we propose a multi-task framework function as a universal retriever for three dominant retrieval tasks during the conversation: persona selection, knowledge selection, and response selection. To this end, we design a dual-encoder architecture consisting of a context-adaptive dialogue encoder and a candidate encoder, aiming to attention to the relevant context from the long dialogue and retrieve suitable candidates by simply a dot product. Furthermore, we introduce two loss constraints to capture t
&lt;/p&gt;</description></item><item><title>ListT5&#36890;&#36807;Fusion-in-Decoder&#25216;&#26415;&#23454;&#29616;&#21015;&#34920;&#37325;&#25490;&#65292;&#22312;&#38646;-shot&#26816;&#32034;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#65292;&#25928;&#29575;&#39640;&#20110;&#20043;&#21069;&#30340;&#27169;&#22411;&#65292;&#24182;&#35299;&#20915;&#20102;&#20197;&#24448;&#21015;&#34920;&#37325;&#25490;&#22120;&#30340;&#20013;&#38388;&#27573;&#20002;&#22833;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.15838</link><description>&lt;p&gt;
ListT5: &#22522;&#20110;&#35299;&#30721;&#22120;&#20869;&#34701;&#21512;&#30340;&#21015;&#34920;&#37325;&#25490;&#26041;&#27861;&#25913;&#21892;&#38646;-shot&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
ListT5: Listwise Reranking with Fusion-in-Decoder Improves Zero-shot Retrieval
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15838
&lt;/p&gt;
&lt;p&gt;
ListT5&#36890;&#36807;Fusion-in-Decoder&#25216;&#26415;&#23454;&#29616;&#21015;&#34920;&#37325;&#25490;&#65292;&#22312;&#38646;-shot&#26816;&#32034;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#65292;&#25928;&#29575;&#39640;&#20110;&#20043;&#21069;&#30340;&#27169;&#22411;&#65292;&#24182;&#35299;&#20915;&#20102;&#20197;&#24448;&#21015;&#34920;&#37325;&#25490;&#22120;&#30340;&#20013;&#38388;&#27573;&#20002;&#22833;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;ListT5&#65292;&#19968;&#31181;&#22522;&#20110;&#22312;&#35757;&#32451;&#21644;&#25512;&#26029;&#26102;&#22788;&#29702;&#22810;&#20010;&#20505;&#36873;&#27573;&#33853;&#30340;Fusion-in-Decoder&#65288;FiD&#65289;&#30340;&#26032;&#22411;&#37325;&#25490;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;m&#20803;&#38182;&#26631;&#36187;&#25490;&#24207;&#21644;&#36755;&#20986;&#32531;&#23384;&#30340;&#21015;&#34920;&#25490;&#24207;&#30340;&#39640;&#25928;&#25512;&#26029;&#26694;&#26550;&#12290;&#25105;&#20204;&#22312;BEIR&#22522;&#20934;&#19978;&#35780;&#20272;&#21644;&#27604;&#36739;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#35777;&#26126;&#20102;ListT5&#65288;1&#65289;&#22312;&#24179;&#22343;NDCG@10&#24471;&#20998;&#19978;&#27604;&#26368;&#20808;&#36827;&#30340;RankT5&#22522;&#32447;&#34920;&#29616;&#20986;&#26126;&#26174;&#30340;+1.3&#22686;&#30410;&#65292;&#65288;2&#65289;&#20855;&#26377;&#19982;&#36880;&#28857;&#25490;&#21517;&#27169;&#22411;&#21487;&#27604;&#25311;&#30340;&#25928;&#29575;&#65292;&#24182;&#36229;&#36234;&#20197;&#21069;&#30340;&#21015;&#34920;&#25490;&#24207;&#27169;&#22411;&#30340;&#25928;&#29575;&#65292;&#65288;3&#65289;&#20811;&#26381;&#20102;&#20197;&#21069;&#21015;&#34920;&#37325;&#25490;&#22120;&#30340;&#20013;&#38388;&#27573;&#20002;&#22833;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#12289;&#27169;&#22411;&#26816;&#26597;&#28857;&#21644;&#35780;&#20272;&#26694;&#26550;&#23436;&#20840;&#24320;&#28304;&#22312; \url{https://github.com/soyoung97/ListT5}&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15838v1 Announce Type: new  Abstract: We propose ListT5, a novel reranking approach based on Fusion-in-Decoder (FiD) that handles multiple candidate passages at both train and inference time. We also introduce an efficient inference framework for listwise ranking based on m-ary tournament sort with output caching. We evaluate and compare our model on the BEIR benchmark for zero-shot retrieval task, demonstrating that ListT5 (1) outperforms the state-of-the-art RankT5 baseline with a notable +1.3 gain in the average NDCG@10 score, (2) has an efficiency comparable to pointwise ranking models and surpasses the efficiency of previous listwise ranking models, and (3) overcomes the lost-in-the-middle problem of previous listwise rerankers. Our code, model checkpoints, and the evaluation framework are fully open-sourced at \url{https://github.com/soyoung97/ListT5}.
&lt;/p&gt;</description></item><item><title>Text2Pic Swift&#26694;&#26550;&#38024;&#23545;&#22823;&#35268;&#27169;&#24211;&#20013;&#25991;&#26412;&#25551;&#36848;&#21040;&#22270;&#20687;&#30340;&#26816;&#32034;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#19988;&#24378;&#22823;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#38454;&#27573;&#31574;&#30053;&#35299;&#20915;&#20102;&#38271;&#25991;&#26412;&#26597;&#35810;&#20013;&#30340;&#27495;&#20041;&#38382;&#39064;</title><link>https://arxiv.org/abs/2402.15276</link><description>&lt;p&gt;
Text2Pic Swift&#65306;&#22686;&#24378;&#22823;&#35268;&#27169;&#24211;&#20013;&#38271;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Text2Pic Swift: Enhancing Long-Text to Image Retrieval for Large-Scale Libraries
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15276
&lt;/p&gt;
&lt;p&gt;
Text2Pic Swift&#26694;&#26550;&#38024;&#23545;&#22823;&#35268;&#27169;&#24211;&#20013;&#25991;&#26412;&#25551;&#36848;&#21040;&#22270;&#20687;&#30340;&#26816;&#32034;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#19988;&#24378;&#22823;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#38454;&#27573;&#31574;&#30053;&#35299;&#20915;&#20102;&#38271;&#25991;&#26412;&#26597;&#35810;&#20013;&#30340;&#27495;&#20041;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15276v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#39046;&#22495; &#25688;&#35201;&#65306;&#25991;&#26412;&#21040;&#22270;&#20687;&#26816;&#32034;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#21253;&#25324;&#25968;&#23383;&#22270;&#20070;&#39302;&#12289;&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#21644;&#22810;&#23186;&#20307;&#25968;&#25454;&#24211;&#65292;&#36890;&#36807;&#20351;&#29992;&#25991;&#26412;&#26597;&#35810;&#26469;&#25628;&#32034;&#22270;&#20687;&#12290;&#23613;&#31649;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#21462;&#24471;&#20102;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#22312;&#22823;&#35268;&#27169;&#12289;&#22810;&#26679;&#21270;&#21644;&#27169;&#31946;&#30340;&#26816;&#32034;&#22330;&#26223;&#20013;&#30340;&#36866;&#29992;&#24615;&#21463;&#21040;&#26174;&#30528;&#30340;&#35745;&#31639;&#38656;&#27714;&#21644;&#29983;&#25104;&#21487;&#27880;&#20837;&#30340;&#23884;&#20837;&#25152;&#38480;&#21046;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;Text2Pic Swift&#26694;&#26550;&#65292;&#19987;&#20026;&#22312;&#24222;&#22823;&#30340;&#25968;&#25454;&#38598;&#20013;&#26377;&#25928;&#21644;&#31283;&#20581;&#22320;&#26816;&#32034;&#19982;&#24191;&#27867;&#25991;&#26412;&#25551;&#36848;&#23545;&#24212;&#30340;&#22270;&#20687;&#32780;&#35774;&#35745;&#12290;&#35813;&#26694;&#26550;&#37319;&#29992;&#20102;&#20004;&#38454;&#27573;&#26041;&#27861;&#65306;&#21021;&#22987;&#22522;&#20110;&#23454;&#20307;&#30340;&#25490;&#24207;&#65288;ER&#65289;&#38454;&#27573;&#36890;&#36807;&#22810;&#26597;&#35810;&#23545;&#22810;&#30446;&#26631;&#30340;&#31574;&#30053;&#26469;&#35299;&#20915;&#38271;&#25991;&#26412;&#26597;&#35810;&#20013;&#22266;&#26377;&#30340;&#27495;&#20041;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#32553;&#23567;&#20102;&#21487;&#33021;&#30340;&#20505;&#36873;&#39033;&#65292;&#20197;&#20415;&#36827;&#34892;&#21518;&#32493;&#20998;&#26512;&#12290;&#25509;&#19979;&#26469;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15276v1 Announce Type: cross  Abstract: Text-to-image retrieval plays a crucial role across various applications, including digital libraries, e-commerce platforms, and multimedia databases, by enabling the search for images using text queries. Despite the advancements in Multimodal Large Language Models (MLLMs), which offer leading-edge performance, their applicability in large-scale, varied, and ambiguous retrieval scenarios is constrained by significant computational demands and the generation of injective embeddings. This paper introduces the Text2Pic Swift framework, tailored for efficient and robust retrieval of images corresponding to extensive textual descriptions in sizable datasets. The framework employs a two-tier approach: the initial Entity-based Ranking (ER) stage addresses the ambiguity inherent in lengthy text queries through a multiple-queries-to-multiple-targets strategy, effectively narrowing down potential candidates for subsequent analysis. Following thi
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30456;&#20851;&#20302;&#31209;&#32467;&#26500;&#65288;CoLR&#65289;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#32852;&#37030;&#25512;&#33616;&#31995;&#32479;&#20013;&#39640;&#25928;&#30340;&#36890;&#20449;&#21644;&#23433;&#20840;&#24615;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#36890;&#20449;&#24320;&#38144;&#21516;&#26102;&#20445;&#25345;&#19982;&#23433;&#20840;&#32858;&#21512;&#21327;&#35758;&#30340;&#20860;&#23481;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.03748</link><description>&lt;p&gt;
&#36890;&#36807;&#20302;&#31209;&#35757;&#32451;&#23454;&#29616;&#39640;&#25928;&#36890;&#20449;&#21644;&#23433;&#20840;&#30340;&#32852;&#37030;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Towards Efficient Communication and Secure Federated Recommendation System via Low-rank Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.03748
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30456;&#20851;&#20302;&#31209;&#32467;&#26500;&#65288;CoLR&#65289;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#32852;&#37030;&#25512;&#33616;&#31995;&#32479;&#20013;&#39640;&#25928;&#30340;&#36890;&#20449;&#21644;&#23433;&#20840;&#24615;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#36890;&#20449;&#24320;&#38144;&#21516;&#26102;&#20445;&#25345;&#19982;&#23433;&#20840;&#32858;&#21512;&#21327;&#35758;&#30340;&#20860;&#23481;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2401.03748v2 &#20844;&#21578;&#31867;&#22411;: &#26367;&#25442; &#25688;&#35201;: &#38024;&#23545;&#19981;&#26029;&#22686;&#38271;&#30340;&#30417;&#31649;&#20851;&#27880;&#24341;&#21457;&#30340;&#38656;&#27714;&#65292;&#32852;&#37030;&#25512;&#33616;&#65288;FedRec&#65289;&#31995;&#32479;&#24050;&#32463;&#25104;&#20026;&#20445;&#25252;&#29992;&#25143;&#25968;&#25454;&#30340;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#36825;&#31867;&#31995;&#32479;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#22312;&#20110;&#20256;&#36755;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#25152;&#24102;&#26469;&#30340;&#36890;&#20449;&#25104;&#26412;&#65292;&#21363;&#22312;&#29992;&#25143;&#35774;&#22791;&#21644;&#20013;&#22830;&#26381;&#21153;&#22120;&#20043;&#38388;&#20256;&#36755;&#12290;&#20808;&#21069;&#38024;&#23545;&#36825;&#20123;&#25361;&#25112;&#30340;&#26041;&#27861;&#36890;&#24120;&#20250;&#23548;&#33268;&#35832;&#22914;&#35745;&#31639;&#24320;&#38144;&#12289;&#27169;&#22411;&#29305;&#24615;&#38480;&#21046;&#20197;&#21450;&#19982;&#23433;&#20840;&#32858;&#21512;&#21327;&#35758;&#20860;&#23481;&#24615;&#30340;&#38382;&#39064;&#12290; &#20316;&#20026;&#21709;&#24212;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;&#30456;&#20851;&#20302;&#31209;&#32467;&#26500;&#65288;CoLR&#65289;&#65292;&#23427;&#21033;&#29992;&#20102;&#35843;&#25972;&#36731;&#37327;&#32423;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#27010;&#24565;&#65292;&#21516;&#26102;&#20445;&#25345;&#22823;&#37096;&#20998;&#21442;&#25968;&#20923;&#32467;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#20943;&#23569;&#20102;&#36890;&#20449;&#24320;&#38144;&#32780;&#19981;&#24341;&#20837;&#39069;&#22806;&#30340;&#35745;&#31639;&#36127;&#25285;&#12290;&#33267;&#20851;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#19982;&#23433;&#20840;&#32858;&#21512;&#21327;&#35758;&#23436;&#20840;&#20860;&#23481;&#65292;&#21253;&#25324;&#40065;&#26834;&#22320;&#20351;&#29992;&#21516;&#24577;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.03748v2 Announce Type: replace  Abstract: Federated Recommendation (FedRec) systems have emerged as a solution to safeguard users' data in response to growing regulatory concerns. However, one of the major challenges in these systems lies in the communication costs that arise from the need to transmit neural network models between user devices and a central server. Prior approaches to these challenges often lead to issues such as computational overheads, model specificity constraints, and compatibility issues with secure aggregation protocols. In response, we propose a novel framework, called Correlated Low-rank Structure (CoLR), which leverages the concept of adjusting lightweight trainable parameters while keeping most parameters frozen. Our approach substantially reduces communication overheads without introducing additional computational burdens. Critically, our framework remains fully compatible with secure aggregation protocols, including the robust use of Homomorphic 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;NLP&#26041;&#27861;&#21644;&#24037;&#20855;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#20174;&#25991;&#26412;&#23186;&#20307;&#21453;&#24212;&#20013;&#33258;&#21160;&#25552;&#21462;ESG&#30456;&#20851;&#20449;&#24687;&#65292;&#23454;&#29616;&#20844;&#21496;ESG&#30340;&#33258;&#21160;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2212.06540</link><description>&lt;p&gt;
&#36890;&#36807;&#25366;&#25496;&#21644;&#35780;&#20272;&#23186;&#20307;&#25253;&#36947;&#25968;&#25454;&#23454;&#29616;&#20844;&#21496;&#30340;ESG&#33258;&#21160;&#35780;&#20272;&#65306;NLP&#26041;&#27861;&#19982;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
Automatic ESG Assessment of Companies by Mining and Evaluating Media Coverage Data: NLP Approach and Tool
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2212.06540
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;NLP&#26041;&#27861;&#21644;&#24037;&#20855;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#20174;&#25991;&#26412;&#23186;&#20307;&#21453;&#24212;&#20013;&#33258;&#21160;&#25552;&#21462;ESG&#30456;&#20851;&#20449;&#24687;&#65292;&#23454;&#29616;&#20844;&#21496;ESG&#30340;&#33258;&#21160;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#25345;&#32493;&#24615;&#20225;&#19994;&#34892;&#20026;&#21463;&#21040;&#31038;&#20250;&#36234;&#26469;&#36234;&#37325;&#35270;&#65292;&#24433;&#21709;&#30528;&#20225;&#19994;&#22768;&#35465;&#21644;&#23458;&#25143;&#20449;&#20219;&#12290;&#22240;&#27492;&#65292;&#20844;&#21496;&#23450;&#26399;&#21457;&#24067;&#21487;&#25345;&#32493;&#21457;&#23637;&#25253;&#21578;&#65292;&#20197;&#38416;&#26126;&#20854;&#23545;&#29615;&#22659;&#12289;&#31038;&#20250;&#21644;&#27835;&#29702;&#65288;ESG&#65289;&#22240;&#32032;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#20998;&#26512;&#26377;&#20851;ESG&#22240;&#32032;&#30340;&#23186;&#20307;&#25253;&#36947;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#65288;1&#65289;&#27599;&#22825;&#21457;&#24067;&#30340;&#26032;&#38395;&#25991;&#31456;&#25968;&#37327;&#19981;&#26029;&#22686;&#21152;&#65292;&#65288;2&#65289;&#23186;&#20307;&#25253;&#36947;&#25968;&#25454;&#19981;&#19968;&#23450;&#28041;&#21450;ESG&#30456;&#20851;&#20027;&#39064;&#65292;&#24517;&#39035;&#32463;&#36807;&#20180;&#32454;&#31579;&#36873;&#65292;&#65288;3&#65289;&#22823;&#22810;&#25968;&#23186;&#20307;&#25253;&#36947;&#25968;&#25454;&#26159;&#38750;&#32467;&#26500;&#21270;&#30340;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#30446;&#26631;&#26159;&#33258;&#21160;&#20174;&#25991;&#26412;&#23186;&#20307;&#21453;&#24212;&#20013;&#25552;&#21462;ESG&#30456;&#20851;&#20449;&#24687;&#65292;&#20197;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
arXiv:2212.06540v2 Announce Type: replace  Abstract: Context: Sustainable corporate behavior is increasingly valued by society and impacts corporate reputation and customer trust. Hence, companies regularly publish sustainability reports to shed light on their impact on environmental, social, and governance (ESG) factors. Problem: Sustainability reports are written by companies themselves and are therefore considered a company-controlled source. Contrary, studies reveal that non-corporate channels (e.g., media coverage) represent the main driver for ESG transparency. However, analysing media coverage regarding ESG factors is challenging since (1) the amount of published news articles grows daily, (2) media coverage data does not necessarily deal with an ESG-relevant topic, meaning that it must be carefully filtered, and (3) the majority of media coverage data is unstructured. Research Goal: We aim to extract ESG-relevant information from textual media reactions automatically to calcula
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23433;&#20840;&#30340;&#21327;&#21516;&#36807;&#28388;&#31639;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#26465;&#20214;&#39118;&#38505;&#20215;&#20540;&#65292;&#25552;&#39640;&#20302;&#28385;&#24847;&#24230;&#29992;&#25143;&#30340;&#25512;&#33616;&#36136;&#37327;&#12290;&#22312;&#23454;&#38469;&#25968;&#25454;&#38598;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#21516;&#26102;&#20063;&#20445;&#25345;&#24635;&#20307;&#25512;&#33616;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2306.05292</link><description>&lt;p&gt;
&#23433;&#20840;&#30340;&#21327;&#21516;&#36807;&#28388;
&lt;/p&gt;
&lt;p&gt;
Safe Collaborative Filtering. (arXiv:2306.05292v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05292
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23433;&#20840;&#30340;&#21327;&#21516;&#36807;&#28388;&#31639;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#26465;&#20214;&#39118;&#38505;&#20215;&#20540;&#65292;&#25552;&#39640;&#20302;&#28385;&#24847;&#24230;&#29992;&#25143;&#30340;&#25512;&#33616;&#36136;&#37327;&#12290;&#22312;&#23454;&#38469;&#25968;&#25454;&#38598;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#21516;&#26102;&#20063;&#20445;&#25345;&#24635;&#20307;&#25512;&#33616;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#65292;&#20363;&#22914;&#31639;&#27861;&#20844;&#24179;&#24615;&#12289;&#31867;&#21035;&#19981;&#24179;&#34913;&#21644;&#39118;&#38505;&#25935;&#24863;&#30340;&#20915;&#31574;&#21046;&#23450;&#65292;&#20248;&#31168;&#30340;&#23614;&#37096;&#24615;&#33021;&#38750;&#24120;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#30830;&#20445;&#20102;&#23545;&#25968;&#25454;&#38598;&#20013;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26679;&#26412;&#30340;&#26377;&#25928;&#22788;&#29702;&#12290;&#23614;&#37096;&#24615;&#33021;&#20063;&#26159;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#25104;&#21151;&#30340;&#37325;&#35201;&#20915;&#23450;&#22240;&#32032;&#65292;&#20197;&#20943;&#23569;&#23545;&#20302;&#28385;&#24847;&#24230;&#29992;&#25143;&#30340;&#27969;&#22833;&#39118;&#38505;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#8220;&#23433;&#20840;&#8221;&#30340;&#21327;&#21516;&#36807;&#28388;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20248;&#20808;&#32771;&#34385;&#20302;&#28385;&#24847;&#24230;&#29992;&#25143;&#30340;&#25512;&#33616;&#36136;&#37327;&#65292;&#32780;&#19981;&#26159;&#20851;&#27880;&#24179;&#22343;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26368;&#23567;&#21270;&#26465;&#20214;&#39118;&#38505;&#20215;&#20540;&#65288;CVaR&#65289;&#65292;&#34920;&#31034;&#29992;&#25143;&#25439;&#22833;&#23614;&#37096;&#30340;&#24179;&#22343;&#39118;&#38505;&#12290;&#20026;&#20102;&#20811;&#26381;&#32593;&#32476;&#35268;&#27169;&#30340;&#25512;&#33616;&#31995;&#32479;&#30340;&#35745;&#31639;&#38590;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#24378;&#22823;&#32780;&#23454;&#29992;&#30340;&#31639;&#27861;&#65292;&#25193;&#23637;&#20102;&#26368;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#38544;&#24335;&#20132;&#26367;&#26368;&#23567;&#20108;&#20056;&#27861;&#65288;iALS&#65289;&#12290;&#22312;&#23454;&#38469;&#25968;&#25454;&#38598;&#30340;&#32463;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#20986;&#33394;&#30340;&#23614;&#37096;&#24615;&#33021;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#24635;&#20307;&#25512;&#33616;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Excellent tail performance is crucial for modern machine learning tasks, such as algorithmic fairness, class imbalance, and risk-sensitive decision making, as it ensures the effective handling of challenging samples within a dataset. Tail performance is also a vital determinant of success for personalised recommender systems to reduce the risk of losing users with low satisfaction. This study introduces a "safe" collaborative filtering method that prioritises recommendation quality for less-satisfied users rather than focusing on the average performance. Our approach minimises the conditional value at risk (CVaR), which represents the average risk over the tails of users' loss. To overcome computational challenges for web-scale recommender systems, we develop a robust yet practical algorithm that extends the most scalable method, implicit alternating least squares (iALS). Empirical evaluation on real-world datasets demonstrates the excellent tail performance of our approach while maint
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#28145;&#24230;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#30740;&#31350;&#29616;&#29366;&#21644;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#24182;&#25351;&#20986;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#24050;&#32463;&#26174;&#31034;&#20986;&#24040;&#22823;&#30340;&#20248;&#21183;&#21644;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.05055</link><description>&lt;p&gt;
&#28145;&#24230;&#22270;&#34920;&#31034;&#23398;&#20064;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Survey on Deep Graph Representation Learning. (arXiv:2304.05055v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05055
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#28145;&#24230;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#30740;&#31350;&#29616;&#29366;&#21644;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#24182;&#25351;&#20986;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#24050;&#32463;&#26174;&#31034;&#20986;&#24040;&#22823;&#30340;&#20248;&#21183;&#21644;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#34920;&#31034;&#23398;&#20064;&#26088;&#22312;&#23558;&#39640;&#32500;&#31232;&#30095;&#30340;&#22270;&#32467;&#26500;&#25968;&#25454;&#26377;&#25928;&#22320;&#32534;&#30721;&#25104;&#20302;&#32500;&#23494;&#38598;&#21521;&#37327;&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#26412;&#20219;&#21153;&#65292;&#22312;&#21253;&#25324;&#26426;&#22120;&#23398;&#20064;&#21644;&#25968;&#25454;&#25366;&#25496;&#22312;&#20869;&#30340;&#19968;&#31995;&#21015;&#39046;&#22495;&#37117;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#12290;&#20256;&#32479;&#22270;&#23884;&#20837;&#26041;&#27861;&#36981;&#24490;&#36825;&#26679;&#19968;&#31181;&#22522;&#26412;&#24605;&#24819;&#65292;&#21363;&#22270;&#20013;&#30456;&#20114;&#36830;&#25509;&#30340;&#33410;&#28857;&#30340;&#23884;&#20837;&#30690;&#37327;&#20173;&#28982;&#33021;&#22815;&#20445;&#25345;&#30456;&#23545;&#25509;&#36817;&#30340;&#36317;&#31163;&#65292;&#20174;&#32780;&#20445;&#30041;&#20102;&#22270;&#20013;&#33410;&#28857;&#20043;&#38388;&#30340;&#32467;&#26500;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#23384;&#22312;&#20197;&#19979;&#38382;&#39064;&#65306;&#65288;i&#65289;&#20256;&#32479;&#26041;&#27861;&#30340;&#27169;&#22411;&#23481;&#37327;&#21463;&#38480;&#65292;&#38480;&#21046;&#20102;&#23398;&#20064;&#24615;&#33021;; &#65288;ii&#65289;&#29616;&#26377;&#25216;&#26415;&#36890;&#24120;&#20381;&#36182;&#20110;&#26080;&#30417;&#30563;&#23398;&#20064;&#31574;&#30053;&#65292;&#26080;&#27861;&#19982;&#26368;&#26032;&#30340;&#23398;&#20064;&#33539;&#24335;&#30456;&#32467;&#21512;&#65307;&#65288;iii&#65289;&#34920;&#31034;&#23398;&#20064;&#21644;&#19979;&#28216;&#20219;&#21153;&#30456;&#20114;&#20381;&#23384;&#65292;&#24212;&#20849;&#21516;&#21152;&#24378;&#12290;&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#30340;&#26174;&#30528;&#25104;&#21151;&#65292;&#28145;&#24230;&#22270;&#34920;&#31034;&#23398;&#20064;&#24050;&#32463;&#26174;&#31034;&#20986;&#24040;&#22823;&#30340;&#28508;&#21147;&#21644;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph representation learning aims to effectively encode high-dimensional sparse graph-structured data into low-dimensional dense vectors, which is a fundamental task that has been widely studied in a range of fields, including machine learning and data mining. Classic graph embedding methods follow the basic idea that the embedding vectors of interconnected nodes in the graph can still maintain a relatively close distance, thereby preserving the structural information between the nodes in the graph. However, this is sub-optimal due to: (i) traditional methods have limited model capacity which limits the learning performance; (ii) existing techniques typically rely on unsupervised learning strategies and fail to couple with the latest learning paradigms; (iii) representation learning and downstream tasks are dependent on each other which should be jointly enhanced. With the remarkable success of deep learning, deep graph representation learning has shown great potential and advantages 
&lt;/p&gt;</description></item></channel></rss>