<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#26412;&#35770;&#25991;&#24378;&#35843;&#20102;&#36807;&#24230;&#30456;&#20851;&#24615;&#38382;&#39064;&#22312;&#38477;&#20302;GNN&#34920;&#31034;&#21644;&#25512;&#33616;&#24615;&#33021;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#65292;&#24182;&#33268;&#21147;&#20110;&#35299;&#20915;&#22914;&#20309;&#20943;&#36731;&#36807;&#24230;&#20851;&#32852;&#30340;&#24433;&#21709;&#21516;&#26102;&#20445;&#30041;&#21327;&#21516;&#36807;&#28388;&#20449;&#21495;&#36825;&#19968;&#37325;&#22823;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.17416</link><description>&lt;p&gt;
AFDGCF&#65306;&#33258;&#36866;&#24212;&#29305;&#24449;&#21435;&#30456;&#20851;&#22270;&#21327;&#21516;&#36807;&#28388;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
AFDGCF: Adaptive Feature De-correlation Graph Collaborative Filtering for Recommendations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17416
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24378;&#35843;&#20102;&#36807;&#24230;&#30456;&#20851;&#24615;&#38382;&#39064;&#22312;&#38477;&#20302;GNN&#34920;&#31034;&#21644;&#25512;&#33616;&#24615;&#33021;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#65292;&#24182;&#33268;&#21147;&#20110;&#35299;&#20915;&#22914;&#20309;&#20943;&#36731;&#36807;&#24230;&#20851;&#32852;&#30340;&#24433;&#21709;&#21516;&#26102;&#20445;&#30041;&#21327;&#21516;&#36807;&#28388;&#20449;&#21495;&#36825;&#19968;&#37325;&#22823;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30340;&#21327;&#21516;&#36807;&#28388;&#26041;&#27861;&#22312;&#25512;&#33616;&#31995;&#32479;&#65288;RS&#65289;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#21033;&#29992;&#20102;&#23427;&#20204;&#36890;&#36807;&#28040;&#24687;&#20256;&#36882;&#26426;&#21046;&#25429;&#33719;&#22797;&#26434;&#29992;&#25143;-&#29289;&#21697;&#20851;&#31995;&#20013;&#30340;&#21327;&#21516;&#20449;&#21495;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22522;&#20110;GNN&#30340;RS&#19981;&#32463;&#24847;&#22320;&#24341;&#20837;&#20102;&#29992;&#25143;&#21644;&#29289;&#21697;&#23884;&#20837;&#20043;&#38388;&#36807;&#22810;&#30340;&#32447;&#24615;&#30456;&#20851;&#24615;&#65292;&#36829;&#21453;&#20102;&#25552;&#20379;&#20010;&#24615;&#21270;&#25512;&#33616;&#30340;&#30446;&#26631;&#12290;&#23613;&#31649;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#23558;&#36825;&#19968;&#32570;&#38519;&#24402;&#22240;&#20110;&#36807;&#24230;&#24179;&#28369;&#30340;&#38382;&#39064;&#65292;&#20294;&#26412;&#25991;&#24378;&#35843;&#20102;&#36807;&#24230;&#30456;&#20851;&#24615;&#38382;&#39064;&#22312;&#38477;&#20302;GNN&#34920;&#31034;&#21644;&#38543;&#21518;&#25512;&#33616;&#24615;&#33021;&#30340;&#26377;&#25928;&#24615;&#20013;&#21457;&#25381;&#30528;&#20851;&#38190;&#19988;&#24120;&#24120;&#34987;&#24573;&#35270;&#30340;&#20316;&#29992;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#22312;RS&#20013;&#23545;&#36807;&#24230;&#30456;&#20851;&#24615;&#38382;&#39064;&#23578;&#26410;&#36827;&#34892;&#25506;&#35752;&#12290;&#21516;&#26102;&#65292;&#22914;&#20309;&#20943;&#36731;&#36807;&#24230;&#20851;&#32852;&#30340;&#24433;&#21709;&#21516;&#26102;&#20445;&#30041;&#21327;&#21516;&#36807;&#28388;&#20449;&#21495;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#30340;&#30446;&#26631;&#26159;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17416v1 Announce Type: new  Abstract: Collaborative filtering methods based on graph neural networks (GNNs) have witnessed significant success in recommender systems (RS), capitalizing on their ability to capture collaborative signals within intricate user-item relationships via message-passing mechanisms. However, these GNN-based RS inadvertently introduce excess linear correlation between user and item embeddings, contradicting the goal of providing personalized recommendations. While existing research predominantly ascribes this flaw to the over-smoothing problem, this paper underscores the critical, often overlooked role of the over-correlation issue in diminishing the effectiveness of GNN representations and subsequent recommendation performance. Up to now, the over-correlation issue remains unexplored in RS. Meanwhile, how to mitigate the impact of over-correlation while preserving collaborative filtering signals is a significant challenge. To this end, this paper aims
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33021;&#22815;&#35299;&#20915;&#23454;&#20307;&#23545;&#40784;&#35299;&#30721;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#26368;&#23567;&#21270;&#33021;&#37327;&#26469;&#20248;&#21270;&#35299;&#30721;&#36807;&#31243;&#65292;&#20197;&#23454;&#29616;&#22270;&#21516;&#36136;&#24615;&#65292;&#24182;&#19988;&#20165;&#20381;&#36182;&#20110;&#23454;&#20307;&#23884;&#20837;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#36890;&#29992;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.12798</link><description>&lt;p&gt;
&#33021;&#37327;&#30340;&#26799;&#24230;&#27969;&#65306;&#23454;&#20307;&#23545;&#40784;&#35299;&#30721;&#30340;&#36890;&#29992;&#39640;&#25928;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Gradient Flow of Energy: A General and Efficient Approach for Entity Alignment Decoding. (arXiv:2401.12798v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12798
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33021;&#22815;&#35299;&#20915;&#23454;&#20307;&#23545;&#40784;&#35299;&#30721;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#26368;&#23567;&#21270;&#33021;&#37327;&#26469;&#20248;&#21270;&#35299;&#30721;&#36807;&#31243;&#65292;&#20197;&#23454;&#29616;&#22270;&#21516;&#36136;&#24615;&#65292;&#24182;&#19988;&#20165;&#20381;&#36182;&#20110;&#23454;&#20307;&#23884;&#20837;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#36890;&#29992;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#20307;&#23545;&#40784;&#65288;EA&#65289;&#26159;&#22312;&#38598;&#25104;&#22810;&#28304;&#30693;&#35782;&#22270;&#35889;&#65288;KGs&#65289;&#20013;&#30340;&#20851;&#38190;&#36807;&#31243;&#65292;&#26088;&#22312;&#35782;&#21035;&#36825;&#20123;&#22270;&#35889;&#20013;&#30340;&#31561;&#20215;&#23454;&#20307;&#23545;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#23558;EA&#35270;&#20026;&#22270;&#34920;&#31034;&#23398;&#20064;&#20219;&#21153;&#65292;&#19987;&#27880;&#20110;&#22686;&#24378;&#22270;&#32534;&#30721;&#22120;&#12290;&#28982;&#32780;&#65292;&#22312;EA&#20013;&#35299;&#30721;&#36807;&#31243;-&#23545;&#20110;&#26377;&#25928;&#30340;&#25805;&#20316;&#21644;&#23545;&#40784;&#20934;&#30830;&#24615;&#33267;&#20851;&#37325;&#35201;-&#24471;&#21040;&#20102;&#26377;&#38480;&#30340;&#20851;&#27880;&#65292;&#24182;&#19988;&#20173;&#28982;&#38024;&#23545;&#29305;&#23450;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#26550;&#26500;&#36827;&#34892;&#23450;&#21046;&#65292;&#38656;&#35201;&#23454;&#20307;&#21644;&#39069;&#22806;&#30340;&#26174;&#24335;&#20851;&#31995;&#23884;&#20837;&#12290;&#36825;&#31181;&#29305;&#27530;&#24615;&#38480;&#21046;&#20102;&#23427;&#30340;&#36866;&#29992;&#24615;&#65292;&#23588;&#20854;&#26159;&#22312;&#22522;&#20110;GNN&#30340;&#27169;&#22411;&#20013;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#12289;&#36890;&#29992;&#21644;&#39640;&#25928;&#30340;EA&#35299;&#30721;&#26041;&#27861;&#65292;&#20165;&#20381;&#36182;&#20110;&#23454;&#20307;&#23884;&#20837;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#26368;&#23567;&#21270;&#29380;&#21033;&#20811;&#38647;&#33021;&#37327;&#26469;&#20248;&#21270;&#35299;&#30721;&#36807;&#31243;&#65292;&#22312;&#22270;&#20869;&#24341;&#23548;&#26799;&#24230;&#27969;&#65292;&#20197;&#20419;&#36827;&#22270;&#21516;&#36136;&#24615;&#12290;&#26799;&#24230;&#27969;&#30340;&#31163;&#25955;&#21270;&#20135;&#29983;&#20102;&#19968;&#31181;&#24555;&#36895;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#19977;&#20803;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Entity alignment (EA), a pivotal process in integrating multi-source Knowledge Graphs (KGs), seeks to identify equivalent entity pairs across these graphs. Most existing approaches regard EA as a graph representation learning task, concentrating on enhancing graph encoders. However, the decoding process in EA - essential for effective operation and alignment accuracy - has received limited attention and remains tailored to specific datasets and model architectures, necessitating both entity and additional explicit relation embeddings. This specificity limits its applicability, particularly in GNN-based models. To address this gap, we introduce a novel, generalized, and efficient decoding approach for EA, relying solely on entity embeddings. Our method optimizes the decoding process by minimizing Dirichlet energy, leading to the gradient flow within the graph, to promote graph homophily. The discretization of the gradient flow produces a fast and scalable approach, termed Triple Feature
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#39034;&#24207;&#25512;&#33616;&#33539;&#24335; LANCER&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#20041;&#29702;&#35299;&#33021;&#21147;&#29983;&#25104;&#26356;&#21152;&#20154;&#24615;&#21270;&#30340;&#20010;&#24615;&#21270;&#25512;&#33616;&#12290;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#26377;&#25928;&#19988;&#26377;&#24076;&#26395;&#65292;&#24182;&#20026;&#20102;&#35299;&#39034;&#24207;&#25512;&#33616;&#30340;&#24433;&#21709;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2309.10435</link><description>&lt;p&gt;
&#37325;&#22609;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#65306;&#21033;&#29992;&#20869;&#23481;&#22686;&#24378;&#35821;&#35328;&#24314;&#27169;&#23398;&#20064;&#21160;&#24577;&#29992;&#25143;&#20852;&#36259;
&lt;/p&gt;
&lt;p&gt;
Reformulating Sequential Recommendation: Learning Dynamic User Interest with Content-enriched Language Modeling. (arXiv:2309.10435v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10435
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#39034;&#24207;&#25512;&#33616;&#33539;&#24335; LANCER&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#20041;&#29702;&#35299;&#33021;&#21147;&#29983;&#25104;&#26356;&#21152;&#20154;&#24615;&#21270;&#30340;&#20010;&#24615;&#21270;&#25512;&#33616;&#12290;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#26377;&#25928;&#19988;&#26377;&#24076;&#26395;&#65292;&#24182;&#20026;&#20102;&#35299;&#39034;&#24207;&#25512;&#33616;&#30340;&#24433;&#21709;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#23545;&#22312;&#32447;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#65292;&#32780;&#39034;&#24207;&#25512;&#33616;&#30001;&#20110;&#20854;&#34920;&#36798;&#33021;&#21147;&#24378;&#22823;&#65292;&#33021;&#22815;&#25429;&#25417;&#21040;&#21160;&#24577;&#29992;&#25143;&#20852;&#36259;&#32780;&#24191;&#27867;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#39034;&#24207;&#24314;&#27169;&#26041;&#27861;&#22312;&#25429;&#25417;&#19978;&#19979;&#25991;&#20449;&#24687;&#26041;&#38754;&#20173;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#20027;&#35201;&#30340;&#21407;&#22240;&#26159;&#35821;&#35328;&#27169;&#22411;&#24120;&#24120;&#32570;&#20047;&#23545;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#21644;&#29289;&#21697;&#30456;&#20851;&#25991;&#26412;&#20869;&#23481;&#30340;&#29702;&#35299;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#30340;&#39034;&#24207;&#25512;&#33616;&#33539;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;LANCER&#65292;&#23427;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#20041;&#29702;&#35299;&#33021;&#21147;&#29983;&#25104;&#20010;&#24615;&#21270;&#25512;&#33616;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24357;&#21512;&#20102;&#35821;&#35328;&#27169;&#22411;&#19982;&#25512;&#33616;&#31995;&#32479;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#20135;&#29983;&#20102;&#26356;&#21152;&#20154;&#24615;&#21270;&#30340;&#25512;&#33616;&#12290;&#36890;&#36807;&#23545;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#23637;&#31034;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#25105;&#20204;&#27169;&#22411;&#23545;&#39034;&#24207;&#25512;&#33616;&#30340;&#24433;&#21709;&#30340;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommender systems are essential for online applications, and sequential recommendation has enjoyed significant prevalence due to its expressive ability to capture dynamic user interests. However, previous sequential modeling methods still have limitations in capturing contextual information. The primary reason for this issue is that language models often lack an understanding of domain-specific knowledge and item-related textual content. To address this issue, we adopt a new sequential recommendation paradigm and propose LANCER, which leverages the semantic understanding capabilities of pre-trained language models to generate personalized recommendations. Our approach bridges the gap between language models and recommender systems, resulting in more human-like recommendations. We demonstrate the effectiveness of our approach through experiments on several benchmark datasets, showing promising results and providing valuable insights into the influence of our model on sequential recomm
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33021;&#22815;&#25552;&#39640;&#23545;ChatGPT&#29983;&#25104;&#30340;&#20551;&#31185;&#23398;&#36827;&#34892;&#26816;&#27979;&#30340;&#31639;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#26032;&#35774;&#35745;&#30340;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#20934;&#30830;&#22320;&#23558;&#26426;&#22120;&#29983;&#25104;&#30340;&#20986;&#29256;&#29289;&#19982;&#31185;&#23398;&#23478;&#29983;&#25104;&#30340;&#20986;&#29256;&#29289;&#21306;&#20998;&#24320;&#26469;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;ChatGPT&#22312;&#25216;&#26415;&#26415;&#35821;&#26041;&#38754;&#19982;&#30495;&#23454;&#31185;&#23398;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#31639;&#27861;&#22312;&#20998;&#31867;&#36807;&#31243;&#20013;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.11767</link><description>&lt;p&gt;
&#25552;&#39640;ChatGPT&#29983;&#25104;&#30340;&#20551;&#31185;&#23398;&#26816;&#27979;&#30340;&#26041;&#27861;&#65306;&#24341;&#20837;xFakeBibs&#30417;&#30563;&#23398;&#20064;&#32593;&#32476;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Improving Detection of ChatGPT-Generated Fake Science Using Real Publication Text: Introducing xFakeBibs a Supervised-Learning Network Algorithm. (arXiv:2308.11767v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11767
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33021;&#22815;&#25552;&#39640;&#23545;ChatGPT&#29983;&#25104;&#30340;&#20551;&#31185;&#23398;&#36827;&#34892;&#26816;&#27979;&#30340;&#31639;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#26032;&#35774;&#35745;&#30340;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#20934;&#30830;&#22320;&#23558;&#26426;&#22120;&#29983;&#25104;&#30340;&#20986;&#29256;&#29289;&#19982;&#31185;&#23398;&#23478;&#29983;&#25104;&#30340;&#20986;&#29256;&#29289;&#21306;&#20998;&#24320;&#26469;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;ChatGPT&#22312;&#25216;&#26415;&#26415;&#35821;&#26041;&#38754;&#19982;&#30495;&#23454;&#31185;&#23398;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#31639;&#27861;&#22312;&#20998;&#31867;&#36807;&#31243;&#20013;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#27491;&#22312;&#25104;&#20026;&#29616;&#23454;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#21306;&#20998;ChatGPT&#29983;&#25104;&#30340;&#20986;&#29256;&#29289;&#19982;&#31185;&#23398;&#23478;&#29983;&#25104;&#30340;&#20986;&#29256;&#29289;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#26032;&#35774;&#35745;&#30340;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#22914;&#20309;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#30340;&#20986;&#29256;&#29289;&#21644;&#31185;&#23398;&#23478;&#29983;&#25104;&#30340;&#20986;&#29256;&#29289;&#12290;&#35813;&#31639;&#27861;&#20351;&#29992;100&#20010;&#30495;&#23454;&#20986;&#29256;&#29289;&#25688;&#35201;&#36827;&#34892;&#35757;&#32451;&#65292;&#28982;&#21518;&#37319;&#29992;10&#20493;&#20132;&#21449;&#39564;&#35777;&#26041;&#27861;&#24314;&#31435;&#20102;&#19968;&#20010;&#25509;&#21463;&#33539;&#22260;&#30340;&#19979;&#38480;&#21644;&#19978;&#38480;&#12290;&#19982;ChatGPT&#20869;&#23481;&#36827;&#34892;&#27604;&#36739;&#65292;&#26126;&#26174;&#21487;&#35265;ChatGPT&#20165;&#36129;&#29486;&#20102;23\%&#30340;&#20108;&#20803;&#32452;&#20869;&#23481;&#65292;&#36825;&#27604;&#20854;&#20182;10&#20010;&#20132;&#21449;&#39564;&#35777;&#20013;&#30340;&#20219;&#20309;&#19968;&#20010;&#37117;&#23569;50\%&#12290;&#36825;&#20010;&#20998;&#26512;&#20984;&#26174;&#20102;ChatGPT&#22312;&#25216;&#26415;&#26415;&#35821;&#19978;&#19982;&#30495;&#23454;&#31185;&#23398;&#30340;&#26126;&#26174;&#24046;&#24322;&#12290;&#22312;&#23545;&#27599;&#31687;&#25991;&#31456;&#36827;&#34892;&#20998;&#31867;&#26102;&#65292;xFakeBibs&#31639;&#27861;&#20934;&#30830;&#22320;&#23558;98&#31687;&#20986;&#29256;&#29289;&#35782;&#21035;&#20026;&#20551;&#30340;&#65292;&#26377;2&#31687;&#25991;&#29486;&#38169;&#35823;&#22320;&#20998;&#31867;&#20026;&#30495;&#23454;&#20986;&#29256;&#29289;&#12290;&#23613;&#31649;&#36825;&#39033;&#24037;&#20316;&#24341;&#20837;&#20102;&#19968;&#31181;&#31639;&#27861;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
ChatGPT is becoming a new reality. In this paper, we show how to distinguish ChatGPT-generated publications from counterparts produced by scientists. Using a newly designed supervised Machine Learning algorithm, we demonstrate how to detect machine-generated publications from those produced by scientists. The algorithm was trained using 100 real publication abstracts, followed by a 10-fold calibration approach to establish a lower-upper bound range of acceptance. In the comparison with ChatGPT content, it was evident that ChatGPT contributed merely 23\% of the bigram content, which is less than 50\% of any of the other 10 calibrating folds. This analysis highlights a significant disparity in technical terms where ChatGPT fell short of matching real science. When categorizing the individual articles, the xFakeBibs algorithm accurately identified 98 out of 100 publications as fake, with 2 articles incorrectly classified as real publications. Though this work introduced an algorithmic app
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#24050;&#32463;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;DNN&#26041;&#27861;&#30340;&#38480;&#21046;&#65292;&#24182;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#35821;&#35328;&#29702;&#35299;&#12289;&#29983;&#25104;&#12289;&#25512;&#29702;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.02046</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#30340;&#25512;&#33616;&#31995;&#32479; (LLMs)
&lt;/p&gt;
&lt;p&gt;
Recommender Systems in the Era of Large Language Models (LLMs). (arXiv:2307.02046v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02046
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#24050;&#32463;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;DNN&#26041;&#27861;&#30340;&#38480;&#21046;&#65292;&#24182;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#35821;&#35328;&#29702;&#35299;&#12289;&#29983;&#25104;&#12289;&#25512;&#29702;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#30005;&#23376;&#21830;&#21153;&#21644;&#32593;&#32476;&#24212;&#29992;&#30340;&#32321;&#33635;&#65292;&#25512;&#33616;&#31995;&#32479;&#65288;RecSys&#65289;&#24050;&#32463;&#25104;&#20026;&#25105;&#20204;&#26085;&#24120;&#29983;&#27963;&#20013;&#37325;&#35201;&#30340;&#32452;&#25104;&#37096;&#20998;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#20010;&#24615;&#21270;&#24314;&#35758;&#20197;&#28385;&#36275;&#20854;&#21916;&#22909;&#12290;&#23613;&#31649;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#36890;&#36807;&#27169;&#25311;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#21644;&#25972;&#21512;&#25991;&#26412;&#20391;&#20449;&#24687;&#22312;&#25552;&#21319;&#25512;&#33616;&#31995;&#32479;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#65292;&#20294;&#26159;DNN&#26041;&#27861;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65292;&#20363;&#22914;&#29702;&#35299;&#29992;&#25143;&#20852;&#36259;&#12289;&#25429;&#25417;&#25991;&#26412;&#20391;&#20449;&#24687;&#30340;&#22256;&#38590;&#65292;&#20197;&#21450;&#22312;&#19981;&#21516;&#25512;&#33616;&#22330;&#26223;&#20013;&#27867;&#21270;&#21644;&#25512;&#29702;&#33021;&#21147;&#30340;&#19981;&#36275;&#31561;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#65288;&#20363;&#22914;ChatGPT&#21644;GPT4&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#39046;&#22495;&#24341;&#36215;&#20102;&#38761;&#21629;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#30340;&#22522;&#26412;&#32844;&#36131;&#19978;&#26377;&#30528;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#20855;&#26377;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#27867;&#21270;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the prosperity of e-commerce and web applications, Recommender Systems (RecSys) have become an important component of our daily life, providing personalized suggestions that cater to user preferences. While Deep Neural Networks (DNNs) have made significant advancements in enhancing recommender systems by modeling user-item interactions and incorporating textual side information, DNN-based methods still face limitations, such as difficulties in understanding users' interests and capturing textual side information, inabilities in generalizing to various recommendation scenarios and reasoning on their predictions, etc. Meanwhile, the emergence of Large Language Models (LLMs), such as ChatGPT and GPT4, has revolutionized the fields of Natural Language Processing (NLP) and Artificial Intelligence (AI), due to their remarkable abilities in fundamental responsibilities of language understanding and generation, as well as impressive generalization and reasoning capabilities. As a result, 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#22238;&#24402;&#25628;&#32034;&#24341;&#25806;&#26694;&#26550;AutoTSG&#65292;&#20854;&#29305;&#28857;&#26159;&#22522;&#20110;&#26080;&#24207;&#26415;&#35821;&#38598;&#30340;&#25991;&#26723;&#26631;&#35782;&#31526;&#21644;&#22522;&#20110;&#38598;&#21512;&#30340;&#29983;&#25104;&#31649;&#36947;&#65292;&#22823;&#22823;&#25918;&#26494;&#20102;&#23545;&#26631;&#35782;&#31526;&#31934;&#30830;&#29983;&#25104;&#30340;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2305.13859</link><description>&lt;p&gt;
&#26415;&#35821;&#38598;&#21487;&#20197;&#25104;&#20026;&#33258;&#22238;&#24402;&#25628;&#32034;&#24341;&#25806;&#30340;&#24378;&#25991;&#26723;&#26631;&#35782;&#31526;
&lt;/p&gt;
&lt;p&gt;
Term-Sets Can Be Strong Document Identifiers For Auto-Regressive Search Engines. (arXiv:2305.13859v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13859
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#22238;&#24402;&#25628;&#32034;&#24341;&#25806;&#26694;&#26550;AutoTSG&#65292;&#20854;&#29305;&#28857;&#26159;&#22522;&#20110;&#26080;&#24207;&#26415;&#35821;&#38598;&#30340;&#25991;&#26723;&#26631;&#35782;&#31526;&#21644;&#22522;&#20110;&#38598;&#21512;&#30340;&#29983;&#25104;&#31649;&#36947;&#65292;&#22823;&#22823;&#25918;&#26494;&#20102;&#23545;&#26631;&#35782;&#31526;&#31934;&#30830;&#29983;&#25104;&#30340;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#22238;&#24402;&#25628;&#32034;&#24341;&#25806;&#26159;&#19979;&#19968;&#20195;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#30340;&#26377;&#21069;&#36884;&#30340;&#33539;&#20363;&#12290;&#36825;&#20123;&#26041;&#27861;&#21033;&#29992;Seq2Seq&#27169;&#22411;&#65292;&#20854;&#20013;&#27599;&#20010;&#26597;&#35810;&#21487;&#20197;&#30452;&#25509;&#26144;&#23556;&#21040;&#20854;&#30456;&#20851;&#25991;&#26723;&#30340;&#26631;&#35782;&#31526;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#22240;&#20855;&#26377;&#31471;&#21040;&#31471;&#21487;&#24494;&#20998;&#24615;&#31561;&#20248;&#28857;&#32780;&#21463;&#21040;&#36190;&#25196;&#12290;&#28982;&#32780;&#65292;&#33258;&#22238;&#24402;&#25628;&#32034;&#24341;&#25806;&#22312;&#26816;&#32034;&#36136;&#37327;&#19978;&#20063;&#38754;&#20020;&#30528;&#25361;&#25112;&#65292;&#22240;&#20026;&#20854;&#38656;&#35201;&#23545;&#25991;&#26723;&#26631;&#35782;&#31526;&#36827;&#34892;&#31934;&#30830;&#29983;&#25104;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#22914;&#26524;&#22312;&#29983;&#25104;&#36807;&#31243;&#30340;&#20219;&#20309;&#19968;&#27493;&#20013;&#23545;&#20854;&#26631;&#35782;&#31526;&#20570;&#20986;&#20102;&#38169;&#35823;&#30340;&#39044;&#27979;&#65292;&#21017;&#30446;&#26631;&#25991;&#26723;&#23558;&#20174;&#26816;&#32034;&#32467;&#26524;&#20013;&#28431;&#22833;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21363;AutoTSG(&#33258;&#22238;&#24402;&#25628;&#32034;&#24341;&#25806;&#19982;&#26415;&#35821;&#38598;&#29983;&#25104;)&#65292;&#20854;&#29305;&#28857;&#26159;1)&#26080;&#24207;&#22522;&#20110;&#26415;&#35821;&#30340;&#25991;&#26723;&#26631;&#35782;&#31526;&#21644;2)&#22522;&#20110;&#38598;&#21512;&#30340;&#29983;&#25104;&#31649;&#36947;&#12290;&#21033;&#29992;AutoTSG&#65292;&#26415;&#35821;&#38598;&#26631;&#35782;&#31526;&#30340;&#20219;&#20309;&#25490;&#21015;&#37117;&#23558;&#23548;&#33268;&#30456;&#24212;&#25991;&#26723;&#30340;&#26816;&#32034;&#65292;&#20174;&#32780;&#22823;&#22823;&#25918;&#26494;&#20102;&#23545;&#26631;&#35782;&#31526;&#31934;&#30830;&#29983;&#25104;&#30340;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Auto-regressive search engines emerge as a promising paradigm for next-gen information retrieval systems. These methods work with Seq2Seq models, where each query can be directly mapped to the identifier of its relevant document. As such, they are praised for merits like being end-to-end differentiable. However, auto-regressive search engines also confront challenges in retrieval quality, given the requirement for the exact generation of the document identifier. That's to say, the targeted document will be missed from the retrieval result if a false prediction about its identifier is made in any step of the generation process. In this work, we propose a novel framework, namely AutoTSG (Auto-regressive Search Engine with Term-Set Generation), which is featured by 1) the unordered term-based document identifier and 2) the set-oriented generation pipeline. With AutoTSG, any permutation of the term-set identifier will lead to the retrieval of the corresponding document, thus largely relaxi
&lt;/p&gt;</description></item></channel></rss>