<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#20165;&#20351;&#29992;20&#65285;-50&#65285;&#30340;&#30456;&#20851;&#25991;&#26723;&#65292;&#36890;&#36807;&#22810;&#20803;&#32447;&#24615;&#22238;&#24402;&#35757;&#32451;&#24471;&#21040;&#30340;&#26435;&#37325;&#19982;&#20351;&#29992;&#20256;&#32479;&#26041;&#27861;&#24471;&#21040;&#30340;&#26435;&#37325;&#38750;&#24120;&#25509;&#36817;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26356;&#39640;&#25928;&#21644;&#21487;&#36127;&#25285;&#30340;&#25968;&#25454;&#34701;&#21512;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.04981</link><description>&lt;p&gt;
&#31934;&#31616;&#25968;&#25454;&#34701;&#21512;: &#20197;&#26368;&#23569;&#30340;&#30456;&#20851;&#24615;&#21028;&#26029;&#37322;&#25918;&#32447;&#24615;&#32452;&#21512;&#30340;&#21147;&#37327;
&lt;/p&gt;
&lt;p&gt;
Streamlined Data Fusion: Unleashing the Power of Linear Combination with Minimal Relevance Judgments. (arXiv:2309.04981v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04981
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#20165;&#20351;&#29992;20&#65285;-50&#65285;&#30340;&#30456;&#20851;&#25991;&#26723;&#65292;&#36890;&#36807;&#22810;&#20803;&#32447;&#24615;&#22238;&#24402;&#35757;&#32451;&#24471;&#21040;&#30340;&#26435;&#37325;&#19982;&#20351;&#29992;&#20256;&#32479;&#26041;&#27861;&#24471;&#21040;&#30340;&#26435;&#37325;&#38750;&#24120;&#25509;&#36817;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26356;&#39640;&#25928;&#21644;&#21487;&#36127;&#25285;&#30340;&#25968;&#25454;&#34701;&#21512;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32447;&#24615;&#32452;&#21512;&#26159;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#20013;&#19968;&#31181;&#24378;&#22823;&#30340;&#25968;&#25454;&#34701;&#21512;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#26681;&#25454;&#19981;&#21516;&#30340;&#24773;&#22659;&#35843;&#25972;&#26435;&#37325;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#19978;&#23454;&#29616;&#26368;&#20248;&#26435;&#37325;&#35757;&#32451;&#36890;&#24120;&#38656;&#35201;&#23545;&#22823;&#37096;&#20998;&#25991;&#26723;&#36827;&#34892;&#20154;&#24037;&#30456;&#20851;&#24615;&#21028;&#26029;&#65292;&#36825;&#26159;&#19968;&#39033;&#36153;&#26102;&#36153;&#21147;&#30340;&#36807;&#31243;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20165;&#20351;&#29992;20&#65285;-50&#65285;&#30340;&#30456;&#20851;&#25991;&#26723;&#33719;&#21462;&#25509;&#36817;&#26368;&#20248;&#26435;&#37325;&#30340;&#21487;&#34892;&#24615;&#12290;&#36890;&#36807;&#23545;&#22235;&#20010;TREC&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#20351;&#29992;&#36825;&#31181;&#20943;&#23569;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#22810;&#20803;&#32447;&#24615;&#22238;&#24402;&#35757;&#32451;&#24471;&#21040;&#30340;&#26435;&#37325;&#19982;&#20351;&#29992;TREC&#23448;&#26041;"qrels"&#24471;&#21040;&#30340;&#26435;&#37325;&#38750;&#24120;&#25509;&#36817;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;&#26356;&#39640;&#25928;&#12289;&#26356;&#32463;&#27982;&#30340;&#25968;&#25454;&#34701;&#21512;&#28508;&#21147;&#65292;&#20351;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#33021;&#22815;&#22312;&#26356;&#23569;&#30340;&#24037;&#20316;&#37327;&#19979;&#20805;&#20998;&#20139;&#21463;&#20854;&#25152;&#24102;&#26469;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Linear combination is a potent data fusion method in information retrieval tasks, thanks to its ability to adjust weights for diverse scenarios. However, achieving optimal weight training has traditionally required manual relevance judgments on a large percentage of documents, a labor-intensive and expensive process. In this study, we investigate the feasibility of obtaining near-optimal weights using a mere 20\%-50\% of relevant documents. Through experiments on four TREC datasets, we find that weights trained with multiple linear regression using this reduced set closely rival those obtained with TREC's official "qrels." Our findings unlock the potential for more efficient and affordable data fusion, empowering researchers and practitioners to reap its full benefits with significantly less effort.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MUFIN&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#22788;&#29702;&#20855;&#26377;&#25968;&#30334;&#19975;&#20010;&#26631;&#31614;&#30340;&#22810;&#27169;&#24577;&#26497;&#31471;&#20998;&#31867;&#20219;&#21153;&#65292;&#35813;&#25216;&#26415;&#22312;&#20135;&#21697;&#25512;&#33616;&#21644;&#31454;&#26631;&#26597;&#35810;&#39044;&#27979;&#20013;&#20855;&#26377;&#24212;&#29992;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2309.04961</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#26497;&#31471;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Multi-modal Extreme Classification. (arXiv:2309.04961v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04961
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MUFIN&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#22788;&#29702;&#20855;&#26377;&#25968;&#30334;&#19975;&#20010;&#26631;&#31614;&#30340;&#22810;&#27169;&#24577;&#26497;&#31471;&#20998;&#31867;&#20219;&#21153;&#65292;&#35813;&#25216;&#26415;&#22312;&#20135;&#21697;&#25512;&#33616;&#21644;&#31454;&#26631;&#26597;&#35810;&#39044;&#27979;&#20013;&#20855;&#26377;&#24212;&#29992;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#20855;&#26377;&#25968;&#30334;&#19975;&#20010;&#26631;&#31614;&#30340;&#26497;&#31471;&#20998;&#31867;&#20219;&#21153;&#65292;&#21457;&#23637;&#20102;&#19968;&#31181;&#21517;&#20026;MUFIN&#30340;&#25216;&#26415;&#65292;&#20854;&#20013;&#25968;&#25454;&#28857;&#21644;&#26631;&#31614;&#20855;&#26377;&#35270;&#35273;&#21644;&#25991;&#26412;&#25551;&#36848;&#31526;&#12290;&#23558;MUFIN&#24212;&#29992;&#20110;&#25968;&#30334;&#19975;&#20010;&#20135;&#21697;&#30340;&#20135;&#21697;&#25512;&#33616;&#21644;&#31454;&#26631;&#26597;&#35810;&#39044;&#27979;&#20013;&#12290;&#24403;&#20195;&#30340;&#22810;&#27169;&#24577;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#20165;&#23884;&#20837;&#24335;&#26041;&#27861;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;XC&#26041;&#27861;&#21033;&#29992;&#20998;&#31867;&#22120;&#26550;&#26500;&#25552;&#20379;&#27604;&#20165;&#23884;&#20837;&#24335;&#26041;&#27861;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#20294;&#20027;&#35201;&#19987;&#27880;&#20110;&#22522;&#20110;&#25991;&#26412;&#30340;&#20998;&#31867;&#20219;&#21153;&#12290;MUFIN&#36890;&#36807;&#23558;&#22810;&#27169;&#24577;&#20998;&#31867;&#37325;&#26032;&#21046;&#23450;&#20026;&#20855;&#26377;&#25968;&#30334;&#19975;&#20010;&#26631;&#31614;&#30340;XC&#38382;&#39064;&#26469;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#12290;&#36825;&#25552;&#20986;&#20102;&#20004;&#20010;&#25361;&#25112;&#65306;&#24320;&#21457;&#33021;&#22815;&#25552;&#20379;&#36275;&#22815;&#34920;&#36798;&#21147;&#20197;&#23454;&#29616;&#23545;&#25968;&#30334;&#19975;&#20010;&#26631;&#31614;&#36827;&#34892;&#20934;&#30830;&#20998;&#31867;&#30340;&#22810;&#27169;&#24577;&#26550;&#26500;&#65307;&#20197;&#21450;&#22312;&#26631;&#31614;&#25968;&#37327;&#30340;&#23545;&#25968;&#23610;&#24230;&#19978;&#25193;&#23637;&#35757;&#32451;&#21644;&#25512;&#29702;&#20363;&#31243;&#12290;MUFIN&#22522;&#20110;&#20132;&#21449;&#30340;&#26041;&#27861;&#24320;&#21457;&#20102;&#19968;&#31181;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper develops the MUFIN technique for extreme classification (XC) tasks with millions of labels where datapoints and labels are endowed with visual and textual descriptors. Applications of MUFIN to product-to-product recommendation and bid query prediction over several millions of products are presented. Contemporary multi-modal methods frequently rely on purely embedding-based methods. On the other hand, XC methods utilize classifier architectures to offer superior accuracies than embedding only methods but mostly focus on text-based categorization tasks. MUFIN bridges this gap by reformulating multi-modal categorization as an XC problem with several millions of labels. This presents the twin challenges of developing multi-modal architectures that can offer embeddings sufficiently expressive to allow accurate categorization over millions of labels; and training and inference routines that scale logarithmically in the number of labels. MUFIN develops an architecture based on cros
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#30340;&#22810;&#20010;k-means&#32858;&#31867;&#38598;&#25104;&#26694;&#26550;&#65292;&#29992;&#20110;&#32858;&#31867;&#24341;&#29992;&#36712;&#36857;&#12290;&#36825;&#26377;&#21161;&#20110;&#29702;&#35299;&#30693;&#35782;&#20256;&#25773;&#36807;&#31243;&#65292;&#24182;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#21442;&#25968;&#12289;&#23450;&#20041;&#27169;&#31946;&#21644;&#21482;&#25429;&#25417;&#26497;&#31471;&#36712;&#36857;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.04949</link><description>&lt;p&gt;
&#29992;&#20110;&#32858;&#31867;&#24341;&#29992;&#36712;&#36857;&#30340;&#22810;&#20010;K-means&#32858;&#31867;&#38598;&#25104;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A multiple k-means cluster ensemble framework for clustering citation trajectories. (arXiv:2309.04949v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04949
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#30340;&#22810;&#20010;k-means&#32858;&#31867;&#38598;&#25104;&#26694;&#26550;&#65292;&#29992;&#20110;&#32858;&#31867;&#24341;&#29992;&#36712;&#36857;&#12290;&#36825;&#26377;&#21161;&#20110;&#29702;&#35299;&#30693;&#35782;&#20256;&#25773;&#36807;&#31243;&#65292;&#24182;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#21442;&#25968;&#12289;&#23450;&#20041;&#27169;&#31946;&#21644;&#21482;&#25429;&#25417;&#26497;&#31471;&#36712;&#36857;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24341;&#29992;&#25104;&#29087;&#26102;&#38388;&#22240;&#25991;&#31456;&#32780;&#24322;&#65292;&#28982;&#32780;&#25152;&#26377;&#25991;&#31456;&#30340;&#24433;&#21709;&#21147;&#37117;&#26159;&#22312;&#19968;&#20010;&#22266;&#23450;&#31383;&#21475;&#20869;&#34913;&#37327;&#30340;&#12290;&#23545;&#23427;&#20204;&#30340;&#24341;&#29992;&#36712;&#36857;&#36827;&#34892;&#32858;&#31867;&#26377;&#21161;&#20110;&#29702;&#35299;&#30693;&#35782;&#25193;&#25955;&#36807;&#31243;&#65292;&#24182;&#25581;&#31034;&#24182;&#38750;&#25152;&#26377;&#25991;&#31456;&#22312;&#21457;&#34920;&#21518;&#37117;&#31435;&#21363;&#33719;&#24471;&#25104;&#21151;&#12290;&#27492;&#22806;&#65292;&#23545;&#36712;&#36857;&#36827;&#34892;&#32858;&#31867;&#20063;&#23545;&#35770;&#25991;&#24433;&#21709;&#21147;&#25512;&#33616;&#31639;&#27861;&#33267;&#20851;&#37325;&#35201;&#12290;&#30001;&#20110;&#24341;&#29992;&#26102;&#38388;&#24207;&#21015;&#20855;&#26377;&#38750;&#32447;&#24615;&#21644;&#38750;&#24179;&#31283;&#29305;&#24615;&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#32452;&#20219;&#24847;&#30340;&#38408;&#20540;&#21644;&#22522;&#20110;&#35268;&#21017;&#30340;&#22266;&#23450;&#26041;&#27861;&#12290;&#25152;&#26377;&#30340;&#26041;&#27861;&#20027;&#35201;&#37117;&#20381;&#36182;&#20110;&#21442;&#25968;&#65292;&#22240;&#27492;&#22312;&#23450;&#20041;&#30456;&#20284;&#30340;&#36712;&#36857;&#21644;&#20851;&#20110;&#29305;&#23450;&#25968;&#30446;&#30340;&#27169;&#31946;&#24615;&#26041;&#38754;&#23548;&#33268;&#20102;&#19981;&#19968;&#33268;&#24615;&#12290;&#22823;&#22810;&#25968;&#30740;&#31350;&#21482;&#25429;&#25417;&#20102;&#26497;&#31471;&#30340;&#36712;&#36857;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#19968;&#20010;&#36890;&#29992;&#30340;&#32858;&#31867;&#26694;&#26550;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#29305;&#24449;&#30340;&#22810;&#20010;k-means&#32858;&#31867;&#38598;&#25104;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Citation maturity time varies for different articles. However, the impact of all articles is measured in a fixed window. Clustering their citation trajectories helps understand the knowledge diffusion process and reveals that not all articles gain immediate success after publication. Moreover, clustering trajectories is necessary for paper impact recommendation algorithms. It is a challenging problem because citation time series exhibit significant variability due to non linear and non stationary characteristics. Prior works propose a set of arbitrary thresholds and a fixed rule based approach. All methods are primarily parameter dependent. Consequently, it leads to inconsistencies while defining similar trajectories and ambiguities regarding their specific number. Most studies only capture extreme trajectories. Thus, a generalised clustering framework is required. This paper proposes a feature based multiple k means cluster ensemble framework. 1,95,783 and 41,732 well cited articles f
&lt;/p&gt;</description></item><item><title>RecAD&#26159;&#19968;&#20010;&#26088;&#22312;&#24314;&#31435;&#25512;&#33616;&#25915;&#20987;&#21644;&#38450;&#24481;&#30340;&#24320;&#25918;&#22522;&#20934;&#30340;&#32479;&#19968;&#24211;&#65292;&#36890;&#36807;&#25972;&#21512;&#25968;&#25454;&#38598;&#12289;&#28304;&#20195;&#30721;&#12289;&#21442;&#25968;&#35774;&#32622;&#12289;&#36816;&#34892;&#26085;&#24535;&#12289;&#25915;&#20987;&#30693;&#35782;&#12289;&#25915;&#20987;&#39044;&#31639;&#21644;&#35780;&#20272;&#32467;&#26524;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#19968;&#20010;&#21487;&#22797;&#29616;&#30340;&#30740;&#31350;&#27969;&#31243;&#12290;</title><link>http://arxiv.org/abs/2309.04884</link><description>&lt;p&gt;
RecAD: &#21521;&#32479;&#19968;&#30340;&#25512;&#33616;&#25915;&#20987;&#21644;&#38450;&#24481;&#24211;&#36808;&#36827;
&lt;/p&gt;
&lt;p&gt;
RecAD: Towards A Unified Library for Recommender Attack and Defense. (arXiv:2309.04884v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04884
&lt;/p&gt;
&lt;p&gt;
RecAD&#26159;&#19968;&#20010;&#26088;&#22312;&#24314;&#31435;&#25512;&#33616;&#25915;&#20987;&#21644;&#38450;&#24481;&#30340;&#24320;&#25918;&#22522;&#20934;&#30340;&#32479;&#19968;&#24211;&#65292;&#36890;&#36807;&#25972;&#21512;&#25968;&#25454;&#38598;&#12289;&#28304;&#20195;&#30721;&#12289;&#21442;&#25968;&#35774;&#32622;&#12289;&#36816;&#34892;&#26085;&#24535;&#12289;&#25915;&#20987;&#30693;&#35782;&#12289;&#25915;&#20987;&#39044;&#31639;&#21644;&#35780;&#20272;&#32467;&#26524;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#19968;&#20010;&#21487;&#22797;&#29616;&#30340;&#30740;&#31350;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#25512;&#33616;&#31995;&#32479;&#24050;&#32463;&#25104;&#20026;&#25105;&#20204;&#26085;&#24120;&#29983;&#27963;&#20013;&#26080;&#22788;&#19981;&#22312;&#30340;&#19968;&#37096;&#20998;&#65292;&#28982;&#32780;&#30001;&#20110;&#19981;&#26029;&#22686;&#38271;&#30340;&#21830;&#19994;&#21644;&#31038;&#20250;&#20215;&#20540;&#65292;&#23427;&#20204;&#38754;&#20020;&#30528;&#34987;&#25915;&#20987;&#30340;&#39640;&#39118;&#38505;&#12290;&#23613;&#31649;&#22312;&#25512;&#33616;&#25915;&#20987;&#21644;&#38450;&#24481;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#30340;&#30740;&#31350;&#36827;&#23637;&#65292;&#20294;&#35813;&#39046;&#22495;&#32570;&#20047;&#24191;&#27867;&#35748;&#21487;&#30340;&#22522;&#20934;&#26631;&#20934;&#65292;&#23548;&#33268;&#24615;&#33021;&#27604;&#36739;&#19981;&#20844;&#24179;&#19988;&#23454;&#39564;&#21487;&#20449;&#24230;&#26377;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RecAD&#65292;&#19968;&#20010;&#26088;&#22312;&#24314;&#31435;&#25512;&#33616;&#25915;&#20987;&#21644;&#38450;&#24481;&#30340;&#24320;&#25918;&#22522;&#20934;&#30340;&#32479;&#19968;&#24211;&#12290;RecAD&#36890;&#36807;&#25972;&#21512;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#12289;&#26631;&#20934;&#28304;&#20195;&#30721;&#12289;&#36229;&#21442;&#25968;&#35774;&#32622;&#12289;&#36816;&#34892;&#26085;&#24535;&#12289;&#25915;&#20987;&#30693;&#35782;&#12289;&#25915;&#20987;&#39044;&#31639;&#21644;&#35780;&#20272;&#32467;&#26524;&#65292;&#21021;&#27493;&#24314;&#31435;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#22522;&#20934;&#27979;&#35797;&#27969;&#31243;&#65292;&#20197;&#23454;&#29616;&#21487;&#22797;&#29616;&#30340;&#30740;&#31350;&#12290;&#35813;&#22522;&#20934;&#27979;&#35797;&#26088;&#22312;&#20840;&#38754;&#19988;&#21487;&#25345;&#32493;&#65292;&#28085;&#30422;&#25915;&#20987;&#12289;&#38450;&#24481;&#21644;&#35780;&#20272;&#20219;&#21153;&#65292;&#20351;&#26356;&#22810;&#30340;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#36731;&#26494;&#22320;&#36861;&#38543;&#21644;&#36129;&#29486;&#36825;&#20010;&#26377;&#21069;&#26223;&#30340;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, recommender systems have become a ubiquitous part of our daily lives, while they suffer from a high risk of being attacked due to the growing commercial and social values. Despite significant research progress in recommender attack and defense, there is a lack of a widely-recognized benchmarking standard in the field, leading to unfair performance comparison and limited credibility of experiments. To address this, we propose RecAD, a unified library aiming at establishing an open benchmark for recommender attack and defense. RecAD takes an initial step to set up a unified benchmarking pipeline for reproducible research by integrating diverse datasets, standard source codes, hyper-parameter settings, running logs, attack knowledge, attack budget, and evaluation results. The benchmark is designed to be comprehensive and sustainable, covering both attack, defense, and evaluation tasks, enabling more researchers to easily follow and contribute to this promising field. RecA
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38899;&#20048;&#27969;&#27966;&#20998;&#31867;&#65292;&#20351;&#29992;&#20102;&#25968;&#23383;&#20449;&#21495;&#22788;&#29702;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#20174;&#38899;&#39057;&#20449;&#21495;&#20013;&#25552;&#21462;&#29305;&#24449;&#24182;&#36827;&#34892;&#20998;&#31867;&#12290;&#35813;&#31639;&#27861;&#22312;GTZAN&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#39640;&#31934;&#24230;&#65292;&#21516;&#26102;&#36824;&#25552;&#20986;&#20102;&#31471;&#21040;&#31471;&#30340;&#37096;&#32626;&#26550;&#26500;&#65292;&#21487;&#29992;&#20110;&#38899;&#20048;&#24212;&#29992;&#31243;&#24207;&#30340;&#38598;&#25104;&#12290;</title><link>http://arxiv.org/abs/2309.04861</link><description>&lt;p&gt;
&#25506;&#32034;&#38899;&#20048;&#27969;&#27966;&#20998;&#31867;&#65306;&#31639;&#27861;&#20998;&#26512;&#19982;&#37096;&#32626;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Exploring Music Genre Classification: Algorithm Analysis and Deployment Architecture. (arXiv:2309.04861v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04861
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38899;&#20048;&#27969;&#27966;&#20998;&#31867;&#65292;&#20351;&#29992;&#20102;&#25968;&#23383;&#20449;&#21495;&#22788;&#29702;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#20174;&#38899;&#39057;&#20449;&#21495;&#20013;&#25552;&#21462;&#29305;&#24449;&#24182;&#36827;&#34892;&#20998;&#31867;&#12290;&#35813;&#31639;&#27861;&#22312;GTZAN&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#39640;&#31934;&#24230;&#65292;&#21516;&#26102;&#36824;&#25552;&#20986;&#20102;&#31471;&#21040;&#31471;&#30340;&#37096;&#32626;&#26550;&#26500;&#65292;&#21487;&#29992;&#20110;&#38899;&#20048;&#24212;&#29992;&#31243;&#24207;&#30340;&#38598;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#21508;&#31181;&#27969;&#23186;&#20307;&#24212;&#29992;&#30340;&#20986;&#29616;&#65292;&#38899;&#20048;&#27969;&#27966;&#20998;&#31867;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#22914;&#20170;&#65292;&#22312;&#19968;&#20010;&#22797;&#26434;&#30340;&#38899;&#20048;&#24212;&#29992;&#31243;&#24207;&#20013;&#65292;&#25105;&#20204;&#26080;&#27861;&#24819;&#35937;&#20165;&#36890;&#36807;&#33402;&#26415;&#23478;&#30340;&#21517;&#23383;&#21644;&#27468;&#26354;&#26631;&#39064;&#26469;&#25628;&#32034;&#38899;&#20048;&#12290;&#27491;&#30830;&#20998;&#31867;&#38899;&#20048;&#19968;&#30452;&#24456;&#22256;&#38590;&#65292;&#22240;&#20026;&#19982;&#38899;&#20048;&#30456;&#20851;&#30340;&#20449;&#24687;&#65292;&#22914;&#22320;&#21306;&#12289;&#33402;&#26415;&#23478;&#12289;&#19987;&#36753;&#25110;&#38750;&#19987;&#36753;&#65292;&#26159;&#22914;&#27492;&#22810;&#21464;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#39033;&#20851;&#20110;&#38899;&#20048;&#27969;&#27966;&#20998;&#31867;&#30340;&#30740;&#31350;&#65292;&#20351;&#29992;&#20102;&#25968;&#23383;&#20449;&#21495;&#22788;&#29702;&#65288;DSP&#65289;&#21644;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#25216;&#26415;&#30340;&#32452;&#21512;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#65292;&#21033;&#29992;DSP&#21644;DL&#26041;&#27861;&#20174;&#38899;&#39057;&#20449;&#21495;&#20013;&#25552;&#21462;&#30456;&#20851;&#29305;&#24449;&#65292;&#24182;&#23558;&#20854;&#20998;&#31867;&#21040;&#21508;&#31181;&#27969;&#27966;&#20013;&#12290;&#35813;&#31639;&#27861;&#22312;GTZAN&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#24182;&#21462;&#24471;&#20102;&#39640;&#31934;&#24230;&#12290;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#37096;&#32626;&#26550;&#26500;&#65292;&#29992;&#20110;&#38598;&#25104;&#21040;&#19982;&#38899;&#20048;&#30456;&#20851;&#30340;&#24212;&#29992;&#31243;&#24207;&#20013;&#12290;&#23545;&#31639;&#27861;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#24182;&#35752;&#35770;&#20102;&#25913;&#36827;&#30340;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Music genre classification has become increasingly critical with the advent of various streaming applications. Nowadays, we find it impossible to imagine using the artist's name and song title to search for music in a sophisticated music app. It is always difficult to classify music correctly because the information linked to music, such as region, artist, album, or non-album, is so variable. This paper presents a study on music genre classification using a combination of Digital Signal Processing (DSP) and Deep Learning (DL) techniques. A novel algorithm is proposed that utilizes both DSP and DL methods to extract relevant features from audio signals and classify them into various genres. The algorithm was tested on the GTZAN dataset and achieved high accuracy. An end-to-end deployment architecture is also proposed for integration into music-related applications. The performance of the algorithm is analyzed and future directions for improvement are discussed. The proposed DSP and DL-b
&lt;/p&gt;</description></item><item><title>CPMR&#26159;&#19968;&#20010;&#22522;&#20110;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#22686;&#37327;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#21019;&#24314;&#38745;&#24577;&#23884;&#20837;&#12289;&#21382;&#21490;&#26102;&#38388;&#29366;&#24577;&#21644;&#19978;&#19979;&#25991;&#26102;&#38388;&#29366;&#24577;&#30340;&#19977;&#20010;&#34920;&#31034;&#65292;&#20934;&#30830;&#22320;&#24314;&#27169;&#20102;&#29992;&#25143;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#34920;&#31034;&#21644;&#20852;&#36259;&#21160;&#24577;&#30340;&#28436;&#21270;&#12290;</title><link>http://arxiv.org/abs/2309.04802</link><description>&lt;p&gt;
CPMR: &#22522;&#20110;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#22686;&#37327;&#39034;&#24207;&#25512;&#33616;&#19982;&#20266;&#22810;&#20219;&#21153;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
CPMR: Context-Aware Incremental Sequential Recommendation with Pseudo-Multi-Task Learning. (arXiv:2309.04802v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04802
&lt;/p&gt;
&lt;p&gt;
CPMR&#26159;&#19968;&#20010;&#22522;&#20110;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#22686;&#37327;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#21019;&#24314;&#38745;&#24577;&#23884;&#20837;&#12289;&#21382;&#21490;&#26102;&#38388;&#29366;&#24577;&#21644;&#19978;&#19979;&#25991;&#26102;&#38388;&#29366;&#24577;&#30340;&#19977;&#20010;&#34920;&#31034;&#65292;&#20934;&#30830;&#22320;&#24314;&#27169;&#20102;&#29992;&#25143;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#34920;&#31034;&#21644;&#20852;&#36259;&#21160;&#24577;&#30340;&#28436;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#25143;&#36827;&#34892;&#20114;&#21160;&#30340;&#21160;&#26426;&#21487;&#20197;&#20998;&#20026;&#38745;&#24577;&#20559;&#22909;&#21644;&#21160;&#24577;&#20852;&#36259;&#12290;&#20026;&#20102;&#20934;&#30830;&#22320;&#24314;&#27169;&#29992;&#25143;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#34920;&#31034;&#65292;&#26368;&#36817;&#30340;&#39034;&#24207;&#25512;&#33616;&#30740;&#31350;&#21033;&#29992;&#20449;&#24687;&#20256;&#25773;&#21644;&#28436;&#21270;&#20174;&#25209;&#37327;&#21040;&#36798;&#30340;&#20114;&#21160;&#20013;&#36827;&#34892;&#25366;&#25496;&#12290;&#28982;&#32780;&#65292;&#20182;&#20204;&#24573;&#30053;&#20102;&#22312;&#19978;&#19979;&#25991;&#22330;&#26223;&#20013;&#20154;&#20204;&#24456;&#23481;&#26131;&#21463;&#21040;&#20854;&#20182;&#29992;&#25143;&#30340;&#26368;&#36817;&#34892;&#20026;&#30340;&#24433;&#21709;&#65292;&#24182;&#19988;&#22312;&#25152;&#26377;&#21382;&#21490;&#20114;&#21160;&#20013;&#24212;&#29992;&#28436;&#21270;&#20250;&#31232;&#37322;&#26368;&#36817;&#20114;&#21160;&#30340;&#37325;&#35201;&#24615;&#65292;&#20174;&#32780;&#26080;&#27861;&#20934;&#30830;&#22320;&#24314;&#27169;&#20852;&#36259;&#21160;&#24577;&#30340;&#28436;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#20266;&#22810;&#20219;&#21153;&#25512;&#33616;&#31995;&#32479;&#65288;CPMR&#65289;&#65292;&#36890;&#36807;&#20026;&#27599;&#20010;&#29992;&#25143;&#21644;&#39033;&#30446;&#21019;&#24314;&#19977;&#20010;&#34920;&#31034;&#65288;&#38745;&#24577;&#23884;&#20837;&#12289;&#21382;&#21490;&#26102;&#38388;&#29366;&#24577;&#21644;&#19978;&#19979;&#25991;&#26102;&#38388;&#29366;&#24577;&#65289;&#65292;&#26469;&#24314;&#27169;&#21382;&#21490;&#21644;&#19978;&#19979;&#25991;&#24773;&#22659;&#20013;&#30340;&#28436;&#21270;&#12290;&#20026;&#20102;&#21516;&#26102;&#25552;&#39640;&#26102;&#38388;&#29366;&#24577;&#28436;&#21270;&#21644;&#22686;&#37327;&#25512;&#33616;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The motivations of users to make interactions can be divided into static preference and dynamic interest. To accurately model user representations over time, recent studies in sequential recommendation utilize information propagation and evolution to mine from batches of arriving interactions. However, they ignore the fact that people are easily influenced by the recent actions of other users in the contextual scenario, and applying evolution across all historical interactions dilutes the importance of recent ones, thus failing to model the evolution of dynamic interest accurately. To address this issue, we propose a Context-Aware Pseudo-Multi-Task Recommender System (CPMR) to model the evolution in both historical and contextual scenarios by creating three representations for each user and item under different dynamics: static embedding, historical temporal states, and contextual temporal states. To dually improve the performance of temporal states evolution and incremental recommenda
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#30740;&#32508;&#21512;&#23457;&#26597;&#20102;&#22312;&#25945;&#32946;&#25968;&#25454;&#25366;&#25496;&#20013;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#65292;&#21253;&#25324;&#23545;&#30693;&#35782;&#36319;&#36394;&#12289;&#23398;&#29983;&#19981;&#33391;&#34892;&#20026;&#26816;&#27979;&#12289;&#24615;&#33021;&#39044;&#27979;&#21644;&#20010;&#24615;&#21270;&#25512;&#33616;&#31561;&#20856;&#22411;&#25945;&#32946;&#22330;&#26223;&#30340;&#24212;&#29992;&#12290;&#21516;&#26102;&#25552;&#20379;&#20102;&#20844;&#20849;&#25968;&#25454;&#38598;&#21644;&#22788;&#29702;&#24037;&#20855;&#30340;&#32508;&#21512;&#27010;&#36848;&#65292;&#24182;&#25351;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2309.04761</link><description>&lt;p&gt;
&#22312;&#25945;&#32946;&#25968;&#25454;&#25366;&#25496;&#20013;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#32508;&#21512;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Survey on Deep Learning Techniques in Educational Data Mining. (arXiv:2309.04761v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04761
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#30740;&#32508;&#21512;&#23457;&#26597;&#20102;&#22312;&#25945;&#32946;&#25968;&#25454;&#25366;&#25496;&#20013;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#65292;&#21253;&#25324;&#23545;&#30693;&#35782;&#36319;&#36394;&#12289;&#23398;&#29983;&#19981;&#33391;&#34892;&#20026;&#26816;&#27979;&#12289;&#24615;&#33021;&#39044;&#27979;&#21644;&#20010;&#24615;&#21270;&#25512;&#33616;&#31561;&#20856;&#22411;&#25945;&#32946;&#22330;&#26223;&#30340;&#24212;&#29992;&#12290;&#21516;&#26102;&#25552;&#20379;&#20102;&#20844;&#20849;&#25968;&#25454;&#38598;&#21644;&#22788;&#29702;&#24037;&#20855;&#30340;&#32508;&#21512;&#27010;&#36848;&#65292;&#24182;&#25351;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25945;&#32946;&#25968;&#25454;&#25366;&#25496;(EDM)&#20316;&#20026;&#30740;&#31350;&#30340;&#37325;&#35201;&#39046;&#22495;&#65292;&#21033;&#29992;&#35745;&#31639;&#25216;&#26415;&#26469;&#20998;&#26512;&#25945;&#32946;&#25968;&#25454;&#12290;&#38543;&#30528;&#25945;&#32946;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#21644;&#22810;&#26679;&#24615;&#22686;&#21152;&#65292;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#35299;&#20915;&#20998;&#26512;&#21644;&#24314;&#27169;&#36825;&#20123;&#25968;&#25454;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#20248;&#21183;&#12290;&#26412;&#35843;&#30740;&#26088;&#22312;&#31995;&#32479;&#22320;&#23457;&#26597;&#28145;&#24230;&#23398;&#20064;&#22312;EDM&#39046;&#22495;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20379;&#20102;&#20851;&#20110;EDM&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#31616;&#35201;&#20171;&#32461;&#65292;&#24378;&#35843;&#20102;&#23427;&#20204;&#22312;&#29616;&#20195;&#25945;&#32946;&#29615;&#22659;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#35814;&#32454;&#22238;&#39038;&#20102;&#22312;&#22235;&#20010;&#20856;&#22411;&#25945;&#32946;&#22330;&#26223;&#20013;&#24212;&#29992;&#30340;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#21253;&#25324;&#30693;&#35782;&#36319;&#36394;&#12289;&#23398;&#29983;&#19981;&#33391;&#34892;&#20026;&#26816;&#27979;&#12289;&#24615;&#33021;&#39044;&#27979;&#21644;&#20010;&#24615;&#21270;&#25512;&#33616;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;EDM&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#21644;&#22788;&#29702;&#24037;&#20855;&#30340;&#32508;&#21512;&#27010;&#36848;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25351;&#20986;&#20102;&#35813;&#30740;&#31350;&#39046;&#22495;&#30340;&#26032;&#20852;&#36235;&#21183;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Educational Data Mining (EDM) has emerged as a vital field of research, which harnesses the power of computational techniques to analyze educational data. With the increasing complexity and diversity of educational data, Deep Learning techniques have shown significant advantages in addressing the challenges associated with analyzing and modeling this data. This survey aims to systematically review the state-of-the-art in EDM with Deep Learning. We begin by providing a brief introduction to EDM and Deep Learning, highlighting their relevance in the context of modern education. Next, we present a detailed review of Deep Learning techniques applied in four typical educational scenarios, including knowledge tracing, undesirable student detecting, performance prediction, and personalized recommendation. Furthermore, a comprehensive overview of public datasets and processing tools for EDM is provided. Finally, we point out emerging trends and future directions in this research area.
&lt;/p&gt;</description></item><item><title>&#26412;&#25945;&#31243;&#25552;&#20379;&#20102;&#23545;&#35805;&#24335;&#20154;&#24037;&#26234;&#33021;&#20013;&#25968;&#25454;&#22686;&#24378;&#30340;&#32508;&#36848;&#65292;&#21253;&#25324;&#23545;&#35805;&#22686;&#24378;&#12289;&#24320;&#25918;&#22495;&#21644;&#20219;&#21153;&#23548;&#21521;&#30340;&#23545;&#35805;&#29983;&#25104;&#20197;&#21450;&#35780;&#20272;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#36824;&#35752;&#35770;&#20102;&#24403;&#21069;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#30340;&#21457;&#23637;&#26041;&#21521;&#65292;&#20197;&#24110;&#21161;&#25512;&#21160;&#35813;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2309.04739</link><description>&lt;p&gt;
&#23545;&#35805;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#25968;&#25454;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Data Augmentation for Conversational AI. (arXiv:2309.04739v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04739
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25945;&#31243;&#25552;&#20379;&#20102;&#23545;&#35805;&#24335;&#20154;&#24037;&#26234;&#33021;&#20013;&#25968;&#25454;&#22686;&#24378;&#30340;&#32508;&#36848;&#65292;&#21253;&#25324;&#23545;&#35805;&#22686;&#24378;&#12289;&#24320;&#25918;&#22495;&#21644;&#20219;&#21153;&#23548;&#21521;&#30340;&#23545;&#35805;&#29983;&#25104;&#20197;&#21450;&#35780;&#20272;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#36824;&#35752;&#35770;&#20102;&#24403;&#21069;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#30340;&#21457;&#23637;&#26041;&#21521;&#65292;&#20197;&#24110;&#21161;&#25512;&#21160;&#35813;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#31995;&#32479;&#30340;&#21457;&#23637;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#20449;&#24687;&#33719;&#21462;&#26041;&#24335;&#65292;&#36229;&#36234;&#20102;&#21333;&#19968;&#26597;&#35810;&#30340;&#38480;&#21046;&#12290;&#28982;&#32780;&#65292;&#24320;&#21457;&#23545;&#35805;&#31995;&#32479;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#39046;&#22495;&#21644;&#35821;&#35328;&#20013;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20256;&#32479;&#30340;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;&#65292;&#22914;&#20247;&#21253;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#20154;&#21147;&#21644;&#26102;&#38388;&#65292;&#22240;&#27492;&#22312;&#27492;&#24773;&#26223;&#19979;&#25928;&#29575;&#20302;&#19979;&#12290;&#25968;&#25454;&#22686;&#24378;&#65288;DA&#65289;&#26159;&#19968;&#31181;&#32531;&#35299;&#23545;&#35805;&#31995;&#32479;&#20013;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#26412;&#25945;&#31243;&#20840;&#38754;&#19988;&#26368;&#26032;&#22320;&#27010;&#36848;&#20102;&#22312;&#23545;&#35805;&#31995;&#32479;&#20013;&#20351;&#29992;&#30340;DA&#26041;&#27861;&#65292;&#21253;&#25324;&#23545;&#35805;&#22686;&#24378;&#12289;&#24320;&#25918;&#22495;&#21644;&#20219;&#21153;&#23548;&#21521;&#30340;&#23545;&#35805;&#29983;&#25104;&#20197;&#21450;&#19981;&#21516;&#30340;&#35780;&#20272;&#27169;&#22411;&#30340;&#33539;&#24335;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#24403;&#21069;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#30340;&#21457;&#23637;&#26041;&#21521;&#65292;&#20197;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#36827;&#19968;&#27493;&#25512;&#21160;&#36825;&#19968;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Advancements in conversational systems have revolutionized information access, surpassing the limitations of single queries. However, developing dialogue systems requires a large amount of training data, which is a challenge in low-resource domains and languages. Traditional data collection methods like crowd-sourcing are labor-intensive and time-consuming, making them ineffective in this context. Data augmentation (DA) is an affective approach to alleviate the data scarcity problem in conversational systems. This tutorial provides a comprehensive and up-to-date overview of DA approaches in the context of conversational systems. It highlights recent advances in conversation augmentation, open domain and task-oriented conversation generation, and different paradigms of evaluating these models. We also discuss current challenges and future directions in order to help researchers and practitioners to further advance the field in this area.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#32771;&#34385;&#20351;&#29992;LLM&#27169;&#22411;&#36890;&#36807;&#32454;&#35843;&#23454;&#29616;&#34394;&#20551;&#20449;&#24687;&#21644;&#20551;&#26032;&#38395;&#30340;&#28145;&#20837;&#20998;&#26512;&#65292;&#25581;&#31034;&#22797;&#26434;&#30340;&#39118;&#26684;&#21644;&#21465;&#20107;&#65292;&#24182;&#25552;&#21462;&#21629;&#21517;&#23454;&#20307;&#30340;&#24773;&#24863;&#65292;&#20197;&#27492;&#20316;&#20026;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#39044;&#27979;&#24615;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2309.04704</link><description>&lt;p&gt;
&#36890;&#36807;&#20248;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#34394;&#20551;&#20449;&#24687;&#21644;&#20551;&#26032;&#38395;&#30340;&#26816;&#27979;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Analysis of Disinformation and Fake News Detection Using Fine-Tuned Large Language Model. (arXiv:2309.04704v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04704
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32771;&#34385;&#20351;&#29992;LLM&#27169;&#22411;&#36890;&#36807;&#32454;&#35843;&#23454;&#29616;&#34394;&#20551;&#20449;&#24687;&#21644;&#20551;&#26032;&#38395;&#30340;&#28145;&#20837;&#20998;&#26512;&#65292;&#25581;&#31034;&#22797;&#26434;&#30340;&#39118;&#26684;&#21644;&#21465;&#20107;&#65292;&#24182;&#25552;&#21462;&#21629;&#21517;&#23454;&#20307;&#30340;&#24773;&#24863;&#65292;&#20197;&#27492;&#20316;&#20026;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#39044;&#27979;&#24615;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20351;&#29992;LLM&#65288;Llama 2&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65289;&#36890;&#36807;&#32454;&#35843;&#36827;&#34892;&#34394;&#20551;&#20449;&#24687;&#20998;&#26512;&#21644;&#20551;&#26032;&#38395;&#30340;&#26816;&#27979;&#12290;&#37319;&#29992;&#20102;&#22522;&#20110;PEFT/LoRA&#30340;&#32454;&#35843;&#26041;&#27861;&#12290;&#30740;&#31350;&#20013;&#65292;&#35813;&#27169;&#22411;&#23545;&#20197;&#19979;&#20219;&#21153;&#36827;&#34892;&#20102;&#32454;&#35843;&#65306;&#25581;&#31034;&#34394;&#20551;&#20449;&#24687;&#21644;&#23459;&#20256;&#21465;&#20107;&#30340;&#25991;&#26412;&#20998;&#26512;&#65292;&#20107;&#23454;&#26680;&#26597;&#65292;&#20551;&#26032;&#38395;&#26816;&#27979;&#65292;&#25805;&#32437;&#20998;&#26512;&#20197;&#21450;&#25552;&#21462;&#24102;&#26377;&#24773;&#24863;&#30340;&#21629;&#21517;&#23454;&#20307;&#12290;&#25152;&#24471;&#32467;&#26524;&#34920;&#26126;&#65292;&#32463;&#36807;&#32454;&#35843;&#30340;Llama 2&#27169;&#22411;&#33021;&#22815;&#23545;&#25991;&#26412;&#36827;&#34892;&#28145;&#20837;&#20998;&#26512;&#65292;&#24182;&#25581;&#31034;&#22797;&#26434;&#30340;&#39118;&#26684;&#21644;&#21465;&#20107;&#12290;&#24102;&#26377;&#24773;&#24863;&#30340;&#21629;&#21517;&#23454;&#20307;&#21487;&#20197;&#20316;&#20026;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#39044;&#27979;&#24615;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
The paper considers the possibility of fine-tuning Llama 2 large language model (LLM) for the disinformation analysis and fake news detection. For fine-tuning, the PEFT/LoRA based approach was used. In the study, the model was fine-tuned for the following tasks: analysing a text on revealing disinformation and propaganda narratives, fact checking, fake news detection, manipulation analytics, extracting named entities with their sentiments. The obtained results show that the fine-tuned Llama 2 model can perform a deep analysis of texts and reveal complex styles and narratives. Extracted sentiments for named entities can be considered as predictive features in supervised machine learning models.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#31934;&#32454;&#35843;&#25972;&#30340;Llama 2&#27169;&#22411;&#23454;&#29616;&#20102;&#37329;&#34701;&#26032;&#38395;&#30340;&#22810;&#20219;&#21153;&#20998;&#26512;&#65292;&#21253;&#25324;&#25991;&#26412;&#20998;&#26512;&#12289;&#25688;&#35201;&#21644;&#24773;&#24863;&#25552;&#21462;&#31561;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25552;&#21462;&#30340;&#21629;&#21517;&#23454;&#20307;&#24773;&#24863;&#21487;&#20197;&#20316;&#20026;&#26377;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2308.13032</link><description>&lt;p&gt;
&#20351;&#29992;&#31934;&#32454;&#35843;&#25972;&#30340;Llama 2 GPT&#27169;&#22411;&#36827;&#34892;&#37329;&#34701;&#26032;&#38395;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Financial News Analytics Using Fine-Tuned Llama 2 GPT Model. (arXiv:2308.13032v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13032
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#31934;&#32454;&#35843;&#25972;&#30340;Llama 2&#27169;&#22411;&#23454;&#29616;&#20102;&#37329;&#34701;&#26032;&#38395;&#30340;&#22810;&#20219;&#21153;&#20998;&#26512;&#65292;&#21253;&#25324;&#25991;&#26412;&#20998;&#26512;&#12289;&#25688;&#35201;&#21644;&#24773;&#24863;&#25552;&#21462;&#31561;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25552;&#21462;&#30340;&#21629;&#21517;&#23454;&#20307;&#24773;&#24863;&#21487;&#20197;&#20316;&#20026;&#26377;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#20351;&#29992;&#31934;&#32454;&#35843;&#25972;&#30340;Llama 2 Large Language Model (LLM) &#23545;&#37329;&#34701;&#26032;&#38395;&#36827;&#34892;&#22810;&#20219;&#21153;&#20998;&#26512;&#30340;&#21487;&#33021;&#24615;&#12290;&#36890;&#36807;PEFT/LoRA&#26041;&#27861;&#23545;&#27169;&#22411;&#36827;&#34892;&#31934;&#32454;&#35843;&#25972;&#65292;&#20027;&#35201;&#21253;&#25324;&#20174;&#37329;&#34701;&#24066;&#22330;&#35282;&#24230;&#20998;&#26512;&#25991;&#26412;&#12289;&#31361;&#20986;&#25991;&#26412;&#30340;&#20027;&#35201;&#35266;&#28857;&#12289;&#23545;&#25991;&#26412;&#36827;&#34892;&#25688;&#35201;&#21644;&#25552;&#21462;&#20855;&#26377;&#36866;&#24403;&#24773;&#24863;&#30340;&#21629;&#21517;&#23454;&#20307;&#31561;&#20219;&#21153;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#32463;&#36807;&#31934;&#32454;&#35843;&#25972;&#30340;Llama 2&#27169;&#22411;&#33021;&#22815;&#36827;&#34892;&#22810;&#20219;&#21153;&#30340;&#37329;&#34701;&#26032;&#38395;&#20998;&#26512;&#65292;&#20854;&#21709;&#24212;&#30340;&#32467;&#26500;&#21487;&#20197;&#37096;&#20998;&#20026;&#32467;&#26500;&#21270;&#25991;&#26412;&#65292;&#21478;&#19968;&#37096;&#20998;&#25968;&#25454;&#21487;&#20197;&#37319;&#29992;JSON&#26684;&#24335;&#36827;&#19968;&#27493;&#22788;&#29702;&#12290;&#25552;&#21462;&#30340;&#21629;&#21517;&#23454;&#20307;&#24773;&#24863;&#21487;&#20197;&#34987;&#35270;&#20026;&#20855;&#26377;&#23450;&#37327;&#30446;&#26631;&#21464;&#37327;&#30340;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
The paper considers the possibility to fine-tune Llama 2 Large Language Model (LLM) for the multitask analysis of financial news. For fine-tuning, the PEFT/LoRA based approach was used. In the study, the model was fine-tuned for the following tasks: analysing a text from financial market perspectives, highlighting main points of a text, summarizing a text and extracting named entities with appropriate sentiments. The obtained results show that the fine-tuned Llama 2 model can perform a multitask financial news analysis with a specified structure of response, part of response can be a structured text and another part of data can have JSON format for further processing. Extracted sentiments for named entities can be considered as predictive features in supervised machine learning models with quantitative target variables.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#29305;&#24449;&#20132;&#20114;&#23398;&#20064;&#27169;&#22411;EulerNet&#65292;&#23427;&#37319;&#29992;&#27431;&#25289;&#20844;&#24335;&#23558;&#39640;&#38454;&#29305;&#24449;&#20132;&#20114;&#26144;&#23556;&#21040;&#22797;&#26434;&#21521;&#37327;&#31354;&#38388;&#20013;&#23398;&#20064;&#65292;&#20174;&#32780;&#22312;&#20445;&#25345;&#25928;&#29575;&#30340;&#21516;&#26102;&#25552;&#39640;&#27169;&#22411;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.10711</link><description>&lt;p&gt;
EulerNet: &#22522;&#20110;&#27431;&#25289;&#20844;&#24335;&#30340;&#22797;&#26434;&#21521;&#37327;&#31354;&#38388;&#29305;&#24449;&#20132;&#20114;&#23398;&#20064;&#20197;&#23454;&#29616;&#28857;&#20987;&#29575;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
EulerNet: Adaptive Feature Interaction Learning via Euler's Formula for CTR Prediction. (arXiv:2304.10711v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10711
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#29305;&#24449;&#20132;&#20114;&#23398;&#20064;&#27169;&#22411;EulerNet&#65292;&#23427;&#37319;&#29992;&#27431;&#25289;&#20844;&#24335;&#23558;&#39640;&#38454;&#29305;&#24449;&#20132;&#20114;&#26144;&#23556;&#21040;&#22797;&#26434;&#21521;&#37327;&#31354;&#38388;&#20013;&#23398;&#20064;&#65292;&#20174;&#32780;&#22312;&#20445;&#25345;&#25928;&#29575;&#30340;&#21516;&#26102;&#25552;&#39640;&#27169;&#22411;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28857;&#20987;&#29575;&#39044;&#27979;&#20219;&#21153;&#20013;&#65292;&#23398;&#20064;&#39640;&#38454;&#29305;&#24449;&#20132;&#20114;&#26159;&#38750;&#24120;&#20851;&#38190;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#22312;&#32447;&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#20013;&#65292;&#30001;&#20110;&#28023;&#37327;&#29305;&#24449;&#30340;&#23384;&#22312;&#65292;&#35745;&#31639;&#39640;&#38454;&#29305;&#24449;&#20132;&#20114;&#38750;&#24120;&#32791;&#26102;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#25163;&#21160;&#35774;&#35745;&#26368;&#22823;&#38454;&#25968;&#65292;&#24182;&#20174;&#20013;&#36807;&#28388;&#20986;&#26080;&#29992;&#30340;&#20132;&#20114;&#12290;&#23613;&#31649;&#23427;&#20204;&#20943;&#23569;&#20102;&#39640;&#38454;&#29305;&#24449;&#32452;&#21512;&#30340;&#25351;&#25968;&#32423;&#22686;&#38271;&#25152;&#24341;&#36215;&#30340;&#39640;&#35745;&#31639;&#25104;&#26412;&#65292;&#20294;&#30001;&#20110;&#21463;&#21040;&#21463;&#38480;&#30340;&#29305;&#24449;&#38454;&#25968;&#30340;&#27425;&#20248;&#23398;&#20064;&#30340;&#24433;&#21709;&#65292;&#23427;&#20204;&#20173;&#28982;&#20250;&#21463;&#21040;&#27169;&#22411;&#33021;&#21147;&#19979;&#38477;&#30340;&#24433;&#21709;&#12290;&#20445;&#25345;&#27169;&#22411;&#33021;&#21147;&#24182;&#21516;&#26102;&#20445;&#25345;&#20854;&#25928;&#29575;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#19968;&#20010;&#25216;&#26415;&#25361;&#25112;&#65292;&#35813;&#38382;&#39064;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#35299;&#20915;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#36866;&#24212;&#29305;&#24449;&#20132;&#20114;&#23398;&#20064;&#27169;&#22411;&#65292;&#21517;&#20026;EulerNet&#65292;&#22312;&#35813;&#27169;&#22411;&#20013;&#65292;&#36890;&#36807;&#26681;&#25454;&#27431;&#25289;&#20844;&#24335;&#36827;&#34892;&#31354;&#38388;&#26144;&#23556;&#22312;&#22797;&#26434;&#21521;&#37327;&#31354;&#38388;&#20013;&#23398;&#20064;&#29305;&#24449;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning effective high-order feature interactions is very crucial in the CTR prediction task. However, it is very time-consuming to calculate high-order feature interactions with massive features in online e-commerce platforms. Most existing methods manually design a maximal order and further filter out the useless interactions from them. Although they reduce the high computational costs caused by the exponential growth of high-order feature combinations, they still suffer from the degradation of model capability due to the suboptimal learning of the restricted feature orders. The solution to maintain the model capability and meanwhile keep it efficient is a technical challenge, which has not been adequately addressed. To address this issue, we propose an adaptive feature interaction learning model, named as EulerNet, in which the feature interactions are learned in a complex vector space by conducting space mapping according to Euler's formula. EulerNet converts the exponential power
&lt;/p&gt;</description></item></channel></rss>