<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#21160;&#29983;&#25104;&#30340;&#30456;&#20851;&#24615;&#21028;&#26029;&#30340;&#26597;&#35810;&#24615;&#33021;&#39044;&#27979;&#26694;&#26550;&#65292;&#33021;&#22815;&#35299;&#20915;&#20808;&#21069;&#26041;&#27861;&#20013;&#23545;&#19981;&#21516;IR&#35780;&#20272;&#25351;&#26631;&#20934;&#30830;&#24615;&#21644;&#35299;&#37322;&#24615;&#30340;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2404.01012</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#30456;&#20851;&#24615;&#21028;&#26029;&#26469;&#39044;&#27979;&#26597;&#35810;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Query Performance Prediction using Relevance Judgments Generated by Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01012
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#21160;&#29983;&#25104;&#30340;&#30456;&#20851;&#24615;&#21028;&#26029;&#30340;&#26597;&#35810;&#24615;&#33021;&#39044;&#27979;&#26694;&#26550;&#65292;&#33021;&#22815;&#35299;&#20915;&#20808;&#21069;&#26041;&#27861;&#20013;&#23545;&#19981;&#21516;IR&#35780;&#20272;&#25351;&#26631;&#20934;&#30830;&#24615;&#21644;&#35299;&#37322;&#24615;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26597;&#35810;&#24615;&#33021;&#39044;&#27979;&#65288;QPP&#65289;&#26088;&#22312;&#20272;&#35745;&#25628;&#32034;&#31995;&#32479;&#23545;&#26597;&#35810;&#30340;&#26816;&#32034;&#36136;&#37327;&#65292;&#32780;&#26080;&#38656;&#20154;&#24037;&#30456;&#20851;&#24615;&#21028;&#26029;&#12290;&#20808;&#21069;&#30340;QPP&#26041;&#27861;&#36890;&#24120;&#36820;&#22238;&#21333;&#20010;&#26631;&#37327;&#20540;&#65292;&#24182;&#19981;&#35201;&#27714;&#39044;&#27979;&#20540;&#25509;&#36817;&#29305;&#23450;&#30340;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#35780;&#20272;&#25351;&#26631;&#65292;&#20174;&#32780;&#23548;&#33268;&#20197;&#19979;&#26576;&#20123;&#32570;&#28857;&#65306;&#65288;i&#65289;&#21333;&#20010;&#26631;&#37327;&#26080;&#27861;&#20934;&#30830;&#34920;&#31034;&#19981;&#21516;&#30340;IR&#35780;&#20272;&#25351;&#26631;&#65292;&#29305;&#21035;&#26159;&#24403;&#24230;&#37327;&#19981;&#39640;&#24230;&#30456;&#20851;&#26102;&#65292;&#65288;ii&#65289;&#21333;&#20010;&#26631;&#37327;&#38480;&#21046;&#20102;QPP&#26041;&#27861;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#22240;&#20026;&#20165;&#20351;&#29992;&#26631;&#37327;&#26080;&#27861;&#35299;&#37322;QPP&#32467;&#26524;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#33258;&#21160;&#29983;&#25104;&#30340;&#30456;&#20851;&#24615;&#21028;&#26029;&#30340;QPP&#26694;&#26550;&#65288;QPP-GenRE&#65289;&#65292;&#23558;QPP&#20998;&#35299;&#20026;&#29420;&#31435;&#30340;&#23376;&#20219;&#21153;&#65292;&#21363;&#23545;&#25490;&#21517;&#21015;&#34920;&#20013;&#27599;&#20010;&#39033;&#30446;&#23545;&#32473;&#23450;&#26597;&#35810;&#30340;&#30456;&#20851;&#24615;&#36827;&#34892;&#21028;&#26029;&#12290;&#36825;&#26679;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#29983;&#25104;&#30340;&#30456;&#20851;&#24615;&#21028;&#26029;&#26469;&#39044;&#27979;&#20219;&#20309;IR&#35780;&#20272;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01012v1 Announce Type: cross  Abstract: Query performance prediction (QPP) aims to estimate the retrieval quality of a search system for a query without human relevance judgments. Previous QPP methods typically return a single scalar value and do not require the predicted values to approximate a specific information retrieval (IR) evaluation measure, leading to certain drawbacks: (i) a single scalar is insufficient to accurately represent different IR evaluation measures, especially when metrics do not highly correlate, and (ii) a single scalar limits the interpretability of QPP methods because solely using a scalar is insufficient to explain QPP results. To address these issues, we propose a QPP framework using automatically generated relevance judgments (QPP-GenRE), which decomposes QPP into independent subtasks of judging the relevance of each item in a ranked list to a given query. This allows us to predict any IR evaluation measure using the generated relevance judgment
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#26102;&#24207;&#24207;&#21015;&#39044;&#27979;&#26694;&#26550;(RATSF)&#65292;&#36890;&#36807;&#24341;&#20837;&#20132;&#21449;&#27880;&#24847;&#21147;&#27169;&#22359;(RACA)&#21450;&#30693;&#35782;&#24211;&#35774;&#35745;&#65292;&#26377;&#25928;&#21033;&#29992;&#21382;&#21490;&#25968;&#25454;&#27573;&#36827;&#34892;&#23458;&#26381;&#37327;&#39044;&#27979;&#65292;&#22312;&#38750;&#24179;&#31283;&#25968;&#25454;&#24773;&#20917;&#19979;&#26174;&#33879;&#25552;&#21319;&#20102;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.04180</link><description>&lt;p&gt;
RATSF&#65306;&#36890;&#36807;&#26816;&#32034;&#22686;&#24378;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26469;&#36171;&#33021;&#23458;&#26381;&#37327;&#31649;&#29702;
&lt;/p&gt;
&lt;p&gt;
RATSF: Empowering Customer Service Volume Management through Retrieval-Augmented Time-Series Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04180
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#26102;&#24207;&#24207;&#21015;&#39044;&#27979;&#26694;&#26550;(RATSF)&#65292;&#36890;&#36807;&#24341;&#20837;&#20132;&#21449;&#27880;&#24847;&#21147;&#27169;&#22359;(RACA)&#21450;&#30693;&#35782;&#24211;&#35774;&#35745;&#65292;&#26377;&#25928;&#21033;&#29992;&#21382;&#21490;&#25968;&#25454;&#27573;&#36827;&#34892;&#23458;&#26381;&#37327;&#39044;&#27979;&#65292;&#22312;&#38750;&#24179;&#31283;&#25968;&#25454;&#24773;&#20917;&#19979;&#26174;&#33879;&#25552;&#21319;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20010;&#39640;&#25928;&#30340;&#23458;&#26381;&#31649;&#29702;&#31995;&#32479;&#21462;&#20915;&#20110;&#23545;&#26381;&#21153;&#37327;&#30340;&#31934;&#30830;&#39044;&#27979;&#12290;&#22312;&#36825;&#31181;&#25968;&#25454;&#38750;&#24179;&#31283;&#24615;&#26126;&#26174;&#30340;&#24773;&#20917;&#19979;&#65292;&#25104;&#21151;&#30340;&#39044;&#27979;&#20005;&#37325;&#20381;&#36182;&#20110;&#35782;&#21035;&#21644;&#21033;&#29992;&#31867;&#20284;&#30340;&#21382;&#21490;&#25968;&#25454;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#24635;&#32467;&#21608;&#26399;&#24615;&#27169;&#24335;&#12290;&#29616;&#26377;&#22522;&#20110;RNN&#25110;Transformer&#26550;&#26500;&#30340;&#27169;&#22411;&#36890;&#24120;&#22312;&#28789;&#27963;&#21644;&#26377;&#25928;&#21033;&#29992;&#26041;&#38754;&#23384;&#22312;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#19988;&#21487;&#36866;&#24212;&#30340;&#20132;&#21449;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#31216;&#20026;RACA&#65292;&#23427;&#22312;&#39044;&#27979;&#20219;&#21153;&#20013;&#26377;&#25928;&#21033;&#29992;&#20102;&#21382;&#21490;&#27573;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#31934;&#30830;&#30340;&#21382;&#21490;&#24207;&#21015;&#26597;&#35810;&#34920;&#31034;&#26041;&#26696;&#65292;&#32467;&#21512;&#20102;&#30693;&#35782;&#24211;&#30340;&#35774;&#35745;&#12290;&#36825;&#20123;&#20851;&#38190;&#32452;&#20214;&#20849;&#21516;&#26500;&#25104;&#20102;&#25105;&#20204;&#30340;&#26816;&#32034;&#22686;&#24378;&#26102;&#24207;&#24207;&#21015;&#39044;&#27979;&#26694;&#26550;&#65288;RATSF&#65289;&#12290;RATSF&#19981;&#20165;&#22312;&#33778;&#40481;&#37202;&#24215;&#26381;&#21153;&#37327;&#39044;&#27979;&#29615;&#22659;&#20013;&#26174;&#33879;&#22686;&#24378;&#20102;&#24615;&#33021;&#65292;&#32780;&#19988;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04180v1 Announce Type: new  Abstract: An efficient customer service management system hinges on precise forecasting of service volume. In this scenario, where data non-stationarity is pronounced, successful forecasting heavily relies on identifying and leveraging similar historical data rather than merely summarizing periodic patterns. Existing models based on RNN or Transformer architectures often struggle with this flexible and effective utilization. To address this challenge, we propose an efficient and adaptable cross-attention module termed RACA, which effectively leverages historical segments in forecasting task, and we devised a precise representation scheme for querying historical sequences, coupled with the design of a knowledge repository. These critical components collectively form our Retrieval-Augmented Temporal Sequence Forecasting framework (RATSF). RATSF not only significantly enhances performance in the context of Fliggy hotel service volume forecasting but,
&lt;/p&gt;</description></item><item><title>InteraRec&#24341;&#20837;&#20102;&#19968;&#31181;&#22797;&#26434;&#30340;&#20132;&#20114;&#24335;&#25512;&#33616;&#26694;&#26550;&#65292;&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#65292;&#23427;&#19981;&#20165;&#20381;&#36182;Weblog&#29983;&#25104;&#25512;&#33616;&#65292;&#36824;&#25429;&#33719;&#29992;&#25143;&#23548;&#33322;&#26102;&#32593;&#39029;&#30340;&#39640;&#39057;&#25130;&#22270;&#12290;</title><link>https://arxiv.org/abs/2403.00822</link><description>&lt;p&gt;
InteraRec&#65306;&#20351;&#29992;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20132;&#20114;&#24335;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
InteraRec: Interactive Recommendations Using Multimodal Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00822
&lt;/p&gt;
&lt;p&gt;
InteraRec&#24341;&#20837;&#20102;&#19968;&#31181;&#22797;&#26434;&#30340;&#20132;&#20114;&#24335;&#25512;&#33616;&#26694;&#26550;&#65292;&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#65292;&#23427;&#19981;&#20165;&#20381;&#36182;Weblog&#29983;&#25104;&#25512;&#33616;&#65292;&#36824;&#25429;&#33719;&#29992;&#25143;&#23548;&#33322;&#26102;&#32593;&#39029;&#30340;&#39640;&#39057;&#25130;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Weblog&#30001;&#35760;&#24405;&#20219;&#20309;&#32593;&#31449;&#19978;&#29992;&#25143;&#27963;&#21160;&#30340;&#35760;&#24405;&#32452;&#25104;&#65292;&#21487;&#20197;&#20026;&#29992;&#25143;&#20559;&#22909;&#12289;&#34892;&#20026;&#21644;&#20852;&#36259;&#25552;&#20379;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;&#35768;&#22810;&#25512;&#33616;&#31639;&#27861;&#21033;&#29992;&#36890;&#36807;&#36825;&#20123;Weblog&#25366;&#25496;&#30340;&#25968;&#25454;&#65292;&#37319;&#29992;&#21327;&#21516;&#36807;&#28388;&#12289;&#22522;&#20110;&#20869;&#23481;&#30340;&#36807;&#28388;&#21644;&#28151;&#21512;&#26041;&#27861;&#31561;&#31574;&#30053;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#20010;&#24615;&#21270;&#25512;&#33616;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;InteraRec&#30340;&#22797;&#26434;&#20132;&#20114;&#24335;&#25512;&#33616;&#26694;&#26550;&#65292;&#23427;&#19981;&#21516;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#21518;&#32773;&#20165;&#20381;&#36182;Weblog&#29983;&#25104;&#25512;&#33616;&#12290;&#35813;&#26694;&#26550;&#25429;&#33719;&#29992;&#25143;&#23548;&#33322;&#26102;&#32593;&#39029;&#30340;&#39640;&#39057;&#25130;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00822v1 Announce Type: cross  Abstract: Weblogs, comprised of records detailing user activities on any website, offer valuable insights into user preferences, behavior, and interests. Numerous recommendation algorithms, employing strategies such as collaborative filtering, content-based filtering, and hybrid methods, leverage the data mined through these weblogs to provide personalized recommendations to users. Despite the abundance of information available in these weblogs, identifying and extracting pertinent information and key features necessitates extensive engineering endeavors. The intricate nature of the data also poses a challenge for interpretation, especially for non-experts. In this study, we introduce a sophisticated and interactive recommendation framework denoted as InteraRec, which diverges from conventional approaches that exclusively depend on weblogs for recommendation generation. This framework captures high-frequency screenshots of web pages as users nav
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#26041;&#27861;&#30340;&#21333;&#20010;&#32534;&#30721;-&#35299;&#30721;&#27169;&#22411;&#26469;&#35299;&#20915;&#36328;&#35821;&#35328;&#38382;&#31572;&#38382;&#39064;&#65292;&#36890;&#36807;&#21033;&#29992;&#32500;&#22522;&#30334;&#31185;&#20869;&#30340;&#36328;&#35821;&#35328;&#38142;&#25509;&#32467;&#26500;&#21512;&#25104;&#30417;&#30563;&#20449;&#21495;&#65292;&#21462;&#24471;&#20102;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.16508</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#35268;&#27169;&#21512;&#25104;&#30417;&#30563;&#36827;&#34892;&#36328;&#35821;&#35328;&#24320;&#25918;&#22495;&#38382;&#31572;&#30340;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Pre-training Cross-lingual Open Domain Question Answering with Large-scale Synthetic Supervision
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16508
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#26041;&#27861;&#30340;&#21333;&#20010;&#32534;&#30721;-&#35299;&#30721;&#27169;&#22411;&#26469;&#35299;&#20915;&#36328;&#35821;&#35328;&#38382;&#31572;&#38382;&#39064;&#65292;&#36890;&#36807;&#21033;&#29992;&#32500;&#22522;&#30334;&#31185;&#20869;&#30340;&#36328;&#35821;&#35328;&#38142;&#25509;&#32467;&#26500;&#21512;&#25104;&#30417;&#30563;&#20449;&#21495;&#65292;&#21462;&#24471;&#20102;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#35821;&#35328;&#38382;&#31572;&#65288;CLQA&#65289;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#38382;&#39064;&#65292;&#21253;&#25324;&#20174;&#22810;&#35821;&#35328;&#30693;&#35782;&#24211;&#36827;&#34892;&#36328;&#35821;&#35328;&#26816;&#32034;&#65292;&#28982;&#21518;&#22312;&#33521;&#35821;&#25110;&#26597;&#35810;&#35821;&#35328;&#20013;&#29983;&#25104;&#31572;&#26696;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#21487;&#20197;&#20351;&#29992;&#21333;&#20010;&#32534;&#30721;-&#35299;&#30721;&#27169;&#22411;&#26469;&#35299;&#20915;CLQA&#12290;&#20026;&#20102;&#26377;&#25928;&#35757;&#32451;&#36825;&#20010;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21033;&#29992;&#32500;&#22522;&#30334;&#31185;&#20869;&#36328;&#35821;&#35328;&#38142;&#25509;&#32467;&#26500;&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#38142;&#25509;&#30340;&#32500;&#22522;&#30334;&#31185;&#39029;&#38754;&#26469;&#21512;&#25104;&#36328;&#35821;&#35328;&#26816;&#32034;&#30340;&#30417;&#30563;&#20449;&#21495;&#65292;&#36890;&#36807;&#19968;&#31181;&#22635;&#31354;&#26597;&#35810;&#24418;&#24335;&#29983;&#25104;&#26356;&#33258;&#28982;&#30340;&#26597;&#35810;&#20197;&#30417;&#30563;&#31572;&#26696;&#29983;&#25104;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;CLASS&#22312;&#30417;&#30563;&#23398;&#20064;&#21644;&#38646;-shot&#24773;&#20917;&#19979;&#22343;&#20248;&#20110;&#21487;&#27604;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16508v1 Announce Type: new  Abstract: Cross-lingual question answering (CLQA) is a complex problem, comprising cross-lingual retrieval from a multilingual knowledge base, followed by answer generation either in English or the query language. Both steps are usually tackled by separate models, requiring substantial annotated datasets, and typically auxiliary resources, like machine translation systems to bridge between languages. In this paper, we show that CLQA can be addressed using a single encoder-decoder model. To effectively train this model, we propose a self-supervised method based on exploiting the cross-lingual link structure within Wikipedia. We demonstrate how linked Wikipedia pages can be used to synthesise supervisory signals for cross-lingual retrieval, through a form of cloze query, and generate more natural queries to supervise answer generation. Together, we show our approach, \texttt{CLASS}, outperforms comparable methods on both supervised and zero-shot lan
&lt;/p&gt;</description></item><item><title>&#22312;&#26631;&#20934;&#25910;&#38598;&#30340;&#21021;&#27493;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#38271;&#25991;&#26723;&#27169;&#22411;&#22312;MRR&#25110;NDCG&#26041;&#38754;&#24615;&#33021;&#19981;&#20339;&#65292;&#34920;&#29616;&#20302;&#20110;FirstP&#65292;&#25110;&#24179;&#22343;&#26368;&#22810;&#36229;&#36234;5&#65285;&#12290;&#25105;&#20204;&#25512;&#27979;&#36825;&#19981;&#26159;&#22240;&#20026;&#27169;&#22411;&#26080;&#27861;&#22788;&#29702;&#38271;&#19978;&#19979;&#25991;&#65292;&#32780;&#26159;&#30001;&#20110;&#30456;&#20851;&#27573;&#33853;&#20855;&#26377;&#20301;&#32622;&#20559;&#35265;&#65292;&#24448;&#24448;&#20301;&#20110;&#21069;512&#20010;&#25991;&#26723;&#26631;&#35760;&#20043;&#20013;&#12290;&#25105;&#20204;&#25214;&#21040;&#35777;&#25454;&#34920;&#26126;&#36825;&#31181;&#20559;&#35265;&#33267;&#23569;&#23384;&#22312;&#20110;&#20004;&#20010;&#27979;&#35797;&#38598;&#20013;&#65292;&#36825;&#20419;&#20351;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#25910;&#38598;MS MARCO FarRelevant&#65292;&#20854;&#20013;&#21253;&#21547;</title><link>https://arxiv.org/abs/2207.01262</link><description>&lt;p&gt;
&#36890;&#36807;&#20840;&#38754;&#35780;&#20272;&#21644;Leaderboarding&#29702;&#35299;&#38271;&#25991;&#26723;&#25490;&#21517;&#27169;&#22411;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Understanding Performance of Long-Document Ranking Models through Comprehensive Evaluation and Leaderboarding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2207.01262
&lt;/p&gt;
&lt;p&gt;
&#22312;&#26631;&#20934;&#25910;&#38598;&#30340;&#21021;&#27493;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#38271;&#25991;&#26723;&#27169;&#22411;&#22312;MRR&#25110;NDCG&#26041;&#38754;&#24615;&#33021;&#19981;&#20339;&#65292;&#34920;&#29616;&#20302;&#20110;FirstP&#65292;&#25110;&#24179;&#22343;&#26368;&#22810;&#36229;&#36234;5&#65285;&#12290;&#25105;&#20204;&#25512;&#27979;&#36825;&#19981;&#26159;&#22240;&#20026;&#27169;&#22411;&#26080;&#27861;&#22788;&#29702;&#38271;&#19978;&#19979;&#25991;&#65292;&#32780;&#26159;&#30001;&#20110;&#30456;&#20851;&#27573;&#33853;&#20855;&#26377;&#20301;&#32622;&#20559;&#35265;&#65292;&#24448;&#24448;&#20301;&#20110;&#21069;512&#20010;&#25991;&#26723;&#26631;&#35760;&#20043;&#20013;&#12290;&#25105;&#20204;&#25214;&#21040;&#35777;&#25454;&#34920;&#26126;&#36825;&#31181;&#20559;&#35265;&#33267;&#23569;&#23384;&#22312;&#20110;&#20004;&#20010;&#27979;&#35797;&#38598;&#20013;&#65292;&#36825;&#20419;&#20351;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#25910;&#38598;MS MARCO FarRelevant&#65292;&#20854;&#20013;&#21253;&#21547;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35780;&#20272;&#20102;20&#22810;&#20010;&#29992;&#20110;&#38271;&#25991;&#26723;&#25490;&#21517;&#30340;Transformer&#27169;&#22411;&#65288;&#21253;&#25324;&#26368;&#36817;&#20351;&#29992;FlashAttention&#35757;&#32451;&#30340;LongP&#27169;&#22411;&#65289;&#65292;&#24182;&#23558;&#23427;&#20204;&#19982;&#31616;&#21333;&#30340;FirstP&#22522;&#32447;&#36827;&#34892;&#20102;&#27604;&#36739;&#65288;&#23558;&#30456;&#21516;&#27169;&#22411;&#24212;&#29992;&#20110;&#36755;&#20837;&#25130;&#26029;&#20026;&#21069;512&#20010;&#26631;&#35760;&#65289;&#12290;&#25105;&#20204;&#20351;&#29992;MS MARCO&#25991;&#26723;v1&#20316;&#20026;&#20027;&#35201;&#35757;&#32451;&#38598;&#65292;&#24182;&#22312;&#38646;-shot&#22330;&#26223;&#19979;&#35780;&#20272;&#20102;&#27169;&#22411;&#65292;&#20197;&#21450;&#22312;&#23545;&#20854;&#20182;&#25910;&#38598;&#36827;&#34892;&#24494;&#35843;&#21518;&#35780;&#20272;&#20102;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2207.01262v2 Announce Type: replace-cross  Abstract: We evaluated 20+ Transformer models for ranking of long documents (including recent LongP models trained with FlashAttention) and compared them with simple FirstP baselines (applying the same model to input truncated to the first 512 tokens). We used MS MARCO Documents v1 as a primary training set and evaluated models in the zero-shot scenario as well as after fine-tuning on other collections.   In our initial experiments with standard collections we found that long-document models underperformed FirstP or outperformed it by at most 5% on average in terms of MRR or NDCG. We then conjectured that this was not due to models inability to process long context but rather due to a positional bias of relevant passages, which tended to be among the first 512 document tokens. We found evidence that this bias was, indeed, present in at least two test sets, which motivated us to create a new collection MS MARCO FarRelevant where the relev
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21435;&#22122;&#25193;&#25955;&#25512;&#33616;&#27169;&#22411;&#65288;DDRM&#65289;&#65292;&#36890;&#36807;&#22312;&#25512;&#33616;&#27169;&#22411;&#20013;&#27880;&#20837;&#22122;&#22768;&#24182;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#22810;&#27493;&#21435;&#22122;&#36807;&#31243;&#65292;&#22686;&#24378;&#29992;&#25143;&#21644;&#39033;&#30446;&#23884;&#20837;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.06982</link><description>&lt;p&gt;
&#21435;&#22122;&#25193;&#25955;&#25512;&#33616;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Denoising Diffusion Recommender Model. (arXiv:2401.06982v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06982
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21435;&#22122;&#25193;&#25955;&#25512;&#33616;&#27169;&#22411;&#65288;DDRM&#65289;&#65292;&#36890;&#36807;&#22312;&#25512;&#33616;&#27169;&#22411;&#20013;&#27880;&#20837;&#22122;&#22768;&#24182;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#22810;&#27493;&#21435;&#22122;&#36807;&#31243;&#65292;&#22686;&#24378;&#29992;&#25143;&#21644;&#39033;&#30446;&#23884;&#20837;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#36890;&#24120;&#38754;&#20020;&#30528;&#20855;&#26377;&#22122;&#22768;&#30340;&#38544;&#24335;&#21453;&#39304;&#38382;&#39064;&#12290;&#22823;&#22810;&#25968;&#30740;&#31350;&#20174;&#25968;&#25454;&#28165;&#27927;&#30340;&#35282;&#24230;&#32531;&#35299;&#22122;&#22768;&#38382;&#39064;&#65292;&#22914;&#25968;&#25454;&#37325;&#26032;&#37319;&#26679;&#21644;&#37325;&#26032;&#21152;&#26435;&#65292;&#20294;&#23427;&#20204;&#21463;&#21040;&#21551;&#21457;&#24335;&#20551;&#35774;&#30340;&#38480;&#21046;&#12290;&#21478;&#19968;&#31181;&#21435;&#22122;&#26041;&#27861;&#26159;&#20174;&#27169;&#22411;&#30340;&#35282;&#24230;&#65292;&#31215;&#26497;&#22320;&#21521;&#29992;&#25143;-&#39033;&#30446;&#20132;&#20114;&#20013;&#27880;&#20837;&#22122;&#22768;&#65292;&#24182;&#22686;&#24378;&#27169;&#22411;&#30340;&#20869;&#22312;&#21435;&#22122;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#21435;&#22122;&#36807;&#31243;&#23545;&#25512;&#33616;&#27169;&#22411;&#30340;&#34920;&#31034;&#33021;&#21147;&#26469;&#25429;&#25417;&#22122;&#22768;&#27169;&#24335;&#25552;&#20986;&#20102;&#26174;&#33879;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21435;&#22122;&#25193;&#25955;&#25512;&#33616;&#27169;&#22411;&#65288;DDRM&#65289;&#65292;&#23427;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#20351;&#29992;&#22810;&#27493;&#21435;&#22122;&#36807;&#31243;&#26469;&#22686;&#24378;&#26469;&#33258;&#20219;&#20309;&#25512;&#33616;&#27169;&#22411;&#30340;&#29992;&#25143;&#21644;&#39033;&#30446;&#23884;&#20837;&#30340;&#40065;&#26834;&#24615;&#12290;DDRM&#22312;&#21069;&#21521;&#36807;&#31243;&#20013;&#27880;&#20837;&#21463;&#25511;&#39640;&#26031;&#22122;&#22768;&#65292;&#24182;&#22312;&#21453;&#21521;&#21435;&#22122;&#36807;&#31243;&#20013;&#36845;&#20195;&#22320;&#21435;&#38500;&#22122;&#22768;&#65292;&#20174;&#32780;&#25552;&#39640;&#23545;&#22122;&#22768;&#21453;&#39304;&#30340;&#23884;&#20837;&#40065;&#26834;&#24615;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#20851;&#38190;&#22312;&#20110;&#25552;&#20379;&#19968;&#31181;&#33021;&#22815;&#25429;&#25417;&#22122;&#22768;&#27169;&#24335;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#22810;&#27493;&#21435;&#22122;&#36807;&#31243;&#26469;&#25913;&#21892;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommender systems often grapple with noisy implicit feedback. Most studies alleviate the noise issues from data cleaning perspective such as data resampling and reweighting, but they are constrained by heuristic assumptions. Another denoising avenue is from model perspective, which proactively injects noises into user-item interactions and enhance the intrinsic denoising ability of models. However, this kind of denoising process poses significant challenges to the recommender model's representation capacity to capture noise patterns. To address this issue, we propose Denoising Diffusion Recommender Model (DDRM), which leverages multi-step denoising process based on diffusion models to robustify user and item embeddings from any recommender models. DDRM injects controlled Gaussian noises in the forward process and iteratively removes noises in the reverse denoising process, thereby improving embedding robustness against noisy feedback. To achieve this target, the key lies in offering 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;MARVEL&#30340;&#22810;&#27169;&#24577;&#26816;&#32034;&#27169;&#22411;&#65292;&#36890;&#36807;&#35270;&#35273;&#27169;&#22359;&#25554;&#20214;&#20026;&#23494;&#38598;&#26816;&#32034;&#22120;&#28155;&#21152;&#22270;&#20687;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#19988;&#22312;&#22810;&#27169;&#24577;&#26816;&#32034;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#20248;&#20110;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.14037</link><description>&lt;p&gt;
&#36890;&#36807;&#35270;&#35273;&#27169;&#22359;&#25554;&#20214;&#35299;&#38145;&#23494;&#38598;&#26816;&#32034;&#30340;&#22810;&#27169;&#24577;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Unlock Multi-Modal Capability of Dense Retrieval via Visual Module Plugin. (arXiv:2310.14037v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14037
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;MARVEL&#30340;&#22810;&#27169;&#24577;&#26816;&#32034;&#27169;&#22411;&#65292;&#36890;&#36807;&#35270;&#35273;&#27169;&#22359;&#25554;&#20214;&#20026;&#23494;&#38598;&#26816;&#32034;&#22120;&#28155;&#21152;&#22270;&#20687;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#19988;&#22312;&#22810;&#27169;&#24577;&#26816;&#32034;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#20248;&#20110;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#35270;&#35273;&#27169;&#22359;&#25554;&#20214;&#65288;MARVEL&#65289;&#23398;&#20064;&#26597;&#35810;&#21644;&#22810;&#27169;&#24577;&#25991;&#26723;&#30340;&#23884;&#20837;&#31354;&#38388;&#20197;&#36827;&#34892;&#26816;&#32034;&#30340;&#22810;&#27169;&#24577;&#26816;&#32034;&#27169;&#22411;&#12290;MARVEL&#20351;&#29992;&#32479;&#19968;&#30340;&#32534;&#30721;&#22120;&#27169;&#22411;&#23545;&#26597;&#35810;&#21644;&#22810;&#27169;&#24577;&#25991;&#26723;&#36827;&#34892;&#32534;&#30721;&#65292;&#26377;&#21161;&#20110;&#20943;&#23567;&#22270;&#20687;&#21644;&#25991;&#26412;&#20043;&#38388;&#30340;&#27169;&#24577;&#24046;&#36317;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#35270;&#35273;&#27169;&#22359;&#32534;&#30721;&#30340;&#22270;&#20687;&#29305;&#24449;&#20316;&#20026;&#20854;&#36755;&#20837;&#65292;&#20351;&#24471;&#32463;&#36807;&#35757;&#32451;&#30340;&#23494;&#38598;&#26816;&#32034;&#22120;T5-ANCE&#20855;&#26377;&#22270;&#20687;&#29702;&#35299;&#33021;&#21147;&#12290;&#20026;&#20102;&#20419;&#36827;&#22810;&#27169;&#24577;&#26816;&#32034;&#20219;&#21153;&#65292;&#25105;&#20204;&#22522;&#20110;ClueWeb22&#25968;&#25454;&#38598;&#26500;&#24314;&#20102;ClueWeb22-MM&#25968;&#25454;&#38598;&#65292;&#23558;&#38170;&#25991;&#26412;&#20316;&#20026;&#26597;&#35810;&#65292;&#24182;&#20174;&#38170;&#38142;&#25509;&#30340;&#32593;&#39029;&#20013;&#25552;&#21462;&#30456;&#20851;&#25991;&#26412;&#21644;&#22270;&#20687;&#25991;&#26723;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;MARVEL&#22312;&#22810;&#27169;&#24577;&#26816;&#32034;&#25968;&#25454;&#38598;WebQA&#21644;ClueWeb22-MM&#19978;&#26126;&#26174;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#35270;&#35273;&#27169;&#22359;&#25554;&#20214;&#26041;&#27861;&#20026;&#23454;&#29616;&#22270;&#20687;&#29702;&#35299;&#33021;&#21147;&#37327;&#36523;&#23450;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes Multi-modAl Retrieval model via Visual modulE pLugin (MARVEL) to learn an embedding space for queries and multi-modal documents to conduct retrieval. MARVEL encodes queries and multi-modal documents with a unified encoder model, which helps to alleviate the modality gap between images and texts. Specifically, we enable the image understanding ability of a well-trained dense retriever, T5-ANCE, by incorporating the image features encoded by the visual module as its inputs. To facilitate the multi-modal retrieval tasks, we build the ClueWeb22-MM dataset based on the ClueWeb22 dataset, which regards anchor texts as queries, and exact the related texts and image documents from anchor linked web pages. Our experiments show that MARVEL significantly outperforms the state-of-the-art methods on the multi-modal retrieval dataset WebQA and ClueWeb22-MM. Our further analyses show that the visual module plugin method is tailored to enable the image understanding ability for an 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36719;&#25552;&#31034;&#35843;&#20248;&#26469;&#22686;&#24378;&#23494;&#38598;&#26816;&#32034;&#30340;&#26041;&#27861;&#65288;SPTAR&#65289;&#12290;&#36890;&#36807;&#20248;&#21270;&#20219;&#21153;&#29305;&#23450;&#30340;&#36719;&#25552;&#31034;&#24182;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;&#26410;&#26631;&#35760;&#30340;&#25991;&#26723;&#29983;&#25104;&#24369;&#26597;&#35810;&#65292;&#21487;&#20197;&#25552;&#39640;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#30340;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.08303</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#23494;&#38598;&#26816;&#32034;&#30340;&#36719;&#25552;&#31034;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
Soft Prompt Tuning for Augmenting Dense Retrieval with Large Language Models. (arXiv:2307.08303v1 [cs.IR] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08303
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36719;&#25552;&#31034;&#35843;&#20248;&#26469;&#22686;&#24378;&#23494;&#38598;&#26816;&#32034;&#30340;&#26041;&#27861;&#65288;SPTAR&#65289;&#12290;&#36890;&#36807;&#20248;&#21270;&#20219;&#21153;&#29305;&#23450;&#30340;&#36719;&#25552;&#31034;&#24182;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;&#26410;&#26631;&#35760;&#30340;&#25991;&#26723;&#29983;&#25104;&#24369;&#26597;&#35810;&#65292;&#21487;&#20197;&#25552;&#39640;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#30340;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23494;&#38598;&#26816;&#32034;&#65288;DR&#65289;&#23558;&#26597;&#35810;&#21644;&#25991;&#26723;&#36716;&#21270;&#20026;&#23494;&#38598;&#21521;&#37327;&#34920;&#31034;&#65292;&#24182;&#22312;&#21521;&#37327;&#31354;&#38388;&#20013;&#27979;&#37327;&#26597;&#35810;&#19982;&#25991;&#26723;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;DR&#30340;&#19968;&#20010;&#25361;&#25112;&#26159;&#32570;&#20047;&#39046;&#22495;&#29305;&#23450;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#34429;&#28982;DR&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#20174;&#22823;&#35268;&#27169;&#20844;&#20849;&#25968;&#25454;&#38598;&#65288;&#22914;MS MARCO&#65289;&#20013;&#23398;&#20064;&#65292;&#20294;&#35777;&#25454;&#34920;&#26126;&#65292;&#24182;&#38750;&#25152;&#26377;DR&#27169;&#22411;&#21644;&#39046;&#22495;&#37117;&#33021;&#21516;&#31561;&#21463;&#30410;&#20110;&#36801;&#31227;&#23398;&#20064;&#12290;&#26368;&#36817;&#65292;&#19968;&#20123;&#30740;&#31350;&#20154;&#21592;&#36716;&#21521;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#25913;&#36827;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#30340;DR&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20013;&#37319;&#29992;&#30340;&#30828;&#25552;&#31034;&#25110;&#20154;&#24037;&#32534;&#20889;&#30340;&#25552;&#31034;&#26080;&#27861;&#20445;&#35777;&#29983;&#25104;&#30340;&#24369;&#26597;&#35810;&#30340;&#36136;&#37327;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#22686;&#24378;DR&#30340;&#36719;&#25552;&#31034;&#35843;&#20248;&#65288;SPTAR&#65289;&#65306;&#23545;&#20110;&#27599;&#20010;&#20219;&#21153;&#65292;&#25105;&#20204;&#21033;&#29992;&#36719;&#25552;&#31034;&#35843;&#20248;&#22312;&#26377;&#38480;&#30340;&#30495;&#23454;&#25968;&#25454;&#19978;&#20248;&#21270;&#20219;&#21153;&#29305;&#23450;&#30340;&#36719;&#25552;&#31034;&#65292;&#28982;&#21518;&#29992;&#36825;&#20123;&#25552;&#31034;&#24341;&#23548;LLMs&#20026;&#26410;&#26631;&#35760;&#30340;&#25991;&#26723;&#26631;&#35760;&#24369;&#26597;&#35810;&#65292;&#20174;&#32780;&#24471;&#21040;&#36275;&#22815;&#30340;&#24369;&#25991;&#26723;-&#26597;&#35810;&#23545;&#26469;&#35757;&#32451;&#20219;&#21153;&#29305;&#23450;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dense retrieval (DR) converts queries and documents into dense embeddings and measures the similarity between queries and documents in vector space. One of the challenges in DR is the lack of domain-specific training data. While DR models can learn from large-scale public datasets like MS MARCO through transfer learning, evidence shows that not all DR models and domains can benefit from transfer learning equally. Recently, some researchers have resorted to large language models (LLMs) to improve the zero-shot and few-shot DR models. However, the hard prompts or human-written prompts utilized in these works cannot guarantee the good quality of generated weak queries. To tackle this, we propose soft prompt tuning for augmenting DR (SPTAR): For each task, we leverage soft prompt-tuning to optimize a task-specific soft prompt on limited ground truth data and then prompt the LLMs to tag unlabeled documents with weak queries, yielding enough weak document-query pairs to train task-specific d
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21457;&#29616;&#23478;&#24237;&#25104;&#21592;&#28385;&#24847;&#24230;&#19982;&#23478;&#26063;&#26641;&#22270;&#65292;&#23478;&#24237;&#35268;&#27169;&#65292;&#23401;&#23376;&#26159;&#21542;&#20026;&#21516;&#29238;&#27597;&#65292;&#20197;&#21450;&#23478;&#24237;&#25910;&#20837;&#26377;&#20851;&#12290;</title><link>http://arxiv.org/abs/2305.01552</link><description>&lt;p&gt;
&#23478;&#26063;&#26641;&#22270;&#19982;&#23478;&#24237;&#25104;&#21592;&#28385;&#24847;&#24230;&#20043;&#38388;&#30340;&#20851;&#31995;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
The Family Tree Graph as a Predictor of the Family Members' Satisfaction with One Another. (arXiv:2305.01552v1 [physics.soc-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01552
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21457;&#29616;&#23478;&#24237;&#25104;&#21592;&#28385;&#24847;&#24230;&#19982;&#23478;&#26063;&#26641;&#22270;&#65292;&#23478;&#24237;&#35268;&#27169;&#65292;&#23401;&#23376;&#26159;&#21542;&#20026;&#21516;&#29238;&#27597;&#65292;&#20197;&#21450;&#23478;&#24237;&#25910;&#20837;&#26377;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#20154;&#23545;&#26680;&#24515;&#23478;&#24237;&#21644;&#25193;&#23637;&#23478;&#24237;&#30340;&#28385;&#24847;&#24230;&#22312;&#19968;&#20010;&#20154;&#30340;&#26085;&#24120;&#29983;&#27963;&#20013;&#25198;&#28436;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#35282;&#33394;&#12290;&#22240;&#27492;&#65292;&#26356;&#22909;&#22320;&#20102;&#35299;&#20915;&#23450;&#19968;&#20010;&#20154;&#23545;&#23478;&#24237;&#28385;&#24847;&#24230;&#30340;&#29305;&#24449;&#21487;&#20197;&#20026;&#26356;&#22909;&#30340;&#31038;&#20250;&#25919;&#31574;&#35774;&#35745;&#25171;&#24320;&#22823;&#38376;&#12290;&#20026;&#27492;&#65292;&#35813;&#30740;&#31350;&#32771;&#23519;&#20102;&#23478;&#26063;&#26641;&#22270;&#19982;&#23478;&#24237;&#25104;&#21592;&#23545;&#26680;&#24515;&#23478;&#24237;&#21644;&#25193;&#23637;&#23478;&#24237;&#30340;&#28385;&#24847;&#24230;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;&#26469;&#33258;486&#20010;&#23478;&#24237;&#30340;&#25968;&#25454;&#65292;&#20854;&#20013;&#21253;&#25324;&#23478;&#26063;&#26641;&#22270;&#21644;&#23478;&#24237;&#25104;&#21592;&#20043;&#38388;&#30340;&#28385;&#24847;&#24230;&#12290;&#25105;&#20204;&#24471;&#21040;&#20102;&#19968;&#20010;&#33021;&#22815;&#35299;&#37322;75%&#23478;&#24237;&#25104;&#21592;&#23545;&#24444;&#27492;&#28385;&#24847;&#24230;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#21457;&#29616;&#20102;&#35753;&#23478;&#24237;&#25104;&#21592;&#26356;&#21152;&#28385;&#24847;&#30340;&#19977;&#20010;&#25351;&#26631;&#12290;&#39318;&#20808;&#65292;&#24179;&#22343;&#32780;&#35328;&#65292;&#36739;&#22823;&#30340;&#23478;&#24237;&#26377;&#26356;&#28385;&#24847;&#30340;&#25104;&#21592;&#12290;&#27492;&#22806;&#65292;&#27809;&#26377;&#32487;&#20804;&#24351;&#22992;&#22969;&#30340;&#21516;&#29238;&#27597;&#23478;&#24237;-&#21363;&#25104;&#24180;&#23376;&#22899;&#24050;&#32463;&#25104;&#23478;&#30340;&#23478;&#24237;&#65292;&#22312;&#20804;&#24351;&#22992;&#22969;&#21644;&#29238;&#27597;&#26041;&#38754;&#20063;&#26356;&#28385;&#24847;&#12290;&#26368;&#21518;&#65292;&#23478;&#24237;&#25104;&#21592;&#30340;&#24179;&#22343;&#28385;&#24847;&#24230;&#21644;&#23478;&#24237;&#30340;&#25910;&#20837;&#27700;&#24179;&#24687;&#24687;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
Individuals' satisfaction with their nuclear and extended family plays a critical role in individuals everyday life. Thus, a better understanding of the features that determine one's satisfaction with her family can open the door to the design of better sociological policies. To this end, this study examines the relationship between the family tree graph and family members' satisfaction with their nuclear and extended family. We collected data from 486 families which included a family tree graph and family members' satisfaction with each other. We obtain a model that is able to explain 75\% of the family members' satisfaction with one another. We found three indicators for more satisfied families. First, larger families, on average, have more satisfied members. Moreover, families with kids from the same parents - i.e., without step-siblings also express more satisfaction from both their siblings and parents when the children are already adults. Lastly, the average satisfaction of the f
&lt;/p&gt;</description></item><item><title>RLTP&#31639;&#27861;&#26159;&#19968;&#20010;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#24191;&#21578;&#39044;&#21152;&#36733;&#36807;&#31243;&#20013;&#30340;&#24310;&#36831;&#21360;&#35937;&#29616;&#35937;&#12290;</title><link>http://arxiv.org/abs/2302.02592</link><description>&lt;p&gt;
RLTP&#31639;&#27861;&#65306;&#29992;&#20110;&#39044;&#21152;&#36733;&#24191;&#21578;&#20013;&#30340;&#24310;&#36831;&#21360;&#35937;&#24314;&#27169;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
RLTP: Reinforcement Learning to Pace for Delayed Impression Modeling in Preloaded Ads. (arXiv:2302.02592v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02592
&lt;/p&gt;
&lt;p&gt;
RLTP&#31639;&#27861;&#26159;&#19968;&#20010;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#24191;&#21578;&#39044;&#21152;&#36733;&#36807;&#31243;&#20013;&#30340;&#24310;&#36831;&#21360;&#35937;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22686;&#21152;&#21697;&#29260;&#30693;&#21517;&#24230;&#65292;&#35768;&#22810;&#24191;&#21578;&#21830;&#19982;&#24191;&#21578;&#24179;&#21488;&#31614;&#35746;&#21512;&#21516;&#36141;&#20080;&#24191;&#21578;&#27969;&#37327;&#65292;&#28982;&#21518;&#23558;&#24191;&#21578;&#25237;&#25918;&#21040;&#30446;&#26631;&#21463;&#20247;&#20013;&#12290;&#22312;&#25972;&#20010;&#24191;&#21578;&#25237;&#25918;&#26399;&#38388;&#65292;&#24191;&#21578;&#21830;&#36890;&#24120;&#24076;&#26395;&#24191;&#21578;&#33719;&#24471;&#29305;&#23450;&#30340;&#21360;&#35937;&#25968;&#65292;&#24182;&#26399;&#26395;&#24191;&#21578;&#23637;&#31034;&#30340;&#25928;&#26524;&#36234;&#22909;&#36234;&#22909;&#65288;&#22914;&#39640;&#28857;&#20987;&#29575;&#65289;&#12290;&#24191;&#21578;&#24179;&#21488;&#36890;&#36807;&#23454;&#26102;&#35843;&#25972;&#27969;&#37327;&#35831;&#27714;&#30340;&#36873;&#25321;&#27010;&#29575;&#26469;&#28385;&#36275;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#21457;&#24067;&#32773;&#30340;&#31574;&#30053;&#20063;&#20250;&#24433;&#21709;&#24191;&#21578;&#25237;&#25918;&#36807;&#31243;&#65292;&#36825;&#26159;&#24191;&#21578;&#24179;&#21488;&#26080;&#27861;&#25511;&#21046;&#30340;&#12290;&#39044;&#21152;&#36733;&#26159;&#35768;&#22810;&#31867;&#22411;&#24191;&#21578;&#65288;&#22914;&#35270;&#39057;&#24191;&#21578;&#65289;&#30340;&#24120;&#29992;&#31574;&#30053;&#65292;&#20197;&#30830;&#20445;&#22312;&#27969;&#37327;&#35831;&#27714;&#21518;&#26174;&#31034;&#30340;&#21709;&#24212;&#26102;&#38388;&#26159;&#21512;&#29702;&#30340;&#65292;&#36825;&#23558;&#23548;&#33268;&#24310;&#36831;&#21360;&#35937;&#29616;&#35937;&#12290;&#20256;&#32479;&#30340;&#37197;&#36895;&#31639;&#27861;&#26080;&#27861;&#24456;&#22909;&#22320;&#22788;&#29702;&#39044;&#21152;&#36733;&#30340;&#29305;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#20381;&#36182;&#20110;&#21363;&#26102;&#21453;&#39304;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;
To increase brand awareness, many advertisers conclude contracts with advertising platforms to purchase traffic and then deliver advertisements to target audiences. In a whole delivery period, advertisers usually desire a certain impression count for the ads, and they also expect that the delivery performance is as good as possible (e.g., obtaining high click-through rate). Advertising platforms employ pacing algorithms to satisfy the demands via adjusting the selection probabilities to traffic requests in real-time. However, the delivery procedure is also affected by the strategies from publishers, which cannot be controlled by advertising platforms. Preloading is a widely used strategy for many types of ads (e.g., video ads) to make sure that the response time for displaying after a traffic request is legitimate, which results in delayed impression phenomenon. Traditional pacing algorithms cannot handle the preloading nature well because they rely on immediate feedback signals, and m
&lt;/p&gt;</description></item></channel></rss>