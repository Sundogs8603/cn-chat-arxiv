<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#20195;&#29702;&#30340;&#31995;&#32479;VideoAgent&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20013;&#22830;&#20195;&#29702;&#65292;&#37319;&#29992;&#20114;&#21160;&#25512;&#29702;&#21644;&#35745;&#21010;&#26469;&#22788;&#29702;&#38271;&#35270;&#39057;&#29702;&#35299;&#38382;&#39064;&#65292;&#22312;&#25361;&#25112;&#24615;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#25928;&#26524;&#21644;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.10517</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35270;&#39057;&#20195;&#29702;&#65306;&#38271;&#35270;&#39057;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
VideoAgent: Long-form Video Understanding with Large Language Model as Agent
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10517
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#20195;&#29702;&#30340;&#31995;&#32479;VideoAgent&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20013;&#22830;&#20195;&#29702;&#65292;&#37319;&#29992;&#20114;&#21160;&#25512;&#29702;&#21644;&#35745;&#21010;&#26469;&#22788;&#29702;&#38271;&#35270;&#39057;&#29702;&#35299;&#38382;&#39064;&#65292;&#22312;&#25361;&#25112;&#24615;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#25928;&#26524;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#35270;&#39057;&#29702;&#35299;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#20195;&#34920;&#30528;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#65292;&#38656;&#35201;&#19968;&#20010;&#33021;&#22815;&#25512;&#29702;&#38271;&#26102;&#38388;&#22810;&#27169;&#24577;&#24207;&#21015;&#30340;&#27169;&#22411;&#12290;&#21463;&#20154;&#31867;&#35748;&#30693;&#38271;&#35270;&#39057;&#36807;&#31243;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24378;&#35843;&#20114;&#21160;&#25512;&#29702;&#21644;&#35745;&#21010;&#65292;&#32780;&#19981;&#26159;&#22788;&#29702;&#38271;&#31687;&#35270;&#35273;&#36755;&#20837;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20110;&#20195;&#29702;&#30340;&#31995;&#32479;VideoAgent&#65292;&#23427;&#37319;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20013;&#22830;&#20195;&#29702;&#65292;&#36845;&#20195;&#22320;&#35782;&#21035;&#21644;&#25972;&#29702;&#20851;&#38190;&#20449;&#24687;&#20197;&#22238;&#31572;&#38382;&#39064;&#65292;&#35270;&#35273;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#20316;&#20026;&#24037;&#20855;&#26469;&#32763;&#35793;&#21644;&#26816;&#32034;&#35270;&#35273;&#20449;&#24687;&#12290;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;EgoSchema&#21644;NExT-QA&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;VideoAgent&#22312;&#24179;&#22343;&#20165;&#20351;&#29992;8.4&#21644;8.2&#24103;&#30340;&#24773;&#20917;&#19979;&#20998;&#21035;&#23454;&#29616;&#20102;54.1%&#21644;71.3%&#30340;&#38646;-shot&#20934;&#30830;&#29575;&#12290;&#36825;&#20123;&#32467;&#26524;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30456;&#23545;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#21331;&#36234;&#25928;&#26524;&#21644;&#25928;&#29575;&#65292;&#31361;&#20986;&#20102;&#20195;&#29702;&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10517v1 Announce Type: cross  Abstract: Long-form video understanding represents a significant challenge within computer vision, demanding a model capable of reasoning over long multi-modal sequences. Motivated by the human cognitive process for long-form video understanding, we emphasize interactive reasoning and planning over the ability to process lengthy visual inputs. We introduce a novel agent-based system, VideoAgent, that employs a large language model as a central agent to iteratively identify and compile crucial information to answer a question, with vision-language foundation models serving as tools to translate and retrieve visual information. Evaluated on the challenging EgoSchema and NExT-QA benchmarks, VideoAgent achieves 54.1% and 71.3% zero-shot accuracy with only 8.4 and 8.2 frames used on average. These results demonstrate superior effectiveness and efficiency of our method over the current state-of-the-art methods, highlighting the potential of agent-base
&lt;/p&gt;</description></item><item><title>FeatUp&#26159;&#19968;&#20010;&#20219;&#21153;&#21644;&#27169;&#22411;&#26080;&#20851;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#28145;&#24230;&#29305;&#24449;&#20013;&#24674;&#22797;&#20002;&#22833;&#30340;&#31354;&#38388;&#20449;&#24687;&#65292;&#20174;&#32780;&#20351;&#29305;&#24449;&#21487;&#20197;&#20197;&#20219;&#20309;&#20998;&#36776;&#29575;&#37325;&#24314;&#65292;&#22312;&#29616;&#26377;&#24212;&#29992;&#20013;&#21462;&#24471;&#20998;&#36776;&#29575;&#21644;&#24615;&#33021;&#30340;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2403.10516</link><description>&lt;p&gt;
FeatUp: &#19968;&#20010;&#19982;&#27169;&#22411;&#26080;&#20851;&#30340;&#29305;&#24449;&#20219;&#24847;&#20998;&#36776;&#29575;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
FeatUp: A Model-Agnostic Framework for Features at Any Resolution
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10516
&lt;/p&gt;
&lt;p&gt;
FeatUp&#26159;&#19968;&#20010;&#20219;&#21153;&#21644;&#27169;&#22411;&#26080;&#20851;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#28145;&#24230;&#29305;&#24449;&#20013;&#24674;&#22797;&#20002;&#22833;&#30340;&#31354;&#38388;&#20449;&#24687;&#65292;&#20174;&#32780;&#20351;&#29305;&#24449;&#21487;&#20197;&#20197;&#20219;&#20309;&#20998;&#36776;&#29575;&#37325;&#24314;&#65292;&#22312;&#29616;&#26377;&#24212;&#29992;&#20013;&#21462;&#24471;&#20998;&#36776;&#29575;&#21644;&#24615;&#33021;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#29305;&#24449;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#30740;&#31350;&#30340;&#22522;&#30707;&#65292;&#25429;&#25417;&#22270;&#20687;&#35821;&#20041;&#24182;&#20351;&#31038;&#21306;&#33021;&#22815;&#35299;&#20915;&#19979;&#28216;&#20219;&#21153;&#65292;&#21363;&#20351;&#22312;&#38646;&#25110;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#20063;&#33021;&#20570;&#21040;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#29305;&#24449;&#36890;&#24120;&#32570;&#20047;&#31354;&#38388;&#20998;&#36776;&#29575;&#65292;&#26080;&#27861;&#30452;&#25509;&#25191;&#34892;&#20687;&#20998;&#21106;&#21644;&#28145;&#24230;&#39044;&#27979;&#36825;&#26679;&#30340;&#31264;&#23494;&#39044;&#27979;&#20219;&#21153;&#65292;&#22240;&#20026;&#27169;&#22411;&#20250;&#36807;&#20110;&#32858;&#21512;&#22823;&#33539;&#22260;&#30340;&#20449;&#24687;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;FeatUp&#65292;&#19968;&#20010;&#20219;&#21153;&#21644;&#27169;&#22411;&#26080;&#20851;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#24674;&#22797;&#28145;&#24230;&#29305;&#24449;&#20013;&#20002;&#22833;&#30340;&#31354;&#38388;&#20449;&#24687;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;FeatUp&#30340;&#20004;&#20010;&#21464;&#20307;&#65306;&#19968;&#20010;&#22312;&#21333;&#27425;&#21069;&#21521;&#20256;&#36882;&#20013;&#24341;&#23548;&#20855;&#26377;&#39640;&#20998;&#36776;&#29575;&#20449;&#21495;&#30340;&#29305;&#24449;&#65292;&#21478;&#19968;&#20010;&#36866;&#24212;&#21333;&#20010;&#22270;&#20687;&#24182;&#20197;&#20219;&#20309;&#20998;&#36776;&#29575;&#37325;&#26500;&#29305;&#24449;&#30340;&#38544;&#24335;&#27169;&#22411;&#12290;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#20351;&#29992;&#20102;&#19968;&#20010;&#20855;&#26377;&#19982; NeRF &#31867;&#20284;&#30340;&#28145;&#24230;&#31867;&#27604;&#30340;&#22810;&#35270;&#22270;&#19968;&#33268;&#24615;&#25439;&#22833;&#12290;&#25105;&#20204;&#30340;&#29305;&#24449;&#20445;&#30041;&#20854;&#21407;&#22987;&#35821;&#20041;&#65292;&#24182;&#21487;&#20197;&#26367;&#25442;&#29616;&#26377;&#24212;&#29992;&#31243;&#24207;&#65292;&#21363;&#20351;&#19981;&#37325;&#26032;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10516v1 Announce Type: cross  Abstract: Deep features are a cornerstone of computer vision research, capturing image semantics and enabling the community to solve downstream tasks even in the zero- or few-shot regime. However, these features often lack the spatial resolution to directly perform dense prediction tasks like segmentation and depth prediction because models aggressively pool information over large areas. In this work, we introduce FeatUp, a task- and model-agnostic framework to restore lost spatial information in deep features. We introduce two variants of FeatUp: one that guides features with high-resolution signal in a single forward pass, and one that fits an implicit model to a single image to reconstruct features at any resolution. Both approaches use a multi-view consistency loss with deep analogies to NeRFs. Our features retain their original semantics and can be swapped into existing applications to yield resolution and performance gains even without re-
&lt;/p&gt;</description></item><item><title>SocialGenPod&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#31169;&#21451;&#22909;&#30340;&#29983;&#25104;&#24335;AI&#31038;&#20132;&#32593;&#32476;&#24212;&#29992;&#37096;&#32626;&#26041;&#24335;&#65292;&#36890;&#36807;&#20351;&#29992;&#20998;&#24067;&#24335;&#30340;Solid&#35268;&#33539;&#26469;&#35299;&#32806;&#29992;&#25143;&#25968;&#25454;&#19982;&#24212;&#29992;&#31243;&#24207;&#65292;&#23454;&#29616;&#29992;&#25143;&#22312;&#20010;&#20154;Pod&#20013;&#23433;&#20840;&#23384;&#20648;&#25152;&#26377;&#25968;&#25454;&#30340;&#25511;&#21046;&#12290;</title><link>https://arxiv.org/abs/2403.10408</link><description>&lt;p&gt;
SocialGenPod: &#38544;&#31169;&#21451;&#22909;&#30340;&#20998;&#24067;&#24335;&#20010;&#20154;&#25968;&#25454;&#23384;&#20648;&#30340;&#29983;&#25104;&#24335;AI&#31038;&#20132;&#32593;&#32476;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
SocialGenPod: Privacy-Friendly Generative AI Social Web Applications with Decentralised Personal Data Stores
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10408
&lt;/p&gt;
&lt;p&gt;
SocialGenPod&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#31169;&#21451;&#22909;&#30340;&#29983;&#25104;&#24335;AI&#31038;&#20132;&#32593;&#32476;&#24212;&#29992;&#37096;&#32626;&#26041;&#24335;&#65292;&#36890;&#36807;&#20351;&#29992;&#20998;&#24067;&#24335;&#30340;Solid&#35268;&#33539;&#26469;&#35299;&#32806;&#29992;&#25143;&#25968;&#25454;&#19982;&#24212;&#29992;&#31243;&#24207;&#65292;&#23454;&#29616;&#29992;&#25143;&#22312;&#20010;&#20154;Pod&#20013;&#23433;&#20840;&#23384;&#20648;&#25152;&#26377;&#25968;&#25454;&#30340;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;SocialGenPod&#65292;&#36825;&#26159;&#19968;&#31181;&#20998;&#24067;&#24335;&#19988;&#38544;&#31169;&#21451;&#22909;&#30340;&#26041;&#24335;&#29992;&#20110;&#37096;&#32626;&#29983;&#25104;&#24335;AI Web&#24212;&#29992;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;Solid&#35268;&#33539;&#26469;&#23558;&#29992;&#25143;&#25968;&#25454;&#19982;&#29983;&#25104;&#24335;AI&#24212;&#29992;&#31243;&#24207;&#35299;&#32806;&#65292;&#19982;&#20445;&#25345;&#29992;&#25143;&#25968;&#25454;&#19982;&#24212;&#29992;&#31243;&#24207;&#21644;&#26381;&#21153;&#25552;&#20379;&#21830;&#32465;&#23450;&#30340;&#38598;&#20013;&#24335;Web&#21644;&#25968;&#25454;&#26550;&#26500;&#19981;&#21516;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#21407;&#22411;&#28436;&#31034;&#20102;SocialGenPod&#65292;&#20801;&#35768;&#29992;&#25143;&#19982;&#19981;&#21516;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#35805;&#65292;&#21487;&#36873;&#25321;&#21033;&#29992;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#25216;&#26415;&#29983;&#25104;&#20197;&#29992;&#25143;&#34987;&#20801;&#35768;&#30452;&#25509;&#25110;&#38388;&#25509;&#35775;&#38382;&#30340;&#20219;&#20309;Solid Pod&#20013;&#23384;&#20648;&#30340;&#31169;&#20154;&#25991;&#26723;&#20026;&#22522;&#30784;&#30340;&#31572;&#26696;&#12290;SocialGenPod&#21033;&#29992;Solid&#35775;&#38382;&#25511;&#21046;&#26426;&#21046;&#65292;&#35753;&#29992;&#25143;&#23436;&#20840;&#25511;&#21046;&#30830;&#23450;&#35841;&#21487;&#20197;&#35775;&#38382;&#23384;&#20648;&#22312;&#20182;&#20204;Pod&#20013;&#30340;&#25968;&#25454;&#12290;SocialGenPod&#23558;&#25152;&#26377;&#29992;&#25143;&#25968;&#25454;&#65288;&#32842;&#22825;&#35760;&#24405;&#65292;&#24212;&#29992;&#31243;&#24207;&#37197;&#32622;&#65292;&#20010;&#20154;&#25991;&#26723;&#31561;&#65289;&#23433;&#20840;&#22320;&#23384;&#20648;&#22312;&#29992;&#25143;&#30340;&#20010;&#20154;Pod&#20013;&#65307;&#19982;&#29305;&#23450;&#27169;&#22411;&#20998;&#24320;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10408v1 Announce Type: cross  Abstract: We present SocialGenPod, a decentralised and privacy-friendly way of deploying generative AI Web applications. Unlike centralised Web and data architectures that keep user data tied to application and service providers, we show how one can use Solid -- a decentralised Web specification -- to decouple user data from generative AI applications. We demonstrate SocialGenPod using a prototype that allows users to converse with different Large Language Models, optionally leveraging Retrieval Augmented Generation to generate answers grounded in private documents stored in any Solid Pod that the user is allowed to access, directly or indirectly. SocialGenPod makes use of Solid access control mechanisms to give users full control of determining who has access to data stored in their Pods. SocialGenPod keeps all user data (chat history, app configuration, personal documents, etc) securely in the user's personal Pod; separate from specific model 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;&#36328;&#32534;&#30721;&#22120;&#21644;LLMs&#22312;&#37325;&#26032;&#25490;&#24207;SPLADE&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#22312;MS MARCO&#19978;&#65292;&#20004;&#32773;&#38590;&#20197;&#21306;&#20998;&#65292;&#20294;&#22312;&#39046;&#22495;&#22806;&#24773;&#20917;&#19979;&#65292;&#27169;&#22411;&#31867;&#22411;&#21644;&#37325;&#26032;&#25490;&#24207;&#25991;&#26723;&#25968;&#37327;&#23545;&#25928;&#26524;&#26377;&#24433;&#21709;&#65292;&#21516;&#26102;GPT-4&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20256;&#32479;&#36328;&#32534;&#30721;&#22120;&#20173;&#28982;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.10407</link><description>&lt;p&gt;
&#36328;&#32534;&#30721;&#22120;&#21644;LLMs&#22312;&#37325;&#26032;&#25490;&#24207;SPLADE&#20013;&#30340;&#24443;&#24213;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
A Thorough Comparison of Cross-Encoders and LLMs for Reranking SPLADE
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10407
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;&#36328;&#32534;&#30721;&#22120;&#21644;LLMs&#22312;&#37325;&#26032;&#25490;&#24207;SPLADE&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#22312;MS MARCO&#19978;&#65292;&#20004;&#32773;&#38590;&#20197;&#21306;&#20998;&#65292;&#20294;&#22312;&#39046;&#22495;&#22806;&#24773;&#20917;&#19979;&#65292;&#27169;&#22411;&#31867;&#22411;&#21644;&#37325;&#26032;&#25490;&#24207;&#25991;&#26723;&#25968;&#37327;&#23545;&#25928;&#26524;&#26377;&#24433;&#21709;&#65292;&#21516;&#26102;GPT-4&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20256;&#32479;&#36328;&#32534;&#30721;&#22120;&#20173;&#28982;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#37325;&#26032;&#25490;&#24207;&#26377;&#25928;&#30340;SPLADE&#26816;&#32034;&#22120;&#30340;&#32972;&#26223;&#19979;&#65292;&#20171;&#32461;&#20102;&#36328;&#32534;&#30721;&#22120;&#21644;LLMs&#37325;&#26032;&#25490;&#24207;&#22120;&#20043;&#38388;&#30340;&#27604;&#36739;&#24615;&#30740;&#31350;&#12290;&#25105;&#20204;&#23545;TREC&#28145;&#24230;&#23398;&#20064;&#25968;&#25454;&#38598;&#21644;BEIR&#12289;LoTTE&#31561;&#39046;&#22495;&#22806;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#22823;&#37327;&#35780;&#20272;&#12290;&#22312;&#31532;&#19968;&#32452;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#37325;&#26032;&#25490;&#24207;SPLADE&#22312;MS MARCO&#19978;&#26102;&#65292;&#36328;&#32534;&#30721;&#22120;&#37325;&#26032;&#25490;&#24207;&#22120;&#26159;&#38590;&#20197;&#21306;&#20998;&#30340;&#12290;&#35266;&#23519;&#32467;&#26524;&#22312;&#39046;&#22495;&#22806;&#24773;&#20917;&#19979;&#21457;&#29983;&#20102;&#21464;&#21270;&#65292;&#27169;&#22411;&#31867;&#22411;&#21644;&#37325;&#26032;&#25490;&#24207;&#25991;&#26723;&#25968;&#37327;&#23545;&#25928;&#26524;&#20135;&#29983;&#24433;&#21709;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21015;&#34920;&#24335;&#37325;&#26032;&#25490;&#24207;&#22120;&#65292;&#29305;&#21035;&#26159;GPT-4&#12290;&#34429;&#28982;GPT-4&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#65288;&#38646;&#26679;&#26412;&#65289;&#24615;&#33021;&#65292;&#20294;&#25105;&#20204;&#34920;&#26126;&#20256;&#32479;&#30340;&#36328;&#32534;&#30721;&#22120;&#20173;&#28982;&#38750;&#24120;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26088;&#22312;&#25552;&#20379;&#23545;&#22260;&#32469;LLM-based&#37325;&#26032;&#25490;&#24207;&#22120;&#26368;&#36817;&#30340;&#20852;&#22859;&#24773;&#32490;&#26356;&#32454;&#33268;&#30340;&#35266;&#28857;&#65292;&#23558;&#23427;&#20204;&#23450;&#20301;&#20026;&#22312;&#24179;&#34913;&#25928;&#26524;&#26102;&#38656;&#35201;&#32771;&#34385;&#30340;&#21478;&#19968;&#20010;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10407v1 Announce Type: new  Abstract: We present a comparative study between cross-encoder and LLMs rerankers in the context of re-ranking effective SPLADE retrievers. We conduct a large evaluation on TREC Deep Learning datasets and out-of-domain datasets such as BEIR and LoTTE. In the first set of experiments, we show how cross-encoder rerankers are hard to distinguish when it comes to re-rerank SPLADE on MS MARCO. Observations shift in the out-of-domain scenario, where both the type of model and the number of documents to re-rank have an impact on effectiveness. Then, we focus on listwise rerankers based on Large Language Models -- especially GPT-4. While GPT-4 demonstrates impressive (zero-shot) performance, we show that traditional cross-encoders remain very competitive. Overall, our findings aim to to provide a more nuanced perspective on the recent excitement surrounding LLM-based re-rankers -- by positioning them as another factor to consider in balancing effectivenes
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EDITOR&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#20026;&#22810;&#27169;&#24577;&#29289;&#20307;&#20877;&#35782;&#21035;&#36873;&#25321;&#26469;&#33258;&#35270;&#35273;Transformer&#30340;&#22810;&#26679;&#21270;&#20196;&#29260;&#65292;&#36890;&#36807;Spatial-Frequency Token Selection&#65288;SFTS&#65289;&#27169;&#22359;&#33258;&#36866;&#24212;&#36873;&#25321;&#20197;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#20196;&#29260;&#65292;&#21516;&#26102;&#36890;&#36807;Hierarchical Masked Aggregation&#65288;HMA&#65289;&#27169;&#22359;&#20419;&#36827;&#36328;&#27169;&#24577;&#20869;&#37096;&#21644;&#20043;&#38388;&#30340;&#29305;&#24449;&#20132;&#20114;</title><link>https://arxiv.org/abs/2403.10254</link><description>&lt;p&gt;
&#39764;&#27861;&#20196;&#29260;&#65306;&#20026;&#22810;&#27169;&#24577;&#29289;&#20307;&#20877;&#35782;&#21035;&#36873;&#25321;&#22810;&#26679;&#21270;&#20196;&#29260;
&lt;/p&gt;
&lt;p&gt;
Magic Tokens: Select Diverse Tokens for Multi-modal Object Re-Identification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10254
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EDITOR&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#20026;&#22810;&#27169;&#24577;&#29289;&#20307;&#20877;&#35782;&#21035;&#36873;&#25321;&#26469;&#33258;&#35270;&#35273;Transformer&#30340;&#22810;&#26679;&#21270;&#20196;&#29260;&#65292;&#36890;&#36807;Spatial-Frequency Token Selection&#65288;SFTS&#65289;&#27169;&#22359;&#33258;&#36866;&#24212;&#36873;&#25321;&#20197;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#20196;&#29260;&#65292;&#21516;&#26102;&#36890;&#36807;Hierarchical Masked Aggregation&#65288;HMA&#65289;&#27169;&#22359;&#20419;&#36827;&#36328;&#27169;&#24577;&#20869;&#37096;&#21644;&#20043;&#38388;&#30340;&#29305;&#24449;&#20132;&#20114;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21333;&#27169;&#24577;&#29289;&#20307;&#20877;&#35782;&#21035;&#65288;ReID&#65289;&#22312;&#22797;&#26434;&#35270;&#35273;&#22330;&#26223;&#20013;&#20445;&#25345;&#31283;&#20581;&#24615;&#38754;&#20020;&#24040;&#22823;&#25361;&#25112;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#22810;&#27169;&#24577;&#29289;&#20307;ReID&#21033;&#29992;&#26469;&#33258;&#22810;&#26679;&#21270;&#27169;&#24577;&#30340;&#20114;&#34917;&#20449;&#24687;&#65292;&#26174;&#31034;&#20986;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#26041;&#27861;&#21487;&#33021;&#23481;&#26131;&#21463;&#21040;&#26080;&#20851;&#32972;&#26223;&#30340;&#24433;&#21709;&#65292;&#24182;&#36890;&#24120;&#24573;&#35270;&#27169;&#24577;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;\textbf{EDITOR}&#30340;&#26032;&#22411;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#20026;&#22810;&#27169;&#24577;&#29289;&#20307;ReID&#36873;&#25321;&#26469;&#33258;&#35270;&#35273;Transformer&#30340;&#22810;&#26679;&#21270;&#20196;&#29260;&#12290;&#25105;&#20204;&#20174;&#19968;&#20010;&#20849;&#20139;&#30340;&#35270;&#35273;Transformer&#24320;&#22987;&#65292;&#20174;&#19981;&#21516;&#30340;&#36755;&#20837;&#27169;&#24577;&#20013;&#25552;&#21462;&#20196;&#29260;&#21270;&#29305;&#24449;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;&#31354;&#38388;&#39057;&#29575;&#20196;&#29260;&#36873;&#25321;&#65288;SFTS&#65289;&#27169;&#22359;&#65292;&#33258;&#36866;&#24212;&#22320;&#36873;&#25321;&#20855;&#26377;&#31354;&#38388;&#21644;&#39057;&#29575;&#20449;&#24687;&#30340;&#20197;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#20196;&#29260;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#20010;&#21517;&#20026;&#20998;&#23618;&#25513;&#30721;&#32858;&#21512;&#65288;HMA&#65289;&#27169;&#22359;&#65292;&#20419;&#36827;&#36328;&#27169;&#24577;&#20869;&#37096;&#21644;&#20043;&#38388;&#30340;&#29305;&#24449;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10254v1 Announce Type: cross  Abstract: Single-modal object re-identification (ReID) faces great challenges in maintaining robustness within complex visual scenarios. In contrast, multi-modal object ReID utilizes complementary information from diverse modalities, showing great potentials for practical applications. However, previous methods may be easily affected by irrelevant backgrounds and usually ignore the modality gaps. To address above issues, we propose a novel learning framework named \textbf{EDITOR} to select diverse tokens from vision Transformers for multi-modal object ReID. We begin with a shared vision Transformer to extract tokenized features from different input modalities. Then, we introduce a Spatial-Frequency Token Selection (SFTS) module to adaptively select object-centric tokens with both spatial and frequency information. Afterwards, we employ a Hierarchical Masked Aggregation (HMA) module to facilitate feature interactions within and across modalities.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#20174;&#19981;&#21516;&#35270;&#35282;&#21644;&#32454;&#31890;&#24230;&#22810;&#23610;&#24230;&#21305;&#37197;&#22810;&#27169;&#24577;&#20449;&#24687;&#30340;&#26032;&#22411;ATR&#26694;&#26550;</title><link>https://arxiv.org/abs/2403.10146</link><description>&lt;p&gt;
&#22522;&#20110;&#36328;&#27169;&#24577;&#30456;&#20284;&#24615;&#19968;&#33268;&#24615;&#39537;&#21160;&#30340;&#22810;&#23610;&#24230;&#21305;&#37197;&#29992;&#20110;&#38899;&#39057;-&#25991;&#26412;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Multiscale Matching Driven by Cross-Modal Similarity Consistency for Audio-Text Retrieval
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10146
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#20174;&#19981;&#21516;&#35270;&#35282;&#21644;&#32454;&#31890;&#24230;&#22810;&#23610;&#24230;&#21305;&#37197;&#22810;&#27169;&#24577;&#20449;&#24687;&#30340;&#26032;&#22411;ATR&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#39057;-&#25991;&#26412;&#26816;&#32034;&#65288;ATR&#65289;&#36817;&#24180;&#26469;&#21463;&#21040;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#20851;&#27880;&#65292;&#23427;&#21487;&#20197;&#22312;&#32473;&#23450;&#38899;&#39057;&#21098;&#36753;&#65288;A2T&#65289;&#26102;&#26816;&#32034;&#30456;&#20851;&#23383;&#24149;&#65292;&#21453;&#20043;&#20134;&#28982;&#65288;T2A&#65289;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#23558;&#27599;&#31181;&#27169;&#24577;&#30340;&#20449;&#24687;&#32858;&#21512;&#21040;&#21333;&#20010;&#21521;&#37327;&#20013;&#36827;&#34892;&#21305;&#37197;&#65292;&#20294;&#36825;&#26679;&#20570;&#20250;&#29306;&#29298;&#23616;&#37096;&#32454;&#33410;&#65292;&#24182;&#19988;&#24456;&#38590;&#25429;&#33719;&#27169;&#24577;&#20869;&#37096;&#21644;&#20043;&#38388;&#22797;&#26434;&#30340;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#24403;&#21069;ATR&#25968;&#25454;&#38598;&#32570;&#20047;&#20840;&#38754;&#30340;&#23545;&#40784;&#20449;&#24687;&#65292;&#24182;&#19988;&#31616;&#21333;&#30340;&#20108;&#20803;&#23545;&#27604;&#23398;&#20064;&#26631;&#31614;&#24573;&#30053;&#20102;&#26679;&#26412;&#38388;&#32454;&#31890;&#24230;&#35821;&#20041;&#24046;&#24322;&#30340;&#34913;&#37327;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;ATR&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20840;&#38754;&#25429;&#25417;&#20102;&#19981;&#21516;&#35270;&#35282;&#21644;&#26356;&#32454;&#30340;&#31890;&#24230;&#20013;&#22810;&#27169;&#24577;&#20449;&#24687;&#30340;&#21305;&#37197;&#20851;&#31995;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#32454;&#31890;&#24230;&#23545;&#40784;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#23616;&#37096;&#21040;&#20840;&#23616;&#23618;&#27425;&#30340;&#22810;&#23610;&#24230;&#36807;&#31243;&#23454;&#29616;&#26356;&#35814;&#32454;&#30340;&#21305;&#37197;&#65292;&#20197;&#25429;&#33719;metic
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10146v1 Announce Type: cross  Abstract: Audio-text retrieval (ATR), which retrieves a relevant caption given an audio clip (A2T) and vice versa (T2A), has recently attracted much research attention. Existing methods typically aggregate information from each modality into a single vector for matching, but this sacrifices local details and can hardly capture intricate relationships within and between modalities. Furthermore, current ATR datasets lack comprehensive alignment information, and simple binary contrastive learning labels overlook the measurement of fine-grained semantic differences between samples. To counter these challenges, we present a novel ATR framework that comprehensively captures the matching relationships of multimodal information from different perspectives and finer granularities. Specifically, a fine-grained alignment method is introduced, achieving a more detail-oriented matching through a multiscale process from local to global levels to capture metic
&lt;/p&gt;</description></item><item><title>&#25506;&#32034;&#22312;&#39034;&#24207;&#25512;&#33616;&#20013;&#20351;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#32858;&#21512;&#28436;&#31034;&#30340;&#26032;&#39062;&#26041;&#27861;LLMSRec-Syn&#65292;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#35777;&#26126;&#20854;&#20248;&#20110;&#29616;&#26377;&#22522;&#20110;LLM&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.10135</link><description>&lt;p&gt;
&#25972;&#20307;&#20248;&#20110;&#24635;&#21644;&#65306;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#20351;&#29992;&#32858;&#21512;&#28436;&#31034;&#36827;&#34892;&#39034;&#24207;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
The Whole is Better than the Sum: Using Aggregated Demonstrations in In-Context Learning for Sequential Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10135
&lt;/p&gt;
&lt;p&gt;
&#25506;&#32034;&#22312;&#39034;&#24207;&#25512;&#33616;&#20013;&#20351;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#32858;&#21512;&#28436;&#31034;&#30340;&#26032;&#39062;&#26041;&#27861;LLMSRec-Syn&#65292;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#35777;&#26126;&#20854;&#20248;&#20110;&#29616;&#26377;&#22522;&#20110;LLM&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20248;&#31168;&#24615;&#33021;&#12290;&#20026;&#20102;&#23558;LLMs&#20316;&#20026;&#24378;&#22823;&#30340;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#39034;&#24207;&#25512;&#33616;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#25351;&#23548;&#26684;&#24335;&#12289;&#20219;&#21153;&#19968;&#33268;&#24615;&#12289;&#28436;&#31034;&#36873;&#25321;&#21644;&#28436;&#31034;&#25968;&#37327;&#23545;&#27169;&#22411;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;LLMSRec-Syn&#65292;&#36890;&#36807;&#23558;&#22810;&#20010;&#28436;&#31034;&#29992;&#25143;&#25972;&#21512;&#25104;&#19968;&#20010;&#32858;&#21512;&#28436;&#31034;&#26469;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#25512;&#33616;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#26126;&#65292;LLMSRec-Syn&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;LLM&#30340;&#39034;&#24207;&#25512;&#33616;&#26041;&#27861;&#12290;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;LLMSRec-Syn&#21487;&#20197;&#19982;&#29978;&#33267;&#20248;&#20110;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#20844;&#24320;&#22312;https://github.com/demoleiwang/LLMSRec_Syn&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10135v1 Announce Type: cross  Abstract: Large language models (LLMs) have shown excellent performance on various NLP tasks. To use LLMs as strong sequential recommenders, we explore the in-context learning approach to sequential recommendation. We investigate the effects of instruction format, task consistency, demonstration selection, and number of demonstrations. As increasing the number of demonstrations in ICL does not improve accuracy despite using a long prompt, we propose a novel method called LLMSRec-Syn that incorporates multiple demonstration users into one aggregated demonstration. Our experiments on three recommendation datasets show that LLMSRec-Syn outperforms state-of-the-art LLM-based sequential recommendation methods. In some cases, LLMSRec-Syn can perform on par with or even better than supervised learning methods. Our code is publicly available at https://github.com/demoleiwang/LLMSRec_Syn.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26694;&#26550;DRAGIN&#65292;&#26088;&#22312;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#29983;&#25104;&#36807;&#31243;&#20013;&#21160;&#24577;&#26816;&#32034;&#21644;&#29983;&#25104;&#20013;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.10081</link><description>&lt;p&gt;
DRAGIN&#65306;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#26102;&#20449;&#24687;&#38656;&#27714;&#30340;&#21160;&#24577;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
DRAGIN: Dynamic Retrieval Augmented Generation based on the Real-time Information Needs of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10081
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26694;&#26550;DRAGIN&#65292;&#26088;&#22312;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#29983;&#25104;&#36807;&#31243;&#20013;&#21160;&#24577;&#26816;&#32034;&#21644;&#29983;&#25104;&#20013;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#33539;&#24335;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25991;&#26412;&#29983;&#25104;&#36807;&#31243;&#20013;&#20027;&#21160;&#20915;&#23450;&#20309;&#26102;&#20197;&#21450;&#20309;&#26102;&#26816;&#32034;&#12290;&#35813;&#33539;&#24335;&#30340;&#20004;&#20010;&#20851;&#38190;&#20803;&#32032;&#26159;&#30830;&#23450;&#28608;&#27963;&#26816;&#32034;&#27169;&#22359;&#30340;&#26368;&#20339;&#26102;&#26426;&#65288;&#20915;&#23450;&#20309;&#26102;&#26816;&#32034;&#65289;&#20197;&#21450;&#19968;&#26086;&#35302;&#21457;&#26816;&#32034;&#65292;&#21046;&#23450;&#36866;&#24403;&#30340;&#26597;&#35810;&#65288;&#30830;&#23450;&#35201;&#26816;&#32034;&#20160;&#20040;&#65289;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#21160;&#24577;RAG&#26041;&#27861;&#22312;&#20004;&#20010;&#26041;&#38754;&#37117;&#23384;&#22312;&#19981;&#36275;&#12290;&#39318;&#20808;&#65292;&#20915;&#23450;&#20309;&#26102;&#36827;&#34892;&#26816;&#32034;&#30340;&#31574;&#30053;&#36890;&#24120;&#20381;&#36182;&#20110;&#38745;&#24577;&#35268;&#21017;&#12290;&#27492;&#22806;&#65292;&#20915;&#23450;&#35201;&#26816;&#32034;&#20160;&#20040;&#30340;&#31574;&#30053;&#36890;&#24120;&#23616;&#38480;&#20110;LLM&#30340;&#26368;&#36817;&#19968;&#21477;&#25110;&#26368;&#21518;&#20960;&#20010;&#26631;&#35760;&#65292;&#32780;LLM&#30340;&#23454;&#26102;&#20449;&#24687;&#38656;&#27714;&#21487;&#33021;&#36328;&#36234;&#25972;&#20010;&#19978;&#19979;&#25991;&#12290;&#20026;&#20811;&#26381;&#36825;&#20123;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#26694;&#26550;DRAGIN&#65292; &#21363;&#22522;&#20110;LLMs&#23454;&#26102;&#20449;&#24687;&#38656;&#27714;&#30340;&#21160;&#24577;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10081v1 Announce Type: new  Abstract: Dynamic retrieval augmented generation (RAG) paradigm actively decides when and what to retrieve during the text generation process of Large Language Models (LLMs). There are two key elements of this paradigm: identifying the optimal moment to activate the retrieval module (deciding when to retrieve) and crafting the appropriate query once retrieval is triggered (determining what to retrieve). However, current dynamic RAG methods fall short in both aspects. Firstly, the strategies for deciding when to retrieve often rely on static rules. Moreover, the strategies for deciding what to retrieve typically limit themselves to the LLM's most recent sentence or the last few tokens, while the LLM's real-time information needs may span across the entire context. To overcome these limitations, we introduce a new framework, DRAGIN, i.e., Dynamic Retrieval Augmented Generation based on the real-time Information Needs of LLMs. Our framework is specif
&lt;/p&gt;</description></item><item><title>PPM&#26159;&#29992;&#20110;&#28857;&#20987;&#29575;&#39044;&#27979;&#30340;&#39044;&#35757;&#32451;&#25554;&#20214;&#27169;&#22411;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;&#26041;&#27861;&#22312;&#20919;&#21551;&#21160;&#38382;&#39064;&#21644;&#35757;&#32451;&#25968;&#25454;&#38271;&#24230;&#19978;&#30340;&#38480;&#21046;&#65292;&#24182;&#33021;&#22815;&#22312;&#24037;&#19994;&#25512;&#33616;&#31995;&#32479;&#20013;&#23454;&#29616;&#31471;&#21040;&#31471;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2403.10049</link><description>&lt;p&gt;
PPM&#65306;&#29992;&#20110;&#28857;&#20987;&#29575;&#39044;&#27979;&#30340;&#39044;&#35757;&#32451;&#25554;&#20214;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
PPM : A Pre-trained Plug-in Model for Click-through Rate Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10049
&lt;/p&gt;
&lt;p&gt;
PPM&#26159;&#29992;&#20110;&#28857;&#20987;&#29575;&#39044;&#27979;&#30340;&#39044;&#35757;&#32451;&#25554;&#20214;&#27169;&#22411;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;&#26041;&#27861;&#22312;&#20919;&#21551;&#21160;&#38382;&#39064;&#21644;&#35757;&#32451;&#25968;&#25454;&#38271;&#24230;&#19978;&#30340;&#38480;&#21046;&#65292;&#24182;&#33021;&#22815;&#22312;&#24037;&#19994;&#25512;&#33616;&#31995;&#32479;&#20013;&#23454;&#29616;&#31471;&#21040;&#31471;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10049v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#23398;&#31185; &#25688;&#35201;: &#28857;&#20987;&#29575;&#65288;CTR&#65289;&#39044;&#27979;&#26159;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#26680;&#24515;&#20219;&#21153;&#12290;&#29616;&#26377;&#26041;&#27861;&#65288;&#31616;&#31216;&#20026;IDRec&#65289;&#20381;&#36182;&#20110;&#21807;&#19968;&#36523;&#20221;&#26469;&#34920;&#31034;&#20960;&#21313;&#24180;&#26469;&#30427;&#34892;&#30340;&#19981;&#21516;&#29992;&#25143;&#21644;&#29289;&#21697;&#12290;&#19968;&#26041;&#38754;&#65292;IDRec&#22312;&#20919;&#21551;&#21160;&#38382;&#39064;&#19978;&#32463;&#24120;&#38754;&#20020;&#26174;&#33879;&#30340;&#24615;&#33021;&#19979;&#38477;&#65307;&#21478;&#19968;&#26041;&#38754;&#65292;&#30001;&#20110;&#36845;&#20195;&#25928;&#29575;&#30340;&#32422;&#26463;&#65292;IDRec&#26080;&#27861;&#20351;&#29992;&#26356;&#38271;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#39044;&#35757;&#32451;&#30693;&#35782;&#65288;&#20363;&#22914;&#65292;&#39044;&#35757;&#32451;&#29992;&#25143;&#27169;&#22411;&#25110;&#22810;&#27169;&#24577;&#23884;&#20837;&#65289;&#26469;&#32531;&#35299;&#19978;&#36848;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22312;&#32447;&#24310;&#36831;&#30340;&#29190;&#28856;&#24615;&#22686;&#38271;&#21487;&#20197;&#24402;&#22240;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#30340;&#24040;&#22823;&#21442;&#25968;&#12290;&#22240;&#27492;&#65292;&#22823;&#22810;&#25968;&#26080;&#27861;&#22312;&#24037;&#19994;&#25512;&#33616;&#31995;&#32479;&#20013;&#20351;&#29992;&#19982;IDRec&#31471;&#21040;&#31471;&#35757;&#32451;&#30340;&#32479;&#19968;&#27169;&#22411;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;$\textbf{P}$re-trained $\textbf{P}$lug-in CTR $\textbf{M}$odel&#65292;&#21363;PPM&#12290;PPM em
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10049v1 Announce Type: cross  Abstract: Click-through rate (CTR) prediction is a core task in recommender systems. Existing methods (IDRec for short) rely on unique identities to represent distinct users and items that have prevailed for decades. On one hand, IDRec often faces significant performance degradation on cold-start problem; on the other hand, IDRec cannot use longer training data due to constraints imposed by iteration efficiency. Most prior studies alleviate the above problems by introducing pre-trained knowledge(e.g. pre-trained user model or multi-modal embeddings). However, the explosive growth of online latency can be attributed to the huge parameters in the pre-trained model. Therefore, most of them cannot employ the unified model of end-to-end training with IDRec in industrial recommender systems, thus limiting the potential of the pre-trained model. To this end, we propose a $\textbf{P}$re-trained $\textbf{P}$lug-in CTR $\textbf{M}$odel, namely PPM. PPM em
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#20107;&#23454;&#30693;&#35782;&#25552;&#21462;&#20013;&#23384;&#22312;&#30340;&#8220;&#25552;&#31034;&#20559;&#35265;&#8221;&#65292;&#25214;&#21040;&#20102;&#19981;&#21516;&#31867;&#22411;&#25552;&#31034;&#30340;&#20559;&#35265;&#31243;&#24230;&#65292;&#20197;&#21450;&#36825;&#31181;&#20559;&#35265;&#23545;&#19981;&#21516;&#22522;&#20934;&#27979;&#35797;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#34920;&#31034;&#30340;&#26041;&#27861;&#26469;&#20943;&#36731;&#36825;&#31181;&#25552;&#31034;&#20559;&#35265;&#12290;</title><link>https://arxiv.org/abs/2403.09963</link><description>&lt;p&gt;
&#22788;&#29702;&#22909;&#24744;&#30340;&#25552;&#31034;&#20559;&#35265;&#65281;&#35843;&#26597;&#21644;&#20943;&#36731;&#20107;&#23454;&#30693;&#35782;&#25552;&#21462;&#20013;&#30340;&#25552;&#31034;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Take Care of Your Prompt Bias! Investigating and Mitigating Prompt Bias in Factual Knowledge Extraction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09963
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#20107;&#23454;&#30693;&#35782;&#25552;&#21462;&#20013;&#23384;&#22312;&#30340;&#8220;&#25552;&#31034;&#20559;&#35265;&#8221;&#65292;&#25214;&#21040;&#20102;&#19981;&#21516;&#31867;&#22411;&#25552;&#31034;&#30340;&#20559;&#35265;&#31243;&#24230;&#65292;&#20197;&#21450;&#36825;&#31181;&#20559;&#35265;&#23545;&#19981;&#21516;&#22522;&#20934;&#27979;&#35797;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#34920;&#31034;&#30340;&#26041;&#27861;&#26469;&#20943;&#36731;&#36825;&#31181;&#25552;&#31034;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#22312;&#20107;&#23454;&#30693;&#35782;&#25552;&#21462;&#20013;&#23384;&#22312;&#8220;&#25552;&#31034;&#20559;&#35265;&#8221;&#65292;&#21363;&#25552;&#31034;&#24448;&#24448;&#20250;&#24341;&#20837;&#23545;&#29305;&#23450;&#26631;&#31614;&#30340;&#20559;&#35265;&#12290;&#28982;&#32780;&#65292;&#27169;&#22411;&#20869;&#37096;&#25552;&#31034;&#20559;&#35265;&#30340;&#31243;&#24230;&#21644;&#24433;&#21709;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;&#20026;&#20102;&#22238;&#24212;&#36825;&#19968;&#28857;&#65292;&#26412;&#25991;&#37327;&#21270;&#20102;&#19981;&#21516;&#31867;&#22411;&#25552;&#31034;&#30340;&#20559;&#35265;&#65292;&#24182;&#35780;&#20272;&#20102;&#23427;&#20204;&#23545;&#19981;&#21516;&#22522;&#20934;&#27979;&#35797;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#65306;1&#65289;&#23454;&#39564;&#20013;&#30340;&#25152;&#26377;&#25552;&#31034;&#37117;&#34920;&#29616;&#20986;&#19981;&#21487;&#24573;&#35270;&#30340;&#20559;&#35265;&#65292;&#22522;&#20110;&#26799;&#24230;&#30340;&#25552;&#31034;&#22914;AutoPrompt&#21644;OptiPrompt&#26174;&#31034;&#20986;&#26356;&#39640;&#27700;&#24179;&#30340;&#20559;&#35265;&#65307;2&#65289;&#25552;&#31034;&#20559;&#35265;&#21487;&#20197;&#36890;&#36807;&#36807;&#24230;&#25311;&#21512;&#27979;&#35797;&#25968;&#25454;&#38598;&#19981;&#21512;&#29702;&#22320;&#25918;&#22823;&#22522;&#20934;&#27979;&#35797;&#30340;&#20934;&#30830;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#31867;&#20284;LAMA&#36825;&#26679;&#30340;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#19978;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#34920;&#31034;&#30340;&#26041;&#27861;&#26469;&#20943;&#36731;&#25552;&#31034;&#20559;&#35265;&#65292;&#22312;&#25512;&#26029;&#26102;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#20165;&#25552;&#31034;&#26597;&#35810;&#26469;&#20272;&#35745;&#26377;&#20559;&#24046;&#30340;&#34920;&#31034;&#65292;&#28982;&#21518;&#20174;&#20013;&#21024;&#38500;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09963v1 Announce Type: cross  Abstract: Recent research shows that pre-trained language models (PLMs) suffer from "prompt bias" in factual knowledge extraction, i.e., prompts tend to introduce biases toward specific labels. However, the extent and impact of prompt bias within the model remain underexplored. In response, this paper quantifies the bias with various types of prompts and assesses their impact on different benchmarks. We show that: 1) all prompts in the experiments exhibit non-negligible bias, with gradient-based prompts like AutoPrompt and OptiPrompt displaying significantly higher levels of bias; 2) prompt bias can amplify benchmark accuracy unreasonably by overfitting the test datasets, especially on imbalanced datasets like LAMA. Based on these findings, we propose a representation-based approach to mitigate the prompt bias during inference time. Specifically, we first estimate the biased representation using prompt-only querying, and then remove it from the 
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#29983;&#25104;&#24335;&#29992;&#25143;&#27169;&#25311;&#22120;&#22312;&#23545;&#35805;&#25512;&#33616;&#20013;&#23637;&#29616;&#20986;&#28508;&#21147;&#65292;&#26032;&#30340;&#21327;&#35758;&#36890;&#36807;&#20116;&#20010;&#20219;&#21153;&#35780;&#20272;&#20102;&#35821;&#35328;&#27169;&#22411;&#27169;&#25311;&#20154;&#31867;&#34892;&#20026;&#30340;&#20934;&#30830;&#31243;&#24230;&#65292;&#25581;&#31034;&#20102;&#27169;&#22411;&#19982;&#20154;&#31867;&#34892;&#20026;&#30340;&#20559;&#24046;&#65292;&#24182;&#25552;&#20986;&#20102;&#22914;&#20309;&#36890;&#36807;&#27169;&#22411;&#36873;&#25321;&#21644;&#25552;&#31034;&#31574;&#30053;&#20943;&#23569;&#36825;&#20123;&#20559;&#24046;&#12290;</title><link>https://arxiv.org/abs/2403.09738</link><description>&lt;p&gt;
&#35780;&#20272;&#22823;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#23545;&#35805;&#25512;&#33616;&#20013;&#29983;&#25104;&#29992;&#25143;&#27169;&#25311;&#22120;
&lt;/p&gt;
&lt;p&gt;
Evaluating Large Language Models as Generative User Simulators for Conversational Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09738
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#29983;&#25104;&#24335;&#29992;&#25143;&#27169;&#25311;&#22120;&#22312;&#23545;&#35805;&#25512;&#33616;&#20013;&#23637;&#29616;&#20986;&#28508;&#21147;&#65292;&#26032;&#30340;&#21327;&#35758;&#36890;&#36807;&#20116;&#20010;&#20219;&#21153;&#35780;&#20272;&#20102;&#35821;&#35328;&#27169;&#22411;&#27169;&#25311;&#20154;&#31867;&#34892;&#20026;&#30340;&#20934;&#30830;&#31243;&#24230;&#65292;&#25581;&#31034;&#20102;&#27169;&#22411;&#19982;&#20154;&#31867;&#34892;&#20026;&#30340;&#20559;&#24046;&#65292;&#24182;&#25552;&#20986;&#20102;&#22914;&#20309;&#36890;&#36807;&#27169;&#22411;&#36873;&#25321;&#21644;&#25552;&#31034;&#31574;&#30053;&#20943;&#23569;&#36825;&#20123;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#25104;&#29992;&#25143;&#26159;&#23545;&#35805;&#25512;&#33616;&#31995;&#32479;&#35780;&#20272;&#20013;&#25104;&#26412;&#25928;&#30410;&#36739;&#39640;&#30340;&#30495;&#23454;&#29992;&#25143;&#20195;&#29702;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#22312;&#27169;&#25311;&#31867;&#20284;&#20154;&#31867;&#34892;&#20026;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#36825;&#24341;&#21457;&#20102;&#23427;&#20204;&#33021;&#21542;&#20195;&#34920;&#22810;&#26679;&#21270;&#29992;&#25143;&#32676;&#20307;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#21327;&#35758;&#65292;&#29992;&#20110;&#34913;&#37327;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#20934;&#30830;&#27169;&#25311;&#23545;&#35805;&#25512;&#33616;&#20013;&#20154;&#31867;&#34892;&#20026;&#30340;&#31243;&#24230;&#12290;&#35813;&#21327;&#35758;&#30001;&#20116;&#20010;&#20219;&#21153;&#32452;&#25104;&#65292;&#27599;&#20010;&#20219;&#21153;&#26088;&#22312;&#35780;&#20272;&#21512;&#25104;&#29992;&#25143;&#24212;&#35813;&#34920;&#29616;&#20986;&#30340;&#20851;&#38190;&#29305;&#24615;&#65306;&#36873;&#25321;&#35201;&#35848;&#35770;&#30340;&#29289;&#21697;&#65292;&#34920;&#36798;&#20108;&#36827;&#21046;&#20559;&#22909;&#65292;&#34920;&#36798;&#24320;&#25918;&#24335;&#20559;&#22909;&#65292;&#35831;&#27714;&#25512;&#33616;&#20197;&#21450;&#25552;&#20379;&#21453;&#39304;&#12290;&#36890;&#36807;&#23545;&#22522;&#20934;&#27169;&#25311;&#22120;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#20219;&#21153;&#26377;&#25928;&#22320;&#25581;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#34892;&#20026;&#30340;&#20559;&#24046;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;&#22914;&#20309;&#36890;&#36807;&#27169;&#22411;&#36873;&#25321;&#21644;&#25552;&#31034;&#31574;&#30053;&#20943;&#23569;&#36825;&#20123;&#20559;&#24046;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09738v1 Announce Type: cross  Abstract: Synthetic users are cost-effective proxies for real users in the evaluation of conversational recommender systems. Large language models show promise in simulating human-like behavior, raising the question of their ability to represent a diverse population of users. We introduce a new protocol to measure the degree to which language models can accurately emulate human behavior in conversational recommendation. This protocol is comprised of five tasks, each designed to evaluate a key property that a synthetic user should exhibit: choosing which items to talk about, expressing binary preferences, expressing open-ended preferences, requesting recommendations, and giving feedback. Through evaluation of baseline simulators, we demonstrate these tasks effectively reveal deviations of language models from human behavior, and offer insights on how to reduce the deviations with model selection and prompting strategies.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;AI&#65288;ChatGPT-4&#65289;&#22914;&#20309;&#36731;&#26494;&#21046;&#36896;&#20196;&#20154;&#20449;&#26381;&#20294;&#23436;&#20840;&#34394;&#26500;&#30340;&#31185;&#23398;&#25968;&#25454;&#65292;&#20197;&#21046;&#36896;&#20986;&#19968;&#20010;&#23436;&#20840;&#34394;&#26500;&#30340;&#21307;&#23398;&#26696;&#20363;&#26469;&#35686;&#31034;&#21307;&#23398;&#35823;&#20449;&#24687;&#30340;&#21361;&#23475;&#12290;</title><link>https://arxiv.org/abs/2403.09674</link><description>&lt;p&gt;
&#36991;&#24320;&#29983;&#25104;&#30340;&#26367;&#20195;&#20107;&#23454;&#30340;&#21361;&#38505;&#65306;&#20197;ChatGPT-4&#21046;&#36896;&#30340;&#937;&#21464;&#31181;&#26696;&#20363;&#20316;&#20026;&#21307;&#23398;&#35823;&#20449;&#24687;&#30340;&#35686;&#31034;&#25925;&#20107;
&lt;/p&gt;
&lt;p&gt;
Navigating the Peril of Generated Alternative Facts: A ChatGPT-4 Fabricated Omega Variant Case as a Cautionary Tale in Medical Misinformation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09674
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;AI&#65288;ChatGPT-4&#65289;&#22914;&#20309;&#36731;&#26494;&#21046;&#36896;&#20196;&#20154;&#20449;&#26381;&#20294;&#23436;&#20840;&#34394;&#26500;&#30340;&#31185;&#23398;&#25968;&#25454;&#65292;&#20197;&#21046;&#36896;&#20986;&#19968;&#20010;&#23436;&#20840;&#34394;&#26500;&#30340;&#21307;&#23398;&#26696;&#20363;&#26469;&#35686;&#31034;&#21307;&#23398;&#35823;&#20449;&#24687;&#30340;&#21361;&#23475;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#19982;&#21307;&#23398;&#30740;&#31350;&#20132;&#32455;&#30340;&#26102;&#20195;&#65292;&#30495;&#30456;&#30340;&#25259;&#38706;&#21464;&#24471;&#26085;&#30410;&#22797;&#26434;&#12290;&#26412;&#30740;&#31350;&#34920;&#38754;&#19978;&#23457;&#26597;&#20102;&#19968;&#31181;&#25152;&#35859;&#30340;&#26032;&#22411;SARS-CoV-2&#21464;&#31181;&#65292;&#34987;&#31216;&#20026;&#937;&#21464;&#31181;&#65292;&#23637;&#31034;&#22312;S&#22522;&#22240;&#21306;&#22495;&#20013;&#26377;31&#20010;&#29420;&#29305;&#31361;&#21464;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#25925;&#20107;&#30340;&#30495;&#27491;&#28508;&#21488;&#35789;&#26159;&#23637;&#31034;&#20102;AI&#65288;&#20855;&#20307;&#26469;&#35828;&#26159;ChatGPT-4&#65289;&#21487;&#20197;&#22914;&#20309;&#36731;&#26494;&#22320;&#21046;&#36896;&#20196;&#20154;&#20449;&#26381;&#20294;&#23436;&#20840;&#34394;&#26500;&#30340;&#31185;&#23398;&#25968;&#25454;&#12290;&#25152;&#35859;&#30340;&#937;&#21464;&#31181;&#22312;&#19968;&#20010;&#23436;&#20840;&#25509;&#31181;&#30123;&#33495;&#12289;&#20043;&#21069;&#24863;&#26579;&#36807;&#30340;35&#23681;&#30007;&#24615;&#20013;&#34987;&#37492;&#23450;&#20986;&#29616;&#20005;&#37325;COVID-19&#30151;&#29366;&#12290;&#36890;&#36807;&#35814;&#32454;&#30340;&#65292;&#23613;&#31649;&#26159;&#34394;&#25311;&#30340;&#65292;&#22522;&#22240;&#32452;&#20998;&#26512;&#21644;&#25509;&#35302;&#32773;&#36861;&#36394;&#65292;&#26412;&#30740;&#31350;&#27169;&#25311;&#20102;&#30495;&#23454;&#30149;&#20363;&#25253;&#21578;&#30340;&#20005;&#35880;&#26041;&#27861;&#65292;&#20174;&#32780;&#20026;&#19968;&#20010;&#24341;&#20154;&#20837;&#32988;&#20294;&#23436;&#20840;&#26500;&#36896;&#30340;&#21465;&#36848;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;&#25972;&#20010;&#30149;&#20363;&#30740;&#31350;&#26159;&#30001;OpenAI&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;ChatGPT-4&#29983;&#25104;&#30340;&#937;&#21464;&#31181;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09674v1 Announce Type: new  Abstract: In an era where artificial intelligence (AI) intertwines with medical research, the delineation of truth becomes increasingly complex. This study ostensibly examines a purported novel SARS-CoV-2 variant, dubbed the Omega variant, showcasing 31 unique mutations in the S gene region. However, the real undercurrent of this narrative is a demonstration of the ease with which AI, specifically ChatGPT-4, can fabricate convincing yet entirely fictional scientific data. The so-called Omega variant was identified in a fully vaccinated, previously infected 35-year-old male presenting with severe COVID-19 symptoms. Through a detailed, albeit artificial, genomic analysis and contact tracing, this study mirrors the rigorous methodology of genuine case reports, thereby setting the stage for a compelling but entirely constructed narrative. The entire case study was generated by ChatGPT-4, a large language model by OpenAI. The fabricated Omega variant f
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#29992;&#20110;&#26032;&#38395;&#25512;&#33616;&#30340;&#31532;&#19968;&#20010;&#32511;&#33394;AI&#22522;&#20934;&#27979;&#35797;&#26694;&#26550;GreenRec&#65292;&#24182;&#24341;&#20837;&#35780;&#20272;&#25512;&#33616;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#26435;&#34913;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;OLEO&#33539;&#24335;&#22312;&#21487;&#25345;&#32493;&#24615;&#26041;&#38754;&#21462;&#24471;&#31454;&#20105;&#20934;&#30830;&#24615;&#19988;&#25552;&#20379;&#39640;&#36798;2992%&#30340;&#21487;&#25345;&#32493;&#24615;&#25913;&#36827;</title><link>https://arxiv.org/abs/2403.04736</link><description>&lt;p&gt;
&#22312;&#32511;&#33394;AI&#26102;&#20195;&#23545;&#26032;&#38395;&#25512;&#33616;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Benchmarking News Recommendation in the Era of Green AI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04736
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#29992;&#20110;&#26032;&#38395;&#25512;&#33616;&#30340;&#31532;&#19968;&#20010;&#32511;&#33394;AI&#22522;&#20934;&#27979;&#35797;&#26694;&#26550;GreenRec&#65292;&#24182;&#24341;&#20837;&#35780;&#20272;&#25512;&#33616;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#26435;&#34913;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;OLEO&#33539;&#24335;&#22312;&#21487;&#25345;&#32493;&#24615;&#26041;&#38754;&#21462;&#24471;&#31454;&#20105;&#20934;&#30830;&#24615;&#19988;&#25552;&#20379;&#39640;&#36798;2992%&#30340;&#21487;&#25345;&#32493;&#24615;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#26032;&#38395;&#25512;&#33616;&#31995;&#32479;&#22312;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#22791;&#21463;&#20851;&#27880;&#65292;&#24378;&#35843;&#20102;&#38656;&#35201;&#19968;&#20010;&#26631;&#20934;&#21270;&#22522;&#20934;&#26469;&#35780;&#20272;&#21644;&#27604;&#36739;&#36825;&#20123;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;&#21516;&#26102;&#65292;&#32511;&#33394;&#20154;&#24037;&#26234;&#33021;&#20513;&#23548;&#20943;&#23569;&#26426;&#22120;&#23398;&#20064;&#30340;&#33021;&#32791;&#21644;&#29615;&#22659;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;&#29992;&#20110;&#26032;&#38395;&#25512;&#33616;&#30340;&#32511;&#33394;AI&#22522;&#20934;&#27979;&#35797;&#26694;&#26550;&#65292;&#31216;&#20026;GreenRec&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#25512;&#33616;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#20043;&#38388;&#26435;&#34913;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#28085;&#30422;30&#20010;&#22522;&#30784;&#27169;&#22411;&#21450;&#20854;&#21464;&#20307;&#65292;&#28085;&#30422;&#20102;&#20256;&#32479;&#30340;&#31471;&#21040;&#31471;&#35757;&#32451;&#33539;&#24335;&#20197;&#21450;&#25105;&#20204;&#25552;&#20986;&#30340;&#39640;&#25928;&#30340;&#20165;&#32534;&#30721;&#19968;&#27425;&#65288;OLEO&#65289;&#33539;&#24335;&#12290;&#36890;&#36807;&#28040;&#32791;2000&#20010;GPU&#23567;&#26102;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;OLEO&#33539;&#24335;&#22312;&#21487;&#25345;&#32493;&#24615;&#26041;&#38754;&#30456;&#36739;&#20110;&#26368;&#20808;&#36827;&#30340;&#31471;&#21040;&#31471;&#33539;&#24335;&#23454;&#29616;&#20102;&#20855;&#31454;&#20105;&#21147;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#25552;&#20379;&#39640;&#36798;2992%&#30340;&#21487;&#25345;&#32493;&#24615;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04736v1 Announce Type: new  Abstract: Over recent years, news recommender systems have gained significant attention in both academia and industry, emphasizing the need for a standardized benchmark to evaluate and compare the performance of these systems. Concurrently, Green AI advocates for reducing the energy consumption and environmental impact of machine learning. To address these concerns, we introduce the first Green AI benchmarking framework for news recommendation, known as GreenRec, and propose a metric for assessing the tradeoff between recommendation accuracy and efficiency. Our benchmark encompasses 30 base models and their variants, covering traditional end-to-end training paradigms as well as our proposed efficient only-encode-once (OLEO) paradigm. Through experiments consuming 2000 GPU hours, we observe that the OLEO paradigm achieves competitive accuracy compared to state-of-the-art end-to-end paradigms and delivers up to a 2992\% improvement in sustainability
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22810;&#37325;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#26377;&#25928;&#22320;&#32467;&#21512;&#20102;&#22522;&#20110;&#24207;&#21015;&#21644;&#22522;&#20110;&#22270;&#30340;&#25512;&#33616;&#26041;&#27861;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#25512;&#33616;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.00895</link><description>&lt;p&gt;
&#31934;&#30830;&#25512;&#33616;&#30340;&#31471;&#21040;&#31471;&#22270;-&#24207;&#21015;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
End-to-end Graph-Sequential Representation Learning for Accurate Recommendations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00895
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22810;&#37325;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#26377;&#25928;&#22320;&#32467;&#21512;&#20102;&#22522;&#20110;&#24207;&#21015;&#21644;&#22522;&#20110;&#22270;&#30340;&#25512;&#33616;&#26041;&#27861;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#25512;&#33616;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#25512;&#33616;&#31995;&#32479;&#30340;&#35768;&#22810;&#26032;&#36827;&#23637;&#38598;&#20013;&#22312;&#24320;&#21457;&#22522;&#20110;&#24207;&#21015;&#21644;&#22522;&#20110;&#22270;&#30340;&#26041;&#27861;&#19978;&#12290;&#36825;&#20004;&#31181;&#26041;&#27861;&#22312;&#24314;&#27169;&#34892;&#20026;&#25968;&#25454;&#20013;&#30340;&#22797;&#26434;&#20851;&#31995;&#26041;&#38754;&#37117;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#65292;&#20174;&#32780;&#22312;&#20010;&#24615;&#21270;&#25490;&#21517;&#21644;&#19979;&#19968;&#20010;&#25512;&#33616;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26377;&#30410;&#30340;&#25104;&#26524;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#33391;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20174;&#25968;&#25454;&#20013;&#25429;&#25417;&#21040;&#30340;&#20449;&#21495;&#25130;&#28982;&#19981;&#21516;&#12290;&#21069;&#32773;&#30452;&#25509;&#36890;&#36807;&#19982;&#26368;&#36817;&#29289;&#21697;&#30340;&#26377;&#24207;&#20132;&#20114;&#26469;&#34920;&#31034;&#29992;&#25143;&#65292;&#32780;&#21518;&#32773;&#26088;&#22312;&#25429;&#25417;&#20132;&#20114;&#22270;&#20013;&#30340;&#38388;&#25509;&#20381;&#36182;&#20851;&#31995;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22810;&#37325;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#36825;&#20004;&#31181;&#33539;&#24335;&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;&#21033;&#29992;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#30456;&#20114;&#35757;&#32451;&#24207;&#21015;&#21644;&#22270;&#32452;&#20214;&#26174;&#33879;&#25913;&#21892;&#20102;&#25512;&#33616;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00895v1 Announce Type: cross  Abstract: Many recent advancements in recommender systems have focused on developing sequence-based and graph-based approaches. Both approaches proved useful in modeling intricate relationships within behavioral data, leading to promising outcomes in personalized ranking and next-item recommendation tasks while maintaining good scalability. However, they capture very different signals from data. While the former approach represents users directly through ordered interactions with recent items, the latter one aims to capture indirect dependencies across the interactions graph. This paper presents a novel multi-representational learning framework that exploits the synergies between these two paradigms. Our empirical evaluation on several datasets demonstrates that mutual training of sequential and graph components with the proposed framework significantly improves recommendations performance.
&lt;/p&gt;</description></item><item><title>&#22312;&#39044;&#31639;&#38480;&#21046;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#20248;&#21270;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#20248;&#21270;&#20256;&#36882;&#21457;&#29616;&#34892;&#20026;&#29992;&#25143;&#32454;&#20998;&#12290;</title><link>https://arxiv.org/abs/2402.03388</link><description>&lt;p&gt;
&#22312;&#39044;&#31639;&#38480;&#21046;&#19979;&#34892;&#20026;&#29992;&#25143;&#20998;&#21106;&#20013;&#30340;&#20248;&#21270;&#20256;&#36882;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Delivery Optimized Discovery in Behavioral User Segmentation under Budget Constrain
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03388
&lt;/p&gt;
&lt;p&gt;
&#22312;&#39044;&#31639;&#38480;&#21046;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#20248;&#21270;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#20248;&#21270;&#20256;&#36882;&#21457;&#29616;&#34892;&#20026;&#29992;&#25143;&#32454;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#25143;&#22312;&#32447;&#34892;&#20026;&#36275;&#36857;&#21487;&#20197;&#20351;&#20844;&#21496;&#21457;&#29616;&#22522;&#20110;&#34892;&#20026;&#30340;&#29992;&#25143;&#32454;&#20998;&#65292;&#24182;&#21521;&#29992;&#25143;&#21457;&#36865;&#29305;&#23450;&#32454;&#20998;&#30340;&#20449;&#24687;&#12290;&#22312;&#21457;&#29616;&#32454;&#20998;&#20043;&#21518;&#65292;&#36890;&#36807;&#20687;Facebook&#21644;Google&#36825;&#26679;&#30340;&#39318;&#36873;&#23186;&#20307;&#28192;&#36947;&#21521;&#29992;&#25143;&#21457;&#36865;&#20449;&#24687;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#21482;&#26377;&#37096;&#20998;&#34892;&#20026;&#32454;&#20998;&#20013;&#30340;&#29992;&#25143;&#22312;&#23186;&#20307;&#19978;&#25214;&#21040;&#21305;&#37197;&#65292;&#24182;&#19988;&#21482;&#26377;&#20854;&#20013;&#19968;&#23567;&#37096;&#20998;&#30475;&#21040;&#28040;&#24687;&#65288;&#26333;&#20809;&#65289;&#12290;&#21363;&#20351;&#39640;&#36136;&#37327;&#30340;&#21457;&#29616;&#20063;&#20250;&#22312;&#20256;&#36882;&#22833;&#36133;&#26102;&#21464;&#24471;&#26080;&#29992;&#12290;&#35768;&#22810;&#22797;&#26434;&#30340;&#31639;&#27861;&#29992;&#20110;&#21457;&#29616;&#34892;&#20026;&#32454;&#20998;&#65292;&#28982;&#32780;&#36825;&#20123;&#31639;&#27861;&#24573;&#30053;&#20102;&#20256;&#36882;&#32452;&#20214;&#12290;&#38382;&#39064;&#21464;&#24471;&#22797;&#26434;&#26159;&#22240;&#20026;&#65288;i&#65289;&#21457;&#29616;&#26159;&#22312;&#20844;&#21496;&#25968;&#25454;&#65288;&#20363;&#22914;&#29992;&#25143;&#28857;&#20987;&#65289;&#30340;&#34892;&#20026;&#25968;&#25454;&#31354;&#38388;&#20013;&#36827;&#34892;&#30340;&#65292;&#32780;&#20256;&#36882;&#21017;&#26159;&#22522;&#20110;&#23186;&#20307;&#23450;&#20041;&#30340;&#38745;&#24577;&#25968;&#25454;&#31354;&#38388;&#65288;&#20363;&#22914;&#22320;&#29702;&#20301;&#32622;&#65292;&#24180;&#40836;&#65289;&#36827;&#34892;&#30340;&#65307;&#65288;ii&#65289;&#20844;&#21496;&#22312;&#39044;&#31639;&#38480;&#21046;&#19979;&#36816;&#20316;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#20248;&#21270;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#39044;&#31639;&#38480;&#21046;&#19979;&#20248;&#21270;&#20256;&#36882;&#21457;&#29616;&#34892;&#20026;&#29992;&#25143;&#32454;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Users' behavioral footprints online enable firms to discover behavior-based user segments (or, segments) and deliver segment specific messages to users. Following the discovery of segments, delivery of messages to users through preferred media channels like Facebook and Google can be challenging, as only a portion of users in a behavior segment find match in a medium, and only a fraction of those matched actually see the message (exposure). Even high quality discovery becomes futile when delivery fails. Many sophisticated algorithms exist for discovering behavioral segments; however, these ignore the delivery component. The problem is compounded because (i) the discovery is performed on the behavior data space in firms' data (e.g., user clicks), while the delivery is predicated on the static data space (e.g., geo, age) as defined by media; and (ii) firms work under budget constraint. We introduce a stochastic optimization based algorithm for delivery optimized discovery of behavioral u
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#31070;&#32463;&#27169;&#24335;&#20851;&#32852;&#22120;&#65288;NPA&#65289;&#30340;&#28145;&#24230;&#21830;&#21697;&#20851;&#32852;&#25366;&#25496;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#26126;&#30830;&#22320;&#24314;&#27169;&#36141;&#29289;&#36807;&#31243;&#20013;&#30340;&#22797;&#26434;&#29992;&#25143;&#34892;&#20026;&#65292;&#24182;&#36890;&#36807;&#27880;&#24847;&#21147;&#39537;&#21160;&#30340;&#26597;&#25214;&#26469;&#35782;&#21035;&#29992;&#25143;&#30340;&#36141;&#29289;&#24847;&#22270;&#12290;</title><link>http://arxiv.org/abs/2401.16433</link><description>&lt;p&gt;
&#36890;&#36807;&#31070;&#32463;&#27169;&#24335;&#20851;&#32852;&#22120;&#36827;&#34892;&#31726;&#20869;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Within-basket Recommendation via Neural Pattern Associator. (arXiv:2401.16433v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16433
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#31070;&#32463;&#27169;&#24335;&#20851;&#32852;&#22120;&#65288;NPA&#65289;&#30340;&#28145;&#24230;&#21830;&#21697;&#20851;&#32852;&#25366;&#25496;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#26126;&#30830;&#22320;&#24314;&#27169;&#36141;&#29289;&#36807;&#31243;&#20013;&#30340;&#22797;&#26434;&#29992;&#25143;&#34892;&#20026;&#65292;&#24182;&#36890;&#36807;&#27880;&#24847;&#21147;&#39537;&#21160;&#30340;&#26597;&#25214;&#26469;&#35782;&#21035;&#29992;&#25143;&#30340;&#36141;&#29289;&#24847;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31726;&#20869;&#25512;&#33616;&#65288;WBR&#65289;&#26159;&#25351;&#22312;&#36141;&#29289;&#36807;&#31243;&#20013;&#20026;&#20102;&#23436;&#25104;&#19968;&#20010;&#38750;&#31354;&#36141;&#29289;&#31726;&#32780;&#25512;&#33616;&#21830;&#21697;&#30340;&#20219;&#21153;&#12290;&#23613;&#31649;&#36825;&#20010;&#39046;&#22495;&#30340;&#26368;&#26032;&#21019;&#26032;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#20294;&#23427;&#20204;&#24120;&#24120;&#24573;&#35270;&#20102;&#23454;&#38469;&#29992;&#25143;&#34892;&#20026;&#30340;&#22797;&#26434;&#24615;&#65292;&#27604;&#22914;1&#65289;&#22810;&#20010;&#36141;&#29289;&#24847;&#22270;&#30340;&#20849;&#23384;&#65292;2&#65289;&#36825;&#20123;&#24847;&#22270;&#30340;&#22810;&#31890;&#24230;&#21644;3&#65289;&#36141;&#29289;&#36807;&#31243;&#20013;&#30340;&#20132;&#32455;&#34892;&#20026;&#65288;&#20999;&#25442;&#24847;&#22270;&#65289;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#31070;&#32463;&#27169;&#24335;&#20851;&#32852;&#22120;&#65288;NPA&#65289;&#30340;&#28145;&#24230;&#21830;&#21697;&#20851;&#32852;&#25366;&#25496;&#27169;&#22411;&#65292;&#26126;&#30830;&#22320;&#24314;&#27169;&#20102;&#19978;&#36848;&#22240;&#32032;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#21463;&#21040;&#21521;&#37327;&#37327;&#21270;&#30340;&#21551;&#21457;&#65292;NPA&#27169;&#22411;&#23398;&#20064;&#23558;&#24120;&#35265;&#30340;&#29992;&#25143;&#24847;&#22270;&#65288;&#25110;&#21830;&#21697;&#32452;&#21512;&#27169;&#24335;&#65289;&#32534;&#30721;&#20026;&#37327;&#21270;&#34920;&#31034;&#65288;&#20063;&#31216;&#20026;&#30721;&#26412;&#65289;&#65292;&#36825;&#20801;&#35768;&#22312;&#25512;&#29702;&#38454;&#27573;&#36890;&#36807;&#27880;&#24847;&#21147;&#39537;&#21160;&#30340;&#26597;&#25214;&#26469;&#35782;&#21035;&#29992;&#25143;&#30340;&#36141;&#29289;&#24847;&#22270;&#12290;&#36825;&#26679;&#20135;&#29983;&#30340;&#25512;&#33616;&#32467;&#26524;&#36830;&#36143;&#19988;&#33258;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Within-basket recommendation (WBR) refers to the task of recommending items to the end of completing a non-empty shopping basket during a shopping session. While the latest innovations in this space demonstrate remarkable performance improvement on benchmark datasets, they often overlook the complexity of user behaviors in practice, such as 1) co-existence of multiple shopping intentions, 2) multi-granularity of such intentions, and 3) interleaving behavior (switching intentions) in a shopping session. This paper presents Neural Pattern Associator (NPA), a deep item-association-mining model that explicitly models the aforementioned factors. Specifically, inspired by vector quantization, the NPA model learns to encode common user intentions (or item-combination patterns) as quantized representations (a.k.a. codebook), which permits identification of users's shopping intentions via attention-driven lookup during the reasoning phase. This yields coherent and self-interpretable recommendat
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20010;&#20154;&#30693;&#35782;&#22270;&#35889;&#65288;PKG&#65289;&#30340;&#29983;&#24577;&#31995;&#32479;&#65292;PKG&#30340;&#20027;&#35201;&#30446;&#30340;&#26159;&#25968;&#25454;&#31649;&#29702;&#21644;&#20010;&#24615;&#21270;&#26381;&#21153;&#12290;&#35201;&#35299;&#38145;PKG&#30340;&#20840;&#37096;&#28508;&#21147;&#65292;&#38656;&#35201;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;PKG&#30340;&#32508;&#21512;&#35270;&#22270;&#12290;</title><link>http://arxiv.org/abs/2304.09572</link><description>&lt;p&gt;
&#20010;&#20154;&#30693;&#35782;&#22270;&#35889;&#29983;&#24577;&#31995;&#32479;&#65306;&#35843;&#26597;&#19982;&#30740;&#31350;&#36335;&#32447;&#22270;
&lt;/p&gt;
&lt;p&gt;
An Ecosystem for Personal Knowledge Graphs: A Survey and Research Roadmap. (arXiv:2304.09572v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09572
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20010;&#20154;&#30693;&#35782;&#22270;&#35889;&#65288;PKG&#65289;&#30340;&#29983;&#24577;&#31995;&#32479;&#65292;PKG&#30340;&#20027;&#35201;&#30446;&#30340;&#26159;&#25968;&#25454;&#31649;&#29702;&#21644;&#20010;&#24615;&#21270;&#26381;&#21153;&#12290;&#35201;&#35299;&#38145;PKG&#30340;&#20840;&#37096;&#28508;&#21147;&#65292;&#38656;&#35201;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;PKG&#30340;&#32508;&#21512;&#35270;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20010;&#20154;&#30693;&#35782;&#22270;&#35889;&#65288;PKG&#65289;&#30340;&#29983;&#24577;&#31995;&#32479;&#65292;&#36890;&#24120;&#23450;&#20041;&#20026;&#26377;&#20851;&#20010;&#20154;&#30456;&#20851;&#23454;&#20307;&#12289;&#20854;&#23646;&#24615;&#21644;&#23427;&#20204;&#20043;&#38388;&#20851;&#31995;&#30340;&#32467;&#26500;&#21270;&#20449;&#24687;&#36164;&#28304;&#12290;PKG&#26159;&#23433;&#20840;&#12289;&#31934;&#23494;&#30340;&#20010;&#20154;&#25968;&#25454;&#31649;&#29702;&#21644;&#20010;&#24615;&#21270;&#26381;&#21153;&#30340;&#20851;&#38190;&#25903;&#25345;&#12290;&#28982;&#32780;&#65292;&#22312;PKG&#33021;&#22815;&#24191;&#27867;&#24212;&#29992;&#20043;&#21069;&#38656;&#35201;&#35299;&#20915;&#19968;&#20123;&#25361;&#25112;&#12290;&#20854;&#20013;&#19968;&#20010;&#22522;&#26412;&#25361;&#25112;&#26159;&#20851;&#20110;PKG&#30340;&#23450;&#20041;&#65292;&#22240;&#20026;&#26415;&#35821;&#26377;&#22810;&#31181;&#35299;&#37322;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#24049;&#30340;PKG&#23450;&#20041;&#65292;&#24378;&#35843;&#20102;&#65288;1&#65289;&#21333;&#20010;&#20010;&#20307;&#25317;&#26377;&#25968;&#25454;&#21644;&#65288;2&#65289;&#25552;&#20379;&#20010;&#24615;&#21270;&#26381;&#21153;&#20316;&#20026;&#20027;&#35201;&#30446;&#30340;&#30340;&#26041;&#38754;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#30340;PKG&#35270;&#22270;&#65292;&#38656;&#35201;&#35299;&#38145;&#23427;&#20204;&#30340;&#20840;&#37096;&#28508;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#20854;&#20013;PKG&#26159;&#26356;&#22823;&#30340;&#29983;&#24577;&#31995;&#32479;&#30340;&#19968;&#37096;&#20998;&#65292;&#20855;&#26377;&#26126;&#30830;&#30340;&#19982;&#25968;&#25454;&#26381;&#21153;&#21644;&#25968;&#25454;&#28304;&#30340;&#25509;&#21475;&#12290;&#26412;&#25991;&#36824;&#23637;&#31034;&#20102;&#23545;&#24403;&#21069;PKG&#30740;&#31350;&#30340;&#20840;&#38754;&#35843;&#26597;&#21644;&#30740;&#31350;&#36335;&#32447;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents an ecosystem for personal knowledge graphs (PKG), commonly defined as resources of structured information about entities related to an individual, their attributes, and the relations between them. PKGs are a key enabler of secure and sophisticated personal data management and personalized services. However, there are challenges that need to be addressed before PKGs can achieve widespread adoption. One of the fundamental challenges is the very definition of what constitutes a PKG, as there are multiple interpretations of the term. We propose our own definition of a PKG, emphasizing the aspects of (1) data ownership by a single individual and (2) the delivery of personalized services as the primary purpose. We further argue that a holistic view of PKGs is needed to unlock their full potential, and propose a unified framework for PKGs, where the PKG is a part of a larger ecosystem with clear interfaces towards data services and data sources. A comprehensive survey and 
&lt;/p&gt;</description></item></channel></rss>