<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35270;&#35273;-&#35821;&#35328;&#26694;&#26550;&#65292;&#22312;&#22823;&#35268;&#27169;&#22270;&#20687;&#25991;&#26412;&#23545;&#19978;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;&#65292;&#24182;&#22312;&#24191;&#21578;&#21830;&#19994;&#25968;&#25454;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#36890;&#36807;&#22810;&#30446;&#26631;&#23398;&#20064;&#32479;&#19968;&#30456;&#20851;&#24615;&#24314;&#27169;&#21644;&#26816;&#32034;&#12290;&#35813;&#26694;&#26550;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#21160;&#24577;&#22270;&#20687;&#24191;&#21578;(DIA)&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.14112</link><description>&lt;p&gt;
&#36890;&#36807;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#22686;&#24378;&#21160;&#24577;&#22270;&#20687;&#24191;&#21578;
&lt;/p&gt;
&lt;p&gt;
Enhancing Dynamic Image Advertising with Vision-Language Pre-training. (arXiv:2306.14112v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14112
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35270;&#35273;-&#35821;&#35328;&#26694;&#26550;&#65292;&#22312;&#22823;&#35268;&#27169;&#22270;&#20687;&#25991;&#26412;&#23545;&#19978;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;&#65292;&#24182;&#22312;&#24191;&#21578;&#21830;&#19994;&#25968;&#25454;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#36890;&#36807;&#22810;&#30446;&#26631;&#23398;&#20064;&#32479;&#19968;&#30456;&#20851;&#24615;&#24314;&#27169;&#21644;&#26816;&#32034;&#12290;&#35813;&#26694;&#26550;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#21160;&#24577;&#22270;&#20687;&#24191;&#21578;(DIA)&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#23186;&#20307;&#26102;&#20195;&#65292;&#22270;&#20687;&#26159;&#25628;&#32034;&#24191;&#21578;&#20013;&#19968;&#31181;&#26377;&#25928;&#30340;&#23186;&#20171;&#12290;&#21160;&#24577;&#22270;&#20687;&#24191;&#21578;(DIA)&#26159;&#19968;&#31181;&#31995;&#32479;&#65292;&#23427;&#23558;&#26597;&#35810;&#19982;&#24191;&#21578;&#22270;&#20687;&#21305;&#37197;&#24182;&#29983;&#25104;&#22810;&#27169;&#24335;&#24191;&#21578;&#65292;&#20174;&#32780;&#25552;&#39640;&#29992;&#25143;&#20307;&#39564;&#21644;&#24191;&#21578;&#25910;&#30410;&#12290;DIA&#30340;&#26680;&#24515;&#26159;&#26597;&#35810;-&#22270;&#20687;&#21305;&#37197;&#27169;&#22359;&#65292;&#25191;&#34892;&#24191;&#21578;&#22270;&#20687;&#26816;&#32034;&#21644;&#30456;&#20851;&#24615;&#24314;&#27169;&#12290;&#24403;&#21069;&#30340;&#26597;&#35810;-&#22270;&#20687;&#21305;&#37197;&#23384;&#22312;&#25968;&#25454;&#26377;&#38480;&#21644;&#19981;&#19968;&#33268;&#20197;&#21450;&#36328;&#27169;&#24577;&#20132;&#20114;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;&#32780;&#19988;&#65292;&#26816;&#32034;&#21644;&#30456;&#20851;&#24615;&#27169;&#22411;&#30340;&#20998;&#21035;&#20248;&#21270;&#24433;&#21709;&#20102;&#25972;&#20307;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35270;&#35273;-&#35821;&#35328;&#26694;&#26550;&#65292;&#30001;&#20004;&#37096;&#20998;&#32452;&#25104;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#22312;&#22823;&#35268;&#27169;&#22270;&#20687;&#25991;&#26412;&#23545;&#19978;&#35757;&#32451;&#19968;&#20010;&#22522;&#30784;&#27169;&#22411;&#65292;&#23398;&#20064;&#36890;&#29992;&#30340;&#22810;&#27169;&#24577;&#34920;&#31034;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;&#24191;&#21578;&#21830;&#19994;&#25968;&#25454;&#19978;&#23545;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#36890;&#36807;&#22810;&#30446;&#26631;&#23398;&#20064;&#32479;&#19968;&#30456;&#20851;&#24615;&#24314;&#27169;&#21644;&#26816;&#32034;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#24050;&#32463;&#22312;&#30334;&#24230;&#25628;&#32034;&#24191;&#21578;&#31995;&#32479;"Phoneix Nest"&#19978;&#23454;&#29616;&#12290;&#22312;&#32447;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the multimedia era, image is an effective medium in search advertising. Dynamic Image Advertising (DIA), a system that matches queries with ad images and generates multimodal ads, is introduced to improve user experience and ad revenue. The core of DIA is a query-image matching module performing ad image retrieval and relevance modeling. Current query-image matching suffers from limited and inconsistent data, and insufficient cross-modal interaction. Also, the separate optimization of retrieval and relevance models affects overall performance. To address this issue, we propose a vision-language framework consisting of two parts. First, we train a base model on large-scale image-text pairs to learn general multimodal representation. Then, we fine-tune the base model on advertising business data, unifying relevance modeling and retrieval through multi-objective learning. Our framework has been implemented in Baidu search advertising system "Phoneix Nest". Online evaluation shows that 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22810;&#27169;&#24577;&#39046;&#22495;&#33258;&#36866;&#24212;&#25216;&#26415;&#23454;&#29616;&#36328;&#39046;&#22495;&#25512;&#33616;&#31995;&#32479;&#65292;&#35299;&#20915;&#25968;&#25454;&#31232;&#30095;&#24615;&#38382;&#39064;&#65292;&#25552;&#21319;&#25512;&#33616;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.13887</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#27169;&#24577;&#39046;&#22495;&#33258;&#36866;&#24212;&#23454;&#29616;&#36328;&#39046;&#22495;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Cross-domain Recommender Systems via Multimodal Domain Adaptation. (arXiv:2306.13887v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13887
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22810;&#27169;&#24577;&#39046;&#22495;&#33258;&#36866;&#24212;&#25216;&#26415;&#23454;&#29616;&#36328;&#39046;&#22495;&#25512;&#33616;&#31995;&#32479;&#65292;&#35299;&#20915;&#25968;&#25454;&#31232;&#30095;&#24615;&#38382;&#39064;&#65292;&#25552;&#21319;&#25512;&#33616;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21327;&#21516;&#36807;&#28388;&#65288;CF&#65289;&#24050;&#25104;&#20026;&#25512;&#33616;&#31995;&#32479;&#26368;&#37325;&#35201;&#30340;&#23454;&#29616;&#31574;&#30053;&#20043;&#19968;&#12290;&#20851;&#38190;&#24605;&#24819;&#26159;&#21033;&#29992;&#20010;&#20154;&#20351;&#29992;&#27169;&#24335;&#29983;&#25104;&#20010;&#24615;&#21270;&#25512;&#33616;&#12290;&#23588;&#20854;&#26159;&#23545;&#20110;&#26032;&#25512;&#20986;&#30340;&#24179;&#21488;&#65292;CF&#25216;&#26415;&#24120;&#24120;&#38754;&#20020;&#25968;&#25454;&#31232;&#30095;&#24615;&#30340;&#38382;&#39064;&#65292;&#36825;&#26497;&#22823;&#22320;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;&#22312;&#35299;&#20915;&#25968;&#25454;&#31232;&#30095;&#24615;&#38382;&#39064;&#26041;&#38754;&#65292;&#25991;&#29486;&#20013;&#25552;&#20986;&#20102;&#20960;&#31181;&#26041;&#27861;&#65292;&#20854;&#20013;&#36328;&#39046;&#22495;&#21327;&#21516;&#36807;&#28388;&#65288;CDCF&#65289;&#22312;&#26368;&#36817;&#21463;&#21040;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#12290;&#20026;&#20102;&#34917;&#20607;&#30446;&#26631;&#39046;&#22495;&#20013;&#21487;&#29992;&#21453;&#39304;&#30340;&#19981;&#36275;&#65292;CDCF&#26041;&#27861;&#21033;&#29992;&#20854;&#20182;&#36741;&#21161;&#39046;&#22495;&#20013;&#30340;&#20449;&#24687;&#12290;&#22823;&#22810;&#25968;&#20256;&#32479;&#30340;CDCF&#26041;&#27861;&#30340;&#30446;&#26631;&#26159;&#22312;&#39046;&#22495;&#20043;&#38388;&#25214;&#21040;&#19968;&#32452;&#20849;&#21516;&#30340;&#23454;&#20307;&#65288;&#29992;&#25143;&#25110;&#39033;&#30446;&#65289;&#65292;&#28982;&#21518;&#23558;&#23427;&#20204;&#29992;&#20316;&#30693;&#35782;&#36716;&#31227;&#30340;&#26725;&#26753;&#12290;&#20294;&#26159;&#65292;&#22823;&#22810;&#25968;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#26159;&#20174;&#19981;&#21516;&#30340;&#39046;&#22495;&#25910;&#38598;&#30340;&#65292;&#36825;&#20351;&#24471;&#36328;&#39046;&#22495;&#21327;&#21516;&#36807;&#28388;&#26356;&#21152;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Collaborative Filtering (CF) has emerged as one of the most prominent implementation strategies for building recommender systems. The key idea is to exploit the usage patterns of individuals to generate personalized recommendations. CF techniques, especially for newly launched platforms, often face a critical issue known as the data sparsity problem, which greatly limits their performance. Several approaches have been proposed in the literature to tackle the problem of data sparsity, among which cross-domain collaborative filtering (CDCF) has gained significant attention in the recent past. In order to compensate for the scarcity of available feedback in a target domain, the CDCF approach makes use of information available in other auxiliary domains. Most of the traditional CDCF approach aim is to find a common set of entities (users or items) across the domains and then use them as a bridge for knowledge transfer. However, most real-world datasets are collected from different domains,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;DEKGCI&#65292;&#19968;&#31181;&#21452;&#38754;&#25512;&#33616;&#27169;&#22411;&#65292;&#22312;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#22270;&#21644;&#30693;&#35782;&#22270;&#35889;&#20013;&#21516;&#26102;&#20016;&#23500;&#29992;&#25143;&#21644;&#29289;&#21697;&#34920;&#31034;&#65292;&#20197;&#26377;&#25928;&#25429;&#25417;&#29992;&#25143;&#21644;&#29289;&#21697;&#20043;&#38388;&#30340;&#32852;&#21512;&#20132;&#20114;&#12290;</title><link>http://arxiv.org/abs/2306.13837</link><description>&lt;p&gt;
DEKGCI&#65306;&#19968;&#31181;&#29992;&#20110;&#25972;&#21512;&#30693;&#35782;&#22270;&#35889;&#19982;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#22270;&#30340;&#21452;&#38754;&#25512;&#33616;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DEKGCI: A double-sided recommendation model for integrating knowledge graph and user-item interaction graph. (arXiv:2306.13837v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13837
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;DEKGCI&#65292;&#19968;&#31181;&#21452;&#38754;&#25512;&#33616;&#27169;&#22411;&#65292;&#22312;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#22270;&#21644;&#30693;&#35782;&#22270;&#35889;&#20013;&#21516;&#26102;&#20016;&#23500;&#29992;&#25143;&#21644;&#29289;&#21697;&#34920;&#31034;&#65292;&#20197;&#26377;&#25928;&#25429;&#25417;&#29992;&#25143;&#21644;&#29289;&#21697;&#20043;&#38388;&#30340;&#32852;&#21512;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#33021;&#22815;&#25552;&#20379;&#20016;&#23500;&#30340;&#20449;&#24687;&#65292;&#30693;&#35782;&#22270;&#35889;&#21644;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#22270;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#34987;&#39057;&#32321;&#20351;&#29992;&#26469;&#24314;&#27169;&#29992;&#25143;&#21644;&#29289;&#21697;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#36890;&#24120;&#21482;&#20851;&#27880;&#20854;&#20013;&#19968;&#31181;&#20449;&#24687;&#28304;&#65288;&#21363;&#30693;&#35782;&#22270;&#35889;&#25110;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#22270;&#65289;&#65292;&#23548;&#33268;&#26410;&#20805;&#20998;&#21033;&#29992;&#25972;&#21512;&#20004;&#31181;&#20449;&#24687;&#28304;&#25152;&#24102;&#26469;&#30340;&#22909;&#22788;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;&#38754;&#25512;&#33616;&#27169;&#22411;DEKGCI&#12290;&#22312;DEKGCI&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#26469;&#33258;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#22270;&#30340;&#39640;&#38454;&#21327;&#20316;&#20449;&#21495;&#26469;&#20016;&#23500;&#29992;&#25143;&#34920;&#31034;&#65292;&#21516;&#26102;&#21033;&#29992;&#26469;&#33258;&#30693;&#35782;&#22270;&#35889;&#30340;&#39640;&#38454;&#32467;&#26500;&#21644;&#35821;&#20041;&#20449;&#24687;&#26469;&#20016;&#23500;&#29289;&#21697;&#34920;&#31034;&#12290;DEKGCI&#21516;&#26102;&#23398;&#20064;&#29992;&#25143;&#21644;&#29289;&#21697;&#34920;&#31034;&#65292;&#20197;&#26377;&#25928;&#25429;&#25417;&#29992;&#25143;&#21644;&#29289;&#21697;&#20043;&#38388;&#30340;&#32852;&#21512;&#20132;&#20114;&#12290;&#23454;&#39564;&#37319;&#29992;&#20102;&#19977;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;DEKGCI&#12290;
&lt;/p&gt;
&lt;p&gt;
Both knowledge graphs and user-item interaction graphs are frequently used in recommender systems due to their ability to provide rich information for modeling users and items. However, existing studies often focused on one of these sources (either the knowledge graph or the user-item interaction graph), resulting in underutilization of the benefits that can be obtained by integrating both sources of information. In this paper, we propose DEKGCI, a novel double-sided recommendation model. In DEKGCI, we use the high-order collaborative signals from the user-item interaction graph to enrich the user representations on the user side. Additionally, we utilize the high-order structural and semantic information from the knowledge graph to enrich the item representations on the item side. DEKGCI simultaneously learns the user and item representations to effectively capture the joint interactions between users and items. Three real-world datasets are adopted in the experiments to evaluate DEKG
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#26816;&#32034;&#22806;&#37096;&#35821;&#26009;&#24211;&#33719;&#21462;LLMs&#29983;&#25104;&#31572;&#26696;&#30340;&#25903;&#25345;&#35777;&#25454;&#30340;&#23454;&#39564;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;LLMs&#23481;&#26131;&#20135;&#29983;&#38169;&#35823;&#31572;&#26696;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.13781</link><description>&lt;p&gt;
&#33719;&#21462;LLMs&#29983;&#25104;&#31572;&#26696;&#30340;&#25903;&#25345;&#35777;&#25454;
&lt;/p&gt;
&lt;p&gt;
Retrieving Supporting Evidence for LLMs Generated Answers. (arXiv:2306.13781v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13781
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#26816;&#32034;&#22806;&#37096;&#35821;&#26009;&#24211;&#33719;&#21462;LLMs&#29983;&#25104;&#31572;&#26696;&#30340;&#25903;&#25345;&#35777;&#25454;&#30340;&#23454;&#39564;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;LLMs&#23481;&#26131;&#20135;&#29983;&#38169;&#35823;&#31572;&#26696;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#65292;&#21253;&#25324;&#24320;&#25918;&#22495;&#38382;&#31572;&#26041;&#38754;&#37117;&#34920;&#29616;&#20986;&#25509;&#36817;&#20154;&#31867;&#27700;&#24179;&#30340;&#24615;&#33021;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#23427;&#20204;&#20063;&#20250;&#35825;&#23548;&#20986;&#19981;&#27491;&#30830;&#30340;&#31572;&#26696;&#65292;&#22240;&#27492;&#22312;&#25509;&#21463;&#22238;&#31572;&#20043;&#21069;&#24517;&#39035;&#23545;&#20854;&#36827;&#34892;&#39564;&#35777;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25253;&#21578;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#23454;&#39564;&#65292;&#20197;&#33258;&#21160;&#39564;&#35777;&#29983;&#25104;&#30340;&#31572;&#26696;&#26159;&#21542;&#19982;&#35821;&#26009;&#24211;&#21305;&#37197;&#12290;&#25105;&#20204;&#23558;&#38382;&#39064;&#23637;&#31034;&#32473; LLM &#24182;&#33719;&#24471;&#29983;&#25104;&#30340;&#31572;&#26696;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#38382;&#39064; + &#29983;&#25104;&#30340;&#31572;&#26696;&#36825;&#19968;&#32452;&#21512;&#22312;&#35821;&#26009;&#24211;&#20013;&#26597;&#35810;&#12290;&#28982;&#21518;&#25105;&#20204;&#21521; LLM &#25552;&#20379;&#38382;&#39064; + &#29983;&#25104;&#30340;&#31572;&#26696; + &#26816;&#32034;&#21040;&#30340;&#31572;&#26696;&#36825;&#19968;&#32452;&#21512;&#65292;&#20419;&#20351;&#20854;&#25351;&#31034;&#29983;&#25104;&#30340;&#31572;&#26696;&#26159;&#21542;&#21487;&#20197;&#24471;&#21040;&#25903;&#25345;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#22522;&#20110; MS MARCO (V1) &#27979;&#35797;&#38598;&#20013;&#30340;&#38382;&#39064;&#21644;&#27573;&#33853;&#65292;&#25506;&#32034;&#20102;&#19977;&#31181;&#26816;&#32034;&#26041;&#27861;&#65292;&#20174;&#26631;&#20934; BM25&#21040;&#23436;&#25972;&#30340;&#38382;&#31572;&#22534;&#26632;&#65292;&#21253;&#25324;&#22522;&#20110;&#38405;&#35835;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current large language models (LLMs) can exhibit near-human levels of performance on many natural language tasks, including open-domain question answering. Unfortunately, they also convincingly hallucinate incorrect answers, so that responses to questions must be verified against external sources before they can be accepted at face value. In this paper, we report a simple experiment to automatically verify generated answers against a corpus. After presenting a question to an LLM and receiving a generated answer, we query the corpus with the combination of the question + generated answer. We then present the LLM with the combination of the question + generated answer + retrieved answer, prompting it to indicate if the generated answer can be supported by the retrieved answer. We base our experiment on questions and passages from the MS MARCO (V1) test collection, exploring three retrieval approaches ranging from standard BM25 to a full question answering stack, including a reader based 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22810;&#27169;&#24577;&#21307;&#23398;&#25968;&#25454;&#34701;&#21512;&#22312;&#26234;&#24935;&#21307;&#30103;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#31526;&#21512;DIKW&#26426;&#21046;&#30340;&#36890;&#29992;&#34701;&#21512;&#26694;&#26550;&#65292;&#25506;&#35752;&#20102;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2306.11963</link><description>&lt;p&gt;
&#26234;&#24935;&#21307;&#30103;&#20013;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;:&#20174;&#25968;&#25454;&#12289;&#20449;&#24687;&#12289;&#30693;&#35782;&#21040;&#26234;&#24935;&#20043;&#26053;
&lt;/p&gt;
&lt;p&gt;
Multimodality Fusion for Smart Healthcare: a Journey from Data, Information, Knowledge to Wisdom. (arXiv:2306.11963v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11963
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22810;&#27169;&#24577;&#21307;&#23398;&#25968;&#25454;&#34701;&#21512;&#22312;&#26234;&#24935;&#21307;&#30103;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#31526;&#21512;DIKW&#26426;&#21046;&#30340;&#36890;&#29992;&#34701;&#21512;&#26694;&#26550;&#65292;&#25506;&#35752;&#20102;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#21307;&#23398;&#25968;&#25454;&#34701;&#21512;&#24050;&#25104;&#20026;&#26234;&#24935;&#21307;&#30103;&#20013;&#30340;&#19968;&#31181;&#38761;&#26032;&#24615;&#26041;&#27861;&#65292;&#33021;&#22815;&#20840;&#38754;&#20102;&#35299;&#24739;&#32773;&#20581;&#24247;&#29366;&#20917;&#21644;&#20010;&#24615;&#21270;&#27835;&#30103;&#26041;&#26696;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22810;&#27169;&#24577;&#34701;&#21512;&#20026;&#26234;&#24935;&#21307;&#30103;&#24102;&#26469;&#30340;&#20174;&#25968;&#25454;&#12289;&#20449;&#24687;&#21644;&#30693;&#35782;&#21040;&#26234;&#24935;&#65288;DIKW&#65289;&#20043;&#26053;&#12290;&#20840;&#38754;&#22238;&#39038;&#20102;&#22810;&#27169;&#24577;&#21307;&#23398;&#25968;&#25454;&#34701;&#21512;&#30340;&#30740;&#31350;&#29616;&#29366;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#19981;&#21516;&#25968;&#25454;&#27169;&#24577;&#30340;&#38598;&#25104;&#26041;&#24335;&#12290;&#25991;&#31456;&#25506;&#35752;&#20102;&#29305;&#24449;&#36873;&#25321;&#12289;&#22522;&#20110;&#35268;&#21017;&#30340;&#31995;&#32479;&#12289;&#26426;&#22120;&#23398;&#20064;&#12289;&#28145;&#24230;&#23398;&#20064;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#19981;&#21516;&#26041;&#27861;&#65292;&#29992;&#20110;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#34701;&#21512;&#21644;&#20998;&#26512;&#12290;&#21516;&#26102;&#65292;&#25991;&#31456;&#20063;&#30528;&#37325;&#35752;&#35770;&#20102;&#22810;&#27169;&#24577;&#34701;&#21512;&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#32508;&#21512;&#35780;&#36848;&#30340;&#26694;&#26550;&#21644;&#35265;&#35299;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#31526;&#21512;DIKW&#26426;&#21046;&#30340;&#36890;&#29992;&#22810;&#27169;&#24577;&#21307;&#30103;&#25968;&#25454;&#34701;&#21512;&#26694;&#26550;&#12290;&#27492;&#22806;&#65292;&#25991;&#31456;&#36824;&#25506;&#35752;&#20102;&#26410;&#26469;&#19982;&#39044;&#27979;&#12289;&#39044;&#38450;&#12289;&#20010;&#24615;&#21270;&#21644;&#27835;&#30103;&#26377;&#20851;&#30340;&#21307;&#30103;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal medical data fusion has emerged as a transformative approach in smart healthcare, enabling a comprehensive understanding of patient health and personalized treatment plans. In this paper, a journey from data, information, and knowledge to wisdom (DIKW) is explored through multimodal fusion for smart healthcare. A comprehensive review of multimodal medical data fusion focuses on the integration of various data modalities are presented. It explores different approaches such as Feature selection, Rule-based systems, Machine learning, Deep learning, and Natural Language Processing for fusing and analyzing multimodal data. The paper also highlights the challenges associated with multimodal fusion in healthcare. By synthesizing the reviewed frameworks and insights, a generic framework for multimodal medical data fusion is proposed while aligning with the DIKW mechanism. Moreover, it discusses future directions aligned with the four pillars of healthcare: Predictive, Preventive, Pe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;KAR&#26694;&#26550;&#65292;&#23427;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#33719;&#21462;&#20004;&#31181;&#31867;&#22411;&#30340;&#22806;&#37096;&#30693;&#35782;&#65292;&#20998;&#21035;&#26159;&#29992;&#25143;&#20559;&#22909;&#30340;&#25512;&#29702;&#30693;&#35782;&#21644;&#39033;&#30446;&#30340;&#20107;&#23454;&#30693;&#35782;&#12290;&#36890;&#36807;&#28151;&#21512;&#19987;&#23478;&#36866;&#37197;&#22120;&#23558;&#25512;&#29702;&#21644;&#20107;&#23454;&#30693;&#35782;&#36716;&#25442;&#20026;&#22686;&#24378;&#21521;&#37327;&#65292;&#20197;&#20415;&#19982;&#29616;&#26377;&#30340;&#21327;&#21516;&#36807;&#28388;&#25512;&#33616;&#31639;&#27861;&#20860;&#23481;&#12290;</title><link>http://arxiv.org/abs/2306.10933</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24320;&#25918;&#19990;&#30028;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Towards Open-World Recommendation with Knowledge Augmentation from Large Language Models. (arXiv:2306.10933v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10933
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;KAR&#26694;&#26550;&#65292;&#23427;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#33719;&#21462;&#20004;&#31181;&#31867;&#22411;&#30340;&#22806;&#37096;&#30693;&#35782;&#65292;&#20998;&#21035;&#26159;&#29992;&#25143;&#20559;&#22909;&#30340;&#25512;&#29702;&#30693;&#35782;&#21644;&#39033;&#30446;&#30340;&#20107;&#23454;&#30693;&#35782;&#12290;&#36890;&#36807;&#28151;&#21512;&#19987;&#23478;&#36866;&#37197;&#22120;&#23558;&#25512;&#29702;&#21644;&#20107;&#23454;&#30693;&#35782;&#36716;&#25442;&#20026;&#22686;&#24378;&#21521;&#37327;&#65292;&#20197;&#20415;&#19982;&#29616;&#26377;&#30340;&#21327;&#21516;&#36807;&#28388;&#25512;&#33616;&#31639;&#27861;&#20860;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#22312;&#21508;&#31181;&#22312;&#32447;&#26381;&#21153;&#20013;&#37117;&#25198;&#28436;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#35282;&#33394;&#12290;&#20294;&#26159;&#65292;&#23427;&#20204;&#22312;&#29305;&#23450;&#39046;&#22495;&#20869;&#36827;&#34892;&#35757;&#32451;&#21644;&#37096;&#32626;&#30340;&#23553;&#38381;&#24615;&#38480;&#21046;&#20102;&#23427;&#20204;&#35775;&#38382;&#24320;&#25918;&#19990;&#30028;&#30693;&#35782;&#30340;&#33021;&#21147;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#20986;&#29616;&#22312;&#32534;&#30721;&#24191;&#27867;&#30340;&#19990;&#30028;&#30693;&#35782;&#21644;&#23637;&#31034;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#24076;&#26395;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#30452;&#25509;&#20351;&#29992;LLM&#20316;&#20026;&#25512;&#33616;&#20154;&#20043;&#21069;&#30340;&#23581;&#35797;&#24182;&#27809;&#26377;&#21462;&#24471;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24320;&#25918;&#19990;&#30028;&#30693;&#35782;&#22686;&#24378;&#25512;&#33616;&#26694;&#26550;(KAR)&#65292;&#20197;&#20174;LLM&#33719;&#21462;&#20004;&#31181;&#31867;&#22411;&#30340;&#22806;&#37096;&#30693;&#35782;--&#29992;&#25143;&#20559;&#22909;&#30340;&#25512;&#29702;&#30693;&#35782;&#21644;&#39033;&#30446;&#30340;&#20107;&#23454;&#30693;&#35782;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#22240;&#23376;&#20998;&#35299;&#25552;&#31034;&#26469;&#24341;&#23548;&#23545;&#29992;&#25143;&#21916;&#22909;&#30340;&#20934;&#30830;&#25512;&#29702;&#12290;&#29983;&#25104;&#30340;&#25512;&#29702;&#21644;&#20107;&#23454;&#30693;&#35782;&#36890;&#36807;&#28151;&#21512;&#19987;&#23478;&#36866;&#37197;&#22120;&#26377;&#25928;&#22320;&#36716;&#25442;&#24182;&#21387;&#32553;&#20026;&#22686;&#24378;&#21521;&#37327;&#65292;&#20197;&#20415;&#19982;&#29616;&#26377;&#30340;&#21327;&#21516;&#36807;&#28388;&#25512;&#33616;&#31639;&#27861;&#20860;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommender systems play a vital role in various online services. However, the insulated nature of training and deploying separately within a specific domain limits their access to open-world knowledge. Recently, the emergence of large language models (LLMs) has shown promise in bridging this gap by encoding extensive world knowledge and demonstrating reasoning capability. Nevertheless, previous attempts to directly use LLMs as recommenders have not achieved satisfactory results. In this work, we propose an Open-World Knowledge Augmented Recommendation Framework with Large Language Models, dubbed KAR, to acquire two types of external knowledge from LLMs -- the reasoning knowledge on user preferences and the factual knowledge on items. We introduce factorization prompting to elicit accurate reasoning on user preferences. The generated reasoning and factual knowledge are effectively transformed and condensed into augmented vectors by a hybrid-expert adaptor in order to be compatible with
&lt;/p&gt;</description></item><item><title>CLaMP&#26159;&#19968;&#31181;&#23545;&#27604;&#35821;&#35328;-&#38899;&#20048;&#39044;&#35757;&#32451;&#25216;&#26415;&#65292;&#33021;&#22815;&#23398;&#20064;&#31526;&#21495;&#38899;&#20048;&#21644;&#33258;&#28982;&#35821;&#35328;&#20043;&#38388;&#30340;&#36328;&#27169;&#24577;&#34920;&#31034;&#12290;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#21644;&#20998;&#22359;&#22788;&#29702;&#65292;&#23427;&#23558;&#31526;&#21495;&#38899;&#20048;&#34920;&#31034;&#25104;&#38271;&#24230;&#19981;&#21040;10&#65285;&#30340;&#24207;&#21015;&#65292;&#24182;&#20351;&#29992;&#25513;&#34109;&#38899;&#20048;&#27169;&#22411;&#39044;&#35757;&#32451;&#30446;&#26631;&#26469;&#22686;&#24378;&#38899;&#20048;&#32534;&#30721;&#22120;&#23545;&#38899;&#20048;&#19978;&#19979;&#25991;&#21644;&#32467;&#26500;&#30340;&#29702;&#35299;&#12290;&#36825;&#31181;&#25216;&#26415;&#36229;&#36234;&#20102;&#29616;&#26377;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#21487;&#20197;&#23454;&#29616;&#31526;&#21495;&#38899;&#20048;&#30340;&#35821;&#20041;&#25628;&#32034;&#21644;&#38646;&#26679;&#26412;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2304.11029</link><description>&lt;p&gt;
CLaMP&#65306;&#29992;&#20110;&#36328;&#27169;&#24577;&#31526;&#21495;&#38899;&#20048;&#20449;&#24687;&#26816;&#32034;&#30340;&#23545;&#27604;&#35821;&#35328;-&#38899;&#20048;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
CLaMP: Contrastive Language-Music Pre-training for Cross-Modal Symbolic Music Information Retrieval. (arXiv:2304.11029v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11029
&lt;/p&gt;
&lt;p&gt;
CLaMP&#26159;&#19968;&#31181;&#23545;&#27604;&#35821;&#35328;-&#38899;&#20048;&#39044;&#35757;&#32451;&#25216;&#26415;&#65292;&#33021;&#22815;&#23398;&#20064;&#31526;&#21495;&#38899;&#20048;&#21644;&#33258;&#28982;&#35821;&#35328;&#20043;&#38388;&#30340;&#36328;&#27169;&#24577;&#34920;&#31034;&#12290;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#21644;&#20998;&#22359;&#22788;&#29702;&#65292;&#23427;&#23558;&#31526;&#21495;&#38899;&#20048;&#34920;&#31034;&#25104;&#38271;&#24230;&#19981;&#21040;10&#65285;&#30340;&#24207;&#21015;&#65292;&#24182;&#20351;&#29992;&#25513;&#34109;&#38899;&#20048;&#27169;&#22411;&#39044;&#35757;&#32451;&#30446;&#26631;&#26469;&#22686;&#24378;&#38899;&#20048;&#32534;&#30721;&#22120;&#23545;&#38899;&#20048;&#19978;&#19979;&#25991;&#21644;&#32467;&#26500;&#30340;&#29702;&#35299;&#12290;&#36825;&#31181;&#25216;&#26415;&#36229;&#36234;&#20102;&#29616;&#26377;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#21487;&#20197;&#23454;&#29616;&#31526;&#21495;&#38899;&#20048;&#30340;&#35821;&#20041;&#25628;&#32034;&#21644;&#38646;&#26679;&#26412;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;CLaMP&#65306;&#23545;&#27604;&#35821;&#35328;-&#38899;&#20048;&#39044;&#35757;&#32451;&#65292;&#23427;&#20351;&#29992;&#38899;&#20048;&#32534;&#30721;&#22120;&#21644;&#25991;&#26412;&#32534;&#30721;&#22120;&#36890;&#36807;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#32852;&#21512;&#35757;&#32451;&#26469;&#23398;&#20064;&#33258;&#28982;&#35821;&#35328;&#21644;&#31526;&#21495;&#38899;&#20048;&#20043;&#38388;&#30340;&#36328;&#27169;&#24577;&#34920;&#31034;&#12290;&#20026;&#20102;&#39044;&#35757;&#32451;CLaMP&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;140&#19975;&#20010;&#38899;&#20048;-&#25991;&#26412;&#23545;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#12290;&#23427;&#20351;&#29992;&#20102;&#25991;&#26412;&#38543;&#26426;&#22833;&#27963;&#26469;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#21644;&#20998;&#22359;&#22788;&#29702;&#20197;&#39640;&#25928;&#22320;&#34920;&#31034;&#38899;&#20048;&#25968;&#25454;&#65292;&#20174;&#32780;&#23558;&#24207;&#21015;&#38271;&#24230;&#32553;&#30701;&#21040;&#19981;&#21040;10&#65285;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#25513;&#34109;&#38899;&#20048;&#27169;&#22411;&#39044;&#35757;&#32451;&#30446;&#26631;&#65292;&#20197;&#22686;&#24378;&#38899;&#20048;&#32534;&#30721;&#22120;&#23545;&#38899;&#20048;&#19978;&#19979;&#25991;&#21644;&#32467;&#26500;&#30340;&#29702;&#35299;&#12290;CLaMP&#38598;&#25104;&#20102;&#25991;&#26412;&#20449;&#24687;&#65292;&#20197;&#23454;&#29616;&#31526;&#21495;&#38899;&#20048;&#30340;&#35821;&#20041;&#25628;&#32034;&#21644;&#38646;&#26679;&#26412;&#20998;&#31867;&#65292;&#36229;&#36234;&#20102;&#20808;&#21069;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#20026;&#25903;&#25345;&#35821;&#20041;&#25628;&#32034;&#21644;&#38899;&#20048;&#20998;&#31867;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#20844;&#24320;&#21457;&#24067;&#20102;WikiMusicText&#65288;WikiMT&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;1010&#20010;ABC&#31526;&#21495;&#35889;&#30340;&#25968;&#25454;&#38598;&#65292;&#27599;&#20010;&#35889;&#37117;&#38468;&#24102;&#26377;&#26631;&#39064;&#12289;&#33402;&#26415;&#23478;&#12289;&#27969;&#27966;&#21644;&#25551;&#36848;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce CLaMP: Contrastive Language-Music Pre-training, which learns cross-modal representations between natural language and symbolic music using a music encoder and a text encoder trained jointly with a contrastive loss. To pre-train CLaMP, we collected a large dataset of 1.4 million music-text pairs. It employed text dropout as a data augmentation technique and bar patching to efficiently represent music data which reduces sequence length to less than 10%. In addition, we developed a masked music model pre-training objective to enhance the music encoder's comprehension of musical context and structure. CLaMP integrates textual information to enable semantic search and zero-shot classification for symbolic music, surpassing the capabilities of previous models. To support the evaluation of semantic search and music classification, we publicly release WikiMusicText (WikiMT), a dataset of 1010 lead sheets in ABC notation, each accompanied by a title, artist, genre, and description.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#25512;&#33616;&#39046;&#22495;&#24191;&#27867;&#20351;&#29992;&#30340;ChatGPT&#30340;&#28508;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#21363;&#20351;&#27809;&#26377;&#24494;&#35843;&#65292;ChatGPT&#22312;&#20116;&#20010;&#25512;&#33616;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20855;&#26377;&#24456;&#22909;&#30340;&#25512;&#33616;&#31934;&#24230;&#21644;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.10149</link><description>&lt;p&gt;
ChatGPT&#26159;&#19968;&#20010;&#22909;&#30340;&#25512;&#33616;&#31639;&#27861;&#21527;&#65311;&#21021;&#27493;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Is ChatGPT a Good Recommender? A Preliminary Study. (arXiv:2304.10149v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10149
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#25512;&#33616;&#39046;&#22495;&#24191;&#27867;&#20351;&#29992;&#30340;ChatGPT&#30340;&#28508;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#21363;&#20351;&#27809;&#26377;&#24494;&#35843;&#65292;ChatGPT&#22312;&#20116;&#20010;&#25512;&#33616;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20855;&#26377;&#24456;&#22909;&#30340;&#25512;&#33616;&#31934;&#24230;&#21644;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#22312;&#36807;&#21435;&#20960;&#21313;&#24180;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#24182;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20256;&#32479;&#25512;&#33616;&#26041;&#27861;&#37117;&#26159;&#29305;&#23450;&#20219;&#21153;&#30340;&#65292;&#22240;&#27492;&#32570;&#20047;&#26377;&#25928;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#26368;&#36817;&#65292;ChatGPT&#30340;&#20986;&#29616;&#36890;&#36807;&#22686;&#24378;&#23545;&#35805;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#26174;&#33879;&#25512;&#36827;&#20102;NLP&#20219;&#21153;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;ChatGPT&#22312;&#25512;&#33616;&#39046;&#22495;&#30340;&#24212;&#29992;&#36824;&#27809;&#26377;&#24471;&#21040;&#20805;&#20998;&#30340;&#30740;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;ChatGPT&#20316;&#20026;&#36890;&#29992;&#25512;&#33616;&#27169;&#22411;&#65292;&#25506;&#35752;&#23427;&#23558;&#20174;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;&#20013;&#33719;&#24471;&#30340;&#24191;&#27867;&#35821;&#35328;&#21644;&#19990;&#30028;&#30693;&#35782;&#36716;&#31227;&#21040;&#25512;&#33616;&#22330;&#26223;&#20013;&#30340;&#28508;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#32452;&#25552;&#31034;&#65292;&#24182;&#35780;&#20272;ChatGPT&#22312;&#20116;&#20010;&#25512;&#33616;&#22330;&#26223;&#20013;&#30340;&#34920;&#29616;&#12290;&#19982;&#20256;&#32479;&#30340;&#25512;&#33616;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#22312;&#25972;&#20010;&#35780;&#20272;&#36807;&#31243;&#20013;&#25105;&#20204;&#19981;&#24494;&#35843;ChatGPT&#65292;&#20165;&#20381;&#38752;&#25552;&#31034;&#33258;&#36523;&#23558;&#25512;&#33616;&#20219;&#21153;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommendation systems have witnessed significant advancements and have been widely used over the past decades. However, most traditional recommendation methods are task-specific and therefore lack efficient generalization ability. Recently, the emergence of ChatGPT has significantly advanced NLP tasks by enhancing the capabilities of conversational models. Nonetheless, the application of ChatGPT in the recommendation domain has not been thoroughly investigated. In this paper, we employ ChatGPT as a general-purpose recommendation model to explore its potential for transferring extensive linguistic and world knowledge acquired from large-scale corpora to recommendation scenarios. Specifically, we design a set of prompts and evaluate ChatGPT's performance on five recommendation scenarios. Unlike traditional recommendation methods, we do not fine-tune ChatGPT during the entire evaluation process, relying only on the prompts themselves to convert recommendation tasks into natural language 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28176;&#36827;&#24335;&#33976;&#39311;&#26041;&#27861;PROD&#65292;&#29992;&#20110;&#23494;&#38598;&#26816;&#32034;&#65292;&#36890;&#36807;&#36880;&#27493;&#25913;&#36827;&#23398;&#29983;&#27169;&#22411;&#26469;&#22635;&#34917;&#25945;&#24072;&#21644;&#23398;&#29983;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#24182;&#22312;&#20116;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2209.13335</link><description>&lt;p&gt;
PROD&#65306;&#28176;&#36827;&#24335;&#33976;&#39311;&#29992;&#20110;&#23494;&#38598;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
PROD: Progressive Distillation for Dense Retrieval. (arXiv:2209.13335v3 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.13335
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28176;&#36827;&#24335;&#33976;&#39311;&#26041;&#27861;PROD&#65292;&#29992;&#20110;&#23494;&#38598;&#26816;&#32034;&#65292;&#36890;&#36807;&#36880;&#27493;&#25913;&#36827;&#23398;&#29983;&#27169;&#22411;&#26469;&#22635;&#34917;&#25945;&#24072;&#21644;&#23398;&#29983;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#24182;&#22312;&#20116;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#26159;&#23558;&#24378;&#25945;&#24072;&#30340;&#30693;&#35782;&#20256;&#36882;&#32473;&#39640;&#25928;&#23398;&#29983;&#27169;&#22411;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36890;&#24120;&#24773;&#20917;&#19979;&#39044;&#26399;&#30340;&#26356;&#22909;&#30340;&#25945;&#24072;&#20250;&#23548;&#33268;&#32463;&#36807;&#33976;&#39311;&#21518;&#23398;&#29983;&#26356;&#31967;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#23494;&#38598;&#26816;&#32034;&#30340;PROgressive Distillation (PROD)&#26041;&#27861;&#65292;&#21253;&#25324;&#25945;&#24072;&#28176;&#36827;&#24335;&#33976;&#39311;&#21644;&#25968;&#25454;&#28176;&#36827;&#24335;&#33976;&#39311;&#20004;&#20010;&#38454;&#27573;&#65292;&#20174;&#32780;&#36880;&#27493;&#25552;&#39640;&#23398;&#29983;&#30340;&#26816;&#32034;&#32489;&#25928;&#12290;&#22312;&#20116;&#20010;&#34987;&#24191;&#27867;&#20351;&#29992;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65288;MS MARCO Passage&#12289;TREC Passage 19&#12289;TREC Document 19&#12289;MS MARCO Document&#21644;&#33258;&#28982;&#38382;&#39064;&#65289;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#39564;&#35777;&#65292;PROD&#22312;&#23494;&#38598;&#26816;&#32034;&#30340;&#33976;&#39311;&#26041;&#27861;&#20013;&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#20195;&#30721;&#21644;&#27169;&#22411;&#23558;&#20250;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge distillation is an effective way to transfer knowledge from a strong teacher to an efficient student model. Ideally, we expect the better the teacher is, the better the student. However, this expectation does not always come true. It is common that a better teacher model results in a bad student via distillation due to the nonnegligible gap between teacher and student. To bridge the gap, we propose PROD, a PROgressive Distillation method, for dense retrieval. PROD consists of a teacher progressive distillation and a data progressive distillation to gradually improve the student. We conduct extensive experiments on five widely-used benchmarks, MS MARCO Passage, TREC Passage 19, TREC Document 19, MS MARCO Document and Natural Questions, where PROD achieves the state-of-the-art within the distillation methods for dense retrieval. The code and models will be released.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#22411;&#25361;&#25112;&#20219;&#21153;&#65292;&#21363;&#36890;&#36807;&#36828;&#31243;&#30417;&#30563;&#26041;&#24335;&#20174;&#31185;&#23398;&#25991;&#31456;&#20013;&#30340;&#34920;&#26684;&#20013;&#25552;&#21462;&#26377;&#20851;&#26448;&#26009;&#32452;&#25104;&#30340;&#20449;&#24687;&#12290;&#20026;&#27492;&#65292;&#30740;&#31350;&#32773;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;4408&#20010;&#36828;&#31243;&#30417;&#30563;&#34920;&#26684;&#21644;1475&#20010;&#25163;&#21160;&#27880;&#37322;&#30340;&#24320;&#21457;&#21644;&#27979;&#35797;&#34920;&#26684;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#24378;&#22522;&#32447;&#8212;&#8212;DiSCoMaT&#12290;</title><link>http://arxiv.org/abs/2207.01079</link><description>&lt;p&gt;
DiSCoMaT&#65306;&#26448;&#26009;&#31185;&#23398;&#25991;&#31456;&#20013;&#22522;&#20110;&#36828;&#31243;&#30417;&#30563;&#30340;&#34920;&#26684;&#32452;&#25104;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
DiSCoMaT: Distantly Supervised Composition Extraction from Tables in Materials Science Articles. (arXiv:2207.01079v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.01079
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#22411;&#25361;&#25112;&#20219;&#21153;&#65292;&#21363;&#36890;&#36807;&#36828;&#31243;&#30417;&#30563;&#26041;&#24335;&#20174;&#31185;&#23398;&#25991;&#31456;&#20013;&#30340;&#34920;&#26684;&#20013;&#25552;&#21462;&#26377;&#20851;&#26448;&#26009;&#32452;&#25104;&#30340;&#20449;&#24687;&#12290;&#20026;&#27492;&#65292;&#30740;&#31350;&#32773;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;4408&#20010;&#36828;&#31243;&#30417;&#30563;&#34920;&#26684;&#21644;1475&#20010;&#25163;&#21160;&#27880;&#37322;&#30340;&#24320;&#21457;&#21644;&#27979;&#35797;&#34920;&#26684;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#24378;&#22522;&#32447;&#8212;&#8212;DiSCoMaT&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#31185;&#23398;&#39046;&#22495;&#25991;&#31456;&#20013;&#30340;&#34920;&#26684;&#20013;&#25552;&#21462;&#26377;&#20851;&#26448;&#26009;&#32452;&#25104;&#30340;&#20449;&#24687;&#26159;&#30693;&#35782;&#24211;&#31574;&#21010;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#34920;&#26684;&#25552;&#21462;&#22120;&#20551;&#23450;&#24744;&#24050;&#32463;&#20102;&#35299;&#34920;&#26684;&#32467;&#26500;&#21644;&#26684;&#24335;&#65292;&#32780;&#31185;&#23398;&#34920;&#26684;&#20013;&#21487;&#33021;&#27809;&#26377;&#36825;&#20123;&#20808;&#21069;&#30340;&#30693;&#35782;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#29305;&#23450;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#34920;&#26684;&#25552;&#21462;&#38382;&#39064;&#65306;&#25552;&#21462;&#26448;&#26009;&#65288;&#20363;&#22914;&#29627;&#29827;&#65292;&#21512;&#37329;&#65289;&#30340;&#32452;&#25104;&#12290;&#25105;&#20204;&#39318;&#20808;&#35266;&#23519;&#21040;&#26448;&#26009;&#31185;&#23398;&#30740;&#31350;&#20154;&#21592;&#20351;&#29992;&#21508;&#31181;&#34920;&#26684;&#26679;&#24335;&#32452;&#32455;&#31867;&#20284;&#30340;&#32452;&#25104;&#65292;&#36825;&#38656;&#35201;&#19968;&#20010;&#26234;&#33021;&#27169;&#22411;&#26469;&#29702;&#35299;&#34920;&#26684;&#21644;&#25552;&#21462;&#32452;&#25104;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23558;&#20854;&#23450;&#20041;&#20026;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#26032;&#22411;&#25361;&#25112;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#30001;4408&#20010;&#36828;&#31243;&#30417;&#30563;&#34920;&#26684;&#21644;1475&#20010;&#25163;&#21160;&#27880;&#37322;&#30340;&#24320;&#21457;&#21644;&#27979;&#35797;&#34920;&#26684;&#32452;&#25104;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;DiSCoMaT&#65292;&#23427;&#26159;&#19968;&#20010;&#38024;&#23545;&#35813;&#38382;&#39064;&#30340;&#24378;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
A crucial component in the curation of KB for a scientific domain is information extraction from tables in the domain's published articles -- tables carry important information (often numeric), which must be adequately extracted for a comprehensive machine understanding of an article. Existing table extractors assume prior knowledge of table structure and format, which may not be known in scientific tables. We study a specific and challenging table extraction problem: extracting compositions of materials (e.g., glasses, alloys). We first observe that materials science researchers organize similar compositions in a wide variety of table styles, necessitating an intelligent model for table understanding and composition extraction. Consequently, we define this novel task as a challenge for the ML community and create a training dataset comprising 4,408 distantly supervised tables, along with 1,475 manually annotated dev and test tables. We also present DiSCoMaT, a strong baseline geared t
&lt;/p&gt;</description></item><item><title>ReuseKNN&#26159;&#19968;&#31181;&#38754;&#21521;&#24046;&#20998;&#38544;&#31169;&#30340;KNN&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#35782;&#21035;&#23567;&#20294;&#39640;&#24230;&#21487;&#37325;&#29992;&#30340;&#37051;&#22495;&#26469;&#20943;&#23569;&#24046;&#20998;&#38544;&#31169;&#30340;&#20351;&#29992;&#65292;&#20943;&#23567;&#20102;&#38544;&#31169;&#27844;&#38706;&#30340;&#39118;&#38505;&#65292;&#24182;&#19988;&#38656;&#35201;&#20445;&#25252;&#30340;&#37051;&#23621;&#36739;&#23569;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#25512;&#33616;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2206.11561</link><description>&lt;p&gt;
ReuseKNN: &#38754;&#21521;&#24046;&#20998;&#38544;&#31169;&#30340;KNN&#25512;&#33616;&#30340;&#37051;&#22495;&#37325;&#29992;
&lt;/p&gt;
&lt;p&gt;
ReuseKNN: Neighborhood Reuse for Differentially-Private KNN-Based Recommendations. (arXiv:2206.11561v3 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.11561
&lt;/p&gt;
&lt;p&gt;
ReuseKNN&#26159;&#19968;&#31181;&#38754;&#21521;&#24046;&#20998;&#38544;&#31169;&#30340;KNN&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#35782;&#21035;&#23567;&#20294;&#39640;&#24230;&#21487;&#37325;&#29992;&#30340;&#37051;&#22495;&#26469;&#20943;&#23569;&#24046;&#20998;&#38544;&#31169;&#30340;&#20351;&#29992;&#65292;&#20943;&#23567;&#20102;&#38544;&#31169;&#27844;&#38706;&#30340;&#39118;&#38505;&#65292;&#24182;&#19988;&#38656;&#35201;&#20445;&#25252;&#30340;&#37051;&#23621;&#36739;&#23569;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#25512;&#33616;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#29992;&#25143;&#30340;KNN&#25512;&#33616;&#31995;&#32479;&#65288;UserKNN&#65289;&#22312;&#25512;&#33616;&#36807;&#31243;&#20013;&#21033;&#29992;&#30446;&#26631;&#29992;&#25143;&#30340;k&#20010;&#26368;&#36817;&#37051;&#23621;&#30340;&#35780;&#20998;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#36825;&#22686;&#21152;&#20102;&#37051;&#23621;&#30340;&#38544;&#31169;&#39118;&#38505;&#65292;&#22240;&#20026;&#20182;&#20204;&#30340;&#35780;&#20998;&#25968;&#25454;&#21487;&#33021;&#34987;&#20854;&#20182;&#29992;&#25143;&#25110;&#24694;&#24847;&#26041;&#26292;&#38706;&#12290;&#20026;&#20102;&#20943;&#23567;&#36825;&#20010;&#39118;&#38505;&#65292;&#29616;&#26377;&#30340;&#24037;&#20316;&#36890;&#36807;&#21521;&#37051;&#23621;&#30340;&#35780;&#20998;&#28155;&#21152;&#38543;&#26426;&#24615;&#26469;&#24212;&#29992;&#24046;&#20998;&#38544;&#31169;&#65292;&#20294;&#36825;&#20250;&#38477;&#20302;UserKNN&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;ReuseKNN&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#38754;&#21521;&#24046;&#20998;&#38544;&#31169;&#30340;KNN&#25512;&#33616;&#31995;&#32479;&#12290;&#20027;&#35201;&#24605;&#24819;&#26159;&#35782;&#21035;&#20986;&#23567;&#20294;&#39640;&#24230;&#21487;&#37325;&#29992;&#30340;&#37051;&#22495;&#65292;&#20197;&#20415;(i)&#21482;&#26377;&#19968;&#23567;&#37096;&#20998;&#29992;&#25143;&#38656;&#35201;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#36827;&#34892;&#20445;&#25252;&#65292;(ii)&#22823;&#37096;&#20998;&#29992;&#25143;&#19981;&#38656;&#35201;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#36827;&#34892;&#20445;&#25252;&#65292;&#22240;&#20026;&#23427;&#20204;&#24456;&#23569;&#34987;&#21033;&#29992;&#20316;&#20026;&#37051;&#23621;&#12290;&#22312;&#25105;&#20204;&#23545;&#20116;&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#24471;&#20986;&#20102;&#20004;&#20010;&#20851;&#38190;&#35266;&#23519;&#32467;&#26524;:&#39318;&#20808;&#65292;ReuseKNN&#38656;&#35201;&#36739;&#23567;&#30340;&#37051;&#22495;&#65292;&#22240;&#27492;&#38656;&#35201;&#20445;&#25252;&#30340;&#37051;&#23621;&#36739;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
User-based KNN recommender systems (UserKNN) utilize the rating data of a target user's k nearest neighbors in the recommendation process. This, however, increases the privacy risk of the neighbors since their rating data might be exposed to other users or malicious parties. To reduce this risk, existing work applies differential privacy by adding randomness to the neighbors' ratings, which reduces the accuracy of UserKNN. In this work, we introduce ReuseKNN, a novel differentially-private KNN-based recommender system. The main idea is to identify small but highly reusable neighborhoods so that (i) only a minimal set of users requires protection with differential privacy, and (ii) most users do not need to be protected with differential privacy, since they are only rarely exploited as neighbors. In our experiments on five diverse datasets, we make two key observations: Firstly, ReuseKNN requires significantly smaller neighborhoods, and thus, fewer neighbors need to be protected with di
&lt;/p&gt;</description></item></channel></rss>