<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#25506;&#32034;&#22522;&#20110;&#32842;&#22825;&#30340;&#25628;&#32034;&#24341;&#25806;&#30340;&#21019;&#36896;&#24615;&#26426;&#21046;&#65292;&#24182;&#35299;&#26512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#36873;&#25321;&#20449;&#24687;&#28304;&#20197;&#29983;&#25104;&#20154;&#31867;&#19968;&#26679;&#29702;&#35299;&#24615;&#21644;&#21019;&#36896;&#24615;&#30340;&#21709;&#24212;&#12290;</title><link>https://arxiv.org/abs/2402.19421</link><description>&lt;p&gt;
&#30693;&#35782;&#22609;&#36896;&#65306;&#25506;&#32034;&#22522;&#20110;&#32842;&#22825;&#30340;&#25628;&#32034;&#24341;&#25806;&#30340;&#21019;&#36896;&#24615;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Crafting Knowledge: Exploring the Creative Mechanisms of Chat-Based Search Engines
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19421
&lt;/p&gt;
&lt;p&gt;
&#25506;&#32034;&#22522;&#20110;&#32842;&#22825;&#30340;&#25628;&#32034;&#24341;&#25806;&#30340;&#21019;&#36896;&#24615;&#26426;&#21046;&#65292;&#24182;&#35299;&#26512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#36873;&#25321;&#20449;&#24687;&#28304;&#20197;&#29983;&#25104;&#20154;&#31867;&#19968;&#26679;&#29702;&#35299;&#24615;&#21644;&#21019;&#36896;&#24615;&#30340;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#23383;&#20449;&#24687;&#20256;&#25773;&#39046;&#22495;&#65292;&#25628;&#32034;&#24341;&#25806;&#25198;&#28436;&#30528;&#20851;&#38190;&#30340;&#35282;&#33394;&#65292;&#36830;&#25509;&#20449;&#24687;&#23547;&#25214;&#32773;&#21644;&#20449;&#24687;&#25552;&#20379;&#32773;&#12290;&#37319;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#25216;&#26415;&#30340;&#22522;&#20110;&#32842;&#22825;&#30340;&#25628;&#32034;&#24341;&#25806;&#30340;&#20986;&#29616;&#65292;&#20363;&#22914;&#24517;&#24212;&#32842;&#22825;&#65292;&#26631;&#24535;&#30528;&#25628;&#32034;&#29983;&#24577;&#31995;&#32479;&#30340;&#36827;&#21270;&#39134;&#36291;&#12290;&#23427;&#20204;&#23637;&#31034;&#20102;&#20803;&#35748;&#30693;&#33021;&#21147;&#65292;&#33021;&#22815;&#35299;&#37322;&#32593;&#32476;&#20449;&#24687;&#24182;&#20855;&#26377;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#29702;&#35299;&#21644;&#21019;&#36896;&#21147;&#12290;&#28982;&#32780;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#20351;&#24471;&#23427;&#20204;&#30340;&#8220;&#35748;&#30693;&#8221;&#36807;&#31243;&#21464;&#24471;&#19981;&#36879;&#26126;&#65292;&#29978;&#33267;&#25361;&#25112;&#20102;&#35774;&#35745;&#24072;&#23545;&#20854;&#29702;&#35299;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#35299;&#21078;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#20110;&#32842;&#22825;&#30340;&#25628;&#32034;&#24341;&#25806;&#65288;&#20855;&#20307;&#20026;&#24517;&#24212;&#32842;&#22825;&#65289;&#36873;&#25321;&#20449;&#24687;&#28304;&#20197;&#20316;&#20026;&#20854;&#21709;&#24212;&#30340;&#26426;&#21046;&#12290;&#20026;&#27492;&#65292;&#36890;&#36807;&#19982;&#26032;&#29256;&#24517;&#24212;&#30340;&#20114;&#21160;&#65292;&#32534;&#21046;&#20102;&#19968;&#20010;&#24222;&#22823;&#30340;&#25968;&#25454;&#38598;&#65292;&#35760;&#24405;&#20102;&#23427;&#24341;&#29992;&#30340;&#32593;&#31449;&#20197;&#21450;&#20256;&#32479;&#25628;&#32034;&#24341;&#25806;&#21015;&#20986;&#30340;&#32593;&#31449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19421v1 Announce Type: cross  Abstract: In the domain of digital information dissemination, search engines act as pivotal conduits linking information seekers with providers. The advent of chat-based search engines utilizing Large Language Models (LLMs) and Retrieval Augmented Generation (RAG), exemplified by Bing Chat, marks an evolutionary leap in the search ecosystem. They demonstrate metacognitive abilities in interpreting web information and crafting responses with human-like understanding and creativity. Nonetheless, the intricate nature of LLMs renders their "cognitive" processes opaque, challenging even their designers' understanding. This research aims to dissect the mechanisms through which an LLM-powered chat-based search engine, specifically Bing Chat, selects information sources for its responses. To this end, an extensive dataset has been compiled through engagements with New Bing, documenting the websites it cites alongside those listed by the conventional sea
&lt;/p&gt;</description></item><item><title>PaECTER&#26159;&#19968;&#20010;&#19987;&#20026;&#19987;&#21033;&#35774;&#35745;&#30340;&#24320;&#25918;&#28304;&#30721;&#25991;&#26723;&#32423;&#32534;&#30721;&#22120;&#65292;&#21033;&#29992;&#24341;&#25991;&#20449;&#24687;&#23545;BERT&#36827;&#34892;&#24494;&#35843;&#65292;&#29983;&#25104;&#19987;&#21033;&#25991;&#26723;&#30340;&#25968;&#20540;&#34920;&#31034;&#65292;&#24182;&#22312;&#19987;&#21033;&#39046;&#22495;&#30340;&#30456;&#20284;&#24615;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>https://arxiv.org/abs/2402.19411</link><description>&lt;p&gt;
PaECTER&#65306;&#20351;&#29992;&#24341;&#25991;&#20449;&#24687;&#30340;&#19987;&#21033;&#32423;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
PaECTER: Patent-level Representation Learning using Citation-informed Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19411
&lt;/p&gt;
&lt;p&gt;
PaECTER&#26159;&#19968;&#20010;&#19987;&#20026;&#19987;&#21033;&#35774;&#35745;&#30340;&#24320;&#25918;&#28304;&#30721;&#25991;&#26723;&#32423;&#32534;&#30721;&#22120;&#65292;&#21033;&#29992;&#24341;&#25991;&#20449;&#24687;&#23545;BERT&#36827;&#34892;&#24494;&#35843;&#65292;&#29983;&#25104;&#19987;&#21033;&#25991;&#26723;&#30340;&#25968;&#20540;&#34920;&#31034;&#65292;&#24182;&#22312;&#19987;&#21033;&#39046;&#22495;&#30340;&#30456;&#20284;&#24615;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
PaECTER&#26159;&#19968;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#12289;&#38754;&#21521;&#19987;&#21033;&#30340;&#25991;&#26723;&#32423;&#32534;&#30721;&#22120;&#65292;&#25105;&#20204;&#21033;&#29992;&#23457;&#26680;&#21592;&#28155;&#21152;&#30340;&#24341;&#25991;&#20449;&#24687;&#23545;BERT&#36827;&#34892;&#24494;&#35843;&#65292;&#20026;&#19987;&#21033;&#25991;&#26723;&#29983;&#25104;&#25968;&#20540;&#34920;&#31034;&#12290;&#19982;&#19987;&#21033;&#39046;&#22495;&#20013;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#30456;&#27604;&#65292;PaECTER&#22312;&#30456;&#20284;&#24615;&#20219;&#21153;&#20013;&#34920;&#29616;&#26356;&#22909;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#19987;&#21033;&#24341;&#25991;&#39044;&#27979;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#20004;&#31181;&#19981;&#21516;&#30340;&#25490;&#21517;&#35780;&#20272;&#25351;&#26631;&#19978;&#22343;&#20248;&#20110;&#19979;&#19968;&#20010;&#26368;&#20339;&#19987;&#21033;&#29305;&#23450;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;&#19987;&#21033;BERT&#65289;&#12290;&#19982;25&#20010;&#19981;&#30456;&#20851;&#30340;&#19987;&#21033;&#30456;&#27604;&#65292;PaECTER&#22312;&#24179;&#22343;&#25490;&#21517;1.32&#22788;&#39044;&#27979;&#21040;&#33267;&#23569;&#19968;&#20010;&#26368;&#30456;&#20284;&#30340;&#19987;&#21033;&#12290;PaECTER&#20174;&#19987;&#21033;&#25991;&#26412;&#29983;&#25104;&#30340;&#25968;&#20540;&#34920;&#31034;&#21487;&#29992;&#20110;&#20998;&#31867;&#12289;&#36861;&#36394;&#30693;&#35782;&#27969;&#21160;&#25110;&#35821;&#20041;&#30456;&#20284;&#24615;&#25628;&#32034;&#31561;&#19979;&#28216;&#20219;&#21153;&#12290;&#35821;&#20041;&#30456;&#20284;&#24615;&#25628;&#32034;&#22312;&#21457;&#26126;&#20154;&#21644;&#19987;&#21033;&#30340;&#20808;&#21069;&#25216;&#26415;&#25628;&#32034;&#32972;&#26223;&#20013;&#23588;&#20026;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19411v1 Announce Type: cross  Abstract: PaECTER is a publicly available, open-source document-level encoder specific for patents. We fine-tune BERT for Patents with examiner-added citation information to generate numerical representations for patent documents. PaECTER performs better in similarity tasks than current state-of-the-art models used in the patent domain. More specifically, our model outperforms the next-best patent specific pre-trained language model (BERT for Patents) on our patent citation prediction test dataset on two different rank evaluation metrics. PaECTER predicts at least one most similar patent at a rank of 1.32 on average when compared against 25 irrelevant patents. Numerical representations generated by PaECTER from patent text can be used for downstream tasks such as classification, tracing knowledge flows, or semantic similarity search. Semantic similarity search is especially relevant in the context of prior art search for both inventors and paten
&lt;/p&gt;</description></item><item><title>MENTOR&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#32423;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#27169;&#24577;&#25512;&#33616;&#20013;&#30340;&#26631;&#31614;&#31232;&#30095;&#21644;&#27169;&#24577;&#23545;&#40784;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.19407</link><description>&lt;p&gt;
MENTOR&#65306;&#22810;&#32423;&#33258;&#30417;&#30563;&#23398;&#20064;&#29992;&#20110;&#22810;&#27169;&#24577;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
MENTOR: Multi-level Self-supervised Learning for Multimodal Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19407
&lt;/p&gt;
&lt;p&gt;
MENTOR&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#32423;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#27169;&#24577;&#25512;&#33616;&#20013;&#30340;&#26631;&#31614;&#31232;&#30095;&#21644;&#27169;&#24577;&#23545;&#40784;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22810;&#23186;&#20307;&#20449;&#24687;&#30340;&#19981;&#26029;&#22686;&#21152;&#65292;&#22810;&#27169;&#24577;&#25512;&#33616;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#23427;&#21033;&#29992;&#22810;&#27169;&#24577;&#20449;&#24687;&#26469;&#32531;&#35299;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#25968;&#25454;&#31232;&#30095;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#25512;&#33616;&#30340;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#23545;&#26631;&#35760;&#25968;&#25454;&#30340;&#20381;&#36182;&#20005;&#37325;&#38480;&#21046;&#20102;&#22810;&#27169;&#24577;&#25512;&#33616;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#26368;&#36817;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#24050;&#34987;&#29992;&#20110;&#22810;&#27169;&#24577;&#25512;&#33616;&#20197;&#20943;&#36731;&#26631;&#31614;&#31232;&#30095;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#22312;&#23545;&#40784;&#22810;&#27169;&#24577;&#20449;&#24687;&#26102;&#26080;&#27861;&#36991;&#20813;&#27169;&#24577;&#22122;&#22768;&#65292;&#22240;&#20026;&#19981;&#21516;&#27169;&#24577;&#30340;&#20998;&#24067;&#24046;&#24322;&#36739;&#22823;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#26631;&#31614;&#31232;&#30095;&#38382;&#39064;&#21644;&#27169;&#24577;&#23545;&#40784;&#38382;&#39064;&#30340;&#22810;&#32423;&#33258;&#30417;&#30563;&#23398;&#20064;&#29992;&#20110;&#22810;&#27169;&#24577;&#25512;&#33616;&#65288;MENTOR&#65289;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19407v1 Announce Type: new  Abstract: With the increasing multimedia information, multimodal recommendation has received extensive attention. It utilizes multimodal information to alleviate the data sparsity problem in recommendation systems, thus improving recommendation accuracy. However, the reliance on labeled data severely limits the performance of multimodal recommendation models. Recently, self-supervised learning has been used in multimodal recommendations to mitigate the label sparsity problem. Nevertheless, the state-of-the-art methods cannot avoid the modality noise when aligning multimodal information due to the large differences in the distributions of different modalities. To this end, we propose a Multi-level sElf-supervised learNing for mulTimOdal Recommendation (MENTOR) method to address the label sparsity problem and the modality alignment problem. Specifically, MENTOR first enhances the specific features of each modality using the graph convolutional netwo
&lt;/p&gt;</description></item><item><title>OpenMedLM &#25552;&#20986;&#20102;&#19968;&#20010;&#25552;&#31034;&#24179;&#21488;&#65292;&#21033;&#29992;&#25552;&#31034;&#24037;&#31243;&#22312;&#21307;&#23398;&#38382;&#31572;&#20013;&#33021;&#22815;&#36229;&#36234;&#23545;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#23454;&#29616;&#20102;&#22312;&#21307;&#23398;&#22522;&#20934;&#19978;&#30340; SOTA &#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.19371</link><description>&lt;p&gt;
OpenMedLM&#65306;&#22312;&#21307;&#23398;&#38382;&#31572;&#20013;&#65292;&#25552;&#31034;&#24037;&#31243;&#21487;&#20197;&#32988;&#36807;&#23545;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
OpenMedLM: Prompt engineering can out-perform fine-tuning in medical question-answering with open-source large language models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19371
&lt;/p&gt;
&lt;p&gt;
OpenMedLM &#25552;&#20986;&#20102;&#19968;&#20010;&#25552;&#31034;&#24179;&#21488;&#65292;&#21033;&#29992;&#25552;&#31034;&#24037;&#31243;&#22312;&#21307;&#23398;&#38382;&#31572;&#20013;&#33021;&#22815;&#36229;&#36234;&#23545;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#23454;&#29616;&#20102;&#22312;&#21307;&#23398;&#22522;&#20934;&#19978;&#30340; SOTA &#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLMs &#22312;&#23436;&#25104;&#19968;&#31995;&#21015;&#19987;&#38376;&#20219;&#21153;&#26041;&#38754;&#21464;&#24471;&#36234;&#26469;&#36234;&#26377;&#33021;&#21147;&#65292;&#24182;&#19988;&#21487;&#20197;&#29992;&#26469;&#25193;&#22823;&#23545;&#21307;&#23398;&#30693;&#35782;&#30340;&#20844;&#24179;&#35775;&#38382;&#12290;&#22823;&#22810;&#25968;&#21307;&#23398; LLMs &#37117;&#28041;&#21450;&#22823;&#37327;&#24494;&#35843;&#65292;&#21033;&#29992;&#19987;&#38376;&#30340;&#21307;&#23398;&#25968;&#25454;&#21644;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#22240;&#27492;&#25104;&#26412;&#39640;&#26114;&#12290;&#35768;&#22810;&#34920;&#29616;&#21069;&#21015;&#30340; LLMs &#26159;&#19987;&#26377;&#30340;&#65292;&#20182;&#20204;&#30340;&#35775;&#38382;&#20165;&#38480;&#20110;&#23569;&#25968;&#30740;&#31350;&#22242;&#20307;&#12290;&#28982;&#32780;&#65292;&#24320;&#28304;&#65288;OS&#65289;&#27169;&#22411;&#20195;&#34920;&#20102;&#21307;&#23398; LLMs &#30340;&#19968;&#20010;&#37325;&#35201;&#22686;&#38271;&#39046;&#22495;&#65292;&#30001;&#20110;&#24615;&#33021;&#26174;&#33879;&#25552;&#21319;&#20197;&#21450;&#25552;&#20379;&#21355;&#29983;&#20445;&#20581;&#25152;&#38656;&#30340;&#36879;&#26126;&#24230;&#21644;&#21512;&#35268;&#24615;&#30340;&#20869;&#22312;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102; OpenMedLM&#65292;&#36825;&#26159;&#19968;&#20010;&#25552;&#31034;&#24179;&#21488;&#65292;&#20026;&#21307;&#23398;&#22522;&#20934;&#19978;&#30340; OS LLMs &#25552;&#20379;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#22235;&#20010;&#21307;&#23398;&#22522;&#20934;&#65288;MedQA&#12289;MedMCQA&#12289;PubMedQA&#12289;MMLU &#21307;&#23398;&#23376;&#38598;&#65289;&#19978;&#35780;&#20272;&#20102;&#19968;&#31995;&#21015; OS &#22522;&#30784; LLMs&#65288;7B-70B&#65289;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31995;&#21015;&#25552;&#31034;&#31574;&#30053;&#65292;&#21253;&#25324;&#38646;s
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19371v1 Announce Type: cross  Abstract: LLMs have become increasingly capable at accomplishing a range of specialized-tasks and can be utilized to expand equitable access to medical knowledge. Most medical LLMs have involved extensive fine-tuning, leveraging specialized medical data and significant, thus costly, amounts of computational power. Many of the top performing LLMs are proprietary and their access is limited to very few research groups. However, open-source (OS) models represent a key area of growth for medical LLMs due to significant improvements in performance and an inherent ability to provide the transparency and compliance required in healthcare. We present OpenMedLM, a prompting platform which delivers state-of-the-art (SOTA) performance for OS LLMs on medical benchmarks. We evaluated a range of OS foundation LLMs (7B-70B) on four medical benchmarks (MedQA, MedMCQA, PubMedQA, MMLU medical-subset). We employed a series of prompting strategies, including zero-s
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#38899;&#39057;&#25551;&#36848;&#30340;&#26041;&#27861;&#22312;&#33258;&#25105;&#20013;&#24515;&#35270;&#39057;&#35774;&#32622;&#19979;&#30340;&#25991;&#26412;-&#38899;&#39057;&#26816;&#32034;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.19106</link><description>&lt;p&gt;
&#19968;&#31181;&#22768;&#38899;&#26041;&#27861;&#65306;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;&#33258;&#25105;&#20013;&#24515;&#25991;&#26412; - &#38899;&#39057;&#26816;&#32034;&#29983;&#25104;&#38899;&#39057;&#25551;&#36848;
&lt;/p&gt;
&lt;p&gt;
A SOUND APPROACH: Using Large Language Models to generate audio descriptions for egocentric text-audio retrieval
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19106
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#38899;&#39057;&#25551;&#36848;&#30340;&#26041;&#27861;&#22312;&#33258;&#25105;&#20013;&#24515;&#35270;&#39057;&#35774;&#32622;&#19979;&#30340;&#25991;&#26412;-&#38899;&#39057;&#26816;&#32034;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20114;&#32852;&#32593;&#35270;&#39057;&#25968;&#25454;&#24211;&#26159;&#25991;&#26412;-&#38899;&#39057;&#26816;&#32034;&#25968;&#25454;&#38598;&#30340;&#23453;&#36149;&#26469;&#28304;&#12290;&#28982;&#32780;&#65292;&#32771;&#34385;&#21040;&#22768;&#38899;&#21644;&#35270;&#35273;&#27969;&#20195;&#34920;&#25968;&#25454;&#30340;&#19981;&#21516;&#8220;&#35270;&#22270;&#8221;&#65292;&#23558;&#35270;&#35273;&#25551;&#36848;&#35270;&#20026;&#38899;&#39057;&#25551;&#36848;&#36828;&#38750;&#26368;&#20339;&#36873;&#25321;&#12290;&#21363;&#20351;&#23384;&#22312;&#38899;&#39057;&#31867;&#26631;&#31614;&#65292;&#23427;&#20204;&#36890;&#24120;&#20063;&#19981;&#22826;&#35814;&#32454;&#65292;&#20351;&#20854;&#19981;&#36866;&#29992;&#20110;&#25991;&#26412;-&#38899;&#39057;&#26816;&#32034;&#12290;&#20026;&#20102;&#21033;&#29992;&#35270;&#39057;&#25991;&#26412;&#25968;&#25454;&#38598;&#20013;&#30340;&#30456;&#20851;&#38899;&#39057;&#20449;&#24687;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#29983;&#25104;&#20197;&#38899;&#39057;&#20026;&#20013;&#24515;&#25551;&#36848;&#30340;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#33258;&#25105;&#20013;&#24515;&#35270;&#39057;&#35774;&#32622;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;EpicMIR&#21644;EgoMCQ&#20219;&#21153;&#20197;&#21450;EpicSounds&#25968;&#25454;&#38598;&#30340;&#19977;&#20010;&#26032;&#30340;&#25991;&#26412;-&#38899;&#39057;&#26816;&#32034;&#22522;&#20934;&#12290;&#25105;&#20204;&#33719;&#24471;&#38899;&#39057;&#20026;&#20013;&#24515;&#30340;&#25551;&#36848;&#30340;&#26041;&#27861;&#27604;&#20351;&#29992;&#21407;&#22987;&#35270;&#35273;&#20026;&#20013;&#24515;&#30340;&#25551;&#36848;&#20855;&#26377;&#26174;&#30528;&#26356;&#39640;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#30456;&#21516;&#25552;&#31034;&#65292;&#25105;&#20204;&#21487;&#20197;&#25104;&#21151;&#22320;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19106v1 Announce Type: cross  Abstract: Video databases from the internet are a valuable source of text-audio retrieval datasets. However, given that sound and vision streams represent different "views" of the data, treating visual descriptions as audio descriptions is far from optimal. Even if audio class labels are present, they commonly are not very detailed, making them unsuited for text-audio retrieval. To exploit relevant audio information from video-text datasets, we introduce a methodology for generating audio-centric descriptions using Large Language Models (LLMs). In this work, we consider the egocentric video setting and propose three new text-audio retrieval benchmarks based on the EpicMIR and EgoMCQ tasks, and on the EpicSounds dataset. Our approach for obtaining audio-centric descriptions gives significantly higher zero-shot performance than using the original visual-centric descriptions. Furthermore, we show that using the same prompts, we can successfully emp
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#20004;&#38454;&#27573;&#36328;&#23454;&#20307;&#36328;&#22495;&#25512;&#33616;&#30693;&#35782;&#20256;&#36755;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22810;&#23454;&#20307;&#25512;&#33616;&#20013;&#28304;&#23454;&#20307;&#25968;&#25454;&#20998;&#24067;&#19981;&#21516;&#21644;&#29305;&#24449;&#27169;&#24335;&#19981;&#23545;&#40784;&#31561;&#37325;&#35201;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.19101</link><description>&lt;p&gt;
&#26377;&#25928;&#30340;&#20004;&#38454;&#27573;&#36328;&#23454;&#20307;&#36328;&#22495;&#25512;&#33616;&#30693;&#35782;&#20256;&#36755;
&lt;/p&gt;
&lt;p&gt;
Effective Two-Stage Knowledge Transfer for Multi-Entity Cross-Domain Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19101
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#20004;&#38454;&#27573;&#36328;&#23454;&#20307;&#36328;&#22495;&#25512;&#33616;&#30693;&#35782;&#20256;&#36755;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22810;&#23454;&#20307;&#25512;&#33616;&#20013;&#28304;&#23454;&#20307;&#25968;&#25454;&#20998;&#24067;&#19981;&#21516;&#21644;&#29305;&#24449;&#27169;&#24335;&#19981;&#23545;&#40784;&#31561;&#37325;&#35201;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#19978;&#30340;&#25512;&#33616;&#20869;&#23481;&#21464;&#24471;&#36234;&#26469;&#36234;&#20016;&#23500; -- &#21333;&#20010;&#29992;&#25143;&#21453;&#39304;&#21487;&#33021;&#21253;&#21547;&#22810;&#20010;&#23454;&#20307;&#65292;&#22914;&#38144;&#21806;&#20135;&#21697;&#12289;&#30701;&#35270;&#39057;&#21644;&#20869;&#23481;&#24086;&#23376;&#12290;&#20026;&#20102;&#35299;&#20915;&#22810;&#23454;&#20307;&#25512;&#33616;&#38382;&#39064;&#65292;&#19968;&#20010;&#30452;&#35266;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#37319;&#29992;&#22522;&#20110;&#20849;&#20139;&#32593;&#32476;&#30340;&#26550;&#26500;&#36827;&#34892;&#32852;&#21512;&#35757;&#32451;&#12290;&#36825;&#19968;&#24819;&#27861;&#26159;&#23558;&#19968;&#20010;&#31867;&#22411;&#23454;&#20307;&#65288;&#28304;&#23454;&#20307;&#65289;&#20013;&#25552;&#21462;&#30340;&#30693;&#35782;&#20256;&#36755;&#21040;&#21478;&#19968;&#20010;&#31867;&#22411;&#23454;&#20307;&#65288;&#30446;&#26631;&#23454;&#20307;&#65289;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19101v1 Announce Type: cross  Abstract: In recent years, the recommendation content on e-commerce platforms has become increasingly rich -- a single user feed may contain multiple entities, such as selling products, short videos, and content posts. To deal with the multi-entity recommendation problem, an intuitive solution is to adopt the shared-network-based architecture for joint training. The idea is to transfer the extracted knowledge from one type of entity (source entity) to another (target entity). However, different from the conventional same-entity cross-domain recommendation, multi-entity knowledge transfer encounters several important issues: (1) data distributions of the source entity and target entity are naturally different, making the shared-network-based joint training susceptible to the negative transfer issue, (2) more importantly, the corresponding feature schema of each entity is not exactly aligned (e.g., price is an essential feature for selling product
&lt;/p&gt;</description></item><item><title>&#38024;&#23545;&#24102;&#20559;&#22909;&#21453;&#39304;&#30340;&#31215;&#26497;&#22312;&#32447;&#20998;&#31867;&#20248;&#21270;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26082;&#39640;&#25928;&#21448;&#20855;&#26377;&#26368;&#20248;&#21518;&#24724;&#20445;&#35777;&#30340;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.18917</link><description>&lt;p&gt;
&#19981;&#35201;&#20381;&#36182;&#26080;&#36873;&#25321;&#65292;&#19981;&#35201;&#37325;&#22797;&#31227;&#21160;&#65306;&#19968;&#31181;&#29992;&#20110;&#20998;&#31867;&#20248;&#21270;&#30340;&#26368;&#20339;&#12289;&#39640;&#25928;&#21644;&#23454;&#29992;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Stop Relying on No-Choice and Do not Repeat the Moves: Optimal, Efficient and Practical Algorithms for Assortment Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18917
&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#24102;&#20559;&#22909;&#21453;&#39304;&#30340;&#31215;&#26497;&#22312;&#32447;&#20998;&#31867;&#20248;&#21270;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26082;&#39640;&#25928;&#21448;&#20855;&#26377;&#26368;&#20248;&#21518;&#24724;&#20445;&#35777;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35299;&#20915;&#20102;&#24102;&#20559;&#22909;&#21453;&#39304;&#30340;&#31215;&#26497;&#22312;&#32447;&#20998;&#31867;&#20248;&#21270;&#38382;&#39064;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#24314;&#27169;&#29992;&#25143;&#36873;&#25321;&#21644;&#23376;&#38598;&#25928;&#29992;&#26368;&#22823;&#21270;&#30340;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#22312;&#21508;&#31181;&#30495;&#23454;&#24212;&#29992;&#20013;&#38750;&#24120;&#26377;&#29992;&#65292;&#21253;&#25324;&#24191;&#21578;&#25918;&#32622;&#12289;&#22312;&#32447;&#38646;&#21806;&#12289;&#25512;&#33616;&#31995;&#32479;&#12289;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#31561;&#12290;&#34429;&#28982;&#36825;&#20010;&#38382;&#39064;&#36807;&#21435;&#24050;&#32463;&#34987;&#30740;&#31350;&#36807;&#65292;&#20294;&#32570;&#20047;&#30452;&#35266;&#21644;&#23454;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21516;&#26102;&#38656;&#35201;&#39640;&#25928;&#30340;&#31639;&#27861;&#21644;&#26368;&#20248;&#30340;&#21518;&#24724;&#20445;&#35777;&#12290;&#20363;&#22914;&#65292;&#36890;&#24120;&#20351;&#29992;&#30340;&#20998;&#31867;&#36873;&#25321;&#31639;&#27861;&#36890;&#24120;&#38656;&#35201;&#23384;&#22312;&#19968;&#20010;&#22987;&#32456;&#21253;&#21547;&#22312;&#36873;&#25321;&#38598;&#20013;&#30340;&#8220;&#24378;&#21442;&#32771;&#39033;&#8221;&#65292;&#27492;&#22806;&#65292;&#23427;&#20204;&#36824;&#34987;&#35774;&#35745;&#20026;&#37325;&#22797;&#25552;&#20379;&#30456;&#21516;&#30340;&#20998;&#31867;&#65292;&#30452;&#21040;&#21442;&#32771;&#39033;&#34987;&#36873;&#25321; &#8212; &#25152;&#26377;&#36825;&#20123;&#35201;&#27714;&#23545;&#20110;&#23454;&#38469;&#24212;&#29992;&#32780;&#35328;&#37117;&#38750;&#24120;&#19981;&#29616;&#23454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20026;&#20998;&#31867;&#20013;&#30340;&#21518;&#24724;&#26368;&#23567;&#21270;&#38382;&#39064;&#35774;&#35745;&#20102;&#39640;&#25928;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18917v1 Announce Type: new  Abstract: We address the problem of active online assortment optimization problem with preference feedback, which is a framework for modeling user choices and subsetwise utility maximization. The framework is useful in various real-world applications including ad placement, online retail, recommender systems, fine-tuning language models, amongst many. The problem, although has been studied in the past, lacks an intuitive and practical solution approach with simultaneously efficient algorithm and optimal regret guarantee. E.g., popularly used assortment selection algorithms often require the presence of a `strong reference' which is always included in the choice sets, further they are also designed to offer the same assortments repeatedly until the reference item gets selected -- all such requirements are quite unrealistic for practical applications. In this paper, we designed efficient algorithms for the problem of regret minimization in assortmen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#36890;&#29992;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#19982;&#39033;&#30446;&#26816;&#32034;&#20219;&#21153;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#36890;&#36807;&#29983;&#25104;&#29305;&#23450;&#39046;&#22495;&#25968;&#25454;&#38598;&#21644;&#24494;&#35843;&#23884;&#20837;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;&#21508;&#31181;&#26816;&#32034;&#20219;&#21153;&#20013;&#26174;&#30528;&#25913;&#36827;&#24615;&#33021;&#65292;&#21516;&#26102;&#22312;&#20250;&#35805;&#35774;&#32622;&#19979;&#23637;&#31034;&#20102;&#27169;&#22411;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.18899</link><description>&lt;p&gt;
&#29992;&#20110;&#22810;&#21151;&#33021;&#22522;&#20110;&#25991;&#26412;&#30340;&#39033;&#30446;&#26816;&#32034;&#30340;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Aligning Language Models for Versatile Text-based Item Retrieval
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18899
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#36890;&#29992;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#19982;&#39033;&#30446;&#26816;&#32034;&#20219;&#21153;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#36890;&#36807;&#29983;&#25104;&#29305;&#23450;&#39046;&#22495;&#25968;&#25454;&#38598;&#21644;&#24494;&#35843;&#23884;&#20837;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;&#21508;&#31181;&#26816;&#32034;&#20219;&#21153;&#20013;&#26174;&#30528;&#25913;&#36827;&#24615;&#33021;&#65292;&#21516;&#26102;&#22312;&#20250;&#35805;&#35774;&#32622;&#19979;&#23637;&#31034;&#20102;&#27169;&#22411;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35299;&#20915;&#20102;&#36890;&#29992;&#25991;&#26412;&#23884;&#20837;&#21644;&#39033;&#30446;&#26816;&#32034;&#20219;&#21153;&#30340;&#29305;&#23450;&#38656;&#27714;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#29616;&#26377;&#27169;&#22411;&#22312;&#25429;&#25417;&#39033;&#30446;&#26816;&#32034;&#20219;&#21153;&#25152;&#38656;&#30340;&#24494;&#22937;&#20043;&#22788;&#26041;&#38754;&#30340;&#19981;&#36275;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20174;&#19987;&#38376;&#38024;&#23545;&#35299;&#38145;&#27169;&#22411;&#34920;&#31034;&#33021;&#21147;&#20197;&#36827;&#34892;&#39033;&#30446;&#26816;&#32034;&#30340;&#21313;&#39033;&#20219;&#21153;&#29983;&#25104;&#39046;&#22495;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#35813;&#25968;&#25454;&#38598;&#19978;&#24494;&#35843;&#23884;&#20837;&#27169;&#22411;&#20250;&#26174;&#30528;&#25913;&#21892;&#21508;&#31181;&#26816;&#32034;&#20219;&#21153;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25105;&#20204;&#20248;&#21270;&#27169;&#22411;&#22312;&#20250;&#35805;&#35774;&#32622;&#20013;&#30340;&#23454;&#38469;&#24212;&#29992;&#65292;&#23427;&#22686;&#24378;&#20102;&#20687;Chat-Rec&#36825;&#26679;&#30340;&#22522;&#20110;LLM&#30340;&#25512;&#33616;&#20195;&#29702;&#30340;&#21151;&#33021;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#22312;https://github.com/microsoft/RecAI&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18899v1 Announce Type: new  Abstract: This paper addresses the gap between general-purpose text embeddings and the specific demands of item retrieval tasks. We demonstrate the shortcomings of existing models in capturing the nuances necessary for zero-shot performance on item retrieval tasks. To overcome these limitations, we propose generate in-domain dataset from ten tasks tailored to unlocking models' representation ability for item retrieval. Our empirical studies demonstrate that fine-tuning embedding models on the dataset leads to remarkable improvements in a variety of retrieval tasks. We also illustrate the practical application of our refined model in a conversational setting, where it enhances the capabilities of LLM-based Recommender Agents like Chat-Rec. Our code is available at https://github.com/microsoft/RecAI.
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#25512;&#33616;&#33021;&#21147;&#21644;&#35821;&#35328;&#29702;&#35299;&#65292;&#37325;&#22609;&#20102;&#25512;&#33616;&#20219;&#21153;&#30340;&#22522;&#30784;&#12290;</title><link>https://arxiv.org/abs/2402.18590</link><description>&lt;p&gt;
&#25506;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#25512;&#33616;&#31995;&#32479;&#30340;&#24433;&#21709;&#65306;&#19968;&#39033;&#24191;&#27867;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Exploring the Impact of Large Language Models on Recommender Systems: An Extensive Review
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18590
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#25512;&#33616;&#33021;&#21147;&#21644;&#35821;&#35328;&#29702;&#35299;&#65292;&#37325;&#22609;&#20102;&#25512;&#33616;&#20219;&#21153;&#30340;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24378;&#35843;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#37325;&#22609;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#23558;&#23427;&#20204;&#30340;&#20215;&#20540;&#24402;&#22240;&#20110;&#20256;&#32479;&#25512;&#33616;&#31995;&#32479;&#25152;&#32570;&#20047;&#30340;&#29420;&#29305;&#25512;&#29702;&#33021;&#21147;&#12290;&#19981;&#21516;&#20110;&#32570;&#20047;&#30452;&#25509;&#29992;&#25143;&#20114;&#21160;&#25968;&#25454;&#30340;&#20256;&#32479;&#31995;&#32479;&#65292;LLMs&#22312;&#25512;&#33616;&#29289;&#21697;&#26041;&#38754;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#29702;&#35299;&#35821;&#35328;&#22797;&#26434;&#24615;&#26041;&#38754;&#30340;&#29087;&#32451;&#31243;&#24230;&#12290;&#36825;&#26631;&#24535;&#30528;&#25512;&#33616;&#39046;&#22495;&#30340;&#19968;&#20010;&#26681;&#26412;&#24615;&#33539;&#24335;&#36716;&#21464;&#12290;&#22312;&#20805;&#28385;&#27963;&#21147;&#30340;&#30740;&#31350;&#39046;&#22495;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#31215;&#26497;&#21033;&#29992;LLMs&#30340;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#37325;&#26032;&#23450;&#20041;&#25512;&#33616;&#20219;&#21153;&#30340;&#22522;&#30784;&#12290;&#35813;&#30740;&#31350;&#24443;&#24213;&#25506;&#35752;&#20102;LLMs&#22312;&#25512;&#33616;&#26694;&#26550;&#20869;&#22266;&#26377;&#30340;&#20248;&#21183;&#65292;&#21253;&#25324;&#32454;&#33268;&#30340;&#35821;&#22659;&#29702;&#35299;&#65292;&#36328;&#19981;&#21516;&#39046;&#22495;&#30340;&#24179;&#31283;&#36807;&#28193;&#65292;&#37319;&#29992;&#32479;&#19968;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20849;&#20139;&#25968;&#25454;&#27744;&#30340;&#20840;&#38754;&#23398;&#20064;&#31574;&#30053;&#65292;&#36879;&#26126;&#24230;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18590v1 Announce Type: cross  Abstract: The paper underscores the significance of Large Language Models (LLMs) in reshaping recommender systems, attributing their value to unique reasoning abilities absent in traditional recommenders. Unlike conventional systems lacking direct user interaction data, LLMs exhibit exceptional proficiency in recommending items, showcasing their adeptness in comprehending intricacies of language. This marks a fundamental paradigm shift in the realm of recommendations. Amidst the dynamic research landscape, researchers actively harness the language comprehension and generation capabilities of LLMs to redefine the foundations of recommendation tasks. The investigation thoroughly explores the inherent strengths of LLMs within recommendation frameworks, encompassing nuanced contextual comprehension, seamless transitions across diverse domains, adoption of unified approaches, holistic learning strategies leveraging shared data reservoirs, transparent
&lt;/p&gt;</description></item><item><title>Verif.ai&#26159;&#19968;&#20010;&#20855;&#26377;&#24341;&#29992;&#21644;&#21487;&#39564;&#35777;&#31572;&#26696;&#30340;&#24320;&#28304;&#31185;&#23398;&#29983;&#25104;&#24335;&#38382;&#31572;&#31995;&#32479;&#65292;&#36890;&#36807;&#20449;&#24687;&#26816;&#32034;&#12289;&#29983;&#25104;&#27169;&#22411;&#21644;&#39564;&#35777;&#24341;&#25806;&#30340;&#32467;&#21512;&#23454;&#29616;&#23545;&#20027;&#24352;&#30340;&#29983;&#25104;&#21644;&#39564;&#35777;&#12290;</title><link>https://arxiv.org/abs/2402.18589</link><description>&lt;p&gt;
Verif.ai: &#19968;&#31181;&#20855;&#26377;&#24341;&#29992;&#21644;&#21487;&#39564;&#35777;&#31572;&#26696;&#30340;&#24320;&#28304;&#31185;&#23398;&#29983;&#25104;&#24335;&#38382;&#31572;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Verif.ai: Towards an Open-Source Scientific Generative Question-Answering System with Referenced and Verifiable Answers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18589
&lt;/p&gt;
&lt;p&gt;
Verif.ai&#26159;&#19968;&#20010;&#20855;&#26377;&#24341;&#29992;&#21644;&#21487;&#39564;&#35777;&#31572;&#26696;&#30340;&#24320;&#28304;&#31185;&#23398;&#29983;&#25104;&#24335;&#38382;&#31572;&#31995;&#32479;&#65292;&#36890;&#36807;&#20449;&#24687;&#26816;&#32034;&#12289;&#29983;&#25104;&#27169;&#22411;&#21644;&#39564;&#35777;&#24341;&#25806;&#30340;&#32467;&#21512;&#23454;&#29616;&#23545;&#20027;&#24352;&#30340;&#29983;&#25104;&#21644;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#39033;&#30446;Verif.ai&#30340;&#24403;&#21069;&#36827;&#23637;&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#24341;&#29992;&#21644;&#21487;&#39564;&#35777;&#31572;&#26696;&#30340;&#24320;&#28304;&#31185;&#23398;&#29983;&#25104;&#24335;&#38382;&#31572;&#31995;&#32479;&#12290;&#35813;&#31995;&#32479;&#30340;&#32452;&#25104;&#37096;&#20998;&#21253;&#25324;&#65288;1&#65289;&#19968;&#20010;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#65292;&#32467;&#21512;&#35821;&#20041;&#21644;&#35789;&#27719;&#25628;&#32034;&#25216;&#26415;&#23545;&#31185;&#23398;&#35770;&#25991;&#65288;PubMed&#65289;&#36827;&#34892;&#26816;&#32034;&#65292;&#65288;2&#65289;&#19968;&#20010;&#32463;&#36807;&#24494;&#35843;&#30340;&#29983;&#25104;&#27169;&#22411;&#65288;Mistral 7B&#65289;&#65292;&#33719;&#21462;&#21069;&#20960;&#20010;&#31572;&#26696;&#24182;&#29983;&#25104;&#38468;&#26377;&#20174;&#20013;&#24471;&#20986;&#20027;&#24352;&#30340;&#35770;&#25991;&#24341;&#29992;&#30340;&#31572;&#26696;&#65292;&#20197;&#21450;&#65288;3&#65289;&#19968;&#20010;&#39564;&#35777;&#24341;&#25806;&#65292;&#29992;&#20110;&#20132;&#21449;&#26816;&#26597;&#29983;&#25104;&#30340;&#20027;&#24352;&#21644;&#20174;&#20013;&#24471;&#20986;&#20027;&#24352;&#30340;&#25688;&#35201;&#25110;&#35770;&#25991;&#65292;&#39564;&#35777;&#29983;&#25104;&#20027;&#24352;&#26102;&#26159;&#21542;&#23384;&#22312;&#20219;&#20309;&#38169;&#35273;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20379;&#19978;&#19979;&#25991;&#20013;&#30340;&#25688;&#35201;&#21152;&#24378;&#20102;&#29983;&#25104;&#27169;&#22411;&#65292;&#20294;&#27492;&#22806;&#65292;&#19968;&#20010;&#29420;&#31435;&#30340;&#26041;&#27861;&#21644;&#27169;&#22411;&#38598;&#27491;&#22312;&#39564;&#35777;&#31572;&#26696;&#24182;&#26816;&#26597;&#26159;&#21542;&#23384;&#22312;&#38169;&#35273;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30456;&#20449;&#36890;&#36807;&#20351;&#29992;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#31185;&#23398;&#23478;&#20204;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18589v1 Announce Type: cross  Abstract: In this paper, we present the current progress of the project Verif.ai, an open-source scientific generative question-answering system with referenced and verified answers. The components of the system are (1) an information retrieval system combining semantic and lexical search techniques over scientific papers (PubMed), (2) a fine-tuned generative model (Mistral 7B) taking top answers and generating answers with references to the papers from which the claim was derived, and (3) a verification engine that cross-checks the generated claim and the abstract or paper from which the claim was derived, verifying whether there may have been any hallucinations in generating the claim. We are reinforcing the generative model by providing the abstract in context, but in addition, an independent set of methods and models are verifying the answer and checking for hallucinations. Therefore, we believe that by using our method, we can make scientis
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#29983;&#25104;&#24335;&#26381;&#35013;&#25512;&#33616;&#20219;&#21153;&#65288;GOR&#65289;&#65292;&#26088;&#22312;&#21512;&#25104;&#19968;&#32452;&#26102;&#23578;&#22270;&#29255;&#24182;&#32452;&#35013;&#25104;&#35270;&#35273;&#21644;&#35856;&#30340;&#12289;&#23450;&#21046;&#32473;&#20010;&#20154;&#29992;&#25143;&#30340;&#26381;&#35013;&#12290;</title><link>https://arxiv.org/abs/2402.17279</link><description>&lt;p&gt;
DiFashion: &#36808;&#21521;&#20010;&#24615;&#21270;&#26381;&#35013;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
DiFashion: Towards Personalized Outfit Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17279
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#29983;&#25104;&#24335;&#26381;&#35013;&#25512;&#33616;&#20219;&#21153;&#65288;GOR&#65289;&#65292;&#26088;&#22312;&#21512;&#25104;&#19968;&#32452;&#26102;&#23578;&#22270;&#29255;&#24182;&#32452;&#35013;&#25104;&#35270;&#35273;&#21644;&#35856;&#30340;&#12289;&#23450;&#21046;&#32473;&#20010;&#20154;&#29992;&#25143;&#30340;&#26381;&#35013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26381;&#35013;&#25512;&#33616;&#65288;OR&#65289;&#22312;&#26102;&#23578;&#39046;&#22495;&#30340;&#21457;&#23637;&#32463;&#21382;&#20102;&#20004;&#20010;&#19981;&#21516;&#38454;&#27573;&#65306;&#39044;&#23450;&#20041;&#30340;&#26381;&#35013;&#25512;&#33616;&#21644;&#20010;&#24615;&#21270;&#30340;&#26381;&#35013;&#32452;&#21512;&#12290;&#34429;&#28982;&#21462;&#24471;&#20102;&#36825;&#20123;&#36827;&#23637;&#65292;&#20294;&#20004;&#20010;&#38454;&#27573;&#37117;&#38754;&#20020;&#29616;&#26377;&#26102;&#23578;&#20135;&#21697;&#24102;&#26469;&#30340;&#38480;&#21046;&#65292;&#38459;&#30861;&#20102;&#23427;&#20204;&#28385;&#36275;&#29992;&#25143;&#22810;&#26679;&#21270;&#26102;&#23578;&#38656;&#27714;&#30340;&#26377;&#25928;&#24615;&#12290;AI&#29983;&#25104;&#20869;&#23481;&#30340;&#20986;&#29616;&#20026;OR&#20811;&#26381;&#36825;&#20123;&#32422;&#26463;&#38138;&#24179;&#20102;&#36947;&#36335;&#65292;&#23637;&#31034;&#20102;&#20010;&#24615;&#21270;&#26381;&#35013;&#29983;&#25104;&#30340;&#28508;&#21147;&#12290;&#20026;&#20102;&#36861;&#27714;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#39033;&#21517;&#20026;&#29983;&#25104;&#24335;&#26381;&#35013;&#25512;&#33616;&#65288;GOR&#65289;&#30340;&#21019;&#26032;&#20219;&#21153;&#65292;&#20854;&#30446;&#26631;&#26159;&#21512;&#25104;&#19968;&#32452;&#26102;&#23578;&#22270;&#29255;&#65292;&#24182;&#23558;&#23427;&#20204;&#32452;&#35013;&#25104;&#35270;&#35273;&#21644;&#35856;&#30340;&#12289;&#23450;&#21046;&#32473;&#20010;&#20154;&#29992;&#25143;&#30340;&#26381;&#35013;&#12290;GOR&#30340;&#20027;&#35201;&#30446;&#26631;&#38598;&#20013;&#22312;&#23454;&#29616;&#29983;&#25104;&#26381;&#35013;&#30340;&#39640;&#20445;&#30495;&#24230;&#12289;&#20860;&#23481;&#24615;&#21644;&#20010;&#24615;&#21270;&#12290;&#20026;&#23454;&#29616;&#36825;&#20123;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DiFashion&#65292;&#19968;&#20010;&#29983;&#25104;&#24335;&#26381;&#35013;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17279v1 Announce Type: new  Abstract: The evolution of Outfit Recommendation (OR) in the realm of fashion has progressed through two distinct phases: Pre-defined Outfit Recommendation and Personalized Outfit Composition. Despite these advancements, both phases face limitations imposed by existing fashion products, hindering their effectiveness in meeting users' diverse fashion needs. The emergence of AI-generated content has paved the way for OR to overcome these constraints, demonstrating the potential for personalized outfit generation.   In pursuit of this, we introduce an innovative task named Generative Outfit Recommendation (GOR), with the goal of synthesizing a set of fashion images and assembling them to form visually harmonious outfits customized to individual users. The primary objectives of GOR revolve around achieving high fidelity, compatibility, and personalization of the generated outfits. To accomplish these, we propose DiFashion, a generative outfit recommen
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#24335;&#36879;&#26126;&#30340;&#39034;&#24207;&#25512;&#33616;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#39033;&#30446;&#24207;&#21015;&#20998;&#35299;&#20026;&#22810;&#32423;&#27169;&#24335;&#24182;&#22312;&#27010;&#29575;&#31354;&#38388;&#20013;&#37327;&#21270;&#27599;&#20010;&#27169;&#24335;&#23545;&#32467;&#26524;&#30340;&#36129;&#29486;&#65292;&#23454;&#29616;&#20102;&#36879;&#26126;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2402.11480</link><description>&lt;p&gt;
&#27169;&#24335;&#36879;&#26126;&#30340;&#39034;&#24207;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Pattern-wise Transparent Sequential Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11480
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#24335;&#36879;&#26126;&#30340;&#39034;&#24207;&#25512;&#33616;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#39033;&#30446;&#24207;&#21015;&#20998;&#35299;&#20026;&#22810;&#32423;&#27169;&#24335;&#24182;&#22312;&#27010;&#29575;&#31354;&#38388;&#20013;&#37327;&#21270;&#27599;&#20010;&#27169;&#24335;&#23545;&#32467;&#26524;&#30340;&#36129;&#29486;&#65292;&#23454;&#29616;&#20102;&#36879;&#26126;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36879;&#26126;&#30340;&#20915;&#31574;&#36807;&#31243;&#23545;&#20110;&#24320;&#21457;&#21487;&#38752;&#21644;&#20540;&#24471;&#20449;&#36182;&#30340;&#25512;&#33616;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#12290;&#23545;&#20110;&#39034;&#24207;&#25512;&#33616;&#26469;&#35828;&#65292;&#24847;&#21619;&#30528;&#27169;&#22411;&#33021;&#22815;&#35782;&#21035;&#20851;&#38190;&#39033;&#30446;&#20316;&#20026;&#20854;&#25512;&#33616;&#32467;&#26524;&#30340;&#29702;&#30001;&#12290;&#28982;&#32780;&#65292;&#21516;&#26102;&#23454;&#29616;&#27169;&#22411;&#36879;&#26126;&#24230;&#21644;&#25512;&#33616;&#24615;&#33021;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#23558;&#25972;&#20010;&#39033;&#30446;&#24207;&#21015;&#20316;&#20026;&#36755;&#20837;&#32780;&#19981;&#21152;&#31579;&#36873;&#30340;&#27169;&#22411;&#32780;&#35328;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PTSR&#30340;&#21487;&#35299;&#37322;&#26694;&#26550;&#65292;&#23427;&#23454;&#29616;&#20102;&#19968;&#31181;&#27169;&#24335;&#36879;&#26126;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;&#23427;&#23558;&#39033;&#30446;&#24207;&#21015;&#20998;&#35299;&#20026;&#22810;&#32423;&#27169;&#24335;&#65292;&#36825;&#20123;&#27169;&#24335;&#20316;&#20026;&#25972;&#20010;&#25512;&#33616;&#36807;&#31243;&#30340;&#21407;&#23376;&#21333;&#20803;&#12290;&#27599;&#20010;&#27169;&#24335;&#23545;&#32467;&#26524;&#30340;&#36129;&#29486;&#22312;&#27010;&#29575;&#31354;&#38388;&#20013;&#24471;&#21040;&#37327;&#21270;&#12290;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#27169;&#24335;&#21152;&#26435;&#26657;&#27491;&#65292;&#21363;&#20351;&#22312;&#27809;&#26377;&#30495;&#23454;&#20851;&#38190;&#27169;&#24335;&#30340;&#24773;&#20917;&#19979;&#65292;&#20063;&#33021;&#23398;&#20064;&#27169;&#24335;&#30340;&#36129;&#29486;&#12290;&#26368;&#32456;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11480v1 Announce Type: new  Abstract: A transparent decision-making process is essential for developing reliable and trustworthy recommender systems. For sequential recommendation, it means that the model can identify critical items asthe justifications for its recommendation results. However, achieving both model transparency and recommendation performance simultaneously is challenging, especially for models that take the entire sequence of items as input without screening. In this paper,we propose an interpretable framework (named PTSR) that enables a pattern-wise transparent decision-making process. It breaks the sequence of items into multi-level patterns that serve as atomic units for the entire recommendation process. The contribution of each pattern to the outcome is quantified in the probability space. With a carefully designed pattern weighting correction, the pattern contribution can be learned in the absence of ground-truth critical patterns. The final recommended
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#36328;&#39046;&#22495;&#24207;&#21015;&#25512;&#33616;&#30340;&#33258;&#36866;&#24212;&#22810;&#20852;&#36259;&#21435;&#20559;&#35265;&#26694;&#26550;&#65288;AMID&#65289;&#65292;&#22312;&#24320;&#25918;&#19990;&#30028;&#20551;&#35774;&#19979;&#35774;&#35745;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#22312;&#22312;&#32447;&#30495;&#23454;&#24179;&#21488;&#19978;&#30001;&#20110;&#25968;&#25454;&#20998;&#24067;&#36716;&#31227;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2311.04590</link><description>&lt;p&gt;
&#22312;&#24320;&#25918;&#19990;&#30028;&#20551;&#35774;&#19979;&#37325;&#26032;&#24605;&#32771;&#36328;&#39046;&#22495;&#24207;&#21015;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Rethinking Cross-Domain Sequential Recommendation under Open-World Assumptions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.04590
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#36328;&#39046;&#22495;&#24207;&#21015;&#25512;&#33616;&#30340;&#33258;&#36866;&#24212;&#22810;&#20852;&#36259;&#21435;&#20559;&#35265;&#26694;&#26550;&#65288;AMID&#65289;&#65292;&#22312;&#24320;&#25918;&#19990;&#30028;&#20551;&#35774;&#19979;&#35774;&#35745;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#22312;&#22312;&#32447;&#30495;&#23454;&#24179;&#21488;&#19978;&#30001;&#20110;&#25968;&#25454;&#20998;&#24067;&#36716;&#31227;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#39046;&#22495;&#39034;&#24207;&#25512;&#33616;&#65288;CDSR&#65289;&#26041;&#27861;&#26088;&#22312;&#35299;&#20915;&#21333;&#19968;&#39046;&#22495;&#39034;&#24207;&#25512;&#33616;&#65288;SDSR&#65289;&#20013;&#23384;&#22312;&#30340;&#25968;&#25454;&#31232;&#30095;&#21644;&#20919;&#21551;&#21160;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;CDSR&#20316;&#21697;&#35774;&#35745;&#20854;&#31934;&#24515;&#30340;&#32467;&#26500;&#65292;&#20381;&#36182;&#20110;&#37325;&#21472;&#29992;&#25143;&#26469;&#20256;&#25773;&#36328;&#39046;&#22495;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;CDSR&#26041;&#27861;&#37319;&#29992;&#23553;&#38381;&#19990;&#30028;&#20551;&#35774;&#65292;&#20551;&#35774;&#22312;&#22810;&#20010;&#39046;&#22495;&#20043;&#38388;&#23436;&#20840;&#37325;&#21472;&#30340;&#29992;&#25143;&#65292;&#24182;&#19988;&#25968;&#25454;&#20998;&#24067;&#20174;&#35757;&#32451;&#29615;&#22659;&#21040;&#27979;&#35797;&#29615;&#22659;&#20445;&#25345;&#19981;&#21464;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#22240;&#25968;&#25454;&#20998;&#24067;&#36716;&#31227;&#32780;&#22312;&#22312;&#32447;&#30495;&#23454;&#24179;&#21488;&#19978;&#34920;&#29616;&#36739;&#24046;&#12290;&#20026;&#20102;&#22312;&#24320;&#25918;&#19990;&#30028;&#20551;&#35774;&#19979;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#33258;&#36866;&#24212;&#22810;&#20852;&#36259;&#21435;&#20559;&#35265;&#26694;&#26550;&#65292;&#29992;&#20110;&#36328;&#39046;&#22495;&#24207;&#21015;&#25512;&#33616;&#65288;AMID&#65289;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#20010;&#22810;&#20852;&#36259;&#20449;&#24687;&#27169;&#22359;&#65288;MIM&#65289;&#21644;&#19968;&#20010;&#21452;&#37325;&#40065;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.04590v3 Announce Type: replace  Abstract: Cross-Domain Sequential Recommendation (CDSR) methods aim to tackle the data sparsity and cold-start problems present in Single-Domain Sequential Recommendation (SDSR). Existing CDSR works design their elaborate structures relying on overlapping users to propagate the cross-domain information. However, current CDSR methods make closed-world assumptions, assuming fully overlapping users across multiple domains and that the data distribution remains unchanged from the training environment to the test environment. As a result, these methods typically result in lower performance on online real-world platforms due to the data distribution shifts. To address these challenges under open-world assumptions, we design an \textbf{A}daptive \textbf{M}ulti-\textbf{I}nterest \textbf{D}ebiasing framework for cross-domain sequential recommendation (\textbf{AMID}), which consists of a multi-interest information module (\textbf{MIM}) and a doubly robu
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#31639;&#23884;&#20837;&#34920;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#25512;&#33616;&#31995;&#32479;&#20013;&#22266;&#23450;&#23884;&#20837;&#22823;&#23567;&#38590;&#20197;&#25193;&#23637;&#30340;&#38382;&#39064;&#65292;&#33021;&#22815;&#26377;&#25928;&#24212;&#23545;&#19981;&#21516;&#29992;&#25143;&#21644;&#39033;&#30446;&#30340;&#22810;&#26679;&#24615;&#23884;&#20837;&#22823;&#23567;&#12290;</title><link>https://arxiv.org/abs/2310.14884</link><description>&lt;p&gt;
&#38754;&#21521;&#25512;&#33616;&#31995;&#32479;&#30340;&#39044;&#31639;&#23884;&#20837;&#34920;
&lt;/p&gt;
&lt;p&gt;
Budgeted Embedding Table For Recommender Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.14884
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#31639;&#23884;&#20837;&#34920;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#25512;&#33616;&#31995;&#32479;&#20013;&#22266;&#23450;&#23884;&#20837;&#22823;&#23567;&#38590;&#20197;&#25193;&#23637;&#30340;&#38382;&#39064;&#65292;&#33021;&#22815;&#26377;&#25928;&#24212;&#23545;&#19981;&#21516;&#29992;&#25143;&#21644;&#39033;&#30446;&#30340;&#22810;&#26679;&#24615;&#23884;&#20837;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20170;&#25512;&#33616;&#31995;&#32479;&#30340;&#26680;&#24515;&#26159;&#25552;&#20379;&#32473;&#29992;&#25143;&#20248;&#36136;&#25512;&#33616;&#20307;&#39564;&#30340;&#28508;&#22312;&#22240;&#32032;&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#20351;&#29992;&#23884;&#20837;&#21521;&#37327;&#26469;&#34920;&#31034;&#29992;&#25143;&#21644;&#39033;&#30446;&#12290;&#26368;&#36817;&#30340;&#36731;&#37327;&#32423;&#23884;&#20837;&#26041;&#27861;&#20351;&#19981;&#21516;&#29992;&#25143;&#21644;&#39033;&#30446;&#33021;&#22815;&#20855;&#26377;&#19981;&#21516;&#30340;&#23884;&#20837;&#22823;&#23567;&#65292;&#20294;&#36890;&#24120;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#32570;&#28857;&#12290;&#39318;&#20808;&#65292;&#23427;&#20204;&#23558;&#23884;&#20837;&#22823;&#23567;&#25628;&#32034;&#38480;&#21046;&#22312;&#20248;&#21270;&#21551;&#21457;&#24335;&#24179;&#34913;&#25512;&#33616;&#36136;&#37327;&#21644;&#20869;&#23384;&#22797;&#26434;&#24615;&#30340;&#33539;&#22260;&#20869;&#65292;&#20854;&#20013;&#25240;&#34935;&#31995;&#25968;&#38656;&#35201;&#20026;&#27599;&#20010;&#20869;&#23384;&#39044;&#31639;&#25163;&#21160;&#35843;&#25972;&#12290;&#38544;&#24335;&#24378;&#21046;&#30340;&#20869;&#23384;&#22797;&#26434;&#24615;&#39033;&#29978;&#33267;&#21487;&#33021;&#26080;&#27861;&#38480;&#21046;&#21442;&#25968;&#20351;&#29992;&#37327;&#65292;&#20351;&#24471;&#24471;&#21040;&#30340;&#23884;&#20837;&#34920;&#26080;&#27861;&#20005;&#26684;&#28385;&#36275;&#20869;&#23384;&#39044;&#31639;&#12290;&#20854;&#27425;&#65292;&#22823;&#22810;&#25968;&#35299;&#20915;&#26041;&#26696;&#65292;&#29305;&#21035;&#26159;&#8230;&#8230;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.14884v4 Announce Type: replace  Abstract: At the heart of contemporary recommender systems (RSs) are latent factor models that provide quality recommendation experience to users. These models use embedding vectors, which are typically of a uniform and fixed size, to represent users and items. As the number of users and items continues to grow, this design becomes inefficient and hard to scale. Recent lightweight embedding methods have enabled different users and items to have diverse embedding sizes, but are commonly subject to two major drawbacks. Firstly, they limit the embedding size search to optimizing a heuristic balancing the recommendation quality and the memory complexity, where the trade-off coefficient needs to be manually tuned for every memory budget requested. The implicitly enforced memory complexity term can even fail to cap the parameter usage, making the resultant embedding table fail to meet the memory budget strictly. Secondly, most solutions, especially 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ReLLa&#30340;&#26816;&#32034;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65292;&#29992;&#20110;&#38646;&#26679;&#26412;&#21644;&#23567;&#26679;&#26412;&#25512;&#33616;&#20219;&#21153;&#12290;&#36890;&#36807;&#35821;&#20041;&#29992;&#25143;&#34892;&#20026;&#26816;&#32034;&#65288;SUBR&#65289;&#26469;&#25552;&#21462;&#19978;&#19979;&#25991;&#20013;&#30340;&#26377;&#29992;&#20449;&#24687;&#65292;&#20197;&#25913;&#21892;LLMs&#30340;&#25512;&#33616;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.11131</link><description>&lt;p&gt;
ReLLa: &#22522;&#20110;&#26816;&#32034;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#29983;&#21629;&#21608;&#26399;&#24207;&#21015;&#34892;&#20026;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
ReLLa: Retrieval-enhanced Large Language Models for Lifelong Sequential Behavior Comprehension in Recommendation. (arXiv:2308.11131v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11131
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ReLLa&#30340;&#26816;&#32034;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65292;&#29992;&#20110;&#38646;&#26679;&#26412;&#21644;&#23567;&#26679;&#26412;&#25512;&#33616;&#20219;&#21153;&#12290;&#36890;&#36807;&#35821;&#20041;&#29992;&#25143;&#34892;&#20026;&#26816;&#32034;&#65288;SUBR&#65289;&#26469;&#25552;&#21462;&#19978;&#19979;&#25991;&#20013;&#30340;&#26377;&#29992;&#20449;&#24687;&#65292;&#20197;&#25913;&#21892;LLMs&#30340;&#25512;&#33616;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#31361;&#30772;&#65292;&#22522;&#20110;LLM&#30340;&#25512;&#33616;&#31995;&#32479;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#24182;&#34987;&#31215;&#26497;&#25506;&#32034;&#12290;&#26412;&#25991;&#19987;&#27880;&#20110;&#36866;&#24212;&#21644;&#22686;&#24378;&#32431;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#29992;&#20110;&#38646;&#26679;&#26412;&#21644;&#23567;&#26679;&#26412;&#25512;&#33616;&#20219;&#21153;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#38024;&#23545;&#25512;&#33616;&#39046;&#22495;&#20013;LLMs&#26080;&#27861;&#20174;&#38271;&#29992;&#25143;&#34892;&#20026;&#24207;&#21015;&#30340;&#25991;&#26412;&#19978;&#19979;&#25991;&#20013;&#25552;&#21462;&#26377;&#29992;&#20449;&#24687;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#24182;&#23450;&#20041;&#20102;&#29983;&#21629;&#21608;&#26399;&#24207;&#21015;&#34892;&#20026;&#29702;&#35299;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#24182;&#25552;&#39640;LLMs&#30340;&#25512;&#33616;&#24615;&#33021;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21363;&#26816;&#32034;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;ReLLa&#65289;&#12290;&#38024;&#23545;&#38646;&#26679;&#26412;&#25512;&#33616;&#65292;&#25105;&#20204;&#25191;&#34892;&#35821;&#20041;&#29992;&#25143;&#34892;&#20026;&#26816;&#32034;&#65288;SUBR&#65289;&#26469;&#25552;&#39640;&#25968;&#25454;&#30340;&#21033;&#29992;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
With large language models (LLMs) achieving remarkable breakthroughs in natural language processing (NLP) domains, LLM-enhanced recommender systems have received much attention and have been actively explored currently. In this paper, we focus on adapting and empowering a pure large language model for zero-shot and few-shot recommendation tasks. First and foremost, we identify and formulate the lifelong sequential behavior incomprehension problem for LLMs in recommendation domains, i.e., LLMs fail to extract useful information from a textual context of long user behavior sequence, even if the length of context is far from reaching the context limitation of LLMs. To address such an issue and improve the recommendation performance of LLMs, we propose a novel framework, namely Retrieval-enhanced Large Language models (ReLLa) for recommendation tasks in both zero-shot and few-shot settings. For zero-shot recommendation, we perform semantic user behavior retrieval (SUBR) to improve the data
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;robot.txt&#38480;&#21046;&#19979;&#30340;&#32593;&#32476;&#29228;&#34411;&#31574;&#30053;&#65292;&#35752;&#35770;&#20102;&#25628;&#32034;&#24341;&#25806;&#22914;&#20309;&#30830;&#23450;&#32593;&#39029;&#25490;&#21517;&#20197;&#21450;&#22914;&#20309;&#33719;&#21462;&#25968;&#25454;&#24211;&#20013;&#30340;&#32593;&#39029;&#12290;&#24182;&#20171;&#32461;&#20102;&#26426;&#22120;&#20154;&#25490;&#38500;&#21327;&#35758;&#35268;&#21017;&#21644;robot.txt&#25991;&#20214;&#30340;&#22522;&#26412;&#26684;&#24335;&#12290;</title><link>http://arxiv.org/abs/2308.04689</link><description>&lt;p&gt;
&#32593;&#32476;&#29228;&#34411;&#22312;robot.txt&#38480;&#21046;&#19979;&#30340;&#31574;&#30053;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
web crawler strategies for web pages under robot.txt restriction. (arXiv:2308.04689v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04689
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;robot.txt&#38480;&#21046;&#19979;&#30340;&#32593;&#32476;&#29228;&#34411;&#31574;&#30053;&#65292;&#35752;&#35770;&#20102;&#25628;&#32034;&#24341;&#25806;&#22914;&#20309;&#30830;&#23450;&#32593;&#39029;&#25490;&#21517;&#20197;&#21450;&#22914;&#20309;&#33719;&#21462;&#25968;&#25454;&#24211;&#20013;&#30340;&#32593;&#39029;&#12290;&#24182;&#20171;&#32461;&#20102;&#26426;&#22120;&#20154;&#25490;&#38500;&#21327;&#35758;&#35268;&#21017;&#21644;robot.txt&#25991;&#20214;&#30340;&#22522;&#26412;&#26684;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20170;&#65292;&#25152;&#26377;&#20154;&#37117;&#20102;&#35299;&#20114;&#32852;&#32593;&#24182;&#27599;&#22825;&#22312;&#20114;&#32852;&#32593;&#19978;&#24037;&#20316;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#20026;&#29992;&#25143;&#36755;&#20837;&#30340;&#20851;&#38190;&#23383;&#36827;&#34892;&#25628;&#32034;&#30340;&#25628;&#32034;&#24341;&#25806;&#12290;&#25628;&#32034;&#24341;&#25806;&#20351;&#29992;&#19981;&#21516;&#30340;&#25628;&#32034;&#31639;&#27861;&#65292;&#20026;&#19978;&#32593;&#32773;&#25552;&#20379;&#26041;&#20415;&#30340;&#32467;&#26524;&#12290;&#19978;&#32593;&#32773;&#36873;&#25321;&#25490;&#21517;&#38752;&#21069;&#30340;&#25628;&#32034;&#32467;&#26524;&#65292;&#20294;&#26159;&#32593;&#39029;&#30340;&#25490;&#21517;&#26159;&#22914;&#20309;&#30001;&#25628;&#32034;&#24341;&#25806;&#30830;&#23450;&#30340;&#65311;&#25628;&#32034;&#24341;&#25806;&#22914;&#20309;&#33719;&#21462;&#25968;&#25454;&#24211;&#20013;&#30340;&#25152;&#26377;&#32593;&#39029;&#65311;&#26412;&#25991;&#32473;&#20986;&#20102;&#25152;&#26377;&#36825;&#20123;&#22522;&#26412;&#38382;&#39064;&#30340;&#31572;&#26696;&#12290;&#26412;&#30740;&#31350;&#35770;&#25991;&#36824;&#35752;&#35770;&#20102;&#20026;&#25628;&#32034;&#24341;&#25806;&#24037;&#20316;&#30340;&#32593;&#32476;&#29228;&#34411;&#21644;&#32593;&#32476;&#29228;&#34411;&#30340;&#26426;&#22120;&#20154;&#25490;&#38500;&#21327;&#35758;&#35268;&#21017;&#12290;&#32593;&#31449;&#31649;&#29702;&#21592;&#20351;&#29992;robot.txt&#25991;&#20214;&#20013;&#30340;&#19981;&#21516;&#38480;&#21046;&#35268;&#21017;&#25351;&#23548;&#32593;&#32476;&#29228;&#34411;&#65292;&#26412;&#25991;&#36824;&#25552;&#21040;&#20102;&#19968;&#20123;&#22522;&#26412;&#30340;robot.txt&#26684;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the present time, all know about World Wide Web and work over the Internet daily. In this paper, we introduce the search engines working for keywords that are entered by users to find something. The search engine uses different search algorithms for convenient results for providing to the net surfer. Net surfers go with the top search results but how did the results of web pages get higher ranks over search engines? how the search engine got that all the web pages in the database? This paper gives the answers to all these kinds of basic questions. Web crawlers working for search engines and robot exclusion protocol rules for web crawlers are also addressed in this research paper. Webmaster uses different restriction facts in robot.txt file to instruct web crawler, some basic formats of robot.txt are also mentioned in this paper.
&lt;/p&gt;</description></item></channel></rss>