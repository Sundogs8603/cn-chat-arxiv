<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#22312;IR&#35780;&#20272;&#20013;&#22788;&#29702;&#22810;&#32500;&#24230;&#21644;&#22810;&#32423;&#30456;&#20851;&#24615;&#35780;&#20272;&#65292;&#25991;&#26723;&#37325;&#21472;&#20197;&#21450;&#22914;&#20309;&#23558;&#25991;&#26723;&#30340;&#21487;&#29992;&#24615;&#23646;&#24615;&#19982;&#30456;&#20851;&#24615;&#35780;&#20272;&#30456;&#32467;&#21512;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#27491;&#24335;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.00747</link><description>&lt;p&gt;
&#19968;&#31181;&#34701;&#21512;&#20219;&#21153;&#19982;&#29992;&#25143;&#29305;&#24449;&#30340;IR&#35780;&#20272;&#34013;&#22270;&#65306;&#27979;&#35797;&#38598;&#19982;&#35780;&#20272;&#25351;&#26631;
&lt;/p&gt;
&lt;p&gt;
A Blueprint of IR Evaluation Integrating Task and User Characteristics: Test Collection and Evaluation Metrics. (arXiv:2305.00747v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00747
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#22312;IR&#35780;&#20272;&#20013;&#22788;&#29702;&#22810;&#32500;&#24230;&#21644;&#22810;&#32423;&#30456;&#20851;&#24615;&#35780;&#20272;&#65292;&#25991;&#26723;&#37325;&#21472;&#20197;&#21450;&#22914;&#20309;&#23558;&#25991;&#26723;&#30340;&#21487;&#29992;&#24615;&#23646;&#24615;&#19982;&#30456;&#20851;&#24615;&#35780;&#20272;&#30456;&#32467;&#21512;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#27491;&#24335;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#20851;&#24615;&#36890;&#24120;&#34987;&#29702;&#35299;&#20026;&#20449;&#24687;&#38656;&#27714;&#19982;&#20449;&#24687;&#23545;&#35937;&#20043;&#38388;&#30340;&#22810;&#32423;&#21644;&#22810;&#32500;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;IR&#35780;&#20272;&#25351;&#26631;&#36807;&#20110;&#31616;&#21333;&#21270;&#65292;&#20551;&#23450;&#30456;&#20851;&#24615;&#26159;&#21333;&#19968;&#32500;&#24230;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20960;&#20010;&#38382;&#39064;&#65306;&#22914;&#20309;&#22788;&#29702;IR&#35780;&#20272;&#20013;&#30340;&#22810;&#32500;&#24230;&#21644;&#22810;&#32423;&#30456;&#20851;&#24615;&#35780;&#20272;&#65311;&#22914;&#20309;&#22788;&#29702;&#25991;&#26723;&#30340;&#37325;&#21472;&#21644;&#35780;&#20272;&#65311;&#22914;&#20309;&#23558;&#25991;&#26723;&#30340;&#21487;&#29992;&#24615;&#23646;&#24615;&#19982;&#22810;&#32500;&#24230;&#30456;&#20851;&#24615;&#35780;&#20272;&#30456;&#32467;&#21512;&#65311;&#26368;&#32456;&#65292;&#25105;&#20204;&#25506;&#35752;&#22914;&#20309;&#23450;&#20041;&#19968;&#20010;&#27491;&#24335;&#30340;&#27169;&#22411;&#65292;&#20197;&#22788;&#29702;&#22810;&#32500;&#24230;&#20998;&#32423;&#30456;&#20851;&#24615;&#35780;&#20272;&#12289;&#25991;&#26723;&#37325;&#21472;&#21644;&#25991;&#26723;&#21487;&#29992;&#24615;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Relevance is generally understood as a multi-level and multi-dimensional relationship between an information need and an information object. However, traditional IR evaluation metrics naively assume mono-dimensionality. We ask: How to deal with multidimensional and graded relevance assessments in IR evaluation? Moreover, search result evaluation metrics neglect document overlaps and naively assume gains piling up as the searcher examines the ranked list into greater length. Consequently, we examine: How to deal with document overlap in IR evaluation? The usability of a document for a person-in-need also depends on document usability attributes beyond relevance. Therefore, we ask: How to deal with usability attributes, and how to combine this with multidimensional relevance assessments in IR evaluation? Finally, we ask how to define a formal model, which deals with multidimensional graded relevance assessments, document overlaps, and document usability attributes in a coherent framework
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#33258;&#21160;&#23545;&#35805;&#31995;&#32479;&#36827;&#34892;&#24066;&#22330;&#30740;&#31350;&#32467;&#26500;&#21270;&#38754;&#35797;&#30340;&#25361;&#25112;&#65292;&#24182;&#36890;&#36807;&#23558;&#22810;&#39033;&#36873;&#25321;&#39064;&#36716;&#21270;&#20026;&#23545;&#35805;&#26684;&#24335;&#24182;&#36827;&#34892;&#29992;&#25143;&#30740;&#31350;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2305.00577</link><description>&lt;p&gt;
&#33258;&#21160;&#21270;&#32467;&#26500;&#21270;&#38754;&#35797;&#30340;&#35821;&#22659;&#21709;&#24212;&#35299;&#37322;&#65306;&#20197;&#24066;&#22330;&#30740;&#31350;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Contextual Response Interpretation for Automated Structured Interviews: A Case Study in Market Research. (arXiv:2305.00577v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00577
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#33258;&#21160;&#23545;&#35805;&#31995;&#32479;&#36827;&#34892;&#24066;&#22330;&#30740;&#31350;&#32467;&#26500;&#21270;&#38754;&#35797;&#30340;&#25361;&#25112;&#65292;&#24182;&#36890;&#36807;&#23558;&#22810;&#39033;&#36873;&#25321;&#39064;&#36716;&#21270;&#20026;&#23545;&#35805;&#26684;&#24335;&#24182;&#36827;&#34892;&#29992;&#25143;&#30740;&#31350;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#26500;&#21270;&#38754;&#35797;&#22312;&#35768;&#22810;&#22330;&#26223;&#19979;&#34987;&#20351;&#29992;&#65292;&#23588;&#20854;&#26159;&#22312;&#21697;&#29260;&#24863;&#30693;&#12289;&#23458;&#25143;&#20064;&#24815;&#25110;&#20559;&#22909;&#31561;&#24066;&#22330;&#30740;&#31350;&#20013;&#12290;&#36825;&#20123;&#38754;&#35797;&#19968;&#33324;&#30001;&#19968;&#31995;&#21015;&#38382;&#39064;&#32452;&#25104;&#65292;&#24182;&#30001;&#29087;&#32451;&#30340;&#38754;&#35797;&#23448;&#36827;&#34892;&#35299;&#37322;&#12290;&#20351;&#29992;&#33258;&#21160;&#23545;&#35805;&#31995;&#32479;&#26469;&#36827;&#34892;&#36825;&#20123;&#38754;&#35797;&#21487;&#20197;&#35302;&#36798;&#26356;&#24191;&#27867;&#21644;&#22810;&#26679;&#21270;&#30340;&#21463;&#35775;&#32773;&#32676;&#20307;&#65292;&#20294;&#30456;&#24212;&#25216;&#26415;&#25361;&#25112;&#23578;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#35813;&#25991;&#23558;&#24066;&#22330;&#30740;&#31350;&#22810;&#39033;&#36873;&#25321;&#39064;&#36716;&#21270;&#20026;&#23545;&#35805;&#26684;&#24335;&#65292;&#24182;&#36827;&#34892;&#20102;&#29992;&#25143;&#30740;&#31350;&#65292;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#36825;&#20123;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Structured interviews are used in many settings, importantly in market research on topics such as brand perception, customer habits, or preferences, which are critical to product development, marketing, and e-commerce at large. Such interviews generally consist of a series of questions that are asked to a participant. These interviews are typically conducted by skilled interviewers, who interpret the responses from the participants and can adapt the interview accordingly. Using automated conversational agents to conduct such interviews would enable reaching a much larger and potentially more diverse group of participants than currently possible. However, the technical challenges involved in building such a conversational system are relatively unexplored. To learn more about these challenges, we convert a market research multiple-choice questionnaire to a conversational format and conduct a user study. We address the key task of conducting structured interviews, namely interpreting the 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35752;&#35770;&#20102;&#35299;&#37322;&#33021;&#21147;&#30340;&#21453;&#38754;&#65292;&#30740;&#31350;&#20351;&#29992;&#21453;&#20107;&#23454;&#20363;&#23376;&#27745;&#26579;&#25512;&#33616;&#31995;&#32479;&#12290;&#23454;&#39564;&#26174;&#31034;&#65292;&#20182;&#20204;&#30340;&#31574;&#30053;&#33021;&#22815;&#25104;&#21151;&#24178;&#25200;&#25512;&#33616;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.00574</link><description>&lt;p&gt;
&#35299;&#37322;&#30340;&#40657;&#26263;&#38754;&#65306;&#29992;&#21453;&#20107;&#23454;&#20363;&#23376;&#27745;&#26579;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
The Dark Side of Explanations: Poisoning Recommender Systems with Counterfactual Examples. (arXiv:2305.00574v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00574
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35752;&#35770;&#20102;&#35299;&#37322;&#33021;&#21147;&#30340;&#21453;&#38754;&#65292;&#30740;&#31350;&#20351;&#29992;&#21453;&#20107;&#23454;&#20363;&#23376;&#27745;&#26579;&#25512;&#33616;&#31995;&#32479;&#12290;&#23454;&#39564;&#26174;&#31034;&#65292;&#20182;&#20204;&#30340;&#31574;&#30053;&#33021;&#22815;&#25104;&#21151;&#24178;&#25200;&#25512;&#33616;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#25512;&#33616;&#31995;&#32479;&#24050;&#32463;&#25104;&#20026;&#20960;&#20010;&#22312;&#32447;&#24179;&#21488;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#40657;&#21283;&#23376;&#26412;&#36136;&#24378;&#35843;&#20102;&#23545;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26041;&#27861;&#30340;&#38656;&#27714;&#65292;&#20197;&#25552;&#20379;&#20154;&#31867;&#21487;&#20197;&#29702;&#35299;&#30340;&#21407;&#22240;&#65292;&#35828;&#26126;&#20026;&#20160;&#20040;&#21521;&#29305;&#23450;&#29992;&#25143;&#25512;&#33616;&#29305;&#23450;&#30340;&#39033;&#30446;&#12290;&#20854;&#20013;&#19968;&#31181;&#26041;&#27861;&#26159;&#21453;&#20107;&#23454;&#35828;&#26126;&#65288;CF&#65289;&#12290;&#34429;&#28982;CF&#23545;&#29992;&#25143;&#21644;&#31995;&#32479;&#35774;&#35745;&#20154;&#21592;&#21487;&#33021;&#20250;&#38750;&#24120;&#26377;&#30410;&#65292;&#20294;&#24694;&#24847;&#34892;&#20026;&#32773;&#20063;&#21487;&#33021;&#21033;&#29992;&#36825;&#20123;&#35828;&#26126;&#26469;&#30772;&#22351;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;H-CARS&#65292;&#36890;&#36807;CF&#27745;&#26579;&#25512;&#33616;&#31995;&#32479;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#22312;&#20174;&#21453;&#20107;&#23454;&#35828;&#26126;&#27966;&#29983;&#30340;&#35757;&#32451;&#25968;&#25454;&#19978;&#35757;&#32451;&#20102;&#19968;&#31181;&#22522;&#20110;&#36923;&#36753;&#25512;&#29702;&#30340;&#20195;&#29702;&#27169;&#22411;&#12290;&#36890;&#36807;&#39072;&#20498;&#25512;&#33616;&#27169;&#22411;&#30340;&#23398;&#20064;&#36807;&#31243;&#65292;&#25105;&#20204;&#22240;&#27492;&#24320;&#21457;&#20986;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#36138;&#23146;&#31639;&#27861;&#65292;&#20026;&#19978;&#36848;&#20195;&#29702;&#27169;&#22411;&#29983;&#25104;&#34394;&#20551;&#29992;&#25143;&#36164;&#26009;&#21450;&#20854;&#30456;&#20851;&#20132;&#20114;&#35760;&#24405;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#20351;&#29992;&#20102;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;CF&#27169;&#22411;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#33104;&#36133;&#25216;&#26415;&#65292;&#32467;&#26524;&#26174;&#31034;&#33021;&#22815;&#25104;&#21151;&#24178;&#25200;&#25512;&#33616;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning-based recommender systems have become an integral part of several online platforms. However, their black-box nature emphasizes the need for explainable artificial intelligence (XAI) approaches to provide human-understandable reasons why a specific item gets recommended to a given user. One such method is counterfactual explanation (CF). While CFs can be highly beneficial for users and system designers, malicious actors may also exploit these explanations to undermine the system's security. In this work, we propose H-CARS, a novel strategy to poison recommender systems via CFs. Specifically, we first train a logical-reasoning-based surrogate model on training data derived from counterfactual explanations. By reversing the learning process of the recommendation model, we thus develop a proficient greedy algorithm to generate fabricated user profiles and their associated interaction records for the aforementioned surrogate model. Our experiments, which employ a well-known CF
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#21464;&#25991;&#26412;&#25628;&#32034;&#24341;&#25806;&#65292;&#20801;&#35768;&#29992;&#25143;&#25214;&#21040;&#32593;&#39029;&#20013;&#30340;&#26356;&#25913;&#12290;&#20351;&#29992;&#35813;&#24341;&#25806;&#21487;&#20197;&#28165;&#26970;&#22320;&#26174;&#31034;&#32593;&#39029;&#20013;&#28155;&#21152;&#25110;&#21024;&#38500;&#30340;&#26415;&#35821;&#21644;&#30701;&#35821;&#30340;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2305.00546</link><description>&lt;p&gt;
&#20351;&#32593;&#39029;&#21464;&#21270;&#21487;&#21457;&#29616;&#24615;:&#38754;&#21521;Web&#26723;&#26696;&#39302;&#30340;&#25991;&#26412;&#21464;&#21270;&#25628;&#32034;&#30028;&#38754;
&lt;/p&gt;
&lt;p&gt;
Making Changes in Webpages Discoverable: A Change-Text Search Interface for Web Archives. (arXiv:2305.00546v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00546
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#21464;&#25991;&#26412;&#25628;&#32034;&#24341;&#25806;&#65292;&#20801;&#35768;&#29992;&#25143;&#25214;&#21040;&#32593;&#39029;&#20013;&#30340;&#26356;&#25913;&#12290;&#20351;&#29992;&#35813;&#24341;&#25806;&#21487;&#20197;&#28165;&#26970;&#22320;&#26174;&#31034;&#32593;&#39029;&#20013;&#28155;&#21152;&#25110;&#21024;&#38500;&#30340;&#26415;&#35821;&#21644;&#30701;&#35821;&#30340;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#39029;&#38543;&#26102;&#38388;&#25913;&#21464;&#65292;&#32780;Web&#26723;&#26696;&#39302;&#20445;&#23384;&#30528;&#32593;&#39029;&#30340;&#21382;&#21490;&#29256;&#26412;&#21103;&#26412;&#12290;Web&#26723;&#26696;&#39302;&#30340;&#29992;&#25143;&#65292;&#22914;&#35760;&#32773;&#65292;&#24819;&#35201;&#25214;&#21040;&#24182;&#26597;&#30475;&#32593;&#39029;&#38543;&#26102;&#38388;&#30340;&#21464;&#21270;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;Web&#26723;&#26696;&#39302;&#25628;&#32034;&#30028;&#38754;&#19981;&#25903;&#25345;&#36825;&#39033;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#21464;&#25991;&#26412;&#25628;&#32034;&#24341;&#25806;&#65292;&#20801;&#35768;&#29992;&#25143;&#25214;&#21040;&#32593;&#39029;&#20013;&#30340;&#26356;&#25913;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#25628;&#32034;&#24341;&#25806;&#21518;&#31471;&#21644;&#21069;&#31471;&#30340;&#23454;&#29616;&#65292;&#21253;&#25324;&#19968;&#20010;&#24037;&#20855;&#65292;&#20801;&#35768;&#29992;&#25143;&#22312;&#19978;&#19979;&#25991;&#20013;&#26597;&#30475;&#20004;&#20010;&#32593;&#39029;&#29256;&#26412;&#20043;&#38388;&#30340;&#26356;&#25913;&#65292;&#20316;&#20026;&#21160;&#30011;&#21576;&#29616;&#12290;&#25105;&#20204;&#20351;&#29992;2016&#24180;&#33267;2020&#24180;&#20043;&#38388;&#26356;&#25913;&#36807;&#30340;&#32654;&#22269;&#32852;&#37030;&#29615;&#22659;&#32593;&#39029;&#35780;&#20272;&#20102;&#25628;&#32034;&#24341;&#25806;&#12290;&#21464;&#21270;&#25991;&#26412;&#25628;&#32034;&#32467;&#26524;&#39029;&#38754;&#21487;&#20197;&#28165;&#26970;&#22320;&#26174;&#31034;&#20174;&#32593;&#39029;&#20013;&#28155;&#21152;&#25110;&#21024;&#38500;&#30340;&#26415;&#35821;&#21644;&#30701;&#35821;&#30340;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Webpages change over time, and web archives hold copies of historical versions of webpages. Users of web archives, such as journalists, want to find and view changes on webpages over time. However, the current search interfaces for web archives do not support this task. For the web archives that include a full-text search feature, multiple versions of the same webpage that match the search query are shown individually without enumerating changes, or are grouped together in a way that hides changes. We present a change text search engine that allows users to find changes in webpages. We describe the implementation of the search engine backend and frontend, including a tool that allows users to view the changes between two webpage versions in context as an animation. We evaluate the search engine with U.S. federal environmental webpages that changed between 2016 and 2020. The change text search results page can clearly show when terms and phrases were added or removed from webpages. The 
&lt;/p&gt;</description></item><item><title>TALLRec&#26159;&#23545;LLMs&#36827;&#34892;&#35843;&#25972;&#30340;&#19968;&#31181;&#39640;&#25928;&#19988;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;LLMs&#19982;&#25512;&#33616;&#31995;&#32479;&#23545;&#40784;&#65292;&#20174;&#32780;&#22686;&#24378;LLMs&#22312;&#25512;&#33616;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.00447</link><description>&lt;p&gt;
TALLRec: &#19968;&#31181;&#19982;&#25512;&#33616;&#31995;&#32479;&#23545;&#40784;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26377;&#25928;&#19988;&#39640;&#25928;&#30340;&#35843;&#25972;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
TALLRec: An Effective and Efficient Tuning Framework to Align Large Language Model with Recommendation. (arXiv:2305.00447v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00447
&lt;/p&gt;
&lt;p&gt;
TALLRec&#26159;&#23545;LLMs&#36827;&#34892;&#35843;&#25972;&#30340;&#19968;&#31181;&#39640;&#25928;&#19988;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;LLMs&#19982;&#25512;&#33616;&#31995;&#32479;&#23545;&#40784;&#65292;&#20174;&#32780;&#22686;&#24378;LLMs&#22312;&#25512;&#33616;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#23637;&#29616;&#20102;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#26174;&#33879;&#24615;&#33021;&#65292;&#22240;&#27492;&#30740;&#31350;&#20154;&#21592;&#24320;&#22987;&#25506;&#32034;&#23427;&#20204;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#28508;&#21147;&#12290;&#34429;&#28982;&#21021;&#22987;&#30340;&#23581;&#35797;&#24050;&#32463;&#21033;&#29992;&#20102;LLMs&#30340;&#20248;&#24322;&#33021;&#21147;&#65292;&#27604;&#22914;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#25552;&#31034;&#35789;&#26469;&#20016;&#23500;&#30693;&#35782;&#24182;&#36827;&#34892;&#24378;&#21270;&#27867;&#21270;&#65292;&#20294;&#26159;&#30001;&#20110;LLMs&#30340;&#35757;&#32451;&#20219;&#21153;&#19982;&#25512;&#33616;&#20219;&#21153;&#20043;&#38388;&#30340;&#24040;&#22823;&#24046;&#24322;&#20197;&#21450;&#39044;&#35757;&#32451;&#26399;&#38388;&#30340;&#19981;&#36275;&#30340;&#25512;&#33616;&#25968;&#25454;&#65292;LLMs&#22312;&#25512;&#33616;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#20173;&#28982;&#19981;&#29702;&#24819;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#32771;&#34385;&#20351;&#29992;&#25512;&#33616;&#25968;&#25454;&#23545;LLMs&#36827;&#34892;&#35843;&#25972;&#26469;&#26500;&#24314;&#22823;&#22411;&#25512;&#33616;&#35821;&#35328;&#27169;&#22411;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TALLRec&#30340;&#39640;&#25928;&#19988;&#26377;&#25928;&#30340;&#35843;&#25972;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;LLMs&#19982;&#25512;&#33616;&#31995;&#32479;&#23545;&#40784;&#12290;&#25105;&#20204;&#24050;&#32463;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;TALLRec&#26694;&#26550;&#21487;&#20197;&#26174;&#33879;&#22686;&#24378;LLMs&#22312;&#25512;&#33616;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated remarkable performance across diverse domains, thereby prompting researchers to explore their potential for use in recommendation systems. Initial attempts have leveraged the exceptional capabilities of LLMs, such as rich knowledge and strong generalization through In-context Learning, which involves phrasing the recommendation task as prompts. Nevertheless, the performance of LLMs in recommendation tasks remains suboptimal due to a substantial disparity between the training tasks for LLMs and recommendation tasks, as well as inadequate recommendation data during pre-training. To bridge the gap, we consider building a Large Recommendation Language Model by tunning LLMs with recommendation data. To this end, we propose an efficient and effective Tuning framework for Aligning LLMs with Recommendation, namely TALLRec. We have demonstrated that the proposed TALLRec framework can significantly enhance the recommendation capabilities of LLMs in 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#19987;&#27880;&#20110;&#31185;&#23398;&#34920;&#26684;&#30340; EL &#25968;&#25454;&#38598; S2abEL&#65292;&#29992;&#20110;&#23454;&#20307;&#38142;&#25509;&#20219;&#21153;&#12290;&#30001;&#20110;&#31185;&#23398;&#30693;&#35782;&#24211;&#30340;&#19981;&#23436;&#25972;&#24615;&#21644;&#35821;&#22659;&#24433;&#21709;&#65292;&#31185;&#23398;&#34920;&#26684;&#19978;&#30340; EL &#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#35813;&#25968;&#25454;&#38598;&#19987;&#27880;&#20110;&#26426;&#22120;&#23398;&#20064;&#32467;&#26524;&#34920;&#20013;&#30340; EL&#65292;&#21253;&#21547;&#25163;&#24037;&#26631;&#35760;&#30340;&#21333;&#20803;&#26684;&#31867;&#22411;&#12289;&#23646;&#24615;&#21644;&#23454;&#20307;&#38142;&#25509;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#31070;&#32463;&#22522;&#32447;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.00366</link><description>&lt;p&gt;
S2abEL&#65306;&#19968;&#20221;&#29992;&#20110;&#31185;&#23398;&#34920;&#26684;&#23454;&#20307;&#38142;&#25509;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
S2abEL: A Dataset for Entity Linking from Scientific Tables. (arXiv:2305.00366v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00366
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#19987;&#27880;&#20110;&#31185;&#23398;&#34920;&#26684;&#30340; EL &#25968;&#25454;&#38598; S2abEL&#65292;&#29992;&#20110;&#23454;&#20307;&#38142;&#25509;&#20219;&#21153;&#12290;&#30001;&#20110;&#31185;&#23398;&#30693;&#35782;&#24211;&#30340;&#19981;&#23436;&#25972;&#24615;&#21644;&#35821;&#22659;&#24433;&#21709;&#65292;&#31185;&#23398;&#34920;&#26684;&#19978;&#30340; EL &#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#35813;&#25968;&#25454;&#38598;&#19987;&#27880;&#20110;&#26426;&#22120;&#23398;&#20064;&#32467;&#26524;&#34920;&#20013;&#30340; EL&#65292;&#21253;&#21547;&#25163;&#24037;&#26631;&#35760;&#30340;&#21333;&#20803;&#26684;&#31867;&#22411;&#12289;&#23646;&#24615;&#21644;&#23454;&#20307;&#38142;&#25509;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#31070;&#32463;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#20307;&#38142;&#25509;&#65288;EL&#65289;&#26159;&#23558;&#25991;&#26412;&#25552;&#21450;&#38142;&#25509;&#21040;&#30693;&#35782;&#24211;&#20013;&#30456;&#24212;&#26465;&#30446;&#30340;&#20219;&#21153;&#65292;&#36825;&#23545;&#20110;&#35768;&#22810;&#30693;&#35782;&#23494;&#38598;&#22411;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#26469;&#35828;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#24403;&#24212;&#29992;&#20110;&#31185;&#23398;&#35770;&#25991;&#20013;&#30340;&#34920;&#26684;&#26102;&#65292;EL&#26159;&#23454;&#29616;&#22823;&#35268;&#27169;&#31185;&#23398;&#30693;&#35782;&#24211;&#30340;&#19968;&#27493;&#65292;&#36825;&#21487;&#20197;&#23454;&#29616;&#20808;&#36827;&#30340;&#31185;&#23398;&#38382;&#31572;&#21644;&#20998;&#26512;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#38024;&#23545;&#31185;&#23398;&#34920;&#26684;&#20013;&#30340;EL&#30340;&#25968;&#25454;&#38598;&#12290;&#31185;&#23398;&#34920;&#26684;&#30340;EL&#23588;&#20854;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#31185;&#23398;&#30693;&#35782;&#24211;&#21487;&#33021;&#38750;&#24120;&#19981;&#23436;&#25972;&#65292;&#24182;&#19988;&#36890;&#24120;&#38656;&#35201;&#29702;&#35299;&#35770;&#25991;&#20013;&#30340;&#25991;&#26412;&#20197;&#21450;&#34920;&#26684;&#30340;&#19978;&#19979;&#25991;&#26469;&#28040;&#38500;&#27495;&#20041;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;S2abEL&#19987;&#27880;&#20110;&#26426;&#22120;&#23398;&#20064;&#32467;&#26524;&#34920;&#20013;&#30340;EL&#65292;&#24182;&#21253;&#25324;&#26469;&#33258;PaperswithCode&#20998;&#31867;&#27861;&#30340;8,429&#20010;&#21333;&#20803;&#26684;&#30340;&#25163;&#24037;&#26631;&#35760;&#30340;&#21333;&#20803;&#26684;&#31867;&#22411;&#12289;&#26469;&#28304;&#23646;&#24615;&#21644;&#23454;&#20307;&#38142;&#25509;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#38024;&#23545;&#31185;&#23398;&#34920;&#26684;&#30340;&#31070;&#32463;&#22522;&#32447;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21253;&#21547;&#35768;&#22810;&#30693;&#35782;&#24211;&#20043;&#22806;&#25552;&#21450;&#30340;&#23454;&#20307;&#65292;&#24182;&#26174;&#31034;&#23427;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Entity linking (EL) is the task of linking a textual mention to its corresponding entry in a knowledge base, and is critical for many knowledge-intensive NLP applications. When applied to tables in scientific papers, EL is a step toward large-scale scientific knowledge bases that could enable advanced scientific question answering and analytics. We present the first dataset for EL in scientific tables. EL for scientific tables is especially challenging because scientific knowledge bases can be very incomplete, and disambiguating table mentions typically requires understanding the papers's tet in addition to the table. Our dataset, S2abEL, focuses on EL in machine learning results tables and includes hand-labeled cell types, attributed sources, and entity links from the PaperswithCode taxonomy for 8,429 cells from 732 tables. We introduce a neural baseline method designed for EL on scientific tables containing many out-of-knowledge-base mentions, and show that it significantly outperfor
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;JH-POLO CLIR&#35757;&#32451;&#38598;&#21019;&#24314;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#29983;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#35299;&#20915;&#36328;&#35821;&#35328;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#25152;&#38754;&#20020;&#30340;&#35757;&#32451;&#25968;&#25454;&#21294;&#20047;&#12289;&#22266;&#23450;&#36164;&#28304;&#22823;&#23567;&#21644;&#25991;&#20307;&#35805;&#35821;&#39046;&#22495;&#22266;&#23450;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.00331</link><description>&lt;p&gt;
&#21512;&#25104;&#36328;&#35821;&#35328;&#20449;&#24687;&#26816;&#32034;&#35757;&#32451;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Synthetic Cross-language Information Retrieval Training Data. (arXiv:2305.00331v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00331
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;JH-POLO CLIR&#35757;&#32451;&#38598;&#21019;&#24314;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#29983;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#35299;&#20915;&#36328;&#35821;&#35328;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#25152;&#38754;&#20020;&#30340;&#35757;&#32451;&#25968;&#25454;&#21294;&#20047;&#12289;&#22266;&#23450;&#36164;&#28304;&#22823;&#23567;&#21644;&#25991;&#20307;&#35805;&#35821;&#39046;&#22495;&#22266;&#23450;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#36328;&#35821;&#35328;&#20449;&#24687;&#26816;&#32034;(CLIR)&#31995;&#32479;&#30340;&#19968;&#20010;&#20851;&#38190;&#38590;&#28857;&#22312;&#20110;&#32570;&#20047;&#35757;&#32451;&#25968;&#25454;&#12290;MS MARCO&#21333;&#35821;&#35757;&#32451;&#38598;&#30340;&#20986;&#29616;&#26174;&#33879;&#25552;&#39640;&#20102;&#31070;&#32463;&#21333;&#35821;&#26816;&#32034;&#25216;&#26415;&#30340;&#21457;&#23637;&#27700;&#24179;&#12290;&#36890;&#36807;&#20351;&#29992;&#26426;&#22120;&#32763;&#35793;&#23558;MS MARCO&#25991;&#26723;&#32763;&#35793;&#25104;&#20854;&#20182;&#35821;&#35328;&#65292;&#36825;&#19968;&#36164;&#28304;&#24050;&#32463;&#34987;&#29992;&#20110;&#36328;&#35821;&#35328;&#20449;&#24687;&#26816;&#32034;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#32763;&#35793;&#23384;&#22312;&#22810;&#20010;&#38382;&#39064;&#65292;&#23427;&#26159;&#19968;&#20010;&#22266;&#23450;&#22823;&#23567;&#30340;&#36164;&#28304;&#65292;&#23427;&#30340;&#25991;&#20307;&#21644;&#35805;&#35821;&#39046;&#22495;&#26159;&#22266;&#23450;&#30340;&#65292;&#32763;&#35793;&#25991;&#26723;&#19981;&#26159;&#29992;&#27597;&#35821;&#32780;&#26159;&#29992;&#32763;&#35793;&#35821;&#35328;&#32534;&#20889;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;JH-POLO CLIR&#35757;&#32451;&#38598;&#21019;&#24314;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#39318;&#20808;&#36873;&#25321;&#19968;&#23545;&#38750;&#33521;&#35821;&#27573;&#33853;&#65292;&#28982;&#21518;&#20351;&#29992;&#29983;&#25104;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#29983;&#25104;&#19968;&#20010;&#33521;&#35821;&#26597;&#35810;&#65292;&#20351;&#24471;&#31532;&#19968;&#20010;&#27573;&#33853;&#30456;&#20851;&#32780;&#31532;&#20108;&#20010;&#27573;&#33853;&#19981;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
A key stumbling block for neural cross-language information retrieval (CLIR) systems has been the paucity of training data. The appearance of the MS MARCO monolingual training set led to significant advances in the state of the art in neural monolingual retrieval. By translating the MS MARCO documents into other languages using machine translation, this resource has been made useful to the CLIR community. Yet such translation suffers from a number of problems. While MS MARCO is a large resource, it is of fixed size; its genre and domain of discourse are fixed; and the translated documents are not written in the language of a native speaker of the language, but rather in translationese. To address these problems, we introduce the JH-POLO CLIR training set creation methodology. The approach begins by selecting a pair of non-English passages. A generative large language model is then used to produce an English query for which the first passage is relevant and the second passage is not rel
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#20020;&#24202;&#29615;&#22659;&#20013;&#20351;&#29992;GPT-3.5&#21644;GPT-4&#35299;&#20915;&#21307;&#23398;&#38382;&#39064;&#30340;&#23433;&#20840;&#24615;&#20197;&#21450;&#19982;&#20449;&#24687;&#25216;&#26415;&#21672;&#35810;&#26381;&#21153;&#25253;&#21578;&#30340;&#19968;&#33268;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20004;&#20010;LLMs&#37117;&#21487;&#20197;&#20197;&#23433;&#20840;&#21644;&#19968;&#33268;&#30340;&#26041;&#24335;&#28385;&#36275;&#21307;&#29983;&#30340;&#20449;&#24687;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2304.13714</link><description>&lt;p&gt;
&#35780;&#20272;GPT-3.5&#21644;GPT-4&#22312;&#25903;&#25345;&#21307;&#30103;&#20445;&#20581;&#20449;&#24687;&#38656;&#27714;&#26041;&#38754;&#30340;&#23454;&#38469;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Evaluation of GPT-3.5 and GPT-4 for supporting real-world information needs in healthcare delivery. (arXiv:2304.13714v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13714
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#20020;&#24202;&#29615;&#22659;&#20013;&#20351;&#29992;GPT-3.5&#21644;GPT-4&#35299;&#20915;&#21307;&#23398;&#38382;&#39064;&#30340;&#23433;&#20840;&#24615;&#20197;&#21450;&#19982;&#20449;&#24687;&#25216;&#26415;&#21672;&#35810;&#26381;&#21153;&#25253;&#21578;&#30340;&#19968;&#33268;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20004;&#20010;LLMs&#37117;&#21487;&#20197;&#20197;&#23433;&#20840;&#21644;&#19968;&#33268;&#30340;&#26041;&#24335;&#28385;&#36275;&#21307;&#29983;&#30340;&#20449;&#24687;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#65292;&#20294;&#24403;&#21069;&#30340;&#25506;&#32034;&#24182;&#26410;&#35780;&#20272;LLMs&#22312;&#20020;&#24202;&#29615;&#22659;&#20013;&#30340;&#23454;&#29992;&#24615;&#21644;&#23433;&#20840;&#24615;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#30830;&#23450;&#20004;&#20010;LLM&#26159;&#21542;&#21487;&#20197;&#20197;&#23433;&#20840;&#21644;&#19968;&#33268;&#30340;&#26041;&#24335;&#28385;&#36275;&#30001;&#21307;&#29983;&#25552;&#20132;&#30340;&#20449;&#24687;&#38656;&#27714;&#38382;&#39064;&#12290;&#25105;&#20204;&#23558;66&#20010;&#26469;&#33258;&#20449;&#24687;&#25216;&#26415;&#21672;&#35810;&#26381;&#21153;&#30340;&#38382;&#39064;&#36890;&#36807;&#31616;&#21333;&#30340;&#25552;&#31034;&#25552;&#20132;&#32473;GPT-3.5&#21644;GPT-4&#12290;12&#21517;&#21307;&#29983;&#35780;&#20272;&#20102;LLM&#21709;&#24212;&#23545;&#24739;&#32773;&#36896;&#25104;&#20260;&#23475;&#30340;&#21487;&#33021;&#24615;&#20197;&#21450;&#19982;&#20449;&#24687;&#25216;&#26415;&#21672;&#35810;&#26381;&#21153;&#30340;&#29616;&#26377;&#25253;&#21578;&#30340;&#19968;&#33268;&#24615;&#12290;&#21307;&#29983;&#30340;&#35780;&#20272;&#22522;&#20110;&#22810;&#25968;&#31080;&#27719;&#24635;&#12290;&#23545;&#20110;&#27809;&#26377;&#20219;&#20309;&#38382;&#39064;&#65292;&#22823;&#22810;&#25968;&#21307;&#29983;&#35748;&#20026;&#20219;&#20309;&#19968;&#20010;LLM&#21709;&#24212;&#37117;&#19981;&#20250;&#36896;&#25104;&#20260;&#23475;&#12290;&#23545;&#20110;GPT-3.5&#65292;8&#20010;&#38382;&#39064;&#30340;&#21709;&#24212;&#19982;&#20449;&#24687;&#25216;&#26415;&#21672;&#35810;&#25253;&#21578;&#19968;&#33268;&#65292;20&#20010;&#19981;&#19968;&#33268;&#65292;9&#20010;&#26080;&#27861;&#35780;&#20272;&#12290;&#26377;29&#20010;&#21709;&#24212;&#27809;&#26377;&#22810;&#25968;&#31080;&#34920;&#31034;&#8220;&#21516;&#24847;&#8221;&#12289;&#8220;&#19981;&#21516;&#24847;&#8221;&#21644;&#8220;&#26080;&#27861;&#35780;&#20272;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite growing interest in using large language models (LLMs) in healthcare, current explorations do not assess the real-world utility and safety of LLMs in clinical settings. Our objective was to determine whether two LLMs can serve information needs submitted by physicians as questions to an informatics consultation service in a safe and concordant manner. Sixty six questions from an informatics consult service were submitted to GPT-3.5 and GPT-4 via simple prompts. 12 physicians assessed the LLM responses' possibility of patient harm and concordance with existing reports from an informatics consultation service. Physician assessments were summarized based on majority vote. For no questions did a majority of physicians deem either LLM response as harmful. For GPT-3.5, responses to 8 questions were concordant with the informatics consult report, 20 discordant, and 9 were unable to be assessed. There were 29 responses with no majority on "Agree", "Disagree", and "Unable to assess". Fo
&lt;/p&gt;</description></item><item><title>&#25512;&#33616;&#31995;&#32479;&#20013;&#65292;&#20351;&#29992;&#21807;&#19968;&#26631;&#35782;&#30340;IDRec&#27169;&#22411;&#30456;&#27604;&#20351;&#29992;&#27169;&#24577;&#30340;MoRec&#27169;&#22411;&#22312;&#25512;&#33616;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#19978;&#34920;&#29616;&#26356;&#22909;&#65292;&#28982;&#32780;&#65292;&#38656;&#35201;&#26681;&#25454;&#20855;&#20307;&#24773;&#20917;&#36873;&#25321;&#36866;&#21512;&#30340;&#25512;&#33616;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2303.13835</link><description>&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#20309;&#21435;&#20309;&#20174;&#65311;ID- vs. &#22522;&#20110;&#27169;&#24577;&#30340;&#25512;&#33616;&#27169;&#22411;&#20877;&#25506;&#35752;
&lt;/p&gt;
&lt;p&gt;
Where to Go Next for Recommender Systems? ID- vs. Modality-based recommender models revisited. (arXiv:2303.13835v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13835
&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#20013;&#65292;&#20351;&#29992;&#21807;&#19968;&#26631;&#35782;&#30340;IDRec&#27169;&#22411;&#30456;&#27604;&#20351;&#29992;&#27169;&#24577;&#30340;MoRec&#27169;&#22411;&#22312;&#25512;&#33616;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#19978;&#34920;&#29616;&#26356;&#22909;&#65292;&#28982;&#32780;&#65292;&#38656;&#35201;&#26681;&#25454;&#20855;&#20307;&#24773;&#20917;&#36873;&#25321;&#36866;&#21512;&#30340;&#25512;&#33616;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#21313;&#24180;&#65292;&#21033;&#29992;&#21807;&#19968;&#26631;&#35782;&#65288;ID&#65289;&#26469;&#34920;&#31034;&#19981;&#21516;&#29992;&#25143;&#21644;&#29289;&#21697;&#30340;&#25512;&#33616;&#27169;&#22411;&#19968;&#30452;&#26159;&#26368;&#20808;&#36827;&#30340;&#65292;&#24182;&#19988;&#22312;&#25512;&#33616;&#31995;&#32479;&#25991;&#29486;&#20013;&#21344;&#20027;&#23548;&#22320;&#20301;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#39044;&#35757;&#32451;&#27169;&#24577;&#32534;&#30721;&#22120;&#65288;&#22914;BERT&#21644;ViT&#65289;&#22312;&#23545;&#29289;&#21697;&#30340;&#21407;&#22987;&#27169;&#24577;&#29305;&#24449;&#65288;&#22914;&#25991;&#26412;&#21644;&#22270;&#20687;&#65289;&#36827;&#34892;&#24314;&#27169;&#26041;&#38754;&#21464;&#24471;&#36234;&#26469;&#36234;&#24378;&#22823;&#12290;&#22240;&#27492;&#65292;&#33258;&#28982;&#32780;&#28982;&#30340;&#38382;&#39064;&#26159;&#65306;&#36890;&#36807;&#29992;&#26368;&#20808;&#36827;&#30340;&#27169;&#24577;&#32534;&#30721;&#22120;&#26367;&#25442;&#29289;&#21697;ID&#23884;&#20837;&#21521;&#37327;&#65292;&#19968;&#20010;&#32431;&#31929;&#30340;&#22522;&#20110;&#27169;&#24577;&#30340;&#25512;&#33616;&#27169;&#22411;&#65288;MoRec&#65289;&#33021;&#21542;&#32988;&#36807;&#25110;&#19982;&#32431;ID&#22522;&#30784;&#27169;&#22411;&#65288;IDRec&#65289;&#30456;&#21305;&#37197;&#65311;&#23454;&#38469;&#19978;&#65292;&#26089;&#22312;&#21313;&#24180;&#21069;&#65292;&#36825;&#20010;&#38382;&#39064;&#23601;&#34987;&#22238;&#31572;&#20102;&#65292;IDRec&#22312;&#25512;&#33616;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#37117;&#36828;&#36828;&#32988;&#36807;MoRec&#12290;&#25105;&#20204;&#26088;&#22312;&#37325;&#26032;&#23457;&#35270;&#36825;&#20010;&#8220;&#32769;&#38382;&#39064;&#8221;&#65292;&#20174;&#22810;&#20010;&#26041;&#38754;&#23545;MoRec&#36827;&#34892;&#31995;&#32479;&#30740;&#31350;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20960;&#20010;&#23376;&#38382;&#39064;&#65306;&#65288;i&#65289;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;MoRec&#25110;IDRec&#21738;&#20010;&#25512;&#33616;&#27169;&#24335;&#34920;&#29616;&#26356;&#22909;&#65292;&#29305;&#21035;&#26159;&#22312;&#19968;&#33324;&#24773;&#20917;&#21644;......
&lt;/p&gt;
&lt;p&gt;
Recommendation models that utilize unique identities (IDs) to represent distinct users and items have been state-of-the-art (SOTA) and dominated the recommender systems (RS) literature for over a decade. Meanwhile, the pre-trained modality encoders, such as BERT and ViT, have become increasingly powerful in modeling the raw modality features of an item, such as text and images. Given this, a natural question arises: can a purely modality-based recommendation model (MoRec) outperforms or matches a pure ID-based model (IDRec) by replacing the itemID embedding with a SOTA modality encoder? In fact, this question was answered ten years ago when IDRec beats MoRec by a strong margin in both recommendation accuracy and efficiency. We aim to revisit this `old' question and systematically study MoRec from several aspects. Specifically, we study several sub-questions: (i) which recommendation paradigm, MoRec or IDRec, performs better in practical scenarios, especially in the general setting and 
&lt;/p&gt;</description></item><item><title>GSim&#26159;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24322;&#26500;&#22270;&#20851;&#32852;&#24230;&#37327;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#39044;&#23450;&#20041;&#30340;&#20803;&#36335;&#24452;&#65292;&#33021;&#22815;&#25429;&#25417;&#24322;&#26500;&#22270;&#30340;&#38544;&#21547;&#32467;&#26500;&#65292;&#24050;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2208.06144</link><description>&lt;p&gt;
GSim &#65306;&#38754;&#21521;&#24322;&#26500;&#22270;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#20851;&#32852;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
GSim: A Graph Neural Network based Relevance Measure for Heterogeneous Graphs. (arXiv:2208.06144v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.06144
&lt;/p&gt;
&lt;p&gt;
GSim&#26159;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24322;&#26500;&#22270;&#20851;&#32852;&#24230;&#37327;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#39044;&#23450;&#20041;&#30340;&#20803;&#36335;&#24452;&#65292;&#33021;&#22815;&#25429;&#25417;&#24322;&#26500;&#22270;&#30340;&#38544;&#21547;&#32467;&#26500;&#65292;&#24050;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#26500;&#22270;&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#24191;&#27867;&#23384;&#22312;&#65292;&#21253;&#21547;&#20855;&#26377;&#22810;&#31181;&#31867;&#22411;&#30340;&#33410;&#28857;&#21644;&#36793;&#32536;&#12290;&#20851;&#32852;&#24230;&#37327;&#26159;&#20998;&#26512;&#24322;&#26500;&#22270;&#30340;&#22522;&#26412;&#20219;&#21153;&#20043;&#19968;&#65292;&#30446;&#30340;&#26159;&#35745;&#31639;&#19981;&#21516;&#31867;&#22411;&#30340;&#20004;&#20010;&#23545;&#35937;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#24050;&#32463;&#22312;&#35768;&#22810;&#24212;&#29992;&#31243;&#24207;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#20363;&#22914;&#32593;&#32476;&#25628;&#32034;&#12289;&#25512;&#33616;&#21644;&#31038;&#21306;&#26816;&#27979;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#20851;&#32852;&#24230;&#37327;&#26041;&#27861;&#19987;&#27880;&#20110;&#21516;&#36136;&#32593;&#32476;&#65292;&#20294;&#20026;&#24322;&#26500;&#22270;&#24320;&#21457;&#20102;&#19968;&#20123;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#32463;&#24120;&#38656;&#35201;&#39044;&#23450;&#20041;&#30340;&#20803;&#36335;&#24452;&#12290;&#23450;&#20041;&#26377;&#24847;&#20041;&#30340;&#20803;&#36335;&#24452;&#38656;&#35201;&#22823;&#37327;&#39046;&#22495;&#30693;&#35782;&#65292;&#36825;&#22312;&#22522;&#20110;&#26550;&#26500;&#20016;&#23500;&#30340;&#24322;&#26500;&#22270;&#65288;&#22914;&#30693;&#35782;&#22270;&#35889;&#65289;&#19978;&#26497;&#22823;&#22320;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#24212;&#29992;&#12290;&#26368;&#36817;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#24050;&#24191;&#27867;&#24212;&#29992;&#20110;&#35768;&#22810;&#22270;&#25366;&#25496;&#20219;&#21153;&#20013;&#65292;&#20294;&#23578;&#26410;&#29992;&#20110;&#34913;&#37327;&#20851;&#32852;&#24615;&#12290;&#20026;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GSim&#65292;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24322;&#26500;&#22270;&#20851;&#32852;&#24230;&#37327;&#26041;&#27861;&#12290;GSim&#33021;&#22815;&#25429;&#25417;&#24322;&#26500;&#22270;&#30340;&#38544;&#21547;&#32467;&#26500;&#65292;&#19981;&#38656;&#35201;&#39044;&#23450;&#20041;&#30340;&#20803;&#36335;&#24452;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#32467;&#26524;&#34920;&#26126;GSim&#26174;&#33879;&#20248;&#20110;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Heterogeneous graphs, which contain nodes and edges of multiple types, are prevalent in various domains, including bibliographic networks, social media, and knowledge graphs. As a fundamental task in analyzing heterogeneous graphs, relevance measure aims to calculate the relevance between two objects of different types, which has been used in many applications such as web search, recommendation, and community detection. Most of existing relevance measures focus on homogeneous networks where objects are of the same type, and a few measures are developed for heterogeneous graphs, but they often need the pre-defined meta-path. Defining meaningful meta-paths requires much domain knowledge, which largely limits their applications, especially on schema-rich heterogeneous graphs like knowledge graphs. Recently, the Graph Neural Network (GNN) has been widely applied in many graph mining tasks, but it has not been applied for measuring relevance yet. To address the aforementioned problems, we p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#33258;&#21161;&#28508;&#22312;&#34920;&#31034;&#65288;BLR&#65289;&#30340;&#26032;&#22411;&#33258;&#30417;&#30563;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#22312;&#19981;&#28041;&#21450;&#36741;&#21161;&#22270;&#24418;&#30340;&#24773;&#20917;&#19979;&#22686;&#24378;&#29992;&#25143;/&#29289;&#21697;&#34920;&#31034;&#24182;&#26377;&#25928;&#22320;&#23545;&#24453;&#27491;&#36127;&#26679;&#26412;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22810;&#27169;&#24577;&#25512;&#33616;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2207.05969</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#25512;&#33616;&#20013;&#30340; Bootstrap &#28508;&#22312;&#34920;&#31034;&#27861;
&lt;/p&gt;
&lt;p&gt;
Bootstrap Latent Representations for Multi-modal Recommendation. (arXiv:2207.05969v3 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.05969
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#33258;&#21161;&#28508;&#22312;&#34920;&#31034;&#65288;BLR&#65289;&#30340;&#26032;&#22411;&#33258;&#30417;&#30563;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#22312;&#19981;&#28041;&#21450;&#36741;&#21161;&#22270;&#24418;&#30340;&#24773;&#20917;&#19979;&#22686;&#24378;&#29992;&#25143;/&#29289;&#21697;&#34920;&#31034;&#24182;&#26377;&#25928;&#22320;&#23545;&#24453;&#27491;&#36127;&#26679;&#26412;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22810;&#27169;&#24577;&#25512;&#33616;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#27169;&#24577;&#25512;&#33616;&#38382;&#39064;&#65292;&#20854;&#20013;&#21033;&#29992;&#29289;&#21697;&#30340;&#22810;&#27169;&#24577;&#20449;&#24687;&#65288;&#20363;&#22914;&#22270;&#20687;&#21644;&#25991;&#26412;&#25551;&#36848;&#65289;&#26469;&#25552;&#39640;&#25512;&#33616;&#20934;&#30830;&#24615;&#12290;&#38024;&#23545;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#36890;&#24120;&#20351;&#29992;&#36741;&#21161;&#22270;&#24418;&#65288;&#20363;&#22914;&#29992;&#25143;-&#29992;&#25143;&#25110;&#29289;&#21697;-&#29289;&#21697;&#20851;&#31995;&#22270;&#65289;&#26469;&#22686;&#24378;&#29992;&#25143;&#21644;/&#25110;&#29289;&#21697;&#30340;&#23398;&#20064;&#34920;&#31034;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#33258;&#21161;&#28508;&#22312;&#34920;&#31034;&#65288;BLR&#65289;&#30340;&#26032;&#22411;&#33258;&#30417;&#30563;&#26041;&#27861;&#65292;&#21487;&#20197;&#32467;&#21512;&#23398;&#20064;&#29992;&#25143;/&#29289;&#21697;&#34920;&#31034;&#21644;&#22270;&#24418;&#32467;&#26500;&#12290;&#21516;&#26102;&#65292;&#20026;&#20102;&#20943;&#36731;&#22122;&#22768;&#30417;&#30563;&#20449;&#21495;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#20030;&#37319;&#26679;&#31574;&#30053;&#26469;&#26377;&#25928;&#22320;&#23545;&#24453;&#27491;&#36127;&#26679;&#26412;&#12290;&#22312;&#19977;&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#25512;&#33616;&#31934;&#24230;&#21644;&#25928;&#29575;&#26041;&#38754;&#37117;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies the multi-modal recommendation problem, where the item multi-modality information (e.g., images and textual descriptions) is exploited to improve the recommendation accuracy. Besides the user-item interaction graph, existing state-of-the-art methods usually use auxiliary graphs (e.g., user-user or item-item relation graph) to augment the learned representations of users and/or items. These representations are often propagated and aggregated on auxiliary graphs using graph convolutional networks, which can be prohibitively expensive in computation and memory, especially for large graphs. Moreover, existing multi-modal recommendation methods usually leverage randomly sampled negative examples in Bayesian Personalized Ranking (BPR) loss to guide the learning of user/item representations, which increases the computational cost on large graphs and may also bring noisy supervision signals into the training process. To tackle the above issues, we propose a novel self-superv
&lt;/p&gt;</description></item><item><title>SelfCF&#26159;&#19968;&#31181;&#33258;&#30417;&#30563;&#21327;&#21516;&#36807;&#28388;&#26694;&#26550;&#65292;&#29992;&#20110;&#25512;&#33616;&#22330;&#26223;&#65292;&#36890;&#36807;&#22686;&#24378;&#29616;&#26377;&#30340;&#28145;&#24230;&#23398;&#20064;&#21327;&#21516;&#36807;&#28388;&#27169;&#22411;&#20013;&#36755;&#20986;&#30340;&#23884;&#20837;&#26469;&#31616;&#21270;&#31639;&#27861;&#20197;&#21450;&#36991;&#20813;&#26114;&#36149;&#30340;&#35745;&#31639;&#21644;&#28508;&#22312;&#30340;&#36127;&#26679;&#26412;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2107.03019</link><description>&lt;p&gt;
SelfCF&#65306;&#19968;&#31181;&#31616;&#21333;&#30340;&#33258;&#30417;&#30563;&#21327;&#21516;&#36807;&#28388;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
SelfCF: A Simple Framework for Self-supervised Collaborative Filtering. (arXiv:2107.03019v3 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2107.03019
&lt;/p&gt;
&lt;p&gt;
SelfCF&#26159;&#19968;&#31181;&#33258;&#30417;&#30563;&#21327;&#21516;&#36807;&#28388;&#26694;&#26550;&#65292;&#29992;&#20110;&#25512;&#33616;&#22330;&#26223;&#65292;&#36890;&#36807;&#22686;&#24378;&#29616;&#26377;&#30340;&#28145;&#24230;&#23398;&#20064;&#21327;&#21516;&#36807;&#28388;&#27169;&#22411;&#20013;&#36755;&#20986;&#30340;&#23884;&#20837;&#26469;&#31616;&#21270;&#31639;&#27861;&#20197;&#21450;&#36991;&#20813;&#26114;&#36149;&#30340;&#35745;&#31639;&#21644;&#28508;&#22312;&#30340;&#36127;&#26679;&#26412;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21327;&#21516;&#36807;&#28388;&#65288;CF&#65289;&#34987;&#24191;&#27867;&#29992;&#20110;&#20174;&#35266;&#23519;&#21040;&#30340;&#20132;&#20114;&#20013;&#23398;&#20064;&#26377;&#29992;&#30340;&#29992;&#25143;&#21644;&#39033;&#30446;&#30340;&#28508;&#22312;&#34920;&#31034;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;CF&#30340;&#26041;&#27861;&#36890;&#24120;&#37319;&#29992;&#36127;&#37319;&#26679;&#26469;&#21306;&#20998;&#19981;&#21516;&#30340;&#39033;&#30446;&#12290;&#22312;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#36127;&#37319;&#26679;&#36827;&#34892;&#35757;&#32451;&#35745;&#31639;&#25104;&#26412;&#24456;&#39640;&#12290;&#27492;&#22806;&#65292;&#24517;&#39035;&#26681;&#25454;&#23450;&#20041;&#30340;&#20998;&#24067;&#35880;&#24910;&#36873;&#25321;&#36127;&#39033;&#65292;&#20197;&#36991;&#20813;&#22312;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#36873;&#25321;&#35266;&#23519;&#21040;&#30340;&#27491;&#39033;&#12290;&#19981;&#21487;&#36991;&#20813;&#22320;&#65292;&#20174;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#37319;&#26679;&#30340;&#19968;&#20123;&#36127;&#39033;&#22312;&#27979;&#35797;&#38598;&#20013;&#21487;&#33021;&#26159;&#27491;&#39033;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#38544;&#24335;&#21453;&#39304;&#25512;&#33616;&#22330;&#26223;&#30340;&#33258;&#30417;&#30563;&#21327;&#21516;&#36807;&#28388;&#26694;&#26550;&#65288;SelfCF&#65289;&#12290;&#25152;&#25552;&#20986;&#30340;SelfCF&#26694;&#26550;&#31616;&#21270;&#20102;&#36830;&#20307;&#32593;&#32476;&#65292;&#24182;&#21487;&#36731;&#26494;&#24212;&#29992;&#20110;&#29616;&#26377;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;CF&#27169;&#22411;&#65292;&#25105;&#20204;&#31216;&#20854;&#20026;&#39592;&#24178;&#32593;&#32476;&#12290;SelfCF&#30340;&#20027;&#35201;&#24605;&#24819;&#26159;&#22686;&#24378;&#30001;&#39592;&#24178;&#32593;&#32476;&#29983;&#25104;&#30340;&#36755;&#20986;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Collaborative filtering (CF) is widely used to learn informative latent representations of users and items from observed interactions. Existing CF-based methods commonly adopt negative sampling to discriminate different items. Training with negative sampling on large datasets is computationally expensive. Further, negative items should be carefully sampled under the defined distribution, in order to avoid selecting an observed positive item in the training dataset. Unavoidably, some negative items sampled from the training dataset could be positive in the test set. In this paper, we propose a self-supervised collaborative filtering framework (SelfCF), that is specially designed for recommender scenario with implicit feedback. The proposed SelfCF framework simplifies the Siamese networks and can be easily applied to existing deep-learning based CF models, which we refer to as backbone networks. The main idea of SelfCF is to augment the output embeddings generated by backbone networks, b
&lt;/p&gt;</description></item></channel></rss>