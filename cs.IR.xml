<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>OAT-v2&#26159;&#19968;&#20010;&#21487;&#25193;&#23637;&#19988;&#28789;&#27963;&#30340;&#24320;&#28304;&#21161;&#25163;&#24179;&#21488;&#65292;&#25903;&#25345;&#22810;&#20010;&#39046;&#22495;&#21644;&#29992;&#25143;&#20132;&#20114;&#26041;&#24335;&#65292;&#25552;&#20379;&#27169;&#22359;&#21270;&#31995;&#32479;&#32452;&#20214;&#21644;&#24320;&#25918;&#27169;&#22411;&#19982;&#36719;&#20214;&#65292;&#26377;&#21161;&#20110;&#26410;&#26469;&#22810;&#27169;&#24577;&#34394;&#25311;&#21161;&#25163;&#30340;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2403.00586</link><description>&lt;p&gt;
&#24320;&#25918;&#21161;&#25163;&#24037;&#20855;&#21253;--&#31532;2&#29256;
&lt;/p&gt;
&lt;p&gt;
Open Assistant Toolkit -- version 2
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00586
&lt;/p&gt;
&lt;p&gt;
OAT-v2&#26159;&#19968;&#20010;&#21487;&#25193;&#23637;&#19988;&#28789;&#27963;&#30340;&#24320;&#28304;&#21161;&#25163;&#24179;&#21488;&#65292;&#25903;&#25345;&#22810;&#20010;&#39046;&#22495;&#21644;&#29992;&#25143;&#20132;&#20114;&#26041;&#24335;&#65292;&#25552;&#20379;&#27169;&#22359;&#21270;&#31995;&#32479;&#32452;&#20214;&#21644;&#24320;&#25918;&#27169;&#22411;&#19982;&#36719;&#20214;&#65292;&#26377;&#21161;&#20110;&#26410;&#26469;&#22810;&#27169;&#24577;&#34394;&#25311;&#21161;&#25163;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#24320;&#25918;&#21161;&#25163;&#24037;&#20855;&#21253;&#65288;OAT-v2&#65289;&#30340;&#31532;&#20108;&#20010;&#29256;&#26412;&#65292;&#36825;&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#38754;&#21521;&#20219;&#21153;&#30340;&#23545;&#35805;&#31995;&#32479;&#65292;&#29992;&#20110;&#26500;&#24314;&#29983;&#25104;&#24335;&#31070;&#32463;&#27169;&#22411;&#12290;OAT-v2&#26159;&#19968;&#20010;&#21487;&#25193;&#23637;&#19988;&#28789;&#27963;&#30340;&#21161;&#25163;&#24179;&#21488;&#65292;&#25903;&#25345;&#22810;&#20010;&#39046;&#22495;&#21644;&#29992;&#25143;&#20132;&#20114;&#26041;&#24335;&#12290;&#23427;&#23558;&#29992;&#25143;&#35805;&#35821;&#22788;&#29702;&#20998;&#20026;&#27169;&#22359;&#21270;&#31995;&#32479;&#32452;&#20214;&#65292;&#21253;&#25324;&#21160;&#20316;&#20195;&#30721;&#29983;&#25104;&#12289;&#22810;&#27169;&#24577;&#20869;&#23481;&#26816;&#32034;&#21644;&#30693;&#35782;&#22686;&#24378;&#21709;&#24212;&#29983;&#25104;&#31561;&#23376;&#27169;&#22359;&#12290;&#32463;&#36807;&#22810;&#24180;&#30340;Alexa TaskBot&#25361;&#25112;&#30340;&#24320;&#21457;&#65292;OAT-v2&#26159;&#19968;&#20010;&#32463;&#36807;&#39564;&#35777;&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#22312;&#23454;&#39564;&#21644;&#23454;&#38469;&#37096;&#32626;&#20013;&#36827;&#34892;&#21487;&#25193;&#23637;&#21644;&#20581;&#22766;&#30340;&#23454;&#39564;&#12290;OAT-v2&#25552;&#20379;&#29992;&#20110;&#30740;&#31350;&#21644;&#21830;&#19994;&#24212;&#29992;&#30340;&#24320;&#25918;&#27169;&#22411;&#21644;&#36719;&#20214;&#65292;&#20197;&#20419;&#36827;&#26410;&#26469;&#22810;&#26679;&#24212;&#29992;&#21644;&#20016;&#23500;&#20132;&#20114;&#31867;&#22411;&#30340;&#22810;&#27169;&#24577;&#34394;&#25311;&#21161;&#25163;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00586v1 Announce Type: new  Abstract: We present the second version of the Open Assistant Toolkit (OAT-v2), an open-source task-oriented conversational system for composing generative neural models. OAT-v2 is a scalable and flexible assistant platform supporting multiple domains and modalities of user interaction. It splits processing a user utterance into modular system components, including submodules such as action code generation, multimodal content retrieval, and knowledge-augmented response generation. Developed over multiple years of the Alexa TaskBot challenge, OAT-v2 is a proven system that enables scalable and robust experimentation in experimental and real-world deployment. OAT-v2 provides open models and software for research and commercial applications to enable the future of multimodal virtual assistants across diverse applications and types of rich interaction.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#22823;&#35268;&#27169;&#25512;&#33616;&#31995;&#32479;&#20013;&#26377;&#25928;&#34920;&#31034;&#29992;&#25143;&#21475;&#21619;&#30340;&#36890;&#29992;&#29992;&#25143;&#34920;&#31034;&#26694;&#26550;&#65292;&#32467;&#21512;&#34920;&#31034;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#31649;&#29702;&#29983;&#20135;&#27169;&#22411;&#20013;&#35813;&#26694;&#26550;&#37096;&#32626;&#30340;&#26032;&#39062;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2403.00584</link><description>&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#36890;&#29992;&#29992;&#25143;&#34920;&#31034;&#30340;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Generalized User Representations for Transfer Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00584
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#22823;&#35268;&#27169;&#25512;&#33616;&#31995;&#32479;&#20013;&#26377;&#25928;&#34920;&#31034;&#29992;&#25143;&#21475;&#21619;&#30340;&#36890;&#29992;&#29992;&#25143;&#34920;&#31034;&#26694;&#26550;&#65292;&#32467;&#21512;&#34920;&#31034;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#31649;&#29702;&#29983;&#20135;&#27169;&#22411;&#20013;&#35813;&#26694;&#26550;&#37096;&#32626;&#30340;&#26032;&#39062;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22823;&#35268;&#27169;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#29992;&#25143;&#34920;&#31034;&#65292;&#26088;&#22312;&#20197;&#36890;&#29992;&#26041;&#24335;&#26377;&#25928;&#34920;&#31034;&#22810;&#26679;&#21270;&#30340;&#29992;&#25143;&#21475;&#21619;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#20004;&#38454;&#27573;&#26041;&#27861;&#65292;&#32467;&#21512;&#34920;&#31034;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#12290;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#20351;&#29992;&#33258;&#21160;&#32534;&#30721;&#22120;&#23558;&#21508;&#31181;&#29992;&#25143;&#29305;&#24449;&#21387;&#32553;&#25104;&#34920;&#31034;&#31354;&#38388;&#12290;&#22312;&#31532;&#20108;&#38454;&#27573;&#65292;&#19979;&#28216;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#21033;&#29992;&#29992;&#25143;&#34920;&#31034;&#65292;&#32780;&#19981;&#26159;&#21333;&#29420;&#31574;&#21010;&#29992;&#25143;&#29305;&#24449;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#22312;&#34920;&#31034;&#30340;&#36755;&#20837;&#29305;&#24449;&#19978;&#22686;&#24378;&#36825;&#31181;&#26041;&#27861;&#65292;&#20197;&#22686;&#21152;&#28789;&#27963;&#24615;&#65292;&#24182;&#23454;&#29616;&#23545;&#29992;&#25143;&#20107;&#20214;&#65288;&#21253;&#25324;&#26032;&#29992;&#25143;&#20307;&#39564;&#65289;&#30340;&#20960;&#20046;&#23454;&#26102;&#21453;&#24212;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#31649;&#29702;&#35813;&#26694;&#26550;&#22312;&#29983;&#20135;&#27169;&#22411;&#20013;&#30340;&#37096;&#32626;&#65292;&#20801;&#35768;&#19979;&#28216;&#27169;&#22411;&#29420;&#31435;&#24037;&#20316;&#12290;&#25105;&#20204;&#36890;&#36807;&#20005;&#26684;&#30340;&#32447;&#19979;&#39564;&#35777;&#20102;&#25105;&#20204;&#26694;&#26550;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00584v1 Announce Type: cross  Abstract: We present a novel framework for user representation in large-scale recommender systems, aiming at effectively representing diverse user taste in a generalized manner. Our approach employs a two-stage methodology combining representation learning and transfer learning. The representation learning model uses an autoencoder that compresses various user features into a representation space. In the second stage, downstream task-specific models leverage user representations via transfer learning instead of curating user features individually. We further augment this methodology on the representation's input features to increase flexibility and enable reaction to user events, including new user experiences, in Near-Real Time. Additionally, we propose a novel solution to manage deployment of this framework in production models, allowing downstream models to work independently. We validate the performance of our framework through rigorous offl
&lt;/p&gt;</description></item><item><title>IAI MovieBot 2.0&#22686;&#24378;&#20102;&#21487;&#35757;&#32451;&#30340;&#31070;&#32463;&#20803;&#20214;&#65292;&#36879;&#26126;&#29992;&#25143;&#24314;&#27169;&#21644;&#29992;&#25143;&#30028;&#38754;&#25913;&#36827;&#65292;&#20351;&#20854;&#25104;&#20026;&#19968;&#20010;&#30740;&#31350;&#24179;&#21488;&#12290;</title><link>https://arxiv.org/abs/2403.00520</link><description>&lt;p&gt;
IAI MovieBot 2.0&#65306;&#20855;&#26377;&#21487;&#35757;&#32451;&#31070;&#32463;&#20803;&#20214;&#21644;&#36879;&#26126;&#29992;&#25143;&#24314;&#27169;&#30340;&#22686;&#24378;&#30740;&#31350;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
IAI MovieBot 2.0: An Enhanced Research Platform with Trainable Neural Components and Transparent User Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00520
&lt;/p&gt;
&lt;p&gt;
IAI MovieBot 2.0&#22686;&#24378;&#20102;&#21487;&#35757;&#32451;&#30340;&#31070;&#32463;&#20803;&#20214;&#65292;&#36879;&#26126;&#29992;&#25143;&#24314;&#27169;&#21644;&#29992;&#25143;&#30028;&#38754;&#25913;&#36827;&#65292;&#20351;&#20854;&#25104;&#20026;&#19968;&#20010;&#30740;&#31350;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#23545;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;&#30340;&#20852;&#36259;&#26085;&#30410;&#22686;&#38271;&#65292;&#20294;&#30446;&#21069;&#32570;&#20047;&#36866;&#21512;&#20316;&#20026;&#32508;&#21512;&#30740;&#31350;&#24179;&#21488;&#30340;&#36816;&#20316;&#31995;&#32479;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;IAI MovieBot&#23545;&#35805;&#24335;&#30005;&#24433;&#25512;&#33616;&#31995;&#32479;&#30340;&#22686;&#24378;&#29256;&#26412;&#65292;&#26088;&#22312;&#23558;&#20854;&#21457;&#23637;&#20026;&#19968;&#20010;&#31283;&#20581;&#19988;&#21487;&#36866;&#24212;&#30340;&#24179;&#21488;&#65292;&#29992;&#20110;&#36827;&#34892;&#38754;&#21521;&#29992;&#25143;&#30340;&#23454;&#39564;&#12290;&#27492;&#27425;&#22686;&#24378;&#30340;&#20851;&#38190;&#20142;&#28857;&#21253;&#25324;&#28155;&#21152;&#20102;&#21487;&#35757;&#32451;&#30340;&#31070;&#32463;&#20803;&#20214;&#36827;&#34892;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#23545;&#35805;&#31574;&#30053;&#65292;&#36879;&#26126;&#19988;&#21487;&#35299;&#37322;&#30340;&#29992;&#25143;&#20559;&#22909;&#24314;&#27169;&#65292;&#20197;&#21450;&#25913;&#36827;&#29992;&#25143;&#30028;&#38754;&#21644;&#30740;&#31350;&#22522;&#30784;&#35774;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00520v1 Announce Type: new  Abstract: While interest in conversational recommender systems has been on the rise, operational systems suitable for serving as research platforms for comprehensive studies are currently lacking. This paper introduces an enhanced version of the IAI MovieBot conversational movie recommender system, aiming to evolve it into a robust and adaptable platform for conducting user-facing experiments. The key highlights of this enhancement include the addition of trainable neural components for natural language understanding and dialogue policy, transparent and explainable modeling of user preferences, along with improvements in the user interface and research infrastructure.
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#25512;&#33616;&#27169;&#22411;&#24212;&#23545;&#20445;&#38505;&#39046;&#22495;&#25968;&#25454;&#31232;&#32570;&#24615;&#65292;&#23398;&#20064;&#39044;&#27979;&#20250;&#35805;&#22806;&#30340;&#30446;&#26631;&#21160;&#20316;&#12290;</title><link>https://arxiv.org/abs/2403.00368</link><description>&lt;p&gt;
&#22312;&#25968;&#25454;&#21294;&#20047;&#30340;&#20445;&#38505;&#39046;&#22495;&#25512;&#33616;&#20250;&#35805;&#22806;&#30340;&#30446;&#26631;&#21160;&#20316;
&lt;/p&gt;
&lt;p&gt;
Recommending Target Actions Outside Sessions in the Data-poor Insurance Domain
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00368
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#25512;&#33616;&#27169;&#22411;&#24212;&#23545;&#20445;&#38505;&#39046;&#22495;&#25968;&#25454;&#31232;&#32570;&#24615;&#65292;&#23398;&#20064;&#39044;&#27979;&#20250;&#35805;&#22806;&#30340;&#30446;&#26631;&#21160;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20445;&#38505;&#20135;&#21697;&#20013;&#25552;&#20379;&#20010;&#24615;&#21270;&#25512;&#33616;&#29305;&#21035;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#20445;&#38505;&#39046;&#22495;&#20855;&#26377;&#22266;&#26377;&#21644;&#29420;&#29305;&#30340;&#29305;&#24449;&#12290;&#26412;&#25991;&#38024;&#23545;&#20445;&#38505;&#39046;&#22495;&#25968;&#25454;&#31232;&#32570;&#24615;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#25512;&#33616;&#27169;&#22411;&#65292;&#21033;&#29992;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21644;&#19977;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#25439;&#22833;&#20989;&#25968;&#19982;&#26550;&#26500;&#65288;&#20132;&#21449;&#29109;&#12289;&#25130;&#23614;&#23041;&#24067;&#23572;&#12289;&#27880;&#24847;&#21147;&#65289;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#36890;&#36807;&#23398;&#20064;&#22810;&#20010;&#20250;&#35805;&#21644;&#19981;&#21516;&#31867;&#22411;&#30340;&#29992;&#25143;&#34892;&#20026;&#26469;&#24212;&#23545;&#25968;&#25454;&#31232;&#32570;&#24615;&#12290;&#27492;&#22806;&#65292;&#19982;&#20197;&#24448;&#22522;&#20110;&#20250;&#35805;&#30340;&#27169;&#22411;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#23398;&#20250;&#39044;&#27979;&#20250;&#35805;&#20869;&#26410;&#21457;&#29983;&#30340;&#30446;&#26631;&#21160;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00368v1 Announce Type: new  Abstract: Providing personalized recommendations for insurance products is particularly challenging due to the intrinsic and distinctive features of the insurance domain. First, unlike more traditional domains like retail, movie etc., a large amount of user feedback is not available and the item catalog is smaller. Second, due to the higher complexity of products, the majority of users still prefer to complete their purchases over the phone instead of online. We present different recommender models to address such data scarcity in the insurance domain. We use recurrent neural networks with 3 different types of loss functions and architectures (cross-entropy, censored Weibull, attention). Our models cope with data scarcity by learning from multiple sessions and different types of user actions. Moreover, differently from previous session-based models, our models learn to predict a target action that does not happen within the session. Our models out
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#21452;&#32534;&#30721;&#22120;&#26550;&#26500;&#21644;&#29305;&#27530;&#35774;&#35745;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#26412;&#25991;&#23454;&#29616;&#20102;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21516;&#26102;&#23398;&#20064;&#22522;&#20110;&#31867;&#21035;&#21644;&#22522;&#20110;&#23545;&#35937;&#36523;&#20221;&#30340;&#23884;&#20837;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#23039;&#21183;&#19981;&#21464;&#30340;&#23545;&#35937;&#35782;&#21035;&#21644;&#26816;&#32034;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.00272</link><description>&lt;p&gt;
&#21452;&#37325;&#23039;&#21183;&#19981;&#21464;&#23884;&#20837;&#65306;&#23398;&#20064;&#29992;&#20110;&#35782;&#21035;&#21644;&#26816;&#32034;&#30340;&#31867;&#21035;&#21644;&#23545;&#35937;&#29305;&#23450;&#30340;&#21028;&#21035;&#24615;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Dual Pose-invariant Embeddings: Learning Category and Object-specific Discriminative Representations for Recognition and Retrieval
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00272
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#21452;&#32534;&#30721;&#22120;&#26550;&#26500;&#21644;&#29305;&#27530;&#35774;&#35745;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#26412;&#25991;&#23454;&#29616;&#20102;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21516;&#26102;&#23398;&#20064;&#22522;&#20110;&#31867;&#21035;&#21644;&#22522;&#20110;&#23545;&#35937;&#36523;&#20221;&#30340;&#23884;&#20837;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#23039;&#21183;&#19981;&#21464;&#30340;&#23545;&#35937;&#35782;&#21035;&#21644;&#26816;&#32034;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23039;&#21183;&#19981;&#21464;&#30340;&#23545;&#35937;&#35782;&#21035;&#21644;&#26816;&#32034;&#30340;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21516;&#26102;&#23398;&#20064;&#22522;&#20110;&#31867;&#21035;&#21644;&#22522;&#20110;&#23545;&#35937;&#36523;&#20221;&#30340;&#23884;&#20837;&#26159;&#21487;&#33021;&#30340;&#65292;&#24182;&#19988;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#21452;&#32534;&#30721;&#22120;&#26550;&#26500;&#65292;&#37197;&#21512;&#29305;&#21035;&#35774;&#35745;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#21516;&#26102;&#20248;&#21270;&#20004;&#20010;&#19981;&#21516;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#31867;&#38388;&#21644;&#31867;&#20869;&#36317;&#31163;&#65292;&#19968;&#20010;&#29992;&#20110;&#31867;&#21035;&#23884;&#20837;&#65292;&#21478;&#19968;&#20010;&#29992;&#20110;&#23545;&#35937;&#32423;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00272v1 Announce Type: cross  Abstract: In the context of pose-invariant object recognition and retrieval, we demonstrate that it is possible to achieve significant improvements in performance if both the category-based and the object-identity-based embeddings are learned simultaneously during training. In hindsight, that sounds intuitive because learning about the categories is more fundamental than learning about the individual objects that correspond to those categories. However, to the best of what we know, no prior work in pose-invariant learning has demonstrated this effect. This paper presents an attention-based dual-encoder architecture with specially designed loss functions that optimize the inter- and intra-class distances simultaneously in two different embedding spaces, one for the category embeddings and the other for the object-level embeddings. The loss functions we have proposed are pose-invariant ranking losses that are designed to minimize the intra-class d
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#32771;&#34385;&#20102;&#22312;&#38750;&#38745;&#24577;&#22810;&#33218;&#36172;&#21338;&#26426;&#20013;&#65292;&#36890;&#36807;&#35266;&#23519;&#22870;&#21169;&#26469;&#31215;&#26497;&#21644;&#28040;&#26497;&#22320;&#24378;&#21270;&#20154;&#32676;&#20559;&#22909;&#65292;&#24182;&#25552;&#20986;&#20102;&#29992;&#20110;&#26368;&#22823;&#21270;&#25903;&#25345;&#39044;&#23450;&#25163;&#33218;&#30340;&#20154;&#21475;&#27604;&#20363;&#30340;&#31639;&#27861;&#12290;&#23545;&#20110;&#19981;&#21516;&#24847;&#35265;&#21160;&#24577;&#65292;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#31574;&#30053;&#24182;&#20998;&#26512;&#20102;&#21518;&#24724;&#65292;&#26368;&#21518;&#35752;&#35770;&#20102;&#22810;&#20010;&#25512;&#33616;&#31995;&#32479;&#20849;&#23384;&#30340;&#24773;&#20917;&#12290;</title><link>https://arxiv.org/abs/2403.00036</link><description>&lt;p&gt;
&#24433;&#21709;Bandits&#65306;&#29992;&#20110;&#24418;&#22609;&#20559;&#22909;&#30340;&#25163;&#33218;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Influencing Bandits: Arm Selection for Preference Shaping
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00036
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#32771;&#34385;&#20102;&#22312;&#38750;&#38745;&#24577;&#22810;&#33218;&#36172;&#21338;&#26426;&#20013;&#65292;&#36890;&#36807;&#35266;&#23519;&#22870;&#21169;&#26469;&#31215;&#26497;&#21644;&#28040;&#26497;&#22320;&#24378;&#21270;&#20154;&#32676;&#20559;&#22909;&#65292;&#24182;&#25552;&#20986;&#20102;&#29992;&#20110;&#26368;&#22823;&#21270;&#25903;&#25345;&#39044;&#23450;&#25163;&#33218;&#30340;&#20154;&#21475;&#27604;&#20363;&#30340;&#31639;&#27861;&#12290;&#23545;&#20110;&#19981;&#21516;&#24847;&#35265;&#21160;&#24577;&#65292;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#31574;&#30053;&#24182;&#20998;&#26512;&#20102;&#21518;&#24724;&#65292;&#26368;&#21518;&#35752;&#35770;&#20102;&#22810;&#20010;&#25512;&#33616;&#31995;&#32479;&#20849;&#23384;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#19968;&#20010;&#38750;&#38745;&#24577;&#22810;&#33218;&#36172;&#21338;&#26426;&#65292;&#22312;&#36825;&#20854;&#20013;&#20154;&#32676;&#30340;&#20559;&#22909;&#21463;&#21040;&#35266;&#23519;&#21040;&#30340;&#22870;&#21169;&#30340;&#31215;&#26497;&#21644;&#28040;&#26497;&#24378;&#21270;&#12290;&#31639;&#27861;&#30340;&#30446;&#26631;&#26159;&#22609;&#36896;&#20154;&#32676;&#30340;&#20559;&#22909;&#65292;&#20197;&#26368;&#22823;&#21270;&#25903;&#25345;&#39044;&#23450;&#25163;&#33218;&#30340;&#20154;&#21475;&#27604;&#20363;&#12290;&#23545;&#20110;&#20108;&#20803;&#24847;&#35265;&#30340;&#24773;&#20917;&#65292;&#32771;&#34385;&#20102;&#20004;&#31181;&#24847;&#35265;&#21160;&#24577; -- &#36882;&#20943;&#24377;&#24615;&#65288;&#24314;&#27169;&#20026;&#20855;&#26377;&#22686;&#21152;&#29699;&#25968;&#30340;Polya&#37319;&#26679;&#65289;&#21644;&#24120;&#37327;&#24377;&#24615;&#65288;&#20351;&#29992;&#25237;&#31080;&#32773;&#27169;&#22411;&#65289;&#12290;&#23545;&#20110;&#31532;&#19968;&#31181;&#24773;&#20917;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#25506;&#32034;-&#28982;&#21518;-&#25215;&#35834;&#31574;&#30053;&#21644;&#19968;&#31181;Thompson&#37319;&#26679;&#31574;&#30053;&#65292;&#24182;&#20998;&#26512;&#20102;&#27599;&#31181;&#31574;&#30053;&#30340;&#21518;&#24724;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#31639;&#27861;&#21450;&#20854;&#20998;&#26512;&#21487;&#25512;&#24191;&#21040;&#24120;&#24377;&#24615;&#24773;&#20917;&#12290;&#25105;&#20204;&#36824;&#25551;&#36848;&#20102;&#19968;&#31181;&#22522;&#20110;Thompson&#37319;&#26679;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#24403;&#23384;&#22312;&#20004;&#31181;&#20197;&#19978;&#31867;&#22411;&#30340;&#24847;&#35265;&#24773;&#20917;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#23384;&#22312;&#22810;&#20010;&#25512;&#33616;&#31995;&#32479;&#30340;&#24773;&#20917;&#24341;&#21457;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00036v1 Announce Type: cross  Abstract: We consider a non stationary multi-armed bandit in which the population preferences are positively and negatively reinforced by the observed rewards. The objective of the algorithm is to shape the population preferences to maximize the fraction of the population favouring a predetermined arm. For the case of binary opinions, two types of opinion dynamics are considered -- decreasing elasticity (modeled as a Polya urn with increasing number of balls) and constant elasticity (using the voter model). For the first case, we describe an Explore-then-commit policy and a Thompson sampling policy and analyse the regret for each of these policies. We then show that these algorithms and their analyses carry over to the constant elasticity case. We also describe a Thompson sampling based algorithm for the case when more than two types of opinions are present. Finally, we discuss the case where presence of multiple recommendation systems gives ris
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;Web-DRO&#65292;&#23427;&#21033;&#29992;&#32593;&#32476;&#32467;&#26500;&#36827;&#34892;&#32858;&#31867;&#24182;&#37325;&#26032;&#21152;&#26435;&#65292;&#22312;&#26080;&#30417;&#30563;&#22330;&#26223;&#20013;&#26174;&#33879;&#25552;&#39640;&#20102;&#26816;&#32034;&#25928;&#26524;&#12290;&#32676;&#32452;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#26041;&#27861;&#25351;&#23548;&#27169;&#22411;&#23545;&#39640;&#23545;&#27604;&#25439;&#22833;&#30340;&#32676;&#32452;&#20998;&#37197;&#26356;&#22810;&#26435;&#37325;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#26356;&#21152;&#20851;&#27880;&#26368;&#22351;&#24773;&#20917;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#32467;&#21512;URL&#20449;&#24687;&#30340;&#32593;&#32476;&#22270;&#35757;&#32451;&#33021;&#36798;&#21040;&#26368;&#20339;&#30340;&#32858;&#31867;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.16605</link><description>&lt;p&gt;
&#22522;&#20110;&#32593;&#32476;&#22270;&#30340;&#20998;&#24067;&#40065;&#26834;&#26080;&#30417;&#30563;&#23494;&#38598;&#26816;&#32034;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Distributionally Robust Unsupervised Dense Retrieval Training on Web Graphs. (arXiv:2310.16605v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16605
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;Web-DRO&#65292;&#23427;&#21033;&#29992;&#32593;&#32476;&#32467;&#26500;&#36827;&#34892;&#32858;&#31867;&#24182;&#37325;&#26032;&#21152;&#26435;&#65292;&#22312;&#26080;&#30417;&#30563;&#22330;&#26223;&#20013;&#26174;&#33879;&#25552;&#39640;&#20102;&#26816;&#32034;&#25928;&#26524;&#12290;&#32676;&#32452;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#26041;&#27861;&#25351;&#23548;&#27169;&#22411;&#23545;&#39640;&#23545;&#27604;&#25439;&#22833;&#30340;&#32676;&#32452;&#20998;&#37197;&#26356;&#22810;&#26435;&#37325;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#26356;&#21152;&#20851;&#27880;&#26368;&#22351;&#24773;&#20917;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#32467;&#21512;URL&#20449;&#24687;&#30340;&#32593;&#32476;&#22270;&#35757;&#32451;&#33021;&#36798;&#21040;&#26368;&#20339;&#30340;&#32858;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Web-DRO&#65292;&#19968;&#31181;&#22522;&#20110;&#32593;&#32476;&#32467;&#26500;&#36827;&#34892;&#32858;&#31867;&#24182;&#22312;&#23545;&#27604;&#35757;&#32451;&#26399;&#38388;&#37325;&#26032;&#21152;&#26435;&#30340;&#26080;&#30417;&#30563;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#21033;&#29992;&#32593;&#32476;&#22270;&#38142;&#25509;&#24182;&#23545;&#38170;&#28857;-&#25991;&#26723;&#23545;&#36827;&#34892;&#23545;&#27604;&#35757;&#32451;&#65292;&#35757;&#32451;&#19968;&#20010;&#23884;&#20837;&#27169;&#22411;&#29992;&#20110;&#32858;&#31867;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#32676;&#32452;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#26041;&#27861;&#26469;&#37325;&#26032;&#21152;&#26435;&#19981;&#21516;&#30340;&#38170;&#28857;-&#25991;&#26723;&#23545;&#32676;&#32452;&#65292;&#36825;&#25351;&#23548;&#27169;&#22411;&#23558;&#26356;&#22810;&#26435;&#37325;&#20998;&#37197;&#32473;&#23545;&#27604;&#25439;&#22833;&#26356;&#39640;&#30340;&#32676;&#32452;&#65292;&#24182;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#26356;&#21152;&#20851;&#27880;&#26368;&#22351;&#24773;&#20917;&#12290;&#22312;MS MARCO&#21644;BEIR&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;Web-DRO&#22312;&#26080;&#30417;&#30563;&#22330;&#26223;&#20013;&#26174;&#33879;&#25552;&#39640;&#20102;&#26816;&#32034;&#25928;&#26524;&#12290;&#23545;&#32858;&#31867;&#25216;&#26415;&#30340;&#27604;&#36739;&#34920;&#26126;&#65292;&#32467;&#21512;URL&#20449;&#24687;&#30340;&#32593;&#32476;&#22270;&#35757;&#32451;&#33021;&#36798;&#21040;&#26368;&#20339;&#30340;&#32858;&#31867;&#24615;&#33021;&#12290;&#36827;&#19968;&#27493;&#20998;&#26512;&#35777;&#23454;&#20102;&#32676;&#32452;&#26435;&#37325;&#30340;&#31283;&#23450;&#24615;&#21644;&#26377;&#25928;&#24615;&#65292;&#34920;&#26126;&#20102;&#19968;&#33268;&#30340;&#27169;&#22411;&#20559;&#22909;&#20197;&#21450;&#23545;&#26377;&#20215;&#20540;&#25991;&#26723;&#30340;&#26377;&#25928;&#21152;&#26435;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces Web-DRO, an unsupervised dense retrieval model, which clusters documents based on web structures and reweights the groups during contrastive training. Specifically, we first leverage web graph links and contrastively train an embedding model for clustering anchor-document pairs. Then we use Group Distributional Robust Optimization to reweight different clusters of anchor-document pairs, which guides the model to assign more weights to the group with higher contrastive loss and pay more attention to the worst case during training. Our experiments on MS MARCO and BEIR show that our model, Web-DRO, significantly improves the retrieval effectiveness in unsupervised scenarios. A comparison of clustering techniques shows that training on the web graph combining URL information reaches optimal performance on clustering. Further analysis confirms that group weights are stable and valid, indicating consistent model preferences as well as effective up-weighting of valuable 
&lt;/p&gt;</description></item></channel></rss>