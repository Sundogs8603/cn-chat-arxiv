<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#26597;&#35810;&#25193;&#23637;&#30340;&#25991;&#26723;&#26816;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#20808;&#37325;&#26032;&#25490;&#24207;&#20877;&#36827;&#34892;&#26597;&#35810;&#25193;&#23637;&#65292;&#20197;&#21450;&#24341;&#20837;&#26032;&#30340;&#25193;&#23637;&#27169;&#22411;&#21644;&#33258;&#36866;&#24212;&#32452;&#20214;&#26469;&#25552;&#39640;&#26816;&#32034;&#25928;&#26524;&#12290;&#20351;&#29992;&#35813;&#26041;&#27861;&#65292;&#25105;&#20204;&#22312;NDCG&#12289;MAP&#21644;R@1000&#31561;&#25351;&#26631;&#19978;&#21462;&#24471;&#20102;&#26368;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.17082</link><description>&lt;p&gt;
&#37325;&#26032;&#25490;&#24207;-&#25193;&#23637;-&#37325;&#22797;&#65306;&#20351;&#29992;&#21333;&#35789;&#21644;&#23454;&#20307;&#30340;&#33258;&#36866;&#24212;&#26597;&#35810;&#25193;&#23637;&#30340;&#25991;&#26723;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Re-Rank - Expand - Repeat: Adaptive Query Expansion for Document Retrieval Using Words and Entities. (arXiv:2306.17082v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17082
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#26597;&#35810;&#25193;&#23637;&#30340;&#25991;&#26723;&#26816;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#20808;&#37325;&#26032;&#25490;&#24207;&#20877;&#36827;&#34892;&#26597;&#35810;&#25193;&#23637;&#65292;&#20197;&#21450;&#24341;&#20837;&#26032;&#30340;&#25193;&#23637;&#27169;&#22411;&#21644;&#33258;&#36866;&#24212;&#32452;&#20214;&#26469;&#25552;&#39640;&#26816;&#32034;&#25928;&#26524;&#12290;&#20351;&#29992;&#35813;&#26041;&#27861;&#65292;&#25105;&#20204;&#22312;NDCG&#12289;MAP&#21644;R@1000&#31561;&#25351;&#26631;&#19978;&#21462;&#24471;&#20102;&#26368;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#21644;&#23494;&#38598;&#30340;&#20266;&#30456;&#20851;&#21453;&#39304;(PRf)&#26041;&#27861;&#22312;&#25361;&#25112;&#24615;&#26597;&#35810;&#20013;&#34920;&#29616;&#19981;&#20339;&#65292;&#21407;&#22240;&#26159;&#22312;&#31532;&#19968;&#27425;&#26816;&#32034;&#20013;&#30340;&#20302;&#20934;&#30830;&#29575;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;(NLMs)&#30340;&#36827;&#23637;&#21487;&#20197;&#23558;&#30456;&#20851;&#25991;&#26723;&#37325;&#26032;&#25490;&#21517;&#21040;&#21069;&#20960;&#21517;&#65292;&#21363;&#20351;&#37325;&#26032;&#25490;&#21517;&#27744;&#20013;&#21482;&#26377;&#24456;&#23569;&#30340;&#25991;&#26723;&#12290;&#26412;&#25991;&#39318;&#20808;&#36890;&#36807;&#31616;&#21333;&#22320;&#22312;&#26597;&#35810;&#25193;&#23637;&#20043;&#21069;&#24212;&#29992;&#37325;&#26032;&#25490;&#24207;&#24182;&#37325;&#26032;&#25191;&#34892;&#26597;&#35810;&#26469;&#35299;&#20915;&#20266;&#30456;&#20851;&#21453;&#39304;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#21457;&#29616;&#20165;&#20973;&#36825;&#20010;&#25913;&#21464;&#23601;&#21487;&#20197;&#23558;&#31232;&#30095;&#21644;&#23494;&#38598;PRF&#26041;&#27861;&#30340;&#26816;&#32034;&#25928;&#26524;&#25552;&#21319;5-8%&#12290;&#36827;&#19968;&#27493;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25193;&#23637;&#27169;&#22411;&#65292;&#28508;&#22312;&#23454;&#20307;&#25193;&#23637;(LEE)&#65292;&#23427;&#26159;&#19968;&#31181;&#32454;&#31890;&#24230;&#30340;&#22522;&#20110;&#35789;&#21644;&#23454;&#20307;&#30340;&#30456;&#20851;&#24615;&#24314;&#27169;&#65292;&#21253;&#25324;&#23616;&#37096;&#29305;&#24449;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;&#26816;&#32034;&#36807;&#31243;&#20013;&#24341;&#20837;&#20102;&#19968;&#20010;&#8220;&#33258;&#36866;&#24212;&#8221;&#32452;&#20214;&#65292;&#23427;&#22312;&#35780;&#20998;&#36807;&#31243;&#20013;&#20351;&#29992;&#25193;&#23637;&#27169;&#22411;&#36845;&#20195;&#22320;&#35843;&#25972;&#37325;&#26032;&#25490;&#21517;&#27744;&#65292;&#21363;&#8220;&#37325;&#26032;&#25490;&#24207;-&#25193;&#23637;-&#37325;&#22797;&#8221;&#12290;&#20351;&#29992;LEE&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;(&#25454;&#25105;&#20204;&#25152;&#30693;)&#26368;&#22909;&#30340;NDCG&#12289;MAP&#21644;R@1000&#12290;
&lt;/p&gt;
&lt;p&gt;
Sparse and dense pseudo-relevance feedback (PRF) approaches perform poorly on challenging queries due to low precision in first-pass retrieval. However, recent advances in neural language models (NLMs) can re-rank relevant documents to top ranks, even when few are in the re-ranking pool. This paper first addresses the problem of poor pseudo-relevance feedback by simply applying re-ranking prior to query expansion and re-executing this query. We find that this change alone can improve the retrieval effectiveness of sparse and dense PRF approaches by 5-8%. Going further, we propose a new expansion model, Latent Entity Expansion (LEE), a fine-grained word and entity-based relevance modelling incorporating localized features. Finally, we include an "adaptive" component to the retrieval process, which iteratively refines the re-ranking pool during scoring using the expansion model, i.e. we "re-rank - expand repeat". Using LEE, we achieve (to our knowledge) the best NDCG, MAP and R@1000 re
&lt;/p&gt;</description></item></channel></rss>