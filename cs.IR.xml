<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#24773;&#24863;&#30340;&#22810;&#20041;&#35789;&#28040;&#27495;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#23558;&#26234;&#33021;&#25628;&#32034;&#21151;&#33021;&#23884;&#20837;&#21040;&#25628;&#32034;&#24341;&#25806;&#20013;&#65292;&#21033;&#29992;&#24773;&#24863;&#20998;&#26512;&#25216;&#26415;&#23545;&#25991;&#31456;&#36827;&#34892;&#20998;&#31867;&#21644;&#24402;&#31867;&#65292;&#20197;&#25552;&#20379;&#26356;&#26377;&#24847;&#20041;&#21644;&#20010;&#24615;&#21270;&#30340;&#25628;&#32034;&#32467;&#26524;&#12290;&#35813;&#26041;&#27861;&#30340;&#20934;&#30830;&#29575;&#36798;&#21040;&#20102;85%&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;&#25628;&#32034;&#24341;&#25806;&#12290;</title><link>http://arxiv.org/abs/2311.01895</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#24773;&#24863;&#30340;&#22810;&#20041;&#35789;&#28040;&#27495;&#25552;&#21319;&#25628;&#32034;&#24341;&#25806;&#30340;&#20934;&#30830;&#24615;&#21644;&#29992;&#25143;&#20307;&#39564;
&lt;/p&gt;
&lt;p&gt;
Enhancing search engine precision and user experience through sentiment-based polysemy resolution. (arXiv:2311.01895v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01895
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#24773;&#24863;&#30340;&#22810;&#20041;&#35789;&#28040;&#27495;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#23558;&#26234;&#33021;&#25628;&#32034;&#21151;&#33021;&#23884;&#20837;&#21040;&#25628;&#32034;&#24341;&#25806;&#20013;&#65292;&#21033;&#29992;&#24773;&#24863;&#20998;&#26512;&#25216;&#26415;&#23545;&#25991;&#31456;&#36827;&#34892;&#20998;&#31867;&#21644;&#24402;&#31867;&#65292;&#20197;&#25552;&#20379;&#26356;&#26377;&#24847;&#20041;&#21644;&#20010;&#24615;&#21270;&#30340;&#25628;&#32034;&#32467;&#26524;&#12290;&#35813;&#26041;&#27861;&#30340;&#20934;&#30830;&#29575;&#36798;&#21040;&#20102;85%&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;&#25628;&#32034;&#24341;&#25806;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25968;&#23383;&#20869;&#23481;&#30340;&#28608;&#22686;&#21644;&#39640;&#25928;&#20449;&#24687;&#26816;&#32034;&#30340;&#38656;&#27714;&#65292;&#26412;&#30740;&#31350;&#30340;&#27934;&#35265;&#21487;&#24212;&#29992;&#20110;&#21253;&#25324;&#26032;&#38395;&#26381;&#21153;&#12289;&#30005;&#23376;&#21830;&#21153;&#21644;&#25968;&#23383;&#33829;&#38144;&#22312;&#20869;&#30340;&#21508;&#20010;&#39046;&#22495;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#26356;&#26377;&#24847;&#20041;&#21644;&#20010;&#24615;&#21270;&#30340;&#20307;&#39564;&#12290;&#35813;&#30740;&#31350;&#35299;&#20915;&#20102;&#25628;&#32034;&#24341;&#25806;&#20013;&#24120;&#35265;&#30340;&#22810;&#20041;&#24615;&#38382;&#39064;&#65292;&#21363;&#21516;&#19968;&#20010;&#20851;&#38190;&#35789;&#21487;&#33021;&#26377;&#22810;&#20010;&#24847;&#24605;&#12290;&#23427;&#36890;&#36807;&#23558;&#26234;&#33021;&#25628;&#32034;&#21151;&#33021;&#23884;&#20837;&#21040;&#25628;&#32034;&#24341;&#25806;&#20013;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#35813;&#21151;&#33021;&#21487;&#20197;&#22522;&#20110;&#24773;&#24863;&#21306;&#20998;&#19981;&#21516;&#30340;&#21547;&#20041;&#12290;&#30740;&#31350;&#21033;&#29992;&#24773;&#24863;&#20998;&#26512;&#36825;&#19968;&#24378;&#22823;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#65292;&#26681;&#25454;&#25991;&#31456;&#30340;&#24773;&#24863;&#20542;&#21521;&#36827;&#34892;&#20998;&#31867;&#21644;&#24402;&#31867;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#26377;&#35265;&#22320;&#21644;&#32454;&#33268;&#20837;&#24494;&#30340;&#25628;&#32034;&#32467;&#26524;&#12290;&#25991;&#31456;&#25253;&#21578;&#20102;85%&#30340;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#20934;&#30830;&#29575;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;&#25628;&#32034;&#24341;&#25806;&#12290;&#36825;&#34920;&#26126;&#20102;&#22522;&#20110;&#24773;&#24863;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the proliferation of digital content and the need for efficient information retrieval, this study's insights can be applied to various domains, including news services, e-commerce, and digital marketing, to provide users with more meaningful and tailored experiences. The study addresses the common problem of polysemy in search engines, where the same keyword may have multiple meanings. It proposes a solution to this issue by embedding a smart search function into the search engine, which can differentiate between different meanings based on sentiment. The study leverages sentiment analysis, a powerful natural language processing (NLP) technique, to classify and categorize news articles based on their emotional tone. This can provide more insightful and nuanced search results. The article reports an impressive accuracy rate of 85% for the proposed smart search function, which outperforms conventional search engines. This indicates the effectiveness of the sentiment-based approach. 
&lt;/p&gt;</description></item><item><title>Multi-EuP&#26159;&#19968;&#20010;&#26032;&#30340;&#22810;&#35821;&#35328;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;&#26469;&#33258;&#27431;&#27954;&#35758;&#20250;&#30340;22K&#20010;&#22810;&#35821;&#35328;&#25991;&#26723;&#65292;&#26088;&#22312;&#30740;&#31350;&#20449;&#24687;&#26816;&#32034;&#20013;&#30340;&#20844;&#24179;&#24615;&#65292;&#21253;&#25324;&#35821;&#35328;&#21644;&#20154;&#21475;&#20559;&#35265;&#12290;&#23427;&#25552;&#20379;&#20102;&#30495;&#23454;&#30340;&#22810;&#35821;&#35328;&#35821;&#26009;&#24211;&#21644;&#36328;&#35821;&#35328;&#30456;&#20851;&#24615;&#35780;&#21028;&#65292;&#24182;&#25552;&#20379;&#20102;&#19982;&#25991;&#26723;&#30456;&#20851;&#30340;&#20016;&#23500;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#65292;&#21487;&#29992;&#20110;&#35780;&#20272;&#21333;&#35821;&#21644;&#22810;&#35821;&#20449;&#24687;&#26816;&#32034;&#12290;</title><link>http://arxiv.org/abs/2311.01870</link><description>&lt;p&gt;
&#22810;&#35821;&#35328;&#27431;&#27954;&#35758;&#20250;&#25968;&#25454;&#38598;&#29992;&#20110;&#20998;&#26512;&#20449;&#24687;&#26816;&#32034;&#20013;&#30340;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Multi-EuP: The Multilingual European Parliament Dataset for Analysis of Bias in Information Retrieval. (arXiv:2311.01870v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01870
&lt;/p&gt;
&lt;p&gt;
Multi-EuP&#26159;&#19968;&#20010;&#26032;&#30340;&#22810;&#35821;&#35328;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;&#26469;&#33258;&#27431;&#27954;&#35758;&#20250;&#30340;22K&#20010;&#22810;&#35821;&#35328;&#25991;&#26723;&#65292;&#26088;&#22312;&#30740;&#31350;&#20449;&#24687;&#26816;&#32034;&#20013;&#30340;&#20844;&#24179;&#24615;&#65292;&#21253;&#25324;&#35821;&#35328;&#21644;&#20154;&#21475;&#20559;&#35265;&#12290;&#23427;&#25552;&#20379;&#20102;&#30495;&#23454;&#30340;&#22810;&#35821;&#35328;&#35821;&#26009;&#24211;&#21644;&#36328;&#35821;&#35328;&#30456;&#20851;&#24615;&#35780;&#21028;&#65292;&#24182;&#25552;&#20379;&#20102;&#19982;&#25991;&#26723;&#30456;&#20851;&#30340;&#20016;&#23500;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#65292;&#21487;&#29992;&#20110;&#35780;&#20272;&#21333;&#35821;&#21644;&#22810;&#35821;&#20449;&#24687;&#26816;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;Multi-EuP&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#22810;&#35821;&#35328;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#26469;&#33258;&#27431;&#27954;&#35758;&#20250;&#30340;22K&#20010;&#22810;&#35821;&#35328;&#25991;&#26723;&#65292;&#28085;&#30422;&#20102;24&#31181;&#35821;&#35328;&#12290;&#35813;&#25968;&#25454;&#38598;&#26088;&#22312;&#30740;&#31350;&#22810;&#35821;&#35328;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#29615;&#22659;&#19979;&#30340;&#20844;&#24179;&#24615;&#65292;&#20197;&#20998;&#26512;&#22312;&#25490;&#21517;&#19978;&#30340;&#35821;&#35328;&#21644;&#20154;&#21475;&#20559;&#35265;&#12290;&#23427;&#25317;&#26377;&#19968;&#20010;&#30495;&#23454;&#30340;&#22810;&#35821;&#35328;&#35821;&#26009;&#24211;&#65292;&#20854;&#20013;&#30340;&#20027;&#39064;&#34987;&#32763;&#35793;&#25104;&#20102;&#25152;&#26377;24&#31181;&#35821;&#35328;&#65292;&#24182;&#25552;&#20379;&#36328;&#35821;&#35328;&#30456;&#20851;&#24615;&#35780;&#21028;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#25552;&#20379;&#20102;&#19982;&#25991;&#26723;&#30456;&#20851;&#30340;&#20016;&#23500;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#65292;&#20415;&#20110;&#30740;&#31350;&#20154;&#21475;&#20559;&#35265;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;Multi-EuP&#22312;&#21333;&#35821;&#21644;&#22810;&#35821;&#20449;&#24687;&#26816;&#32034;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#19968;&#39033;&#20851;&#20110;&#26631;&#35760;&#21270;&#31574;&#30053;&#36873;&#25321;&#24341;&#36215;&#30340;&#35821;&#35328;&#20559;&#35265;&#30340;&#21021;&#27493;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Multi-EuP, a new multilingual benchmark dataset, comprising 22K multi-lingual documents collected from the European Parliament, spanning 24 languages. This dataset is designed to investigate fairness in a multilingual information retrieval (IR) context to analyze both language and demographic bias in a ranking context. It boasts an authentic multilingual corpus, featuring topics translated into all 24 languages, as well as cross-lingual relevance judgments. Furthermore, it offers rich demographic information associated with its documents, facilitating the study of demographic bias. We report the effectiveness of Multi-EuP for benchmarking both monolingual and multilingual IR. We also conduct a preliminary experiment on language bias caused by the choice of tokenization strategy.
&lt;/p&gt;</description></item><item><title>SortNet&#26159;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#27604;&#36739;&#22120;&#26469;&#36827;&#34892;&#33258;&#36866;&#24212;&#25490;&#24207;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#36807;&#31243;&#26500;&#24314;&#35757;&#32451;&#38598;&#65292;&#26681;&#25454;&#25104;&#23545;&#39033;&#30446;&#20043;&#38388;&#30340;&#25490;&#24207;&#31034;&#20363;&#26469;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2311.01864</link><description>&lt;p&gt;
SortNet: &#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#25490;&#24207;&#31639;&#27861;&#36827;&#34892;&#23398;&#20064;&#25490;&#24207;
&lt;/p&gt;
&lt;p&gt;
SortNet: Learning To Rank By a Neural-Based Sorting Algorithm. (arXiv:2311.01864v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01864
&lt;/p&gt;
&lt;p&gt;
SortNet&#26159;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#27604;&#36739;&#22120;&#26469;&#36827;&#34892;&#33258;&#36866;&#24212;&#25490;&#24207;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#36807;&#31243;&#26500;&#24314;&#35757;&#32451;&#38598;&#65292;&#26681;&#25454;&#25104;&#23545;&#39033;&#30446;&#20043;&#38388;&#30340;&#25490;&#24207;&#31034;&#20363;&#26469;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#20110;&#30456;&#20851;&#24615;&#25490;&#21517;&#30340;&#38382;&#39064;&#65292;&#21363;&#26681;&#25454;&#32473;&#23450;&#30340;&#26631;&#20934;&#23545;&#19968;&#32452;&#23545;&#35937;&#36827;&#34892;&#25490;&#24207;&#12290;&#30001;&#20110;&#29992;&#25143;&#21487;&#33021;&#20559;&#22909;&#19981;&#21516;&#30340;&#30456;&#20851;&#24615;&#26631;&#20934;&#65292;&#22240;&#27492;&#25490;&#24207;&#31639;&#27861;&#24212;&#35813;&#33021;&#22815;&#26681;&#25454;&#29992;&#25143;&#38656;&#27714;&#36827;&#34892;&#35843;&#25972;&#12290;&#23398;&#20064;&#25490;&#24207;&#30340;&#20219;&#21153;&#22312;&#25991;&#29486;&#20013;&#23384;&#22312;&#20004;&#31181;&#20027;&#35201;&#26041;&#27861;&#65306;1&#65289;&#36890;&#36807;&#31034;&#20363;&#23398;&#20064;&#30340;&#24471;&#20998;&#20989;&#25968;&#65292;&#35780;&#20272;&#27599;&#20010;&#23545;&#35937;&#30340;&#23646;&#24615;&#65292;&#29983;&#25104;&#21487;&#29992;&#20110;&#23545;&#23545;&#35937;&#36827;&#34892;&#25490;&#24207;&#30340;&#32477;&#23545;&#30456;&#20851;&#24615;&#20540;&#65307;2&#65289;&#19968;&#31181;&#25104;&#23545;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#23545;&#35937;&#23545;&#26469;&#23398;&#20064;&#8220;&#20559;&#22909;&#20989;&#25968;&#8221;&#65292;&#23450;&#20041;&#21738;&#19968;&#20010;&#23545;&#35937;&#24212;&#35813;&#39318;&#20808;&#25490;&#21517;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SortNet&#65292;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#27604;&#36739;&#22120;&#26469;&#23545;&#23545;&#35937;&#36827;&#34892;&#33258;&#36866;&#24212;&#25490;&#24207;&#30340;&#31639;&#27861;&#12290;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#38598;&#25552;&#20379;&#20102;&#23545;&#20110;&#25104;&#23545;&#39033;&#30446;&#20043;&#38388;&#25152;&#38656;&#25490;&#24207;&#30340;&#31034;&#20363;&#65292;&#24182;&#19988;&#36890;&#36807;&#36845;&#20195;&#36807;&#31243;&#26500;&#24314;&#65292;&#27599;&#27425;&#36845;&#20195;&#37117;&#20250;&#28155;&#21152;&#26368;&#20855;&#20449;&#24687;&#24615;&#30340;&#35757;&#32451;&#31034;&#20363;&#12290;&#27492;&#22806;&#65292;&#27604;&#36739;&#22120;&#37319;&#29992;&#20102;&#36830;&#25509;&#20027;&#20041;&#20307;&#31995;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
The problem of relevance ranking consists of sorting a set of objects with respect to a given criterion. Since users may prefer different relevance criteria, the ranking algorithms should be adaptable to the user needs. Two main approaches exist in literature for the task of learning to rank: 1) a score function, learned by examples, which evaluates the properties of each object yielding an absolute relevance value that can be used to order the objects or 2) a pairwise approach, where a "preference function" is learned using pairs of objects to define which one has to be ranked first. In this paper, we present SortNet, an adaptive ranking algorithm which orders objects using a neural network as a comparator. The neural network training set provides examples of the desired ordering between pairs of items and it is constructed by an iterative procedure which, at each iteration, adds the most informative training examples. Moreover, the comparator adopts a connectionist architecture that 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#29992;&#22810;&#27169;&#24577;&#22810;&#39046;&#22495;&#39044;&#35757;&#32451;&#25512;&#33616;&#27169;&#22411;UniM^2Rec&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#20174;&#25152;&#26377;&#39046;&#22495;&#24179;&#28369;&#22320;&#23398;&#20064;&#22810;&#27169;&#24577;&#29289;&#21697;&#20869;&#23481;&#21644;&#29992;&#25143;&#20559;&#22909;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#22810;&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.01831</link><description>&lt;p&gt;
&#36890;&#29992;&#22810;&#27169;&#24577;&#22810;&#39046;&#22495;&#39044;&#35757;&#32451;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Universal Multi-modal Multi-domain Pre-trained Recommendation. (arXiv:2311.01831v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01831
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#29992;&#22810;&#27169;&#24577;&#22810;&#39046;&#22495;&#39044;&#35757;&#32451;&#25512;&#33616;&#27169;&#22411;UniM^2Rec&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#20174;&#25152;&#26377;&#39046;&#22495;&#24179;&#28369;&#22320;&#23398;&#20064;&#22810;&#27169;&#24577;&#29289;&#21697;&#20869;&#23481;&#21644;&#29992;&#25143;&#20559;&#22909;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#22810;&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#22810;&#39046;&#22495;&#20132;&#20114;&#27169;&#22411;&#26469;&#24314;&#27169;&#29992;&#25143;&#20559;&#22909;&#30340;&#30740;&#31350;&#20852;&#36259;&#27491;&#22312;&#36805;&#36895;&#22686;&#38271;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#22810;&#39046;&#22495;&#25512;&#33616;&#22823;&#22810;&#36873;&#25321;&#23558;&#29289;&#21697;&#25991;&#26412;&#20316;&#20026;&#36328;&#39046;&#22495;&#30340;&#26725;&#26753;&#65292;&#24182;&#31616;&#21333;&#22320;&#25506;&#32034;&#30446;&#26631;&#39046;&#22495;&#20013;&#30340;&#29992;&#25143;&#34892;&#20026;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#24573;&#35270;&#20102;&#20854;&#20182;&#20449;&#24687;&#20016;&#23500;&#30340;&#22810;&#27169;&#24577;&#29289;&#21697;&#20869;&#23481;&#65288;&#22914;&#35270;&#35273;&#20449;&#24687;&#65289;&#65292;&#24182;&#19988;&#20063;&#32570;&#20047;&#23545;&#25152;&#26377;&#20132;&#20114;&#39046;&#22495;&#30340;&#29992;&#25143;&#34892;&#20026;&#30340;&#24443;&#24213;&#32771;&#34385;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22810;&#39046;&#22495;&#25512;&#33616;&#30340;&#36890;&#29992;&#22810;&#27169;&#24577;&#29289;&#21697;&#20869;&#23481;&#23637;&#31034;&#39044;&#35757;&#32451;&#27169;&#22411;UniM^2Rec&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#24179;&#28369;&#22320;&#23398;&#20064;&#25152;&#26377;&#39046;&#22495;&#30340;&#22810;&#27169;&#24577;&#29289;&#21697;&#20869;&#23481;&#23637;&#31034;&#21644;&#22810;&#27169;&#24577;&#29992;&#25143;&#20559;&#22909;&#12290;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;&#22810;&#39046;&#22495;&#25512;&#33616;&#27169;&#22411;&#65292;UniM^2Rec&#21487;&#20197;&#22312;&#23454;&#36341;&#20013;&#39640;&#25928;&#19988;&#26377;&#25928;&#22320;&#36716;&#31227;&#21040;&#26032;&#30340;&#30446;&#26631;&#39046;&#22495;&#12290;&#22312;&#20116;&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;UniM^2Rec&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is a rapidly-growing research interest in modeling user preferences via pre-training multi-domain interactions for recommender systems. However, Existing pre-trained multi-domain recommendations mostly select the item texts to be bridges across domains, and simply explore the user behaviors in target domains. Hence, they ignore other informative multi-modal item contents (e.g., visual information), and also lack of thorough consideration of user behaviors from all interactive domains. To address these issues, in this paper, we propose to pre-train universal multi-modal item content presentation for multi-domain recommendation, called UniM^2Rec, which could smoothly learn the multi-modal item content presentations and the multi-modal user preferences from all domains. With the pre-trained multi-domain recommendation model, UniM^2Rec could be efficiently and effectively transferred to new target domains in practice. Extensive experiments conducted on five real-world datasets in tar
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35843;&#26597;&#20102;&#24037;&#19994;&#32423;&#23398;&#20064;&#25490;&#24207;&#27169;&#22411;&#30340;&#31163;&#32447;&#35780;&#20272;&#21463;&#21040;&#21518;&#22788;&#29702;&#36923;&#36753;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Birkhoff-von-Neumann&#20998;&#35299;&#30340;&#26657;&#27491;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2311.01828</link><description>&lt;p&gt;
&#20351;&#29992;&#19994;&#21153;&#35268;&#21017;&#36827;&#34892;&#26080;&#20559;&#31163;&#32447;&#35780;&#20272;&#23398;&#20064;&#25490;&#24207;
&lt;/p&gt;
&lt;p&gt;
Unbiased Offline Evaluation for Learning to Rank with Business Rules. (arXiv:2311.01828v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01828
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35843;&#26597;&#20102;&#24037;&#19994;&#32423;&#23398;&#20064;&#25490;&#24207;&#27169;&#22411;&#30340;&#31163;&#32447;&#35780;&#20272;&#21463;&#21040;&#21518;&#22788;&#29702;&#36923;&#36753;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Birkhoff-von-Neumann&#20998;&#35299;&#30340;&#26657;&#27491;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#24037;&#19994;&#32423;&#23398;&#20064;&#25490;&#24207;&#65288;LTR&#65289;&#31995;&#32479;&#26469;&#35828;&#65292;&#36890;&#24120;&#20250;&#23545;&#25490;&#24207;&#27169;&#22411;&#30340;&#36755;&#20986;&#36827;&#34892;&#20462;&#25913;&#65292;&#36825;&#21487;&#33021;&#26159;&#30001;&#20110;&#25191;&#34892;&#19994;&#21153;&#35201;&#27714;&#30340;&#21518;&#22788;&#29702;&#36923;&#36753;&#65292;&#20063;&#21487;&#33021;&#26159;&#30001;&#20110;&#23454;&#38469;&#29983;&#20135;&#31995;&#32479;&#20013;&#23384;&#22312;&#30340;&#35774;&#35745;&#32570;&#38519;&#25110;&#38169;&#35823;&#32780;&#23548;&#33268;&#30340;&#12290;&#36825;&#23545;&#20110;&#37096;&#32626;&#31163;&#32447;&#23398;&#20064;&#21644;&#35780;&#20272;&#26041;&#27861;&#26500;&#25104;&#20102;&#25361;&#25112;&#65292;&#22240;&#20026;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#27169;&#22411;&#35780;&#20998;&#25152;&#26263;&#31034;&#30340;&#25490;&#24207;&#19982;&#21521;&#29992;&#25143;&#26174;&#31034;&#30340;&#39033;&#30446;&#19968;&#33268;&#12290;&#21487;&#38752;&#30340;&#31163;&#32447;&#35780;&#20272;&#36824;&#38656;&#35201;&#27491;&#30830;&#30340;&#38543;&#26426;&#21270;&#21644;&#27491;&#30830;&#20272;&#35745;&#22312;&#25490;&#21517;&#30340;&#20219;&#20309;&#32473;&#23450;&#20301;&#32622;&#19978;&#26174;&#31034;&#27599;&#20010;&#39033;&#30446;&#30340;&#20542;&#21521;&#65292;&#32780;&#36825;&#20123;&#20063;&#21463;&#21040;&#19978;&#36848;&#21518;&#22788;&#29702;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#20102;&#36825;&#20123;&#24773;&#20917;&#22914;&#20309;&#24433;&#21709;&#23398;&#20064;&#25490;&#24207;&#27169;&#22411;&#30340;&#31163;&#32447;&#35780;&#20272;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Birkhoff-von-Neumann&#20998;&#35299;&#30340;&#26032;&#30340;&#26657;&#27491;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23545;&#20110;&#36825;&#31181;&#31867;&#22411;&#30340;&#21518;&#22788;&#29702;&#26159;&#31283;&#20581;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
For industrial learning-to-rank (LTR) systems, it is common that the output of a ranking model is modified, either as a results of post-processing logic that enforces business requirements, or as a result of unforeseen design flaws or bugs present in real-world production systems. This poses a challenge for deploying off-policy learning and evaluation methods, as these often rely on the assumption that rankings implied by the model's scores coincide with displayed items to the users. Further requirements for reliable offline evaluation are proper randomization and correct estimation of the propensities of displaying each item in any given position of the ranking, which are also impacted by the aforementioned post-processing. We investigate empirically how these scenarios impair off-policy evaluation for learning-to-rank models. We then propose a novel correction method based on the Birkhoff-von-Neumann decomposition that is robust to this type of post-processing. We obtain more accurat
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#32852;&#37030;&#24378;&#21270;&#23398;&#20064;&#30340;&#27969;&#34892;&#30149;&#20915;&#31574;&#31995;&#32479;&#21487;&#20197;&#24110;&#21161;&#25919;&#24220;&#22312;&#20860;&#39038;&#20844;&#20849;&#23433;&#20840;&#21644;&#32463;&#27982;&#21457;&#23637;&#26041;&#38754;&#20570;&#20986;&#20915;&#31574;&#65292;&#24182;&#22788;&#29702;&#27969;&#34892;&#30149;&#25968;&#25454;&#26679;&#26412;&#26377;&#38480;&#21644;&#38544;&#31169;&#24615;&#39640;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2311.01749</link><description>&lt;p&gt;
&#22522;&#20110;&#32852;&#37030;&#24378;&#21270;&#23398;&#20064;&#30340;&#27969;&#34892;&#30149;&#20915;&#31574;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Epidemic Decision-making System Based Federated Reinforcement Learning. (arXiv:2311.01749v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01749
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#32852;&#37030;&#24378;&#21270;&#23398;&#20064;&#30340;&#27969;&#34892;&#30149;&#20915;&#31574;&#31995;&#32479;&#21487;&#20197;&#24110;&#21161;&#25919;&#24220;&#22312;&#20860;&#39038;&#20844;&#20849;&#23433;&#20840;&#21644;&#32463;&#27982;&#21457;&#23637;&#26041;&#38754;&#20570;&#20986;&#20915;&#31574;&#65292;&#24182;&#22788;&#29702;&#27969;&#34892;&#30149;&#25968;&#25454;&#26679;&#26412;&#26377;&#38480;&#21644;&#38544;&#31169;&#24615;&#39640;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27969;&#34892;&#30149;&#20915;&#31574;&#21487;&#20197;&#26377;&#25928;&#24110;&#21161;&#25919;&#24220;&#32508;&#21512;&#32771;&#34385;&#20844;&#20849;&#23433;&#20840;&#21644;&#32463;&#27982;&#21457;&#23637;&#65292;&#20197;&#24212;&#23545;&#20844;&#20849;&#21355;&#29983;&#21644;&#23433;&#20840;&#32039;&#24613;&#24773;&#20917;&#12290;&#19968;&#20123;&#30740;&#31350;&#34920;&#26126;&#65292;&#24378;&#21270;&#23398;&#20064;&#21487;&#20197;&#26377;&#25928;&#24110;&#21161;&#25919;&#24220;&#20570;&#20986;&#27969;&#34892;&#30149;&#20915;&#31574;&#65292;&#20174;&#32780;&#23454;&#29616;&#21355;&#29983;&#23433;&#20840;&#21644;&#32463;&#27982;&#21457;&#23637;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#28982;&#32780;&#65292;&#27969;&#34892;&#30149;&#25968;&#25454;&#24448;&#24448;&#20855;&#26377;&#26679;&#26412;&#26377;&#38480;&#21644;&#38544;&#31169;&#24615;&#39640;&#30340;&#29305;&#28857;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#23558;&#21508;&#30465;&#20221;&#30340;&#30123;&#24773;&#25968;&#25454;&#36827;&#34892;&#21512;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Epidemic decision-making can effectively help the government to comprehensively consider public security and economic development to respond to public health and safety emergencies. Epidemic decision-making can effectively help the government to comprehensively consider public security and economic development to respond to public health and safety emergencies. Some studies have shown that intensive learning can effectively help the government to make epidemic decision, thus achieving the balance between health security and economic development. Some studies have shown that intensive learning can effectively help the government to make epidemic decision, thus achieving the balance between health security and economic development. However, epidemic data often has the characteristics of limited samples and high privacy. However, epidemic data often has the characteristics of limited samples and high privacy. This model can combine the epidemic situation data of various provinces for coop
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#24773;&#33410;&#26816;&#32034;&#30340;&#26032;&#20219;&#21153;&#65292;&#36890;&#36807;&#29983;&#25104;&#19968;&#20010;&#26631;&#35760;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#21644;&#35780;&#20272;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#22312;&#27492;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;&#36825;&#20010;&#20219;&#21153;&#35201;&#27714;&#27169;&#22411;&#33021;&#22815;&#20934;&#30830;&#20272;&#35745;&#26597;&#35810;&#21644;&#20505;&#36873;&#24773;&#33410;&#20043;&#38388;&#30340;&#25277;&#35937;&#35821;&#20041;&#20851;&#32852;&#24230;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#20381;&#36182;&#20110;&#35789;&#27719;&#25110;&#35821;&#20041;&#21305;&#37197;&#12290;</title><link>http://arxiv.org/abs/2311.01666</link><description>&lt;p&gt;
&#23558;&#24773;&#33410;&#26816;&#32034;&#20316;&#20026;&#25277;&#35937;&#35821;&#20041;&#20851;&#32852;&#24230;&#35780;&#20272;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Plot Retrieval as an Assessment of Abstract Semantic Association. (arXiv:2311.01666v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01666
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#24773;&#33410;&#26816;&#32034;&#30340;&#26032;&#20219;&#21153;&#65292;&#36890;&#36807;&#29983;&#25104;&#19968;&#20010;&#26631;&#35760;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#21644;&#35780;&#20272;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#22312;&#27492;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;&#36825;&#20010;&#20219;&#21153;&#35201;&#27714;&#27169;&#22411;&#33021;&#22815;&#20934;&#30830;&#20272;&#35745;&#26597;&#35810;&#21644;&#20505;&#36873;&#24773;&#33410;&#20043;&#38388;&#30340;&#25277;&#35937;&#35821;&#20041;&#20851;&#32852;&#24230;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#20381;&#36182;&#20110;&#35789;&#27719;&#25110;&#35821;&#20041;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20070;&#31821;&#20013;&#26816;&#32034;&#30456;&#20851;&#24773;&#33410;&#26159;&#19968;&#39033;&#20851;&#38190;&#20219;&#21153;&#65292;&#21487;&#20197;&#25552;&#39640;&#35835;&#32773;&#30340;&#38405;&#35835;&#20307;&#39564;&#21644;&#25928;&#29575;&#12290;&#35835;&#32773;&#36890;&#24120;&#21482;&#25552;&#20379;&#19968;&#20010;&#22522;&#20110;&#33258;&#24049;&#29702;&#35299;&#12289;&#25688;&#35201;&#25110;&#29468;&#27979;&#30340;&#25277;&#35937;&#21644;&#27169;&#31946;&#30340;&#25551;&#36848;&#20316;&#20026;&#26597;&#35810;&#65292;&#36825;&#35201;&#27714;&#26816;&#32034;&#27169;&#22411;&#20855;&#26377;&#24378;&#22823;&#30340;&#33021;&#21147;&#26469;&#20272;&#35745;&#26597;&#35810;&#21644;&#20505;&#36873;&#24773;&#33410;&#20043;&#38388;&#30340;&#25277;&#35937;&#35821;&#20041;&#20851;&#32852;&#24230;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20449;&#24687;&#26816;&#32034;&#25968;&#25454;&#38598;&#19981;&#33021;&#24456;&#22909;&#22320;&#21453;&#26144;&#36825;&#31181;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#24773;&#33410;&#26816;&#32034;&#30340;&#26631;&#35760;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;IR&#27169;&#22411;&#22312;&#26032;&#20219;&#21153;&#24773;&#33410;&#26816;&#32034;&#19978;&#30340;&#24615;&#33021;&#12290;&#24773;&#33410;&#26816;&#32034;&#20013;&#30340;&#25991;&#26412;&#23545;&#20855;&#26377;&#36739;&#23569;&#30340;&#35789;&#37325;&#21472;&#21644;&#26356;&#22810;&#30340;&#25277;&#35937;&#35821;&#20041;&#20851;&#32852;&#24230;&#65292;&#21487;&#20197;&#21453;&#26144;IR&#27169;&#22411;&#20272;&#35745;&#25277;&#35937;&#35821;&#20041;&#20851;&#32852;&#24230;&#30340;&#33021;&#21147;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#20256;&#32479;&#30340;&#35789;&#27719;&#25110;&#35821;&#20041;&#21305;&#37197;&#12290;&#36890;&#36807;&#21508;&#31181;&#35789;&#27719;&#26816;&#32034;&#12289;&#31232;&#30095;&#26816;&#32034;&#21644;&#23494;&#38598;&#26816;&#32034;&#30340;&#22823;&#37327;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
Retrieving relevant plots from the book for a query is a critical task, which can improve the reading experience and efficiency of readers. Readers usually only give an abstract and vague description as the query based on their own understanding, summaries, or speculations of the plot, which requires the retrieval model to have a strong ability to estimate the abstract semantic associations between the query and candidate plots. However, existing information retrieval (IR) datasets cannot reflect this ability well. In this paper, we propose Plot Retrieval, a labeled dataset to train and evaluate the performance of IR models on the novel task Plot Retrieval. Text pairs in Plot Retrieval have less word overlap and more abstract semantic association, which can reflect the ability of the IR models to estimate the abstract semantic association, rather than just traditional lexical or semantic matching. Extensive experiments across various lexical retrieval, sparse retrieval, dense retrieval
&lt;/p&gt;</description></item><item><title>&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;&#65306;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25351;&#20196;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19968;&#23545;&#19968;&#25490;&#24207;&#33021;&#21147;&#33976;&#39311;&#20026;&#26356;&#39640;&#25928;&#30340;&#21333;&#28857;&#25490;&#24207;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#38646;-shot&#25490;&#24207;&#22120;&#30340;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.01555</link><description>&lt;p&gt;
&#32763;&#35793;&#21518;&#30340;&#35770;&#25991;&#26631;&#39064;&#65306;&#25351;&#20196;&#33976;&#39311;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25104;&#20026;&#39640;&#25928;&#30340;&#38646;-shot&#25490;&#24207;&#22120;
&lt;/p&gt;
&lt;p&gt;
Instruction Distillation Makes Large Language Models Efficient Zero-shot Rankers. (arXiv:2311.01555v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01555
&lt;/p&gt;
&lt;p&gt;
&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;&#65306;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25351;&#20196;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19968;&#23545;&#19968;&#25490;&#24207;&#33021;&#21147;&#33976;&#39311;&#20026;&#26356;&#39640;&#25928;&#30340;&#21333;&#28857;&#25490;&#24207;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#38646;-shot&#25490;&#24207;&#22120;&#30340;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#38646;-shot&#30456;&#20851;&#24615;&#25490;&#24207;&#22120;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;&#20856;&#22411;&#30340;&#26041;&#27861;&#28041;&#21450;&#23545;&#25991;&#26723;&#36827;&#34892;&#19968;&#23545;&#19968;&#25110;&#19968;&#23545;&#22810;&#30340;&#27604;&#36739;&#12290;&#23613;&#31649;&#36825;&#20123;&#19968;&#23545;&#22810;&#21644;&#19968;&#23545;&#19968;&#30340;&#26041;&#27861;&#26377;&#25928;&#65292;&#20294;&#25928;&#29575;&#19981;&#39640;&#65292;&#19988;&#20005;&#37325;&#20381;&#36182;&#22797;&#26434;&#30340;&#25552;&#31034;&#24037;&#31243;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25351;&#20196;&#33976;&#39311;&#26041;&#27861;&#12290;&#20854;&#26680;&#24515;&#24605;&#24819;&#26159;&#23558;&#24320;&#28304;LLMs&#30340;&#19968;&#23545;&#19968;&#25490;&#24207;&#33021;&#21147;&#33976;&#39311;&#20026;&#26356;&#31616;&#21333;&#20294;&#26356;&#39640;&#25928;&#30340;&#21333;&#28857;&#25490;&#24207;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#32473;&#23450;&#30456;&#21516;&#30340;LLMs&#65292;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#22797;&#26434;&#30340;&#25351;&#20196;&#37319;&#29992;&#26377;&#25928;&#30340;&#19968;&#23545;&#19968;&#26041;&#27861;&#23545;&#25991;&#26723;&#36827;&#34892;&#25490;&#24207;&#65292;&#28982;&#21518;&#23558;&#25945;&#24072;&#30340;&#39044;&#27979;&#32467;&#26524;&#36716;&#21270;&#20026;&#37319;&#29992;&#26356;&#31616;&#21333;&#30340;&#25351;&#20196;&#30340;&#21333;&#28857;&#25490;&#24207;&#26041;&#27861;&#12290;&#22312;BEIR&#12289;TREC&#21644;ReDial&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#25351;&#20196;&#33976;&#39311;&#21487;&#20197;&#23558;&#25928;&#29575;&#25552;&#39640;10&#21040;100&#20493;&#65292;&#21516;&#26102;&#25552;&#39640;LLMs&#30340;&#25490;&#24207;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36229;&#36807;&#20102;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Recent studies have demonstrated the great potential of Large Language Models (LLMs) serving as zero-shot relevance rankers. The typical approach involves making comparisons between pairs or lists of documents. Although effective, these listwise and pairwise methods are not efficient and also heavily rely on intricate prompt engineering. To tackle this problem, we introduce a novel instruction distillation method. The key idea is to distill the pairwise ranking ability of open-sourced LLMs to a simpler but more efficient pointwise ranking. Specifically, given the same LLM, we first rank documents using the effective pairwise approach with complex instructions, and then distill the teacher predictions to the pointwise approach with simpler instructions. Evaluation results on the BEIR, TREC, and ReDial datasets demonstrate that instruction distillation can improve efficiency by 10 to 100x and also enhance the ranking performance of LLMs. Furthermore, our approach surpasses the performanc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#20351;&#29992;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#32534;&#30721;&#22120;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#25512;&#33616;&#24615;&#33021;&#12290;&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;&#25512;&#33616;&#31995;&#32479;&#20027;&#35201;&#20381;&#36182;&#20110;&#21333;&#29420;&#25552;&#21462;&#30340;&#29305;&#24449;&#65292;&#24182;&#19988;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#21482;&#26377;&#27973;&#23618;&#27425;&#30340;&#23545;&#40784;&#65292;&#32780;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#32534;&#30721;&#22120;&#33021;&#22815;&#26356;&#22909;&#22320;&#25429;&#25417;&#27169;&#24577;&#20043;&#38388;&#30340;&#28508;&#22312;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2310.20343</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#32534;&#30721;&#22120;&#29992;&#20110;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Large Multi-modal Encoders for Recommendation. (arXiv:2310.20343v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20343
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#20351;&#29992;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#32534;&#30721;&#22120;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#25512;&#33616;&#24615;&#33021;&#12290;&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;&#25512;&#33616;&#31995;&#32479;&#20027;&#35201;&#20381;&#36182;&#20110;&#21333;&#29420;&#25552;&#21462;&#30340;&#29305;&#24449;&#65292;&#24182;&#19988;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#21482;&#26377;&#27973;&#23618;&#27425;&#30340;&#23545;&#40784;&#65292;&#32780;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#32534;&#30721;&#22120;&#33021;&#22815;&#26356;&#22909;&#22320;&#25429;&#25417;&#27169;&#24577;&#20043;&#38388;&#30340;&#28508;&#22312;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#24555;&#36895;&#22686;&#38271;&#30340;&#22312;&#32447;&#22810;&#23186;&#20307;&#26381;&#21153;&#65288;&#22914;&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#65289;&#20351;&#24471;&#38656;&#35201;&#24320;&#21457;&#20010;&#24615;&#21270;&#25512;&#33616;&#26041;&#27861;&#26469;&#23545;&#27599;&#20010;&#39033;&#30446;&#30340;&#22810;&#26679;&#20869;&#23481;&#36827;&#34892;&#32534;&#30721;&#12290;&#29616;&#20195;&#30340;&#22810;&#27169;&#24577;&#25512;&#33616;&#31995;&#32479;&#21033;&#29992;&#20174;&#21407;&#22987;&#22270;&#20687;&#21644;&#29289;&#21697;&#25551;&#36848;&#20013;&#33719;&#21462;&#30340;&#22810;&#31181;&#29305;&#24449;&#26469;&#25552;&#39640;&#25512;&#33616;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;&#25512;&#33616;&#31995;&#32479;&#20027;&#35201;&#20381;&#36182;&#20110;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;&#23186;&#20307;&#29305;&#23450;&#32534;&#30721;&#22120;&#21333;&#29420;&#25552;&#21462;&#30340;&#29305;&#24449;&#65292;&#24182;&#19988;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#21482;&#26377;&#27973;&#23618;&#27425;&#30340;&#23545;&#40784;&#65292;&#38480;&#21046;&#20102;&#36825;&#20123;&#31995;&#32479;&#25429;&#25417;&#27169;&#24577;&#20043;&#38388;&#28508;&#22312;&#20851;&#31995;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#22312;&#25512;&#33616;&#31995;&#32479;&#29305;&#23450;&#32972;&#26223;&#19979;&#20351;&#29992;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#32534;&#30721;&#22120;&#30340;&#29992;&#27861;&#65292;&#22240;&#20026;&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#35780;&#20272;&#29289;&#21697;&#25490;&#21517;&#26102;&#65292;&#36825;&#20123;&#32534;&#30721;&#22120;&#20197;&#21069;&#24050;&#32463;&#23637;&#31034;&#20986;&#26368;&#20808;&#36827;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, the rapid growth of online multimedia services, such as e-commerce platforms, has necessitated the development of personalised recommendation approaches that can encode diverse content about each item. Indeed, modern multi-modal recommender systems exploit diverse features obtained from raw images and item descriptions to enhance the recommendation performance. However, the existing multi-modal recommenders primarily depend on the features extracted individually from different media through pre-trained modality-specific encoders, and exhibit only shallow alignments between different modalities - limiting these systems' ability to capture the underlying relationships between the modalities. In this paper, we investigate the usage of large multi-modal encoders within the specific context of recommender systems, as these have previously demonstrated state-of-the-art effectiveness when ranking items across various domains. Specifically, we tailor two state-of-the-art multi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#27010;&#24565;&#65292;&#21363;&#22522;&#20110;&#21518;&#26524;&#30340;&#35299;&#37322;&#65292;&#20197;&#24378;&#35843;&#25512;&#33616;&#39033;&#23545;&#29992;&#25143;&#20010;&#20154;&#28040;&#36153;&#34892;&#20026;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#25552;&#21319;&#25512;&#33616;&#31995;&#32479;&#30340;&#29992;&#25143;&#20307;&#39564;&#21644;&#28385;&#24847;&#24230;&#12290;</title><link>http://arxiv.org/abs/2308.16708</link><description>&lt;p&gt;
&#19987;&#27880;&#20110;&#24433;&#21709;&#65306;&#22522;&#20110;&#21518;&#26524;&#30340;&#25512;&#33616;&#31995;&#32479;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Concentrating on the Impact: Consequence-based Explanations in Recommender Systems. (arXiv:2308.16708v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16708
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#27010;&#24565;&#65292;&#21363;&#22522;&#20110;&#21518;&#26524;&#30340;&#35299;&#37322;&#65292;&#20197;&#24378;&#35843;&#25512;&#33616;&#39033;&#23545;&#29992;&#25143;&#20010;&#20154;&#28040;&#36153;&#34892;&#20026;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#25552;&#21319;&#25512;&#33616;&#31995;&#32479;&#30340;&#29992;&#25143;&#20307;&#39564;&#21644;&#28385;&#24847;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#22312;&#29992;&#25143;&#20915;&#31574;&#20013;&#36215;&#21040;&#36741;&#21161;&#20316;&#29992;&#65292;&#25512;&#33616;&#39033;&#30340;&#21576;&#29616;&#26041;&#24335;&#21644;&#35299;&#37322;&#26159;&#25552;&#21319;&#29992;&#25143;&#20307;&#39564;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#29983;&#25104;&#35299;&#37322;&#30340;&#26041;&#27861;&#65292;&#20294;&#20173;&#26377;&#25913;&#36827;&#30340;&#31354;&#38388;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#22312;&#29305;&#23450;&#39046;&#22495;&#32570;&#20047;&#19987;&#19994;&#30693;&#35782;&#30340;&#29992;&#25143;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#27010;&#24565;&#65292;&#21363;&#22522;&#20110;&#21518;&#26524;&#30340;&#35299;&#37322;&#65292;&#36825;&#31181;&#35299;&#37322;&#24378;&#35843;&#25512;&#33616;&#39033;&#23545;&#29992;&#25143;&#20010;&#20154;&#28040;&#36153;&#34892;&#20026;&#30340;&#24433;&#21709;&#65292;&#20351;&#24471;&#36981;&#24490;&#25512;&#33616;&#30340;&#25928;&#26524;&#26356;&#21152;&#28165;&#26224;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#22312;&#32447;&#29992;&#25143;&#30740;&#31350;&#65292;&#20197;&#39564;&#35777;&#20851;&#20110;&#21518;&#26524;&#35299;&#37322;&#30340;&#27427;&#36175;&#24230;&#20197;&#21450;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#19981;&#21516;&#35299;&#37322;&#30446;&#26631;&#19978;&#30340;&#24433;&#21709;&#30340;&#20551;&#35774;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#21518;&#26524;&#35299;&#37322;&#30340;&#37325;&#35201;&#24615;&#24471;&#21040;&#20102;&#29992;&#25143;&#30340;&#35748;&#21487;&#65292;&#24182;&#19988;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#29992;&#25143;&#28385;&#24847;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommender systems assist users in decision-making, where the presentation of recommended items and their explanations are critical factors for enhancing the overall user experience. Although various methods for generating explanations have been proposed, there is still room for improvement, particularly for users who lack expertise in a specific item domain. In this study, we introduce the novel concept of \textit{consequence-based explanations}, a type of explanation that emphasizes the individual impact of consuming a recommended item on the user, which makes the effect of following recommendations clearer. We conducted an online user study to examine our assumption about the appreciation of consequence-based explanations and their impacts on different explanation aims in recommender systems. Our findings highlight the importance of consequence-based explanations, which were well-received by users and effectively improved user satisfaction in recommender systems. These results prov
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#20010;&#24615;&#21270;&#22270;&#24418;&#25552;&#31034;&#25512;&#33616;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#65288;CL&#65289;&#29983;&#25104;&#39044;&#35757;&#32451;&#30340;&#23884;&#20837;&#65292;&#20174;&#32780;&#22312;&#36328;&#22495;&#25512;&#33616;&#20013;&#25552;&#39640;&#20102;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.10685</link><description>&lt;p&gt;
&#36328;&#22495;&#25512;&#33616;&#20013;&#23545;&#27604;&#22270;&#24418;&#25552;&#31034;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
Contrastive Graph Prompt-tuning for Cross-domain Recommendation. (arXiv:2308.10685v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10685
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#20010;&#24615;&#21270;&#22270;&#24418;&#25552;&#31034;&#25512;&#33616;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#65288;CL&#65289;&#29983;&#25104;&#39044;&#35757;&#32451;&#30340;&#23884;&#20837;&#65292;&#20174;&#32780;&#22312;&#36328;&#22495;&#25512;&#33616;&#20013;&#25552;&#39640;&#20102;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#32463;&#24120;&#38754;&#20020;&#25968;&#25454;&#31232;&#30095;&#38382;&#39064;&#12290;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#20043;&#19968;&#26159;&#37319;&#29992;&#36328;&#22495;&#25512;&#33616;&#25216;&#26415;&#12290;&#22312;&#36328;&#22495;&#24773;&#22659;&#19979;&#65292;&#39046;&#22495;&#20043;&#38388;&#30340;&#30693;&#35782;&#20849;&#20139;&#21487;&#20197;&#25552;&#39640;&#30446;&#26631;&#39046;&#22495;&#30340;&#25928;&#26524;&#12290;&#26368;&#36817;&#30340;&#36328;&#22495;&#26041;&#27861;&#37319;&#29992;&#20102;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#20294;&#25105;&#20204;&#35748;&#20026;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#23548;&#33268;&#27425;&#20248;&#30340;&#24494;&#35843;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#22823;&#22411;&#31070;&#32463;&#27169;&#22411;&#12290;&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#21033;&#29992;&#25552;&#31034;&#36827;&#34892;&#39640;&#25928;&#30340;&#27169;&#22411;&#35843;&#20248;&#12290;&#36825;&#20123;&#25552;&#31034;&#20316;&#20026;&#21487;&#35843;&#25972;&#30340;&#28508;&#22312;&#21521;&#37327;&#65292;&#21487;&#20197;&#20923;&#32467;&#20027;&#35201;&#27169;&#22411;&#21442;&#25968;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20010;&#24615;&#21270;&#22270;&#24418;&#25552;&#31034;&#25512;&#33616;&#65288;PGPRec&#65289;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#20102;&#25552;&#31034;&#35843;&#20248;&#30340;&#20248;&#21183;&#12290;&#22312;&#35813;&#26694;&#26550;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#65288;CL&#65289;&#20135;&#29983;&#39044;&#35757;&#32451;&#30340;&#23884;&#20837;&#65292;&#36825;&#20123;&#23884;&#20837;&#25552;&#20379;&#20102;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommender systems are frequently challenged by the data sparsity problem. One approach to mitigate this issue is through cross-domain recommendation techniques. In a cross-domain context, sharing knowledge between domains can enhance the effectiveness in the target domain. Recent cross-domain methods have employed a pre-training approach, but we argue that these methods often result in suboptimal fine-tuning, especially with large neural models. Modern language models utilize prompts for efficient model tuning. Such prompts act as a tunable latent vector, allowing for the freezing of the main model parameters. In our research, we introduce the Personalised Graph Prompt-based Recommendation (PGPRec) framework. This leverages the advantages of prompt-tuning. Within this framework, we formulate personalized graph prompts item-wise, rooted in items that a user has previously engaged with. Specifically, we employ Contrastive Learning (CL) to produce pre-trained embeddings that offer great
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;ChatGPT&#35821;&#35328;&#27169;&#22411;&#20174;GTFS&#25968;&#25454;&#20013;&#26816;&#32034;&#20449;&#24687;&#30340;&#21487;&#34892;&#24615;&#65292;&#39564;&#35777;&#20102;ChatGPT&#65288;GPT-3.5&#65289;&#22312;GTFS&#35268;&#33539;&#29702;&#35299;&#21644;&#20449;&#24687;&#25552;&#21462;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#31243;&#24207;&#21512;&#25104;&#26041;&#27861;&#22312;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#20934;&#30830;&#29575;&#65292;&#20026;&#35299;&#20915;GTFS&#25968;&#25454;&#20449;&#24687;&#33719;&#21462;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.02618</link><description>&lt;p&gt;
ChatGPT&#29992;&#20110;GTFS: &#20174;&#25991;&#23383;&#21040;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
ChatGPT for GTFS: From Words to Information. (arXiv:2308.02618v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02618
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;ChatGPT&#35821;&#35328;&#27169;&#22411;&#20174;GTFS&#25968;&#25454;&#20013;&#26816;&#32034;&#20449;&#24687;&#30340;&#21487;&#34892;&#24615;&#65292;&#39564;&#35777;&#20102;ChatGPT&#65288;GPT-3.5&#65289;&#22312;GTFS&#35268;&#33539;&#29702;&#35299;&#21644;&#20449;&#24687;&#25552;&#21462;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#31243;&#24207;&#21512;&#25104;&#26041;&#27861;&#22312;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#20934;&#30830;&#29575;&#65292;&#20026;&#35299;&#20915;GTFS&#25968;&#25454;&#20449;&#24687;&#33719;&#21462;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24191;&#27867;&#20351;&#29992;&#30340;&#20844;&#20132;&#36890;&#34892;&#25968;&#25454;&#21457;&#24067;&#26631;&#20934;General Transit Feed Specification&#65288;GTFS&#65289;&#26159;&#34920;&#26684;&#25968;&#25454;&#65292;&#20449;&#24687;&#20998;&#25955;&#22312;&#19981;&#21516;&#30340;&#25991;&#20214;&#20013;&#65292;&#38656;&#35201;&#19987;&#38376;&#30340;&#24037;&#20855;&#25110;&#21253;&#26469;&#26816;&#32034;&#20449;&#24687;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25991;&#26412;&#21644;&#20449;&#24687;&#26816;&#32034;&#30340;&#36235;&#21183;&#20063;&#22312;&#22686;&#38271;&#12290;&#26412;&#30740;&#31350;&#30340;&#24819;&#27861;&#26159;&#30475;&#30475;&#24403;&#21069;&#24191;&#27867;&#37319;&#29992;&#30340;LLMs&#65288;ChatGPT&#65289;&#26159;&#21542;&#33021;&#22815;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#20174;GTFS&#20013;&#26816;&#32034;&#20449;&#24687;&#12290;&#25105;&#20204;&#39318;&#20808;&#27979;&#35797;ChatGPT&#65288;GPT-3.5&#65289;&#26159;&#21542;&#29702;&#35299;GTFS&#35268;&#33539;&#12290;GPT-3.5&#22312;&#25105;&#20204;&#30340;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#65288;MCQ&#65289;&#20013;&#27491;&#30830;&#22238;&#31572;&#20102;77%&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#21033;&#29992;&#36807;&#28388;&#30340;GTFS&#25968;&#25454;&#38598;&#23545;LLM&#36827;&#34892;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#12290;&#23545;&#20110;&#20449;&#24687;&#26816;&#32034;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#38646;-shot&#21644;&#31243;&#24207;&#21512;&#25104;&#12290;&#31243;&#24207;&#21512;&#25104;&#30340;&#25928;&#26524;&#26356;&#22909;&#65292;&#22312;&#31616;&#21333;&#38382;&#39064;&#19978;&#36798;&#21040;&#20102;&#32422;90%&#30340;&#20934;&#30830;&#29575;&#65292;&#22312;&#22797;&#26434;&#38382;&#39064;&#19978;&#36798;&#21040;&#20102;&#32422;40%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
The General Transit Feed Specification (GTFS) standard for publishing transit data is ubiquitous. GTFS being tabular data, with information spread across different files, necessitates specialized tools or packages to retrieve information. Concurrently, the use of Large Language Models for text and information retrieval is growing. The idea of this research is to see if the current widely adopted LLMs (ChatGPT) are able to retrieve information from GTFS using natural language instructions. We first test whether ChatGPT (GPT-3.5) understands the GTFS specification. GPT-3.5 answers 77% of our multiple-choice questions (MCQ) correctly. Next, we task the LLM with information extractions from a filtered GTFS feed with 4 routes. For information retrieval, we compare zero-shot and program synthesis. Program synthesis works better, achieving ~90% accuracy on simple questions and ~40% accuracy on complex questions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#24605;&#32771;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#19979;&#23545;&#35805;&#22411;&#25512;&#33616;&#31995;&#32479;&#30340;&#35780;&#20272;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20132;&#20114;&#24335;&#35780;&#20272;&#26041;&#27861;iEvaLM&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30456;&#36739;&#20110;&#29616;&#26377;&#35780;&#20272;&#21327;&#35758;&#20855;&#26377;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#24182;&#24378;&#35843;&#20102;&#23545;&#22806;&#37096;&#30693;&#35782;&#30340;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2305.13112</link><description>&lt;p&gt;
&#22312;&#22823;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#37325;&#26032;&#24605;&#32771;&#23545;&#35805;&#22411;&#25512;&#33616;&#31995;&#32479;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Rethinking the Evaluation for Conversational Recommendation in the Era of Large Language Models. (arXiv:2305.13112v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13112
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#24605;&#32771;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#19979;&#23545;&#35805;&#22411;&#25512;&#33616;&#31995;&#32479;&#30340;&#35780;&#20272;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20132;&#20114;&#24335;&#35780;&#20272;&#26041;&#27861;iEvaLM&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30456;&#36739;&#20110;&#29616;&#26377;&#35780;&#20272;&#21327;&#35758;&#20855;&#26377;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#24182;&#24378;&#35843;&#20102;&#23545;&#22806;&#37096;&#30693;&#35782;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25104;&#21151;&#34920;&#26126;&#20854;&#22312;&#21457;&#23637;&#26356;&#24378;&#22823;&#30340;&#23545;&#35805;&#22411;&#25512;&#33616;&#31995;&#32479;&#65288;CRSs&#65289;&#26041;&#38754;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#36825;&#20123;&#31995;&#32479;&#20381;&#36182;&#20110;&#33258;&#28982;&#35821;&#35328;&#23545;&#35805;&#26469;&#28385;&#36275;&#29992;&#25143;&#38656;&#27714;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#21033;&#29992;ChatGPT&#36827;&#34892;&#23545;&#35805;&#22411;&#25512;&#33616;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#25581;&#31034;&#20102;&#29616;&#26377;&#35780;&#20272;&#21327;&#35758;&#30340;&#19981;&#36275;&#20043;&#22788;&#12290;&#29616;&#26377;&#35780;&#20272;&#21327;&#35758;&#21487;&#33021;&#36807;&#20998;&#24378;&#35843;&#19982;&#30001;&#20154;&#31867;&#26631;&#27880;&#32773;&#29983;&#25104;&#30340;&#22320;&#38754;&#30495;&#23454;&#29289;&#21697;&#25110;&#35805;&#35821;&#30340;&#21305;&#37197;&#65292;&#32780;&#24573;&#35270;&#20102;&#20316;&#20026;&#19968;&#31181;&#26377;&#33021;&#21147;&#30340;CRS&#30340;&#20132;&#20114;&#24615;&#36136;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#31181;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LLMs&#30340;&#20132;&#20114;&#24335;&#35780;&#20272;&#26041;&#27861;&#65292;&#21517;&#20026;iEvaLM&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#22522;&#20110;LLMs&#30340;&#29992;&#25143;&#27169;&#25311;&#22120;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#26041;&#27861;&#21487;&#20197;&#27169;&#25311;&#29992;&#25143;&#21644;&#31995;&#32479;&#20043;&#38388;&#30340;&#21508;&#31181;&#20132;&#20114;&#22330;&#26223;&#12290;&#36890;&#36807;&#23545;&#20004;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;CRS&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19982;&#27969;&#34892;&#30340;&#35780;&#20272;&#21327;&#35758;&#30456;&#27604;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#22806;&#37096;&#30693;&#35782;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent success of large language models (LLMs) has shown great potential to develop more powerful conversational recommender systems (CRSs), which rely on natural language conversations to satisfy user needs. In this paper, we embark on an investigation into the utilization of ChatGPT for conversational recommendation, revealing the inadequacy of the existing evaluation protocol. It might over-emphasize the matching with the ground-truth items or utterances generated by human annotators, while neglecting the interactive nature of being a capable CRS. To overcome the limitation, we further propose an interactive Evaluation approach based on LLMs named iEvaLM that harnesses LLM-based user simulators. Our evaluation approach can simulate various interaction scenarios between users and systems. Through the experiments on two publicly available CRS datasets, we demonstrate notable improvements compared to the prevailing evaluation protocol. Furthermore, we emphasize the evaluation of ex
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#20219;&#21153;&#23548;&#21521;&#30340;&#23545;&#35805;&#31995;&#32479;&#30340;&#38544;&#21947;&#29992;&#25143;&#27169;&#25311;&#22120;&#12290;&#35813;&#27169;&#25311;&#22120;&#21487;&#20197;&#29983;&#25104;&#31867;&#20284;&#20154;&#31867;&#23545;&#35805;&#30340;&#27169;&#25311;&#35780;&#20272;&#65292;&#24182;&#19988;&#25552;&#20379;&#20102;&#19968;&#20010;&#27979;&#35797;&#32773;&#26694;&#26550;&#26469;&#29983;&#25104;&#19981;&#21516;&#33021;&#21147;&#30340;&#23545;&#35805;&#31995;&#32479;&#21464;&#20307;&#12290;</title><link>http://arxiv.org/abs/2204.00763</link><description>&lt;p&gt;
&#29992;&#20110;&#35780;&#20272;&#20219;&#21153;&#23548;&#21521;&#30340;&#23545;&#35805;&#31995;&#32479;&#30340;&#38544;&#21947;&#29992;&#25143;&#27169;&#25311;&#22120;
&lt;/p&gt;
&lt;p&gt;
Metaphorical User Simulators for Evaluating Task-oriented Dialogue Systems. (arXiv:2204.00763v5 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.00763
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#20219;&#21153;&#23548;&#21521;&#30340;&#23545;&#35805;&#31995;&#32479;&#30340;&#38544;&#21947;&#29992;&#25143;&#27169;&#25311;&#22120;&#12290;&#35813;&#27169;&#25311;&#22120;&#21487;&#20197;&#29983;&#25104;&#31867;&#20284;&#20154;&#31867;&#23545;&#35805;&#30340;&#27169;&#25311;&#35780;&#20272;&#65292;&#24182;&#19988;&#25552;&#20379;&#20102;&#19968;&#20010;&#27979;&#35797;&#32773;&#26694;&#26550;&#26469;&#29983;&#25104;&#19981;&#21516;&#33021;&#21147;&#30340;&#23545;&#35805;&#31995;&#32479;&#21464;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20219;&#21153;&#23548;&#21521;&#30340;&#23545;&#35805;&#31995;&#32479;&#65288;TDS&#65289;&#20027;&#35201;&#22312;&#31163;&#32447;&#29615;&#22659;&#20013;&#25110;&#36890;&#36807;&#20154;&#24037;&#35780;&#20272;&#36827;&#34892;&#35780;&#20272;&#12290;&#35780;&#20272;&#36890;&#24120;&#20165;&#38480;&#20110;&#21333;&#36718;&#25110;&#38750;&#24120;&#32791;&#26102;&#12290;&#20316;&#20026;&#19968;&#20010;&#26367;&#20195;&#26041;&#26696;&#65292;&#27169;&#25311;&#29992;&#25143;&#34892;&#20026;&#30340;&#29992;&#25143;&#27169;&#25311;&#22120;&#21487;&#20197;&#35753;&#25105;&#20204;&#32771;&#34385;&#19968;&#31995;&#21015;&#29992;&#25143;&#30446;&#26631;&#65292;&#20197;&#29983;&#25104;&#31867;&#20284;&#20154;&#31867;&#23545;&#35805;&#30340;&#27169;&#25311;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#21033;&#29992;&#29616;&#26377;&#29992;&#25143;&#27169;&#25311;&#22120;&#35780;&#20272;TDS&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#29992;&#25143;&#27169;&#25311;&#22120;&#20027;&#35201;&#35774;&#35745;&#29992;&#20110;&#20248;&#21270;TDS&#30340;&#23545;&#35805;&#31574;&#30053;&#65292;&#24182;&#20855;&#26377;&#26377;&#38480;&#30340;&#35780;&#20272;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#29992;&#25143;&#27169;&#25311;&#22120;&#30340;&#35780;&#20272;&#26159;&#19968;&#20010;&#24320;&#25918;&#24615;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#31471;&#21040;&#31471;TDS&#35780;&#20272;&#30340;&#38544;&#21947;&#29992;&#25143;&#27169;&#25311;&#22120;&#65292;&#20854;&#20013;&#25105;&#20204;&#23558;&#27169;&#25311;&#22120;&#23450;&#20041;&#20026;&#38544;&#21947;&#24615;&#30340;&#65292;&#22914;&#26524;&#23427;&#22312;&#19982;&#31995;&#32479;&#30340;&#20132;&#20114;&#20013;&#27169;&#25311;&#29992;&#25143;&#30340;&#31867;&#27604;&#24605;&#32500;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#27979;&#35797;&#32773;&#30340;&#35780;&#20272;&#26694;&#26550;&#26469;&#29983;&#25104;&#20855;&#26377;&#19981;&#21516;&#33021;&#21147;&#30340;&#23545;&#35805;&#31995;&#32479;&#21464;&#20307;&#12290;&#25105;&#20204;&#30340;&#29992;&#25143;&#27169;&#25311;&#22120;&#26500;&#24314;&#20102;&#19968;&#20010;&#38544;&#21947;&#24615;&#29992;&#25143;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Task-oriented dialogue systems (TDSs) are assessed mainly in an offline setting or through human evaluation. The evaluation is often limited to single-turn or is very time-intensive. As an alternative, user simulators that mimic user behavior allow us to consider a broad set of user goals to generate human-like conversations for simulated evaluation. Employing existing user simulators to evaluate TDSs is challenging as user simulators are primarily designed to optimize dialogue policies for TDSs and have limited evaluation capabilities. Moreover, the evaluation of user simulators is an open challenge.  In this work, we propose a metaphorical user simulator for end-to-end TDS evaluation, where we define a simulator to be metaphorical if it simulates user's analogical thinking in interactions with systems. We also propose a tester-based evaluation framework to generate variants, i.e., dialogue systems with different capabilities. Our user simulator constructs a metaphorical user model th
&lt;/p&gt;</description></item></channel></rss>