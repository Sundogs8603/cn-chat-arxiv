<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#24046;&#20998;&#38544;&#31169;&#23545;&#25512;&#33616;&#20934;&#30830;&#24615;&#21644;&#27969;&#34892;&#20559;&#24046;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#24212;&#29992;&#24046;&#20998;&#38544;&#31169;&#21518;&#25512;&#33616;&#20934;&#30830;&#24615;&#26174;&#33879;&#19979;&#38477;&#65292;&#25512;&#33616;&#29289;&#21697;&#30340;&#27969;&#34892;&#24230;&#22823;&#24133;&#22686;&#21152;&#65292;&#23588;&#20854;&#23545;&#20110;&#21916;&#27426;&#19981;&#21463;&#27426;&#36814;&#29289;&#21697;&#30340;&#29992;&#25143;&#24433;&#21709;&#26356;&#20026;&#20005;&#37325;&#12290;</title><link>http://arxiv.org/abs/2401.03883</link><description>&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#23545;&#25512;&#33616;&#20934;&#30830;&#24615;&#21644;&#27969;&#34892;&#20559;&#24046;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
The Impact of Differential Privacy on Recommendation Accuracy and Popularity Bias. (arXiv:2401.03883v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03883
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#24046;&#20998;&#38544;&#31169;&#23545;&#25512;&#33616;&#20934;&#30830;&#24615;&#21644;&#27969;&#34892;&#20559;&#24046;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#24212;&#29992;&#24046;&#20998;&#38544;&#31169;&#21518;&#25512;&#33616;&#20934;&#30830;&#24615;&#26174;&#33879;&#19979;&#38477;&#65292;&#25512;&#33616;&#29289;&#21697;&#30340;&#27969;&#34892;&#24230;&#22823;&#24133;&#22686;&#21152;&#65292;&#23588;&#20854;&#23545;&#20110;&#21916;&#27426;&#19981;&#21463;&#27426;&#36814;&#29289;&#21697;&#30340;&#29992;&#25143;&#24433;&#21709;&#26356;&#20026;&#20005;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#21327;&#21516;&#36807;&#28388;&#30340;&#25512;&#33616;&#31995;&#32479;&#21033;&#29992;&#22823;&#37327;&#30340;&#29992;&#25143;&#34892;&#20026;&#25968;&#25454;&#65292;&#36825;&#24102;&#26469;&#20102;&#20005;&#37325;&#30340;&#38544;&#31169;&#39118;&#38505;&#12290;&#22240;&#27492;&#65292;&#36890;&#24120;&#20250;&#21521;&#25968;&#25454;&#20013;&#28155;&#21152;&#38543;&#26426;&#22122;&#22768;&#20197;&#30830;&#20445;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#20154;&#20204;&#23545;DP&#22914;&#20309;&#24433;&#21709;&#20010;&#24615;&#21270;&#25512;&#33616;&#30340;&#26041;&#24335;&#24182;&#19981;&#28165;&#26970;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;DP&#24212;&#29992;&#20110;&#26368;&#20808;&#36827;&#30340;&#25512;&#33616;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#26102;&#65292;&#23545;&#25512;&#33616;&#20934;&#30830;&#24615;&#21644;&#27969;&#34892;&#20559;&#24046;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#26377;&#19977;&#20010;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#21457;&#29616;&#20960;&#20046;&#25152;&#26377;&#29992;&#25143;&#30340;&#25512;&#33616;&#37117;&#20250;&#22312;&#24212;&#29992;DP&#21518;&#21457;&#29983;&#21464;&#21270;&#12290;&#20854;&#27425;&#65292;&#25512;&#33616;&#20934;&#30830;&#24615;&#22823;&#24133;&#19979;&#38477;&#65292;&#32780;&#25512;&#33616;&#29289;&#21697;&#30340;&#27969;&#34892;&#24230;&#24613;&#21095;&#22686;&#21152;&#65292;&#34920;&#26126;&#27969;&#34892;&#20559;&#24046;&#21152;&#21095;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#19982;&#21916;&#27426;&#27969;&#34892;&#29289;&#21697;&#30340;&#29992;&#25143;&#30456;&#27604;&#65292;&#21916;&#27426;&#19981;&#21463;&#27426;&#36814;&#29289;&#21697;&#30340;&#29992;&#25143;&#20250;&#26356;&#20005;&#37325;&#22320;&#21463;&#21040;DP&#30340;&#27969;&#34892;&#20559;&#24046;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Collaborative filtering-based recommender systems leverage vast amounts of behavioral user data, which poses severe privacy risks. Thus, often, random noise is added to the data to ensure Differential Privacy (DP). However, to date, it is not well understood, in which ways this impacts personalized recommendations. In this work, we study how DP impacts recommendation accuracy and popularity bias, when applied to the training data of state-of-the-art recommendation models. Our findings are three-fold: First, we find that nearly all users' recommendations change when DP is applied. Second, recommendation accuracy drops substantially while recommended item popularity experiences a sharp increase, suggesting that popularity bias worsens. Third, we find that DP exacerbates popularity bias more severely for users who prefer unpopular items than for users that prefer popular items.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#22810;&#26041;&#38754;&#23494;&#38598;&#26816;&#32034;&#22120;&#30340;&#21487;&#37325;&#29616;&#24615;&#20998;&#26512;&#19982;&#22686;&#24378;&#12290;&#36890;&#36807;&#23545;MADRAL&#27169;&#22411;&#30340;&#22797;&#29616;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#20174;&#22836;&#23398;&#20064;&#8220;OTHER&#8221;&#22312;&#26041;&#38754;&#34701;&#21512;&#20013;&#26159;&#26377;&#23475;&#30340;&#65292;&#32780;&#25105;&#20204;&#25552;&#20986;&#30340;&#26367;&#20195;&#26041;&#26696;&#21487;&#20197;&#22823;&#22823;&#25552;&#39640;&#26816;&#32034;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.03648</link><description>&lt;p&gt;
&#12298;&#22810;&#26041;&#38754;&#23494;&#38598;&#26816;&#32034;&#22120;&#19982;&#26041;&#38754;&#23398;&#20064;&#30340;&#21487;&#37325;&#29616;&#24615;&#20998;&#26512;&#19982;&#22686;&#24378;&#12299;&#30340;&#32763;&#35793;&#26631;&#39064;
&lt;/p&gt;
&lt;p&gt;
Reproducibility Analysis and Enhancements for Multi-Aspect Dense Retriever with Aspect Learning. (arXiv:2401.03648v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03648
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#22810;&#26041;&#38754;&#23494;&#38598;&#26816;&#32034;&#22120;&#30340;&#21487;&#37325;&#29616;&#24615;&#20998;&#26512;&#19982;&#22686;&#24378;&#12290;&#36890;&#36807;&#23545;MADRAL&#27169;&#22411;&#30340;&#22797;&#29616;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#20174;&#22836;&#23398;&#20064;&#8220;OTHER&#8221;&#22312;&#26041;&#38754;&#34701;&#21512;&#20013;&#26159;&#26377;&#23475;&#30340;&#65292;&#32780;&#25105;&#20204;&#25552;&#20986;&#30340;&#26367;&#20195;&#26041;&#26696;&#21487;&#20197;&#22823;&#22823;&#25552;&#39640;&#26816;&#32034;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26041;&#38754;&#23494;&#38598;&#26816;&#32034;&#26088;&#22312;&#23558;&#26041;&#38754;&#20449;&#24687;&#65288;&#20363;&#22914;&#21697;&#29260;&#21644;&#31867;&#21035;&#65289;&#19982;&#21452;&#32534;&#30721;&#22120;&#30456;&#32467;&#21512;&#65292;&#20197;&#20419;&#36827;&#30456;&#20851;&#24615;&#21305;&#37197;&#12290;&#20316;&#20026;&#19968;&#20010;&#26089;&#26399;&#20195;&#34920;&#24615;&#30340;&#22810;&#26041;&#38754;&#23494;&#38598;&#26816;&#32034;&#22120;&#65292;MADRAL&#23398;&#20064;&#20102;&#20960;&#20010;&#39069;&#22806;&#30340;&#26041;&#38754;&#23884;&#20837;&#65292;&#24182;&#23558;&#26174;&#24615;&#26041;&#38754;&#19982;&#38544;&#24615;&#26041;&#38754;&#8220;OTHER&#8221;&#34701;&#21512;&#20197;&#33719;&#21462;&#26368;&#32456;&#34920;&#31034;&#12290;MADRAL&#22312;&#19987;&#26377;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#20294;&#20854;&#20195;&#30721;&#26410;&#21457;&#24067;&#65292;&#36825;&#20351;&#24471;&#38590;&#20197;&#39564;&#35777;&#20854;&#22312;&#20854;&#20182;&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#26410;&#33021;&#22312;&#20844;&#24320;&#30340;MA-Amazon&#25968;&#25454;&#19978;&#22797;&#29616;&#20854;&#26377;&#25928;&#24615;&#65292;&#22240;&#27492;&#25105;&#20204;&#24076;&#26395;&#25506;&#32034;&#21407;&#22240;&#24182;&#37325;&#26032;&#23457;&#35270;&#20854;&#32452;&#25104;&#37096;&#20998;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#31181;&#32452;&#20214;&#26367;&#20195;&#26041;&#26696;&#36827;&#34892;&#27604;&#36739;&#65292;&#21253;&#25324;&#23558;&#8220;OTHER&#8221;&#26367;&#25442;&#20026;&#8220;CLS&#8221;&#24182;&#20351;&#29992;&#21069;&#20960;&#20010;&#20869;&#23481;&#26631;&#35760;&#34920;&#31034;&#26041;&#38754;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#35777;&#23454;&#65292;&#20174;&#22836;&#24320;&#22987;&#23398;&#20064;&#8220;OTHER&#8221;&#22312;&#26041;&#38754;&#34701;&#21512;&#20013;&#26159;&#26377;&#23475;&#30340;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#21464;&#20307;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#26816;&#32034;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#19981;&#20165;&#25581;&#31034;&#20102;&#21407;&#26377;&#27169;&#22411;&#30340;&#38382;&#39064;&#65292;&#36824;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-aspect dense retrieval aims to incorporate aspect information (e.g., brand and category) into dual encoders to facilitate relevance matching. As an early and representative multi-aspect dense retriever, MADRAL learns several extra aspect embeddings and fuses the explicit aspects with an implicit aspect "OTHER" for final representation. MADRAL was evaluated on proprietary data and its code was not released, making it challenging to validate its effectiveness on other datasets. We failed to reproduce its effectiveness on the public MA-Amazon data, motivating us to probe the reasons and re-examine its components. We propose several component alternatives for comparisons, including replacing "OTHER" with "CLS" and representing aspects with the first several content tokens. Through extensive experiments, we confirm that learning "OTHER" from scratch in aspect fusion is harmful. In contrast, our proposed variants can greatly enhance the retrieval performance. Our research not only shed
&lt;/p&gt;</description></item><item><title>Starling&#26159;&#19968;&#31181;I/O&#39640;&#25928;&#30340;&#22522;&#20110;&#30913;&#30424;&#30340;&#22270;&#32034;&#24341;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#25968;&#25454;&#29255;&#27573;&#19978;&#36827;&#34892;&#39640;&#32500;&#21521;&#37327;&#30456;&#20284;&#24615;&#25628;&#32034;&#65292;&#22312;&#20934;&#30830;&#24615;&#12289;&#25928;&#29575;&#21644;&#31354;&#38388;&#25104;&#26412;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2401.02116</link><description>&lt;p&gt;
Starling: &#19968;&#31181;&#29992;&#20110;&#39640;&#32500;&#21521;&#37327;&#30456;&#20284;&#24615;&#25628;&#32034;&#30340;I/O&#39640;&#25928;&#30340;&#22522;&#20110;&#30913;&#30424;&#30340;&#22270;&#32034;&#24341;&#26694;&#26550;&#65292;&#29992;&#20110;&#25968;&#25454;&#29255;&#27573;&#20013; (arXiv:2401.02116v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
Starling: An I/O-Efficient Disk-Resident Graph Index Framework for High-Dimensional Vector Similarity Search on Data Segment. (arXiv:2401.02116v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02116
&lt;/p&gt;
&lt;p&gt;
Starling&#26159;&#19968;&#31181;I/O&#39640;&#25928;&#30340;&#22522;&#20110;&#30913;&#30424;&#30340;&#22270;&#32034;&#24341;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#25968;&#25454;&#29255;&#27573;&#19978;&#36827;&#34892;&#39640;&#32500;&#21521;&#37327;&#30456;&#20284;&#24615;&#25628;&#32034;&#65292;&#22312;&#20934;&#30830;&#24615;&#12289;&#25928;&#29575;&#21644;&#31354;&#38388;&#25104;&#26412;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#32500;&#21521;&#37327;&#30456;&#20284;&#24615;&#25628;&#32034;(HVSS)&#20316;&#20026;&#25968;&#25454;&#31185;&#23398;&#21644;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#27491;&#21463;&#21040;&#20851;&#27880;&#12290;&#38543;&#30528;&#21521;&#37327;&#25968;&#25454;&#30340;&#22686;&#38271;&#65292;&#20869;&#23384;&#32034;&#24341;&#21464;&#24471;&#38750;&#24120;&#26114;&#36149;&#65292;&#22240;&#20026;&#23427;&#20204;&#38656;&#35201;&#22823;&#37327;&#25193;&#23637;&#20027;&#20869;&#23384;&#36164;&#28304;&#12290;&#19968;&#31181;&#21487;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#20351;&#29992;&#22522;&#20110;&#30913;&#30424;&#30340;&#23454;&#29616;&#65292;&#23558;&#21521;&#37327;&#25968;&#25454;&#23384;&#20648;&#21644;&#25628;&#32034;&#22312;&#39640;&#24615;&#33021;&#35774;&#22791;(&#22914;NVMe SSD)&#20013;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#25968;&#25454;&#29255;&#27573;&#30340;HVSS&#20173;&#28982;&#26159;&#21521;&#37327;&#25968;&#25454;&#24211;&#20013;&#30340;&#25361;&#25112;&#65292;&#20854;&#20013;&#19968;&#20010;&#26426;&#22120;&#26377;&#22810;&#20010;&#29255;&#27573;&#26469;&#23454;&#29616;&#31995;&#32479;&#21151;&#33021;&#65288;&#22914;&#25193;&#23637;&#65289;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#27599;&#20010;&#29255;&#27573;&#30340;&#20869;&#23384;&#21644;&#30913;&#30424;&#31354;&#38388;&#26377;&#38480;&#65292;&#22240;&#27492;&#25968;&#25454;&#29255;&#27573;&#19978;&#30340;HVSS&#38656;&#35201;&#22312;&#20934;&#30830;&#24615;&#65292;&#25928;&#29575;&#21644;&#31354;&#38388;&#25104;&#26412;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;&#30913;&#30424;&#30340;&#26041;&#27861;&#24182;&#27809;&#26377;&#21516;&#26102;&#32771;&#34385;&#21040;&#25152;&#26377;&#36825;&#20123;&#35201;&#27714;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Starling&#65292;&#19968;&#31181;I/O&#39640;&#25928;&#30340;&#22522;&#20110;&#30913;&#30424;&#30340;&#22270;&#32034;&#24341;&#26694;&#26550;&#65292;&#23427;&#22312;&#29255;&#27573;&#20013;&#20248;&#21270;&#25968;&#25454;&#24067;&#23616;&#21644;&#25628;&#32034;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-dimensional vector similarity search (HVSS) is receiving a spotlight as a powerful tool for various data science and AI applications. As vector data grows larger, in-memory indexes become extremely expensive because they necessitate substantial expansion of main memory resources. One possible solution is to use disk-based implementation, which stores and searches vector data in high-performance devices like NVMe SSDs. However, HVSS for data segments is still challenging in vector databases, where one machine has multiple segments for system features (like scaling) purposes. In this setting, each segment has limited memory and disk space, so HVSS on the data segment needs to balance accuracy, efficiency, and space cost. Existing disk-based methods are sub-optimal because they do not consider all these requirements together. In this paper, we present Starling, an I/O-efficient disk-resident graph index framework that optimizes data layout and search strategy in the segment. It has t
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#26597;&#35770;&#25991;&#31995;&#32479;&#22320;&#12289;&#26368;&#26032;&#22320;&#22238;&#39038;&#20102;&#38024;&#23545;&#25512;&#33616;&#31995;&#32479;&#30340;&#27602;&#21270;&#25915;&#20987;&#30740;&#31350;&#39046;&#22495;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#32780;&#20840;&#38754;&#30340;&#20998;&#31867;&#27861;&#65292;&#23637;&#31034;&#20102;&#28508;&#22312;&#30340;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#24320;&#28304;&#24211;ARLib&#29992;&#20110;&#27604;&#36739;&#27602;&#21270;&#25915;&#20987;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.01527</link><description>&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#24694;&#24847;&#25915;&#20987;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Poisoning Attacks against Recommender Systems: A Survey. (arXiv:2401.01527v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01527
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#26597;&#35770;&#25991;&#31995;&#32479;&#22320;&#12289;&#26368;&#26032;&#22320;&#22238;&#39038;&#20102;&#38024;&#23545;&#25512;&#33616;&#31995;&#32479;&#30340;&#27602;&#21270;&#25915;&#20987;&#30740;&#31350;&#39046;&#22495;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#32780;&#20840;&#38754;&#30340;&#20998;&#31867;&#27861;&#65292;&#23637;&#31034;&#20102;&#28508;&#22312;&#30340;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#24320;&#28304;&#24211;ARLib&#29992;&#20110;&#27604;&#36739;&#27602;&#21270;&#25915;&#20987;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#25512;&#33616;&#31995;&#32479;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#24694;&#24847;&#27963;&#21160;&#30340;&#25915;&#20987;&#65292;&#29305;&#21035;&#26159;&#27602;&#21270;&#25915;&#20987;&#12290;&#36825;&#20123;&#25915;&#20987;&#28041;&#21450;&#23558;&#24694;&#24847;&#25968;&#25454;&#27880;&#20837;&#21040;&#25512;&#33616;&#31995;&#32479;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#65292;&#20174;&#32780;&#25439;&#23475;&#20854;&#23436;&#25972;&#24615;&#65292;&#24182;&#36890;&#36807;&#25805;&#32437;&#25512;&#33616;&#32467;&#26524;&#26469;&#33719;&#21462;&#38750;&#27861;&#21033;&#28070;&#12290;&#26412;&#35843;&#26597;&#35770;&#25991;&#31995;&#32479;&#22320;&#12289;&#26368;&#26032;&#22320;&#22238;&#39038;&#20102;&#38024;&#23545;&#25512;&#33616;&#31995;&#32479;&#30340;&#27602;&#21270;&#25915;&#20987;&#30740;&#31350;&#39046;&#22495;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#32780;&#20840;&#38754;&#30340;&#20998;&#31867;&#27861;&#65292;&#23558;&#29616;&#26377;&#30340;&#27602;&#21270;&#25915;&#20987;&#26041;&#27861;&#20998;&#20026;&#19977;&#20010;&#19981;&#21516;&#30340;&#31867;&#21035;&#65306;&#32452;&#20214;&#29305;&#23450;&#12289;&#30446;&#26631;&#39537;&#21160;&#21644;&#33021;&#21147;&#25506;&#27979;&#12290;&#23545;&#20110;&#27599;&#20010;&#31867;&#21035;&#65292;&#25105;&#20204;&#35814;&#32454;&#35752;&#35770;&#20102;&#20854;&#26426;&#21046;&#20197;&#21450;&#30456;&#20851;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#31361;&#20986;&#20102;&#35813;&#39046;&#22495;&#30340;&#28508;&#22312;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;&#21478;&#22806;&#65292;&#20026;&#20102;&#20419;&#36827;&#21644;&#22522;&#20934;&#27979;&#35797;&#27602;&#21270;&#25915;&#20987;&#30340;&#23454;&#35777;&#27604;&#36739;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#24320;&#28304;&#24211;ARLib&#65292;&#35813;&#24211;&#21253;&#21547;&#20102;&#19968;&#25972;&#22871;&#27602;&#21270;&#25915;&#20987;&#27169;&#22411;&#21644;&#24120;&#29992;&#30340;&#27979;&#35797;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern recommender systems have seen substantial success, yet they remain vulnerable to malicious activities, notably poisoning attacks. These attacks involve injecting malicious data into the training datasets of RS, thereby compromising their integrity and manipulating recommendation outcomes for gaining illicit profits. This survey paper provides a systematic and up-to-date review of the research landscape on Poisoning Attacks against Recommendation (PAR). A novel and comprehensive taxonomy is proposed, categorizing existing PAR methodologies into three distinct categories: Component-Specific, Goal-Driven, and Capability Probing. For each category, we discuss its mechanism in detail, along with associated methods. Furthermore, this paper highlights potential future research avenues in this domain. Additionally, to facilitate and benchmark the empirical comparison of PAR, we introduce an open-source library, ARLib, which encompasses a comprehensive collection of PAR models and common
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;RecRanker&#65292;&#19987;&#38376;&#29992;&#20110;&#25351;&#20196;&#35843;&#20248;LLM&#20197;&#20316;&#20026;&#21069;k&#39033;&#25512;&#33616;&#30340;&#25490;&#21517;&#22120;&#12290;</title><link>http://arxiv.org/abs/2312.16018</link><description>&lt;p&gt;
RecRanker: &#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25490;&#21517;&#22120;&#36827;&#34892;&#21069;k&#39033;&#25512;&#33616;&#30340;&#25351;&#20196;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
RecRanker: Instruction Tuning Large Language Model as Ranker for Top-k Recommendation. (arXiv:2312.16018v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.16018
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;RecRanker&#65292;&#19987;&#38376;&#29992;&#20110;&#25351;&#20196;&#35843;&#20248;LLM&#20197;&#20316;&#20026;&#21069;k&#39033;&#25512;&#33616;&#30340;&#25490;&#21517;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#31034;&#20102;&#21331;&#36234;&#30340;&#33021;&#21147;&#24182;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#20010;&#39046;&#22495;&#65292;&#21253;&#25324;&#25512;&#33616;&#31995;&#32479;&#12290;&#35768;&#22810;&#30740;&#31350;&#37319;&#29992;&#19987;&#38376;&#30340;&#8220;&#25552;&#31034;&#8221;&#26469;&#21033;&#29992;LLMs&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#12290;&#20363;&#22914;&#65292;LLMs&#34987;&#25552;&#31034;&#20026;&#38646;-shot&#25490;&#21517;&#22120;&#65292;&#29992;&#20110;&#23545;&#30001;&#26816;&#32034;&#27169;&#22411;&#29983;&#25104;&#30340;&#20505;&#36873;&#39033;&#36827;&#34892;&#21015;&#34920;&#25490;&#21517;&#65292;&#20197;&#29992;&#20110;&#25512;&#33616;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#36824;&#20351;&#29992;&#25351;&#20196;&#35843;&#20248;&#25216;&#26415;&#65292;&#36890;&#36807;&#19982;&#20154;&#31867;&#20559;&#22909;&#30340;&#23545;&#40784;&#26469;&#25552;&#20379;&#26356;&#26377;&#21069;&#26223;&#30340;&#25512;&#33616;&#12290;&#23613;&#31649;&#20855;&#26377;&#28508;&#21147;&#65292;&#20294;&#30446;&#21069;&#30340;&#30740;&#31350;&#24573;&#35270;&#20102;&#25972;&#21512;&#22810;&#20010;&#25490;&#21517;&#20219;&#21153;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#20256;&#32479;&#25512;&#33616;&#27169;&#22411;&#30340;&#20449;&#21495;&#26410;&#19982;LLM&#25972;&#21512;&#65292;&#38480;&#21046;&#20102;&#24403;&#21069;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated remarkable capabilities and have been extensively deployed across various domains, including recommender systems. Numerous studies have employed specialized \textit{prompts} to harness the in-context learning capabilities intrinsic to LLMs. For example, LLMs are prompted to act as zero-shot rankers for listwise ranking, evaluating candidate items generated by a retrieval model for recommendation. Recent research further uses instruction tuning techniques to align LLM with human preference for more promising recommendations. Despite its potential, current research overlooks the integration of multiple ranking tasks to enhance model performance. Moreover, the signal from the conventional recommendation model is not integrated into the LLM, limiting the current system performance.  In this paper, we introduce RecRanker, tailored for instruction tuning LLM to serve as the \textbf{Ranker} for top-\textit{k} \textbf{Rec}ommendations. Specificall
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20250;&#35805;&#30340;&#25512;&#33616;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#22810;&#20010;&#34892;&#20026;&#31867;&#22411;&#20197;&#21450;&#20135;&#21697;&#20043;&#38388;&#30340;&#21487;&#26367;&#20195;&#21644;&#20114;&#34917;&#20851;&#31995;&#65292;&#20197;&#22686;&#24378;&#25512;&#33616;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2312.14957</link><description>&lt;p&gt;
&#21033;&#29992;&#22810;&#34892;&#20026;&#25968;&#25454;&#20013;&#30340;&#21487;&#26367;&#20195;&#21644;&#20114;&#34917;&#20851;&#31995;&#36827;&#34892;&#22522;&#20110;&#20250;&#35805;&#30340;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Session-Based Recommendation by Exploiting Substitutable and Complementary Relationships from Multi-behavior Data. (arXiv:2312.14957v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.14957
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20250;&#35805;&#30340;&#25512;&#33616;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#22810;&#20010;&#34892;&#20026;&#31867;&#22411;&#20197;&#21450;&#20135;&#21697;&#20043;&#38388;&#30340;&#21487;&#26367;&#20195;&#21644;&#20114;&#34917;&#20851;&#31995;&#65292;&#20197;&#22686;&#24378;&#25512;&#33616;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20250;&#35805;&#30340;&#25512;&#33616;&#26088;&#22312;&#26681;&#25454;&#29992;&#25143;&#26368;&#36817;&#30340;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#24207;&#21015;&#21160;&#24577;&#22320;&#21521;&#29992;&#25143;&#25512;&#33616;&#29289;&#21697;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#22522;&#20110;&#20250;&#35805;&#30340;&#25512;&#33616;&#30740;&#31350;&#37319;&#29992;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#21482;&#32771;&#34385;&#20102;&#19968;&#31181;&#29305;&#27530;&#30340;&#34892;&#20026;&#31867;&#22411;&#65288;&#22914;&#28857;&#20987;&#65289;&#65292;&#32780;&#37027;&#20123;&#32771;&#34385;&#22810;&#31181;&#31867;&#22411;&#34892;&#20026;&#30340;&#30740;&#31350;&#21017;&#24573;&#35270;&#20102;&#20805;&#20998;&#21033;&#29992;&#20135;&#21697;&#20043;&#38388;&#20851;&#31995;&#30340;&#26426;&#20250;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#22522;&#20110;&#22810;&#34892;&#20026;&#25968;&#25454;&#30340;&#21487;&#26367;&#20195;&#21644;&#20114;&#34917;&#20851;&#31995;&#65288;&#31616;&#31216;SCRM&#65289;&#65292;&#20197;&#26356;&#22909;&#22320;&#25506;&#32034;&#20135;&#21697;&#20043;&#38388;&#30340;&#20851;&#31995;&#36827;&#34892;&#26377;&#25928;&#30340;&#25512;&#33616;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#26681;&#25454;&#29992;&#25143;&#22312;&#27599;&#20010;&#20250;&#35805;&#20013;&#30340;&#39034;&#24207;&#34892;&#20026;&#20849;&#21516;&#32771;&#34385;&#8220;&#28857;&#20987;&#8221;&#21644;&#8220;&#36141;&#20080;&#8221;&#34892;&#20026;&#65292;&#26500;&#24314;&#21487;&#26367;&#20195;&#21644;&#20114;&#34917;&#30340;&#22270;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21435;&#22122;&#32593;&#32476;&#26469;&#28040;&#38500;&#38169;&#35823;&#20851;&#31995;&#65292;&#24182;&#36890;&#36807;&#19968;&#20010;&#29305;&#21035;&#35774;&#35745;&#30340;&#25439;&#22833;&#20989;&#25968;&#36827;&#19968;&#27493;&#32771;&#34385;&#36825;&#20004;&#20010;&#20851;&#31995;&#30340;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
Session-based recommendation (SR) aims to dynamically recommend items to a user based on a sequence of the most recent user-item interactions. Most existing studies on SR adopt advanced deep learning methods. However, the majority only consider a special behavior type (e.g., click), while those few considering multi-typed behaviors ignore to take full advantage of the relationships between products (items). In this case, the paper proposes a novel approach, called Substitutable and Complementary Relationships from Multi-behavior Data (denoted as SCRM) to better explore the relationships between products for effective recommendation. Specifically, we firstly construct substitutable and complementary graphs based on a user's sequential behaviors in every session by jointly considering `click' and `purchase' behaviors. We then design a denoising network to remove false relationships, and further consider constraints on the two relationships via a particularly designed loss function. Exten
&lt;/p&gt;</description></item><item><title>WellFactor&#26159;&#19968;&#31181;&#20351;&#29992;&#32508;&#21512;&#23884;&#20837;&#21307;&#30103;&#25968;&#25454;&#30340;&#24739;&#32773;&#20998;&#31867;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#21463;&#32422;&#26463;&#30340;&#20302;&#31209;&#36924;&#36817;&#12289;&#32467;&#21512;&#26631;&#31614;&#20449;&#24687;&#26469;&#20248;&#21270;&#23884;&#20837;&#32467;&#26524;&#65292;&#21516;&#26102;&#20855;&#26377;&#21363;&#26102;&#35745;&#31639;&#26032;&#25968;&#25454;&#23884;&#20837;&#30340;&#29305;&#28857;&#12290;&#22312;&#23454;&#38469;&#21307;&#30103;&#25968;&#25454;&#19978;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2312.14129</link><description>&lt;p&gt;
WellFactor:&#20351;&#29992;&#32508;&#21512;&#23884;&#20837;&#21307;&#30103;&#25968;&#25454;&#30340;&#24739;&#32773;&#20998;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
WellFactor: Patient Profiling using Integrative Embedding of Healthcare Data. (arXiv:2312.14129v1 [cs.LG] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.14129
&lt;/p&gt;
&lt;p&gt;
WellFactor&#26159;&#19968;&#31181;&#20351;&#29992;&#32508;&#21512;&#23884;&#20837;&#21307;&#30103;&#25968;&#25454;&#30340;&#24739;&#32773;&#20998;&#31867;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#21463;&#32422;&#26463;&#30340;&#20302;&#31209;&#36924;&#36817;&#12289;&#32467;&#21512;&#26631;&#31614;&#20449;&#24687;&#26469;&#20248;&#21270;&#23884;&#20837;&#32467;&#26524;&#65292;&#21516;&#26102;&#20855;&#26377;&#21363;&#26102;&#35745;&#31639;&#26032;&#25968;&#25454;&#23884;&#20837;&#30340;&#29305;&#28857;&#12290;&#22312;&#23454;&#38469;&#21307;&#30103;&#25968;&#25454;&#19978;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24555;&#36895;&#21457;&#23637;&#30340;&#21307;&#30103;&#34892;&#19994;&#20013;&#65292;&#24179;&#21488;&#29616;&#22312;&#19981;&#20165;&#21487;&#20197;&#35775;&#38382;&#20256;&#32479;&#30340;&#21307;&#30103;&#35760;&#24405;&#65292;&#36824;&#21487;&#20197;&#33719;&#21462;&#28085;&#30422;&#21508;&#31181;&#24739;&#32773;&#20114;&#21160;&#30340;&#21508;&#31181;&#25968;&#25454;&#38598;&#65292;&#20363;&#22914;&#26469;&#33258;&#21307;&#30103;&#32593;&#31449;&#30340;&#25968;&#25454;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#31181;&#20016;&#23500;&#30340;&#25968;&#25454;&#22810;&#26679;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;WellFactor&#65306;&#19968;&#31181;&#36890;&#36807;&#25972;&#21512;&#36825;&#20123;&#26469;&#28304;&#20449;&#24687;&#26469;&#24471;&#20986;&#24739;&#32773;&#20998;&#31867;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#21033;&#29992;&#21463;&#32422;&#26463;&#30340;&#20302;&#31209;&#36924;&#36817;&#12290;WellFactor&#34987;&#20248;&#21270;&#20026;&#22788;&#29702;&#21307;&#30103;&#25968;&#25454;&#20013;&#32463;&#24120;&#23384;&#22312;&#30340;&#31232;&#30095;&#24615;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#32467;&#21512;&#29305;&#23450;&#20219;&#21153;&#30340;&#26631;&#31614;&#20449;&#24687;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25913;&#36827;&#20102;&#23884;&#20837;&#32467;&#26524;&#65292;&#25552;&#20379;&#20102;&#26356;&#21152;&#26126;&#26234;&#30340;&#24739;&#32773;&#35270;&#35282;&#12290;WellFactor&#30340;&#19968;&#20010;&#37325;&#35201;&#29305;&#28857;&#26159;&#33021;&#22815;&#21363;&#26102;&#35745;&#31639;&#26032;&#30340;&#12289;&#20197;&#21069;&#26410;&#35266;&#23519;&#21040;&#30340;&#24739;&#32773;&#25968;&#25454;&#30340;&#23884;&#20837;&#65292;&#28040;&#38500;&#20102;&#37325;&#26032;&#35775;&#38382;&#25972;&#20010;&#25968;&#25454;&#38598;&#25110;&#37325;&#26032;&#35745;&#31639;&#23884;&#20837;&#30340;&#38656;&#35201;&#12290;&#23545;&#23454;&#38469;&#21307;&#30103;&#25968;&#25454;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#35777;&#26126;&#20102;WellFactor&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the rapidly evolving healthcare industry, platforms now have access to not only traditional medical records, but also diverse data sets encompassing various patient interactions, such as those from healthcare web portals. To address this rich diversity of data, we introduce WellFactor: a method that derives patient profiles by integrating information from these sources. Central to our approach is the utilization of constrained low-rank approximation. WellFactor is optimized to handle the sparsity that is often inherent in healthcare data. Moreover, by incorporating task-specific label information, our method refines the embedding results, offering a more informed perspective on patients. One important feature of WellFactor is its ability to compute embeddings for new, previously unobserved patient data instantaneously, eliminating the need to revisit the entire data set or recomputing the embedding. Comprehensive evaluations on real-world healthcare data demonstrate WellFactor's eff
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;33881&#20221;&#19987;&#21033;&#25991;&#20214;&#30340;&#26679;&#26412;&#65292;&#23558;&#24037;&#31243;&#35774;&#35745;&#30693;&#35782;&#38416;&#37322;&#20026;&#30693;&#35782;&#22270;&#35889;&#65292;&#20174;&#32780;&#25581;&#31034;&#24037;&#31243;&#35774;&#35745;&#30693;&#35782;&#30340;&#35821;&#35328;&#21644;&#32467;&#26500;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2312.06355</link><description>&lt;p&gt;
&#24037;&#31243;&#35774;&#35745;&#30693;&#35782;&#30340;&#35821;&#35328;&#21644;&#32467;&#26500;&#22522;&#30784;
&lt;/p&gt;
&lt;p&gt;
Linguistic and Structural Basis of Engineering Design Knowledge. (arXiv:2312.06355v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.06355
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;33881&#20221;&#19987;&#21033;&#25991;&#20214;&#30340;&#26679;&#26412;&#65292;&#23558;&#24037;&#31243;&#35774;&#35745;&#30693;&#35782;&#38416;&#37322;&#20026;&#30693;&#35782;&#22270;&#35889;&#65292;&#20174;&#32780;&#25581;&#31034;&#24037;&#31243;&#35774;&#35745;&#30693;&#35782;&#30340;&#35821;&#35328;&#21644;&#32467;&#26500;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#21697;&#25551;&#36848;&#26159;&#24037;&#31243;&#35774;&#35745;&#30693;&#35782;&#30340;&#20027;&#35201;&#36733;&#20307;&#65292;&#26082;&#26159;&#35774;&#35745;&#36807;&#31243;&#30340;&#20135;&#29289;&#65292;&#20063;&#26159;&#39537;&#21160;&#35774;&#35745;&#36807;&#31243;&#30340;&#22240;&#32032;&#12290;&#23613;&#31649;&#29289;&#21697;&#21487;&#20197;&#20197;&#19981;&#21516;&#30340;&#20869;&#28085;&#36827;&#34892;&#25551;&#36848;&#65292;&#20294;&#35774;&#35745;&#36807;&#31243;&#38656;&#35201;&#19968;&#31181;&#25551;&#36848;&#26469;&#20307;&#29616;&#24037;&#31243;&#35774;&#35745;&#30693;&#35782;&#65292;&#36825;&#36890;&#36807;&#23454;&#20307;&#21644;&#20851;&#31995;&#30340;&#22797;&#26434;&#23433;&#25490;&#22312;&#25991;&#26412;&#20013;&#34920;&#29616;&#20986;&#26469;&#12290;&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#20174;&#21508;&#31181;&#25991;&#26412;&#20013;&#23398;&#20064;&#65292;&#20294;&#23427;&#20204;&#23578;&#26410;&#29983;&#25104;&#20307;&#29616;&#26126;&#30830;&#30340;&#24037;&#31243;&#35774;&#35745;&#20107;&#23454;&#30340;&#25991;&#26412;&#12290;&#29616;&#26377;&#30340;&#26412;&#20307;&#35770;&#35774;&#35745;&#29702;&#35770;&#24456;&#23569;&#33021;&#25351;&#23548;&#30446;&#21069;&#20165;&#38480;&#20110;&#26500;&#24605;&#21644;&#23398;&#20064;&#30446;&#30340;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24212;&#29992;&#12290;&#26412;&#25991;&#20174;33881&#20221;&#19987;&#21033;&#25991;&#20214;&#30340;&#22823;&#26679;&#26412;&#20013;&#23558;&#24037;&#31243;&#35774;&#35745;&#30693;&#35782;&#38416;&#37322;&#20026;&#30693;&#35782;&#22270;&#35889;&#12290;&#25105;&#20204;&#30740;&#31350;&#36825;&#20123;&#30693;&#35782;&#22270;&#35889;&#30340;&#32452;&#25104;&#37096;&#20998;&#65292;&#20197;&#29702;&#35299;&#24037;&#31243;&#35774;&#35745;&#30693;&#35782;&#30340;&#35821;&#35328;&#21644;&#32467;&#26500;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artefact descriptions are the primary carriers of engineering design knowledge that is both an outcome and a driver of the design process. While an artefact could be described in different connotations, the design process requires a description to embody engineering design knowledge, which is expressed in the text through intricate placement of entities and relationships. As large-language models learn from all kinds of text merely as a sequence of characters/tokens, these are yet to generate text that embodies explicit engineering design facts. Existing ontological design theories are less likely to guide the large-language models whose applications are currently limited to ideation and learning purposes. In this article, we explicate engineering design knowledge as knowledge graphs from a large sample of 33,881 patent documents. We examine the constituents of these knowledge graphs to understand the linguistic and structural basis of engineering design knowledge. In terms of linguist
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#39063;&#31890;&#24230;&#24863;&#30693;&#30340;&#22810;&#26041;&#38754;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#31264;&#23494;&#26816;&#32034;&#38382;&#39064;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#35813;&#27169;&#22411;&#19981;&#20165;&#32771;&#34385;&#20102;&#39033;&#30446;&#26041;&#38754;&#20540;&#20043;&#38388;&#30340;&#32454;&#31890;&#24230;&#35821;&#20041;&#20851;&#31995;&#65292;&#36824;&#36991;&#20813;&#20102;&#23558;&#26041;&#38754;&#23398;&#20064;&#38598;&#20013;&#21040;CLS&#20196;&#29260;&#25110;&#20165;&#36890;&#36807;&#20540;&#39044;&#27979;&#30446;&#26631;&#23398;&#20064;&#26041;&#38754;&#23884;&#20837;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2312.02538</link><description>&lt;p&gt;
&#22810;&#39063;&#31890;&#24230;&#24863;&#30693;&#30340;&#22810;&#26041;&#38754;&#23398;&#20064;&#27169;&#22411;&#29992;&#20110;&#22810;&#26041;&#38754;&#31264;&#23494;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
A Multi-Granularity-Aware Aspect Learning Model for Multi-Aspect Dense Retrieval. (arXiv:2312.02538v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.02538
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#39063;&#31890;&#24230;&#24863;&#30693;&#30340;&#22810;&#26041;&#38754;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#31264;&#23494;&#26816;&#32034;&#38382;&#39064;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#35813;&#27169;&#22411;&#19981;&#20165;&#32771;&#34385;&#20102;&#39033;&#30446;&#26041;&#38754;&#20540;&#20043;&#38388;&#30340;&#32454;&#31890;&#24230;&#35821;&#20041;&#20851;&#31995;&#65292;&#36824;&#36991;&#20813;&#20102;&#23558;&#26041;&#38754;&#23398;&#20064;&#38598;&#20013;&#21040;CLS&#20196;&#29260;&#25110;&#20165;&#36890;&#36807;&#20540;&#39044;&#27979;&#30446;&#26631;&#23398;&#20064;&#26041;&#38754;&#23884;&#20837;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31264;&#23494;&#26816;&#32034;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#65292;&#23545;&#20110;&#20855;&#26377;&#19981;&#21516;&#26041;&#38754;&#30340;&#32467;&#26500;&#21270;&#25968;&#25454;(&#20363;&#22914;&#65292;&#20855;&#26377;&#31867;&#21035;&#21644;&#21697;&#29260;&#31561;&#26041;&#38754;&#30340;&#20135;&#21697;)&#30340;&#20851;&#27880;&#36739;&#23569;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#23558;&#26041;&#38754;&#20449;&#24687;&#21512;&#24182;&#21040;&#39033;&#30446;&#34920;&#31034;&#20013;&#65292;&#20197;&#36890;&#36807;&#39044;&#27979;&#19982;&#39033;&#30446;&#26041;&#38754;&#30456;&#20851;&#32852;&#30340;&#20540;&#23454;&#29616;&#26377;&#25928;&#26816;&#32034;&#12290;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#23427;&#20204;&#23558;&#20540;&#35270;&#20026;&#23396;&#31435;&#30340;&#31867;&#21035;(&#20363;&#22914;&#65292;&#8220;&#26234;&#33021;&#23478;&#23621;&#8221;&#12289;&#8220;&#23478;&#23621;&#12289;&#33457;&#22253;&#21644;&#24037;&#20855;&#8221;&#21644;&#8220;&#32654;&#23481;&#21644;&#20581;&#24247;&#8221;)&#65292;&#24573;&#30053;&#20102;&#23427;&#20204;&#20043;&#38388;&#30340;&#32454;&#31890;&#24230;&#35821;&#20041;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#35201;&#20040;&#24378;&#36843;&#23558;&#26041;&#38754;&#30340;&#23398;&#20064;&#38598;&#20013;&#21040;CLS&#20196;&#29260;&#20013;&#65292;&#36825;&#21487;&#33021;&#20250;&#28151;&#28102;&#20854;&#20316;&#20026;&#34920;&#36798;&#25972;&#20010;&#20869;&#23481;&#35821;&#20041;&#30340;&#25351;&#23450;&#29992;&#36884;&#65292;&#35201;&#20040;&#20165;&#36890;&#36807;&#20540;&#39044;&#27979;&#30446;&#26631;&#23398;&#20064;&#39069;&#22806;&#30340;&#26041;&#38754;&#23884;&#20837;&#65292;&#36825;&#22312;&#27809;&#26377;&#20026;&#39033;&#30446;&#26041;&#38754;&#27880;&#37322;&#30340;&#20540;&#26102;&#21487;&#33021;&#19981;&#36275;&#22815;&#12290;&#37492;&#20110;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#39063;&#31890;&#24230;&#24863;&#30693;&#30340;&#22810;&#26041;&#38754;&#23398;&#20064;&#27169;&#22411;&#65288;MUlti-granulaRity-aware Aspect Learning model&#65289;
&lt;/p&gt;
&lt;p&gt;
Dense retrieval methods have been mostly focused on unstructured text and less attention has been drawn to structured data with various aspects, e.g., products with aspects such as category and brand. Recent work has proposed two approaches to incorporate the aspect information into item representations for effective retrieval by predicting the values associated with the item aspects. Despite their efficacy, they treat the values as isolated classes (e.g., "Smart Homes", "Home, Garden &amp; Tools", and "Beauty &amp; Health") and ignore their fine-grained semantic relation. Furthermore, they either enforce the learning of aspects into the CLS token, which could confuse it from its designated use for representing the entire content semantics, or learn extra aspect embeddings only with the value prediction objective, which could be insufficient especially when there are no annotated values for an item aspect. Aware of these limitations, we propose a MUlti-granulaRity-aware Aspect Learning model (
&lt;/p&gt;</description></item><item><title>TSRankLLM&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#38454;&#27573;&#36866;&#24212;&#26041;&#27861;&#29992;&#20110;&#25991;&#26412;&#25490;&#24207;&#65292;&#36890;&#36807;&#36830;&#32493;&#39044;&#35757;&#32451;&#21644;&#25913;&#36827;&#30340;&#20248;&#21270;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.16720</link><description>&lt;p&gt;
TSRankLLM: &#19968;&#31181;&#29992;&#20110;&#25991;&#26412;&#25490;&#24207;&#30340;&#20004;&#38454;&#27573;LLM&#36866;&#24212;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
TSRankLLM: A Two-Stage Adaptation of LLMs for Text Ranking. (arXiv:2311.16720v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.16720
&lt;/p&gt;
&lt;p&gt;
TSRankLLM&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#38454;&#27573;&#36866;&#24212;&#26041;&#27861;&#29992;&#20110;&#25991;&#26412;&#25490;&#24207;&#65292;&#36890;&#36807;&#36830;&#32493;&#39044;&#35757;&#32451;&#21644;&#25913;&#36827;&#30340;&#20248;&#21270;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#25490;&#24207;&#26159;&#21508;&#31181;&#20449;&#24687;&#26816;&#32034;&#24212;&#29992;&#20013;&#30340;&#20851;&#38190;&#20219;&#21153;&#65292;&#26368;&#36817;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#65292;&#29305;&#21035;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25104;&#21151;&#24341;&#36215;&#20102;&#20154;&#20204;&#23545;&#20854;&#22312;&#25991;&#26412;&#25490;&#24207;&#20013;&#30340;&#24212;&#29992;&#30340;&#20852;&#36259;&#12290;&#20026;&#20102;&#28040;&#38500;PLMs&#21644;&#25991;&#26412;&#25490;&#24207;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#35768;&#22810;&#23398;&#32773;&#24050;&#32463;&#24191;&#27867;&#25506;&#32034;&#20102;&#20351;&#29992;&#26377;&#30417;&#30563;&#25490;&#24207;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20197;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#20165;&#32534;&#30721;&#22120;&#21644;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;PLMs&#19978;&#65292;&#32570;&#20047;&#23545;&#20165;&#35299;&#30721;&#22120;LLM&#30340;&#30740;&#31350;&#12290;&#19968;&#20010;&#20363;&#22806;&#26159;RankLLaMA&#65292;&#23427;&#24314;&#35758;&#30452;&#25509;&#20351;&#29992;&#26377;&#30417;&#30563;&#30340;&#24494;&#35843;&#65288;SFT&#65289;&#26469;&#20840;&#38754;&#25506;&#32034;LLaMA&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#37319;&#29992;&#20004;&#38454;&#27573;&#28176;&#36827;&#33539;&#24335;&#20250;&#26356;&#26377;&#30410;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#22823;&#35268;&#27169;&#24369;&#30417;&#30563;&#35821;&#26009;&#24211;&#23545;LLMs&#36827;&#34892;&#36830;&#32493;&#39044;&#35757;&#32451;&#65288;CPT&#65289;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25191;&#34892;&#19982;RankLLaMA&#19968;&#33268;&#30340;SFT&#65292;&#24182;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#25913;&#36827;&#30340;&#20248;&#21270;&#31574;&#30053;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#26041;&#27861;&#20855;&#26377;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text ranking is a critical task in various information retrieval applications, and the recent success of pre-trained language models (PLMs), especially large language models (LLMs), has sparked interest in their application to text ranking. To eliminate the misalignment between PLMs and text ranking, fine-tuning with supervised ranking data has been widely explored. However, previous studies focus mainly on encoder-only and encoder-decoder PLMs, and decoder-only LLM research is still lacking. An exception to this is RankLLaMA, which suggests direct supervised fine-tuning (SFT) to explore LLaMA fully. In our work, we argue that a two-stage progressive paradigm would be more beneficial. First, we suggest continual pre-training (CPT) on LLMs by using a large-scale weakly-supervised corpus. Second, we perform SFT consistent with RankLLaMA, and propose an improved optimization strategy further. Our experimental results on multiple benchmarks demonstrate the superior performance of our metho
&lt;/p&gt;</description></item><item><title>GraphPro&#26159;&#19968;&#20010;&#32467;&#21512;&#20102;&#21442;&#25968;&#39640;&#25928;&#21644;&#21160;&#24577;&#22270;&#39044;&#35757;&#32451;&#19982;&#25552;&#31034;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#26377;&#25928;&#25429;&#25417;&#38271;&#26399;&#29992;&#25143;&#20559;&#22909;&#21644;&#30701;&#26399;&#34892;&#20026;&#21160;&#24577;&#65292;&#20174;&#32780;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#25512;&#33616;&#31995;&#32479;&#20013;&#25552;&#20379;&#20934;&#30830;&#21644;&#21450;&#26102;&#30340;&#25512;&#33616;&#12290;</title><link>http://arxiv.org/abs/2311.16716</link><description>&lt;p&gt;
GraphPro: &#38754;&#21521;&#25512;&#33616;&#31995;&#32479;&#30340;&#22270;&#39044;&#35757;&#32451;&#21644;&#25552;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
GraphPro: Graph Pre-training and Prompt Learning for Recommendation. (arXiv:2311.16716v3 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.16716
&lt;/p&gt;
&lt;p&gt;
GraphPro&#26159;&#19968;&#20010;&#32467;&#21512;&#20102;&#21442;&#25968;&#39640;&#25928;&#21644;&#21160;&#24577;&#22270;&#39044;&#35757;&#32451;&#19982;&#25552;&#31034;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#26377;&#25928;&#25429;&#25417;&#38271;&#26399;&#29992;&#25143;&#20559;&#22909;&#21644;&#30701;&#26399;&#34892;&#20026;&#21160;&#24577;&#65292;&#20174;&#32780;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#25512;&#33616;&#31995;&#32479;&#20013;&#25552;&#20379;&#20934;&#30830;&#21644;&#21450;&#26102;&#30340;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;GNN&#30340;&#25512;&#33616;&#31995;&#32479;&#36890;&#36807;&#22810;&#27425;&#28040;&#24687;&#20256;&#36882;&#22312;&#24314;&#27169;&#22797;&#26434;&#30340;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#24448;&#24448;&#24573;&#35270;&#20102;&#19981;&#26029;&#21464;&#21270;&#30340;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#30340;&#21160;&#24577;&#24615;&#65292;&#36825;&#38480;&#21046;&#20102;&#20854;&#22312;&#36866;&#24212;&#29992;&#25143;&#20559;&#22909;&#21464;&#21270;&#21644;&#26032;&#21040;&#36798;&#25968;&#25454;&#20998;&#24067;&#21464;&#21270;&#26041;&#38754;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#21160;&#24577;&#29615;&#22659;&#20013;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#24615;&#33021;&#21463;&#21040;&#20102;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GraphPro&#65292;&#36825;&#26159;&#19968;&#20010;&#23558;&#21442;&#25968;&#39640;&#25928;&#21644;&#21160;&#24577;&#22270;&#39044;&#35757;&#32451;&#19982;&#25552;&#31034;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#26694;&#26550;&#12290;&#36825;&#31181;&#26032;&#39062;&#30340;&#32452;&#21512;&#33021;&#22815;&#26377;&#25928;&#25429;&#25417;&#38271;&#26399;&#29992;&#25143;&#20559;&#22909;&#21644;&#30701;&#26399;&#34892;&#20026;&#21160;&#24577;&#65292;&#20174;&#32780;&#23454;&#29616;&#20934;&#30830;&#21644;&#21450;&#26102;&#30340;&#25512;&#33616;&#12290;&#25105;&#20204;&#30340;GraphPro&#26694;&#26550;&#36890;&#36807;&#26080;&#32541;&#38598;&#25104;&#20020;&#26102;&#25552;&#31034;&#26426;&#21046;&#21644;&#22270;&#32467;&#26500;&#25552;&#31034;&#23398;&#20064;&#26426;&#21046;&#21040;&#39044;&#35757;&#32451;&#30340;GNN&#27169;&#22411;&#20013;&#26469;&#35299;&#20915;&#29992;&#25143;&#20559;&#22909;&#19981;&#26029;&#21464;&#21270;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
GNN-based recommenders have excelled in modeling intricate user-item interactions through multi-hop message passing. However, existing methods often overlook the dynamic nature of evolving user-item interactions, which impedes the adaption to changing user preferences and distribution shifts in newly arriving data. Thus, their scalability and performances in real-world dynamic environments are limited. In this study, we propose GraphPro, a framework that incorporates parameter-efficient and dynamic graph pre-training with prompt learning. This novel combination empowers GNNs to effectively capture both long-term user preferences and short-term behavior dynamics, enabling the delivery of accurate and timely recommendations. Our GraphPro framework addresses the challenge of evolving user preferences by seamlessly integrating a temporal prompt mechanism and a graph-structural prompt learning mechanism into the pre-trained GNN model. The temporal prompt mechanism encodes time information o
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#26500;&#24314;&#21512;&#36866;&#30340;&#22522;&#20934;&#21644;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#21457;&#29616;&#65292;&#30001;AI&#29983;&#25104;&#30340;&#22270;&#20687;&#23545;&#20110;&#25991;&#26412;-&#22270;&#20687;&#26816;&#32034;&#27169;&#22411;&#24341;&#20837;&#20102;&#19968;&#20010;&#19981;&#21487;&#35265;&#30340;&#30456;&#20851;&#20559;&#24046;&#65292;&#21363;&#20351;&#36825;&#20123;&#29983;&#25104;&#30340;&#22270;&#20687;&#22312;&#35270;&#35273;&#19978;&#19982;&#26597;&#35810;&#30340;&#30456;&#20851;&#29305;&#24449;&#30456;&#27604;&#24182;&#27809;&#26377;&#26356;&#22810;&#12290;&#36825;&#31181;&#19981;&#21487;&#35265;&#30340;&#30456;&#20851;&#20559;&#24046;&#26222;&#36941;&#23384;&#22312;&#20110;&#19981;&#21516;&#35757;&#32451;&#25968;&#25454;&#21644;&#26550;&#26500;&#30340;&#26816;&#32034;&#27169;&#22411;&#20013;&#12290;</title><link>http://arxiv.org/abs/2311.14084</link><description>&lt;p&gt;
&#30001;AI&#29983;&#25104;&#30340;&#22270;&#20687;&#23545;&#25991;&#26412;-&#22270;&#20687;&#26816;&#32034;&#24341;&#20837;&#20102;&#19981;&#21487;&#35265;&#30340;&#30456;&#20851;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
AI-Generated Images Introduce Invisible Relevance Bias to Text-Image Retrieval. (arXiv:2311.14084v3 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.14084
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#26500;&#24314;&#21512;&#36866;&#30340;&#22522;&#20934;&#21644;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#21457;&#29616;&#65292;&#30001;AI&#29983;&#25104;&#30340;&#22270;&#20687;&#23545;&#20110;&#25991;&#26412;-&#22270;&#20687;&#26816;&#32034;&#27169;&#22411;&#24341;&#20837;&#20102;&#19968;&#20010;&#19981;&#21487;&#35265;&#30340;&#30456;&#20851;&#20559;&#24046;&#65292;&#21363;&#20351;&#36825;&#20123;&#29983;&#25104;&#30340;&#22270;&#20687;&#22312;&#35270;&#35273;&#19978;&#19982;&#26597;&#35810;&#30340;&#30456;&#20851;&#29305;&#24449;&#30456;&#27604;&#24182;&#27809;&#26377;&#26356;&#22810;&#12290;&#36825;&#31181;&#19981;&#21487;&#35265;&#30340;&#30456;&#20851;&#20559;&#24046;&#26222;&#36941;&#23384;&#22312;&#20110;&#19981;&#21516;&#35757;&#32451;&#25968;&#25454;&#21644;&#26550;&#26500;&#30340;&#26816;&#32034;&#27169;&#22411;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29983;&#25104;&#27169;&#22411;&#30340;&#36827;&#27493;&#65292;&#30001;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#30340;&#20869;&#23481;&#65288;AIGC&#65289;&#21464;&#24471;&#26356;&#21152;&#36924;&#30495;&#65292;&#28044;&#20837;&#20114;&#32852;&#32593;&#12290;&#26368;&#36817;&#30340;&#19968;&#39033;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#31181;&#29616;&#35937;&#23548;&#33268;&#20102;&#32593;&#32476;&#25628;&#32034;&#20013;&#30340;&#28304;&#20559;&#24046;&#12290;&#29305;&#21035;&#26159;&#65292;&#31070;&#32463;&#26816;&#32034;&#27169;&#22411;&#24448;&#24448;&#23558;&#29983;&#25104;&#30340;&#25991;&#26412;&#25490;&#21517;&#39640;&#20110;&#20154;&#24037;&#32534;&#20889;&#30340;&#25991;&#26412;&#12290;&#26412;&#25991;&#23558;&#36825;&#31181;&#20559;&#24046;&#30340;&#30740;&#31350;&#25193;&#23637;&#21040;&#36328;&#27169;&#24577;&#26816;&#32034;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25104;&#21151;&#26500;&#24314;&#20102;&#19968;&#20010;&#36866;&#21512;&#25506;&#32034;&#20559;&#24046;&#23384;&#22312;&#30340;&#22522;&#20934;&#12290;&#38543;&#21518;&#65292;&#22312;&#36825;&#20010;&#22522;&#20934;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#21457;&#29616;AI&#29983;&#25104;&#30340;&#22270;&#20687;&#23545;&#20110;&#25991;&#26412;-&#22270;&#20687;&#26816;&#32034;&#27169;&#22411;&#24341;&#20837;&#20102;&#19968;&#20010;&#19981;&#21487;&#35265;&#30340;&#30456;&#20851;&#20559;&#24046;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#23613;&#31649;AI&#29983;&#25104;&#30340;&#22270;&#20687;&#19982;&#26597;&#35810;&#30456;&#27604;&#27809;&#26377;&#26356;&#22810;&#30340;&#35270;&#35273;&#30456;&#20851;&#29305;&#24449;&#65292;&#20294;&#25991;&#26412;-&#22270;&#20687;&#26816;&#32034;&#27169;&#22411;&#24448;&#24448;&#23558;AI&#29983;&#25104;&#30340;&#22270;&#20687;&#25490;&#21517;&#39640;&#20110;&#30495;&#23454;&#22270;&#20687;&#12290;&#36825;&#31181;&#19981;&#21487;&#35265;&#30340;&#30456;&#20851;&#20559;&#24046;&#22312;&#19981;&#21516;&#35757;&#32451;&#25968;&#25454;&#21644;&#26550;&#26500;&#30340;&#26816;&#32034;&#27169;&#22411;&#20013;&#26222;&#36941;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the advancement of generation models, AI-generated content (AIGC) is becoming more realistic, flooding the Internet. A recent study suggests that this phenomenon causes source bias in text retrieval for web search. Specifically, neural retrieval models tend to rank generated texts higher than human-written texts. In this paper, we extend the study of this bias to cross-modal retrieval. Firstly, we successfully construct a suitable benchmark to explore the existence of the bias. Subsequent extensive experiments on this benchmark reveal that AI-generated images introduce an invisible relevance bias to text-image retrieval models. Specifically, our experiments show that text-image retrieval models tend to rank the AI-generated images higher than the real images, even though the AI-generated images do not exhibit more visually relevant features to the query than real images. This invisible relevance bias is prevalent across retrieval models with varying training data and architectures
&lt;/p&gt;</description></item><item><title>&#22522;&#20110; Householder &#37327;&#21270;&#30340;&#28145;&#24230;&#21704;&#24076;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#30340;&#37327;&#21270;&#31574;&#30053;&#65292;&#36890;&#36807;&#23558;&#23398;&#20064;&#38382;&#39064;&#20998;&#35299;&#20026;&#20004;&#20010;&#38454;&#27573;&#26469;&#25913;&#21892;&#21704;&#24076;&#26041;&#27861;&#20013;&#30456;&#20284;&#24230;&#23398;&#20064;&#21644;&#37327;&#21270;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#20174;&#32780;&#25552;&#39640;&#23884;&#20837;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2311.04207</link><description>&lt;p&gt;
&#22522;&#20110; Householder &#37327;&#21270;&#30340;&#28145;&#24230;&#21704;&#24076;
&lt;/p&gt;
&lt;p&gt;
Deep Hashing via Householder Quantization. (arXiv:2311.04207v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.04207
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110; Householder &#37327;&#21270;&#30340;&#28145;&#24230;&#21704;&#24076;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#30340;&#37327;&#21270;&#31574;&#30053;&#65292;&#36890;&#36807;&#23558;&#23398;&#20064;&#38382;&#39064;&#20998;&#35299;&#20026;&#20004;&#20010;&#38454;&#27573;&#26469;&#25913;&#21892;&#21704;&#24076;&#26041;&#27861;&#20013;&#30456;&#20284;&#24230;&#23398;&#20064;&#21644;&#37327;&#21270;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#20174;&#32780;&#25552;&#39640;&#23884;&#20837;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21704;&#24076;&#26159;&#22823;&#35268;&#27169;&#22270;&#20687;&#30456;&#20284;&#24615;&#25628;&#32034;&#30340;&#26680;&#24515;&#65292;&#26368;&#36817;&#30340;&#26041;&#27861;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#36825;&#20123;&#31639;&#27861;&#36890;&#24120;&#23398;&#20064;&#25968;&#25454;&#30340;&#36830;&#32493;&#23884;&#20837;&#12290;&#20026;&#20102;&#36991;&#20813;&#21518;&#32493;&#26114;&#36149;&#30340;&#20108;&#20540;&#21270;&#27493;&#39588;&#65292;&#24120;&#35265;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#37319;&#29992;&#23558;&#30456;&#20284;&#24230;&#23398;&#20064;&#39033;&#65288;&#30830;&#20445;&#30456;&#20284;&#30340;&#22270;&#20687;&#34987;&#20998;&#32452;&#21040;&#38468;&#36817;&#30340;&#23884;&#20837;&#20013;&#65289;&#21644;&#37327;&#21270;&#24809;&#32602;&#39033;&#65288;&#30830;&#20445;&#23884;&#20837;&#39033;&#25509;&#36817;&#20108;&#20540;&#21270;&#39033;&#65292;&#20363;&#22914;-1&#25110;1&#65289;&#32467;&#21512;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#28982;&#32780;&#65292;&#36825;&#20004;&#20010;&#39033;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#21487;&#33021;&#20351;&#23398;&#20064;&#21464;&#24471;&#26356;&#22256;&#38590;&#65292;&#24182;&#19988;&#23884;&#20837;&#32467;&#26524;&#26356;&#24046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#30340;&#37327;&#21270;&#31574;&#30053;&#65292;&#23558;&#23398;&#20064;&#38382;&#39064;&#20998;&#35299;&#20026;&#20004;&#20010;&#38454;&#27573;&#65306;&#39318;&#20808;&#65292;&#19981;&#36827;&#34892;&#37327;&#21270;&#65292;&#22312;&#23884;&#20837;&#31354;&#38388;&#19978;&#36827;&#34892;&#30456;&#20284;&#24230;&#23398;&#20064;&#65307;&#20854;&#27425;&#65292;&#25214;&#21040;&#23884;&#20837;&#30340;&#26368;&#20339;&#27491;&#20132;&#21464;&#25442;&#65292;&#20351;&#24471;&#23884;&#20837;&#30340;&#27599;&#20010;&#22352;&#26631;&#37117;&#25509;&#36817;&#20854;&#31526;&#21495;&#65292;&#28982;&#21518;&#36890;&#36807;&#31526;&#21495;&#20989;&#25968;&#23545;&#21464;&#25442;&#21518;&#30340;&#23884;&#20837;&#36827;&#34892;&#37327;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hashing is at the heart of large-scale image similarity search, and recent methods have been substantially improved through deep learning techniques. Such algorithms typically learn continuous embeddings of the data. To avoid a subsequent costly binarization step, a common solution is to employ loss functions that combine a similarity learning term (to ensure similar images are grouped to nearby embeddings) and a quantization penalty term (to ensure that the embedding entries are close to binarized entries, e.g., -1 or 1). Still, the interaction between these two terms can make learning harder and the embeddings worse. We propose an alternative quantization strategy that decomposes the learning problem in two stages: first, perform similarity learning over the embedding space with no quantization; second, find an optimal orthogonal transformation of the embeddings so each coordinate of the embedding is close to its sign, and then quantize the transformed embedding through the sign func
&lt;/p&gt;</description></item><item><title>&#36817;&#26399;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#20135;&#29983;&#20102;&#19968;&#31181;&#20559;&#35265;&#65292;&#20542;&#21521;&#20110;&#23558;LLM&#29983;&#25104;&#30340;&#25991;&#26723;&#25490;&#21517;&#36739;&#39640;&#12290;&#36825;&#31181;&#8220;&#26469;&#28304;&#20559;&#35265;&#8221;&#21487;&#33021;&#23545;&#20449;&#24687;&#35775;&#38382;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.20501</link><description>&lt;p&gt;
LLM&#21487;&#33021;&#20027;&#23548;&#20449;&#24687;&#35775;&#38382;&#65306;&#31070;&#32463;&#26816;&#32034;&#22120;&#23545;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#23384;&#22312;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
LLMs may Dominate Information Access: Neural Retrievers are Biased Towards LLM-Generated Texts. (arXiv:2310.20501v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20501
&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#20135;&#29983;&#20102;&#19968;&#31181;&#20559;&#35265;&#65292;&#20542;&#21521;&#20110;&#23558;LLM&#29983;&#25104;&#30340;&#25991;&#26723;&#25490;&#21517;&#36739;&#39640;&#12290;&#36825;&#31181;&#8220;&#26469;&#28304;&#20559;&#35265;&#8221;&#21487;&#33021;&#23545;&#20449;&#24687;&#35775;&#38382;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#22312;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#24212;&#29992;&#65292;&#23588;&#20854;&#26159;&#22312;&#32593;&#32476;&#25628;&#32034;&#26041;&#38754;&#65292;&#24443;&#24213;&#25913;&#21464;&#20102;&#33539;&#24335;&#12290;&#30001;&#20110;&#20854;&#22312;&#29983;&#25104;&#31867;&#20154;&#25991;&#26412;&#26041;&#38754;&#30340;&#21331;&#36234;&#33021;&#21147;&#65292;LLMs&#22312;&#20114;&#32852;&#32593;&#19978;&#21019;&#36896;&#20102;&#22823;&#37327;&#30340;&#25991;&#26412;&#12290;&#22240;&#27492;&#65292;LLMs&#26102;&#20195;&#30340;IR&#31995;&#32479;&#38754;&#20020;&#19968;&#20010;&#26032;&#30340;&#25361;&#25112;&#65306;&#32034;&#24341;&#30340;&#25991;&#26723;&#19981;&#20165;&#26159;&#30001;&#20154;&#31867;&#25776;&#20889;&#30340;&#65292;&#32780;&#19988;&#36824;&#21253;&#25324;&#30001;LLMs&#33258;&#21160;&#29983;&#25104;&#30340;&#25991;&#26723;&#12290;&#36825;&#20123;LLM&#29983;&#25104;&#30340;&#25991;&#26723;&#22914;&#20309;&#24433;&#21709;IR&#31995;&#32479;&#26159;&#19968;&#20010;&#32039;&#36843;&#19988;&#23578;&#26410;&#25506;&#32034;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#22312;&#28041;&#21450;&#20154;&#31867;&#32534;&#20889;&#21644;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#30340;&#19981;&#21516;IR&#27169;&#22411;&#30340;&#22330;&#26223;&#20013;&#36827;&#34892;&#20102;&#23450;&#37327;&#35780;&#20272;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#31070;&#32463;&#26816;&#32034;&#27169;&#22411;&#20542;&#21521;&#20110;&#23558;LLM&#29983;&#25104;&#30340;&#25991;&#26723;&#25490;&#21517;&#36739;&#39640;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#31070;&#32463;&#26816;&#32034;&#27169;&#22411;&#23545;LLM&#29983;&#25104;&#25991;&#26412;&#30340;&#20559;&#35265;&#31216;&#20026;&#8220;&#26469;&#28304;&#20559;&#35265;&#8221;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#31181;&#20559;&#35265;&#19981;&#20165;&#38480;&#20110;f&#26041;&#30456;&#24403;&#30340;&#24773;&#20917;&#65292;&#32780;&#19988;&#22312;&#20998;&#31867;&#20219;&#21153;&#19978;&#20063;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, the emergence of large language models (LLMs) has revolutionized the paradigm of information retrieval (IR) applications, especially in web search. With their remarkable capabilities in generating human-like texts, LLMs have created enormous texts on the Internet. As a result, IR systems in the LLMs era are facing a new challenge: the indexed documents now are not only written by human beings but also automatically generated by the LLMs. How these LLM-generated documents influence the IR systems is a pressing and still unexplored question. In this work, we conduct a quantitative evaluation of different IR models in scenarios where both human-written and LLM-generated texts are involved. Surprisingly, our findings indicate that neural retrieval models tend to rank LLM-generated documents higher. We refer to this category of biases in neural retrieval models towards the LLM-generated text as the \textbf{source bias}. Moreover, we discover that this bias is not confined to the f
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TMR&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#20174;&#27169;&#25311;&#25968;&#25454;&#29615;&#22659;&#20013;&#25366;&#25496;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2306.10345</link><description>&lt;p&gt;
&#20570;&#25105;&#33021;&#20570;&#30340;&#65292;&#32780;&#19981;&#26159;&#25105;&#24471;&#21040;&#30340;&#12290;(arXiv:2306.10345v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
Do as I can, not as I get. (arXiv:2306.10345v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10345
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TMR&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#20174;&#27169;&#25311;&#25968;&#25454;&#29615;&#22659;&#20013;&#25366;&#25496;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TMR&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#20174;&#27169;&#25311;&#25968;&#25454;&#29615;&#22659;&#20013;&#25366;&#25496;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#25171;&#31639;&#23436;&#25104;&#26412;&#25991;&#30340;&#25237;&#31295;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a model called TMR to mine valuable information from simulated data environments. We intend to complete the submission of this paper.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#32423;&#32852;&#25351;&#23548;&#19979;&#30340;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#65292;&#22686;&#24378;&#20102;&#20018;&#32852;&#25512;&#33616;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#65292;&#21462;&#24471;&#20102;&#27604;&#24050;&#26377;&#26041;&#27861;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.05492</link><description>&lt;p&gt;
&#25913;&#36827;&#20018;&#32852;&#25512;&#33616;&#30340;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;: &#20276;&#38543;&#32423;&#32852;&#25351;&#23548;&#30340;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards More Robust and Accurate Sequential Recommendation with Cascade-guided Adversarial Training. (arXiv:2304.05492v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05492
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#32423;&#32852;&#25351;&#23548;&#19979;&#30340;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#65292;&#22686;&#24378;&#20102;&#20018;&#32852;&#25512;&#33616;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#65292;&#21462;&#24471;&#20102;&#27604;&#24050;&#26377;&#26041;&#27861;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20018;&#32852;&#25512;&#33616;&#27169;&#22411;&#26159;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#29992;&#25143;&#19982;&#29289;&#21697;&#38388;&#30340;&#26102;&#38388;&#39034;&#24207;&#20114;&#21160;&#26469;&#36827;&#34892;&#25512;&#33616;&#30340;&#27169;&#22411;&#65292;&#20854;&#24050;&#32463;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#23637;&#29616;&#20986;&#20102;&#33391;&#22909;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#36817;&#26399;&#20018;&#32852;&#25512;&#33616;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#22791;&#21463;&#36136;&#30097;&#12290;&#36825;&#31181;&#27169;&#22411;&#30340;&#20004;&#20010;&#29305;&#24615;&#20351;&#20854;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987; - &#22312;&#35757;&#32451;&#20013;&#20250;&#20135;&#29983;&#32423;&#32852;&#25928;&#24212;&#65292;&#22312;&#27169;&#22411;&#36807;&#24230;&#20381;&#36182;&#26102;&#38388;&#20449;&#24687;&#30340;&#21516;&#26102;&#20250;&#24573;&#30053;&#20854;&#20182;&#29305;&#24449;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20018;&#32852;&#25512;&#33616;&#27169;&#22411;&#30340;&#32423;&#32852;&#25351;&#23548;&#19979;&#30340;&#23545;&#25239;&#35757;&#32451;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20018;&#32852;&#24314;&#27169;&#20013;&#30340;&#20869;&#22312;&#32423;&#32852;&#25928;&#24212;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20135;&#29983;&#25112;&#30053;&#24615;&#30340;&#23545;&#25239;&#24615;&#25200;&#21160;&#26469;&#24433;&#21709;&#29289;&#21697;&#23884;&#20837;&#12290;&#22312;&#20351;&#29992;&#19981;&#21516;&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#35757;&#32451;&#22235;&#31181;&#26368;&#20808;&#36827;&#30340;&#20018;&#32852;&#27169;&#22411;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#35757;&#32451;&#26041;&#27861;&#20135;&#29983;&#20102;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#39640;&#30340;&#27169;&#22411;&#40065;&#26834;&#24615;&#65292;&#24182;&#33719;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequential recommendation models, models that learn from chronological user-item interactions, outperform traditional recommendation models in many settings. Despite the success of sequential recommendation models, their robustness has recently come into question. Two properties unique to the nature of sequential recommendation models may impair their robustness - the cascade effects induced during training and the model's tendency to rely too heavily on temporal information. To address these vulnerabilities, we propose Cascade-guided Adversarial training, a new adversarial training procedure that is specifically designed for sequential recommendation models. Our approach harnesses the intrinsic cascade effects present in sequential modeling to produce strategic adversarial perturbations to item embeddings during training. Experiments on training state-of-the-art sequential models on four public datasets from different domains show that our training approach produces superior model ran
&lt;/p&gt;</description></item></channel></rss>