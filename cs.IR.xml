<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#28120;&#23453;&#25628;&#32034;&#30340;&#26816;&#32034;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#20102;&#38024;&#23545;&#22823;&#35268;&#27169;&#26816;&#32034;&#20219;&#21153;&#30340;&#36127;&#37319;&#26679;&#31574;&#30053;&#65292;&#24182;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#21462;&#24471;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#30446;&#21069;&#26381;&#21153;&#25968;&#20159;&#29992;&#25143;&#12290;</title><link>http://arxiv.org/abs/2304.04377</link><description>&lt;p&gt;
&#25506;&#31350;&#22522;&#20110;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#30340;&#30005;&#21830;&#20135;&#21697;&#26816;&#32034;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Delving into E-Commerce Product Retrieval with Vision-Language Pre-training. (arXiv:2304.04377v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04377
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#28120;&#23453;&#25628;&#32034;&#30340;&#26816;&#32034;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#20102;&#38024;&#23545;&#22823;&#35268;&#27169;&#26816;&#32034;&#20219;&#21153;&#30340;&#36127;&#37319;&#26679;&#31574;&#30053;&#65292;&#24182;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#21462;&#24471;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#30446;&#21069;&#26381;&#21153;&#25968;&#20159;&#29992;&#25143;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#21830;&#25628;&#32034;&#24341;&#25806;&#21253;&#25324;&#26816;&#32034;&#38454;&#27573;&#21644;&#25490;&#21517;&#38454;&#27573;&#65292;&#20854;&#20013;&#26816;&#32034;&#38454;&#27573;&#26681;&#25454;&#29992;&#25143;&#26597;&#35810;&#36820;&#22238;&#20505;&#36873;&#20135;&#21697;&#38598;&#12290;&#26368;&#36817;&#65292;&#23558;&#25991;&#26412;&#20449;&#24687;&#21644;&#35270;&#35273;&#32447;&#32034;&#32467;&#21512;&#30340;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#22312;&#26816;&#32034;&#20219;&#21153;&#20013;&#24191;&#21463;&#27426;&#36814;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;V+L&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#28120;&#23453;&#25628;&#32034;&#30340;&#26816;&#32034;&#38382;&#39064;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#35270;&#35273;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#20248;&#20110;&#24120;&#35268;&#22522;&#20110;&#22238;&#24402;&#30340;&#35270;&#35273;&#39044;&#35757;&#32451;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#20004;&#31181;&#38024;&#23545;&#22823;&#35268;&#27169;&#26816;&#32034;&#20219;&#21153;&#37327;&#36523;&#23450;&#21046;&#30340;&#36127;&#37319;&#26679;&#31574;&#30053;&#12290;&#38500;&#27492;&#20043;&#22806;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#30340;&#22312;&#32447;&#37096;&#32626;&#32454;&#33410;&#12290;&#28145;&#20837;&#30340;&#31163;&#32447;/&#22312;&#32447;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26816;&#32034;&#20219;&#21153;&#20013;&#20855;&#26377;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#34987;&#29992;&#20316;&#28120;&#23453;&#25628;&#32034;&#30340;&#19968;&#20010;&#26816;&#32034;&#36890;&#36947;&#65292;&#24182;&#23454;&#26102;&#26381;&#21153;&#30528;&#25968;&#20159;&#29992;&#25143;&#12290;
&lt;/p&gt;
&lt;p&gt;
E-commerce search engines comprise a retrieval phase and a ranking phase, where the first one returns a candidate product set given user queries. Recently, vision-language pre-training, combining textual information with visual clues, has been popular in the application of retrieval tasks. In this paper, we propose a novel V+L pre-training method to solve the retrieval problem in Taobao Search. We design a visual pre-training task based on contrastive learning, outperforming common regression-based visual pre-training tasks. In addition, we adopt two negative sampling schemes, tailored for the large-scale retrieval task. Besides, we introduce the details of the online deployment of our proposed method in real-world situations. Extensive offline/online experiments demonstrate the superior performance of our method on the retrieval task. Our proposed method is employed as one retrieval channel of Taobao Search and serves hundreds of millions of users in real time.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27010;&#24565;&#20540;&#29942;&#39048;&#27169;&#22411;LACE&#65292;&#29992;&#20110;&#21487;&#25511;&#25991;&#26412;&#25512;&#33616;&#12290;&#35813;&#27169;&#22411;&#22522;&#20110;&#29992;&#25143;&#25991;&#26723;&#23398;&#20064;&#20010;&#24615;&#21270;&#30340;&#27010;&#24565;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#22810;&#31181;&#20132;&#20114;&#26041;&#24335;&#20026;&#29992;&#25143;&#25552;&#20379;&#20102;&#25511;&#21046;&#25512;&#33616;&#30340;&#26426;&#21046;&#65292;&#39564;&#35777;&#20102;&#22312;&#31163;&#32447;&#21644;&#22312;&#32447;&#23454;&#39564;&#20013;&#35813;&#27169;&#22411;&#30340;&#25512;&#33616;&#36136;&#37327;&#21644;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.04250</link><description>&lt;p&gt;
&#21487;&#32534;&#36753;&#29992;&#25143;&#26723;&#26696;&#30340;&#21487;&#25511;&#25991;&#26412;&#25512;&#33616;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Editable User Profiles for Controllable Text Recommendation. (arXiv:2304.04250v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04250
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27010;&#24565;&#20540;&#29942;&#39048;&#27169;&#22411;LACE&#65292;&#29992;&#20110;&#21487;&#25511;&#25991;&#26412;&#25512;&#33616;&#12290;&#35813;&#27169;&#22411;&#22522;&#20110;&#29992;&#25143;&#25991;&#26723;&#23398;&#20064;&#20010;&#24615;&#21270;&#30340;&#27010;&#24565;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#22810;&#31181;&#20132;&#20114;&#26041;&#24335;&#20026;&#29992;&#25143;&#25552;&#20379;&#20102;&#25511;&#21046;&#25512;&#33616;&#30340;&#26426;&#21046;&#65292;&#39564;&#35777;&#20102;&#22312;&#31163;&#32447;&#21644;&#22312;&#32447;&#23454;&#39564;&#20013;&#35813;&#27169;&#22411;&#30340;&#25512;&#33616;&#36136;&#37327;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#29616;&#39640;&#36136;&#37327;&#25512;&#33616;&#30340;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#20174;&#20132;&#20114;&#25968;&#25454;&#20013;&#23398;&#20064;&#28508;&#22312;&#34920;&#31034;&#12290;&#28982;&#32780;&#36825;&#20123;&#26041;&#27861;&#27809;&#26377;&#25552;&#20379;&#32473;&#29992;&#25143;&#25511;&#21046;&#25152;&#25509;&#25910;&#30340;&#25512;&#33616;&#30340;&#26426;&#21046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;LACE&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#27010;&#24565;&#20540;&#29942;&#39048;&#27169;&#22411;&#65292;&#29992;&#20110;&#21487;&#25511;&#25991;&#26412;&#25512;&#33616;&#12290;LACE&#22522;&#20110;&#29992;&#25143;&#20132;&#20114;&#30340;&#25991;&#26723;&#26816;&#32034;&#65292;&#23558;&#27599;&#20010;&#29992;&#25143;&#34920;&#31034;&#20026;&#31616;&#27905;&#30340;&#21487;&#35835;&#30340;&#27010;&#24565;&#38598;&#65292;&#24182;&#22522;&#20110;&#29992;&#25143;&#25991;&#26723;&#23398;&#20064;&#27010;&#24565;&#30340;&#20010;&#24615;&#21270;&#34920;&#31034;&#12290;&#35813;&#22522;&#20110;&#27010;&#24565;&#30340;&#29992;&#25143;&#26723;&#26696;&#34987;&#21033;&#29992;&#26469;&#20570;&#20986;&#25512;&#33616;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#35774;&#35745;&#36890;&#36807;&#36879;&#26126;&#30340;&#29992;&#25143;&#26723;&#26696;&#65292;&#25552;&#20379;&#20102;&#25511;&#21046;&#25512;&#33616;&#30340;&#22810;&#31181;&#30452;&#35266;&#20132;&#20114;&#26041;&#24335;&#12290;&#25105;&#20204;&#39318;&#20808;&#22312;&#19977;&#20010;&#25512;&#33616;&#20219;&#21153;&#65288;&#28201;&#21551;&#21160;&#12289;&#20919;&#21551;&#21160;&#21644;&#38646;&#26679;&#26412;&#65289;&#30340;&#20845;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#31163;&#32447;&#35780;&#20272;&#65292;&#39564;&#35777;&#20102;&#20174;LACE&#33719;&#24471;&#30340;&#25512;&#33616;&#36136;&#37327;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#22312;&#22312;&#32447;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;LACE&#30340;&#26377;&#25928;&#24615;&#21644;&#29992;&#25143;&#25511;&#21046;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Methods for making high-quality recommendations often rely on learning latent representations from interaction data. These methods, while performant, do not provide ready mechanisms for users to control the recommendation they receive. Our work tackles this problem by proposing LACE, a novel concept value bottleneck model for controllable text recommendations. LACE represents each user with a succinct set of human-readable concepts through retrieval given user-interacted documents and learns personalized representations of the concepts based on user documents. This concept based user profile is then leveraged to make recommendations. The design of our model affords control over the recommendations through a number of intuitive interactions with a transparent user profile. We first establish the quality of recommendations obtained from LACE in an offline evaluation on three recommendation tasks spanning six datasets in warm-start, cold-start, and zero-shot setups. Next, we validate the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#28145;&#24230;&#22270;&#20687;&#26816;&#32034;&#20013;&#23545;&#25239;&#25915;&#20987;&#30340;&#26080;&#30417;&#30563;&#22810;&#26631;&#20934;&#26816;&#27979;&#26041;&#26696;&#65292;&#33021;&#22815;&#26377;&#25928;&#38480;&#21046;&#23545;&#25239;&#31354;&#38388;&#24182;&#25552;&#39640;&#26816;&#27979;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.04228</link><description>&lt;p&gt;
&#28145;&#24230;&#22270;&#20687;&#26816;&#32034;&#20013;&#26080;&#30417;&#30563;&#22810;&#26631;&#20934;&#23545;&#25239;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Multi-Criteria Adversarial Detection in Deep Image Retrieval. (arXiv:2304.04228v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04228
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#28145;&#24230;&#22270;&#20687;&#26816;&#32034;&#20013;&#23545;&#25239;&#25915;&#20987;&#30340;&#26080;&#30417;&#30563;&#22810;&#26631;&#20934;&#26816;&#27979;&#26041;&#26696;&#65292;&#33021;&#22815;&#26377;&#25928;&#38480;&#21046;&#23545;&#25239;&#31354;&#38388;&#24182;&#25552;&#39640;&#26816;&#27979;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#20379;&#24212;&#38142;&#20013;&#30340;&#28431;&#27934;&#23545;&#19979;&#28216;&#22270;&#20687;&#26816;&#32034;&#31995;&#32479;&#25552;&#20986;&#20102;&#26032;&#30340;&#25361;&#25112;&#12290;&#22312;&#21508;&#31181;&#25216;&#26415;&#20013;&#65292;&#28145;&#24230;&#21704;&#24076;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#30001;&#20110;&#32487;&#25215;&#20102;&#26469;&#33258;&#28145;&#24230;&#23398;&#20064;&#30340;&#31639;&#27861;&#21518;&#31471;&#65292;&#26368;&#36817;&#25552;&#20986;&#20102;&#19968;&#20123;&#25915;&#20987;&#26041;&#27861;&#26469;&#30772;&#22351;&#27491;&#24120;&#30340;&#22270;&#20687;&#26816;&#32034;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#20998;&#31867;softmax&#20013;&#30340;&#38450;&#24481;&#31574;&#30053;&#24182;&#19981;&#36866;&#29992;&#20110;&#22270;&#20687;&#26816;&#32034;&#39046;&#22495;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#26080;&#30417;&#30563;&#26041;&#26696;&#26469;&#35782;&#21035;&#21704;&#24076;&#31354;&#38388;&#20013;&#30340;&#29420;&#29305;&#23545;&#25239;&#34892;&#20026;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#20174;&#27721;&#26126;&#36317;&#31163;&#12289;&#37327;&#21270;&#25439;&#22833;&#21644;&#21435;&#22122;&#30340;&#35282;&#24230;&#35774;&#35745;&#20102;&#19977;&#20010;&#26631;&#20934;&#65292;&#20197;&#23545;&#25239;&#38750;&#23450;&#21521;&#21644;&#23450;&#21521;&#25915;&#20987;&#65292;&#20849;&#21516;&#38480;&#21046;&#23545;&#25239;&#31354;&#38388;&#12290;&#22312;&#22235;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;&#26816;&#27979;&#29575;&#25552;&#39640;&#20102;2-23%&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#20102;&#23454;&#26102;&#22270;&#20687;&#26597;&#35810;&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;
The vulnerability in the algorithm supply chain of deep learning has imposed new challenges to image retrieval systems in the downstream. Among a variety of techniques, deep hashing is gaining popularity. As it inherits the algorithmic backend from deep learning, a handful of attacks are recently proposed to disrupt normal image retrieval. Unfortunately, the defense strategies in softmax classification are not readily available to be applied in the image retrieval domain. In this paper, we propose an efficient and unsupervised scheme to identify unique adversarial behaviors in the hamming space. In particular, we design three criteria from the perspectives of hamming distance, quantization loss and denoising to defend against both untargeted and targeted attacks, which collectively limit the adversarial space. The extensive experiments on four datasets demonstrate 2-23% improvements of detection rates with minimum computational overhead for real-time image queries.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#38750;&#37325;&#21472;&#20132;&#21449;&#39046;&#22495;&#39034;&#24207;&#25512;&#33616;&#30340;&#33258;&#21160;&#25552;&#31034;&#26694;&#26550;PLNCSR&#65292;&#21487;&#36890;&#36807;&#20351;&#29992;&#28304;&#39046;&#22495;&#20013;&#30340;&#29992;&#25143;&#39034;&#24207;&#20132;&#20114;&#27169;&#24335;&#26469;&#25512;&#26029;&#30446;&#26631;&#39046;&#22495;&#20013;&#30340;&#32570;&#22833;&#20132;&#20114;&#65292;&#20174;&#32780;&#26377;&#25928;&#32531;&#35299;&#25968;&#25454;&#31232;&#30095;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.04218</link><description>&lt;p&gt;
&#38754;&#21521;&#38750;&#37325;&#21472;&#20132;&#21449;&#39046;&#22495;&#39034;&#24207;&#25512;&#33616;&#30340;&#33258;&#21160;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Automated Prompting for Non-overlapping Cross-domain Sequential Recommendation. (arXiv:2304.04218v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04218
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#38750;&#37325;&#21472;&#20132;&#21449;&#39046;&#22495;&#39034;&#24207;&#25512;&#33616;&#30340;&#33258;&#21160;&#25552;&#31034;&#26694;&#26550;PLNCSR&#65292;&#21487;&#36890;&#36807;&#20351;&#29992;&#28304;&#39046;&#22495;&#20013;&#30340;&#29992;&#25143;&#39034;&#24207;&#20132;&#20114;&#27169;&#24335;&#26469;&#25512;&#26029;&#30446;&#26631;&#39046;&#22495;&#20013;&#30340;&#32570;&#22833;&#20132;&#20114;&#65292;&#20174;&#32780;&#26377;&#25928;&#32531;&#35299;&#25968;&#25454;&#31232;&#30095;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#20132;&#21449;&#39046;&#22495;&#25512;&#33616;&#65288;CR&#65289;&#34987;&#24191;&#27867;&#30740;&#31350;&#65292;&#36890;&#36807;&#21033;&#29992;&#19981;&#21516;&#39046;&#22495;&#30340;&#20449;&#24687;&#26469;&#32531;&#35299;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#25968;&#25454;&#31232;&#30095;&#38382;&#39064;&#12290;&#26412;&#25991;&#32858;&#28966;&#20110;&#26356;&#19968;&#33324;&#30340;&#38750;&#37325;&#21472;&#20132;&#21449;&#39046;&#22495;&#39034;&#24207;&#25512;&#33616;&#65288;NCSR&#65289;&#22330;&#26223;&#12290;NCSR&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#39046;&#22495;&#20043;&#38388;&#27809;&#26377;&#37325;&#21472;&#30340;&#23454;&#20307;&#65288;&#20363;&#22914;&#29992;&#25143;&#21644;&#39033;&#30446;&#65289;&#65292;&#21482;&#26377;&#29992;&#25143;&#30340;&#38544;&#24335;&#21453;&#39304;&#21644;&#27809;&#26377;&#20869;&#23481;&#20449;&#24687;&#12290;&#20197;&#21069;&#30340;CR&#26041;&#27861;&#26080;&#27861;&#24456;&#22909;&#22320;&#35299;&#20915;NCSR&#38382;&#39064;&#65292;&#22240;&#20026;&#20182;&#20204;&#35201;&#20040;&#38656;&#35201;&#39069;&#22806;&#30340;&#20869;&#23481;&#26469;&#23545;&#40784;&#39046;&#22495;&#65292;&#35201;&#20040;&#38656;&#35201;&#26174;&#24335;&#30340;&#39046;&#22495;&#23545;&#40784;&#32422;&#26463;&#26469;&#38477;&#20302;&#39046;&#22495;&#24046;&#24322;&#65292;&#35201;&#20040;&#26356;&#20851;&#27880;&#29992;&#25143;&#30340;&#26174;&#24335;&#21453;&#39304;&#65288;&#21363;&#29992;&#25143;&#30340;&#35780;&#20998;&#25968;&#25454;&#65289;&#65292;&#32780;&#26080;&#27861;&#24456;&#22909;&#22320;&#25429;&#25417;&#20182;&#20204;&#30340;&#39034;&#24207;&#20132;&#20114;&#27169;&#24335;&#65292;&#20182;&#20204;&#36890;&#24120;&#21482;&#36827;&#34892;&#21333;&#30446;&#26631;&#30340;&#36328;&#39046;&#22495;&#25512;&#33616;&#20219;&#21153;&#65292;&#24456;&#23569;&#30740;&#31350;&#21452;&#30446;&#26631;&#30340;&#20219;&#21153;&#12290;&#32771;&#34385;&#21040;&#19978;&#36848;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#38754;&#21521;&#38750;&#37325;&#21472;&#20132;&#21449;&#39046;&#22495;&#39034;&#24207;&#25512;&#33616;&#30340;&#33258;&#21160;&#25552;&#31034;&#26694;&#26550;&#65288;PLNCSR&#65289;&#65292;&#36890;&#36807;&#21033;&#29992;&#29992;&#25143;&#22312;&#28304;&#39046;&#22495;&#20013;&#30340;&#39034;&#24207;&#20132;&#20114;&#27169;&#24335;&#65292;&#26088;&#22312;&#25552;&#31034;&#65288;&#21363;&#25512;&#26029;&#65289;&#30446;&#26631;&#39046;&#22495;&#20013;&#29992;&#25143;&#21644;&#39033;&#30446;&#20043;&#38388;&#30340;&#32570;&#22833;&#20132;&#20114;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#26426;&#21046;&#65292;&#20197;&#32852;&#21512;&#20248;&#21270;&#36328;&#39046;&#22495;&#25512;&#33616;&#21644;&#25552;&#31034;&#25512;&#26029;&#30446;&#26631;&#12290;&#22312;&#20004;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;PLNCSR&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-domain Recommendation (CR) has been extensively studied in recent years to alleviate the data sparsity issue in recommender systems by utilizing different domain information. In this work, we focus on the more general Non-overlapping Cross-domain Sequential Recommendation (NCSR) scenario. NCSR is challenging because there are no overlapped entities (e.g., users and items) between domains, and there is only users' implicit feedback and no content information. Previous CR methods cannot solve NCSR well, since (1) they either need extra content to align domains or need explicit domain alignment constraints to reduce the domain discrepancy from domain-invariant features, (2) they pay more attention to users' explicit feedback (i.e., users' rating data) and cannot well capture their sequential interaction patterns, (3) they usually do a single-target cross-domain recommendation task and seldom investigate the dual-target ones. Considering the above challenges, we propose Prompt Learni
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GenRet&#30340;&#25991;&#26723;&#20998;&#35789;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#26816;&#32034;&#20219;&#21153;&#20013;&#30340;&#25991;&#26723;&#26631;&#35782;&#31526;&#23450;&#20041;&#65292;&#23427;&#33021;&#22815;&#26356;&#22909;&#22320;&#25429;&#25417;&#25991;&#26723;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2304.04171</link><description>&lt;p&gt;
&#23398;&#20064;&#29983;&#25104;&#26816;&#32034;&#30340;&#20998;&#35789;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Learning to Tokenize for Generative Retrieval. (arXiv:2304.04171v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04171
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GenRet&#30340;&#25991;&#26723;&#20998;&#35789;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#26816;&#32034;&#20219;&#21153;&#20013;&#30340;&#25991;&#26723;&#26631;&#35782;&#31526;&#23450;&#20041;&#65292;&#23427;&#33021;&#22815;&#26356;&#22909;&#22320;&#25429;&#25417;&#25991;&#26723;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#25991;&#26723;&#26816;&#32034;&#25216;&#26415;&#20027;&#35201;&#22522;&#20110;&#32034;&#24341;&#26816;&#32034;&#33539;&#24335;&#65292;&#22312;&#31471;&#21040;&#31471;&#30340;&#20248;&#21270;&#27969;&#31243;&#20013;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20316;&#20026;&#26367;&#20195;&#26041;&#26696;&#65292;&#29983;&#25104;&#24335;&#26816;&#32034;&#23558;&#25991;&#26723;&#34920;&#31034;&#20026;&#26631;&#35782;&#31526;&#65288;docid&#65289;&#65292;&#24182;&#36890;&#36807;&#29983;&#25104;docid&#26469;&#26816;&#32034;&#25991;&#26723;&#65292;&#23454;&#29616;&#20102;&#25991;&#26723;&#26816;&#32034;&#20219;&#21153;&#30340;&#31471;&#21040;&#31471;&#24314;&#27169;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#23450;&#20041;&#25991;&#26723;&#26631;&#35782;&#31526;&#20173;&#26159;&#19968;&#20010;&#24320;&#25918;&#24615;&#38382;&#39064;&#12290;&#24403;&#21069;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#22522;&#20110;&#22266;&#23450;&#35268;&#21017;&#30340;docid&#29983;&#25104;&#65292;&#22914;&#25991;&#26723;&#26631;&#39064;&#25110;&#32858;&#31867;BERT&#23884;&#20837;&#30340;&#32467;&#26524;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#24448;&#24448;&#26080;&#27861;&#23436;&#25972;&#25429;&#25417;&#25991;&#26723;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GenRet&#30340;&#25991;&#26723;&#20998;&#35789;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#24212;&#23545;&#20026;&#29983;&#25104;&#26816;&#32034;&#23450;&#20041;&#25991;&#26723;&#26631;&#35782;&#31526;&#30340;&#25361;&#25112;&#12290;GenRet&#36890;&#36807;&#31163;&#25955;&#33258;&#32534;&#30721;&#26041;&#27861;&#23398;&#20064;&#23558;&#25991;&#26723;&#20998;&#35789;&#20026;&#30701;&#31163;&#25955;&#34920;&#31034;&#65288;&#21363;docid&#65289;&#12290;&#35813;&#26041;&#27861;&#21253;&#25324;&#19977;&#20010;&#32452;&#25104;&#37096;&#20998;&#65306;
&lt;/p&gt;
&lt;p&gt;
Conventional document retrieval techniques are mainly based on the index-retrieve paradigm. It is challenging to optimize pipelines based on this paradigm in an end-to-end manner. As an alternative, generative retrieval represents documents as identifiers (docid) and retrieves documents by generating docids, enabling end-to-end modeling of document retrieval tasks. However, it is an open question how one should define the document identifiers. Current approaches to the task of defining document identifiers rely on fixed rule-based docids, such as the title of a document or the result of clustering BERT embeddings, which often fail to capture the complete semantic information of a document. We propose GenRet, a document tokenization learning method to address the challenge of defining document identifiers for generative retrieval. GenRet learns to tokenize documents into short discrete representations (i.e., docids) via a discrete auto-encoding approach. Three components are included in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#25552;&#31034;&#21644;&#22320;&#29702;&#24863;&#30693;&#30340;&#20301;&#32622;&#25512;&#33616;&#26694;&#26550;&#65292;&#33021;&#26356;&#22909;&#22320;&#39044;&#27979;&#36827;&#19968;&#27493;&#30340;&#20301;&#32622;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#32534;&#30721;&#22320;&#29702;&#20449;&#24687;&#26102;&#36935;&#21040;&#30340;&#22256;&#38590;&#36793;&#30028;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.04151</link><description>&lt;p&gt;
&#26102;&#38388;&#25139;&#20316;&#20026;&#22320;&#29702;&#24863;&#30693;&#30340;&#20301;&#32622;&#25512;&#33616;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Timestamps as Prompts for Geography-Aware Location Recommendation. (arXiv:2304.04151v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04151
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#25552;&#31034;&#21644;&#22320;&#29702;&#24863;&#30693;&#30340;&#20301;&#32622;&#25512;&#33616;&#26694;&#26550;&#65292;&#33021;&#26356;&#22909;&#22320;&#39044;&#27979;&#36827;&#19968;&#27493;&#30340;&#20301;&#32622;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#32534;&#30721;&#22320;&#29702;&#20449;&#24687;&#26102;&#36935;&#21040;&#30340;&#22256;&#38590;&#36793;&#30028;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20301;&#32622;&#25512;&#33616;&#22312;&#25552;&#39640;&#29992;&#25143;&#26053;&#34892;&#20307;&#39564;&#26041;&#38754;&#25198;&#28436;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#35282;&#33394;&#12290;&#34987;&#39044;&#27979;&#30340;POI&#30340;&#26102;&#38388;&#25139;&#38750;&#24120;&#37325;&#35201;&#65292;&#22240;&#20026;&#29992;&#25143;&#20250;&#22312;&#19981;&#21516;&#26102;&#38388;&#21435;&#19981;&#21516;&#30340;&#22320;&#26041;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#35201;&#20040;&#19981;&#20351;&#29992;&#36825;&#31181;&#26102;&#38388;&#20449;&#24687;&#65292;&#35201;&#20040;&#21482;&#26159;&#38544;&#21547;&#22320;&#19982;&#20854;&#20182;&#19978;&#19979;&#25991;&#20449;&#24687;&#34701;&#21512;&#12290;&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#20301;&#32622;&#25512;&#33616;&#38382;&#39064;&#65292;&#24182;&#25351;&#20986;&#26174;&#24335;&#22320;&#24314;&#27169;&#26102;&#38388;&#20449;&#24687;&#22312;&#27169;&#22411;&#38656;&#35201;&#39044;&#27979;&#19981;&#20165;&#19979;&#19968;&#20010;&#20301;&#32622;&#65292;&#36824;&#38656;&#35201;&#39044;&#27979;&#36827;&#19968;&#27493;&#20301;&#32622;&#26102;&#22823;&#26377;&#35048;&#30410;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#27809;&#26377;&#26377;&#25928;&#21033;&#29992;&#22320;&#29702;&#20449;&#24687;&#65292;&#24182;&#19988;&#22312;&#36890;&#36807;&#32593;&#26684;&#21270;&#32534;&#30721;&#22320;&#29702;&#20449;&#24687;&#26102;&#36935;&#21040;&#20102;&#22256;&#38590;&#36793;&#30028;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#25552;&#31034;&#30340;&#22320;&#29702;&#24863;&#30693;&#65288;TPG&#65289;&#26694;&#26550;&#12290;&#39318;&#20808;&#35774;&#35745;&#20102;&#26102;&#38388;&#25552;&#31034;&#20197;&#34701;&#21512;&#20219;&#20309;&#36827;&#19968;&#27493;&#30340;&#31614;&#21040;&#30340;&#26102;&#38388;&#20449;&#24687;&#12290;&#28982;&#21518;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#31227;&#21160;&#31383;&#21475;&#26426;&#21046;&#23545;&#22320;&#29702;&#20449;&#24687;&#36827;&#34892;&#32534;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Location recommendation plays a vital role in improving users' travel experience. The timestamp of the POI to be predicted is of great significance, since a user will go to different places at different times. However, most existing methods either do not use this kind of temporal information, or just implicitly fuse it with other contextual information. In this paper, we revisit the problem of location recommendation and point out that explicitly modeling temporal information is a great help when the model needs to predict not only the next location but also further locations. In addition, state-of-the-art methods do not make effective use of geographic information and suffer from the hard boundary problem when encoding geographic information by gridding. To this end, a Temporal Prompt-based and Geography-aware (TPG) framework is proposed. The temporal prompt is firstly designed to incorporate temporal information of any further check-in. A shifted window mechanism is then devised to a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20027;&#39064;&#23884;&#20837;&#26041;&#27861;&#21644;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#26080;&#30417;&#30563;&#22312;&#32447;&#25925;&#20107;&#21457;&#29616;&#26694;&#26550;USTORY&#65292;&#21487;&#20197;&#21160;&#24577;&#34920;&#31034;&#25991;&#31456;&#21644;&#25925;&#20107;&#65292;&#24182;&#32771;&#34385;&#23427;&#20204;&#20849;&#20139;&#30340;&#26102;&#38388;&#20027;&#39064;&#21644;&#26032;&#39062;&#24615;&#65292;&#20197;&#24110;&#21161;&#20154;&#20204;&#28040;&#21270;&#22823;&#37327;&#30340;&#26032;&#38395;&#27969;&#12290;</title><link>http://arxiv.org/abs/2304.04099</link><description>&lt;p&gt;
&#36890;&#36807;&#21487;&#25193;&#23637;&#30340;&#20027;&#39064;&#23884;&#20837;&#20174;&#36830;&#32493;&#26032;&#38395;&#27969;&#20013;&#26080;&#30417;&#30563;&#22320;&#21457;&#29616;&#25925;&#20107;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Story Discovery from Continuous News Streams via Scalable Thematic Embedding. (arXiv:2304.04099v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04099
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20027;&#39064;&#23884;&#20837;&#26041;&#27861;&#21644;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#26080;&#30417;&#30563;&#22312;&#32447;&#25925;&#20107;&#21457;&#29616;&#26694;&#26550;USTORY&#65292;&#21487;&#20197;&#21160;&#24577;&#34920;&#31034;&#25991;&#31456;&#21644;&#25925;&#20107;&#65292;&#24182;&#32771;&#34385;&#23427;&#20204;&#20849;&#20139;&#30340;&#26102;&#38388;&#20027;&#39064;&#21644;&#26032;&#39062;&#24615;&#65292;&#20197;&#24110;&#21161;&#20154;&#20204;&#28040;&#21270;&#22823;&#37327;&#30340;&#26032;&#38395;&#27969;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#22320;&#21457;&#29616;&#23454;&#26102;&#30456;&#20851;&#26032;&#38395;&#25991;&#31456;&#25925;&#20107;&#65292;&#26377;&#21161;&#20110;&#20154;&#20204;&#22312;&#19981;&#38656;&#35201;&#26114;&#36149;&#20154;&#24037;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#28040;&#21270;&#22823;&#37327;&#30340;&#26032;&#38395;&#27969;&#12290;&#29616;&#26377;&#30340;&#26080;&#30417;&#30563;&#22312;&#32447;&#25925;&#20107;&#21457;&#29616;&#30740;&#31350;&#30340;&#26222;&#36941;&#26041;&#27861;&#26159;&#29992;&#31526;&#21495;&#25110;&#22522;&#20110;&#22270;&#30340;&#23884;&#20837;&#26469;&#34920;&#31034;&#26032;&#38395;&#25991;&#31456;&#65292;&#24182;&#23558;&#23427;&#20204;&#36880;&#27493;&#32858;&#31867;&#25104;&#25925;&#20107;&#12290;&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26377;&#26395;&#36827;&#19968;&#27493;&#25913;&#21892;&#23884;&#20837;&#65292;&#20294;&#26159;&#36890;&#36807;&#26080;&#24046;&#21035;&#22320;&#32534;&#30721;&#25991;&#31456;&#20013;&#30340;&#25152;&#26377;&#20449;&#24687;&#26469;&#30452;&#25509;&#37319;&#29992;&#36825;&#20123;&#27169;&#22411;&#26080;&#27861;&#26377;&#25928;&#22788;&#29702;&#23500;&#21547;&#25991;&#26412;&#19988;&#19981;&#26029;&#21457;&#23637;&#30340;&#26032;&#38395;&#27969;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20027;&#39064;&#23884;&#20837;&#26041;&#27861;&#65292;&#20351;&#29992;&#29616;&#25104;&#30340;&#39044;&#35757;&#32451;&#21477;&#23376;&#32534;&#30721;&#22120;&#26469;&#21160;&#24577;&#34920;&#31034;&#25991;&#31456;&#21644;&#25925;&#20107;&#65292;&#24182;&#32771;&#34385;&#23427;&#20204;&#20849;&#20139;&#30340;&#26102;&#38388;&#20027;&#39064;&#12290;&#20026;&#20102;&#23454;&#29616;&#26080;&#30417;&#30563;&#22312;&#32447;&#25925;&#20107;&#21457;&#29616;&#30340;&#24819;&#27861;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#26694;&#26550;USTORY&#65292;&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#25216;&#26415;&#65292;&#21363;&#20027;&#39064;&#21644;&#26102;&#38388;&#24863;&#30693;&#30340;&#21160;&#24577;&#23884;&#20837;&#21644;&#26032;&#39062;&#24615;&#24863;&#30693;&#30340;&#33258;&#36866;&#24212;&#32858;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised discovery of stories with correlated news articles in real-time helps people digest massive news streams without expensive human annotations. A common approach of the existing studies for unsupervised online story discovery is to represent news articles with symbolic- or graph-based embedding and incrementally cluster them into stories. Recent large language models are expected to improve the embedding further, but a straightforward adoption of the models by indiscriminately encoding all information in articles is ineffective to deal with text-rich and evolving news streams. In this work, we propose a novel thematic embedding with an off-the-shelf pretrained sentence encoder to dynamically represent articles and stories by considering their shared temporal themes. To realize the idea for unsupervised online story discovery, a scalable framework USTORY is introduced with two main techniques, theme- and time-aware dynamic embedding and novelty-aware adaptive clustering, fuel
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#33258;&#21160;&#26631;&#24341;&#31639;&#27861;&#19981;&#23436;&#32654;&#24615;&#23545;&#35821;&#20041;&#25628;&#32034;&#30340;&#24433;&#21709;&#65292;&#22312;&#20581;&#24247;&#39046;&#22495;&#30340;&#23454;&#39564;&#20013;&#21457;&#29616;&#20351;&#29992;&#27010;&#29575;&#36923;&#36753;&#33021;&#26356;&#22909;&#22320;&#22788;&#29702;&#38169;&#35823;&#20998;&#37197;&#30340;&#26415;&#35821;&#65292;&#24182;&#19988;&#28151;&#21512;&#23569;&#37327;&#20154;&#24037;&#26631;&#24341;&#21644;&#33258;&#21160;&#26631;&#24341;&#21487;&#20197;&#24674;&#22797;&#20986;&#33394;&#30340;&#26597;&#35810;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.04057</link><description>&lt;p&gt;
&#19981;&#23436;&#32654;&#30340;&#33258;&#21160;&#26631;&#24341;&#23545;&#35821;&#20041;&#25628;&#32034;&#24615;&#33021;&#30340;&#24433;&#21709;&#26159;&#22914;&#20309;&#30340;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Does Imperfect Automatic Indexing Affect Semantic Search Performance?. (arXiv:2304.04057v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04057
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#33258;&#21160;&#26631;&#24341;&#31639;&#27861;&#19981;&#23436;&#32654;&#24615;&#23545;&#35821;&#20041;&#25628;&#32034;&#30340;&#24433;&#21709;&#65292;&#22312;&#20581;&#24247;&#39046;&#22495;&#30340;&#23454;&#39564;&#20013;&#21457;&#29616;&#20351;&#29992;&#27010;&#29575;&#36923;&#36753;&#33021;&#26356;&#22909;&#22320;&#22788;&#29702;&#38169;&#35823;&#20998;&#37197;&#30340;&#26415;&#35821;&#65292;&#24182;&#19988;&#28151;&#21512;&#23569;&#37327;&#20154;&#24037;&#26631;&#24341;&#21644;&#33258;&#21160;&#26631;&#24341;&#21487;&#20197;&#24674;&#22797;&#20986;&#33394;&#30340;&#26597;&#35810;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20581;&#24247;&#39046;&#22495;&#30340;&#25991;&#29486;&#36890;&#24120;&#20250;&#20351;&#29992;&#21463;&#25511;&#35789;&#27719;&#30340;&#35821;&#20041;&#27010;&#24565;&#65288;&#21363;&#26415;&#35821;&#65289;&#36827;&#34892;&#27880;&#37322;&#12290;&#38543;&#30528;&#36825;&#20123;&#25991;&#29486;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#27880;&#37322;&#24037;&#20316;&#36234;&#26469;&#36234;&#22810;&#22320;&#30001;&#31639;&#27861;&#23436;&#25104;&#12290;&#19982;&#20154;&#31867;&#30456;&#27604;&#65292;&#33258;&#21160;&#26631;&#24341;&#31639;&#27861;&#23384;&#22312;&#19981;&#23436;&#32654;&#24615;&#65292;&#21487;&#33021;&#20250;&#38169;&#35823;&#22320;&#23558;&#26415;&#35821;&#20998;&#37197;&#32473;&#25991;&#29486;&#65292;&#20174;&#32780;&#24433;&#21709;&#21253;&#21547;&#36825;&#20123;&#26415;&#35821;&#30340;&#26597;&#35810;&#30340;&#21518;&#32493;&#25628;&#32034;&#20219;&#21153;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#29702;&#35299;&#22312;&#24067;&#23572;&#35821;&#20041;&#25628;&#32034;&#20013;&#20351;&#29992;&#19981;&#23436;&#32654;&#20998;&#37197;&#26415;&#35821;&#30340;&#24615;&#33021;&#24433;&#21709;&#12290;&#25105;&#20204;&#20351;&#29992;MeSH&#26415;&#35821;&#21644;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#25628;&#32034;&#20316;&#20026;&#26696;&#20363;&#30740;&#31350;&#12290;&#25105;&#20204;&#22312;&#30001;MeSH&#26415;&#35821;&#32452;&#25104;&#30340;&#30495;&#23454;&#24067;&#23572;&#26597;&#35810;&#19978;&#23454;&#29616;&#20102;&#22810;&#20010;&#33258;&#21160;&#26631;&#24341;&#31639;&#27861;&#65292;&#24182;&#21457;&#29616;&#65288;1&#65289;&#27010;&#29575;&#36923;&#36753;&#27604;&#20256;&#32479;&#30340;&#24067;&#23572;&#36923;&#36753;&#26356;&#33021;&#22788;&#29702;&#38169;&#35823;&#20998;&#37197;&#30340;&#26415;&#35821;&#65292;&#65288;2&#65289;&#26597;&#35810;&#32423;&#24615;&#33021;&#20027;&#35201;&#21463;&#26597;&#35810;&#20013;&#24615;&#33021;&#26368;&#20302;&#30340;&#26415;&#35821;&#38480;&#21046;&#65292;&#20197;&#21450;&#65288;3&#65289;&#23558;&#23569;&#37327;&#20154;&#24037;&#26631;&#24341;&#19982;&#33258;&#21160;&#26631;&#24341;&#28151;&#21512;&#20351;&#29992;&#21487;&#20197;&#24674;&#22797;&#20986;&#33394;&#30340;&#26597;&#35810;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Documents in the health domain are often annotated with semantic concepts (i.e., terms) from controlled vocabularies. As the volume of these documents gets large, the annotation work is increasingly done by algorithms. Compared to humans, automatic indexing algorithms are imperfect and may assign wrong terms to documents, which affect subsequent search tasks where queries contain these terms. In this work, we aim to understand the performance impact of using imperfectly assigned terms in Boolean semantic searches. We used MeSH terms and biomedical literature search as a case study. We implemented multiple automatic indexing algorithms on real-world Boolean queries that consist of MeSH terms, and found that (1) probabilistic logic can handle inaccurately assigned terms better than traditional Boolean logic, (2) query-level performance is mostly limited by lowest-performing terms in a query, and (3) mixing a small amount of human indexing with automatic indexing can regain excellent quer
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#23545;&#20110;SemEval-2023&#30340;&#20219;&#21153;9&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;transformer&#30340;&#31995;&#32479;&#65292;&#20351;&#29992;&#20102;&#38598;&#25104;&#23398;&#20064;&#65292;&#22312;&#22810;&#35821;&#35328;&#25512;&#29305;&#20146;&#23494;&#24230;&#26816;&#27979;&#20013;&#25490;&#21517;&#31532;4&#65292;&#36798;&#21040;&#20102;0.5688&#30340;&#23439;&#24179;&#22343;F1&#20998;&#25968;&#12290;&#20026;&#20102;&#25552;&#39640;&#23545;&#26410;&#35265;&#35821;&#35328;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#27599;&#20010;&#25512;&#29305;&#37117;&#36827;&#34892;&#20102;&#33521;&#25991;&#32763;&#35793;&#30340;&#34917;&#20805;&#12290;</title><link>http://arxiv.org/abs/2304.04054</link><description>&lt;p&gt;
tmn&#22312;SemEval-2023&#20219;&#21153;9&#20013;&#30340;&#24212;&#29992;&#65306;&#20351;&#29992;XLM-T&#12289;Google&#32763;&#35793;&#21644;&#38598;&#25104;&#23398;&#20064;&#36827;&#34892;&#22810;&#35821;&#35328;&#25512;&#29305;&#20146;&#23494;&#24230;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
tmn at SemEval-2023 Task 9: Multilingual Tweet Intimacy Detection using XLM-T, Google Translate, and Ensemble Learning. (arXiv:2304.04054v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04054
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#23545;&#20110;SemEval-2023&#30340;&#20219;&#21153;9&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;transformer&#30340;&#31995;&#32479;&#65292;&#20351;&#29992;&#20102;&#38598;&#25104;&#23398;&#20064;&#65292;&#22312;&#22810;&#35821;&#35328;&#25512;&#29305;&#20146;&#23494;&#24230;&#26816;&#27979;&#20013;&#25490;&#21517;&#31532;4&#65292;&#36798;&#21040;&#20102;0.5688&#30340;&#23439;&#24179;&#22343;F1&#20998;&#25968;&#12290;&#20026;&#20102;&#25552;&#39640;&#23545;&#26410;&#35265;&#35821;&#35328;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#27599;&#20010;&#25512;&#29305;&#37117;&#36827;&#34892;&#20102;&#33521;&#25991;&#32763;&#35793;&#30340;&#34917;&#20805;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;transformer&#30340;&#31995;&#32479;&#65292;&#38024;&#23545;SemEval-2023&#20219;&#21153;9&#65306;&#22810;&#35821;&#35328;&#25512;&#29305;&#20146;&#23494;&#24230;&#20998;&#26512;&#36827;&#34892;&#35774;&#35745;&#12290;&#20219;&#21153;&#30340;&#30446;&#30340;&#26159;&#39044;&#27979;&#19968;&#31995;&#21015;&#25512;&#29305;&#30340;&#20146;&#23494;&#24230;&#65292;&#33539;&#22260;&#20174;1&#65288;&#23436;&#20840;&#19981;&#20146;&#23494;&#65289;&#21040;5&#65288;&#38750;&#24120;&#20146;&#23494;&#65289;&#12290;&#27604;&#36187;&#30340;&#23448;&#26041;&#35757;&#32451;&#38598;&#21253;&#21547;&#20845;&#31181;&#35821;&#35328;&#30340;&#25512;&#29305;&#65288;&#33521;&#35821;&#12289;&#35199;&#29677;&#29273;&#35821;&#12289;&#24847;&#22823;&#21033;&#35821;&#12289;&#33889;&#33796;&#29273;&#35821;&#12289;&#27861;&#35821;&#21644;&#20013;&#25991;&#65289;&#12290;&#27979;&#35797;&#38598;&#21253;&#25324;&#20845;&#31181;&#32473;&#23450;&#30340;&#35821;&#35328;&#20197;&#21450;&#22806;&#37096;&#25968;&#25454;&#65292;&#20854;&#20013;&#21253;&#25324;&#35757;&#32451;&#38598;&#20013;&#26410;&#20986;&#29616;&#30340;&#22235;&#31181;&#35821;&#35328;&#65288;&#21360;&#22320;&#35821;&#12289;&#38463;&#25289;&#20271;&#35821;&#12289;&#33655;&#20848;&#35821;&#21644;&#38889;&#35821;&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;XLM-T&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21363;&#36866;&#29992;&#20110;Twitter&#39046;&#22495;&#30340;&#22810;&#35821;&#31181;RoBERTa&#27169;&#22411;&#30340;&#38598;&#25104;&#12290;&#20026;&#20102;&#25552;&#39640;&#23545;&#26410;&#35265;&#35821;&#35328;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#25105;&#20204;&#23545;&#27599;&#26465;&#25512;&#29305;&#36827;&#34892;&#20102;&#33521;&#25991;&#32763;&#35793;&#30340;&#34917;&#20805;&#12290;&#25105;&#20204;&#25506;&#31350;&#20102;&#23558;&#32763;&#35793;&#25968;&#25454;&#24212;&#29992;&#20110;&#24494;&#35843;&#20013;&#30475;&#21040;&#30340;&#35821;&#35328;&#19982;&#26410;&#30475;&#21040;&#30340;&#35821;&#35328;&#30340;transformer&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#20272;&#35745;&#20351;&#29992;&#32763;&#35793;&#25968;&#25454;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#22312;50&#20010;&#22242;&#38431;&#20013;&#25490;&#21517;&#31532;4&#65292;&#24182;&#23454;&#29616;&#20102;0.5688&#30340;&#23439;&#24179;&#22343;F1&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
The paper describes a transformer-based system designed for SemEval-2023 Task 9: Multilingual Tweet Intimacy Analysis. The purpose of the task was to predict the intimacy of tweets in a range from 1 (not intimate at all) to 5 (very intimate). The official training set for the competition consisted of tweets in six languages (English, Spanish, Italian, Portuguese, French, and Chinese). The test set included the given six languages as well as external data with four languages not presented in the training set (Hindi, Arabic, Dutch, and Korean). We presented a solution based on an ensemble of XLM-T, a multilingual RoBERTa model adapted to the Twitter domain. To improve the performance of unseen languages, each tweet was supplemented by its English translation. We explored the effectiveness of translated data for the languages seen in fine-tuning compared to unseen languages and estimated strategies for using translated data in transformer-based models. Our solution ranked 4th on the leade
&lt;/p&gt;</description></item><item><title>DREAM&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#65292;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#29992;&#20110;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#65292;&#33021;&#22815;&#39044;&#27979;&#26410;&#26469;&#30340;&#32570;&#22833;&#20803;&#32032;&#21644;&#29702;&#35299;&#25512;&#29702;&#36335;&#24452;&#12290;</title><link>http://arxiv.org/abs/2304.03984</link><description>&lt;p&gt;
DREAM: &#33258;&#36866;&#24212;&#27880;&#24847;&#21147;&#26426;&#21046;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
DREAM: Adaptive Reinforcement Learning based on Attention Mechanism for Temporal Knowledge Graph Reasoning. (arXiv:2304.03984v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03984
&lt;/p&gt;
&lt;p&gt;
DREAM&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#65292;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#29992;&#20110;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#65292;&#33021;&#22815;&#39044;&#27979;&#26410;&#26469;&#30340;&#32570;&#22833;&#20803;&#32032;&#21644;&#29702;&#35299;&#25512;&#29702;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#65288;TKG&#65289;&#27169;&#22411;&#25551;&#32472;&#20102;&#20107;&#20214;&#30340;&#26102;&#38388;&#28436;&#21270;&#65292;&#36817;&#26469;&#22791;&#21463;&#20851;&#27880;&#12290;&#30001;&#20110;TKG&#22266;&#26377;&#30340;&#19981;&#23436;&#22791;&#24615;&#65292;&#38656;&#35201;&#25512;&#29702;&#20986;&#32570;&#22833;&#30340;&#20803;&#32032;&#12290;&#34429;&#28982;&#29616;&#26377;&#30340;TKG&#25512;&#29702;&#26041;&#27861;&#33021;&#22815;&#39044;&#27979;&#32570;&#22833;&#30340;&#26410;&#26469;&#20107;&#20214;&#65292;&#20294;&#26159;&#32570;&#20047;&#26174;&#24335;&#30340;&#25512;&#29702;&#36335;&#24452;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#30001;&#20110;&#20256;&#32479;&#30693;&#35782;&#22270;&#35889;&#19978;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#22810;&#36339;&#25512;&#29702;&#22312;&#26368;&#36817;&#30340;&#36827;&#23637;&#20013;&#26174;&#31034;&#20986;&#20248;&#36234;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#24615;&#33021;&#65292;&#22240;&#27492;&#22312;TKG&#25512;&#29702;&#19978;&#25506;&#32034;RL&#25216;&#26415;&#30340;&#26426;&#20250;&#24050;&#32463;&#24320;&#21551;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;RL&#30340;TKG&#25512;&#29702;&#26041;&#27861;&#30340;&#24615;&#33021;&#21463;&#21040;&#20197;&#19979;&#38480;&#21046;&#65306;&#65288;1&#65289;&#32570;&#20047;&#21516;&#26102;&#25429;&#25417;&#26102;&#38388;&#28436;&#21270;&#21644;&#35821;&#20041;&#20381;&#36182;&#30340;&#33021;&#21147;&#65307;&#65288;2&#65289;&#36807;&#24230;&#20381;&#36182;&#25163;&#21160;&#35774;&#35745;&#30340;&#22870;&#21169;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#33258;&#36866;&#24212;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#65288;DREAM&#65289;&#26469;&#39044;&#27979;&#26410;&#26469;&#30340;&#32570;&#22833;&#20803;&#32032;&#12290;&#20855;&#20307;&#22320;&#35828;&#65292;
&lt;/p&gt;
&lt;p&gt;
Temporal knowledge graphs (TKGs) model the temporal evolution of events and have recently attracted increasing attention. Since TKGs are intrinsically incomplete, it is necessary to reason out missing elements. Although existing TKG reasoning methods have the ability to predict missing future events, they fail to generate explicit reasoning paths and lack explainability. As reinforcement learning (RL) for multi-hop reasoning on traditional knowledge graphs starts showing superior explainability and performance in recent advances, it has opened up opportunities for exploring RL techniques on TKG reasoning. However, the performance of RL-based TKG reasoning methods is limited due to: (1) lack of ability to capture temporal evolution and semantic dependence jointly; (2) excessive reliance on manually designed rewards. To overcome these challenges, we propose an adaptive reinforcement learning model based on attention mechanism (DREAM) to predict missing elements in the future. Specificall
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35774;&#22791;&#31471;&#30340;&#12289;&#27169;&#22411;&#26080;&#20851;&#30340;&#21435;&#20013;&#24515;&#21270;&#21327;&#20316;&#23398;&#20064;&#30340;POI&#25512;&#33616;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#26500;&#24863;&#30693;&#21327;&#21516;&#21098;&#26525;&#21644;&#27169;&#22411;&#26080;&#20851;&#21435;&#20013;&#24515;&#21270;&#21327;&#21516;&#20248;&#21270;&#20004;&#31181;&#36731;&#37327;&#32423;&#31639;&#27861;&#65292;&#20445;&#35777;&#20102;&#22312;&#21435;&#20013;&#24515;&#21270;&#29615;&#22659;&#20013;&#39640;&#25928;&#22320;&#35757;&#32451;&#20010;&#24615;&#21270;POI&#25512;&#33616;&#22120;&#65292;&#21516;&#26102;&#19981;&#27844;&#38706;&#29992;&#25143;&#30340;&#25935;&#24863;&#25968;&#25454;&#24182;&#20445;&#25345;&#25512;&#33616;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2304.03947</link><description>&lt;p&gt;
&#22522;&#20110;&#27169;&#22411;&#26080;&#20851;&#30340;&#21435;&#20013;&#24515;&#21270;&#21327;&#20316;&#23398;&#20064;&#30340;&#35774;&#22791;&#31471;POI&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Model-Agnostic Decentralized Collaborative Learning for On-Device POI Recommendation. (arXiv:2304.03947v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03947
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35774;&#22791;&#31471;&#30340;&#12289;&#27169;&#22411;&#26080;&#20851;&#30340;&#21435;&#20013;&#24515;&#21270;&#21327;&#20316;&#23398;&#20064;&#30340;POI&#25512;&#33616;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#26500;&#24863;&#30693;&#21327;&#21516;&#21098;&#26525;&#21644;&#27169;&#22411;&#26080;&#20851;&#21435;&#20013;&#24515;&#21270;&#21327;&#21516;&#20248;&#21270;&#20004;&#31181;&#36731;&#37327;&#32423;&#31639;&#27861;&#65292;&#20445;&#35777;&#20102;&#22312;&#21435;&#20013;&#24515;&#21270;&#29615;&#22659;&#20013;&#39640;&#25928;&#22320;&#35757;&#32451;&#20010;&#24615;&#21270;POI&#25512;&#33616;&#22120;&#65292;&#21516;&#26102;&#19981;&#27844;&#38706;&#29992;&#25143;&#30340;&#25935;&#24863;&#25968;&#25454;&#24182;&#20445;&#25345;&#25512;&#33616;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#22522;&#20110;&#20301;&#32622;&#30340;&#31038;&#20132;&#32593;&#32476;&#20013;&#19981;&#21487;&#25110;&#32570;&#30340;&#20010;&#24615;&#21270;&#26381;&#21153;&#65292;&#19979;&#19968;&#20010;&#20852;&#36259;&#28857;&#65288;POI&#65289;&#30340;&#25512;&#33616;&#26088;&#22312;&#24110;&#21161;&#20154;&#20204;&#21457;&#29616;&#26377;&#21560;&#24341;&#21147;&#21644;&#26377;&#36259;&#30340;&#22320;&#26041;&#12290;&#30446;&#21069;&#65292;&#22823;&#22810;&#25968;POI&#25512;&#33616;&#31995;&#32479;&#22522;&#20110;&#20256;&#32479;&#30340;&#38598;&#20013;&#24335;&#33539;&#24335;&#65292;&#20005;&#37325;&#20381;&#36182;&#20113;&#31471;&#20351;&#29992;&#22823;&#37327;&#25910;&#38598;&#30340;&#29992;&#25143;&#25935;&#24863;&#31614;&#21040;&#25968;&#25454;&#26469;&#35757;&#32451;&#25512;&#33616;&#27169;&#22411;&#12290;&#23613;&#31649;&#26368;&#36817;&#19968;&#20123;&#24037;&#20316;&#25506;&#32034;&#20102;&#35774;&#22791;&#19978;&#30340;&#24377;&#24615;&#21644;&#38544;&#31169;&#20445;&#25252;POI&#25512;&#33616;&#26694;&#26550;&#65292;&#20294;&#23427;&#20204;&#26080;&#19968;&#20363;&#22806;&#22320;&#20551;&#35774;&#20102;&#27169;&#22411;&#22312;&#21442;&#25968;/&#26799;&#24230;&#32858;&#21512;&#21644;&#21327;&#20316;&#26041;&#38754;&#30340;&#21516;&#36136;&#24615;&#12290;&#28982;&#32780;&#65292;&#23454;&#38469;&#24773;&#20917;&#19979;&#29992;&#25143;&#30340;&#31227;&#21160;&#35774;&#22791;&#20855;&#26377;&#21508;&#31181;&#19981;&#21516;&#30340;&#30828;&#20214;&#37197;&#32622;&#65288;&#20363;&#22914;&#35745;&#31639;&#36164;&#28304;&#65289;&#65292;&#23548;&#33268;&#19981;&#21516;&#26550;&#26500;&#21644;&#22823;&#23567;&#30340;&#24322;&#26500;&#35774;&#22791;&#31471;&#27169;&#22411;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35774;&#22791;&#31471;POI&#25512;&#33616;&#26694;&#26550;&#65292;&#21363;&#22522;&#20110;&#27169;&#22411;&#26080;&#20851;&#30340;&#21435;&#20013;&#24515;&#21270;&#21327;&#20316;&#23398;&#20064;&#30340;&#35774;&#22791;&#31471;POI&#25512;&#33616;&#65288;MACDL-POI&#65289;&#65292;&#36890;&#36807;&#20849;&#21516;&#35774;&#35745;&#20004;&#20010;&#36731;&#37327;&#32423;&#31639;&#27861;&#23454;&#29616;&#20102;&#35774;&#22791;&#31471;&#21327;&#20316;&#23398;&#20064;&#65292;&#21363;&#27169;&#22411;&#26080;&#20851;&#21435;&#20013;&#24515;&#21270;&#21327;&#20316;&#20248;&#21270;&#21644;&#32467;&#26500;&#24863;&#30693;&#21327;&#21516;&#21098;&#26525;&#12290;&#36890;&#36807;&#38598;&#25104;&#36825;&#20004;&#20010;&#31639;&#27861;&#65292;MACDL-POI&#33021;&#22815;&#20197;&#21435;&#20013;&#24515;&#21270;&#30340;&#26041;&#24335;&#39640;&#25928;&#22320;&#35757;&#32451;&#35774;&#22791;&#31471;&#30340;&#20010;&#24615;&#21270;POI&#25512;&#33616;&#22120;&#65292;&#32780;&#19981;&#27844;&#38706;&#29992;&#25143;&#30340;&#25935;&#24863;&#25968;&#25454;&#24182;&#20445;&#25345;&#25512;&#33616;&#36136;&#37327;&#12290;&#23545;&#30495;&#23454;&#19990;&#30028;LBSN&#25968;&#25454;&#38598;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;MACDL-POI&#22312;&#25512;&#33616;&#31934;&#24230;&#21644;&#38544;&#31169;&#20445;&#25252;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#38598;&#20013;&#24335;&#21644;&#21435;&#20013;&#24515;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
As an indispensable personalized service in Location-based Social Networks (LBSNs), the next Point-of-Interest (POI) recommendation aims to help people discover attractive and interesting places. Currently, most POI recommenders are based on the conventional centralized paradigm that heavily relies on the cloud to train the recommendation models with large volumes of collected users' sensitive check-in data. Although a few recent works have explored on-device frameworks for resilient and privacy-preserving POI recommendations, they invariably hold the assumption of model homogeneity for parameters/gradients aggregation and collaboration. However, users' mobile devices in the real world have various hardware configurations (e.g., compute resources), leading to heterogeneous on-device models with different architectures and sizes. In light of this, We propose a novel on-device POI recommendation framework, namely Model-Agnostic Collaborative learning for on-device POI recommendation (MAC
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;C^2DSR&#26469;&#35299;&#20915;&#20132;&#21449;&#39046;&#22495;&#24207;&#21015;&#25512;&#33616;&#20013;&#20851;&#20110;&#29992;&#25143;&#20559;&#22909;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#21516;&#26102;&#21033;&#29992;&#24207;&#21015;&#20869;&#21644;&#24207;&#21015;&#38388;&#29289;&#21697;&#20851;&#31995;&#24182;&#20849;&#21516;&#23398;&#20064;&#21333;&#39046;&#22495;&#21644;&#36328;&#39046;&#22495;&#29992;&#25143;&#20559;&#22909;&#65292;&#20197;&#21450;&#20351;&#29992;&#23545;&#27604;&#20132;&#21449;&#27880;&#24847;&#26426;&#21046;&#26469;&#33719;&#21462;&#36328;&#39046;&#22495;&#20559;&#22909;&#12290;&#22312;&#19977;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.03891</link><description>&lt;p&gt;
&#23545;&#27604;&#20132;&#21449;&#39046;&#22495;&#24207;&#21015;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Contrastive Cross-Domain Sequential Recommendation. (arXiv:2304.03891v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03891
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;C^2DSR&#26469;&#35299;&#20915;&#20132;&#21449;&#39046;&#22495;&#24207;&#21015;&#25512;&#33616;&#20013;&#20851;&#20110;&#29992;&#25143;&#20559;&#22909;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#21516;&#26102;&#21033;&#29992;&#24207;&#21015;&#20869;&#21644;&#24207;&#21015;&#38388;&#29289;&#21697;&#20851;&#31995;&#24182;&#20849;&#21516;&#23398;&#20064;&#21333;&#39046;&#22495;&#21644;&#36328;&#39046;&#22495;&#29992;&#25143;&#20559;&#22909;&#65292;&#20197;&#21450;&#20351;&#29992;&#23545;&#27604;&#20132;&#21449;&#27880;&#24847;&#26426;&#21046;&#26469;&#33719;&#21462;&#36328;&#39046;&#22495;&#20559;&#22909;&#12290;&#22312;&#19977;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#21449;&#39046;&#22495;&#24207;&#21015;&#25512;&#33616;&#65288;CDSR&#65289;&#26088;&#22312;&#22522;&#20110;&#22810;&#20010;&#39046;&#22495;&#20013;&#29992;&#25143;&#30340;&#21382;&#21490;&#24207;&#21015;&#20114;&#21160;&#26469;&#39044;&#27979;&#26410;&#26469;&#30340;&#20114;&#21160;&#12290;&#28982;&#32780;&#65292;CDSR&#30340;&#20851;&#38190;&#25361;&#25112;&#22312;&#20110;&#22914;&#20309;&#22522;&#20110;&#24207;&#21015;&#20869;&#21644;&#24207;&#21015;&#38388;&#29289;&#21697;&#20114;&#21160;&#25366;&#25496;&#31934;&#30830;&#30340;&#36328;&#39046;&#22495;&#29992;&#25143;&#20559;&#22909;&#12290;&#29616;&#26377;&#26041;&#27861;&#39318;&#20808;&#20165;&#21033;&#29992;&#24207;&#21015;&#20869;&#29289;&#21697;&#20114;&#21160;&#23398;&#20064;&#21333;&#39046;&#22495;&#29992;&#25143;&#20559;&#22909;&#65292;&#28982;&#21518;&#26500;&#24314;&#20256;&#36882;&#27169;&#22359;&#20197;&#33719;&#21462;&#36328;&#39046;&#22495;&#29992;&#25143;&#20559;&#22909;&#12290;&#35813;&#26041;&#27861;&#23384;&#22312;&#19968;&#20123;&#29942;&#39048;&#65292;&#24182;&#24573;&#30053;&#20102;&#32771;&#34385;&#24207;&#21015;&#38388;&#29289;&#21697;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;C^2DSR&#26469;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#20197;&#25429;&#25417;&#31934;&#30830;&#30340;&#29992;&#25143;&#20559;&#22909;&#12290;&#20854;&#20027;&#35201;&#24605;&#24819;&#26159;&#21516;&#26102;&#21033;&#29992;&#24207;&#21015;&#20869;&#21644;&#24207;&#21015;&#38388;&#29289;&#21697;&#20851;&#31995;&#65292;&#24182;&#20849;&#21516;&#23398;&#20064;&#21333;&#39046;&#22495;&#21644;&#36328;&#39046;&#22495;&#29992;&#25143;&#20559;&#22909;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#25366;&#25496;&#36328;&#39046;&#22495;&#29289;&#21697;&#20851;&#31995;&#65292;&#28982;&#21518;&#20351;&#29992;&#24207;&#21015;&#27169;&#22411;&#25429;&#33719;&#21333;&#39046;&#22495;&#29289;&#21697;&#20851;&#31995;&#12290;&#32467;&#21512;&#25152;&#23398;&#30340;&#20559;&#22909;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#27604;&#20132;&#21449;&#27880;&#24847;&#26426;&#21046;&#20197;&#33719;&#21462;&#36328;&#39046;&#22495;&#20559;&#22909;&#12290;&#22312;&#19977;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-Domain Sequential Recommendation (CDSR) aims to predict future interactions based on user's historical sequential interactions from multiple domains. Generally, a key challenge of CDSR is how to mine precise cross-domain user preference based on the intra-sequence and inter-sequence item interactions. Existing works first learn single-domain user preference only with intra-sequence item interactions, and then build a transferring module to obtain cross-domain user preference. However, such a pipeline and implicit solution can be severely limited by the bottleneck of the designed transferring module, and ignores to consider inter-sequence item relationships. In this paper, we propose C^2DSR to tackle the above problems to capture precise user preferences. The main idea is to simultaneously leverage the intra- and intersequence item relationships, and jointly learn the single- and cross- domain user preferences. Specifically, we first utilize a graph neural network to mine inter-
&lt;/p&gt;</description></item><item><title>GPT4Rec&#26159;&#19968;&#20010;&#29983;&#25104;&#24335;&#26694;&#26550;&#65292;&#29992;&#20110;&#20010;&#24615;&#21270;&#25512;&#33616;&#21644;&#29992;&#25143;&#20852;&#36259;&#35299;&#37322;&#65292;&#23427;&#36890;&#36807;&#29983;&#25104;&#8220;&#25628;&#32034;&#26597;&#35810;&#8221;&#26469;&#20805;&#20998;&#21033;&#29992;&#29289;&#21697;&#20869;&#23481;&#20449;&#24687;&#21644;&#35821;&#35328;&#24314;&#27169;&#33021;&#21147;&#65292;&#24182;&#20351;&#29992;&#23618;&#27425;&#28508;&#22312;&#21464;&#37327;&#27169;&#22411;&#26469;&#25552;&#39640;&#20010;&#24615;&#21270;&#25512;&#33616;&#30340;&#30456;&#20851;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.03879</link><description>&lt;p&gt;
GPT4Rec&#65306;&#19968;&#20010;&#29992;&#20110;&#20010;&#24615;&#21270;&#25512;&#33616;&#21644;&#29992;&#25143;&#20852;&#36259;&#35299;&#37322;&#30340;&#29983;&#25104;&#24335;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
GPT4Rec: A Generative Framework for Personalized Recommendation and User Interests Interpretation. (arXiv:2304.03879v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03879
&lt;/p&gt;
&lt;p&gt;
GPT4Rec&#26159;&#19968;&#20010;&#29983;&#25104;&#24335;&#26694;&#26550;&#65292;&#29992;&#20110;&#20010;&#24615;&#21270;&#25512;&#33616;&#21644;&#29992;&#25143;&#20852;&#36259;&#35299;&#37322;&#65292;&#23427;&#36890;&#36807;&#29983;&#25104;&#8220;&#25628;&#32034;&#26597;&#35810;&#8221;&#26469;&#20805;&#20998;&#21033;&#29992;&#29289;&#21697;&#20869;&#23481;&#20449;&#24687;&#21644;&#35821;&#35328;&#24314;&#27169;&#33021;&#21147;&#65292;&#24182;&#20351;&#29992;&#23618;&#27425;&#28508;&#22312;&#21464;&#37327;&#27169;&#22411;&#26469;&#25552;&#39640;&#20010;&#24615;&#21270;&#25512;&#33616;&#30340;&#30456;&#20851;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#30340;&#26368;&#26032;&#36827;&#23637;&#24050;&#32463;&#23548;&#33268;&#20102;NLP&#22522;&#20110;&#25512;&#33616;&#31995;&#32479;&#30340;&#21457;&#23637;&#65292;&#24050;&#32463;&#34920;&#29616;&#20986;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#27169;&#22411;&#36890;&#24120;&#23558;&#29289;&#21697;&#20165;&#35270;&#20026;ID&#24182;&#37319;&#29992;&#21028;&#21035;&#24615;&#24314;&#27169;&#65292;&#23548;&#33268;&#20102;&#20197;&#19979;&#38480;&#21046;:(1)&#26080;&#27861;&#20805;&#20998;&#21033;&#29992;&#29289;&#21697;&#20869;&#23481;&#20449;&#24687;&#21644;NLP&#27169;&#22411;&#30340;&#35821;&#35328;&#24314;&#27169;&#33021;&#21147;;(2)&#29702;&#35299;&#29992;&#25143;&#20852;&#36259;&#20197;&#25552;&#39640;&#30456;&#20851;&#24615;&#21644;&#22810;&#26679;&#24615;;(3)&#36866;&#24212;&#19981;&#26029;&#22686;&#38271;&#30340;&#29289;&#21697;&#24211;&#23384;&#30340;&#23454;&#38469;&#24773;&#20917;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GPT4Rec&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#28789;&#27963;&#30340;&#29983;&#25104;&#26694;&#26550;&#65292;&#21551;&#21457;&#33258;&#25628;&#32034;&#24341;&#25806;&#12290; &#23427;&#39318;&#20808;&#29983;&#25104;&#22522;&#20110;&#29992;&#25143;&#21382;&#21490;&#20013;&#29289;&#21697;&#26631;&#39064;&#30340;&#20551;&#35774;&#8220;&#25628;&#32034;&#26597;&#35810;&#8221;&#65292;&#28982;&#21518;&#36890;&#36807;&#25628;&#32034;&#36825;&#20123;&#26597;&#35810;&#26469;&#26816;&#32034;&#25512;&#33616;&#39033;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#22312;&#35821;&#35328;&#31354;&#38388;&#20013;&#23398;&#20064;&#29992;&#25143;&#21644;&#29289;&#21697;&#23884;&#20837;&#26469;&#20811;&#26381;&#20197;&#21069;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#24456;&#22909;&#22320;&#25429;&#25417;&#29992;&#25143;&#21033;&#30410;&#30340;&#19981;&#21516;&#26041;&#38754;&#21644;&#31890;&#24230;&#20197;&#25913;&#36827;&#20010;&#24615;&#21270;&#25512;&#33616;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#23618;&#27425;&#28508;&#22312;&#21464;&#37327;&#27169;&#22411;&#26469;&#35299;&#37322;&#29992;&#25143;&#20852;&#36259;&#12290; GPT4Rec&#36824;&#36890;&#36807;&#20351;&#29992;served&#26399;&#38388;&#30475;&#21040;&#30340;&#39033;&#30446;&#26631;&#39064;&#21160;&#24577;&#25193;&#23637;&#35789;&#27719;&#34920;&#26469;&#36866;&#24212;&#19981;&#26029;&#22686;&#38271;&#30340;&#39033;&#30446;&#24211;&#23384;&#12290; &#22312;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;GPT4Rec&#22312;&#25512;&#33616;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in Natural Language Processing (NLP) have led to the development of NLP-based recommender systems that have shown superior performance. However, current models commonly treat items as mere IDs and adopt discriminative modeling, resulting in limitations of (1) fully leveraging the content information of items and the language modeling capabilities of NLP models; (2) interpreting user interests to improve relevance and diversity; and (3) adapting practical circumstances such as growing item inventories. To address these limitations, we present GPT4Rec, a novel and flexible generative framework inspired by search engines. It first generates hypothetical "search queries" given item titles in a user's history, and then retrieves items for recommendation by searching these queries. The framework overcomes previous limitations by learning both user and item embeddings in the language space. To well-capture user interests with different aspects and granularity for improving
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#22312;&#27809;&#26377;&#36523;&#20221;&#27880;&#37322;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#20154;&#33080;&#35782;&#21035;&#23884;&#20837;&#21521;&#37327;&#20316;&#20026;&#36523;&#20221;&#26631;&#35782;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#20154;&#33080;&#27169;&#22411;&#30340;&#36523;&#20221;&#40065;&#26834;&#24615;&#21644;&#20844;&#24179;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.03838</link><description>&lt;p&gt;
&#25552;&#39640;&#20154;&#33080;&#27169;&#22411;&#30340;&#36523;&#20221;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving Identity-Robustness for Face Models. (arXiv:2304.03838v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03838
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#22312;&#27809;&#26377;&#36523;&#20221;&#27880;&#37322;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#20154;&#33080;&#35782;&#21035;&#23884;&#20837;&#21521;&#37327;&#20316;&#20026;&#36523;&#20221;&#26631;&#35782;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#20154;&#33080;&#27169;&#22411;&#30340;&#36523;&#20221;&#40065;&#26834;&#24615;&#21644;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#20154;&#20204;&#20173;&#28982;&#25285;&#24515;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#23398;&#20064;&#21040;&#24555;&#25463;&#26041;&#24335;&#65292;&#24182;&#19988;&#32570;&#20047;&#23545;&#26080;&#20851;&#28151;&#28102;&#22240;&#32032;&#30340;&#40065;&#26834;&#24615;&#12290;&#22312;&#30452;&#25509;&#35757;&#32451;&#20110;&#20154;&#33080;&#19978;&#30340;&#27169;&#22411;&#20013;&#65292;&#19968;&#20010;&#25935;&#24863;&#30340;&#28151;&#28102;&#22240;&#32032;&#26159;&#20154;&#30340;&#36523;&#20221;&#12290;&#35768;&#22810;&#19982;&#20154;&#33080;&#30456;&#20851;&#30340;&#20219;&#21153;&#29702;&#24819;&#24773;&#20917;&#19979;&#24212;&#35813;&#26159;&#19982;&#36523;&#20221;&#26080;&#20851;&#30340;&#65292;&#24182;&#22312;&#19981;&#21516;&#20010;&#20307;&#20043;&#38388;&#34920;&#29616;&#19968;&#33268;&#65288;&#21363;&#20844;&#24179;&#65289;&#12290;&#36890;&#36807;&#22312;&#35757;&#32451;&#26399;&#38388;&#24378;&#21046;&#25191;&#34892;&#36825;&#31181;&#40065;&#26834;&#24615;&#21644;&#24615;&#33021;&#22343;&#21248;&#24615;&#26159;&#24230;&#37327;&#21644;&#23454;&#26045;&#30340;&#19968;&#31181;&#26041;&#27861;&#65292;&#20551;&#35774;&#21487;&#20197;&#22312;&#35268;&#27169;&#19978;&#33719;&#21462;&#19982;&#36523;&#20221;&#30456;&#20851;&#30340;&#20449;&#24687;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#38544;&#31169;&#38382;&#39064;&#20197;&#21450;&#25910;&#38598;&#27492;&#31867;&#20449;&#24687;&#30340;&#25104;&#26412;&#65292;&#36825;&#36890;&#24120;&#19981;&#26159;&#24773;&#20917;&#65292;&#22823;&#22810;&#25968;&#20154;&#33080;&#25968;&#25454;&#38598;&#21482;&#21253;&#21547;&#36755;&#20837;&#22270;&#20687;&#21450;&#20854;&#30456;&#24212;&#30340;&#20219;&#21153;&#26631;&#31614;&#12290;&#22240;&#27492;&#65292;&#26080;&#38656;&#27492;&#31867;&#27880;&#37322;&#21363;&#21487;&#25552;&#39640;&#36523;&#20221;&#30456;&#20851;&#40065;&#26834;&#24615;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25506;&#35752;&#20351;&#29992;&#20154;&#33080;&#35782;&#21035;&#23884;&#20837;&#21521;&#37327;&#20316;&#20026;&#36523;&#20221;&#26631;&#35782;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#20197;&#25191;&#34892;&#36825;&#31181;&#40065;&#26834;&#24615;&#21644;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the success of deep-learning models in many tasks, there have been concerns about such models learning shortcuts, and their lack of robustness to irrelevant confounders. When it comes to models directly trained on human faces, a sensitive confounder is that of human identities. Many face-related tasks should ideally be identity-independent, and perform uniformly across different individuals (i.e. be fair). One way to measure and enforce such robustness and performance uniformity is through enforcing it during training, assuming identity-related information is available at scale. However, due to privacy concerns and also the cost of collecting such information, this is often not the case, and most face datasets simply contain input images and their corresponding task-related labels. Thus, improving identity-related robustness without the need for such annotations is of great importance. Here, we explore using face-recognition embedding vectors, as proxies for identities, to enfo
&lt;/p&gt;</description></item><item><title>CAPOT&#20351;&#29992;&#21518;&#35757;&#32451;&#23545;&#27604;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#27169;&#22411;&#23545;&#20110;&#22122;&#22768;&#26597;&#35810;&#30340;&#20581;&#22766;&#24615;&#65292;&#34920;&#29616;&#31867;&#20284;&#20110;&#25968;&#25454;&#22686;&#24378;&#20294;&#27809;&#26377;&#20854;&#24320;&#38144;&#12290;</title><link>http://arxiv.org/abs/2304.03401</link><description>&lt;p&gt;
CAPOT: &#20351;&#29992;&#21518;&#35757;&#32451;&#23545;&#27604;&#23545;&#40784;&#21019;&#24314;&#24378;&#20581;&#30340;&#23494;&#38598;&#26597;&#35810;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
CAPOT: Creating Robust Dense Query Encoders using Post Training Contrastive Alignment. (arXiv:2304.03401v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03401
&lt;/p&gt;
&lt;p&gt;
CAPOT&#20351;&#29992;&#21518;&#35757;&#32451;&#23545;&#27604;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#27169;&#22411;&#23545;&#20110;&#22122;&#22768;&#26597;&#35810;&#30340;&#20581;&#22766;&#24615;&#65292;&#34920;&#29616;&#31867;&#20284;&#20110;&#25968;&#25454;&#22686;&#24378;&#20294;&#27809;&#26377;&#20854;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#35789;&#34920;&#31034;&#30340;&#25104;&#21151;&#21644;&#31070;&#32463;&#20449;&#24687;&#26816;&#32034;&#30340;&#36827;&#27493;&#20351;&#24471;&#22522;&#20110;&#23494;&#38598;&#21521;&#37327;&#30340;&#26816;&#32034;&#25104;&#20026;&#27573;&#33853;&#21644;&#25991;&#26723;&#25490;&#21517;&#30340;&#26631;&#20934;&#26041;&#27861;&#12290;&#21452;&#32534;&#30721;&#22120;&#34429;&#28982;&#26377;&#25928;&#21644;&#39640;&#25928;&#65292;&#20294;&#23545;&#26597;&#35810;&#20998;&#24067;&#21644;&#22024;&#26434;&#26597;&#35810;&#21464;&#21270;&#24456;&#33030;&#24369;&#12290;&#25968;&#25454;&#22686;&#24378;&#21487;&#20197;&#20351;&#27169;&#22411;&#26356;&#21152;&#20581;&#22766;&#65292;&#20294;&#20250;&#24341;&#20837;&#35757;&#32451;&#38598;&#29983;&#25104;&#30340;&#24320;&#38144;&#65292;&#24182;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#21644;&#32034;&#24341;&#37325;&#24314;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102; Contrastive Alignment POst Training (CAPOT)&#65292;&#19968;&#31181;&#39640;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#36890;&#36807;&#20923;&#32467;&#25991;&#26723;&#32534;&#30721;&#22120;&#65292;&#35753;&#26597;&#35810;&#32534;&#30721;&#22120;&#23398;&#20064;&#23558;&#22024;&#26434;&#26597;&#35810;&#19982;&#20854;&#26410;&#26356;&#25913;&#30340;&#26681;&#23545;&#40784;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#20581;&#22766;&#24615;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102; CAPOT &#22312; MSMARCO&#12289;&#33258;&#28982;&#38382;&#39064;&#21644; Trivia QA &#27573;&#33853;&#26816;&#32034;&#30340;&#22024;&#26434;&#21464;&#20307;&#19978;&#65292;&#21457;&#29616; CAPOT &#20855;&#26377;&#19982;&#25968;&#25454;&#22686;&#24378;&#31867;&#20284;&#30340;&#24433;&#21709;&#65292;&#20294;&#27809;&#26377;&#23427;&#30340;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;
The success of contextual word representations and advances in neural information retrieval have made dense vector-based retrieval a standard approach for passage and document ranking. While effective and efficient, dual-encoders are brittle to variations in query distributions and noisy queries. Data augmentation can make models more robust but introduces overhead to training set generation and requires retraining and index regeneration. We present Contrastive Alignment POst Training (CAPOT), a highly efficient finetuning method that improves model robustness without requiring index regeneration, the training set optimization, or alteration. CAPOT enables robust retrieval by freezing the document encoder while the query encoder learns to align noisy queries with their unaltered root. We evaluate CAPOT noisy variants of MSMARCO, Natural Questions, and Trivia QA passage retrieval, finding CAPOT has a similar impact as data augmentation with none of its overhead.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DiffuRec &#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#20854;&#23558;&#29289;&#21697;&#34920;&#31034;&#20026;&#20998;&#24067;&#32780;&#19981;&#26159;&#22266;&#23450;&#21521;&#37327;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#21453;&#26144;&#20102;&#29992;&#25143;&#30340;&#22810;&#31181;&#20559;&#22909;&#21644;&#29289;&#21697;&#30340;&#22810;&#20010;&#26041;&#38754;&#65292;&#24182;&#25104;&#21151;&#22320;&#24212;&#29992;&#20110;&#39034;&#24207;&#25512;&#33616;&#12290;</title><link>http://arxiv.org/abs/2304.00686</link><description>&lt;p&gt;
DiffuRec: &#19968;&#31181;&#29992;&#20110;&#39034;&#24207;&#25512;&#33616;&#30340;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DiffuRec: A Diffusion Model for Sequential Recommendation. (arXiv:2304.00686v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00686
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DiffuRec &#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#20854;&#23558;&#29289;&#21697;&#34920;&#31034;&#20026;&#20998;&#24067;&#32780;&#19981;&#26159;&#22266;&#23450;&#21521;&#37327;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#21453;&#26144;&#20102;&#29992;&#25143;&#30340;&#22810;&#31181;&#20559;&#22909;&#21644;&#29289;&#21697;&#30340;&#22810;&#20010;&#26041;&#38754;&#65292;&#24182;&#25104;&#21151;&#22320;&#24212;&#29992;&#20110;&#39034;&#24207;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#20915;&#39034;&#24207;&#25512;&#33616;&#30340;&#20027;&#27969;&#26041;&#27861;&#26159;&#20351;&#29992;&#22266;&#23450;&#21521;&#37327;&#26469;&#34920;&#31034;&#29289;&#21697;&#12290;&#36825;&#20123;&#21521;&#37327;&#22312;&#25429;&#25417;&#29289;&#21697;&#30340;&#28508;&#22312;&#26041;&#38754;&#21644;&#29992;&#25143;&#30340;&#22810;&#26679;&#21270;&#20559;&#22909;&#26041;&#38754;&#26041;&#38754;&#30340;&#33021;&#21147;&#26377;&#38480;&#12290;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#33539;&#24335;&#65292;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#39046;&#22495;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#24615;&#33021;&#12290;&#22312;&#25105;&#20204;&#30475;&#26469;&#65292;&#20854;&#22312;&#34920;&#24449;&#29983;&#25104;&#26041;&#38754;&#30340;&#29420;&#29305;&#20248;&#21183;&#24456;&#22909;&#22320;&#36866;&#24212;&#20102;&#39034;&#24207;&#25512;&#33616;&#30340;&#38382;&#39064;&#35774;&#32622;&#12290;&#26412;&#25991;&#39318;&#27425;&#23581;&#35797;&#23558;&#25193;&#25955;&#27169;&#22411;&#24212;&#29992;&#20110;&#39034;&#24207;&#25512;&#33616;&#65292;&#24182;&#25552;&#20986;&#20102;DiffuRec&#65292;&#29992;&#20110;&#29289;&#21697;&#34920;&#31034;&#26500;&#24314;&#21644;&#19981;&#30830;&#23450;&#24615;&#27880;&#20837;&#12290;&#19982;&#23558;&#29289;&#21697;&#34920;&#31034;&#24314;&#27169;&#20026;&#22266;&#23450;&#21521;&#37327;&#19981;&#21516;&#65292;&#25105;&#20204;&#22312;DiffuRec&#20013;&#23558;&#20854;&#34920;&#31034;&#20026;&#20998;&#24067;&#65292;&#36825;&#21453;&#26144;&#20102;&#29992;&#25143;&#30340;&#22810;&#37325;&#20852;&#36259;&#21644;&#29289;&#21697;&#30340;&#21508;&#20010;&#26041;&#38754;&#30340;&#36866;&#24212;&#24615;&#12290;&#22312;&#25193;&#25955;&#38454;&#27573;&#65292;DiffuRec&#36890;&#36807;&#28155;&#21152;&#22122;&#22768;&#23558;&#30446;&#26631;&#29289;&#21697;&#23884;&#20837;&#25104;&#39640;&#26031;&#20998;&#24067;&#65292;&#36827;&#19968;&#27493;&#24212;&#29992;&#20110;&#39034;&#24207;&#29289;&#21697;&#20998;&#24067;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mainstream solutions to Sequential Recommendation (SR) represent items with fixed vectors. These vectors have limited capability in capturing items' latent aspects and users' diverse preferences. As a new generative paradigm, Diffusion models have achieved excellent performance in areas like computer vision and natural language processing. To our understanding, its unique merit in representation generation well fits the problem setting of sequential recommendation. In this paper, we make the very first attempt to adapt Diffusion model to SR and propose DiffuRec, for item representation construction and uncertainty injection. Rather than modeling item representations as fixed vectors, we represent them as distributions in DiffuRec, which reflect user's multiple interests and item's various aspects adaptively. In diffusion phase, DiffuRec corrupts the target item embedding into a Gaussian distribution via noise adding, which is further applied for sequential item distribution representat
&lt;/p&gt;</description></item><item><title>iQPP&#26159;&#31532;&#19968;&#20010;&#22312;&#20869;&#23481;&#20026;&#22270;&#20687;&#30340;&#26597;&#35810;&#22330;&#26223;&#19979;&#25506;&#32034;&#26597;&#35810;&#24615;&#33021;&#39044;&#27979;&#30340;&#22522;&#20934;&#65292;&#20854;&#32508;&#21512;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#65292;&#25581;&#31034;&#20102;&#38656;&#35201;&#35299;&#20915;&#30340;&#37325;&#35201;&#30740;&#31350;&#31354;&#30333;&#12290;</title><link>http://arxiv.org/abs/2302.10126</link><description>&lt;p&gt;
iQPP&#65306;&#22270;&#20687;&#26597;&#35810;&#24615;&#33021;&#39044;&#27979;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
iQPP: A Benchmark for Image Query Performance Prediction. (arXiv:2302.10126v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10126
&lt;/p&gt;
&lt;p&gt;
iQPP&#26159;&#31532;&#19968;&#20010;&#22312;&#20869;&#23481;&#20026;&#22270;&#20687;&#30340;&#26597;&#35810;&#22330;&#26223;&#19979;&#25506;&#32034;&#26597;&#35810;&#24615;&#33021;&#39044;&#27979;&#30340;&#22522;&#20934;&#65292;&#20854;&#32508;&#21512;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#65292;&#25581;&#31034;&#20102;&#38656;&#35201;&#35299;&#20915;&#30340;&#37325;&#35201;&#30740;&#31350;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#22312;&#22522;&#20110;&#20869;&#23481;&#30340;&#22270;&#20687;&#26816;&#32034;&#20013;&#65292;&#38024;&#23545;&#26597;&#35810;&#24615;&#33021;&#39044;&#27979;&#65288;QPP&#65289;&#30340;&#30740;&#31350;&#20173;&#28982;&#26159;&#19968;&#20010;&#36739;&#23569;&#34987;&#25506;&#32034;&#30340;&#20219;&#21153;&#65292;&#29305;&#21035;&#26159;&#22312;&#26597;&#35810;-by-example&#22330;&#26223;&#19979;&#65292;&#20854;&#20013;&#26597;&#35810;&#20026;&#22270;&#20687;&#12290;&#20026;&#20102;&#20419;&#36827;&#22270;&#20687;&#26816;&#32034;&#20013;QPP&#20219;&#21153;&#30340;&#25506;&#32034;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22270;&#20687;&#26597;&#35810;&#24615;&#33021;&#39044;&#27979;&#30340;&#31532;&#19968;&#20010;&#22522;&#20934;&#65288;iQPP&#65289;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#22235;&#20010;&#25968;&#25454;&#38598;&#65288;PASCAL VOC 2012&#65292; Caltech-101&#65292; ROxford5k&#21644;RParis6k&#65289;&#65292;&#24182;&#20351;&#29992;&#20004;&#31181;&#26368;&#20808;&#36827;&#30340;&#22270;&#20687;&#26816;&#32034;&#27169;&#22411;&#20272;&#35745;&#27599;&#20010;&#26597;&#35810;&#30340;&#22522;&#26412;&#30495;&#23454;&#38590;&#24230;&#65288;&#24179;&#22343;&#20934;&#30830;&#29575;&#25110;precision@k&#65289;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#26032;&#30340;&#39044;&#26816;&#32034;&#21644;&#21518;&#26816;&#32034;&#26597;&#35810;&#24615;&#33021;&#39044;&#27979;&#22120;&#65292;&#23558;&#23427;&#20204;&#19982;&#29616;&#26377;&#30340;&#25110;&#20174;&#25991;&#26412;&#21040;&#22270;&#20687;&#36866;&#24212;&#30340;&#39044;&#27979;&#22120;&#36827;&#34892;&#27604;&#36739;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22823;&#22810;&#25968;&#39044;&#27979;&#22120;&#22312;&#19981;&#21516;&#30340;&#35780;&#20272;&#22330;&#26223;&#19979;&#37117;&#19981;&#33021;&#24456;&#22909;&#22320;&#25512;&#24191;&#12290;&#25105;&#20204;&#30340;&#32508;&#21512;&#23454;&#39564;&#34920;&#26126;&#65292;iQPP &#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#65292;&#25581;&#31034;&#20102;&#38656;&#35201;&#35299;&#20915;&#30340;&#37325;&#35201;&#30740;&#31350;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;
To date, query performance prediction (QPP) in the context of content-based image retrieval remains a largely unexplored task, especially in the query-by-example scenario, where the query is an image. To boost the exploration of the QPP task in image retrieval, we propose the first benchmark for image query performance prediction (iQPP). First, we establish a set of four data sets (PASCAL VOC 2012, Caltech-101, ROxford5k and RParis6k) and estimate the ground-truth difficulty of each query as the average precision or the precision@k, using two state-of-the-art image retrieval models. Next, we propose and evaluate novel pre-retrieval and post-retrieval query performance predictors, comparing them with existing or adapted (from text to image) predictors. The empirical results show that most predictors do not generalize across evaluation scenarios. Our comprehensive experiments indicate that iQPP is a challenging benchmark, revealing an important research gap that needs to be addressed in 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20221;&#21327;&#35843;&#21152;&#23494;&#36135;&#24065;&#30456;&#20851;&#31038;&#20132;&#23186;&#20307;&#27963;&#21160;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#36328;&#23186;&#20307;&#36175;&#37329;&#27963;&#21160;&#30340;&#20449;&#24687;&#12289;&#21442;&#19982;&#32773;&#12289;&#35770;&#22363;&#35780;&#35770;&#21644;&#31038;&#20132;&#23186;&#20307;URL&#12290;&#35813;&#25968;&#25454;&#38598;&#21487;&#20197;&#20026;&#19981;&#21516;&#39046;&#22495;&#30340;&#30740;&#31350;&#25552;&#20379;&#28508;&#22312;&#26426;&#36935;&#65292;&#24182;&#24378;&#35843;&#20102;&#28508;&#22312;&#30340;&#21019;&#26032;&#28857;&#12290;</title><link>http://arxiv.org/abs/2301.06601</link><description>&lt;p&gt;
&#19968;&#20221;&#21327;&#35843;&#21152;&#23494;&#36135;&#24065;&#30456;&#20851;&#31038;&#20132;&#23186;&#20307;&#27963;&#21160;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
A Dataset of Coordinated Cryptocurrency-Related Social Media Campaigns. (arXiv:2301.06601v2 [cs.HC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.06601
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20221;&#21327;&#35843;&#21152;&#23494;&#36135;&#24065;&#30456;&#20851;&#31038;&#20132;&#23186;&#20307;&#27963;&#21160;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#36328;&#23186;&#20307;&#36175;&#37329;&#27963;&#21160;&#30340;&#20449;&#24687;&#12289;&#21442;&#19982;&#32773;&#12289;&#35770;&#22363;&#35780;&#35770;&#21644;&#31038;&#20132;&#23186;&#20307;URL&#12290;&#35813;&#25968;&#25454;&#38598;&#21487;&#20197;&#20026;&#19981;&#21516;&#39046;&#22495;&#30340;&#30740;&#31350;&#25552;&#20379;&#28508;&#22312;&#26426;&#36935;&#65292;&#24182;&#24378;&#35843;&#20102;&#28508;&#22312;&#30340;&#21019;&#26032;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21152;&#23494;&#36164;&#20135;&#30340;&#26222;&#21450;&#20351;&#24471;&#35768;&#22810;&#26032;&#25163;&#25237;&#36164;&#32773;&#36827;&#20837;&#20102;&#21152;&#23494;&#36135;&#24065;&#39046;&#22495;&#12290;&#36825;&#20123;&#25237;&#36164;&#32773;&#21487;&#20197;&#21463;&#21040;&#20182;&#20204;&#20174;&#31038;&#20132;&#23186;&#20307;&#19978;&#25509;&#25910;&#21040;&#30340;&#20449;&#24687;&#30340;&#19981;&#25104;&#27604;&#20363;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26377;&#20851;&#21152;&#23494;&#36135;&#24065;&#36175;&#37329;&#27963;&#21160;&#21644;&#21442;&#19982;&#32773;&#30340;&#25968;&#25454;&#38598;&#12290;&#36825;&#20123;&#27963;&#21160;&#21327;&#35843;&#31038;&#20132;&#23186;&#20307;&#27963;&#21160;&#65292;&#21019;&#36896;&#20154;&#20026;&#30340;&#8220;&#28818;&#20316;&#8221;&#65292;&#20197;&#24433;&#21709;&#21152;&#23494;&#39033;&#30446;&#20195;&#24065;&#30340;&#20215;&#26684;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20174;2014&#24180;5&#26376;&#21040;2022&#24180;12&#26376;&#26399;&#38388;&#20174;BitcoinTalk&#22312;&#32447;&#35770;&#22363;&#30340;Bounties(Altcoins)&#23376;&#35770;&#22363;&#25910;&#38598;&#30340;15.8K&#20010;&#36328;&#23186;&#20307;&#36175;&#37329;&#27963;&#21160;&#30340;&#20449;&#24687;&#12289;185K&#20010;&#21442;&#19982;&#32773;&#12289;10M&#26465;&#35770;&#22363;&#35780;&#35770;&#21644;82M&#20010;&#31038;&#20132;&#23186;&#20307;URL&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#25968;&#25454;&#25910;&#38598;&#21644;&#25968;&#25454;&#22788;&#29702;&#26041;&#27861;&#65292;&#24182;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#22522;&#26412;&#29305;&#24449;&#30340;&#25551;&#36848;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#35813;&#25968;&#25454;&#38598;&#22312;&#35768;&#22810;&#39046;&#22495;&#20869;&#25552;&#20379;&#30340;&#28508;&#22312;&#30740;&#31350;&#26426;&#36935;&#65292;&#24182;&#24378;&#35843;&#20102;&#28508;&#22312;&#30340;&#21019;&#26032;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rise in adoption of cryptoassets has brought many new and inexperienced investors in the cryptocurrency space. These investors can be disproportionally influenced by information they receive online, and particularly from social media. This paper presents a dataset of crypto-related bounty events and the users that participate in them. These events coordinate social media campaigns to create artificial "hype" around a crypto project in order to influence the price of its token. The dataset consists of information about 15.8K cross-media bounty events, 185K participants, 10M forum comments and 82M social media URLs collected from the Bounties(Altcoins) subforum of the BitcoinTalk online forum from May 2014 to December 2022. We describe the data collection and the data processing methods employed and we present a basic characterization of the dataset. Furthermore, we discuss potential research opportunities afforded by the dataset across many disciplines and we highlight potential nov
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#26367;&#20195;&#21697;&#25512;&#33616;&#36866;&#24212;&#21040;&#35821;&#35328;&#21305;&#37197;&#38382;&#39064;&#20013;&#65292;&#24182;&#36890;&#36807;&#35774;&#35745;&#26032;&#30340;&#36716;&#25442;&#26041;&#27861;&#21435;&#38500;&#20449;&#21495;&#22122;&#38899;&#65292;&#24182;&#32771;&#34385;&#20102;&#22810;&#35821;&#35328;&#25903;&#25345;&#12290;&#35813;&#27169;&#22411;&#24050;&#25104;&#21151;&#22312;&#19968;&#20010;&#22823;&#22411;&#30005;&#23376;&#21830;&#21153;&#32593;&#31449;&#19978;&#30340;11&#20010;&#24066;&#22330;&#21644;6&#31181;&#35821;&#35328;&#20013;&#37096;&#32626;&#65292;&#25552;&#39640;&#20102;&#39038;&#23458;&#24544;&#35802;&#24230;&#21644;&#36141;&#20080;&#29575;&#12290;</title><link>http://arxiv.org/abs/2211.02533</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#26367;&#20195;&#21697;&#25512;&#33616;&#27169;&#22411;&#65292;&#34701;&#21512;&#20102;&#24369;&#30417;&#30563;&#30340;&#39038;&#23458;&#34892;&#20026;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
A Transformer-Based Substitute Recommendation Model Incorporating Weakly Supervised Customer Behavior Data. (arXiv:2211.02533v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.02533
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#26367;&#20195;&#21697;&#25512;&#33616;&#36866;&#24212;&#21040;&#35821;&#35328;&#21305;&#37197;&#38382;&#39064;&#20013;&#65292;&#24182;&#36890;&#36807;&#35774;&#35745;&#26032;&#30340;&#36716;&#25442;&#26041;&#27861;&#21435;&#38500;&#20449;&#21495;&#22122;&#38899;&#65292;&#24182;&#32771;&#34385;&#20102;&#22810;&#35821;&#35328;&#25903;&#25345;&#12290;&#35813;&#27169;&#22411;&#24050;&#25104;&#21151;&#22312;&#19968;&#20010;&#22823;&#22411;&#30005;&#23376;&#21830;&#21153;&#32593;&#31449;&#19978;&#30340;11&#20010;&#24066;&#22330;&#21644;6&#31181;&#35821;&#35328;&#20013;&#37096;&#32626;&#65292;&#25552;&#39640;&#20102;&#39038;&#23458;&#24544;&#35802;&#24230;&#21644;&#36141;&#20080;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26367;&#20195;&#21697;&#30340;&#25512;&#33616;&#22312;&#30005;&#23376;&#21830;&#21153;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#65292;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#26367;&#20195;&#21697;&#32473;&#39038;&#23458;&#12290;&#20294;&#26159;&#29616;&#26377;&#30740;&#31350;&#36890;&#24120;&#20351;&#29992;&#39038;&#23458;&#30340;&#34892;&#20026;&#20449;&#21495;&#65288;&#22914;&#20849;&#21516;&#27983;&#35272;&#21644;&#27983;&#35272;&#20294;&#36141;&#20080;&#21478;&#19968;&#20010;&#20135;&#21697;&#65289;&#26469;&#25429;&#25417;&#26367;&#20195;&#20851;&#31995;&#12290;&#23613;&#31649;&#36825;&#20010;&#26041;&#27861;&#21548;&#36215;&#26469;&#24456;&#30452;&#35266;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#36825;&#31181;&#20570;&#27861;&#21487;&#33021;&#20250;&#24573;&#30053;&#20135;&#21697;&#30340;&#21151;&#33021;&#21644;&#29305;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20197;&#20135;&#21697;&#26631;&#39064;&#25551;&#36848;&#20316;&#20026;&#27169;&#22411;&#36755;&#20837;&#65292;&#24182;&#32771;&#34385;&#20135;&#21697;&#21151;&#33021;&#65292;&#23558;&#26367;&#20195;&#21697;&#25512;&#33616;&#36866;&#24212;&#21040;&#35821;&#35328;&#21305;&#37197;&#38382;&#39064;&#20013;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#36716;&#25442;&#26041;&#27861;&#26469;&#21435;&#38500;&#20174;&#29983;&#20135;&#25968;&#25454;&#20013;&#24471;&#20986;&#30340;&#20449;&#21495;&#22122;&#22768;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20174;&#24037;&#31243;&#35282;&#24230;&#32771;&#34385;&#22810;&#35821;&#35328;&#25903;&#25345;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#31471;&#21040;&#31471;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#22312;&#31163;&#32447;&#21644;&#22312;&#32447;&#23454;&#39564;&#20013;&#22343;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#24050;&#37096;&#32626;&#22312;&#19968;&#20010;&#22823;&#22411;&#30005;&#23376;&#21830;&#21153;&#32593;&#31449;&#19978;&#30340;11&#20010;&#24066;&#22330;&#21644;6&#31181;&#35821;&#35328;&#20013;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#34987;&#35777;&#26126;&#21487;&#20197;&#25552;&#39640;&#39038;&#23458;&#24544;&#35802;&#24230;&#21644;&#36141;&#20080;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
The substitute-based recommendation is widely used in E-commerce to provide better alternatives to customers. However, existing research typically uses the customer behavior signals like co-view and view-but-purchase-another to capture the substitute relationship. Despite its intuitive soundness, we find that such an approach might ignore the functionality and characteristics of products. In this paper, we adapt substitute recommendation into language matching problem by taking product title description as model input to consider product functionality. We design a new transformation method to de-noise the signals derived from production data. In addition, we consider multilingual support from the engineering point of view. Our proposed end-to-end transformer-based model achieves both successes from offline and online experiments. The proposed model has been deployed in a large-scale E-commerce website for 11 marketplaces in 6 languages. Our proposed model is demonstrated to increase re
&lt;/p&gt;</description></item></channel></rss>