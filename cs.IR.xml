<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#65292;&#36873;&#25321;&#19982;&#27979;&#35797;&#36755;&#20837;&#35821;&#20041;&#30456;&#20284;&#30340;&#28436;&#31034;&#26377;&#21161;&#20110;&#25552;&#39640;&#19979;&#28216;&#24615;&#33021;&#65292;&#20294;&#26159;&#32771;&#34385;&#21040;&#35821;&#35328;&#27169;&#22411;&#20851;&#20110;&#20219;&#21153;&#30340;&#29616;&#26377;&#30693;&#35782;&#33021;&#22815;&#26356;&#22909;&#22320;&#25351;&#23548;&#28436;&#31034;&#36873;&#25321;&#12290;</title><link>http://arxiv.org/abs/2309.07900</link><description>&lt;p&gt;
&#20855;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#27495;&#20041;&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;
Ambiguity-Aware In-Context Learning with Large Language Models. (arXiv:2309.07900v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07900
&lt;/p&gt;
&lt;p&gt;
&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#65292;&#36873;&#25321;&#19982;&#27979;&#35797;&#36755;&#20837;&#35821;&#20041;&#30456;&#20284;&#30340;&#28436;&#31034;&#26377;&#21161;&#20110;&#25552;&#39640;&#19979;&#28216;&#24615;&#33021;&#65292;&#20294;&#26159;&#32771;&#34385;&#21040;&#35821;&#35328;&#27169;&#22411;&#20851;&#20110;&#20219;&#21153;&#30340;&#29616;&#26377;&#30693;&#35782;&#33021;&#22815;&#26356;&#22909;&#22320;&#25351;&#23548;&#28436;&#31034;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;In-context learning, ICL&#65289;&#20013;&#65292;&#20165;&#21521;LLMs&#23637;&#31034;&#23569;&#37327;&#20219;&#21153;&#29305;&#23450;&#28436;&#31034;&#24050;&#32463;&#23548;&#33268;&#20102;&#19979;&#28216;&#22686;&#30410;&#65292;&#26080;&#38656;&#36827;&#34892;&#20219;&#21153;&#29305;&#23450;&#30340;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;LLMs&#23545;&#20110;&#25552;&#31034;&#36873;&#25321;&#38750;&#24120;&#25935;&#24863;&#65292;&#22240;&#27492;&#19968;&#20010;&#20851;&#38190;&#30340;&#30740;&#31350;&#38382;&#39064;&#26159;&#22914;&#20309;&#20026;ICL&#36873;&#25321;&#22909;&#30340;&#28436;&#31034;&#12290;&#19968;&#31181;&#26377;&#25928;&#30340;&#31574;&#30053;&#26159;&#21033;&#29992;ICL&#28436;&#31034;&#21644;&#27979;&#35797;&#36755;&#20837;&#20043;&#38388;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#65292;&#24182;&#20351;&#29992;&#25991;&#26412;&#26816;&#32034;&#22120;&#65292;&#28982;&#32780;&#36825;&#31181;&#26041;&#27861;&#24182;&#19981;&#32771;&#34385;LLM&#20851;&#20110;&#35813;&#20219;&#21153;&#30340;&#29616;&#26377;&#30693;&#35782;&#65292;&#22240;&#27492;&#24182;&#19981;&#26368;&#20248;&#12290;&#26681;&#25454;&#20043;&#21069;&#30340;&#24037;&#20316;&#65288;Min&#31561;&#65292;2022&#65289;&#65292;&#25105;&#20204;&#24050;&#32463;&#30693;&#36947;&#19982;&#28436;&#31034;&#37197;&#23545;&#30340;&#26631;&#31614;&#20250;&#23545;&#27169;&#22411;&#39044;&#27979;&#36896;&#25104;&#20559;&#35265;&#12290;&#36825;&#24341;&#23548;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20551;&#35774;&#65306;&#32771;&#34385;&#21040;LLM&#20851;&#20110;&#20219;&#21153;&#30340;&#29616;&#26377;&#30693;&#35782;&#65292;&#29305;&#21035;&#26159;&#19982;&#36755;&#20986;&#26631;&#31614;&#31354;&#38388;&#30456;&#20851;&#30340;&#30693;&#35782;&#65292;&#26159;&#21542;&#26377;&#21161;&#20110;&#26356;&#22909;&#30340;&#28436;&#31034;&#36873;&#25321;&#31574;&#30053;&#12290;&#36890;&#36807;&#22312;&#19977;&#20010;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#19978;&#36827;&#34892;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#19981;&#20165;&#36873;&#25321;&#35821;&#20041;&#30456;&#20284;&#30340;ICL&#28436;&#31034;&#26159;&#26377;&#30410;&#30340;&#65292;&#21516;&#26102;&#20063;&#35201;&#32771;&#34385;LLM&#20851;&#20110;&#20219;&#21153;&#30340;&#29616;&#26377;&#30693;&#35782;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#28436;&#31034;&#36873;&#25321;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
In-context learning (ICL) i.e. showing LLMs only a few task-specific demonstrations has led to downstream gains with no task-specific fine-tuning required. However, LLMs are sensitive to the choice of prompts, and therefore a crucial research question is how to select good demonstrations for ICL. One effective strategy is leveraging semantic similarity between the ICL demonstrations and test inputs by using a text retriever, which however is sub-optimal as that does not consider the LLM's existing knowledge about that task. From prior work (Min et al., 2022), we already know that labels paired with the demonstrations bias the model predictions. This leads us to our hypothesis whether considering LLM's existing knowledge about the task, especially with respect to the output label space can help in a better demonstration selection strategy. Through extensive experimentation on three text classification tasks, we find that it is beneficial to not only choose semantically similar ICL demon
&lt;/p&gt;</description></item><item><title>NineRec&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#21487;&#36801;&#31227;&#25512;&#33616;&#30340;&#25968;&#25454;&#38598;&#22871;&#20214;&#65292;&#21253;&#25324;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#28304;&#22495;&#25512;&#33616;&#25968;&#25454;&#38598;&#21644;&#20061;&#20010;&#22810;&#26679;&#30340;&#30446;&#26631;&#22495;&#25512;&#33616;&#25968;&#25454;&#38598;&#12290;&#27599;&#20010;&#29289;&#21697;&#30001;&#25991;&#26412;&#25551;&#36848;&#21644;&#39640;&#20998;&#36776;&#29575;&#23553;&#38754;&#22270;&#20687;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2309.07705</link><description>&lt;p&gt;
NineRec: &#29992;&#20110;&#35780;&#20272;&#21487;&#36801;&#31227;&#25512;&#33616;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#22871;&#20214;
&lt;/p&gt;
&lt;p&gt;
NineRec: A Benchmark Dataset Suite for Evaluating Transferable Recommendation. (arXiv:2309.07705v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07705
&lt;/p&gt;
&lt;p&gt;
NineRec&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#21487;&#36801;&#31227;&#25512;&#33616;&#30340;&#25968;&#25454;&#38598;&#22871;&#20214;&#65292;&#21253;&#25324;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#28304;&#22495;&#25512;&#33616;&#25968;&#25454;&#38598;&#21644;&#20061;&#20010;&#22810;&#26679;&#30340;&#30446;&#26631;&#22495;&#25512;&#33616;&#25968;&#25454;&#38598;&#12290;&#27599;&#20010;&#29289;&#21697;&#30001;&#25991;&#26412;&#25551;&#36848;&#21644;&#39640;&#20998;&#36776;&#29575;&#23553;&#38754;&#22270;&#20687;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20174;&#29289;&#21697;&#30340;&#21407;&#22987;&#29305;&#24449;&#65288;&#22914;&#22270;&#20687;&#12289;&#25991;&#26412;&#12289;&#38899;&#39057;&#31561;&#65289;&#23398;&#20064;&#25512;&#33616;&#31995;&#32479;&#27169;&#22411;&#65292;&#31216;&#20026;MoRec&#65292;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20852;&#36259;&#12290; MoRec&#30340;&#19968;&#20010;&#20851;&#38190;&#20248;&#21183;&#26159;&#23427;&#21487;&#20197;&#36731;&#26494;&#21463;&#30410;&#20110;&#20854;&#20182;&#39046;&#22495;&#30340;&#36827;&#23637;&#65292;&#22914;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#65288;CV&#65289;&#12290;&#27492;&#22806;&#65292;&#23427;&#36890;&#36807;&#29305;&#24449;&#33258;&#28982;&#25903;&#25345;&#19981;&#21516;&#31995;&#32479;&#20043;&#38388;&#30340;&#36801;&#31227;&#23398;&#20064;&#65292;&#31216;&#20026;&#21487;&#36801;&#31227;&#25512;&#33616;&#31995;&#32479;&#25110;TransRec&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#19982;NLP&#21644;CV&#39046;&#22495;&#30340;&#24320;&#21019;&#24615;&#22522;&#30784;&#27169;&#22411;&#30456;&#27604;&#65292;TransRec&#21462;&#24471;&#20102;&#24456;&#23567;&#30340;&#36827;&#23637;&#12290;&#32570;&#20047;&#22823;&#35268;&#27169;&#12289;&#39640;&#36136;&#37327;&#30340;&#25512;&#33616;&#25968;&#25454;&#38598;&#26159;&#19968;&#20010;&#37325;&#22823;&#38556;&#30861;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;NineRec&#65292;&#36825;&#26159;&#19968;&#20010;TransRec&#25968;&#25454;&#38598;&#22871;&#20214;&#65292;&#21253;&#25324;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#28304;&#22495;&#25512;&#33616;&#25968;&#25454;&#38598;&#21644;&#20061;&#20010;&#22810;&#26679;&#30340;&#30446;&#26631;&#22495;&#25512;&#33616;&#25968;&#25454;&#38598;&#12290;NineRec&#20013;&#30340;&#27599;&#20010;&#29289;&#21697;&#37117;&#30001;&#19968;&#20010;&#25991;&#26412;&#25551;&#36848;&#21644;&#19968;&#20010;&#39640;&#20998;&#36776;&#29575;&#30340;&#23553;&#38754;&#22270;&#20687;&#34920;&#31034;&#12290;&#36890;&#36807;NineRec&#65292;&#25105;&#20204;&#21487;&#20197;&#23454;&#29616;Tran
&lt;/p&gt;
&lt;p&gt;
Learning a recommender system model from an item's raw modality features (such as image, text, audio, etc.), called MoRec, has attracted growing interest recently. One key advantage of MoRec is that it can easily benefit from advances in other fields, such as natural language processing (NLP) and computer vision (CV). Moreover, it naturally supports transfer learning across different systems through modality features, known as transferable recommender systems, or TransRec.  However, so far, TransRec has made little progress, compared to groundbreaking foundation models in the fields of NLP and CV. The lack of large-scale, high-quality recommendation datasets poses a major obstacle. To this end, we introduce NineRec, a TransRec dataset suite that includes a large-scale source domain recommendation dataset and nine diverse target domain recommendation datasets. Each item in NineRec is represented by a text description and a high-resolution cover image. With NineRec, we can implement Tran
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#32508;&#21512;&#23545;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;&#30340;&#26041;&#27861;&#65292;&#20256;&#32479;&#26041;&#27861;&#26080;&#27861;&#24212;&#29992;&#20110;&#30495;&#23454;&#19990;&#30028;&#22330;&#26223;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21363;&#20351;&#29992;&#30495;&#23454;&#23545;&#35805;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2309.07682</link><description>&lt;p&gt;
&#19968;&#27425;&#23545;&#35805;&#32988;&#36807;&#21315;&#19975;&#30340;&#25512;&#33616;&#65306;&#32508;&#36848;&#32508;&#21512;&#23545;&#35805;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
A Conversation is Worth A Thousand Recommendations: A Survey of Holistic Conversational Recommender Systems. (arXiv:2309.07682v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07682
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#32508;&#21512;&#23545;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;&#30340;&#26041;&#27861;&#65292;&#20256;&#32479;&#26041;&#27861;&#26080;&#27861;&#24212;&#29992;&#20110;&#30495;&#23454;&#19990;&#30028;&#22330;&#26223;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21363;&#20351;&#29992;&#30495;&#23454;&#23545;&#35805;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;&#36890;&#36807;&#20132;&#20114;&#36807;&#31243;&#29983;&#25104;&#25512;&#33616;&#12290;&#28982;&#32780;&#65292;&#24182;&#38750;&#25152;&#26377;&#23545;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;&#26041;&#27861;&#37117;&#20351;&#29992;&#20154;&#31867;&#23545;&#35805;&#20316;&#20026;&#20132;&#20114;&#25968;&#25454;&#28304;&#65307;&#20808;&#21069;&#30340;&#22823;&#37096;&#20998;&#23545;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;&#24037;&#20316;&#36890;&#36807;&#20132;&#25442;&#23454;&#20307;&#32423;&#20449;&#24687;&#26469;&#27169;&#25311;&#20132;&#20114;&#12290;&#22240;&#27492;&#65292;&#20808;&#21069;&#30340;&#23545;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;&#24037;&#20316;&#26080;&#27861;&#36866;&#29992;&#20110;&#30495;&#23454;&#19990;&#30028;&#22330;&#26223;&#20013;&#30340;&#23545;&#35805;&#65292;&#20854;&#20013;&#23545;&#35805;&#20250;&#20986;&#29616;&#24847;&#22806;&#36716;&#21464;&#65292;&#25110;&#32773;&#23545;&#35805;&#21644;&#24847;&#22270;&#29702;&#35299;&#24182;&#38750;&#23436;&#32654;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#30740;&#31350;&#30028;&#24050;&#32463;&#24320;&#22987;&#30740;&#31350;&#32508;&#21512;&#23545;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;&#65292;&#36825;&#20123;&#31995;&#32479;&#20351;&#29992;&#20174;&#30495;&#23454;&#22330;&#26223;&#20013;&#25910;&#38598;&#21040;&#30340;&#23545;&#35805;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#23613;&#31649;&#36825;&#20123;&#32508;&#21512;&#26041;&#27861;&#24050;&#32463;&#20986;&#29616;&#65292;&#20294;&#20854;&#23578;&#26410;&#20805;&#20998;&#25506;&#32034;&#12290;&#25105;&#20204;&#36890;&#36807;&#32467;&#26500;&#21270;&#26041;&#24335;&#24635;&#32467;&#25991;&#29486;&#65292;&#23545;&#32508;&#21512;&#23545;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;&#26041;&#27861;&#36827;&#34892;&#20840;&#38754;&#35843;&#26597;&#12290;&#35843;&#26597;&#23558;&#32508;&#21512;&#23545;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;&#26041;&#27861;&#20998;&#20026;&#19977;&#20010;&#32452;&#25104;&#37096;&#20998;&#65306;1&#65289;&#39592;&#24178;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#36873;&#20351;&#29992;2&#65289;&#22806;&#37096;&#30693;&#35782;&#21644;/&#25110;3&#65289;&#22806;&#37096;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conversational recommender systems (CRS) generate recommendations through an interactive process. However, not all CRS approaches use human conversations as their source of interaction data; the majority of prior CRS work simulates interactions by exchanging entity-level information. As a result, claims of prior CRS work do not generalise to real-world settings where conversations take unexpected turns, or where conversational and intent understanding is not perfect. To tackle this challenge, the research community has started to examine holistic CRS, which are trained using conversational data collected from real-world scenarios. Despite their emergence, such holistic approaches are under-explored.  We present a comprehensive survey of holistic CRS methods by summarizing the literature in a structured manner. Our survey recognises holistic CRS approaches as having three components: 1) a backbone language model, the optional use of 2) external knowledge, and/or 3) external guidance. We
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#31038;&#21306;&#38382;&#31572;&#20219;&#21153;&#20013;&#23398;&#20064;&#21040;&#25490;&#21517;&#20013;&#30340;&#29305;&#24449;&#24037;&#31243;&#30340;&#20960;&#20010;&#26041;&#38754;&#12290;&#39318;&#20808;&#65292;&#24341;&#20837;&#20102;&#22522;&#20110;BERT&#30340;&#29305;&#24449;&#65292;&#25429;&#25417;&#35821;&#20041;&#30456;&#20284;&#24615;&#65307;&#20854;&#27425;&#65292;&#32467;&#21512;&#38382;&#39064;&#21644;&#31572;&#26696;&#20004;&#31181;&#31867;&#22411;&#30340;&#29305;&#24449;&#65307;&#31532;&#19977;&#65292;&#36890;&#36807;&#32463;&#39564;&#24615;&#30740;&#31350;&#25506;&#32034;&#20102;&#19981;&#21516;&#25490;&#21517;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.07610</link><description>&lt;p&gt;
&#23398;&#20064;&#21040;&#25490;&#21517;&#20013;&#30340;&#29305;&#24449;&#24037;&#31243;&#22312;&#31038;&#21306;&#38382;&#31572;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Feature Engineering in Learning-to-Rank for Community Question Answering Task. (arXiv:2309.07610v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07610
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#31038;&#21306;&#38382;&#31572;&#20219;&#21153;&#20013;&#23398;&#20064;&#21040;&#25490;&#21517;&#20013;&#30340;&#29305;&#24449;&#24037;&#31243;&#30340;&#20960;&#20010;&#26041;&#38754;&#12290;&#39318;&#20808;&#65292;&#24341;&#20837;&#20102;&#22522;&#20110;BERT&#30340;&#29305;&#24449;&#65292;&#25429;&#25417;&#35821;&#20041;&#30456;&#20284;&#24615;&#65307;&#20854;&#27425;&#65292;&#32467;&#21512;&#38382;&#39064;&#21644;&#31572;&#26696;&#20004;&#31181;&#31867;&#22411;&#30340;&#29305;&#24449;&#65307;&#31532;&#19977;&#65292;&#36890;&#36807;&#32463;&#39564;&#24615;&#30740;&#31350;&#25506;&#32034;&#20102;&#19981;&#21516;&#25490;&#21517;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#21306;&#38382;&#31572;&#65288;CQA&#65289;&#35770;&#22363;&#26159;&#22522;&#20110;&#20114;&#32852;&#32593;&#30340;&#24179;&#21488;&#65292;&#29992;&#25143;&#22312;&#36825;&#37324;&#25552;&#20986;&#38382;&#39064;&#65292;&#20854;&#20182;&#19987;&#23478;&#29992;&#25143;&#35797;&#22270;&#25552;&#20379;&#35299;&#20915;&#26041;&#26696;&#12290;&#35768;&#22810;CQA&#35770;&#22363;&#65292;&#22914;Quora&#65292;Stackoverflow&#65292;Yahoo&#65281;Answer&#65292;StackExchange&#31561;&#37117;&#26377;&#22823;&#37327;&#29992;&#25143;&#29983;&#25104;&#30340;&#25968;&#25454;&#12290;&#36825;&#20123;&#25968;&#25454;&#22312;&#33258;&#21160;&#21270;&#30340;CQA&#25490;&#21517;&#31995;&#32479;&#20013;&#24471;&#21040;&#21033;&#29992;&#65292;&#20197;&#22238;&#24212;&#29992;&#25143;&#30340;&#26597;&#35810;&#65292;&#21576;&#29616;&#31867;&#20284;&#30340;&#38382;&#39064;&#65288;&#21644;&#31572;&#26696;&#65289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32463;&#39564;&#24615;&#22320;&#35843;&#26597;&#20102;&#35813;&#39046;&#22495;&#30340;&#19968;&#20123;&#26041;&#38754;&#12290;&#39318;&#20808;&#65292;&#38500;&#20102;&#20256;&#32479;&#30340;&#29305;&#24449;&#22914;TF-IDF&#12289;BM25&#31561;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;BERT&#30340;&#29305;&#24449;&#65292;&#25429;&#25417;&#38382;&#39064;&#21644;&#31572;&#26696;&#20043;&#38388;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#12290;&#20854;&#27425;&#65292;&#22823;&#37096;&#20998;&#29616;&#26377;&#30740;&#31350;&#24037;&#20316;&#37117;&#38598;&#20013;&#22312;&#20165;&#20174;&#38382;&#39064;&#37096;&#20998;&#25552;&#21462;&#30340;&#29305;&#24449;&#19978;&#65292;&#23578;&#26410;&#24191;&#27867;&#25506;&#32034;&#20174;&#31572;&#26696;&#20013;&#25552;&#21462;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#20197;&#32447;&#24615;&#26041;&#24335;&#32467;&#21512;&#20102;&#20004;&#31181;&#31867;&#22411;&#30340;&#29305;&#24449;&#12290;&#31532;&#19977;&#65292;&#20351;&#29992;&#25105;&#20204;&#25552;&#20986;&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#23545;&#19981;&#21516;&#25490;&#21517;&#31639;&#27861;&#36827;&#34892;&#20102;&#32463;&#39564;&#24615;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Community question answering (CQA) forums are Internet-based platforms where users ask questions about a topic and other expert users try to provide solutions. Many CQA forums such as Quora, Stackoverflow, Yahoo!Answer, StackExchange exist with a lot of user-generated data. These data are leveraged in automated CQA ranking systems where similar questions (and answers) are presented in response to the query of the user. In this work, we empirically investigate a few aspects of this domain. Firstly, in addition to traditional features like TF-IDF, BM25 etc., we introduce a BERT-based feature that captures the semantic similarity between the question and answer. Secondly, most of the existing research works have focused on features extracted only from the question part; features extracted from answers have not been explored extensively. We combine both types of features in a linear fashion. Thirdly, using our proposed concepts, we conduct an empirical investigation with different rank-lea
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;-shot&#37325;&#26032;&#25490;&#24207;&#26041;&#27861;&#65292;&#20197;&#25913;&#21892;&#22522;&#20110;&#20027;&#39064;&#30340;&#35270;&#39057;&#26816;&#32034;&#24615;&#33021;&#65292;&#26080;&#38656;&#20219;&#20309;&#29305;&#23450;&#20219;&#21153;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2309.07606</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38646;-shot&#38899;&#39057;&#20027;&#39064;&#37325;&#25490;&#24207;
&lt;/p&gt;
&lt;p&gt;
Zero-shot Audio Topic Reranking using Large Language Models. (arXiv:2309.07606v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07606
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;-shot&#37325;&#26032;&#25490;&#24207;&#26041;&#27861;&#65292;&#20197;&#25913;&#21892;&#22522;&#20110;&#20027;&#39064;&#30340;&#35270;&#39057;&#26816;&#32034;&#24615;&#33021;&#65292;&#26080;&#38656;&#20219;&#20309;&#29305;&#23450;&#20219;&#21153;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#35270;&#39057;&#25628;&#32034;&#39033;&#30446;&#36890;&#36807;&#20351;&#29992;&#35270;&#39057;&#29255;&#27573;&#20316;&#20026;&#26597;&#35810;&#39033;&#65292;&#32780;&#19981;&#26159;&#20256;&#32479;&#30340;&#25991;&#26412;&#26597;&#35810;&#65292;&#26469;&#30740;&#31350;&#20449;&#24687;&#26816;&#32034;&#12290;&#36825;&#20351;&#24471;&#25628;&#32034;&#27169;&#24577;&#26356;&#21152;&#20016;&#23500;&#65292;&#20363;&#22914;&#22270;&#20687;&#12289;&#35828;&#35805;&#32773;&#12289;&#20869;&#23481;&#12289;&#20027;&#39064;&#21644;&#24773;&#24863;&#12290;&#36825;&#20010;&#36807;&#31243;&#30340;&#20851;&#38190;&#35201;&#32032;&#26159;&#23545;&#22823;&#22411;&#23384;&#26723;&#30340;&#39640;&#36895;&#12289;&#28789;&#27963;&#30340;&#25628;&#32034;&#25903;&#25345;&#65292;MVSE&#36890;&#36807;&#29992;&#23884;&#20837;&#34920;&#31034;&#35270;&#39057;&#23646;&#24615;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#12290;&#36825;&#39033;&#24037;&#20316;&#26088;&#22312;&#36890;&#36807;&#26816;&#26597;&#37325;&#26032;&#25490;&#24207;&#26041;&#27861;&#26469;&#20943;&#23569;&#26469;&#33258;&#24555;&#36895;&#23384;&#26723;&#25628;&#32034;&#30340;&#24615;&#33021;&#25439;&#22833;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#30740;&#31350;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;-shot &#37325;&#26032;&#25490;&#24207;&#26041;&#27861;&#65292;&#22240;&#20026;&#36825;&#20123;&#26041;&#27861;&#36866;&#29992;&#20110;&#20219;&#20309;&#35270;&#39057;&#23384;&#26723;&#38899;&#39057;&#20869;&#23481;&#12290;&#22312;&#20844;&#24320;&#21487;&#29992;&#30340;&#35270;&#39057;&#23384;&#26723;BBC Rewind&#35821;&#26009;&#24211;&#19978;&#35780;&#20272;&#20102;&#22522;&#20110;&#20027;&#39064;&#30340;&#26816;&#32034;&#24615;&#33021;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#19981;&#38656;&#35201;&#20219;&#20309;&#20219;&#21153;&#29305;&#23450;&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#37325;&#26032;&#25490;&#24207;&#21487;&#20197;&#23454;&#29616;&#25913;&#36827;&#30340;&#26816;&#32034;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Multimodal Video Search by Examples (MVSE) project investigates using video clips as the query term for information retrieval, rather than the more traditional text query. This enables far richer search modalities such as images, speaker, content, topic, and emotion. A key element for this process is highly rapid, flexible, search to support large archives, which in MVSE is facilitated by representing video attributes by embeddings. This work aims to mitigate any performance loss from this rapid archive search by examining reranking approaches. In particular, zero-shot reranking methods using large language models are investigated as these are applicable to any video archive audio content. Performance is evaluated for topic-based retrieval on a publicly available video archive, the BBC Rewind corpus. Results demonstrate that reranking can achieve improved retrieval ranking without the need for any task-specific training data.
&lt;/p&gt;</description></item><item><title>&#22312;&#27604;&#36739;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#20004;&#31181;&#27169;&#22411;SASRec&#21644;BERT4Rec&#26102;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#22914;&#26524;&#20004;&#20010;&#27169;&#22411;&#37117;&#20351;&#29992;&#30456;&#21516;&#30340;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#35757;&#32451;&#65292;SASRec&#22312;&#36136;&#37327;&#21644;&#35757;&#32451;&#36895;&#24230;&#26041;&#38754;&#34920;&#29616;&#26126;&#26174;&#20248;&#20110;BERT4Rec&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#21363;&#20351;&#20351;&#29992;&#36127;&#37319;&#26679;&#65292;SASRec&#20173;&#28982;&#33021;&#22815;&#26377;&#25928;&#35757;&#32451;&#24182;&#20248;&#20110;BERT4Rec&#65292;&#20294;&#38656;&#35201;&#26356;&#22810;&#30340;&#36127;&#26679;&#26412;&#12290;</title><link>http://arxiv.org/abs/2309.07602</link><description>&lt;p&gt;
&#23558;&#24223;&#26009;&#21464;&#20026;&#40644;&#37329;&#30340;&#25439;&#22833;&#65306;BERT4Rec&#30495;&#30340;&#27604;SASRec&#26356;&#22909;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Turning Dross Into Gold Loss: is BERT4Rec really better than SASRec?. (arXiv:2309.07602v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07602
&lt;/p&gt;
&lt;p&gt;
&#22312;&#27604;&#36739;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#20004;&#31181;&#27169;&#22411;SASRec&#21644;BERT4Rec&#26102;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#22914;&#26524;&#20004;&#20010;&#27169;&#22411;&#37117;&#20351;&#29992;&#30456;&#21516;&#30340;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#35757;&#32451;&#65292;SASRec&#22312;&#36136;&#37327;&#21644;&#35757;&#32451;&#36895;&#24230;&#26041;&#38754;&#34920;&#29616;&#26126;&#26174;&#20248;&#20110;BERT4Rec&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#21363;&#20351;&#20351;&#29992;&#36127;&#37319;&#26679;&#65292;SASRec&#20173;&#28982;&#33021;&#22815;&#26377;&#25928;&#35757;&#32451;&#24182;&#20248;&#20110;BERT4Rec&#65292;&#20294;&#38656;&#35201;&#26356;&#22810;&#30340;&#36127;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22312;&#25512;&#33616;&#31995;&#32479;&#39046;&#22495;&#65292;&#39034;&#24207;&#25512;&#33616;&#21644;&#19979;&#19968;&#20010;&#39033;&#30446;&#39044;&#27979;&#20219;&#21153;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#30446;&#21069;&#65292;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;SASRec&#21644;BERT4Rec&#26159;&#20004;&#31181;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#27169;&#22411;&#12290;&#22312;&#36807;&#21435;&#30340;&#20960;&#24180;&#20013;&#65292;&#26377;&#24456;&#22810;&#21457;&#34920;&#30340;&#35770;&#25991;&#27604;&#36739;&#20102;&#36825;&#20004;&#20010;&#31639;&#27861;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;&#22312;&#22823;&#22810;&#25968;&#35770;&#25991;&#20013;&#65292;BERT4Rec&#30340;&#24615;&#33021;&#20248;&#20110;SASRec&#12290;&#20294;&#26159;&#65292;BERT4Rec&#23545;&#25152;&#26377;&#39033;&#30446;&#20351;&#29992;&#20132;&#21449;&#29109;&#65292;&#32780;SASRec&#20351;&#29992;&#36127;&#37319;&#26679;&#23545;&#19968;&#20010;&#27491;&#26679;&#26412;&#21644;&#19968;&#20010;&#36127;&#26679;&#26412;&#35745;&#31639;&#20108;&#20803;&#20132;&#21449;&#29109;&#25439;&#22833;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#26524;&#20004;&#20010;&#27169;&#22411;&#37117;&#20351;&#29992;BERT4Rec&#25152;&#29992;&#30340;&#25439;&#22833;&#36827;&#34892;&#35757;&#32451;&#65292;&#37027;&#20040;SASRec&#22312;&#36136;&#37327;&#21644;&#35757;&#32451;&#36895;&#24230;&#26041;&#38754;&#23558;&#26126;&#26174;&#20248;&#20110;BERT4Rec&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#21363;&#20351;&#20351;&#29992;&#36127;&#37319;&#26679;&#65292;SASRec&#20173;&#28982;&#33021;&#22815;&#26377;&#25928;&#35757;&#32451;&#24182;&#20248;&#20110;BERT4Rec&#65292;&#20294;&#36127;&#26679;&#26412;&#30340;&#25968;&#37327;&#24212;&#35813;&#27604;BERT4Rec&#35201;&#22823;&#24471;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently sequential recommendations and next-item prediction task has become increasingly popular in the field of recommender systems. Currently, two state-of-the-art baselines are Transformer-based models SASRec and BERT4Rec. Over the past few years, there have been quite a few publications comparing these two algorithms and proposing new state-of-the-art models. In most of the publications, BERT4Rec achieves better performance than SASRec. But BERT4Rec uses cross-entropy over softmax for all items, while SASRec uses negative sampling and calculates binary cross-entropy loss for one positive and one negative item. In our work, we show that if both models are trained with the same loss, which is used by BERT4Rec, then SASRec will significantly outperform BERT4Rec both in terms of quality and training speed. In addition, we show that SASRec could be effectively trained with negative sampling and still outperform BERT4Rec, but the number of negative examples should be much larger than on
&lt;/p&gt;</description></item><item><title>C-Pack&#26159;&#19968;&#22871;&#25512;&#36827;&#26222;&#36890;&#27721;&#35821;&#23884;&#20837;&#39046;&#22495;&#30340;&#36164;&#28304;&#65292;&#21253;&#25324;&#20840;&#38754;&#27721;&#35821;&#25991;&#26412;&#23884;&#20837;&#22522;&#20934;&#12289;&#22823;&#35268;&#27169;&#25991;&#26412;&#23884;&#20837;&#25968;&#25454;&#38598;&#21644;&#28085;&#30422;&#22810;&#20010;&#23610;&#23544;&#30340;&#23884;&#20837;&#27169;&#22411;&#31995;&#21015;&#12290;&#35813;&#36164;&#28304;&#38598;&#22312;C-MTEB&#22522;&#20934;&#19978;&#23454;&#29616;&#20102;&#26368;&#39640;+10%&#30340;&#34920;&#29616;&#65292;&#24182;&#36890;&#36807;&#25972;&#21512;&#21644;&#20248;&#21270;&#19968;&#22871;&#35757;&#32451;&#26041;&#27861;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;C-Pack&#36824;&#21457;&#24067;&#20102;&#33521;&#35821;&#25991;&#26412;&#23884;&#20837;&#25968;&#25454;&#21644;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#35813;&#36164;&#28304;&#38598;&#21487;&#20844;&#24320;&#33719;&#21462;&#12290;</title><link>http://arxiv.org/abs/2309.07597</link><description>&lt;p&gt;
C-Pack: &#25512;&#36827;&#26222;&#36890;&#27721;&#35821;&#23884;&#20837;&#30340;&#25171;&#21253;&#36164;&#28304;
&lt;/p&gt;
&lt;p&gt;
C-Pack: Packaged Resources To Advance General Chinese Embedding. (arXiv:2309.07597v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07597
&lt;/p&gt;
&lt;p&gt;
C-Pack&#26159;&#19968;&#22871;&#25512;&#36827;&#26222;&#36890;&#27721;&#35821;&#23884;&#20837;&#39046;&#22495;&#30340;&#36164;&#28304;&#65292;&#21253;&#25324;&#20840;&#38754;&#27721;&#35821;&#25991;&#26412;&#23884;&#20837;&#22522;&#20934;&#12289;&#22823;&#35268;&#27169;&#25991;&#26412;&#23884;&#20837;&#25968;&#25454;&#38598;&#21644;&#28085;&#30422;&#22810;&#20010;&#23610;&#23544;&#30340;&#23884;&#20837;&#27169;&#22411;&#31995;&#21015;&#12290;&#35813;&#36164;&#28304;&#38598;&#22312;C-MTEB&#22522;&#20934;&#19978;&#23454;&#29616;&#20102;&#26368;&#39640;+10%&#30340;&#34920;&#29616;&#65292;&#24182;&#36890;&#36807;&#25972;&#21512;&#21644;&#20248;&#21270;&#19968;&#22871;&#35757;&#32451;&#26041;&#27861;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;C-Pack&#36824;&#21457;&#24067;&#20102;&#33521;&#35821;&#25991;&#26412;&#23884;&#20837;&#25968;&#25454;&#21644;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#35813;&#36164;&#28304;&#38598;&#21487;&#20844;&#24320;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;C-Pack&#65292;&#36825;&#26159;&#19968;&#22871;&#26174;&#33879;&#25512;&#36827;&#26222;&#36890;&#27721;&#35821;&#23884;&#20837;&#39046;&#22495;&#30340;&#36164;&#28304;&#12290;C-Pack&#21253;&#25324;&#19977;&#20010;&#20851;&#38190;&#36164;&#28304;&#12290;1&#65289;C-MTEB&#26159;&#19968;&#20010;&#28085;&#30422;6&#20010;&#20219;&#21153;&#21644;35&#20010;&#25968;&#25454;&#38598;&#30340;&#20840;&#38754;&#27721;&#35821;&#25991;&#26412;&#23884;&#20837;&#22522;&#20934;&#12290;2&#65289;C-MTP&#26159;&#19968;&#20010;&#20174;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#30340;&#27721;&#35821;&#35821;&#26009;&#24211;&#20013;&#31574;&#21010;&#30340;&#22823;&#35268;&#27169;&#25991;&#26412;&#23884;&#20837;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35757;&#32451;&#23884;&#20837;&#27169;&#22411;&#12290;3&#65289;C-TEM&#26159;&#19968;&#20010;&#28085;&#30422;&#22810;&#20010;&#23610;&#23544;&#30340;&#23884;&#20837;&#27169;&#22411;&#31995;&#21015;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;C-MTEB&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#20043;&#21069;&#30340;&#25152;&#26377;&#27721;&#35821;&#25991;&#26412;&#23884;&#20837;&#36798;&#21040;&#20102;&#21457;&#24067;&#26102;&#30340;&#26368;&#39640;+10%&#12290;&#25105;&#20204;&#36824;&#25972;&#21512;&#21644;&#20248;&#21270;&#20102;C-TEM&#30340;&#25972;&#22871;&#35757;&#32451;&#26041;&#27861;&#12290;&#38500;&#20102;&#25105;&#20204;&#20851;&#20110;&#26222;&#36890;&#27721;&#35821;&#23884;&#20837;&#30340;&#36164;&#28304;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#33521;&#35821;&#25991;&#26412;&#23884;&#20837;&#25968;&#25454;&#21644;&#27169;&#22411;&#12290;&#36825;&#20123;&#33521;&#35821;&#27169;&#22411;&#22312;MTEB&#22522;&#20934;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65307;&#19982;&#27492;&#21516;&#26102;&#65292;&#25105;&#20204;&#21457;&#24067;&#30340;&#33521;&#35821;&#25968;&#25454;&#27604;&#27721;&#35821;&#25968;&#25454;&#22823;2&#20493;&#12290;&#25152;&#26377;&#36825;&#20123;&#36164;&#28304;&#37117;&#21487;&#20197;&#22312;https://github.com/FlagOpen/FlagEmbedding&#19978;&#20844;&#24320;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce C-Pack, a package of resources that significantly advance the field of general Chinese embeddings. C-Pack includes three critical resources. 1) C-MTEB is a comprehensive benchmark for Chinese text embeddings covering 6 tasks and 35 datasets. 2) C-MTP is a massive text embedding dataset curated from labeled and unlabeled Chinese corpora for training embedding models. 3) C-TEM is a family of embedding models covering multiple sizes. Our models outperform all prior Chinese text embeddings on C-MTEB by up to +10% upon the time of the release. We also integrate and optimize the entire suite of training methods for C-TEM. Along with our resources on general Chinese embedding, we release our data and models for English text embeddings. The English models achieve state-of-the-art performance on MTEB benchmark; meanwhile, our released English data is 2 times larger than the Chinese data. All these resources are made publicly available at https://github.com/FlagOpen/FlagEmbedding.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#36923;&#36753;&#26597;&#35810;&#30340;&#31070;&#32463;&#31526;&#21495;&#25512;&#33616;&#27169;&#22411;&#65292;&#23558;&#29992;&#25143;&#21382;&#21490;&#20132;&#20114;&#36716;&#21270;&#20026;&#36923;&#36753;&#34920;&#36798;&#24335;&#65292;&#24182;&#36890;&#36807;&#36923;&#36753;&#26597;&#35810;&#23454;&#29616;&#25512;&#33616;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2309.07594</link><description>&lt;p&gt;
&#22522;&#20110;&#36923;&#36753;&#26597;&#35810;&#30340;&#31070;&#32463;&#31526;&#21495;&#25512;&#33616;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Neuro-Symbolic Recommendation Model based on Logic Query. (arXiv:2309.07594v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07594
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#36923;&#36753;&#26597;&#35810;&#30340;&#31070;&#32463;&#31526;&#21495;&#25512;&#33616;&#27169;&#22411;&#65292;&#23558;&#29992;&#25143;&#21382;&#21490;&#20132;&#20114;&#36716;&#21270;&#20026;&#36923;&#36753;&#34920;&#36798;&#24335;&#65292;&#24182;&#36890;&#36807;&#36923;&#36753;&#26597;&#35810;&#23454;&#29616;&#25512;&#33616;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#24110;&#21161;&#29992;&#25143;&#25214;&#21040;&#19982;&#20182;&#20204;&#30456;&#20851;&#30340;&#29289;&#21697;&#12290;&#29616;&#26377;&#30340;&#25512;&#33616;&#27169;&#22411;&#20027;&#35201;&#22522;&#20110;&#39044;&#27979;&#29992;&#25143;&#21644;&#29289;&#21697;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#20351;&#29992;&#22797;&#26434;&#30340;&#21305;&#37197;&#27169;&#22411;&#25110;&#24341;&#20837;&#22823;&#37327;&#30340;&#22806;&#37096;&#20449;&#24687;&#26469;&#25429;&#25417;&#25968;&#25454;&#20013;&#30340;&#20851;&#32852;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#25512;&#33616;&#19981;&#20165;&#26159;&#19968;&#20010;&#21033;&#29992;&#25968;&#25454;&#36827;&#34892;&#24402;&#32435;&#32479;&#35745;&#30340;&#38382;&#39064;&#65292;&#20063;&#26159;&#19968;&#20010;&#22522;&#20110;&#20174;&#20449;&#24687;&#20013;&#25552;&#21462;&#30340;&#30693;&#35782;&#36827;&#34892;&#25512;&#29702;&#20915;&#31574;&#30340;&#35748;&#30693;&#20219;&#21153;&#12290;&#22240;&#27492;&#65292;&#22312;&#25512;&#33616;&#20219;&#21153;&#20013;&#65292;&#36923;&#36753;&#31995;&#32479;&#33258;&#28982;&#21487;&#20197;&#29992;&#20110;&#25512;&#29702;&#12290;&#28982;&#32780;&#65292;&#34429;&#28982;&#22522;&#20110;&#36923;&#36753;&#31995;&#32479;&#30340;&#30828;&#35268;&#21017;&#26041;&#27861;&#21487;&#20197;&#25552;&#20379;&#24378;&#22823;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#22312;&#22788;&#29702;&#19981;&#19968;&#33268;&#21644;&#19981;&#23436;&#25972;&#30340;&#29616;&#23454;&#20219;&#21153;&#20013;&#24456;&#38590;&#24212;&#23545;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#22797;&#26434;&#20219;&#21153;&#22914;&#25512;&#33616;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31070;&#32463;&#31526;&#21495;&#25512;&#33616;&#27169;&#22411;&#65292;&#23558;&#29992;&#25143;&#21382;&#21490;&#20132;&#20114;&#36716;&#21270;&#20026;&#36923;&#36753;&#34920;&#36798;&#24335;&#65292;&#28982;&#21518;&#23558;&#25512;&#33616;&#36807;&#31243;&#21464;&#25442;&#20026;&#36923;&#36753;&#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;
A recommendation system assists users in finding items that are relevant to them. Existing recommendation models are primarily based on predicting relationships between users and items and use complex matching models or incorporate extensive external information to capture association patterns in data. However, recommendation is not only a problem of inductive statistics using data; it is also a cognitive task of reasoning decisions based on knowledge extracted from information. Hence, a logic system could naturally be incorporated for the reasoning in a recommendation task. However, although hard-rule approaches based on logic systems can provide powerful reasoning ability, they struggle to cope with inconsistent and incomplete knowledge in real-world tasks, especially for complex tasks such as recommendation. Therefore, in this paper, we propose a neuro-symbolic recommendation model, which transforms the user history interactions into a logic expression and then transforms the recomm
&lt;/p&gt;</description></item><item><title>MMEAD&#26159;&#29992;&#20110;MS MARCO&#25968;&#25454;&#38598;&#30340;&#23454;&#20307;&#38142;&#25509;&#36164;&#28304;&#12290;&#23427;&#25552;&#20379;&#20102;&#23454;&#20307;&#27880;&#37322;&#21644;&#28040;&#27495;&#21151;&#33021;&#65292;&#24182;&#25913;&#36827;&#20102;&#20351;&#29992;&#23454;&#20307;&#20449;&#24687;&#30340;&#20449;&#24687;&#26816;&#32034;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2309.07574</link><description>&lt;p&gt;
MMEAD: MS MARCO&#23454;&#20307;&#27880;&#37322;&#21644;&#28040;&#27495;
&lt;/p&gt;
&lt;p&gt;
MMEAD: MS MARCO Entity Annotations and Disambiguations. (arXiv:2309.07574v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07574
&lt;/p&gt;
&lt;p&gt;
MMEAD&#26159;&#29992;&#20110;MS MARCO&#25968;&#25454;&#38598;&#30340;&#23454;&#20307;&#38142;&#25509;&#36164;&#28304;&#12290;&#23427;&#25552;&#20379;&#20102;&#23454;&#20307;&#27880;&#37322;&#21644;&#28040;&#27495;&#21151;&#33021;&#65292;&#24182;&#25913;&#36827;&#20102;&#20351;&#29992;&#23454;&#20307;&#20449;&#24687;&#30340;&#20449;&#24687;&#26816;&#32034;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
MMEAD&#26159;&#29992;&#20110;MS MARCO&#25968;&#25454;&#38598;&#30340;&#23454;&#20307;&#38142;&#25509;&#36164;&#28304;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#31181;&#26684;&#24335;&#26469;&#23384;&#20648;&#21644;&#20849;&#20139;MS MARCO&#25991;&#26723;&#21644;&#27573;&#33853;&#38598;&#21512;&#30340;&#38142;&#25509;&#12290;&#25353;&#29031;&#27492;&#35268;&#33539;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;MS MARCO v1&#21644;v2&#20013;&#25991;&#26723;&#21644;&#27573;&#33853;&#30340;&#23454;&#20307;&#38142;&#25509;&#21040;&#32500;&#22522;&#30334;&#31185;&#12290;&#23454;&#20307;&#38142;&#25509;&#30001;REL&#21644;BLINK&#31995;&#32479;&#29983;&#25104;&#12290;MMEAD&#26159;&#19968;&#20010;&#26131;&#20110;&#23433;&#35013;&#30340;Python&#21253;&#65292;&#20801;&#35768;&#29992;&#25143;&#36731;&#26494;&#21152;&#36733;&#38142;&#25509;&#25968;&#25454;&#21644;&#23454;&#20307;&#23884;&#20837;&#12290;&#21482;&#38656;&#35201;&#20960;&#34892;&#20195;&#30721;&#21363;&#21487;&#20351;&#29992;MMEAD&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;MMEAD&#26469;&#25913;&#36827;&#20351;&#29992;&#23454;&#20307;&#20449;&#24687;&#30340;&#20449;&#24687;&#26816;&#32034;&#30740;&#31350;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#20351;&#29992;&#36825;&#20010;&#36164;&#28304;&#65292;&#22312;MS MARCO v1&#27573;&#33853;&#25968;&#25454;&#38598;&#19978;&#23545;&#26356;&#22797;&#26434;&#30340;&#26597;&#35810;&#25552;&#39640;recall@1000&#21644;MRR@10&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#23454;&#20307;&#25193;&#23637;&#26469;&#36827;&#34892;&#20132;&#20114;&#24335;&#25628;&#32034;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
MMEAD, or MS MARCO Entity Annotations and Disambiguations, is a resource for entity links for the MS MARCO datasets. We specify a format to store and share links for both document and passage collections of MS MARCO. Following this specification, we release entity links to Wikipedia for documents and passages in both MS MARCO collections (v1 and v2). Entity links have been produced by the REL and BLINK systems. MMEAD is an easy-to-install Python package, allowing users to load the link data and entity embeddings effortlessly. Using MMEAD takes only a few lines of code. Finally, we show how MMEAD can be used for IR research that uses entity information. We show how to improve recall@1000 and MRR@10 on more complex queries on the MS MARCO v1 passage dataset by using this resource. We also demonstrate how entity expansions can be used for interactive search applications.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38899;&#20048;&#27969;&#27966;&#20998;&#31867;&#65292;&#20351;&#29992;&#20102;&#25968;&#23383;&#20449;&#21495;&#22788;&#29702;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#20174;&#38899;&#39057;&#20449;&#21495;&#20013;&#25552;&#21462;&#29305;&#24449;&#24182;&#36827;&#34892;&#20998;&#31867;&#12290;&#35813;&#31639;&#27861;&#22312;GTZAN&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#39640;&#31934;&#24230;&#65292;&#21516;&#26102;&#36824;&#25552;&#20986;&#20102;&#31471;&#21040;&#31471;&#30340;&#37096;&#32626;&#26550;&#26500;&#65292;&#21487;&#29992;&#20110;&#38899;&#20048;&#24212;&#29992;&#31243;&#24207;&#30340;&#38598;&#25104;&#12290;</title><link>http://arxiv.org/abs/2309.04861</link><description>&lt;p&gt;
&#25506;&#32034;&#38899;&#20048;&#27969;&#27966;&#20998;&#31867;&#65306;&#31639;&#27861;&#20998;&#26512;&#19982;&#37096;&#32626;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Exploring Music Genre Classification: Algorithm Analysis and Deployment Architecture. (arXiv:2309.04861v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04861
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38899;&#20048;&#27969;&#27966;&#20998;&#31867;&#65292;&#20351;&#29992;&#20102;&#25968;&#23383;&#20449;&#21495;&#22788;&#29702;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#20174;&#38899;&#39057;&#20449;&#21495;&#20013;&#25552;&#21462;&#29305;&#24449;&#24182;&#36827;&#34892;&#20998;&#31867;&#12290;&#35813;&#31639;&#27861;&#22312;GTZAN&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#39640;&#31934;&#24230;&#65292;&#21516;&#26102;&#36824;&#25552;&#20986;&#20102;&#31471;&#21040;&#31471;&#30340;&#37096;&#32626;&#26550;&#26500;&#65292;&#21487;&#29992;&#20110;&#38899;&#20048;&#24212;&#29992;&#31243;&#24207;&#30340;&#38598;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#21508;&#31181;&#27969;&#23186;&#20307;&#24212;&#29992;&#30340;&#20986;&#29616;&#65292;&#38899;&#20048;&#27969;&#27966;&#20998;&#31867;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#22914;&#20170;&#65292;&#22312;&#19968;&#20010;&#22797;&#26434;&#30340;&#38899;&#20048;&#24212;&#29992;&#31243;&#24207;&#20013;&#65292;&#25105;&#20204;&#26080;&#27861;&#24819;&#35937;&#20165;&#36890;&#36807;&#33402;&#26415;&#23478;&#30340;&#21517;&#23383;&#21644;&#27468;&#26354;&#26631;&#39064;&#26469;&#25628;&#32034;&#38899;&#20048;&#12290;&#27491;&#30830;&#20998;&#31867;&#38899;&#20048;&#19968;&#30452;&#24456;&#22256;&#38590;&#65292;&#22240;&#20026;&#19982;&#38899;&#20048;&#30456;&#20851;&#30340;&#20449;&#24687;&#65292;&#22914;&#22320;&#21306;&#12289;&#33402;&#26415;&#23478;&#12289;&#19987;&#36753;&#25110;&#38750;&#19987;&#36753;&#65292;&#26159;&#22914;&#27492;&#22810;&#21464;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#39033;&#20851;&#20110;&#38899;&#20048;&#27969;&#27966;&#20998;&#31867;&#30340;&#30740;&#31350;&#65292;&#20351;&#29992;&#20102;&#25968;&#23383;&#20449;&#21495;&#22788;&#29702;&#65288;DSP&#65289;&#21644;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#25216;&#26415;&#30340;&#32452;&#21512;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#65292;&#21033;&#29992;DSP&#21644;DL&#26041;&#27861;&#20174;&#38899;&#39057;&#20449;&#21495;&#20013;&#25552;&#21462;&#30456;&#20851;&#29305;&#24449;&#65292;&#24182;&#23558;&#20854;&#20998;&#31867;&#21040;&#21508;&#31181;&#27969;&#27966;&#20013;&#12290;&#35813;&#31639;&#27861;&#22312;GTZAN&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#24182;&#21462;&#24471;&#20102;&#39640;&#31934;&#24230;&#12290;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#37096;&#32626;&#26550;&#26500;&#65292;&#29992;&#20110;&#38598;&#25104;&#21040;&#19982;&#38899;&#20048;&#30456;&#20851;&#30340;&#24212;&#29992;&#31243;&#24207;&#20013;&#12290;&#23545;&#31639;&#27861;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#24182;&#35752;&#35770;&#20102;&#25913;&#36827;&#30340;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Music genre classification has become increasingly critical with the advent of various streaming applications. Nowadays, we find it impossible to imagine using the artist's name and song title to search for music in a sophisticated music app. It is always difficult to classify music correctly because the information linked to music, such as region, artist, album, or non-album, is so variable. This paper presents a study on music genre classification using a combination of Digital Signal Processing (DSP) and Deep Learning (DL) techniques. A novel algorithm is proposed that utilizes both DSP and DL methods to extract relevant features from audio signals and classify them into various genres. The algorithm was tested on the GTZAN dataset and achieved high accuracy. An end-to-end deployment architecture is also proposed for integration into music-related applications. The performance of the algorithm is analyzed and future directions for improvement are discussed. The proposed DSP and DL-b
&lt;/p&gt;</description></item><item><title>CPMR&#26159;&#19968;&#20010;&#22522;&#20110;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#22686;&#37327;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#21019;&#24314;&#38745;&#24577;&#23884;&#20837;&#12289;&#21382;&#21490;&#26102;&#38388;&#29366;&#24577;&#21644;&#19978;&#19979;&#25991;&#26102;&#38388;&#29366;&#24577;&#30340;&#19977;&#20010;&#34920;&#31034;&#65292;&#20934;&#30830;&#22320;&#24314;&#27169;&#20102;&#29992;&#25143;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#34920;&#31034;&#21644;&#20852;&#36259;&#21160;&#24577;&#30340;&#28436;&#21270;&#12290;</title><link>http://arxiv.org/abs/2309.04802</link><description>&lt;p&gt;
CPMR: &#22522;&#20110;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#22686;&#37327;&#39034;&#24207;&#25512;&#33616;&#19982;&#20266;&#22810;&#20219;&#21153;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
CPMR: Context-Aware Incremental Sequential Recommendation with Pseudo-Multi-Task Learning. (arXiv:2309.04802v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04802
&lt;/p&gt;
&lt;p&gt;
CPMR&#26159;&#19968;&#20010;&#22522;&#20110;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#22686;&#37327;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#21019;&#24314;&#38745;&#24577;&#23884;&#20837;&#12289;&#21382;&#21490;&#26102;&#38388;&#29366;&#24577;&#21644;&#19978;&#19979;&#25991;&#26102;&#38388;&#29366;&#24577;&#30340;&#19977;&#20010;&#34920;&#31034;&#65292;&#20934;&#30830;&#22320;&#24314;&#27169;&#20102;&#29992;&#25143;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#34920;&#31034;&#21644;&#20852;&#36259;&#21160;&#24577;&#30340;&#28436;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#25143;&#36827;&#34892;&#20114;&#21160;&#30340;&#21160;&#26426;&#21487;&#20197;&#20998;&#20026;&#38745;&#24577;&#20559;&#22909;&#21644;&#21160;&#24577;&#20852;&#36259;&#12290;&#20026;&#20102;&#20934;&#30830;&#22320;&#24314;&#27169;&#29992;&#25143;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#34920;&#31034;&#65292;&#26368;&#36817;&#30340;&#39034;&#24207;&#25512;&#33616;&#30740;&#31350;&#21033;&#29992;&#20449;&#24687;&#20256;&#25773;&#21644;&#28436;&#21270;&#20174;&#25209;&#37327;&#21040;&#36798;&#30340;&#20114;&#21160;&#20013;&#36827;&#34892;&#25366;&#25496;&#12290;&#28982;&#32780;&#65292;&#20182;&#20204;&#24573;&#30053;&#20102;&#22312;&#19978;&#19979;&#25991;&#22330;&#26223;&#20013;&#20154;&#20204;&#24456;&#23481;&#26131;&#21463;&#21040;&#20854;&#20182;&#29992;&#25143;&#30340;&#26368;&#36817;&#34892;&#20026;&#30340;&#24433;&#21709;&#65292;&#24182;&#19988;&#22312;&#25152;&#26377;&#21382;&#21490;&#20114;&#21160;&#20013;&#24212;&#29992;&#28436;&#21270;&#20250;&#31232;&#37322;&#26368;&#36817;&#20114;&#21160;&#30340;&#37325;&#35201;&#24615;&#65292;&#20174;&#32780;&#26080;&#27861;&#20934;&#30830;&#22320;&#24314;&#27169;&#20852;&#36259;&#21160;&#24577;&#30340;&#28436;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#20266;&#22810;&#20219;&#21153;&#25512;&#33616;&#31995;&#32479;&#65288;CPMR&#65289;&#65292;&#36890;&#36807;&#20026;&#27599;&#20010;&#29992;&#25143;&#21644;&#39033;&#30446;&#21019;&#24314;&#19977;&#20010;&#34920;&#31034;&#65288;&#38745;&#24577;&#23884;&#20837;&#12289;&#21382;&#21490;&#26102;&#38388;&#29366;&#24577;&#21644;&#19978;&#19979;&#25991;&#26102;&#38388;&#29366;&#24577;&#65289;&#65292;&#26469;&#24314;&#27169;&#21382;&#21490;&#21644;&#19978;&#19979;&#25991;&#24773;&#22659;&#20013;&#30340;&#28436;&#21270;&#12290;&#20026;&#20102;&#21516;&#26102;&#25552;&#39640;&#26102;&#38388;&#29366;&#24577;&#28436;&#21270;&#21644;&#22686;&#37327;&#25512;&#33616;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The motivations of users to make interactions can be divided into static preference and dynamic interest. To accurately model user representations over time, recent studies in sequential recommendation utilize information propagation and evolution to mine from batches of arriving interactions. However, they ignore the fact that people are easily influenced by the recent actions of other users in the contextual scenario, and applying evolution across all historical interactions dilutes the importance of recent ones, thus failing to model the evolution of dynamic interest accurately. To address this issue, we propose a Context-Aware Pseudo-Multi-Task Recommender System (CPMR) to model the evolution in both historical and contextual scenarios by creating three representations for each user and item under different dynamics: static embedding, historical temporal states, and contextual temporal states. To dually improve the performance of temporal states evolution and incremental recommenda
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#31639;&#27861;&#37319;&#26679;&#29992;&#25143;&#31354;&#38388;&#20559;&#22909;&#30340;POI&#25512;&#33616;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#21482;&#22522;&#20110;&#29992;&#25143;&#20808;&#21069;&#35775;&#38382;&#20301;&#32622;&#32858;&#21512;&#30340;&#32570;&#28857;&#65292;&#36866;&#29992;&#20110;&#25512;&#33616;&#26032;&#39062;&#21306;&#22495;&#30340;POI&#12290;</title><link>http://arxiv.org/abs/2304.07041</link><description>&lt;p&gt;
&#19968;&#31181;POI&#25512;&#33616;&#30340;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Diffusion model for POI recommendation. (arXiv:2304.07041v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07041
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#31639;&#27861;&#37319;&#26679;&#29992;&#25143;&#31354;&#38388;&#20559;&#22909;&#30340;POI&#25512;&#33616;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#21482;&#22522;&#20110;&#29992;&#25143;&#20808;&#21069;&#35775;&#38382;&#20301;&#32622;&#32858;&#21512;&#30340;&#32570;&#28857;&#65292;&#36866;&#29992;&#20110;&#25512;&#33616;&#26032;&#39062;&#21306;&#22495;&#30340;POI&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19979;&#19968;&#20010;&#20852;&#36259;&#28857;&#65288;POI&#65289;&#30340;&#25512;&#33616;&#26159;&#23450;&#20301;&#26381;&#21153;&#20013;&#30340;&#20851;&#38190;&#20219;&#21153;&#65292;&#26088;&#22312;&#20026;&#29992;&#25143;&#30340;&#19979;&#19968;&#20010;&#30446;&#30340;&#22320;&#25552;&#20379;&#20010;&#24615;&#21270;&#24314;&#35758;&#12290;&#20808;&#21069;&#20851;&#20110;POI&#25512;&#33616;&#30340;&#24037;&#20316;&#20391;&#37325;&#20110;&#23545;&#29992;&#25143;&#31354;&#38388;&#20559;&#22909;&#30340;&#24314;&#27169;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#21033;&#29992;&#31354;&#38388;&#20449;&#24687;&#30340;&#26041;&#27861;&#20165;&#22522;&#20110;&#29992;&#25143;&#20808;&#21069;&#35775;&#38382;&#20301;&#32622;&#30340;&#32858;&#21512;&#65292;&#36825;&#20250;&#20351;&#27169;&#22411;&#19981;&#20250;&#25512;&#33616;&#26032;&#39062;&#21306;&#22495;&#30340;POI&#65292;&#20174;&#32780;&#25439;&#23475;&#20854;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#23558;&#26102;&#38388;&#39034;&#24207;&#20449;&#24687;&#34701;&#20837;&#29992;&#25143;&#30340;&#31354;&#38388;&#20559;&#22909;&#20173;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Diff-POI&#65306;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#37319;&#26679;&#29992;&#25143;&#30340;&#31354;&#38388;&#20559;&#22909;&#65292;&#20197;&#36827;&#34892;&#19979;&#19968;&#27493;POI&#25512;&#33616;&#12290;&#22312;&#25193;&#25955;&#31639;&#27861;&#22312;&#20174;&#20998;&#24067;&#20013;&#36827;&#34892;&#37319;&#26679;&#26041;&#38754;&#30340;&#24191;&#27867;&#24212;&#29992;&#30340;&#21551;&#21457;&#19979;&#65292;Diff-POI&#20351;&#29992;&#20004;&#20010;&#37327;&#36523;&#23450;&#21046;&#30340;&#22270;&#32534;&#30721;&#27169;&#22359;&#23545;&#29992;&#25143;&#30340;&#35775;&#38382;&#24207;&#21015;&#21644;&#31354;&#38388;&#29305;&#24615;&#36827;&#34892;&#32534;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Next Point-of-Interest (POI) recommendation is a critical task in location-based services that aim to provide personalized suggestions for the user's next destination. Previous works on POI recommendation have laid focused on modeling the user's spatial preference. However, existing works that leverage spatial information are only based on the aggregation of users' previous visited positions, which discourages the model from recommending POIs in novel areas. This trait of position-based methods will harm the model's performance in many situations. Additionally, incorporating sequential information into the user's spatial preference remains a challenge. In this paper, we propose Diff-POI: a Diffusion-based model that samples the user's spatial preference for the next POI recommendation. Inspired by the wide application of diffusion algorithm in sampling from distributions, Diff-POI encodes the user's visiting sequence and spatial character with two tailor-designed graph encoding modules
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#36827;&#34892;&#25512;&#29702;&#30340;&#21069;&#27839;&#30740;&#31350;&#32508;&#21512;&#35843;&#26597;&#12290;&#35752;&#35770;&#20102;&#26032;&#20852;&#25512;&#29702;&#33021;&#21147;&#20986;&#29616;&#30340;&#28508;&#22312;&#21407;&#22240;&#65292;&#24182;&#25552;&#20379;&#31995;&#32479;&#36164;&#28304;&#24110;&#21161;&#21021;&#23398;&#32773;&#12290;</title><link>http://arxiv.org/abs/2212.09597</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#36827;&#34892;&#25512;&#29702;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Reasoning with Language Model Prompting: A Survey. (arXiv:2212.09597v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09597
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#36827;&#34892;&#25512;&#29702;&#30340;&#21069;&#27839;&#30740;&#31350;&#32508;&#21512;&#35843;&#26597;&#12290;&#35752;&#35770;&#20102;&#26032;&#20852;&#25512;&#29702;&#33021;&#21147;&#20986;&#29616;&#30340;&#28508;&#22312;&#21407;&#22240;&#65292;&#24182;&#25552;&#20379;&#31995;&#32479;&#36164;&#28304;&#24110;&#21161;&#21021;&#23398;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#29702;&#20316;&#20026;&#22797;&#26434;&#38382;&#39064;&#35299;&#20915;&#30340;&#37325;&#35201;&#33021;&#21147;&#65292;&#21487;&#20197;&#20026;&#21307;&#30103;&#35786;&#26029;&#12289;&#35848;&#21028;&#31561;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#25552;&#20379;&#21518;&#31471;&#25903;&#25345;&#12290;&#26412;&#25991;&#23545;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#36827;&#34892;&#25512;&#29702;&#30340;&#21069;&#27839;&#30740;&#31350;&#36827;&#34892;&#20102;&#32508;&#21512;&#35843;&#26597;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#30740;&#31350;&#25104;&#26524;&#30340;&#27604;&#36739;&#21644;&#24635;&#32467;&#65292;&#24182;&#25552;&#20379;&#20102;&#31995;&#32479;&#36164;&#28304;&#20197;&#24110;&#21161;&#21021;&#23398;&#32773;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#26032;&#20852;&#25512;&#29702;&#33021;&#21147;&#20986;&#29616;&#30340;&#28508;&#22312;&#21407;&#22240;&#65292;&#24182;&#31361;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#36164;&#28304;&#21487;&#22312; https://github.com/zjunlp/Prompt4ReasoningPapers &#19978;&#33719;&#21462;&#65288;&#23450;&#26399;&#26356;&#26032;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reasoning, as an essential ability for complex problem-solving, can provide back-end support for various real-world applications, such as medical diagnosis, negotiation, etc. This paper provides a comprehensive survey of cutting-edge research on reasoning with language model prompting. We introduce research works with comparisons and summaries and provide systematic resources to help beginners. We also discuss the potential reasons for emerging such reasoning abilities and highlight future research directions. Resources are available at https://github.com/zjunlp/Prompt4ReasoningPapers (updated periodically).
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;DisenPOI&#65292;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20110;&#21452;&#22270;&#30340;POI&#25512;&#33616;&#35299;&#24320;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#39034;&#24207;&#21644;&#22320;&#29702;&#20851;&#31995;&#24182;&#20351;&#29992;&#33258;&#25105;&#30417;&#30563;&#35299;&#24320;&#36825;&#20004;&#31181;&#24433;&#21709;&#65292;&#20197;&#25552;&#39640;&#25512;&#33616;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.16591</link><description>&lt;p&gt;
DisenPOI: &#35299;&#24320;&#39034;&#24207;&#21644;&#22320;&#29702;&#24433;&#21709;&#30340;POI&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
DisenPOI: Disentangling Sequential and Geographical Influence for Point-of-Interest Recommendation. (arXiv:2210.16591v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.16591
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;DisenPOI&#65292;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20110;&#21452;&#22270;&#30340;POI&#25512;&#33616;&#35299;&#24320;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#39034;&#24207;&#21644;&#22320;&#29702;&#20851;&#31995;&#24182;&#20351;&#29992;&#33258;&#25105;&#30417;&#30563;&#35299;&#24320;&#36825;&#20004;&#31181;&#24433;&#21709;&#65292;&#20197;&#25552;&#39640;&#25512;&#33616;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
POI&#65288;&#20852;&#36259;&#28857;&#65289;&#25512;&#33616;&#22312;&#21508;&#31181;&#20301;&#32622;&#24863;&#30693;&#26381;&#21153;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#24050;&#32463;&#35266;&#23519;&#21040;POI&#25512;&#33616;&#21463;&#21040;&#39034;&#24207;&#21644;&#22320;&#29702;&#24433;&#21709;&#30340;&#39537;&#21160;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22312;&#25512;&#33616;&#36807;&#31243;&#20013;&#27809;&#26377;&#26126;&#30830;&#25351;&#23450;&#20027;&#23548;&#24433;&#21709;&#30340;&#27880;&#37322;&#26631;&#31614;&#65292;&#29616;&#26377;&#26041;&#27861;&#24448;&#24448;&#20250;&#28151;&#28102;&#36825;&#20004;&#31181;&#24433;&#21709;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#27425;&#20248;&#30340;&#25512;&#33616;&#24615;&#33021;&#21644;&#24046;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;DisenPOI&#65292;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20110;&#21452;&#22270;&#30340;POI&#25512;&#33616;&#35299;&#24320;&#26694;&#26550;&#65292;&#26469;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#12290;DisenPOI&#22312;&#20004;&#20010;&#29420;&#31435;&#30340;&#22270;&#19978;&#21516;&#26102;&#21033;&#29992;&#39034;&#24207;&#21644;&#22320;&#29702;&#20851;&#31995;&#65292;&#24182;&#36890;&#36807;&#33258;&#25105;&#30417;&#30563;&#35299;&#24320;&#36825;&#20004;&#31181;&#24433;&#21709;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#30340;&#20851;&#38190;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#25552;&#21462;&#39034;&#24207;&#21644;&#22320;&#29702;&#24433;&#21709;&#30340;&#35299;&#24320;&#34920;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#26681;&#25454;&#31614;&#21040;&#24207;&#21015;&#26500;&#24314;&#20102;&#19968;&#20010;&#22320;&#29702;&#22270;&#21644;&#19968;&#20010;&#39034;&#24207;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Point-of-Interest (POI) recommendation plays a vital role in various location-aware services. It has been observed that POI recommendation is driven by both sequential and geographical influences. However, since there is no annotated label of the dominant influence during recommendation, existing methods tend to entangle these two influences, which may lead to sub-optimal recommendation performance and poor interpretability. In this paper, we address the above challenge by proposing DisenPOI, a novel Disentangled dual-graph framework for POI recommendation, which jointly utilizes sequential and geographical relationships on two separate graphs and disentangles the two influences with self-supervision. The key novelty of our model compared with existing approaches is to extract disentangled representations of both sequential and geographical influences with contrastive learning. To be specific, we construct a geographical graph and a sequential graph based on the check-in sequence of a 
&lt;/p&gt;</description></item><item><title>LambdaKG&#26159;&#19968;&#20010;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#24211;&#65292;&#25552;&#20379;&#20102;&#22810;&#20010;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#25903;&#25345;&#22810;&#31181;&#20219;&#21153;&#65292;&#22914;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#12289;&#38382;&#31572;&#12289;&#25512;&#33616;&#21644;&#30693;&#35782;&#25506;&#32034;&#12290;</title><link>http://arxiv.org/abs/2210.00305</link><description>&lt;p&gt;
LambdaKG:&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#24211;
&lt;/p&gt;
&lt;p&gt;
LambdaKG: A Library for Pre-trained Language Model-Based Knowledge Graph Embeddings. (arXiv:2210.00305v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.00305
&lt;/p&gt;
&lt;p&gt;
LambdaKG&#26159;&#19968;&#20010;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#24211;&#65292;&#25552;&#20379;&#20102;&#22810;&#20010;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#25903;&#25345;&#22810;&#31181;&#20219;&#21153;&#65292;&#22914;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#12289;&#38382;&#31572;&#12289;&#25512;&#33616;&#21644;&#30693;&#35782;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#36890;&#24120;&#20855;&#26377;&#24322;&#26500;&#30340;&#22270;&#32467;&#26500;&#21644;&#25991;&#26412;&#20016;&#23500;&#30340;&#23454;&#20307;/&#20851;&#31995;&#20449;&#24687;&#12290;&#22522;&#20110;&#25991;&#26412;&#30340;KG&#23884;&#20837;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#23545;&#25551;&#36848;&#36827;&#34892;&#32534;&#30721;&#26469;&#34920;&#31034;&#23454;&#20307;&#65292;&#20294;&#30446;&#21069;&#23578;&#26080;&#19987;&#38376;&#20026;PLM&#19982;KG&#35774;&#35745;&#30340;&#24320;&#28304;&#24211;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;LambdaKG&#65292;&#19968;&#20010;&#24102;&#26377;&#22810;&#20010;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;BERT&#65292;BART&#65292;T5&#65292;GPT-3&#65289;&#24182;&#25903;&#25345;&#21508;&#31181;&#20219;&#21153;&#65288;&#22914;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#65292;&#38382;&#31572;&#65292;&#25512;&#33616;&#21644;&#30693;&#35782;&#25506;&#32034;&#65289;&#30340;KGE&#24211;&#12290;LambdaKG&#22312;https://github.com/zjunlp/PromptKG/tree/main/lambdaKG&#19978;&#20844;&#24320;&#24320;&#28304;&#65292;&#24182;&#25552;&#20379;&#20102;&#28436;&#31034;&#35270;&#39057;&#21644;&#38271;&#26399;&#32500;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graphs (KGs) often have two characteristics: heterogeneous graph structure and text-rich entity/relation information. Text-based KG embeddings can represent entities by encoding descriptions with pre-trained language models, but no open-sourced library is specifically designed for KGs with PLMs at present. In this paper, we present LambdaKG, a library for KGE that equips with many pre-trained language models (e.g., BERT, BART, T5, GPT-3), and supports various tasks (e.g., knowledge graph completion, question answering, recommendation, and knowledge probing). LambdaKG is publicly open-sourced at https://github.com/zjunlp/PromptKG/tree/main/lambdaKG, with a demo video at this http URL and long-term maintenance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#20174;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#29983;&#25104;SPARQL&#26597;&#35810;&#30340;&#20219;&#21153;&#65292;&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#26032;&#30340;&#22522;&#20934;&#27169;&#22411;&#65292;&#24182;&#22312;DBpedia&#21644;Wikidata&#30693;&#35782;&#22270;&#35889;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;T5&#27169;&#22411;&#22312;LC-QuAD 1.0&#21644;LC-QuAD 2.0&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#33021;&#22815;&#35299;&#26512;&#38656;&#35201;&#23558;&#19968;&#37096;&#20998;&#36755;&#20837;&#22797;&#21046;&#21040;&#36755;&#20986;&#26597;&#35810;&#20013;&#30340;&#38382;&#39064;&#65292;&#36825;&#20026;&#30693;&#35782;&#22270;&#35889;&#35821;&#20041;&#35299;&#26512;&#24102;&#26469;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2204.12793</link><description>&lt;p&gt;
SPARQL&#35821;&#20041;&#35299;&#26512;&#30340;&#29616;&#20195;&#22522;&#20934;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Modern Baselines for SPARQL Semantic Parsing. (arXiv:2204.12793v3 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.12793
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20174;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#29983;&#25104;SPARQL&#26597;&#35810;&#30340;&#20219;&#21153;&#65292;&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#26032;&#30340;&#22522;&#20934;&#27169;&#22411;&#65292;&#24182;&#22312;DBpedia&#21644;Wikidata&#30693;&#35782;&#22270;&#35889;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;T5&#27169;&#22411;&#22312;LC-QuAD 1.0&#21644;LC-QuAD 2.0&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#33021;&#22815;&#35299;&#26512;&#38656;&#35201;&#23558;&#19968;&#37096;&#20998;&#36755;&#20837;&#22797;&#21046;&#21040;&#36755;&#20986;&#26597;&#35810;&#20013;&#30340;&#38382;&#39064;&#65292;&#36825;&#20026;&#30693;&#35782;&#22270;&#35889;&#35821;&#20041;&#35299;&#26512;&#24102;&#26469;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20851;&#27880;&#20110;&#20174;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#29983;&#25104;SPARQL&#26597;&#35810;&#30340;&#20219;&#21153;&#65292;&#36825;&#20123;&#26597;&#35810;&#21487;&#20197;&#22312;&#30693;&#35782;&#22270;&#35889;&#19978;&#25191;&#34892;&#12290;&#25105;&#20204;&#20551;&#35774;&#24050;&#32463;&#25552;&#20379;&#20102;&#40644;&#37329;&#23454;&#20307;&#21644;&#20851;&#31995;&#65292;&#21097;&#19979;&#30340;&#20219;&#21153;&#26159;&#23558;&#23427;&#20204;&#19982;SPARQL&#35789;&#27719;&#21644;&#36755;&#20837;&#26631;&#35760;&#19968;&#36215;&#25353;&#27491;&#30830;&#30340;&#39034;&#24207;&#25490;&#21015;&#65292;&#20197;&#29983;&#25104;&#27491;&#30830;&#30340;SPARQL&#26597;&#35810;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#23578;&#26410;&#28145;&#20837;&#30740;&#31350;&#65292;&#22240;&#27492;&#25105;&#20204;&#23581;&#35797;&#20102;&#20351;&#29992;BART&#12289;T5&#21644;PGNs&#65288;&#25351;&#38024;&#29983;&#25104;&#32593;&#32476;&#65289;&#19982;BERT&#23884;&#20837;&#26469;&#23547;&#25214;&#36825;&#20010;&#20219;&#21153;&#22312;PLM&#26102;&#20195;&#30340;&#26032;&#22522;&#20934;&#65292;&#22312;DBpedia&#21644;Wikidata&#30693;&#35782;&#22270;&#35889;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;T5&#38656;&#35201;&#29305;&#27530;&#30340;&#36755;&#20837;&#26631;&#35760;&#21270;&#65292;&#20294;&#22312;LC-QuAD 1.0&#21644;LC-QuAD 2.0&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#36229;&#36807;&#20102;&#20197;&#21069;&#24037;&#20316;&#20013;&#30340;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26041;&#27861;&#20351;&#24471;&#23545;&#38382;&#39064;&#36827;&#34892;&#35821;&#20041;&#35299;&#26512;&#25104;&#20026;&#21487;&#33021;&#65292;&#20854;&#20013;&#36755;&#20837;&#30340;&#19968;&#37096;&#20998;&#38656;&#35201;&#22797;&#21046;&#21040;&#36755;&#20986;&#26597;&#35810;&#20013;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#30693;&#35782;&#22270;&#35889;&#35821;&#20041;&#35299;&#26512;&#30340;&#26032;&#33539;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we focus on the task of generating SPARQL queries from natural language questions, which can then be executed on Knowledge Graphs (KGs). We assume that gold entity and relations have been provided, and the remaining task is to arrange them in the right order along with SPARQL vocabulary, and input tokens to produce the correct SPARQL query. Pre-trained Language Models (PLMs) have not been explored in depth on this task so far, so we experiment with BART, T5 and PGNs (Pointer Generator Networks) with BERT embeddings, looking for new baselines in the PLM era for this task, on DBpedia and Wikidata KGs. We show that T5 requires special input tokenisation, but produces state of the art performance on LC-QuAD 1.0 and LC-QuAD 2.0 datasets, and outperforms task-specific models from previous works. Moreover, the methods enable semantic parsing for questions where a part of the input needs to be copied to the output query, thus enabling a new paradigm in KG semantic parsing.
&lt;/p&gt;</description></item></channel></rss>