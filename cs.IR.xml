<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#30693;&#35782;&#31934;&#28860;&#21435;&#22122;&#32593;&#32476;&#65288;KRDN&#65289;&#30340;&#24378;&#20581;&#30693;&#35782;&#24863;&#30693;&#25512;&#33616;&#26694;&#26550;&#65292;&#21487;&#20197;&#21516;&#26102;&#20462;&#21098;&#20219;&#21153;&#26080;&#20851;&#30340;&#30693;&#35782;&#20851;&#32852;&#21644;&#22122;&#22768;&#38544;&#24335;&#21453;&#39304;&#12290;</title><link>http://arxiv.org/abs/2304.14987</link><description>&lt;p&gt;
&#30693;&#35782;&#31934;&#28860;&#21435;&#22122;&#32593;&#32476;&#29992;&#20110;&#24378;&#20581;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Knowledge-refined Denoising Network for Robust Recommendation. (arXiv:2304.14987v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14987
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#30693;&#35782;&#31934;&#28860;&#21435;&#22122;&#32593;&#32476;&#65288;KRDN&#65289;&#30340;&#24378;&#20581;&#30693;&#35782;&#24863;&#30693;&#25512;&#33616;&#26694;&#26550;&#65292;&#21487;&#20197;&#21516;&#26102;&#20462;&#21098;&#20219;&#21153;&#26080;&#20851;&#30340;&#30693;&#35782;&#20851;&#32852;&#21644;&#22122;&#22768;&#38544;&#24335;&#21453;&#39304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#20316;&#20026;&#20016;&#23500;&#30340;&#36741;&#21161;&#20449;&#24687;&#65292;&#25104;&#20026;&#20102;&#25552;&#21319;&#25512;&#33616;&#24615;&#33021;&#21644;&#25913;&#21892;&#21487;&#35299;&#37322;&#24615;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30693;&#35782;&#24863;&#30693;&#25512;&#33616;&#26041;&#27861;&#30452;&#25509;&#22312;&#30693;&#35782;&#22270;&#35889;&#21644;&#29992;&#25143;-&#29289;&#21697;&#20108;&#20998;&#22270;&#19978;&#25191;&#34892;&#20449;&#24687;&#20256;&#36882;&#65292;&#24573;&#30053;&#20102;&#20219;&#21153;&#26080;&#20851;&#30693;&#35782;&#20256;&#25773;&#21644;&#20132;&#20114;&#22122;&#22768;&#30340;&#24433;&#21709;&#65292;&#38480;&#21046;&#20102;&#20854;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24378;&#20581;&#30340;&#30693;&#35782;&#24863;&#30693;&#25512;&#33616;&#26694;&#26550;&#65292;&#31216;&#20026;&#30693;&#35782;&#31934;&#28860;&#21435;&#22122;&#32593;&#32476;&#65288;KRDN&#65289;&#65292;&#20197;&#21516;&#26102;&#20462;&#21098;&#20219;&#21153;&#26080;&#20851;&#30340;&#30693;&#35782;&#20851;&#32852;&#21644;&#22122;&#22768;&#38544;&#24335;&#21453;&#39304;&#12290;KRDN&#21253;&#25324;&#33258;&#36866;&#24212;&#30693;&#35782;&#31934;&#28860;&#31574;&#30053;&#21644;&#23545;&#27604;&#21435;&#22122;&#26426;&#21046;&#65292;&#33021;&#22815;&#33258;&#21160;&#25910;&#38598;&#39640;&#36136;&#37327;&#30340;&#30693;&#35782;&#22270;&#35889;&#19977;&#20803;&#32452;&#65292;&#24182;&#35009;&#21098;&#22122;&#22768;&#38544;&#24335;&#21453;&#39304;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#33258;&#36866;&#24212;&#25439;&#22833;&#20989;&#25968;&#21644;&#26799;&#24230;&#20272;&#35745;&#22120;&#65292;&#29992;&#20110;&#27169;&#22411;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graph (KG), which contains rich side information, becomes an essential part to boost the recommendation performance and improve its explainability. However, existing knowledge-aware recommendation methods directly perform information propagation on KG and user-item bipartite graph, ignoring the impacts of \textit{task-irrelevant knowledge propagation} and \textit{vulnerability to interaction noise}, which limits their performance. To solve these issues, we propose a robust knowledge-aware recommendation framework, called \textit{Knowledge-refined Denoising Network} (KRDN), to prune the task-irrelevant knowledge associations and noisy implicit feedback simultaneously. KRDN consists of an adaptive knowledge refining strategy and a contrastive denoising mechanism, which are able to automatically distill high-quality KG triplets for aggregation and prune noisy implicit feedback respectively. Besides, we also design the self-adapted loss function and the gradient estimator for mod
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38754;&#21521;&#20027;&#39064;&#30340;&#23545;&#25239;&#25490;&#21517;&#25915;&#20987;&#20219;&#21153;&#65292;&#20197;&#25552;&#39640;&#21516;&#19968;&#20027;&#39064;&#30340;&#19968;&#32452;&#26597;&#35810;&#20013;&#30340;&#29305;&#23450;&#25991;&#26723;&#25490;&#21517;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#26367;&#20195;&#25490;&#21517;&#27169;&#22411;&#30340;&#26032;&#26694;&#26550;&#26469;&#25913;&#21892;&#25915;&#20987;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.14867</link><description>&lt;p&gt;
&#38754;&#21521;&#20027;&#39064;&#30340;&#40657;&#30418;&#31070;&#32463;&#25490;&#21517;&#27169;&#22411;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Topic-oriented Adversarial Attacks against Black-box Neural Ranking Models. (arXiv:2304.14867v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14867
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38754;&#21521;&#20027;&#39064;&#30340;&#23545;&#25239;&#25490;&#21517;&#25915;&#20987;&#20219;&#21153;&#65292;&#20197;&#25552;&#39640;&#21516;&#19968;&#20027;&#39064;&#30340;&#19968;&#32452;&#26597;&#35810;&#20013;&#30340;&#29305;&#23450;&#25991;&#26723;&#25490;&#21517;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#26367;&#20195;&#25490;&#21517;&#27169;&#22411;&#30340;&#26032;&#26694;&#26550;&#26469;&#25913;&#21892;&#25915;&#20987;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#25490;&#21517;&#27169;&#22411;&#65288;NRMs&#65289;&#22312;&#20449;&#24687;&#26816;&#32034;&#20013;&#21463;&#21040;&#20102;&#26497;&#22823;&#30340;&#20851;&#27880;&#12290;&#20294;&#19981;&#24184;&#30340;&#26159;&#65292;NRMs&#21487;&#33021;&#32487;&#25215;&#20102;&#19968;&#33324;&#31070;&#32463;&#32593;&#32476;&#30340;&#23545;&#25239;&#24615;&#28431;&#27934;&#65292;&#36825;&#21487;&#33021;&#34987;&#40657;&#24125;&#25628;&#32034;&#24341;&#25806;&#20248;&#21270;&#20174;&#19994;&#32773;&#21033;&#29992;&#12290;&#26368;&#36817;&#65292;&#20154;&#20204;&#22312;&#37197;&#23545;&#25915;&#20987;&#35774;&#32622;&#20013;&#25506;&#32034;&#20102;&#23545;NRMs&#30340;&#23545;&#25239;&#25915;&#20987;&#65292;&#20026;&#29305;&#23450;&#26597;&#35810;&#30340;&#30446;&#26631;&#25991;&#26723;&#29983;&#25104;&#23545;&#25239;&#25200;&#21160;&#12290;&#26412;&#25991;&#20851;&#27880;&#26356;&#36890;&#29992;&#31867;&#22411;&#30340;&#25200;&#21160;&#65292;&#24341;&#20837;&#20102;&#38024;&#23545;NRMs&#30340;&#38754;&#21521;&#20027;&#39064;&#30340;&#23545;&#25239;&#25490;&#21517;&#25915;&#20987;&#20219;&#21153;&#65292;&#26088;&#22312;&#25214;&#21040;&#19968;&#31181;&#26080;&#27861;&#23519;&#35273;&#30340;&#25200;&#21160;&#65292;&#21487;&#20197;&#22312;&#21516;&#19968;&#20027;&#39064;&#30340;&#19968;&#32452;&#26597;&#35810;&#20013;&#25552;&#39640;&#30446;&#26631;&#25991;&#26723;&#30340;&#25490;&#21517;&#12290;&#25105;&#20204;&#20026;&#20219;&#21153;&#23450;&#20041;&#20102;&#38745;&#24577;&#21644;&#21160;&#24577;&#35774;&#32622;&#65292;&#24182;&#19987;&#27880;&#20110;&#22522;&#20110;&#20915;&#31574;&#30340;&#40657;&#30418;&#25915;&#20987;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#26469;&#25913;&#36827;&#22522;&#20110;&#20027;&#39064;&#30340;&#25915;&#20987;&#24615;&#33021;&#65292;&#22522;&#20110;&#26367;&#20195;&#25490;&#21517;&#27169;&#22411;&#12290;&#25915;&#20987;&#38382;&#39064;&#34987;&#24418;&#24335;&#21270;&#20026;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural ranking models (NRMs) have attracted considerable attention in information retrieval. Unfortunately, NRMs may inherit the adversarial vulnerabilities of general neural networks, which might be leveraged by black-hat search engine optimization practitioners. Recently, adversarial attacks against NRMs have been explored in the paired attack setting, generating an adversarial perturbation to a target document for a specific query. In this paper, we focus on a more general type of perturbation and introduce the topic-oriented adversarial ranking attack task against NRMs, which aims to find an imperceptible perturbation that can promote a target document in ranking for a group of queries with the same topic. We define both static and dynamic settings for the task and focus on decision-based black-box attacks. We propose a novel framework to improve topic-oriented attack performance based on a surrogate ranking model. The attack problem is formalized as a Markov decision process (MDP)
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#29983;&#25104;&#26816;&#32034;&#22120;(UGR)&#65292;&#23427;&#23558;&#20219;&#21153;&#29305;&#23450;&#30340;&#25928;&#26524;&#19982;&#22312;KILTs&#20013;&#19981;&#21516;&#26816;&#32034;&#20219;&#21153;&#30340;&#24378;&#20581;&#24615;&#33021;&#30456;&#32467;&#21512;&#12290;UGR&#37319;&#29992;&#22522;&#20110;Prompt&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#20854;&#33021;&#22815;&#36866;&#24212;&#21508;&#31181;&#26816;&#32034;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#22312;&#20116;&#20010;KILTs&#22522;&#20934;&#27979;&#35797;&#19978;&#26174;&#33879;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#32447;&#12290;</title><link>http://arxiv.org/abs/2304.14856</link><description>&lt;p&gt;
&#36890;&#36807;Prompt&#23398;&#20064;&#30340;&#30693;&#35782;&#23494;&#38598;&#35821;&#35328;&#20219;&#21153;&#30340;&#32479;&#19968;&#29983;&#25104;&#26816;&#32034;&#22120;
&lt;/p&gt;
&lt;p&gt;
A Unified Generative Retriever for Knowledge-Intensive Language Tasks via Prompt Learning. (arXiv:2304.14856v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14856
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#29983;&#25104;&#26816;&#32034;&#22120;(UGR)&#65292;&#23427;&#23558;&#20219;&#21153;&#29305;&#23450;&#30340;&#25928;&#26524;&#19982;&#22312;KILTs&#20013;&#19981;&#21516;&#26816;&#32034;&#20219;&#21153;&#30340;&#24378;&#20581;&#24615;&#33021;&#30456;&#32467;&#21512;&#12290;UGR&#37319;&#29992;&#22522;&#20110;Prompt&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#20854;&#33021;&#22815;&#36866;&#24212;&#21508;&#31181;&#26816;&#32034;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#22312;&#20116;&#20010;KILTs&#22522;&#20934;&#27979;&#35797;&#19978;&#26174;&#33879;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#23494;&#38598;&#35821;&#35328;&#20219;&#21153;(KILTs)&#21463;&#30410;&#20110;&#20174;&#22823;&#22411;&#22806;&#37096;&#30693;&#35782;&#35821;&#26009;&#24211;&#20013;&#26816;&#32034;&#39640;&#36136;&#37327;&#30340;&#30456;&#20851;&#20869;&#23481;&#12290;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#30340;&#26816;&#32034;&#22120;&#65292;&#36820;&#22238;&#24688;&#24403;&#35821;&#20041;&#31890;&#24230;&#30340;&#30456;&#20851;&#20869;&#23481;(&#22914;&#25991;&#26723;&#26816;&#32034;&#22120;&#12289;&#27573;&#33853;&#26816;&#32034;&#22120;&#12289;&#21477;&#23376;&#26816;&#32034;&#22120;&#21644;&#23454;&#20363;&#26816;&#32034;&#22120;)&#26377;&#21161;&#20110;&#22312;&#31471;&#21040;&#31471;&#20219;&#21153;&#19978;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#20294;&#26159;&#20219;&#21153;&#29305;&#23450;&#30340;&#26816;&#32034;&#22120;&#36890;&#24120;&#23545;&#26032;&#39046;&#22495;&#21644;&#20219;&#21153;&#20855;&#26377;&#36739;&#24046;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#22312;&#23454;&#36341;&#20013;&#37096;&#32626;&#22810;&#31181;&#19987;&#38376;&#30340;&#26816;&#32034;&#22120;&#21487;&#33021;&#25104;&#26412;&#36807;&#39640;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#29983;&#25104;&#26816;&#32034;&#22120;(UGR)&#65292;&#23427;&#23558;&#20219;&#21153;&#29305;&#23450;&#30340;&#25928;&#26524;&#19982;&#22312;KILTs&#20013;&#19981;&#21516;&#26816;&#32034;&#20219;&#21153;&#30340;&#24378;&#20581;&#24615;&#33021;&#30456;&#32467;&#21512;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#20570;&#20986;&#20102;&#20004;&#20010;&#20027;&#35201;&#36129;&#29486;:(i)&#20026;&#20102;&#23558;&#19981;&#21516;&#30340;&#26816;&#32034;&#20219;&#21153;&#32479;&#19968;&#25104;&#21333;&#19968;&#30340;&#29983;&#25104;&#24418;&#24335;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;n-gram&#30340;&#26631;&#35782;&#31526;&#65292;&#29992;&#20110;&#22312;KILTs&#20013;&#19981;&#21516;&#31890;&#24230;&#32423;&#21035;&#19978;&#35782;&#21035;&#30456;&#20851;&#20869;&#23481;&#12290;(ii)&#20026;&#20102;&#35299;&#20915;&#19981;&#21516;&#20219;&#21153;&#30340;&#19981;&#21516;&#26816;&#32034;&#38656;&#27714;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Prompt&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;UGR&#33021;&#22815;&#36890;&#36807;&#20174;&#20219;&#21153;&#29305;&#23450;&#30340;Prompts&#20013;&#23398;&#20064;&#26469;&#36866;&#24212;&#21508;&#31181;&#26816;&#32034;&#20219;&#21153;&#12290;&#22312;&#20116;&#20010;KILTs&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;UGR&#22312;&#25152;&#26377;&#20219;&#21153;&#19978;&#21482;&#29992;&#19968;&#20010;&#27169;&#22411;&#23601;&#26174;&#33879;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge-intensive language tasks (KILTs) benefit from retrieving high-quality relevant contexts from large external knowledge corpora. Learning task-specific retrievers that return relevant contexts at an appropriate level of semantic granularity, such as a document retriever, passage retriever, sentence retriever, and entity retriever, may help to achieve better performance on the end-to-end task. But a task-specific retriever usually has poor generalization ability to new domains and tasks, and it may be costly to deploy a variety of specialised retrievers in practice. We propose a unified generative retriever (UGR) that combines task-specific effectiveness with robust performance over different retrieval tasks in KILTs. To achieve this goal, we make two major contributions: (i) To unify different retrieval tasks into a single generative form, we introduce an n-gram-based identifier for relevant contexts at different levels of granularity in KILTs. And (ii) to address different ret
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#25506;&#32034;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#23398;&#20064;&#36710;&#36742;&#32500;&#20462;&#39046;&#22495;&#32452;&#20214;&#30340;&#29305;&#23450;&#26448;&#26009;&#65292;&#25104;&#21151;&#20811;&#26381;&#20102;&#25968;&#25454;&#31232;&#30095;&#24615;&#38382;&#39064;&#21644;&#32570;&#20047;&#27880;&#37322;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.14745</link><description>&lt;p&gt;
&#30001;&#20160;&#20040;&#26500;&#25104;&#65311;&#23398;&#20064;&#20462;&#36710;&#39046;&#22495;&#32452;&#20214;&#30340;&#21487;&#20449;&#26448;&#26009;
&lt;/p&gt;
&lt;p&gt;
Made of Steel? Learning Plausible Materials for Components in the Vehicle Repair Domain. (arXiv:2304.14745v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14745
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#25506;&#32034;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#23398;&#20064;&#36710;&#36742;&#32500;&#20462;&#39046;&#22495;&#32452;&#20214;&#30340;&#29305;&#23450;&#26448;&#26009;&#65292;&#25104;&#21151;&#20811;&#26381;&#20102;&#25968;&#25454;&#31232;&#30095;&#24615;&#38382;&#39064;&#21644;&#32570;&#20047;&#27880;&#37322;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25506;&#32034;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#20013;&#30340;cloze&#20219;&#21153;&#26679;&#24335;&#35774;&#32622;&#26469;&#23398;&#20064;&#36710;&#36742;&#32500;&#20462;&#39046;&#22495;&#32452;&#20214;&#30340;&#29305;&#23450;&#26448;&#26009;&#65292;&#20197;&#20811;&#26381;&#32570;&#20047;&#27880;&#37322;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#32858;&#21512;&#20102;&#19968;&#32452;cloze&#26597;&#35810;&#27169;&#26495;&#30340;&#26174;&#33879;&#39044;&#27979;&#65292;&#24182;&#34920;&#26126;&#20351;&#29992;&#23567;&#22411;&#39640;&#36136;&#37327;&#25110;&#23450;&#21046;&#30340;&#32500;&#22522;&#30334;&#31185;&#35821;&#26009;&#24211;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#24403;&#25506;&#32034;&#36164;&#28304;&#32039;&#32570;&#30340;&#26367;&#20195;&#26041;&#26696;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;&#31934;&#31616;&#30340;PLM&#26126;&#26174;&#20248;&#20110;&#32463;&#20856;&#30340;&#22522;&#20110;&#27169;&#24335;&#30340;&#31639;&#27861;&#12290;&#27492;&#22806;&#65292;&#32771;&#34385;&#21040;&#25105;&#20204;&#39046;&#22495;&#29305;&#23450;&#32452;&#20214;&#30340;98&#65285;&#37117;&#26159;&#22810;&#35789;&#34920;&#36798;&#24335;&#65292;&#25105;&#20204;&#25104;&#21151;&#22320;&#21033;&#29992;&#32452;&#25104;&#24615;&#20551;&#35774;&#26469;&#35299;&#20915;&#25968;&#25454;&#31232;&#30095;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel approach to learn domain-specific plausible materials for components in the vehicle repair domain by probing Pretrained Language Models (PLMs) in a cloze task style setting to overcome the lack of annotated datasets. We devise a new method to aggregate salient predictions from a set of cloze query templates and show that domain-adaptation using either a small, high-quality or a customized Wikipedia corpus boosts performance. When exploring resource-lean alternatives, we find a distilled PLM clearly outperforming a classic pattern-based algorithm. Further, given that 98% of our domain-specific components are multiword expressions, we successfully exploit the compositionality assumption as a way to address data sparsity.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#30693;&#35782;&#33976;&#39311;&#30340;&#38598;&#25104;&#24314;&#27169;&#26041;&#27861;EMKD&#65292;&#23427;&#37319;&#29992;&#22810;&#20010;&#24182;&#34892;&#32593;&#32476;&#20316;&#20026;&#24207;&#21015;&#32534;&#30721;&#22120;&#65292;&#22312;&#24207;&#21015;&#25512;&#33616;&#20013;&#26681;&#25454;&#25152;&#26377;&#32593;&#32476;&#30340;&#36755;&#20986;&#20998;&#24067;&#25512;&#33616;&#29289;&#21697;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;EMKD&#22312;&#20004;&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#26174;&#33879;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.14668</link><description>&lt;p&gt;
&#22522;&#20110;&#23545;&#27604;&#30693;&#35782;&#33976;&#39311;&#30340;&#38598;&#25104;&#24314;&#27169;&#22312;&#24207;&#21015;&#25512;&#33616;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Ensemble Modeling with Contrastive Knowledge Distillation for Sequential Recommendation. (arXiv:2304.14668v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14668
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#30693;&#35782;&#33976;&#39311;&#30340;&#38598;&#25104;&#24314;&#27169;&#26041;&#27861;EMKD&#65292;&#23427;&#37319;&#29992;&#22810;&#20010;&#24182;&#34892;&#32593;&#32476;&#20316;&#20026;&#24207;&#21015;&#32534;&#30721;&#22120;&#65292;&#22312;&#24207;&#21015;&#25512;&#33616;&#20013;&#26681;&#25454;&#25152;&#26377;&#32593;&#32476;&#30340;&#36755;&#20986;&#20998;&#24067;&#25512;&#33616;&#29289;&#21697;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;EMKD&#22312;&#20004;&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#26174;&#33879;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24207;&#21015;&#25512;&#33616;&#26088;&#22312;&#25429;&#25417;&#29992;&#25143;&#30340;&#21160;&#24577;&#20852;&#36259;&#65292;&#39044;&#27979;&#29992;&#25143;&#19979;&#19968;&#27425;&#30340;&#20559;&#22909;&#29289;&#21697;&#12290;&#22810;&#25968;&#26041;&#27861;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#24207;&#21015;&#32534;&#30721;&#22120;&#29983;&#25104;&#29992;&#25143;&#21644;&#29289;&#21697;&#34920;&#31034;&#12290;&#29616;&#26377;&#24037;&#20316;&#20027;&#35201;&#20391;&#37325;&#20110;&#35774;&#35745;&#26356;&#24378;&#30340;&#24207;&#21015;&#32534;&#30721;&#22120;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#23581;&#35797;&#20351;&#29992;&#35757;&#32451;&#19968;&#32452;&#32593;&#32476;&#20316;&#20026;&#24207;&#21015;&#32534;&#30721;&#22120;&#30340;&#26041;&#27861;&#65292;&#36825;&#27604;&#21333;&#20010;&#32593;&#32476;&#26356;&#24378;&#22823;&#65292;&#22240;&#20026;&#19968;&#32452;&#24182;&#34892;&#32593;&#32476;&#21487;&#20197;&#20135;&#29983;&#22810;&#26679;&#21270;&#30340;&#39044;&#27979;&#32467;&#26524;&#65292;&#20174;&#32780;&#33719;&#24471;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#30693;&#35782;&#33976;&#39311;&#30340;&#38598;&#25104;&#24314;&#27169;&#26041;&#27861;&#65292;&#21363;EMKD&#65292;&#22312;&#24207;&#21015;&#25512;&#33616;&#20013;&#20351;&#29992;&#22810;&#20010;&#24182;&#34892;&#32593;&#32476;&#20316;&#20026;&#24207;&#21015;&#32534;&#30721;&#22120;&#65292;&#24182;&#26681;&#25454;&#25152;&#26377;&#36825;&#20123;&#32593;&#32476;&#30340;&#36755;&#20986;&#20998;&#24067;&#25512;&#33616;&#29289;&#21697;&#12290;&#20026;&#20102;&#20419;&#36827;&#24182;&#34892;&#32593;&#32476;&#20043;&#38388;&#30340;&#30693;&#35782;&#36716;&#31227;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#27604;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#23427;&#23558;&#30693;&#35782;&#20174;&#25945;&#24072;&#32593;&#32476;&#36716;&#31227;&#21040;&#22810;&#20010;&#23398;&#29983;&#32593;&#32476;&#20013;&#12290;&#22312;&#20004;&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;EMKD&#26174;&#33879;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#24207;&#21015;&#25512;&#33616;&#26041;&#27861;&#21644;&#38598;&#25104;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequential recommendation aims to capture users' dynamic interest and predicts the next item of users' preference. Most sequential recommendation methods use a deep neural network as sequence encoder to generate user and item representations. Existing works mainly center upon designing a stronger sequence encoder. However, few attempts have been made with training an ensemble of networks as sequence encoders, which is more powerful than a single network because an ensemble of parallel networks can yield diverse prediction results and hence better accuracy. In this paper, we present Ensemble Modeling with contrastive Knowledge Distillation for sequential recommendation (EMKD). Our framework adopts multiple parallel networks as an ensemble of sequence encoders and recommends items based on the output distributions of all these networks. To facilitate knowledge transfer between parallel networks, we propose a novel contrastive knowledge distillation approach, which performs knowledge tran
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#22810;&#20803;&#20998;&#24067;&#27169;&#22411;&#30340;&#20449;&#24687;&#26816;&#32034;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#26080;&#32541;&#38598;&#25104;&#21040;&#29616;&#26377;&#36817;&#20284;&#26368;&#36817;&#37051;&#31639;&#27861;&#20013;&#20197;&#23454;&#29616;&#39640;&#25928;&#26816;&#32034;&#12290;</title><link>http://arxiv.org/abs/2304.14522</link><description>&lt;p&gt;
&#20449;&#24687;&#26816;&#32034;&#30340;&#22810;&#20803;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Multivariate Representation Learning for Information Retrieval. (arXiv:2304.14522v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14522
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#22810;&#20803;&#20998;&#24067;&#27169;&#22411;&#30340;&#20449;&#24687;&#26816;&#32034;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#26080;&#32541;&#38598;&#25104;&#21040;&#29616;&#26377;&#36817;&#20284;&#26368;&#36817;&#37051;&#31639;&#27861;&#20013;&#20197;&#23454;&#29616;&#39640;&#25928;&#26816;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31264;&#23494;&#26816;&#32034;&#27169;&#22411;&#20351;&#29992;&#21452;&#32534;&#30721;&#22120;&#32593;&#32476;&#26550;&#26500;&#26469;&#23398;&#20064;&#26597;&#35810;&#21644;&#25991;&#26723;&#30340;&#34920;&#31034;&#24418;&#24335;&#65292;&#36825;&#20123;&#34920;&#31034;&#24418;&#24335;&#36890;&#24120;&#37319;&#29992;&#21521;&#37327;&#34920;&#31034;&#65292;&#23427;&#20204;&#30340;&#30456;&#20284;&#24615;&#36890;&#24120;&#20351;&#29992;&#28857;&#31215;&#20989;&#25968;&#35745;&#31639;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#31264;&#23494;&#26816;&#32034;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#19981;&#26159;&#23398;&#20064;&#27599;&#20010;&#26597;&#35810;&#21644;&#25991;&#26723;&#30340;&#21521;&#37327;&#65292;&#32780;&#26159;&#23398;&#20064;&#22810;&#20803;&#20998;&#24067;&#65292;&#24182;&#20351;&#29992;&#36127;&#22810;&#20803;KL&#25955;&#24230;&#35745;&#31639;&#20998;&#24067;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#20026;&#20102;&#31616;&#21270;&#21644;&#25552;&#39640;&#25928;&#29575;&#65292;&#25105;&#20204;&#20551;&#35774;&#36825;&#20123;&#20998;&#24067;&#26159;&#22810;&#32500;&#27491;&#24577;&#20998;&#24067;&#65292;&#28982;&#21518;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#29983;&#25104;&#36825;&#20123;&#20998;&#24067;&#30340;&#22343;&#20540;&#21644;&#26041;&#24046;&#21521;&#37327;&#12290;&#25105;&#20204;&#20026;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#25552;&#20379;&#20102;&#29702;&#35770;&#22522;&#30784;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#21487;&#20197;&#26080;&#32541;&#22320;&#38598;&#25104;&#21040;&#29616;&#26377;&#30340;&#36817;&#20284;&#26368;&#36817;&#37051;&#31639;&#27861;&#20013;&#20197;&#23454;&#29616;&#39640;&#25928;&#26816;&#32034;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#35206;&#30422;&#20102;&#21508;&#31181;&#19981;&#21516;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dense retrieval models use bi-encoder network architectures for learning query and document representations. These representations are often in the form of a vector representation and their similarities are often computed using the dot product function. In this paper, we propose a new representation learning framework for dense retrieval. Instead of learning a vector for each query and document, our framework learns a multivariate distribution and uses negative multivariate KL divergence to compute the similarity between distributions. For simplicity and efficiency reasons, we assume that the distributions are multivariate normals and then train large language models to produce mean and variance vectors for these distributions. We provide a theoretical foundation for the proposed framework and show that it can be seamlessly integrated into the existing approximate nearest neighbor algorithms to perform retrieval efficiently. We conduct an extensive suite of experiments on a wide range 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#20020;&#24202;&#29615;&#22659;&#20013;&#20351;&#29992;GPT-3.5&#21644;GPT-4&#35299;&#20915;&#21307;&#23398;&#38382;&#39064;&#30340;&#23433;&#20840;&#24615;&#20197;&#21450;&#19982;&#20449;&#24687;&#25216;&#26415;&#21672;&#35810;&#26381;&#21153;&#25253;&#21578;&#30340;&#19968;&#33268;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20004;&#20010;LLMs&#37117;&#21487;&#20197;&#20197;&#23433;&#20840;&#21644;&#19968;&#33268;&#30340;&#26041;&#24335;&#28385;&#36275;&#21307;&#29983;&#30340;&#20449;&#24687;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2304.13714</link><description>&lt;p&gt;
&#35780;&#20272;GPT-3.5&#21644;GPT-4&#22312;&#25903;&#25345;&#21307;&#30103;&#20445;&#20581;&#20449;&#24687;&#38656;&#27714;&#26041;&#38754;&#30340;&#23454;&#38469;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Evaluation of GPT-3.5 and GPT-4 for supporting real-world information needs in healthcare delivery. (arXiv:2304.13714v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13714
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#20020;&#24202;&#29615;&#22659;&#20013;&#20351;&#29992;GPT-3.5&#21644;GPT-4&#35299;&#20915;&#21307;&#23398;&#38382;&#39064;&#30340;&#23433;&#20840;&#24615;&#20197;&#21450;&#19982;&#20449;&#24687;&#25216;&#26415;&#21672;&#35810;&#26381;&#21153;&#25253;&#21578;&#30340;&#19968;&#33268;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20004;&#20010;LLMs&#37117;&#21487;&#20197;&#20197;&#23433;&#20840;&#21644;&#19968;&#33268;&#30340;&#26041;&#24335;&#28385;&#36275;&#21307;&#29983;&#30340;&#20449;&#24687;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#65292;&#20294;&#24403;&#21069;&#30340;&#25506;&#32034;&#24182;&#26410;&#35780;&#20272;LLMs&#22312;&#20020;&#24202;&#29615;&#22659;&#20013;&#30340;&#23454;&#29992;&#24615;&#21644;&#23433;&#20840;&#24615;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#30830;&#23450;&#20004;&#20010;LLM&#26159;&#21542;&#21487;&#20197;&#20197;&#23433;&#20840;&#21644;&#19968;&#33268;&#30340;&#26041;&#24335;&#28385;&#36275;&#30001;&#21307;&#29983;&#25552;&#20132;&#30340;&#20449;&#24687;&#38656;&#27714;&#38382;&#39064;&#12290;&#25105;&#20204;&#23558;66&#20010;&#26469;&#33258;&#20449;&#24687;&#25216;&#26415;&#21672;&#35810;&#26381;&#21153;&#30340;&#38382;&#39064;&#36890;&#36807;&#31616;&#21333;&#30340;&#25552;&#31034;&#25552;&#20132;&#32473;GPT-3.5&#21644;GPT-4&#12290;12&#21517;&#21307;&#29983;&#35780;&#20272;&#20102;LLM&#21709;&#24212;&#23545;&#24739;&#32773;&#36896;&#25104;&#20260;&#23475;&#30340;&#21487;&#33021;&#24615;&#20197;&#21450;&#19982;&#20449;&#24687;&#25216;&#26415;&#21672;&#35810;&#26381;&#21153;&#30340;&#29616;&#26377;&#25253;&#21578;&#30340;&#19968;&#33268;&#24615;&#12290;&#21307;&#29983;&#30340;&#35780;&#20272;&#22522;&#20110;&#22810;&#25968;&#31080;&#27719;&#24635;&#12290;&#23545;&#20110;&#27809;&#26377;&#20219;&#20309;&#38382;&#39064;&#65292;&#22823;&#22810;&#25968;&#21307;&#29983;&#35748;&#20026;&#20219;&#20309;&#19968;&#20010;LLM&#21709;&#24212;&#37117;&#19981;&#20250;&#36896;&#25104;&#20260;&#23475;&#12290;&#23545;&#20110;GPT-3.5&#65292;8&#20010;&#38382;&#39064;&#30340;&#21709;&#24212;&#19982;&#20449;&#24687;&#25216;&#26415;&#21672;&#35810;&#25253;&#21578;&#19968;&#33268;&#65292;20&#20010;&#19981;&#19968;&#33268;&#65292;9&#20010;&#26080;&#27861;&#35780;&#20272;&#12290;&#26377;29&#20010;&#21709;&#24212;&#27809;&#26377;&#22810;&#25968;&#31080;&#34920;&#31034;&#8220;&#21516;&#24847;&#8221;&#12289;&#8220;&#19981;&#21516;&#24847;&#8221;&#21644;&#8220;&#26080;&#27861;&#35780;&#20272;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite growing interest in using large language models (LLMs) in healthcare, current explorations do not assess the real-world utility and safety of LLMs in clinical settings. Our objective was to determine whether two LLMs can serve information needs submitted by physicians as questions to an informatics consultation service in a safe and concordant manner. Sixty six questions from an informatics consult service were submitted to GPT-3.5 and GPT-4 via simple prompts. 12 physicians assessed the LLM responses' possibility of patient harm and concordance with existing reports from an informatics consultation service. Physician assessments were summarized based on majority vote. For no questions did a majority of physicians deem either LLM response as harmful. For GPT-3.5, responses to 8 questions were concordant with the informatics consult report, 20 discordant, and 9 were unable to be assessed. There were 29 responses with no majority on "Agree", "Disagree", and "Unable to assess". Fo
&lt;/p&gt;</description></item></channel></rss>