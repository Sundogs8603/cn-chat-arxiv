<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25552;&#21462;&#21306;&#22359;&#38142;&#39046;&#22495;&#30340;&#20449;&#24687;&#24182;&#32452;&#32455;&#65292;&#20197;&#24110;&#21161;&#29992;&#25143;&#27983;&#35272;&#35813;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2305.10408</link><description>&lt;p&gt;
&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#21306;&#22359;&#38142;&#27010;&#24565;
&lt;/p&gt;
&lt;p&gt;
Extracting Blockchain Concepts from Text. (arXiv:2305.10408v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10408
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25552;&#21462;&#21306;&#22359;&#38142;&#39046;&#22495;&#30340;&#20449;&#24687;&#24182;&#32452;&#32455;&#65292;&#20197;&#24110;&#21161;&#29992;&#25143;&#27983;&#35272;&#35813;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21306;&#22359;&#38142;&#25552;&#20379;&#20102;&#19968;&#31181;&#26426;&#21046;&#65292;&#36890;&#36807;&#35813;&#26426;&#21046;&#65292;&#30456;&#20114;&#19981;&#20449;&#20219;&#30340;&#36828;&#31243;&#26041;&#21487;&#20197;&#23601;&#20449;&#24687;&#20998;&#31867;&#36134;&#30340;&#29366;&#24577;&#36798;&#25104;&#20849;&#35782;&#12290;&#38543;&#30528;&#36825;&#20010;&#39046;&#22495;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#38656;&#35201;&#23398;&#20064;&#21306;&#22359;&#38142;&#30340;&#20154;&#20063;&#36234;&#26469;&#36234;&#22810;&#12290;&#30001;&#20110;&#36825;&#26159;&#19968;&#20010;&#25216;&#26415;&#24615;&#30340;&#20027;&#39064;&#65292;&#24320;&#22987;&#23398;&#20064;&#21487;&#33021;&#20250;&#24863;&#21040;&#30456;&#24403;&#19981;&#21487;&#24605;&#35758;&#12290;&#22240;&#27492;&#65292;&#35813;&#39033;&#30446;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20174;&#30333;&#30382;&#20070;&#21644;&#23398;&#26415;&#35770;&#25991;&#20013;&#25552;&#21462;&#20851;&#20110;&#21306;&#22359;&#38142;&#39046;&#22495;&#30340;&#20449;&#24687;&#65292;&#20197;&#32452;&#32455;&#36825;&#20123;&#20449;&#24687;&#24182;&#24110;&#21161;&#29992;&#25143;&#27983;&#35272;&#35813;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Blockchains provide a mechanism through which mutually distrustful remote parties can reach consensus on the state of a ledger of information. With the great acceleration with which this space is developed, the demand for those seeking to learn about blockchain also grows. Being a technical subject, it can be quite intimidating to start learning. For this reason, the main objective of this project was to apply machine learning models to extract information from whitepapers and academic articles focused on the blockchain area to organize this information and aid users to navigate the space.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;GPT-4&#36827;&#34892;&#22823;&#35268;&#27169;&#25991;&#26412;&#20998;&#26512;&#65292;&#22312;US AI&#19987;&#21033;&#20013;&#21457;&#29616;&#20844;&#20849;&#20215;&#20540;&#34920;&#36798;&#12290;&#37319;&#29992;&#39640;&#32423;&#24067;&#23572;&#26597;&#35810;&#25910;&#38598;&#20102;154,934&#20010;&#19987;&#21033;&#25991;&#26723;&#65292;&#24182;&#19982;USPTO&#30340;&#23436;&#25972;&#19987;&#21033;&#25991;&#26412;&#21512;&#24182;&#12290;&#24471;&#20986;5.4&#30334;&#19975;&#21477;&#23376;&#30340;&#35821;&#26009;&#24211;&#65292;&#20351;&#29992;&#26694;&#26550;&#20197;&#21450;GPT-4&#25552;&#31034;&#36827;&#34892;&#26631;&#35760;&#21644;&#29702;&#24615;&#21270;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#24456;&#20934;&#30830;&#12290;</title><link>http://arxiv.org/abs/2305.10383</link><description>&lt;p&gt;
&#20351;&#29992;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22823;&#35268;&#27169;&#25991;&#26412;&#20998;&#26512;&#65306;&#22312;AI&#19987;&#21033;&#20013;&#21457;&#29616;&#20844;&#20849;&#20215;&#20540;&#34920;&#36798;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Large-Scale Text Analysis Using Generative Language Models: A Case Study in Discovering Public Value Expressions in AI Patents. (arXiv:2305.10383v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10383
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;GPT-4&#36827;&#34892;&#22823;&#35268;&#27169;&#25991;&#26412;&#20998;&#26512;&#65292;&#22312;US AI&#19987;&#21033;&#20013;&#21457;&#29616;&#20844;&#20849;&#20215;&#20540;&#34920;&#36798;&#12290;&#37319;&#29992;&#39640;&#32423;&#24067;&#23572;&#26597;&#35810;&#25910;&#38598;&#20102;154,934&#20010;&#19987;&#21033;&#25991;&#26723;&#65292;&#24182;&#19982;USPTO&#30340;&#23436;&#25972;&#19987;&#21033;&#25991;&#26412;&#21512;&#24182;&#12290;&#24471;&#20986;5.4&#30334;&#19975;&#21477;&#23376;&#30340;&#35821;&#26009;&#24211;&#65292;&#20351;&#29992;&#26694;&#26550;&#20197;&#21450;GPT-4&#25552;&#31034;&#36827;&#34892;&#26631;&#35760;&#21644;&#29702;&#24615;&#21270;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#24456;&#20934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#35760;&#25968;&#25454;&#23545;&#20110;&#35757;&#32451;&#25991;&#26412;&#20998;&#31867;&#22120;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#23545;&#20110;&#22797;&#26434;&#21644;&#25277;&#35937;&#30340;&#27010;&#24565;&#32780;&#35328;&#65292;&#20934;&#30830;&#26631;&#35760;&#24120;&#24120;&#24456;&#38590;&#23454;&#29616;&#12290;&#26412;&#25991;&#37319;&#29992;&#19968;&#31181;&#26032;&#39062;&#26041;&#27861;&#65292;&#20351;&#29992;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#65288;GPT-4&#65289;&#36827;&#34892;&#22823;&#35268;&#27169;&#25991;&#26412;&#20998;&#26512;&#30340;&#26631;&#35760;&#21644;&#29702;&#24615;&#21270;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#24212;&#29992;&#20110;&#22312;&#32654;&#22269;AI&#19987;&#21033;&#20013;&#21457;&#29616;&#20844;&#20849;&#20215;&#20540;&#34920;&#36798;&#30340;&#20219;&#21153;&#19978;&#12290;&#25105;&#20204;&#20351;&#29992;&#22312;InnovationQ+&#19978;&#25552;&#20132;&#30340;&#39640;&#32423;&#24067;&#23572;&#26597;&#35810;&#25910;&#38598;&#20102;&#19968;&#20010;&#21253;&#21547;154,934&#20010;&#19987;&#21033;&#25991;&#26723;&#30340;&#25968;&#25454;&#24211;&#65292;&#36825;&#20123;&#32467;&#26524;&#19982;&#26469;&#33258;USPTO&#30340;&#23436;&#25972;&#19987;&#21033;&#25991;&#26412;&#21512;&#24182;&#65292;&#24635;&#35745;5.4&#30334;&#19975;&#21477;&#23376;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#35782;&#21035;&#21644;&#26631;&#35760;&#36825;&#20123;AI&#19987;&#21033;&#21477;&#23376;&#20013;&#30340;&#20844;&#20849;&#20215;&#20540;&#34920;&#36798;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;GPT-4&#30340;&#25552;&#31034;&#65292;&#20854;&#20013;&#21253;&#25324;&#25991;&#26412;&#20998;&#31867;&#30340;&#23450;&#20041;&#12289;&#25351;&#23548;&#26041;&#38024;&#12289;&#31034;&#20363;&#21644;&#29702;&#24615;&#21270;&#12290;&#25105;&#20204;&#20351;&#29992;BLEU&#20998;&#25968;&#21644;&#20027;&#39064;&#24314;&#27169;&#35780;&#20272;&#20102;GPT-4&#29983;&#25104;&#30340;&#26631;&#31614;&#21644;&#29702;&#24615;&#21270;&#30340;&#36136;&#37327;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#26159;&#20934;&#30830;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Labeling data is essential for training text classifiers but is often difficult to accomplish accurately, especially for complex and abstract concepts. Seeking an improved method, this paper employs a novel approach using a generative language model (GPT-4) to produce labels and rationales for large-scale text analysis. We apply this approach to the task of discovering public value expressions in US AI patents. We collect a database comprising 154,934 patent documents using an advanced Boolean query submitted to InnovationQ+. The results are merged with full patent text from the USPTO, resulting in 5.4 million sentences. We design a framework for identifying and labeling public value expressions in these AI patent sentences. A prompt for GPT-4 is developed which includes definitions, guidelines, examples, and rationales for text classification. We evaluate the quality of the labels and rationales produced by GPT-4 using BLEU scores and topic modeling and find that they are accurate, di
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#24773;&#24863;&#25903;&#25345;&#23545;&#35805;&#31995;&#32479;&#30340;&#28151;&#21512;&#20027;&#21160;&#29305;&#28857;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#30693;&#35782;&#22686;&#24378;&#30340;&#28151;&#21512;&#20027;&#21160;&#23545;&#35805;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20174;&#22823;&#22411;&#24515;&#29702;&#20581;&#24247;&#30693;&#35782;&#22270;&#35889;&#20013;&#26816;&#32034;&#23454;&#38469;&#26696;&#20363;&#30693;&#35782;&#26469;&#29983;&#25104;&#28151;&#21512;&#20027;&#21160;&#21709;&#24212;&#65292;&#24182;&#22312;&#20849;&#24773;&#21644;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#20960;&#20010;&#22522;&#32447;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.10172</link><description>&lt;p&gt;
&#22522;&#20110;&#30693;&#35782;&#22686;&#24378;&#30340;&#28151;&#21512;&#20027;&#21160;&#23545;&#35805;&#24773;&#24863;&#25903;&#25345;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Knowledge-enhanced Mixed-initiative Dialogue System for Emotional Support Conversations. (arXiv:2305.10172v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10172
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#24773;&#24863;&#25903;&#25345;&#23545;&#35805;&#31995;&#32479;&#30340;&#28151;&#21512;&#20027;&#21160;&#29305;&#28857;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#30693;&#35782;&#22686;&#24378;&#30340;&#28151;&#21512;&#20027;&#21160;&#23545;&#35805;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20174;&#22823;&#22411;&#24515;&#29702;&#20581;&#24247;&#30693;&#35782;&#22270;&#35889;&#20013;&#26816;&#32034;&#23454;&#38469;&#26696;&#20363;&#30693;&#35782;&#26469;&#29983;&#25104;&#28151;&#21512;&#20027;&#21160;&#21709;&#24212;&#65292;&#24182;&#22312;&#20849;&#24773;&#21644;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#20960;&#20010;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#20849;&#24773;&#23545;&#35805;&#19981;&#21516;&#65292;&#24773;&#24863;&#25903;&#25345;&#23545;&#35805;&#31995;&#32479;&#38656;&#35201;&#22312;&#23433;&#24944;&#27714;&#21161;&#32773;&#30340;&#21516;&#26102;&#20027;&#21160;&#24110;&#21161;&#25506;&#32034;&#21644;&#35299;&#20915;&#38382;&#39064;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#28151;&#21512;&#20027;&#21160;&#24773;&#24863;&#25903;&#25345;&#23545;&#35805;&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;&#29992;&#25143;&#21644;&#31995;&#32479;&#37117;&#21487;&#20197;&#22312;&#23545;&#35805;&#20013;&#37319;&#21462;&#20027;&#21160;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#28151;&#21512;&#20027;&#21160;&#24773;&#24863;&#25903;&#25345;&#23545;&#35805;&#30340;&#26032;&#22411;&#27169;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;&#22235;&#20010;&#24773;&#24863;&#25903;&#25345;&#25351;&#26631;&#26469;&#35780;&#20215;&#28151;&#21512;&#20027;&#21160;&#20132;&#20114;&#12290;&#20998;&#26512;&#25581;&#31034;&#20102;&#26500;&#24314;&#28151;&#21512;&#20027;&#21160;&#24773;&#24863;&#25903;&#25345;&#23545;&#35805;&#31995;&#32479;&#30340;&#24517;&#35201;&#24615;&#21644;&#25361;&#25112;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#30693;&#35782;&#22686;&#24378;&#30340;&#28151;&#21512;&#20027;&#21160;&#23545;&#35805;&#26694;&#26550;&#65288;KEMI&#65289;&#65292;&#35813;&#26694;&#26550;&#20174;&#22823;&#22411;&#24515;&#29702;&#20581;&#24247;&#30693;&#35782;&#22270;&#35889;&#20013;&#26816;&#32034;&#23454;&#38469;&#26696;&#20363;&#30693;&#35782;&#26469;&#29983;&#25104;&#28151;&#21512;&#20027;&#21160;&#21709;&#24212;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;KEMI&#22312;&#20849;&#24773;&#21644;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#20960;&#20010;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unlike empathetic dialogues, the system in emotional support conversations (ESC) is expected to not only convey empathy for comforting the help-seeker, but also proactively assist in exploring and addressing their problems during the conversation. In this work, we study the problem of mixed-initiative ESC where the user and system can both take the initiative in leading the conversation. Specifically, we conduct a novel analysis on mixed-initiative ESC systems with a tailor-designed schema that divides utterances into different types with speaker roles and initiative types. Four emotional support metrics are proposed to evaluate the mixed-initiative interactions. The analysis reveals the necessity and challenges of building mixed-initiative ESC systems. In the light of this, we propose a knowledge-enhanced mixed-initiative framework (KEMI) for ESC, which retrieves actual case knowledge from a large-scale mental health knowledge graph for generating mixed-initiative responses. Experimen
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;PropensityNet&#65292;&#29992;&#20110;&#22312;&#24378;&#26085;&#24535;&#35760;&#24405;&#31574;&#30053;&#19979;&#36827;&#34892;&#26080;&#20559;&#23398;&#20064;&#25490;&#21517;&#65288;ULTR&#65289;&#30340;&#20542;&#21521;&#24615;&#20272;&#35745;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;ULTR&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.09918</link><description>&lt;p&gt;
&#26080;&#20559;&#20542;&#21521;&#20272;&#35745;&#29992;&#20110;&#26080;&#20559;&#25490;&#24207;
&lt;/p&gt;
&lt;p&gt;
Unconfounded Propensity Estimation for Unbiased Ranking. (arXiv:2305.09918v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09918
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;PropensityNet&#65292;&#29992;&#20110;&#22312;&#24378;&#26085;&#24535;&#35760;&#24405;&#31574;&#30053;&#19979;&#36827;&#34892;&#26080;&#20559;&#23398;&#20064;&#25490;&#21517;&#65288;ULTR&#65289;&#30340;&#20542;&#21521;&#24615;&#20272;&#35745;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;ULTR&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#20559;&#23398;&#20064;&#25490;&#21517;&#65288;ULTR&#65289;&#30340;&#30446;&#26631;&#26159;&#21033;&#29992;&#38544;&#21547;&#30340;&#29992;&#25143;&#21453;&#39304;&#26469;&#20248;&#21270;&#23398;&#20064;&#25490;&#24207;&#31995;&#32479;&#12290;&#22312;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#20013;&#65292;&#33258;&#21160;ULTR&#31639;&#27861;&#22312;&#23454;&#36341;&#20013;&#22240;&#20854;&#21331;&#36234;&#30340;&#24615;&#33021;&#21644;&#20302;&#37096;&#32626;&#25104;&#26412;&#32780;&#21463;&#21040;&#20851;&#27880;&#65292;&#35813;&#31639;&#27861;&#21516;&#26102;&#23398;&#20064;&#29992;&#25143;&#20559;&#24046;&#27169;&#22411;&#65288;&#21363;&#20542;&#21521;&#24615;&#27169;&#22411;&#65289;&#21644;&#26080;&#20559;&#25490;&#21517;&#22120;&#12290;&#23613;&#31649;&#35813;&#31639;&#27861;&#22312;&#29702;&#35770;&#19978;&#26159;&#21487;&#38752;&#30340;&#65292;&#20294;&#20854;&#26377;&#25928;&#24615;&#36890;&#24120;&#22312;&#24369;&#26085;&#24535;&#35760;&#24405;&#31574;&#30053;&#19979;&#36827;&#34892;&#39564;&#35777;&#65292;&#20854;&#20013;&#25490;&#21517;&#27169;&#22411;&#20960;&#20046;&#26080;&#27861;&#26681;&#25454;&#19982;&#26597;&#35810;&#30456;&#20851;&#24615;&#26469;&#23545;&#25991;&#26723;&#36827;&#34892;&#25490;&#21517;&#12290;&#28982;&#32780;&#65292;&#24403;&#26085;&#24535;&#35760;&#24405;&#31574;&#30053;&#24456;&#24378;&#26102;&#65292;&#20363;&#22914;&#24037;&#19994;&#37096;&#32626;&#30340;&#25490;&#21517;&#31574;&#30053;&#65292;&#25152;&#25253;&#21578;&#30340;&#26377;&#25928;&#24615;&#26080;&#27861;&#20877;&#29616;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#20174;&#22240;&#26524;&#35282;&#24230;&#35843;&#26597;ULTR&#65292;&#24182;&#25581;&#31034;&#19968;&#20010;&#36127;&#38754;&#32467;&#26524;&#65306;&#29616;&#26377;&#30340;ULTR&#31639;&#27861;&#26410;&#33021;&#35299;&#20915;&#30001;&#26597;&#35810;-&#25991;&#26723;&#30456;&#20851;&#24615;&#28151;&#28102;&#23548;&#33268;&#30340;&#20542;&#21521;&#24615;&#39640;&#20272;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21453;&#38376;&#35843;&#25972;&#30340;&#26032;&#30340;&#23398;&#20064;&#30446;&#26631;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PropensityNet&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#24378;&#26085;&#24535;&#35760;&#24405;&#31574;&#30053;&#19979;&#20026;ULTR&#20272;&#35745;&#26080;&#20559;&#30340;&#20542;&#21521;&#24615;&#20998;&#25968;&#12290;&#22810;&#20010;&#25968;&#25454;&#38598;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;PropensityNet&#22312;&#24378;&#26085;&#24535;&#35760;&#24405;&#31574;&#30053;&#21644;&#24369;&#26085;&#24535;&#35760;&#24405;&#31574;&#30053;&#19979;&#22343;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;ULTR&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of unbiased learning to rank~(ULTR) is to leverage implicit user feedback for optimizing learning-to-rank systems. Among existing solutions, automatic ULTR algorithms that jointly learn user bias models (\ie propensity models) with unbiased rankers have received a lot of attention due to their superior performance and low deployment cost in practice. Despite their theoretical soundness, the effectiveness is usually justified under a weak logging policy, where the ranking model can barely rank documents according to their relevance to the query. However, when the logging policy is strong, e.g., an industry-deployed ranking policy, the reported effectiveness cannot be reproduced. In this paper, we first investigate ULTR from a causal perspective and uncover a negative result: existing ULTR algorithms fail to address the issue of propensity overestimation caused by the query-document relevance confounder. Then, we propose a new learning objective based on backdoor adjustment and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#37319;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#20851;&#38190;&#35789;&#24863;&#30693;&#20132;&#21449;&#32534;&#30721;&#22120;&#25490;&#24207;&#25688;&#35201;&#31243;&#24207;&#65292;&#20174;&#25991;&#26412;&#20869;&#23481;&#20013;&#25552;&#21462;&#35821;&#20041;&#20449;&#24687;&#65292;&#24182;&#24230;&#37327; GIS&amp;T BoK &#35805;&#39064;&#20043;&#38388;&#30340;&#35821;&#20041;&#30456;&#20284;&#24230;&#65292;&#20197;&#35299;&#20915;&#25163;&#21160;&#23450;&#20041;&#35805;&#39064;&#20851;&#31995;&#24102;&#26469;&#30340;&#19981;&#23436;&#25972;&#35780;&#20272;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#22312;&#20934;&#30830;&#24230;&#37327;&#35805;&#39064;&#20851;&#31995;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#23545; GIS&amp;T &#39046;&#22495;&#30340;&#30740;&#31350;&#21644;&#23454;&#36341;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2305.09877</link><description>&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#21644;&#20851;&#38190;&#35789;&#24863;&#30693;&#20132;&#21449;&#32534;&#30721;&#22120;&#25490;&#24207;&#25688;&#35201;&#31243;&#24207;&#30340;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#35821;&#20041;&#30456;&#20284;&#24230;&#24230;&#37327;&#8212;&#8212;&#20197;UCGIS GIS&amp;T&#30693;&#35782;&#20307;&#31995;&#20026;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Semantic Similarity Measure of Natural Language Text through Machine Learning and a Keyword-Aware Cross-Encoder-Ranking Summarizer -- A Case Study Using UCGIS GIS&amp;T Body of Knowledge. (arXiv:2305.09877v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09877
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#37319;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#20851;&#38190;&#35789;&#24863;&#30693;&#20132;&#21449;&#32534;&#30721;&#22120;&#25490;&#24207;&#25688;&#35201;&#31243;&#24207;&#65292;&#20174;&#25991;&#26412;&#20869;&#23481;&#20013;&#25552;&#21462;&#35821;&#20041;&#20449;&#24687;&#65292;&#24182;&#24230;&#37327; GIS&amp;T BoK &#35805;&#39064;&#20043;&#38388;&#30340;&#35821;&#20041;&#30456;&#20284;&#24230;&#65292;&#20197;&#35299;&#20915;&#25163;&#21160;&#23450;&#20041;&#35805;&#39064;&#20851;&#31995;&#24102;&#26469;&#30340;&#19981;&#23436;&#25972;&#35780;&#20272;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#22312;&#20934;&#30830;&#24230;&#37327;&#35805;&#39064;&#20851;&#31995;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#23545; GIS&amp;T &#39046;&#22495;&#30340;&#30740;&#31350;&#21644;&#23454;&#36341;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
GIS&amp;T &#30693;&#35782;&#20307;&#31995;&#26159;&#30001;&#22320;&#29702;&#20449;&#24687;&#31185;&#23398;&#19982;&#25216;&#26415;&#30456;&#20851;&#22242;&#20307;&#21457;&#36215;&#30340;&#19968;&#20010;&#31038;&#21306;&#39033;&#30446;&#65292;&#26088;&#22312;&#23450;&#20041;&#12289;&#24320;&#21457;&#21644;&#35760;&#24405;&#22320;&#29702;&#20449;&#24687;&#31185;&#23398;&#19982;&#25216;&#26415;&#30456;&#20851;&#35805;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#37319;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#20851;&#38190;&#35789;&#24863;&#30693;&#20132;&#21449;&#32534;&#30721;&#22120;&#25490;&#24207;&#25688;&#35201;&#31243;&#24207;&#65292;&#20174;&#25991;&#26412;&#20869;&#23481;&#20013;&#25552;&#21462;&#35821;&#20041;&#20449;&#24687;&#65292;&#24182;&#24230;&#37327; BoK &#35805;&#39064;&#20043;&#38388;&#30340;&#35821;&#20041;&#30456;&#20284;&#24230;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35782;&#21035; BoK &#35805;&#39064;&#20043;&#38388;&#30340;&#35821;&#20041;&#30456;&#20284;&#24230;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182; NLP &#25216;&#26415;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#33258;&#21160;&#19988;&#20934;&#30830;&#22320;&#24230;&#37327;&#35805;&#39064;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20174;&#32780;&#20351; GIS&amp;T &#39046;&#22495;&#30340;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#21463;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Initiated by the University Consortium of Geographic Information Science (UCGIS), GIS&amp;T Body of Knowledge (BoK) is a community-driven endeavor to define, develop, and document geospatial topics related to geographic information science and technologies (GIS&amp;T). In recent years, GIS&amp;T BoK has undergone rigorous development in terms of its topic re-organization and content updating, resulting in a new digital version of the project. While the BoK topics provide useful materials for researchers and students to learn about GIS, the semantic relationships among the topics, such as semantic similarity, should also be identified so that a better and automated topic navigation can be achieved. Currently, the related topics are either defined manually by editors or authors, which may result in an incomplete assessment of topic relationship. To address this challenge, our research evaluates the effectiveness of multiple natural language processing (NLP) techniques in extracting semantics from te
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#30693;&#35782;&#22270;&#35889;&#20013;&#20851;&#31995;&#26631;&#27880;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#24378;&#22823;&#30340;&#23398;&#20064;&#33021;&#21147;&#20197;&#21450;&#22312;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#19979;&#39044;&#27979;&#20135;&#21697;&#31867;&#22411;&#20043;&#38388;&#20851;&#31995;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.09858</link><description>&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#27169;&#22411;&#26159;&#23569;&#26679;&#26412;&#23398;&#20064;&#32773;&#65306;&#20197; LLMS &#22312;&#30005;&#21830;&#20013;&#30340;&#20851;&#31995;&#26631;&#27880;&#20026;&#20363;&#30340;&#32463;&#39564;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graph Completion Models are Few-shot Learners: An Empirical Study of Relation Labeling in E-commerce with LLMs. (arXiv:2305.09858v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09858
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#30693;&#35782;&#22270;&#35889;&#20013;&#20851;&#31995;&#26631;&#27880;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#24378;&#22823;&#30340;&#23398;&#20064;&#33021;&#21147;&#20197;&#21450;&#22312;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#19979;&#39044;&#27979;&#20135;&#21697;&#31867;&#22411;&#20043;&#38388;&#20851;&#31995;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#22312;&#22686;&#24378;&#30005;&#23376;&#21830;&#21153;&#31995;&#32479;&#24615;&#33021;&#26041;&#38754;&#21457;&#25381;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#23454;&#20307;&#21450;&#20854;&#20851;&#31995;&#30340;&#32467;&#26500;&#21270;&#20449;&#24687;&#65292;&#20363;&#22914;&#20135;&#21697;&#25110;&#20135;&#21697;&#31867;&#22411;&#20043;&#38388;&#30340;&#20114;&#34917;&#25110;&#26367;&#20195;&#20851;&#31995;&#65292;&#36825;&#20123;&#20449;&#24687;&#21487;&#20197;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#21033;&#29992;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#30005;&#23376;&#21830;&#21153;&#39046;&#22495;&#30340;&#21160;&#24577;&#24615;&#21644;&#20154;&#21147;&#25104;&#26412;&#30456;&#20851;&#30340;&#21407;&#22240;&#65292;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#20851;&#31995;&#26631;&#27880;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#31361;&#30772;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20986;&#20046;&#24847;&#26009;&#30340;&#32467;&#26524;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#20010;&#20851;&#20110; LLM &#22312;&#30005;&#23376;&#21830;&#21153;&#30693;&#35782;&#22270;&#35889;&#20013;&#36827;&#34892;&#20851;&#31995;&#26631;&#27880;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#30740;&#31350;&#23427;&#20204;&#22312;&#33258;&#28982;&#35821;&#35328;&#26041;&#38754;&#24378;&#22823;&#30340;&#23398;&#20064;&#33021;&#21147;&#20197;&#21450;&#22312;&#26377;&#38480;&#26631;&#35760;&#25968;&#25454;&#19979;&#39044;&#27979;&#20135;&#21697;&#31867;&#22411;&#20043;&#38388;&#20851;&#31995;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#21508;&#31181; LLM&#65292;&#21253;&#25324; PaLM &#21644; GPT-3.5&#65292;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#65292;&#35777;&#26126;&#23427;&#20204;&#33021;&#22815;&#36798;&#21040;&#19982;&#20154;&#31867;&#30456;&#24403;&#30340;&#20851;&#31995;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graphs (KGs) play a crucial role in enhancing e-commerce system performance by providing structured information about entities and their relationships, such as complementary or substitutable relations between products or product types, which can be utilized in recommender systems. However, relation labeling in KGs remains a challenging task due to the dynamic nature of e-commerce domains and the associated cost of human labor. Recently, breakthroughs in Large Language Models (LLMs) have shown surprising results in numerous natural language processing tasks. In this paper, we conduct an empirical study of LLMs for relation labeling in e-commerce KGs, investigating their powerful learning capabilities in natural language and effectiveness in predicting relations between product types with limited labeled data. We evaluate various LLMs, including PaLM and GPT-3.5, on benchmark datasets, demonstrating their ability to achieve competitive performance compared to humans on relation
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35762;&#36848;&#20102;&#25968;&#25454;&#20559;&#24046;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#12289;&#24433;&#21709;&#21450;&#21487;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;</title><link>http://arxiv.org/abs/2305.09686</link><description>&lt;p&gt;
&#25968;&#25454;&#20559;&#24046;&#31649;&#29702;
&lt;/p&gt;
&lt;p&gt;
Data Bias Management. (arXiv:2305.09686v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09686
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35762;&#36848;&#20102;&#25968;&#25454;&#20559;&#24046;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#12289;&#24433;&#21709;&#21450;&#21487;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#25968;&#25454;&#39537;&#21160;&#31995;&#32479;&#22312;&#25105;&#20204;&#26085;&#24120;&#29983;&#27963;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#20559;&#24046;&#21644;&#20844;&#24179;&#31561;&#27010;&#24565;&#22312;&#31185;&#30740;&#20154;&#21592;&#21644;&#20174;&#19994;&#20154;&#21592;&#65292;&#26080;&#35770;&#26159;&#22312;&#20135;&#19994;&#30028;&#36824;&#26159;&#23398;&#26415;&#30028;&#20013;&#65292;&#37117;&#21463;&#21040;&#20102;&#37325;&#35270;&#12290;&#36825;&#20123;&#38382;&#39064;&#36890;&#24120;&#28304;&#20110;&#29992;&#20110;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#25968;&#25454;&#36136;&#37327;&#19981;&#21516;&#12290;&#38543;&#30528;&#36825;&#20123;&#31995;&#32479;&#34987;&#21830;&#19994;&#21270;&#21644;&#37096;&#32626;&#65292;&#26377;&#26102;&#34987;&#22996;&#25176;&#20570;&#20986;&#25913;&#21464;&#29983;&#27963;&#30340;&#20915;&#31574;&#65292;&#20154;&#20204;&#27491;&#22312;&#20570;&#20986;&#37325;&#22823;&#21162;&#21147;&#26469;&#30830;&#23450;&#21644;&#28040;&#38500;&#21487;&#33021;&#23548;&#33268;&#25968;&#25454;&#20559;&#24046;&#30340;&#26469;&#28304;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#30740;&#31350;&#32467;&#26524;&#65292;&#23637;&#31034;&#25968;&#25454;&#20559;&#35265;&#22914;&#20309;&#24433;&#21709;&#26368;&#32456;&#29992;&#25143;&#65292;&#20559;&#24046;&#30340;&#36215;&#28304;&#20197;&#21450;&#25105;&#20204;&#24212;&#35813;&#22914;&#20309;&#35299;&#20915;&#35813;&#38382;&#39064;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#19981;&#24517;&#22312;&#25152;&#26377;&#24773;&#20917;&#19979;&#28040;&#38500;&#25968;&#25454;&#20559;&#24046;&#65292;&#32780;&#26159;&#24212;&#23558;&#30740;&#31350;&#37325;&#28857;&#36716;&#21521;&#20559;&#35265;&#30340;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the widespread use of data-powered systems in our everyday lives, concepts like bias and fairness gained significant attention among researchers and practitioners, in both industry and academia. Such issues typically emerge from the data, which comes with varying levels of quality, used to train supervised machine learning systems. With the commercialization and deployment of such systems that are sometimes delegated to make life-changing decisions, significant efforts are being made towards the identification and removal of possible sources of data bias that may resurface to the final end user or in the decisions being made. In this paper, we present research results that show how bias in data affects end users, where bias is originated, and provide a viewpoint about what we should do about it. We argue that data bias is not something that should necessarily be removed in all cases, and that research attention should instead shift from bias removal towards the identification, m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#19988;&#36731;&#37327;&#32423;&#30340;&#38899;&#39057;&#25351;&#32441;&#35782;&#21035;&#31995;&#32479;&#65292;&#21487;&#29992;&#20110;&#33258;&#21160;&#20869;&#23481;&#35782;&#21035;&#65292;&#20855;&#26377;&#39640;&#24230;&#21487;&#25193;&#23637;&#24615;&#21644;&#20302;&#21151;&#32791;&#65292;&#24182;&#22312;&#19987;&#26377;ACR&#25968;&#25454;&#38598;&#31934;&#24230;&#12289;&#26816;&#32034;&#36895;&#24230;&#12289;&#20869;&#23384;&#20351;&#29992;&#21644;&#40065;&#26834;&#24615;&#31561;&#25152;&#26377;&#35780;&#20272;&#25351;&#26631;&#19978;&#22343;&#26174;&#33879;&#20248;&#20110;&#22522;&#20110;&#26368;&#23567;&#21704;&#24076;&#30340;&#38899;&#39057;&#25351;&#32441;&#12290;</title><link>http://arxiv.org/abs/2305.09559</link><description>&lt;p&gt;
&#33258;&#21160;&#20869;&#23481;&#35782;&#21035;&#20013;&#30340;&#40065;&#26834;&#19988;&#36731;&#37327;&#32423;&#38899;&#39057;&#25351;&#32441;&#35782;&#21035;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Robust and lightweight audio fingerprint for Automatic Content Recognition. (arXiv:2305.09559v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09559
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#19988;&#36731;&#37327;&#32423;&#30340;&#38899;&#39057;&#25351;&#32441;&#35782;&#21035;&#31995;&#32479;&#65292;&#21487;&#29992;&#20110;&#33258;&#21160;&#20869;&#23481;&#35782;&#21035;&#65292;&#20855;&#26377;&#39640;&#24230;&#21487;&#25193;&#23637;&#24615;&#21644;&#20302;&#21151;&#32791;&#65292;&#24182;&#22312;&#19987;&#26377;ACR&#25968;&#25454;&#38598;&#31934;&#24230;&#12289;&#26816;&#32034;&#36895;&#24230;&#12289;&#20869;&#23384;&#20351;&#29992;&#21644;&#40065;&#26834;&#24615;&#31561;&#25152;&#26377;&#35780;&#20272;&#25351;&#26631;&#19978;&#22343;&#26174;&#33879;&#20248;&#20110;&#22522;&#20110;&#26368;&#23567;&#21704;&#24076;&#30340;&#38899;&#39057;&#25351;&#32441;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38899;&#39057;&#25351;&#32441;&#35782;&#21035;&#31995;&#32479;&#65292;&#29992;&#20110;&#33258;&#21160;&#20869;&#23481;&#35782;&#21035;&#65288;ACR&#65289;&#12290;&#36890;&#36807;&#20351;&#29992;&#20449;&#21495;&#22788;&#29702;&#25216;&#26415;&#21644;&#32479;&#35745;&#21464;&#25442;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#29983;&#25104;&#38899;&#39057;&#29255;&#27573;&#30340;&#32039;&#20945;&#25351;&#32441;&#65292;&#36825;&#20123;&#25351;&#32441;&#23545;&#29616;&#23454;&#19990;&#30028;&#20013;&#23384;&#22312;&#30340;&#22122;&#22768;&#38477;&#32423;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#35813;&#31995;&#32479;&#20855;&#26377;&#39640;&#24230;&#21487;&#25193;&#23637;&#24615;&#65292;&#33021;&#22815;&#20351;&#29992;&#26469;&#33258;&#25968;&#30334;&#19975;&#21488;&#30005;&#35270;&#30340;&#25351;&#32441;&#35782;&#21035;&#25968;&#21315;&#23567;&#26102;&#30340;&#20869;&#23481;&#12290;&#25351;&#32441;&#30340;&#39640;&#26102;&#38388;&#30456;&#20851;&#24615;&#21644;&#21033;&#29992;&#29616;&#26377;&#30340;GPU&#20860;&#23481;&#30340;&#36817;&#20284;&#26368;&#36817;&#37051;&#65288;ANN&#65289;&#25628;&#32034;&#31639;&#27861;&#20351;&#36825;&#19968;&#28857;&#25104;&#20026;&#21487;&#33021;&#12290;&#27492;&#22806;&#65292;&#25351;&#32441;&#29983;&#25104;&#21487;&#20197;&#22312;&#35745;&#31639;&#21463;&#38480;&#30340;&#20302;&#21151;&#32791;&#35774;&#22791;&#19978;&#36816;&#34892;&#65292;&#20351;&#20854;&#21487;&#20197;&#34987;&#24191;&#27867;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
This research paper presents a novel audio fingerprinting system for Automatic Content Recognition (ACR). By using signal processing techniques and statistical transformations, our proposed method generates compact fingerprints of audio segments that are robust to noise degradations present in real-world audio. The system is designed to be highly scalable, with the ability to identify thousands of hours of content using fingerprints generated from millions of TVs. The fingerprint's high temporal correlation and utilization of existing GPU-compatible Approximate Nearest Neighbour (ANN) search algorithms make this possible. Furthermore, the fingerprint generation can run on low-power devices with limited compute, making it accessible to a wide range of applications. Experimental results show improvements in our proposed system compared to a min-hash based audio fingerprint on all evaluated metrics, including accuracy on proprietary ACR datasets, retrieval speed, memory usage, and robustn
&lt;/p&gt;</description></item><item><title>&#8220;Life of PII&#8221;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#28151;&#28102;&#21464;&#25442;&#22120;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;PII&#36716;&#21270;&#20026;&#20154;&#36896;PII&#21516;&#26102;&#23613;&#21487;&#33021;&#22320;&#20445;&#30041;&#21407;&#22987;&#20449;&#24687;&#12289;&#24847;&#22270;&#21644;&#19978;&#19979;&#25991;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#26377;&#36873;&#25321;&#22320;&#28151;&#28102;&#25991;&#26723;&#20013;&#30340;&#25935;&#24863;&#20449;&#24687;&#65292;&#21516;&#26102;&#20445;&#30041;&#25991;&#26723;&#30340;&#32479;&#35745;&#21644;&#35821;&#20041;&#29305;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.09550</link><description>&lt;p&gt;
PII&#30340;&#29983;&#21629;--&#19968;&#31181;PII&#28151;&#28102;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Life of PII -- A PII Obfuscation Transformer. (arXiv:2305.09550v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09550
&lt;/p&gt;
&lt;p&gt;
&#8220;Life of PII&#8221;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#28151;&#28102;&#21464;&#25442;&#22120;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;PII&#36716;&#21270;&#20026;&#20154;&#36896;PII&#21516;&#26102;&#23613;&#21487;&#33021;&#22320;&#20445;&#30041;&#21407;&#22987;&#20449;&#24687;&#12289;&#24847;&#22270;&#21644;&#19978;&#19979;&#25991;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#26377;&#36873;&#25321;&#22320;&#28151;&#28102;&#25991;&#26723;&#20013;&#30340;&#25935;&#24863;&#20449;&#24687;&#65292;&#21516;&#26102;&#20445;&#30041;&#25991;&#26723;&#30340;&#32479;&#35745;&#21644;&#35821;&#20041;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#20170;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#25968;&#25454;&#39537;&#21160;&#26381;&#21153;&#30340;&#19990;&#30028;&#20013;&#65292;&#20445;&#25252;&#25935;&#24863;&#20449;&#24687;&#33267;&#20851;&#37325;&#35201;&#12290;&#19968;&#31181;&#24120;&#35265;&#30340;&#26041;&#27861;&#26159;&#20351;&#29992;&#25968;&#25454;&#25200;&#21160;&#25216;&#26415;&#26469;&#20943;&#23569;(&#25935;&#24863;)&#20010;&#20154;&#36523;&#20221;&#35782;&#21035;&#20449;&#24687;(PII)&#25968;&#25454;&#30340;&#36807;&#24230;&#23454;&#29992;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#32479;&#35745;&#21644;&#35821;&#20041;&#29305;&#24615;&#12290;&#25968;&#25454;&#25200;&#21160;&#26041;&#27861;&#32463;&#24120;&#23548;&#33268;&#26174;&#30528;&#30340;&#20449;&#24687;&#25439;&#22833;&#65292;&#20351;&#23427;&#20204;&#38590;&#20197;&#20351;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;PII&#30340;&#29983;&#21629;&#8221;--&#19968;&#31181;&#26032;&#39062;&#30340;&#28151;&#28102;&#21464;&#25442;&#22120;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;PII&#36716;&#21270;&#20026;&#20154;&#36896;PII&#21516;&#26102;&#23613;&#21487;&#33021;&#22320;&#20445;&#30041;&#21407;&#22987;&#20449;&#24687;&#12289;&#24847;&#22270;&#21644;&#19978;&#19979;&#25991;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#19968;&#20010;API&#26469;&#19982;&#32473;&#23450;&#30340;&#25991;&#26723;&#36827;&#34892;&#25509;&#21475;&#65292;&#19968;&#20010;&#22522;&#20110;&#37197;&#32622;&#30340;&#28151;&#28102;&#22120;&#21644;&#19968;&#20010;&#22522;&#20110;Transformer&#26550;&#26500;&#30340;&#27169;&#22411;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#21644;LLMs&#20013;&#34920;&#29616;&#20986;&#39640;&#30340;&#19978;&#19979;&#25991;&#20445;&#23384;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#22522;&#20110;Transformer&#30340;&#26041;&#27861;&#23398;&#20064;&#20102;&#21407;&#22987;PII&#21644;&#20854;&#36716;&#25442;&#21518;&#30340;&#20154;&#36896;PII&#23545;&#24212;&#30340;&#26144;&#23556;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#26377;&#36873;&#25321;&#22320;&#28151;&#28102;&#25991;&#26723;&#20013;&#30340;&#25935;&#24863;&#20449;&#24687;&#65292;&#21516;&#26102;&#20445;&#30041;&#25991;&#26723;&#30340;&#32479;&#35745;&#21644;&#35821;&#20041;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Protecting sensitive information is crucial in today's world of Large Language Models (LLMs) and data-driven services. One common method used to preserve privacy is by using data perturbation techniques to reduce overreaching utility of (sensitive) Personal Identifiable Information (PII) data while maintaining its statistical and semantic properties. Data perturbation methods often result in significant information loss, making them impractical for use. In this paper, we propose 'Life of PII', a novel Obfuscation Transformer framework for transforming PII into faux-PII while preserving the original information, intent, and context as much as possible. Our approach includes an API to interface with the given document, a configuration-based obfuscator, and a model based on the Transformer architecture, which has shown high context preservation and performance in natural language processing tasks and LLMs.  Our Transformer-based approach learns mapping between the original PII and its tra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21487;&#35270;&#21270;&#20449;&#24687;&#23545;&#32852;&#37030;&#25512;&#33616;&#31995;&#32479;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#24403;&#21152;&#20837;&#35270;&#35273;&#20449;&#24687;&#26102;&#65292;&#29616;&#26377;&#30340;&#24694;&#24847;&#25512;&#24191;&#25915;&#20987;&#23558;&#21464;&#24471;&#26080;&#25928;&#12290;</title><link>http://arxiv.org/abs/2305.08183</link><description>&lt;p&gt;
&#21487;&#35270;&#21270;&#20449;&#24687;&#23545;&#32852;&#37030;&#25512;&#33616;&#31995;&#32479;&#30340;&#24433;&#21709;&#21450;&#20854;&#23545;&#31574;
&lt;/p&gt;
&lt;p&gt;
Manipulating Visually-aware Federated Recommender Systems and Its Countermeasures. (arXiv:2305.08183v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08183
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21487;&#35270;&#21270;&#20449;&#24687;&#23545;&#32852;&#37030;&#25512;&#33616;&#31995;&#32479;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#24403;&#21152;&#20837;&#35270;&#35273;&#20449;&#24687;&#26102;&#65292;&#29616;&#26377;&#30340;&#24694;&#24847;&#25512;&#24191;&#25915;&#20987;&#23558;&#21464;&#24471;&#26080;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#32852;&#37030;&#25512;&#33616;&#31995;&#32479;&#65288;FedRec&#65289;&#22240;&#20854;&#20445;&#25252;&#29992;&#25143;&#25968;&#25454;&#38544;&#31169;&#30340;&#33021;&#21147;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#22312;FedRec&#20013;&#65292;&#20013;&#22830;&#26381;&#21153;&#22120;&#36890;&#36807;&#19982;&#23458;&#25143;&#31471;&#20849;&#20139;&#27169;&#22411;&#20844;&#20849;&#21442;&#25968;&#26469;&#21327;&#21516;&#23398;&#20064;&#25512;&#33616;&#27169;&#22411;&#65292;&#20174;&#32780;&#25552;&#20379;&#19968;&#31181;&#20445;&#25252;&#38544;&#31169;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#27169;&#22411;&#21442;&#25968;&#30340;&#20844;&#24320;&#24615;&#20026;&#25915;&#20987;&#32773;&#25805;&#32437;FedRec&#30041;&#19979;&#20102;&#21518;&#38376;&#12290;&#29616;&#26377;&#30340;&#19982;FedRec&#23433;&#20840;&#30456;&#20851;&#30340;&#30740;&#31350;&#24050;&#32463;&#34920;&#26126;&#65292;&#36890;&#36807;&#27169;&#22411;&#27745;&#26579;&#25915;&#20987;&#65292;&#24694;&#24847;&#29992;&#25143;&#21487;&#20197;&#36731;&#26131;&#22320;&#25512;&#24191;&#39033;&#30446;&#65292;&#20294;&#26159;&#23427;&#20204;&#20027;&#35201;&#38598;&#20013;&#20110;&#21482;&#20855;&#26377;&#21327;&#20316;&#20449;&#24687;&#65288;&#21363;&#29992;&#25143;-&#39033;&#30446;&#20132;&#20114;&#65289;&#30340;FedRec&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#20123;&#25915;&#20987;&#20043;&#25152;&#20197;&#26377;&#25928;&#65292;&#26159;&#22240;&#20026;&#21327;&#20316;&#20449;&#21495;&#30340;&#25968;&#25454;&#31232;&#30095;&#24615;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#36741;&#21161;&#20449;&#24687;&#65288;&#22914;&#20135;&#21697;&#30340;&#35270;&#35273;&#25551;&#36848;&#65289;&#29992;&#20110;&#32531;&#35299;&#21327;&#20316;&#36807;&#28388;&#25968;&#25454;&#30340;&#31232;&#30095;&#24615;&#12290;&#22240;&#27492;&#65292;&#24403;&#22312;FedRec&#20013;&#21152;&#20837;&#35270;&#35273;&#20449;&#24687;&#26102;&#65292;&#25152;&#26377;&#29616;&#26377;&#30340;&#27169;&#22411;&#27745;&#26579;&#25915;&#20987;&#30340;&#26377;&#25928;&#24615;&#37117;&#23558;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated recommender systems (FedRecs) have been widely explored recently due to their ability to protect user data privacy. In FedRecs, a central server collaboratively learns recommendation models by sharing model public parameters with clients, thereby offering a privacy-preserving solution. Unfortunately, the exposure of model parameters leaves a backdoor for adversaries to manipulate FedRecs. Existing works about FedRec security already reveal that items can easily be promoted by malicious users via model poisoning attacks, but all of them mainly focus on FedRecs with only collaborative information (i.e., user-item interactions). We argue that these attacks are effective because of the data sparsity of collaborative signals. In practice, auxiliary information, such as products' visual descriptions, is used to alleviate collaborative filtering data's sparsity. Therefore, when incorporating visual information in FedRecs, all existing model poisoning attacks' effectiveness becomes q
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#31471;&#21040;&#31471;&#22823;&#35268;&#27169;&#23545;&#35805;&#25512;&#33616;&#31995;&#32479;&#30340;&#36335;&#32447;&#22270;&#65292;&#35299;&#20915;&#22312;&#35813;&#31995;&#32479;&#20013;&#26377;&#25928;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25152;&#38754;&#20020;&#30340;&#25216;&#26415;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2305.07961</link><description>&lt;p&gt;
&#22312;&#23545;&#35805;&#25512;&#33616;&#31995;&#32479;&#20013;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Leveraging Large Language Models in Conversational Recommender Systems. (arXiv:2305.07961v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07961
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#31471;&#21040;&#31471;&#22823;&#35268;&#27169;&#23545;&#35805;&#25512;&#33616;&#31995;&#32479;&#30340;&#36335;&#32447;&#22270;&#65292;&#35299;&#20915;&#22312;&#35813;&#31995;&#32479;&#20013;&#26377;&#25928;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25152;&#38754;&#20020;&#30340;&#25216;&#26415;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#25512;&#33616;&#31995;&#32479;&#36890;&#36807;&#21551;&#29992;&#23454;&#26102;&#30340;&#22810;&#36718;&#23545;&#35805;&#20351;&#29992;&#25143;&#26356;&#21152;&#36879;&#26126;&#21644;&#25484;&#25511;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23637;&#29616;&#20102;&#19982;&#20154;&#31867;&#23545;&#35805;&#33258;&#28982;&#30340;&#33021;&#21147;&#65292;&#24182;&#23558;&#19990;&#30028;&#30693;&#35782;&#21644;&#24120;&#35782;&#25512;&#29702;&#34701;&#20837;&#21040;&#35821;&#35328;&#29702;&#35299;&#20013;&#65292;&#36827;&#19968;&#27493;&#37322;&#25918;&#20102;&#36825;&#19968;&#33539;&#24335;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#23545;&#35805;&#25512;&#33616;&#31995;&#32479;&#20013;&#26377;&#25928;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24341;&#20837;&#20102;&#26032;&#30340;&#25216;&#26415;&#25361;&#25112;&#65292;&#21253;&#25324;&#36866;&#24403;&#22320;&#29702;&#35299;&#21644;&#25511;&#21046;&#22797;&#26434;&#30340;&#23545;&#35805;&#21644;&#20174;&#22806;&#37096;&#20449;&#24687;&#28304;&#26816;&#32034;&#12290;&#30001;&#20110;&#22823;&#32780;&#19981;&#26029;&#22686;&#38271;&#30340;&#39033;&#30446;&#35821;&#26009;&#24211;&#21644;&#32570;&#20047;&#23545;&#35805;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#36825;&#20123;&#38382;&#39064;&#21152;&#21095;&#20102;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#31471;&#21040;&#31471;&#22823;&#35268;&#27169;&#23545;&#35805;&#25512;&#33616;&#31995;&#32479;&#30340;&#36335;&#32447;&#22270;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#25143;&#20559;&#22909;&#29702;&#35299;&#12289;&#28789;&#27963;&#30340;&#23545;&#35805;&#31649;&#29702;&#21644;&#21487;&#35299;&#37322;&#30340;&#25512;&#33616;&#20316;&#20026;&#25972;&#20010;&#31995;&#32479;&#30340;&#19968;&#37096;&#20998;&#30340;&#26032;&#23454;&#29616;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
A Conversational Recommender System (CRS) offers increased transparency and control to users by enabling them to engage with the system through a real-time multi-turn dialogue. Recently, Large Language Models (LLMs) have exhibited an unprecedented ability to converse naturally and incorporate world knowledge and common-sense reasoning into language understanding, unlocking the potential of this paradigm. However, effectively leveraging LLMs within a CRS introduces new technical challenges, including properly understanding and controlling a complex conversation and retrieving from external sources of information. These issues are exacerbated by a large, evolving item corpus and a lack of conversational data for training. In this paper, we provide a roadmap for building an end-to-end large-scale CRS using LLMs. In particular, we propose new implementations for user preference understanding, flexible dialogue management and explainable recommendations as part of an integrated architecture
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24207;&#21015;&#25512;&#33616;&#31639;&#27861;&#8212;&#8212;&#39057;&#29575;&#22686;&#24378;&#30340;&#28151;&#21512;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;FEARec&#65289;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#19968;&#20010;&#26012;&#22369;&#32467;&#26500;&#23558;&#21407;&#26377;&#30340;&#33258;&#27880;&#24847;&#21147;&#20174;&#26102;&#38388;&#22495;&#36716;&#25442;&#21040;&#39057;&#29575;&#22495;&#65292;&#20351;&#24471;&#20302;&#39057;&#21644;&#39640;&#39057;&#20449;&#24687;&#21487;&#20197;&#34987;&#26126;&#30830;&#22320;&#23398;&#20064;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#33258;&#30456;&#20851;&#30340;&#35774;&#35745;&#65292;&#35813;&#31639;&#27861;&#36824;&#21487;&#20197;&#25429;&#25417;&#21040;&#29992;&#25143;&#34892;&#20026;&#30340;&#22266;&#26377;&#21608;&#26399;&#24615;&#12290;&#22312;&#22235;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#35813;&#31639;&#27861;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.09184</link><description>&lt;p&gt;
&#39057;&#29575;&#22686;&#24378;&#30340;&#28151;&#21512;&#27880;&#24847;&#21147;&#32593;&#32476;&#29992;&#20110;&#24207;&#21015;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Frequency Enhanced Hybrid Attention Network for Sequential Recommendation. (arXiv:2304.09184v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09184
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24207;&#21015;&#25512;&#33616;&#31639;&#27861;&#8212;&#8212;&#39057;&#29575;&#22686;&#24378;&#30340;&#28151;&#21512;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;FEARec&#65289;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#19968;&#20010;&#26012;&#22369;&#32467;&#26500;&#23558;&#21407;&#26377;&#30340;&#33258;&#27880;&#24847;&#21147;&#20174;&#26102;&#38388;&#22495;&#36716;&#25442;&#21040;&#39057;&#29575;&#22495;&#65292;&#20351;&#24471;&#20302;&#39057;&#21644;&#39640;&#39057;&#20449;&#24687;&#21487;&#20197;&#34987;&#26126;&#30830;&#22320;&#23398;&#20064;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#33258;&#30456;&#20851;&#30340;&#35774;&#35745;&#65292;&#35813;&#31639;&#27861;&#36824;&#21487;&#20197;&#25429;&#25417;&#21040;&#29992;&#25143;&#34892;&#20026;&#30340;&#22266;&#26377;&#21608;&#26399;&#24615;&#12290;&#22312;&#22235;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#35813;&#31639;&#27861;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#27880;&#24847;&#26426;&#21046;&#26159;&#24207;&#21015;&#25512;&#33616;&#39046;&#22495;&#24191;&#27867;&#20351;&#29992;&#30340;&#25216;&#26415;&#20043;&#19968;&#65292;&#20855;&#26377;&#24314;&#27169;&#38271;&#31243;&#20381;&#36182;&#24615;&#30340;&#24378;&#22823;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#30446;&#21069;&#22522;&#20110;&#33258;&#27880;&#24847;&#21147;&#30340;&#27169;&#22411;&#26159;&#20302;&#36890;&#28388;&#27874;&#22120;&#65292;&#19981;&#33021;&#25429;&#25417;&#39640;&#39057;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#29992;&#25143;&#34892;&#20026;&#20013;&#30340;&#39033;&#30446;&#30456;&#20114;&#20132;&#32455;&#65292;&#36825;&#20123;&#27169;&#22411;&#26080;&#27861;&#21306;&#20998;&#26102;&#38388;&#22495;&#20013;&#27169;&#31946;&#30340;&#22266;&#26377;&#21608;&#26399;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#35270;&#35282;&#36716;&#31227;&#21040;&#39057;&#29575;&#22495;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24207;&#21015;&#25512;&#33616;&#31639;&#27861;&#8212;&#8212;&#39057;&#29575;&#22686;&#24378;&#30340;&#28151;&#21512;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#21363;FEARec&#12290;&#22312;&#36825;&#20010;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#19968;&#20010;&#26012;&#22369;&#32467;&#26500;&#23558;&#21407;&#22987;&#26102;&#38388;&#22495;&#33258;&#27880;&#24847;&#21147;&#25913;&#36827;&#21040;&#39057;&#29575;&#22495;&#20013;&#65292;&#20351;&#24471;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#26126;&#30830;&#22320;&#23398;&#20064;&#20302;&#39057;&#21644;&#39640;&#39057;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#33258;&#30456;&#20851;&#35774;&#35745;&#20102;&#31867;&#20284;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#25429;&#25417;&#29992;&#25143;&#34892;&#20026;&#30340;&#22266;&#26377;&#21608;&#26399;&#24615;&#12290;&#22312;&#22235;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#27604;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#34920;&#29616;&#26356;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;
The self-attention mechanism, which equips with a strong capability of modeling long-range dependencies, is one of the extensively used techniques in the sequential recommendation field. However, many recent studies represent that current self-attention based models are low-pass filters and are inadequate to capture high-frequency information. Furthermore, since the items in the user behaviors are intertwined with each other, these models are incomplete to distinguish the inherent periodicity obscured in the time domain. In this work, we shift the perspective to the frequency domain, and propose a novel Frequency Enhanced Hybrid Attention Network for Sequential Recommendation, namely FEARec. In this model, we firstly improve the original time domain self-attention in the frequency domain with a ramp structure to make both low-frequency and high-frequency information could be explicitly learned in our approach. Moreover, we additionally design a similar attention mechanism via auto-corr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102; MCLRec &#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#25968;&#25454;&#22686;&#24378;&#21644;&#21487;&#23398;&#20064;&#27169;&#22411;&#22686;&#24378;&#25805;&#20316;&#30340;&#22522;&#30784;&#19978;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#38590;&#20197;&#25512;&#24191;&#21644;&#35757;&#32451;&#25968;&#25454;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.07763</link><description>&lt;p&gt;
&#24207;&#21015;&#25512;&#33616;&#20013;&#30340;&#20803;&#20248;&#21270;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Meta-optimized Contrastive Learning for Sequential Recommendation. (arXiv:2304.07763v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07763
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; MCLRec &#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#25968;&#25454;&#22686;&#24378;&#21644;&#21487;&#23398;&#20064;&#27169;&#22411;&#22686;&#24378;&#25805;&#20316;&#30340;&#22522;&#30784;&#19978;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#38590;&#20197;&#25512;&#24191;&#21644;&#35757;&#32451;&#25968;&#25454;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#26159;&#35299;&#20915;&#31232;&#30095;&#19988;&#21547;&#22122;&#22768;&#25512;&#33616;&#25968;&#25454;&#30340;&#19968;&#20010;&#26032;&#20852;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#35201;&#20040;&#21482;&#38024;&#23545;&#25163;&#24037;&#21046;&#20316;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#25968;&#25454;&#21644;&#27169;&#22411;&#22686;&#24378;&#65292;&#35201;&#20040;&#21482;&#20351;&#29992;&#27169;&#22411;&#22686;&#24378;&#26041;&#27861;&#65292;&#36825;&#20351;&#24471;&#27169;&#22411;&#24456;&#38590;&#25512;&#24191;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#35757;&#32451;&#27169;&#22411;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#20803;&#20248;&#21270;&#23545;&#27604;&#23398;&#20064;&#30340;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#32467;&#21512;&#20102;&#25968;&#25454;&#22686;&#24378;&#21644;&#21487;&#23398;&#20064;&#27169;&#22411;&#22686;&#24378;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive Learning (CL) performances as a rising approach to address the challenge of sparse and noisy recommendation data. Although having achieved promising results, most existing CL methods only perform either hand-crafted data or model augmentation for generating contrastive pairs to find a proper augmentation operation for different datasets, which makes the model hard to generalize. Additionally, since insufficient input data may lead the encoder to learn collapsed embeddings, these CL methods expect a relatively large number of training data (e.g., large batch size or memory bank) to contrast. However, not all contrastive pairs are always informative and discriminative enough for the training processing. Therefore, a more general CL-based recommendation model called Meta-optimized Contrastive Learning for sequential Recommendation (MCLRec) is proposed in this work. By applying both data augmentation and learnable model augmentation operations, this work innovates the standard 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#29992;&#20110;&#35299;&#37322;&#40657;&#30418;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#24046;&#23481;&#24525;&#22240;&#23376;&#65288;VTF&#65289;&#29702;&#35770;&#65292;&#36890;&#36807;&#25490;&#21517;&#29305;&#24449;&#30340;&#26041;&#24335;&#25506;&#32034;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#65292;&#21516;&#26102;&#26500;&#24314;&#19968;&#20010;&#22522;&#26412;&#27169;&#22411;&#21644;&#29305;&#24449;&#27169;&#22411;&#30340;&#26032;&#22411;&#26550;&#26500;&#65292;&#26469;&#25506;&#32034;&#25152;&#26377;&#34920;&#29616;&#33391;&#22909;&#30340;&#31070;&#32463;&#32593;&#32476;&#20013;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#19988;&#32463;&#36807;&#22522;&#20934;&#27979;&#35797;&#21644;&#24212;&#29992;&#20110;&#23454;&#38469;&#29615;&#22659;&#20013;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#26041;&#27861;&#30340;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2209.13858</link><description>&lt;p&gt;
&#35299;&#37322;&#25152;&#26377;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#24046;&#23481;&#24525;&#22240;&#23376;
&lt;/p&gt;
&lt;p&gt;
Variance Tolerance Factors For Interpreting ALL Neural Networks. (arXiv:2209.13858v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.13858
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#29992;&#20110;&#35299;&#37322;&#40657;&#30418;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#24046;&#23481;&#24525;&#22240;&#23376;&#65288;VTF&#65289;&#29702;&#35770;&#65292;&#36890;&#36807;&#25490;&#21517;&#29305;&#24449;&#30340;&#26041;&#24335;&#25506;&#32034;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#65292;&#21516;&#26102;&#26500;&#24314;&#19968;&#20010;&#22522;&#26412;&#27169;&#22411;&#21644;&#29305;&#24449;&#27169;&#22411;&#30340;&#26032;&#22411;&#26550;&#26500;&#65292;&#26469;&#25506;&#32034;&#25152;&#26377;&#34920;&#29616;&#33391;&#22909;&#30340;&#31070;&#32463;&#32593;&#32476;&#20013;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#19988;&#32463;&#36807;&#22522;&#20934;&#27979;&#35797;&#21644;&#24212;&#29992;&#20110;&#23454;&#38469;&#29615;&#22659;&#20013;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#26041;&#27861;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40657;&#21283;&#23376;&#27169;&#22411;&#21482;&#25552;&#20379;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#30340;&#32467;&#26524;&#65292;&#32570;&#20047;&#26377;&#20851;&#22914;&#20309;&#33719;&#24471;&#36825;&#20123;&#32467;&#26524;&#30340;&#35814;&#32454;&#20449;&#24687;&#12290;&#30693;&#36947;&#36755;&#20837;&#21464;&#37327;&#19982;&#36755;&#20986;&#30340;&#20851;&#31995;&#65292;&#20197;&#21450;&#20026;&#20160;&#20040;&#23427;&#20204;&#30456;&#20851;&#65292;&#21487;&#20197;&#22312;&#23558;&#39044;&#27979;&#36716;&#21270;&#20026;&#23454;&#39564;&#25110;&#22312;&#21463;&#21040;&#23457;&#26597;&#26102;&#32500;&#25252;&#27169;&#22411;&#39044;&#27979;&#30340;&#20851;&#38190;&#26102;&#21051;&#36215;&#21040;&#37325;&#35201;&#20316;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19968;&#33324;&#24615;&#29702;&#35770;&#65292;&#36890;&#36807;&#23450;&#20041;&#19968;&#20010;&#21463;&#24433;&#21709;&#20989;&#25968;&#21551;&#21457;&#30340;&#26041;&#24046;&#23481;&#24525;&#22240;&#23376;&#65288;VTF&#65289;&#65292;&#20174;&#25490;&#21517;&#29305;&#24449;&#30340;&#35282;&#24230;&#35299;&#37322;&#40657;&#21283;&#23376;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#29305;&#24449;&#65292;&#24182;&#26500;&#24314;&#19968;&#20010;&#21253;&#21547;&#22522;&#26412;&#27169;&#22411;&#21644;&#29305;&#24449;&#27169;&#22411;&#30340;&#26032;&#22411;&#26550;&#26500;&#65292;&#20197;&#25506;&#32034;&#21253;&#21547;&#25152;&#26377;&#34920;&#29616;&#33391;&#22909;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#29790;&#22763;&#20891;&#20992;&#38598;&#20013;&#30340;&#29305;&#24449;&#37325;&#35201;&#24615;&#12290;&#21019;&#24314;&#24182;&#25506;&#32034;&#20102;&#20004;&#31181;Rashomon&#38598;&#20013;&#30340;&#29305;&#24449;&#37325;&#35201;&#24615;&#25490;&#21517;&#26041;&#27861;&#21644;&#22522;&#20110;VTF&#30340;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#24443;&#24213;&#35780;&#20272;&#65292;&#24182;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#22522;&#22240;&#32452;&#23398;&#21644;&#26448;&#26009;&#31185;&#23398;&#20013;&#30340;&#20004;&#20010;&#30495;&#23454;&#19990;&#30028;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Black box models only provide results for deep learning tasks, and lack informative details about how these results were obtained. Knowing how input variables are related to outputs, in addition to why they are related, can be critical to translating predictions into laboratory experiments, or defending a model prediction under scrutiny. In this paper, we propose a general theory that defines a variance tolerance factor (VTF) inspired by influence function, to interpret features in the context of black box neural networks by ranking the importance of features, and construct a novel architecture consisting of a base model and feature model to explore the feature importance in a Rashomon set that contains all well-performing neural networks. Two feature importance ranking methods in the Rashomon set and a feature selection method based on the VTF are created and explored. A thorough evaluation on synthetic and benchmark datasets is provided, and the method is applied to two real world ex
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#65306;&#21487;&#33021;&#24615;&#35757;&#32451;&#21644;&#19981;&#21487;&#33021;&#24615;&#35757;&#32451;&#65292;&#20197;&#35299;&#37322;&#23884;&#20837;&#21521;&#37327;&#32972;&#21518;&#30340;&#35821;&#20041;&#24182;&#35299;&#20915;&#26631;&#31614;&#31232;&#30095;&#38382;&#39064;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#22270;&#20687;&#21644;&#35270;&#39057;&#20998;&#31867;&#12289;&#26816;&#32034;&#21644;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#25552;&#39640;&#20102;&#23398;&#20064;&#23884;&#20837;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2207.00282</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#23884;&#20837;&#30340;(&#19981;)&#21487;&#33021;&#24615;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
(Un)likelihood Training for Interpretable Embedding. (arXiv:2207.00282v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.00282
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#65306;&#21487;&#33021;&#24615;&#35757;&#32451;&#21644;&#19981;&#21487;&#33021;&#24615;&#35757;&#32451;&#65292;&#20197;&#35299;&#37322;&#23884;&#20837;&#21521;&#37327;&#32972;&#21518;&#30340;&#35821;&#20041;&#24182;&#35299;&#20915;&#26631;&#31614;&#31232;&#30095;&#38382;&#39064;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#22270;&#20687;&#21644;&#35270;&#39057;&#20998;&#31867;&#12289;&#26816;&#32034;&#21644;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#25552;&#39640;&#20102;&#23398;&#20064;&#23884;&#20837;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#24050;&#25104;&#20026;&#24357;&#21512;&#25991;&#26412;&#21644;&#35270;&#35273;&#25968;&#25454;&#35821;&#20041;&#24046;&#36317;&#30340;&#26032;&#24120;&#24577;&#12290;&#28982;&#32780;&#65292;&#22312;&#36830;&#32493;&#28508;&#22312;&#31354;&#38388;&#20013;&#23398;&#20064;&#27169;&#24577;&#19981;&#21487;&#30693;&#34920;&#31034;&#32463;&#24120;&#34987;&#35270;&#20026;&#40657;&#21283;&#23376;&#25968;&#25454;&#39537;&#21160;&#30340;&#35757;&#32451;&#36807;&#31243;&#12290;&#28145;&#24230;&#34920;&#31034;&#23398;&#20064;&#30340;&#26377;&#25928;&#24615;&#20005;&#37325;&#20381;&#36182;&#20110;&#35757;&#32451;&#25968;&#25454;&#30340;&#36136;&#37327;&#21644;&#35268;&#27169;&#12290;&#23545;&#20110;&#35270;&#39057;&#34920;&#31034;&#23398;&#20064;&#65292;&#35201;&#23436;&#25972;&#22320;&#26631;&#27880;&#35270;&#39057;&#20869;&#23481;&#30340;&#25968;&#25454;&#38598;&#26159;&#39640;&#24230;&#22256;&#38590;&#29978;&#33267;&#19981;&#21487;&#33021;&#30340;&#12290;&#36825;&#20123;&#38382;&#39064;&#65292;&#40657;&#21283;&#23376;&#35757;&#32451;&#21644;&#25968;&#25454;&#38598;&#20559;&#24046;&#65292;&#20351;&#24471;&#35299;&#37322;&#24615;&#36739;&#24046;&#21644;&#32467;&#26524;&#38590;&#20197;&#39044;&#27979;&#65292;&#38590;&#20197;&#22312;&#35270;&#39057;&#29702;&#35299;&#26041;&#38754;&#36827;&#34892;&#23454;&#38469;&#24212;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#33021;&#24615;&#21644;&#19981;&#21487;&#33021;&#24615;&#20989;&#25968;&#65292;&#20197;&#23637;&#31034;&#23884;&#20837;&#32972;&#21518;&#30340;&#35821;&#20041;&#65292;&#24182;&#35299;&#20915;&#35757;&#32451;&#20013;&#30340;&#26631;&#31614;&#31232;&#30095;&#38382;&#39064;&#12290;&#21487;&#33021;&#24615;&#35757;&#32451;&#26088;&#22312;&#36890;&#36807;&#23398;&#20064;&#25968;&#25454;&#20998;&#24067;&#26469;&#35299;&#37322;&#23884;&#20837;&#21521;&#37327;&#30340;&#35821;&#20041;&#65292;&#32780;&#19981;&#21487;&#33021;&#24615;&#35757;&#32451;&#21017;&#24378;&#35843;&#27491;&#36127;&#23545;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#23558;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#22270;&#20687;&#21644;&#35270;&#39057;&#20998;&#31867;&#12289;&#26816;&#32034;&#21644;&#29983;&#25104;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#25552;&#39640;&#23398;&#21040;&#30340;&#23884;&#20837;&#30340;&#21487;&#35299;&#37322;&#24615;&#20197;&#21450;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#31454;&#20105;&#24615;&#24615;&#33021;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-modal representation learning has become a new normal for bridging the semantic gap between text and visual data. Learning modality agnostic representations in a continuous latent space, however, is often treated as a black-box data-driven training process. It is well-known that the effectiveness of representation learning depends heavily on the quality and scale of training data. For video representation learning, having a complete set of labels that annotate the full spectrum of video content for training is highly difficult if not impossible. These issues, black-box training and dataset bias, make representation learning practically challenging to be deployed for video understanding due to unexplainable and unpredictable results. In this paper, we propose two novel training objectives, likelihood and unlikelihood functions, to unroll semantics behind embeddings while addressing the label sparsity problem in training. The likelihood training aims to interpret semantics of embed
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#21512;&#24314;&#27169;&#22240;&#26524;&#21644;&#30456;&#20851;&#20851;&#31995;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#22240;&#26524;&#12289;&#25928;&#24212;&#21644;&#30456;&#20851;&#22270;&#23454;&#29616;&#26377;&#25928;&#21644;&#21487;&#35299;&#37322;&#30340;&#22522;&#20110;&#20250;&#35805;&#30340;&#25512;&#33616;&#65292;&#25506;&#32034;&#29289;&#21697;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2201.10782</link><description>&lt;p&gt;
&#22240;&#26524;&#21644;&#30456;&#20851;&#24615;&#22270;&#24314;&#27169;&#23454;&#29616;&#26377;&#25928;&#19988;&#21487;&#35299;&#37322;&#30340;&#22522;&#20110;&#20250;&#35805;&#30340;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Causality and Correlation Graph Modeling for Effective and Explainable Session-based Recommendation. (arXiv:2201.10782v3 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.10782
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#21512;&#24314;&#27169;&#22240;&#26524;&#21644;&#30456;&#20851;&#20851;&#31995;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#22240;&#26524;&#12289;&#25928;&#24212;&#21644;&#30456;&#20851;&#22270;&#23454;&#29616;&#26377;&#25928;&#21644;&#21487;&#35299;&#37322;&#30340;&#22522;&#20110;&#20250;&#35805;&#30340;&#25512;&#33616;&#65292;&#25506;&#32034;&#29289;&#21697;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#20250;&#35805;&#30340;&#25512;&#33616;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#39640;&#24230;&#20851;&#27880;&#65292;&#36825;&#20123;&#25512;&#33616;&#26041;&#27861;&#26088;&#22312;&#26681;&#25454;&#21311;&#21517;&#20250;&#35805;&#39044;&#27979;&#29992;&#25143;&#30340;&#19979;&#19968;&#20010;&#24863;&#20852;&#36259;&#30340;&#29289;&#21697;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30740;&#31350;&#37319;&#29992;&#22797;&#26434;&#30340;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65288;&#20363;&#22914;&#22270;&#31070;&#32463;&#32593;&#32476;&#65289;&#26469;&#36827;&#34892;&#26377;&#25928;&#30340;&#22522;&#20110;&#20250;&#35805;&#30340;&#25512;&#33616;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20165;&#28041;&#21450;&#29289;&#21697;&#20043;&#38388;&#30340;&#20849;&#21516;&#21457;&#29983;&#65292;&#20294;&#26410;&#33021;&#24456;&#22909;&#22320;&#21306;&#20998;&#22240;&#26524;&#20851;&#31995;&#21644;&#30456;&#20851;&#20851;&#31995;&#12290;&#32771;&#34385;&#21040;&#22240;&#26524;&#20851;&#31995;&#21644;&#30456;&#20851;&#20851;&#31995;&#20043;&#38388;&#30340;&#19981;&#21516;&#35299;&#37322;&#21644;&#29305;&#24449;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#31216;&#20026;CGSR&#65292;&#36890;&#36807;&#32852;&#21512;&#24314;&#27169;&#29289;&#21697;&#20043;&#38388;&#30340;&#22240;&#26524;&#21644;&#30456;&#20851;&#20851;&#31995;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#36890;&#36807;&#21516;&#26102;&#32771;&#34385;&#38169;&#35823;&#22240;&#26524;&#38382;&#39064;&#65292;&#20174;&#20250;&#35805;&#20013;&#26500;&#24314;&#20102;&#22240;&#26524;&#12289;&#25928;&#24212;&#21644;&#30456;&#20851;&#22270;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#22522;&#20110;&#20250;&#35805;&#25512;&#33616;&#26041;&#27861;&#12290;&#24635;&#20043;&#65292;&#25105;&#20204;&#21162;&#21147;&#25506;&#32034;&#29305;&#23450;&#8220;&#22240;&#26524;&#20851;&#31995;&#8221;&#30340;&#29289;&#21697;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Session-based recommendation which has been witnessed a booming interest recently, focuses on predicting a user's next interested item(s) based on an anonymous session. Most existing studies adopt complex deep learning techniques (e.g., graph neural networks) for effective session-based recommendation. However, they merely address co-occurrence between items, but fail to well distinguish causality and correlation relationship. Considering the varied interpretations and characteristics of causality and correlation relationship between items, in this study, we propose a novel method denoted as CGSR by jointly modeling causality and correlation relationship between items. In particular, we construct cause, effect and correlation graphs from sessions by simultaneously considering the false causality problem. We further design a graph neural network-based method for session-based recommendation. To conclude, we strive to explore the relationship between items from specific ``causality" (dir
&lt;/p&gt;</description></item></channel></rss>