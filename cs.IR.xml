<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#20026;&#29992;&#25143;&#20986;&#21457;&#22320;&#21487;&#21040;&#36798;&#30340;&#22478;&#24066;&#26053;&#34892;&#20998;&#37197;&#21487;&#25345;&#32493;&#24615;&#25351;&#26631;&#65288;SF&#25351;&#25968;&#65289;&#12290;</title><link>https://arxiv.org/abs/2403.18604</link><description>&lt;p&gt;
&#24314;&#27169;&#21487;&#25345;&#32493;&#22478;&#24066;&#26053;&#34892;&#65306;&#23558;CO2&#25490;&#25918;&#12289;&#28909;&#24230;&#21644;&#23395;&#33410;&#24615;&#25972;&#21512;&#21040;&#26053;&#28216;&#25512;&#33616;&#31995;&#32479;&#20013;
&lt;/p&gt;
&lt;p&gt;
Modeling Sustainable City Trips: Integrating CO2 Emissions, Popularity, and Seasonality into Tourism Recommender Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18604
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#20026;&#29992;&#25143;&#20986;&#21457;&#22320;&#21487;&#21040;&#36798;&#30340;&#22478;&#24066;&#26053;&#34892;&#20998;&#37197;&#21487;&#25345;&#32493;&#24615;&#25351;&#26631;&#65288;SF&#25351;&#25968;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20449;&#24687;&#36807;&#36733;&#21644;&#22797;&#26434;&#20915;&#31574;&#36807;&#31243;&#30340;&#26102;&#20195;&#65292;&#25512;&#33616;&#31995;&#32479;&#65288;RS&#65289;&#24050;&#25104;&#20026;&#21508;&#20010;&#39046;&#22495;&#19981;&#21487;&#25110;&#32570;&#30340;&#24037;&#20855;&#65292;&#23588;&#20854;&#26159;&#26053;&#34892;&#21644;&#26053;&#28216;&#39046;&#22495;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#20026;&#29992;&#25143;&#20986;&#21457;&#22320;&#21487;&#21040;&#36798;&#30340;&#22478;&#24066;&#26053;&#34892;&#20998;&#37197;&#21487;&#25345;&#32493;&#24615;&#25351;&#26631;&#65288;SF&#25351;&#25968;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18604v1 Announce Type: new  Abstract: In an era of information overload and complex decision-making processes, Recommender Systems (RS) have emerged as indispensable tools across diverse domains, particularly travel and tourism. These systems simplify trip planning by offering personalized recommendations that consider individual preferences and address broader challenges like seasonality, travel regulations, and capacity constraints. The intricacies of the tourism domain, characterized by multiple stakeholders, including consumers, item providers, platforms, and society, underscore the complexity of achieving balance among diverse interests. Although previous research has focused on fairness in Tourism Recommender Systems (TRS) from a multistakeholder perspective, limited work has focused on generating sustainable recommendations.   Our paper introduces a novel approach for assigning a sustainability indicator (SF index) for city trips accessible from the users' starting po
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;FollowIR&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20005;&#26684;&#30340;&#35828;&#26126;&#20070;&#35780;&#20272;&#22522;&#20934;&#21644;&#35757;&#32451;&#38598;&#65292;&#24110;&#21161;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#26356;&#22909;&#22320;&#36981;&#24490;&#30495;&#23454;&#19990;&#30028;&#30340;&#35828;&#26126;&#20070;&#12290;&#35758;&#35770;&#22522;&#20110;TREC&#20250;&#35758;&#30340;&#21382;&#21490;&#65292;&#26088;&#22312;&#20351;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#33021;&#22815;&#26681;&#25454;&#35814;&#32454;&#35828;&#26126;&#20070;&#29702;&#35299;&#21644;&#21028;&#26029;&#30456;&#20851;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.15246</link><description>&lt;p&gt;
FollowIR: &#35780;&#20272;&#21644;&#25945;&#25480;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#20197;&#36981;&#24490;&#35828;&#26126;&#20070;
&lt;/p&gt;
&lt;p&gt;
FollowIR: Evaluating and Teaching Information Retrieval Models to Follow Instructions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15246
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;FollowIR&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20005;&#26684;&#30340;&#35828;&#26126;&#20070;&#35780;&#20272;&#22522;&#20934;&#21644;&#35757;&#32451;&#38598;&#65292;&#24110;&#21161;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#26356;&#22909;&#22320;&#36981;&#24490;&#30495;&#23454;&#19990;&#30028;&#30340;&#35828;&#26126;&#20070;&#12290;&#35758;&#35770;&#22522;&#20110;TREC&#20250;&#35758;&#30340;&#21382;&#21490;&#65292;&#26088;&#22312;&#20351;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#33021;&#22815;&#26681;&#25454;&#35814;&#32454;&#35828;&#26126;&#20070;&#29702;&#35299;&#21644;&#21028;&#26029;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#22815;&#36981;&#24490;&#38271;&#19988;&#22797;&#26434;&#30340;&#35828;&#26126;&#20070;&#65292;&#20174;&#32780;&#23454;&#29616;&#22810;&#26679;&#21270;&#30340;&#29992;&#25143;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#27169;&#22411;&#20351;&#29992;LLMs&#20316;&#20026;&#20854;&#26550;&#26500;&#30340;&#25903;&#26609;&#65292;&#20960;&#20046;&#25152;&#26377;&#36825;&#20123;&#27169;&#22411;&#20173;&#28982;&#21482;&#25509;&#21463;&#26597;&#35810;&#20316;&#20026;&#36755;&#20837;&#65292;&#27809;&#26377;&#35828;&#26126;&#20070;&#12290;&#23545;&#20110;&#26368;&#36817;&#19968;&#20123;&#25509;&#21463;&#35828;&#26126;&#20070;&#30340;&#27169;&#22411;&#26469;&#35828;&#65292;&#23427;&#20204;&#22914;&#20309;&#20351;&#29992;&#36825;&#20123;&#35828;&#26126;&#20070;&#36824;&#19981;&#28165;&#26970;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;FollowIR&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20005;&#26684;&#30340;&#35828;&#26126;&#20070;&#35780;&#20272;&#22522;&#20934;&#65292;&#20197;&#21450;&#19968;&#20010;&#35757;&#32451;&#38598;&#65292;&#24110;&#21161;IR&#27169;&#22411;&#23398;&#20064;&#26356;&#22909;&#22320;&#36981;&#24490;&#29616;&#23454;&#19990;&#30028;&#30340;&#35828;&#26126;&#20070;&#12290;FollowIR&#22522;&#20110;TREC&#20250;&#35758;&#30340;&#24736;&#20037;&#21382;&#21490;&#65306;&#27491;&#22914;TREC&#20026;&#20154;&#31867;&#26631;&#27880;&#21592;&#25552;&#20379;&#35828;&#26126;&#20070;&#65288;&#20063;&#31216;&#20026;&#21465;&#36848;&#65289;&#26469;&#21028;&#26029;&#25991;&#26723;&#30340;&#30456;&#20851;&#24615;&#19968;&#26679;&#65292;&#22240;&#27492;IR&#27169;&#22411;&#24212;&#35813;&#33021;&#22815;&#26681;&#25454;&#36825;&#20123;&#35814;&#32454;&#35828;&#26126;&#20070;&#29702;&#35299;&#21644;&#30830;&#23450;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#22522;&#20934;&#20174;&#19977;&#20010;&#32463;&#36807;&#28145;&#24230;&#21028;&#26029;&#30340;TREC&#25910;&#34255;&#24320;&#22987;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15246v1 Announce Type: cross  Abstract: Modern Large Language Models (LLMs) are capable of following long and complex instructions that enable a diverse amount of user tasks. However, despite Information Retrieval (IR) models using LLMs as the backbone of their architectures, nearly all of them still only take queries as input, with no instructions. For the handful of recent models that do take instructions, it's unclear how they use them. We introduce our dataset FollowIR, which contains a rigorous instruction evaluation benchmark as well as a training set for helping IR models learn to better follow real-world instructions. FollowIR builds off the long history of the TREC conferences: as TREC provides human annotators with instructions (also known as narratives) to determine document relevance, so should IR models be able to understand and decide relevance based on these detailed instructions. Our evaluation benchmark starts with three deeply judged TREC collections and al
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;HSTU&#26550;&#26500;&#65292;&#29992;&#20110;&#39640;&#22522;&#25968;&#12289;&#38750;&#24179;&#31283;&#27969;&#25512;&#33616;&#25968;&#25454;&#65292;&#24615;&#33021;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#39640;&#36798;65.8&#65285;&#30340;NDCG&#65292;&#24182;&#19988;&#27604;&#22522;&#20110;FlashAttention2&#30340;Transformer&#22312;8192&#38271;&#24230;&#24207;&#21015;&#19978;&#24555;5.3&#20493;&#21040;15.2&#20493;&#12290;</title><link>https://arxiv.org/abs/2402.17152</link><description>&lt;p&gt;
&#34892;&#21160;&#32988;&#36807;&#35328;&#36766;&#65306;&#29992;&#20110;&#29983;&#25104;&#25512;&#33616;&#30340;&#21315;&#20159;&#21442;&#25968;&#39034;&#24207;&#36716;&#23548;&#22120;
&lt;/p&gt;
&lt;p&gt;
Actions Speak Louder than Words: Trillion-Parameter Sequential Transducers for Generative Recommendations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17152
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;HSTU&#26550;&#26500;&#65292;&#29992;&#20110;&#39640;&#22522;&#25968;&#12289;&#38750;&#24179;&#31283;&#27969;&#25512;&#33616;&#25968;&#25454;&#65292;&#24615;&#33021;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#39640;&#36798;65.8&#65285;&#30340;NDCG&#65292;&#24182;&#19988;&#27604;&#22522;&#20110;FlashAttention2&#30340;Transformer&#22312;8192&#38271;&#24230;&#24207;&#21015;&#19978;&#24555;5.3&#20493;&#21040;15.2&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#25512;&#33616;&#31995;&#32479;&#30340;&#29305;&#28857;&#26159;&#20381;&#36182;&#20110;&#39640;&#22522;&#25968;&#12289;&#24322;&#26500;&#29305;&#24449;&#65292;&#24182;&#19988;&#38656;&#35201;&#27599;&#22825;&#22788;&#29702;&#25968;&#21313;&#20159;&#29992;&#25143;&#34892;&#20026;&#12290;&#23613;&#31649;&#22312;&#25104;&#21315;&#19978;&#19975;&#20010;&#29305;&#24449;&#19978;&#35757;&#32451;&#20102;&#22823;&#37327;&#25968;&#25454;&#65292;&#20294;&#22823;&#22810;&#25968;&#34892;&#19994;&#20013;&#30340;&#28145;&#24230;&#23398;&#20064;&#25512;&#33616;&#27169;&#22411;(DLRMs)&#22312;&#35745;&#31639;&#26041;&#38754;&#26080;&#27861;&#25193;&#23637;&#12290;&#21463;&#21040;&#22312;&#35821;&#35328;&#21644;&#35270;&#35273;&#39046;&#22495;&#21462;&#24471;&#25104;&#21151;&#30340;Transformer&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#22522;&#26412;&#35774;&#35745;&#36873;&#25321;&#12290;&#25105;&#20204;&#23558;&#25512;&#33616;&#38382;&#39064;&#37325;&#26032;&#26500;&#24314;&#20026;&#29983;&#25104;&#24314;&#27169;&#26694;&#26550;&#20013;&#30340;&#39034;&#24207;&#36716;&#23548;&#20219;&#21153;&#65288;&#8220;&#29983;&#25104;&#25512;&#33616;&#32773;&#8221;&#65289;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#39640;&#22522;&#25968;&#12289;&#38750;&#24179;&#31283;&#27969;&#25512;&#33616;&#25968;&#25454;&#35774;&#35745;&#30340;&#26032;&#26550;&#26500;HSTU&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17152v1 Announce Type: new  Abstract: Large-scale recommendation systems are characterized by their reliance on high cardinality, heterogeneous features and the need to handle tens of billions of user actions on a daily basis. Despite being trained on huge volume of data with thousands of features, most Deep Learning Recommendation Models (DLRMs) in industry fail to scale with compute.   Inspired by success achieved by Transformers in language and vision domains, we revisit fundamental design choices in recommendation systems. We reformulate recommendation problems as sequential transduction tasks within a generative modeling framework (``Generative Recommenders''), and propose a new architecture, HSTU, designed for high cardinality, non-stationary streaming recommendation data.   HSTU outperforms baselines over synthetic and public datasets by up to 65.8\% in NDCG, and is 5.3x to 15.2x faster than FlashAttention2-based Transformers on 8192 length sequences. HSTU-based Gener
&lt;/p&gt;</description></item><item><title>LLaRA&#26159;&#19968;&#20010;&#23558;&#20256;&#32479;&#25512;&#33616;&#22120;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#28151;&#21512;&#26041;&#27861;&#26469;&#20195;&#34920;&#39033;&#30446;&#65292;&#22312;&#39034;&#24207;&#25512;&#33616;&#20013;&#20805;&#20998;&#21033;&#29992;&#20102;&#20256;&#32479;&#25512;&#33616;&#22120;&#30340;&#29992;&#25143;&#34892;&#20026;&#30693;&#35782;&#21644;LLMs&#30340;&#19990;&#30028;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2312.02445</link><description>&lt;p&gt;
LLaRA: &#20351;&#29992;&#39034;&#24207;&#25512;&#33616;&#22120;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
LLaRA: Aligning Large Language Models with Sequential Recommenders. (arXiv:2312.02445v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.02445
&lt;/p&gt;
&lt;p&gt;
LLaRA&#26159;&#19968;&#20010;&#23558;&#20256;&#32479;&#25512;&#33616;&#22120;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#28151;&#21512;&#26041;&#27861;&#26469;&#20195;&#34920;&#39033;&#30446;&#65292;&#22312;&#39034;&#24207;&#25512;&#33616;&#20013;&#20805;&#20998;&#21033;&#29992;&#20102;&#20256;&#32479;&#25512;&#33616;&#22120;&#30340;&#29992;&#25143;&#34892;&#20026;&#30693;&#35782;&#21644;LLMs&#30340;&#19990;&#30028;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39034;&#24207;&#25512;&#33616;&#26088;&#22312;&#26681;&#25454;&#29992;&#25143;&#30340;&#21382;&#21490;&#20132;&#20114;&#39044;&#27979;&#19982;&#29992;&#25143;&#20559;&#22909;&#30456;&#21305;&#37197;&#30340;&#21518;&#32493;&#39033;&#30446;&#12290;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#30340;&#21457;&#23637;&#65292;&#20154;&#20204;&#23545;&#20110;&#23558;LLMs &#24212;&#29992;&#20110;&#39034;&#24207;&#25512;&#33616;&#24182;&#23558;&#20854;&#35270;&#20026;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#30340;&#28508;&#21147;&#36234;&#26469;&#36234;&#24863;&#20852;&#36259;&#12290;&#20043;&#21069;&#30340;&#24037;&#20316;&#20013;&#65292;&#20351;&#29992;ID&#32034;&#24341;&#25110;&#25991;&#26412;&#32034;&#24341;&#26469;&#34920;&#31034;&#25991;&#26412;&#25552;&#31034;&#20013;&#30340;&#39033;&#30446;&#65292;&#24182;&#23558;&#25552;&#31034;&#36755;&#20837;LLMs&#65292;&#20294;&#26080;&#27861;&#20840;&#38754;&#34701;&#21512;&#19990;&#30028;&#30693;&#35782;&#25110;&#23637;&#31034;&#36275;&#22815;&#30340;&#39034;&#24207;&#29702;&#35299;&#33021;&#21147;&#12290;&#20026;&#20102;&#20805;&#20998;&#21457;&#25381;&#20256;&#32479;&#25512;&#33616;&#22120;&#65288;&#21487;&#20197;&#32534;&#30721;&#29992;&#25143;&#34892;&#20026;&#30693;&#35782;&#65289;&#21644;LLMs&#65288;&#20855;&#26377;&#39033;&#30446;&#30340;&#19990;&#30028;&#30693;&#35782;&#65289;&#30340;&#20114;&#34917;&#20248;&#21183;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LLaRA - &#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#21644;&#25512;&#33616;&#21161;&#25163;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;LLaRA&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#28151;&#21512;&#26041;&#27861;&#65292;&#23558;&#20256;&#32479;&#25512;&#33616;&#22120;&#30340;&#22522;&#20110;ID&#30340;&#39033;&#30446;&#23884;&#20837;&#19982;&#25991;&#26412;&#39033;&#30446;&#29305;&#24449;&#25972;&#21512;&#21040;LLM&#30340;&#36755;&#20837;&#25552;&#31034;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequential recommendation aims to predict the subsequent items matching user preference based on her/his historical interactions. With the development of Large Language Models (LLMs), there is growing interest in exploring the potential of LLMs for sequential recommendation by framing it as a language modeling task. Prior works represent items in the textual prompts using either ID indexing or text indexing and feed the prompts into LLMs, but falling short of either encapsulating comprehensive world knowledge or exhibiting sufficient sequential understanding. To harness the complementary strengths of traditional recommenders (which encode user behavioral knowledge) and LLMs (which possess world knowledge about items), we propose LLaRA -- a Large Language and Recommendation Assistant framework. Specifically, LLaRA represents items in LLM's input prompts using a novel hybrid approach that integrates ID-based item embeddings from traditional recommenders with textual item features. Viewin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#37325;&#26032;&#25490;&#21517;&#22120;FiT5&#65292;&#23427;&#23558;&#25991;&#26723;&#25991;&#26412;&#20449;&#24687;&#12289;&#26816;&#32034;&#29305;&#24449;&#21644;&#20840;&#23616;&#25991;&#26723;&#20449;&#24687;&#32479;&#19968;&#21040;&#19968;&#20010;&#21333;&#19968;&#30340;&#27169;&#22411;&#20013;&#65292;&#36890;&#36807;&#20840;&#23616;&#27880;&#24847;&#21147;&#20351;&#24471;FiT5&#33021;&#22815;&#20849;&#21516;&#21033;&#29992;&#25490;&#21517;&#29305;&#24449;&#65292;&#20174;&#32780;&#25913;&#21892;&#26816;&#27979;&#24494;&#22937;&#24046;&#21035;&#30340;&#33021;&#21147;&#65292;&#22312;&#23454;&#39564;&#34920;&#29616;&#19978;&#26174;&#33879;&#25552;&#39640;&#20102;&#25490;&#21517;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.14685</link><description>&lt;p&gt;
Fusion-in-T5: &#23558;&#25991;&#26723;&#25490;&#21517;&#20449;&#21495;&#32479;&#19968;&#36215;&#26469;&#20197;&#25913;&#36827;&#20449;&#24687;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Fusion-in-T5: Unifying Document Ranking Signals for Improved Information Retrieval. (arXiv:2305.14685v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14685
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#37325;&#26032;&#25490;&#21517;&#22120;FiT5&#65292;&#23427;&#23558;&#25991;&#26723;&#25991;&#26412;&#20449;&#24687;&#12289;&#26816;&#32034;&#29305;&#24449;&#21644;&#20840;&#23616;&#25991;&#26723;&#20449;&#24687;&#32479;&#19968;&#21040;&#19968;&#20010;&#21333;&#19968;&#30340;&#27169;&#22411;&#20013;&#65292;&#36890;&#36807;&#20840;&#23616;&#27880;&#24847;&#21147;&#20351;&#24471;FiT5&#33021;&#22815;&#20849;&#21516;&#21033;&#29992;&#25490;&#21517;&#29305;&#24449;&#65292;&#20174;&#32780;&#25913;&#21892;&#26816;&#27979;&#24494;&#22937;&#24046;&#21035;&#30340;&#33021;&#21147;&#65292;&#22312;&#23454;&#39564;&#34920;&#29616;&#19978;&#26174;&#33879;&#25552;&#39640;&#20102;&#25490;&#21517;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24120;&#35265;&#30340;&#20449;&#24687;&#26816;&#32034;&#27969;&#31243;&#36890;&#24120;&#37319;&#29992;&#32423;&#32852;&#31995;&#32479;&#65292;&#21487;&#33021;&#28041;&#21450;&#22810;&#20010;&#25490;&#21517;&#22120;&#21644;/&#25110;&#34701;&#21512;&#27169;&#22411;&#36880;&#27493;&#25972;&#21512;&#19981;&#21516;&#30340;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Fusion-in-T5&#65288;FiT5&#65289;&#30340;&#26032;&#22411;&#37325;&#26032;&#25490;&#21517;&#22120;&#65292;&#23427;&#20351;&#29992;&#22522;&#20110;&#27169;&#26495;&#30340;&#36755;&#20837;&#21644;&#20840;&#23616;&#27880;&#24847;&#21147;&#23558;&#25991;&#26723;&#25991;&#26412;&#20449;&#24687;&#12289;&#26816;&#32034;&#29305;&#24449;&#21644;&#20840;&#23616;&#25991;&#26723;&#20449;&#24687;&#32479;&#19968;&#21040;&#19968;&#20010;&#21333;&#19968;&#30340;&#27169;&#22411;&#20013;&#12290;&#22312;MS MARCO&#21644;TREC DL&#30340;&#27573;&#33853;&#25490;&#21517;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#23454;&#39564;&#34920;&#26126;FiT5&#22312;&#20808;&#21069;&#30340;&#27969;&#27700;&#32447;&#24615;&#33021;&#19978;&#26174;&#33879;&#25552;&#39640;&#20102;&#25490;&#21517;&#34920;&#29616;&#12290;&#20998;&#26512;&#21457;&#29616;&#65292;&#36890;&#36807;&#20840;&#23616;&#27880;&#24847;&#21147;&#65292;FiT5&#33021;&#22815;&#36880;&#28176;&#20851;&#27880;&#30456;&#20851;&#25991;&#26723;&#65292;&#20174;&#32780;&#20849;&#21516;&#21033;&#29992;&#25490;&#21517;&#29305;&#24449;&#65292;&#25913;&#21892;&#26816;&#27979;&#23427;&#20204;&#20043;&#38388;&#24494;&#22937;&#24046;&#21035;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#23558;&#24320;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Common IR pipelines are typically cascade systems that may involve multiple rankers and/or fusion models to integrate different information step-by-step. In this paper, we propose a novel re-ranker named Fusion-in-T5 (FiT5), which integrates document text information, retrieval features, and global document information into a single unified model using templated-based input and global attention. Experiments on passage ranking benchmarks MS MARCO and TREC DL show that FiT5 significantly improves ranking performance over prior pipelines. Analyses find that through global attention, FiT5 is able to jointly utilize the ranking features via gradually attending to related documents, and thus improve the detection of subtle nuances between them. Our code will be open-sourced.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20027;&#35201;&#30740;&#31350;&#20102;Web&#25628;&#32034;&#31995;&#32479;&#20013;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#21033;&#29992;&#24515;&#29702;&#27979;&#37327;&#21644;&#20247;&#21253;&#25216;&#26415;&#65292;&#20998;&#26512;&#20102;&#21487;&#35299;&#37322;&#24615;&#30340;&#22810;&#20010;&#22240;&#32032;&#65292;&#20197;&#26399;&#25214;&#21040;&#35299;&#37322;&#24615;&#19982;&#20154;&#31867;&#22240;&#32032;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2210.09430</link><description>&lt;p&gt;
&#36890;&#36807;&#24515;&#29702;&#27979;&#37327;&#21644;&#20247;&#21253;&#35780;&#20272;&#25628;&#32034;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Evaluating Search Explainability with Psychometrics and Crowdsourcing. (arXiv:2210.09430v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.09430
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20027;&#35201;&#30740;&#31350;&#20102;Web&#25628;&#32034;&#31995;&#32479;&#20013;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#21033;&#29992;&#24515;&#29702;&#27979;&#37327;&#21644;&#20247;&#21253;&#25216;&#26415;&#65292;&#20998;&#26512;&#20102;&#21487;&#35299;&#37322;&#24615;&#30340;&#22810;&#20010;&#22240;&#32032;&#65292;&#20197;&#26399;&#25214;&#21040;&#35299;&#37322;&#24615;&#19982;&#20154;&#31867;&#22240;&#32032;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#31995;&#32479;&#24050;&#25104;&#20026;&#25105;&#20204;&#26085;&#24120;&#29983;&#27963;&#20013;&#19981;&#21487;&#25110;&#32570;&#30340;&#19968;&#37096;&#20998;&#12290;&#30001;&#20110;&#25628;&#32034;&#24341;&#25806;&#12289;&#25512;&#33616;&#31995;&#32479;&#21644;&#23545;&#35805;&#20195;&#29702;&#22312;&#20174;&#23089;&#20048;&#25628;&#32034;&#21040;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#31561;&#21508;&#20010;&#39046;&#22495;&#24471;&#21040;&#24212;&#29992;&#65292;&#22240;&#27492;&#38656;&#35201;&#36879;&#26126;&#21644;&#21487;&#35299;&#37322;&#30340;&#31995;&#32479;&#26469;&#30830;&#20445;&#21487;&#36861;&#28335;&#12289;&#20844;&#27491;&#21644;&#26080;&#20559;&#35265;&#30340;&#32467;&#26524;&#12290;&#23613;&#31649;&#22312;&#21487;&#35299;&#37322;&#30340;AI&#21644;IR&#25216;&#26415;&#26041;&#38754;&#21462;&#24471;&#20102;&#35768;&#22810;&#36817;&#26399;&#36827;&#23637;&#65292;&#20294;&#20173;&#26080;&#27861;&#23601;&#31995;&#32479;&#21487;&#35299;&#37322;&#24615;&#30340;&#21547;&#20041;&#36798;&#25104;&#20849;&#35782;&#12290;&#34429;&#28982;&#36234;&#26469;&#36234;&#22810;&#30340;&#25991;&#29486;&#34920;&#26126;&#65292;&#35299;&#37322;&#24615;&#21253;&#21547;&#22810;&#20010;&#23376;&#22240;&#32032;&#65292;&#20294;&#23454;&#38469;&#19978;&#25152;&#26377;&#29616;&#26377;&#26041;&#27861;&#20960;&#20046;&#37117;&#23558;&#20854;&#35270;&#20026;&#21333;&#19968;&#27010;&#24565;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#24515;&#29702;&#27979;&#37327;&#21644;&#20247;&#21253;&#30740;&#31350;&#20102;Web&#25628;&#32034;&#31995;&#32479;&#20013;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#20197;&#30830;&#23450;&#20154;&#31867;&#20013;&#24515;&#22240;&#32032;&#19982;&#21487;&#35299;&#37322;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Information retrieval (IR) systems have become an integral part of our everyday lives. As search engines, recommender systems, and conversational agents are employed across various domains from recreational search to clinical decision support, there is an increasing need for transparent and explainable systems to guarantee accountable, fair, and unbiased results. Despite many recent advances towards explainable AI and IR techniques, there is no consensus on what it means for a system to be explainable. Although a growing body of literature suggests that explainability is comprised of multiple subfactors, virtually all existing approaches treat it as a singular notion. In this paper, we examine explainability in Web search systems, leveraging psychometrics and crowdsourcing to identify human-centered factors of explainability.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#33258;&#21160;&#21307;&#30103;&#32534;&#30721;&#39046;&#22495;&#30340;&#21457;&#23637;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#65292;&#24635;&#32467;&#20102;&#26368;&#26032;&#30340;&#39640;&#32423;&#27169;&#22411;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#21457;&#23637;&#30340;&#25361;&#25112;&#21644;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2201.02797</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#33258;&#21160;&#21307;&#30103;&#32534;&#30721;&#20013;&#30340;&#24212;&#29992;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Unified Review of Deep Learning for Automated Medical Coding. (arXiv:2201.02797v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.02797
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#33258;&#21160;&#21307;&#30103;&#32534;&#30721;&#39046;&#22495;&#30340;&#21457;&#23637;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#65292;&#24635;&#32467;&#20102;&#26368;&#26032;&#30340;&#39640;&#32423;&#27169;&#22411;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#21457;&#23637;&#30340;&#25361;&#25112;&#21644;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21307;&#30103;&#32534;&#30721;&#26159;&#21307;&#30103;&#36816;&#33829;&#21644;&#26381;&#21153;&#30340;&#22522;&#26412;&#20219;&#21153;&#65292;&#36890;&#36807;&#20174;&#20020;&#24202;&#25991;&#26723;&#20013;&#39044;&#27979;&#21307;&#30103;&#32534;&#30721;&#26469;&#31649;&#29702;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#12290;&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#36827;&#27493;&#24050;&#24191;&#27867;&#24212;&#29992;&#20110;&#35813;&#20219;&#21153;&#12290;&#20294;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#33258;&#21160;&#21307;&#30103;&#32534;&#30721;&#32570;&#20047;&#23545;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#35774;&#35745;&#30340;&#32479;&#19968;&#35270;&#22270;&#12290;&#26412;&#32508;&#36848;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#65292;&#20197;&#25552;&#20379;&#23545;&#21307;&#30103;&#32534;&#30721;&#27169;&#22411;&#32452;&#20214;&#30340;&#19968;&#33324;&#29702;&#35299;&#65292;&#24182;&#24635;&#32467;&#20102;&#22312;&#27492;&#26694;&#26550;&#19979;&#26368;&#36817;&#30340;&#39640;&#32423;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#32479;&#19968;&#26694;&#26550;&#23558;&#21307;&#30103;&#32534;&#30721;&#20998;&#35299;&#20026;&#22235;&#20010;&#20027;&#35201;&#32452;&#20214;&#65292;&#21363;&#29992;&#20110;&#25991;&#26412;&#29305;&#24449;&#25552;&#21462;&#30340;&#32534;&#30721;&#22120;&#27169;&#22359;&#12289;&#26500;&#24314;&#28145;&#24230;&#32534;&#30721;&#22120;&#26550;&#26500;&#30340;&#26426;&#21046;&#12289;&#29992;&#20110;&#23558;&#38544;&#34255;&#34920;&#31034;&#36716;&#25442;&#25104;&#21307;&#30103;&#20195;&#30721;&#30340;&#35299;&#30721;&#22120;&#27169;&#22359;&#20197;&#21450;&#36741;&#21161;&#20449;&#24687;&#30340;&#20351;&#29992;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#22522;&#20934;&#21644;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#20351;&#29992;&#24773;&#20917;&#65292;&#35752;&#35770;&#20102;&#20851;&#38190;&#30340;&#30740;&#31350;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated medical coding, an essential task for healthcare operation and delivery, makes unstructured data manageable by predicting medical codes from clinical documents. Recent advances in deep learning and natural language processing have been widely applied to this task. However, deep learning-based medical coding lacks a unified view of the design of neural network architectures. This review proposes a unified framework to provide a general understanding of the building blocks of medical coding models and summarizes recent advanced models under the proposed framework. Our unified framework decomposes medical coding into four main components, i.e., encoder modules for text feature extraction, mechanisms for building deep encoder architectures, decoder modules for transforming hidden representations into medical codes, and the usage of auxiliary information. Finally, we introduce the benchmarks and real-world usage and discuss key research challenges and future directions.
&lt;/p&gt;</description></item></channel></rss>