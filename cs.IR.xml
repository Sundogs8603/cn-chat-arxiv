<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20004;&#22612;&#25512;&#33616;&#27169;&#22411;&#20013;&#20351;&#29992;&#21333;&#27425;&#21453;&#21521;&#20256;&#25773;&#26356;&#26032;&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#25361;&#25112;&#20102;&#29616;&#26377;&#31639;&#27861;&#20013;&#24179;&#31561;&#23545;&#24453;&#29992;&#25143;&#21644;&#29289;&#21697;&#30340;&#20551;&#35774;&#12290;</title><link>https://arxiv.org/abs/2403.18227</link><description>&lt;p&gt;
&#20004;&#22612;&#25512;&#33616;&#27169;&#22411;&#20013;&#30340;&#21333;&#27425;&#21453;&#21521;&#20256;&#25773;
&lt;/p&gt;
&lt;p&gt;
One Backpropagation in Two Tower Recommendation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18227
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20004;&#22612;&#25512;&#33616;&#27169;&#22411;&#20013;&#20351;&#29992;&#21333;&#27425;&#21453;&#21521;&#20256;&#25773;&#26356;&#26032;&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#25361;&#25112;&#20102;&#29616;&#26377;&#31639;&#27861;&#20013;&#24179;&#31561;&#23545;&#24453;&#29992;&#25143;&#21644;&#29289;&#21697;&#30340;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#24050;&#32463;&#30475;&#21040;&#20026;&#20102;&#20943;&#36731;&#20449;&#24687;&#36807;&#36733;&#32780;&#24320;&#21457;&#20004;&#22612;&#25512;&#33616;&#27169;&#22411;&#30340;&#24191;&#27867;&#30740;&#31350;&#12290;&#36825;&#31181;&#27169;&#22411;&#20013;&#21487;&#20197;&#35782;&#21035;&#20986;&#22235;&#20010;&#26500;&#24314;&#27169;&#22359;&#65292;&#20998;&#21035;&#26159;&#29992;&#25143;-&#29289;&#21697;&#32534;&#30721;&#12289;&#36127;&#37319;&#26679;&#12289;&#25439;&#22833;&#35745;&#31639;&#21644;&#21453;&#21521;&#20256;&#25773;&#26356;&#26032;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#29616;&#26377;&#31639;&#27861;&#20165;&#30740;&#31350;&#20102;&#21069;&#19977;&#20010;&#27169;&#22359;&#65292;&#21364;&#24573;&#30053;&#20102;&#21453;&#21521;&#20256;&#25773;&#27169;&#22359;&#12290;&#20182;&#20204;&#37117;&#37319;&#29992;&#26576;&#31181;&#24418;&#24335;&#30340;&#21452;&#21453;&#21521;&#20256;&#25773;&#31574;&#30053;&#65292;&#22522;&#20110;&#19968;&#20010;&#38544;&#21547;&#30340;&#20551;&#35774;&#65292;&#21363;&#22312;&#35757;&#32451;&#38454;&#27573;&#24179;&#31561;&#23545;&#24453;&#29992;&#25143;&#21644;&#29289;&#21697;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25361;&#25112;&#20102;&#36825;&#31181;&#24179;&#31561;&#35757;&#32451;&#20551;&#35774;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21333;&#27425;&#21453;&#21521;&#20256;&#25773;&#26356;&#26032;&#31574;&#30053;&#65292;&#36825;&#31181;&#31574;&#30053;&#20445;&#30041;&#20102;&#29289;&#21697;&#32534;&#30721;&#22612;&#30340;&#27491;&#24120;&#26799;&#24230;&#21453;&#21521;&#20256;&#25773;&#65292;&#20294;&#21066;&#20943;&#20102;&#29992;&#25143;&#32534;&#30721;&#22612;&#30340;&#21453;&#21521;&#20256;&#25773;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31227;&#21160;&#32858;&#21512;&#26356;&#26032;&#31574;&#30053;&#26469;&#26356;&#26032;&#27599;&#20010;&#35757;&#32451;&#21608;&#26399;&#20013;&#30340;&#29992;&#25143;&#32534;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18227v1 Announce Type: new  Abstract: Recent years have witnessed extensive researches on developing two tower recommendation models for relieving information overload. Four building modules can be identified in such models, namely, user-item encoding, negative sampling, loss computing and back-propagation updating. To the best of our knowledge, existing algorithms have researched only on the first three modules, yet neglecting the backpropagation module. They all adopt a kind of two backpropagation strategy, which are based on an implicit assumption of equally treating users and items in the training phase. In this paper, we challenge such an equal training assumption and propose a novel one backpropagation updating strategy, which keeps the normal gradient backpropagation for the item encoding tower, but cuts off the backpropagation for the user encoding tower. Instead, we propose a moving-aggregation updating strategy to update a user encoding in each training epoch. Exce
&lt;/p&gt;</description></item></channel></rss>