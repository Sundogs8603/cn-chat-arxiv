<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25552;&#20986;&#38646;&#26679;&#26412;&#19979;&#19968;&#20010;&#39033;&#30446;&#25512;&#33616;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#20351;&#29992;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#19979;&#19968;&#20010;&#39033;&#30446;&#25512;&#33616;&#20013;&#36935;&#21040;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2304.03153</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38646;&#26679;&#26412;&#19979;&#19968;&#20010;&#39033;&#30446;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Next-Item Recommendation using Large Pretrained Language Models. (arXiv:2304.03153v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03153
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25552;&#20986;&#38646;&#26679;&#26412;&#19979;&#19968;&#20010;&#39033;&#30446;&#25512;&#33616;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#20351;&#29992;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#19979;&#19968;&#20010;&#39033;&#30446;&#25512;&#33616;&#20013;&#36935;&#21040;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#38646;&#26679;&#26412;&#34920;&#29616;&#65292;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#27809;&#26377;&#35757;&#32451;&#31034;&#20363;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23578;&#26410;&#26377;&#30740;&#31350;&#25506;&#32034;LLMs&#22312;&#38646;&#26679;&#26412;&#24773;&#20917;&#19979;&#25191;&#34892;&#19979;&#19968;&#20010;&#39033;&#30446;&#25512;&#33616;&#30340;&#28508;&#21147;&#12290;&#20316;&#32773;&#20204;&#30830;&#23450;&#20102;&#24517;&#39035;&#35299;&#20915;&#30340;&#20004;&#20010;&#20027;&#35201;&#38382;&#39064;&#65292;&#20197;&#20351;LLMs&#26377;&#25928;&#22320;&#20805;&#24403;&#25512;&#33616;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have achieved impressive zero-shot performance in various natural language processing (NLP) tasks, demonstrating their capabilities for inference without training examples. Despite their success, no research has yet explored the potential of LLMs to perform next-item recommendations in the zero-shot setting. We have identified two major challenges that must be addressed to enable LLMs to act effectively as recommenders. First, the recommendation space can be extremely large for LLMs, and LLMs do not know about the target user's past interacted items and preferences. To address this gap, we propose a prompting strategy called Zero-Shot Next-Item Recommendation (NIR) prompting that directs LLMs to make next-item recommendations. Specifically, the NIR-based strategy involves using an external module to generate candidate items based on user-filtering or item-filtering. Our strategy incorporates a 3-step prompting that guides GPT-3 to carry subtasks that captur
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#38395;&#25512;&#33616;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#20197;&#20415;&#22312;&#20505;&#36873;&#20154;&#24863;&#30693;&#29992;&#25143;&#24314;&#27169;&#12289;&#28857;&#20987;&#34892;&#20026;&#34701;&#21512;&#21644;&#22521;&#35757;&#30446;&#26631;&#31561;&#20851;&#38190;&#35774;&#35745;&#32500;&#24230;&#19978;&#36827;&#34892;&#31995;&#32479;&#21644;&#20844;&#24179;&#30340;&#27604;&#36739;&#12290;&#35813;&#26694;&#26550;&#31616;&#21270;&#20102;&#27169;&#22411;&#35774;&#35745;&#21644;&#35757;&#32451;&#30446;&#26631;&#65292;&#24182;&#33719;&#24471;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.03112</link><description>&lt;p&gt;
&#31616;&#21270;&#22522;&#20110;&#20869;&#23481;&#30340;&#31070;&#32463;&#26032;&#38395;&#25512;&#33616;&#65306;&#20851;&#20110;&#29992;&#25143;&#24314;&#27169;&#21644;&#35757;&#32451;&#30446;&#26631;
&lt;/p&gt;
&lt;p&gt;
Simplifying Content-Based Neural News Recommendation: On User Modeling and Training Objectives. (arXiv:2304.03112v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03112
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#38395;&#25512;&#33616;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#20197;&#20415;&#22312;&#20505;&#36873;&#20154;&#24863;&#30693;&#29992;&#25143;&#24314;&#27169;&#12289;&#28857;&#20987;&#34892;&#20026;&#34701;&#21512;&#21644;&#22521;&#35757;&#30446;&#26631;&#31561;&#20851;&#38190;&#35774;&#35745;&#32500;&#24230;&#19978;&#36827;&#34892;&#31995;&#32479;&#21644;&#20844;&#24179;&#30340;&#27604;&#36739;&#12290;&#35813;&#26694;&#26550;&#31616;&#21270;&#20102;&#27169;&#22411;&#35774;&#35745;&#21644;&#35757;&#32451;&#30446;&#26631;&#65292;&#24182;&#33719;&#24471;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#26032;&#38395;&#25512;&#33616;&#30340;&#20986;&#29616;&#20351;&#24471;&#25512;&#33616;&#20307;&#31995;&#32467;&#26500;&#21464;&#24471;&#36234;&#26469;&#36234;&#22797;&#26434;&#12290;&#22823;&#22810;&#25968;&#31070;&#32463;&#26032;&#38395;&#25512;&#33616;&#22120;&#20381;&#36182;&#20110;&#29992;&#25143;&#28857;&#20987;&#34892;&#20026;&#65292;&#36890;&#24120;&#24341;&#20837;&#19987;&#38376;&#30340;&#29992;&#25143;&#32534;&#30721;&#22120;&#23558;&#28857;&#20987;&#26032;&#38395;&#20869;&#23481;&#32858;&#21512;&#25104;&#29992;&#25143;&#23884;&#20837;&#65288;&#26089;&#26399;&#34701;&#21512;&#65289;&#12290;&#36825;&#20123;&#27169;&#22411;&#20027;&#35201;&#36890;&#36807;&#26631;&#20934;&#30340;&#36880;&#28857;&#20998;&#31867;&#30446;&#26631;&#36827;&#34892;&#35757;&#32451;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#32570;&#28857;&#65306;&#65288;1&#65289;&#23613;&#31649;&#35774;&#35745;&#26222;&#36941;&#30456;&#21516;&#65292;&#20294;&#30001;&#20110;&#35780;&#20272;&#25968;&#25454;&#38598;&#21644;&#21327;&#35758;&#30340;&#19981;&#21516;&#65292;&#27169;&#22411;&#20043;&#38388;&#30340;&#30452;&#25509;&#27604;&#36739;&#21463;&#21040;&#20102;&#38459;&#30861;; &#65288;2&#65289;&#30041;&#32473;&#20102;&#26367;&#20195;&#30340;&#27169;&#22411;&#35774;&#35745;&#21644;&#35757;&#32451;&#30446;&#26631;&#22823;&#37327;&#30340;&#26410;&#24320;&#21457;&#31354;&#38388;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#38395;&#25512;&#33616;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#20801;&#35768;&#22312;&#20960;&#20010;&#20851;&#38190;&#35774;&#35745;&#32500;&#24230;&#19978;&#31995;&#32479;&#22320;&#21644;&#20844;&#24179;&#22320;&#27604;&#36739;&#26032;&#38395;&#25512;&#33616;&#22120;: &#65288;i&#65289;&#20505;&#36873;&#20154;&#24863;&#30693;&#29992;&#25143;&#24314;&#27169;&#65292;&#65288;ii&#65289;&#28857;&#20987;&#34892;&#20026;&#34701;&#21512;&#65292;&#21644;&#65288;iii&#65289;&#22521;&#35757;&#30446;&#26631;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#25361;&#25112;&#20102;&#31070;&#32463;&#26032;&#38395;&#25512;&#33616;&#30340;&#29616;&#29366;&#65292;&#24182;&#31361;&#26174;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#65292;&#23427;&#20351;&#29992;&#38750;&#24120;&#31616;&#21270;&#30340;&#27169;&#22411;&#35774;&#35745;&#21644;&#35757;&#32451;&#30446;&#26631;&#23454;&#29616;&#20102;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advent of personalized news recommendation has given rise to increasingly complex recommender architectures. Most neural news recommenders rely on user click behavior and typically introduce dedicated user encoders that aggregate the content of clicked news into user embeddings (early fusion). These models are predominantly trained with standard point-wise classification objectives. The existing body of work exhibits two main shortcomings: (1) despite general design homogeneity, direct comparisons between models are hindered by varying evaluation datasets and protocols; (2) it leaves alternative model designs and training objectives vastly unexplored. In this work, we present a unified framework for news recommendation, allowing for a systematic and fair comparison of news recommenders across several crucial design dimensions: (i) candidate-awareness in user modeling, (ii) click behavior fusion, and (iii) training objectives. Our findings challenge the status quo in neural news rec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#21033;&#29992;&#21512;&#25104;&#30340;&#24694;&#24847;&#29992;&#25143;&#19978;&#20256;&#26377;&#27602;&#30340;&#26799;&#24230;&#26469;&#22312;&#32852;&#37030;&#25512;&#33616;&#31995;&#32479;&#20013;&#26377;&#25928;&#22320;&#25805;&#32437;&#30446;&#26631;&#29289;&#21697;&#30340;&#25490;&#21517;&#21644;&#26333;&#20809;&#29575;&#12290;&#22312;&#20004;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#25512;&#33616;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;</title><link>http://arxiv.org/abs/2304.03054</link><description>&lt;p&gt;
&#25805;&#32437;&#32852;&#37030;&#25512;&#33616;&#31995;&#32479;: &#29992;&#21512;&#25104;&#29992;&#25143;&#36827;&#34892;&#25915;&#20987;&#21450;&#20854;&#23545;&#31574;
&lt;/p&gt;
&lt;p&gt;
Manipulating Federated Recommender Systems: Poisoning with Synthetic Users and Its Countermeasures. (arXiv:2304.03054v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03054
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#21033;&#29992;&#21512;&#25104;&#30340;&#24694;&#24847;&#29992;&#25143;&#19978;&#20256;&#26377;&#27602;&#30340;&#26799;&#24230;&#26469;&#22312;&#32852;&#37030;&#25512;&#33616;&#31995;&#32479;&#20013;&#26377;&#25928;&#22320;&#25805;&#32437;&#30446;&#26631;&#29289;&#21697;&#30340;&#25490;&#21517;&#21644;&#26333;&#20809;&#29575;&#12290;&#22312;&#20004;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#25512;&#33616;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#25512;&#33616;&#31995;&#32479;&#65288;FedRecs&#65289;&#34987;&#35748;&#20026;&#26159;&#19968;&#31181;&#20445;&#25252;&#38544;&#31169;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#19981;&#20849;&#20139;&#29992;&#25143;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#21327;&#21516;&#23398;&#20064;&#25512;&#33616;&#27169;&#22411;&#12290;&#22240;&#20026;&#25152;&#26377;&#21442;&#19982;&#32773;&#37117;&#21487;&#20197;&#36890;&#36807;&#19978;&#20256;&#26799;&#24230;&#30452;&#25509;&#24433;&#21709;&#31995;&#32479;&#65292;&#25152;&#20197;FedRecs&#23481;&#26131;&#21463;&#21040;&#24694;&#24847;&#23458;&#25143;&#30340;&#25915;&#20987;&#65292;&#23588;&#20854;&#26159;&#21033;&#29992;&#21512;&#25104;&#29992;&#25143;&#36827;&#34892;&#30340;&#25915;&#20987;&#26356;&#21152;&#26377;&#25928;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#20381;&#36182;&#20219;&#20309;&#20808;&#21069;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#19968;&#32452;&#21512;&#25104;&#30340;&#24694;&#24847;&#29992;&#25143;&#19978;&#20256;&#26377;&#27602;&#30340;&#26799;&#24230;&#26469;&#26377;&#25928;&#22320;&#25805;&#32437;&#30446;&#26631;&#29289;&#21697;&#30340;&#25490;&#21517;&#21644;&#26333;&#20809;&#29575;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#25512;&#33616;&#25968;&#25454;&#38598;&#19978;&#23545;&#20004;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;FedRecs &#65288;Fed-NCF&#21644;Fed-LightGCN&#65289;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Recommender Systems (FedRecs) are considered privacy-preserving techniques to collaboratively learn a recommendation model without sharing user data. Since all participants can directly influence the systems by uploading gradients, FedRecs are vulnerable to poisoning attacks of malicious clients. However, most existing poisoning attacks on FedRecs are either based on some prior knowledge or with less effectiveness. To reveal the real vulnerability of FedRecs, in this paper, we present a new poisoning attack method to manipulate target items' ranks and exposure rates effectively in the top-$K$ recommendation without relying on any prior knowledge. Specifically, our attack manipulates target items' exposure rate by a group of synthetic malicious users who upload poisoned gradients considering target items' alternative products. We conduct extensive experiments with two widely used FedRecs (Fed-NCF and Fed-LightGCN) on two real-world recommendation datasets. The experimental res
&lt;/p&gt;</description></item><item><title>TagGPT&#26159;&#19968;&#20010;&#38646;-shot&#22810;&#27169;&#24577;&#26631;&#27880;&#22120;&#65292;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#25552;&#31034;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#19968;&#31995;&#21015;&#21407;&#22987;&#25968;&#25454;&#20013;&#39044;&#27979;&#22823;&#35268;&#27169;&#30340;&#20505;&#36873;&#26631;&#31614;&#65292;&#24182;&#36827;&#34892;&#36807;&#28388;&#21644;&#35821;&#20041;&#20998;&#26512;&#65292;&#20026;&#29305;&#23450;&#24212;&#29992;&#31243;&#24207;&#26500;&#24314;&#39640;&#36136;&#37327;&#26631;&#31614;&#38598;&#12290;</title><link>http://arxiv.org/abs/2304.03022</link><description>&lt;p&gt;
TagGPT&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#38646;-shot&#22810;&#27169;&#24577;&#26631;&#27880;&#22120;
&lt;/p&gt;
&lt;p&gt;
TagGPT: Large Language Models are Zero-shot Multimodal Taggers. (arXiv:2304.03022v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03022
&lt;/p&gt;
&lt;p&gt;
TagGPT&#26159;&#19968;&#20010;&#38646;-shot&#22810;&#27169;&#24577;&#26631;&#27880;&#22120;&#65292;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#25552;&#31034;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#19968;&#31995;&#21015;&#21407;&#22987;&#25968;&#25454;&#20013;&#39044;&#27979;&#22823;&#35268;&#27169;&#30340;&#20505;&#36873;&#26631;&#31614;&#65292;&#24182;&#36827;&#34892;&#36807;&#28388;&#21644;&#35821;&#20041;&#20998;&#26512;&#65292;&#20026;&#29305;&#23450;&#24212;&#29992;&#31243;&#24207;&#26500;&#24314;&#39640;&#36136;&#37327;&#26631;&#31614;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#31614;&#22312;&#20419;&#36827;&#24403;&#20195;&#20114;&#32852;&#32593;&#26102;&#20195;&#21508;&#31181;&#24212;&#29992;&#20013;&#22810;&#23186;&#20307;&#20869;&#23481;&#30340;&#26377;&#25928;&#20998;&#21457;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#22914;&#25628;&#32034;&#24341;&#25806;&#21644;&#25512;&#33616;&#31995;&#32479;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#24778;&#20154;&#30340;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TagGPT&#65292;&#36825;&#26159;&#19968;&#20010;&#23436;&#20840;&#33258;&#21160;&#21270;&#30340;&#31995;&#32479;&#65292;&#33021;&#22815;&#20197;&#23436;&#20840;&#38646;-shot&#30340;&#26041;&#24335;&#36827;&#34892;&#26631;&#31614;&#25552;&#21462;&#21644;&#22810;&#27169;&#24577;&#26631;&#27880;&#12290;&#25105;&#20204;&#30340;&#26680;&#24515;&#35265;&#35299;&#26159;&#65292;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#25552;&#31034;&#65292;LLM&#33021;&#22815;&#22312;&#32473;&#23450;&#35270;&#35273;&#12289;&#35821;&#38899;&#31561;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#25991;&#26412;&#32447;&#32034;&#30340;&#24773;&#20917;&#19979;&#25552;&#21462;&#21644;&#25512;&#29702;&#27491;&#30830;&#30340;&#26631;&#31614;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#20026;&#20102;&#33258;&#21160;&#26500;&#24314;&#21453;&#26144;&#29305;&#23450;&#24212;&#29992;&#31243;&#24207;&#20013;&#29992;&#25143;&#24847;&#22270;&#21644;&#20852;&#36259;&#30340;&#39640;&#36136;&#37327;&#26631;&#31614;&#38598;&#65292;TagGPT&#36890;&#36807;&#25552;&#31034;LLM&#20174;&#19968;&#31995;&#21015;&#21407;&#22987;&#25968;&#25454;&#20013;&#39044;&#27979;&#22823;&#35268;&#27169;&#30340;&#20505;&#36873;&#26631;&#31614;&#65292;&#24182;&#36827;&#34892;&#20102;&#36807;&#28388;&#21644;&#35821;&#20041;&#20998;&#26512;&#12290;&#23545;&#20110;&#38656;&#35201;&#20998;&#21457;&#26631;&#35760;&#30340;&#26032;&#23454;&#20307;&#65292;TagGPT&#25552;&#20986;&#20102;&#20004;&#20010;&#38646;-shot&#36873;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tags are pivotal in facilitating the effective distribution of multimedia content in various applications in the contemporary Internet era, such as search engines and recommendation systems. Recently, large language models (LLMs) have demonstrated impressive capabilities across a wide range of tasks. In this work, we propose TagGPT, a fully automated system capable of tag extraction and multimodal tagging in a completely zero-shot fashion. Our core insight is that, through elaborate prompt engineering, LLMs are able to extract and reason about proper tags given textual clues of multimodal data, e.g., OCR, ASR, title, etc. Specifically, to automatically build a high-quality tag set that reflects user intent and interests for a specific application, TagGPT predicts large-scale candidate tags from a series of raw data via prompting LLMs, filtered with frequency and semantics. Given a new entity that needs tagging for distribution, TagGPT introduces two alternative options for zero-shot ta
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HGCC&#30340;&#21327;&#21516;&#36807;&#28388;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#28155;&#21152;&#24130;&#24459;&#20559;&#24046;&#26469;&#20445;&#25345;&#21327;&#21516;&#22270;&#30340;&#38271;&#23614;&#24615;&#36136;&#65292;&#24182;&#30452;&#25509;&#32858;&#21512;&#37051;&#23621;&#33410;&#28857;&#65292;&#20197;&#25552;&#39640;&#25512;&#33616;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.02961</link><description>&lt;p&gt;
HGCC&#65306;&#25552;&#39640;&#24322;&#26500;&#21327;&#21516;&#22270;&#19978;&#30340;&#36229;&#20960;&#20309;&#22270;&#21367;&#31215;&#32593;&#32476;&#29992;&#20110;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
HGCC: Enhancing Hyperbolic Graph Convolution Networks on Heterogeneous Collaborative Graph for Recommendation. (arXiv:2304.02961v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02961
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HGCC&#30340;&#21327;&#21516;&#36807;&#28388;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#28155;&#21152;&#24130;&#24459;&#20559;&#24046;&#26469;&#20445;&#25345;&#21327;&#21516;&#22270;&#30340;&#38271;&#23614;&#24615;&#36136;&#65292;&#24182;&#30452;&#25509;&#32858;&#21512;&#37051;&#23621;&#33410;&#28857;&#65292;&#20197;&#25552;&#39640;&#25512;&#33616;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#25512;&#33616;&#20219;&#21153;&#20013;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#25968;&#25454;&#30340;&#33258;&#28982;&#24130;&#24459;&#20998;&#24067;&#29305;&#24615;&#65292;&#36229;&#20960;&#20309;&#31354;&#38388;&#24314;&#27169;&#24050;&#34987;&#24341;&#20837;&#21327;&#21516;&#36807;&#28388;&#26041;&#27861;&#20013;&#12290;&#20854;&#20013;&#65292;&#36229;&#20960;&#20309;GCN&#32467;&#21512;&#20102;GCN&#21644;&#36229;&#20960;&#20309;&#31354;&#38388;&#30340;&#20248;&#21183;&#65292;&#24182;&#21462;&#24471;&#20102;&#20196;&#20154;&#24778;&#35766;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20165;&#22312;&#35774;&#35745;&#20013;&#37096;&#20998;&#21033;&#29992;&#20102;&#36229;&#20960;&#20309;&#31354;&#38388;&#30340;&#29305;&#24615;&#65292;&#30001;&#20110;&#23436;&#20840;&#38543;&#26426;&#30340;&#23884;&#20837;&#21021;&#22987;&#21270;&#21644;&#19981;&#31934;&#30830;&#30340;&#20999;&#32447;&#31354;&#38388;&#32858;&#21512;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#24037;&#20316;&#20013;&#20351;&#29992;&#30340;&#25968;&#25454;&#20027;&#35201;&#38598;&#20013;&#22312;&#20165;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#25968;&#25454;&#20013;&#65292;&#36825;&#36827;&#19968;&#27493;&#38480;&#21046;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36229;&#20960;&#20309;GCN&#21327;&#21516;&#36807;&#28388;&#27169;&#22411;HGCC&#65292;&#23427;&#25913;&#36827;&#20102;&#29616;&#26377;&#30340;&#36229;&#20960;&#20309;GCN&#32467;&#26500;&#65292;&#29992;&#20110;&#21327;&#21516;&#36807;&#28388;&#24182;&#32435;&#20837;&#20102;&#38468;&#21152;&#20449;&#24687;&#12290;&#23427;&#36890;&#36807;&#22312;&#33410;&#28857;&#23884;&#20837;&#21021;&#22987;&#21270;&#26102;&#28155;&#21152;&#24130;&#24459;&#20559;&#24046;&#26469;&#20445;&#25345;&#21327;&#21516;&#22270;&#30340;&#38271;&#23614;&#24615;&#36136;&#65307;&#28982;&#21518;&#65292;&#23427;&#30452;&#25509;&#32858;&#21512;&#37051;&#23621;&#33410;&#28857;&#65292;&#20197;&#25552;&#39640;&#25512;&#33616;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the naturally power-law distributed nature of user-item interaction data in recommendation tasks, hyperbolic space modeling has recently been introduced into collaborative filtering methods. Among them, hyperbolic GCN combines the advantages of GCN and hyperbolic space and achieves a surprising performance. However, these methods only partially exploit the nature of hyperbolic space in their designs due to completely random embedding initialization and an inaccurate tangent space aggregation. In addition, the data used in these works mainly focus on user-item interaction data only, which further limits the performance of the models. In this paper, we propose a hyperbolic GCN collaborative filtering model, HGCC, which improves the existing hyperbolic GCN structure for collaborative filtering and incorporates side information. It keeps the long-tailed nature of the collaborative graph by adding power law prior to node embedding initialization; then, it aggregates neighbors directl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#36848;&#20102;&#35774;&#35745;&#30693;&#35782;&#20998;&#31867;&#21644;&#34920;&#31034;&#26041;&#27861;&#65292;&#20197;&#21450;&#36807;&#21435;&#25903;&#25345;&#35774;&#35745;&#24072;&#33719;&#24471;&#30693;&#35782;&#30340;&#21162;&#21147;&#12290;&#36890;&#36807;&#20998;&#26512; ChatGPT &#24433;&#21709;&#35774;&#35745;&#30693;&#35782;&#31649;&#29702;&#30340;&#26426;&#36935;&#21644;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#65292;&#24182;&#36827;&#34892;&#20102;&#39564;&#35777;&#65292;&#32467;&#26524;&#34920;&#26126;&#35774;&#35745;&#24072;&#21487;&#20197;&#20174;&#21508;&#20010;&#39046;&#22495;&#33719;&#21462;&#26377;&#38024;&#23545;&#24615;&#30340;&#30693;&#35782;&#65292;&#20294;&#30693;&#35782;&#30340;&#36136;&#37327;&#39640;&#24230;&#20381;&#36182;&#20110;&#25552;&#31034;&#12290;</title><link>http://arxiv.org/abs/2304.02796</link><description>&lt;p&gt;
ChatGPT &#22312;&#35774;&#35745;&#30693;&#35782;&#31649;&#29702;&#20013;&#30340;&#26426;&#36935;&#21644;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Opportunities and challenges of ChatGPT for design knowledge management. (arXiv:2304.02796v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02796
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#36848;&#20102;&#35774;&#35745;&#30693;&#35782;&#20998;&#31867;&#21644;&#34920;&#31034;&#26041;&#27861;&#65292;&#20197;&#21450;&#36807;&#21435;&#25903;&#25345;&#35774;&#35745;&#24072;&#33719;&#24471;&#30693;&#35782;&#30340;&#21162;&#21147;&#12290;&#36890;&#36807;&#20998;&#26512; ChatGPT &#24433;&#21709;&#35774;&#35745;&#30693;&#35782;&#31649;&#29702;&#30340;&#26426;&#36935;&#21644;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#65292;&#24182;&#36827;&#34892;&#20102;&#39564;&#35777;&#65292;&#32467;&#26524;&#34920;&#26126;&#35774;&#35745;&#24072;&#21487;&#20197;&#20174;&#21508;&#20010;&#39046;&#22495;&#33719;&#21462;&#26377;&#38024;&#23545;&#24615;&#30340;&#30693;&#35782;&#65292;&#20294;&#30693;&#35782;&#30340;&#36136;&#37327;&#39640;&#24230;&#20381;&#36182;&#20110;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#30340;&#36827;&#27493;&#20351;&#24471;&#20687; ChatGPT &#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#20415;&#25463;&#22320;&#20026;&#35774;&#35745;&#24072;&#25552;&#20379;&#24191;&#27867;&#30340;&#30456;&#20851;&#20449;&#24687;&#65292;&#20197;&#20419;&#36827;&#35774;&#35745;&#36807;&#31243;&#20013;&#30340;&#30693;&#35782;&#31649;&#29702;&#12290;&#28982;&#32780;&#65292;&#23558; ChatGPT &#23548;&#20837;&#35774;&#35745;&#36807;&#31243;&#20063;&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#31616;&#35201;&#35780;&#36848;&#20102;&#35774;&#35745;&#30693;&#35782;&#30340;&#20998;&#31867;&#21644;&#34920;&#31034;&#26041;&#24335;&#65292;&#20197;&#21450;&#25903;&#25345;&#35774;&#35745;&#24072;&#33719;&#21462;&#30693;&#35782;&#30340;&#20808;&#21069;&#21162;&#21147;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102; ChatGPT &#22312;&#35774;&#35745;&#30693;&#35782;&#31649;&#29702;&#20013;&#25152;&#24102;&#26469;&#30340;&#26426;&#36935;&#21644;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#26377;&#21069;&#36884;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126; ChatGPT &#21487;&#20197;&#35753;&#35774;&#35745;&#24072;&#33719;&#21462;&#26469;&#33258;&#21508;&#20010;&#39046;&#22495;&#30340;&#26377;&#38024;&#23545;&#24615;&#30340;&#30693;&#35782;&#65292;&#20294;&#25152;&#33719;&#21462;&#30340;&#30693;&#35782;&#36136;&#37327;&#26497;&#22823;&#22320;&#20381;&#36182;&#20110;&#25552;&#31034;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in Natural Language Processing have opened up new possibilities for the development of large language models like ChatGPT, which can facilitate knowledge management in the design process by providing designers with access to a vast array of relevant information. However, integrating ChatGPT into the design process also presents new challenges. In this paper, we provide a concise review of the classification and representation of design knowledge, and past efforts to support designers in acquiring knowledge. We analyze the opportunities and challenges that ChatGPT presents for knowledge management in design and propose promising future research directions. A case study is conducted to validate the advantages and drawbacks of ChatGPT, showing that designers can acquire targeted knowledge from various domains, but the quality of the acquired knowledge is highly dependent on the prompt.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#20381;&#36182;&#30340;&#21704;&#24076;&#26041;&#27861;USR-LSH&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23637;&#24320;&#23454;&#20363;&#32423;&#25968;&#25454;&#37325;&#24314;&#30340;&#20248;&#21270;&#26356;&#26032;&#65292;&#25552;&#39640;&#20102;&#25968;&#25454;&#30340;&#20449;&#24687;&#20445;&#30041;&#33021;&#21147;&#65292;&#21516;&#26102;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#30340;&#36951;&#24536;&#26426;&#21046;&#65292;&#20351;&#24471;&#25968;&#25454;&#21487;&#20197;&#24555;&#36895;&#21024;&#38500;&#21644;&#25554;&#20837;&#65292;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#65292;&#36825;&#26159;&#19968;&#31181;&#20855;&#26377;&#25968;&#25454;&#38544;&#31169;&#21644;&#23433;&#20840;&#35201;&#27714;&#30340;&#22312;&#32447;ANN&#25628;&#32034;&#30340;&#23454;&#38469;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2304.02350</link><description>&lt;p&gt;
&#26410;&#25240;&#21472;&#33258;&#37325;&#24314;&#23616;&#37096;&#25935;&#24863;&#21704;&#24076;&#65306;&#36208;&#21521;&#36817;&#20284;&#26368;&#36817;&#37051;&#25628;&#32034;&#20013;&#30340;&#26426;&#22120;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
Unfolded Self-Reconstruction LSH: Towards Machine Unlearning in Approximate Nearest Neighbour Search. (arXiv:2304.02350v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02350
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#20381;&#36182;&#30340;&#21704;&#24076;&#26041;&#27861;USR-LSH&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23637;&#24320;&#23454;&#20363;&#32423;&#25968;&#25454;&#37325;&#24314;&#30340;&#20248;&#21270;&#26356;&#26032;&#65292;&#25552;&#39640;&#20102;&#25968;&#25454;&#30340;&#20449;&#24687;&#20445;&#30041;&#33021;&#21147;&#65292;&#21516;&#26102;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#30340;&#36951;&#24536;&#26426;&#21046;&#65292;&#20351;&#24471;&#25968;&#25454;&#21487;&#20197;&#24555;&#36895;&#21024;&#38500;&#21644;&#25554;&#20837;&#65292;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#65292;&#36825;&#26159;&#19968;&#31181;&#20855;&#26377;&#25968;&#25454;&#38544;&#31169;&#21644;&#23433;&#20840;&#35201;&#27714;&#30340;&#22312;&#32447;ANN&#25628;&#32034;&#30340;&#23454;&#38469;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#20284;&#26368;&#36817;&#37051;&#25628;&#32034;&#26159;&#25628;&#32034;&#24341;&#25806;&#12289;&#25512;&#33616;&#31995;&#32479;&#31561;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#35768;&#22810;&#26368;&#36817;&#30340;&#24037;&#20316;&#37117;&#26159;&#22522;&#20110;&#23398;&#20064;&#30340;&#25968;&#25454;&#20998;&#24067;&#20381;&#36182;&#21704;&#24076;&#65292;&#23454;&#29616;&#20102;&#33391;&#22909;&#30340;&#26816;&#32034;&#24615;&#33021;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#23545;&#29992;&#25143;&#38544;&#31169;&#21644;&#23433;&#20840;&#30340;&#38656;&#27714;&#19981;&#26029;&#22686;&#21152;&#65292;&#25105;&#20204;&#32463;&#24120;&#38656;&#35201;&#20174;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#21024;&#38500;&#29992;&#25143;&#25968;&#25454;&#20449;&#24687;&#20197;&#28385;&#36275;&#29305;&#23450;&#30340;&#38544;&#31169;&#21644;&#23433;&#20840;&#35201;&#27714;&#12290;&#36825;&#31181;&#38656;&#27714;&#38656;&#35201;ANN&#25628;&#32034;&#31639;&#27861;&#25903;&#25345;&#24555;&#36895;&#30340;&#22312;&#32447;&#25968;&#25454;&#21024;&#38500;&#21644;&#25554;&#20837;&#12290;&#24403;&#21069;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#21704;&#24076;&#26041;&#27861;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#21704;&#24076;&#20989;&#25968;&#65292;&#36825;&#26159;&#30001;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#30340;&#26102;&#38388;&#25104;&#26412;&#22826;&#39640;&#32780;&#38590;&#20197;&#25215;&#21463;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#25968;&#25454;&#20381;&#36182;&#21704;&#24076;&#26041;&#27861;&#65292;&#21517;&#20026;unfolded self-reconstruction locality-sensitive hashing (USR-LSH)&#12290;&#25105;&#20204;&#30340;USR-LSH&#23637;&#24320;&#20102;&#23454;&#20363;&#32423;&#25968;&#25454;&#37325;&#24314;&#30340;&#20248;&#21270;&#26356;&#26032;&#65292;&#36825;&#27604;&#25968;&#25454;&#26080;&#20851;&#30340;LSH&#26356;&#33021;&#20445;&#30041;&#25968;&#25454;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;USR-LSH&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#30340;&#36951;&#24536;&#26426;&#21046;&#65292;&#29992;&#20110;&#24555;&#36895;&#30340;&#25968;&#25454;&#21024;&#38500;&#21644;&#25554;&#20837;&#65292;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;USR-LSH&#22312;&#26816;&#32034;&#20934;&#30830;&#24615;&#21644;&#26102;&#38388;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#21704;&#24076;&#26041;&#27861;&#12290;USR-LSH&#26159;&#20855;&#26377;&#25968;&#25454;&#38544;&#31169;&#21644;&#23433;&#20840;&#35201;&#27714;&#30340;&#22312;&#32447;ANN&#25628;&#32034;&#30340;&#23454;&#38469;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Approximate nearest neighbour (ANN) search is an essential component of search engines, recommendation systems, etc. Many recent works focus on learning-based data-distribution-dependent hashing and achieve good retrieval performance. However, due to increasing demand for users' privacy and security, we often need to remove users' data information from Machine Learning (ML) models to satisfy specific privacy and security requirements. This need requires the ANN search algorithm to support fast online data deletion and insertion. Current learning-based hashing methods need retraining the hash function, which is prohibitable due to the vast time-cost of large-scale data. To address this problem, we propose a novel data-dependent hashing method named unfolded self-reconstruction locality-sensitive hashing (USR-LSH). Our USR-LSH unfolded the optimization update for instance-wise data reconstruction, which is better for preserving data information than data-independent LSH. Moreover, our US
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;CTR&#39044;&#27979;&#30340;&#22686;&#24378;&#21452;&#27969;MLP&#27169;&#22411;&#65292;&#32463;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#20165;&#26159;&#31616;&#21333;&#22320;&#32467;&#21512;&#20004;&#20010;MLP&#23601;&#21487;&#20197;&#23454;&#29616;&#20196;&#20154;&#24778;&#35766;&#30340;&#33391;&#22909;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.00902</link><description>&lt;p&gt;
FinalMLP: &#29992;&#20110;CTR&#39044;&#27979;&#30340;&#22686;&#24378;&#21452;&#27969;MLP&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
FinalMLP: An Enhanced Two-Stream MLP Model for CTR Prediction. (arXiv:2304.00902v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00902
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;CTR&#39044;&#27979;&#30340;&#22686;&#24378;&#21452;&#27969;MLP&#27169;&#22411;&#65292;&#32463;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#20165;&#26159;&#31616;&#21333;&#22320;&#32467;&#21512;&#20004;&#20010;MLP&#23601;&#21487;&#20197;&#23454;&#29616;&#20196;&#20154;&#24778;&#35766;&#30340;&#33391;&#22909;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28857;&#20987;&#29575;&#39044;&#27979;&#26159;&#22312;&#32447;&#24191;&#21578;&#21644;&#25512;&#33616;&#20013;&#30340;&#22522;&#26412;&#20219;&#21153;&#20043;&#19968;&#12290;&#34429;&#28982;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#22312;&#35768;&#22810;&#28145;&#24230;CTR&#39044;&#27979;&#27169;&#22411;&#20013;&#20316;&#20026;&#26680;&#24515;&#32452;&#20214;&#65292;&#20294;&#24191;&#20026;&#20154;&#30693;&#30340;&#26159;&#65292;&#20165;&#24212;&#29992;&#19968;&#20010;&#22522;&#26412;MLP&#32593;&#32476;&#22312;&#23398;&#20064;&#20056;&#27861;&#29305;&#24449;&#30456;&#20114;&#20316;&#29992;&#26041;&#38754;&#24182;&#19981;&#39640;&#25928;&#12290;&#22240;&#27492;&#65292;&#35768;&#22810;&#20004;&#20010;&#27969;&#20132;&#20114;&#27169;&#22411;&#65288;&#20363;&#22914;&#65292;DeepFM&#21644;DCN&#65289;&#36890;&#36807;&#23558;MLP&#32593;&#32476;&#19982;&#21478;&#19968;&#20010;&#19987;&#29992;&#32593;&#32476;&#38598;&#25104;&#20197;&#22686;&#24378;CTR&#39044;&#27979;&#12290;&#30001;&#20110;MLP&#27969;&#38544;&#24335;&#22320;&#23398;&#20064;&#29305;&#24449;&#20132;&#20114;&#20316;&#29992;&#65292;&#22240;&#27492;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#20110;&#22686;&#24378;&#34917;&#20805;&#27969;&#20013;&#30340;&#26174;&#24335;&#29305;&#24449;&#20132;&#20114;&#20316;&#29992;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#30340;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#19968;&#20010;&#32463;&#36807;&#33391;&#22909;&#35843;&#25972;&#30340;&#21452;&#27969;MLP&#27169;&#22411;&#65292;&#23427;&#21482;&#26159;&#31616;&#21333;&#22320;&#32467;&#21512;&#20102;&#20004;&#20010;MLP&#65292;&#29978;&#33267;&#21487;&#20197;&#23454;&#29616;&#20196;&#20154;&#24778;&#35766;&#30340;&#33391;&#22909;&#24615;&#33021;&#65292;&#36825;&#22312;&#29616;&#26377;&#30340;&#24037;&#20316;&#20013;&#20174;&#26410;&#34987;&#25253;&#36947;&#36807;&#12290;&#22522;&#20110;&#36825;&#20010;&#35266;&#23519;&#32467;&#26524;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#29305;&#24449;&#36873;&#25321;&#21644;&#20132;&#20114;&#32858;&#21512;&#23618;&#12290;
&lt;/p&gt;
&lt;p&gt;
Click-through rate (CTR) prediction is one of the fundamental tasks for online advertising and recommendation. While multi-layer perceptron (MLP) serves as a core component in many deep CTR prediction models, it has been widely recognized that applying a vanilla MLP network alone is inefficient in learning multiplicative feature interactions. As such, many two-stream interaction models (e.g., DeepFM and DCN) have been proposed by integrating an MLP network with another dedicated network for enhanced CTR prediction. As the MLP stream learns feature interactions implicitly, existing research focuses mainly on enhancing explicit feature interactions in the complementary stream. In contrast, our empirical study shows that a well-tuned two-stream MLP model that simply combines two MLPs can even achieve surprisingly good performance, which has never been reported before by existing work. Based on this observation, we further propose feature selection and interaction aggregation layers that c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#21516;&#36807;&#28388;&#30340;&#27169;&#31946;-&#38160;&#21270;&#36807;&#31243;&#27169;&#22411;&#65288;BSPM&#65289;&#65292;&#24182;&#21033;&#29992;&#26399;&#26395;&#26368;&#22823;&#21270;&#31639;&#27861;&#23398;&#20064;&#27169;&#22411;&#21442;&#25968;&#65292;&#22312;&#26174;&#24335;&#21644;&#38544;&#24335;&#21453;&#39304;&#20013;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2211.09324</link><description>&lt;p&gt;
&#21327;&#21516;&#36807;&#28388;&#30340;&#27169;&#31946;-&#38160;&#21270;&#36807;&#31243;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Blurring-Sharpening Process Models for Collaborative Filtering. (arXiv:2211.09324v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.09324
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#21516;&#36807;&#28388;&#30340;&#27169;&#31946;-&#38160;&#21270;&#36807;&#31243;&#27169;&#22411;&#65288;BSPM&#65289;&#65292;&#24182;&#21033;&#29992;&#26399;&#26395;&#26368;&#22823;&#21270;&#31639;&#27861;&#23398;&#20064;&#27169;&#22411;&#21442;&#25968;&#65292;&#22312;&#26174;&#24335;&#21644;&#38544;&#24335;&#21453;&#39304;&#20013;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21327;&#21516;&#36807;&#28388;&#26159;&#25512;&#33616;&#31995;&#32479;&#20013;&#26368;&#22522;&#26412;&#30340;&#20027;&#39064;&#20043;&#19968;&#12290;&#20174;&#30697;&#38453;&#20998;&#35299;&#21040;&#22270;&#21367;&#31215;&#26041;&#27861;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#21508;&#26679;&#30340;&#21327;&#21516;&#36807;&#28388;&#26041;&#27861;&#12290;&#22312;&#22270;&#36807;&#28388;&#26041;&#27861;&#21644;&#22522;&#20110;&#20998;&#25968;&#30340;&#29983;&#25104;&#27169;&#22411;&#65288;SGM&#65289;&#30340;&#26368;&#36817;&#25104;&#21151;&#21551;&#21457;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#31946;-&#38160;&#21270;&#36807;&#31243;&#27169;&#22411;&#65288;BSPM&#65289;&#30340;&#27010;&#24565;&#12290;SGM&#21644;BSPM&#20849;&#20139;&#30456;&#21516;&#30340;&#22788;&#29702;&#21746;&#23398;&#65292;&#21363;&#22312;&#23558;&#21407;&#22987;&#20449;&#24687;&#39318;&#20808;&#25200;&#20081;&#28982;&#21518;&#24674;&#22797;&#21040;&#21407;&#22987;&#24418;&#24335;&#30340;&#36807;&#31243;&#20013;&#21487;&#20197;&#21457;&#29616;&#26032;&#20449;&#24687;&#65288;&#20363;&#22914;&#65292;&#22312;SGM&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#26032;&#22270;&#20687;&#65289;&#12290;&#28982;&#32780;&#65292;SGM&#21644;&#25105;&#20204;&#30340;BSPM&#22788;&#29702;&#19981;&#21516;&#31867;&#22411;&#30340;&#20449;&#24687;&#65292;&#24182;&#19988;&#23427;&#20204;&#30340;&#26368;&#20248;&#25200;&#21160;&#21644;&#24674;&#22797;&#36807;&#31243;&#23384;&#22312;&#26681;&#26412;&#19978;&#30340;&#24046;&#24322;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;BSPM&#19982;SGM&#20855;&#26377;&#19981;&#21516;&#30340;&#24418;&#24335;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#27010;&#24565;&#19981;&#20165;&#29702;&#35770;&#19978;&#21253;&#25324;&#20102;&#35768;&#22810;&#29616;&#26377;&#30340;&#21327;&#21516;&#36807;&#28388;&#27169;&#22411;&#65292;&#32780;&#19988;&#22312;&#26174;&#24335;&#21644;&#38544;&#24335;&#21453;&#39304;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#21484;&#22238;&#29575;&#21644;NDCG&#26041;&#38754;&#20063;&#20248;&#20110;&#23427;&#20204;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#32452;&#20855;&#26377;&#19981;&#21516;&#27169;&#31946;&#21644;&#38160;&#21270;&#28388;&#27874;&#22120;&#35774;&#32622;&#30340;BSPM&#65292;&#24182;&#25512;&#23548;&#20102;&#26399;&#26395;&#26368;&#22823;&#21270;&#65288;EM&#65289;&#31639;&#27861;&#20197;&#23398;&#20064;&#27169;&#22411;&#21442;&#25968;&#12290;&#22235;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#30456;&#23545;&#20110;&#29616;&#26377;&#25216;&#26415;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Collaborative filtering is one of the most fundamental topics for recommender systems. Various methods have been proposed for collaborative filtering, ranging from matrix factorization to graph convolutional methods. Being inspired by recent successes of graph filtering-based methods and score-based generative models (SGMs), we present a novel concept of blurring-sharpening process model (BSPM). SGMs and BSPMs share the same processing philosophy that new information can be discovered (e.g., new images are generated in the case of SGMs) while original information is first perturbed and then recovered to its original form. However, SGMs and our BSPMs deal with different types of information, and their optimal perturbation and recovery processes have fundamental discrepancies. Therefore, our BSPMs have different forms from SGMs. In addition, our concept not only theoretically subsumes many existing collaborative filtering models but also outperforms them in terms of Recall and NDCG in th
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#8212;&#8212;&#20010;&#24615;&#21270;&#23637;&#31034;&#65292;&#36890;&#36807;&#25552;&#20379;&#25991;&#26412;&#21644;&#35270;&#35273;&#20449;&#24687;&#36827;&#19968;&#27493;&#20016;&#23500;&#25512;&#33616;&#30340;&#35299;&#37322;&#12290;&#20316;&#32773;&#20174; Google Local&#65288;&#21363;&#22320;&#22270;&#65289;&#25910;&#38598;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20010;&#24615;&#21270;&#22810;&#27169;&#24577;&#26694;&#26550;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#20135;&#29983;&#27604;&#20808;&#21069;&#26041;&#27861;&#26356;&#22810;&#26679;&#21270;&#21644;&#26356;&#20855;&#34920;&#29616;&#21147;&#30340;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2207.00422</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#23637;&#31034;&#65306;&#29983;&#25104;&#38754;&#21521;&#25512;&#33616;&#30340;&#22810;&#27169;&#24577;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Personalized Showcases: Generating Multi-Modal Explanations for Recommendations. (arXiv:2207.00422v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.00422
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#8212;&#8212;&#20010;&#24615;&#21270;&#23637;&#31034;&#65292;&#36890;&#36807;&#25552;&#20379;&#25991;&#26412;&#21644;&#35270;&#35273;&#20449;&#24687;&#36827;&#19968;&#27493;&#20016;&#23500;&#25512;&#33616;&#30340;&#35299;&#37322;&#12290;&#20316;&#32773;&#20174; Google Local&#65288;&#21363;&#22320;&#22270;&#65289;&#25910;&#38598;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20010;&#24615;&#21270;&#22810;&#27169;&#24577;&#26694;&#26550;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#20135;&#29983;&#27604;&#20808;&#21069;&#26041;&#27861;&#26356;&#22810;&#26679;&#21270;&#21644;&#26356;&#20855;&#34920;&#29616;&#21147;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#35299;&#37322;&#27169;&#22411;&#21482;&#20026;&#25512;&#33616;&#29983;&#25104;&#25991;&#26412;&#65292;&#20294;&#20173;&#28982;&#38590;&#20197;&#20135;&#29983;&#22810;&#26679;&#21270;&#30340;&#20869;&#23481;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;&#20010;&#24615;&#21270;&#23637;&#31034;&#8221;&#30340;&#26032;&#20219;&#21153;&#65292;&#36890;&#36807;&#22312;&#35299;&#37322;&#20013;&#25552;&#20379;&#25991;&#26412;&#21644;&#35270;&#35273;&#20449;&#24687;&#26469;&#36827;&#19968;&#27493;&#20016;&#23500;&#35299;&#37322;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#36873;&#25321;&#19968;&#20010;&#23450;&#21046;&#30340;&#22270;&#20687;&#38598;&#65292;&#35813;&#38598;&#21512;&#19982;&#29992;&#25143;&#23545;&#25512;&#33616;&#29289;&#21697;&#30340;&#20852;&#36259;&#26368;&#30456;&#20851;&#12290;&#28982;&#21518;&#65292;&#26681;&#25454;&#25105;&#20204;&#25152;&#36873;&#30340;&#22270;&#20687;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#12290; &#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#26032;&#20219;&#21153;&#65292;&#25105;&#20204;&#20174; Google Local&#65288;&#21363;&#22320;&#22270;&#65289;&#25910;&#38598;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#23376;&#38598;&#20197;&#29983;&#25104;&#22810;&#27169;&#24577;&#35299;&#37322;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20010;&#24615;&#21270;&#22810;&#27169;&#24577;&#26694;&#26550;&#65292;&#21487;&#20197;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#29983;&#25104;&#22810;&#26679;&#21270;&#21644;&#35270;&#35273;&#19968;&#33268;&#30340;&#35299;&#37322;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21463;&#30410;&#20110;&#19981;&#21516;&#30340;&#36755;&#20837;&#27169;&#24577;&#65292;&#24182;&#19988;&#33021;&#22815;&#20135;&#29983;&#27604;&#20808;&#21069;&#26041;&#27861;&#26356;&#22810;&#26679;&#21270;&#21644;&#26356;&#20855;&#34920;&#29616;&#21147;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing explanation models generate only text for recommendations but still struggle to produce diverse contents. In this paper, to further enrich explanations, we propose a new task named personalized showcases, in which we provide both textual and visual information to explain our recommendations. Specifically, we first select a personalized image set that is the most relevant to a user's interest toward a recommended item. Then, natural language explanations are generated accordingly given our selected images. For this new task, we collect a large-scale dataset from Google Local (i.e.,~maps) and construct a high-quality subset for generating multi-modal explanations. We propose a personalized multi-modal framework which can generate diverse and visually-aligned explanations via contrastive learning. Experiments show that our framework benefits from different modalities as inputs, and is able to produce more diverse and expressive explanations compared to previous methods on a varie
&lt;/p&gt;</description></item></channel></rss>