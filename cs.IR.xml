<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;DP-Fair&#65292;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#21327;&#21516;&#36807;&#28388;&#31639;&#27861;&#26694;&#26550;&#65292;&#23427;&#32467;&#21512;&#20102;&#24046;&#20998;&#38544;&#31169;&#26426;&#21046;&#21644;&#20844;&#24179;&#32422;&#26463;&#65292;&#26088;&#22312;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#12289;&#30830;&#20445;&#20844;&#24179;&#25512;&#33616;&#12290;</title><link>http://arxiv.org/abs/2303.09527</link><description>&lt;p&gt;
&#20844;&#24179;&#24863;&#30693;&#30340;&#24046;&#20998;&#38544;&#31169;&#21327;&#21516;&#36807;&#28388;
&lt;/p&gt;
&lt;p&gt;
Fairness-aware Differentially Private Collaborative Filtering. (arXiv:2303.09527v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09527
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;DP-Fair&#65292;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#21327;&#21516;&#36807;&#28388;&#31639;&#27861;&#26694;&#26550;&#65292;&#23427;&#32467;&#21512;&#20102;&#24046;&#20998;&#38544;&#31169;&#26426;&#21046;&#21644;&#20844;&#24179;&#32422;&#26463;&#65292;&#26088;&#22312;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#12289;&#30830;&#20445;&#20844;&#24179;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#38544;&#31169;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#37319;&#29992;&#24046;&#20998;&#38544;&#31169;&#24341;&#23548;&#31639;&#27861;&#65292;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#31639;&#27861;&#20351;&#29992;&#22312;&#31639;&#27861;&#20844;&#24179;&#24615;&#26041;&#38754;&#26377;&#25240;&#34935;&#65292;&#36825;&#19968;&#28857;&#34987;&#24191;&#27867;&#35748;&#21487;&#12290;&#26412;&#25991;&#38024;&#23545;&#24046;&#20998;&#38544;&#31169;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;DP-SGD&#65289;&#35757;&#32451;&#30340;&#32463;&#20856;&#21327;&#21516;&#36807;&#28388;&#26041;&#27861;&#23548;&#33268;&#29992;&#25143;&#32676;&#20307;&#19982;&#19981;&#21516;&#29992;&#25143;&#21442;&#19982;&#27700;&#24179;&#20043;&#38388;&#23384;&#22312;&#19981;&#20844;&#24179;&#24433;&#21709;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#26694;&#26550;DP-Fair&#65292;&#23427;&#23558;&#24046;&#20998;&#38544;&#31169;&#26426;&#21046;&#19982;&#20844;&#24179;&#38480;&#21046;&#30456;&#32467;&#21512;&#65292;&#20174;&#32780;&#22312;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#30340;&#21516;&#26102;&#30830;&#20445;&#20844;&#24179;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, there has been an increasing adoption of differential privacy guided algorithms for privacy-preserving machine learning tasks. However, the use of such algorithms comes with trade-offs in terms of algorithmic fairness, which has been widely acknowledged. Specifically, we have empirically observed that the classical collaborative filtering method, trained by differentially private stochastic gradient descent (DP-SGD), results in a disparate impact on user groups with respect to different user engagement levels. This, in turn, causes the original unfair model to become even more biased against inactive users. To address the above issues, we propose \textbf{DP-Fair}, a two-stage framework for collaborative filtering based algorithms. Specifically, it combines differential privacy mechanisms with fairness constraints to protect user privacy while ensuring fair recommendations. The experimental results, based on Amazon datasets, and user history logs collected from Etsy, one of th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#39564;&#26041;&#26696;&#65292;&#36890;&#36807;&#31215;&#26497;&#25110;&#28040;&#26497;&#20559;&#35265;&#30340;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#26469;&#27979;&#37327;&#35299;&#37322;&#23545;&#29992;&#25143;&#36873;&#25321;&#24314;&#35758;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#35299;&#37322;&#21487;&#20197;&#26174;&#33879;&#24433;&#21709;&#29992;&#25143;&#36873;&#25321;&#12290;</title><link>http://arxiv.org/abs/2303.09498</link><description>&lt;p&gt;
&#37327;&#21270;&#35299;&#37322;&#20559;&#24046;&#30340;&#24433;&#21709;&#65306;&#20851;&#20110;&#25512;&#33616;&#31995;&#32479;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Measuring the Impact of Explanation Bias: A Study of Natural Language Justifications for Recommender Systems. (arXiv:2303.09498v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09498
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#39564;&#26041;&#26696;&#65292;&#36890;&#36807;&#31215;&#26497;&#25110;&#28040;&#26497;&#20559;&#35265;&#30340;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#26469;&#27979;&#37327;&#35299;&#37322;&#23545;&#29992;&#25143;&#36873;&#25321;&#24314;&#35758;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#35299;&#37322;&#21487;&#20197;&#26174;&#33879;&#24433;&#21709;&#29992;&#25143;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#35299;&#37322;&#21487;&#33021;&#23545;&#20915;&#31574;&#20135;&#29983;&#24433;&#21709;&#65292;&#20294;&#32570;&#20047;&#30740;&#31350;&#26469;&#37327;&#21270;&#20854;&#23545;&#29992;&#25143;&#36873;&#25321;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#39564;&#26041;&#26696;&#65292;&#29992;&#20110;&#27979;&#37327;&#31215;&#26497;&#25110;&#28040;&#26497;&#20559;&#35265;&#35299;&#37322;&#21487;&#33021;&#23548;&#33268;&#29992;&#25143;&#36873;&#25321;&#27425;&#20248;&#24314;&#35758;&#30340;&#31243;&#24230;&#12290;&#35813;&#26041;&#26696;&#30340;&#20851;&#38190;&#35201;&#32032;&#21253;&#25324;&#20559;&#22909;&#24341;&#23548;&#38454;&#27573;&#20197;&#20801;&#35768;&#20010;&#24615;&#21270;&#24314;&#35758;&#12289;&#25163;&#21160;&#35782;&#21035;&#21644;&#25552;&#21462;&#35780;&#35770;&#20013;&#30340;&#39033;&#30446;&#35201;&#32032;&#20197;&#21450;&#36890;&#36807;&#23558;&#31215;&#26497;&#21644;&#28040;&#26497;&#35201;&#32032;&#32467;&#21512;&#32780;&#24341;&#20837;&#20559;&#35265;&#30340;&#25511;&#21046;&#26041;&#27861;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#25991;&#26412;&#26684;&#24335;&#30340;&#35299;&#37322;&#65306;&#20316;&#20026;&#39033;&#30446;&#35201;&#32032;&#21015;&#34920;&#30340;&#24418;&#24335;&#21644;&#20316;&#20026;&#27969;&#30021;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#30340;&#24418;&#24335;&#12290;&#36890;&#36807;&#23545;129&#21517;&#21442;&#19982;&#32773;&#36827;&#34892;&#29992;&#25143;&#30740;&#31350;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35299;&#37322;&#21487;&#20197;&#26174;&#33879;&#24433;&#21709;&#29992;&#25143;&#30340;&#36873;&#25321;&#65292;&#24182;&#19988;&#36825;&#20123;&#21457;&#29616;&#21487;&#20197;&#25512;&#24191;&#21040;&#35299;&#37322;&#26684;&#24335;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the potential impact of explanations on decision making, there is a lack of research on quantifying their effect on users' choices. This paper presents an experimental protocol for measuring the degree to which positively or negatively biased explanations can lead to users choosing suboptimal recommendations. Key elements of this protocol include a preference elicitation stage to allow for personalizing recommendations, manual identification and extraction of item aspects from reviews, and a controlled method for introducing bias through the combination of both positive and negative aspects. We study explanations in two different textual formats: as a list of item aspects and as fluent natural language text. Through a user study with 129 participants, we demonstrate that explanations can significantly affect users' selections and that these findings generalize across explanation formats.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010; LLMSecEval &#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547; 150 &#20010;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#65292;&#21487;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#23481;&#26131;&#20986;&#29616;&#23433;&#20840;&#28431;&#27934;&#30340;&#20195;&#30721;&#26102;&#30340;&#23433;&#20840;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.09384</link><description>&lt;p&gt;
LLMSecEval: &#19968;&#20010;&#29992;&#20110;&#23433;&#20840;&#35780;&#20272;&#30340;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
LLMSecEval: A Dataset of Natural Language Prompts for Security Evaluations. (arXiv:2303.09384v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09384
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010; LLMSecEval &#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547; 150 &#20010;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#65292;&#21487;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#23481;&#26131;&#20986;&#29616;&#23433;&#20840;&#28431;&#27934;&#30340;&#20195;&#30721;&#26102;&#30340;&#23433;&#20840;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914; Codex &#22312;&#20195;&#30721;&#33258;&#21160;&#34917;&#20840;&#21644;&#29983;&#25104;&#20219;&#21153;&#26041;&#38754;&#20855;&#26377;&#24378;&#22823;&#30340;&#33021;&#21147;&#65292;&#22240;&#20026;&#23427;&#20204;&#36890;&#36807;&#20844;&#24320;&#21487;&#29992;&#30340;&#20195;&#30721;&#20174;&#25968;&#21313;&#20159;&#34892;&#20195;&#30721;&#20013;&#36827;&#34892;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#20174;&#20844;&#20849; GitHub &#20179;&#24211;&#23398;&#20064;&#35821;&#35328;&#21644;&#32534;&#31243;&#23454;&#36341;&#26469;&#29983;&#25104;&#26469;&#33258;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#20195;&#30721;&#29255;&#27573;&#12290;&#23613;&#31649; LLM &#25215;&#35834;&#23454;&#29616;&#36719;&#20214;&#24212;&#29992;&#30340; NL &#39537;&#21160;&#37096;&#32626;&#65292;&#20294;&#26159;&#23427;&#20204;&#29983;&#25104;&#30340;&#20195;&#30721;&#30340;&#23433;&#20840;&#24615;&#23578;&#26410;&#24471;&#21040;&#24191;&#27867;&#35843;&#26597;&#21644;&#35760;&#24405;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; LLMSecEval&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547; 150 &#20010; NL &#25552;&#31034;&#30340;&#25968;&#25454;&#38598;&#65292;&#21487;&#29992;&#20110;&#35780;&#20272;&#27492;&#31867;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#33021;&#12290;&#36825;&#20123;&#25552;&#31034;&#26159;&#22522;&#20110;MITRE&#30340;&#21069;25&#20010;&#24120;&#35265;&#24369;&#28857;&#21015;&#34920;&#20013;&#23481;&#26131;&#20986;&#29616;&#21508;&#31181;&#23433;&#20840;&#28431;&#27934;&#30340;&#20195;&#30721;&#29255;&#27573;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#12290;&#25105;&#20204;&#25968;&#25454;&#38598;&#20013;&#30340;&#27599;&#20010;&#25552;&#31034;&#37117;&#37197;&#26377;&#19968;&#20010;&#23433;&#20840;&#23454;&#29616;&#31034;&#20363;&#65292;&#20197;&#20415;&#19982;&#30001;LLM&#29983;&#25104;&#30340;&#20195;&#30721;&#36827;&#34892;&#27604;&#36739;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) like Codex are powerful tools for performing code completion and code generation tasks as they are trained on billions of lines of code from publicly available sources. Moreover, these models are capable of generating code snippets from Natural Language (NL) descriptions by learning languages and programming practices from public GitHub repositories. Although LLMs promise an effortless NL-driven deployment of software applications, the security of the code they generate has not been extensively investigated nor documented. In this work, we present LLMSecEval, a dataset containing 150 NL prompts that can be leveraged for assessing the security performance of such models. Such prompts are NL descriptions of code snippets prone to various security vulnerabilities listed in MITRE's Top 25 Common Weakness Enumeration (CWE) ranking. Each prompt in our dataset comes with a secure implementation example to facilitate comparative evaluations against code produced by
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#36807;&#28388;&#22120;&#24863;&#30693;&#30340;&#36890;&#29992;&#36817;&#20284;&#26694;&#26550;&#65292;&#35813;&#26041;&#27861;&#23450;&#20041;&#20102;&#21512;&#36866;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#36816;&#34892;&#26102;&#35757;&#32451;&#20197;&#28385;&#36275;&#32479;&#35745;&#24179;&#31561;&#32422;&#26463;&#65292;&#21516;&#26102;&#26368;&#23567;&#31243;&#24230;&#25200;&#21160;&#21407;&#22987;&#21518;&#39564;&#24773;&#20917;&#19979;&#23454;&#29616;&#27492;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2303.08157</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20844;&#24179;&#22270;&#36807;&#28388;&#26367;&#20195;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Network Surrogates of Fair Graph Filtering. (arXiv:2303.08157v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08157
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#36807;&#28388;&#22120;&#24863;&#30693;&#30340;&#36890;&#29992;&#36817;&#20284;&#26694;&#26550;&#65292;&#35813;&#26041;&#27861;&#23450;&#20041;&#20102;&#21512;&#36866;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#36816;&#34892;&#26102;&#35757;&#32451;&#20197;&#28385;&#36275;&#32479;&#35745;&#24179;&#31561;&#32422;&#26463;&#65292;&#21516;&#26102;&#26368;&#23567;&#31243;&#24230;&#25200;&#21160;&#21407;&#22987;&#21518;&#39564;&#24773;&#20917;&#19979;&#23454;&#29616;&#27492;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36793;&#20256;&#25773;&#23558;&#20808;&#21069;&#30340;&#33410;&#28857;&#20540;&#36716;&#25442;&#20026;&#21518;&#26469;&#30340;&#20998;&#25968;&#30340;&#22270;&#28388;&#27874;&#22120;&#36890;&#24120;&#25903;&#25345;&#24433;&#21709;&#20154;&#31867;&#30340;&#22270;&#25366;&#25496;&#20219;&#21153;&#65292;&#20363;&#22914;&#25512;&#33616;&#21644;&#25490;&#21517;&#12290;&#22240;&#27492;&#65292;&#37325;&#35201;&#30340;&#26159;&#22312;&#28385;&#36275;&#33410;&#28857;&#32452;&#20043;&#38388;&#30340;&#32479;&#35745;&#24179;&#31561;&#32422;&#26463;&#26041;&#38754;&#20351;&#23427;&#20204;&#20844;&#24179;&#65288;&#20363;&#22914;&#65292;&#25353;&#20854;&#20195;&#34920;&#24615;&#23558;&#20998;&#25968;&#36136;&#37327;&#22312;&#24615;&#21035;&#20043;&#38388;&#22343;&#34913;&#20998;&#37197;&#65289;&#12290;&#20026;&#20102;&#22312;&#26368;&#23567;&#31243;&#24230;&#22320;&#25200;&#21160;&#21407;&#22987;&#21518;&#39564;&#24773;&#20917;&#19979;&#23454;&#29616;&#27492;&#30446;&#26631;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#36807;&#28388;&#22120;&#24863;&#30693;&#30340;&#36890;&#29992;&#36817;&#20284;&#26694;&#26550;&#65292;&#29992;&#20110;&#21518;&#39564;&#30446;&#26631;&#12290;&#36825;&#23450;&#20041;&#20102;&#36866;&#24403;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#20854;&#22312;&#36816;&#34892;&#26102;&#35757;&#32451;&#65292;&#31867;&#20284;&#20110;&#36807;&#28388;&#22120;&#65292;&#20294;&#20063;&#22312;&#26412;&#22320;&#20248;&#21270;&#21253;&#25324;&#20844;&#24179;&#24863;&#30693;&#22312;&#20869;&#30340;&#22823;&#31867;&#30446;&#26631;&#12290;&#22312;&#19968;&#32452;8&#20010;&#36807;&#28388;&#22120;&#21644;5&#20010;&#22270;&#24418;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#28385;&#36275;&#32479;&#35745;&#24179;&#31561;&#32422;&#26463;&#26041;&#38754;&#34920;&#29616;&#24471;&#19981;&#20122;&#20110;&#26367;&#20195;&#21697;&#65292;&#21516;&#26102;&#20445;&#30041;&#22522;&#20110;&#20998;&#25968;&#30340;&#31038;&#21306;&#25104;&#21592;&#25512;&#33616;&#30340;AUC&#24182;&#22312;&#20256;&#25773;&#20808;&#21069;&#33410;&#25293;&#26102;&#21019;&#24314;&#26368;&#23567;&#23454;&#29992;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph filters that transform prior node values to posterior scores via edge propagation often support graph mining tasks affecting humans, such as recommendation and ranking. Thus, it is important to make them fair in terms of satisfying statistical parity constraints between groups of nodes (e.g., distribute score mass between genders proportionally to their representation). To achieve this while minimally perturbing the original posteriors, we introduce a filter-aware universal approximation framework for posterior objectives. This defines appropriate graph neural networks trained at runtime to be similar to filters but also locally optimize a large class of objectives, including fairness-aware ones. Experiments on a collection of 8 filters and 5 graphs show that our approach performs equally well or better than alternatives in meeting parity constraints while preserving the AUC of score-based community member recommendation and creating minimal utility loss in prior diffusion.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#22914;&#20309;&#20174;&#19981;&#21516;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#21644;&#36716;&#31227;&#30693;&#35782;&#65292;&#25552;&#39640;&#25512;&#33616;&#31995;&#32479;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#31867;&#27861;&#65292;&#20998;&#26512;&#21644;&#24635;&#32467;&#20102;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#33616;&#31995;&#32479;&#30340;&#22521;&#35757;&#31574;&#30053;&#21644;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2302.03735</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;&#12289;&#25552;&#31034;&#21644;&#25512;&#33616;&#65306;&#35821;&#35328;&#27169;&#22411;&#33539;&#24335;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#32508;&#21512;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Pre-train, Prompt and Recommendation: A Comprehensive Survey of Language Modelling Paradigm Adaptations in Recommender Systems. (arXiv:2302.03735v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03735
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#22914;&#20309;&#20174;&#19981;&#21516;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#21644;&#36716;&#31227;&#30693;&#35782;&#65292;&#25552;&#39640;&#25512;&#33616;&#31995;&#32479;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#31867;&#27861;&#65292;&#20998;&#26512;&#21644;&#24635;&#32467;&#20102;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#33616;&#31995;&#32479;&#30340;&#22521;&#35757;&#31574;&#30053;&#21644;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#30340;&#20986;&#29616;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#26041;&#24335;&#22312;&#22823;&#22411;&#35821;&#26009;&#24211;&#19978;&#23398;&#20064;&#36890;&#29992;&#34920;&#31034;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#23398;&#21040;&#30340;&#34920;&#31034;&#21487;&#21463;&#30410;&#20110;&#19968;&#31995;&#21015;&#19979;&#28216;NLP&#20219;&#21153;&#12290;&#36825;&#31181;&#22521;&#35757;&#33539;&#24335;&#26368;&#36817;&#34987;&#36866;&#29992;&#20110;&#25512;&#33616;&#39046;&#22495;&#65292;&#24182;&#34987;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#35748;&#20026;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#22914;&#20309;&#20174;&#19981;&#21516;PLM&#30456;&#20851;&#35757;&#32451;&#33539;&#24335;&#23398;&#20064;&#21040;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#25552;&#21462;&#21644;&#36716;&#31227;&#30693;&#35782;&#65292;&#20174;&#22810;&#20010;&#35282;&#24230;&#65288;&#22914;&#36890;&#29992;&#24615;&#12289;&#31232;&#30095;&#24615;&#12289;&#25928;&#29575;&#21644;&#25928;&#26524;&#65289;&#25552;&#39640;&#25512;&#33616;&#24615;&#33021;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27491;&#20132;&#20998;&#31867;&#27861;&#26469;&#21010;&#20998;&#29616;&#26377;&#30340;&#22522;&#20110;PLM&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#38024;&#23545;&#20854;&#22521;&#35757;&#31574;&#30053;&#21644;&#30446;&#26631;&#36827;&#34892;&#20998;&#26512;&#21644;&#24635;&#32467;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergency of Pre-trained Language Models (PLMs) has achieved tremendous success in the field of Natural Language Processing (NLP) by learning universal representations on large corpora in a self-supervised manner. The pre-trained models and the learned representations can be beneficial to a series of downstream NLP tasks. This training paradigm has recently been adapted to the recommendation domain and is considered a promising approach by both academia and industry. In this paper, we systematically investigate how to extract and transfer knowledge from pre-trained models learned by different PLM-related training paradigms to improve recommendation performance from various perspectives, such as generality, sparsity, efficiency and effectiveness. Specifically, we propose an orthogonal taxonomy to divide existing PLM-based recommender systems w.r.t. their training strategies and objectives. Then, we analyze and summarize the connection between PLM-based training paradigms and differe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;UNIQORN&#30340;&#38382;&#31572;&#31995;&#32479;&#65292;&#23427;&#33021;&#22815;&#26080;&#32541;&#22320;&#22788;&#29702;RDF&#25968;&#25454;&#21644;&#25991;&#26412;&#65292;&#20351;&#29992;fine-tuned BERT&#27169;&#22411;&#20026;&#38382;&#39064;&#26500;&#24314;&#19978;&#19979;&#25991;&#22270;&#65292;&#24182;&#20351;&#29992;&#22270;&#31639;&#27861;&#30830;&#23450;&#19982;&#38382;&#39064;&#30456;&#20851;&#30340;&#23376;&#22270;&#26469;&#22238;&#31572;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2108.08614</link><description>&lt;p&gt;
UNIQORN&#65306;&#32479;&#19968;&#30340;RDF&#30693;&#35782;&#22270;&#35889;&#19982;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#38382;&#31572;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
UNIQORN: Unified Question Answering over RDF Knowledge Graphs and Natural Language Text. (arXiv:2108.08614v5 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2108.08614
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;UNIQORN&#30340;&#38382;&#31572;&#31995;&#32479;&#65292;&#23427;&#33021;&#22815;&#26080;&#32541;&#22320;&#22788;&#29702;RDF&#25968;&#25454;&#21644;&#25991;&#26412;&#65292;&#20351;&#29992;fine-tuned BERT&#27169;&#22411;&#20026;&#38382;&#39064;&#26500;&#24314;&#19978;&#19979;&#25991;&#22270;&#65292;&#24182;&#20351;&#29992;&#22270;&#31639;&#27861;&#30830;&#23450;&#19982;&#38382;&#39064;&#30456;&#20851;&#30340;&#23376;&#22270;&#26469;&#22238;&#31572;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38382;&#39064;&#22238;&#31572;&#22312;&#30693;&#35782;&#22270;&#35889;&#21644;&#20854;&#20182;RDF&#25968;&#25454;&#19978;&#24050;&#32463;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#36827;&#23637;&#65292;&#35768;&#22810;&#20248;&#31168;&#30340;&#31995;&#32479;&#21487;&#20197;&#20026;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#25110;&#30005;&#25253;&#26597;&#35810;&#25552;&#20379;&#28165;&#26224;&#30340;&#31572;&#26696;&#12290;&#20854;&#20013;&#19968;&#20123;&#31995;&#32479;&#23558;&#25991;&#26412;&#28304;&#20316;&#20026;&#38468;&#21152;&#35777;&#25454;&#32435;&#20837;&#22238;&#31572;&#36807;&#31243;&#65292;&#20294;&#19981;&#33021;&#35745;&#31639;&#20165;&#23384;&#22312;&#20110;&#25991;&#26412;&#20013;&#30340;&#31572;&#26696;&#12290;&#30456;&#21453;&#65292;IR&#21644;NLP&#31038;&#21306;&#30340;&#31995;&#32479;&#24050;&#32463;&#35299;&#20915;&#20102;&#26377;&#20851;&#25991;&#26412;&#30340;QA&#38382;&#39064;&#65292;&#20294;&#26159;&#36825;&#20123;&#31995;&#32479;&#20960;&#20046;&#19981;&#21033;&#29992;&#35821;&#20041;&#25968;&#25454;&#21644;&#30693;&#35782;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#21487;&#20197;&#26080;&#32541;&#25805;&#20316;&#28151;&#21512;RDF&#25968;&#25454;&#38598;&#21644;&#25991;&#26412;&#35821;&#26009;&#24211;&#25110;&#21333;&#20010;&#26469;&#28304;&#30340;&#22797;&#26434;&#38382;&#39064;&#30340;&#31995;&#32479;&#65292;&#22312;&#32479;&#19968;&#26694;&#26550;&#20013;&#36827;&#34892;&#25805;&#20316;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#31216;&#20026;UNIQORN&#65292;&#36890;&#36807;&#20351;&#29992;&#32463;&#36807;&#31934;&#32454;&#35843;&#25972;&#30340;BERT&#27169;&#22411;&#20174;RDF&#25968;&#25454;&#21644;/&#25110;&#25991;&#26412;&#35821;&#26009;&#24211;&#20013;&#26816;&#32034;&#19982;&#38382;&#39064;&#30456;&#20851;&#30340;&#35777;&#25454;&#26469;&#21160;&#24577;&#26500;&#24314;&#19978;&#19979;&#25991;&#22270;&#12290;&#32467;&#26524;&#22270;&#36890;&#24120;&#38750;&#24120;&#20016;&#23500;&#20294;&#39640;&#24230;&#22024;&#26434;&#12290;UNIQORN&#36890;&#36807;&#29992;&#20110;&#32452;Steiner&#26641;&#30340;&#22270;&#31639;&#27861;&#26469;&#22788;&#29702;&#36825;&#20010;&#36755;&#20837;&#65292;&#20174;&#32780;&#30830;&#23450;&#19982;&#38382;&#39064;&#30456;&#20851;&#30340;&#23376;&#22270;&#65292;&#36827;&#32780;&#22238;&#31572;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Question answering over knowledge graphs and other RDF data has been greatly advanced, with a number of good systems providing crisp answers for natural language questions or telegraphic queries. Some of these systems incorporate textual sources as additional evidence for the answering process, but cannot compute answers that are present in text alone. Conversely, systems from the IR and NLP communities have addressed QA over text, but such systems barely utilize semantic data and knowledge. This paper presents the first system for complex questions that can seamlessly operate over a mixture of RDF datasets and text corpora, or individual sources, in a unified framework. Our method, called UNIQORN, builds a context graph on-the-fly, by retrieving question-relevant evidences from the RDF data and/or a text corpus, using fine-tuned BERT models. The resulting graph is typically rich but highly noisy. UNIQORN copes with this input by a graph algorithm for Group Steiner Trees, that identifi
&lt;/p&gt;</description></item></channel></rss>