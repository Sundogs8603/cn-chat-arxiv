<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#20171;&#32461;&#20102;&#19968;&#20010;&#29420;&#29305;&#30340;&#25968;&#25454;&#38598;CODE-ACCORD&#65292;&#26088;&#22312;&#35299;&#20915;&#33258;&#21160;&#21512;&#35268;&#26816;&#26597;&#20013;&#35299;&#37322;&#24314;&#31569;&#27861;&#35268;&#30340;&#25361;&#25112;&#65292;&#25104;&#20026;&#26426;&#22120;&#21487;&#35835;&#35268;&#21017;&#29983;&#25104;&#30340;&#22522;&#30784;&#12290;</title><link>https://arxiv.org/abs/2403.02231</link><description>&lt;p&gt;
CODE-ACCORD&#65306;&#29992;&#20110;&#35268;&#21017;&#29983;&#25104;&#30340;&#24314;&#31569;&#27861;&#35268;&#25968;&#25454;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
CODE-ACCORD: A Corpus of Building Regulatory Data for Rule Generation towards Automatic Compliance Checking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02231
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#20010;&#29420;&#29305;&#30340;&#25968;&#25454;&#38598;CODE-ACCORD&#65292;&#26088;&#22312;&#35299;&#20915;&#33258;&#21160;&#21512;&#35268;&#26816;&#26597;&#20013;&#35299;&#37322;&#24314;&#31569;&#27861;&#35268;&#30340;&#25361;&#25112;&#65292;&#25104;&#20026;&#26426;&#22120;&#21487;&#35835;&#35268;&#21017;&#29983;&#25104;&#30340;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21512;&#35268;&#26816;&#26597;&#65288;ACC&#65289;&#22312;&#24314;&#31569;&#12289;&#24037;&#31243;&#21644;&#26045;&#24037;&#65288;AEC&#65289;&#39046;&#22495;&#20869;&#30340;&#33258;&#21160;&#21512;&#35268;&#26816;&#26597;&#38656;&#35201;&#33258;&#21160;&#35299;&#37322;&#24314;&#31569;&#27861;&#35268;&#65292;&#20197;&#21457;&#25381;&#20854;&#20840;&#37096;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#20174;&#25991;&#26412;&#35268;&#21017;&#20013;&#25552;&#21462;&#20449;&#24687;&#20197;&#23558;&#20854;&#36716;&#25442;&#20026;&#26426;&#22120;&#21487;&#35835;&#26684;&#24335;&#30001;&#20110;&#33258;&#28982;&#35821;&#35328;&#30340;&#22797;&#26434;&#24615;&#20197;&#21450;&#20165;&#33021;&#25903;&#25345;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#26377;&#38480;&#36164;&#28304;&#32780;&#25104;&#20026;&#19968;&#20010;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#29420;&#29305;&#30340;&#25968;&#25454;&#38598;CODE-ACCORD&#65292;&#36825;&#26159;&#22312;&#27431;&#30431;Horizon ACCORD&#39033;&#30446;&#19979;&#32534;&#21046;&#30340;&#12290;CODE-ACCORD&#21253;&#21547;862&#20010;&#26469;&#33258;&#33521;&#26684;&#20848;&#21644;&#33452;&#20848;&#24314;&#31569;&#27861;&#35268;&#30340;&#33258;&#21253;&#21547;&#21477;&#23376;&#12290;&#19982;&#25105;&#20204;&#30340;&#26680;&#24515;&#30446;&#26631;&#19968;&#33268;&#65292;&#21363;&#20419;&#36827;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#20449;&#24687;&#20197;&#29983;&#25104;&#26426;&#22120;&#21487;&#35835;&#35268;&#21017;&#65292;&#27599;&#20010;&#21477;&#23376;&#37117;&#27880;&#37322;&#20102;&#23454;&#20307;&#21644;&#20851;&#31995;&#12290;&#23454;&#20307;&#20195;&#34920;&#29305;&#23450;&#32452;&#20214;&#65292;&#22914;&#8220;&#31383;&#25143;&#8221;&#21644;&#8220;&#28895;&#38654;&#25506;&#27979;&#22120;&#8221;&#65292;&#32780;re
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02231v1 Announce Type: new  Abstract: Automatic Compliance Checking (ACC) within the Architecture, Engineering, and Construction (AEC) sector necessitates automating the interpretation of building regulations to achieve its full potential. However, extracting information from textual rules to convert them to a machine-readable format has been a challenge due to the complexities associated with natural language and the limited resources that can support advanced machine-learning techniques. To address this challenge, we introduce CODE-ACCORD, a unique dataset compiled under the EU Horizon ACCORD project. CODE-ACCORD comprises 862 self-contained sentences extracted from the building regulations of England and Finland. Aligned with our core objective of facilitating information extraction from text for machine-readable rule generation, each sentence was annotated with entities and relations. Entities represent specific components such as "window" and "smoke detectors", while re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#35780;&#20272;&#31070;&#32463;&#25490;&#24207;&#22120;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20849;&#21516;&#30340;&#35780;&#20272;&#24179;&#21488;&#65292;&#35201;&#27714;&#27599;&#20010;&#27169;&#22411;&#38500;&#20102;&#36820;&#22238;&#25490;&#21517;&#30340;&#25991;&#26723;&#21015;&#34920;&#22806;&#65292;&#36824;&#38656;&#36820;&#22238;&#27599;&#20010;&#25991;&#26723;&#30340;&#35299;&#37322;&#21333;&#20803;&#25110;&#35299;&#37322;&#29702;&#30001;&#21015;&#34920;&#12290;</title><link>https://arxiv.org/abs/2403.01981</link><description>&lt;p&gt;
&#35780;&#20272;&#31070;&#32463;&#25490;&#24207;&#22120;&#30340;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Explainability of Neural Rankers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01981
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#35780;&#20272;&#31070;&#32463;&#25490;&#24207;&#22120;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20849;&#21516;&#30340;&#35780;&#20272;&#24179;&#21488;&#65292;&#35201;&#27714;&#27599;&#20010;&#27169;&#22411;&#38500;&#20102;&#36820;&#22238;&#25490;&#21517;&#30340;&#25991;&#26723;&#21015;&#34920;&#22806;&#65292;&#36824;&#38656;&#36820;&#22238;&#27599;&#20010;&#25991;&#26723;&#30340;&#35299;&#37322;&#21333;&#20803;&#25110;&#35299;&#37322;&#29702;&#30001;&#21015;&#34920;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#24050;&#32463;&#32463;&#21382;&#20102;&#20174;&#26080;&#30417;&#30563;&#32479;&#35745;&#26041;&#27861;&#21040;&#22522;&#20110;&#29305;&#24449;&#30340;&#30417;&#30563;&#26041;&#27861;&#65292;&#20877;&#21040;&#23436;&#20840;&#25968;&#25454;&#39537;&#21160;&#30340;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#30340;&#33539;&#24335;&#36716;&#21464;&#12290;&#34429;&#28982;&#25628;&#32034;&#27169;&#22411;&#30340;&#22686;&#21152;&#22797;&#26434;&#24615;&#24050;&#32463;&#33021;&#22815;&#23637;&#31034;&#22312;&#25928;&#26524;&#26041;&#38754;&#30340;&#25913;&#36827;&#65288;&#20197;&#26816;&#32034;&#32467;&#26524;&#30340;&#30456;&#20851;&#24615;&#20026;&#34913;&#37327;&#26631;&#20934;&#65289;&#65292;&#20294;&#19968;&#20010;&#20540;&#24471;&#24443;&#24213;&#26816;&#26597;&#30340;&#38382;&#39064;&#26159; - "&#36825;&#20123;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#26377;&#22810;&#39640;&#65311;"&#65292;&#36825;&#23601;&#26159;&#26412;&#25991;&#30340;&#35780;&#20272;&#30446;&#26631;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20849;&#21516;&#30340;&#35780;&#20272;&#24179;&#21488;&#65292;&#31995;&#32479;&#22320;&#35780;&#20272;&#20219;&#20309;&#25490;&#24207;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#65288;&#35828;&#26126;&#31639;&#27861;&#23545;&#20110;&#25152;&#26377;&#24453;&#35780;&#20272;&#27169;&#22411;&#37117;&#26159;&#30456;&#21516;&#30340;&#65289;&#12290;&#22312;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#20013;&#65292;&#27599;&#20010;&#27169;&#22411;&#38500;&#20102;&#36820;&#22238;&#19968;&#20010;&#25490;&#21517;&#30340;&#25991;&#26723;&#21015;&#34920;&#20043;&#22806;&#65292;&#36824;&#38656;&#35201;&#36820;&#22238;&#27599;&#20010;&#25991;&#26723;&#30340;&#35299;&#37322;&#21333;&#20803;&#25110;&#35299;&#37322;&#29702;&#30001;&#30340;&#21015;&#34920;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01981v1 Announce Type: new  Abstract: Information retrieval models have witnessed a paradigm shift from unsupervised statistical approaches to feature-based supervised approaches to completely data-driven ones that make use of the pre-training of large language models. While the increasing complexity of the search models have been able to demonstrate improvements in effectiveness (measured in terms of relevance of top-retrieved results), a question worthy of a thorough inspection is - "how explainable are these models?", which is what this paper aims to evaluate. In particular, we propose a common evaluation platform to systematically evaluate the explainability of any ranking model (the explanation algorithm being identical for all the models that are to be evaluated). In our proposed framework, each model, in addition to returning a ranked list of documents, also requires to return a list of explanation units or rationales for each document. This meta-information from each
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#23450;&#20041;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#8212;&#8212;&#25512;&#33616;&#35780;&#35770;&#32773;&#26631;&#35782;&#30340;&#36951;&#28431;&#24341;&#29992;(RMC)&#65292;&#24182;&#26500;&#24314;&#20102;&#23545;&#24212;&#30340;&#25968;&#25454;&#38598;CitationR&#65292;&#20197;&#25913;&#36827;&#23436;&#25972;&#35770;&#25991;&#30340;&#24341;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.01873</link><description>&lt;p&gt;
&#25512;&#33616;&#35780;&#35770;&#32773;&#26631;&#35782;&#30340;&#36951;&#28431;&#24341;&#29992;&#65306;&#19968;&#39033;&#26032;&#20219;&#21153;&#12289;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Recommending Missed Citations Identified by Reviewers: A New Task, Dataset and Baselines
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01873
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#23450;&#20041;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#8212;&#8212;&#25512;&#33616;&#35780;&#35770;&#32773;&#26631;&#35782;&#30340;&#36951;&#28431;&#24341;&#29992;(RMC)&#65292;&#24182;&#26500;&#24314;&#20102;&#23545;&#24212;&#30340;&#25968;&#25454;&#38598;CitationR&#65292;&#20197;&#25913;&#36827;&#23436;&#25972;&#35770;&#25991;&#30340;&#24341;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31185;&#23398;&#20986;&#29256;&#29289;&#25968;&#37327;&#30340;&#29190;&#28856;&#24615;&#22686;&#38271;&#65292;&#20840;&#38754;&#21644;&#24688;&#24403;&#22320;&#24341;&#29992;&#24050;&#32463;&#25104;&#20026;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#30446;&#21069;&#30340;&#24341;&#29992;&#25512;&#33616;&#31995;&#32479;&#26088;&#22312;&#20026;&#32473;&#23450;&#25991;&#26412;&#29615;&#22659;&#25110;&#33609;&#31295;&#25512;&#33616;&#19968;&#31995;&#21015;&#31185;&#23398;&#35770;&#25991;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#24037;&#20316;&#37117;&#27809;&#26377;&#30528;&#37325;&#20110;&#24050;&#32463;&#21253;&#21547;&#22312;&#23436;&#25972;&#35770;&#25991;&#20013;&#30340;&#24341;&#29992;&#65292;&#36825;&#20123;&#24341;&#29992;&#24182;&#19981;&#23436;&#32654;&#65292;&#20173;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#31354;&#38388;&#12290;&#22312;&#21516;&#34892;&#35780;&#23457;&#30340;&#22330;&#26223;&#20013;&#65292;&#25552;&#20132;&#30340;&#35770;&#25991;&#34987;&#35780;&#35770;&#32773;&#26631;&#35782;&#20026;&#36951;&#28431;&#37325;&#35201;&#24341;&#29992;&#26159;&#19968;&#31181;&#24120;&#35265;&#29616;&#35937;&#12290;&#36825;&#21487;&#33021;&#20250;&#23545;&#25152;&#21576;&#29616;&#30340;&#30740;&#31350;&#30340;&#21487;&#20449;&#24230;&#21644;&#26377;&#25928;&#24615;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#20026;&#20102;&#24110;&#21161;&#25913;&#36827;&#23436;&#25972;&#35770;&#25991;&#30340;&#24341;&#29992;&#65292;&#25105;&#20204;&#39318;&#20808;&#23450;&#20041;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#8212;&#8212;&#25512;&#33616;&#35780;&#35770;&#32773;&#26631;&#35782;&#30340;&#36951;&#28431;&#24341;&#29992;&#65288;RMC&#65289;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#30456;&#24212;&#30340;&#19987;&#23478;&#26631;&#35760;&#25968;&#25454;&#38598;&#31216;&#20026;CitationR&#12290;&#25105;&#20204;&#22312;CitationR&#19978;&#23545;&#20960;&#31181;&#26368;&#20808;&#36827;&#26041;&#27861;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01873v1 Announce Type: new  Abstract: Citing comprehensively and appropriately has become a challenging task with the explosive growth of scientific publications. Current citation recommendation systems aim to recommend a list of scientific papers for a given text context or a draft paper. However, none of the existing work focuses on already included citations of full papers, which are imperfect and still have much room for improvement. In the scenario of peer reviewing, it is a common phenomenon that submissions are identified as missing vital citations by reviewers. This may lead to a negative impact on the credibility and validity of the research presented. To help improve citations of full papers, we first define a novel task of Recommending Missed Citations Identified by Reviewers (RMC) and construct a corresponding expert-labeled dataset called CitationR. We conduct an extensive evaluation of several state-of-the-art methods on CitationR. Furthermore, we propose a new
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35774;&#35745;&#20102;&#21487;&#19982;&#20219;&#20309;&#20998;&#21306;&#26041;&#27861;&#19968;&#36215;&#20351;&#29992;&#30340;&#31616;&#21333;&#26377;&#25928;&#30340;&#36335;&#30001;&#26041;&#27861;&#65292;&#24182;&#22312;&#24615;&#33021;&#19978;&#35777;&#26126;&#20102;&#24378;&#22823;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;</title><link>https://arxiv.org/abs/2403.01797</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#26368;&#36817;&#37051;&#25628;&#32034;&#30340;&#22270;&#20998;&#21306;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Unleashing Graph Partitioning for Large-Scale Nearest Neighbor Search
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01797
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35774;&#35745;&#20102;&#21487;&#19982;&#20219;&#20309;&#20998;&#21306;&#26041;&#27861;&#19968;&#36215;&#20351;&#29992;&#30340;&#31616;&#21333;&#26377;&#25928;&#30340;&#36335;&#30001;&#26041;&#27861;&#65292;&#24182;&#22312;&#24615;&#33021;&#19978;&#35777;&#26126;&#20102;&#24378;&#22823;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#23558;&#22823;&#35268;&#27169;&#36817;&#20284;&#26368;&#36817;&#37051;&#25628;&#32034;&#65288;ANNS&#65289;&#38382;&#39064;&#20998;&#35299;&#20026;&#36739;&#23567;&#23376;&#38382;&#39064;&#30340;&#22522;&#26412;&#38382;&#39064;&#12290;&#30446;&#26631;&#26159;&#23558;&#36755;&#20837;&#28857;&#20998;&#21306;&#20026;&#20445;&#30041;&#37051;&#22495;&#30340;&#30862;&#29255;&#65292;&#20197;&#20415;&#20219;&#20309;&#28857;&#30340;&#26368;&#36817;&#37051;&#23621;&#20165;&#21253;&#21547;&#22312;&#23569;&#25968;&#30862;&#29255;&#20013;&#12290;&#24403;&#26597;&#35810;&#21040;&#36798;&#26102;&#65292;&#23558;&#20351;&#29992;&#36335;&#30001;&#31639;&#27861;&#35782;&#21035;&#24212;&#35813;&#25628;&#32034;&#20854;&#26368;&#36817;&#37051;&#23621;&#30340;&#30862;&#29255;&#12290;&#36825;&#31181;&#26041;&#27861;&#26500;&#24314;&#20102;&#20998;&#24067;&#24335;ANNS&#30340;&#39592;&#24178;&#65292;&#20854;&#20013;&#25968;&#25454;&#38598;&#38750;&#24120;&#24222;&#22823;&#65292;&#24517;&#39035;&#36328;&#22810;&#21488;&#26426;&#22120;&#36827;&#34892;&#25286;&#20998;&#12290;&#26412;&#25991;&#35774;&#35745;&#20102;&#31616;&#21333;&#19988;&#39640;&#25928;&#30340;&#36335;&#30001;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#20204;&#24615;&#33021;&#30340;&#24378;&#22823;&#29702;&#35770;&#20445;&#35777;&#12290;&#25105;&#20204;&#30340;&#36335;&#30001;&#31639;&#27861;&#30340;&#19968;&#20010;&#20851;&#38190;&#29305;&#24449;&#26159;&#23427;&#20204;&#26159;&#22266;&#26377;&#27169;&#22359;&#21270;&#30340;&#65292;&#21487;&#20197;&#19982;&#20219;&#20309;&#20998;&#21306;&#26041;&#27861;&#19968;&#36215;&#20351;&#29992;&#12290;&#36825;&#35299;&#20915;&#20102;&#20197;&#24448;&#26041;&#27861;&#30340;&#19968;&#20010;&#20851;&#38190;&#32570;&#28857;&#65292;&#21363;&#36335;&#30001;&#31639;&#27861;&#19982;&#20854;&#20851;&#32852;&#37096;&#20998;&#32039;&#23494;&#30456;&#36830;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01797v1 Announce Type: cross  Abstract: We consider the fundamental problem of decomposing a large-scale approximate nearest neighbor search (ANNS) problem into smaller sub-problems. The goal is to partition the input points into neighborhood-preserving shards, so that the nearest neighbors of any point are contained in only a few shards. When a query arrives, a routing algorithm is used to identify the shards which should be searched for its nearest neighbors. This approach forms the backbone of distributed ANNS, where the dataset is so large that it must be split across multiple machines.   In this paper, we design simple and highly efficient routing methods, and prove strong theoretical guarantees on their performance. A crucial characteristic of our routing algorithms is that they are inherently modular, and can be used with any partitioning method. This addresses a key drawback of prior approaches, where the routing algorithms are inextricably linked to their associated
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#20250;&#35805;&#25628;&#32034;&#20013;&#30340;&#22522;&#20110;&#23454;&#20307;&#30340;&#31572;&#26696;&#37325;&#20889;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#31572;&#26696;&#37325;&#20889;&#31574;&#30053;&#65292;&#20197;&#25913;&#21892;&#29992;&#25143;&#20307;&#39564;&#12290;</title><link>https://arxiv.org/abs/2403.01747</link><description>&lt;p&gt;
&#26397;&#21521;&#33258;&#21253;&#21547;&#31572;&#26696;&#30340;&#26041;&#21521;&#65306;&#20250;&#35805;&#25628;&#32034;&#20013;&#22522;&#20110;&#23454;&#20307;&#30340;&#31572;&#26696;&#37325;&#20889;
&lt;/p&gt;
&lt;p&gt;
Towards Self-Contained Answers: Entity-Based Answer Rewriting in Conversational Search
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01747
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#20250;&#35805;&#25628;&#32034;&#20013;&#30340;&#22522;&#20110;&#23454;&#20307;&#30340;&#31572;&#26696;&#37325;&#20889;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#31572;&#26696;&#37325;&#20889;&#31574;&#30053;&#65292;&#20197;&#25913;&#21892;&#29992;&#25143;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20250;&#35805;&#20449;&#24687;&#26816;&#32034;&#65288;CIS&#65289;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#30693;&#35782;&#33719;&#21462;&#21644;&#25506;&#32034;&#24615;&#25628;&#32034;&#33539;&#24335;&#12290;&#20256;&#32479;&#30340;&#32593;&#32476;&#25628;&#32034;&#30028;&#38754;&#21487;&#20197;&#36731;&#26494;&#25506;&#32034;&#23454;&#20307;&#65292;&#20294;&#22312;&#20250;&#35805;&#29615;&#22659;&#20013;&#21463;&#38480;&#20110;&#24102;&#23485;&#26377;&#38480;&#30340;&#30028;&#38754;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;CIS&#20013;&#37325;&#20889;&#31572;&#26696;&#30340;&#26041;&#27861;&#65292;&#20197;&#20415;&#29992;&#25143;&#21487;&#20197;&#29702;&#35299;&#31572;&#26696;&#32780;&#26080;&#38656;&#27714;&#21161;&#22806;&#37096;&#26381;&#21153;&#25110;&#26469;&#28304;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20851;&#27880;&#31361;&#20986;&#30340;&#23454;&#20307;--&#23545;&#20110;&#29702;&#35299;&#31572;&#26696;&#33267;&#20851;&#37325;&#35201;&#30340;&#23454;&#20307;&#12290;&#20316;&#20026;&#25105;&#20204;&#30340;&#31532;&#19968;&#20010;&#36129;&#29486;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#24102;&#26377;&#31361;&#20986;&#23454;&#20307;&#27880;&#37322;&#30340;&#23545;&#35805;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#23545;&#25910;&#38598;&#21040;&#30340;&#25968;&#25454;&#36827;&#34892;&#20998;&#26512;&#21518;&#21457;&#29616;&#65292;&#22823;&#22810;&#25968;&#31572;&#26696;&#21253;&#21547;&#20102;&#31361;&#20986;&#23454;&#20307;&#12290;&#20316;&#20026;&#25105;&#20204;&#30340;&#31532;&#20108;&#20010;&#36129;&#29486;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26088;&#22312;&#25913;&#21892;CIS&#20013;&#29992;&#25143;&#20307;&#39564;&#30340;&#31572;&#26696;&#37325;&#20889;&#31574;&#30053;&#12290;&#20854;&#19968;&#36890;&#36807;&#20869;&#32852;&#23450;&#20041;&#31361;&#20986;&#23454;&#20307;&#26469;&#25193;&#23637;&#31572;&#26696;&#65292;&#20351;&#31572;&#26696;&#33258;&#21253;&#21547;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01747v1 Announce Type: cross  Abstract: Conversational information-seeking (CIS) is an emerging paradigm for knowledge acquisition and exploratory search. Traditional web search interfaces enable easy exploration of entities, but this is limited in conversational settings due to the limited-bandwidth interface. This paper explore ways to rewrite answers in CIS, so that users can understand them without having to resort to external services or sources. Specifically, we focus on salient entities -- entities that are central to understanding the answer. As our first contribution, we create a dataset of conversations annotated with entities for saliency. Our analysis of the collected data reveals that the majority of answers contain salient entities. As our second contribution, we propose two answer rewriting strategies aimed at improving the overall user experience in CIS. One approach expands answers with inline definitions of salient entities, making the answer self-contained
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;NoteLLM&#30340;&#26032;&#39062;&#32479;&#19968;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26469;&#23454;&#29616;&#29289;&#21697;&#21040;&#29289;&#21697;(I2I)&#30340;&#31508;&#35760;&#25512;&#33616;&#65292;&#36890;&#36807;&#23398;&#20064;&#29983;&#25104;&#21704;&#24076;&#26631;&#31614;/&#31867;&#21035;&#28508;&#22312;&#22320;&#22686;&#24378;&#31508;&#35760;&#23884;&#20837;&#65292;&#25552;&#39640;&#20102;&#23545;&#20851;&#38190;&#31508;&#35760;&#20449;&#24687;&#30340;&#21387;&#32553;&#12290;</title><link>https://arxiv.org/abs/2403.01744</link><description>&lt;p&gt;
NoteLLM: &#19968;&#31181;&#21487;&#26816;&#32034;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#31508;&#35760;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
NoteLLM: A Retrievable Large Language Model for Note Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01744
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;NoteLLM&#30340;&#26032;&#39062;&#32479;&#19968;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26469;&#23454;&#29616;&#29289;&#21697;&#21040;&#29289;&#21697;(I2I)&#30340;&#31508;&#35760;&#25512;&#33616;&#65292;&#36890;&#36807;&#23398;&#20064;&#29983;&#25104;&#21704;&#24076;&#26631;&#31614;/&#31867;&#21035;&#28508;&#22312;&#22320;&#22686;&#24378;&#31508;&#35760;&#23884;&#20837;&#65292;&#25552;&#39640;&#20102;&#23545;&#20851;&#38190;&#31508;&#35760;&#20449;&#24687;&#30340;&#21387;&#32553;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20204;&#21916;&#27426;&#22312;&#22312;&#32447;&#31038;&#21306;&#20869;&#20998;&#20139;&#8220;&#31508;&#35760;&#8221;&#65292;&#21253;&#25324;&#20182;&#20204;&#30340;&#32463;&#39564;&#12290;&#22240;&#27492;&#65292;&#25512;&#33616;&#19982;&#29992;&#25143;&#20852;&#36259;&#30456;&#31526;&#30340;&#31508;&#35760;&#24050;&#32463;&#25104;&#20026;&#19968;&#39033;&#20851;&#38190;&#20219;&#21153;&#12290;&#29616;&#26377;&#30340;&#22312;&#32447;&#26041;&#27861;&#21482;&#23558;&#31508;&#35760;&#36755;&#20837;&#21040;&#22522;&#20110;BERT&#30340;&#27169;&#22411;&#20013;&#65292;&#29992;&#20110;&#29983;&#25104;&#31508;&#35760;&#23884;&#20837;&#20197;&#35780;&#20272;&#30456;&#20284;&#24615;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#21487;&#33021;&#26410;&#20805;&#20998;&#21033;&#29992;&#19968;&#20123;&#37325;&#35201;&#30340;&#32447;&#32034;&#65292;&#20363;&#22914;&#21704;&#24076;&#26631;&#31614;&#25110;&#31867;&#21035;&#65292;&#36825;&#20123;&#20195;&#34920;&#20102;&#31508;&#35760;&#30340;&#20851;&#38190;&#27010;&#24565;&#12290;&#20107;&#23454;&#19978;&#65292;&#23398;&#20064;&#29983;&#25104;&#21704;&#24076;&#26631;&#31614;/&#31867;&#21035;&#21487;&#20197;&#28508;&#22312;&#22320;&#22686;&#24378;&#31508;&#35760;&#23884;&#20837;&#65292;&#20108;&#32773;&#37117;&#23558;&#37325;&#35201;&#30340;&#31508;&#35760;&#20449;&#24687;&#21387;&#32553;&#20026;&#26377;&#38480;&#20869;&#23481;&#12290;&#27492;&#22806;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29702;&#35299;&#33258;&#28982;&#35821;&#35328;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;BERT&#12290;&#23558;LLMs&#24341;&#20837;&#31508;&#35760;&#25512;&#33616;&#26159;&#24456;&#26377;&#21069;&#36884;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;NoteLLM&#30340;&#26032;&#39062;&#32479;&#19968;&#26694;&#26550;&#65292;&#21033;&#29992;LLMs&#26469;&#22788;&#29702;&#29289;&#21697;&#21040;&#29289;&#21697;&#65288;I2I&#65289;&#31508;&#35760;&#25512;&#33616;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21033;&#29992;&#31508;&#35760;&#21387;&#32553;&#25552;&#31034;&#26469;&#21387;&#32553;&#19968;&#26465;&#31508;&#35760;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01744v1 Announce Type: new  Abstract: People enjoy sharing "notes" including their experiences within online communities. Therefore, recommending notes aligned with user interests has become a crucial task. Existing online methods only input notes into BERT-based models to generate note embeddings for assessing similarity. However, they may underutilize some important cues, e.g., hashtags or categories, which represent the key concepts of notes. Indeed, learning to generate hashtags/categories can potentially enhance note embeddings, both of which compress key note information into limited content. Besides, Large Language Models (LLMs) have significantly outperformed BERT in understanding natural languages. It is promising to introduce LLMs into note recommendation. In this paper, we propose a novel unified framework called NoteLLM, which leverages LLMs to address the item-to-item (I2I) note recommendation. Specifically, we utilize Note Compression Prompt to compress a note 
&lt;/p&gt;</description></item><item><title>TweetInfo&#26159;&#19968;&#20010;&#20114;&#21160;&#31995;&#32479;&#65292;&#36890;&#36807;&#25552;&#20379;&#26377;&#20851;&#24086;&#23376;&#30340;&#20803;&#20449;&#24687;&#26469;&#20943;&#36731;&#26377;&#23475;&#20869;&#23481;&#30340;&#28040;&#36153;&#65292;&#37325;&#28857;&#20851;&#27880;&#20167;&#24680;&#35328;&#35770;&#21644;&#19981;&#23454;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2403.01646</link><description>&lt;p&gt;
TweetInfo&#65306;&#19968;&#20010;&#20114;&#21160;&#31995;&#32479;&#20197;&#20943;&#36731;&#22312;&#32447;&#20260;&#23475;
&lt;/p&gt;
&lt;p&gt;
TweetInfo: An Interactive System to Mitigate Online Harm
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01646
&lt;/p&gt;
&lt;p&gt;
TweetInfo&#26159;&#19968;&#20010;&#20114;&#21160;&#31995;&#32479;&#65292;&#36890;&#36807;&#25552;&#20379;&#26377;&#20851;&#24086;&#23376;&#30340;&#20803;&#20449;&#24687;&#26469;&#20943;&#36731;&#26377;&#23475;&#20869;&#23481;&#30340;&#28040;&#36153;&#65292;&#37325;&#28857;&#20851;&#27880;&#20167;&#24680;&#35328;&#35770;&#21644;&#19981;&#23454;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#32593;&#32476;&#31449;&#28857;&#65288;SNSs&#65289;&#19978;&#27963;&#36291;&#29992;&#25143;&#30340;&#22686;&#21152;&#20063;&#35266;&#23519;&#21040;&#31038;&#20132;&#23186;&#20307;&#31449;&#28857;&#19978;&#26377;&#23475;&#20869;&#23481;&#30340;&#22686;&#21152;&#12290;&#26377;&#23475;&#20869;&#23481;&#34987;&#25551;&#36848;&#20026;&#19968;&#31181;&#25439;&#23475;&#25110;&#27450;&#39575;&#20010;&#20154;&#25110;&#19968;&#32676;&#29992;&#25143;&#30340;&#19981;&#24403;&#27963;&#21160;&#12290;&#38500;&#20102;&#29616;&#26377;&#30340;&#26816;&#27979;&#19981;&#23454;&#20449;&#24687;&#21644;&#20167;&#24680;&#35328;&#35770;&#30340;&#26041;&#27861;&#22806;&#65292;&#29992;&#25143;&#20173;&#28982;&#38656;&#35201;&#20805;&#20998;&#20102;&#35299;SNSs&#19978;&#20869;&#23481;&#30340;&#26377;&#23475;&#24615;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;TweetInfo&#30340;&#29992;&#25143;&#20114;&#21160;&#31995;&#32479;&#65292;&#36890;&#36807;&#25552;&#20379;&#26377;&#20851;&#24086;&#23376;&#30340;&#20803;&#20449;&#24687;&#26469;&#20943;&#36731;&#26377;&#23475;&#20869;&#23481;&#30340;&#28040;&#36153;&#12290;&#23427;&#19987;&#27880;&#20110;&#20004;&#31181;&#31867;&#22411;&#30340;&#26377;&#23475;&#20869;&#23481;&#65306;&#20167;&#24680;&#35328;&#35770;&#21644;&#19981;&#23454;&#20449;&#24687;&#12290;TweetInfo&#36890;&#36807;&#20869;&#23481;&#20998;&#26512;&#25552;&#20379;&#20102;&#26377;&#20851;&#25512;&#25991;&#30340;&#35265;&#35299;&#12290;&#26681;&#25454;&#20197;&#24448;&#30740;&#31350;&#65292;&#25105;&#20204;&#36873;&#25321;&#20102;&#19968;&#31995;&#21015;&#20803;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#26681;&#25454;&#20803;&#20449;&#24687;Bot&#12289;&#20167;&#24680;&#35328;&#35770;&#12289;&#38169;&#35823;&#20449;&#24687;&#12289;&#24050;&#39564;&#35777;&#36134;&#25143;&#12289;&#24773;&#24863;&#12289;&#25512;&#25991;&#31867;&#21035;&#12289;&#35821;&#35328;&#23545;&#20869;&#23481;&#36827;&#34892;&#36807;&#28388;&#30340;&#36873;&#39033;&#12290;&#25152;&#25552;&#20986;&#30340;&#29992;&#25143;&#30028;&#38754;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01646v1 Announce Type: cross  Abstract: The increase in active users on social networking sites (SNSs) has also observed an increase in harmful content on social media sites. Harmful content is described as an inappropriate activity to harm or deceive an individual or a group of users. Alongside existing methods to detect misinformation and hate speech, users still need to be well-informed about the harmfulness of the content on SNSs. This study proposes a user-interactive system TweetInfo for mitigating the consumption of harmful content by providing metainformation about the posts. It focuses on two types of harmful content: hate speech and misinformation. TweetInfo provides insights into tweets by doing content analysis. Based on previous research, we have selected a list of metainformation. We offer the option to filter content based on metainformation Bot, Hate Speech, Misinformation, Verified Account, Sentiment, Tweet Category, Language. The proposed user interface all
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21033;&#29992;&#25991;&#26412;&#25366;&#25496;&#21644;&#25991;&#29486;&#35745;&#37327;&#20998;&#26512;&#26041;&#27861;&#65292;&#22522;&#20110;OpenAlex&#30446;&#24405;&#65292;&#39044;&#27979;&#20102;&#21152;&#23494;&#25216;&#26415;&#30340;&#25216;&#26415;&#25509;&#36817;&#24230;&#25351;&#25968;&#65292;&#21457;&#29616;&#20102;&#21306;&#22359;&#38142;&#19982;&#20844;&#38053;&#23494;&#30721;&#23398;&#20043;&#38388;&#26174;&#33879;&#30340;&#25216;&#26415;&#34701;&#21512;&#12290;</title><link>https://arxiv.org/abs/2403.01601</link><description>&lt;p&gt;
&#20351;&#29992;Proximity Indices&#27979;&#37327;&#21152;&#23494;&#25216;&#26415;&#20013;&#30340;&#25216;&#26415;&#34701;&#21512;&#65306;&#22522;&#20110;OpenAlex&#30340;&#25991;&#26412;&#25366;&#25496;&#21644;&#25991;&#29486;&#35745;&#37327;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Measuring Technological Convergence in Encryption Technologies with Proximity Indices: A Text Mining and Bibliometric Analysis using OpenAlex
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01601
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21033;&#29992;&#25991;&#26412;&#25366;&#25496;&#21644;&#25991;&#29486;&#35745;&#37327;&#20998;&#26512;&#26041;&#27861;&#65292;&#22522;&#20110;OpenAlex&#30446;&#24405;&#65292;&#39044;&#27979;&#20102;&#21152;&#23494;&#25216;&#26415;&#30340;&#25216;&#26415;&#25509;&#36817;&#24230;&#25351;&#25968;&#65292;&#21457;&#29616;&#20102;&#21306;&#22359;&#38142;&#19982;&#20844;&#38053;&#23494;&#30721;&#23398;&#20043;&#38388;&#26174;&#33879;&#30340;&#25216;&#26415;&#34701;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30830;&#23450;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#26032;&#20852;&#25216;&#26415;&#20043;&#38388;&#30340;&#25216;&#26415;&#34701;&#21512;&#23545;&#20110;&#25512;&#21160;&#31185;&#23398;&#21457;&#23637;&#21644;&#20419;&#36827;&#21019;&#26032;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#21516;&#20110;&#20197;&#24448;&#30740;&#31350;&#20165;&#20851;&#27880;&#35770;&#25991;&#19982;&#25216;&#26415;&#27010;&#24565;&#20043;&#38388;&#30340;&#20108;&#20803;&#20851;&#31995;&#65292;&#32780;&#26159;&#21033;&#29992;&#24402;&#22240;&#20998;&#25968;&#22686;&#24378;&#30740;&#31350;&#35770;&#25991;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#32467;&#21512;&#20851;&#38190;&#35789;&#12289;&#24341;&#29992;&#29575;&#21644;&#21512;&#20316;&#29366;&#24577;&#31561;&#19982;&#29305;&#23450;&#25216;&#26415;&#27010;&#24565;&#30456;&#20851;&#30340;&#22240;&#32032;&#12290;&#25552;&#20986;&#30340;&#26041;&#27861;&#25972;&#21512;&#20102;&#25991;&#26412;&#25366;&#25496;&#21644;&#25991;&#29486;&#35745;&#37327;&#20998;&#26512;&#65292;&#21033;&#29992;&#8220;OpenAlex&#8221;&#30446;&#24405;&#21046;&#23450;&#24182;&#39044;&#27979;&#21152;&#23494;&#25216;&#26415;&#30340;&#25216;&#26415;&#25509;&#36817;&#24230;&#25351;&#25968;&#12290;&#25105;&#20204;&#30340;&#26696;&#20363;&#30740;&#31350;&#32467;&#26524;&#31361;&#26174;&#20102;&#21306;&#22359;&#38142;&#19982;&#20844;&#38053;&#23494;&#30721;&#23398;&#20043;&#38388;&#26174;&#33879;&#30340;&#34701;&#21512;&#65292;&#34920;&#29616;&#20026;&#25509;&#36817;&#24230;&#25351;&#25968;&#30340;&#22686;&#21152;&#12290;&#36825;&#20123;&#32467;&#26524;&#20026;&#37027;&#20123;&#32771;&#34385;&#22312;&#36825;&#20123;&#39046;&#22495;&#36827;&#34892;&#25237;&#36164;&#30340;&#20154;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#25112;&#30053;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01601v1 Announce Type: cross  Abstract: Identifying technological convergence among emerging technologies in cybersecurity is crucial for advancing science and fostering innovation. Unlike previous studies focusing on the binary relationship between a paper and the concept it attributes to technology, our approach utilizes attribution scores to enhance the relationships between research papers, combining keywords, citation rates, and collaboration status with specific technological concepts. The proposed method integrates text mining and bibliometric analyses to formulate and predict technological proximity indices for encryption technologies using the "OpenAlex" catalog. Our case study findings highlight a significant convergence between blockchain and public-key cryptography, evidenced by the increasing proximity indices. These results offer valuable strategic insights for those contemplating investments in these domains.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31070;&#32463;&#31526;&#21495;&#22686;&#24378;&#30340;&#27861;&#24459;&#26696;&#20363;&#26816;&#32034;&#65288;NS-LCR&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#26696;&#20363;&#32423;&#21035;&#21644;&#27861;&#24459;&#32423;&#21035;&#30340;&#36923;&#36753;&#35268;&#21017;&#65292;&#23558;&#35268;&#21017;&#20197;&#31070;&#32463;&#31526;&#21495;&#26041;&#24335;&#38598;&#25104;&#21040;&#26816;&#32034;&#36807;&#31243;&#20013;&#65292;&#20197;&#25552;&#20379;&#36923;&#36753;&#19988;&#21487;&#35299;&#37322;&#30340;&#35299;&#37322;&#12290;</title><link>https://arxiv.org/abs/2403.01457</link><description>&lt;p&gt;
&#36923;&#36753;&#35268;&#21017;&#20316;&#20026;&#35299;&#37322;&#27861;&#24459;&#26696;&#20363;&#26816;&#32034;&#30340;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
Logic Rules as Explanations for Legal Case Retrieval
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01457
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31070;&#32463;&#31526;&#21495;&#22686;&#24378;&#30340;&#27861;&#24459;&#26696;&#20363;&#26816;&#32034;&#65288;NS-LCR&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#26696;&#20363;&#32423;&#21035;&#21644;&#27861;&#24459;&#32423;&#21035;&#30340;&#36923;&#36753;&#35268;&#21017;&#65292;&#23558;&#35268;&#21017;&#20197;&#31070;&#32463;&#31526;&#21495;&#26041;&#24335;&#38598;&#25104;&#21040;&#26816;&#32034;&#36807;&#31243;&#20013;&#65292;&#20197;&#25552;&#20379;&#36923;&#36753;&#19988;&#21487;&#35299;&#37322;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20351;&#29992;&#36923;&#36753;&#35268;&#21017;&#26469;&#35299;&#37322;&#27861;&#24459;&#26696;&#20363;&#26816;&#32034;&#32467;&#26524;&#30340;&#38382;&#39064;&#12290;&#36825;&#39033;&#20219;&#21153;&#23545;&#20110;&#27861;&#24459;&#26696;&#20363;&#26816;&#32034;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#29992;&#25143;&#65288;&#22914;&#24459;&#24072;&#25110;&#27861;&#23448;&#65289;&#20855;&#26377;&#39640;&#24230;&#19987;&#19994;&#21270;&#65292;&#38656;&#35201;&#31995;&#32479;&#22312;&#20570;&#20986;&#27861;&#24459;&#20915;&#31574;&#20043;&#21069;&#25552;&#20379;&#36923;&#36753;&#12289;&#24544;&#23454;&#21644;&#21487;&#35299;&#37322;&#30340;&#35299;&#37322;&#12290;&#26368;&#36817;&#65292;&#30740;&#31350;&#24037;&#20316;&#26088;&#22312;&#23398;&#20064;&#21487;&#35299;&#37322;&#30340;&#27861;&#24459;&#26696;&#20363;&#26816;&#32034;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#20174;&#27861;&#24459;&#26696;&#20363;&#20013;&#36873;&#25321;&#22522;&#26412;&#21407;&#29702;&#65288;&#20851;&#38190;&#21477;&#65289;&#20316;&#20026;&#35299;&#37322;&#65292;&#26410;&#33021;&#25552;&#20379;&#24544;&#23454;&#21644;&#36923;&#36753;&#27491;&#30830;&#30340;&#35299;&#37322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31070;&#32463;&#31526;&#21495;&#22686;&#24378;&#30340;&#27861;&#24459;&#26696;&#20363;&#26816;&#32034;&#65288;NS-LCR&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#23398;&#20064;&#26696;&#20363;&#32423;&#21035;&#21644;&#27861;&#24459;&#32423;&#21035;&#30340;&#36923;&#36753;&#35268;&#21017;&#26469;&#26126;&#30830;&#22320;&#23545;&#27861;&#24459;&#26696;&#20363;&#30340;&#21305;&#37197;&#36827;&#34892;&#25512;&#29702;&#12290;&#28982;&#21518;&#23558;&#23398;&#20064;&#21040;&#30340;&#35268;&#21017;&#20197;&#31070;&#32463;&#31526;&#21495;&#26041;&#24335;&#38598;&#25104;&#21040;&#26816;&#32034;&#36807;&#31243;&#20013;&#12290;&#30001;&#20110;&#36923;&#36753;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#29305;&#24615;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01457v1 Announce Type: cross  Abstract: In this paper, we address the issue of using logic rules to explain the results from legal case retrieval. The task is critical to legal case retrieval because the users (e.g., lawyers or judges) are highly specialized and require the system to provide logical, faithful, and interpretable explanations before making legal decisions. Recently, research efforts have been made to learn explainable legal case retrieval models. However, these methods usually select rationales (key sentences) from the legal cases as explanations, failing to provide faithful and logically correct explanations. In this paper, we propose Neural-Symbolic enhanced Legal Case Retrieval (NS-LCR), a framework that explicitly conducts reasoning on the matching of legal cases through learning case-level and law-level logic rules. The learned rules are then integrated into the retrieval process in a neuro-symbolic manner. Benefiting from the logic and interpretable natu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20960;&#31181;&#30693;&#21517;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#32763;&#35793;&#35821;&#35328;&#25551;&#36848;&#20026;&#25968;&#23398;&#20248;&#21270;&#38382;&#39064;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;GPT-4&#22312;&#19968;&#27425;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#24341;&#20837;&#20102;&#8220;LM4OPT&#8221;&#26694;&#26550;&#36827;&#34892;Llama-2-7b&#30340;&#28176;&#36827;&#24494;&#35843;&#12290;</title><link>https://arxiv.org/abs/2403.01342</link><description>&lt;p&gt;
LM4OPT&#65306;&#25581;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21046;&#23450;&#25968;&#23398;&#20248;&#21270;&#38382;&#39064;&#20013;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
LM4OPT: Unveiling the Potential of Large Language Models in Formulating Mathematical Optimization Problems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01342
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20960;&#31181;&#30693;&#21517;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#32763;&#35793;&#35821;&#35328;&#25551;&#36848;&#20026;&#25968;&#23398;&#20248;&#21270;&#38382;&#39064;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;GPT-4&#22312;&#19968;&#27425;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#24341;&#20837;&#20102;&#8220;LM4OPT&#8221;&#26694;&#26550;&#36827;&#34892;Llama-2-7b&#30340;&#28176;&#36827;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#36825;&#19968;&#24555;&#36895;&#21457;&#23637;&#30340;&#39046;&#22495;&#20013;&#65292;&#23558;&#35821;&#35328;&#25551;&#36848;&#32763;&#35793;&#25104;&#25968;&#23398;&#20248;&#21270;&#38382;&#39064;&#30340;&#25968;&#23398;&#20844;&#24335;&#26159;&#19968;&#39033;&#24040;&#22823;&#25361;&#25112;&#65292;&#35201;&#27714;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20855;&#22791;&#22797;&#26434;&#30340;&#29702;&#35299;&#21644;&#22788;&#29702;&#33021;&#21147;&#12290;&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20960;&#31181;&#30693;&#21517;&#30340;LLMs&#65292;&#21253;&#25324;GPT-3.5&#12289;GPT-4&#21644;Llama-2-7b&#65292;&#22312;&#38646;&#27425;&#21644;&#19968;&#27425;&#35774;&#32622;&#20013;&#23545;&#36825;&#19968;&#20219;&#21153;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#26174;&#31034;&#20986;GPT-4&#22312;&#19968;&#27425;&#22330;&#26223;&#20013;&#30340;&#21331;&#36234;&#34920;&#29616;&#12290;&#20854;&#20013;&#24515;&#37096;&#20998;&#26159;&#24341;&#20837;&#20102;&#8220;LM4OPT&#8221;&#65292;&#36825;&#26159;&#19968;&#20010;&#21033;&#29992;&#22122;&#22768;&#23884;&#20837;&#21644;&#19987;&#38376;&#25968;&#25454;&#38598;&#36827;&#34892;Llama-2-7b&#28176;&#36827;&#24494;&#35843;&#30340;&#26694;&#26550;&#12290;&#28982;&#32780;&#65292;&#36825;&#39033;&#30740;&#31350;&#31361;&#20986;&#20102;&#23567;&#22411;&#27169;&#22411;&#65288;&#22914;Llama-2-7b&#65289;&#22312;&#22788;&#29702;&#20887;&#38271;&#21644;&#22797;&#26434;&#36755;&#20837;&#19978;&#30340;&#19978;&#19979;&#25991;&#29702;&#35299;&#33021;&#21147;&#19982;&#26356;&#22823;&#22411;&#23545;&#24212;&#27169;&#22411;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#24046;&#36317;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#30740;&#31350;&#21033;&#29992;&#20102;NL4Opt
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01342v1 Announce Type: new  Abstract: In the rapidly evolving field of natural language processing, the translation of linguistic descriptions into mathematical formulation of optimization problems presents a formidable challenge, demanding intricate understanding and processing capabilities from Large Language Models (LLMs). This study compares prominent LLMs, including GPT-3.5, GPT-4, and Llama-2-7b, in zero-shot and one-shot settings for this task. Our findings show GPT-4's superior performance, particularly in the one-shot scenario. A central part of this research is the introduction of `LM4OPT,' a progressive fine-tuning framework for Llama-2-7b that utilizes noisy embeddings and specialized datasets. However, this research highlights a notable gap in the contextual understanding capabilities of smaller models such as Llama-2-7b compared to larger counterparts, especially in processing lengthy and complex input contexts. Our empirical investigation, utilizing the NL4Opt
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#25512;&#33616;&#31995;&#32479;&#65292;&#29992;&#20110;&#22312;&#36947;&#36335;&#36135;&#36816;&#22312;&#32447;&#37319;&#36141;&#20013;&#36741;&#21161;&#36827;&#34892;&#20379;&#24212;&#21830;&#21457;&#29616;&#65292;&#33021;&#22815;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#20379;&#24212;&#21830;&#25512;&#33616;&#65292;&#32771;&#34385;&#21040;&#23458;&#25143;&#30340;&#38656;&#27714;&#21644;&#20559;&#22909;&#12290;</title><link>https://arxiv.org/abs/2403.01301</link><description>&lt;p&gt;
&#22312;&#32447;&#37319;&#36141;&#20013;&#30340;&#20379;&#24212;&#21830;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Supplier Recommendation in Online Procurement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01301
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#25512;&#33616;&#31995;&#32479;&#65292;&#29992;&#20110;&#22312;&#36947;&#36335;&#36135;&#36816;&#22312;&#32447;&#37319;&#36141;&#20013;&#36741;&#21161;&#36827;&#34892;&#20379;&#24212;&#21830;&#21457;&#29616;&#65292;&#33021;&#22815;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#20379;&#24212;&#21830;&#25512;&#33616;&#65292;&#32771;&#34385;&#21040;&#23458;&#25143;&#30340;&#38656;&#27714;&#21644;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20379;&#24212;&#38142;&#20248;&#21270;&#23545;&#20110;&#20581;&#24247;&#21644;&#30408;&#21033;&#30340;&#20225;&#19994;&#33267;&#20851;&#37325;&#35201;&#12290;&#35768;&#22810;&#20844;&#21496;&#20351;&#29992;&#22312;&#32447;&#37319;&#36141;&#31995;&#32479;&#19982;&#20379;&#24212;&#21830;&#31614;&#35746;&#21512;&#21516;&#12290;&#36992;&#35831;&#26368;&#20855;&#31454;&#20105;&#21147;&#30340;&#20379;&#24212;&#21830;&#31454;&#26631;&#36825;&#20123;&#21512;&#21516;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25512;&#33616;&#31995;&#32479;&#65292;&#20197;&#21327;&#21161;&#22312;&#36947;&#36335;&#36135;&#36816;&#22312;&#32447;&#37319;&#36141;&#20013;&#36827;&#34892;&#20379;&#24212;&#21830;&#21457;&#29616;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#33021;&#22815;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#20379;&#24212;&#21830;&#25512;&#33616;&#65292;&#32771;&#34385;&#21040;&#23458;&#25143;&#30340;&#38656;&#27714;&#21644;&#20559;&#22909;&#12290;&#36825;&#26159;&#25512;&#33616;&#31995;&#32479;&#30340;&#19968;&#31181;&#26032;&#39062;&#24212;&#29992;&#65292;&#38656;&#35201;&#35774;&#35745;&#36873;&#25321;&#20197;&#31526;&#21512;&#22312;&#32447;&#37319;&#36141;&#30340;&#29420;&#29305;&#35201;&#27714;&#12290;&#25105;&#20204;&#20351;&#29992;&#30495;&#23454;&#25968;&#25454;&#36827;&#34892;&#30340;&#21021;&#27493;&#32467;&#26524;&#20196;&#20154;&#40723;&#33310;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01301v1 Announce Type: cross  Abstract: Supply chain optimization is key to a healthy and profitable business. Many companies use online procurement systems to agree contracts with suppliers. It is vital that the most competitive suppliers are invited to bid for such contracts. In this work, we propose a recommender system to assist with supplier discovery in road freight online procurement. Our system is able to provide personalized supplier recommendations, taking into account customer needs and preferences. This is a novel application of recommender systems, calling for design choices that fit the unique requirements of online procurement. Our preliminary results, using real-world data, are promising.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;COOL&#30340;Conjoint Spatio-Temporal&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#26088;&#22312;&#20849;&#21516;&#25429;&#25417;&#20132;&#36890;&#39044;&#27979;&#20013;&#30340;&#39640;&#38454;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2403.01091</link><description>&lt;p&gt;
COOL&#65306;&#19968;&#31181;&#34701;&#21512;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#20132;&#36890;&#39044;&#27979;&#30340;&#20849;&#21516;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
COOL: A Conjoint Perspective on Spatio-Temporal Graph Neural Network for Traffic Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01091
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;COOL&#30340;Conjoint Spatio-Temporal&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#26088;&#22312;&#20849;&#21516;&#25429;&#25417;&#20132;&#36890;&#39044;&#27979;&#20013;&#30340;&#39640;&#38454;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20132;&#36890;&#39044;&#27979;&#65292;&#26088;&#22312;&#26681;&#25454;&#21382;&#21490;&#24773;&#20917;&#39044;&#27979;&#20132;&#36890;&#30340;&#26410;&#26469;&#29366;&#24577;&#12290;&#37492;&#20110;&#20854;&#23545;&#22810;&#20010;&#22330;&#26223;&#30340;&#25345;&#32493;&#20851;&#27880;&#65292;&#24182;&#20419;&#36827;&#20102;&#35768;&#22810;&#19979;&#28216;&#24212;&#29992;&#31243;&#24207;&#30340;&#21457;&#23637;&#65292;&#20363;&#22914;&#22478;&#24066;&#35268;&#21010;&#21644;&#20132;&#36890;&#31649;&#29702;&#65292;&#35813;&#38382;&#39064;&#24050;&#32463;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#29616;&#26377;&#26041;&#27861;&#20542;&#21521;&#20110;&#29420;&#31435;&#22320;&#24314;&#27169;&#26102;&#31354;&#20851;&#31995;&#65292;&#22240;&#27492;&#26410;&#33021;&#20805;&#20998;&#32771;&#34385;&#20004;&#32773;&#30340;&#22797;&#26434;&#39640;&#38454;&#20114;&#21160;&#65292;&#23548;&#33268;&#29616;&#26377;&#26041;&#27861;&#30340;&#25928;&#26524;&#19981;&#20339;&#12290;&#27492;&#22806;&#65292;&#20132;&#36890;&#39044;&#27979;&#20013;&#30340;&#36807;&#28193;&#27169;&#24335;&#30340;&#22810;&#26679;&#24615;&#20351;&#24471;&#29616;&#26377;&#26041;&#27861;&#38590;&#20197;&#25429;&#25417;&#65292;&#38656;&#35201;&#26356;&#28145;&#20837;&#22320;&#25506;&#32034;&#36825;&#31181;&#22810;&#26679;&#24615;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;Conjoint Spatio-Temporal&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;&#32553;&#20889;&#20026;COOL&#65289;&#65292;&#23427;&#20174;&#20808;&#21069;&#21644;&#21518;&#32493;&#20449;&#24687;&#20013;&#24314;&#27169;&#24322;&#26500;&#22270;&#65292;&#20197;&#20849;&#21516;&#25429;&#25417;&#39640;&#38454;&#20114;&#21160;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01091v1 Announce Type: cross  Abstract: This paper investigates traffic forecasting, which attempts to forecast the future state of traffic based on historical situations. This problem has received ever-increasing attention in various scenarios and facilitated the development of numerous downstream applications such as urban planning and transportation management. However, the efficacy of existing methods remains sub-optimal due to their tendency to model temporal and spatial relationships independently, thereby inadequately accounting for complex high-order interactions of both worlds. Moreover, the diversity of transitional patterns in traffic forecasting makes them challenging to capture for existing approaches, warranting a deeper exploration of their diversity. Toward this end, this paper proposes Conjoint Spatio-Temporal graph neural network (abbreviated as COOL), which models heterogeneous graphs from prior and posterior information to conjointly capture high-order sp
&lt;/p&gt;</description></item><item><title>BasedAI&#24341;&#20837;&#20102;Cerberus Squeezing&#26426;&#21046;&#65292;&#23558;&#26631;&#20934;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36716;&#21270;&#20026;&#21152;&#23494;&#30340;&#38646;&#30693;&#35782;&#27169;&#22411;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22312;&#20840;&#21516;&#24577;&#21152;&#23494;&#35745;&#31639;&#29615;&#22659;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.01008</link><description>&lt;p&gt;
BasedAI&#65306;&#29992;&#20110;&#38646;&#30693;&#35782;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;ZK-LLMs&#65289;&#30340;&#21435;&#20013;&#24515;&#21270;P2P&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
BasedAI: A decentralized P2P network for Zero Knowledge Large Language Models (ZK-LLMs)
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01008
&lt;/p&gt;
&lt;p&gt;
BasedAI&#24341;&#20837;&#20102;Cerberus Squeezing&#26426;&#21046;&#65292;&#23558;&#26631;&#20934;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36716;&#21270;&#20026;&#21152;&#23494;&#30340;&#38646;&#30693;&#35782;&#27169;&#22411;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22312;&#20840;&#21516;&#24577;&#21152;&#23494;&#35745;&#31639;&#29615;&#22659;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
BasedAI&#26159;&#19968;&#20010;&#30001;&#20998;&#24067;&#24335;&#26426;&#22120;&#26500;&#25104;&#30340;&#32593;&#32476;&#65292;&#24341;&#20837;&#20102;&#21435;&#20013;&#24515;&#21270;&#22522;&#30784;&#35774;&#26045;&#65292;&#33021;&#22815;&#23558;&#20840;&#21516;&#24577;&#21152;&#23494;&#65288;FHE&#65289;&#19982;&#36830;&#25509;&#21040;&#20854;&#32593;&#32476;&#30340;&#20219;&#20309;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30456;&#32467;&#21512;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#23558;&#19968;&#31181;&#21517;&#20026;&#8220;Cerberus Squeezing&#8221;&#30340;&#40664;&#35748;&#26426;&#21046;&#23884;&#20837;&#21040;&#25366;&#30719;&#36807;&#31243;&#20013;&#65292;&#23454;&#29616;&#20102;&#23558;&#26631;&#20934;LLM&#36716;&#21270;&#20026;&#21152;&#23494;&#30340;&#38646;&#30693;&#35782;LLM&#65292;&#25110;&#8220;ZK-LLMs&#8221;&#65292;&#21033;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#35265;&#35299;&#26469;&#23454;&#29616;&#25968;&#25454;&#38544;&#31169;&#20445;&#25252;&#12290;&#36825;&#31181;&#26032;&#39062;&#30340;&#37327;&#21270;&#26426;&#21046;&#36171;&#20104;BasedAI&#30719;&#24037;&#33021;&#21147;&#65292;&#33021;&#22815;&#22788;&#29702;&#24182;&#21709;&#24212;&#26469;&#33258;&#29992;&#25143;&#19982;LLMs&#30340;&#20132;&#20114;&#24471;&#21040;&#30340;&#25552;&#31034;&#65292;&#32780;&#26080;&#38656;&#35299;&#23494;&#26597;&#35810;&#25110;&#30456;&#24212;&#20869;&#23481;&#12290;&#24341;&#20837;Cerberus Squeezing&#26174;&#33879;&#25913;&#21892;&#20102;&#24403;&#21069;FHE&#20860;&#23481;&#35745;&#31639;&#29615;&#22659;&#20013;&#30001;&#37327;&#21270;&#20989;&#25968;&#24341;&#36215;&#30340;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#65292;&#36890;&#36807;&#31215;&#26497;&#20248;&#21270;&#29992;&#25143;&#12289;&#30719;&#24037;&#21644;&#39564;&#35777;&#32773;&#20043;&#38388;&#30340;&#36890;&#20449;&#35843;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01008v1 Announce Type: cross  Abstract: BasedAI is a distributed network of machines which introduces decentralized infrastructure capable of integrating Fully Homomorphic Encryption (FHE) with any large language model (LLM) connected to its network. The proposed framework embeds a default mechanism, called "Cerberus Squeezing", into the mining process which enables the transformation of a standard LLMs into encrypted zero-knowledge LLMs, or "ZK-LLMs", leveraging insights from generative adversarial networks for data privacy. This novel quantization mechanism empowers BasedAI miners to process and respond to prompts derived from User interaction with LLMs without the need for decrypting ei- ther the queries or their corresponding responses. The introduction of Cerberus Squeezing significantly improves performance degradation caused by quantized functions in current FHE-compliant computing environments by proactively optimizing calls between users, miners, and validators.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#22270;&#24418;&#21644;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;&#30005;&#23376;&#21830;&#21153;&#39046;&#22495;&#20013;&#25628;&#32034;&#30456;&#20851;&#24615;&#65292;&#24357;&#34917;&#20102;&#29616;&#26377;&#27169;&#22411;&#22312;&#23454;&#38469;&#37096;&#32626;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#30340;&#19981;&#36275;&#12290;</title><link>https://arxiv.org/abs/2403.00923</link><description>&lt;p&gt;
&#29992;&#20110;&#25552;&#39640;&#30005;&#23376;&#21830;&#21153;&#25628;&#32034;&#30456;&#20851;&#24615;&#30340;&#22270;&#24418;&#21644;&#35821;&#35328;&#27169;&#22411;&#21487;&#35299;&#37322;&#30340;&#38598;&#25104;
&lt;/p&gt;
&lt;p&gt;
An Interpretable Ensemble of Graph and Language Models for Improving Search Relevance in E-Commerce
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00923
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#22270;&#24418;&#21644;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;&#30005;&#23376;&#21830;&#21153;&#39046;&#22495;&#20013;&#25628;&#32034;&#30456;&#20851;&#24615;&#65292;&#24357;&#34917;&#20102;&#29616;&#26377;&#27169;&#22411;&#22312;&#23454;&#38469;&#37096;&#32626;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30005;&#23376;&#21830;&#21153;&#39046;&#22495;&#65292;&#25628;&#32034;&#30456;&#20851;&#24615;&#30340;&#38382;&#39064;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#28041;&#21450;&#29702;&#35299;&#29992;&#25143;&#30340;&#31616;&#30701;&#24494;&#22937;&#26597;&#35810;&#30340;&#24847;&#22270;&#65292;&#24182;&#23558;&#20854;&#19982;&#30446;&#24405;&#20013;&#30340;&#36866;&#24403;&#20135;&#21697;&#30456;&#21305;&#37197;&#12290;&#20256;&#32479;&#19978;&#65292;&#36825;&#20010;&#38382;&#39064;&#26159;&#36890;&#36807;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#26469;&#24212;&#23545;&#30340;&#65292;&#20197;&#25429;&#25417;&#35821;&#20041;&#21644;&#20135;&#21697;&#38388;&#34892;&#20026;&#20449;&#21495;&#12290;&#28982;&#32780;&#65292;&#26032;&#26550;&#26500;&#30340;&#24555;&#36895;&#21457;&#23637;&#36896;&#25104;&#20102;&#30740;&#31350;&#21644;&#36825;&#20123;&#25216;&#26415;&#23454;&#38469;&#24212;&#29992;&#20043;&#38388;&#30340;&#40511;&#27807;&#12290;&#35780;&#20272;&#36825;&#20123;&#27169;&#22411;&#22312;&#37096;&#32626;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#38656;&#35201;&#23545;&#22797;&#26434;&#30340;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#36827;&#34892;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#36825;&#21487;&#33021;&#24182;&#19981;&#31616;&#21333;&#19988;&#26114;&#36149;&#12290;&#27492;&#22806;&#65292;&#36825;&#31181;&#27169;&#22411;&#36890;&#24120;&#22312;&#19981;&#20026;&#20154;&#31867;&#25152;&#29702;&#35299;&#30340;&#28508;&#22312;&#31354;&#38388;&#34920;&#31034;&#19978;&#36816;&#34892;&#65292;&#36825;&#20351;&#24471;&#35780;&#20272;&#21644;&#27604;&#36739;&#19981;&#21516;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#21464;&#24471;&#22256;&#38590;&#12290;&#36825;&#31181;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#38459;&#30861;&#20102;&#24320;&#21457;&#21644;ad
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00923v1 Announce Type: cross  Abstract: The problem of search relevance in the E-commerce domain is a challenging one since it involves understanding the intent of a user's short nuanced query and matching it with the appropriate products in the catalog. This problem has traditionally been addressed using language models (LMs) and graph neural networks (GNNs) to capture semantic and inter-product behavior signals, respectively. However, the rapid development of new architectures has created a gap between research and the practical adoption of these techniques. Evaluating the generalizability of these models for deployment requires extensive experimentation on complex, real-world datasets, which can be non-trivial and expensive. Furthermore, such models often operate on latent space representations that are incomprehensible to humans, making it difficult to evaluate and compare the effectiveness of different models. This lack of interpretability hinders the development and ad
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22810;&#37325;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#26377;&#25928;&#22320;&#32467;&#21512;&#20102;&#22522;&#20110;&#24207;&#21015;&#21644;&#22522;&#20110;&#22270;&#30340;&#25512;&#33616;&#26041;&#27861;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#25512;&#33616;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.00895</link><description>&lt;p&gt;
&#31934;&#30830;&#25512;&#33616;&#30340;&#31471;&#21040;&#31471;&#22270;-&#24207;&#21015;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
End-to-end Graph-Sequential Representation Learning for Accurate Recommendations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00895
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22810;&#37325;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#26377;&#25928;&#22320;&#32467;&#21512;&#20102;&#22522;&#20110;&#24207;&#21015;&#21644;&#22522;&#20110;&#22270;&#30340;&#25512;&#33616;&#26041;&#27861;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#25512;&#33616;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#25512;&#33616;&#31995;&#32479;&#30340;&#35768;&#22810;&#26032;&#36827;&#23637;&#38598;&#20013;&#22312;&#24320;&#21457;&#22522;&#20110;&#24207;&#21015;&#21644;&#22522;&#20110;&#22270;&#30340;&#26041;&#27861;&#19978;&#12290;&#36825;&#20004;&#31181;&#26041;&#27861;&#22312;&#24314;&#27169;&#34892;&#20026;&#25968;&#25454;&#20013;&#30340;&#22797;&#26434;&#20851;&#31995;&#26041;&#38754;&#37117;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#65292;&#20174;&#32780;&#22312;&#20010;&#24615;&#21270;&#25490;&#21517;&#21644;&#19979;&#19968;&#20010;&#25512;&#33616;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26377;&#30410;&#30340;&#25104;&#26524;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#33391;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20174;&#25968;&#25454;&#20013;&#25429;&#25417;&#21040;&#30340;&#20449;&#21495;&#25130;&#28982;&#19981;&#21516;&#12290;&#21069;&#32773;&#30452;&#25509;&#36890;&#36807;&#19982;&#26368;&#36817;&#29289;&#21697;&#30340;&#26377;&#24207;&#20132;&#20114;&#26469;&#34920;&#31034;&#29992;&#25143;&#65292;&#32780;&#21518;&#32773;&#26088;&#22312;&#25429;&#25417;&#20132;&#20114;&#22270;&#20013;&#30340;&#38388;&#25509;&#20381;&#36182;&#20851;&#31995;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22810;&#37325;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#36825;&#20004;&#31181;&#33539;&#24335;&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;&#21033;&#29992;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#30456;&#20114;&#35757;&#32451;&#24207;&#21015;&#21644;&#22270;&#32452;&#20214;&#26174;&#33879;&#25913;&#21892;&#20102;&#25512;&#33616;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00895v1 Announce Type: cross  Abstract: Many recent advancements in recommender systems have focused on developing sequence-based and graph-based approaches. Both approaches proved useful in modeling intricate relationships within behavioral data, leading to promising outcomes in personalized ranking and next-item recommendation tasks while maintaining good scalability. However, they capture very different signals from data. While the former approach represents users directly through ordered interactions with recent items, the latter one aims to capture indirect dependencies across the interactions graph. This paper presents a novel multi-representational learning framework that exploits the synergies between these two paradigms. Our empirical evaluation on several datasets demonstrates that mutual training of sequential and graph components with the proposed framework significantly improves recommendations performance.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;LLMs&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#20351;&#29992;ChatGPT-3.5&#12289;GoogleBard&#21644;GoogleGemini&#23545;&#21015;&#26631;&#39064;&#36827;&#34892;&#20027;&#39064;&#27880;&#37322;&#30340;&#20803;&#25968;&#25454;&#20016;&#23500;&#21270;&#65292;&#30740;&#31350;&#23427;&#20204;&#22312;&#23545;&#39046;&#22495;&#29305;&#23450;&#20027;&#39064;&#36827;&#34892;&#20998;&#31867;&#30340;&#33021;&#21147;&#65292;&#24182;&#35780;&#20272;&#20854;&#20869;&#37096;&#19968;&#33268;&#24615;&#12289;&#26426;&#22120;&#23545;&#40784;&#24615;&#21644;&#20154;&#26426;&#19968;&#33268;&#24615;&#12290;ChatGPT&#21644;GoogleGemini&#22312;&#20869;&#37096;&#19968;&#33268;&#24615;&#20197;&#21450;&#19982;&#20154;&#19968;&#33268;&#24615;&#26041;&#38754;&#20248;&#20110;GoogleBard&#12290;</title><link>https://arxiv.org/abs/2403.00884</link><description>&lt;p&gt;
&#21033;&#29992;LLMs&#36827;&#34892;&#20803;&#25968;&#25454;&#20016;&#23500;&#21270;&#30340;&#21463;&#25511;&#35789;&#27719;&#21015;&#26631;&#39064;&#25991;&#26412;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Text classification of column headers with a controlled vocabulary: leveraging LLMs for metadata enrichment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00884
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;LLMs&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#20351;&#29992;ChatGPT-3.5&#12289;GoogleBard&#21644;GoogleGemini&#23545;&#21015;&#26631;&#39064;&#36827;&#34892;&#20027;&#39064;&#27880;&#37322;&#30340;&#20803;&#25968;&#25454;&#20016;&#23500;&#21270;&#65292;&#30740;&#31350;&#23427;&#20204;&#22312;&#23545;&#39046;&#22495;&#29305;&#23450;&#20027;&#39064;&#36827;&#34892;&#20998;&#31867;&#30340;&#33021;&#21147;&#65292;&#24182;&#35780;&#20272;&#20854;&#20869;&#37096;&#19968;&#33268;&#24615;&#12289;&#26426;&#22120;&#23545;&#40784;&#24615;&#21644;&#20154;&#26426;&#19968;&#33268;&#24615;&#12290;ChatGPT&#21644;GoogleGemini&#22312;&#20869;&#37096;&#19968;&#33268;&#24615;&#20197;&#21450;&#19982;&#20154;&#19968;&#33268;&#24615;&#26041;&#38754;&#20248;&#20110;GoogleBard&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#25968;&#25454;&#38598;&#26816;&#32034;&#31995;&#32479;&#20027;&#35201;&#22312;&#20803;&#25968;&#25454;&#20449;&#24687;&#32780;&#38750;&#25968;&#25454;&#20540;&#19978;&#24314;&#31435;&#32034;&#24341;&#12290;&#22240;&#27492;&#20027;&#35201;&#20381;&#36182;&#20110;&#25163;&#21160;&#27880;&#37322;&#21644;&#39640;&#36136;&#37327;&#30340;&#20803;&#25968;&#25454;&#65292;&#36825;&#20123;&#36807;&#31243;&#34987;&#35748;&#20026;&#26159;&#32791;&#26102;&#19988;&#38590;&#20197;&#33258;&#21160;&#21270;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21033;&#29992;&#19977;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25903;&#25345;&#23545;&#21015;&#26631;&#39064;&#36827;&#34892;&#20027;&#39064;&#27880;&#37322;&#30340;&#20803;&#25968;&#25454;&#20016;&#23500;&#21270;&#65306;ChatGPT-3.5&#12289;GoogleBard&#21644;GoogleGemini&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;LLMs&#22522;&#20110;&#21463;&#25511;&#35789;&#27719;&#30340;&#39046;&#22495;&#29305;&#23450;&#20027;&#39064;&#23545;&#21015;&#26631;&#39064;&#36827;&#34892;&#20998;&#31867;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#35780;&#20272;LLMs&#30340;&#20869;&#37096;&#19968;&#33268;&#24615;&#12289;&#26426;&#22120;&#38388;&#23545;&#40784;&#20197;&#21450;&#20154;&#26426;&#23545;&#20027;&#39064;&#20998;&#31867;&#20219;&#21153;&#30340;&#19968;&#33268;&#24615;&#26469;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#19978;&#19979;&#25991;&#20449;&#24687;&#65288;&#21363;&#25968;&#25454;&#38598;&#25551;&#36848;&#65289;&#23545;&#20998;&#31867;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;ChatGPT&#21644;GoogleGemini&#22312;&#20869;&#37096;&#19968;&#33268;&#24615;&#20197;&#21450;LLM&#19982;&#20154;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;GoogleBard&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00884v1 Announce Type: cross  Abstract: Traditional dataset retrieval systems index on metadata information rather than on the data values. Thus relying primarily on manual annotations and high-quality metadata, processes known to be labour-intensive and challenging to automate. We propose a method to support metadata enrichment with topic annotations of column headers using three Large Language Models (LLMs): ChatGPT-3.5, GoogleBard and GoogleGemini. We investigate the LLMs ability to classify column headers based on domain-specific topics from a controlled vocabulary. We evaluate our approach by assessing the internal consistency of the LLMs, the inter-machine alignment, and the human-machine agreement for the topic classification task. Additionally, we investigate the impact of contextual information (i.e. dataset description) on the classification outcomes. Our results suggest that ChatGPT and GoogleGemini outperform GoogleBard for internal consistency as well as LLM-hum
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;DGMed&#26694;&#26550;&#65292;&#21033;&#29992;&#22240;&#26524;&#25512;&#26029;&#21644;&#21019;&#26032;&#30340;&#29305;&#24449;&#23545;&#40784;&#26041;&#27861;&#36827;&#34892;&#21452;&#31890;&#24230;&#33647;&#29289;&#25512;&#33616;</title><link>https://arxiv.org/abs/2403.00880</link><description>&lt;p&gt;
&#22522;&#20110;&#22240;&#26524;&#25512;&#26029;&#30340;&#21452;&#31890;&#24230;&#33647;&#29289;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Dual-Granularity Medication Recommendation Based on Causal Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00880
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;DGMed&#26694;&#26550;&#65292;&#21033;&#29992;&#22240;&#26524;&#25512;&#26029;&#21644;&#21019;&#26032;&#30340;&#29305;&#24449;&#23545;&#40784;&#26041;&#27861;&#36827;&#34892;&#21452;&#31890;&#24230;&#33647;&#29289;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#21307;&#30103;&#38656;&#27714;&#22686;&#38271;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#36827;&#27493;&#65292;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#35786;&#26029;&#21644;&#27835;&#30103;&#31995;&#32479;&#22791;&#21463;&#20851;&#27880;&#12290;&#33647;&#29289;&#25512;&#33616;&#26088;&#22312;&#23558;&#24739;&#32773;&#30340;&#38271;&#26399;&#20581;&#24247;&#35760;&#24405;&#19982;&#21307;&#23398;&#30693;&#35782;&#25972;&#21512;&#65292;&#20026;&#29305;&#23450;&#30142;&#30149;&#25512;&#33616;&#20934;&#30830;&#21644;&#23433;&#20840;&#30340;&#33647;&#29289;&#32452;&#21512;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30740;&#31350;&#23558;&#33647;&#29289;&#25512;&#33616;&#31995;&#32479;&#20165;&#35270;&#20026;&#20256;&#32479;&#25512;&#33616;&#31995;&#32479;&#30340;&#21464;&#20307;&#65292;&#24573;&#35270;&#20102;&#33647;&#29289;&#21644;&#30142;&#30149;&#20043;&#38388;&#30340;&#24322;&#36136;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DGMed&#65292;&#19968;&#20010;&#29992;&#20110;&#33647;&#29289;&#25512;&#33616;&#30340;&#26694;&#26550;&#12290;DGMed&#21033;&#29992;&#22240;&#26524;&#25512;&#26029;&#25581;&#31034;&#21307;&#23398;&#23454;&#20307;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#29305;&#24449;&#23545;&#40784;&#26041;&#27861;&#26469;&#35299;&#20915;&#24322;&#36136;&#24615;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35813;&#30740;&#31350;&#39318;&#20808;&#24212;&#29992;&#22240;&#26524;&#25512;&#26029;&#20998;&#26512;&#21382;&#21490;&#35760;&#24405;&#20013;&#33647;&#29289;&#23545;&#29305;&#23450;&#30142;&#30149;&#30340;&#37327;&#21270;&#27835;&#30103;&#25928;&#26524;&#65292;&#25581;&#31034;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00880v1 Announce Type: cross  Abstract: As medical demands grow and machine learning technology advances, AI-based diagnostic and treatment systems are garnering increasing attention. Medication recommendation aims to integrate patients' long-term health records with medical knowledge, recommending accuracy and safe medication combinations for specific conditions. However, most existing researches treat medication recommendation systems merely as variants of traditional recommendation systems, overlooking the heterogeneity between medications and diseases. To address this challenge, we propose DGMed, a framework for medication recommendation. DGMed utilizes causal inference to uncover the connections among medical entities and presents an innovative feature alignment method to tackle heterogeneity issues. Specifically, this study first applies causal inference to analyze the quantified therapeutic effects of medications on specific diseases from historical records, uncoverin
&lt;/p&gt;</description></item><item><title>Disaggregated Multi-Tower&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#25299;&#25169;&#24863;&#30693;&#30340;&#24314;&#27169;&#25216;&#26415;&#65292;&#36890;&#36807;SPTT&#12289;TM&#21644;TP&#19977;&#20010;&#32452;&#20214;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#22823;&#35268;&#27169;&#25512;&#33616;&#65292;&#21152;&#36895;&#24615;&#33021;&#25552;&#21319;&#20102;1.9&#20493;&#12290;</title><link>https://arxiv.org/abs/2403.00877</link><description>&lt;p&gt;
Disaggregated Multi-Tower: &#38754;&#21521;&#25299;&#25169;&#24863;&#30693;&#30340;&#39640;&#25928;&#22823;&#35268;&#27169;&#25512;&#33616;&#24314;&#27169;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Disaggregated Multi-Tower: Topology-aware Modeling Technique for Efficient Large-Scale Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00877
&lt;/p&gt;
&lt;p&gt;
Disaggregated Multi-Tower&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#25299;&#25169;&#24863;&#30693;&#30340;&#24314;&#27169;&#25216;&#26415;&#65292;&#36890;&#36807;SPTT&#12289;TM&#21644;TP&#19977;&#20010;&#32452;&#20214;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#22823;&#35268;&#27169;&#25512;&#33616;&#65292;&#21152;&#36895;&#24615;&#33021;&#25552;&#21319;&#20102;1.9&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#28145;&#24230;&#23398;&#20064;&#25512;&#33616;&#27169;&#22411;&#30340;&#25153;&#24179;&#26550;&#26500;&#12289;&#24120;&#35265;&#30340;&#20998;&#24067;&#24335;&#35757;&#32451;&#27169;&#24335;&#21644;&#20998;&#23618;&#25968;&#25454;&#20013;&#24515;&#25299;&#25169;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#12290;&#20026;&#20102;&#35299;&#20915;&#30456;&#20851;&#30340;&#20302;&#25928;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Disaggregated Multi-Tower&#65288;DMT&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#24314;&#27169;&#25216;&#26415;&#65292;&#21253;&#25324;&#65288;1&#65289;&#35821;&#20041;&#20445;&#30041;&#30340;Tower Transform&#65288;SPTT&#65289;&#65292;&#19968;&#20010;&#23558;&#21333;&#29255;&#20840;&#23616;&#23884;&#20837;&#26597;&#25214;&#36807;&#31243;&#20998;&#35299;&#20026;&#19981;&#30456;&#20132;&#22612;&#20197;&#21033;&#29992;&#25968;&#25454;&#20013;&#24515;&#20301;&#32622;&#20851;&#31995;&#30340;&#26032;&#22411;&#35757;&#32451;&#27169;&#24335;&#65307;&#65288;2&#65289;Tower Module&#65288;TM&#65289;&#65292;&#19968;&#20010;&#38468;&#21152;&#21040;&#27599;&#20010;&#22612;&#30340;&#21327;&#21516;&#31264;&#23494;&#32452;&#20214;&#65292;&#36890;&#36807;&#20998;&#23618;&#29305;&#24449;&#20132;&#20114;&#38477;&#20302;&#27169;&#22411;&#22797;&#26434;&#24615;&#21644;&#36890;&#20449;&#37327;&#65307;&#21644;&#65288;3&#65289;Tower Partitioner&#65288;TP&#65289;&#65292;&#19968;&#20010;&#29305;&#24449;&#20998;&#21306;&#22120;&#65292;&#31995;&#32479;&#22320;&#21019;&#24314;&#20855;&#26377;&#26377;&#24847;&#20041;&#29305;&#24449;&#20132;&#20114;&#21644;&#36127;&#36733;&#24179;&#34913;&#20998;&#37197;&#30340;&#22612;&#65292;&#36890;&#36807;&#23398;&#20064;&#30340;&#23884;&#20837;&#26469;&#20445;&#25345;&#27169;&#22411;&#36136;&#37327;&#21644;&#35757;&#32451;&#21534;&#21520;&#37327;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;DMT&#30456;&#27604;&#20110;&#26368;&#26032;&#30340;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#39640;&#36798;1.9&#20493;&#30340;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00877v1 Announce Type: new  Abstract: We study a mismatch between the deep learning recommendation models' flat architecture, common distributed training paradigm and hierarchical data center topology. To address the associated inefficiencies, we propose Disaggregated Multi-Tower (DMT), a modeling technique that consists of (1) Semantic-preserving Tower Transform (SPTT), a novel training paradigm that decomposes the monolithic global embedding lookup process into disjoint towers to exploit data center locality; (2) Tower Module (TM), a synergistic dense component attached to each tower to reduce model complexity and communication volume through hierarchical feature interaction; and (3) Tower Partitioner (TP), a feature partitioner to systematically create towers with meaningful feature interactions and load balanced assignments to preserve model quality and training throughput via learned embeddings. We show that DMT can achieve up to 1.9x speedup compared to the state-of-th
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LLM-ensemble&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#38598;&#25104;&#19981;&#21516;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#25552;&#39640;&#30005;&#23376;&#21830;&#21153;&#20135;&#21697;&#23646;&#24615;&#20540;&#25552;&#21462;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.00863</link><description>&lt;p&gt;
LLM-Ensemble: &#29992;&#20110;&#30005;&#23376;&#21830;&#21153;&#20135;&#21697;&#23646;&#24615;&#20540;&#25552;&#21462;&#30340;&#26368;&#20339;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
LLM-Ensemble: Optimal Large Language Model Ensemble Method for E-commerce Product Attribute Value Extraction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00863
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LLM-ensemble&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#38598;&#25104;&#19981;&#21516;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#25552;&#39640;&#30005;&#23376;&#21830;&#21153;&#20135;&#21697;&#23646;&#24615;&#20540;&#25552;&#21462;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00863v1 &#20844;&#21578;&#31867;&#22411;:&#36328;&#39046;&#22495;&#25688;&#35201;: &#20135;&#21697;&#23646;&#24615;&#20540;&#25552;&#21462;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#24403;&#20195;&#30005;&#23376;&#21830;&#21153;&#34892;&#19994;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#32452;&#25104;&#37096;&#20998;&#12290;&#25552;&#20379;&#31934;&#30830;&#30340;&#20135;&#21697;&#23646;&#24615;&#20540;&#22312;&#30830;&#20445;&#39640;&#36136;&#37327;&#25512;&#33616;&#21644;&#25552;&#21319;&#23458;&#25143;&#28385;&#24847;&#24230;&#26041;&#38754;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#20986;&#29616;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35768;&#22810;&#23646;&#24615;&#25552;&#21462;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#39046;&#22495;&#29305;&#23450;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25968;&#25454;&#12289;&#26550;&#26500;&#21644;&#36229;&#21442;&#25968;&#30340;&#22810;&#26679;&#24615;&#65292;&#19981;&#21516;LLMs&#34920;&#29616;&#20986;&#19981;&#21516;&#30340;&#20248;&#21183;&#21644;&#21155;&#21183;&#12290;&#36825;&#31181;&#21464;&#21270;&#20351;&#23427;&#20204;&#24444;&#27492;&#20114;&#34917;&#65292;&#27809;&#26377;&#21738;&#20010;LLM&#33021;&#23436;&#20840;&#21387;&#20498;&#20854;&#20182;LLM&#12290;&#32771;&#34385;&#21040;LLMs&#30340;&#22810;&#26679;&#20248;&#21183;&#21644;&#21155;&#21183;&#65292;&#24320;&#21457;&#19968;&#31181;&#21033;&#29992;&#23427;&#20204;&#20114;&#34917;&#28508;&#21147;&#30340;&#38598;&#25104;&#26041;&#27861;&#21464;&#24471;&#24517;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LLM-ensemble&#30340;&#26032;&#31639;&#27861;&#65292;&#29992;&#20110;&#38598;&#25104;&#19981;&#21516;LLMs&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00863v1 Announce Type: cross  Abstract: Product attribute value extraction is a pivotal component in Natural Language Processing (NLP) and the contemporary e-commerce industry. The provision of precise product attribute values is fundamental in ensuring high-quality recommendations and enhancing customer satisfaction. The recently emerging Large Language Models (LLMs) have demonstrated state-of-the-art performance in numerous attribute extraction tasks, without the need for domain-specific training data. Nevertheless, varying strengths and weaknesses are exhibited by different LLMs due to the diversity in data, architectures, and hyperparameters. This variation makes them complementary to each other, with no single LLM dominating all others. Considering the diverse strengths and weaknesses of LLMs, it becomes necessary to develop an ensemble method that leverages their complementary potentials. In this paper, we propose a novel algorithm called LLM-ensemble to ensemble diffe
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#33268;&#21147;&#20110;&#35299;&#20915;&#24191;&#21578;&#25293;&#21334;&#20013;&#30340;&#22312;&#32447;&#23398;&#20064;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#38024;&#23545;&#24191;&#21578;&#20027;&#20004;&#31181;&#19981;&#21516;&#25112;&#30053;&#34892;&#20026;&#27169;&#22411;&#30340;&#22312;&#32447;&#26426;&#21046;&#65292;&#20854;&#20013;&#19968;&#31181;&#22312;&#26368;&#22351;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#32039;&#33268;&#30340;&#36951;&#25022;&#30028;&#38480;&#65292;&#21478;&#19968;&#31181;&#21017;&#35299;&#20915;&#20102;&#26356;&#22797;&#26434;&#30340;&#38271;&#26399;&#25928;&#29992;&#20248;&#21270;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.00845</link><description>&lt;p&gt;
&#25913;&#36827;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#29992;&#20110;&#24191;&#21578;&#25293;&#21334;&#20013;&#30340;&#28857;&#20987;&#29575;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Improved Online Learning Algorithms for CTR Prediction in Ad Auctions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00845
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#33268;&#21147;&#20110;&#35299;&#20915;&#24191;&#21578;&#25293;&#21334;&#20013;&#30340;&#22312;&#32447;&#23398;&#20064;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#38024;&#23545;&#24191;&#21578;&#20027;&#20004;&#31181;&#19981;&#21516;&#25112;&#30053;&#34892;&#20026;&#27169;&#22411;&#30340;&#22312;&#32447;&#26426;&#21046;&#65292;&#20854;&#20013;&#19968;&#31181;&#22312;&#26368;&#22351;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#32039;&#33268;&#30340;&#36951;&#25022;&#30028;&#38480;&#65292;&#21478;&#19968;&#31181;&#21017;&#35299;&#20915;&#20102;&#26356;&#22797;&#26434;&#30340;&#38271;&#26399;&#25928;&#29992;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24191;&#21578;&#25293;&#21334;&#20013;&#25910;&#20837;&#26368;&#22823;&#21270;&#30340;&#22312;&#32447;&#23398;&#20064;&#38382;&#39064;&#65292;&#20854;&#20013;&#21334;&#26041;&#38656;&#35201;&#23398;&#20064;&#27599;&#20010;&#24191;&#21578;&#20505;&#36873;&#30340;&#28857;&#20987;&#29575;(CTR)&#65292;&#24182;&#36890;&#36807;&#25353;&#28857;&#20987;&#20184;&#36153;&#30340;&#26041;&#24335;&#25910;&#21462;&#33719;&#32988;&#32773;&#30340;&#20215;&#26684;&#12290;&#25105;&#20204;&#20851;&#27880;&#24191;&#21578;&#20027;&#30340;&#20004;&#31181;&#25112;&#30053;&#34892;&#20026;&#27169;&#22411;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20551;&#35774;&#24191;&#21578;&#20027;&#23436;&#20840;&#36817;&#35270;&#65307;&#21363;&#22312;&#27599;&#19968;&#36718;&#20013;&#65292;&#20182;&#20204;&#21482;&#38024;&#23545;&#24403;&#21069;&#36718;&#27425;&#26368;&#22823;&#21270;&#20182;&#20204;&#30340;&#25928;&#29992;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#22522;&#20110;&#32622;&#20449;&#19978;&#30028;&#24320;&#21457;&#20102;&#19968;&#20010;&#22312;&#32447;&#26426;&#21046;&#65292;&#22312;&#26368;&#22351;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#20005;&#26684;&#30340;$O(\sqrt{T})$&#36951;&#25022;&#65292;&#24403;&#20540;&#22312;&#25152;&#26377;&#25293;&#21334;&#20013;&#38745;&#24577;&#24182;&#19988;&#26368;&#39640;&#26399;&#26395;&#20540;(&#21363;&#20540;&#20056;&#20197;&#20182;&#20204;&#30340;CTR)&#19982;&#27425;&#39640;&#26399;&#26395;&#20540;&#24191;&#21578;&#20043;&#38388;&#23384;&#22312;&#24046;&#36317;&#26102;&#65292;&#23384;&#22312;&#36127;&#36951;&#25022;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#20551;&#35774;&#24191;&#21578;&#20027;&#26159;&#38750;&#36817;&#35270;&#30340;&#65292;&#24182;&#20851;&#24515;&#20182;&#20204;&#30340;&#38271;&#26399;&#25928;&#29992;&#12290;&#36825;&#31181;&#35774;&#32622;&#35201;&#22797;&#26434;&#24471;&#22810;&#65292;&#22240;&#20026;&#24191;&#21578;&#20027;&#26377;&#21160;&#26426;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00845v1 Announce Type: cross  Abstract: In this work, we investigate the online learning problem of revenue maximization in ad auctions, where the seller needs to learn the click-through rates (CTRs) of each ad candidate and charge the price of the winner through a pay-per-click manner. We focus on two models of the advertisers' strategic behaviors. First, we assume that the advertiser is completely myopic; i.e.~in each round, they aim to maximize their utility only for the current round. In this setting, we develop an online mechanism based on upper-confidence bounds that achieves a tight $O(\sqrt{T})$ regret in the worst-case and negative regret when the values are static across all the auctions and there is a gap between the highest expected value (i.e.~value multiplied by their CTR) and second highest expected value ad. Next, we assume that the advertiser is non-myopic and cares about their long term utility. This setting is much more complex since an advertiser is incen
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20248;&#21270;&#25351;&#26631;Lower-Left Partial AUC&#65288;LLPAUC&#65289;&#65292;&#22312;&#35745;&#31639;&#25928;&#29575;&#19978;&#31867;&#20284;&#20110;AUC&#65292;&#20294;&#19982;Top-K&#25490;&#21517;&#25351;&#26631;&#24378;&#30456;&#20851;&#65292;&#33021;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#26377;&#25928;&#25552;&#21319;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.00844</link><description>&lt;p&gt;
&#19979;-&#24038;&#37096;&#20998;AUC&#65306;&#19968;&#31181;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#30340;&#26377;&#25928;&#21644;&#39640;&#25928;&#20248;&#21270;&#25351;&#26631;
&lt;/p&gt;
&lt;p&gt;
Lower-Left Partial AUC: An Effective and Efficient Optimization Metric for Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00844
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20248;&#21270;&#25351;&#26631;Lower-Left Partial AUC&#65288;LLPAUC&#65289;&#65292;&#22312;&#35745;&#31639;&#25928;&#29575;&#19978;&#31867;&#20284;&#20110;AUC&#65292;&#20294;&#19982;Top-K&#25490;&#21517;&#25351;&#26631;&#24378;&#30456;&#20851;&#65292;&#33021;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#26377;&#25928;&#25552;&#21319;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20248;&#21270;&#25351;&#26631;&#23545;&#20110;&#26500;&#24314;&#22823;&#35268;&#27169;&#25512;&#33616;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#19968;&#31181;&#23454;&#29992;&#30340;&#26377;&#25928;&#21644;&#39640;&#25928;&#25351;&#26631;&#20173;&#28982;&#38590;&#20197;&#25214;&#21040;&#12290;&#23613;&#31649;Top-K&#25490;&#21517;&#25351;&#26631;&#26159;&#20248;&#21270;&#30340;&#40644;&#37329;&#26631;&#20934;&#65292;&#20294;&#23427;&#20204;&#23384;&#22312;&#30528;&#26174;&#30528;&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#26356;&#39640;&#25928;&#30340;&#20934;&#30830;&#24615;&#21644;AUC&#25351;&#26631;&#24448;&#24448;&#26080;&#27861;&#25429;&#25417;&#25512;&#33616;&#20219;&#21153;&#30340;&#30495;&#27491;&#30446;&#26631;&#65292;&#23548;&#33268;&#24615;&#33021;&#20122;&#20248;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#22256;&#22659;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20248;&#21270;&#25351;&#26631;&#65292;Lower-Left Partial AUC&#65288;LLPAUC&#65289;&#65292;&#23427;&#22312;&#35745;&#31639;&#25928;&#29575;&#19978;&#31867;&#20284;&#20110;AUC&#65292;&#20294;&#19982;Top-K&#25490;&#21517;&#25351;&#26631;&#24378;&#30456;&#20851;&#12290;&#19982;AUC&#30456;&#27604;&#65292;LLPAUC&#20165;&#32771;&#34385;ROC&#26354;&#32447;&#19979;&#26041;&#30340;&#23616;&#37096;&#21306;&#22495;&#65292;&#20197;&#23558;&#20248;&#21270;&#28966;&#28857;&#25918;&#22312;Top-K&#19978;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;LLPAUC&#19982;Top-K&#25490;&#21517;&#25351;&#26631;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#30340;&#29702;&#35770;&#39564;&#35777;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#23545;&#22024;&#26434;&#29992;&#25143;&#21453;&#39304;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00844v1 Announce Type: cross  Abstract: Optimization metrics are crucial for building recommendation systems at scale. However, an effective and efficient metric for practical use remains elusive. While Top-K ranking metrics are the gold standard for optimization, they suffer from significant computational overhead. Alternatively, the more efficient accuracy and AUC metrics often fall short of capturing the true targets of recommendation tasks, leading to suboptimal performance. To overcome this dilemma, we propose a new optimization metric, Lower-Left Partial AUC (LLPAUC), which is computationally efficient like AUC but strongly correlates with Top-K ranking metrics. Compared to AUC, LLPAUC considers only the partial area under the ROC curve in the Lower-Left corner to push the optimization focus on Top-K. We provide theoretical validation of the correlation between LLPAUC and Top-K ranking metrics and demonstrate its robustness to noisy user feedback. We further design an 
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35268;&#21010;&#33021;&#21147;&#26469;&#22686;&#24378;&#38271;&#26399;&#25512;&#33616;&#65292;&#20351;&#27169;&#22411;&#22312;&#20010;&#24615;&#21270;&#25512;&#33616;&#20013;&#26356;&#26377;&#25928;&#22320;&#29702;&#35299;&#21644;&#24212;&#29992;&#20219;&#21153;&#35299;&#20915;&#21407;&#21017;</title><link>https://arxiv.org/abs/2403.00843</link><description>&lt;p&gt;
&#21033;&#29992;&#21452;&#23618;&#21487;&#23398;&#20064;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35268;&#21010;&#22686;&#24378;&#38271;&#26399;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Enhancing Long-Term Recommendation with Bi-level Learnable Large Language Model Planning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00843
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35268;&#21010;&#33021;&#21147;&#26469;&#22686;&#24378;&#38271;&#26399;&#25512;&#33616;&#65292;&#20351;&#27169;&#22411;&#22312;&#20010;&#24615;&#21270;&#25512;&#33616;&#20013;&#26356;&#26377;&#25928;&#22320;&#29702;&#35299;&#21644;&#24212;&#29992;&#20219;&#21153;&#35299;&#20915;&#21407;&#21017;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#25512;&#33616;&#31995;&#32479;&#20542;&#21521;&#20110;&#36807;&#20998;&#36814;&#21512;&#29992;&#25143;&#30340;&#21363;&#26102;&#20852;&#36259;&#32780;&#24573;&#35270;&#20182;&#20204;&#30340;&#38271;&#26399;&#21442;&#19982;&#12290; &#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22312;&#25512;&#33616;&#20915;&#31574;&#36807;&#31243;&#20013;&#21512;&#24182;&#35268;&#21010;&#33021;&#21147;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#20197;&#24320;&#21457;&#33021;&#22815;&#21516;&#26102;&#32771;&#34385;&#21363;&#26102;&#20852;&#36259;&#21644;&#38271;&#26399;&#21442;&#19982;&#30340;&#31574;&#30053;&#12290;&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#31232;&#30095;&#25968;&#25454;&#30340;&#26174;&#33879;&#35268;&#21010;&#33021;&#21147;&#29992;&#20110;&#38271;&#26399;&#25512;&#33616;&#12290;&#20851;&#38190;&#22312;&#20110;&#20351;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#22312;&#20010;&#24615;&#21270;&#25512;&#33616;&#22330;&#26223;&#20013;&#26377;&#25928;&#29702;&#35299;&#21644;&#24212;&#29992;&#20219;&#21153;&#35299;&#20915;&#21407;&#21017;&#65292;&#22240;&#20026;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#21487;&#33021;&#24182;&#26410;&#33258;&#28982;&#21253;&#21547;&#36825;&#20123;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00843v1 Announce Type: cross  Abstract: Traditional recommendation setting tends to excessively cater to users' immediate interests and neglect their long-term engagement. To address it, it is crucial to incorporate planning capabilities into the recommendation decision-making process to develop policies that take into account both immediate interests and long-term engagement. Despite Reinforcement Learning (RL) can learn planning capacity by maximizing cumulative reward, the scarcity of recommendation data presents challenges such as instability and susceptibility to overfitting when training RL models from scratch.   In this context, we propose to leverage the remarkable planning capabilities over sparse data of Large Language Models (LLMs) for long-term recommendation. The key lies in enabling a language model to understand and apply task-solving principles effectively in personalized recommendation scenarios, as the model's pre-training may not naturally encompass these 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36335;&#24452;&#25512;&#29702;&#30340;&#27867;&#21270;&#23618;&#27425;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#25552;&#39640;&#20102;&#22522;&#20110;&#20250;&#35805;&#30340;&#25512;&#33616;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#35774;&#35745;&#20102;&#22810;&#30446;&#26631;&#22870;&#21169;&#26426;&#21046;&#21644;&#36335;&#24452;&#20013;&#38388;&#28857;&#22870;&#21169;&#20197;&#24212;&#23545;&#39034;&#24207;&#27169;&#24335;&#30340;&#36339;&#36807;&#34892;&#20026;&#21644;&#22686;&#24378;&#30693;&#35782;&#22270;&#30340;&#25506;&#32034;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.00832</link><description>&lt;p&gt;
&#36890;&#36807;&#36335;&#24452;&#25512;&#29702;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#22522;&#20110;&#20250;&#35805;&#30340;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Explainable Session-based Recommendation via Path Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00832
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36335;&#24452;&#25512;&#29702;&#30340;&#27867;&#21270;&#23618;&#27425;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#25552;&#39640;&#20102;&#22522;&#20110;&#20250;&#35805;&#30340;&#25512;&#33616;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#35774;&#35745;&#20102;&#22810;&#30446;&#26631;&#22870;&#21169;&#26426;&#21046;&#21644;&#36335;&#24452;&#20013;&#38388;&#28857;&#22870;&#21169;&#20197;&#24212;&#23545;&#39034;&#24207;&#27169;&#24335;&#30340;&#36339;&#36807;&#34892;&#20026;&#21644;&#22686;&#24378;&#30693;&#35782;&#22270;&#30340;&#25506;&#32034;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#36890;&#36807;&#36335;&#24452;&#25512;&#29702;&#20026;&#22522;&#20110;&#20250;&#35805;&#30340;&#25512;&#33616;&#65288;SR&#65289;&#25552;&#20379;&#21487;&#35299;&#37322;&#24615;&#30340;&#26041;&#27861;&#12290;&#29616;&#26377;&#30340;SR&#27169;&#22411;&#24378;&#35843;&#20934;&#30830;&#24615;&#20294;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#65292;&#32780;&#20256;&#32479;&#30340;&#36335;&#24452;&#25512;&#29702;&#20391;&#37325;&#20110;&#30693;&#35782;&#22270;&#25506;&#32034;&#65292;&#24573;&#30053;&#20102;&#20250;&#35805;&#21382;&#21490;&#20013;&#23384;&#22312;&#30340;&#39034;&#24207;&#27169;&#24335;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;SR&#30340;&#27867;&#21270;&#23618;&#27425;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#36335;&#24452;&#25512;&#29702;&#65288;PR4SR&#65289;&#26469;&#25552;&#39640;&#29616;&#26377;SR&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#32771;&#34385;&#21040;&#39033;&#30446;&#23545;&#20250;&#35805;&#30340;&#37325;&#35201;&#24615;&#19981;&#21516;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20250;&#35805;&#32423;&#21035;&#20195;&#29702;&#26469;&#36873;&#25321;&#20250;&#35805;&#20013;&#30340;&#39033;&#30446;&#20316;&#20026;&#36335;&#24452;&#25512;&#29702;&#30340;&#36215;&#28857;&#65292;&#20197;&#21450;&#36335;&#24452;&#32423;&#21035;&#20195;&#29702;&#26469;&#25191;&#34892;&#36335;&#24452;&#25512;&#29702;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#22810;&#30446;&#26631;&#22870;&#21169;&#26426;&#21046;&#26469;&#36866;&#24212;SR&#20013;&#39034;&#24207;&#27169;&#24335;&#30340;&#36339;&#36807;&#34892;&#20026;&#65292;&#24182;&#24341;&#20837;&#36335;&#24452;&#20013;&#38388;&#28857;&#22870;&#21169;&#26469;&#22686;&#24378;&#30693;&#35782;&#22270;&#20013;&#30340;&#25506;&#32034;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00832v1 Announce Type: cross  Abstract: This paper explores providing explainability for session-based recommendation (SR) by path reasoning. Current SR models emphasize accuracy but lack explainability, while traditional path reasoning prioritizes knowledge graph exploration, ignoring sequential patterns present in the session history. Therefore, we propose a generalized hierarchical reinforcement learning framework for SR, which improves the explainability of existing SR models via Path Reasoning, namely PR4SR. Considering the different importance of items to the session, we design the session-level agent to select the items in the session as the starting point for path reasoning and the path-level agent to perform path reasoning. In particular, we design a multi-target reward mechanism to adapt to the skip behaviors of sequential patterns in SR, and introduce path midpoint reward to enhance the exploration efficiency in knowledge graphs. To improve the completeness of the
&lt;/p&gt;</description></item><item><title>InteraRec&#24341;&#20837;&#20102;&#19968;&#31181;&#22797;&#26434;&#30340;&#20132;&#20114;&#24335;&#25512;&#33616;&#26694;&#26550;&#65292;&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#65292;&#23427;&#19981;&#20165;&#20381;&#36182;Weblog&#29983;&#25104;&#25512;&#33616;&#65292;&#36824;&#25429;&#33719;&#29992;&#25143;&#23548;&#33322;&#26102;&#32593;&#39029;&#30340;&#39640;&#39057;&#25130;&#22270;&#12290;</title><link>https://arxiv.org/abs/2403.00822</link><description>&lt;p&gt;
InteraRec&#65306;&#20351;&#29992;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20132;&#20114;&#24335;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
InteraRec: Interactive Recommendations Using Multimodal Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00822
&lt;/p&gt;
&lt;p&gt;
InteraRec&#24341;&#20837;&#20102;&#19968;&#31181;&#22797;&#26434;&#30340;&#20132;&#20114;&#24335;&#25512;&#33616;&#26694;&#26550;&#65292;&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#65292;&#23427;&#19981;&#20165;&#20381;&#36182;Weblog&#29983;&#25104;&#25512;&#33616;&#65292;&#36824;&#25429;&#33719;&#29992;&#25143;&#23548;&#33322;&#26102;&#32593;&#39029;&#30340;&#39640;&#39057;&#25130;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Weblog&#30001;&#35760;&#24405;&#20219;&#20309;&#32593;&#31449;&#19978;&#29992;&#25143;&#27963;&#21160;&#30340;&#35760;&#24405;&#32452;&#25104;&#65292;&#21487;&#20197;&#20026;&#29992;&#25143;&#20559;&#22909;&#12289;&#34892;&#20026;&#21644;&#20852;&#36259;&#25552;&#20379;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;&#35768;&#22810;&#25512;&#33616;&#31639;&#27861;&#21033;&#29992;&#36890;&#36807;&#36825;&#20123;Weblog&#25366;&#25496;&#30340;&#25968;&#25454;&#65292;&#37319;&#29992;&#21327;&#21516;&#36807;&#28388;&#12289;&#22522;&#20110;&#20869;&#23481;&#30340;&#36807;&#28388;&#21644;&#28151;&#21512;&#26041;&#27861;&#31561;&#31574;&#30053;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#20010;&#24615;&#21270;&#25512;&#33616;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;InteraRec&#30340;&#22797;&#26434;&#20132;&#20114;&#24335;&#25512;&#33616;&#26694;&#26550;&#65292;&#23427;&#19981;&#21516;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#21518;&#32773;&#20165;&#20381;&#36182;Weblog&#29983;&#25104;&#25512;&#33616;&#12290;&#35813;&#26694;&#26550;&#25429;&#33719;&#29992;&#25143;&#23548;&#33322;&#26102;&#32593;&#39029;&#30340;&#39640;&#39057;&#25130;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00822v1 Announce Type: cross  Abstract: Weblogs, comprised of records detailing user activities on any website, offer valuable insights into user preferences, behavior, and interests. Numerous recommendation algorithms, employing strategies such as collaborative filtering, content-based filtering, and hybrid methods, leverage the data mined through these weblogs to provide personalized recommendations to users. Despite the abundance of information available in these weblogs, identifying and extracting pertinent information and key features necessitates extensive engineering endeavors. The intricate nature of the data also poses a challenge for interpretation, especially for non-experts. In this study, we introduce a sophisticated and interactive recommendation framework denoted as InteraRec, which diverges from conventional approaches that exclusively depend on weblogs for recommendation generation. This framework captures high-frequency screenshots of web pages as users nav
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20005;&#26684;&#30340;&#25968;&#25454;&#38598;&#21019;&#24314;&#21644;&#35780;&#20272;&#24037;&#20316;&#27969;&#31243;&#65292;&#29992;&#20110;&#37327;&#21270;&#27604;&#36739;&#19981;&#21516;&#30340;RAG&#31574;&#30053;&#65292;&#21516;&#26102;&#24320;&#21457;&#21644;&#35780;&#20272;&#20102;&#19968;&#20010;&#24067;&#23572;&#20195;&#29702;RAG&#35774;&#32622;&#65292;&#20351;&#24471;&#22823;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#33410;&#30465;&#26631;&#35760;&#26469;&#20915;&#23450;&#26159;&#21542;&#26597;&#35810;&#21521;&#37327;&#25968;&#25454;&#24211;&#12290;</title><link>https://arxiv.org/abs/2403.00820</link><description>&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#31995;&#32479;&#65306;&#33258;&#21160;&#25968;&#25454;&#38598;&#21019;&#24314;&#65292;&#35780;&#20272;&#21644;&#24067;&#23572;&#20195;&#29702;&#35774;&#32622;
&lt;/p&gt;
&lt;p&gt;
Retrieval Augmented Generation Systems: Automatic Dataset Creation, Evaluation and Boolean Agent Setup
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00820
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20005;&#26684;&#30340;&#25968;&#25454;&#38598;&#21019;&#24314;&#21644;&#35780;&#20272;&#24037;&#20316;&#27969;&#31243;&#65292;&#29992;&#20110;&#37327;&#21270;&#27604;&#36739;&#19981;&#21516;&#30340;RAG&#31574;&#30053;&#65292;&#21516;&#26102;&#24320;&#21457;&#21644;&#35780;&#20272;&#20102;&#19968;&#20010;&#24067;&#23572;&#20195;&#29702;RAG&#35774;&#32622;&#65292;&#20351;&#24471;&#22823;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#33410;&#30465;&#26631;&#35760;&#26469;&#20915;&#23450;&#26159;&#21542;&#26597;&#35810;&#21521;&#37327;&#25968;&#25454;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#31995;&#32479;&#22312;&#22686;&#24378;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36755;&#20986;&#20013;&#19982;&#39046;&#22495;&#29305;&#23450;&#21644;&#26102;&#38388;&#25935;&#24863;&#25968;&#25454;&#27969;&#34892;&#24230;&#26497;&#39640;&#12290;&#26368;&#36817;&#65292;&#20174;&#31616;&#21333;&#30340;RAG&#35774;&#32622;&#27599;&#27425;&#29992;&#25143;&#36755;&#20837;&#37117;&#26597;&#35810;&#21521;&#37327;&#25968;&#25454;&#24211;&#20197;&#33719;&#21462;&#38468;&#21152;&#20449;&#24687;&#30340;&#26041;&#24335;&#65292;&#27491;&#22312;&#36716;&#21464;&#20026;&#26356;&#22797;&#26434;&#24418;&#24335;&#30340;RAG&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#21508;&#31181;&#20855;&#20307;&#26041;&#27861;&#20173;&#20027;&#35201;&#22522;&#20110;&#22823;&#22810;&#26159;&#20598;&#28982;&#35777;&#25454;&#31454;&#20105;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20005;&#26684;&#30340;&#25968;&#25454;&#38598;&#21019;&#24314;&#21644;&#35780;&#20272;&#24037;&#20316;&#27969;&#31243;&#65292;&#20197;&#23450;&#37327;&#27604;&#36739;&#19981;&#21516;&#30340;RAG&#31574;&#30053;&#12290;&#25105;&#20204;&#20351;&#29992;&#20197;&#36825;&#31181;&#26041;&#24335;&#21019;&#24314;&#30340;&#25968;&#25454;&#38598;&#26469;&#24320;&#21457;&#21644;&#35780;&#20272;&#24067;&#23572;&#20195;&#29702;RAG&#35774;&#32622;&#65306;&#19968;&#20010;&#31995;&#32479;&#65292;&#20854;&#20013;LLM&#21487;&#20197;&#20915;&#23450;&#26159;&#21542;&#26597;&#35810;&#21521;&#37327;&#25968;&#25454;&#24211;&#65292;&#20174;&#32780;&#33410;&#30465;&#21487;&#20197;&#29992;&#20869;&#37096;&#30693;&#35782;&#22238;&#31572;&#30340;&#38382;&#39064;&#30340;&#26631;&#35760;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#22312;&#32447;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00820v1 Announce Type: cross  Abstract: Retrieval Augmented Generation (RAG) systems have seen huge popularity in augmenting Large-Language Model (LLM) outputs with domain specific and time sensitive data. Very recently a shift is happening from simple RAG setups that query a vector database for additional information with every user input to more sophisticated forms of RAG. However, different concrete approaches compete on mostly anecdotal evidence at the moment. In this paper we present a rigorous dataset creation and evaluation workflow to quantitatively compare different RAG strategies. We use a dataset created this way for the development and evaluation of a boolean agent RAG setup: A system in which a LLM can decide whether to query a vector database or not, thus saving tokens on questions that can be answered with internal knowledge. We publish our code and generated dataset online.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#21452;&#37325;&#26657;&#20934;&#20272;&#35745;&#22120;&#65292;&#36890;&#36807;&#26657;&#20934;&#25554;&#34917;&#21644;&#27010;&#29575;&#27169;&#22411;&#26469;&#35299;&#20915;&#25512;&#33616;&#31995;&#32479;&#20013;&#32570;&#22833;&#25968;&#25454;&#19981;&#38543;&#26426;&#30340;&#25361;&#25112;</title><link>https://arxiv.org/abs/2403.00817</link><description>&lt;p&gt;
&#21452;&#37325;&#26657;&#20934;&#20272;&#35745;&#22120;&#29992;&#20110;&#32570;&#22833;&#25968;&#25454;&#19981;&#38543;&#26426;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Doubly Calibrated Estimator for Recommendation on Data Missing Not At Random
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00817
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#21452;&#37325;&#26657;&#20934;&#20272;&#35745;&#22120;&#65292;&#36890;&#36807;&#26657;&#20934;&#25554;&#34917;&#21644;&#27010;&#29575;&#27169;&#22411;&#26469;&#35299;&#20915;&#25512;&#33616;&#31995;&#32479;&#20013;&#32570;&#22833;&#25968;&#25454;&#19981;&#38543;&#26426;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#24448;&#24448;&#21463;&#21040;&#36873;&#25321;&#20559;&#24046;&#30340;&#24433;&#21709;&#65292;&#22240;&#20026;&#29992;&#25143;&#20542;&#21521;&#20110;&#35780;&#20215;&#20182;&#20204;&#21916;&#27426;&#30340;&#29289;&#21697;&#12290;&#22312;&#36825;&#31181;&#26465;&#20214;&#19979;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#34920;&#29616;&#20986;&#19981;&#38543;&#26426;&#32570;&#22833;&#30340;&#26465;&#30446;&#65292;&#22240;&#27492;&#19981;&#26159;&#20195;&#34920;&#30446;&#26631;&#20154;&#32676;&#30340;&#38543;&#26426;&#23545;&#29031;&#35797;&#39564;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#21452;&#37325;&#31283;&#20581;&#20272;&#35745;&#22120;&#21450;&#20854;&#22686;&#24378;&#22411;&#21464;&#20307;&#65292;&#22240;&#20026;&#23427;&#20204;&#30830;&#20445;&#22312;&#25552;&#20379;&#20934;&#30830;&#30340;&#25554;&#34917;&#35823;&#24046;&#25110;&#39044;&#27979;&#27010;&#29575;&#26102;&#26080;&#20559;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35748;&#20026;&#29616;&#26377;&#30340;&#20272;&#35745;&#22120;&#20381;&#36182;&#20110;&#38169;&#35823;&#26657;&#20934;&#30340;&#25554;&#34917;&#35823;&#24046;&#21644;&#27010;&#29575;&#20998;&#25968;&#65292;&#22240;&#20026;&#23427;&#20204;&#20381;&#36182;&#20110;&#20272;&#35745;&#30340;&#22522;&#26412;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20379;&#29702;&#35770;&#27934;&#23519;&#65292;&#35828;&#26126;&#38169;&#35823;&#26657;&#20934;&#30340;&#25554;&#34917;&#21644;&#27010;&#29575;&#27169;&#22411;&#21487;&#33021;&#38480;&#21046;&#21452;&#37325;&#31283;&#20581;&#20272;&#35745;&#22120;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#21033;&#29992;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#39564;&#35777;&#25105;&#20204;&#30340;&#23450;&#29702;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#25324;&#23545;&#25554;&#34917;&#21644;&#27010;&#29575;&#27169;&#22411;&#36827;&#34892;&#26657;&#20934;&#30340;&#21452;&#37325;&#26657;&#20934;&#20272;&#35745;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00817v1 Announce Type: cross  Abstract: Recommender systems often suffer from selection bias as users tend to rate their preferred items. The datasets collected under such conditions exhibit entries missing not at random and thus are not randomized-controlled trials representing the target population. To address this challenge, a doubly robust estimator and its enhanced variants have been proposed as they ensure unbiasedness when accurate imputed errors or predicted propensities are provided. However, we argue that existing estimators rely on miscalibrated imputed errors and propensity scores as they depend on rudimentary models for estimation. We provide theoretical insights into how miscalibrated imputation and propensity models may limit the effectiveness of doubly robust estimators and validate our theorems using real-world datasets. On this basis, we propose a Doubly Calibrated Estimator that involves the calibration of both the imputation and propensity models. To achi
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CFRet-DVQA&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26816;&#32034;&#21644;&#39640;&#25928;&#35843;&#20248;&#65292;&#35299;&#20915;&#20102;&#25991;&#26723;&#35270;&#35273;&#38382;&#31572;&#20013;&#23450;&#20301;&#20449;&#24687;&#21644;&#38480;&#21046;&#27169;&#22411;&#36755;&#20837;&#30340;&#38271;&#24230;&#31561;&#38382;&#39064;&#65292;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;&#31572;&#26696;&#30340;&#29983;&#25104;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.00816</link><description>&lt;p&gt;
CFRet-DVQA&#65306;&#31895;&#21040;&#31934;&#26816;&#32034;&#21644;&#39640;&#25928;&#35843;&#20248;&#29992;&#20110;&#25991;&#26723;&#35270;&#35273;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
CFRet-DVQA: Coarse-to-Fine Retrieval and Efficient Tuning for Document Visual Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00816
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CFRet-DVQA&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26816;&#32034;&#21644;&#39640;&#25928;&#35843;&#20248;&#65292;&#35299;&#20915;&#20102;&#25991;&#26723;&#35270;&#35273;&#38382;&#31572;&#20013;&#23450;&#20301;&#20449;&#24687;&#21644;&#38480;&#21046;&#27169;&#22411;&#36755;&#20837;&#30340;&#38271;&#24230;&#31561;&#38382;&#39064;&#65292;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;&#31572;&#26696;&#30340;&#29983;&#25104;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26723;&#35270;&#35273;&#38382;&#31572;&#65288;DVQA&#65289;&#26159;&#19968;&#20010;&#28041;&#21450;&#26681;&#25454;&#22270;&#20687;&#20869;&#23481;&#22238;&#31572;&#26597;&#35810;&#30340;&#20219;&#21153;&#12290;&#29616;&#26377;&#24037;&#20316;&#20165;&#38480;&#20110;&#23450;&#20301;&#21333;&#39029;&#20869;&#30340;&#20449;&#24687;&#65292;&#19981;&#25903;&#25345;&#36328;&#39029;&#38754;&#38382;&#31572;&#20132;&#20114;&#12290;&#27492;&#22806;&#65292;&#23545;&#27169;&#22411;&#36755;&#20837;&#30340;&#26631;&#35760;&#38271;&#24230;&#38480;&#21046;&#21487;&#33021;&#23548;&#33268;&#19982;&#31572;&#26696;&#30456;&#20851;&#30340;&#37096;&#20998;&#34987;&#25130;&#26029;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26041;&#27861;&#23398;&#65292;&#31216;&#20026;CFRet-DVQA&#65292;&#37325;&#28857;&#25918;&#22312;&#26816;&#32034;&#21644;&#39640;&#25928;&#35843;&#20248;&#19978;&#65292;&#20197;&#26377;&#25928;&#35299;&#20915;&#36825;&#19968;&#20851;&#38190;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#20174;&#25991;&#26723;&#20013;&#26816;&#32034;&#19982;&#25152;&#25552;&#38382;&#39064;&#30456;&#20851;&#30340;&#22810;&#20010;&#29255;&#27573;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20808;&#36827;&#25512;&#29702;&#33021;&#21147;&#65292;&#36890;&#36807;&#25351;&#23548;&#35843;&#20248;&#36827;&#19968;&#27493;&#22686;&#24378;&#20854;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#20351;&#24471;&#29983;&#25104;&#30340;&#31572;&#26696;&#19982;&#25991;&#26723;&#26631;&#31614;&#30340;&#39118;&#26684;&#30456;&#31526;&#12290;&#23454;&#39564;&#28436;&#31034;&#20102;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00816v1 Announce Type: cross  Abstract: Document Visual Question Answering (DVQA) is a task that involves responding to queries based on the content of images. Existing work is limited to locating information within a single page and does not facilitate cross-page question-and-answer interaction. Furthermore, the token length limitation imposed on inputs to the model may lead to truncation of segments pertinent to the answer. In this study, we introduce a simple but effective methodology called CFRet-DVQA, which focuses on retrieval and efficient tuning to address this critical issue effectively. For that, we initially retrieve multiple segments from the document that correlate with the question at hand. Subsequently, we leverage the advanced reasoning abilities of the large language model (LLM), further augmenting its performance through instruction tuning. This approach enables the generation of answers that align with the style of the document labels. The experiments demo
&lt;/p&gt;</description></item><item><title>RAM-EHR&#36890;&#36807;&#22686;&#24378;&#26816;&#32034;&#24182;&#21033;&#29992;&#24635;&#32467;&#30693;&#35782;&#65292;&#25552;&#39640;&#20102;&#38024;&#23545;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#30340;&#20020;&#24202;&#39044;&#27979;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.00815</link><description>&lt;p&gt;
RAM-EHR: &#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#19978;&#30340;&#26816;&#32034;&#22686;&#24378;&#19982;&#20020;&#24202;&#39044;&#27979;&#30456;&#36935;
&lt;/p&gt;
&lt;p&gt;
RAM-EHR: Retrieval Augmentation Meets Clinical Predictions on Electronic Health Records
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00815
&lt;/p&gt;
&lt;p&gt;
RAM-EHR&#36890;&#36807;&#22686;&#24378;&#26816;&#32034;&#24182;&#21033;&#29992;&#24635;&#32467;&#30693;&#35782;&#65292;&#25552;&#39640;&#20102;&#38024;&#23545;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#30340;&#20020;&#24202;&#39044;&#27979;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;RAM-EHR&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#25913;&#21892;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#19978;&#20020;&#24202;&#39044;&#27979;&#30340;&#26816;&#32034;&#22686;&#24378;&#65288;Retrieval Augmentation&#65289;&#27969;&#31243;&#12290;RAM-EHR&#39318;&#20808;&#25910;&#38598;&#22810;&#20010;&#30693;&#35782;&#26469;&#28304;&#65292;&#23558;&#23427;&#20204;&#36716;&#25442;&#20026;&#25991;&#26412;&#26684;&#24335;&#65292;&#24182;&#20351;&#29992;&#23494;&#38598;&#26816;&#32034;&#26469;&#33719;&#21462;&#19982;&#21307;&#23398;&#27010;&#24565;&#30456;&#20851;&#30340;&#20449;&#24687;&#12290;&#36825;&#19968;&#31574;&#30053;&#35299;&#20915;&#20102;&#19982;&#22797;&#26434;&#27010;&#24565;&#21517;&#31216;&#30456;&#20851;&#30340;&#22256;&#38590;&#12290;RAM-EHR&#28982;&#21518;&#22686;&#24191;&#20102;&#19982;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#20195;&#30721;&#32852;&#21512;&#35757;&#32451;&#30340;&#26412;&#22320;EHR&#39044;&#27979;&#27169;&#22411;&#65292;&#20197;&#25429;&#33719;&#26469;&#33258;&#24739;&#32773;&#23601;&#35786;&#21644;&#24635;&#32467;&#30693;&#35782;&#30340;&#20114;&#34917;&#20449;&#24687;&#12290;&#22312;&#20004;&#20010;EHR&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;RAM-EHR&#30456;&#23545;&#20110;&#20043;&#21069;&#30340;&#30693;&#35782;&#22686;&#24378;&#22522;&#32447;&#25928;&#26524;&#26174;&#33879;&#65288;AUROC&#22686;&#30410;3.4&#65285;&#65292;AUPR&#22686;&#30410;7.2&#65285;&#65289;&#65292;&#24378;&#35843;&#20102;RAM-EHR&#30340;&#24635;&#32467;&#30693;&#35782;&#23545;&#20020;&#24202;&#39044;&#27979;&#20219;&#21153;&#30340;&#26377;&#25928;&#24615;&#12290;&#20195;&#30721;&#23558;&#21457;&#24067;&#22312;\url{https://github.com/ritaranx/RAM-EHR}&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00815v1 Announce Type: cross  Abstract: We present RAM-EHR, a Retrieval AugMentation pipeline to improve clinical predictions on Electronic Health Records (EHRs). RAM-EHR first collects multiple knowledge sources, converts them into text format, and uses dense retrieval to obtain information related to medical concepts. This strategy addresses the difficulties associated with complex names for the concepts. RAM-EHR then augments the local EHR predictive model co-trained with consistency regularization to capture complementary information from patient visits and summarized knowledge. Experiments on two EHR datasets show the efficacy of RAM-EHR over previous knowledge-enhanced baselines (3.4% gain in AUROC and 7.2% gain in AUPR), emphasizing the effectiveness of the summarized knowledge from RAM-EHR for clinical prediction tasks. The code will be published at \url{https://github.com/ritaranx/RAM-EHR}.
&lt;/p&gt;</description></item><item><title>&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#30340;&#29992;&#25143;&#23454;&#39564;&#26694;&#26550;&#26469;&#30740;&#31350;&#27861;&#24459;&#26696;&#20363;&#25628;&#32034;&#32467;&#26524;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#23545;&#29992;&#25143;&#35748;&#30693;&#30340;&#24433;&#21709;</title><link>https://arxiv.org/abs/2403.00814</link><description>&lt;p&gt;
&#29992;&#25143;&#20915;&#31574;&#36807;&#31243;&#20013;&#24615;&#21035;&#20559;&#35265;&#30340;&#27861;&#24459;&#26696;&#20363;&#26816;&#32034;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Gender Biased Legal Case Retrieval System on Users' Decision Process
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00814
&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#30340;&#29992;&#25143;&#23454;&#39564;&#26694;&#26550;&#26469;&#30740;&#31350;&#27861;&#24459;&#26696;&#20363;&#25628;&#32034;&#32467;&#26524;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#23545;&#29992;&#25143;&#35748;&#30693;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#27861;&#24459;&#26696;&#20363;&#25628;&#32034;&#24050;&#25104;&#20026;&#27861;&#24459;&#20174;&#19994;&#32773;&#24037;&#20316;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#22312;&#27861;&#24459;&#26696;&#20363;&#25628;&#32034;&#20013;&#65292;&#25628;&#32034;&#24341;&#25806;&#20174;&#28023;&#37327;&#25968;&#25454;&#20013;&#26816;&#32034;&#20986;&#35768;&#22810;&#30456;&#20851;&#26696;&#20363;&#24182;&#25552;&#20379;&#32473;&#29992;&#25143;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26696;&#20363;&#26159;&#21542;&#23384;&#22312;&#24615;&#21035;&#20559;&#35265;&#65292;&#20197;&#21450;&#36825;&#31181;&#20559;&#35265;&#26159;&#21542;&#20250;&#24433;&#21709;&#29992;&#25143;&#30340;&#35748;&#30693;&#65292;&#30446;&#21069;&#23578;&#19981;&#30830;&#23450;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#30340;&#29992;&#25143;&#23454;&#39564;&#26694;&#26550;&#26469;&#27169;&#25311;&#27861;&#23448;&#38405;&#35835;&#30456;&#20851;&#26696;&#20363;&#30340;&#36807;&#31243;&#12290;&#36992;&#35831;&#20102;72&#21517;&#20855;&#26377;&#27861;&#24459;&#32972;&#26223;&#30340;&#21442;&#19982;&#32773;&#36827;&#34892;&#23454;&#39564;&#12290;&#21442;&#19982;&#32773;&#34987;&#35201;&#27714;&#27169;&#25311;&#27861;&#23448;&#30340;&#35282;&#33394;&#65292;&#22312;3&#20010;&#25351;&#23450;&#30340;&#26696;&#20363;&#20013;&#36827;&#34892;&#27861;&#24459;&#26696;&#20363;&#25628;&#32034;&#65292;&#24182;&#30830;&#23450;&#36825;&#20123;&#26696;&#20363;&#20013;&#34987;&#21578;&#30340;&#21028;&#20915;&#32467;&#26524;&#12290;&#21516;&#26102;&#23545;&#20219;&#21153;&#21644;&#30456;&#20851;&#26696;&#20363;&#20013;&#34987;&#21578;&#30340;&#24615;&#21035;&#36827;&#34892;&#32534;&#36753;&#65292;&#20197;&#32479;&#35745;&#24615;&#22320;&#34913;&#37327;&#27861;&#24459;&#26696;&#20363;&#25628;&#32034;&#32467;&#26524;&#20013;&#24615;&#21035;&#20559;&#35265;&#23545;&#21442;&#19982;&#32773;&#35748;&#30693;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#27861;&#24459;&#26696;&#20363;&#25628;&#32034;&#32467;&#26524;&#20013;&#23384;&#22312;&#24615;&#21035;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00814v1 Announce Type: new  Abstract: In the last decade, legal case search has become an important part of a legal practitioner's work. During legal case search, search engines retrieval a number of relevant cases from huge amounts of data and serve them to users. However, it is uncertain whether these cases are gender-biased and whether such bias has impact on user perceptions. We designed a new user experiment framework to simulate the judges' reading of relevant cases. 72 participants with backgrounds in legal affairs invited to conduct the experiment. Participants were asked to simulate the role of the judge in conducting a legal case search on 3 assigned cases and determine the sentences of the defendants in these cases. Gender of the defendants in both the task and relevant cases was edited to statistically measure the effect of gender bias in the legal case search results on participants' perceptions. The results showed that gender bias in the legal case search resul
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;Elasticsearch&#21644;Transformer&#27169;&#22411;&#25552;&#21319;&#20113;&#31471;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22788;&#29702;&#65292;&#23588;&#20854;&#22312;&#23454;&#29616;&#35821;&#20041;&#25628;&#32034;&#26041;&#38754;&#26377;&#26174;&#33879;&#24110;&#21161;&#12290;</title><link>https://arxiv.org/abs/2403.00807</link><description>&lt;p&gt;
&#21033;&#29992;Elasticsearch&#21644;Transformer&#27169;&#22411;&#22686;&#24378;&#22522;&#20110;&#20113;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
Enhancing Cloud-Based Large Language Model Processing with Elasticsearch and Transformer Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00807
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;Elasticsearch&#21644;Transformer&#27169;&#22411;&#25552;&#21319;&#20113;&#31471;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22788;&#29702;&#65292;&#23588;&#20854;&#22312;&#23454;&#29616;&#35821;&#20041;&#25628;&#32034;&#26041;&#38754;&#26377;&#26174;&#33879;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26159;&#19968;&#31867;&#21033;&#29992;Transformer&#32593;&#32476;&#26500;&#24314;&#30340;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65292;&#33021;&#22815;&#21033;&#29992;&#22823;&#37327;&#25968;&#25454;&#38598;&#35782;&#21035;&#12289;&#24635;&#32467;&#12289;&#32763;&#35793;&#12289;&#39044;&#27979;&#21644;&#29983;&#25104;&#35821;&#35328;&#12290;LLMs&#25215;&#35834;&#25913;&#21464;&#31038;&#20250;&#65292;&#28982;&#32780;&#35757;&#32451;&#36825;&#20123;&#22522;&#30784;&#27169;&#22411;&#38754;&#20020;&#24040;&#22823;&#25361;&#25112;&#12290;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36827;&#34892;&#35821;&#20041;&#21521;&#37327;&#25628;&#32034;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#26174;&#33879;&#22686;&#24378;&#25628;&#32034;&#32467;&#26524;&#30340;&#20934;&#30830;&#24615;&#21644;&#30456;&#20851;&#24615;&#12290;&#19982;&#20256;&#32479;&#22522;&#20110;&#20851;&#38190;&#35789;&#30340;&#25628;&#32034;&#26041;&#27861;&#19981;&#21516;&#65292;&#35821;&#20041;&#25628;&#32034;&#21033;&#29992;&#21333;&#35789;&#30340;&#21547;&#20041;&#21644;&#19978;&#19979;&#25991;&#26469;&#29702;&#35299;&#26597;&#35810;&#32972;&#21518;&#30340;&#24847;&#22270;&#65292;&#24182;&#25552;&#20379;&#26356;&#31934;&#30830;&#30340;&#32467;&#26524;&#12290;Elasticsearch&#26159;&#19968;&#31181;&#26368;&#27969;&#34892;&#30340;&#24037;&#20855;&#20043;&#19968;&#65292;&#29992;&#20110;&#23454;&#29616;&#35821;&#20041;&#25628;&#32034;&#65292;&#26159;&#19968;&#20010;&#19987;&#20026;&#32034;&#24341;&#21644;&#25628;&#32034;&#22823;&#25968;&#25454;&#38598;&#35774;&#35745;&#30340;&#21487;&#25193;&#23637;&#21644;&#31283;&#20581;&#30340;&#25628;&#32034;&#24341;&#25806;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#35821;&#20041;&#25628;&#32034;&#30340;&#22522;&#30784;&#30693;&#35782;&#65292;&#24182;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;Elasticsearch&#21644;Transformer&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00807v1 Announce Type: cross  Abstract: Large Language Models (LLMs) are a class of generative AI models built using the Transformer network, capable of leveraging vast datasets to identify, summarize, translate, predict, and generate language. LLMs promise to revolutionize society, yet training these foundational models poses immense challenges. Semantic vector search within large language models is a potent technique that can significantly enhance search result accuracy and relevance. Unlike traditional keyword-based search methods, semantic search utilizes the meaning and context of words to grasp the intent behind queries and deliver more precise outcomes. Elasticsearch emerges as one of the most popular tools for implementing semantic search an exceptionally scalable and robust search engine designed for indexing and searching extensive datasets. In this article, we delve into the fundamentals of semantic search and explore how to harness Elasticsearch and Transformer m
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32467;&#21512;&#20132;&#20114;&#35774;&#35745;&#21644;&#26426;&#22120;&#23398;&#20064;&#65292;&#25552;&#20379;&#26356;&#39640;&#25928;&#12289;&#20010;&#24615;&#21270;&#30340;&#29992;&#25143;&#20307;&#39564;&#65292;&#28385;&#36275;&#29992;&#25143;&#29305;&#23450;&#38656;&#27714;&#65292;&#25345;&#32493;&#25913;&#21892;&#20135;&#21697;&#36136;&#37327;&#21644;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.00806</link><description>&lt;p&gt;
&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#25805;&#20316;&#31995;&#32479;&#20013;&#30340;&#29992;&#25143;&#20132;&#20114;
&lt;/p&gt;
&lt;p&gt;
Enhanced User Interaction in Operating Systems through Machine Learning Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00806
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32467;&#21512;&#20132;&#20114;&#35774;&#35745;&#21644;&#26426;&#22120;&#23398;&#20064;&#65292;&#25552;&#20379;&#26356;&#39640;&#25928;&#12289;&#20010;&#24615;&#21270;&#30340;&#29992;&#25143;&#20307;&#39564;&#65292;&#28385;&#36275;&#29992;&#25143;&#29305;&#23450;&#38656;&#27714;&#65292;&#25345;&#32493;&#25913;&#21892;&#20135;&#21697;&#36136;&#37327;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20986;&#31867;&#20284;&#20154;&#31867;&#30340;&#36923;&#36753;&#25512;&#29702;&#21644;&#29702;&#35299;&#33021;&#21147;&#65292;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20195;&#29702;&#26159;&#21542;&#33021;&#27169;&#25311;&#30495;&#23454;&#29992;&#25143;&#30340;&#20132;&#20114;&#34892;&#20026;&#65292;&#20174;&#32780;&#26500;&#24314;&#19968;&#20010;&#21487;&#38752;&#30340;&#34394;&#25311;&#25512;&#33616;A/B&#27979;&#35797;&#22330;&#26223;&#65292;&#24110;&#21161;&#25512;&#33616;&#30740;&#31350;&#30340;&#24212;&#29992;&#26159;&#19968;&#20010;&#36843;&#20999;&#12289;&#37325;&#35201;&#24182;&#20855;&#26377;&#32463;&#27982;&#20215;&#20540;&#30340;&#38382;&#39064;&#12290;&#20132;&#20114;&#35774;&#35745;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#32467;&#21512;&#33021;&#20026;&#20135;&#21697;&#21644;&#26381;&#21153;&#25552;&#20379;&#26356;&#39640;&#25928;&#12289;&#20010;&#24615;&#21270;&#30340;&#29992;&#25143;&#20307;&#39564;&#12290;&#36825;&#31181;&#20010;&#24615;&#21270;&#26381;&#21153;&#21487;&#20197;&#28385;&#36275;&#29992;&#25143;&#30340;&#29305;&#23450;&#38656;&#27714;&#65292;&#25552;&#39640;&#29992;&#25143;&#28385;&#24847;&#24230;&#21644;&#24544;&#35802;&#24230;&#12290;&#27492;&#22806;&#65292;&#20132;&#20114;&#31995;&#32479;&#21487;&#20197;&#36890;&#36807;&#25552;&#20379;&#33391;&#22909;&#30340;&#29992;&#25143;&#30028;&#38754;&#21644;&#20132;&#20114;&#20307;&#39564;&#26469;&#29702;&#35299;&#29992;&#25143;&#23545;&#20135;&#21697;&#30340;&#30475;&#27861;&#21644;&#38656;&#27714;&#65292;&#28982;&#21518;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#25913;&#36827;&#21644;&#20248;&#21270;&#20135;&#21697;&#12290;&#36825;&#31181;&#36845;&#20195;&#20248;&#21270;&#36807;&#31243;&#21487;&#20197;&#25345;&#32493;&#25913;&#21892;&#20135;&#21697;&#30340;&#36136;&#37327;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00806v1 Announce Type: cross  Abstract: With the large language model showing human-like logical reasoning and understanding ability, whether agents based on the large language model can simulate the interaction behavior of real users, so as to build a reliable virtual recommendation A/B test scene to help the application of recommendation research is an urgent, important and economic value problem. The combination of interaction design and machine learning can provide a more efficient and personalized user experience for products and services. This personalized service can meet the specific needs of users and improve user satisfaction and loyalty. Second, the interactive system can understand the user's views and needs for the product by providing a good user interface and interactive experience, and then use machine learning algorithms to improve and optimize the product. This iterative optimization process can continuously improve the quality and performance of the produc
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#20803;&#23398;&#20064;&#23454;&#29616;&#20010;&#24615;&#21270;&#28145;&#24230;&#25512;&#33616;&#27169;&#22411;&#30340;&#21019;&#26032;&#35299;&#20915;&#26041;&#26696;&#65292;&#33021;&#22815;&#26681;&#25454;&#26368;&#26032;&#29992;&#25143;&#20114;&#21160;&#20449;&#21495;&#39057;&#32321;&#26356;&#26032;&#27169;&#22411;&#65292;&#20197;&#30830;&#20445;&#21521;&#19981;&#21516;&#25104;&#21592;&#25552;&#20379;&#30456;&#20851;&#19988;&#26356;&#26032;&#30340;&#20307;&#39564;&#12290;</title><link>https://arxiv.org/abs/2403.00803</link><description>&lt;p&gt;
LiMAML: &#36890;&#36807;&#20803;&#23398;&#20064;&#20010;&#24615;&#21270;&#28145;&#24230;&#25512;&#33616;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
LiMAML: Personalization of Deep Recommender Models via Meta Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00803
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#20803;&#23398;&#20064;&#23454;&#29616;&#20010;&#24615;&#21270;&#28145;&#24230;&#25512;&#33616;&#27169;&#22411;&#30340;&#21019;&#26032;&#35299;&#20915;&#26041;&#26696;&#65292;&#33021;&#22815;&#26681;&#25454;&#26368;&#26032;&#29992;&#25143;&#20114;&#21160;&#20449;&#21495;&#39057;&#32321;&#26356;&#26032;&#27169;&#22411;&#65292;&#20197;&#30830;&#20445;&#21521;&#19981;&#21516;&#25104;&#21592;&#25552;&#20379;&#30456;&#20851;&#19988;&#26356;&#26032;&#30340;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25512;&#33616;&#31995;&#32479;&#39046;&#22495;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26222;&#36941;&#37319;&#29992;&#24050;&#32463;&#25104;&#20026;&#24314;&#27169;&#21508;&#31181;&#19994;&#21153;&#30446;&#26631;&#30340;&#20027;&#23548;&#33539;&#24335;&#12290;&#38543;&#30528;&#29992;&#25143;&#22522;&#25968;&#30340;&#25345;&#32493;&#22686;&#38271;&#65292;&#20010;&#24615;&#21270;&#21644;&#39057;&#32321;&#30340;&#27169;&#22411;&#26356;&#26032;&#30340;&#24517;&#35201;&#24615;&#24050;&#32463;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#65292;&#20197;&#30830;&#20445;&#21521;&#21508;&#31181;&#25104;&#21592;&#25552;&#20379;&#30456;&#20851;&#19988;&#26356;&#26032;&#30340;&#20307;&#39564;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#20803;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#38024;&#23545;&#20010;&#20154;&#25104;&#21592;&#21644;&#20854;&#20182;&#23454;&#20307;&#30340;&#27169;&#22411;&#20010;&#24615;&#21270;&#65292;&#32467;&#21512;&#20102;&#26681;&#25454;&#26368;&#26032;&#29992;&#25143;&#20114;&#21160;&#20449;&#21495;&#36827;&#34892;&#39057;&#32321;&#26356;&#26032;&#30340;&#21151;&#33021;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#27169;&#22411;&#26080;&#20851;&#30340;&#20803;&#23398;&#20064;&#65288;MAML&#65289;&#31639;&#27861;&#65292;&#20351;&#29992;&#26368;&#36817;&#30340;&#29992;&#25143;&#20114;&#21160;&#25968;&#25454;&#26469;&#35843;&#25972;&#27599;&#20010;&#20219;&#21153;&#30340;&#23376;&#32593;&#32476;&#12290;&#32771;&#34385;&#21040;&#22312;&#32447;&#25512;&#33616;&#31995;&#32479;&#20013;&#29983;&#20135;&#21407;&#22987;MAML&#27169;&#22411;&#20960;&#20046;&#19981;&#21487;&#34892;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#31574;&#30053;&#26469;&#23558;&#20803;&#23398;&#20064;&#30340;&#23376;&#32593;&#32476;&#25512;&#24191;&#24212;&#29992;&#21040;&#29983;&#20135;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00803v1 Announce Type: cross  Abstract: In the realm of recommender systems, the ubiquitous adoption of deep neural networks has emerged as a dominant paradigm for modeling diverse business objectives. As user bases continue to expand, the necessity of personalization and frequent model updates have assumed paramount significance to ensure the delivery of relevant and refreshed experiences to a diverse array of members. In this work, we introduce an innovative meta-learning solution tailored to the personalization of models for individual members and other entities, coupled with the frequent updates based on the latest user interaction signals. Specifically, we leverage the Model-Agnostic Meta Learning (MAML) algorithm to adapt per-task sub-networks using recent user interaction data. Given the near infeasibility of productionizing original MAML-based models in online recommendation systems, we propose an efficient strategy to operationalize meta-learned sub-networks in prod
&lt;/p&gt;</description></item><item><title>&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;: &#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20004;&#38454;&#27573;&#25512;&#33616;&#31995;&#32479;&#30340;&#29702;&#35770;&#34892;&#20026;&#65292;&#35777;&#26126;&#20102;&#20854;&#21521;&#26368;&#20339;&#25512;&#33616;&#31995;&#32479;&#30340;&#24378;&#25910;&#25947;&#24615;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#23427;&#22312;&#36755;&#20837;&#29305;&#24449;&#30340;&#22266;&#26377;&#32500;&#24230;&#19978;&#23454;&#29616;&#26356;&#24555;&#25910;&#25947;&#30340;&#29305;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.00802</link><description>&lt;p&gt;
&#26397;&#21521;&#29702;&#35770;&#29702;&#35299;&#20004;&#38454;&#27573;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Towards a Theoretical Understanding of Two-Stage Recommender Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00802
&lt;/p&gt;
&lt;p&gt;
&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;: &#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20004;&#38454;&#27573;&#25512;&#33616;&#31995;&#32479;&#30340;&#29702;&#35770;&#34892;&#20026;&#65292;&#35777;&#26126;&#20102;&#20854;&#21521;&#26368;&#20339;&#25512;&#33616;&#31995;&#32479;&#30340;&#24378;&#25910;&#25947;&#24615;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#23427;&#22312;&#36755;&#20837;&#29305;&#24449;&#30340;&#22266;&#26377;&#32500;&#24230;&#19978;&#23454;&#29616;&#26356;&#24555;&#25910;&#25947;&#30340;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00802v1 &#20844;&#21578;&#31867;&#22411;:&#36328;&#22495; &#25688;&#35201;:&#29983;&#20135;&#32423;&#25512;&#33616;&#31995;&#32479;&#22312;&#22312;&#32447;&#23186;&#20307;&#26381;&#21153;&#20013;&#24191;&#27867;&#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;&#65292;&#21253;&#25324;Netflix&#12289;Pinterest&#21644;Amazon&#12290;&#36825;&#20123;&#31995;&#32479;&#36890;&#36807;&#23398;&#20064;&#29992;&#25143;&#21644;&#29289;&#21697;&#22312;&#20302;&#32500;&#31354;&#38388;&#20013;&#25237;&#24433;&#30340;&#23884;&#20837;&#12289;&#36890;&#36807;&#20004;&#38454;&#27573;&#27169;&#22411;&#65288;&#20004;&#20010;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65289;&#20016;&#23500;&#25512;&#33616;&#65292;&#36825;&#26377;&#21161;&#20110;&#23427;&#20204;&#30340;&#23884;&#20837;&#26500;&#24314;&#20197;&#39044;&#27979;&#19982;&#29289;&#21697;&#30456;&#20851;&#30340;&#29992;&#25143;&#21453;&#39304;&#12290;&#23613;&#31649;&#23427;&#22312;&#25512;&#33616;&#20013;&#24456;&#21463;&#27426;&#36814;&#65292;&#20294;&#20854;&#29702;&#35770;&#34892;&#20026;&#20173;&#26410;&#24471;&#21040;&#20840;&#38754;&#25506;&#35752;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#38454;&#27573;&#25512;&#33616;&#30340;&#28176;&#36817;&#34892;&#20026;&#65292;&#36825;&#21253;&#25324;&#23545;&#26368;&#20339;&#25512;&#33616;&#31995;&#32479;&#30340;&#24378;&#25910;&#25947;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#20004;&#38454;&#27573;&#25512;&#33616;&#30340;&#19968;&#20123;&#29702;&#35770;&#24615;&#36136;&#21644;&#32479;&#35745;&#20445;&#35777;&#12290;&#38500;&#20102;&#28176;&#36817;&#34892;&#20026;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#20004;&#38454;&#27573;&#25512;&#33616;&#31995;&#32479;&#36890;&#36807;&#20381;&#36182;&#36755;&#20837;&#29305;&#24449;&#30340;&#22266;&#26377;&#32500;&#24230;&#23454;&#29616;&#26356;&#24555;&#30340;&#25910;&#25947;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#25968;&#20540;&#26041;&#27861;&#23637;&#31034;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00802v1 Announce Type: cross  Abstract: Production-grade recommender systems rely heavily on a large-scale corpus used by online media services, including Netflix, Pinterest, and Amazon. These systems enrich recommendations by learning users' and items' embeddings projected in a low-dimensional space with two-stage models (two deep neural networks), which facilitate their embedding constructs to predict users' feedback associated with items. Despite its popularity for recommendations, its theoretical behaviors remain comprehensively unexplored. We study the asymptotic behaviors of the two-stage recommender that entail a strong convergence to the optimal recommender system. We establish certain theoretical properties and statistical assurance of the two-stage recommender. In addition to asymptotic behaviors, we demonstrate that the two-stage recommender system attains faster convergence by relying on the intrinsic dimensions of the input features. Finally, we show numerically
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#33258;&#20027;&#26816;&#32034;(Self-Retrieval)&#65292;&#21033;&#29992;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23436;&#20840;&#20869;&#21270;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#30340;&#33021;&#21147;&#65292;&#28145;&#24230;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20449;&#24687;&#26816;&#32034;&#36807;&#31243;&#20013;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.00801</link><description>&lt;p&gt;
&#33258;&#20027;&#26816;&#32034;&#65306;&#21033;&#29992;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Self-Retrieval: Building an Information Retrieval System with One Large Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00801
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#33258;&#20027;&#26816;&#32034;(Self-Retrieval)&#65292;&#21033;&#29992;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23436;&#20840;&#20869;&#21270;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#30340;&#33021;&#21147;&#65292;&#28145;&#24230;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20449;&#24687;&#26816;&#32034;&#36807;&#31243;&#20013;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20852;&#36215;&#25913;&#21464;&#20102;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#22312;&#20154;&#31867;&#33719;&#21462;&#20449;&#24687;&#36807;&#31243;&#20013;&#30340;&#35282;&#33394;&#12290;&#30001;&#20110;&#29616;&#26377;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#20855;&#26377;&#23396;&#31435;&#30340;&#26550;&#26500;&#21644;&#26377;&#38480;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#26080;&#27861;&#23436;&#20840;&#36866;&#24212;&#30452;&#25509;&#21521;&#20154;&#31867;&#25552;&#20379;&#20449;&#24687;&#36716;&#21464;&#20026;&#38388;&#25509;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#26381;&#21153;&#30340;&#21464;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#33258;&#20027;&#26816;&#32034;(Self-Retrieval)&#65292;&#36825;&#26159;&#19968;&#20010;&#31471;&#21040;&#31471;&#12289;&#20197;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#20449;&#24687;&#26816;&#32034;&#26550;&#26500;&#65292;&#21487;&#20197;&#23436;&#20840;&#20869;&#21270;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#25152;&#38656;&#30340;&#33021;&#21147;&#21040;&#21333;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#24182;&#28145;&#24230;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20449;&#24687;&#26816;&#32034;&#36807;&#31243;&#20013;&#30340;&#33021;&#21147;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#33258;&#20027;&#26816;&#32034;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#32034;&#24341;&#26550;&#26500;&#23558;&#35201;&#26816;&#32034;&#30340;&#35821;&#26009;&#20869;&#21270;&#20026;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#28982;&#21518;&#25972;&#20010;&#26816;&#32034;&#36807;&#31243;&#34987;&#37325;&#26032;&#23450;&#20041;&#20026;&#25991;&#26723;&#29983;&#25104;&#21644;&#33258;&#25105;&#35780;&#20272;&#30340;&#36807;&#31243;&#65292;&#21487;&#20197;&#20351;&#29992;&#21333;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#31471;&#21040;&#31471;&#25191;&#34892;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;S
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00801v1 Announce Type: cross  Abstract: The rise of large language models (LLMs) has transformed the role of information retrieval (IR) systems in the way to humans accessing information. Due to the isolated architecture and the limited interaction, existing IR systems are unable to fully accommodate the shift from directly providing information to humans to indirectly serving large language models. In this paper, we propose Self-Retrieval, an end-to-end, LLM-driven information retrieval architecture that can fully internalize the required abilities of IR systems into a single LLM and deeply leverage the capabilities of LLMs during IR process. Specifically, Self-retrieval internalizes the corpus to retrieve into a LLM via a natural language indexing architecture. Then the entire retrieval process is redefined as a procedure of document generation and self-assessment, which can be end-to-end executed using a single large language model. Experimental results demonstrate that S
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#20248;&#21270;&#30340;&#35282;&#24230;&#25506;&#35752;CTR&#39044;&#27979;&#38382;&#39064;&#65292;&#25581;&#31034;&#20102;&#29305;&#24449;&#39057;&#29575;&#19982;&#26368;&#22823;Hessian&#29305;&#24449;&#20540;&#20043;&#38388;&#30340;&#24378;&#27491;&#30456;&#20851;&#24615;&#65292;&#25552;&#20986;&#39057;&#32321;&#20986;&#29616;&#30340;&#29305;&#24449;&#20250;&#36235;&#21521;&#20110;&#25910;&#25947;&#21040;&#23574;&#38160;&#30340;&#23616;&#37096;&#26368;&#23567;&#20540;&#65292;&#20174;&#32780;&#23548;&#33268;&#27425;&#20248;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.00798</link><description>&lt;p&gt;
Helen: &#20351;&#29992;&#39057;&#29575;&#21152;&#26435;Hessian&#29305;&#24449;&#20540;&#27491;&#21017;&#21270;&#20248;&#21270;CTR&#39044;&#27979;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Helen: Optimizing CTR Prediction Models with Frequency-wise Hessian Eigenvalue Regularization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00798
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#20248;&#21270;&#30340;&#35282;&#24230;&#25506;&#35752;CTR&#39044;&#27979;&#38382;&#39064;&#65292;&#25581;&#31034;&#20102;&#29305;&#24449;&#39057;&#29575;&#19982;&#26368;&#22823;Hessian&#29305;&#24449;&#20540;&#20043;&#38388;&#30340;&#24378;&#27491;&#30456;&#20851;&#24615;&#65292;&#25552;&#20986;&#39057;&#32321;&#20986;&#29616;&#30340;&#29305;&#24449;&#20250;&#36235;&#21521;&#20110;&#25910;&#25947;&#21040;&#23574;&#38160;&#30340;&#23616;&#37096;&#26368;&#23567;&#20540;&#65292;&#20174;&#32780;&#23548;&#33268;&#27425;&#20248;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21333;&#20987;&#29575;(CTR)&#39044;&#27979;&#22312;&#22312;&#32447;&#24191;&#21578;&#21644;&#25512;&#33616;&#22330;&#26223;&#20013;&#20855;&#26377;&#33267;&#20851;&#37325;&#35201;&#30340;&#24847;&#20041;&#12290;&#23613;&#31649;&#26368;&#36817;CTR&#39044;&#27979;&#27169;&#22411;&#30340;&#24191;&#27867;&#22686;&#21152;&#65292;&#20294;&#24615;&#33021;&#25913;&#36827;&#20173;&#28982;&#26377;&#38480;&#65292;&#36825;&#19968;&#28857;&#21487;&#20197;&#20174;&#24320;&#28304;&#22522;&#20934;&#35780;&#20272;&#20013;&#24471;&#21040;&#35777;&#23454;&#12290;&#24403;&#21069;&#30740;&#31350;&#20154;&#21592;&#20542;&#21521;&#20110;&#38024;&#23545;&#19981;&#21516;&#25968;&#25454;&#38598;&#21644;&#35774;&#32622;&#24320;&#21457;&#26032;&#27169;&#22411;&#65292;&#24120;&#24120;&#24573;&#35270;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#26159;&#20160;&#20040;&#30495;&#27491;&#20351;CTR&#39044;&#27979;&#22914;&#27492;&#20855;&#26377;&#25361;&#25112;&#24615;&#65311;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00798v1 Announce Type: cross  Abstract: Click-Through Rate (CTR) prediction holds paramount significance in online advertising and recommendation scenarios. Despite the proliferation of recent CTR prediction models, the improvements in performance have remained limited, as evidenced by open-source benchmark assessments. Current researchers tend to focus on developing new models for various datasets and settings, often neglecting a crucial question: What is the key challenge that truly makes CTR prediction so demanding?   In this paper, we approach the problem of CTR prediction from an optimization perspective. We explore the typical data characteristics and optimization statistics of CTR prediction, revealing a strong positive correlation between the top hessian eigenvalue and feature frequency. This correlation implies that frequently occurring features tend to converge towards sharp local minima, ultimately leading to suboptimal performance. Motivated by the recent advance
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#34892;&#19994;&#24191;&#21578;&#25512;&#33616;&#31995;&#32479;&#65292;&#37325;&#28857;&#20851;&#27880;&#23398;&#20064;&#36866;&#24403;&#34920;&#31034;&#30340;&#25361;&#25112;&#21644;&#23454;&#36341;&#65292;&#37319;&#29992;&#22810;&#31181;&#26041;&#27861;&#22788;&#29702;&#29305;&#24449;&#34920;&#31034;&#20013;&#30340;&#20851;&#38190;&#25361;&#25112;&#65292;&#21253;&#25324;&#23884;&#20837;&#30340;&#32500;&#24230;&#22349;&#32553;&#21644;&#36328;&#20219;&#21153;&#25110;&#22330;&#26223;&#30340;&#20852;&#36259;&#32416;&#32544;&#12290;</title><link>https://arxiv.org/abs/2403.00793</link><description>&lt;p&gt;
&#22312;&#19968;&#20010;&#28151;&#20081;&#32780;&#32416;&#32544;&#30340;&#19990;&#30028;&#20013;&#30340;&#24191;&#21578;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Ad Recommendation in a Collapsed and Entangled World
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00793
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#34892;&#19994;&#24191;&#21578;&#25512;&#33616;&#31995;&#32479;&#65292;&#37325;&#28857;&#20851;&#27880;&#23398;&#20064;&#36866;&#24403;&#34920;&#31034;&#30340;&#25361;&#25112;&#21644;&#23454;&#36341;&#65292;&#37319;&#29992;&#22810;&#31181;&#26041;&#27861;&#22788;&#29702;&#29305;&#24449;&#34920;&#31034;&#20013;&#30340;&#20851;&#38190;&#25361;&#25112;&#65292;&#21253;&#25324;&#23884;&#20837;&#30340;&#32500;&#24230;&#22349;&#32553;&#21644;&#36328;&#20219;&#21153;&#25110;&#22330;&#26223;&#30340;&#20852;&#36259;&#32416;&#32544;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#34892;&#19994;&#24191;&#21578;&#25512;&#33616;&#31995;&#32479;&#65292;&#20851;&#27880;&#23398;&#20064;&#36866;&#24403;&#34920;&#31034;&#30340;&#25361;&#25112;&#21644;&#23454;&#36341;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20174;&#23637;&#31034;&#22914;&#20309;&#22312;&#23545;&#21508;&#31181;&#31867;&#22411;&#30340;&#29305;&#24449;&#36827;&#34892;&#23884;&#20837;&#34920;&#31034;&#26102;&#20445;&#30041;&#20808;&#39564;&#24320;&#22987;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#24207;&#21015;&#29305;&#24449;&#12289;&#25968;&#20540;&#29305;&#24449;&#12289;&#39044;&#35757;&#32451;&#23884;&#20837;&#29305;&#24449;&#20197;&#21450;&#31232;&#30095;ID&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#19982;&#29305;&#24449;&#34920;&#31034;&#30456;&#20851;&#30340;&#20004;&#20010;&#20851;&#38190;&#25361;&#25112;&#65306;&#23884;&#20837;&#30340;&#32500;&#24230;&#22349;&#32553;&#21644;&#36328;&#22810;&#20010;&#20219;&#21153;&#25110;&#22330;&#26223;&#30340;&#20852;&#36259;&#32416;&#32544;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#31181;&#23454;&#29992;&#26041;&#27861;&#26469;&#26377;&#25928;&#24212;&#23545;&#36825;&#20004;&#20010;&#25361;&#25112;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20960;&#31181;&#35757;&#32451;&#25216;&#26415;&#65292;&#20197;&#20419;&#36827;&#27169;&#22411;&#20248;&#21270;&#65292;&#20943;&#23569;&#20559;&#24046;&#24182;&#22686;&#24378;&#25506;&#32034;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19977;&#31181;&#20998;&#26512;&#24037;&#20855;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#20840;&#38754;&#30740;&#31350;&#29305;&#24449;&#30456;&#20851;&#24615;&#12289;&#32500;&#24230;&#22349;&#32553;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00793v1 Announce Type: cross  Abstract: In this paper, we present an industry ad recommendation system, paying attention to the challenges and practices of learning appropriate representations. Our study begins by showcasing our approaches to preserving priors when encoding features of diverse types into embedding representations. Specifically, we address sequence features, numeric features, pre-trained embedding features, as well as sparse ID features. Moreover, we delve into two pivotal challenges associated with feature representation: the dimensional collapse of embeddings and the interest entanglement across various tasks or scenarios. Subsequently, we propose several practical approaches to effectively tackle these two challenges. We then explore several training techniques to facilitate model optimization, reduce bias, and enhance exploration. Furthermore, we introduce three analysis tools that enable us to comprehensively study feature correlation, dimensional collap
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;ContrastGeo&#26694;&#26550;&#65292;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#21152;&#24378;&#20102;&#23569;&#26679;&#26412;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#22320;&#29702;&#23450;&#20301;&#65292;&#36890;&#36807;&#24341;&#20837;Tweet-Location&#23545;&#27604;&#23398;&#20064;&#21644;&#21305;&#37197;&#30446;&#26631;&#65292;&#24182;&#37319;&#29992;&#22312;&#32447;&#22256;&#38590;&#36127;&#26679;&#26412;&#25366;&#25496;&#26041;&#27861;&#26469;&#25429;&#25417;&#24086;&#23376;&#21644;&#20301;&#32622;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;</title><link>https://arxiv.org/abs/2403.00786</link><description>&lt;p&gt;
&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#36827;&#34892;&#23569;&#26679;&#26412;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#22320;&#29702;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
Leveraging Contrastive Learning for Few-shot Geolocation of Social Posts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00786
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;ContrastGeo&#26694;&#26550;&#65292;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#21152;&#24378;&#20102;&#23569;&#26679;&#26412;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#22320;&#29702;&#23450;&#20301;&#65292;&#36890;&#36807;&#24341;&#20837;Tweet-Location&#23545;&#27604;&#23398;&#20064;&#21644;&#21305;&#37197;&#30446;&#26631;&#65292;&#24182;&#37319;&#29992;&#22312;&#32447;&#22256;&#38590;&#36127;&#26679;&#26412;&#25366;&#25496;&#26041;&#27861;&#26469;&#25429;&#25417;&#24086;&#23376;&#21644;&#20301;&#32622;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#22320;&#29702;&#23450;&#20301;&#26159;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#65292;&#21363;&#39044;&#27979;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#30340;&#26469;&#28304;&#22320;&#28857;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38656;&#35201;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#65292;&#20197;&#21450;&#23436;&#21892;&#30340;&#26631;&#27880;&#26631;&#31614;&#65292;&#36825;&#20010;&#20219;&#21153;&#24456;&#20855;&#25361;&#25112;&#24615;&#12290;&#36825;&#20123;&#38382;&#39064;&#36827;&#19968;&#27493;&#24694;&#21270;&#65292;&#22240;&#20026;&#26032;&#30340;&#25110;&#19981;&#22826;&#28909;&#38376;&#30340;&#22320;&#28857;&#32570;&#20047;&#36275;&#22815;&#30340;&#26631;&#31614;&#65292;&#23548;&#33268;&#25968;&#25454;&#38598;&#19981;&#24179;&#34913;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ContrastGeo&#65292;&#36825;&#26159;&#19968;&#20010;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#22686;&#24378;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23569;&#26679;&#26412;&#31038;&#20132;&#22320;&#29702;&#23450;&#20301;&#12290;&#20855;&#20307;&#22320;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;Tweet-Location&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#65292;&#20197;&#22312;&#24086;&#23376;-&#20301;&#32622;&#23545;&#20043;&#38388;&#23545;&#40784;&#34920;&#31034;&#12290;&#20026;&#20102;&#25429;&#25417;&#24086;&#23376;&#21644;&#20301;&#32622;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#36827;&#19968;&#27493;&#37319;&#29992;&#20102;Tweet-Location&#21305;&#37197;&#30446;&#26631;&#65292;&#24182;&#36890;&#36807;&#22312;&#32447;&#22256;&#38590;&#36127;&#26679;&#26412;&#25366;&#25496;&#26041;&#27861;&#36827;&#34892;&#20102;&#25913;&#36827;&#12290;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19977;&#31181;&#34701;&#21512;&#31574;&#30053;&#65292;&#37319;&#29992;&#19981;&#21516;&#30340;&#34701;&#21512;&#32534;&#30721;&#22120;&#65292;&#20197;&#26356;&#22909;&#22320;&#29983;&#25104;&#32852;&#21512;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00786v1 Announce Type: new  Abstract: Social geolocation is an important problem of predicting the originating locations of social media posts. However, this task is challenging due to the need for a substantial volume of training data, alongside well-annotated labels. These issues are further exacerbated by new or less popular locations with insufficient labels, further leading to an imbalanced dataset. In this paper, we propose \textbf{ContrastGeo}, a \textbf{Contrast}ive learning enhanced framework for few-shot social \textbf{Geo}location. Specifically, a Tweet-Location Contrastive learning objective is introduced to align representations of tweets and locations within tweet-location pairs. To capture the correlations between tweets and locations, a Tweet-Location Matching objective is further adopted into the framework and refined via an online hard negative mining approach. We also develop three fusion strategies with various fusion encoders to better generate joint rep
&lt;/p&gt;</description></item><item><title>BERT&#30340;&#24341;&#20837;&#20026;&#20449;&#24687;&#26816;&#32034;&#39046;&#22495;&#24102;&#26469;&#20102;&#31361;&#30772;&#65292;&#30740;&#31350;&#32773;&#20204;&#23558;&#20854;&#24212;&#29992;&#20110;&#35299;&#20915;&#23454;&#38469;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#32508;&#21512;&#20998;&#26512;&#20854;&#22312;&#20449;&#24687;&#26816;&#32034;&#20013;&#30340;&#24212;&#29992;&#26041;&#27861;&#65292;&#20026;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#25552;&#20379;&#20102;&#26377;&#30410;&#30340;&#21442;&#32771;&#12290;</title><link>https://arxiv.org/abs/2403.00784</link><description>&lt;p&gt;
&#21033;&#29992;BERT&#36827;&#34892;&#20449;&#24687;&#26816;&#32034;&#65306;&#35843;&#30740;&#12289;&#24212;&#29992;&#12289;&#36164;&#28304;&#21644;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Utilizing BERT for Information Retrieval: Survey, Applications, Resources, and Challenges
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00784
&lt;/p&gt;
&lt;p&gt;
BERT&#30340;&#24341;&#20837;&#20026;&#20449;&#24687;&#26816;&#32034;&#39046;&#22495;&#24102;&#26469;&#20102;&#31361;&#30772;&#65292;&#30740;&#31350;&#32773;&#20204;&#23558;&#20854;&#24212;&#29992;&#20110;&#35299;&#20915;&#23454;&#38469;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#32508;&#21512;&#20998;&#26512;&#20854;&#22312;&#20449;&#24687;&#26816;&#32034;&#20013;&#30340;&#24212;&#29992;&#26041;&#27861;&#65292;&#20026;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#25552;&#20379;&#20102;&#26377;&#30410;&#30340;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#22312;&#35299;&#20915;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#38382;&#39064;&#26041;&#38754;&#24471;&#21040;&#20102;&#26174;&#33879;&#22686;&#38271;&#12290;&#26368;&#21021;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21463;&#21040;&#23427;&#20204;&#39034;&#24207;&#25110;&#21333;&#21521;&#24615;&#36136;&#30340;&#38480;&#21046;&#65292;&#22240;&#27492;&#38590;&#20197;&#25429;&#25417;&#25991;&#26412;&#36755;&#20837;&#20043;&#38388;&#30340;&#19978;&#19979;&#25991;&#20851;&#31995;&#12290;&#20174;&#21464;&#21387;&#22120;&#65288;BERT&#65289;&#20013;&#24341;&#20837;&#30340;&#21452;&#21521;&#32534;&#30721;&#22120;&#34920;&#24449;&#25552;&#20379;&#20102;&#21464;&#21387;&#22120;&#27169;&#22411;&#30340;&#24378;&#22823;&#32534;&#30721;&#22120;&#65292;&#21487;&#20197;&#29702;&#35299;&#26356;&#24191;&#27867;&#30340;&#19978;&#19979;&#25991;&#65292;&#24182;&#22312;&#21508;&#31181;NLP&#20219;&#21153;&#20013;&#33719;&#24471;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#36825;&#28608;&#21457;&#20102;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#23558;BERT&#24212;&#29992;&#20110;&#23454;&#38469;&#38382;&#39064;&#65292;&#22914;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#12290;&#22240;&#27492;&#65292;&#19968;&#39033;&#20851;&#27880;&#23558;&#39044;&#35757;&#32451;&#30340;&#21464;&#21387;&#22120;&#32534;&#30721;&#22120;&#22914;BERT&#24212;&#29992;&#20110;IR&#30340;&#26222;&#36941;&#26041;&#27861;&#30340;&#32508;&#21512;&#20998;&#26512;&#30340;&#35843;&#26597;&#23545;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#37117;&#26377;&#29992;&#12290;&#37492;&#20110;&#27492;&#65292;&#26412;&#35843;&#26597;&#37325;&#26032;&#23457;&#35270;&#20102;&#21508;&#31181;&#22522;&#20110;BERT&#30340;&#26041;&#27861;&#65292;&#28085;&#30422;&#20102;&#21508;&#31181;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00784v1 Announce Type: cross  Abstract: Recent years have witnessed a substantial increase in the use of deep learning to solve various natural language processing (NLP) problems. Early deep learning models were constrained by their sequential or unidirectional nature, such that they struggled to capture the contextual relationships across text inputs. The introduction of bidirectional encoder representations from transformers (BERT) leads to a robust encoder for the transformer model that can understand the broader context and deliver state-of-the-art performance across various NLP tasks. This has inspired researchers and practitioners to apply BERT to practical problems, such as information retrieval (IR). A survey that focuses on a comprehensive analysis of prevalent approaches that apply pretrained transformer encoders like BERT to IR can thus be useful for academia and the industry. In light of this, we revisit a variety of BERT-based methods in this survey, cover a wid
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;ChatDiet&#65292;&#19968;&#20010;&#20511;&#21161;LLM&#25216;&#26415;&#26500;&#24314;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#24110;&#21161;&#20010;&#24615;&#21270;&#33829;&#20859;&#23548;&#21521;&#39135;&#21697;&#25512;&#33616;&#32842;&#22825;&#26426;&#22120;&#20154;&#25552;&#20379;&#20010;&#24615;&#21270;&#21644;&#21487;&#35299;&#37322;&#30340;&#25512;&#33616;&#12290;</title><link>https://arxiv.org/abs/2403.00781</link><description>&lt;p&gt;
ChatDiet&#65306;&#36890;&#36807;LLM&#22686;&#24378;&#26694;&#26550;&#36171;&#33021;&#20010;&#24615;&#21270;&#33829;&#20859;&#23548;&#21521;&#39135;&#21697;&#25512;&#33616;&#32842;&#22825;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
ChatDiet: Empowering Personalized Nutrition-Oriented Food Recommender Chatbots through an LLM-Augmented Framework
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00781
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;ChatDiet&#65292;&#19968;&#20010;&#20511;&#21161;LLM&#25216;&#26415;&#26500;&#24314;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#24110;&#21161;&#20010;&#24615;&#21270;&#33829;&#20859;&#23548;&#21521;&#39135;&#21697;&#25512;&#33616;&#32842;&#22825;&#26426;&#22120;&#20154;&#25552;&#20379;&#20010;&#24615;&#21270;&#21644;&#21487;&#35299;&#37322;&#30340;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39135;&#29289;&#23545;&#20581;&#24247;&#30340;&#28145;&#36828;&#24433;&#21709;&#20351;&#24471;&#20808;&#36827;&#30340;&#33829;&#20859;&#23548;&#21521;&#39135;&#21697;&#25512;&#33616;&#26381;&#21153;&#25104;&#20026;&#24517;&#35201;&#12290;&#20256;&#32479;&#26041;&#27861;&#24448;&#24448;&#32570;&#20047;&#20010;&#24615;&#21270;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#20114;&#21160;&#24615;&#31561;&#20851;&#38190;&#20803;&#32032;&#12290;&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24102;&#26469;&#20102;&#35299;&#37322;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#20294;&#23427;&#20204;&#21333;&#29420;&#30340;&#20351;&#29992;&#26410;&#33021;&#23454;&#29616;&#30495;&#27491;&#30340;&#20010;&#24615;&#21270;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;ChatDiet&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;LLM&#39537;&#21160;&#26694;&#26550;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#20010;&#24615;&#21270;&#33829;&#20859;&#23548;&#21521;&#39135;&#21697;&#25512;&#33616;&#32842;&#22825;&#26426;&#22120;&#20154;&#12290;ChatDiet&#38598;&#25104;&#20102;&#20010;&#20154;&#21644;&#20154;&#32676;&#27169;&#22411;&#65292;&#36741;&#20197;&#19968;&#20010;&#21327;&#35843;&#22120;&#65292;&#26080;&#32541;&#26816;&#32034;&#21644;&#22788;&#29702;&#30456;&#20851;&#20449;&#24687;&#12290;&#20854;&#32467;&#26524;&#26159;&#21160;&#24577;&#25552;&#20379;&#20010;&#24615;&#21270;&#21644;&#21487;&#35299;&#37322;&#30340;&#39135;&#21697;&#25512;&#33616;&#65292;&#26681;&#25454;&#20010;&#20154;&#29992;&#25143;&#21916;&#22909;&#23450;&#21046;&#12290;&#25105;&#20204;&#23545;ChatDiet&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21253;&#25324;&#19968;&#20010;&#24341;&#20154;&#20837;&#32988;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#22312;&#26696;&#20363;&#30740;&#31350;&#20013;&#24314;&#31435;&#20102;&#19968;&#20010;&#22240;&#26524;&#20010;&#20154;&#27169;&#22411;&#26469;&#20272;&#35745;&#20010;&#20154;&#33829;&#20859;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00781v1 Announce Type: cross  Abstract: The profound impact of food on health necessitates advanced nutrition-oriented food recommendation services. Conventional methods often lack the crucial elements of personalization, explainability, and interactivity. While Large Language Models (LLMs) bring interpretability and explainability, their standalone use falls short of achieving true personalization. In this paper, we introduce ChatDiet, a novel LLM-powered framework designed specifically for personalized nutrition-oriented food recommendation chatbots. ChatDiet integrates personal and population models, complemented by an orchestrator, to seamlessly retrieve and process pertinent information. The result is a dynamic delivery of personalized and explainable food recommendations, tailored to individual user preferences. Our evaluation of ChatDiet includes a compelling case study, where we establish a causal personal model to estimate individual nutrition effects. Our assessmen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31995;&#32479;&#27010;&#36848;&#20102;&#24403;&#21069;&#25945;&#32946;&#25991;&#26412;&#25366;&#25496;&#39046;&#22495;&#30340;&#29616;&#29366;&#65292;&#26088;&#22312;&#22238;&#31572;&#25945;&#32946;&#29615;&#22659;&#20013;&#26368;&#24120;&#29992;&#30340;&#25991;&#26412;&#25366;&#25496;&#25216;&#26415;&#12289;&#26368;&#24120;&#29992;&#30340;&#25945;&#32946;&#36164;&#28304;&#20197;&#21450;&#20027;&#35201;&#30340;&#24212;&#29992;&#25110;&#25945;&#32946;&#30446;&#26631;&#65292;&#21516;&#26102;&#27010;&#36848;&#20102;&#32467;&#35770;&#21644;&#26410;&#26469;&#36235;&#21183;&#12290;</title><link>https://arxiv.org/abs/2403.00769</link><description>&lt;p&gt;
&#22312;&#25945;&#32946;&#39046;&#22495;&#30340;&#25991;&#26412;&#25366;&#25496;
&lt;/p&gt;
&lt;p&gt;
Text mining in education
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00769
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31995;&#32479;&#27010;&#36848;&#20102;&#24403;&#21069;&#25945;&#32946;&#25991;&#26412;&#25366;&#25496;&#39046;&#22495;&#30340;&#29616;&#29366;&#65292;&#26088;&#22312;&#22238;&#31572;&#25945;&#32946;&#29615;&#22659;&#20013;&#26368;&#24120;&#29992;&#30340;&#25991;&#26412;&#25366;&#25496;&#25216;&#26415;&#12289;&#26368;&#24120;&#29992;&#30340;&#25945;&#32946;&#36164;&#28304;&#20197;&#21450;&#20027;&#35201;&#30340;&#24212;&#29992;&#25110;&#25945;&#32946;&#30446;&#26631;&#65292;&#21516;&#26102;&#27010;&#36848;&#20102;&#32467;&#35770;&#21644;&#26410;&#26469;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#25945;&#32946;&#29615;&#22659;&#30340;&#36805;&#29467;&#22686;&#38271;&#20135;&#29983;&#20102;&#22823;&#37327;&#25968;&#25454;&#65292;&#29305;&#21035;&#26159;&#26469;&#33258;&#35770;&#22363;&#12289;&#32842;&#22825;&#12289;&#31038;&#20132;&#32593;&#32476;&#12289;&#35780;&#20272;&#12289;&#35770;&#25991;&#31561;&#25991;&#26412;&#26684;&#24335;&#30340;&#25968;&#25454;&#12290;&#22914;&#20309;&#25366;&#25496;&#25991;&#26412;&#25968;&#25454;&#20197;&#25214;&#21040;&#23545;&#25945;&#32946;&#30456;&#20851;&#20154;&#21592;&#26377;&#29992;&#30340;&#30693;&#35782;&#65292;&#26159;&#19968;&#20010;&#28608;&#21160;&#20154;&#24515;&#30340;&#25361;&#25112;&#12290;&#23613;&#31649;&#26368;&#36817;&#24050;&#21457;&#34920;&#20102;&#36234;&#26469;&#36234;&#22810;&#24212;&#29992;&#25991;&#26412;&#25366;&#25496;&#20110;&#25945;&#32946;&#39046;&#22495;&#30340;&#25991;&#31456;&#65292;&#20294;&#25105;&#20204;&#23578;&#26410;&#25214;&#21040;&#20219;&#20309;&#32508;&#36848;&#36825;&#20123;&#24037;&#20316;&#30340;&#35770;&#25991;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#23545;&#24403;&#21069;&#25945;&#32946;&#25991;&#26412;&#25366;&#25496;&#39046;&#22495;&#30340;&#29616;&#29366;&#36827;&#34892;&#20102;&#31995;&#32479;&#27010;&#36848;&#12290;&#25105;&#20204;&#30340;&#26368;&#32456;&#30446;&#26631;&#26159;&#22238;&#31572;&#19977;&#20010;&#20027;&#35201;&#30740;&#31350;&#38382;&#39064;&#65306;&#22312;&#25945;&#32946;&#29615;&#22659;&#20013;&#26368;&#24120;&#29992;&#30340;&#25991;&#26412;&#25366;&#25496;&#25216;&#26415;&#26159;&#20160;&#20040;&#65311;&#26368;&#24120;&#29992;&#30340;&#25945;&#32946;&#36164;&#28304;&#26159;&#20160;&#20040;&#65311;&#20027;&#35201;&#24212;&#29992;&#25110;&#25945;&#32946;&#30446;&#26631;&#26159;&#20160;&#20040;&#65311;&#26368;&#21518;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#32467;&#35770;&#21644;&#26356;&#26377;&#36259;&#30340;&#26410;&#26469;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00769v1 Announce Type: cross  Abstract: The explosive growth of online education environments is generating a massive volume of data, specially in text format from forums, chats, social networks, assessments, essays, among others. It produces exciting challenges on how to mine text data in order to find useful knowledge for educational stakeholders. Despite the increasing number of educational applications of text mining published recently, we have not found any paper surveying them. In this line, this work presents a systematic overview of the current status of the Educational Text Mining field. Our final goal is to answer three main research questions: Which are the text mining techniques most used in educational environments? Which are the most used educational resources? And which are the main applications or educational goals? Finally, we outline the conclusions and the more interesting future trends.
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#25512;&#33616;&#33021;&#21147;&#21644;&#35821;&#35328;&#29702;&#35299;&#65292;&#37325;&#22609;&#20102;&#25512;&#33616;&#20219;&#21153;&#30340;&#22522;&#30784;&#12290;</title><link>https://arxiv.org/abs/2402.18590</link><description>&lt;p&gt;
&#25506;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#25512;&#33616;&#31995;&#32479;&#30340;&#24433;&#21709;&#65306;&#19968;&#39033;&#24191;&#27867;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Exploring the Impact of Large Language Models on Recommender Systems: An Extensive Review
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18590
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#25512;&#33616;&#33021;&#21147;&#21644;&#35821;&#35328;&#29702;&#35299;&#65292;&#37325;&#22609;&#20102;&#25512;&#33616;&#20219;&#21153;&#30340;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24378;&#35843;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#37325;&#22609;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#23558;&#23427;&#20204;&#30340;&#20215;&#20540;&#24402;&#22240;&#20110;&#20256;&#32479;&#25512;&#33616;&#31995;&#32479;&#25152;&#32570;&#20047;&#30340;&#29420;&#29305;&#25512;&#29702;&#33021;&#21147;&#12290;&#19981;&#21516;&#20110;&#32570;&#20047;&#30452;&#25509;&#29992;&#25143;&#20114;&#21160;&#25968;&#25454;&#30340;&#20256;&#32479;&#31995;&#32479;&#65292;LLMs&#22312;&#25512;&#33616;&#29289;&#21697;&#26041;&#38754;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#29702;&#35299;&#35821;&#35328;&#22797;&#26434;&#24615;&#26041;&#38754;&#30340;&#29087;&#32451;&#31243;&#24230;&#12290;&#36825;&#26631;&#24535;&#30528;&#25512;&#33616;&#39046;&#22495;&#30340;&#19968;&#20010;&#26681;&#26412;&#24615;&#33539;&#24335;&#36716;&#21464;&#12290;&#22312;&#20805;&#28385;&#27963;&#21147;&#30340;&#30740;&#31350;&#39046;&#22495;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#31215;&#26497;&#21033;&#29992;LLMs&#30340;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#37325;&#26032;&#23450;&#20041;&#25512;&#33616;&#20219;&#21153;&#30340;&#22522;&#30784;&#12290;&#35813;&#30740;&#31350;&#24443;&#24213;&#25506;&#35752;&#20102;LLMs&#22312;&#25512;&#33616;&#26694;&#26550;&#20869;&#22266;&#26377;&#30340;&#20248;&#21183;&#65292;&#21253;&#25324;&#32454;&#33268;&#30340;&#35821;&#22659;&#29702;&#35299;&#65292;&#36328;&#19981;&#21516;&#39046;&#22495;&#30340;&#24179;&#31283;&#36807;&#28193;&#65292;&#37319;&#29992;&#32479;&#19968;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20849;&#20139;&#25968;&#25454;&#27744;&#30340;&#20840;&#38754;&#23398;&#20064;&#31574;&#30053;&#65292;&#36879;&#26126;&#24230;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18590v1 Announce Type: cross  Abstract: The paper underscores the significance of Large Language Models (LLMs) in reshaping recommender systems, attributing their value to unique reasoning abilities absent in traditional recommenders. Unlike conventional systems lacking direct user interaction data, LLMs exhibit exceptional proficiency in recommending items, showcasing their adeptness in comprehending intricacies of language. This marks a fundamental paradigm shift in the realm of recommendations. Amidst the dynamic research landscape, researchers actively harness the language comprehension and generation capabilities of LLMs to redefine the foundations of recommendation tasks. The investigation thoroughly explores the inherent strengths of LLMs within recommendation frameworks, encompassing nuanced contextual comprehension, seamless transitions across diverse domains, adoption of unified approaches, holistic learning strategies leveraging shared data reservoirs, transparent
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#30340;&#26032;&#27010;&#24565;&#25554;&#20837;&#21040;&#26412;&#20307;&#20013;&#65292;&#22312;&#36793;&#25628;&#32034;&#12289;&#36793;&#24418;&#25104;&#21644;&#22686;&#24378;&#12289;&#36793;&#36873;&#25321;&#19977;&#20010;&#27493;&#39588;&#20013;&#20998;&#21035;&#21033;&#29992;&#31070;&#32463;&#26041;&#27861;&#65292;&#24182;&#22312; SNOMED CT &#26412;&#20307;&#21644; MedMentions &#23454;&#20307;&#38142;&#25509;&#22522;&#20934;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;</title><link>https://arxiv.org/abs/2402.17897</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#26412;&#20307;&#35770;&#20013;&#26032;&#27010;&#24565;&#25918;&#32622;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Language Model based Framework for New Concept Placement in Ontologies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17897
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#30340;&#26032;&#27010;&#24565;&#25554;&#20837;&#21040;&#26412;&#20307;&#20013;&#65292;&#22312;&#36793;&#25628;&#32034;&#12289;&#36793;&#24418;&#25104;&#21644;&#22686;&#24378;&#12289;&#36793;&#36873;&#25321;&#19977;&#20010;&#27493;&#39588;&#20013;&#20998;&#21035;&#21033;&#29992;&#31070;&#32463;&#26041;&#27861;&#65292;&#24182;&#22312; SNOMED CT &#26412;&#20307;&#21644; MedMentions &#23454;&#20307;&#38142;&#25509;&#22522;&#20934;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#23558;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#30340;&#26032;&#27010;&#24565;&#25554;&#20837;&#26412;&#20307;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#20010;&#19977;&#27493;&#26041;&#27861;&#65306;&#36793;&#25628;&#32034;&#65292;&#21363;&#25214;&#21040;&#35201;&#25554;&#20837;&#30340;&#20505;&#36873;&#20301;&#32622;&#38598;&#65288;&#21363;&#27010;&#24565;&#20043;&#38388;&#30340;&#21253;&#21547;&#20851;&#31995;&#65289;&#65292;&#36793;&#24418;&#25104;&#21644;&#22686;&#24378;&#65292;&#21033;&#29992;&#26412;&#20307;&#32467;&#26500;&#29983;&#25104;&#21644;&#22686;&#24378;&#36793;&#20505;&#36873;&#65292;&#20197;&#21450;&#36793;&#36873;&#25321;&#65292;&#26368;&#32456;&#30830;&#23450;&#35201;&#25918;&#32622;&#30340;&#36793;&#12290;&#22312;&#25152;&#26377;&#27493;&#39588;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#31070;&#32463;&#26041;&#27861;&#65292;&#20854;&#20013;&#24212;&#29992;&#22522;&#20110;&#23884;&#20837;&#30340;&#26041;&#27861;&#21644;&#23545;&#27604;&#23398;&#20064;&#65292;&#22914;BERT&#29992;&#20110;&#36793;&#25628;&#32034;&#65292;&#37319;&#29992;&#22522;&#20110;BERT&#24494;&#35843;&#30340;&#22810;&#26631;&#31614;&#36793;&#20132;&#21449;&#32534;&#30721;&#22120;&#65292;&#20197;&#21450;GPT&#31995;&#21015;&#12289;FLAN-T5 &#21644; Llama 2 &#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#29992;&#20110;&#36793;&#36873;&#25321;&#12290;&#25105;&#20204;&#22312;&#20351;&#29992; SNOMED CT &#26412;&#20307;&#21644; MedMentions &#23454;&#20307;&#38142;&#25509;&#22522;&#20934;&#21019;&#24314;&#30340;&#26368;&#26032;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#36825;&#20123;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17897v1 Announce Type: new  Abstract: We investigate the task of inserting new concepts extracted from texts into an ontology using language models. We explore an approach with three steps: edge search which is to find a set of candidate locations to insert (i.e., subsumptions between concepts), edge formation and enrichment which leverages the ontological structure to produce and enhance the edge candidates, and edge selection which eventually locates the edge to be placed into. In all steps, we propose to leverage neural methods, where we apply embedding-based methods and contrastive learning with Pre-trained Language Models (PLMs) such as BERT for edge search, and adapt a BERT fine-tuning-based multi-label Edge-Cross-encoder, and Large Language Models (LLMs) such as GPT series, FLAN-T5, and Llama 2, for edge selection. We evaluate the methods on recent datasets created using the SNOMED CT ontology and the MedMentions entity linking benchmark. The best settings in our fram
&lt;/p&gt;</description></item><item><title>JMLR&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#21307;&#23398;&#39046;&#22495;&#25552;&#39640;&#38382;&#39064;&#22238;&#31572;&#31995;&#32479;&#24615;&#33021;&#65292;&#38477;&#20302;&#35745;&#31639;&#36164;&#28304;&#38656;&#27714;&#65292;&#22686;&#24378;&#27169;&#22411;&#21033;&#29992;&#21307;&#30103;&#30693;&#35782;&#36827;&#34892;&#25512;&#29702;&#21644;&#22238;&#31572;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.17887</link><description>&lt;p&gt;
JMLR&#65306;&#32852;&#21512;&#21307;&#30103;LLM&#21644;&#26816;&#32034;&#35757;&#32451;&#20197;&#22686;&#24378;&#25512;&#29702;&#21644;&#19987;&#19994;&#38382;&#39064;&#22238;&#31572;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
JMLR: Joint Medical LLM and Retrieval Training for Enhancing Reasoning and Professional Question Answering Capability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17887
&lt;/p&gt;
&lt;p&gt;
JMLR&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#21307;&#23398;&#39046;&#22495;&#25552;&#39640;&#38382;&#39064;&#22238;&#31572;&#31995;&#32479;&#24615;&#33021;&#65292;&#38477;&#20302;&#35745;&#31639;&#36164;&#28304;&#38656;&#27714;&#65292;&#22686;&#24378;&#27169;&#22411;&#21033;&#29992;&#21307;&#30103;&#30693;&#35782;&#36827;&#34892;&#25512;&#29702;&#21644;&#22238;&#31572;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#21307;&#30103;&#25968;&#25454;&#30340;&#29190;&#28856;&#24615;&#22686;&#38271;&#21644;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#31934;&#20934;&#21307;&#23398;&#24050;&#32463;&#25104;&#20026;&#22686;&#24378;&#21307;&#30103;&#26381;&#21153;&#36136;&#37327;&#21644;&#25928;&#29575;&#30340;&#20851;&#38190;&#12290;&#22312;&#36825;&#31181;&#32972;&#26223;&#19979;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21307;&#30103;&#30693;&#35782;&#33719;&#21462;&#21644;&#38382;&#39064;&#22238;&#31572;&#31995;&#32479;&#20013;&#21457;&#25381;&#36234;&#26469;&#36234;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#36825;&#20123;&#31995;&#32479;&#22312;&#21307;&#23398;&#39046;&#22495;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#22312;&#24494;&#35843;&#38454;&#27573;&#21516;&#26102;&#35757;&#32451;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#31995;&#32479;&#21644;LLM&#12290;&#25105;&#20204;&#31216;&#20043;&#20026;&#32852;&#21512;&#21307;&#30103;LLM&#21644;&#26816;&#32034;&#35757;&#32451;&#65288;JMLR&#65289;&#30340;&#26041;&#27861;&#26088;&#22312;&#20811;&#26381;&#20256;&#32479;&#27169;&#22411;&#22312;&#22788;&#29702;&#21307;&#23398;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#26102;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#37319;&#29992;&#21516;&#27493;&#35757;&#32451;&#26426;&#21046;&#65292;JMLR&#20943;&#23569;&#20102;&#23545;&#35745;&#31639;&#36164;&#28304;&#30340;&#38656;&#27714;&#65292;&#24182;&#22686;&#24378;&#20102;&#27169;&#22411;&#21033;&#29992;&#21307;&#30103;&#30693;&#35782;&#36827;&#34892;&#25512;&#29702;&#21644;&#22238;&#31572;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17887v1 Announce Type: new  Abstract: With the explosive growth of medical data and the rapid development of artificial intelligence technology, precision medicine has emerged as a key to enhancing the quality and efficiency of healthcare services. In this context, Large Language Models (LLMs) play an increasingly vital role in medical knowledge acquisition and question-answering systems. To further improve the performance of these systems in the medical domain, we introduce an innovative method that jointly trains an Information Retrieval (IR) system and an LLM during the fine-tuning phase. This approach, which we call Joint Medical LLM and Retrieval Training (JMLR), is designed to overcome the challenges faced by traditional models in handling medical question-answering tasks. By employing a synchronized training mechanism, JMLR reduces the demand for computational resources and enhances the model's ability to leverage medical knowledge for reasoning and answering question
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;Prompt-Tuning&#36171;&#33021;&#30340;PromptMM&#22810;&#27169;&#24335;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#29992;&#20110;&#31616;&#21270;&#21644;&#22686;&#24378;&#25512;&#33616;&#31995;&#32479;&#65292;&#23454;&#29616;&#33258;&#36866;&#24212;&#30340;&#36136;&#37327;&#33976;&#39311;&#12290;</title><link>https://arxiv.org/abs/2402.17188</link><description>&lt;p&gt;
PromptMM&#65306;&#22810;&#27169;&#24335;&#30693;&#35782;&#33976;&#39311;&#29992;&#20110;&#22522;&#20110;Prompt-Tuning&#30340;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
PromptMM: Multi-Modal Knowledge Distillation for Recommendation with Prompt-Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17188
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;Prompt-Tuning&#36171;&#33021;&#30340;PromptMM&#22810;&#27169;&#24335;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#29992;&#20110;&#31616;&#21270;&#21644;&#22686;&#24378;&#25512;&#33616;&#31995;&#32479;&#65292;&#23454;&#29616;&#33258;&#36866;&#24212;&#30340;&#36136;&#37327;&#33976;&#39311;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#23186;&#20307;&#22312;&#32447;&#24179;&#21488;&#65288;&#20363;&#22914;&#20122;&#39532;&#36874;&#12289;TikTok&#65289;&#36890;&#36807;&#23558;&#22810;&#23186;&#20307;&#65288;&#20363;&#22914;&#35270;&#35273;&#12289;&#25991;&#26412;&#21644;&#22768;&#23398;&#65289;&#20869;&#23481;&#32435;&#20837;&#20854;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#20013;&#33719;&#30410;&#21290;&#27973;&#12290;&#36825;&#20123;&#27169;&#24577;&#25552;&#20379;&#30452;&#35266;&#35821;&#20041;&#65292;&#26377;&#21161;&#20110;&#36827;&#34892;&#27169;&#24577;&#24863;&#30693;&#30340;&#29992;&#25143;&#20559;&#22909;&#24314;&#27169;&#12290;&#28982;&#32780;&#65292;&#22810;&#27169;&#24335;&#25512;&#33616;&#22120;&#20013;&#23384;&#22312;&#20004;&#20010;&#20851;&#38190;&#25361;&#25112;&#23578;&#26410;&#35299;&#20915;&#65306;i&#65289;&#24341;&#20837;&#20855;&#26377;&#22823;&#37327;&#39069;&#22806;&#21442;&#25968;&#30340;&#22810;&#27169;&#24335;&#32534;&#30721;&#22120;&#20250;&#23548;&#33268;&#36807;&#25311;&#21512;&#65292;&#32771;&#34385;&#21040;&#25552;&#21462;&#22120;&#65288;&#20363;&#22914;ViT&#12289;BERT&#65289;&#25552;&#20379;&#30340;&#39640;&#32500;&#22810;&#27169;&#24335;&#29305;&#24449;&#12290;ii&#65289;&#36741;&#21161;&#20449;&#24687;&#19981;&#21487;&#36991;&#20813;&#22320;&#24341;&#20837;&#19981;&#20934;&#30830;&#24615;&#21644;&#20887;&#20313;&#65292;&#23548;&#33268;&#27169;&#24577;&#20132;&#20114;&#20381;&#36182;&#20559;&#31163;&#30495;&#23454;&#29992;&#25143;&#20559;&#22909;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;Prompt-Tuning&#36171;&#33021;&#12289;&#31616;&#21270;&#25512;&#33616;&#22120;&#30340;PromptMM&#65288;&#22810;&#27169;&#24335;&#30693;&#35782;&#33976;&#39311;&#65289;&#65292;&#23454;&#29616;&#33258;&#36866;&#24212;&#36136;&#37327;&#33976;&#39311;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17188v1 Announce Type: new  Abstract: Multimedia online platforms (e.g., Amazon, TikTok) have greatly benefited from the incorporation of multimedia (e.g., visual, textual, and acoustic) content into their personal recommender systems. These modalities provide intuitive semantics that facilitate modality-aware user preference modeling. However, two key challenges in multi-modal recommenders remain unresolved: i) The introduction of multi-modal encoders with a large number of additional parameters causes overfitting, given high-dimensional multi-modal features provided by extractors (e.g., ViT, BERT). ii) Side information inevitably introduces inaccuracies and redundancies, which skew the modality-interaction dependency from reflecting true user preference. To tackle these problems, we propose to simplify and empower recommenders through Multi-modal Knowledge Distillation (PromptMM) with the prompt-tuning that enables adaptive quality distillation. Specifically, PromptMM cond
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Brain-Aug&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#33041;&#20449;&#21495;&#20013;&#35299;&#30721;&#30340;&#35821;&#20041;&#20449;&#24687;&#22686;&#24378;&#26597;&#35810;&#65292;&#21487;&#20197;&#29983;&#25104;&#26356;&#20934;&#30830;&#30340;&#26597;&#35810;&#65292;&#25913;&#21892;&#25991;&#26723;&#25490;&#24207;&#24615;&#33021;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#27169;&#31946;&#26597;&#35810;&#12290;</title><link>https://arxiv.org/abs/2402.15708</link><description>&lt;p&gt;
&#20174;&#33041;&#20449;&#21495;&#35299;&#30721;&#26597;&#35810;&#35821;&#20041;&#30340;&#26597;&#35810;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
Query Augmentation by Decoding Semantics from Brain Signals
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15708
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Brain-Aug&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#33041;&#20449;&#21495;&#20013;&#35299;&#30721;&#30340;&#35821;&#20041;&#20449;&#24687;&#22686;&#24378;&#26597;&#35810;&#65292;&#21487;&#20197;&#29983;&#25104;&#26356;&#20934;&#30830;&#30340;&#26597;&#35810;&#65292;&#25913;&#21892;&#25991;&#26723;&#25490;&#24207;&#24615;&#33021;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#27169;&#31946;&#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26597;&#35810;&#25193;&#23637;&#26159;&#29992;&#20110;&#32454;&#21270;&#35821;&#20041;&#19981;&#20934;&#30830;&#26597;&#35810;&#30340;&#20851;&#38190;&#25216;&#26415;&#12290;&#20256;&#32479;&#19978;&#65292;&#26597;&#35810;&#25193;&#23637;&#20381;&#36182;&#20110;&#20174;&#26368;&#21021;&#26816;&#32034;&#21040;&#30340;&#12289;&#28508;&#22312;&#30456;&#20851;&#30340;&#25991;&#26723;&#20013;&#25552;&#21462;&#20449;&#24687;&#12290;&#22914;&#26524;&#26368;&#21021;&#26816;&#32034;&#21040;&#30340;&#25991;&#26723;&#36136;&#37327;&#36739;&#20302;&#65292;&#21017;&#26597;&#35810;&#25193;&#23637;&#30340;&#26377;&#25928;&#24615;&#20063;&#20250;&#21463;&#21040;&#38480;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Brain-Aug&#65292;&#36890;&#36807;&#23558;&#20174;&#33041;&#20449;&#21495;&#35299;&#30721;&#30340;&#35821;&#20041;&#20449;&#24687;&#32467;&#21512;&#21040;&#26597;&#35810;&#20013;&#26469;&#22686;&#24378;&#26597;&#35810;&#12290;Brain-Aug&#20351;&#29992;&#20102;&#22312;&#33041;&#20449;&#21495;&#20449;&#24687;&#26500;&#24314;&#30340;&#25552;&#31034;&#21644;&#38754;&#21521;&#25490;&#21517;&#30340;&#25512;&#29702;&#26041;&#27861;&#29983;&#25104;&#21407;&#22987;&#26597;&#35810;&#30340;&#24310;&#32493;&#37096;&#20998;&#12290;&#23545;fMRI&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;Brain-Aug&#29983;&#25104;&#30340;&#26597;&#35810;&#22312;&#35821;&#20041;&#19978;&#26356;&#20934;&#30830;&#65292;&#23548;&#33268;&#25913;&#36827;&#30340;&#25991;&#26723;&#25490;&#24207;&#24615;&#33021;&#12290;&#33041;&#20449;&#21495;&#24102;&#26469;&#30340;&#36825;&#31181;&#25913;&#36827;&#23545;&#20110;&#27169;&#31946;&#26597;&#35810;&#29305;&#21035;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15708v1 Announce Type: cross  Abstract: Query augmentation is a crucial technique for refining semantically imprecise queries. Traditionally, query augmentation relies on extracting information from initially retrieved, potentially relevant documents. If the quality of the initially retrieved documents is low, then the effectiveness of query augmentation would be limited as well. We propose Brain-Aug, which enhances a query by incorporating semantic information decoded from brain signals. BrainAug generates the continuation of the original query with a prompt constructed with brain signal information and a ranking-oriented inference approach. Experimental results on fMRI (functional magnetic resonance imaging) datasets show that Brain-Aug produces semantically more accurate queries, leading to improved document ranking performance. Such improvement brought by brain signals is particularly notable for ambiguous queries.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#24577;&#24863;&#30693;&#30340;LLM&#38598;&#25104;&#26041;&#27861;&#65288;MAIL&#65289;&#29992;&#20110;&#38024;&#23545;KVQA&#65292;&#36890;&#36807;&#32454;&#33268;&#22320;&#21033;&#29992;&#22810;&#27169;&#24577;&#30693;&#35782;&#26469;&#22788;&#29702;&#22270;&#20687;&#29702;&#35299;&#21644;&#30693;&#35782;&#25512;&#29702;&#12290;</title><link>https://arxiv.org/abs/2402.12728</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#24577;&#24863;&#30693;&#38598;&#25104;&#29992;&#20110;&#22522;&#20110;&#30693;&#35782;&#30340;&#35270;&#35273;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Modality-Aware Integration with Large Language Models for Knowledge-based Visual Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12728
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#24577;&#24863;&#30693;&#30340;LLM&#38598;&#25104;&#26041;&#27861;&#65288;MAIL&#65289;&#29992;&#20110;&#38024;&#23545;KVQA&#65292;&#36890;&#36807;&#32454;&#33268;&#22320;&#21033;&#29992;&#22810;&#27169;&#24577;&#30693;&#35782;&#26469;&#22788;&#29702;&#22270;&#20687;&#29702;&#35299;&#21644;&#30693;&#35782;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#39537;&#21160;&#30340;&#35270;&#35273;&#38382;&#31572;&#65288;KVQA&#65289;&#24050;&#34987;&#24191;&#27867;&#30740;&#31350;&#65292;&#20197;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#22914;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#26469;&#22238;&#31572;&#35270;&#35273;&#38382;&#39064;&#12290;&#23613;&#31649;&#24050;&#25552;&#20986;&#20960;&#31181;&#23581;&#35797;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#38544;&#21547;&#30693;&#35782;&#28304;&#65292;&#20294;&#30001;&#20110;LLMs&#21487;&#33021;&#29983;&#25104;&#24187;&#35273;&#65292;&#22240;&#27492;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#27492;&#22806;&#65292;&#22810;&#31181;&#30693;&#35782;&#26469;&#28304;&#65292;&#20363;&#22914;&#22270;&#20687;&#12289;&#30693;&#35782;&#22270;&#35889;&#21644;LLMs&#65292;&#19981;&#33021;&#36731;&#26131;&#23545;&#40784;&#20197;&#24212;&#23545;&#22797;&#26434;&#22330;&#26223;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;KVQA&#30340;&#26032;&#39062;&#30340;&#20855;&#26377;&#27169;&#24577;&#24863;&#30693;&#30340;LLM&#38598;&#25104;&#26041;&#27861;&#65288;MAIL&#65289;&#12290;&#23427;&#31934;&#24515;&#21033;&#29992;&#22810;&#27169;&#24577;&#30693;&#35782;&#36827;&#34892;&#22270;&#20687;&#29702;&#35299;&#21644;&#30693;&#35782;&#25512;&#29702;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#65288;i&#65289;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;LLMs&#30340;&#20004;&#38454;&#27573;&#25552;&#31034;&#31574;&#30053;&#65292;&#23558;&#22270;&#20687;&#23494;&#38598;&#22320;&#34701;&#20837;&#24102;&#26377;&#35814;&#32454;&#35270;&#35273;&#29305;&#24449;&#30340;&#22330;&#26223;&#22270;&#20013;&#65307;&#65288;ii&#65289;&#25105;&#20204;&#36890;&#36807;&#23558;&#25552;&#21040;&#30340;&#23454;&#20307;&#19982;&#22806;&#37096;&#20107;&#23454;&#32852;&#31995;&#36215;&#26469;&#26500;&#24314;&#19968;&#20010;&#32806;&#21512;&#30340;&#27010;&#24565;&#22270;&#65307;&#65288;iii&#65289;&#35774;&#35745;&#20102;&#19968;&#20010;&#23450;&#21046;&#30340;&#20266;&#23402;&#29983;&#22270;&#20013;&#20171;&#34701;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12728v1 Announce Type: cross  Abstract: Knowledge-based visual question answering (KVQA) has been extensively studied to answer visual questions with external knowledge, e.g., knowledge graphs (KGs). While several attempts have been proposed to leverage large language models (LLMs) as an implicit knowledge source, it remains challenging since LLMs may generate hallucinations. Moreover, multiple knowledge sources, e.g., images, KGs and LLMs, cannot be readily aligned for complex scenarios. To tackle these, we present a novel modality-aware integration with LLMs for KVQA (MAIL). It carefully leverages multimodal knowledge for both image understanding and knowledge reasoning. Specifically, (i) we propose a two-stage prompting strategy with LLMs to densely embody the image into a scene graph with detailed visual features; (ii) We construct a coupled concept graph by linking the mentioned entities with external facts. (iii) A tailored pseudo-siamese graph medium fusion is designe
&lt;/p&gt;</description></item><item><title>C-RAG&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#35748;&#35777;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#39118;&#38505;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#20379;&#31526;&#21512;&#39118;&#38505;&#20998;&#26512;&#21644;&#29983;&#25104;&#39118;&#38505;&#30340;&#19978;&#30028;&#65292;&#30830;&#20445;&#29983;&#25104;&#32467;&#26524;&#30340;&#21487;&#20449;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.03181</link><description>&lt;p&gt;
C-RAG: &#38024;&#23545;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#35748;&#35777;&#29983;&#25104;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
C-RAG: Certified Generation Risks for Retrieval-Augmented Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03181
&lt;/p&gt;
&lt;p&gt;
C-RAG&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#35748;&#35777;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#39118;&#38505;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#20379;&#31526;&#21512;&#39118;&#38505;&#20998;&#26512;&#21644;&#29983;&#25104;&#39118;&#38505;&#30340;&#19978;&#30028;&#65292;&#30830;&#20445;&#29983;&#25104;&#32467;&#26524;&#30340;&#21487;&#20449;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#20855;&#22791;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#23384;&#22312;&#21487;&#20449;&#24230;&#38382;&#39064;&#65292;&#22914;&#24187;&#35273;&#21644;&#38169;&#20301;&#12290;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#65288;RAG&#65289;&#34987;&#25552;&#20986;&#26469;&#22686;&#24378;&#29983;&#25104;&#32467;&#26524;&#30340;&#21487;&#20449;&#24615;&#65292;&#36890;&#36807;&#24341;&#20837;&#22806;&#37096;&#30693;&#35782;&#12290;&#20294;&#26159;&#65292;&#23545;&#20110;RAG&#27169;&#22411;&#30340;&#29983;&#25104;&#39118;&#38505;&#30340;&#29702;&#35770;&#29702;&#35299;&#23578;&#26410;&#34987;&#30740;&#31350;&#12290;&#26412;&#25991;&#22238;&#31572;&#20102;&#20197;&#19979;&#38382;&#39064;&#65306;1&#65289;RAG&#26159;&#21542;&#30830;&#23454;&#33021;&#22815;&#38477;&#20302;&#29983;&#25104;&#39118;&#38505;&#65292;2&#65289;&#22914;&#20309;&#23545;RAG&#21644;&#20256;&#32479;LLM&#30340;&#29983;&#25104;&#39118;&#38505;&#25552;&#20379;&#21487;&#35777;&#26126;&#30340;&#20445;&#35777;&#65292;&#20197;&#21450;3&#65289;&#21738;&#20123;&#20805;&#20998;&#26465;&#20214;&#20351;&#24471;RAG&#27169;&#22411;&#33021;&#22815;&#38477;&#20302;&#29983;&#25104;&#39118;&#38505;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;C-RAG&#65292;&#31532;&#19968;&#20010;&#29992;&#20110;&#35748;&#35777;RAG&#27169;&#22411;&#29983;&#25104;&#39118;&#38505;&#30340;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20026;RAG&#27169;&#22411;&#25552;&#20379;&#20102;&#31526;&#21512;&#39118;&#38505;&#20998;&#26512;&#65292;&#24182;&#30830;&#20445;&#20102;&#29983;&#25104;&#39118;&#38505;&#30340;&#19978;&#30028;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#31526;&#21512;&#29983;&#25104;&#39118;&#38505;&#12290;&#25105;&#20204;&#36824;&#23545;&#19968;&#33324;&#26377;&#30028;&#39118;&#38505;&#19979;&#30340;&#31526;&#21512;&#29983;&#25104;&#39118;&#38505;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the impressive capabilities of large language models (LLMs) across diverse applications, they still suffer from trustworthiness issues, such as hallucinations and misalignments. Retrieval-augmented language models (RAG) have been proposed to enhance the credibility of generations by grounding external knowledge, but the theoretical understandings of their generation risks remains unexplored. In this paper, we answer: 1) whether RAG can indeed lead to low generation risks, 2) how to provide provable guarantees on the generation risks of RAG and vanilla LLMs, and 3) what sufficient conditions enable RAG models to reduce generation risks. We propose C-RAG, the first framework to certify generation risks for RAG models. Specifically, we provide conformal risk analysis for RAG models and certify an upper confidence bound of generation risks, which we refer to as conformal generation risk. We also provide theoretical guarantees on conformal generation risks for general bounded risk f
&lt;/p&gt;</description></item><item><title>&#24314;&#31435;&#20102;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#35821;&#35328;&#29615;&#22659;&#20013;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#20013;&#30340;&#40065;&#26834;&#24615;&#30340;NoMIRACL&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#20010;&#34913;&#37327;&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#25351;&#26631;&#65306;&#24187;&#35273;&#29575;&#21644;&#38169;&#35823;&#29575;&#12290;</title><link>https://arxiv.org/abs/2312.11361</link><description>&lt;p&gt;
NoMIRACL: &#30693;&#36947;&#33258;&#24049;&#19981;&#30693;&#36947;&#30340;&#40065;&#26834;&#22810;&#35821;&#35328;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
NoMIRACL: Knowing When You Don't Know for Robust Multilingual Retrieval-Augmented Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.11361
&lt;/p&gt;
&lt;p&gt;
&#24314;&#31435;&#20102;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#35821;&#35328;&#29615;&#22659;&#20013;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#20013;&#30340;&#40065;&#26834;&#24615;&#30340;NoMIRACL&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#20010;&#34913;&#37327;&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#25351;&#26631;&#65306;&#24187;&#35273;&#29575;&#21644;&#38169;&#35823;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2312.11361v2 &#20844;&#21578;&#31867;&#22411;: &#26367;&#25442; &#25688;&#35201;: &#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#36890;&#36807;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#28304;&#26469;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36755;&#20986;&#19982;&#29616;&#23454;&#32852;&#31995;&#36215;&#26469;&#65292;&#20197;&#20943;&#23569;&#20107;&#23454;&#24187;&#35273;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#32570;&#20047;&#23545;&#19981;&#21516;&#35821;&#35328;&#26063;&#30340;&#20840;&#38754;&#35780;&#20272;&#65292;&#36825;&#20351;&#24471;&#24456;&#38590;&#35780;&#20272;LLM&#23545;&#22806;&#37096;&#26816;&#32034;&#30693;&#35782;&#38169;&#35823;&#30340;&#40065;&#26834;&#24615;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;NoMIRACL&#65292;&#36825;&#26159;&#19968;&#20010;&#20154;&#31867;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;RAG&#20013;LLM&#23545;18&#31181;&#22312;&#31867;&#22411;&#19978;&#22810;&#26679;&#21270;&#30340;&#35821;&#35328;&#30340;&#40065;&#26834;&#24615;&#12290;NoMIRACL&#21253;&#25324;&#19968;&#20010;&#38750;&#30456;&#20851;&#23376;&#38598;&#21644;&#19968;&#20010;&#30456;&#20851;&#23376;&#38598;&#12290;&#38750;&#30456;&#20851;&#23376;&#38598;&#20013;&#30340;&#26597;&#35810;&#21253;&#21547;&#34987;&#21028;&#26029;&#20026;&#19981;&#30456;&#20851;&#30340;&#27573;&#33853;&#65292;&#32780;&#30456;&#20851;&#23376;&#38598;&#20013;&#30340;&#26597;&#35810;&#33267;&#23569;&#21253;&#21547;&#19968;&#20010;&#34987;&#21028;&#26029;&#20026;&#30456;&#20851;&#30340;&#27573;&#33853;&#12290;&#25105;&#20204;&#20351;&#29992;&#20004;&#20010;&#25351;&#26631;&#26469;&#34913;&#37327;LLM&#30340;&#40065;&#26834;&#24615;&#65306;&#65288;i&#65289;&#24187;&#35273;&#29575;&#65292;&#34913;&#37327;&#27169;&#22411;&#20542;&#21521;&#20110;&#22312;&#38750;&#30456;&#20851;&#23376;&#38598;&#30340;&#27573;&#33853;&#20013;&#20135;&#29983;&#24187;&#35273;&#31572;&#26696;&#30340;&#31243;&#24230;&#65292;&#20197;&#21450;&#65288;ii&#65289;&#38169;&#35823;&#29575;&#65292;&#34913;&#37327;&#27169;&#22411;&#30340;&#19981;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.11361v2 Announce Type: replace  Abstract: Retrieval-augmented generation (RAG) grounds large language model (LLM) output by leveraging external knowledge sources to reduce factual hallucinations. However, prior works lack a comprehensive evaluation of different language families, making it challenging to evaluate LLM robustness against errors in external retrieved knowledge. To overcome this, we establish NoMIRACL, a human-annotated dataset for evaluating LLM robustness in RAG across 18 typologically diverse languages. NoMIRACL includes both a non-relevant and a relevant subset. Queries in the non-relevant subset contain passages judged as non-relevant, whereas queries in the relevant subset include at least a single judged relevant passage. We measure LLM robustness using two metrics: (i) hallucination rate, measuring model tendency to hallucinate an answer, when the answer is not present in passages in the non-relevant subset, and (ii) error rate, measuring model inaccurac
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#35299;&#37322;&#21644;&#35782;&#21035;&#20234;&#26031;&#20848;&#25945;&#20167;&#24680;&#35328;&#35770;&#65292;&#27169;&#22411;&#22312;&#20445;&#25345;&#20986;&#33394;&#24615;&#33021;&#30340;&#21516;&#26102;&#33021;&#22815;&#35299;&#37322;&#30456;&#20851;&#24615;&#21644;&#22240;&#26524;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2311.04916</link><description>&lt;p&gt;
&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#35299;&#37322;&#20234;&#26031;&#20848;&#25945;&#20167;&#24680;&#35328;&#35770;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Explainable Identification of Hate Speech towards Islam using Graph Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.04916
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#35299;&#37322;&#21644;&#35782;&#21035;&#20234;&#26031;&#20848;&#25945;&#20167;&#24680;&#35328;&#35770;&#65292;&#27169;&#22411;&#22312;&#20445;&#25345;&#20986;&#33394;&#24615;&#33021;&#30340;&#21516;&#26102;&#33021;&#22815;&#35299;&#37322;&#30456;&#20851;&#24615;&#21644;&#22240;&#26524;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20234;&#26031;&#20848;&#25945;&#20167;&#24680;&#35328;&#35770;&#22312;&#22312;&#32447;&#31038;&#20132;&#20114;&#21160;&#24179;&#21488;&#19978;&#26159;&#19968;&#20010;&#26222;&#36941;&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;&#35782;&#21035;&#21644;&#28040;&#38500;&#36825;&#31181;&#20167;&#24680;&#26159;&#36808;&#21521;&#21644;&#35856;&#19982;&#21644;&#24179;&#26410;&#26469;&#30340;&#20851;&#38190;&#19968;&#27493;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#20363;&#65292;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#35782;&#21035;&#21644;&#35299;&#37322;&#38024;&#23545;&#20234;&#26031;&#20848;&#25945;&#30340;&#20167;&#24680;&#35328;&#35770;&#12290;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21457;&#29616;&#12289;&#25552;&#21462;&#24182;&#21033;&#29992;&#19981;&#21516;&#25968;&#25454;&#28857;&#20043;&#38388;&#30340;&#20851;&#31995;&#30340;&#20869;&#22312;&#33021;&#21147;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22987;&#32456;&#33021;&#22815;&#22312;&#20445;&#25345;&#20986;&#33394;&#24615;&#33021;&#30340;&#21516;&#26102;&#25552;&#20379;&#23545;&#28508;&#22312;&#30456;&#20851;&#24615;&#21644;&#22240;&#26524;&#20851;&#31995;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.04916v2 Announce Type: cross  Abstract: Islamophobic language is a prevalent challenge on online social interaction platforms. Identifying and eliminating such hatred is a crucial step towards a future of harmony and peace. This study presents a novel paradigm for identifying and explaining hate speech towards Islam using graph neural networks. Utilizing the intrinsic ability of graph neural networks to find, extract, and use relationships across disparate data points, our model consistently achieves outstanding performance while offering explanations for the underlying correlations and causation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;BEQUE&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;&#38454;&#27573;&#27969;&#31243;&#65292;&#26377;&#25928;&#20248;&#21270;&#38271;&#23614;&#26597;&#35810;&#12289;&#24357;&#34917;&#35821;&#20041;&#24046;&#36317;&#65292;&#25552;&#39640;&#26597;&#35810;&#37325;&#20889;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2311.03758</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#28120;&#23453;&#25628;&#32034;&#38271;&#23614;&#26597;&#35810;&#37325;&#20889;
&lt;/p&gt;
&lt;p&gt;
Large Language Model based Long-tail Query Rewriting in Taobao Search
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.03758
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;BEQUE&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;&#38454;&#27573;&#27969;&#31243;&#65292;&#26377;&#25928;&#20248;&#21270;&#38271;&#23614;&#26597;&#35810;&#12289;&#24357;&#34917;&#35821;&#20041;&#24046;&#36317;&#65292;&#25552;&#39640;&#26597;&#35810;&#37325;&#20889;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30005;&#23376;&#21830;&#21153;&#25628;&#32034;&#39046;&#22495;&#65292;&#35821;&#20041;&#21305;&#37197;&#30340;&#37325;&#35201;&#24615;&#19981;&#35328;&#32780;&#21933;&#65292;&#22240;&#20026;&#23427;&#30452;&#25509;&#24433;&#21709;&#29992;&#25143;&#20307;&#39564;&#21644;&#20844;&#21496;&#25910;&#20837;&#12290;&#22312;&#36825;&#26041;&#38754;&#65292;&#26597;&#35810;&#37325;&#20889;&#20316;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#25216;&#26415;&#65292;&#29992;&#26469;&#24357;&#34917;&#35821;&#20041;&#21305;&#37197;&#36807;&#31243;&#20013;&#22266;&#26377;&#30340;&#35821;&#20041;&#24046;&#36317;&#65292;&#21463;&#21040;&#20102;&#34892;&#19994;&#21644;&#23398;&#26415;&#30028;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26597;&#35810;&#37325;&#20889;&#26041;&#27861;&#24448;&#24448;&#38590;&#20197;&#26377;&#25928;&#20248;&#21270;&#38271;&#23614;&#26597;&#35810;&#65292;&#32531;&#35299;&#30001;&#35821;&#20041;&#24046;&#36317;&#24341;&#36215;&#30340;&#8220;&#23569;&#21484;&#22238;&#8221;&#29616;&#35937;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BEQUE&#65292;&#19968;&#20010;&#26725;&#25509;&#38271;&#23614;&#26597;&#35810;&#35821;&#20041;&#24046;&#36317;&#30340;&#32508;&#21512;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;BEQUE&#21253;&#25324;&#19977;&#20010;&#38454;&#27573;&#65306;&#22810;&#25351;&#23548;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#12289;&#31163;&#32447;&#21453;&#39304;&#21644;&#23458;&#35266;&#23545;&#40784;&#12290;&#25105;&#20204;&#39318;&#20808;&#22522;&#20110;&#25298;&#32477;&#25277;&#26679;&#21644;&#36741;&#21161;&#20219;&#21153;&#28151;&#21512;&#26500;&#24314;&#19968;&#20010;&#37325;&#20889;&#25968;&#25454;&#38598;&#65292;&#20197;&#30417;&#30563;&#26041;&#24335;&#24494;&#35843;&#25105;&#20204;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#12290;&#38543;&#21518;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.03758v3 Announce Type: replace  Abstract: In the realm of e-commerce search, the significance of semantic matching cannot be overstated, as it directly impacts both user experience and company revenue. Along this line, query rewriting, serving as an important technique to bridge the semantic gaps inherent in the semantic matching process, has attached wide attention from the industry and academia. However, existing query rewriting methods often struggle to effectively optimize long-tail queries and alleviate the phenomenon of "few-recall" caused by semantic gap. In this paper, we present BEQUE, a comprehensive framework that Bridges the sEmantic gap for long-tail QUEries. In detail, BEQUE comprises three stages: multi-instruction supervised fine tuning (SFT), offline feedback, and objective alignment. We first construct a rewriting dataset based on rejection sampling and auxiliary tasks mixing to fine-tune our large language model (LLM) in a supervised fashion. Subsequently,
&lt;/p&gt;</description></item><item><title>&#25512;&#33616;&#31995;&#32479;&#30740;&#31350;&#25506;&#32034;&#20102;&#20844;&#24179;&#24615;&#21644;&#22810;&#26679;&#24615;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#36890;&#36807;&#25193;&#23637;&#23545;&#29992;&#25143;&#21644;&#29289;&#21697;&#32423;&#21035;&#22810;&#26679;&#24615;&#30340;&#29702;&#35299;&#65292;&#37325;&#26032;&#35299;&#37322;&#20102;&#20844;&#24179;&#24615;&#30740;&#31350;&#65292;&#25552;&#21319;&#20102;&#23545;&#20844;&#24179;&#24615;&#30456;&#20851;&#24037;&#20316;&#30340;&#35748;&#35782;</title><link>https://arxiv.org/abs/2307.04644</link><description>&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#20844;&#24179;&#24615;&#21644;&#22810;&#26679;&#24615;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Fairness and Diversity in Recommender Systems: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2307.04644
&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#30740;&#31350;&#25506;&#32034;&#20102;&#20844;&#24179;&#24615;&#21644;&#22810;&#26679;&#24615;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#36890;&#36807;&#25193;&#23637;&#23545;&#29992;&#25143;&#21644;&#29289;&#21697;&#32423;&#21035;&#22810;&#26679;&#24615;&#30340;&#29702;&#35299;&#65292;&#37325;&#26032;&#35299;&#37322;&#20102;&#20844;&#24179;&#24615;&#30740;&#31350;&#65292;&#25552;&#21319;&#20102;&#23545;&#20844;&#24179;&#24615;&#30456;&#20851;&#24037;&#20316;&#30340;&#35748;&#35782;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#26159;&#20943;&#36731;&#20449;&#24687;&#36807;&#36733;&#30340;&#26377;&#25928;&#24037;&#20855;&#65292;&#24182;&#22312;&#21508;&#20010;&#39046;&#22495;&#24191;&#27867;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#23545;&#25928;&#29992;&#30446;&#26631;&#30340;&#21333;&#19968;&#20851;&#27880;&#35777;&#26126;&#26080;&#27861;&#35299;&#20915;&#29616;&#23454;&#20851;&#20999;&#65292;&#23548;&#33268;&#23545;&#20851;&#27880;&#20844;&#24179;&#24863;&#30693;&#21644;&#22810;&#26679;&#24615;&#24863;&#30693;&#30340;&#25512;&#33616;&#31995;&#32479;&#36234;&#26469;&#36234;&#37325;&#35270;&#12290;&#23613;&#31649;&#22823;&#22810;&#25968;&#29616;&#26377;&#30740;&#31350;&#29420;&#31435;&#25506;&#35752;&#20844;&#24179;&#21644;&#22810;&#26679;&#24615;&#65292;&#25105;&#20204;&#35782;&#21035;&#20986;&#36825;&#20004;&#20010;&#39046;&#22495;&#20043;&#38388;&#30340;&#32039;&#23494;&#36830;&#25509;&#12290;&#22312;&#36825;&#39033;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#20998;&#21035;&#35752;&#35770;&#23427;&#20204;&#65292;&#28982;&#21518;&#28145;&#20837;&#25506;&#35752;&#23427;&#20204;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#27492;&#22806;&#65292;&#21463;&#29992;&#25143;&#32423;&#21644;&#29289;&#21697;&#32423;&#20844;&#24179;&#24615;&#27010;&#24565;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#23558;&#22810;&#26679;&#24615;&#30340;&#29702;&#35299;&#25193;&#23637;&#21040;&#19981;&#20165;&#21253;&#25324;&#29289;&#21697;&#32423;&#21035;&#65292;&#36824;&#21253;&#25324;&#29992;&#25143;&#32423;&#21035;&#12290;&#36890;&#36807;&#23545;&#29992;&#25143;&#21644;&#29289;&#21697;&#32423;&#21035;&#22810;&#26679;&#24615;&#30340;&#25193;&#23637;&#35266;&#28857;&#65292;&#25105;&#20204;&#20174;&#22810;&#26679;&#24615;&#30340;&#35282;&#24230;&#37325;&#26032;&#35299;&#37322;&#20844;&#24179;&#24615;&#30740;&#31350;&#12290;&#36825;&#31181;&#26032;&#35270;&#35282;&#22686;&#36827;&#20102;&#25105;&#20204;&#23545;&#19982;&#20844;&#24179;&#26377;&#20851;&#30340;&#24037;&#20316;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2307.04644v2 Announce Type: replace  Abstract: Recommender systems are effective tools for mitigating information overload and have seen extensive applications across various domains. However, the single focus on utility goals proves to be inadequate in addressing real-world concerns, leading to increasing attention to fairness-aware and diversity-aware recommender systems. While most existing studies explore fairness and diversity independently, we identify strong connections between these two domains. In this survey, we first discuss each of them individually and then dive into their connections. Additionally, motivated by the concepts of user-level and item-level fairness, we broaden the understanding of diversity to encompass not only the item level but also the user level. With this expanded perspective on user and item-level diversity, we re-interpret fairness studies from the viewpoint of diversity. This fresh perspective enhances our understanding of fairness-related work
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21453;&#20107;&#23454;&#25512;&#29702;&#30340;&#21487;&#35299;&#37322;&#26694;&#26550;&#65292;&#29992;&#20110;&#36335;&#24452;&#25512;&#33616;&#65292;&#36890;&#36807;&#23398;&#20064;&#36335;&#24452;&#30340;&#21487;&#35299;&#37322;&#26435;&#37325;&#26469;&#26367;&#20195;&#20851;&#27880;&#26435;&#37325;&#12290;&#36825;&#31181;&#26694;&#26550;&#27604;&#20256;&#32479;&#30340;&#20851;&#27880;&#26426;&#21046;&#26356;&#31283;&#23450;&#19988;&#26356;&#31526;&#21512;&#20154;&#31867;&#30452;&#35273;&#12290;</title><link>http://arxiv.org/abs/2401.05744</link><description>&lt;p&gt;
&#36873;&#25321;&#24182;&#38750;&#21482;&#26377;&#20851;&#27880;&#21147;&#65306;&#36335;&#24452;&#25512;&#26029;&#22312;&#21487;&#35299;&#37322;&#30340;&#22522;&#20110;&#36335;&#24452;&#30340;&#25512;&#33616;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Attention Is Not the Only Choice: Counterfactual Reasoning for Path-Based Explainable Recommendation. (arXiv:2401.05744v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05744
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21453;&#20107;&#23454;&#25512;&#29702;&#30340;&#21487;&#35299;&#37322;&#26694;&#26550;&#65292;&#29992;&#20110;&#36335;&#24452;&#25512;&#33616;&#65292;&#36890;&#36807;&#23398;&#20064;&#36335;&#24452;&#30340;&#21487;&#35299;&#37322;&#26435;&#37325;&#26469;&#26367;&#20195;&#20851;&#27880;&#26435;&#37325;&#12290;&#36825;&#31181;&#26694;&#26550;&#27604;&#20256;&#32479;&#30340;&#20851;&#27880;&#26426;&#21046;&#26356;&#31283;&#23450;&#19988;&#26356;&#31526;&#21512;&#20154;&#31867;&#30452;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#20165;&#36861;&#27714;&#25512;&#33616;&#20934;&#30830;&#24615;&#30456;&#27604;&#65292;&#25512;&#33616;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#36817;&#24180;&#26469;&#26356;&#21463;&#20851;&#27880;&#12290;&#35768;&#22810;&#22522;&#20110;&#22270;&#30340;&#25512;&#33616;&#26041;&#27861;&#20351;&#29992;&#20851;&#27880;&#26426;&#21046;&#26469;&#35299;&#37322;&#25512;&#33616;&#36807;&#31243;&#20013;&#30340;&#36335;&#24452;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20851;&#27880;&#26435;&#37325;&#26159;&#20026;&#20102;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#32780;&#35774;&#35745;&#30340;&#65292;&#32780;&#38750;&#21487;&#35299;&#37322;&#24615;&#12290;&#36817;&#26399;&#65292;&#19968;&#20123;&#30740;&#31350;&#32773;&#24320;&#22987;&#23545;&#22522;&#20110;&#20851;&#27880;&#26426;&#21046;&#30340;&#21487;&#35299;&#37322;&#24615;&#25552;&#20986;&#36136;&#30097;&#65292;&#22240;&#20026;&#36825;&#20123;&#20851;&#27880;&#26435;&#37325;&#22312;&#19981;&#21516;&#30340;&#36816;&#34892;&#20013;&#26159;&#19981;&#31283;&#23450;&#30340;&#65292;&#19988;&#19981;&#19968;&#23450;&#19982;&#20154;&#31867;&#30340;&#30452;&#35273;&#19968;&#33268;&#12290;&#21463;&#22240;&#26524;&#23398;&#20064;&#29702;&#35770;&#20013;&#30340;&#21453;&#20107;&#23454;&#25512;&#29702;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#35299;&#37322;&#26694;&#26550;&#65292;&#29992;&#20110;&#22522;&#20110;&#36335;&#24452;&#30340;&#25512;&#33616;&#65292;&#20854;&#20013;&#36890;&#36807;&#23398;&#20064;&#36335;&#24452;&#30340;&#21487;&#35299;&#37322;&#26435;&#37325;&#26469;&#26367;&#20195;&#20851;&#27880;&#26435;&#37325;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20174;&#36335;&#24452;&#34920;&#31034;&#21644;&#36335;&#24452;&#25299;&#25169;&#32467;&#26500;&#30340;&#35282;&#24230;&#35774;&#35745;&#20102;&#20004;&#31181;&#21453;&#20107;&#23454;&#25512;&#29702;&#31639;&#27861;&#12290;&#27492;&#22806;&#65292;&#19982;&#20256;&#32479;&#30340;&#26696;&#20363;&#30740;&#31350;&#19981;&#21516;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Compared with only pursuing recommendation accuracy, the explainability of a recommendation model has drawn more attention in recent years. Many graph-based recommendations resort to informative paths with the attention mechanism for the explanation. Unfortunately, these attention weights are intentionally designed for model accuracy but not explainability. Recently, some researchers have started to question attention-based explainability because the attention weights are unstable for different reproductions, and they may not always align with human intuition. Inspired by the counterfactual reasoning from causality learning theory, we propose a novel explainable framework targeting path-based recommendations, wherein the explainable weights of paths are learned to replace attention weights. Specifically, we design two counterfactual reasoning algorithms from both path representation and path topological structure perspectives. Moreover, unlike traditional case studies, we also propose 
&lt;/p&gt;</description></item><item><title>Starling&#26159;&#19968;&#31181;I/O&#39640;&#25928;&#30340;&#22522;&#20110;&#30913;&#30424;&#30340;&#22270;&#32034;&#24341;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#25968;&#25454;&#29255;&#27573;&#19978;&#36827;&#34892;&#39640;&#32500;&#21521;&#37327;&#30456;&#20284;&#24615;&#25628;&#32034;&#65292;&#22312;&#20934;&#30830;&#24615;&#12289;&#25928;&#29575;&#21644;&#31354;&#38388;&#25104;&#26412;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2401.02116</link><description>&lt;p&gt;
Starling: &#19968;&#31181;&#29992;&#20110;&#39640;&#32500;&#21521;&#37327;&#30456;&#20284;&#24615;&#25628;&#32034;&#30340;I/O&#39640;&#25928;&#30340;&#22522;&#20110;&#30913;&#30424;&#30340;&#22270;&#32034;&#24341;&#26694;&#26550;&#65292;&#29992;&#20110;&#25968;&#25454;&#29255;&#27573;&#20013; (arXiv:2401.02116v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
Starling: An I/O-Efficient Disk-Resident Graph Index Framework for High-Dimensional Vector Similarity Search on Data Segment. (arXiv:2401.02116v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02116
&lt;/p&gt;
&lt;p&gt;
Starling&#26159;&#19968;&#31181;I/O&#39640;&#25928;&#30340;&#22522;&#20110;&#30913;&#30424;&#30340;&#22270;&#32034;&#24341;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#25968;&#25454;&#29255;&#27573;&#19978;&#36827;&#34892;&#39640;&#32500;&#21521;&#37327;&#30456;&#20284;&#24615;&#25628;&#32034;&#65292;&#22312;&#20934;&#30830;&#24615;&#12289;&#25928;&#29575;&#21644;&#31354;&#38388;&#25104;&#26412;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#32500;&#21521;&#37327;&#30456;&#20284;&#24615;&#25628;&#32034;(HVSS)&#20316;&#20026;&#25968;&#25454;&#31185;&#23398;&#21644;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#27491;&#21463;&#21040;&#20851;&#27880;&#12290;&#38543;&#30528;&#21521;&#37327;&#25968;&#25454;&#30340;&#22686;&#38271;&#65292;&#20869;&#23384;&#32034;&#24341;&#21464;&#24471;&#38750;&#24120;&#26114;&#36149;&#65292;&#22240;&#20026;&#23427;&#20204;&#38656;&#35201;&#22823;&#37327;&#25193;&#23637;&#20027;&#20869;&#23384;&#36164;&#28304;&#12290;&#19968;&#31181;&#21487;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#20351;&#29992;&#22522;&#20110;&#30913;&#30424;&#30340;&#23454;&#29616;&#65292;&#23558;&#21521;&#37327;&#25968;&#25454;&#23384;&#20648;&#21644;&#25628;&#32034;&#22312;&#39640;&#24615;&#33021;&#35774;&#22791;(&#22914;NVMe SSD)&#20013;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#25968;&#25454;&#29255;&#27573;&#30340;HVSS&#20173;&#28982;&#26159;&#21521;&#37327;&#25968;&#25454;&#24211;&#20013;&#30340;&#25361;&#25112;&#65292;&#20854;&#20013;&#19968;&#20010;&#26426;&#22120;&#26377;&#22810;&#20010;&#29255;&#27573;&#26469;&#23454;&#29616;&#31995;&#32479;&#21151;&#33021;&#65288;&#22914;&#25193;&#23637;&#65289;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#27599;&#20010;&#29255;&#27573;&#30340;&#20869;&#23384;&#21644;&#30913;&#30424;&#31354;&#38388;&#26377;&#38480;&#65292;&#22240;&#27492;&#25968;&#25454;&#29255;&#27573;&#19978;&#30340;HVSS&#38656;&#35201;&#22312;&#20934;&#30830;&#24615;&#65292;&#25928;&#29575;&#21644;&#31354;&#38388;&#25104;&#26412;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;&#30913;&#30424;&#30340;&#26041;&#27861;&#24182;&#27809;&#26377;&#21516;&#26102;&#32771;&#34385;&#21040;&#25152;&#26377;&#36825;&#20123;&#35201;&#27714;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Starling&#65292;&#19968;&#31181;I/O&#39640;&#25928;&#30340;&#22522;&#20110;&#30913;&#30424;&#30340;&#22270;&#32034;&#24341;&#26694;&#26550;&#65292;&#23427;&#22312;&#29255;&#27573;&#20013;&#20248;&#21270;&#25968;&#25454;&#24067;&#23616;&#21644;&#25628;&#32034;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-dimensional vector similarity search (HVSS) is receiving a spotlight as a powerful tool for various data science and AI applications. As vector data grows larger, in-memory indexes become extremely expensive because they necessitate substantial expansion of main memory resources. One possible solution is to use disk-based implementation, which stores and searches vector data in high-performance devices like NVMe SSDs. However, HVSS for data segments is still challenging in vector databases, where one machine has multiple segments for system features (like scaling) purposes. In this setting, each segment has limited memory and disk space, so HVSS on the data segment needs to balance accuracy, efficiency, and space cost. Existing disk-based methods are sub-optimal because they do not consider all these requirements together. In this paper, we present Starling, an I/O-efficient disk-resident graph index framework that optimizes data layout and search strategy in the segment. It has t
&lt;/p&gt;</description></item><item><title>&#26412;&#25945;&#31243;&#25552;&#20379;&#20102;&#23545;&#35805;&#24335;&#20154;&#24037;&#26234;&#33021;&#20013;&#25968;&#25454;&#22686;&#24378;&#30340;&#32508;&#36848;&#65292;&#21253;&#25324;&#23545;&#35805;&#22686;&#24378;&#12289;&#24320;&#25918;&#22495;&#21644;&#20219;&#21153;&#23548;&#21521;&#30340;&#23545;&#35805;&#29983;&#25104;&#20197;&#21450;&#35780;&#20272;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#36824;&#35752;&#35770;&#20102;&#24403;&#21069;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#30340;&#21457;&#23637;&#26041;&#21521;&#65292;&#20197;&#24110;&#21161;&#25512;&#21160;&#35813;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2309.04739</link><description>&lt;p&gt;
&#23545;&#35805;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#25968;&#25454;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Data Augmentation for Conversational AI. (arXiv:2309.04739v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04739
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25945;&#31243;&#25552;&#20379;&#20102;&#23545;&#35805;&#24335;&#20154;&#24037;&#26234;&#33021;&#20013;&#25968;&#25454;&#22686;&#24378;&#30340;&#32508;&#36848;&#65292;&#21253;&#25324;&#23545;&#35805;&#22686;&#24378;&#12289;&#24320;&#25918;&#22495;&#21644;&#20219;&#21153;&#23548;&#21521;&#30340;&#23545;&#35805;&#29983;&#25104;&#20197;&#21450;&#35780;&#20272;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#36824;&#35752;&#35770;&#20102;&#24403;&#21069;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#30340;&#21457;&#23637;&#26041;&#21521;&#65292;&#20197;&#24110;&#21161;&#25512;&#21160;&#35813;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#31995;&#32479;&#30340;&#21457;&#23637;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#20449;&#24687;&#33719;&#21462;&#26041;&#24335;&#65292;&#36229;&#36234;&#20102;&#21333;&#19968;&#26597;&#35810;&#30340;&#38480;&#21046;&#12290;&#28982;&#32780;&#65292;&#24320;&#21457;&#23545;&#35805;&#31995;&#32479;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#39046;&#22495;&#21644;&#35821;&#35328;&#20013;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20256;&#32479;&#30340;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;&#65292;&#22914;&#20247;&#21253;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#20154;&#21147;&#21644;&#26102;&#38388;&#65292;&#22240;&#27492;&#22312;&#27492;&#24773;&#26223;&#19979;&#25928;&#29575;&#20302;&#19979;&#12290;&#25968;&#25454;&#22686;&#24378;&#65288;DA&#65289;&#26159;&#19968;&#31181;&#32531;&#35299;&#23545;&#35805;&#31995;&#32479;&#20013;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#26412;&#25945;&#31243;&#20840;&#38754;&#19988;&#26368;&#26032;&#22320;&#27010;&#36848;&#20102;&#22312;&#23545;&#35805;&#31995;&#32479;&#20013;&#20351;&#29992;&#30340;DA&#26041;&#27861;&#65292;&#21253;&#25324;&#23545;&#35805;&#22686;&#24378;&#12289;&#24320;&#25918;&#22495;&#21644;&#20219;&#21153;&#23548;&#21521;&#30340;&#23545;&#35805;&#29983;&#25104;&#20197;&#21450;&#19981;&#21516;&#30340;&#35780;&#20272;&#27169;&#22411;&#30340;&#33539;&#24335;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#24403;&#21069;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#30340;&#21457;&#23637;&#26041;&#21521;&#65292;&#20197;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#36827;&#19968;&#27493;&#25512;&#21160;&#36825;&#19968;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Advancements in conversational systems have revolutionized information access, surpassing the limitations of single queries. However, developing dialogue systems requires a large amount of training data, which is a challenge in low-resource domains and languages. Traditional data collection methods like crowd-sourcing are labor-intensive and time-consuming, making them ineffective in this context. Data augmentation (DA) is an affective approach to alleviate the data scarcity problem in conversational systems. This tutorial provides a comprehensive and up-to-date overview of DA approaches in the context of conversational systems. It highlights recent advances in conversation augmentation, open domain and task-oriented conversation generation, and different paradigms of evaluating these models. We also discuss current challenges and future directions in order to help researchers and practitioners to further advance the field in this area.
&lt;/p&gt;</description></item><item><title>Mol-Instructions&#26159;&#19968;&#20010;&#19987;&#38376;&#20026;&#29983;&#29289;&#20998;&#23376;&#39046;&#22495;&#35774;&#35745;&#30340;&#32508;&#21512;&#25351;&#20196;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#29289;&#39046;&#22495;&#20013;&#30340;&#36866;&#24212;&#33021;&#21147;&#21644;&#35748;&#30693;&#25935;&#38160;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.08018</link><description>&lt;p&gt;
Mol-Instructions: &#19968;&#20010;&#22823;&#35268;&#27169;&#29983;&#29289;&#20998;&#23376;&#25351;&#20196;&#25968;&#25454;&#38598;&#65292;&#20026;&#22823;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#25903;&#25345;
&lt;/p&gt;
&lt;p&gt;
Mol-Instructions: A Large-Scale Biomolecular Instruction Dataset for Large Language Models. (arXiv:2306.08018v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08018
&lt;/p&gt;
&lt;p&gt;
Mol-Instructions&#26159;&#19968;&#20010;&#19987;&#38376;&#20026;&#29983;&#29289;&#20998;&#23376;&#39046;&#22495;&#35774;&#35745;&#30340;&#32508;&#21512;&#25351;&#20196;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#29289;&#39046;&#22495;&#20013;&#30340;&#36866;&#24212;&#33021;&#21147;&#21644;&#35748;&#30693;&#25935;&#38160;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20197;&#20854;&#21331;&#36234;&#30340;&#20219;&#21153;&#22788;&#29702;&#33021;&#21147;&#21644;&#21019;&#26032;&#30340;&#36755;&#20986;&#65292;&#22312;&#35768;&#22810;&#39046;&#22495;&#25512;&#21160;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#29983;&#29289;&#20998;&#23376;&#30740;&#31350;&#31561;&#19987;&#19994;&#39046;&#22495;&#30340;&#29087;&#32451;&#24212;&#29992;&#36824;&#21463;&#21040;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Mol-Instructions&#65292;&#36825;&#26159;&#19968;&#20010;&#32463;&#36807;&#31934;&#24515;&#31574;&#21010;&#12289;&#19987;&#38376;&#38024;&#23545;&#29983;&#29289;&#20998;&#23376;&#39046;&#22495;&#35774;&#35745;&#30340;&#32508;&#21512;&#25351;&#20196;&#25968;&#25454;&#38598;&#12290;Mol-Instructions&#30001;&#19977;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#32452;&#25104;&#65306;&#20998;&#23376;&#23548;&#21521;&#25351;&#20196;&#12289;&#34507;&#30333;&#36136;&#23548;&#21521;&#25351;&#20196;&#21644;&#29983;&#29289;&#20998;&#23376;&#25991;&#26412;&#25351;&#20196;&#65292;&#27599;&#20010;&#37096;&#20998;&#37117;&#34987;&#31574;&#21010;&#29992;&#20110;&#22686;&#24378;LLM&#23545;&#29983;&#29289;&#20998;&#23376;&#29305;&#24615;&#21644;&#34892;&#20026;&#30340;&#29702;&#35299;&#21644;&#39044;&#27979;&#33021;&#21147;&#12290;&#36890;&#36807;&#23545;&#20195;&#34920;&#24615;LLM&#30340;&#24191;&#27867;&#25351;&#20196;&#35843;&#25972;&#23454;&#39564;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;Mol-Instructions&#22312;&#22686;&#24378;&#22823;&#27169;&#22411;&#22312;&#29983;&#29289;&#20998;&#23376;&#30740;&#31350;&#22797;&#26434;&#39046;&#22495;&#20869;&#30340;&#36866;&#24212;&#33021;&#21147;&#21644;&#35748;&#30693;&#25935;&#38160;&#24230;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#20174;&#32780;&#20419;&#36827;&#29983;&#29289;&#20998;&#23376;&#39046;&#22495;&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), with their remarkable task-handling capabilities and innovative outputs, have catalyzed significant advancements across a spectrum of fields. However, their proficiency within specialized domains such as biomolecular studies remains limited. To address this challenge, we introduce Mol-Instructions, a meticulously curated, comprehensive instruction dataset expressly designed for the biomolecular realm. Mol-Instructions is composed of three pivotal components: molecule-oriented instructions, protein-oriented instructions, and biomolecular text instructions, each curated to enhance the understanding and prediction capabilities of LLMs concerning biomolecular features and behaviors. Through extensive instruction tuning experiments on the representative LLM, we underscore the potency of Mol-Instructions to enhance the adaptability and cognitive acuity of large models within the complex sphere of biomolecular studies, thereby promoting advancements in the biomol
&lt;/p&gt;</description></item></channel></rss>