<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#25512;&#33616;&#31995;&#32479;&#20013;&#65292;&#20351;&#29992;&#21807;&#19968;&#26631;&#35782;&#30340;IDRec&#27169;&#22411;&#30456;&#27604;&#20351;&#29992;&#27169;&#24577;&#30340;MoRec&#27169;&#22411;&#22312;&#25512;&#33616;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#19978;&#34920;&#29616;&#26356;&#22909;&#65292;&#28982;&#32780;&#65292;&#38656;&#35201;&#26681;&#25454;&#20855;&#20307;&#24773;&#20917;&#36873;&#25321;&#36866;&#21512;&#30340;&#25512;&#33616;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2303.13835</link><description>&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#20309;&#21435;&#20309;&#20174;&#65311;ID- vs. &#22522;&#20110;&#27169;&#24577;&#30340;&#25512;&#33616;&#27169;&#22411;&#20877;&#25506;&#35752;
&lt;/p&gt;
&lt;p&gt;
Where to Go Next for Recommender Systems? ID- vs. Modality-based recommender models revisited. (arXiv:2303.13835v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13835
&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#20013;&#65292;&#20351;&#29992;&#21807;&#19968;&#26631;&#35782;&#30340;IDRec&#27169;&#22411;&#30456;&#27604;&#20351;&#29992;&#27169;&#24577;&#30340;MoRec&#27169;&#22411;&#22312;&#25512;&#33616;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#19978;&#34920;&#29616;&#26356;&#22909;&#65292;&#28982;&#32780;&#65292;&#38656;&#35201;&#26681;&#25454;&#20855;&#20307;&#24773;&#20917;&#36873;&#25321;&#36866;&#21512;&#30340;&#25512;&#33616;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#21313;&#24180;&#65292;&#21033;&#29992;&#21807;&#19968;&#26631;&#35782;&#65288;ID&#65289;&#26469;&#34920;&#31034;&#19981;&#21516;&#29992;&#25143;&#21644;&#29289;&#21697;&#30340;&#25512;&#33616;&#27169;&#22411;&#19968;&#30452;&#26159;&#26368;&#20808;&#36827;&#30340;&#65292;&#24182;&#19988;&#22312;&#25512;&#33616;&#31995;&#32479;&#25991;&#29486;&#20013;&#21344;&#20027;&#23548;&#22320;&#20301;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#39044;&#35757;&#32451;&#27169;&#24577;&#32534;&#30721;&#22120;&#65288;&#22914;BERT&#21644;ViT&#65289;&#22312;&#23545;&#29289;&#21697;&#30340;&#21407;&#22987;&#27169;&#24577;&#29305;&#24449;&#65288;&#22914;&#25991;&#26412;&#21644;&#22270;&#20687;&#65289;&#36827;&#34892;&#24314;&#27169;&#26041;&#38754;&#21464;&#24471;&#36234;&#26469;&#36234;&#24378;&#22823;&#12290;&#22240;&#27492;&#65292;&#33258;&#28982;&#32780;&#28982;&#30340;&#38382;&#39064;&#26159;&#65306;&#36890;&#36807;&#29992;&#26368;&#20808;&#36827;&#30340;&#27169;&#24577;&#32534;&#30721;&#22120;&#26367;&#25442;&#29289;&#21697;ID&#23884;&#20837;&#21521;&#37327;&#65292;&#19968;&#20010;&#32431;&#31929;&#30340;&#22522;&#20110;&#27169;&#24577;&#30340;&#25512;&#33616;&#27169;&#22411;&#65288;MoRec&#65289;&#33021;&#21542;&#32988;&#36807;&#25110;&#19982;&#32431;ID&#22522;&#30784;&#27169;&#22411;&#65288;IDRec&#65289;&#30456;&#21305;&#37197;&#65311;&#23454;&#38469;&#19978;&#65292;&#26089;&#22312;&#21313;&#24180;&#21069;&#65292;&#36825;&#20010;&#38382;&#39064;&#23601;&#34987;&#22238;&#31572;&#20102;&#65292;IDRec&#22312;&#25512;&#33616;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#37117;&#36828;&#36828;&#32988;&#36807;MoRec&#12290;&#25105;&#20204;&#26088;&#22312;&#37325;&#26032;&#23457;&#35270;&#36825;&#20010;&#8220;&#32769;&#38382;&#39064;&#8221;&#65292;&#20174;&#22810;&#20010;&#26041;&#38754;&#23545;MoRec&#36827;&#34892;&#31995;&#32479;&#30740;&#31350;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20960;&#20010;&#23376;&#38382;&#39064;&#65306;&#65288;i&#65289;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;MoRec&#25110;IDRec&#21738;&#20010;&#25512;&#33616;&#27169;&#24335;&#34920;&#29616;&#26356;&#22909;&#65292;&#29305;&#21035;&#26159;&#22312;&#19968;&#33324;&#24773;&#20917;&#21644;......
&lt;/p&gt;
&lt;p&gt;
Recommendation models that utilize unique identities (IDs) to represent distinct users and items have been state-of-the-art (SOTA) and dominated the recommender systems (RS) literature for over a decade. Meanwhile, the pre-trained modality encoders, such as BERT and ViT, have become increasingly powerful in modeling the raw modality features of an item, such as text and images. Given this, a natural question arises: can a purely modality-based recommendation model (MoRec) outperforms or matches a pure ID-based model (IDRec) by replacing the itemID embedding with a SOTA modality encoder? In fact, this question was answered ten years ago when IDRec beats MoRec by a strong margin in both recommendation accuracy and efficiency. We aim to revisit this `old' question and systematically study MoRec from several aspects. Specifically, we study several sub-questions: (i) which recommendation paradigm, MoRec or IDRec, performs better in practical scenarios, especially in the general setting and 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;GETT-QA&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#20351;&#29992;T5&#23545;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#29983;&#25104;&#31616;&#21270;&#30340;SPARQL&#26597;&#35810;&#65292;&#24182;&#20351;&#29992;&#25130;&#26029;&#30340;KG&#23884;&#20837;&#25552;&#39640;&#20102;&#30693;&#35782;&#22270;&#35889;&#38382;&#31572;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.13284</link><description>&lt;p&gt;
GETT-QA&#65306;&#22522;&#20110;&#22270;&#23884;&#20837;&#30340;&#30693;&#35782;&#22270;&#35889;&#38382;&#31572;&#20013;&#30340;T2T Transformer
&lt;/p&gt;
&lt;p&gt;
GETT-QA: Graph Embedding based T2T Transformer for Knowledge Graph Question Answering. (arXiv:2303.13284v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13284
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;GETT-QA&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#20351;&#29992;T5&#23545;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#29983;&#25104;&#31616;&#21270;&#30340;SPARQL&#26597;&#35810;&#65292;&#24182;&#20351;&#29992;&#25130;&#26029;&#30340;KG&#23884;&#20837;&#25552;&#39640;&#20102;&#30693;&#35782;&#22270;&#35889;&#38382;&#31572;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GETT-QA&#30340;&#31471;&#21040;&#31471;&#30693;&#35782;&#22270;&#35889;&#38382;&#31572;&#31995;&#32479;&#12290;GETT-QA&#20351;&#29992;&#20102;T5&#65292;&#36825;&#26159;&#19968;&#31181;&#28909;&#38376;&#30340;&#25991;&#26412;&#21040;&#25991;&#26412;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#20197;&#33258;&#28982;&#35821;&#35328;&#24418;&#24335;&#30340;&#38382;&#39064;&#20316;&#20026;&#36755;&#20837;&#24182;&#29983;&#25104;&#25152;&#38656;SPARQL&#26597;&#35810;&#30340;&#31616;&#21270;&#24418;&#24335;&#12290;&#22312;&#31616;&#21270;&#24418;&#24335;&#20013;&#65292;&#27169;&#22411;&#19981;&#30452;&#25509;&#29983;&#25104;&#23454;&#20307;&#21644;&#20851;&#31995;ID&#65292;&#32780;&#26159;&#20135;&#29983;&#30456;&#24212;&#30340;&#23454;&#20307;&#21644;&#20851;&#31995;&#26631;&#31614;&#12290;&#26631;&#31614;&#22312;&#38543;&#21518;&#30340;&#27493;&#39588;&#20013;&#19982;KG&#23454;&#20307;&#21644;&#20851;&#31995;ID&#32852;&#31995;&#36215;&#26469;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25913;&#36827;&#32467;&#26524;&#65292;&#25105;&#20204;&#25351;&#23548;&#27169;&#22411;&#20026;&#27599;&#20010;&#23454;&#20307;&#29983;&#25104;KG&#23884;&#20837;&#30340;&#25130;&#26029;&#29256;&#26412;&#12290;&#25130;&#26029;&#30340;KG&#23884;&#20837;&#20351;&#24471;&#26356;&#31934;&#32454;&#30340;&#25628;&#32034;&#20174;&#32780;&#26356;&#26377;&#25928;&#36827;&#34892;&#28040;&#27495;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;T5&#33021;&#22815;&#22312;&#19981;&#25913;&#21464;&#25439;&#22833;&#20989;&#25968;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#25130;&#26029;&#30340;KG&#23884;&#20837;&#65292;&#25552;&#39640;&#20102;KGQA&#30340;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#22312;Wikidata&#30340;LC-QuAD 2.0&#21644;SimpleQuestions-Wikidata&#25968;&#25454;&#38598;&#19978;&#25253;&#21578;&#20102;&#31471;&#21040;&#31471;KGQA&#30340;&#24378;&#22823;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we present an end-to-end Knowledge Graph Question Answering (KGQA) system named GETT-QA. GETT-QA uses T5, a popular text-to-text pre-trained language model. The model takes a question in natural language as input and produces a simpler form of the intended SPARQL query. In the simpler form, the model does not directly produce entity and relation IDs. Instead, it produces corresponding entity and relation labels. The labels are grounded to KG entity and relation IDs in a subsequent step. To further improve the results, we instruct the model to produce a truncated version of the KG embedding for each entity. The truncated KG embedding enables a finer search for disambiguation purposes. We find that T5 is able to learn the truncated KG embeddings without any change of loss function, improving KGQA performance. As a result, we report strong results for LC-QuAD 2.0 and SimpleQuestions-Wikidata datasets on end-to-end KGQA over Wikidata.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#21327;&#20316;&#22495;&#21069;&#32512;&#35843;&#25972;&#30340;&#36328;&#39046;&#22495;&#23454;&#20307;&#35782;&#21035;&#65292;&#20351;&#29992;&#25991;&#26412;&#21040;&#25991;&#26412;&#29983;&#25104;&#30340;&#25903;&#25745;&#39046;&#22495;&#30456;&#20851;&#25351;&#23548;&#26469;&#23558;&#30693;&#35782;&#36716;&#31227;&#33267;&#26032;&#22495;NER&#20219;&#21153;&#65292;&#36991;&#20813;&#20102;&#20808;&#21069;&#30340;&#20026;&#27599;&#20010;&#39046;&#22495;&#32467;&#26463;&#19968;&#20010;&#20840;&#26032;&#30340;NER&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2301.10410</link><description>&lt;p&gt;
&#36866;&#29992;&#20110;&#25152;&#26377;&#39046;&#22495;&#30340;&#19968;&#20010;&#27169;&#22411;&#65306;&#22522;&#20110;&#21327;&#20316;&#22495;&#21069;&#32512;&#35843;&#25972;&#30340;&#36328;&#39046;&#22495;&#23454;&#20307;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
One Model for All Domains: Collaborative Domain-Prefix Tuning for Cross-Domain NER. (arXiv:2301.10410v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10410
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#21327;&#20316;&#22495;&#21069;&#32512;&#35843;&#25972;&#30340;&#36328;&#39046;&#22495;&#23454;&#20307;&#35782;&#21035;&#65292;&#20351;&#29992;&#25991;&#26412;&#21040;&#25991;&#26412;&#29983;&#25104;&#30340;&#25903;&#25745;&#39046;&#22495;&#30456;&#20851;&#25351;&#23548;&#26469;&#23558;&#30693;&#35782;&#36716;&#31227;&#33267;&#26032;&#22495;NER&#20219;&#21153;&#65292;&#36991;&#20813;&#20102;&#20808;&#21069;&#30340;&#20026;&#27599;&#20010;&#39046;&#22495;&#32467;&#26463;&#19968;&#20010;&#20840;&#26032;&#30340;NER&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#20915;&#23454;&#38469;&#22330;&#26223;&#20013;&#20302;&#36164;&#28304;&#38382;&#39064;&#26159;&#36328;&#39046;&#22495;&#23454;&#20307;&#35782;&#21035;&#30340;&#19968;&#20010;&#25361;&#25112;&#24615;&#20219;&#21153;&#12290;&#20808;&#21069;&#20856;&#22411;&#30340;&#35299;&#20915;&#26041;&#26696;&#20027;&#35201;&#36890;&#36807;&#20351;&#29992;&#26469;&#33258;&#20016;&#23500;&#36164;&#28304;&#39046;&#22495;&#30340;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(PLMs)&#33719;&#24471;NER&#27169;&#22411;&#24182;&#23558;&#20854;&#36866;&#24212;&#20110;&#30446;&#26631;&#39046;&#22495;&#12290;&#30001;&#20110;&#19981;&#21516;&#39046;&#22495;&#23454;&#20307;&#31867;&#22411;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#20808;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#35843;&#25972;&#25152;&#26377;PLMs&#30340;&#21442;&#25968;&#65292;&#20174;&#32780;&#20026;&#27599;&#20010;&#39046;&#22495;&#32467;&#26463;&#19968;&#20010;&#20840;&#26032;&#30340;NER&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#24403;&#21069;&#30340;&#27169;&#22411;&#21482;&#20851;&#27880;&#20110;&#21033;&#29992;&#19968;&#20010;&#26222;&#36890;&#26469;&#28304;&#39046;&#22495;&#20013;&#30340;&#30693;&#35782;&#65292;&#32780;&#26410;&#33021;&#25104;&#21151;&#22320;&#23558;&#26469;&#33258;&#22810;&#20010;&#26469;&#28304;&#39046;&#22495;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#30446;&#26631;&#19978;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#22522;&#20110;&#25991;&#26412;&#21040;&#25991;&#26412;&#29983;&#25104;&#30340;PLM&#24341;&#20837;&#20102;&#21327;&#20316;&#22495;&#21069;&#32512;&#35843;&#25972;&#36328;&#39046;&#22495;NER(CP-NER)&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21576;&#29616;&#20102;&#29992;&#20110;&#25991;&#26412;&#21040;&#25991;&#26412;&#29983;&#25104;&#30340;&#25903;&#25745;&#39046;&#22495;&#30456;&#20851;&#25351;&#23548;&#26469;&#23558;&#30693;&#35782;&#36716;&#31227;&#33267;&#26032;&#22495;NER&#20219;&#21153;&#32780;&#26080;&#38656;&#32467;&#26500;&#20462;&#25913;&#12290;&#25105;&#20204;&#21033;&#29992;&#20923;&#32467;&#30340;PLMs&#24182;&#36827;&#34892;&#21327;&#20316;&#22495;&#21069;&#32512;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-domain NER is a challenging task to address the low-resource problem in practical scenarios. Previous typical solutions mainly obtain a NER model by pre-trained language models (PLMs) with data from a rich-resource domain and adapt it to the target domain. Owing to the mismatch issue among entity types in different domains, previous approaches normally tune all parameters of PLMs, ending up with an entirely new NER model for each domain. Moreover, current models only focus on leveraging knowledge in one general source domain while failing to successfully transfer knowledge from multiple sources to the target. To address these issues, we introduce Collaborative Domain-Prefix Tuning for cross-domain NER (CP-NER) based on text-to-text generative PLMs. Specifically, we present text-to-text generation grounding domain-related instructors to transfer knowledge to new domain NER tasks without structural modifications. We utilize frozen PLMs and conduct collaborative domain-prefix tuning
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25506;&#35752;&#20102;&#27169;&#22411;&#22312;&#35299;&#20915;&#30495;&#23454;&#19990;&#30028;&#38382;&#39064;&#21644;&#22522;&#20934;&#25968;&#25454;&#38598;&#20013;&#34920;&#29616;&#19981;&#21516;&#30340;&#21407;&#22240;&#65292;&#25351;&#20986;&#27169;&#22411;&#21463;&#21040;&#25968;&#25454;&#38598;&#20449;&#24687;&#38656;&#27714;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2212.02726</link><description>&lt;p&gt;
&#25968;&#25454;&#38598;&#19982;&#29616;&#23454;&#65306;&#20174;&#20449;&#24687;&#38656;&#27714;&#30340;&#35282;&#24230;&#29702;&#35299;&#27169;&#22411;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Dataset vs Reality: Understanding Model Performance from the Perspective of Information Need. (arXiv:2212.02726v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.02726
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25506;&#35752;&#20102;&#27169;&#22411;&#22312;&#35299;&#20915;&#30495;&#23454;&#19990;&#30028;&#38382;&#39064;&#21644;&#22522;&#20934;&#25968;&#25454;&#38598;&#20013;&#34920;&#29616;&#19981;&#21516;&#30340;&#21407;&#22240;&#65292;&#25351;&#20986;&#27169;&#22411;&#21463;&#21040;&#25968;&#25454;&#38598;&#20449;&#24687;&#38656;&#27714;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#24102;&#26469;&#20102;&#35768;&#22810;&#22312;&#19968;&#20123;&#22522;&#20934;&#27979;&#35797;&#20013;&#32988;&#36807;&#20154;&#31867;&#30340;&#27169;&#22411;&#12290;&#19968;&#20010;&#26377;&#36259;&#30340;&#38382;&#39064;&#26159;&#65306;&#36825;&#20123;&#27169;&#22411;&#33021;&#21542;&#24456;&#22909;&#22320;&#35299;&#20915;&#19982;&#22522;&#20934;&#25968;&#25454;&#38598;&#30456;&#20284;&#30340;&#30495;&#23454;&#19990;&#30028;&#38382;&#39064;&#65288;&#20363;&#22914;&#65292;&#30456;&#21516;&#30340;&#36755;&#20837;/&#36755;&#20986;&#65289;&#65311;&#25105;&#20204;&#35748;&#20026;&#19968;&#20010;&#27169;&#22411;&#26159;&#34987;&#35757;&#32451;&#26469;&#22238;&#31572;&#21019;&#24314;&#35757;&#32451;&#25968;&#25454;&#38598;&#26102;&#30456;&#21516;&#30340;&#20449;&#24687;&#38656;&#27714;&#30340;&#12290;&#34429;&#28982;&#19968;&#20123;&#25968;&#25454;&#38598;&#21487;&#33021;&#20855;&#26377;&#39640;&#24230;&#32467;&#26500;&#30456;&#20284;&#24615;&#65292;&#20363;&#22914;&#38382;&#31572;&#65288;QA&#65289;&#20219;&#21153;&#30340;&#38382;&#31572;&#23545;&#21644;&#22270;&#20687;&#23383;&#24149;&#65288;IC&#65289;&#20219;&#21153;&#30340;&#22270;&#20687;&#23383;&#24149;&#23545;&#65292;&#20294;&#23427;&#20204;&#21487;&#33021;&#20195;&#34920;&#19981;&#21516;&#30340;&#30740;&#31350;&#20219;&#21153;&#65292;&#26088;&#22312;&#22238;&#31572;&#19981;&#21516;&#30340;&#20449;&#24687;&#38656;&#27714;&#12290;&#20026;&#20102;&#25903;&#25345;&#25105;&#20204;&#30340;&#35770;&#28857;&#65292;&#25105;&#20204;&#20351;&#29992;&#38382;&#31572;&#20219;&#21153;&#21644;&#22270;&#20687;&#23383;&#24149;&#20219;&#21153;&#20316;&#20026;&#20004;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#24182;&#27604;&#36739;&#23427;&#20204;&#30340;&#24191;&#27867;&#20351;&#29992;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#20174;&#20449;&#24687;&#26816;&#32034;&#30340;&#20449;&#24687;&#38656;&#27714;&#35282;&#24230;&#20986;&#21457;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25968;&#25454;&#38598;&#21019;&#24314;&#36807;&#31243;&#20013;&#30340;&#24046;&#24322;&#20197;&#21450;&#25968;&#25454;&#38598;&#20043;&#38388;&#24418;&#24577;&#21644;&#35821;&#27861;&#23646;&#24615;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning technologies have brought us many models that outperform human beings on a few benchmarks. An interesting question is: can these models well solve real-world problems with similar settings (e.g., identical input/output) to the benchmark datasets? We argue that a model is trained to answer the same information need for which the training dataset is created. Although some datasets may share high structural similarities, e.g., question-answer pairs for the question answering (QA) task and image-caption pairs for the image captioning (IC) task, they may represent different research tasks aiming for answering different information needs. To support our argument, we use the QA task and IC task as two case studies and compare their widely used benchmark datasets. From the perspective of information need in the context of information retrieval, we show the differences in the dataset creation processes, and the differences in morphosyntactic properties between datasets. The differ
&lt;/p&gt;</description></item></channel></rss>