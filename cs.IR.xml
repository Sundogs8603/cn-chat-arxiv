<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20004;&#38454;&#27573;&#26041;&#27861;&#65292;&#29992;&#20110;&#26174;&#24335;&#22320;&#24314;&#27169;&#23618;&#27425;&#21270;&#22810;&#20852;&#36259;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#23618;&#27425;&#32858;&#31867;&#21644;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#26469;&#25366;&#25496;&#23618;&#27425;&#21270;&#30340;&#22810;&#20852;&#36259;&#20449;&#24687;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01253</link><description>&lt;p&gt;
HimiRec: &#24314;&#27169;&#23618;&#27425;&#21270;&#22810;&#20852;&#36259;&#30340;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
HimiRec: Modeling Hierarchical Multi-interest for Recommendation
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01253
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20004;&#38454;&#27573;&#26041;&#27861;&#65292;&#29992;&#20110;&#26174;&#24335;&#22320;&#24314;&#27169;&#23618;&#27425;&#21270;&#22810;&#20852;&#36259;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#23618;&#27425;&#32858;&#31867;&#21644;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#26469;&#25366;&#25496;&#23618;&#27425;&#21270;&#30340;&#22810;&#20852;&#36259;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24037;&#19994;&#32423;&#25512;&#33616;&#31995;&#32479;&#36890;&#24120;&#21253;&#21547;&#26816;&#32034;&#38454;&#27573;&#21644;&#25490;&#21517;&#38454;&#27573;&#65292;&#20197;&#22788;&#29702;&#20159;&#32423;&#29992;&#25143;&#21644;&#29289;&#21697;&#12290;&#26816;&#32034;&#38454;&#27573;&#29992;&#20110;&#26816;&#32034;&#19982;&#29992;&#25143;&#20852;&#36259;&#30456;&#20851;&#30340;&#20505;&#36873;&#29289;&#21697;&#36827;&#34892;&#25512;&#33616;&#65292;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#32463;&#24120;&#24773;&#20917;&#19979;&#65292;&#29992;&#25143;&#23637;&#31034;&#20986;&#23618;&#27425;&#21270;&#30340;&#22810;&#20010;&#20852;&#36259;&#65292;&#27604;&#22914;&#19968;&#20010;&#22312;&#20307;&#32946;&#20013;&#28909;&#34935;&#25903;&#25345;&#37329;&#24030;&#21191;&#22763;&#38431;&#30340;&#29992;&#25143;&#65292;&#20063;&#20250;&#23545;&#20960;&#20046;&#25152;&#26377;&#21160;&#30011;&#26377;&#20852;&#36259;&#65292;&#20307;&#32946;&#21644;&#21160;&#30011;&#22788;&#20110;&#21516;&#26679;&#30340;&#23618;&#27425;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#38544;&#24335;&#22320;&#23398;&#20064;&#36825;&#31181;&#23618;&#27425;&#21270;&#24046;&#24322;&#65292;&#23548;&#33268;&#26356;&#32454;&#31890;&#24230;&#30340;&#20852;&#36259;&#20449;&#24687;&#34987;&#24179;&#22343;&#21270;&#65292;&#38480;&#21046;&#20102;&#23545;&#29992;&#25143;&#22312;&#28909;&#38376;&#20852;&#36259;&#21644;&#20854;&#20182;&#36731;&#20852;&#36259;&#26041;&#38754;&#30340;&#35814;&#32454;&#29702;&#35299;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20004;&#38454;&#27573;&#26041;&#27861;&#65292;&#29992;&#20110;&#26174;&#24335;&#22320;&#24314;&#27169;&#23618;&#27425;&#21270;&#22810;&#20852;&#36259;&#30340;&#25512;&#33616;&#31995;&#32479;&#12290;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;&#25105;&#20204;&#20351;&#29992;&#23618;&#27425;&#32858;&#31867;&#21644;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#26469;&#25366;&#25496;&#23618;&#27425;&#21270;&#30340;&#22810;&#20852;&#36259;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Industrial recommender systems usually consist of the retrieval stage and the ranking stage, to handle the billion-scale of users and items. The retrieval stage retrieves candidate items relevant to user interests for recommendations and has attracted much attention. Frequently, users show hierarchical multi-interests reflected in a heavy user of a certain NBA team Golden State Warriors in Sports, who is also a light user of almost the whole Animation. Both Sports and Animation are at the same level. However, most existing methods implicitly learn this hierarchical difference, making more fine-grained interest information to be averaged and limiting detailed understanding of the user's different needs in heavy interests and other light interests. Therefore, we propose a novel two-stage approach to explicitly modeling hierarchical multi-interest for recommendation in this work. In the first hierarchical multi-interest mining stage, the hierarchical clustering and transformer-based model
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#20107;&#20214;&#30340;&#20135;&#21697;&#25512;&#33616;&#36718;&#25773;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26597;&#35810;-&#28857;&#20987;&#20108;&#37096;&#22270;&#30340;&#26377;&#25928;&#25512;&#33616;&#31995;&#32479;&#26469;&#35299;&#20915;&#35813;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.03277</link><description>&lt;p&gt;
&#22522;&#20110;&#20107;&#20214;&#30340;&#26597;&#35810;-&#28857;&#20987;&#22270;&#30340;&#20135;&#21697;&#25512;&#33616;&#36718;&#25773;
&lt;/p&gt;
&lt;p&gt;
Event-based Product Carousel Recommendation with Query-Click Graph
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03277
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#20107;&#20214;&#30340;&#20135;&#21697;&#25512;&#33616;&#36718;&#25773;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26597;&#35810;-&#28857;&#20987;&#20108;&#37096;&#22270;&#30340;&#26377;&#25928;&#25512;&#33616;&#31995;&#32479;&#26469;&#35299;&#20915;&#35813;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#35768;&#22810;&#25512;&#33616;&#31995;&#32479;&#20027;&#35201;&#20851;&#27880;&#30340;&#26159;&#20135;&#21697;&#23545;&#20135;&#21697;&#30340;&#25512;&#33616;&#21644;&#29992;&#25143;&#23545;&#20135;&#21697;&#30340;&#25512;&#33616;&#65292;&#21363;&#20351;&#22312;&#20107;&#20214;&#21457;&#29983;&#26102;&#20063;&#27809;&#26377;&#27169;&#25311;&#30446;&#26631;&#20107;&#20214;&#30340;&#20856;&#22411;&#25512;&#33616;&#65288;&#20363;&#22914;&#33410;&#26085;&#12289;&#23395;&#33410;&#27963;&#21160;&#25110;&#31038;&#20132;&#27963;&#21160;&#65289;&#65292;&#32780;&#24573;&#30053;&#20102;&#30446;&#26631;&#20107;&#20214;&#30340;&#22810;&#26041;&#38754;&#36141;&#29289;&#38656;&#27714;&#12290;&#36890;&#24120;&#65292;&#38024;&#23545;&#30446;&#26631;&#20107;&#20214;&#30340;&#22810;&#20010;&#26041;&#38754;&#30340;&#20135;&#21697;&#25512;&#33616;&#26159;&#30001;&#20154;&#24037;&#31574;&#21010;&#32773;&#29983;&#25104;&#30340;&#65292;&#20182;&#20204;&#25163;&#21160;&#35782;&#21035;&#20986;&#21508;&#20010;&#26041;&#38754;&#65292;&#24182;&#36873;&#25321;&#19968;&#31995;&#21015;&#19982;&#26041;&#38754;&#30456;&#20851;&#30340;&#20135;&#21697;&#65288;&#21363;&#20135;&#21697;&#36718;&#25773;&#65289;&#20316;&#20026;&#25512;&#33616;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#20107;&#20214;&#30456;&#20851;&#26041;&#38754;&#21644;&#30456;&#20851;&#20135;&#21697;&#30340;&#20934;&#30830;&#20449;&#24687;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26500;&#24314;&#25512;&#33616;&#31995;&#32479;&#26159;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#22312;&#30005;&#23376;&#21830;&#21153;&#39046;&#22495;&#23450;&#20041;&#20102;&#26032;&#38382;&#39064;&#65292;&#21363;&#22522;&#20110;&#20107;&#20214;&#30340;&#20135;&#21697;&#25512;&#33616;&#36718;&#25773;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26597;&#35810;-&#28857;&#20987;&#20108;&#37096;&#22270;&#30340;&#26377;&#25928;&#25512;&#33616;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many current recommender systems mainly focus on the product-to-product recommendations and user-to-product recommendations even during the time of events rather than modeling the typical recommendations for the target event (e.g., festivals, seasonal activities, or social activities) without addressing the multiple aspects of the shopping demands for the target event. Product recommendations for the multiple aspects of the target event are usually generated by human curators who manually identify the aspects and select a list of aspect-related products (i.e., product carousel) for each aspect as recommendations. However, building a recommender system with machine learning is non-trivial due to the lack of both the ground truth of event-related aspects and the aspect-related products. To fill this gap, we define the novel problem as the event-based product carousel recommendations in e-commerce and propose an effective recommender system based on the query-click bipartite graph. We app
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#24187;&#35273;&#26816;&#27979;&#26694;&#26550;UNIHD&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#35780;&#20272;&#22522;&#20934;&#26041;&#27861;MHaluBench&#26469;&#35780;&#20272;&#24187;&#35273;&#26816;&#27979;&#26041;&#27861;&#30340;&#36827;&#23637;&#12290;&#36825;&#39033;&#24037;&#20316;&#25193;&#23637;&#20102;&#24187;&#35273;&#26816;&#27979;&#30340;&#30740;&#31350;&#33539;&#22260;&#24182;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2402.03190</link><description>&lt;p&gt;
&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24187;&#35273;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Unified Hallucination Detection for Multimodal Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03190
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#24187;&#35273;&#26816;&#27979;&#26694;&#26550;UNIHD&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#35780;&#20272;&#22522;&#20934;&#26041;&#27861;MHaluBench&#26469;&#35780;&#20272;&#24187;&#35273;&#26816;&#27979;&#26041;&#27861;&#30340;&#36827;&#23637;&#12290;&#36825;&#39033;&#24037;&#20316;&#25193;&#23637;&#20102;&#24187;&#35273;&#26816;&#27979;&#30340;&#30740;&#31350;&#33539;&#22260;&#24182;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#22810;&#27169;&#24577;&#20219;&#21153;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(MLLMs)&#20173;&#28982;&#23384;&#22312;&#24187;&#35273;&#30340;&#20005;&#37325;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#21487;&#38752;&#22320;&#26816;&#27979;MLLMs&#20013;&#30340;&#24187;&#35273;&#24050;&#25104;&#20026;&#27169;&#22411;&#35780;&#20272;&#21644;&#23454;&#38469;&#24212;&#29992;&#37096;&#32626;&#20445;&#38556;&#30340;&#37325;&#35201;&#26041;&#38754;&#12290;&#20043;&#21069;&#22312;&#36825;&#20010;&#39046;&#22495;&#30340;&#30740;&#31350;&#21463;&#21040;&#20102;&#29421;&#31364;&#30340;&#20219;&#21153;&#28966;&#28857;&#12289;&#19981;&#36275;&#30340;&#24187;&#35273;&#31867;&#21035;&#28085;&#30422;&#33539;&#22260;&#20197;&#21450;&#32570;&#20047;&#35814;&#32454;&#30340;&#32454;&#31890;&#24230;&#30340;&#38480;&#21046;&#12290;&#38024;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#25193;&#23637;&#20102;&#24187;&#35273;&#26816;&#27979;&#30340;&#30740;&#31350;&#33539;&#22260;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20803;&#35780;&#20272;&#22522;&#20934;&#26041;&#27861;&#65292;MHaluBench&#65292;&#31934;&#24515;&#35774;&#35745;&#20197;&#20419;&#36827;&#24187;&#35273;&#26816;&#27979;&#26041;&#27861;&#30340;&#36827;&#23637;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#32479;&#19968;&#22810;&#27169;&#24577;&#24187;&#35273;&#26816;&#27979;&#26694;&#26550;&#65292;UNIHD&#65292;&#23427;&#21033;&#29992;&#19968;&#22871;&#36741;&#21161;&#24037;&#20855;&#26469;&#31283;&#20581;&#22320;&#39564;&#35777;&#24187;&#35273;&#30340;&#21457;&#29983;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;UNIHD&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite significant strides in multimodal tasks, Multimodal Large Language Models (MLLMs) are plagued by the critical issue of hallucination. The reliable detection of such hallucinations in MLLMs has, therefore, become a vital aspect of model evaluation and the safeguarding of practical application deployment. Prior research in this domain has been constrained by a narrow focus on singular tasks, an inadequate range of hallucination categories addressed, and a lack of detailed granularity. In response to these challenges, our work expands the investigative horizons of hallucination detection. We present a novel meta-evaluation benchmark, MHaluBench, meticulously crafted to facilitate the evaluation of advancements in hallucination detection methods. Additionally, we unveil a novel unified multimodal hallucination detection framework, UNIHD, which leverages a suite of auxiliary tools to validate the occurrence of hallucinations robustly. We demonstrate the effectiveness of UNIHD throug
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#38134;&#34892;&#19994;&#32972;&#26223;&#19979;&#27604;&#36739;&#20102;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;KernelPCA&#21644;K-means Clustering&#22312;BERTopic&#26550;&#26500;&#20013;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#36830;&#36143;&#30340;&#20027;&#39064;&#65292;&#36830;&#36143;&#24615;&#24471;&#20998;&#20026;0.8463&#12290;</title><link>https://arxiv.org/abs/2402.03176</link><description>&lt;p&gt;
&#38134;&#34892;&#19994;&#32972;&#26223;&#19979;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#30340;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Comparison of Topic Modelling Approaches in the Banking Context
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03176
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#38134;&#34892;&#19994;&#32972;&#26223;&#19979;&#27604;&#36739;&#20102;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;KernelPCA&#21644;K-means Clustering&#22312;BERTopic&#26550;&#26500;&#20013;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#36830;&#36143;&#30340;&#20027;&#39064;&#65292;&#36830;&#36143;&#24615;&#24471;&#20998;&#20026;0.8463&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#39064;&#24314;&#27169;&#26159;&#35768;&#22810;&#24212;&#29992;&#31243;&#24207;&#20013;&#33258;&#21160;&#25552;&#21462;&#20027;&#39064;&#30340;&#37325;&#35201;&#20219;&#21153;&#65292;&#20363;&#22914;&#24773;&#24863;&#20998;&#26512;&#21644;&#25512;&#33616;&#31995;&#32479;&#12290;&#35813;&#26041;&#27861;&#23545;&#20110;&#26381;&#21153;&#34892;&#19994;&#26469;&#35828;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#21487;&#20197;&#29992;&#20110;&#30417;&#25511;&#23458;&#25143;&#35752;&#35770;&#12290;&#20256;&#32479;&#26041;&#27861;&#22914;&#28508;&#22312;&#29380;&#21033;&#20811;&#38647;&#20998;&#37197;&#65288;Latent Dirichlet Allocation&#65292;LDA&#65289;&#22312;&#20027;&#39064;&#21457;&#29616;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#30001;&#20110;&#25968;&#25454;&#31232;&#30095;&#24615;&#21644;&#26080;&#27861;&#23545;&#25991;&#26723;&#20013;&#30340;&#21333;&#35789;&#39034;&#24207;&#24314;&#27169;&#65292;&#23427;&#20204;&#30340;&#32467;&#26524;&#24182;&#19981;&#19968;&#33268;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22312;BERTopic&#26550;&#26500;&#20013;&#20351;&#29992;&#26680;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;Kernel Principal Component Analysis&#65292;KernelPCA&#65289;&#21644;K-means&#32858;&#31867;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#23612;&#26085;&#21033;&#20122;&#38134;&#34892;&#23458;&#25143;&#30340;&#25512;&#25991;&#20934;&#22791;&#20102;&#19968;&#20010;&#26032;&#25968;&#25454;&#38598;&#65292;&#28982;&#21518;&#20351;&#29992;&#35813;&#25968;&#25454;&#38598;&#26469;&#27604;&#36739;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;BERTopic&#26550;&#26500;&#20013;&#20351;&#29992;KernelPCA&#21644;K-means&#21487;&#20197;&#29983;&#25104;&#36830;&#36143;&#30340;&#20027;&#39064;&#65292;&#20854;&#20013;&#36830;&#36143;&#24615;&#24471;&#20998;&#20026;0.8463&#12290;
&lt;/p&gt;
&lt;p&gt;
Topic modelling is a prominent task for automatic topic extraction in many applications such as sentiment analysis and recommendation systems. The approach is vital for service industries to monitor their customer discussions. The use of traditional approaches such as Latent Dirichlet Allocation (LDA) for topic discovery has shown great performances, however, they are not consistent in their results as these approaches suffer from data sparseness and inability to model the word order in a document. Thus, this study presents the use of Kernel Principal Component Analysis (KernelPCA) and K-means Clustering in the BERTopic architecture. We have prepared a new dataset using tweets from customers of Nigerian banks and we use this to compare the topic modelling approaches. Our findings showed KernelPCA and K-means in the BERTopic architecture-produced coherent topics with a coherence score of 0.8463.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#38754;&#21521;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#20013;&#21477;&#23376;&#22256;&#38590;&#24230;&#30340;&#23450;&#20041;&#12290;&#36890;&#36807;&#23454;&#39564;&#30740;&#31350;&#20102;&#39046;&#22495;&#22810;&#26679;&#24615;&#21644;&#21477;&#27861;&#22810;&#26679;&#24615;&#23545;&#21477;&#23376;&#22256;&#38590;&#24230;&#30340;&#24433;&#21709;&#65292;&#24182;&#20351;&#29992;&#19968;&#32452;&#20998;&#31867;&#22120;&#35782;&#21035;&#20102;&#26368;&#22256;&#38590;&#30340;&#21477;&#23376;&#12290;</title><link>https://arxiv.org/abs/2402.03163</link><description>&lt;p&gt;
&#29992;&#20110;ABSA&#20013;&#21477;&#23376;&#22256;&#38590;&#24230;&#39044;&#27979;&#30340;&#35821;&#35328;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Linguistic features for sentence difficulty prediction in ABSA
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03163
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#38754;&#21521;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#20013;&#21477;&#23376;&#22256;&#38590;&#24230;&#30340;&#23450;&#20041;&#12290;&#36890;&#36807;&#23454;&#39564;&#30740;&#31350;&#20102;&#39046;&#22495;&#22810;&#26679;&#24615;&#21644;&#21477;&#27861;&#22810;&#26679;&#24615;&#23545;&#21477;&#23376;&#22256;&#38590;&#24230;&#30340;&#24433;&#21709;&#65292;&#24182;&#20351;&#29992;&#19968;&#32452;&#20998;&#31867;&#22120;&#35782;&#21035;&#20102;&#26368;&#22256;&#38590;&#30340;&#21477;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#30340;&#19968;&#20010;&#25361;&#25112;&#26159;&#22788;&#29702;&#21477;&#23376;&#30340;&#20027;&#35266;&#24615;&#65292;&#36825;&#20123;&#21477;&#23376;&#21487;&#33021;&#34920;&#36798;&#24847;&#35265;&#21644;&#24773;&#24863;&#65292;&#22686;&#21152;&#20102;&#22797;&#26434;&#24615;&#21644;&#32454;&#24494;&#24046;&#21035;&#12290;&#24773;&#24863;&#20998;&#26512;&#26159;&#19968;&#20010;&#26088;&#22312;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#21644;&#20998;&#26512;&#36825;&#20123;&#20027;&#35266;&#20803;&#32032;&#30340;&#39046;&#22495;&#65292;&#21487;&#20197;&#24212;&#29992;&#22312;&#19981;&#21516;&#30340;&#31890;&#24230;&#32423;&#21035;&#65292;&#22914;&#25991;&#26723;&#12289;&#27573;&#33853;&#12289;&#21477;&#23376;&#25110;&#26041;&#38754;&#12290;&#38754;&#21521;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#26159;&#19968;&#20010;&#32463;&#36807;&#30740;&#31350;&#30340;&#35838;&#39064;&#65292;&#26377;&#24456;&#22810;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#38754;&#21521;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#26469;&#35828;&#65292;&#20160;&#20040;&#22240;&#32032;&#20351;&#19968;&#20010;&#21477;&#23376;&#21464;&#24471;&#22256;&#38590;&#24182;&#27809;&#26377;&#28165;&#26224;&#30340;&#23450;&#20041;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#19977;&#20010;&#25968;&#25454;&#38598;&#65306;&#8220;&#31508;&#35760;&#26412;&#30005;&#33041;&#8221;&#12289;&#8220;&#39184;&#39302;&#8221;&#21644;&#8220;MTSC&#8221;&#65288;&#22810;&#30446;&#26631;&#20381;&#36182;&#24773;&#24863;&#20998;&#31867;&#65289;&#20197;&#21450;&#36825;&#19977;&#20010;&#25968;&#25454;&#38598;&#30340;&#21512;&#24182;&#29256;&#26412;&#36827;&#34892;&#23454;&#39564;&#65292;&#25506;&#35752;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#39046;&#22495;&#22810;&#26679;&#24615;&#21644;&#21477;&#27861;&#22810;&#26679;&#24615;&#23545;&#22256;&#38590;&#24230;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#32452;&#20998;&#31867;&#22120;&#26469;&#35782;&#21035;&#26368;&#22256;&#38590;&#30340;&#21477;&#23376;&#24182;&#20998;&#26512;&#23427;&#20204;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the challenges of natural language understanding is to deal with the subjectivity of sentences, which may express opinions and emotions that add layers of complexity and nuance. Sentiment analysis is a field that aims to extract and analyze these subjective elements from text, and it can be applied at different levels of granularity, such as document, paragraph, sentence, or aspect. Aspect-based sentiment analysis is a well-studied topic with many available data sets and models. However, there is no clear definition of what makes a sentence difficult for aspect-based sentiment analysis. In this paper, we explore this question by conducting an experiment with three data sets: "Laptops", "Restaurants", and "MTSC" (Multi-Target-dependent Sentiment Classification), and a merged version of these three datasets. We study the impact of domain diversity and syntactic diversity on difficulty. We use a combination of classifiers to identify the most difficult sentences and analyze their c
&lt;/p&gt;</description></item><item><title>EasyInstruct&#26159;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#20196;&#22788;&#29702;&#26694;&#26550;&#65292;&#36890;&#36807;&#27169;&#22359;&#21270;&#25351;&#20196;&#29983;&#25104;&#12289;&#36873;&#25321;&#21644;&#25552;&#31034;&#65292;&#24182;&#32771;&#34385;&#23427;&#20204;&#30340;&#32452;&#21512;&#21644;&#20132;&#20114;&#65292;&#20351;&#25351;&#20196;&#22788;&#29702;&#26356;&#21152;&#26041;&#20415;&#21644;&#39640;&#25928;&#12290;</title><link>https://arxiv.org/abs/2402.03049</link><description>&lt;p&gt;
EasyInstruct&#65306;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#20196;&#22788;&#29702;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
EasyInstruct: An Easy-to-use Instruction Processing Framework for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03049
&lt;/p&gt;
&lt;p&gt;
EasyInstruct&#26159;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#20196;&#22788;&#29702;&#26694;&#26550;&#65292;&#36890;&#36807;&#27169;&#22359;&#21270;&#25351;&#20196;&#29983;&#25104;&#12289;&#36873;&#25321;&#21644;&#25552;&#31034;&#65292;&#24182;&#32771;&#34385;&#23427;&#20204;&#30340;&#32452;&#21512;&#21644;&#20132;&#20114;&#65292;&#20351;&#25351;&#20196;&#22788;&#29702;&#26356;&#21152;&#26041;&#20415;&#21644;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#25351;&#20196;&#35843;&#25972;&#24050;&#32463;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#24182;&#25104;&#20026;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#21147;&#30340;&#19968;&#31181;&#20851;&#38190;&#25216;&#26415;&#12290;&#20026;&#20102;&#26500;&#24314;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#25968;&#25454;&#38598;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#25351;&#20196;&#22788;&#29702;&#26041;&#27861;&#65292;&#26088;&#22312;&#22312;&#25968;&#25454;&#25968;&#37327;&#21644;&#25968;&#25454;&#36136;&#37327;&#20043;&#38388;&#36798;&#21040;&#31934;&#24039;&#30340;&#24179;&#34913;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21508;&#31181;&#25351;&#20196;&#22788;&#29702;&#26041;&#27861;&#20043;&#38388;&#20173;&#28982;&#23384;&#22312;&#19981;&#19968;&#33268;&#65292;&#30446;&#21069;&#27809;&#26377;&#26631;&#20934;&#30340;&#24320;&#28304;&#25351;&#20196;&#22788;&#29702;&#23454;&#29616;&#26694;&#26550;&#21487;&#20379;&#31038;&#21306;&#20351;&#29992;&#65292;&#36825;&#20351;&#24471;&#20174;&#19994;&#32773;&#26080;&#27861;&#36827;&#19968;&#27493;&#24320;&#21457;&#21644;&#25512;&#36827;&#12290;&#20026;&#20102;&#20419;&#36827;&#25351;&#20196;&#22788;&#29702;&#30340;&#30740;&#31350;&#21644;&#24320;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EasyInstruct&#65292;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;&#29992;&#20110;LLMs&#30340;&#25351;&#20196;&#22788;&#29702;&#26694;&#26550;&#65292;&#23427;&#23558;&#25351;&#20196;&#29983;&#25104;&#12289;&#36873;&#25321;&#21644;&#25552;&#31034;&#27169;&#22359;&#21270;&#65292;&#24182;&#32771;&#34385;&#23427;&#20204;&#30340;&#32452;&#21512;&#21644;&#20132;&#20114;&#12290;EasyInstruct&#24050;&#32463;&#22312;https://github.com/zjunlp/EasyInstruct&#19978;&#20844;&#24320;&#21457;&#24067;&#65292;&#24182;&#24471;&#21040;&#20102;&#31215;&#26497;&#32500;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, instruction tuning has gained increasing attention and emerged as a crucial technique to enhance the capabilities of Large Language Models (LLMs). To construct high-quality instruction datasets, many instruction processing approaches have been proposed, aiming to achieve a delicate balance between data quantity and data quality. Nevertheless, due to inconsistencies that persist among various instruction processing methods, there is no standard open-source instruction processing implementation framework available for the community, which hinders practitioners from further developing and advancing. To facilitate instruction processing research and development, we present EasyInstruct, an easy-to-use instruction processing framework for LLMs, which modularizes instruction generation, selection, and prompting, while also considering their combination and interaction. EasyInstruct is publicly released and actively maintained at https://github.com/zjunlp/EasyInstruct, along 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20256;&#25773;&#35270;&#35282;&#20998;&#26512;&#20102;&#24369;&#30417;&#30563;&#30340;&#23454;&#20307;&#23545;&#40784;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#28508;&#22312;&#21516;&#26500;&#20256;&#25773;&#25805;&#20316;&#31526;&#26469;&#22686;&#24378;&#30693;&#35782;&#22270;&#35889;&#20043;&#38388;&#30340;&#37051;&#22495;&#20449;&#24687;&#20256;&#25773;&#12290;&#36890;&#36807;&#39564;&#35777;&#65292;&#21457;&#29616;&#22522;&#20110;&#32858;&#21512;&#30340;&#23454;&#20307;&#23545;&#40784;&#27169;&#22411;&#20013;&#30340;&#28508;&#22312;&#23545;&#40784;&#23454;&#20307;&#20855;&#26377;&#21516;&#26500;&#23376;&#22270;&#12290;</title><link>https://arxiv.org/abs/2402.03025</link><description>&lt;p&gt;
&#36890;&#36807;&#28508;&#22312;&#21516;&#26500;&#20256;&#25773;&#29702;&#35299;&#21644;&#24341;&#23548;&#24369;&#30417;&#30563;&#30340;&#23454;&#20307;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Understanding and Guiding Weakly Supervised Entity Alignment with Potential Isomorphism Propagation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03025
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20256;&#25773;&#35270;&#35282;&#20998;&#26512;&#20102;&#24369;&#30417;&#30563;&#30340;&#23454;&#20307;&#23545;&#40784;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#28508;&#22312;&#21516;&#26500;&#20256;&#25773;&#25805;&#20316;&#31526;&#26469;&#22686;&#24378;&#30693;&#35782;&#22270;&#35889;&#20043;&#38388;&#30340;&#37051;&#22495;&#20449;&#24687;&#20256;&#25773;&#12290;&#36890;&#36807;&#39564;&#35777;&#65292;&#21457;&#29616;&#22522;&#20110;&#32858;&#21512;&#30340;&#23454;&#20307;&#23545;&#40784;&#27169;&#22411;&#20013;&#30340;&#28508;&#22312;&#23545;&#40784;&#23454;&#20307;&#20855;&#26377;&#21516;&#26500;&#23376;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24369;&#30417;&#30563;&#30340;&#23454;&#20307;&#23545;&#40784;&#26159;&#20351;&#29992;&#26377;&#38480;&#25968;&#37327;&#30340;&#31181;&#23376;&#23545;&#40784;&#65292;&#22312;&#19981;&#21516;&#30693;&#35782;&#22270;&#35889;&#20043;&#38388;&#35782;&#21035;&#31561;&#20215;&#23454;&#20307;&#30340;&#20219;&#21153;&#12290;&#23613;&#31649;&#22312;&#22522;&#20110;&#32858;&#21512;&#30340;&#24369;&#30417;&#30563;&#23454;&#20307;&#23545;&#40784;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#22312;&#36825;&#31181;&#35774;&#32622;&#19979;&#30340;&#22522;&#26412;&#26426;&#21046;&#20173;&#26410;&#34987;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20256;&#25773;&#35270;&#35282;&#26469;&#20998;&#26512;&#24369;&#30417;&#30563;&#23454;&#20307;&#23545;&#40784;&#65292;&#24182;&#35299;&#37322;&#20102;&#29616;&#26377;&#30340;&#22522;&#20110;&#32858;&#21512;&#30340;&#23454;&#20307;&#23545;&#40784;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#25581;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#23454;&#36136;&#19978;&#26159;&#23547;&#25214;&#29992;&#20110;&#23545;&#23454;&#20307;&#30456;&#20284;&#24230;&#36827;&#34892;&#20256;&#25773;&#30340;&#25805;&#20316;&#31526;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#26126;&#65292;&#23613;&#31649;&#19981;&#21516;&#30693;&#35782;&#22270;&#35889;&#20043;&#38388;&#23384;&#22312;&#32467;&#26500;&#24322;&#36136;&#24615;&#65292;&#22522;&#20110;&#32858;&#21512;&#30340;&#23454;&#20307;&#23545;&#40784;&#27169;&#22411;&#20013;&#30340;&#28508;&#22312;&#23545;&#40784;&#23454;&#20307;&#20855;&#26377;&#21516;&#26500;&#23376;&#22270;&#65292;&#36825;&#26159;&#23454;&#20307;&#23545;&#40784;&#30340;&#26680;&#24515;&#21069;&#25552;&#65292;&#20294;&#23578;&#26410;&#34987;&#30740;&#31350;&#12290;&#21033;&#29992;&#36825;&#19968;&#27934;&#35265;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#28508;&#22312;&#21516;&#26500;&#20256;&#25773;&#25805;&#20316;&#31526;&#26469;&#22686;&#24378;&#36328;&#30693;&#35782;&#22270;&#35889;&#30340;&#37051;&#22495;&#20449;&#24687;&#20256;&#25773;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#23454;&#20307;&#23545;&#40784;&#26694;&#26550;PipEA&#65292;&#23454;&#29616;&#20102;&#25928;&#26524;&#26174;&#33879;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Weakly Supervised Entity Alignment (EA) is the task of identifying equivalent entities across diverse knowledge graphs (KGs) using only a limited number of seed alignments. Despite substantial advances in aggregation-based weakly supervised EA, the underlying mechanisms in this setting remain unexplored. In this paper, we present a propagation perspective to analyze weakly supervised EA and explain the existing aggregation-based EA models. Our theoretical analysis reveals that these models essentially seek propagation operators for pairwise entity similarities. We further prove that, despite the structural heterogeneity of different KGs, the potentially aligned entities within aggregation-based EA models have isomorphic subgraphs, which is the core premise of EA but has not been investigated. Leveraging this insight, we introduce a potential isomorphism propagation operator to enhance the propagation of neighborhood information across KGs. We develop a general EA framework, PipEA, inco
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#25991;&#29486;&#32508;&#36848;&#23545;&#24403;&#21069;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#36827;&#34892;&#39046;&#22495;&#33258;&#36866;&#24212;&#21644;&#22810;&#35821;&#20041;&#25628;&#32034;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#27010;&#36848;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#32858;&#31867;&#21644;&#26377;&#25928;&#22320;&#32452;&#21512;&#36825;&#20123;&#26041;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#23558;&#22810;&#35821;&#20041;&#25628;&#32034;&#19982;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#36827;&#34892;&#32452;&#21512;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02932</link><description>&lt;p&gt;
&#22810;&#35821;&#20041;&#25628;&#32034;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;- &#25991;&#29486;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Domain Adaptation of Multilingual Semantic Search - Literature Review
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02932
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#25991;&#29486;&#32508;&#36848;&#23545;&#24403;&#21069;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#36827;&#34892;&#39046;&#22495;&#33258;&#36866;&#24212;&#21644;&#22810;&#35821;&#20041;&#25628;&#32034;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#27010;&#36848;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#32858;&#31867;&#21644;&#26377;&#25928;&#22320;&#32452;&#21512;&#36825;&#20123;&#26041;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#23558;&#22810;&#35821;&#20041;&#25628;&#32034;&#19982;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#36827;&#34892;&#32452;&#21512;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#25991;&#29486;&#32508;&#36848;&#27010;&#36848;&#20102;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#36827;&#34892;&#39046;&#22495;&#33258;&#36866;&#24212;&#21644;&#36827;&#34892;&#22810;&#35821;&#20041;&#25628;&#32034;&#30340;&#24403;&#21069;&#26041;&#27861;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#31867;&#26041;&#27861;&#26469;&#23545;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#36827;&#34892;&#32858;&#31867;&#65292;&#22522;&#20110;&#23494;&#38598;&#25991;&#26412;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#30340;&#37096;&#20998;&#65292;&#24182;&#27880;&#37325;&#22914;&#20309;&#26377;&#25928;&#22320;&#23558;&#23427;&#20204;&#36827;&#34892;&#32452;&#21512;&#12290;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#23558;&#22810;&#35821;&#20041;&#25628;&#32034;&#19982;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#30340;&#23494;&#38598;&#26816;&#32034;&#22120;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#36827;&#34892;&#32452;&#21512;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This literature review gives an overview of current approaches to perform domain adaptation in a low-resource and approaches to perform multilingual semantic search in a low-resource setting. We developed a new typology to cluster domain adaptation approaches based on the part of dense textual information retrieval systems, which they adapt, focusing on how to combine them efficiently. We also explore the possibilities of combining multilingual semantic search with domain adaptation approaches for dense retrievers in a low-resource setting.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#25512;&#33616;&#27169;&#22411;&#30340;&#26032;&#22411;&#23398;&#20064;&#33539;&#24335;&#65292;&#31216;&#20026;&#21160;&#24577;&#31232;&#30095;&#23398;&#20064;&#65288;DSL&#65289;&#65292;&#36890;&#36807;&#20174;&#22836;&#35757;&#32451;&#19968;&#20010;&#36731;&#37327;&#32423;&#31232;&#30095;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#27169;&#22411;&#22823;&#23567;&#21644;&#23398;&#20064;&#25928;&#29575;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.02855</link><description>&lt;p&gt;
&#21160;&#24577;&#31232;&#30095;&#23398;&#20064;&#65306;&#39640;&#25928;&#25512;&#33616;&#30340;&#19968;&#31181;&#26032;&#33539;&#24335;
&lt;/p&gt;
&lt;p&gt;
Dynamic Sparse Learning: A Novel Paradigm for Efficient Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02855
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#25512;&#33616;&#27169;&#22411;&#30340;&#26032;&#22411;&#23398;&#20064;&#33539;&#24335;&#65292;&#31216;&#20026;&#21160;&#24577;&#31232;&#30095;&#23398;&#20064;&#65288;DSL&#65289;&#65292;&#36890;&#36807;&#20174;&#22836;&#35757;&#32451;&#19968;&#20010;&#36731;&#37327;&#32423;&#31232;&#30095;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#27169;&#22411;&#22823;&#23567;&#21644;&#23398;&#20064;&#25928;&#29575;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#25512;&#33616;&#31995;&#32479;&#39046;&#22495;&#20013;&#65292;&#26085;&#30410;&#22686;&#38271;&#30340;&#29992;&#25143;&#21644;&#29289;&#21697;&#25968;&#37327;&#25152;&#24102;&#26469;&#30340;&#35745;&#31639;&#38656;&#27714;&#22686;&#21152;&#65292;&#32473;&#23454;&#38469;&#37096;&#32626;&#24102;&#26469;&#20102;&#26174;&#33879;&#25361;&#25112;&#12290;&#36825;&#20010;&#25361;&#25112;&#20027;&#35201;&#26377;&#20004;&#20010;&#26041;&#38754;&#65306;&#22312;&#38477;&#20302;&#27169;&#22411;&#22823;&#23567;&#30340;&#21516;&#26102;&#65292;&#26377;&#25928;&#23398;&#20064;&#29992;&#25143;&#21644;&#29289;&#21697;&#34920;&#31034;&#20197;&#23454;&#29616;&#39640;&#25928;&#25512;&#33616;&#12290;&#23613;&#31649;&#27169;&#22411;&#21387;&#32553;&#21644;&#26550;&#26500;&#25628;&#32034;&#26041;&#38754;&#26377;&#20102;&#30456;&#24403;&#22823;&#30340;&#36827;&#23637;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#38754;&#20020;&#30528;&#26126;&#26174;&#30340;&#38480;&#21046;&#12290;&#20854;&#20013;&#21253;&#25324;&#27169;&#22411;&#21387;&#32553;&#20013;&#39044;&#35757;&#32451;/&#37325;&#26032;&#35757;&#32451;&#30340;&#39069;&#22806;&#35745;&#31639;&#24320;&#38144;&#20197;&#21450;&#26550;&#26500;&#35774;&#35745;&#20013;&#24191;&#27867;&#30340;&#25628;&#32034;&#31354;&#38388;&#12290;&#27492;&#22806;&#65292;&#22312;&#20855;&#26377;&#20005;&#26684;&#26102;&#38388;&#25110;&#31354;&#38388;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#65292;&#31649;&#29702;&#22797;&#26434;&#24615;&#21644;&#36981;&#23432;&#20869;&#23384;&#38480;&#21046;&#26159;&#26377;&#38382;&#39064;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#31216;&#20026;&#21160;&#24577;&#31232;&#30095;&#23398;&#20064;&#65288;DSL&#65289;&#65292;&#19987;&#38376;&#29992;&#20110;&#25512;&#33616;&#27169;&#22411;&#12290;DSL&#21019;&#26032;&#24615;&#22320;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#19968;&#20010;&#36731;&#37327;&#32423;&#31232;&#30095;&#27169;&#22411;&#65292;&#21608;&#26399;&#30340;&#37325;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the realm of deep learning-based recommendation systems, the increasing computational demands, driven by the growing number of users and items, pose a significant challenge to practical deployment. This challenge is primarily twofold: reducing the model size while effectively learning user and item representations for efficient recommendations. Despite considerable advancements in model compression and architecture search, prevalent approaches face notable constraints. These include substantial additional computational costs from pre-training/re-training in model compression and an extensive search space in architecture design. Additionally, managing complexity and adhering to memory constraints is problematic, especially in scenarios with strict time or space limitations. Addressing these issues, this paper introduces a novel learning paradigm, Dynamic Sparse Learning (DSL), tailored for recommendation models. DSL innovatively trains a lightweight sparse model from scratch, periodi
&lt;/p&gt;</description></item><item><title>&#22312;&#26412;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#24320;&#25918;&#39046;&#22495;&#31185;&#23398;&#20027;&#24352;&#39564;&#35777;&#31995;&#32479;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#30340;&#30693;&#35782;&#26469;&#28304;&#65288;PubMed&#12289;&#32500;&#22522;&#30334;&#31185;&#12289;&#35895;&#27468;&#65289;&#21644;&#20449;&#24687;&#26816;&#32034;&#26041;&#27861;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#30495;&#23454;&#19990;&#30028;&#29615;&#22659;&#20013;&#36827;&#34892;&#31185;&#23398;&#20027;&#24352;&#39564;&#35777;&#30340;&#25361;&#25112;&#21644;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.02844</link><description>&lt;p&gt;
&#23545;&#24320;&#25918;&#39046;&#22495;&#31185;&#23398;&#20027;&#24352;&#39564;&#35777;&#30340;&#30693;&#35782;&#26469;&#28304;&#30340;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Comparing Knowledge Sources for Open-Domain Scientific Claim Verification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02844
&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#24320;&#25918;&#39046;&#22495;&#31185;&#23398;&#20027;&#24352;&#39564;&#35777;&#31995;&#32479;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#30340;&#30693;&#35782;&#26469;&#28304;&#65288;PubMed&#12289;&#32500;&#22522;&#30334;&#31185;&#12289;&#35895;&#27468;&#65289;&#21644;&#20449;&#24687;&#26816;&#32034;&#26041;&#27861;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#30495;&#23454;&#19990;&#30028;&#29615;&#22659;&#20013;&#36827;&#34892;&#31185;&#23398;&#20027;&#24352;&#39564;&#35777;&#30340;&#25361;&#25112;&#21644;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#30693;&#35782;&#30340;&#21457;&#29616;&#36895;&#24230;&#21644;&#22312;&#32447;&#20581;&#24247;&#20027;&#24352;&#30340;&#20998;&#20139;&#22686;&#21152;&#65292;&#20984;&#26174;&#20102;&#20026;&#31185;&#23398;&#20027;&#24352;&#24320;&#21457;&#39640;&#25928;&#20107;&#23454;&#26816;&#26680;&#31995;&#32479;&#30340;&#37325;&#35201;&#24615;&#12290;&#29616;&#26377;&#25991;&#29486;&#20013;&#65292;&#36825;&#39033;&#20219;&#21153;&#30340;&#24120;&#35265;&#35774;&#32622;&#20551;&#35774;&#24050;&#32463;&#25552;&#20379;&#24182;&#27880;&#37322;&#25110;&#21253;&#21547;&#22312;&#26377;&#38480;&#35821;&#26009;&#24211;&#20013;&#30340;&#21253;&#21547;&#35777;&#25454;&#30340;&#25991;&#26723;&#12290;&#36825;&#20351;&#24471;&#35813;&#31995;&#32479;&#22312;&#21487;&#33021;&#38656;&#35201;&#26597;&#35810;&#25968;&#30334;&#19975;&#20010;&#25991;&#26723;&#20197;&#25214;&#21040;&#30456;&#20851;&#35777;&#25454;&#30340;&#29616;&#23454;&#19990;&#30028;&#29615;&#22659;&#20013;&#21464;&#24471;&#19981;&#20999;&#23454;&#38469;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#20197;&#27979;&#35797;&#24320;&#25918;&#39046;&#22495;&#20027;&#24352;&#39564;&#35777;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#19981;&#21516;&#35774;&#32622;&#19979;&#27979;&#35797;&#20102;&#22235;&#20010;&#25968;&#25454;&#38598;&#20013;&#30340;&#29983;&#29289;&#21307;&#23398;&#21644;&#20581;&#24247;&#20027;&#24352;&#31995;&#32479;&#30340;&#26368;&#32456;&#21028;&#26029;&#39044;&#27979;&#12290;&#22312;&#20445;&#25345;&#27969;&#27700;&#32447;&#30340;&#35777;&#25454;&#36873;&#25321;&#21644;&#21028;&#26029;&#39044;&#27979;&#37096;&#20998;&#19981;&#21464;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#19977;&#31181;&#24120;&#35265;&#30693;&#35782;&#26469;&#28304;&#65288;PubMed&#12289;&#32500;&#22522;&#30334;&#31185;&#12289;&#35895;&#27468;&#65289;&#21644;&#20004;&#31181;&#19981;&#21516;&#30340;&#20449;&#24687;&#26816;&#32034;&#26041;&#27861;&#36827;&#34892;&#25991;&#26723;&#26816;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing rate at which scientific knowledge is discovered and health claims shared online has highlighted the importance of developing efficient fact-checking systems for scientific claims. The usual setting for this task in the literature assumes that the documents containing the evidence for claims are already provided and annotated or contained in a limited corpus. This renders the systems unrealistic for real-world settings where knowledge sources with potentially millions of documents need to be queried to find relevant evidence. In this paper, we perform an array of experiments to test the performance of open-domain claim verification systems. We test the final verdict prediction of systems on four datasets of biomedical and health claims in different settings. While keeping the pipeline's evidence selection and verdict prediction parts constant, document retrieval is performed over three common knowledge sources (PubMed, Wikipedia, Google) and using two different informati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#20852;&#36259;&#24314;&#27169;&#26694;&#26550;&#8220;Trinity&#8221;&#65292;&#36890;&#36807;&#21033;&#29992;&#38271;&#26399;&#32447;&#32034;&#26469;&#35299;&#20915;&#20852;&#36259;&#36951;&#24536;&#38382;&#39064;&#65292;&#24182;&#25913;&#21892;&#22810;&#20852;&#36259;&#24314;&#27169;&#20219;&#21153;&#12290;&#36890;&#36807;&#26500;&#24314;&#23454;&#26102;&#32858;&#31867;&#31995;&#32479;&#21644;&#35745;&#31639;&#32479;&#35745;&#20852;&#36259;&#30452;&#26041;&#22270;&#65292;Trinity&#33021;&#22815;&#35782;&#21035;&#29992;&#25143;&#30340;&#20852;&#36259;&#24182;&#36827;&#34892;&#20010;&#24615;&#21270;&#25512;&#33616;&#12290;</title><link>https://arxiv.org/abs/2402.02842</link><description>&lt;p&gt;
Trinity&#65306;&#23558;&#22810;/&#23567;&#20247;/&#38271;&#26399;&#20852;&#36259;&#25972;&#21512;&#20026;&#19968;&#20010;
&lt;/p&gt;
&lt;p&gt;
Trinity: Syncretizing Multi-/Long-tail/Long-term Interests All in One
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02842
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#20852;&#36259;&#24314;&#27169;&#26694;&#26550;&#8220;Trinity&#8221;&#65292;&#36890;&#36807;&#21033;&#29992;&#38271;&#26399;&#32447;&#32034;&#26469;&#35299;&#20915;&#20852;&#36259;&#36951;&#24536;&#38382;&#39064;&#65292;&#24182;&#25913;&#21892;&#22810;&#20852;&#36259;&#24314;&#27169;&#20219;&#21153;&#12290;&#36890;&#36807;&#26500;&#24314;&#23454;&#26102;&#32858;&#31867;&#31995;&#32479;&#21644;&#35745;&#31639;&#32479;&#35745;&#20852;&#36259;&#30452;&#26041;&#22270;&#65292;Trinity&#33021;&#22815;&#35782;&#21035;&#29992;&#25143;&#30340;&#20852;&#36259;&#24182;&#36827;&#34892;&#20010;&#24615;&#21270;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#20852;&#36259;&#24314;&#27169;&#19968;&#30452;&#26159;&#25913;&#21892;&#29992;&#25143;&#20307;&#39564;&#30340;&#19968;&#20010;&#37325;&#35201;&#39046;&#22495;&#65292;&#35768;&#22810;&#29616;&#26377;&#24037;&#20316;&#24050;&#32463;&#30740;&#31350;&#20102;&#20856;&#22411;&#30340;&#20852;&#36259;&#24314;&#27169;&#20219;&#21153;&#65288;&#20363;&#22914;&#22810;&#20852;&#36259;&#12289;&#23567;&#20247;&#20852;&#36259;&#21644;&#38271;&#26399;&#20852;&#36259;&#65289;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#21482;&#32771;&#34385;&#20102;&#20854;&#20013;&#19968;&#20010;&#20852;&#36259;&#65292;&#24182;&#24573;&#30053;&#20102;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#36825;&#20123;&#20219;&#21153;&#38754;&#20020;&#20849;&#21516;&#30340;&#8220;&#20852;&#36259;&#36951;&#24536;&#8221;&#38382;&#39064;&#65292;&#32780;&#19988;&#23384;&#22312;&#19968;&#31181;&#35299;&#20915;&#26041;&#27861;&#21487;&#20197;&#21516;&#26102;&#20943;&#36731;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#35748;&#20026;&#38271;&#26399;&#32447;&#32034;&#21487;&#20197;&#25104;&#20026;&#22522;&#30707;&#65292;&#22240;&#20026;&#23427;&#20204;&#25581;&#31034;&#20102;&#22810;&#31181;&#20852;&#36259;&#24182;&#28548;&#28165;&#20102;&#23567;&#20247;&#20852;&#36259;&#12290;&#21463;&#21040;&#36825;&#20010;&#35266;&#23519;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#22312;&#26816;&#32034;&#38454;&#27573;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32479;&#19968;&#26694;&#26550;&#8220;Trinity&#8221;&#65292;&#26469;&#35299;&#20915;&#20852;&#36259;&#36951;&#24536;&#38382;&#39064;&#24182;&#25913;&#21892;&#22810;&#20852;&#36259;&#24314;&#27169;&#20219;&#21153;&#12290;&#25105;&#20204;&#25645;&#24314;&#20102;&#19968;&#20010;&#23454;&#26102;&#32858;&#31867;&#31995;&#32479;&#65292;&#21487;&#20197;&#23558;&#29289;&#21697;&#25237;&#24433;&#21040;&#21487;&#26522;&#20030;&#30340;&#31751;&#20013;&#65292;&#24182;&#22312;&#36825;&#20123;&#31751;&#19978;&#35745;&#31639;&#32479;&#35745;&#20852;&#36259;&#30452;&#26041;&#22270;&#12290;&#22522;&#20110;&#36825;&#20123;&#30452;&#26041;&#22270;&#65292;Trinity&#21487;&#20197;&#35782;&#21035;&#29992;&#25143;&#30340;&#20852;&#36259;&#65292;&#24182;&#36827;&#34892;&#20010;&#24615;&#21270;&#30340;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interest modeling in recommender system has been a constant topic for improving user experience, and typical interest modeling tasks (e.g. multi-interest, long-tail interest and long-term interest) have been investigated in many existing works. However, most of them only consider one interest in isolation, while neglecting their interrelationships. In this paper, we argue that these tasks suffer from a common "interest amnesia" problem, and a solution exists to mitigate it simultaneously. We figure that long-term cues can be the cornerstone since they reveal multi-interest and clarify long-tail interest. Inspired by the observation, we propose a novel and unified framework in the retrieval stage, "Trinity", to solve interest amnesia problem and improve multiple interest modeling tasks. We construct a real-time clustering system that enables us to project items into enumerable clusters, and calculate statistical interest histograms over these clusters. Based on these histograms, Trinity
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#20132;&#21449;&#21452;&#36793;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20132;&#21449;&#21452;&#36793;&#20844;&#24179;&#25512;&#33616;&#65288;ITFR&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#38160;&#24230;&#24863;&#30693;&#25439;&#22833;&#24863;&#30693;&#21155;&#21183;&#32676;&#20307;&#65292;&#20351;&#29992;&#21327;&#20316;&#25439;&#22833;&#24179;&#34913;&#24320;&#21457;&#19981;&#21516;&#20132;&#21449;&#32676;&#20307;&#30340;&#19968;&#33268;&#21306;&#20998;&#33021;&#21147;&#65292;&#24182;&#21033;&#29992;&#39044;&#27979;&#24471;&#20998;&#24402;&#19968;&#21270;&#26469;&#20844;&#24179;&#23545;&#24453;&#19981;&#21516;&#20132;&#21449;&#32676;&#20307;&#20013;&#30340;&#27491;&#20363;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#25552;&#39640;&#20132;&#21449;&#21452;&#36793;&#20844;&#24179;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.02816</link><description>&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#20132;&#21449;&#21452;&#36793;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Intersectional Two-sided Fairness in Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02816
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#20132;&#21449;&#21452;&#36793;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20132;&#21449;&#21452;&#36793;&#20844;&#24179;&#25512;&#33616;&#65288;ITFR&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#38160;&#24230;&#24863;&#30693;&#25439;&#22833;&#24863;&#30693;&#21155;&#21183;&#32676;&#20307;&#65292;&#20351;&#29992;&#21327;&#20316;&#25439;&#22833;&#24179;&#34913;&#24320;&#21457;&#19981;&#21516;&#20132;&#21449;&#32676;&#20307;&#30340;&#19968;&#33268;&#21306;&#20998;&#33021;&#21147;&#65292;&#24182;&#21033;&#29992;&#39044;&#27979;&#24471;&#20998;&#24402;&#19968;&#21270;&#26469;&#20844;&#24179;&#23545;&#24453;&#19981;&#21516;&#20132;&#21449;&#32676;&#20307;&#20013;&#30340;&#27491;&#20363;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#25552;&#39640;&#20132;&#21449;&#21452;&#36793;&#20844;&#24179;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#25512;&#33616;&#31995;&#32479;&#30340;&#20844;&#24179;&#24615;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#26681;&#25454;&#28041;&#21450;&#30340;&#21033;&#30410;&#30456;&#20851;&#32773;&#65292;&#25512;&#33616;&#31995;&#32479;&#30340;&#20844;&#24179;&#24615;&#21487;&#20998;&#20026;&#29992;&#25143;&#20844;&#24179;&#24615;&#12289;&#29289;&#21697;&#20844;&#24179;&#24615;&#21644;&#21516;&#26102;&#32771;&#34385;&#29992;&#25143;&#21644;&#29289;&#21697;&#20844;&#24179;&#24615;&#30340;&#21452;&#36793;&#20844;&#24179;&#24615;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35748;&#20026;&#21363;&#20351;&#25512;&#33616;&#31995;&#32479;&#26159;&#21452;&#36793;&#20844;&#24179;&#30340;&#65292;&#20132;&#21449;&#21452;&#36793;&#19981;&#20844;&#24179;&#20173;&#28982;&#21487;&#33021;&#23384;&#22312;&#65292;&#36825;&#22312;&#26412;&#25991;&#20351;&#29992;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#23454;&#35777;&#30740;&#31350;&#20013;&#24471;&#21040;&#20102;&#35266;&#23519;&#21644;&#23637;&#31034;&#65292;&#24182;&#19988;&#20197;&#21069;&#23578;&#26410;&#24471;&#21040;&#24456;&#22909;&#30340;&#30740;&#31350;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#31216;&#20026;&#20132;&#21449;&#21452;&#36793;&#20844;&#24179;&#25512;&#33616;&#65288;ITFR&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#19968;&#20010;&#38160;&#24230;&#24863;&#30693;&#25439;&#22833;&#26469;&#24863;&#30693;&#21155;&#21183;&#32676;&#20307;&#65292;&#28982;&#21518;&#20351;&#29992;&#21327;&#20316;&#25439;&#22833;&#24179;&#34913;&#26469;&#24320;&#21457;&#19981;&#21516;&#20132;&#21449;&#32676;&#20307;&#30340;&#19968;&#33268;&#21306;&#20998;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#39044;&#27979;&#24471;&#20998;&#24402;&#19968;&#21270;&#26469;&#35843;&#25972;&#27491;&#38754;&#39044;&#27979;&#24471;&#20998;&#65292;&#20197;&#20844;&#24179;&#22320;&#23545;&#24453;&#19981;&#21516;&#20132;&#21449;&#32676;&#20307;&#20013;&#30340;&#27491;&#20363;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25552;&#39640;&#20132;&#21449;&#21452;&#36793;&#20844;&#24179;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fairness of recommender systems (RS) has attracted increasing attention recently. Based on the involved stakeholders, the fairness of RS can be divided into user fairness, item fairness, and two-sided fairness which considers both user and item fairness simultaneously. However, we argue that the intersectional two-sided unfairness may still exist even if the RS is two-sided fair, which is observed and shown by empirical studies on real-world data in this paper, and has not been well-studied previously. To mitigate this problem, we propose a novel approach called Intersectional Two-sided Fairness Recommendation (ITFR). Our method utilizes a sharpness-aware loss to perceive disadvantaged groups, and then uses collaborative loss balance to develop consistent distinguishing abilities for different intersectional groups. Additionally, predicted score normalization is leveraged to align positive predicted scores to fairly treat positives in different intersectional groups. Extensive experime
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25913;&#36827;&#33647;&#29289;&#25512;&#33616;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#20256;&#32479;&#27169;&#22411;&#22312;&#21307;&#23398;&#25968;&#25454;&#35821;&#20041;&#29702;&#35299;&#21644;&#26032;&#24739;&#32773;&#22788;&#26041;&#25512;&#33616;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.02803</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33976;&#39311;&#33647;&#29289;&#25512;&#33616;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Large Language Model Distilling Medication Recommendation Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02803
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25913;&#36827;&#33647;&#29289;&#25512;&#33616;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#20256;&#32479;&#27169;&#22411;&#22312;&#21307;&#23398;&#25968;&#25454;&#35821;&#20041;&#29702;&#35299;&#21644;&#26032;&#24739;&#32773;&#22788;&#26041;&#25512;&#33616;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33647;&#29289;&#25512;&#33616;&#26159;&#26234;&#33021;&#21307;&#30103;&#31995;&#32479;&#20013;&#30340;&#37325;&#35201;&#26041;&#38754;&#65292;&#23427;&#26681;&#25454;&#24739;&#32773;&#29305;&#23450;&#30340;&#20581;&#24247;&#38656;&#27714;&#26469;&#25512;&#33616;&#26368;&#21512;&#36866;&#30340;&#33647;&#29289;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#20351;&#29992;&#30340;&#35768;&#22810;&#22797;&#26434;&#27169;&#22411;&#24448;&#24448;&#24573;&#35270;&#21307;&#23398;&#25968;&#25454;&#30340;&#32454;&#24494;&#35821;&#20041;&#65292;&#32780;&#21482;&#26159;&#36807;&#24230;&#20381;&#36182;&#26631;&#35782;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#22788;&#29702;&#39318;&#27425;&#35775;&#38382;&#21307;&#38498;&#30340;&#24739;&#32773;&#30340;&#24773;&#20917;&#26102;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#20204;&#32570;&#20047;&#20043;&#21069;&#30340;&#22788;&#26041;&#21382;&#21490;&#21487;&#20197;&#21442;&#32771;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24378;&#22823;&#30340;&#35821;&#20041;&#29702;&#35299;&#21644;&#36755;&#20837;&#19981;&#21487;&#30693;&#30340;&#29305;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;LLMs&#25913;&#36827;&#29616;&#26377;&#30340;&#33647;&#29289;&#25512;&#33616;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33976;&#39311;&#33647;&#29289;&#25512;&#33616;&#65288;LEADER&#65289;&#12290;&#25105;&#20204;&#39318;&#20808;&#21019;&#24314;&#21512;&#36866;&#30340;&#25552;&#31034;&#27169;&#26495;&#65292;&#20351;LLMs&#33021;&#22815;&#26377;&#25928;&#22320;&#25512;&#33616;&#33647;&#29289;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recommendation of medication is a vital aspect of intelligent healthcare systems, as it involves prescribing the most suitable drugs based on a patient's specific health needs. Unfortunately, many sophisticated models currently in use tend to overlook the nuanced semantics of medical data, while only relying heavily on identities. Furthermore, these models face significant challenges in handling cases involving patients who are visiting the hospital for the first time, as they lack prior prescription histories to draw upon. To tackle these issues, we harness the powerful semantic comprehension and input-agnostic characteristics of Large Language Models (LLMs). Our research aims to transform existing medication recommendation methodologies using LLMs. In this paper, we introduce a novel approach called Large Language Model Distilling Medication Recommendation (LEADER). We begin by creating appropriate prompt templates that enable LLMs to suggest medications effectively. However, the
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21015;&#34920;&#24863;&#30693;&#30340;&#37325;&#26032;&#25490;&#24207;-&#25130;&#26029;&#32852;&#21512;&#27169;&#22411;&#65292;&#29992;&#20110;&#25628;&#32034;&#21644;&#29983;&#25104;&#22686;&#24378;&#26816;&#32034;&#12290;&#35813;&#27169;&#22411;&#26088;&#22312;&#25429;&#25417;&#21015;&#34920;&#32423;&#30340;&#19978;&#19979;&#25991;&#29305;&#24449;&#65292;&#36890;&#36807;&#37325;&#26032;&#25490;&#24207;&#21644;&#25130;&#26029;&#26469;&#36820;&#22238;&#26356;&#22909;&#30340;&#21015;&#34920;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#23558;&#37325;&#26032;&#25490;&#24207;&#21644;&#25130;&#26029;&#35270;&#20026;&#20004;&#20010;&#21333;&#29420;&#30340;&#20219;&#21153;&#65292;&#20294;&#36825;&#31181;&#20998;&#31163;&#19981;&#26159;&#26368;&#20248;&#30340;&#12290;&#22240;&#27492;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#30340;&#32852;&#21512;&#27169;&#22411;&#21487;&#20197;&#35299;&#20915;&#20449;&#24687;&#20849;&#20139;&#21644;&#38169;&#35823;&#31215;&#32047;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.02764</link><description>&lt;p&gt;
&#22522;&#20110;&#21015;&#34920;&#24863;&#30693;&#30340;&#37325;&#26032;&#25490;&#24207;-&#25130;&#26029;&#32852;&#21512;&#27169;&#22411;&#29992;&#20110;&#25628;&#32034;&#21644;&#29983;&#25104;&#22686;&#24378;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
List-aware Reranking-Truncation Joint Model for Search and Retrieval-augmented Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02764
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21015;&#34920;&#24863;&#30693;&#30340;&#37325;&#26032;&#25490;&#24207;-&#25130;&#26029;&#32852;&#21512;&#27169;&#22411;&#65292;&#29992;&#20110;&#25628;&#32034;&#21644;&#29983;&#25104;&#22686;&#24378;&#26816;&#32034;&#12290;&#35813;&#27169;&#22411;&#26088;&#22312;&#25429;&#25417;&#21015;&#34920;&#32423;&#30340;&#19978;&#19979;&#25991;&#29305;&#24449;&#65292;&#36890;&#36807;&#37325;&#26032;&#25490;&#24207;&#21644;&#25130;&#26029;&#26469;&#36820;&#22238;&#26356;&#22909;&#30340;&#21015;&#34920;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#23558;&#37325;&#26032;&#25490;&#24207;&#21644;&#25130;&#26029;&#35270;&#20026;&#20004;&#20010;&#21333;&#29420;&#30340;&#20219;&#21153;&#65292;&#20294;&#36825;&#31181;&#20998;&#31163;&#19981;&#26159;&#26368;&#20248;&#30340;&#12290;&#22240;&#27492;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#30340;&#32852;&#21512;&#27169;&#22411;&#21487;&#20197;&#35299;&#20915;&#20449;&#24687;&#20849;&#20139;&#21644;&#38169;&#35823;&#31215;&#32047;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#24687;&#26816;&#32034;&#30340;&#32467;&#26524;&#36890;&#24120;&#20197;&#25490;&#21517;&#21015;&#34920;&#30340;&#24418;&#24335;&#21576;&#29616;&#65292;&#20363;&#22914;&#38754;&#21521;&#20154;&#31867;&#30340;&#32593;&#32476;&#25628;&#32034;&#21644;&#38754;&#21521;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#12290;&#21015;&#34920;&#24863;&#30693;&#26816;&#32034;&#26088;&#22312;&#25429;&#25417;&#21015;&#34920;&#32423;&#30340;&#19978;&#19979;&#25991;&#29305;&#24449;&#65292;&#36820;&#22238;&#26356;&#22909;&#30340;&#21015;&#34920;&#65292;&#20027;&#35201;&#21253;&#25324;&#37325;&#26032;&#25490;&#24207;&#21644;&#25130;&#26029;&#12290;&#37325;&#26032;&#25490;&#24207;&#23545;&#21015;&#34920;&#20013;&#30340;&#25991;&#26723;&#36827;&#34892;&#31934;&#32454;&#37325;&#26032;&#35780;&#20998;&#12290;&#25130;&#26029;&#21160;&#24577;&#30830;&#23450;&#25490;&#21517;&#21015;&#34920;&#30340;&#25130;&#26029;&#28857;&#65292;&#20197;&#22312;&#25972;&#20307;&#30456;&#20851;&#24615;&#21644;&#36991;&#20813;&#26080;&#20851;&#25991;&#26723;&#30340;&#38169;&#35823;&#20449;&#24687;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#12290;&#36807;&#21435;&#30340;&#30740;&#31350;&#23558;&#23427;&#20204;&#35270;&#20026;&#20004;&#20010;&#21333;&#29420;&#30340;&#20219;&#21153;&#24182;&#20998;&#21035;&#23545;&#20854;&#36827;&#34892;&#24314;&#27169;&#65292;&#28982;&#32780;&#65292;&#36825;&#31181;&#20998;&#31163;&#26159;&#19981;&#29702;&#24819;&#30340;&#12290;&#39318;&#20808;&#65292;&#24456;&#38590;&#22312;&#20004;&#20010;&#20219;&#21153;&#20043;&#38388;&#20849;&#20139;&#25490;&#21517;&#21015;&#34920;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#20854;&#27425;&#65292;&#29420;&#31435;&#30340;&#27969;&#27700;&#32447;&#36890;&#24120;&#20250;&#36935;&#21040;&#38169;&#35823;&#31215;&#32047;&#38382;&#39064;&#65292;&#21363;&#37325;&#26032;&#25490;&#24207;&#38454;&#27573;&#30340;&#23567;&#38169;&#35823;&#21487;&#33021;&#20250;&#20005;&#37325;&#24433;&#21709;&#25130;&#26029;&#38454;&#27573;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;...&#65288;&#32487;&#32493;&#25551;&#36848;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#65289;
&lt;/p&gt;
&lt;p&gt;
The results of information retrieval (IR) are usually presented in the form of a ranked list of candidate documents, such as web search for humans and retrieval-augmented generation for large language models (LLMs). List-aware retrieval aims to capture the list-level contextual features to return a better list, mainly including reranking and truncation. Reranking finely re-scores the documents in the list. Truncation dynamically determines the cut-off point of the ranked list to achieve the trade-off between overall relevance and avoiding misinformation from irrelevant documents. Previous studies treat them as two separate tasks and model them separately. However, the separation is not optimal. First, it is hard to share the contextual information of the ranking list between the two tasks. Second, the separate pipeline usually meets the error accumulation problem, where the small error from the reranking stage can largely affect the truncation stage. To solve these problems, we propose
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#38477;&#22122;&#26102;&#38388;&#21608;&#26399;&#24314;&#27169;&#65288;DiCycle&#65289;&#65292;&#29992;&#20110;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#24314;&#27169;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#30340;&#26102;&#38388;&#27169;&#24335;&#12290;&#36890;&#36807;&#36873;&#25321;&#19982;&#30446;&#26631;&#29289;&#21697;&#39640;&#24230;&#30456;&#20851;&#30340;&#29992;&#25143;&#34892;&#20026;&#23376;&#38598;&#65292;DiCycle&#33021;&#22815;&#26356;&#22909;&#22320;&#24314;&#27169;&#26102;&#38388;&#21608;&#26399;&#27169;&#24335;&#65292;&#20174;&#32780;&#25552;&#39640;&#25512;&#33616;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.02718</link><description>&lt;p&gt;
&#38477;&#22122;&#26102;&#38388;&#21608;&#26399;&#24314;&#27169;&#29992;&#20110;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Denoising Time Cycle Modeling for Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02718
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#38477;&#22122;&#26102;&#38388;&#21608;&#26399;&#24314;&#27169;&#65288;DiCycle&#65289;&#65292;&#29992;&#20110;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#24314;&#27169;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#30340;&#26102;&#38388;&#27169;&#24335;&#12290;&#36890;&#36807;&#36873;&#25321;&#19982;&#30446;&#26631;&#29289;&#21697;&#39640;&#24230;&#30456;&#20851;&#30340;&#29992;&#25143;&#34892;&#20026;&#23376;&#38598;&#65292;DiCycle&#33021;&#22815;&#26356;&#22909;&#22320;&#24314;&#27169;&#26102;&#38388;&#21608;&#26399;&#27169;&#24335;&#65292;&#20174;&#32780;&#25552;&#39640;&#25512;&#33616;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#65292;&#24314;&#27169;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#30340;&#26102;&#38388;&#27169;&#24335;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#25105;&#20204;&#35748;&#20026;&#29616;&#26377;&#26041;&#27861;&#24573;&#30053;&#20102;&#29992;&#25143;&#34892;&#20026;&#30340;&#26102;&#38388;&#27169;&#24335;&#30340;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#23558;&#19982;&#30446;&#26631;&#29289;&#21697;&#26080;&#20851;&#30340;&#29992;&#25143;&#34892;&#20026;&#23450;&#20041;&#20026;&#22122;&#22768;&#65292;&#36825;&#38480;&#21046;&#20102;&#19982;&#30446;&#26631;&#30456;&#20851;&#30340;&#26102;&#38388;&#21608;&#26399;&#24314;&#27169;&#30340;&#24615;&#33021;&#65292;&#24182;&#24433;&#21709;&#20102;&#25512;&#33616;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#38477;&#22122;&#26102;&#38388;&#21608;&#26399;&#24314;&#27169;&#65288;DiCycle&#65289;&#65292;&#29992;&#20110;&#38477;&#22122;&#29992;&#25143;&#34892;&#20026;&#24182;&#36873;&#25321;&#19982;&#30446;&#26631;&#29289;&#21697;&#39640;&#24230;&#30456;&#20851;&#30340;&#29992;&#25143;&#34892;&#20026;&#23376;&#38598;&#12290;DiCycle&#33021;&#22815;&#26126;&#30830;&#22320;&#24314;&#27169;&#22810;&#26679;&#21270;&#30340;&#26102;&#38388;&#21608;&#26399;&#27169;&#24335;&#20197;&#36827;&#34892;&#25512;&#33616;&#12290;&#25105;&#20204;&#22312;&#20844;&#20849;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;DiCycle&#30456;&#27604;&#26368;&#20808;&#36827;&#30340;&#25512;&#33616;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, modeling temporal patterns of user-item interactions have attracted much attention in recommender systems. We argue that existing methods ignore the variety of temporal patterns of user behaviors. We define the subset of user behaviors that are irrelevant to the target item as noises, which limits the performance of target-related time cycle modeling and affect the recommendation performance. In this paper, we propose Denoising Time Cycle Modeling (DiCycle), a novel approach to denoise user behaviors and select the subset of user behaviors that are highly related to the target item. DiCycle is able to explicitly model diverse time cycle patterns for recommendation. Extensive experiments are conducted on both public benchmarks and a real-world dataset, demonstrating the superior performance of DiCycle over the state-of-the-art recommendation methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25628;&#32034;&#24341;&#25806;&#30340;&#21160;&#24577;&#25490;&#21517;&#31995;&#32479;&#30340;&#29305;&#24449;&#65292;&#35813;&#29305;&#24449;&#21487;&#20197;&#20934;&#30830;&#20272;&#35745;&#25991;&#26723;&#30456;&#20851;&#24615;&#65292;&#20294;&#26159;&#22312;&#23384;&#22312;&#20301;&#32622;&#20559;&#24046;&#30340;&#24773;&#20917;&#19979;&#20250;&#26377;&#39640;&#26041;&#24046;&#65292;&#24182;&#24378;&#35843;&#20102;&#20934;&#30830;&#20272;&#35745;&#20301;&#32622;&#20559;&#24046;&#30340;&#24517;&#35201;&#24615;&#65292;&#24314;&#35758;&#21516;&#26102;&#20351;&#29992;&#26377;&#20559;&#21644;&#26080;&#20559;&#30340;&#20301;&#32622;&#20559;&#24046;&#29305;&#24449;&#12290;</title><link>https://arxiv.org/abs/2402.02626</link><description>&lt;p&gt;
&#29305;&#24449;&#20013;&#30340;&#20301;&#32622;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Position bias in features
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02626
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25628;&#32034;&#24341;&#25806;&#30340;&#21160;&#24577;&#25490;&#21517;&#31995;&#32479;&#30340;&#29305;&#24449;&#65292;&#35813;&#29305;&#24449;&#21487;&#20197;&#20934;&#30830;&#20272;&#35745;&#25991;&#26723;&#30456;&#20851;&#24615;&#65292;&#20294;&#26159;&#22312;&#23384;&#22312;&#20301;&#32622;&#20559;&#24046;&#30340;&#24773;&#20917;&#19979;&#20250;&#26377;&#39640;&#26041;&#24046;&#65292;&#24182;&#24378;&#35843;&#20102;&#20934;&#30830;&#20272;&#35745;&#20301;&#32622;&#20559;&#24046;&#30340;&#24517;&#35201;&#24615;&#65292;&#24314;&#35758;&#21516;&#26102;&#20351;&#29992;&#26377;&#20559;&#21644;&#26080;&#20559;&#30340;&#20301;&#32622;&#20559;&#24046;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25628;&#32034;&#24341;&#25806;&#20013;&#24314;&#27169;&#25991;&#20214;&#30456;&#20851;&#24615;&#30340;&#30446;&#30340;&#26159;&#22312;&#21518;&#32493;&#25628;&#32034;&#20013;&#26356;&#22909;&#22320;&#25490;&#21517;&#12290;&#25991;&#26723;&#29305;&#23450;&#30340;&#21382;&#21490;&#28857;&#20987;&#29575;&#21487;&#20197;&#20316;&#20026;&#21160;&#24577;&#25490;&#21517;&#31995;&#32479;&#20013;&#30340;&#37325;&#35201;&#29305;&#24449;&#65292;&#38543;&#30528;&#25105;&#20204;&#31215;&#32047;&#26356;&#22810;&#26679;&#26412;&#65292;&#31995;&#32479;&#20250;&#36827;&#34892;&#26356;&#26032;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;&#20960;&#31181;&#36825;&#26679;&#30340;&#29305;&#24449;&#30340;&#23646;&#24615;&#65292;&#24182;&#22312;&#25511;&#21046;&#23454;&#39564;&#20013;&#23545;&#20854;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#23558;&#21453;&#21521;&#20542;&#21521;&#21152;&#26435;&#26041;&#27861;&#25193;&#23637;&#21040;&#25991;&#26723;&#19978;&#21487;&#20197;&#20135;&#29983;&#23545;&#25991;&#26723;&#30456;&#20851;&#24615;&#30340;&#26080;&#20559;&#20272;&#35745;&#12290;&#36825;&#20010;&#29305;&#24449;&#21487;&#20197;&#20934;&#30830;&#22320;&#36817;&#20284;&#30456;&#20851;&#24615;&#65292;&#22312;&#29702;&#24819;&#24773;&#20917;&#19979;&#21487;&#20197;&#23454;&#29616;&#25509;&#36817;&#26368;&#20248;&#30340;&#25490;&#21517;&#12290;&#28982;&#32780;&#65292;&#23427;&#20855;&#26377;&#39640;&#26041;&#24046;&#65292;&#19988;&#38543;&#30528;&#20301;&#32622;&#20559;&#24046;&#30340;&#31243;&#24230;&#22686;&#21152;&#32780;&#22686;&#21152;&#12290;&#27492;&#22806;&#65292;&#19981;&#20934;&#30830;&#30340;&#20301;&#32622;&#20559;&#24046;&#20272;&#35745;&#20250;&#23548;&#33268;&#24615;&#33021;&#19981;&#20339;&#12290;&#22312;&#20960;&#31181;&#24773;&#20917;&#19979;&#65292;&#36825;&#20010;&#29305;&#24449;&#30340;&#34920;&#29616;&#21487;&#33021;&#19981;&#22914;&#20559;&#20506;&#30340;&#28857;&#20987;&#29575;&#12290;&#26412;&#25991;&#24378;&#35843;&#20102;&#20934;&#30830;&#30340;&#20301;&#32622;&#20559;&#24046;&#20272;&#35745;&#30340;&#24517;&#35201;&#24615;&#65292;&#24182;&#29420;&#29305;&#22320;&#24314;&#35758;&#21516;&#26102;&#20351;&#29992;&#26377;&#20559;&#21644;&#26080;&#20559;&#30340;&#20301;&#32622;&#20559;&#24046;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
The purpose of modeling document relevance for search engines is to rank better in subsequent searches. Document-specific historical click-through rates can be important features in a dynamic ranking system which updates as we accumulate more sample. This paper describes the properties of several such features, and tests them in controlled experiments. Extending the inverse propensity weighting method to documents creates an unbiased estimate of document relevance. This feature can approximate relevance accurately, leading to near-optimal ranking in ideal circumstances. However, it has high variance that is increasing with respect to the degree of position bias. Furthermore, inaccurate position bias estimation leads to poor performance. Under several scenarios this feature can perform worse than biased click-through rates. This paper underscores the need for accurate position bias estimation, and is unique in suggesting simultaneous use of biased and unbiased position bias features.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#19981;&#30830;&#23450;&#24615;&#26657;&#20934;&#21644;&#21487;&#35299;&#37322;&#24615;&#25972;&#21512;&#21040;&#19968;&#20010;&#26816;&#32034;&#27969;&#31243;&#20013;&#65292;&#36890;&#36807;&#24341;&#20837;&#36125;&#21494;&#26031;&#26041;&#27861;&#21644;&#22810;&#36879;&#35270;&#26816;&#32034;&#26469;&#26657;&#20934;&#19981;&#30830;&#23450;&#24615;&#12290;&#20351;&#29992;&#35299;&#37322;&#26041;&#27861;&#20998;&#26512;&#40657;&#30418;&#27169;&#22411;&#34892;&#20026;&#65292;&#24182;&#23558;&#37325;&#35201;&#24615;&#20998;&#25968;&#20316;&#20026;&#34917;&#20805;&#30456;&#20851;&#24615;&#20998;&#25968;&#65292;&#25552;&#21319;&#22522;&#30784;&#27169;&#22411;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#38382;&#31572;&#21644;&#20107;&#23454;&#26816;&#39564;&#20219;&#21153;&#19978;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.02418</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#36125;&#21494;&#26031;&#22810;&#36879;&#35270;&#29983;&#25104;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
eXplainable Bayesian Multi-Perspective Generative Retrieval
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02418
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#19981;&#30830;&#23450;&#24615;&#26657;&#20934;&#21644;&#21487;&#35299;&#37322;&#24615;&#25972;&#21512;&#21040;&#19968;&#20010;&#26816;&#32034;&#27969;&#31243;&#20013;&#65292;&#36890;&#36807;&#24341;&#20837;&#36125;&#21494;&#26031;&#26041;&#27861;&#21644;&#22810;&#36879;&#35270;&#26816;&#32034;&#26469;&#26657;&#20934;&#19981;&#30830;&#23450;&#24615;&#12290;&#20351;&#29992;&#35299;&#37322;&#26041;&#27861;&#20998;&#26512;&#40657;&#30418;&#27169;&#22411;&#34892;&#20026;&#65292;&#24182;&#23558;&#37325;&#35201;&#24615;&#20998;&#25968;&#20316;&#20026;&#34917;&#20805;&#30456;&#20851;&#24615;&#20998;&#25968;&#65292;&#25552;&#21319;&#22522;&#30784;&#27169;&#22411;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#38382;&#31572;&#21644;&#20107;&#23454;&#26816;&#39564;&#20219;&#21153;&#19978;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#30830;&#23450;&#24615;&#26816;&#32034;&#27969;&#27700;&#32447;&#27880;&#37325;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20294;&#22312;&#20915;&#31574;&#36807;&#31243;&#20013;&#36890;&#24120;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#35780;&#20272;&#19981;&#30830;&#23450;&#24615;&#26102;&#38754;&#20020;&#25361;&#25112;&#65292;&#23548;&#33268;&#36807;&#20110;&#33258;&#20449;&#30340;&#39044;&#27979;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#23558;&#19981;&#30830;&#23450;&#24615;&#26657;&#20934;&#21644;&#21487;&#35299;&#37322;&#24615;&#25972;&#21512;&#21040;&#26816;&#32034;&#27969;&#31243;&#20013;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#36125;&#21494;&#26031;&#26041;&#27861;&#21644;&#22810;&#36879;&#35270;&#26816;&#32034;&#26469;&#26657;&#20934;&#26816;&#32034;&#27969;&#31243;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#32467;&#21512;LIME&#21644;SHAP&#31561;&#25216;&#26415;&#26469;&#20998;&#26512;&#40657;&#30418;&#37325;&#25490;&#24207;&#27169;&#22411;&#30340;&#34892;&#20026;&#12290;&#20174;&#36825;&#20123;&#35299;&#37322;&#26041;&#27861;&#20013;&#24471;&#20986;&#30340;&#37325;&#35201;&#24615;&#20998;&#25968;&#20316;&#20026;&#34917;&#20805;&#30456;&#20851;&#24615;&#20998;&#25968;&#65292;&#20197;&#22686;&#24378;&#22522;&#30784;&#37325;&#25490;&#24207;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#38382;&#31572;&#21644;&#20107;&#23454;&#26816;&#39564;&#20219;&#21153;&#19978;&#35780;&#20272;&#36890;&#36807;&#19981;&#30830;&#23450;&#24615;&#26657;&#20934;&#21644;&#21487;&#35299;&#37322;&#24615;&#37325;&#25490;&#23454;&#29616;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19977;&#20010;KILT&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20986;&#20102;&#26126;&#26174;&#30340;&#24615;&#33021;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern deterministic retrieval pipelines prioritize achieving state-of-the-art performance but often lack interpretability in decision-making. These models face challenges in assessing uncertainty, leading to overconfident predictions. To overcome these limitations, we integrate uncertainty calibration and interpretability into a retrieval pipeline. Specifically, we introduce Bayesian methodologies and multi-perspective retrieval to calibrate uncertainty within a retrieval pipeline. We incorporate techniques such as LIME and SHAP to analyze the behavior of a black-box reranker model. The importance scores derived from these explanation methodologies serve as supplementary relevance scores to enhance the base reranker model. We evaluate the resulting performance enhancements achieved through uncertainty calibration and interpretable reranking on Question Answering and Fact Checking tasks. Our methods demonstrate substantial performance improvements across three KILT datasets.
&lt;/p&gt;</description></item><item><title>ExTTNet&#26159;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#65292;&#33021;&#22815;&#33258;&#21160;&#20174;&#21457;&#31080;&#22270;&#20687;&#20013;&#25552;&#21462;&#34920;&#26684;&#25991;&#23383;&#12290;&#20351;&#29992;&#20809;&#23398;&#23383;&#31526;&#35782;&#21035;&#65288;OCR&#65289;&#25216;&#26415;&#33719;&#21462;&#25991;&#26412;&#65292;&#24182;&#36890;&#36807;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#25552;&#39640;&#20934;&#30830;&#24230;&#12290;&#36890;&#36807;&#22810;&#23618;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#26368;&#32456;&#33719;&#24471;&#20102;0.92&#30340;F1&#20998;&#25968;&#12290;</title><link>https://arxiv.org/abs/2402.02246</link><description>&lt;p&gt;
ExTTNet:&#19968;&#31181;&#29992;&#20110;&#20174;&#21457;&#31080;&#22270;&#20687;&#20013;&#25552;&#21462;&#34920;&#26684;&#25991;&#23383;&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
ExTTNet: A Deep Learning Algorithm for Extracting Table Texts from Invoice Images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02246
&lt;/p&gt;
&lt;p&gt;
ExTTNet&#26159;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#65292;&#33021;&#22815;&#33258;&#21160;&#20174;&#21457;&#31080;&#22270;&#20687;&#20013;&#25552;&#21462;&#34920;&#26684;&#25991;&#23383;&#12290;&#20351;&#29992;&#20809;&#23398;&#23383;&#31526;&#35782;&#21035;&#65288;OCR&#65289;&#25216;&#26415;&#33719;&#21462;&#25991;&#26412;&#65292;&#24182;&#36890;&#36807;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#25552;&#39640;&#20934;&#30830;&#24230;&#12290;&#36890;&#36807;&#22810;&#23618;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#26368;&#32456;&#33719;&#24471;&#20102;0.92&#30340;F1&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#36890;&#36807;&#19968;&#20010;&#34987;&#31216;&#20026;ExTTNet&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#33258;&#21160;&#22320;&#20174;&#21457;&#31080;&#22270;&#20687;&#20013;&#25552;&#21462;&#20135;&#21697;&#34920;&#26684;&#12290;&#39318;&#20808;&#20351;&#29992;&#20809;&#23398;&#23383;&#31526;&#35782;&#21035;&#65288;OCR&#65289;&#25216;&#26415;&#20174;&#21457;&#31080;&#22270;&#20687;&#20013;&#33719;&#21462;&#25991;&#26412;&#12290;&#22312;&#27492;&#36807;&#31243;&#20013;&#65292;&#20351;&#29992;&#20102;Tesseract OCR&#24341;&#25806; [37]&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#20351;&#29992;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#22686;&#21152;&#29616;&#26377;&#29305;&#24449;&#30340;&#25968;&#37327;&#26469;&#25552;&#39640;&#20934;&#30830;&#24230;&#12290;&#26681;&#25454;&#27599;&#20010;OCR&#33719;&#24471;&#30340;&#25991;&#26412;&#26159;&#21542;&#26159;&#34920;&#26684;&#20803;&#32032;&#65292;&#36827;&#34892;&#26631;&#35760;&#22788;&#29702;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#20351;&#29992;&#20102;&#22810;&#23618;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#12290;&#20351;&#29992;Nvidia RTX 3090&#26174;&#21345;&#36827;&#34892;&#35757;&#32451;&#65292;&#32791;&#26102;162&#20998;&#38047;&#12290;&#35757;&#32451;&#30340;&#32467;&#26524;&#65292;F1&#20998;&#25968;&#20026;0.92&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, product tables in invoices are obtained autonomously via a deep learning model, which is named as ExTTNet. Firstly, text is obtained from invoice images using Optical Character Recognition (OCR) techniques. Tesseract OCR engine [37] is used for this process. Afterwards, the number of existing features is increased by using feature extraction methods to increase the accuracy. Labeling process is done according to whether each text obtained as a result of OCR is a table element or not. In this study, a multilayer artificial neural network model is used. The training has been carried out with an Nvidia RTX 3090 graphics card and taken $162$ minutes. As a result of the training, the F1 score is $0.92$.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#36328;&#22495;&#25512;&#33616;&#20013;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#20174;&#36741;&#21161;&#39046;&#22495;&#20013;&#28155;&#21152;&#25968;&#25454;&#26469;&#35299;&#20915;&#20919;&#21551;&#21160;&#29992;&#25143;&#25968;&#25454;&#31232;&#30095;&#38382;&#39064;&#12290;&#20854;&#20013;&#65292;&#26144;&#23556;&#27169;&#22359;&#36215;&#30528;&#20851;&#38190;&#30340;&#20316;&#29992;&#65292;&#30830;&#23450;&#20102;&#36328;&#22495;&#25512;&#33616;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.02182</link><description>&lt;p&gt;
&#36328;&#22495;&#25512;&#33616;&#20013;&#30340;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Diffusion Cross-domain Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02182
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#36328;&#22495;&#25512;&#33616;&#20013;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#20174;&#36741;&#21161;&#39046;&#22495;&#20013;&#28155;&#21152;&#25968;&#25454;&#26469;&#35299;&#20915;&#20919;&#21551;&#21160;&#29992;&#25143;&#25968;&#25454;&#31232;&#30095;&#38382;&#39064;&#12290;&#20854;&#20013;&#65292;&#26144;&#23556;&#27169;&#22359;&#36215;&#30528;&#20851;&#38190;&#30340;&#20316;&#29992;&#65292;&#30830;&#23450;&#20102;&#36328;&#22495;&#25512;&#33616;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#25512;&#33616;&#31995;&#32479;&#26469;&#35828;&#65292;&#32473;&#19982;&#20919;&#21551;&#21160;&#29992;&#25143;&#39640;&#36136;&#37327;&#30340;&#25512;&#33616;&#32467;&#26524;&#19968;&#30452;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#35299;&#20915;&#30446;&#26631;&#39046;&#22495;&#20013;&#20919;&#21551;&#21160;&#29992;&#25143;&#25968;&#25454;&#31232;&#30095;&#38382;&#39064;&#30340;&#19968;&#20010;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#26159;&#20174;&#36741;&#21161;&#39046;&#22495;&#28155;&#21152;&#25968;&#25454;&#12290;&#25214;&#21040;&#19968;&#31181;&#21512;&#36866;&#30340;&#26041;&#27861;&#20174;&#36741;&#21161;&#39046;&#22495;&#25552;&#21462;&#30693;&#35782;&#24182;&#23558;&#20854;&#36716;&#31227;&#21040;&#30446;&#26631;&#39046;&#22495;&#26159;&#36328;&#22495;&#25512;&#33616;&#65288;CDR&#65289;&#30740;&#31350;&#30340;&#20027;&#35201;&#30446;&#26631;&#20043;&#19968;&#12290;&#22312;&#29616;&#26377;&#30340;&#26041;&#27861;&#20013;&#65292;&#26144;&#23556;&#26041;&#27861;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#36328;&#22495;&#25512;&#33616;&#27169;&#22411;&#65288;CDRs&#65289;&#12290;&#23545;&#20110;&#36825;&#31181;&#31867;&#22411;&#30340;&#27169;&#22411;&#65292;&#26144;&#23556;&#27169;&#22359;&#36215;&#30528;&#23558;&#25968;&#25454;&#20174;&#19968;&#20010;&#22495;&#36716;&#25442;&#21040;&#21478;&#19968;&#20010;&#22495;&#30340;&#20316;&#29992;&#12290;&#23427;&#20027;&#35201;&#20915;&#23450;&#20102;&#26144;&#23556;&#26041;&#27861;CDRs&#30340;&#24615;&#33021;&#12290;&#26368;&#36817;&#65292;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DPMs&#65289;&#22312;&#22270;&#20687;&#21512;&#25104;&#30456;&#20851;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25104;&#21151;&#12290;&#23427;&#20204;&#28041;&#21450;&#20174;&#28155;&#21152;&#22122;&#22768;&#30340;&#26679;&#26412;&#20013;&#24674;&#22797;&#22270;&#20687;&#65292;&#36825;&#21487;&#20197;&#30475;&#20316;&#26159;&#20855;&#26377;&#20986;&#33394;&#24615;&#33021;&#30340;&#25968;&#25454;&#36716;&#25442;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is always a challenge for recommender systems to give high-quality outcomes to cold-start users. One potential solution to alleviate the data sparsity problem for cold-start users in the target domain is to add data from the auxiliary domain. Finding a proper way to extract knowledge from an auxiliary domain and transfer it into a target domain is one of the main objectives for cross-domain recommendation (CDR) research. Among the existing methods, mapping approach is a popular one to implement cross-domain recommendation models (CDRs). For models of this type, a mapping module plays the role of transforming data from one domain to another. It primarily determines the performance of mapping approach CDRs. Recently, diffusion probability models (DPMs) have achieved impressive success for image synthesis related tasks. They involve recovering images from noise-added samples, which can be viewed as a data transformation process with outstanding performance. To further enhance the perfo
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EPR&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32034;&#24341;&#21407;&#23376;&#37051;&#25509;&#27169;&#24335;&#24182;&#23558;&#20854;&#32452;&#21512;&#65292;&#26126;&#30830;&#24314;&#27169;&#32467;&#26500;&#20381;&#36182;&#65292;&#24182;&#22312;&#30693;&#35782;&#22270;&#35889;&#20013;&#36827;&#34892;&#23376;&#22270;&#25552;&#21462;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;EPR&#26041;&#27861;&#22312;&#22797;&#26434;&#38382;&#39064;&#22238;&#31572;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#25552;&#39640;&#20102;F1&#24471;&#20998;&#65292;&#24182;&#22312;WebQuestionsSP&#19978;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.02175</link><description>&lt;p&gt;
&#36890;&#36807;&#35777;&#25454;&#27169;&#24335;&#26816;&#32034;&#22686;&#24378;&#30693;&#35782;&#22270;&#35889;&#19978;&#30340;&#22797;&#26434;&#38382;&#39064;&#22238;&#31572;
&lt;/p&gt;
&lt;p&gt;
Enhancing Complex Question Answering over Knowledge Graphs through Evidence Pattern Retrieval
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02175
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EPR&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32034;&#24341;&#21407;&#23376;&#37051;&#25509;&#27169;&#24335;&#24182;&#23558;&#20854;&#32452;&#21512;&#65292;&#26126;&#30830;&#24314;&#27169;&#32467;&#26500;&#20381;&#36182;&#65292;&#24182;&#22312;&#30693;&#35782;&#22270;&#35889;&#20013;&#36827;&#34892;&#23376;&#22270;&#25552;&#21462;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;EPR&#26041;&#27861;&#22312;&#22797;&#26434;&#38382;&#39064;&#22238;&#31572;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#25552;&#39640;&#20102;F1&#24471;&#20998;&#65292;&#24182;&#22312;WebQuestionsSP&#19978;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#38382;&#31572;&#31995;&#32479;&#30340;&#20449;&#24687;&#26816;&#32034;&#26041;&#27861;&#21253;&#25324;&#20004;&#20010;&#38454;&#27573;&#65306;&#23376;&#22270;&#25552;&#21462;&#21644;&#31572;&#26696;&#25512;&#29702;&#12290;&#25105;&#20204;&#35748;&#20026;&#24403;&#21069;&#30340;&#23376;&#22270;&#25552;&#21462;&#26041;&#27861;&#20302;&#20272;&#20102;&#35777;&#25454;&#20107;&#23454;&#20043;&#38388;&#30340;&#32467;&#26500;&#20381;&#36182;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#35777;&#25454;&#27169;&#24335;&#26816;&#32034;&#65288;EPR&#65289;&#26469;&#22312;&#23376;&#22270;&#25552;&#21462;&#36807;&#31243;&#20013;&#26126;&#30830;&#22320;&#24314;&#27169;&#32467;&#26500;&#20381;&#36182;&#12290;&#25105;&#20204;&#36890;&#36807;&#32034;&#24341;&#36164;&#28304;&#23545;&#30340;&#21407;&#23376;&#37051;&#25509;&#27169;&#24335;&#26469;&#23454;&#29616;EPR&#12290;&#32473;&#23450;&#19968;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#36827;&#34892;&#23494;&#38598;&#26816;&#32034;&#20197;&#33719;&#21462;&#30001;&#36164;&#28304;&#23545;&#24418;&#25104;&#30340;&#21407;&#23376;&#27169;&#24335;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#26522;&#20030;&#23427;&#20204;&#30340;&#32452;&#21512;&#20197;&#26500;&#24314;&#20505;&#36873;&#35777;&#25454;&#27169;&#24335;&#12290;&#36825;&#20123;&#35777;&#25454;&#27169;&#24335;&#20351;&#29992;&#31070;&#32463;&#27169;&#22411;&#36827;&#34892;&#25171;&#20998;&#65292;&#24182;&#36873;&#25321;&#26368;&#20339;&#27169;&#24335;&#26469;&#25552;&#21462;&#19979;&#28216;&#31572;&#26696;&#25512;&#29702;&#25152;&#38656;&#30340;&#23376;&#22270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;EPR&#30340;&#26041;&#27861;&#22312;ComplexWebQuestions&#19978;&#23558;IR-KGQA&#26041;&#27861;&#30340;F1&#24471;&#20998;&#26174;&#33879;&#25552;&#39640;&#20102;10&#20010;&#30334;&#20998;&#28857;&#20197;&#19978;&#65292;&#24182;&#22312;WebQuestionsSP&#19978;&#21462;&#24471;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Information retrieval (IR) methods for KGQA consist of two stages: subgraph extraction and answer reasoning. We argue current subgraph extraction methods underestimate the importance of structural dependencies among evidence facts. We propose Evidence Pattern Retrieval (EPR) to explicitly model the structural dependencies during subgraph extraction. We implement EPR by indexing the atomic adjacency pattern of resource pairs. Given a question, we perform dense retrieval to obtain atomic patterns formed by resource pairs. We then enumerate their combinations to construct candidate evidence patterns. These evidence patterns are scored using a neural model, and the best one is selected to extract a subgraph for downstream answer reasoning. Experimental results demonstrate that the EPR-based approach has significantly improved the F1 scores of IR-KGQA methods by over 10 points on ComplexWebQuestions and achieves competitive performance on WebQuestionsSP.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#21512;&#35821;&#20041;-&#25299;&#25169;&#36827;&#21270;&#22270;&#23398;&#20064;&#26041;&#27861;&#65288;PatSTEG&#65289;&#65292;&#29992;&#20110;&#24314;&#27169;&#19987;&#21033;&#24341;&#29992;&#32593;&#32476;&#30340;&#24418;&#25104;&#21160;&#24577;&#12290;&#36890;&#36807;&#32771;&#34385;&#35821;&#20041;&#21644;&#25299;&#25169;&#20449;&#24687;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#25506;&#32034;&#19987;&#21033;&#25968;&#25454;&#24211;&#20013;&#22797;&#26434;&#30340;&#25991;&#26412;&#21644;&#24341;&#29992;&#32593;&#32476;&#32467;&#26500;&#65292;&#24182;&#20026;&#19987;&#21033;&#25968;&#25454;&#25366;&#25496;&#25552;&#20379;&#26032;&#30340;&#26426;&#20250;&#12290;</title><link>https://arxiv.org/abs/2402.02158</link><description>&lt;p&gt;
PatSTEG&#65306;&#36890;&#36807;&#35821;&#20041;-&#25299;&#25169;&#36827;&#21270;&#22270;&#23545;&#19987;&#21033;&#24341;&#29992;&#32593;&#32476;&#30340;&#24418;&#25104;&#21160;&#24577;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
PatSTEG: Modeling Formation Dynamics of Patent Citation Networks via The Semantic-Topological Evolutionary Graph
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02158
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#21512;&#35821;&#20041;-&#25299;&#25169;&#36827;&#21270;&#22270;&#23398;&#20064;&#26041;&#27861;&#65288;PatSTEG&#65289;&#65292;&#29992;&#20110;&#24314;&#27169;&#19987;&#21033;&#24341;&#29992;&#32593;&#32476;&#30340;&#24418;&#25104;&#21160;&#24577;&#12290;&#36890;&#36807;&#32771;&#34385;&#35821;&#20041;&#21644;&#25299;&#25169;&#20449;&#24687;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#25506;&#32034;&#19987;&#21033;&#25968;&#25454;&#24211;&#20013;&#22797;&#26434;&#30340;&#25991;&#26412;&#21644;&#24341;&#29992;&#32593;&#32476;&#32467;&#26500;&#65292;&#24182;&#20026;&#19987;&#21033;&#25968;&#25454;&#25366;&#25496;&#25552;&#20379;&#26032;&#30340;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19987;&#21033;&#25968;&#25454;&#24211;&#20013;&#30340;&#19987;&#21033;&#25991;&#20214;&#23545;&#20110;&#30740;&#31350;&#12289;&#24320;&#21457;&#21644;&#21019;&#26032;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#20204;&#21253;&#21547;&#20102;&#23453;&#36149;&#30340;&#25216;&#26415;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#19982;&#20844;&#24320;&#21487;&#29992;&#30340;&#39044;&#22788;&#29702;&#25968;&#25454;&#24211;&#30456;&#27604;&#65292;&#19987;&#21033;&#25968;&#25454;&#24211;&#23384;&#22312;&#22810;&#26041;&#38754;&#30340;&#25361;&#25112;&#65292;&#21407;&#22240;&#26159;&#19987;&#21033;&#25991;&#26412;&#30340;&#22797;&#26434;&#24615;&#20197;&#21450;&#19987;&#21033;&#24341;&#29992;&#32593;&#32476;&#20869;&#22312;&#30340;&#31232;&#30095;&#24615;&#12290;&#23613;&#31649;&#19987;&#21033;&#25991;&#26412;&#20998;&#26512;&#21644;&#24341;&#25991;&#20998;&#26512;&#20026;&#25506;&#32034;&#19987;&#21033;&#25968;&#25454;&#25366;&#25496;&#24102;&#26469;&#20102;&#26032;&#30340;&#26426;&#20250;&#65292;&#20294;&#30446;&#21069;&#19981;&#23384;&#22312;&#21033;&#29992;&#23427;&#20204;&#30340;&#20114;&#34917;&#24615;&#30340;&#30740;&#31350;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#21512;&#35821;&#20041;-&#25299;&#25169;&#36827;&#21270;&#22270;&#23398;&#20064;&#26041;&#27861;&#65288;PatSTEG&#65289;&#26469;&#24314;&#27169;&#19987;&#21033;&#24341;&#29992;&#32593;&#32476;&#30340;&#24418;&#25104;&#21160;&#24577;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#21019;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;CNPat&#30340;&#20013;&#22269;&#19987;&#21033;&#30340;&#23454;&#38469;&#25968;&#25454;&#38598;&#65292;&#24182;&#21033;&#29992;&#20854;&#19987;&#21033;&#25991;&#26412;&#21644;&#24341;&#25991;&#26500;&#24314;&#20102;&#19968;&#20010;&#19987;&#21033;&#24341;&#29992;&#32593;&#32476;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#32771;&#34385;&#35821;&#20041;&#21644;&#25299;&#25169;&#20449;&#24687;&#65292;&#24314;&#31435;&#20102;PatSTEG&#26469;&#30740;&#31350;&#19987;&#21033;&#24341;&#29992;&#24418;&#25104;&#30340;&#36827;&#21270;&#21160;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
Patent documents in the patent database (PatDB) are crucial for research, development, and innovation as they contain valuable technical information. However, PatDB presents a multifaceted challenge compared to publicly available preprocessed databases due to the intricate nature of the patent text and the inherent sparsity within the patent citation network. Although patent text analysis and citation analysis bring new opportunities to explore patent data mining, no existing work exploits the complementation of them. To this end, we propose a joint semantic-topological evolutionary graph learning approach (PatSTEG) to model the formation dynamics of patent citation networks. More specifically, we first create a real-world dataset of Chinese patents named CNPat and leverage its patent texts and citations to construct a patent citation network. Then, PatSTEG is modeled to study the evolutionary dynamics of patent citation formation by considering the semantic and topological information
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#36136;&#30097;&#20102;&#25512;&#33616;&#31995;&#32479;&#23454;&#36341;&#20013;&#30446;&#21069;&#24120;&#29992;&#30340;&#8220;&#25720;&#30528;&#30707;&#22836;&#36807;&#27827;&#8221;&#26041;&#27861;&#65292;&#21628;&#21505;&#25682;&#24323;&#21453;&#20044;&#25176;&#37030;&#24605;&#32500;&#12290;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#22534;&#26632;&#30340;&#38750;&#26631;&#20934;&#29992;&#27861;&#65292;&#20197;&#35299;&#38145;&#22870;&#21169;&#20248;&#21270;&#30340;&#25512;&#33616;&#31995;&#32479;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.02152</link><description>&lt;p&gt;
&#35770;&#25991;&#39064;&#30446;&#65306;&#20026;&#20160;&#20040;&#8220;&#25720;&#30528;&#30707;&#22836;&#36807;&#27827;&#8221;&#26041;&#27861;&#20027;&#23548;&#25512;&#33616;&#31995;&#32479;&#23454;&#36341;&#65307;&#21628;&#21505;&#25682;&#24323;&#21453;&#20044;&#25176;&#37030;&#24605;&#32500;
&lt;/p&gt;
&lt;p&gt;
Position Paper: Why the Shooting in the Dark Method Dominates Recommender Systems Practice; A Call to Abandon Anti-Utopian Thinking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02152
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#36136;&#30097;&#20102;&#25512;&#33616;&#31995;&#32479;&#23454;&#36341;&#20013;&#30446;&#21069;&#24120;&#29992;&#30340;&#8220;&#25720;&#30528;&#30707;&#22836;&#36807;&#27827;&#8221;&#26041;&#27861;&#65292;&#21628;&#21505;&#25682;&#24323;&#21453;&#20044;&#25176;&#37030;&#24605;&#32500;&#12290;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#22534;&#26632;&#30340;&#38750;&#26631;&#20934;&#29992;&#27861;&#65292;&#20197;&#35299;&#38145;&#22870;&#21169;&#20248;&#21270;&#30340;&#25512;&#33616;&#31995;&#32479;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24212;&#29992;&#25512;&#33616;&#31995;&#32479;&#30740;&#31350;&#22788;&#20110;&#19968;&#31181;&#22855;&#29305;&#30340;&#22659;&#22320;&#12290;&#23613;&#31649;&#22312;&#36890;&#36807;A/B&#27979;&#35797;&#26469;&#34913;&#37327;&#24615;&#33021;&#26041;&#38754;&#26377;&#19968;&#20010;&#38750;&#24120;&#20005;&#26684;&#30340;&#21327;&#35758;&#65292;&#20294;&#25214;&#21040;&#35201;&#27979;&#35797;&#30340;&#8220;B&#8221;&#30340;&#26368;&#20339;&#26041;&#27861;&#24182;&#27809;&#26377;&#26126;&#30830;&#22320;&#38024;&#23545;&#24615;&#33021;&#65292;&#32780;&#26159;&#38024;&#23545;&#19968;&#20010;&#20195;&#29702;&#25351;&#26631;&#12290;&#22240;&#27492;&#65292;&#19968;&#20010;A/B&#27979;&#35797;&#30340;&#25104;&#21151;&#25110;&#22833;&#36133;&#23436;&#20840;&#21462;&#20915;&#20110;&#25152;&#25552;&#20986;&#30340;&#20195;&#29702;&#25351;&#26631;&#26159;&#21542;&#19982;&#24615;&#33021;&#30456;&#20851;&#24615;&#26356;&#22909;&#12290;&#27809;&#26377;&#21407;&#21017;&#21487;&#20197;&#22312;&#31163;&#32447;&#24773;&#20917;&#19979;&#30830;&#23450;&#19968;&#20010;&#20195;&#29702;&#25351;&#26631;&#26159;&#21542;&#27604;&#21478;&#19968;&#20010;&#26356;&#22909;&#65292;&#36825;&#20351;&#24471;&#20174;&#19994;&#32773;&#20204;&#25720;&#19981;&#30528;&#22836;&#33041;&#12290;&#26412;&#35770;&#25991;&#30340;&#30446;&#30340;&#26159;&#36136;&#30097;&#36825;&#31181;&#21453;&#20044;&#25176;&#37030;&#24605;&#32500;&#65292;&#24182;&#20027;&#24352;&#28145;&#24230;&#23398;&#20064;&#22534;&#26632;&#30340;&#38750;&#26631;&#20934;&#29992;&#27861;&#23454;&#38469;&#19978;&#26377;&#28508;&#21147;&#35299;&#38145;&#20248;&#21270;&#22870;&#21169;&#30340;&#25512;&#33616;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Applied recommender systems research is in a curious position. While there is a very rigorous protocol for measuring performance by A/B testing, best practice for finding a `B' to test does not explicitly target performance but rather targets a proxy measure. The success or failure of a given A/B test then depends entirely on if the proposed proxy is better correlated to performance than the previous proxy. No principle exists to identify if one proxy is better than another offline, leaving the practitioners shooting in the dark. The purpose of this position paper is to question this anti-Utopian thinking and argue that a non-standard use of the deep learning stacks actually has the potential to unlock reward optimizing recommendation.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#40784;&#21644;&#19968;&#33268;&#24615;&#30340;&#21407;&#22411;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65288;ProtoAU&#65289;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#20013;&#65292;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#38543;&#26426;&#25277;&#26679;&#24341;&#36215;&#30340;&#25277;&#26679;&#20559;&#24046;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#29305;&#24449;&#34920;&#31034;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02079</link><description>&lt;p&gt;
&#22522;&#20110;&#23545;&#40784;&#21644;&#19968;&#33268;&#24615;&#30340;&#21407;&#22411;&#23545;&#27604;&#23398;&#20064;&#29992;&#20110;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Prototypical Contrastive Learning through Alignment and Uniformity for Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02079
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#40784;&#21644;&#19968;&#33268;&#24615;&#30340;&#21407;&#22411;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65288;ProtoAU&#65289;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#20013;&#65292;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#38543;&#26426;&#25277;&#26679;&#24341;&#36215;&#30340;&#25277;&#26679;&#20559;&#24046;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#29305;&#24449;&#34920;&#31034;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#21327;&#21516;&#36807;&#28388;&#65288;GCF&#65289;&#26159;&#26368;&#24120;&#20351;&#29992;&#30340;&#25512;&#33616;&#31995;&#32479;&#26041;&#27861;&#20043;&#19968;&#65292;&#26377;&#25928;&#22320;&#25429;&#25417;&#29992;&#25143;&#21644;&#29289;&#21697;&#20043;&#38388;&#22797;&#26434;&#30340;&#20851;&#31995;&#12290;&#22522;&#20110;&#22270;&#30340;&#23545;&#27604;&#23398;&#20064;&#65288;GCL&#65289;&#30340;GCF&#24050;&#32463;&#21463;&#21040;&#20102;&#26174;&#33879;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#21033;&#29992;&#33258;&#30417;&#30563;&#25216;&#26415;&#20174;&#29616;&#23454;&#22330;&#26223;&#20013;&#25552;&#21462;&#26377;&#20215;&#20540;&#30340;&#20449;&#21495;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#26041;&#27861;&#36890;&#24120;&#36890;&#36807;&#38543;&#26426;&#25277;&#26679;&#23398;&#20064;&#28041;&#21450;&#26500;&#24314;&#23545;&#27604;&#23545;&#30340;&#37492;&#21035;&#20219;&#21153;&#30340;&#23454;&#20363;&#12290;GCL&#26041;&#27861;&#38754;&#20020;&#25277;&#26679;&#20559;&#24046;&#38382;&#39064;&#65292;&#36127;&#26679;&#26412;&#21487;&#33021;&#20855;&#26377;&#19982;&#27491;&#26679;&#26412;&#31867;&#20284;&#30340;&#35821;&#20041;&#32467;&#26500;&#65292;&#23548;&#33268;&#29305;&#24449;&#34920;&#31034;&#30340;&#26377;&#25928;&#24615;&#19979;&#38477;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#23545;&#40784;&#21644;&#19968;&#33268;&#24615;&#30340;&#21407;&#22411;&#23545;&#27604;&#23398;&#20064;&#29992;&#20110;&#25512;&#33616;&#65292;&#31216;&#20026;ProtoAU&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#21407;&#22411;&#65288;&#32858;&#31867;&#20013;&#24515;&#65289;&#20316;&#20026;&#19968;&#20010;&#28508;&#22312;&#31354;&#38388;&#65292;&#20197;&#30830;&#20445;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Collaborative Filtering (GCF), one of the most widely adopted recommendation system methods, effectively captures intricate relationships between user and item interactions. Graph Contrastive Learning (GCL) based GCF has gained significant attention as it leverages self-supervised techniques to extract valuable signals from real-world scenarios. However, many methods usually learn the instances of discrimination tasks that involve the construction of contrastive pairs through random sampling. GCL approaches suffer from sampling bias issues, where the negatives might have a semantic structure similar to that of the positives, thus leading to a loss of effective feature representation. To address these problems, we present the \underline{Proto}typical contrastive learning through \underline{A}lignment and \underline{U}niformity for recommendation, which is called \textbf{ProtoAU}. Specifically, we first propose prototypes (cluster centroids) as a latent space to ensure consistency 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23616;&#37096;&#33258;&#36866;&#24212;&#37327;&#21270;&#22312;&#27969;&#24335;&#30456;&#20284;&#24230;&#25628;&#32034;&#20013;&#30340;&#24212;&#29992;&#65292;&#24341;&#20837;&#20102;Turbo LVQ&#21644;multi-means LVQ&#20004;&#31181;&#25913;&#36827;&#26041;&#27861;&#65292;&#20998;&#21035;&#25552;&#21319;&#20102;&#25628;&#32034;&#24615;&#33021;28%&#21644;27%&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;LVQ&#21450;&#20854;&#26032;&#21464;&#20307;&#20351;&#24471;&#21521;&#37327;&#25628;&#32034;&#21464;&#24471;&#26497;&#24555;&#12290;</title><link>https://arxiv.org/abs/2402.02044</link><description>&lt;p&gt;
&#29992;&#20110;&#27969;&#24335;&#21521;&#37327;&#25628;&#32034;&#30340;&#23616;&#37096;&#33258;&#36866;&#24212;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Locally-Adaptive Quantization for Streaming Vector Search
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02044
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23616;&#37096;&#33258;&#36866;&#24212;&#37327;&#21270;&#22312;&#27969;&#24335;&#30456;&#20284;&#24230;&#25628;&#32034;&#20013;&#30340;&#24212;&#29992;&#65292;&#24341;&#20837;&#20102;Turbo LVQ&#21644;multi-means LVQ&#20004;&#31181;&#25913;&#36827;&#26041;&#27861;&#65292;&#20998;&#21035;&#25552;&#21319;&#20102;&#25628;&#32034;&#24615;&#33021;28%&#21644;27%&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;LVQ&#21450;&#20854;&#26032;&#21464;&#20307;&#20351;&#24471;&#21521;&#37327;&#25628;&#32034;&#21464;&#24471;&#26497;&#24555;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#37327;&#21521;&#37327;&#38598;&#21512;&#20013;&#25214;&#21040;&#19982;&#32473;&#23450;&#26597;&#35810;&#26368;&#30456;&#20284;&#30340;&#21521;&#37327;&#23884;&#20837;&#19968;&#30452;&#26159;&#26080;&#25968;&#23454;&#38469;&#24212;&#29992;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#26368;&#36817;&#24341;&#20837;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#23601;&#26159;&#20854;&#20013;&#26368;&#31361;&#20986;&#30340;&#20363;&#23376;&#20043;&#19968;&#12290;&#23545;&#20110;&#35768;&#22810;&#36825;&#20123;&#24212;&#29992;&#65292;&#25968;&#25454;&#24211;&#36890;&#36807;&#25554;&#20837;&#26032;&#25968;&#25454;&#21644;&#21024;&#38500;&#36807;&#26102;&#25968;&#25454;&#32780;&#38543;&#26102;&#38388;&#28436;&#21464;&#12290;&#22312;&#36825;&#20123;&#24773;&#20917;&#19979;&#65292;&#26816;&#32034;&#38382;&#39064;&#34987;&#31216;&#20026;&#27969;&#24335;&#30456;&#20284;&#24230;&#25628;&#32034;&#12290;&#34429;&#28982;&#23616;&#37096;&#33258;&#36866;&#24212;&#21521;&#37327;&#37327;&#21270;&#65288;LVQ&#65289;&#20316;&#20026;&#19968;&#31181;&#39640;&#25928;&#30340;&#21521;&#37327;&#21387;&#32553;&#26041;&#27861;&#65292;&#22312;&#38750;&#28436;&#21270;&#25968;&#25454;&#24211;&#20013;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#25628;&#32034;&#24615;&#33021;&#65292;&#20294;&#20854;&#22312;&#27969;&#24335;&#22330;&#26223;&#20013;&#30340;&#23454;&#29992;&#24615;&#23578;&#26410;&#30830;&#23450;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;LVQ&#22312;&#27969;&#24335;&#30456;&#20284;&#24230;&#25628;&#32034;&#20013;&#30340;&#24212;&#29992;&#12290;&#20026;&#20102;&#25903;&#25345;&#25105;&#20204;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#31181;LVQ&#30340;&#25913;&#36827;&#65306;Turbo LVQ&#21644;multi-means LVQ&#65292;&#20998;&#21035;&#25552;&#21319;&#20102;&#20854;&#25628;&#32034;&#24615;&#33021;&#39640;&#36798;28%&#21644;27%&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;LVQ&#21450;&#20854;&#26032;&#21464;&#20307;&#20351;&#24471;&#21521;&#37327;&#25628;&#32034;&#21464;&#24471;&#26497;&#24555;&#12290;
&lt;/p&gt;
&lt;p&gt;
Retrieving the most similar vector embeddings to a given query among a massive collection of vectors has long been a key component of countless real-world applications. The recently introduced Retrieval-Augmented Generation is one of the most prominent examples. For many of these applications, the database evolves over time by inserting new data and removing outdated data. In these cases, the retrieval problem is known as streaming similarity search. While Locally-Adaptive Vector Quantization (LVQ), a highly efficient vector compression method, yields state-of-the-art search performance for non-evolving databases, its usefulness in the streaming setting has not been yet established. In this work, we study LVQ in streaming similarity search. In support of our evaluation, we introduce two improvements of LVQ: Turbo LVQ and multi-means LVQ that boost its search performance by up to 28% and 27%, respectively. Our studies show that LVQ and its new variants enable blazing fast vector search,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26631;&#31614;&#33258;&#32534;&#30721;&#22120;&#25913;&#36827;&#22823;&#35268;&#27169;k&#26368;&#36817;&#37051;&#25991;&#26412;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22823;&#30340;&#26631;&#31614;&#31354;&#38388;&#26144;&#23556;&#21040;&#19968;&#20010;&#32553;&#23567;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#24182;&#20174;&#35813;&#28508;&#22312;&#31354;&#38388;&#20013;&#37325;&#24314;&#20986;&#39044;&#27979;&#30340;&#26631;&#31614;&#65292;&#20197;&#35299;&#20915;&#22312;&#22823;&#22411;&#25991;&#26723;&#38598;&#21512;&#20013;&#22788;&#29702;&#33258;&#21160;&#35821;&#20041;&#32034;&#24341;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.01963</link><description>&lt;p&gt;
&#20351;&#29992;&#26631;&#31614;&#33258;&#32534;&#30721;&#22120;&#25913;&#36827;&#22823;&#35268;&#27169;k&#26368;&#36817;&#37051;&#25991;&#26412;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Improving Large-Scale k-Nearest Neighbor Text Categorization with Label Autoencoders
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01963
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26631;&#31614;&#33258;&#32534;&#30721;&#22120;&#25913;&#36827;&#22823;&#35268;&#27169;k&#26368;&#36817;&#37051;&#25991;&#26412;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22823;&#30340;&#26631;&#31614;&#31354;&#38388;&#26144;&#23556;&#21040;&#19968;&#20010;&#32553;&#23567;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#24182;&#20174;&#35813;&#28508;&#22312;&#31354;&#38388;&#20013;&#37325;&#24314;&#20986;&#39044;&#27979;&#30340;&#26631;&#31614;&#65292;&#20197;&#35299;&#20915;&#22312;&#22823;&#22411;&#25991;&#26723;&#38598;&#21512;&#20013;&#22788;&#29702;&#33258;&#21160;&#35821;&#20041;&#32034;&#24341;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22810;&#26631;&#31614;&#24816;&#24615;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22797;&#26434;&#21644;&#32467;&#26500;&#21270;&#26631;&#31614;&#35789;&#27719;&#20855;&#26377;&#39640;&#30456;&#20851;&#24615;&#30340;&#22823;&#22411;&#25991;&#26723;&#38598;&#21512;&#20013;&#22788;&#29702;&#33258;&#21160;&#35821;&#20041;&#32034;&#24341;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26159;&#20256;&#32479;k&#26368;&#36817;&#37051;&#31639;&#27861;&#30340;&#28436;&#21270;&#65292;&#23427;&#20351;&#29992;&#19968;&#20010;&#22823;&#30340;&#33258;&#32534;&#30721;&#22120;&#26469;&#23558;&#22823;&#30340;&#26631;&#31614;&#31354;&#38388;&#26144;&#23556;&#21040;&#19968;&#20010;&#32553;&#23567;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#24182;&#20174;&#35813;&#28508;&#22312;&#31354;&#38388;&#20013;&#37325;&#24314;&#20986;&#39044;&#27979;&#30340;&#26631;&#31614;&#12290;&#25105;&#20204;&#22312;MEDLINE&#29983;&#29289;&#21307;&#23398;&#25991;&#26723;&#38598;&#21512;&#30340;&#22823;&#37096;&#20998;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#25552;&#35758;&#65292;&#35813;&#38598;&#21512;&#20351;&#29992;&#21307;&#23398;&#20027;&#39064;&#35789;&#65288;MeSH&#65289;&#35789;&#27719;&#34920;&#20316;&#20026;&#25511;&#21046;&#35789;&#27719;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#20960;&#31181;&#25991;&#26723;&#34920;&#31034;&#26041;&#27861;&#21644;&#19981;&#21516;&#30340;&#26631;&#31614;&#33258;&#32534;&#30721;&#22120;&#37197;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce a multi-label lazy learning approach to deal with automatic semantic indexing in large document collections in the presence of complex and structured label vocabularies with high inter-label correlation. The proposed method is an evolution of the traditional k-Nearest Neighbors algorithm which uses a large autoencoder trained to map the large label space to a reduced size latent space and to regenerate the predicted labels from this latent space. We have evaluated our proposal in a large portion of the MEDLINE biomedical document collection which uses the Medical Subject Headings (MeSH) thesaurus as a controlled vocabulary. In our experiments we propose and evaluate several document representation approaches and different label autoencoder configurations.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#28548;&#28165;&#38382;&#39064;&#23545;&#29992;&#25143;&#28385;&#24847;&#24230;&#30340;&#37325;&#35201;&#24433;&#21709;&#65292;&#30740;&#31350;&#21457;&#29616;&#20855;&#20307;&#38382;&#39064;&#12289;&#38382;&#39064;&#30340;&#20027;&#35266;&#24615;&#21644;&#24773;&#24863;&#33394;&#24425;&#20197;&#21450;&#26597;&#35810;&#38271;&#24230;&#31561;&#29305;&#24449;&#23545;&#28548;&#28165;&#30340;&#26377;&#25928;&#24615;&#26377;&#26126;&#26174;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.01934</link><description>&lt;p&gt;
&#28548;&#28165;&#36335;&#24452;&#23545;&#29992;&#25143;&#28385;&#24847;&#24230;&#30340;&#24433;&#21709;&#65306;&#23545;&#28548;&#28165;&#38382;&#39064;&#26377;&#29992;&#24615;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Clarifying the Path to User Satisfaction: An Investigation into Clarification Usefulness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01934
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#28548;&#28165;&#38382;&#39064;&#23545;&#29992;&#25143;&#28385;&#24847;&#24230;&#30340;&#37325;&#35201;&#24433;&#21709;&#65292;&#30740;&#31350;&#21457;&#29616;&#20855;&#20307;&#38382;&#39064;&#12289;&#38382;&#39064;&#30340;&#20027;&#35266;&#24615;&#21644;&#24773;&#24863;&#33394;&#24425;&#20197;&#21450;&#26597;&#35810;&#38271;&#24230;&#31561;&#29305;&#24449;&#23545;&#28548;&#28165;&#30340;&#26377;&#25928;&#24615;&#26377;&#26126;&#26174;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28548;&#28165;&#38382;&#39064;&#26159;&#29616;&#20195;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#30452;&#25509;&#24433;&#21709;&#29992;&#25143;&#28385;&#24847;&#24230;&#21644;&#25972;&#20307;&#31995;&#32479;&#24615;&#33021;&#12290;&#38169;&#35823;&#26500;&#24314;&#30340;&#38382;&#39064;&#21487;&#33021;&#23548;&#33268;&#29992;&#25143;&#30340;&#19981;&#28385;&#21644;&#22256;&#24785;&#65292;&#36827;&#32780;&#23545;&#31995;&#32479;&#30340;&#24615;&#33021;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36843;&#20999;&#38656;&#35201;&#30830;&#23450;&#21644;&#21033;&#29992;&#26377;&#21161;&#20110;&#20998;&#31867;&#28548;&#28165;&#38382;&#39064;&#30340;&#20851;&#38190;&#29305;&#24449;&#65292;&#20197;&#25552;&#21319;&#29992;&#25143;&#28385;&#24847;&#24230;&#12290;&#20026;&#20102;&#26356;&#28145;&#20837;&#22320;&#20102;&#35299;&#19981;&#21516;&#29305;&#24449;&#23545;&#29992;&#25143;&#28385;&#24847;&#24230;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#20998;&#26512;&#65292;&#32771;&#34385;&#20102;&#24191;&#27867;&#30340;&#35789;&#27719;&#12289;&#35821;&#20041;&#21644;&#32479;&#35745;&#29305;&#24449;&#65292;&#22914;&#38382;&#39064;&#38271;&#24230;&#21644;&#24773;&#24863;&#26497;&#24615;&#31561;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#25552;&#20379;&#20102;&#20851;&#20110;&#26377;&#25928;&#26597;&#35810;&#28548;&#28165;&#30340;&#19977;&#20010;&#20027;&#35201;&#35265;&#35299;&#65306;&#65288;1&#65289;&#20855;&#20307;&#30340;&#38382;&#39064;&#27604;&#27867;&#21270;&#30340;&#38382;&#39064;&#26356;&#26377;&#25928;&#65307;&#65288;2&#65289;&#38382;&#39064;&#30340;&#20027;&#35266;&#24615;&#21644;&#24773;&#24863;&#33394;&#24425;&#36215;&#21040;&#19968;&#23450;&#20316;&#29992;&#65307;&#65288;3&#65289;&#36739;&#30701;&#21644;&#26356;&#27169;&#31946;&#30340;&#26597;&#35810;&#22312;&#28548;&#28165;&#36807;&#31243;&#20013;&#24471;&#30410;&#33391;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clarifying questions are an integral component of modern information retrieval systems, directly impacting user satisfaction and overall system performance. Poorly formulated questions can lead to user frustration and confusion, negatively affecting the system's performance. This research addresses the urgent need to identify and leverage key features that contribute to the classification of clarifying questions, enhancing user satisfaction. To gain deeper insights into how different features influence user satisfaction, we conduct a comprehensive analysis, considering a broad spectrum of lexical, semantic, and statistical features, such as question length and sentiment polarity. Our empirical results provide three main insights into the qualities of effective query clarification: (1) specific questions are more effective than generic ones; (2) the subjectivity and emotional tone of a question play a role; and (3) shorter and more ambiguous queries benefit significantly from clarificat
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#30456;&#20284;&#24230;&#30340;&#25551;&#36848;&#31526;&#20998;&#37197;&#26041;&#27861;&#22312;BioASQ MESINESP8&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#20351;&#29992;&#20256;&#32479;&#30340;&#20449;&#24687;&#26816;&#32034;&#24037;&#20855;&#65292;&#24182;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#35199;&#29677;&#29273;&#35821;&#31561;&#35821;&#35328;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.01916</link><description>&lt;p&gt;
CoLe&#21644;LYS&#22312;BioASQ MESINESP8&#20219;&#21153;&#20013;&#30340;&#34920;&#24449;&#20998;&#37197;&#30340;&#30456;&#20284;&#24230;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
CoLe and LYS at BioASQ MESINESP8 Task: similarity based descriptor assignment in Spanish
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01916
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#30456;&#20284;&#24230;&#30340;&#25551;&#36848;&#31526;&#20998;&#37197;&#26041;&#27861;&#22312;BioASQ MESINESP8&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#20351;&#29992;&#20256;&#32479;&#30340;&#20449;&#24687;&#26816;&#32034;&#24037;&#20855;&#65292;&#24182;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#35199;&#29677;&#29273;&#35821;&#31561;&#35821;&#35328;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#25105;&#20204;&#21442;&#19982;&#20102;BioASQ&#29983;&#29289;&#21307;&#23398;&#35821;&#20041;&#25351;&#26631;&#25361;&#25112;&#36187;&#30340;MESINESP&#20219;&#21153;&#12290;&#21442;&#19982;&#30340;&#31995;&#32479;&#20165;&#20351;&#29992;&#20102;&#20256;&#32479;&#30340;&#20449;&#24687;&#26816;&#32034;&#24037;&#20855;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20174;IBECS/LILACS&#25991;&#26723;&#20013;&#25552;&#21462;&#32034;&#24341;&#26415;&#35821;&#30340;&#22810;&#31181;&#26041;&#27861;&#65292;&#20197;&#20415;&#23384;&#20648;&#22312;Apache Lucene&#32034;&#24341;&#20013;&#12290;&#36825;&#20123;&#32034;&#24341;&#34920;&#31034;&#20351;&#29992;&#35201;&#27880;&#37322;&#30340;&#25991;&#31456;&#20869;&#23481;&#36827;&#34892;&#26597;&#35810;&#65292;&#24182;&#20174;&#26816;&#32034;&#21040;&#30340;&#25991;&#26723;&#21019;&#24314;&#19968;&#20010;&#20505;&#36873;&#26631;&#31614;&#30340;&#25490;&#24207;&#21015;&#34920;&#12290;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;&#19968;&#31181;&#26377;&#38480;&#30340;&#26631;&#31614;&#20840;&#38598;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#20855;&#26377;&#39640;&#20849;&#29616;&#24471;&#20998;&#30340;DeCS&#26631;&#31614;&#37197;&#23545;&#24182;&#21019;&#24314;&#20803;&#26631;&#31614;&#65292;&#20197;&#21450;&#19968;&#31181;&#22522;&#20110;&#26631;&#31614;&#20010;&#20154;&#36164;&#26009;&#21305;&#37197;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#22312;&#23448;&#26041;&#36816;&#34892;&#20013;&#33719;&#24471;&#30340;&#32467;&#26524;&#20284;&#20046;&#35777;&#23454;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#35199;&#29677;&#29273;&#35821;&#31561;&#35821;&#35328;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we describe our participation in the MESINESP Task of the BioASQ biomedical semantic indexing challenge. The participating system follows an approach based solely on conventional information retrieval tools. We have evaluated various alternatives for extracting index terms from IBECS/LILACS documents in order to be stored in an Apache Lucene index. Those indexed representations are queried using the contents of the article to be annotated and a ranked list of candidate labels is created from the retrieved documents. We also have evaluated a sort of limited Label Powerset approach which creates meta-labels joining pairs of DeCS labels with high co-occurrence scores, and an alternative method based on label profile matching. Results obtained in official runs seem to confirm the suitability of this approach for languages like Spanish.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Zero-1-to-3&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#19968;&#25209;&#26089;&#40479;&#23398;&#29983;&#20851;&#27880;&#19977;&#20010;&#35786;&#26029;&#30446;&#26631;&#65292;&#20197;&#23454;&#29616;&#39046;&#22495;&#32423;&#38646;&#26679;&#26412;&#35748;&#30693;&#35786;&#26029;&#12290;&#23427;&#35299;&#20915;&#20102;&#36328;&#39046;&#22495;&#35786;&#26029;&#27169;&#22411;&#20013;&#21487;&#33021;&#34701;&#20837;&#19981;&#21487;&#36716;&#31227;&#20449;&#24687;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#30693;&#35782;&#20256;&#36755;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2312.13434</link><description>&lt;p&gt;
Zero-1-to-3: &#22522;&#20110;&#39046;&#22495;&#32423;&#38646;&#26679;&#26412;&#35748;&#30693;&#35786;&#26029;&#30340;&#19968;&#25209;&#26089;&#40479;&#23398;&#29983;&#20851;&#20110;&#19977;&#20010;&#35786;&#26029;&#30446;&#26631;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Zero-1-to-3: Domain-level Zero-shot Cognitive Diagnosis via One Batch of Early-bird Students towards Three Diagnostic Objectives
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.13434
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Zero-1-to-3&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#19968;&#25209;&#26089;&#40479;&#23398;&#29983;&#20851;&#27880;&#19977;&#20010;&#35786;&#26029;&#30446;&#26631;&#65292;&#20197;&#23454;&#29616;&#39046;&#22495;&#32423;&#38646;&#26679;&#26412;&#35748;&#30693;&#35786;&#26029;&#12290;&#23427;&#35299;&#20915;&#20102;&#36328;&#39046;&#22495;&#35786;&#26029;&#27169;&#22411;&#20013;&#21487;&#33021;&#34701;&#20837;&#19981;&#21487;&#36716;&#31227;&#20449;&#24687;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#30693;&#35782;&#20256;&#36755;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35748;&#30693;&#35786;&#26029;&#36890;&#36807;&#25506;&#32034;&#23398;&#29983;&#30340;&#32451;&#20064;&#27979;&#39564;&#25968;&#25454;&#26469;&#20272;&#35745;&#20182;&#20204;&#30340;&#35748;&#30693;&#29366;&#24577;&#12290;&#23427;&#22312;&#26234;&#33021;&#25945;&#32946;&#31995;&#32479;&#20013;&#30340;&#20010;&#24615;&#21270;&#23398;&#20064;&#25351;&#23548;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#26412;&#25991;&#20851;&#27880;&#19968;&#20010;&#37325;&#35201;&#20294;&#24120;&#24120;&#34987;&#24573;&#35270;&#30340;&#20219;&#21153;&#65306;&#39046;&#22495;&#32423;&#38646;&#26679;&#26412;&#35748;&#30693;&#35786;&#26029;&#65288;DZCD&#65289;&#65292;&#22240;&#20026;&#22312;&#26032;&#21551;&#21160;&#30340;&#39046;&#22495;&#20013;&#32570;&#20047;&#23398;&#29983;&#30340;&#32451;&#20064;&#35760;&#24405;&#32780;&#20135;&#29983;&#12290;&#26368;&#36817;&#30340;&#36328;&#39046;&#22495;&#35786;&#26029;&#27169;&#22411;&#24050;&#32463;&#35777;&#26126;&#26159;DZCD&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#31574;&#30053;&#12290;&#36825;&#20123;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#22914;&#20309;&#22312;&#39046;&#22495;&#20043;&#38388;&#20256;&#36882;&#23398;&#29983;&#29366;&#24577;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#21487;&#33021;&#20250;&#26080;&#24847;&#20013;&#23558;&#19981;&#21487;&#36716;&#31227;&#30340;&#20449;&#24687;&#34701;&#20837;&#21040;&#23398;&#29983;&#30340;&#34920;&#31034;&#20013;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#30693;&#35782;&#20256;&#36755;&#30340;&#25928;&#26524;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Zero-1-to-3&#65292;&#19968;&#31181;&#22522;&#20110;&#39046;&#22495;&#32423;&#38646;&#26679;&#26412;&#35748;&#30693;&#35786;&#26029;&#26694;&#26550;&#65292;&#36890;&#36807;&#19968;&#25209;&#26089;&#40479;&#23398;&#29983;&#23454;&#29616;&#19977;&#20010;&#35786;&#26029;&#30446;&#26631;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#39318;&#20808;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35786;&#26029;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cognitive diagnosis seeks to estimate the cognitive states of students by exploring their logged practice quiz data. It plays a pivotal role in personalized learning guidance within intelligent education systems. In this paper, we focus on an important, practical, yet often underexplored task: domain-level zero-shot cognitive diagnosis (DZCD), which arises due to the absence of student practice logs in newly launched domains. Recent cross-domain diagnostic models have been demonstrated to be a promising strategy for DZCD. These methods primarily focus on how to transfer student states across domains. However, they might inadvertently incorporate non-transferable information into student representations, thereby limiting the efficacy of knowledge transfer. To tackle this, we propose Zero-1-to-3, a domain-level zero-shot cognitive diagnosis framework via one batch of early-bird students towards three diagnostic objectives. Our approach initiates with pre-training a diagnosis model with d
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#25512;&#33616;&#27169;&#22411;KGLN&#65292;&#36890;&#36807;&#21512;&#24182;&#33410;&#28857;&#29305;&#24449;&#12289;&#35843;&#25972;&#32858;&#21512;&#26435;&#37325;&#21644;&#36845;&#20195;&#28436;&#21270;&#65292;&#25552;&#39640;&#20102;&#20010;&#24615;&#21270;&#25512;&#33616;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#26524;&#12290;&#22312;&#23454;&#39564;&#20013;&#30456;&#23545;&#20110;&#24050;&#26377;&#22522;&#20934;&#26041;&#27861;&#65292;KGLN&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;AUC&#25552;&#39640;&#20102;0.3%&#33267;5.9%&#21644;1.1%&#33267;8.2%&#12290;</title><link>http://arxiv.org/abs/2401.10244</link><description>&lt;p&gt;
&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#39537;&#21160;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#25512;&#33616;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Knowledge graph driven recommendation model of graph neural network. (arXiv:2401.10244v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10244
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#25512;&#33616;&#27169;&#22411;KGLN&#65292;&#36890;&#36807;&#21512;&#24182;&#33410;&#28857;&#29305;&#24449;&#12289;&#35843;&#25972;&#32858;&#21512;&#26435;&#37325;&#21644;&#36845;&#20195;&#28436;&#21270;&#65292;&#25552;&#39640;&#20102;&#20010;&#24615;&#21270;&#25512;&#33616;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#26524;&#12290;&#22312;&#23454;&#39564;&#20013;&#30456;&#23545;&#20110;&#24050;&#26377;&#22522;&#20934;&#26041;&#27861;&#65292;KGLN&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;AUC&#25552;&#39640;&#20102;0.3%&#33267;5.9%&#21644;1.1%&#33267;8.2%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25512;&#33616;&#27169;&#22411;KGLN&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#20449;&#24687;&#65292;&#25552;&#39640;&#20102;&#20010;&#24615;&#21270;&#25512;&#33616;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#26524;&#12290;&#35813;&#27169;&#22411;&#39318;&#20808;&#21033;&#29992;&#21333;&#23618;&#31070;&#32463;&#32593;&#32476;&#23558;&#22270;&#20013;&#30340;&#20010;&#20307;&#33410;&#28857;&#29305;&#24449;&#21512;&#24182;&#65292;&#28982;&#21518;&#36890;&#36807;&#32467;&#21512;&#24433;&#21709;&#22240;&#32032;&#35843;&#25972;&#30456;&#37051;&#23454;&#20307;&#30340;&#32858;&#21512;&#26435;&#37325;&#12290;&#36890;&#36807;&#36845;&#20195;&#65292;&#27169;&#22411;&#20174;&#21333;&#23618;&#36880;&#28176;&#28436;&#21464;&#20026;&#22810;&#23618;&#65292;&#20351;&#23454;&#20307;&#33021;&#22815;&#33719;&#21462;&#20016;&#23500;&#30340;&#22810;&#38454;&#20851;&#32852;&#23454;&#20307;&#20449;&#24687;&#12290;&#26368;&#21518;&#65292;&#23558;&#23454;&#20307;&#21644;&#29992;&#25143;&#30340;&#29305;&#24449;&#32467;&#21512;&#36215;&#26469;&#20135;&#29983;&#25512;&#33616;&#20998;&#25968;&#12290;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#32858;&#21512;&#26041;&#27861;&#21644;&#24433;&#21709;&#22240;&#32032;&#30340;&#25928;&#26524;&#65292;&#35780;&#20272;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#22312;&#20351;&#29992;MovieLen-1M&#21644;Book-Crossing&#25968;&#25454;&#38598;&#36827;&#34892;&#27979;&#35797;&#26102;&#65292;KGLN&#30456;&#23545;&#20110;LibFM&#21644;D&#31561;&#24050;&#26377;&#22522;&#20934;&#26041;&#27861;&#65292;AUC&#65288;ROC&#26354;&#32447;&#19979;&#30340;&#38754;&#31215;&#65289;&#25552;&#39640;&#20102;0.3%&#33267;5.9%&#21644;1.1%&#33267;8.2%&#12290;
&lt;/p&gt;
&lt;p&gt;
A new graph neural network-based recommendation model called KGLN, which leverages Knowledge Graph (KG) information, was developed to enhance the accuracy and effectiveness of personalized recommendations. This model begins by using a single-layer neural network to merge individual node features in the graph. It then adjusts the aggregation weights of neighboring entities by incorporating influence factors. The model evolves from a single layer to multiple layers through iteration, enabling entities to access extensive multi-order associated entity information. The final step involves integrating features of entities and users to produce a recommendation score. The model's performance was evaluated by comparing its effects on various aggregation methods and influence factors. In tests using the MovieLen-1M and Book-Crossing datasets, KGLN showed an AUC (Area Under the ROC curve) improvement of 0.3% to 5.9% and 1.1% to 8.2%, respectively, over established benchmark methods like LibFM, D
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39057;&#35889;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65288;SComGNN&#65289;&#29992;&#20110;&#27169;&#25311;&#21644;&#29702;&#35299;&#21830;&#21697;&#38388;&#30340;&#20114;&#34917;&#20851;&#31995;&#65292;&#20197;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#20934;&#30830;&#21644;&#21450;&#26102;&#22320;&#25512;&#33616;&#21518;&#32493;&#21830;&#21697;&#12290;</title><link>http://arxiv.org/abs/2401.02130</link><description>&lt;p&gt;
&#22522;&#20110;&#39057;&#35889;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#20114;&#34917;&#21830;&#21697;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Spectral-based Graph Neutral Networks for Complementary Item Recommendation. (arXiv:2401.02130v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02130
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39057;&#35889;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65288;SComGNN&#65289;&#29992;&#20110;&#27169;&#25311;&#21644;&#29702;&#35299;&#21830;&#21697;&#38388;&#30340;&#20114;&#34917;&#20851;&#31995;&#65292;&#20197;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#20934;&#30830;&#21644;&#21450;&#26102;&#22320;&#25512;&#33616;&#21518;&#32493;&#21830;&#21697;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#25311;&#20114;&#34917;&#20851;&#31995;&#26497;&#22823;&#22320;&#24110;&#21161;&#25512;&#33616;&#31995;&#32479;&#22312;&#36141;&#20080;&#19968;&#20010;&#21830;&#21697;&#21518;&#20934;&#30830;&#21644;&#21450;&#26102;&#22320;&#25512;&#33616;&#21518;&#32493;&#30340;&#21830;&#21697;&#12290;&#19982;&#20256;&#32479;&#30340;&#30456;&#20284;&#20851;&#31995;&#19981;&#21516;&#65292;&#20855;&#26377;&#20114;&#34917;&#20851;&#31995;&#30340;&#21830;&#21697;&#21487;&#33021;&#20250;&#36830;&#32493;&#36141;&#20080;&#65288;&#20363;&#22914;iPhone&#21644;AirPods Pro&#65289;&#65292;&#23427;&#20204;&#19981;&#20165;&#20849;&#20139;&#30456;&#20851;&#24615;&#65292;&#36824;&#23637;&#29616;&#20986;&#19981;&#30456;&#20284;&#24615;&#12290;&#30001;&#20110;&#36825;&#20004;&#20010;&#23646;&#24615;&#26159;&#30456;&#21453;&#30340;&#65292;&#24314;&#27169;&#20114;&#34917;&#20851;&#31995;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20808;&#21069;&#23581;&#35797;&#21033;&#29992;&#36825;&#20123;&#20851;&#31995;&#30340;&#26041;&#27861;&#35201;&#20040;&#24573;&#35270;&#20102;&#25110;&#36807;&#24230;&#31616;&#21270;&#20102;&#19981;&#30456;&#20284;&#24615;&#23646;&#24615;&#65292;&#23548;&#33268;&#24314;&#27169;&#26080;&#25928;&#24182;&#19988;&#26080;&#27861;&#24179;&#34913;&#36825;&#20004;&#20010;&#23646;&#24615;&#12290;&#30001;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#21487;&#20197;&#22312;&#39057;&#35889;&#22495;&#20013;&#25429;&#25417;&#33410;&#28857;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#21644;&#19981;&#30456;&#20284;&#24615;&#65292;&#25105;&#20204;&#21487;&#20197;&#21033;&#29992;&#22522;&#20110;&#39057;&#35889;&#30340;GNNs&#26377;&#25928;&#22320;&#29702;&#35299;&#21644;&#24314;&#27169;&#20114;&#34917;&#20851;&#31995;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#31216;&#20026;&#22522;&#20110;&#39057;&#35889;&#30340;&#20114;&#34917;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;SComGNN&#65289;&#65292;&#21033;&#29992;&#36825;&#19968;&#26041;&#27861;&#21487;&#20197;&#27604;&#36739;&#22909;&#22320;&#21033;&#29992;&#20114;&#34917;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modeling complementary relationships greatly helps recommender systems to accurately and promptly recommend the subsequent items when one item is purchased. Unlike traditional similar relationships, items with complementary relationships may be purchased successively (such as iPhone and Airpods Pro), and they not only share relevance but also exhibit dissimilarity. Since the two attributes are opposites, modeling complementary relationships is challenging. Previous attempts to exploit these relationships have either ignored or oversimplified the dissimilarity attribute, resulting in ineffective modeling and an inability to balance the two attributes. Since Graph Neural Networks (GNNs) can capture the relevance and dissimilarity between nodes in the spectral domain, we can leverage spectral-based GNNs to effectively understand and model complementary relationships. In this study, we present a novel approach called Spectral-based Complementary Graph Neural Networks (SComGNN) that utilize
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;NLP&#20998;&#26512;&#20102;ESG&#20027;&#23548;&#30340;DLT&#30740;&#31350;&#30340;&#28436;&#21270;&#65292;&#36890;&#36807;&#26500;&#24314;&#24341;&#29992;&#32593;&#32476;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#65292;&#23545;DLT&#22312;ESG&#32972;&#26223;&#19979;&#30340;&#21457;&#23637;&#36827;&#34892;&#20102;&#25991;&#29486;&#32508;&#36848;&#12290;</title><link>http://arxiv.org/abs/2308.12420</link><description>&lt;p&gt;
ESG&#20027;&#23548;&#30340;DLT&#30740;&#31350;&#30340;&#28436;&#21270;&#65306;&#23545;&#25991;&#29486;&#36827;&#34892;NLP&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Evolution of ESG-focused DLT Research: An NLP Analysis of the Literature. (arXiv:2308.12420v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12420
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;NLP&#20998;&#26512;&#20102;ESG&#20027;&#23548;&#30340;DLT&#30740;&#31350;&#30340;&#28436;&#21270;&#65292;&#36890;&#36807;&#26500;&#24314;&#24341;&#29992;&#32593;&#32476;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#65292;&#23545;DLT&#22312;ESG&#32972;&#26223;&#19979;&#30340;&#21457;&#23637;&#36827;&#34892;&#20102;&#25991;&#29486;&#32508;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#36134;&#26412;&#25216;&#26415;(DLT)&#36805;&#36895;&#21457;&#23637;&#65292;&#38656;&#35201;&#20840;&#38754;&#20102;&#35299;&#20854;&#21508;&#20010;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#38024;&#23545;DLT&#30340;&#29615;&#22659;&#12289;&#21487;&#25345;&#32493;&#24615;&#21644;&#27835;&#29702;(ESG)&#32452;&#25104;&#37096;&#20998;&#30340;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#36824;&#19981;&#36275;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#36873;&#25321;&#20102;107&#31687;&#31181;&#23376;&#25991;&#29486;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;63,083&#20010;&#21442;&#32771;&#25991;&#29486;&#30340;&#24341;&#29992;&#32593;&#32476;&#65292;&#24182;&#23558;&#20854;&#31934;&#28860;&#20026;24,539&#31687;&#25991;&#29486;&#30340;&#35821;&#26009;&#24211;&#36827;&#34892;&#20998;&#26512;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#26681;&#25454;&#19968;&#20010;&#24050;&#24314;&#31435;&#30340;&#25216;&#26415;&#20998;&#31867;&#27861;&#20174;46&#31687;&#35770;&#25991;&#20013;&#26631;&#35760;&#20102;&#21629;&#21517;&#23454;&#20307;&#65292;&#24182;&#36890;&#36807;&#25214;&#20986;DLT&#30340;ESG&#35201;&#32032;&#26469;&#23436;&#21892;&#36825;&#20010;&#20998;&#31867;&#27861;&#12290;&#21033;&#29992;&#22522;&#20110;transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#25105;&#20204;&#23545;&#19968;&#20010;&#39044;&#20808;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#32454;&#21270;&#35843;&#25972;&#65292;&#29992;&#20110;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#65292;&#20351;&#29992;&#25105;&#20204;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#21033;&#29992;&#25105;&#20204;&#35843;&#25972;&#21518;&#30340;&#35821;&#35328;&#27169;&#22411;&#23545;&#35821;&#26009;&#24211;&#36827;&#34892;&#20102;&#31934;&#31616;&#65292;&#24471;&#21040;&#20102;505&#31687;&#20851;&#38190;&#35770;&#25991;&#65292;&#36890;&#36807;&#21629;&#21517;&#23454;&#20307;&#21644;&#26102;&#38388;&#22270;&#20998;&#26512;&#65292;&#20419;&#36827;&#20102;&#23545;DLT&#22312;ESG&#32972;&#26223;&#19979;&#30340;&#28436;&#21270;&#30340;&#25991;&#29486;&#32508;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distributed Ledger Technologies (DLTs) have rapidly evolved, necessitating comprehensive insights into their diverse components. However, a systematic literature review that emphasizes the Environmental, Sustainability, and Governance (ESG) components of DLT remains lacking. To bridge this gap, we selected 107 seed papers to build a citation network of 63,083 references and refined it to a corpus of 24,539 publications for analysis. Then, we labeled the named entities in 46 papers according to twelve top-level categories derived from an established technology taxonomy and enhanced the taxonomy by pinpointing DLT's ESG elements. Leveraging transformer-based language models, we fine-tuned a pre-trained language model for a Named Entity Recognition (NER) task using our labeled dataset. We used our fine-tuned language model to distill the corpus to 505 key papers, facilitating a literature review via named entities and temporal graph analysis on DLT evolution in the context of ESG. Our con
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33021;&#22815;&#25552;&#39640;&#23545;ChatGPT&#29983;&#25104;&#30340;&#20551;&#31185;&#23398;&#36827;&#34892;&#26816;&#27979;&#30340;&#31639;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#26032;&#35774;&#35745;&#30340;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#20934;&#30830;&#22320;&#23558;&#26426;&#22120;&#29983;&#25104;&#30340;&#20986;&#29256;&#29289;&#19982;&#31185;&#23398;&#23478;&#29983;&#25104;&#30340;&#20986;&#29256;&#29289;&#21306;&#20998;&#24320;&#26469;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;ChatGPT&#22312;&#25216;&#26415;&#26415;&#35821;&#26041;&#38754;&#19982;&#30495;&#23454;&#31185;&#23398;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#31639;&#27861;&#22312;&#20998;&#31867;&#36807;&#31243;&#20013;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.11767</link><description>&lt;p&gt;
&#25552;&#39640;ChatGPT&#29983;&#25104;&#30340;&#20551;&#31185;&#23398;&#26816;&#27979;&#30340;&#26041;&#27861;&#65306;&#24341;&#20837;xFakeBibs&#30417;&#30563;&#23398;&#20064;&#32593;&#32476;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Improving Detection of ChatGPT-Generated Fake Science Using Real Publication Text: Introducing xFakeBibs a Supervised-Learning Network Algorithm. (arXiv:2308.11767v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11767
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33021;&#22815;&#25552;&#39640;&#23545;ChatGPT&#29983;&#25104;&#30340;&#20551;&#31185;&#23398;&#36827;&#34892;&#26816;&#27979;&#30340;&#31639;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#26032;&#35774;&#35745;&#30340;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#20934;&#30830;&#22320;&#23558;&#26426;&#22120;&#29983;&#25104;&#30340;&#20986;&#29256;&#29289;&#19982;&#31185;&#23398;&#23478;&#29983;&#25104;&#30340;&#20986;&#29256;&#29289;&#21306;&#20998;&#24320;&#26469;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;ChatGPT&#22312;&#25216;&#26415;&#26415;&#35821;&#26041;&#38754;&#19982;&#30495;&#23454;&#31185;&#23398;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#31639;&#27861;&#22312;&#20998;&#31867;&#36807;&#31243;&#20013;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#27491;&#22312;&#25104;&#20026;&#29616;&#23454;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#21306;&#20998;ChatGPT&#29983;&#25104;&#30340;&#20986;&#29256;&#29289;&#19982;&#31185;&#23398;&#23478;&#29983;&#25104;&#30340;&#20986;&#29256;&#29289;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#26032;&#35774;&#35745;&#30340;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#22914;&#20309;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#30340;&#20986;&#29256;&#29289;&#21644;&#31185;&#23398;&#23478;&#29983;&#25104;&#30340;&#20986;&#29256;&#29289;&#12290;&#35813;&#31639;&#27861;&#20351;&#29992;100&#20010;&#30495;&#23454;&#20986;&#29256;&#29289;&#25688;&#35201;&#36827;&#34892;&#35757;&#32451;&#65292;&#28982;&#21518;&#37319;&#29992;10&#20493;&#20132;&#21449;&#39564;&#35777;&#26041;&#27861;&#24314;&#31435;&#20102;&#19968;&#20010;&#25509;&#21463;&#33539;&#22260;&#30340;&#19979;&#38480;&#21644;&#19978;&#38480;&#12290;&#19982;ChatGPT&#20869;&#23481;&#36827;&#34892;&#27604;&#36739;&#65292;&#26126;&#26174;&#21487;&#35265;ChatGPT&#20165;&#36129;&#29486;&#20102;23\%&#30340;&#20108;&#20803;&#32452;&#20869;&#23481;&#65292;&#36825;&#27604;&#20854;&#20182;10&#20010;&#20132;&#21449;&#39564;&#35777;&#20013;&#30340;&#20219;&#20309;&#19968;&#20010;&#37117;&#23569;50\%&#12290;&#36825;&#20010;&#20998;&#26512;&#20984;&#26174;&#20102;ChatGPT&#22312;&#25216;&#26415;&#26415;&#35821;&#19978;&#19982;&#30495;&#23454;&#31185;&#23398;&#30340;&#26126;&#26174;&#24046;&#24322;&#12290;&#22312;&#23545;&#27599;&#31687;&#25991;&#31456;&#36827;&#34892;&#20998;&#31867;&#26102;&#65292;xFakeBibs&#31639;&#27861;&#20934;&#30830;&#22320;&#23558;98&#31687;&#20986;&#29256;&#29289;&#35782;&#21035;&#20026;&#20551;&#30340;&#65292;&#26377;2&#31687;&#25991;&#29486;&#38169;&#35823;&#22320;&#20998;&#31867;&#20026;&#30495;&#23454;&#20986;&#29256;&#29289;&#12290;&#23613;&#31649;&#36825;&#39033;&#24037;&#20316;&#24341;&#20837;&#20102;&#19968;&#31181;&#31639;&#27861;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
ChatGPT is becoming a new reality. In this paper, we show how to distinguish ChatGPT-generated publications from counterparts produced by scientists. Using a newly designed supervised Machine Learning algorithm, we demonstrate how to detect machine-generated publications from those produced by scientists. The algorithm was trained using 100 real publication abstracts, followed by a 10-fold calibration approach to establish a lower-upper bound range of acceptance. In the comparison with ChatGPT content, it was evident that ChatGPT contributed merely 23\% of the bigram content, which is less than 50\% of any of the other 10 calibrating folds. This analysis highlights a significant disparity in technical terms where ChatGPT fell short of matching real science. When categorizing the individual articles, the xFakeBibs algorithm accurately identified 98 out of 100 publications as fake, with 2 articles incorrectly classified as real publications. Though this work introduced an algorithmic app
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;InstructGLM&#30340;&#32467;&#26500;&#21270;&#35821;&#35328;&#27169;&#22411;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#22270;&#34920;&#23398;&#20064;&#38382;&#39064;&#30456;&#32467;&#21512;&#65292;&#26088;&#22312;&#25506;&#32034;&#26159;&#21542;&#21487;&#20197;&#29992;&#35821;&#35328;&#27169;&#22411;&#21462;&#20195;&#22270;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#22270;&#34920;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2308.07134</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#26159;&#22270;&#34920;&#25152;&#38656;&#35201;&#30340;&#20840;&#37096;&#20869;&#23481;
&lt;/p&gt;
&lt;p&gt;
Natural Language is All a Graph Needs. (arXiv:2308.07134v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07134
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;InstructGLM&#30340;&#32467;&#26500;&#21270;&#35821;&#35328;&#27169;&#22411;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#22270;&#34920;&#23398;&#20064;&#38382;&#39064;&#30456;&#32467;&#21512;&#65292;&#26088;&#22312;&#25506;&#32034;&#26159;&#21542;&#21487;&#20197;&#29992;&#35821;&#35328;&#27169;&#22411;&#21462;&#20195;&#22270;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#22270;&#34920;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#22914;ChatGPT&#65292;&#24050;&#32463;&#22312;&#20154;&#24037;&#26234;&#33021;&#30340;&#21508;&#20010;&#30740;&#31350;&#39046;&#22495;&#20013;&#24341;&#36215;&#20102;&#38761;&#21629;&#12290;&#22522;&#20110;Transformer&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36880;&#28176;&#21462;&#20195;&#20102;CNN&#21644;RNN&#65292;&#23558;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#32479;&#19968;&#36215;&#26469;&#12290;&#19982;&#30456;&#23545;&#29420;&#31435;&#23384;&#22312;&#30340;&#25968;&#25454;&#65288;&#22914;&#22270;&#20687;&#12289;&#35270;&#39057;&#25110;&#25991;&#26412;&#65289;&#30456;&#27604;&#65292;&#22270;&#34920;&#26159;&#19968;&#31181;&#21253;&#21547;&#20016;&#23500;&#32467;&#26500;&#21644;&#20851;&#31995;&#20449;&#24687;&#30340;&#25968;&#25454;&#31867;&#22411;&#12290;&#21516;&#26102;&#65292;&#20316;&#20026;&#26368;&#20855;&#34920;&#29616;&#21147;&#30340;&#23186;&#20171;&#20043;&#19968;&#65292;&#33258;&#28982;&#35821;&#35328;&#22312;&#25551;&#36848;&#22797;&#26434;&#32467;&#26500;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#23558;&#22270;&#34920;&#23398;&#20064;&#38382;&#39064;&#32435;&#20837;&#29983;&#25104;&#24335;&#35821;&#35328;&#24314;&#27169;&#26694;&#26550;&#30340;&#29616;&#26377;&#24037;&#20316;&#20173;&#28982;&#38750;&#24120;&#26377;&#38480;&#12290;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#19981;&#26029;&#22686;&#38271;&#65292;&#25506;&#32034;LLMs&#26159;&#21542;&#20063;&#21487;&#20197;&#26367;&#20195;GNNs&#25104;&#20026;&#22270;&#34920;&#30340;&#22522;&#30784;&#27169;&#22411;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;InstructGLM&#65288;&#32467;&#26500;&#21270;&#35821;&#35328;&#27169;&#22411;&#65289;&#31639;&#27861;&#65292;&#31995;&#32479;&#22320;&#35774;&#35745;&#39640;&#24230;&#21487;&#25193;&#23637;&#30340;&#27169;&#22411;&#26469;&#22788;&#29702;&#22270;&#34920;&#23398;&#20064;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of large-scale pre-trained language models, such as ChatGPT, has revolutionized various research fields in artificial intelligence. Transformers-based large language models (LLMs) have gradually replaced CNNs and RNNs to unify fields of computer vision and natural language processing. Compared with the data that exists relatively independently such as images, videos or texts, graph is a type of data that contains rich structural and relational information. Meanwhile, natural language, as one of the most expressive mediums, excels in describing complex structures. However, existing work on incorporating graph learning problems into the generative language modeling framework remains very limited. As the importance of large language models continues to grow, it becomes essential to explore whether LLMs can also replace GNNs as the foundation model for graphs. In this paper, we propose InstructGLM (Instruction-finetuned Graph Language Model), systematically design highly scal
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#23545;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#27861;&#24459;&#39046;&#22495;&#30340;&#20154;&#24037;&#26234;&#33021;&#38382;&#39064;&#21644;&#20219;&#21153;&#20013;&#30340;&#26041;&#27861;&#30340;&#31995;&#32479;&#27010;&#36848;&#12290;&#25991;&#31456;&#26088;&#22312;&#31361;&#20986;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#36827;&#23637;&#65292;&#20197;&#36827;&#19968;&#27493;&#20102;&#35299;Transformer&#22312;&#25903;&#25345;&#27861;&#24459;&#27969;&#31243;&#20013;&#30340;AI&#25104;&#21151;&#36129;&#29486;&#20197;&#21450;&#24403;&#21069;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.05502</link><description>&lt;p&gt;
&#23558;&#39034;&#24207;&#24102;&#20837;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#29992;&#20110;&#20154;&#24037;&#26234;&#33021;&#21644;&#27861;&#24459;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Bringing order into the realm of Transformer-based language models for artificial intelligence and law. (arXiv:2308.05502v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05502
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#23545;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#27861;&#24459;&#39046;&#22495;&#30340;&#20154;&#24037;&#26234;&#33021;&#38382;&#39064;&#21644;&#20219;&#21153;&#20013;&#30340;&#26041;&#27861;&#30340;&#31995;&#32479;&#27010;&#36848;&#12290;&#25991;&#31456;&#26088;&#22312;&#31361;&#20986;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#36827;&#23637;&#65292;&#20197;&#36827;&#19968;&#27493;&#20102;&#35299;Transformer&#22312;&#25903;&#25345;&#27861;&#24459;&#27969;&#31243;&#20013;&#30340;AI&#25104;&#21151;&#36129;&#29486;&#20197;&#21450;&#24403;&#21069;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;TLM&#65289;&#34987;&#24191;&#27867;&#35748;&#21487;&#26159;&#19968;&#31181;&#20808;&#36827;&#30340;&#25216;&#26415;&#65292;&#33021;&#22815;&#25104;&#21151;&#24320;&#21457;&#20986;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#38656;&#35201;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#29702;&#35299;&#30340;&#38382;&#39064;&#21644;&#24212;&#29992;&#12290;&#19982;&#20854;&#20182;&#25991;&#26412;&#39046;&#22495;&#19968;&#26679;&#65292;TLM&#30830;&#23454;&#25512;&#21160;&#20102;&#27861;&#24459;&#39046;&#22495;&#35768;&#22810;&#24863;&#20852;&#36259;&#20219;&#21153;&#23545;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#23613;&#31649;&#31532;&#19968;&#20010;Transformer&#27169;&#22411;&#25552;&#20986;&#20102;&#22823;&#32422;6&#24180;&#26102;&#38388;&#65292;&#20294;&#36825;&#39033;&#25216;&#26415;&#20197;&#21069;&#25152;&#26410;&#26377;&#30340;&#36895;&#24230;&#36805;&#29467;&#21457;&#23637;&#65292;BERT&#21644;&#30456;&#20851;&#27169;&#22411;&#25104;&#20026;&#20027;&#35201;&#21442;&#32771;&#65292;&#20063;&#22312;&#27861;&#24459;&#39046;&#22495;&#21344;&#26377;&#37325;&#35201;&#22320;&#20301;&#12290;&#26412;&#25991;&#39318;&#27425;&#31995;&#32479;&#27010;&#36848;&#20102;TLM&#22312;&#27861;&#24459;&#39046;&#22495;&#30340;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#38382;&#39064;&#21644;&#20219;&#21153;&#20013;&#30340;&#26041;&#27861;&#12290;&#19968;&#20010;&#20027;&#35201;&#30446;&#26631;&#26159;&#31361;&#20986;&#30740;&#31350;&#22312;&#36825;&#19968;&#39046;&#22495;&#30340;&#36827;&#23637;&#65292;&#20197;&#20415;&#19968;&#26041;&#38754;&#20102;&#35299;Transformer&#22312;&#25903;&#25345;&#27861;&#24459;&#27969;&#31243;&#20013;&#21462;&#24471;&#30340;AI&#25104;&#21151;&#36129;&#29486;&#26159;&#20160;&#20040;&#65292;&#21478;&#19968;&#26041;&#38754;&#20102;&#35299;&#24403;&#21069;&#30340;&#23616;&#38480;&#24615;&#26159;&#20160;&#20040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based language models (TLMs) have widely been recognized to be a cutting-edge technology for the successful development of deep-learning-based solutions to problems and applications that require natural language processing and understanding. Like for other textual domains, TLMs have indeed pushed the state-of-the-art of AI approaches for many tasks of interest in the legal domain. Despite the first Transformer model being proposed about six years ago, there has been a rapid progress of this technology at an unprecedented rate, whereby BERT and related models represent a major reference, also in the legal domain. This article provides the first systematic overview of TLM-based methods for AI-driven problems and tasks in the legal sphere. A major goal is to highlight research advances in this field so as to understand, on the one hand, how the Transformers have contributed to the success of AI in supporting legal processes, and on the other hand, what are the current limitati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#35805;&#24335;&#25628;&#32034;&#20013;&#29992;&#25143;&#21709;&#24212;&#27169;&#25311;&#30340;&#26041;&#27861;&#12290;&#24403;&#21069;&#30340;&#27169;&#25311;&#31995;&#32479;&#35201;&#20040;&#21482;&#33021;&#23545;&#26159;&#38750;&#38382;&#39064;&#36827;&#34892;&#22238;&#31572;&#65292;&#35201;&#20040;&#26080;&#27861;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#21709;&#24212;&#12290;&#36890;&#36807;&#29992;&#26356;&#23567;&#20294;&#20808;&#36827;&#30340;&#31995;&#32479;&#26367;&#25442;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#29992;&#25143;&#27169;&#25311;&#31995;&#32479;&#65292;&#33021;&#22815;&#26174;&#33879;&#25913;&#36827;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.07944</link><description>&lt;p&gt;
&#35770;&#29992;&#25143;&#21709;&#24212;&#27169;&#25311;&#22312;&#23545;&#35805;&#24335;&#25628;&#32034;&#20013;&#30340;&#28145;&#20837;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An In-depth Investigation of User Response Simulation for Conversational Search. (arXiv:2304.07944v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07944
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#35805;&#24335;&#25628;&#32034;&#20013;&#29992;&#25143;&#21709;&#24212;&#27169;&#25311;&#30340;&#26041;&#27861;&#12290;&#24403;&#21069;&#30340;&#27169;&#25311;&#31995;&#32479;&#35201;&#20040;&#21482;&#33021;&#23545;&#26159;&#38750;&#38382;&#39064;&#36827;&#34892;&#22238;&#31572;&#65292;&#35201;&#20040;&#26080;&#27861;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#21709;&#24212;&#12290;&#36890;&#36807;&#29992;&#26356;&#23567;&#20294;&#20808;&#36827;&#30340;&#31995;&#32479;&#26367;&#25442;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#29992;&#25143;&#27169;&#25311;&#31995;&#32479;&#65292;&#33021;&#22815;&#26174;&#33879;&#25913;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#24335;&#25628;&#32034;&#22312;&#20449;&#24687;&#26816;&#32034;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#23427;&#36890;&#36807;&#22810;&#27425;&#33258;&#28982;&#35821;&#35328;&#20132;&#20114;&#26469;&#28548;&#28165;&#21644;&#35299;&#20915;&#29992;&#25143;&#30340;&#25628;&#32034;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#31995;&#32479;&#26159;&#36890;&#36807;&#35760;&#24405;&#25110;&#20154;&#24037;&#23545;&#35805;&#26085;&#24535;&#36827;&#34892;&#35757;&#32451;&#21644;&#28436;&#31034;&#30340;&#12290;&#26368;&#32456;&#65292;&#23545;&#35805;&#24335;&#25628;&#32034;&#31995;&#32479;&#24212;&#35813;&#22312;&#26410;&#35265;&#36807;&#30340;&#23545;&#35805;&#36712;&#36857;&#30340;&#24320;&#25918;&#29615;&#22659;&#20013;&#36827;&#34892;&#35757;&#32451;&#12289;&#35780;&#20272;&#21644;&#37096;&#32626;&#12290;&#19968;&#20010;&#20851;&#38190;&#30340;&#25361;&#25112;&#26159;&#35757;&#32451;&#21644;&#35780;&#20272;&#36825;&#26679;&#30340;&#31995;&#32479;&#37117;&#38656;&#35201;&#20154;&#24037;&#21442;&#19982;&#65292;&#36825;&#26082;&#26114;&#36149;&#21448;&#19981;&#21487;&#25193;&#23637;&#12290;&#20854;&#20013;&#19968;&#31181;&#31574;&#30053;&#26159;&#27169;&#25311;&#29992;&#25143;&#65292;&#20197;&#27492;&#26469;&#20943;&#23569;&#25193;&#23637;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#29992;&#25143;&#27169;&#25311;&#22120;&#35201;&#20040;&#20165;&#38480;&#20110;&#23545;&#23545;&#35805;&#25628;&#32034;&#31995;&#32479;&#30340;&#26159;&#38750;&#38382;&#39064;&#36827;&#34892;&#22238;&#31572;&#65292;&#35201;&#20040;&#26080;&#27861;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#21709;&#24212;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#36890;&#36807;&#29992;&#19968;&#20010;&#26356;&#23567;&#20294;&#20808;&#36827;&#30340;&#31995;&#32479;&#26469;&#26367;&#25442;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#29992;&#25143;&#27169;&#25311;&#31995;&#32479;&#65292;&#33021;&#22815;&#22823;&#24133;&#25913;&#36827;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conversational search has seen increased recent attention in both the IR and NLP communities. It seeks to clarify and solve a user's search need through multi-turn natural language interactions. However, most existing systems are trained and demonstrated with recorded or artificial conversation logs. Eventually, conversational search systems should be trained, evaluated, and deployed in an open-ended setting with unseen conversation trajectories. A key challenge is that training and evaluating such systems both require a human-in-the-loop, which is expensive and does not scale. One strategy for this is to simulate users, thereby reducing the scaling costs. However, current user simulators are either limited to only respond to yes-no questions from the conversational search system, or unable to produce high quality responses in general.  In this paper, we show that current state-of-the-art user simulation system could be significantly improved by replacing it with a smaller but advanced
&lt;/p&gt;</description></item></channel></rss>