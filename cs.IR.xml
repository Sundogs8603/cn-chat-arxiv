<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#36890;&#36807;&#23545;11&#31181;&#25193;&#23637;&#25216;&#26415;&#12289;12&#20010;&#19981;&#21516;&#20998;&#24067;&#21464;&#21270;&#30340;&#25968;&#25454;&#38598;&#21644;24&#20010;&#26816;&#32034;&#27169;&#22411;&#30340;&#20840;&#38754;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#26597;&#35810;&#25110;&#25991;&#26723;&#25193;&#23637;&#30340;&#25928;&#26524;&#19982;&#26816;&#32034;&#22120;&#24615;&#33021;&#30456;&#20851;&#65292;&#23545;&#20110;&#24369;&#27169;&#22411;&#26469;&#35828;&#25193;&#23637;&#25552;&#39640;&#20102;&#20998;&#25968;&#65292;&#20294;&#23545;&#20110;&#24378;&#27169;&#22411;&#26469;&#35828;&#25193;&#23637;&#36890;&#24120;&#20250;&#25439;&#23475;&#20998;&#25968;&#12290;</title><link>http://arxiv.org/abs/2309.08541</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;&#26597;&#35810;&#21644;&#25991;&#26723;&#25193;&#23637;&#20309;&#26102;&#22833;&#36133;&#65311;&#26041;&#27861;&#12289;&#26816;&#32034;&#22120;&#21644;&#25968;&#25454;&#38598;&#30340;&#20840;&#38754;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
When do Generative Query and Document Expansions Fail? A Comprehensive Study Across Methods, Retrievers, and Datasets. (arXiv:2309.08541v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08541
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;11&#31181;&#25193;&#23637;&#25216;&#26415;&#12289;12&#20010;&#19981;&#21516;&#20998;&#24067;&#21464;&#21270;&#30340;&#25968;&#25454;&#38598;&#21644;24&#20010;&#26816;&#32034;&#27169;&#22411;&#30340;&#20840;&#38754;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#26597;&#35810;&#25110;&#25991;&#26723;&#25193;&#23637;&#30340;&#25928;&#26524;&#19982;&#26816;&#32034;&#22120;&#24615;&#33021;&#30456;&#20851;&#65292;&#23545;&#20110;&#24369;&#27169;&#22411;&#26469;&#35828;&#25193;&#23637;&#25552;&#39640;&#20102;&#20998;&#25968;&#65292;&#20294;&#23545;&#20110;&#24378;&#27169;&#22411;&#26469;&#35828;&#25193;&#23637;&#36890;&#24120;&#20250;&#25439;&#23475;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#36827;&#34892;&#26597;&#35810;&#25110;&#25991;&#26723;&#25193;&#23637;&#21487;&#20197;&#25913;&#21892;&#20449;&#24687;&#26816;&#32034;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#36825;&#20123;&#25216;&#26415;&#26159;&#21542;&#26222;&#36941;&#26377;&#30410;&#65292;&#36824;&#26159;&#20165;&#22312;&#29305;&#23450;&#35774;&#32622;&#19979;&#26377;&#25928;&#65292;&#20363;&#22914;&#23545;&#20110;&#29305;&#23450;&#30340;&#26816;&#32034;&#27169;&#22411;&#12289;&#25968;&#25454;&#38598;&#39046;&#22495;&#25110;&#26597;&#35810;&#31867;&#22411;&#12290;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#31532;&#19968;&#27425;&#23545;&#22522;&#20110;LM&#30340;&#25193;&#23637;&#30340;&#20840;&#38754;&#20998;&#26512;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#26816;&#32034;&#22120;&#24615;&#33021;&#19982;&#25193;&#23637;&#30340;&#22686;&#30410;&#20043;&#38388;&#23384;&#22312;&#24378;&#28872;&#30340;&#36127;&#30456;&#20851;&#20851;&#31995;&#65306;&#25193;&#23637;&#25913;&#21892;&#20102;&#36739;&#24369;&#27169;&#22411;&#30340;&#20998;&#25968;&#65292;&#20294;&#36890;&#24120;&#20250;&#25439;&#23475;&#36739;&#24378;&#27169;&#22411;&#30340;&#20998;&#25968;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#19968;&#36235;&#21183;&#22312;11&#31181;&#25193;&#23637;&#25216;&#26415;&#12289;12&#20010;&#20855;&#26377;&#19981;&#21516;&#20998;&#24067;&#21464;&#21270;&#30340;&#25968;&#25454;&#38598;&#21644;24&#20010;&#26816;&#32034;&#27169;&#22411;&#30340;&#19968;&#32452;&#23454;&#39564;&#20013;&#25104;&#31435;&#12290;&#36890;&#36807;&#23450;&#24615;&#38169;&#35823;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20551;&#35774;&#65292;&#21363;&#23613;&#31649;&#25193;&#23637;&#25552;&#20379;&#20102;&#39069;&#22806;&#30340;&#20449;&#24687;&#65288;&#21487;&#33021;&#25913;&#21892;&#20102;&#21484;&#22238;&#29575;&#65289;&#65292;&#20294;&#23427;&#20204;&#20063;&#22686;&#21152;&#20102;&#22122;&#22768;&#65292;&#20351;&#24471;&#24456;&#38590;&#21306;&#20998;&#20986;&#39030;&#32423;&#30456;&#20851;&#25991;&#26723;&#65288;&#20174;&#32780;&#24341;&#20837;&#20102;&#38169;&#35823;&#30340;&#27491;&#20363;&#65289;
&lt;/p&gt;
&lt;p&gt;
Using large language models (LMs) for query or document expansion can improve generalization in information retrieval. However, it is unknown whether these techniques are universally beneficial or only effective in specific settings, such as for particular retrieval models, dataset domains, or query types. To answer this, we conduct the first comprehensive analysis of LM-based expansion. We find that there exists a strong negative correlation between retriever performance and gains from expansion: expansion improves scores for weaker models, but generally harms stronger models. We show this trend holds across a set of eleven expansion techniques, twelve datasets with diverse distribution shifts, and twenty-four retrieval models. Through qualitative error analysis, we hypothesize that although expansions provide extra information (potentially improving recall), they add additional noise that makes it difficult to discern between the top relevant documents (thus introducing false positiv
&lt;/p&gt;</description></item><item><title>SilverRetriever&#26159;&#19968;&#20010;&#29305;&#20026;&#27874;&#20848;&#35821;&#38382;&#31572;&#31995;&#32479;&#24320;&#21457;&#30340;&#31070;&#32463;&#26816;&#32034;&#22120;&#65292;&#36890;&#36807;&#35757;&#32451;&#22312;&#22810;&#31181;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#25928;&#26524;&#65292;&#24182;&#19988;&#19982;&#26356;&#22823;&#30340;&#22810;&#35821;&#31181;&#27169;&#22411;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.08469</link><description>&lt;p&gt;
SilverRetriever&#65306;&#25552;&#21319;&#27874;&#20848;&#38382;&#31572;&#31995;&#32479;&#30340;&#31070;&#32463;&#36890;&#36947;&#26816;&#32034;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
SilverRetriever: Advancing Neural Passage Retrieval for Polish Question Answering. (arXiv:2309.08469v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08469
&lt;/p&gt;
&lt;p&gt;
SilverRetriever&#26159;&#19968;&#20010;&#29305;&#20026;&#27874;&#20848;&#35821;&#38382;&#31572;&#31995;&#32479;&#24320;&#21457;&#30340;&#31070;&#32463;&#26816;&#32034;&#22120;&#65292;&#36890;&#36807;&#35757;&#32451;&#22312;&#22810;&#31181;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#25928;&#26524;&#65292;&#24182;&#19988;&#19982;&#26356;&#22823;&#30340;&#22810;&#35821;&#31181;&#27169;&#22411;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#24320;&#25918;&#39046;&#22495;&#30340;&#38382;&#31572;&#31995;&#32479;&#36890;&#24120;&#20381;&#36182;&#20110;&#20934;&#30830;&#21644;&#39640;&#25928;&#30340;&#26816;&#32034;&#32452;&#20214;&#26469;&#25214;&#21040;&#21253;&#21547;&#22238;&#31572;&#38382;&#39064;&#25152;&#38656;&#20107;&#23454;&#30340;&#27573;&#33853;&#12290;&#36817;&#24180;&#26469;&#65292;&#30001;&#20110;&#20854;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#31070;&#32463;&#26816;&#32034;&#22120;&#27604;&#35789;&#27719;&#26367;&#20195;&#26041;&#24335;&#26356;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#22823;&#37096;&#20998;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#27969;&#34892;&#35821;&#35328;&#22914;&#33521;&#35821;&#25110;&#20013;&#25991;&#19978;&#65292;&#23545;&#20110;&#20854;&#20182;&#35821;&#35328;&#22914;&#27874;&#20848;&#35821;&#65292;&#21487;&#29992;&#30340;&#27169;&#22411;&#24456;&#23569;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;SilverRetriever&#65292;&#19968;&#20010;&#22522;&#20110;&#22810;&#31181;&#25163;&#21160;&#26631;&#35760;&#25110;&#24369;&#26631;&#35760;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#27874;&#20848;&#35821;&#31070;&#32463;&#26816;&#32034;&#22120;&#12290;SilverRetriever&#22312;&#27874;&#20848;&#35821;&#27169;&#22411;&#20013;&#21462;&#24471;&#20102;&#27604;&#20854;&#20182;&#27169;&#22411;&#26356;&#22909;&#30340;&#32467;&#26524;&#65292;&#24182;&#19982;&#26356;&#22823;&#30340;&#22810;&#35821;&#31181;&#27169;&#22411;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;&#19982;&#35813;&#27169;&#22411;&#19968;&#36215;&#65292;&#25105;&#20204;&#36824;&#24320;&#28304;&#20102;&#20116;&#20010;&#26032;&#30340;&#27573;&#33853;&#26816;&#32034;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern open-domain question answering systems often rely on accurate and efficient retrieval components to find passages containing the facts necessary to answer the question. Recently, neural retrievers have gained popularity over lexical alternatives due to their superior performance. However, most of the work concerns popular languages such as English or Chinese. For others, such as Polish, few models are available. In this work, we present SilverRetriever, a neural retriever for Polish trained on a diverse collection of manually or weakly labeled datasets. SilverRetriever achieves much better results than other Polish models and is competitive with larger multilingual models. Together with the model, we open-source five new passage retrieval datasets.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#21521;&#26377;&#35266;&#28857;&#30340;&#20154;&#35299;&#37322;&#25628;&#32034;&#32467;&#26524;&#31435;&#22330;&#30340;&#25928;&#26524;&#65292;&#21457;&#29616;&#31435;&#22330;&#26631;&#31614;&#21644;&#35299;&#37322;&#21487;&#20197;&#24110;&#21161;&#29992;&#25143;&#28040;&#36153;&#26356;&#22810;&#19981;&#21516;&#30340;&#25628;&#32034;&#32467;&#26524;&#65292;&#20294;&#27809;&#26377;&#21457;&#29616;&#31995;&#32479;&#24615;&#35266;&#28857;&#25913;&#21464;&#30340;&#35777;&#25454;&#12290;</title><link>http://arxiv.org/abs/2309.08460</link><description>&lt;p&gt;
&#21521;&#26377;&#35266;&#28857;&#30340;&#20154;&#35299;&#37322;&#25628;&#32034;&#32467;&#26524;&#31435;&#22330;
&lt;/p&gt;
&lt;p&gt;
Explaining Search Result Stances to Opinionated People. (arXiv:2309.08460v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08460
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#21521;&#26377;&#35266;&#28857;&#30340;&#20154;&#35299;&#37322;&#25628;&#32034;&#32467;&#26524;&#31435;&#22330;&#30340;&#25928;&#26524;&#65292;&#21457;&#29616;&#31435;&#22330;&#26631;&#31614;&#21644;&#35299;&#37322;&#21487;&#20197;&#24110;&#21161;&#29992;&#25143;&#28040;&#36153;&#26356;&#22810;&#19981;&#21516;&#30340;&#25628;&#32034;&#32467;&#26524;&#65292;&#20294;&#27809;&#26377;&#21457;&#29616;&#31995;&#32479;&#24615;&#35266;&#28857;&#25913;&#21464;&#30340;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20204;&#22312;&#24418;&#25104;&#35266;&#28857;&#20043;&#21069;&#20351;&#29992;&#32593;&#32476;&#25628;&#32034;&#24341;&#25806;&#25214;&#21040;&#20449;&#24687;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#20855;&#26377;&#19981;&#21516;&#24433;&#21709;&#27700;&#24179;&#30340;&#23454;&#38469;&#20915;&#31574;&#12290;&#25628;&#32034;&#30340;&#35748;&#30693;&#21162;&#21147;&#21487;&#33021;&#20351;&#26377;&#35266;&#28857;&#30340;&#29992;&#25143;&#23481;&#26131;&#21463;&#21040;&#35748;&#30693;&#20559;&#35265;&#30340;&#24433;&#21709;&#65292;&#20363;&#22914;&#30830;&#35748;&#20559;&#35265;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#31435;&#22330;&#26631;&#31614;&#21450;&#20854;&#35299;&#37322;&#26159;&#21542;&#21487;&#20197;&#24110;&#21161;&#29992;&#25143;&#28040;&#36153;&#26356;&#22810;&#19981;&#21516;&#30340;&#25628;&#32034;&#32467;&#26524;&#12290;&#25105;&#20204;&#33258;&#21160;&#23545;&#19977;&#20010;&#20027;&#39064;&#65288;&#30693;&#35782;&#20135;&#26435;&#12289;&#26657;&#26381;&#21644;&#26080;&#31070;&#35770;&#65289;&#30340;&#25628;&#32034;&#32467;&#26524;&#36827;&#34892;&#20998;&#31867;&#21644;&#26631;&#35760;&#65292;&#20998;&#20026;&#21453;&#23545;&#12289;&#20013;&#31435;&#21644;&#25903;&#25345;&#65292;&#24182;&#20026;&#36825;&#20123;&#26631;&#31614;&#29983;&#25104;&#35299;&#37322;&#12290;&#22312;&#19968;&#39033;&#29992;&#25143;&#30740;&#31350;&#20013;&#65288;N =203&#65289;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#25628;&#32034;&#32467;&#26524;&#31435;&#22330;&#20559;&#35265;&#65288;&#24179;&#34913; vs &#20559;&#35265;&#65289;&#21644;&#35299;&#37322;&#27700;&#24179;&#65288;&#32431;&#25991;&#26412;&#12289;&#20165;&#26631;&#31614;&#12289;&#26631;&#31614;&#21644;&#35299;&#37322;&#65289;&#26159;&#21542;&#20250;&#24433;&#21709;&#34987;&#28857;&#20987;&#30340;&#25628;&#32034;&#32467;&#26524;&#30340;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#31435;&#22330;&#26631;&#31614;&#21644;&#35299;&#37322;&#21487;&#20197;&#23548;&#33268;&#26356;&#22810;&#26679;&#21270;&#30340;&#25628;&#32034;&#32467;&#26524;&#28040;&#36153;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#24182;&#27809;&#26377;&#21457;&#29616;&#31995;&#32479;&#24615;&#35266;&#28857;&#25913;&#21464;&#30340;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
People use web search engines to find information before forming opinions, which can lead to practical decisions with different levels of impact. The cognitive effort of search can leave opinionated users vulnerable to cognitive biases, e.g., the confirmation bias. In this paper, we investigate whether stance labels and their explanations can help users consume more diverse search results. We automatically classify and label search results on three topics (i.e., intellectual property rights, school uniforms, and atheism) as against, neutral, and in favor, and generate explanations for these labels. In a user study (N =203), we then investigate whether search result stance bias (balanced vs biased) and the level of explanation (plain text, label only, label and explanation) influence the diversity of search results clicked. We find that stance labels and explanations lead to a more diverse search result consumption. However, we do not find evidence for systematic opinion change among us
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedDCSR&#30340;&#32852;&#37030;&#36328;&#39046;&#22495;&#39034;&#24207;&#25512;&#33616;&#26694;&#26550;&#65292;&#36890;&#36807;&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;&#26469;&#22788;&#29702;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#24207;&#21015;&#29305;&#24449;&#24322;&#36136;&#24615;&#65292;&#24182;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;</title><link>http://arxiv.org/abs/2309.08420</link><description>&lt;p&gt;
FedDCSR: &#36890;&#36807;&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;&#23454;&#29616;&#32852;&#37030;&#36328;&#39046;&#22495;&#39034;&#24207;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
FedDCSR: Federated Cross-domain Sequential Recommendation via Disentangled Representation Learning. (arXiv:2309.08420v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08420
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedDCSR&#30340;&#32852;&#37030;&#36328;&#39046;&#22495;&#39034;&#24207;&#25512;&#33616;&#26694;&#26550;&#65292;&#36890;&#36807;&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;&#26469;&#22788;&#29702;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#24207;&#21015;&#29305;&#24449;&#24322;&#36136;&#24615;&#65292;&#24182;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#21033;&#29992;&#26469;&#33258;&#22810;&#20010;&#39046;&#22495;&#30340;&#29992;&#25143;&#24207;&#21015;&#25968;&#25454;&#30340;&#36328;&#39046;&#22495;&#39034;&#24207;&#25512;&#33616;(CSR)&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;CSR&#26041;&#27861;&#38656;&#35201;&#22312;&#39046;&#22495;&#20043;&#38388;&#20849;&#20139;&#21407;&#22987;&#29992;&#25143;&#25968;&#25454;&#65292;&#36825;&#36829;&#21453;&#20102;&#12298;&#36890;&#29992;&#25968;&#25454;&#20445;&#25252;&#26465;&#20363;&#12299;(GDPR)&#12290;&#22240;&#27492;&#65292;&#26377;&#24517;&#35201;&#23558;&#32852;&#37030;&#23398;&#20064;(FL)&#21644;CSR&#30456;&#32467;&#21512;&#65292;&#20805;&#20998;&#21033;&#29992;&#19981;&#21516;&#39046;&#22495;&#30340;&#30693;&#35782;&#65292;&#21516;&#26102;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#24207;&#21015;&#29305;&#24449;&#24322;&#36136;&#24615;&#23545;FL&#30340;&#25972;&#20307;&#24615;&#33021;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FedDCSR&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#36807;&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;&#30340;&#26032;&#22411;&#32852;&#37030;&#36328;&#39046;&#22495;&#39034;&#24207;&#25512;&#33616;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20026;&#20102;&#35299;&#20915;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#24207;&#21015;&#29305;&#24449;&#24322;&#36136;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#39046;&#22495;&#20869;-&#39046;&#22495;&#38388;&#24207;&#21015;&#34920;&#31034;&#35299;&#32544;(SRD)&#30340;&#26041;&#27861;&#65292;&#23558;&#29992;&#25143;&#24207;&#21015;&#29305;&#24449;&#35299;&#32544;&#25104;&#39046;&#22495;&#20849;&#20139;&#21644;&#39046;&#22495;&#19987;&#23646;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-domain Sequential Recommendation (CSR) which leverages user sequence data from multiple domains has received extensive attention in recent years. However, the existing CSR methods require sharing origin user data across domains, which violates the General Data Protection Regulation (GDPR). Thus, it is necessary to combine federated learning (FL) and CSR to fully utilize knowledge from different domains while preserving data privacy. Nonetheless, the sequence feature heterogeneity across different domains significantly impacts the overall performance of FL. In this paper, we propose FedDCSR, a novel federated cross-domain sequential recommendation framework via disentangled representation learning. Specifically, to address the sequence feature heterogeneity across domains, we introduce an approach called inter-intra domain sequence representation disentanglement (SRD) to disentangle the user sequence features into domain-shared and domain-exclusive features. In addition, we design
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#26367;&#20195;BERT&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#30446;&#26631;&#65292;&#21253;&#25324;&#38543;&#26426;&#26631;&#35760;&#32622;&#25442;&#65288;RTS&#65289;&#12289;&#22522;&#20110;&#31751;&#30340;&#38543;&#26426;&#26631;&#35760;&#32622;&#25442;&#65288;C-RTS&#65289;&#21644;&#20132;&#25442;&#35821;&#35328;&#24314;&#27169;&#65288;SLM&#65289;&#65292;&#24182;&#19988;&#35777;&#26126;&#36825;&#20123;&#30446;&#26631;&#22312;&#20445;&#25345;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#38656;&#35201;&#26356;&#23569;&#30340;&#39044;&#35757;&#32451;&#26102;&#38388;&#12290;&#27492;&#22806;&#65292;&#26412;&#35770;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;&#19982;&#19979;&#28216;&#24212;&#29992;&#21305;&#37197;&#30340;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#20943;&#23569;&#20102;&#23545;&#26631;&#35760;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2309.08272</link><description>&lt;p&gt;
Transformer&#32467;&#26500;&#33258;&#30417;&#30563;&#30446;&#26631;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Structural Self-Supervised Objectives for Transformers. (arXiv:2309.08272v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08272
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#26367;&#20195;BERT&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#30446;&#26631;&#65292;&#21253;&#25324;&#38543;&#26426;&#26631;&#35760;&#32622;&#25442;&#65288;RTS&#65289;&#12289;&#22522;&#20110;&#31751;&#30340;&#38543;&#26426;&#26631;&#35760;&#32622;&#25442;&#65288;C-RTS&#65289;&#21644;&#20132;&#25442;&#35821;&#35328;&#24314;&#27169;&#65288;SLM&#65289;&#65292;&#24182;&#19988;&#35777;&#26126;&#36825;&#20123;&#30446;&#26631;&#22312;&#20445;&#25345;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#38656;&#35201;&#26356;&#23569;&#30340;&#39044;&#35757;&#32451;&#26102;&#38388;&#12290;&#27492;&#22806;&#65292;&#26412;&#35770;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;&#19982;&#19979;&#28216;&#24212;&#29992;&#21305;&#37197;&#30340;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#20943;&#23569;&#20102;&#23545;&#26631;&#35760;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#26080;&#30417;&#30563;&#21407;&#22987;&#25968;&#25454;&#25913;&#36827;&#33258;&#28982;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#65292;&#20351;&#20854;&#26356;&#39640;&#25928;&#19988;&#19982;&#19979;&#28216;&#24212;&#29992;&#26356;&#21152;&#19968;&#33268;&#12290;&#22312;&#31532;&#19968;&#37096;&#20998;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19977;&#20010;&#26367;&#20195;BERT&#30340;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#65288;MLM&#65289;&#30340;&#39044;&#35757;&#32451;&#30446;&#26631;&#65292;&#20998;&#21035;&#26159;&#38543;&#26426;&#26631;&#35760;&#32622;&#25442;&#65288;RTS&#65289;&#12289;&#22522;&#20110;&#31751;&#30340;&#38543;&#26426;&#26631;&#35760;&#32622;&#25442;&#65288;C-RTS&#65289;&#21644;&#20132;&#25442;&#35821;&#35328;&#24314;&#27169;&#65288;SLM&#65289;&#12290;&#36825;&#20123;&#30446;&#26631;&#28041;&#21450;&#21040;&#26631;&#35760;&#30340;&#20132;&#25442;&#32780;&#19981;&#26159;&#23631;&#34109;&#65292;&#20854;&#20013;RTS&#21644;C-RTS&#26088;&#22312;&#39044;&#27979;&#26631;&#35760;&#30340;&#21407;&#22987;&#24615;&#65292;&#32780;SLM&#21017;&#39044;&#27979;&#21407;&#22987;&#26631;&#35760;&#30340;&#20540;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;RTS&#21644;C-RTS&#38656;&#35201;&#26356;&#23569;&#30340;&#39044;&#35757;&#32451;&#26102;&#38388;&#65292;&#21516;&#26102;&#20445;&#25345;&#19982;MLM&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#23613;&#31649;&#20351;&#29992;&#20102;&#30456;&#21516;&#30340;&#35745;&#31639;&#39044;&#31639;&#65292;SLM&#22312;&#26576;&#20123;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;MLM&#12290;&#22312;&#31532;&#20108;&#37096;&#20998;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;&#19982;&#19979;&#28216;&#24212;&#29992;&#21305;&#37197;&#30340;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#23545;&#26631;&#35760;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#20351;&#29992;&#32500;&#22522;&#30334;&#31185;&#21644;CC-News&#31561;&#22823;&#22411;&#35821;&#26009;&#24211;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
This thesis focuses on improving the pre-training of natural language models using unsupervised raw data to make them more efficient and aligned with downstream applications.  In the first part, we introduce three alternative pre-training objectives to BERT's Masked Language Modeling (MLM), namely Random Token Substitution (RTS), Cluster-based Random Token Substitution (C-RTS), and Swapped Language Modeling (SLM). These objectives involve token swapping instead of masking, with RTS and C-RTS aiming to predict token originality and SLM predicting the original token values. Results show that RTS and C-RTS require less pre-training time while maintaining performance comparable to MLM. Surprisingly, SLM outperforms MLM on certain tasks despite using the same computational budget.  In the second part, we proposes self-supervised pre-training tasks that align structurally with downstream applications, reducing the need for labeled data. We use large corpora like Wikipedia and CC-News to trai
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22270;&#20687;&#26679;&#24335;&#32534;&#36753;&#23545;&#24191;&#21578;&#21560;&#24341;&#21147;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;StyleGAN&#30340;&#38754;&#37096;&#35821;&#20041;&#32534;&#36753;&#21644;&#21453;&#36716;&#65292;&#24182;&#32467;&#21512;&#20256;&#32479;&#30340;&#35270;&#35273;&#21644;&#25991;&#26412;&#29305;&#24449;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AdSEE&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#39044;&#27979;&#22312;&#32447;&#24191;&#21578;&#30340;&#28857;&#20987;&#29575;&#12290;&#36890;&#36807;&#23545;QQ-AD&#25968;&#25454;&#38598;&#30340;&#35780;&#20272;&#65292;&#39564;&#35777;&#20102;AdSEE&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.08159</link><description>&lt;p&gt;
AdSEE: &#30740;&#31350;&#22270;&#20687;&#26679;&#24335;&#32534;&#36753;&#23545;&#24191;&#21578;&#21560;&#24341;&#21147;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
AdSEE: Investigating the Impact of Image Style Editing on Advertisement Attractiveness. (arXiv:2309.08159v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08159
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22270;&#20687;&#26679;&#24335;&#32534;&#36753;&#23545;&#24191;&#21578;&#21560;&#24341;&#21147;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;StyleGAN&#30340;&#38754;&#37096;&#35821;&#20041;&#32534;&#36753;&#21644;&#21453;&#36716;&#65292;&#24182;&#32467;&#21512;&#20256;&#32479;&#30340;&#35270;&#35273;&#21644;&#25991;&#26412;&#29305;&#24449;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AdSEE&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#39044;&#27979;&#22312;&#32447;&#24191;&#21578;&#30340;&#28857;&#20987;&#29575;&#12290;&#36890;&#36807;&#23545;QQ-AD&#25968;&#25454;&#38598;&#30340;&#35780;&#20272;&#65292;&#39564;&#35777;&#20102;AdSEE&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30005;&#23376;&#21830;&#21153;&#32593;&#31449;&#12289;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#21644;&#25628;&#32034;&#24341;&#25806;&#20013;&#65292;&#22312;&#32447;&#24191;&#21578;&#26159;&#37325;&#35201;&#30340;&#20803;&#32032;&#12290;&#38543;&#30528;&#31227;&#21160;&#27983;&#35272;&#30340;&#26085;&#30410;&#27969;&#34892;&#65292;&#35768;&#22810;&#22312;&#32447;&#24191;&#21578;&#37117;&#36890;&#36807;&#23553;&#38754;&#22270;&#29255;&#20197;&#21450;&#25991;&#26412;&#25551;&#36848;&#26469;&#21560;&#24341;&#29992;&#25143;&#30340;&#27880;&#24847;&#21147;&#12290;&#26368;&#36817;&#30340;&#21508;&#31181;&#30740;&#31350;&#33268;&#21147;&#20110;&#36890;&#36807;&#32771;&#34385;&#35270;&#35273;&#29305;&#24449;&#26469;&#39044;&#27979;&#22312;&#32447;&#24191;&#21578;&#30340;&#28857;&#20987;&#29575;&#65292;&#25110;&#32773;&#36890;&#36807;&#32452;&#21512;&#26368;&#20339;&#30340;&#24191;&#21578;&#20803;&#32032;&#26469;&#22686;&#24378;&#21487;&#35265;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#24191;&#21578;&#26679;&#24335;&#32534;&#36753;&#21644;&#21560;&#24341;&#21147;&#22686;&#24378;&#65288;AdSEE&#65289;&#65292;&#25506;&#35752;&#20102;&#24191;&#21578;&#22270;&#20687;&#30340;&#35821;&#20041;&#32534;&#36753;&#26159;&#21542;&#20250;&#24433;&#21709;&#25110;&#25913;&#21464;&#22312;&#32447;&#24191;&#21578;&#30340;&#21463;&#27426;&#36814;&#31243;&#24230;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;StyleGAN&#30340;&#38754;&#37096;&#35821;&#20041;&#32534;&#36753;&#21644;&#21453;&#36716;&#65292;&#23545;&#24191;&#21578;&#22270;&#20687;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;GAN&#30340;&#38754;&#37096;&#28508;&#22312;&#34920;&#31034;&#20197;&#21450;&#20256;&#32479;&#30340;&#35270;&#35273;&#21644;&#25991;&#26412;&#29305;&#24449;&#26469;&#39044;&#27979;&#28857;&#20987;&#29575;&#12290;&#36890;&#36807;&#19968;&#20010;&#21517;&#20026;QQ-AD&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;20,527&#20010;&#26679;&#26412;&#65292;&#25105;&#20204;&#23545;AdSEE&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online advertisements are important elements in e-commerce sites, social media platforms, and search engines. With the increasing popularity of mobile browsing, many online ads are displayed with visual information in the form of a cover image in addition to text descriptions to grab the attention of users. Various recent studies have focused on predicting the click rates of online advertisements aware of visual features or composing optimal advertisement elements to enhance visibility. In this paper, we propose Advertisement Style Editing and Attractiveness Enhancement (AdSEE), which explores whether semantic editing to ads images can affect or alter the popularity of online advertisements. We introduce StyleGAN-based facial semantic editing and inversion to ads images and train a click rate predictor attributing GAN-based face latent representations in addition to traditional visual and textual features to click rates. Through a large collected dataset named QQ-AD, containing 20,527 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#22810;&#35270;&#22270;&#35270;&#35273;&#35821;&#20041;&#23884;&#20837;&#26694;&#26550;&#65292;&#22312;&#22270;&#20687;-&#25991;&#26412;&#26816;&#32034;&#20013;&#26377;&#25928;&#22320;&#21033;&#29992;&#35821;&#20041;&#20449;&#24687;&#36827;&#34892;&#30456;&#20284;&#24615;&#24230;&#37327;&#65292;&#36890;&#36807;&#24341;&#20837;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#25439;&#22833;&#20989;&#25968;&#65292;&#20805;&#20998;&#21033;&#29992;&#20108;&#36827;&#21046;&#26631;&#31614;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#23558;&#25972;&#20307;&#21305;&#37197;&#20998;&#35299;&#20026;&#22810;&#20010;&#35270;&#22270;-&#25991;&#26412;&#21305;&#37197;&#12290;</title><link>http://arxiv.org/abs/2309.08154</link><description>&lt;p&gt;
&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#22810;&#35270;&#22270;&#35270;&#35273;&#35821;&#20041;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Uncertainty-Aware Multi-View Visual Semantic Embedding. (arXiv:2309.08154v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08154
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#22810;&#35270;&#22270;&#35270;&#35273;&#35821;&#20041;&#23884;&#20837;&#26694;&#26550;&#65292;&#22312;&#22270;&#20687;-&#25991;&#26412;&#26816;&#32034;&#20013;&#26377;&#25928;&#22320;&#21033;&#29992;&#35821;&#20041;&#20449;&#24687;&#36827;&#34892;&#30456;&#20284;&#24615;&#24230;&#37327;&#65292;&#36890;&#36807;&#24341;&#20837;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#25439;&#22833;&#20989;&#25968;&#65292;&#20805;&#20998;&#21033;&#29992;&#20108;&#36827;&#21046;&#26631;&#31614;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#23558;&#25972;&#20307;&#21305;&#37197;&#20998;&#35299;&#20026;&#22810;&#20010;&#35270;&#22270;-&#25991;&#26412;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;-&#25991;&#26412;&#26816;&#32034;&#30340;&#20851;&#38190;&#25361;&#25112;&#26159;&#26377;&#25928;&#22320;&#21033;&#29992;&#35821;&#20041;&#20449;&#24687;&#26469;&#34913;&#37327;&#35270;&#35273;&#21644;&#35821;&#35328;&#25968;&#25454;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#23454;&#20363;&#32423;&#30340;&#20108;&#36827;&#21046;&#26631;&#31614;&#65292;&#20854;&#20013;&#27599;&#20010;&#22270;&#20687;&#19982;&#19968;&#20010;&#25991;&#26412;&#37197;&#23545;&#65292;&#26080;&#27861;&#25429;&#25417;&#19981;&#21516;&#35821;&#20041;&#21333;&#20803;&#20043;&#38388;&#30340;&#22810;&#20010;&#23545;&#24212;&#20851;&#31995;&#65292;&#20174;&#32780;&#23548;&#33268;&#22810;&#27169;&#24577;&#35821;&#20041;&#29702;&#35299;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;&#30740;&#31350;&#36890;&#36807;&#26356;&#22797;&#26434;&#30340;&#27169;&#22411;&#32467;&#26500;&#25110;&#39044;&#35757;&#32451;&#25216;&#26415;&#25429;&#25417;&#20102;&#32454;&#31890;&#24230;&#20449;&#24687;&#65292;&#20294;&#24456;&#23569;&#26377;&#30740;&#31350;&#30452;&#25509;&#24314;&#27169;&#23545;&#24212;&#20851;&#31995;&#30340;&#19981;&#30830;&#23450;&#24615;&#20197;&#20805;&#20998;&#21033;&#29992;&#20108;&#36827;&#21046;&#26631;&#31614;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#22810;&#35270;&#22270;&#35270;&#35273;&#35821;&#20041;&#23884;&#20837;&#65288;UAMVSE&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#23558;&#25972;&#20307;&#22270;&#20687;-&#25991;&#26412;&#21305;&#37197;&#20998;&#35299;&#20026;&#22810;&#20010;&#35270;&#22270;-&#25991;&#26412;&#21305;&#37197;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#24341;&#20837;&#20102;&#19968;&#31181;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#25439;&#22833;&#20989;&#25968;&#65288;UALoss&#65289;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#22320;&#24314;&#27169;&#27599;&#20010;&#35270;&#22270;-&#25991;&#26412;&#23545;&#24212;&#20851;&#31995;&#30340;&#19981;&#30830;&#23450;&#24615;&#26469;&#35745;&#31639;&#27599;&#20010;&#35270;&#22270;-&#25991;&#26412;&#25439;&#22833;&#30340;&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;
The key challenge in image-text retrieval is effectively leveraging semantic information to measure the similarity between vision and language data. However, using instance-level binary labels, where each image is paired with a single text, fails to capture multiple correspondences between different semantic units, leading to uncertainty in multi-modal semantic understanding. Although recent research has captured fine-grained information through more complex model structures or pre-training techniques, few studies have directly modeled uncertainty of correspondence to fully exploit binary labels. To address this issue, we propose an Uncertainty-Aware Multi-View Visual Semantic Embedding (UAMVSE)} framework that decomposes the overall image-text matching into multiple view-text matchings. Our framework introduce an uncertainty-aware loss function (UALoss) to compute the weighting of each view-text loss by adaptively modeling the uncertainty in each view-text correspondence. Different we
&lt;/p&gt;</description></item><item><title>iHAS&#26159;&#19968;&#20010;&#23454;&#20363;&#32423;&#23618;&#27425;&#32467;&#26500;&#25628;&#32034;&#30340;&#25512;&#33616;&#27169;&#22411;&#65292;&#36890;&#36807;&#33258;&#21160;&#21270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#25628;&#32034;&#65292;&#21487;&#20197;&#22312;&#27599;&#20010;&#23454;&#20363;&#32423;&#21035;&#19978;&#20026;&#19981;&#21516;&#31867;&#22411;&#30340;&#29305;&#24449;&#36873;&#25321;&#26368;&#20248;&#23884;&#20837;&#32500;&#24230;&#65292;&#24182;&#32771;&#34385;&#21040;&#23454;&#20363;&#20043;&#38388;&#30340;&#24322;&#36136;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.07967</link><description>&lt;p&gt;
iHAS: &#28145;&#24230;&#23398;&#20064;&#25512;&#33616;&#27169;&#22411;&#30340;&#23454;&#20363;&#32423;&#23618;&#27425;&#32467;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
iHAS: Instance-wise Hierarchical Architecture Search for Deep Learning Recommendation Models. (arXiv:2309.07967v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07967
&lt;/p&gt;
&lt;p&gt;
iHAS&#26159;&#19968;&#20010;&#23454;&#20363;&#32423;&#23618;&#27425;&#32467;&#26500;&#25628;&#32034;&#30340;&#25512;&#33616;&#27169;&#22411;&#65292;&#36890;&#36807;&#33258;&#21160;&#21270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#25628;&#32034;&#65292;&#21487;&#20197;&#22312;&#27599;&#20010;&#23454;&#20363;&#32423;&#21035;&#19978;&#20026;&#19981;&#21516;&#31867;&#22411;&#30340;&#29305;&#24449;&#36873;&#25321;&#26368;&#20248;&#23884;&#20837;&#32500;&#24230;&#65292;&#24182;&#32771;&#34385;&#21040;&#23454;&#20363;&#20043;&#38388;&#30340;&#24322;&#36136;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#25512;&#33616;&#31995;&#32479;&#20351;&#29992;&#20855;&#26377;&#32479;&#19968;&#32500;&#24230;&#30340;&#22823;&#22411;&#23884;&#20837;&#34920;&#65292;&#23548;&#33268;&#36807;&#25311;&#21512;&#12289;&#39640;&#35745;&#31639;&#25104;&#26412;&#21644;&#27425;&#20248;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#35768;&#22810;&#25216;&#26415;&#26088;&#22312;&#36890;&#36807;&#29305;&#24449;&#36873;&#25321;&#25110;&#23884;&#20837;&#32500;&#24230;&#25628;&#32034;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25216;&#26415;&#36890;&#24120;&#20026;&#25152;&#26377;&#23454;&#20363;&#36873;&#25321;&#19968;&#32452;&#22266;&#23450;&#30340;&#29305;&#24449;&#23376;&#38598;&#25110;&#23884;&#20837;&#32500;&#24230;&#65292;&#24182;&#23558;&#25152;&#26377;&#23454;&#20363;&#37117;&#36755;&#20837;&#21040;&#19968;&#20010;&#25512;&#33616;&#27169;&#22411;&#20013;&#65292;&#32780;&#19981;&#32771;&#34385;&#29289;&#21697;&#25110;&#29992;&#25143;&#20043;&#38388;&#30340;&#24322;&#36136;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23454;&#20363;&#32423;&#23618;&#27425;&#32467;&#26500;&#25628;&#32034;&#26694;&#26550;iHAS&#65292;&#23427;&#21487;&#20197;&#33258;&#21160;&#22312;&#23454;&#20363;&#32423;&#21035;&#19978;&#36827;&#34892;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#25628;&#32034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;iHAS&#21253;&#25324;&#19977;&#20010;&#38454;&#27573;:&#25628;&#32034;&#12289;&#32858;&#31867;&#21644;&#37325;&#35757;&#32451;&#12290;&#22312;&#25628;&#32034;&#38454;&#27573;&#65292;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#20271;&#21162;&#21033;&#38376;&#21644;&#27491;&#21017;&#21270;&#22120;&#65292;iHAS&#35782;&#21035;&#20986;&#19981;&#21516;&#23383;&#27573;&#29305;&#24449;&#19978;&#30340;&#26368;&#20248;&#23454;&#20363;&#32423;&#23884;&#20837;&#32500;&#24230;&#12290;&#22312;&#33719;&#24471;&#36825;&#20123;&#32500;&#24230;&#21518;&#65292;&#32858;&#31867;&#38454;&#27573;&#23558;&#23454;&#20363;&#20998;&#25104;&#19981;&#21516;&#30340;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current recommender systems employ large-sized embedding tables with uniform dimensions for all features, leading to overfitting, high computational cost, and suboptimal generalizing performance. Many techniques aim to solve this issue by feature selection or embedding dimension search. However, these techniques typically select a fixed subset of features or embedding dimensions for all instances and feed all instances into one recommender model without considering heterogeneity between items or users. This paper proposes a novel instance-wise Hierarchical Architecture Search framework, iHAS, which automates neural architecture search at the instance level. Specifically, iHAS incorporates three stages: searching, clustering, and retraining. The searching stage identifies optimal instance-wise embedding dimensions across different field features via carefully designed Bernoulli gates with stochastic selection and regularizers. After obtaining these dimensions, the clustering stage divid
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#24494;&#30340;&#26816;&#32034;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#24335;&#35821;&#35328;&#24314;&#27169;&#65292;&#22312;&#30005;&#23376;&#21830;&#21153;&#26597;&#35810;&#24847;&#22270;&#20998;&#31867;&#20219;&#21153;&#20013;&#26174;&#33879;&#25552;&#21319;&#20102;&#24615;&#33021;&#65292;&#35299;&#20915;&#20102;&#26816;&#32034;&#22120;&#21644;&#19979;&#28216;&#27169;&#22411;&#20043;&#38388;&#30340;&#19981;&#21487;&#24494;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.09308</link><description>&lt;p&gt;
&#21487;&#24494;&#26816;&#32034;&#22686;&#24378;&#36890;&#36807;&#29983;&#25104;&#24335;&#35821;&#35328;&#24314;&#27169;&#30340;&#30005;&#23376;&#21830;&#21153;&#26597;&#35810;&#24847;&#22270;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Differentiable Retrieval Augmentation via Generative Language Modeling for E-commerce Query Intent Classification. (arXiv:2308.09308v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09308
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#24494;&#30340;&#26816;&#32034;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#24335;&#35821;&#35328;&#24314;&#27169;&#65292;&#22312;&#30005;&#23376;&#21830;&#21153;&#26597;&#35810;&#24847;&#22270;&#20998;&#31867;&#20219;&#21153;&#20013;&#26174;&#33879;&#25552;&#21319;&#20102;&#24615;&#33021;&#65292;&#35299;&#20915;&#20102;&#26816;&#32034;&#22120;&#21644;&#19979;&#28216;&#27169;&#22411;&#20043;&#38388;&#30340;&#19981;&#21487;&#24494;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#36890;&#36807;&#20351;&#29992;&#30693;&#35782;&#26816;&#32034;&#22120;&#21644;&#22806;&#37096;&#35821;&#26009;&#24211;&#26469;&#22686;&#24378;&#19979;&#28216;&#27169;&#22411;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#22686;&#21152;&#27169;&#22411;&#21442;&#25968;&#30340;&#25968;&#37327;&#65292;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#65292;&#22914;&#25991;&#26412;&#20998;&#31867;&#12289;&#38382;&#39064;&#22238;&#31572;&#31561;&#26041;&#38754;&#24050;&#32463;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20004;&#20010;&#37096;&#20998;&#20043;&#38388;&#30340;&#19981;&#21487;&#24494;&#24615;&#65292;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#36890;&#36807;&#20998;&#21035;&#25110;&#24322;&#27493;&#35757;&#32451;&#26816;&#32034;&#22120;&#21644;&#19979;&#28216;&#27169;&#22411;&#26469;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#65292;&#19982;&#31471;&#21040;&#31471;&#32852;&#21512;&#35757;&#32451;&#30456;&#27604;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Differentiable Retrieval Augmentation via Generative lANguage modeling&#65288;Dragan&#65289;&#65292;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#24494;&#37325;&#26500;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#30005;&#23376;&#21830;&#21153;&#25628;&#32034;&#20013;&#30340;&#19968;&#20010;&#26377;&#25361;&#25112;&#24615;&#30340;NLP&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#21363;&#26597;&#35810;&#24847;&#22270;&#20998;&#31867;&#12290;&#23454;&#39564;&#32467;&#26524;&#21644;&#28040;&#34701;&#30740;&#31350;&#22343;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26174;&#33879;&#19988;&#21512;&#29702;&#22320;&#25913;&#36827;&#20102;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Retrieval augmentation, which enhances downstream models by a knowledge retriever and an external corpus instead of by merely increasing the number of model parameters, has been successfully applied to many natural language processing (NLP) tasks such as text classification, question answering and so on. However, existing methods that separately or asynchronously train the retriever and downstream model mainly due to the non-differentiability between the two parts, usually lead to degraded performance compared to end-to-end joint training. In this paper, we propose Differentiable Retrieval Augmentation via Generative lANguage modeling(Dragan), to address this problem by a novel differentiable reformulation. We demonstrate the effectiveness of our proposed method on a challenging NLP task in e-commerce search, namely query intent classification. Both the experimental results and ablation study show that the proposed method significantly and reasonably improves the state-of-the-art basel
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28857;&#20987;&#24863;&#30693;&#30340;&#32467;&#26500;&#36716;&#31227;&#21450;&#26679;&#26412;&#26435;&#37325;&#20998;&#37197;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#21518;&#28857;&#20987;&#36716;&#21270;&#29575;&#39044;&#27979;&#20013;&#30340;&#25968;&#25454;&#31232;&#30095;&#24615;&#38382;&#39064;&#21644;&#30693;&#35782;&#35781;&#21650;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.01169</link><description>&lt;p&gt;
&#28857;&#20987;&#24863;&#30693;&#30340;&#32467;&#26500;&#36716;&#31227;&#21450;&#26679;&#26412;&#26435;&#37325;&#20998;&#37197;&#22312;&#21518;&#28857;&#20987;&#36716;&#21270;&#29575;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Click-aware Structure Transfer with Sample Weight Assignment for Post-Click Conversion Rate Estimation. (arXiv:2304.01169v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01169
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28857;&#20987;&#24863;&#30693;&#30340;&#32467;&#26500;&#36716;&#31227;&#21450;&#26679;&#26412;&#26435;&#37325;&#20998;&#37197;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#21518;&#28857;&#20987;&#36716;&#21270;&#29575;&#39044;&#27979;&#20013;&#30340;&#25968;&#25454;&#31232;&#30095;&#24615;&#38382;&#39064;&#21644;&#30693;&#35782;&#35781;&#21650;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21518;&#28857;&#20987;&#36716;&#21270;&#29575;&#65288;CVR&#65289;&#39044;&#27979;&#22312;&#25512;&#33616;&#21644;&#24191;&#21578;&#31561;&#24037;&#19994;&#24212;&#29992;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#20256;&#32479;&#30340;CVR&#26041;&#27861;&#36890;&#24120;&#21463;&#21040;&#25968;&#25454;&#31232;&#30095;&#24615;&#38382;&#39064;&#30340;&#22256;&#25200;&#65292;&#22240;&#20026;&#23427;&#20204;&#20165;&#20381;&#36182;&#20110;&#29992;&#25143;&#28857;&#20987;&#30340;&#26679;&#26412;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#30740;&#31350;&#20154;&#21592;&#24341;&#20837;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#26410;&#28857;&#20987;&#26679;&#26412;&#24182;&#19982;CTR&#20219;&#21153;&#20849;&#20139;&#29305;&#24449;&#34920;&#31034;&#26469;&#36827;&#34892;CVR&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#38656;&#35201;&#27880;&#24847;&#30340;&#26159;&#65292;CVR&#21644;CTR&#20219;&#21153;&#22312;&#26412;&#36136;&#19978;&#26159;&#19981;&#21516;&#30340;&#65292;&#29978;&#33267;&#21487;&#33021;&#30456;&#20114;&#30683;&#30462;&#12290;&#22240;&#27492;&#65292;&#24341;&#20837;&#22823;&#37327;CTR&#20449;&#24687;&#32780;&#19981;&#21152;&#21306;&#20998;&#21487;&#33021;&#20250;&#28153;&#27809;&#19982;CVR&#30456;&#20851;&#30340;&#26377;&#20215;&#20540;&#20449;&#24687;&#12290;&#26412;&#25991;&#31216;&#27492;&#29616;&#35937;&#20026;&#30693;&#35782;&#35781;&#21650;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35748;&#20026;&#22312;&#24341;&#20837;&#22823;&#37327;&#36741;&#21161;&#20449;&#24687;&#21644;&#20445;&#25252;&#19982;CVR&#30456;&#20851;&#30340;&#26377;&#20215;&#20540;&#20449;&#24687;&#20043;&#38388;&#24212;&#35813;&#23454;&#29616;&#19968;&#31181;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Post-click Conversion Rate (CVR) prediction task plays an essential role in industrial applications, such as recommendation and advertising. Conventional CVR methods typically suffer from the data sparsity problem as they rely only on samples where the user has clicked. To address this problem, researchers have introduced the method of multi-task learning, which utilizes non-clicked samples and shares feature representations of the Click-Through Rate (CTR) task with the CVR task. However, it should be noted that the CVR and CTR tasks are fundamentally different and may even be contradictory. Therefore, introducing a large amount of CTR information without distinction may drown out valuable information related to CVR. This phenomenon is called the curse of knowledge problem in this paper. To tackle this issue, we argue that a trade-off should be achieved between the introduction of large amounts of auxiliary information and the protection of valuable information related to CVR. Hence, w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20559;&#24046;&#23884;&#20837;&#24335;&#20559;&#22909;&#27169;&#22411;&#8212;&#8212;Probe&#65292;&#26088;&#22312;&#35299;&#20915;&#29992;&#25143;&#22312;&#26102;&#38388;&#36328;&#24230;&#30340;&#36141;&#29289;&#36873;&#25321;&#20013;&#30340;&#25237;&#24433;&#20559;&#24046;&#21644;&#21442;&#29031;&#28857;&#25928;&#24212;&#65292;&#25552;&#39640;&#20915;&#31574;&#30340;&#26377;&#25928;&#24615;&#21644;&#20010;&#24615;&#21270;&#12290;</title><link>http://arxiv.org/abs/2303.06016</link><description>&lt;p&gt;
Probe&#65306;&#23398;&#20064;&#29992;&#25143;&#22312;&#26102;&#38388;&#36328;&#24230;&#30340;&#25414;&#32465;&#36873;&#25321;&#20013;&#30340;&#20010;&#24615;&#21270;&#25237;&#24433;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Probe: Learning Users' Personalized Projection Bias in Intertemporal Bundle Choices. (arXiv:2303.06016v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06016
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20559;&#24046;&#23884;&#20837;&#24335;&#20559;&#22909;&#27169;&#22411;&#8212;&#8212;Probe&#65292;&#26088;&#22312;&#35299;&#20915;&#29992;&#25143;&#22312;&#26102;&#38388;&#36328;&#24230;&#30340;&#36141;&#29289;&#36873;&#25321;&#20013;&#30340;&#25237;&#24433;&#20559;&#24046;&#21644;&#21442;&#29031;&#28857;&#25928;&#24212;&#65292;&#25552;&#39640;&#20915;&#31574;&#30340;&#26377;&#25928;&#24615;&#21644;&#20010;&#24615;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#36328;&#24230;&#30340;&#36873;&#25321;&#38656;&#35201;&#26435;&#34913;&#29616;&#22312;&#30340;&#25104;&#26412;&#21644;&#26410;&#26469;&#30340;&#25910;&#30410;&#12290;&#20854;&#20013;&#19968;&#31181;&#20855;&#20307;&#30340;&#36873;&#25321;&#26159;&#20915;&#23450;&#36141;&#20080;&#21333;&#20010;&#29289;&#21697;&#36824;&#26159;&#36873;&#25321;&#21253;&#21547;&#35813;&#29289;&#21697;&#30340;&#25414;&#32465;&#38144;&#21806;&#26041;&#24335;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#20551;&#35774;&#20010;&#20154;&#23545;&#36825;&#20123;&#36873;&#25321;&#20013;&#28041;&#21450;&#30340;&#22240;&#32032;&#26377;&#20934;&#30830;&#30340;&#26399;&#26395;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#20013;&#65292;&#29992;&#25143;&#23545;&#36825;&#20123;&#22240;&#32032;&#30340;&#24863;&#30693;&#24448;&#24448;&#23384;&#22312;&#20559;&#24046;&#65292;&#23548;&#33268;&#20102;&#38750;&#29702;&#24615;&#21644;&#27425;&#20248;&#30340;&#20915;&#31574;&#12290;&#26412;&#25991;&#37325;&#28857;&#20851;&#27880;&#20004;&#31181;&#24120;&#35265;&#30340;&#20559;&#24046;&#65306;&#25237;&#24433;&#20559;&#24046;&#21644;&#21442;&#29031;&#28857;&#25928;&#24212;&#65292;&#24182;&#20026;&#27492;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20559;&#24046;&#23884;&#20837;&#24335;&#20559;&#22909;&#27169;&#22411;&#8212;&#8212;Probe&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#21152;&#26435;&#20989;&#25968;&#26469;&#25429;&#25417;&#29992;&#25143;&#30340;&#25237;&#24433;&#20559;&#24046;&#65292;&#21033;&#29992;&#20215;&#20540;&#20989;&#25968;&#26469;&#32771;&#34385;&#21442;&#29031;&#28857;&#25928;&#24212;&#65292;&#24182;&#24341;&#20837;&#34892;&#20026;&#32463;&#27982;&#23398;&#20013;&#30340;&#21069;&#26223;&#29702;&#35770;&#26469;&#32452;&#21512;&#21152;&#26435;&#21644;&#20215;&#20540;&#20989;&#25968;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#30830;&#23450;&#29992;&#25143;&#36141;&#20080;&#25414;&#32465;&#38144;&#21806;&#30340;&#27010;&#29575;&#65292;&#20174;&#32780;&#25552;&#39640;&#20915;&#31574;&#30340;&#26377;&#25928;&#24615;&#21644;&#20010;&#24615;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intertemporal choices involve making decisions that require weighing the costs in the present against the benefits in the future. One specific type of intertemporal choice is the decision between purchasing an individual item or opting for a bundle that includes that item. Previous research assumes that individuals have accurate expectations of the factors involved in these choices. However, in reality, users' perceptions of these factors are often biased, leading to irrational and suboptimal decision-making. In this work, we specifically focus on two commonly observed biases: projection bias and the reference-point effect. To address these biases, we propose a novel bias-embedded preference model called Probe. The Probe incorporates a weight function to capture users' projection bias and a value function to account for the reference-point effect, and introduce prospect theory from behavioral economics to combine the weight and value functions. This allows us to determine the probabili
&lt;/p&gt;</description></item><item><title>SPEC5G&#26159;&#39318;&#20010;&#20844;&#20849;5G&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;5G&#34562;&#31389;&#32593;&#32476;&#21327;&#35758;&#30340;&#23433;&#20840;&#24615;&#20998;&#26512;&#21644;&#25991;&#26412;&#25688;&#35201;&#12290;</title><link>http://arxiv.org/abs/2301.09201</link><description>&lt;p&gt;
SPEC5G&#65306;&#29992;&#20110;5G&#34562;&#31389;&#32593;&#32476;&#21327;&#35758;&#20998;&#26512;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
SPEC5G: A Dataset for 5G Cellular Network Protocol Analysis. (arXiv:2301.09201v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.09201
&lt;/p&gt;
&lt;p&gt;
SPEC5G&#26159;&#39318;&#20010;&#20844;&#20849;5G&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;5G&#34562;&#31389;&#32593;&#32476;&#21327;&#35758;&#30340;&#23433;&#20840;&#24615;&#20998;&#26512;&#21644;&#25991;&#26412;&#25688;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
5G&#26159;&#31532;&#20116;&#20195;&#34562;&#31389;&#32593;&#32476;&#21327;&#35758;&#65292;&#26159;&#26368;&#20808;&#36827;&#30340;&#20840;&#29699;&#26080;&#32447;&#26631;&#20934;&#65292;&#33021;&#22815;&#20197;&#25552;&#39640;&#36895;&#24230;&#21644;&#38477;&#20302;&#24310;&#36831;&#30340;&#26041;&#24335;&#36830;&#25509;&#20960;&#20046;&#25152;&#26377;&#20154;&#21644;&#29289;&#12290;&#22240;&#27492;&#65292;&#20854;&#21457;&#23637;&#12289;&#20998;&#26512;&#21644;&#23433;&#20840;&#24615;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;5G&#21327;&#35758;&#30340;&#24320;&#21457;&#21644;&#23433;&#20840;&#20998;&#26512;&#26041;&#27861;&#37117;&#26159;&#23436;&#20840;&#25163;&#21160;&#30340;&#65292;&#27604;&#22914;&#23646;&#24615;&#25552;&#21462;&#12289;&#21327;&#35758;&#25688;&#35201;&#21644;&#21327;&#35758;&#35268;&#33539;&#21644;&#23454;&#29616;&#30340;&#35821;&#20041;&#20998;&#26512;&#12290;&#20026;&#20102;&#20943;&#23569;&#36825;&#31181;&#25163;&#21160;&#24037;&#20316;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;SPEC5G&#65292;&#36825;&#26159;&#39318;&#20010;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#30340;&#20844;&#20849;5G&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#26469;&#33258;13094&#20221;&#34562;&#31389;&#32593;&#32476;&#35268;&#33539;&#21644;13&#20010;&#32593;&#31449;&#30340;3,547,586&#20010;&#21477;&#23376;&#65292;&#24635;&#35745;134M&#20010;&#21333;&#35789;&#12290;&#36890;&#36807;&#21033;&#29992;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#21462;&#24471;&#26368;&#20808;&#36827;&#32467;&#26524;&#30340;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#25105;&#20204;&#20351;&#29992;&#36825;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#19982;&#23433;&#20840;&#30456;&#20851;&#30340;&#25991;&#26412;&#20998;&#31867;&#21644;&#25688;&#35201;&#12290;&#23433;&#20840;&#30456;&#20851;&#30340;&#25991;&#26412;&#20998;&#31867;&#21487;&#20197;
&lt;/p&gt;
&lt;p&gt;
5G is the 5th generation cellular network protocol. It is the state-of-the-art global wireless standard that enables an advanced kind of network designed to connect virtually everyone and everything with increased speed and reduced latency. Therefore, its development, analysis, and security are critical. However, all approaches to the 5G protocol development and security analysis, e.g., property extraction, protocol summarization, and semantic analysis of the protocol specifications and implementations are completely manual. To reduce such manual effort, in this paper, we curate SPEC5G the first-ever public 5G dataset for NLP research. The dataset contains 3,547,586 sentences with 134M words, from 13094 cellular network specifications and 13 online websites. By leveraging large-scale pre-trained language models that have achieved state-of-the-art results on NLP tasks, we use this dataset for security-related text classification and summarization. Security-related text classification ca
&lt;/p&gt;</description></item></channel></rss>