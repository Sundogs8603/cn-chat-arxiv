<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24773;&#24863;&#30456;&#20284;&#24615;&#26694;&#26550;&#26469;&#36827;&#34892;&#26080;&#35821;&#38899;&#25991;&#26412;&#30340;&#35821;&#38899;&#21040;&#38899;&#20048;&#26816;&#32034;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#36328;&#22495;&#26816;&#32034;&#31995;&#32479;&#26469;&#24357;&#21512;&#35821;&#38899;&#21644;&#38899;&#20048;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#35813;&#27169;&#22411;&#26159;&#26377;&#25928;&#30340;&#12290;</title><link>http://arxiv.org/abs/2303.10539</link><description>&lt;p&gt;
&#22522;&#20110;&#24773;&#24863;&#30456;&#20284;&#24615;&#30340;&#26080;&#35821;&#38899;&#25991;&#26412;&#30340;&#35821;&#38899;&#21040;&#38899;&#20048;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Textless Speech-to-Music Retrieval Using Emotion Similarity. (arXiv:2303.10539v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10539
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24773;&#24863;&#30456;&#20284;&#24615;&#26694;&#26550;&#26469;&#36827;&#34892;&#26080;&#35821;&#38899;&#25991;&#26412;&#30340;&#35821;&#38899;&#21040;&#38899;&#20048;&#26816;&#32034;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#36328;&#22495;&#26816;&#32034;&#31995;&#32479;&#26469;&#24357;&#21512;&#35821;&#38899;&#21644;&#38899;&#20048;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#35813;&#27169;&#22411;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#26681;&#25454;&#35821;&#38899;&#24773;&#32490;&#25512;&#33616;&#38899;&#20048;&#12290;&#22312;&#20869;&#23481;&#21019;&#24314;&#21644;&#26085;&#24120;&#29983;&#27963;&#20013;&#65292;&#35821;&#38899;&#21253;&#21547;&#26377;&#20851;&#20154;&#31867;&#24773;&#24863;&#30340;&#20449;&#24687;&#65292;&#36825;&#20123;&#20449;&#24687;&#21487;&#20197;&#36890;&#36807;&#38899;&#20048;&#26469;&#22686;&#24378;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20851;&#27880;&#36328;&#22495;&#26816;&#32034;&#31995;&#32479;&#65292;&#36890;&#36807;&#24773;&#24863;&#26631;&#31614;&#26469;&#24357;&#21512;&#35821;&#38899;&#21644;&#38899;&#20048;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#19981;&#21516;&#30340;&#35821;&#38899;&#34920;&#31034;&#65292;&#24182;&#25253;&#21578;&#20102;&#23427;&#20204;&#23545;&#19981;&#21516;&#35821;&#38899;&#31867;&#22411;&#65288;&#21253;&#25324;&#34920;&#28436;&#35821;&#38899;&#21644;&#21796;&#37266;&#35789;&#65289;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36824;&#22312;&#36328;&#22495;&#26816;&#32034;&#20219;&#21153;&#20013;&#25552;&#20986;&#20102;&#24773;&#24863;&#30456;&#20284;&#24615;&#27491;&#21017;&#21270;&#39033;&#12290;&#36890;&#36807;&#23558;&#27491;&#21017;&#21270;&#39033;&#32435;&#20837;&#35757;&#32451;&#20013;&#65292;&#24773;&#24863;&#31354;&#38388;&#20013;&#30456;&#20284;&#30340;&#35821;&#38899;&#21644;&#38899;&#20048;&#23545;&#22312;&#32852;&#21512;&#23884;&#20837;&#31354;&#38388;&#20013;&#26356;&#21152;&#25509;&#36817;&#12290;&#25105;&#20204;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#23545;&#20110;&#25991;&#26412;&#26080;&#20851;&#30340;&#35821;&#38899;&#21040;&#38899;&#20048;&#26816;&#32034;&#38750;&#24120;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a framework that recommends music based on the emotions of speech. In content creation and daily life, speech contains information about human emotions, which can be enhanced by music. Our framework focuses on a cross-domain retrieval system to bridge the gap between speech and music via emotion labels. We explore different speech representations and report their impact on different speech types, including acting voice and wake-up words. We also propose an emotion similarity regularization term in cross-domain retrieval tasks. By incorporating the regularization term into training, similar speech-and-music pairs in the emotion space are closer in the joint embedding space. Our comprehensive experimental results show that the proposed model is effective in textless speech-to-music retrieval.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26234;&#33021;&#38899;&#31665;&#20013;&#20351;&#29992;&#23545;&#35805;&#24335;&#25506;&#32034;&#24615;&#25628;&#32034;&#30340;&#28508;&#21147;&#65292;&#24182;&#20351;&#29992;&#25193;&#23637;Alexa&#30340;&#26041;&#27861;&#20811;&#26381;&#20854;&#30701;&#22788;&#65292;&#23454;&#29616;&#20102;&#25506;&#32034;&#24615;&#25628;&#32034;&#12290;</title><link>http://arxiv.org/abs/2303.10497</link><description>&lt;p&gt;
&#20351;&#29992;&#26234;&#33021;&#38899;&#31665;&#25968;&#23383;&#21161;&#25163;&#36827;&#34892;&#23545;&#35805;&#24335;&#25506;&#32034;&#24615;&#25628;&#32034;&#30340;&#28508;&#21147;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Examining the Potential for Conversational Exploratory Search using a Smart Speaker Digital Assistant. (arXiv:2303.10497v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10497
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26234;&#33021;&#38899;&#31665;&#20013;&#20351;&#29992;&#23545;&#35805;&#24335;&#25506;&#32034;&#24615;&#25628;&#32034;&#30340;&#28508;&#21147;&#65292;&#24182;&#20351;&#29992;&#25193;&#23637;Alexa&#30340;&#26041;&#27861;&#20811;&#26381;&#20854;&#30701;&#22788;&#65292;&#23454;&#29616;&#20102;&#25506;&#32034;&#24615;&#25628;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#25968;&#23383;&#21161;&#25163;&#65288;&#22914;Amazon Alexa&#65292;Google Assistant&#65292;Apple Siri&#65289;&#38750;&#24120;&#27969;&#34892;&#65292;&#24182;&#20026;&#20854;&#29992;&#25143;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#26381;&#21153;&#12290;&#20854;&#20851;&#38190;&#21151;&#33021;&#26159;&#33021;&#22815;&#28385;&#36275;&#29992;&#25143;&#20449;&#24687;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#34429;&#28982;&#36825;&#20123;&#24212;&#29992;&#31243;&#24207;&#36890;&#24120;&#33021;&#22815;&#26377;&#25928;&#22238;&#31572;&#20107;&#23454;&#38382;&#39064;&#65292;&#20294;&#25903;&#25345;&#36739;&#19981;&#20855;&#20307;&#25110;&#25506;&#32034;&#24615;&#30340;&#25628;&#32034;&#20219;&#21153;&#30340;&#33021;&#21147;&#21364;&#19981;&#37027;&#20040;&#26126;&#26174;&#12290;&#26412;&#30740;&#31350;&#23545;&#26631;&#20934;&#30340;Amazon Alexa&#36827;&#34892;&#20102;&#25506;&#32034;&#24615;&#25628;&#32034;&#20219;&#21153;&#30340;&#34892;&#20026;&#20998;&#26512;&#65292;&#24182;&#19988;&#32467;&#26524;&#34920;&#26126;&#20854;&#19981;&#33021;&#26377;&#25928;&#22320;&#28385;&#36275;&#36825;&#20123;&#20449;&#24687;&#38656;&#27714;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#25193;&#23637;Alexa&#30340;&#26041;&#27861;&#65292;&#20197;&#20811;&#26381;&#36825;&#20123;&#19981;&#36275;&#12290;&#25105;&#20204;&#30340;&#23450;&#21046;Alexa&#24212;&#29992;&#31243;&#24207;&#25193;&#23637;&#20102;Alexa&#30340;&#23545;&#35805;&#21151;&#33021;&#65292;&#20197;&#25903;&#25345;&#25506;&#32034;&#24615;&#25628;&#32034;&#65292;&#29992;&#25143;&#30740;&#31350;&#34920;&#26126;&#25105;&#20204;&#30340;&#25193;&#23637;&#24212;&#29992;&#31243;&#24207;&#33021;&#22815;&#26377;&#25928;&#22320;&#23454;&#29616;&#25506;&#32034;&#24615;&#25628;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online Digital Assistants, such as Amazon Alexa, Google Assistant, Apple Siri are very popular and provide a range or services to their users, a key function is their ability to satisfy user information needs from the sources available to them. Users may often regard these applications as providing search services similar to Google type search engines. However, while it is clear that they are in general able to answer factoid questions effectively, it is much less obvious how well they support less specific or exploratory type search tasks. We describe an investigation examining the behaviour of the standard Amazon Alexa for exploratory search tasks. The results of our study show that it not effective in addressing these types of information needs. We propose extensions to Alexa designed to overcome these shortcomings. Our Custom Alexa application extends Alexa's conversational functionality for exploratory search. A user study shows that our extended Alexa application both enables use
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;DFT&#25991;&#26723;&#30340;&#24341;&#29992;&#32858;&#31867;&#65292;&#20351;&#29992;&#20027;&#39064;&#24314;&#27169;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#23545;&#20986;&#29256;&#29289;&#36827;&#34892;&#20998;&#31867;&#65292;&#21487;&#36866;&#29992;&#20110;&#20854;&#20182;&#23398;&#31185;&#65292;&#24182;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#32452;&#32455;&#21644;&#35775;&#38382;&#30456;&#20851;&#30340;&#31185;&#23398;&#35770;&#25991;&#12290;</title><link>http://arxiv.org/abs/2303.10239</link><description>&lt;p&gt;
&#22522;&#20110;&#24341;&#29992;&#32858;&#31867;&#30340;&#23494;&#24230;&#27867;&#20989;&#29702;&#35770;&#20027;&#39064;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Topic Modeling in Density Functional Theory on Citations of Condensed Matter Electronic Structure Packages. (arXiv:2303.10239v1 [cond-mat.other])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10239
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;DFT&#25991;&#26723;&#30340;&#24341;&#29992;&#32858;&#31867;&#65292;&#20351;&#29992;&#20027;&#39064;&#24314;&#27169;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#23545;&#20986;&#29256;&#29289;&#36827;&#34892;&#20998;&#31867;&#65292;&#21487;&#36866;&#29992;&#20110;&#20854;&#20182;&#23398;&#31185;&#65292;&#24182;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#32452;&#32455;&#21644;&#35775;&#38382;&#30456;&#20851;&#30340;&#31185;&#23398;&#35770;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36234;&#26469;&#36234;&#22810;&#30340;&#31185;&#23398;&#35770;&#25991;&#21457;&#24067;&#65292;&#30740;&#31350;&#20154;&#21592;&#35201;&#24847;&#35782;&#21040;&#20182;&#20204;&#25152;&#30740;&#31350;&#39046;&#22495;&#20013;&#30340;&#26368;&#26032;&#25991;&#31456;&#21464;&#24471;&#26356;&#21152;&#22256;&#38590;&#12290;&#31934;&#30830;&#22320;&#23545;&#35770;&#25991;&#36827;&#34892;&#20998;&#31867;&#26159;&#20010;&#24615;&#21270;&#23450;&#21046;&#20197;&#21450;&#26131;&#20110;&#35775;&#38382;&#24863;&#20852;&#36259;&#30740;&#31350;&#30340;&#31532;&#19968;&#27493;&#12290; &#29305;&#21035;&#26159;&#23494;&#24230;&#27867;&#20989;&#29702;&#35770;&#65288;DFT&#65289;&#39046;&#22495;&#26159;&#19968;&#20010;&#24456;&#22909;&#30340;&#20363;&#23376;&#65292;&#29992;&#20110;&#19981;&#21516;&#30340;&#30740;&#31350;&#21644;&#30456;&#20114;&#20851;&#32852;&#30340;&#23398;&#31185;&#65292;&#24182;&#19988;&#20855;&#26377;&#38750;&#24120;&#24378;&#22823;&#30340;&#31038;&#21306;&#20986;&#29256;&#35768;&#22810;&#30740;&#31350;&#25991;&#31456;&#12290; &#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#26469;&#23545;&#20986;&#29256;&#29289;&#36827;&#34892;&#20998;&#31867;&#65292;&#22522;&#20110;&#20027;&#39064;&#24314;&#27169;&#65292;&#20351;&#29992;DFT&#30456;&#20851;&#30340;&#25991;&#26723;&#20316;&#20026;&#29992;&#20363;&#12290; &#25105;&#20204;&#39318;&#20808;&#20174;&#20986;&#29256;&#29289;&#30340;&#25688;&#35201;&#20013;&#36827;&#34892;&#21333;&#35789;&#20998;&#26512;&#21644;&#32858;&#31867;&#26469;&#21019;&#24314;&#20027;&#39064;&#65292;&#28982;&#21518;&#26681;&#25454;&#21333;&#35789;&#30456;&#20284;&#24615;&#23558;&#27599;&#20010;&#20986;&#29256;&#29289;/&#35770;&#25991;&#24402;&#23646;&#20110;&#26576;&#19968;&#20027;&#39064;&#12290; &#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;&#20027;&#39064;&#21644;&#20986;&#29256;&#21830;&#12289;&#26399;&#21002;&#12289;&#22269;&#23478;&#25110;&#20986;&#29256;&#24180;&#20221;&#20043;&#38388;&#30340;&#32852;&#31995;&#26469;&#20316;&#20986;&#26377;&#36259;&#30340;&#35266;&#23519;&#12290; &#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#36866;&#29992;&#20110;DFT&#33539;&#22260;&#20043;&#22806;&#30340;&#20854;&#20182;&#23398;&#31185;&#65292;&#24182;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#32452;&#32455;&#21644;&#35775;&#38382;&#30456;&#20851;&#30340;&#31185;&#23398;&#35770;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;
With an increasing number of new scientific papers being released, it becomes harder for researchers to be aware of recent articles in their field of study. Accurately classifying papers is a first step in the direction of personalized catering and easy access to research of interest. The field of Density Functional Theory (DFT) in particular is a good example of a methodology used in very different studies, and interconnected disciplines, which has a very strong community publishing many research articles. We devise a new unsupervised method for classifying publications, based on topic modeling, and use a DFT-related selection of documents as a use case. We first create topics from word analysis and clustering of the abstracts from the publications, then attribute each publication/paper to a topic based on word similarity. We then make interesting observations by analyzing connections between the topics and publishers, journals, country or year of publication. The proposed approach is
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21457;&#24067;&#20102;&#19968;&#20010;&#25945;&#32946;&#25512;&#33616;&#31995;&#32479;&#30340;&#24320;&#25918;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#20256;&#32479;&#35780;&#20998;&#26465;&#30446;&#21644;&#20016;&#23500;&#30340;&#20449;&#24687;&#65292;&#20026;&#24320;&#21457;&#21644;&#26816;&#39564;&#21508;&#31181;&#25945;&#32946;&#25512;&#33616;&#31995;&#32479;&#25552;&#20379;&#20102;&#26356;&#22810;&#26426;&#20250;&#12290;</title><link>http://arxiv.org/abs/2303.10230</link><description>&lt;p&gt;
ITM-Rec&#65306;&#25945;&#32946;&#25512;&#33616;&#31995;&#32479;&#30340;&#24320;&#25918;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
ITM-Rec: An Open Data Set for Educational Recommender Systems. (arXiv:2303.10230v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10230
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21457;&#24067;&#20102;&#19968;&#20010;&#25945;&#32946;&#25512;&#33616;&#31995;&#32479;&#30340;&#24320;&#25918;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#20256;&#32479;&#35780;&#20998;&#26465;&#30446;&#21644;&#20016;&#23500;&#30340;&#20449;&#24687;&#65292;&#20026;&#24320;&#21457;&#21644;&#26816;&#39564;&#21508;&#31181;&#25945;&#32946;&#25512;&#33616;&#31995;&#32479;&#25552;&#20379;&#20102;&#26356;&#22810;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25512;&#33616;&#31995;&#32479;&#30340;&#21457;&#23637;&#65292;&#20986;&#29616;&#20102;&#19968;&#20123;&#26377;&#21069;&#36884;&#30340;&#31995;&#32479;&#65292;&#22914;&#19978;&#19979;&#25991;&#24863;&#30693;&#25512;&#33616;&#31995;&#32479;&#65292;&#22810;&#20934;&#21017;&#25512;&#33616;&#31995;&#32479;&#21644;&#32676;&#32452;&#25512;&#33616;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#25945;&#32946;&#39046;&#22495;&#21487;&#33021;&#26080;&#27861;&#20174;&#36825;&#20123;&#21457;&#23637;&#20013;&#21463;&#30410;&#65292;&#22240;&#20026;&#25945;&#32946;&#25968;&#25454;&#38598;&#20013;&#32570;&#20047;&#19978;&#19979;&#25991;&#21644;&#22810;&#20010;&#20934;&#21017;&#31561;&#32570;&#22833;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#19968;&#20010;&#25945;&#32946;&#25512;&#33616;&#31995;&#32479;&#30340;&#24320;&#25918;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#19981;&#20165;&#21253;&#25324;&#20256;&#32479;&#30340;&#35780;&#20998;&#26465;&#30446;&#65292;&#36824;&#21253;&#25324;&#20016;&#23500;&#30340;&#20449;&#24687;&#65292;&#20363;&#22914;&#19978;&#19979;&#25991;&#12289;&#29992;&#25143;&#22810;&#20934;&#21017;&#20559;&#22909;&#12289;&#32676;&#32452;&#32452;&#25104;&#21644;&#20559;&#22909;&#31561;&#12290;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#27979;&#35797;&#24179;&#21488;&#65292;&#20026;&#24320;&#21457;&#21644;&#26816;&#39564;&#21508;&#31181;&#25945;&#32946;&#25512;&#33616;&#31995;&#32479;&#25552;&#20379;&#20102;&#26356;&#22810;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the development of recommender systems (RS), several promising systems have emerged, such as context-aware RS, multi-criteria RS, and group RS. However, the education domain may not benefit from these developments due to missing information, such as contexts and multiple criteria, in educational data sets. In this paper, we announce and release an open data set for educational recommender systems. This data set includes not only traditional rating entries, but also enriched information, e.g., contexts, user preferences in multiple criteria, group compositions and preferences, etc. It provides a testbed and enables more opportunities to develop and examine various educational recommender systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#22270;&#30340;&#21327;&#21516;&#36807;&#28388;&#27169;&#22411;SimRec&#65292;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#21644;&#23545;&#27604;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#25945;&#24072;GNN&#27169;&#22411;&#19982;&#36731;&#37327;&#32423;&#23398;&#29983;&#32593;&#32476;&#20043;&#38388;&#30340;&#33258;&#36866;&#24212;&#30693;&#35782;&#36716;&#31227;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#29616;&#26377;&#22522;&#20110;GNN&#30340;CF&#27169;&#22411;&#21487;&#33021;&#20986;&#29616;&#30340;&#36807;&#24230;&#24179;&#28369;&#21644;&#22122;&#22768;&#25928;&#24212;&#30340;&#38382;&#39064;&#65292;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#37117;&#36229;&#36807;&#20102;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.08537</link><description>&lt;p&gt;
&#26080;&#22270;&#21327;&#21516;&#36807;&#28388;
&lt;/p&gt;
&lt;p&gt;
Graph-less Collaborative Filtering. (arXiv:2303.08537v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08537
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#22270;&#30340;&#21327;&#21516;&#36807;&#28388;&#27169;&#22411;SimRec&#65292;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#21644;&#23545;&#27604;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#25945;&#24072;GNN&#27169;&#22411;&#19982;&#36731;&#37327;&#32423;&#23398;&#29983;&#32593;&#32476;&#20043;&#38388;&#30340;&#33258;&#36866;&#24212;&#30693;&#35782;&#36716;&#31227;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#29616;&#26377;&#22522;&#20110;GNN&#30340;CF&#27169;&#22411;&#21487;&#33021;&#20986;&#29616;&#30340;&#36807;&#24230;&#24179;&#28369;&#21644;&#22122;&#22768;&#25928;&#24212;&#30340;&#38382;&#39064;&#65292;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#37117;&#36229;&#36807;&#20102;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#24050;&#32463;&#22312;&#21327;&#21516;&#36807;&#28388;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20102;&#20854;&#22312;&#22270;&#32467;&#26500;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#25968;&#25454;&#19978;&#34920;&#31034;&#23398;&#20064;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20302;&#36890;Laplacian&#24179;&#28369;&#31639;&#23376;&#30340;&#36807;&#24230;&#24179;&#28369;&#21644;&#22122;&#22768;&#25928;&#24212;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;GNN&#30340;CF&#27169;&#22411;&#21487;&#33021;&#20250;&#29983;&#25104;&#38590;&#20197;&#21306;&#20998;&#19988;&#19981;&#20934;&#30830;&#30340;&#29992;&#25143;&#65288;&#29289;&#21697;&#65289;&#34920;&#31034;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#21327;&#21516;&#36807;&#28388;&#27169;&#22411;&#65288;SimRec&#65289;&#65292;&#23558;&#30693;&#35782;&#33976;&#39311;&#21644;&#23545;&#27604;&#23398;&#20064;&#30340;&#33021;&#21147;&#34701;&#21512;&#22312;&#19968;&#36215;&#65292;&#23454;&#29616;&#20102;&#25945;&#24072;GNN&#27169;&#22411;&#19982;&#36731;&#37327;&#32423;&#23398;&#29983;&#32593;&#32476;&#20043;&#38388;&#30340;&#33258;&#36866;&#24212;&#30693;&#35782;&#36716;&#31227;&#65292;&#22312;&#19981;&#38656;&#35201;&#26500;&#24314;&#22270;&#30340;&#24773;&#20917;&#19979;&#26356;&#22909;&#22320;&#21457;&#29616;&#29992;&#25143;&#21644;&#29289;&#21697;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) have shown the power in representation learning over graph-structured user-item interaction data for collaborative filtering (CF) task. However, with their inherently recursive message propagation among neighboring nodes, existing GNN-based CF models may generate indistinguishable and inaccurate user (item) representations due to the over-smoothing and noise effect with low-pass Laplacian smoothing operators. In addition, the recursive information propagation with the stacked aggregators in the entire graph structures may result in poor scalability in practical applications. Motivated by these limitations, we propose a simple and effective collaborative filtering model (SimRec) that marries the power of knowledge distillation and contrastive learning. In SimRec, adaptive transferring knowledge is enabled between the teacher GNN model and a lightweight student network, to not only preserve the global collaborative signals, but also address the over-smoothing
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26469;&#33258;&#21160;&#35782;&#21035;&#21644;&#21457;&#29616;&#31185;&#23398;&#25991;&#29486;&#20013;&#25968;&#23398;&#27010;&#24565;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#20844;&#24335;&#27010;&#24565;&#21457;&#29616;&#21644;&#20844;&#24335;&#27010;&#24565;&#35782;&#21035;&#20004;&#20010;&#23376;&#20219;&#21153;&#65292;&#36890;&#36807;&#23545;arXiv&#23376;&#38598;&#20013;1.8M&#20010;&#20844;&#24335;&#20986;&#29616;&#36827;&#34892;&#23454;&#39564;&#65292;&#32467;&#26524;&#26174;&#31034;&#35813;&#26041;&#27861;&#20248;&#20110;&#24378;&#22522;&#32447;&#12290;</title><link>http://arxiv.org/abs/2303.01994</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21457;&#29616;&#21644;&#35782;&#21035;&#25968;&#23398;&#20844;&#24335;&#27010;&#24565;
&lt;/p&gt;
&lt;p&gt;
Discovery and Recognition of Formula Concepts using Machine Learning. (arXiv:2303.01994v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01994
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26469;&#33258;&#21160;&#35782;&#21035;&#21644;&#21457;&#29616;&#31185;&#23398;&#25991;&#29486;&#20013;&#25968;&#23398;&#27010;&#24565;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#20844;&#24335;&#27010;&#24565;&#21457;&#29616;&#21644;&#20844;&#24335;&#27010;&#24565;&#35782;&#21035;&#20004;&#20010;&#23376;&#20219;&#21153;&#65292;&#36890;&#36807;&#23545;arXiv&#23376;&#38598;&#20013;1.8M&#20010;&#20844;&#24335;&#20986;&#29616;&#36827;&#34892;&#23454;&#39564;&#65292;&#32467;&#26524;&#26174;&#31034;&#35813;&#26041;&#27861;&#20248;&#20110;&#24378;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#24341;&#29992;&#30340;&#20449;&#24687;&#26816;&#32034;&#26041;&#27861;&#24050;&#32463;&#34987;&#35777;&#26126;&#21487;&#20197;&#22312;&#38656;&#35201;&#24341;&#29992;&#22823;&#37327;&#25991;&#29486;&#30340;&#23398;&#26415;&#23398;&#31185;&#20013;&#26377;&#25928;&#22320;&#36827;&#34892;&#20316;&#24330;&#26816;&#27979;&#25110;&#25991;&#29486;&#25512;&#33616;&#31995;&#32479;&#31561;&#20449;&#24687;&#26816;&#32034;&#24212;&#29992;&#12290;&#22312;&#31185;&#23398;&#12289;&#25216;&#26415;&#12289;&#24037;&#31243;&#21644;&#25968;&#23398;&#39046;&#22495;&#65292;&#30740;&#31350;&#20154;&#21592;&#36890;&#24120;&#36890;&#36807;&#20844;&#24335;&#31526;&#21495;&#26469;&#24341;&#29992;&#20808;&#21069;&#30340;&#30693;&#35782;&#12290;&#25105;&#20204;&#30340;&#38271;&#26399;&#30446;&#26631;&#26159;&#23558;&#22522;&#20110;&#24341;&#29992;&#30340;&#20449;&#24687;&#26816;&#32034;&#26041;&#27861;&#25512;&#24191;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#32463;&#20856;&#21442;&#32771;&#25991;&#29486;&#21644;&#25968;&#23398;&#27010;&#24565;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22914;&#20309;&#24341;&#29992;&#25968;&#23398;&#20844;&#24335;&#65292;&#24182;&#23450;&#20041;&#20102;&#19968;&#20010;&#8220;&#20844;&#24335;&#27010;&#24565;&#26816;&#32034;&#20219;&#21153;&#8221;&#65292;&#20854;&#20013;&#21253;&#25324;&#20844;&#24335;&#27010;&#24565;&#21457;&#29616;&#65288;Formula Concept Discovery&#65292;FCD&#65289;&#21644;&#20844;&#24335;&#27010;&#24565;&#35782;&#21035;&#65288;Formula Concept Recognition&#65292;FCR&#65289;&#20004;&#20010;&#23376;&#20219;&#21153;&#12290;FCD&#26088;&#22312;&#23450;&#20041;&#21644;&#25506;&#32034;&#21629;&#21517;&#20026;&#32465;&#23450;&#31561;&#25928;&#34920;&#31034;&#30340;&#8220;&#20844;&#24335;&#27010;&#24565;&#8221;&#65292;&#32780;FCR&#26088;&#22312;&#23558;&#32473;&#23450;&#30340;&#20844;&#24335;&#21305;&#37197;&#21040;&#20808;&#21069;&#20998;&#37197;&#30340;&#21807;&#19968;&#25968;&#23398;&#27010;&#24565;&#26631;&#35782;&#31526;&#19978;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20004;&#20010;&#20219;&#21153;&#65292;&#24182;&#22312;&#19968;&#20010;arXiv&#23376;&#38598;&#30340;1.8M&#20010;&#20844;&#24335;&#20986;&#29616;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#20248;&#20110;&#24378;&#22522;&#32447;&#65292;&#24182;&#20026;&#31185;&#23398;&#25991;&#29486;&#20013;&#30340;&#25968;&#23398;&#27010;&#24565;&#30340;&#33258;&#21160;&#35782;&#21035;&#21644;&#21457;&#29616;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
Citation-based Information Retrieval (IR) methods for scientific documents have proven effective for IR applications, such as Plagiarism Detection or Literature Recommender Systems in academic disciplines that use many references. In science, technology, engineering, and mathematics, researchers often employ mathematical concepts through formula notation to refer to prior knowledge. Our long-term goal is to generalize citation-based IR methods and apply this generalized method to both classical references and mathematical concepts. In this paper, we suggest how mathematical formulas could be cited and define a Formula Concept Retrieval task with two subtasks: Formula Concept Discovery (FCD) and Formula Concept Recognition (FCR). While FCD aims at the definition and exploration of a 'Formula Concept' that names bundled equivalent representations of a formula, FCR is designed to match a given formula to a prior assigned unique mathematical concept identifier. We present machine learning-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#36328;&#31995;&#32479;&#25512;&#38144;&#25915;&#20987;&#30340;&#26032;&#27010;&#24565;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#21517;&#20026; PC-Attack &#30340;&#23454;&#29992;&#26694;&#26550;&#12290;&#36825;&#20010;&#26694;&#26550;&#21482;&#38656;&#35201;&#24456;&#23569;&#30340;&#21463;&#23475;&#32773;&#25512;&#33616;&#31995;&#32479;&#27169;&#22411;&#21644;&#30446;&#26631;&#25512;&#33616;&#31995;&#32479;&#25968;&#25454;&#30340;&#20449;&#24687;&#21363;&#21487;&#36827;&#34892;&#25915;&#20987;&#65292;&#24182;&#22312;&#20844;&#20849;&#25512;&#33616;&#31995;&#32479;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#33258;&#25105;&#30417;&#30563;&#35757;&#32451;&#21644;&#24494;&#35843;&#65292;&#22240;&#27492;&#25915;&#20987;&#25104;&#21151;&#29575;&#36739;&#39640;&#12290;</title><link>http://arxiv.org/abs/2302.07145</link><description>&lt;p&gt;
&#20855;&#26377;&#26377;&#38480;&#25968;&#25454;&#35775;&#38382;&#30340;&#23454;&#29992;&#36328;&#31995;&#32479;&#25512;&#38144;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Practical Cross-System Shilling Attacks with Limited Access to Data. (arXiv:2302.07145v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07145
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#36328;&#31995;&#32479;&#25512;&#38144;&#25915;&#20987;&#30340;&#26032;&#27010;&#24565;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#21517;&#20026; PC-Attack &#30340;&#23454;&#29992;&#26694;&#26550;&#12290;&#36825;&#20010;&#26694;&#26550;&#21482;&#38656;&#35201;&#24456;&#23569;&#30340;&#21463;&#23475;&#32773;&#25512;&#33616;&#31995;&#32479;&#27169;&#22411;&#21644;&#30446;&#26631;&#25512;&#33616;&#31995;&#32479;&#25968;&#25454;&#30340;&#20449;&#24687;&#21363;&#21487;&#36827;&#34892;&#25915;&#20987;&#65292;&#24182;&#22312;&#20844;&#20849;&#25512;&#33616;&#31995;&#32479;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#33258;&#25105;&#30417;&#30563;&#35757;&#32451;&#21644;&#24494;&#35843;&#65292;&#22240;&#27492;&#25915;&#20987;&#25104;&#21151;&#29575;&#36739;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25512;&#38144;&#25915;&#20987;&#20013;&#65292;&#25932;&#23545;&#26041;&#20250;&#21521;&#25512;&#33616;&#31995;&#32479;&#27880;&#20837;&#23569;&#37327;&#30340;&#34394;&#20551;&#29992;&#25143;&#36164;&#26009;&#65292;&#20197;&#20415;&#25512;&#24191;&#25110;&#38477;&#20302;&#30446;&#26631;&#29289;&#21697;&#30340;&#25490;&#21517;&#12290;&#23613;&#31649;&#24050;&#32463;&#25226;&#24456;&#22810;&#30340;&#21162;&#21147;&#25918;&#22312;&#24320;&#21457;&#27169;&#25311;&#25915;&#20987;&#26041;&#27861;&#19978;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#26041;&#27861;&#20173;&#28982;&#36828;&#31163;&#23454;&#29992;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#20855;&#26377;&#23454;&#29992;&#25512;&#38144;&#25915;&#20987;&#26041;&#27861;&#30340;&#23646;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#36328;&#31995;&#32479;&#25915;&#20987;&#30340;&#26032;&#27010;&#24565;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22522;&#20110;&#36328;&#31995;&#32479;&#25915;&#20987;&#30340;&#24605;&#24819;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#23454;&#29992;&#30340;&#36328;&#31995;&#32479;&#25512;&#38144;&#25915;&#20987; (PC-Attack) &#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#23545;&#30446;&#26631;&#25512;&#33616;&#31995;&#32479;&#27169;&#22411;&#21644;&#30446;&#26631;&#25512;&#33616;&#31995;&#32479;&#25968;&#25454;&#30340;&#20449;&#24687;&#38656;&#27714;&#37327;&#24456;&#23567;&#12290;PC-Attack &#33258;&#25105;&#30417;&#30563;&#22320;&#20174;&#20844;&#20849;&#25512;&#33616;&#31995;&#32479;&#25968;&#25454;&#20013;&#33719;&#21462;&#20102;&#22270;&#24418;&#25299;&#25169;&#30693;&#35782;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23545;&#23427;&#36827;&#34892;&#24494;&#35843;&#65292;&#21033;&#29992;&#26131;&#20110;&#35775;&#38382;&#30340;&#37096;&#20998;&#30446;&#26631;&#25968;&#25454;&#26500;&#24314;&#34394;&#20551;&#36164;&#26009;&#12290;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;PC-Attack &#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#22522;&#32447;&#12290;&#25105;&#20204;&#30340; PC-Attack &#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
In shilling attacks, an adversarial party injects a few fake user profiles into a Recommender System (RS) so that the target item can be promoted or demoted. Although much effort has been devoted to developing shilling attack methods, we find that existing approaches are still far from practical. In this paper, we analyze the properties a practical shilling attack method should have and propose a new concept of Cross-system Attack. With the idea of Cross-system Attack, we design a Practical Cross-system Shilling Attack (PC-Attack) framework that requires little information about the victim RS model and the target RS data for conducting attacks. PC-Attack is trained to capture graph topology knowledge from public RS data in a self-supervised manner. Then, it is fine-tuned on a small portion of target data that is easy to access to construct fake profiles. Extensive experiments have demonstrated the superiority of PC-Attack over state-of-the-art baselines. Our implementation of PC-Attack
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#26597;&#35810;&#20316;&#20026;&#19978;&#19979;&#25991;&#30340;&#39044;&#35757;&#32451;&#25216;&#26415;&#65292;&#23558;&#26597;&#35810;&#20316;&#20026;&#19978;&#19979;&#25991;&#65292;&#24418;&#25104;&#19968;&#23545;&#36890;&#36947;-&#26597;&#35810;&#23545;&#65292;&#29992;&#20110;&#32531;&#35299;&#23494;&#38598;&#22411;&#36890;&#36947;&#26816;&#32034;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#24369;&#30456;&#20851;&#23545;&#65292;&#24182;&#22312;&#22823;&#35268;&#27169;&#22522;&#20934;&#27979;&#35797;&#19978;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2212.09598</link><description>&lt;p&gt;
&#26597;&#35810;&#20316;&#20026;&#19978;&#19979;&#25991;&#30340;&#39044;&#35757;&#32451;&#25216;&#26415;&#29992;&#20110;&#23494;&#38598;&#22411;&#36890;&#36947;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Query-as-context Pre-training for Dense Passage Retrieval. (arXiv:2212.09598v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09598
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#26597;&#35810;&#20316;&#20026;&#19978;&#19979;&#25991;&#30340;&#39044;&#35757;&#32451;&#25216;&#26415;&#65292;&#23558;&#26597;&#35810;&#20316;&#20026;&#19978;&#19979;&#25991;&#65292;&#24418;&#25104;&#19968;&#23545;&#36890;&#36947;-&#26597;&#35810;&#23545;&#65292;&#29992;&#20110;&#32531;&#35299;&#23494;&#38598;&#22411;&#36890;&#36947;&#26816;&#32034;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#24369;&#30456;&#20851;&#23545;&#65292;&#24182;&#22312;&#22823;&#35268;&#27169;&#22522;&#20934;&#27979;&#35797;&#19978;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20154;&#20204;&#30740;&#31350;&#20986;&#36890;&#36807;&#20351;&#29992;&#19978;&#19979;&#25991;&#26377;&#30417;&#30563;&#30340;&#39044;&#35757;&#32451;&#25216;&#26415;&#26469;&#25552;&#39640;&#23494;&#38598;&#22411;&#36890;&#36947;&#26816;&#32034;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;&#36825;&#20123;&#26041;&#27861;&#31616;&#21333;&#22320;&#35748;&#20026;&#26469;&#33258;&#21516;&#19968;&#25991;&#26723;&#30340;&#20004;&#20010;&#36890;&#36947;&#26159;&#30456;&#20851;&#30340;&#65292;&#32780;&#19981;&#32771;&#34385;&#21487;&#33021;&#23384;&#22312;&#30340;&#24369;&#30456;&#20851;&#23545;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#26597;&#35810;&#20316;&#20026;&#19978;&#19979;&#25991;&#30340;&#39044;&#35757;&#32451;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#31616;&#21333;&#32780;&#26377;&#25928;&#65292;&#29992;&#20110;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;&#26597;&#35810;&#20316;&#20026;&#19978;&#19979;&#25991;&#30340;&#39044;&#35757;&#32451;&#25216;&#26415;&#20551;&#23450;&#20174;&#36890;&#36947;&#20013;&#25552;&#21462;&#30340;&#26597;&#35810;&#26356;&#21487;&#33021;&#19982;&#35813;&#36890;&#36947;&#30456;&#20851;&#65292;&#24182;&#24418;&#25104;&#19968;&#23545;&#36890;&#36947;-&#26597;&#35810;&#23545;&#12290;&#36825;&#20123;&#36890;&#36947;-&#26597;&#35810;&#23545;&#28982;&#21518;&#29992;&#20110;&#23545;&#27604;&#24615;&#25110;&#29983;&#25104;&#24615;&#19978;&#19979;&#25991;&#26377;&#30417;&#30563;&#30340;&#39044;&#35757;&#32451;&#12290;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#22823;&#35268;&#27169;&#36890;&#36947;&#26816;&#32034;&#22522;&#20934;&#27979;&#35797;&#21644;&#36328;&#39046;&#22495;&#38646;-shot&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#26597;&#35810;&#20316;&#20026;&#19978;&#19979;&#25991;&#30340;&#39044;&#35757;&#32451;&#25216;&#26415;&#24102;&#26469;&#20102;&#30456;&#24403;&#22823;&#30340;&#22686;&#30410;&#65292;&#21516;&#26102;&#21152;&#36895;&#20102;&#35757;&#32451;&#65292;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#23558;&#20250;&#22312;https://github.com/deepset-ai/haystack&#19978;&#25552;&#20379;&#19979;&#36733;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, methods have been developed to improve the performance of dense passage retrieval by using context-supervised pre-training. These methods simply consider two passages from the same document to be relevant, without taking into account the possibility of weakly correlated pairs. Thus, this paper proposes query-as-context pre-training, a simple yet effective pre-training technique to alleviate the issue. Query-as-context pre-training assumes that the query derived from a passage is more likely to be relevant to that passage and forms a passage-query pair. These passage-query pairs are then used in contrastive or generative context-supervised pre-training. The pre-trained models are evaluated on large-scale passage retrieval benchmarks and out-of-domain zero-shot benchmarks. Experimental results show that query-as-context pre-training brings considerable gains and meanwhile speeds up training, demonstrating its effectiveness and efficiency. Our code will be available at https://g
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#25991;&#26412;&#23884;&#20837;&#22522;&#20934;&#27979;&#35797;(MTEB)&#65292;&#35813;&#22522;&#20934;&#27979;&#35797;&#28085;&#30422;&#20102;8&#20010;&#23884;&#20837;&#20219;&#21153;&#12289;58&#20010;&#25968;&#25454;&#38598;&#21644;112&#31181;&#35821;&#35328;&#65292;&#20197;&#35299;&#20915;&#25991;&#26412;&#23884;&#20837;&#22312;&#19981;&#21516;&#20219;&#21153;&#20013;&#34920;&#29616;&#24046;&#24322;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;33&#20010;&#27169;&#22411;&#30340;&#27979;&#35797;&#65292;&#20316;&#32773;&#21457;&#29616;&#35813;&#39046;&#22495;&#23578;&#26410;&#25910;&#25947;&#20110;&#19968;&#31181;&#36890;&#29992;&#30340;&#25991;&#26412;&#23884;&#20837;&#26041;&#27861;&#65292;</title><link>http://arxiv.org/abs/2210.07316</link><description>&lt;p&gt;
MTEB: &#22823;&#35268;&#27169;&#25991;&#26412;&#23884;&#20837;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
MTEB: Massive Text Embedding Benchmark. (arXiv:2210.07316v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.07316
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#25991;&#26412;&#23884;&#20837;&#22522;&#20934;&#27979;&#35797;(MTEB)&#65292;&#35813;&#22522;&#20934;&#27979;&#35797;&#28085;&#30422;&#20102;8&#20010;&#23884;&#20837;&#20219;&#21153;&#12289;58&#20010;&#25968;&#25454;&#38598;&#21644;112&#31181;&#35821;&#35328;&#65292;&#20197;&#35299;&#20915;&#25991;&#26412;&#23884;&#20837;&#22312;&#19981;&#21516;&#20219;&#21153;&#20013;&#34920;&#29616;&#24046;&#24322;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;33&#20010;&#27169;&#22411;&#30340;&#27979;&#35797;&#65292;&#20316;&#32773;&#21457;&#29616;&#35813;&#39046;&#22495;&#23578;&#26410;&#25910;&#25947;&#20110;&#19968;&#31181;&#36890;&#29992;&#30340;&#25991;&#26412;&#23884;&#20837;&#26041;&#27861;&#65292;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#23884;&#20837;&#36890;&#24120;&#22312;&#35206;&#30422;&#20854;&#20182;&#20219;&#21153;&#30340;&#21487;&#33021;&#24212;&#29992;&#26102;&#65292;&#20165;&#22312;&#21333;&#20010;&#20219;&#21153;&#30340;&#23569;&#37327;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;&#30446;&#21069;&#36824;&#19981;&#28165;&#26970;&#22312;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24230;&#65288;STS&#65289;&#19978;&#26368;&#20808;&#36827;&#30340;&#23884;&#20837;&#26041;&#27861;&#26159;&#21542;&#21516;&#26679;&#36866;&#29992;&#20110;&#20854;&#20182;&#20219;&#21153;&#65292;&#27604;&#22914;&#32858;&#31867;&#25110;&#37325;&#26032;&#25490;&#24207;&#12290;&#36825;&#20351;&#24471;&#35780;&#20272;&#35813;&#39046;&#22495;&#30340;&#36827;&#23637;&#21464;&#24471;&#22256;&#38590;&#65292;&#22240;&#20026;&#21508;&#31181;&#27169;&#22411;&#19981;&#26029;&#34987;&#25552;&#20986;&#21364;&#27809;&#26377;&#24471;&#21040;&#36866;&#24403;&#30340;&#35780;&#20272;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22823;&#35268;&#27169;&#25991;&#26412;&#23884;&#20837;&#22522;&#20934;&#27979;&#35797;&#65288;MTEB&#65289;&#12290;MTEB&#28085;&#30422;8&#20010;&#23884;&#20837;&#20219;&#21153;&#65292;&#28085;&#30422;58&#20010;&#25968;&#25454;&#38598;&#21644;112&#20010;&#35821;&#35328;&#12290;&#36890;&#36807;&#22312;MTEB&#19978;&#23545;33&#20010;&#27169;&#22411;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#36804;&#20170;&#20026;&#27490;&#26368;&#20840;&#38754;&#30340;&#25991;&#26412;&#23884;&#20837;&#22522;&#20934;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#27809;&#26377;&#20219;&#20309;&#19968;&#31181;&#29305;&#23450;&#30340;&#25991;&#26412;&#23884;&#20837;&#26041;&#27861;&#22312;&#25152;&#26377;&#20219;&#21153;&#20013;&#37117;&#21344;&#25454;&#20248;&#21183;&#12290;&#36825;&#34920;&#26126;&#35813;&#39046;&#22495;&#23578;&#26410;&#25910;&#25947;&#20110;&#19968;&#31181;&#36890;&#29992;&#30340;&#25991;&#26412;&#23884;&#20837;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#25193;&#23637;&#36275;&#22815;&#22823;&#20197;&#22312;&#25152;&#26377;&#23884;&#20837;&#20219;&#21153;&#20013;&#25552;&#20379;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;MTEB&#38468;&#24102;&#24320;&#28304;&#20195;&#30721;&#21644;&#25968;&#25454;&#65292;&#20197;&#20351;&#31038;&#21306;&#33021;&#22815;&#22522;&#20934;&#27979;&#35797;&#26032;&#30340;&#23884;&#20837;&#27169;&#22411;&#24182;&#36319;&#36394;&#35813;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text embeddings are commonly evaluated on a small set of datasets from a single task not covering their possible applications to other tasks. It is unclear whether state-of-the-art embeddings on semantic textual similarity (STS) can be equally well applied to other tasks like clustering or reranking. This makes progress in the field difficult to track, as various models are constantly being proposed without proper evaluation. To solve this problem, we introduce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding tasks covering a total of 58 datasets and 112 languages. Through the benchmarking of 33 models on MTEB, we establish the most comprehensive benchmark of text embeddings to date. We find that no particular text embedding method dominates across all tasks. This suggests that the field has yet to converge on a universal text embedding method and scale it up sufficiently to provide state-of-the-art results on all embedding tasks. MTEB comes with open-source code and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#38463;&#37324;&#24052;&#24052;&#38598;&#22242;&#30340;&#31354;&#21069;&#35268;&#27169;&#30340; OpenBG &#21830;&#19994;&#30693;&#35782;&#22270;&#35889;&#65292;&#21253;&#21547;&#36229;&#36807; 88 &#30334;&#19975;&#23454;&#20307;&#21644; 26 &#20159;&#19977;&#20803;&#32452;&#12290;&#23427;&#20855;&#26377;&#31934;&#32454;&#30340;&#20998;&#31867;&#21644;&#22810;&#27169;&#24577;&#20107;&#23454;&#65292;&#26377;&#21161;&#20110;&#25512;&#21160;&#21830;&#19994;&#26234;&#33021;&#21270;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2209.15214</link><description>&lt;p&gt;
&#21313;&#20159;&#32423;&#39044;&#35757;&#32451;&#22810;&#27169;&#24577;&#21830;&#19994;&#30693;&#35782;&#22270;&#35889;&#30340;&#26500;&#24314;&#19982;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Construction and Applications of Billion-Scale Pre-Trained Multimodal Business Knowledge Graph. (arXiv:2209.15214v6 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.15214
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#38463;&#37324;&#24052;&#24052;&#38598;&#22242;&#30340;&#31354;&#21069;&#35268;&#27169;&#30340; OpenBG &#21830;&#19994;&#30693;&#35782;&#22270;&#35889;&#65292;&#21253;&#21547;&#36229;&#36807; 88 &#30334;&#19975;&#23454;&#20307;&#21644; 26 &#20159;&#19977;&#20803;&#32452;&#12290;&#23427;&#20855;&#26377;&#31934;&#32454;&#30340;&#20998;&#31867;&#21644;&#22810;&#27169;&#24577;&#20107;&#23454;&#65292;&#26377;&#21161;&#20110;&#25512;&#21160;&#21830;&#19994;&#26234;&#33021;&#21270;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21830;&#19994;&#30693;&#35782;&#22270;&#35889;&#26159;&#24403;&#21069;&#35768;&#22810;&#20225;&#19994;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#20026;&#35768;&#22810;&#20135;&#21697;&#25552;&#20379;&#20107;&#23454;&#30693;&#35782;&#21644;&#32467;&#26500;&#21270;&#25968;&#25454;&#65292;&#20351;&#23427;&#20204;&#21464;&#24471;&#26356;&#21152;&#26234;&#33021;&#21270;&#12290;&#23613;&#31649;&#23427;&#20204;&#26377;&#30528;&#35768;&#22810;&#28508;&#22312;&#30340;&#22909;&#22788;&#65292;&#20294;&#26500;&#24314;&#21830;&#19994;&#30693;&#35782;&#22270;&#35889;&#38656;&#35201;&#35299;&#20915;&#32467;&#26500;&#19981;&#36275;&#21644;&#22810;&#27169;&#24577;&#30340;&#38480;&#21046;&#31561;&#38382;&#39064;&#12290;&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#22312;&#38750;&#24494;&#19981;&#36275;&#36947;&#30340;&#23454;&#38469;&#24212;&#29992;&#31995;&#32479;&#20013;&#26500;&#24314;&#30693;&#35782;&#22270;&#35889;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#38463;&#37324;&#24052;&#24052;&#38598;&#22242;&#30340; OpenBG &#21830;&#19994;&#30693;&#35782;&#22270;&#35889;&#30340;&#26500;&#24314;&#36807;&#31243;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#26680;&#24515;&#26412;&#20307;&#65292;&#28085;&#30422;&#21508;&#31181;&#25277;&#35937;&#20135;&#21697;&#21644;&#28040;&#36153;&#38656;&#27714;&#65292;&#24182;&#22312;&#37096;&#32626;&#30340;&#24212;&#29992;&#20013;&#25552;&#20379;&#31934;&#32454;&#30340;&#20998;&#31867;&#21644;&#22810;&#27169;&#24577;&#20107;&#23454;&#12290;OpenBG &#26159;&#19968;&#20010;&#31354;&#21069;&#35268;&#27169;&#30340;&#21830;&#19994;&#30693;&#35782;&#22270;&#35889;&#65306;&#21253;&#21547;&#36229;&#36807; 88 &#30334;&#19975;&#23454;&#20307;&#12289;&#35206;&#30422;&#36229;&#36807; 1 &#30334;&#19975;&#20010;&#26680;&#24515;&#31867;/&#27010;&#24565;&#21644; 2,681 &#31181;&#20851;&#31995;&#30340; 26 &#20159;&#19977;&#20803;&#32452;&#12290;&#25105;&#20204;&#20844;&#24320;&#20102;&#25152;&#26377;&#30340;&#36164;&#28304;&#21644;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20197;&#20419;&#36827;&#30693;&#35782;&#22270;&#35889;&#30340;&#21457;&#23637;&#21644;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Business Knowledge Graphs (KGs) are important to many enterprises today, providing factual knowledge and structured data that steer many products and make them more intelligent. Despite their promising benefits, building business KG necessitates solving prohibitive issues of deficient structure and multiple modalities. In this paper, we advance the understanding of the practical challenges related to building KG in non-trivial real-world systems. We introduce the process of building an open business knowledge graph (OpenBG) derived from a well-known enterprise, Alibaba Group. Specifically, we define a core ontology to cover various abstract products and consumption demands, with fine-grained taxonomy and multimodal facts in deployed applications. OpenBG is an open business KG of unprecedented scale: 2.6 billion triples with more than 88 million entities covering over 1 million core classes/concepts and 2,681 types of relations. We release all the open resources (OpenBG benchmarks) deri
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#32852;&#37030;&#25512;&#33616;&#31995;&#32479;&#30340;&#38544;&#31169;&#38382;&#39064;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;ccGAN&#26041;&#27861;&#30340;&#32852;&#37030;&#26694;&#26550;&#65292;&#20197;&#38450;&#27490;&#23646;&#24615;&#25512;&#26029;&#25915;&#20987;&#65292;&#24182;&#22312;&#20840;&#23616;&#25512;&#33616;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2205.11857</link><description>&lt;p&gt;
&#38754;&#21521;&#23646;&#24615;&#25512;&#26029;&#25915;&#20987;&#30340;&#32852;&#37030;&#25512;&#33616;&#31995;&#32479;&#30340;&#20840;&#38754;&#38544;&#31169;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Comprehensive Privacy Analysis on Federated Recommender System against Attribute Inference Attacks. (arXiv:2205.11857v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.11857
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#32852;&#37030;&#25512;&#33616;&#31995;&#32479;&#30340;&#38544;&#31169;&#38382;&#39064;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;ccGAN&#26041;&#27861;&#30340;&#32852;&#37030;&#26694;&#26550;&#65292;&#20197;&#38450;&#27490;&#23646;&#24615;&#25512;&#26029;&#25915;&#20987;&#65292;&#24182;&#22312;&#20840;&#23616;&#25512;&#33616;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#25512;&#33616;&#31995;&#32479;&#23545;&#20110;&#25552;&#20379;&#28385;&#36275;&#29992;&#25143;&#20559;&#22909;&#30340;&#20010;&#24615;&#21270;&#26381;&#21153;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20026;&#29992;&#25143;&#24314;&#27169;&#21644;&#20998;&#26512;&#24517;&#39035;&#25910;&#38598;&#20010;&#20154;&#25968;&#25454;&#65292;&#36825;&#20351;&#29992;&#25143;&#23481;&#26131;&#21463;&#21040;&#23646;&#24615;&#25512;&#26029;&#25915;&#20987;&#12290;&#26412;&#25991;&#38024;&#23545;&#32852;&#37030;&#25512;&#33616;&#31995;&#32479;&#23545;&#23646;&#24615;&#25512;&#26029;&#25915;&#20987;&#36827;&#34892;&#20840;&#38754;&#38544;&#31169;&#20998;&#26512;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#32852;&#37030;&#26694;&#26550;&#65292;&#21033;&#29992;&#22240;&#26524;&#25511;&#21046;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;ccGAN&#65289;&#21512;&#25104;&#29992;&#25143;&#30340;&#31169;&#26377;&#23646;&#24615;&#65292;&#26082;&#20445;&#25252;&#20102;&#29992;&#25143;&#30340;&#38544;&#31169;&#65292;&#21448;&#20419;&#36827;&#20102;&#20840;&#23616;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, recommender systems are crucially important for the delivery of personalized services that satisfy users' preferences. With personalized recommendation services, users can enjoy a variety of recommendations such as movies, books, ads, restaurants, and more. Despite the great benefits, personalized recommendations typically require the collection of personal data for user modelling and analysis, which can make users susceptible to attribute inference attacks. Specifically, the vulnerability of existing centralized recommenders under attribute inference attacks leaves malicious attackers a backdoor to infer users' private attributes, as the systems remember information of their training data (i.e., interaction data and side information). An emerging practice is to implement recommender systems in the federated setting, which enables all user devices to collaboratively learn a shared global recommender while keeping all the training data on device. However, the privacy is
&lt;/p&gt;</description></item></channel></rss>