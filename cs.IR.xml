<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>Disaggregated Multi-Tower&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#25299;&#25169;&#24863;&#30693;&#30340;&#24314;&#27169;&#25216;&#26415;&#65292;&#36890;&#36807;SPTT&#12289;TM&#21644;TP&#19977;&#20010;&#32452;&#20214;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#22823;&#35268;&#27169;&#25512;&#33616;&#65292;&#21152;&#36895;&#24615;&#33021;&#25552;&#21319;&#20102;1.9&#20493;&#12290;</title><link>https://arxiv.org/abs/2403.00877</link><description>&lt;p&gt;
Disaggregated Multi-Tower: &#38754;&#21521;&#25299;&#25169;&#24863;&#30693;&#30340;&#39640;&#25928;&#22823;&#35268;&#27169;&#25512;&#33616;&#24314;&#27169;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Disaggregated Multi-Tower: Topology-aware Modeling Technique for Efficient Large-Scale Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00877
&lt;/p&gt;
&lt;p&gt;
Disaggregated Multi-Tower&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#25299;&#25169;&#24863;&#30693;&#30340;&#24314;&#27169;&#25216;&#26415;&#65292;&#36890;&#36807;SPTT&#12289;TM&#21644;TP&#19977;&#20010;&#32452;&#20214;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#22823;&#35268;&#27169;&#25512;&#33616;&#65292;&#21152;&#36895;&#24615;&#33021;&#25552;&#21319;&#20102;1.9&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#28145;&#24230;&#23398;&#20064;&#25512;&#33616;&#27169;&#22411;&#30340;&#25153;&#24179;&#26550;&#26500;&#12289;&#24120;&#35265;&#30340;&#20998;&#24067;&#24335;&#35757;&#32451;&#27169;&#24335;&#21644;&#20998;&#23618;&#25968;&#25454;&#20013;&#24515;&#25299;&#25169;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#12290;&#20026;&#20102;&#35299;&#20915;&#30456;&#20851;&#30340;&#20302;&#25928;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Disaggregated Multi-Tower&#65288;DMT&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#24314;&#27169;&#25216;&#26415;&#65292;&#21253;&#25324;&#65288;1&#65289;&#35821;&#20041;&#20445;&#30041;&#30340;Tower Transform&#65288;SPTT&#65289;&#65292;&#19968;&#20010;&#23558;&#21333;&#29255;&#20840;&#23616;&#23884;&#20837;&#26597;&#25214;&#36807;&#31243;&#20998;&#35299;&#20026;&#19981;&#30456;&#20132;&#22612;&#20197;&#21033;&#29992;&#25968;&#25454;&#20013;&#24515;&#20301;&#32622;&#20851;&#31995;&#30340;&#26032;&#22411;&#35757;&#32451;&#27169;&#24335;&#65307;&#65288;2&#65289;Tower Module&#65288;TM&#65289;&#65292;&#19968;&#20010;&#38468;&#21152;&#21040;&#27599;&#20010;&#22612;&#30340;&#21327;&#21516;&#31264;&#23494;&#32452;&#20214;&#65292;&#36890;&#36807;&#20998;&#23618;&#29305;&#24449;&#20132;&#20114;&#38477;&#20302;&#27169;&#22411;&#22797;&#26434;&#24615;&#21644;&#36890;&#20449;&#37327;&#65307;&#21644;&#65288;3&#65289;Tower Partitioner&#65288;TP&#65289;&#65292;&#19968;&#20010;&#29305;&#24449;&#20998;&#21306;&#22120;&#65292;&#31995;&#32479;&#22320;&#21019;&#24314;&#20855;&#26377;&#26377;&#24847;&#20041;&#29305;&#24449;&#20132;&#20114;&#21644;&#36127;&#36733;&#24179;&#34913;&#20998;&#37197;&#30340;&#22612;&#65292;&#36890;&#36807;&#23398;&#20064;&#30340;&#23884;&#20837;&#26469;&#20445;&#25345;&#27169;&#22411;&#36136;&#37327;&#21644;&#35757;&#32451;&#21534;&#21520;&#37327;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;DMT&#30456;&#27604;&#20110;&#26368;&#26032;&#30340;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#39640;&#36798;1.9&#20493;&#30340;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00877v1 Announce Type: new  Abstract: We study a mismatch between the deep learning recommendation models' flat architecture, common distributed training paradigm and hierarchical data center topology. To address the associated inefficiencies, we propose Disaggregated Multi-Tower (DMT), a modeling technique that consists of (1) Semantic-preserving Tower Transform (SPTT), a novel training paradigm that decomposes the monolithic global embedding lookup process into disjoint towers to exploit data center locality; (2) Tower Module (TM), a synergistic dense component attached to each tower to reduce model complexity and communication volume through hierarchical feature interaction; and (3) Tower Partitioner (TP), a feature partitioner to systematically create towers with meaningful feature interactions and load balanced assignments to preserve model quality and training throughput via learned embeddings. We show that DMT can achieve up to 1.9x speedup compared to the state-of-th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#29983;&#25104;&#24615;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#35821;&#20041;ID&#30340;&#33258;&#30417;&#30563;&#26694;&#26550;LMINDEXER&#12290;</title><link>http://arxiv.org/abs/2310.07815</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35821;&#20041;&#32034;&#24341;&#22120;
&lt;/p&gt;
&lt;p&gt;
Language Models As Semantic Indexers. (arXiv:2310.07815v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07815
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#29983;&#25104;&#24615;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#35821;&#20041;ID&#30340;&#33258;&#30417;&#30563;&#26694;&#26550;LMINDEXER&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#26631;&#35782;&#31526;&#65288;ID&#65289;&#26159;&#20449;&#24687;&#26816;&#32034;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#27010;&#24565;&#65292;&#26088;&#22312;&#20445;&#30041;&#23545;&#35937;&#65288;&#22914;&#25991;&#26723;&#21644;&#39033;&#65289;&#20869;&#37096;&#30340;&#35821;&#20041;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#36890;&#24120;&#37319;&#29992;&#20004;&#38454;&#27573;&#27969;&#31243;&#26469;&#23398;&#20064;&#35821;&#20041;ID&#65292;&#39318;&#20808;&#20351;&#29992;&#29616;&#25104;&#30340;&#25991;&#26412;&#32534;&#30721;&#22120;&#33719;&#21462;&#23884;&#20837;&#65292;&#24182;&#26681;&#25454;&#23884;&#20837;&#26469;&#25512;&#23548;ID&#12290;&#28982;&#32780;&#65292;&#27599;&#20010;&#27493;&#39588;&#37117;&#20250;&#24341;&#20837;&#28508;&#22312;&#30340;&#20449;&#24687;&#25439;&#22833;&#65292;&#24182;&#19988;&#25991;&#26412;&#32534;&#30721;&#22120;&#29983;&#25104;&#30340;&#28508;&#22312;&#31354;&#38388;&#20869;&#30340;&#23884;&#20837;&#20998;&#24067;&#36890;&#24120;&#19982;&#35821;&#20041;&#32034;&#24341;&#25152;&#38656;&#30340;&#39044;&#26399;&#20998;&#24067;&#23384;&#22312;&#22266;&#26377;&#30340;&#19981;&#21305;&#37197;&#12290;&#28982;&#32780;&#65292;&#35774;&#35745;&#19968;&#20010;&#26082;&#33021;&#23398;&#20064;&#25991;&#26723;&#30340;&#35821;&#20041;&#34920;&#31034;&#21448;&#33021;&#21516;&#26102;&#23398;&#20064;&#20854;&#20998;&#23618;&#32467;&#26500;&#30340;&#26041;&#27861;&#24182;&#19981;&#23481;&#26131;&#65292;&#22240;&#20026;&#35821;&#20041;ID&#26159;&#31163;&#25955;&#21644;&#39034;&#24207;&#32467;&#26500;&#30340;&#65292;&#24182;&#19988;&#35821;&#20041;&#30417;&#30563;&#26159;&#19981;&#20805;&#20998;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;LMINDEXER&#65292;&#23427;&#26159;&#19968;&#20010;&#33258;&#30417;&#30563;&#26694;&#26550;&#65292;&#29992;&#20110;&#20351;&#29992;&#29983;&#25104;&#24615;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#35821;&#20041;ID&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic identifier (ID) is an important concept in information retrieval that aims to preserve the semantics of objects such as documents and items inside their IDs. Previous studies typically adopt a two-stage pipeline to learn semantic IDs by first procuring embeddings using off-the-shelf text encoders and then deriving IDs based on the embeddings. However, each step introduces potential information loss and there is usually an inherent mismatch between the distribution of embeddings within the latent space produced by text encoders and the anticipated distribution required for semantic indexing. Nevertheless, it is non-trivial to design a method that can learn the document's semantic representations and its hierarchical structure simultaneously, given that semantic IDs are discrete and sequentially structured, and the semantic supervision is deficient. In this paper, we introduce LMINDEXER, a self-supervised framework to learn semantic IDs with a generative language model. We tackl
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#20174;&#30495;&#23454;&#29992;&#25143;&#37027;&#37324;&#33719;&#21462;&#39640;&#36136;&#37327;&#30340;&#31532;&#19968;&#26041;&#25968;&#25454;&#26469;&#20934;&#30830;&#39044;&#27979;&#25628;&#32034;&#32773;&#30340;&#20559;&#22909;&#12290;</title><link>http://arxiv.org/abs/2309.10621</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#25628;&#32034;&#32773;&#30340;&#20559;&#22909;
&lt;/p&gt;
&lt;p&gt;
Large language models can accurately predict searcher preferences. (arXiv:2309.10621v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10621
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#20174;&#30495;&#23454;&#29992;&#25143;&#37027;&#37324;&#33719;&#21462;&#39640;&#36136;&#37327;&#30340;&#31532;&#19968;&#26041;&#25968;&#25454;&#26469;&#20934;&#30830;&#39044;&#27979;&#25628;&#32034;&#32773;&#30340;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#20851;&#24615;&#26631;&#31614;&#26159;&#35780;&#20272;&#21644;&#20248;&#21270;&#25628;&#32034;&#31995;&#32479;&#30340;&#20851;&#38190;&#12290;&#33719;&#21462;&#22823;&#37327;&#30456;&#20851;&#24615;&#26631;&#31614;&#36890;&#24120;&#38656;&#35201;&#31532;&#19977;&#26041;&#26631;&#27880;&#20154;&#21592;&#65292;&#20294;&#23384;&#22312;&#20302;&#36136;&#37327;&#25968;&#25454;&#30340;&#39118;&#38505;&#12290;&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25913;&#36827;&#26631;&#31614;&#36136;&#37327;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#30495;&#23454;&#29992;&#25143;&#37027;&#37324;&#33719;&#24471;&#20180;&#32454;&#21453;&#39304;&#26469;&#33719;&#21462;&#39640;&#36136;&#37327;&#30340;&#31532;&#19968;&#26041;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Relevance labels, which indicate whether a search result is valuable to a searcher, are key to evaluating and optimising search systems. The best way to capture the true preferences of users is to ask them for their careful feedback on which results would be useful, but this approach does not scale to produce a large number of labels. Getting relevance labels at scale is usually done with third-party labellers, who judge on behalf of the user, but there is a risk of low-quality data if the labeller doesn't understand user needs. To improve quality, one standard approach is to study real users through interviews, user studies and direct feedback, find areas where labels are systematically disagreeing with users, then educate labellers about user needs through judging guidelines, training and monitoring. This paper introduces an alternate approach for improving label quality. It takes careful feedback from real users, which by definition is the highest-quality first-party gold data that 
&lt;/p&gt;</description></item></channel></rss>