<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#23545;&#35805;&#24773;&#22659;&#19979;&#30340;&#27169;&#22411;&#36827;&#34892;&#37325;&#25490;&#24207;&#30340;&#26041;&#27861;&#65292;&#24110;&#21161;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#32553;&#23567;&#35757;&#32451;&#21644;&#25512;&#26029;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#20197;&#25913;&#36827;&#20154;&#24037;&#26234;&#33021;&#21307;&#23398;&#21490;&#37319;&#38598;&#12290;</title><link>http://arxiv.org/abs/2304.01974</link><description>&lt;p&gt;
&#21307;&#23398;&#21490;&#37319;&#38598;&#30340;&#23545;&#35805;&#24773;&#22659;&#37325;&#25490;&#24207;
&lt;/p&gt;
&lt;p&gt;
Dialogue-Contextualized Re-ranking for Medical History-Taking. (arXiv:2304.01974v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01974
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#23545;&#35805;&#24773;&#22659;&#19979;&#30340;&#27169;&#22411;&#36827;&#34892;&#37325;&#25490;&#24207;&#30340;&#26041;&#27861;&#65292;&#24110;&#21161;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#32553;&#23567;&#35757;&#32451;&#21644;&#25512;&#26029;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#20197;&#25913;&#36827;&#20154;&#24037;&#26234;&#33021;&#21307;&#23398;&#21490;&#37319;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#21307;&#23398;&#21490;&#37319;&#38598;&#26159;&#30151;&#29366;&#26816;&#26597;&#12289;&#33258;&#21160;&#24739;&#32773;&#25509;&#24453;&#12289;&#20998;&#35786;&#21644;&#20854;&#20182;&#20154;&#24037;&#26234;&#33021;&#34394;&#25311;&#25252;&#29702;&#24212;&#29992;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#30001;&#20110;&#30149;&#21490;&#37319;&#38598;&#26041;&#24335;&#30340;&#22810;&#26679;&#24615;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#25361;&#25112;&#65292;&#29616;&#26377;&#30340;&#31995;&#32479;&#20351;&#29992;&#38388;&#25509;&#25968;&#25454;&#25110;&#19987;&#23478;&#30693;&#35782;&#36827;&#34892;&#24320;&#21457;&#12290;&#36825;&#23548;&#33268;&#20102;&#35757;&#32451;&#21644;&#25512;&#26029;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#22240;&#20026;&#27169;&#22411;&#26159;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#32780;&#19981;&#26159;&#22312;&#25512;&#26029;&#26102;&#35266;&#23519;&#21040;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#37325;&#25490;&#24207;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#23545;&#35805;&#24773;&#22659;&#19979;&#30340;&#27169;&#22411;&#37325;&#26032;&#23545;&#31532;&#19968;&#38454;&#27573;&#30340;&#38382;&#39064;&#20505;&#36873;&#32773;&#36827;&#34892;&#25490;&#24207;&#65292;&#24110;&#21161;&#32553;&#23567;&#35757;&#32451;&#21644;&#25512;&#26029;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#27169;&#22411;&#8212;&#8212;&#20840;&#23616;&#37325;&#25490;&#24207;&#22120;&#65292;&#35813;&#27169;&#22411;&#21516;&#26102;&#23558;&#25152;&#26377;&#38382;&#39064;&#19982;&#23545;&#35805;&#36827;&#34892;&#20132;&#21449;&#32534;&#30721;&#65292;&#24182;&#23558;&#20854;&#19982;&#20960;&#31181;&#29616;&#26377;&#30340;&#31070;&#32463;&#32447;&#36335;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#27979;&#35797;&#20102;transformer&#21644;S4&#35821;&#35328;&#27169;&#22411;&#32972;&#26223;&#19979;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#30456;&#23545;&#20110;&#19987;&#23478;&#31995;&#32479;&#65292;&#26368;&#20339;&#34920;&#29616;&#26159;&#23454;&#29616;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
AI-driven medical history-taking is an important component in symptom checking, automated patient intake, triage, and other AI virtual care applications. As history-taking is extremely varied, machine learning models require a significant amount of data to train. To overcome this challenge, existing systems are developed using indirect data or expert knowledge. This leads to a training-inference gap as models are trained on different kinds of data than what they observe at inference time. In this work, we present a two-stage re-ranking approach that helps close the training-inference gap by re-ranking the first-stage question candidates using a dialogue-contextualized model. For this, we propose a new model, global re-ranker, which cross-encodes the dialogue with all questions simultaneously, and compare it with several existing neural baselines. We test both transformer and S4-based language model backbones. We find that relative to the expert system, the best performance is achieved 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#25903;&#25345;&#22810;&#23186;&#20307;&#20869;&#23481;&#21019;&#24314;&#30340;&#22270;&#20687;/&#25991;&#26412;&#26816;&#32034;&#27979;&#35797;&#38598;&#8212;&#8212;AToMiC&#65292;&#23427;&#20351;&#29992;&#20102;&#32500;&#22522;&#30334;&#31185;&#20013;&#30340;&#22823;&#35268;&#27169;&#22270;&#20687;-&#25991;&#26723;&#20851;&#32852;&#65292;&#24182;&#24314;&#31435;&#20102;&#22810;&#26679;&#30340;&#39046;&#22495;&#25991;&#26412;&#21644;&#22270;&#29255;&#12290;AToMiC &#20026;&#21487;&#25193;&#23637;&#12289;&#22810;&#26679;&#21270;&#12289;&#21487;&#22797;&#29616;&#30340;&#22810;&#23186;&#20307;&#26816;&#32034;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#27979;&#35797;&#24179;&#21488;&#12290;</title><link>http://arxiv.org/abs/2304.01961</link><description>&lt;p&gt;
AToMiC&#65306;&#25903;&#25345;&#22810;&#23186;&#20307;&#20869;&#23481;&#21019;&#24314;&#30340;&#22270;&#20687;/&#25991;&#26412;&#26816;&#32034;&#27979;&#35797;&#38598;
&lt;/p&gt;
&lt;p&gt;
AToMiC: An Image/Text Retrieval Test Collection to Support Multimedia Content Creation. (arXiv:2304.01961v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01961
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#25903;&#25345;&#22810;&#23186;&#20307;&#20869;&#23481;&#21019;&#24314;&#30340;&#22270;&#20687;/&#25991;&#26412;&#26816;&#32034;&#27979;&#35797;&#38598;&#8212;&#8212;AToMiC&#65292;&#23427;&#20351;&#29992;&#20102;&#32500;&#22522;&#30334;&#31185;&#20013;&#30340;&#22823;&#35268;&#27169;&#22270;&#20687;-&#25991;&#26723;&#20851;&#32852;&#65292;&#24182;&#24314;&#31435;&#20102;&#22810;&#26679;&#30340;&#39046;&#22495;&#25991;&#26412;&#21644;&#22270;&#29255;&#12290;AToMiC &#20026;&#21487;&#25193;&#23637;&#12289;&#22810;&#26679;&#21270;&#12289;&#21487;&#22797;&#29616;&#30340;&#22810;&#23186;&#20307;&#26816;&#32034;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#27979;&#35797;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;AToMiC&#65288;&#22810;&#23186;&#20307;&#20869;&#23481;&#21019;&#20316;&#24037;&#20855;&#65289;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25512;&#21160;&#22270;&#20687;/&#25991;&#26412;&#36328;&#27169;&#24577;&#26816;&#32034;&#39046;&#22495;&#30340;&#30740;&#31350;&#12290;&#34429;&#28982;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#24050;&#32463;&#22312;&#25552;&#39640;&#26816;&#32034;&#25928;&#26524;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#29616;&#26377;&#30740;&#31350;&#20173;&#20381;&#36182;&#20110;&#20165;&#20855;&#26377;&#31616;&#21333;&#22270;&#20687;-&#25991;&#26412;&#20851;&#31995;&#21644;&#26816;&#32034;&#20219;&#21153;&#29992;&#25143;&#27169;&#22411;&#19981;&#36275;&#30340;&#22270;&#20687;&#26631;&#39064;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#20123;&#36807;&#24230;&#31616;&#21270;&#30340;&#35774;&#32622;&#21644;&#22810;&#23186;&#20307;&#20869;&#23481;&#21019;&#24314;&#30340;&#30495;&#23454;&#24212;&#29992;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26500;&#24314;&#26816;&#32034;&#27979;&#35797;&#38598;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#21033;&#29992;&#32500;&#22522;&#30334;&#31185;&#20013;&#23884;&#20837;&#30340;&#22823;&#35268;&#27169;&#22270;&#20687;-&#25991;&#26723;&#20851;&#32852;&#65292;&#24314;&#31435;&#20102;&#21253;&#25324;&#20998;&#23618;&#32467;&#26500;&#12289;&#25991;&#26412;&#26679;&#24335;&#21644;&#31867;&#22411;&#22312;&#20869;&#30340;&#22810;&#26679;&#21270;&#39046;&#22495;&#30340;&#25991;&#26412;&#21644;&#22270;&#29255;&#12290;&#25105;&#20204;&#22522;&#20110;&#19968;&#20010;&#29616;&#23454;&#30340;&#29992;&#25143;&#27169;&#22411;&#21046;&#23450;&#20102;&#20004;&#20010;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#22522;&#32447;&#27169;&#22411;&#30340;&#26816;&#32034;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#12290;AToMiC&#20026;&#21487;&#25193;&#23637;&#12289;&#22810;&#26679;&#21270;&#12289;&#21487;&#22797;&#29616;&#30340;&#22810;&#23186;&#20307;&#26816;&#32034;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#27979;&#35797;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents the AToMiC (Authoring Tools for Multimedia Content) dataset, designed to advance research in image/text cross-modal retrieval. While vision-language pretrained transformers have led to significant improvements in retrieval effectiveness, existing research has relied on image-caption datasets that feature only simplistic image-text relationships and underspecified user models of retrieval tasks. To address the gap between these oversimplified settings and real-world applications for multimedia content creation, we introduce a new approach for building retrieval test collections. We leverage hierarchical structures and diverse domains of texts, styles, and types of images, as well as large-scale image-document associations embedded in Wikipedia. We formulate two tasks based on a realistic user model and validate our dataset through retrieval experiments using baseline models. AToMiC offers a testbed for scalable, diverse, and reproducible multimedia retrieval research
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;N-CODH&#30340;&#21021;&#22987;&#26412;&#20307;&#35770;&#65292;&#26088;&#22312;&#32479;&#19968;&#25152;&#26377;&#38750;&#20020;&#24202;&#20915;&#23450;&#22240;&#32032;&#65292;&#24182;&#23558;&#21830;&#19994;&#21644;&#31038;&#20250;&#20915;&#23450;&#22240;&#32032;&#34701;&#21512;&#21040;&#19968;&#20010;&#32479;&#19968;&#30340;&#32467;&#26500;&#20013;&#12290;</title><link>http://arxiv.org/abs/2304.01446</link><description>&lt;p&gt;
&#23558;&#21830;&#19994;&#21644;&#31038;&#20250;&#20915;&#23450;&#22240;&#32032;&#34701;&#21512;: &#38750;&#20020;&#24202;&#20915;&#23450;&#22240;&#32032;&#30340;&#32479;&#19968;&#26412;&#20307;&#35770;
&lt;/p&gt;
&lt;p&gt;
Integrating Commercial and Social Determinants of Health: A Unified Ontology for Non-Clinical Determinants of Health. (arXiv:2304.01446v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01446
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;N-CODH&#30340;&#21021;&#22987;&#26412;&#20307;&#35770;&#65292;&#26088;&#22312;&#32479;&#19968;&#25152;&#26377;&#38750;&#20020;&#24202;&#20915;&#23450;&#22240;&#32032;&#65292;&#24182;&#23558;&#21830;&#19994;&#21644;&#31038;&#20250;&#20915;&#23450;&#22240;&#32032;&#34701;&#21512;&#21040;&#19968;&#20010;&#32479;&#19968;&#30340;&#32467;&#26500;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#21033;&#29992;PubMed&#25991;&#31456;&#21644;ChatGPT&#24320;&#21457;&#21830;&#19994;&#20915;&#23450;&#22240;&#32032;&#30340;&#26412;&#20307;&#35770;&#65292;&#23558;&#20854;&#19982;&#29616;&#26377;&#30340;&#31038;&#20250;&#20915;&#23450;&#22240;&#32032;&#26412;&#20307;&#35770;&#32467;&#21512;&#36215;&#26469;&#65292;&#24418;&#25104;&#19968;&#20010;&#32479;&#19968;&#30340;&#32467;&#26500;&#65292;&#20026;&#25152;&#26377;&#30340;&#38750;&#20020;&#24202;&#20915;&#23450;&#22240;&#32032;&#26500;&#24314;&#19968;&#20010;&#26368;&#21021;&#30340;&#26412;&#20307;&#35770;&#24182;&#39564;&#35777;ChatGPT&#25552;&#20379;&#30340;&#27010;&#24565;&#19982;&#29616;&#26377;&#31038;&#20250;&#20915;&#23450;&#22240;&#32032;&#26412;&#20307;&#35770;&#20043;&#38388;&#30340;&#30456;&#24212;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
The objectives of this research are 1) to develop an ontology for CDoH by utilizing PubMed articles and ChatGPT; 2) to foster ontology reuse by integrating CDoH with an existing SDoH ontology into a unified structure; 3) to devise an overarching conception for all non-clinical determinants of health and to create an initial ontology, called N-CODH, for them; 4) and to validate the degree of correspondence between concepts provided by ChatGPT with the existing SDoH ontology
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#36328;&#35821;&#35328;&#25220;&#34989;&#26816;&#27979;&#26041;&#27861;&#65292;&#19981;&#20381;&#36182;&#26426;&#22120;&#32763;&#35793;&#21644;&#35789;&#20041;&#28040;&#27495;&#65292;&#20351;&#29992;&#24320;&#25918;&#30340;&#22810;&#35821;&#35328;&#21516;&#20041;&#35789;&#24211;&#36827;&#34892;&#20505;&#36873;&#26816;&#32034;&#20219;&#21153;&#21644;&#39044;&#35757;&#32451;&#30340;&#22522;&#20110;&#22810;&#35821;&#35328;BERT&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35814;&#32454;&#20998;&#26512;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.01352</link><description>&lt;p&gt;
&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#36328;&#35821;&#35328;&#25220;&#34989;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Simple and Effective Method of Cross-Lingual Plagiarism Detection. (arXiv:2304.01352v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01352
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#36328;&#35821;&#35328;&#25220;&#34989;&#26816;&#27979;&#26041;&#27861;&#65292;&#19981;&#20381;&#36182;&#26426;&#22120;&#32763;&#35793;&#21644;&#35789;&#20041;&#28040;&#27495;&#65292;&#20351;&#29992;&#24320;&#25918;&#30340;&#22810;&#35821;&#35328;&#21516;&#20041;&#35789;&#24211;&#36827;&#34892;&#20505;&#36873;&#26816;&#32034;&#20219;&#21153;&#21644;&#39044;&#35757;&#32451;&#30340;&#22522;&#20110;&#22810;&#35821;&#35328;BERT&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35814;&#32454;&#20998;&#26512;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#36328;&#35821;&#35328;&#25220;&#34989;&#26816;&#27979;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#22823;&#37327;&#30340;&#35821;&#35328;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#24320;&#25918;&#30340;&#22810;&#35821;&#35328;&#21516;&#20041;&#35789;&#24211;&#36827;&#34892;&#20505;&#36873;&#26816;&#32034;&#20219;&#21153;&#65292;&#24182;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22522;&#20110;&#22810;&#35821;&#35328;BERT&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35814;&#32454;&#20998;&#26512;&#12290;&#35813;&#26041;&#27861;&#22312;&#20351;&#29992;&#26102;&#19981;&#20381;&#36182;&#26426;&#22120;&#32763;&#35793;&#21644;&#35789;&#20041;&#28040;&#27495;&#65292;&#22240;&#27492;&#36866;&#29992;&#20110;&#35768;&#22810;&#35821;&#35328;&#65292;&#21253;&#25324;&#36164;&#28304;&#21294;&#20047;&#30340;&#35821;&#35328;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#29616;&#26377;&#21644;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#65292;&#22312;&#27861;&#35821;&#12289;&#20420;&#35821;&#21644;&#20122;&#32654;&#23612;&#20122;&#35821;&#31561;&#35821;&#35328;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a simple cross-lingual plagiarism detection method applicable to a large number of languages. The presented approach leverages open multilingual thesauri for candidate retrieval task and pre-trained multilingual BERT-based language models for detailed analysis. The method does not rely on machine translation and word sense disambiguation when in use, and therefore is suitable for a large number of languages, including under-resourced languages. The effectiveness of the proposed approach is demonstrated for several existing and new benchmarks, achieving state-of-the-art results for French, Russian, and Armenian languages.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#36138;&#24515;&#31574;&#30053;&#30340;&#36335;&#32447;&#25512;&#33616;&#26041;&#27861;&#65292;&#21487;&#20197;&#22686;&#21152;&#36710;&#36742;&#21033;&#29992;&#29575;&#65292;&#32531;&#35299;&#25340;&#36710;&#26381;&#21153;&#23545;&#29615;&#22659;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2304.01225</link><description>&lt;p&gt;
&#22522;&#20110;&#36138;&#24515;&#31574;&#30053;&#25552;&#39640;&#25340;&#36710;&#32593;&#32476;&#20013;&#30340;&#36710;&#36742;&#21033;&#29992;&#29575;
&lt;/p&gt;
&lt;p&gt;
A greedy approach for increased vehicle utilization in ridesharing networks. (arXiv:2304.01225v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01225
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#36138;&#24515;&#31574;&#30053;&#30340;&#36335;&#32447;&#25512;&#33616;&#26041;&#27861;&#65292;&#21487;&#20197;&#22686;&#21152;&#36710;&#36742;&#21033;&#29992;&#29575;&#65292;&#32531;&#35299;&#25340;&#36710;&#26381;&#21153;&#23545;&#29615;&#22659;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#25340;&#36710;&#24179;&#21488;&#24050;&#25104;&#20026;&#22478;&#24066;&#23621;&#27665;&#30340;&#20027;&#35201;&#20132;&#36890;&#26041;&#24335;&#12290;&#23545;&#20110;&#36825;&#20123;&#24179;&#21488;&#26469;&#35762;&#65292;&#36335;&#32447;&#25512;&#33616;&#26159;&#19968;&#20010;&#33267;&#20851;&#37325;&#35201;&#30340;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#30740;&#31350;&#24050;&#32463;&#24314;&#35758;&#20102;&#20855;&#26377;&#26356;&#39640;&#20056;&#23458;&#38656;&#27714;&#30340;&#36335;&#32447;&#12290;&#28982;&#32780;&#65292;&#32479;&#35745;&#25968;&#25454;&#34920;&#26126;&#65292;&#19982;&#31169;&#20154;&#36710;&#36742;&#30456;&#27604;&#65292;&#36825;&#20123;&#26381;&#21153;&#20250;&#23548;&#33268;&#22686;&#21152;&#28201;&#23460;&#27668;&#20307;&#25490;&#25918;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#23547;&#25214;&#20056;&#23458;&#26102;&#22235;&#22788;&#28459;&#28216;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#25340;&#36710;&#31995;&#32479;&#21151;&#33021;&#30340;&#26356;&#35814;&#32454;&#32454;&#33410;&#65292;&#24182;&#25581;&#31034;&#20102;&#22312;&#25340;&#36710;&#31995;&#32479;&#34028;&#21187;&#21457;&#23637;&#30340;&#24773;&#20917;&#19979;&#23427;&#20204;&#24182;&#26410;&#26377;&#25928;&#22320;&#21033;&#29992;&#36710;&#36742;&#23481;&#37327;&#12290;&#25105;&#20204;&#24314;&#35758;&#20811;&#26381;&#20197;&#19978;&#38480;&#21046;&#65292;&#24182;&#25512;&#33616;&#21516;&#26102;&#33719;&#21462;&#22810;&#20010;&#20056;&#23458;&#30340;&#36335;&#32447;&#65292;&#20174;&#32780;&#22686;&#21152;&#36710;&#36742;&#21033;&#29992;&#29575;&#65292;&#20174;&#32780;&#20943;&#23569;&#36825;&#20123;&#31995;&#32479;&#23545;&#29615;&#22659;&#30340;&#24433;&#21709;&#12290;&#30001;&#20110;&#36335;&#32447;&#25512;&#33616;&#26159;NP-hard&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;k&#36339;&#30340;&#28369;&#21160;&#31383;&#21475;&#36817;&#20284;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, ridesharing platforms have become a prominent mode of transportation for the residents of urban areas. As a fundamental problem, route recommendation for these platforms is vital for their sustenance. The works done in this direction have recommended routes with higher passenger demand. Despite the existing works, statistics have suggested that these services cause increased greenhouse emissions compared to private vehicles as they roam around in search of riders. This analysis provides finer details regarding the functionality of ridesharing systems and it reveals that in the face of their boom, they have not utilized the vehicle capacity efficiently. We propose to overcome the above limitations and recommend routes that will fetch multiple passengers simultaneously which will result in increased vehicle utilization and thereby decrease the effect of these systems on the environment. As route recommendation is NP-hard, we propose a k-hop-based sliding window approxima
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#8220;&#22522;&#20110;&#25552;&#31034;&#30340;&#24320;&#25918;&#20851;&#31995;&#25277;&#21462;&#8221;&#27169;&#22411;&#65292;&#22312;&#26080;&#30417;&#30563;&#35774;&#32622;&#19979;&#19981;&#38656;&#35201;&#36229;&#21442;&#25968;&#35843;&#25972;&#65292;&#23454;&#29616;&#20102;&#20840;&#26032;&#30340;&#26080;&#30417;&#30563;&#20851;&#31995;&#25277;&#21462;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.01209</link><description>&lt;p&gt;
PromptORE -- &#19968;&#31181;&#20840;&#26032;&#30340;&#26080;&#30417;&#30563;&#20851;&#31995;&#25277;&#21462;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
PromptORE -- A Novel Approach Towards Fully Unsupervised Relation Extraction. (arXiv:2304.01209v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01209
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#8220;&#22522;&#20110;&#25552;&#31034;&#30340;&#24320;&#25918;&#20851;&#31995;&#25277;&#21462;&#8221;&#27169;&#22411;&#65292;&#22312;&#26080;&#30417;&#30563;&#35774;&#32622;&#19979;&#19981;&#38656;&#35201;&#36229;&#21442;&#25968;&#35843;&#25972;&#65292;&#23454;&#29616;&#20102;&#20840;&#26032;&#30340;&#26080;&#30417;&#30563;&#20851;&#31995;&#25277;&#21462;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#20851;&#31995;&#25277;&#21462;&#26088;&#22312;&#35782;&#21035;&#25991;&#26412;&#20013;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#32780;&#22312;&#35757;&#32451;&#26399;&#38388;&#27809;&#26377;&#26631;&#35760;&#30340;&#25968;&#25454;&#21487;&#29992;&#12290;&#36825;&#23545;&#20110;&#27809;&#26377;&#27880;&#37322;&#25968;&#25454;&#38598;&#30340;&#29305;&#23450;&#39046;&#22495;&#20851;&#31995;&#25277;&#21462;&#21644;&#20808;&#39564;&#26410;&#30693;&#20851;&#31995;&#31867;&#22411;&#30340;&#24320;&#25918;&#39046;&#22495;&#20851;&#31995;&#25277;&#21462;&#29305;&#21035;&#30456;&#20851;&#12290;&#34429;&#28982;&#26368;&#36817;&#30340;&#26041;&#27861;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#23427;&#20204;&#20005;&#37325;&#20381;&#36182;&#20110;&#36229;&#21442;&#25968;&#65292;&#35843;&#25972;&#36825;&#20123;&#36229;&#21442;&#25968;&#36890;&#24120;&#38656;&#35201;&#26631;&#35760;&#25968;&#25454;&#12290;&#20026;&#20102;&#20943;&#36731;&#23545;&#36229;&#21442;&#25968;&#30340;&#20381;&#36182;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PromptORE&#65292;&#21363;&#8220;&#22522;&#20110;&#25552;&#31034;&#30340;&#24320;&#25918;&#20851;&#31995;&#25277;&#21462;&#8221;&#27169;&#22411;&#12290;&#25105;&#20204;&#23558;&#26032;&#30340;&#25552;&#31034;&#35843;&#25972;&#33539;&#20363;&#36866;&#24212;&#20110;&#26080;&#30417;&#30563;&#35774;&#32622;&#65292;&#24182;&#29992;&#23427;&#26469;&#23884;&#20837;&#34920;&#36798;&#20851;&#31995;&#30340;&#21477;&#23376;&#12290;&#28982;&#21518;&#25105;&#20204;&#23545;&#36825;&#20123;&#23884;&#20837;&#36827;&#34892;&#32858;&#31867;&#65292;&#21457;&#29616;&#20505;&#36873;&#20851;&#31995;&#65292;&#24182;&#23581;&#35797;&#19981;&#21516;&#30340;&#31574;&#30053;&#26469;&#33258;&#21160;&#20272;&#35745;&#36866;&#24403;&#30340;&#32858;&#31867;&#25968;&#37327;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;PromptORE&#26159;&#31532;&#19968;&#20010;&#19981;&#38656;&#35201;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#26080;&#30417;&#30563;&#20851;&#31995;&#25277;&#21462;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Relation Extraction (RE) aims to identify relations between entities in text, without having access to labeled data during training. This setting is particularly relevant for domain specific RE where no annotated dataset is available and for open-domain RE where the types of relations are a priori unknown. Although recent approaches achieve promising results, they heavily depend on hyperparameters whose tuning would most often require labeled data. To mitigate the reliance on hyperparameters, we propose PromptORE, a ''Prompt-based Open Relation Extraction'' model. We adapt the novel prompt-tuning paradigm to work in an unsupervised setting, and use it to embed sentences expressing a relation. We then cluster these embeddings to discover candidate relations, and we experiment different strategies to automatically estimate an adequate number of clusters. To the best of our knowledge, PromptORE is the first unsupervised RE model that does not need hyperparameter tuning. Resul
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DiffuRec &#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#20854;&#23558;&#29289;&#21697;&#34920;&#31034;&#20026;&#20998;&#24067;&#32780;&#19981;&#26159;&#22266;&#23450;&#21521;&#37327;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#21453;&#26144;&#20102;&#29992;&#25143;&#30340;&#22810;&#31181;&#20559;&#22909;&#21644;&#29289;&#21697;&#30340;&#22810;&#20010;&#26041;&#38754;&#65292;&#24182;&#25104;&#21151;&#22320;&#24212;&#29992;&#20110;&#39034;&#24207;&#25512;&#33616;&#12290;</title><link>http://arxiv.org/abs/2304.00686</link><description>&lt;p&gt;
DiffuRec: &#19968;&#31181;&#29992;&#20110;&#39034;&#24207;&#25512;&#33616;&#30340;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DiffuRec: A Diffusion Model for Sequential Recommendation. (arXiv:2304.00686v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00686
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DiffuRec &#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#20854;&#23558;&#29289;&#21697;&#34920;&#31034;&#20026;&#20998;&#24067;&#32780;&#19981;&#26159;&#22266;&#23450;&#21521;&#37327;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#21453;&#26144;&#20102;&#29992;&#25143;&#30340;&#22810;&#31181;&#20559;&#22909;&#21644;&#29289;&#21697;&#30340;&#22810;&#20010;&#26041;&#38754;&#65292;&#24182;&#25104;&#21151;&#22320;&#24212;&#29992;&#20110;&#39034;&#24207;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#20915;&#39034;&#24207;&#25512;&#33616;&#30340;&#20027;&#27969;&#26041;&#27861;&#26159;&#20351;&#29992;&#22266;&#23450;&#21521;&#37327;&#26469;&#34920;&#31034;&#29289;&#21697;&#12290;&#36825;&#20123;&#21521;&#37327;&#22312;&#25429;&#25417;&#29289;&#21697;&#30340;&#28508;&#22312;&#26041;&#38754;&#21644;&#29992;&#25143;&#30340;&#22810;&#26679;&#21270;&#20559;&#22909;&#26041;&#38754;&#26041;&#38754;&#30340;&#33021;&#21147;&#26377;&#38480;&#12290;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#33539;&#24335;&#65292;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#39046;&#22495;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#24615;&#33021;&#12290;&#22312;&#25105;&#20204;&#30475;&#26469;&#65292;&#20854;&#22312;&#34920;&#24449;&#29983;&#25104;&#26041;&#38754;&#30340;&#29420;&#29305;&#20248;&#21183;&#24456;&#22909;&#22320;&#36866;&#24212;&#20102;&#39034;&#24207;&#25512;&#33616;&#30340;&#38382;&#39064;&#35774;&#32622;&#12290;&#26412;&#25991;&#39318;&#27425;&#23581;&#35797;&#23558;&#25193;&#25955;&#27169;&#22411;&#24212;&#29992;&#20110;&#39034;&#24207;&#25512;&#33616;&#65292;&#24182;&#25552;&#20986;&#20102;DiffuRec&#65292;&#29992;&#20110;&#29289;&#21697;&#34920;&#31034;&#26500;&#24314;&#21644;&#19981;&#30830;&#23450;&#24615;&#27880;&#20837;&#12290;&#19982;&#23558;&#29289;&#21697;&#34920;&#31034;&#24314;&#27169;&#20026;&#22266;&#23450;&#21521;&#37327;&#19981;&#21516;&#65292;&#25105;&#20204;&#22312;DiffuRec&#20013;&#23558;&#20854;&#34920;&#31034;&#20026;&#20998;&#24067;&#65292;&#36825;&#21453;&#26144;&#20102;&#29992;&#25143;&#30340;&#22810;&#37325;&#20852;&#36259;&#21644;&#29289;&#21697;&#30340;&#21508;&#20010;&#26041;&#38754;&#30340;&#36866;&#24212;&#24615;&#12290;&#22312;&#25193;&#25955;&#38454;&#27573;&#65292;DiffuRec&#36890;&#36807;&#28155;&#21152;&#22122;&#22768;&#23558;&#30446;&#26631;&#29289;&#21697;&#23884;&#20837;&#25104;&#39640;&#26031;&#20998;&#24067;&#65292;&#36827;&#19968;&#27493;&#24212;&#29992;&#20110;&#39034;&#24207;&#29289;&#21697;&#20998;&#24067;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mainstream solutions to Sequential Recommendation (SR) represent items with fixed vectors. These vectors have limited capability in capturing items' latent aspects and users' diverse preferences. As a new generative paradigm, Diffusion models have achieved excellent performance in areas like computer vision and natural language processing. To our understanding, its unique merit in representation generation well fits the problem setting of sequential recommendation. In this paper, we make the very first attempt to adapt Diffusion model to SR and propose DiffuRec, for item representation construction and uncertainty injection. Rather than modeling item representations as fixed vectors, we represent them as distributions in DiffuRec, which reflect user's multiple interests and item's various aspects adaptively. In diffusion phase, DiffuRec corrupts the target item embedding into a Gaussian distribution via noise adding, which is further applied for sequential item distribution representat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#33258;1992&#24180;&#20197;&#26469;&#65292;103&#31687;&#20851;&#20110;&#35780;&#23457;&#20154;&#33258;&#21160;&#20998;&#37197;&#30340;&#30740;&#31350;&#25104;&#26524;&#65292;&#24182;&#20171;&#32461;&#20102;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#28909;&#28857;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2304.00353</link><description>&lt;p&gt;
&#35780;&#23457;&#20154;&#20998;&#37197;&#38382;&#39064;&#65306;&#25991;&#29486;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Reviewer Assignment Problem: A Systematic Review of the Literature. (arXiv:2304.00353v1 [cs.DL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00353
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#33258;1992&#24180;&#20197;&#26469;&#65292;103&#31687;&#20851;&#20110;&#35780;&#23457;&#20154;&#33258;&#21160;&#20998;&#37197;&#30340;&#30740;&#31350;&#25104;&#26524;&#65292;&#24182;&#20171;&#32461;&#20102;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#28909;&#28857;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36866;&#24403;&#30340;&#35780;&#23457;&#20154;&#20998;&#37197;&#26174;&#33879;&#24433;&#21709;&#35780;&#20272;&#30340;&#36136;&#37327;&#65292;&#22240;&#20026;&#20934;&#30830;&#21644;&#20844;&#27491;&#30340;&#23457;&#26597;&#21462;&#20915;&#20110;&#23558;&#20854;&#20998;&#37197;&#32473;&#30456;&#20851;&#30340;&#35780;&#23457;&#20154;&#12290;&#23558;&#35780;&#23457;&#20154;&#20998;&#37197;&#32473;&#25552;&#20132;&#30340;&#25552;&#26696;&#26159;&#23457;&#26597;&#36807;&#31243;&#30340;&#36215;&#28857;&#65292;&#20063;&#31216;&#20026;&#35780;&#23457;&#20154;&#20998;&#37197;&#38382;&#39064;&#65288;RAP&#65289;&#12290;&#30001;&#20110;&#25163;&#21160;&#20998;&#37197;&#30340;&#26126;&#26174;&#38480;&#21046;&#65292;&#26399;&#21002;&#32534;&#36753;&#12289;&#20250;&#35758;&#32452;&#32455;&#32773;&#21644;&#25320;&#27454;&#32463;&#29702;&#35201;&#27714;&#33258;&#21160;&#35780;&#23457;&#20154;&#20998;&#37197;&#26041;&#27861;&#12290;&#33258;1992&#24180;&#20197;&#26469;&#65292;&#35768;&#22810;&#30740;&#31350;&#25552;&#20986;&#20102;&#20998;&#37197;&#35299;&#20915;&#26041;&#26696;&#20197;&#21709;&#24212;&#33258;&#21160;&#21270;&#31243;&#24207;&#30340;&#38656;&#27714;&#12290;&#26412;&#27425;&#35843;&#26597;&#25253;&#21578;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#20026;&#23398;&#32773;&#21644;&#23454;&#36341;&#32773;&#25552;&#20379;&#20851;&#20110;RAP&#39046;&#22495;&#20013;&#21487;&#29992;&#30740;&#31350;&#30340;&#20840;&#38754;&#27010;&#36848;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#26412;&#25991;&#23545;&#36807;&#21435;&#19977;&#21313;&#24180;&#20869;&#21457;&#34920;&#22312;Web of Science&#12289;Scopus&#12289;ScienceDirect&#21644;Google Scholar&#31561;&#25968;&#25454;&#24211;&#20013;&#30340;103&#31687;&#35780;&#23457;&#20154;&#20998;&#37197;&#39046;&#22495;&#30340;&#20986;&#29256;&#29289;&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#31995;&#32479;&#32508;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
Appropriate reviewer assignment significantly impacts the quality of proposal evaluation, as accurate and fair reviews are contingent on their assignment to relevant reviewers. The crucial task of assigning reviewers to submitted proposals is the starting point of the review process and is also known as the reviewer assignment problem (RAP). Due to the obvious restrictions of manual assignment, journal editors, conference organizers, and grant managers demand automatic reviewer assignment approaches. Many studies have proposed assignment solutions in response to the demand for automated procedures since 1992. The primary objective of this survey paper is to provide scholars and practitioners with a comprehensive overview of available research on the RAP. To achieve this goal, this article presents an in-depth systematic review of 103 publications in the field of reviewer assignment published in the past three decades and available in the Web of Science, Scopus, ScienceDirect, Google Sc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#25512;&#33616;&#31995;&#32479;&#27169;&#24335;-Chat-Rec&#65292;&#36890;&#36807;&#23558;LLMs&#19982;&#23545;&#35805;&#24335;&#25512;&#33616;&#30456;&#32467;&#21512;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#25512;&#33616;&#31995;&#32479;&#20013;&#20114;&#21160;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;Chat-Rec&#33021;&#22815;&#26356;&#26377;&#25928;&#22320;&#23398;&#20064;&#29992;&#25143;&#20559;&#22909;&#65292;&#24182;&#22312;&#25512;&#33616;&#36807;&#31243;&#20013;&#24314;&#31435;&#29992;&#25143;-&#20135;&#21697;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#20855;&#26377;&#26356;&#22823;&#30340;&#36879;&#26126;&#24230;&#21644;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2303.14524</link><description>&lt;p&gt;
Chat-REC&#65306;&#38754;&#21521;&#20114;&#21160;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;LLM&#22686;&#24378;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Chat-REC: Towards Interactive and Explainable LLMs-Augmented Recommender System. (arXiv:2303.14524v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14524
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#25512;&#33616;&#31995;&#32479;&#27169;&#24335;-Chat-Rec&#65292;&#36890;&#36807;&#23558;LLMs&#19982;&#23545;&#35805;&#24335;&#25512;&#33616;&#30456;&#32467;&#21512;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#25512;&#33616;&#31995;&#32479;&#20013;&#20114;&#21160;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;Chat-Rec&#33021;&#22815;&#26356;&#26377;&#25928;&#22320;&#23398;&#20064;&#29992;&#25143;&#20559;&#22909;&#65292;&#24182;&#22312;&#25512;&#33616;&#36807;&#31243;&#20013;&#24314;&#31435;&#29992;&#25143;-&#20135;&#21697;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#20855;&#26377;&#26356;&#22823;&#30340;&#36879;&#26126;&#24230;&#21644;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#35299;&#20915;&#21508;&#31181;&#24212;&#29992;&#20219;&#21153;&#26041;&#38754;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#25512;&#33616;&#31995;&#32479;&#20173;&#38754;&#20020;&#24456;&#22823;&#30340;&#25361;&#25112;&#65292;&#22914;&#20114;&#21160;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#24046;&#65292;&#36825;&#23454;&#38469;&#19978;&#20063;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#30495;&#23454;&#19990;&#30028;&#31995;&#32479;&#20013;&#30340;&#24191;&#27867;&#37096;&#32626;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#27169;&#24335;&#65292;&#31216;&#20026;Chat-REC&#65288;ChatGPT&#22686;&#24378;&#25512;&#33616;&#31995;&#32479;&#65289;&#65292;&#36890;&#36807;&#23558;&#29992;&#25143;&#37197;&#32622;&#25991;&#20214;&#21644;&#21382;&#21490;&#20132;&#20114;&#36716;&#25442;&#20026;&#25552;&#31034;&#65292;&#21019;&#26032;&#22320;&#22686;&#24378;LLMs&#29992;&#20110;&#26500;&#24314;&#23545;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;&#12290;&#36890;&#36807;&#22312;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#65292;Chat-Rec&#34987;&#35777;&#26126;&#22312;&#23398;&#20064;&#29992;&#25143;&#20559;&#22909;&#21644;&#24314;&#31435;&#29992;&#25143;&#19982;&#20135;&#21697;&#20043;&#38388;&#30340;&#32852;&#31995;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#65292;&#36825;&#20063;&#20351;&#24471;&#25512;&#33616;&#36807;&#31243;&#26356;&#20855;&#20114;&#21160;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#27492;&#22806;&#65292;&#22312;Chat-Rec&#26694;&#26550;&#20869;&#65292;&#29992;&#25143;&#30340;&#20559;&#22909;&#21487;&#20197;&#36716;&#31227;&#21040;&#19981;&#21516;&#30340;&#20135;&#21697;&#36827;&#34892;&#36328;&#39046;&#22495;&#25512;&#33616;&#65292;&#24182;&#19988;&#22522;&#20110;&#25552;&#31034;&#30340;&#27880;&#20837;&#20801;&#35768;&#26356;&#22823;&#30340;&#36879;&#26126;&#24230;&#21644;&#23545;&#25512;&#33616;&#36807;&#31243;&#30340;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated their significant potential to be applied for addressing various application tasks. However, traditional recommender systems continue to face great challenges such as poor interactivity and explainability, which actually also hinder their broad deployment in real-world systems. To address these limitations, this paper proposes a novel paradigm called Chat-Rec (ChatGPT Augmented Recommender System) that innovatively augments LLMs for building conversational recommender systems by converting user profiles and historical interactions into prompts. Chat-Rec is demonstrated to be effective in learning user preferences and establishing connections between users and products through in-context learning, which also makes the recommendation process more interactive and explainable. What's more, within the Chat-Rec framework, user's preferences can transfer to different products for cross-domain recommendations, and prompt-based injection of informa
&lt;/p&gt;</description></item></channel></rss>