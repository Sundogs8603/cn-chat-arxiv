<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#23637;&#20256;&#23548;&#30693;&#35782;&#22270;&#23884;&#20837;&#26041;&#27861;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#22788;&#29702;&#24402;&#32435;&#25512;&#29702;&#20219;&#21153;&#12290;&#36890;&#36807;&#24341;&#20837;&#24191;&#20041;&#30340;&#35856;&#27874;&#25193;&#23637;&#65292;&#21033;&#29992;&#20256;&#23548;&#23884;&#20837;&#26041;&#27861;&#23398;&#20064;&#30340;&#34920;&#31034;&#26469;&#25512;&#26029;&#22312;&#25512;&#29702;&#26102;&#24341;&#20837;&#30340;&#26032;&#23454;&#20307;&#30340;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2309.03773</link><description>&lt;p&gt;
&#25193;&#23637;&#20256;&#23548;&#30693;&#35782;&#22270;&#23884;&#20837;&#27169;&#22411;&#29992;&#20110;&#24402;&#32435;&#36923;&#36753;&#20851;&#31995;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Extending Transductive Knowledge Graph Embedding Models for Inductive Logical Relational Inference. (arXiv:2309.03773v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03773
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#23637;&#20256;&#23548;&#30693;&#35782;&#22270;&#23884;&#20837;&#26041;&#27861;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#22788;&#29702;&#24402;&#32435;&#25512;&#29702;&#20219;&#21153;&#12290;&#36890;&#36807;&#24341;&#20837;&#24191;&#20041;&#30340;&#35856;&#27874;&#25193;&#23637;&#65292;&#21033;&#29992;&#20256;&#23548;&#23884;&#20837;&#26041;&#27861;&#23398;&#20064;&#30340;&#34920;&#31034;&#26469;&#25512;&#26029;&#22312;&#25512;&#29702;&#26102;&#24341;&#20837;&#30340;&#26032;&#23454;&#20307;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#30693;&#35782;&#22270;&#30340;&#19979;&#28216;&#25512;&#29702;&#20219;&#21153;&#65292;&#20363;&#22914;&#20851;&#31995;&#39044;&#27979;&#65292;&#22312;&#20256;&#23548;&#35774;&#32622;&#19979;&#24050;&#32463;&#25104;&#21151;&#22788;&#29702;&#20102;&#12290;&#20026;&#20102;&#22788;&#29702;&#24402;&#32435;&#35774;&#32622;&#65292;&#20063;&#23601;&#26159;&#22312;&#25512;&#29702;&#26102;&#24341;&#20837;&#26032;&#23454;&#20307;&#21040;&#30693;&#35782;&#22270;&#20013;&#65292;&#36739;&#26032;&#30340;&#24037;&#20316;&#36873;&#25321;&#20102;&#36890;&#36807;&#32593;&#32476;&#23376;&#22270;&#32467;&#26500;&#30340;&#22797;&#26434;&#20989;&#25968;&#23398;&#20064;&#30693;&#35782;&#22270;&#30340;&#38544;&#24335;&#34920;&#31034;&#30340;&#27169;&#22411;&#65292;&#36890;&#24120;&#30001;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#21442;&#25968;&#21270;&#12290;&#36825;&#20123;&#27169;&#22411;&#30340;&#25104;&#26412;&#26159;&#22686;&#21152;&#30340;&#21442;&#25968;&#21270;&#12289;&#38477;&#20302;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#23545;&#20854;&#20182;&#19979;&#28216;&#25512;&#29702;&#20219;&#21153;&#30340;&#26377;&#38480;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#24191;&#20041;&#30340;&#35856;&#27874;&#25193;&#23637;&#26469;&#24357;&#21512;&#20256;&#32479;&#20256;&#23548;&#30693;&#35782;&#22270;&#23884;&#20837;&#26041;&#27861;&#21644;&#36739;&#26032;&#30340;&#24402;&#32435;&#20851;&#31995;&#39044;&#27979;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#36890;&#36807;&#21033;&#29992;&#36890;&#36807;&#20256;&#23548;&#23884;&#20837;&#26041;&#27861;&#23398;&#20064;&#30340;&#34920;&#31034;&#26469;&#25512;&#26029;&#22312;&#25512;&#29702;&#26102;&#24341;&#20837;&#30340;&#26032;&#23454;&#20307;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many downstream inference tasks for knowledge graphs, such as relation prediction, have been handled successfully by knowledge graph embedding techniques in the transductive setting. To address the inductive setting wherein new entities are introduced into the knowledge graph at inference time, more recent work opts for models which learn implicit representations of the knowledge graph through a complex function of a network's subgraph structure, often parametrized by graph neural network architectures. These come at the cost of increased parametrization, reduced interpretability and limited generalization to other downstream inference tasks. In this work, we bridge the gap between traditional transductive knowledge graph embedding approaches and more recent inductive relation prediction models by introducing a generalized form of harmonic extension which leverages representations learned through transductive embedding methods to infer representations of new entities introduced at infe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;VideolandGPT&#25913;&#36827;&#20102;&#20250;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;&#65292;&#23454;&#39564;&#34920;&#26126;&#20010;&#24615;&#21270;&#29256;&#26412;&#22312;&#20934;&#30830;&#24615;&#21644;&#29992;&#25143;&#28385;&#24847;&#24230;&#26041;&#38754;&#20248;&#20110;&#38750;&#20010;&#24615;&#21270;&#29256;&#26412;&#65292;&#20294;&#20004;&#20010;&#29256;&#26412;&#22312;&#20844;&#24179;&#24615;&#26041;&#38754;&#23384;&#22312;&#19981;&#19968;&#33268;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2309.03645</link><description>&lt;p&gt;
VideolandGPT&#65306;&#20851;&#20110;&#20250;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;&#30340;&#29992;&#25143;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
VideolandGPT: A User Study on a Conversational Recommender System. (arXiv:2309.03645v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03645
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;VideolandGPT&#25913;&#36827;&#20102;&#20250;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;&#65292;&#23454;&#39564;&#34920;&#26126;&#20010;&#24615;&#21270;&#29256;&#26412;&#22312;&#20934;&#30830;&#24615;&#21644;&#29992;&#25143;&#28385;&#24847;&#24230;&#26041;&#38754;&#20248;&#20110;&#38750;&#20010;&#24615;&#21270;&#29256;&#26412;&#65292;&#20294;&#20004;&#20010;&#29256;&#26412;&#22312;&#20844;&#24179;&#24615;&#26041;&#38754;&#23384;&#22312;&#19981;&#19968;&#33268;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22686;&#24378;&#25512;&#33616;&#31995;&#32479;&#65292;&#37325;&#28857;&#20851;&#27880;&#21033;&#29992;&#29992;&#25143;&#20559;&#22909;&#21644;&#29616;&#26377;&#25490;&#21517;&#27169;&#22411;&#30340;&#20010;&#24615;&#21270;&#20505;&#36873;&#36873;&#25321;&#30340;&#20250;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;VideolandGPT&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35270;&#39057;&#28857;&#25773;&#24179;&#21488;Videoland&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#23427;&#20351;&#29992;ChatGPT&#20174;&#39044;&#23450;&#20869;&#23481;&#38598;&#21512;&#20013;&#36827;&#34892;&#36873;&#25321;&#65292;&#32771;&#34385;&#21040;&#29992;&#25143;&#19982;&#32842;&#22825;&#30028;&#38754;&#30340;&#20132;&#20114;&#25152;&#31034;&#30340;&#39069;&#22806;&#19978;&#19979;&#25991;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#39033;&#29992;&#25143;&#30740;&#31350;&#65292;&#27604;&#36739;&#20102;&#20010;&#24615;&#21270;&#21644;&#38750;&#20010;&#24615;&#21270;&#29256;&#26412;&#30340;&#31995;&#32479;&#22312;&#25490;&#21517;&#25351;&#26631;&#12289;&#29992;&#25143;&#20307;&#39564;&#21644;&#25512;&#33616;&#30340;&#20844;&#24179;&#24615;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20010;&#24615;&#21270;&#29256;&#26412;&#22312;&#20934;&#30830;&#24615;&#21644;&#19968;&#33324;&#29992;&#25143;&#28385;&#24847;&#24230;&#26041;&#38754;&#20248;&#20110;&#38750;&#20010;&#24615;&#21270;&#29256;&#26412;&#65292;&#32780;&#20004;&#20010;&#29256;&#26412;&#37117;&#22686;&#21152;&#20102;&#25490;&#21517;&#25512;&#33616;&#21015;&#34920;&#20013;&#38750;&#21069;&#21015;&#30340;&#39033;&#30446;&#30340;&#21487;&#35265;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#20844;&#24179;&#24615;&#26041;&#38754;&#65292;&#20004;&#20010;&#29256;&#26412;&#30340;&#34892;&#20026;&#37117;&#19981;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates how large language models (LLMs) can enhance recommender systems, with a specific focus on Conversational Recommender Systems that leverage user preferences and personalised candidate selections from existing ranking models. We introduce VideolandGPT, a recommender system for a Video-on-Demand (VOD) platform, Videoland, which uses ChatGPT to select from a predetermined set of contents, considering the additional context indicated by users' interactions with a chat interface. We evaluate ranking metrics, user experience, and fairness of recommendations, comparing a personalised and a non-personalised version of the system, in a between-subject user study. Our results indicate that the personalised version outperforms the non-personalised in terms of accuracy and general user satisfaction, while both versions increase the visibility of items which are not in the top of the recommendation lists. However, both versions present inconsistent behavior in terms of fairn
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35780;&#20272;&#20102;ChatGPT&#20316;&#20026;&#25512;&#33616;&#31995;&#32479;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#25506;&#32034;&#20854;&#21033;&#29992;&#29992;&#25143;&#20559;&#22909;&#36827;&#34892;&#25512;&#33616;&#12289;&#37325;&#26032;&#25490;&#24207;&#25512;&#33616;&#21015;&#34920;&#12289;&#21033;&#29992;&#30456;&#20284;&#29992;&#25143;&#20449;&#24687;&#20197;&#21450;&#22788;&#29702;&#20919;&#21551;&#21160;&#24773;&#20917;&#30340;&#33021;&#21147;&#65292;&#24182;&#20351;&#29992;&#19977;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#20840;&#38754;&#23454;&#39564;&#12290;</title><link>http://arxiv.org/abs/2309.03613</link><description>&lt;p&gt;
&#35780;&#20272;ChatGPT&#20316;&#20026;&#25512;&#33616;&#31995;&#32479;&#30340;&#20005;&#35880;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Evaluating ChatGPT as a Recommender System: A Rigorous Approach. (arXiv:2309.03613v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03613
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35780;&#20272;&#20102;ChatGPT&#20316;&#20026;&#25512;&#33616;&#31995;&#32479;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#25506;&#32034;&#20854;&#21033;&#29992;&#29992;&#25143;&#20559;&#22909;&#36827;&#34892;&#25512;&#33616;&#12289;&#37325;&#26032;&#25490;&#24207;&#25512;&#33616;&#21015;&#34920;&#12289;&#21033;&#29992;&#30456;&#20284;&#29992;&#25143;&#20449;&#24687;&#20197;&#21450;&#22788;&#29702;&#20919;&#21551;&#21160;&#24773;&#20917;&#30340;&#33021;&#21147;&#65292;&#24182;&#20351;&#29992;&#19977;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#20840;&#38754;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#21331;&#36234;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#33021;&#21147;&#65292;&#22823;&#22411;AI&#35821;&#35328;&#27169;&#22411;&#36817;&#24180;&#26469;&#22791;&#21463;&#20851;&#27880;&#12290;&#23427;&#20204;&#22312;&#35821;&#35328;&#30456;&#20851;&#20219;&#21153;&#20013;&#20855;&#26377;&#37325;&#35201;&#36129;&#29486;&#65292;&#21253;&#25324;&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#65292;&#22240;&#27492;&#23545;&#20110;&#21508;&#31181;&#29305;&#23450;&#20219;&#21153;&#38750;&#24120;&#26377;&#20215;&#20540;&#12290;&#36825;&#31181;&#26041;&#27861;&#37322;&#25918;&#20102;&#23427;&#20204;&#30340;&#20840;&#37096;&#28508;&#21147;&#65292;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#21644;&#27867;&#21270;&#24615;&#12290;&#30740;&#31350;&#30028;&#27491;&#22312;&#31215;&#26497;&#25506;&#32034;&#23427;&#20204;&#30340;&#24212;&#29992;&#65292;ChatGPT&#20063;&#22240;&#27492;&#33719;&#24471;&#20102;&#35748;&#21487;&#12290;&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#26377;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#65292;&#20294;&#20854;&#22312;&#25512;&#33616;&#22330;&#26223;&#20013;&#30340;&#28508;&#21147;&#20173;&#24453;&#25506;&#32034;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#36890;&#36807;&#25506;&#31350;ChatGPT&#20316;&#20026;&#38646;-shot&#25512;&#33616;&#31995;&#32479;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#21253;&#25324;&#35780;&#20272;&#20854;&#21033;&#29992;&#29992;&#25143;&#20559;&#22909;&#36827;&#34892;&#25512;&#33616;&#12289;&#37325;&#26032;&#25490;&#24207;&#29616;&#26377;&#25512;&#33616;&#21015;&#34920;&#12289;&#21033;&#29992;&#30456;&#20284;&#29992;&#25143;&#30340;&#20449;&#24687;&#20197;&#21450;&#22788;&#29702;&#20919;&#21551;&#21160;&#24773;&#20917;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#19977;&#20010;&#25968;&#25454;&#38598;&#65288;MovieLens Small&#12289;Last.FM&#21644;Facebook Bo&#65289;&#36827;&#34892;&#20840;&#38754;&#23454;&#39564;&#26469;&#35780;&#20272;ChatGPT&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent popularity surrounds large AI language models due to their impressive natural language capabilities. They contribute significantly to language-related tasks, including prompt-based learning, making them valuable for various specific tasks. This approach unlocks their full potential, enhancing precision and generalization. Research communities are actively exploring their applications, with ChatGPT receiving recognition. Despite extensive research on large language models, their potential in recommendation scenarios still needs to be explored. This study aims to fill this gap by investigating ChatGPT's capabilities as a zero-shot recommender system. Our goals include evaluating its ability to use user preferences for recommendations, reordering existing recommendation lists, leveraging information from similar users, and handling cold-start situations. We assess ChatGPT's performance through comprehensive experiments using three datasets (MovieLens Small, Last.FM, and Facebook Bo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#30340;&#26032;&#22411;&#32039;&#20945;&#23884;&#20837;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#27491;&#21017;&#21270;&#20462;&#21098;&#30340;&#26041;&#24335;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#29615;&#22659;&#20013;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#20869;&#23384;&#25928;&#29575;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#39640;&#20934;&#30830;&#24230;&#30340;&#25512;&#33616;&#12290;</title><link>http://arxiv.org/abs/2309.03518</link><description>&lt;p&gt;
&#36890;&#36807;&#27491;&#21017;&#21270;&#20462;&#21098;&#26469;&#23398;&#20064;&#32039;&#20945;&#30340;&#32452;&#21512;&#23884;&#20837;&#20197;&#29992;&#20110;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Learning Compact Compositional Embeddings via Regularized Pruning for Recommendation. (arXiv:2309.03518v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03518
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#30340;&#26032;&#22411;&#32039;&#20945;&#23884;&#20837;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#27491;&#21017;&#21270;&#20462;&#21098;&#30340;&#26041;&#24335;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#29615;&#22659;&#20013;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#20869;&#23384;&#25928;&#29575;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#39640;&#20934;&#30830;&#24230;&#30340;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28508;&#22312;&#22240;&#32032;&#27169;&#22411;&#26159;&#24403;&#20195;&#25512;&#33616;&#31995;&#32479;&#30340;&#20027;&#35201;&#25903;&#26609;&#65292;&#30001;&#20110;&#23427;&#20204;&#30340;&#24615;&#33021;&#20248;&#21183;&#65292;&#22312;&#36825;&#20123;&#27169;&#22411;&#20013;&#65292;&#27599;&#20010;&#23454;&#20307;&#65288;&#36890;&#24120;&#26159;&#29992;&#25143;/&#29289;&#21697;&#65289;&#38656;&#35201;&#29992;&#19968;&#20010;&#22266;&#23450;&#32500;&#24230;&#65288;&#20363;&#22914;128&#65289;&#30340;&#21807;&#19968;&#21521;&#37327;&#23884;&#20837;&#26469;&#34920;&#31034;&#12290;&#30001;&#20110;&#30005;&#23376;&#21830;&#21153;&#32593;&#31449;&#19978;&#29992;&#25143;&#21644;&#29289;&#21697;&#30340;&#25968;&#37327;&#24040;&#22823;&#65292;&#23884;&#20837;&#34920;&#26684;&#21487;&#20197;&#35828;&#26159;&#25512;&#33616;&#31995;&#32479;&#20013;&#26368;&#19981;&#33410;&#30465;&#20869;&#23384;&#30340;&#32452;&#20214;&#12290;&#23545;&#20110;&#20219;&#20309;&#24076;&#26395;&#33021;&#22815;&#26377;&#25928;&#22320;&#25353;&#27604;&#20363;&#25193;&#23637;&#21040;&#19981;&#26029;&#22686;&#38271;&#30340;&#29992;&#25143;/&#29289;&#21697;&#25968;&#37327;&#25110;&#22312;&#36164;&#28304;&#21463;&#38480;&#29615;&#22659;&#20013;&#20173;&#28982;&#36866;&#29992;&#30340;&#36731;&#37327;&#32423;&#25512;&#33616;&#31995;&#32479;&#65292;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#35201;&#20040;&#36890;&#36807;&#21704;&#24076;&#20943;&#23569;&#25152;&#38656;&#30340;&#23884;&#20837;&#25968;&#37327;&#65292;&#35201;&#20040;&#36890;&#36807;&#31232;&#30095;&#21270;&#23436;&#25972;&#30340;&#23884;&#20837;&#34920;&#26684;&#20197;&#20851;&#38381;&#36873;&#23450;&#30340;&#23884;&#20837;&#32500;&#24230;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21704;&#24076;&#20914;&#31361;&#25110;&#23884;&#20837;&#36807;&#20110;&#31232;&#30095;&#65292;&#23588;&#20854;&#26159;&#22312;&#36866;&#24212;&#26356;&#32039;&#20945;&#30340;&#20869;&#23384;&#39044;&#31639;&#26102;&#65292;&#36825;&#20123;&#36731;&#37327;&#32423;&#25512;&#33616;&#22120;&#19981;&#21487;&#36991;&#20813;&#22320;&#20250;&#29306;&#29298;&#20854;&#20934;&#30830;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32039;&#20945;&#23884;&#20837;&#26694;&#26550;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#65292;&#31216;&#20026;Compos&#12290;
&lt;/p&gt;
&lt;p&gt;
Latent factor models are the dominant backbones of contemporary recommender systems (RSs) given their performance advantages, where a unique vector embedding with a fixed dimensionality (e.g., 128) is required to represent each entity (commonly a user/item). Due to the large number of users and items on e-commerce sites, the embedding table is arguably the least memory-efficient component of RSs. For any lightweight recommender that aims to efficiently scale with the growing size of users/items or to remain applicable in resource-constrained settings, existing solutions either reduce the number of embeddings needed via hashing, or sparsify the full embedding table to switch off selected embedding dimensions. However, as hash collision arises or embeddings become overly sparse, especially when adapting to a tighter memory budget, those lightweight recommenders inevitably have to compromise their accuracy. To this end, we propose a novel compact embedding framework for RSs, namely Compos
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#21442;&#19982;ACM&#20250;&#35758;&#30340;&#20316;&#32773;&#25152;&#23646;&#22269;&#23478;&#65292;&#25506;&#35752;&#20102;&#25512;&#33616;&#31995;&#32479;&#30740;&#31350;&#31038;&#21306;&#30340;&#22320;&#29702;&#22810;&#26679;&#24615;&#12290;&#36825;&#24378;&#35843;&#20102;&#22312;&#25512;&#33616;&#31995;&#32479;&#35774;&#35745;&#21644;&#24320;&#21457;&#30340;&#26089;&#26399;&#38454;&#27573;&#65292;&#38656;&#35201;&#28041;&#21450;&#26469;&#33258;&#19981;&#21516;&#32972;&#26223;&#30340;&#35266;&#28857;&#21644;&#22242;&#38431;&#30340;&#21442;&#19982;&#12290;</title><link>http://arxiv.org/abs/2309.03512</link><description>&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#30740;&#31350;&#20013;&#30340;&#22320;&#29702;&#22240;&#32032;&#65306;ACM RecSys&#31038;&#21306;&#30340;&#22320;&#29702;&#20998;&#24067;
&lt;/p&gt;
&lt;p&gt;
Behind Recommender Systems: the Geography of the ACM RecSys Community. (arXiv:2309.03512v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03512
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#21442;&#19982;ACM&#20250;&#35758;&#30340;&#20316;&#32773;&#25152;&#23646;&#22269;&#23478;&#65292;&#25506;&#35752;&#20102;&#25512;&#33616;&#31995;&#32479;&#30740;&#31350;&#31038;&#21306;&#30340;&#22320;&#29702;&#22810;&#26679;&#24615;&#12290;&#36825;&#24378;&#35843;&#20102;&#22312;&#25512;&#33616;&#31995;&#32479;&#35774;&#35745;&#21644;&#24320;&#21457;&#30340;&#26089;&#26399;&#38454;&#27573;&#65292;&#38656;&#35201;&#28041;&#21450;&#26469;&#33258;&#19981;&#21516;&#32972;&#26223;&#30340;&#35266;&#28857;&#21644;&#22242;&#38431;&#30340;&#21442;&#19982;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#22312;&#22312;&#32447;&#21487;&#35775;&#38382;&#30340;&#23186;&#20307;&#20869;&#23481;&#25968;&#37327;&#21644;&#20256;&#25773;&#36895;&#24230;&#26159;&#21387;&#20498;&#24615;&#30340;&#12290;&#25512;&#33616;&#31995;&#32479;&#21487;&#20197;&#23558;&#36825;&#20123;&#20449;&#24687;&#36807;&#28388;&#25104;&#36866;&#24212;&#25105;&#20204;&#20010;&#20154;&#38656;&#27714;&#25110;&#20559;&#22909;&#30340;&#21487;&#31649;&#29702;&#30340;&#27969;&#25110;&#21160;&#24577;&#12290;&#20294;&#26159;&#65292;&#24456;&#37325;&#35201;&#30340;&#19968;&#28857;&#26159;&#36807;&#28388;&#20449;&#24687;&#30340;&#31639;&#27861;&#19981;&#33021;&#25197;&#26354;&#25110;&#21066;&#20943;&#25105;&#20204;&#23545;&#19990;&#30028;&#30340;&#35266;&#28857;&#20013;&#30340;&#37325;&#35201;&#20803;&#32032;&#12290;&#26681;&#25454;&#36825;&#19968;&#21407;&#21017;&#65292;&#26368;&#26089;&#26399;&#30340;&#35774;&#35745;&#21644;&#24320;&#21457;&#38454;&#27573;&#24517;&#39035;&#28041;&#21450;&#22810;&#20803;&#21270;&#30340;&#35266;&#28857;&#21644;&#22242;&#38431;&#30340;&#21442;&#19982;&#12290;&#20363;&#22914;&#65292;&#26368;&#36817;&#27431;&#30431;&#30340;&#30456;&#20851;&#27861;&#35268;&#65288;&#22914;&#25968;&#23383;&#26381;&#21153;&#27861;&#26696;&#21644;AI&#27861;&#26696;&#65289;&#24378;&#35843;&#20102;&#39118;&#38505;&#30417;&#27979;&#65292;&#21253;&#25324;&#27495;&#35270;&#39118;&#38505;&#65292;&#24182;&#35201;&#27714;&#22312;AI&#31995;&#32479;&#30340;&#24320;&#21457;&#20013;&#21560;&#24341;&#22810;&#32972;&#26223;&#30340;&#20154;&#21442;&#19982;&#12290;&#26412;&#30740;&#31350;&#30528;&#30524;&#20110;&#25512;&#33616;&#31995;&#32479;&#30740;&#31350;&#31038;&#21306;&#30340;&#22320;&#29702;&#22810;&#26679;&#24615;&#65292;&#20855;&#20307;&#36890;&#36807;&#20998;&#26512;&#22312;ACM&#20250;&#35758;&#19978;&#36129;&#29486;&#35770;&#25991;&#30340;&#20316;&#32773;&#25152;&#23646;&#22269;&#23478;&#12290;
&lt;/p&gt;
&lt;p&gt;
The amount and dissemination rate of media content accessible online is nowadays overwhelming. Recommender Systems filter this information into manageable streams or feeds, adapted to our personal needs or preferences. It is of utter importance that algorithms employed to filter information do not distort or cut out important elements from our perspectives of the world. Under this principle, it is essential to involve diverse views and teams from the earliest stages of their design and development. This has been highlighted, for instance, in recent European Union regulations such as the Digital Services Act, via the requirement of risk monitoring, including the risk of discrimination, and the AI Act, through the requirement to involve people with diverse backgrounds in the development of AI systems. We look into the geographic diversity of the recommender systems research community, specifically by analyzing the affiliation countries of the authors who contributed to the ACM Conference
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21360;&#35937;&#24863;&#30693;&#30340;&#22810;&#34892;&#20026;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;&#27880;&#24847;&#26426;&#21046;&#20174;&#34892;&#20026;&#38388;&#21644;&#34892;&#20026;&#20869;&#37096;&#33719;&#21462;&#20449;&#24687;&#65292;&#24182;&#37319;&#29992;&#22810;&#23618;&#32423;&#22270;&#27880;&#24847;&#21147;&#26041;&#27861;&#65292;&#26469;&#35299;&#20915;&#25512;&#33616;&#31995;&#32479;&#22312;&#22788;&#29702;&#22810;&#20010;&#34892;&#20026;&#20043;&#38388;&#20114;&#21160;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.03169</link><description>&lt;p&gt;
&#22522;&#20110;&#21360;&#35937;&#24863;&#30693;&#30340;&#22810;&#34892;&#20026;&#25512;&#33616;&#31995;&#32479;&#65306;&#19968;&#31181;&#23618;&#27425;&#22270;&#27880;&#24847;&#21147;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Impression-Informed Multi-Behavior Recommender System: A Hierarchical Graph Attention Approach. (arXiv:2309.03169v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03169
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21360;&#35937;&#24863;&#30693;&#30340;&#22810;&#34892;&#20026;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;&#27880;&#24847;&#26426;&#21046;&#20174;&#34892;&#20026;&#38388;&#21644;&#34892;&#20026;&#20869;&#37096;&#33719;&#21462;&#20449;&#24687;&#65292;&#24182;&#37319;&#29992;&#22810;&#23618;&#32423;&#22270;&#27880;&#24847;&#21147;&#26041;&#27861;&#65292;&#26469;&#35299;&#20915;&#25512;&#33616;&#31995;&#32479;&#22312;&#22788;&#29702;&#22810;&#20010;&#34892;&#20026;&#20043;&#38388;&#20114;&#21160;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#25512;&#33616;&#31995;&#32479;&#20174;&#38544;&#24335;&#21453;&#39304;&#20013;&#33719;&#30410;&#33391;&#22810;&#65292;&#20294;&#24448;&#24448;&#20250;&#24573;&#30053;&#29992;&#25143;&#19982;&#29289;&#21697;&#20043;&#38388;&#30340;&#22810;&#34892;&#20026;&#20114;&#21160;&#30340;&#32454;&#24494;&#24046;&#21035;&#12290;&#21382;&#21490;&#19978;&#65292;&#36825;&#20123;&#31995;&#32479;&#35201;&#20040;&#23558;&#25152;&#26377;&#34892;&#20026;&#65292;&#22914;&#8220;&#21360;&#35937;&#8221;&#65288;&#20197;&#21069;&#31216;&#20026;&#8220;&#27983;&#35272;&#8221;&#65289;&#12289;&#8220;&#28155;&#21152;&#21040;&#36141;&#29289;&#36710;&#8221;&#21644;&#8220;&#36141;&#20080;&#8221;&#65292;&#24402;&#24182;&#20026;&#19968;&#20010;&#32479;&#19968;&#30340;&#8220;&#20114;&#21160;&#8221;&#26631;&#31614;&#65292;&#35201;&#20040;&#20165;&#20248;&#20808;&#32771;&#34385;&#30446;&#26631;&#34892;&#20026;&#65292;&#36890;&#24120;&#26159;&#8220;&#36141;&#20080;&#8221;&#34892;&#20026;&#65292;&#24182;&#20002;&#24323;&#26377;&#20215;&#20540;&#30340;&#36741;&#21161;&#20449;&#21495;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;&#36827;&#23637;&#35797;&#22270;&#35299;&#20915;&#36825;&#31181;&#31616;&#21270;&#65292;&#20294;&#23427;&#20204;&#20027;&#35201;&#38598;&#20013;&#20110;&#20248;&#21270;&#30446;&#26631;&#34892;&#20026;&#65292;&#19982;&#25968;&#25454;&#31232;&#32570;&#20316;&#26007;&#20105;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#24448;&#24448;&#32469;&#36807;&#20102;&#19982;&#34892;&#20026;&#20869;&#22312;&#23618;&#27425;&#32467;&#26500;&#26377;&#20851;&#30340;&#24494;&#22937;&#24046;&#24322;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#20123;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#8220;H&#8221;ierarchical &#8220;M&#8221;ulti-behavior &#8220;G&#8221;raph Attention &#8220;N&#8221;etwork&#65288;HMGN&#65289;&#12290;&#36825;&#20010;&#24320;&#21019;&#24615;&#30340;&#26694;&#26550;&#21033;&#29992;&#27880;&#24847;&#26426;&#21046;&#20174;&#34892;&#20026;&#38388;&#21644;&#34892;&#20026;&#20869;&#37096;&#33719;&#21462;&#20449;&#24687;&#65292;&#21516;&#26102;&#37319;&#29992;&#22810;
&lt;/p&gt;
&lt;p&gt;
While recommender systems have significantly benefited from implicit feedback, they have often missed the nuances of multi-behavior interactions between users and items. Historically, these systems either amalgamated all behaviors, such as \textit{impression} (formerly \textit{view}), \textit{add-to-cart}, and \textit{buy}, under a singular 'interaction' label, or prioritized only the target behavior, often the \textit{buy} action, discarding valuable auxiliary signals. Although recent advancements tried addressing this simplification, they primarily gravitated towards optimizing the target behavior alone, battling with data scarcity. Additionally, they tended to bypass the nuanced hierarchy intrinsic to behaviors. To bridge these gaps, we introduce the \textbf{H}ierarchical \textbf{M}ulti-behavior \textbf{G}raph Attention \textbf{N}etwork (HMGN). This pioneering framework leverages attention mechanisms to discern information from both inter and intra-behaviors while employing a multi-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ZC3&#30340;&#36328;&#35821;&#35328;&#38646;&#26679;&#26412;&#20195;&#30721;&#20811;&#38534;&#26816;&#27979;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#35774;&#35745;&#20102;&#23545;&#27604;&#20195;&#30721;&#29255;&#27573;&#39044;&#27979;&#65292;&#24418;&#25104;&#19981;&#21516;&#32534;&#31243;&#35821;&#35328;&#20043;&#38388;&#30340;&#21516;&#26500;&#34920;&#31034;&#31354;&#38388;&#65292;&#24182;&#21033;&#29992;&#39046;&#22495;&#24863;&#30693;&#23398;&#20064;&#21644;&#24490;&#29615;&#19968;&#33268;&#24615;&#23398;&#20064;&#26469;&#36827;&#19968;&#27493;&#32422;&#26463;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2308.13754</link><description>&lt;p&gt;
ZC3: &#36328;&#35821;&#35328;&#38646;&#26679;&#26412;&#20195;&#30721;&#20811;&#38534;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
ZC3: Zero-Shot Cross-Language Code Clone Detection. (arXiv:2308.13754v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13754
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ZC3&#30340;&#36328;&#35821;&#35328;&#38646;&#26679;&#26412;&#20195;&#30721;&#20811;&#38534;&#26816;&#27979;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#35774;&#35745;&#20102;&#23545;&#27604;&#20195;&#30721;&#29255;&#27573;&#39044;&#27979;&#65292;&#24418;&#25104;&#19981;&#21516;&#32534;&#31243;&#35821;&#35328;&#20043;&#38388;&#30340;&#21516;&#26500;&#34920;&#31034;&#31354;&#38388;&#65292;&#24182;&#21033;&#29992;&#39046;&#22495;&#24863;&#30693;&#23398;&#20064;&#21644;&#24490;&#29615;&#19968;&#33268;&#24615;&#23398;&#20064;&#26469;&#36827;&#19968;&#27493;&#32422;&#26463;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#20154;&#21592;&#24341;&#20837;&#20195;&#30721;&#20811;&#38534;&#20197;&#25552;&#39640;&#32534;&#31243;&#25928;&#29575;&#12290;&#35768;&#22810;&#29616;&#26377;&#30740;&#31350;&#22312;&#21333;&#35821;&#35328;&#20195;&#30721;&#20811;&#38534;&#26816;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#25104;&#26524;&#12290;&#28982;&#32780;&#65292;&#22312;&#36719;&#20214;&#24320;&#21457;&#36807;&#31243;&#20013;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#24320;&#21457;&#20154;&#21592;&#20351;&#29992;&#19981;&#21516;&#30340;&#35821;&#35328;&#32534;&#20889;&#35821;&#20041;&#19978;&#31561;&#20215;&#30340;&#31243;&#24207;&#65292;&#20197;&#25903;&#25345;&#19981;&#21516;&#30340;&#24179;&#21488;&#65292;&#24182;&#24110;&#21161;&#24320;&#21457;&#20154;&#21592;&#20174;&#19968;&#31181;&#35821;&#35328;&#32763;&#35793;&#39033;&#30446;&#21040;&#21478;&#19968;&#31181;&#35821;&#35328;&#12290;&#32771;&#34385;&#21040;&#25910;&#38598;&#36328;&#35821;&#35328;&#24182;&#34892;&#25968;&#25454;&#65288;&#23588;&#20854;&#26159;&#20302;&#36164;&#28304;&#35821;&#35328;&#65289;&#30340;&#25104;&#26412;&#39640;&#26114;&#19988;&#32791;&#26102;&#65292;&#35774;&#35745;&#19968;&#31181;&#19981;&#20381;&#36182;&#20219;&#20309;&#24182;&#34892;&#25968;&#25454;&#30340;&#26377;&#25928;&#36328;&#35821;&#35328;&#27169;&#22411;&#26159;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ZC3&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#20195;&#30721;&#20811;&#38534;&#26816;&#27979;&#12290;ZC3&#36890;&#36807;&#35774;&#35745;&#23545;&#27604;&#20195;&#30721;&#29255;&#27573;&#39044;&#27979;&#26469;&#24418;&#25104;&#19981;&#21516;&#32534;&#31243;&#35821;&#35328;&#20043;&#38388;&#30340;&#21516;&#26500;&#34920;&#31034;&#31354;&#38388;&#12290;&#22522;&#20110;&#27492;&#65292;ZC3&#21033;&#29992;&#39046;&#22495;&#24863;&#30693;&#23398;&#20064;&#21644;&#24490;&#29615;&#19968;&#33268;&#24615;&#23398;&#20064;&#36827;&#19968;&#27493;&#32422;&#26463;&#27169;&#22411;&#20197;&#29983;&#25104;&#34920;&#36798;&#12290;
&lt;/p&gt;
&lt;p&gt;
Developers introduce code clones to improve programming productivity. Many existing studies have achieved impressive performance in monolingual code clone detection. However, during software development, more and more developers write semantically equivalent programs with different languages to support different platforms and help developers translate projects from one language to another. Considering that collecting cross-language parallel data, especially for low-resource languages, is expensive and time-consuming, how designing an effective cross-language model that does not rely on any parallel data is a significant problem. In this paper, we propose a novel method named ZC3 for Zero-shot Cross-language Code Clone detection. ZC3 designs the contrastive snippet prediction to form an isomorphic representation space among different programming languages. Based on this, ZC3 exploits domain-aware learning and cycle consistency learning to further constrain the model to generate represen
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#22411;&#28151;&#21512;&#32593;&#32476;(RAHNet)&#29992;&#20110;&#38271;&#23614;&#22270;&#20998;&#31867;&#20219;&#21153;&#65292;&#36890;&#36807;&#32852;&#21512;&#23398;&#20064;&#31283;&#20581;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;&#26080;&#20559;&#30340;&#20998;&#31867;&#22120;&#65292;&#35299;&#20915;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#38271;&#23614;&#31867;&#21035;&#20998;&#24067;&#19979;&#30340;&#20559;&#24046;&#21644;&#27867;&#21270;&#33021;&#21147;&#26377;&#38480;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.02335</link><description>&lt;p&gt;
RAHNet: &#26816;&#32034;&#22686;&#24378;&#22411;&#28151;&#21512;&#32593;&#32476;&#29992;&#20110;&#38271;&#23614;&#22270;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
RAHNet: Retrieval Augmented Hybrid Network for Long-tailed Graph Classification. (arXiv:2308.02335v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02335
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#22411;&#28151;&#21512;&#32593;&#32476;(RAHNet)&#29992;&#20110;&#38271;&#23614;&#22270;&#20998;&#31867;&#20219;&#21153;&#65292;&#36890;&#36807;&#32852;&#21512;&#23398;&#20064;&#31283;&#20581;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;&#26080;&#20559;&#30340;&#20998;&#31867;&#22120;&#65292;&#35299;&#20915;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#38271;&#23614;&#31867;&#21035;&#20998;&#24067;&#19979;&#30340;&#20559;&#24046;&#21644;&#27867;&#21270;&#33021;&#21147;&#26377;&#38480;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20998;&#31867;&#26159;&#35768;&#22810;&#23454;&#38469;&#22810;&#23186;&#20307;&#24212;&#29992;&#20013;&#30340;&#20851;&#38190;&#20219;&#21153;&#65292;&#22270;&#21487;&#20197;&#34920;&#31034;&#21508;&#31181;&#22810;&#23186;&#20307;&#25968;&#25454;&#31867;&#22411;&#65292;&#22914;&#22270;&#20687;&#12289;&#35270;&#39057;&#21644;&#31038;&#20132;&#32593;&#32476;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#22312;&#24179;&#34913;&#30340;&#24773;&#20917;&#19979;&#24212;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#65292;&#20854;&#20013;&#31867;&#20998;&#24067;&#26159;&#24179;&#34913;&#30340;&#12290;&#28982;&#32780;&#65292;&#23454;&#38469;&#25968;&#25454;&#36890;&#24120;&#21576;&#29616;&#20986;&#38271;&#23614;&#31867;&#21035;&#20998;&#24067;&#65292;&#23548;&#33268;&#22312;&#20351;&#29992;GNN&#26102;&#23545;&#22836;&#37096;&#31867;&#21035;&#23384;&#22312;&#20559;&#24046;&#65292;&#19988;&#23545;&#23614;&#37096;&#31867;&#21035;&#30340;&#27867;&#21270;&#33021;&#21147;&#26377;&#38480;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#37325;&#26032;&#24179;&#34913;&#19981;&#21516;&#30340;&#31867;&#21035;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#26410;&#33021;&#26126;&#30830;&#24341;&#20837;&#26032;&#30693;&#35782;&#65292;&#24182;&#29306;&#29298;&#20102;&#22836;&#37096;&#31867;&#21035;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#32570;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;&#26816;&#32034;&#22686;&#24378;&#22411;&#28151;&#21512;&#32593;&#32476;(RAHNet)&#65292;&#20197;&#20998;&#31163;&#30340;&#26041;&#24335;&#32852;&#21512;&#23398;&#20064;&#31283;&#20581;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;&#26080;&#20559;&#30340;&#20998;&#31867;&#22120;&#12290;&#22312;&#29305;&#24449;&#25552;&#21462;&#22120;&#35757;&#32451;&#38454;&#27573;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22270;&#26816;&#32034;&#27169;&#22359;&#26469;&#25628;&#32034;&#30456;&#20851;&#22270;&#24418;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph classification is a crucial task in many real-world multimedia applications, where graphs can represent various multimedia data types such as images, videos, and social networks. Previous efforts have applied graph neural networks (GNNs) in balanced situations where the class distribution is balanced. However, real-world data typically exhibit long-tailed class distributions, resulting in a bias towards the head classes when using GNNs and limited generalization ability over the tail classes. Recent approaches mainly focus on re-balancing different classes during model training, which fails to explicitly introduce new knowledge and sacrifices the performance of the head classes. To address these drawbacks, we propose a novel framework called Retrieval Augmented Hybrid Network (RAHNet) to jointly learn a robust feature extractor and an unbiased classifier in a decoupled manner. In the feature extractor training stage, we develop a graph retrieval module to search for relevant grap
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#36807;&#21435;&#21313;&#24180;&#24320;&#25918;&#30693;&#35782;&#22270;&#35889;&#30340;&#23398;&#26415;&#30740;&#31350;&#36235;&#21183;&#21644;&#20027;&#39064;&#65292;&#24182;&#30830;&#23450;&#20102;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#21644;&#22686;&#24378;&#12289;&#35780;&#20272;&#21644;&#22797;&#29992;&#20197;&#21450;&#23558;&#30693;&#35782;&#22270;&#35889;&#34701;&#20837;NLP&#31995;&#32479;&#30340;&#19977;&#20010;&#20027;&#35201;&#30740;&#31350;&#20027;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.13186</link><description>&lt;p&gt;
&#24320;&#25918;&#30693;&#35782;&#22270;&#35889;&#30340;&#21313;&#24180;&#23398;&#26415;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Decade of Scholarly Research on Open Knowledge Graphs. (arXiv:2306.13186v1 [cs.DL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13186
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#36807;&#21435;&#21313;&#24180;&#24320;&#25918;&#30693;&#35782;&#22270;&#35889;&#30340;&#23398;&#26415;&#30740;&#31350;&#36235;&#21183;&#21644;&#20027;&#39064;&#65292;&#24182;&#30830;&#23450;&#20102;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#21644;&#22686;&#24378;&#12289;&#35780;&#20272;&#21644;&#22797;&#29992;&#20197;&#21450;&#23558;&#30693;&#35782;&#22270;&#35889;&#34701;&#20837;NLP&#31995;&#32479;&#30340;&#19977;&#20010;&#20027;&#35201;&#30740;&#31350;&#20027;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#21313;&#24180;&#38388;&#65292;&#24320;&#25918;&#30693;&#35782;&#22270;&#35889;&#30340;&#26222;&#21450;&#23548;&#33268;&#20102;&#23545;&#35813;&#35805;&#39064;&#30340;&#23398;&#26415;&#30740;&#31350;&#30340;&#28608;&#22686;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#38024;&#23545;2013&#24180;&#33267;2023&#24180;&#38388;&#20986;&#29256;&#30340;&#26377;&#20851;&#24320;&#25918;&#30693;&#35782;&#22270;&#35889;&#30340;&#23398;&#26415;&#25991;&#29486;&#30340;&#25991;&#29486;&#35745;&#37327;&#20998;&#26512;&#12290;&#35813;&#30740;&#31350;&#26088;&#22312;&#35782;&#21035;&#35813;&#39046;&#22495;&#20013;&#30340;&#36235;&#21183;&#65292;&#27169;&#24335;&#21644;&#30740;&#31350;&#30340;&#24433;&#21709;&#65292;&#20197;&#21450;&#20986;&#29616;&#30340;&#20851;&#38190;&#20027;&#39064;&#21644;&#30740;&#31350;&#38382;&#39064;&#12290;&#35813;&#20316;&#21697;&#20351;&#29992;&#25991;&#29486;&#35745;&#37327;&#25216;&#26415;&#20998;&#26512;&#20102;&#20174;Scopus&#26816;&#32034;&#21040;&#30340;4445&#31687;&#23398;&#26415;&#25991;&#31456;&#30340;&#26679;&#26412;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#27599;&#24180;&#20851;&#20110;&#24320;&#25918;&#30693;&#35782;&#22270;&#35889;&#30340;&#20986;&#29256;&#29289;&#25968;&#37327;&#19981;&#26029;&#22686;&#21152;&#65292;&#29305;&#21035;&#26159;&#22312;&#21457;&#36798;&#22269;&#23478;(+50 per year)&#12290;&#36825;&#20123;&#25104;&#26524;&#21457;&#34920;&#22312;&#39640;&#24230;&#24341;&#29992;&#30340;&#23398;&#26415;&#26399;&#21002;&#21644;&#20250;&#35758;&#19978;&#12290;&#35813;&#30740;&#31350;&#30830;&#23450;&#20102;&#19977;&#20010;&#20027;&#35201;&#30740;&#31350;&#20027;&#39064;&#65306;(1)&#30693;&#35782;&#22270;&#35889;&#30340;&#26500;&#24314;&#21644;&#22686;&#24378;&#65292;(2)&#35780;&#20272;&#21644;&#22797;&#29992;&#65292;&#20197;&#21450;(3)&#23558;&#30693;&#35782;&#22270;&#35889;&#34701;&#20837;NLP&#31995;&#32479;&#20013;&#12290;&#22312;&#36825;&#20123;&#20027;&#39064;&#20013;&#65292;&#30740;&#31350;&#30830;&#23450;&#20102;&#24191;&#27867;&#30740;&#31350;&#30340;&#20855;&#20307;&#20219;&#21153;&#65292;&#20363;&#22914;&#23454;&#20307;&#38142;&#25509;&#65292;&#20851;&#31995;&#25552;&#21462;&#21644;&#26412;&#20307;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The proliferation of open knowledge graphs has led to a surge in scholarly research on the topic over the past decade. This paper presents a bibliometric analysis of the scholarly literature on open knowledge graphs published between 2013 and 2023. The study aims to identify the trends, patterns, and impact of research in this field, as well as the key topics and research questions that have emerged. The work uses bibliometric techniques to analyze a sample of 4445 scholarly articles retrieved from Scopus. The findings reveal an ever-increasing number of publications on open knowledge graphs published every year, particularly in developed countries (+50 per year). These outputs are published in highly-referred scholarly journals and conferences. The study identifies three main research themes: (1) knowledge graph construction and enrichment, (2) evaluation and reuse, and (3) fusion of knowledge graphs into NLP systems. Within these themes, the study identifies specific tasks that have 
&lt;/p&gt;</description></item></channel></rss>