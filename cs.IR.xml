<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#27169;&#24577;&#26816;&#32034;&#20219;&#21153;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;ReViz&#8221;&#30340;&#26816;&#32034;&#27169;&#22411;&#65292;&#21487;&#20197;&#30452;&#25509;&#22788;&#29702;&#25991;&#26412;&#21644;&#22270;&#20687;&#36755;&#20837;&#20197;&#23454;&#29616;&#31471;&#21040;&#31471;&#30340;&#30693;&#35782;&#26816;&#32034;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#24182;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#20248;&#36234;&#30340;&#26816;&#32034;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.00424</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#26597;&#35810;&#30340;&#31471;&#21040;&#31471;&#30693;&#35782;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
End-to-end Knowledge Retrieval with Multi-modal Queries. (arXiv:2306.00424v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00424
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#27169;&#24577;&#26816;&#32034;&#20219;&#21153;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;ReViz&#8221;&#30340;&#26816;&#32034;&#27169;&#22411;&#65292;&#21487;&#20197;&#30452;&#25509;&#22788;&#29702;&#25991;&#26412;&#21644;&#22270;&#20687;&#36755;&#20837;&#20197;&#23454;&#29616;&#31471;&#21040;&#31471;&#30340;&#30693;&#35782;&#26816;&#32034;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#24182;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#20248;&#36234;&#30340;&#26816;&#32034;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22810;&#27169;&#24577;&#26597;&#35810;&#19979;&#30340;&#30693;&#35782;&#26816;&#32034;&#65292;&#21363;&#21253;&#21547;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#26597;&#35810;&#30340;&#20219;&#21153;&#65292;&#36825;&#26159;&#19982;&#20043;&#21069;&#30340;&#36328;&#27169;&#24577;&#26816;&#32034;&#30740;&#31350;&#19981;&#21516;&#30340;&#25361;&#25112;&#24615;&#20219;&#21153;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;ReMuQ&#30340;&#26032;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#36825;&#20010;&#20219;&#21153;&#30340;&#36827;&#23637;&#12290;ReMuQ&#38656;&#35201;&#19968;&#20010;&#31995;&#32479;&#36890;&#36807;&#25972;&#21512;&#26469;&#33258;&#25991;&#26412;&#21644;&#22270;&#20687;&#26597;&#35810;&#30340;&#20869;&#23481;&#26469;&#26816;&#32034;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;&#20013;&#30340;&#30693;&#35782;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21483;&#20570;&#8220;ReViz&#8221;&#30340;&#26816;&#32034;&#27169;&#22411;&#65292;&#36825;&#20010;&#27169;&#22411;&#21487;&#20197;&#30452;&#25509;&#22788;&#29702;&#36755;&#20837;&#30340;&#25991;&#26412;&#21644;&#22270;&#20687;&#65292;&#20197;&#31471;&#21040;&#31471;&#26041;&#24335;&#26816;&#32034;&#30456;&#20851;&#30693;&#35782;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;&#20687;&#30446;&#26631;&#26816;&#27979;&#22120;&#25110;&#26631;&#39064;&#29983;&#25104;&#22120;&#31561;&#20013;&#38388;&#27169;&#22359;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#26377;&#25928;&#22320;&#23398;&#20064;&#22810;&#27169;&#24577;&#26597;&#35810;&#19979;&#30340;&#30693;&#35782;&#26816;&#32034;&#65292;&#24182;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#38646;-shot&#35774;&#32622;&#19979;&#65292;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#65288;ReMuQ&#21644;OK-VQA&#65289;&#19978;&#30340;&#26816;&#32034;&#24615;&#33021;&#20248;&#36234;&#20197;&#21450;&#22312;&#36825;&#20123;&#25968;&#25454;&#38598;&#19978;&#24494;&#35843;&#21518;&#36827;&#19968;&#27493;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate knowledge retrieval with multi-modal queries, i.e. queries containing information split across image and text inputs, a challenging task that differs from previous work on cross-modal retrieval. We curate a new dataset called ReMuQ for benchmarking progress on this task. ReMuQ requires a system to retrieve knowledge from a large corpus by integrating contents from both text and image queries. We introduce a retriever model ``ReViz'' that can directly process input text and images to retrieve relevant knowledge in an end-to-end fashion without being dependent on intermediate modules such as object detectors or caption generators. We introduce a new pretraining task that is effective for learning knowledge retrieval with multimodal queries and also improves performance on downstream tasks. We demonstrate superior performance in retrieval on two datasets (ReMuQ and OK-VQA) under zero-shot settings as well as further improvements when finetuned on these datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#23545;&#29616;&#26377;&#30340;&#20844;&#27491;&#24863;&#30693;&#25512;&#33616;&#31995;&#32479;&#26041;&#27861;&#21644;&#23454;&#36341;&#36827;&#34892;&#20102;&#24635;&#32467;&#20998;&#26512;&#65292;&#35814;&#32454;&#20171;&#32461;&#20102;&#30456;&#20851;&#30340;&#27010;&#24565;&#23450;&#20041;&#12289;&#20998;&#31867;&#12289;&#26041;&#27861;&#21644;&#38656;&#35299;&#20915;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2306.00403</link><description>&lt;p&gt;
&#20844;&#27491;&#24863;&#30693;&#25512;&#33616;&#31995;&#32479;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Fairness-aware Recommender Systems. (arXiv:2306.00403v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00403
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#23545;&#29616;&#26377;&#30340;&#20844;&#27491;&#24863;&#30693;&#25512;&#33616;&#31995;&#32479;&#26041;&#27861;&#21644;&#23454;&#36341;&#36827;&#34892;&#20102;&#24635;&#32467;&#20998;&#26512;&#65292;&#35814;&#32454;&#20171;&#32461;&#20102;&#30456;&#20851;&#30340;&#27010;&#24565;&#23450;&#20041;&#12289;&#20998;&#31867;&#12289;&#26041;&#27861;&#21644;&#38656;&#35299;&#20915;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#20449;&#24687;&#36807;&#28388;&#26381;&#21153;&#65292;&#25512;&#33616;&#31995;&#32479;&#36890;&#36807;&#25552;&#20379;&#20010;&#24615;&#21270;&#24314;&#35758;&#21644;&#24110;&#21161;&#20154;&#20204;&#20570;&#20986;&#20915;&#31574;&#26497;&#22823;&#22320;&#20016;&#23500;&#20102;&#25105;&#20204;&#30340;&#26085;&#24120;&#29983;&#27963;&#65292;&#20351;&#23427;&#20204;&#22312;&#20449;&#24687;&#26102;&#20195;&#23545;&#20154;&#31867;&#31038;&#20250;&#33267;&#20851;&#37325;&#35201;&#21644;&#19981;&#21487;&#25110;&#32570;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#20154;&#20204;&#23545;&#23427;&#20204;&#30340;&#20381;&#36182;&#31243;&#24230;&#22686;&#21152;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#26174;&#31034;&#65292;&#30001;&#20110;&#20854;&#19981;&#20844;&#24179;&#24615;&#65288;&#20363;&#22914;&#24037;&#20316;&#25512;&#33616;&#20013;&#30340;&#24615;&#21035;&#27495;&#35270;&#65289;&#65292;&#25512;&#33616;&#31995;&#32479;&#23545;&#31038;&#20250;&#21644;&#20010;&#20154;&#21487;&#33021;&#25317;&#26377;&#26080;&#24847;&#35782;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#24320;&#21457;&#21487;&#20449;&#36182;&#30340;&#26381;&#21153;&#65292;&#35774;&#35745;&#20844;&#27491;&#24863;&#30693;&#30340;&#25512;&#33616;&#31995;&#32479;&#20197;&#32531;&#35299;&#36825;&#20123;&#20559;&#35265;&#38382;&#39064;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#32508;&#36848;&#27010;&#36848;&#20102;&#29616;&#26377;&#30340;&#20844;&#27491;&#24615;&#25512;&#33616;&#31995;&#32479;&#26041;&#27861;&#21644;&#23454;&#36341;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19981;&#21516;&#25512;&#33616;&#22330;&#26223;&#19979;&#30340;&#20844;&#27491;&#24615;&#27010;&#24565;&#65292;&#20840;&#38754;&#20998;&#31867;&#24403;&#21069;&#30340;&#36827;&#23637;&#24182;&#20171;&#32461;&#20102;&#20419;&#36827;&#25512;&#33616;&#31995;&#32479;&#19981;&#21516;&#38454;&#27573;&#30340;&#20844;&#27491;&#24615;&#30340;&#20856;&#22411;&#26041;&#27861;&#12290;&#25509;&#19979;&#26469;&#65292;&#22312;&#20171;&#32461;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#26041;&#27861;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#19968;&#20123;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
As information filtering services, recommender systems have extremely enriched our daily life by providing personalized suggestions and facilitating people in decision-making, which makes them vital and indispensable to human society in the information era. However, as people become more dependent on them, recent studies show that recommender systems potentially own unintentional impacts on society and individuals because of their unfairness (e.g., gender discrimination in job recommendations). To develop trustworthy services, it is crucial to devise fairness-aware recommender systems that can mitigate these bias issues. In this survey, we summarise existing methodologies and practices of fairness in recommender systems. Firstly, we present concepts of fairness in different recommendation scenarios, comprehensively categorize current advances, and introduce typical methods to promote fairness in different stages of recommender systems. Next, after introducing datasets and evaluation me
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;Pinterest&#25512;&#33616;&#31995;&#32479;&#30340;&#26550;&#26500;&#21644;TransAct&#27169;&#22411;&#12290;TransAct&#26159;&#19968;&#20010;&#20174;&#29992;&#25143;&#23454;&#26102;&#27963;&#21160;&#20013;&#25552;&#21462;&#30701;&#26399;&#20559;&#22909;&#30340;&#24207;&#21015;&#27169;&#22411;&#12290;&#26412;&#25991;&#36824;&#20171;&#32461;&#20102;&#36890;&#36807;&#28151;&#21512;&#25490;&#21517;&#26041;&#27861;&#32467;&#21512;&#30452;&#25509;&#22312;&#23454;&#26102;&#29992;&#25143;&#27963;&#21160;&#19978;&#23398;&#20064;&#21644;&#22312;&#36739;&#38271;&#26102;&#38388;&#27573;&#20869;&#23398;&#20064;&#25209;&#37327;&#29992;&#25143;&#34920;&#31034;&#30340;&#20248;&#28857;&#12290;</title><link>http://arxiv.org/abs/2306.00248</link><description>&lt;p&gt;
TransAct: Pinterest&#23454;&#26102;&#29992;&#25143;&#34892;&#20026;&#27169;&#22411;&#20013;&#30340;Transformer-Based&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
TransAct: Transformer-based Realtime User Action Model for Recommendation at Pinterest. (arXiv:2306.00248v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00248
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Pinterest&#25512;&#33616;&#31995;&#32479;&#30340;&#26550;&#26500;&#21644;TransAct&#27169;&#22411;&#12290;TransAct&#26159;&#19968;&#20010;&#20174;&#29992;&#25143;&#23454;&#26102;&#27963;&#21160;&#20013;&#25552;&#21462;&#30701;&#26399;&#20559;&#22909;&#30340;&#24207;&#21015;&#27169;&#22411;&#12290;&#26412;&#25991;&#36824;&#20171;&#32461;&#20102;&#36890;&#36807;&#28151;&#21512;&#25490;&#21517;&#26041;&#27861;&#32467;&#21512;&#30452;&#25509;&#22312;&#23454;&#26102;&#29992;&#25143;&#27963;&#21160;&#19978;&#23398;&#20064;&#21644;&#22312;&#36739;&#38271;&#26102;&#38388;&#27573;&#20869;&#23398;&#20064;&#25209;&#37327;&#29992;&#25143;&#34920;&#31034;&#30340;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32534;&#30721;&#29992;&#25143;&#27963;&#21160;&#20197;&#36827;&#34892;&#19979;&#19968;&#27493;&#34892;&#21160;&#39044;&#27979;&#30340;&#24207;&#21015;&#27169;&#22411;&#24050;&#25104;&#20026;&#26500;&#24314;&#22823;&#35268;&#27169;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#30340;&#24120;&#35265;&#35774;&#35745;&#36873;&#25321;&#12290; &#20256;&#32479;&#30340;&#24207;&#21015;&#25512;&#33616;&#26041;&#27861;&#35201;&#20040;&#21033;&#29992;&#23454;&#26102;&#29992;&#25143;&#25805;&#20316;&#36827;&#34892;&#31471;&#21040;&#31471;&#23398;&#20064;&#65292;&#35201;&#20040;&#20197;&#33073;&#26426;&#25209;&#37327;&#29983;&#25104;&#30340;&#26041;&#24335;&#21333;&#29420;&#23398;&#20064;&#29992;&#25143;&#34920;&#31034;&#12290; &#26412;&#25991;(1)&#20171;&#32461;&#20102;Pinterest Homefeed&#30340;&#25490;&#21517;&#26550;&#26500;&#65292;&#21363;&#25105;&#20204;&#30340;&#20010;&#24615;&#21270;&#25512;&#33616;&#20135;&#21697;&#21644;&#26368;&#22823;&#30340;&#21442;&#19982;&#38754;&#65307;(2)&#25552;&#20986;&#20102;TransAct&#65292;&#19968;&#31181;&#20174;&#29992;&#25143;&#23454;&#26102;&#27963;&#21160;&#20013;&#25552;&#21462;&#30701;&#26399;&#20559;&#22909;&#30340;&#24207;&#21015;&#27169;&#22411;&#65307;(3)&#25551;&#36848;&#20102;&#25105;&#20204;&#30340;&#28151;&#21512;&#25490;&#21517;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;TransAct&#30340;&#31471;&#21040;&#31471;&#24207;&#21015;&#24314;&#27169;&#19982;&#25209;&#37327;&#29983;&#25104;&#30340;&#29992;&#25143;&#23884;&#20837;&#28151;&#21512;&#12290; &#28151;&#21512;&#26041;&#27861;&#20801;&#35768;&#25105;&#20204;&#32467;&#21512;&#30452;&#25509;&#22312;&#23454;&#26102;&#29992;&#25143;&#27963;&#21160;&#19978;&#23398;&#20064;&#20197;&#33719;&#24471;&#21709;&#24212;&#24615;&#30340;&#20248;&#28857;&#21644;&#22312;&#36739;&#38271;&#26102;&#38388;&#27573;&#20869;&#23398;&#20064;&#30340;&#25209;&#37327;&#29992;&#25143;&#34920;&#31034;&#30340;&#25104;&#26412;&#25928;&#30410;&#12290; &#25105;&#20204;&#25551;&#36848;&#20102;&#23454;&#39564;&#32467;&#26524;......&#65288;&#21407;&#25991;&#20869;&#23481;&#30465;&#30053;&#65289;
&lt;/p&gt;
&lt;p&gt;
Sequential models that encode user activity for next action prediction have become a popular design choice for building web-scale personalized recommendation systems. Traditional methods of sequential recommendation either utilize end-to-end learning on realtime user actions, or learn user representations separately in an offline batch-generated manner. This paper (1) presents Pinterest's ranking architecture for Homefeed, our personalized recommendation product and the largest engagement surface; (2) proposes TransAct, a sequential model that extracts users' short-term preferences from their realtime activities; (3) describes our hybrid approach to ranking, which combines end-to-end sequential modeling via TransAct with batch-generated user embeddings. The hybrid approach allows us to combine the advantages of responsiveness from learning directly on realtime user activity with the cost-effectiveness of batch user representations learned over a longer time period. We describe the resu
&lt;/p&gt;</description></item><item><title>VMap&#26159;&#19968;&#31181;&#20132;&#20114;&#24335;&#30697;&#24418;&#22635;&#20805;&#22320;&#22270;&#21487;&#35270;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#39030;&#28857;&#20026;&#20013;&#24515;&#30340;&#22270;&#34920;&#25506;&#32034;&#12290;&#36890;&#36807;&#38598;&#25104;DAR&#30697;&#24418;&#20998;&#21106;&#31639;&#27861;&#12289;&#21452;&#38454;&#27573;&#30697;&#24418;&#35843;&#25972;&#31639;&#27861;&#21644;&#22522;&#20110;&#27169;&#25311;&#36864;&#28779;&#30340;&#21551;&#21457;&#24335;&#20248;&#21270;&#22120;&#65292;&#23427;&#33021;&#22815;&#20248;&#21270;&#30697;&#24418;&#32437;&#27178;&#27604;&#12289;&#39030;&#28857;-&#36793;&#20132;&#21449;&#21644;&#25968;&#25454;&#32534;&#30721;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.00120</link><description>&lt;p&gt;
VMap: &#19968;&#31181;&#20132;&#20114;&#24335;&#30697;&#24418;&#22635;&#20805;&#21487;&#35270;&#21270;&#22320;&#22270;&#65292;&#29992;&#20110;&#39030;&#28857;&#20026;&#20013;&#24515;&#30340;&#22270;&#34920;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
VMap: An Interactive Rectangular Space-filling Visualization for Map-like Vertex-centric Graph Exploration. (arXiv:2306.00120v1 [cs.GR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00120
&lt;/p&gt;
&lt;p&gt;
VMap&#26159;&#19968;&#31181;&#20132;&#20114;&#24335;&#30697;&#24418;&#22635;&#20805;&#22320;&#22270;&#21487;&#35270;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#39030;&#28857;&#20026;&#20013;&#24515;&#30340;&#22270;&#34920;&#25506;&#32034;&#12290;&#36890;&#36807;&#38598;&#25104;DAR&#30697;&#24418;&#20998;&#21106;&#31639;&#27861;&#12289;&#21452;&#38454;&#27573;&#30697;&#24418;&#35843;&#25972;&#31639;&#27861;&#21644;&#22522;&#20110;&#27169;&#25311;&#36864;&#28779;&#30340;&#21551;&#21457;&#24335;&#20248;&#21270;&#22120;&#65292;&#23427;&#33021;&#22815;&#20248;&#21270;&#30697;&#24418;&#32437;&#27178;&#27604;&#12289;&#39030;&#28857;-&#36793;&#20132;&#21449;&#21644;&#25968;&#25454;&#32534;&#30721;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;VMap&#30340;&#22320;&#22270;&#29366;&#30697;&#24418;&#22635;&#20805;&#21487;&#35270;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#39030;&#28857;&#20026;&#20013;&#24515;&#30340;&#22270;&#34920;&#25506;&#32034;&#12290;&#20026;&#20248;&#21270;&#30697;&#24418;&#30340;&#32437;&#27178;&#27604;&#12289;&#39030;&#28857;-&#36793;&#20132;&#21449;&#21644;&#25968;&#25454;&#32534;&#30721;&#31934;&#24230;&#31561;&#26041;&#38754;&#65292;&#29616;&#26377;&#30340;&#21487;&#35270;&#21270;&#26041;&#27861;&#23384;&#22312;&#25903;&#25345;&#19981;&#36275;&#12290;&#20026;&#24212;&#23545;&#36825;&#20123;&#38382;&#39064;&#65292;VMap&#38598;&#25104;&#20102;&#19977;&#31181;&#26032;&#32452;&#24314;&#65306;&#65288;1&#65289;&#22522;&#20110;&#24819;&#35201;&#30340;&#32437;&#27178;&#27604;&#65288;DAR&#65289;&#30340;&#30697;&#24418;&#20998;&#21106;&#31639;&#27861;&#65292;&#65288;2&#65289;&#21452;&#38454;&#27573;&#30697;&#24418;&#35843;&#25972;&#31639;&#27861;&#65292;&#65288;3&#65289;&#22522;&#20110;&#27169;&#25311;&#36864;&#28779;&#30340;&#21551;&#21457;&#24335;&#20248;&#21270;&#22120;&#12290;&#39318;&#20808;&#65292;&#20026;&#20102;&#29983;&#25104;&#36755;&#20837;&#22270;&#34920;&#30340;&#30697;&#24418;&#22635;&#20805;&#24067;&#23616;&#65292;&#25105;&#20204;&#23558;&#22270;&#34920;&#30340;2D&#23884;&#20837;&#26041;&#24335;&#21010;&#20998;&#20026;&#30697;&#24418;&#65292;&#24182;&#20248;&#21270;&#30697;&#24418;&#30340;&#32437;&#27178;&#27604;&#65292;&#20197;&#36798;&#21040;&#24819;&#35201;&#30340;&#32437;&#27178;&#27604;&#12290;&#20854;&#27425;&#65292;&#20026;&#20102;&#22312;&#30697;&#24418;&#20043;&#38388;&#36335;&#30001;&#22270;&#34920;&#36793;&#32536;&#65292;&#32780;&#19981;&#20250;&#20986;&#29616;&#39030;&#28857;-&#36793;&#32536;&#37325;&#21472;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21452;&#38454;&#27573;&#31639;&#27861;&#26469;&#35843;&#25972;&#30697;&#24418;&#24067;&#23616;&#65292;&#20197;&#22312;&#30697;&#24418;&#20043;&#38388;&#25554;&#20837;&#36793;&#26694;&#31354;&#38388;&#12290;&#31532;&#19977;&#65292;&#20026;&#20102;&#36890;&#36807;&#32771;&#34385;&#22810;&#31181;&#35270;&#35273;&#20934;&#21017;&#26469;&#29983;&#25104;&#21644;&#25490;&#21015;&#30697;&#24418;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21253;&#21547;&#27169;&#25311;&#36864;&#28779;&#30340;&#21551;&#21457;&#24335;&#20248;&#21270;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present VMap, a map-like rectangular space-filling visualization, to perform vertex-centric graph exploration. Existing visualizations have limited support for quality optimization among rectangular aspect ratios, vertex-edge intersection, and data encoding accuracy. To tackle this problem, VMap integrates three novel components: (1) a desired-aspect-ratio (DAR) rectangular partitioning algorithm, (2) a two-stage rectangle adjustment algorithm, and (3) a simulated annealing based heuristic optimizer. First, to generate a rectangular space-filling layout of an input graph, we subdivide the 2D embedding of the graph into rectangles with optimization of rectangles' aspect ratios toward a desired aspect ratio. Second, to route graph edges between rectangles without vertex-edge occlusion, we devise a two-stage algorithm to adjust a rectangular layout to insert border space between rectangles. Third, to produce and arrange rectangles by considering multiple visual criteria, we design a si
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#20171;&#32461;&#20102;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#25552;&#20986;&#20102;&#21028;&#21035;&#24335;LLMs&#21644;&#29983;&#25104;&#24335;LLMs&#20004;&#31181;&#27169;&#22411;&#33539;&#24335;&#65292;&#24635;&#32467;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24378;&#35843;&#20102;&#35813;&#39046;&#22495;&#30340;&#25361;&#25112;&#21644;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2305.19860</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#33616;&#31995;&#32479;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Large Language Models for Recommendation. (arXiv:2305.19860v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19860
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#20171;&#32461;&#20102;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#25552;&#20986;&#20102;&#21028;&#21035;&#24335;LLMs&#21644;&#29983;&#25104;&#24335;LLMs&#20004;&#31181;&#27169;&#22411;&#33539;&#24335;&#65292;&#24635;&#32467;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24378;&#35843;&#20102;&#35813;&#39046;&#22495;&#30340;&#25361;&#25112;&#21644;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#24182;&#22312;&#25512;&#33616;&#31995;&#32479;&#39046;&#22495;&#24341;&#36215;&#20102;&#37325;&#35270;&#12290;&#36825;&#20123;&#27169;&#22411;&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#28023;&#37327;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#24050;&#22312;&#23398;&#20064;&#36890;&#29992;&#34920;&#31034;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#30528;&#25104;&#21151;&#65292;&#24182;&#26377;&#21487;&#33021;&#36890;&#36807;&#19968;&#20123;&#26377;&#25928;&#30340;&#36716;&#31227;&#25216;&#26415;&#65288;&#22914;&#24494;&#35843;&#21644;&#25552;&#31034;&#35843;&#25972;&#65289;&#31561;&#25163;&#27573;&#25552;&#39640;&#25512;&#33616;&#31995;&#32479;&#30340;&#21508;&#20010;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#25512;&#33616;&#36136;&#37327;&#30340;&#20851;&#38190;&#26159;&#21033;&#29992;&#23427;&#20204;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#29305;&#24449;&#34920;&#31034;&#21644;&#22823;&#37327;&#30340;&#22806;&#37096;&#30693;&#35782;&#35206;&#30422;&#65292;&#24314;&#31435;&#39033;&#30446;&#21644;&#29992;&#25143;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#20026;&#20102;&#20840;&#38754;&#20102;&#35299;&#29616;&#26377;&#22522;&#20110;LLM&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#26412;&#32508;&#36848;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#31867;&#27861;&#65292;&#23558;&#36825;&#20123;&#27169;&#22411;&#20998;&#20026;&#20004;&#31181;&#20027;&#35201;&#33539;&#24335;&#65292;&#20998;&#21035;&#26159;&#21028;&#21035;&#24335;LLMs&#21644;&#29983;&#25104;&#24335;LLMs&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#36825;&#20123;&#33539;&#24335;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24182;&#24378;&#35843;&#20102;&#36825;&#20010;&#26032;&#20852;&#39046;&#22495;&#30340;&#25361;&#25112;&#21644;&#24320;&#25918;&#24615;&#30740;&#31350;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have emerged as powerful tools in the field of Natural Language Processing (NLP) and have recently gained significant attention in the domain of Recommendation Systems (RS). These models, trained on massive amounts of data using self-supervised learning, have demonstrated remarkable success in learning universal representations and have the potential to enhance various aspects of recommendation systems by some effective transfer techniques such as fine-tuning and prompt tuning, and so on. The crucial aspect of harnessing the power of language models in enhancing recommendation quality is the utilization of their high-quality representations of textual features and their extensive coverage of external knowledge to establish correlations between items and users. To provide a comprehensive understanding of the existing LLM-based recommendation systems, this survey presents a taxonomy that categorizes these models into two major paradigms, respectively Discrimi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#22810;&#20934;&#21017;&#25512;&#33616;&#30340;&#26631;&#20934;&#20559;&#22909;&#24863;&#30693;&#36731;&#37327;&#22270;&#21367;&#31215;&#32593;&#32476;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;MC&#25193;&#23637;&#22270;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#25429;&#25417;&#29992;&#25143;&#30340;&#26631;&#20934;&#20559;&#22909;&#65292;&#24182;&#36827;&#19968;&#27493;&#23558;&#29992;&#25143;&#23545;&#21508;&#20010;&#26631;&#20934;&#30340;&#20559;&#22909;&#21512;&#24182;&#21040;&#26368;&#32456;&#30340;&#25512;&#33616;&#21015;&#34920;&#20013;&#12290;</title><link>http://arxiv.org/abs/2305.18885</link><description>&lt;p&gt;
&#26631;&#20934;&#27604;&#35780;&#20998;&#26356;&#37325;&#35201;&#65306;&#38754;&#21521;&#22810;&#20934;&#21017;&#25512;&#33616;&#30340;&#26631;&#20934;&#20559;&#22909;&#24863;&#30693;&#36731;&#37327;&#22270;&#21367;&#31215;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Criteria Tell You More than Ratings: Criteria Preference-Aware Light Graph Convolution for Effective Multi-Criteria Recommendation. (arXiv:2305.18885v2 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18885
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#22810;&#20934;&#21017;&#25512;&#33616;&#30340;&#26631;&#20934;&#20559;&#22909;&#24863;&#30693;&#36731;&#37327;&#22270;&#21367;&#31215;&#32593;&#32476;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;MC&#25193;&#23637;&#22270;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#25429;&#25417;&#29992;&#25143;&#30340;&#26631;&#20934;&#20559;&#22909;&#65292;&#24182;&#36827;&#19968;&#27493;&#23558;&#29992;&#25143;&#23545;&#21508;&#20010;&#26631;&#20934;&#30340;&#20559;&#22909;&#21512;&#24182;&#21040;&#26368;&#32456;&#30340;&#25512;&#33616;&#21015;&#34920;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20934;&#21017;&#25512;&#33616;&#31995;&#32479;&#29616;&#22312;&#22312;&#24191;&#27867;&#30340;&#30005;&#23376;&#21830;&#21153;&#39046;&#22495;&#20013;&#21033;&#29992;&#22810;&#20934;&#21017; (MC) &#35780;&#20998;&#20449;&#24687;&#65292;&#32780;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476; (GNN) &#24050;&#32463;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#25512;&#33616;&#31995;&#32479;&#30340;&#24320;&#21457;&#20013;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#26412;&#25991;&#39318;&#27425;&#23581;&#35797;&#20351;&#29992;GNN&#36741;&#21161;&#35774;&#35745;MC&#25512;&#33616;&#31995;&#32479;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26631;&#20934;&#20559;&#22909;&#24863;&#30693;&#36731;&#37327;&#22270;&#21367;&#31215;&#26041;&#27861;(CPA-LGC),&#21487;&#20197;&#20934;&#30830;&#25429;&#25417;&#29992;&#25143;&#30340;&#26631;&#20934;&#20559;&#22909;&#20197;&#21450;&#22797;&#26434;&#39640;&#38454;&#36830;&#25509;&#20013;&#30340;&#21327;&#20316;&#20449;&#21495;&#12290;&#26412;&#25991;&#22312;MC&#25193;&#23637;&#22270;&#19978;&#26500;&#24314;&#20102;&#19968;&#20010;&#33021;&#22815;&#23558;&#29992;&#25143;-&#29289;&#21697;MC&#35780;&#20998;&#36716;&#25442;&#20026;&#25193;&#23637;&#20108;&#20998;&#22270;&#30340;MC&#25193;&#23637;&#22270;&#65292;&#20877;&#36827;&#19968;&#27493;&#23558;&#26631;&#20934;&#37325;&#35201;&#24615;&#32534;&#30721;&#21040;&#22270;&#21367;&#31215;&#36807;&#31243;&#20013;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26631;&#20934;&#20559;&#22909;&#24863;&#30693;&#32858;&#21512;&#26041;&#27861;&#26469;&#23558;&#29992;&#25143;&#23545;&#19981;&#21516;&#26631;&#20934;&#30340;&#20559;&#22909;&#21512;&#24182;&#21040;&#26368;&#32456;&#30340;&#25512;&#33616;&#21015;&#34920;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
The multi-criteria (MC) recommender system, which leverages MC rating information in a wide range of e-commerce areas, is ubiquitous nowadays. Surprisingly, although graph neural networks (GNNs) have been widely applied to develop various recommender systems due to GNN's high expressive capability in learning graph representations, it has been still unexplored how to design MC recommender systems with GNNs. In light of this, we make the first attempt towards designing a GNN-aided MC recommender system. Specifically, rather than straightforwardly adopting existing GNN-based recommendation methods, we devise a novel criteria preference-aware light graph convolution CPA-LGC method, which is capable of precisely capturing the criteria preference of users as well as the collaborative signal in complex high-order connectivities. To this end, we first construct an MC expansion graph that transforms user--item MC ratings into an expanded bipartite graph to potentially learn from the collaborat
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22522;&#20110;&#22270;&#36974;&#30422;&#33258;&#32534;&#30721;&#22120;&#30340;&#24207;&#21015;&#25512;&#33616;&#31995;&#32479;&#65292;&#23427;&#20351;&#29992;&#22522;&#20110;&#22270;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#26292;&#38706;&#20986;&#24102;&#26377;&#36974;&#30422;&#30340;&#39033;&#30446;&#24207;&#21015;&#65292;&#33258;&#36866;&#24212;&#21160;&#24577;&#25552;&#21462;&#20840;&#23616;&#39033;&#30446;&#36716;&#25442;&#20449;&#24687;&#36827;&#34892;&#33258;&#30417;&#30563;&#22686;&#24378;&#65292;&#22312;&#20855;&#26377;&#36739;&#23569;&#26631;&#35760;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#22987;&#32456;&#27604;&#26368;&#20808;&#36827;&#30340;&#24207;&#21015;&#25512;&#33616;&#26041;&#27861;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#32780;&#19988;&#23545;&#25968;&#25454;&#25439;&#22351;&#21644;&#32570;&#22833;&#24773;&#20917;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.04619</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#24418;&#36974;&#30422;&#33258;&#32534;&#30721;&#22120;&#30340;&#24207;&#21015;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Graph Masked Autoencoder for Sequential Recommendation. (arXiv:2305.04619v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04619
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22522;&#20110;&#22270;&#36974;&#30422;&#33258;&#32534;&#30721;&#22120;&#30340;&#24207;&#21015;&#25512;&#33616;&#31995;&#32479;&#65292;&#23427;&#20351;&#29992;&#22522;&#20110;&#22270;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#26292;&#38706;&#20986;&#24102;&#26377;&#36974;&#30422;&#30340;&#39033;&#30446;&#24207;&#21015;&#65292;&#33258;&#36866;&#24212;&#21160;&#24577;&#25552;&#21462;&#20840;&#23616;&#39033;&#30446;&#36716;&#25442;&#20449;&#24687;&#36827;&#34892;&#33258;&#30417;&#30563;&#22686;&#24378;&#65292;&#22312;&#20855;&#26377;&#36739;&#23569;&#26631;&#35760;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#22987;&#32456;&#27604;&#26368;&#20808;&#36827;&#30340;&#24207;&#21015;&#25512;&#33616;&#26041;&#27861;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#32780;&#19988;&#23545;&#25968;&#25454;&#25439;&#22351;&#21644;&#32570;&#22833;&#24773;&#20917;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#19968;&#20123;&#24378;&#22823;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65288;&#20363;&#22914;Transformer&#12289;&#22270;&#31070;&#32463;&#32593;&#32476;&#65289;&#36890;&#36807;&#39640;&#38454;&#39033;&#20381;&#36182;&#24314;&#27169;&#22312;&#24207;&#21015;&#25512;&#33616;&#20013;&#23454;&#29616;&#20102;&#25913;&#36827;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#21487;&#33021;&#22312;&#26631;&#31614;&#31232;&#32570;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#36739;&#24046;&#30340;&#34920;&#24449;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#26631;&#31614;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;&#23545;&#27604;&#23398;&#20064;&#65288;CL&#65289;&#24050;&#32463;&#24341;&#36215;&#20102;&#36817;&#26399;&#30340;&#20851;&#27880;&#65292;&#36890;&#36807;&#23884;&#20837;&#23545;&#27604;&#26469;&#36827;&#34892;&#33258;&#25105;&#30417;&#30563;&#30340;&#25968;&#25454;&#22686;&#24378;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#23545;&#27604;&#35270;&#22270;&#29983;&#25104;&#31574;&#30053;&#30340;&#25163;&#24037;&#21046;&#23450;&#29305;&#24615;&#65292;&#29616;&#26377;&#30340;CL&#22686;&#24378;&#27169;&#22411;&#19981;&#20165;&#38590;&#20197;&#22312;&#19981;&#21516;&#30340;&#24207;&#21015;&#25512;&#33616;&#20219;&#21153;&#20013;&#20135;&#29983;&#19968;&#33268;&#30340;&#24615;&#33021;&#65292;&#36824;&#21487;&#33021;&#23545;&#29992;&#25143;&#34892;&#20026;&#25968;&#25454;&#22122;&#22768;&#19981;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#37492;&#20110;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#33258;&#36866;&#24212;&#20840;&#23616;&#20449;&#24687;&#25552;&#21462;&#30340;&#22270;&#36974;&#30422;&#33258;&#32534;&#30721;&#22120;&#22686;&#24378;&#30340;&#24207;&#21015;&#25512;&#33616;&#31995;&#32479;&#65288;MAERec&#65289;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#23427;&#33258;&#28982;&#22320;&#36991;&#20813;&#20102;&#19978;&#36848;&#38382;&#39064;&#65292;&#24471;&#30410;&#20110;&#20854;&#29420;&#29305;&#30340;&#25968;&#25454;&#37325;&#26500;&#26426;&#21046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20351;&#29992;&#22522;&#20110;&#22270;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#26292;&#38706;&#20986;&#24102;&#26377;&#36974;&#30422;&#30340;&#39033;&#30446;&#24207;&#21015;&#65292;&#20351;&#34920;&#31034;&#19981;&#20165;&#21033;&#29992;&#26412;&#22320;&#39034;&#24207;&#20449;&#24687;&#65292;&#36824;&#21033;&#29992;&#39033;&#30446;&#20043;&#38388;&#30340;&#20840;&#23616;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#22312;&#22235;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#20855;&#26377;&#36739;&#23569;&#26631;&#35760;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#22987;&#32456;&#27604;&#26368;&#20808;&#36827;&#30340;&#24207;&#21015;&#25512;&#33616;&#26041;&#27861;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#32780;&#19988;&#23545;&#25968;&#25454;&#25439;&#22351;&#21644;&#32570;&#22833;&#24773;&#20917;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
While some powerful neural network architectures (e.g., Transformer, Graph Neural Networks) have achieved improved performance in sequential recommendation with high-order item dependency modeling, they may suffer from poor representation capability in label scarcity scenarios. To address the issue of insufficient labels, Contrastive Learning (CL) has attracted much attention in recent methods to perform data augmentation through embedding contrasting for self-supervision. However, due to the hand-crafted property of their contrastive view generation strategies, existing CL-enhanced models i) can hardly yield consistent performance on diverse sequential recommendation tasks; ii) may not be immune to user behavior data noise. In light of this, we propose a simple yet effective Graph Masked AutoEncoder-enhanced sequential Recommender system (MAERec) that adaptively and dynamically distills global item transitional information for self-supervised augmentation. It naturally avoids the abov
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;GPT-3.5&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#20013;&#65292;&#23454;&#29616;&#22312;&#22235;&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;&#26032;&#30340;&#26368;&#20248;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#25351;&#23548;&#35828;&#26126;&#21644;&#32422;&#26463;&#27169;&#24335;&#19979;&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.01555</link><description>&lt;p&gt;
&#22914;&#20309;&#21457;&#25381;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#20013;&#30340;&#33021;&#21147;&#65311;
&lt;/p&gt;
&lt;p&gt;
How to Unleash the Power of Large Language Models for Few-shot Relation Extraction?. (arXiv:2305.01555v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01555
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;GPT-3.5&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#20013;&#65292;&#23454;&#29616;&#22312;&#22235;&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;&#26032;&#30340;&#26368;&#20248;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#25351;&#23548;&#35828;&#26126;&#21644;&#32422;&#26463;&#27169;&#24335;&#19979;&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#25193;&#23637;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#24191;&#27867;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65292;&#20294;&#26159;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#36824;&#27809;&#26377;&#24471;&#21040;&#20840;&#38754;&#25506;&#32034;&#12290;&#26412;&#25991;&#36890;&#36807;&#35814;&#32454;&#23454;&#39564;&#65292;&#30740;&#31350;&#20102;&#20351;&#29992;GPT-3.5&#36827;&#34892;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#30340;&#22522;&#26412;&#26041;&#27861;&#8212;&#8212;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#25968;&#25454;&#29983;&#25104;&#12290;&#20026;&#20102;&#22686;&#24378;&#23569;&#26679;&#26412;&#24615;&#33021;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#25351;&#23548;&#35828;&#26126;&#21644;&#32422;&#26463;&#27169;&#24335;&#19979;&#30340;&#25968;&#25454;&#29983;&#25104;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#23454;&#29616;&#19982;&#20197;&#21069;&#30340;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#32780;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#25454;&#29983;&#25104;&#21487;&#20197;&#25512;&#21160;&#20197;&#21069;&#30340;&#35299;&#20915;&#26041;&#26696;&#20197;&#22312;&#22235;&#20010;&#24191;&#27867;&#30740;&#31350;&#30340;&#20851;&#31995;&#25277;&#21462;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#26032;&#30340;&#26368;&#20808;&#36827;&#30340;&#23569;&#26679;&#26412;&#32467;&#26524;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#24037;&#20316;&#21487;&#20197;&#28608;&#21457;&#26410;&#26469;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#20013;&#30340;&#33021;&#21147;&#30340;&#30740;&#31350;&#12290;&#20195;&#30721;&#21487;&#20197;&#22312; \url{https://github.com/zjunlp/DeepKE/tree/main/example/llm} &#20013;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scaling language models have revolutionized widespread NLP tasks, yet little comprehensively explored few-shot relation extraction with large language models. In this paper, we investigate principal methodologies, in-context learning and data generation, for few-shot relation extraction via GPT-3.5 through exhaustive experiments. To enhance few-shot performance, we further propose task-related instructions and schema-constrained data generation. We observe that in-context learning can achieve performance on par with previous prompt learning approaches, and data generation with the large language model can boost previous solutions to obtain new state-of-the-art few-shot results on four widely-studied relation extraction datasets. We hope our work can inspire future research for the capabilities of large language models in few-shot relation extraction. Code is available in \url{https://github.com/zjunlp/DeepKE/tree/main/example/llm.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#21033;&#29992;&#21453;&#20107;&#23454;&#25512;&#29702;&#26469;&#29983;&#25104;&#21487;&#29702;&#35299;&#35299;&#37322;&#30340;&#26041;&#27861;&#65292;&#20854;&#22312;&#25512;&#33616;&#31995;&#32479;&#19978;&#21462;&#24471;&#20102;&#25104;&#21151;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2303.11160</link><description>&lt;p&gt;
&#21033;&#29992;&#21453;&#20107;&#23454;&#25991;&#26412;&#35299;&#37322;&#26469;&#35299;&#37322;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Explaining Recommendation System Using Counterfactual Textual Explanations. (arXiv:2303.11160v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11160
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#21033;&#29992;&#21453;&#20107;&#23454;&#25512;&#29702;&#26469;&#29983;&#25104;&#21487;&#29702;&#35299;&#35299;&#37322;&#30340;&#26041;&#27861;&#65292;&#20854;&#22312;&#25512;&#33616;&#31995;&#32479;&#19978;&#21462;&#24471;&#20102;&#25104;&#21151;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#26377;&#22823;&#37327;&#30340;&#30740;&#31350;&#33268;&#21147;&#20110;&#25913;&#36827;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#35299;&#35835;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22914;&#26524;&#26368;&#32456;&#29992;&#25143;&#29702;&#35299;&#26576;&#20123;&#36755;&#20986;&#30340;&#21407;&#22240;&#65292;&#23601;&#26356;&#23481;&#26131;&#20449;&#20219;&#31995;&#32479;&#12290;&#25512;&#33616;&#31995;&#32479;&#26159;&#38656;&#35201;&#36827;&#34892;&#25913;&#36827;&#20197;&#20351;&#20854;&#36755;&#20986;&#26356;&#21152;&#21487;&#35299;&#37322;&#30340;&#31995;&#32479;&#20043;&#19968;&#12290;&#20135;&#29983;&#26356;&#21487;&#35299;&#37322;&#30340;&#36755;&#20986;&#30340;&#19968;&#31181;&#26041;&#27861;&#26159;&#20351;&#29992;&#21453;&#20107;&#23454;&#25512;&#29702;&#65292;&#36825;&#28041;&#21450;&#23545;&#26368;&#23567;&#35201;&#32032;&#36827;&#34892;&#20462;&#25913;&#65292;&#20197;&#29983;&#25104;&#23548;&#33268;&#31995;&#32479;&#36755;&#20986;&#21464;&#21270;&#30340;&#21453;&#20107;&#23454;&#39033;&#30446;&#12290;&#36825;&#19968;&#36807;&#31243;&#20801;&#35768;&#35782;&#21035;&#23545;&#26399;&#26395;&#36755;&#20986;&#26377;&#37325;&#22823;&#24433;&#21709;&#30340;&#36755;&#20837;&#35201;&#32032;&#65292;&#20174;&#32780;&#25552;&#20379;&#26377;&#25928;&#30340;&#35299;&#37322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#29983;&#25104;&#38024;&#23545;&#34920;&#26684;&#21644;&#25991;&#26412;&#35201;&#32032;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#35777;&#26126;&#23427;&#22312;&#20026;&#26368;&#32456;&#29992;&#25143;&#25552;&#20379;&#21487;&#29702;&#35299;&#30340;&#35299;&#37322;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Currently, there is a significant amount of research being conducted in the field of artificial intelligence to improve the explainability and interpretability of deep learning models. It is found that if end-users understand the reason for the production of some output, it is easier to trust the system. Recommender systems are one example of systems that great efforts have been conducted to make their output more explainable. One method for producing a more explainable output is using counterfactual reasoning, which involves altering minimal features to generate a counterfactual item that results in changing the output of the system. This process allows the identification of input features that have a significant impact on the desired output, leading to effective explanations. In this paper, we present a method for generating counterfactual explanations for both tabular and textual features. We evaluated the performance of our proposed method on three real-world datasets and demonstra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#36827;&#34892;&#25512;&#29702;&#30340;&#21069;&#27839;&#30740;&#31350;&#32508;&#21512;&#35843;&#26597;&#12290;&#35752;&#35770;&#20102;&#26032;&#20852;&#25512;&#29702;&#33021;&#21147;&#20986;&#29616;&#30340;&#28508;&#22312;&#21407;&#22240;&#65292;&#24182;&#25552;&#20379;&#31995;&#32479;&#36164;&#28304;&#24110;&#21161;&#21021;&#23398;&#32773;&#12290;</title><link>http://arxiv.org/abs/2212.09597</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#36827;&#34892;&#25512;&#29702;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Reasoning with Language Model Prompting: A Survey. (arXiv:2212.09597v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09597
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#36827;&#34892;&#25512;&#29702;&#30340;&#21069;&#27839;&#30740;&#31350;&#32508;&#21512;&#35843;&#26597;&#12290;&#35752;&#35770;&#20102;&#26032;&#20852;&#25512;&#29702;&#33021;&#21147;&#20986;&#29616;&#30340;&#28508;&#22312;&#21407;&#22240;&#65292;&#24182;&#25552;&#20379;&#31995;&#32479;&#36164;&#28304;&#24110;&#21161;&#21021;&#23398;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#29702;&#20316;&#20026;&#22797;&#26434;&#38382;&#39064;&#35299;&#20915;&#30340;&#37325;&#35201;&#33021;&#21147;&#65292;&#21487;&#20197;&#20026;&#21307;&#30103;&#35786;&#26029;&#12289;&#35848;&#21028;&#31561;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#25552;&#20379;&#21518;&#31471;&#25903;&#25345;&#12290;&#26412;&#25991;&#23545;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#36827;&#34892;&#25512;&#29702;&#30340;&#21069;&#27839;&#30740;&#31350;&#36827;&#34892;&#20102;&#32508;&#21512;&#35843;&#26597;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#30740;&#31350;&#25104;&#26524;&#30340;&#27604;&#36739;&#21644;&#24635;&#32467;&#65292;&#24182;&#25552;&#20379;&#20102;&#31995;&#32479;&#36164;&#28304;&#20197;&#24110;&#21161;&#21021;&#23398;&#32773;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#26032;&#20852;&#25512;&#29702;&#33021;&#21147;&#20986;&#29616;&#30340;&#28508;&#22312;&#21407;&#22240;&#65292;&#24182;&#31361;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#36164;&#28304;&#21487;&#22312; https://github.com/zjunlp/Prompt4ReasoningPapers &#19978;&#33719;&#21462;&#65288;&#23450;&#26399;&#26356;&#26032;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reasoning, as an essential ability for complex problem-solving, can provide back-end support for various real-world applications, such as medical diagnosis, negotiation, etc. This paper provides a comprehensive survey of cutting-edge research on reasoning with language model prompting. We introduce research works with comparisons and summaries and provide systematic resources to help beginners. We also discuss the potential reasons for emerging such reasoning abilities and highlight future research directions. Resources are available at https://github.com/zjunlp/Prompt4ReasoningPapers (updated periodically).
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25512;&#33616;&#21435;&#22122;&#30340;&#39640;&#25928;&#21452;&#23618;&#20248;&#21270;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#36845;&#20195;&#35843;&#25972;&#25512;&#33616;&#27169;&#22411;&#65292;&#20197;&#32771;&#34385;&#21069;&#20960;&#27425;&#36845;&#20195;&#20013;&#20026;&#27599;&#20010;&#21453;&#39304;&#20998;&#37197;&#30340;&#26435;&#37325;&#12290;</title><link>http://arxiv.org/abs/2210.10321</link><description>&lt;p&gt;
&#25512;&#33616;&#21435;&#22122;&#30340;&#39640;&#25928;&#21452;&#23618;&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficient Bi-Level Optimization for Recommendation Denoising. (arXiv:2210.10321v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.10321
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25512;&#33616;&#21435;&#22122;&#30340;&#39640;&#25928;&#21452;&#23618;&#20248;&#21270;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#36845;&#20195;&#35843;&#25972;&#25512;&#33616;&#27169;&#22411;&#65292;&#20197;&#32771;&#34385;&#21069;&#20960;&#27425;&#36845;&#20195;&#20013;&#20026;&#27599;&#20010;&#21453;&#39304;&#20998;&#37197;&#30340;&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#25512;&#33616;&#31995;&#32479;&#20013;&#65292;&#33719;&#24471;&#26126;&#30830;&#30340;&#29992;&#25143;&#21453;&#39304;&#65288;&#20363;&#22914;&#35780;&#20998;&#65289;&#36890;&#24120;&#20250;&#21463;&#21040;&#38656;&#35201;&#29992;&#25143;&#31215;&#26497;&#21442;&#19982;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#21033;&#29992;&#29992;&#25143;&#27983;&#35272;&#26399;&#38388;&#29983;&#25104;&#30340;&#38544;&#24335;&#21453;&#39304;&#65288;&#20363;&#22914;&#28857;&#20987;&#65289;&#20316;&#20026;&#21487;&#34892;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#38544;&#24335;&#21453;&#39304;&#20855;&#26377;&#24456;&#39640;&#30340;&#22122;&#22768;&#65292;&#36825;&#20250;&#26174;&#30528;&#25439;&#23475;&#25512;&#33616;&#36136;&#37327;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#25512;&#33616;&#21435;&#22122;&#24314;&#27169;&#20026;&#19968;&#20010;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#32771;&#34385;&#21069;&#20960;&#27425;&#36845;&#20195;&#20013;&#20026;&#27599;&#20010;&#21453;&#39304;&#20998;&#37197;&#30340;&#26435;&#37325;&#26469;&#36845;&#20195;&#22320;&#35843;&#25972;&#25512;&#33616;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20004;&#20010;&#22823;&#35268;&#27169;&#25512;&#33616;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The acquisition of explicit user feedback (e.g., ratings) in real-world recommender systems is often hindered by the need for active user involvement. To mitigate this issue, implicit feedback (e.g., clicks) generated during user browsing is exploited as a viable substitute. However, implicit feedback possesses a high degree of noise, which significantly undermines recommendation quality. While many methods have been proposed to address this issue by assigning varying weights to implicit feedback, two shortcomings persist: (1) the weight calculation in these methods is iteration-independent, without considering the influence of weights in previous iterations, and (2) the weight calculation often relies on prior knowledge, which may not always be readily available or universally applicable.  To overcome these two limitations, we model recommendation denoising as a bi-level optimization problem. The inner optimization aims to derive an effective model for the recommendation, as well as g
&lt;/p&gt;</description></item></channel></rss>