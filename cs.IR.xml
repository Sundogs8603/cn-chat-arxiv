<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;-&#23545;&#27604;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#23545;&#27604;&#35270;&#22270;&#22686;&#24378;&#31574;&#30053;&#12289;&#20301;&#32622;&#24863;&#30693;&#21644;&#35821;&#20041;&#24863;&#30693;&#27491;&#26679;&#26412;&#37319;&#26679;&#31574;&#30053;&#20197;&#21450;&#20998;&#23618;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#26469;&#20811;&#26381;&#22270;&#25968;&#25454;&#22686;&#24378;&#30340;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2404.02810</link><description>&lt;p&gt;
&#29983;&#25104;-&#23545;&#27604;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Generative-Contrastive Heterogeneous Graph Neural Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02810
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;-&#23545;&#27604;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#23545;&#27604;&#35270;&#22270;&#22686;&#24378;&#31574;&#30053;&#12289;&#20301;&#32622;&#24863;&#30693;&#21644;&#35821;&#20041;&#24863;&#30693;&#27491;&#26679;&#26412;&#37319;&#26679;&#31574;&#30053;&#20197;&#21450;&#20998;&#23618;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#26469;&#20811;&#26381;&#22270;&#25968;&#25454;&#22686;&#24378;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#26500;&#22270;&#34920;&#36798;&#20102;&#29616;&#23454;&#19990;&#30028;&#20013;&#22797;&#26434;&#20851;&#31995;&#65292;&#21253;&#25324;&#22810;&#31181;&#31867;&#22411;&#30340;&#33410;&#28857;&#21644;&#36793;&#12290;&#21463;&#33258;&#30417;&#30563;&#23398;&#20064;&#21551;&#21457;&#65292;&#23545;&#27604;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;(HGNNs)&#21033;&#29992;&#25968;&#25454;&#22686;&#24378;&#21644;&#36776;&#21035;&#22120;&#23637;&#29616;&#20102;&#24040;&#22823;&#28508;&#21147;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22270;&#30340;&#31163;&#25955;&#21644;&#25277;&#35937;&#29305;&#24615;&#65292;&#25968;&#25454;&#22686;&#24378;&#20173;&#28982;&#23384;&#22312;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;\textit{&#29983;&#25104;-&#23545;&#27604;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;(GC-HGNN)}&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02810v1 Announce Type: new  Abstract: Heterogeneous Graphs (HGs) can effectively model complex relationships in the real world by multi-type nodes and edges. In recent years, inspired by self-supervised learning, contrastive Heterogeneous Graphs Neural Networks (HGNNs) have shown great potential by utilizing data augmentation and discriminators for downstream tasks. However, data augmentation is still limited due to the discrete and abstract nature of graphs. To tackle the above limitations, we propose a novel \textit{Generative-Contrastive Heterogeneous Graph Neural Network (GC-HGNN)}. Specifically, we first propose a heterogeneous graph generative learning enhanced contrastive paradigm. This paradigm includes: 1) A contrastive view augmentation strategy by using masked autoencoder. 2) Position-aware and semantics-aware positive sample sampling strategy for generate hard negative samples. 3) A hierarchical contrastive learning strategy for capturing local and global informa
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;EMVB&#65292;&#21033;&#29992;&#20301;&#21521;&#37327;&#23454;&#29616;&#39640;&#25928;&#30340;&#22810;&#21521;&#37327;&#23494;&#38598;&#26816;&#32034;&#65292;&#36890;&#36807;&#24341;&#20837;&#36136;&#24515;&#20132;&#20114;&#26426;&#21046;&#26469;&#20943;&#23569;&#38750;&#30456;&#20851;&#25991;&#26723;&#65292;&#38477;&#20302;&#25490;&#21517;&#38454;&#27573;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2404.02805</link><description>&lt;p&gt;
&#20351;&#29992;&#20301;&#21521;&#37327;&#36827;&#34892;&#39640;&#25928;&#30340;&#22810;&#21521;&#37327;&#23494;&#38598;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Efficient Multi-Vector Dense Retrieval Using Bit Vectors
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02805
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;EMVB&#65292;&#21033;&#29992;&#20301;&#21521;&#37327;&#23454;&#29616;&#39640;&#25928;&#30340;&#22810;&#21521;&#37327;&#23494;&#38598;&#26816;&#32034;&#65292;&#36890;&#36807;&#24341;&#20837;&#36136;&#24515;&#20132;&#20114;&#26426;&#21046;&#26469;&#20943;&#23569;&#38750;&#30456;&#20851;&#25991;&#26723;&#65292;&#38477;&#20302;&#25490;&#21517;&#38454;&#27573;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23494;&#38598;&#26816;&#32034;&#25216;&#26415;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#26597;&#35810;&#21644;&#27573;&#33853;&#30340;&#39640;&#32500;&#34920;&#31034;&#65292;&#20351;&#29992;&#39640;&#25928;&#30340;&#30456;&#20284;&#24230;&#25514;&#26045;&#35745;&#31639;&#27573;&#33853;&#30456;&#23545;&#20110;&#26597;&#35810;&#30340;&#30456;&#20851;&#24615;&#12290;&#26368;&#36817;&#65292;PLAID&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#36136;&#24515;&#30340;&#26415;&#35821;&#34920;&#31034;&#26469;&#20943;&#23569;&#22810;&#21521;&#37327;&#20307;&#31995;&#30340;&#20869;&#23384;&#24433;&#21709;&#65292;&#36890;&#36807;&#21033;&#29992;&#36136;&#24515;&#20132;&#20114;&#26426;&#21046;&#65292;PLAID&#36807;&#28388;&#20986;&#38750;&#30456;&#20851;&#25991;&#26723;&#65292;&#20174;&#32780;&#20943;&#23569;&#21518;&#32493;&#25490;&#21517;&#38454;&#27573;&#30340;&#25104;&#26412;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21517;&#20026;&#8220;&#20351;&#29992;&#20301;&#21521;&#37327;&#36827;&#34892;&#39640;&#25928;&#30340;&#22810;&#21521;&#37327;&#23494;&#38598;&#26816;&#32034;&#8221;&#65288;EMVB&#65289;&#65292;&#29992;&#20110;&#22810;&#21521;&#37327;&#23494;&#38598;&#26816;&#32034;&#20013;&#30340;&#39640;&#25928;&#26597;&#35810;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02805v1 Announce Type: new  Abstract: Dense retrieval techniques employ pre-trained large language models to build a high-dimensional representation of queries and passages. These representations compute the relevance of a passage w.r.t. to a query using efficient similarity measures. In this line, multi-vector representations show improved effectiveness at the expense of a one-order-of-magnitude increase in memory footprint and query latency by encoding queries and documents on a per-token level. Recently, PLAID has tackled these problems by introducing a centroid-based term representation to reduce the memory impact of multi-vector systems. By exploiting a centroid interaction mechanism, PLAID filters out non-relevant documents, thus reducing the cost of the successive ranking stages. This paper proposes ``Efficient Multi-Vector dense retrieval with Bit vectors'' (EMVB), a novel framework for efficient query processing in multi-vector dense retrieval. First, EMVB employs a
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#28151;&#21512;&#32467;&#26500;&#21270;&#25688;&#35201;&#21644;&#22522;&#20110;LLM&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#25913;&#36827;&#20102;&#20027;&#39064;&#30456;&#20851;&#24615;&#27169;&#22411;&#65292;&#20351;&#20854;&#33021;&#22815;&#26356;&#22909;&#22320;&#23398;&#20064;&#26597;&#35810;&#19982;&#25991;&#26723;&#20043;&#38388;&#30340;&#30456;&#20851;&#24230;&#12290;</title><link>https://arxiv.org/abs/2404.02616</link><description>&lt;p&gt;
&#36890;&#36807;&#28151;&#21512;&#32467;&#26500;&#21270;&#25688;&#35201;&#21644;&#22522;&#20110;LLM&#30340;&#25968;&#25454;&#22686;&#24378;&#26469;&#25913;&#36827;&#20027;&#39064;&#30456;&#20851;&#24615;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Improving Topic Relevance Model by Mix-structured Summarization and LLM-based Data Augmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02616
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#28151;&#21512;&#32467;&#26500;&#21270;&#25688;&#35201;&#21644;&#22522;&#20110;LLM&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#25913;&#36827;&#20102;&#20027;&#39064;&#30456;&#20851;&#24615;&#27169;&#22411;&#65292;&#20351;&#20854;&#33021;&#22815;&#26356;&#22909;&#22320;&#23398;&#20064;&#26597;&#35810;&#19982;&#25991;&#26723;&#20043;&#38388;&#30340;&#30456;&#20851;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26597;&#35810;&#21644;&#25991;&#26723;&#20043;&#38388;&#30340;&#20027;&#39064;&#30456;&#20851;&#24615;&#26159;&#31038;&#20132;&#25628;&#32034;&#30340;&#19968;&#20010;&#38750;&#24120;&#37325;&#35201;&#30340;&#37096;&#20998;&#65292;&#21487;&#20197;&#35780;&#20272;&#25991;&#26723;&#19982;&#29992;&#25143;&#38656;&#27714;&#20043;&#38388;&#30340;&#21305;&#37197;&#31243;&#24230;&#12290;&#22312;&#22823;&#22810;&#25968;&#31038;&#20132;&#25628;&#32034;&#22330;&#26223;&#20013;&#65292;&#22914;&#22823;&#20247;&#28857;&#35780;&#65292;&#24314;&#27169;&#25628;&#32034;&#30456;&#20851;&#24615;&#24635;&#26159;&#38754;&#20020;&#20004;&#20010;&#25361;&#25112;&#12290;&#19968;&#20010;&#26159;&#35768;&#22810;&#31038;&#20132;&#25628;&#32034;&#20013;&#30340;&#25991;&#26723;&#38750;&#24120;&#38271;&#19988;&#21253;&#21547;&#22823;&#37327;&#20887;&#20313;&#20449;&#24687;&#12290;&#21478;&#19968;&#20010;&#38382;&#39064;&#26159;&#25628;&#32034;&#30456;&#20851;&#24615;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#24456;&#38590;&#33719;&#24471;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#22810;&#20998;&#31867;&#30456;&#20851;&#24615;&#27169;&#22411;&#12290;&#20026;&#20102;&#35299;&#20915;&#20197;&#19978;&#20004;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#26597;&#35810;&#19982;&#22522;&#20110;&#26597;&#35810;&#30340;&#25688;&#35201;&#20197;&#21450;&#19981;&#24102;&#26597;&#35810;&#30340;&#25991;&#26723;&#25688;&#35201;&#21512;&#24182;&#65292;&#20316;&#20026;&#20027;&#39064;&#30456;&#20851;&#24615;&#27169;&#22411;&#30340;&#36755;&#20837;&#65292;&#36825;&#26377;&#21161;&#20110;&#27169;&#22411;&#23398;&#20064;&#26597;&#35810;&#21644;&#25991;&#26723;&#26680;&#24515;&#20027;&#39064;&#20043;&#38388;&#30340;&#30456;&#20851;&#24230;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#65292;&#20174;&#29616;&#26377;&#35757;&#32451;&#25968;&#25454;&#20013;&#37325;&#26032;&#32534;&#20889;&#21644;&#29983;&#25104;&#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02616v1 Announce Type: cross  Abstract: Topic relevance between query and document is a very important part of social search, which can evaluate the degree of matching between document and user's requirement. In most social search scenarios such as Dianping, modeling search relevance always faces two challenges. One is that many documents in social search are very long and have much redundant information. The other is that the training data for search relevance model is difficult to get, especially for multi-classification relevance model. To tackle above two problems, we first take query concatenated with the query-based summary and the document summary without query as the input of topic relevance model, which can help model learn the relevance degree between query and the core topic of document. Then, we utilize the language understanding and generation abilities of large language model (LLM) to rewrite and generate query from queries and documents in existing training da
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30340;&#25193;&#23637;&#21644;&#22256;&#38590;&#26597;&#35810;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#25552;&#39640;&#22256;&#38590;&#26597;&#35810;&#30340;&#25490;&#24207;&#24615;&#33021;&#65292;&#32780;&#19981;&#38477;&#20302;&#20854;&#20182;&#26597;&#35810;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2404.02587</link><description>&lt;p&gt;
&#35757;&#32451;&#25193;&#23637;&#26597;&#35810;&#30340;&#25490;&#24207;&#22120;&#30340;&#20986;&#20046;&#24847;&#26009;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Surprising Effectiveness of Rankers Trained on Expanded Queries
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02587
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30340;&#25193;&#23637;&#21644;&#22256;&#38590;&#26597;&#35810;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#25552;&#39640;&#22256;&#38590;&#26597;&#35810;&#30340;&#25490;&#24207;&#24615;&#33021;&#65292;&#32780;&#19981;&#38477;&#20302;&#20854;&#20182;&#26597;&#35810;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#25490;&#24207;&#31995;&#32479;&#20013;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#26159;&#22788;&#29702;&#26597;&#35810;&#20998;&#24067;&#23614;&#37096;&#30340;&#22256;&#38590;&#26597;&#35810;&#12290;&#36825;&#31181;&#22256;&#38590;&#21487;&#33021;&#28304;&#20110;&#23384;&#22312;&#19981;&#24120;&#35265;&#12289;&#26410;&#26126;&#30830;&#25110;&#19981;&#23436;&#25972;&#30340;&#26597;&#35810;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#30456;&#20851;&#25991;&#26723;&#23545;&#35757;&#32451;&#26597;&#35810;&#36827;&#34892;&#20102;&#22522;&#20110;LLM&#30340;&#26597;&#35810;&#25193;&#23637;&#26469;&#25552;&#39640;&#22256;&#38590;&#26597;&#35810;&#30340;&#25490;&#24207;&#24615;&#33021;&#65292;&#32780;&#19981;&#25439;&#23475;&#20854;&#20182;&#26597;&#35810;&#30340;&#24615;&#33021;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#22522;&#20110;LLM&#36827;&#34892;&#26597;&#35810;&#20016;&#23500;&#21270;&#65292;&#20351;&#29992;&#30456;&#20851;&#25991;&#26723;&#36827;&#34892;&#35757;&#32451;&#12290;&#25509;&#19979;&#26469;&#65292;&#19987;&#38376;&#30340;&#25490;&#24207;&#22120;&#20165;&#22312;&#20016;&#23500;&#30340;&#22256;&#38590;&#26597;&#35810;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#32780;&#19981;&#26159;&#22312;&#21407;&#22987;&#26597;&#35810;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#23558;&#26469;&#33258;&#19987;&#38376;&#25490;&#24207;&#22120;&#21644;&#22522;&#26412;&#25490;&#24207;&#22120;&#30340;&#30456;&#20851;&#24615;&#24471;&#20998;&#20197;&#21450;&#20026;&#27599;&#20010;&#26597;&#35810;&#20272;&#35745;&#30340;&#26597;&#35810;&#24615;&#33021;&#24471;&#20998;&#36827;&#34892;&#32452;&#21512;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#21516;&#20110;&#36890;&#24120;&#23545;&#25152;&#26377;&#26597;&#35810;&#20351;&#29992;&#21333;&#20010;&#25490;&#24207;&#22120;&#30340;&#29616;&#26377;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#23545;&#26131;&#26597;&#35810;&#26377;&#20559;&#35265;&#65292;&#26131;&#26597;&#35810;&#26500;&#25104;&#26597;&#35810;&#20998;&#24067;&#30340;&#22823;&#22810;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02587v1 Announce Type: cross  Abstract: An important problem in text-ranking systems is handling the hard queries that form the tail end of the query distribution. The difficulty may arise due to the presence of uncommon, underspecified, or incomplete queries. In this work, we improve the ranking performance of hard or difficult queries without compromising the performance of other queries. Firstly, we do LLM based query enrichment for training queries using relevant documents. Next, a specialized ranker is fine-tuned only on the enriched hard queries instead of the original queries. We combine the relevance scores from the specialized ranker and the base ranker, along with a query performance score estimated for each query. Our approach departs from existing methods that usually employ a single ranker for all queries, which is biased towards easy queries, which form the majority of the query distribution. In our extensive experiments on the DL-Hard dataset, we find that a p
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22810;&#31890;&#24230;&#24341;&#23548;&#30340;&#35299;&#30721;&#22120;&#34701;&#21512;&#65288;MGFiD&#65289;&#65292;&#36890;&#36807;&#36328;&#22810;&#20010;&#31890;&#24230;&#36776;&#21035;&#35777;&#25454;&#65292;&#24182;&#32467;&#21512;&#27573;&#33853;&#37325;&#26032;&#25490;&#24207;&#21644;&#21477;&#23376;&#20998;&#31867;&#65292;&#25552;&#39640;&#24320;&#25918;&#22495;&#38382;&#31572;&#20013;&#35299;&#30721;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2404.02581</link><description>&lt;p&gt;
&#22810;&#31890;&#24230;&#24341;&#23548;&#30340;&#35299;&#30721;&#22120;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
Multi-Granularity Guided Fusion-in-Decoder
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02581
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22810;&#31890;&#24230;&#24341;&#23548;&#30340;&#35299;&#30721;&#22120;&#34701;&#21512;&#65288;MGFiD&#65289;&#65292;&#36890;&#36807;&#36328;&#22810;&#20010;&#31890;&#24230;&#36776;&#21035;&#35777;&#25454;&#65292;&#24182;&#32467;&#21512;&#27573;&#33853;&#37325;&#26032;&#25490;&#24207;&#21644;&#21477;&#23376;&#20998;&#31867;&#65292;&#25552;&#39640;&#24320;&#25918;&#22495;&#38382;&#31572;&#20013;&#35299;&#30721;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24320;&#25918;&#22495;&#38382;&#31572;&#20013;&#65292;&#35782;&#21035;&#30456;&#20851;&#19978;&#19979;&#25991;&#20316;&#20026;&#35777;&#25454;&#24182;&#36991;&#20813;&#22312;&#26816;&#32034;&#32467;&#26524;&#20013;&#20986;&#29616;&#34394;&#20551;&#19978;&#19979;&#25991;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#35299;&#30721;&#38454;&#27573;&#20351;&#29992;&#22810;&#20010;&#19978;&#19979;&#25991;&#36827;&#34892;&#20018;&#32852;&#30340;&#27169;&#22411;&#26550;&#26500;&#65288;&#21363;Fusion-in-Decoder&#65289;&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#65292;&#20294;&#20250;&#20174;&#30475;&#20284;&#21512;&#29702;&#30340;&#19978;&#19979;&#25991;&#20013;&#29983;&#25104;&#19981;&#27491;&#30830;&#30340;&#36755;&#20986;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#31890;&#24230;&#24341;&#23548;&#30340;&#35299;&#30721;&#22120;&#34701;&#21512;&#65288;MGFiD&#65289;&#65292;&#36328;&#22810;&#20010;&#31890;&#24230;&#36776;&#21035;&#35777;&#25454;&#12290;&#22522;&#20110;&#22810;&#20219;&#21153;&#23398;&#20064;&#65292;MGFiD&#23558;&#27573;&#33853;&#37325;&#26032;&#25490;&#24207;&#19982;&#21477;&#23376;&#20998;&#31867;&#36827;&#34892;&#21327;&#35843;&#12290;&#23427;&#23558;&#26126;&#26174;&#30340;&#21477;&#23376;&#32858;&#21512;&#21040;&#19968;&#20010;&#38170;&#23450;&#21521;&#37327;&#20013;&#65292;&#25351;&#23548;&#35299;&#30721;&#22120;&#12290;&#27492;&#22806;&#65292;&#23427;&#36890;&#36807;&#37325;&#29992;&#27573;&#33853;&#37325;&#26032;&#25490;&#24207;&#30340;&#32467;&#26524;&#26469;&#25552;&#39640;&#35299;&#30721;&#25928;&#29575;&#36827;&#34892;&#27573;&#33853;&#20462;&#21098;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;MGFiD&#22312;&#33258;&#28982;&#38382;&#31572;&#65288;NQ&#65289;&#21644;TriviaQA&#65288;TQA&#65289;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#65292;&#31361;&#26174;&#20102;&#20854;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02581v1 Announce Type: new  Abstract: In Open-domain Question Answering (ODQA), it is essential to discern relevant contexts as evidence and avoid spurious ones among retrieved results. The model architecture that uses concatenated multiple contexts in the decoding phase, i.e., Fusion-in-Decoder, demonstrates promising performance but generates incorrect outputs from seemingly plausible contexts. To address this problem, we propose the Multi-Granularity guided Fusion-in-Decoder (MGFiD), discerning evidence across multiple levels of granularity. Based on multi-task learning, MGFiD harmonizes passage re-ranking with sentence classification. It aggregates evident sentences into an anchor vector that instructs the decoder. Additionally, it improves decoding efficiency by reusing the results of passage re-ranking for passage pruning. Through our experiments, MGFiD outperforms existing models on the Natural Questions (NQ) and TriviaQA (TQA) datasets, highlighting the benefits of i
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20174;&#30334;&#24230;&#25628;&#32034;&#24341;&#25806;&#21457;&#24067;&#30340;&#22823;&#35268;&#27169;&#25628;&#32034;&#25968;&#25454;&#38598;&#20986;&#21457;&#65292;&#25506;&#35752;&#20102;&#26080;&#20559;&#21521;&#23398;&#20064;&#25490;&#24207;&#25216;&#26415;&#22312;&#23454;&#38469;&#25628;&#32034;&#24341;&#25806;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#19982;&#25490;&#21517;&#25439;&#22833;&#21644;&#26597;&#35810;-&#25991;&#26723;&#29305;&#24449;&#36873;&#25321;&#30456;&#27604;&#65292;ULTR&#25216;&#26415;&#24182;&#26410;&#24102;&#26469;&#26126;&#26174;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2404.02543</link><description>&lt;p&gt;
&#26080;&#20559;&#21521;&#23398;&#20064;&#25490;&#24207;&#36935;&#21040;&#29616;&#23454;&#65306;&#30334;&#24230;&#22823;&#35268;&#27169;&#25628;&#32034;&#25968;&#25454;&#38598;&#30340;&#32463;&#39564;&#25945;&#35757;
&lt;/p&gt;
&lt;p&gt;
Unbiased Learning to Rank Meets Reality: Lessons from Baidu's Large-Scale Search Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02543
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20174;&#30334;&#24230;&#25628;&#32034;&#24341;&#25806;&#21457;&#24067;&#30340;&#22823;&#35268;&#27169;&#25628;&#32034;&#25968;&#25454;&#38598;&#20986;&#21457;&#65292;&#25506;&#35752;&#20102;&#26080;&#20559;&#21521;&#23398;&#20064;&#25490;&#24207;&#25216;&#26415;&#22312;&#23454;&#38469;&#25628;&#32034;&#24341;&#25806;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#19982;&#25490;&#21517;&#25439;&#22833;&#21644;&#26597;&#35810;-&#25991;&#26723;&#29305;&#24449;&#36873;&#25321;&#30456;&#27604;&#65292;ULTR&#25216;&#26415;&#24182;&#26410;&#24102;&#26469;&#26126;&#26174;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#20559;&#21521;&#23398;&#20064;&#25490;&#24207;&#65288;ULTR&#65289;&#26159;&#19968;&#20010;&#29992;&#20110;&#23398;&#20064;&#29992;&#25143;&#28857;&#20987;&#25968;&#25454;&#30340;&#25104;&#29087;&#26694;&#26550;&#65292;&#32780;&#36825;&#20123;&#25968;&#25454;&#24448;&#24448;&#21463;&#25910;&#38598;&#25968;&#25454;&#30340;&#25490;&#21517;&#32773;&#30340;&#20559;&#35265;&#24433;&#21709;&#12290;&#34429;&#28982;&#22312;&#29702;&#35770;&#19978;&#24471;&#21040;&#35777;&#26126;&#24182;&#22312;&#27169;&#25311;&#20013;&#36827;&#34892;&#20102;&#24191;&#27867;&#27979;&#35797;&#65292;&#20294;ULTR&#25216;&#26415;&#32570;&#20047;&#32463;&#39564;&#39564;&#35777;&#65292;&#23588;&#20854;&#26159;&#22312;&#29616;&#20195;&#25628;&#32034;&#24341;&#25806;&#20013;&#12290;&#30334;&#24230;&#25628;&#32034;&#24341;&#25806;&#21457;&#24067;&#30340;WSDM Cup 2023&#25968;&#25454;&#38598;&#20026;&#35780;&#20272;&#20027;&#35201;ULTR&#25216;&#26415;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#34920;&#29616;&#25552;&#20379;&#20102;&#38590;&#24471;&#30340;&#26426;&#20250;&#12290;&#23613;&#31649;&#22312;WSDM Cup 2023&#26399;&#38388;&#26377;&#22810;&#27425;&#25552;&#20132;&#65292;&#20197;&#21450;&#38543;&#21518;&#30340;NTCIR ULTRE-2&#20219;&#21153;&#65292;&#20294;&#30446;&#21069;&#36824;&#19981;&#28165;&#26970;&#35266;&#23519;&#21040;&#30340;&#25913;&#36827;&#26159;&#21542;&#28304;&#33258;&#24212;&#29992;ULTR&#25110;&#20854;&#20182;&#23398;&#20064;&#25216;&#26415;&#12290;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#24182;&#25193;&#23637;&#20102;&#29616;&#26377;&#23454;&#39564;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#26080;&#20559;&#21521;&#23398;&#20064;&#25490;&#24207;&#25216;&#26415;&#24182;&#19981;&#33021;&#26126;&#26174;&#25552;&#21319;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#19982;&#25490;&#21517;&#25439;&#22833;&#21644;&#26597;&#35810;-&#25991;&#26723;&#29305;&#24449;&#36873;&#25321;&#24102;&#26469;&#30340;&#26126;&#26174;&#24046;&#24322;&#30456;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02543v1 Announce Type: cross  Abstract: Unbiased learning-to-rank (ULTR) is a well-established framework for learning from user clicks, which are often biased by the ranker collecting the data. While theoretically justified and extensively tested in simulation, ULTR techniques lack empirical validation, especially on modern search engines. The dataset released for the WSDM Cup 2023, collected from Baidu's search engine, offers a rare opportunity to assess the real-world performance of prominent ULTR techniques. Despite multiple submissions during the WSDM Cup 2023 and the subsequent NTCIR ULTRE-2 task, it remains unclear whether the observed improvements stem from applying ULTR or other learning techniques. We revisit and extend the available experiments. We find that unbiased learning-to-rank techniques do not bring clear performance improvements, especially compared to the stark differences brought by the choice of ranking loss and query-document features. Our experiments 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DUQGen&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#33258;&#21160;&#29983;&#25104;&#30340;&#26377;&#25928;&#21644;&#22810;&#26679;&#21270;&#30340;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;&#26469;&#36827;&#34892;&#29616;&#20195;&#31070;&#32463;&#25490;&#24207;&#22120;&#30340;&#26032;&#39046;&#22495;&#24494;&#35843;</title><link>https://arxiv.org/abs/2404.02489</link><description>&lt;p&gt;
DUQGen: &#36890;&#36807;&#22810;&#26679;&#21270;&#21512;&#25104;&#26597;&#35810;&#29983;&#25104;&#23454;&#29616;&#31070;&#32463;&#25490;&#24207;&#22120;&#30340;&#26377;&#25928;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
DUQGen: Effective Unsupervised Domain Adaptation of Neural Rankers by Diversifying Synthetic Query Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02489
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DUQGen&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#33258;&#21160;&#29983;&#25104;&#30340;&#26377;&#25928;&#21644;&#22810;&#26679;&#21270;&#30340;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;&#26469;&#36827;&#34892;&#29616;&#20195;&#31070;&#32463;&#25490;&#24207;&#22120;&#30340;&#26032;&#39046;&#22495;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20808;&#36827;&#30340;&#22312;&#22823;&#22411;&#20219;&#21153;&#29305;&#23450;&#35757;&#32451;&#25968;&#25454;&#38598;(&#22914;MS-MARCO)&#19978;&#39044;&#35757;&#32451;&#30340;&#31070;&#32463;&#25490;&#24207;&#22120;&#22312;&#21508;&#31181;&#25490;&#21517;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#29978;&#33267;&#26080;&#38656;&#39046;&#22495;&#33258;&#36866;&#24212;&#21363;&#21487;&#23454;&#29616;&#38646;-shot&#25928;&#26524;&#12290;&#28982;&#32780;&#65292;&#38646;-shot&#31070;&#32463;&#25490;&#24207;&#21487;&#33021;&#19981;&#22815;&#20248;&#21270;&#65292;&#22240;&#20026;&#23427;&#26410;&#21033;&#29992;&#30446;&#26631;&#39046;&#22495;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#25490;&#21517;&#30340;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;DUQGen&#65292;&#22635;&#34917;&#20102;&#20808;&#21069;&#25991;&#29486;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#31354;&#30333;&#65292;&#21363;&#22914;&#20309;&#33258;&#21160;&#29983;&#25104;&#26082;&#26377;&#25928;&#21448;&#22810;&#26679;&#21270;&#30340;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;&#65292;&#20197;&#24494;&#35843;&#29616;&#20195;&#31070;&#32463;&#25490;&#24207;&#22120;&#36866;&#24212;&#26032;&#39046;&#22495;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;DUQGen&#36890;&#36807;&#35782;&#21035;&#30456;&#20284;&#25991;&#26723;&#30340;&#32858;&#31867;&#20135;&#29983;&#20102;&#26356;&#26377;&#25928;&#30340;&#30446;&#26631;&#39046;&#22495;&#34920;&#31034;&#65307;&#24182;&#29983;&#25104;&#20102;&#26356;&#22810;&#20803;&#21270;&#30340;trai
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02489v1 Announce Type: cross  Abstract: State-of-the-art neural rankers pre-trained on large task-specific training data such as MS-MARCO, have been shown to exhibit strong performance on various ranking tasks without domain adaptation, also called zero-shot. However, zero-shot neural ranking may be sub-optimal, as it does not take advantage of the target domain information. Unfortunately, acquiring sufficiently large and high quality target training data to improve a modern neural ranker can be costly and time-consuming. To address this problem, we propose a new approach to unsupervised domain adaptation for ranking, DUQGen, which addresses a critical gap in prior literature, namely how to automatically generate both effective and diverse synthetic training data to fine tune a modern neural ranker for a new domain. Specifically, DUQGen produces a more effective representation of the target domain by identifying clusters of similar documents; and generates a more diverse tra
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#25552;&#31034;&#24037;&#31243;&#26041;&#27861;&#22914;&#20309;&#22686;&#24378;LLMs&#22312;&#27178;&#21521;&#24605;&#32771;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#25581;&#31034;&#20102;&#20854;&#22266;&#26377;&#30340;&#36229;&#36234;&#24605;&#32500;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#21387;&#32553;&#30340;&#20449;&#24687;&#24615;&#25552;&#31034;&#21644;&#21160;&#24577;&#30340;&#24773;&#22659;&#23398;&#20064;&#26174;&#33879;&#25552;&#21319;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2404.02474</link><description>&lt;p&gt;
uTeBC-NLP&#22312;SemEval-2024&#20219;&#21153;9&#20013;&#65306;LLMs&#33021;&#25104;&#20026;&#27178;&#21521;&#24605;&#32771;&#32773;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
uTeBC-NLP at SemEval-2024 Task 9: Can LLMs be Lateral Thinkers?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02474
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#25552;&#31034;&#24037;&#31243;&#26041;&#27861;&#22914;&#20309;&#22686;&#24378;LLMs&#22312;&#27178;&#21521;&#24605;&#32771;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#25581;&#31034;&#20102;&#20854;&#22266;&#26377;&#30340;&#36229;&#36234;&#24605;&#32500;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#21387;&#32553;&#30340;&#20449;&#24687;&#24615;&#25552;&#31034;&#21644;&#21160;&#24577;&#30340;&#24773;&#22659;&#23398;&#20064;&#26174;&#33879;&#25552;&#21319;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#20154;&#31867;&#35748;&#30693;&#21551;&#21457;&#65292;Jiang&#31561;&#20154;&#65288;2023c&#65289;&#21019;&#24314;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;LLMs&#27178;&#21521;&#24605;&#32500;&#65288;&#36229;&#36234;&#24605;&#32500;&#23450;&#21183;&#65289;&#30340;&#22522;&#20934;&#12290;&#22312;&#36825;&#19968;&#22522;&#20934;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;&#30340;&#25552;&#31034;&#26041;&#27861;&#22914;&#20309;&#22686;&#24378;LLMs&#22312;&#36825;&#19968;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#20197;&#25581;&#31034;&#20854;&#22266;&#26377;&#30340;&#36229;&#36234;&#24605;&#32500;&#33021;&#21147;&#12290;&#36890;&#36807;&#21442;&#21152;SemEval-2024&#30340;&#31532;9&#39033;&#20219;&#21153;&#65292;&#21363;&#21477;&#23376;&#25340;&#22270;&#23376;&#20219;&#21153;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#25552;&#31034;&#24037;&#31243;&#26041;&#27861;&#65306;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#21644;&#30452;&#25509;&#25552;&#31034;&#65292;&#20351;&#29992;&#20449;&#24687;&#24615;&#25551;&#36848;&#36827;&#34892;&#22686;&#24378;&#65292;&#24182;&#21033;&#29992;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#31649;&#36947;&#36827;&#34892;&#24773;&#22659;&#21270;&#25552;&#31034;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#28041;&#21450;&#19977;&#31181;LLMs&#65292;&#21253;&#25324;GPT-3.5&#12289;GPT-4&#21644;Zephyr-7B-beta&#12290;&#25105;&#20204;&#20351;&#29992;GPT-4&#29983;&#25104;&#20102;&#35868;&#39064;&#21644;&#36873;&#39033;&#20043;&#38388;&#30340;&#24605;&#32500;&#36335;&#24452;&#25968;&#25454;&#38598;&#65292;&#24182;&#36890;&#36807;&#20154;&#31867;&#36827;&#34892;&#20102;&#36136;&#37327;&#39564;&#35777;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#21387;&#32553;&#30340;&#20449;&#24687;&#24615;&#25552;&#31034;&#33021;&#22815;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#12290;&#21160;&#24577;&#30340;&#24773;&#22659;&#23398;&#20064;&#26174;&#33879;&#25552;&#21319;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02474v1 Announce Type: cross  Abstract: Inspired by human cognition, Jiang et al.(2023c) create a benchmark for assessing LLMs' lateral thinking-thinking outside the box. Building upon this benchmark, we investigate how different prompting methods enhance LLMs' performance on this task to reveal their inherent power for outside-the-box thinking ability. Through participating in SemEval-2024, task 9, Sentence Puzzle sub-task, we explore prompt engineering methods: chain of thoughts (CoT) and direct prompting, enhancing with informative descriptions, and employing contextualizing prompts using a retrieval augmented generation (RAG) pipeline. Our experiments involve three LLMs including GPT-3.5, GPT-4, and Zephyr-7B-beta. We generate a dataset of thinking paths between riddles and options using GPT-4, validated by humans for quality. Findings indicate that compressed informative prompts enhance performance. Dynamic in-context learning enhances model performance significantly. F
&lt;/p&gt;</description></item><item><title>Token Trails&#26159;&#19968;&#31181;&#21033;&#29992;token-type&#23884;&#20837;&#23548;&#33322;&#23545;&#35805;&#20013;&#22797;&#26434;&#19978;&#19979;&#25991;&#32454;&#24494;&#24046;&#21035;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#39640;&#23545;&#35805;&#29702;&#35299;&#21644;&#22238;&#22797;&#29983;&#25104;&#25928;&#26524;&#65292;&#22312;&#20419;&#36827;&#19978;&#19979;&#25991;&#24847;&#35782;&#32842;&#22825;&#26426;&#22120;&#20154;&#20132;&#20114;&#26041;&#38754;&#20855;&#26377;&#21069;&#27839;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2404.02402</link><description>&lt;p&gt;
&#20351;&#29992;ChatLLM&#22312;&#23545;&#35805;AI&#20013;&#23548;&#33322;&#35821;&#22659;&#28145;&#24230;&#30340;Token Trails
&lt;/p&gt;
&lt;p&gt;
Token Trails: Navigating Contextual Depths in Conversational AI with ChatLLM
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02402
&lt;/p&gt;
&lt;p&gt;
Token Trails&#26159;&#19968;&#31181;&#21033;&#29992;token-type&#23884;&#20837;&#23548;&#33322;&#23545;&#35805;&#20013;&#22797;&#26434;&#19978;&#19979;&#25991;&#32454;&#24494;&#24046;&#21035;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#39640;&#23545;&#35805;&#29702;&#35299;&#21644;&#22238;&#22797;&#29983;&#25104;&#25928;&#26524;&#65292;&#22312;&#20419;&#36827;&#19978;&#19979;&#25991;&#24847;&#35782;&#32842;&#22825;&#26426;&#22120;&#20154;&#20132;&#20114;&#26041;&#38754;&#20855;&#26377;&#21069;&#27839;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36827;&#34892;&#23545;&#35805;&#24314;&#27169;&#38656;&#35201;&#23545;&#19978;&#19979;&#25991;&#36827;&#34892;&#32454;&#33268;&#29702;&#35299;&#65292;&#20197;&#29983;&#25104;&#36830;&#36143;&#19988;&#19982;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#22238;&#22797;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Token Trails&#65292;&#36825;&#26159;&#19968;&#31181;&#21033;&#29992;token-type&#23884;&#20837;&#26469;&#23548;&#33322;&#23545;&#35805;&#20013;&#22797;&#26434;&#19978;&#19979;&#25991;&#32454;&#24494;&#24046;&#21035;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21033;&#29992;token-type&#23884;&#20837;&#26469;&#21306;&#20998;&#29992;&#25143;&#35805;&#35821;&#21644;&#26426;&#22120;&#20154;&#22238;&#22797;&#65292;&#20174;&#32780;&#20419;&#36827;&#29983;&#25104;&#20855;&#26377;&#19978;&#19979;&#25991;&#24847;&#35782;&#30340;&#22238;&#22797;&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#23454;&#39564;&#21644;&#35780;&#20272;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;Token Trails&#22312;&#25552;&#39640;&#23545;&#35805;&#29702;&#35299;&#21644;&#22238;&#22797;&#29983;&#25104;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#31361;&#26174;&#20102;&#23545;&#35805;AI&#20013;&#19978;&#19979;&#25991;&#24314;&#27169;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#24378;&#35843;&#20102;Token Trails&#22312;&#25512;&#21160;&#35813;&#39046;&#22495;&#21457;&#23637;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#20026;&#26356;&#22797;&#26434;&#21644;&#20855;&#26377;&#19978;&#19979;&#25991;&#24847;&#35782;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#20132;&#20114;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02402v1 Announce Type: cross  Abstract: Conversational modeling using Large Language Models (LLMs) requires a nuanced understanding of context to generate coherent and contextually relevant responses. In this paper, we present Token Trails, a novel approach that leverages token-type embeddings to navigate the intricate contextual nuances within conversations. Our framework utilizes token-type embeddings to distinguish between user utterances and bot responses, facilitating the generation of context-aware replies. Through comprehensive experimentation and evaluation, we demonstrate the effectiveness of Token Trails in improving conversational understanding and response generation, achieving state-of-the-art performance. Our results highlight the significance of contextual modeling in conversational AI and underscore the promising potential of Token Trails to advance the field, paving the way for more sophisticated and contextually aware chatbot interactions.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;&#29983;&#25104;AI&#39046;&#22495;&#32593;&#31449;&#20869;&#23481;&#25511;&#21046;&#30340;&#29616;&#29366;&#65292;&#25552;&#20986;&#20102;&#35299;&#20915;&#20869;&#23481;&#21019;&#20316;&#32773;&#21644;&#21457;&#24067;&#32773;&#30693;&#35782;&#20135;&#26435;&#20445;&#25252;&#38382;&#39064;&#30340;&#25216;&#26415;&#26041;&#26696;&#21644;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2404.02309</link><description>&lt;p&gt;
&#19968;&#20010;&#20851;&#20110;&#29983;&#25104;AI&#30340;&#32593;&#32476;&#20869;&#23481;&#25511;&#21046;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey of Web Content Control for Generative AI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02309
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;&#29983;&#25104;AI&#39046;&#22495;&#32593;&#31449;&#20869;&#23481;&#25511;&#21046;&#30340;&#29616;&#29366;&#65292;&#25552;&#20986;&#20102;&#35299;&#20915;&#20869;&#23481;&#21019;&#20316;&#32773;&#21644;&#21457;&#24067;&#32773;&#30693;&#35782;&#20135;&#26435;&#20445;&#25252;&#38382;&#39064;&#30340;&#25216;&#26415;&#26041;&#26696;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22260;&#32469;&#29983;&#25104;AI&#30340;&#31361;&#30772;&#24615;&#36827;&#23637;&#24341;&#36215;&#20102;&#19968;&#27874;&#20851;&#27880;&#65292;&#23548;&#33268;&#20102;&#19968;&#31995;&#21015;&#35785;&#35772;&#65292;&#21253;&#25324;&#23545;Stability AI&#21644;OpenAI&#30340;&#37325;&#22823;&#34892;&#21160;&#12290;&#27861;&#24459;&#19981;&#30830;&#23450;&#24615;&#30340;&#36825;&#31181;&#24773;&#20917;&#24341;&#21457;&#20102;&#23545;&#20869;&#23481;&#21019;&#20316;&#32773;&#21644;&#21457;&#24067;&#32773;&#20445;&#25252;&#20854;&#22312;&#32593;&#32476;&#19978;&#30693;&#35782;&#20135;&#26435;&#30340;&#26435;&#21033;&#30340;&#24191;&#27867;&#35752;&#35770;&#12290;&#27431;&#27954;&#21644;&#32654;&#22269;&#30340;&#27861;&#24459;&#24050;&#32463;&#25552;&#20379;&#20102;&#22823;&#33268;&#30340;&#25351;&#23548;&#26041;&#38024;&#65292;&#20026;&#35843;&#33410;&#32593;&#32476;&#25968;&#25454;&#20351;&#29992;&#30340;&#25216;&#26415;&#35299;&#20915;&#26041;&#26696;&#25351;&#26126;&#20102;&#26041;&#21521;&#12290;&#22312;&#36825;&#20010;&#36807;&#31243;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#24050;&#32463;&#33268;&#21147;&#20110;&#20247;&#22810;&#32593;&#32476;&#26631;&#20934;&#21644;&#36873;&#25321;&#36864;&#20986;&#26684;&#24335;&#65292;&#20351;&#20986;&#29256;&#21830;&#26377;&#33021;&#21147;&#23558;&#20182;&#20204;&#30340;&#25968;&#25454;&#25490;&#38500;&#22312;&#29983;&#25104;AI&#27169;&#22411;&#30340;&#21457;&#23637;&#20043;&#22806;&#12290;&#26032;&#20852;&#30340;AI/ML&#36873;&#25321;&#36864;&#20986;&#21327;&#35758;&#22312;&#25968;&#25454;&#20027;&#26435;&#26041;&#38754;&#20855;&#26377;&#20215;&#20540;&#65292;&#20294;&#21516;&#26102;&#65292;&#23545;&#20110;&#34987;&#26368;&#36817;&#35832;&#22810;&#20020;&#26102;&#26631;&#20934;&#25152;&#28153;&#27809;&#30340;&#32593;&#31449;&#25152;&#26377;&#32773;&#26469;&#35828;&#65292;&#36825;&#21448;&#21019;&#36896;&#20102;&#19968;&#31181;&#19981;&#21033;&#30340;&#23616;&#38754;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24076;&#26395;&#35843;&#26597;&#19981;&#21516;&#30340;&#32593;&#31449;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02309v1 Announce Type: new  Abstract: The groundbreaking advancements around generative AI have recently caused a wave of concern culminating in a row of lawsuits, including high-profile actions against Stability AI and OpenAI. This situation of legal uncertainty has sparked a broad discussion on the rights of content creators and publishers to protect their intellectual property on the web. European as well as US law already provides rough guidelines, setting a direction for technical solutions to regulate web data use. In this course, researchers and practitioners have worked on numerous web standards and opt-out formats that empower publishers to keep their data out of the development of generative AI models. The emerging AI/ML opt-out protocols are valuable in regards to data sovereignty, but again, it creates an adverse situation for a site owners who are overwhelmed by the multitude of recent ad hoc standards to consider. In our work, we want to survey the different pr
&lt;/p&gt;</description></item><item><title>&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#65292;&#36890;&#36807;&#23558;LLMs&#38598;&#25104;&#21040;&#20027;&#21160;&#23398;&#20064;&#24490;&#29615;&#20013;&#36827;&#34892;&#25968;&#25454;&#27880;&#37322;&#65292;&#26377;&#25928;&#20943;&#23569;&#25152;&#38656;&#25968;&#25454;&#37327;&#65292;&#24182;&#21462;&#24471;&#25509;&#36817;&#26368;&#20808;&#36827;&#24615;&#33021;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2404.02261</link><description>&lt;p&gt;
&#22312;LLMs&#20013;&#24490;&#29615;&#65306;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27880;&#37322;&#36827;&#34892;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
LLMs in the Loop: Leveraging Large Language Model Annotations for Active Learning in Low-Resource Languages
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02261
&lt;/p&gt;
&lt;p&gt;
&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#65292;&#36890;&#36807;&#23558;LLMs&#38598;&#25104;&#21040;&#20027;&#21160;&#23398;&#20064;&#24490;&#29615;&#20013;&#36827;&#34892;&#25968;&#25454;&#27880;&#37322;&#65292;&#26377;&#25928;&#20943;&#23569;&#25152;&#38656;&#25968;&#25454;&#37327;&#65292;&#24182;&#21462;&#24471;&#25509;&#36817;&#26368;&#20808;&#36827;&#24615;&#33021;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#35821;&#35328;&#36164;&#28304;&#21644;&#25968;&#25454;&#26631;&#27880;&#19987;&#19994;&#30693;&#35782;&#26377;&#38480;&#65292;&#20302;&#36164;&#28304;&#35821;&#35328;&#22312;&#20154;&#24037;&#26234;&#33021;&#24320;&#21457;&#20013;&#38754;&#20020;&#30528;&#37325;&#22823;&#38556;&#30861;&#65292;&#20351;&#23427;&#20204;&#21464;&#24471;&#32597;&#35265;&#19988;&#25104;&#26412;&#39640;&#26114;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#19981;&#36275;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;LLMs&#30340;&#28508;&#21147;&#22312;&#20027;&#21160;&#23398;&#20064;&#29615;&#33410;&#20013;&#36827;&#34892;&#25968;&#25454;&#27880;&#37322;&#12290;&#25105;&#20204;&#39318;&#20808;&#36827;&#34892;&#35780;&#20272;&#20197;&#35780;&#20272;&#27880;&#37322;&#32773;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#65292;&#20174;&#32780;&#36873;&#25321;&#36866;&#24403;&#30340;LLM&#27880;&#37322;&#32773;&#12290;&#28982;&#21518;&#65292;&#36873;&#25321;&#30340;&#27880;&#37322;&#32773;&#34987;&#38598;&#25104;&#21040;&#19968;&#20010;&#20998;&#31867;&#22120;&#30340;&#35757;&#32451;&#24490;&#29615;&#20013;&#65292;&#20351;&#29992;&#20027;&#21160;&#23398;&#20064;&#33539;&#24335;&#65292;&#26368;&#23567;&#21270;&#25152;&#38656;&#30340;&#26597;&#35810;&#25968;&#25454;&#37327;&#12290;&#23454;&#35777;&#35780;&#20272;&#65292;&#29305;&#21035;&#26159;&#20351;&#29992;GPT-4-Turbo&#65292;&#23637;&#31034;&#20102;&#20960;&#20046;&#36798;&#21040;&#26368;&#20808;&#36827;&#24615;&#33021;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#22823;&#22823;&#20943;&#23569;&#20102;&#25968;&#25454;&#38656;&#27714;&#65292;&#30001;&#20272;&#31639;&#30340;&#28508;&#22312;&#24615;&#33021;&#25351;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02261v1 Announce Type: cross  Abstract: Low-resource languages face significant barriers in AI development due to limited linguistic resources and expertise for data labeling, rendering them rare and costly. The scarcity of data and the absence of preexisting tools exacerbate these challenges, especially since these languages may not be adequately represented in various NLP datasets. To address this gap, we propose leveraging the potential of LLMs in the active learning loop for data annotation. Initially, we conduct evaluations to assess inter-annotator agreement and consistency, facilitating the selection of a suitable LLM annotator. The chosen annotator is then integrated into a training loop for a classifier using an active learning paradigm, minimizing the amount of queried data required. Empirical evaluations, notably employing GPT-4-Turbo, demonstrate near-state-of-the-art performance with significantly reduced data requirements, as indicated by estimated potential co
&lt;/p&gt;</description></item><item><title>RAT&#27169;&#22411;&#26159;&#20026;&#20102;&#35299;&#20915;&#24403;&#21069;CTR&#39044;&#27979;&#27169;&#22411;&#20165;&#20851;&#27880;&#26679;&#26412;&#20869;&#29305;&#24449;&#20132;&#20114;&#32780;&#24573;&#30053;&#36328;&#26679;&#26412;&#20851;&#31995;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#26816;&#32034;&#30456;&#20284;&#26679;&#26412;&#26500;&#24314;&#22686;&#24378;&#36755;&#20837;&#65292;&#23454;&#29616;&#20102;&#23545;&#26679;&#26412;&#20869;&#21644;&#36328;&#26679;&#26412;&#30340;&#20840;&#38754;&#29305;&#24449;&#20132;&#20114;&#25512;&#29702;&#65292;&#25552;&#39640;&#20102;CTR&#39044;&#27979;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2404.02249</link><description>&lt;p&gt;
RAT: &#26816;&#32034;&#22686;&#24378;&#21464;&#25442;&#22120;&#29992;&#20110;&#28857;&#20987;&#29575;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
RAT: Retrieval-Augmented Transformer for Click-Through Rate Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02249
&lt;/p&gt;
&lt;p&gt;
RAT&#27169;&#22411;&#26159;&#20026;&#20102;&#35299;&#20915;&#24403;&#21069;CTR&#39044;&#27979;&#27169;&#22411;&#20165;&#20851;&#27880;&#26679;&#26412;&#20869;&#29305;&#24449;&#20132;&#20114;&#32780;&#24573;&#30053;&#36328;&#26679;&#26412;&#20851;&#31995;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#26816;&#32034;&#30456;&#20284;&#26679;&#26412;&#26500;&#24314;&#22686;&#24378;&#36755;&#20837;&#65292;&#23454;&#29616;&#20102;&#23545;&#26679;&#26412;&#20869;&#21644;&#36328;&#26679;&#26412;&#30340;&#20840;&#38754;&#29305;&#24449;&#20132;&#20114;&#25512;&#29702;&#65292;&#25552;&#39640;&#20102;CTR&#39044;&#27979;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#28857;&#20987;&#29575;&#65288;CTR&#65289;&#26159;Web&#24212;&#29992;&#31243;&#24207;&#30340;&#22522;&#26412;&#20219;&#21153;&#65292;&#20854;&#20013;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#26159;&#35774;&#35745;&#26377;&#25928;&#30340;&#29305;&#24449;&#20132;&#20114;&#27169;&#22411;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#20110;&#23545;&#21333;&#20010;&#26679;&#26412;&#20869;&#30340;&#29305;&#24449;&#20132;&#20114;&#36827;&#34892;&#24314;&#27169;&#65292;&#32780;&#24573;&#30053;&#20102;&#21487;&#20197;&#20316;&#20026;&#21442;&#32771;&#32972;&#26223;&#26469;&#22686;&#24378;&#39044;&#27979;&#30340;&#28508;&#22312;&#36328;&#26679;&#26412;&#20851;&#31995;&#12290;&#20026;&#24357;&#34917;&#36825;&#31181;&#19981;&#36275;&#65292;&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#21464;&#25442;&#22120;&#65288;RAT&#65289;&#65292;&#26088;&#22312;&#33719;&#21462;&#26679;&#26412;&#20869;&#21644;&#36328;&#26679;&#26412;&#20043;&#38388;&#30340;&#32454;&#31890;&#24230;&#29305;&#24449;&#20132;&#20114;&#12290;&#36890;&#36807;&#26816;&#32034;&#30456;&#20284;&#26679;&#26412;&#65292;&#25105;&#20204;&#20026;&#27599;&#20010;&#30446;&#26631;&#26679;&#26412;&#26500;&#24314;&#22686;&#24378;&#36755;&#20837;&#12290;&#28982;&#21518;&#21033;&#29992;&#32423;&#32852;&#27880;&#24847;&#21147;&#26500;&#24314;Transformer&#23618;&#65292;&#20197;&#25429;&#33719;&#26679;&#26412;&#20869;&#21644;&#36328;&#26679;&#26412;&#29305;&#24449;&#20132;&#20114;&#65292;&#20419;&#36827;&#20840;&#38754;&#25512;&#29702;&#20197;&#25913;&#21892;CTR&#39044;&#27979;&#30340;&#21516;&#26102;&#20445;&#25345;&#25928;&#29575;&#12290;&#23545;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#23454;&#20102;RAT&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02249v1 Announce Type: cross  Abstract: Predicting click-through rates (CTR) is a fundamental task for Web applications, where a key issue is to devise effective models for feature interactions. Current methodologies predominantly concentrate on modeling feature interactions within an individual sample, while overlooking the potential cross-sample relationships that can serve as a reference context to enhance the prediction. To make up for such deficiency, this paper develops a Retrieval-Augmented Transformer (RAT), aiming to acquire fine-grained feature interactions within and across samples. By retrieving similar samples, we construct augmented input for each target sample. We then build Transformer layers with cascaded attention to capture both intra- and cross-sample feature interactions, facilitating comprehensive reasoning for improved CTR prediction while retaining efficiency. Extensive experiments on real-world datasets substantiate the effectiveness of RAT and sugge
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;NFT&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#32508;&#21512;&#21033;&#29992;NFT&#20132;&#26131;&#35760;&#24405;&#21644;&#22806;&#37096;&#39033;&#30446;&#29305;&#24449;&#31561;&#22810;&#31181;&#25968;&#25454;&#28304;&#65292;&#36890;&#36807;&#25968;&#25454;&#39640;&#25928;&#30340;&#22522;&#20110;&#22270;&#30340;&#26041;&#27861;&#29983;&#25104;&#20010;&#24615;&#21270;&#25512;&#33616;&#65292;&#24182;&#21033;&#29992;&#36229;&#20986;&#29992;&#25143;-&#39033;&#30446;&#20114;&#21160;&#30340;&#36755;&#20837;&#39564;&#35777;&#20102;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.18305</link><description>&lt;p&gt;
&#19968;&#31181;&#20855;&#26377;&#39033;&#30446;&#29305;&#24449;&#30340;NFT&#21487;&#25910;&#34255;&#21697;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
A Recommender System for NFT Collectibles with Item Feature
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18305
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;NFT&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#32508;&#21512;&#21033;&#29992;NFT&#20132;&#26131;&#35760;&#24405;&#21644;&#22806;&#37096;&#39033;&#30446;&#29305;&#24449;&#31561;&#22810;&#31181;&#25968;&#25454;&#28304;&#65292;&#36890;&#36807;&#25968;&#25454;&#39640;&#25928;&#30340;&#22522;&#20110;&#22270;&#30340;&#26041;&#27861;&#29983;&#25104;&#20010;&#24615;&#21270;&#25512;&#33616;&#65292;&#24182;&#21033;&#29992;&#36229;&#20986;&#29992;&#25143;-&#39033;&#30446;&#20114;&#21160;&#30340;&#36755;&#20837;&#39564;&#35777;&#20102;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#24050;&#34987;&#31215;&#26497;&#30740;&#31350;&#24182;&#24212;&#29992;&#20110;&#21508;&#20010;&#39046;&#22495;&#20197;&#35299;&#20915;&#20449;&#24687;&#36807;&#36733;&#38382;&#39064;&#12290;&#23613;&#31649;&#26377;&#35768;&#22810;&#20851;&#20110;&#30005;&#24433;&#12289;&#38899;&#20048;&#21644;&#30005;&#23376;&#21830;&#21153;&#30340;&#25512;&#33616;&#31995;&#32479;&#30340;&#30740;&#31350;&#65292;&#20294;&#30456;&#27604;&#20043;&#19979;&#65292;&#23613;&#31649;NFT&#24066;&#22330;&#25345;&#32493;&#22686;&#38271;&#65292;&#23545;&#20110;NFT&#30340;&#25512;&#33616;&#31995;&#32479;&#21364;&#21463;&#21040;&#20102;&#30456;&#23545;&#36739;&#23569;&#30340;&#20851;&#27880;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;NFT&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#21033;&#29992;&#21508;&#31181;&#25968;&#25454;&#28304;&#65292;&#20174;NFT&#20132;&#26131;&#35760;&#24405;&#21040;&#22806;&#37096;&#39033;&#30446;&#29305;&#24449;&#65292;&#29983;&#25104;&#31526;&#21512;&#20010;&#20154;&#20559;&#22909;&#30340;&#31934;&#30830;&#25512;&#33616;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#25968;&#25454;&#39640;&#25928;&#30340;&#22522;&#20110;&#22270;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#20197;&#26377;&#25928;&#25429;&#25417;&#27599;&#20010;&#39033;&#30446;&#19982;&#29992;&#25143;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#65292;&#24182;&#29983;&#25104;&#21253;&#21547;&#33410;&#28857;&#29305;&#24449;&#20449;&#24687;&#21644;&#22270;&#32467;&#26500;&#30340;&#33410;&#28857;&#65288;&#39033;&#30446;&#65289;&#23884;&#20837;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#36229;&#20986;&#29992;&#25143;-&#39033;&#30446;&#20114;&#21160;&#30340;&#36755;&#20837;&#65292;&#22914;&#22270;&#20687;&#29305;&#24449;&#12289;&#25991;&#26412;&#29305;&#24449;&#21644;&#20215;&#26684;&#29305;&#24449;&#12290;&#25968;&#20540;&#23454;&#39564;&#35777;&#23454;&#20102;&#35813;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18305v1 Announce Type: cross  Abstract: Recommender systems have been actively studied and applied in various domains to deal with information overload. Although there are numerous studies on recommender systems for movies, music, and e-commerce, comparatively less attention has been paid to the recommender system for NFTs despite the continuous growth of the NFT market. This paper presents a recommender system for NFTs that utilizes a variety of data sources, from NFT transaction records to external item features, to generate precise recommendations that cater to individual preferences. We develop a data-efficient graph-based recommender system to efficiently capture the complex relationship between each item and users and generate node(item) embeddings which incorporate both node feature information and graph structure. Furthermore, we exploit inputs beyond user-item interactions, such as image feature, text feature, and price feature. Numerical experiments verify the perf
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#32654;&#22269;&#22269;&#20250;&#22270;&#20070;&#39302;&#20027;&#39064;&#26631;&#22836;&#30340;&#28508;&#21147;&#65292;&#23637;&#31034;&#20102;&#20854;&#23545;&#20110;&#35299;&#20915;&#23398;&#26415;&#22270;&#20070;&#39302;&#24453;&#32534;&#30446;&#39033;&#30446;&#31215;&#21387;&#38382;&#39064;&#20855;&#26377;&#25112;&#30053;&#24212;&#23545;&#24847;&#20041;&#65292;&#21516;&#26102;&#20063;&#24378;&#35843;&#20102;&#20154;&#31867;&#32534;&#30446;&#21592;&#20173;&#28982;&#22312;&#39564;&#35777;&#21644;&#22686;&#24378;&#29983;&#25104;&#20027;&#39064;&#26631;&#22836;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.16424</link><description>&lt;p&gt;
&#20351;&#29992;ChatGPT&#20026;&#30005;&#23376;&#23398;&#20301;&#35770;&#25991;&#25351;&#23450;LCSH&#20027;&#39064;&#30340;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
An Experiment with the Use of ChatGPT for LCSH Subject Assignment on Electronic Theses and Dissertations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16424
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#32654;&#22269;&#22269;&#20250;&#22270;&#20070;&#39302;&#20027;&#39064;&#26631;&#22836;&#30340;&#28508;&#21147;&#65292;&#23637;&#31034;&#20102;&#20854;&#23545;&#20110;&#35299;&#20915;&#23398;&#26415;&#22270;&#20070;&#39302;&#24453;&#32534;&#30446;&#39033;&#30446;&#31215;&#21387;&#38382;&#39064;&#20855;&#26377;&#25112;&#30053;&#24212;&#23545;&#24847;&#20041;&#65292;&#21516;&#26102;&#20063;&#24378;&#35843;&#20102;&#20154;&#31867;&#32534;&#30446;&#21592;&#20173;&#28982;&#22312;&#39564;&#35777;&#21644;&#22686;&#24378;&#29983;&#25104;&#20027;&#39064;&#26631;&#22836;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#32654;&#22269;&#22269;&#20250;&#22270;&#20070;&#39302;&#20027;&#39064;&#26631;&#22836;&#65288;LCSH&#65289;&#30340;&#28508;&#21147;&#12290;&#20316;&#32773;&#20351;&#29992;ChatGPT&#26681;&#25454;&#30005;&#23376;&#23398;&#20301;&#35770;&#25991;&#30340;&#26631;&#39064;&#21644;&#25688;&#35201;&#29983;&#25104;&#20027;&#39064;&#26631;&#22836;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#23613;&#31649;&#19968;&#20123;&#29983;&#25104;&#30340;&#20027;&#39064;&#26631;&#22836;&#26159;&#26377;&#25928;&#30340;&#65292;&#20294;&#23384;&#22312;&#29305;&#23450;&#24615;&#21644;&#35814;&#23613;&#24615;&#26041;&#38754;&#30340;&#38382;&#39064;&#12290;&#35813;&#30740;&#31350;&#23637;&#31034;&#20102;LLMs&#21487;&#20197;&#20316;&#20026;&#23398;&#26415;&#22270;&#20070;&#39302;&#24453;&#32534;&#30446;&#39033;&#30446;&#30340;&#25112;&#30053;&#24615;&#24212;&#23545;&#25514;&#26045;&#65292;&#21516;&#26102;&#20063;&#25552;&#20379;&#20102;&#19968;&#31181;&#25104;&#26412;&#25928;&#30410;&#39640;&#19988;&#24555;&#36895;&#29983;&#25104;LCSH&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#32534;&#30446;&#21592;&#20173;&#28982;&#26159;&#39564;&#35777;&#21644;&#22686;&#24378;LLMs&#29983;&#25104;&#30340;LCSH&#30340;&#26377;&#25928;&#24615;&#12289;&#35814;&#23613;&#24615;&#21644;&#29305;&#23450;&#24615;&#30340;&#24517;&#35201;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16424v1 Announce Type: new  Abstract: This study delves into the potential use of Large Language Models (LLMs) for generating Library of Congress Subject Headings (LCSH). The authors employed ChatGPT to generate subject headings for electronic theses and dissertations (ETDs) based on their titles and summaries. The results revealed that although some generated subject headings were valid, there were issues regarding specificity and exhaustiveness. The study showcases that LLMs can serve as a strategic response to the backlog of items awaiting cataloging in academic libraries, while also offering a cost-effective approach for promptly generating LCSH. Nonetheless, human catalogers remain essential for verifying and enhancing the validity, exhaustiveness, and specificity of LCSH generated by LLMs.
&lt;/p&gt;</description></item><item><title>Text2Pic Swift&#26694;&#26550;&#38024;&#23545;&#22823;&#35268;&#27169;&#24211;&#20013;&#25991;&#26412;&#25551;&#36848;&#21040;&#22270;&#20687;&#30340;&#26816;&#32034;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#19988;&#24378;&#22823;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#38454;&#27573;&#31574;&#30053;&#35299;&#20915;&#20102;&#38271;&#25991;&#26412;&#26597;&#35810;&#20013;&#30340;&#27495;&#20041;&#38382;&#39064;</title><link>https://arxiv.org/abs/2402.15276</link><description>&lt;p&gt;
Text2Pic Swift&#65306;&#22686;&#24378;&#22823;&#35268;&#27169;&#24211;&#20013;&#38271;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Text2Pic Swift: Enhancing Long-Text to Image Retrieval for Large-Scale Libraries
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15276
&lt;/p&gt;
&lt;p&gt;
Text2Pic Swift&#26694;&#26550;&#38024;&#23545;&#22823;&#35268;&#27169;&#24211;&#20013;&#25991;&#26412;&#25551;&#36848;&#21040;&#22270;&#20687;&#30340;&#26816;&#32034;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#19988;&#24378;&#22823;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#38454;&#27573;&#31574;&#30053;&#35299;&#20915;&#20102;&#38271;&#25991;&#26412;&#26597;&#35810;&#20013;&#30340;&#27495;&#20041;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15276v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#39046;&#22495; &#25688;&#35201;&#65306;&#25991;&#26412;&#21040;&#22270;&#20687;&#26816;&#32034;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#21253;&#25324;&#25968;&#23383;&#22270;&#20070;&#39302;&#12289;&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#21644;&#22810;&#23186;&#20307;&#25968;&#25454;&#24211;&#65292;&#36890;&#36807;&#20351;&#29992;&#25991;&#26412;&#26597;&#35810;&#26469;&#25628;&#32034;&#22270;&#20687;&#12290;&#23613;&#31649;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#21462;&#24471;&#20102;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#22312;&#22823;&#35268;&#27169;&#12289;&#22810;&#26679;&#21270;&#21644;&#27169;&#31946;&#30340;&#26816;&#32034;&#22330;&#26223;&#20013;&#30340;&#36866;&#29992;&#24615;&#21463;&#21040;&#26174;&#30528;&#30340;&#35745;&#31639;&#38656;&#27714;&#21644;&#29983;&#25104;&#21487;&#27880;&#20837;&#30340;&#23884;&#20837;&#25152;&#38480;&#21046;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;Text2Pic Swift&#26694;&#26550;&#65292;&#19987;&#20026;&#22312;&#24222;&#22823;&#30340;&#25968;&#25454;&#38598;&#20013;&#26377;&#25928;&#21644;&#31283;&#20581;&#22320;&#26816;&#32034;&#19982;&#24191;&#27867;&#25991;&#26412;&#25551;&#36848;&#23545;&#24212;&#30340;&#22270;&#20687;&#32780;&#35774;&#35745;&#12290;&#35813;&#26694;&#26550;&#37319;&#29992;&#20102;&#20004;&#38454;&#27573;&#26041;&#27861;&#65306;&#21021;&#22987;&#22522;&#20110;&#23454;&#20307;&#30340;&#25490;&#24207;&#65288;ER&#65289;&#38454;&#27573;&#36890;&#36807;&#22810;&#26597;&#35810;&#23545;&#22810;&#30446;&#26631;&#30340;&#31574;&#30053;&#26469;&#35299;&#20915;&#38271;&#25991;&#26412;&#26597;&#35810;&#20013;&#22266;&#26377;&#30340;&#27495;&#20041;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#32553;&#23567;&#20102;&#21487;&#33021;&#30340;&#20505;&#36873;&#39033;&#65292;&#20197;&#20415;&#36827;&#34892;&#21518;&#32493;&#20998;&#26512;&#12290;&#25509;&#19979;&#26469;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15276v1 Announce Type: cross  Abstract: Text-to-image retrieval plays a crucial role across various applications, including digital libraries, e-commerce platforms, and multimedia databases, by enabling the search for images using text queries. Despite the advancements in Multimodal Large Language Models (MLLMs), which offer leading-edge performance, their applicability in large-scale, varied, and ambiguous retrieval scenarios is constrained by significant computational demands and the generation of injective embeddings. This paper introduces the Text2Pic Swift framework, tailored for efficient and robust retrieval of images corresponding to extensive textual descriptions in sizable datasets. The framework employs a two-tier approach: the initial Entity-based Ranking (ER) stage addresses the ambiguity inherent in lengthy text queries through a multiple-queries-to-multiple-targets strategy, effectively narrowing down potential candidates for subsequent analysis. Following thi
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#23637;&#31034;&#20102;n-gram&#35821;&#35328;&#27169;&#22411;&#30340;&#20215;&#20540;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;infini-gram&#30340;&#24341;&#25806;&#65292;&#23427;&#21487;&#20197;&#20197;&#27627;&#31186;&#32423;&#30340;&#24310;&#36831;&#35745;&#31639;&#20219;&#24847;n&#30340;n-gram&#27010;&#29575;&#65292;&#20351;&#24471;&#22312;&#31070;&#32463;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23545;&#25991;&#26412;&#36827;&#34892;&#26356;&#20934;&#30830;&#30340;&#20998;&#26512;&#25104;&#20026;&#21487;&#33021;&#12290;</title><link>https://arxiv.org/abs/2401.17377</link><description>&lt;p&gt;
&#26080;&#38480;-gram&#65306;&#23558;&#26080;&#38480;n-gram&#35821;&#35328;&#27169;&#22411;&#25193;&#23637;&#21040;&#19975;&#20159;&#26631;&#35760;
&lt;/p&gt;
&lt;p&gt;
Infini-gram: Scaling Unbounded n-gram Language Models to a Trillion Tokens
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17377
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#23637;&#31034;&#20102;n-gram&#35821;&#35328;&#27169;&#22411;&#30340;&#20215;&#20540;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;infini-gram&#30340;&#24341;&#25806;&#65292;&#23427;&#21487;&#20197;&#20197;&#27627;&#31186;&#32423;&#30340;&#24310;&#36831;&#35745;&#31639;&#20219;&#24847;n&#30340;n-gram&#27010;&#29575;&#65292;&#20351;&#24471;&#22312;&#31070;&#32463;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23545;&#25991;&#26412;&#36827;&#34892;&#26356;&#20934;&#30830;&#30340;&#20998;&#26512;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31070;&#32463;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26102;&#20195;&#65292;n-gram&#35821;&#35328;&#27169;&#22411;&#36824;&#20855;&#26377;&#30456;&#20851;&#24615;&#21527;&#65311;&#25105;&#20204;&#30340;&#31572;&#26696;&#26159;&#32943;&#23450;&#30340;&#65292;&#24182;&#19988;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#25991;&#26412;&#20998;&#26512;&#21644;&#25913;&#36827;&#31070;&#32463;LLM&#26041;&#38754;&#30340;&#20215;&#20540;&#12290;&#28982;&#32780;&#65292;&#36825;&#38656;&#35201;&#22312;&#20004;&#20010;&#26041;&#38754;&#23545;n-gram&#27169;&#22411;&#36827;&#34892;&#29616;&#20195;&#21270;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;&#23427;&#20204;&#19982;&#31070;&#32463;LLM&#30456;&#21516;&#30340;&#25968;&#25454;&#35268;&#27169;&#35757;&#32451;- 1.4&#19975;&#20159;&#20010;&#26631;&#35760;&#12290;&#36825;&#26159;&#36804;&#20170;&#20026;&#27490;&#26500;&#24314;&#30340;&#26368;&#22823;&#30340;n-gram&#27169;&#22411;&#12290;&#20854;&#27425;&#65292;&#29616;&#26377;&#30340;n-gram&#27169;&#22411;&#20351;&#29992;&#30340;n&#24456;&#23567;&#65292;&#36825;&#22952;&#30861;&#20102;&#23427;&#20204;&#30340;&#24615;&#33021;&#65307;&#30456;&#21453;&#65292;&#25105;&#20204;&#20801;&#35768;n&#21487;&#20197;&#26159;&#20219;&#24847;&#22823;&#30340;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#26032;&#30340;&#26080;&#38480;-gram LM&#19982;&#22238;&#36864;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;infini-gram&#30340;&#24341;&#25806;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#21518;&#32512;&#25968;&#32452;&#35745;&#31639;&#26080;&#38480;-gram&#65288;&#20197;&#21450;&#20219;&#24847;n&#30340;n-gram&#65289;&#27010;&#29575;&#65292;&#24182;&#19988;&#20855;&#26377;&#27627;&#31186;&#32423;&#30340;&#24310;&#36831;&#65292;&#32780;&#26080;&#38656;&#39044;&#20808;&#35745;&#31639;n-gram&#35745;&#25968;&#34920;&#65288;&#36825;&#23558;&#38750;&#24120;&#26114;&#36149;&#65289;&#12290;&#26080;&#38480;-gram&#26694;&#26550;&#21644;infini-gram&#24341;&#25806;&#20351;&#25105;&#20204;&#33021;&#22815;&#23545;&#20154;&#31867;&#20889;&#20316;&#21644;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#36827;&#34892;&#35768;&#22810;&#26032;&#39062;&#21644;&#26377;&#24847;&#24605;&#30340;&#20998;&#26512;&#65306;&#25105;&#20204;&#21457;&#29616;&#26080;&#38480;-gram LM...
&lt;/p&gt;
&lt;p&gt;
Are n-gram language models still relevant in this era of neural large language models (LLMs)? Our answer is yes, and we show their values in both text analysis and improving neural LLMs. Yet this necessitates modernizing n-gram models in two aspects. First, we train them at the same data scale as neural LLMs -- 1.4 trillion tokens. This is the largest n-gram model ever built. Second, existing n-gram models use small n which hinders their performance; we instead allow n to be arbitrarily large, by introducing a new $\infty$-gram LM with backoff. Instead of pre-computing n-gram count tables (which would be very expensive), we develop an engine named infini-gram -- powered by suffix arrays -- that can compute $\infty$-gram (as well as n-gram with arbitrary n) probabilities with millisecond-level latency. The $\infty$-gram framework and infini-gram engine enable us to conduct many novel and interesting analyses of human-written and machine-generated text: we find that the $\infty$-gram LM 
&lt;/p&gt;</description></item><item><title>SCTc-TE&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#38754;&#30340;&#26102;&#38388;&#20107;&#20214;&#39044;&#27979;&#24418;&#24335;&#21270;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#26500;&#24314;MidEast-TE&#25968;&#25454;&#38598;&#21644;&#21306;&#20998;&#21508;&#31181;&#19978;&#19979;&#25991;&#20449;&#24687;&#25552;&#21319;&#20102;&#39044;&#27979;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2312.01052</link><description>&lt;p&gt;
SCTc-TE&#65306;&#19968;&#31181;&#20840;&#38754;&#30340;&#24418;&#24335;&#21270;&#21644;&#22522;&#20934;&#27979;&#35797;&#29992;&#20110;&#26102;&#38388;&#20107;&#20214;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
SCTc-TE: A Comprehensive Formulation and Benchmark for Temporal Event Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.01052
&lt;/p&gt;
&lt;p&gt;
SCTc-TE&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#38754;&#30340;&#26102;&#38388;&#20107;&#20214;&#39044;&#27979;&#24418;&#24335;&#21270;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#26500;&#24314;MidEast-TE&#25968;&#25454;&#38598;&#21644;&#21306;&#20998;&#21508;&#31181;&#19978;&#19979;&#25991;&#20449;&#24687;&#25552;&#21319;&#20102;&#39044;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#22797;&#26434;&#20107;&#20214;&#39044;&#27979;&#26088;&#22312;&#26681;&#25454;&#21382;&#21490;&#19978;&#35266;&#23519;&#21040;&#30340;&#20107;&#20214;&#26469;&#39044;&#27979;&#26410;&#26469;&#20107;&#20214;&#12290;&#22823;&#22810;&#25968;&#26102;&#38388;&#22797;&#26434;&#20107;&#20214;&#30340;&#24418;&#24335;&#21270;&#26159;&#38750;&#32467;&#26500;&#21270;&#30340;&#25110;&#32570;&#20047;&#24191;&#27867;&#30340;&#26102;&#38388;&#20449;&#24687;&#65292;&#23548;&#33268;&#34920;&#24449;&#19981;&#36275;&#21644;&#39044;&#27979;&#33021;&#21147;&#26377;&#38480;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20123;&#24046;&#36317;&#65292;&#25105;&#20204;&#21019;&#26032;&#22320;&#24341;&#20837;&#20102;&#32467;&#26500;&#21270;&#12289;&#22797;&#26434;&#21644;&#26102;&#38388;&#23436;&#25972;&#30340;&#26102;&#38388;&#20107;&#20214;&#65288;SCTc-TE&#65289;&#30340;&#24418;&#24335;&#21270;&#12290;&#22312;&#36825;&#20010;&#20840;&#38754;&#30340;&#24418;&#24335;&#21270;&#20043;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#23436;&#20840;&#33258;&#21160;&#21270;&#30340;&#27969;&#31243;&#65292;&#24182;&#20174;&#32422;60&#19975;&#31687;&#26032;&#38395;&#25991;&#31456;&#20013;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;MidEast-TE&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#20391;&#37325;&#20110;2015&#24180;&#33267;2022&#24180;&#38388;&#20027;&#35201;&#28041;&#21450;&#20013;&#19996;&#22320;&#21306;&#21508;&#22269;&#20043;&#38388;&#30340;&#21512;&#20316;&#21644;&#20914;&#31361;&#20107;&#20214;&#12290;&#38500;&#20102;&#25968;&#25454;&#38598;&#26500;&#24314;&#20043;&#22806;&#65292;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#36890;&#36807;&#21306;&#20998;&#21508;&#31181;&#19978;&#19979;&#25991;&#20449;&#24687;&#65288;&#21363;&#65292;&#26412;&#22320;&#21644;&#20840;&#29699;&#19978;&#19979;&#25991;&#65289;&#26469;&#25512;&#36827;&#39044;&#27979;&#26041;&#27861;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.01052v2 Announce Type: replace-cross  Abstract: Temporal complex event forecasting aims to predict the future events given the observed events from history. Most formulations of temporal complex event are unstructured or without extensive temporal information, resulting in inferior representations and limited forecasting capabilities. To bridge these gaps, we innovatively introduce the formulation of Structured, Complex, and Time-complete temporal event (SCTc-TE). Following this comprehensive formulation, we develop a fully automated pipeline and construct a large-scale dataset named MidEast-TE from about 0.6 million news articles. This dataset focuses on the cooperation and conflict events among countries mainly in the MidEast region from 2015 to 2022. Not limited to the dataset construction, more importantly, we advance the forecasting methods by discriminating the crucial roles of various contextual information, i.e., local and global contexts. Thereby, we propose a novel
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Agent-OM&#65292;&#21033;&#29992;LLM&#20195;&#29702;&#20026;&#26412;&#20307;&#21305;&#37197;&#31995;&#32479;&#24341;&#20837;&#20102;&#26032;&#30340;&#35774;&#35745;&#33539;&#24335;&#12290;</title><link>https://arxiv.org/abs/2312.00326</link><description>&lt;p&gt;
Agent-OM&#65306;&#21033;&#29992;LLM&#20195;&#29702;&#36827;&#34892;&#26412;&#20307;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Agent-OM: Leveraging LLM Agents for Ontology Matching
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.00326
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Agent-OM&#65292;&#21033;&#29992;LLM&#20195;&#29702;&#20026;&#26412;&#20307;&#21305;&#37197;&#31995;&#32479;&#24341;&#20837;&#20102;&#26032;&#30340;&#35774;&#35745;&#33539;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#20307;&#21305;&#37197;&#65288;OM&#65289;&#33021;&#22815;&#23454;&#29616;&#19981;&#21516;&#26412;&#20307;&#20043;&#38388;&#30340;&#35821;&#20041;&#20114;&#25805;&#20316;&#24615;&#65292;&#36890;&#36807;&#23545;&#40784;&#30456;&#20851;&#23454;&#20307;&#26469;&#35299;&#20915;&#20854;&#27010;&#24565;&#24322;&#26500;&#24615;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#20195;&#29702;&#30340;LLM&#35774;&#35745;&#33539;&#24335;&#65292;&#21629;&#21517;&#20026;Agent-OM&#65292;&#21253;&#25324;&#20004;&#20010;&#29992;&#20110;&#26816;&#32034;&#21644;&#21305;&#37197;&#30340;&#21516;&#20307;&#20195;&#29702;&#20197;&#21450;&#19968;&#32452;&#22522;&#20110;&#25552;&#31034;&#30340;&#31616;&#21333;OM&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.00326v2 Announce Type: replace  Abstract: Ontology matching (OM) enables semantic interoperability between different ontologies and resolves their conceptual heterogeneity by aligning related entities. OM systems currently have two prevailing design paradigms: conventional knowledge-based expert systems and newer machine learning-based predictive systems. While large language models (LLMs) and LLM agents have revolutionised data engineering and have been applied creatively in many domains, their potential for OM remains underexplored. This study introduces a novel agent-powered LLM-based design paradigm for OM systems. With consideration of several specific challenges in leveraging LLM agents for OM, we propose a generic framework, namely Agent-OM, consisting of two Siamese agents for retrieval and matching, with a set of simple prompt-based OM tools. Our framework is implemented in a proof-of-concept system. Evaluations of three Ontology Alignment Evaluation Initiative (OAE
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;CARLG&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#19978;&#19979;&#25991;&#32447;&#32034;&#21644;&#35282;&#33394;&#30456;&#20851;&#24615;&#65292;&#25552;&#21319;&#20102;&#25991;&#26723;&#32423;&#20107;&#20214;&#35770;&#35777;&#25552;&#21462;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2310.05116</link><description>&lt;p&gt;
&#21033;&#29992;&#19978;&#19979;&#25991;&#32447;&#32034;&#21644;&#35282;&#33394;&#30456;&#20851;&#24615;&#25552;&#21319;&#25991;&#26723;&#32423;&#20107;&#20214;&#35770;&#35777;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Utilizing Contextual Clues and Role Correlations for Enhancing Document-level Event Argument Extraction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.05116
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;CARLG&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#19978;&#19979;&#25991;&#32447;&#32034;&#21644;&#35282;&#33394;&#30456;&#20851;&#24615;&#65292;&#25552;&#21319;&#20102;&#25991;&#26723;&#32423;&#20107;&#20214;&#35770;&#35777;&#25552;&#21462;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26723;&#32423;&#20107;&#20214;&#35770;&#35777;&#25552;&#21462;&#65288;EAE&#65289;&#26159;&#20449;&#24687;&#25552;&#21462;&#20013;&#33267;&#20851;&#37325;&#35201;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#23376;&#20219;&#21153;&#20043;&#19968;&#12290;&#29616;&#26377;&#26041;&#27861;&#22823;&#22810;&#20851;&#27880;&#35770;&#35777;&#21644;&#20107;&#20214;&#35302;&#21457;&#22120;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#24573;&#35270;&#20102;&#20004;&#20010;&#20851;&#38190;&#28857;&#65306;&#19978;&#19979;&#25991;&#32447;&#32034;&#30340;&#20449;&#24687;&#21644;&#35770;&#35777;&#35282;&#33394;&#20043;&#38388;&#30340;&#35821;&#20041;&#30456;&#20851;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;CARLG&#27169;&#22411;&#65292;&#21253;&#25324;&#20004;&#20010;&#27169;&#22359;&#65306;&#19978;&#19979;&#25991;&#32447;&#32034;&#32858;&#21512;&#65288;CCA&#65289;&#21644;&#22522;&#20110;&#35282;&#33394;&#30340;&#28508;&#22312;&#20449;&#24687;&#24341;&#23548;&#65288;RLIG&#65289;&#65292;&#36890;&#36807;&#26377;&#25928;&#21033;&#29992;&#19978;&#19979;&#25991;&#32447;&#32034;&#21644;&#35282;&#33394;&#30456;&#20851;&#24615;&#26469;&#25552;&#39640;&#25991;&#26723;&#32423;EAE&#12290;CCA&#27169;&#22359;&#36890;&#36807;&#21033;&#29992;&#26469;&#33258;&#39044;&#35757;&#32451;&#32534;&#30721;&#22120;&#30340;&#19978;&#19979;&#25991;&#27880;&#24847;&#26435;&#37325;&#65292;&#33258;&#36866;&#24212;&#22320;&#25429;&#25417;&#21644;&#25972;&#21512;&#19978;&#19979;&#25991;&#32447;&#32034;&#12290;RLIG&#27169;&#22359;&#36890;&#36807;&#35282;&#33394;&#20132;&#20114;&#32534;&#30721;&#25429;&#25417;&#35821;&#20041;&#30456;&#20851;&#24615;&#65292;&#24182;&#36890;&#36807;&#28508;&#22312;&#35282;&#33394;&#34920;&#31034;&#25552;&#20379;&#23453;&#36149;&#30340;&#20449;&#24687;&#24341;&#23548;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;CCA&#21644;RLIG&#27169;&#22359;&#32039;&#20945;&#12289;&#21487;&#31227;&#26893;&#19988;&#39640;&#25928;&#65292;&#24341;&#20837;&#30340;&#26032;&#21442;&#25968;&#19981;&#36229;&#36807;1%&#65292;&#19988;&#26131;&#20110;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Document-level event argument extraction (EAE) is a vital but challenging subtask in information extraction. Most existing approaches focus on the interaction between arguments and event triggers, ignoring two critical points: the information of contextual clues and the semantic correlations among argument roles. In this paper, we propose the CARLG model, which consists of two modules: Contextual Clues Aggregation (CCA) and Role-based Latent Information Guidance (RLIG), effectively leveraging contextual clues and role correlations for improving document-level EAE. The CCA module adaptively captures and integrates contextual clues by utilizing context attention weights from a pre-trained encoder. The RLIG module captures semantic correlations through role-interactive encoding and provides valuable information guidance with latent role representation. Notably, our CCA and RLIG modules are compact, transplantable and efficient, which introduce no more than 1% new parameters and can be eas
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Self-BioRAG&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#26816;&#32034;&#21644;&#33258;&#25105;&#21453;&#24605;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#21307;&#30103;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;&#35813;&#26694;&#26550;&#19987;&#27880;&#20110;&#29983;&#25104;&#35299;&#37322;&#12289;&#26816;&#32034;&#39046;&#22495;&#29305;&#23450;&#25991;&#26723;&#20197;&#21450;&#23545;&#29983;&#25104;&#30340;&#21709;&#24212;&#36827;&#34892;&#33258;&#25105;&#21453;&#24605;&#12290;</title><link>http://arxiv.org/abs/2401.15269</link><description>&lt;p&gt;
&#36890;&#36807;&#26816;&#32034;&#21644;&#33258;&#25105;&#21453;&#24605;&#25913;&#21892;&#21307;&#30103;&#25512;&#29702;&#33021;&#21147;&#30340;&#26816;&#32034;&#22686;&#24378;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Improving Medical Reasoning through Retrieval and Self-Reflection with Retrieval-Augmented Large Language Models. (arXiv:2401.15269v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15269
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Self-BioRAG&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#26816;&#32034;&#21644;&#33258;&#25105;&#21453;&#24605;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#21307;&#30103;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;&#35813;&#26694;&#26550;&#19987;&#27880;&#20110;&#29983;&#25104;&#35299;&#37322;&#12289;&#26816;&#32034;&#39046;&#22495;&#29305;&#23450;&#25991;&#26723;&#20197;&#21450;&#23545;&#29983;&#25104;&#30340;&#21709;&#24212;&#36827;&#34892;&#33258;&#25105;&#21453;&#24605;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#19987;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#20363;&#22914;GPT-4&#65292;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20013;&#35299;&#20915;&#20102;&#20174;&#22810;&#39033;&#36873;&#25321;&#39064;&#21040;&#38271;&#31687;&#29983;&#25104;&#31561;&#22810;&#26679;&#21270;&#25361;&#25112;&#30340;&#37324;&#31243;&#30865;&#12290;&#20026;&#20102;&#35299;&#20915;LLMs&#32534;&#30721;&#30693;&#35782;&#26080;&#27861;&#22788;&#29702;&#30340;&#25361;&#25112;&#65292;&#24050;&#32463;&#24320;&#21457;&#20102;&#21508;&#31181;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#30693;&#35782;&#35821;&#26009;&#24211;&#20013;&#25628;&#32034;&#25991;&#26723;&#24182;&#26080;&#26465;&#20214;&#25110;&#26377;&#36873;&#25321;&#22320;&#23558;&#20854;&#38468;&#21152;&#21040;LLMs&#30340;&#36755;&#20837;&#26469;&#36827;&#34892;&#29983;&#25104;&#12290;&#28982;&#32780;&#65292;&#23558;&#29616;&#26377;&#26041;&#27861;&#24212;&#29992;&#20110;&#19981;&#21516;&#39046;&#22495;&#29305;&#23450;&#38382;&#39064;&#26102;&#65292;&#20986;&#29616;&#20102;&#27867;&#21270;&#33021;&#21147;&#24046;&#30340;&#38382;&#39064;&#65292;&#23548;&#33268;&#33719;&#21462;&#19981;&#27491;&#30830;&#30340;&#25991;&#26723;&#25110;&#20570;&#20986;&#19981;&#20934;&#30830;&#30340;&#21028;&#26029;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#38752;&#30340;&#21307;&#23398;&#25991;&#26412;&#26694;&#26550;Self-BioRAG&#65292;&#19987;&#38376;&#29992;&#20110;&#29983;&#25104;&#35299;&#37322;&#12289;&#26816;&#32034;&#39046;&#22495;&#29305;&#23450;&#25991;&#26723;&#21644;&#33258;&#25105;&#21453;&#24605;&#29983;&#25104;&#30340;&#21709;&#24212;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;84k&#20010;&#32463;&#36807;&#36807;&#28388;&#30340;&#29983;&#29289;&#21307;&#23398;&#25351;&#20196;&#38598;&#26469;&#35757;&#32451;Self-BioRAG&#65292;&#23427;&#20855;&#22791;&#35780;&#20272;&#33258;&#24049;&#30340;&#22522;&#22240;
&lt;/p&gt;
&lt;p&gt;
Recent proprietary large language models (LLMs), such as GPT-4, have achieved a milestone in tackling diverse challenges in the biomedical domain, ranging from multiple-choice questions to long-form generations. To address challenges that still cannot be handled with the encoded knowledge of LLMs, various retrieval-augmented generation (RAG) methods have been developed by searching documents from the knowledge corpus and appending them unconditionally or selectively to the input of LLMs for generation. However, when applying existing methods to different domain-specific problems, poor generalization becomes apparent, leading to fetching incorrect documents or making inaccurate judgments. In this paper, we introduce Self-BioRAG, a framework reliable for biomedical text that specializes in generating explanations, retrieving domain-specific documents, and self-reflecting generated responses. We utilize 84k filtered biomedical instruction sets to train Self-BioRAG that can assess its gene
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;AI&#21103;&#39550;&#39542;&#21592;&#26469;&#23548;&#33322;&#22797;&#26434;&#25628;&#32034;&#20219;&#21153;&#65292;&#24182;&#25506;&#35752;&#20102;&#29983;&#25104;AI&#21644;&#36741;&#21161;&#20195;&#29702;&#30340;&#20986;&#29616;&#23545;&#20110;&#25903;&#25345;&#22797;&#26434;&#25628;&#32034;&#20219;&#21153;&#30340;&#28508;&#21147;&#21644;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.01235</link><description>&lt;p&gt;
&#20351;&#29992;AI&#21103;&#39550;&#39542;&#21592;&#23548;&#33322;&#22797;&#26434;&#25628;&#32034;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Navigating Complex Search Tasks with AI Copilots. (arXiv:2311.01235v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01235
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;AI&#21103;&#39550;&#39542;&#21592;&#26469;&#23548;&#33322;&#22797;&#26434;&#25628;&#32034;&#20219;&#21153;&#65292;&#24182;&#25506;&#35752;&#20102;&#29983;&#25104;AI&#21644;&#36741;&#21161;&#20195;&#29702;&#30340;&#20986;&#29616;&#23545;&#20110;&#25903;&#25345;&#22797;&#26434;&#25628;&#32034;&#20219;&#21153;&#30340;&#28508;&#21147;&#21644;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27491;&#22914;&#20449;&#24687;&#26816;&#32034;(IR)&#30740;&#31350;&#30028;&#30340;&#35768;&#22810;&#20154;&#25152;&#30693;&#21644;&#27427;&#36175;&#30340;&#37027;&#26679;&#65292;&#25628;&#32034;&#36828;&#26410;&#35299;&#20915;&#12290;&#27599;&#22825;&#37117;&#26377;&#25968;&#30334;&#19975;&#20154;&#22312;&#25628;&#32034;&#24341;&#25806;&#19978;&#38754;&#23545;&#20219;&#21153;&#30340;&#22256;&#38590;&#12290;&#20182;&#20204;&#30340;&#22256;&#38590;&#36890;&#24120;&#19982;&#20219;&#21153;&#30340;&#20869;&#22312;&#22797;&#26434;&#24615;&#20197;&#21450;&#25628;&#32034;&#31995;&#32479;&#26080;&#27861;&#23436;&#20840;&#29702;&#35299;&#20219;&#21153;&#21644;&#25552;&#20379;&#30456;&#20851;&#32467;&#26524;&#26377;&#20851;&#12290;&#20219;&#21153;&#28608;&#21457;&#20102;&#25628;&#32034;&#65292;&#21019;&#24314;&#20102;&#25628;&#32034;&#32773;&#23581;&#35797;&#36830;&#25509;/&#35299;&#20915;&#30340;&#24046;&#36317;/&#38382;&#39064;&#24773;&#20917;&#65292;&#24182;&#22312;&#20182;&#20204;&#22788;&#29702;&#19981;&#21516;&#20219;&#21153;&#26041;&#38754;&#26102;&#39537;&#21160;&#25628;&#32034;&#34892;&#20026;&#12290;&#22797;&#26434;&#25628;&#32034;&#20219;&#21153;&#38656;&#35201;&#30340;&#19981;&#20165;&#26159;&#22522;&#26412;&#20107;&#23454;&#26597;&#25214;&#25110;&#25628;&#32034;&#30340;&#25903;&#25345;&#12290;&#25903;&#25345;&#22797;&#26434;&#20219;&#21153;&#30340;&#26041;&#27861;&#30740;&#31350;&#21253;&#25324;&#29983;&#25104;&#26597;&#35810;&#21644;&#32593;&#31449;&#24314;&#35758;&#65292;&#20010;&#24615;&#21270;&#21644;&#19978;&#19979;&#25991;&#21270;&#25628;&#32034;&#65292;&#20197;&#21450;&#24320;&#21457;&#26032;&#30340;&#25628;&#32034;&#20307;&#39564;&#65292;&#21253;&#25324;&#36328;&#26102;&#38388;&#21644;&#31354;&#38388;&#12290;&#26368;&#36817;&#20852;&#36215;&#30340;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;(AI)&#21644;&#22522;&#20110;&#35813;&#25216;&#26415;&#30340;&#36741;&#21161;&#20195;&#29702;&#65292;&#25110;&#32773;&#35828;&#21103;&#39550;&#39542;&#21592;&#65292;&#30340;&#20986;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
As many of us in the information retrieval (IR) research community know and appreciate, search is far from being a solved problem. Millions of people struggle with tasks on search engines every day. Often, their struggles relate to the intrinsic complexity of their task and the failure of search systems to fully understand the task and serve relevant results. The task motivates the search, creating the gap/problematic situation that searchers attempt to bridge/resolve and drives search behavior as they work through different task facets. Complex search tasks require more than support for rudimentary fact finding or re-finding. Research on methods to support complex tasks includes work on generating query and website suggestions, personalizing and contextualizing search, and developing new search experiences, including those that span time and space. The recent emergence of generative artificial intelligence (AI) and the arrival of assistive agents, or copilots, based on this technology
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;MTAMCR&#30340;&#23545;&#35805;&#25512;&#33616;&#38382;&#39064;&#35774;&#23450;&#65292;&#36890;&#36807;&#27599;&#36718;&#35810;&#38382;&#28085;&#30422;&#22810;&#20010;&#23646;&#24615;&#31867;&#22411;&#30340;&#22810;&#36873;&#39064;&#65292;&#25552;&#39640;&#20102;&#20114;&#21160;&#25928;&#29575;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;Chain-of-Choice&#23618;&#27425;&#21270;&#31574;&#30053;&#23398;&#20064;&#26694;&#26550;&#65292;&#25552;&#39640;&#20102;&#23545;&#35805;&#25512;&#33616;&#31995;&#32479;&#30340;&#35810;&#38382;&#25928;&#29575;&#21644;&#25512;&#33616;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.17922</link><description>&lt;p&gt;
Chain-of-Choice&#23618;&#27425;&#21270;&#31574;&#30053;&#23398;&#20064;&#29992;&#20110;&#23545;&#35805;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Chain-of-Choice Hierarchical Policy Learning for Conversational Recommendation. (arXiv:2310.17922v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17922
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;MTAMCR&#30340;&#23545;&#35805;&#25512;&#33616;&#38382;&#39064;&#35774;&#23450;&#65292;&#36890;&#36807;&#27599;&#36718;&#35810;&#38382;&#28085;&#30422;&#22810;&#20010;&#23646;&#24615;&#31867;&#22411;&#30340;&#22810;&#36873;&#39064;&#65292;&#25552;&#39640;&#20102;&#20114;&#21160;&#25928;&#29575;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;Chain-of-Choice&#23618;&#27425;&#21270;&#31574;&#30053;&#23398;&#20064;&#26694;&#26550;&#65292;&#25552;&#39640;&#20102;&#23545;&#35805;&#25512;&#33616;&#31995;&#32479;&#30340;&#35810;&#38382;&#25928;&#29575;&#21644;&#25512;&#33616;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#25512;&#33616;&#31995;&#32479;&#36890;&#36807;&#22810;&#36718;&#20114;&#21160;&#23545;&#35805;&#26469;&#25581;&#31034;&#29992;&#25143;&#20559;&#22909;&#65292;&#26368;&#32456;&#23548;&#21521;&#31934;&#30830;&#21644;&#28385;&#24847;&#30340;&#25512;&#33616;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#23545;&#35805;&#25512;&#33616;&#31995;&#32479;&#20165;&#38480;&#20110;&#26681;&#25454;&#27599;&#36718;&#21333;&#20010;&#23646;&#24615;&#31867;&#22411;&#65288;&#22914;&#39068;&#33394;&#65289;&#35810;&#38382;&#20108;&#36827;&#21046;&#25110;&#22810;&#36873;&#39064;&#65292;&#23548;&#33268;&#20114;&#21160;&#36718;&#25968;&#36807;&#22810;&#65292;&#38477;&#20302;&#20102;&#29992;&#25143;&#20307;&#39564;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#29616;&#23454;&#21644;&#39640;&#25928;&#30340;&#23545;&#35805;&#25512;&#33616;&#38382;&#39064;&#35774;&#23450;&#65292;&#31216;&#20026;&#22810;&#31867;&#22411;&#23646;&#24615;&#22810;&#36718;&#23545;&#35805;&#25512;&#33616;&#65288;MTAMCR&#65289;&#65292;&#35813;&#38382;&#39064;&#35774;&#23450;&#20351;&#24471;&#23545;&#35805;&#25512;&#33616;&#31995;&#32479;&#33021;&#22815;&#22312;&#27599;&#36718;&#20013;&#35810;&#38382;&#28085;&#30422;&#22810;&#20010;&#23646;&#24615;&#31867;&#22411;&#30340;&#22810;&#36873;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#20114;&#21160;&#25928;&#29575;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#23558;MTAMCR&#23450;&#20041;&#20026;&#19968;&#39033;&#23618;&#27425;&#21270;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;Chain-of-Choice&#23618;&#27425;&#21270;&#31574;&#30053;&#23398;&#20064;&#65288;CoCHPL&#65289;&#26694;&#26550;&#26469;&#25552;&#39640;MTAMCR&#20013;&#30340;&#35810;&#38382;&#25928;&#29575;&#21644;&#25512;&#33616;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conversational Recommender Systems (CRS) illuminate user preferences via multi-round interactive dialogues, ultimately navigating towards precise and satisfactory recommendations. However, contemporary CRS are limited to inquiring binary or multi-choice questions based on a single attribute type (e.g., color) per round, which causes excessive rounds of interaction and diminishes the user's experience. To address this, we propose a more realistic and efficient conversational recommendation problem setting, called Multi-Type-Attribute Multi-round Conversational Recommendation (MTAMCR), which enables CRS to inquire about multi-choice questions covering multiple types of attributes in each round, thereby improving interactive efficiency. Moreover, by formulating MTAMCR as a hierarchical reinforcement learning task, we propose a Chain-of-Choice Hierarchical Policy Learning (CoCHPL) framework to enhance both the questioning efficiency and recommendation effectiveness in MTAMCR. Specifically,
&lt;/p&gt;</description></item></channel></rss>