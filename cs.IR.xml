<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#26412;&#30740;&#31350;&#20851;&#27880;&#22522;&#20110;&#32500;&#24230;&#30340;&#25628;&#32034;&#28548;&#28165;&#25552;&#31034;&#29983;&#25104;&#20013;&#30340;&#32500;&#24230;&#36136;&#37327;&#21644;&#30456;&#20851;&#24615;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#30456;&#20851;&#24615;&#20998;&#31867;&#22120;&#26469;&#35780;&#20272;&#28548;&#28165;&#36807;&#31243;&#20013;&#23384;&#22312;&#30340;&#32500;&#24230;&#30456;&#20851;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.04524</link><description>&lt;p&gt;
&#20998;&#26512;&#22522;&#20110;&#32500;&#24230;&#30340;&#25628;&#32034;&#28548;&#28165;&#25552;&#31034;&#29983;&#25104;&#20013;&#30340;&#30456;&#20851;&#24615;
&lt;/p&gt;
&lt;p&gt;
Analyzing Coherency in Facet-based Clarification Prompt Generation for Search. (arXiv:2401.04524v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04524
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20851;&#27880;&#22522;&#20110;&#32500;&#24230;&#30340;&#25628;&#32034;&#28548;&#28165;&#25552;&#31034;&#29983;&#25104;&#20013;&#30340;&#32500;&#24230;&#36136;&#37327;&#21644;&#30456;&#20851;&#24615;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#30456;&#20851;&#24615;&#20998;&#31867;&#22120;&#26469;&#35780;&#20272;&#28548;&#28165;&#36807;&#31243;&#20013;&#23384;&#22312;&#30340;&#32500;&#24230;&#30456;&#20851;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28548;&#28165;&#29992;&#25143;&#30340;&#20449;&#24687;&#38656;&#27714;&#26159;&#29616;&#20195;&#25628;&#32034;&#31995;&#32479;&#20013;&#24517;&#19981;&#21487;&#23569;&#30340;&#32452;&#25104;&#37096;&#20998;&#12290;&#23613;&#31649;&#22823;&#22810;&#25968;&#26500;&#24314;&#28548;&#28165;&#25552;&#31034;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#26597;&#35810;&#32500;&#24230;&#65292;&#20294;&#32500;&#24230;&#36136;&#37327;&#30340;&#24433;&#21709;&#30456;&#23545;&#26410;&#32463;&#25506;&#32034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#32500;&#24230;&#30456;&#20851;&#24615;&#30340;&#27010;&#24565;&#26469;&#20851;&#27880;&#32500;&#24230;&#36136;&#37327;&#65292;&#24182;&#35780;&#20272;&#20854;&#23545;&#25628;&#32034;&#20013;&#28548;&#28165;&#30340;&#25972;&#20307;&#23454;&#29992;&#24615;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#30340;&#35780;&#20272;&#31243;&#24207;&#26410;&#32771;&#34385;&#32500;&#24230;&#30456;&#20851;&#24615;&#65292;&#36825;&#19968;&#28857;&#21487;&#20197;&#20174;&#30456;&#20851;&#24615;&#19982;&#33258;&#21160;&#24230;&#37327;&#26631;&#20934;&#30340;&#36739;&#24046;&#30456;&#20851;&#24615;&#30475;&#20986;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#30456;&#20851;&#24615;&#20998;&#31867;&#22120;&#65292;&#24182;&#35780;&#20272;&#20102;&#19968;&#20010;&#20851;&#20110;&#28548;&#28165;&#30340;&#24050;&#30830;&#31435;&#25968;&#25454;&#38598;&#20013;&#19981;&#19968;&#33268;&#32500;&#24230;&#30340;&#26222;&#36941;&#24615;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#21487;&#20197;&#20026;&#26410;&#26469;&#22312;&#36825;&#20010;&#20027;&#39064;&#19978;&#30340;&#24037;&#20316;&#25552;&#20379;&#21160;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clarifying user's information needs is an essential component of modern search systems. While most of the approaches for constructing clarifying prompts rely on query facets, the impact of the quality of the facets is relatively unexplored. In this work, we concentrate on facet quality through the notion of facet coherency and assess its importance for overall usefulness for clarification in search. We find that existing evaluation procedures do not account for facet coherency, as evident by the poor correlation of coherency with automated metrics. Moreover, we propose a coherency classifier and assess the prevalence of incoherent facets in a well-established dataset on clarification. Our findings can serve as motivation for future work on the topic.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#25193;&#23637;&#30340;&#29983;&#25104;&#22686;&#24378;&#26816;&#32034;&#65288;GAR&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#20195;&#30721;&#36827;&#34892;&#37325;&#20889;&#26469;&#35299;&#20915;&#20195;&#30721;&#25628;&#32034;&#20013;&#23384;&#22312;&#30340;&#39118;&#26684;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#26816;&#32034;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.04514</link><description>&lt;p&gt;
&#37325;&#20889;&#20195;&#30721;&#65306;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#20195;&#30721;&#25628;&#32034;&#30340;&#31616;&#21333;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Rewriting the Code: A Simple Method for Large Language Model Augmented Code Search. (arXiv:2401.04514v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04514
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#25193;&#23637;&#30340;&#29983;&#25104;&#22686;&#24378;&#26816;&#32034;&#65288;GAR&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#20195;&#30721;&#36827;&#34892;&#37325;&#20889;&#26469;&#35299;&#20915;&#20195;&#30721;&#25628;&#32034;&#20013;&#23384;&#22312;&#30340;&#39118;&#26684;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#26816;&#32034;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20195;&#30721;&#25628;&#32034;&#20013;&#65292;&#29983;&#25104;&#22686;&#24378;&#26816;&#32034;&#65288;GAR&#65289;&#26694;&#26550;&#26159;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#31574;&#30053;&#65292;&#36890;&#36807;&#29983;&#25104;&#31034;&#20363;&#20195;&#30721;&#29255;&#27573;&#26469;&#22686;&#24378;&#26597;&#35810;&#65292;&#20197;&#35299;&#20915;&#20195;&#30721;&#29255;&#27573;&#21644;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#20043;&#38388;&#30340;&#20027;&#35201;&#27169;&#24577;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#23588;&#20854;&#26159;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23637;&#31034;&#20102;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#30340;&#24773;&#20917;&#19979;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#21021;&#27493;&#35843;&#26597;&#21457;&#29616;&#65292;LLM&#22686;&#24378;&#26694;&#26550;&#25152;&#25552;&#20379;&#30340;&#25913;&#36827;&#26377;&#19968;&#23450;&#30340;&#38480;&#21046;&#12290;&#36825;&#31181;&#38480;&#21046;&#21487;&#33021;&#26159;&#22240;&#20026;&#29983;&#25104;&#30340;&#20195;&#30721;&#65292;&#23613;&#31649;&#22312;&#21151;&#33021;&#19978;&#20934;&#30830;&#65292;&#20294;&#22312;&#20195;&#30721;&#24211;&#20013;&#19982;&#22522;&#20934;&#20195;&#30721;&#20043;&#38388;&#32463;&#24120;&#26174;&#31034;&#20986;&#26126;&#26174;&#30340;&#39118;&#26684;&#20559;&#24046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#22522;&#30784;GAR&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#20195;&#30721;&#24211;&#20013;&#30340;&#20195;&#30721;&#36827;&#34892;&#37325;&#20889;&#65288;ReCo&#65289;&#26469;&#36827;&#34892;&#39118;&#26684;&#35268;&#33539;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ReCo&#26174;&#33879;&#25552;&#39640;&#20102;&#26816;&#32034;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In code search, the Generation-Augmented Retrieval (GAR) framework, which generates exemplar code snippets to augment queries, has emerged as a promising strategy to address the principal challenge of modality misalignment between code snippets and natural language queries, particularly with the demonstrated code generation capabilities of Large Language Models (LLMs). Nevertheless, our preliminary investigations indicate that the improvements conferred by such an LLM-augmented framework are somewhat constrained. This limitation could potentially be ascribed to the fact that the generated codes, albeit functionally accurate, frequently display a pronounced stylistic deviation from the ground truth code in the codebase. In this paper, we extend the foundational GAR framework and propose a simple yet effective method that additionally Rewrites the Code (ReCo) within the codebase for style normalization. Experimental results demonstrate that ReCo significantly boosts retrieval accuracy ac
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#22522;&#20110;&#23884;&#20837;&#21644;&#22522;&#20110;&#35821;&#20041;&#30340;&#27169;&#22411;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#26469;&#29983;&#25104;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#20107;&#21518;&#35299;&#37322;&#65292;&#24182;&#21033;&#29992;&#22522;&#20110;&#26412;&#20307;&#30340;&#30693;&#35782;&#22270;&#35889;&#26469;&#25552;&#39640;&#21487;&#35299;&#37322;&#24615;&#12290;&#36825;&#26679;&#30340;&#26694;&#26550;&#26088;&#22312;&#20135;&#29983;&#26377;&#24847;&#20041;&#19988;&#26131;&#20110;&#29702;&#35299;&#30340;&#35299;&#37322;&#65292;&#22686;&#24378;&#29992;&#25143;&#30340;&#20449;&#20219;&#21644;&#28385;&#24847;&#24230;&#12290;</title><link>http://arxiv.org/abs/2401.04474</link><description>&lt;p&gt;
&#32467;&#21512;&#22522;&#20110;&#23884;&#20837;&#21644;&#22522;&#20110;&#35821;&#20041;&#30340;&#27169;&#22411;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#20107;&#21518;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Combining Embedding-Based and Semantic-Based Models for Post-hoc Explanations in Recommender Systems. (arXiv:2401.04474v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04474
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#22522;&#20110;&#23884;&#20837;&#21644;&#22522;&#20110;&#35821;&#20041;&#30340;&#27169;&#22411;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#26469;&#29983;&#25104;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#20107;&#21518;&#35299;&#37322;&#65292;&#24182;&#21033;&#29992;&#22522;&#20110;&#26412;&#20307;&#30340;&#30693;&#35782;&#22270;&#35889;&#26469;&#25552;&#39640;&#21487;&#35299;&#37322;&#24615;&#12290;&#36825;&#26679;&#30340;&#26694;&#26550;&#26088;&#22312;&#20135;&#29983;&#26377;&#24847;&#20041;&#19988;&#26131;&#20110;&#29702;&#35299;&#30340;&#35299;&#37322;&#65292;&#22686;&#24378;&#29992;&#25143;&#30340;&#20449;&#20219;&#21644;&#28385;&#24847;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#20170;&#25968;&#25454;&#20016;&#23500;&#30340;&#29615;&#22659;&#20013;&#65292;&#25512;&#33616;&#31995;&#32479;&#22312;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#20013;&#25198;&#28436;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#35282;&#33394;&#12290;&#23427;&#20204;&#20026;&#29992;&#25143;&#25552;&#20379;&#20010;&#24615;&#21270;&#25512;&#33616;&#21644;&#23545;&#36825;&#20123;&#25512;&#33616;&#30340;&#35299;&#37322;&#12290;&#23884;&#20837;&#24335;&#27169;&#22411;&#23613;&#31649;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#24448;&#24448;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#65292;&#36825;&#21487;&#33021;&#25439;&#23475;&#20449;&#20219;&#21644;&#29992;&#25143;&#21442;&#19982;&#24230;&#12290;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#23884;&#20837;&#24335;&#21644;&#22522;&#20110;&#35821;&#20041;&#30340;&#27169;&#22411;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#29983;&#25104;&#20107;&#21518;&#35299;&#37322;&#65292;&#21033;&#29992;&#22522;&#20110;&#26412;&#20307;&#30340;&#30693;&#35782;&#22270;&#35889;&#26469;&#25552;&#39640;&#21487;&#35299;&#37322;&#24615;&#12290;&#36890;&#36807;&#22312;&#19968;&#20010;&#32467;&#26500;&#21270;&#26694;&#26550;&#20869;&#32452;&#32455;&#25968;&#25454;&#65292;&#26412;&#20307;&#20801;&#35768;&#23545;&#23454;&#20307;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#36827;&#34892;&#24314;&#27169;&#65292;&#36825;&#23545;&#20110;&#29983;&#25104;&#35299;&#37322;&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#12290;&#36890;&#36807;&#32467;&#21512;&#22522;&#20110;&#23884;&#20837;&#21644;&#22522;&#20110;&#35821;&#20041;&#30340;&#27169;&#22411;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#20107;&#21518;&#35299;&#37322;&#65292;&#25105;&#20204;&#23450;&#20041;&#30340;&#26694;&#26550;&#26088;&#22312;&#20135;&#29983;&#26377;&#24847;&#20041;&#19988;&#26131;&#20110;&#29702;&#35299;&#30340;&#35299;&#37322;&#65292;&#22686;&#24378;&#29992;&#25143;&#30340;&#20449;&#20219;&#21644;&#28385;&#24847;&#24230;&#65292;&#24182;&#28508;&#22312;&#22320;&#22686;&#21152;&#25512;&#33616;&#30340;&#21487;&#38752;&#24615;&#21644;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In today's data-rich environment, recommender systems play a crucial role in decision support systems. They provide to users personalized recommendations and explanations about these recommendations. Embedding-based models, despite their widespread use, often suffer from a lack of interpretability, which can undermine trust and user engagement. This paper presents an approach that combines embedding-based and semantic-based models to generate post-hoc explanations in recommender systems, leveraging ontology-based knowledge graphs to improve interpretability and explainability. By organizing data within a structured framework, ontologies enable the modeling of intricate relationships between entities, which is essential for generating explanations. By combining embedding-based and semantic based models for post-hoc explanations in recommender systems, the framework we defined aims at producing meaningful and easy-to-understand explanations, enhancing user trust and satisfaction, and pot
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21327;&#21516;&#28151;&#28102;&#30340;&#38544;&#31169;&#20445;&#25252;&#39034;&#24207;&#25512;&#33616;&#26041;&#27861;&#65292;&#36890;&#36807;&#27880;&#20837;&#19981;&#21487;&#21306;&#20998;&#30340;&#39033;&#30446;&#26469;&#22686;&#21152;&#30446;&#26631;&#24207;&#21015;&#30340;&#22256;&#24785;&#24230;&#65292;&#20174;&#32780;&#25552;&#39640;&#38544;&#31169;&#20445;&#25252;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.04423</link><description>&lt;p&gt;
&#22522;&#20110;&#21327;&#21516;&#28151;&#28102;&#30340;&#38544;&#31169;&#20445;&#25252;&#39034;&#24207;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Privacy-Preserving Sequential Recommendation with Collaborative Confusion. (arXiv:2401.04423v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04423
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21327;&#21516;&#28151;&#28102;&#30340;&#38544;&#31169;&#20445;&#25252;&#39034;&#24207;&#25512;&#33616;&#26041;&#27861;&#65292;&#36890;&#36807;&#27880;&#20837;&#19981;&#21487;&#21306;&#20998;&#30340;&#39033;&#30446;&#26469;&#22686;&#21152;&#30446;&#26631;&#24207;&#21015;&#30340;&#22256;&#24785;&#24230;&#65292;&#20174;&#32780;&#25552;&#39640;&#38544;&#31169;&#20445;&#25252;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#31169;&#20445;&#25252;&#30340;&#39034;&#24207;&#25512;&#33616;&#21463;&#21040;&#23398;&#26415;&#30028;&#21644;&#34892;&#19994;&#30340;&#24191;&#27867;&#20851;&#27880;&#65292;&#28982;&#32780;&#23545;&#20110;&#25910;&#38598;&#21644;&#20256;&#36755;&#29992;&#25143;&#20010;&#20154;&#20132;&#20114;&#25968;&#25454;&#25152;&#24102;&#26469;&#30340;&#38544;&#31169;&#39118;&#38505;&#24448;&#24448;&#34987;&#20302;&#20272;&#25110;&#24573;&#35270;&#12290;&#29616;&#26377;&#30340;&#38544;&#31169;&#20445;&#25252;&#30740;&#31350;&#20027;&#35201;&#24212;&#29992;&#20110;&#20256;&#32479;&#30340;&#21327;&#21516;&#36807;&#28388;&#25110;&#30697;&#38453;&#20998;&#35299;&#65292;&#32780;&#38750;&#39034;&#24207;&#25512;&#33616;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#30740;&#31350;&#22823;&#22810;&#22522;&#20110;&#24046;&#20998;&#38544;&#31169;&#25110;&#32852;&#37030;&#23398;&#20064;&#65292;&#24448;&#24448;&#23548;&#33268;&#24615;&#33021;&#26126;&#26174;&#19979;&#38477;&#65292;&#25110;&#23545;&#36890;&#20449;&#26377;&#24456;&#39640;&#30340;&#35201;&#27714;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20174;&#19981;&#21516;&#30340;&#35282;&#24230;&#35299;&#20915;&#38544;&#31169;&#20445;&#25252;&#38382;&#39064;&#12290;&#19982;&#29616;&#26377;&#30740;&#31350;&#19981;&#21516;&#65292;&#25105;&#20204;&#25429;&#25417;&#37051;&#23621;&#20132;&#20114;&#24207;&#21015;&#30340;&#21327;&#21516;&#20449;&#21495;&#65292;&#24182;&#22312;&#25512;&#33616;&#36807;&#31243;&#24320;&#22987;&#21069;&#30452;&#25509;&#23558;&#19981;&#21487;&#21306;&#20998;&#30340;&#39033;&#30446;&#27880;&#20837;&#30446;&#26631;&#24207;&#21015;&#65292;&#20174;&#32780;&#22686;&#21152;&#30446;&#26631;&#24207;&#21015;&#30340;&#22256;&#24785;&#24230;&#12290;&#21363;&#20351;&#25915;&#20987;&#32773;&#33719;&#24471;&#30446;&#26631;&#20132;&#20114;&#24207;&#21015;&#65292;&#20063;&#24456;&#38590;&#21306;&#20998;&#30495;&#23454;&#25968;&#25454;&#21644;&#27880;&#20837;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequential recommendation has attracted a lot of attention from both academia and industry, however the privacy risks associated to gathering and transferring users' personal interaction data are often underestimated or ignored. Existing privacy-preserving studies are mainly applied to traditional collaborative filtering or matrix factorization rather than sequential recommendation. Moreover, these studies are mostly based on differential privacy or federated learning, which often leads to significant performance degradation, or has high requirements for communication. In this work, we address privacy-preserving from a different perspective. Unlike existing research, we capture collaborative signals of neighbor interaction sequences and directly inject indistinguishable items into the target sequence before the recommendation process begins, thereby increasing the perplexity of the target sequence. Even if the target interaction sequence is obtained by attackers, it is difficult to dis
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32454;&#31890;&#24230;&#23884;&#20837;&#32500;&#24230;&#20248;&#21270;&#26041;&#27861;&#65288;FIITED&#65289;&#65292;&#33021;&#22815;&#22312;&#25512;&#33616;&#31995;&#32479;&#30340;&#35757;&#32451;&#36807;&#31243;&#20013;&#26681;&#25454;&#23884;&#20837;&#21521;&#37327;&#30340;&#37325;&#35201;&#24615;&#19981;&#26029;&#35843;&#25972;&#20854;&#32500;&#24230;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#34394;&#25311;&#21704;&#24076;&#32034;&#24341;&#21704;&#24076;&#34920;&#30340;&#23884;&#20837;&#23384;&#20648;&#31995;&#32479;&#20197;&#26377;&#25928;&#33410;&#30465;&#20869;&#23384;&#12290;</title><link>http://arxiv.org/abs/2401.04408</link><description>&lt;p&gt;
&#22312;&#25512;&#33616;&#31995;&#32479;&#30340;&#35757;&#32451;&#36807;&#31243;&#20013;&#20248;&#21270;&#32454;&#31890;&#24230;&#23884;&#20837;&#32500;&#24230;
&lt;/p&gt;
&lt;p&gt;
Fine-Grained Embedding Dimension Optimization During Training for Recommender Systems. (arXiv:2401.04408v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04408
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32454;&#31890;&#24230;&#23884;&#20837;&#32500;&#24230;&#20248;&#21270;&#26041;&#27861;&#65288;FIITED&#65289;&#65292;&#33021;&#22815;&#22312;&#25512;&#33616;&#31995;&#32479;&#30340;&#35757;&#32451;&#36807;&#31243;&#20013;&#26681;&#25454;&#23884;&#20837;&#21521;&#37327;&#30340;&#37325;&#35201;&#24615;&#19981;&#26029;&#35843;&#25972;&#20854;&#32500;&#24230;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#34394;&#25311;&#21704;&#24076;&#32034;&#24341;&#21704;&#24076;&#34920;&#30340;&#23884;&#20837;&#23384;&#20648;&#31995;&#32479;&#20197;&#26377;&#25928;&#33410;&#30465;&#20869;&#23384;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#25512;&#33616;&#27169;&#22411;&#20013;&#30340;&#22823;&#22411;&#23884;&#20837;&#34920;&#22312;&#35757;&#32451;&#21644;&#25512;&#26029;&#36807;&#31243;&#20013;&#38656;&#35201;&#36807;&#22823;&#30340;&#20869;&#23384;&#12290;&#20026;&#20102;&#20943;&#23567;&#35757;&#32451;&#26102;&#30340;&#20869;&#23384;&#21344;&#29992;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32454;&#31890;&#24230;&#23884;&#20837;&#32500;&#24230;&#20248;&#21270;&#26041;&#27861; (FIITED)&#12290;&#26681;&#25454;&#23884;&#20837;&#21521;&#37327;&#30340;&#37325;&#35201;&#24615;&#19981;&#21516;&#65292;FIITED&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36830;&#32493;&#35843;&#25972;&#27599;&#20010;&#23884;&#20837;&#21521;&#37327;&#30340;&#32500;&#24230;&#65292;&#23558;&#26356;&#37325;&#35201;&#30340;&#23884;&#20837;&#21521;&#37327;&#20998;&#37197;&#26356;&#38271;&#30340;&#32500;&#24230;&#65292;&#24182;&#33021;&#22815;&#36866;&#24212;&#25968;&#25454;&#30340;&#21160;&#24577;&#21464;&#21270;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#34394;&#25311;&#21704;&#24076;&#30340;&#29289;&#29702;&#32034;&#24341;&#21704;&#24076;&#34920;&#30340;&#23884;&#20837;&#23384;&#20648;&#31995;&#32479;&#65292;&#20197;&#23454;&#29616;&#23884;&#20837;&#32500;&#24230;&#30340;&#35843;&#25972;&#24182;&#26377;&#25928;&#22320;&#33410;&#30465;&#20869;&#23384;&#12290;&#23545;&#20004;&#20010;&#34892;&#19994;&#27169;&#22411;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;FIITED&#33021;&#22815;&#23558;&#23884;&#20837;&#30340;&#22823;&#23567;&#20943;&#23567;&#36229;&#36807;65%&#65292;&#21516;&#26102;&#20445;&#25345;&#35757;&#32451;&#27169;&#22411;&#30340;&#36136;&#37327;&#65292;&#27604;&#29616;&#26377;&#30340;&#19968;&#31181;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36827;&#34892;&#23884;&#20837;&#20462;&#21098;&#30340;&#26041;&#27861;&#33410;&#30465;&#26356;&#22810;&#20869;&#23384;&#12290;
&lt;/p&gt;
&lt;p&gt;
Huge embedding tables in modern Deep Learning Recommender Models (DLRM) require prohibitively large memory during training and inference. Aiming to reduce the memory footprint of training, this paper proposes FIne-grained In-Training Embedding Dimension optimization (FIITED). Given the observation that embedding vectors are not equally important, FIITED adjusts the dimension of each individual embedding vector continuously during training, assigning longer dimensions to more important embeddings while adapting to dynamic changes in data. A novel embedding storage system based on virtually-hashed physically-indexed hash tables is designed to efficiently implement the embedding dimension adjustment and effectively enable memory saving. Experiments on two industry models show that FIITED is able to reduce the size of embeddings by more than 65% while maintaining the trained model's quality, saving significantly more memory than a state-of-the-art in-training embedding pruning method. On p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#22823;&#35268;&#27169;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;GPU&#38598;&#32676;&#20998;&#24067;&#24335;&#20803;&#23398;&#20064;&#30340;&#39640;&#24615;&#33021;&#26694;&#26550;G-Meta&#65292;&#36890;&#36807;&#21033;&#29992;&#25968;&#25454;&#24182;&#34892;&#24615;&#21644;&#27169;&#22411;&#24182;&#34892;&#24615;&#20197;&#21450;&#35774;&#35745;&#39640;&#25928;&#30340;&#20803;-IO&#27969;&#27700;&#32447;&#65292;&#23454;&#29616;&#20102;&#39640;&#36895;&#20998;&#24067;&#24335;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2401.04338</link><description>&lt;p&gt;
G-Meta: &#22823;&#35268;&#27169;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;GPU&#38598;&#32676;&#20998;&#24067;&#24335;&#20803;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
G-Meta: Distributed Meta Learning in GPU Clusters for Large-Scale Recommender Systems. (arXiv:2401.04338v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04338
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#22823;&#35268;&#27169;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;GPU&#38598;&#32676;&#20998;&#24067;&#24335;&#20803;&#23398;&#20064;&#30340;&#39640;&#24615;&#33021;&#26694;&#26550;G-Meta&#65292;&#36890;&#36807;&#21033;&#29992;&#25968;&#25454;&#24182;&#34892;&#24615;&#21644;&#27169;&#22411;&#24182;&#34892;&#24615;&#20197;&#21450;&#35774;&#35745;&#39640;&#25928;&#30340;&#20803;-IO&#27969;&#27700;&#32447;&#65292;&#23454;&#29616;&#20102;&#39640;&#36895;&#20998;&#24067;&#24335;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#19968;&#20010;&#21517;&#20026;&#20803;&#23398;&#20064;&#30340;&#26032;&#33539;&#24335;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#25512;&#33616;&#27169;&#22411;(DLRM)&#65292;&#24182;&#22312;&#32479;&#35745;&#24615;&#33021;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#29305;&#21035;&#26159;&#22312;&#20919;&#21551;&#21160;&#22330;&#26223;&#20013;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#31995;&#32479;&#24182;&#27809;&#26377;&#20026;&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;DLRM&#27169;&#22411;&#37327;&#36523;&#23450;&#21046;&#65292;&#24182;&#19988;&#22312;GPU&#38598;&#32676;&#30340;&#20998;&#24067;&#24335;&#35757;&#32451;&#20013;&#23384;&#22312;&#20851;&#20110;&#25928;&#29575;&#30340;&#37325;&#35201;&#38382;&#39064;&#12290;&#36825;&#26159;&#22240;&#20026;&#20256;&#32479;&#30340;&#28145;&#24230;&#23398;&#20064;&#27969;&#27700;&#32447;&#23545;&#20110;&#20803;&#23398;&#20064;&#20013;&#30340;&#20004;&#20010;&#20219;&#21153;&#29305;&#23450;&#25968;&#25454;&#38598;&#21644;&#20004;&#20010;&#26356;&#26032;&#24490;&#29615;&#24182;&#27809;&#26377;&#36827;&#34892;&#20248;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#24615;&#33021;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;GPU&#38598;&#32676;&#19978;&#36827;&#34892;&#22522;&#20110;&#20248;&#21270;&#30340;&#20803;DLRM&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#35757;&#32451;&#65292;&#21363;G-Meta&#12290;&#39318;&#20808;&#65292;G-Meta&#21033;&#29992;&#25968;&#25454;&#24182;&#34892;&#24615;&#21644;&#27169;&#22411;&#24182;&#34892;&#24615;&#65292;&#24182;&#23545;&#35745;&#31639;&#21644;&#36890;&#20449;&#25928;&#29575;&#36827;&#34892;&#31934;&#24515;&#21327;&#35843;&#65292;&#23454;&#29616;&#39640;&#36895;&#20998;&#24067;&#24335;&#35757;&#32451;&#12290;&#20854;&#27425;&#65292;&#23427;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#39640;&#25928;&#25968;&#25454;&#25668;&#20837;&#30340;&#20803;-IO&#27969;&#27700;&#32447;&#65292;&#20197;&#32531;&#35299;&#36755;&#20837;/&#36755;&#20986;&#29942;&#39048;&#12290;&#36827;&#34892;&#20102;&#21508;&#31181;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
Recently, a new paradigm, meta learning, has been widely applied to Deep Learning Recommendation Models (DLRM) and significantly improves statistical performance, especially in cold-start scenarios. However, the existing systems are not tailored for meta learning based DLRM models and have critical problems regarding efficiency in distributed training in the GPU cluster. It is because the conventional deep learning pipeline is not optimized for two task-specific datasets and two update loops in meta learning. This paper provides a high-performance framework for large-scale training for Optimization-based Meta DLRM models over the \textbf{G}PU cluster, namely \textbf{G}-Meta. Firstly, G-Meta utilizes both data parallelism and model parallelism with careful orchestration regarding computation and communication efficiency, to enable high-speed distributed training. Secondly, it proposes a Meta-IO pipeline for efficient data ingestion to alleviate the I/O bottleneck. Various experimental r
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#32769;&#40836;&#30456;&#20851;&#22522;&#22240;&#30740;&#31350;&#20013;&#19981;&#21516;&#20986;&#29256;&#31867;&#22411;&#30340;&#29305;&#24449;&#24046;&#24322;&#65292;&#21457;&#29616;&#36825;&#20123;&#20986;&#29256;&#31867;&#22411;&#22312;&#23545;&#32769;&#40836;&#21270;&#30740;&#31350;&#30340;&#20851;&#27880;&#24230;&#12289;&#30740;&#31350;&#22522;&#22240;&#30340;&#33539;&#22260;&#21644;&#20027;&#39064;&#20559;&#22909;&#19978;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#23613;&#31649;&#23384;&#22312;&#24046;&#24322;&#65292;&#19968;&#20123;&#39030;&#32423;&#22522;&#22240;&#22914;&#33008;&#23707;&#32032;&#26222;&#36941;&#21463;&#21040;&#37325;&#35270;&#65292;&#32780;&#19988;&#20986;&#29256;&#31867;&#22411;&#22312;&#30740;&#31350;&#22522;&#22240;&#26041;&#38754;&#20063;&#21576;&#29616;&#30456;&#20284;&#30340;&#19981;&#24179;&#34913;&#27700;&#24179;&#12290;&#21478;&#22806;&#65292;&#20986;&#29256;&#31867;&#22411;&#36824;&#22312;&#20316;&#32773;&#25968;&#37327;&#12289;&#24341;&#29992;&#25968;&#37327;&#31561;&#26041;&#38754;&#23384;&#22312;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2401.04323</link><description>&lt;p&gt;
&#20986;&#29256;&#31867;&#22411;&#23545;&#29983;&#29289;&#21307;&#23398;&#30740;&#31350;&#30340;&#24046;&#24322;&#29305;&#24449;&#65306;&#20197;&#32769;&#40836;&#30456;&#20851;&#30740;&#31350;&#20026;&#20363;&#30340;&#23450;&#37327;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Divergent Characteristics of Biomedical Research across Publication Types: A Quantitative Analysis on the Aging-related Research. (arXiv:2401.04323v1 [cs.DL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04323
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#32769;&#40836;&#30456;&#20851;&#22522;&#22240;&#30740;&#31350;&#20013;&#19981;&#21516;&#20986;&#29256;&#31867;&#22411;&#30340;&#29305;&#24449;&#24046;&#24322;&#65292;&#21457;&#29616;&#36825;&#20123;&#20986;&#29256;&#31867;&#22411;&#22312;&#23545;&#32769;&#40836;&#21270;&#30740;&#31350;&#30340;&#20851;&#27880;&#24230;&#12289;&#30740;&#31350;&#22522;&#22240;&#30340;&#33539;&#22260;&#21644;&#20027;&#39064;&#20559;&#22909;&#19978;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#23613;&#31649;&#23384;&#22312;&#24046;&#24322;&#65292;&#19968;&#20123;&#39030;&#32423;&#22522;&#22240;&#22914;&#33008;&#23707;&#32032;&#26222;&#36941;&#21463;&#21040;&#37325;&#35270;&#65292;&#32780;&#19988;&#20986;&#29256;&#31867;&#22411;&#22312;&#30740;&#31350;&#22522;&#22240;&#26041;&#38754;&#20063;&#21576;&#29616;&#30456;&#20284;&#30340;&#19981;&#24179;&#34913;&#27700;&#24179;&#12290;&#21478;&#22806;&#65292;&#20986;&#29256;&#31867;&#22411;&#36824;&#22312;&#20316;&#32773;&#25968;&#37327;&#12289;&#24341;&#29992;&#25968;&#37327;&#31561;&#26041;&#38754;&#23384;&#22312;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32769;&#40836;&#30456;&#20851;&#22522;&#22240;&#30740;&#31350;&#20013;&#19981;&#21516;&#20986;&#29256;&#31867;&#22411;&#30340;&#29305;&#24449;&#24046;&#24322;&#12290;&#25105;&#20204;&#21033;&#29992;&#26469;&#33258;PubMed&#31561;&#26435;&#23041;&#25968;&#25454;&#24211;&#30340;&#20116;&#20010;&#27169;&#22411;&#29289;&#31181;&#30340;&#25991;&#29486;&#35745;&#37327;&#25968;&#25454;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;&#26681;&#25454;PubMed&#30340;&#20998;&#31867;&#65292;&#23558;&#25991;&#31456;&#20998;&#20026;&#19981;&#21516;&#30340;&#31867;&#22411;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#19981;&#21516;&#20986;&#29256;&#31867;&#22411;&#22312;&#23545;&#32769;&#40836;&#30456;&#20851;&#30740;&#31350;&#30340;&#20851;&#27880;&#24230;&#12289;&#30740;&#31350;&#22522;&#22240;&#30340;&#33539;&#22260;&#21644;&#20027;&#39064;&#20559;&#22909;&#19978;&#23384;&#22312;&#26174;&#33879;&#30340;&#24046;&#24322;&#12290;&#20363;&#22914;&#65292;&#27604;&#36739;&#30740;&#31350;&#21644;&#20803;&#20998;&#26512;&#27604;&#39564;&#35777;&#30740;&#31350;&#26356;&#27880;&#37325;&#32769;&#40836;&#21270;&#38382;&#39064;&#12290;&#32508;&#36848;&#26356;&#20851;&#27880;&#32454;&#32990;&#29983;&#29289;&#23398;&#65292;&#32780;&#20020;&#24202;&#30740;&#31350;&#21017;&#24378;&#35843;&#36716;&#21270;&#24615;&#20027;&#39064;&#12290;&#20986;&#29256;&#31867;&#22411;&#22312;&#39640;&#24230;&#30740;&#31350;&#30340;&#22522;&#22240;&#19978;&#20063;&#34920;&#29616;&#20986;&#24046;&#24322;&#65292;&#20363;&#22914;&#32508;&#36848;&#26356;&#20851;&#27880;APOE&#22522;&#22240;&#65292;&#32780;&#20020;&#24202;&#30740;&#31350;&#26356;&#20851;&#27880;GH1&#22522;&#22240;&#12290;&#23613;&#31649;&#23384;&#22312;&#24046;&#24322;&#65292;&#19968;&#20123;&#39030;&#32423;&#22522;&#22240;&#22914;&#33008;&#23707;&#32032;&#26222;&#36941;&#21463;&#21040;&#37325;&#35270;&#12290;&#20986;&#29256;&#31867;&#22411;&#22312;&#30740;&#31350;&#22522;&#22240;&#26041;&#38754;&#20063;&#20307;&#29616;&#20986;&#30456;&#20284;&#30340;&#19981;&#24179;&#34913;&#27700;&#24179;&#12290;&#22312;&#20316;&#32773;&#25968;&#37327;&#12289;&#24341;&#29992;&#25968;&#37327;&#31561;&#25991;&#29486;&#35745;&#37327;&#26041;&#38754;&#20063;&#23384;&#22312;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates differences in characteristics across publication types for aging-related genetic research. We utilized bibliometric data for five model species retrieved from authoritative databases including PubMed. Publications are classified into types according to PubMed. Results indicate substantial divergence across publication types in attention paid to aging-related research, scopes of studied genes, and topical preferences. For instance, comparative studies and meta-analyses show a greater focus on aging than validation studies. Reviews concentrate more on cell biology while clinical studies emphasize translational topics. Publication types also manifest variations in highly studied genes, like APOE for reviews versus GH1 for clinical studies. Despite differences, top genes like insulin are universally emphasized. Publication types demonstrate similar levels of imbalance in research efforts to genes. Differences also exist in bibliometrics like authorship numbers, cit
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#22810;&#20852;&#36259;&#23398;&#20064;&#26041;&#27861;&#65288;PoMRec&#65289;&#65292;&#29992;&#20110;&#39034;&#24207;&#25512;&#33616;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#20855;&#20307;&#30340;&#25552;&#31034;&#26469;&#25351;&#23548;&#20852;&#36259;&#25552;&#21462;&#21644;&#26435;&#37325;&#39044;&#27979;&#27169;&#22359;&#30340;&#23398;&#20064;&#65292;&#20197;&#20811;&#26381;&#29616;&#26377;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2401.04312</link><description>&lt;p&gt;
&#22522;&#20110;&#25552;&#31034;&#30340;&#22810;&#20852;&#36259;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#39034;&#24207;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Prompt-based Multi-interest Learning Method for Sequential Recommendation. (arXiv:2401.04312v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04312
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#22810;&#20852;&#36259;&#23398;&#20064;&#26041;&#27861;&#65288;PoMRec&#65289;&#65292;&#29992;&#20110;&#39034;&#24207;&#25512;&#33616;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#20855;&#20307;&#30340;&#25552;&#31034;&#26469;&#25351;&#23548;&#20852;&#36259;&#25552;&#21462;&#21644;&#26435;&#37325;&#39044;&#27979;&#27169;&#22359;&#30340;&#23398;&#20064;&#65292;&#20197;&#20811;&#26381;&#29616;&#26377;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39034;&#24207;&#25512;&#33616;&#30340;&#22810;&#20852;&#36259;&#23398;&#20064;&#26041;&#27861;&#26088;&#22312;&#26681;&#25454;&#29992;&#25143;&#30340;&#22810;&#26041;&#38754;&#20852;&#36259;&#39044;&#27979;&#19979;&#19968;&#20010;&#39033;&#30446;&#65292;&#32473;&#23450;&#29992;&#25143;&#30340;&#21382;&#21490;&#20114;&#21160;&#12290;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#30001;&#20004;&#20010;&#27169;&#22359;&#32452;&#25104;&#65306;&#22810;&#20852;&#36259;&#25552;&#21462;&#27169;&#22359;&#23398;&#20064;&#29992;&#25143;&#30340;&#22810;&#20852;&#36259;&#23884;&#20837;&#20197;&#25429;&#25417;&#29992;&#25143;&#22810;&#20852;&#36259;&#65292;&#22810;&#20852;&#36259;&#26435;&#37325;&#39044;&#27979;&#27169;&#22359;&#23398;&#20064;&#27599;&#20010;&#20852;&#36259;&#30340;&#26435;&#37325;&#65292;&#20197;&#32858;&#21512;&#23398;&#20064;&#21040;&#30340;&#22810;&#20852;&#36259;&#23884;&#20837;&#20197;&#24471;&#21040;&#29992;&#25143;&#23884;&#20837;&#65292;&#29992;&#20110;&#39044;&#27979;&#29992;&#25143;&#23545;&#39033;&#30446;&#30340;&#35780;&#20998;&#12290;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#24456;&#26377;&#25928;&#65292;&#20294;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#38480;&#21046;&#65306;1)&#23427;&#20204;&#30452;&#25509;&#23558;&#29992;&#25143;&#20114;&#21160;&#36755;&#20837;&#20004;&#20010;&#27169;&#22359;&#20013;&#65292;&#32780;&#24573;&#30053;&#20102;&#23427;&#20204;&#19981;&#21516;&#30340;&#23398;&#20064;&#30446;&#26631;&#65307;2)&#23427;&#20204;&#20165;&#32771;&#34385;&#29992;&#25143;&#20114;&#21160;&#30340;&#20013;&#24515;&#24615;&#26469;&#23398;&#20064;&#29992;&#25143;&#30340;&#22810;&#20852;&#36259;&#65292;&#24573;&#35270;&#20102;&#23427;&#20204;&#30340;&#31163;&#25955;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#22810;&#20852;&#36259;&#23398;&#20064;&#26041;&#27861;&#65288;PoMRec&#65289;&#65292;&#20854;&#20013;&#20855;&#20307;&#30340;&#25552;&#31034;&#34987;&#29992;&#26469;&#25351;&#23548;&#22810;&#20852;&#36259;&#23884;&#20837;&#27169;&#22359;&#21644;&#22810;&#20852;&#36259;&#26435;&#37325;&#39044;&#27979;&#27169;&#22359;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-interest learning method for sequential recommendation aims to predict the next item according to user multi-faceted interests given the user historical interactions. Existing methods mainly consist of two modules: the multi-interest extraction module that learns user multi-interest embeddings to capture the user multi-interests, and the multi-interest weight prediction module that learns the weight of each interest for aggregating the learned multi-interest embeddings to derive the user embedding, used for predicting the user rating to an item. Despite their effectiveness, existing methods have two key limitations: 1) they directly feed the user interactions into the two modules, while ignoring their different learning objectives, and 2) they merely consider the centrality of the user interactions to learn the user multi-interests, while overlooking their dispersion. To tackle these limitations, we propose a prompt-based multi-interest learning method (PoMRec), where specific pr
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#35299;&#20915;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#20919;&#21551;&#21160;&#21644;&#25968;&#25454;&#31232;&#30095;&#38382;&#39064;&#65292;&#20197;&#24212;&#23545;&#20449;&#24687;&#36229;&#36733;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2312.01840</link><description>&lt;p&gt;
&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#35299;&#20915;&#25512;&#33616;&#31995;&#32479;&#20013;&#20919;&#21551;&#21160;&#21644;&#25968;&#25454;&#31232;&#30095;&#38382;&#39064;&#30340;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
An AI-based solution for the cold start and data sparsity problems in the recommendation systems. (arXiv:2312.01840v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.01840
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#35299;&#20915;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#20919;&#21551;&#21160;&#21644;&#25968;&#25454;&#31232;&#30095;&#38382;&#39064;&#65292;&#20197;&#24212;&#23545;&#20449;&#24687;&#36229;&#36733;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20114;&#32852;&#32593;&#19978;&#21487;&#29992;&#30340;&#25968;&#25454;&#37327;&#20197;&#21450;&#21033;&#29992;&#20114;&#32852;&#32593;&#30340;&#29992;&#25143;&#25968;&#37327;&#20197;&#21069;&#25152;&#26410;&#26377;&#30340;&#36895;&#24230;&#22686;&#38271;&#12290;&#25968;&#23383;&#20449;&#24687;&#37327;&#21644;&#20114;&#32852;&#32593;&#29992;&#25143;&#25968;&#37327;&#30340;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#24102;&#26469;&#20102;&#20449;&#24687;&#36229;&#36733;&#30340;&#21487;&#33021;&#24615;&#65292;&#38459;&#30861;&#20102;&#23545;&#20114;&#32852;&#32593;&#19978;&#24863;&#20852;&#36259;&#20869;&#23481;&#30340;&#24555;&#36895;&#35775;&#38382;&#12290;&#20687;Google&#12289;DevilFinder&#21644;Altavista&#31561;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#20811;&#26381;&#20102;&#36825;&#19968;&#25361;&#25112;&#65292;&#20294;&#32570;&#20047;&#20449;&#24687;&#30340;&#20248;&#20808;&#32423;&#21644;&#20010;&#24615;&#21270;&#65288;&#21363;&#31995;&#32479;&#23558;&#21487;&#33719;&#21462;&#30340;&#26448;&#26009;&#26144;&#23556;&#21040;&#29992;&#25143;&#30340;&#20852;&#36259;&#21644;&#20559;&#22909;&#65289;&#22788;&#29702;&#12290;&#36825;&#23548;&#33268;&#20102;&#23545;&#25512;&#33616;&#31995;&#32479;&#30340;&#38656;&#27714;&#31354;&#21069;&#22686;&#21152;&#12290;&#25512;&#33616;&#31995;&#32479;&#26159;&#20449;&#24687;&#36807;&#28388;&#31995;&#32479;&#65292;&#23427;&#36890;&#36807;&#26681;&#25454;&#29992;&#25143;&#30340;&#20852;&#36259;&#12289;&#21916;&#22909;&#20197;&#21450;&#23545;&#25152;&#38656;&#39033;&#30446;&#30340;&#35780;&#32423;&#65292;&#20174;&#22823;&#37327;&#21160;&#24577;&#29983;&#25104;&#30340;&#25968;&#25454;&#20013;&#36807;&#28388;&#20986;&#37325;&#35201;&#30340;&#20449;&#24687;&#29255;&#27573;&#65292;&#20197;&#35299;&#20915;&#20449;&#24687;&#36229;&#36733;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, the amount of data available on the internet and the number of users who utilize the Internet have increased at an unparalleled pace. The exponential development in the quantity of digital information accessible and the number of Internet users has created the possibility for information overload, impeding fast access to items of interest on the Internet. Information retrieval systems like as Google, DevilFinder, and Altavista have partly overcome this challenge, but prioritizing and customization of information (where a system maps accessible material to a user's interests and preferences) were lacking. This has resulted in a higher-than-ever need for recommender systems. Recommender systems are information filtering systems that address the issue of information overload by filtering important information fragments from a huge volume of dynamically produced data based on the user's interests, favorite things, preferences and ratings on the desired item. Recommender sy
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#39033;&#37325;&#35201;&#20219;&#21153;&#65292;&#21363;&#20174;&#35270;&#39057;&#25991;&#26412;&#20013;&#25552;&#21462;&#23618;&#27425;&#21270;&#20851;&#38190;&#20449;&#24687;&#12290;&#30740;&#31350;&#32773;&#20204;&#36890;&#36807;&#25286;&#20998;&#20219;&#21153;&#20026;&#22235;&#20010;&#23376;&#20219;&#21153;&#65292;&#24182;&#20171;&#32461;&#20102;&#20004;&#31181;&#23454;&#29616;&#26041;&#26696;&#65292;&#21363;PipVKIE&#21644;UniVKIE&#12290;&#20004;&#31181;&#26041;&#26696;&#37117;&#21033;&#29992;&#20102;&#35270;&#35273;&#12289;&#25991;&#26412;&#21644;&#22352;&#26631;&#30340;&#22810;&#27169;&#24577;&#20449;&#24687;&#36827;&#34892;&#29305;&#24449;&#34920;&#31034;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20123;&#26041;&#26696;&#22312;&#24615;&#33021;&#21644;&#25512;&#29702;&#36895;&#24230;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2310.11650</link><description>&lt;p&gt;
VKIE:&#24212;&#29992;&#20851;&#38190;&#20449;&#24687;&#25552;&#21462;&#20110;&#35270;&#39057;&#25991;&#26412;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
VKIE: The Application of Key Information Extraction on Video Text. (arXiv:2310.11650v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11650
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#39033;&#37325;&#35201;&#20219;&#21153;&#65292;&#21363;&#20174;&#35270;&#39057;&#25991;&#26412;&#20013;&#25552;&#21462;&#23618;&#27425;&#21270;&#20851;&#38190;&#20449;&#24687;&#12290;&#30740;&#31350;&#32773;&#20204;&#36890;&#36807;&#25286;&#20998;&#20219;&#21153;&#20026;&#22235;&#20010;&#23376;&#20219;&#21153;&#65292;&#24182;&#20171;&#32461;&#20102;&#20004;&#31181;&#23454;&#29616;&#26041;&#26696;&#65292;&#21363;PipVKIE&#21644;UniVKIE&#12290;&#20004;&#31181;&#26041;&#26696;&#37117;&#21033;&#29992;&#20102;&#35270;&#35273;&#12289;&#25991;&#26412;&#21644;&#22352;&#26631;&#30340;&#22810;&#27169;&#24577;&#20449;&#24687;&#36827;&#34892;&#29305;&#24449;&#34920;&#31034;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20123;&#26041;&#26696;&#22312;&#24615;&#33021;&#21644;&#25512;&#29702;&#36895;&#24230;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#35270;&#39057;&#20013;&#25552;&#21462;&#32467;&#26500;&#21270;&#20449;&#24687;&#23545;&#20110;&#34892;&#19994;&#20013;&#35768;&#22810;&#19979;&#28216;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#23450;&#20041;&#20102;&#20174;&#35270;&#39057;&#25991;&#26412;&#20013;&#25552;&#21462;&#23618;&#27425;&#21270;&#20851;&#38190;&#20449;&#24687;&#30340;&#37325;&#35201;&#20219;&#21153;&#12290;&#20026;&#20102;&#23436;&#25104;&#36825;&#20010;&#20219;&#21153;&#65292;&#25105;&#20204;&#23558;&#20854;&#25286;&#20998;&#20026;&#22235;&#20010;&#23376;&#20219;&#21153;&#65292;&#24182;&#20171;&#32461;&#20102;&#20004;&#31181;&#23454;&#29616;&#26041;&#26696;&#65292;&#20998;&#21035;&#31216;&#20026;PipVKIE&#21644;UniVKIE&#12290;PipVKIE&#25353;&#29031;&#36830;&#32493;&#38454;&#27573;&#39034;&#24207;&#23436;&#25104;&#22235;&#20010;&#23376;&#20219;&#21153;&#65292;&#32780;UniVKIE&#36890;&#36807;&#23558;&#25152;&#26377;&#23376;&#20219;&#21153;&#32479;&#19968;&#21040;&#19968;&#20010;&#20027;&#24178;&#20013;&#36827;&#34892;&#25913;&#36827;&#12290;PipVKIE&#21644;UniVKIE&#37117;&#21033;&#29992;&#20102;&#26469;&#33258;&#35270;&#35273;&#12289;&#25991;&#26412;&#21644;&#22352;&#26631;&#30340;&#22810;&#27169;&#24577;&#20449;&#24687;&#36827;&#34892;&#29305;&#24449;&#34920;&#31034;&#12290;&#22312;&#19968;&#20010;&#26126;&#30830;&#23450;&#20041;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#21487;&#20197;&#21462;&#24471;&#26174;&#33879;&#30340;&#24615;&#33021;&#21644;&#39640;&#25928;&#30340;&#25512;&#29702;&#36895;&#24230;&#12290;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#23558;&#20844;&#24320;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
Extracting structured information from videos is critical for numerous downstream applications in the industry. In this paper, we define a significant task of extracting hierarchical key information from visual texts on videos. To fulfill this task, we decouples it into four subtasks and introduce two implementation solutions called PipVKIE and UniVKIE. PipVKIE sequentially completes the four subtasks in continuous stages, while UniVKIE is improved by unifying all the subtasks into one backbone. Both PipVKIE and UniVKIE leverage multimodal information from vision, text, and coordinates for feature representation. Extensive experiments on one well-defined dataset demonstrate that our solutions can achieve remarkable performance and efficient inference speed. The code and dataset will be publicly available.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedDCSR&#30340;&#32852;&#37030;&#36328;&#39046;&#22495;&#39034;&#24207;&#25512;&#33616;&#26694;&#26550;&#65292;&#36890;&#36807;&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;&#26469;&#22788;&#29702;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#24207;&#21015;&#29305;&#24449;&#24322;&#36136;&#24615;&#65292;&#24182;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;</title><link>http://arxiv.org/abs/2309.08420</link><description>&lt;p&gt;
FedDCSR: &#36890;&#36807;&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;&#23454;&#29616;&#32852;&#37030;&#36328;&#39046;&#22495;&#39034;&#24207;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
FedDCSR: Federated Cross-domain Sequential Recommendation via Disentangled Representation Learning. (arXiv:2309.08420v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08420
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedDCSR&#30340;&#32852;&#37030;&#36328;&#39046;&#22495;&#39034;&#24207;&#25512;&#33616;&#26694;&#26550;&#65292;&#36890;&#36807;&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;&#26469;&#22788;&#29702;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#24207;&#21015;&#29305;&#24449;&#24322;&#36136;&#24615;&#65292;&#24182;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#21033;&#29992;&#26469;&#33258;&#22810;&#20010;&#39046;&#22495;&#30340;&#29992;&#25143;&#24207;&#21015;&#25968;&#25454;&#30340;&#36328;&#39046;&#22495;&#39034;&#24207;&#25512;&#33616;(CSR)&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;CSR&#26041;&#27861;&#38656;&#35201;&#22312;&#39046;&#22495;&#20043;&#38388;&#20849;&#20139;&#21407;&#22987;&#29992;&#25143;&#25968;&#25454;&#65292;&#36825;&#36829;&#21453;&#20102;&#12298;&#36890;&#29992;&#25968;&#25454;&#20445;&#25252;&#26465;&#20363;&#12299;(GDPR)&#12290;&#22240;&#27492;&#65292;&#26377;&#24517;&#35201;&#23558;&#32852;&#37030;&#23398;&#20064;(FL)&#21644;CSR&#30456;&#32467;&#21512;&#65292;&#20805;&#20998;&#21033;&#29992;&#19981;&#21516;&#39046;&#22495;&#30340;&#30693;&#35782;&#65292;&#21516;&#26102;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#24207;&#21015;&#29305;&#24449;&#24322;&#36136;&#24615;&#23545;FL&#30340;&#25972;&#20307;&#24615;&#33021;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FedDCSR&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#36807;&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;&#30340;&#26032;&#22411;&#32852;&#37030;&#36328;&#39046;&#22495;&#39034;&#24207;&#25512;&#33616;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20026;&#20102;&#35299;&#20915;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#24207;&#21015;&#29305;&#24449;&#24322;&#36136;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#39046;&#22495;&#20869;-&#39046;&#22495;&#38388;&#24207;&#21015;&#34920;&#31034;&#35299;&#32544;(SRD)&#30340;&#26041;&#27861;&#65292;&#23558;&#29992;&#25143;&#24207;&#21015;&#29305;&#24449;&#35299;&#32544;&#25104;&#39046;&#22495;&#20849;&#20139;&#21644;&#39046;&#22495;&#19987;&#23646;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-domain Sequential Recommendation (CSR) which leverages user sequence data from multiple domains has received extensive attention in recent years. However, the existing CSR methods require sharing origin user data across domains, which violates the General Data Protection Regulation (GDPR). Thus, it is necessary to combine federated learning (FL) and CSR to fully utilize knowledge from different domains while preserving data privacy. Nonetheless, the sequence feature heterogeneity across different domains significantly impacts the overall performance of FL. In this paper, we propose FedDCSR, a novel federated cross-domain sequential recommendation framework via disentangled representation learning. Specifically, to address the sequence feature heterogeneity across domains, we introduce an approach called inter-intra domain sequence representation disentanglement (SRD) to disentangle the user sequence features into domain-shared and domain-exclusive features. In addition, we design
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#22823;&#35821;&#35328;&#27169;&#22411;&#21512;&#25104;&#26597;&#35810;&#30340;&#38544;&#31169;&#20445;&#25252;&#25512;&#33616;&#31995;&#32479;&#65292;&#21487;&#20197;&#23433;&#20840;&#26377;&#25928;&#22320;&#35757;&#32451;&#28145;&#24230;&#26816;&#32034;&#27169;&#22411;&#24182;&#25552;&#39640;&#26816;&#32034;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.05973</link><description>&lt;p&gt;
&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#22823;&#35821;&#35328;&#27169;&#22411;&#21512;&#25104;&#26597;&#35810;&#30340;&#38544;&#31169;&#20445;&#25252;&#25512;&#33616;&#31995;&#32479;.
&lt;/p&gt;
&lt;p&gt;
Privacy-Preserving Recommender Systems with Synthetic Query Generation using Differentially Private Large Language Models. (arXiv:2305.05973v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05973
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#22823;&#35821;&#35328;&#27169;&#22411;&#21512;&#25104;&#26597;&#35810;&#30340;&#38544;&#31169;&#20445;&#25252;&#25512;&#33616;&#31995;&#32479;&#65292;&#21487;&#20197;&#23433;&#20840;&#26377;&#25928;&#22320;&#35757;&#32451;&#28145;&#24230;&#26816;&#32034;&#27169;&#22411;&#24182;&#25552;&#39640;&#26816;&#32034;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24320;&#21457;&#38544;&#31169;&#20445;&#25252;&#30340;&#22823;&#35268;&#27169;&#25512;&#33616;&#31995;&#32479;&#65292;&#20811;&#26381;&#20102;&#22312;&#35757;&#32451;&#36825;&#20123;&#22797;&#26434;&#31995;&#32479;&#26102;&#30340;&#26576;&#20123;&#25361;&#25112;&#21644;&#38480;&#21046;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#29305;&#21035;&#36866;&#29992;&#20110;&#22522;&#20110;LLM&#30340;&#25512;&#33616;&#31995;&#32479;&#30340;&#26032;&#20852;&#39046;&#22495;&#65292;&#20294;&#20063;&#21487;&#20197;&#36731;&#26494;&#22320;&#29992;&#20110;&#22788;&#29702;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#34920;&#31034;&#30340;&#20219;&#20309;&#25512;&#33616;&#31995;&#32479;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#20351;&#29992;DP&#35757;&#32451;&#26041;&#27861;&#65292;&#23545;&#20844;&#24320;&#39044;&#35757;&#32451;&#30340;LLM&#22312;&#26597;&#35810;&#29983;&#25104;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#29983;&#25104;&#30340;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#31169;&#26377;&#21512;&#25104;&#26597;&#35810;&#65292;&#20195;&#34920;&#21407;&#22987;&#26597;&#35810;&#65292;&#21487;&#20197;&#22312;&#20219;&#20309;&#19979;&#28216;&#38750;&#31169;&#26377;&#25512;&#33616;&#35757;&#32451;&#36807;&#31243;&#20013;&#33258;&#30001;&#20849;&#20139;&#65292;&#32780;&#19981;&#20250;&#20135;&#29983;&#20219;&#20309;&#39069;&#22806;&#30340;&#38544;&#31169;&#25104;&#26412;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#23433;&#20840;&#35757;&#32451;&#26377;&#25928;&#30340;&#28145;&#24230;&#26816;&#32034;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#23427;&#20204;&#30340;&#26816;&#32034;&#36136;&#37327;&#26377;&#26174;&#30528;&#30340;&#25552;&#39640;&#65292;&#32780;&#19981;&#20250;&#25439;&#23475;&#26597;&#35810;&#32423;&#21035;&#30340;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel approach for developing privacy-preserving large-scale recommender systems using differentially private (DP) large language models (LLMs) which overcomes certain challenges and limitations in DP training these complex systems. Our method is particularly well suited for the emerging area of LLM-based recommender systems, but can be readily employed for any recommender systems that process representations of natural language inputs. Our approach involves using DP training methods to fine-tune a publicly pre-trained LLM on a query generation task. The resulting model can generate private synthetic queries representative of the original queries which can be freely shared for any downstream non-private recommendation training procedures without incurring any additional privacy cost. We evaluate our method on its ability to securely train effective deep retrieval models, and we observe significant improvements in their retrieval quality without compromising query-level pri
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#38454;&#27573;&#26377;&#32422;&#26463;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#30701;&#35270;&#39057;&#25512;&#33616;&#38382;&#39064;&#12290;&#36890;&#36807;&#23558;&#30701;&#35270;&#39057;&#25512;&#33616;&#38382;&#39064;&#24314;&#27169;&#20026;&#32422;&#26463;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#22312;&#29992;&#25143;&#20132;&#20114;&#21644;&#22810;&#26679;&#30340;&#21709;&#24212;&#20013;&#20248;&#21270;&#32047;&#35745;&#35266;&#30475;&#26102;&#38388;&#30340;&#38382;&#39064;&#12290;&#36827;&#34892;&#20102;&#20004;&#38454;&#27573;&#30340;&#31574;&#30053;&#23398;&#20064;&#65292;&#24182;&#19988;&#33021;&#22815;&#21516;&#26102;&#28385;&#36275;&#20027;&#35201;&#30446;&#26631;&#21644;&#36741;&#21161;&#30446;&#26631;&#30340;&#32422;&#26463;&#12290;</title><link>http://arxiv.org/abs/2302.01680</link><description>&lt;p&gt;
&#20004;&#38454;&#27573;&#26377;&#32422;&#26463;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#29992;&#20110;&#30701;&#35270;&#39057;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Two-Stage Constrained Actor-Critic for Short Video Recommendation. (arXiv:2302.01680v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01680
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#38454;&#27573;&#26377;&#32422;&#26463;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#30701;&#35270;&#39057;&#25512;&#33616;&#38382;&#39064;&#12290;&#36890;&#36807;&#23558;&#30701;&#35270;&#39057;&#25512;&#33616;&#38382;&#39064;&#24314;&#27169;&#20026;&#32422;&#26463;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#22312;&#29992;&#25143;&#20132;&#20114;&#21644;&#22810;&#26679;&#30340;&#21709;&#24212;&#20013;&#20248;&#21270;&#32047;&#35745;&#35266;&#30475;&#26102;&#38388;&#30340;&#38382;&#39064;&#12290;&#36827;&#34892;&#20102;&#20004;&#38454;&#27573;&#30340;&#31574;&#30053;&#23398;&#20064;&#65292;&#24182;&#19988;&#33021;&#22815;&#21516;&#26102;&#28385;&#36275;&#20027;&#35201;&#30446;&#26631;&#21644;&#36741;&#21161;&#30446;&#26631;&#30340;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#19978;&#30701;&#35270;&#39057;&#30340;&#24191;&#27867;&#27969;&#34892;&#20026;&#35270;&#39057;&#20998;&#20139;&#24179;&#21488;&#19978;&#30340;&#25512;&#33616;&#31995;&#32479;&#20248;&#21270;&#25552;&#20379;&#20102;&#26032;&#30340;&#26426;&#36935;&#21644;&#25361;&#25112;&#12290;&#29992;&#25143;&#19982;&#31995;&#32479;&#20381;&#27425;&#20132;&#20114;&#65292;&#24182;&#25552;&#20379;&#21253;&#25324;&#35266;&#30475;&#26102;&#38388;&#21644;&#23545;&#22810;&#20010;&#35270;&#39057;&#30340;&#21508;&#31181;&#31867;&#22411;&#20132;&#20114;&#22312;&#20869;&#30340;&#22797;&#26434;&#22810;&#38754; responses&#12290;&#19968;&#26041;&#38754;&#65292;&#24179;&#21488;&#26088;&#22312;&#38271;&#26399;&#20248;&#21270;&#29992;&#25143;&#30340;&#32047;&#35745;&#35266;&#30475;&#26102;&#38388;&#65288;&#20027;&#35201;&#30446;&#26631;&#65289;&#65292;&#36825;&#21487;&#20197;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#26377;&#25928;&#20248;&#21270;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#24179;&#21488;&#36824;&#38656;&#35201;&#28385;&#36275;&#36866;&#24212;&#22810;&#20010;&#29992;&#25143;&#20132;&#20114; responses&#65288;&#36741;&#21161;&#30446;&#26631;&#65289;&#30340;&#32422;&#26463;&#65292;&#22914; follow&#12289;share &#31561;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#30701;&#35270;&#39057;&#25512;&#33616;&#38382;&#39064;&#20316;&#20026;&#32422;&#26463;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;CMDP&#65289;&#36827;&#34892;&#20102;&#24314;&#27169;&#12290;&#25105;&#20204;&#21457;&#29616;&#20256;&#32479;&#30340;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#25928;&#26524;&#19981;&#22909;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20004;&#38454;&#27573;&#26377;&#32422;&#26463;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26041;&#27861;&#65306;&#31532;&#19968;&#38454;&#27573;&#65292;&#25105;&#20204;&#23398;&#20064;&#20010;&#20307;&#30340;&#31574;&#30053;&#65292;&#20197;&#20248;&#21270;&#20027;&#35201;&#30446;&#26631;&#12290;&#31532;&#20108;&#38454;&#27573;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#23398;&#20064;&#20849;&#20139;&#30340;&#31574;&#30053;&#65292;&#20197;&#28385;&#36275;&#36741;&#21161;&#30446;&#26631;&#30340;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
The wide popularity of short videos on social media poses new opportunities and challenges to optimize recommender systems on the video-sharing platforms. Users sequentially interact with the system and provide complex and multi-faceted responses, including watch time and various types of interactions with multiple videos. One the one hand, the platforms aims at optimizing the users' cumulative watch time (main goal) in long term, which can be effectively optimized by Reinforcement Learning. On the other hand, the platforms also needs to satisfy the constraint of accommodating the responses of multiple user interactions (auxiliary goals) such like, follow, share etc. In this paper, we formulate the problem of short video recommendation as a Constrained Markov Decision Process (CMDP). We find that traditional constrained reinforcement learning algorithms can not work well in this setting. We propose a novel two-stage constrained actor-critic method: At stage one, we learn individual pol
&lt;/p&gt;</description></item><item><title>PHPQ&#26159;&#19968;&#31181;&#29992;&#20110;&#39640;&#25928;&#32454;&#31890;&#24230;&#22270;&#20687;&#26816;&#32034;&#30340;&#37329;&#23383;&#22612;&#28151;&#21512;&#27744;&#21270;&#37327;&#21270;&#26041;&#27861;&#12290;&#23427;&#36890;&#36807;&#37329;&#23383;&#22612;&#28151;&#21512;&#27744;&#21270;&#27169;&#22359;&#25429;&#33719;&#21644;&#20445;&#30041;&#22810;&#23618;&#27425;&#29305;&#24449;&#20013;&#30340;&#32454;&#31890;&#24230;&#35821;&#20041;&#20449;&#24687;&#65292;&#24182;&#24341;&#20837;&#21487;&#23398;&#20064;&#30340;&#37327;&#21270;&#27169;&#22359;&#26469;&#25552;&#39640;&#21704;&#24076;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2109.05206</link><description>&lt;p&gt;
PHPQ: &#37329;&#23383;&#22612;&#28151;&#21512;&#27744;&#21270;&#37327;&#21270;&#29992;&#20110;&#39640;&#25928;&#30340;&#32454;&#31890;&#24230;&#22270;&#20687;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
PHPQ: Pyramid Hybrid Pooling Quantization for Efficient Fine-Grained Image Retrieval. (arXiv:2109.05206v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.05206
&lt;/p&gt;
&lt;p&gt;
PHPQ&#26159;&#19968;&#31181;&#29992;&#20110;&#39640;&#25928;&#32454;&#31890;&#24230;&#22270;&#20687;&#26816;&#32034;&#30340;&#37329;&#23383;&#22612;&#28151;&#21512;&#27744;&#21270;&#37327;&#21270;&#26041;&#27861;&#12290;&#23427;&#36890;&#36807;&#37329;&#23383;&#22612;&#28151;&#21512;&#27744;&#21270;&#27169;&#22359;&#25429;&#33719;&#21644;&#20445;&#30041;&#22810;&#23618;&#27425;&#29305;&#24449;&#20013;&#30340;&#32454;&#31890;&#24230;&#35821;&#20041;&#20449;&#24687;&#65292;&#24182;&#24341;&#20837;&#21487;&#23398;&#20064;&#30340;&#37327;&#21270;&#27169;&#22359;&#26469;&#25552;&#39640;&#21704;&#24076;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#20854;&#39640;&#35745;&#31639;&#21644;&#23384;&#20648;&#25928;&#29575;&#65292;&#28145;&#24230;&#21704;&#24076;&#26041;&#27861;&#65288;&#21253;&#25324;&#28145;&#24230;&#37327;&#21270;&#21644;&#28145;&#24230;&#20108;&#36827;&#21046;&#21704;&#24076;&#65289;&#24050;&#25104;&#20026;&#22823;&#35268;&#27169;&#22270;&#20687;&#26816;&#32034;&#30340;&#24120;&#35265;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#21704;&#24076;&#26041;&#27861;&#23545;&#20110;&#32454;&#31890;&#24230;&#26816;&#32034;&#26080;&#27861;&#20135;&#29983;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#65292;&#22240;&#20026;&#23427;&#20204;&#36890;&#24120;&#37319;&#29992;&#26368;&#21518;&#19968;&#20010;CNN&#23618;&#30340;&#36755;&#20986;&#29983;&#25104;&#20108;&#36827;&#21046;&#30721;&#12290;&#30001;&#20110;&#26356;&#28145;&#30340;&#23618;&#20542;&#21521;&#20110;&#23558;&#35270;&#35273;&#32447;&#32034;&#65288;&#22914;&#32441;&#29702;&#65289;&#24635;&#32467;&#20026;&#25277;&#35937;&#30340;&#35821;&#20041;&#65288;&#22914;&#29399;&#21644;&#29483;&#65289;&#65292;&#26368;&#21518;&#19968;&#20010;CNN&#23618;&#20135;&#29983;&#30340;&#29305;&#24449;&#22312;&#25429;&#25417;&#27973;&#23618;&#20013;&#23384;&#22312;&#20294;&#20855;&#26377;&#36776;&#21035;&#21147;&#30340;&#32454;&#24494;&#35270;&#35273;&#32454;&#33410;&#26041;&#38754;&#25928;&#26524;&#36739;&#24046;&#12290;&#20026;&#20102;&#25913;&#21892;&#32454;&#31890;&#24230;&#22270;&#20687;&#21704;&#24076;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#37329;&#23383;&#22612;&#28151;&#21512;&#27744;&#21270;&#37327;&#21270;&#65288;PHPQ&#65289;&#26041;&#27861;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#37329;&#23383;&#22612;&#28151;&#21512;&#27744;&#21270;&#65288;PHP&#65289;&#27169;&#22359;&#65292;&#29992;&#20110;&#20174;&#22810;&#23618;&#27425;&#29305;&#24449;&#20013;&#25429;&#33719;&#21644;&#20445;&#30041;&#32454;&#31890;&#24230;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#24378;&#35843;&#19981;&#21516;&#23376;&#31867;&#21035;&#20043;&#38388;&#30340;&#32454;&#24494;&#21306;&#20998;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#23398;&#20064;&#30340;&#37327;&#21270;&#27169;&#22359;&#26469;&#36827;&#19968;&#27493;&#25552;&#39640;&#21704;&#24076;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep hashing approaches, including deep quantization and deep binary hashing, have become a common solution to large-scale image retrieval due to their high computation and storage efficiency. Most existing hashing methods cannot produce satisfactory results for fine-grained retrieval, because they usually adopt the outputs of the last CNN layer to generate binary codes. Since deeper layers tend to summarize visual clues, e.g., texture, into abstract semantics, e.g., dogs and cats, the feature produced by the last CNN layer is less effective in capturing subtle but discriminative visual details that mostly exist in shallow layers. To improve fine-grained image hashing, we propose Pyramid Hybrid Pooling Quantization (PHPQ). Specifically, we propose a Pyramid Hybrid Pooling (PHP) module to capture and preserve fine-grained semantic information from multi-level features, which emphasizes the subtle discrimination of different sub-categories. Besides, we propose a learnable quantization mo
&lt;/p&gt;</description></item></channel></rss>