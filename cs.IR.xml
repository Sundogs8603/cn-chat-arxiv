<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#20248;&#21270;&#30340;DiskANN++&#26041;&#27861;&#65292;&#20351;&#29992;&#26597;&#35810;&#25935;&#24863;&#30340;&#20837;&#21475;&#39030;&#28857;&#22312;&#21516;&#26500;&#26144;&#23556;&#22270;&#32034;&#24341;&#19978;&#36827;&#34892;&#39640;&#25928;&#22522;&#20110;&#39029;&#38754;&#30340;&#25628;&#32034;&#12290;</title><link>http://arxiv.org/abs/2310.00402</link><description>&lt;p&gt;
DiskANN++: &#20351;&#29992;&#26597;&#35810;&#25935;&#24863;&#30340;&#20837;&#21475;&#39030;&#28857;&#22312;&#21516;&#26500;&#26144;&#23556;&#22270;&#32034;&#24341;&#19978;&#36827;&#34892;&#39640;&#25928;&#22522;&#20110;&#39029;&#38754;&#30340;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
DiskANN++: Efficient Page-based Search over Isomorphic Mapped Graph Index using Query-sensitivity Entry Vertex. (arXiv:2310.00402v3 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00402
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#20248;&#21270;&#30340;DiskANN++&#26041;&#27861;&#65292;&#20351;&#29992;&#26597;&#35810;&#25935;&#24863;&#30340;&#20837;&#21475;&#39030;&#28857;&#22312;&#21516;&#26500;&#26144;&#23556;&#22270;&#32034;&#24341;&#19978;&#36827;&#34892;&#39640;&#25928;&#22522;&#20110;&#39029;&#38754;&#30340;&#25628;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32473;&#23450;&#19968;&#20010;&#21521;&#37327;&#25968;&#25454;&#38598;X&#21644;&#19968;&#20010;&#26597;&#35810;&#21521;&#37327;xq&#65292;&#22522;&#20110;&#22270;&#30340;&#36817;&#20284;&#26368;&#36817;&#37051;&#25628;&#32034;(ANNS)&#26088;&#22312;&#26500;&#24314;&#19968;&#20010;&#22270;&#32034;&#24341;G&#65292;&#24182;&#36890;&#36807;&#22312;G&#19978;&#25628;&#32034;&#26469;&#36817;&#20284;&#36820;&#22238;&#19982;xq&#30340;&#26368;&#23567;&#36317;&#31163;&#21521;&#37327;&#12290;&#22522;&#20110;&#22270;&#30340;ANNS&#30340;&#20027;&#35201;&#32570;&#28857;&#26159;&#22270;&#32034;&#24341;&#22826;&#22823;&#65292;&#26080;&#27861;&#36866;&#24212;&#23588;&#20854;&#26159;&#22823;&#35268;&#27169;X&#30340;&#20869;&#23384;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20135;&#21697;&#37327;&#21270;(PQ)&#30340;&#28151;&#21512;&#26041;&#27861;DiskANN&#65292;&#23427;&#22312;&#20869;&#23384;&#20013;&#23384;&#20648;&#20302;&#32500;&#24230;&#30340;PQ&#32034;&#24341;&#65292;&#24182;&#22312;SSD&#20013;&#20445;&#30041;&#22270;&#32034;&#24341;&#65292;&#20174;&#32780;&#20943;&#23567;&#20869;&#23384;&#24320;&#38144;&#21516;&#26102;&#30830;&#20445;&#39640;&#25628;&#32034;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#23427;&#23384;&#22312;&#20004;&#20010;&#37325;&#35201;&#30340;I/O&#38382;&#39064;&#65292;&#20005;&#37325;&#24433;&#21709;&#20102;&#25972;&#20307;&#25928;&#29575;&#65306;(1)&#20174;&#20837;&#21475;&#39030;&#28857;&#21040;&#26597;&#35810;&#37051;&#22495;&#30340;&#38271;&#36335;&#24452;&#23548;&#33268;&#22823;&#37327;&#30340;I/O&#35831;&#27714;&#65292;&#20197;&#21450;(2)&#22312;&#36335;&#30001;&#36807;&#31243;&#20013;&#30340;&#20887;&#20313;I/O&#35831;&#27714;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20248;&#21270;&#30340;DiskANN++&#12290;
&lt;/p&gt;
&lt;p&gt;
Given a vector dataset $\mathcal{X}$ and a query vector $\vec{x}_q$, graph-based Approximate Nearest Neighbor Search (ANNS) aims to build a graph index $G$ and approximately return vectors with minimum distances to $\vec{x}_q$ by searching over $G$. The main drawback of graph-based ANNS is that a graph index would be too large to fit into the memory especially for a large-scale $\mathcal{X}$. To solve this, a Product Quantization (PQ)-based hybrid method called DiskANN is proposed to store a low-dimensional PQ index in memory and retain a graph index in SSD, thus reducing memory overhead while ensuring a high search accuracy. However, it suffers from two I/O issues that significantly affect the overall efficiency: (1) long routing path from an entry vertex to the query's neighborhood that results in large number of I/O requests and (2) redundant I/O requests during the routing process. We propose an optimized DiskANN++ to overcome above issues. Specifically, for the first issue, we pre
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;ReLoop2&#65292;&#19968;&#31181;&#21033;&#29992;&#21709;&#24212;&#24335;&#35823;&#24046;&#34917;&#20607;&#24490;&#29615;&#26500;&#24314;&#33258;&#36866;&#24212;&#25512;&#33616;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38169;&#35823;&#35760;&#24518;&#27169;&#22359;&#26469;&#34917;&#20607;&#27169;&#22411;&#39044;&#27979;&#35823;&#24046;&#65292;&#23454;&#29616;&#24555;&#36895;&#27169;&#22411;&#36866;&#24212;&#12290;</title><link>http://arxiv.org/abs/2306.08808</link><description>&lt;p&gt;
ReLoop2: &#36890;&#36807;&#21709;&#24212;&#24335;&#35823;&#24046;&#34917;&#20607;&#24490;&#29615;&#26500;&#24314;&#33258;&#36866;&#24212;&#25512;&#33616;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ReLoop2: Building Self-Adaptive Recommendation Models via Responsive Error Compensation Loop. (arXiv:2306.08808v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08808
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;ReLoop2&#65292;&#19968;&#31181;&#21033;&#29992;&#21709;&#24212;&#24335;&#35823;&#24046;&#34917;&#20607;&#24490;&#29615;&#26500;&#24314;&#33258;&#36866;&#24212;&#25512;&#33616;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38169;&#35823;&#35760;&#24518;&#27169;&#22359;&#26469;&#34917;&#20607;&#27169;&#22411;&#39044;&#27979;&#35823;&#24046;&#65292;&#23454;&#29616;&#24555;&#36895;&#27169;&#22411;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24037;&#19994;&#25512;&#33616;&#31995;&#32479;&#38754;&#20020;&#38750;&#38745;&#24577;&#29615;&#22659;&#19979;&#36816;&#34892;&#30340;&#25361;&#25112;&#65292;&#20854;&#20013;&#25968;&#25454;&#20998;&#24067;&#30340;&#36716;&#31227;&#28304;&#20110;&#29992;&#25143;&#34892;&#20026;&#30340;&#28436;&#21464;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#19968;&#31181;&#24120;&#35265;&#30340;&#26041;&#27861;&#26159;&#20351;&#29992;&#26032;&#35266;&#23519;&#21040;&#30340;&#25968;&#25454;&#23450;&#26399;&#37325;&#26032;&#35757;&#32451;&#25110;&#22686;&#37327;&#26356;&#26032;&#24050;&#37096;&#32626;&#30340;&#28145;&#24230;&#27169;&#22411;&#65292;&#20135;&#29983;&#36830;&#32493;&#30340;&#35757;&#32451;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#31070;&#32463;&#32593;&#32476;&#30340;&#20256;&#32479;&#23398;&#20064;&#33539;&#24335;&#20381;&#36182;&#20110;&#23567;&#23398;&#20064;&#29575;&#30340;&#36845;&#20195;&#26799;&#24230;&#26356;&#26032;&#65292;&#20351;&#24471;&#22823;&#25512;&#33616;&#27169;&#22411;&#24456;&#38590;&#36866;&#24212;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;ReLoop2&#65292;&#19968;&#31181;&#33258;&#25105;&#32416;&#27491;&#30340;&#23398;&#20064;&#29615;&#36335;&#65292;&#36890;&#36807;&#21709;&#24212;&#24335;&#35823;&#24046;&#34917;&#20607;&#20419;&#36827;&#22312;&#32447;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#24555;&#36895;&#27169;&#22411;&#36866;&#24212;&#12290;&#26412;&#25991;&#21463;&#20154;&#31867;&#22823;&#33041;&#20013;&#35266;&#23519;&#21040;&#30340;&#24930;-&#24555;&#20114;&#34917;&#23398;&#20064;&#31995;&#32479;&#30340;&#21551;&#21457;&#65292;&#25552;&#20986;&#19968;&#20010;&#38169;&#35823;&#35760;&#24518;&#27169;&#22359;&#65292;&#30452;&#25509;&#23384;&#20648;&#26469;&#33258;&#25968;&#25454;&#27969;&#30340;&#38169;&#35823;&#26679;&#26412;&#12290;&#38543;&#21518;&#21033;&#29992;&#36825;&#20123;&#23384;&#20648;&#30340;&#26679;&#26412;&#26469;&#34917;&#20607;&#27169;&#22411;&#39044;&#27979;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Industrial recommender systems face the challenge of operating in non-stationary environments, where data distribution shifts arise from evolving user behaviors over time. To tackle this challenge, a common approach is to periodically re-train or incrementally update deployed deep models with newly observed data, resulting in a continual training process. However, the conventional learning paradigm of neural networks relies on iterative gradient-based updates with a small learning rate, making it slow for large recommendation models to adapt. In this paper, we introduce ReLoop2, a self-correcting learning loop that facilitates fast model adaptation in online recommender systems through responsive error compensation. Inspired by the slow-fast complementary learning system observed in human brains, we propose an error memory module that directly stores error samples from incoming data streams. These stored samples are subsequently leveraged to compensate for model prediction errors durin
&lt;/p&gt;</description></item><item><title>Mol-Instructions&#26159;&#19968;&#20010;&#19987;&#38376;&#20026;&#29983;&#29289;&#20998;&#23376;&#39046;&#22495;&#35774;&#35745;&#30340;&#32508;&#21512;&#25351;&#20196;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#29289;&#39046;&#22495;&#20013;&#30340;&#36866;&#24212;&#33021;&#21147;&#21644;&#35748;&#30693;&#25935;&#38160;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.08018</link><description>&lt;p&gt;
Mol-Instructions: &#19968;&#20010;&#22823;&#35268;&#27169;&#29983;&#29289;&#20998;&#23376;&#25351;&#20196;&#25968;&#25454;&#38598;&#65292;&#20026;&#22823;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#25903;&#25345;
&lt;/p&gt;
&lt;p&gt;
Mol-Instructions: A Large-Scale Biomolecular Instruction Dataset for Large Language Models. (arXiv:2306.08018v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08018
&lt;/p&gt;
&lt;p&gt;
Mol-Instructions&#26159;&#19968;&#20010;&#19987;&#38376;&#20026;&#29983;&#29289;&#20998;&#23376;&#39046;&#22495;&#35774;&#35745;&#30340;&#32508;&#21512;&#25351;&#20196;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#29289;&#39046;&#22495;&#20013;&#30340;&#36866;&#24212;&#33021;&#21147;&#21644;&#35748;&#30693;&#25935;&#38160;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20197;&#20854;&#21331;&#36234;&#30340;&#20219;&#21153;&#22788;&#29702;&#33021;&#21147;&#21644;&#21019;&#26032;&#30340;&#36755;&#20986;&#65292;&#22312;&#35768;&#22810;&#39046;&#22495;&#25512;&#21160;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#29983;&#29289;&#20998;&#23376;&#30740;&#31350;&#31561;&#19987;&#19994;&#39046;&#22495;&#30340;&#29087;&#32451;&#24212;&#29992;&#36824;&#21463;&#21040;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Mol-Instructions&#65292;&#36825;&#26159;&#19968;&#20010;&#32463;&#36807;&#31934;&#24515;&#31574;&#21010;&#12289;&#19987;&#38376;&#38024;&#23545;&#29983;&#29289;&#20998;&#23376;&#39046;&#22495;&#35774;&#35745;&#30340;&#32508;&#21512;&#25351;&#20196;&#25968;&#25454;&#38598;&#12290;Mol-Instructions&#30001;&#19977;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#32452;&#25104;&#65306;&#20998;&#23376;&#23548;&#21521;&#25351;&#20196;&#12289;&#34507;&#30333;&#36136;&#23548;&#21521;&#25351;&#20196;&#21644;&#29983;&#29289;&#20998;&#23376;&#25991;&#26412;&#25351;&#20196;&#65292;&#27599;&#20010;&#37096;&#20998;&#37117;&#34987;&#31574;&#21010;&#29992;&#20110;&#22686;&#24378;LLM&#23545;&#29983;&#29289;&#20998;&#23376;&#29305;&#24615;&#21644;&#34892;&#20026;&#30340;&#29702;&#35299;&#21644;&#39044;&#27979;&#33021;&#21147;&#12290;&#36890;&#36807;&#23545;&#20195;&#34920;&#24615;LLM&#30340;&#24191;&#27867;&#25351;&#20196;&#35843;&#25972;&#23454;&#39564;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;Mol-Instructions&#22312;&#22686;&#24378;&#22823;&#27169;&#22411;&#22312;&#29983;&#29289;&#20998;&#23376;&#30740;&#31350;&#22797;&#26434;&#39046;&#22495;&#20869;&#30340;&#36866;&#24212;&#33021;&#21147;&#21644;&#35748;&#30693;&#25935;&#38160;&#24230;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#20174;&#32780;&#20419;&#36827;&#29983;&#29289;&#20998;&#23376;&#39046;&#22495;&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), with their remarkable task-handling capabilities and innovative outputs, have catalyzed significant advancements across a spectrum of fields. However, their proficiency within specialized domains such as biomolecular studies remains limited. To address this challenge, we introduce Mol-Instructions, a meticulously curated, comprehensive instruction dataset expressly designed for the biomolecular realm. Mol-Instructions is composed of three pivotal components: molecule-oriented instructions, protein-oriented instructions, and biomolecular text instructions, each curated to enhance the understanding and prediction capabilities of LLMs concerning biomolecular features and behaviors. Through extensive instruction tuning experiments on the representative LLM, we underscore the potency of Mol-Instructions to enhance the adaptability and cognitive acuity of large models within the complex sphere of biomolecular studies, thereby promoting advancements in the biomol
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#32534;&#36753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38382;&#39064;&#12289;&#26041;&#27861;&#21644;&#26426;&#20250;&#65292;&#25552;&#20379;&#20102;&#20219;&#21153;&#23450;&#20041;&#21644;&#25361;&#25112;&#30340;&#27010;&#36848;&#12289;&#20808;&#36827;&#26041;&#27861;&#30340;&#23454;&#35777;&#20998;&#26512;&#65292;&#20197;&#21450;&#26500;&#24314;&#20102;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#36825;&#20123;&#32467;&#26524;&#26377;&#21161;&#20110;&#25913;&#36827;LLMs&#30340;&#32534;&#36753;&#25216;&#26415;&#65292;&#25552;&#39640;&#20854;&#25928;&#26524;&#21644;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.13172</link><description>&lt;p&gt;
&#32534;&#36753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#38382;&#39064;&#12289;&#26041;&#27861;&#21644;&#26426;&#20250;
&lt;/p&gt;
&lt;p&gt;
Editing Large Language Models: Problems, Methods, and Opportunities. (arXiv:2305.13172v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13172
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#32534;&#36753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38382;&#39064;&#12289;&#26041;&#27861;&#21644;&#26426;&#20250;&#65292;&#25552;&#20379;&#20102;&#20219;&#21153;&#23450;&#20041;&#21644;&#25361;&#25112;&#30340;&#27010;&#36848;&#12289;&#20808;&#36827;&#26041;&#27861;&#30340;&#23454;&#35777;&#20998;&#26512;&#65292;&#20197;&#21450;&#26500;&#24314;&#20102;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#36825;&#20123;&#32467;&#26524;&#26377;&#21161;&#20110;&#25913;&#36827;LLMs&#30340;&#32534;&#36753;&#25216;&#26415;&#65292;&#25552;&#39640;&#20854;&#25928;&#26524;&#21644;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#33021;&#22815;&#35757;&#32451;&#20986;&#34920;&#29616;&#20248;&#31168;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#20294;&#20854;&#20445;&#25345;&#30456;&#20851;&#24615;&#21644;&#32416;&#27491;&#38169;&#35823;&#30340;&#26041;&#27861;&#20173;&#28982;&#38590;&#20197;&#30830;&#23450;&#12290;&#20026;&#27492;&#65292;&#26368;&#36817;&#20960;&#24180;&#20986;&#29616;&#20102;&#35768;&#22810;&#32534;&#36753;LLMs&#30340;&#25216;&#26415;&#65292;&#20854;&#30446;&#26631;&#26159;&#22312;&#29305;&#23450;&#39046;&#22495;&#20869;&#39640;&#25928;&#22320;&#25913;&#21464;LLMs&#30340;&#34892;&#20026;&#65292;&#21516;&#26102;&#19981;&#23545;&#20854;&#20182;&#36755;&#20837;&#30340;&#24615;&#33021;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#19982;LLMs&#27169;&#22411;&#32534;&#36753;&#30456;&#20851;&#30340;&#38382;&#39064;&#12289;&#26041;&#27861;&#21644;&#26426;&#20250;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#27169;&#22411;&#32534;&#36753;&#20219;&#21153;&#23450;&#20041;&#21644;&#30456;&#20851;&#25361;&#25112;&#30340;&#20840;&#38754;&#27010;&#36848;&#65292;&#20197;&#21450;&#23545;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30340;&#28145;&#20837;&#23454;&#35777;&#20998;&#26512;&#12290;&#25105;&#20204;&#36824;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20197;&#20419;&#36827;&#26356;&#24378;&#22823;&#30340;&#35780;&#20272;&#65292;&#24182;&#25351;&#20986;&#29616;&#26377;&#25216;&#26415;&#22266;&#26377;&#30340;&#25345;&#20037;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#20026;&#27599;&#31181;&#32534;&#36753;&#25216;&#26415;&#30340;&#25928;&#26524;&#21644;&#21487;&#34892;&#24615;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#65292;&#20174;&#32780;&#24110;&#21161;&#31038;&#21306;&#22312;LLMs&#30340;&#31649;&#29702;&#20013;&#21462;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the ability to train capable LLMs, the methodology for maintaining their relevancy and rectifying errors remains elusive. To this end, the past few years have witnessed a surge in techniques for editing LLMs, the objective of which is to efficiently alter the behavior of LLMs within a specific domain without negatively impacting performance across other inputs. This paper embarks on a deep exploration of the problems, methods, and opportunities related to model editing for LLMs. In particular, we provide an exhaustive overview of the task definition and challenges associated with model editing, along with an in-depth empirical analysis of the most progressive methods currently at our disposal. We also build a new benchmark dataset to facilitate a more robust evaluation and pinpoint enduring issues intrinsic to existing techniques. Our objective is to provide valuable insights into the effectiveness and feasibility of each editing technique, thereby assisting the community in ma
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#20998;&#26512;Hugging Face&#19978;1,417&#20010;ML&#27169;&#22411;&#21450;&#30456;&#20851;&#25968;&#25454;&#38598;&#30340;&#30899;&#36275;&#36857;&#27979;&#37327;&#24773;&#20917;&#65292;&#25552;&#20986;&#20102;&#26377;&#20851;&#22914;&#20309;&#25253;&#21578;&#21644;&#20248;&#21270;ML&#27169;&#22411;&#30340;&#30899;&#25928;&#29575;&#30340;&#35265;&#35299;&#21644;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2305.11164</link><description>&lt;p&gt;
&#25506;&#32034;&#25265;&#25265;&#33080;ML&#27169;&#22411;&#30340;&#30899;&#36275;&#36857;&#65306;&#19968;&#39033;&#23384;&#20648;&#24211;&#25366;&#25496;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Exploring the Carbon Footprint of Hugging Face's ML Models: A Repository Mining Study. (arXiv:2305.11164v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11164
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#20998;&#26512;Hugging Face&#19978;1,417&#20010;ML&#27169;&#22411;&#21450;&#30456;&#20851;&#25968;&#25454;&#38598;&#30340;&#30899;&#36275;&#36857;&#27979;&#37327;&#24773;&#20917;&#65292;&#25552;&#20986;&#20102;&#26377;&#20851;&#22914;&#20309;&#25253;&#21578;&#21644;&#20248;&#21270;ML&#27169;&#22411;&#30340;&#30899;&#25928;&#29575;&#30340;&#35265;&#35299;&#21644;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;(ML)&#31995;&#32479;&#30340;&#23835;&#36215;&#21152;&#21095;&#20102;&#23427;&#20204;&#30340;&#30899;&#36275;&#36857;&#65292;&#36825;&#26159;&#30001;&#20110;&#20854;&#22686;&#21152;&#30340;&#33021;&#21147;&#21644;&#27169;&#22411;&#22823;&#23567;&#25152;&#33268;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23545;ML&#27169;&#22411;&#30340;&#30899;&#36275;&#36857;&#22914;&#20309;&#23454;&#38469;&#27979;&#37327;&#12289;&#25253;&#21578;&#21644;&#35780;&#20272;&#30340;&#35748;&#35782;&#30456;&#23545;&#36739;&#23569;&#12290;&#22240;&#27492;&#65292;&#26412;&#35770;&#25991;&#26088;&#22312;&#20998;&#26512;&#22312;Hugging Face&#19978;1,417&#20010;ML&#27169;&#22411;&#21644;&#30456;&#20851;&#25968;&#25454;&#38598;&#30340;&#30899;&#36275;&#36857;&#27979;&#37327;&#24773;&#20917;&#65292;Hugging Face&#26159;&#26368;&#21463;&#27426;&#36814;&#30340;&#39044;&#35757;&#32451;ML&#27169;&#22411;&#30340;&#23384;&#20648;&#24211;&#12290;&#30446;&#26631;&#26159;&#25552;&#20379;&#26377;&#20851;&#22914;&#20309;&#25253;&#21578;&#21644;&#20248;&#21270;ML&#27169;&#22411;&#30340;&#30899;&#25928;&#29575;&#30340;&#35265;&#35299;&#21644;&#24314;&#35758;&#12290;&#35813;&#30740;&#31350;&#21253;&#25324;Hugging Face Hub API&#19978;&#26377;&#20851;&#30899;&#25490;&#25918;&#30340;&#31532;&#19968;&#39033;&#23384;&#20648;&#24211;&#25366;&#25496;&#30740;&#31350;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#22238;&#31572;&#20004;&#20010;&#30740;&#31350;&#38382;&#39064;&#65306;(1) ML&#27169;&#22411;&#30340;&#21019;&#24314;&#32773;&#22914;&#20309;&#22312;Hugging Face Hub&#19978;&#27979;&#37327;&#21644;&#25253;&#21578;&#30899;&#25490;&#25918;&#65311;(2) &#21738;&#20123;&#26041;&#38754;&#24433;&#21709;&#20102;&#35757;&#32451;ML&#27169;&#22411;&#30340;&#30899;&#25490;&#25918;&#65311;&#35813;&#30740;&#31350;&#24471;&#20986;&#20102;&#20960;&#20010;&#20851;&#38190;&#21457;&#29616;&#12290;&#20854;&#20013;&#21253;&#25324;&#30899;&#25490;&#25918;&#25253;&#21578;&#27169;&#24335;&#27604;&#20363;&#30340;&#36880;&#27493;&#19979;&#38477;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rise of machine learning (ML) systems has exacerbated their carbon footprint due to increased capabilities and model sizes. However, there is scarce knowledge on how the carbon footprint of ML models is actually measured, reported, and evaluated. In light of this, the paper aims to analyze the measurement of the carbon footprint of 1,417 ML models and associated datasets on Hugging Face, which is the most popular repository for pretrained ML models. The goal is to provide insights and recommendations on how to report and optimize the carbon efficiency of ML models. The study includes the first repository mining study on the Hugging Face Hub API on carbon emissions. This study seeks to answer two research questions: (1) how do ML model creators measure and report carbon emissions on Hugging Face Hub?, and (2) what aspects impact the carbon emissions of training ML models? The study yielded several key findings. These include a decreasing proportion of carbon emissions-reporting mode
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#20280;&#32553;&#39640;&#25928;&#30340;&#22270;&#20687;&#21040;&#36328;&#27169;&#24577;&#26816;&#32034;&#33539;&#24335;Mixer&#65292;&#35299;&#20915;&#20102;&#39046;&#22495;&#24046;&#36317;&#12289;&#36328;&#27169;&#24577;&#25968;&#25454;&#23545;&#40784;&#21644;&#34701;&#21512;&#12289;&#32321;&#26434;&#30340;&#25968;&#25454;&#35757;&#32451;&#26631;&#31614;&#20197;&#21450;&#28023;&#37327;&#26597;&#35810;&#21644;&#21450;&#26102;&#21709;&#24212;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.03972</link><description>&lt;p&gt;
Mixer: &#24212;&#29992;&#20110;&#24037;&#19994;&#24212;&#29992;&#30340;&#22270;&#20687;&#21040;&#36328;&#27169;&#24577;&#26816;&#32034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Mixer: Image to Multi-Modal Retrieval Learning for Industrial Application. (arXiv:2305.03972v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03972
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#20280;&#32553;&#39640;&#25928;&#30340;&#22270;&#20687;&#21040;&#36328;&#27169;&#24577;&#26816;&#32034;&#33539;&#24335;Mixer&#65292;&#35299;&#20915;&#20102;&#39046;&#22495;&#24046;&#36317;&#12289;&#36328;&#27169;&#24577;&#25968;&#25454;&#23545;&#40784;&#21644;&#34701;&#21512;&#12289;&#32321;&#26434;&#30340;&#25968;&#25454;&#35757;&#32451;&#26631;&#31614;&#20197;&#21450;&#28023;&#37327;&#26597;&#35810;&#21644;&#21450;&#26102;&#21709;&#24212;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#27169;&#24577;&#26816;&#32034;&#19968;&#30452;&#26159;&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#21644;&#20869;&#23481;&#20998;&#20139;&#31038;&#20132;&#23186;&#20307;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#38656;&#27714;&#65292;&#20854;&#20013;&#26597;&#35810;&#26159;&#19968;&#24352;&#22270;&#29255;&#65292;&#25991;&#26723;&#26159;&#20855;&#26377;&#22270;&#29255;&#21644;&#25991;&#26412;&#25551;&#36848;&#30340;&#39033;&#30446;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36825;&#31181;&#26816;&#32034;&#20219;&#21153;&#20173;&#38754;&#20020;&#35832;&#22810;&#25361;&#25112;&#65292;&#21253;&#25324;&#39046;&#22495;&#24046;&#36317;&#12289;&#36328;&#27169;&#24577;&#25968;&#25454;&#23545;&#40784;&#21644;&#34701;&#21512;&#12289;&#32321;&#26434;&#30340;&#25968;&#25454;&#35757;&#32451;&#26631;&#31614;&#20197;&#21450;&#28023;&#37327;&#26597;&#35810;&#21644;&#21450;&#26102;&#21709;&#24212;&#31561;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Mixer&#30340;&#26032;&#22411;&#21487;&#20280;&#32553;&#21644;&#39640;&#25928;&#30340;&#22270;&#20687;&#26597;&#35810;&#21040;&#36328;&#27169;&#24577;&#26816;&#32034;&#23398;&#20064;&#33539;&#24335;&#12290;Mixer&#36890;&#36807;&#33258;&#36866;&#24212;&#22320;&#25972;&#21512;&#22810;&#27169;&#24577;&#25968;&#25454;&#12289;&#26356;&#39640;&#25928;&#22320;&#25366;&#25496;&#20559;&#26012;&#21644;&#22024;&#26434;&#30340;&#25968;&#25454;&#65292;&#24182;&#21487;&#25193;&#23637;&#21040;&#39640;&#36127;&#36733;&#37327;&#65292;&#35299;&#20915;&#20102;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-modal retrieval, where the query is an image and the doc is an item with both image and text description, is ubiquitous in e-commerce platforms and content-sharing social media. However, little research attention has been paid to this important application. This type of retrieval task is challenging due to the facts: 1)~domain gap exists between query and doc. 2)~multi-modality alignment and fusion. 3)~skewed training data and noisy labels collected from user behaviors. 4)~huge number of queries and timely responses while the large-scale candidate docs exist. To this end, we propose a novel scalable and efficient image query to multi-modal retrieval learning paradigm called Mixer, which adaptively integrates multi-modality data, mines skewed and noisy data more efficiently and scalable to high traffic. The Mixer consists of three key ingredients: First, for query and doc image, a shared encoder network followed by separate transformation networks are utilized to account for their
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;CTR&#39044;&#27979;&#30340;&#22686;&#24378;&#21452;&#27969;MLP&#27169;&#22411;&#65292;&#32463;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#20165;&#26159;&#31616;&#21333;&#22320;&#32467;&#21512;&#20004;&#20010;MLP&#23601;&#21487;&#20197;&#23454;&#29616;&#20196;&#20154;&#24778;&#35766;&#30340;&#33391;&#22909;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.00902</link><description>&lt;p&gt;
FinalMLP: &#29992;&#20110;CTR&#39044;&#27979;&#30340;&#22686;&#24378;&#21452;&#27969;MLP&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
FinalMLP: An Enhanced Two-Stream MLP Model for CTR Prediction. (arXiv:2304.00902v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00902
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;CTR&#39044;&#27979;&#30340;&#22686;&#24378;&#21452;&#27969;MLP&#27169;&#22411;&#65292;&#32463;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#20165;&#26159;&#31616;&#21333;&#22320;&#32467;&#21512;&#20004;&#20010;MLP&#23601;&#21487;&#20197;&#23454;&#29616;&#20196;&#20154;&#24778;&#35766;&#30340;&#33391;&#22909;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28857;&#20987;&#29575;&#39044;&#27979;&#26159;&#22312;&#32447;&#24191;&#21578;&#21644;&#25512;&#33616;&#20013;&#30340;&#22522;&#26412;&#20219;&#21153;&#20043;&#19968;&#12290;&#34429;&#28982;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#22312;&#35768;&#22810;&#28145;&#24230;CTR&#39044;&#27979;&#27169;&#22411;&#20013;&#20316;&#20026;&#26680;&#24515;&#32452;&#20214;&#65292;&#20294;&#24191;&#20026;&#20154;&#30693;&#30340;&#26159;&#65292;&#20165;&#24212;&#29992;&#19968;&#20010;&#22522;&#26412;MLP&#32593;&#32476;&#22312;&#23398;&#20064;&#20056;&#27861;&#29305;&#24449;&#30456;&#20114;&#20316;&#29992;&#26041;&#38754;&#24182;&#19981;&#39640;&#25928;&#12290;&#22240;&#27492;&#65292;&#35768;&#22810;&#20004;&#20010;&#27969;&#20132;&#20114;&#27169;&#22411;&#65288;&#20363;&#22914;&#65292;DeepFM&#21644;DCN&#65289;&#36890;&#36807;&#23558;MLP&#32593;&#32476;&#19982;&#21478;&#19968;&#20010;&#19987;&#29992;&#32593;&#32476;&#38598;&#25104;&#20197;&#22686;&#24378;CTR&#39044;&#27979;&#12290;&#30001;&#20110;MLP&#27969;&#38544;&#24335;&#22320;&#23398;&#20064;&#29305;&#24449;&#20132;&#20114;&#20316;&#29992;&#65292;&#22240;&#27492;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#20110;&#22686;&#24378;&#34917;&#20805;&#27969;&#20013;&#30340;&#26174;&#24335;&#29305;&#24449;&#20132;&#20114;&#20316;&#29992;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#30340;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#19968;&#20010;&#32463;&#36807;&#33391;&#22909;&#35843;&#25972;&#30340;&#21452;&#27969;MLP&#27169;&#22411;&#65292;&#23427;&#21482;&#26159;&#31616;&#21333;&#22320;&#32467;&#21512;&#20102;&#20004;&#20010;MLP&#65292;&#29978;&#33267;&#21487;&#20197;&#23454;&#29616;&#20196;&#20154;&#24778;&#35766;&#30340;&#33391;&#22909;&#24615;&#33021;&#65292;&#36825;&#22312;&#29616;&#26377;&#30340;&#24037;&#20316;&#20013;&#20174;&#26410;&#34987;&#25253;&#36947;&#36807;&#12290;&#22522;&#20110;&#36825;&#20010;&#35266;&#23519;&#32467;&#26524;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#29305;&#24449;&#36873;&#25321;&#21644;&#20132;&#20114;&#32858;&#21512;&#23618;&#12290;
&lt;/p&gt;
&lt;p&gt;
Click-through rate (CTR) prediction is one of the fundamental tasks for online advertising and recommendation. While multi-layer perceptron (MLP) serves as a core component in many deep CTR prediction models, it has been widely recognized that applying a vanilla MLP network alone is inefficient in learning multiplicative feature interactions. As such, many two-stream interaction models (e.g., DeepFM and DCN) have been proposed by integrating an MLP network with another dedicated network for enhanced CTR prediction. As the MLP stream learns feature interactions implicitly, existing research focuses mainly on enhancing explicit feature interactions in the complementary stream. In contrast, our empirical study shows that a well-tuned two-stream MLP model that simply combines two MLPs can even achieve surprisingly good performance, which has never been reported before by existing work. Based on this observation, we further propose feature selection and interaction aggregation layers that c
&lt;/p&gt;</description></item></channel></rss>