<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#39046;&#22495;&#30693;&#35782;&#30340;&#33647;&#29289;&#25512;&#33616;&#26694;&#26550;DKINet&#65292;&#23558;&#39046;&#22495;&#30693;&#35782;&#19982;&#24739;&#32773;&#20020;&#24202;&#34920;&#29616;&#30456;&#32467;&#21512;&#65292;&#27492;&#20026;&#39318;&#27425;&#23454;&#39564;&#12290;</title><link>http://arxiv.org/abs/2305.19604</link><description>&lt;p&gt;
&#36890;&#36807;&#39046;&#22495;&#30693;&#35782;&#21551;&#31034;&#30340;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#33647;&#29289;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Medication Recommendation via Domain Knowledge Informed Deep Learning. (arXiv:2305.19604v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19604
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#39046;&#22495;&#30693;&#35782;&#30340;&#33647;&#29289;&#25512;&#33616;&#26694;&#26550;DKINet&#65292;&#23558;&#39046;&#22495;&#30693;&#35782;&#19982;&#24739;&#32773;&#20020;&#24202;&#34920;&#29616;&#30456;&#32467;&#21512;&#65292;&#27492;&#20026;&#39318;&#27425;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33647;&#29289;&#25512;&#33616;&#26159;&#21307;&#30103;&#20445;&#20581;&#30340;&#22522;&#26412;&#20294;&#33267;&#20851;&#37325;&#35201;&#30340;&#20998;&#25903;&#65292;&#25552;&#20379;&#26426;&#20250;&#20026;&#22797;&#26434;&#20581;&#24247;&#29366;&#20917;&#30340;&#24739;&#32773;&#25903;&#25345;&#20020;&#24202;&#21307;&#29983;&#26356;&#31934;&#30830;&#30340;&#33647;&#29289;&#22788;&#26041;&#12290;&#20174;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#20013;&#23398;&#20064;&#25512;&#33616;&#33647;&#29289;&#26159;&#20808;&#21069;&#30740;&#31350;&#20013;&#26368;&#24120;&#35265;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#24573;&#35270;&#20102;&#26681;&#25454;&#24739;&#32773;&#30340;EHR&#20013;&#30340;&#20020;&#24202;&#34920;&#29616;&#32435;&#20837;&#39046;&#22495;&#30693;&#35782;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#21160;&#24577;&#39046;&#22495;&#30693;&#35782;&#30340;&#33647;&#29289;&#25512;&#33616;&#26694;&#26550;&#65292;&#21363;&#39046;&#22495;&#30693;&#35782;&#21551;&#31034;&#32593;&#32476;&#65288;DKINet&#65289;&#65292;&#29992;&#20110;&#23558;&#39046;&#22495;&#30693;&#35782;&#19982;&#21487;&#35266;&#23519;&#30340;&#24739;&#32773;&#20020;&#24202;&#34920;&#29616;&#30456;&#32467;&#21512;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#39318;&#20808;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#39046;&#22495;&#30693;&#35782;&#30340;&#32534;&#30721;&#22120;&#26469;&#25429;&#25417;&#39046;&#22495;&#20449;&#24687;&#65292;&#28982;&#21518;&#24320;&#21457;&#20102;&#19968;&#20010;&#25968;&#25454;&#39537;&#21160;&#30340;&#32534;&#30721;&#22120;&#23558;&#39046;&#22495;&#30693;&#35782;&#25972;&#21512;&#21040;&#21487;&#35266;&#23519;&#30340;EHR&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medication recommendation is a fundamental yet crucial branch of healthcare, which provides opportunities to support clinical physicians with more accurate medication prescriptions for patients with complex health conditions. Learning from electronic health records (EHR) to recommend medications is the most common way in previous studies. However, most of them neglect incorporating domain knowledge according to the clinical manifestations in the EHR of the patient. To address these issues, we propose a novel \textbf{D}omain \textbf{K}nowledge \textbf{I}nformed \textbf{Net}work (DKINet) to integrate domain knowledge with observable clinical manifestations of the patient, which is the first dynamic domain knowledge informed framework toward medication recommendation. In particular, we first design a knowledge-driven encoder to capture the domain information and then develop a data-driven encoder to integrate domain knowledge into the observable EHR. To endow the model with the capability
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#22270;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;UGNN&#65292; &#35299;&#20915;&#20102;&#21322;&#30417;&#30563;&#26222;&#36866;&#22270;&#20998;&#31867;&#38382;&#39064;&#65292;&#36890;&#36807;&#20272;&#35745;&#26410;&#26631;&#35760;&#22270;&#30340;&#30830;&#23450;&#24615;&#35299;&#20915;&#20102;&#31867;&#21035;&#20559;&#31227;&#65292;&#20855;&#26377;&#26368;&#26032;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.19598</link><description>&lt;p&gt;
&#21322;&#30417;&#30563;&#26222;&#36866;&#22270;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Towards Semi-supervised Universal Graph Classification. (arXiv:2305.19598v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19598
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#22270;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;UGNN&#65292; &#35299;&#20915;&#20102;&#21322;&#30417;&#30563;&#26222;&#36866;&#22270;&#20998;&#31867;&#38382;&#39064;&#65292;&#36890;&#36807;&#20272;&#35745;&#26410;&#26631;&#35760;&#22270;&#30340;&#30830;&#23450;&#24615;&#35299;&#20915;&#20102;&#31867;&#21035;&#20559;&#31227;&#65292;&#20855;&#26377;&#26368;&#26032;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#25512;&#21160;&#20102;&#22270;&#20998;&#31867;&#30340;&#26368;&#26032;&#25216;&#26415;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;UGNN&#30340;&#26032;&#22411;&#22270;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#20197;&#20174;&#23376;&#22270;&#35282;&#24230;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#26469;&#35299;&#20915;&#21322;&#30417;&#30563;&#26222;&#36866;&#22270;&#20998;&#31867;&#38382;&#39064;&#12290;&#35813;&#38382;&#39064;&#30340;&#25361;&#25112;&#22312;&#20110;&#32570;&#20047;&#26631;&#31614;&#25968;&#25454;&#21644;&#21487;&#33021;&#30340;&#31867;&#21035;&#20559;&#31227;&#12290;UGNN&#36890;&#36807;&#20272;&#35745;&#26410;&#26631;&#35760;&#22270;&#30340;&#30830;&#23450;&#24615;&#26469;&#35299;&#20915;&#31867;&#21035;&#20559;&#31227;&#65292;&#20351;&#20854;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#26368;&#26032;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks have pushed state-of-the-arts in graph classifications recently. Typically, these methods are studied within the context of supervised end-to-end training, which necessities copious task-specific labels. However, in real-world circumstances, labeled data could be limited, and there could be a massive corpus of unlabeled data, even from unknown classes as a complementary. Towards this end, we study the problem of semi-supervised universal graph classification, which not only identifies graph samples which do not belong to known classes, but also classifies the remaining samples into their respective classes. This problem is challenging due to a severe lack of labels and potential class shifts. In this paper, we propose a novel graph neural network framework named UGNN, which makes the best of unlabeled data from the subgraph perspective. To tackle class shifts, we estimate the certainty of unlabeled graphs using multiple subgraphs, which facilities the discovery of
&lt;/p&gt;</description></item><item><title>AdANNS&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#35821;&#20041;&#25628;&#32034;&#26694;&#26550;&#65292;&#21033;&#29992;&#19981;&#21516;&#23481;&#37327;&#30340;&#33258;&#36866;&#24212;&#34920;&#31034;&#24418;&#24335;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#31934;&#24230;-&#35745;&#31639;&#25240;&#34935;&#26435;&#34913;&#65292;&#30456;&#20284;&#24230;&#35745;&#31639;&#36234;&#25509;&#36817;&#30340;&#25968;&#25454;&#28857;&#23558;&#20351;&#29992;&#26356;&#20302;&#23481;&#37327;&#30340;&#34920;&#31034;&#24418;&#24335;&#36827;&#34892;&#35745;&#31639;&#65292;&#28436;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#31934;&#24230;-&#35745;&#31639;&#25240;&#34935;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2305.19435</link><description>&lt;p&gt;
AdANNS: &#19968;&#31181;&#33258;&#36866;&#24212;&#35821;&#20041;&#25628;&#32034;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
AdANNS: A Framework for Adaptive Semantic Search. (arXiv:2305.19435v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19435
&lt;/p&gt;
&lt;p&gt;
AdANNS&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#35821;&#20041;&#25628;&#32034;&#26694;&#26550;&#65292;&#21033;&#29992;&#19981;&#21516;&#23481;&#37327;&#30340;&#33258;&#36866;&#24212;&#34920;&#31034;&#24418;&#24335;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#31934;&#24230;-&#35745;&#31639;&#25240;&#34935;&#26435;&#34913;&#65292;&#30456;&#20284;&#24230;&#35745;&#31639;&#36234;&#25509;&#36817;&#30340;&#25968;&#25454;&#28857;&#23558;&#20351;&#29992;&#26356;&#20302;&#23481;&#37327;&#30340;&#34920;&#31034;&#24418;&#24335;&#36827;&#34892;&#35745;&#31639;&#65292;&#28436;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#31934;&#24230;-&#35745;&#31639;&#25240;&#34935;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#35268;&#27169;&#30340;&#25628;&#32034;&#31995;&#32479;&#23398;&#20064;&#19968;&#20010;&#32534;&#30721;&#22120;&#26469;&#23884;&#20837;&#19968;&#20010;&#32473;&#23450;&#30340;&#26597;&#35810;&#65292;&#28982;&#21518;&#23558;&#20854;&#36830;&#25509;&#21040;&#36817;&#20284;&#26368;&#36817;&#37051;&#25628;&#32034;(ANNS)&#31649;&#36947;&#20013;&#26469;&#26816;&#32034;&#30456;&#20284;&#30340;&#25968;&#25454;&#28857;&#12290;&#20026;&#20102;&#20934;&#30830;&#22320;&#25429;&#25417;&#23614;&#37096;&#26597;&#35810;&#21644;&#25968;&#25454;&#28857;&#65292;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#36890;&#24120;&#26159;&#21018;&#24615;&#30340;&#12289;&#39640;&#32500;&#30340;&#21521;&#37327;&#65292;&#36890;&#24120;&#22312;&#25972;&#20010;ANNS&#31649;&#36947;&#20013;&#19968;&#25104;&#19981;&#21464;&#65292;&#24182;&#19988;&#21487;&#33021;&#23548;&#33268;&#35745;&#31639;&#19978;&#26114;&#36149;&#30340;&#26816;&#32034;&#12290;&#26412;&#25991;&#35748;&#20026;&#65292;&#19982;&#20854;&#20351;&#29992;&#21018;&#24615;&#30340;&#34920;&#31034;&#24418;&#24335;&#65292;ANNS&#30340;&#19981;&#21516;&#38454;&#27573;&#21487;&#20197;&#21033;&#29992;&#19981;&#21516;&#23481;&#37327;&#30340;&#33258;&#36866;&#24212;&#34920;&#31034;&#24418;&#24335;&#20197;&#33719;&#24471;&#26174;&#33879;&#30340;&#31934;&#24230;-&#35745;&#31639;&#25240;&#34935;&#26435;&#34913;&#65292;&#21363;&#21487;&#20197;&#36827;&#34892;&#26356;&#21152;&#36817;&#20284;&#35745;&#31639;&#30340;ANNS&#38454;&#27573;&#24212;&#35813;&#20351;&#29992;&#30456;&#21516;&#25968;&#25454;&#28857;&#30340;&#20302;&#23481;&#37327;&#34920;&#31034;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;AdANNS&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;ANNS&#35774;&#35745;&#26694;&#26550;&#65292;&#26126;&#30830;&#21033;&#29992;Matryoshka&#34920;&#31034;&#30340;&#28789;&#27963;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;AdANNS&#30340;&#26032;&#22411;&#20851;&#38190;ANNS&#26500;&#24314;&#28436;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#31934;&#24230;-&#35745;&#31639;&#25240;&#34935;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Web-scale search systems learn an encoder to embed a given query which is then hooked into an approximate nearest neighbor search (ANNS) pipeline to retrieve similar data points. To accurately capture tail queries and data points, learned representations typically are rigid, high-dimensional vectors that are generally used as-is in the entire ANNS pipeline and can lead to computationally expensive retrieval. In this paper, we argue that instead of rigid representations, different stages of ANNS can leverage adaptive representations of varying capacities to achieve significantly better accuracy-compute trade-offs, i.e., stages of ANNS that can get away with more approximate computation should use a lower-capacity representation of the same data point. To this end, we introduce AdANNS, a novel ANNS design framework that explicitly leverages the flexibility of Matryoshka Representations. We demonstrate state-of-the-art accuracy-compute trade-offs using novel AdANNS-based key ANNS building
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21382;&#21490;&#25991;&#20214;&#25628;&#32034;&#24341;&#25806;DuoSearch&#65292;&#35813;&#24341;&#25806;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#35299;&#20915;&#25968;&#23383;&#21270;&#21382;&#21490;&#25991;&#29486;&#25628;&#32034;&#20013;&#30340;&#27491;&#23383;&#27861;&#21464;&#20307;&#21644;&#23383;&#31526;&#35782;&#21035;&#38169;&#35823;&#38382;&#39064;&#65292;&#24182;&#25104;&#21151;&#24212;&#29992;&#20110;&#20445;&#21152;&#21033;&#20122;&#20013;&#26399;&#21040;20&#19990;&#32426;&#20013;&#26399;&#30340;&#21382;&#21490;&#25253;&#32440;&#38598;&#21512;&#12290;</title><link>http://arxiv.org/abs/2305.19392</link><description>&lt;p&gt;
DuoSearch&#65306;&#19968;&#31181;&#29992;&#20110;&#20445;&#21152;&#21033;&#20122;&#21382;&#21490;&#25991;&#20214;&#30340;&#26032;&#22411;&#25628;&#32034;&#24341;&#25806;
&lt;/p&gt;
&lt;p&gt;
DuoSearch: A Novel Search Engine for Bulgarian Historical Documents. (arXiv:2305.19392v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19392
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21382;&#21490;&#25991;&#20214;&#25628;&#32034;&#24341;&#25806;DuoSearch&#65292;&#35813;&#24341;&#25806;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#35299;&#20915;&#25968;&#23383;&#21270;&#21382;&#21490;&#25991;&#29486;&#25628;&#32034;&#20013;&#30340;&#27491;&#23383;&#27861;&#21464;&#20307;&#21644;&#23383;&#31526;&#35782;&#21035;&#38169;&#35823;&#38382;&#39064;&#65292;&#24182;&#25104;&#21151;&#24212;&#29992;&#20110;&#20445;&#21152;&#21033;&#20122;&#20013;&#26399;&#21040;20&#19990;&#32426;&#20013;&#26399;&#30340;&#21382;&#21490;&#25253;&#32440;&#38598;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#21270;&#21382;&#21490;&#25991;&#29486;&#30340;&#25628;&#32034;&#23384;&#22312;&#27491;&#23383;&#27861;&#21464;&#20307;&#21644;&#23383;&#31526;&#35782;&#21035;&#38169;&#35823;&#20004;&#22823;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21382;&#21490;&#25991;&#20214;&#25628;&#32034;&#24341;&#25806;&#8212;&#8212;DuoSearch&#65292;&#23427;&#20351;&#29992;ElasticSearch&#21644;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#26696;&#12290;&#35813;&#31995;&#32479;&#22312;&#20445;&#21152;&#21033;&#20122;&#20013;&#26399;&#21040;20&#19990;&#32426;&#20013;&#26399;&#30340;&#21382;&#21490;&#25253;&#32440;&#38598;&#21512;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#20026;&#26368;&#32456;&#29992;&#25143;&#25552;&#20379;&#20102;&#20132;&#20114;&#24335;&#21644;&#30452;&#35266;&#30340;&#30028;&#38754;&#65292;&#20801;&#35768;&#20182;&#20204;&#20197;&#29616;&#20195;&#20445;&#21152;&#21033;&#20122;&#35821;&#36755;&#20837;&#25628;&#32034;&#35789;&#24182;&#36328;&#21382;&#21490;&#25340;&#20889;&#36827;&#34892;&#25628;&#32034;&#12290;&#36825;&#26159;&#39318;&#20010;&#26131;&#20110;&#20351;&#29992;&#25968;&#23383;&#21270;&#20445;&#21152;&#21033;&#20122;&#21382;&#21490;&#25991;&#29486;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Search in collections of digitised historical documents is hindered by a two-prong problem, orthographic variety and optical character recognition (OCR) mistakes. We present a new search engine for historical documents, DuoSearch, which uses ElasticSearch and machine learning methods based on deep neural networks to offer a solution to this problem. It was tested on a collection of historical newspapers in Bulgarian from the mid-19th to the mid-20th century. The system provides an interactive and intuitive interface for the end-users allowing them to enter search terms in modern Bulgarian and search across historical spellings. This is the first solution facilitating the use of digitised historical documents in Bulgarian.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21518;&#32622;&#20559;&#24046;&#32531;&#35299;&#65288;PBM&#65289;&#25216;&#26415;&#65292;&#35299;&#20915;&#20102;&#22914;&#20309;&#22312;&#20013;&#24615;&#25991;&#26412;&#26597;&#35810;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20844;&#24179;&#30340;&#22270;&#20687;&#26816;&#32034;&#12290;&#35813;&#26041;&#27861;&#22312;&#23454;&#38469;&#25968;&#25454;&#38598;&#20013;&#23454;&#29616;&#20102;&#26368;&#20302;&#30340;&#20559;&#24046;&#12290;</title><link>http://arxiv.org/abs/2305.19329</link><description>&lt;p&gt;
&#32531;&#35299;&#27979;&#35797;&#26102;&#38388;&#20559;&#24046;&#65292;&#23454;&#29616;&#20844;&#24179;&#30340;&#22270;&#20687;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Mitigating Test-Time Bias for Fair Image Retrieval. (arXiv:2305.19329v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19329
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21518;&#32622;&#20559;&#24046;&#32531;&#35299;&#65288;PBM&#65289;&#25216;&#26415;&#65292;&#35299;&#20915;&#20102;&#22914;&#20309;&#22312;&#20013;&#24615;&#25991;&#26412;&#26597;&#35810;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20844;&#24179;&#30340;&#22270;&#20687;&#26816;&#32034;&#12290;&#35813;&#26041;&#27861;&#22312;&#23454;&#38469;&#25968;&#25454;&#38598;&#20013;&#23454;&#29616;&#20102;&#26368;&#20302;&#30340;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#22914;&#20309;&#22312;&#20013;&#24615;&#25991;&#26412;&#26597;&#35810;&#30340;&#24773;&#20917;&#19979;&#65288;&#27809;&#26377;&#26126;&#30830;&#30340;&#24615;&#21035;&#25110;&#31181;&#26063;&#20869;&#28085;&#65289;&#29983;&#25104;&#20844;&#24179;&#21644;&#26080;&#20559;&#35265;&#30340;&#22270;&#20687;&#26816;&#32034;&#32467;&#26524;&#65292;&#21516;&#26102;&#20445;&#25345;&#24213;&#23618;&#35270;&#35273;&#35821;&#35328;&#65288;VL&#65289;&#27169;&#22411;&#30340;&#25928;&#29992;&#65288;&#24615;&#33021;&#65289;&#30340;&#25361;&#25112;&#12290;&#20808;&#21069;&#30340;&#26041;&#27861;&#26088;&#22312;&#23558;&#22270;&#20687;&#21644;&#25991;&#26412;&#26597;&#35810;&#30340;&#23398;&#20064;&#34920;&#31034;&#19982;&#24615;&#21035;&#21644;&#31181;&#26063;&#29305;&#24449;&#20998;&#31163;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#26041;&#27861;&#19981;&#33021;&#20943;&#36731;&#27979;&#35797;&#38598;&#20013;&#30340;&#20559;&#24046;&#65292;&#20174;&#32780;&#23454;&#29616;&#25152;&#38656;&#30340;&#24179;&#31561;&#34920;&#31034;&#32467;&#26524;&#12290;&#20986;&#20110;&#36825;&#20010;&#21160;&#26426;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#25216;&#26415;&#65292;&#21518;&#32622;&#20559;&#24046;&#32531;&#35299;&#65288;PBM&#65289;&#65292;&#26469;&#21518;&#22788;&#29702;&#39044;&#35757;&#32451;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;&#25105;&#20204;&#22312;&#23454;&#38469;&#22270;&#20687;&#25628;&#32034;&#25968;&#25454;&#38598;Occupation 1&#21644;2&#65292;&#20197;&#21450;&#20004;&#20010;&#22823;&#35268;&#27169;&#30340;&#22270;&#20687;&#25991;&#26412;&#25968;&#25454;&#38598;MS-COCO&#21644;Flickr30k&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#12290;&#19982;&#21508;&#31181;&#29616;&#26377;&#30340;&#20559;&#24046;&#32531;&#35299;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22522;&#20110;&#25991;&#26412;&#30340;&#22270;&#20687;&#26816;&#32034;&#32467;&#26524;&#20013;&#23454;&#29616;&#20102;&#26368;&#20302;&#30340;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
We address the challenge of generating fair and unbiased image retrieval results given neutral textual queries (with no explicit gender or race connotations), while maintaining the utility (performance) of the underlying vision-language (VL) model. Previous methods aim to disentangle learned representations of images and text queries from gender and racial characteristics. However, we show these are inadequate at alleviating bias for the desired equal representation result, as there usually exists test-time bias in the target retrieval set. So motivated, we introduce a straightforward technique, Post-hoc Bias Mitigation (PBM), that post-processes the outputs from the pre-trained vision-language model. We evaluate our algorithm on real-world image search datasets, Occupation 1 and 2, as well as two large-scale image-text datasets, MS-COCO and Flickr30k. Our approach achieves the lowest bias, compared with various existing bias-mitigation methods, in text-based image retrieval result whi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#29305;&#24449;&#22797;&#29992;&#8221;&#30340;&#26694;&#26550;&#65292;&#23427;&#20351;&#29992;&#21333;&#19968;&#30340;&#34920;&#31034;&#31354;&#38388; &#33021;&#22815;&#39640;&#25928;&#26377;&#25928;&#22320;&#23398;&#20064;&#39640;&#36136;&#37327;&#30340;&#29305;&#24449;&#23884;&#20837;&#65292;&#21516;&#26102;&#21306;&#20998;&#19981;&#21516;&#30340;&#20998;&#31867;&#29305;&#24449;&#12290;&#36890;&#36807;&#22312;&#22810;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#21644;&#26032;&#25968;&#25454;&#38598;&#8220;Web-Available Image Search (WAIS)&#8221;&#19978;&#30340;&#27979;&#35797;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.12102</link><description>&lt;p&gt;
&#32479;&#19968;&#23884;&#20837;&#65306;&#38754;&#21521; Web &#35268;&#27169; ML &#31995;&#32479;&#30340;&#32463;&#36807;&#39564;&#35777;&#30340;&#29305;&#24449;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Unified Embedding: Battle-Tested Feature Representations for Web-Scale ML Systems. (arXiv:2305.12102v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12102
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#29305;&#24449;&#22797;&#29992;&#8221;&#30340;&#26694;&#26550;&#65292;&#23427;&#20351;&#29992;&#21333;&#19968;&#30340;&#34920;&#31034;&#31354;&#38388; &#33021;&#22815;&#39640;&#25928;&#26377;&#25928;&#22320;&#23398;&#20064;&#39640;&#36136;&#37327;&#30340;&#29305;&#24449;&#23884;&#20837;&#65292;&#21516;&#26102;&#21306;&#20998;&#19981;&#21516;&#30340;&#20998;&#31867;&#29305;&#24449;&#12290;&#36890;&#36807;&#22312;&#22810;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#21644;&#26032;&#25968;&#25454;&#38598;&#8220;Web-Available Image Search (WAIS)&#8221;&#19978;&#30340;&#27979;&#35797;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#12289;&#26377;&#25928;&#22320;&#23398;&#20064;&#39640;&#36136;&#37327;&#30340;&#29305;&#24449;&#23884;&#20837;&#23545;&#20110; Web &#35268;&#27169;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#26631;&#20934;&#26041;&#27861;&#26159;&#23558;&#27599;&#20010;&#29305;&#24449;&#20540;&#34920;&#31034;&#20026;&#19968;&#20010; d &#32500;&#23884;&#20837;&#65292;&#24341;&#20837;&#25968;&#30334;&#20159;&#20010;&#21442;&#25968;&#65292;&#32780;&#36825;&#20123;&#29305;&#24449;&#30340;&#22522;&#25968;&#38750;&#24120;&#39640;&#12290;&#36825;&#20010;&#29942;&#39048;&#23548;&#33268;&#20102;&#22791;&#36873;&#23884;&#20837;&#31639;&#27861;&#30340;&#37325;&#22823;&#36827;&#23637;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#38750;&#24120;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;&#21363;&#8220;&#29305;&#24449;&#22797;&#29992;&#8221;&#65292;&#22312;&#35768;&#22810;&#19981;&#21516;&#30340;&#20998;&#31867;&#29305;&#24449;&#20043;&#38388;&#20351;&#29992;&#19968;&#20010;&#21333;&#19968;&#30340;&#34920;&#31034;&#31354;&#38388;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#21644;&#23454;&#35777;&#20998;&#26512;&#34920;&#26126;&#65292;&#22797;&#29992;&#30340;&#23884;&#20837;&#21487;&#20197;&#20998;&#35299;&#20026;&#27599;&#20010;&#32452;&#25104;&#29305;&#24449;&#30340;&#32452;&#20214;&#65292;&#20351;&#24471;&#27169;&#22411;&#21487;&#20197;&#21306;&#20998;&#29305;&#24449;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22797;&#29992;&#30340;&#23884;&#20837;&#22312;&#20960;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;Web-Available Image Search (WAIS)&#8221;&#30340;&#26032;&#25968;&#25454;&#38598;&#65292;&#20197;&#20005;&#26684;&#35780;&#20272; Web &#35268;&#27169;&#19979;&#30340;&#26032;&#23884;&#20837;&#31639;&#27861;&#12290;&#25105;&#20204;&#36992;&#35831;&#31038;&#21306;&#36890;&#36807;&#25552;&#20986;&#21487;&#20197;&#20934;&#30830;&#12289;&#39640;&#25928;&#22320;&#23558;&#25968;&#30334;&#19975;&#24352;&#22270;&#20687;&#23884;&#20837;&#21644;&#20998;&#31867;&#21040;&#25104;&#21315;&#19978;&#19975;&#20010;&#31867;&#21035;&#30340;&#26032;&#27169;&#22411;&#26469;&#36129;&#29486; WAIS &#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning high-quality feature embeddings efficiently and effectively is critical for the performance of web-scale machine learning systems. A typical model ingests hundreds of features with vocabularies on the order of millions to billions of tokens. The standard approach is to represent each feature value as a d-dimensional embedding, introducing hundreds of billions of parameters for extremely high-cardinality features. This bottleneck has led to substantial progress in alternative embedding algorithms. Many of these methods, however, make the assumption that each feature uses an independent embedding table. This work introduces a simple yet highly effective framework, Feature Multiplexing, where one single representation space is used across many different categorical features. Our theoretical and empirical analysis reveals that multiplexed embeddings can be decomposed into components from each constituent feature, allowing models to distinguish between features. We show that multip
&lt;/p&gt;</description></item><item><title>EvalRS 2023&#26088;&#22312;&#25506;&#35752;&#25512;&#33616;&#31995;&#32479;&#30340;&#20840;&#38754;&#35780;&#20272;&#65292;&#20851;&#27880;&#20854;&#22312;&#29616;&#23454;&#22330;&#26223;&#19979;&#30340;&#23454;&#38469;&#24433;&#21709;&#12290;&#36807;&#21435;&#21482;&#26377;&#20934;&#30830;&#24230;&#30340;&#27979;&#37327;&#26041;&#27861;&#21487;&#33021;&#26080;&#27861;&#20840;&#38754;&#35780;&#20272;&#20854;&#24615;&#33021;&#65292;&#20844;&#24179;&#24615;&#12289;&#20559;&#35265;&#12289;&#26377;&#29992;&#24615;&#21644;&#20449;&#24687;&#37327;&#31561;&#26041;&#38754;&#20063;&#24212;&#35813;&#34987;&#20851;&#27880;&#12290;&#26412;&#27425;&#30740;&#35752;&#20250;&#26159;&#21435;&#24180;CIKM&#30740;&#35752;&#20250;&#30340;&#32487;&#25215;&#21644;&#21457;&#23637;&#65292;&#24102;&#26377;&#23454;&#38469;&#25805;&#20316;&#24615;&#21644;&#20114;&#21160;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.07145</link><description>&lt;p&gt;
EvalRS 2023. &#38754;&#21521;&#23454;&#38469;&#24212;&#29992;&#30340;&#20840;&#38754;&#25512;&#33616;&#31995;&#32479;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
EvalRS 2023. Well-Rounded Recommender Systems For Real-World Deployments. (arXiv:2304.07145v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07145
&lt;/p&gt;
&lt;p&gt;
EvalRS 2023&#26088;&#22312;&#25506;&#35752;&#25512;&#33616;&#31995;&#32479;&#30340;&#20840;&#38754;&#35780;&#20272;&#65292;&#20851;&#27880;&#20854;&#22312;&#29616;&#23454;&#22330;&#26223;&#19979;&#30340;&#23454;&#38469;&#24433;&#21709;&#12290;&#36807;&#21435;&#21482;&#26377;&#20934;&#30830;&#24230;&#30340;&#27979;&#37327;&#26041;&#27861;&#21487;&#33021;&#26080;&#27861;&#20840;&#38754;&#35780;&#20272;&#20854;&#24615;&#33021;&#65292;&#20844;&#24179;&#24615;&#12289;&#20559;&#35265;&#12289;&#26377;&#29992;&#24615;&#21644;&#20449;&#24687;&#37327;&#31561;&#26041;&#38754;&#20063;&#24212;&#35813;&#34987;&#20851;&#27880;&#12290;&#26412;&#27425;&#30740;&#35752;&#20250;&#26159;&#21435;&#24180;CIKM&#30740;&#35752;&#20250;&#30340;&#32487;&#25215;&#21644;&#21457;&#23637;&#65292;&#24102;&#26377;&#23454;&#38469;&#25805;&#20316;&#24615;&#21644;&#20114;&#21160;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
EvalRS&#26088;&#22312;&#27719;&#32858;&#20135;&#19994;&#21644;&#23398;&#26415;&#30028;&#30340;&#20174;&#19994;&#32773;&#65292;&#20419;&#36827;&#23545;&#25512;&#33616;&#31995;&#32479;&#30340;&#20840;&#38754;&#35780;&#20272;&#30340;&#35752;&#35770;&#65292;&#24182;&#37325;&#28857;&#20851;&#27880;&#22312;&#21508;&#31181;&#37096;&#32626;&#22330;&#26223;&#19979;&#30340;&#23454;&#38469;&#24433;&#21709;&#12290;&#25512;&#33616;&#31995;&#32479;&#36890;&#24120;&#21482;&#36890;&#36807;&#20934;&#30830;&#24615;&#25351;&#26631;&#36827;&#34892;&#35780;&#20272;&#65292;&#36825;&#20123;&#25351;&#26631;&#26080;&#27861;&#23436;&#20840;&#25551;&#36848;&#20854;&#27867;&#21270;&#33021;&#21147;&#24182;&#24573;&#35270;&#20102;&#37325;&#35201;&#30340;&#26041;&#38754;&#65292;&#22914;&#20844;&#24179;&#24615;&#12289;&#20559;&#35265;&#12289;&#26377;&#29992;&#24615;&#12289;&#20449;&#24687;&#37327;&#31561;&#12290;&#26412;&#27425;&#30740;&#35752;&#20250;&#22312;CIKM&#21435;&#24180;&#30740;&#35752;&#20250;&#30340;&#25104;&#21151;&#22522;&#30784;&#19978;&#36827;&#19968;&#27493;&#25193;&#22823;&#33539;&#22260;&#65292;&#24182;&#37319;&#21462;&#20114;&#21160;&#24418;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
EvalRS aims to bring together practitioners from industry and academia to foster a debate on rounded evaluation of recommender systems, with a focus on real-world impact across a multitude of deployment scenarios. Recommender systems are often evaluated only through accuracy metrics, which fall short of fully characterizing their generalization capabilities and miss important aspects, such as fairness, bias, usefulness, informativeness. This workshop builds on the success of last year's workshop at CIKM, but with a broader scope and an interactive format.
&lt;/p&gt;</description></item><item><title>RARR&#26159;&#19968;&#20010;&#21487;&#20197;&#23545;&#19981;&#30830;&#23450;&#20449;&#24687;&#36827;&#34892;&#30740;&#31350;&#21644;&#20462;&#35746;&#30340;&#31995;&#32479;&#65292;&#23427;&#21487;&#20197;&#33258;&#21160;&#25214;&#21040;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#36755;&#20986;&#30340;&#24402;&#22240;&#24182;&#20462;&#27491;&#19981;&#25903;&#25345;&#30340;&#20869;&#23481;&#12290;</title><link>http://arxiv.org/abs/2210.08726</link><description>&lt;p&gt;
RARR: &#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30740;&#31350;&#21644;&#20462;&#27491;&#20854;&#36755;&#20986;&#32467;&#26524;&#20013;&#30340;&#19981;&#30830;&#23450;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
RARR: Researching and Revising What Language Models Say, Using Language Models. (arXiv:2210.08726v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.08726
&lt;/p&gt;
&lt;p&gt;
RARR&#26159;&#19968;&#20010;&#21487;&#20197;&#23545;&#19981;&#30830;&#23450;&#20449;&#24687;&#36827;&#34892;&#30740;&#31350;&#21644;&#20462;&#35746;&#30340;&#31995;&#32479;&#65292;&#23427;&#21487;&#20197;&#33258;&#21160;&#25214;&#21040;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#36755;&#20986;&#30340;&#24402;&#22240;&#24182;&#20462;&#27491;&#19981;&#25903;&#25345;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#22312;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#35832;&#22914;&#23569;&#26679;&#26412;&#23398;&#20064;&#12289;&#38382;&#31572;&#12289;&#25512;&#29702;&#21644;&#23545;&#35805;&#31561;&#35768;&#22810;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#26377;&#26102;&#20250;&#29983;&#25104;&#26080;&#25903;&#25345;&#25110;&#35823;&#23548;&#24615;&#30340;&#20869;&#23481;&#12290;&#30001;&#20110;&#22823;&#22810;&#25968;&#35821;&#35328;&#27169;&#22411;&#27809;&#26377;&#20219;&#20309;&#20869;&#32622;&#30340;&#24402;&#22240;&#22806;&#37096;&#35777;&#25454;&#30340;&#26426;&#21046;&#65292;&#29992;&#25143;&#24456;&#38590;&#30830;&#23450;&#23427;&#20204;&#30340;&#36755;&#20986;&#26159;&#21542;&#21487;&#38752;&#12290;&#20026;&#20102;&#22312;&#20445;&#30041;&#26368;&#26032;&#19968;&#20195;&#27169;&#22411;&#30340;&#25152;&#26377;&#24378;&#22823;&#20248;&#21183;&#30340;&#21516;&#26102;&#23454;&#29616;&#24402;&#22240;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; RARR (&#20351;&#29992;&#30740;&#31350;&#21644;&#20462;&#35746;&#36827;&#34892;&#25913;&#36827;&#24402;&#22240;)&#31995;&#32479;&#65292;&#23427; 1) &#33258;&#21160;&#25214;&#21040;&#20219;&#20309;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#36755;&#20986;&#30340;&#24402;&#22240;&#24182; 2) &#22312;&#23613;&#21487;&#33021;&#20445;&#30041;&#21407;&#22987;&#36755;&#20986;&#30340;&#21516;&#26102;&#65292;&#20462;&#27491;&#19981;&#25903;&#25345;&#30340;&#20869;&#23481;&#12290;&#24403;&#24212;&#29992;&#20110;&#20960;&#20010;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#36755;&#20986;&#20219;&#21153;&#19978;&#30340;&#32467;&#26524;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;RARR&#22312;&#26174;&#33879;&#25552;&#39640;&#24402;&#22240;&#29575;&#30340;&#21516;&#26102;&#65292;&#27604;&#20197;&#21069;&#25506;&#32034;&#30340;&#32534;&#36753;&#27169;&#22411;&#26356;&#33021;&#20445;&#30041;&#21407;&#22987;&#36755;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models (LMs) now excel at many tasks such as few-shot learning, question answering, reasoning, and dialog. However, they sometimes generate unsupported or misleading content. A user cannot easily determine whether their outputs are trustworthy or not, because most LMs do not have any built-in mechanism for attribution to external evidence. To enable attribution while still preserving all the powerful advantages of recent generation models, we propose RARR (Retrofit Attribution using Research and Revision), a system that 1) automatically finds attribution for the output of any text generation model and 2) post-edits the output to fix unsupported content while preserving the original output as much as possible. When applied to the output of several state-of-the-art LMs on a diverse set of generation tasks, we find that RARR significantly improves attribution while otherwise preserving the original input to a much greater degree than previously explored edit models. Furthermore, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#21464;&#20998;&#24320;&#25918;&#39046;&#22495;&#65288;VOD&#65289;&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#24402;&#19968;&#21270;&#30340;R&#233;nyi&#21464;&#20998;&#30028;&#30340;&#20272;&#35745;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#35757;&#32451;&#20855;&#26377;&#26816;&#32034;&#22686;&#24378;&#21151;&#33021;&#30340;&#27169;&#22411;&#65292;&#20363;&#22914;&#38405;&#35835;&#22120;-&#26816;&#32034;&#22120;BERT-sized&#27169;&#22411;&#65292;&#24182;&#23454;&#29616;&#20102;&#22312;&#22810;&#39033;&#36873;&#25321;&#21307;&#23398;&#32771;&#35797;&#38382;&#39064;&#19978;&#30340;&#20248;&#24322;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2210.06345</link><description>&lt;p&gt;
&#21464;&#20998;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Variational Open-Domain Question Answering. (arXiv:2210.06345v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.06345
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#21464;&#20998;&#24320;&#25918;&#39046;&#22495;&#65288;VOD&#65289;&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#24402;&#19968;&#21270;&#30340;R&#233;nyi&#21464;&#20998;&#30028;&#30340;&#20272;&#35745;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#35757;&#32451;&#20855;&#26377;&#26816;&#32034;&#22686;&#24378;&#21151;&#33021;&#30340;&#27169;&#22411;&#65292;&#20363;&#22914;&#38405;&#35835;&#22120;-&#26816;&#32034;&#22120;BERT-sized&#27169;&#22411;&#65292;&#24182;&#23454;&#29616;&#20102;&#22312;&#22810;&#39033;&#36873;&#25321;&#21307;&#23398;&#32771;&#35797;&#38382;&#39064;&#19978;&#30340;&#20248;&#24322;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#24050;&#34987;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#65292;&#20294;&#26159;&#23545;&#23427;&#20204;&#36827;&#34892;&#21464;&#20998;&#25512;&#26029;&#30340;&#20248;&#21270;&#30740;&#31350;&#20173;&#28982;&#19981;&#36275;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#21464;&#20998;&#24320;&#25918;&#39046;&#22495;&#65288;VOD&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#26816;&#32034;&#22686;&#24378;&#27169;&#22411;&#30340;&#31471;&#21040;&#31471;&#35757;&#32451;&#21644;&#35780;&#20272;&#65292;&#37325;&#28857;&#25918;&#22312;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#21644;&#35821;&#35328;&#24314;&#27169;&#26041;&#38754;&#12290;VOD&#30446;&#26631;&#26159;&#19968;&#31181;&#33258;&#24402;&#19968;&#21270;&#30340;R&#233;nyi&#21464;&#20998;&#30028;&#30340;&#20272;&#35745;&#65292;&#36817;&#20284;&#20110;&#20219;&#21153;&#36793;&#32536;&#20284;&#28982;&#65292;&#24182;&#22312;&#19968;&#20010;&#36741;&#21161;&#37319;&#26679;&#20998;&#24067;&#65288;&#32531;&#23384;&#30340;&#26816;&#32034;&#22120;&#21644;/&#25110;&#36817;&#20284;&#21518;&#39564;&#65289;&#20013;&#36827;&#34892;&#26679;&#26412;&#25277;&#21462;&#35780;&#20272;&#12290;&#23427;&#20173;&#28982;&#26159;&#21487;&#34892;&#30340;&#65292;&#21363;&#20351;&#26159;&#22312;&#23545;&#22823;&#37327;&#35821;&#26009;&#24211;&#23450;&#20041;&#30340;&#26816;&#32034;&#22120;&#20998;&#24067;&#19979;&#12290;&#25105;&#20204;&#36890;&#36807;&#35757;&#32451;&#38024;&#23545;&#22810;&#39033;&#36873;&#25321;&#21307;&#23398;&#32771;&#35797;&#38382;&#39064;&#30340;&#38405;&#35835;&#22120;-&#26816;&#32034;&#22120;BERT-sized&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;VOD&#30340;&#22810;&#21151;&#33021;&#24615;&#12290;&#22312;MedMCQA&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#36229;&#36807;&#20102;&#39046;&#22495;&#24494;&#35843;&#30340;Med-PaLM 5.3&#65285;&#65292;&#23613;&#31649;&#20351;&#29992;&#30340;&#21442;&#25968;&#23569;&#20102;2500&#20493;&#12290;&#25105;&#20204;&#30340;&#26816;&#32034;&#22686;&#24378;BioLinkBERT&#27169;&#22411;&#24471;&#20998;&#20026;62.9&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Retrieval-augmented models have proven to be effective in natural language processing tasks, yet there remains a lack of research on their optimization using variational inference. We introduce the Variational Open-Domain (VOD) framework for end-to-end training and evaluation of retrieval-augmented models, focusing on open-domain question answering and language modelling. The VOD objective, a self-normalized estimate of the R\'enyi variational bound, approximates the task marginal likelihood and is evaluated under samples drawn from an auxiliary sampling distribution (cached retriever and/or approximate posterior). It remains tractable, even for retriever distributions defined on large corpora. We demonstrate VOD's versatility by training reader-retriever BERT-sized models on multiple-choice medical exam questions. On the MedMCQA dataset, we outperform the domain-tuned Med-PaLM by +5.3% despite using 2.500$\times$ fewer parameters. Our retrieval-augmented BioLinkBERT model scored 62.9%
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;CONE&#65292;&#19968;&#20010;&#39640;&#25928;&#30340;&#31895;-&#32454;&#23545;&#40784;&#26694;&#26550;&#65292;&#21487;&#29992;&#20110;&#38271;&#35270;&#39057;&#26102;&#38388;&#23450;&#20301;&#12290;CONE&#36890;&#36807;&#22522;&#20110;&#26597;&#35810;&#30340;&#31383;&#21475;&#36873;&#25321;&#31574;&#30053;&#21644;&#23545;&#27604;&#23398;&#20064;&#26426;&#21046;&#25552;&#21319;&#20102;&#22810;&#27169;&#24577;&#23545;&#40784;&#65292;&#24182;&#22312;&#20004;&#20010;&#22823;&#35268;&#27169;&#38271;&#35270;&#39057;&#26102;&#38388;&#23450;&#20301;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2209.10918</link><description>&lt;p&gt;
CONE&#65306;&#29992;&#20110;&#38271;&#35270;&#39057;&#26102;&#38388;&#23450;&#20301;&#30340;&#39640;&#25928;&#31895;-&#32454;&#23545;&#40784;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
CONE: An Efficient COarse-to-fiNE Alignment Framework for Long Video Temporal Grounding. (arXiv:2209.10918v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.10918
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;CONE&#65292;&#19968;&#20010;&#39640;&#25928;&#30340;&#31895;-&#32454;&#23545;&#40784;&#26694;&#26550;&#65292;&#21487;&#29992;&#20110;&#38271;&#35270;&#39057;&#26102;&#38388;&#23450;&#20301;&#12290;CONE&#36890;&#36807;&#22522;&#20110;&#26597;&#35810;&#30340;&#31383;&#21475;&#36873;&#25321;&#31574;&#30053;&#21644;&#23545;&#27604;&#23398;&#20064;&#26426;&#21046;&#25552;&#21319;&#20102;&#22810;&#27169;&#24577;&#23545;&#40784;&#65292;&#24182;&#22312;&#20004;&#20010;&#22823;&#35268;&#27169;&#38271;&#35270;&#39057;&#26102;&#38388;&#23450;&#20301;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#19968;&#20010;&#26032;&#20852;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#8212;&#8212;&#38271;&#35270;&#39057;&#26102;&#38388;&#23450;&#20301;&#65288;VTG&#65289;&#65292;&#21363;&#23450;&#20301;&#19982;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#30456;&#20851;&#30340;&#35270;&#39057;&#29255;&#27573;&#12290;&#30456;&#27604;&#20110;&#30701;&#35270;&#39057;&#65292;&#38271;&#35270;&#39057;&#21516;&#26679;&#38750;&#24120;&#21463;&#27426;&#36814;&#65292;&#20294;&#26159;&#25506;&#32034;&#36739;&#23569;&#65292;&#36825;&#24102;&#26469;&#20102;&#22810;&#20010;&#25361;&#25112;&#65292;&#20363;&#22914;&#26356;&#39640;&#30340;&#25512;&#29702;&#35745;&#31639;&#25104;&#26412;&#21644;&#24369;&#30340;&#22810;&#27169;&#24577;&#23545;&#40784;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CONE&#65292;&#19968;&#20010;&#39640;&#25928;&#30340;&#31895;-&#32454;&#23545;&#40784;&#26694;&#26550;&#12290;CONE&#26159;&#19968;&#20010;&#25554;&#25300;&#24335;&#30340;&#26694;&#26550;&#65292;&#21487;&#22312;&#29616;&#26377;&#30340;VTG&#27169;&#22411;&#19978;&#22788;&#29702;&#38271;&#35270;&#39057;&#65292;&#36890;&#36807;&#28369;&#21160;&#31383;&#21475;&#26426;&#21046;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;CONE&#65288;1&#65289;&#24341;&#20837;&#20102;&#22522;&#20110;&#26597;&#35810;&#30340;&#31383;&#21475;&#36873;&#25321;&#31574;&#30053;&#20197;&#21152;&#24555;&#25512;&#29702;&#36895;&#24230;&#65292;&#65288;2&#65289;&#25552;&#35758;&#20102;&#36890;&#36807;&#26032;&#22686;&#23545;&#27604;&#23398;&#20064;&#26469;&#22686;&#24378;&#38271;&#35270;&#39057;&#30340;&#22810;&#27169;&#24577;&#23545;&#40784;&#30340;&#31895;&#32454;&#26426;&#21046;&#12290;&#23545;&#20004;&#20010;&#22823;&#35268;&#27169;&#38271;VTG&#22522;&#20934;&#27979;&#35797;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#22343;&#34920;&#26126;&#65292;CONE&#22312;&#24615;&#33021;&#19978;&#37117;&#26377;&#24456;&#22823;&#25552;&#21319;&#65288;&#20363;&#22914;&#22312;MAD&#19978;&#20174;3.13&#65285;&#21040;6.87&#65285;&#65289;&#65292;&#24182;&#19988;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#20998;&#26512;&#20063;&#35777;&#26126;&#20102;CONE&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper tackles an emerging and challenging problem of long video temporal grounding~(VTG) that localizes video moments related to a natural language (NL) query. Compared with short videos, long videos are also highly demanded but less explored, which brings new challenges in higher inference computation cost and weaker multi-modal alignment. To address these challenges, we propose CONE, an efficient COarse-to-fiNE alignment framework. CONE is a plug-and-play framework on top of existing VTG models to handle long videos through a sliding window mechanism. Specifically, CONE (1) introduces a query-guided window selection strategy to speed up inference, and (2) proposes a coarse-to-fine mechanism via a novel incorporation of contrastive learning to enhance multi-modal alignment for long videos. Extensive experiments on two large-scale long VTG benchmarks consistently show both substantial performance gains (e.g., from 3.13% to 6.87% on MAD) and state-of-the-art results. Analyses also 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#25991;&#31456;&#25506;&#35752;&#20102;&#22522;&#20110;&#20250;&#35805;&#30340;&#25512;&#33616;&#31995;&#32479;&#22312;&#20934;&#30830;&#24230;&#21644;&#22810;&#26679;&#24615;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#24182;&#25552;&#20379;&#20102;&#35774;&#35745;&#22810;&#26679;&#21270;SBRS&#30340;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2208.13453</link><description>&lt;p&gt;
&#29702;&#35299;&#22522;&#20110;&#20250;&#35805;&#30340;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#22810;&#26679;&#24615;
&lt;/p&gt;
&lt;p&gt;
Understanding Diversity in Session-Based Recommendation. (arXiv:2208.13453v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.13453
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#25991;&#31456;&#25506;&#35752;&#20102;&#22522;&#20110;&#20250;&#35805;&#30340;&#25512;&#33616;&#31995;&#32479;&#22312;&#20934;&#30830;&#24230;&#21644;&#22810;&#26679;&#24615;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#24182;&#25552;&#20379;&#20102;&#35774;&#35745;&#22810;&#26679;&#21270;SBRS&#30340;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#22522;&#20110;&#20250;&#35805;&#30340;&#25512;&#33616;&#31995;&#32479;&#65288;SBRSs&#65289;&#20027;&#35201;&#20851;&#27880;&#20110;&#26368;&#22823;&#21270;&#25512;&#33616;&#20934;&#30830;&#24230;&#65292;&#32780;&#24456;&#23569;&#26377;&#30740;&#31350;&#19987;&#38376;&#33268;&#21147;&#20110;&#22312;&#20934;&#30830;&#24230;&#20043;&#22806;&#25552;&#39640;&#22810;&#26679;&#24615;&#12290;&#21516;&#26102;&#65292;&#19981;&#28165;&#26970;&#20197;&#20934;&#30830;&#24230;&#20026;&#23548;&#21521;&#30340;SBRS&#22312;&#22810;&#26679;&#24615;&#26041;&#38754;&#30340;&#34920;&#29616;&#22914;&#20309;&#12290;&#27492;&#22806;&#65292;&#22312;&#25991;&#29486;&#20013;&#36234;&#26469;&#36234;&#22810;&#22320;&#36136;&#30097;&#8220;&#20934;&#30830;&#24230;&#8221;&#19982;&#8220;&#22810;&#26679;&#24615;&#8221;&#20043;&#38388;&#30340;&#8220;&#26435;&#34913;&#8221;&#20851;&#31995;&#12290;&#38024;&#23545;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#20840;&#38754;&#30340;&#30740;&#31350;&#65292;&#29305;&#21035;&#26159;&#32771;&#23519;&#20855;&#26377;&#20195;&#34920;&#24615;&#30340;&#22522;&#20110;&#20250;&#35805;&#30340;&#25512;&#33616;&#31995;&#32479;&#22312;&#20934;&#30830;&#24230;&#21644;&#22810;&#26679;&#24615;&#26041;&#38754;&#30340;&#25512;&#33616;&#24615;&#33021;&#65292;&#21162;&#21147;&#26356;&#22909;&#22320;&#29702;&#35299;SBRS&#30340;&#22810;&#26679;&#24615;&#30456;&#20851;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#35774;&#35745;&#22810;&#26679;&#21270;SBRS&#30340;&#25351;&#23548;&#12290;&#29305;&#21035;&#22320;&#65292;&#20026;&#20102;&#36827;&#34892;&#20844;&#24179;&#21644;&#20840;&#38754;&#30340;&#27604;&#36739;&#65292;&#25105;&#20204;&#31934;&#24515;&#36873;&#25321;&#20102;&#26368;&#20808;&#36827;&#30340;&#38750;&#31070;&#32463;&#65292;&#28145;&#24230;&#31070;&#32463;&#21644;&#22810;&#26679;&#21270;SBRS&#65292;&#36890;&#36807;&#35206;&#30422;&#26356;&#22810;&#20855;&#26377;&#36866;&#24403;&#23454;&#39564;&#35774;&#32622;&#30340;&#22330;&#26223;&#65292;&#20363;&#22914;&#20195;&#34920;&#24615;&#25968;&#25454;&#38598;&#65292;&#35780;&#20272;&#25351;&#26631;&#21644;&#36229;&#21442;&#25968;&#20248;&#21270;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current session-based recommender systems (SBRSs) mainly focus on maximizing recommendation accuracy, while few studies have been devoted to improve diversity beyond accuracy. Meanwhile, it is unclear how the accuracy-oriented SBRSs perform in terms of diversity. Besides, the asserted "trade-off" relationship between accuracy and diversity has been increasingly questioned in the literature. Towards the aforementioned issues, we conduct a holistic study to particularly examine the recommendation performance of representative SBRSs w.r.t. both accuracy and diversity, striving for better understanding the diversity-related issues for SBRSs and providing guidance on designing diversified SBRSs. Particularly, for a fair and thorough comparison, we deliberately select state-of-the-art non-neural, deep neural, and diversified SBRSs, by covering more scenarios with appropriate experimental setups, e.g., representative datasets, evaluation metrics, and hyper-parameter optimization technique. Ou
&lt;/p&gt;</description></item></channel></rss>