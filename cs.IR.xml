<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20004;&#22612;&#25512;&#33616;&#27169;&#22411;&#20013;&#20351;&#29992;&#21333;&#27425;&#21453;&#21521;&#20256;&#25773;&#26356;&#26032;&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#25361;&#25112;&#20102;&#29616;&#26377;&#31639;&#27861;&#20013;&#24179;&#31561;&#23545;&#24453;&#29992;&#25143;&#21644;&#29289;&#21697;&#30340;&#20551;&#35774;&#12290;</title><link>https://arxiv.org/abs/2403.18227</link><description>&lt;p&gt;
&#20004;&#22612;&#25512;&#33616;&#27169;&#22411;&#20013;&#30340;&#21333;&#27425;&#21453;&#21521;&#20256;&#25773;
&lt;/p&gt;
&lt;p&gt;
One Backpropagation in Two Tower Recommendation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18227
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20004;&#22612;&#25512;&#33616;&#27169;&#22411;&#20013;&#20351;&#29992;&#21333;&#27425;&#21453;&#21521;&#20256;&#25773;&#26356;&#26032;&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#25361;&#25112;&#20102;&#29616;&#26377;&#31639;&#27861;&#20013;&#24179;&#31561;&#23545;&#24453;&#29992;&#25143;&#21644;&#29289;&#21697;&#30340;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#24050;&#32463;&#30475;&#21040;&#20026;&#20102;&#20943;&#36731;&#20449;&#24687;&#36807;&#36733;&#32780;&#24320;&#21457;&#20004;&#22612;&#25512;&#33616;&#27169;&#22411;&#30340;&#24191;&#27867;&#30740;&#31350;&#12290;&#36825;&#31181;&#27169;&#22411;&#20013;&#21487;&#20197;&#35782;&#21035;&#20986;&#22235;&#20010;&#26500;&#24314;&#27169;&#22359;&#65292;&#20998;&#21035;&#26159;&#29992;&#25143;-&#29289;&#21697;&#32534;&#30721;&#12289;&#36127;&#37319;&#26679;&#12289;&#25439;&#22833;&#35745;&#31639;&#21644;&#21453;&#21521;&#20256;&#25773;&#26356;&#26032;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#29616;&#26377;&#31639;&#27861;&#20165;&#30740;&#31350;&#20102;&#21069;&#19977;&#20010;&#27169;&#22359;&#65292;&#21364;&#24573;&#30053;&#20102;&#21453;&#21521;&#20256;&#25773;&#27169;&#22359;&#12290;&#20182;&#20204;&#37117;&#37319;&#29992;&#26576;&#31181;&#24418;&#24335;&#30340;&#21452;&#21453;&#21521;&#20256;&#25773;&#31574;&#30053;&#65292;&#22522;&#20110;&#19968;&#20010;&#38544;&#21547;&#30340;&#20551;&#35774;&#65292;&#21363;&#22312;&#35757;&#32451;&#38454;&#27573;&#24179;&#31561;&#23545;&#24453;&#29992;&#25143;&#21644;&#29289;&#21697;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25361;&#25112;&#20102;&#36825;&#31181;&#24179;&#31561;&#35757;&#32451;&#20551;&#35774;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21333;&#27425;&#21453;&#21521;&#20256;&#25773;&#26356;&#26032;&#31574;&#30053;&#65292;&#36825;&#31181;&#31574;&#30053;&#20445;&#30041;&#20102;&#29289;&#21697;&#32534;&#30721;&#22612;&#30340;&#27491;&#24120;&#26799;&#24230;&#21453;&#21521;&#20256;&#25773;&#65292;&#20294;&#21066;&#20943;&#20102;&#29992;&#25143;&#32534;&#30721;&#22612;&#30340;&#21453;&#21521;&#20256;&#25773;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31227;&#21160;&#32858;&#21512;&#26356;&#26032;&#31574;&#30053;&#26469;&#26356;&#26032;&#27599;&#20010;&#35757;&#32451;&#21608;&#26399;&#20013;&#30340;&#29992;&#25143;&#32534;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18227v1 Announce Type: new  Abstract: Recent years have witnessed extensive researches on developing two tower recommendation models for relieving information overload. Four building modules can be identified in such models, namely, user-item encoding, negative sampling, loss computing and back-propagation updating. To the best of our knowledge, existing algorithms have researched only on the first three modules, yet neglecting the backpropagation module. They all adopt a kind of two backpropagation strategy, which are based on an implicit assumption of equally treating users and items in the training phase. In this paper, we challenge such an equal training assumption and propose a novel one backpropagation updating strategy, which keeps the normal gradient backpropagation for the item encoding tower, but cuts off the backpropagation for the user encoding tower. Instead, we propose a moving-aggregation updating strategy to update a user encoding in each training epoch. Exce
&lt;/p&gt;</description></item><item><title>&#21457;&#24067;&#20102;IEPile&#65292;&#19968;&#20010;&#21253;&#21547;&#32422;0.32B&#20010;&#26631;&#35760;&#30340;&#32508;&#21512;&#21452;&#35821;IE&#25351;&#20196;&#35821;&#26009;&#24211;&#65292;&#36890;&#36807;&#25910;&#38598;&#21644;&#28165;&#29702;33&#20010;&#29616;&#26377;IE&#25968;&#25454;&#38598;&#24182;&#24341;&#20837;&#22522;&#20110;&#27169;&#24335;&#30340;&#25351;&#20196;&#29983;&#25104;&#65292;&#21487;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20449;&#24687;&#25277;&#21462;&#39046;&#22495;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#38646;&#26679;&#26412;&#27867;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.14710</link><description>&lt;p&gt;
IEPile: &#25366;&#25496;&#22823;&#35268;&#27169;&#22522;&#20110;&#27169;&#24335;&#30340;&#20449;&#24687;&#25277;&#21462;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
IEPile: Unearthing Large-Scale Schema-Based Information Extraction Corpus
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14710
&lt;/p&gt;
&lt;p&gt;
&#21457;&#24067;&#20102;IEPile&#65292;&#19968;&#20010;&#21253;&#21547;&#32422;0.32B&#20010;&#26631;&#35760;&#30340;&#32508;&#21512;&#21452;&#35821;IE&#25351;&#20196;&#35821;&#26009;&#24211;&#65292;&#36890;&#36807;&#25910;&#38598;&#21644;&#28165;&#29702;33&#20010;&#29616;&#26377;IE&#25968;&#25454;&#38598;&#24182;&#24341;&#20837;&#22522;&#20110;&#27169;&#24335;&#30340;&#25351;&#20196;&#29983;&#25104;&#65292;&#21487;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20449;&#24687;&#25277;&#21462;&#39046;&#22495;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#38646;&#26679;&#26412;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#23637;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#28508;&#21147;&#65307;&#28982;&#32780;&#65292;&#22312;&#20449;&#24687;&#25277;&#21462;&#65288;IE&#65289;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#25968;&#25454;&#26159;&#25552;&#21319;LLMs&#29305;&#23450;&#33021;&#21147;&#30340;&#20851;&#38190;&#65292;&#32780;&#24403;&#21069;&#30340;IE&#25968;&#25454;&#38598;&#24448;&#24448;&#35268;&#27169;&#36739;&#23567;&#12289;&#20998;&#25955;&#19988;&#32570;&#20047;&#26631;&#20934;&#21270;&#30340;&#27169;&#24335;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;IEPile&#65292;&#19968;&#20010;&#32508;&#21512;&#30340;&#21452;&#35821;&#65288;&#33521;&#25991;&#21644;&#20013;&#25991;&#65289;IE&#25351;&#20196;&#35821;&#26009;&#24211;&#65292;&#21253;&#21547;&#32422;0.32B&#20010;&#26631;&#35760;&#12290;&#25105;&#20204;&#36890;&#36807;&#25910;&#38598;&#21644;&#28165;&#29702;33&#20010;&#29616;&#26377;IE&#25968;&#25454;&#38598;&#26500;&#24314;IEPile&#65292;&#24182;&#24341;&#20837;&#22522;&#20110;&#27169;&#24335;&#30340;&#25351;&#20196;&#29983;&#25104;&#26469;&#25366;&#25496;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;&#12290;&#22312;LLaMA&#21644;Baichuan&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;IEPile&#21487;&#20197;&#25552;&#39640;LLMs&#22312;IE&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#38646;&#26679;&#26412;&#27867;&#21270;&#12290;&#25105;&#20204;&#24320;&#28304;&#20102;&#36164;&#28304;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24076;&#26395;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31038;&#21306;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14710v1 Announce Type: cross  Abstract: Large Language Models (LLMs) demonstrate remarkable potential across various domains; however, they exhibit a significant performance gap in Information Extraction (IE). Note that high-quality instruction data is the vital key for enhancing the specific capabilities of LLMs, while current IE datasets tend to be small in scale, fragmented, and lack standardized schema. To this end, we introduce IEPile, a comprehensive bilingual (English and Chinese) IE instruction corpus, which contains approximately 0.32B tokens. We construct IEPile by collecting and cleaning 33 existing IE datasets, and introduce schema-based instruction generation to unearth a large-scale corpus. Experimental results on LLaMA and Baichuan demonstrate that using IEPile can enhance the performance of LLMs for IE, especially the zero-shot generalization. We open-source the resource and pre-trained models, hoping to provide valuable support to the NLP community.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#24187;&#35273;&#26816;&#27979;&#26694;&#26550;UNIHD&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#35780;&#20272;&#22522;&#20934;&#26041;&#27861;MHaluBench&#26469;&#35780;&#20272;&#24187;&#35273;&#26816;&#27979;&#26041;&#27861;&#30340;&#36827;&#23637;&#12290;&#36825;&#39033;&#24037;&#20316;&#25193;&#23637;&#20102;&#24187;&#35273;&#26816;&#27979;&#30340;&#30740;&#31350;&#33539;&#22260;&#24182;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2402.03190</link><description>&lt;p&gt;
&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24187;&#35273;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Unified Hallucination Detection for Multimodal Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03190
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#24187;&#35273;&#26816;&#27979;&#26694;&#26550;UNIHD&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#35780;&#20272;&#22522;&#20934;&#26041;&#27861;MHaluBench&#26469;&#35780;&#20272;&#24187;&#35273;&#26816;&#27979;&#26041;&#27861;&#30340;&#36827;&#23637;&#12290;&#36825;&#39033;&#24037;&#20316;&#25193;&#23637;&#20102;&#24187;&#35273;&#26816;&#27979;&#30340;&#30740;&#31350;&#33539;&#22260;&#24182;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#22810;&#27169;&#24577;&#20219;&#21153;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(MLLMs)&#20173;&#28982;&#23384;&#22312;&#24187;&#35273;&#30340;&#20005;&#37325;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#21487;&#38752;&#22320;&#26816;&#27979;MLLMs&#20013;&#30340;&#24187;&#35273;&#24050;&#25104;&#20026;&#27169;&#22411;&#35780;&#20272;&#21644;&#23454;&#38469;&#24212;&#29992;&#37096;&#32626;&#20445;&#38556;&#30340;&#37325;&#35201;&#26041;&#38754;&#12290;&#20043;&#21069;&#22312;&#36825;&#20010;&#39046;&#22495;&#30340;&#30740;&#31350;&#21463;&#21040;&#20102;&#29421;&#31364;&#30340;&#20219;&#21153;&#28966;&#28857;&#12289;&#19981;&#36275;&#30340;&#24187;&#35273;&#31867;&#21035;&#28085;&#30422;&#33539;&#22260;&#20197;&#21450;&#32570;&#20047;&#35814;&#32454;&#30340;&#32454;&#31890;&#24230;&#30340;&#38480;&#21046;&#12290;&#38024;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#25193;&#23637;&#20102;&#24187;&#35273;&#26816;&#27979;&#30340;&#30740;&#31350;&#33539;&#22260;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20803;&#35780;&#20272;&#22522;&#20934;&#26041;&#27861;&#65292;MHaluBench&#65292;&#31934;&#24515;&#35774;&#35745;&#20197;&#20419;&#36827;&#24187;&#35273;&#26816;&#27979;&#26041;&#27861;&#30340;&#36827;&#23637;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#32479;&#19968;&#22810;&#27169;&#24577;&#24187;&#35273;&#26816;&#27979;&#26694;&#26550;&#65292;UNIHD&#65292;&#23427;&#21033;&#29992;&#19968;&#22871;&#36741;&#21161;&#24037;&#20855;&#26469;&#31283;&#20581;&#22320;&#39564;&#35777;&#24187;&#35273;&#30340;&#21457;&#29983;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;UNIHD&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite significant strides in multimodal tasks, Multimodal Large Language Models (MLLMs) are plagued by the critical issue of hallucination. The reliable detection of such hallucinations in MLLMs has, therefore, become a vital aspect of model evaluation and the safeguarding of practical application deployment. Prior research in this domain has been constrained by a narrow focus on singular tasks, an inadequate range of hallucination categories addressed, and a lack of detailed granularity. In response to these challenges, our work expands the investigative horizons of hallucination detection. We present a novel meta-evaluation benchmark, MHaluBench, meticulously crafted to facilitate the evaluation of advancements in hallucination detection methods. Additionally, we unveil a novel unified multimodal hallucination detection framework, UNIHD, which leverages a suite of auxiliary tools to validate the occurrence of hallucinations robustly. We demonstrate the effectiveness of UNIHD throug
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#26500;&#24314;&#21512;&#36866;&#30340;&#22522;&#20934;&#21644;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#21457;&#29616;&#65292;&#30001;AI&#29983;&#25104;&#30340;&#22270;&#20687;&#23545;&#20110;&#25991;&#26412;-&#22270;&#20687;&#26816;&#32034;&#27169;&#22411;&#24341;&#20837;&#20102;&#19968;&#20010;&#19981;&#21487;&#35265;&#30340;&#30456;&#20851;&#20559;&#24046;&#65292;&#21363;&#20351;&#36825;&#20123;&#29983;&#25104;&#30340;&#22270;&#20687;&#22312;&#35270;&#35273;&#19978;&#19982;&#26597;&#35810;&#30340;&#30456;&#20851;&#29305;&#24449;&#30456;&#27604;&#24182;&#27809;&#26377;&#26356;&#22810;&#12290;&#36825;&#31181;&#19981;&#21487;&#35265;&#30340;&#30456;&#20851;&#20559;&#24046;&#26222;&#36941;&#23384;&#22312;&#20110;&#19981;&#21516;&#35757;&#32451;&#25968;&#25454;&#21644;&#26550;&#26500;&#30340;&#26816;&#32034;&#27169;&#22411;&#20013;&#12290;</title><link>http://arxiv.org/abs/2311.14084</link><description>&lt;p&gt;
&#30001;AI&#29983;&#25104;&#30340;&#22270;&#20687;&#23545;&#25991;&#26412;-&#22270;&#20687;&#26816;&#32034;&#24341;&#20837;&#20102;&#19981;&#21487;&#35265;&#30340;&#30456;&#20851;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
AI-Generated Images Introduce Invisible Relevance Bias to Text-Image Retrieval. (arXiv:2311.14084v3 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.14084
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#26500;&#24314;&#21512;&#36866;&#30340;&#22522;&#20934;&#21644;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#21457;&#29616;&#65292;&#30001;AI&#29983;&#25104;&#30340;&#22270;&#20687;&#23545;&#20110;&#25991;&#26412;-&#22270;&#20687;&#26816;&#32034;&#27169;&#22411;&#24341;&#20837;&#20102;&#19968;&#20010;&#19981;&#21487;&#35265;&#30340;&#30456;&#20851;&#20559;&#24046;&#65292;&#21363;&#20351;&#36825;&#20123;&#29983;&#25104;&#30340;&#22270;&#20687;&#22312;&#35270;&#35273;&#19978;&#19982;&#26597;&#35810;&#30340;&#30456;&#20851;&#29305;&#24449;&#30456;&#27604;&#24182;&#27809;&#26377;&#26356;&#22810;&#12290;&#36825;&#31181;&#19981;&#21487;&#35265;&#30340;&#30456;&#20851;&#20559;&#24046;&#26222;&#36941;&#23384;&#22312;&#20110;&#19981;&#21516;&#35757;&#32451;&#25968;&#25454;&#21644;&#26550;&#26500;&#30340;&#26816;&#32034;&#27169;&#22411;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29983;&#25104;&#27169;&#22411;&#30340;&#36827;&#27493;&#65292;&#30001;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#30340;&#20869;&#23481;&#65288;AIGC&#65289;&#21464;&#24471;&#26356;&#21152;&#36924;&#30495;&#65292;&#28044;&#20837;&#20114;&#32852;&#32593;&#12290;&#26368;&#36817;&#30340;&#19968;&#39033;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#31181;&#29616;&#35937;&#23548;&#33268;&#20102;&#32593;&#32476;&#25628;&#32034;&#20013;&#30340;&#28304;&#20559;&#24046;&#12290;&#29305;&#21035;&#26159;&#65292;&#31070;&#32463;&#26816;&#32034;&#27169;&#22411;&#24448;&#24448;&#23558;&#29983;&#25104;&#30340;&#25991;&#26412;&#25490;&#21517;&#39640;&#20110;&#20154;&#24037;&#32534;&#20889;&#30340;&#25991;&#26412;&#12290;&#26412;&#25991;&#23558;&#36825;&#31181;&#20559;&#24046;&#30340;&#30740;&#31350;&#25193;&#23637;&#21040;&#36328;&#27169;&#24577;&#26816;&#32034;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25104;&#21151;&#26500;&#24314;&#20102;&#19968;&#20010;&#36866;&#21512;&#25506;&#32034;&#20559;&#24046;&#23384;&#22312;&#30340;&#22522;&#20934;&#12290;&#38543;&#21518;&#65292;&#22312;&#36825;&#20010;&#22522;&#20934;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#21457;&#29616;AI&#29983;&#25104;&#30340;&#22270;&#20687;&#23545;&#20110;&#25991;&#26412;-&#22270;&#20687;&#26816;&#32034;&#27169;&#22411;&#24341;&#20837;&#20102;&#19968;&#20010;&#19981;&#21487;&#35265;&#30340;&#30456;&#20851;&#20559;&#24046;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#23613;&#31649;AI&#29983;&#25104;&#30340;&#22270;&#20687;&#19982;&#26597;&#35810;&#30456;&#27604;&#27809;&#26377;&#26356;&#22810;&#30340;&#35270;&#35273;&#30456;&#20851;&#29305;&#24449;&#65292;&#20294;&#25991;&#26412;-&#22270;&#20687;&#26816;&#32034;&#27169;&#22411;&#24448;&#24448;&#23558;AI&#29983;&#25104;&#30340;&#22270;&#20687;&#25490;&#21517;&#39640;&#20110;&#30495;&#23454;&#22270;&#20687;&#12290;&#36825;&#31181;&#19981;&#21487;&#35265;&#30340;&#30456;&#20851;&#20559;&#24046;&#22312;&#19981;&#21516;&#35757;&#32451;&#25968;&#25454;&#21644;&#26550;&#26500;&#30340;&#26816;&#32034;&#27169;&#22411;&#20013;&#26222;&#36941;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the advancement of generation models, AI-generated content (AIGC) is becoming more realistic, flooding the Internet. A recent study suggests that this phenomenon causes source bias in text retrieval for web search. Specifically, neural retrieval models tend to rank generated texts higher than human-written texts. In this paper, we extend the study of this bias to cross-modal retrieval. Firstly, we successfully construct a suitable benchmark to explore the existence of the bias. Subsequent extensive experiments on this benchmark reveal that AI-generated images introduce an invisible relevance bias to text-image retrieval models. Specifically, our experiments show that text-image retrieval models tend to rank the AI-generated images higher than the real images, even though the AI-generated images do not exhibit more visually relevant features to the query than real images. This invisible relevance bias is prevalent across retrieval models with varying training data and architectures
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;&#30340;FactCHD&#20107;&#23454;&#20914;&#31361;&#24187;&#35273;&#26816;&#27979;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;LLMs&#29983;&#25104;&#25991;&#26412;&#30340;&#20107;&#23454;&#24615;&#12290;&#22522;&#20934;&#21253;&#21547;&#20102;&#22810;&#31181;&#20107;&#23454;&#27169;&#24335;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;&#20107;&#23454;&#30340;&#35777;&#25454;&#38142;&#36827;&#34892;&#32452;&#21512;&#24615;&#24187;&#35273;&#30340;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2310.12086</link><description>&lt;p&gt;
&#21457;&#29616;&#22622;&#22764;&#20043;&#27468;&#65306;&#21487;&#38752;&#30340;&#20107;&#23454;&#20914;&#31361;&#24187;&#35273;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Unveiling the Siren's Song: Towards Reliable Fact-Conflicting Hallucination Detection. (arXiv:2310.12086v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12086
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;&#30340;FactCHD&#20107;&#23454;&#20914;&#31361;&#24187;&#35273;&#26816;&#27979;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;LLMs&#29983;&#25104;&#25991;&#26412;&#30340;&#20107;&#23454;&#24615;&#12290;&#22522;&#20934;&#21253;&#21547;&#20102;&#22810;&#31181;&#20107;&#23454;&#27169;&#24335;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;&#20107;&#23454;&#30340;&#35777;&#25454;&#38142;&#36827;&#34892;&#32452;&#21512;&#24615;&#24187;&#35273;&#30340;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;ChatGPT/GPT-4&#65292;&#22240;&#20854;&#24191;&#27867;&#30340;&#23454;&#38469;&#24212;&#29992;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#65292;&#20294;&#20854;&#22312;&#32593;&#32476;&#24179;&#21488;&#19978;&#23384;&#22312;&#20107;&#23454;&#20914;&#31361;&#24187;&#35273;&#30340;&#38382;&#39064;&#38480;&#21046;&#20102;&#20854;&#37319;&#29992;&#12290;&#23545;&#30001;LLMs&#20135;&#29983;&#30340;&#25991;&#26412;&#30340;&#20107;&#23454;&#24615;&#35780;&#20272;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#65292;&#19981;&#20165;&#28041;&#21450;&#23545;&#22522;&#26412;&#20107;&#23454;&#30340;&#21028;&#26029;&#65292;&#36824;&#21253;&#25324;&#23545;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#65288;&#22914;&#22810;&#36339;&#31561;&#65289;&#20013;&#20986;&#29616;&#30340;&#20107;&#23454;&#38169;&#35823;&#30340;&#35780;&#20272;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;FactCHD&#65292;&#19968;&#31181;&#20026;LLMs&#31934;&#24515;&#35774;&#35745;&#30340;&#20107;&#23454;&#20914;&#31361;&#24187;&#35273;&#26816;&#27979;&#22522;&#20934;&#12290;&#20316;&#20026;&#22312;&#8220;&#26597;&#35810;-&#21709;&#24212;&#8221;&#19978;&#19979;&#25991;&#20013;&#35780;&#20272;&#20107;&#23454;&#24615;&#30340;&#20851;&#38190;&#24037;&#20855;&#65292;&#25105;&#20204;&#30340;&#22522;&#20934;&#37319;&#29992;&#20102;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;&#24191;&#27867;&#30340;&#20107;&#23454;&#27169;&#24335;&#65292;&#22914;&#22522;&#26412;&#20107;&#23454;&#65292;&#22810;&#36339;&#65292;&#27604;&#36739;&#21644;&#38598;&#21512;&#25805;&#20316;&#27169;&#24335;&#12290;&#25105;&#20204;&#22522;&#20934;&#30340;&#19968;&#20010;&#29420;&#29305;&#29305;&#28857;&#26159;&#20854;&#21253;&#21547;&#22522;&#20110;&#20107;&#23454;&#30340;&#35777;&#25454;&#38142;&#65292;&#20174;&#32780;&#20415;&#20110;&#36827;&#34892;&#32452;&#21512;&#24615;&#24187;&#35273;&#30340;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), such as ChatGPT/GPT-4, have garnered widespread attention owing to their myriad of practical applications, yet their adoption has been constrained by issues of fact-conflicting hallucinations across web platforms. The assessment of factuality in text, produced by LLMs, remains inadequately explored, extending not only to the judgment of vanilla facts but also encompassing the evaluation of factual errors emerging in complex inferential tasks like multi-hop, and etc. In response, we introduce FactCHD, a fact-conflicting hallucination detection benchmark meticulously designed for LLMs. Functioning as a pivotal tool in evaluating factuality within "Query-Respons" contexts, our benchmark assimilates a large-scale dataset, encapsulating a broad spectrum of factuality patterns, such as vanilla, multi-hops, comparison, and set-operation patterns. A distinctive feature of our benchmark is its incorporation of fact-based chains of evidence, thereby facilitating com
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#25361;&#25112;&#22270;&#21327;&#21516;&#36807;&#28388;&#30340;&#31070;&#35805;&#65292;&#36890;&#36807;&#20851;&#27880;&#32467;&#26524;&#30340;&#21487;&#22797;&#21046;&#24615;&#65292;&#25104;&#21151;&#22797;&#21046;&#20102;&#20845;&#20010;&#27969;&#34892;&#30340;&#22270;&#25512;&#33616;&#27169;&#22411;&#22312;&#20960;&#20010;&#24120;&#35265;&#21644;&#26032;&#25968;&#25454;&#38598;&#19978;&#30340;&#32467;&#26524;&#65292;&#24182;&#19982;&#20256;&#32479;&#21327;&#21516;&#36807;&#28388;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2308.00404</link><description>&lt;p&gt;
&#25361;&#25112;&#22270;&#21327;&#21516;&#36807;&#28388;&#30340;&#31070;&#35805;&#65306;&#19968;&#39033;&#22522;&#20110;&#25512;&#29702;&#21644;&#21487;&#22797;&#21046;&#24615;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Challenging the Myth of Graph Collaborative Filtering: a Reasoned and Reproducibility-driven Analysis. (arXiv:2308.00404v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00404
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#25361;&#25112;&#22270;&#21327;&#21516;&#36807;&#28388;&#30340;&#31070;&#35805;&#65292;&#36890;&#36807;&#20851;&#27880;&#32467;&#26524;&#30340;&#21487;&#22797;&#21046;&#24615;&#65292;&#25104;&#21151;&#22797;&#21046;&#20102;&#20845;&#20010;&#27969;&#34892;&#30340;&#22270;&#25512;&#33616;&#27169;&#22411;&#22312;&#20960;&#20010;&#24120;&#35265;&#21644;&#26032;&#25968;&#25454;&#38598;&#19978;&#30340;&#32467;&#26524;&#65292;&#24182;&#19982;&#20256;&#32479;&#21327;&#21516;&#36807;&#28388;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65288;GNNs&#65289;&#30340;&#25104;&#21151;&#26174;&#33879;&#25512;&#21160;&#20102;&#25512;&#33616;&#31995;&#32479;&#30340;&#21457;&#23637;&#65292;&#36890;&#36807;&#23558;&#29992;&#25143;&#21644;&#29289;&#21697;&#26377;&#25928;&#22320;&#24314;&#27169;&#20026;&#19968;&#20010;&#20108;&#20998;&#22270;&#21644;&#26080;&#21521;&#22270;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#21407;&#22987;&#30340;&#22522;&#20110;&#22270;&#30340;&#20316;&#21697;&#36890;&#24120;&#22312;&#26410;&#39564;&#35777;&#20854;&#22312;&#20855;&#20307;&#37197;&#32622;&#19979;&#30340;&#26377;&#25928;&#24615;&#30340;&#24773;&#20917;&#19979;&#37319;&#29992;&#22522;&#32447;&#35770;&#25991;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#30528;&#37325;&#20851;&#27880;&#32467;&#26524;&#30340;&#21487;&#22797;&#21046;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25104;&#21151;&#22797;&#21046;&#20102;&#20845;&#20010;&#27969;&#34892;&#19988;&#26368;&#26032;&#30340;&#22270;&#25512;&#33616;&#27169;&#22411;&#65288;NGCF&#12289;DGCF&#12289;LightGCN&#12289;SGL&#12289;UltraGCN&#21644;GFCF&#65289;&#22312;&#19977;&#20010;&#24120;&#35265;&#22522;&#20934;&#25968;&#25454;&#38598;&#65288;Gowalla&#12289;Yelp 2018&#21644;&#20122;&#39532;&#36874;&#22270;&#20070;&#65289;&#19978;&#30340;&#32467;&#26524;&#30340;&#20195;&#30721;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#22270;&#27169;&#22411;&#19982;&#22312;&#31163;&#32447;&#35780;&#20272;&#20013;&#34920;&#29616;&#33391;&#22909;&#30340;&#20256;&#32479;&#21327;&#21516;&#36807;&#28388;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25193;&#23637;&#20102;&#23545;&#20004;&#20010;&#32570;&#20047;&#29616;&#26377;&#25991;&#29486;&#20013;&#24050;&#24314;&#31435;&#35774;&#32622;&#30340;&#26032;&#25968;&#25454;&#38598;&#65288;Allrecipes&#21644;BookCrossing&#65289;&#30340;&#30740;&#31350;&#12290;&#30001;&#20110;&#22312;&#36825;&#20123;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#19982;&#20197;&#21069;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19981;&#21516;&#65292;&#20351;&#24471;&#25105;&#20204;&#23545;&#22270;&#25512;&#33616;&#27169;&#22411;&#24615;&#33021;&#30340;&#35780;&#20272;&#32467;&#26524;&#26356;&#21152;&#28145;&#20837;&#21644;&#20840;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
The success of graph neural network-based models (GNNs) has significantly advanced recommender systems by effectively modeling users and items as a bipartite, undirected graph. However, many original graph-based works often adopt results from baseline papers without verifying their validity for the specific configuration under analysis. Our work addresses this issue by focusing on the replicability of results. We present a code that successfully replicates results from six popular and recent graph recommendation models (NGCF, DGCF, LightGCN, SGL, UltraGCN, and GFCF) on three common benchmark datasets (Gowalla, Yelp 2018, and Amazon Book). Additionally, we compare these graph models with traditional collaborative filtering models that historically performed well in offline evaluations. Furthermore, we extend our study to two new datasets (Allrecipes and BookCrossing) that lack established setups in existing literature. As the performance on these datasets differs from the previous bench
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#24179;&#34913;&#39033;&#30446;&#21644;&#29992;&#25143;&#20844;&#24179;&#24615;&#30340;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#20302;&#21518;&#24724;&#30340;&#22312;&#32447;&#20248;&#21270;&#31639;&#27861;&#23454;&#29616;&#20102;&#32500;&#25345;&#25910;&#30410;&#21516;&#26102;&#23454;&#29616;&#20844;&#24179;&#25512;&#33616;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2306.10050</link><description>&lt;p&gt;
&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#25554;&#20540;&#39033;&#30446;&#21644;&#29992;&#25143;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Interpolating Item and User Fairness in Recommendation Systems. (arXiv:2306.10050v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10050
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#24179;&#34913;&#39033;&#30446;&#21644;&#29992;&#25143;&#20844;&#24179;&#24615;&#30340;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#20302;&#21518;&#24724;&#30340;&#22312;&#32447;&#20248;&#21270;&#31639;&#27861;&#23454;&#29616;&#20102;&#32500;&#25345;&#25910;&#30410;&#21516;&#26102;&#23454;&#29616;&#20844;&#24179;&#25512;&#33616;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#36793;&#24179;&#21488;&#20013;&#65292;&#24179;&#21488;&#19982;&#21334;&#23478;&#65288;&#39033;&#30446;&#65289;&#21644;&#23458;&#25143;&#65288;&#29992;&#25143;&#65289;&#31561;&#21508;&#31181;&#21508;&#26679;&#30340;&#21033;&#30410;&#30456;&#20851;&#32773;&#20114;&#21160;&#65292;&#27599;&#20010;&#30456;&#20851;&#32773;&#37117;&#26377;&#33258;&#24049;&#30340;&#26399;&#26395;&#32467;&#26524;&#65292;&#23547;&#25214;&#21512;&#36866;&#30340;&#24179;&#34913;&#28857;&#21464;&#24471;&#38750;&#24120;&#22797;&#26434;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#8220;&#20844;&#24179;&#25104;&#26412;&#8221;&#65292;&#23427;&#25429;&#25417;&#20102;&#24179;&#21488;&#22312;&#24179;&#34913;&#19981;&#21516;&#21033;&#30410;&#30456;&#20851;&#32773;&#21033;&#30410;&#26102;&#21487;&#33021;&#20570;&#20986;&#30340;&#22949;&#21327;&#12290;&#20986;&#20110;&#36825;&#20010;&#30446;&#30340;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20844;&#24179;&#25512;&#33616;&#26694;&#26550;&#65292;&#20854;&#20013;&#24179;&#21488;&#22312;&#25554;&#20540;&#39033;&#30446;&#21644;&#29992;&#25143;&#20844;&#24179;&#24615;&#32422;&#26463;&#26102;&#26368;&#22823;&#21270;&#20854;&#25910;&#30410;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#26356;&#29616;&#23454;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22312;&#32447;&#35774;&#32622;&#20013;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#20844;&#24179;&#25512;&#33616;&#38382;&#39064;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#24179;&#21488;&#32570;&#20047;&#20102;&#35299;&#29992;&#25143;&#20559;&#22909;&#30340;&#30693;&#35782;&#65292;&#21482;&#33021;&#35266;&#23519;&#20108;&#36827;&#21046;&#36141;&#20080;&#20915;&#31574;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#20302;&#21518;&#24724;&#30340;&#22312;&#32447;&#20248;&#21270;&#31639;&#27861;&#65292;&#23427;&#22312;&#32500;&#25252;&#24179;&#21488;&#25910;&#30410;&#30340;&#21516;&#26102;&#31649;&#29702;&#39033;&#30446;&#21644;&#29992;&#25143;&#20844;&#24179;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#22312;&#23454;&#29616;&#20844;&#24179;&#25512;&#33616;&#21516;&#26102;&#20445;&#25345;&#39640;&#25910;&#30410;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online platforms employ recommendation systems to enhance customer engagement and drive revenue. However, in a multi-sided platform where the platform interacts with diverse stakeholders such as sellers (items) and customers (users), each with their own desired outcomes, finding an appropriate middle ground becomes a complex operational challenge. In this work, we investigate the ``price of fairness'', which captures the platform's potential compromises when balancing the interests of different stakeholders. Motivated by this, we propose a fair recommendation framework where the platform maximizes its revenue while interpolating between item and user fairness constraints. We further examine the fair recommendation problem in a more realistic yet challenging online setting, where the platform lacks knowledge of user preferences and can only observe binary purchase decisions. To address this, we design a low-regret online optimization algorithm that preserves the platform's revenue while
&lt;/p&gt;</description></item></channel></rss>