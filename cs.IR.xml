<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#26412;&#20307;&#35770;&#30340;&#27169;&#22411;&#21345;&#31995;&#32479;&#65292;&#29992;&#20110;&#32467;&#26500;&#21270;&#21644;&#27491;&#24335;&#21270;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35780;&#20272;&#12289;&#38480;&#21046;&#21644;&#39044;&#26399;&#29992;&#36884;&#31561;&#30456;&#20851;&#20449;&#24687;&#65292;&#20197;&#25903;&#25345;FAIR&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2303.11991</link><description>&lt;p&gt;
&#22522;&#20110;&#26412;&#20307;&#35770;&#30340;&#27169;&#22411;&#21345;&#22312;&#29983;&#29289;&#21307;&#23398;&#30740;&#31350;&#20013;&#26426;&#22120;&#23398;&#20064;&#20449;&#24687;&#38142;&#25509;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Application of an ontology for model cards to generate computable artifacts for linking machine learning information from biomedical research. (arXiv:2303.11991v1 [cs.DL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11991
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#26412;&#20307;&#35770;&#30340;&#27169;&#22411;&#21345;&#31995;&#32479;&#65292;&#29992;&#20110;&#32467;&#26500;&#21270;&#21644;&#27491;&#24335;&#21270;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35780;&#20272;&#12289;&#38480;&#21046;&#21644;&#39044;&#26399;&#29992;&#36884;&#31561;&#30456;&#20851;&#20449;&#24687;&#65292;&#20197;&#25903;&#25345;FAIR&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#21345;&#25253;&#21578;&#25552;&#20379;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#36879;&#26126;&#25551;&#36848;&#65292;&#20854;&#20013;&#21253;&#25324;&#26377;&#20851;&#20854;&#35780;&#20272;&#12289;&#38480;&#21046;&#12289;&#39044;&#26399;&#29992;&#36884;&#31561;&#20449;&#24687;&#12290;&#32852;&#37030;&#21355;&#29983;&#26426;&#26500;&#23545;&#20351;&#29992;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20154;&#24037;&#26234;&#33021;&#36827;&#34892;&#30740;&#31350;&#30340;&#27169;&#22411;&#21345;&#25253;&#21578;&#34920;&#36798;&#20102;&#20852;&#36259;&#12290;&#20043;&#21069;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#29992;&#20110;&#27169;&#22411;&#21345;&#25253;&#21578;&#30340;&#26412;&#20307;&#27169;&#22411;&#65292;&#20197;&#32467;&#26500;&#21270;&#21644;&#27491;&#24335;&#21270;&#36825;&#20123;&#25253;&#21578;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#22522;&#20110;Java&#30340;&#24211;(OWL API&#65292;FaCT++)&#65292;&#21033;&#29992;&#25105;&#20204;&#30340;&#26412;&#20307;&#21457;&#24067;&#21487;&#35745;&#31639;&#30340;&#27169;&#22411;&#21345;&#25253;&#21578;&#12290;&#25105;&#20204;&#35752;&#35770;&#26410;&#26469;&#30340;&#26041;&#21521;&#21644;&#20854;&#20182;&#30340;&#29992;&#20363;&#65292;&#20197;&#20984;&#26174;&#26412;&#20307;&#39537;&#21160;&#31995;&#32479;&#25903;&#25345;FAIR&#25361;&#25112;&#30340;&#36866;&#29992;&#24615;&#21644;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model card reports provide a transparent description of machine learning models which includes information about their evaluation, limitations, intended use, etc. Federal health agencies have expressed an interest in model cards report for research studies using machine-learning based AI. Previously, we have developed an ontology model for model card reports to structure and formalize these reports. In this paper, we demonstrate a Java-based library (OWL API, FaCT++) that leverages our ontology to publish computable model card reports. We discuss future directions and other use cases that highlight applicability and feasibility of ontology-driven systems to support FAIR challenges.
&lt;/p&gt;</description></item><item><title>CompoDiff &#26159;&#19968;&#31181;&#22810;&#21151;&#33021;&#30340;&#32452;&#21512;&#22270;&#20687;&#26816;&#32034;&#27169;&#22411;&#65292;&#36890;&#36807;&#25509;&#21463;&#21508;&#31181;&#26465;&#20214;&#65292;&#20855;&#26377;&#28508;&#22312;&#25193;&#25955;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312; FashionIQ &#19978;&#23454;&#29616;&#20102;&#26032;&#30340;&#38646;&#26679;&#26412;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#12290;&#20854;&#29305;&#24449;&#20301;&#20110;&#23436;&#25972;&#30340; CLIP &#23884;&#20837;&#31354;&#38388;&#20013;&#65292;&#21487;&#20197;&#30452;&#25509;&#29992;&#20110;&#25152;&#26377;&#21033;&#29992; CLIP &#31354;&#38388;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2303.11916</link><description>&lt;p&gt;
CompoDiff: &#22522;&#20110;&#28508;&#22312;&#25193;&#25955;&#30340;&#22810;&#21151;&#33021;&#32452;&#21512;&#22270;&#20687;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
CompoDiff: Versatile Composed Image Retrieval With Latent Diffusion. (arXiv:2303.11916v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11916
&lt;/p&gt;
&lt;p&gt;
CompoDiff &#26159;&#19968;&#31181;&#22810;&#21151;&#33021;&#30340;&#32452;&#21512;&#22270;&#20687;&#26816;&#32034;&#27169;&#22411;&#65292;&#36890;&#36807;&#25509;&#21463;&#21508;&#31181;&#26465;&#20214;&#65292;&#20855;&#26377;&#28508;&#22312;&#25193;&#25955;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312; FashionIQ &#19978;&#23454;&#29616;&#20102;&#26032;&#30340;&#38646;&#26679;&#26412;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#12290;&#20854;&#29305;&#24449;&#20301;&#20110;&#23436;&#25972;&#30340; CLIP &#23884;&#20837;&#31354;&#38388;&#20013;&#65292;&#21487;&#20197;&#30452;&#25509;&#29992;&#20110;&#25152;&#26377;&#21033;&#29992; CLIP &#31354;&#38388;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#27169;&#22411; CompoDiff&#65292;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#28508;&#22312;&#25193;&#25955;&#30340;&#32452;&#21512;&#22270;&#20687;&#26816;&#32034;&#65288;CIR&#65289;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#30001; 1800 &#19975;&#20010;&#21442;&#32771;&#22270;&#20687;&#12289;&#26465;&#20214;&#21644;&#30456;&#24212;&#30340;&#30446;&#26631;&#22270;&#20687;&#19977;&#20803;&#32452;&#32452;&#25104;&#30340;&#26032;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35757;&#32451;&#27169;&#22411;&#12290;CompoDiff &#19981;&#20165;&#22312;&#20687; FashionIQ &#36825;&#26679;&#30340; CIR &#22522;&#20934;&#27979;&#35797;&#19978;&#23454;&#29616;&#20102;&#26032;&#30340;&#38646;&#26679;&#26412;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#65292;&#32780;&#19988;&#36824;&#36890;&#36807;&#25509;&#25910;&#21508;&#31181;&#26465;&#20214;&#65288;&#22914;&#36127;&#25991;&#26412;&#21644;&#22270;&#20687;&#36974;&#32617;&#26465;&#20214;&#65289;&#65292;&#20351;&#24471; CIR &#26356;&#21152;&#22810;&#21151;&#33021;&#65292;&#36825;&#26159;&#29616;&#26377; CIR &#26041;&#27861;&#25152;&#19981;&#20855;&#22791;&#30340;&#12290;&#27492;&#22806;&#65292;CompoDiff &#29305;&#24449;&#20301;&#20110;&#23436;&#25972;&#30340; CLIP &#23884;&#20837;&#31354;&#38388;&#20013;&#65292;&#22240;&#27492;&#23427;&#20204;&#21487;&#20197;&#30452;&#25509;&#29992;&#20110;&#21033;&#29992; CLIP &#31354;&#38388;&#30340;&#25152;&#26377;&#29616;&#26377;&#27169;&#22411;&#12290;&#35757;&#32451;&#25152;&#20351;&#29992;&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#39044;&#35757;&#32451;&#26435;&#37325;&#21487;&#22312; https://github.com/navervision/CompoDiff &#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a novel diffusion-based model, CompoDiff, for solving Composed Image Retrieval (CIR) with latent diffusion and presents a newly created dataset of 18 million reference images, conditions, and corresponding target image triplets to train the model. CompoDiff not only achieves a new zero-shot state-of-the-art on a CIR benchmark such as FashionIQ but also enables a more versatile CIR by accepting various conditions, such as negative text and image mask conditions, which are unavailable with existing CIR methods. In addition, the CompoDiff features are on the intact CLIP embedding space so that they can be directly used for all existing models exploiting the CLIP space. The code and dataset used for the training, and the pre-trained weights are available at https://github.com/navervision/CompoDiff
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#30340;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#26694;&#26550;&#21033;&#29992;&#29992;&#25143;&#30340;&#24207;&#21015;&#34892;&#20026;&#21644;&#29289;&#21697;&#30340;&#22810;&#27169;&#24577;&#20869;&#23481;&#36827;&#34892;&#24207;&#21015;&#25512;&#33616;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39592;&#24178;&#32593;&#32476;&#36827;&#34892;&#29305;&#24449;&#34701;&#21512;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.11879</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#30340;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#26694;&#26550;&#29992;&#20110;&#24207;&#21015;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Multimodal Pre-training Framework for Sequential Recommendation via Contrastive Learning. (arXiv:2303.11879v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11879
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#30340;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#26694;&#26550;&#21033;&#29992;&#29992;&#25143;&#30340;&#24207;&#21015;&#34892;&#20026;&#21644;&#29289;&#21697;&#30340;&#22810;&#27169;&#24577;&#20869;&#23481;&#36827;&#34892;&#24207;&#21015;&#25512;&#33616;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39592;&#24178;&#32593;&#32476;&#36827;&#34892;&#29305;&#24449;&#34701;&#21512;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24207;&#21015;&#25512;&#33616;&#31995;&#32479;&#21033;&#29992;&#29992;&#25143;&#19982;&#29289;&#21697;&#20043;&#38388;&#30340;&#24207;&#21015;&#20132;&#20114;&#20316;&#20026;&#20027;&#35201;&#30340;&#30417;&#30563;&#20449;&#21495;&#26469;&#23398;&#20064;&#29992;&#25143;&#30340;&#21916;&#22909;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#29992;&#25143;&#34892;&#20026;&#25968;&#25454;&#30340;&#31232;&#30095;&#24615;&#65292;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#29983;&#25104;&#19981;&#23613;&#22914;&#20154;&#24847;&#30340;&#32467;&#26524;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#21517;&#20026;&#22810;&#27169;&#24577;&#24207;&#21015;&#28151;&#21512;&#65288;MSM4SR&#65289;&#65292;&#23427;&#21033;&#29992;&#29992;&#25143;&#30340;&#24207;&#21015;&#34892;&#20026;&#21644;&#29289;&#21697;&#30340;&#22810;&#27169;&#24577;&#20869;&#23481;&#65288;&#21363;&#25991;&#26412;&#21644;&#22270;&#20687;&#65289;&#36827;&#34892;&#26377;&#25928;&#25512;&#33616;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;MSM4SR&#23558;&#27599;&#20010;&#29289;&#21697;&#22270;&#20687;&#26631;&#35760;&#25104;&#22810;&#20010;&#25991;&#26412;&#20851;&#38190;&#35789;&#65292;&#24182;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;BERT&#27169;&#22411;&#33719;&#21462;&#29289;&#21697;&#30340;&#21021;&#22987;&#25991;&#26412;&#21644;&#35270;&#35273;&#29305;&#24449;&#65292;&#20197;&#28040;&#38500;&#25991;&#26412;&#21644;&#22270;&#20687;&#27169;&#24577;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39592;&#24178;&#32593;&#32476;&#65292;&#21363;&#22810;&#27169;&#24577;&#28151;&#21512;&#24207;&#21015;&#32534;&#30721;&#22120;&#65288;M $^2$ SE&#65289;&#65292;&#23427;&#20351;&#29992;&#20114;&#34917;&#30340;&#24207;&#21015;&#28151;&#21512;&#31574;&#30053;&#26469;&#24357;&#21512;&#29289;&#21697;&#22810;&#27169;&#24577;&#20869;&#23481;&#21644;&#29992;&#25143;&#34892;&#20026;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#27492;&#22806;&#65292;&#24341;&#20837;&#23545;&#27604;&#23398;&#20064;&#26426;&#21046;&#26469;&#24378;&#21046;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#21464;&#24471;&#26356;&#26377;&#21306;&#20998;&#24230;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#24207;&#21015;&#25512;&#33616;&#30340;&#24615;&#33021;&#12290;&#22312;&#20004;&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequential recommendation systems utilize the sequential interactions of users with items as their main supervision signals in learning users' preferences. However, existing methods usually generate unsatisfactory results due to the sparsity of user behavior data. To address this issue, we propose a novel pre-training framework, named Multimodal Sequence Mixup for Sequential Recommendation (MSM4SR), which leverages both users' sequential behaviors and items' multimodal content (\ie text and images) for effectively recommendation. Specifically, MSM4SR tokenizes each item image into multiple textual keywords and uses the pre-trained BERT model to obtain initial textual and visual features of items, for eliminating the discrepancy between the text and image modalities. A novel backbone network, \ie Multimodal Mixup Sequence Encoder (M$^2$SE), is proposed to bridge the gap between the item multimodal content and the user behavior, using a complementary sequence mixup strategy. In addition,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;Debiased Contrastive learning&#33539;&#24335;&#65292;&#21517;&#20026;DCRec&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#19968;&#33268;&#24615;&#24863;&#30693;&#22686;&#24378;&#23558;&#39034;&#24207;&#27169;&#24335;&#32534;&#30721;&#19982;&#20840;&#23616;&#21327;&#20316;&#20851;&#31995;&#24314;&#27169;&#30456;&#32479;&#19968;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#22312;&#21435;&#38500;&#27969;&#34892;&#24230;&#20559;&#35265;&#21644;&#21306;&#20998;&#29992;&#25143;&#34892;&#20026;&#20013;&#23384;&#22312;&#30340;&#23616;&#38480;&#24615;&#65292;&#33021;&#22815;&#25552;&#39640;&#25512;&#33616;&#36136;&#37327;&#24182;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.11780</link><description>&lt;p&gt;
&#20026;&#39034;&#24207;&#25512;&#33616;&#25552;&#20379;&#26080;&#20559;&#24046;&#30340;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Debiased Contrastive Learning for Sequential Recommendation. (arXiv:2303.11780v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11780
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;Debiased Contrastive learning&#33539;&#24335;&#65292;&#21517;&#20026;DCRec&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#19968;&#33268;&#24615;&#24863;&#30693;&#22686;&#24378;&#23558;&#39034;&#24207;&#27169;&#24335;&#32534;&#30721;&#19982;&#20840;&#23616;&#21327;&#20316;&#20851;&#31995;&#24314;&#27169;&#30456;&#32479;&#19968;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#22312;&#21435;&#38500;&#27969;&#34892;&#24230;&#20559;&#35265;&#21644;&#21306;&#20998;&#29992;&#25143;&#34892;&#20026;&#20013;&#23384;&#22312;&#30340;&#23616;&#38480;&#24615;&#65292;&#33021;&#22815;&#25552;&#39640;&#25512;&#33616;&#36136;&#37327;&#24182;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#21033;&#29992;&#21508;&#31181;&#31070;&#32463;&#25216;&#26415;&#65288;&#22914;Transformer&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#65289;&#35299;&#20915;&#21160;&#24577;&#29992;&#25143;&#20559;&#22909;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#26469;&#33258;&#39640;&#24230;&#31232;&#30095;&#29992;&#25143;&#34892;&#20026;&#25968;&#25454;&#30340;&#25512;&#35770;&#21487;&#33021;&#20250;&#22952;&#30861;&#39034;&#24207;&#27169;&#24335;&#32534;&#30721;&#30340;&#34920;&#24449;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#26631;&#31614;&#30701;&#32570;&#38382;&#39064;&#65292;&#26368;&#36817;&#25552;&#20986;&#20102;&#23545;&#27604;&#23398;&#20064;&#65288;CL&#65289;&#26041;&#27861;&#65292;&#20197;&#20004;&#31181;&#26041;&#24335;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#65306;&#65288;i&#65289;&#38543;&#26426;&#25439;&#22351;&#24207;&#21015;&#25968;&#25454;&#65288;&#20363;&#22914;&#38543;&#26426;&#36974;&#32617;&#12289;&#37325;&#25490;&#24207;&#65289;&#65307;&#65288;ii&#65289;&#22312;&#39044;&#23450;&#20041;&#30340;&#23545;&#27604;&#35270;&#22270;&#20043;&#38388;&#23545;&#40784;&#34920;&#31034;&#12290;&#34429;&#28982;&#26377;&#25928;&#65292;&#20294;&#25105;&#20204;&#35748;&#20026;&#24403;&#21069;&#22522;&#20110;CL&#30340;&#26041;&#27861;&#22312;&#35299;&#20915;&#27969;&#34892;&#24230;&#20559;&#35265;&#21644;&#29992;&#25143;&#19968;&#33268;&#24615;&#19982;&#30495;&#23454;&#20852;&#36259;&#30456;&#20998;&#31163;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#20559;&#24046;&#30340;&#25512;&#33616;&#23545;&#27604;&#23398;&#20064;&#33539;&#24335;&#65288;DCRec&#65289;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#19968;&#33268;&#24615;&#24863;&#30693;&#22686;&#24378;&#23558;&#39034;&#24207;&#27169;&#24335;&#32534;&#30721;&#19982;&#20840;&#23616;&#21327;&#20316;&#20851;&#31995;&#24314;&#27169;&#30456;&#32479;&#19968;&#12290;&#36825;&#20351;&#24471;DCRec&#33021;&#22815;&#21435;&#38500;&#30001;&#20110;&#27969;&#34892;&#24230;&#25928;&#24212;&#32780;&#24341;&#36215;&#30340;&#38544;&#24335;&#21453;&#39304;&#30340;&#20559;&#24046;&#65292;&#24182;&#25552;&#21462;&#29992;&#25143;&#30340;&#30495;&#23454;&#20852;&#36259;&#12290;&#23545;&#22235;&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;DCRec&#25552;&#39640;&#20102;&#25512;&#33616;&#36136;&#37327;&#65292;&#24182;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current sequential recommender systems are proposed to tackle the dynamic user preference learning with various neural techniques, such as Transformer and Graph Neural Networks (GNNs). However, inference from the highly sparse user behavior data may hinder the representation ability of sequential pattern encoding. To address the label shortage issue, contrastive learning (CL) methods are proposed recently to perform data augmentation in two fashions: (i) randomly corrupting the sequence data (e.g. stochastic masking, reordering); (ii) aligning representations across pre-defined contrastive views. Although effective, we argue that current CL-based methods have limitations in addressing popularity bias and disentangling of user conformity and real interest. In this paper, we propose a new Debiased Contrastive learning paradigm for Recommendation (DCRec) that unifies sequential pattern encoding with global collaborative relation modeling through adaptive conformity-aware augmentation. Thi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22270;&#20070;&#39302;&#25512;&#33616;&#31995;&#32479;&#65292;&#21033;&#29992;&#20102;&#26469;&#33258;&#24322;&#26500;&#25968;&#25454;&#28304;&#30340;&#25968;&#25454;&#65292;&#21253;&#25324;&#20511;&#38405;&#35760;&#24405;&#21644;&#22312;&#32447;&#31038;&#20132;&#35835;&#32773;&#30340;&#21453;&#39304;&#21644;&#20449;&#24687;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#21327;&#21516;&#36807;&#28388;&#65288;CF&#65289;&#26041;&#27861;&#20248;&#20110;&#22522;&#20110;&#20869;&#23481;&#30340;&#65288;CB&#65289;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#39640;&#25512;&#33616;&#31995;&#32479;&#30340;&#24615;&#33021;&#36798;&#21040;47%&#12290;</title><link>http://arxiv.org/abs/2303.11746</link><description>&lt;p&gt;
&#22270;&#20070;&#39302;&#20013;&#30340;&#25512;&#33616;&#31995;&#32479;&#65306;&#19968;&#31181;&#24322;&#26500;&#25968;&#25454;&#28304;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Recommendation Systems in Libraries: an Application with Heterogeneous Data Sources. (arXiv:2303.11746v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11746
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22270;&#20070;&#39302;&#25512;&#33616;&#31995;&#32479;&#65292;&#21033;&#29992;&#20102;&#26469;&#33258;&#24322;&#26500;&#25968;&#25454;&#28304;&#30340;&#25968;&#25454;&#65292;&#21253;&#25324;&#20511;&#38405;&#35760;&#24405;&#21644;&#22312;&#32447;&#31038;&#20132;&#35835;&#32773;&#30340;&#21453;&#39304;&#21644;&#20449;&#24687;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#21327;&#21516;&#36807;&#28388;&#65288;CF&#65289;&#26041;&#27861;&#20248;&#20110;&#22522;&#20110;&#20869;&#23481;&#30340;&#65288;CB&#65289;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#39640;&#25512;&#33616;&#31995;&#32479;&#30340;&#24615;&#33021;&#36798;&#21040;47%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Reading&amp;Machine&#39033;&#30446;&#21033;&#29992;&#25968;&#23383;&#21270;&#25903;&#25345;&#65292;&#25552;&#39640;&#22270;&#20070;&#39302;&#30340;&#21560;&#24341;&#21147;&#21644;&#25913;&#21892;&#29992;&#25143;&#20307;&#39564;&#12290;&#35813;&#39033;&#30446;&#23454;&#29616;&#20102;&#19968;&#20010;&#24212;&#29992;&#31243;&#24207;&#65292;&#36890;&#36807;&#25512;&#33616;&#31995;&#32479;&#29983;&#25104;&#29992;&#25143;&#21487;&#33021;&#24863;&#20852;&#36259;&#30340;&#20070;&#31821;&#21015;&#34920;&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;&#34394;&#25311;&#29616;&#23454;&#65288;VR&#65289;&#30340;&#22270;&#24418;&#29992;&#25143;&#30028;&#38754;&#65288;GUI&#65289;&#20114;&#21160;&#26174;&#31034;&#65292;&#24110;&#21161;&#29992;&#25143;&#36827;&#34892;&#20915;&#31574;&#12290;&#26412;&#25991;&#37325;&#28857;&#30740;&#31350;&#20102;&#25512;&#33616;&#31995;&#32479;&#30340;&#35774;&#35745;&#21644;&#27979;&#35797;&#65292;&#37319;&#29992;&#24847;&#22823;&#21033;&#37117;&#28789;&#22270;&#20070;&#39302;&#32593;&#32476;&#36807;&#21435;9&#24180;&#25152;&#26377;&#29992;&#25143;&#20511;&#38405;&#30340;&#25968;&#25454;&#65292;&#20197;&#21450;&#35835;&#32773;&#22312;&#32447;&#31038;&#20132;&#24179;&#21488;Anobii&#25910;&#38598;&#30340;&#21453;&#39304;&#21644;&#20851;&#20110;&#35835;&#36807;&#20070;&#30340;&#38468;&#21152;&#20449;&#24687;&#12290;&#20973;&#20511;&#36825;&#31181;&#24322;&#26500;&#30340;&#25968;&#25454;&#65292;&#25105;&#20204;&#26500;&#24314;&#21644;&#35780;&#20272;&#20102;&#22522;&#20110;&#20869;&#23481;&#30340;&#65288;CB&#65289;&#21644;&#21327;&#21516;&#36807;&#28388;&#65288;CF&#65289;&#26041;&#27861;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;CF&#20248;&#20110;CB&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#26368;&#39640;47%&#12290;
&lt;/p&gt;
&lt;p&gt;
The Reading&amp;Machine project exploits the support of digitalization to increase the attractiveness of libraries and improve the users' experience. The project implements an application that helps the users in their decision-making process, providing recommendation system (RecSys)-generated lists of books the users might be interested in, and showing them through an interactive Virtual Reality (VR)-based Graphical User Interface (GUI). In this paper, we focus on the design and testing of the recommendation system, employing data about all users' loans over the past 9 years from the network of libraries located in Turin, Italy. In addition, we use data collected by the Anobii online social community of readers, who share their feedback and additional information about books they read. Armed with this heterogeneous data, we build and evaluate Content Based (CB) and Collaborative Filtering (CF) approaches. Our results show that the CF outperforms the CB approach, improving by up to 47\% the
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#21487;&#25193;&#23637;&#30340;&#22270;&#21367;&#31215;&#27169;&#22411;&#29992;&#20110;&#27969;&#24335;&#25512;&#33616;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#21452;&#36890;&#36947;&#35757;&#32451;&#31574;&#30053;&#65292;&#36890;&#36807;&#20174;&#26032;&#22270;&#24418;&#24555;&#29031;&#20013;&#20132;&#20114;&#22320;&#28155;&#21152;&#26032;&#33410;&#28857;&#21644;&#36793;&#30028;&#26469;&#33258;&#36866;&#24212;&#22320;&#25193;&#23637;&#27599;&#20010;&#22270;&#24418;&#21367;&#31215;&#23618;&#30340;&#25509;&#25910;&#23383;&#27573;&#12290;&#20316;&#32773;&#30340;&#26041;&#27861;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#31454;&#20105;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.11700</link><description>&lt;p&gt;
&#21160;&#24577;&#21487;&#25193;&#23637;&#22270;&#21367;&#31215;&#29992;&#20110;&#27969;&#24335;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Dynamically Expandable Graph Convolution for Streaming Recommendation. (arXiv:2303.11700v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11700
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#21487;&#25193;&#23637;&#30340;&#22270;&#21367;&#31215;&#27169;&#22411;&#29992;&#20110;&#27969;&#24335;&#25512;&#33616;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#21452;&#36890;&#36947;&#35757;&#32451;&#31574;&#30053;&#65292;&#36890;&#36807;&#20174;&#26032;&#22270;&#24418;&#24555;&#29031;&#20013;&#20132;&#20114;&#22320;&#28155;&#21152;&#26032;&#33410;&#28857;&#21644;&#36793;&#30028;&#26469;&#33258;&#36866;&#24212;&#22320;&#25193;&#23637;&#27599;&#20010;&#22270;&#24418;&#21367;&#31215;&#23618;&#30340;&#25509;&#25910;&#23383;&#27573;&#12290;&#20316;&#32773;&#30340;&#26041;&#27861;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#31454;&#20105;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#24050;&#34987;&#24191;&#27867;&#30740;&#31350;&#21644;&#37096;&#32626;&#65292;&#20197;&#20943;&#23569;&#20449;&#24687;&#36807;&#36733;&#24182;&#28385;&#36275;&#29992;&#25143;&#30340;&#22810;&#26679;&#21270;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#25512;&#33616;&#27169;&#22411;&#20165;&#36827;&#34892;&#19968;&#27425;&#35757;&#32451;-&#27979;&#35797;&#65292;&#26080;&#27861;&#36866;&#24212;&#19981;&#26029;&#21464;&#21270;&#30340;&#38656;&#27714;&#65292;&#32771;&#34385;&#21040;&#29992;&#25143;&#20559;&#22909;&#36716;&#31227;&#21644;&#29616;&#23454;&#19990;&#30028;&#20013;&#19981;&#26029;&#22686;&#38271;&#30340;&#29992;&#25143;&#21644;&#39033;&#30446;&#12290;&#38024;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#27969;&#24335;&#25512;&#33616;&#65292;&#24182;&#22312;&#23398;&#26415;&#30028;&#21644;&#20135;&#19994;&#30028;&#24341;&#36215;&#20102;&#26497;&#22823;&#20851;&#27880;&#12290;&#22312;&#36825;&#20123;&#26041;&#27861;&#20013;&#65292;&#36830;&#32493;&#22270;&#23398;&#20064;&#34987;&#24191;&#27867;&#35748;&#20026;&#26159;&#27969;&#24335;&#25512;&#33616;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#35201;&#20040;&#20381;&#36182;&#20110;&#21382;&#21490;&#25968;&#25454;&#30340;&#22238;&#25918;&#65292;&#32780;&#36825;&#22312;&#26085;&#30410;&#20005;&#26684;&#30340;&#25968;&#25454;&#30417;&#31649;&#19979;&#36890;&#24120;&#19981;&#21487;&#34892;&#65292;&#35201;&#20040;&#24456;&#23569;&#33021;&#35299;&#20915;&#36807;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#22256;&#38590;&#65292;&#20174;&#8220;&#27169;&#22411;&#38548;&#31163;&#8221;&#30340;&#35282;&#24230;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21160;&#24577;&#21487;&#25193;&#23637;&#22270;&#21367;&#31215;&#65288;DEGC&#65289;&#31639;&#27861;&#29992;&#20110;&#27969;&#24335;&#25512;&#33616;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;DEGC&#23558;&#28436;&#21464;&#30340;&#22270;&#24418;&#24314;&#27169;&#20026;&#19968;&#31995;&#21015;&#22270;&#24418;&#24555;&#29031;&#65292;&#24182;&#36890;&#36807;&#20174;&#26032;&#22270;&#24418;&#24555;&#29031;&#20013;&#20132;&#20114;&#22320;&#28155;&#21152;&#26032;&#33410;&#28857;&#21644;&#36793;&#32536;&#26469;&#33258;&#36866;&#24212;&#22320;&#25193;&#23637;&#27599;&#20010;&#22270;&#24418;&#21367;&#31215;&#23618;&#30340;&#25509;&#25910;&#23383;&#27573;&#12290;&#27492;&#22806;&#65292;DEGC&#24341;&#20837;&#20102;&#19968;&#31181;&#21452;&#36890;&#36947;&#35757;&#32451;&#31574;&#30053;&#65292;&#21033;&#29992;&#21382;&#21490;&#21644;&#24403;&#21069;&#22270;&#24418;&#24555;&#29031;&#30340;&#20869;&#22312;&#8220;&#21018;&#24230;&#8221;&#21644;&#8220;&#20114;&#34917;&#24615;&#8221;&#65292;&#20998;&#21035;&#36827;&#34892;&#35757;&#32451;&#12290;&#22312;&#20004;&#20010;&#22823;&#35268;&#27169;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;DEGC&#22312;&#25512;&#33616;&#20934;&#30830;&#24615;&#21644;&#27169;&#22411;&#31283;&#23450;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#31454;&#20105;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Personalized recommender systems have been widely studied and deployed to reduce information overload and satisfy users' diverse needs. However, conventional recommendation models solely conduct a one-time training-test fashion and can hardly adapt to evolving demands, considering user preference shifts and ever-increasing users and items in the real world. To tackle such challenges, the streaming recommendation is proposed and has attracted great attention recently. Among these, continual graph learning is widely regarded as a promising approach for the streaming recommendation by academia and industry. However, existing methods either rely on the historical data replay which is often not practical under increasingly strict data regulations, or can seldom solve the \textit{over-stability} issue. To overcome these difficulties, we propose a novel \textbf{D}ynamically \textbf{E}xpandable \textbf{G}raph \textbf{C}onvolution (DEGC) algorithm from a \textit{model isolation} perspective for
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#21319;&#32423;&#20102;ByteCover&#31995;&#32479;&#65292;&#35774;&#35745;&#20102;&#23616;&#37096;&#23545;&#40784;&#25439;&#22833;&#27169;&#22359;&#21644;&#20004;&#32423;&#29305;&#24449;&#26816;&#32034;&#31649;&#36947;&#12290;ByteCover3&#22312;&#30701;&#38899;&#20048;&#26597;&#35810;&#30340;&#35782;&#21035;&#24615;&#33021;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20987;&#36133;&#20102;&#25152;&#26377;&#27604;&#36739;&#26041;&#27861;&#65292;&#21253;&#25324;&#20854;&#20197;&#21069;&#30340;&#29256;&#26412;&#12290;</title><link>http://arxiv.org/abs/2303.11692</link><description>&lt;p&gt;
ByteCover3: &#30701;&#26597;&#35810;&#19979;&#20934;&#30830;&#30340;&#32763;&#21809;&#27468;&#26354;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
ByteCover3: Accurate Cover Song Identification on Short Queries. (arXiv:2303.11692v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11692
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#21319;&#32423;&#20102;ByteCover&#31995;&#32479;&#65292;&#35774;&#35745;&#20102;&#23616;&#37096;&#23545;&#40784;&#25439;&#22833;&#27169;&#22359;&#21644;&#20004;&#32423;&#29305;&#24449;&#26816;&#32034;&#31649;&#36947;&#12290;ByteCover3&#22312;&#30701;&#38899;&#20048;&#26597;&#35810;&#30340;&#35782;&#21035;&#24615;&#33021;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20987;&#36133;&#20102;&#25152;&#26377;&#27604;&#36739;&#26041;&#27861;&#65292;&#21253;&#25324;&#20854;&#20197;&#21069;&#30340;&#29256;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#24050;&#25104;&#20026;&#32763;&#21809;&#27468;&#26354;&#35782;&#21035;(CSI)&#30340;&#33539;&#20363;&#65292;&#20854;&#20013;ByteCover&#31995;&#32479;&#22312;&#25152;&#26377;&#20027;&#27969;CSI&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#25104;&#26524;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#30701;&#35270;&#39057;&#30340;&#20852;&#36215;&#65292;&#35768;&#22810;&#30495;&#23454;&#24212;&#29992;&#38656;&#35201;&#23558;&#30701;&#38899;&#20048;&#29255;&#27573;&#19982;&#25968;&#25454;&#24211;&#20013;&#30340;&#23436;&#25972;&#38899;&#20048;&#36712;&#36947;&#30456;&#21305;&#37197;&#65292;&#36825;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#30340;&#38382;&#39064;&#65292;&#38656;&#35201;&#31561;&#24453;&#24037;&#19994;&#32423;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#20043;&#21069;&#30340;ByteCover&#31995;&#32479;&#21319;&#32423;&#20026;ByteCover3&#65292;&#21033;&#29992;&#23616;&#37096;&#29305;&#24449;&#36827;&#19968;&#27493;&#25552;&#39640;&#30701;&#38899;&#20048;&#26597;&#35810;&#30340;&#35782;&#21035;&#24615;&#33021;&#12290;ByteCover3&#35774;&#35745;&#20102;&#19968;&#20010;&#23616;&#37096;&#23545;&#40784;&#25439;&#22833;(LAL)&#27169;&#22359;&#21644;&#19968;&#20010;&#20004;&#38454;&#27573;&#29305;&#24449;&#26816;&#32034;&#31649;&#36947;&#65292;&#20351;&#31995;&#32479;&#33021;&#22815;&#20197;&#26356;&#31934;&#30830;&#21644;&#39640;&#25928;&#30340;&#26041;&#24335;&#36827;&#34892;CSI&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#19981;&#21516;&#30340;&#22522;&#20934;&#35774;&#32622;&#23545;ByteCover3&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#20854;&#20013;ByteCover3&#20987;&#36133;&#20102;&#25152;&#26377;&#27604;&#36739;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#20854;&#20197;&#21069;&#30340;&#29256;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning based methods have become a paradigm for cover song identification (CSI) in recent years, where the ByteCover systems have achieved state-of-the-art results on all the mainstream datasets of CSI. However, with the burgeon of short videos, many real-world applications require matching short music excerpts to full-length music tracks in the database, which is still under-explored and waiting for an industrial-level solution. In this paper, we upgrade the previous ByteCover systems to ByteCover3 that utilizes local features to further improve the identification performance of short music queries. ByteCover3 is designed with a local alignment loss (LAL) module and a two-stage feature retrieval pipeline, allowing the system to perform CSI in a more precise and efficient way. We evaluated ByteCover3 on multiple datasets with different benchmark settings, where ByteCover3 beat all the compared methods including its previous versions.
&lt;/p&gt;</description></item><item><title>&#35813;&#32508;&#36848;&#20174;&#29702;&#35770;&#30340;&#35282;&#24230;&#31995;&#32479;&#32508;&#36848;&#22240;&#26524;&#25512;&#26029;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#65292;&#23558;&#29616;&#26377;&#25991;&#29486;&#20998;&#20026;&#19977;&#31867;&#36827;&#34892;&#35752;&#35770;&#65292;&#24182;&#25506;&#35752;&#20102;&#21508;&#33258;&#30340;&#20248;&#32570;&#28857;&#12290;&#38024;&#23545;&#25512;&#33616;&#31995;&#32479;&#30740;&#31350;&#20154;&#21592;&#23545;&#22240;&#26524;&#20851;&#31995;&#30340;&#38476;&#29983;&#31243;&#24230;&#65292;&#26412;&#25991;&#35797;&#22270;&#25552;&#39640;&#35835;&#32773;&#23545;&#22240;&#26524;&#20851;&#31995;&#30340;&#35748;&#35782;&#65292;&#24182;&#20026;&#20170;&#21518;&#30340;&#30740;&#31350;&#25552;&#20379;&#25351;&#23548;&#21644;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.11666</link><description>&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#22240;&#26524;&#25512;&#26029;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Causal Inference for Recommendation. (arXiv:2303.11666v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11666
&lt;/p&gt;
&lt;p&gt;
&#35813;&#32508;&#36848;&#20174;&#29702;&#35770;&#30340;&#35282;&#24230;&#31995;&#32479;&#32508;&#36848;&#22240;&#26524;&#25512;&#26029;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#65292;&#23558;&#29616;&#26377;&#25991;&#29486;&#20998;&#20026;&#19977;&#31867;&#36827;&#34892;&#35752;&#35770;&#65292;&#24182;&#25506;&#35752;&#20102;&#21508;&#33258;&#30340;&#20248;&#32570;&#28857;&#12290;&#38024;&#23545;&#25512;&#33616;&#31995;&#32479;&#30740;&#31350;&#20154;&#21592;&#23545;&#22240;&#26524;&#20851;&#31995;&#30340;&#38476;&#29983;&#31243;&#24230;&#65292;&#26412;&#25991;&#35797;&#22270;&#25552;&#39640;&#35835;&#32773;&#23545;&#22240;&#26524;&#20851;&#31995;&#30340;&#35748;&#35782;&#65292;&#24182;&#20026;&#20170;&#21518;&#30340;&#30740;&#31350;&#25552;&#20379;&#25351;&#23548;&#21644;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22240;&#26524;&#25512;&#26029;&#24341;&#36215;&#20102;&#25512;&#33616;&#31995;&#32479;&#30740;&#31350;&#20154;&#21592;&#30340;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#22240;&#26524;&#25512;&#26029;&#20998;&#26512;&#22240;&#26524;&#20851;&#31995;&#24182;&#22312;&#22810;&#20010;&#39046;&#22495;&#20855;&#26377;&#24191;&#27867;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#22240;&#26524;&#25512;&#26029;&#21487;&#20197;&#23545;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#22240;&#26524;&#20851;&#31995;&#36827;&#34892;&#24314;&#27169;&#22914;&#28151;&#28102;&#25928;&#24212;&#65292;&#24182;&#22788;&#29702;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;&#21644;&#25968;&#25454;&#22686;&#24378;&#31561;&#21453;&#20107;&#23454;&#38382;&#39064;&#12290;&#34429;&#28982;&#24050;&#32463;&#26377;&#19968;&#20123;&#26377;&#20215;&#20540;&#30340;&#22240;&#26524;&#25512;&#33616;&#32508;&#36848;&#65292;&#20294;&#26159;&#36825;&#20123;&#32508;&#36848;&#30456;&#23545;&#23396;&#31435;&#22320;&#20171;&#32461;&#20102;&#26041;&#27861;&#65292;&#24182;&#32570;&#20047;&#23545;&#29616;&#26377;&#26041;&#27861;&#30340;&#29702;&#35770;&#20998;&#26512;&#12290;&#30001;&#20110;&#25512;&#33616;&#31995;&#32479;&#30740;&#31350;&#20154;&#21592;&#23545;&#22240;&#26524;&#20851;&#31995;&#30340;&#38476;&#29983;&#31243;&#24230;&#65292;&#20174;&#22240;&#26524;&#29702;&#35770;&#30340;&#35282;&#24230;&#20840;&#38754;&#23457;&#26597;&#30456;&#20851;&#30740;&#31350;&#23545;&#20110;&#25552;&#20986;&#26032;&#30340;&#23454;&#36341;&#26041;&#27861;&#20855;&#26377;&#25351;&#23548;&#24847;&#20041;&#65292;&#20063;&#26159;&#24517;&#35201;&#30340;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#36825;&#31687;&#32508;&#36848;&#35797;&#22270;&#20174;&#29702;&#35770;&#30340;&#35282;&#24230;&#31995;&#32479;&#32508;&#36848;&#36825;&#19968;&#39046;&#22495;&#30340;&#26368;&#26032;&#35770;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, causal inference has attracted increasing attention from researchers of recommender systems (RS), which analyzes the relationship between a cause and its effect and has a wide range of real-world applications in multiple fields. Causal inference can model the causality in recommender systems like confounding effects and deal with counterfactual problems such as offline policy evaluation and data augmentation. Although there are already some valuable surveys on causal recommendations, these surveys introduce approaches in a relatively isolated way and lack theoretical analysis of existing methods. Due to the unfamiliarity with causality to RS researchers, it is both necessary and challenging to comprehensively review the relevant studies from the perspective of causal theory, which might be instructive for the readers to propose new approaches in practice. This survey attempts to provide a systematic review of up-to-date papers in this area from a theoretical standpoint. First
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#21435;&#20559;&#24046;&#25512;&#33616;&#31995;&#32479;&#26041;&#27861;&#65292;&#30452;&#25509;&#26368;&#23567;&#21270;&#29702;&#24819;&#30446;&#26631;&#20989;&#25968;&#19978;&#38480;&#20540;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#35299;&#20915;&#31995;&#32479;&#35825;&#21457;&#30340;&#20559;&#24046;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.11574</link><description>&lt;p&gt;
&#22312;&#38543;&#26426;&#25968;&#25454;&#38598;&#19978;&#38480;&#21046;&#31995;&#32479;&#35825;&#23548;&#20559;&#35265;&#30340;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Bounding System-Induced Biases in Recommender Systems with A Randomized Dataset. (arXiv:2303.11574v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11574
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#21435;&#20559;&#24046;&#25512;&#33616;&#31995;&#32479;&#26041;&#27861;&#65292;&#30452;&#25509;&#26368;&#23567;&#21270;&#29702;&#24819;&#30446;&#26631;&#20989;&#25968;&#19978;&#38480;&#20540;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#35299;&#20915;&#31995;&#32479;&#35825;&#21457;&#30340;&#20559;&#24046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38543;&#26426;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#21435;&#20559;&#24046;&#25512;&#33616;&#24050;&#32463;&#26174;&#31034;&#20986;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;&#20294;&#19982;&#27809;&#26377;&#38543;&#26426;&#25968;&#25454;&#38598;&#30340;&#20854;&#20182;&#26356;&#20026;&#30740;&#31350;&#22522;&#30784;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#23427;&#20173;&#32570;&#20047;&#26356;&#22810;&#30340;&#29702;&#35770;&#27934;&#35265;&#25110;&#29702;&#24819;&#30340;&#20248;&#21270;&#30446;&#26631;&#20989;&#25968;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#20174;&#26032;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#21435;&#20559;&#24046;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#30452;&#25509;&#26368;&#23567;&#21270;&#29702;&#24819;&#30446;&#26631;&#20989;&#25968;&#30340;&#19978;&#38480;&#20540;&#65292;&#36825;&#26377;&#21033;&#20110;&#26356;&#22909;&#22320;&#35299;&#20915;&#31995;&#32479;&#35825;&#21457;&#30340;&#20559;&#24046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Debiased recommendation with a randomized dataset has shown very promising results in mitigating the system-induced biases. However, it still lacks more theoretical insights or an ideal optimization objective function compared with the other more well studied route without a randomized dataset. To bridge this gap, we study the debiasing problem from a new perspective and propose to directly minimize the upper bound of an ideal objective function, which facilitates a better potential solution to the system-induced biases. Firstly, we formulate a new ideal optimization objective function with a randomized dataset. Secondly, according to the prior constraints that an adopted loss function may satisfy, we derive two different upper bounds of the objective function, i.e., a generalization error bound with the triangle inequality and a generalization error bound with the separability. Thirdly, we show that most existing related methods can be regarded as the insufficient optimization of thes
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#22270;&#30340;&#21327;&#21516;&#36807;&#28388;&#27169;&#22411;SimRec&#65292;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#21644;&#23545;&#27604;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#25945;&#24072;GNN&#27169;&#22411;&#19982;&#36731;&#37327;&#32423;&#23398;&#29983;&#32593;&#32476;&#20043;&#38388;&#30340;&#33258;&#36866;&#24212;&#30693;&#35782;&#36716;&#31227;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#29616;&#26377;&#22522;&#20110;GNN&#30340;CF&#27169;&#22411;&#21487;&#33021;&#20986;&#29616;&#30340;&#36807;&#24230;&#24179;&#28369;&#21644;&#22122;&#22768;&#25928;&#24212;&#30340;&#38382;&#39064;&#65292;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#37117;&#36229;&#36807;&#20102;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.08537</link><description>&lt;p&gt;
&#26080;&#22270;&#21327;&#21516;&#36807;&#28388;
&lt;/p&gt;
&lt;p&gt;
Graph-less Collaborative Filtering. (arXiv:2303.08537v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08537
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#22270;&#30340;&#21327;&#21516;&#36807;&#28388;&#27169;&#22411;SimRec&#65292;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#21644;&#23545;&#27604;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#25945;&#24072;GNN&#27169;&#22411;&#19982;&#36731;&#37327;&#32423;&#23398;&#29983;&#32593;&#32476;&#20043;&#38388;&#30340;&#33258;&#36866;&#24212;&#30693;&#35782;&#36716;&#31227;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#29616;&#26377;&#22522;&#20110;GNN&#30340;CF&#27169;&#22411;&#21487;&#33021;&#20986;&#29616;&#30340;&#36807;&#24230;&#24179;&#28369;&#21644;&#22122;&#22768;&#25928;&#24212;&#30340;&#38382;&#39064;&#65292;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#37117;&#36229;&#36807;&#20102;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#24050;&#32463;&#22312;&#21327;&#21516;&#36807;&#28388;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20102;&#20854;&#22312;&#22270;&#32467;&#26500;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#25968;&#25454;&#19978;&#34920;&#31034;&#23398;&#20064;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20302;&#36890;Laplacian&#24179;&#28369;&#31639;&#23376;&#30340;&#36807;&#24230;&#24179;&#28369;&#21644;&#22122;&#22768;&#25928;&#24212;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;GNN&#30340;CF&#27169;&#22411;&#21487;&#33021;&#20250;&#29983;&#25104;&#38590;&#20197;&#21306;&#20998;&#19988;&#19981;&#20934;&#30830;&#30340;&#29992;&#25143;&#65288;&#29289;&#21697;&#65289;&#34920;&#31034;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#21327;&#21516;&#36807;&#28388;&#27169;&#22411;&#65288;SimRec&#65289;&#65292;&#23558;&#30693;&#35782;&#33976;&#39311;&#21644;&#23545;&#27604;&#23398;&#20064;&#30340;&#33021;&#21147;&#34701;&#21512;&#22312;&#19968;&#36215;&#65292;&#23454;&#29616;&#20102;&#25945;&#24072;GNN&#27169;&#22411;&#19982;&#36731;&#37327;&#32423;&#23398;&#29983;&#32593;&#32476;&#20043;&#38388;&#30340;&#33258;&#36866;&#24212;&#30693;&#35782;&#36716;&#31227;&#65292;&#22312;&#19981;&#38656;&#35201;&#26500;&#24314;&#22270;&#30340;&#24773;&#20917;&#19979;&#26356;&#22909;&#22320;&#21457;&#29616;&#29992;&#25143;&#21644;&#29289;&#21697;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) have shown the power in representation learning over graph-structured user-item interaction data for collaborative filtering (CF) task. However, with their inherently recursive message propagation among neighboring nodes, existing GNN-based CF models may generate indistinguishable and inaccurate user (item) representations due to the over-smoothing and noise effect with low-pass Laplacian smoothing operators. In addition, the recursive information propagation with the stacked aggregators in the entire graph structures may result in poor scalability in practical applications. Motivated by these limitations, we propose a simple and effective collaborative filtering model (SimRec) that marries the power of knowledge distillation and contrastive learning. In SimRec, adaptive transferring knowledge is enabled between the teacher GNN model and a lightweight student network, to not only preserve the global collaborative signals, but also address the over-smoothing
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;AutoCF&#30340;&#33258;&#21160;&#25512;&#33616;&#31995;&#32479;&#65292;&#23427;&#20351;&#29992;&#29983;&#25104;&#24615;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#21644;&#21487;&#23398;&#20064;&#25968;&#25454;&#22686;&#24378;&#33539;&#24335;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#65292;&#20197;&#25552;&#39640;&#25512;&#33616;&#31995;&#32479;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.07797</link><description>&lt;p&gt;
&#33258;&#21160;&#33258;&#30417;&#30563;&#23398;&#20064;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Automated Self-Supervised Learning for Recommendation. (arXiv:2303.07797v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07797
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;AutoCF&#30340;&#33258;&#21160;&#25512;&#33616;&#31995;&#32479;&#65292;&#23427;&#20351;&#29992;&#29983;&#25104;&#24615;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#21644;&#21487;&#23398;&#20064;&#25968;&#25454;&#22686;&#24378;&#33539;&#24335;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#65292;&#20197;&#25552;&#39640;&#25512;&#33616;&#31995;&#32479;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#24050;&#25104;&#20026;&#21327;&#21516;&#36807;&#28388;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;&#20026;&#20102;&#25552;&#39640;&#22312;&#26377;&#38480;&#26631;&#27880;&#25968;&#25454;&#19978;&#30340;&#34920;&#29616;&#65292;&#23545;&#27604;&#23398;&#20064;&#21560;&#24341;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#65292;&#21463;&#30410;&#20110;&#22270;&#24418;&#21327;&#21516;&#36807;&#28388;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#23545;&#27604;&#26041;&#27861;&#30340;&#25104;&#21151;&#19982;&#25163;&#21160;&#29983;&#25104;&#26377;&#25928;&#23545;&#27604;&#35270;&#22270;&#26469;&#25191;&#34892;&#21551;&#21457;&#24335;&#25968;&#25454;&#22686;&#24378;&#23494;&#20999;&#30456;&#20851;&#65292;&#36825;&#19981;&#21033;&#20110;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#21644;&#19979;&#28216;&#25512;&#33616;&#20219;&#21153;&#20013;&#27867;&#21270;&#65292;&#20063;&#38590;&#20197;&#36866;&#24212;&#20110;&#25968;&#25454;&#22686;&#24378;&#21644;&#22122;&#22768;&#25200;&#21160;&#30340;&#40065;&#26834;&#24615;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#37325;&#35201;&#32570;&#21475;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#33258;&#21160;&#21327;&#21516;&#36807;&#28388;&#31995;&#32479;&#65288;AutoCF&#65289;&#65292;&#20197;&#33258;&#21160;&#25191;&#34892;&#25512;&#33616;&#30340;&#25968;&#25454;&#22686;&#24378;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#20855;&#26377;&#21487;&#23398;&#20064;&#22686;&#24378;&#33539;&#24335;&#30340;&#29983;&#25104;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#36825;&#26377;&#21161;&#20110;&#37325;&#35201;&#33258;&#30417;&#30563;&#20449;&#21495;&#30340;&#33258;&#21160;&#31934;&#39311;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) have emerged as the state-of-the-art paradigm for collaborative filtering (CF). To improve the representation quality over limited labeled data, contrastive learning has attracted attention in recommendation and benefited graph-based CF model recently. However, the success of most contrastive methods heavily relies on manually generating effective contrastive views for heuristic-based data augmentation. This does not generalize across different datasets and downstream recommendation tasks, which is difficult to be adaptive for data augmentation and robust to noise perturbation. To fill this crucial gap, this work proposes a unified Automated Collaborative Filtering (AutoCF) to automatically perform data augmentation for recommendation. Specifically, we focus on the generative self-supervised learning framework with a learnable augmentation paradigm that benefits the automated distillation of important self-supervised signals. To enhance the representation d
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20174;&#20135;&#21697;&#35780;&#35770;&#20013;&#33258;&#21160;&#25552;&#21462;&#20010;&#24615;&#36164;&#26009;&#65292;&#24182;&#36890;&#36807;&#35774;&#35745;&#19977;&#31181;&#19978;&#19979;&#25991;&#24863;&#30693;&#25512;&#33616;&#20307;&#31995;&#32467;&#26500;&#26469;&#21033;&#29992;&#36825;&#20123;&#20010;&#24615;&#36164;&#26009;&#65292;&#39564;&#35777;&#20102;&#20010;&#24615;&#36164;&#26009;&#33021;&#22815;&#25552;&#39640;&#25512;&#33616;&#31995;&#32479;&#24615;&#33021;&#30340;&#20551;&#35774;&#65292;&#30740;&#31350;&#21457;&#29616;&#19981;&#21516;&#31867;&#22411;&#30340;&#20010;&#24615;&#23545;&#25512;&#33616;&#31995;&#32479;&#24615;&#33021;&#30340;&#36129;&#29486;&#26159;&#19981;&#21516;&#30340;&#12290;</title><link>http://arxiv.org/abs/2303.05039</link><description>&lt;p&gt;
&#20174;&#20135;&#21697;&#35780;&#35770;&#20013;&#25512;&#26029;&#29992;&#25143;&#20010;&#24615;&#20197;&#25552;&#39640;&#25512;&#33616;&#31995;&#32479;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Improving Recommendation Systems with User Personality Inferred from Product Reviews. (arXiv:2303.05039v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05039
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20174;&#20135;&#21697;&#35780;&#35770;&#20013;&#33258;&#21160;&#25552;&#21462;&#20010;&#24615;&#36164;&#26009;&#65292;&#24182;&#36890;&#36807;&#35774;&#35745;&#19977;&#31181;&#19978;&#19979;&#25991;&#24863;&#30693;&#25512;&#33616;&#20307;&#31995;&#32467;&#26500;&#26469;&#21033;&#29992;&#36825;&#20123;&#20010;&#24615;&#36164;&#26009;&#65292;&#39564;&#35777;&#20102;&#20010;&#24615;&#36164;&#26009;&#33021;&#22815;&#25552;&#39640;&#25512;&#33616;&#31995;&#32479;&#24615;&#33021;&#30340;&#20551;&#35774;&#65292;&#30740;&#31350;&#21457;&#29616;&#19981;&#21516;&#31867;&#22411;&#30340;&#20010;&#24615;&#23545;&#25512;&#33616;&#31995;&#32479;&#24615;&#33021;&#30340;&#36129;&#29486;&#26159;&#19981;&#21516;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#26159;&#24433;&#21709;&#20154;&#20204;&#20915;&#31574;&#30340;&#24515;&#29702;&#22240;&#32032;&#65292;&#21453;&#26144;&#20986;&#20154;&#20204;&#30340;&#20559;&#22909;&#12290;&#25105;&#20204;&#20551;&#35774;&#20934;&#30830;&#22320;&#24314;&#27169;&#29992;&#25143;&#20010;&#24615;&#33021;&#22815;&#25552;&#39640;&#25512;&#33616;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#33719;&#21462;&#36825;&#26679;&#30340;&#20010;&#24615;&#36164;&#26009;&#26082;&#25935;&#24863;&#21448;&#26114;&#36149;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#20174;&#20844;&#20849;&#20135;&#21697;&#35780;&#35770;&#25991;&#26412;&#20013;&#33258;&#21160;&#25552;&#21462;&#29992;&#25143;&#20010;&#24615;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#35774;&#35745;&#21644;&#35780;&#20272;&#20102;&#19977;&#31181;&#19978;&#19979;&#25991;&#24863;&#30693;&#25512;&#33616;&#20307;&#31995;&#32467;&#26500;&#65292;&#21033;&#29992;&#36825;&#20123;&#20010;&#24615;&#36164;&#26009;&#26469;&#27979;&#35797;&#25105;&#20204;&#30340;&#20551;&#35774;&#12290;&#22312;&#25105;&#20204;&#36129;&#29486;&#30340;&#20004;&#20010;&#26032;&#30340;&#20010;&#24615;&#25968;&#25454;&#38598; -  Amazon &#32654;&#23481;&#21644; Amazon &#38899;&#20048;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#20551;&#35774;&#65292;&#26174;&#31034;&#20986; 3--28% &#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#19981;&#21516;&#31867;&#22411;&#30340;&#20010;&#24615;&#23545;&#25512;&#33616;&#31995;&#32479;&#24615;&#33021;&#30340;&#36129;&#29486;&#26159;&#19981;&#21516;&#30340;&#65306;&#22312;&#38899;&#20048;&#25512;&#33616;&#20013;&#65292;&#24320;&#25918;&#21644;&#22806;&#21521;&#30340;&#20010;&#24615;&#26368;&#26377;&#24110;&#21161;&#65292;&#32780;&#22312;&#32654;&#23481;&#20135;&#21697;&#25512;&#33616;&#20013;&#65292;&#26377;&#36131;&#20219;&#24515;&#30340;&#20010;&#24615;&#26368;&#26377;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;
Personality is a psychological factor that reflects people's preferences, which in turn influences their decision-making. We hypothesize that accurate modeling of users' personalities improves recommendation systems' performance. However, acquiring such personality profiles is both sensitive and expensive. We address this problem by introducing a novel method to automatically extract personality profiles from public product review text. We then design and assess three context-aware recommendation architectures that leverage the profiles to test our hypothesis.  Experiments on our two newly contributed personality datasets -Amazon-beauty and Amazon-music -- validate our hypothesis, showing performance boosts of 3--28%.Our analysis uncovers that varying personality types contribute differently to recommendation performance: open and extroverted personalities are most helpful in music recommendation, while a conscientious personality is most helpful in beauty product recommendation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21551;&#21457;&#24335;&#30340;Few-shot&#24847;&#22270;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#24847;&#22270;&#26816;&#27979;&#37325;&#26032;&#23450;&#20041;&#20026;&#19968;&#20010;&#38382;&#39064;-&#22238;&#31572;&#26816;&#32034;&#20219;&#21153;&#26469;&#35299;&#20915;&#35821;&#20041;&#30456;&#20284;&#30340;&#32454;&#31890;&#24230;&#24847;&#22270;&#38382;&#39064;&#65292;&#32467;&#26524;&#22312;&#19977;&#20010;few-shot&#24847;&#22270;&#26816;&#27979;&#22522;&#20934;&#27979;&#35797;&#19978;&#21462;&#24471;&#20102;&#26368;&#20248;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.01593</link><description>&lt;p&gt;
QAID&#65306;&#21551;&#21457;&#24335;&#30340;Few-shot&#24847;&#22270;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
QAID: Question Answering Inspired Few-shot Intent Detection. (arXiv:2303.01593v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01593
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21551;&#21457;&#24335;&#30340;Few-shot&#24847;&#22270;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#24847;&#22270;&#26816;&#27979;&#37325;&#26032;&#23450;&#20041;&#20026;&#19968;&#20010;&#38382;&#39064;-&#22238;&#31572;&#26816;&#32034;&#20219;&#21153;&#26469;&#35299;&#20915;&#35821;&#20041;&#30456;&#20284;&#30340;&#32454;&#31890;&#24230;&#24847;&#22270;&#38382;&#39064;&#65292;&#32467;&#26524;&#22312;&#19977;&#20010;few-shot&#24847;&#22270;&#26816;&#27979;&#22522;&#20934;&#27979;&#35797;&#19978;&#21462;&#24471;&#20102;&#26368;&#20248;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24847;&#22270;&#26816;&#27979;&#28041;&#21450;&#21040;&#19968;&#20123;&#35821;&#20041;&#30456;&#20284;&#30340;&#32454;&#31890;&#24230;&#24847;&#22270;&#65292;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#24847;&#22270;&#26816;&#27979;&#37325;&#26032;&#23450;&#20041;&#20026;&#19968;&#20010;&#38382;&#39064;-&#22238;&#31572;&#26816;&#32034;&#20219;&#21153;&#65292;&#23558;&#35805;&#35821;&#21644;&#24847;&#22270;&#21517;&#20316;&#20026;&#38382;&#39064;&#21644;&#31572;&#26696;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#19968;&#20010;&#38382;&#39064;-&#22238;&#31572;&#26816;&#32034;&#20307;&#31995;&#32467;&#26500;&#65292;&#24182;&#37319;&#29992;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#22521;&#35757;&#27169;&#24335;&#65292;&#20854;&#20013;&#21253;&#25324;&#25209;&#37327;&#23545;&#27604;&#25439;&#22833;&#12290;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#65292;&#25105;&#20204;&#36890;&#36807;&#33258;&#25105;&#30417;&#30563;&#22521;&#35757;&#26469;&#25913;&#21892;&#26597;&#35810;&#34920;&#31034;&#12290;&#28982;&#21518;&#65292;&#22312;&#24494;&#35843;&#38454;&#27573;&#20013;&#65292;&#25105;&#20204;&#22686;&#21152;&#20102;&#26597;&#35810;&#21644;&#21516;&#19968;&#24847;&#22270;&#31572;&#26696;&#20043;&#38388;&#30340;&#19978;&#19979;&#25991;&#21270;&#20196;&#29260;&#32423;&#30456;&#20284;&#24230;&#20998;&#25968;&#12290;&#25105;&#20497;&#22312;&#19977;&#20010;few-shot&#24847;&#22270;&#26816;&#27979;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#32467;&#26524;&#36798;&#21040;&#20102;&#26368;&#20248;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intent detection with semantically similar fine-grained intents is a challenging task. To address it, we reformulate intent detection as a question-answering retrieval task by treating utterances and intent names as questions and answers. To that end, we utilize a question-answering retrieval architecture and adopt a two stages training schema with batch contrastive loss. In the pre-training stage, we improve query representations through self-supervised training. Then, in the fine-tuning stage, we increase contextualized token-level similarity scores between queries and answers from the same intent. Our results on three few-shot intent detection benchmarks achieve state-of-the-art performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#24402;&#19968;&#21270;&#26041;&#26696;Adap-$\tau$&#65292;&#36890;&#36807;&#21160;&#24577;&#35843;&#33410;&#27599;&#20010;&#29992;&#25143;-&#27599;&#20010;&#29289;&#21697;&#23545;&#30340;&#23884;&#20837;&#24133;&#24230;&#65292;&#23454;&#29616;&#20102;&#29702;&#24819;&#30340;&#25512;&#33616;&#24615;&#33021;&#65292;&#26041;&#27861;&#22312;&#22235;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#19978;&#37117;&#36229;&#36807;&#20102;&#22522;&#20934;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.04775</link><description>&lt;p&gt;
Adap-$\tau$:&#33258;&#36866;&#24212;&#35843;&#25972;&#23884;&#20837;&#30340;&#24133;&#24230;&#29992;&#20110;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Adap-$\tau$: Adaptively Modulating Embedding Magnitude for Recommendation. (arXiv:2302.04775v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04775
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#24402;&#19968;&#21270;&#26041;&#26696;Adap-$\tau$&#65292;&#36890;&#36807;&#21160;&#24577;&#35843;&#33410;&#27599;&#20010;&#29992;&#25143;-&#27599;&#20010;&#29289;&#21697;&#23545;&#30340;&#23884;&#20837;&#24133;&#24230;&#65292;&#23454;&#29616;&#20102;&#29702;&#24819;&#30340;&#25512;&#33616;&#24615;&#33021;&#65292;&#26041;&#27861;&#22312;&#22235;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#19978;&#37117;&#36229;&#36807;&#20102;&#22522;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#26469;&#65292;&#22522;&#20110;&#23884;&#20837;&#30340;&#26041;&#27861;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#12290;&#23613;&#31649;&#23427;&#20204;&#30340;&#24615;&#33021;&#36824;&#19981;&#38169;&#65292;&#20294;&#25105;&#20204;&#35748;&#20026;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#23384;&#22312;&#19968;&#20010;&#28508;&#22312;&#30340;&#38480;&#21046;&#8212;&#8212;&#23884;&#20837;&#24133;&#24230;&#27809;&#26377;&#26126;&#30830;&#35843;&#33410;&#65292;&#36825;&#21487;&#33021;&#21152;&#21095;&#27969;&#34892;&#24230;&#20559;&#35265;&#21644;&#35757;&#32451;&#19981;&#31283;&#23450;&#24615;&#65292;&#20174;&#32780;&#38459;&#30861;&#27169;&#22411;&#20570;&#20986;&#22909;&#30340;&#25512;&#33616;&#12290;&#36825;&#20419;&#20351;&#25105;&#20204;&#21033;&#29992;&#23884;&#20837;&#24402;&#19968;&#21270;&#26469;&#25512;&#33616;&#12290;&#36890;&#36807;&#23558;&#29992;&#25143;/&#29289;&#21697;&#23884;&#20837;&#24402;&#19968;&#21270;&#20026;&#29305;&#23450;&#20540;&#65292;&#25105;&#20204;&#22312;&#22235;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#19978;&#23454;&#35777;&#35266;&#23519;&#21040;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#25552;&#21319;&#65288;&#24179;&#22343;9&#65285;&#65289;&#12290;&#34429;&#28982;&#36825;&#26159;&#20196;&#20154;&#40723;&#33310;&#30340;&#65292;&#20294;&#25105;&#20204;&#20063;&#25581;&#31034;&#20102;&#22312;&#25512;&#33616;&#20013;&#24212;&#29992;&#24402;&#19968;&#21270;&#30340;&#20005;&#37325;&#23616;&#38480;&#24615;&#8212;&#8212;&#24615;&#33021;&#39640;&#24230;&#25935;&#24863;&#20110;&#25511;&#21046;&#26631;&#20934;&#21270;&#23884;&#20837;&#27604;&#20363;&#30340;&#28201;&#24230;&#964;&#30340;&#36873;&#25321;&#12290;&#20026;&#20102;&#20805;&#20998;&#21457;&#25381;&#24402;&#19968;&#21270;&#30340;&#20248;&#28857;&#24182;&#36991;&#20813;&#20854;&#23616;&#38480;&#24615;&#65292;&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22914;&#20309;&#33258;&#36866;&#24212;&#35774;&#32622;&#36866;&#24403;&#30340;&#964;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#25551;&#36848;&#20102;&#25512;&#33616;&#20013;&#24402;&#19968;&#21270;&#25805;&#20316;&#19982;&#20559;&#24046;-&#26041;&#24046;&#25240;&#34935;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#24402;&#19968;&#21270;&#26041;&#26696;&#65292;&#21517;&#20026;Adap-$\tau$&#65292;&#23427;&#21160;&#24577;&#35843;&#33410;&#27599;&#20010;&#29992;&#25143;-&#27599;&#20010;&#29289;&#21697;&#23545;&#30340;&#23884;&#20837;&#24133;&#24230;&#65292;&#26088;&#22312;&#23454;&#29616;&#29702;&#24819;&#30340;&#25512;&#33616;&#24615;&#33021;&#12290;&#22312;&#22235;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;Adap-$\tau$&#22987;&#32456;&#20248;&#20110;&#24378;&#22522;&#20934;&#26041;&#27861;&#65292;&#24182;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have witnessed the great successes of embedding-based methods in recommender systems. Despite their decent performance, we argue one potential limitation of these methods -- the embedding magnitude has not been explicitly modulated, which may aggravate popularity bias and training instability, hindering the model from making a good recommendation. It motivates us to leverage the embedding normalization in recommendation. By normalizing user/item embeddings to a specific value, we empirically observe impressive performance gains (9\% on average) on four real-world datasets. Although encouraging, we also reveal a serious limitation when applying normalization in recommendation -- the performance is highly sensitive to the choice of the temperature $\tau$ which controls the scale of the normalized embeddings.  To fully foster the merits of the normalization while circumvent its limitation, this work studied on how to adaptively set the proper $\tau$. Towards this end, we firs
&lt;/p&gt;</description></item><item><title>TaDaa&#26159;&#19968;&#20010;&#21033;&#29992;&#26368;&#26032;&#30340;Transformer&#27169;&#22411;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#29992;&#20110;&#23458;&#25143;&#25903;&#25345;&#12289;&#24110;&#21161;&#21488;&#21644;&#38382;&#39064;&#30331;&#35760;&#31995;&#32479;&#30340;&#23454;&#26102;&#31080;&#21153;&#20998;&#37197;&#28145;&#24230;&#23398;&#20064;&#33258;&#21160;&#39038;&#38382;&#65292;&#23427;&#21487;&#20197;&#20998;&#37197;&#38382;&#39064;&#32473;&#27491;&#30830;&#30340;&#32452;&#12289;&#20998;&#37197;&#38382;&#39064;&#32473;&#26368;&#20339;&#30340;&#35299;&#20915;&#32773;&#65292;&#24182;&#21521;&#35299;&#20915;&#32773;&#25552;&#20379;&#26368;&#30456;&#20851;&#30340;&#20808;&#21069;&#35299;&#20915;&#30340;&#38382;&#39064;&#65292;&#20854;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#33021;&#22815;&#26497;&#22823;&#22320;&#25552;&#39640;&#24179;&#22343;&#35299;&#20915;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2207.11187</link><description>&lt;p&gt;
TaDaa: &#29992;&#20110;&#23458;&#25143;&#25903;&#25345;&#12289;&#24110;&#21161;&#21488;&#21644;&#38382;&#39064;&#30331;&#35760;&#31995;&#32479;&#30340;&#23454;&#26102;&#31080;&#21153;&#20998;&#37197;&#28145;&#24230;&#23398;&#20064;&#33258;&#21160;&#39038;&#38382;
&lt;/p&gt;
&lt;p&gt;
TaDaa: real time Ticket Assignment Deep learning Auto Advisor for customer support, help desk, and issue ticketing systems. (arXiv:2207.11187v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.11187
&lt;/p&gt;
&lt;p&gt;
TaDaa&#26159;&#19968;&#20010;&#21033;&#29992;&#26368;&#26032;&#30340;Transformer&#27169;&#22411;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#29992;&#20110;&#23458;&#25143;&#25903;&#25345;&#12289;&#24110;&#21161;&#21488;&#21644;&#38382;&#39064;&#30331;&#35760;&#31995;&#32479;&#30340;&#23454;&#26102;&#31080;&#21153;&#20998;&#37197;&#28145;&#24230;&#23398;&#20064;&#33258;&#21160;&#39038;&#38382;&#65292;&#23427;&#21487;&#20197;&#20998;&#37197;&#38382;&#39064;&#32473;&#27491;&#30830;&#30340;&#32452;&#12289;&#20998;&#37197;&#38382;&#39064;&#32473;&#26368;&#20339;&#30340;&#35299;&#20915;&#32773;&#65292;&#24182;&#21521;&#35299;&#20915;&#32773;&#25552;&#20379;&#26368;&#30456;&#20851;&#30340;&#20808;&#21069;&#35299;&#20915;&#30340;&#38382;&#39064;&#65292;&#20854;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#33021;&#22815;&#26497;&#22823;&#22320;&#25552;&#39640;&#24179;&#22343;&#35299;&#20915;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;TaDaa&#65306;&#31080;&#21153;&#20998;&#37197;&#28145;&#24230;&#23398;&#20064;&#33258;&#21160;&#39038;&#38382;&#65292;&#21033;&#29992;&#26368;&#26032;&#30340;Transformer&#27169;&#22411;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#24555;&#36895;&#20998;&#37197;&#32452;&#32455;&#20869;&#30340;&#38382;&#39064;&#65292;&#22914;&#23458;&#25143;&#25903;&#25345;&#12289;&#24110;&#21161;&#21488;&#21644;&#20854;&#20182;&#38382;&#39064;&#30331;&#35760;&#31995;&#32479;&#12290;&#35813;&#39033;&#30446;&#25552;&#20379;&#20197;&#19979;&#21151;&#33021;&#65306;1&#65289;&#23558;&#38382;&#39064;&#20998;&#37197;&#32473;&#27491;&#30830;&#30340;&#32452;&#65307;2&#65289;&#23558;&#38382;&#39064;&#20998;&#37197;&#32473;&#26368;&#20339;&#30340;&#35299;&#20915;&#32773;&#65307;3&#65289;&#21521;&#35299;&#20915;&#32773;&#25552;&#20379;&#26368;&#30456;&#20851;&#30340;&#20808;&#21069;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#21033;&#29992;&#19968;&#20010;&#26679;&#26412;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#25324;3k+&#20010;&#32452;&#21644;10k+&#20010;&#35299;&#20915;&#32773;&#65292;&#23454;&#29616;&#20102;95.2%&#30340;&#21069;&#19977;&#24314;&#35758;&#20934;&#30830;&#29575;&#21644;79.0%&#30340;&#21069;&#20116;&#35299;&#20915;&#32773;&#24314;&#35758;&#20934;&#30830;&#29575;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#39033;&#30740;&#31350;&#23558;&#22823;&#22823;&#25552;&#39640;&#23458;&#25143;&#25903;&#25345;&#12289;&#24110;&#21161;&#21488;&#21644;&#38382;&#39064;&#30331;&#35760;&#31995;&#32479;&#30340;&#24179;&#22343;&#35299;&#20915;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes TaDaa: Ticket Assignment Deep learning Auto Advisor, which leverages the latest Transformers models and machine learning techniques quickly assign issues within an organization, like customer support, help desk and alike issue ticketing systems. The project provides functionality to 1) assign an issue to the correct group, 2) assign an issue to the best resolver, and 3) provide the most relevant previously solved tickets to resolvers. We leverage one ticketing system sample dataset, with over 3k+ groups and over 10k+ resolvers to obtain a 95.2% top 3 accuracy on group suggestions and a 79.0% top 5 accuracy on resolver suggestions. We hope this research will greatly improve average issue resolution time on customer support, help desk, and issue ticketing systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#24182;&#37325;&#29616;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20266;&#30456;&#20851;&#21453;&#39304;&#26041;&#27861;&#30340;&#23494;&#38598;&#26816;&#32034;&#22120;&#65292;&#31216;&#20026;ANCE-PRF&#12290;&#22312;&#35813;&#26041;&#27861;&#20013;&#65292;&#26597;&#35810;&#25991;&#26412;&#21644;&#21069;K&#20010;&#21453;&#39304;&#27573;&#33853;&#30340;&#25991;&#26412;&#34987;&#20018;&#32852;&#22312;&#19968;&#36215;&#24418;&#25104;&#19968;&#20010;&#26032;&#30340;&#26597;&#35810;&#36755;&#20837;&#65292;&#20197;&#25552;&#39640;&#26816;&#32034;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2112.06400</link><description>&lt;p&gt;
&#21033;&#29992;&#20266;&#30456;&#20851;&#21453;&#39304;&#25913;&#36827;&#23494;&#38598;&#26816;&#32034;&#30340;&#26597;&#35810;&#34920;&#31034;&#65306;&#19968;&#39033;&#21487;&#37325;&#22797;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Improving Query Representations for Dense Retrieval with Pseudo Relevance Feedback: A Reproducibility Study. (arXiv:2112.06400v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.06400
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#24182;&#37325;&#29616;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20266;&#30456;&#20851;&#21453;&#39304;&#26041;&#27861;&#30340;&#23494;&#38598;&#26816;&#32034;&#22120;&#65292;&#31216;&#20026;ANCE-PRF&#12290;&#22312;&#35813;&#26041;&#27861;&#20013;&#65292;&#26597;&#35810;&#25991;&#26412;&#21644;&#21069;K&#20010;&#21453;&#39304;&#27573;&#33853;&#30340;&#25991;&#26412;&#34987;&#20018;&#32852;&#22312;&#19968;&#36215;&#24418;&#25104;&#19968;&#20010;&#26032;&#30340;&#26597;&#35810;&#36755;&#20837;&#65292;&#20197;&#25552;&#39640;&#26816;&#32034;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20266;&#30456;&#20851;&#21453;&#39304;&#65288;PRF&#65289;&#21033;&#29992;&#31532;&#19968;&#36718;&#26816;&#32034;&#20013;&#21069;K&#20010;&#26816;&#32034;&#27573;&#33853;&#30340;&#30456;&#20851;&#20449;&#21495;&#26469;&#25191;&#34892;&#31532;&#20108;&#36718;&#26816;&#32034;&#65292;&#20197;&#25552;&#39640;&#25628;&#32034;&#25928;&#26524;&#12290;&#26368;&#36817;&#30340;&#19968;&#20010;&#30740;&#31350;&#26041;&#21521;&#26159;&#23545;&#22522;&#20110;&#28145;&#24230;&#35821;&#35328;&#27169;&#22411;&#30340;&#25490;&#21517;&#22120;&#36827;&#34892;PRF&#26041;&#27861;&#30340;&#30740;&#31350;&#21644;&#24320;&#21457;&#65292;&#29305;&#21035;&#26159;&#22312;&#23494;&#38598;&#26816;&#32034;&#30340;&#29615;&#22659;&#20013;&#12290;&#19982;&#26356;&#22797;&#26434;&#30340;&#31070;&#32463;&#25490;&#21517;&#22120;&#30456;&#27604;&#65292;&#23494;&#38598;&#26816;&#32034;&#22120;&#22312;&#20248;&#21270;&#25928;&#26524;&#21644;&#26597;&#35810;&#24310;&#36831;&#20043;&#38388;&#25552;&#20379;&#20102;&#19968;&#31181;&#24179;&#34913;&#65292;&#20351;&#24471;&#26816;&#32034;&#31649;&#36947;&#26356;&#21152;&#39640;&#25928;&#12290;&#24341;&#20837;PRF&#26041;&#27861;&#26469;&#25913;&#36827;&#23494;&#38598;&#26816;&#32034;&#22120;&#30340;&#25928;&#26524;&#24050;&#25104;&#20026;&#19968;&#31181;&#23581;&#35797;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37325;&#22797;&#24182;&#30740;&#31350;&#20102;&#19968;&#31181;&#26368;&#36817;&#30340;&#23494;&#38598;&#26816;&#32034;&#22120;&#30340;PRF&#26041;&#27861;&#65292;&#31216;&#20026;ANCE-PRF&#12290;&#35813;&#26041;&#27861;&#23558;&#26597;&#35810;&#25991;&#26412;&#21644;&#21069;K&#20010;&#21453;&#39304;&#27573;&#33853;&#30340;&#25991;&#26412;&#20018;&#32852;&#22312;&#19968;&#36215;&#65292;&#24418;&#25104;&#19968;&#20010;&#26032;&#30340;&#26597;&#35810;&#36755;&#20837;&#65292;&#28982;&#21518;&#34987;&#32534;&#30721;&#25104;&#19968;&#20010;&#23494;&#38598;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pseudo-Relevance Feedback (PRF) utilises the relevance signals from the top-k passages from the first round of retrieval to perform a second round of retrieval aiming to improve search effectiveness. A recent research direction has been the study and development of PRF methods for deep language models based rankers, and in particular in the context of dense retrievers. Dense retrievers, compared to more complex neural rankers, provide a trade-off between effectiveness, which is often reduced compared to more complex neural rankers, and query latency, which also is reduced making the retrieval pipeline more efficient. The introduction of PRF methods for dense retrievers has been motivated as an attempt to further improve their effectiveness.  In this paper, we reproduce and study a recent method for PRF with dense retrievers, called ANCE-PRF. This method concatenates the query text and that of the top-k feedback passages to form a new query input, which is then encoded into a dense repr
&lt;/p&gt;</description></item></channel></rss>