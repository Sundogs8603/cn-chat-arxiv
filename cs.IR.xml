<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#25506;&#32034;&#21033;&#29992;&#39640;&#20998;&#36776;&#29575;&#29305;&#24449;&#36827;&#34892;&#22270;&#20687;&#26816;&#32034;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;High-Resolution Networks&#20316;&#20026;&#39592;&#24178;&#30340;&#28145;&#24230;&#21704;&#24076;&#32593;&#32476;&#65288;HHNet&#65289;&#65292;&#22312;&#21508;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.13747</link><description>&lt;p&gt;
&#21033;&#29992;&#39640;&#20998;&#36776;&#29575;&#29305;&#24449;&#25913;&#36827;&#22522;&#20110;&#28145;&#24230;&#21704;&#24076;&#30340;&#22270;&#20687;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Leveraging High-Resolution Features for Improved Deep Hashing-based Image Retrieval
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13747
&lt;/p&gt;
&lt;p&gt;
&#25506;&#32034;&#21033;&#29992;&#39640;&#20998;&#36776;&#29575;&#29305;&#24449;&#36827;&#34892;&#22270;&#20687;&#26816;&#32034;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;High-Resolution Networks&#20316;&#20026;&#39592;&#24178;&#30340;&#28145;&#24230;&#21704;&#24076;&#32593;&#32476;&#65288;HHNet&#65289;&#65292;&#22312;&#21508;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#21704;&#24076;&#25216;&#26415;&#24050;&#32463;&#25104;&#20026;&#39640;&#25928;&#22270;&#20687;&#26816;&#32034;&#30340;&#20027;&#35201;&#26041;&#27861;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#21033;&#29992;&#39640;&#20998;&#36776;&#29575;&#29305;&#24449;&#36827;&#34892;&#22270;&#20687;&#26816;&#32034;&#20219;&#21153;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#39640;&#20998;&#36776;&#29575;&#32593;&#32476;&#65288;HRNets&#65289;&#20316;&#20026;&#28145;&#24230;&#21704;&#24076;&#20219;&#21153;&#39592;&#24178;&#30340;&#26032;&#26041;&#27861;&#65292;&#31216;&#20026;&#39640;&#20998;&#36776;&#29575;&#21704;&#24076;&#32593;&#32476;&#65288;HHNet&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30456;&#36739;&#20110;&#29616;&#26377;&#26041;&#27861;&#22312;&#21508;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65288;&#21253;&#25324;CIFAR-10&#65292;NUS-WIDE&#65292;MS COCO&#21644;ImageNet&#65289;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13747v1 Announce Type: cross  Abstract: Deep hashing techniques have emerged as the predominant approach for efficient image retrieval. Traditionally, these methods utilize pre-trained convolutional neural networks (CNNs) such as AlexNet and VGG-16 as feature extractors. However, the increasing complexity of datasets poses challenges for these backbone architectures in capturing meaningful features essential for effective image retrieval. In this study, we explore the efficacy of employing high-resolution features learned through state-of-the-art techniques for image retrieval tasks. Specifically, we propose a novel methodology that utilizes High-Resolution Networks (HRNets) as the backbone for the deep hashing task, termed High-Resolution Hashing Network (HHNet). Our approach demonstrates superior performance compared to existing methods across all tested benchmark datasets, including CIFAR-10, NUS-WIDE, MS COCO, and ImageNet. This performance improvement is more pronounced
&lt;/p&gt;</description></item><item><title>LLM&#21551;&#29992;&#20102;&#22522;&#20110;&#31574;&#30053;&#30340;&#22810;&#27169;&#26597;&#35810;&#20248;&#21270;&#22120;&#65292;&#25670;&#33073;&#20102;&#20256;&#32479;&#30340;&#22522;&#20110;&#35268;&#21017;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#20026;&#26597;&#35810;&#20248;&#21270;&#24102;&#26469;&#20840;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.13597</link><description>&lt;p&gt;
&#19981;&#20877;&#26377;&#20248;&#21270;&#35268;&#21017;: &#22522;&#20110;LLM&#30340;&#22522;&#20110;&#31574;&#30053;&#30340;&#22810;&#27169;&#26597;&#35810;&#20248;&#21270;&#22120;&#65288;&#29256;&#26412;1&#65289;
&lt;/p&gt;
&lt;p&gt;
No more optimization rules: LLM-enabled policy-based multi-modal query optimizer (version 1)
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13597
&lt;/p&gt;
&lt;p&gt;
LLM&#21551;&#29992;&#20102;&#22522;&#20110;&#31574;&#30053;&#30340;&#22810;&#27169;&#26597;&#35810;&#20248;&#21270;&#22120;&#65292;&#25670;&#33073;&#20102;&#20256;&#32479;&#30340;&#22522;&#20110;&#35268;&#21017;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#20026;&#26597;&#35810;&#20248;&#21270;&#24102;&#26469;&#20840;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;(LLM)&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#26631;&#24535;&#30528;&#19968;&#20010;&#37325;&#35201;&#26102;&#21051;&#12290;&#26368;&#36817;&#65292;&#20154;&#20204;&#30740;&#31350;&#20102;LLM&#22312;&#26597;&#35810;&#35268;&#21010;&#20013;&#30340;&#33021;&#21147;&#65292;&#21253;&#25324;&#21333;&#27169;&#21644;&#22810;&#27169;&#26597;&#35810;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;LLM&#30340;&#26597;&#35810;&#20248;&#21270;&#33021;&#21147;&#36824;&#27809;&#26377;&#30456;&#20851;&#30740;&#31350;&#12290;&#20316;&#20026;&#26174;&#33879;&#24433;&#21709;&#26597;&#35810;&#35745;&#21010;&#25191;&#34892;&#24615;&#33021;&#30340;&#20851;&#38190;&#27493;&#39588;&#65292;&#19981;&#24212;&#38169;&#36807;&#36825;&#31181;&#20998;&#26512;&#21644;&#23581;&#35797;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#29616;&#26377;&#30340;&#26597;&#35810;&#20248;&#21270;&#22120;&#36890;&#24120;&#26159;&#22522;&#20110;&#35268;&#21017;&#25110;&#22522;&#20110;&#35268;&#21017;+&#22522;&#20110;&#25104;&#26412;&#30340;&#65292;&#21363;&#23427;&#20204;&#20381;&#36182;&#20110;&#20154;&#24037;&#21019;&#24314;&#30340;&#35268;&#21017;&#26469;&#23436;&#25104;&#26597;&#35810;&#35745;&#21010;&#37325;&#20889;/&#36716;&#25442;&#12290;&#37492;&#20110;&#29616;&#20195;&#20248;&#21270;&#22120;&#21253;&#25324;&#25968;&#30334;&#33267;&#25968;&#21315;&#26465;&#35268;&#21017;&#65292;&#25353;&#29031;&#31867;&#20284;&#26041;&#24335;&#35774;&#35745;&#19968;&#20010;&#22810;&#27169;&#26597;&#35810;&#20248;&#21270;&#22120;&#23558;&#32791;&#36153;&#22823;&#37327;&#26102;&#38388;&#65292;&#22240;&#20026;&#25105;&#20204;&#23558;&#19981;&#24471;&#19981;&#21015;&#20030;&#23613;&#21487;&#33021;&#22810;&#30340;&#22810;&#27169;&#20248;&#21270;&#35268;&#21017;&#65292;&#32780;&#36825;&#24182;&#27809;&#26377;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13597v1 Announce Type: cross  Abstract: Large language model (LLM) has marked a pivotal moment in the field of machine learning and deep learning. Recently its capability for query planning has been investigated, including both single-modal and multi-modal queries. However, there is no work on the query optimization capability of LLM. As a critical (or could even be the most important) step that significantly impacts the execution performance of the query plan, such analysis and attempts should not be missed. From another aspect, existing query optimizers are usually rule-based or rule-based + cost-based, i.e., they are dependent on manually created rules to complete the query plan rewrite/transformation. Given the fact that modern optimizers include hundreds to thousands of rules, designing a multi-modal query optimizer following a similar way is significantly time-consuming since we will have to enumerate as many multi-modal optimization rules as possible, which has not be
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#25512;&#33616;&#26041;&#27861;LSVCR&#65292;&#36890;&#36807;&#34701;&#21512;&#29992;&#25143;&#19982;&#35270;&#39057;&#21644;&#35780;&#35770;&#30340;&#20132;&#20114;&#21382;&#21490;&#65292;&#32852;&#21512;&#36827;&#34892;&#20010;&#24615;&#21270;&#35270;&#39057;&#21644;&#35780;&#35770;&#25512;&#33616;</title><link>https://arxiv.org/abs/2403.13574</link><description>&lt;p&gt;
&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#30340;&#24207;&#21015;&#25512;&#33616;&#22120;&#65292;&#29992;&#20110;&#32852;&#21512;&#35270;&#39057;&#21644;&#35780;&#35770;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
A Large Language Model Enhanced Sequential Recommender for Joint Video and Comment Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13574
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#25512;&#33616;&#26041;&#27861;LSVCR&#65292;&#36890;&#36807;&#34701;&#21512;&#29992;&#25143;&#19982;&#35270;&#39057;&#21644;&#35780;&#35770;&#30340;&#20132;&#20114;&#21382;&#21490;&#65292;&#32852;&#21512;&#36827;&#34892;&#20010;&#24615;&#21270;&#35270;&#39057;&#21644;&#35780;&#35770;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22312;&#32447;&#35270;&#39057;&#24179;&#21488;&#19978;&#65292;&#38405;&#35835;&#25110;&#25776;&#20889;&#26377;&#36259;&#35270;&#39057;&#30340;&#35780;&#35770;&#24050;&#32463;&#25104;&#20026;&#35270;&#39057;&#35266;&#30475;&#20307;&#39564;&#20013;&#19981;&#21487;&#25110;&#32570;&#30340;&#19968;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#35270;&#39057;&#25512;&#33616;&#31995;&#32479;&#20027;&#35201;&#23545;&#29992;&#25143;&#19982;&#35270;&#39057;&#30340;&#20132;&#20114;&#34892;&#20026;&#36827;&#34892;&#24314;&#27169;&#65292;&#32570;&#20047;&#23545;&#35780;&#35770;&#22312;&#29992;&#25143;&#34892;&#20026;&#24314;&#27169;&#20013;&#30340;&#32771;&#34385;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LSVCR&#30340;&#26032;&#39062;&#25512;&#33616;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#29992;&#25143;&#19982;&#35270;&#39057;&#21644;&#35780;&#35770;&#30340;&#20132;&#20114;&#21382;&#21490;&#65292;&#20849;&#21516;&#36827;&#34892;&#20010;&#24615;&#21270;&#35270;&#39057;&#21644;&#35780;&#35770;&#25512;&#33616;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30001;&#20004;&#20010;&#20851;&#38190;&#32452;&#20214;&#32452;&#25104;&#65292;&#21363;&#24207;&#21015;&#25512;&#33616;&#65288;SR&#65289;&#27169;&#22411;&#21644;&#34917;&#20805;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25512;&#33616;&#22120;&#12290;SR&#27169;&#22411;&#20316;&#20026;&#25105;&#20204;&#26041;&#27861;&#30340;&#20027;&#35201;&#25512;&#33616;&#39592;&#24178;&#65288;&#22312;&#37096;&#32626;&#20013;&#20445;&#30041;&#65289;&#65292;&#21487;&#23454;&#29616;&#39640;&#25928;&#30340;&#29992;&#25143;&#20559;&#22909;&#24314;&#27169;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#25105;&#20204;&#21033;&#29992;LLM&#25512;&#33616;&#22120;&#20316;&#20026;&#19968;&#20010;&#34917;&#20805;&#32452;&#20214;&#65288;&#22312;&#37096;&#32626;&#20013;&#20002;&#24323;&#65289;&#65292;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#28508;&#22312;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13574v1 Announce Type: new  Abstract: In online video platforms, reading or writing comments on interesting videos has become an essential part of the video watching experience. However, existing video recommender systems mainly model users' interaction behaviors with videos, lacking consideration of comments in user behavior modeling. In this paper, we propose a novel recommendation approach called LSVCR by leveraging user interaction histories with both videos and comments, so as to jointly conduct personalized video and comment recommendation. Specifically, our approach consists of two key components, namely sequential recommendation (SR) model and supplemental large language model (LLM) recommender. The SR model serves as the primary recommendation backbone (retained in deployment) of our approach, allowing for efficient user preference modeling. Meanwhile, we leverage the LLM recommender as a supplemental component (discarded in deployment) to better capture underlying 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;UOT-RCL&#65292;&#19968;&#20010;&#22522;&#20110;&#26368;&#20248;&#36755;&#36816;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#24102;&#26377;&#22122;&#22768;&#26631;&#31614;&#30340;&#36328;&#27169;&#24577;&#26816;&#32034;&#20013;&#30340;&#20004;&#20010;&#20851;&#38190;&#25361;&#25112;&#65292;&#21363;&#24378;&#21046;&#22810;&#27169;&#24577;&#26679;&#26412;&#23545;&#40784;&#19981;&#27491;&#30830;&#30340;&#35821;&#20041;&#20197;&#21450;&#25193;&#22823;&#24322;&#36136;&#24615;&#24046;&#36317;&#65292;&#20174;&#32780;&#25552;&#21319;&#26816;&#32034;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.13480</link><description>&lt;p&gt;
&#19968;&#20010;&#32479;&#19968;&#30340;&#22522;&#20110;&#26368;&#20248;&#36755;&#36816;&#30340;&#36328;&#27169;&#24577;&#26816;&#32034;&#26694;&#26550;&#65292;&#24212;&#23545;&#24102;&#26377;&#22122;&#22768;&#26631;&#31614;&#30340;&#24773;&#20917;
&lt;/p&gt;
&lt;p&gt;
A Unified Optimal Transport Framework for Cross-Modal Retrieval with Noisy Labels
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13480
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;UOT-RCL&#65292;&#19968;&#20010;&#22522;&#20110;&#26368;&#20248;&#36755;&#36816;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#24102;&#26377;&#22122;&#22768;&#26631;&#31614;&#30340;&#36328;&#27169;&#24577;&#26816;&#32034;&#20013;&#30340;&#20004;&#20010;&#20851;&#38190;&#25361;&#25112;&#65292;&#21363;&#24378;&#21046;&#22810;&#27169;&#24577;&#26679;&#26412;&#23545;&#40784;&#19981;&#27491;&#30830;&#30340;&#35821;&#20041;&#20197;&#21450;&#25193;&#22823;&#24322;&#36136;&#24615;&#24046;&#36317;&#65292;&#20174;&#32780;&#25552;&#21319;&#26816;&#32034;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#27169;&#24577;&#26816;&#32034;&#65288;CMR&#65289;&#26088;&#22312;&#24314;&#31435;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#20854;&#20013;&#30417;&#30563;&#24335;CMR&#30001;&#20110;&#22312;&#23398;&#20064;&#35821;&#20041;&#31867;&#21035;&#35782;&#21035;&#19978;&#30340;&#28789;&#27963;&#24615;&#32780;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#23613;&#31649;&#20808;&#21069;&#30340;&#30417;&#30563;&#24335;CMR&#26041;&#27861;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23427;&#20204;&#30340;&#25104;&#21151;&#24456;&#22823;&#31243;&#24230;&#19978;&#24402;&#22240;&#20110;&#33391;&#22909;&#27880;&#37322;&#30340;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#23545;&#20110;&#21333;&#27169;&#24577;&#25968;&#25454;&#65292;&#31934;&#30830;&#30340;&#27880;&#37322;&#20063;&#26159;&#26114;&#36149;&#19988;&#32791;&#26102;&#30340;&#65292;&#32780;&#22312;&#22810;&#27169;&#24577;&#22330;&#26223;&#19979;&#21464;&#24471;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#20174;&#20114;&#32852;&#32593;&#25910;&#38598;&#20102;&#22823;&#37327;&#20855;&#26377;&#31895;&#31961;&#27880;&#37322;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#36825;&#24517;&#28982;&#23548;&#33268;&#26377;&#22122;&#22768;&#30340;&#26631;&#31614;&#12290;&#20351;&#29992;&#36825;&#20123;&#35823;&#23548;&#24615;&#26631;&#31614;&#36827;&#34892;&#35757;&#32451;&#20250;&#24102;&#26469;&#20004;&#20010;&#20851;&#38190;&#25361;&#25112;--&#24378;&#21046;&#22810;&#27169;&#24577;&#26679;&#26412;\emph{&#23545;&#40784;&#19981;&#27491;&#30830;&#30340;&#35821;&#20041;}&#21644;\emph{&#25193;&#22823;&#24322;&#36136;&#24615;&#24046;&#36317;}&#65292;&#20174;&#32780;&#23548;&#33268;&#26816;&#32034;&#24615;&#33021;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;UOT-RCL&#65292;&#19968;&#20010;&#22522;&#20110;&#26368;&#20248;&#36755;&#36816;&#65288;OT&#65289;&#30340;&#32479;&#19968;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13480v1 Announce Type: cross  Abstract: Cross-modal retrieval (CMR) aims to establish interaction between different modalities, among which supervised CMR is emerging due to its flexibility in learning semantic category discrimination. Despite the remarkable performance of previous supervised CMR methods, much of their success can be attributed to the well-annotated data. However, even for unimodal data, precise annotation is expensive and time-consuming, and it becomes more challenging with the multimodal scenario. In practice, massive multimodal data are collected from the Internet with coarse annotation, which inevitably introduces noisy labels. Training with such misleading labels would bring two key challenges -- enforcing the multimodal samples to \emph{align incorrect semantics} and \emph{widen the heterogeneous gap}, resulting in poor retrieval performance. To tackle these challenges, this work proposes UOT-RCL, a Unified framework based on Optimal Transport (OT) for
&lt;/p&gt;</description></item><item><title>DESIRE-ME&#26159;&#19968;&#20010;&#31070;&#32463;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#65292;&#21033;&#29992;&#19987;&#23478;&#28151;&#21512;&#26694;&#26550;&#32467;&#21512;&#22810;&#20010;&#19987;&#19994;&#31070;&#32463;&#27169;&#22411;&#65292;&#36890;&#36807;&#35757;&#32451;&#31070;&#32463;&#38376;&#25511;&#26426;&#21046;&#23545;&#19981;&#21516;&#39046;&#22495;&#30340;&#19987;&#23478;&#39044;&#27979;&#36827;&#34892;&#21152;&#26435;&#65292;&#33021;&#22815;&#33258;&#36866;&#24212;&#22320;&#22312;&#22810;&#20010;&#39046;&#22495;&#20013;&#19987;&#38376;&#21270;&#65292;&#24182;&#22312;&#22788;&#29702;&#24320;&#25918;&#39046;&#22495;&#38382;&#39064;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2403.13468</link><description>&lt;p&gt;
DESIRE-ME&#65306;&#20351;&#29992;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#30340;&#22686;&#24378;&#39046;&#22495;&#30417;&#30563;&#20449;&#24687;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
DESIRE-ME: Domain-Enhanced Supervised Information REtrieval using Mixture-of-Experts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13468
&lt;/p&gt;
&lt;p&gt;
DESIRE-ME&#26159;&#19968;&#20010;&#31070;&#32463;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#65292;&#21033;&#29992;&#19987;&#23478;&#28151;&#21512;&#26694;&#26550;&#32467;&#21512;&#22810;&#20010;&#19987;&#19994;&#31070;&#32463;&#27169;&#22411;&#65292;&#36890;&#36807;&#35757;&#32451;&#31070;&#32463;&#38376;&#25511;&#26426;&#21046;&#23545;&#19981;&#21516;&#39046;&#22495;&#30340;&#19987;&#23478;&#39044;&#27979;&#36827;&#34892;&#21152;&#26435;&#65292;&#33021;&#22815;&#33258;&#36866;&#24212;&#22320;&#22312;&#22810;&#20010;&#39046;&#22495;&#20013;&#19987;&#38376;&#21270;&#65292;&#24182;&#22312;&#22788;&#29702;&#24320;&#25918;&#39046;&#22495;&#38382;&#39064;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#39046;&#22495;&#38382;&#39064;&#22238;&#31572;&#38656;&#35201;&#26816;&#32034;&#31995;&#32479;&#33021;&#22815;&#22788;&#29702;&#22810;&#26679;&#21270;&#21644;&#21464;&#21270;&#22810;&#26679;&#30340;&#38382;&#39064;&#65292;&#25552;&#20379;&#20934;&#30830;&#30340;&#31572;&#26696;&#28085;&#30422;&#24191;&#27867;&#30340;&#26597;&#35810;&#31867;&#22411;&#21644;&#20027;&#39064;&#12290;&#20026;&#20102;&#36890;&#36807;&#19968;&#20010;&#29420;&#29305;&#30340;&#27169;&#22411;&#22788;&#29702;&#36825;&#31181;&#20027;&#39064;&#24322;&#36136;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DESIRE-ME&#65292;&#36825;&#26159;&#19968;&#20010;&#31070;&#32463;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#65292;&#21033;&#29992;&#19987;&#23478;&#28151;&#21512;&#26694;&#26550;&#32467;&#21512;&#22810;&#20010;&#19987;&#19994;&#31070;&#32463;&#27169;&#22411;&#12290;&#25105;&#20204;&#20381;&#36182;&#20110;&#32500;&#22522;&#30334;&#31185;&#25968;&#25454;&#26469;&#35757;&#32451;&#19968;&#20010;&#26377;&#25928;&#30340;&#31070;&#32463;&#38376;&#25511;&#26426;&#21046;&#65292;&#23545;&#20256;&#20837;&#26597;&#35810;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#30456;&#24212;&#22320;&#21152;&#26435;&#19981;&#21516;&#39046;&#22495;&#19987;&#23478;&#30340;&#39044;&#27979;&#12290;&#36825;&#20351;&#24471;DESIRE-ME&#33021;&#22815;&#33258;&#36866;&#24212;&#22320;&#22312;&#22810;&#20010;&#39046;&#22495;&#20013;&#19987;&#38376;&#21270;&#12290;&#36890;&#36807;&#23545;&#20844;&#24320;&#21487;&#29992;&#25968;&#25454;&#38598;&#30340;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#25552;&#35758;&#33021;&#22815;&#26377;&#25928;&#22320;&#27010;&#25324;&#39046;&#22495;&#22686;&#24378;&#30340;&#31070;&#32463;&#27169;&#22411;&#12290;DESIRE-ME&#22312;&#33258;&#36866;&#24212;&#22788;&#29702;&#24320;&#25918;&#39046;&#22495;&#38382;&#39064;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#33021;&#23558;NDCG@10&#25552;&#39640;&#39640;&#36798;12%&#65292;P@1&#25552;&#39640;&#39640;&#36798;22%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13468v1 Announce Type: new  Abstract: Open-domain question answering requires retrieval systems able to cope with the diverse and varied nature of questions, providing accurate answers across a broad spectrum of query types and topics. To deal with such topic heterogeneity through a unique model, we propose DESIRE-ME, a neural information retrieval model that leverages the Mixture-of-Experts framework to combine multiple specialized neural models. We rely on Wikipedia data to train an effective neural gating mechanism that classifies the incoming query and that weighs the predictions of the different domain-specific experts correspondingly. This allows DESIRE-ME to specialize adaptively in multiple domains. Through extensive experiments on publicly available datasets, we show that our proposal can effectively generalize domain-enhanced neural models. DESIRE-ME excels in handling open-domain questions adaptively, boosting by up to 12% in NDCG@10 and 22% in P@1, the underlying
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;User Stateful Embedding&#65288;USE&#65289;&#26469;&#35299;&#20915;&#21160;&#24577;&#29992;&#25143;&#24314;&#27169;&#20013;&#23384;&#22312;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#23384;&#20648;&#20808;&#21069;&#27169;&#22411;&#29366;&#24577;&#65292;&#29983;&#25104;&#29992;&#25143;&#23884;&#20837;&#24182;&#21453;&#26144;&#29992;&#25143;&#19981;&#26029;&#21457;&#23637;&#30340;&#34892;&#20026;&#12290;</title><link>https://arxiv.org/abs/2403.13344</link><description>&lt;p&gt;
&#20351;&#29992;&#65306;&#24102;&#26377;&#26377;&#29366;&#24577;&#24207;&#21015;&#27169;&#22411;&#30340;&#21160;&#24577;&#29992;&#25143;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
USE: Dynamic User Modeling with Stateful Sequence Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13344
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;User Stateful Embedding&#65288;USE&#65289;&#26469;&#35299;&#20915;&#21160;&#24577;&#29992;&#25143;&#24314;&#27169;&#20013;&#23384;&#22312;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#23384;&#20648;&#20808;&#21069;&#27169;&#22411;&#29366;&#24577;&#65292;&#29983;&#25104;&#29992;&#25143;&#23884;&#20837;&#24182;&#21453;&#26144;&#29992;&#25143;&#19981;&#26029;&#21457;&#23637;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#25143;&#23884;&#20837;&#22312;&#29992;&#25143;&#21442;&#19982;&#24230;&#39044;&#27979;&#21644;&#20010;&#24615;&#21270;&#26381;&#21153;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#24207;&#21015;&#24314;&#27169;&#30340;&#26368;&#26032;&#36827;&#23637;&#24341;&#21457;&#20102;&#20174;&#34892;&#20026;&#25968;&#25454;&#20013;&#23398;&#20064;&#29992;&#25143;&#23884;&#20837;&#30340;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#34892;&#20026;&#30340;&#29992;&#25143;&#23884;&#20837;&#23398;&#20064;&#38754;&#20020;&#21160;&#24577;&#29992;&#25143;&#24314;&#27169;&#30340;&#29420;&#29305;&#25361;&#25112;&#12290;&#38543;&#30528;&#29992;&#25143;&#19981;&#26029;&#19982;&#24212;&#29992;&#31243;&#24207;&#20132;&#20114;&#65292;&#29992;&#25143;&#23884;&#20837;&#24212;&#23450;&#26399;&#26356;&#26032;&#20197;&#32771;&#34385;&#29992;&#25143;&#30340;&#26368;&#36817;&#21644;&#38271;&#26399;&#34892;&#20026;&#27169;&#24335;&#12290;&#29616;&#26377;&#26041;&#27861;&#39640;&#24230;&#20381;&#36182;&#20110;&#32570;&#20047;&#21382;&#21490;&#34892;&#20026;&#35760;&#24518;&#30340;&#26080;&#29366;&#24577;&#24207;&#21015;&#27169;&#22411;&#12290;&#23427;&#20204;&#24517;&#39035;&#35201;&#20040;&#20002;&#24323;&#21382;&#21490;&#25968;&#25454;&#20165;&#20351;&#29992;&#26368;&#26032;&#25968;&#25454;&#65292;&#35201;&#20040;&#37325;&#26032;&#22788;&#29702;&#26087;&#25968;&#25454;&#21644;&#26032;&#25968;&#25454;&#12290;&#20004;&#31181;&#24773;&#20917;&#22343;&#20250;&#20135;&#29983;&#22823;&#37327;&#35745;&#31639;&#24320;&#38144;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#29992;&#25143;&#26377;&#29366;&#24577;&#23884;&#20837;&#65288;USE&#65289;&#12290;USE&#29983;&#25104;&#29992;&#25143;&#23884;&#20837;&#24182;&#21453;&#26144;&#29992;&#25143;&#19981;&#26029;&#21457;&#23637;&#30340;&#34892;&#20026;&#65292;&#32780;&#26080;&#38656;&#36890;&#36807;&#23384;&#20648;&#20808;&#21069;&#30340;&#27169;&#22411;&#29366;&#24577;&#26469;&#36827;&#34892;&#35814;&#23613;&#30340;&#37325;&#26032;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13344v1 Announce Type: cross  Abstract: User embeddings play a crucial role in user engagement forecasting and personalized services. Recent advances in sequence modeling have sparked interest in learning user embeddings from behavioral data. Yet behavior-based user embedding learning faces the unique challenge of dynamic user modeling. As users continuously interact with the apps, user embeddings should be periodically updated to account for users' recent and long-term behavior patterns. Existing methods highly rely on stateless sequence models that lack memory of historical behavior. They have to either discard historical data and use only the most recent data or reprocess the old and new data jointly. Both cases incur substantial computational overhead. To address this limitation, we introduce User Stateful Embedding (USE). USE generates user embeddings and reflects users' evolving behaviors without the need for exhaustive reprocessing by storing previous model states and
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#26694;&#26550;&#29992;&#20110;&#25991;&#26412;&#20016;&#23500;&#30340;&#39034;&#24207;&#25512;&#33616;&#65292;&#36890;&#36807;&#23545;&#29992;&#25143;&#21382;&#21490;&#34892;&#20026;&#20998;&#27573;&#24182;&#37319;&#29992;LLM&#25688;&#35201;&#24037;&#20855;&#36827;&#34892;&#24635;&#32467;&#65292;&#20174;&#32780;&#35299;&#20915;&#25512;&#33616;&#31995;&#32479;&#20013;&#38271;&#25991;&#26412;&#20449;&#24687;&#24102;&#26469;&#30340;&#25361;&#25112;</title><link>https://arxiv.org/abs/2403.13325</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25991;&#26412;&#20016;&#23500;&#30340;&#39034;&#24207;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Harnessing Large Language Models for Text-Rich Sequential Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13325
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#26694;&#26550;&#29992;&#20110;&#25991;&#26412;&#20016;&#23500;&#30340;&#39034;&#24207;&#25512;&#33616;&#65292;&#36890;&#36807;&#23545;&#29992;&#25143;&#21382;&#21490;&#34892;&#20026;&#20998;&#27573;&#24182;&#37319;&#29992;LLM&#25688;&#35201;&#24037;&#20855;&#36827;&#34892;&#24635;&#32467;&#65292;&#20174;&#32780;&#35299;&#20915;&#25512;&#33616;&#31995;&#32479;&#20013;&#38271;&#25991;&#26412;&#20449;&#24687;&#24102;&#26469;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#24050;&#32463;&#25913;&#21464;&#20102;&#25512;&#33616;&#31995;&#32479;&#65288;RS&#65289;&#30340;&#33539;&#24335;&#12290;&#28982;&#32780;&#65292;&#24403;&#25512;&#33616;&#22330;&#26223;&#20013;&#30340;&#29289;&#21697;&#21253;&#21547;&#20016;&#23500;&#30340;&#25991;&#26412;&#20449;&#24687;&#65292;&#20363;&#22914;&#22312;&#32447;&#36141;&#29289;&#20013;&#30340;&#20135;&#21697;&#25551;&#36848;&#25110;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#26032;&#38395;&#26631;&#39064;&#26102;&#65292;LLMs&#38656;&#35201;&#26356;&#38271;&#30340;&#25991;&#26412;&#25165;&#33021;&#20840;&#38754;&#25551;&#36848;&#21382;&#21490;&#29992;&#25143;&#34892;&#20026;&#24207;&#21015;&#12290;&#36825;&#32473;&#22522;&#20110;LLM&#30340;&#25512;&#33616;&#31995;&#32479;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#65292;&#20363;&#22914;&#38271;&#24230;&#38480;&#21046;&#12289;&#24222;&#22823;&#30340;&#26102;&#38388;&#21644;&#31354;&#38388;&#24320;&#38144;&#20197;&#21450;&#23376;&#20248;&#21270;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;&#20026;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25991;&#26412;&#20016;&#23500;&#30340;&#39034;&#24207;&#25512;&#33616;&#65288;LLM-TRSR&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#23545;&#29992;&#25143;&#21382;&#21490;&#34892;&#20026;&#36827;&#34892;&#20998;&#27573;&#65292;&#38543;&#21518;&#37319;&#29992;&#22522;&#20110;LLM&#30340;&#25688;&#35201;&#24037;&#20855;&#23545;&#36825;&#20123;&#29992;&#25143;&#34892;&#20026;&#22359;&#36827;&#34892;&#24635;&#32467;&#12290;&#29305;&#21035;&#22320;&#65292;&#20511;&#37492;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#22312;&#25104;&#21151;&#24212;&#29992;&#20013;&#30340;&#28789;&#24863;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13325v1 Announce Type: new  Abstract: Recent advances in Large Language Models (LLMs) have been changing the paradigm of Recommender Systems (RS). However, when items in the recommendation scenarios contain rich textual information, such as product descriptions in online shopping or news headlines on social media, LLMs require longer texts to comprehensively depict the historical user behavior sequence. This poses significant challenges to LLM-based recommenders, such as over-length limitations, extensive time and space overheads, and suboptimal model performance. To this end, in this paper, we design a novel framework for harnessing Large Language Models for Text-Rich Sequential Recommendation (LLM-TRSR). Specifically, we first propose to segment the user historical behaviors and subsequently employ an LLM-based summarizer for summarizing these user behavior blocks. Particularly, drawing inspiration from the successful application of Convolutional Neural Networks (CNN) and 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#32039;&#20945;&#30862;&#29255;&#21270;&#26597;&#35810;&#25361;&#25112;&#25968;&#25454;&#38598;&#65288;Flickr30K-CFQ&#65289;&#65292;&#27169;&#25311;&#32771;&#34385;&#22810;&#20010;&#26597;&#35810;&#20869;&#23481;&#21644;&#39118;&#26684;&#30340;&#25991;&#26412;&#22270;&#20687;&#26816;&#32034;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#26597;&#35810;&#22686;&#24378;&#25991;&#26412;&#22270;&#20687;&#26816;&#32034;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.13317</link><description>&lt;p&gt;
Flickr30K-CFQ&#65306;&#29992;&#20110;&#25991;&#26412;&#22270;&#20687;&#26816;&#32034;&#30340;&#32039;&#20945;&#30862;&#29255;&#21270;&#26597;&#35810;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Flickr30K-CFQ: A Compact and Fragmented Query Dataset for Text-image Retrieval
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13317
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#32039;&#20945;&#30862;&#29255;&#21270;&#26597;&#35810;&#25361;&#25112;&#25968;&#25454;&#38598;&#65288;Flickr30K-CFQ&#65289;&#65292;&#27169;&#25311;&#32771;&#34385;&#22810;&#20010;&#26597;&#35810;&#20869;&#23481;&#21644;&#39118;&#26684;&#30340;&#25991;&#26412;&#22270;&#20687;&#26816;&#32034;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#26597;&#35810;&#22686;&#24378;&#25991;&#26412;&#22270;&#20687;&#26816;&#32034;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20114;&#32852;&#32593;&#19978;&#22810;&#27169;&#24577;&#20449;&#24687;&#30340;&#29190;&#28856;&#24615;&#22686;&#38271;&#65292;&#21333;&#27169;&#24577;&#25628;&#32034;&#26080;&#27861;&#28385;&#36275;&#20114;&#32852;&#32593;&#24212;&#29992;&#30340;&#38656;&#27714;&#12290;&#38656;&#35201;&#36827;&#34892;&#25991;&#26412;&#22270;&#20687;&#26816;&#32034;&#30740;&#31350;&#65292;&#23454;&#29616;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#39640;&#36136;&#37327;&#39640;&#25928;&#30340;&#26816;&#32034;&#12290;&#29616;&#26377;&#30340;&#25991;&#26412;&#22270;&#20687;&#26816;&#32034;&#30740;&#31350;&#22823;&#22810;&#22522;&#20110;&#36890;&#29992;&#30340;&#35270;&#35273;-&#35821;&#35328;&#25968;&#25454;&#38598;&#65288;&#22914;MS-COCO&#12289;Flickr30K&#65289;&#65292;&#20854;&#20013;&#26597;&#35810;&#35805;&#35821;&#21051;&#26495;&#32780;&#19981;&#33258;&#28982;&#65288;&#21363;&#20887;&#38271;&#21644;&#36807;&#20110;&#27491;&#24335;&#65289;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#32570;&#28857;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#32039;&#20945;&#30862;&#29255;&#21270;&#26597;&#35810;&#25361;&#25112;&#25968;&#25454;&#38598;&#65288;&#21517;&#20026;Flickr30K-CFQ&#65289;&#65292;&#20197;&#24314;&#27169;&#32771;&#34385;&#22810;&#20010;&#26597;&#35810;&#20869;&#23481;&#21644;&#39118;&#26684;&#30340;&#25991;&#26412;&#22270;&#20687;&#26816;&#32034;&#20219;&#21153;&#65292;&#21253;&#25324;&#32039;&#20945;&#21644;&#32454;&#31890;&#24230;&#30340;&#23454;&#20307;&#20851;&#31995;&#35821;&#26009;&#24211;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#26032;&#22411;&#26597;&#35810;&#22686;&#24378;&#25991;&#26412;&#22270;&#20687;&#26816;&#32034;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;Flickr30-CFQ&#25581;&#31034;&#20102;&#29616;&#26377;&#35270;&#35273;-&#35821;&#35328;&#25968;&#25454;&#38598;&#22312;&#29616;&#23454;&#25991;&#26412;&#22270;&#20687;&#26816;&#32034;&#20219;&#21153;&#20013;&#30340;&#19981;&#36275;&#20043;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13317v1 Announce Type: new  Abstract: With the explosive growth of multi-modal information on the Internet, unimodal search cannot satisfy the requirement of Internet applications. Text-image retrieval research is needed to realize high-quality and efficient retrieval between different modalities. Existing text-image retrieval research is mostly based on general vision-language datasets (e.g. MS-COCO, Flickr30K), in which the query utterance is rigid and unnatural (i.e. verbosity and formality). To overcome the shortcoming, we construct a new Compact and Fragmented Query challenge dataset (named Flickr30K-CFQ) to model text-image retrieval task considering multiple query content and style, including compact and fine-grained entity-relation corpus. We propose a novel query-enhanced text-image retrieval method using prompt engineering based on LLM. Experiments show that our proposed Flickr30-CFQ reveals the insufficiency of existing vision-language datasets in realistic text-i
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;Mathlib4&#30340;&#35821;&#20041;&#25628;&#32034;&#24341;&#25806;&#65292;&#33021;&#22815;&#25509;&#21463;&#38750;&#27491;&#24335;&#26597;&#35810;&#24182;&#25214;&#21040;&#30456;&#20851;&#23450;&#29702;&#65292;&#20026;&#35299;&#20915;&#22312;mathlib4&#20013;&#25628;&#32034;&#22256;&#38590;&#38382;&#39064;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.13310</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;Mathlib4&#30340;&#35821;&#20041;&#25628;&#32034;&#24341;&#25806;
&lt;/p&gt;
&lt;p&gt;
A Semantic Search Engine for Mathlib4
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13310
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;Mathlib4&#30340;&#35821;&#20041;&#25628;&#32034;&#24341;&#25806;&#65292;&#33021;&#22815;&#25509;&#21463;&#38750;&#27491;&#24335;&#26597;&#35810;&#24182;&#25214;&#21040;&#30456;&#20851;&#23450;&#29702;&#65292;&#20026;&#35299;&#20915;&#22312;mathlib4&#20013;&#25628;&#32034;&#22256;&#38590;&#38382;&#39064;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#20114;&#24335;&#23450;&#29702;&#35777;&#26126;&#22120;Lean&#20351;&#24471;&#21487;&#20197;&#39564;&#35777;&#27491;&#24335;&#25968;&#23398;&#35777;&#26126;&#65292;&#24182;&#19988;&#24471;&#21040;&#19968;&#20010;&#19981;&#26029;&#25193;&#22823;&#30340;&#31038;&#21306;&#30340;&#25903;&#25345;&#12290;&#35813;&#29983;&#24577;&#31995;&#32479;&#30340;&#26680;&#24515;&#26159;&#20854;&#25968;&#23398;&#24211;mathlib4&#65292;&#20026;&#25193;&#23637;&#33539;&#22260;&#30340;&#25968;&#23398;&#29702;&#35770;&#30340;&#24418;&#24335;&#21270;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;&#28982;&#32780;&#65292;&#22312;mathlib4&#20013;&#25628;&#32034;&#23450;&#29702;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#25104;&#21151;&#22312;mathlib4&#20013;&#25628;&#32034;&#65292;&#29992;&#25143;&#36890;&#24120;&#38656;&#35201;&#29087;&#24713;&#20854;&#21629;&#21517;&#32422;&#23450;&#25110;&#25991;&#26723;&#23383;&#31526;&#20018;&#12290;&#22240;&#27492;&#65292;&#21019;&#24314;&#19968;&#20010;&#35821;&#20041;&#25628;&#32034;&#24341;&#25806;&#65292;&#21487;&#20197;&#26041;&#20415;&#22320;&#34987;&#20855;&#26377;&#19981;&#21516;&#29087;&#24713;&#31243;&#24230;&#30340;mathlib4&#30340;&#20010;&#20154;&#20351;&#29992;&#26159;&#38750;&#24120;&#37325;&#35201;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;mathlib4&#30340;&#35821;&#20041;&#25628;&#32034;&#24341;&#25806;&#65292;&#21487;&#20197;&#25509;&#21463;&#38750;&#27491;&#24335;&#26597;&#35810;&#24182;&#25214;&#21040;&#30456;&#20851;&#23450;&#29702;&#12290;&#25105;&#20204;&#36824;&#24314;&#31435;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#21508;&#31181;mathlib4&#25628;&#32034;&#24341;&#25806;&#24615;&#33021;&#30340;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13310v1 Announce Type: cross  Abstract: The interactive theorem prover, Lean, enables the verification of formal mathematical proofs and is backed by an expanding community. Central to this ecosystem is its mathematical library, mathlib4, which lays the groundwork for the formalization of an expanding range of mathematical theories. However, searching for theorems in mathlib4 can be challenging. To successfully search in mathlib4, users often need to be familiar with its naming conventions or documentation strings. Therefore, creating a semantic search engine that can be used easily by individuals with varying familiarity with mathlib4 is very important. In this paper, we present a semantic search engine for mathlib4 that accepts informal queries and finds the relevant theorems. We also establish a benchmark for assessing the performance of various search engines for mathlib4.
&lt;/p&gt;</description></item><item><title>&#31995;&#32479;&#20998;&#26512;&#20102;&#21518;&#20132;&#20114;&#27169;&#22411;&#30340;&#21305;&#37197;&#26426;&#21046;&#65292;&#25581;&#31034;&#20102;&#26368;&#22823;&#20540;&#25805;&#20316;&#20005;&#37325;&#20381;&#36182;&#20110;&#25991;&#26723;&#20013;&#30340;&#20849;&#29616;&#20449;&#21495;&#21644;&#37325;&#35201;&#21333;&#35789;&#65292;&#23545;&#20110;&#31264;&#23494;&#26816;&#32034;&#27169;&#22411;&#30340;&#21457;&#23637;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>https://arxiv.org/abs/2403.13291</link><description>&lt;p&gt;
&#20851;&#20110;&#21518;&#20132;&#20114;&#27169;&#22411;&#30340;&#21305;&#37197;&#26426;&#21046;&#21644;&#26631;&#35760;&#20462;&#21098;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
An Analysis on Matching Mechanisms and Token Pruning for Late-interaction Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13291
&lt;/p&gt;
&lt;p&gt;
&#31995;&#32479;&#20998;&#26512;&#20102;&#21518;&#20132;&#20114;&#27169;&#22411;&#30340;&#21305;&#37197;&#26426;&#21046;&#65292;&#25581;&#31034;&#20102;&#26368;&#22823;&#20540;&#25805;&#20316;&#20005;&#37325;&#20381;&#36182;&#20110;&#25991;&#26723;&#20013;&#30340;&#20849;&#29616;&#20449;&#21495;&#21644;&#37325;&#35201;&#21333;&#35789;&#65292;&#23545;&#20110;&#31264;&#23494;&#26816;&#32034;&#27169;&#22411;&#30340;&#21457;&#23637;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#65292;&#31264;&#23494;&#26816;&#32034;&#27169;&#22411;&#24050;&#25104;&#20026;&#20256;&#32479;&#20381;&#36182;&#31934;&#30830;&#21305;&#37197;&#21644;&#31232;&#30095;&#35789;&#34955;&#34920;&#31034;&#30340;&#26816;&#32034;&#27169;&#22411;&#30340;&#26377;&#24076;&#26395;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#19982;&#22823;&#22810;&#25968;&#31264;&#23494;&#26816;&#32034;&#27169;&#22411;&#20351;&#29992;&#21452;&#32534;&#30721;&#22120;&#23558;&#27599;&#20010;&#26597;&#35810;&#25110;&#25991;&#26723;&#32534;&#30721;&#20026;&#31264;&#23494;&#21521;&#37327;&#19981;&#21516;&#65292;&#26368;&#36817;&#25552;&#20986;&#30340;&#21518;&#20132;&#20114;&#22810;&#21521;&#37327;&#27169;&#22411;&#65288;&#20363;&#22914;ColBERT&#21644;COIL&#65289;&#36890;&#36807;&#20351;&#29992;&#25152;&#26377;&#26631;&#35760;&#23884;&#20837;&#26469;&#34920;&#31034;&#25991;&#26723;&#21644;&#26597;&#35810;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#26368;&#22823;&#25805;&#20316;&#30340;&#21644;&#26469;&#24314;&#27169;&#23427;&#20204;&#30340;&#30456;&#20851;&#24615;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#26816;&#32034;&#25928;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#32454;&#31890;&#24230;&#34920;&#31034;&#21487;&#33021;&#20250;&#23548;&#33268;&#23454;&#38469;&#25628;&#32034;&#31995;&#32479;&#26080;&#27861;&#25509;&#21463;&#30340;&#23384;&#20648;&#24320;&#38144;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#20998;&#26512;&#20102;&#36825;&#20123;&#21518;&#20132;&#20114;&#27169;&#22411;&#30340;&#21305;&#37197;&#26426;&#21046;&#65292;&#24182;&#23637;&#31034;&#20102;&#26368;&#22823;&#20540;&#25805;&#20316;&#20005;&#37325;&#20381;&#36182;&#20110;&#20849;&#29616;&#20449;&#21495;&#21644;&#25991;&#26723;&#20013;&#30340;&#19968;&#20123;&#37325;&#35201;&#21333;&#35789;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13291v1 Announce Type: new  Abstract: With the development of pre-trained language models, the dense retrieval models have become promising alternatives to the traditional retrieval models that rely on exact match and sparse bag-of-words representations. Different from most dense retrieval models using a bi-encoder to encode each query or document into a dense vector, the recently proposed late-interaction multi-vector models (i.e., ColBERT and COIL) achieve state-of-the-art retrieval effectiveness by using all token embeddings to represent documents and queries and modeling their relevance with a sum-of-max operation. However, these fine-grained representations may cause unacceptable storage overhead for practical search systems. In this study, we systematically analyze the matching mechanism of these late-interaction models and show that the sum-of-max operation heavily relies on the co-occurrence signals and some important words in the document. Based on these findings, w
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#33041;&#20449;&#21495;&#25913;&#36827;&#27861;&#24459;&#26696;&#20363;&#26816;&#32034;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#21453;&#39304;&#25216;&#26415;&#38590;&#20197;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.13242</link><description>&lt;p&gt;
&#29992;&#33041;&#20449;&#21495;&#25913;&#36827;&#27861;&#24459;&#26696;&#20363;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Improving Legal Case Retrieval with Brain Signals
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13242
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#33041;&#20449;&#21495;&#25913;&#36827;&#27861;&#24459;&#26696;&#20363;&#26816;&#32034;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#21453;&#39304;&#25216;&#26415;&#38590;&#20197;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13242v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#25688;&#35201;&#65306;&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#27861;&#24459;&#26696;&#20363;&#26816;&#32034;&#30340;&#20219;&#21153;&#21463;&#21040;&#20102;&#20449;&#24687;&#26816;&#32034;&#39046;&#22495;&#30340;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#20855;&#26377;&#38544;&#24335;&#29992;&#25143;&#21453;&#39304;&#65288;&#20363;&#22914;&#28857;&#20987;&#65289;&#30340;&#30456;&#20851;&#21453;&#39304;&#25216;&#26415;&#24050;&#34987;&#35777;&#26126;&#22312;&#20256;&#32479;&#25628;&#32034;&#20219;&#21153;&#65288;&#20363;&#22914;&#32593;&#32476;&#25628;&#32034;&#65289;&#20013;&#26159;&#26377;&#25928;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#27861;&#24459;&#26696;&#20363;&#26816;&#32034;&#20013;&#65292;&#25910;&#38598;&#30456;&#20851;&#21453;&#39304;&#38754;&#20020;&#19968;&#20123;&#38590;&#20197;&#22312;&#29616;&#26377;&#21453;&#39304;&#33539;&#24335;&#19979;&#35299;&#20915;&#30340;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#27861;&#24459;&#26696;&#20363;&#26816;&#32034;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#29992;&#25143;&#36890;&#24120;&#38656;&#35201;&#35814;&#32454;&#20102;&#35299;&#27861;&#24459;&#26696;&#20363;&#20043;&#38388;&#30340;&#20851;&#31995;&#25165;&#33021;&#27491;&#30830;&#21028;&#26029;&#23427;&#20204;&#30340;&#30456;&#20851;&#24615;&#12290;&#20256;&#32479;&#30340;&#21453;&#39304;&#20449;&#21495;&#65288;&#20363;&#22914;&#28857;&#20987;&#65289;&#22826;&#31895;&#31961;&#20102;&#65292;&#22240;&#20026;&#23427;&#20204;&#19981;&#21453;&#26144;&#20219;&#20309;&#32454;&#31890;&#24230;&#30340;&#30456;&#20851;&#20449;&#24687;&#12290;&#20854;&#27425;&#65292;&#27861;&#24459;&#26696;&#20363;&#25991;&#20214;&#36890;&#24120;&#24456;&#38271;&#65292;&#29992;&#25143;&#36890;&#24120;&#38656;&#35201;&#29978;&#33267;&#21313;&#20960;&#20998;&#38047;&#26469;&#38405;&#35835;&#21644;&#29702;&#35299;&#23427;&#20204;&#12290;&#24403;&#29992;&#25143;&#20960;&#20046;&#28857;&#20987;&#21644;&#26816;&#26597;&#26102;&#65292;&#31616;&#21333;&#30340;&#34892;&#20026;&#20449;&#21495;&#65288;&#20363;&#22914;&#28857;&#20987;&#21644;&#27880;&#35270;&#36319;&#36394;&#65289;&#20960;&#20046;&#27809;&#26377;&#29992;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13242v1 Announce Type: new  Abstract: The tasks of legal case retrieval have received growing attention from the IR community in the last decade. Relevance feedback techniques with implicit user feedback (e.g., clicks) have been demonstrated to be effective in traditional search tasks (e.g., Web search). In legal case retrieval, however, collecting relevance feedback faces a couple of challenges that are difficult to resolve under existing feedback paradigms. First, legal case retrieval is a complex task as users often need to understand the relationship between legal cases in detail to correctly judge their relevance. Traditional feedback signal such as clicks is too coarse to use as they do not reflect any fine-grained relevance information. Second, legal case documents are usually long, users often need even tens of minutes to read and understand them. Simple behavior signal such as clicks and eye-tracking fixations can hardly be useful when users almost click and examine
&lt;/p&gt;</description></item><item><title>&#23558;&#33647;&#29289;SMILES&#23383;&#31526;&#20018;&#35270;&#20026;&#21477;&#23376;&#24182;&#21033;&#29992;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#36827;&#34892;&#33647;&#29289;&#20998;&#31867;&#65292;&#35777;&#23454;&#20102;&#36890;&#36807;&#31616;&#21333;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#30340;&#21487;&#33021;&#24615;</title><link>https://arxiv.org/abs/2403.12984</link><description>&lt;p&gt;
&#24403;SMILES&#25317;&#26377;&#35821;&#35328;&#65306;&#20351;&#29992;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#23545;&#33647;&#29289;SMILES&#23383;&#31526;&#20018;&#36827;&#34892;&#33647;&#29289;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
When SMILES have Language: Drug Classification using Text Classification Methods on Drug SMILES Strings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12984
&lt;/p&gt;
&lt;p&gt;
&#23558;&#33647;&#29289;SMILES&#23383;&#31526;&#20018;&#35270;&#20026;&#21477;&#23376;&#24182;&#21033;&#29992;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#36827;&#34892;&#33647;&#29289;&#20998;&#31867;&#65292;&#35777;&#23454;&#20102;&#36890;&#36807;&#31616;&#21333;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#30340;&#21487;&#33021;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22797;&#26434;&#30340;&#21270;&#23398;&#32467;&#26500;&#65292;&#22914;&#33647;&#29289;&#65292;&#36890;&#24120;&#30001;SMILES&#23383;&#31526;&#20018;&#26469;&#23450;&#20041;&#65292;&#20316;&#20026;&#20998;&#23376;&#21644;&#38190;&#30340;&#24207;&#21015;&#12290;&#36825;&#20123;SMILES&#23383;&#31526;&#20018;&#22312;&#19981;&#21516;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#33647;&#29289;&#30456;&#20851;&#30740;&#31350;&#21644;&#34920;&#31034;&#24037;&#20316;&#20013;&#20351;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25670;&#33073;&#22797;&#26434;&#30340;&#34920;&#31034;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#22914;&#26524;&#25105;&#20204;&#23558;&#33647;&#29289;SMILES&#35270;&#20026;&#24120;&#35268;&#21477;&#23376;&#65292;&#24182;&#36827;&#34892;&#25991;&#26412;&#20998;&#31867;&#20197;&#36827;&#34892;&#33647;&#29289;&#20998;&#31867;&#20250;&#24590;&#26679;&#65311;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#23454;&#20102;&#36825;&#31181;&#21487;&#33021;&#24615;&#65292;&#33719;&#24471;&#20102;&#38750;&#24120;&#26377;&#31454;&#20105;&#21147;&#30340;&#20998;&#25968;&#12290;&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#23558;&#27599;&#20010;&#21407;&#23376;&#21644;&#38190;&#35270;&#20026;&#21477;&#23376;&#32452;&#20214;&#30340;&#27010;&#24565;&#65292;&#21033;&#29992;&#22522;&#26412;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#23545;&#33647;&#29289;&#31867;&#22411;&#36827;&#34892;&#20998;&#31867;&#65292;&#34920;&#26126;&#22797;&#26434;&#30340;&#38382;&#39064;&#20063;&#21487;&#20197;&#29992;&#26356;&#31616;&#21333;&#30340;&#35270;&#35282;&#26469;&#35299;&#20915;&#12290;&#25968;&#25454;&#21644;&#20195;&#30721;&#21487;&#22312;&#27492;&#22788;&#25214;&#21040;&#65306;https://github.com/azminewasi/Drug-Classification-NLP&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12984v1 Announce Type: cross  Abstract: Complex chemical structures, like drugs, are usually defined by SMILES strings as a sequence of molecules and bonds. These SMILES strings are used in different complex machine learning-based drug-related research and representation works. Escaping from complex representation, in this work, we pose a single question: What if we treat drug SMILES as conventional sentences and engage in text classification for drug classification? Our experiments affirm the possibility with very competitive scores. The study explores the notion of viewing each atom and bond as sentence components, employing basic NLP methods to categorize drug types, proving that complex problems can also be solved with simpler perspectives. The data and code are available here: https://github.com/azminewasi/Drug-Classification-NLP.
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#30740;&#31350;&#38754;&#20020;&#30528;&#20844;&#24179;&#27604;&#36739;&#12289;&#36873;&#25321;&#23646;&#24615;&#20998;&#26512;&#32570;&#20047;&#20197;&#21450;&#36807;&#24230;&#20851;&#27880;&#23792;&#20540;&#24615;&#33021;&#31561;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.12660</link><description>&lt;p&gt;
ERASE&#65306;&#28145;&#24230;&#25512;&#33616;&#31995;&#32479;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
ERASE: Benchmarking Feature Selection Methods for Deep Recommender Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12660
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#30740;&#31350;&#38754;&#20020;&#30528;&#20844;&#24179;&#27604;&#36739;&#12289;&#36873;&#25321;&#23646;&#24615;&#20998;&#26512;&#32570;&#20047;&#20197;&#21450;&#36807;&#24230;&#20851;&#27880;&#23792;&#20540;&#24615;&#33021;&#31561;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#25512;&#33616;&#31995;&#32479;(DRS)&#36234;&#26469;&#36234;&#20381;&#36182;&#20110;&#22823;&#37327;&#29305;&#24449;&#23383;&#27573;&#26469;&#25552;&#20379;&#26356;&#31934;&#20934;&#30340;&#25512;&#33616;&#12290;&#26377;&#25928;&#30340;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#22240;&#27492;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#65292;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#20934;&#30830;&#24615;&#24182;&#20248;&#21270;&#23384;&#20648;&#25928;&#29575;&#65292;&#20197;&#28385;&#36275;&#37096;&#32626;&#38656;&#27714;&#12290;&#30740;&#31350;&#39046;&#22495;&#65292;&#29305;&#21035;&#26159;&#22312;DRS&#30340;&#32972;&#26223;&#19979;&#65292;&#23578;&#22788;&#20110;&#21021;&#26399;&#38454;&#27573;&#65292;&#38754;&#20020;&#19977;&#20010;&#26680;&#24515;&#25361;&#25112;&#65306;&#39318;&#20808;&#65292;&#30740;&#31350;&#35770;&#25991;&#20043;&#38388;&#23454;&#39564;&#35774;&#32622;&#30340;&#24046;&#24322;&#24448;&#24448;&#23548;&#33268;&#19981;&#20844;&#24179;&#27604;&#36739;&#65292;&#36974;&#34109;&#20102;&#23454;&#36341;&#35265;&#35299;&#12290;&#20854;&#27425;&#65292;&#29616;&#26377;&#25991;&#29486;&#32570;&#20047;&#22522;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#36873;&#25321;&#23646;&#24615;&#30340;&#35814;&#32454;&#20998;&#26512;&#65292;&#24182;&#19988;&#32570;&#20047;&#23545;&#36873;&#25321;&#25216;&#26415;&#21644;DRS&#39592;&#24178;&#20043;&#38388;&#36827;&#34892;&#20840;&#38754;&#27604;&#36739;&#30340;&#38480;&#21046;&#24615;&#25991;&#31456;&#30340;&#36890;&#29992;&#24615;&#30740;&#31350;&#21644;&#37096;&#32626;&#12290;&#26368;&#21518;&#65292;&#30740;&#31350;&#24448;&#24448;&#19987;&#27880;&#20110;&#27604;&#36739;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#21487;&#36798;&#21040;&#30340;&#23792;&#20540;&#24615;&#33021;&#65292;&#36825;&#31181;&#26041;&#27861;&#36890;&#24120;&#22312;&#35745;&#31639;&#26041;&#38754;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12660v1 Announce Type: cross  Abstract: Deep Recommender Systems (DRS) are increasingly dependent on a large number of feature fields for more precise recommendations. Effective feature selection methods are consequently becoming critical for further enhancing the accuracy and optimizing storage efficiencies to align with the deployment demands. This research area, particularly in the context of DRS, is nascent and faces three core challenges. Firstly, variant experimental setups across research papers often yield unfair comparisons, obscuring practical insights. Secondly, the existing literature's lack of detailed analysis on selection attributes, based on large-scale datasets and a thorough comparison among selection techniques and DRS backbones, restricts the generalizability of findings and impedes deployment on DRS. Lastly, research often focuses on comparing the peak performance achievable by feature selection methods, an approach that is typically computationally infe
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AlignRec&#30340;&#23545;&#40784;&#21644;&#35757;&#32451;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#27169;&#24577;&#25512;&#33616;&#20013;&#30340;&#19981;&#23545;&#40784;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#25512;&#33616;&#30446;&#26631;&#20998;&#35299;&#20026;&#19977;&#20010;&#23545;&#40784;&#37096;&#20998;&#65292;&#23454;&#29616;&#20869;&#23481;&#20869;&#37096;&#23545;&#40784;&#12289;&#20869;&#23481;&#19982;&#20998;&#31867;ID&#20043;&#38388;&#30340;&#23545;&#40784;&#20197;&#21450;&#29992;&#25143;&#21644;&#39033;&#30446;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290;</title><link>https://arxiv.org/abs/2403.12384</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#22810;&#27169;&#24577;&#25512;&#33616;&#30340;&#23545;&#40784;&#21644;&#35757;&#32451;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
An Aligning and Training Framework for Multimodal Recommendations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12384
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AlignRec&#30340;&#23545;&#40784;&#21644;&#35757;&#32451;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#27169;&#24577;&#25512;&#33616;&#20013;&#30340;&#19981;&#23545;&#40784;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#25512;&#33616;&#30446;&#26631;&#20998;&#35299;&#20026;&#19977;&#20010;&#23545;&#40784;&#37096;&#20998;&#65292;&#23454;&#29616;&#20869;&#23481;&#20869;&#37096;&#23545;&#40784;&#12289;&#20869;&#23481;&#19982;&#20998;&#31867;ID&#20043;&#38388;&#30340;&#23545;&#40784;&#20197;&#21450;&#29992;&#25143;&#21644;&#39033;&#30446;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22810;&#23186;&#20307;&#24212;&#29992;&#30340;&#21457;&#23637;&#65292;&#22810;&#27169;&#24577;&#25512;&#33616;&#27491;&#22312;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#21033;&#29992;&#36229;&#36234;&#29992;&#25143;&#20132;&#20114;&#30340;&#20016;&#23500;&#19978;&#19979;&#25991;&#12290;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#23558;&#22810;&#27169;&#24577;&#20449;&#24687;&#35270;&#20026;&#36741;&#21161;&#65292;&#29992;&#20110;&#24110;&#21161;&#23398;&#20064;ID&#29305;&#24449;&#65307;&#28982;&#32780;&#65292;&#22810;&#27169;&#24577;&#20869;&#23481;&#29305;&#24449;&#21644;ID&#29305;&#24449;&#20043;&#38388;&#23384;&#22312;&#35821;&#20041;&#24046;&#36317;&#65292;&#30452;&#25509;&#23558;&#22810;&#27169;&#24577;&#20449;&#24687;&#20316;&#20026;&#36741;&#21161;&#20351;&#29992;&#20250;&#23548;&#33268;&#29992;&#25143;&#21644;&#39033;&#30446;&#34920;&#31034;&#30340;&#19981;&#23545;&#40784;&#12290;&#26412;&#25991;&#39318;&#20808;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#22810;&#27169;&#24577;&#25512;&#33616;&#20013;&#30340;&#19981;&#23545;&#40784;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AlignRec&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;AlignRec&#20013;&#65292;&#25512;&#33616;&#30446;&#26631;&#34987;&#20998;&#35299;&#20026;&#19977;&#20010;&#23545;&#40784;&#37096;&#20998;&#65292;&#21363;&#20869;&#23481;&#20869;&#37096;&#23545;&#40784;&#65292;&#20869;&#23481;&#19982;&#20998;&#31867;ID&#20043;&#38388;&#30340;&#23545;&#40784;&#20197;&#21450;&#29992;&#25143;&#21644;&#39033;&#30446;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290;&#27599;&#20010;&#23545;&#40784;&#37096;&#20998;&#37117;&#30001;&#29305;&#23450;&#30340;&#30446;&#26631;&#20989;&#25968;&#26469;&#34920;&#24449;&#65292;&#24182;&#25972;&#21512;&#21040;&#25105;&#20204;&#30340;&#22810;&#27169;&#24577;&#25512;&#33616;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12384v1 Announce Type: cross  Abstract: With the development of multimedia applications, multimodal recommendations are playing an essential role, as they can leverage rich contexts beyond user interactions. Existing methods mainly regard multimodal information as an auxiliary, using them to help learn ID features; however, there exist semantic gaps among multimodal content features and ID features, for which directly using multimodal information as an auxiliary would lead to misalignment in representations of users and items. In this paper, we first systematically investigate the misalignment issue in multimodal recommendations, and propose a solution named AlignRec. In AlignRec, the recommendation objective is decomposed into three alignments, namely alignment within contents, alignment between content and categorical ID, and alignment between users and items. Each alignment is characterized by a specific objective function and is integrated into our multimodal recommendat
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#29983;&#25104;&#24335;&#29992;&#25143;&#27169;&#25311;&#22120;&#22312;&#23545;&#35805;&#25512;&#33616;&#20013;&#23637;&#29616;&#20986;&#28508;&#21147;&#65292;&#26032;&#30340;&#21327;&#35758;&#36890;&#36807;&#20116;&#20010;&#20219;&#21153;&#35780;&#20272;&#20102;&#35821;&#35328;&#27169;&#22411;&#27169;&#25311;&#20154;&#31867;&#34892;&#20026;&#30340;&#20934;&#30830;&#31243;&#24230;&#65292;&#25581;&#31034;&#20102;&#27169;&#22411;&#19982;&#20154;&#31867;&#34892;&#20026;&#30340;&#20559;&#24046;&#65292;&#24182;&#25552;&#20986;&#20102;&#22914;&#20309;&#36890;&#36807;&#27169;&#22411;&#36873;&#25321;&#21644;&#25552;&#31034;&#31574;&#30053;&#20943;&#23569;&#36825;&#20123;&#20559;&#24046;&#12290;</title><link>https://arxiv.org/abs/2403.09738</link><description>&lt;p&gt;
&#35780;&#20272;&#22823;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#23545;&#35805;&#25512;&#33616;&#20013;&#29983;&#25104;&#29992;&#25143;&#27169;&#25311;&#22120;
&lt;/p&gt;
&lt;p&gt;
Evaluating Large Language Models as Generative User Simulators for Conversational Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09738
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#29983;&#25104;&#24335;&#29992;&#25143;&#27169;&#25311;&#22120;&#22312;&#23545;&#35805;&#25512;&#33616;&#20013;&#23637;&#29616;&#20986;&#28508;&#21147;&#65292;&#26032;&#30340;&#21327;&#35758;&#36890;&#36807;&#20116;&#20010;&#20219;&#21153;&#35780;&#20272;&#20102;&#35821;&#35328;&#27169;&#22411;&#27169;&#25311;&#20154;&#31867;&#34892;&#20026;&#30340;&#20934;&#30830;&#31243;&#24230;&#65292;&#25581;&#31034;&#20102;&#27169;&#22411;&#19982;&#20154;&#31867;&#34892;&#20026;&#30340;&#20559;&#24046;&#65292;&#24182;&#25552;&#20986;&#20102;&#22914;&#20309;&#36890;&#36807;&#27169;&#22411;&#36873;&#25321;&#21644;&#25552;&#31034;&#31574;&#30053;&#20943;&#23569;&#36825;&#20123;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#25104;&#29992;&#25143;&#26159;&#23545;&#35805;&#25512;&#33616;&#31995;&#32479;&#35780;&#20272;&#20013;&#25104;&#26412;&#25928;&#30410;&#36739;&#39640;&#30340;&#30495;&#23454;&#29992;&#25143;&#20195;&#29702;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#22312;&#27169;&#25311;&#31867;&#20284;&#20154;&#31867;&#34892;&#20026;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#36825;&#24341;&#21457;&#20102;&#23427;&#20204;&#33021;&#21542;&#20195;&#34920;&#22810;&#26679;&#21270;&#29992;&#25143;&#32676;&#20307;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#21327;&#35758;&#65292;&#29992;&#20110;&#34913;&#37327;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#20934;&#30830;&#27169;&#25311;&#23545;&#35805;&#25512;&#33616;&#20013;&#20154;&#31867;&#34892;&#20026;&#30340;&#31243;&#24230;&#12290;&#35813;&#21327;&#35758;&#30001;&#20116;&#20010;&#20219;&#21153;&#32452;&#25104;&#65292;&#27599;&#20010;&#20219;&#21153;&#26088;&#22312;&#35780;&#20272;&#21512;&#25104;&#29992;&#25143;&#24212;&#35813;&#34920;&#29616;&#20986;&#30340;&#20851;&#38190;&#29305;&#24615;&#65306;&#36873;&#25321;&#35201;&#35848;&#35770;&#30340;&#29289;&#21697;&#65292;&#34920;&#36798;&#20108;&#36827;&#21046;&#20559;&#22909;&#65292;&#34920;&#36798;&#24320;&#25918;&#24335;&#20559;&#22909;&#65292;&#35831;&#27714;&#25512;&#33616;&#20197;&#21450;&#25552;&#20379;&#21453;&#39304;&#12290;&#36890;&#36807;&#23545;&#22522;&#20934;&#27169;&#25311;&#22120;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#20219;&#21153;&#26377;&#25928;&#22320;&#25581;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#34892;&#20026;&#30340;&#20559;&#24046;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;&#22914;&#20309;&#36890;&#36807;&#27169;&#22411;&#36873;&#25321;&#21644;&#25552;&#31034;&#31574;&#30053;&#20943;&#23569;&#36825;&#20123;&#20559;&#24046;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09738v1 Announce Type: cross  Abstract: Synthetic users are cost-effective proxies for real users in the evaluation of conversational recommender systems. Large language models show promise in simulating human-like behavior, raising the question of their ability to represent a diverse population of users. We introduce a new protocol to measure the degree to which language models can accurately emulate human behavior in conversational recommendation. This protocol is comprised of five tasks, each designed to evaluate a key property that a synthetic user should exhibit: choosing which items to talk about, expressing binary preferences, expressing open-ended preferences, requesting recommendations, and giving feedback. Through evaluation of baseline simulators, we demonstrate these tasks effectively reveal deviations of language models from human behavior, and offer insights on how to reduce the deviations with model selection and prompting strategies.
&lt;/p&gt;</description></item><item><title>&#31383;&#21475;&#22823;&#23567;&#38750;&#24120;&#23567;&#29978;&#33267;&#21482;&#26377;4&#20010;&#26631;&#35760;&#26102;&#65292;&#20173;&#21487;&#20445;&#25345;&#19982;&#20197;&#24448;&#20132;&#21449;&#32534;&#30721;&#22120;&#30456;&#24403;&#30340;&#25928;&#26524;&#65292;&#21516;&#26102;&#38477;&#20302;&#20869;&#23384;&#38656;&#27714;&#24182;&#25552;&#39640;&#25512;&#29702;&#36895;&#24230;&#12290;</title><link>https://arxiv.org/abs/2312.17649</link><description>&lt;p&gt;
&#30740;&#31350;&#31232;&#30095;&#27880;&#24847;&#21147;&#23545;&#20132;&#21449;&#32534;&#30721;&#22120;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Investigating the Effects of Sparse Attention on Cross-Encoders
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.17649
&lt;/p&gt;
&lt;p&gt;
&#31383;&#21475;&#22823;&#23567;&#38750;&#24120;&#23567;&#29978;&#33267;&#21482;&#26377;4&#20010;&#26631;&#35760;&#26102;&#65292;&#20173;&#21487;&#20445;&#25345;&#19982;&#20197;&#24448;&#20132;&#21449;&#32534;&#30721;&#22120;&#30456;&#24403;&#30340;&#25928;&#26524;&#65292;&#21516;&#26102;&#38477;&#20302;&#20869;&#23384;&#38656;&#27714;&#24182;&#25552;&#39640;&#25512;&#29702;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#21449;&#32534;&#30721;&#22120;&#26159;&#26377;&#25928;&#30340;&#27573;&#33853;&#21644;&#25991;&#26723;&#37325;&#26032;&#25490;&#24207;&#22120;&#65292;&#20294;&#25928;&#29575;&#19981;&#22914;&#20854;&#20182;&#31070;&#32463;&#25110;&#32463;&#20856;&#26816;&#32034;&#27169;&#22411;&#12290;&#19968;&#20123;&#20808;&#21069;&#30340;&#30740;&#31350;&#24212;&#29992;&#31383;&#21475;&#33258;&#27880;&#24847;&#21147;&#26469;&#20351;&#20132;&#21449;&#32534;&#30721;&#22120;&#26356;&#26377;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30740;&#31350;&#24182;&#26410;&#25506;&#35752;&#19981;&#21516;&#27880;&#24847;&#21147;&#27169;&#24335;&#25110;&#31383;&#21475;&#22823;&#23567;&#30340;&#28508;&#21147;&#21644;&#38480;&#21046;&#12290;&#25105;&#20204;&#22635;&#34917;&#20102;&#36825;&#19968;&#31354;&#30333;&#65292;&#24182;&#31995;&#32479;&#22320;&#20998;&#26512;&#20102;&#22914;&#20309;&#20943;&#23569;&#26631;&#35760;&#20132;&#20114;&#32780;&#19981;&#25439;&#23475;&#37325;&#26032;&#25490;&#24207;&#30340;&#26377;&#25928;&#24615;&#12290;&#36890;&#36807;&#23454;&#39564;&#20351;&#29992;&#38750;&#23545;&#31216;&#27880;&#24847;&#21147;&#21644;&#19981;&#21516;&#30340;&#31383;&#21475;&#22823;&#23567;&#65292;&#25105;&#20204;&#21457;&#29616;&#26597;&#35810;&#26631;&#35760;&#19981;&#38656;&#35201;&#20851;&#27880;&#27573;&#33853;&#25110;&#25991;&#26723;&#26631;&#35760;&#20063;&#33021;&#23454;&#29616;&#26377;&#25928;&#30340;&#37325;&#26032;&#25490;&#24207;&#65292;&#32780;&#38750;&#24120;&#23567;&#30340;&#31383;&#21475;&#22823;&#23567;&#23601;&#36275;&#22815;&#20102;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#21363;&#20351;&#26159;4&#20010;&#26631;&#35760;&#30340;&#31383;&#21475;&#20173;&#28982;&#33021;&#22815;&#36798;&#21040;&#19982;&#20197;&#21069;&#30340;&#20132;&#21449;&#32534;&#30721;&#22120;&#30456;&#24403;&#30340;&#26377;&#25928;&#24615;&#65292;&#21516;&#26102;&#23558;&#20869;&#23384;&#35201;&#27714;&#38477;&#20302;&#33267;&#23569;22% / 59%&#65292;&#24182;&#22312; passages / documents &#30340;&#25512;&#29702;&#26102;&#38388;&#19978;&#24555;1% / 43%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.17649v2 Announce Type: replace  Abstract: Cross-encoders are effective passage and document re-rankers but less efficient than other neural or classic retrieval models. A few previous studies have applied windowed self-attention to make cross-encoders more efficient. However, these studies did not investigate the potential and limits of different attention patterns or window sizes. We close this gap and systematically analyze how token interactions can be reduced without harming the re-ranking effectiveness. Experimenting with asymmetric attention and different window sizes, we find that the query tokens do not need to attend to the passage or document tokens for effective re-ranking and that very small window sizes suffice. In our experiments, even windows of 4 tokens still yield effectiveness on par with previous cross-encoders while reducing the memory requirements by at least 22% / 59% and being 1% / 43% faster at inference time for passages / documents.
&lt;/p&gt;</description></item><item><title>&#21487;&#39564;&#35777;&#29983;&#25104;&#20013;&#26816;&#32034;&#30340;&#25991;&#20214;&#19981;&#20165;&#24110;&#21161;LLM&#29983;&#25104;&#27491;&#30830;&#31572;&#26696;&#65292;&#36824;&#20316;&#20026;&#29992;&#25143;&#39564;&#35777;LLM&#36755;&#20986;&#30340;&#35777;&#25454;&#65292;&#20294;&#30446;&#21069;&#24191;&#27867;&#20351;&#29992;&#30340;&#26816;&#32034;&#22120;&#24050;&#25104;&#20026;&#24615;&#33021;&#29942;&#39048;&#65292;&#38656;&#35201;&#35299;&#20915;&#12290;</title><link>https://arxiv.org/abs/2311.07838</link><description>&lt;p&gt;
LLatrieval&#65306;LLM-&#39564;&#35777;&#26816;&#32034;&#29992;&#20110;&#21487;&#39564;&#35777;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
LLatrieval: LLM-Verified Retrieval for Verifiable Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.07838
&lt;/p&gt;
&lt;p&gt;
&#21487;&#39564;&#35777;&#29983;&#25104;&#20013;&#26816;&#32034;&#30340;&#25991;&#20214;&#19981;&#20165;&#24110;&#21161;LLM&#29983;&#25104;&#27491;&#30830;&#31572;&#26696;&#65292;&#36824;&#20316;&#20026;&#29992;&#25143;&#39564;&#35777;LLM&#36755;&#20986;&#30340;&#35777;&#25454;&#65292;&#20294;&#30446;&#21069;&#24191;&#27867;&#20351;&#29992;&#30340;&#26816;&#32034;&#22120;&#24050;&#25104;&#20026;&#24615;&#33021;&#29942;&#39048;&#65292;&#38656;&#35201;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#39564;&#35777;&#29983;&#25104;&#26088;&#22312;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#29983;&#25104;&#20855;&#26377;&#25903;&#25745;&#25991;&#20214;&#30340;&#25991;&#26412;&#65292;&#36825;&#20351;&#29992;&#25143;&#33021;&#22815;&#28789;&#27963;&#39564;&#35777;&#31572;&#26696;&#65292;&#24182;&#20351;LLM&#30340;&#36755;&#20986;&#26356;&#21487;&#38752;&#12290;&#26816;&#32034;&#22312;&#21487;&#39564;&#35777;&#29983;&#25104;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#26816;&#32034;&#21040;&#30340;&#25991;&#20214;&#19981;&#20165;&#34917;&#20805;&#30693;&#35782;&#20197;&#24110;&#21161;LLM&#29983;&#25104;&#27491;&#30830;&#31572;&#26696;&#65292;&#36824;&#20316;&#20026;&#25903;&#25345;&#29992;&#25143;&#39564;&#35777;LLM&#36755;&#20986;&#30340;&#35777;&#25454;&#12290;&#28982;&#32780;&#65292;&#24191;&#27867;&#20351;&#29992;&#30340;&#26816;&#32034;&#22120;&#25104;&#20026;&#25972;&#20010;&#27969;&#31243;&#30340;&#29942;&#39048;&#65292;&#24182;&#38480;&#21046;&#20102;&#25972;&#20307;&#24615;&#33021;&#12290;&#30001;&#20110;&#36890;&#24120;&#20855;&#26377;&#30340;&#21442;&#25968;&#27604;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23569;&#24471;&#22810;&#65292;&#24182;&#19988;&#23578;&#26410;&#35777;&#26126;&#33021;&#22815;&#33391;&#22909;&#22320;&#25193;&#23637;&#21040;LLM&#30340;&#35268;&#27169;&#65292;&#22240;&#27492;&#23427;&#20204;&#30340;&#33021;&#21147;&#36890;&#24120;&#27604;LLMs&#24046;&#12290;&#22914;&#26524;&#26816;&#32034;&#22120;&#26410;&#33021;&#27491;&#30830;&#25214;&#21040;&#25903;&#25345;&#25991;&#20214;&#65292;&#21017;LLM&#23558;&#26080;&#27861;&#29983;&#25104;&#27491;&#30830;&#21644;&#21487;&#39564;&#35777;&#30340;&#31572;&#26696;&#65292;&#36825;&#20250;&#25513;&#30422;LLM&#30340;&#26174;&#33879;&#33021;&#21147;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.07838v2 Announce Type: replace  Abstract: Verifiable generation aims to let the large language model (LLM) generate text with supporting documents, which enables the user to flexibly verify the answer and makes the LLM's output more reliable. Retrieval plays a crucial role in verifiable generation. Specifically, the retrieved documents not only supplement knowledge to help the LLM generate correct answers, but also serve as supporting evidence for the user to verify the LLM's output. However, the widely used retrievers become the bottleneck of the entire pipeline and limit the overall performance. Their capabilities are usually inferior to LLMs since they often have much fewer parameters than the large language model and have not been demonstrated to scale well to the size of LLMs. If the retriever does not correctly find the supporting documents, the LLM can not generate the correct and verifiable answer, which overshadows the LLM's remarkable abilities. To address these li
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24037;&#20316;&#36127;&#36733;&#21644;&#23398;&#20064;&#30340;Z-&#32034;&#24341;&#21464;&#20307;&#65292;&#36890;&#36807;&#20248;&#21270;&#23384;&#20648;&#24067;&#23616;&#21644;&#25628;&#32034;&#32467;&#26500;&#65292;&#25913;&#21892;&#20102;&#33539;&#22260;&#26597;&#35810;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#39029;&#38754;&#36339;&#36291;&#26426;&#21046;&#36827;&#19968;&#27493;&#25552;&#21319;&#26597;&#35810;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#32034;&#24341;&#22312;&#33539;&#22260;&#26597;&#35810;&#26102;&#38388;&#12289;&#28857;&#26597;&#35810;&#24615;&#33021;&#21644;&#26500;&#24314;&#26102;&#38388;&#19982;&#32034;&#24341;&#22823;&#23567;&#20043;&#38388;&#20445;&#25345;&#20102;&#33391;&#22909;&#30340;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2310.04268</link><description>&lt;p&gt;
&#22522;&#20110;&#24037;&#20316;&#36127;&#36733;&#21644;&#23398;&#20064;&#30340;Z-&#32034;&#24341;
&lt;/p&gt;
&lt;p&gt;
Workload-aware and Learned Z-Indexes. (arXiv:2310.04268v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04268
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24037;&#20316;&#36127;&#36733;&#21644;&#23398;&#20064;&#30340;Z-&#32034;&#24341;&#21464;&#20307;&#65292;&#36890;&#36807;&#20248;&#21270;&#23384;&#20648;&#24067;&#23616;&#21644;&#25628;&#32034;&#32467;&#26500;&#65292;&#25913;&#21892;&#20102;&#33539;&#22260;&#26597;&#35810;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#39029;&#38754;&#36339;&#36291;&#26426;&#21046;&#36827;&#19968;&#27493;&#25552;&#21319;&#26597;&#35810;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#32034;&#24341;&#22312;&#33539;&#22260;&#26597;&#35810;&#26102;&#38388;&#12289;&#28857;&#26597;&#35810;&#24615;&#33021;&#21644;&#26500;&#24314;&#26102;&#38388;&#19982;&#32034;&#24341;&#22823;&#23567;&#20043;&#38388;&#20445;&#25345;&#20102;&#33391;&#22909;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24037;&#20316;&#36127;&#36733;&#21644;&#23398;&#20064;&#30340;Z-&#32034;&#24341;&#30340;&#21464;&#20307;&#65292;&#35813;&#32034;&#24341;&#21516;&#26102;&#20248;&#21270;&#23384;&#20648;&#24067;&#23616;&#21644;&#25628;&#32034;&#32467;&#26500;&#65292;&#20316;&#20026;&#35299;&#20915;&#31354;&#38388;&#32034;&#24341;&#30340;&#25361;&#25112;&#30340;&#21487;&#34892;&#35299;&#20915;&#26041;&#26696;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#21046;&#23450;&#20102;&#19968;&#20010;&#25104;&#26412;&#20989;&#25968;&#65292;&#29992;&#20110;&#34913;&#37327;Z-&#32034;&#24341;&#22312;&#25968;&#25454;&#38598;&#19978;&#30340;&#33539;&#22260;&#26597;&#35810;&#24037;&#20316;&#36127;&#36733;&#19979;&#30340;&#24615;&#33021;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#20998;&#21306;&#21644;&#25490;&#24207;&#20248;&#21270;Z-&#32034;&#24341;&#32467;&#26500;&#65292;&#26368;&#23567;&#21270;&#25104;&#26412;&#20989;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39029;&#38754;&#36339;&#36291;&#26426;&#21046;&#65292;&#36890;&#36807;&#20943;&#23569;&#23545;&#26080;&#20851;&#25968;&#25454;&#39029;&#38754;&#30340;&#35775;&#38382;&#26469;&#25913;&#21892;&#26597;&#35810;&#24615;&#33021;&#12290;&#25105;&#20204;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#30456;&#27604;&#22522;&#32447;&#65292;&#25105;&#20204;&#30340;&#32034;&#24341;&#24179;&#22343;&#25913;&#21892;&#20102;40%&#30340;&#33539;&#22260;&#26597;&#35810;&#26102;&#38388;&#65292;&#21516;&#26102;&#22987;&#32456;&#34920;&#29616;&#24471;&#26356;&#22909;&#25110;&#19982;&#26368;&#20808;&#36827;&#30340;&#31354;&#38388;&#32034;&#24341;&#30456;&#24403;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#32034;&#24341;&#22312;&#25552;&#20379;&#26377;&#21033;&#30340;&#26500;&#24314;&#26102;&#38388;&#21644;&#32034;&#24341;&#22823;&#23567;&#26435;&#34913;&#30340;&#21516;&#26102;&#65292;&#20445;&#25345;&#33391;&#22909;&#30340;&#28857;&#26597;&#35810;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, a learned and workload-aware variant of a Z-index, which jointly optimizes storage layout and search structures, as a viable solution for the above challenges of spatial indexing. Specifically, we first formulate a cost function to measure the performance of a Z-index on a dataset for a range-query workload. Then, we optimize the Z-index structure by minimizing the cost function through adaptive partitioning and ordering for index construction. Moreover, we design a novel page-skipping mechanism to improve its query performance by reducing access to irrelevant data pages. Our extensive experiments show that our index improves range query time by 40% on average over the baselines, while always performing better or comparably to state-of-the-art spatial indexes. Additionally, our index maintains good point query performance while providing favourable construction time and index size tradeoffs.
&lt;/p&gt;</description></item><item><title>I^3 Retriever&#23558;&#38544;&#24335;&#20132;&#20114;&#32435;&#20837;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#27573;&#33853;&#26816;&#32034;&#30340;&#25928;&#26524;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.02371</link><description>&lt;p&gt;
I^3 Retriever: &#23558;&#38544;&#24335;&#20132;&#20114;&#32435;&#20837;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20197;&#29992;&#20110;&#27573;&#33853;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
I^3 Retriever: Incorporating Implicit Interaction in Pre-trained Language Models for Passage Retrieval. (arXiv:2306.02371v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02371
&lt;/p&gt;
&lt;p&gt;
I^3 Retriever&#23558;&#38544;&#24335;&#20132;&#20114;&#32435;&#20837;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#27573;&#33853;&#26816;&#32034;&#30340;&#25928;&#26524;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27573;&#33853;&#26816;&#32034;&#26159;&#35768;&#22810;&#20449;&#24687;&#31995;&#32479;&#20013;&#30340;&#22522;&#26412;&#20219;&#21153;&#65292;&#20363;&#22914;&#32593;&#32476;&#25628;&#32034;&#21644;&#38382;&#31572;&#31995;&#32479;&#65292;&#20854;&#20013;&#25928;&#29575;&#21644;&#25928;&#26524;&#37117;&#26159;&#20851;&#38190;&#38382;&#39064;&#12290;&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#30340;&#31070;&#32463;&#26816;&#32034;&#22120;&#65292;&#22914;&#21452;&#32534;&#30721;&#22120;&#65292;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#21457;&#29616;&#65292;&#21452;&#32534;&#30721;&#22120;&#30340;&#24615;&#33021;&#36890;&#24120;&#21463;&#38480;&#20110;&#24573;&#30053;&#26597;&#35810;&#21644;&#20505;&#36873;&#27573;&#33853;&#20043;&#38388;&#30340;&#20132;&#20114;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#25552;&#20986;&#20102;&#21508;&#31181;&#20132;&#20114;&#33539;&#24335;&#26469;&#25913;&#21892;&#32431;&#21452;&#32534;&#30721;&#22120;&#30340;&#24615;&#33021;&#12290;&#29305;&#21035;&#26159;&#65292;&#26368;&#36817;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#36890;&#24120;&#22312;&#27169;&#22411;&#25512;&#29702;&#36807;&#31243;&#20013;&#24341;&#20837;&#21518;&#26399;&#20132;&#20114;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#22522;&#20110;&#21518;&#26399;&#20132;&#20114;&#30340;&#26041;&#27861;&#36890;&#24120;&#20250;&#23545;&#22823;&#35821;&#26009;&#24211;&#20135;&#29983;&#22823;&#37327;&#30340;&#35745;&#31639;&#21644;&#23384;&#20648;&#25104;&#26412;&#12290;&#23613;&#31649;&#23427;&#20204;&#26377;&#25928;&#65292;&#20294;&#25928;&#29575;&#21644;&#31354;&#38388;&#21344;&#29992;&#30340;&#38382;&#39064;&#20173;&#28982;&#26159;&#38480;&#21046;&#20132;&#20114;&#24335;&#31070;&#32463;&#26816;&#32034;&#27169;&#22411;&#24212;&#29992;&#30340;&#37325;&#35201;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Passage retrieval is a fundamental task in many information systems, such as web search and question answering, where both efficiency and effectiveness are critical concerns. In recent years, neural retrievers based on pre-trained language models (PLM), such as dual-encoders, have achieved huge success. Yet, studies have found that the performance of dual-encoders are often limited due to the neglecting of the interaction information between queries and candidate passages. Therefore, various interaction paradigms have been proposed to improve the performance of vanilla dual-encoders. Particularly, recent state-of-the-art methods often introduce late-interaction during the model inference process. However, such late-interaction based methods usually bring extensive computation and storage cost on large corpus. Despite their effectiveness, the concern of efficiency and space footprint is still an important factor that limits the application of interaction-based neural retrieval models. T
&lt;/p&gt;</description></item></channel></rss>