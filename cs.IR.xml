<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>ChatQA&#26159;&#19968;&#31995;&#21015;&#23545;&#35805;&#38382;&#31572;&#27169;&#22411;&#65292;&#21487;&#20197;&#36798;&#21040;GPT-4&#32423;&#21035;&#30340;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#20004;&#38454;&#27573;&#30340;&#25351;&#20196;&#35843;&#25972;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;-shot&#23545;&#35805;&#38382;&#31572;&#20013;&#30340;&#32467;&#26524;&#12290;&#20351;&#29992;&#23494;&#38598;&#26816;&#32034;&#22120;&#36827;&#34892;&#38382;&#31572;&#25968;&#25454;&#38598;&#30340;&#24494;&#35843;&#21487;&#20197;&#23454;&#29616;&#19982;&#26368;&#20808;&#36827;&#30340;&#26597;&#35810;&#37325;&#20889;&#27169;&#22411;&#30456;&#24403;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#38477;&#20302;&#37096;&#32626;&#25104;&#26412;&#12290;ChatQA-70B&#22312;10&#20010;&#23545;&#35805;&#38382;&#31572;&#25968;&#25454;&#38598;&#19978;&#30340;&#24179;&#22343;&#24471;&#20998;&#36229;&#36807;&#20102;GPT-4&#65292;&#19988;&#19981;&#20381;&#36182;&#20110;&#20219;&#20309;&#26469;&#33258;OpenAI GPT&#27169;&#22411;&#30340;&#21512;&#25104;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2401.10225</link><description>&lt;p&gt;
ChatQA: &#26500;&#24314;GPT-4&#32423;&#23545;&#35805;&#38382;&#31572;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ChatQA: Building GPT-4 Level Conversational QA Models. (arXiv:2401.10225v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10225
&lt;/p&gt;
&lt;p&gt;
ChatQA&#26159;&#19968;&#31995;&#21015;&#23545;&#35805;&#38382;&#31572;&#27169;&#22411;&#65292;&#21487;&#20197;&#36798;&#21040;GPT-4&#32423;&#21035;&#30340;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#20004;&#38454;&#27573;&#30340;&#25351;&#20196;&#35843;&#25972;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;-shot&#23545;&#35805;&#38382;&#31572;&#20013;&#30340;&#32467;&#26524;&#12290;&#20351;&#29992;&#23494;&#38598;&#26816;&#32034;&#22120;&#36827;&#34892;&#38382;&#31572;&#25968;&#25454;&#38598;&#30340;&#24494;&#35843;&#21487;&#20197;&#23454;&#29616;&#19982;&#26368;&#20808;&#36827;&#30340;&#26597;&#35810;&#37325;&#20889;&#27169;&#22411;&#30456;&#24403;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#38477;&#20302;&#37096;&#32626;&#25104;&#26412;&#12290;ChatQA-70B&#22312;10&#20010;&#23545;&#35805;&#38382;&#31572;&#25968;&#25454;&#38598;&#19978;&#30340;&#24179;&#22343;&#24471;&#20998;&#36229;&#36807;&#20102;GPT-4&#65292;&#19988;&#19981;&#20381;&#36182;&#20110;&#20219;&#20309;&#26469;&#33258;OpenAI GPT&#27169;&#22411;&#30340;&#21512;&#25104;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;ChatQA&#65292;&#19968;&#31995;&#21015;&#20855;&#26377;GPT-4&#32423;&#21035;&#20934;&#30830;&#24615;&#30340;&#23545;&#35805;&#38382;&#31572;&#27169;&#22411;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#25351;&#20196;&#35843;&#25972;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#38646;-shot&#23545;&#35805;&#38382;&#31572;&#20013;&#30340;&#32467;&#26524;&#12290;&#20026;&#20102;&#22788;&#29702;&#23545;&#35805;&#38382;&#31572;&#20013;&#30340;&#26816;&#32034;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;&#22810;&#36718;&#38382;&#31572;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23494;&#38598;&#26816;&#32034;&#22120;&#30340;&#24494;&#35843;&#65292;&#36825;&#26679;&#21487;&#20197;&#25552;&#20379;&#19982;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#26597;&#35810;&#37325;&#20889;&#27169;&#22411;&#30456;&#24403;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#22823;&#22823;&#38477;&#20302;&#37096;&#32626;&#25104;&#26412;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;ChatQA-70B&#21487;&#20197;&#22312;10&#20010;&#23545;&#35805;&#38382;&#31572;&#25968;&#25454;&#38598;&#30340;&#24179;&#22343;&#20998;&#19978;&#36229;&#36807;GPT-4&#65288;54.14 vs. 53.90&#65289;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;OpenAI GPT&#27169;&#22411;&#30340;&#20219;&#20309;&#21512;&#25104;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we introduce ChatQA, a family of conversational question answering (QA) models, that obtain GPT-4 level accuracies. Specifically, we propose a two-stage instruction tuning method that can significantly improve the zero-shot conversational QA results from large language models (LLMs). To handle retrieval in conversational QA, we fine-tune a dense retriever on a multi-turn QA dataset, which provides comparable results to using the state-of-the-art query rewriting model while largely reducing deployment cost. Notably, our ChatQA-70B can outperform GPT-4 in terms of average score on 10 conversational QA datasets (54.14 vs. 53.90), without relying on any synthetic data from OpenAI GPT models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;&#20256;&#32479;&#21644;&#22522;&#20110;LLM&#30340;&#22270;&#20687;&#22320;&#29702;&#20301;&#32622;&#25628;&#32034;&#26041;&#27861;&#65292;&#32467;&#26524;&#21457;&#29616;&#20351;&#29992;&#20256;&#32479;&#25628;&#32034;&#24341;&#25806;&#30340;&#21442;&#19982;&#32773;&#23545;&#22270;&#20687;&#20301;&#32622;&#30340;&#39044;&#27979;&#26356;&#20934;&#30830;&#65292;&#32780;&#20351;&#29992;&#22522;&#20110;LLM&#30340;&#25628;&#32034;&#24341;&#25806;&#30340;&#21442;&#19982;&#32773;&#21017;&#25552;&#20986;&#20102;&#26356;&#38271;&#12289;&#26356;&#33258;&#28982;&#35821;&#35328;&#30340;&#26597;&#35810;&#12290;</title><link>http://arxiv.org/abs/2401.10184</link><description>&lt;p&gt;
&#27604;&#36739;&#20256;&#32479;&#21644;&#22522;&#20110;LLM&#30340;&#22270;&#20687;&#22320;&#29702;&#20301;&#32622;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Comparing Traditional and LLM-based Search for Image Geolocation. (arXiv:2401.10184v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10184
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;&#20256;&#32479;&#21644;&#22522;&#20110;LLM&#30340;&#22270;&#20687;&#22320;&#29702;&#20301;&#32622;&#25628;&#32034;&#26041;&#27861;&#65292;&#32467;&#26524;&#21457;&#29616;&#20351;&#29992;&#20256;&#32479;&#25628;&#32034;&#24341;&#25806;&#30340;&#21442;&#19982;&#32773;&#23545;&#22270;&#20687;&#20301;&#32622;&#30340;&#39044;&#27979;&#26356;&#20934;&#30830;&#65292;&#32780;&#20351;&#29992;&#22522;&#20110;LLM&#30340;&#25628;&#32034;&#24341;&#25806;&#30340;&#21442;&#19982;&#32773;&#21017;&#25552;&#20986;&#20102;&#26356;&#38271;&#12289;&#26356;&#33258;&#28982;&#35821;&#35328;&#30340;&#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Web&#25628;&#32034;&#24341;&#25806;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#26159;&#20449;&#24687;&#26816;&#32034;&#30340;&#24517;&#19981;&#21487;&#23569;&#30340;&#24037;&#20855;&#65307;&#29992;&#25143;&#34892;&#20026;&#21644;&#26597;&#35810;&#24418;&#24335;&#30340;&#31574;&#30053;&#24050;&#32463;&#24471;&#21040;&#20102;&#28145;&#20837;&#30740;&#31350;&#12290;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#39537;&#21160;&#30340;&#25628;&#32034;&#24341;&#25806;&#30340;&#24341;&#20837;&#25552;&#20986;&#20102;&#26356;&#21152;&#23545;&#35805;&#24335;&#30340;&#25628;&#32034;&#21644;&#26032;&#31867;&#22411;&#30340;&#26597;&#35810;&#31574;&#30053;&#12290;&#26412;&#25991;&#27604;&#36739;&#20102;&#20256;&#32479;&#21644;&#22522;&#20110;LLM&#30340;&#25628;&#32034;&#22312;&#22270;&#20687;&#22320;&#29702;&#20301;&#32622;&#25628;&#32034;&#20219;&#21153;&#20013;&#30340;&#25928;&#26524;&#65292;&#21363;&#30830;&#23450;&#22270;&#20687;&#25293;&#25668;&#22320;&#28857;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#29992;&#25143;&#30340;&#20132;&#20114;&#65292;&#23588;&#20854;&#26159;&#26597;&#35810;&#24418;&#24335;&#30340;&#31574;&#30053;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20026;60&#21517;&#21442;&#19982;&#32773;&#20998;&#37197;&#20102;&#20256;&#32479;&#25110;&#22522;&#20110;LLM&#30340;&#25628;&#32034;&#24341;&#25806;&#20316;&#20026;&#22320;&#29702;&#20301;&#32622;&#25628;&#32034;&#30340;&#21161;&#25163;&#12290;&#30456;&#27604;&#20351;&#29992;LLM-based&#25628;&#32034;&#30340;&#21442;&#19982;&#32773;&#65292;&#20351;&#29992;&#20256;&#32479;&#25628;&#32034;&#30340;&#21442;&#19982;&#32773;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#20102;&#22270;&#20687;&#30340;&#20301;&#32622;&#12290;&#26681;&#25454;&#21161;&#25163;&#30340;&#31867;&#22411;&#65292;&#20351;&#29992;&#32773;&#20043;&#38388;&#20986;&#29616;&#20102;&#19981;&#21516;&#30340;&#31574;&#30053;&#12290;&#20351;&#29992;LLM-based&#25628;&#32034;&#30340;&#21442;&#19982;&#32773;&#25552;&#20986;&#20102;&#26356;&#38271;&#12289;&#26356;&#33258;&#28982;&#35821;&#35328;&#30340;&#26597;&#35810;&#65292;&#20294;&#25628;&#32034;&#20250;&#35805;&#26356;&#30701;&#12290;
&lt;/p&gt;
&lt;p&gt;
Web search engines have long served as indispensable tools for information retrieval; user behavior and query formulation strategies have been well studied. The introduction of search engines powered by large language models (LLMs) suggested more conversational search and new types of query strategies. In this paper, we compare traditional and LLM-based search for the task of image geolocation, i.e., determining the location where an image was captured. Our work examines user interactions, with a particular focus on query formulation strategies. In our study, 60 participants were assigned either traditional or LLM-based search engines as assistants for geolocation. Participants using traditional search more accurately predicted the location of the image compared to those using the LLM-based search. Distinct strategies emerged between users depending on the type of assistant. Participants using the LLM-based search issued longer, more natural language queries, but had shorter search ses
&lt;/p&gt;</description></item><item><title>LOCALINTEL&#26159;&#19968;&#20010;&#33258;&#21160;&#21270;&#30340;&#30693;&#35782;&#19978;&#19979;&#25991;&#21270;&#31995;&#32479;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#20174;&#20840;&#29699;&#21644;&#26412;&#22320;&#30693;&#35782;&#25968;&#25454;&#24211;&#20013;&#33258;&#21160;&#29983;&#25104;&#32452;&#32455;&#30340;&#23041;&#32961;&#24773;&#25253;&#12290;</title><link>http://arxiv.org/abs/2401.10036</link><description>&lt;p&gt;
LOCALINTEL&#65306;&#20174;&#20840;&#29699;&#21644;&#26412;&#22320;&#32593;&#32476;&#30693;&#35782;&#29983;&#25104;&#32452;&#32455;&#23041;&#32961;&#24773;&#25253;
&lt;/p&gt;
&lt;p&gt;
LOCALINTEL: Generating Organizational Threat Intelligence from Global and Local Cyber Knowledge. (arXiv:2401.10036v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10036
&lt;/p&gt;
&lt;p&gt;
LOCALINTEL&#26159;&#19968;&#20010;&#33258;&#21160;&#21270;&#30340;&#30693;&#35782;&#19978;&#19979;&#25991;&#21270;&#31995;&#32479;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#20174;&#20840;&#29699;&#21644;&#26412;&#22320;&#30693;&#35782;&#25968;&#25454;&#24211;&#20013;&#33258;&#21160;&#29983;&#25104;&#32452;&#32455;&#30340;&#23041;&#32961;&#24773;&#25253;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#25805;&#20316;&#20013;&#24515;&#65288;SoC&#65289;&#20998;&#26512;&#24072;&#20174;&#20844;&#24320;&#35775;&#38382;&#30340;&#20840;&#29699;&#23041;&#32961;&#25968;&#25454;&#24211;&#20013;&#25910;&#38598;&#23041;&#32961;&#25253;&#21578;&#65292;&#24182;&#25163;&#21160;&#33258;&#23450;&#20041;&#20197;&#36866;&#24212;&#29305;&#23450;&#32452;&#32455;&#30340;&#38656;&#27714;&#12290;&#36825;&#20123;&#20998;&#26512;&#24072;&#36824;&#20381;&#36182;&#20110;&#20869;&#37096;&#23384;&#20648;&#24211;&#65292;&#20316;&#20026;&#32452;&#32455;&#30340;&#31169;&#26377;&#26412;&#22320;&#30693;&#35782;&#25968;&#25454;&#24211;&#12290;&#21487;&#20449;&#30340;&#32593;&#32476;&#24773;&#25253;&#12289;&#20851;&#38190;&#25805;&#20316;&#32454;&#33410;&#21644;&#30456;&#20851;&#32452;&#32455;&#20449;&#24687;&#37117;&#23384;&#20648;&#22312;&#36825;&#20123;&#26412;&#22320;&#30693;&#35782;&#25968;&#25454;&#24211;&#20013;&#12290;&#20998;&#26512;&#24072;&#21033;&#29992;&#36825;&#20123;&#20840;&#29699;&#21644;&#26412;&#22320;&#30693;&#35782;&#25968;&#25454;&#24211;&#20174;&#20107;&#19968;&#39033;&#32321;&#37325;&#30340;&#20219;&#21153;&#65292;&#25163;&#21160;&#21019;&#24314;&#32452;&#32455;&#29420;&#29305;&#30340;&#23041;&#32961;&#21709;&#24212;&#21644;&#32531;&#35299;&#31574;&#30053;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#23637;&#31034;&#20102;&#39640;&#25928;&#22788;&#29702;&#22823;&#35268;&#27169;&#22810;&#26679;&#21270;&#30693;&#35782;&#28304;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#31181;&#33021;&#21147;&#26469;&#22788;&#29702;&#20840;&#29699;&#21644;&#26412;&#22320;&#30693;&#35782;&#25968;&#25454;&#24211;&#65292;&#33258;&#21160;&#21270;&#29983;&#25104;&#32452;&#32455;&#29305;&#23450;&#30340;&#23041;&#32961;&#24773;&#25253;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LOCALINTEL&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#33258;&#21160;&#21270;&#30693;&#35782;&#19978;&#19979;&#25991;&#21270;&#31995;&#32479;&#65292;&#21487;&#20197;&#20174;&#20840;&#29699;&#21644;&#26412;&#22320;&#30693;&#35782;&#25968;&#25454;&#24211;&#20013;&#29983;&#25104;&#32452;&#32455;&#30340;&#23041;&#32961;&#24773;&#25253;&#12290;
&lt;/p&gt;
&lt;p&gt;
Security Operations Center (SoC) analysts gather threat reports from openly accessible global threat databases and customize them manually to suit a particular organization's needs. These analysts also depend on internal repositories, which act as private local knowledge database for an organization. Credible cyber intelligence, critical operational details, and relevant organizational information are all stored in these local knowledge databases. Analysts undertake a labor intensive task utilizing these global and local knowledge databases to manually create organization's unique threat response and mitigation strategies. Recently, Large Language Models (LLMs) have shown the capability to efficiently process large diverse knowledge sources. We leverage this ability to process global and local knowledge databases to automate the generation of organization-specific threat intelligence.  In this work, we present LOCALINTEL, a novel automated knowledge contextualization system that, upon 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;HGAttack&#65292;&#19968;&#31181;&#38024;&#23545;&#24322;&#26500;&#22270;&#30340;&#25915;&#20987;&#26041;&#27861;&#12290;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#25311;&#21512;&#27169;&#22411;&#21644;&#21033;&#29992;&#26799;&#24230;&#29983;&#25104;&#25200;&#21160;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#21033;&#29992;&#24322;&#26500;&#20449;&#24687;&#65292;&#25552;&#39640;&#25915;&#20987;&#30340;&#21487;&#36716;&#31227;&#24615;&#21644;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.09945</link><description>&lt;p&gt;
HGAttack: &#21487;&#36716;&#31227;&#30340;&#24322;&#26500;&#22270;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
HGAttack: Transferable Heterogeneous Graph Adversarial Attack. (arXiv:2401.09945v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09945
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;HGAttack&#65292;&#19968;&#31181;&#38024;&#23545;&#24322;&#26500;&#22270;&#30340;&#25915;&#20987;&#26041;&#27861;&#12290;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#25311;&#21512;&#27169;&#22411;&#21644;&#21033;&#29992;&#26799;&#24230;&#29983;&#25104;&#25200;&#21160;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#21033;&#29992;&#24322;&#26500;&#20449;&#24687;&#65292;&#25552;&#39640;&#25915;&#20987;&#30340;&#21487;&#36716;&#31227;&#24615;&#21644;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;HGNN&#65289;&#22312;&#32593;&#32476;&#21644;&#30005;&#23376;&#21830;&#21153;&#31561;&#39046;&#22495;&#30340;&#24615;&#33021;&#36234;&#26469;&#36234;&#21463;&#21040;&#35748;&#21487;&#65292;&#23545;&#25239;&#25915;&#20987;&#30340;&#38887;&#24615;&#23545;&#20110;&#36825;&#20123;&#39046;&#22495;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#20027;&#35201;&#38024;&#23545;&#21516;&#36136;&#22270;&#35774;&#35745;&#65292;&#24212;&#29992;&#20110;HGNN&#26102;&#65292;&#30001;&#20110;&#20854;&#23545;HGNN&#32467;&#26500;&#21644;&#35821;&#20041;&#22797;&#26434;&#24615;&#30340;&#38480;&#21046;&#65292;&#36825;&#20123;&#26041;&#27861;&#25928;&#26524;&#19981;&#20339;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;HGAttack&#65292;&#36825;&#26159;&#19968;&#31181;&#38024;&#23545;&#24322;&#26500;&#22270;&#30340;&#31532;&#19968;&#31181;&#19987;&#29992;&#28784;&#30418;&#36867;&#36991;&#25915;&#20987;&#26041;&#27861;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#30340;&#25311;&#21512;&#27169;&#22411;&#65292;&#20197;&#19982;&#30446;&#26631;HGNN&#30340;&#34892;&#20026;&#32039;&#23494;&#30456;&#20284;&#65292;&#24182;&#21033;&#29992;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#29983;&#25104;&#25200;&#21160;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25152;&#25552;&#20986;&#30340;&#25311;&#21512;&#27169;&#22411;&#36890;&#36807;&#25552;&#21462;&#20803;&#36335;&#24452;&#35825;&#23548;&#30340;&#23376;&#22270;&#24182;&#24212;&#29992;GNN&#26469;&#23398;&#20064;&#27599;&#20010;&#23376;&#22270;&#20013;&#20855;&#26377;&#19981;&#21516;&#35821;&#20041;&#30340;&#33410;&#28857;&#23884;&#20837;&#65292;&#26377;&#25928;&#22320;&#21033;&#29992;&#20102;&#24322;&#26500;&#20449;&#24687;&#12290;&#36825;&#31181;&#26041;&#27861;&#25552;&#39640;&#20102;&#29983;&#25104;&#30340;&#25915;&#20987;&#23545;&#30446;&#26631;HGNN&#30340;&#21487;&#36716;&#31227;&#24615;&#65292;&#24182;&#26174;&#33879;&#20943;&#23569;&#20102;&#25915;&#20987;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Heterogeneous Graph Neural Networks (HGNNs) are increasingly recognized for their performance in areas like the web and e-commerce, where resilience against adversarial attacks is crucial. However, existing adversarial attack methods, which are primarily designed for homogeneous graphs, fall short when applied to HGNNs due to their limited ability to address the structural and semantic complexity of HGNNs. This paper introduces HGAttack, the first dedicated gray box evasion attack method for heterogeneous graphs. We design a novel surrogate model to closely resemble the behaviors of the target HGNN and utilize gradient-based methods for perturbation generation. Specifically, the proposed surrogate model effectively leverages heterogeneous information by extracting meta-path induced subgraphs and applying GNNs to learn node embeddings with distinct semantics from each subgraph. This approach improves the transferability of generated attacks on the target HGNN and significantly reduces m
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#20351;&#29992;&#26080;&#30417;&#30563;&#30456;&#20284;&#24230;&#24230;&#37327;&#36827;&#34892;&#28304;&#20195;&#30721;&#20811;&#38534;&#26816;&#27979;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#65292;&#26088;&#22312;&#20026;&#36719;&#20214;&#24037;&#31243;&#24072;&#25552;&#20379;&#25351;&#23548;&#65292;&#20197;&#36873;&#25321;&#36866;&#21512;&#20854;&#29305;&#23450;&#29992;&#20363;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.09885</link><description>&lt;p&gt;
&#20351;&#29992;&#26080;&#30417;&#30563;&#30456;&#20284;&#24230;&#24230;&#37327;&#36827;&#34892;&#28304;&#20195;&#30721;&#20811;&#38534;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Source Code Clone Detection Using Unsupervised Similarity Measures. (arXiv:2401.09885v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09885
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#20351;&#29992;&#26080;&#30417;&#30563;&#30456;&#20284;&#24230;&#24230;&#37327;&#36827;&#34892;&#28304;&#20195;&#30721;&#20811;&#38534;&#26816;&#27979;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#65292;&#26088;&#22312;&#20026;&#36719;&#20214;&#24037;&#31243;&#24072;&#25552;&#20379;&#25351;&#23548;&#65292;&#20197;&#36873;&#25321;&#36866;&#21512;&#20854;&#29305;&#23450;&#29992;&#20363;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22312;&#36719;&#20214;&#24037;&#31243;&#20219;&#21153;&#20013;&#20811;&#38534;&#26816;&#27979;&#21644;&#20195;&#30721;&#25628;&#32034;&#19982;&#25512;&#33616;&#30340;&#37325;&#35201;&#24615;&#65292;&#23545;&#28304;&#20195;&#30721;&#30340;&#30456;&#20284;&#24615;&#36827;&#34892;&#35780;&#20272;&#36817;&#24180;&#26469;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#27604;&#36739;&#20998;&#26512;&#26080;&#30417;&#30563;&#30456;&#20284;&#24230;&#24230;&#37327;&#29992;&#20110;&#35782;&#21035;&#28304;&#20195;&#30721;&#20811;&#38534;&#26816;&#27979;&#30340;&#26041;&#27861;&#12290;&#30446;&#26631;&#26159;&#27010;&#36848;&#30446;&#21069;&#30340;&#26368;&#26032;&#25216;&#26415;&#12289;&#23427;&#20204;&#30340;&#20248;&#28857;&#21644;&#32570;&#28857;&#12290;&#20026;&#20102;&#36798;&#21040;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#32534;&#35793;&#20102;&#29616;&#26377;&#30340;&#26080;&#30417;&#30563;&#31574;&#30053;&#65292;&#24182;&#35780;&#20272;&#20854;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#65292;&#20197;&#25351;&#23548;&#36719;&#20214;&#24037;&#31243;&#24072;&#22312;&#36873;&#25321;&#36866;&#29992;&#20110;&#20854;&#29305;&#23450;&#29992;&#20363;&#30340;&#26041;&#27861;&#26102;&#25552;&#20379;&#25351;&#23548;&#12290;&#26412;&#30740;&#31350;&#30340;&#28304;&#20195;&#30721;&#21487;&#22312;\url{https://github.com/jorge-martinez-gil/codesim}&#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Assessing similarity in source code has gained significant attention in recent years due to its importance in software engineering tasks such as clone detection and code search and recommendation. This work presents a comparative analysis of unsupervised similarity measures for identifying source code clone detection. The goal is to overview the current state-of-the-art techniques, their strengths, and weaknesses. To do that, we compile the existing unsupervised strategies and evaluate their performance on a benchmark dataset to guide software engineers in selecting appropriate methods for their specific use cases. The source code of this study is available at \url{https://github.com/jorge-martinez-gil/codesim}
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;MatSciRE&#65292;&#19968;&#31181;&#22522;&#20110;&#25351;&#38024;&#32593;&#32476;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#26448;&#26009;&#31185;&#23398;&#25991;&#31456;&#20013;&#33258;&#21160;&#25552;&#21462;&#23454;&#20307;&#21644;&#20851;&#31995;&#20197;&#26500;&#24314;&#19968;&#20010;&#26448;&#26009;&#31185;&#23398;&#30693;&#35782;&#24211;&#12290;&#36890;&#36807;&#38024;&#23545;&#30005;&#27744;&#26448;&#26009;&#30340;&#20116;&#20010;&#20851;&#31995;&#30340;&#25552;&#21462;&#20219;&#21153;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;F1&#20998;&#25968;&#19978;&#21462;&#24471;&#20102;&#27604;&#20043;&#21069;&#20351;&#29992;ChemDataExtractor&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.09839</link><description>&lt;p&gt;
MatSciRE:&#21033;&#29992;&#25351;&#38024;&#32593;&#32476;&#33258;&#21160;&#21270;&#26448;&#26009;&#31185;&#23398;&#30693;&#35782;&#24211;&#26500;&#24314;&#20013;&#30340;&#23454;&#20307;&#21644;&#20851;&#31995;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
MatSciRE: Leveraging Pointer Networks to Automate Entity and Relation Extraction for Material Science Knowledge-base Construction. (arXiv:2401.09839v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09839
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;MatSciRE&#65292;&#19968;&#31181;&#22522;&#20110;&#25351;&#38024;&#32593;&#32476;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#26448;&#26009;&#31185;&#23398;&#25991;&#31456;&#20013;&#33258;&#21160;&#25552;&#21462;&#23454;&#20307;&#21644;&#20851;&#31995;&#20197;&#26500;&#24314;&#19968;&#20010;&#26448;&#26009;&#31185;&#23398;&#30693;&#35782;&#24211;&#12290;&#36890;&#36807;&#38024;&#23545;&#30005;&#27744;&#26448;&#26009;&#30340;&#20116;&#20010;&#20851;&#31995;&#30340;&#25552;&#21462;&#20219;&#21153;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;F1&#20998;&#25968;&#19978;&#21462;&#24471;&#20102;&#27604;&#20043;&#21069;&#20351;&#29992;ChemDataExtractor&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26448;&#26009;&#31185;&#23398;&#25991;&#29486;&#26159;&#20851;&#20110;&#21508;&#31181;&#23454;&#20307;&#65288;&#22914;&#26448;&#26009;&#21644;&#25104;&#20998;&#65289;&#21644;&#36825;&#20123;&#23454;&#20307;&#20043;&#38388;&#21508;&#31181;&#20851;&#31995;&#65288;&#22914;&#23548;&#30005;&#24615;&#12289;&#30005;&#21387;&#31561;&#65289;&#30340;&#20016;&#23500;&#26469;&#28304;&#12290;&#33258;&#21160;&#25552;&#21462;&#36825;&#20123;&#20449;&#24687;&#20197;&#29983;&#25104;&#19968;&#20010;&#26448;&#26009;&#31185;&#23398;&#30693;&#35782;&#24211;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MatSciRE&#65288;&#26448;&#26009;&#31185;&#23398;&#20851;&#31995;&#25552;&#21462;&#22120;&#65289;&#65292;&#19968;&#31181;&#22522;&#20110;&#25351;&#38024;&#32593;&#32476;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#26448;&#26009;&#31185;&#23398;&#25991;&#31456;&#20013;&#21516;&#26102;&#25552;&#21462;&#23454;&#20307;&#21644;&#20851;&#31995;&#20316;&#20026;&#19977;&#20803;&#32452;&#65288;$&#23454;&#20307;1&#65292;&#20851;&#31995;&#65292;&#23454;&#20307;2$&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#38024;&#23545;&#30005;&#27744;&#26448;&#26009;&#65292;&#24182;&#30830;&#23450;&#20102;&#20116;&#20010;&#35201;&#22788;&#29702;&#30340;&#20851;&#31995; - &#23548;&#30005;&#24615;&#12289;&#24211;&#20262;&#25928;&#29575;&#12289;&#23481;&#37327;&#12289;&#30005;&#21387;&#21644;&#33021;&#37327;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;F1&#20998;&#25968;&#19978;&#21462;&#24471;&#20102;&#27604;&#20351;&#29992;ChemDataExtractor&#65288;0.716&#65289;&#26356;&#22909;&#30340;&#32467;&#26524;&#65288;0.771&#65289;&#12290;MatSciRE&#30340;&#25972;&#20307;&#22270;&#24418;&#26694;&#26550;&#22914;&#22270;1&#25152;&#31034;&#12290;&#26448;&#26009;&#20449;&#24687;&#20197;&#23454;&#20307;&#21644;&#20851;&#31995;&#30340;&#24418;&#24335;&#20174;&#26448;&#26009;&#31185;&#23398;&#25991;&#29486;&#20013;&#25552;&#21462;&#20986;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
Material science literature is a rich source of factual information about various categories of entities (like materials and compositions) and various relations between these entities, such as conductivity, voltage, etc. Automatically extracting this information to generate a material science knowledge base is a challenging task. In this paper, we propose MatSciRE (Material Science Relation Extractor), a Pointer Network-based encoder-decoder framework, to jointly extract entities and relations from material science articles as a triplet ($entity1, relation, entity2$). Specifically, we target the battery materials and identify five relations to work on - conductivity, coulombic efficiency, capacity, voltage, and energy. Our proposed approach achieved a much better F1-score (0.771) than a previous attempt using ChemDataExtractor (0.716). The overall graphical framework of MatSciRE is shown in Fig 1. The material information is extracted from material science literature in the form of ent
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#22270;&#20687;-&#25991;&#26412;&#21305;&#37197;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#29305;&#24449;&#22686;&#24378;&#27169;&#22359;&#21644;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#23454;&#29616;&#20102;&#26356;&#24179;&#34913;&#21644;&#31283;&#20581;&#30340;&#22270;&#20687;-&#25991;&#26412;&#26816;&#32034;&#12290;</title><link>http://arxiv.org/abs/2401.09725</link><description>&lt;p&gt;
&#25913;&#36827;&#22270;&#20687;-&#25991;&#26412;&#21305;&#37197;&#30340;&#33258;&#36866;&#24212;&#29305;&#24449;&#32858;&#21512;
&lt;/p&gt;
&lt;p&gt;
Enhancing Image-Text Matching with Adaptive Feature Aggregation. (arXiv:2401.09725v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09725
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#22270;&#20687;-&#25991;&#26412;&#21305;&#37197;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#29305;&#24449;&#22686;&#24378;&#27169;&#22359;&#21644;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#23454;&#29616;&#20102;&#26356;&#24179;&#34913;&#21644;&#31283;&#20581;&#30340;&#22270;&#20687;-&#25991;&#26412;&#26816;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;-&#25991;&#26412;&#21305;&#37197;&#26088;&#22312;&#20934;&#30830;&#25214;&#21040;&#21305;&#37197;&#30340;&#36328;&#27169;&#24577;&#23545;&#12290;&#24403;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#23558;&#36328;&#27169;&#24577;&#29305;&#24449;&#26144;&#23556;&#21040;&#19968;&#20010;&#20849;&#21516;&#30340;&#23884;&#20837;&#31354;&#38388;&#65292;&#20294;&#24448;&#24448;&#23384;&#22312;&#19981;&#24179;&#34913;&#30340;&#29305;&#24449;&#34920;&#31034;&#65292;&#36896;&#25104;&#26816;&#32034;&#32467;&#26524;&#19981;&#21487;&#38752;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#29305;&#24449;&#22686;&#24378;&#27169;&#22359;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#32858;&#21512;&#21333;&#27169;&#24577;&#29305;&#24449;&#65292;&#23454;&#29616;&#26356;&#24179;&#34913;&#21644;&#31283;&#20581;&#30340;&#22270;&#20687;-&#25991;&#26412;&#26816;&#32034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#20811;&#26381;&#20102;&#21407;&#22987;&#19977;&#20803;&#25490;&#21517;&#25439;&#22833;&#30340;&#19981;&#36275;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#26816;&#32034;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#23545;&#27169;&#22411;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#19982;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#30456;&#27604;&#65292;&#22312;&#26816;&#32034;&#24615;&#33021;&#19978;&#21462;&#24471;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;&#23454;&#29616;&#20195;&#30721;&#21487;&#20197;&#22312;&#36825;&#37324;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image-text matching aims to find matched cross-modal pairs accurately. While current methods often rely on projecting cross-modal features into a common embedding space, they frequently suffer from imbalanced feature representations across different modalities, leading to unreliable retrieval results. To address these limitations, we introduce a novel Feature Enhancement Module that adaptively aggregates single-modal features for more balanced and robust image-text retrieval. Additionally, we propose a new loss function that overcomes the shortcomings of original triplet ranking loss, thereby significantly improving retrieval performance. The proposed model has been evaluated on two public datasets and achieves competitive retrieval performance when compared with several state-of-the-art models. Implementation codes can be found here.
&lt;/p&gt;</description></item><item><title>EfficientRec&#26159;&#19968;&#31181;&#24212;&#29992;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#37319;&#29992;&#20102;&#36719;&#32858;&#31867;&#26550;&#26500;&#65292;&#33021;&#20197;&#20302;&#35745;&#31639;&#25104;&#26412;&#23398;&#20064;&#29992;&#25143;&#20559;&#22909;&#65292;&#24182;&#20855;&#26377;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#23545;&#26080;&#38480;&#29992;&#25143;&#21644;&#20135;&#21697;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.09693</link><description>&lt;p&gt;
&#22522;&#20110;&#32858;&#31867;&#21644;&#29992;&#25143;&#20132;&#20114;&#23884;&#20837;&#26723;&#26696;&#30340;EfficientRec&#26080;&#38480;&#29992;&#25143;-&#29289;&#21697;&#35268;&#27169;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
EfficientRec an unlimited user-item scale recommendation system based on clustering and users interaction embedding profile. (arXiv:2401.09693v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09693
&lt;/p&gt;
&lt;p&gt;
EfficientRec&#26159;&#19968;&#31181;&#24212;&#29992;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#37319;&#29992;&#20102;&#36719;&#32858;&#31867;&#26550;&#26500;&#65292;&#33021;&#20197;&#20302;&#35745;&#31639;&#25104;&#26412;&#23398;&#20064;&#29992;&#25143;&#20559;&#22909;&#65292;&#24182;&#20855;&#26377;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#23545;&#26080;&#38480;&#29992;&#25143;&#21644;&#20135;&#21697;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#26159;&#22914;&#20170;&#31185;&#25216;&#20844;&#21496;&#39640;&#24230;&#20851;&#27880;&#30340;&#25216;&#26415;&#65292;&#30001;&#20110;&#29992;&#25143;&#21644;&#20135;&#21697;&#25968;&#37327;&#19981;&#26029;&#22686;&#38271;&#65292;&#20256;&#32479;&#30340;&#25512;&#33616;&#31639;&#27861;&#22312;&#23545;&#24037;&#19994;&#29615;&#22659;&#30340;&#36866;&#24212;&#24615;&#19978;&#23384;&#22312;&#22256;&#38590;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#24212;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#26469;&#25552;&#21462;&#29992;&#25143;&#20559;&#22909;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#36719;&#32858;&#31867;&#26550;&#26500;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#25512;&#29702;&#36807;&#31243;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#20197;&#36739;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#22312;&#35757;&#32451;&#21644;&#39044;&#27979;&#38454;&#27573;&#23398;&#20064;&#29992;&#25143;&#20559;&#22909;&#65292;&#24182;&#19988;&#20855;&#26377;&#24456;&#22909;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#31216;&#36825;&#31181;&#26550;&#26500;&#20026;EfficientRec&#65292;&#24847;&#21619;&#30528;&#27169;&#22411;&#30340;&#32039;&#20945;&#24615;&#21644;&#23545;&#26080;&#38480;&#29992;&#25143;&#21644;&#20135;&#21697;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommendation systems are highly interested in technology companies nowadays. The businesses are constantly growing users and products, causing the number of users and items to continuously increase over time, to very large numbers. Traditional recommendation algorithms with complexity dependent on the number of users and items make them difficult to adapt to the industrial environment. In this paper, we introduce a new method applying graph neural networks with a contrastive learning framework in extracting user preferences. We incorporate a soft clustering architecture that significantly reduces the computational cost of the inference process. Experiments show that the model is able to learn user preferences with low computational cost in both training and prediction phases. At the same time, the model gives a very good accuracy. We call this architecture EfficientRec with the implication of model compactness and the ability to scale to unlimited users and products.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#21019;&#26032;&#25216;&#26415;&#26469;&#35299;&#20915;&#24314;&#35758;&#31995;&#32479;&#20013;&#39640;&#22522;&#25968;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#37319;&#29992;&#35789;&#34955;&#27169;&#22411;&#21644;&#23618;&#20849;&#20139;&#26469;&#20943;&#23567;&#27169;&#22411;&#22823;&#23567;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;&#36890;&#36807;&#23545;Uber&#20351;&#29992;&#24773;&#20917;&#30340;&#23454;&#39564;&#39564;&#35777;&#65292;&#35777;&#26126;&#20102;&#36825;&#20123;&#25216;&#26415;&#22312;&#20248;&#21270;&#24314;&#35758;&#31995;&#32479;&#21644;&#25552;&#39640;&#24615;&#33021;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.09572</link><description>&lt;p&gt;
&#22788;&#29702;&#24314;&#35758;&#31995;&#32479;&#20013;&#22823;&#35268;&#27169;&#22522;&#25968;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Handling Large-scale Cardinality in building recommendation systems. (arXiv:2401.09572v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09572
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#21019;&#26032;&#25216;&#26415;&#26469;&#35299;&#20915;&#24314;&#35758;&#31995;&#32479;&#20013;&#39640;&#22522;&#25968;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#37319;&#29992;&#35789;&#34955;&#27169;&#22411;&#21644;&#23618;&#20849;&#20139;&#26469;&#20943;&#23567;&#27169;&#22411;&#22823;&#23567;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;&#36890;&#36807;&#23545;Uber&#20351;&#29992;&#24773;&#20917;&#30340;&#23454;&#39564;&#39564;&#35777;&#65292;&#35777;&#26126;&#20102;&#36825;&#20123;&#25216;&#26415;&#22312;&#20248;&#21270;&#24314;&#35758;&#31995;&#32479;&#21644;&#25552;&#39640;&#24615;&#33021;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#30340;&#24314;&#35758;&#31995;&#32479;&#20381;&#36182;&#20110;&#25429;&#25417;&#29992;&#25143;&#20559;&#22909;&#65292;&#36890;&#24120;&#38656;&#35201;&#21253;&#21547;&#26080;&#25968;&#23454;&#20307;&#30340;&#21807;&#19968;&#26631;&#35782;&#31526;&#65288;UUID&#65289;&#31561;&#22810;&#31181;&#21151;&#33021;&#12290;&#28982;&#32780;&#65292;UUID&#30340;&#24322;&#24120;&#39640;&#22522;&#25968;&#22312;&#27169;&#22411;&#36864;&#21270;&#21644;&#31232;&#30095;&#24615;&#23548;&#33268;&#27169;&#22411;&#22823;&#23567;&#22686;&#21152;&#26041;&#38754;&#26500;&#25104;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#21019;&#26032;&#25216;&#26415;&#26469;&#35299;&#20915;&#24314;&#35758;&#31995;&#32479;&#20013;&#39640;&#22522;&#25968;&#30340;&#25361;&#25112;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35789;&#34955;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#23618;&#20849;&#20139;&#65292;&#20197;&#26174;&#33879;&#20943;&#23567;&#27169;&#22411;&#22823;&#23567;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;Uber&#20351;&#29992;&#24773;&#20917;&#36827;&#34892;&#31163;&#32447;&#21644;&#22312;&#32447;&#23454;&#39564;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#25216;&#26415;&#65292;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20248;&#21270;&#24314;&#35758;&#31995;&#32479;&#21644;&#25552;&#39640;&#20854;&#25972;&#20307;&#24615;&#33021;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Effective recommendation systems rely on capturing user preferences, often requiring incorporating numerous features such as universally unique identifiers (UUIDs) of entities. However, the exceptionally high cardinality of UUIDs poses a significant challenge in terms of model degradation and increased model size due to sparsity. This paper presents two innovative techniques to address the challenge of high cardinality in recommendation systems. Specifically, we propose a bag-of-words approach, combined with layer sharing, to substantially decrease the model size while improving performance. Our techniques were evaluated through offline and online experiments on Uber use cases, resulting in promising results demonstrating our approach's effectiveness in optimizing recommendation systems and enhancing their overall performance.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#33258;&#21160;&#22320;&#20174;&#21307;&#23398;&#25991;&#29486;&#20013;&#31579;&#36873;&#36951;&#20256;&#21464;&#24322;&#19982;&#30142;&#30149;&#30456;&#20851;&#30340;&#35777;&#25454;&#65292;&#20174;&#32780;&#22686;&#21152;&#30142;&#30149;&#35782;&#21035;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.09490</link><description>&lt;p&gt;
&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#22522;&#22240;&#30456;&#20851;&#30142;&#30149;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Gene-associated Disease Discovery Powered by Large Language Models. (arXiv:2401.09490v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09490
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#33258;&#21160;&#22320;&#20174;&#21307;&#23398;&#25991;&#29486;&#20013;&#31579;&#36873;&#36951;&#20256;&#21464;&#24322;&#19982;&#30142;&#30149;&#30456;&#20851;&#30340;&#35777;&#25454;&#65292;&#20174;&#32780;&#22686;&#21152;&#30142;&#30149;&#35782;&#21035;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36951;&#20256;&#21464;&#24322;&#19982;&#20154;&#31867;&#30142;&#30149;&#20043;&#38388;&#38169;&#32508;&#22797;&#26434;&#30340;&#20851;&#31995;&#19968;&#30452;&#26159;&#21307;&#23398;&#30740;&#31350;&#30340;&#28966;&#28857;&#65292;&#36825;&#22312;&#29305;&#23450;&#30142;&#30149;&#30340;&#39118;&#38505;&#22522;&#22240;&#30830;&#23450;&#26041;&#38754;&#24471;&#21040;&#20102;&#35777;&#23454;&#12290;&#20808;&#36827;&#30340;&#22522;&#22240;&#32452;&#27979;&#24207;&#25216;&#26415;&#30340;&#20986;&#29616;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#26816;&#27979;&#36825;&#20123;&#36951;&#20256;&#26631;&#35760;&#30340;&#25928;&#29575;&#21644;&#32463;&#27982;&#24615;&#65292;&#23545;&#30142;&#30149;&#35786;&#26029;&#36215;&#21040;&#20102;&#20851;&#38190;&#20316;&#29992;&#65292;&#24182;&#25104;&#20026;&#20020;&#24202;&#20915;&#31574;&#21644;&#26089;&#26399;&#39118;&#38505;&#35780;&#20272;&#30340;&#22522;&#30784;&#12290;&#20026;&#20102;&#20811;&#26381;&#29616;&#26377;&#25968;&#25454;&#24211;&#30340;&#23616;&#38480;&#24615;&#65292;&#36825;&#20123;&#25968;&#25454;&#24211;&#20174;&#29616;&#26377;&#25991;&#29486;&#20013;&#35760;&#24405;&#30142;&#30149;&#22522;&#22240;&#20851;&#32852;&#65292;&#36890;&#24120;&#32570;&#20047;&#23454;&#26102;&#26356;&#26032;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#21457;&#29616;&#19982;&#29305;&#23450;&#22522;&#22240;&#30456;&#20851;&#30340;&#30142;&#30149;&#12290;&#35813;&#26694;&#26550;&#26088;&#22312;&#33258;&#21160;&#21270;&#32321;&#37325;&#30340;&#36807;&#31243;&#65292;&#36890;&#36807;&#31579;&#36873;&#21307;&#23398;&#25991;&#29486;&#35777;&#25454;&#65292;&#23558;&#36951;&#20256;&#21464;&#24322;&#19982;&#30142;&#30149;&#32852;&#31995;&#36215;&#26469;&#65292;&#20174;&#32780;&#25552;&#39640;&#30142;&#30149;&#35782;&#21035;&#30340;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#20351;&#29992;LLMs&#36827;&#34892;...
&lt;/p&gt;
&lt;p&gt;
The intricate relationship between genetic variation and human diseases has been a focal point of medical research, evidenced by the identification of risk genes regarding specific diseases. The advent of advanced genome sequencing techniques has significantly improved the efficiency and cost-effectiveness of detecting these genetic markers, playing a crucial role in disease diagnosis and forming the basis for clinical decision-making and early risk assessment. To overcome the limitations of existing databases that record disease-gene associations from existing literature, which often lack real-time updates, we propose a novel framework employing Large Language Models (LLMs) for the discovery of diseases associated with specific genes. This framework aims to automate the labor-intensive process of sifting through medical literature for evidence linking genetic variations to diseases, thereby enhancing the efficiency of disease identification. Our approach involves using LLMs to conduct
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#27604;&#36739;&#20998;&#26512;&#20102;&#20351;&#29992;&#19981;&#21516;&#31354;&#38388;&#28388;&#27874;&#25216;&#26415;&#36827;&#34892;&#22270;&#20687;&#21435;&#22122;&#30340;&#25928;&#26524;&#65292;&#24182;&#30830;&#23450;&#20102;&#26576;&#20123;&#28388;&#27874;&#22120;&#22312;&#29305;&#23450;&#22122;&#22768;&#27169;&#22411;&#19978;&#30340;&#26356;&#39640;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.09460</link><description>&lt;p&gt;
&#22270;&#20687;&#24674;&#22797;&#65306;&#20351;&#29992;&#19981;&#21516;&#31354;&#38388;&#28388;&#27874;&#25216;&#26415;&#30340;&#22270;&#20687;&#21435;&#22122;&#30340;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Image Restoration: A Comparative Analysis of Image De noising Using Different Spatial Filtering Techniques. (arXiv:2401.09460v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09460
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#27604;&#36739;&#20998;&#26512;&#20102;&#20351;&#29992;&#19981;&#21516;&#31354;&#38388;&#28388;&#27874;&#25216;&#26415;&#36827;&#34892;&#22270;&#20687;&#21435;&#22122;&#30340;&#25928;&#26524;&#65292;&#24182;&#30830;&#23450;&#20102;&#26576;&#20123;&#28388;&#27874;&#22120;&#22312;&#29305;&#23450;&#22122;&#22768;&#27169;&#22411;&#19978;&#30340;&#26356;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#24674;&#22797;&#21463;&#21040;&#22122;&#22768;&#24433;&#21709;&#30340;&#21307;&#23398;&#21644;&#20854;&#20182;&#29992;&#36884;&#30340;&#22270;&#20687;&#65292;&#26412;&#25991;&#36890;&#36807;&#39318;&#20808;&#21521;&#22270;&#20687;&#24341;&#20837;&#22122;&#22768;&#65292;&#28982;&#21518;&#24212;&#29992;&#19981;&#21516;&#30340;&#31354;&#22495;&#28388;&#27874;&#25216;&#26415;&#26469;&#38500;&#21435;&#22122;&#22768;&#65292;&#25506;&#35752;&#20102;&#19981;&#21516;&#30340;&#21435;&#22122;&#28388;&#27874;&#22120;&#12290;&#37319;&#29992;&#23792;&#20540;&#20449;&#22122;&#27604;&#65288;PSNR&#65289;&#21644;&#22343;&#26041;&#26681;&#35823;&#24046;&#65288;RMSE&#65289;&#31561;&#35780;&#20272;&#25216;&#26415;&#26469;&#30830;&#23450;&#27599;&#20010;&#28388;&#27874;&#22120;&#22312;&#32473;&#23450;&#22270;&#20687;&#22122;&#22768;&#19978;&#30340;&#25928;&#26524;&#22914;&#20309;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#26576;&#20123;&#28388;&#27874;&#22120;&#22312;&#26576;&#20123;&#22122;&#22768;&#27169;&#22411;&#19978;&#27604;&#20854;&#20182;&#28388;&#27874;&#22120;&#26356;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Acquired images for medical and other purposes can be affected by noise from both the equipment used in the capturing or the environment. This can have adverse effect on the information therein. Thus, the need to restore the image to its original state by removing the noise. To effectively remove such noise, pre knowledge of the type of noise model present is necessary. This work explores different noise removal filters by first introducing noise to an image and then applying different spatial domain filtering techniques to the image to get rid of the noise. Different evaluation techniques such as Peak to Signal Noise Ratio(PSNR) and Root Mean Square Error(RMSE) were adopted to determine how effective each filter is on a given image noise. Result showed that some filters are more effective on some noise models than others.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;&#30340;FactCHD&#20107;&#23454;&#20914;&#31361;&#24187;&#35273;&#26816;&#27979;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;LLMs&#29983;&#25104;&#25991;&#26412;&#30340;&#20107;&#23454;&#24615;&#12290;&#22522;&#20934;&#21253;&#21547;&#20102;&#22810;&#31181;&#20107;&#23454;&#27169;&#24335;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;&#20107;&#23454;&#30340;&#35777;&#25454;&#38142;&#36827;&#34892;&#32452;&#21512;&#24615;&#24187;&#35273;&#30340;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2310.12086</link><description>&lt;p&gt;
&#21457;&#29616;&#22622;&#22764;&#20043;&#27468;&#65306;&#21487;&#38752;&#30340;&#20107;&#23454;&#20914;&#31361;&#24187;&#35273;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Unveiling the Siren's Song: Towards Reliable Fact-Conflicting Hallucination Detection. (arXiv:2310.12086v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12086
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;&#30340;FactCHD&#20107;&#23454;&#20914;&#31361;&#24187;&#35273;&#26816;&#27979;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;LLMs&#29983;&#25104;&#25991;&#26412;&#30340;&#20107;&#23454;&#24615;&#12290;&#22522;&#20934;&#21253;&#21547;&#20102;&#22810;&#31181;&#20107;&#23454;&#27169;&#24335;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;&#20107;&#23454;&#30340;&#35777;&#25454;&#38142;&#36827;&#34892;&#32452;&#21512;&#24615;&#24187;&#35273;&#30340;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;ChatGPT/GPT-4&#65292;&#22240;&#20854;&#24191;&#27867;&#30340;&#23454;&#38469;&#24212;&#29992;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#65292;&#20294;&#20854;&#22312;&#32593;&#32476;&#24179;&#21488;&#19978;&#23384;&#22312;&#20107;&#23454;&#20914;&#31361;&#24187;&#35273;&#30340;&#38382;&#39064;&#38480;&#21046;&#20102;&#20854;&#37319;&#29992;&#12290;&#23545;&#30001;LLMs&#20135;&#29983;&#30340;&#25991;&#26412;&#30340;&#20107;&#23454;&#24615;&#35780;&#20272;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#65292;&#19981;&#20165;&#28041;&#21450;&#23545;&#22522;&#26412;&#20107;&#23454;&#30340;&#21028;&#26029;&#65292;&#36824;&#21253;&#25324;&#23545;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#65288;&#22914;&#22810;&#36339;&#31561;&#65289;&#20013;&#20986;&#29616;&#30340;&#20107;&#23454;&#38169;&#35823;&#30340;&#35780;&#20272;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;FactCHD&#65292;&#19968;&#31181;&#20026;LLMs&#31934;&#24515;&#35774;&#35745;&#30340;&#20107;&#23454;&#20914;&#31361;&#24187;&#35273;&#26816;&#27979;&#22522;&#20934;&#12290;&#20316;&#20026;&#22312;&#8220;&#26597;&#35810;-&#21709;&#24212;&#8221;&#19978;&#19979;&#25991;&#20013;&#35780;&#20272;&#20107;&#23454;&#24615;&#30340;&#20851;&#38190;&#24037;&#20855;&#65292;&#25105;&#20204;&#30340;&#22522;&#20934;&#37319;&#29992;&#20102;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;&#24191;&#27867;&#30340;&#20107;&#23454;&#27169;&#24335;&#65292;&#22914;&#22522;&#26412;&#20107;&#23454;&#65292;&#22810;&#36339;&#65292;&#27604;&#36739;&#21644;&#38598;&#21512;&#25805;&#20316;&#27169;&#24335;&#12290;&#25105;&#20204;&#22522;&#20934;&#30340;&#19968;&#20010;&#29420;&#29305;&#29305;&#28857;&#26159;&#20854;&#21253;&#21547;&#22522;&#20110;&#20107;&#23454;&#30340;&#35777;&#25454;&#38142;&#65292;&#20174;&#32780;&#20415;&#20110;&#36827;&#34892;&#32452;&#21512;&#24615;&#24187;&#35273;&#30340;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), such as ChatGPT/GPT-4, have garnered widespread attention owing to their myriad of practical applications, yet their adoption has been constrained by issues of fact-conflicting hallucinations across web platforms. The assessment of factuality in text, produced by LLMs, remains inadequately explored, extending not only to the judgment of vanilla facts but also encompassing the evaluation of factual errors emerging in complex inferential tasks like multi-hop, and etc. In response, we introduce FactCHD, a fact-conflicting hallucination detection benchmark meticulously designed for LLMs. Functioning as a pivotal tool in evaluating factuality within "Query-Respons" contexts, our benchmark assimilates a large-scale dataset, encapsulating a broad spectrum of factuality patterns, such as vanilla, multi-hops, comparison, and set-operation patterns. A distinctive feature of our benchmark is its incorporation of fact-based chains of evidence, thereby facilitating com
&lt;/p&gt;</description></item><item><title>&#25919;&#27835;&#36777;&#35770;&#12289;&#28436;&#35762;&#21644;&#35775;&#35848;&#20013;&#30340;&#20540;&#24471;&#26680;&#23454;&#30340;&#35770;&#26029;&#21487;&#20197;&#20351;&#29992;&#38899;&#39057;&#25968;&#25454;&#36827;&#34892;&#26816;&#27979;&#21644;&#30830;&#35748;&#65292;&#36825;&#21487;&#24110;&#21161;&#20027;&#25345;&#20154;&#12289;&#35760;&#32773;&#21644;&#20107;&#23454;&#26680;&#26597;&#32452;&#32455;&#36827;&#34892;&#24037;&#20316;&#12290;</title><link>http://arxiv.org/abs/2306.05535</link><description>&lt;p&gt;
&#20351;&#29992;&#38899;&#39057;&#25968;&#25454;&#26816;&#27979;&#25919;&#27835;&#36777;&#35770;&#12289;&#28436;&#35762;&#21644;&#35775;&#35848;&#20013;&#20540;&#24471;&#26680;&#23454;&#30340;&#35770;&#26029;
&lt;/p&gt;
&lt;p&gt;
Detecting Check-Worthy Claims in Political Debates, Speeches, and Interviews Using Audio Data. (arXiv:2306.05535v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05535
&lt;/p&gt;
&lt;p&gt;
&#25919;&#27835;&#36777;&#35770;&#12289;&#28436;&#35762;&#21644;&#35775;&#35848;&#20013;&#30340;&#20540;&#24471;&#26680;&#23454;&#30340;&#35770;&#26029;&#21487;&#20197;&#20351;&#29992;&#38899;&#39057;&#25968;&#25454;&#36827;&#34892;&#26816;&#27979;&#21644;&#30830;&#35748;&#65292;&#36825;&#21487;&#24110;&#21161;&#20027;&#25345;&#20154;&#12289;&#35760;&#32773;&#21644;&#20107;&#23454;&#26680;&#26597;&#32452;&#32455;&#36827;&#34892;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20250;&#30340;&#19968;&#22823;&#37096;&#20998;&#22242;&#32467;&#22312;&#30456;&#21516;&#30340;&#24895;&#26223;&#21644;&#24605;&#24819;&#21608;&#22260;&#65292;&#20855;&#26377;&#24040;&#22823;&#30340;&#33021;&#37327;&#12290;&#36825;&#27491;&#26159;&#25919;&#27835;&#20154;&#29289;&#24076;&#26395;&#20026;&#20182;&#20204;&#30340;&#20107;&#19994;&#25152;&#32047;&#31215;&#30340;&#12290;&#20026;&#20102;&#36798;&#21040;&#36825;&#20010;&#30446;&#26631;&#65292;&#20182;&#20204;&#26377;&#26102;&#20250;&#20351;&#29992;&#25197;&#26354;&#25110;&#38544;&#34255;&#30495;&#30456;&#30340;&#25163;&#27573;&#65292;&#26080;&#35770;&#26159;&#26080;&#24847;&#30340;&#36824;&#26159;&#26377;&#24847;&#30340;&#65292;&#36825;&#20026;&#38169;&#35823;&#20449;&#24687;&#21644;&#35823;&#23548;&#24320;&#20102;&#22823;&#38376;&#12290;&#33258;&#21160;&#26816;&#27979;&#20540;&#24471;&#26680;&#23454;&#30340;&#35770;&#26029;&#30340;&#24037;&#20855;&#23558;&#23545;&#36777;&#35770;&#20027;&#25345;&#20154;&#12289;&#35760;&#32773;&#21644;&#20107;&#23454;&#26680;&#26597;&#32452;&#32455;&#26377;&#24456;&#22823;&#24110;&#21161;&#12290;&#34429;&#28982;&#20197;&#21069;&#20851;&#20110;&#26816;&#27979;&#20540;&#24471;&#26680;&#23454;&#30340;&#35770;&#26029;&#30340;&#24037;&#20316;&#37325;&#28857;&#26159;&#25991;&#26412;&#65292;&#20294;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#38899;&#39057;&#20449;&#21495;&#20316;&#20026;&#39069;&#22806;&#20449;&#24687;&#28304;&#30340;&#23454;&#29992;&#24615;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#65288;&#33521;&#35821;&#25991;&#26412;&#21644;&#38899;&#39057;&#65289;&#65292;&#21253;&#21547;48&#23567;&#26102;&#30340;&#28436;&#35762;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#22810;&#20010;&#28436;&#35762;&#32773;&#30340;&#24773;&#20917;&#19979;&#65292;&#38899;&#39057;&#27169;&#24577;&#19982;&#25991;&#26412;&#32467;&#21512;&#20351;&#29992;&#27604;&#20165;&#20351;&#29992;&#25991;&#26412;&#20855;&#26377;&#25913;&#36827;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;&#21333;&#22768;&#36947;&#38899;&#39057;&#27169;&#22411;&#21487;&#20197;&#32988;&#36807;&#21333;&#22768;&#36947;&#25991;&#26412;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
A large portion of society united around the same vision and ideas carries enormous energy. That is precisely what political figures would like to accumulate for their cause. With this goal in mind, they can sometimes resort to distorting or hiding the truth, unintentionally or on purpose, which opens the door for misinformation and disinformation. Tools for automatic detection of check-worthy claims would be of great help to moderators of debates, journalists, and fact-checking organizations. While previous work on detecting check-worthy claims has focused on text, here we explore the utility of the audio signal as an additional information source. We create a new multimodal dataset (text and audio in English) containing 48 hours of speech. Our evaluation results show that the audio modality together with text yields improvements over text alone in the case of multiple speakers. Moreover, an audio-only model could outperform a text-only one for a single speaker.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#22788;&#29702;&#29983;&#25104;&#24335;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#20869;&#30340;&#35821;&#20041;&#32467;&#26500;&#65292;&#25552;&#39640;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.09048</link><description>&lt;p&gt;
CodeKGC&#65306;&#29992;&#20110;&#29983;&#25104;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#30340;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CodeKGC: Code Language Model for Generative Knowledge Graph Construction. (arXiv:2304.09048v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09048
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#22788;&#29702;&#29983;&#25104;&#24335;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#20869;&#30340;&#35821;&#20041;&#32467;&#26500;&#65292;&#25552;&#39640;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#30340;&#29983;&#25104;&#24335;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#26041;&#27861;&#36890;&#24120;&#26080;&#27861;&#25429;&#25417;&#32467;&#26500;&#24615;&#30693;&#35782;&#65292;&#32780;&#21482;&#26159;&#23558;&#33258;&#28982;&#35821;&#35328;&#36716;&#21270;&#20026;&#24207;&#21015;&#21270;&#25991;&#26412;&#25110;&#35268;&#33539;&#35821;&#35328;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20687;&#20195;&#30721;&#36825;&#26679;&#30340;&#32467;&#26500;&#21270;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#30340;&#22823;&#22411;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#23637;&#29616;&#20102;&#22312;&#29702;&#35299;&#33258;&#28982;&#35821;&#35328;&#20197;&#36827;&#34892;&#32467;&#26500;&#24615;&#39044;&#27979;&#21644;&#25512;&#29702;&#20219;&#21153;&#26041;&#38754;&#30340;&#21331;&#36234;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#22788;&#29702;&#29983;&#25104;&#24335;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#32473;&#23450;&#20195;&#30721;&#26684;&#24335;&#30340;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#30340;&#24773;&#20917;&#19979;&#65292;&#30446;&#26631;&#26159;&#29983;&#25104;&#21487;&#20197;&#34920;&#31034;&#20026;&#20195;&#30721;&#34917;&#20840;&#20219;&#21153;&#30340;&#19977;&#20803;&#32452;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#20855;&#26377;&#27169;&#24335;&#24863;&#30693;&#22411;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#20869;&#30340;&#35821;&#20041;&#32467;&#26500;&#12290;&#30001;&#20110;&#20195;&#30721;&#26412;&#36136;&#19978;&#20855;&#26377;&#32467;&#26500;&#65292;&#22914;&#31867;&#21644;&#20989;&#25968;&#23450;&#20041;&#65292;&#22240;&#27492;&#23427;&#20316;&#20026;&#20808;&#39564;&#30340;&#35821;&#20041;&#32467;&#26500;&#30693;&#35782;&#27169;&#22411;&#38750;&#24120;&#26377;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#22522;&#20110;&#21407;&#29702;&#30340;&#29983;&#25104;&#26041;&#27861;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;&#21407;&#29702;&#25552;&#20379;&#20102;&#27169;&#22411;&#29983;&#25104;&#32467;&#26524;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current generative knowledge graph construction approaches usually fail to capture structural knowledge by simply flattening natural language into serialized texts or a specification language. However, large generative language model trained on structured data such as code has demonstrated impressive capability in understanding natural language for structural prediction and reasoning tasks. Intuitively, we address the task of generative knowledge graph construction with code language model: given a code-format natural language input, the target is to generate triples which can be represented as code completion tasks. Specifically, we develop schema-aware prompts that effectively utilize the semantic structure within the knowledge graph. As code inherently possesses structure, such as class and function definitions, it serves as a useful model for prior semantic structural knowledge. Furthermore, we employ a rationale-enhanced generation method to boost the performance. Rationales provi
&lt;/p&gt;</description></item><item><title>&#36825;&#20221;&#32508;&#36848;&#20840;&#38754;&#35843;&#30740;&#20102;&#22522;&#20110;&#22823;&#25968;&#25454;&#30340;&#29616;&#20195;&#25512;&#33616;&#31995;&#32479;&#30340;&#21457;&#23637;&#21644;&#25361;&#25112;&#65292;&#24635;&#32467;&#20102;&#22235;&#31181;&#20027;&#35201;&#31867;&#22411;&#30340;&#25512;&#33616;&#25216;&#26415;&#65292;&#24182;&#25351;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#22312;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2206.02631</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#25968;&#25454;&#30340;&#29616;&#20195;&#25512;&#33616;&#31995;&#32479;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Modern Recommendation System based on Big Data. (arXiv:2206.02631v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.02631
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20221;&#32508;&#36848;&#20840;&#38754;&#35843;&#30740;&#20102;&#22522;&#20110;&#22823;&#25968;&#25454;&#30340;&#29616;&#20195;&#25512;&#33616;&#31995;&#32479;&#30340;&#21457;&#23637;&#21644;&#25361;&#25112;&#65292;&#24635;&#32467;&#20102;&#22235;&#31181;&#20027;&#35201;&#31867;&#22411;&#30340;&#25512;&#33616;&#25216;&#26415;&#65292;&#24182;&#25351;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#22312;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#20840;&#38754;&#25506;&#32034;&#20102;&#25512;&#33616;&#31995;&#32479;&#30340;&#21457;&#23637;&#21644;&#24403;&#21069;&#29366;&#24577;&#65292;&#36825;&#20123;&#31995;&#32479;&#24050;&#24191;&#27867;&#25972;&#21512;&#21040;&#21508;&#31181;&#32593;&#32476;&#24212;&#29992;&#20013;&#12290;&#23427;&#37325;&#28857;&#20851;&#27880;&#20010;&#24615;&#21270;&#25512;&#33616;&#31574;&#30053;&#22312;&#22312;&#32447;&#20135;&#21697;&#25110;&#26381;&#21153;&#20013;&#30340;&#36827;&#23637;&#12290;&#25105;&#20204;&#23558;&#25512;&#33616;&#25216;&#26415;&#20998;&#20026;&#22235;&#31181;&#20027;&#35201;&#31867;&#22411;&#65306;&#22522;&#20110;&#20869;&#23481;&#30340;&#12289;&#21327;&#21516;&#36807;&#28388;&#30340;&#12289;&#22522;&#20110;&#30693;&#35782;&#30340;&#21644;&#28151;&#21512;&#30340;&#65292;&#27599;&#31181;&#31867;&#22411;&#37117;&#35299;&#20915;&#20102;&#29420;&#29305;&#30340;&#24773;&#26223;&#12290;&#26412;&#32508;&#36848;&#35814;&#32454;&#23457;&#35270;&#20102;&#25512;&#33616;&#31995;&#32479;&#30340;&#21382;&#21490;&#32972;&#26223;&#21644;&#26368;&#26032;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#37027;&#20123;&#20351;&#29992;&#22823;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#26412;&#32508;&#36848;&#36824;&#30830;&#23450;&#24182;&#35752;&#35770;&#20102;&#29616;&#20195;&#25512;&#33616;&#31995;&#32479;&#38754;&#20020;&#30340;&#20851;&#38190;&#25361;&#25112;&#65292;&#22914;&#25968;&#25454;&#31232;&#30095;&#24615;&#12289;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#20197;&#21450;&#23545;&#25512;&#33616;&#30340;&#22810;&#26679;&#24615;&#38656;&#27714;&#12290;&#32508;&#36848;&#26368;&#21518;&#24378;&#35843;&#20102;&#36825;&#20123;&#25361;&#25112;&#20316;&#20026;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#22312;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
This survey provides an exhaustive exploration of the evolution and current state of recommendation systems, which have seen widespread integration in various web applications. It focuses on the advancement of personalized recommendation strategies for online products or services. We categorize recommendation techniques into four primary types: content-based, collaborative filtering-based, knowledge-based, and hybrid-based, each addressing unique scenarios. The survey offers a detailed examination of the historical context and the latest innovative approaches in recommendation systems, particularly those employing big data. Additionally, it identifies and discusses key challenges faced by modern recommendation systems, such as data sparsity, scalability issues, and the need for diversity in recommendations. The survey concludes by highlighting these challenges as potential areas for fruitful future research in the field.
&lt;/p&gt;</description></item></channel></rss>