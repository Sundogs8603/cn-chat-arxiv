<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;CLLM4Rec&#65292;&#39318;&#20010;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#25512;&#33616;&#31995;&#32479;&#30340; ID &#27169;&#24335;&#32039;&#23494;&#38598;&#25104;&#30340;&#21327;&#21516;&#25512;&#33616;&#31639;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#35821;&#20041;&#24046;&#36317;&#12289;&#34394;&#20551;&#30456;&#20851;&#21644;&#20302;&#25928;&#25512;&#33616;&#31561;&#38382;&#39064;&#12290;&#36890;&#36807;&#25193;&#23637;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#35789;&#27719;&#34920;&#65292;&#24182;&#24341;&#20837;&#36719;&#30828;&#25552;&#31034;&#31574;&#30053;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#20934;&#30830;&#22320;&#27169;&#25311;&#29992;&#25143;&#21644;&#39033;&#30446;&#30340;&#21327;&#21516;&#19982;&#20869;&#23481;&#35821;&#20041;&#12290;</title><link>http://arxiv.org/abs/2311.01343</link><description>&lt;p&gt;
&#21327;&#21516;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Collaborative Large Language Model for Recommender Systems. (arXiv:2311.01343v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01343
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;CLLM4Rec&#65292;&#39318;&#20010;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#25512;&#33616;&#31995;&#32479;&#30340; ID &#27169;&#24335;&#32039;&#23494;&#38598;&#25104;&#30340;&#21327;&#21516;&#25512;&#33616;&#31639;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#35821;&#20041;&#24046;&#36317;&#12289;&#34394;&#20551;&#30456;&#20851;&#21644;&#20302;&#25928;&#25512;&#33616;&#31561;&#38382;&#39064;&#12290;&#36890;&#36807;&#25193;&#23637;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#35789;&#27719;&#34920;&#65292;&#24182;&#24341;&#20837;&#36719;&#30828;&#25552;&#31034;&#31574;&#30053;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#20934;&#30830;&#22320;&#27169;&#25311;&#29992;&#25143;&#21644;&#39033;&#30446;&#30340;&#21327;&#21516;&#19982;&#20869;&#23481;&#35821;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#23545;&#22522;&#20110;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24320;&#21457;&#19979;&#19968;&#20195;&#25512;&#33616;&#31995;&#32479;&#65288;RS&#65289;&#20135;&#29983;&#20102;&#20852;&#36259;&#65292;&#20805;&#20998;&#21033;&#29992;&#20854;&#32534;&#30721;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#33258;&#28982;&#35821;&#35328;&#19982;&#25512;&#33616;&#20219;&#21153;&#20043;&#38388;&#30340;&#35821;&#20041;&#24046;&#36317;&#20173;&#26410;&#24471;&#21040;&#24456;&#22909;&#30340;&#35299;&#20915;&#65292;&#23548;&#33268;&#19968;&#20123;&#38382;&#39064;&#65292;&#22914;&#34394;&#20551;&#30456;&#20851;&#30340;&#29992;&#25143;/&#39033;&#30446;&#25551;&#36848;&#31526;&#12289;&#23545;&#29992;&#25143;/&#39033;&#30446;&#20869;&#23481;&#30340;&#20302;&#25928;&#35821;&#35328;&#24314;&#27169;&#20197;&#21450;&#36890;&#36807;&#33258;&#21160;&#22238;&#24402;&#36827;&#34892;&#20302;&#25928;&#30340;&#25512;&#33616;&#31561;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CLLM4Rec&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#32039;&#23494;&#38598;&#25104;LLM&#33539;&#24335;&#21644;RS&#30340;ID&#33539;&#24335;&#30340;&#29983;&#25104;RS&#65292;&#26088;&#22312;&#21516;&#26102;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#12290;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#29992;&#25143;/&#39033;&#30446;ID&#26631;&#35760;&#25193;&#23637;&#20102;&#39044;&#35757;&#32451;LLM&#30340;&#35789;&#27719;&#34920;&#65292;&#20197;&#24544;&#23454;&#22320;&#27169;&#25311;&#29992;&#25143;/&#39033;&#30446;&#30340;&#21327;&#21516;&#21644;&#20869;&#23481;&#35821;&#20041;&#12290;&#22240;&#27492;&#65292;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36719;&#30828;&#25552;&#31034;&#31574;&#30053;&#65292;&#36890;&#36807;&#35821;&#35328;&#24314;&#27169;&#26377;&#25928;&#22320;&#23398;&#20064;&#29992;&#25143;/&#39033;&#30446;&#30340;&#21327;&#21516;/&#20869;&#23481;&#26631;&#35760;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, there is a growing interest in developing next-generation recommender systems (RSs) based on pretrained large language models (LLMs), fully utilizing their encoded knowledge and reasoning ability. However, the semantic gap between natural language and recommendation tasks is still not well addressed, leading to multiple issues such as spuriously-correlated user/item descriptors, ineffective language modeling on user/item contents, and inefficient recommendations via auto-regression, etc. In this paper, we propose CLLM4Rec, the first generative RS that tightly integrates the LLM paradigm and ID paradigm of RS, aiming to address the above challenges simultaneously. We first extend the vocabulary of pretrained LLMs with user/item ID tokens to faithfully model the user/item collaborative and content semantics. Accordingly, in the pretraining stage, a novel soft+hard prompting strategy is proposed to effectively learn user/item collaborative/content token embeddings via language m
&lt;/p&gt;</description></item><item><title>LLMRec&#26159;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22270;&#22686;&#24378;&#31574;&#30053;&#26469;&#25913;&#36827;&#25512;&#33616;&#31995;&#32479;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#35299;&#20915;&#20102;&#25968;&#25454;&#31232;&#32570;&#24615;&#21644;&#38468;&#21152;&#20449;&#24687;&#24341;&#20837;&#21103;&#20316;&#29992;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#21152;&#24378;&#20132;&#20114;&#36793;&#12289;&#22686;&#24378;&#29289;&#21697;&#33410;&#28857;&#23646;&#24615;&#29702;&#35299;&#21644;&#36827;&#34892;&#29992;&#25143;&#33410;&#28857;&#24314;&#27169;&#26469;&#25552;&#39640;&#25512;&#33616;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.00423</link><description>&lt;p&gt;
LLMRec: &#20351;&#29992;&#22270;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
LLMRec: Large Language Models with Graph Augmentation for Recommendation. (arXiv:2311.00423v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00423
&lt;/p&gt;
&lt;p&gt;
LLMRec&#26159;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22270;&#22686;&#24378;&#31574;&#30053;&#26469;&#25913;&#36827;&#25512;&#33616;&#31995;&#32479;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#35299;&#20915;&#20102;&#25968;&#25454;&#31232;&#32570;&#24615;&#21644;&#38468;&#21152;&#20449;&#24687;&#24341;&#20837;&#21103;&#20316;&#29992;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#21152;&#24378;&#20132;&#20114;&#36793;&#12289;&#22686;&#24378;&#29289;&#21697;&#33410;&#28857;&#23646;&#24615;&#29702;&#35299;&#21644;&#36827;&#34892;&#29992;&#25143;&#33410;&#28857;&#24314;&#27169;&#26469;&#25552;&#39640;&#25512;&#33616;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#31232;&#30095;&#24615;&#19968;&#30452;&#26159;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#19968;&#20010;&#25361;&#25112;&#65292;&#20043;&#21069;&#30340;&#30740;&#31350;&#23581;&#35797;&#36890;&#36807;&#24341;&#20837;&#38468;&#21152;&#20449;&#24687;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#24448;&#24448;&#20250;&#24102;&#26469;&#22122;&#22768;&#12289;&#21487;&#29992;&#24615;&#38382;&#39064;&#21644;&#25968;&#25454;&#36136;&#37327;&#20302;&#19979;&#31561;&#21103;&#20316;&#29992;&#65292;&#20174;&#32780;&#24433;&#21709;&#23545;&#29992;&#25143;&#20559;&#22909;&#30340;&#20934;&#30830;&#24314;&#27169;&#65292;&#36827;&#32780;&#23545;&#25512;&#33616;&#24615;&#33021;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#12290;&#37492;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#30693;&#35782;&#24211;&#21644;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;LLMRec&#30340;&#26032;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#37319;&#29992;&#19977;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22522;&#20110;LLM&#30340;&#22270;&#22686;&#24378;&#31574;&#30053;&#26469;&#22686;&#24378;&#25512;&#33616;&#31995;&#32479;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#22312;&#32447;&#24179;&#21488;&#65288;&#22914;Netflix&#65292;MovieLens&#65289;&#20013;&#20016;&#23500;&#30340;&#20869;&#23481;&#65292;&#22312;&#19977;&#20010;&#26041;&#38754;&#22686;&#24378;&#20132;&#20114;&#22270;&#65306;&#65288;i&#65289;&#21152;&#24378;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#36793;&#65292;&#65288;ii&#65289;&#22686;&#24378;&#23545;&#29289;&#21697;&#33410;&#28857;&#23646;&#24615;&#30340;&#29702;&#35299;&#65292;&#65288;iii&#65289;&#36827;&#34892;&#29992;&#25143;&#33410;&#28857;&#24314;&#27169;&#65292;&#30452;&#35266;&#22320;&#34920;&#31034;&#29992;&#25143;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
The problem of data sparsity has long been a challenge in recommendation systems, and previous studies have attempted to address this issue by incorporating side information. However, this approach often introduces side effects such as noise, availability issues, and low data quality, which in turn hinder the accurate modeling of user preferences and adversely impact recommendation performance. In light of the recent advancements in large language models (LLMs), which possess extensive knowledge bases and strong reasoning capabilities, we propose a novel framework called LLMRec that enhances recommender systems by employing three simple yet effective LLM-based graph augmentation strategies. Our approach leverages the rich content available within online platforms (e.g., Netflix, MovieLens) to augment the interaction graph in three ways: (i) reinforcing user-item interaction egde, (ii) enhancing the understanding of item node attributes, and (iii) conducting user node profiling, intuiti
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32479;&#19968;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#39034;&#24207;&#25512;&#33616;&#26041;&#27861;&#65288;UPSR&#65289;&#65292;&#26088;&#22312;&#26500;&#24314;&#19968;&#20010;&#32479;&#19968;&#30340;&#39044;&#35757;&#32451;&#25512;&#33616;&#27169;&#22411;&#29992;&#20110;&#22810;&#39046;&#22495;&#25512;&#33616;&#20219;&#21153;&#12290;&#30740;&#31350;&#32773;&#35774;&#35745;&#20102;&#20116;&#20010;&#20851;&#38190;&#25351;&#26631;&#26469;&#25351;&#23548;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#25991;&#26412;-&gt;&#29289;&#21697;&#36866;&#24212;&#21644;&#34892;&#20026;&#24207;&#21015;-&gt;&#25991;&#26412;&#24207;&#21015;&#36866;&#24212;&#12290;</title><link>http://arxiv.org/abs/2310.13540</link><description>&lt;p&gt;
&#20840;&#38754;&#23558;&#22810;&#39046;&#22495;&#39044;&#35757;&#32451;&#25512;&#33616;&#24314;&#27169;&#20026;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
Thoroughly Modeling Multi-domain Pre-trained Recommendation as Language. (arXiv:2310.13540v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13540
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32479;&#19968;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#39034;&#24207;&#25512;&#33616;&#26041;&#27861;&#65288;UPSR&#65289;&#65292;&#26088;&#22312;&#26500;&#24314;&#19968;&#20010;&#32479;&#19968;&#30340;&#39044;&#35757;&#32451;&#25512;&#33616;&#27169;&#22411;&#29992;&#20110;&#22810;&#39046;&#22495;&#25512;&#33616;&#20219;&#21153;&#12290;&#30740;&#31350;&#32773;&#35774;&#35745;&#20102;&#20116;&#20010;&#20851;&#38190;&#25351;&#26631;&#26469;&#25351;&#23548;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#25991;&#26412;-&gt;&#29289;&#21697;&#36866;&#24212;&#21644;&#34892;&#20026;&#24207;&#21015;-&gt;&#25991;&#26412;&#24207;&#21015;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#20808;&#39537;&#24615;&#24037;&#20316;&#35797;&#22270;&#25506;&#32034;&#23558;PLM&#20013;&#30340;&#36890;&#29992;&#25991;&#26412;&#20449;&#24687;&#19982;&#29992;&#25143;&#21382;&#21490;&#34892;&#20026;&#24207;&#21015;&#20013;&#30340;&#20010;&#24615;&#21270;&#34892;&#20026;&#20449;&#24687;&#30456;&#32467;&#21512;&#65292;&#20197;&#22686;&#24378;&#39034;&#24207;&#25512;&#33616;&#65288;SR&#65289;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#36755;&#20837;&#26684;&#24335;&#21644;&#20219;&#21153;&#30446;&#26631;&#23384;&#22312;&#20849;&#24615;&#65292;&#34892;&#20026;&#21644;&#25991;&#26412;&#20449;&#24687;&#20043;&#38388;&#23384;&#22312;&#24040;&#22823;&#24046;&#36317;&#65292;&#36825;&#38459;&#30861;&#20102;&#23558;SR&#20316;&#20026;&#35821;&#35328;&#24314;&#27169;&#23436;&#20840;&#24314;&#27169;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32479;&#19968;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#39034;&#24207;&#25512;&#33616;&#65288;UPSR&#65289;&#26041;&#27861;&#65292;&#26088;&#22312;&#26500;&#24314;&#19968;&#20010;&#32479;&#19968;&#30340;&#39044;&#35757;&#32451;&#25512;&#33616;&#27169;&#22411;&#29992;&#20110;&#22810;&#39046;&#22495;&#25512;&#33616;&#20219;&#21153;&#12290;&#25105;&#20204;&#27491;&#24335;&#35774;&#35745;&#20102;&#33258;&#28982;&#24615;&#12289;&#39046;&#22495;&#19968;&#33268;&#24615;&#12289;&#20449;&#24687;&#24615;&#12289;&#22122;&#22768;&#21644;&#27169;&#31946;&#24615;&#20197;&#21450;&#25991;&#26412;&#38271;&#24230;&#31561;&#20116;&#20010;&#20851;&#38190;&#25351;&#26631;&#65292;&#20998;&#21035;&#29992;&#20110;&#25351;&#23548;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#25991;&#26412;-&gt;&#29289;&#21697;&#36866;&#24212;&#21644;&#34892;&#20026;&#24207;&#21015;-&gt;&#25991;&#26412;&#24207;&#21015;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the thriving of pre-trained language model (PLM) widely verified in various of NLP tasks, pioneer efforts attempt to explore the possible cooperation of the general textual information in PLM with the personalized behavioral information in user historical behavior sequences to enhance sequential recommendation (SR). However, despite the commonalities of input format and task goal, there are huge gaps between the behavioral and textual information, which obstruct thoroughly modeling SR as language modeling via PLM. To bridge the gap, we propose a novel Unified pre-trained language model enhanced sequential recommendation (UPSR), aiming to build a unified pre-trained recommendation model for multi-domain recommendation tasks. We formally design five key indicators, namely naturalness, domain consistency, informativeness, noise &amp; ambiguity, and text length, to guide the text-&gt;item adaptation and behavior sequence-&gt;text sequence adaptation differently for pre-training and fine-tuning 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;REIGN&#65292;&#36890;&#36807;&#29983;&#25104;&#35757;&#32451;&#38382;&#39064;&#30340;&#25913;&#20889;&#65292;&#24182;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26469;&#25351;&#23548;&#23545;&#35805;&#38382;&#31572;&#27169;&#22411;&#65292;&#22686;&#21152;&#27169;&#22411;&#23545;&#34920;&#38754;&#24418;&#24335;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#22312;&#19981;&#21516;&#30340;&#22522;&#20934;&#19978;&#36827;&#34892;&#38646;-shot&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2310.13505</link><description>&lt;p&gt;
&#20855;&#26377;&#24378;&#21270;&#25913;&#20889;&#29983;&#25104;&#30340;&#23545;&#35805;&#38382;&#31572;&#27169;&#22411;&#30340;&#40065;&#26834;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Robust Training for Conversational Question Answering Models with Reinforced Reformulation Generation. (arXiv:2310.13505v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13505
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;REIGN&#65292;&#36890;&#36807;&#29983;&#25104;&#35757;&#32451;&#38382;&#39064;&#30340;&#25913;&#20889;&#65292;&#24182;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26469;&#25351;&#23548;&#23545;&#35805;&#38382;&#31572;&#27169;&#22411;&#65292;&#22686;&#21152;&#27169;&#22411;&#23545;&#34920;&#38754;&#24418;&#24335;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#22312;&#19981;&#21516;&#30340;&#22522;&#20934;&#19978;&#36827;&#34892;&#38646;-shot&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#19978;&#30340;&#23545;&#35805;&#38382;&#31572;&#65288;ConvQA&#65289;&#27169;&#22411;&#36890;&#24120;&#22312;&#40644;&#37329;QA&#23545;&#30340;&#22522;&#20934;&#19978;&#36827;&#34892;&#35757;&#32451;&#21644;&#27979;&#35797;&#12290;&#36825;&#24847;&#21619;&#30528;&#35757;&#32451;&#20165;&#38480;&#20110;&#22312;&#30456;&#24212;&#25968;&#25454;&#38598;&#20013;&#35265;&#21040;&#30340;&#34920;&#38754;&#24418;&#24335;&#65292;&#35780;&#20272;&#20165;&#38024;&#23545;&#19968;&#23567;&#37096;&#20998;&#38382;&#39064;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#25552;&#20986;&#30340;&#26694;&#26550;REIGN&#65292;&#25105;&#20204;&#37319;&#21462;&#20102;&#20960;&#20010;&#27493;&#39588;&#26469;&#35299;&#20915;&#36825;&#20010;&#21463;&#38480;&#30340;&#23398;&#20064;&#35774;&#32622;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#29983;&#25104;&#35757;&#32451;&#38382;&#39064;&#30340;&#25913;&#20889;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#23545;&#34920;&#38754;&#24418;&#24335;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#12290;&#36825;&#26159;&#19968;&#20010;&#29305;&#21035;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#36825;&#20123;&#38382;&#39064;&#30340;&#19981;&#23436;&#25972;&#24615;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#23558;ConvQA&#27169;&#22411;&#24341;&#23548;&#21040;&#26356;&#39640;&#30340;&#24615;&#33021;&#65292;&#21482;&#25552;&#20379;&#37027;&#20123;&#26377;&#21161;&#20110;&#25552;&#39640;&#22238;&#31572;&#36136;&#37327;&#30340;&#25913;&#20889;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#19968;&#20010;&#22522;&#20934;&#19978;&#35757;&#32451;&#20027;&#35201;&#27169;&#22411;&#32452;&#20214;&#24182;&#23558;&#20854;&#38646;-shot&#24212;&#29992;&#20110;&#21478;&#19968;&#20010;&#30340;&#21487;&#34892;&#24615;&#12290;&#26368;&#21518;&#65292;&#20026;&#20102;&#23545;&#35757;&#32451;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#36827;&#34892;&#20005;&#26684;&#35780;&#20272;&#65292;&#25105;&#20204;&#20351;&#29992;&#21644;&#37325;&#26032;&#37197;&#32622;&#21021;&#22987;&#30340;&#25913;&#20889;&#12289;&#27979;&#35797;&#35821;&#26009;&#12290;
&lt;/p&gt;
&lt;p&gt;
Models for conversational question answering (ConvQA) over knowledge graphs (KGs) are usually trained and tested on benchmarks of gold QA pairs. This implies that training is limited to surface forms seen in the respective datasets, and evaluation is on a small set of held-out questions. Through our proposed framework REIGN, we take several steps to remedy this restricted learning setup. First, we systematically generate reformulations of training questions to increase robustness of models to surface form variations. This is a particularly challenging problem, given the incomplete nature of such questions. Second, we guide ConvQA models towards higher performance by feeding it only those reformulations that help improve their answering quality, using deep reinforcement learning. Third, we demonstrate the viability of training major model components on one benchmark and applying them zero-shot to another. Finally, for a rigorous evaluation of robustness for trained models, we use and re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#29983;&#25104;&#24335;&#26816;&#32034;&#27169;&#22411;&#65292;&#23558;&#26816;&#32034;&#21644;&#29983;&#25104;&#32452;&#21512;&#22312;&#19968;&#36215;&#20197;&#20135;&#29983;&#25512;&#33616;&#12290;</title><link>http://arxiv.org/abs/2305.05065</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;&#26816;&#32034;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Recommender Systems with Generative Retrieval. (arXiv:2305.05065v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05065
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#29983;&#25104;&#24335;&#26816;&#32034;&#27169;&#22411;&#65292;&#23558;&#26816;&#32034;&#21644;&#29983;&#25104;&#32452;&#21512;&#22312;&#19968;&#36215;&#20197;&#20135;&#29983;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#25512;&#33616;&#31995;&#32479;&#20351;&#29992;&#22823;&#35268;&#27169;&#26816;&#32034;&#27169;&#22411;&#36827;&#34892;&#25512;&#33616;&#65292;&#21253;&#25324;&#20004;&#20010;&#38454;&#27573;&#65306;&#35757;&#32451;&#21452;&#32534;&#30721;&#27169;&#22411;&#23558;&#26597;&#35810;&#21644;&#20505;&#36873;&#39033;&#23884;&#20837;&#21040;&#30456;&#21516;&#30340;&#31354;&#38388;&#20013;&#65292;&#28982;&#21518;&#20351;&#29992;&#36817;&#20284;&#26368;&#36817;&#37051;&#25628;&#32034;&#26469;&#36873;&#25321;&#32473;&#23450;&#26597;&#35810;&#23884;&#20837;&#30340;&#39030;&#37096;&#20505;&#36873;&#39033;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21333;&#38454;&#27573;&#33539;&#20363;&#65306;&#29983;&#25104;&#24335;&#26816;&#32034;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#33258;&#22238;&#24402;&#26041;&#24335;&#22312;&#19968;&#20010;&#38454;&#27573;&#20013;&#35299;&#30721;&#30446;&#26631;&#20505;&#36873;&#39033;&#30340;&#26631;&#35782;&#31526;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#19981;&#26159;&#20026;&#27599;&#20010;&#39033;&#30446;&#20998;&#37197;&#38543;&#26426;&#29983;&#25104;&#30340;&#21407;&#23376;ID&#65292;&#32780;&#26159;&#29983;&#25104;&#35821;&#20041;ID&#65306;&#27599;&#20010;&#39033;&#30446;&#30340;&#35821;&#20041;&#26377;&#24847;&#20041;&#30340;&#20803;&#32452;&#32534;&#30721;&#35789;&#65292;&#23427;&#20316;&#20026;&#20854;&#21807;&#19968;&#26631;&#35782;&#31526;&#12290;&#25105;&#20204;&#20351;&#29992;&#31216;&#20026;RQ-VAE&#30340;&#20998;&#23618;&#26041;&#27861;&#29983;&#25104;&#36825;&#20123;&#32534;&#30721;&#35789;&#12290;&#19968;&#26086;&#25105;&#20204;&#23545;&#25152;&#26377;&#39033;&#30446;&#37117;&#26377;&#20102;&#35821;&#20041;ID&#65292;&#23601;&#20250;&#35757;&#32451;&#22522;&#20110;Transformer&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#26469;&#39044;&#27979;&#19979;&#19968;&#20010;&#39033;&#30446;&#30340;&#35821;&#20041;ID&#12290;&#30001;&#20110;&#36825;&#20010;&#27169;&#22411;&#20197;&#33258;&#22238;&#24402;&#30340;&#26041;&#24335;&#30452;&#25509;&#39044;&#27979;&#26631;&#35782;&#19979;&#19968;&#20010;&#39033;&#30340;&#32534;&#30721;&#35789;&#20803;&#32452;&#65292;&#22240;&#27492;&#23427;&#21487;&#20197;&#23558;&#26816;&#32034;&#21644;&#29983;&#25104;&#32452;&#21512;&#22312;&#19968;&#36215;&#20197;&#20135;&#29983;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern recommender systems leverage large-scale retrieval models consisting of two stages: training a dual-encoder model to embed queries and candidates in the same space, followed by an Approximate Nearest Neighbor (ANN) search to select top candidates given a query's embedding. In this paper, we propose a new single-stage paradigm: a generative retrieval model which autoregressively decodes the identifiers for the target candidates in one phase. To do this, instead of assigning randomly generated atomic IDs to each item, we generate Semantic IDs: a semantically meaningful tuple of codewords for each item that serves as its unique identifier. We use a hierarchical method called RQ-VAE to generate these codewords. Once we have the Semantic IDs for all the items, a Transformer based sequence-to-sequence model is trained to predict the Semantic ID of the next item. Since this model predicts the tuple of codewords identifying the next item directly in an autoregressive manner, it can be c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25991;&#26412;&#25552;&#31034;&#23398;&#20064;&#21644;&#21452;&#21521;&#35757;&#32451;&#30340;&#32452;&#25104;&#22270;&#20687;&#26816;&#32034;&#26041;&#27861;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#29616;&#26377;&#30340;&#20307;&#31995;&#32467;&#26500;&#65292;&#24182;&#19988;&#22312;&#20462;&#25913;&#25991;&#26412;&#23384;&#22312;&#22122;&#22768;&#25110;&#27495;&#20041;&#30340;&#24773;&#20917;&#19979;&#29305;&#21035;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2303.16604</link><description>&lt;p&gt;
&#22522;&#20110;&#25991;&#26412;&#25552;&#31034;&#23398;&#20064;&#21644;&#21452;&#21521;&#35757;&#32451;&#30340;&#32452;&#25104;&#22270;&#20687;&#26816;&#32034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Bi-directional Training for Composed Image Retrieval via Text Prompt Learning. (arXiv:2303.16604v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16604
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25991;&#26412;&#25552;&#31034;&#23398;&#20064;&#21644;&#21452;&#21521;&#35757;&#32451;&#30340;&#32452;&#25104;&#22270;&#20687;&#26816;&#32034;&#26041;&#27861;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#29616;&#26377;&#30340;&#20307;&#31995;&#32467;&#26500;&#65292;&#24182;&#19988;&#22312;&#20462;&#25913;&#25991;&#26412;&#23384;&#22312;&#22122;&#22768;&#25110;&#27495;&#20041;&#30340;&#24773;&#20917;&#19979;&#29305;&#21035;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32452;&#25104;&#22270;&#20687;&#26816;&#32034;&#26159;&#26681;&#25454;&#21253;&#21547;&#21442;&#32771;&#22270;&#20687;&#21644;&#25551;&#36848;&#25152;&#38656;&#26356;&#25913;&#30340;&#20462;&#25913;&#25991;&#26412;&#30340;&#22810;&#27169;&#24577;&#29992;&#25143;&#26597;&#35810;&#26469;&#25628;&#32034;&#30446;&#26631;&#22270;&#20687;&#30340;&#26041;&#27861;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;&#36825;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#30340;&#26041;&#27861;&#23398;&#20064;&#20174;&#65288;&#21442;&#32771;&#22270;&#20687;&#65292;&#20462;&#25913;&#25991;&#26412;&#65289;&#23545;&#21040;&#22270;&#20687;&#23884;&#20837;&#30340;&#26144;&#23556;&#65292;&#28982;&#21518;&#23558;&#20854;&#19982;&#22823;&#22411;&#22270;&#20687;&#35821;&#26009;&#24211;&#36827;&#34892;&#21305;&#37197;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#21521;&#35757;&#32451;&#26041;&#26696;&#65292;&#21033;&#29992;&#20102;&#36825;&#31181;&#21453;&#21521;&#26597;&#35810;&#65292;&#24182;&#21487;&#24212;&#29992;&#20110;&#29616;&#26377;&#30340;&#32452;&#25104;&#22270;&#20687;&#26816;&#32034;&#20307;&#31995;&#32467;&#26500;&#12290;&#20026;&#20102;&#32534;&#30721;&#21452;&#21521;&#26597;&#35810;&#65292;&#25105;&#20204;&#22312;&#20462;&#25913;&#25991;&#26412;&#21069;&#38754;&#28155;&#21152;&#19968;&#20010;&#21487;&#23398;&#20064;&#30340;&#20196;&#29260;&#65292;&#25351;&#23450;&#26597;&#35810;&#30340;&#26041;&#21521;&#65292;&#28982;&#21518;&#24494;&#35843;&#25991;&#26412;&#23884;&#20837;&#27169;&#22359;&#30340;&#21442;&#25968;&#12290;&#25105;&#20204;&#27809;&#26377;&#23545;&#32593;&#32476;&#26550;&#26500;&#36827;&#34892;&#20854;&#20182;&#26356;&#25913;&#12290;&#22312;&#20004;&#20010;&#26631;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#21452;&#21521;&#35757;&#32451;&#22312;&#25552;&#39640;&#32452;&#25104;&#22270;&#20687;&#26816;&#32034;&#24615;&#33021;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#65292;&#29305;&#21035;&#26159;&#22312;&#20462;&#25913;&#25991;&#26412;&#23384;&#22312;&#22122;&#22768;&#25110;&#27495;&#20041;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Composed image retrieval searches for a target image based on a multi-modal user query comprised of a reference image and modification text describing the desired changes. Existing approaches to solving this challenging task learn a mapping from the (reference image, modification text)-pair to an image embedding that is then matched against a large image corpus. One area that has not yet been explored is the reverse direction, which asks the question, what reference image when modified as describe by the text would produce the given target image? In this work we propose a bi-directional training scheme that leverages such reversed queries and can be applied to existing composed image retrieval architectures. To encode the bi-directional query we prepend a learnable token to the modification text that designates the direction of the query and then finetune the parameters of the text embedding module. We make no other changes to the network architecture. Experiments on two standard datas
&lt;/p&gt;</description></item></channel></rss>