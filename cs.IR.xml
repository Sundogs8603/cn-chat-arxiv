<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>SPLADE-v3&#30456;&#23545;&#20110;BM25&#21644;SPLADE++&#22312;&#25928;&#26524;&#19978;&#26356;&#20026;&#26174;&#33879;&#65292;&#21516;&#26102;&#19982;&#20132;&#21449;&#32534;&#30721;&#22120;&#37325;&#26032;&#25490;&#24207;&#22120;&#30456;&#27604;&#36739;&#20986;&#33394;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.06789</link><description>&lt;p&gt;
SPLADE-v3&#65306;SPLADE&#30340;&#26032;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
SPLADE-v3: New baselines for SPLADE
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06789
&lt;/p&gt;
&lt;p&gt;
SPLADE-v3&#30456;&#23545;&#20110;BM25&#21644;SPLADE++&#22312;&#25928;&#26524;&#19978;&#26356;&#20026;&#26174;&#33879;&#65292;&#21516;&#26102;&#19982;&#20132;&#21449;&#32534;&#30721;&#22120;&#37325;&#26032;&#25490;&#24207;&#22120;&#30456;&#27604;&#36739;&#20986;&#33394;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#26159;SPLADE&#24211;&#26368;&#26032;&#29256;&#26412;&#21457;&#24067;&#30340;&#20276;&#38543;&#29289;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#23545;&#35757;&#32451;&#32467;&#26500;&#30340;&#26356;&#25913;&#65292;&#24182;&#23637;&#31034;&#20102;&#26368;&#26032;&#19968;&#31995;&#21015;&#30340;&#27169;&#22411;--SPLADE-v3&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#26032;&#29256;&#26412;&#19982;BM25&#12289;SPLADE++&#20197;&#21450;&#37325;&#26032;&#25490;&#24207;&#22120;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#36890;&#36807;&#23545;40&#22810;&#20010;&#26597;&#35810;&#38598;&#30340;&#20803;&#20998;&#26512;&#23637;&#31034;&#20854;&#26377;&#25928;&#24615;&#12290;SPLADE-v3&#36827;&#19968;&#27493;&#25512;&#21160;&#20102;SPLADE&#27169;&#22411;&#30340;&#26497;&#38480;&#65306;&#23427;&#22312;&#32479;&#35745;&#19978;&#26174;&#33879;&#27604;BM25&#21644;SPLADE++&#26356;&#20026;&#26377;&#25928;&#65292;&#21516;&#26102;&#19982;&#20132;&#21449;&#32534;&#30721;&#22120;&#37325;&#26032;&#25490;&#24207;&#22120;&#30456;&#27604;&#36739;&#20986;&#33394;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23427;&#22312;MS MARCO dev&#38598;&#19978;&#33719;&#24471;&#20102;40&#20010;&#20197;&#19978;&#30340;MRR@10&#65292;&#24182;&#22312;BEIR&#22522;&#20934;&#27979;&#35797;&#20013;&#23558;&#22495;&#22806;&#32467;&#26524;&#25552;&#39640;&#20102;2%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06789v1 Announce Type: cross  Abstract: A companion to the release of the latest version of the SPLADE library. We describe changes to the training structure and present our latest series of models -- SPLADE-v3. We compare this new version to BM25, SPLADE++, as well as re-rankers, and showcase its effectiveness via a meta-analysis over more than 40 query sets. SPLADE-v3 further pushes the limit of SPLADE models: it is statistically significantly more effective than both BM25 and SPLADE++, while comparing well to cross-encoder re-rankers. Specifically, it gets more than 40 MRR@10 on the MS MARCO dev set, and improves by 2% the out-of-domain results on the BEIR benchmark.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Meta-Split&#32593;&#32476;&#65288;MSN&#65289;&#26469;&#35299;&#20915;&#28040;&#36153;&#32773;&#20043;&#38388;&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#20013;&#38480;&#37327;&#24211;&#23384;&#20135;&#21697;&#25512;&#33616;&#20013;&#30340;&#29420;&#29305;&#25361;&#25112;&#65292;&#36890;&#36807;&#20998;&#21106;&#29992;&#25143;&#21382;&#21490;&#24207;&#21015;&#26469;&#26377;&#25928;&#21033;&#29992;&#29992;&#25143;&#21382;&#21490;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2403.06747</link><description>&lt;p&gt;
MetaSplit: &#29992;&#20110;&#38480;&#37327;&#20135;&#21697;&#25512;&#33616;&#30340;Meta-Split&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
MetaSplit: Meta-Split Network for Limited-Stock Product Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06747
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Meta-Split&#32593;&#32476;&#65288;MSN&#65289;&#26469;&#35299;&#20915;&#28040;&#36153;&#32773;&#20043;&#38388;&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#20013;&#38480;&#37327;&#24211;&#23384;&#20135;&#21697;&#25512;&#33616;&#20013;&#30340;&#29420;&#29305;&#25361;&#25112;&#65292;&#36890;&#36807;&#20998;&#21106;&#29992;&#25143;&#21382;&#21490;&#24207;&#21015;&#26469;&#26377;&#25928;&#21033;&#29992;&#29992;&#25143;&#21382;&#21490;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#23545;&#20110;&#38754;&#21521;&#28040;&#36153;&#32773;&#30340;&#30005;&#23376;&#21830;&#21153;&#31995;&#32479;&#65292;&#28040;&#36153;&#32773;&#20043;&#38388;&#30340;&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#36890;&#24120;&#20250;&#36935;&#21040;&#38480;&#37327;&#24211;&#23384;&#38382;&#39064;&#65292;&#21363;&#20135;&#21697;&#22312;C2C&#31995;&#32479;&#20013;&#21482;&#33021;&#38144;&#21806;&#19968;&#27425;&#12290;&#36825;&#20026;&#28857;&#20987;&#29575;&#65288;CTR&#65289;&#39044;&#27979;&#24102;&#26469;&#20102;&#20960;&#20010;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#37492;&#20110;&#27599;&#20010;&#20135;&#21697;&#65288;&#21363;&#21830;&#21697;&#65289;&#30340;&#26377;&#38480;&#29992;&#25143;&#20132;&#20114;&#65292;CTR&#27169;&#22411;&#20013;&#23545;&#24212;&#30340;&#21830;&#21697;&#23884;&#20837;&#21487;&#33021;&#19981;&#23481;&#26131;&#25910;&#25947;&#12290;&#36825;&#20351;&#24471;&#20256;&#32479;&#22522;&#20110;&#24207;&#21015;&#24314;&#27169;&#30340;&#26041;&#27861;&#26080;&#27861;&#26377;&#25928;&#21033;&#29992;&#29992;&#25143;&#21382;&#21490;&#20449;&#24687;&#65292;&#22240;&#20026;&#21382;&#21490;&#29992;&#25143;&#34892;&#20026;&#21253;&#21547;&#20102;&#19981;&#21516;&#24211;&#23384;&#37327;&#30340;&#21830;&#21697;&#28151;&#21512;&#12290;&#29305;&#21035;&#26159;&#65292;&#24207;&#21015;&#27169;&#22411;&#20013;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#20542;&#21521;&#20110;&#23558;&#26356;&#22810;&#32047;&#31215;&#29992;&#25143;&#20132;&#20114;&#30340;&#20135;&#21697;&#20998;&#37197;&#26356;&#39640;&#30340;&#20998;&#25968;&#65292;&#23548;&#33268;&#38480;&#37327;&#20135;&#21697;&#34987;&#24573;&#35270;&#19988;&#23545;&#26368;&#32456;&#36755;&#20986;&#30340;&#36129;&#29486;&#36739;&#23569;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Meta-Split&#32593;&#32476;&#65288;MSN&#65289;&#26469;&#20998;&#21106;&#29992;&#25143;&#21382;&#21490;&#24207;&#21015;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06747v1 Announce Type: new  Abstract: Compared to business-to-consumer (B2C) e-commerce systems, consumer-to-consumer (C2C) e-commerce platforms usually encounter the limited-stock problem, that is, a product can only be sold one time in a C2C system. This poses several unique challenges for click-through rate (CTR) prediction. Due to limited user interactions for each product (i.e. item), the corresponding item embedding in the CTR model may not easily converge. This makes the conventional sequence modeling based approaches cannot effectively utilize user history information since historical user behaviors contain a mixture of items with different volume of stocks. Particularly, the attention mechanism in a sequence model tends to assign higher score to products with more accumulated user interactions, making limited-stock products being ignored and contribute less to the final output. To this end, we propose the Meta-Split Network (MSN) to split user history sequence regar
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21518;&#35757;&#32451;&#23646;&#24615;&#21462;&#28040;&#23398;&#20064;&#65288;PoT-AU&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#20004;&#37096;&#20998;&#25439;&#22833;&#20989;&#25968;&#65292;&#26088;&#22312;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#20445;&#25252;&#29992;&#25143;&#30340;&#25935;&#24863;&#23646;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.06737</link><description>&lt;p&gt;
&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#36827;&#34892;&#21518;&#35757;&#32451;&#23646;&#24615;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
Post-Training Attribute Unlearning in Recommender Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06737
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21518;&#35757;&#32451;&#23646;&#24615;&#21462;&#28040;&#23398;&#20064;&#65288;PoT-AU&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#20004;&#37096;&#20998;&#25439;&#22833;&#20989;&#25968;&#65292;&#26088;&#22312;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#20445;&#25252;&#29992;&#25143;&#30340;&#25935;&#24863;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25512;&#33616;&#31995;&#32479;&#20013;&#26085;&#30410;&#22686;&#38271;&#30340;&#38544;&#31169;&#38382;&#39064;&#65292;&#25512;&#33616;&#21462;&#28040;&#23398;&#20064;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#20351;&#29992;&#35757;&#32451;&#25968;&#25454;&#65292;&#21363;&#27169;&#22411;&#36755;&#20837;&#65292;&#20316;&#20026;&#21462;&#28040;&#23398;&#20064;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#27169;&#22411;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#27809;&#26377;&#26126;&#30830;&#36935;&#21040;&#65292;&#25915;&#20987;&#32773;&#20173;&#21487;&#20197;&#20174;&#27169;&#22411;&#20013;&#25552;&#21462;&#31169;&#20154;&#20449;&#24687;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#26410;&#35265;&#20449;&#24687;&#31216;&#20026;&#23646;&#24615;&#65292;&#24182;&#23558;&#20854;&#35270;&#20026;&#21462;&#28040;&#23398;&#20064;&#30446;&#26631;&#12290;&#20026;&#20102;&#20445;&#25252;&#29992;&#25143;&#30340;&#25935;&#24863;&#23646;&#24615;&#65292;&#23646;&#24615;&#21462;&#28040;&#23398;&#20064;&#65288;AU&#65289;&#26088;&#22312;&#20351;&#30446;&#26631;&#23646;&#24615;&#38590;&#20197;&#20998;&#36776;&#12290;&#26412;&#25991;&#20391;&#37325;&#20110;AU&#30340;&#19968;&#20010;&#20005;&#26684;&#20294;&#23454;&#38469;&#30340;&#35774;&#32622;&#65292;&#21363;&#21518;&#35757;&#32451;&#23646;&#24615;&#21462;&#28040;&#23398;&#20064;&#65288;PoT-AU&#65289;&#65292;&#20854;&#20013;&#21462;&#28040;&#23398;&#20064;&#21482;&#33021;&#22312;&#25512;&#33616;&#27169;&#22411;&#35757;&#32451;&#23436;&#25104;&#21518;&#25191;&#34892;&#12290;&#20026;&#20102;&#35299;&#20915;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;PoT-AU&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#37096;&#20998;&#25439;&#22833;&#20989;&#25968;&#12290;&#31532;&#19968;&#37096;&#20998;&#26159;&#21487;&#21306;&#20998;&#24615;&#25439;&#22833;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#20998;&#24067;&#30340;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06737v1 Announce Type: new  Abstract: With the growing privacy concerns in recommender systems, recommendation unlearning is getting increasing attention. Existing studies predominantly use training data, i.e., model inputs, as unlearning target. However, attackers can extract private information from the model even if it has not been explicitly encountered during training. We name this unseen information as \textit{attribute} and treat it as unlearning target. To protect the sensitive attribute of users, Attribute Unlearning (AU) aims to make target attributes indistinguishable. In this paper, we focus on a strict but practical setting of AU, namely Post-Training Attribute Unlearning (PoT-AU), where unlearning can only be performed after the training of the recommendation model is completed. To address the PoT-AU problem in recommender systems, we propose a two-component loss function. The first component is distinguishability loss, where we design a distribution-based meas
&lt;/p&gt;</description></item><item><title>ERIMap&#26159;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#20026;&#32039;&#24613;&#24773;&#20917;&#20013;&#30340;&#20915;&#31574;&#25552;&#20379;&#31995;&#32479;&#24615;&#21644;&#36805;&#36895;&#30340;&#20449;&#24687;&#22788;&#29702;&#65292;&#20943;&#23569;&#20915;&#31574;&#32773;&#30340;&#35748;&#30693;&#36127;&#33655;</title><link>https://arxiv.org/abs/2403.06716</link><description>&lt;p&gt;
&#32039;&#24613;&#21709;&#24212;&#25512;&#29702;&#26144;&#23556;&#65288;ERIMap&#65289;&#65306;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#31354;&#38388;&#20998;&#24067;&#32039;&#24613;&#24773;&#20917;&#30340;&#21160;&#24577;&#35266;&#27979;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
Emergency Response Inference Mapping (ERIMap): A Bayesian Network-based Method for Dynamic Observation Processing in Spatially Distributed Emergencies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06716
&lt;/p&gt;
&lt;p&gt;
ERIMap&#26159;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#20026;&#32039;&#24613;&#24773;&#20917;&#20013;&#30340;&#20915;&#31574;&#25552;&#20379;&#31995;&#32479;&#24615;&#21644;&#36805;&#36895;&#30340;&#20449;&#24687;&#22788;&#29702;&#65292;&#20943;&#23569;&#20915;&#31574;&#32773;&#30340;&#35748;&#30693;&#36127;&#33655;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32039;&#24613;&#24773;&#20917;&#19979;&#65292;&#39640;&#39118;&#38505;&#20915;&#31574;&#36890;&#24120;&#38656;&#35201;&#22312;&#26102;&#38388;&#32039;&#36843;&#21644;&#21387;&#21147;&#19979;&#20570;&#20986;&#12290;&#20026;&#20102;&#25903;&#25345;&#36825;&#26679;&#30340;&#20915;&#31574;&#65292;&#38656;&#35201;&#24555;&#36895;&#25910;&#38598;&#21644;&#22788;&#29702;&#26469;&#33258;&#21508;&#31181;&#20449;&#24687;&#28304;&#30340;&#20449;&#24687;&#12290;&#21487;&#29992;&#20449;&#24687;&#24448;&#24448;&#22312;&#26102;&#38388;&#21644;&#31354;&#38388;&#19978;&#21464;&#21270;&#65292;&#19981;&#30830;&#23450;&#65292;&#24182;&#19988;&#26377;&#26102;&#23384;&#22312;&#20914;&#31361;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#20915;&#31574;&#20013;&#30340;&#28508;&#22312;&#20559;&#35265;&#12290;&#30446;&#21069;&#65292;&#32570;&#20047;&#31995;&#32479;&#21270;&#30340;&#20449;&#24687;&#22788;&#29702;&#21644;&#24773;&#20917;&#35780;&#20272;&#26041;&#27861;&#65292;&#20197;&#28385;&#36275;&#32039;&#24613;&#24773;&#20917;&#30340;&#29305;&#27530;&#38656;&#27714;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ERIMap&#30340;&#22522;&#20110;&#36125;&#21494;&#26031;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#38024;&#23545;&#32039;&#24613;&#24773;&#20917;&#20013;&#22797;&#26434;&#30340;&#20449;&#24687;&#29615;&#22659;&#36827;&#34892;&#20102;&#23450;&#21046;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#31995;&#32479;&#22320;&#24555;&#36895;&#22788;&#29702;&#24322;&#26500;&#19988;&#21487;&#33021;&#19981;&#30830;&#23450;&#30340;&#35266;&#27979;&#65292;&#24182;&#23545;&#32039;&#24613;&#24773;&#20917;&#30340;&#20851;&#38190;&#21464;&#37327;&#36827;&#34892;&#25512;&#29702;&#12290;&#20174;&#32780;&#38477;&#20302;&#20102;&#20915;&#31574;&#32773;&#30340;&#22797;&#26434;&#24615;&#21644;&#35748;&#30693;&#36127;&#33655;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06716v1 Announce Type: new  Abstract: In emergencies, high stake decisions often have to be made under time pressure and strain. In order to support such decisions, information from various sources needs to be collected and processed rapidly. The information available tends to be temporally and spatially variable, uncertain, and sometimes conflicting, leading to potential biases in decisions. Currently, there is a lack of systematic approaches for information processing and situation assessment which meet the particular demands of emergency situations. To address this gap, we present a Bayesian network-based method called ERIMap that is tailored to the complex information-scape during emergencies. The method enables the systematic and rapid processing of heterogeneous and potentially uncertain observations and draws inferences about key variables of an emergency. It thereby reduces complexity and cognitive load for decision makers. The output of the ERIMap method is a dynami
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#25512;&#33616;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22806;&#37096;&#30693;&#35782;&#26469;&#24110;&#21161;&#29983;&#25104;&#30495;&#23454;&#21487;&#29992;&#30340;&#25991;&#26412;&#65292;&#24182;&#21253;&#25324;&#30693;&#35782;&#20026;&#22522;&#30784;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#26696;&#36827;&#34892;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2403.06642</link><description>&lt;p&gt;
KELLMRec: &#30693;&#35782;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
KELLMRec: Knowledge-Enhanced Large Language Models for Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06642
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#25512;&#33616;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22806;&#37096;&#30693;&#35782;&#26469;&#24110;&#21161;&#29983;&#25104;&#30495;&#23454;&#21487;&#29992;&#30340;&#25991;&#26412;&#65292;&#24182;&#21253;&#25324;&#30693;&#35782;&#20026;&#22522;&#30784;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#26696;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25512;&#33616;&#31995;&#32479;&#39046;&#22495;&#65292;&#21033;&#29992;&#35821;&#20041;&#20449;&#24687;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#38382;&#39064;&#65292;&#26088;&#22312;&#34917;&#20805;&#20027;&#27969;&#22522;&#20110;ID&#30340;&#26041;&#27861;&#30340;&#32570;&#22833;&#37096;&#20998;&#12290;&#38543;&#30528;LLM&#30340;&#20852;&#36215;&#65292;&#23427;&#20316;&#20026;&#30693;&#35782;&#24211;&#30340;&#33021;&#21147;&#21644;&#25512;&#29702;&#33021;&#21147;&#20026;&#36825;&#19968;&#30740;&#31350;&#39046;&#22495;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#65292;&#20351;&#22522;&#20110;LLM&#30340;&#25512;&#33616;&#25104;&#20026;&#26032;&#20852;&#30740;&#31350;&#26041;&#21521;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#20351;&#29992;LLM&#26469;&#22788;&#29702;&#25512;&#33616;&#22330;&#26223;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#26159;&#19981;&#21487;&#38752;&#21644;&#27425;&#20248;&#30340;&#65292;&#30001;&#20110;&#23384;&#22312;&#24187;&#35273;&#31561;&#38382;&#39064;&#12290;&#24212;&#23545;&#36825;&#19968;&#38382;&#39064;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#26159;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#26469;&#24110;&#21161;LLM&#29983;&#25104;&#30495;&#23454;&#21487;&#29992;&#30340;&#25991;&#26412;&#12290;&#21463;&#20197;&#19978;&#21160;&#26426;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#22686;&#24378;&#30340;LLMRec&#26041;&#27861;&#12290;&#38500;&#20102;&#22312;&#25552;&#31034;&#20013;&#20351;&#29992;&#22806;&#37096;&#30693;&#35782;&#22806;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#36824;&#21253;&#25324;&#19968;&#20010;&#22522;&#20110;&#30693;&#35782;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#26696;&#29992;&#20110;&#35757;&#32451;&#12290;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#21644;&#20225;&#19994;&#20013;&#36827;&#34892;&#30340;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06642v1 Announce Type: cross  Abstract: The utilization of semantic information is an important research problem in the field of recommender systems, which aims to complement the missing parts of mainstream ID-based approaches. With the rise of LLM, its ability to act as a knowledge base and its reasoning capability have opened up new possibilities for this research area, making LLM-based recommendation an emerging research direction. However, directly using LLM to process semantic information for recommendation scenarios is unreliable and sub-optimal due to several problems such as hallucination. A promising way to cope with this is to use external knowledge to aid LLM in generating truthful and usable text. Inspired by the above motivation, we propose a Knowledge-Enhanced LLMRec method. In addition to using external knowledge in prompts, the proposed method also includes a knowledge-based contrastive learning scheme for training. Experiments on public datasets and in-enter
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#20869;&#23481;&#30340;&#21307;&#23398;&#22270;&#20687;&#26816;&#32034;&#20013;&#65292;&#21033;&#29992;&#22522;&#30784;&#27169;&#22411;&#20316;&#20026;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#26080;&#38656;&#24494;&#35843;&#21363;&#21487;&#21462;&#24471;&#19982;&#19987;&#38376;&#27169;&#22411;&#31454;&#20105;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#22312;&#26816;&#32034;&#30149;&#29702;&#29305;&#24449;&#26041;&#38754;&#20855;&#26377;&#36739;&#22823;&#22256;&#38590;&#12290;</title><link>https://arxiv.org/abs/2403.06567</link><description>&lt;p&gt;
&#21033;&#29992;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#25918;&#23556;&#23398;&#20013;&#22522;&#20110;&#20869;&#23481;&#30340;&#21307;&#23398;&#22270;&#20687;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Leveraging Foundation Models for Content-Based Medical Image Retrieval in Radiology
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06567
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20869;&#23481;&#30340;&#21307;&#23398;&#22270;&#20687;&#26816;&#32034;&#20013;&#65292;&#21033;&#29992;&#22522;&#30784;&#27169;&#22411;&#20316;&#20026;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#26080;&#38656;&#24494;&#35843;&#21363;&#21487;&#21462;&#24471;&#19982;&#19987;&#38376;&#27169;&#22411;&#31454;&#20105;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#22312;&#26816;&#32034;&#30149;&#29702;&#29305;&#24449;&#26041;&#38754;&#20855;&#26377;&#36739;&#22823;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Content-based image retrieval&#65288;CBIR&#65289;&#26377;&#26395;&#26174;&#33879;&#25913;&#21892;&#25918;&#23556;&#23398;&#20013;&#30340;&#35786;&#26029;&#36741;&#21161;&#21644;&#21307;&#23398;&#30740;&#31350;&#12290;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#20316;&#20026;&#24378;&#22823;&#19988;&#22810;&#21151;&#33021;&#30340;&#29616;&#25104;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#29992;&#20110;&#22522;&#20110;&#20869;&#23481;&#30340;&#21307;&#23398;&#22270;&#20687;&#26816;&#32034;&#12290;&#36890;&#36807;&#22312;&#28085;&#30422;&#22235;&#31181;&#27169;&#24577;&#21644;161&#31181;&#30149;&#29702;&#23398;&#30340;160&#19975;&#24352;2D&#25918;&#23556;&#22270;&#20687;&#30340;&#20840;&#38754;&#25968;&#25454;&#38598;&#19978;&#23545;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#25105;&#20204;&#21457;&#29616;&#24369;&#30417;&#30563;&#27169;&#22411;&#34920;&#29616;&#20248;&#24322;&#65292;P@1&#21487;&#36798;0.594&#12290;&#36825;&#31181;&#24615;&#33021;&#19981;&#20165;&#19982;&#19987;&#38376;&#21270;&#27169;&#22411;&#31454;&#20105;&#65292;&#32780;&#19988;&#26080;&#38656;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;&#26816;&#32034;&#30149;&#29702;&#23398;&#19982;&#35299;&#21078;&#32467;&#26500;&#30340;&#25361;&#25112;&#65292;&#34920;&#26126;&#20934;&#30830;&#26816;&#32034;&#30149;&#29702;&#29305;&#24449;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06567v1 Announce Type: cross  Abstract: Content-based image retrieval (CBIR) has the potential to significantly improve diagnostic aid and medical research in radiology. Current CBIR systems face limitations due to their specialization to certain pathologies, limiting their utility. In response, we propose using vision foundation models as powerful and versatile off-the-shelf feature extractors for content-based medical image retrieval. By benchmarking these models on a comprehensive dataset of 1.6 million 2D radiological images spanning four modalities and 161 pathologies, we identify weakly-supervised models as superior, achieving a P@1 of up to 0.594. This performance not only competes with a specialized model but does so without the need for fine-tuning. Our analysis further explores the challenges in retrieving pathological versus anatomical structures, indicating that accurate retrieval of pathological features presents greater difficulty. Despite these challenges, our
&lt;/p&gt;</description></item><item><title>ToolRerank&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#21644;&#23618;&#27425;&#24863;&#30693;&#30340;&#37325;&#26032;&#25490;&#21517;&#26041;&#27861;&#65292;&#29992;&#20110;&#24037;&#20855;&#26816;&#32034;&#65292;&#36890;&#36807;Adaptive Truncation&#21644;Hierarchy-Aware Reranking&#26469;&#20248;&#21270;&#26816;&#32034;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.06551</link><description>&lt;p&gt;
ToolRerank&#65306;&#38754;&#21521;&#24037;&#20855;&#26816;&#32034;&#30340;&#33258;&#36866;&#24212;&#21644;&#23618;&#27425;&#24863;&#30693;&#37325;&#26032;&#25490;&#21517;
&lt;/p&gt;
&lt;p&gt;
ToolRerank: Adaptive and Hierarchy-Aware Reranking for Tool Retrieval
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06551
&lt;/p&gt;
&lt;p&gt;
ToolRerank&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#21644;&#23618;&#27425;&#24863;&#30693;&#30340;&#37325;&#26032;&#25490;&#21517;&#26041;&#27861;&#65292;&#29992;&#20110;&#24037;&#20855;&#26816;&#32034;&#65292;&#36890;&#36807;Adaptive Truncation&#21644;Hierarchy-Aware Reranking&#26469;&#20248;&#21270;&#26816;&#32034;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06551v1 &#31867;&#22411;&#65306;&#26032; &#21407;&#25688;&#35201;&#65306;&#24037;&#20855;&#23398;&#20064;&#26088;&#22312;&#36890;&#36807;&#22806;&#37096;&#24037;&#20855;&#25193;&#23637;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#12290;&#24037;&#20855;&#23398;&#20064;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#26159;&#22914;&#20309;&#25903;&#25345;&#22823;&#37327;&#24037;&#20855;&#65292;&#21253;&#25324;&#26410;&#35265;&#36807;&#30340;&#24037;&#20855;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#25552;&#20986;&#26681;&#25454;&#29992;&#25143;&#26597;&#35810;&#20026;LLM&#26816;&#32034;&#21512;&#36866;&#30340;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#25552;&#20986;&#30340;&#26041;&#27861;&#26082;&#27809;&#26377;&#32771;&#34385;&#35265;&#36807;&#21644;&#26410;&#35265;&#36807;&#24037;&#20855;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#20063;&#27809;&#26377;&#32771;&#34385;&#24037;&#20855;&#24211;&#30340;&#23618;&#27425;&#32467;&#26500;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#24037;&#20855;&#26816;&#32034;&#24615;&#33021;&#19981;&#20339;&#12290;&#22240;&#27492;&#65292;&#20026;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ToolRerank&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#24037;&#20855;&#26816;&#32034;&#30340;&#33258;&#36866;&#24212;&#21644;&#23618;&#27425;&#24863;&#30693;&#30340;&#37325;&#26032;&#25490;&#21517;&#26041;&#27861;&#65292;&#20197;&#36827;&#19968;&#27493;&#32454;&#21270;&#26816;&#32034;&#32467;&#26524;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;ToolRerank&#21253;&#25324;&#33258;&#36866;&#24212;&#25130;&#26029;&#65292;&#35813;&#26041;&#27861;&#23558;&#19982;&#35265;&#36807;&#21644;&#26410;&#35265;&#36807;&#24037;&#20855;&#30456;&#20851;&#30340;&#26816;&#32034;&#32467;&#26524;&#22312;&#19981;&#21516;&#20301;&#32622;&#25130;&#26029;&#65292;&#24182;&#21253;&#25324;&#23618;&#27425;&#24863;&#30693;&#37325;&#26032;&#25490;&#21517;&#65292;&#35813;&#26041;&#27861;&#35753;&#26816;&#32034;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06551v1 Announce Type: new  Abstract: Tool learning aims to extend the capabilities of large language models (LLMs) with external tools. A major challenge in tool learning is how to support a large number of tools, including unseen tools. To address this challenge, previous studies have proposed retrieving suitable tools for the LLM based on the user query. However, previously proposed methods do not consider the differences between seen and unseen tools, nor do they take the hierarchy of the tool library into account, which may lead to suboptimal performance for tool retrieval. Therefore, to address the aforementioned issues, we propose ToolRerank, an adaptive and hierarchy-aware reranking method for tool retrieval to further refine the retrieval results. Specifically, our proposed ToolRerank includes Adaptive Truncation, which truncates the retrieval results related to seen and unseen tools at different positions, and Hierarchy-Aware Reranking, which makes retrieval result
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;RecAI&#65292;&#19968;&#20010;&#26088;&#22312;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#25512;&#33616;&#31995;&#32479;&#30340;&#23454;&#29992;&#24037;&#20855;&#21253;&#65292;&#26032;&#19968;&#20195;&#30001;LLMs&#36171;&#33021;&#30340;&#25512;&#33616;&#31995;&#32479;&#23558;&#26356;&#21152;&#22810;&#25165;&#22810;&#33402;&#12289;&#21487;&#35299;&#37322;&#12289;&#23545;&#35805;&#24335;&#21644;&#21487;&#25511;&#65292;&#20026;&#26356;&#26234;&#33021;&#21644;&#29992;&#25143;&#20013;&#24515;&#30340;&#25512;&#33616;&#20307;&#39564;&#38138;&#24179;&#36947;&#36335;&#12290;</title><link>https://arxiv.org/abs/2403.06465</link><description>&lt;p&gt;
RecAI&#65306;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;&#19979;&#19968;&#20195;&#25512;&#33616;&#31995;&#32479;&#22686;&#28155;&#21147;&#37327;
&lt;/p&gt;
&lt;p&gt;
RecAI: Leveraging Large Language Models for Next-Generation Recommender Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06465
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;RecAI&#65292;&#19968;&#20010;&#26088;&#22312;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#25512;&#33616;&#31995;&#32479;&#30340;&#23454;&#29992;&#24037;&#20855;&#21253;&#65292;&#26032;&#19968;&#20195;&#30001;LLMs&#36171;&#33021;&#30340;&#25512;&#33616;&#31995;&#32479;&#23558;&#26356;&#21152;&#22810;&#25165;&#22810;&#33402;&#12289;&#21487;&#35299;&#37322;&#12289;&#23545;&#35805;&#24335;&#21644;&#21487;&#25511;&#65292;&#20026;&#26356;&#26234;&#33021;&#21644;&#29992;&#25143;&#20013;&#24515;&#30340;&#25512;&#33616;&#20307;&#39564;&#38138;&#24179;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;RecAI&#65292;&#19968;&#20010;&#23454;&#29992;&#30340;&#24037;&#20855;&#21253;&#65292;&#26088;&#22312;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20808;&#36827;&#33021;&#21147;&#26469;&#22686;&#24378;&#29978;&#33267;&#38761;&#26032;&#25512;&#33616;&#31995;&#32479;&#12290;RecAI&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#24037;&#20855;&#65292;&#21253;&#25324;&#25512;&#33616;AI&#20195;&#29702;&#12289;&#38754;&#21521;&#25512;&#33616;&#30340;&#35821;&#35328;&#27169;&#22411;&#12289;&#30693;&#35782;&#25554;&#20214;&#12289;&#25512;&#33616;&#35299;&#37322;&#22120;&#21644;&#35780;&#20272;&#22120;&#65292;&#20197;&#22810;&#35282;&#24230;&#20419;&#36827;LLMs&#34701;&#20837;&#25512;&#33616;&#31995;&#32479;&#12290;LLMs&#36171;&#33021;&#30340;&#26032;&#19968;&#20195;&#25512;&#33616;&#31995;&#32479;&#39044;&#35745;&#23558;&#26356;&#21152;&#22810;&#25165;&#22810;&#33402;&#12289;&#21487;&#35299;&#37322;&#12289;&#23545;&#35805;&#24335;&#21644;&#21487;&#25511;&#65292;&#20026;&#26356;&#26234;&#33021;&#21644;&#29992;&#25143;&#20013;&#24515;&#30340;&#25512;&#33616;&#20307;&#39564;&#38138;&#24179;&#36947;&#36335;&#12290;&#25105;&#20204;&#24076;&#26395;RecAI&#30340;&#24320;&#28304;&#33021;&#22815;&#21152;&#36895;&#26032;&#19968;&#20195;&#20808;&#36827;&#25512;&#33616;&#31995;&#32479;&#30340;&#28436;&#36827;&#12290;RecAI&#30340;&#28304;&#20195;&#30721;&#21487;&#22312; \url{https://github.com/microsoft/RecAI} &#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06465v1 Announce Type: cross  Abstract: This paper introduces RecAI, a practical toolkit designed to augment or even revolutionize recommender systems with the advanced capabilities of Large Language Models (LLMs). RecAI provides a suite of tools, including Recommender AI Agent, Recommendation-oriented Language Models, Knowledge Plugin, RecExplainer, and Evaluator, to facilitate the integration of LLMs into recommender systems from multifaceted perspectives. The new generation of recommender systems, empowered by LLMs, are expected to be more versatile, explainable, conversational, and controllable, paving the way for more intelligent and user-centric recommendation experiences. We hope the open-source of RecAI can help accelerate evolution of new advanced recommender systems. The source code of RecAI is available at \url{https://github.com/microsoft/RecAI}.
&lt;/p&gt;</description></item><item><title>CoRAL&#24341;&#20837;&#20102;&#21327;&#20316;&#26816;&#32034;&#22686;&#24378;LLMs&#65292;&#23558;&#21327;&#20316;&#35777;&#25454;&#30452;&#25509;&#32435;&#20837;&#21040;&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#29702;&#35299;&#29992;&#25143;&#20043;&#38388;&#30340;&#20559;&#22909;&#65292;&#25913;&#36827;&#38271;&#23614;&#25512;&#33616;&#12290;</title><link>https://arxiv.org/abs/2403.06447</link><description>&lt;p&gt;
CoRAL: &#21327;&#20316;&#26816;&#32034;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25913;&#36827;&#38271;&#23614;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
CoRAL: Collaborative Retrieval-Augmented Large Language Models Improve Long-tail Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06447
&lt;/p&gt;
&lt;p&gt;
CoRAL&#24341;&#20837;&#20102;&#21327;&#20316;&#26816;&#32034;&#22686;&#24378;LLMs&#65292;&#23558;&#21327;&#20316;&#35777;&#25454;&#30452;&#25509;&#32435;&#20837;&#21040;&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#29702;&#35299;&#29992;&#25143;&#20043;&#38388;&#30340;&#20559;&#22909;&#65292;&#25913;&#36827;&#38271;&#23614;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#23614;&#25512;&#33616;&#23545;&#20256;&#32479;&#25512;&#33616;&#31995;&#32479;&#26469;&#35828;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#25968;&#25454;&#31232;&#30095;&#21644;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21457;&#23637;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#22797;&#26434;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#21487;&#20197;&#24110;&#21161;&#22522;&#20110;&#38750;&#24120;&#23569;&#30340;&#20808;&#21069;&#20132;&#20114;&#26469;&#25512;&#26029;&#29992;&#25143;&#30340;&#20559;&#22909;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22823;&#22810;&#25968;&#22522;&#20110;LLM&#30340;&#31995;&#32479;&#20165;&#20381;&#36182;&#20110;&#29289;&#21697;&#30340;&#35821;&#20041;&#21547;&#20041;&#20316;&#20026;&#25512;&#29702;&#30340;&#21807;&#19968;&#35777;&#25454;&#65292;&#24573;&#30053;&#20102;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#30340;&#21327;&#20316;&#20449;&#24687;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;LLM&#30340;&#25512;&#29702;&#19982;&#25968;&#25454;&#38598;&#30340;&#20219;&#21153;&#29305;&#23450;&#21327;&#20316;&#20449;&#24687;&#19981;&#19968;&#33268;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#23558;LLMs&#30340;&#25512;&#29702;&#19982;&#20219;&#21153;&#29305;&#23450;&#30340;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#30693;&#35782;&#30456;&#19968;&#33268;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21327;&#20316;&#26816;&#32034;&#22686;&#24378;LLMs&#65292;&#31216;&#20026;CoRAL&#65292;&#30452;&#25509;&#23558;&#21327;&#20316;&#35777;&#25454;&#32435;&#20837;&#20132;&#20114;&#20013;&#12290;&#22522;&#20110;&#26816;&#32034;&#21040;&#30340;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#65292;LLMs&#21487;&#20197;&#20998;&#26512;&#29992;&#25143;&#20043;&#38388;&#30340;&#20849;&#20139;&#21644;&#19981;&#21516;&#20559;&#22909;&#65292;&#24182;&#24635;&#32467;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06447v1 Announce Type: cross  Abstract: The long-tail recommendation is a challenging task for traditional recommender systems, due to data sparsity and data imbalance issues. The recent development of large language models (LLMs) has shown their abilities in complex reasoning, which can help to deduce users' preferences based on very few previous interactions. However, since most LLM-based systems rely on items' semantic meaning as the sole evidence for reasoning, the collaborative information of user-item interactions is neglected, which can cause the LLM's reasoning to be misaligned with task-specific collaborative information of the dataset. To further align LLMs' reasoning to task-specific user-item interaction knowledge, we introduce collaborative retrieval-augmented LLMs, CoRAL, which directly incorporate collaborative evidence into the prompts. Based on the retrieved user-item interactions, the LLM can analyze shared and distinct preferences among users, and summariz
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"RepPad"&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22635;&#20805;&#26041;&#27861;&#65292;&#26088;&#22312;&#20805;&#20998;&#21033;&#29992;&#22635;&#20805;&#31354;&#38388;&#26469;&#25552;&#39640;&#39034;&#24207;&#25512;&#33616;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#35757;&#32451;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.06372</link><description>&lt;p&gt;
&#37325;&#22797;&#22635;&#20805;&#20316;&#20026;&#39034;&#24207;&#25512;&#33616;&#30340;&#25968;&#25454;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Repeated Padding as Data Augmentation for Sequential Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06372
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"RepPad"&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22635;&#20805;&#26041;&#27861;&#65292;&#26088;&#22312;&#20805;&#20998;&#21033;&#29992;&#22635;&#20805;&#31354;&#38388;&#26469;&#25552;&#39640;&#39034;&#24207;&#25512;&#33616;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#35757;&#32451;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39034;&#24207;&#25512;&#33616;&#26088;&#22312;&#26681;&#25454;&#29992;&#25143;&#30340;&#21382;&#21490;&#20114;&#21160;&#25552;&#20379;&#20010;&#24615;&#21270;&#24314;&#35758;&#12290;&#22312;&#35757;&#32451;&#39034;&#24207;&#27169;&#22411;&#26102;&#65292;&#22635;&#20805;&#26159;&#19968;&#31181;&#34987;&#24191;&#27867;&#37319;&#29992;&#30340;&#25216;&#26415;&#65292;&#20027;&#35201;&#21407;&#22240;&#26377;&#20004;&#20010;&#65306;1&#65289;&#32477;&#22823;&#22810;&#25968;&#27169;&#22411;&#21482;&#33021;&#22788;&#29702;&#22266;&#23450;&#38271;&#24230;&#30340;&#24207;&#21015;&#65307;2&#65289;&#22522;&#20110;&#25209;&#22788;&#29702;&#30340;&#35757;&#32451;&#38656;&#35201;&#30830;&#20445;&#27599;&#20010;&#25209;&#27425;&#20013;&#30340;&#24207;&#21015;&#20855;&#26377;&#30456;&#21516;&#30340;&#38271;&#24230;&#12290;&#36890;&#24120;&#20351;&#29992;&#29305;&#27530;&#20540;0&#20316;&#20026;&#22635;&#20805;&#20869;&#23481;&#65292;&#19981;&#21253;&#21547;&#23454;&#38469;&#20449;&#24687;&#24182;&#22312;&#27169;&#22411;&#35745;&#31639;&#20013;&#34987;&#24573;&#30053;&#12290;&#36825;&#31181;&#24120;&#35782;&#22635;&#20805;&#31574;&#30053;&#24341;&#20986;&#20102;&#19968;&#20010;&#20197;&#21069;&#20174;&#26410;&#25506;&#35752;&#36807;&#30340;&#38382;&#39064;&#65306;&#25105;&#20204;&#33021;&#21542;&#36890;&#36807;&#22635;&#20805;&#20854;&#20182;&#20869;&#23481;&#20805;&#20998;&#21033;&#29992;&#36825;&#19968;&#38386;&#32622;&#36755;&#20837;&#31354;&#38388;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#21644;&#35757;&#32451;&#25928;&#29575;&#65311; &#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22635;&#20805;&#26041;&#27861;&#65292;&#21517;&#20026;RepPad (&#37325;&#22797;&#22635;&#20805;)&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06372v1 Announce Type: new  Abstract: Sequential recommendation aims to provide users with personalized suggestions based on their historical interactions. When training sequential models, padding is a widely adopted technique for two main reasons: 1) The vast majority of models can only handle fixed-length sequences; 2) Batching-based training needs to ensure that the sequences in each batch have the same length. The special value \emph{0} is usually used as the padding content, which does not contain the actual information and is ignored in the model calculations. This common-sense padding strategy leads us to a problem that has never been explored before: \emph{Can we fully utilize this idle input space by padding other content to further improve model performance and training efficiency?}   In this paper, we propose a simple yet effective padding method called \textbf{Rep}eated \textbf{Pad}ding (\textbf{RepPad}). Specifically, we use the original interaction sequences as
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#39318;&#27425;&#30740;&#31350;&#20102;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32534;&#36753;&#27010;&#24565;&#30693;&#35782;&#65292;&#36890;&#36807;&#26500;&#24314;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#24314;&#31435;&#26032;&#35780;&#20272;&#25351;&#26631;&#65292;&#21457;&#29616;&#29616;&#26377;&#26041;&#27861;&#34429;&#28982;&#33021;&#19968;&#23450;&#31243;&#24230;&#19978;&#20462;&#25913;&#27010;&#24565;&#23450;&#20041;&#65292;&#20294;&#20063;&#21487;&#33021;&#36896;&#25104;LLMs&#20013;&#30456;&#20851;&#23454;&#20363;&#30693;&#35782;&#30340;&#25197;&#26354;&#65292;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#12290;</title><link>https://arxiv.org/abs/2403.06259</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27010;&#24565;&#30693;&#35782;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Editing Conceptual Knowledge for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06259
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#39318;&#27425;&#30740;&#31350;&#20102;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32534;&#36753;&#27010;&#24565;&#30693;&#35782;&#65292;&#36890;&#36807;&#26500;&#24314;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#24314;&#31435;&#26032;&#35780;&#20272;&#25351;&#26631;&#65292;&#21457;&#29616;&#29616;&#26377;&#26041;&#27861;&#34429;&#28982;&#33021;&#19968;&#23450;&#31243;&#24230;&#19978;&#20462;&#25913;&#27010;&#24565;&#23450;&#20041;&#65292;&#20294;&#20063;&#21487;&#33021;&#36896;&#25104;LLMs&#20013;&#30456;&#20851;&#23454;&#20363;&#30693;&#35782;&#30340;&#25197;&#26354;&#65292;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#30693;&#35782;&#32534;&#36753;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#24403;&#21069;&#30340;&#26041;&#27861;&#21644;&#35780;&#20272;&#20165;&#25506;&#35752;&#20102;&#23454;&#20363;&#32423;&#21035;&#30340;&#32534;&#36753;&#65292;&#28982;&#32780;LLMs&#26159;&#21542;&#20855;&#26377;&#20462;&#25913;&#27010;&#24565;&#30340;&#33021;&#21147;&#20173;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#39318;&#27425;&#30740;&#31350;&#20102;&#20026;LLMs&#32534;&#36753;&#27010;&#24565;&#30693;&#35782;&#65292;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;ConceptEdit&#24182;&#24314;&#31435;&#20102;&#19968;&#22871;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;&#29616;&#26377;&#30340;&#32534;&#36753;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#20462;&#25913;&#27010;&#24565;&#32423;&#21035;&#30340;&#23450;&#20041;&#65292;&#20294;&#23427;&#20204;&#20063;&#26377;&#28508;&#21147;&#25197;&#26354;LLMs&#20013;&#30456;&#20851;&#30340;&#23454;&#20363;&#30693;&#35782;&#65292;&#23548;&#33268;&#24615;&#33021;&#19981;&#20339;&#12290;&#25105;&#20204;&#26399;&#26395;&#36825;&#21487;&#20197;&#28608;&#21457;&#23545;&#26356;&#22909;&#29702;&#35299;LLMs&#30340;&#36827;&#19968;&#27493;&#36827;&#23637;&#12290;&#25105;&#20204;&#30340;&#39033;&#30446;&#20027;&#39029;&#20301;&#20110;https://zjunlp.github.io/project/ConceptEdit&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06259v1 Announce Type: cross  Abstract: Recently, there has been a growing interest in knowledge editing for Large Language Models (LLMs). Current approaches and evaluations merely explore the instance-level editing, while whether LLMs possess the capability to modify concepts remains unclear. This paper pioneers the investigation of editing conceptual knowledge for LLMs, by constructing a novel benchmark dataset ConceptEdit and establishing a suite of new metrics for evaluation. The experimental results reveal that, although existing editing methods can efficiently modify concept-level definition to some extent, they also have the potential to distort the related instantial knowledge in LLMs, leading to poor performance. We anticipate this can inspire further progress in better understanding LLMs. Our project homepage is available at https://zjunlp.github.io/project/ConceptEdit.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;TRAD&#26694;&#26550;&#65292;&#36890;&#36807;&#27493;&#39588;&#24335;&#24605;&#32500;&#26816;&#32034;&#21644;&#23545;&#40784;&#20915;&#31574;&#35299;&#20915;&#20102;&#21033;&#29992;&#19978;&#19979;&#25991;&#31034;&#20363;&#26102;&#21487;&#33021;&#20986;&#29616;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.06221</link><description>&lt;p&gt;
&#29992;&#27493;&#39588;&#24335;&#24605;&#32500;&#26816;&#32034;&#21644;&#23545;&#40784;&#20915;&#31574;&#22686;&#24378;LLM&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
TRAD: Enhancing LLM Agents with Step-Wise Thought Retrieval and Aligned Decision
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06221
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;TRAD&#26694;&#26550;&#65292;&#36890;&#36807;&#27493;&#39588;&#24335;&#24605;&#32500;&#26816;&#32034;&#21644;&#23545;&#40784;&#20915;&#31574;&#35299;&#20915;&#20102;&#21033;&#29992;&#19978;&#19979;&#25991;&#31034;&#20363;&#26102;&#21487;&#33021;&#20986;&#29616;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20195;&#29702;&#24050;&#32463;&#34987;&#26500;&#24314;&#29992;&#20110;&#19981;&#21516;&#20219;&#21153;&#65292;&#22914;&#32593;&#32476;&#23548;&#33322;&#21644;&#22312;&#32447;&#36141;&#29289;&#65292;&#36825;&#26159;&#22240;&#20026;LLM&#20855;&#26377;&#24191;&#27867;&#30340;&#30693;&#35782;&#21644;&#25991;&#26412;&#29702;&#35299;&#33021;&#21147;&#12290;&#22312;&#36825;&#20123;&#30740;&#31350;&#20013;&#65292;&#35768;&#22810;&#21033;&#29992;&#19978;&#19979;&#25991;&#31034;&#20363;&#26469;&#23454;&#29616;&#27867;&#21270;&#65292;&#32780;&#26080;&#38656;&#24494;&#35843;&#65292;&#20294;&#23569;&#25968;&#32771;&#34385;&#20102;&#22914;&#20309;&#36873;&#25321;&#21644;&#26377;&#25928;&#21033;&#29992;&#36825;&#20123;&#31034;&#20363;&#30340;&#38382;&#39064;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#36712;&#36857;&#32423;&#26816;&#32034;&#21644;&#20351;&#29992;&#36712;&#36857;&#20316;&#20026;&#19978;&#19979;&#25991;&#31034;&#20363;&#30340;&#26041;&#27861;&#24050;&#32463;&#25552;&#20986;&#65292;&#20197;&#25552;&#39640;&#20195;&#29702;&#22312;&#19968;&#20123;&#39034;&#24207;&#20915;&#31574;&#20219;&#21153;&#20013;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#23384;&#22312;&#38382;&#39064;&#65292;&#22240;&#20026;&#26816;&#32034;&#20986;&#30340;&#21487;&#20449;&#31034;&#20363;&#32570;&#20047;&#29305;&#23450;&#20110;&#20219;&#21153;&#30340;&#29366;&#24577;&#36716;&#31227;&#21160;&#24577;&#65292;&#19988;&#36755;&#20837;&#38271;&#19988;&#21253;&#21547;&#22823;&#37327;&#26080;&#20851;&#19978;&#19979;&#25991;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26694;&#26550;&#65288;TRAD&#65289;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;TRAD&#39318;&#20808;&#36827;&#34892;&#24605;&#32500;&#26816;&#32034;&#65292;&#23454;&#29616;&#27493;&#39588;&#32423;&#28436;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06221v1 Announce Type: new  Abstract: Numerous large language model (LLM) agents have been built for different tasks like web navigation and online shopping due to LLM's wide knowledge and text-understanding ability. Among these works, many of them utilize in-context examples to achieve generalization without the need for fine-tuning, while few of them have considered the problem of how to select and effectively utilize these examples. Recently, methods based on trajectory-level retrieval with task meta-data and using trajectories as in-context examples have been proposed to improve the agent's overall performance in some sequential decision making tasks. However, these methods can be problematic due to plausible examples retrieved without task-specific state transition dynamics and long input with plenty of irrelevant context. In this paper, we propose a novel framework (TRAD) to address these issues. TRAD first conducts Thought Retrieval, achieving step-level demonstration
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#26080;&#20154;&#26426;&#20132;&#20184;&#31995;&#32479;&#20013;&#22320;&#22336;&#35299;&#26512;&#20219;&#21153;&#30340;&#32454;&#31890;&#24230;&#20013;&#25991;&#22995;&#21517;&#23454;&#20307;&#35782;&#21035;&#25968;&#25454;&#38598;CNER-UAV&#65292;&#21253;&#21547;&#20116;&#20010;&#31867;&#21035;&#30340;&#22810;&#26679;&#21270;&#25968;&#25454;&#65292;&#32463;&#36807;&#20005;&#26684;&#30340;&#25968;&#25454;&#28165;&#27927;&#21644;&#21435;&#25935;&#22788;&#29702;&#65292;&#32422;&#26377;12,000&#20010;&#26631;&#27880;&#26679;&#26412;&#65292;&#35780;&#20272;&#20102;&#20256;&#32479;&#30340;&#23454;&#20307;&#35782;&#21035;&#27169;&#22411;&#24182;&#25552;&#20379;&#20102;&#28145;&#20837;&#20998;&#26512;</title><link>https://arxiv.org/abs/2403.06097</link><description>&lt;p&gt;
&#33021;&#21542;&#29992;LLM&#26367;&#20195;&#20154;&#24037;&#26631;&#27880;&#65311; &#26080;&#20154;&#26426;&#20132;&#20184;&#20219;&#21153;&#19979;&#30340;&#32454;&#31890;&#24230;&#20013;&#25991;&#22320;&#22336;&#23454;&#20307;&#35782;&#21035;&#25968;&#25454;&#38598;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Can LLM Substitute Human Labeling? A Case Study of Fine-grained Chinese Address Entity Recognition Dataset for UAV Delivery
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06097
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#26080;&#20154;&#26426;&#20132;&#20184;&#31995;&#32479;&#20013;&#22320;&#22336;&#35299;&#26512;&#20219;&#21153;&#30340;&#32454;&#31890;&#24230;&#20013;&#25991;&#22995;&#21517;&#23454;&#20307;&#35782;&#21035;&#25968;&#25454;&#38598;CNER-UAV&#65292;&#21253;&#21547;&#20116;&#20010;&#31867;&#21035;&#30340;&#22810;&#26679;&#21270;&#25968;&#25454;&#65292;&#32463;&#36807;&#20005;&#26684;&#30340;&#25968;&#25454;&#28165;&#27927;&#21644;&#21435;&#25935;&#22788;&#29702;&#65292;&#32422;&#26377;12,000&#20010;&#26631;&#27880;&#26679;&#26412;&#65292;&#35780;&#20272;&#20102;&#20256;&#32479;&#30340;&#23454;&#20307;&#35782;&#21035;&#27169;&#22411;&#24182;&#25552;&#20379;&#20102;&#28145;&#20837;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;CNER-UAV&#65292;&#19968;&#20010;&#19987;&#20026;&#26080;&#20154;&#26426;&#20132;&#20184;&#31995;&#32479;&#20013;&#22320;&#22336;&#35299;&#26512;&#20219;&#21153;&#35774;&#35745;&#30340;&#32454;&#31890;&#24230;&#20013;&#25991;&#22995;&#21517;&#23454;&#20307;&#35782;&#21035;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#28085;&#30422;&#20102;&#20116;&#20010;&#31867;&#21035;&#65292;&#21487;&#20197;&#20840;&#38754;&#35757;&#32451;&#21644;&#35780;&#20272;&#23454;&#20307;&#35782;&#21035;&#27169;&#22411;&#12290;&#20026;&#26500;&#24314;&#36825;&#19968;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#20174;&#30495;&#23454;&#26080;&#20154;&#26426;&#20132;&#20184;&#31995;&#32479;&#20013;&#33719;&#21462;&#25968;&#25454;&#65292;&#24182;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#25968;&#25454;&#28165;&#27927;&#21644;&#21435;&#25935;&#22788;&#29702;&#65292;&#30830;&#20445;&#25968;&#25454;&#30340;&#38544;&#31169;&#24615;&#21644;&#23436;&#25972;&#24615;&#12290;&#26368;&#32456;&#30340;&#25968;&#25454;&#38598;&#32422;&#21253;&#21547;12,000&#20010;&#26631;&#27880;&#26679;&#26412;&#65292;&#32463;&#36807;&#20154;&#24037;&#19987;&#23478;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27880;&#37322;&#12290;&#25105;&#20204;&#22312;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#20256;&#32479;&#30340;&#23454;&#20307;&#35782;&#21035;&#27169;&#22411;&#65292;&#24182;&#25552;&#20379;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#21487;&#20197;&#22312; \url{https://github.com/zhhvvv/CNER-UAV} &#19978;&#20844;&#24320;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06097v1 Announce Type: cross  Abstract: We present CNER-UAV, a fine-grained \textbf{C}hinese \textbf{N}ame \textbf{E}ntity \textbf{R}ecognition dataset specifically designed for the task of address resolution in \textbf{U}nmanned \textbf{A}erial \textbf{V}ehicle delivery systems. The dataset encompasses a diverse range of five categories, enabling comprehensive training and evaluation of NER models. To construct this dataset, we sourced the data from a real-world UAV delivery system and conducted a rigorous data cleaning and desensitization process to ensure privacy and data integrity. The resulting dataset, consisting of around 12,000 annotated samples, underwent human experts and \textbf{L}arge \textbf{L}anguage \textbf{M}odel annotation. We evaluated classical NER models on our dataset and provided in-depth analysis. The dataset and models are publicly available at \url{https://github.com/zhhvvv/CNER-UAV}.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#20301;&#25513;&#30721;&#20581;&#22766;&#23545;&#27604;&#24230;&#30693;&#35782;&#33976;&#39311;&#65288;BRCD&#65289;&#26041;&#27861;&#65292;&#19987;&#38376;&#20026;&#35821;&#20041;&#21704;&#24076;&#27169;&#22411;&#30340;&#33976;&#39311;&#32780;&#35774;&#35745;&#65292;&#20197;&#21462;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.06071</link><description>&lt;p&gt;
&#20301;&#25513;&#30721;&#20581;&#22766;&#23545;&#27604;&#24230;&#30693;&#35782;&#33976;&#39311;&#29992;&#20110;&#26080;&#30417;&#30563;&#35821;&#20041;&#21704;&#24076;
&lt;/p&gt;
&lt;p&gt;
Bit-mask Robust Contrastive Knowledge Distillation for Unsupervised Semantic Hashing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06071
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#20301;&#25513;&#30721;&#20581;&#22766;&#23545;&#27604;&#24230;&#30693;&#35782;&#33976;&#39311;&#65288;BRCD&#65289;&#26041;&#27861;&#65292;&#19987;&#38376;&#20026;&#35821;&#20041;&#21704;&#24076;&#27169;&#22411;&#30340;&#33976;&#39311;&#32780;&#35774;&#35745;&#65292;&#20197;&#21462;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#35821;&#20041;&#21704;&#24076;&#24050;&#32463;&#25104;&#20026;&#24555;&#36895;&#22270;&#20687;&#25628;&#32034;&#30340;&#19981;&#21487;&#25110;&#32570;&#30340;&#25216;&#26415;&#65292;&#26088;&#22312;&#23558;&#22270;&#20687;&#36716;&#25442;&#20026;&#20108;&#36827;&#21046;&#21704;&#24076;&#30721;&#32780;&#19981;&#20381;&#36182;&#26631;&#31614;&#12290;&#26368;&#36817;&#22312;&#35813;&#39046;&#22495;&#30340;&#36827;&#23637;&#34920;&#26126;&#65292;&#22312;&#26080;&#30417;&#30563;&#35821;&#20041;&#21704;&#24076;&#27169;&#22411;&#20013;&#20351;&#29992;&#22823;&#35268;&#27169;&#39592;&#24178;&#65288;&#20363;&#22914; ViT&#65289;&#21487;&#20197;&#24102;&#26469;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#28982;&#32780;&#65292;&#25512;&#26029;&#24310;&#36831;&#21464;&#24471;&#36234;&#26469;&#36234;&#38590;&#20197;&#24573;&#35270;&#12290;&#30693;&#35782;&#33976;&#39311;&#25552;&#20379;&#20102;&#19968;&#31181;&#23454;&#29616;&#27169;&#22411;&#21387;&#32553;&#20197;&#32531;&#35299;&#27492;&#24310;&#36831;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#24182;&#26410;&#19987;&#38376;&#38024;&#23545;&#35821;&#20041;&#21704;&#24076;&#36827;&#34892;&#35774;&#35745;&#12290;&#23427;&#20204;&#24573;&#30053;&#20102;&#35821;&#20041;&#21704;&#24076;&#30340;&#29420;&#29305;&#25628;&#32034;&#33539;&#24335;&#65292;&#33976;&#39311;&#36807;&#31243;&#30340;&#22266;&#26377;&#24517;&#38656;&#24615;&#20197;&#21450;&#21704;&#24076;&#30721;&#30340;&#23646;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#20301;&#25513;&#30721;&#20581;&#22766;&#23545;&#27604;&#24230;&#30693;&#35782;&#33976;&#39311;&#65288;BRCD&#65289;&#26041;&#27861;&#65292;&#19987;&#20026;&#35821;&#20041;&#21704;&#24076;&#27169;&#22411;&#30340;&#33976;&#39311;&#32780;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06071v1 Announce Type: cross  Abstract: Unsupervised semantic hashing has emerged as an indispensable technique for fast image search, which aims to convert images into binary hash codes without relying on labels. Recent advancements in the field demonstrate that employing large-scale backbones (e.g., ViT) in unsupervised semantic hashing models can yield substantial improvements. However, the inference delay has become increasingly difficult to overlook. Knowledge distillation provides a means for practical model compression to alleviate this delay. Nevertheless, the prevailing knowledge distillation approaches are not explicitly designed for semantic hashing. They ignore the unique search paradigm of semantic hashing, the inherent necessities of the distillation process, and the property of hash codes. In this paper, we propose an innovative Bit-mask Robust Contrastive knowledge Distillation (BRCD) method, specifically devised for the distillation of semantic hashing model
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#20998;&#23618;&#20449;&#24687;&#30340;&#26032;&#26694;&#26550;&#65292;&#20197;&#22686;&#24378;&#34920;&#31034;&#23398;&#20064;&#65292;&#35299;&#20915;&#20102;&#20998;&#23618;&#26597;&#35810;&#20998;&#31867;&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#21644;&#26597;&#35810;&#27169;&#31946;&#24615;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.06021</link><description>&lt;p&gt;
&#30005;&#23376;&#21830;&#21153;&#25628;&#32034;&#20013;&#30340;&#20998;&#23618;&#26597;&#35810;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Query Classification in E-commerce Search
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06021
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#20998;&#23618;&#20449;&#24687;&#30340;&#26032;&#26694;&#26550;&#65292;&#20197;&#22686;&#24378;&#34920;&#31034;&#23398;&#20064;&#65292;&#35299;&#20915;&#20102;&#20998;&#23618;&#26597;&#35810;&#20998;&#31867;&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#21644;&#26597;&#35810;&#27169;&#31946;&#24615;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#36890;&#24120;&#20197;&#23618;&#27425;&#32467;&#26500;&#23384;&#20648;&#21644;&#26500;&#36896;&#20135;&#21697;&#20449;&#24687;&#21644;&#25628;&#32034;&#25968;&#25454;&#12290;&#23558;&#29992;&#25143;&#25628;&#32034;&#26597;&#35810;&#26377;&#25928;&#20998;&#31867;&#21040;&#31867;&#20284;&#30340;&#23618;&#27425;&#32467;&#26500;&#20013;&#65292;&#22312;&#25552;&#21319;&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#29992;&#25143;&#20307;&#39564;&#30340;&#21516;&#26102;&#65292;&#23545;&#20110;&#26032;&#38395;&#25972;&#29702;&#21644;&#23398;&#26415;&#30740;&#31350;&#20063;&#33267;&#20851;&#37325;&#35201;&#12290;&#22788;&#29702;&#25935;&#24863;&#26597;&#35810;&#20998;&#31867;&#25110;&#20851;&#38190;&#20449;&#24687;&#20256;&#25773;&#26102;&#65292;&#31934;&#30830;&#24615;&#30340;&#37325;&#35201;&#24615;&#24471;&#21040;&#25918;&#22823;&#65292;&#22240;&#20026;&#19981;&#20934;&#30830;&#21487;&#33021;&#24102;&#26469;&#30456;&#24403;&#22823;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;&#20998;&#23618;&#26597;&#35810;&#20998;&#31867;&#30340;&#22266;&#26377;&#22797;&#26434;&#24615;&#21463;&#21040;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#30340;&#24433;&#21709;&#65306;&#65288;1&#65289;&#26126;&#26174;&#30340;&#20559;&#21521;&#20027;&#23548;&#31867;&#21035;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#24615;&#65292;&#21644;&#65288;2&#65289;&#25628;&#32034;&#26597;&#35810;&#30340;&#22266;&#26377;&#31616;&#27905;&#24615;&#21644;&#27169;&#31946;&#24615;&#65292;&#38459;&#30861;&#20102;&#20934;&#30830;&#20998;&#31867;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#65288;i&#65289;&#22686;&#24378;&#30340;&#34920;&#31034;&#23398;&#20064;&#26469;&#21033;&#29992;&#20998;&#23618;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06021v1 Announce Type: cross  Abstract: E-commerce platforms typically store and structure product information and search data in a hierarchy. Efficiently categorizing user search queries into a similar hierarchical structure is paramount in enhancing user experience on e-commerce platforms as well as news curation and academic research. The significance of this task is amplified when dealing with sensitive query categorization or critical information dissemination, where inaccuracies can lead to considerable negative impacts. The inherent complexity of hierarchical query classification is compounded by two primary challenges: (1) the pronounced class imbalance that skews towards dominant categories, and (2) the inherent brevity and ambiguity of search queries that hinder accurate classification.   To address these challenges, we introduce a novel framework that leverages hierarchical information through (i) enhanced representation learning that utilizes the contrastive loss
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Legion&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PTMs&#65289;&#20026;GitHub&#23384;&#20648;&#24211;&#25512;&#33616;&#20027;&#39064;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#25216;&#26415;&#22312;&#20027;&#39064;&#25512;&#33616;&#20013;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.05873</link><description>&lt;p&gt;
LEGION&#65306;&#21033;&#29992;&#20998;&#24067;&#24179;&#34913;&#25439;&#22833;&#35843;&#25972;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;GitHub&#20027;&#39064;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
LEGION: Harnessing Pre-trained Language Models for GitHub Topic Recommendations with Distribution-Balance Loss
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05873
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Legion&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PTMs&#65289;&#20026;GitHub&#23384;&#20648;&#24211;&#25512;&#33616;&#20027;&#39064;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#25216;&#26415;&#22312;&#20027;&#39064;&#25512;&#33616;&#20013;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#28304;&#24320;&#21457;&#36890;&#36807;&#20419;&#36827;&#21327;&#20316;&#12289;&#36879;&#26126;&#24615;&#21644;&#31038;&#21306;&#39537;&#21160;&#30340;&#21019;&#26032;&#65292;&#24443;&#24213;&#25913;&#21464;&#20102;&#36719;&#20214;&#34892;&#19994;&#12290;&#22914;&#20170;&#65292;&#22823;&#37327;&#21508;&#31181;&#31867;&#22411;&#30340;&#24320;&#28304;&#36719;&#20214;&#65292;&#24418;&#25104;&#20102;&#32593;&#32476;&#23384;&#20648;&#24211;&#65292;&#36890;&#24120;&#25176;&#31649;&#22312;GitHub&#19978;-&#19968;&#31181;&#27969;&#34892;&#30340;&#36719;&#20214;&#24320;&#21457;&#24179;&#21488;&#12290;&#20026;&#20102;&#22686;&#24378;&#23384;&#20648;&#24211;&#32593;&#32476;&#30340;&#21487;&#21457;&#29616;&#24615;&#65292;&#21363;&#30456;&#20284;&#23384;&#20648;&#24211;&#32452;&#65292;GitHub&#22312;2017&#24180;&#24341;&#20837;&#20102;&#23384;&#20648;&#24211;&#20027;&#39064;&#65292;&#20351;&#29992;&#25143;&#26356;&#23481;&#26131;&#25353;&#31867;&#22411;&#12289;&#25216;&#26415;&#31561;&#27983;&#35272;&#30456;&#20851;&#39033;&#30446;&#12290;&#22240;&#27492;&#65292;&#20934;&#30830;&#20026;&#27599;&#20010;GitHub&#23384;&#20648;&#24211;&#20998;&#37197;&#20027;&#39064;&#33267;&#20851;&#37325;&#35201;&#12290;&#30446;&#21069;&#29992;&#20110;&#33258;&#21160;&#20027;&#39064;&#25512;&#33616;&#30340;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;TF-IDF&#26469;&#23545;&#25991;&#26412;&#25968;&#25454;&#36827;&#34892;&#32534;&#30721;&#65292;&#23384;&#22312;&#29702;&#35299;&#35821;&#20041;&#32454;&#24494;&#24046;&#21035;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;Legion&#65292;&#19968;&#31181;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PTMs&#65289;&#25512;&#33616;&#20027;&#39064;&#30340;&#26032;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#25216;&#26415;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05873v1 Announce Type: cross  Abstract: Open-source development has revolutionized the software industry by promoting collaboration, transparency, and community-driven innovation. Today, a vast amount of various kinds of open-source software, which form networks of repositories, is often hosted on GitHub - a popular software development platform. To enhance the discoverability of the repository networks, i.e., groups of similar repositories, GitHub introduced repository topics in 2017 that enable users to more easily explore relevant projects by type, technology, and more. It is thus crucial to accurately assign topics for each GitHub repository. Current methods for automatic topic recommendation rely heavily on TF-IDF for encoding textual data, presenting challenges in understanding semantic nuances. This paper addresses the limitations of existing techniques by proposing Legion, a novel approach that leverages Pre-trained Language Models (PTMs) for recommending topics for 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#35780;&#20272;&#26694;&#26550; CFaiRLLM&#65292;&#26088;&#22312;&#35780;&#20272;&#21644;&#20943;&#36731; RecLLMs &#20013;&#28040;&#36153;&#32773;&#31471;&#30340;&#20559;&#35265;</title><link>https://arxiv.org/abs/2403.05668</link><description>&lt;p&gt;
CFaiRLLM&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#28040;&#36153;&#32773;&#20844;&#24179;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
CFaiRLLM: Consumer Fairness Evaluation in Large-Language Model Recommender System
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05668
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#35780;&#20272;&#26694;&#26550; CFaiRLLM&#65292;&#26088;&#22312;&#35780;&#20272;&#21644;&#20943;&#36731; RecLLMs &#20013;&#28040;&#36153;&#32773;&#31471;&#30340;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25512;&#33616;&#31995;&#32479;&#19981;&#26029;&#21457;&#23637;&#30340;&#36807;&#31243;&#20013;&#65292;&#20687;ChatGPT&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25972;&#21512;&#26631;&#24535;&#30528;&#24341;&#20837;&#20102;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#33616;&#65288;RecLLM&#65289;&#30340;&#26032;&#26102;&#20195;&#12290;&#34429;&#28982;&#36825;&#20123;&#36827;&#23637;&#25215;&#35834;&#25552;&#20379;&#21069;&#25152;&#26410;&#26377;&#30340;&#20010;&#24615;&#21270;&#21644;&#25928;&#29575;&#65292;&#20294;&#20063;&#24341;&#21457;&#20102;&#23545;&#20844;&#24179;&#24615;&#30340;&#37325;&#35201;&#20851;&#20999;&#65292;&#29305;&#21035;&#26159;&#22312;&#25512;&#33616;&#21487;&#33021;&#26080;&#24847;&#20013;&#32487;&#32493;&#25110;&#25918;&#22823;&#19982;&#25935;&#24863;&#29992;&#25143;&#23646;&#24615;&#30456;&#20851;&#30340;&#20559;&#35265;&#30340;&#24773;&#20917;&#19979;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#35780;&#20272;&#26694;&#26550;CFaiRLLM&#65292;&#26088;&#22312;&#35780;&#20272;&#65288;&#20174;&#32780;&#20943;&#36731;&#65289;RecLLMs&#20013;&#28040;&#36153;&#32773;&#31471;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05668v1 Announce Type: new  Abstract: In the evolving landscape of recommender systems, the integration of Large Language Models (LLMs) such as ChatGPT marks a new era, introducing the concept of Recommendation via LLM (RecLLM). While these advancements promise unprecedented personalization and efficiency, they also bring to the fore critical concerns regarding fairness, particularly in how recommendations might inadvertently perpetuate or amplify biases associated with sensitive user attributes. In order to address these concerns, our study introduces a comprehensive evaluation framework, CFaiRLLM, aimed at evaluating (and thereby mitigating) biases on the consumer side within RecLLMs.   Our research methodically assesses the fairness of RecLLMs by examining how recommendations might vary with the inclusion of sensitive attributes such as gender, age, and their intersections, through both similarity alignment and true preference alignment. By analyzing recommendations gener
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#20010;&#24615;&#21270;&#32593;&#39029;&#27178;&#24133;&#65292;&#26681;&#25454;&#29992;&#25143;&#20114;&#21160;&#21160;&#24577;&#20869;&#23481;&#65292;&#24182;&#19988;&#26080;&#38656;&#20154;&#24037;&#24178;&#39044;&#12290;</title><link>https://arxiv.org/abs/2403.05578</link><description>&lt;p&gt;
&#23558;&#25991;&#26412;&#21040;&#22270;&#20687;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20018;&#32852;&#65306;&#29983;&#25104;&#20010;&#24615;&#21270;&#30005;&#23376;&#21830;&#21153;&#27178;&#24133;&#30340;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Chaining text-to-image and large language model: A novel approach for generating personalized e-commerce banners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05578
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#20010;&#24615;&#21270;&#32593;&#39029;&#27178;&#24133;&#65292;&#26681;&#25454;&#29992;&#25143;&#20114;&#21160;&#21160;&#24577;&#20869;&#23481;&#65292;&#24182;&#19988;&#26080;&#38656;&#20154;&#24037;&#24178;&#39044;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05578v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449;&#25688;&#35201;&#65306;&#31283;&#23450;&#25193;&#25955;&#31561;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#20026;&#29983;&#25104;&#33402;&#26415;&#20316;&#21697;&#24320;&#36767;&#20102;&#22823;&#37327;&#26426;&#20250;&#12290;&#26368;&#36817;&#30340;&#25991;&#29486;&#35843;&#26597;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#22312;&#22686;&#24378;&#35768;&#22810;&#21019;&#24847;&#33402;&#26415;&#23478;&#24037;&#20316;&#20013;&#30340;&#24212;&#29992;&#12290;&#35768;&#22810;&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#37319;&#29992;&#25163;&#21160;&#27969;&#31243;&#29983;&#25104;&#27178;&#24133;&#65292;&#36825;&#26159;&#32791;&#26102;&#30340;&#19988;&#23384;&#22312;&#21487;&#25193;&#23637;&#24615;&#30340;&#23616;&#38480;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21033;&#29992;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#26681;&#25454;&#22312;&#32447;&#36141;&#29289;&#32773;&#30340;&#20114;&#21160;&#29983;&#25104;&#20855;&#26377;&#21160;&#24577;&#20869;&#23481;&#30340;&#20010;&#24615;&#21270;&#32593;&#39029;&#27178;&#24133;&#30340;&#29992;&#36884;&#12290;&#27492;&#26041;&#27861;&#30340;&#26032;&#39062;&#20043;&#22788;&#22312;&#20110;&#22312;&#27809;&#26377;&#20154;&#20026;&#24178;&#39044;&#30340;&#24773;&#20917;&#19979;&#23558;&#29992;&#25143;&#20114;&#21160;&#25968;&#25454;&#36716;&#25442;&#20026;&#26377;&#24847;&#20041;&#30340;&#25552;&#31034;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#31995;&#32479;&#22320;&#20174;&#39033;&#30446;&#20803;&#20449;&#24687;&#20013;&#25552;&#21462;&#23646;&#24615;&#20803;&#32452;&#12290;&#28982;&#21518;&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#23558;&#36825;&#20123;&#23646;&#24615;&#20256;&#36882;&#32473;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#65292;&#20197;&#29983;&#25104;&#27178;&#24133;&#30340;&#22270;&#20687;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#21019;&#24314;&#39640;-
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05578v1 Announce Type: cross  Abstract: Text-to-image models such as stable diffusion have opened a plethora of opportunities for generating art. Recent literature has surveyed the use of text-to-image models for enhancing the work of many creative artists. Many e-commerce platforms employ a manual process to generate the banners, which is time-consuming and has limitations of scalability. In this work, we demonstrate the use of text-to-image models for generating personalized web banners with dynamic content for online shoppers based on their interactions. The novelty in this approach lies in converting users' interaction data to meaningful prompts without human intervention. To this end, we utilize a large language model (LLM) to systematically extract a tuple of attributes from item meta-information. The attributes are then passed to a text-to-image model via prompt engineering to generate images for the banner. Our results show that the proposed approach can create high-
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;BERT&#30417;&#27979;&#26497;&#31471;&#31038;&#20132;&#23186;&#20307;&#19978;&#21453;&#29369;&#22826;&#20027;&#20041;&#35805;&#35821;&#28436;&#21464;&#30340;&#33258;&#21160;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#25163;&#21160;&#30417;&#27979;&#30340;&#19981;&#21487;&#34892;&#24615;&#65292;&#20026;&#24178;&#39044;&#21644;&#38450;&#27490;&#20167;&#24680;&#21319;&#32423;&#25552;&#20379;&#20102;&#26032;&#36884;&#24452;&#12290;</title><link>https://arxiv.org/abs/2403.05548</link><description>&lt;p&gt;
&#20351;&#29992;BERT&#30417;&#27979;&#26497;&#31471;&#31038;&#20132;&#23186;&#20307;&#19978;&#21453;&#29369;&#22826;&#20027;&#20041;&#35805;&#35821;&#30340;&#28436;&#21464;
&lt;/p&gt;
&lt;p&gt;
Monitoring the evolution of antisemitic discourse on extremist social media using BERT
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05548
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;BERT&#30417;&#27979;&#26497;&#31471;&#31038;&#20132;&#23186;&#20307;&#19978;&#21453;&#29369;&#22826;&#20027;&#20041;&#35805;&#35821;&#28436;&#21464;&#30340;&#33258;&#21160;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#25163;&#21160;&#30417;&#27979;&#30340;&#19981;&#21487;&#34892;&#24615;&#65292;&#20026;&#24178;&#39044;&#21644;&#38450;&#27490;&#20167;&#24680;&#21319;&#32423;&#25552;&#20379;&#20102;&#26032;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#34920;&#26126;&#65292;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#31181;&#26063;&#20027;&#20041;&#21644;&#19981;&#23485;&#23481;&#26377;&#21487;&#33021;&#22312;&#32447;&#19979;&#20135;&#29983;&#20167;&#24680;&#65292;&#26368;&#32456;&#23548;&#33268;&#36523;&#20307;&#26292;&#21147;&#12290;&#26412;&#30740;&#31350;&#32771;&#34385;&#30340;&#26159;&#22312;&#32447;&#21453;&#29369;&#20027;&#20041;&#65292;&#36861;&#36394;&#22312;&#32447;&#35752;&#35770;&#20013;&#30340;&#21453;&#29369;&#20027;&#39064;&#21450;&#20854;&#30456;&#20851;&#26415;&#35821;&#30340;&#28436;&#21464;&#65292;&#26377;&#21161;&#20110;&#30417;&#27979;&#21442;&#19982;&#32773;&#30340;&#24773;&#32490;&#21644;&#28436;&#21464;&#65292;&#24182;&#21487;&#33021;&#25552;&#20379;&#24178;&#39044;&#26041;&#27861;&#65292;&#38450;&#27490;&#20167;&#24680;&#21319;&#32423;&#12290;&#37492;&#20110;&#22312;&#32447;&#27969;&#37327;&#24222;&#22823;&#19988;&#19981;&#26029;&#21464;&#21270;&#65292;&#25163;&#21160;&#30417;&#27979;&#35848;&#35805;&#23454;&#38469;&#19978;&#26159;&#19981;&#29616;&#23454;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#26497;&#31471;&#31038;&#20132;&#23186;&#20307;&#20013;&#25552;&#21462;&#21453;&#29369;&#20027;&#39064;&#21644;&#26415;&#35821;&#65292;&#36319;&#36394;&#23427;&#20204;&#30340;&#28436;&#21464;&#12290;&#30001;&#20110;&#30417;&#30563;&#23398;&#20064;&#22312;&#36825;&#26679;&#30340;&#20219;&#21153;&#20013;&#36807;&#20110;&#21463;&#38480;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#22312;&#32447;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05548v1 Announce Type: cross  Abstract: Racism and intolerance on social media contribute to a toxic online environment which may spill offline to foster hatred, and eventually lead to physical violence. That is the case with online antisemitism, the specific category of hatred considered in this study. Tracking antisemitic themes and their associated terminology over time in online discussions could help monitor the sentiments of their participants and their evolution, and possibly offer avenues for intervention that may prevent the escalation of hatred. Due to the large volume and constant evolution of online traffic, monitoring conversations manually is impractical. Instead, we propose an automated method that extracts antisemitic themes and terminology from extremist social media over time and captures their evolution. Since supervised learning would be too limited for such a task, we created an unsupervised online machine learning approach that uses large language model
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;Prompt-Tuning&#36171;&#33021;&#30340;PromptMM&#22810;&#27169;&#24335;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#29992;&#20110;&#31616;&#21270;&#21644;&#22686;&#24378;&#25512;&#33616;&#31995;&#32479;&#65292;&#23454;&#29616;&#33258;&#36866;&#24212;&#30340;&#36136;&#37327;&#33976;&#39311;&#12290;</title><link>https://arxiv.org/abs/2402.17188</link><description>&lt;p&gt;
PromptMM&#65306;&#22810;&#27169;&#24335;&#30693;&#35782;&#33976;&#39311;&#29992;&#20110;&#22522;&#20110;Prompt-Tuning&#30340;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
PromptMM: Multi-Modal Knowledge Distillation for Recommendation with Prompt-Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17188
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;Prompt-Tuning&#36171;&#33021;&#30340;PromptMM&#22810;&#27169;&#24335;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#29992;&#20110;&#31616;&#21270;&#21644;&#22686;&#24378;&#25512;&#33616;&#31995;&#32479;&#65292;&#23454;&#29616;&#33258;&#36866;&#24212;&#30340;&#36136;&#37327;&#33976;&#39311;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#23186;&#20307;&#22312;&#32447;&#24179;&#21488;&#65288;&#20363;&#22914;&#20122;&#39532;&#36874;&#12289;TikTok&#65289;&#36890;&#36807;&#23558;&#22810;&#23186;&#20307;&#65288;&#20363;&#22914;&#35270;&#35273;&#12289;&#25991;&#26412;&#21644;&#22768;&#23398;&#65289;&#20869;&#23481;&#32435;&#20837;&#20854;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#20013;&#33719;&#30410;&#21290;&#27973;&#12290;&#36825;&#20123;&#27169;&#24577;&#25552;&#20379;&#30452;&#35266;&#35821;&#20041;&#65292;&#26377;&#21161;&#20110;&#36827;&#34892;&#27169;&#24577;&#24863;&#30693;&#30340;&#29992;&#25143;&#20559;&#22909;&#24314;&#27169;&#12290;&#28982;&#32780;&#65292;&#22810;&#27169;&#24335;&#25512;&#33616;&#22120;&#20013;&#23384;&#22312;&#20004;&#20010;&#20851;&#38190;&#25361;&#25112;&#23578;&#26410;&#35299;&#20915;&#65306;i&#65289;&#24341;&#20837;&#20855;&#26377;&#22823;&#37327;&#39069;&#22806;&#21442;&#25968;&#30340;&#22810;&#27169;&#24335;&#32534;&#30721;&#22120;&#20250;&#23548;&#33268;&#36807;&#25311;&#21512;&#65292;&#32771;&#34385;&#21040;&#25552;&#21462;&#22120;&#65288;&#20363;&#22914;ViT&#12289;BERT&#65289;&#25552;&#20379;&#30340;&#39640;&#32500;&#22810;&#27169;&#24335;&#29305;&#24449;&#12290;ii&#65289;&#36741;&#21161;&#20449;&#24687;&#19981;&#21487;&#36991;&#20813;&#22320;&#24341;&#20837;&#19981;&#20934;&#30830;&#24615;&#21644;&#20887;&#20313;&#65292;&#23548;&#33268;&#27169;&#24577;&#20132;&#20114;&#20381;&#36182;&#20559;&#31163;&#30495;&#23454;&#29992;&#25143;&#20559;&#22909;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;Prompt-Tuning&#36171;&#33021;&#12289;&#31616;&#21270;&#25512;&#33616;&#22120;&#30340;PromptMM&#65288;&#22810;&#27169;&#24335;&#30693;&#35782;&#33976;&#39311;&#65289;&#65292;&#23454;&#29616;&#33258;&#36866;&#24212;&#36136;&#37327;&#33976;&#39311;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17188v1 Announce Type: new  Abstract: Multimedia online platforms (e.g., Amazon, TikTok) have greatly benefited from the incorporation of multimedia (e.g., visual, textual, and acoustic) content into their personal recommender systems. These modalities provide intuitive semantics that facilitate modality-aware user preference modeling. However, two key challenges in multi-modal recommenders remain unresolved: i) The introduction of multi-modal encoders with a large number of additional parameters causes overfitting, given high-dimensional multi-modal features provided by extractors (e.g., ViT, BERT). ii) Side information inevitably introduces inaccuracies and redundancies, which skew the modality-interaction dependency from reflecting true user preference. To tackle these problems, we propose to simplify and empower recommenders through Multi-modal Knowledge Distillation (PromptMM) with the prompt-tuning that enables adaptive quality distillation. Specifically, PromptMM cond
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#24335;&#36879;&#26126;&#30340;&#39034;&#24207;&#25512;&#33616;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#39033;&#30446;&#24207;&#21015;&#20998;&#35299;&#20026;&#22810;&#32423;&#27169;&#24335;&#24182;&#22312;&#27010;&#29575;&#31354;&#38388;&#20013;&#37327;&#21270;&#27599;&#20010;&#27169;&#24335;&#23545;&#32467;&#26524;&#30340;&#36129;&#29486;&#65292;&#23454;&#29616;&#20102;&#36879;&#26126;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2402.11480</link><description>&lt;p&gt;
&#27169;&#24335;&#36879;&#26126;&#30340;&#39034;&#24207;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Pattern-wise Transparent Sequential Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11480
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#24335;&#36879;&#26126;&#30340;&#39034;&#24207;&#25512;&#33616;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#39033;&#30446;&#24207;&#21015;&#20998;&#35299;&#20026;&#22810;&#32423;&#27169;&#24335;&#24182;&#22312;&#27010;&#29575;&#31354;&#38388;&#20013;&#37327;&#21270;&#27599;&#20010;&#27169;&#24335;&#23545;&#32467;&#26524;&#30340;&#36129;&#29486;&#65292;&#23454;&#29616;&#20102;&#36879;&#26126;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36879;&#26126;&#30340;&#20915;&#31574;&#36807;&#31243;&#23545;&#20110;&#24320;&#21457;&#21487;&#38752;&#21644;&#20540;&#24471;&#20449;&#36182;&#30340;&#25512;&#33616;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#12290;&#23545;&#20110;&#39034;&#24207;&#25512;&#33616;&#26469;&#35828;&#65292;&#24847;&#21619;&#30528;&#27169;&#22411;&#33021;&#22815;&#35782;&#21035;&#20851;&#38190;&#39033;&#30446;&#20316;&#20026;&#20854;&#25512;&#33616;&#32467;&#26524;&#30340;&#29702;&#30001;&#12290;&#28982;&#32780;&#65292;&#21516;&#26102;&#23454;&#29616;&#27169;&#22411;&#36879;&#26126;&#24230;&#21644;&#25512;&#33616;&#24615;&#33021;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#23558;&#25972;&#20010;&#39033;&#30446;&#24207;&#21015;&#20316;&#20026;&#36755;&#20837;&#32780;&#19981;&#21152;&#31579;&#36873;&#30340;&#27169;&#22411;&#32780;&#35328;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PTSR&#30340;&#21487;&#35299;&#37322;&#26694;&#26550;&#65292;&#23427;&#23454;&#29616;&#20102;&#19968;&#31181;&#27169;&#24335;&#36879;&#26126;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;&#23427;&#23558;&#39033;&#30446;&#24207;&#21015;&#20998;&#35299;&#20026;&#22810;&#32423;&#27169;&#24335;&#65292;&#36825;&#20123;&#27169;&#24335;&#20316;&#20026;&#25972;&#20010;&#25512;&#33616;&#36807;&#31243;&#30340;&#21407;&#23376;&#21333;&#20803;&#12290;&#27599;&#20010;&#27169;&#24335;&#23545;&#32467;&#26524;&#30340;&#36129;&#29486;&#22312;&#27010;&#29575;&#31354;&#38388;&#20013;&#24471;&#21040;&#37327;&#21270;&#12290;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#27169;&#24335;&#21152;&#26435;&#26657;&#27491;&#65292;&#21363;&#20351;&#22312;&#27809;&#26377;&#30495;&#23454;&#20851;&#38190;&#27169;&#24335;&#30340;&#24773;&#20917;&#19979;&#65292;&#20063;&#33021;&#23398;&#20064;&#27169;&#24335;&#30340;&#36129;&#29486;&#12290;&#26368;&#32456;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11480v1 Announce Type: new  Abstract: A transparent decision-making process is essential for developing reliable and trustworthy recommender systems. For sequential recommendation, it means that the model can identify critical items asthe justifications for its recommendation results. However, achieving both model transparency and recommendation performance simultaneously is challenging, especially for models that take the entire sequence of items as input without screening. In this paper,we propose an interpretable framework (named PTSR) that enables a pattern-wise transparent decision-making process. It breaks the sequence of items into multi-level patterns that serve as atomic units for the entire recommendation process. The contribution of each pattern to the outcome is quantified in the probability space. With a carefully designed pattern weighting correction, the pattern contribution can be learned in the absence of ground-truth critical patterns. The final recommended
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#36890;&#36807;&#20132;&#21449;&#27169;&#22411;&#27604;&#36739;&#25439;&#22833;&#30340;&#26041;&#27861;&#26469;&#22686;&#24378;&#35821;&#35328;&#29702;&#35299;&#27169;&#22411;&#20013;&#31070;&#32463;&#20803;&#30340;&#25928;&#29992;&#65292;&#23454;&#29616;&#20943;&#23569;&#20887;&#20313;&#21442;&#25968;&#21644;&#25233;&#21046;&#36755;&#20837;&#22122;&#22768;&#30340;&#30446;&#26631;&#12290;</title><link>https://arxiv.org/abs/2301.03765</link><description>&lt;p&gt;
&#36890;&#36807;&#20132;&#21449;&#27169;&#22411;&#27604;&#36739;&#25439;&#22833;&#22686;&#24378;&#35821;&#35328;&#29702;&#35299;&#20013;&#31070;&#32463;&#20803;&#25928;&#29992;
&lt;/p&gt;
&lt;p&gt;
Cross-Model Comparative Loss for Enhancing Neuronal Utility in Language Understanding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2301.03765
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#36890;&#36807;&#20132;&#21449;&#27169;&#22411;&#27604;&#36739;&#25439;&#22833;&#30340;&#26041;&#27861;&#26469;&#22686;&#24378;&#35821;&#35328;&#29702;&#35299;&#27169;&#22411;&#20013;&#31070;&#32463;&#20803;&#30340;&#25928;&#29992;&#65292;&#23454;&#29616;&#20943;&#23569;&#20887;&#20313;&#21442;&#25968;&#21644;&#25233;&#21046;&#36755;&#20837;&#22122;&#22768;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65288;NLU&#65289;&#27169;&#22411;&#22312;&#27169;&#22411;&#35268;&#27169;&#21644;&#36755;&#20837;&#32972;&#26223;&#26041;&#38754;&#19981;&#26029;&#25193;&#22823;&#65292;&#24341;&#20837;&#20102;&#26356;&#22810;&#38544;&#34255;&#31070;&#32463;&#20803;&#21644;&#36755;&#20837;&#31070;&#32463;&#20803;&#65292;&#22823;&#20307;&#19978;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#39069;&#22806;&#30340;&#31070;&#32463;&#20803;&#24182;&#19981;&#33021;&#20026;&#25152;&#26377;&#23454;&#20363;&#24102;&#26469;&#19968;&#33268;&#30340;&#25913;&#36827;&#65292;&#22240;&#20026;&#19968;&#20123;&#38544;&#34255;&#31070;&#32463;&#20803;&#26159;&#20887;&#20313;&#30340;&#65292;&#28151;&#20837;&#36755;&#20837;&#31070;&#32463;&#20803;&#30340;&#22122;&#22768;&#24448;&#24448;&#20250;&#20998;&#25955;&#27169;&#22411;&#30340;&#27880;&#24847;&#21147;&#12290;&#20043;&#21069;&#30340;&#24037;&#20316;&#20027;&#35201;&#20391;&#37325;&#20110;&#36890;&#36807;&#38468;&#21152;&#30340;&#21518;&#22788;&#29702;&#25110;&#39044;&#22788;&#29702;&#65292;&#22914;&#32593;&#32476;&#20462;&#21098;&#21644;&#19978;&#19979;&#25991;&#36873;&#25321;&#65292;&#20174;&#22806;&#37096;&#38477;&#20302;&#20302;&#25928;&#31070;&#32463;&#20803;&#30340;&#25968;&#37327;&#65292;&#20197;&#36991;&#20813;&#36825;&#20010;&#38382;&#39064;&#12290;&#38500;&#27492;&#20043;&#22806;&#65292;&#25105;&#20204;&#26159;&#21542;&#21487;&#20197;&#36890;&#36807;&#22686;&#24378;&#27599;&#20010;&#31070;&#32463;&#20803;&#30340;&#25928;&#29992;&#26469;&#20351;&#27169;&#22411;&#20943;&#23569;&#20887;&#20313;&#21442;&#25968;&#24182;&#25233;&#21046;&#36755;&#20837;&#22122;&#22768;&#65311;&#22914;&#26524;&#19968;&#20010;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#22320;&#21033;&#29992;&#31070;&#32463;&#20803;&#65292;&#37027;&#20040;&#19981;&#31649;&#21738;&#20123;&#31070;&#32463;&#20803;&#34987;&#21093;&#31163;&#65288;&#31105;&#29992;&#65289;&#65292;&#21093;&#31163;&#21518;&#30340;&#23376;&#27169;&#22411;&#30340;&#24615;&#33021;&#37117;&#19981;&#24212;&#35813;&#20248;&#20110;&#21407;&#22987;&#23436;&#25972;&#27169;&#22411;&#12290;&#26681;&#25454;&#36825;&#26679;&#30340;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
arXiv:2301.03765v2 Announce Type: replace  Abstract: Current natural language understanding (NLU) models have been continuously scaling up, both in terms of model size and input context, introducing more hidden and input neurons. While this generally improves performance on average, the extra neurons do not yield a consistent improvement for all instances. This is because some hidden neurons are redundant, and the noise mixed in input neurons tends to distract the model. Previous work mainly focuses on extrinsically reducing low-utility neurons by additional post- or pre-processing, such as network pruning and context selection, to avoid this problem. Beyond that, can we make the model reduce redundant parameters and suppress input noise by intrinsically enhancing the utility of each neuron? If a model can efficiently utilize neurons, no matter which neurons are ablated (disabled), the ablated submodel should perform no better than the original full model. Based on such a comparison pr
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;DemiNet&#27169;&#22411;&#65292;&#36890;&#36807;&#20381;&#36182;&#24863;&#30693;&#24322;&#26500;&#27880;&#24847;&#21147;&#12289;&#33258;&#30417;&#30563;&#20852;&#36259;&#23398;&#20064;&#21644;&#20852;&#36259;&#19987;&#23478;&#36827;&#34892;&#35780;&#20998;&#65292;&#26174;&#33879;&#25552;&#39640;&#28857;&#20987;&#29575;&#39044;&#27979;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2109.12512</link><description>&lt;p&gt;
DemiNet: &#22522;&#20110;&#33258;&#30417;&#30563;&#22270;&#23398;&#20064;&#30340;&#20381;&#36182;&#24863;&#30693;&#22810;&#20852;&#36259;&#32593;&#32476;&#29992;&#20110;&#28857;&#20987;&#29575;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
DemiNet: Dependency-Aware Multi-Interest Network with Self-Supervised Graph Learning for Click-Through Rate Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2109.12512
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;DemiNet&#27169;&#22411;&#65292;&#36890;&#36807;&#20381;&#36182;&#24863;&#30693;&#24322;&#26500;&#27880;&#24847;&#21147;&#12289;&#33258;&#30417;&#30563;&#20852;&#36259;&#23398;&#20064;&#21644;&#20852;&#36259;&#19987;&#23478;&#36827;&#34892;&#35780;&#20998;&#65292;&#26174;&#33879;&#25552;&#39640;&#28857;&#20987;&#29575;&#39044;&#27979;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DemiNet&#65288;DEpendency-Aware Multi-Interest Network&#30340;&#32553;&#20889;&#65289;&#30340;&#26032;&#22411;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;&#19978;&#36848;&#20004;&#20010;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#32771;&#34385;&#29289;&#21697;&#33410;&#28857;&#20043;&#38388;&#30340;&#21508;&#31181;&#20381;&#36182;&#31867;&#22411;&#65292;&#24182;&#20026;&#21435;&#22122;&#21644;&#33719;&#21462;&#20934;&#30830;&#30340;&#24207;&#21015;&#29289;&#21697;&#34920;&#31034;&#36827;&#34892;&#20381;&#36182;&#24863;&#30693;&#24322;&#26500;&#27880;&#24847;&#21147;&#12290;&#20854;&#27425;&#65292;&#20026;&#20102;&#25552;&#21462;&#22810;&#20010;&#20852;&#36259;&#65292;&#25105;&#20204;&#22312;&#22270;&#23884;&#20837;&#20043;&#19978;&#36827;&#34892;&#22810;&#22836;&#27880;&#24847;&#21147;&#25805;&#20316;&#12290;&#20026;&#20102;&#28388;&#38500;&#22024;&#26434;&#30340;&#29289;&#21697;&#38388;&#30456;&#20851;&#24615;&#24182;&#22686;&#24378;&#25552;&#21462;&#20852;&#36259;&#30340;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#23558;&#33258;&#30417;&#30563;&#20852;&#36259;&#23398;&#20064;&#24341;&#20837;&#21040;&#19978;&#36848;&#20004;&#20010;&#27493;&#39588;&#20013;&#12290;&#31532;&#19977;&#65292;&#20026;&#20102;&#32858;&#21512;&#22810;&#20010;&#20852;&#36259;&#65292;&#23545;&#24212;&#19981;&#21516;&#20852;&#36259;&#36335;&#32447;&#30340;&#20852;&#36259;&#19987;&#23478;&#20998;&#21035;&#32473;&#20986;&#35780;&#20998;&#65292;&#32780;&#19987;&#38376;&#30340;&#32593;&#32476;&#20998;&#37197;&#27599;&#20010;&#20998;&#25968;&#30340;&#32622;&#20449;&#24230;&#12290;&#22312;&#19977;&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;DemiNet&#26174;&#33879;&#25552;&#39640;&#20102;&#25972;&#20307;&#30340;&#25512;&#33616;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2109.12512v2 Announce Type: replace  Abstract: In this paper, we propose a novel model named DemiNet (short for DEpendency-Aware Multi-Interest Network) to address the above two issues. To be specific, we first consider various dependency types between item nodes and perform dependency-aware heterogeneous attention for denoising and obtaining accurate sequence item representations. Secondly, for multiple interests extraction, multi-head attention is conducted on top of the graph embedding. To filter out noisy inter-item correlations and enhance the robustness of extracted interests, self-supervised interest learning is introduced to the above two steps. Thirdly, to aggregate the multiple interests, interest experts corresponding to different interest routes give rating scores respectively, while a specialized network assigns the confidence of each score. Experimental results on three real-world datasets demonstrate that the proposed DemiNet significantly improves the overall reco
&lt;/p&gt;</description></item><item><title>MuseChat&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#23545;&#35805;&#24335;&#38899;&#20048;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#27169;&#25311;&#29992;&#25143;&#21644;&#25512;&#33616;&#31995;&#32479;&#20043;&#38388;&#30340;&#23545;&#35805;&#20132;&#20114;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#38899;&#20048;&#26631;&#31614;&#21644;&#33402;&#26415;&#23478;&#20449;&#24687;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#23450;&#21046;&#30340;&#38899;&#20048;&#25512;&#33616;&#65292;&#20351;&#29992;&#25143;&#21487;&#20197;&#20010;&#24615;&#21270;&#36873;&#25321;&#20182;&#20204;&#21916;&#27426;&#30340;&#38899;&#20048;&#12290;</title><link>http://arxiv.org/abs/2310.06282</link><description>&lt;p&gt;
MuseChat:&#19968;&#31181;&#35270;&#39057;&#23545;&#35805;&#38899;&#20048;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
MuseChat: A Conversational Music Recommendation System for Videos. (arXiv:2310.06282v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06282
&lt;/p&gt;
&lt;p&gt;
MuseChat&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#23545;&#35805;&#24335;&#38899;&#20048;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#27169;&#25311;&#29992;&#25143;&#21644;&#25512;&#33616;&#31995;&#32479;&#20043;&#38388;&#30340;&#23545;&#35805;&#20132;&#20114;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#38899;&#20048;&#26631;&#31614;&#21644;&#33402;&#26415;&#23478;&#20449;&#24687;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#23450;&#21046;&#30340;&#38899;&#20048;&#25512;&#33616;&#65292;&#20351;&#29992;&#25143;&#21487;&#20197;&#20010;&#24615;&#21270;&#36873;&#25321;&#20182;&#20204;&#21916;&#27426;&#30340;&#38899;&#20048;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;MuseChat&#65292;&#19968;&#31181;&#21019;&#26032;&#30340;&#22522;&#20110;&#23545;&#35805;&#30340;&#38899;&#20048;&#25512;&#33616;&#31995;&#32479;&#12290;&#36825;&#20010;&#29420;&#29305;&#30340;&#24179;&#21488;&#19981;&#20165;&#25552;&#20379;&#20114;&#21160;&#29992;&#25143;&#21442;&#19982;&#65292;&#36824;&#20026;&#36755;&#20837;&#30340;&#35270;&#39057;&#25552;&#20379;&#20102;&#23450;&#21046;&#30340;&#38899;&#20048;&#25512;&#33616;&#65292;&#20351;&#29992;&#25143;&#21487;&#20197;&#25913;&#36827;&#21644;&#20010;&#24615;&#21270;&#20182;&#20204;&#30340;&#38899;&#20048;&#36873;&#25321;&#12290;&#19982;&#20043;&#30456;&#21453;&#65292;&#20197;&#21069;&#30340;&#31995;&#32479;&#20027;&#35201;&#24378;&#35843;&#20869;&#23481;&#30340;&#20860;&#23481;&#24615;&#65292;&#24448;&#24448;&#24573;&#35270;&#20102;&#29992;&#25143;&#20010;&#20307;&#20559;&#22909;&#30340;&#32454;&#24494;&#24046;&#21035;&#12290;&#20363;&#22914;&#65292;&#25152;&#26377;&#30340;&#25968;&#25454;&#38598;&#37117;&#21482;&#25552;&#20379;&#22522;&#26412;&#30340;&#38899;&#20048;-&#35270;&#39057;&#37197;&#23545;&#65292;&#25110;&#32773;&#24102;&#26377;&#38899;&#20048;&#25551;&#36848;&#30340;&#37197;&#23545;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#19977;&#20010;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#23545;&#35805;&#21512;&#25104;&#26041;&#27861;&#65292;&#27169;&#25311;&#20102;&#29992;&#25143;&#21644;&#25512;&#33616;&#31995;&#32479;&#20043;&#38388;&#30340;&#20004;&#36718;&#20132;&#20114;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#38899;&#20048;&#26631;&#31614;&#21644;&#33402;&#26415;&#23478;&#20449;&#24687;&#12290;&#22312;&#36825;&#20010;&#20132;&#20114;&#20013;&#65292;&#29992;&#25143;&#25552;&#20132;&#19968;&#20010;&#35270;&#39057;&#32473;&#31995;&#32479;&#65292;&#31995;&#32479;&#20250;&#25552;&#20379;&#19968;&#20010;&#21512;&#36866;&#30340;&#38899;&#20048;&#29255;&#27573;&#65292;&#24182;&#38468;&#24102;&#35299;&#37322;&#12290;&#20043;&#21518;&#65292;&#29992;&#25143;&#20250;&#34920;&#36798;&#20182;&#20204;&#23545;&#38899;&#20048;&#30340;&#20559;&#22909;&#65292;&#31995;&#32479;&#20250;&#21576;&#29616;&#19968;&#20010;&#25913;&#36827;&#21518;&#30340;&#38899;&#20048;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
We introduce MuseChat, an innovative dialog-based music recommendation system. This unique platform not only offers interactive user engagement but also suggests music tailored for input videos, so that users can refine and personalize their music selections. In contrast, previous systems predominantly emphasized content compatibility, often overlooking the nuances of users' individual preferences. For example, all the datasets only provide basic music-video pairings or such pairings with textual music descriptions. To address this gap, our research offers three contributions. First, we devise a conversation-synthesis method that simulates a two-turn interaction between a user and a recommendation system, which leverages pre-trained music tags and artist information. In this interaction, users submit a video to the system, which then suggests a suitable music piece with a rationale. Afterwards, users communicate their musical preferences, and the system presents a refined music recomme
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;CoHeat&#31639;&#27861;&#65292;&#19968;&#31181;&#20934;&#30830;&#30340;&#20919;&#21551;&#21160;&#25414;&#32465;&#25512;&#33616;&#26041;&#27861;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#32467;&#21512;&#21382;&#21490;&#21644;&#20851;&#32852;&#20449;&#24687;&#65292;&#24212;&#23545;&#25414;&#32465;&#20114;&#21160;&#20998;&#24067;&#30340;&#20542;&#26012;&#65292;&#24182;&#26377;&#25928;&#22320;&#23398;&#20064;&#28508;&#22312;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2310.03813</link><description>&lt;p&gt;
&#20934;&#30830;&#30340;&#20919;&#21551;&#21160;&#25414;&#32465;&#25512;&#33616;&#65306;&#22522;&#20110;&#27969;&#34892;&#24230;&#30340;&#32858;&#21512;&#21644;&#35838;&#31243;&#21152;&#28909;
&lt;/p&gt;
&lt;p&gt;
Accurate Cold-start Bundle Recommendation via Popularity-based Coalescence and Curriculum Heating. (arXiv:2310.03813v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03813
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;CoHeat&#31639;&#27861;&#65292;&#19968;&#31181;&#20934;&#30830;&#30340;&#20919;&#21551;&#21160;&#25414;&#32465;&#25512;&#33616;&#26041;&#27861;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#32467;&#21512;&#21382;&#21490;&#21644;&#20851;&#32852;&#20449;&#24687;&#65292;&#24212;&#23545;&#25414;&#32465;&#20114;&#21160;&#20998;&#24067;&#30340;&#20542;&#26012;&#65292;&#24182;&#26377;&#25928;&#22320;&#23398;&#20064;&#28508;&#22312;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20309;&#20934;&#30830;&#22320;&#21521;&#29992;&#25143;&#25512;&#33616;&#20919;&#21551;&#21160;&#25414;&#32465;&#65311;&#25414;&#32465;&#25512;&#33616;&#20013;&#30340;&#20919;&#21551;&#21160;&#38382;&#39064;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#26032;&#24314;&#25414;&#32465;&#19981;&#26029;&#20986;&#29616;&#20197;&#28385;&#36275;&#21508;&#31181;&#33829;&#38144;&#30446;&#30340;&#12290;&#23613;&#31649;&#20854;&#37325;&#35201;&#24615;&#65292;&#20043;&#21069;&#27809;&#26377;&#30740;&#31350;&#28041;&#21450;&#20919;&#21551;&#21160;&#25414;&#32465;&#25512;&#33616;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;&#20919;&#21551;&#21160;&#29289;&#21697;&#25512;&#33616;&#26041;&#27861;&#36807;&#20110;&#20381;&#36182;&#21382;&#21490;&#20449;&#24687;&#65292;&#21363;&#20351;&#23545;&#20110;&#19981;&#21463;&#27426;&#36814;&#30340;&#25414;&#32465;&#20063;&#26159;&#22914;&#27492;&#65292;&#26080;&#27861;&#24212;&#23545;&#25414;&#32465;&#20114;&#21160;&#20998;&#24067;&#39640;&#24230;&#20542;&#26012;&#30340;&#20027;&#35201;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CoHeat&#65288;&#22522;&#20110;&#27969;&#34892;&#24230;&#30340;&#32858;&#21512;&#21644;&#35838;&#31243;&#21152;&#28909;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#20934;&#30830;&#30340;&#20919;&#21551;&#21160;&#25414;&#32465;&#25512;&#33616;&#26041;&#27861;&#12290;CoHeat&#36890;&#36807;&#32467;&#21512;&#21382;&#21490;&#20449;&#24687;&#21644;&#20851;&#32852;&#20449;&#24687;&#26469;&#20272;&#35745;&#29992;&#25143;&#19982;&#25414;&#32465;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20197;&#24212;&#23545;&#25414;&#32465;&#20114;&#21160;&#20998;&#24067;&#30340;&#39640;&#24230;&#20542;&#26012;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;CoHeat&#36824;&#36890;&#36807;&#21033;&#29992;&#35838;&#31243;&#23398;&#20064;&#21644;&#32858;&#21512;&#29305;&#24449;&#23398;&#20064;&#25928;&#26524;&#22320;&#23398;&#20064;&#28508;&#22312;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
How can we accurately recommend cold-start bundles to users? The cold-start problem in bundle recommendation is critical in practical scenarios since new bundles are continuously created for various marketing purposes. Despite its importance, no previous studies have addressed cold-start bundle recommendation. Moreover, existing methods for cold-start item recommendation overly rely on historical information, even for unpopular bundles, failing to tackle the primary challenge of the highly skewed distribution of bundle interactions. In this work, we propose CoHeat (Popularity-based Coalescence and Curriculum Heating), an accurate approach for the cold-start bundle recommendation. CoHeat tackles the highly skewed distribution of bundle interactions by incorporating both historical and affiliation information based on the bundle's popularity when estimating the user-bundle relationship. Furthermore, CoHeat effectively learns latent representations by exploiting curriculum learning and co
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#39034;&#24207;&#25512;&#33616;&#33539;&#24335; LANCER&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#20041;&#29702;&#35299;&#33021;&#21147;&#29983;&#25104;&#26356;&#21152;&#20154;&#24615;&#21270;&#30340;&#20010;&#24615;&#21270;&#25512;&#33616;&#12290;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#26377;&#25928;&#19988;&#26377;&#24076;&#26395;&#65292;&#24182;&#20026;&#20102;&#35299;&#39034;&#24207;&#25512;&#33616;&#30340;&#24433;&#21709;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2309.10435</link><description>&lt;p&gt;
&#37325;&#22609;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#65306;&#21033;&#29992;&#20869;&#23481;&#22686;&#24378;&#35821;&#35328;&#24314;&#27169;&#23398;&#20064;&#21160;&#24577;&#29992;&#25143;&#20852;&#36259;
&lt;/p&gt;
&lt;p&gt;
Reformulating Sequential Recommendation: Learning Dynamic User Interest with Content-enriched Language Modeling. (arXiv:2309.10435v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10435
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#39034;&#24207;&#25512;&#33616;&#33539;&#24335; LANCER&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#20041;&#29702;&#35299;&#33021;&#21147;&#29983;&#25104;&#26356;&#21152;&#20154;&#24615;&#21270;&#30340;&#20010;&#24615;&#21270;&#25512;&#33616;&#12290;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#26377;&#25928;&#19988;&#26377;&#24076;&#26395;&#65292;&#24182;&#20026;&#20102;&#35299;&#39034;&#24207;&#25512;&#33616;&#30340;&#24433;&#21709;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#23545;&#22312;&#32447;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#65292;&#32780;&#39034;&#24207;&#25512;&#33616;&#30001;&#20110;&#20854;&#34920;&#36798;&#33021;&#21147;&#24378;&#22823;&#65292;&#33021;&#22815;&#25429;&#25417;&#21040;&#21160;&#24577;&#29992;&#25143;&#20852;&#36259;&#32780;&#24191;&#27867;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#39034;&#24207;&#24314;&#27169;&#26041;&#27861;&#22312;&#25429;&#25417;&#19978;&#19979;&#25991;&#20449;&#24687;&#26041;&#38754;&#20173;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#20027;&#35201;&#30340;&#21407;&#22240;&#26159;&#35821;&#35328;&#27169;&#22411;&#24120;&#24120;&#32570;&#20047;&#23545;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#21644;&#29289;&#21697;&#30456;&#20851;&#25991;&#26412;&#20869;&#23481;&#30340;&#29702;&#35299;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#30340;&#39034;&#24207;&#25512;&#33616;&#33539;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;LANCER&#65292;&#23427;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#20041;&#29702;&#35299;&#33021;&#21147;&#29983;&#25104;&#20010;&#24615;&#21270;&#25512;&#33616;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24357;&#21512;&#20102;&#35821;&#35328;&#27169;&#22411;&#19982;&#25512;&#33616;&#31995;&#32479;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#20135;&#29983;&#20102;&#26356;&#21152;&#20154;&#24615;&#21270;&#30340;&#25512;&#33616;&#12290;&#36890;&#36807;&#23545;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#23637;&#31034;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#25105;&#20204;&#27169;&#22411;&#23545;&#39034;&#24207;&#25512;&#33616;&#30340;&#24433;&#21709;&#30340;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommender systems are essential for online applications, and sequential recommendation has enjoyed significant prevalence due to its expressive ability to capture dynamic user interests. However, previous sequential modeling methods still have limitations in capturing contextual information. The primary reason for this issue is that language models often lack an understanding of domain-specific knowledge and item-related textual content. To address this issue, we adopt a new sequential recommendation paradigm and propose LANCER, which leverages the semantic understanding capabilities of pre-trained language models to generate personalized recommendations. Our approach bridges the gap between language models and recommender systems, resulting in more human-like recommendations. We demonstrate the effectiveness of our approach through experiments on several benchmark datasets, showing promising results and providing valuable insights into the influence of our model on sequential recomm
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#20845;&#20010;&#31038;&#21306;&#38382;&#31572;&#24179;&#21488;&#30340;&#30740;&#31350;&#65292;&#21457;&#29616;&#20102;&#26597;&#35810;&#30340;&#20803;&#25968;&#25454;&#12289;&#38382;&#39064;&#26500;&#25104;&#26041;&#24335;&#21644;&#29992;&#25143;&#20114;&#21160;&#27700;&#24179;&#19982;&#31532;&#19968;&#20010;&#22238;&#31572;&#26102;&#38388;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#24182;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#26597;&#35810;&#26159;&#21542;&#33021;&#22815;&#36805;&#36895;&#33719;&#24471;&#22238;&#31572;&#12290;</title><link>http://arxiv.org/abs/2309.05961</link><description>&lt;p&gt;
&#35780;&#20272;&#28526;&#36215;&#28526;&#33853;&#65306;&#23545;&#19981;&#21516;&#24179;&#21488;&#38388;&#38382;&#31572;&#36235;&#21183;&#30340;&#28145;&#20837;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Ebb and Flow: An In-depth Analysis of Question-Answering Trends across Diverse Platforms. (arXiv:2309.05961v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05961
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#20845;&#20010;&#31038;&#21306;&#38382;&#31572;&#24179;&#21488;&#30340;&#30740;&#31350;&#65292;&#21457;&#29616;&#20102;&#26597;&#35810;&#30340;&#20803;&#25968;&#25454;&#12289;&#38382;&#39064;&#26500;&#25104;&#26041;&#24335;&#21644;&#29992;&#25143;&#20114;&#21160;&#27700;&#24179;&#19982;&#31532;&#19968;&#20010;&#22238;&#31572;&#26102;&#38388;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#24182;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#26597;&#35810;&#26159;&#21542;&#33021;&#22815;&#36805;&#36895;&#33719;&#24471;&#22238;&#31572;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#21306;&#38382;&#31572;&#24179;&#21488;&#22240;&#20854;&#24555;&#36895;&#22238;&#31572;&#29992;&#25143;&#26597;&#35810;&#30340;&#33021;&#21147;&#32780;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#36825;&#20123;&#22238;&#31572;&#36895;&#24230;&#30340;&#24555;&#24930;&#21462;&#20915;&#20110;&#26597;&#35810;&#29305;&#23450;&#21644;&#29992;&#25143;&#30456;&#20851;&#30340;&#22240;&#32032;&#30340;&#32508;&#21512;&#12290;&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#20845;&#20010;&#39640;&#24230;&#27969;&#34892;&#30340;&#31038;&#21306;&#38382;&#31572;&#24179;&#21488;&#65292;&#20998;&#26512;&#20102;&#36825;&#20123;&#22240;&#32032;&#22312;&#20854;&#20013;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#25581;&#31034;&#20102;&#38382;&#39064;&#30340;&#31532;&#19968;&#20010;&#22238;&#31572;&#25152;&#33457;&#36153;&#30340;&#26102;&#38388;&#19982;&#20803;&#25968;&#25454;&#12289;&#38382;&#39064;&#30340;&#26500;&#25104;&#26041;&#24335;&#21644;&#29992;&#25143;&#20043;&#38388;&#30340;&#20114;&#21160;&#27700;&#24179;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#20351;&#29992;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20998;&#26512;&#36825;&#20123;&#20803;&#25968;&#25454;&#21644;&#29992;&#25143;&#20114;&#21160;&#27169;&#24335;&#65292;&#25105;&#20204;&#35797;&#22270;&#39044;&#27979;&#21738;&#20123;&#26597;&#35810;&#23558;&#36805;&#36895;&#33719;&#24471;&#21021;&#22987;&#22238;&#31572;&#12290;
&lt;/p&gt;
&lt;p&gt;
Community Question Answering (CQA) platforms steadily gain popularity as they provide users with fast responses to their queries. The swiftness of these responses is contingent on a mixture of query-specific and user-related elements. This paper scrutinizes these contributing factors within the context of six highly popular CQA platforms, identified through their standout answering speed. Our investigation reveals a correlation between the time taken to yield the first response to a question and several variables: the metadata, the formulation of the questions, and the level of interaction among users. Additionally, by employing conventional machine learning models to analyze these metadata and patterns of user interaction, we endeavor to predict which queries will receive their initial responses promptly.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29289;&#21697;&#23884;&#20837;&#26469;&#32531;&#35299;&#24191;&#21578;&#33829;&#38144;&#39046;&#22495;&#20013;&#20301;&#32622;&#20559;&#24046;&#31232;&#30095;&#24615;&#38382;&#39064;&#30340;&#22238;&#24402;EM&#31639;&#27861;&#21464;&#20307;&#12290;</title><link>http://arxiv.org/abs/2305.13931</link><description>&lt;p&gt;
&#21033;&#29992;&#29289;&#21697;&#23884;&#20837;&#25913;&#36827;&#22312;&#31232;&#30095;&#21644;&#20542;&#26012;&#25968;&#25454;&#38598;&#20013;&#23545;&#20301;&#32622;&#20559;&#24046;&#30340;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Improving position bias estimation against sparse and skewed dataset with item embedding. (arXiv:2305.13931v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13931
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29289;&#21697;&#23884;&#20837;&#26469;&#32531;&#35299;&#24191;&#21578;&#33829;&#38144;&#39046;&#22495;&#20013;&#20301;&#32622;&#20559;&#24046;&#31232;&#30095;&#24615;&#38382;&#39064;&#30340;&#22238;&#24402;EM&#31639;&#27861;&#21464;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23398;&#20064;&#25490;&#21517;&#20013;&#65292;&#20272;&#35745;&#20301;&#32622;&#20559;&#24046;&#26159;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#25361;&#25112;&#12290;&#30005;&#23376;&#21830;&#21153;&#24212;&#29992;&#31243;&#24207;&#20013;&#30340;&#28857;&#20987;&#25968;&#25454;&#65288;&#20363;&#22914;&#24191;&#21578;&#23450;&#20301;&#21644;&#25628;&#32034;&#24341;&#25806;&#65289;&#25552;&#20379;&#20102;&#38544;&#21547;&#20294;&#20016;&#23500;&#30340;&#21453;&#39304;&#65292;&#20197;&#25913;&#36827;&#20010;&#24615;&#21270;&#25490;&#21517;&#12290;&#28982;&#32780;&#65292;&#28857;&#20987;&#25968;&#25454;&#26412;&#36136;&#19978;&#21253;&#25324;&#21508;&#31181;&#20559;&#24046;&#65292;&#20363;&#22914;&#20301;&#32622;&#20559;&#24046;&#12290;&#28857;&#20987;&#24314;&#27169;&#26088;&#22312;&#21435;&#22122;&#26377;&#20559;&#30340;&#28857;&#20987;&#25968;&#25454;&#24182;&#25552;&#21462;&#21487;&#38752;&#30340;&#20449;&#21495;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#38543;&#26426;&#21270;&#32467;&#26524;&#21644;&#22238;&#24402;&#26399;&#26395;&#26368;&#22823;&#21270;&#31639;&#27861;&#26469;&#35299;&#20915;&#20301;&#32622;&#20559;&#24046;&#12290;&#20294;&#26159;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#38656;&#35201;&#21508;&#31181;&#35266;&#23519;&#20540;&#23545;&#65288;&#39033;&#30446;&#12289;&#20301;&#32622;&#65289;&#12290;&#28982;&#32780;&#65292;&#22312;&#24191;&#21578;&#33829;&#38144;&#30340;&#23454;&#38469;&#24773;&#20917;&#19979;&#65292;&#33829;&#38144;&#20154;&#21592;&#32463;&#24120;&#25353;&#22266;&#23450;&#30340;&#39044;&#23450;&#39034;&#24207;&#26174;&#31034;&#24191;&#21578;&#65292;&#20272;&#35745;&#22240;&#27492;&#32780;&#21463;&#21040;&#24433;&#21709;&#12290;&#25105;&#20204;&#23558;&#20301;&#32622;&#20559;&#24046;&#20272;&#35745;&#20013;&#30340;&#65288;&#39033;&#30446;&#12289;&#20301;&#32622;&#65289;&#31232;&#30095;&#24615;&#38382;&#39064;&#20316;&#20026;&#26032;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29289;&#21697;&#23884;&#20837;&#26469;&#32531;&#35299;&#31232;&#30095;&#38382;&#39064;&#30340;&#22238;&#24402;EM&#31639;&#27861;&#21464;&#20307;&#12290;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#38598;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimating position bias is a well-known challenge in Learning to rank (L2R). Click data in e-commerce applications, such as advertisement targeting and search engines, provides implicit but abundant feedback to improve personalized rankings. However, click data inherently include various biases like position bias. Click modeling is aimed at denoising biases in click data and extracting reliable signals. Result Randomization and Regression Expectation-maximization algorithm have been proposed to solve position bias. Both methods require various pairs of observations (item, position). However, in real cases of advertising, marketers frequently display advertisements in a fixed pre-determined order, and estimation suffers from it. We propose this sparsity of (item, position) in position bias estimation as a novel problem, and we propose a variant of the Regression EM algorithm which utilizes item embeddings to alleviate the issue of the sparsity. With a synthetic dataset, we first evalua
&lt;/p&gt;</description></item></channel></rss>