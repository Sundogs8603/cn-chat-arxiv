<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#26412;&#25991;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#20316;&#20026;&#29615;&#22659;(LE)&#20248;&#21270;&#20102;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#25552;&#39640;&#20102;&#29366;&#24577;&#24314;&#27169;&#21644;&#22870;&#21169;&#35774;&#32622;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.16948</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#29992;&#20110;&#29366;&#24577;&#12289;&#22870;&#21169;&#21644;&#21160;&#20316;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning-based Recommender Systems with Large Language Models for State Reward and Action Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16948
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#20316;&#20026;&#29615;&#22659;(LE)&#20248;&#21270;&#20102;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#25552;&#39640;&#20102;&#29366;&#24577;&#24314;&#27169;&#21644;&#22870;&#21169;&#35774;&#32622;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#25512;&#33616;&#31995;&#32479;&#22312;&#20174;&#21382;&#21490;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#20013;&#23398;&#20064;&#20934;&#30830;&#30340;&#19979;&#19968;&#20010;&#29289;&#21697;&#25512;&#33616;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#29616;&#26377;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#33719;&#21462;&#26377;&#25928;&#29992;&#25143;&#21453;&#39304;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#12290;&#26412;&#25991;&#21033;&#29992;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#35843;&#25972;&#20026;&#29615;&#22659;(LE)&#20197;&#22686;&#24378;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#23545;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16948v1 Announce Type: new  Abstract: Reinforcement Learning (RL)-based recommender systems have demonstrated promising performance in meeting user expectations by learning to make accurate next-item recommendations from historical user-item interactions. However, existing offline RL-based sequential recommendation methods face the challenge of obtaining effective user feedback from the environment. Effectively modeling the user state and shaping an appropriate reward for recommendation remains a challenge. In this paper, we leverage language understanding capabilities and adapt large language models (LLMs) as an environment (LE) to enhance RL-based recommenders. The LE is learned from a subset of user-item interaction data, thus reducing the need for large training data, and can synthesise user feedback for offline data by: (i) acting as a state model that produces high quality states that enrich the user representation, and (ii) functioning as a reward model to accurately 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#31895;&#35843;&#20248;&#20316;&#20026;&#19968;&#20010;&#20013;&#38388;&#23398;&#20064;&#38454;&#27573;&#65292;&#36830;&#25509;&#20102;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#65292;&#22312;&#19987;&#39064;&#25991;&#26723;&#26816;&#32034;&#20013;&#26174;&#33879;&#25913;&#21892;&#20102;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.16915</link><description>&lt;p&gt;
&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#31895;&#35843;&#20248;&#30340;&#19987;&#39064;&#25991;&#26723;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Coarse-Tuning for Ad-hoc Document Retrieval Using Pre-trained Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16915
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#31895;&#35843;&#20248;&#20316;&#20026;&#19968;&#20010;&#20013;&#38388;&#23398;&#20064;&#38454;&#27573;&#65292;&#36830;&#25509;&#20102;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#65292;&#22312;&#19987;&#39064;&#25991;&#26723;&#26816;&#32034;&#20013;&#26174;&#33879;&#25913;&#21892;&#20102;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#20013;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM-based IR&#65289;&#36827;&#34892;&#24494;&#35843;&#38656;&#35201;&#23398;&#20064;&#26597;&#35810;&#34920;&#31034;&#21644;&#26597;&#35810;-&#25991;&#26723;&#20851;&#31995;&#65292;&#38500;&#20102;&#19979;&#28216;&#20219;&#21153;&#29305;&#23450;&#30340;&#23398;&#20064;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#31895;&#35843;&#20248;&#20316;&#20026;&#19968;&#20010;&#20013;&#38388;&#23398;&#20064;&#38454;&#27573;&#65292;&#36830;&#25509;&#20102;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#12290;&#36890;&#36807;&#22312;&#31895;&#35843;&#20248;&#23398;&#20064;&#26597;&#35810;&#34920;&#31034;&#21644;&#26597;&#35810;-&#25991;&#26723;&#20851;&#31995;&#65292;&#25105;&#20204;&#26088;&#22312;&#20943;&#23569;&#24494;&#35843;&#30340;&#36127;&#25285;&#65292;&#25552;&#39640;&#19979;&#28216;IR&#20219;&#21153;&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#31895;&#35843;&#20248;&#30340;&#26597;&#35810;-&#25991;&#26723;&#23545;&#39044;&#27979;&#65288;QDPP&#65289;&#65292;&#20854;&#39044;&#27979;&#26597;&#35810;-&#25991;&#26723;&#23545;&#30340;&#36866;&#24403;&#24615;&#12290;&#35780;&#20272;&#23454;&#39564;&#26174;&#31034;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26174;&#33879;&#25913;&#21892;&#20102;&#22235;&#20010;&#19987;&#39064;&#25991;&#26723;&#26816;&#32034;&#25968;&#25454;&#38598;&#20013;&#30340;MRR&#21644;/&#25110;nDCG@5&#12290;&#27492;&#22806;&#65292;&#26597;&#35810;&#39044;&#27979;&#20219;&#21153;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#31895;&#35843;&#20248;&#20419;&#36827;&#20102;&#26597;&#35810;&#34920;&#31034;&#21644;&#26597;&#35810;-&#25991;&#26723;&#20851;&#31995;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16915v1 Announce Type: cross  Abstract: Fine-tuning in information retrieval systems using pre-trained language models (PLM-based IR) requires learning query representations and query-document relations, in addition to downstream task-specific learning. This study introduces coarse-tuning as an intermediate learning stage that bridges pre-training and fine-tuning. By learning query representations and query-document relations in coarse-tuning, we aim to reduce the load of fine-tuning and improve the learning effect of downstream IR tasks. We propose Query-Document Pair Prediction (QDPP) for coarse-tuning, which predicts the appropriateness of query-document pairs. Evaluation experiments show that the proposed method significantly improves MRR and/or nDCG@5 in four ad-hoc document retrieval datasets. Furthermore, the results of the query prediction task suggested that coarse-tuning facilitated learning of query representation and query-document relations.
&lt;/p&gt;</description></item><item><title>GSP&#25552;&#20986;&#20102;&#20840;&#29699;&#22303;&#22756;&#20449;&#24687;&#31995;&#32479;(GloSIS)&#65292;&#26088;&#22312;&#36890;&#36807;&#25968;&#25454;&#21327;&#35843;&#21644;&#20132;&#27969;&#20419;&#36827;&#20840;&#29699;&#21487;&#25345;&#32493;&#22303;&#22320;&#31649;&#29702;&#23454;&#36341;&#65292;&#24182;&#23558;&#35821;&#20041;Web&#20316;&#20026;&#20854;&#25805;&#20316;&#21270;&#30340;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2403.16778</link><description>&lt;p&gt;
GloSIS&#65306;&#20840;&#29699;&#22303;&#22756;&#20449;&#24687;&#31995;&#32479;Web&#26412;&#20307;&#35770;
&lt;/p&gt;
&lt;p&gt;
GloSIS: The Global Soil Information System Web Ontology
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16778
&lt;/p&gt;
&lt;p&gt;
GSP&#25552;&#20986;&#20102;&#20840;&#29699;&#22303;&#22756;&#20449;&#24687;&#31995;&#32479;(GloSIS)&#65292;&#26088;&#22312;&#36890;&#36807;&#25968;&#25454;&#21327;&#35843;&#21644;&#20132;&#27969;&#20419;&#36827;&#20840;&#29699;&#21487;&#25345;&#32493;&#22303;&#22320;&#31649;&#29702;&#23454;&#36341;&#65292;&#24182;&#23558;&#35821;&#20041;Web&#20316;&#20026;&#20854;&#25805;&#20316;&#21270;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#32852;&#21512;&#22269;&#31918;&#39135;&#21644;&#20892;&#19994;&#32452;&#32455;(FAO)&#25104;&#21592;&#25104;&#31435;&#20110;2012&#24180;&#30340;&#20840;&#29699;&#22303;&#22756;&#20249;&#20276;&#20851;&#31995;(GSP)&#26159;&#19968;&#20010;&#20840;&#29699;&#21033;&#30410;&#30456;&#20851;&#32773;&#32593;&#32476;&#65292;&#26088;&#22312;&#20419;&#36827;&#21512;&#29702;&#30340;&#22303;&#22320;&#21644;&#22303;&#22756;&#31649;&#29702;&#23454;&#36341;&#65292;&#23454;&#29616;&#21487;&#25345;&#32493;&#30340;&#19990;&#30028;&#31918;&#39135;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#22303;&#22756;&#35843;&#26597;&#20173;&#28982;&#20027;&#35201;&#26159;&#19968;&#39033;&#22320;&#26041;&#25110;&#21306;&#22495;&#24615;&#27963;&#21160;&#65292;&#21463;&#21040;&#19981;&#21516;&#26041;&#27861;&#21644;&#32422;&#23450;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#25512;&#21160;&#21487;&#25345;&#32493;&#22303;&#22320;&#31649;&#29702;&#23454;&#36341;&#65292;GSP&#36873;&#20030;&#25968;&#25454;&#21327;&#35843;&#21644;&#20132;&#27969;&#20316;&#20026;&#20854;&#20027;&#35201;&#34892;&#21160;&#20043;&#19968;&#12290;&#22312;&#22269;&#38469;&#26631;&#20934;&#21644; &#20808;&#21069;&#30340;&#24037;&#20316;&#30340;&#22522;&#30784;&#19978;&#65292;&#21457;&#23637;&#20102;&#19968;&#20010;&#25913;&#36827;&#30340;&#39046;&#22495;&#27169;&#22411;&#65292;&#25104;&#20026;&#20840;&#29699;&#22303;&#22756;&#20449;&#24687;&#31995;&#32479;(GloSIS)&#30340;&#22522;&#30784;&#12290;&#26412;&#25991;&#36824;&#30830;&#23450;&#20102;&#35821;&#20041;Web&#20316;&#20026;&#25805;&#20316;&#21270;&#39046;&#22495;&#27169;&#22411;&#30340;&#19968;&#20010;&#21487;&#33021;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16778v1 Announce Type: new  Abstract: Established in 2012 by members of the Food and Agriculture Organisation (FAO), the Global Soil Partnership (GSP) is a global network of stakeholders promoting sound land and soil management practices towards a sustainable world food system. However, soil survey largely remains a local or regional activity, bound to heterogeneous methods and conventions. Recognising the relevance of global and trans-national policies towards sustainable land management practices, the GSP elected data harmonisation and exchange as one of its key lines of action. Building upon international standards and previous work towards a global soil data ontology, an improved domain model was eventually developed within the GSP [54], the basis for a Global Soil Information System (GloSIS). This work also identified the Semantic Web as a possible avenue to operationalise the domain model. This article presents the GloSIS web ontology, an implementation of the GloSIS d
&lt;/p&gt;</description></item><item><title>ProCQA&#25968;&#25454;&#38598;&#26159;&#20174;StackOverflow&#31038;&#21306;&#25552;&#21462;&#30340;&#65292;&#20026;&#32534;&#31243;&#38382;&#39064;&#22238;&#31572;&#25552;&#20379;&#20102;&#33258;&#28982;&#32467;&#26500;&#21270;&#30340;&#28151;&#21512;&#27169;&#24577;&#38382;&#31572;&#23545;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#27169;&#24577;-&#19981;&#21487;&#30693;&#30340;&#23545;&#27604;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.16702</link><description>&lt;p&gt;
ProCQA&#65306;&#19968;&#20010;&#29992;&#20110;&#20195;&#30721;&#25628;&#32034;&#30340;&#22823;&#35268;&#27169;&#22522;&#20110;&#31038;&#21306;&#30340;&#32534;&#31243;&#38382;&#39064;&#22238;&#31572;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
ProCQA: A Large-scale Community-based Programming Question Answering Dataset for Code Search
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16702
&lt;/p&gt;
&lt;p&gt;
ProCQA&#25968;&#25454;&#38598;&#26159;&#20174;StackOverflow&#31038;&#21306;&#25552;&#21462;&#30340;&#65292;&#20026;&#32534;&#31243;&#38382;&#39064;&#22238;&#31572;&#25552;&#20379;&#20102;&#33258;&#28982;&#32467;&#26500;&#21270;&#30340;&#28151;&#21512;&#27169;&#24577;&#38382;&#31572;&#23545;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#27169;&#24577;-&#19981;&#21487;&#30693;&#30340;&#23545;&#27604;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#24335;&#20195;&#30721;&#38382;&#31572;&#26088;&#22312;&#23558;&#29992;&#25143;&#22312;&#33258;&#28982;&#35821;&#35328;&#20013;&#30340;&#26597;&#35810;&#19982;&#30456;&#20851;&#20195;&#30721;&#29255;&#27573;&#21305;&#37197;&#12290;&#20808;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#20351;&#29992;&#31934;&#24515;&#35774;&#35745;&#30340;&#21452;&#27169;&#24577;&#21644;&#21333;&#27169;&#24577;&#25968;&#25454;&#38598;&#36827;&#34892;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#20197;&#23545;&#40784;&#25991;&#26412;&#21644;&#20195;&#30721;&#34920;&#31034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;ProCQA&#65292;&#36825;&#26159;&#19968;&#20010;&#20174;StackOverflow&#31038;&#21306;&#20013;&#25552;&#21462;&#30340;&#22823;&#35268;&#27169;&#32534;&#31243;&#38382;&#39064;&#22238;&#31572;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#33258;&#28982;&#32467;&#26500;&#21270;&#30340;&#28151;&#21512;&#27169;&#24577;&#38382;&#31572;&#23545;&#12290;&#20026;&#20102;&#39564;&#35777;&#20854;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#24577;&#19981;&#21487;&#30693;&#30340;&#23545;&#27604;&#35757;&#32451;&#26041;&#27861;&#65292;&#20197;&#25913;&#21892;&#24403;&#21069;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26412;&#21644;&#20195;&#30721;&#34920;&#31034;&#30340;&#23545;&#40784;&#12290;&#19982;&#20808;&#21069;&#20027;&#35201;&#20351;&#29992;&#20174;CodeSearchNet&#20013;&#25552;&#21462;&#30340;&#21452;&#27169;&#24577;&#21644;&#21333;&#27169;&#24577;&#23545;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#24191;&#27867;&#30340;&#20195;&#30721;&#26816;&#32034;&#22522;&#20934;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16702v1 Announce Type: new  Abstract: Retrieval-based code question answering seeks to match user queries in natural language to relevant code snippets. Previous approaches typically rely on pretraining models using crafted bi-modal and uni-modal datasets to align text and code representations. In this paper, we introduce ProCQA, a large-scale programming question answering dataset extracted from the StackOverflow community, offering naturally structured mixed-modal QA pairs. To validate its effectiveness, we propose a modality-agnostic contrastive pre-training approach to improve the alignment of text and code representations of current code language models. Compared to previous models that primarily employ bimodal and unimodal pairs extracted from CodeSearchNet for pre-training, our model exhibits significant performance improvements across a wide range of code retrieval benchmarks.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GraphAug&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#24378;&#22823;&#30340;&#25968;&#25454;&#22686;&#24378;&#31243;&#24207;&#29983;&#25104;&#21435;&#22122;&#30340;&#33258;&#30417;&#30563;&#20449;&#21495;&#65292;&#35299;&#20915;&#20102;&#25512;&#33616;&#31995;&#32479;&#20013;&#23545;&#27604;&#23398;&#20064;&#20013;&#23384;&#22312;&#30340;&#25968;&#25454;&#22122;&#22768;&#21644;GNN&#26550;&#26500;&#24179;&#28369;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.16656</link><description>&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#30340;&#22270;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Graph Augmentation for Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16656
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GraphAug&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#24378;&#22823;&#30340;&#25968;&#25454;&#22686;&#24378;&#31243;&#24207;&#29983;&#25104;&#21435;&#22122;&#30340;&#33258;&#30417;&#30563;&#20449;&#21495;&#65292;&#35299;&#20915;&#20102;&#25512;&#33616;&#31995;&#32479;&#20013;&#23545;&#27604;&#23398;&#20064;&#20013;&#23384;&#22312;&#30340;&#25968;&#25454;&#22122;&#22768;&#21644;GNN&#26550;&#26500;&#24179;&#28369;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Graph augmentation&#19982;&#23545;&#27604;&#23398;&#20064;&#22312;&#25512;&#33616;&#31995;&#32479;&#39046;&#22495;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#33021;&#22815;&#22312;&#26631;&#35760;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#20986;&#23500;&#26377;&#34920;&#29616;&#21147;&#30340;&#29992;&#25143;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#23558;&#29616;&#26377;&#30340;GCL&#27169;&#22411;&#24212;&#29992;&#20110;&#29616;&#23454;&#25512;&#33616;&#29615;&#22659;&#20013;&#23384;&#22312;&#25361;&#25112;&#12290;&#20027;&#35201;&#38382;&#39064;&#26377;&#20004;&#20010;&#12290;&#39318;&#20808;&#65292;&#23545;&#27604;&#23398;&#20064;&#20013;&#24573;&#30053;&#25968;&#25454;&#22122;&#22768;&#21487;&#33021;&#23548;&#33268;&#22024;&#26434;&#30340;&#33258;&#30417;&#30563;&#20449;&#21495;&#65292;&#20174;&#32780;&#38477;&#20302;&#24615;&#33021;&#12290;&#20854;&#27425;&#65292;&#35768;&#22810;&#29616;&#26377;&#30340;GCL&#26041;&#27861;&#20381;&#36182;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26550;&#26500;&#65292;&#36825;&#21487;&#33021;&#30001;&#20110;&#38750;&#33258;&#36866;&#24212;&#20449;&#24687;&#20256;&#36882;&#32780;&#23548;&#33268;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#34987;&#31216;&#20026;GraphAug&#30340;&#21407;&#21017;&#24615;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#24341;&#20837;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#25968;&#25454;&#22686;&#24378;&#31243;&#24207;&#65292;&#29983;&#25104;&#21435;&#22122;&#30340;&#33258;&#30417;&#30563;&#20449;&#21495;&#65292;&#25552;&#21319;&#25512;&#33616;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;GraphAug&#26694;&#26550;&#36824;&#34701;&#20837;&#20102;&#22270;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16656v1 Announce Type: new  Abstract: Graph augmentation with contrastive learning has gained significant attention in the field of recommendation systems due to its ability to learn expressive user representations, even when labeled data is limited. However, directly applying existing GCL models to real-world recommendation environments poses challenges. There are two primary issues to address. Firstly, the lack of consideration for data noise in contrastive learning can result in noisy self-supervised signals, leading to degraded performance. Secondly, many existing GCL approaches rely on graph neural network (GNN) architectures, which can suffer from over-smoothing problems due to non-adaptive message passing. To address these challenges, we propose a principled framework called GraphAug. This framework introduces a robust data augmentor that generates denoised self-supervised signals, enhancing recommender systems. The GraphAug framework incorporates a graph information 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;&#19981;&#21516;&#31867;&#22411;&#30340;&#19987;&#21033;&#23884;&#20837;&#27169;&#22411;&#22312;&#19987;&#21033;&#30456;&#20284;&#24615;&#35745;&#31639;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#20855;&#20307;&#25506;&#35752;&#20102;Sentence Transformers (SBERT) &#26550;&#26500;&#22312;&#19987;&#21033;&#30456;&#20284;&#24615;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.16630</link><description>&lt;p&gt;
&#19987;&#21033;&#30456;&#20284;&#24615;&#23884;&#20837;&#27169;&#22411;&#30340;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A comparative analysis of embedding models for patent similarity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16630
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;&#19981;&#21516;&#31867;&#22411;&#30340;&#19987;&#21033;&#23884;&#20837;&#27169;&#22411;&#22312;&#19987;&#21033;&#30456;&#20284;&#24615;&#35745;&#31639;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#20855;&#20307;&#25506;&#35752;&#20102;Sentence Transformers (SBERT) &#26550;&#26500;&#22312;&#19987;&#21033;&#30456;&#20284;&#24615;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#22522;&#20110;&#25991;&#26412;&#30340;&#19987;&#21033;&#30456;&#20284;&#24615;&#39046;&#22495;&#20570;&#20986;&#20102;&#20004;&#20010;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#23427;&#27604;&#36739;&#20102;&#19981;&#21516;&#31867;&#22411;&#30340;&#19987;&#21033;&#29305;&#23450;&#39044;&#35757;&#32451;&#23884;&#20837;&#27169;&#22411;&#65288;&#22914;word2vec&#21644;doc2vec&#27169;&#22411;&#65289;&#21644;&#19978;&#19979;&#25991;&#35789;&#23884;&#20837;&#27169;&#22411;&#65288;&#22914;&#22522;&#20110;transformers&#30340;&#27169;&#22411;&#65289;&#22312;&#19987;&#21033;&#30456;&#20284;&#24615;&#35745;&#31639;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;&#20854;&#27425;&#65292;&#23427;&#20855;&#20307;&#27604;&#36739;&#20102;&#20855;&#26377;&#19981;&#21516;&#35757;&#32451;&#38454;&#27573;&#30340;Sentence Transformers&#65288;SBERT&#65289;&#26550;&#26500;&#22312;&#19987;&#21033;&#30456;&#20284;&#24615;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;&#20026;&#35780;&#20272;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#20351;&#29992;&#20851;&#20110;&#19987;&#21033;&#24178;&#28041;&#30340;&#20449;&#24687;&#65292;&#21363;&#20004;&#20010;&#25110;&#22810;&#20010;&#19987;&#21033;&#30003;&#35831;&#20013;&#30340;&#19987;&#21033;&#35201;&#27714;&#34987;&#19987;&#21033;&#23457;&#26597;&#21592;&#35777;&#26126;&#23384;&#22312;&#37325;&#21472;&#30340;&#29616;&#35937;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#24178;&#28041;&#26696;&#20363;&#35270;&#20026;&#20004;&#20010;&#19987;&#21033;&#20043;&#38388;&#30340;&#26368;&#22823;&#30456;&#20284;&#24615;&#30340;&#20195;&#29702;&#65292;&#24182;&#29992;&#23427;&#20204;&#20316;&#20026;&#22522;&#20934;&#26469;&#35780;&#20272;&#19981;&#21516;&#23884;&#20837;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16630v1 Announce Type: new  Abstract: This paper makes two contributions to the field of text-based patent similarity. First, it compares the performance of different kinds of patent-specific pretrained embedding models, namely static word embeddings (such as word2vec and doc2vec models) and contextual word embeddings (such as transformers based models), on the task of patent similarity calculation. Second, it compares specifically the performance of Sentence Transformers (SBERT) architectures with different training phases on the patent similarity task. To assess the models' performance, we use information about patent interferences, a phenomenon in which two or more patent claims belonging to different patent applications are proven to be overlapping by patent examiners. Therefore, we use these interferences cases as a proxy for maximum similarity between two patents, treating them as ground-truth to evaluate the performance of the different embedding models. Our results p
&lt;/p&gt;</description></item><item><title>LARA&#26159;&#19968;&#20010;Linguistic-Adaptive Retrieval-Augmented Language Models&#65288;&#35821;&#35328;&#33258;&#36866;&#24212;&#26816;&#32034;&#22686;&#24378;LLMs&#65289;&#65292;&#26088;&#22312;&#36890;&#36807;&#32467;&#21512;&#24494;&#35843;&#36807;&#30340;&#36739;&#23567;&#27169;&#22411;&#19982;&#26816;&#32034;&#22686;&#24378;&#26426;&#21046;&#26469;&#25552;&#39640;&#22810;&#35821;&#35328;&#22810;&#36718;&#24847;&#22270;&#20998;&#31867;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#65292;&#20174;&#32780;&#25913;&#21892;&#23545;&#35805;&#32972;&#26223;&#30340;&#29702;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.16504</link><description>&lt;p&gt;
LARA&#65306;&#35821;&#35328;&#33258;&#36866;&#24212;&#26816;&#32034;&#22686;&#24378;LLMs&#29992;&#20110;&#22810;&#36718;&#24847;&#22270;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
LARA: Linguistic-Adaptive Retrieval-Augmented LLMs for Multi-Turn Intent Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16504
&lt;/p&gt;
&lt;p&gt;
LARA&#26159;&#19968;&#20010;Linguistic-Adaptive Retrieval-Augmented Language Models&#65288;&#35821;&#35328;&#33258;&#36866;&#24212;&#26816;&#32034;&#22686;&#24378;LLMs&#65289;&#65292;&#26088;&#22312;&#36890;&#36807;&#32467;&#21512;&#24494;&#35843;&#36807;&#30340;&#36739;&#23567;&#27169;&#22411;&#19982;&#26816;&#32034;&#22686;&#24378;&#26426;&#21046;&#26469;&#25552;&#39640;&#22810;&#35821;&#35328;&#22810;&#36718;&#24847;&#22270;&#20998;&#31867;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#65292;&#20174;&#32780;&#25913;&#21892;&#23545;&#35805;&#32972;&#26223;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21462;&#24471;&#30340;&#26174;&#33879;&#25104;&#23601;&#65292;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#22312;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#37319;&#29992;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30740;&#31350;&#20391;&#37325;&#20110;&#21333;&#35821;&#35328;&#12289;&#21333;&#36718;&#20998;&#31867;&#20219;&#21153;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;LARA&#65288;Linguistic-Adaptive Retrieval-Augmented Language Models&#65289;&#65292;&#26088;&#22312;&#22686;&#24378;&#22810;&#35821;&#35328;&#22810;&#36718;&#20998;&#31867;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#65292;&#20197;&#36866;&#24212;&#32842;&#22825;&#26426;&#22120;&#20154;&#20132;&#20114;&#20013;&#30340;&#20247;&#22810;&#24847;&#22270;&#12290;&#30001;&#20110;&#20250;&#35805;&#32972;&#26223;&#30340;&#22797;&#26434;&#24615;&#21644;&#19981;&#26029;&#21457;&#23637;&#30340;&#24615;&#36136;&#65292;&#22810;&#36718;&#24847;&#22270;&#20998;&#31867;&#23588;&#20026;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;LARA&#36890;&#36807;&#23558;&#24494;&#35843;&#36807;&#30340;&#36739;&#23567;&#27169;&#22411;&#19982;&#26816;&#32034;&#22686;&#24378;&#26426;&#21046;&#32467;&#21512;&#65292;&#23884;&#20837;LLMs&#30340;&#26550;&#26500;&#20013;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#36825;&#31181;&#25972;&#21512;&#20351;LARA&#33021;&#22815;&#21160;&#24577;&#21033;&#29992;&#36807;&#21435;&#30340;&#23545;&#35805;&#21644;&#30456;&#20851;&#24847;&#22270;&#65292;&#20174;&#32780;&#25552;&#39640;&#23545;&#19978;&#19979;&#25991;&#30340;&#29702;&#35299;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#33258;&#36866;&#24212;&#26816;&#32034;&#25216;&#26415;&#22686;&#24378;&#20102;&#36328;&#35821;&#35328;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16504v1 Announce Type: new  Abstract: Following the significant achievements of large language models (LLMs), researchers have employed in-context learning for text classification tasks. However, these studies focused on monolingual, single-turn classification tasks. In this paper, we introduce LARA (Linguistic-Adaptive Retrieval-Augmented Language Models), designed to enhance accuracy in multi-turn classification tasks across six languages, accommodating numerous intents in chatbot interactions. Multi-turn intent classification is notably challenging due to the complexity and evolving nature of conversational contexts. LARA tackles these issues by combining a fine-tuned smaller model with a retrieval-augmented mechanism, integrated within the architecture of LLMs. This integration allows LARA to dynamically utilize past dialogues and relevant intents, thereby improving the understanding of the context. Furthermore, our adaptive retrieval techniques bolster the cross-lingual
&lt;/p&gt;</description></item><item><title>InstUPR&#26159;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26080;&#30417;&#30563;&#27573;&#33853;&#37325;&#26032;&#25490;&#24207;&#26041;&#27861;&#65292;&#21033;&#29992;&#20102;LLMs&#30340;&#25351;&#20196;&#36319;&#36394;&#33021;&#21147;&#65292;&#26080;&#38656;&#39069;&#22806;&#24494;&#35843;&#65292;&#36890;&#36807;&#36719;&#24471;&#20998;&#32858;&#21512;&#25216;&#26415;&#21644;&#25104;&#23545;&#37325;&#26032;&#25490;&#24207;&#65292;&#22312;BEIR&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#31168;&#12290;</title><link>https://arxiv.org/abs/2403.16435</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#20110;&#25351;&#20196;&#30340;&#26080;&#30417;&#30563;&#27573;&#33853;&#37325;&#26032;&#25490;&#24207;&#26041;&#27861;InstUPR
&lt;/p&gt;
&lt;p&gt;
InstUPR : Instruction-based Unsupervised Passage Reranking with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16435
&lt;/p&gt;
&lt;p&gt;
InstUPR&#26159;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26080;&#30417;&#30563;&#27573;&#33853;&#37325;&#26032;&#25490;&#24207;&#26041;&#27861;&#65292;&#21033;&#29992;&#20102;LLMs&#30340;&#25351;&#20196;&#36319;&#36394;&#33021;&#21147;&#65292;&#26080;&#38656;&#39069;&#22806;&#24494;&#35843;&#65292;&#36890;&#36807;&#36719;&#24471;&#20998;&#32858;&#21512;&#25216;&#26415;&#21644;&#25104;&#23545;&#37325;&#26032;&#25490;&#24207;&#65292;&#22312;BEIR&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;InstUPR&#65292;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26080;&#30417;&#30563;&#27573;&#33853;&#37325;&#26032;&#25490;&#24207;&#26041;&#27861;&#12290;&#19982;&#29616;&#26377;&#20381;&#36182;&#20110;query-document&#23545;&#36827;&#34892;&#22823;&#37327;&#35757;&#32451;&#25110;&#29305;&#23450;&#20110;&#26816;&#32034;&#30340;&#25351;&#20196;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#32463;&#36807;&#25351;&#20196;&#35843;&#25972;&#30340;LLMs&#30340;&#25353;&#29031;&#25351;&#20196;&#36827;&#34892;&#25805;&#20316;&#30340;&#33021;&#21147;&#26469;&#36827;&#34892;&#27573;&#33853;&#37325;&#26032;&#25490;&#24207;&#65292;&#32780;&#26080;&#38656;&#20219;&#20309;&#39069;&#22806;&#30340;&#24494;&#35843;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#36719;&#24471;&#20998;&#32858;&#21512;&#25216;&#26415;&#65292;&#24182;&#37319;&#29992;&#20102;&#25104;&#23545;&#37325;&#26032;&#25490;&#24207;&#30340;&#26080;&#30417;&#30563;&#27573;&#33853;&#37325;&#26032;&#25490;&#24207;&#12290;&#22312;BEIR&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;InstUPR&#20248;&#20110;&#26080;&#30417;&#30563;&#22522;&#32447;&#20197;&#21450;&#19968;&#20010;&#32463;&#36807;&#25351;&#20196;&#35843;&#25972;&#30340;&#37325;&#26032;&#25490;&#24207;&#22120;&#65292;&#31361;&#26174;&#20102;&#20854;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;&#22797;&#29616;&#25152;&#26377;&#23454;&#39564;&#30340;&#28304;&#20195;&#30721;&#24050;&#22312;https://github.com/MiuLab/InstUPR &#24320;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16435v1 Announce Type: new  Abstract: This paper introduces InstUPR, an unsupervised passage reranking method based on large language models (LLMs). Different from existing approaches that rely on extensive training with query-document pairs or retrieval-specific instructions, our method leverages the instruction-following capabilities of instruction-tuned LLMs for passage reranking without any additional fine-tuning. To achieve this, we introduce a soft score aggregation technique and employ pairwise reranking for unsupervised passage reranking. Experiments on the BEIR benchmark demonstrate that InstUPR outperforms unsupervised baselines as well as an instruction-tuned reranker, highlighting its effectiveness and superiority. Source code to reproduce all experiments is open-sourced at https://github.com/MiuLab/InstUPR
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#32654;&#22269;&#22269;&#20250;&#22270;&#20070;&#39302;&#20027;&#39064;&#26631;&#22836;&#30340;&#28508;&#21147;&#65292;&#23637;&#31034;&#20102;&#20854;&#23545;&#20110;&#35299;&#20915;&#23398;&#26415;&#22270;&#20070;&#39302;&#24453;&#32534;&#30446;&#39033;&#30446;&#31215;&#21387;&#38382;&#39064;&#20855;&#26377;&#25112;&#30053;&#24212;&#23545;&#24847;&#20041;&#65292;&#21516;&#26102;&#20063;&#24378;&#35843;&#20102;&#20154;&#31867;&#32534;&#30446;&#21592;&#20173;&#28982;&#22312;&#39564;&#35777;&#21644;&#22686;&#24378;&#29983;&#25104;&#20027;&#39064;&#26631;&#22836;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.16424</link><description>&lt;p&gt;
&#20351;&#29992;ChatGPT&#20026;&#30005;&#23376;&#23398;&#20301;&#35770;&#25991;&#25351;&#23450;LCSH&#20027;&#39064;&#30340;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
An Experiment with the Use of ChatGPT for LCSH Subject Assignment on Electronic Theses and Dissertations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16424
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#32654;&#22269;&#22269;&#20250;&#22270;&#20070;&#39302;&#20027;&#39064;&#26631;&#22836;&#30340;&#28508;&#21147;&#65292;&#23637;&#31034;&#20102;&#20854;&#23545;&#20110;&#35299;&#20915;&#23398;&#26415;&#22270;&#20070;&#39302;&#24453;&#32534;&#30446;&#39033;&#30446;&#31215;&#21387;&#38382;&#39064;&#20855;&#26377;&#25112;&#30053;&#24212;&#23545;&#24847;&#20041;&#65292;&#21516;&#26102;&#20063;&#24378;&#35843;&#20102;&#20154;&#31867;&#32534;&#30446;&#21592;&#20173;&#28982;&#22312;&#39564;&#35777;&#21644;&#22686;&#24378;&#29983;&#25104;&#20027;&#39064;&#26631;&#22836;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#32654;&#22269;&#22269;&#20250;&#22270;&#20070;&#39302;&#20027;&#39064;&#26631;&#22836;&#65288;LCSH&#65289;&#30340;&#28508;&#21147;&#12290;&#20316;&#32773;&#20351;&#29992;ChatGPT&#26681;&#25454;&#30005;&#23376;&#23398;&#20301;&#35770;&#25991;&#30340;&#26631;&#39064;&#21644;&#25688;&#35201;&#29983;&#25104;&#20027;&#39064;&#26631;&#22836;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#23613;&#31649;&#19968;&#20123;&#29983;&#25104;&#30340;&#20027;&#39064;&#26631;&#22836;&#26159;&#26377;&#25928;&#30340;&#65292;&#20294;&#23384;&#22312;&#29305;&#23450;&#24615;&#21644;&#35814;&#23613;&#24615;&#26041;&#38754;&#30340;&#38382;&#39064;&#12290;&#35813;&#30740;&#31350;&#23637;&#31034;&#20102;LLMs&#21487;&#20197;&#20316;&#20026;&#23398;&#26415;&#22270;&#20070;&#39302;&#24453;&#32534;&#30446;&#39033;&#30446;&#30340;&#25112;&#30053;&#24615;&#24212;&#23545;&#25514;&#26045;&#65292;&#21516;&#26102;&#20063;&#25552;&#20379;&#20102;&#19968;&#31181;&#25104;&#26412;&#25928;&#30410;&#39640;&#19988;&#24555;&#36895;&#29983;&#25104;LCSH&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#32534;&#30446;&#21592;&#20173;&#28982;&#26159;&#39564;&#35777;&#21644;&#22686;&#24378;LLMs&#29983;&#25104;&#30340;LCSH&#30340;&#26377;&#25928;&#24615;&#12289;&#35814;&#23613;&#24615;&#21644;&#29305;&#23450;&#24615;&#30340;&#24517;&#35201;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16424v1 Announce Type: new  Abstract: This study delves into the potential use of Large Language Models (LLMs) for generating Library of Congress Subject Headings (LCSH). The authors employed ChatGPT to generate subject headings for electronic theses and dissertations (ETDs) based on their titles and summaries. The results revealed that although some generated subject headings were valid, there were issues regarding specificity and exhaustiveness. The study showcases that LLMs can serve as a strategic response to the backlog of items awaiting cataloging in academic libraries, while also offering a cost-effective approach for promptly generating LCSH. Nonetheless, human catalogers remain essential for verifying and enhancing the validity, exhaustiveness, and specificity of LCSH generated by LLMs.
&lt;/p&gt;</description></item><item><title>LLMs&#22312;CRM&#34920;&#29616;&#20449;&#24515;&#21644;&#31934;&#30830;&#24615;&#36739;&#20302;&#30340;&#25968;&#25454;&#27573;&#34920;&#29616;&#20248;&#24322;&#65292;&#32780;CRM&#34920;&#29616;&#20248;&#24322;&#30340;&#26679;&#26412;&#23545;LLM&#32780;&#35328;&#30456;&#23545;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#36825;&#34920;&#26126;LLM&#21644;CRM&#20043;&#38388;&#23384;&#22312;&#28508;&#22312;&#30340;&#21327;&#21516;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.16378</link><description>&lt;p&gt;
&#21033;&#29992;&#21508;&#33258;&#20248;&#21183;&#65306;&#20256;&#32479;&#25512;&#33616;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21327;&#20316;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Play to Your Strengths: Collaborative Intelligence of Conventional Recommender Models and Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16378
&lt;/p&gt;
&lt;p&gt;
LLMs&#22312;CRM&#34920;&#29616;&#20449;&#24515;&#21644;&#31934;&#30830;&#24615;&#36739;&#20302;&#30340;&#25968;&#25454;&#27573;&#34920;&#29616;&#20248;&#24322;&#65292;&#32780;CRM&#34920;&#29616;&#20248;&#24322;&#30340;&#26679;&#26412;&#23545;LLM&#32780;&#35328;&#30456;&#23545;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#36825;&#34920;&#26126;LLM&#21644;CRM&#20043;&#38388;&#23384;&#22312;&#28508;&#22312;&#30340;&#21327;&#21516;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#23835;&#36215;&#20026;&#25512;&#33616;&#31995;&#32479;&#65288;RSs&#65289;&#24102;&#26469;&#20102;&#26032;&#30340;&#26426;&#36935;&#65292;&#36890;&#36807;&#22686;&#24378;&#29992;&#25143;&#34892;&#20026;&#24314;&#27169;&#21644;&#20869;&#23481;&#29702;&#35299;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#23558;LLMs&#25972;&#21512;&#21040;RSs&#30340;&#26041;&#27861;&#20165;&#20165;&#21033;&#29992;LLM&#25110;&#20256;&#32479;&#25512;&#33616;&#27169;&#22411;(CRM)&#26469;&#29983;&#25104;&#26368;&#32456;&#25512;&#33616;&#65292;&#32780;&#27809;&#26377;&#32771;&#34385;LLM&#25110;CRM&#22312;&#21738;&#20123;&#25968;&#25454;&#27573;&#34920;&#29616;&#20248;&#31168;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#22312;MovieLens-1M&#21644;Amazon-Books&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#27604;&#36739;&#20102;&#20195;&#34920;&#24615;CRM&#65288;DCNv2&#65289;&#21644;&#19968;&#20010;LLM&#65288;LLaMA2-7B&#65289;&#22312;&#21508;&#31181;&#25968;&#25454;&#26679;&#26412;&#32452;&#19978;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#21457;&#29616;LLMs&#22312;CRM&#34920;&#29616;&#20449;&#24515;&#21644;&#31934;&#30830;&#24615;&#36739;&#20302;&#30340;&#25968;&#25454;&#27573;&#34920;&#29616;&#20248;&#24322;&#65292;&#32780;CRM&#34920;&#29616;&#20248;&#24322;&#30340;&#26679;&#26412;&#23545;LLM&#32780;&#35328;&#30456;&#23545;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#38656;&#35201;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#21644;&#36739;&#38271;&#30340;&#35757;&#32451;&#26102;&#38388;&#25165;&#33021;&#33719;&#24471;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;&#36825;&#34920;&#26126;LLM&#21644;CRM&#20043;&#38388;&#23384;&#22312;&#28508;&#22312;&#30340;&#21327;&#21516;&#20316;&#29992;&#12290;&#21463;&#21040;&#36825;&#20123;&#35265;&#35299;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16378v1 Announce Type: new  Abstract: The rise of large language models (LLMs) has opened new opportunities in Recommender Systems (RSs) by enhancing user behavior modeling and content understanding. However, current approaches that integrate LLMs into RSs solely utilize either LLM or conventional recommender model (CRM) to generate final recommendations, without considering which data segments LLM or CRM excel in. To fill in this gap, we conduct experiments on MovieLens-1M and Amazon-Books datasets, and compare the performance of a representative CRM (DCNv2) and an LLM (LLaMA2-7B) on various groups of data samples. Our findings reveal that LLMs excel in data segments where CRMs exhibit lower confidence and precision, while samples where CRM excels are relatively challenging for LLM, requiring substantial training data and a long training time for comparable performance. This suggests potential synergies in the combination between LLM and CRM. Motivated by these insights, we
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20154;&#21592;&#35843;&#26597;&#20102;&#20855;&#26377;&#36873;&#25321;&#26426;&#21046;&#30340;Mamba&#22312;&#32456;&#36523;&#24207;&#21015;&#25512;&#33616;&#20013;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#26377;&#36873;&#25321;&#22320;&#21033;&#29992;Mamba&#22359;&#26469;&#27169;&#25311;&#29992;&#25143;&#32456;&#36523;&#24207;&#21015;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#27169;&#22411;&#38590;&#20197;&#22788;&#29702;&#32456;&#36523;&#24207;&#21015;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.16371</link><description>&lt;p&gt;
&#22312;&#32456;&#36523;&#24207;&#21015;&#25512;&#33616;&#20013;&#25581;&#31034;&#26377;&#36873;&#25321;&#24615;&#30340;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Uncovering Selective State Space Model's Capabilities in Lifelong Sequential Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16371
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20154;&#21592;&#35843;&#26597;&#20102;&#20855;&#26377;&#36873;&#25321;&#26426;&#21046;&#30340;Mamba&#22312;&#32456;&#36523;&#24207;&#21015;&#25512;&#33616;&#20013;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#26377;&#36873;&#25321;&#22320;&#21033;&#29992;Mamba&#22359;&#26469;&#27169;&#25311;&#29992;&#25143;&#32456;&#36523;&#24207;&#21015;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#27169;&#22411;&#38590;&#20197;&#22788;&#29702;&#32456;&#36523;&#24207;&#21015;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#24050;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#22312;&#32447;&#26381;&#21153;&#20013;&#65292;&#26088;&#22312;&#20174;&#29992;&#25143;&#30340;&#39034;&#24207;&#20132;&#20114;&#20013;&#27169;&#25311;&#20182;&#20204;&#30340;&#21160;&#24577;&#20852;&#36259;&#12290;&#38543;&#30528;&#29992;&#25143;&#36234;&#26469;&#36234;&#22810;&#22320;&#21442;&#19982;&#22312;&#32447;&#24179;&#21488;&#65292;&#29983;&#25104;&#20102;&#22823;&#37327;&#30340;&#32456;&#36523;&#29992;&#25143;&#34892;&#20026;&#24207;&#21015;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#39034;&#24207;&#25512;&#33616;&#27169;&#22411;&#36890;&#24120;&#24456;&#38590;&#22788;&#29702;&#36825;&#26679;&#30340;&#32456;&#36523;&#24207;&#21015;&#12290;&#20027;&#35201;&#25361;&#25112;&#28304;&#20110;&#35745;&#31639;&#22797;&#26434;&#24615;&#21644;&#25429;&#33719;&#24207;&#21015;&#20013;&#38271;&#36317;&#31163;&#20381;&#36182;&#30340;&#33021;&#21147;&#12290;&#26368;&#36817;&#65292;&#19968;&#31181;&#20855;&#26377;&#36873;&#25321;&#26426;&#21046;&#65288;&#21363;Mamba&#65289;&#30340;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#24320;&#22987;&#20986;&#29616;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;Mamba&#22312;&#32456;&#36523;&#24207;&#21015;&#25512;&#33616;&#65288;&#21363;&#38271;&#24230;&gt;=2k&#65289;&#20013;&#30340;&#24615;&#33021;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21033;&#29992;Mamba&#22359;&#26377;&#36873;&#25321;&#22320;&#27169;&#25311;&#32456;&#36523;&#29992;&#25143;&#24207;&#21015;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#35780;&#20272;&#20102;&#20195;&#34920;&#24615;&#39034;&#24207;&#25512;&#33616;&#27169;&#22411;&#22312;&#32456;&#36523;&#24207;&#21015;&#35774;&#32622;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16371v1 Announce Type: new  Abstract: Sequential Recommenders have been widely applied in various online services, aiming to model users' dynamic interests from their sequential interactions. With users increasingly engaging with online platforms, vast amounts of lifelong user behavioral sequences have been generated. However, existing sequential recommender models often struggle to handle such lifelong sequences. The primary challenges stem from computational complexity and the ability to capture long-range dependencies within the sequence. Recently, a state space model featuring a selective mechanism (i.e., Mamba) has emerged. In this work, we investigate the performance of Mamba for lifelong sequential recommendation (i.e., length&gt;=2k). More specifically, we leverage the Mamba block to model lifelong user sequences selectively. We conduct extensive experiments to evaluate the performance of representative sequential recommendation models in the setting of lifelong sequenc
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#25628;&#32034;&#24341;&#25806;&#33719;&#21462;&#30340;&#25991;&#26723;&#21644;&#30456;&#20851;&#26597;&#35810;&#26469;&#22686;&#24378;&#20998;&#38754;&#39044;&#27979;&#30340;&#31574;&#30053;&#65292;&#24182;&#25552;&#20986;&#20102;&#19987;&#27880;&#20110;&#20165;&#20351;&#29992;&#26597;&#35810;&#20316;&#20026;&#36755;&#20837;&#26469;&#39044;&#27979;&#20998;&#38754;&#30340;&#26694;&#26550;</title><link>https://arxiv.org/abs/2403.16345</link><description>&lt;p&gt;
&#22522;&#20110;LLM&#32534;&#36753;&#30340;&#22686;&#24378;&#24335;&#20998;&#38754;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Enhanced Facet Generation with LLM Editing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16345
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#25628;&#32034;&#24341;&#25806;&#33719;&#21462;&#30340;&#25991;&#26723;&#21644;&#30456;&#20851;&#26597;&#35810;&#26469;&#22686;&#24378;&#20998;&#38754;&#39044;&#27979;&#30340;&#31574;&#30053;&#65292;&#24182;&#25552;&#20986;&#20102;&#19987;&#27880;&#20110;&#20165;&#20351;&#29992;&#26597;&#35810;&#20316;&#20026;&#36755;&#20837;&#26469;&#39044;&#27979;&#20998;&#38754;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20449;&#24687;&#26816;&#32034;&#20013;&#65292;&#29992;&#25143;&#26597;&#35810;&#30340;&#20998;&#38754;&#35782;&#21035;&#26159;&#19968;&#39033;&#37325;&#35201;&#20219;&#21153;&#12290;&#22914;&#26524;&#25628;&#32034;&#26381;&#21153;&#33021;&#22815;&#35782;&#21035;&#29992;&#25143;&#26597;&#35810;&#30340;&#20998;&#38754;&#65292;&#23601;&#26377;&#28508;&#21147;&#20026;&#29992;&#25143;&#25552;&#20379;&#26356;&#24191;&#27867;&#30340;&#25628;&#32034;&#32467;&#26524;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#25628;&#32034;&#24341;&#25806;&#33719;&#21462;&#30340;&#26816;&#32034;&#25991;&#26723;&#21644;&#30456;&#20851;&#26597;&#35810;&#26469;&#22686;&#24378;&#20998;&#38754;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#22312;&#23558;&#25628;&#32034;&#24341;&#25806;&#20316;&#20026;&#27169;&#22411;&#30340;&#19968;&#37096;&#20998;&#36827;&#34892;&#25193;&#23637;&#21040;&#20854;&#20182;&#24212;&#29992;&#26102;&#23384;&#22312;&#25361;&#25112;&#12290;&#31532;&#19968;&#65292;&#25628;&#32034;&#24341;&#25806;&#19981;&#26029;&#26356;&#26032;&#12290;&#22240;&#27492;&#65292;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#26399;&#38388;&#38468;&#21152;&#20449;&#24687;&#21487;&#33021;&#20250;&#26377;&#21464;&#21270;&#65292;&#36825;&#21487;&#33021;&#20250;&#38477;&#20302;&#24615;&#33021;&#12290;&#31532;&#20108;&#20010;&#25361;&#25112;&#26159;&#20844;&#20849;&#25628;&#32034;&#24341;&#25806;&#26080;&#27861;&#25628;&#32034;&#20869;&#37096;&#25991;&#20214;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#26500;&#24314;&#19968;&#20010;&#21333;&#29420;&#30340;&#25628;&#32034;&#31995;&#32479;&#26469;&#23558;&#20844;&#21496;&#20869;&#37096;&#25991;&#26723;&#32435;&#20837;&#20854;&#20013;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#31574;&#30053;&#65292;&#19987;&#27880;&#20110;&#19968;&#20010;&#21487;&#20197;&#20165;&#36890;&#36807;&#26597;&#35810;&#20316;&#20026;&#36755;&#20837;&#26469;&#39044;&#27979;&#20998;&#38754;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16345v1 Announce Type: cross  Abstract: In information retrieval, facet identification of a user query is an important task. If a search service can recognize the facets of a user's query, it has the potential to offer users a much broader range of search results. Previous studies can enhance facet prediction by leveraging retrieved documents and related queries obtained through a search engine. However, there are challenges in extending it to other applications when a search engine operates as part of the model. First, search engines are constantly updated. Therefore, additional information may change during training and test, which may reduce performance. The second challenge is that public search engines cannot search for internal documents. Therefore, a separate search system needs to be built to incorporate documents from private domains within the company. We propose two strategies that focus on a framework that can predict facets by taking only queries as input withou
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36229;&#20302;&#25104;&#26412;&#20004;&#38454;&#27573;&#22810;&#27169;&#24335;&#31995;&#32479;&#65292;&#29992;&#20110;&#20197;&#39640;&#31934;&#30830;&#24230;&#21644;&#21484;&#22238;&#29575;&#35782;&#21035;&#26377;&#23475;&#35780;&#35770;&#21644;&#22270;&#20687;&#65292;&#36890;&#36807;CLIP-ViT&#27169;&#22411;&#23558;&#25512;&#25991;&#21644;&#22270;&#20687;&#36716;&#25442;&#20026;&#23884;&#20837;&#65292;&#24182;&#21033;&#29992;&#24120;&#35268;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#36827;&#34892;&#24555;&#36895;&#35757;&#32451;&#21644;&#20302;&#25104;&#26412;&#25512;&#29702;&#12290;</title><link>https://arxiv.org/abs/2403.16151</link><description>&lt;p&gt;
&#36229;&#20302;&#25104;&#26412;&#20004;&#38454;&#27573;&#22810;&#27169;&#24335;&#31995;&#32479;&#29992;&#20110;&#26816;&#27979;&#38750;&#35268;&#33539;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Ultra Low-Cost Two-Stage Multimodal System for Non-Normative Behavior Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16151
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36229;&#20302;&#25104;&#26412;&#20004;&#38454;&#27573;&#22810;&#27169;&#24335;&#31995;&#32479;&#65292;&#29992;&#20110;&#20197;&#39640;&#31934;&#30830;&#24230;&#21644;&#21484;&#22238;&#29575;&#35782;&#21035;&#26377;&#23475;&#35780;&#35770;&#21644;&#22270;&#20687;&#65292;&#36890;&#36807;CLIP-ViT&#27169;&#22411;&#23558;&#25512;&#25991;&#21644;&#22270;&#20687;&#36716;&#25442;&#20026;&#23884;&#20837;&#65292;&#24182;&#21033;&#29992;&#24120;&#35268;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#36827;&#34892;&#24555;&#36895;&#35757;&#32451;&#21644;&#20302;&#25104;&#26412;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#31038;&#21306;&#26085;&#30410;&#21463;&#21040;&#26377;&#23475;&#35780;&#35770;&#30340;&#27867;&#28389;&#22256;&#25200;&#12290;&#38024;&#23545;&#36825;&#19968;&#26085;&#30410;&#20005;&#23803;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#20004;&#38454;&#27573;&#36229;&#20302;&#25104;&#26412;&#22810;&#27169;&#24335;&#26377;&#23475;&#34892;&#20026;&#26816;&#27979;&#26041;&#27861;&#65292;&#26088;&#22312;&#20197;&#39640;&#31934;&#30830;&#24230;&#21644;&#21484;&#22238;&#29575;&#35782;&#21035;&#26377;&#23475;&#35780;&#35770;&#21644;&#22270;&#20687;&#12290;&#25105;&#20204;&#39318;&#20808;&#21033;&#29992;CLIP-ViT&#27169;&#22411;&#23558;&#25512;&#25991;&#21644;&#22270;&#20687;&#36716;&#25442;&#20026;&#23884;&#20837;&#65292;&#26377;&#25928;&#25429;&#25417;&#25991;&#26412;&#21644;&#22270;&#20687;&#20013;&#35821;&#20041;&#21547;&#20041;&#21644;&#24494;&#22937;&#19978;&#19979;&#25991;&#32447;&#32034;&#30340;&#22797;&#26434;&#20132;&#20114;&#20316;&#29992;&#12290;&#28982;&#21518;&#22312;&#31532;&#20108;&#38454;&#27573;&#65292;&#31995;&#32479;&#23558;&#36825;&#20123;&#23884;&#20837;&#39304;&#36865;&#21040;&#24120;&#35268;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#65288;&#22914;SVM&#25110;&#36923;&#36753;&#22238;&#24402;&#65289;&#20013;&#65292;&#20351;&#31995;&#32479;&#33021;&#22815;&#24555;&#36895;&#35757;&#32451;&#24182;&#20197;&#26497;&#20302;&#25104;&#26412;&#36827;&#34892;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16151v1 Announce Type: cross  Abstract: The online community has increasingly been inundated by a toxic wave of harmful comments. In response to this growing challenge, we introduce a two-stage ultra-low-cost multimodal harmful behavior detection method designed to identify harmful comments and images with high precision and recall rates. We first utilize the CLIP-ViT model to transform tweets and images into embeddings, effectively capturing the intricate interplay of semantic meaning and subtle contextual clues within texts and images. Then in the second stage, the system feeds these embeddings into a conventional machine learning classifier like SVM or logistic regression, enabling the system to be trained rapidly and to perform inference at an ultra-low cost. By converting tweets into rich multimodal embeddings through the CLIP-ViT model and utilizing them to train conventional machine learning classifiers, our system is not only capable of detecting harmful textual info
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20840;&#38754;&#24635;&#32467;&#21644;&#27604;&#36739;&#20102;&#30005;&#23376;&#21830;&#21153;&#39046;&#22495;&#20013;34&#39033;&#20195;&#34920;&#24615;&#20114;&#34917;&#25512;&#33616;&#30740;&#31350;&#65292;&#21253;&#25324;&#24314;&#27169;&#20135;&#21697;&#20043;&#38388;&#30340;&#20114;&#34917;&#20851;&#31995;&#65292;&#19981;&#21516;&#30740;&#31350;&#38382;&#39064;&#19979;&#30340;&#27169;&#22411;&#20998;&#31867;&#19982;&#27604;&#36739;&#65292;&#20197;&#21450;&#22312;&#21516;&#19968;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#32467;&#26524;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2403.16135</link><description>&lt;p&gt;
&#30005;&#23376;&#21830;&#21153;&#20013;&#30340;&#20114;&#34917;&#25512;&#33616;&#65306;&#23450;&#20041;&#12289;&#26041;&#27861;&#19982;&#26410;&#26469;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Complementary Recommendation in E-commerce: Definition, Approaches, and Future Directions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16135
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#24635;&#32467;&#21644;&#27604;&#36739;&#20102;&#30005;&#23376;&#21830;&#21153;&#39046;&#22495;&#20013;34&#39033;&#20195;&#34920;&#24615;&#20114;&#34917;&#25512;&#33616;&#30740;&#31350;&#65292;&#21253;&#25324;&#24314;&#27169;&#20135;&#21697;&#20043;&#38388;&#30340;&#20114;&#34917;&#20851;&#31995;&#65292;&#19981;&#21516;&#30740;&#31350;&#38382;&#39064;&#19979;&#30340;&#27169;&#22411;&#20998;&#31867;&#19982;&#27604;&#36739;&#65292;&#20197;&#21450;&#22312;&#21516;&#19968;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#32467;&#26524;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20114;&#34917;&#25512;&#33616;&#22312;&#30005;&#23376;&#21830;&#21153;&#39046;&#22495;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#26412;&#25991;&#23545;2009&#24180;&#33267;2024&#24180;&#38388;&#36827;&#34892;&#30340;34&#39033;&#20195;&#34920;&#24615;&#30740;&#31350;&#36827;&#34892;&#20102;&#20840;&#38754;&#24635;&#32467;&#21644;&#27604;&#36739;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#29992;&#20110;&#24314;&#27169;&#20135;&#21697;&#20043;&#38388;&#20114;&#34917;&#20851;&#31995;&#30340;&#25968;&#25454;&#21644;&#26041;&#27861;&#65292;&#21253;&#25324;&#31616;&#21333;&#30340;&#20114;&#34917;&#24615;&#20197;&#21450;&#26356;&#22797;&#26434;&#30340;&#24773;&#26223;&#65292;&#20363;&#22914;&#38750;&#23545;&#31216;&#20114;&#34917;&#24615;&#12289;&#20135;&#21697;&#20043;&#38388;&#26367;&#20195;&#21644;&#20114;&#34917;&#20851;&#31995;&#20849;&#23384;&#65292;&#20197;&#21450;&#19981;&#21516;&#20135;&#21697;&#23545;&#20043;&#38388;&#30340;&#20114;&#34917;&#31243;&#24230;&#19981;&#21516;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#26681;&#25454;&#20114;&#34917;&#25512;&#33616;&#30340;&#30740;&#31350;&#38382;&#39064;&#23545;&#27169;&#22411;&#36827;&#34892;&#20998;&#31867;&#24182;&#36827;&#34892;&#27604;&#36739;&#65292;&#22914;&#22810;&#26679;&#24615;&#12289;&#20010;&#24615;&#21270;&#21644;&#20919;&#21551;&#21160;&#31561;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#19981;&#21516;&#30740;&#31350;&#22312;&#21516;&#19968;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#32467;&#26524;&#36827;&#34892;&#27604;&#36739;&#20998;&#26512;&#65292;&#26377;&#21161;&#20110;&#30830;&#23450;&#30740;&#31350;&#30340;&#20248;&#21183;&#21644;&#21155;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16135v1 Announce Type: cross  Abstract: In recent years, complementary recommendation has received extensive attention in the e-commerce domain. In this paper, we comprehensively summarize and compare 34 representative studies conducted between 2009 and 2024. Firstly, we compare the data and methods used for modeling complementary relationships between products, including simple complementarity and more complex scenarios such as asymmetric complementarity, the coexistence of substitution and complementarity relationships between products, and varying degrees of complementarity between different pairs of products. Next, we classify and compare the models based on the research problems of complementary recommendation, such as diversity, personalization, and cold-start. Furthermore, we provide a comparative analysis of experimental results from different studies conducted on the same dataset, which helps identify the strengths and weaknesses of the research. Compared to previou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#25490;&#21517;&#27169;&#22411;&#30340;&#29305;&#24449;&#24402;&#22240;&#36827;&#34892;&#20102;&#20005;&#26684;&#23450;&#20041;&#65292;&#24182;&#25552;&#20986;&#20102;RankingSHAP&#20316;&#20026;&#19968;&#31181;&#36880;&#39033;&#25490;&#21517;&#24402;&#22240;&#26041;&#27861;&#65292;&#31361;&#30772;&#20102;&#24403;&#21069;&#35299;&#37322;&#35780;&#20272;&#26041;&#26696;&#30340;&#23616;&#38480;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#35780;&#20272;&#33539;&#24335;&#12290;</title><link>https://arxiv.org/abs/2403.16085</link><description>&lt;p&gt;
RankingSHAP -- &#38024;&#23545;&#25490;&#21517;&#27169;&#22411;&#30340;&#36880;&#39033;&#29305;&#24449;&#24402;&#22240;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
RankingSHAP -- Listwise Feature Attribution Explanations for Ranking Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16085
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#25490;&#21517;&#27169;&#22411;&#30340;&#29305;&#24449;&#24402;&#22240;&#36827;&#34892;&#20102;&#20005;&#26684;&#23450;&#20041;&#65292;&#24182;&#25552;&#20986;&#20102;RankingSHAP&#20316;&#20026;&#19968;&#31181;&#36880;&#39033;&#25490;&#21517;&#24402;&#22240;&#26041;&#27861;&#65292;&#31361;&#30772;&#20102;&#24403;&#21069;&#35299;&#37322;&#35780;&#20272;&#26041;&#26696;&#30340;&#23616;&#38480;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#35780;&#20272;&#33539;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#24402;&#22240;&#26159;&#19968;&#31181;&#24120;&#29992;&#30340;&#35299;&#37322;&#31867;&#22411;&#65292;&#29992;&#20110;&#22312;&#35757;&#32451;&#27169;&#22411;&#21518;&#20107;&#21518;&#35299;&#37322;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#22312;&#20449;&#24687;&#26816;&#32034;&#39046;&#22495;&#65292;&#36825;&#31181;&#26041;&#27861;&#24182;&#27809;&#26377;&#24471;&#21040;&#24456;&#22909;&#30340;&#30740;&#31350;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#29305;&#24449;&#24402;&#22240;&#24456;&#23569;&#34987;&#20005;&#26684;&#23450;&#20041;&#65292;&#38500;&#20102;&#23558;&#26368;&#37325;&#35201;&#30340;&#29305;&#24449;&#24402;&#22240;&#20026;&#26368;&#39640;&#20540;&#20043;&#22806;&#12290;&#20160;&#20040;&#26159;&#27604;&#20854;&#20182;&#29305;&#24449;&#26356;&#37325;&#35201;&#30340;&#29305;&#24449;&#24448;&#24448;&#34987;&#27169;&#31946;&#22320;&#25551;&#36848;&#12290;&#22240;&#27492;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#21482;&#20851;&#27880;&#36873;&#25321;&#26368;&#37325;&#35201;&#30340;&#29305;&#24449;&#65292;&#19981;&#20805;&#20998;&#21033;&#29992;&#29978;&#33267;&#24573;&#35270;&#29305;&#24449;&#20869;&#30340;&#30456;&#23545;&#37325;&#35201;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20005;&#26684;&#23450;&#20041;&#20102;&#25490;&#21517;&#27169;&#22411;&#29305;&#24449;&#24402;&#22240;&#30340;&#27010;&#24565;&#65292;&#24182;&#21015;&#20986;&#20102;&#19968;&#20010;&#26377;&#25928;&#24402;&#22240;&#24212;&#20855;&#22791;&#30340;&#22522;&#26412;&#23646;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;RankingSHAP&#20316;&#20026;&#36880;&#39033;&#25490;&#21517;&#24402;&#22240;&#26041;&#27861;&#30340;&#20855;&#20307;&#23454;&#20363;&#12290;&#19982;&#30446;&#21069;&#20851;&#27880;&#36873;&#25321;&#30340;&#35299;&#37322;&#35780;&#20272;&#26041;&#26696;&#30456;&#21453;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#29992;&#20110;&#35780;&#20272;&#24402;&#22240;&#30340;&#26032;&#39062;&#35780;&#20272;&#33539;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16085v1 Announce Type: new  Abstract: Feature attributions are a commonly used explanation type, when we want to posthoc explain the prediction of a trained model. Yet, they are not very well explored in IR. Importantly, feature attribution has rarely been rigorously defined, beyond attributing the most important feature the highest value. What it means for a feature to be more important than others is often left vague. Consequently, most approaches focus on just selecting the most important features and under utilize or even ignore the relative importance within features. In this work, we rigorously define the notion of feature attribution for ranking models, and list essential properties that a valid attribution should have. We then propose RankingSHAP as a concrete instantiation of a list-wise ranking attribution method. Contrary to current explanation evaluation schemes that focus on selections, we propose two novel evaluation paradigms for evaluating attributions over l
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#30693;&#35782;&#24863;&#30693;&#30340;&#21452;&#20391;&#23646;&#24615;&#22686;&#24378;&#25512;&#33616;&#8221;&#65288;KDAR&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24314;&#27169;&#29992;&#25143;&#20559;&#22909;&#21644;&#21033;&#29992;&#20559;&#22909;-&#23646;&#24615;&#36830;&#25509;&#26469;&#22686;&#24378;&#21327;&#21516;&#36807;&#28388;&#25512;&#33616;&#24615;&#33021;</title><link>https://arxiv.org/abs/2403.16037</link><description>&lt;p&gt;
&#30693;&#35782;&#24863;&#30693;&#30340;&#21452;&#20391;&#23646;&#24615;&#22686;&#24378;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Knowledge-aware Dual-side Attribute-enhanced Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16037
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#30693;&#35782;&#24863;&#30693;&#30340;&#21452;&#20391;&#23646;&#24615;&#22686;&#24378;&#25512;&#33616;&#8221;&#65288;KDAR&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24314;&#27169;&#29992;&#25143;&#20559;&#22909;&#21644;&#21033;&#29992;&#20559;&#22909;-&#23646;&#24615;&#36830;&#25509;&#26469;&#22686;&#24378;&#21327;&#21516;&#36807;&#28388;&#25512;&#33616;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#23545;&#27604;&#23398;&#20064;&#30340;&#30693;&#35782;&#24863;&#30693;&#25512;&#33616;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#25104;&#32489;&#12290;&#37492;&#20110;&#23427;&#20204;&#22312;&#24314;&#27169;&#32454;&#31890;&#24230;&#29992;&#25143;&#20559;&#22909;&#21644;&#21033;&#29992;&#20559;&#22909;-&#23646;&#24615;&#36830;&#25509;&#36827;&#34892;&#39044;&#27979;&#26041;&#38754;&#30340;&#19981;&#36275;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#30693;&#35782;&#24863;&#30693;&#30340;&#21452;&#20391;&#23646;&#24615;&#22686;&#24378;&#25512;&#33616;&#8221;&#65288;KDAR&#65289;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#22312;&#30693;&#35782;&#22270;&#20013;&#30340;&#23646;&#24615;&#20449;&#24687;&#22522;&#30784;&#19978;&#26500;&#24314;&#20102;&#29992;&#25143;&#20559;&#22909;&#34920;&#31034;&#21644;&#23646;&#24615;&#34701;&#21512;&#34920;&#31034;&#65292;&#29992;&#20110;&#22686;&#24378;&#22522;&#20110;&#21327;&#21516;&#36807;&#28388;&#30340;&#29992;&#25143;&#21644;&#39033;&#30446;&#34920;&#31034;&#12290;&#20026;&#20102;&#21306;&#20998;&#36825;&#20004;&#31181;&#22522;&#20110;&#23646;&#24615;&#30340;&#34920;&#31034;&#20013;&#27599;&#20010;&#23646;&#24615;&#30340;&#36129;&#29486;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#32423;&#21327;&#21516;&#36807;&#28388;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16037v1 Announce Type: new  Abstract: \textit{Knowledge-aware} recommendation methods (KGR) based on \textit{graph neural networks} (GNNs) and \textit{contrastive learning} (CL) have achieved promising performance. However, they fall short in modeling fine-grained user preferences and further fail to leverage the \textit{preference-attribute connection} to make predictions, leading to sub-optimal performance. To address the issue, we propose a method named \textit{\textbf{K}nowledge-aware \textbf{D}ual-side \textbf{A}ttribute-enhanced \textbf{R}ecommendation} (KDAR). Specifically, we build \textit{user preference representations} and \textit{attribute fusion representations} upon the attribute information in knowledge graphs, which are utilized to enhance \textit{collaborative filtering} (CF) based user and item representations, respectively. To discriminate the contribution of each attribute in these two types of attribute-based representations, a \textit{multi-level collab
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#22270;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#27169;&#22411;&#21270;&#21644;&#20998;&#26512;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#20869;&#29992;&#25143;&#20114;&#21160;&#65292;&#25581;&#31034;&#20102;&#27963;&#36291;&#29992;&#25143;&#20043;&#38388;&#30340;&#36830;&#25509;&#27169;&#24335;&#12289;&#23545;&#31038;&#21306;&#21160;&#24577;&#30340;&#36129;&#29486;&#29575;&#20197;&#21450;&#29992;&#25143;&#27963;&#21160;&#19982;&#21463;&#27426;&#36814;&#31243;&#24230;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.15937</link><description>&lt;p&gt;
&#27169;&#22411;&#12289;&#20998;&#26512;&#21644;&#29702;&#35299;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#20869;&#29992;&#25143;&#20114;&#21160;&#21644;&#21508;&#31181;&#23646;&#24615;
&lt;/p&gt;
&lt;p&gt;
Model, Analyze, and Comprehend User Interactions and Various Attributes within a Social Media Platform
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15937
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#22270;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#27169;&#22411;&#21270;&#21644;&#20998;&#26512;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#20869;&#29992;&#25143;&#20114;&#21160;&#65292;&#25581;&#31034;&#20102;&#27963;&#36291;&#29992;&#25143;&#20043;&#38388;&#30340;&#36830;&#25509;&#27169;&#24335;&#12289;&#23545;&#31038;&#21306;&#21160;&#24577;&#30340;&#36129;&#29486;&#29575;&#20197;&#21450;&#29992;&#25143;&#27963;&#21160;&#19982;&#21463;&#27426;&#36814;&#31243;&#24230;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24086;&#23376;-&#35780;&#35770;&#20851;&#31995;&#30340;&#26032;&#39062;&#22522;&#20110;&#22270;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#27169;&#22411;&#21270;&#21644;&#20998;&#26512;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#20869;&#29992;&#25143;&#20114;&#21160;&#65292;&#26500;&#24314;&#29992;&#25143;&#20114;&#21160;&#22270;&#24182;&#20998;&#26512;&#20197;&#28145;&#20837;&#20102;&#35299;&#31038;&#21306;&#21160;&#24577;&#12289;&#29992;&#25143;&#34892;&#20026;&#21644;&#20869;&#23481;&#20559;&#22909;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#31038;&#21306;&#20869;56.05%&#30340;&#27963;&#36291;&#29992;&#25143;&#24378;&#36830;&#25509;&#22312;&#19968;&#36215;&#65292;&#20294;&#21482;&#26377;0.8%&#30340;&#29992;&#25143;&#23545;&#20854;&#21160;&#24577;&#26377;&#26174;&#33879;&#36129;&#29486;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#31038;&#21306;&#27963;&#21160;&#23384;&#22312;&#26102;&#38388;&#21464;&#21270;&#65292;&#26576;&#20123;&#26102;&#26399;&#32463;&#21382;&#20102;&#26356;&#39640;&#30340;&#21442;&#19982;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#21457;&#29616;&#24378;&#35843;&#20102;&#29992;&#25143;&#27963;&#21160;&#19982;&#21463;&#27426;&#36814;&#31243;&#24230;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#34920;&#26126;&#26356;&#27963;&#36291;&#30340;&#29992;&#25143;&#36890;&#24120;&#26356;&#21463;&#27426;&#36814;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15937v1 Announce Type: cross  Abstract: How can we effectively model, analyze, and comprehend user interactions and various attributes within a social media platform based on post-comment relationship? In this study, we propose a novel graph-based approach to model and analyze user interactions within a social media platform based on post-comment relationship. We construct a user interaction graph from social media data and analyze it to gain insights into community dynamics, user behavior, and content preferences. Our investigation reveals that while 56.05% of the active users are strongly connected within the community, only 0.8% of them significantly contribute to its dynamics. Moreover, we observe temporal variations in community activity, with certain periods experiencing heightened engagement. Additionally, our findings highlight a correlation between user activity and popularity showing that more active users are generally more popular. Alongside these, a preference f
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#32858;&#28966;&#20110;&#22312;&#35270;&#35273;&#20016;&#23500;&#25991;&#26723;&#20013;&#36827;&#34892;&#23569;&#26679;&#26412;&#20851;&#31995;&#23398;&#20064;&#65292;&#24341;&#20837;&#20102;&#22522;&#20110;&#29616;&#26377;&#30417;&#30563;&#22522;&#20934;&#25968;&#25454;&#38598;&#26500;&#24314;&#30340;&#20004;&#20010;&#26032;&#30340;&#23569;&#26679;&#26412;&#22522;&#20934;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21253;&#21547;&#20851;&#31995;&#20108;&#32500;&#31354;&#38388;&#20808;&#39564;&#21644;&#26679;&#26412;&#30699;&#27491;&#30340;&#21464;&#20998;&#26041;&#27861;</title><link>https://arxiv.org/abs/2403.15765</link><description>&lt;p&gt;
&#26397;&#21521;&#31867;&#20154;&#26426;&#29702;&#35299;&#30340;&#26041;&#21521;&#65306;&#22312;&#35270;&#35273;&#20016;&#23500;&#25991;&#26723;&#20013;&#36827;&#34892;&#23569;&#26679;&#26412;&#20851;&#31995;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Towards Human-Like Machine Comprehension: Few-Shot Relational Learning in Visually-Rich Documents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15765
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#32858;&#28966;&#20110;&#22312;&#35270;&#35273;&#20016;&#23500;&#25991;&#26723;&#20013;&#36827;&#34892;&#23569;&#26679;&#26412;&#20851;&#31995;&#23398;&#20064;&#65292;&#24341;&#20837;&#20102;&#22522;&#20110;&#29616;&#26377;&#30417;&#30563;&#22522;&#20934;&#25968;&#25454;&#38598;&#26500;&#24314;&#30340;&#20004;&#20010;&#26032;&#30340;&#23569;&#26679;&#26412;&#22522;&#20934;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21253;&#21547;&#20851;&#31995;&#20108;&#32500;&#31354;&#38388;&#20808;&#39564;&#21644;&#26679;&#26412;&#30699;&#27491;&#30340;&#21464;&#20998;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#38190;-&#20540;&#20851;&#31995;&#22312;&#35270;&#35273;&#20016;&#23500;&#25991;&#26723;&#65288;VRDs&#65289;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#36890;&#24120;&#22312;&#19981;&#21516;&#30340;&#31354;&#38388;&#21306;&#22495;&#20013;&#21576;&#29616;&#65292;&#20276;&#38543;&#29305;&#23450;&#30340;&#39068;&#33394;&#21644;&#23383;&#20307;&#39118;&#26684;&#12290;&#36825;&#20123;&#38750;&#25991;&#26412;&#32447;&#32034;&#20316;&#20026;&#37325;&#35201;&#25351;&#31034;&#22120;&#65292;&#26497;&#22823;&#22686;&#24378;&#20102;&#20154;&#31867;&#23545;&#36825;&#31181;&#20851;&#31995;&#19977;&#20803;&#32452;&#30340;&#29702;&#35299;&#21644;&#33719;&#21462;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#25991;&#26723;AI&#26041;&#27861;&#24448;&#24448;&#26410;&#32771;&#34385;&#19982;&#35270;&#35273;&#21644;&#31354;&#38388;&#29305;&#24449;&#30456;&#20851;&#30340;&#36825;&#20123;&#26377;&#20215;&#20540;&#30340;&#20808;&#39564;&#20449;&#24687;&#65292;&#23548;&#33268;&#24615;&#33021;&#19981;&#20339;&#65292;&#29305;&#21035;&#26159;&#22312;&#22788;&#29702;&#26377;&#38480;&#31034;&#20363;&#26102;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32858;&#28966;&#20110;&#23569;&#26679;&#26412;&#20851;&#31995;&#23398;&#20064;&#65292;&#20855;&#20307;&#38024;&#23545;&#22312;VRDs&#20013;&#25552;&#21462;&#20851;&#38190;-&#20540;&#20851;&#31995;&#19977;&#20803;&#32452;&#12290;&#37492;&#20110;&#32570;&#20047;&#36866;&#29992;&#20110;&#36825;&#19968;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#29616;&#26377;&#30417;&#30563;&#22522;&#20934;&#25968;&#25454;&#38598;&#26500;&#24314;&#30340;&#20004;&#20010;&#26032;&#30340;&#23569;&#26679;&#26412;&#22522;&#20934;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21253;&#21547;&#20851;&#31995;&#20108;&#32500;&#31354;&#38388;&#20808;&#39564;&#21644;&#26679;&#26412;&#30699;&#27491;&#30340;&#21464;&#20998;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15765v1 Announce Type: cross  Abstract: Key-value relations are prevalent in Visually-Rich Documents (VRDs), often depicted in distinct spatial regions accompanied by specific color and font styles. These non-textual cues serve as important indicators that greatly enhance human comprehension and acquisition of such relation triplets. However, current document AI approaches often fail to consider this valuable prior information related to visual and spatial features, resulting in suboptimal performance, particularly when dealing with limited examples. To address this limitation, our research focuses on few-shot relational learning, specifically targeting the extraction of key-value relation triplets in VRDs. Given the absence of a suitable dataset for this task, we introduce two new few-shot benchmarks built upon existing supervised benchmark datasets. Furthermore, we propose a variational approach that incorporates relational 2D-spatial priors and prototypical rectification 
&lt;/p&gt;</description></item><item><title>&#29992;&#25143;&#31471;&#23454;&#29616;&#20026;&#29992;&#25143;&#25552;&#20379;&#20102;&#31215;&#26497;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#22312;&#29992;&#25143;&#31471;&#36816;&#34892;&#36890;&#29992;&#31639;&#27861;&#26469;&#35299;&#20915;&#24120;&#35265;&#38382;&#39064;&#65292;&#26080;&#38656;&#26381;&#21153;&#25552;&#20379;&#21830;&#25913;&#21464;&#26381;&#21153;&#26412;&#36523;&#12290;</title><link>https://arxiv.org/abs/2403.15757</link><description>&lt;p&gt;
&#29992;&#25143;&#31471;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
User-Side Realization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15757
&lt;/p&gt;
&lt;p&gt;
&#29992;&#25143;&#31471;&#23454;&#29616;&#20026;&#29992;&#25143;&#25552;&#20379;&#20102;&#31215;&#26497;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#22312;&#29992;&#25143;&#31471;&#36816;&#34892;&#36890;&#29992;&#31639;&#27861;&#26469;&#35299;&#20915;&#24120;&#35265;&#38382;&#39064;&#65292;&#26080;&#38656;&#26381;&#21153;&#25552;&#20379;&#21830;&#25913;&#21464;&#26381;&#21153;&#26412;&#36523;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#25143;&#23545;&#26381;&#21153;&#24863;&#21040;&#19981;&#28385;&#24847;&#12290;&#30001;&#20110;&#26381;&#21153;&#24182;&#38750;&#37327;&#36523;&#23450;&#21046;&#32473;&#29992;&#25143;&#65292;&#22240;&#27492;&#19981;&#28385;&#24847;&#26159;&#33258;&#28982;&#32780;&#28982;&#30340;&#12290;&#38382;&#39064;&#22312;&#20110;&#65292;&#21363;&#20351;&#29992;&#25143;&#24863;&#21040;&#19981;&#28385;&#24847;&#65292;&#20182;&#20204;&#36890;&#24120;&#20063;&#27809;&#26377;&#35299;&#20915;&#19981;&#28385;&#30340;&#25163;&#27573;&#12290;&#29992;&#25143;&#26080;&#27861;&#20462;&#25913;&#26381;&#21153;&#30340;&#28304;&#20195;&#30721;&#65292;&#20063;&#26080;&#27861;&#24378;&#36843;&#26381;&#21153;&#25552;&#20379;&#21830;&#36827;&#34892;&#26356;&#25913;&#12290;&#29992;&#25143;&#21035;&#26080;&#36873;&#25321;&#65292;&#21482;&#33021;&#20445;&#25345;&#19981;&#28385;&#24847;&#25110;&#36864;&#20986;&#26381;&#21153;&#12290;&#29992;&#25143;&#31471;&#23454;&#29616;&#36890;&#36807;&#25552;&#20379;&#36890;&#29992;&#31639;&#27861;&#26469;&#22788;&#29702;&#29992;&#25143;&#31471;&#30340;&#24120;&#35265;&#38382;&#39064;&#65292;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#25552;&#20379;&#20102;&#31215;&#26497;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36825;&#20123;&#31639;&#27861;&#22312;&#29992;&#25143;&#31471;&#36816;&#34892;&#65292;&#24182;&#22312;&#19981;&#38656;&#35201;&#26381;&#21153;&#25552;&#20379;&#21830;&#25913;&#21464;&#26381;&#21153;&#26412;&#36523;&#30340;&#24773;&#20917;&#19979;&#35299;&#20915;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15757v1 Announce Type: cross  Abstract: Users are dissatisfied with services. Since the service is not tailor-made for a user, it is natural for dissatisfaction to arise. The problem is, that even if users are dissatisfied, they often do not have the means to resolve their dissatisfaction. The user cannot alter the source code of the service, nor can they force the service provider to change. The user has no choice but to remain dissatisfied or quit the service. User-side realization offers proactive solutions to this problem by providing general algorithms to deal with common problems on the user's side. These algorithms run on the user's side and solve the problems without having the service provider change the service itself.
&lt;/p&gt;</description></item><item><title>QueryExplorer&#25552;&#20986;&#20102;&#19968;&#20010;&#20132;&#20114;&#24335;&#26597;&#35810;&#29983;&#25104;&#12289;&#37325;&#26032;&#26500;&#36896;&#21644;&#26816;&#32034;&#30028;&#38754;&#65292;&#25903;&#25345;&#22810;&#31181;&#26041;&#27861;&#24110;&#21161;&#29992;&#25143;&#21019;&#24314;&#21644;&#20462;&#25913;&#26377;&#25928;&#26597;&#35810;&#65292;&#20197;&#35299;&#20915;&#29992;&#25143;&#32570;&#20047;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#25110;&#35821;&#35328;&#33021;&#21147;&#26102;&#30340;&#25628;&#32034;&#22256;&#38590;&#12290;</title><link>https://arxiv.org/abs/2403.15667</link><description>&lt;p&gt;
QueryExplorer&#65306;&#29992;&#20110;&#25628;&#32034;&#21644;&#25506;&#32034;&#30340;&#20132;&#20114;&#24335;&#26597;&#35810;&#29983;&#25104;&#21161;&#25163;
&lt;/p&gt;
&lt;p&gt;
QueryExplorer: An Interactive Query Generation Assistant for Search and Exploration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15667
&lt;/p&gt;
&lt;p&gt;
QueryExplorer&#25552;&#20986;&#20102;&#19968;&#20010;&#20132;&#20114;&#24335;&#26597;&#35810;&#29983;&#25104;&#12289;&#37325;&#26032;&#26500;&#36896;&#21644;&#26816;&#32034;&#30028;&#38754;&#65292;&#25903;&#25345;&#22810;&#31181;&#26041;&#27861;&#24110;&#21161;&#29992;&#25143;&#21019;&#24314;&#21644;&#20462;&#25913;&#26377;&#25928;&#26597;&#35810;&#65292;&#20197;&#35299;&#20915;&#29992;&#25143;&#32570;&#20047;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#25110;&#35821;&#35328;&#33021;&#21147;&#26102;&#30340;&#25628;&#32034;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#21046;&#23450;&#25628;&#32034;&#26597;&#35810;&#20173;&#28982;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#29305;&#21035;&#26159;&#24403;&#29992;&#25143;&#32570;&#20047;&#29305;&#23450;&#39046;&#22495;&#30340;&#19987;&#19994;&#30693;&#35782;&#25110;&#19981;&#25797;&#38271;&#20869;&#23481;&#30340;&#35821;&#35328;&#26102;&#12290;&#25552;&#20379;&#29992;&#25143;&#24863;&#20852;&#36259;&#30340;&#31034;&#20363;&#25991;&#26723;&#21487;&#33021;&#26356;&#23481;&#26131;&#19968;&#20123;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#31034;&#20363;&#36827;&#34892;&#26597;&#35810;&#30340;&#24773;&#20917;&#23481;&#26131;&#20986;&#29616;&#27010;&#24565;&#28418;&#31227;&#65292;&#24182;&#19988;&#26816;&#32034;&#25928;&#26524;&#23545;&#26597;&#35810;&#29983;&#25104;&#26041;&#27861;&#38750;&#24120;&#25935;&#24863;&#65292;&#19988;&#27809;&#26377;&#26126;&#30830;&#30340;&#26041;&#27861;&#26469;&#25972;&#21512;&#29992;&#25143;&#21453;&#39304;&#12290;&#20026;&#20102;&#23454;&#29616;&#25506;&#32034;&#24182;&#25903;&#25345;&#20154;&#26426;&#23454;&#39564;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;QueryExplorer -- &#19968;&#20010;&#20132;&#20114;&#24335;&#26597;&#35810;&#29983;&#25104;&#12289;&#37325;&#26032;&#26500;&#36896;&#21644;&#26816;&#32034;&#30028;&#38754;&#65292;&#25903;&#25345;HuggingFace&#29983;&#25104;&#27169;&#22411;&#21644;PyTerrier&#30340;&#26816;&#32034;&#31649;&#36947;&#21644;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#23545;&#20154;&#31867;&#21453;&#39304;&#30340;&#35814;&#23613;&#35760;&#24405;&#12290;&#20026;&#20102;&#35753;&#29992;&#25143;&#21019;&#24314;&#21644;&#20462;&#25913;&#26377;&#25928;&#30340;&#26597;&#35810;&#65292;&#25105;&#20204;&#30340;&#28436;&#31034;&#25903;&#25345;&#20351;&#29992;LLMs&#30340;&#20114;&#34917;&#26041;&#27861;&#65292;&#21327;&#21161;&#29992;&#25143;&#22312;&#22810;&#20010;&#38454;&#27573;&#36827;&#34892;&#32534;&#36753;&#21644;&#21453;&#39304;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15667v1 Announce Type: new  Abstract: Formulating effective search queries remains a challenging task, particularly when users lack expertise in a specific domain or are not proficient in the language of the content. Providing example documents of interest might be easier for a user. However, such query-by-example scenarios are prone to concept drift, and the retrieval effectiveness is highly sensitive to the query generation method, without a clear way to incorporate user feedback. To enable exploration and to support Human-In-The-Loop experiments we propose QueryExplorer -- an interactive query generation, reformulation, and retrieval interface with support for HuggingFace generation models and PyTerrier's retrieval pipelines and datasets, and extensive logging of human feedback. To allow users to create and modify effective queries, our demo supports complementary approaches of using LLMs interactively, assisting the user with edits and feedback at multiple stages of the 
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20559;&#21521;&#31354;&#38388;&#26041;&#21521;&#30340;&#26041;&#27861;&#22312;&#39640;&#32500;&#30456;&#20301;&#24674;&#22797;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#65292;&#22312;&#27979;&#37327;&#27425;&#25968;&#23569;&#20110;&#20449;&#21495;&#32500;&#25968;&#26102;&#23588;&#20026;&#26377;&#25928;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#19968;&#20010;&#20381;&#36182;&#20110;&#26679;&#26412;&#22823;&#23567;&#19982;&#20449;&#21495;&#32500;&#25968;&#27604;&#29575;&#30340;&#30456;&#20301;&#36716;&#25442;&#29616;&#35937;&#12290;</title><link>https://arxiv.org/abs/2403.15548</link><description>&lt;p&gt;
&#20855;&#26377;&#20559;&#21521;&#31354;&#38388;&#26041;&#21521;&#30340;&#39640;&#32500;&#30456;&#20301;&#24674;&#22797;&#30340;&#20809;&#35889;&#21021;&#22987;&#21270;
&lt;/p&gt;
&lt;p&gt;
Spectral Initialization for High-Dimensional Phase Retrieval with Biased Spatial Directions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15548
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20559;&#21521;&#31354;&#38388;&#26041;&#21521;&#30340;&#26041;&#27861;&#22312;&#39640;&#32500;&#30456;&#20301;&#24674;&#22797;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#65292;&#22312;&#27979;&#37327;&#27425;&#25968;&#23569;&#20110;&#20449;&#21495;&#32500;&#25968;&#26102;&#23588;&#20026;&#26377;&#25928;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#19968;&#20010;&#20381;&#36182;&#20110;&#26679;&#26412;&#22823;&#23567;&#19982;&#20449;&#21495;&#32500;&#25968;&#27604;&#29575;&#30340;&#30456;&#20301;&#36716;&#25442;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#35752;&#20102;&#19968;&#31181;&#22312;&#38750;&#20984;&#22330;&#26223;&#20013;&#30340;&#20449;&#21495;&#20272;&#35745;&#30340;&#24403;&#20195;&#30740;&#31350;&#20013;&#36215;&#30528;&#26680;&#24515;&#20316;&#29992;&#30340;&#20809;&#35889;&#21021;&#22987;&#21270;&#26041;&#27861;&#12290;&#22312;&#26080;&#22122;&#22768;&#30456;&#20301;&#24674;&#22797;&#26694;&#26550;&#20013;&#65292;&#25105;&#20204;&#31934;&#30830;&#20998;&#26512;&#20102;&#24403;&#24863;&#30693;&#21521;&#37327;&#36981;&#24490;&#22810;&#20803;&#39640;&#26031;&#20998;&#24067;&#26102;&#35813;&#26041;&#27861;&#22312;&#39640;&#32500;&#26497;&#38480;&#19979;&#30340;&#34920;&#29616;&#65292;&#36866;&#29992;&#20110;&#21327;&#26041;&#24046;&#30697;&#38453;C&#30340;&#20004;&#31181;&#26059;&#36716;&#19981;&#21464;&#27169;&#22411;&#12290;&#31532;&#19968;&#31181;&#27169;&#22411;&#20013;&#30340;C&#26159;&#19968;&#20010;&#25237;&#24433;&#22312;&#19968;&#20010;&#20302;&#32500;&#31354;&#38388;&#19978;&#65292;&#32780;&#31532;&#20108;&#31181;&#27169;&#22411;&#20013;&#30340;C&#26159;&#19968;&#20010;Wishart&#30697;&#38453;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#32467;&#26524;&#25193;&#23637;&#20102;C&#20026;&#21333;&#20301;&#30697;&#38453;&#26102;&#24050;&#32463;&#34987;&#20805;&#20998;&#30830;&#35748;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#24341;&#20837;&#20559;&#21521;&#31354;&#38388;&#26041;&#21521;&#20250;&#26174;&#33879;&#25552;&#39640;&#20809;&#35889;&#26041;&#27861;&#30340;&#25928;&#26524;&#65292;&#29305;&#21035;&#26159;&#24403;&#27979;&#37327;&#27425;&#25968;&#23569;&#20110;&#20449;&#21495;&#32500;&#25968;&#26102;&#12290;&#36825;&#19968;&#25193;&#23637;&#36824;&#22987;&#32456;&#26174;&#31034;&#20102;&#20381;&#36182;&#20110;&#26679;&#26412;&#22823;&#23567;&#19982;&#20449;&#21495;&#32500;&#25968;&#20043;&#38388;&#27604;&#29575;&#30340;&#30456;&#20301;&#36716;&#25442;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15548v1 Announce Type: cross  Abstract: We explore a spectral initialization method that plays a central role in contemporary research on signal estimation in nonconvex scenarios. In a noiseless phase retrieval framework, we precisely analyze the method's performance in the high-dimensional limit when sensing vectors follow a multivariate Gaussian distribution for two rotationally invariant models of the covariance matrix C. In the first model C is a projector on a lower dimensional space while in the second it is a Wishart matrix. Our analytical results extend the well-established case when C is the identity matrix. Our examination shows that the introduction of biased spatial directions leads to a substantial improvement in the spectral method's effectiveness, particularly when the number of measurements is less than the signal's dimension. This extension also consistently reveals a phase transition phenomenon dependent on the ratio between sample size and signal dimension
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;GNN&#21644;Transformer&#30340;&#21512;&#20316;&#23398;&#20064;&#26041;&#26696;&#65292;&#24182;&#26500;&#24314;&#20102;GTC&#26550;&#26500;&#65292;&#36890;&#36807;&#25972;&#21512;GNN&#30340;&#26412;&#22320;&#20449;&#24687;&#32858;&#21512;&#21644;Transformer&#30340;&#20840;&#23616;&#20449;&#24687;&#24314;&#27169;&#33021;&#21147;&#65292;&#35299;&#20915;&#20102;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.15520</link><description>&lt;p&gt;
GTC&#65306;GNN-Transformer&#33258;&#30417;&#30563;&#24322;&#26500;&#22270;&#34920;&#31034;&#30340;&#20849;&#36717;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
GTC: GNN-Transformer Co-contrastive Learning for Self-supervised Heterogeneous Graph Representation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15520
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;GNN&#21644;Transformer&#30340;&#21512;&#20316;&#23398;&#20064;&#26041;&#26696;&#65292;&#24182;&#26500;&#24314;&#20102;GTC&#26550;&#26500;&#65292;&#36890;&#36807;&#25972;&#21512;GNN&#30340;&#26412;&#22320;&#20449;&#24687;&#32858;&#21512;&#21644;Transformer&#30340;&#20840;&#23616;&#20449;&#24687;&#24314;&#27169;&#33021;&#21147;&#65292;&#35299;&#20915;&#20102;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30001;&#20110;&#20855;&#26377;&#20256;&#36882;&#20449;&#24687;&#30340;&#26426;&#21046;&#65292;&#33021;&#22815;&#24456;&#22909;&#22320;&#32858;&#21512;&#26412;&#22320;&#20449;&#24687;&#65292;&#22312;&#21508;&#31181;&#22270;&#20219;&#21153;&#20013;&#24050;&#32463;&#25104;&#20026;&#26368;&#24378;&#22823;&#30340;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#36807;&#24230;&#24179;&#28369;&#19968;&#30452;&#38459;&#30861;&#30528;GNN&#36827;&#19968;&#27493;&#28145;&#20837;&#21644;&#25429;&#33719;&#22810;&#36339;&#37051;&#23621;&#12290;&#19982;GNN&#19981;&#21516;&#65292;Transformer&#21487;&#20197;&#36890;&#36807;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#21644;&#36866;&#24403;&#30340;Transformer&#32467;&#26500;&#26469;&#24314;&#27169;&#20840;&#23616;&#20449;&#24687;&#21644;&#22810;&#36339;&#20132;&#20114;&#65292;&#24182;&#19988;&#33021;&#22815;&#26356;&#22909;&#22320;&#35299;&#20915;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#26159;&#21542;&#21487;&#20197;&#25552;&#20986;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#23558;GNN&#21644;Transformer&#32467;&#21512;&#36215;&#26469;&#65292;&#25972;&#21512;GNN&#30340;&#26412;&#22320;&#20449;&#24687;&#32858;&#21512;&#21644;Transformer&#30340;&#20840;&#23616;&#20449;&#24687;&#24314;&#27169;&#33021;&#21147;&#65292;&#20197;&#28040;&#38500;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#65311;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;GNN-Transformer&#30340;&#21327;&#21516;&#23398;&#20064;&#26041;&#26696;&#65292;&#24182;&#26500;&#24314;&#20102;GTC&#26550;&#26500;&#12290;GTC&#21033;&#29992;GNN&#21644;Transformer&#20998;&#25903;&#20998;&#21035;&#23545;&#26469;&#33258;&#19981;&#21516;&#35270;&#22270;&#30340;&#33410;&#28857;&#20449;&#24687;&#36827;&#34892;&#32534;&#30721;&#65292;&#24182;&#24314;&#31435;&#20102;&#23545;&#27604;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15520v1 Announce Type: new  Abstract: Graph Neural Networks (GNNs) have emerged as the most powerful weapon for various graph tasks due to the message-passing mechanism's great local information aggregation ability. However, over-smoothing has always hindered GNNs from going deeper and capturing multi-hop neighbors. Unlike GNNs, Transformers can model global information and multi-hop interactions via multi-head self-attention and a proper Transformer structure can show more immunity to the over-smoothing problem. So, can we propose a novel framework to combine GNN and Transformer, integrating both GNN's local information aggregation and Transformer's global information modeling ability to eliminate the over-smoothing problem? To realize this, this paper proposes a collaborative learning scheme for GNN-Transformer and constructs GTC architecture. GTC leverages the GNN and Transformer branch to encode node information from different views respectively, and establishes contrast
&lt;/p&gt;</description></item><item><title>LoRAG&#26159;&#19968;&#31181;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#36845;&#20195;&#24490;&#29615;&#26426;&#21046;&#25552;&#39640;&#20102;&#26816;&#32034;&#22686;&#24378;&#22411;&#25991;&#26412;&#29983;&#25104;&#30340;&#36136;&#37327;&#65292;&#22312;&#23454;&#39564;&#35777;&#26126;&#22312;&#29983;&#25104;&#30340;&#25991;&#26412;&#36830;&#36143;&#24615;&#21644;&#30456;&#20851;&#24615;&#26041;&#38754;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.15450</link><description>&lt;p&gt;
Loops On Retrieval Augmented Generation (LoRAG)
&lt;/p&gt;
&lt;p&gt;
Loops On Retrieval Augmented Generation (LoRAG)
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15450
&lt;/p&gt;
&lt;p&gt;
LoRAG&#26159;&#19968;&#31181;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#36845;&#20195;&#24490;&#29615;&#26426;&#21046;&#25552;&#39640;&#20102;&#26816;&#32034;&#22686;&#24378;&#22411;&#25991;&#26412;&#29983;&#25104;&#30340;&#36136;&#37327;&#65292;&#22312;&#23454;&#39564;&#35777;&#26126;&#22312;&#29983;&#25104;&#30340;&#25991;&#26412;&#36830;&#36143;&#24615;&#21644;&#30456;&#20851;&#24615;&#26041;&#38754;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Loops On Retrieval Augmented Generation (LoRAG)&#30340;&#26032;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#24341;&#20837;&#36845;&#20195;&#24490;&#29615;&#26426;&#21046;&#26469;&#25552;&#39640;&#26816;&#32034;&#22686;&#24378;&#22411;&#25991;&#26412;&#29983;&#25104;&#30340;&#36136;&#37327;&#12290;&#35813;&#26550;&#26500;&#38598;&#25104;&#20102;&#29983;&#25104;&#27169;&#22411;&#12289;&#26816;&#32034;&#26426;&#21046;&#21644;&#21160;&#24577;&#24490;&#29615;&#27169;&#22359;&#65292;&#20801;&#35768;&#36890;&#36807;&#19982;&#20174;&#36755;&#20837;&#19978;&#19979;&#25991;&#20013;&#26816;&#32034;&#30340;&#30456;&#20851;&#20449;&#24687;&#36827;&#34892;&#20132;&#20114;&#26469;&#23545;&#29983;&#25104;&#30340;&#25991;&#26412;&#36827;&#34892;&#36845;&#20195;&#25913;&#36827;&#12290;&#23545;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;LoRAG&#22312;BLEU&#20998;&#25968;&#12289;ROUGE&#20998;&#25968;&#21644;&#22256;&#24785;&#24230;&#26041;&#38754;&#22343;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#29983;&#25104;&#25991;&#26412;&#30340;&#36830;&#36143;&#24615;&#21644;&#30456;&#20851;&#24615;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#23450;&#24615;&#35780;&#20272;&#36827;&#19968;&#27493;&#35828;&#26126;&#20102;LoRAG&#20135;&#29983;&#19978;&#19979;&#25991;&#20016;&#23500;&#19988;&#36830;&#36143;&#30340;&#36755;&#20986;&#30340;&#33021;&#21147;&#12290;&#36825;&#39033;&#30740;&#31350;&#20026;&#36845;&#20195;&#24490;&#29615;&#22312;&#32531;&#35299;&#25991;&#26412;&#29983;&#25104;&#20013;&#30340;&#25361;&#25112;&#26041;&#38754;&#30340;&#28508;&#21147;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15450v1 Announce Type: new  Abstract: This paper presents Loops On Retrieval Augmented Generation (LoRAG), a new framework designed to enhance the quality of retrieval-augmented text generation through the incorporation of an iterative loop mechanism. The architecture integrates a generative model, a retrieval mechanism, and a dynamic loop module, allowing for iterative refinement of the generated text through interactions with relevant information retrieved from the input context. Experimental evaluations on benchmark datasets demonstrate that LoRAG surpasses existing state-of-the-art models in terms of BLEU score, ROUGE score, and perplexity, showcasing its effectiveness in achieving both coherence and relevance in generated text. The qualitative assessment further illustrates LoRAG's capability to produce contextually rich and coherent outputs. This research contributes valuable insights into the potential of iterative loops in mitigating challenges in text generation, po
&lt;/p&gt;</description></item><item><title>LLM&#21551;&#29992;&#20102;&#22522;&#20110;&#31574;&#30053;&#30340;&#22810;&#27169;&#26597;&#35810;&#20248;&#21270;&#22120;&#65292;&#25670;&#33073;&#20102;&#20256;&#32479;&#30340;&#22522;&#20110;&#35268;&#21017;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#20026;&#26597;&#35810;&#20248;&#21270;&#24102;&#26469;&#20840;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.13597</link><description>&lt;p&gt;
&#19981;&#20877;&#26377;&#20248;&#21270;&#35268;&#21017;: &#22522;&#20110;LLM&#30340;&#22522;&#20110;&#31574;&#30053;&#30340;&#22810;&#27169;&#26597;&#35810;&#20248;&#21270;&#22120;&#65288;&#29256;&#26412;1&#65289;
&lt;/p&gt;
&lt;p&gt;
No more optimization rules: LLM-enabled policy-based multi-modal query optimizer (version 1)
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13597
&lt;/p&gt;
&lt;p&gt;
LLM&#21551;&#29992;&#20102;&#22522;&#20110;&#31574;&#30053;&#30340;&#22810;&#27169;&#26597;&#35810;&#20248;&#21270;&#22120;&#65292;&#25670;&#33073;&#20102;&#20256;&#32479;&#30340;&#22522;&#20110;&#35268;&#21017;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#20026;&#26597;&#35810;&#20248;&#21270;&#24102;&#26469;&#20840;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;(LLM)&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#26631;&#24535;&#30528;&#19968;&#20010;&#37325;&#35201;&#26102;&#21051;&#12290;&#26368;&#36817;&#65292;&#20154;&#20204;&#30740;&#31350;&#20102;LLM&#22312;&#26597;&#35810;&#35268;&#21010;&#20013;&#30340;&#33021;&#21147;&#65292;&#21253;&#25324;&#21333;&#27169;&#21644;&#22810;&#27169;&#26597;&#35810;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;LLM&#30340;&#26597;&#35810;&#20248;&#21270;&#33021;&#21147;&#36824;&#27809;&#26377;&#30456;&#20851;&#30740;&#31350;&#12290;&#20316;&#20026;&#26174;&#33879;&#24433;&#21709;&#26597;&#35810;&#35745;&#21010;&#25191;&#34892;&#24615;&#33021;&#30340;&#20851;&#38190;&#27493;&#39588;&#65292;&#19981;&#24212;&#38169;&#36807;&#36825;&#31181;&#20998;&#26512;&#21644;&#23581;&#35797;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#29616;&#26377;&#30340;&#26597;&#35810;&#20248;&#21270;&#22120;&#36890;&#24120;&#26159;&#22522;&#20110;&#35268;&#21017;&#25110;&#22522;&#20110;&#35268;&#21017;+&#22522;&#20110;&#25104;&#26412;&#30340;&#65292;&#21363;&#23427;&#20204;&#20381;&#36182;&#20110;&#20154;&#24037;&#21019;&#24314;&#30340;&#35268;&#21017;&#26469;&#23436;&#25104;&#26597;&#35810;&#35745;&#21010;&#37325;&#20889;/&#36716;&#25442;&#12290;&#37492;&#20110;&#29616;&#20195;&#20248;&#21270;&#22120;&#21253;&#25324;&#25968;&#30334;&#33267;&#25968;&#21315;&#26465;&#35268;&#21017;&#65292;&#25353;&#29031;&#31867;&#20284;&#26041;&#24335;&#35774;&#35745;&#19968;&#20010;&#22810;&#27169;&#26597;&#35810;&#20248;&#21270;&#22120;&#23558;&#32791;&#36153;&#22823;&#37327;&#26102;&#38388;&#65292;&#22240;&#20026;&#25105;&#20204;&#23558;&#19981;&#24471;&#19981;&#21015;&#20030;&#23613;&#21487;&#33021;&#22810;&#30340;&#22810;&#27169;&#20248;&#21270;&#35268;&#21017;&#65292;&#32780;&#36825;&#24182;&#27809;&#26377;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13597v1 Announce Type: cross  Abstract: Large language model (LLM) has marked a pivotal moment in the field of machine learning and deep learning. Recently its capability for query planning has been investigated, including both single-modal and multi-modal queries. However, there is no work on the query optimization capability of LLM. As a critical (or could even be the most important) step that significantly impacts the execution performance of the query plan, such analysis and attempts should not be missed. From another aspect, existing query optimizers are usually rule-based or rule-based + cost-based, i.e., they are dependent on manually created rules to complete the query plan rewrite/transformation. Given the fact that modern optimizers include hundreds to thousands of rules, designing a multi-modal query optimizer following a similar way is significantly time-consuming since we will have to enumerate as many multi-modal optimization rules as possible, which has not be
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;NoteLLM&#30340;&#26032;&#39062;&#32479;&#19968;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26469;&#23454;&#29616;&#29289;&#21697;&#21040;&#29289;&#21697;(I2I)&#30340;&#31508;&#35760;&#25512;&#33616;&#65292;&#36890;&#36807;&#23398;&#20064;&#29983;&#25104;&#21704;&#24076;&#26631;&#31614;/&#31867;&#21035;&#28508;&#22312;&#22320;&#22686;&#24378;&#31508;&#35760;&#23884;&#20837;&#65292;&#25552;&#39640;&#20102;&#23545;&#20851;&#38190;&#31508;&#35760;&#20449;&#24687;&#30340;&#21387;&#32553;&#12290;</title><link>https://arxiv.org/abs/2403.01744</link><description>&lt;p&gt;
NoteLLM: &#19968;&#31181;&#21487;&#26816;&#32034;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#31508;&#35760;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
NoteLLM: A Retrievable Large Language Model for Note Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01744
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;NoteLLM&#30340;&#26032;&#39062;&#32479;&#19968;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26469;&#23454;&#29616;&#29289;&#21697;&#21040;&#29289;&#21697;(I2I)&#30340;&#31508;&#35760;&#25512;&#33616;&#65292;&#36890;&#36807;&#23398;&#20064;&#29983;&#25104;&#21704;&#24076;&#26631;&#31614;/&#31867;&#21035;&#28508;&#22312;&#22320;&#22686;&#24378;&#31508;&#35760;&#23884;&#20837;&#65292;&#25552;&#39640;&#20102;&#23545;&#20851;&#38190;&#31508;&#35760;&#20449;&#24687;&#30340;&#21387;&#32553;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20204;&#21916;&#27426;&#22312;&#22312;&#32447;&#31038;&#21306;&#20869;&#20998;&#20139;&#8220;&#31508;&#35760;&#8221;&#65292;&#21253;&#25324;&#20182;&#20204;&#30340;&#32463;&#39564;&#12290;&#22240;&#27492;&#65292;&#25512;&#33616;&#19982;&#29992;&#25143;&#20852;&#36259;&#30456;&#31526;&#30340;&#31508;&#35760;&#24050;&#32463;&#25104;&#20026;&#19968;&#39033;&#20851;&#38190;&#20219;&#21153;&#12290;&#29616;&#26377;&#30340;&#22312;&#32447;&#26041;&#27861;&#21482;&#23558;&#31508;&#35760;&#36755;&#20837;&#21040;&#22522;&#20110;BERT&#30340;&#27169;&#22411;&#20013;&#65292;&#29992;&#20110;&#29983;&#25104;&#31508;&#35760;&#23884;&#20837;&#20197;&#35780;&#20272;&#30456;&#20284;&#24615;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#21487;&#33021;&#26410;&#20805;&#20998;&#21033;&#29992;&#19968;&#20123;&#37325;&#35201;&#30340;&#32447;&#32034;&#65292;&#20363;&#22914;&#21704;&#24076;&#26631;&#31614;&#25110;&#31867;&#21035;&#65292;&#36825;&#20123;&#20195;&#34920;&#20102;&#31508;&#35760;&#30340;&#20851;&#38190;&#27010;&#24565;&#12290;&#20107;&#23454;&#19978;&#65292;&#23398;&#20064;&#29983;&#25104;&#21704;&#24076;&#26631;&#31614;/&#31867;&#21035;&#21487;&#20197;&#28508;&#22312;&#22320;&#22686;&#24378;&#31508;&#35760;&#23884;&#20837;&#65292;&#20108;&#32773;&#37117;&#23558;&#37325;&#35201;&#30340;&#31508;&#35760;&#20449;&#24687;&#21387;&#32553;&#20026;&#26377;&#38480;&#20869;&#23481;&#12290;&#27492;&#22806;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29702;&#35299;&#33258;&#28982;&#35821;&#35328;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;BERT&#12290;&#23558;LLMs&#24341;&#20837;&#31508;&#35760;&#25512;&#33616;&#26159;&#24456;&#26377;&#21069;&#36884;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;NoteLLM&#30340;&#26032;&#39062;&#32479;&#19968;&#26694;&#26550;&#65292;&#21033;&#29992;LLMs&#26469;&#22788;&#29702;&#29289;&#21697;&#21040;&#29289;&#21697;&#65288;I2I&#65289;&#31508;&#35760;&#25512;&#33616;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21033;&#29992;&#31508;&#35760;&#21387;&#32553;&#25552;&#31034;&#26469;&#21387;&#32553;&#19968;&#26465;&#31508;&#35760;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01744v1 Announce Type: new  Abstract: People enjoy sharing "notes" including their experiences within online communities. Therefore, recommending notes aligned with user interests has become a crucial task. Existing online methods only input notes into BERT-based models to generate note embeddings for assessing similarity. However, they may underutilize some important cues, e.g., hashtags or categories, which represent the key concepts of notes. Indeed, learning to generate hashtags/categories can potentially enhance note embeddings, both of which compress key note information into limited content. Besides, Large Language Models (LLMs) have significantly outperformed BERT in understanding natural languages. It is promising to introduce LLMs into note recommendation. In this paper, we propose a novel unified framework called NoteLLM, which leverages LLMs to address the item-to-item (I2I) note recommendation. Specifically, we utilize Note Compression Prompt to compress a note 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#65306;&#32452;&#21512;&#20154;&#21592;&#26816;&#32034;&#65288;CPR&#65289;&#65292;&#26088;&#22312;&#32852;&#21512;&#21033;&#29992;&#22270;&#20687;&#21644;&#25991;&#26412;&#20449;&#24687;&#36827;&#34892;&#30446;&#26631;&#20154;&#21592;&#26816;&#32034;&#65292;&#24341;&#20837;&#38646;&#26679;&#26412;&#32452;&#21512;&#20154;&#21592;&#26816;&#32034;&#65288;ZS-CPR&#65289;&#35299;&#20915;&#20102;CPR&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#23398;&#20064;&#26694;&#26550;Word4Per&#12290;</title><link>https://arxiv.org/abs/2311.16515</link><description>&lt;p&gt;
Word4Per: Zero-shot&#32452;&#21512;&#20154;&#21592;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Word4Per: Zero-shot Composed Person Retrieval
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.16515
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#65306;&#32452;&#21512;&#20154;&#21592;&#26816;&#32034;&#65288;CPR&#65289;&#65292;&#26088;&#22312;&#32852;&#21512;&#21033;&#29992;&#22270;&#20687;&#21644;&#25991;&#26412;&#20449;&#24687;&#36827;&#34892;&#30446;&#26631;&#20154;&#21592;&#26816;&#32034;&#65292;&#24341;&#20837;&#38646;&#26679;&#26412;&#32452;&#21512;&#20154;&#21592;&#26816;&#32034;&#65288;ZS-CPR&#65289;&#35299;&#20915;&#20102;CPR&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#23398;&#20064;&#26694;&#26550;Word4Per&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23547;&#25214;&#29305;&#23450;&#20154;&#21592;&#20855;&#26377;&#26497;&#22823;&#30340;&#31038;&#20250;&#25928;&#30410;&#21644;&#23433;&#20840;&#20215;&#20540;&#65292;&#36890;&#24120;&#28041;&#21450;&#35270;&#35273;&#21644;&#25991;&#26412;&#20449;&#24687;&#30340;&#32467;&#21512;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#26032;&#30340;&#20219;&#21153;&#65292;&#31216;&#20026;&#32452;&#21512;&#20154;&#21592;&#26816;&#32034;&#65288;CPR&#65289;&#65292;&#26088;&#22312;&#32852;&#21512;&#21033;&#29992;&#22270;&#20687;&#21644;&#25991;&#26412;&#20449;&#24687;&#36827;&#34892;&#30446;&#26631;&#20154;&#21592;&#26816;&#32034;&#12290;&#28982;&#32780;&#65292;&#30417;&#30563;CPR&#38656;&#35201;&#26114;&#36149;&#30340;&#25163;&#21160;&#27880;&#37322;&#25968;&#25454;&#38598;&#65292;&#32780;&#30446;&#21069;&#27809;&#26377;&#21487;&#29992;&#36164;&#28304;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#38646;&#26679;&#26412;&#32452;&#21512;&#20154;&#21592;&#26816;&#32034;&#65288;ZS-CPR&#65289;&#65292;&#21033;&#29992;&#29616;&#26377;&#30340;&#39046;&#22495;&#30456;&#20851;&#25968;&#25454;&#35299;&#20915;&#20102;CPR&#38382;&#39064;&#32780;&#19981;&#38656;&#35201;&#26114;&#36149;&#30340;&#27880;&#37322;&#12290;&#20854;&#27425;&#65292;&#20026;&#20102;&#23398;&#20064;ZS-CPR&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#23398;&#20064;&#26694;&#26550;&#65292;&#21363;Word4Per&#65292;&#20854;&#20013;&#21253;&#21547;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#25991;&#26412;&#21453;&#36716;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.16515v2 Announce Type: replace-cross  Abstract: Searching for specific person has great social benefits and security value, and it often involves a combination of visual and textual information. Conventional person retrieval methods, whether image-based or text-based, usually fall short in effectively harnessing both types of information, leading to the loss of accuracy. In this paper, a whole new task called Composed Person Retrieval (CPR) is proposed to jointly utilize both image and text information for target person retrieval. However, the supervised CPR requires very costly manual annotation dataset, while there are currently no available resources. To mitigate this issue, we firstly introduce the Zero-shot Composed Person Retrieval (ZS-CPR), which leverages existing domain-related data to resolve the CPR problem without expensive annotations. Secondly, to learn ZS-CPR model, we propose a two-stage learning framework, Word4Per, where a lightweight Textual Inversion Netw
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;&#25512;&#33616;&#31995;&#32479;&#30340;&#29983;&#25104;&#24335;&#25512;&#33616;&#25552;&#20379;&#20102;&#26032;&#26426;&#36935;&#65292;&#21487;&#20197;&#31616;&#21270;&#25512;&#33616;&#27969;&#31243;&#24182;&#30452;&#25509;&#20174;&#23436;&#25972;&#30340;&#39033;&#30446;&#27744;&#20013;&#29983;&#25104;&#25512;&#33616;&#12290;</title><link>https://arxiv.org/abs/2309.01157</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#29983;&#25104;&#24335;&#25512;&#33616;&#65306;&#19968;&#39033;&#35843;&#26597;&#21644;&#36828;&#35265;&#35752;&#35770;
&lt;/p&gt;
&lt;p&gt;
Large Language Models for Generative Recommendation: A Survey and Visionary Discussions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.01157
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;&#25512;&#33616;&#31995;&#32479;&#30340;&#29983;&#25104;&#24335;&#25512;&#33616;&#25552;&#20379;&#20102;&#26032;&#26426;&#36935;&#65292;&#21487;&#20197;&#31616;&#21270;&#25512;&#33616;&#27969;&#31243;&#24182;&#30452;&#25509;&#20174;&#23436;&#25972;&#30340;&#39033;&#30446;&#27744;&#20013;&#29983;&#25104;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19981;&#20165;&#24443;&#24213;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#65292;&#36824;&#26377;&#28508;&#21147;&#37325;&#22609;&#35768;&#22810;&#20854;&#20182;&#39046;&#22495;&#65292;&#20363;&#22914;&#25512;&#33616;&#31995;&#32479;&#65288;RS&#65289;&#12290;&#26412;&#25991;&#35843;&#26597;&#20102;&#22522;&#20110;LLM&#30340;&#29983;&#25104;&#24335;&#25512;&#33616;&#30340;&#36827;&#23637;&#12289;&#26041;&#27861;&#21644;&#26410;&#26469;&#26041;&#21521;&#65292;&#30528;&#30524;&#20110;&#19977;&#20010;&#38382;&#39064;&#65306;1&#65289;&#29983;&#25104;&#24335;&#25512;&#33616;&#26159;&#20160;&#20040;&#65292;2&#65289;&#20026;&#20160;&#20040;RS&#24212;&#35813;&#21457;&#23637;&#21040;&#29983;&#25104;&#24335;&#25512;&#33616;&#65292;3&#65289;&#22914;&#20309;&#20026;&#21508;&#31181;RS&#23454;&#29616;&#22522;&#20110;LLM&#30340;&#29983;&#25104;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.01157v2 Announce Type: replace-cross  Abstract: Large language models (LLM) not only have revolutionized the field of natural language processing (NLP) but also have the potential to reshape many other fields, e.g., recommender systems (RS). However, most of the related work treats an LLM as a component of the conventional recommendation pipeline (e.g., as a feature extractor), which may not be able to fully leverage the generative power of LLM. Instead of separating the recommendation process into multiple stages, such as score computation and re-ranking, this process can be simplified to one stage with LLM: directly generating recommendations from the complete pool of items. This survey reviews the progress, methods, and future directions of LLM-based generative recommendation by examining three questions: 1) What generative recommendation is, 2) Why RS should advance to generative recommendation, and 3) How to implement LLM-based generative recommendation for various RS t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21327;&#20316;&#23398;&#20064;&#25512;&#33616;&#31995;&#32479;&#38024;&#23545;&#19968;&#31181;&#26032;&#22411;&#38544;&#31169;&#25915;&#20987;&#8212;&#8212;&#31038;&#21306;&#26816;&#27979;&#25915;&#20987;&#65288;CDA&#65289;&#30340;&#38887;&#24615;&#12290;</title><link>https://arxiv.org/abs/2306.08929</link><description>&lt;p&gt;
&#20851;&#20110;&#21327;&#20316;&#23398;&#20064;&#25512;&#33616;&#31995;&#32479;&#38024;&#23545;&#31038;&#21306;&#26816;&#27979;&#25915;&#20987;&#30340;&#38887;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the resilience of Collaborative Learning-based Recommender Systems Against Community Detection Attack
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2306.08929
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21327;&#20316;&#23398;&#20064;&#25512;&#33616;&#31995;&#32479;&#38024;&#23545;&#19968;&#31181;&#26032;&#22411;&#38544;&#31169;&#25915;&#20987;&#8212;&#8212;&#31038;&#21306;&#26816;&#27979;&#25915;&#20987;&#65288;CDA&#65289;&#30340;&#38887;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21327;&#20316;&#23398;&#20064;&#25512;&#33616;&#31995;&#32479;&#28304;&#20110;&#21327;&#20316;&#23398;&#20064;&#25216;&#26415;&#65288;&#22914;&#32852;&#37030;&#23398;&#20064;&#21644;&#20843;&#21350;&#23398;&#20064;&#65289;&#30340;&#25104;&#21151;&#12290;&#22312;&#36825;&#20123;&#31995;&#32479;&#20013;&#65292;&#29992;&#25143;&#21442;&#19982;&#25512;&#33616;&#31995;&#32479;&#30340;&#35757;&#32451;&#21516;&#26102;&#22312;&#20854;&#35774;&#22791;&#19978;&#20445;&#30041;&#24050;&#28040;&#36153;&#39033;&#30446;&#30340;&#21382;&#21490;&#35760;&#24405;&#12290;&#34429;&#28982;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#20045;&#19968;&#30475;&#20284;&#20046;&#26377;&#21033;&#20110;&#20445;&#25252;&#21442;&#19982;&#32773;&#30340;&#38544;&#31169;&#65292;&#20294;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21327;&#20316;&#23398;&#20064;&#21487;&#33021;&#23481;&#26131;&#21463;&#21040;&#21508;&#31181;&#38544;&#31169;&#25915;&#20987;&#30340;&#23041;&#32961;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#21327;&#20316;&#23398;&#20064;&#25512;&#33616;&#31995;&#32479;&#38024;&#23545;&#19968;&#31181;&#31216;&#20026;&#31038;&#21306;&#26816;&#27979;&#25915;&#20987;&#65288;CDA&#65289;&#30340;&#26032;&#22411;&#38544;&#31169;&#25915;&#20987;&#30340;&#38887;&#24615;&#12290;&#36825;&#31181;&#25915;&#20987;&#20351;&#24471;&#23545;&#25163;&#33021;&#22815;&#22522;&#20110;&#19968;&#20010;&#36873;&#25321;&#30340;&#39033;&#30446;&#38598;&#65288;&#22914;&#35782;&#21035;&#23545;&#29305;&#23450;&#20852;&#36259;&#28857;&#24863;&#20852;&#36259;&#30340;&#29992;&#25143;&#65289;&#26469;&#35782;&#21035;&#31038;&#21306;&#25104;&#21592;&#12290;&#36890;&#36807;&#22312;&#19977;&#20010;&#30495;&#23454;&#25512;&#33616;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#20351;&#29992;&#20004;&#31181;&#26368;&#20808;&#36827;&#30340;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
arXiv:2306.08929v2 Announce Type: replace-cross  Abstract: Collaborative-learning-based recommender systems emerged following the success of collaborative learning techniques such as Federated Learning (FL) and Gossip Learning (GL). In these systems, users participate in the training of a recommender system while maintaining their history of consumed items on their devices. While these solutions seemed appealing for preserving the privacy of the participants at first glance, recent studies have revealed that collaborative learning can be vulnerable to various privacy attacks. In this paper, we study the resilience of collaborative learning-based recommender systems against a novel privacy attack called Community Detection Attack (CDA). This attack enables an adversary to identify community members based on a chosen set of items (eg., identifying users interested in specific points-of-interest). Through experiments on three real recommendation datasets using two state-of-the-art recomme
&lt;/p&gt;</description></item><item><title>Spacerini&#26159;&#19968;&#20010;&#38598;&#25104;&#20102;Pyserini&#21644;Hugging Face&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#26080;&#32541;&#26500;&#24314;&#21644;&#37096;&#32626;&#20132;&#20114;&#24335;&#25628;&#32034;&#24341;&#25806;&#65292;&#20351;&#24471;&#38750;IR&#20174;&#19994;&#32773;&#21487;&#20197;&#26356;&#36731;&#26494;&#22320;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#26816;&#32034;&#27169;&#22411;&#65292;&#23545;NLP&#21644;IR&#30740;&#31350;&#20154;&#21592;&#20197;&#21450;&#31532;&#19977;&#26041;&#22797;&#21046;&#30740;&#31350;&#24037;&#20316;&#37117;&#38750;&#24120;&#26377;&#29992;&#12290;</title><link>https://arxiv.org/abs/2302.14534</link><description>&lt;p&gt;
Spacerini&#65306;&#20351;&#29992;Pyserini&#21644;Hugging Face&#23454;&#29616;&#21363;&#25554;&#21363;&#29992;&#30340;&#25628;&#32034;&#24341;&#25806;
&lt;/p&gt;
&lt;p&gt;
Spacerini: Plug-and-play Search Engines with Pyserini and Hugging Face
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.14534
&lt;/p&gt;
&lt;p&gt;
Spacerini&#26159;&#19968;&#20010;&#38598;&#25104;&#20102;Pyserini&#21644;Hugging Face&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#26080;&#32541;&#26500;&#24314;&#21644;&#37096;&#32626;&#20132;&#20114;&#24335;&#25628;&#32034;&#24341;&#25806;&#65292;&#20351;&#24471;&#38750;IR&#20174;&#19994;&#32773;&#21487;&#20197;&#26356;&#36731;&#26494;&#22320;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#26816;&#32034;&#27169;&#22411;&#65292;&#23545;NLP&#21644;IR&#30740;&#31350;&#20154;&#21592;&#20197;&#21450;&#31532;&#19977;&#26041;&#22797;&#21046;&#30740;&#31350;&#24037;&#20316;&#37117;&#38750;&#24120;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Spacerini&#65292;&#36825;&#26159;&#19968;&#20010;&#38598;&#25104;&#20102;Pyserini&#24037;&#20855;&#21253;&#21644;Hugging Face&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#26080;&#32541;&#26500;&#24314;&#21644;&#37096;&#32626;&#20132;&#20114;&#24335;&#25628;&#32034;&#24341;&#25806;&#30340;&#24037;&#20855;&#12290;&#36890;&#36807;Spacerini&#65292;&#38750;IR&#20174;&#19994;&#32773;&#21487;&#20197;&#26356;&#36731;&#26494;&#22320;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#31232;&#30095;&#21644;&#31264;&#23494;&#26816;&#32034;&#27169;&#22411;&#65292;&#24182;&#26368;&#23567;&#21270;&#37096;&#32626;&#24037;&#20316;&#37327;&#12290;Spacerini&#23545;&#20110;&#24076;&#26395;&#36890;&#36807;&#23545;&#35757;&#32451;&#35821;&#26009;&#24211;&#36827;&#34892;&#23450;&#24615;&#20998;&#26512;&#26469;&#26356;&#22909;&#22320;&#29702;&#35299;&#21644;&#39564;&#35777;&#30740;&#31350;&#30340;NLP&#30740;&#31350;&#20154;&#21592;&#12289;&#24076;&#26395;&#22312;&#19981;&#26029;&#21457;&#23637;&#30340;Pyserini&#29983;&#24577;&#31995;&#32479;&#20013;&#23637;&#31034;&#26032;&#26816;&#32034;&#27169;&#22411;&#30340;IR&#30740;&#31350;&#20154;&#21592;&#20197;&#21450;&#22797;&#21046;&#20854;&#20182;&#30740;&#31350;&#20154;&#21592;&#24037;&#20316;&#30340;&#31532;&#19977;&#26041;&#26469;&#35828;&#37117;&#38750;&#24120;&#26377;&#29992;&#12290;Spacerini&#26159;&#24320;&#28304;&#30340;&#65292;&#24182;&#21253;&#25324;&#29992;&#20110;&#26412;&#22320;&#21644;&#36828;&#31243;&#21152;&#36733;&#12289;&#39044;&#22788;&#29702;&#12289;&#32034;&#24341;&#21644;&#37096;&#32626;&#25628;&#32034;&#24341;&#25806;&#30340;&#23454;&#29992;&#31243;&#24207;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;Spacerini&#21019;&#24314;&#30340;13&#20010;&#19981;&#21516;&#29992;&#20363;&#30340;&#25628;&#32034;&#24341;&#25806;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2302.14534v2 Announce Type: replace-cross  Abstract: We present Spacerini, a tool that integrates the Pyserini toolkit for reproducible information retrieval research with Hugging Face to enable the seamless construction and deployment of interactive search engines. Spacerini makes state-of-the-art sparse and dense retrieval models more accessible to non-IR practitioners while minimizing deployment effort. This is useful for NLP researchers who want to better understand and validate their research by performing qualitative analyses of training corpora, for IR researchers who want to demonstrate new retrieval models integrated into the growing Pyserini ecosystem, and for third parties reproducing the work of other researchers. Spacerini is open source and includes utilities for loading, preprocessing, indexing, and deploying search engines locally and remotely. We demonstrate a portfolio of 13 search engines created with Spacerini for different use cases.
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#35843;&#26597;&#20102;&#19968;&#31181;&#21517;&#20026;&#26816;&#32034;&#31034;&#33539;&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;&#29305;&#23450;&#20110;&#36755;&#20837;&#26597;&#35810;&#30340;&#31034;&#33539;&#26469;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#23569;&#37327;&#26679;&#26412;&#24773;&#22659;&#23398;&#20064;&#65288;ICL&#65289;&#33021;&#21147;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#25552;&#39640;&#20102;&#23398;&#20064;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#36824;&#20943;&#23569;&#20102;&#25163;&#21160;&#31034;&#20363;&#36873;&#25321;&#20013;&#30340;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2401.11624</link><description>&lt;p&gt;
&#36890;&#36807;&#26816;&#32034;&#31034;&#33539;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#35821;&#35328;&#27169;&#22411;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
In-context Learning with Retrieved Demonstrations for Language Models: A Survey. (arXiv:2401.11624v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11624
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#35843;&#26597;&#20102;&#19968;&#31181;&#21517;&#20026;&#26816;&#32034;&#31034;&#33539;&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;&#29305;&#23450;&#20110;&#36755;&#20837;&#26597;&#35810;&#30340;&#31034;&#33539;&#26469;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#23569;&#37327;&#26679;&#26412;&#24773;&#22659;&#23398;&#20064;&#65288;ICL&#65289;&#33021;&#21147;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#25552;&#39640;&#20102;&#23398;&#20064;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#36824;&#20943;&#23569;&#20102;&#25163;&#21160;&#31034;&#20363;&#36873;&#25321;&#20013;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24050;&#23637;&#31034;&#20986;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#21487;&#20197;&#22312;&#36755;&#20837;&#19978;&#19979;&#25991;&#20013;&#36827;&#34892;&#23569;&#37327;&#26679;&#26412;&#30340;&#24773;&#22659;&#23398;&#20064;&#65288;ICL&#65289;&#65292;&#24182;&#22312;&#26032;&#20219;&#21153;&#19978;&#20855;&#26377;&#36866;&#24212;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#27169;&#22411;&#30340;ICL&#33021;&#21147;&#23545;&#20110;&#23569;&#26679;&#26412;&#31034;&#33539;&#30340;&#36873;&#25321;&#26159;&#25935;&#24863;&#30340;&#12290;&#26368;&#36817;&#30340;&#19968;&#39033;&#30740;&#31350;&#36827;&#23637;&#26159;&#26816;&#32034;&#38024;&#23545;&#27599;&#20010;&#36755;&#20837;&#26597;&#35810;&#23450;&#21046;&#30340;&#31034;&#33539;&#12290;&#31034;&#33539;&#26816;&#32034;&#30340;&#23454;&#29616;&#30456;&#23545;&#31616;&#21333;&#65292;&#21033;&#29992;&#29616;&#26377;&#30340;&#25968;&#25454;&#24211;&#21644;&#26816;&#32034;&#31995;&#32479;&#12290;&#36825;&#19981;&#20165;&#25552;&#39640;&#20102;&#23398;&#20064;&#36807;&#31243;&#30340;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#32780;&#19988;&#24050;&#32463;&#35777;&#26126;&#21487;&#20197;&#20943;&#23569;&#25163;&#21160;&#31034;&#20363;&#36873;&#25321;&#20013;&#30340;&#20559;&#35265;&#12290;&#37492;&#20110;&#20196;&#20154;&#40723;&#33310;&#30340;&#32467;&#26524;&#21644;&#22312;&#26816;&#32034;&#31034;&#33539;&#30340;ICL&#26041;&#38754;&#19981;&#26029;&#22686;&#38271;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#32508;&#36848;&#12290;&#22312;&#36825;&#39033;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#21644;&#27604;&#36739;&#20102;&#26816;&#32034;&#27169;&#22411;&#30340;&#19981;&#21516;&#35774;&#35745;&#36873;&#25321;&#65292;&#26816;&#32034;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Language models, especially pre-trained large language models, have showcased remarkable abilities as few-shot in-context learners (ICL), adept at adapting to new tasks with just a few demonstrations in the input context. However, the model's ability to perform ICL is sensitive to the choice of the few-shot demonstrations. Instead of using a fixed set of demonstrations, one recent development is to retrieve demonstrations tailored to each input query. The implementation of demonstration retrieval is relatively straightforward, leveraging existing databases and retrieval systems. This not only improves the efficiency and scalability of the learning process but also has been shown to reduce biases inherent in manual example selection. In light of the encouraging results and growing research in ICL with retrieved demonstrations, we conduct an extensive review of studies in this area. In this survey, we discuss and compare different design choices for retrieval models, retrieval training p
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#23545;MovieLens&#25968;&#25454;&#38598;&#30340;&#20998;&#26512;&#65292;&#21457;&#29616;&#29992;&#25143;&#19982;&#35813;&#24179;&#21488;&#30340;&#20132;&#20114;&#22312;&#19981;&#21516;&#38454;&#27573;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#24182;&#19988;&#29992;&#25143;&#20132;&#20114;&#21463;&#21040;&#24179;&#21488;&#25512;&#33616;&#31639;&#27861;&#25512;&#33616;&#30340;&#20505;&#36873;&#30005;&#24433;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2307.09985</link><description>&lt;p&gt;
&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;MovieLens&#19978;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#34920;&#29616;&#65306;&#36825;&#24847;&#21619;&#30528;&#20160;&#20040;&#65311;
&lt;/p&gt;
&lt;p&gt;
Our Model Achieves Excellent Performance on MovieLens: What Does it Mean?. (arXiv:2307.09985v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09985
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#23545;MovieLens&#25968;&#25454;&#38598;&#30340;&#20998;&#26512;&#65292;&#21457;&#29616;&#29992;&#25143;&#19982;&#35813;&#24179;&#21488;&#30340;&#20132;&#20114;&#22312;&#19981;&#21516;&#38454;&#27573;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#24182;&#19988;&#29992;&#25143;&#20132;&#20114;&#21463;&#21040;&#24179;&#21488;&#25512;&#33616;&#31639;&#27861;&#25512;&#33616;&#30340;&#20505;&#36873;&#30005;&#24433;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#35780;&#20272;&#30340;&#20856;&#22411;&#22522;&#20934;&#25968;&#25454;&#38598;&#26159;&#22312;&#26576;&#19968;&#26102;&#38388;&#27573;&#20869;&#22312;&#24179;&#21488;&#19978;&#29983;&#25104;&#30340;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#25968;&#25454;&#12290;&#20132;&#20114;&#29983;&#25104;&#26426;&#21046;&#37096;&#20998;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#29992;&#25143;&#19982;&#29289;&#21697;&#36827;&#34892;&#20132;&#20114;&#65288;&#22914;&#21916;&#27426;&#12289;&#36141;&#20080;&#12289;&#35780;&#20998;&#65289;&#20197;&#21450;&#29305;&#23450;&#20132;&#20114;&#21457;&#29983;&#30340;&#32972;&#26223;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23545;MovieLens&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#32454;&#33268;&#30340;&#20998;&#26512;&#65292;&#24182;&#35299;&#37322;&#20102;&#20351;&#29992;&#35813;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#25512;&#33616;&#31639;&#27861;&#26102;&#21487;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#20174;&#20998;&#26512;&#20013;&#24471;&#20986;&#20102;&#19968;&#20123;&#20027;&#35201;&#21457;&#29616;&#12290;&#39318;&#20808;&#65292;&#22312;&#29992;&#25143;&#19982;MovieLens&#24179;&#21488;&#20132;&#20114;&#30340;&#19981;&#21516;&#38454;&#27573;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#26089;&#26399;&#20132;&#20114;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#23450;&#20041;&#20102;&#29992;&#25143;&#30011;&#20687;&#65292;&#24433;&#21709;&#20102;&#21518;&#32493;&#30340;&#20132;&#20114;&#12290;&#20854;&#27425;&#65292;&#29992;&#25143;&#20132;&#20114;&#21463;&#21040;&#24179;&#21488;&#20869;&#37096;&#25512;&#33616;&#31639;&#27861;&#25512;&#33616;&#30340;&#20505;&#36873;&#30005;&#24433;&#30340;&#24456;&#22823;&#24433;&#21709;&#12290;&#21024;&#38500;&#38752;&#36817;&#26368;&#21518;&#20960;&#27425;&#20132;&#20114;&#30340;&#20132;&#20114;&#20250;&#23545;&#32467;&#26524;&#20135;&#29983;&#36739;&#22823;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
A typical benchmark dataset for recommender system (RecSys) evaluation consists of user-item interactions generated on a platform within a time period. The interaction generation mechanism partially explains why a user interacts with (e.g.,like, purchase, rate) an item, and the context of when a particular interaction happened. In this study, we conduct a meticulous analysis on the MovieLens dataset and explain the potential impact on using the dataset for evaluating recommendation algorithms. We make a few main findings from our analysis. First, there are significant differences in user interactions at the different stages when a user interacts with the MovieLens platform. The early interactions largely define the user portrait which affect the subsequent interactions. Second, user interactions are highly affected by the candidate movies that are recommended by the platform's internal recommendation algorithm(s). Removal of interactions that happen nearer to the last few interactions 
&lt;/p&gt;</description></item></channel></rss>