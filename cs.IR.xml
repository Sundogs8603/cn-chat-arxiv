<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;ILCiteR&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#22522;&#20110;&#35777;&#25454;&#30340;&#23616;&#37096;&#24341;&#25991;&#25512;&#33616;&#20219;&#21153;&#65292;&#20174;&#29616;&#26377;&#30740;&#31350;&#25991;&#29486;&#20013;&#25552;&#21462;&#30456;&#20284;&#35777;&#25454;&#33539;&#22260;&#26469;&#25512;&#33616;&#24341;&#29992;&#30340;&#35770;&#25991;&#65292;&#25552;&#39640;&#20102;&#25512;&#33616;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.08737</link><description>&lt;p&gt;
ILCiteR&#65306;&#22522;&#20110;&#35777;&#25454;&#30340;&#21487;&#35299;&#37322;&#23616;&#37096;&#24341;&#25991;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
ILCiteR: Evidence-grounded Interpretable Local Citation Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08737
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;ILCiteR&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#22522;&#20110;&#35777;&#25454;&#30340;&#23616;&#37096;&#24341;&#25991;&#25512;&#33616;&#20219;&#21153;&#65292;&#20174;&#29616;&#26377;&#30740;&#31350;&#25991;&#29486;&#20013;&#25552;&#21462;&#30456;&#20284;&#35777;&#25454;&#33539;&#22260;&#26469;&#25512;&#33616;&#24341;&#29992;&#30340;&#35770;&#25991;&#65292;&#25552;&#39640;&#20102;&#25512;&#33616;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#29992;&#20110;&#23616;&#37096;&#24341;&#25991;&#25512;&#33616;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30452;&#25509;&#23558;&#26597;&#35810;&#65288;&#36890;&#24120;&#26159;&#22768;&#26126;&#25110;&#23454;&#20307;&#25552;&#21450;&#65289;&#26144;&#23556;&#25110;&#32763;&#35793;&#20026;&#20540;&#24471;&#24341;&#29992;&#30340;&#30740;&#31350;&#35770;&#25991;&#12290;&#22312;&#36825;&#31181;&#34920;&#36848;&#20013;&#65292;&#24456;&#38590;&#30830;&#23450;&#20026;&#20160;&#20040;&#24212;&#35813;&#20026;&#29305;&#23450;&#26597;&#35810;&#24341;&#29992;&#29305;&#23450;&#30740;&#31350;&#35770;&#25991;&#65292;&#20174;&#32780;&#23548;&#33268;&#25512;&#33616;&#30340;&#21487;&#35299;&#37322;&#24615;&#21463;&#38480;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#35777;&#25454;&#30340;&#23616;&#37096;&#24341;&#25991;&#25512;&#33616;&#20219;&#21153;&#65292;&#20854;&#20013;&#30446;&#26631;&#28508;&#22312;&#31354;&#38388;&#21253;&#25324;&#29992;&#20110;&#25512;&#33616;&#29305;&#23450;&#35770;&#25991;&#30340;&#35777;&#25454;&#33539;&#22260;&#12290;&#36890;&#36807;&#20351;&#29992;&#36828;&#31243;&#30417;&#30563;&#35777;&#25454;&#26816;&#32034;&#21644;&#22810;&#27493;&#37325;&#26032;&#25490;&#24207;&#26694;&#26550;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#31995;&#32479;ILCiteR&#22522;&#20110;&#20174;&#29616;&#26377;&#30740;&#31350;&#25991;&#29486;&#20013;&#25552;&#21462;&#30340;&#30456;&#20284;&#35777;&#25454;&#33539;&#22260;&#21521;&#26597;&#35810;&#25512;&#33616;&#24212;&#24341;&#29992;&#30340;&#35770;&#25991;&#12290;&#19982;&#36807;&#21435;&#31616;&#21333;&#36755;&#20986;&#25512;&#33616;&#30340;&#24418;&#24335;&#19981;&#21516;&#65292;ILCiteR&#26816;&#32034;&#20986;&#32463;&#36807;&#25490;&#21517;&#30340;&#35777;&#25454;&#33539;&#22260;&#21644;&#25512;&#33616;&#30340;&#35770;&#25991;&#23545;&#21015;&#34920;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08737v1 Announce Type: cross  Abstract: Existing Machine Learning approaches for local citation recommendation directly map or translate a query, which is typically a claim or an entity mention, to citation-worthy research papers. Within such a formulation, it is challenging to pinpoint why one should cite a specific research paper for a particular query, leading to limited recommendation interpretability. To alleviate this, we introduce the evidence-grounded local citation recommendation task, where the target latent space comprises evidence spans for recommending specific papers. Using a distantly-supervised evidence retrieval and multi-step re-ranking framework, our proposed system, ILCiteR, recommends papers to cite for a query grounded on similar evidence spans extracted from the existing research literature. Unlike past formulations that simply output recommendations, ILCiteR retrieves ranked lists of evidence span and recommended paper pairs. Secondly, previously prop
&lt;/p&gt;</description></item><item><title>NLQxform-UI&#26159;&#19968;&#20010;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#30340;&#20132;&#20114;&#24335;&#26597;&#35810;&#30028;&#38754;&#65292;&#21487;&#20197;&#33258;&#21160;&#23558;&#22797;&#26434;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#36716;&#25442;&#20026;SPARQL&#26597;&#35810;&#65292;&#24182;&#22312;DBLP&#30693;&#35782;&#22270;&#19978;&#25191;&#34892;&#65292;&#25552;&#39640;&#20102;&#31995;&#32479;&#30340;&#21487;&#29992;&#24615;&#21644;&#20934;&#30830;&#24615;</title><link>https://arxiv.org/abs/2403.08475</link><description>&lt;p&gt;
NLQxform-UI&#65306;&#29992;&#20110;&#20132;&#20114;&#24335;&#26597;&#35810;DBLP&#30340;&#33258;&#28982;&#35821;&#35328;&#25509;&#21475;
&lt;/p&gt;
&lt;p&gt;
NLQxform-UI: A Natural Language Interface for Querying DBLP Interactively
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08475
&lt;/p&gt;
&lt;p&gt;
NLQxform-UI&#26159;&#19968;&#20010;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#30340;&#20132;&#20114;&#24335;&#26597;&#35810;&#30028;&#38754;&#65292;&#21487;&#20197;&#33258;&#21160;&#23558;&#22797;&#26434;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#36716;&#25442;&#20026;SPARQL&#26597;&#35810;&#65292;&#24182;&#22312;DBLP&#30693;&#35782;&#22270;&#19978;&#25191;&#34892;&#65292;&#25552;&#39640;&#20102;&#31995;&#32479;&#30340;&#21487;&#29992;&#24615;&#21644;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;DBLP&#35745;&#31639;&#26426;&#31185;&#23398;&#25991;&#29486;&#30446;&#24405;&#24050;&#34987;&#24191;&#27867;&#29992;&#20110;&#25628;&#32034;&#23398;&#26415;&#20449;&#24687;&#65292;&#22914;&#20986;&#29256;&#29289;&#12289;&#23398;&#32773;&#21644;&#20250;&#35758;&#12290;&#28982;&#32780;&#65292;&#20854;&#24403;&#21069;&#30340;&#25628;&#32034;&#26381;&#21153;&#32570;&#20047;&#22788;&#29702;&#22797;&#26434;&#26597;&#35810;&#30340;&#33021;&#21147;&#65292;&#36825;&#38480;&#21046;&#20102;DBLP&#30340;&#21487;&#29992;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;NLQxform-UI&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;web&#30340;&#33258;&#28982;&#35821;&#35328;&#25509;&#21475;&#65292;&#20801;&#35768;&#29992;&#25143;&#30452;&#25509;&#29992;&#22797;&#26434;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#26597;&#35810;DBLP&#12290;NLQxform-UI&#20250;&#33258;&#21160;&#23558;&#32473;&#23450;&#38382;&#39064;&#36716;&#25442;&#20026;SPARQL&#26597;&#35810;&#65292;&#24182;&#22312;DBLP&#30693;&#35782;&#22270;&#19978;&#25191;&#34892;&#26597;&#35810;&#20197;&#26816;&#32034;&#31572;&#26696;&#12290;&#26597;&#35810;&#36807;&#31243;&#20197;&#20132;&#20114;&#26041;&#24335;&#21576;&#29616;&#32473;&#29992;&#25143;&#65292;&#25552;&#39640;&#20102;&#31995;&#32479;&#30340;&#36879;&#26126;&#24230;&#24182;&#26377;&#21161;&#20110;&#26816;&#26597;&#36820;&#22238;&#30340;&#31572;&#26696;&#12290;&#27492;&#22806;&#65292;&#26597;&#35810;&#36807;&#31243;&#20013;&#30340;&#20013;&#38388;&#32467;&#26524;&#21487;&#20197;&#39044;&#35272;&#21644;&#25163;&#21160;&#20462;&#25913;&#20197;&#25552;&#39640;&#31995;&#32479;&#30340;&#20934;&#30830;&#24615;&#12290;NLQxform-UI&#24050;&#23436;&#20840;&#24320;&#28304;&#65306;https://github.com/ruijie-wan
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08475v1 Announce Type: new  Abstract: In recent years, the DBLP computer science bibliography has been prominently used for searching scholarly information, such as publications, scholars, and venues. However, its current search service lacks the capability to handle complex queries, which limits the usability of DBLP. In this paper, we present NLQxform-UI, a web-based natural language interface that enables users to query DBLP directly with complex natural language questions. NLQxform-UI automatically translates given questions into SPARQL queries and executes the queries over the DBLP knowledge graph to retrieve answers. The querying process is presented to users in an interactive manner, which improves the transparency of the system and helps examine the returned answers. Also, intermediate results in the querying process can be previewed and manually altered to improve the accuracy of the system. NLQxform-UI has been completely open-sourced: https://github.com/ruijie-wan
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35843;&#26597;&#28145;&#20837;&#20998;&#26512;&#20102;LLMs&#22312;&#34701;&#21512;&#19978;&#19979;&#25991;&#21644;&#21442;&#25968;&#21270;&#30693;&#35782;&#26102;&#25152;&#38754;&#20020;&#30340;&#30693;&#35782;&#20914;&#31361;&#65292;&#25506;&#35752;&#20102;&#19977;&#31867;&#30693;&#35782;&#20914;&#31361;&#23545;&#20854;&#21487;&#20449;&#24230;&#21644;&#24615;&#33021;&#30340;&#37325;&#35201;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#25913;&#36827;LLMs&#31283;&#20581;&#24615;&#31574;&#30053;&#30340;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2403.08319</link><description>&lt;p&gt;
LLMs&#30340;&#30693;&#35782;&#20914;&#31361;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Knowledge Conflicts for LLMs: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08319
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#26597;&#28145;&#20837;&#20998;&#26512;&#20102;LLMs&#22312;&#34701;&#21512;&#19978;&#19979;&#25991;&#21644;&#21442;&#25968;&#21270;&#30693;&#35782;&#26102;&#25152;&#38754;&#20020;&#30340;&#30693;&#35782;&#20914;&#31361;&#65292;&#25506;&#35752;&#20102;&#19977;&#31867;&#30693;&#35782;&#20914;&#31361;&#23545;&#20854;&#21487;&#20449;&#24230;&#21644;&#24615;&#33021;&#30340;&#37325;&#35201;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#25913;&#36827;LLMs&#31283;&#20581;&#24615;&#31574;&#30053;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#26597;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#30693;&#35782;&#20914;&#31361;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#31361;&#20986;&#20102;&#24403;&#23427;&#20204;&#34701;&#21512;&#19978;&#19979;&#25991;&#21644;&#21442;&#25968;&#21270;&#30693;&#35782;&#26102;&#25152;&#36935;&#21040;&#30340;&#22797;&#26434;&#25361;&#25112;&#12290;&#25105;&#20204;&#20851;&#27880;&#19977;&#31867;&#30693;&#35782;&#20914;&#31361;&#65306;&#19978;&#19979;&#25991;-&#35760;&#24518;&#20914;&#31361;&#12289;&#36328;&#19978;&#19979;&#25991;&#20914;&#31361;&#21644;&#20869;&#37096;&#35760;&#24518;&#20914;&#31361;&#12290;&#36825;&#20123;&#20914;&#31361;&#21487;&#33021;&#20250;&#26174;&#33879;&#24433;&#21709;LLMs&#30340;&#21487;&#20449;&#24230;&#21644;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#65292;&#22122;&#38899;&#21644;&#38169;&#35823;&#20449;&#24687;&#24456;&#24120;&#35265;&#12290;&#36890;&#36807;&#23545;&#36825;&#20123;&#20914;&#31361;&#36827;&#34892;&#20998;&#31867;&#65292;&#25506;&#35752;&#20854;&#21407;&#22240;&#65292;&#30740;&#31350;LLMs&#22312;&#36825;&#20123;&#20914;&#31361;&#19979;&#30340;&#34892;&#20026;&#65292;&#24182;&#22238;&#39038;&#21487;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#26412;&#35843;&#26597;&#26088;&#22312;&#20026;&#25913;&#36827;LLMs&#30340;&#31283;&#20581;&#24615;&#31574;&#30053;&#25552;&#20379;&#21551;&#31034;&#65292;&#20174;&#32780;&#25104;&#20026;&#25512;&#21160;&#36825;&#19968;&#19981;&#26029;&#21457;&#23637;&#39046;&#22495;&#30740;&#31350;&#30340;&#23453;&#36149;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08319v1 Announce Type: cross  Abstract: This survey provides an in-depth analysis of knowledge conflicts for large language models (LLMs), highlighting the complex challenges they encounter when blending contextual and parametric knowledge. Our focus is on three categories of knowledge conflicts: context-memory, inter-context, and intra-memory conflict. These conflicts can significantly impact the trustworthiness and performance of LLMs, especially in real-world applications where noise and misinformation are common. By categorizing these conflicts, exploring the causes, examining the behaviors of LLMs under such conflicts, and reviewing available solutions, this survey aims to shed light on strategies for improving the robustness of LLMs, thereby serving as a valuable resource for advancing research in this evolving area.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#25512;&#33616;&#30340;&#36731;&#37327;&#32423;&#31526;&#21495;&#22270;&#21367;&#31215;&#32593;&#32476;LSGRec&#65292;&#37319;&#29992;&#32479;&#19968;&#24314;&#27169;&#26041;&#27861;&#21516;&#26102;&#23545;&#39640;&#38454;&#29992;&#25143;&#30340;&#27491;&#36127;&#20559;&#22909;&#36827;&#34892;&#24314;&#27169;</title><link>https://arxiv.org/abs/2403.08246</link><description>&lt;p&gt;
&#38754;&#21521;&#27491;&#36127;&#20559;&#22909;&#30340;&#32479;&#19968;&#24314;&#27169;&#30340;&#31526;&#21495;&#24863;&#30693;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Towards Unified Modeling for Positive and Negative Preferences in Sign-Aware Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08246
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#25512;&#33616;&#30340;&#36731;&#37327;&#32423;&#31526;&#21495;&#22270;&#21367;&#31215;&#32593;&#32476;LSGRec&#65292;&#37319;&#29992;&#32479;&#19968;&#24314;&#27169;&#26041;&#27861;&#21516;&#26102;&#23545;&#39640;&#38454;&#29992;&#25143;&#30340;&#27491;&#36127;&#20559;&#22909;&#36827;&#34892;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#31526;&#21495;&#24863;&#30693;&#22270;&#25512;&#33616;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#23558;&#20174;&#29992;&#25143;&#19982;&#39033;&#30446;&#20043;&#38388;&#30340;&#27491;&#36127;&#20132;&#20114;&#65288;&#21363;&#65292;&#22270;&#20013;&#30340;&#38142;&#25509;&#65289;&#20013;&#23398;&#20064;&#29992;&#25143;&#30340;&#36127;&#20559;&#22909;&#65292;&#38500;&#20102;&#27491;&#20559;&#22909;&#12290;&#20026;&#20102;&#36866;&#24212;&#36127;&#38142;&#25509;&#21644;&#27491;&#38142;&#25509;&#30340;&#19981;&#21516;&#35821;&#20041;&#65292;&#29616;&#26377;&#20316;&#21697;&#21033;&#29992;&#20004;&#20010;&#29420;&#31435;&#30340;&#32534;&#30721;&#22120;&#20998;&#21035;&#24314;&#27169;&#29992;&#25143;&#30340;&#27491;&#36127;&#20559;&#22909;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#26080;&#27861;&#20174;&#30001;&#22810;&#20010;&#24102;&#26377;&#19981;&#21516;&#31526;&#21495;&#30340;&#38142;&#25509;&#24418;&#25104;&#30340;&#39640;&#38454;&#24322;&#26500;&#20132;&#20114;&#20013;&#23398;&#20064;&#36127;&#20559;&#22909;&#65292;&#23548;&#33268;&#36127;&#29992;&#25143;&#20559;&#22909;&#19981;&#20934;&#30830;&#21644;&#19981;&#23436;&#25972;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#26840;&#25163;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38754;&#21521;&#25512;&#33616;&#30340;&#36731;&#37327;&#32423;&#31526;&#21495;&#22270;&#21367;&#31215;&#32593;&#32476;LSGRec&#65292;&#37319;&#29992;&#32479;&#19968;&#24314;&#27169;&#26041;&#27861;&#21516;&#26102;&#23545;&#39640;&#38454;&#29992;&#25143;&#30340;&#27491;&#36127;&#20559;&#22909;&#36827;&#34892;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08246v1 Announce Type: cross  Abstract: Recently, sign-aware graph recommendation has drawn much attention as it will learn users' negative preferences besides positive ones from both positive and negative interactions (i.e., links in a graph) with items. To accommodate the different semantics of negative and positive links, existing works utilize two independent encoders to model users' positive and negative preferences, respectively. However, these approaches cannot learn the negative preferences from high-order heterogeneous interactions between users and items formed by multiple links with different signs, resulting in inaccurate and incomplete negative user preferences. To cope with these intractable issues, we propose a novel \textbf{L}ight \textbf{S}igned \textbf{G}raph Convolution Network specifically for \textbf{Rec}ommendation (\textbf{LSGRec}), which adopts a unified modeling approach to simultaneously model high-order users' positive and negative preferences on a
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#35821;&#20041;&#26631;&#35760;&#33539;&#24335;&#24182;&#24341;&#20837;&#31163;&#25955;&#35821;&#20041;&#26631;&#35760;&#21270;&#26041;&#27861;UIST&#65292;&#29992;&#20110;&#29992;&#25143;&#21644;&#39033;&#30446;&#34920;&#31034;&#65292;&#26088;&#22312;&#23558;&#39033;&#30446;&#20869;&#23481;&#20449;&#24687;&#25972;&#21512;&#21040;&#28857;&#20987;&#29575;&#65288;CTR&#65289;&#39044;&#27979;&#27169;&#22411;&#20013;&#65292;&#23454;&#29616;&#24555;&#36895;&#35757;&#32451;&#21644;&#25512;&#26029;&#65292;&#24182;&#22312;&#20445;&#25345;&#20869;&#23384;&#21344;&#29992;&#30340;&#21516;&#26102;&#25552;&#39640;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.08206</link><description>&lt;p&gt;
&#29992;&#20110;&#28145;&#24230;CTR&#39044;&#27979;&#30340;&#31163;&#25955;&#35821;&#20041;&#26631;&#35760;&#21270;
&lt;/p&gt;
&lt;p&gt;
Discrete Semantic Tokenization for Deep CTR Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08206
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#35821;&#20041;&#26631;&#35760;&#33539;&#24335;&#24182;&#24341;&#20837;&#31163;&#25955;&#35821;&#20041;&#26631;&#35760;&#21270;&#26041;&#27861;UIST&#65292;&#29992;&#20110;&#29992;&#25143;&#21644;&#39033;&#30446;&#34920;&#31034;&#65292;&#26088;&#22312;&#23558;&#39033;&#30446;&#20869;&#23481;&#20449;&#24687;&#25972;&#21512;&#21040;&#28857;&#20987;&#29575;&#65288;CTR&#65289;&#39044;&#27979;&#27169;&#22411;&#20013;&#65292;&#23454;&#29616;&#24555;&#36895;&#35757;&#32451;&#21644;&#25512;&#26029;&#65292;&#24182;&#22312;&#20445;&#25345;&#20869;&#23384;&#21344;&#29992;&#30340;&#21516;&#26102;&#25552;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#39033;&#30446;&#20869;&#23481;&#20449;&#24687;&#25972;&#21512;&#21040;&#28857;&#20987;&#29575;&#65288;CTR&#65289;&#39044;&#27979;&#27169;&#22411;&#20013;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#23588;&#20854;&#26159;&#22312;&#24037;&#19994;&#22330;&#26223;&#19979;&#30340;&#26102;&#38388;&#21644;&#31354;&#38388;&#32422;&#26463;&#19979;&#12290;&#20256;&#32479;&#30340;&#20869;&#23481;&#32534;&#30721;&#33539;&#24335;&#23558;&#29992;&#25143;&#21644;&#39033;&#30446;&#32534;&#30721;&#22120;&#30452;&#25509;&#25972;&#21512;&#21040;CTR&#27169;&#22411;&#20013;&#65292;&#20248;&#20808;&#32771;&#34385;&#31354;&#38388;&#32780;&#38750;&#26102;&#38388;&#12290;&#30456;&#21453;&#65292;&#22522;&#20110;&#23884;&#20837;&#30340;&#33539;&#24335;&#23558;&#39033;&#30446;&#21644;&#29992;&#25143;&#35821;&#20041;&#36716;&#25442;&#20026;&#28508;&#22312;&#23884;&#20837;&#65292;&#28982;&#21518;&#23545;&#20854;&#36827;&#34892;&#32531;&#23384;&#65292;&#20248;&#20808;&#32771;&#34385;&#31354;&#38388;&#32780;&#38750;&#26102;&#38388;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#35821;&#20041;&#26631;&#35760;&#33539;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#29992;&#25143;&#21644;&#39033;&#30446;&#34920;&#31034;&#30340;&#31163;&#25955;&#35821;&#20041;&#26631;&#35760;&#21270;&#26041;&#27861;&#65292;&#21363;UIST&#12290;UIST&#23454;&#29616;&#20102;&#24555;&#36895;&#30340;&#35757;&#32451;&#21644;&#25512;&#26029;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#20445;&#23432;&#30340;&#20869;&#23384;&#21344;&#29992;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;UIST&#23558;&#23494;&#38598;&#23884;&#20837;&#21521;&#37327;&#37327;&#21270;&#20026;&#36739;&#30701;&#30340;&#31163;&#25955;&#26631;&#35760;&#65292;&#24182;&#37319;&#29992;&#20998;&#23618;&#28151;&#21512;&#25512;&#26029;&#27169;&#22359;&#26469;&#34913;&#37327;&#27599;&#20010;&#29992;&#25143;-&#39033;&#30446;&#26631;&#35760;&#23545;&#30340;&#36129;&#29486;&#12290;&#25105;&#20204;&#22312;&#26032;&#38395;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;UIST&#22312;&#25552;&#39640;&#25928;&#29575;&#30340;&#21516;&#26102;&#38477;&#20302;&#20102;&#20869;&#23384;&#28040;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08206v1 Announce Type: new  Abstract: Incorporating item content information into click-through rate (CTR) prediction models remains a challenge, especially with the time and space constraints of industrial scenarios. The content-encoding paradigm, which integrates user and item encoders directly into CTR models, prioritizes space over time. In contrast, the embedding-based paradigm transforms item and user semantics into latent embeddings and then caches them, prioritizes space over time. In this paper, we introduce a new semantic-token paradigm and propose a discrete semantic tokenization approach, namely UIST, for user and item representation. UIST facilitates swift training and inference while maintaining a conservative memory footprint. Specifically, UIST quantizes dense embedding vectors into discrete tokens with shorter lengths and employs a hierarchical mixture inference module to weigh the contribution of each user--item token pair. Our experimental results on news 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Meta-Split&#32593;&#32476;&#65288;MSN&#65289;&#26469;&#35299;&#20915;&#28040;&#36153;&#32773;&#20043;&#38388;&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#20013;&#38480;&#37327;&#24211;&#23384;&#20135;&#21697;&#25512;&#33616;&#20013;&#30340;&#29420;&#29305;&#25361;&#25112;&#65292;&#36890;&#36807;&#20998;&#21106;&#29992;&#25143;&#21382;&#21490;&#24207;&#21015;&#26469;&#26377;&#25928;&#21033;&#29992;&#29992;&#25143;&#21382;&#21490;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2403.06747</link><description>&lt;p&gt;
MetaSplit: &#29992;&#20110;&#38480;&#37327;&#20135;&#21697;&#25512;&#33616;&#30340;Meta-Split&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
MetaSplit: Meta-Split Network for Limited-Stock Product Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06747
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Meta-Split&#32593;&#32476;&#65288;MSN&#65289;&#26469;&#35299;&#20915;&#28040;&#36153;&#32773;&#20043;&#38388;&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#20013;&#38480;&#37327;&#24211;&#23384;&#20135;&#21697;&#25512;&#33616;&#20013;&#30340;&#29420;&#29305;&#25361;&#25112;&#65292;&#36890;&#36807;&#20998;&#21106;&#29992;&#25143;&#21382;&#21490;&#24207;&#21015;&#26469;&#26377;&#25928;&#21033;&#29992;&#29992;&#25143;&#21382;&#21490;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#23545;&#20110;&#38754;&#21521;&#28040;&#36153;&#32773;&#30340;&#30005;&#23376;&#21830;&#21153;&#31995;&#32479;&#65292;&#28040;&#36153;&#32773;&#20043;&#38388;&#30340;&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#36890;&#24120;&#20250;&#36935;&#21040;&#38480;&#37327;&#24211;&#23384;&#38382;&#39064;&#65292;&#21363;&#20135;&#21697;&#22312;C2C&#31995;&#32479;&#20013;&#21482;&#33021;&#38144;&#21806;&#19968;&#27425;&#12290;&#36825;&#20026;&#28857;&#20987;&#29575;&#65288;CTR&#65289;&#39044;&#27979;&#24102;&#26469;&#20102;&#20960;&#20010;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#37492;&#20110;&#27599;&#20010;&#20135;&#21697;&#65288;&#21363;&#21830;&#21697;&#65289;&#30340;&#26377;&#38480;&#29992;&#25143;&#20132;&#20114;&#65292;CTR&#27169;&#22411;&#20013;&#23545;&#24212;&#30340;&#21830;&#21697;&#23884;&#20837;&#21487;&#33021;&#19981;&#23481;&#26131;&#25910;&#25947;&#12290;&#36825;&#20351;&#24471;&#20256;&#32479;&#22522;&#20110;&#24207;&#21015;&#24314;&#27169;&#30340;&#26041;&#27861;&#26080;&#27861;&#26377;&#25928;&#21033;&#29992;&#29992;&#25143;&#21382;&#21490;&#20449;&#24687;&#65292;&#22240;&#20026;&#21382;&#21490;&#29992;&#25143;&#34892;&#20026;&#21253;&#21547;&#20102;&#19981;&#21516;&#24211;&#23384;&#37327;&#30340;&#21830;&#21697;&#28151;&#21512;&#12290;&#29305;&#21035;&#26159;&#65292;&#24207;&#21015;&#27169;&#22411;&#20013;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#20542;&#21521;&#20110;&#23558;&#26356;&#22810;&#32047;&#31215;&#29992;&#25143;&#20132;&#20114;&#30340;&#20135;&#21697;&#20998;&#37197;&#26356;&#39640;&#30340;&#20998;&#25968;&#65292;&#23548;&#33268;&#38480;&#37327;&#20135;&#21697;&#34987;&#24573;&#35270;&#19988;&#23545;&#26368;&#32456;&#36755;&#20986;&#30340;&#36129;&#29486;&#36739;&#23569;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Meta-Split&#32593;&#32476;&#65288;MSN&#65289;&#26469;&#20998;&#21106;&#29992;&#25143;&#21382;&#21490;&#24207;&#21015;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06747v1 Announce Type: new  Abstract: Compared to business-to-consumer (B2C) e-commerce systems, consumer-to-consumer (C2C) e-commerce platforms usually encounter the limited-stock problem, that is, a product can only be sold one time in a C2C system. This poses several unique challenges for click-through rate (CTR) prediction. Due to limited user interactions for each product (i.e. item), the corresponding item embedding in the CTR model may not easily converge. This makes the conventional sequence modeling based approaches cannot effectively utilize user history information since historical user behaviors contain a mixture of items with different volume of stocks. Particularly, the attention mechanism in a sequence model tends to assign higher score to products with more accumulated user interactions, making limited-stock products being ignored and contribute less to the final output. To this end, we propose the Meta-Split Network (MSN) to split user history sequence regar
&lt;/p&gt;</description></item><item><title>GPT-4V(ision)&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#32593;&#32476;&#20195;&#29702;&#65292;&#20855;&#26377;&#32508;&#21512;&#35270;&#35273;&#29702;&#35299;&#21644;&#32593;&#39029;&#25805;&#20316;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22914;&#26524;&#23558;&#25991;&#26412;&#35745;&#21010;&#36716;&#21270;&#20026;&#23454;&#38469;&#34892;&#21160;&#65292;GPT-4V&#21487;&#20197;&#22312;50%&#30340;&#20219;&#21153;&#19978;&#21462;&#24471;&#25104;&#21151;&#12290;&#36825;&#19968;&#32467;&#26524;&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.01614</link><description>&lt;p&gt;
GPT-4V(ision)&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#32593;&#32476;&#20195;&#29702;&#65292;&#22914;&#26524;&#26377;&#22522;&#30784;&#30340;&#35805;&#12290;
&lt;/p&gt;
&lt;p&gt;
GPT-4V(ision) is a Generalist Web Agent, if Grounded. (arXiv:2401.01614v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01614
&lt;/p&gt;
&lt;p&gt;
GPT-4V(ision)&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#32593;&#32476;&#20195;&#29702;&#65292;&#20855;&#26377;&#32508;&#21512;&#35270;&#35273;&#29702;&#35299;&#21644;&#32593;&#39029;&#25805;&#20316;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22914;&#26524;&#23558;&#25991;&#26412;&#35745;&#21010;&#36716;&#21270;&#20026;&#23454;&#38469;&#34892;&#21160;&#65292;GPT-4V&#21487;&#20197;&#22312;50%&#30340;&#20219;&#21153;&#19978;&#21462;&#24471;&#25104;&#21151;&#12290;&#36825;&#19968;&#32467;&#26524;&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#23545;&#22823;&#22411;&#22810;&#27169;&#22411;&#65288;LMM&#65289;&#30340;&#30740;&#31350;&#65292;&#29305;&#21035;&#26159;GPT-4V(ision)&#21644;Gemini&#65292;&#24555;&#36895;&#25512;&#21160;&#20102;&#22810;&#27169;&#22411;&#30340;&#33021;&#21147;&#36793;&#30028;&#36229;&#36234;&#20256;&#32479;&#20219;&#21153;&#65292;&#22914;&#22270;&#20687;&#23383;&#24149;&#21644;&#35270;&#35273;&#38382;&#31572;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20687;GPT-4V&#36825;&#26679;&#30340;LMM&#20316;&#20026;&#36890;&#29992;&#32593;&#32476;&#20195;&#29702;&#30340;&#28508;&#21147;&#65292;&#21487;&#20197;&#26681;&#25454;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#22312;&#20219;&#20309;&#32473;&#23450;&#30340;&#32593;&#31449;&#19978;&#23436;&#25104;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;SEEACT&#65292;&#19968;&#31181;&#21033;&#29992;LMM&#30340;&#21147;&#37327;&#36827;&#34892;&#32508;&#21512;&#35270;&#35273;&#29702;&#35299;&#21644;&#32593;&#39029;&#25805;&#20316;&#30340;&#36890;&#29992;&#32593;&#32476;&#20195;&#29702;&#12290;&#25105;&#20204;&#22312;&#26368;&#26032;&#30340;MIND2WEB&#22522;&#20934;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;&#38500;&#20102;&#23545;&#32531;&#23384;&#32593;&#31449;&#30340;&#26631;&#20934;&#31163;&#32447;&#35780;&#20272;&#22806;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#20801;&#35768;&#22312;&#23454;&#26102;&#32593;&#31449;&#19978;&#36816;&#34892;&#32593;&#32476;&#20195;&#29702;&#30340;&#24037;&#20855;&#65292;&#23454;&#29616;&#20102;&#19968;&#31181;&#26032;&#30340;&#22312;&#32447;&#35780;&#20272;&#35774;&#32622;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;GPT-4V&#22312;&#32593;&#39029;&#20195;&#29702;&#26041;&#38754;&#34920;&#29616;&#20986;&#24040;&#22823;&#30340;&#28508;&#21147;-&#22914;&#26524;&#25105;&#20204;&#23558;&#20854;&#25991;&#26412;&#35745;&#21010;&#25163;&#21160;&#22320;&#23454;&#26045;&#20026;&#32593;&#31449;&#19978;&#30340;&#34892;&#21160;&#65292;&#23427;&#21487;&#20197;&#25104;&#21151;&#22320;&#23436;&#25104;50%&#30340;&#20219;&#21153;&#12290;&#27492;&#32467;&#26524;&#26126;&#26174;&#36229;&#36807;&#20102;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent development on large multimodal models (LMMs), especially GPT-4V(ision) and Gemini, has been quickly expanding the capability boundaries of multimodal models beyond traditional tasks like image captioning and visual question answering. In this work, we explore the potential of LMMs like GPT-4V as a generalist web agent that can follow natural language instructions to complete tasks on any given website. We propose SEEACT, a generalist web agent that harnesses the power of LMMs for integrated visual understanding and acting on the web. We evaluate on the recent MIND2WEB benchmark. In addition to standard offline evaluation on cached websites, we enable a new online evaluation setting by developing a tool that allows running web agents on live websites. We show that GPT-4V presents a great potential for web agents - it can successfully complete 50% of the tasks on live websites if we manually ground its textual plans into actions on the websites. This substantially outperforms
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33021;&#22815;&#25552;&#39640;&#23545;ChatGPT&#29983;&#25104;&#30340;&#20551;&#31185;&#23398;&#36827;&#34892;&#26816;&#27979;&#30340;&#31639;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#26032;&#35774;&#35745;&#30340;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#20934;&#30830;&#22320;&#23558;&#26426;&#22120;&#29983;&#25104;&#30340;&#20986;&#29256;&#29289;&#19982;&#31185;&#23398;&#23478;&#29983;&#25104;&#30340;&#20986;&#29256;&#29289;&#21306;&#20998;&#24320;&#26469;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;ChatGPT&#22312;&#25216;&#26415;&#26415;&#35821;&#26041;&#38754;&#19982;&#30495;&#23454;&#31185;&#23398;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#31639;&#27861;&#22312;&#20998;&#31867;&#36807;&#31243;&#20013;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.11767</link><description>&lt;p&gt;
&#25552;&#39640;ChatGPT&#29983;&#25104;&#30340;&#20551;&#31185;&#23398;&#26816;&#27979;&#30340;&#26041;&#27861;&#65306;&#24341;&#20837;xFakeBibs&#30417;&#30563;&#23398;&#20064;&#32593;&#32476;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Improving Detection of ChatGPT-Generated Fake Science Using Real Publication Text: Introducing xFakeBibs a Supervised-Learning Network Algorithm. (arXiv:2308.11767v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11767
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33021;&#22815;&#25552;&#39640;&#23545;ChatGPT&#29983;&#25104;&#30340;&#20551;&#31185;&#23398;&#36827;&#34892;&#26816;&#27979;&#30340;&#31639;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#26032;&#35774;&#35745;&#30340;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#20934;&#30830;&#22320;&#23558;&#26426;&#22120;&#29983;&#25104;&#30340;&#20986;&#29256;&#29289;&#19982;&#31185;&#23398;&#23478;&#29983;&#25104;&#30340;&#20986;&#29256;&#29289;&#21306;&#20998;&#24320;&#26469;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;ChatGPT&#22312;&#25216;&#26415;&#26415;&#35821;&#26041;&#38754;&#19982;&#30495;&#23454;&#31185;&#23398;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#31639;&#27861;&#22312;&#20998;&#31867;&#36807;&#31243;&#20013;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#27491;&#22312;&#25104;&#20026;&#29616;&#23454;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#21306;&#20998;ChatGPT&#29983;&#25104;&#30340;&#20986;&#29256;&#29289;&#19982;&#31185;&#23398;&#23478;&#29983;&#25104;&#30340;&#20986;&#29256;&#29289;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#26032;&#35774;&#35745;&#30340;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#22914;&#20309;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#30340;&#20986;&#29256;&#29289;&#21644;&#31185;&#23398;&#23478;&#29983;&#25104;&#30340;&#20986;&#29256;&#29289;&#12290;&#35813;&#31639;&#27861;&#20351;&#29992;100&#20010;&#30495;&#23454;&#20986;&#29256;&#29289;&#25688;&#35201;&#36827;&#34892;&#35757;&#32451;&#65292;&#28982;&#21518;&#37319;&#29992;10&#20493;&#20132;&#21449;&#39564;&#35777;&#26041;&#27861;&#24314;&#31435;&#20102;&#19968;&#20010;&#25509;&#21463;&#33539;&#22260;&#30340;&#19979;&#38480;&#21644;&#19978;&#38480;&#12290;&#19982;ChatGPT&#20869;&#23481;&#36827;&#34892;&#27604;&#36739;&#65292;&#26126;&#26174;&#21487;&#35265;ChatGPT&#20165;&#36129;&#29486;&#20102;23\%&#30340;&#20108;&#20803;&#32452;&#20869;&#23481;&#65292;&#36825;&#27604;&#20854;&#20182;10&#20010;&#20132;&#21449;&#39564;&#35777;&#20013;&#30340;&#20219;&#20309;&#19968;&#20010;&#37117;&#23569;50\%&#12290;&#36825;&#20010;&#20998;&#26512;&#20984;&#26174;&#20102;ChatGPT&#22312;&#25216;&#26415;&#26415;&#35821;&#19978;&#19982;&#30495;&#23454;&#31185;&#23398;&#30340;&#26126;&#26174;&#24046;&#24322;&#12290;&#22312;&#23545;&#27599;&#31687;&#25991;&#31456;&#36827;&#34892;&#20998;&#31867;&#26102;&#65292;xFakeBibs&#31639;&#27861;&#20934;&#30830;&#22320;&#23558;98&#31687;&#20986;&#29256;&#29289;&#35782;&#21035;&#20026;&#20551;&#30340;&#65292;&#26377;2&#31687;&#25991;&#29486;&#38169;&#35823;&#22320;&#20998;&#31867;&#20026;&#30495;&#23454;&#20986;&#29256;&#29289;&#12290;&#23613;&#31649;&#36825;&#39033;&#24037;&#20316;&#24341;&#20837;&#20102;&#19968;&#31181;&#31639;&#27861;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
ChatGPT is becoming a new reality. In this paper, we show how to distinguish ChatGPT-generated publications from counterparts produced by scientists. Using a newly designed supervised Machine Learning algorithm, we demonstrate how to detect machine-generated publications from those produced by scientists. The algorithm was trained using 100 real publication abstracts, followed by a 10-fold calibration approach to establish a lower-upper bound range of acceptance. In the comparison with ChatGPT content, it was evident that ChatGPT contributed merely 23\% of the bigram content, which is less than 50\% of any of the other 10 calibrating folds. This analysis highlights a significant disparity in technical terms where ChatGPT fell short of matching real science. When categorizing the individual articles, the xFakeBibs algorithm accurately identified 98 out of 100 publications as fake, with 2 articles incorrectly classified as real publications. Though this work introduced an algorithmic app
&lt;/p&gt;</description></item></channel></rss>