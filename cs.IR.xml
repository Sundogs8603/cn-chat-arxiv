<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#29616;&#26377;&#30740;&#31350;&#24050;&#32463;&#34920;&#26126;&#65292;&#36890;&#36807;&#25913;&#36827;&#23545;&#40784;&#21644;&#22343;&#21248;&#24615;&#35774;&#35745;&#30340;&#25439;&#22833;&#20989;&#25968;&#21487;&#20197;&#23454;&#29616;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#31216;&#20026;MAWU&#65292;&#23427;&#32771;&#34385;&#20102;&#25968;&#25454;&#38598;&#30340;&#29420;&#29305;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2308.06091</link><description>&lt;p&gt;
&#23545;&#21327;&#21516;&#36807;&#28388;&#20002;&#22833;&#20989;&#25968;&#30340;&#26356;&#22909;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Toward a Better Understanding of Loss Functions for Collaborative Filtering. (arXiv:2308.06091v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06091
&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30740;&#31350;&#24050;&#32463;&#34920;&#26126;&#65292;&#36890;&#36807;&#25913;&#36827;&#23545;&#40784;&#21644;&#22343;&#21248;&#24615;&#35774;&#35745;&#30340;&#25439;&#22833;&#20989;&#25968;&#21487;&#20197;&#23454;&#29616;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#31216;&#20026;MAWU&#65292;&#23427;&#32771;&#34385;&#20102;&#25968;&#25454;&#38598;&#30340;&#29420;&#29305;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21327;&#21516;&#36807;&#28388;&#65288;CF&#65289;&#26159;&#29616;&#20195;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#20851;&#38190;&#25216;&#26415;&#12290;CF&#27169;&#22411;&#30340;&#23398;&#20064;&#36807;&#31243;&#36890;&#24120;&#30001;&#19977;&#20010;&#32452;&#20214;&#32452;&#25104;&#65306;&#20132;&#20114;&#32534;&#30721;&#22120;&#12289;&#25439;&#22833;&#20989;&#25968;&#21644;&#36127;&#37319;&#26679;&#12290;&#23613;&#31649;&#35768;&#22810;&#29616;&#26377;&#30740;&#31350;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;CF&#27169;&#22411;&#26469;&#35774;&#35745;&#22797;&#26434;&#30340;&#20132;&#20114;&#32534;&#30721;&#22120;&#65292;&#20294;&#26368;&#36817;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#31616;&#21333;&#22320;&#37325;&#26032;&#21046;&#23450;&#25439;&#22833;&#20989;&#25968;&#21487;&#20197;&#23454;&#29616;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#26412;&#25991;&#28145;&#20837;&#20998;&#26512;&#20102;&#29616;&#26377;&#25439;&#22833;&#20989;&#25968;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#25968;&#23398;&#20998;&#26512;&#25581;&#31034;&#20102;&#20808;&#21069;&#30340;&#25439;&#22833;&#20989;&#25968;&#21487;&#20197;&#35299;&#37322;&#20026;&#23545;&#40784;&#21644;&#22343;&#21248;&#24615;&#20989;&#25968;&#65306;&#65288;i&#65289;&#23545;&#40784;&#21305;&#37197;&#29992;&#25143;&#21644;&#29289;&#21697;&#34920;&#31034;&#65292;&#65288;ii&#65289;&#22343;&#21248;&#24615;&#20998;&#25955;&#29992;&#25143;&#21644;&#29289;&#21697;&#20998;&#24067;&#12290;&#21463;&#21040;&#36825;&#20010;&#20998;&#26512;&#30340;&#21551;&#31034;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#23545;&#40784;&#21644;&#22343;&#21248;&#24615;&#35774;&#35745;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#32771;&#34385;&#21040;&#25968;&#25454;&#38598;&#30340;&#29420;&#29305;&#27169;&#24335;&#65292;&#31216;&#20026;Margin-aware Alignment and Weighted Uniformity&#65288;MAWU&#65289;&#12290;MAWU&#30340;&#20851;&#38190;&#21019;&#26032;&#26159;
&lt;/p&gt;
&lt;p&gt;
Collaborative filtering (CF) is a pivotal technique in modern recommender systems. The learning process of CF models typically consists of three components: interaction encoder, loss function, and negative sampling. Although many existing studies have proposed various CF models to design sophisticated interaction encoders, recent work shows that simply reformulating the loss functions can achieve significant performance gains. This paper delves into analyzing the relationship among existing loss functions. Our mathematical analysis reveals that the previous loss functions can be interpreted as alignment and uniformity functions: (i) the alignment matches user and item representations, and (ii) the uniformity disperses user and item distributions. Inspired by this analysis, we propose a novel loss function that improves the design of alignment and uniformity considering the unique patterns of datasets called Margin-aware Alignment and Weighted Uniformity (MAWU). The key novelty of MAWU 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#22522;&#20110;&#27169;&#22411;&#30340;&#26641;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#65288;MOB-HSMM&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#22797;&#26434;&#40657;&#30418;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24212;&#29992;&#20110;&#24207;&#21015;&#25968;&#25454;&#26102;&#30340;&#21487;&#35299;&#37322;&#24615;&#38382;&#39064;&#12290;&#36890;&#36807;&#20174;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#33976;&#39311;&#30340;&#30693;&#35782;&#65292;&#23454;&#29616;&#20102;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#30340;&#21516;&#26102;&#25552;&#20379;&#28165;&#26224;&#35299;&#37322;&#30340;&#30446;&#26631;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#36890;&#36807;&#23558;LSTM&#23398;&#20064;&#21040;&#30340;&#39034;&#24207;&#27169;&#24335;&#36716;&#31227;&#21040;MOB&#26641;&#20013;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;MOB&#26641;&#30340;&#24615;&#33021;&#65292;&#24182;&#21033;&#29992;MOB-HSMM&#23558;MOB&#26641;&#19982;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#65288;HSMM&#65289;&#25972;&#21512;&#65292;&#23454;&#29616;&#20102;&#28508;&#22312;&#21644;&#21487;&#35299;&#37322;&#30340;&#24207;&#21015;&#30340;&#21457;&#29616;&#12290;</title><link>http://arxiv.org/abs/2307.15367</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#27169;&#22411;&#30340;&#26641;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#65292;&#23454;&#29616;&#36879;&#26126;&#30340;&#24207;&#21015;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Toward Transparent Sequence Models with Model-Based Tree Markov Model. (arXiv:2307.15367v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15367
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#22522;&#20110;&#27169;&#22411;&#30340;&#26641;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#65288;MOB-HSMM&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#22797;&#26434;&#40657;&#30418;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24212;&#29992;&#20110;&#24207;&#21015;&#25968;&#25454;&#26102;&#30340;&#21487;&#35299;&#37322;&#24615;&#38382;&#39064;&#12290;&#36890;&#36807;&#20174;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#33976;&#39311;&#30340;&#30693;&#35782;&#65292;&#23454;&#29616;&#20102;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#30340;&#21516;&#26102;&#25552;&#20379;&#28165;&#26224;&#35299;&#37322;&#30340;&#30446;&#26631;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#36890;&#36807;&#23558;LSTM&#23398;&#20064;&#21040;&#30340;&#39034;&#24207;&#27169;&#24335;&#36716;&#31227;&#21040;MOB&#26641;&#20013;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;MOB&#26641;&#30340;&#24615;&#33021;&#65292;&#24182;&#21033;&#29992;MOB-HSMM&#23558;MOB&#26641;&#19982;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#65288;HSMM&#65289;&#25972;&#21512;&#65292;&#23454;&#29616;&#20102;&#28508;&#22312;&#21644;&#21487;&#35299;&#37322;&#30340;&#24207;&#21015;&#30340;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#24212;&#29992;&#20110;&#24207;&#21015;&#25968;&#25454;&#30340;&#22797;&#26434;&#12289;&#40657;&#30418;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#27169;&#22411;&#30340;&#26641;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#65288;MOB-HSMM&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#22266;&#26377;&#21487;&#35299;&#37322;&#24615;&#30340;&#27169;&#22411;&#65292;&#26088;&#22312;&#26816;&#27979;&#39640;&#27515;&#20129;&#39118;&#38505;&#20107;&#20214;&#65292;&#24182;&#21457;&#29616;&#19982;&#27515;&#20129;&#39118;&#38505;&#30456;&#20851;&#30340;&#38544;&#34255;&#27169;&#24335;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#20174;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#20013;&#33976;&#39311;&#30340;&#30693;&#35782;&#65292;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#30340;&#21516;&#26102;&#25552;&#20379;&#28165;&#26224;&#30340;&#35299;&#37322;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#20351;&#29992;LSTM&#23398;&#20064;&#39034;&#24207;&#27169;&#24335;&#65292;&#36827;&#32780;&#23558;&#20854;&#36716;&#31227;&#32473;MOB&#26641;&#65292;&#21487;&#20197;&#25552;&#39640;&#22522;&#20110;&#27169;&#22411;&#30340;&#26641;&#65288;MOB&#26641;&#65289;&#30340;&#24615;&#33021;&#12290;&#23558;MOB&#26641;&#19982;&#22522;&#20110;&#27169;&#22411;&#30340;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#65288;HSMM&#65289;&#38598;&#25104;&#22312;MOB-HSMM&#20013;&#65292;&#21487;&#20197;&#20351;&#29992;&#21487;&#29992;&#20449;&#24687;&#25581;&#31034;&#28508;&#22312;&#30340;&#21644;&#21487;&#35299;&#37322;&#30340;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we address the interpretability issue in complex, black-box Machine Learning models applied to sequence data. We introduce the Model-Based tree Hidden Semi-Markov Model (MOB-HSMM), an inherently interpretable model aimed at detecting high mortality risk events and discovering hidden patterns associated with the mortality risk in Intensive Care Units (ICU). This model leverages knowledge distilled from Deep Neural Networks (DNN) to enhance predictive performance while offering clear explanations. Our experimental results indicate the improved performance of Model-Based trees (MOB trees) via employing LSTM for learning sequential patterns, which are then transferred to MOB trees. Integrating MOB trees with the Hidden Semi-Markov Model (HSMM) in the MOB-HSMM enables uncovering potential and explainable sequences using available information.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#19987;&#21033;&#25991;&#20214;&#20013;&#25552;&#21462;&#24037;&#31243;&#35774;&#35745;&#30693;&#35782;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#30693;&#35782;&#22270;&#26469;&#22635;&#20805;&#36890;&#29992;&#35774;&#35745;&#30693;&#35782;&#65292;&#24182;&#19982;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2307.06985</link><description>&lt;p&gt;
&#36808;&#21521;&#22635;&#20805;&#36890;&#29992;&#24037;&#31243;&#35774;&#35745;&#30693;&#35782;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Populating Generalizable Engineering Design Knowledge. (arXiv:2307.06985v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06985
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#19987;&#21033;&#25991;&#20214;&#20013;&#25552;&#21462;&#24037;&#31243;&#35774;&#35745;&#30693;&#35782;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#30693;&#35782;&#22270;&#26469;&#22635;&#20805;&#36890;&#29992;&#35774;&#35745;&#30693;&#35782;&#65292;&#24182;&#19982;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22635;&#20805;&#36890;&#29992;&#24037;&#31243;&#35774;&#35745;&#30693;&#35782;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#19987;&#21033;&#25991;&#20214;&#20013;&#25552;&#21462;head entity :: relationship :: tail entity&#24418;&#24335;&#20107;&#23454;&#30340;&#26041;&#27861;&#12290;&#36825;&#20123;&#20107;&#23454;&#21487;&#20197;&#22312;&#19987;&#21033;&#25991;&#20214;&#20869;&#37096;&#21644;&#36328;&#25991;&#20214;&#20043;&#38388;&#32452;&#21512;&#24418;&#25104;&#30693;&#35782;&#22270;&#65292;&#29992;&#20316;&#34920;&#31034;&#21644;&#23384;&#20648;&#35774;&#35745;&#30693;&#35782;&#30340;&#26041;&#26696;&#12290;&#29616;&#26377;&#30340;&#24037;&#31243;&#35774;&#35745;&#25991;&#29486;&#20013;&#30340;&#26041;&#27861;&#36890;&#24120;&#21033;&#29992;&#19968;&#32452;&#39044;&#23450;&#20041;&#30340;&#20851;&#31995;&#26469;&#22635;&#20805;&#32479;&#35745;&#36817;&#20284;&#32780;&#38750;&#20107;&#23454;&#30340;&#19977;&#20803;&#32452;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#35757;&#32451;&#19968;&#20010;&#26631;&#35760;&#22120;&#26469;&#35782;&#21035;&#21477;&#23376;&#20013;&#30340;&#23454;&#20307;&#21644;&#20851;&#31995;&#12290;&#22312;&#30830;&#23450;&#20102;&#19968;&#23545;&#23454;&#20307;&#21518;&#65292;&#25105;&#20204;&#35757;&#32451;&#21478;&#19968;&#20010;&#26631;&#35760;&#22120;&#26469;&#35782;&#21035;&#29305;&#23450;&#34920;&#31034;&#36825;&#23545;&#23454;&#20307;&#20043;&#38388;&#20851;&#31995;&#30340;&#20851;&#31995;&#26631;&#35760;&#12290;&#20026;&#20102;&#35757;&#32451;&#36825;&#20123;&#26631;&#35760;&#22120;&#65292;&#25105;&#20204;&#25163;&#21160;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;44,227&#20010;&#21477;&#23376;&#21644;&#30456;&#24212;&#20107;&#23454;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36824;&#23558;&#35813;&#26041;&#27861;&#30340;&#24615;&#33021;&#19982;&#36890;&#24120;&#25512;&#33616;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#20854;&#20013;&#25105;&#20204;&#39044;.
&lt;/p&gt;
&lt;p&gt;
Aiming to populate generalizable engineering design knowledge, we propose a method to extract facts of the form head entity :: relationship :: tail entity from sentences found in patent documents. These facts could be combined within and across patent documents to form knowledge graphs that serve as schemes for representing as well as storing design knowledge. Existing methods in engineering design literature often utilise a set of predefined relationships to populate triples that are statistical approximations rather than facts. In our method, we train a tagger to identify both entities and relationships from a sentence. Given a pair of entities thus identified, we train another tagger to identify the relationship tokens that specifically denote the relationship between the pair. For training these taggers, we manually construct a dataset of 44,227 sentences and corresponding facts. We also compare the performance of the method against typically recommended approaches, wherein, we pre
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#28857;&#20987;&#27169;&#22411;&#19981;&#21487;&#30693;&#30340;&#32479;&#19968;&#38750;&#21516;&#31574;&#30053;&#23398;&#20064;&#25490;&#24207;&#65288;CUOLR&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30452;&#25509;&#23398;&#20064;&#26368;&#20248;&#25490;&#21517;&#65292;&#21487;&#20197;&#36731;&#26494;&#22320;&#24212;&#29992;&#20110;&#21508;&#31181;&#28857;&#20987;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.07528</link><description>&lt;p&gt;
&#32479;&#19968;&#30340;&#38750;&#21516;&#31574;&#30053;&#23398;&#20064;&#25490;&#24207;&#65306;&#24378;&#21270;&#23398;&#20064;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Unified Off-Policy Learning to Rank: a Reinforcement Learning Perspective. (arXiv:2306.07528v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07528
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#28857;&#20987;&#27169;&#22411;&#19981;&#21487;&#30693;&#30340;&#32479;&#19968;&#38750;&#21516;&#31574;&#30053;&#23398;&#20064;&#25490;&#24207;&#65288;CUOLR&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30452;&#25509;&#23398;&#20064;&#26368;&#20248;&#25490;&#21517;&#65292;&#21487;&#20197;&#36731;&#26494;&#22320;&#24212;&#29992;&#20110;&#21508;&#31181;&#28857;&#20987;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#21516;&#31574;&#30053;&#23398;&#20064;&#25490;&#24207;&#65288;LTR&#65289;&#26088;&#22312;&#36890;&#36807;&#24050;&#37096;&#32626;&#30340;&#35760;&#24405;&#31574;&#30053;&#25910;&#38598;&#30340;&#25968;&#25454;&#20248;&#21270;&#25490;&#21517;&#22120;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#38750;&#21516;&#31574;&#30053;&#23398;&#20064;&#25490;&#24207;&#26041;&#27861;&#32463;&#24120;&#23545;&#29992;&#25143;&#22914;&#20309;&#29983;&#25104;&#28857;&#20987;&#25968;&#25454;&#21363;&#28857;&#20987;&#27169;&#22411;&#36827;&#34892;&#20551;&#35774;&#65292;&#22240;&#27492;&#38656;&#35201;&#26681;&#25454;&#19981;&#21516;&#30340;&#28857;&#20987;&#27169;&#22411;&#19987;&#38376;&#35843;&#25972;&#20182;&#20204;&#30340;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#25490;&#21517;&#36807;&#31243;&#22312;&#19968;&#33324;&#38543;&#26426;&#28857;&#20987;&#27169;&#22411;&#19979;&#32479;&#19968;&#20026;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#65292;&#36890;&#36807;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#65292;&#21487;&#20197;&#30452;&#25509;&#23398;&#20064;&#26368;&#20248;&#25490;&#21517;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#21033;&#29992;&#31163;&#32447;RL&#25216;&#26415;&#36827;&#34892;&#38750;&#21516;&#31574;&#30053;LTR&#65292;&#24182;&#25552;&#20986;&#28857;&#20987;&#27169;&#22411;&#19981;&#21487;&#30693;&#30340;&#32479;&#19968;&#38750;&#21516;&#31574;&#30053;&#23398;&#20064;&#25490;&#24207;&#65288;CUOLR&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#36731;&#26494;&#22320;&#24212;&#29992;&#20110;&#21508;&#31181;&#28857;&#20987;&#27169;&#22411;&#12290;&#36890;&#36807;&#23545;MDP&#30340;&#19987;&#38376;&#21046;&#23450;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#31163;&#32447;RL&#31639;&#27861;&#21487;&#20197;&#36866;&#24212;&#21508;&#31181;&#28857;&#20987;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#22797;&#26434;&#30340;&#21435;&#20559;&#20506;&#25216;&#26415;&#21644;&#20808;&#39564;&#30693;&#35782;&#12290;&#22312;&#21508;&#31181;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#37117;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Off-policy Learning to Rank (LTR) aims to optimize a ranker from data collected by a deployed logging policy. However, existing off-policy learning to rank methods often make strong assumptions about how users generate the click data, i.e., the click model, and hence need to tailor their methods specifically under different click models. In this paper, we unified the ranking process under general stochastic click models as a Markov Decision Process (MDP), and the optimal ranking could be learned with offline reinforcement learning (RL) directly. Building upon this, we leverage offline RL techniques for off-policy LTR and propose the Click Model-Agnostic Unified Off-policy Learning to Rank (CUOLR) method, which could be easily applied to a wide range of click models. Through a dedicated formulation of the MDP, we show that offline RL algorithms can adapt to various click models without complex debiasing techniques and prior knowledge of the model. Results on various large-scale datasets
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20840;&#38754;&#32508;&#36848;&#20102;&#26102;&#23578;&#25512;&#33616;&#30340;&#35745;&#31639;&#25216;&#26415;&#30740;&#31350;&#12290;&#30740;&#31350;&#32773;&#20174;&#23439;&#35266;&#23618;&#38754;&#20171;&#32461;&#20102;&#35813;&#39046;&#22495;&#65292;&#20998;&#26512;&#20102;&#20854;&#29305;&#28857;&#21644;&#19981;&#21516;&#20043;&#22788;&#65292;&#24182;&#23558;&#26102;&#23578;&#25512;&#33616;&#20219;&#21153;&#20998;&#20026;&#20960;&#20010;&#23376;&#20219;&#21153;&#36827;&#34892;&#37325;&#28857;&#35752;&#35770;&#12290;</title><link>http://arxiv.org/abs/2306.03395</link><description>&lt;p&gt;
&#26102;&#23578;&#25512;&#33616;&#30340;&#35745;&#31639;&#25216;&#26415;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Computational Technologies for Fashion Recommendation: A Survey. (arXiv:2306.03395v1 [cs.MM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03395
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#32508;&#36848;&#20102;&#26102;&#23578;&#25512;&#33616;&#30340;&#35745;&#31639;&#25216;&#26415;&#30740;&#31350;&#12290;&#30740;&#31350;&#32773;&#20174;&#23439;&#35266;&#23618;&#38754;&#20171;&#32461;&#20102;&#35813;&#39046;&#22495;&#65292;&#20998;&#26512;&#20102;&#20854;&#29305;&#28857;&#21644;&#19981;&#21516;&#20043;&#22788;&#65292;&#24182;&#23558;&#26102;&#23578;&#25512;&#33616;&#20219;&#21153;&#20998;&#20026;&#20960;&#20010;&#23376;&#20219;&#21153;&#36827;&#34892;&#37325;&#28857;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#23578;&#25512;&#33616;&#26159;&#35745;&#31639;&#26102;&#23578;&#30740;&#31350;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#30740;&#31350;&#39046;&#22495;&#65292;&#36817;&#24180;&#26469;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#22810;&#23186;&#20307;&#21644;&#20449;&#24687;&#26816;&#32034;&#31038;&#21306;&#24341;&#36215;&#20102;&#30456;&#24403;&#22823;&#30340;&#20852;&#36259;&#12290;&#30001;&#20110;&#24212;&#29992;&#30340;&#24040;&#22823;&#38656;&#27714;&#65292;&#25991;&#29486;&#20013;&#25552;&#20986;&#24182;&#25506;&#32034;&#20102;&#21508;&#31181;&#26102;&#23578;&#25512;&#33616;&#20219;&#21153;&#65292;&#22914;&#20010;&#24615;&#21270;&#26102;&#23578;&#20135;&#21697;&#25512;&#33616;&#12289;&#30456;&#20114;&#34917;&#20805;&#65288;&#25645;&#37197;&#65289;&#25512;&#33616;&#21644;&#25645;&#37197;&#25512;&#33616;&#12290;&#25345;&#32493;&#30340;&#30740;&#31350;&#27880;&#24847;&#21644;&#36827;&#23637;&#20419;&#20351;&#25105;&#20204;&#22238;&#39038;&#24182;&#28145;&#20837;&#20102;&#35299;&#36825;&#20010;&#39046;&#22495;&#12290;&#26412;&#25991;&#20174;&#25216;&#26415;&#35282;&#24230;&#20840;&#38754;&#22238;&#39038;&#20102;&#36817;&#24180;&#26469;&#20851;&#20110;&#26102;&#23578;&#25512;&#33616;&#30340;&#30740;&#31350;&#24037;&#20316;&#12290;&#25105;&#20204;&#39318;&#20808;&#20174;&#23439;&#35266;&#23618;&#38754;&#20171;&#32461;&#20102;&#26102;&#23578;&#25512;&#33616;&#65292;&#24182;&#20998;&#26512;&#20102;&#23427;&#30340;&#29305;&#28857;&#21644;&#19982;&#19968;&#33324;&#25512;&#33616;&#20219;&#21153;&#30340;&#21306;&#21035;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#19981;&#21516;&#30340;&#26102;&#23578;&#25512;&#33616;&#20219;&#21153;&#28165;&#26224;&#22320;&#24402;&#31867;&#20026;&#20960;&#20010;&#23376;&#20219;&#21153;&#65292;&#24182;&#20174;&#38382;&#39064;&#35299;&#20915;&#30340;&#35282;&#24230;&#37325;&#28857;&#20851;&#27880;&#27599;&#20010;&#23376;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fashion recommendation is a key research field in computational fashion research and has attracted considerable interest in the computer vision, multimedia, and information retrieval communities in recent years. Due to the great demand for applications, various fashion recommendation tasks, such as personalized fashion product recommendation, complementary (mix-and-match) recommendation, and outfit recommendation, have been posed and explored in the literature. The continuing research attention and advances impel us to look back and in-depth into the field for a better understanding. In this paper, we comprehensively review recent research efforts on fashion recommendation from a technological perspective. We first introduce fashion recommendation at a macro level and analyse its characteristics and differences with general recommendation tasks. We then clearly categorize different fashion recommendation efforts into several sub-tasks and focus on each sub-task in terms of its problem 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#20004;&#20010;&#20219;&#21153;&#23454;&#29616;&#26377;&#25928;&#30340;&#29992;&#25143;&#34892;&#20026;&#24314;&#27169;&#65292;&#20197;&#25552;&#39640;&#26032;&#38395;&#25512;&#33616;&#31995;&#32479;&#30340;&#20934;&#30830;&#24615;&#21644;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.12633</link><description>&lt;p&gt;
PUNR: &#29992;&#25143;&#34892;&#20026;&#24314;&#27169;&#30340;&#26032;&#38395;&#25512;&#33616;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
PUNR: Pre-training with User Behavior Modeling for News Recommendation. (arXiv:2304.12633v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12633
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#20004;&#20010;&#20219;&#21153;&#23454;&#29616;&#26377;&#25928;&#30340;&#29992;&#25143;&#34892;&#20026;&#24314;&#27169;&#65292;&#20197;&#25552;&#39640;&#26032;&#38395;&#25512;&#33616;&#31995;&#32479;&#30340;&#20934;&#30830;&#24615;&#21644;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#38395;&#25512;&#33616;&#26088;&#22312;&#22522;&#20110;&#29992;&#25143;&#34892;&#20026;&#39044;&#27979;&#28857;&#20987;&#34892;&#20026;&#12290;&#22914;&#20309;&#26377;&#25928;&#22320;&#24314;&#27169;&#29992;&#25143;&#34920;&#31034;&#26159;&#25512;&#33616;&#39318;&#36873;&#26032;&#38395;&#30340;&#20851;&#38190;&#12290;&#29616;&#26377;&#26041;&#27861;&#22823;&#22810;&#38598;&#20013;&#22312;&#30417;&#30563;&#24494;&#35843;&#38454;&#27573;&#30340;&#25913;&#36827;&#19978;&#12290;&#28982;&#32780;&#65292;&#36824;&#32570;&#20047;&#38024;&#23545;&#29992;&#25143;&#34920;&#31034;&#20248;&#21270;&#30340;&#22522;&#20110;PLM&#30340;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#20004;&#20010;&#20219;&#21153;&#30340;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#33539;&#20363;&#65292;&#21363;&#29992;&#25143;&#34892;&#20026;&#25513;&#34109;&#21644;&#29992;&#25143;&#34892;&#20026;&#29983;&#25104;&#65292;&#22343;&#33268;&#21147;&#20110;&#26377;&#25928;&#30340;&#29992;&#25143;&#34892;&#20026;&#24314;&#27169;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#29992;&#25143;&#34892;&#20026;&#25513;&#34109;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#20197;&#24674;&#22797;&#22522;&#20110;&#19978;&#19979;&#25991;&#34892;&#20026;&#30340;&#25513;&#34109;&#29992;&#25143;&#34892;&#20026;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#27169;&#22411;&#21487;&#20197;&#25429;&#25417;&#21040;&#26356;&#24378;&#22823;&#12289;&#26356;&#20840;&#38754;&#30340;&#29992;&#25143;&#26032;&#38395;&#38405;&#35835;&#27169;&#24335;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#32467;&#21512;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36741;&#21161;&#29992;&#25143;&#34892;&#20026;&#29983;&#25104;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#20197;&#22686;&#24378;&#20174;&#29992;&#25143;&#32534;&#30721;&#22120;&#27966;&#29983;&#20986;&#30340;&#29992;&#25143;&#34920;&#31034;&#21521;&#37327;&#12290;&#25105;&#20204;&#20351;&#29992;&#19978;&#36848;&#39044;&#35757;&#32451;&#30340;&#29992;&#25143;&#24314;&#27169;&#26469;&#36827;&#34892;&#26032;&#38395;&#25512;&#33616;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
News recommendation aims to predict click behaviors based on user behaviors. How to effectively model the user representations is the key to recommending preferred news. Existing works are mostly focused on improvements in the supervised fine-tuning stage. However, there is still a lack of PLM-based unsupervised pre-training methods optimized for user representations. In this work, we propose an unsupervised pre-training paradigm with two tasks, i.e. user behavior masking and user behavior generation, both towards effective user behavior modeling. Firstly, we introduce the user behavior masking pre-training task to recover the masked user behaviors based on their contextual behaviors. In this way, the model could capture a much stronger and more comprehensive user news reading pattern. Besides, we incorporate a novel auxiliary user behavior generation pre-training task to enhance the user representation vector derived from the user encoder. We use the above pre-trained user modeling en
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#31639;&#27861;&#37319;&#26679;&#29992;&#25143;&#31354;&#38388;&#20559;&#22909;&#30340;POI&#25512;&#33616;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#21482;&#22522;&#20110;&#29992;&#25143;&#20808;&#21069;&#35775;&#38382;&#20301;&#32622;&#32858;&#21512;&#30340;&#32570;&#28857;&#65292;&#36866;&#29992;&#20110;&#25512;&#33616;&#26032;&#39062;&#21306;&#22495;&#30340;POI&#12290;</title><link>http://arxiv.org/abs/2304.07041</link><description>&lt;p&gt;
&#19968;&#31181;POI&#25512;&#33616;&#30340;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Diffusion model for POI recommendation. (arXiv:2304.07041v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07041
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#31639;&#27861;&#37319;&#26679;&#29992;&#25143;&#31354;&#38388;&#20559;&#22909;&#30340;POI&#25512;&#33616;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#21482;&#22522;&#20110;&#29992;&#25143;&#20808;&#21069;&#35775;&#38382;&#20301;&#32622;&#32858;&#21512;&#30340;&#32570;&#28857;&#65292;&#36866;&#29992;&#20110;&#25512;&#33616;&#26032;&#39062;&#21306;&#22495;&#30340;POI&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19979;&#19968;&#20010;&#20852;&#36259;&#28857;&#65288;POI&#65289;&#30340;&#25512;&#33616;&#26159;&#23450;&#20301;&#26381;&#21153;&#20013;&#30340;&#20851;&#38190;&#20219;&#21153;&#65292;&#26088;&#22312;&#20026;&#29992;&#25143;&#30340;&#19979;&#19968;&#20010;&#30446;&#30340;&#22320;&#25552;&#20379;&#20010;&#24615;&#21270;&#24314;&#35758;&#12290;&#20808;&#21069;&#20851;&#20110;POI&#25512;&#33616;&#30340;&#24037;&#20316;&#20391;&#37325;&#20110;&#23545;&#29992;&#25143;&#31354;&#38388;&#20559;&#22909;&#30340;&#24314;&#27169;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#21033;&#29992;&#31354;&#38388;&#20449;&#24687;&#30340;&#26041;&#27861;&#20165;&#22522;&#20110;&#29992;&#25143;&#20808;&#21069;&#35775;&#38382;&#20301;&#32622;&#30340;&#32858;&#21512;&#65292;&#36825;&#20250;&#20351;&#27169;&#22411;&#19981;&#20250;&#25512;&#33616;&#26032;&#39062;&#21306;&#22495;&#30340;POI&#65292;&#20174;&#32780;&#25439;&#23475;&#20854;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#23558;&#26102;&#38388;&#39034;&#24207;&#20449;&#24687;&#34701;&#20837;&#29992;&#25143;&#30340;&#31354;&#38388;&#20559;&#22909;&#20173;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Diff-POI&#65306;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#37319;&#26679;&#29992;&#25143;&#30340;&#31354;&#38388;&#20559;&#22909;&#65292;&#20197;&#36827;&#34892;&#19979;&#19968;&#27493;POI&#25512;&#33616;&#12290;&#22312;&#25193;&#25955;&#31639;&#27861;&#22312;&#20174;&#20998;&#24067;&#20013;&#36827;&#34892;&#37319;&#26679;&#26041;&#38754;&#30340;&#24191;&#27867;&#24212;&#29992;&#30340;&#21551;&#21457;&#19979;&#65292;Diff-POI&#20351;&#29992;&#20004;&#20010;&#37327;&#36523;&#23450;&#21046;&#30340;&#22270;&#32534;&#30721;&#27169;&#22359;&#23545;&#29992;&#25143;&#30340;&#35775;&#38382;&#24207;&#21015;&#21644;&#31354;&#38388;&#29305;&#24615;&#36827;&#34892;&#32534;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Next Point-of-Interest (POI) recommendation is a critical task in location-based services that aim to provide personalized suggestions for the user's next destination. Previous works on POI recommendation have laid focused on modeling the user's spatial preference. However, existing works that leverage spatial information are only based on the aggregation of users' previous visited positions, which discourages the model from recommending POIs in novel areas. This trait of position-based methods will harm the model's performance in many situations. Additionally, incorporating sequential information into the user's spatial preference remains a challenge. In this paper, we propose Diff-POI: a Diffusion-based model that samples the user's spatial preference for the next POI recommendation. Inspired by the wide application of diffusion algorithm in sampling from distributions, Diff-POI encodes the user's visiting sequence and spatial character with two tailor-designed graph encoding modules
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DiffuRec &#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#20854;&#23558;&#29289;&#21697;&#34920;&#31034;&#20026;&#20998;&#24067;&#32780;&#19981;&#26159;&#22266;&#23450;&#21521;&#37327;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#21453;&#26144;&#20102;&#29992;&#25143;&#30340;&#22810;&#31181;&#20559;&#22909;&#21644;&#29289;&#21697;&#30340;&#22810;&#20010;&#26041;&#38754;&#65292;&#24182;&#25104;&#21151;&#22320;&#24212;&#29992;&#20110;&#39034;&#24207;&#25512;&#33616;&#12290;</title><link>http://arxiv.org/abs/2304.00686</link><description>&lt;p&gt;
DiffuRec: &#19968;&#31181;&#29992;&#20110;&#39034;&#24207;&#25512;&#33616;&#30340;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DiffuRec: A Diffusion Model for Sequential Recommendation. (arXiv:2304.00686v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00686
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DiffuRec &#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#20854;&#23558;&#29289;&#21697;&#34920;&#31034;&#20026;&#20998;&#24067;&#32780;&#19981;&#26159;&#22266;&#23450;&#21521;&#37327;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#21453;&#26144;&#20102;&#29992;&#25143;&#30340;&#22810;&#31181;&#20559;&#22909;&#21644;&#29289;&#21697;&#30340;&#22810;&#20010;&#26041;&#38754;&#65292;&#24182;&#25104;&#21151;&#22320;&#24212;&#29992;&#20110;&#39034;&#24207;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#20915;&#39034;&#24207;&#25512;&#33616;&#30340;&#20027;&#27969;&#26041;&#27861;&#26159;&#20351;&#29992;&#22266;&#23450;&#21521;&#37327;&#26469;&#34920;&#31034;&#29289;&#21697;&#12290;&#36825;&#20123;&#21521;&#37327;&#22312;&#25429;&#25417;&#29289;&#21697;&#30340;&#28508;&#22312;&#26041;&#38754;&#21644;&#29992;&#25143;&#30340;&#22810;&#26679;&#21270;&#20559;&#22909;&#26041;&#38754;&#26041;&#38754;&#30340;&#33021;&#21147;&#26377;&#38480;&#12290;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#33539;&#24335;&#65292;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#39046;&#22495;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#24615;&#33021;&#12290;&#22312;&#25105;&#20204;&#30475;&#26469;&#65292;&#20854;&#22312;&#34920;&#24449;&#29983;&#25104;&#26041;&#38754;&#30340;&#29420;&#29305;&#20248;&#21183;&#24456;&#22909;&#22320;&#36866;&#24212;&#20102;&#39034;&#24207;&#25512;&#33616;&#30340;&#38382;&#39064;&#35774;&#32622;&#12290;&#26412;&#25991;&#39318;&#27425;&#23581;&#35797;&#23558;&#25193;&#25955;&#27169;&#22411;&#24212;&#29992;&#20110;&#39034;&#24207;&#25512;&#33616;&#65292;&#24182;&#25552;&#20986;&#20102;DiffuRec&#65292;&#29992;&#20110;&#29289;&#21697;&#34920;&#31034;&#26500;&#24314;&#21644;&#19981;&#30830;&#23450;&#24615;&#27880;&#20837;&#12290;&#19982;&#23558;&#29289;&#21697;&#34920;&#31034;&#24314;&#27169;&#20026;&#22266;&#23450;&#21521;&#37327;&#19981;&#21516;&#65292;&#25105;&#20204;&#22312;DiffuRec&#20013;&#23558;&#20854;&#34920;&#31034;&#20026;&#20998;&#24067;&#65292;&#36825;&#21453;&#26144;&#20102;&#29992;&#25143;&#30340;&#22810;&#37325;&#20852;&#36259;&#21644;&#29289;&#21697;&#30340;&#21508;&#20010;&#26041;&#38754;&#30340;&#36866;&#24212;&#24615;&#12290;&#22312;&#25193;&#25955;&#38454;&#27573;&#65292;DiffuRec&#36890;&#36807;&#28155;&#21152;&#22122;&#22768;&#23558;&#30446;&#26631;&#29289;&#21697;&#23884;&#20837;&#25104;&#39640;&#26031;&#20998;&#24067;&#65292;&#36827;&#19968;&#27493;&#24212;&#29992;&#20110;&#39034;&#24207;&#29289;&#21697;&#20998;&#24067;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mainstream solutions to Sequential Recommendation (SR) represent items with fixed vectors. These vectors have limited capability in capturing items' latent aspects and users' diverse preferences. As a new generative paradigm, Diffusion models have achieved excellent performance in areas like computer vision and natural language processing. To our understanding, its unique merit in representation generation well fits the problem setting of sequential recommendation. In this paper, we make the very first attempt to adapt Diffusion model to SR and propose DiffuRec, for item representation construction and uncertainty injection. Rather than modeling item representations as fixed vectors, we represent them as distributions in DiffuRec, which reflect user's multiple interests and item's various aspects adaptively. In diffusion phase, DiffuRec corrupts the target item embedding into a Gaussian distribution via noise adding, which is further applied for sequential item distribution representat
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#38480;&#21046;&#29627;&#23572;&#20857;&#26364;&#26426;&#22312;&#27169;&#24335;&#37325;&#26500;&#20013;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#38544;&#34255;&#23618;&#20808;&#39564;&#20998;&#24067;&#30340;&#23614;&#37096;&#34892;&#20026;&#23545;&#20110;&#24674;&#22797;&#38543;&#26426;&#27169;&#24335;&#30340;&#25928;&#26524;&#26377;&#20851;&#38190;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2205.07087</link><description>&lt;p&gt;
&#38480;&#21046;&#29627;&#23572;&#20857;&#26364;&#26426;&#30340;&#27169;&#24335;&#37325;&#26500;
&lt;/p&gt;
&lt;p&gt;
Pattern reconstruction with restricted Boltzmann machines. (arXiv:2205.07087v3 [math.PR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.07087
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#38480;&#21046;&#29627;&#23572;&#20857;&#26364;&#26426;&#22312;&#27169;&#24335;&#37325;&#26500;&#20013;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#38544;&#34255;&#23618;&#20808;&#39564;&#20998;&#24067;&#30340;&#23614;&#37096;&#34892;&#20026;&#23545;&#20110;&#24674;&#22797;&#38543;&#26426;&#27169;&#24335;&#30340;&#25928;&#26524;&#26377;&#20851;&#38190;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38480;&#21046;&#29627;&#23572;&#20857;&#26364;&#26426;&#26159;&#30001;&#21487;&#35265;&#23618;&#21644;&#38544;&#34255;&#23618;&#32452;&#25104;&#30340;&#33021;&#37327;&#27169;&#22411;&#12290;&#25105;&#20204;&#25214;&#21040;&#20102;&#25551;&#36848;&#21487;&#35265;&#21333;&#20803;&#19978;&#38646;&#28201;&#24230;&#29366;&#24577;&#30340;&#26377;&#25928;&#33021;&#37327;&#20989;&#25968;&#65292;&#35813;&#20989;&#25968;&#21482;&#20381;&#36182;&#20110;&#38544;&#34255;&#23618;&#20808;&#39564;&#20998;&#24067;&#30340;&#23614;&#37096;&#34892;&#20026;&#12290;&#36890;&#36807;&#30740;&#31350;&#35813;&#33021;&#37327;&#20989;&#25968;&#30340;&#23616;&#37096;&#26497;&#23567;&#20540;&#30340;&#20301;&#32622;&#65292;&#25105;&#20204;&#34920;&#26126;&#38480;&#21046;&#29627;&#23572;&#20857;&#26364;&#26426;&#37325;&#26500;&#38543;&#26426;&#27169;&#24335;&#30340;&#33021;&#21147;&#30830;&#23454;&#21482;&#21462;&#20915;&#20110;&#38544;&#34255;&#20808;&#39564;&#20998;&#24067;&#30340;&#23614;&#37096;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20855;&#26377;&#20005;&#26684;&#36229;&#39640;&#26031;&#23614;&#37096;&#30340;&#38544;&#34255;&#20808;&#39564;&#20165;&#23548;&#33268;&#23545;&#27169;&#24335;&#24674;&#22797;&#30340;&#23545;&#25968;&#25439;&#22833;&#65292;&#32780;&#20855;&#26377;&#20005;&#26684;&#27425;&#39640;&#26031;&#23614;&#37096;&#30340;&#38544;&#34255;&#21333;&#20803;&#21017;&#23548;&#33268;&#26356;&#38590;&#36827;&#34892;&#26377;&#25928;&#30340;&#24674;&#22797;&#65307;&#22914;&#26524;&#38544;&#34255;&#20808;&#39564;&#20855;&#26377;&#39640;&#26031;&#23614;&#37096;&#65292;&#24674;&#22797;&#33021;&#21147;&#21462;&#20915;&#20110;&#38544;&#34255;&#21333;&#20803;&#30340;&#25968;&#37327;&#65288;&#19982;&#38669;&#26222;&#33778;&#23572;&#24503;&#27169;&#22411;&#31867;&#20284;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Restricted Boltzmann machines are energy models made of a visible and a hidden layer. We identify an effective energy function describing the zero-temperature landscape on the visible units and depending only on the tail behaviour of the hidden layer prior distribution. Studying the location of the local minima of such an energy function, we show that the ability of a restricted Boltzmann machine to reconstruct a random pattern depends indeed only on the tail of the hidden prior distribution. We find that hidden priors with strictly super-Gaussian tails give only a logarithmic loss in pattern retrieval, while an efficient retrieval is much harder with hidden units with strictly sub-Gaussian tails; if the hidden prior has Gaussian tails, the retrieval capability is determined by the number of hidden units (as in the Hopfield model).
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;CLIP&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#23884;&#20837;&#36136;&#37327;&#65292;&#24182;&#21457;&#29616;&#20854;&#32479;&#19968;&#24615;&#21644;&#23545;&#40784;&#24615;&#19981;&#36275;&#65292;&#38480;&#21046;&#20102;&#23884;&#20837;&#30340;&#20256;&#36882;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#40065;&#26834;&#24494;&#35843;&#26041;&#27861;&#65292;&#36890;&#36807;&#39640;&#24230;&#20960;&#20309;&#22810;&#27169;&#22411;&#28151;&#21512;&#29983;&#25104;&#38590;&#36127;&#26679;&#26412;&#65292;&#24182;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;</title><link>http://arxiv.org/abs/2203.03897</link><description>&lt;p&gt;
&#39640;&#24230;&#20960;&#20309;&#22810;&#27169;&#22411;&#28151;&#21512;&#29992;&#20110;&#40065;&#26834;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Geodesic Multi-Modal Mixup for Robust Fine-Tuning. (arXiv:2203.03897v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.03897
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;CLIP&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#23884;&#20837;&#36136;&#37327;&#65292;&#24182;&#21457;&#29616;&#20854;&#32479;&#19968;&#24615;&#21644;&#23545;&#40784;&#24615;&#19981;&#36275;&#65292;&#38480;&#21046;&#20102;&#23884;&#20837;&#30340;&#20256;&#36882;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#40065;&#26834;&#24494;&#35843;&#26041;&#27861;&#65292;&#36890;&#36807;&#39640;&#24230;&#20960;&#20309;&#22810;&#27169;&#22411;&#28151;&#21512;&#29983;&#25104;&#38590;&#36127;&#26679;&#26412;&#65292;&#24182;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#22810;&#27169;&#22411;&#27169;&#22411;&#65292;&#22914;CLIP&#65292;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#25552;&#20379;&#21487;&#36716;&#31227;&#30340;&#23884;&#20837;&#65292;&#24182;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#23545;&#23398;&#20064;&#21040;&#30340;&#22810;&#27169;&#22411;&#23884;&#20837;&#30340;&#20998;&#26512;&#30456;&#23545;&#36739;&#23569;&#65292;&#23884;&#20837;&#30340;&#21487;&#36716;&#31227;&#24615;&#26377;&#24453;&#25913;&#36827;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;CLIP&#20026;&#20004;&#31181;&#19981;&#21516;&#30340;&#27169;&#24577;&#20445;&#30041;&#20102;&#20998;&#31163;&#30340;&#23884;&#20837;&#23376;&#31354;&#38388;&#65292;&#24182;&#36890;&#36807;&#32479;&#19968;&#23545;&#40784;&#30340;&#35270;&#35282;&#23545;&#20854;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#20197;&#34913;&#37327;&#23398;&#20064;&#34920;&#31034;&#30340;&#36136;&#37327;&#12290;&#29702;&#35770;&#19978;&#21644;&#23454;&#35777;&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21363;&#20351;&#22312;&#24494;&#35843;&#20043;&#21518;&#65292;CLIP&#20173;&#28982;&#20445;&#25345;&#30528;&#36739;&#24046;&#30340;&#32479;&#19968;&#24615;&#21644;&#23545;&#40784;&#24615;&#12290;&#36825;&#31181;&#32570;&#20047;&#23545;&#40784;&#21644;&#32479;&#19968;&#24615;&#21487;&#33021;&#38480;&#21046;&#20102;&#23884;&#20837;&#30340;&#20256;&#36882;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#40065;&#26834;&#34920;&#31034;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#25552;&#20379;&#26356;&#22909;&#30340;&#23545;&#40784;&#21644;&#32479;&#19968;&#24615;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#24230;&#20960;&#20309;&#22810;&#27169;&#22411;&#28151;&#21512;&#26041;&#27861;&#65292;&#23558;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#23884;&#20837;&#28151;&#21512;&#22312;&#19968;&#36215;&#65292;&#22312;&#36229;&#29699;&#38754;&#19978;&#29983;&#25104;&#38590;&#36127;&#26679;&#26412;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23545;&#27169;&#22411;&#36827;&#34892;&#40065;&#26834;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained multi-modal models, such as CLIP, provide transferable embeddings and show promising results in diverse applications. However, the analysis of learned multi-modal embeddings is relatively unexplored, and the embedding transferability can be improved. In this work, we observe that CLIP holds separated embedding subspaces for two different modalities, and then we investigate it through the lens of uniformity-alignment to measure the quality of learned representation. Both theoretically and empirically, we show that CLIP retains poor uniformity and alignment even after fine-tuning. Such a lack of alignment and uniformity might restrict the transferability and robustness of embeddings. To this end, we devise a new fine-tuning method for robust representation equipping better alignment and uniformity. First, we propose a Geodesic Multi-Modal Mixup that mixes the embeddings of image and text to generate hard negative samples on the hypersphere. Then, we fine-tune the model on har
&lt;/p&gt;</description></item></channel></rss>