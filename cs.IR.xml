<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>Text2Cohort&#26159;&#19968;&#20010;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#24037;&#20855;&#31665;&#65292;&#21487;&#20197;&#23558;&#29992;&#25143;&#36755;&#20837;&#36716;&#21270;&#20026;IDC&#25968;&#25454;&#24211;&#26597;&#35810;&#65292;&#20419;&#36827;&#33258;&#28982;&#35821;&#35328;&#38431;&#21015;&#21457;&#29616;&#65292;&#20943;&#23569;&#30740;&#31350;&#20154;&#21592;&#26597;&#35810;IDC&#25968;&#25454;&#24211;&#30340;&#23398;&#20064;&#26354;&#32447;&#65292;&#23454;&#29616;&#20102;&#30284;&#30151;&#25104;&#20687;&#25968;&#25454;&#30340;&#27665;&#20027;&#21270;&#12290;</title><link>http://arxiv.org/abs/2305.07637</link><description>&lt;p&gt;
Text2Cohort: &#33258;&#28982;&#35821;&#35328;&#38431;&#21015;&#21457;&#29616;&#23545;&#30284;&#30151;&#24433;&#20687;&#25968;&#25454;&#20849;&#20139;&#24179;&#21488;&#30340;&#27665;&#20027;&#21270;
&lt;/p&gt;
&lt;p&gt;
Text2Cohort: Democratizing the NCI Imaging Data Commons with Natural Language Cohort Discovery. (arXiv:2305.07637v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07637
&lt;/p&gt;
&lt;p&gt;
Text2Cohort&#26159;&#19968;&#20010;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#24037;&#20855;&#31665;&#65292;&#21487;&#20197;&#23558;&#29992;&#25143;&#36755;&#20837;&#36716;&#21270;&#20026;IDC&#25968;&#25454;&#24211;&#26597;&#35810;&#65292;&#20419;&#36827;&#33258;&#28982;&#35821;&#35328;&#38431;&#21015;&#21457;&#29616;&#65292;&#20943;&#23569;&#30740;&#31350;&#20154;&#21592;&#26597;&#35810;IDC&#25968;&#25454;&#24211;&#30340;&#23398;&#20064;&#26354;&#32447;&#65292;&#23454;&#29616;&#20102;&#30284;&#30151;&#25104;&#20687;&#25968;&#25454;&#30340;&#27665;&#20027;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24433;&#20687;&#25968;&#25454;&#20849;&#20139;&#24179;&#21488;(IDC)&#26159;&#19968;&#20010;&#22522;&#20110;&#20113;&#30340;&#25968;&#25454;&#24211;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#24320;&#25918;&#33719;&#21462;&#30340;&#30284;&#30151;&#25104;&#20687;&#25968;&#25454;&#21644;&#20998;&#26512;&#24037;&#20855;&#65292;&#26088;&#22312;&#20419;&#36827;&#21307;&#23398;&#25104;&#20687;&#30740;&#31350;&#20013;&#30340;&#21327;&#20316;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#22797;&#26434;&#21644;&#25216;&#26415;&#24615;&#36136;&#65292;&#26597;&#35810;IDC&#25968;&#25454;&#24211;&#20197;&#36827;&#34892;&#38431;&#21015;&#21457;&#29616;&#21644;&#35775;&#38382;&#25104;&#20687;&#25968;&#25454;&#23545;&#30740;&#31350;&#20154;&#21592;&#26469;&#35828;&#20855;&#26377;&#26174;&#33879;&#30340;&#23398;&#20064;&#26354;&#32447;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;Text2Cohort&#24037;&#20855;&#31665;&#65292;&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#23558;&#29992;&#25143;&#36755;&#20837;&#36716;&#21270;&#20026;IDC&#25968;&#25454;&#24211;&#26597;&#35810;&#65292;&#24182;&#23558;&#26597;&#35810;&#30340;&#21709;&#24212;&#36820;&#22238;&#32473;&#29992;&#25143;&#65292;&#20197;&#20419;&#36827;&#33258;&#28982;&#35821;&#35328;&#38431;&#21015;&#21457;&#29616;&#12290;&#27492;&#22806;&#65292;&#23454;&#29616;&#20102;&#33258;&#21160;&#26657;&#27491;&#20197;&#35299;&#20915;&#26597;&#35810;&#20013;&#30340;&#35821;&#27861;&#21644;&#35821;&#20041;&#38169;&#35823;&#65292;&#36890;&#36807;&#23558;&#38169;&#35823;&#20256;&#22238;&#27169;&#22411;&#36827;&#34892;&#35299;&#37322;&#21644;&#26657;&#27491;&#12290;&#25105;&#20204;&#23545;50&#20010;&#33258;&#28982;&#35821;&#35328;&#29992;&#25143;&#36755;&#20837;&#36827;&#34892;&#20102;Text2Cohort&#35780;&#20272;&#65292;&#33539;&#22260;&#20174;&#20449;&#24687;&#25552;&#21462;&#21040;&#38431;&#21015;&#21457;&#29616;&#12290;&#32467;&#26524;&#26597;&#35810;&#21644;&#36755;&#20986;&#30001;&#20004;&#20301;&#35745;&#31639;&#26426;&#31185;&#23398;&#23478;&#36827;&#34892;&#20102;&#30830;&#35748;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Imaging Data Commons (IDC) is a cloud-based database that provides researchers with open access to cancer imaging data and tools for analysis, with the goal of facilitating collaboration in medical imaging research. However, querying the IDC database for cohort discovery and access to imaging data has a significant learning curve for researchers due to its complex and technical nature. We developed Text2Cohort, a large language model (LLM) based toolkit to facilitate natural language cohort discovery by translating user input into IDC database queries through prompt engineering and returning the query's response to the user. Furthermore, autocorrection is implemented to resolve syntax and semantic errors in queries by passing the errors back to the model for interpretation and correction. We evaluate Text2Cohort on 50 natural language user inputs ranging from information extraction to cohort discovery. The resulting queries and outputs were verified by two computer scientists to me
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20135;&#21697;&#30693;&#35782;&#22270;&#35889;&#39044;&#35757;&#32451;&#27169;&#22411;&#20174;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#25552;&#21462;&#39033;&#30446;&#29305;&#24449;&#65292;&#20197;&#35299;&#20915;&#38646;&#26679;&#26412;&#39033;&#30446;&#25512;&#33616;&#20219;&#21153;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25552;&#20986;&#22235;&#20010;&#39044;&#35757;&#32451;&#20219;&#21153;&#21644;&#20219;&#21153;&#23548;&#21521;&#30340;&#36866;&#24212;&#23618;&#26469;&#35299;&#20915;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#25361;&#25112;&#65292;&#24182;&#23558;&#27169;&#22411;&#24494;&#35843;&#21040;&#26032;&#30340;&#25512;&#33616;&#20219;&#21153;&#20013;&#12290;</title><link>http://arxiv.org/abs/2305.07633</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#20219;&#21153;&#20135;&#21697;&#30693;&#35782;&#22270;&#35889;&#39044;&#35757;&#32451;&#30340;&#38646;&#26679;&#26412;&#22522;&#20110;&#39033;&#30446;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Zero-shot Item-based Recommendation via Multi-task Product Knowledge Graph Pre-Training. (arXiv:2305.07633v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07633
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20135;&#21697;&#30693;&#35782;&#22270;&#35889;&#39044;&#35757;&#32451;&#27169;&#22411;&#20174;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#25552;&#21462;&#39033;&#30446;&#29305;&#24449;&#65292;&#20197;&#35299;&#20915;&#38646;&#26679;&#26412;&#39033;&#30446;&#25512;&#33616;&#20219;&#21153;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25552;&#20986;&#22235;&#20010;&#39044;&#35757;&#32451;&#20219;&#21153;&#21644;&#20219;&#21153;&#23548;&#21521;&#30340;&#36866;&#24212;&#23618;&#26469;&#35299;&#20915;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#25361;&#25112;&#65292;&#24182;&#23558;&#27169;&#22411;&#24494;&#35843;&#21040;&#26032;&#30340;&#25512;&#33616;&#20219;&#21153;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#25512;&#33616;&#31995;&#32479;&#22312;&#22788;&#29702;&#38646;&#26679;&#26412;&#39033;&#30446;&#65288;&#21363;&#22312;&#35757;&#32451;&#38454;&#27573;&#27809;&#26377;&#19982;&#29992;&#25143;&#36827;&#34892;&#36807;&#21382;&#21490;&#20114;&#21160;&#30340;&#39033;&#30446;&#65289;&#26102;&#38754;&#20020;&#22256;&#38590;&#12290;&#34429;&#28982;&#26368;&#36817;&#30340;&#24037;&#20316;&#36890;&#36807;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#25552;&#21462;&#36890;&#29992;&#39033;&#30446;&#34920;&#31034;&#65292;&#20294;&#23427;&#20204;&#24573;&#30053;&#20102;&#20851;&#38190;&#30340;&#39033;&#30446;&#20851;&#31995;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#20135;&#21697;&#30693;&#35782;&#22270;&#35889;&#65288;PKG&#65289;&#23545;&#27169;&#22411;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#20197;&#20174;PLMs&#20013;&#25552;&#28860;&#20986;&#39033;&#30446;&#29305;&#24449;&#26469;&#35299;&#20915;&#38646;&#26679;&#26412;&#39033;&#30446;&#25512;&#33616;&#65288;ZSIR&#65289;&#20219;&#21153;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#39044;&#35757;&#32451;PKG&#30340;&#19977;&#20010;&#25361;&#25112;&#65292;&#21363;PKG&#20013;&#30340;&#22810;&#31867;&#22411;&#20851;&#31995;&#65292;&#39033;&#30446;&#36890;&#29992;&#20449;&#24687;&#21644;&#20851;&#31995;&#20043;&#38388;&#30340;&#35821;&#20041;&#24046;&#24322;&#20197;&#21450;&#20174;PKG&#21040;&#19979;&#28216;ZSIR&#20219;&#21153;&#30340;&#22495;&#24046;&#24322;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#22235;&#20010;&#39044;&#35757;&#32451;&#20219;&#21153;&#21644;&#26032;&#39062;&#30340;&#38754;&#21521;&#20219;&#21153;&#30340;&#36866;&#24212;&#65288;ToA&#65289;&#23618;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#35752;&#35770;&#20102;&#22914;&#20309;&#23545;&#26032;&#30340;&#25512;&#33616;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#65292;&#20351;&#24471;ToA&#23618;&#36866;&#24212;&#20110;ZSIR&#20219;&#21153;&#12290;&#22312;18&#20010;&#24066;&#22330;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing recommender systems face difficulties with zero-shot items, i.e. items that have no historical interactions with users during the training stage. Though recent works extract universal item representation via pre-trained language models (PLMs), they ignore the crucial item relationships. This paper presents a novel paradigm for the Zero-Shot Item-based Recommendation (ZSIR) task, which pre-trains a model on product knowledge graph (PKG) to refine the item features from PLMs. We identify three challenges for pre-training PKG, which are multi-type relations in PKG, semantic divergence between item generic information and relations and domain discrepancy from PKG to downstream ZSIR task. We address the challenges by proposing four pre-training tasks and novel task-oriented adaptation (ToA) layers. Moreover, this paper discusses how to fine-tune the model on new recommendation task such that the ToA layers are adapted to ZSIR task. Comprehensive experiments on 18 markets dataset ar
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;PALR&#30340;&#26694;&#26550;&#65292;&#23558;&#29992;&#25143;&#30340;&#21382;&#21490;&#34892;&#20026;&#19982;LLMs&#30456;&#32467;&#21512;&#65292;&#29983;&#25104;&#29992;&#25143;&#21916;&#27426;&#30340;&#29289;&#21697;&#30340;&#25512;&#33616;&#12290;&#19982;&#29616;&#26377;&#30340;&#25512;&#33616;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;PALR&#26694;&#26550;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.07622</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#24863;&#30693;&#30340;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;LMMs&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
PALR: Personalization Aware LLMs for Recommendation. (arXiv:2305.07622v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07622
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;PALR&#30340;&#26694;&#26550;&#65292;&#23558;&#29992;&#25143;&#30340;&#21382;&#21490;&#34892;&#20026;&#19982;LLMs&#30456;&#32467;&#21512;&#65292;&#29983;&#25104;&#29992;&#25143;&#21916;&#27426;&#30340;&#29289;&#21697;&#30340;&#25512;&#33616;&#12290;&#19982;&#29616;&#26377;&#30340;&#25512;&#33616;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;PALR&#26694;&#26550;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30001;&#20110;&#20854;&#20986;&#33394;&#30340;&#24615;&#33021;&#32780;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;PALR&#65292;&#23558;&#29992;&#25143;&#30340;&#21382;&#21490;&#34892;&#20026;&#19982;LLMs&#30456;&#32467;&#21512;&#65292;&#20197;&#29983;&#25104;&#29992;&#25143;&#21916;&#27426;&#30340;&#29289;&#21697;&#30340;&#25512;&#33616;&#12290;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#29992;&#25143;/&#29289;&#21697;&#20114;&#21160;&#20316;&#20026;&#20505;&#36873;&#26816;&#32034;&#30340;&#25351;&#23548;&#65292;&#28982;&#21518;&#37319;&#29992;&#22522;&#20110;LLMs&#30340;&#25490;&#24207;&#27169;&#22411;&#29983;&#25104;&#25512;&#33616;&#29289;&#21697;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#30340;&#25512;&#33616;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;PALR&#26694;&#26550;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have recently received significant attention for their exceptional capabilities. Despite extensive efforts in developing general-purpose LLMs that can be utilized in various natural language processing (NLP) tasks, there has been less research exploring their potential in recommender systems. In this paper, we propose a novel framework, named PALR, which aiming to combine user history behaviors (such as clicks, purchases, ratings, etc.) with LLMs to generate user preferred items. Specifically, we first use user/item interactions as guidance for candidate retrieval. Then we adopt a LLM-based ranking model to generate recommended items. Unlike existing approaches that typically adopt general-purpose LLMs for zero/few-shot recommendation testing or training on small-sized language models (with less than 1 billion parameters), which cannot fully elicit LLMs' reasoning abilities and leverage rich item side parametric knowledge, we fine-tune a 7 billion parameter
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#21542;&#23450;&#22312;&#31070;&#32463;&#20449;&#24687;&#26816;&#32034;&#20013;&#30340;&#24433;&#21709;&#65292;&#26500;&#24314;&#20102;&#22522;&#20934;&#27169;&#22411;&#65292;&#32467;&#26524;&#34920;&#26126;&#24403;&#21069;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#22823;&#22810;&#25968;&#37117;&#27809;&#26377;&#32771;&#34385;&#21542;&#23450;&#65292;&#32780;&#20132;&#21449;&#32534;&#30721;&#22120;&#26159;&#30446;&#21069;&#34920;&#29616;&#26368;&#22909;&#30340;&#26550;&#26500;&#12290;</title><link>http://arxiv.org/abs/2305.07614</link><description>&lt;p&gt;
NevIR: &#31070;&#32463;&#20449;&#24687;&#26816;&#32034;&#20013;&#30340;&#21542;&#23450;
&lt;/p&gt;
&lt;p&gt;
NevIR: Negation in Neural Information Retrieval. (arXiv:2305.07614v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07614
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#21542;&#23450;&#22312;&#31070;&#32463;&#20449;&#24687;&#26816;&#32034;&#20013;&#30340;&#24433;&#21709;&#65292;&#26500;&#24314;&#20102;&#22522;&#20934;&#27169;&#22411;&#65292;&#32467;&#26524;&#34920;&#26126;&#24403;&#21069;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#22823;&#22810;&#25968;&#37117;&#27809;&#26377;&#32771;&#34385;&#21542;&#23450;&#65292;&#32780;&#20132;&#21449;&#32534;&#30721;&#22120;&#26159;&#30446;&#21069;&#34920;&#29616;&#26368;&#22909;&#30340;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21542;&#23450;&#26159;&#19968;&#31181;&#24120;&#35265;&#32780;&#26085;&#24120;&#21270;&#30340;&#29616;&#35937;&#65292;&#20063;&#19968;&#30452;&#26159;&#35821;&#35328;&#27169;&#22411;&#30340;&#19968;&#20010;&#24369;&#28857;&#12290;&#34429;&#28982;&#20449;&#24687;&#26816;&#32034;&#39046;&#22495;&#37319;&#29992;&#20102;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#29616;&#20195;&#21270;&#26550;&#26500;&#30340;&#20027;&#24178;&#65292;&#20294;&#20960;&#20046;&#27809;&#26377;&#30740;&#31350;&#28145;&#20837;&#20102;&#35299;&#21542;&#23450;&#23545;&#31070;&#32463;&#20449;&#24687;&#26816;&#32034;&#30340;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#22522;&#20934;&#26469;&#30740;&#31350;&#36825;&#20010;&#20027;&#39064;&#65306;&#35201;&#27714;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#23545;&#20165;&#20165;&#22240;&#20026;&#26159;&#21542;&#23450;&#32780;&#19981;&#21516;&#30340;&#20004;&#20010;&#25991;&#26723;&#36827;&#34892;&#25490;&#21517;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#32467;&#26524;&#26681;&#25454;&#19981;&#21516;&#30340;&#20449;&#24687;&#26816;&#32034;&#26550;&#26500;&#32780;&#26377;&#24456;&#22823;&#24046;&#24322;&#65306;&#20132;&#21449;&#32534;&#30721;&#22120;&#34920;&#29616;&#26368;&#22909;&#65292;&#21518;&#26399;&#20132;&#20114;&#27169;&#22411;&#27425;&#20043;&#65292;&#32780;&#21452;&#32534;&#30721;&#22120;&#21644;&#31232;&#30095;&#31070;&#32463;&#26550;&#26500;&#25490;&#21517;&#26368;&#21518;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22823;&#22810;&#25968;&#24403;&#21069;&#30340;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#37117;&#27809;&#26377;&#32771;&#34385;&#21542;&#23450;&#65292;&#34920;&#29616;&#19982;&#38543;&#26426;&#25490;&#21517;&#30456;&#20284;&#25110;&#26356;&#24046;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#23613;&#31649;&#22312;&#19968;&#20010;&#21253;&#21547;&#21542;&#23450;&#23545;&#29031;&#25991;&#26723;&#30340;&#25968;&#25454;&#38598;&#19978;&#32487;&#32493;&#24494;&#35843;&#26126;&#26174;&#30340;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#65288;&#27169;&#22411;&#22823;&#23567;&#20063;&#26159;&#22914;&#27492;&#65289;&#65292;&#20294;&#26159;&#26426;&#22120;&#21644;&#20154;&#20043;&#38388;&#20173;&#23384;&#22312;&#24456;&#22823;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
Negation is a common everyday phenomena and has been a consistent area of weakness for language models (LMs). Although the Information Retrieval (IR) community has adopted LMs as the backbone of modern IR architectures, there has been little to no research in understanding how negation impacts neural IR. We therefore construct a straightforward benchmark on this theme: asking IR models to rank two documents that differ only by negation. We show that the results vary widely according to the type of IR architecture: cross-encoders perform best, followed by late-interaction models, and in last place are bi-encoder and sparse neural architectures. We find that most current information retrieval models do not consider negation, performing similarly or worse than randomly ranking. We show that although the obvious approach of continued fine-tuning on a dataset of contrastive documents containing negations increases performance (as does model size), there is still a large gap between machine 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25512;&#33616;&#33539;&#24335;&#8212;&#8212;&#36890;&#36807;LLM&#36827;&#34892;&#25512;&#33616;&#65292;&#20294;&#30001;&#20110;LLMs&#21487;&#33021;&#23384;&#22312;&#31038;&#20250;&#20559;&#35265;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#35843;&#26597;RecLLM&#25152;&#20570;&#25512;&#33616;&#30340;&#20844;&#27491;&#24615;&#12290;&#20026;&#27492;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20844;&#24179;&#24615;&#22522;&#20934;&#8212;&#8212;FaiRLLM&#65292;&#24182;&#38024;&#23545;&#38899;&#20048;&#21644;&#30005;&#24433;&#25512;&#33616;&#22330;&#26223;&#20013;&#30340;&#20843;&#20010;&#25935;&#24863;&#23646;&#24615;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2305.07609</link><description>&lt;p&gt;
ChatGPT&#26159;&#21542;&#20844;&#24179;&#21487;&#38752;&#65311;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#33616;&#20013;&#30340;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Is ChatGPT Fair for Recommendation? Evaluating Fairness in Large Language Model Recommendation. (arXiv:2305.07609v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07609
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25512;&#33616;&#33539;&#24335;&#8212;&#8212;&#36890;&#36807;LLM&#36827;&#34892;&#25512;&#33616;&#65292;&#20294;&#30001;&#20110;LLMs&#21487;&#33021;&#23384;&#22312;&#31038;&#20250;&#20559;&#35265;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#35843;&#26597;RecLLM&#25152;&#20570;&#25512;&#33616;&#30340;&#20844;&#27491;&#24615;&#12290;&#20026;&#27492;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20844;&#24179;&#24615;&#22522;&#20934;&#8212;&#8212;FaiRLLM&#65292;&#24182;&#38024;&#23545;&#38899;&#20048;&#21644;&#30005;&#24433;&#25512;&#33616;&#22330;&#26223;&#20013;&#30340;&#20843;&#20010;&#25935;&#24863;&#23646;&#24615;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26174;&#30528;&#25104;&#23601;&#23548;&#33268;&#19968;&#31181;&#26032;&#30340;&#25512;&#33616;&#33539;&#24335;&#8212;&#8212;&#36890;&#36807;LLM&#36827;&#34892;&#25512;&#33616;&#65288;RecLLM&#65289;&#12290;&#28982;&#32780;&#65292;&#38656;&#35201;&#27880;&#24847;LLMs&#21487;&#33021;&#21253;&#21547;&#31038;&#20250;&#20559;&#35265;&#65292;&#22240;&#27492;&#38656;&#35201;&#36827;&#19968;&#27493;&#35843;&#26597;RecLLM&#25152;&#20570;&#25512;&#33616;&#30340;&#20844;&#27491;&#24615;&#12290;&#20026;&#20102;&#36991;&#20813;RecLLM&#30340;&#28508;&#22312;&#39118;&#38505;&#65292;&#26377;&#24517;&#35201;&#20174;&#29992;&#25143;&#30340;&#21508;&#31181;&#25935;&#24863;&#23646;&#24615;&#35282;&#24230;&#35780;&#20272;RecLLM&#30340;&#20844;&#24179;&#24615;&#12290;&#30001;&#20110;RecLLM&#33539;&#24335;&#19982;&#20256;&#32479;&#25512;&#33616;&#33539;&#24335;&#20043;&#38388;&#23384;&#22312;&#24046;&#24322;&#65292;&#22240;&#27492;&#30452;&#25509;&#20351;&#29992;&#20256;&#32479;&#25512;&#33616;&#30340;&#20844;&#24179;&#24615;&#22522;&#20934;&#26159;&#26377;&#38382;&#39064;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#22256;&#22659;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#31216;&#20026;&#8220;&#36890;&#36807;LLM&#30340;&#25512;&#33616;&#30340;&#20844;&#24179;&#24615;&#8221;&#65288;FaiRLLM&#65289;&#12290;&#35813;&#22522;&#20934;&#21253;&#25324;&#31934;&#24515;&#35774;&#35745;&#30340;&#25351;&#26631;&#21644;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20004;&#20010;&#25512;&#33616;&#22330;&#26223;&#20013;&#30340;&#20843;&#20010;&#25935;&#24863;&#23646;&#24615;&#65306;&#38899;&#20048;&#21644;&#30005;&#24433;&#12290;&#36890;&#36807;&#21033;&#29992;&#25105;&#20204;&#30340;FaiRLLM&#22522;&#20934;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
The remarkable achievements of Large Language Models (LLMs) have led to the emergence of a novel recommendation paradigm -- Recommendation via LLM (RecLLM). Nevertheless, it is important to note that LLMs may contain social prejudices, and therefore, the fairness of recommendations made by RecLLM requires further investigation. To avoid the potential risks of RecLLM, it is imperative to evaluate the fairness of RecLLM with respect to various sensitive attributes on the user side. Due to the differences between the RecLLM paradigm and the traditional recommendation paradigm, it is problematic to directly use the fairness benchmark of traditional recommendation. To address the dilemma, we propose a novel benchmark called Fairness of Recommendation via LLM (FaiRLLM). This benchmark comprises carefully crafted metrics and a dataset that accounts for eight sensitive attributes1 in two recommendation scenarios: music and movies. By utilizing our FaiRLLM benchmark, we conducted an evaluation 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#30524;&#21160;&#36861;&#36394;&#21487;&#20197;&#20026;&#25512;&#33616;&#31995;&#32479;&#25552;&#20379;&#19968;&#20010;&#39069;&#22806;&#30340;&#38544;&#21547;&#21453;&#39304;&#26469;&#28304;&#65292;&#24182;&#19988;AOI&#25345;&#32493;&#26102;&#38388;&#19982;&#24050;&#30693;&#30340;&#28857;&#20987;&#25968;&#25454;&#21644;&#20808;&#21069;&#30475;&#36807;&#30340;&#30005;&#24433;&#30456;&#20851;&#65292;&#20026;&#25512;&#33616;&#27169;&#22411;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2305.07516</link><description>&lt;p&gt;
&#30524;&#21160;&#36861;&#36394;&#20316;&#20026;&#25512;&#33616;&#31995;&#32479;&#20013;&#38544;&#21547;&#21453;&#39304;&#30340;&#26469;&#28304;&#65306;&#21021;&#27493;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Eye Tracking as a Source of Implicit Feedback in Recommender Systems: A Preliminary Analysis. (arXiv:2305.07516v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07516
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#30524;&#21160;&#36861;&#36394;&#21487;&#20197;&#20026;&#25512;&#33616;&#31995;&#32479;&#25552;&#20379;&#19968;&#20010;&#39069;&#22806;&#30340;&#38544;&#21547;&#21453;&#39304;&#26469;&#28304;&#65292;&#24182;&#19988;AOI&#25345;&#32493;&#26102;&#38388;&#19982;&#24050;&#30693;&#30340;&#28857;&#20987;&#25968;&#25454;&#21644;&#20808;&#21069;&#30475;&#36807;&#30340;&#30005;&#24433;&#30456;&#20851;&#65292;&#20026;&#25512;&#33616;&#27169;&#22411;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#20351;&#29992;&#30524;&#21160;&#36861;&#36394;&#21487;&#20197;&#25552;&#20379;&#39069;&#22806;&#30340;&#38544;&#21547;&#21453;&#39304;&#26469;&#28304;&#65292;&#21516;&#26102;&#24110;&#21161;&#35780;&#20272;&#20854;&#20182;&#21453;&#39304;&#26469;&#28304;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#30524;&#21160;&#36861;&#36394;&#25968;&#25454;&#26469;&#25351;&#23548;&#30005;&#24433;&#25512;&#33616;&#30340;&#21327;&#21516;&#36807;&#28388;&#27169;&#22411;&#65292;&#30456;&#23545;&#20110;&#22522;&#20110;&#28857;&#20987;&#30340;&#23454;&#29616;&#25552;&#20379;&#20102;&#25913;&#36827;&#65292;&#24182;&#19988;&#36824;&#20998;&#26512;&#20102;&#24863;&#20852;&#36259;&#21306;&#22495;&#65288;AOI&#65289;&#25345;&#32493;&#26102;&#38388;&#19982;&#24050;&#30693;&#30340;&#28857;&#20987;&#25968;&#25454;&#21644;&#20808;&#21069;&#30475;&#36807;&#30340;&#30005;&#24433;&#30456;&#20851;&#24615;&#65292;&#34920;&#26126;AOI&#20449;&#24687;&#22987;&#32456;&#19982;&#36825;&#20123;&#24863;&#20852;&#36259;&#30340;&#20107;&#39033;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
Eye tracking in recommender systems can provide an additional source of implicit feedback, while helping to evaluate other sources of feedback. In this study, we use eye tracking data to inform a collaborative filtering model for movie recommendation providing an improvement over the click-based implementations and additionally analyze the area of interest (AOI) duration as related to the known information of click data and movies seen previously, showing AOI information consistently coincides with these items of interest.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#23558;&#29983;&#25104;&#24335;&#21644;&#20266;&#30456;&#20851;&#21453;&#39304;&#22312;&#31232;&#30095;&#12289;&#31264;&#23494;&#21644;&#23398;&#20064;&#31232;&#30095;&#26816;&#32034;&#20013;&#30340;&#24212;&#29992;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#30456;&#27604;&#31867;&#20284;&#30340;&#20266;&#30456;&#20851;&#21453;&#39304;&#25216;&#26415;&#65292;&#29983;&#25104;&#24335;&#30456;&#20851;&#21453;&#39304;&#21487;&#20197;&#25552;&#39640;&#32422;10%&#30340;&#31934;&#30830;&#24230;&#21644;&#21484;&#22238;&#29575;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#20004;&#31181;&#21453;&#39304;&#20449;&#21495;&#32467;&#21512;&#36215;&#26469;&#20197;&#23454;&#29616;&#23427;&#20204;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2305.07477</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;&#21644;&#20266;&#30456;&#20851;&#21453;&#39304;&#22312;&#31232;&#30095;&#12289;&#31264;&#23494;&#21644;&#23398;&#20064;&#31232;&#30095;&#26816;&#32034;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Generative and Pseudo-Relevant Feedback for Sparse, Dense and Learned Sparse Retrieval. (arXiv:2305.07477v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07477
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#23558;&#29983;&#25104;&#24335;&#21644;&#20266;&#30456;&#20851;&#21453;&#39304;&#22312;&#31232;&#30095;&#12289;&#31264;&#23494;&#21644;&#23398;&#20064;&#31232;&#30095;&#26816;&#32034;&#20013;&#30340;&#24212;&#29992;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#30456;&#27604;&#31867;&#20284;&#30340;&#20266;&#30456;&#20851;&#21453;&#39304;&#25216;&#26415;&#65292;&#29983;&#25104;&#24335;&#30456;&#20851;&#21453;&#39304;&#21487;&#20197;&#25552;&#39640;&#32422;10%&#30340;&#31934;&#30830;&#24230;&#21644;&#21484;&#22238;&#29575;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#20004;&#31181;&#21453;&#39304;&#20449;&#21495;&#32467;&#21512;&#36215;&#26469;&#20197;&#23454;&#29616;&#23427;&#20204;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20266;&#30456;&#20851;&#21453;&#39304;&#65288;PRF&#65289;&#26159;&#19968;&#31181;&#32463;&#20856;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#31532;&#19968;&#27425;&#26816;&#32034;&#26469;&#20016;&#23500;&#26597;&#35810;&#26469;&#35299;&#20915;&#35789;&#27719;&#19981;&#21305;&#37197;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#26368;&#36817;&#30340;&#29983;&#25104;&#24335;&#30456;&#20851;&#21453;&#39304;&#65288;GRF&#65289;&#24037;&#20316;&#34920;&#26126;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#25991;&#26412;&#36827;&#34892;&#26597;&#35810;&#25193;&#23637;&#27169;&#22411;&#21487;&#20197;&#25913;&#21892;&#31232;&#30095;&#26816;&#32034;&#65292;&#32780;&#26080;&#38656;&#20381;&#36182;&#20110;&#31532;&#19968;&#27425;&#26816;&#32034;&#30340;&#25928;&#26524;&#12290;&#26412;&#25991;&#23558;GRF&#25193;&#23637;&#21040;&#31264;&#23494;&#21644;&#23398;&#20064;&#31232;&#30095;&#26816;&#32034;&#33539;&#20363;&#65292;&#24182;&#22312;&#20845;&#20010;&#26631;&#20934;&#25991;&#26723;&#25490;&#24207;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#23454;&#39564;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;GRF&#22312;&#31934;&#30830;&#24230;&#21644;&#21484;&#22238;&#29575;&#26041;&#38754;&#30340;&#34920;&#29616;&#22343;&#27604;&#31867;&#20284;&#30340;PRF&#25216;&#26415;&#25552;&#39640;&#20102;&#32422;10%&#12290;&#28982;&#32780;&#65292;&#26597;&#35810;&#20998;&#26512;&#26174;&#31034;&#65292;GRF&#21644;PRF&#20855;&#26377;&#30456;&#21453;&#30340;&#20248;&#28857;&#65292;GRF&#25552;&#20379;&#20102;&#22806;&#37096;&#19978;&#19979;&#25991;&#65292;&#32780;PRF&#21017;&#23558;&#26597;&#35810;&#22522;&#20110;&#30446;&#26631;&#35821;&#26009;&#24211;&#20013;&#21253;&#21547;&#30340;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24314;&#35758;&#23558;&#29983;&#25104;&#24335;&#21644;&#20266;&#30456;&#20851;&#21453;&#39304;&#25490;&#21517;&#20449;&#21495;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#23454;&#29616;&#20004;&#20010;&#21453;&#39304;&#31867;&#21035;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pseudo-relevance feedback (PRF) is a classical approach to address lexical mismatch by enriching the query using first-pass retrieval. Moreover, recent work on generative-relevance feedback (GRF) shows that query expansion models using text generated from large language models can improve sparse retrieval without depending on first-pass retrieval effectiveness. This work extends GRF to dense and learned sparse retrieval paradigms with experiments over six standard document ranking benchmarks. We find that GRF improves over comparable PRF techniques by around 10% on both precision and recall-oriented measures. Nonetheless, query analysis shows that GRF and PRF have contrasting benefits, with GRF providing external context not present in first-pass retrieval, whereas PRF grounds the query to the information contained within the target corpus. Thus, we propose combining generative and pseudo-relevance feedback ranking signals to achieve the benefits of both feedback classes, which signifi
&lt;/p&gt;</description></item><item><title>BactInt&#26159;&#19968;&#31181;&#38754;&#21521;&#39046;&#22495;&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#65292;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#20174;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#20013;&#25552;&#21462;&#32454;&#33740;&#38388;&#30456;&#20114;&#20316;&#29992;&#24182;&#25366;&#25496;&#29305;&#23450;&#32454;&#33740;&#32676;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#20844;&#24320;&#21487;&#29992;&#30340;BactInt&#35821;&#26009;&#24211;&#26631;&#27880;&#20102;1200&#31687;PubMed&#25688;&#35201;&#12290;</title><link>http://arxiv.org/abs/2305.07468</link><description>&lt;p&gt;
BactInt:&#19968;&#31181;&#38754;&#21521;&#39046;&#22495;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#21644;&#19968;&#20010;&#35821;&#26009;&#24211;&#65292;&#29992;&#20110;&#20174;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#20013;&#25552;&#21462;&#32454;&#33740;&#38388;&#30456;&#20114;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
BactInt: A domain driven transfer learning approach and a corpus for extracting inter-bacterial interactions from biomedical text. (arXiv:2305.07468v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07468
&lt;/p&gt;
&lt;p&gt;
BactInt&#26159;&#19968;&#31181;&#38754;&#21521;&#39046;&#22495;&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#65292;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#20174;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#20013;&#25552;&#21462;&#32454;&#33740;&#38388;&#30456;&#20114;&#20316;&#29992;&#24182;&#25366;&#25496;&#29305;&#23450;&#32454;&#33740;&#32676;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#20844;&#24320;&#21487;&#29992;&#30340;BactInt&#35821;&#26009;&#24211;&#26631;&#27880;&#20102;1200&#31687;PubMed&#25688;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#23398;&#39046;&#22495;&#20013;&#19981;&#21516;&#31867;&#22411;&#24494;&#29983;&#29289;&#22312;&#29983;&#29289;&#23398;&#31354;&#38388;&#20013;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#36825;&#20123;&#24494;&#29983;&#29289;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#26159;&#24494;&#29983;&#29289;&#32676;&#33853;&#32467;&#26500;&#30340;&#22522;&#26412;&#26500;&#24314;&#21333;&#20803;&#12290;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#20013;&#30340;&#35777;&#25454;&#21487;&#20316;&#20026;&#39044;&#27979;&#36825;&#31181;&#30456;&#20114;&#20316;&#29992;&#30340;&#21487;&#38752;&#26469;&#28304;&#12290;&#28982;&#32780;&#65292;&#38405;&#35835;&#28023;&#37327;&#19988;&#19981;&#26029;&#22686;&#38271;&#30340;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#26159;&#19968;&#39033;&#32791;&#26102;&#24182;&#20196;&#20154;&#26395;&#32780;&#29983;&#30031;&#30340;&#24037;&#20316;&#12290;&#36825;&#23601;&#24517;&#28982;&#38656;&#35201;&#24320;&#21457;&#33258;&#21160;&#21270;&#26041;&#27861;&#26469;&#20934;&#30830;&#25552;&#21462;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#20013;&#25152;&#25253;&#36947;&#30340;&#32454;&#33740;&#20851;&#31995;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20174;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#20013;&#33258;&#21160;&#25552;&#21462;&#24494;&#29983;&#29289;&#30456;&#20114;&#20316;&#29992;&#65288;&#29305;&#21035;&#26159;&#32454;&#33740;&#20043;&#38388;&#65289;&#30340;&#26041;&#27861;&#20197;&#21450;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#26469;&#25552;&#39640;&#20854;&#20934;&#30830;&#24615;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#25551;&#36848;&#20102;&#19968;&#20010;&#31649;&#36947;&#65292;&#29992;&#20110;&#25366;&#25496;&#29305;&#23450;&#32454;&#33740;&#32676;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#31532;&#19968;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;Bacterial Interaction (BactInt)&#35821;&#26009;&#24211;&#65292;&#20854;&#20013;&#21253;&#25324;1200&#31687;PubMed&#25688;&#35201;&#65292;&#27880;&#37322;&#26377;&#32454;&#33740;&#38388;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
The community of different types of microbes present in a biological niche plays a very important role in functioning of the system. The crosstalk or interactions among the different microbes contributes to the building blocks of such microbial community structures. Evidence reported in biomedical text serves as a reliable source for predicting such interactions. However, going through the vast and ever-increasing volume of biomedical literature is an intimidating and time consuming process. This necessitates development of automated methods capable of accurately extracting bacterial relations reported in biomedical literature. In this paper, we introduce a method for automated extraction of microbial interactions (specifically between bacteria) from biomedical literature along with ways of using transfer learning to improve its accuracy. We also describe a pipeline using which relations among specific bacteria groups can be mined. Additionally, we introduce the first publicly availabl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#30693;&#35782;&#36719;&#34701;&#21512;&#26694;&#26550;&#29992;&#20110;&#22810;&#27169;&#24577;&#25512;&#33616;&#65292;&#35813;&#26694;&#26550;&#22312;&#29305;&#24449;&#25552;&#21462;&#21644;&#25512;&#33616;&#36807;&#31243;&#20013;&#38598;&#25104;&#20102;&#39046;&#22495;&#29305;&#23450;&#30340;&#30693;&#35782;&#65292;&#35299;&#20915;&#20102;&#27169;&#22411;&#25311;&#21512;&#20559;&#24046;&#21644;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.07419</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#25512;&#33616;&#20013;&#30340;&#30693;&#35782;&#36719;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
Knowledge Soft Integration for Multimodal Recommendation. (arXiv:2305.07419v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07419
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#30693;&#35782;&#36719;&#34701;&#21512;&#26694;&#26550;&#29992;&#20110;&#22810;&#27169;&#24577;&#25512;&#33616;&#65292;&#35813;&#26694;&#26550;&#22312;&#29305;&#24449;&#25552;&#21462;&#21644;&#25512;&#33616;&#36807;&#31243;&#20013;&#38598;&#25104;&#20102;&#39046;&#22495;&#29305;&#23450;&#30340;&#30693;&#35782;&#65292;&#35299;&#20915;&#20102;&#27169;&#22411;&#25311;&#21512;&#20559;&#24046;&#21644;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#26159;&#22914;&#20309;&#26377;&#25928;&#22320;&#21033;&#29992;&#22810;&#27169;&#24577;&#20869;&#23481;&#23454;&#29616;&#26356;&#20010;&#24615;&#21270;&#30340;&#25512;&#33616;&#12290;&#23613;&#31649;&#26377;&#21508;&#31181;&#21508;&#26679;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#22823;&#22810;&#25968;&#35299;&#20915;&#26041;&#26696;&#37117;&#24573;&#30053;&#20102;&#29420;&#31435;&#29305;&#24449;&#25552;&#21462;&#36807;&#31243;&#25152;&#33719;&#24471;&#30693;&#35782;&#19982;&#19979;&#28216;&#25512;&#33616;&#20219;&#21153;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22810;&#27169;&#24577;&#29305;&#24449;&#25552;&#21462;&#36807;&#31243;&#26410;&#32435;&#20837;&#19982;&#25512;&#33616;&#20219;&#21153;&#30456;&#20851;&#30340;&#20808;&#21069;&#30693;&#35782;&#65292;&#32780;&#25512;&#33616;&#20219;&#21153;&#32463;&#24120;&#30452;&#25509;&#23558;&#36825;&#20123;&#22810;&#27169;&#24577;&#29305;&#24449;&#29992;&#20316;&#36741;&#21161;&#20449;&#24687;&#12290;&#36825;&#31181;&#19981;&#21305;&#37197;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#25311;&#21512;&#20559;&#24046;&#21644;&#24615;&#33021;&#19979;&#38477;&#65292;&#26412;&#25991;&#23558;&#20854;&#31216;&#20026;&#8220;&#30693;&#35782;&#35781;&#21650;&#8221;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#30693;&#35782;&#36719;&#34701;&#21512;&#24179;&#34913;&#22810;&#27169;&#24577;&#29305;&#24449;&#21033;&#29992;&#21644;&#30693;&#35782;&#35781;&#21650;&#38382;&#39064;&#24102;&#26469;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#22810;&#27169;&#24577;&#25512;&#33616;&#30340;&#30693;&#35782;&#36719;&#34701;&#21512;&#26694;&#26550;&#65292;&#31616;&#31216;KSI&#65292;&#23427;&#23558;&#39046;&#22495;&#29305;&#23450;&#30340;&#30693;&#35782;&#38598;&#25104;&#21040;&#29305;&#24449;&#25552;&#21462;&#21644;&#25512;&#33616;&#36807;&#31243;&#20013;&#12290;&#22312;&#19977;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;KSI&#22312;&#25512;&#33616;&#20934;&#30830;&#24615;&#21644;&#22810;&#26679;&#24615;&#26041;&#38754;&#36229;&#36234;&#20102;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the main challenges in modern recommendation systems is how to effectively utilize multimodal content to achieve more personalized recommendations. Despite various proposed solutions, most of them overlook the mismatch between the knowledge gained from independent feature extraction processes and downstream recommendation tasks. Specifically, multimodal feature extraction processes do not incorporate prior knowledge relevant to recommendation tasks, while recommendation tasks often directly use these multimodal features as side information. This mismatch can lead to model fitting biases and performance degradation, which this paper refers to as the \textit{curse of knowledge} problem. To address this issue, we propose using knowledge soft integration to balance the utilization of multimodal features and the curse of knowledge problem it brings about. To achieve this, we put forward a Knowledge Soft Integration framework for the multimodal recommendation, abbreviated as KSI, whic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;InteR&#65292;&#36890;&#36807;&#25628;&#32034;&#24341;&#25806;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#20132;&#20114;&#20419;&#36827;&#30693;&#35782;&#31934;&#28860;&#65292;&#20174;&#32780;&#25552;&#39640;&#26816;&#32034;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.07402</link><description>&lt;p&gt;
&#25628;&#32034;&#24341;&#25806;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38388;&#30340;&#20132;&#20114;&#20248;&#21270;&#30693;&#35782;&#31934;&#28860;
&lt;/p&gt;
&lt;p&gt;
Knowledge Refinement via Interaction Between Search Engines and Large Language Models. (arXiv:2305.07402v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07402
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;InteR&#65292;&#36890;&#36807;&#25628;&#32034;&#24341;&#25806;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#20132;&#20114;&#20419;&#36827;&#30693;&#35782;&#31934;&#28860;&#65292;&#20174;&#32780;&#25552;&#39640;&#26816;&#32034;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#24687;&#26816;&#32034;&#22312;&#20174;&#22823;&#37327;&#25968;&#25454;&#20013;&#23450;&#20301;&#30456;&#20851;&#36164;&#28304;&#26041;&#38754;&#20855;&#26377;&#37325;&#35201;&#20316;&#29992;&#65292;&#20854;&#24212;&#29992;&#24050;&#20174;&#20256;&#32479;&#30693;&#35782;&#24211;&#21457;&#23637;&#33267;&#29616;&#20195;&#25628;&#32034;&#24341;&#25806;&#65288;SEs&#65289;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#36827;&#19968;&#27493;&#36890;&#36807;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#19982;&#25628;&#32034;&#31995;&#32479;&#20132;&#20114;&#38761;&#21629;&#24615;&#22320;&#25913;&#21464;&#20102;&#35813;&#39046;&#22495;&#12290;&#26412;&#25991;&#25506;&#32034;&#20102;LLMs&#21644;SEs&#30340;&#20248;&#32570;&#28857;&#65292;&#24378;&#35843;&#23427;&#20204;&#22312;&#29702;&#35299;&#29992;&#25143;&#26597;&#35810;&#21644;&#26816;&#32034;&#26368;&#26032;&#20449;&#24687;&#26041;&#38754;&#30340;&#21508;&#33258;&#20248;&#21183;&#12290;&#20026;&#20102;&#21033;&#29992;&#20004;&#31181;&#33539;&#20363;&#30340;&#20248;&#21183;&#24182;&#36991;&#20813;&#20854;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;InteR&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#36807;SEs&#21644;LLMs&#20043;&#38388;&#30340;&#20132;&#20114;&#20419;&#36827;&#30693;&#35782;&#31934;&#28860;&#30340;&#26032;&#26694;&#26550;&#12290; InteR&#20351;SEs&#33021;&#22815;&#20351;&#29992;LLM&#29983;&#25104;&#30340;&#25688;&#35201;&#26469;&#35843;&#25972;&#26597;&#35810;&#65292;&#21516;&#26102;&#20351;LLMs&#33021;&#22815;&#20351;&#29992;SE&#26816;&#32034;&#21040;&#30340;&#25991;&#26723;&#26469;&#22686;&#24378;&#25552;&#31034;&#12290;&#36825;&#31181;&#36845;&#20195;&#30340;&#31934;&#28860;&#36807;&#31243;&#22686;&#24378;&#20102;SEs&#21644;LLMs&#30340;&#36755;&#20837;&#65292;&#20174;&#32780;&#23548;&#33268;&#26356;&#20934;&#30830;&#30340;&#26816;&#32034;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Information retrieval (IR) plays a crucial role in locating relevant resources from vast amounts of data, and its applications have evolved from traditional knowledge bases to modern search engines (SEs). The emergence of large language models (LLMs) has further revolutionized the field by enabling users to interact with search systems in natural language. In this paper, we explore the advantages and disadvantages of LLMs and SEs, highlighting their respective strengths in understanding user-issued queries and retrieving up-to-date information. To leverage the benefits of both paradigms while circumventing their limitations, we propose InteR, a novel framework that facilitates knowledge refinement through interaction between SEs and LLMs. InteR allows SEs to refine knowledge in query using LLM-generated summaries and enables LLMs to enhance prompts using SE-retrieved documents. This iterative refinement process augments the inputs of SEs and LLMs, leading to more accurate retrieval. Ex
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#38899;&#20048;&#37325;&#25490;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#28145;&#24230;&#38899;&#39057;&#34920;&#31034;&#23545;&#38899;&#20048;&#36827;&#34892;&#20998;&#23618;&#20998;&#21106;&#65292;&#32771;&#34385;&#27573;&#33853;&#36793;&#30028;&#21644;&#38899;&#20048;&#21151;&#33021;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20855;&#26377;&#19968;&#33268;&#38899;&#20048;&#21457;&#23637;&#21644;&#21548;&#35273;&#26080;&#24863;&#30693;&#30340;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2305.07347</link><description>&lt;p&gt;
&#20351;&#29992;&#20998;&#23618;&#20998;&#21106;&#30340;&#26041;&#27861;&#36827;&#34892;&#38899;&#20048;&#37325;&#25490;
&lt;/p&gt;
&lt;p&gt;
Music Rearrangement Using Hierarchical Segmentation. (arXiv:2305.07347v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07347
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#38899;&#20048;&#37325;&#25490;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#28145;&#24230;&#38899;&#39057;&#34920;&#31034;&#23545;&#38899;&#20048;&#36827;&#34892;&#20998;&#23618;&#20998;&#21106;&#65292;&#32771;&#34385;&#27573;&#33853;&#36793;&#30028;&#21644;&#38899;&#20048;&#21151;&#33021;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20855;&#26377;&#19968;&#33268;&#38899;&#20048;&#21457;&#23637;&#21644;&#21548;&#35273;&#26080;&#24863;&#30693;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#20048;&#37325;&#25490;&#28041;&#21450;&#21040;&#38899;&#20048;&#26354;&#30446;&#30340;&#37325;&#26032;&#32452;&#21512;&#12289;&#21024;&#38500;&#21644;&#37325;&#22797;&#65292;&#30446;&#30340;&#26159;&#20135;&#29983;&#19968;&#20010;&#19981;&#21516;&#26102;&#38388;&#38271;&#24230;&#30340;&#29420;&#31435;&#29256;&#26412;&#12290;&#36825;&#36890;&#24120;&#30001;&#19987;&#19994;&#38899;&#20048;&#24037;&#31243;&#24072;&#36827;&#34892;&#21019;&#36896;&#24615;&#32780;&#32791;&#26102;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#37325;&#26032;&#25490;&#21015;&#38899;&#20048;&#24405;&#38899;&#30340;&#26041;&#27861;&#65292;&#32771;&#34385;&#20102;&#24405;&#38899;&#30340;&#20998;&#23618;&#32467;&#26500;&#12290;&#20197;&#24448;&#30340;&#26041;&#27861;&#20165;&#20851;&#27880;&#38899;&#39057;&#20013;&#21487;&#33021;&#23548;&#33268;&#24179;&#28369;&#36716;&#25442;&#30340;&#20999;&#21106;&#28857;&#30340;&#35782;&#21035;&#12290;&#25105;&#20204;&#21033;&#29992;&#28145;&#24230;&#38899;&#39057;&#34920;&#31034;&#26469;&#23545;&#20316;&#21697;&#36827;&#34892;&#20998;&#23618;&#20998;&#21106;&#65292;&#24182;&#23450;&#20041;&#23545;&#27573;&#33853;&#36793;&#30028;&#21644;&#38899;&#20048;&#21151;&#33021;&#36827;&#34892;&#20999;&#21106;&#28857;&#25628;&#32034;&#12290;&#25105;&#20204;&#26681;&#25454;&#30456;&#20284;&#24615;&#21644;&#23427;&#20204;&#25152;&#23646;&#30340;&#27573;&#33853;&#26469;&#35780;&#20998;&#36866;&#24403;&#30340;&#36827;&#20837;&#28857;&#21644;&#36864;&#20986;&#28857;&#23545;&#65292;&#24182;&#23450;&#20041;&#19968;&#20010;&#26368;&#20248;&#36335;&#24452;&#25628;&#32034;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#36873;&#25321;&#30340;&#20999;&#21106;&#28857;&#36890;&#24120;&#23545;&#21548;&#20247;&#26368;&#19981;&#21487;&#23519;&#35273;&#65292;&#24182;&#23548;&#33268;&#26356;&#19968;&#33268;&#38899;&#20048;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Music rearrangement involves reshuffling, deleting, and repeating sections of a music piece with the goal of producing a standalone version that has a different duration. It is a creative and time-consuming task commonly performed by an expert music engineer. In this paper, we propose a method for automatically rearranging music recordings that takes into account the hierarchical structure of the recording. Previous approaches focus solely on identifying cut-points in the audio that could result in smooth transitions. We instead utilize deep audio representations to hierarchically segment the piece and define a cut-point search subject to the boundaries and musical functions of the segments. We score suitable entry- and exit-point pairs based on their similarity and the segments they belong to, and define an optimal path search. Experimental results demonstrate the selected cut-points are most commonly imperceptible by listeners and result in more consistent musical development with le
&lt;/p&gt;</description></item><item><title>&#35813;&#39033;&#30446;&#30740;&#31350;&#20102;&#25552;&#39640;&#25968;&#23383;&#22270;&#20070;&#39302;&#20013;&#25968;&#23398;&#20869;&#23481;&#21450;&#35821;&#20041;&#20449;&#24687;&#30340;&#26816;&#32034;&#12289;&#25512;&#33616;&#21644;&#36741;&#21161;&#31995;&#32479;&#30340;&#26032;&#26041;&#27861;&#19982;&#25216;&#26415;&#65292;&#24182;&#20844;&#24067;&#20102;&#30456;&#24212;&#24037;&#20855;&#65292;&#20197;&#20415;&#30740;&#31350;&#20154;&#21592;&#26356;&#22909;&#22320;&#21033;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.07335</link><description>&lt;p&gt;
&#25968;&#23383;&#22270;&#20070;&#39302;&#20013;&#25968;&#23398;&#30693;&#35782;&#26816;&#32034;&#12289;&#25512;&#33616;&#21644;&#36741;&#21161;&#31995;&#32479;&#30340;&#21457;&#23637;&#26041;&#27861;&#21644;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
Methods and Tools to Advance the Retrieval of Mathematical Knowledge from Digital Libraries for Search-, Recommendation-, and Assistance-Systems. (arXiv:2305.07335v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07335
&lt;/p&gt;
&lt;p&gt;
&#35813;&#39033;&#30446;&#30740;&#31350;&#20102;&#25552;&#39640;&#25968;&#23383;&#22270;&#20070;&#39302;&#20013;&#25968;&#23398;&#20869;&#23481;&#21450;&#35821;&#20041;&#20449;&#24687;&#30340;&#26816;&#32034;&#12289;&#25512;&#33616;&#21644;&#36741;&#21161;&#31995;&#32479;&#30340;&#26032;&#26041;&#27861;&#19982;&#25216;&#26415;&#65292;&#24182;&#20844;&#24067;&#20102;&#30456;&#24212;&#24037;&#20855;&#65292;&#20197;&#20415;&#30740;&#31350;&#20154;&#21592;&#26356;&#22909;&#22320;&#21033;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#39033;&#30446;&#30740;&#31350;&#20102;&#25552;&#39640;&#25968;&#23398;&#20869;&#23481;&#21450;&#20854;&#35821;&#20041;&#20449;&#24687;&#22312;&#21508;&#31181;&#20449;&#24687;&#26816;&#32034;&#24212;&#29992;&#20013;&#21487;&#35775;&#38382;&#24615;&#30340;&#26032;&#26041;&#27861;&#21644;&#25216;&#26415;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#35813;&#39033;&#30446;&#35299;&#20915;&#20102;&#19977;&#20010;&#20027;&#35201;&#30740;&#31350;&#25361;&#25112;&#65306;&#65288;1&#65289;&#25968;&#23398;&#34920;&#36798;&#24335;&#30340;&#21477;&#27861;&#20998;&#26512;&#65292;&#65288;2&#65289;&#25968;&#23398;&#34920;&#36798;&#24335;&#30340;&#35821;&#20041;&#22686;&#24378;&#65292;&#65288;3&#65289;&#20351;&#29992;&#36136;&#37327;&#24230;&#37327;&#21644;&#28436;&#31034;&#31243;&#24207;&#36827;&#34892;&#35780;&#20272;&#12290;&#20026;&#20102;&#20351;&#25105;&#20204;&#30340;&#30740;&#31350;&#23545;&#30740;&#31350;&#31038;&#21306;&#26377;&#25152;&#29992;&#22788;&#65292;&#25105;&#20204;&#20844;&#24067;&#20102;&#33021;&#22815;&#20351;&#30740;&#31350;&#20154;&#21592;&#26356;&#26377;&#25928;&#12289;&#26356;&#39640;&#25928;&#22320;&#22788;&#29702;&#25968;&#23398;&#34920;&#36798;&#24335;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
This project investigated new approaches and technologies to enhance the accessibility of mathematical content and its semantic information for a broad range of information retrieval applications. To achieve this goal, the project addressed three main research challenges: (1) syntactic analysis of mathematical expressions, (2) semantic enrichment of mathematical expressions, and (3) evaluation using quality metrics and demonstrators. To make our research useful for the research community, we published tools that enable researchers to process mathematical expressions more effectively and efficiently.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#22320;&#29702;&#35299;&#26512;&#21644;&#22320;&#29702;&#26631;&#35760;&#26041;&#27861;&#65292;&#20197;&#30830;&#23450;&#26412;&#22320;&#26032;&#38395;&#25991;&#31456;&#30340;&#20301;&#32622;&#21644;&#24433;&#21709;&#33539;&#22260;&#65292;&#20197;&#20415;&#20026;&#20840;&#29699;&#29992;&#25143;&#25552;&#20379;&#31934;&#20934;&#30340;&#26412;&#22320;&#26032;&#38395;&#25512;&#33616;&#26381;&#21153;&#12290;</title><link>http://arxiv.org/abs/2305.07168</link><description>&lt;p&gt;
&#26412;&#22320;&#29983;&#27963;&#65306;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#22320;&#29702;&#35299;&#26512;&#21644;&#22320;&#29702;&#26631;&#35760;&#26041;&#27861;&#65292;&#20197;&#20026;&#20840;&#29699;&#25552;&#20379;&#26412;&#22320;&#26032;&#38395;&#12290;
&lt;/p&gt;
&lt;p&gt;
Local Life: Stay Informed Around You, A Scalable Geoparsing and Geotagging Approach to Serve Local News Worldwide. (arXiv:2305.07168v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07168
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#22320;&#29702;&#35299;&#26512;&#21644;&#22320;&#29702;&#26631;&#35760;&#26041;&#27861;&#65292;&#20197;&#30830;&#23450;&#26412;&#22320;&#26032;&#38395;&#25991;&#31456;&#30340;&#20301;&#32622;&#21644;&#24433;&#21709;&#33539;&#22260;&#65292;&#20197;&#20415;&#20026;&#20840;&#29699;&#29992;&#25143;&#25552;&#20379;&#31934;&#20934;&#30340;&#26412;&#22320;&#26032;&#38395;&#25512;&#33616;&#26381;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#21508;&#31181;&#22909;&#22788;&#65292;&#26412;&#22320;&#26032;&#38395;&#22312;&#26032;&#38395;&#34892;&#19994;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290; &#23427;&#20026;&#24403;&#22320;&#35266;&#20247;&#25552;&#20379;&#20449;&#24687;&#65292;&#24110;&#21161;&#20182;&#20204;&#21442;&#19982;&#31038;&#21306;&#21644;&#20852;&#36259;&#12290; &#23427;&#36824;&#20316;&#20026;&#21487;&#38752;&#30340;&#20107;&#23454;&#25253;&#36947;&#26469;&#28304;&#65292;&#21487;&#20197;&#38450;&#27490;&#38169;&#35823;&#20449;&#24687;&#12290; &#27492;&#22806;&#65292;&#23427;&#21487;&#20197;&#24433;&#21709;&#22269;&#23478;&#35266;&#20247;&#65292;&#22240;&#20026;&#19968;&#20123;&#26412;&#22320;&#25925;&#20107;&#21487;&#33021;&#23545;&#25919;&#27835;&#65292;&#29615;&#22659;&#25110;&#29359;&#32618;&#20135;&#29983;&#26356;&#24191;&#27867;&#30340;&#24433;&#21709;&#12290; &#22240;&#27492;&#65292;&#26816;&#27979;&#26412;&#22320;&#26032;&#38395;&#30340;&#30830;&#20999;&#22320;&#29702;&#20301;&#32622;&#21644;&#24433;&#21709;&#33539;&#22260;&#23545;&#20110;&#26032;&#38395;&#25512;&#33616;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#12290; &#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#31532;&#20108;&#27493;&#65292;&#24182;&#25552;&#20986;&#65288;1&#65289;&#19968;&#20010;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#30830;&#23450;&#26412;&#22320;&#26032;&#38395;&#25991;&#31456;&#30340;&#20301;&#32622;&#21644;&#21322;&#24452;&#65292;&#65288;2&#65289;&#19968;&#31181;&#23558;&#29992;&#25143;&#20301;&#32622;&#19982;&#25991;&#31456;&#20301;&#32622;&#21327;&#35843;&#30340;&#26041;&#27861;&#65292;&#20197;&#21450;&#65288;3&#65289;&#19968;&#20010;&#24230;&#37327;&#26631;&#20934;&#26469;&#35780;&#20272;&#35813;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Local news has become increasingly important in the news industry due to its various benefits. It offers local audiences information that helps them participate in their communities and interests. It also serves as a reliable source of factual reporting that can prevent misinformation. Moreover, it can influence national audiences as some local stories may have wider implications for politics, environment or crime. Hence, detecting the exact geolocation and impact scope of local news is crucial for news recommendation systems. There are two fundamental things required in this process, (1) classify whether an article belongs to local news, and (2) identify the geolocation of the article and its scope of influence to recommend it to appropriate users. In this paper, we focus on the second step and propose (1) an efficient approach to determine the location and radius of local news articles, (2) a method to reconcile the user's location with the article's location, and (3) a metric to eva
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#25968;&#25454;&#21435;&#22122;&#30340;&#25512;&#33616;&#26694;&#26550;&#8212;&#8212;AutoDenoise&#65292;&#21033;&#29992;&#26174;&#24335;&#25968;&#25454;&#20316;&#20026;&#39564;&#35777;&#38598;&#21160;&#24577; guiding &#25512;&#33616;&#31639;&#27861;&#30340;&#35757;&#32451;&#65292;&#23545;&#38544;&#24335;&#25968;&#25454;&#36827;&#34892;&#21435;&#22122;&#22788;&#29702;&#65292;&#25552;&#39640;&#25512;&#33616;&#31995;&#32479;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.07070</link><description>&lt;p&gt;
&#33258;&#21160;&#25968;&#25454;&#21435;&#22122;&#25216;&#26415;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Automated Data Denoising for Recommendation. (arXiv:2305.07070v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07070
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#25968;&#25454;&#21435;&#22122;&#30340;&#25512;&#33616;&#26694;&#26550;&#8212;&#8212;AutoDenoise&#65292;&#21033;&#29992;&#26174;&#24335;&#25968;&#25454;&#20316;&#20026;&#39564;&#35777;&#38598;&#21160;&#24577; guiding &#25512;&#33616;&#31639;&#27861;&#30340;&#35757;&#32451;&#65292;&#23545;&#38544;&#24335;&#25968;&#25454;&#36827;&#34892;&#21435;&#22122;&#22788;&#29702;&#65292;&#25552;&#39640;&#25512;&#33616;&#31995;&#32479;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#22823;&#22810;&#25968;&#24179;&#21488;&#25910;&#38598;&#30340;&#21453;&#39304;&#25968;&#25454;&#26082;&#26377;&#22823;&#35268;&#27169;&#12289;&#33258;&#28982;&#22024;&#26434;&#30340;&#38544;&#24335;&#21453;&#39304;&#65292;&#20063;&#26377;&#23567;&#35268;&#27169;&#20294;&#39640;&#24230;&#30456;&#20851;&#30340;&#26174;&#24335;&#21453;&#39304;&#12290;&#30001;&#20110;&#25968;&#25454;&#31232;&#32570;&#30340;&#38382;&#39064;&#65292;&#38544;&#24335;&#21453;&#39304;&#36890;&#24120;&#26159;&#35757;&#32451;&#25512;&#33616;&#31995;&#32479;&#30340;&#40664;&#35748;&#36873;&#25321;&#65292;&#20294;&#36825;&#31181;&#25968;&#25454;&#21487;&#33021;&#38750;&#24120;&#22024;&#26434;&#65292;&#22240;&#20026;&#29992;&#25143;&#34892;&#20026;&#30340;&#38543;&#26426;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;&#36890;&#36807;&#21033;&#29992;&#20004;&#31181;&#21453;&#39304;&#30340;&#20248;&#21183;&#26469;&#24357;&#34917;&#21478;&#19968;&#31181;&#30340;&#24369;&#28857;&#65292;&#25105;&#20204;&#21487;&#20197;&#20960;&#20046;&#19981;&#33457;&#36153;&#20160;&#20040;&#20195;&#20215;&#26469;&#32531;&#35299;&#20197;&#19978;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#25968;&#25454;&#21435;&#22122;&#30340;&#26694;&#26550;&#8212;&#8212;AutoDenoise&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#65292;&#23427;&#20351;&#29992;&#23569;&#37327;&#26174;&#24335;&#25968;&#25454;&#20316;&#20026;&#39564;&#35777;&#38598;&#26469;&#25351;&#23548;&#25512;&#33616;&#31639;&#27861;&#30340;&#35757;&#32451;&#12290;AutoDenoise&#21463;&#21040;&#35838;&#31243;&#23398;&#20064;&#65288;CL&#65289;&#30340;&#24191;&#20041;&#23450;&#20041;&#30340;&#21551;&#21457;&#65292;&#23398;&#20250;&#33258;&#21160;&#21160;&#24577;&#22320;&#36827;&#34892;&#35838;&#31243;&#23398;&#20064;&#65292;&#36827;&#32780;&#23545;&#25968;&#25454;&#36827;&#34892;&#21435;&#22122;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
In real-world scenarios, most platforms collect both large-scale, naturally noisy implicit feedback and small-scale yet highly relevant explicit feedback. Due to the issue of data sparsity, implicit feedback is often the default choice for training recommender systems (RS), however, such data could be very noisy due to the randomness and diversity of user behaviors. For instance, a large portion of clicks may not reflect true user preferences and many purchases may result in negative reviews or returns. Fortunately, by utilizing the strengths of both types of feedback to compensate for the weaknesses of the other, we can mitigate the above issue at almost no cost. In this work, we propose an Automated Data Denoising framework, \textbf{\textit{AutoDenoise}}, for recommendation, which uses a small number of explicit data as validation set to guide the recommender training. Inspired by the generalized definition of curriculum learning (CL), AutoDenoise learns to automatically and dynamica
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#24178;&#25200;&#31649;&#29702;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;UAV&#36890;&#20449;&#20013;&#30340;&#24178;&#25200;&#38382;&#39064;&#65292;&#32780;&#26080;&#38656;&#20808;&#30693;&#24178;&#25200;&#20449;&#21495;&#30340;&#20449;&#36947;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2305.07069</link><description>&lt;p&gt;
&#26080;&#20154;&#26426;&#19977;&#32500;&#32593;&#32476;&#20013;&#30340;&#24178;&#25200;&#31649;&#29702;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65306;&#28508;&#21147;&#19982;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning for Interference Management in UAV-based 3D Networks: Potentials and Challenges. (arXiv:2305.07069v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07069
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#24178;&#25200;&#31649;&#29702;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;UAV&#36890;&#20449;&#20013;&#30340;&#24178;&#25200;&#38382;&#39064;&#65292;&#32780;&#26080;&#38656;&#20808;&#30693;&#24178;&#25200;&#20449;&#21495;&#30340;&#20449;&#36947;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#34562;&#31389;&#32593;&#32476;&#26159;&#22810;&#23567;&#21306;&#30340;&#65292;&#37319;&#29992;&#36890;&#29992;&#39057;&#29575;&#37325;&#29992;&#26469;&#26368;&#22823;&#21270;&#39057;&#35889;&#25928;&#29575;&#12290;&#36825;&#23548;&#33268;&#39640;&#24178;&#25200;&#12290;&#38543;&#30528;&#26080;&#20154;&#26426;(UAV)&#30340;&#37319;&#29992;&#65292;&#36825;&#20010;&#38382;&#39064;&#27491;&#22312;&#21464;&#24471;&#36234;&#26469;&#36234;&#20005;&#37325;&#65292;&#22240;&#20026;UAV&#36890;&#20449;&#20013;&#30340;&#30452;&#23556;&#39057;&#36947;&#20250;&#36805;&#36895;&#22686;&#21152;&#24178;&#25200;&#38142;&#36335;&#30340;&#24378;&#24230;&#21644;&#25968;&#37327;&#12290;&#29616;&#26377;&#30340;&#24178;&#25200;&#31649;&#29702;&#35299;&#20915;&#26041;&#26696;&#38656;&#35201;&#27599;&#20010;&#21457;&#23556;&#22120;&#30693;&#36947;&#24178;&#25200;&#20449;&#21495;&#30340;&#20449;&#36947;&#20449;&#24687;&#65292;&#20174;&#32780;&#23548;&#33268;&#36807;&#22810;&#30340;&#20449;&#20196;&#24320;&#38144;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#24178;&#25200;&#31649;&#29702;&#26469;&#35299;&#20915;&#36825;&#20010;&#32570;&#38519;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#34920;&#26126;&#21363;&#20351;&#19981;&#30693;&#36947;&#24178;&#25200;&#20449;&#21495;&#30340;&#20449;&#36947;&#20449;&#24687;&#65292;&#20063;&#20173;&#28982;&#21487;&#20197;&#26377;&#25928;&#22320;&#20943;&#36731;&#24178;&#25200;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#20351;&#29992;&#32447;&#24615;/&#20122;&#32447;&#24615;&#22797;&#26434;&#24230;&#25193;&#23637;&#31639;&#27861;&#21644;&#20351;&#29992;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#23545;&#20854;&#36827;&#34892;&#20998;&#25955;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern cellular networks are multi-cell and use universal frequency reuse to maximize spectral efficiency. This results in high inter-cell interference. This problem is growing as cellular networks become three-dimensional with the adoption of unmanned aerial vehicles (UAVs). This is because the strength and number of interference links rapidly increase due to the line-of-sight channels in UAV communications. Existing interference management solutions need each transmitter to know the channel information of interfering signals, rendering them impractical due to excessive signaling overhead. In this paper, we propose leveraging deep reinforcement learning for interference management to tackle this shortcoming. In particular, we show that interference can still be effectively mitigated even without knowing its channel information. We then discuss novel approaches to scale the algorithms with linear/sublinear complexity and decentralize them using multi-agent reinforcement learning. By ha
&lt;/p&gt;</description></item><item><title>TpD&#20026;&#36845;&#20195;&#25628;&#32034;&#31227;&#21160;&#24212;&#29992;&#23631;&#24149;&#25552;&#20379;&#20102;&#20132;&#20114;&#24335;&#33609;&#22270;&#21644;&#20851;&#38190;&#23383;&#25628;&#32034;&#25216;&#26415;&#30340;&#32467;&#21512;&#26041;&#27861;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#26356;&#24555;&#22320;&#26597;&#25214;&#25152;&#38656;&#23631;&#24149;&#12290;</title><link>http://arxiv.org/abs/2305.06165</link><description>&lt;p&gt;
&#36890;&#36807;&#25991;&#26412;&#21644;&#28034;&#40486;&#25628;&#32034;&#31227;&#21160;&#24212;&#29992;&#23631;&#24149;
&lt;/p&gt;
&lt;p&gt;
Searching Mobile App Screens via Text + Doodle. (arXiv:2305.06165v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06165
&lt;/p&gt;
&lt;p&gt;
TpD&#20026;&#36845;&#20195;&#25628;&#32034;&#31227;&#21160;&#24212;&#29992;&#23631;&#24149;&#25552;&#20379;&#20102;&#20132;&#20114;&#24335;&#33609;&#22270;&#21644;&#20851;&#38190;&#23383;&#25628;&#32034;&#25216;&#26415;&#30340;&#32467;&#21512;&#26041;&#27861;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#26356;&#24555;&#22320;&#26597;&#25214;&#25152;&#38656;&#23631;&#24149;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#36164;&#28304;&#20013;&#35201;&#25214;&#21040;&#29305;&#23450;&#30340;&#31227;&#21160;&#24212;&#29992;&#31243;&#24207;&#23631;&#24149;&#23616;&#38480;&#20110;&#22522;&#26412;&#30340;&#20851;&#38190;&#35789;&#25628;&#32034;&#65292;&#22914;Google&#22270;&#20687;&#25628;&#32034;&#65292;&#25110;&#38656;&#35201;&#23436;&#25972;&#30340;&#26597;&#35810;&#23631;&#24149;&#22270;&#20687;&#65292;&#22914;Swire&#30340;&#24773;&#20917;&#12290;&#28982;&#32780;&#65292;&#31867;&#20284;PSDoodle&#30340;&#20132;&#20114;&#24335;&#37096;&#20998;&#22522;&#20110;&#33609;&#22270;&#30340;&#35299;&#20915;&#26041;&#26696;&#23384;&#22312;&#31934;&#24230;&#19981;&#20934;&#30830;&#21644;&#26080;&#27861;&#32771;&#34385;&#23631;&#24149;&#19978;&#20986;&#29616;&#25991;&#26412;&#31561;&#23616;&#38480;&#24615;&#12290;&#19968;&#31181;&#28508;&#22312;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#28041;&#21450;&#23454;&#26045;&#19968;&#20010;&#31995;&#32479;&#65292;&#20026;&#26377;&#25928;&#26500;&#24314;&#29992;&#25143;&#30028;&#38754;&#20803;&#32032;&#25552;&#20379;&#20132;&#20114;&#24335;&#37096;&#20998;&#33609;&#22270;&#21151;&#33021;&#12290;&#27492;&#22806;&#65292;&#35813;&#31995;&#32479;&#24212;&#32467;&#21512;&#25991;&#26412;&#26597;&#35810;&#20197;&#36827;&#19968;&#27493;&#22686;&#24378;&#20854;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;TpD&#20195;&#34920;&#20102;&#36890;&#36807;&#32467;&#21512;&#20132;&#20114;&#24335;&#33609;&#22270;&#21644;&#20851;&#38190;&#23383;&#25628;&#32034;&#25216;&#26415;&#36827;&#34892;&#23631;&#24149;&#36845;&#20195;&#25628;&#32034;&#30340;&#24320;&#21019;&#24615;&#24037;&#20316;&#12290;TpD&#24314;&#31435;&#22312;&#22823;&#32422;58k&#20010;Android&#24212;&#29992;&#31243;&#24207;&#23631;&#24149;&#30340;Rico&#23384;&#20648;&#24211;&#21644;PSDoodle&#30340;&#32452;&#21512;&#22522;&#30784;&#19978;&#12290;&#25105;&#20204;&#19982;&#31532;&#19977;&#26041;&#36719;&#20214;&#24320;&#21457;&#20154;&#21592;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;PSDoodle&#25552;&#20379;&#20102;&#39640;&#31934;&#24230;&#12289;&#39640;&#21484;&#22238;&#30340;&#32467;&#26524;&#65292;&#24182;&#19988;TpD&#20351;&#29992;&#25143;&#33021;&#22815;&#26356;&#24555;&#22320;&#26597;&#25214;&#25152;&#38656;&#23631;&#24149;&#12290;
&lt;/p&gt;
&lt;p&gt;
Locating a specific mobile application screen from existing repositories is restricted to basic keyword searches, such as Google Image Search, or necessitates a complete query screen image, as in the case of Swire. However, interactive partial sketch-based solutions like PSDoodle have limitations, including inaccuracy and an inability to consider text appearing on the screen. A potentially effective solution involves implementing a system that provides interactive partial sketching functionality for efficiently structuring user interface elements. Additionally, the system should incorporate text queries to enhance its capabilities further. Our approach, TpD, represents the pioneering effort to enable an iterative search of screens by combining interactive sketching and keyword search techniques. TpD is built on a combination of the Rico repository of approximately 58k Android app screens and the PSDoodle. Our evaluation with third-party software developers showed that PSDoodle provided
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#35268;&#27169;&#25628;&#32034;&#21644;&#25512;&#33616;&#23454;&#39564;&#30340;&#26174;&#33879;&#24615;&#26816;&#39564;&#34892;&#20026;&#65292;&#32467;&#26524;&#21457;&#29616;&#22312;&#22823;&#26679;&#26412;&#19979;Wilcoxon&#21644;Sign&#27979;&#35797;&#30340;1&#22411;&#38169;&#35823;&#29575;&#26174;&#33879;&#26356;&#39640;&#65292;&#24314;&#35758;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#20351;&#29992;bootstrap&#12289;&#38543;&#26426;&#21270;&#21644;t&#27979;&#35797;&#12290;</title><link>http://arxiv.org/abs/2305.02461</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#25628;&#32034;&#21644;&#25512;&#33616;&#23454;&#39564;&#30340;&#26174;&#33879;&#24615;&#26816;&#39564;
&lt;/p&gt;
&lt;p&gt;
Inference at Scale Significance Testing for Large Search and Recommendation Experiments. (arXiv:2305.02461v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02461
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#35268;&#27169;&#25628;&#32034;&#21644;&#25512;&#33616;&#23454;&#39564;&#30340;&#26174;&#33879;&#24615;&#26816;&#39564;&#34892;&#20026;&#65292;&#32467;&#26524;&#21457;&#29616;&#22312;&#22823;&#26679;&#26412;&#19979;Wilcoxon&#21644;Sign&#27979;&#35797;&#30340;1&#22411;&#38169;&#35823;&#29575;&#26174;&#33879;&#26356;&#39640;&#65292;&#24314;&#35758;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#20351;&#29992;bootstrap&#12289;&#38543;&#26426;&#21270;&#21644;t&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#20449;&#24687;&#26816;&#32034;&#30740;&#31350;&#24050;&#32463;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#20197;&#30830;&#23450;&#21738;&#31181;&#32479;&#35745;&#25216;&#26415;&#36866;&#29992;&#20110;&#27604;&#36739;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30740;&#31350;&#38598;&#20013;&#20110;TREC&#26679;&#24335;&#30340;&#23454;&#39564;&#65292;&#36890;&#24120;&#23569;&#20110;100&#20010;&#20027;&#39064;&#12290;&#27809;&#26377;&#31867;&#20284;&#30340;&#30740;&#31350;&#36866;&#29992;&#20110;&#22823;&#35268;&#27169;&#25628;&#32034;&#21644;&#25512;&#33616;&#23454;&#39564;&#65307;&#36825;&#20123;&#30740;&#31350;&#36890;&#24120;&#28041;&#21450;&#25968;&#21315;&#20010;&#20027;&#39064;&#25110;&#29992;&#25143;&#20197;&#21450;&#26356;&#31232;&#30095;&#30340;&#30456;&#20851;&#24615;&#21028;&#26029;&#65292;&#22240;&#27492;&#19981;&#28165;&#26970;&#20998;&#26512;&#20256;&#32479;TREC&#23454;&#39564;&#30340;&#24314;&#35758;&#26159;&#21542;&#36866;&#29992;&#20110;&#36825;&#20123;&#24773;&#20917;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23454;&#35777;&#30740;&#31350;&#20102;&#22823;&#35268;&#27169;&#25628;&#32034;&#21644;&#25512;&#33616;&#35780;&#20272;&#25968;&#25454;&#30340;&#26174;&#33879;&#24615;&#26816;&#39564;&#34892;&#20026;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;Wilcoxon&#21644;Sign&#27979;&#35797;&#26174;&#31034;&#20986;&#26174;&#33879;&#26356;&#39640;&#30340;1&#22411;&#38169;&#35823;&#29575;&#65292;&#32780;&#19981;&#26159;&#26356;&#19968;&#33268;&#31526;&#21512;&#39044;&#26399;&#38169;&#35823;&#29575;&#30340;bootstrap&#12289;&#38543;&#26426;&#21270;&#21644;t&#27979;&#35797;&#12290;&#34429;&#28982;&#32479;&#35745;&#27979;&#35797;&#22312;&#26679;&#26412;&#36739;&#23567;&#26102;&#26174;&#31034;&#20986;&#21151;&#29575;&#24046;&#24322;&#65292;&#20294;&#22312;&#21151;&#29575;&#30456;&#21516;&#26102;&#26174;&#31034;&#20986;&#27809;&#26377;&#21306;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
A number of information retrieval studies have been done to assess which statistical techniques are appropriate for comparing systems. However, these studies are focused on TREC-style experiments, which typically have fewer than 100 topics. There is no similar line of work for large search and recommendation experiments; such studies typically have thousands of topics or users and much sparser relevance judgements, so it is not clear if recommendations for analyzing traditional TREC experiments apply to these settings. In this paper, we empirically study the behavior of significance tests with large search and recommendation evaluation data. Our results show that the Wilcoxon and Sign tests show significantly higher Type-1 error rates for large sample sizes than the bootstrap, randomization and t-tests, which were more consistent with the expected error rate. While the statistical tests displayed differences in their power for smaller sample sizes, they showed no difference in their po
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SimLM&#30340;&#23494;&#38598;&#25991;&#26412;&#26816;&#32034;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#37319;&#29992;&#29942;&#39048;&#26550;&#26500;&#21644;&#26367;&#20195;&#35821;&#35328;&#24314;&#27169;&#30446;&#26631;&#12290;&#22312;&#26080;&#26631;&#35760;&#30340;&#24773;&#20917;&#19979;&#65292;&#23427;&#20063;&#26159;&#36866;&#29992;&#30340;&#12290;&#30456;&#27604;&#20110;&#24378;&#22522;&#32447;&#27169;&#22411;&#65292;SimLM&#22312;&#22810;&#20010;&#22823;&#35268;&#27169;&#25490;&#24207;&#20219;&#21153;&#25968;&#25454;&#38598;&#19978;&#26174;&#31034;&#20986;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#29978;&#33267;&#36229;&#36807;&#20102;&#38656;&#35201;&#26356;&#22810;&#23384;&#20648;&#25104;&#26412;&#30340;&#22810;&#21521;&#37327;&#26041;&#27861;ColBERTv2&#12290;</title><link>http://arxiv.org/abs/2207.02578</link><description>&lt;p&gt;
SimLM&#65306;&#34920;&#31034;&#29942;&#39048;&#39044;&#35757;&#32451;&#29992;&#20110;&#23494;&#38598;&#25991;&#26412;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
SimLM: Pre-training with Representation Bottleneck for Dense Passage Retrieval. (arXiv:2207.02578v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.02578
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SimLM&#30340;&#23494;&#38598;&#25991;&#26412;&#26816;&#32034;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#37319;&#29992;&#29942;&#39048;&#26550;&#26500;&#21644;&#26367;&#20195;&#35821;&#35328;&#24314;&#27169;&#30446;&#26631;&#12290;&#22312;&#26080;&#26631;&#35760;&#30340;&#24773;&#20917;&#19979;&#65292;&#23427;&#20063;&#26159;&#36866;&#29992;&#30340;&#12290;&#30456;&#27604;&#20110;&#24378;&#22522;&#32447;&#27169;&#22411;&#65292;SimLM&#22312;&#22810;&#20010;&#22823;&#35268;&#27169;&#25490;&#24207;&#20219;&#21153;&#25968;&#25454;&#38598;&#19978;&#26174;&#31034;&#20986;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#29978;&#33267;&#36229;&#36807;&#20102;&#38656;&#35201;&#26356;&#22810;&#23384;&#20648;&#25104;&#26412;&#30340;&#22810;&#21521;&#37327;&#26041;&#27861;ColBERTv2&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SimLM&#65288;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#30340;&#30456;&#20284;&#24615;&#21305;&#37197;&#65289;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#23494;&#38598;&#25991;&#26412;&#26816;&#32034;&#39044;&#35757;&#32451;&#26041;&#27861;&#12290;&#23427;&#37319;&#29992;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#29942;&#39048;&#26550;&#26500;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#23558;&#25991;&#26412;&#20449;&#24687;&#21387;&#32553;&#20026;&#19968;&#20010;&#23494;&#38598;&#21521;&#37327;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#31181;&#26367;&#20195;&#35821;&#35328;&#24314;&#27169;&#30446;&#26631;&#65292;&#28789;&#24863;&#26469;&#33258;ELECTRA&#65292;&#20197;&#25913;&#21892;&#26679;&#26412;&#25928;&#29575;&#24182;&#20943;&#23569;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#20043;&#38388;&#36755;&#20837;&#20998;&#24067;&#30340;&#20559;&#24046;&#12290;SimLM&#21482;&#38656;&#35201;&#23545;&#26410;&#26631;&#35760;&#30340;&#35821;&#26009;&#24211;&#36827;&#34892;&#35775;&#38382;&#65292;&#24182;&#19988;&#22312;&#27809;&#26377;&#26631;&#35760;&#25968;&#25454;&#25110;&#26597;&#35810;&#30340;&#24773;&#20917;&#19979;&#26356;&#20026;&#24191;&#27867;&#36866;&#29992;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#22823;&#35268;&#27169;&#25490;&#24207;&#20219;&#21153;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#24182;&#22312;&#21508;&#31181;&#35774;&#32622;&#19979;&#26174;&#31034;&#20102;&#27604;&#24378;&#22522;&#32447;&#26356;&#22823;&#30340;&#25913;&#36827;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;SimLM&#29978;&#33267;&#22312;&#38656;&#35201;&#26356;&#22810;&#23384;&#20648;&#25104;&#26412;&#30340;&#22810;&#21521;&#37327;&#26041;&#27861;&#65288;&#22914;ColBERTv2&#65289;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#21462;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#27169;&#22411;&#26816;&#26597;&#28857;&#21487;&#22312;https://github.com/microsoft/unilm/tree/mast&#20013;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose SimLM (Similarity matching with Language Model pre-training), a simple yet effective pre-training method for dense passage retrieval. It employs a simple bottleneck architecture that learns to compress the passage information into a dense vector through self-supervised pre-training. We use a replaced language modeling objective, which is inspired by ELECTRA, to improve the sample efficiency and reduce the mismatch of the input distribution between pre-training and fine-tuning. SimLM only requires access to unlabeled corpus, and is more broadly applicable when there are no labeled data or queries. We conduct experiments on several large-scale passage retrieval datasets, and show substantial improvements over strong baselines under various settings. Remarkably, SimLM even outperforms multi-vector approaches such as ColBERTv2 which incurs significantly more storage cost. Our code and model check points are available at https://github.com/microsoft/unilm/tree/mast
&lt;/p&gt;</description></item></channel></rss>