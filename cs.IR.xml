<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21452;&#36890;&#36947;&#22810;&#37325;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;DCMGNN&#65289;&#30340;&#26032;&#22411;&#25512;&#33616;&#26694;&#26550;&#65292;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#29616;&#26377;&#25512;&#33616;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#22810;&#36890;&#36335;&#20851;&#31995;&#34892;&#20026;&#27169;&#24335;&#24314;&#27169;&#21644;&#23545;&#30446;&#26631;&#20851;&#31995;&#24433;&#21709;&#24573;&#30053;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.11624</link><description>&lt;p&gt;
&#21452;&#36890;&#36947;&#22810;&#37325;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Dual-Channel Multiplex Graph Neural Networks for Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11624
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21452;&#36890;&#36947;&#22810;&#37325;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;DCMGNN&#65289;&#30340;&#26032;&#22411;&#25512;&#33616;&#26694;&#26550;&#65292;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#29616;&#26377;&#25512;&#33616;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#22810;&#36890;&#36335;&#20851;&#31995;&#34892;&#20026;&#27169;&#24335;&#24314;&#27169;&#21644;&#23545;&#30446;&#26631;&#20851;&#31995;&#24433;&#21709;&#24573;&#30053;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#30340;&#25512;&#33616;&#31995;&#32479;&#22312;&#20934;&#30830;&#25429;&#25417;&#21453;&#26144;&#20010;&#20154;&#20559;&#22909;&#30340;&#29992;&#25143;&#21644;&#39033;&#30446;&#23646;&#24615;&#26041;&#38754;&#21457;&#25381;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#19968;&#20123;&#29616;&#26377;&#30340;&#25512;&#33616;&#25216;&#26415;&#24050;&#32463;&#24320;&#22987;&#23558;&#37325;&#28857;&#36716;&#21521;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#25512;&#33616;&#22330;&#26223;&#20013;&#23545;&#29992;&#25143;&#21644;&#39033;&#30446;&#20043;&#38388;&#30340;&#21508;&#31181;&#31867;&#22411;&#20132;&#20114;&#20851;&#31995;&#36827;&#34892;&#24314;&#27169;&#65292;&#20363;&#22914;&#22312;&#32447;&#36141;&#29289;&#24179;&#21488;&#19978;&#30340;&#28857;&#20987;&#12289;&#26631;&#35760;&#25910;&#34255;&#21644;&#36141;&#20080;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20173;&#28982;&#38754;&#20020;&#20004;&#20010;&#37325;&#35201;&#30340;&#32570;&#28857;&#65306;(1) &#19981;&#36275;&#30340;&#24314;&#27169;&#21644;&#21033;&#29992;&#29992;&#25143;&#21644;&#39033;&#30446;&#20043;&#38388;&#22810;&#36890;&#36335;&#20851;&#31995;&#24418;&#25104;&#30340;&#21508;&#31181;&#34892;&#20026;&#27169;&#24335;&#23545;&#34920;&#31034;&#23398;&#20064;&#30340;&#24433;&#21709;&#65292;&#20197;&#21450;(2) &#24573;&#30053;&#20102;&#34892;&#20026;&#27169;&#24335;&#20013;&#19981;&#21516;&#20851;&#31995;&#23545;&#25512;&#33616;&#31995;&#32479;&#22330;&#26223;&#20013;&#30446;&#26631;&#20851;&#31995;&#30340;&#24433;&#21709;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25512;&#33616;&#26694;&#26550;&#65292;&#21363;&#21452;&#36890;&#36947;&#22810;&#37325;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;DCMGNN&#65289;&#65292;&#35813;&#26694;&#26550;&#35299;&#20915;&#20102;&#19978;&#36848;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11624v1 Announce Type: cross  Abstract: Efficient recommender systems play a crucial role in accurately capturing user and item attributes that mirror individual preferences. Some existing recommendation techniques have started to shift their focus towards modeling various types of interaction relations between users and items in real-world recommendation scenarios, such as clicks, marking favorites, and purchases on online shopping platforms. Nevertheless, these approaches still grapple with two significant shortcomings: (1) Insufficient modeling and exploitation of the impact of various behavior patterns formed by multiplex relations between users and items on representation learning, and (2) ignoring the effect of different relations in the behavior patterns on the target relation in recommender system scenarios. In this study, we introduce a novel recommendation framework, Dual-Channel Multiplex Graph Neural Network (DCMGNN), which addresses the aforementioned challenges
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;ConvSDG&#26694;&#26550;&#25506;&#32034;&#20102;&#29983;&#25104;&#26356;&#22810;&#24102;&#30456;&#20851;&#26631;&#31614;&#35757;&#32451;&#20250;&#35805;&#20197;&#25552;&#21319;&#20250;&#35805;&#25628;&#32034;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.11335</link><description>&lt;p&gt;
ConvSDG&#65306;&#29992;&#20110;&#20250;&#35805;&#25628;&#32034;&#30340;&#20250;&#35805;&#25968;&#25454;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
ConvSDG: Session Data Generation for Conversational Search
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11335
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;ConvSDG&#26694;&#26550;&#25506;&#32034;&#20102;&#29983;&#25104;&#26356;&#22810;&#24102;&#30456;&#20851;&#26631;&#31614;&#35757;&#32451;&#20250;&#35805;&#20197;&#25552;&#21319;&#20250;&#35805;&#25628;&#32034;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20250;&#35805;&#25628;&#32034;&#36890;&#36807;&#20801;&#35768;&#29992;&#25143;&#19982;&#25628;&#32034;&#24341;&#25806;&#36827;&#34892;&#22810;&#36718;&#20132;&#20114;&#25552;&#20379;&#20102;&#26356;&#20415;&#25463;&#30340;&#25628;&#32034;&#30028;&#38754;&#12290;&#28982;&#32780;&#65292;&#20250;&#35805;&#23494;&#38598;&#26816;&#32034;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21463;&#21040;&#35757;&#32451;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#38480;&#21046;&#65292;&#36825;&#20123;&#25968;&#25454;&#38656;&#35201;&#29992;&#20110;&#24494;&#35843;&#12290;&#22240;&#27492;&#65292;&#29983;&#25104;&#26356;&#22810;&#24102;&#26377;&#30456;&#20851;&#26631;&#31614;&#30340;&#35757;&#32451;&#20250;&#35805;&#21487;&#33021;&#20250;&#25552;&#39640;&#25628;&#32034;&#24615;&#33021;&#12290;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25991;&#26412;&#29983;&#25104;&#19978;&#30340;&#26377;&#30410;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ConvSDG&#65292;&#36825;&#26159;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#25506;&#32034;&#20351;&#29992;LLM&#36827;&#34892;&#20250;&#35805;&#25968;&#25454;&#29983;&#25104;&#26469;&#25552;&#21319;&#20250;&#35805;&#25628;&#32034;&#30340;&#21487;&#34892;&#24615;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20869;&#65292;&#25105;&#20204;&#26681;&#25454;&#30456;&#20851;&#21028;&#26029;&#30340;&#21487;&#29992;&#24615;&#35774;&#35745;&#20102;&#23545;&#35805;/&#20250;&#35805;&#32423;&#21644;&#26597;&#35810;&#32423;&#25968;&#25454;&#29983;&#25104;&#30340;&#26080;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#12290;&#29983;&#25104;&#30340;&#25968;&#25454;&#34987;&#29992;&#20110;&#24494;&#35843;&#20250;&#35805;&#23494;&#38598;&#26816;&#32034;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11335v1 Announce Type: cross  Abstract: Conversational search provides a more convenient interface for users to search by allowing multi-turn interaction with the search engine. However, the effectiveness of the conversational dense retrieval methods is limited by the scarcity of training data required for their fine-tuning. Thus, generating more training conversational sessions with relevant labels could potentially improve search performance. Based on the promising capabilities of large language models (LLMs) on text generation, we propose ConvSDG, a simple yet effective framework to explore the feasibility of boosting conversational search by using LLM for session data generation. Within this framework, we design dialogue/session-level and query-level data generation with unsupervised and semi-supervised learning, according to the availability of relevance judgments. The generated data are used to fine-tune the conversational dense retriever. Extensive experiments on four
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#25968;&#25454;&#22686;&#24378;&#30340;&#30740;&#31350;&#65292;&#35770;&#25991;&#25506;&#35752;&#20102;&#22312;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#20013;&#65292;&#20165;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#26159;&#21542;&#33021;&#21462;&#24471;&#20248;&#36234;&#30340;&#25512;&#33616;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.11136</link><description>&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#26159;&#21542;&#24517;&#35201;&#65311;&#25968;&#25454;&#22686;&#24378;&#19982;&#23545;&#27604;&#23398;&#20064;&#22312;&#39034;&#24207;&#25512;&#33616;&#20013;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Is Contrastive Learning Necessary? A Study of Data Augmentation vs Contrastive Learning in Sequential Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11136
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#25968;&#25454;&#22686;&#24378;&#30340;&#30740;&#31350;&#65292;&#35770;&#25991;&#25506;&#35752;&#20102;&#22312;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#20013;&#65292;&#20165;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#26159;&#21542;&#33021;&#21462;&#24471;&#20248;&#36234;&#30340;&#25512;&#33616;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#65288;SRS&#65289;&#26088;&#22312;&#22522;&#20110;&#29992;&#25143;&#30340;&#21382;&#21490;&#20132;&#20114;&#25968;&#25454;&#39044;&#27979;&#20854;&#26410;&#26469;&#34892;&#20026;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#36234;&#26469;&#36234;&#22810;&#22320;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#65288;CL&#65289;&#26469;&#21033;&#29992;&#26080;&#30417;&#30563;&#20449;&#21495;&#26469;&#32531;&#35299;SRS&#20013;&#30340;&#25968;&#25454;&#31232;&#30095;&#38382;&#39064;&#12290;&#19968;&#33324;&#26469;&#35828;&#65292;&#22522;&#20110;CL&#30340;SRS&#39318;&#20808;&#36890;&#36807;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#22686;&#24191;&#21407;&#22987;&#30340;&#24207;&#21015;&#20132;&#20114;&#25968;&#25454;&#65292;&#24182;&#37319;&#29992;&#23545;&#27604;&#35757;&#32451;&#26041;&#26696;&#26469;&#24378;&#21270;&#26469;&#33258;&#30456;&#21516;&#21407;&#22987;&#20132;&#20114;&#25968;&#25454;&#30340;&#24207;&#21015;&#30340;&#34920;&#31034;&#20026;&#30456;&#20284;&#12290;&#23613;&#31649;CL&#26085;&#30410;&#26222;&#21450;&#65292;&#20316;&#20026;CL&#30340;&#22522;&#26412;&#32452;&#25104;&#37096;&#20998;&#30340;&#25968;&#25454;&#22686;&#24378;&#24182;&#27809;&#26377;&#21463;&#21040;&#36275;&#22815;&#30340;&#20851;&#27880;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#26159;&#21542;&#21487;&#20197;&#20165;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#23454;&#29616;&#21331;&#36234;&#30340;&#25512;&#33616;&#32467;&#26524;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;&#22235;&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#20843;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#65292;&#20197;&#21450;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;CL&#30340;SRS&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11136v1 Announce Type: new  Abstract: Sequential recommender systems (SRS) are designed to predict users' future behaviors based on their historical interaction data. Recent research has increasingly utilized contrastive learning (CL) to leverage unsupervised signals to alleviate the data sparsity issue in SRS. In general, CL-based SRS first augments the raw sequential interaction data by using data augmentation strategies and employs a contrastive training scheme to enforce the representations of those sequences from the same raw interaction data to be similar. Despite the growing popularity of CL, data augmentation, as a basic component of CL, has not received sufficient attention. This raises the question: Is it possible to achieve superior recommendation results solely through data augmentation? To answer this question, we benchmark eight widely used data augmentation strategies, as well as state-of-the-art CL-based SRS methods, on four real-world datasets under both war
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GNN&#30340;&#26694;&#26550;&#65292;&#22312;&#23454;&#20307;&#23545;&#40784;&#20013;&#35299;&#20915;&#20102;&#26080;&#26631;&#31614;&#24748;&#25346;&#26696;&#20363;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#35774;&#35745;&#27880;&#24847;&#26426;&#21046;&#21644;&#27491;&#26679;&#26412;-&#26080;&#26631;&#31614;&#25439;&#22833;&#26469;&#23454;&#29616;&#26356;&#22909;&#30340;&#23545;&#40784;&#24615;&#33021;</title><link>https://arxiv.org/abs/2403.10978</link><description>&lt;p&gt;
&#20855;&#26377;&#26080;&#26631;&#31614;&#24748;&#25346;&#26696;&#20363;&#30340;&#23454;&#20307;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Entity Alignment with Unlabeled Dangling Cases
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10978
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GNN&#30340;&#26694;&#26550;&#65292;&#22312;&#23454;&#20307;&#23545;&#40784;&#20013;&#35299;&#20915;&#20102;&#26080;&#26631;&#31614;&#24748;&#25346;&#26696;&#20363;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#35774;&#35745;&#27880;&#24847;&#26426;&#21046;&#21644;&#27491;&#26679;&#26412;-&#26080;&#26631;&#31614;&#25439;&#22833;&#26469;&#23454;&#29616;&#26356;&#22909;&#30340;&#23545;&#40784;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20855;&#26377;&#26080;&#26631;&#31614;&#24748;&#25346;&#26696;&#20363;&#30340;&#23454;&#20307;&#23545;&#40784;&#38382;&#39064;&#65292;&#36825;&#24847;&#21619;&#30528;&#28304;&#22270;&#25110;&#30446;&#26631;&#22270;&#20013;&#26377;&#19968;&#20123;&#23454;&#20307;&#22312;&#21478;&#19968;&#26041;&#20013;&#27809;&#26377;&#23545;&#24212;&#23454;&#20307;&#65292;&#24182;&#19988;&#36825;&#20123;&#23454;&#20307;&#20445;&#25345;&#26410;&#26631;&#35760;&#29366;&#24577;&#12290;&#35813;&#38382;&#39064;&#20986;&#29616;&#22312;&#28304;&#22270;&#21644;&#30446;&#26631;&#22270;&#30340;&#35268;&#27169;&#19981;&#21516;&#65292;&#24182;&#19988;&#26631;&#35760;&#21487;&#21305;&#37197;&#23454;&#20307;&#30340;&#25104;&#26412;&#36828;&#20302;&#20110;&#24748;&#25346;&#23454;&#20307;&#30340;&#24773;&#20917;&#19979;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;GNN&#30340;&#24748;&#25346;&#26816;&#27979;&#21644;&#23454;&#20307;&#23545;&#40784;&#26694;&#26550;&#12290;&#34429;&#28982;&#36825;&#20004;&#20010;&#20219;&#21153;&#20849;&#20139;&#30456;&#21516;&#30340;GNN&#65292;&#24182;&#19988;&#19968;&#36215;&#35757;&#32451;&#65292;&#20294;&#26816;&#27979;&#21040;&#30340;&#24748;&#25346;&#23454;&#20307;&#22312;&#23545;&#40784;&#20013;&#34987;&#31227;&#38500;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#29305;&#28857;&#26159;&#20855;&#26377;&#29992;&#20110;&#36873;&#25321;&#24615;&#37051;&#22495;&#32858;&#21512;&#30340;&#35774;&#35745;&#23454;&#20307;&#21644;&#20851;&#31995;&#27880;&#24847;&#26426;&#21046;&#65292;&#20197;&#21450;&#29992;&#20110;&#23545;&#24748;&#25346;&#23454;&#20307;&#36827;&#34892;&#26080;&#20559;&#20272;&#35745;&#30340;&#27491;&#26679;&#26412;-&#26080;&#26631;&#31614;&#23398;&#20064;&#25439;&#22833;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#35774;&#35745;&#30340;&#27599;&#20010;&#32452;&#20214;&#37117;&#23545;&#25972;&#20307;&#23545;&#40784;&#24615;&#33021;&#26377;&#36129;&#29486;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10978v1 Announce Type: new  Abstract: We investigate the entity alignment problem with unlabeled dangling cases, meaning that there are entities in the source or target graph having no counterparts in the other, and those entities remain unlabeled. The problem arises when the source and target graphs are of different scales, and it is much cheaper to label the matchable pairs than the dangling entities. To solve the issue, we propose a novel GNN-based dangling detection and entity alignment framework. While the two tasks share the same GNN and are trained together, the detected dangling entities are removed in the alignment. Our framework is featured by a designed entity and relation attention mechanism for selective neighborhood aggregation in representation learning, as well as a positive-unlabeled learning loss for an unbiased estimation of dangling entities. Experimental results have shown that each component of our design contributes to the overall alignment performance
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#36890;&#36807;&#22810;&#27491;&#23545;&#27604;&#23398;&#20064;&#26469;&#25552;&#39640;&#23494;&#38598;&#26816;&#32034;&#22120;&#23545;&#38169;&#21035;&#23383;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#34920;&#26126;&#25152;&#26377;&#21487;&#29992;&#30340;&#27491;&#26679;&#26412;&#21487;&#20197;&#21516;&#26102;&#20351;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.10939</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#27491;&#23545;&#27604;&#23398;&#20064;&#25552;&#39640;&#23494;&#38598;&#26816;&#32034;&#22120;&#23545;&#38169;&#21035;&#23383;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving the Robustness of Dense Retrievers Against Typos via Multi-Positive Contrastive Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10939
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#36890;&#36807;&#22810;&#27491;&#23545;&#27604;&#23398;&#20064;&#26469;&#25552;&#39640;&#23494;&#38598;&#26816;&#32034;&#22120;&#23545;&#38169;&#21035;&#23383;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#34920;&#26126;&#25152;&#26377;&#21487;&#29992;&#30340;&#27491;&#26679;&#26412;&#21487;&#20197;&#21516;&#26102;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23494;&#38598;&#26816;&#32034;&#24050;&#25104;&#20026;&#27573;&#33853;&#26816;&#32034;&#20013;&#30340;&#26032;&#33539;&#24335;&#12290;&#23613;&#31649;&#22312;&#26080;&#38169;&#21035;&#23383;&#26597;&#35810;&#19978;&#25928;&#26524;&#26174;&#33879;&#65292;&#20294;&#22312;&#22788;&#29702;&#21253;&#21547;&#38169;&#21035;&#23383;&#30340;&#26597;&#35810;&#26102;&#21364;&#19981;&#22815;&#40065;&#26834;&#12290;&#24403;&#21069;&#38024;&#23545;&#25552;&#39640;&#23494;&#38598;&#26816;&#32034;&#22120;&#23545;&#38169;&#21035;&#23383;&#40065;&#26834;&#24615;&#30340;&#30740;&#31350;&#32467;&#21512;&#20102;(i) &#22312;&#35757;&#32451;&#26399;&#38388;&#33719;&#24471;&#38169;&#21035;&#23383;&#26597;&#35810;&#30340;&#25968;&#25454;&#22686;&#24378;&#21644;(ii) &#26088;&#22312;&#23545;&#40784;&#21407;&#22987;&#26080;&#38169;&#21035;&#23383;&#26597;&#35810;&#19982;&#38169;&#21035;&#23383;&#21464;&#20307;&#30340;&#38468;&#21152;&#40065;&#26834;&#24615;&#23376;&#20219;&#21153;&#12290;&#23613;&#31649;&#27599;&#20010;&#26597;&#35810;&#30340;&#22810;&#20010;&#38169;&#21035;&#23383;&#21464;&#20307;&#21487;&#29992;&#20316;&#27491;&#26679;&#26412;&#65292;&#20294;&#26576;&#20123;&#26041;&#27861;&#20551;&#35774;&#21333;&#19968;&#27491;&#26679;&#26412;&#21644;&#19968;&#32452;&#36127;&#26679;&#26412;&#20197;&#21450;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#22788;&#29702;&#25913;&#36827;&#40065;&#26834;&#24615;&#30340;&#23376;&#20219;&#21153;&#65307;&#22240;&#27492;&#65292;&#26410;&#20805;&#20998;&#21033;&#29992;&#22810;&#20010;&#27491;&#26679;&#26412;&#65288;&#38169;&#21035;&#23383;&#26597;&#35810;&#65289;&#12290;&#19982;&#20043;&#30456;&#21453;&#65292;&#26412;&#30740;&#31350;&#35748;&#20026;&#25152;&#26377;&#21487;&#29992;&#30340;&#27491;&#26679;&#26412;&#21487;&#20197;&#21516;&#26102;&#20351;&#29992;&#65292;&#24182;&#37319;&#29992;&#25903;&#25345;&#22810;&#27491;&#23545;&#27604;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10939v1 Announce Type: new  Abstract: Dense retrieval has become the new paradigm in passage retrieval. Despite its effectiveness on typo-free queries, it is not robust when dealing with queries that contain typos. Current works on improving the typo-robustness of dense retrievers combine (i) data augmentation to obtain the typoed queries during training time with (ii) additional robustifying subtasks that aim to align the original, typo-free queries with their typoed variants. Even though multiple typoed variants are available as positive samples per query, some methods assume a single positive sample and a set of negative ones per anchor and tackle the robustifying subtask with contrastive learning; therefore, making insufficient use of the multiple positives (typoed queries). In contrast, in this work, we argue that all available positives can be used at the same time and employ contrastive learning that supports multiple positives (multi-positive). Experimental results o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25253;&#21578;&#20102;&#19968;&#31181;&#22522;&#20110;&#23849;&#28291;&#25253;&#21578;&#25366;&#25496;&#30340;Bug&#23450;&#20301;&#26041;&#27861;&#65292;&#25506;&#35752;&#20102;&#35813;&#26041;&#27861;&#22312;&#36719;&#20214;&#20844;&#21496;&#26085;&#24120;&#29983;&#27963;&#20013;&#30340;&#20351;&#29992;&#65292;&#35299;&#37322;&#20102;&#24320;&#21457;&#32773;&#23545;&#36825;&#19968;&#26041;&#27861;&#30340;&#30475;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.10753</link><description>&lt;p&gt;
&#22522;&#20110;&#23849;&#28291;&#25253;&#21578;&#25366;&#25496;&#30340;Bug&#23450;&#20301;&#24433;&#21709;&#65306;&#24320;&#21457;&#32773;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
The Impact Of Bug Localization Based on Crash Report Mining: A Developers' Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10753
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25253;&#21578;&#20102;&#19968;&#31181;&#22522;&#20110;&#23849;&#28291;&#25253;&#21578;&#25366;&#25496;&#30340;Bug&#23450;&#20301;&#26041;&#27861;&#65292;&#25506;&#35752;&#20102;&#35813;&#26041;&#27861;&#22312;&#36719;&#20214;&#20844;&#21496;&#26085;&#24120;&#29983;&#27963;&#20013;&#30340;&#20351;&#29992;&#65292;&#35299;&#37322;&#20102;&#24320;&#21457;&#32773;&#23545;&#36825;&#19968;&#26041;&#27861;&#30340;&#30475;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#20154;&#21592;&#36890;&#24120;&#20351;&#29992;&#23849;&#28291;&#25253;&#21578;&#26469;&#29702;&#35299;bug&#30340;&#26681;&#26412;&#21407;&#22240;&#12290;&#28982;&#32780;&#65292;&#20174;&#36825;&#20123;&#20449;&#24687;&#20013;&#23450;&#20301;&#26377;&#38169;&#35823;&#30340;&#28304;&#20195;&#30721;&#29255;&#27573;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#29305;&#21035;&#26159;&#24403;&#26085;&#24535;&#25968;&#25454;&#24211;&#21253;&#21547;&#35768;&#22810;&#23849;&#28291;&#25253;&#21578;&#26102;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#20998;&#32452;&#23849;&#28291;&#25253;&#21578;&#25968;&#25454;&#21644;&#20351;&#29992;&#22534;&#26632;&#36319;&#36394;&#20449;&#24687;&#26469;&#23450;&#20301;bug&#30340;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#20027;&#35201;&#36890;&#36807;&#23558;&#20505;&#36873;buggy&#28304;&#20195;&#30721;&#29255;&#27573;&#19982;bug&#20462;&#22797;&#25552;&#20132;&#20013;&#23454;&#38469;&#26356;&#25913;&#30340;&#20195;&#30721;&#36827;&#34892;&#27604;&#36739;&#26469;&#35780;&#20272;&#65292;&#36825;&#26159;&#22312;&#22238;&#39038;&#24615;&#20179;&#24211;&#25366;&#25496;&#30740;&#31350;&#30340;&#32972;&#26223;&#19979;&#21457;&#29983;&#30340;&#12290;&#22240;&#27492;&#65292;&#29616;&#26377;&#25991;&#29486;&#20173;&#32570;&#20047;&#35752;&#35770;&#36825;&#20123;&#26041;&#27861;&#22312;&#36719;&#20214;&#20844;&#21496;&#26085;&#24120;&#29983;&#27963;&#20013;&#30340;&#20351;&#29992;&#65292;&#36825;&#21487;&#20197;&#35299;&#37322;&#24320;&#21457;&#32773;&#23545;&#36825;&#20123;&#26041;&#27861;&#30340;&#30475;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25253;&#21578;&#20102;&#25105;&#20204;&#22312;&#27599;&#21608;&#22522;&#22522;&#30784;&#19978;&#20351;&#29992;&#19968;&#31181;&#20998;&#32452;&#23849;&#28291;&#25253;&#21578;&#24182;&#25214;&#21040;&#38169;&#35823;&#20195;&#30721;&#30340;&#26041;&#27861;&#30340;&#32463;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10753v1 Announce Type: cross  Abstract: Developers often use crash reports to understand the root cause of bugs. However, locating the buggy source code snippet from such information is a challenging task, mainly when the log database contains many crash reports. To mitigate this issue, recent research has proposed and evaluated approaches for grouping crash report data and using stack trace information to locate bugs. The effectiveness of such approaches has been evaluated by mainly comparing the candidate buggy code snippets with the actual changed code in bug-fix commits -- which happens in the context of retrospective repository mining studies. Therefore, the existing literature still lacks discussing the use of such approaches in the daily life of a software company, which could explain the developers' perceptions on the use of these approaches. In this paper, we report our experience of using an approach for grouping crash reports and finding buggy code on a weekly bas
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#24314;&#31435;&#19968;&#20010;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#20010;&#24615;&#21270;&#31995;&#32479;(UniMP)&#65292;&#26377;&#25928;&#21033;&#29992;&#22810;&#27169;&#24577;&#25968;&#25454;&#21516;&#26102;&#28040;&#38500;&#19982;&#20219;&#21153;&#21644;&#27169;&#24577;&#29305;&#23450;&#23450;&#21046;&#30456;&#20851;&#30340;&#22797;&#26434;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.10667</link><description>&lt;p&gt;
&#36890;&#21521;&#32479;&#19968;&#22810;&#27169;&#24335;&#20010;&#24615;&#21270;&#65306;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#29983;&#25104;&#25512;&#33616;&#21644;&#26356;&#22810;&#39046;&#22495;
&lt;/p&gt;
&lt;p&gt;
Towards Unified Multi-Modal Personalization: Large Vision-Language Models for Generative Recommendation and Beyond
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10667
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#24314;&#31435;&#19968;&#20010;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#20010;&#24615;&#21270;&#31995;&#32479;(UniMP)&#65292;&#26377;&#25928;&#21033;&#29992;&#22810;&#27169;&#24577;&#25968;&#25454;&#21516;&#26102;&#28040;&#38500;&#19982;&#20219;&#21153;&#21644;&#27169;&#24577;&#29305;&#23450;&#23450;&#21046;&#30456;&#20851;&#30340;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#19968;&#20010;&#33021;&#22815;&#26377;&#25928;&#21033;&#29992;&#24322;&#26500;&#36164;&#28304;&#24182;&#28385;&#36275;&#21508;&#31181;&#20010;&#24615;&#21270;&#38656;&#27714;&#30340;&#36890;&#29992;&#27169;&#22411;&#19968;&#30452;&#26159;&#31038;&#21306;&#28212;&#26395;&#30340;&#30446;&#26631;&#12290;&#25105;&#20204;&#26085;&#24120;&#30340;&#36873;&#25321;&#65292;&#23588;&#20854;&#26159;&#22312;&#26102;&#23578;&#21644;&#38646;&#21806;&#31561;&#39046;&#22495;&#65292;&#24456;&#22823;&#31243;&#24230;&#19978;&#21463;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#24433;&#21709;&#65292;&#27604;&#22914;&#22270;&#29255;&#21644;&#25991;&#26412;&#25551;&#36848;&#12290;&#36825;&#20123;&#27169;&#24577;&#19981;&#20165;&#25552;&#20379;&#30452;&#35266;&#30340;&#25351;&#23548;&#65292;&#36824;&#36814;&#21512;&#20010;&#24615;&#21270;&#29992;&#25143;&#20559;&#22909;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#20027;&#27969;&#30340;&#20010;&#24615;&#21270;&#26041;&#27861;&#20027;&#35201;&#32858;&#28966;&#20110;&#22522;&#20110;ID&#25110;&#25991;&#26412;&#30340;&#25512;&#33616;&#38382;&#39064;&#65292;&#26410;&#33021;&#29702;&#35299;&#28085;&#30422;&#21508;&#31181;&#20219;&#21153;&#25110;&#27169;&#24577;&#30340;&#20449;&#24687;&#12290;&#26412;&#25991;&#30340;&#30446;&#26631;&#26159;&#24314;&#31435;&#19968;&#20010;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#20010;&#24615;&#21270;&#31995;&#32479;(UniMP)&#65292;&#33021;&#22815;&#26377;&#25928;&#21033;&#29992;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#21516;&#26102;&#28040;&#38500;&#19982;&#20219;&#21153;&#21644;&#27169;&#24577;&#29305;&#23450;&#23450;&#21046;&#30456;&#20851;&#30340;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#35748;&#20026;&#22522;&#30784;&#29983;&#25104;&#24314;&#27169;&#30340;&#36827;&#23637;&#25552;&#20379;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10667v1 Announce Type: cross  Abstract: Developing a universal model that can effectively harness heterogeneous resources and respond to a wide range of personalized needs has been a longstanding community aspiration. Our daily choices, especially in domains like fashion and retail, are substantially shaped by multi-modal data, such as pictures and textual descriptions. These modalities not only offer intuitive guidance but also cater to personalized user preferences. However, the predominant personalization approaches mainly focus on the ID or text-based recommendation problem, failing to comprehend the information spanning various tasks or modalities. In this paper, our goal is to establish a Unified paradigm for Multi-modal Personalization systems (UniMP), which effectively leverages multi-modal data while eliminating the complexities associated with task- and modality-specific customization. We argue that the advancements in foundational generative modeling have provided
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LIST&#30340;&#26032;&#25216;&#26415;&#65292;&#36890;&#36807;&#23398;&#20064;&#20026;&#22522;&#20110;&#23884;&#20837;&#30340;&#31354;&#38388;&#20851;&#38190;&#35789;&#26597;&#35810;&#24314;&#31435;&#31354;&#38388;&#25991;&#26412;&#25968;&#25454;&#32034;&#24341;&#65292;&#20197;&#21152;&#36895;top-k&#25628;&#32034;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2403.07331</link><description>&lt;p&gt;
LIST: &#23398;&#20064;&#20026;&#22522;&#20110;&#23884;&#20837;&#30340;&#31354;&#38388;&#20851;&#38190;&#35789;&#26597;&#35810;&#24314;&#31435;&#31354;&#38388;&#25991;&#26412;&#25968;&#25454;&#32034;&#24341;
&lt;/p&gt;
&lt;p&gt;
LIST: Learning to Index Spatio-Textual Data for Embedding based Spatial Keyword Queries
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07331
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LIST&#30340;&#26032;&#25216;&#26415;&#65292;&#36890;&#36807;&#23398;&#20064;&#20026;&#22522;&#20110;&#23884;&#20837;&#30340;&#31354;&#38388;&#20851;&#38190;&#35789;&#26597;&#35810;&#24314;&#31435;&#31354;&#38388;&#25991;&#26412;&#25968;&#25454;&#32034;&#24341;&#65292;&#20197;&#21152;&#36895;top-k&#25628;&#32034;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31354;&#38388;&#25991;&#26412;&#25968;&#25454;&#30340;&#26222;&#21450;&#65292;&#8220;Top-k KNN&#31354;&#38388;&#20851;&#38190;&#35789;&#26597;&#35810;&#65288;TkQs&#65289;&#8221;&#24050;&#32463;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#21457;&#29616;&#65292;&#23427;&#22522;&#20110;&#19968;&#20010;&#35780;&#20215;&#31354;&#38388;&#21644;&#25991;&#26412;&#30456;&#20851;&#24615;&#30340;&#25490;&#21517;&#20989;&#25968;&#36820;&#22238;&#19968;&#20010;&#23545;&#35937;&#21015;&#34920;&#12290;&#29616;&#26377;&#30340;&#29992;&#20110;TkQs&#30340;geo-textual&#32034;&#24341;&#20351;&#29992;&#20256;&#32479;&#30340;&#26816;&#32034;&#27169;&#22411;&#65288;&#22914;BM25&#65289;&#26469;&#35745;&#31639;&#25991;&#26412;&#30456;&#20851;&#24615;&#65292;&#24182;&#36890;&#24120;&#21033;&#29992;&#31616;&#21333;&#30340;&#32447;&#24615;&#20989;&#25968;&#26469;&#35745;&#31639;&#31354;&#38388;&#30456;&#20851;&#24615;&#65292;&#20294;&#20854;&#25928;&#26524;&#26377;&#38480;&#12290;&#20026;&#20102;&#25552;&#39640;&#25928;&#26524;&#65292;&#26368;&#36817;&#25552;&#20986;&#20102;&#20960;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20294;&#23427;&#20204;&#23384;&#22312;&#20005;&#37325;&#30340;&#25928;&#29575;&#38382;&#39064;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#30446;&#21069;&#27809;&#26377;&#20026;&#21152;&#36895;&#36825;&#20123;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;top-k&#25628;&#32034;&#36807;&#31243;&#19987;&#38376;&#35774;&#35745;&#30340;&#26377;&#25928;&#32034;&#24341;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#25216;&#26415;&#65292;&#36890;&#36807;&#23398;&#20064;&#20026;&#22238;&#31572;&#22522;&#20110;&#23884;&#20837;&#30340;&#31354;&#38388;&#20851;&#38190;&#35789;&#26597;&#35810;&#65288;&#31216;&#20026;LIST&#65289;&#24314;&#31435;&#31354;&#38388;&#25991;&#26412;&#25968;&#25454;&#32034;&#24341;&#12290;LIST&#20855;&#26377;&#20004;&#20010;&#26032;&#39062;&#32452;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07331v1 Announce Type: new  Abstract: With the proliferation of spatio-textual data, Top-k KNN spatial keyword queries (TkQs), which return a list of objects based on a ranking function that evaluates both spatial and textual relevance, have found many real-life applications. Existing geo-textual indexes for TkQs use traditional retrieval models like BM25 to compute text relevance and usually exploit a simple linear function to compute spatial relevance, but its effectiveness is limited. To improve effectiveness, several deep learning models have recently been proposed, but they suffer severe efficiency issues. To the best of our knowledge, there are no efficient indexes specifically designed to accelerate the top-k search process for these deep learning models.   To tackle these issues, we propose a novel technique, which Learns to Index the Spatio-Textual data for answering embedding based spatial keyword queries (called LIST). LIST is featured with two novel components. F
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Meta-Split&#32593;&#32476;&#65288;MSN&#65289;&#26469;&#35299;&#20915;&#28040;&#36153;&#32773;&#20043;&#38388;&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#20013;&#38480;&#37327;&#24211;&#23384;&#20135;&#21697;&#25512;&#33616;&#20013;&#30340;&#29420;&#29305;&#25361;&#25112;&#65292;&#36890;&#36807;&#20998;&#21106;&#29992;&#25143;&#21382;&#21490;&#24207;&#21015;&#26469;&#26377;&#25928;&#21033;&#29992;&#29992;&#25143;&#21382;&#21490;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2403.06747</link><description>&lt;p&gt;
MetaSplit: &#29992;&#20110;&#38480;&#37327;&#20135;&#21697;&#25512;&#33616;&#30340;Meta-Split&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
MetaSplit: Meta-Split Network for Limited-Stock Product Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06747
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Meta-Split&#32593;&#32476;&#65288;MSN&#65289;&#26469;&#35299;&#20915;&#28040;&#36153;&#32773;&#20043;&#38388;&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#20013;&#38480;&#37327;&#24211;&#23384;&#20135;&#21697;&#25512;&#33616;&#20013;&#30340;&#29420;&#29305;&#25361;&#25112;&#65292;&#36890;&#36807;&#20998;&#21106;&#29992;&#25143;&#21382;&#21490;&#24207;&#21015;&#26469;&#26377;&#25928;&#21033;&#29992;&#29992;&#25143;&#21382;&#21490;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#23545;&#20110;&#38754;&#21521;&#28040;&#36153;&#32773;&#30340;&#30005;&#23376;&#21830;&#21153;&#31995;&#32479;&#65292;&#28040;&#36153;&#32773;&#20043;&#38388;&#30340;&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#36890;&#24120;&#20250;&#36935;&#21040;&#38480;&#37327;&#24211;&#23384;&#38382;&#39064;&#65292;&#21363;&#20135;&#21697;&#22312;C2C&#31995;&#32479;&#20013;&#21482;&#33021;&#38144;&#21806;&#19968;&#27425;&#12290;&#36825;&#20026;&#28857;&#20987;&#29575;&#65288;CTR&#65289;&#39044;&#27979;&#24102;&#26469;&#20102;&#20960;&#20010;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#37492;&#20110;&#27599;&#20010;&#20135;&#21697;&#65288;&#21363;&#21830;&#21697;&#65289;&#30340;&#26377;&#38480;&#29992;&#25143;&#20132;&#20114;&#65292;CTR&#27169;&#22411;&#20013;&#23545;&#24212;&#30340;&#21830;&#21697;&#23884;&#20837;&#21487;&#33021;&#19981;&#23481;&#26131;&#25910;&#25947;&#12290;&#36825;&#20351;&#24471;&#20256;&#32479;&#22522;&#20110;&#24207;&#21015;&#24314;&#27169;&#30340;&#26041;&#27861;&#26080;&#27861;&#26377;&#25928;&#21033;&#29992;&#29992;&#25143;&#21382;&#21490;&#20449;&#24687;&#65292;&#22240;&#20026;&#21382;&#21490;&#29992;&#25143;&#34892;&#20026;&#21253;&#21547;&#20102;&#19981;&#21516;&#24211;&#23384;&#37327;&#30340;&#21830;&#21697;&#28151;&#21512;&#12290;&#29305;&#21035;&#26159;&#65292;&#24207;&#21015;&#27169;&#22411;&#20013;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#20542;&#21521;&#20110;&#23558;&#26356;&#22810;&#32047;&#31215;&#29992;&#25143;&#20132;&#20114;&#30340;&#20135;&#21697;&#20998;&#37197;&#26356;&#39640;&#30340;&#20998;&#25968;&#65292;&#23548;&#33268;&#38480;&#37327;&#20135;&#21697;&#34987;&#24573;&#35270;&#19988;&#23545;&#26368;&#32456;&#36755;&#20986;&#30340;&#36129;&#29486;&#36739;&#23569;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Meta-Split&#32593;&#32476;&#65288;MSN&#65289;&#26469;&#20998;&#21106;&#29992;&#25143;&#21382;&#21490;&#24207;&#21015;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06747v1 Announce Type: new  Abstract: Compared to business-to-consumer (B2C) e-commerce systems, consumer-to-consumer (C2C) e-commerce platforms usually encounter the limited-stock problem, that is, a product can only be sold one time in a C2C system. This poses several unique challenges for click-through rate (CTR) prediction. Due to limited user interactions for each product (i.e. item), the corresponding item embedding in the CTR model may not easily converge. This makes the conventional sequence modeling based approaches cannot effectively utilize user history information since historical user behaviors contain a mixture of items with different volume of stocks. Particularly, the attention mechanism in a sequence model tends to assign higher score to products with more accumulated user interactions, making limited-stock products being ignored and contribute less to the final output. To this end, we propose the Meta-Split Network (MSN) to split user history sequence regar
&lt;/p&gt;</description></item><item><title>Ducho 2.0&#25512;&#20986;&#65292;&#25552;&#20379;&#26356;&#20010;&#24615;&#21270;&#30340;&#29992;&#25143;&#20307;&#39564;&#21644;&#25903;&#25345;&#22810;&#27169;&#24577;&#22823;&#22411;&#27169;&#22411;&#25552;&#21462;&#21644;&#22788;&#29702;&#29305;&#24449;&#65292;&#21487;&#29992;&#20110;&#22810;&#27169;&#24335;&#25512;&#33616;&#65292;&#21516;&#26102;&#20248;&#21270;&#25968;&#25454;&#21152;&#36733;&#21644;&#23384;&#20648;&#12290;</title><link>https://arxiv.org/abs/2403.04503</link><description>&lt;p&gt;
Ducho 2.0&#65306;&#38754;&#21521;&#22810;&#27169;&#24335;&#25512;&#33616;&#30340;&#26356;&#20026;&#26102;&#23578;&#30340;&#29305;&#24449;&#25552;&#21462;&#21644;&#22788;&#29702;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Ducho 2.0: Towards a More Up-to-Date Feature Extraction and Processing Framework for Multimodal Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04503
&lt;/p&gt;
&lt;p&gt;
Ducho 2.0&#25512;&#20986;&#65292;&#25552;&#20379;&#26356;&#20010;&#24615;&#21270;&#30340;&#29992;&#25143;&#20307;&#39564;&#21644;&#25903;&#25345;&#22810;&#27169;&#24577;&#22823;&#22411;&#27169;&#22411;&#25552;&#21462;&#21644;&#22788;&#29702;&#29305;&#24449;&#65292;&#21487;&#29992;&#20110;&#22810;&#27169;&#24335;&#25512;&#33616;&#65292;&#21516;&#26102;&#20248;&#21270;&#25968;&#25454;&#21152;&#36733;&#21644;&#23384;&#20648;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Ducho 2.0&#65292;&#25105;&#20204;&#26694;&#26550;&#30340;&#26368;&#26032;&#31283;&#23450;&#29256;&#26412;&#12290;&#19982;Ducho&#19981;&#21516;&#65292;Ducho 2.0&#25552;&#20379;&#20102;&#26356;&#20010;&#24615;&#21270;&#30340;&#29992;&#25143;&#20307;&#39564;&#65292;&#21487;&#20197;&#23450;&#20041;&#21644;&#23548;&#20837;&#22312;&#29305;&#23450;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24494;&#35843;&#30340;&#33258;&#23450;&#20041;&#25552;&#21462;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#26032;&#29256;&#26412;&#33021;&#22815;&#36890;&#36807;&#22810;&#27169;&#24577;&#35774;&#35745;&#30340;&#22823;&#22411;&#27169;&#22411;&#25552;&#21462;&#21644;&#22788;&#29702;&#29305;&#24449;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25152;&#26377;&#36825;&#20123;&#26032;&#21151;&#33021;&#37117;&#21463;&#21040;&#20102;&#23545;&#26412;&#22320;&#23384;&#20648;&#22120;&#36827;&#34892;&#20102;&#20248;&#21270;&#30340;&#25968;&#25454;&#21152;&#36733;&#21644;&#23384;&#20648;&#30340;&#25903;&#25345;&#12290;&#20026;&#20102;&#23637;&#31034;Ducho 2.0&#30340;&#21151;&#33021;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#23436;&#25972;&#30340;&#22810;&#27169;&#24335;&#25512;&#33616;&#27969;&#27700;&#32447;&#65292;&#20174;&#25552;&#21462;/&#22788;&#29702;&#21040;&#26368;&#32456;&#25512;&#33616;&#12290;&#25105;&#20204;&#30340;&#24819;&#27861;&#26159;&#20026;&#20174;&#19994;&#32773;&#21644;&#32463;&#39564;&#20016;&#23500;&#30340;&#23398;&#32773;&#25552;&#20379;&#19968;&#20010;&#21363;&#29992;&#24037;&#20855;&#65292;&#21487;&#20197;&#22312;&#20219;&#20309;&#22810;&#27169;&#24335;&#25512;&#33616;&#26694;&#26550;&#20043;&#19978;&#36816;&#34892;&#24191;&#27867;&#30340;&#22522;&#20934;&#20998;&#26512;&#12290;&#25152;&#26377;&#26448;&#26009;&#37117;&#21487;&#20197;&#22312;&#20197;&#19979;&#32593;&#22336;&#33719;&#24471;&#65306;\url{https://github.com/sisinflab/Ducho}&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04503v1 Announce Type: new  Abstract: In this work, we introduce Ducho 2.0, the latest stable version of our framework. Differently from Ducho, Ducho 2.0 offers a more personalized user experience with the definition and import of custom extraction models fine-tuned on specific tasks and datasets. Moreover, the new version is capable of extracting and processing features through multimodal-by-design large models. Notably, all these new features are supported by optimized data loading and storing to the local memory. To showcase the capabilities of Ducho 2.0, we demonstrate a complete multimodal recommendation pipeline, from the extraction/processing to the final recommendation. The idea is to provide practitioners and experienced scholars with a ready-to-use tool that, put on top of any multimodal recommendation framework, may permit them to run extensive benchmarking analyses. All materials are accessible at: \url{https://github.com/sisinflab/Ducho}.
&lt;/p&gt;</description></item><item><title>Cobweb&#26159;&#19968;&#31181;&#31867;&#20284;&#20154;&#31867;&#31867;&#21035;&#23398;&#20064;&#31995;&#32479;&#65292;&#37319;&#29992;&#31867;&#21035;&#25928;&#29992;&#24230;&#37327;&#26500;&#24314;&#20998;&#23618;&#32452;&#32455;&#30340;&#31867;&#20284;&#26641;&#29366;&#32467;&#26500;&#65292;&#33021;&#22815;&#25429;&#25417;&#24515;&#29702;&#25928;&#24212;&#24182;&#22312;&#21333;&#19968;&#27169;&#22411;&#20013;&#23637;&#29616;&#20986;&#23454;&#20363;&#21644;&#21407;&#22411;&#23398;&#20064;&#30340;&#28789;&#27963;&#24615;&#65292;&#20026;&#23558;&#26469;&#30740;&#31350;&#20154;&#31867;&#31867;&#21035;&#23398;&#20064;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;</title><link>https://arxiv.org/abs/2403.03835</link><description>&lt;p&gt;
Cobweb&#65306;&#19968;&#31181;&#22686;&#37327;&#21644;&#20998;&#23618;&#24335;&#30340;&#20154;&#31867;&#31867;&#21035;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Cobweb: An Incremental and Hierarchical Model of Human-Like Category Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03835
&lt;/p&gt;
&lt;p&gt;
Cobweb&#26159;&#19968;&#31181;&#31867;&#20284;&#20154;&#31867;&#31867;&#21035;&#23398;&#20064;&#31995;&#32479;&#65292;&#37319;&#29992;&#31867;&#21035;&#25928;&#29992;&#24230;&#37327;&#26500;&#24314;&#20998;&#23618;&#32452;&#32455;&#30340;&#31867;&#20284;&#26641;&#29366;&#32467;&#26500;&#65292;&#33021;&#22815;&#25429;&#25417;&#24515;&#29702;&#25928;&#24212;&#24182;&#22312;&#21333;&#19968;&#27169;&#22411;&#20013;&#23637;&#29616;&#20986;&#23454;&#20363;&#21644;&#21407;&#22411;&#23398;&#20064;&#30340;&#28789;&#27963;&#24615;&#65292;&#20026;&#23558;&#26469;&#30740;&#31350;&#20154;&#31867;&#31867;&#21035;&#23398;&#20064;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Cobweb&#26159;&#19968;&#31181;&#31867;&#20284;&#20154;&#31867;&#30340;&#31867;&#21035;&#23398;&#20064;&#31995;&#32479;&#65292;&#19982;&#20854;&#20182;&#22686;&#37327;&#20998;&#31867;&#27169;&#22411;&#19981;&#21516;&#30340;&#26159;&#65292;&#23427;&#21033;&#29992;&#31867;&#21035;&#25928;&#29992;&#24230;&#37327;&#26500;&#24314;&#20998;&#23618;&#32452;&#32455;&#30340;&#31867;&#20284;&#26641;&#29366;&#32467;&#26500;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;Cobweb&#33021;&#22815;&#25429;&#25417;&#24515;&#29702;&#25928;&#24212;&#65292;&#22914;&#22522;&#26412;&#27700;&#24179;&#12289;&#20856;&#22411;&#24615;&#21644;&#25159;&#24418;&#25928;&#24212;&#12290;&#28982;&#32780;&#65292;&#23545;Cobweb&#20316;&#20026;&#20154;&#31867;&#20998;&#31867;&#27169;&#22411;&#30340;&#26356;&#24191;&#27867;&#35780;&#20272;&#20173;&#28982;&#32570;&#20047;&#12290;&#26412;&#30740;&#31350;&#22635;&#34917;&#20102;&#36825;&#19968;&#31354;&#30333;&#12290;&#23427;&#30830;&#23450;&#20102;Cobweb&#19982;&#32463;&#20856;&#30340;&#20154;&#31867;&#31867;&#21035;&#23398;&#20064;&#25928;&#24212;&#30340;&#19968;&#33268;&#24615;&#12290;&#36824;&#25506;&#35752;&#20102;Cobweb&#23637;&#29616;&#20986;&#22312;&#21333;&#19968;&#27169;&#22411;&#20013;&#26082;&#26377;&#23454;&#20363;&#21448;&#26377;&#21407;&#22411;&#23398;&#20064;&#30340;&#28789;&#27963;&#24615;&#12290;&#36825;&#20123;&#21457;&#29616;&#20026;&#23558;&#26469;&#30740;&#31350;Cobweb&#20316;&#20026;&#20154;&#31867;&#31867;&#21035;&#23398;&#20064;&#30340;&#32508;&#21512;&#27169;&#22411;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03835v1 Announce Type: cross  Abstract: Cobweb, a human like category learning system, differs from other incremental categorization models in constructing hierarchically organized cognitive tree-like structures using the category utility measure. Prior studies have shown that Cobweb can capture psychological effects such as the basic level, typicality, and fan effects. However, a broader evaluation of Cobweb as a model of human categorization remains lacking. The current study addresses this gap. It establishes Cobweb's alignment with classical human category learning effects. It also explores Cobweb's flexibility to exhibit both exemplar and prototype like learning within a single model. These findings set the stage for future research on Cobweb as a comprehensive model of human category learning.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;ChatDiet&#65292;&#19968;&#20010;&#20511;&#21161;LLM&#25216;&#26415;&#26500;&#24314;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#24110;&#21161;&#20010;&#24615;&#21270;&#33829;&#20859;&#23548;&#21521;&#39135;&#21697;&#25512;&#33616;&#32842;&#22825;&#26426;&#22120;&#20154;&#25552;&#20379;&#20010;&#24615;&#21270;&#21644;&#21487;&#35299;&#37322;&#30340;&#25512;&#33616;&#12290;</title><link>https://arxiv.org/abs/2403.00781</link><description>&lt;p&gt;
ChatDiet&#65306;&#36890;&#36807;LLM&#22686;&#24378;&#26694;&#26550;&#36171;&#33021;&#20010;&#24615;&#21270;&#33829;&#20859;&#23548;&#21521;&#39135;&#21697;&#25512;&#33616;&#32842;&#22825;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
ChatDiet: Empowering Personalized Nutrition-Oriented Food Recommender Chatbots through an LLM-Augmented Framework
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00781
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;ChatDiet&#65292;&#19968;&#20010;&#20511;&#21161;LLM&#25216;&#26415;&#26500;&#24314;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#24110;&#21161;&#20010;&#24615;&#21270;&#33829;&#20859;&#23548;&#21521;&#39135;&#21697;&#25512;&#33616;&#32842;&#22825;&#26426;&#22120;&#20154;&#25552;&#20379;&#20010;&#24615;&#21270;&#21644;&#21487;&#35299;&#37322;&#30340;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39135;&#29289;&#23545;&#20581;&#24247;&#30340;&#28145;&#36828;&#24433;&#21709;&#20351;&#24471;&#20808;&#36827;&#30340;&#33829;&#20859;&#23548;&#21521;&#39135;&#21697;&#25512;&#33616;&#26381;&#21153;&#25104;&#20026;&#24517;&#35201;&#12290;&#20256;&#32479;&#26041;&#27861;&#24448;&#24448;&#32570;&#20047;&#20010;&#24615;&#21270;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#20114;&#21160;&#24615;&#31561;&#20851;&#38190;&#20803;&#32032;&#12290;&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24102;&#26469;&#20102;&#35299;&#37322;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#20294;&#23427;&#20204;&#21333;&#29420;&#30340;&#20351;&#29992;&#26410;&#33021;&#23454;&#29616;&#30495;&#27491;&#30340;&#20010;&#24615;&#21270;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;ChatDiet&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;LLM&#39537;&#21160;&#26694;&#26550;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#20010;&#24615;&#21270;&#33829;&#20859;&#23548;&#21521;&#39135;&#21697;&#25512;&#33616;&#32842;&#22825;&#26426;&#22120;&#20154;&#12290;ChatDiet&#38598;&#25104;&#20102;&#20010;&#20154;&#21644;&#20154;&#32676;&#27169;&#22411;&#65292;&#36741;&#20197;&#19968;&#20010;&#21327;&#35843;&#22120;&#65292;&#26080;&#32541;&#26816;&#32034;&#21644;&#22788;&#29702;&#30456;&#20851;&#20449;&#24687;&#12290;&#20854;&#32467;&#26524;&#26159;&#21160;&#24577;&#25552;&#20379;&#20010;&#24615;&#21270;&#21644;&#21487;&#35299;&#37322;&#30340;&#39135;&#21697;&#25512;&#33616;&#65292;&#26681;&#25454;&#20010;&#20154;&#29992;&#25143;&#21916;&#22909;&#23450;&#21046;&#12290;&#25105;&#20204;&#23545;ChatDiet&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21253;&#25324;&#19968;&#20010;&#24341;&#20154;&#20837;&#32988;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#22312;&#26696;&#20363;&#30740;&#31350;&#20013;&#24314;&#31435;&#20102;&#19968;&#20010;&#22240;&#26524;&#20010;&#20154;&#27169;&#22411;&#26469;&#20272;&#35745;&#20010;&#20154;&#33829;&#20859;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00781v1 Announce Type: cross  Abstract: The profound impact of food on health necessitates advanced nutrition-oriented food recommendation services. Conventional methods often lack the crucial elements of personalization, explainability, and interactivity. While Large Language Models (LLMs) bring interpretability and explainability, their standalone use falls short of achieving true personalization. In this paper, we introduce ChatDiet, a novel LLM-powered framework designed specifically for personalized nutrition-oriented food recommendation chatbots. ChatDiet integrates personal and population models, complemented by an orchestrator, to seamlessly retrieve and process pertinent information. The result is a dynamic delivery of personalized and explainable food recommendations, tailored to individual user preferences. Our evaluation of ChatDiet includes a compelling case study, where we establish a causal personal model to estimate individual nutrition effects. Our assessmen
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;CPFT&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#20013;&#31934;&#32454;&#35843;&#25972;&#36807;&#31243;&#20013;&#32467;&#21512;&#20132;&#21449;&#29109;&#25439;&#22833;&#20989;&#25968;&#21644;&#22522;&#20110;&#31526;&#21512;&#24615;&#39044;&#27979;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#22686;&#24378;&#20102;&#25512;&#33616;&#31995;&#32479;&#30340;&#32622;&#20449;&#24230;&#12290;CPFT&#21160;&#24577;&#29983;&#25104;&#28508;&#22312;&#30495;&#23454;&#20540;&#30340;&#39033;&#30446;&#38598;&#21512;&#65292;&#25552;&#21319;&#20102;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#39640;&#20102;&#25512;&#33616;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#20449;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.08976</link><description>&lt;p&gt;
&#36890;&#36807;&#31526;&#21512;&#24615;&#39044;&#27979;&#23454;&#29616;&#32622;&#20449;&#24230;&#24863;&#30693;&#30340;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#30340;&#31934;&#32454;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Confidence-aware Fine-tuning of Sequential Recommendation Systems via Conformal Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08976
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;CPFT&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#20013;&#31934;&#32454;&#35843;&#25972;&#36807;&#31243;&#20013;&#32467;&#21512;&#20132;&#21449;&#29109;&#25439;&#22833;&#20989;&#25968;&#21644;&#22522;&#20110;&#31526;&#21512;&#24615;&#39044;&#27979;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#22686;&#24378;&#20102;&#25512;&#33616;&#31995;&#32479;&#30340;&#32622;&#20449;&#24230;&#12290;CPFT&#21160;&#24577;&#29983;&#25104;&#28508;&#22312;&#30495;&#23454;&#20540;&#30340;&#39033;&#30446;&#38598;&#21512;&#65292;&#25552;&#21319;&#20102;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#39640;&#20102;&#25512;&#33616;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#20013;&#65292;&#36890;&#24120;&#20351;&#29992;&#20132;&#21449;&#29109;&#25439;&#22833;&#20989;&#25968;&#65292;&#20294;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#26410;&#33021;&#21033;&#29992;&#39033;&#30446;&#32622;&#20449;&#24230;&#20998;&#25968;&#12290;&#20026;&#20102;&#35748;&#35782;&#21040;&#32622;&#20449;&#24230;&#22312;&#23558;&#35757;&#32451;&#30446;&#26631;&#19982;&#35780;&#20272;&#25351;&#26631;&#23545;&#40784;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CPFT&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#21151;&#33021;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#31934;&#32454;&#35843;&#25972;&#36807;&#31243;&#20013;&#23558;&#22522;&#20110;&#31526;&#21512;&#24615;&#39044;&#27979;&#30340;&#25439;&#22833;&#20989;&#25968;&#19982;&#20132;&#21449;&#29109;&#25439;&#22833;&#20989;&#25968;&#30456;&#32467;&#21512;&#65292;&#22686;&#24378;&#20102;&#25512;&#33616;&#31995;&#32479;&#30340;&#32622;&#20449;&#24230;&#12290;CPFT&#21160;&#24577;&#29983;&#25104;&#19968;&#32452;&#20855;&#26377;&#39640;&#27010;&#29575;&#21253;&#21547;&#30495;&#23454;&#20540;&#30340;&#39033;&#30446;&#65292;&#36890;&#36807;&#23558;&#39564;&#35777;&#25968;&#25454;&#32435;&#20837;&#35757;&#32451;&#36807;&#31243;&#32780;&#19981;&#25439;&#23475;&#20854;&#22312;&#27169;&#22411;&#36873;&#25321;&#20013;&#30340;&#20316;&#29992;&#65292;&#20016;&#23500;&#20102;&#35757;&#32451;&#36807;&#31243;&#12290;&#36825;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#19982;&#22522;&#20110;&#31526;&#21512;&#24615;&#39044;&#27979;&#30340;&#25439;&#22833;&#20989;&#25968;&#30456;&#32467;&#21512;&#65292;&#26356;&#19987;&#27880;&#20110;&#25913;&#21892;&#25512;&#33616;&#38598;&#21512;&#65292;&#20174;&#32780;&#25552;&#39640;&#28508;&#22312;&#39033;&#30446;&#39044;&#27979;&#30340;&#32622;&#20449;&#24230;&#12290;&#36890;&#36807;&#36890;&#36807;&#22522;&#20110;&#31526;&#21512;&#24615;&#39044;&#27979;&#30340;&#25439;&#22833;&#20989;&#25968;&#23545;&#39033;&#30446;&#32622;&#20449;&#24230;&#36827;&#34892;&#31934;&#32454;&#35843;&#25972;&#65292;CPFT&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#65292;&#25552;&#20379;&#20102;&#26356;&#31934;&#30830;&#21644;&#21487;&#20449;&#30340;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08976v1 Announce Type: new Abstract: In Sequential Recommendation Systems, Cross-Entropy (CE) loss is commonly used but fails to harness item confidence scores during training. Recognizing the critical role of confidence in aligning training objectives with evaluation metrics, we propose CPFT, a versatile framework that enhances recommendation confidence by integrating Conformal Prediction (CP)-based losses with CE loss during fine-tuning. CPFT dynamically generates a set of items with a high probability of containing the ground truth, enriching the training process by incorporating validation data without compromising its role in model selection. This innovative approach, coupled with CP-based losses, sharpens the focus on refining recommendation sets, thereby elevating the confidence in potential item predictions. By fine-tuning item confidence through CP-based losses, CPFT significantly enhances model performance, leading to more precise and trustworthy recommendations th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#22522;&#20110;Transformer&#30340;&#39034;&#24207;&#25512;&#33616;&#20013;&#23384;&#22312;&#30340;&#27880;&#24847;&#21147;&#26435;&#37325;&#20998;&#37197;&#19981;&#20934;&#30830;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2308.09419</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#39034;&#24207;&#25512;&#33616;&#20013;&#30340;&#27880;&#24847;&#21147;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
Attention Calibration for Transformer-based Sequential Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2308.09419
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#22522;&#20110;Transformer&#30340;&#39034;&#24207;&#25512;&#33616;&#20013;&#23384;&#22312;&#30340;&#27880;&#24847;&#21147;&#26435;&#37325;&#20998;&#37197;&#19981;&#20934;&#30830;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;Transformer&#30340;&#39034;&#24207;&#25512;&#33616;&#65288;SR&#65289;&#34028;&#21187;&#21457;&#23637;&#65292;&#20854;&#20013;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#26159;&#20854;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#33258;&#27880;&#24847;&#21147;&#34987;&#26222;&#36941;&#35748;&#20026;&#33021;&#22815;&#36890;&#36807;&#20026;&#36825;&#20123;&#39033;&#30446;&#23398;&#20064;&#26356;&#22823;&#30340;&#20851;&#27880;&#26435;&#37325;&#65292;&#26377;&#25928;&#22320;&#36873;&#25321;&#26469;&#33258;&#20114;&#21160;&#39033;&#30446;&#24207;&#21015;&#30340;&#20449;&#24687;&#20016;&#23500;&#19988;&#30456;&#20851;&#39033;&#30446;&#65292;&#20197;&#29992;&#20110;&#19979;&#19968;&#20010;&#39033;&#30446;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#20013;&#65292;&#36825;&#24182;&#19981;&#24635;&#26159;&#27491;&#30830;&#30340;&#12290;&#25105;&#20204;&#23545;&#19968;&#20123;&#20195;&#34920;&#24615;&#30340;&#22522;&#20110;Transformer&#30340;SR&#27169;&#22411;&#36827;&#34892;&#30340;&#32463;&#39564;&#20998;&#26512;&#26174;&#31034;&#65292;&#24456;&#24120;&#35265;&#30340;&#29616;&#35937;&#26159;&#23558;&#36739;&#22823;&#30340;&#27880;&#24847;&#21147;&#26435;&#37325;&#20998;&#37197;&#32473;&#19981;&#22815;&#30456;&#20851;&#30340;&#39033;&#30446;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#19981;&#20934;&#30830;&#30340;&#25512;&#33616;&#12290;&#36890;&#36807;&#36827;&#19968;&#27493;&#28145;&#20837;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#20004;&#20010;&#22240;&#32032;&#21487;&#33021;&#23548;&#33268;&#36825;&#31181;&#19981;&#20934;&#30830;&#30340;&#27880;&#24847;&#21147;&#26435;&#37325;&#20998;&#37197;&#65306;&#27425;&#20248;&#30340;&#20301;&#32622;&#32534;&#30721;&#21644;&#22024;&#26434;&#30340;&#36755;&#20837;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#30740;&#31350;&#20013;&#23384;&#22312;&#30340;&#36825;&#19968;&#37325;&#35201;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24046;&#36317;&#12290;&#26126;&#30830;&#22320;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;
&lt;/p&gt;
&lt;p&gt;
arXiv:2308.09419v2 Announce Type: replace  Abstract: Transformer-based sequential recommendation (SR) has been booming in recent years, with the self-attention mechanism as its key component. Self-attention has been widely believed to be able to effectively select those informative and relevant items from a sequence of interacted items for next-item prediction via learning larger attention weights for these items. However, this may not always be true in reality. Our empirical analysis of some representative Transformer-based SR models reveals that it is not uncommon for large attention weights to be assigned to less relevant items, which can result in inaccurate recommendations. Through further in-depth analysis, we find two factors that may contribute to such inaccurate assignment of attention weights: sub-optimal position encoding and noisy input. To this end, in this paper, we aim to address this significant yet challenging gap in existing works. To be specific, we propose a simple 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22810;&#27169;&#24577;Transformer&#33976;&#39311;&#25216;&#26415;&#26469;&#36827;&#34892;&#35757;&#32451;&#30340;MTDVocaLiST&#27169;&#22411;&#65292;&#33021;&#22815;&#28145;&#24230;&#27169;&#20223;VocaLiST&#20013;Transformer&#30340;&#20132;&#21449;&#27880;&#24847;&#21147;&#20998;&#24067;&#21644;&#20540;&#20851;&#31995;&#65292;&#20174;&#32780;&#22312;&#38899;&#39057;-&#35270;&#35273;&#21516;&#27493;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2210.15563</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;Transformer&#33976;&#39311;&#29992;&#20110;&#38899;&#39057;-&#35270;&#35273;&#21516;&#27493;
&lt;/p&gt;
&lt;p&gt;
Multimodal Transformer Distillation for Audio-Visual Synchronization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2210.15563
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22810;&#27169;&#24577;Transformer&#33976;&#39311;&#25216;&#26415;&#26469;&#36827;&#34892;&#35757;&#32451;&#30340;MTDVocaLiST&#27169;&#22411;&#65292;&#33021;&#22815;&#28145;&#24230;&#27169;&#20223;VocaLiST&#20013;Transformer&#30340;&#20132;&#21449;&#27880;&#24847;&#21147;&#20998;&#24067;&#21644;&#20540;&#20851;&#31995;&#65292;&#20174;&#32780;&#22312;&#38899;&#39057;-&#35270;&#35273;&#21516;&#27493;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#39057;-&#35270;&#35273;&#21516;&#27493;&#26088;&#22312;&#30830;&#23450;&#35270;&#39057;&#20013;&#30340;&#21475;&#22411;&#36816;&#21160;&#21644;&#35821;&#38899;&#26159;&#21542;&#21516;&#27493;&#12290;VocaLiST&#36890;&#36807;&#34701;&#21512;&#22810;&#27169;&#24577;Transformer&#26469;&#27169;&#25311;&#38899;&#39057;-&#35270;&#35273;&#20132;&#20114;&#20449;&#24687;&#65292;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#38656;&#35201;&#39640;&#35745;&#31639;&#36164;&#28304;&#65292;&#20351;&#20854;&#22312;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#19981;&#20999;&#23454;&#38469;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;MTDVocaLiST&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#25105;&#20204;&#25552;&#20986;&#30340;&#22810;&#27169;&#24577;Transformer&#33976;&#39311;&#65288;MTD&#65289;&#25439;&#22833;&#36827;&#34892;&#35757;&#32451;&#12290;MTD&#25439;&#22833;&#20351;MTDVocaLiST&#27169;&#22411;&#33021;&#22815;&#28145;&#24230;&#27169;&#20223;VocaLiST&#20013;Transformer&#30340;&#20132;&#21449;&#27880;&#24847;&#21147;&#20998;&#24067;&#21644;&#20540;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#19981;&#30830;&#23450;&#24615;&#21152;&#26435;&#26469;&#20805;&#20998;&#21033;&#29992;&#25152;&#26377;&#23618;&#20013;&#30340;&#20132;&#20114;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#20004;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#65306;&#20174;&#33976;&#39311;&#26041;&#27861;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;MTD&#25439;&#22833;&#20248;&#20110;&#20854;&#20182;&#24378;&#33976;&#39311;&#22522;&#32447;&#12290;&#20174;&#32463;&#36807;&#33976;&#39311;&#27169;&#22411;&#30340;&#24615;&#33021;&#35282;&#24230;&#30475;&#65306;1) MTDVocaLiST &#32988;&#36807;
&lt;/p&gt;
&lt;p&gt;
arXiv:2210.15563v3 Announce Type: replace-cross  Abstract: Audio-visual synchronization aims to determine whether the mouth movements and speech in the video are synchronized. VocaLiST reaches state-of-the-art performance by incorporating multimodal Transformers to model audio-visual interact information. However, it requires high computing resources, making it impractical for real-world applications. This paper proposed an MTDVocaLiST model, which is trained by our proposed multimodal Transformer distillation (MTD) loss. MTD loss enables MTDVocaLiST model to deeply mimic the cross-attention distribution and value-relation in the Transformer of VocaLiST. Additionally, we harness uncertainty weighting to fully exploit the interaction information across all layers. Our proposed method is effective in two aspects: From the distillation method perspective, MTD loss outperforms other strong distillation baselines. From the distilled model's performance perspective: 1) MTDVocaLiST outperform
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32771;&#34385;&#22810;&#31181;&#29992;&#25143;&#34892;&#20026;&#26469;&#20943;&#36731;&#27969;&#34892;&#24230;&#20559;&#35265;&#65292;&#22788;&#29702;&#20102;&#22810;&#34892;&#20026;&#25512;&#33616;&#20013;&#30340;&#20844;&#24179;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2209.04589</link><description>&lt;p&gt;
&#22810;&#34892;&#20026;&#25512;&#33616;&#20013;&#30340;&#20844;&#24179;&#22240;&#26524;&#24178;&#39044;
&lt;/p&gt;
&lt;p&gt;
Causal Intervention for Fairness in Multi-behavior Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2209.04589
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32771;&#34385;&#22810;&#31181;&#29992;&#25143;&#34892;&#20026;&#26469;&#20943;&#36731;&#27969;&#34892;&#24230;&#20559;&#35265;&#65292;&#22788;&#29702;&#20102;&#22810;&#34892;&#20026;&#25512;&#33616;&#20013;&#30340;&#20844;&#24179;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#36890;&#24120;&#20174;&#21508;&#31181;&#29992;&#25143;&#34892;&#20026;&#20013;&#23398;&#20064;&#29992;&#25143;&#20852;&#36259;&#65292;&#21253;&#25324;&#28857;&#20987;&#21644;&#28857;&#20987;&#21518;&#30340;&#34892;&#20026;&#65288;&#20363;&#22914;&#65292;&#28857;&#36190;&#21644;&#25910;&#34255;&#65289;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#34892;&#20026;&#19981;&#21487;&#36991;&#20813;&#22320;&#34920;&#29616;&#20986;&#27969;&#34892;&#24230;&#20559;&#24046;&#65292;&#23548;&#33268;&#19968;&#20123;&#19981;&#20844;&#24179;&#38382;&#39064;&#65306;1&#65289;&#23545;&#20110;&#30456;&#20284;&#36136;&#37327;&#30340;&#29289;&#21697;&#65292;&#26356;&#21463;&#27426;&#36814;&#30340;&#29289;&#21697;&#20250;&#33719;&#24471;&#26356;&#22810;&#26333;&#20809;&#65307;2&#65289;&#26356;&#31967;&#31957;&#30340;&#26159;&#65292;&#27969;&#34892;&#24230;&#36739;&#20302;&#30340;&#21463;&#27426;&#36814;&#29289;&#21697;&#21487;&#33021;&#20250;&#33719;&#24471;&#26356;&#22810;&#26333;&#20809;&#12290;&#29616;&#26377;&#24037;&#20316;&#22312;&#20943;&#36731;&#27969;&#34892;&#24230;&#20559;&#24046;&#26041;&#38754;&#30450;&#30446;&#28040;&#38500;&#20559;&#35265;&#65292;&#36890;&#24120;&#24573;&#30053;&#29289;&#21697;&#36136;&#37327;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#19981;&#21516;&#29992;&#25143;&#34892;&#20026;&#20043;&#38388;&#65288;&#20363;&#22914;&#36716;&#21270;&#29575;&#65289;&#30340;&#20851;&#31995;&#23454;&#38469;&#19978;&#21453;&#26144;&#20102;&#29289;&#21697;&#36136;&#37327;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#22788;&#29702;&#19981;&#20844;&#24179;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#32771;&#34385;&#22810;&#31181;&#29992;&#25143;&#34892;&#20026;&#26469;&#20943;&#36731;&#27969;&#34892;&#24230;&#20559;&#35265;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22810;&#34892;&#20026;&#25512;&#33616;&#20013;&#20132;&#20114;&#29983;&#25104;&#36807;&#31243;&#32972;&#21518;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2209.04589v2 Announce Type: replace-cross  Abstract: Recommender systems usually learn user interests from various user behaviors, including clicks and post-click behaviors (e.g., like and favorite). However, these behaviors inevitably exhibit popularity bias, leading to some unfairness issues: 1) for items with similar quality, more popular ones get more exposure; and 2) even worse the popular items with lower popularity might receive more exposure. Existing work on mitigating popularity bias blindly eliminates the bias and usually ignores the effect of item quality. We argue that the relationships between different user behaviors (e.g., conversion rate) actually reflect the item quality. Therefore, to handle the unfairness issues, we propose to mitigate the popularity bias by considering multiple user behaviors.   In this work, we examine causal relationships behind the interaction generation procedure in multi-behavior recommendation. Specifically, we find that: 1) item popula
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;3D-MoLM&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#32473;&#35821;&#35328;&#27169;&#22411;&#37197;&#22791;&#19968;&#20010;3D&#20998;&#23376;&#32534;&#30721;&#22120;&#65292;&#23454;&#29616;&#20102;&#23545;3D&#20998;&#23376;-&#25991;&#26412;&#30340;&#35299;&#37322;&#21644;&#20998;&#26512;&#65292;&#27492;&#27169;&#22411;&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#22522;&#32447;&#12290;</title><link>http://arxiv.org/abs/2401.13923</link><description>&lt;p&gt;
&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#23545;3D&#20998;&#23376;-&#25991;&#26412;&#30340;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Towards 3D Molecule-Text Interpretation in Language Models. (arXiv:2401.13923v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13923
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;3D-MoLM&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#32473;&#35821;&#35328;&#27169;&#22411;&#37197;&#22791;&#19968;&#20010;3D&#20998;&#23376;&#32534;&#30721;&#22120;&#65292;&#23454;&#29616;&#20102;&#23545;3D&#20998;&#23376;-&#25991;&#26412;&#30340;&#35299;&#37322;&#21644;&#20998;&#26512;&#65292;&#27492;&#27169;&#22411;&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#26377;&#30528;&#24456;&#22823;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23545;&#20110;&#29702;&#35299;3D&#20998;&#23376;&#32467;&#26500;&#30340;&#22266;&#26377;&#38480;&#21046;&#26497;&#22823;&#22320;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#29983;&#29289;&#20998;&#23376;&#39046;&#22495;&#30340;&#28508;&#21147;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#20851;&#27880;&#20110;3D&#20998;&#23376;-&#25991;&#26412;&#35299;&#37322;&#65292;&#24182;&#25552;&#20986;3D-MoLM&#65306;3D&#20998;&#23376;&#35821;&#35328;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;3D-MoLM&#36890;&#36807;&#20026;LM&#37197;&#22791;&#19968;&#20010;3D&#20998;&#23376;&#32534;&#30721;&#22120;&#65292;&#20351;&#24471;LM&#33021;&#22815;&#35299;&#37322;&#21644;&#20998;&#26512;3D&#20998;&#23376;&#12290;&#36825;&#31181;&#38598;&#25104;&#26159;&#36890;&#36807;&#19968;&#20010;3D&#20998;&#23376;-&#25991;&#26412;&#25237;&#24433;&#22120;&#23454;&#29616;&#30340;&#65292;&#23427;&#36830;&#25509;&#20102;3D&#20998;&#23376;&#32534;&#30721;&#22120;&#30340;&#34920;&#31034;&#31354;&#38388;&#21644;LM&#30340;&#36755;&#20837;&#31354;&#38388;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#22686;&#24378;3D-MoLM&#22312;&#36328;&#27169;&#24577;&#20998;&#23376;&#29702;&#35299;&#21644;&#25351;&#20196;&#36319;&#38543;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#31934;&#24515;&#31574;&#21010;&#20102;&#19968;&#20010;&#20197;3D&#20998;&#23376;&#20026;&#20013;&#24515;&#30340;&#25351;&#24341;&#35843;&#25972;&#25968;&#25454;&#38598;--3D-MoIT&#12290;&#36890;&#36807;3D&#20998;&#23376;-&#25991;&#26412;&#23545;&#40784;&#21644;3D&#20998;&#23376;&#20013;&#24515;&#30340;&#25351;&#24341;&#35843;&#25972;&#65292;3D-MoLM&#24314;&#31435;&#20102;3D&#20998;&#23376;&#32534;&#30721;&#22120;&#21644;LM&#30340;&#38598;&#25104;&#12290;&#23427;&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#26174;&#33879;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language Models (LMs) have greatly influenced diverse domains. However, their inherent limitation in comprehending 3D molecular structures has considerably constrained their potential in the biomolecular domain. To bridge this gap, we focus on 3D molecule-text interpretation, and propose 3D-MoLM: 3D-Molecular Language Modeling. Specifically, 3D-MoLM enables an LM to interpret and analyze 3D molecules by equipping the LM with a 3D molecular encoder. This integration is achieved by a 3D molecule-text projector, bridging the 3D molecular encoder's representation space and the LM's input space. Moreover, to enhance 3D-MoLM's ability of cross-modal molecular understanding and instruction following, we meticulously curated a 3D molecule-centric instruction tuning dataset -- 3D-MoIT. Through 3D molecule-text alignment and 3D molecule-centric instruction tuning, 3D-MoLM establishes an integration of 3D molecular encoder and LM. It significantly surpasses existing baselines on downstream tasks,
&lt;/p&gt;</description></item><item><title>SpeechDPR&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#24320;&#25918;&#39046;&#22495;&#21475;&#35821;&#38382;&#31572;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#33021;&#22815;&#20174;&#21475;&#35821;&#23384;&#26723;&#20013;&#26816;&#32034;&#21487;&#33021;&#21253;&#21547;&#31572;&#26696;&#30340;&#27573;&#33853;&#12290;&#36890;&#36807;&#34701;&#21512;&#26080;&#30417;&#30563;ASR&#21644;&#25991;&#26412;&#23494;&#38598;&#26816;&#32034;&#22120;&#30340;&#30693;&#35782;&#65292;SpeechDPR&#33021;&#22815;&#33719;&#24471;&#36739;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;UASR&#24615;&#33021;&#36739;&#24046;&#26102;&#34920;&#29616;&#26356;&#21152;&#40065;&#26834;&#12290;</title><link>http://arxiv.org/abs/2401.13463</link><description>&lt;p&gt;
SpeechDPR: &#24320;&#25918;&#39046;&#22495;&#21475;&#35821;&#38382;&#31572;&#30340;&#31471;&#21040;&#31471;&#21475;&#35821;&#27573;&#33853;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
SpeechDPR: End-to-End Spoken Passage Retrieval for Open-Domain Spoken Question Answering. (arXiv:2401.13463v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13463
&lt;/p&gt;
&lt;p&gt;
SpeechDPR&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#24320;&#25918;&#39046;&#22495;&#21475;&#35821;&#38382;&#31572;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#33021;&#22815;&#20174;&#21475;&#35821;&#23384;&#26723;&#20013;&#26816;&#32034;&#21487;&#33021;&#21253;&#21547;&#31572;&#26696;&#30340;&#27573;&#33853;&#12290;&#36890;&#36807;&#34701;&#21512;&#26080;&#30417;&#30563;ASR&#21644;&#25991;&#26412;&#23494;&#38598;&#26816;&#32034;&#22120;&#30340;&#30693;&#35782;&#65292;SpeechDPR&#33021;&#22815;&#33719;&#24471;&#36739;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;UASR&#24615;&#33021;&#36739;&#24046;&#26102;&#34920;&#29616;&#26356;&#21152;&#40065;&#26834;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21475;&#35821;&#38382;&#31572;(SQA)&#26159;&#26426;&#22120;&#36890;&#36807;&#22312;&#32473;&#23450;&#21475;&#35821;&#27573;&#33853;&#20013;&#25214;&#21040;&#31572;&#26696;&#33539;&#22260;&#26469;&#22238;&#31572;&#29992;&#25143;&#38382;&#39064;&#30340;&#20851;&#38190;&#12290;&#36807;&#21435;&#30340;SQA&#26041;&#27861;&#27809;&#26377;&#20351;&#29992;ASR&#65292;&#20197;&#36991;&#20813;&#35782;&#21035;&#38169;&#35823;&#21644;&#35789;&#27719;&#22806;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#23454;&#38469;&#30340;&#24320;&#25918;&#39046;&#22495;SQA(openSQA)&#38382;&#39064;&#20013;&#65292;&#26426;&#22120;&#38656;&#35201;&#39318;&#20808;&#20174;&#21475;&#35821;&#23384;&#26723;&#20013;&#26816;&#32034;&#21487;&#33021;&#21253;&#21547;&#31572;&#26696;&#30340;&#27573;&#33853;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#24050;&#30693;&#30340;&#29992;&#20110;openSQA&#38382;&#39064;&#26816;&#32034;&#32452;&#20214;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;SpeechDPR&#12290;SpeechDPR&#36890;&#36807;&#20174;&#26080;&#30417;&#30563;ASR(UASR)&#21644;&#25991;&#26412;&#23494;&#38598;&#26816;&#32034;&#22120;(TDR)&#30340;&#32423;&#32852;&#27169;&#22411;&#20013;&#25552;&#28860;&#30693;&#35782;&#65292;&#23398;&#20064;&#21477;&#23376;&#32423;&#35821;&#20041;&#34920;&#31034;&#12290;&#19981;&#38656;&#35201;&#25163;&#21160;&#36716;&#24405;&#30340;&#35821;&#38899;&#25968;&#25454;&#12290;&#21021;&#27493;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#32423;&#32852;&#30340;UASR&#21644;TDR&#27169;&#22411;&#30456;&#27604;&#65292;&#24615;&#33021;&#30456;&#24403;&#65292;&#24182;&#19988;&#22312;UASR&#24615;&#33021;&#36739;&#24046;&#26102;&#26174;&#33879;&#25552;&#39640;&#65292;&#39564;&#35777;&#20102;&#36825;&#31181;&#26041;&#27861;&#26356;&#21152;&#40065;&#26834;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spoken Question Answering (SQA) is essential for machines to reply to user's question by finding the answer span within a given spoken passage. SQA has been previously achieved without ASR to avoid recognition errors and Out-of-Vocabulary (OOV) problems. However, the real-world problem of Open-domain SQA (openSQA), in which the machine needs to first retrieve passages that possibly contain the answer from a spoken archive in addition, was never considered. This paper proposes the first known end-to-end framework, Speech Dense Passage Retriever (SpeechDPR), for the retrieval component of the openSQA problem. SpeechDPR learns a sentence-level semantic representation by distilling knowledge from the cascading model of unsupervised ASR (UASR) and text dense retriever (TDR). No manually transcribed speech data is needed. Initial experiments showed performance comparable to the cascading model of UASR and TDR, and significantly better when UASR was poor, verifying this approach is more robus
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#27169;&#22411;&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#22810;&#35821;&#35328;&#20167;&#24680;&#35328;&#35770;&#12290;&#35813;&#27169;&#22411;&#22312;&#24847;&#22823;&#21033;&#35821;&#12289;&#33521;&#35821;&#12289;&#24503;&#35821;&#21644;&#23391;&#21152;&#25289;&#35821;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#24182;&#21462;&#24471;&#20102;&#27604;&#29616;&#26377;&#27169;&#22411;&#26356;&#39640;&#30340;&#25104;&#21151;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.11021</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#28145;&#24230;&#23398;&#20064;&#30340;&#22810;&#35821;&#35328;&#20167;&#24680;&#35328;&#35770;&#20998;&#26512;&#21644;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Analysis and Detection of Multilingual Hate Speech Using Transformer Based Deep Learning. (arXiv:2401.11021v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11021
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#27169;&#22411;&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#22810;&#35821;&#35328;&#20167;&#24680;&#35328;&#35770;&#12290;&#35813;&#27169;&#22411;&#22312;&#24847;&#22823;&#21033;&#35821;&#12289;&#33521;&#35821;&#12289;&#24503;&#35821;&#21644;&#23391;&#21152;&#25289;&#35821;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#24182;&#21462;&#24471;&#20102;&#27604;&#29616;&#26377;&#27169;&#22411;&#26356;&#39640;&#30340;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20167;&#24680;&#35328;&#35770;&#26159;&#30452;&#25509;&#25915;&#20987;&#25110;&#23459;&#20256;&#38024;&#23545;&#29305;&#23450;&#32676;&#20307;&#25110;&#20010;&#20154;&#30340;&#24974;&#24680;&#30340;&#26377;&#23475;&#20869;&#23481;&#65292;&#20363;&#22914;&#31181;&#26063;&#20027;&#20041;&#12289;&#23447;&#25945;&#25110;&#24615;&#21462;&#21521;&#31561;&#12290;&#36825;&#20250;&#23545;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#30340;&#31038;&#20250;&#29983;&#27963;&#20135;&#29983;&#24433;&#21709;&#65292;&#22240;&#20026;&#36890;&#36807;&#31038;&#20132;&#23186;&#20307;&#20998;&#20139;&#30340;&#20167;&#24680;&#20869;&#23481;&#21487;&#33021;&#20250;&#23545;&#20010;&#20154;&#21644;&#31038;&#21306;&#36896;&#25104;&#20260;&#23475;&#12290;&#38543;&#30528;&#32593;&#32476;&#19978;&#20167;&#24680;&#35328;&#35770;&#30340;&#22686;&#21152;&#65292;&#33258;&#21160;&#21270;&#26816;&#27979;&#20316;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#19968;&#20010;&#20219;&#21153;&#30340;&#38656;&#27714;&#20063;&#22312;&#22686;&#21152;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22522;&#20110;Transformer&#27169;&#22411;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#26816;&#27979;&#20167;&#24680;&#35328;&#35770;&#30340;&#26041;&#27861;&#65292;&#22914;Twitter&#12289;Facebook&#12289;WhatsApp&#12289;Instagram&#31561;&#12290;&#35813;&#27169;&#22411;&#29420;&#31435;&#20110;&#35821;&#35328;&#65292;&#24182;&#24050;&#22312;&#24847;&#22823;&#21033;&#35821;&#12289;&#33521;&#35821;&#12289;&#24503;&#35821;&#12289;&#23391;&#21152;&#25289;&#35821;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#40644;&#37329;&#26631;&#20934;&#25968;&#25454;&#38598;&#30001;&#30693;&#21517;&#30740;&#31350;&#32773;Zeerak Talat&#12289;Sara Tonelli&#12289;Melanie Siegel&#21644;Rezaul Karim&#25910;&#38598;&#12290;&#25152;&#25552;&#20986;&#27169;&#22411;&#22312;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#26041;&#38754;&#30340;&#25104;&#21151;&#29575;&#39640;&#20110;&#29616;&#26377;&#22522;&#20934;&#21644;&#26368;&#20808;&#36827;&#27169;&#22411;&#65292;&#20934;&#30830;&#29575;&#36739;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hate speech is harmful content that directly attacks or promotes hatred against members of groups or individuals based on actual or perceived aspects of identity, such as racism, religion, or sexual orientation. This can affect social life on social media platforms as hateful content shared through social media can harm both individuals and communities. As the prevalence of hate speech increases online, the demand for automated detection as an NLP task is increasing. In this work, the proposed method is using transformer-based model to detect hate speech in social media, like twitter, Facebook, WhatsApp, Instagram, etc. The proposed model is independent of languages and has been tested on Italian, English, German, Bengali. The Gold standard datasets were collected from renowned researcher Zeerak Talat, Sara Tonelli, Melanie Siegel, and Rezaul Karim. The success rate of the proposed model for hate speech detection is higher than the existing baseline and state-of-the-art models with acc
&lt;/p&gt;</description></item><item><title>NineRec&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#21487;&#36801;&#31227;&#25512;&#33616;&#30340;&#25968;&#25454;&#38598;&#22871;&#20214;&#65292;&#21253;&#25324;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#28304;&#22495;&#25512;&#33616;&#25968;&#25454;&#38598;&#21644;&#20061;&#20010;&#22810;&#26679;&#30340;&#30446;&#26631;&#22495;&#25512;&#33616;&#25968;&#25454;&#38598;&#12290;&#27599;&#20010;&#29289;&#21697;&#30001;&#25991;&#26412;&#25551;&#36848;&#21644;&#39640;&#20998;&#36776;&#29575;&#23553;&#38754;&#22270;&#20687;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2309.07705</link><description>&lt;p&gt;
NineRec: &#29992;&#20110;&#35780;&#20272;&#21487;&#36801;&#31227;&#25512;&#33616;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#22871;&#20214;
&lt;/p&gt;
&lt;p&gt;
NineRec: A Benchmark Dataset Suite for Evaluating Transferable Recommendation. (arXiv:2309.07705v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07705
&lt;/p&gt;
&lt;p&gt;
NineRec&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#21487;&#36801;&#31227;&#25512;&#33616;&#30340;&#25968;&#25454;&#38598;&#22871;&#20214;&#65292;&#21253;&#25324;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#28304;&#22495;&#25512;&#33616;&#25968;&#25454;&#38598;&#21644;&#20061;&#20010;&#22810;&#26679;&#30340;&#30446;&#26631;&#22495;&#25512;&#33616;&#25968;&#25454;&#38598;&#12290;&#27599;&#20010;&#29289;&#21697;&#30001;&#25991;&#26412;&#25551;&#36848;&#21644;&#39640;&#20998;&#36776;&#29575;&#23553;&#38754;&#22270;&#20687;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20174;&#29289;&#21697;&#30340;&#21407;&#22987;&#29305;&#24449;&#65288;&#22914;&#22270;&#20687;&#12289;&#25991;&#26412;&#12289;&#38899;&#39057;&#31561;&#65289;&#23398;&#20064;&#25512;&#33616;&#31995;&#32479;&#27169;&#22411;&#65292;&#31216;&#20026;MoRec&#65292;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20852;&#36259;&#12290; MoRec&#30340;&#19968;&#20010;&#20851;&#38190;&#20248;&#21183;&#26159;&#23427;&#21487;&#20197;&#36731;&#26494;&#21463;&#30410;&#20110;&#20854;&#20182;&#39046;&#22495;&#30340;&#36827;&#23637;&#65292;&#22914;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#65288;CV&#65289;&#12290;&#27492;&#22806;&#65292;&#23427;&#36890;&#36807;&#29305;&#24449;&#33258;&#28982;&#25903;&#25345;&#19981;&#21516;&#31995;&#32479;&#20043;&#38388;&#30340;&#36801;&#31227;&#23398;&#20064;&#65292;&#31216;&#20026;&#21487;&#36801;&#31227;&#25512;&#33616;&#31995;&#32479;&#25110;TransRec&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#19982;NLP&#21644;CV&#39046;&#22495;&#30340;&#24320;&#21019;&#24615;&#22522;&#30784;&#27169;&#22411;&#30456;&#27604;&#65292;TransRec&#21462;&#24471;&#20102;&#24456;&#23567;&#30340;&#36827;&#23637;&#12290;&#32570;&#20047;&#22823;&#35268;&#27169;&#12289;&#39640;&#36136;&#37327;&#30340;&#25512;&#33616;&#25968;&#25454;&#38598;&#26159;&#19968;&#20010;&#37325;&#22823;&#38556;&#30861;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;NineRec&#65292;&#36825;&#26159;&#19968;&#20010;TransRec&#25968;&#25454;&#38598;&#22871;&#20214;&#65292;&#21253;&#25324;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#28304;&#22495;&#25512;&#33616;&#25968;&#25454;&#38598;&#21644;&#20061;&#20010;&#22810;&#26679;&#30340;&#30446;&#26631;&#22495;&#25512;&#33616;&#25968;&#25454;&#38598;&#12290;NineRec&#20013;&#30340;&#27599;&#20010;&#29289;&#21697;&#37117;&#30001;&#19968;&#20010;&#25991;&#26412;&#25551;&#36848;&#21644;&#19968;&#20010;&#39640;&#20998;&#36776;&#29575;&#30340;&#23553;&#38754;&#22270;&#20687;&#34920;&#31034;&#12290;&#36890;&#36807;NineRec&#65292;&#25105;&#20204;&#21487;&#20197;&#23454;&#29616;Tran
&lt;/p&gt;
&lt;p&gt;
Learning a recommender system model from an item's raw modality features (such as image, text, audio, etc.), called MoRec, has attracted growing interest recently. One key advantage of MoRec is that it can easily benefit from advances in other fields, such as natural language processing (NLP) and computer vision (CV). Moreover, it naturally supports transfer learning across different systems through modality features, known as transferable recommender systems, or TransRec.  However, so far, TransRec has made little progress, compared to groundbreaking foundation models in the fields of NLP and CV. The lack of large-scale, high-quality recommendation datasets poses a major obstacle. To this end, we introduce NineRec, a TransRec dataset suite that includes a large-scale source domain recommendation dataset and nine diverse target domain recommendation datasets. Each item in NineRec is represented by a text description and a high-resolution cover image. With NineRec, we can implement Tran
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#20845;&#20010;&#31038;&#21306;&#38382;&#31572;&#24179;&#21488;&#30340;&#30740;&#31350;&#65292;&#21457;&#29616;&#20102;&#26597;&#35810;&#30340;&#20803;&#25968;&#25454;&#12289;&#38382;&#39064;&#26500;&#25104;&#26041;&#24335;&#21644;&#29992;&#25143;&#20114;&#21160;&#27700;&#24179;&#19982;&#31532;&#19968;&#20010;&#22238;&#31572;&#26102;&#38388;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#24182;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#26597;&#35810;&#26159;&#21542;&#33021;&#22815;&#36805;&#36895;&#33719;&#24471;&#22238;&#31572;&#12290;</title><link>http://arxiv.org/abs/2309.05961</link><description>&lt;p&gt;
&#35780;&#20272;&#28526;&#36215;&#28526;&#33853;&#65306;&#23545;&#19981;&#21516;&#24179;&#21488;&#38388;&#38382;&#31572;&#36235;&#21183;&#30340;&#28145;&#20837;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Ebb and Flow: An In-depth Analysis of Question-Answering Trends across Diverse Platforms. (arXiv:2309.05961v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05961
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#20845;&#20010;&#31038;&#21306;&#38382;&#31572;&#24179;&#21488;&#30340;&#30740;&#31350;&#65292;&#21457;&#29616;&#20102;&#26597;&#35810;&#30340;&#20803;&#25968;&#25454;&#12289;&#38382;&#39064;&#26500;&#25104;&#26041;&#24335;&#21644;&#29992;&#25143;&#20114;&#21160;&#27700;&#24179;&#19982;&#31532;&#19968;&#20010;&#22238;&#31572;&#26102;&#38388;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#24182;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#26597;&#35810;&#26159;&#21542;&#33021;&#22815;&#36805;&#36895;&#33719;&#24471;&#22238;&#31572;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#21306;&#38382;&#31572;&#24179;&#21488;&#22240;&#20854;&#24555;&#36895;&#22238;&#31572;&#29992;&#25143;&#26597;&#35810;&#30340;&#33021;&#21147;&#32780;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#36825;&#20123;&#22238;&#31572;&#36895;&#24230;&#30340;&#24555;&#24930;&#21462;&#20915;&#20110;&#26597;&#35810;&#29305;&#23450;&#21644;&#29992;&#25143;&#30456;&#20851;&#30340;&#22240;&#32032;&#30340;&#32508;&#21512;&#12290;&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#20845;&#20010;&#39640;&#24230;&#27969;&#34892;&#30340;&#31038;&#21306;&#38382;&#31572;&#24179;&#21488;&#65292;&#20998;&#26512;&#20102;&#36825;&#20123;&#22240;&#32032;&#22312;&#20854;&#20013;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#25581;&#31034;&#20102;&#38382;&#39064;&#30340;&#31532;&#19968;&#20010;&#22238;&#31572;&#25152;&#33457;&#36153;&#30340;&#26102;&#38388;&#19982;&#20803;&#25968;&#25454;&#12289;&#38382;&#39064;&#30340;&#26500;&#25104;&#26041;&#24335;&#21644;&#29992;&#25143;&#20043;&#38388;&#30340;&#20114;&#21160;&#27700;&#24179;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#20351;&#29992;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20998;&#26512;&#36825;&#20123;&#20803;&#25968;&#25454;&#21644;&#29992;&#25143;&#20114;&#21160;&#27169;&#24335;&#65292;&#25105;&#20204;&#35797;&#22270;&#39044;&#27979;&#21738;&#20123;&#26597;&#35810;&#23558;&#36805;&#36895;&#33719;&#24471;&#21021;&#22987;&#22238;&#31572;&#12290;
&lt;/p&gt;
&lt;p&gt;
Community Question Answering (CQA) platforms steadily gain popularity as they provide users with fast responses to their queries. The swiftness of these responses is contingent on a mixture of query-specific and user-related elements. This paper scrutinizes these contributing factors within the context of six highly popular CQA platforms, identified through their standout answering speed. Our investigation reveals a correlation between the time taken to yield the first response to a question and several variables: the metadata, the formulation of the questions, and the level of interaction among users. Additionally, by employing conventional machine learning models to analyze these metadata and patterns of user interaction, we endeavor to predict which queries will receive their initial responses promptly.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#26102;&#38388;&#20852;&#36259;&#32593;&#32476;&#65288;TIN&#65289;&#65292;&#29992;&#20110;&#25429;&#25417;&#34892;&#20026;&#19982;&#30446;&#26631;&#20043;&#38388;&#30340;&#22235;&#37325;&#35821;&#20041;&#21644;&#26102;&#38388;&#30456;&#20851;&#24615;&#65292;&#20197;&#39044;&#27979;&#28857;&#20987;&#29575;&#30340;&#25928;&#26524;&#21644;&#24050;&#26377;&#26041;&#27861;&#23545;&#36825;&#31181;&#30456;&#20851;&#24615;&#30340;&#23398;&#20064;&#31243;&#24230;&#23578;&#19981;&#28165;&#26970;&#12290;</title><link>http://arxiv.org/abs/2308.08487</link><description>&lt;p&gt;
&#28857;&#20987;&#29575;&#39044;&#27979;&#30340;&#26102;&#38388;&#20852;&#36259;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Temporal Interest Network for Click-Through Rate Prediction. (arXiv:2308.08487v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08487
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#26102;&#38388;&#20852;&#36259;&#32593;&#32476;&#65288;TIN&#65289;&#65292;&#29992;&#20110;&#25429;&#25417;&#34892;&#20026;&#19982;&#30446;&#26631;&#20043;&#38388;&#30340;&#22235;&#37325;&#35821;&#20041;&#21644;&#26102;&#38388;&#30456;&#20851;&#24615;&#65292;&#20197;&#39044;&#27979;&#28857;&#20987;&#29575;&#30340;&#25928;&#26524;&#21644;&#24050;&#26377;&#26041;&#27861;&#23545;&#36825;&#31181;&#30456;&#20851;&#24615;&#30340;&#23398;&#20064;&#31243;&#24230;&#23578;&#19981;&#28165;&#26970;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#25143;&#34892;&#20026;&#30340;&#21382;&#21490;&#26159;&#39044;&#27979;&#28857;&#20987;&#29575;&#26368;&#37325;&#35201;&#30340;&#29305;&#24449;&#20043;&#19968;&#65292;&#22240;&#20026;&#23427;&#20204;&#19982;&#30446;&#26631;&#39033;&#30446;&#20855;&#26377;&#24378;&#28872;&#30340;&#35821;&#20041;&#21644;&#26102;&#38388;&#30456;&#20851;&#24615;&#12290;&#34429;&#28982;&#24050;&#26377;&#25991;&#29486;&#20998;&#21035;&#30740;&#31350;&#20102;&#36825;&#20123;&#30456;&#20851;&#24615;&#65292;&#20294;&#23578;&#26410;&#20998;&#26512;&#23427;&#20204;&#30340;&#32452;&#21512;&#65292;&#21363;&#34892;&#20026;&#35821;&#20041;&#12289;&#30446;&#26631;&#35821;&#20041;&#12289;&#34892;&#20026;&#26102;&#38388;&#21644;&#30446;&#26631;&#26102;&#38388;&#30340;&#22235;&#37325;&#30456;&#20851;&#24615;&#12290;&#36825;&#31181;&#30456;&#20851;&#24615;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#20197;&#21450;&#29616;&#26377;&#26041;&#27861;&#23398;&#20064;&#36825;&#31181;&#30456;&#20851;&#24615;&#30340;&#31243;&#24230;&#23578;&#19981;&#28165;&#26970;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#22312;&#23454;&#36341;&#20013;&#27979;&#37327;&#20102;&#22235;&#37325;&#30456;&#20851;&#24615;&#65292;&#24182;&#35266;&#23519;&#21040;&#30452;&#35266;&#32780;&#24378;&#22823;&#30340;&#22235;&#37325;&#27169;&#24335;&#12290;&#25105;&#20204;&#27979;&#37327;&#20102;&#20960;&#31181;&#20195;&#34920;&#24615;&#30340;&#29992;&#25143;&#34892;&#20026;&#26041;&#27861;&#30340;&#23398;&#20064;&#30456;&#20851;&#24615;&#65292;&#20294;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#23427;&#20204;&#37117;&#27809;&#26377;&#23398;&#20064;&#21040;&#36825;&#26679;&#30340;&#27169;&#24335;&#65292;&#29305;&#21035;&#26159;&#26102;&#38388;&#27169;&#24335;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26102;&#38388;&#20852;&#36259;&#32593;&#32476;&#65288;TIN&#65289;&#26469;&#25429;&#25417;&#34892;&#20026;&#19982;&#30446;&#26631;&#20043;&#38388;&#30340;&#22235;&#37325;&#35821;&#20041;&#21644;&#26102;&#38388;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The history of user behaviors constitutes one of the most significant characteristics in predicting the click-through rate (CTR), owing to their strong semantic and temporal correlation with the target item. While the literature has individually examined each of these correlations, research has yet to analyze them in combination, that is, the quadruple correlation of (behavior semantics, target semantics, behavior temporal, and target temporal). The effect of this correlation on performance and the extent to which existing methods learn it remain unknown. To address this gap, we empirically measure the quadruple correlation and observe intuitive yet robust quadruple patterns. We measure the learned correlation of several representative user behavior methods, but to our surprise, none of them learn such a pattern, especially the temporal one.  In this paper, we propose the Temporal Interest Network (TIN) to capture the quadruple semantic and temporal correlation between behaviors and th
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#23545;MovieLens&#25968;&#25454;&#38598;&#30340;&#20998;&#26512;&#65292;&#21457;&#29616;&#29992;&#25143;&#19982;&#35813;&#24179;&#21488;&#30340;&#20132;&#20114;&#22312;&#19981;&#21516;&#38454;&#27573;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#24182;&#19988;&#29992;&#25143;&#20132;&#20114;&#21463;&#21040;&#24179;&#21488;&#25512;&#33616;&#31639;&#27861;&#25512;&#33616;&#30340;&#20505;&#36873;&#30005;&#24433;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2307.09985</link><description>&lt;p&gt;
&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;MovieLens&#19978;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#34920;&#29616;&#65306;&#36825;&#24847;&#21619;&#30528;&#20160;&#20040;&#65311;
&lt;/p&gt;
&lt;p&gt;
Our Model Achieves Excellent Performance on MovieLens: What Does it Mean?. (arXiv:2307.09985v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09985
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#23545;MovieLens&#25968;&#25454;&#38598;&#30340;&#20998;&#26512;&#65292;&#21457;&#29616;&#29992;&#25143;&#19982;&#35813;&#24179;&#21488;&#30340;&#20132;&#20114;&#22312;&#19981;&#21516;&#38454;&#27573;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#24182;&#19988;&#29992;&#25143;&#20132;&#20114;&#21463;&#21040;&#24179;&#21488;&#25512;&#33616;&#31639;&#27861;&#25512;&#33616;&#30340;&#20505;&#36873;&#30005;&#24433;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#35780;&#20272;&#30340;&#20856;&#22411;&#22522;&#20934;&#25968;&#25454;&#38598;&#26159;&#22312;&#26576;&#19968;&#26102;&#38388;&#27573;&#20869;&#22312;&#24179;&#21488;&#19978;&#29983;&#25104;&#30340;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#25968;&#25454;&#12290;&#20132;&#20114;&#29983;&#25104;&#26426;&#21046;&#37096;&#20998;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#29992;&#25143;&#19982;&#29289;&#21697;&#36827;&#34892;&#20132;&#20114;&#65288;&#22914;&#21916;&#27426;&#12289;&#36141;&#20080;&#12289;&#35780;&#20998;&#65289;&#20197;&#21450;&#29305;&#23450;&#20132;&#20114;&#21457;&#29983;&#30340;&#32972;&#26223;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23545;MovieLens&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#32454;&#33268;&#30340;&#20998;&#26512;&#65292;&#24182;&#35299;&#37322;&#20102;&#20351;&#29992;&#35813;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#25512;&#33616;&#31639;&#27861;&#26102;&#21487;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#20174;&#20998;&#26512;&#20013;&#24471;&#20986;&#20102;&#19968;&#20123;&#20027;&#35201;&#21457;&#29616;&#12290;&#39318;&#20808;&#65292;&#22312;&#29992;&#25143;&#19982;MovieLens&#24179;&#21488;&#20132;&#20114;&#30340;&#19981;&#21516;&#38454;&#27573;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#26089;&#26399;&#20132;&#20114;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#23450;&#20041;&#20102;&#29992;&#25143;&#30011;&#20687;&#65292;&#24433;&#21709;&#20102;&#21518;&#32493;&#30340;&#20132;&#20114;&#12290;&#20854;&#27425;&#65292;&#29992;&#25143;&#20132;&#20114;&#21463;&#21040;&#24179;&#21488;&#20869;&#37096;&#25512;&#33616;&#31639;&#27861;&#25512;&#33616;&#30340;&#20505;&#36873;&#30005;&#24433;&#30340;&#24456;&#22823;&#24433;&#21709;&#12290;&#21024;&#38500;&#38752;&#36817;&#26368;&#21518;&#20960;&#27425;&#20132;&#20114;&#30340;&#20132;&#20114;&#20250;&#23545;&#32467;&#26524;&#20135;&#29983;&#36739;&#22823;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
A typical benchmark dataset for recommender system (RecSys) evaluation consists of user-item interactions generated on a platform within a time period. The interaction generation mechanism partially explains why a user interacts with (e.g.,like, purchase, rate) an item, and the context of when a particular interaction happened. In this study, we conduct a meticulous analysis on the MovieLens dataset and explain the potential impact on using the dataset for evaluating recommendation algorithms. We make a few main findings from our analysis. First, there are significant differences in user interactions at the different stages when a user interacts with the MovieLens platform. The early interactions largely define the user portrait which affect the subsequent interactions. Second, user interactions are highly affected by the candidate movies that are recommended by the platform's internal recommendation algorithm(s). Removal of interactions that happen nearer to the last few interactions 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#31616;&#21270;&#31038;&#20132;&#23186;&#20307;&#20449;&#24687;&#26816;&#32034;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35782;&#21035;&#21307;&#23398;&#23454;&#20307;&#12289;&#26631;&#20934;&#21270;&#23454;&#20307;&#21644;&#20998;&#37197;UMLS&#27010;&#24565;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#29992;&#20110;COVID-19&#30456;&#20851;&#25512;&#25991;&#30340;&#30151;&#29366;&#35789;&#20856;&#12290;</title><link>http://arxiv.org/abs/2306.16001</link><description>&lt;p&gt;
&#29992;&#28145;&#24230;&#23398;&#20064;&#31616;&#21270;&#31038;&#20132;&#23186;&#20307;&#20449;&#24687;&#26816;&#32034;&#20197;&#25903;&#25345;&#20844;&#20849;&#21355;&#29983;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Streamlining Social Media Information Retrieval for Public Health Research with Deep Learning. (arXiv:2306.16001v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16001
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#31616;&#21270;&#31038;&#20132;&#23186;&#20307;&#20449;&#24687;&#26816;&#32034;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35782;&#21035;&#21307;&#23398;&#23454;&#20307;&#12289;&#26631;&#20934;&#21270;&#23454;&#20307;&#21644;&#20998;&#37197;UMLS&#27010;&#24565;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#29992;&#20110;COVID-19&#30456;&#20851;&#25512;&#25991;&#30340;&#30151;&#29366;&#35789;&#20856;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#22312;&#27969;&#34892;&#30149;&#30417;&#27979;&#20013;&#30340;&#21033;&#29992;&#24050;&#32463;&#24471;&#21040;&#20102;&#24456;&#22909;&#30340;&#35777;&#23454;&#12290;&#28982;&#32780;&#65292;&#24403;&#20351;&#29992;&#39044;&#23450;&#20041;&#30340;&#35789;&#27719;&#34920;&#26469;&#26816;&#32034;&#30456;&#20851;&#35821;&#26009;&#24211;&#26102;&#65292;&#24120;&#24120;&#20250;&#24341;&#20837;&#20559;&#35265;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#26088;&#22312;&#26500;&#24314;&#21307;&#23398;&#20439;&#35821;&#21644;&#32479;&#19968;&#21307;&#23398;&#35821;&#35328;&#31995;&#32479;&#65288;UMLS&#65289;&#27010;&#24565;&#30340;&#24191;&#27867;&#23383;&#20856;&#12290;&#35813;&#26694;&#26550;&#30001;&#19977;&#20010;&#27169;&#22359;&#32452;&#25104;&#65306;&#22522;&#20110;BERT&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#27169;&#22411;&#65292;&#29992;&#20110;&#20174;&#31038;&#20132;&#23186;&#20307;&#20869;&#23481;&#20013;&#35782;&#21035;&#20986;&#21307;&#23398;&#23454;&#20307;&#65307;&#28145;&#24230;&#23398;&#20064;&#39537;&#21160;&#30340;&#26631;&#20934;&#21270;&#27169;&#22359;&#65292;&#29992;&#20110;&#23545;&#25552;&#21462;&#20986;&#30340;&#23454;&#20307;&#36827;&#34892;&#35268;&#33539;&#21270;&#22788;&#29702;&#65307;&#21322;&#30417;&#30563;&#32858;&#31867;&#27169;&#22359;&#65292;&#23558;&#26368;&#21487;&#33021;&#30340;UMLS&#27010;&#24565;&#20998;&#37197;&#32473;&#27599;&#20010;&#35268;&#33539;&#21270;&#23454;&#20307;&#12290;&#25105;&#20204;&#23558;&#35813;&#26694;&#26550;&#24212;&#29992;&#20110;&#20174;2020&#24180;2&#26376;1&#26085;&#21040;2022&#24180;4&#26376;30&#26085;&#26399;&#38388;&#19982;COVID-19&#30456;&#20851;&#30340;&#25512;&#25991;&#65292;&#29983;&#25104;&#20102;&#19968;&#20010;&#30151;&#29366;&#35789;&#20856;&#65288;&#21487;&#22312;https://github.com/ningkko/UMLS_colloquialism/&#19978;&#33719;&#21462;&#65289;&#65292;&#20854;&#20013;&#21253;&#21547;9,249&#20010;&#26631;&#20934;&#21270;&#23454;&#20307;&#65292;&#26144;&#23556;&#21040;876&#20010;UMLS&#27010;&#24565;&#21644;38,175&#20010;&#20442;&#35821;&#34920;&#36798;&#12290;&#35813;&#26694;&#26550;&#30340;&#28436;&#31034;
&lt;/p&gt;
&lt;p&gt;
The utilization of social media in epidemic surveillance has been well established. Nonetheless, bias is often introduced when pre-defined lexicons are used to retrieve relevant corpus. This study introduces a framework aimed at curating extensive dictionaries of medical colloquialisms and Unified Medical Language System (UMLS) concepts. The framework comprises three modules: a BERT-based Named Entity Recognition (NER) model that identifies medical entities from social media content, a deep-learning powered normalization module that standardizes the extracted entities, and a semi-supervised clustering module that assigns the most probable UMLS concept to each standardized entity. We applied this framework to COVID-19-related tweets from February 1, 2020, to April 30, 2022, generating a symptom dictionary (available at https://github.com/ningkko/UMLS_colloquialism/) composed of 9,249 standardized entities mapped to 876 UMLS concepts and 38,175 colloquial expressions. This framework demo
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;Sheaf&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#22411;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21521;&#37327;&#31354;&#38388;&#34920;&#31034;&#26041;&#27861;&#65292;&#20351;&#24471;&#20854;&#22312;&#22522;&#20934;&#25512;&#33616;&#20219;&#21153;&#19978;&#33719;&#24471;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.09097</link><description>&lt;p&gt;
&#22522;&#20110;Sheaf&#31070;&#32463;&#32593;&#32476;&#30340;&#22522;&#20110;&#22270;&#30340;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Sheaf Neural Networks for Graph-based Recommender Systems. (arXiv:2304.09097v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09097
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Sheaf&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#22411;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21521;&#37327;&#31354;&#38388;&#34920;&#31034;&#26041;&#27861;&#65292;&#20351;&#24471;&#20854;&#22312;&#22522;&#20934;&#25512;&#33616;&#20219;&#21153;&#19978;&#33719;&#24471;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;Graph&#31070;&#32463;&#32593;&#32476;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#65292;&#21253;&#25324;&#25512;&#33616;&#31995;&#32479;&#12290;Graph&#31070;&#32463;&#32593;&#32476;&#23545;&#20854;&#20182;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#22312;&#20110;&#65292;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#35768;&#22810;&#38382;&#39064;&#21487;&#20197;&#33258;&#28982;&#22320;&#24314;&#27169;&#20026;&#22270;&#65292;&#20854;&#20013;&#33410;&#28857;&#21487;&#20197;&#26159;&#29992;&#25143;&#25110;&#39033;&#30446;&#65292;&#36793;&#20195;&#34920;&#20559;&#22909;&#20851;&#31995;&#12290; &#22312;&#24403;&#21069;&#30340;Graph&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#20013;&#65292;&#33410;&#28857;&#29992;&#22312;&#35757;&#32451;&#26102;&#23398;&#20064;&#21040;&#30340;&#38745;&#24577;&#21521;&#37327;&#34920;&#31034;&#12290;&#36825;&#31181;&#38745;&#24577;&#21521;&#37327;&#21487;&#33021;&#21482;&#36866;&#29992;&#20110;&#25429;&#25417;&#23450;&#20041;&#23427;&#20204;&#30340;&#19968;&#20123;&#29992;&#25143;&#25110;&#39033;&#30446;&#30340;&#24494;&#22937;&#24046;&#21035;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#26368;&#36817;&#25552;&#20986;&#30340;&#21551;&#21457;&#33539;&#30068;&#35770;&#30340;&#27169;&#22411;&#65306;Sheaf&#31070;&#32463;&#32593;&#32476;&#12290;Sheaf&#31070;&#32463;&#32593;&#32476;&#21450;&#20854;&#36830;&#25509;&#30340;&#25289;&#26222;&#25289;&#26031;&#21487;&#20197;&#36890;&#36807;&#23558;&#27599;&#20010;&#33410;&#28857;&#65288;&#20197;&#21450;&#36793;&#65289;&#19982;&#21521;&#37327;&#31354;&#38388;&#32780;&#19981;&#26159;&#21333;&#20010;&#21521;&#37327;&#30456;&#20851;&#32852;&#26469;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#12290;&#21521;&#37327;&#31354;&#38388;&#34920;&#31034;&#26356;&#20016;&#23500;&#65292;&#24182;&#20801;&#35768;&#22312;&#25512;&#29702;&#26102;&#36873;&#25321;&#27491;&#30830;&#30340;&#34920;&#31034;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#25105;&#20204;&#30340;&#27169;&#22411;&#26356;&#20855;&#34920;&#29616;&#21147;&#21644;&#28789;&#27963;&#24615;&#65292;&#22312;&#20960;&#20010;&#22522;&#20934;&#25512;&#33616;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent progress in Graph Neural Networks has resulted in wide adoption by many applications, including recommendation systems. The reason for Graph Neural Networks' superiority over other approaches is that many problems in recommendation systems can be naturally modeled as graphs, where nodes can be either users or items and edges represent preference relationships. In current Graph Neural Network approaches, nodes are represented with a static vector learned at training time. This static vector might only be suitable to capture some of the nuances of users or items they define. To overcome this limitation, we propose using a recently proposed model inspired by category theory: Sheaf Neural Networks. Sheaf Neural Networks, and its connected Laplacian, can address the previous problem by associating every node (and edge) with a vector space instead than a single vector. The vector space representation is richer and allows picking the proper representation at inference time. This approa
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#24207;&#21015;&#25512;&#33616;&#30340;&#31561;&#21464;&#24615;&#23545;&#27604;&#23398;&#20064;&#65288;ECL-SR&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26465;&#20214;&#21028;&#21035;&#22120;&#26469;&#20351;&#24471;&#23398;&#20064;&#21040;&#30340;&#29992;&#25143;&#34892;&#20026;&#34920;&#31034;&#23545;&#20110;&#20405;&#20837;&#24615;&#22686;&#24378;&#25935;&#24863;&#24182;&#23545;&#36731;&#24494;&#22686;&#24378;&#19981;&#25935;&#24863;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#24207;&#21015;&#25512;&#33616;&#27169;&#22411;&#30340;&#21306;&#20998;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2211.05290</link><description>&lt;p&gt;
&#24207;&#21015;&#25512;&#33616;&#30340;&#31561;&#21464;&#24615;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Equivariant Contrastive Learning for Sequential Recommendation. (arXiv:2211.05290v3 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.05290
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#24207;&#21015;&#25512;&#33616;&#30340;&#31561;&#21464;&#24615;&#23545;&#27604;&#23398;&#20064;&#65288;ECL-SR&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26465;&#20214;&#21028;&#21035;&#22120;&#26469;&#20351;&#24471;&#23398;&#20064;&#21040;&#30340;&#29992;&#25143;&#34892;&#20026;&#34920;&#31034;&#23545;&#20110;&#20405;&#20837;&#24615;&#22686;&#24378;&#25935;&#24863;&#24182;&#23545;&#36731;&#24494;&#22686;&#24378;&#19981;&#25935;&#24863;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#24207;&#21015;&#25512;&#33616;&#27169;&#22411;&#30340;&#21306;&#20998;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#65288;CL&#65289;&#36890;&#36807;&#20449;&#24687;&#33258;&#30417;&#30563;&#20449;&#21495;&#26377;&#30410;&#20110;&#35757;&#32451;&#24207;&#21015;&#25512;&#33616;&#27169;&#22411;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#37319;&#29992;&#36890;&#29992;&#30340;&#24207;&#21015;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#29983;&#25104;&#27491;&#26679;&#26412;&#65292;&#24182;&#40723;&#21169;&#23427;&#20204;&#30340;&#34920;&#31034;&#20855;&#26377;&#19981;&#21464;&#24615;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#29992;&#25143;&#34892;&#20026;&#24207;&#21015;&#30340;&#22266;&#26377;&#29305;&#24615;&#65292;&#19968;&#20123;&#22686;&#24378;&#31574;&#30053;&#65288;&#22914;&#29289;&#21697;&#26367;&#25442;&#65289;&#21487;&#33021;&#23548;&#33268;&#29992;&#25143;&#24847;&#22270;&#30340;&#25913;&#21464;&#12290;&#20026;&#20102;&#36991;&#20813;&#19981;&#36873;&#23450;&#36866;&#25152;&#26377;&#22686;&#24378;&#31574;&#30053;&#30340;&#19981;&#21464;&#34920;&#31034;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24207;&#21015;&#25512;&#33616;&#30340;&#31561;&#21464;&#24615;&#23545;&#27604;&#23398;&#20064;&#65288;ECL-SR&#65289;&#65292;&#35813;&#26041;&#27861;&#36171;&#20104;SR&#27169;&#22411;&#24378;&#22823;&#30340;&#21306;&#20998;&#33021;&#21147;&#65292;&#20351;&#23398;&#20064;&#21040;&#30340;&#29992;&#25143;&#34892;&#20026;&#34920;&#31034;&#23545;&#20405;&#20837;&#24615;&#22686;&#24378;&#65288;&#22914;&#29289;&#21697;&#26367;&#25442;&#65289;&#25935;&#24863;&#24182;&#23545;&#36731;&#24494;&#22686;&#24378;&#65288;&#22914;&#29305;&#24449;&#32423;&#20002;&#22833;&#36974;&#34109;&#65289;&#19981;&#25935;&#24863;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#26465;&#20214;&#21028;&#21035;&#22120;&#26469;&#25429;&#25417;&#30001;&#20110;&#29289;&#21697;&#26367;&#25442;&#32780;&#20135;&#29983;&#30340;&#34892;&#20026;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive learning (CL) benefits the training of sequential recommendation models with informative self-supervision signals. Existing solutions apply general sequential data augmentation strategies to generate positive pairs and encourage their representations to be invariant. However, due to the inherent properties of user behavior sequences, some augmentation strategies, such as item substitution, can lead to changes in user intent. Learning indiscriminately invariant representations for all augmentation strategies might be suboptimal. Therefore, we propose Equivariant Contrastive Learning for Sequential Recommendation (ECL-SR), which endows SR models with great discriminative power, making the learned user behavior representations sensitive to invasive augmentations (e.g., item substitution) and insensitive to mild augmentations (e.g., featurelevel dropout masking). In detail, we use the conditional discriminator to capture differences in behavior due to item substitution, which e
&lt;/p&gt;</description></item></channel></rss>