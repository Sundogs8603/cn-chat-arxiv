<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22522;&#20110;ID&#30340;&#25512;&#33616;&#30340;&#20849;&#21516;&#28436;&#21270;&#21521;&#37327;&#37327;&#21270;&#26694;&#26550;&#65288;COVE&#65289;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#33258;&#21160;&#23398;&#20064;&#21644;&#29983;&#25104;&#19981;&#21516;&#31890;&#24230;&#32423;&#21035;&#19979;&#30340;&#23454;&#20307;&#20998;&#31867;&#20449;&#24687;&#65292;&#24182;&#22312;&#21508;&#31181;&#25512;&#33616;&#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.16761</link><description>&lt;p&gt;
&#22522;&#20110;ID&#30340;&#25512;&#33616;&#30340;&#20849;&#21516;&#28436;&#21270;&#21521;&#37327;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Co-evolving Vector Quantization for ID-based Recommendation. (arXiv:2308.16761v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16761
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22522;&#20110;ID&#30340;&#25512;&#33616;&#30340;&#20849;&#21516;&#28436;&#21270;&#21521;&#37327;&#37327;&#21270;&#26694;&#26550;&#65288;COVE&#65289;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#33258;&#21160;&#23398;&#20064;&#21644;&#29983;&#25104;&#19981;&#21516;&#31890;&#24230;&#32423;&#21035;&#19979;&#30340;&#23454;&#20307;&#20998;&#31867;&#20449;&#24687;&#65292;&#24182;&#22312;&#21508;&#31181;&#25512;&#33616;&#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31867;&#21035;&#20449;&#24687;&#23545;&#20110;&#25552;&#39640;&#25512;&#33616;&#30340;&#36136;&#37327;&#21644;&#20010;&#24615;&#21270;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#22312;&#22522;&#20110;ID&#30340;&#25512;&#33616;&#20013;&#65292;&#39033;&#30446;&#31867;&#21035;&#20449;&#24687;&#30340;&#21487;&#29992;&#24615;&#24182;&#19981;&#19968;&#33268;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#65292;&#20197;&#33258;&#21160;&#23398;&#20064;&#21644;&#29983;&#25104;&#23454;&#20307;&#65288;&#21363;&#29992;&#25143;&#21644;&#39033;&#30446;&#65289;&#22312;&#19981;&#21516;&#31890;&#24230;&#32423;&#21035;&#19978;&#30340;&#20998;&#31867;&#20449;&#24687;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#22522;&#20110;ID&#30340;&#25512;&#33616;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#20849;&#21516;&#28436;&#21270;&#21521;&#37327;&#37327;&#21270;&#26694;&#26550;&#65292;&#21363;COVE&#65292;&#23427;&#33021;&#22815;&#21516;&#26102;&#23398;&#20064;&#21644;&#25913;&#36827;&#20195;&#30721;&#34920;&#31034;&#21644;&#23454;&#20307;&#23884;&#20837;&#65292;&#24182;&#20197;&#20174;&#38543;&#26426;&#21021;&#22987;&#21270;&#29366;&#24577;&#24320;&#22987;&#30340;&#31471;&#21040;&#31471;&#26041;&#24335;&#36827;&#34892;&#12290;&#36890;&#36807;&#20854;&#39640;&#24230;&#36866;&#24212;&#24615;&#65292;COVE&#21487;&#20197;&#36731;&#26494;&#38598;&#25104;&#21040;&#29616;&#26377;&#30340;&#25512;&#33616;&#27169;&#22411;&#20013;&#12290;&#25105;&#20204;&#39564;&#35777;&#20102;COVE&#22312;&#21508;&#31181;&#25512;&#33616;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;&#21015;&#34920;&#23436;&#25104;&#12289;&#21327;&#21516;&#36807;&#28388;&#21644;&#28857;&#20987;&#29575;&#39044;&#27979;&#65292;&#28085;&#30422;&#19981;&#21516;&#30340;&#25512;&#33616;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Category information plays a crucial role in enhancing the quality and personalization of recommendations. Nevertheless, the availability of item category information is not consistently present, particularly in the context of ID-based recommendations. In this work, we propose an alternative approach to automatically learn and generate entity (i.e., user and item) categorical information at different levels of granularity, specifically for ID-based recommendation. Specifically, we devise a co-evolving vector quantization framework, namely COVE, which enables the simultaneous learning and refinement of code representation and entity embedding in an end-to-end manner, starting from the randomly initialized states. With its high adaptability, COVE can be easily integrated into existing recommendation models. We validate the effectiveness of COVE on various recommendation tasks including list completion, collaborative filtering, and click-through rate prediction, across different recommend
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21512;&#20316;&#19987;&#23478;&#23454;&#29616;&#20102;&#38271;&#23614;&#22270;&#20998;&#31867;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#22788;&#29702;&#22270;&#25968;&#25454;&#19978;&#30340;&#19981;&#36275;&#12290;</title><link>http://arxiv.org/abs/2308.16609</link><description>&lt;p&gt;
&#36890;&#36807;&#21512;&#20316;&#19987;&#23478;&#23454;&#29616;&#38271;&#23614;&#22270;&#20998;&#31867;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Long-Tailed Recognition for Graph Classification via Collaborative Experts. (arXiv:2308.16609v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16609
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21512;&#20316;&#19987;&#23478;&#23454;&#29616;&#20102;&#38271;&#23614;&#22270;&#20998;&#31867;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#22788;&#29702;&#22270;&#25968;&#25454;&#19978;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20998;&#31867;&#26088;&#22312;&#23398;&#20064;&#29992;&#20110;&#26377;&#25928;&#31867;&#21035;&#20998;&#37197;&#30340;&#22270;&#32423;&#34920;&#31034;&#65292;&#22312;&#24179;&#34913;&#30340;&#31867;&#21035;&#20998;&#24067;&#30340;&#39640;&#36136;&#37327;&#25968;&#25454;&#38598;&#30340;&#25903;&#25345;&#19979;&#21462;&#24471;&#20102;&#26480;&#20986;&#25104;&#26524;&#12290;&#20107;&#23454;&#19978;&#65292;&#22823;&#22810;&#25968;&#29616;&#23454;&#19990;&#30028;&#30340;&#22270;&#25968;&#25454;&#33258;&#28982;&#21576;&#29616;&#38271;&#23614;&#24418;&#24335;&#65292;&#20854;&#20013;&#22836;&#37096;&#31867;&#21035;&#30340;&#26679;&#26412;&#25968;&#37327;&#36828;&#36229;&#36807;&#23614;&#37096;&#31867;&#21035;&#65292;&#22240;&#27492;&#22312;&#38271;&#23614;&#25968;&#25454;&#19978;&#30740;&#31350;&#22270;&#32423;&#20998;&#31867;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#20294;&#20173;&#28982;&#36739;&#23569;&#25506;&#32034;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#35270;&#35273;&#20013;&#30340;&#38271;&#23614;&#23398;&#20064;&#26041;&#27861;&#22823;&#22810;&#26080;&#27861;&#21516;&#26102;&#20248;&#21270;&#34920;&#31034;&#23398;&#20064;&#21644;&#20998;&#31867;&#22120;&#35757;&#32451;&#65292;&#24182;&#19988;&#24573;&#30053;&#20102;&#38590;&#20197;&#20998;&#31867;&#30340;&#31867;&#21035;&#30340;&#25366;&#25496;&#12290;&#30452;&#25509;&#23558;&#29616;&#26377;&#26041;&#27861;&#24212;&#29992;&#20110;&#22270;&#21487;&#33021;&#23548;&#33268;&#27425;&#20248;&#24615;&#33021;&#65292;&#22240;&#20026;&#22312;&#22270;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#30001;&#20110;&#22797;&#26434;&#30340;&#25299;&#25169;&#29305;&#24449;&#20250;&#26356;&#21152;&#25935;&#24863;&#20110;&#38271;&#23614;&#20998;&#24067;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#38271;&#23614;&#22270;&#32423;&#20998;&#31867;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Graph classification, aiming at learning the graph-level representations for effective class assignments, has received outstanding achievements, which heavily relies on high-quality datasets that have balanced class distribution. In fact, most real-world graph data naturally presents a long-tailed form, where the head classes occupy much more samples than the tail classes, it thus is essential to study the graph-level classification over long-tailed data while still remaining largely unexplored. However, most existing long-tailed learning methods in visions fail to jointly optimize the representation learning and classifier training, as well as neglect the mining of the hard-to-classify classes. Directly applying existing methods to graphs may lead to sub-optimal performance, since the model trained on graphs would be more sensitive to the long-tailed distribution due to the complex topological characteristics. Hence, in this paper, we propose a novel long-tailed graph-level classifica
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#25512;&#33616;&#20013;&#30340;&#34920;&#36798;&#33021;&#21147;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#21457;&#29616;&#29616;&#26377;&#30340;&#34920;&#36798;&#33021;&#21147;&#24230;&#37327;&#26631;&#20934;&#21487;&#33021;&#26080;&#27861;&#26377;&#25928;&#35780;&#20272;&#27169;&#22411;&#22312;&#25512;&#33616;&#20013;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#29702;&#35770;&#20998;&#26512;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.11127</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#25512;&#33616;&#20013;&#30340;&#34920;&#36798;&#33021;&#21147;&#26377;&#22810;&#24378;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Expressive are Graph Neural Networks in Recommendation?. (arXiv:2308.11127v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11127
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#25512;&#33616;&#20013;&#30340;&#34920;&#36798;&#33021;&#21147;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#21457;&#29616;&#29616;&#26377;&#30340;&#34920;&#36798;&#33021;&#21147;&#24230;&#37327;&#26631;&#20934;&#21487;&#33021;&#26080;&#27861;&#26377;&#25928;&#35780;&#20272;&#27169;&#22411;&#22312;&#25512;&#33616;&#20013;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#29702;&#35770;&#20998;&#26512;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#21508;&#31181;&#22270;&#23398;&#20064;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#21033;&#29992;&#22270;&#20013;&#30340;&#29992;&#25143;-&#29289;&#21697;&#21327;&#20316;&#36807;&#28388;&#20449;&#21495;&#36827;&#34892;&#25512;&#33616;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#23427;&#20204;&#22312;&#26368;&#20808;&#36827;&#30340;&#25512;&#33616;&#27169;&#22411;&#20013;&#30340;&#32463;&#39564;&#26377;&#25928;&#24615;&#65292;&#20294;&#23545;&#20110;&#23427;&#20204;&#30340;&#33021;&#21147;&#30340;&#29702;&#35770;&#34920;&#36848;&#38750;&#24120;&#31232;&#23569;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;GNNs&#30340;&#19968;&#33324;&#34920;&#36798;&#33021;&#21147;&#65292;&#35777;&#26126;&#20102;&#28040;&#24687;&#20256;&#36882;GNNs&#33267;&#22810;&#19982;Weisfeiler-Lehman&#27979;&#35797;&#19968;&#26679;&#24378;&#22823;&#65292;&#24182;&#19988;&#19982;&#38543;&#26426;&#33410;&#28857;&#21021;&#22987;&#21270;&#30456;&#32467;&#21512;&#30340;GNNs&#26159;&#36890;&#29992;&#30340;&#12290;&#28982;&#32780;&#65292;GNNs&#30340;&#8220;&#34920;&#36798;&#33021;&#21147;&#8221;&#27010;&#24565;&#20173;&#28982;&#23450;&#20041;&#27169;&#31946;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#24037;&#20316;&#37319;&#29992;&#22270;&#21516;&#26500;&#27979;&#35797;&#20316;&#20026;&#34920;&#36798;&#33021;&#21147;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#20294;&#36825;&#31181;&#22270;&#32423;&#20219;&#21153;&#21487;&#33021;&#19981;&#33021;&#26377;&#25928;&#35780;&#20272;&#27169;&#22411;&#22312;&#25512;&#33616;&#20013;&#21306;&#20998;&#19981;&#21516;&#25509;&#36817;&#31243;&#24230;&#33410;&#28857;&#30340;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;GNNs&#22312;&#25512;&#33616;&#20013;&#30340;&#34920;&#36798;&#33021;&#21147;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#29702;&#35770;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have demonstrated superior performance on various graph learning tasks, including recommendation, where they leverage user-item collaborative filtering signals in graphs. However, theoretical formulations of their capability are scarce, despite their empirical effectiveness in state-of-the-art recommender models. Recently, research has explored the expressiveness of GNNs in general, demonstrating that message passing GNNs are at most as powerful as the Weisfeiler-Lehman test, and that GNNs combined with random node initialization are universal. Nevertheless, the concept of "expressiveness" for GNNs remains vaguely defined. Most existing works adopt the graph isomorphism test as the metric of expressiveness, but this graph-level task may not effectively assess a model's ability in recommendation, where the objective is to distinguish nodes of different closeness. In this paper, we provide a comprehensive theoretical analysis of the expressiveness of GNNs in 
&lt;/p&gt;</description></item><item><title>AutoAlign&#26159;&#19968;&#31181;&#20840;&#33258;&#21160;&#30340;&#30693;&#35782;&#22270;&#35889;&#23545;&#40784;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#25163;&#24037;&#21046;&#20316;&#30340;&#31181;&#23376;&#23545;&#40784;&#12290;&#23427;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#25429;&#25417;&#35859;&#35789;&#30456;&#20284;&#24615;&#65292;&#24182;&#20351;&#29992;TransE&#35745;&#31639;&#23454;&#20307;&#23884;&#20837;&#26469;&#23454;&#29616;&#23454;&#20307;&#23545;&#40784;&#12290;</title><link>http://arxiv.org/abs/2307.11772</link><description>&lt;p&gt;
AutoAlign&#65306;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20840;&#33258;&#21160;&#26377;&#25928;&#30693;&#35782;&#22270;&#35889;&#23545;&#40784;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
AutoAlign: Fully Automatic and Effective Knowledge Graph Alignment enabled by Large Language Models. (arXiv:2307.11772v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11772
&lt;/p&gt;
&lt;p&gt;
AutoAlign&#26159;&#19968;&#31181;&#20840;&#33258;&#21160;&#30340;&#30693;&#35782;&#22270;&#35889;&#23545;&#40784;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#25163;&#24037;&#21046;&#20316;&#30340;&#31181;&#23376;&#23545;&#40784;&#12290;&#23427;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#25429;&#25417;&#35859;&#35789;&#30456;&#20284;&#24615;&#65292;&#24182;&#20351;&#29992;TransE&#35745;&#31639;&#23454;&#20307;&#23884;&#20837;&#26469;&#23454;&#29616;&#23454;&#20307;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#38388;&#30340;&#23454;&#20307;&#23545;&#40784;&#20219;&#21153;&#26088;&#22312;&#35782;&#21035;&#20986;&#20004;&#20010;&#19981;&#21516;&#30693;&#35782;&#22270;&#35889;&#20013;&#34920;&#31034;&#30456;&#21516;&#23454;&#20307;&#30340;&#27599;&#23545;&#23454;&#20307;&#12290;&#35768;&#22810;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#24050;&#34987;&#25552;&#20986;&#29992;&#20110;&#36825;&#20010;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#37117;&#38656;&#35201;&#25163;&#24037;&#21046;&#20316;&#30340;&#31181;&#23376;&#23545;&#40784;&#65292;&#36825;&#26159;&#38750;&#24120;&#26114;&#36149;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#21517;&#20026;AutoAlign&#30340;&#23436;&#20840;&#33258;&#21160;&#23545;&#40784;&#26041;&#27861;&#65292;&#23427;&#19981;&#38656;&#35201;&#20219;&#20309;&#25163;&#24037;&#21046;&#20316;&#30340;&#31181;&#23376;&#23545;&#40784;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23545;&#20110;&#35859;&#35789;&#23884;&#20837;&#65292;AutoAlign&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#35859;&#35789;&#36817;&#37051;&#22270;&#65292;&#33258;&#21160;&#25429;&#25417;&#20004;&#20010;&#30693;&#35782;&#22270;&#35889;&#20013;&#35859;&#35789;&#30340;&#30456;&#20284;&#24615;&#12290;&#23545;&#20110;&#23454;&#20307;&#23884;&#20837;&#65292;AutoAlign&#39318;&#20808;&#20351;&#29992;TransE&#29420;&#31435;&#35745;&#31639;&#27599;&#20010;&#30693;&#35782;&#22270;&#35889;&#30340;&#23454;&#20307;&#23884;&#20837;&#65292;&#28982;&#21518;&#36890;&#36807;&#35745;&#31639;&#22522;&#20110;&#23454;&#20307;&#23646;&#24615;&#30340;&#23454;&#20307;&#30456;&#20284;&#24615;&#65292;&#23558;&#20004;&#20010;&#30693;&#35782;&#22270;&#35889;&#30340;&#23454;&#20307;&#23884;&#20837;&#31227;&#21160;&#21040;&#30456;&#21516;&#30340;&#21521;&#37327;&#31354;&#38388;&#20013;&#12290;&#22240;&#27492;&#65292;AutoAlign&#23454;&#29616;&#20102;&#35859;&#35789;&#23545;&#40784;&#21644;&#23454;&#20307;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
The task of entity alignment between knowledge graphs (KGs) aims to identify every pair of entities from two different KGs that represent the same entity. Many machine learning-based methods have been proposed for this task. However, to our best knowledge, existing methods all require manually crafted seed alignments, which are expensive to obtain. In this paper, we propose the first fully automatic alignment method named AutoAlign, which does not require any manually crafted seed alignments. Specifically, for predicate embeddings, AutoAlign constructs a predicate-proximity-graph with the help of large language models to automatically capture the similarity between predicates across two KGs. For entity embeddings, AutoAlign first computes the entity embeddings of each KG independently using TransE, and then shifts the two KGs' entity embeddings into the same vector space by computing the similarity between entities based on their attributes. Thus, both predicate alignment and entity al
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26816;&#32034;&#22686;&#24378;&#30340;GPT-3.5&#25991;&#26412;&#21040;SQL&#26694;&#26550;&#65292;&#37319;&#29992;&#20102;&#26679;&#26412;&#24863;&#30693;&#24341;&#23548;&#21644;&#21160;&#24577;&#20462;&#35746;&#38142;&#30340;&#26041;&#27861;&#65292;&#20197;&#24212;&#23545;&#29616;&#26377;&#26041;&#27861;&#22312;&#22788;&#29702;&#35821;&#20041;&#24046;&#36317;&#36739;&#22823;&#30340;&#26816;&#32034;&#31034;&#20363;&#26102;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2307.05074</link><description>&lt;p&gt;
&#37319;&#29992;&#26679;&#26412;&#24863;&#30693;&#24341;&#23548;&#21644;&#21160;&#24577;&#20462;&#35746;&#38142;&#30340;&#22522;&#20110;&#26816;&#32034;&#22686;&#24378;&#30340;GPT-3.5&#25991;&#26412;&#21040;SQL&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Retrieval-augmented GPT-3.5-based Text-to-SQL Framework with Sample-aware Prompting and Dynamic Revision Chain. (arXiv:2307.05074v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05074
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26816;&#32034;&#22686;&#24378;&#30340;GPT-3.5&#25991;&#26412;&#21040;SQL&#26694;&#26550;&#65292;&#37319;&#29992;&#20102;&#26679;&#26412;&#24863;&#30693;&#24341;&#23548;&#21644;&#21160;&#24577;&#20462;&#35746;&#38142;&#30340;&#26041;&#27861;&#65292;&#20197;&#24212;&#23545;&#29616;&#26377;&#26041;&#27861;&#22312;&#22788;&#29702;&#35821;&#20041;&#24046;&#36317;&#36739;&#22823;&#30340;&#26816;&#32034;&#31034;&#20363;&#26102;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;SQL&#26088;&#22312;&#20026;&#32473;&#23450;&#30340;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#29983;&#25104;SQL&#26597;&#35810;&#65292;&#20174;&#32780;&#24110;&#21161;&#29992;&#25143;&#26597;&#35810;&#25968;&#25454;&#24211;&#12290;&#26368;&#36817;&#20986;&#29616;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#35774;&#35745;&#25552;&#31034;&#20197;&#24341;&#23548;LLMs&#29702;&#35299;&#36755;&#20837;&#38382;&#39064;&#24182;&#29983;&#25104;&#30456;&#24212;&#30340;SQL&#12290;&#28982;&#32780;&#65292;&#23427;&#38754;&#20020;&#30528;&#20005;&#26684;&#30340;SQL&#35821;&#27861;&#35201;&#27714;&#30340;&#25361;&#25112;&#12290;&#29616;&#26377;&#24037;&#20316;&#20351;&#29992;&#19968;&#31995;&#21015;&#31034;&#20363;&#65288;&#21363;&#38382;&#39064;-SQL&#23545;&#65289;&#26469;&#25552;&#31034;LLMs&#29983;&#25104;SQL&#65292;&#20294;&#22266;&#23450;&#30340;&#25552;&#31034;&#20960;&#20046;&#26080;&#27861;&#22788;&#29702;&#26816;&#32034;&#20986;&#30340;&#31034;&#20363;&#19982;&#36755;&#20837;&#38382;&#39064;&#20043;&#38388;&#30340;&#35821;&#20041;&#24046;&#36317;&#36739;&#22823;&#30340;&#24773;&#20917;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26816;&#32034;&#22686;&#24378;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#29992;&#20110;&#22522;&#20110;LLM&#30340;&#25991;&#26412;&#21040;SQL&#26694;&#26550;&#65292;&#21253;&#25324;&#26679;&#26412;&#24863;&#30693;&#25552;&#31034;&#21644;&#21160;&#24577;&#20462;&#35746;&#38142;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#26679;&#26412;&#24863;&#30693;&#31034;&#20363;&#65292;&#20854;&#20013;&#21253;&#25324;SQL&#36816;&#31639;&#31526;&#30340;&#32452;&#21512;&#21644;&#19982;&#32473;&#23450;&#38382;&#39064;&#30456;&#20851;&#30340;&#32454;&#31890;&#24230;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-SQL aims at generating SQL queries for the given natural language questions and thus helping users to query databases. Prompt learning with large language models (LLMs) has emerged as a recent approach, which designs prompts to lead LLMs to understand the input question and generate the corresponding SQL. However, it faces challenges with strict SQL syntax requirements. Existing work prompts the LLMs with a list of demonstration examples (i.e. question-SQL pairs) to generate SQL, but the fixed prompts can hardly handle the scenario where the semantic gap between the retrieved demonstration and the input question is large. In this paper, we propose a retrieval-augmented prompting method for a LLM-based Text-to-SQL framework, involving sample-aware prompting and a dynamic revision chain. Our approach incorporates sample-aware demonstrations, which include the composition of SQL operators and fine-grained information related to the given question. To retrieve questions sharing sim
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22810;&#27169;&#24577;&#21307;&#23398;&#25968;&#25454;&#34701;&#21512;&#22312;&#26234;&#24935;&#21307;&#30103;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#31526;&#21512;DIKW&#26426;&#21046;&#30340;&#36890;&#29992;&#34701;&#21512;&#26694;&#26550;&#65292;&#25506;&#35752;&#20102;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2306.11963</link><description>&lt;p&gt;
&#26234;&#24935;&#21307;&#30103;&#20013;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;:&#20174;&#25968;&#25454;&#12289;&#20449;&#24687;&#12289;&#30693;&#35782;&#21040;&#26234;&#24935;&#20043;&#26053;
&lt;/p&gt;
&lt;p&gt;
Multimodality Fusion for Smart Healthcare: a Journey from Data, Information, Knowledge to Wisdom. (arXiv:2306.11963v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11963
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22810;&#27169;&#24577;&#21307;&#23398;&#25968;&#25454;&#34701;&#21512;&#22312;&#26234;&#24935;&#21307;&#30103;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#31526;&#21512;DIKW&#26426;&#21046;&#30340;&#36890;&#29992;&#34701;&#21512;&#26694;&#26550;&#65292;&#25506;&#35752;&#20102;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#21307;&#23398;&#25968;&#25454;&#34701;&#21512;&#24050;&#25104;&#20026;&#26234;&#24935;&#21307;&#30103;&#20013;&#30340;&#19968;&#31181;&#38761;&#26032;&#24615;&#26041;&#27861;&#65292;&#33021;&#22815;&#20840;&#38754;&#20102;&#35299;&#24739;&#32773;&#20581;&#24247;&#29366;&#20917;&#21644;&#20010;&#24615;&#21270;&#27835;&#30103;&#26041;&#26696;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22810;&#27169;&#24577;&#34701;&#21512;&#20026;&#26234;&#24935;&#21307;&#30103;&#24102;&#26469;&#30340;&#20174;&#25968;&#25454;&#12289;&#20449;&#24687;&#21644;&#30693;&#35782;&#21040;&#26234;&#24935;&#65288;DIKW&#65289;&#20043;&#26053;&#12290;&#20840;&#38754;&#22238;&#39038;&#20102;&#22810;&#27169;&#24577;&#21307;&#23398;&#25968;&#25454;&#34701;&#21512;&#30340;&#30740;&#31350;&#29616;&#29366;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#19981;&#21516;&#25968;&#25454;&#27169;&#24577;&#30340;&#38598;&#25104;&#26041;&#24335;&#12290;&#25991;&#31456;&#25506;&#35752;&#20102;&#29305;&#24449;&#36873;&#25321;&#12289;&#22522;&#20110;&#35268;&#21017;&#30340;&#31995;&#32479;&#12289;&#26426;&#22120;&#23398;&#20064;&#12289;&#28145;&#24230;&#23398;&#20064;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#19981;&#21516;&#26041;&#27861;&#65292;&#29992;&#20110;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#34701;&#21512;&#21644;&#20998;&#26512;&#12290;&#21516;&#26102;&#65292;&#25991;&#31456;&#20063;&#30528;&#37325;&#35752;&#35770;&#20102;&#22810;&#27169;&#24577;&#34701;&#21512;&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#32508;&#21512;&#35780;&#36848;&#30340;&#26694;&#26550;&#21644;&#35265;&#35299;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#31526;&#21512;DIKW&#26426;&#21046;&#30340;&#36890;&#29992;&#22810;&#27169;&#24577;&#21307;&#30103;&#25968;&#25454;&#34701;&#21512;&#26694;&#26550;&#12290;&#27492;&#22806;&#65292;&#25991;&#31456;&#36824;&#25506;&#35752;&#20102;&#26410;&#26469;&#19982;&#39044;&#27979;&#12289;&#39044;&#38450;&#12289;&#20010;&#24615;&#21270;&#21644;&#27835;&#30103;&#26377;&#20851;&#30340;&#21307;&#30103;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal medical data fusion has emerged as a transformative approach in smart healthcare, enabling a comprehensive understanding of patient health and personalized treatment plans. In this paper, a journey from data, information, and knowledge to wisdom (DIKW) is explored through multimodal fusion for smart healthcare. A comprehensive review of multimodal medical data fusion focuses on the integration of various data modalities are presented. It explores different approaches such as Feature selection, Rule-based systems, Machine learning, Deep learning, and Natural Language Processing for fusing and analyzing multimodal data. The paper also highlights the challenges associated with multimodal fusion in healthcare. By synthesizing the reviewed frameworks and insights, a generic framework for multimodal medical data fusion is proposed while aligning with the DIKW mechanism. Moreover, it discusses future directions aligned with the four pillars of healthcare: Predictive, Preventive, Pe
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31038;&#20132;&#24863;&#30693;&#21644;&#26102;&#38388;&#22240;&#32032;&#30340;&#35299;&#30721;&#22120;&#25512;&#33616;&#31995;&#32479;(STUDY)&#65292;&#20351;&#29992;transformer&#35299;&#30721;&#22120;&#32593;&#32476;&#23454;&#29616;&#23545;&#31038;&#20132;&#32593;&#32476;&#22270;&#20013;&#30456;&#37051;&#30340;&#29992;&#25143;&#32452;&#30340;&#32852;&#21512;&#25512;&#26029;&#12290;&#35813;&#26041;&#27861;&#22312;&#25945;&#32946;&#20869;&#23481;&#39046;&#22495;&#20013;&#32463;&#36807;&#27979;&#35797;&#65292;&#33021;&#22815;&#21462;&#24471;&#20248;&#20110;&#31038;&#20132;&#21644;&#39034;&#24207;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.07946</link><description>&lt;p&gt;
&#30740;&#31350;&#65306;&#31038;&#20132;&#24863;&#30693;&#26102;&#38388;&#26494;&#25955;&#35299;&#30721;&#22120;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
STUDY: Socially Aware Temporally Casual Decoder Recommender Systems. (arXiv:2306.07946v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07946
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31038;&#20132;&#24863;&#30693;&#21644;&#26102;&#38388;&#22240;&#32032;&#30340;&#35299;&#30721;&#22120;&#25512;&#33616;&#31995;&#32479;(STUDY)&#65292;&#20351;&#29992;transformer&#35299;&#30721;&#22120;&#32593;&#32476;&#23454;&#29616;&#23545;&#31038;&#20132;&#32593;&#32476;&#22270;&#20013;&#30456;&#37051;&#30340;&#29992;&#25143;&#32452;&#30340;&#32852;&#21512;&#25512;&#26029;&#12290;&#35813;&#26041;&#27861;&#22312;&#25945;&#32946;&#20869;&#23481;&#39046;&#22495;&#20013;&#32463;&#36807;&#27979;&#35797;&#65292;&#33021;&#22815;&#21462;&#24471;&#20248;&#20110;&#31038;&#20132;&#21644;&#39034;&#24207;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29616;&#22312;&#22312;&#32447;&#21644;&#31163;&#32447;&#21487;&#33719;&#21462;&#30340;&#25968;&#25454;&#25968;&#37327;&#36807;&#20110;&#24222;&#22823;&#65292;&#25512;&#33616;&#31995;&#32479;&#21464;&#24471;&#36234;&#26469;&#36234;&#24517;&#35201;&#65292;&#20197;&#24110;&#21161;&#29992;&#25143;&#25214;&#21040;&#31526;&#21512;&#20182;&#20204;&#20852;&#36259;&#30340;&#29289;&#21697;&#12290;&#24403;&#31038;&#20132;&#32593;&#32476;&#20449;&#24687;&#23384;&#22312;&#26102;&#65292;&#26377;&#19968;&#20123;&#26041;&#27861;&#21033;&#29992;&#36825;&#20123;&#20449;&#24687;&#26469;&#20570;&#20986;&#26356;&#22909;&#30340;&#25512;&#33616;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#26377;&#22797;&#26434;&#30340;&#32467;&#26500;&#21644;&#35757;&#32451;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;&#35768;&#22810;&#29616;&#26377;&#30340;&#26041;&#27861;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#32780;&#36825;&#20123;&#32593;&#32476;&#35757;&#32451;&#36215;&#26469;&#38750;&#24120;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#31038;&#20132;&#24863;&#30693;&#21644;&#26102;&#38388;&#22240;&#32032;&#30340;&#35299;&#30721;&#22120;&#25512;&#33616;&#31995;&#32479;(STUDY)&#12290;STUDY&#37319;&#29992;&#19968;&#20010;&#32463;&#36807;&#20462;&#25913;&#30340;transformer&#35299;&#30721;&#22120;&#32593;&#32476;&#30340;&#21333;&#21521;&#21069;&#20256;&#65292;&#23545;&#31038;&#20132;&#32593;&#32476;&#22270;&#20013;&#30456;&#37051;&#30340;&#29992;&#25143;&#32452;&#36827;&#34892;&#32852;&#21512;&#25512;&#26029;&#12290;&#25105;&#20204;&#22312;&#22522;&#20110;&#23398;&#26657;&#35838;&#22530;&#32467;&#26500;&#23450;&#20041;&#31038;&#20132;&#32593;&#32476;&#30340;&#25945;&#32946;&#20869;&#23481;&#39046;&#22495;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20445;&#25345;&#21333;&#19968;&#22343;&#21248;&#32593;&#32476;&#35774;&#35745;&#31616;&#21333;&#24615;&#30340;&#21516;&#26102;&#65292;&#20248;&#20110;&#31038;&#20132;&#21644;&#39034;&#24207;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the overwhelming amount of data available both on and offline today, recommender systems have become much needed to help users find items tailored to their interests. When social network information exists there are methods that utilize this information to make better recommendations, however the methods are often clunky with complex architectures and training procedures. Furthermore many of the existing methods utilize graph neural networks which are notoriously difficult to train. To address this, we propose Socially-aware Temporally caUsal Decoder recommender sYstems (STUDY). STUDY does joint inference over groups of users who are adjacent in the social network graph using a single forward pass of a modified transformer decoder network. We test our method in a school-based educational content setting, using classroom structure to define social networks. Our method outperforms both social and sequential methods while maintaining the design simplicity of a single homogeneous netw
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;CTRL&#26694;&#26550;&#65292;&#23558;&#21407;&#22987;&#34920;&#26684;&#25968;&#25454;&#36716;&#25442;&#20026;&#25991;&#26412;&#25968;&#25454;&#65292;&#20351;&#29992;&#21327;&#20316;CTR&#27169;&#22411;&#20998;&#21035;&#23545;&#20004;&#31181;&#25968;&#25454;&#36827;&#34892;&#24314;&#27169;&#65292;&#25552;&#21462;&#20851;&#20110;CTR&#39044;&#27979;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#24182;&#22312;&#30495;&#23454;&#24037;&#19994;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#26368;&#26032;&#30340;SOTA&#24615;&#33021;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2306.02841</link><description>&lt;p&gt;
CTRL: &#36830;&#25509;&#34920;&#26684;&#21644;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;CTR&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
CTRL: Connect Tabular and Language Model for CTR Prediction. (arXiv:2306.02841v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02841
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;CTRL&#26694;&#26550;&#65292;&#23558;&#21407;&#22987;&#34920;&#26684;&#25968;&#25454;&#36716;&#25442;&#20026;&#25991;&#26412;&#25968;&#25454;&#65292;&#20351;&#29992;&#21327;&#20316;CTR&#27169;&#22411;&#20998;&#21035;&#23545;&#20004;&#31181;&#25968;&#25454;&#36827;&#34892;&#24314;&#27169;&#65292;&#25552;&#21462;&#20851;&#20110;CTR&#39044;&#27979;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#24182;&#22312;&#30495;&#23454;&#24037;&#19994;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#26368;&#26032;&#30340;SOTA&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;CTR&#39044;&#27979;&#27169;&#22411;&#23558;&#34920;&#26684;&#25968;&#25454;&#36716;&#25442;&#20026;one-hot&#21521;&#37327;&#65292;&#24182;&#21033;&#29992;&#29305;&#24449;&#20043;&#38388;&#30340;&#21327;&#20316;&#20851;&#31995;&#26469;&#25512;&#26029;&#29992;&#25143;&#23545;&#39033;&#30446;&#30340;&#20559;&#22909;&#12290;&#36825;&#31181;&#24314;&#27169;&#33539;&#24335;&#25243;&#24323;&#20102;&#22522;&#26412;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;&#23613;&#31649;&#19968;&#20123;&#26368;&#36817;&#30340;&#24037;&#20316;&#65288;&#22914;P5&#21644;M6-Rec&#65289;&#24050;&#32463;&#25506;&#32034;&#20102;&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#25552;&#21462;CTR&#39044;&#27979;&#30340;&#35821;&#20041;&#20449;&#21495;&#30340;&#28508;&#21147;&#65292;&#20294;&#23427;&#20204;&#35745;&#31639;&#25104;&#26412;&#39640;&#65292;&#25928;&#29575;&#20302;&#12290;&#27492;&#22806;&#65292;&#23578;&#26410;&#32771;&#34385;&#21040;&#26377;&#30410;&#30340;&#21327;&#20316;&#20851;&#31995;&#65292;&#20174;&#32780;&#38459;&#30861;&#20102;&#25512;&#33616;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;CTRL&#65292;&#23427;&#26159;&#24037;&#19994;&#21451;&#22909;&#30340;&#21644;&#27169;&#22411;&#19981;&#21487;&#30693;&#30340;&#65292;&#20855;&#26377;&#39640;&#35757;&#32451;&#21644;&#25512;&#29702;&#25928;&#29575;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#21407;&#22987;&#30340;&#34920;&#26684;&#25968;&#25454;&#39318;&#20808;&#34987;&#36716;&#25442;&#20026;&#25991;&#26412;&#25968;&#25454;&#12290;&#20004;&#31181;&#19981;&#21516;&#30340;&#27169;&#24577;&#34987;&#20998;&#21035;&#35270;&#20026;&#20004;&#20010;&#27169;&#24577;&#65292;&#24182;&#20998;&#21035;&#36755;&#20837;&#21327;&#20316;CTR&#27169;&#22411;&#20013;&#20197;&#24314;&#27169;&#23427;&#20204;&#30340;&#20132;&#20114;&#20316;&#29992;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#20449;&#24687;&#33976;&#39311;&#26426;&#21046;&#65292;&#20174;PLMs&#20013;&#25552;&#21462;&#20851;&#20110;CTR&#39044;&#27979;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#22312;&#19977;&#20010;&#30495;&#23454;&#30340;&#24037;&#19994;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#27604;&#36739;&#20854;&#20182;&#29616;&#26377;&#30340;&#27169;&#22411;&#26102;&#22343;&#36798;&#21040;&#20102;&#26368;&#26032;&#30340;SOTA&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional click-through rate (CTR) prediction models convert the tabular data into one-hot vectors and leverage the collaborative relations among features for inferring user's preference over items. This modeling paradigm discards the essential semantic information. Though some recent works like P5 and M6-Rec have explored the potential of using Pre-trained Language Models (PLMs) to extract semantic signals for CTR prediction, they are computationally expensive and suffer from low efficiency. Besides, the beneficial collaborative relations are not considered, hindering the recommendation performance. To solve these problems, in this paper, we propose a novel framework \textbf{CTRL}, which is industrial friendly and model-agnostic with high training and inference efficiency. Specifically, the original tabular data is first converted into textual data. Both tabular data and converted textual data are regarded as two different modalities and are separately fed into the collaborative CTR
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#35821;&#20041;&#22270;&#20013;&#33719;&#21462;&#21487;&#35299;&#37322;&#21644;&#40065;&#26834;&#30340;NLI&#24230;&#37327;&#26041;&#27861;&#65292;&#19982;&#20351;&#29992;&#19978;&#19979;&#25991;&#23884;&#20837;&#30340;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#34917;&#20805;&#24615;&#65292;&#21487;&#20197;&#22312;&#28151;&#21512;&#27169;&#22411;&#20013;&#32467;&#21512;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2306.00936</link><description>&lt;p&gt;
AMR4NLI: &#20174;&#35821;&#20041;&#22270;&#20013;&#33719;&#24471;&#21487;&#35299;&#37322;&#21644;&#40065;&#26834;&#30340;NLI&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
AMR4NLI: Interpretable and robust NLI measures from semantic graphs. (arXiv:2306.00936v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00936
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#35821;&#20041;&#22270;&#20013;&#33719;&#21462;&#21487;&#35299;&#37322;&#21644;&#40065;&#26834;&#30340;NLI&#24230;&#37327;&#26041;&#27861;&#65292;&#19982;&#20351;&#29992;&#19978;&#19979;&#25991;&#23884;&#20837;&#30340;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#34917;&#20805;&#24615;&#65292;&#21487;&#20197;&#22312;&#28151;&#21512;&#27169;&#22411;&#20013;&#32467;&#21512;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#20219;&#21153;&#35201;&#27714;&#21028;&#26029;&#32473;&#23450;&#30340;&#21069;&#25552;&#65288;&#29992;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#65289;&#26159;&#21542;&#34164;&#21547;&#32473;&#23450;&#30340;&#20551;&#35774;&#12290;NLI&#22522;&#20934;&#21253;&#21547;&#20102;&#34164;&#21547;&#24615;&#30340;&#20154;&#24037;&#35780;&#20998;&#65292;&#20294;&#26159;&#39537;&#21160;&#36825;&#20123;&#35780;&#20998;&#30340;&#24847;&#20041;&#20851;&#31995;&#24182;&#26410;&#24418;&#24335;&#21270;&#12290;&#26159;&#21542;&#21487;&#20197;&#20197;&#19968;&#31181;&#21487;&#35299;&#37322;&#19988;&#40065;&#26834;&#30340;&#26041;&#24335;&#26356;&#26126;&#30830;&#22320;&#34920;&#31034;&#21477;&#23376;&#23545;&#20043;&#38388;&#30340;&#20851;&#31995;&#65311;&#25105;&#20204;&#27604;&#36739;&#20102;&#34920;&#31034;&#21069;&#25552;&#21644;&#20551;&#35774;&#30340;&#35821;&#20041;&#32467;&#26500;&#65292;&#21253;&#25324;&#19968;&#32452;&#19978;&#19979;&#25991;&#21270;&#23884;&#20837;&#21644;&#35821;&#20041;&#22270;&#65288;&#25277;&#35937;&#24847;&#20041;&#34920;&#31034;&#65289;&#65292;&#24182;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#24230;&#37327;&#26041;&#27861;&#26469;&#34913;&#37327;&#20551;&#35774;&#26159;&#21542;&#26159;&#21069;&#25552;&#30340;&#35821;&#20041;&#23376;&#32467;&#26500;&#12290;&#22312;&#19977;&#20010;&#33521;&#35821;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#35780;&#20272;&#21457;&#29616;&#65292;&#19978;&#19979;&#25991;&#21270;&#23884;&#20837;&#21644;&#35821;&#20041;&#22270;&#37117;&#26377;&#20854;&#20215;&#20540;&#65307;&#32780;&#19988;&#23427;&#20204;&#25552;&#20379;&#20102;&#20114;&#34917;&#30340;&#20449;&#21495;&#65292;&#24182;&#21487;&#20197;&#22312;&#28151;&#21512;&#27169;&#22411;&#20013;&#19968;&#36215;&#21033;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The task of natural language inference (NLI) asks whether a given premise (expressed in NL) entails a given NL hypothesis. NLI benchmarks contain human ratings of entailment, but the meaning relationships driving these ratings are not formalized. Can the underlying sentence pair relationships be made more explicit in an interpretable yet robust fashion? We compare semantic structures to represent premise and hypothesis, including sets of contextualized embeddings and semantic graphs (Abstract Meaning Representations), and measure whether the hypothesis is a semantic substructure of the premise, utilizing interpretable metrics. Our evaluation on three English benchmarks finds value in both contextualized embeddings and semantic graphs; moreover, they provide complementary signals, and can be leveraged together in a hybrid model.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34394;&#25311;&#37051;&#23621;&#32593;&#32476;(VNC)&#65292;&#29992;&#20110;&#35299;&#20915;&#30693;&#35782;&#22270;&#35889;&#23436;&#25104;&#20013;&#26410;&#30693;&#23454;&#20307;&#34920;&#31034;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35268;&#21017;&#25366;&#25496;&#12289;&#35268;&#21017;&#25512;&#29702;&#21644;&#23884;&#20837;&#19977;&#20010;&#38454;&#27573;&#65292;&#23454;&#29616;&#23545;&#35268;&#21017;&#38388;&#30456;&#20851;&#24615;&#36827;&#34892;&#24314;&#27169;&#12290;</title><link>http://arxiv.org/abs/2305.10531</link><description>&lt;p&gt;
&#36845;&#20195;&#23398;&#20064;&#20855;&#26377;&#35268;&#21017;&#38388;&#30456;&#20851;&#24615;&#30340;&#26410;&#30693;&#23454;&#20307;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Iteratively Learning Representations for Unseen Entities with Inter-Rule Correlations. (arXiv:2305.10531v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10531
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34394;&#25311;&#37051;&#23621;&#32593;&#32476;(VNC)&#65292;&#29992;&#20110;&#35299;&#20915;&#30693;&#35782;&#22270;&#35889;&#23436;&#25104;&#20013;&#26410;&#30693;&#23454;&#20307;&#34920;&#31034;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35268;&#21017;&#25366;&#25496;&#12289;&#35268;&#21017;&#25512;&#29702;&#21644;&#23884;&#20837;&#19977;&#20010;&#38454;&#27573;&#65292;&#23454;&#29616;&#23545;&#35268;&#21017;&#38388;&#30456;&#20851;&#24615;&#36827;&#34892;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#23436;&#25104;(KGC)&#30340;&#26368;&#26032;&#30740;&#31350;&#20391;&#37325;&#20110;&#23398;&#20064;&#30693;&#35782;&#22270;&#35889;&#20013;&#23454;&#20307;&#21644;&#20851;&#31995;&#30340;&#23884;&#20837;&#12290;&#36825;&#20123;&#23884;&#20837;&#26041;&#27861;&#35201;&#27714;&#25152;&#26377;&#27979;&#35797;&#23454;&#20307;&#22312;&#35757;&#32451;&#26102;&#34987;&#35266;&#23519;&#21040;&#65292;&#23548;&#33268;&#23545;&#36229;&#20986;&#30693;&#35782;&#22270;&#35889;&#65288;OOKG&#65289;&#23454;&#20307;&#30340;&#32791;&#26102;&#37325;&#26032;&#35757;&#32451;&#36807;&#31243;&#12290;&#20026;&#35299;&#20915;&#27492;&#38382;&#39064;&#65292;&#24403;&#21069;&#24402;&#32435;&#30693;&#35782;&#23884;&#20837;&#26041;&#27861;&#37319;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#36890;&#36807;&#32858;&#21512;&#24050;&#30693;&#37051;&#23621;&#30340;&#20449;&#24687;&#26469;&#34920;&#31034;&#26410;&#30693;&#23454;&#20307;&#12290;&#20182;&#20204;&#38754;&#20020;&#19977;&#20010;&#37325;&#35201;&#25361;&#25112;:i)&#25968;&#25454;&#31232;&#30095;&#24615;&#65292;ii)&#30693;&#35782;&#22270;&#35889;&#20013;&#23384;&#22312;&#22797;&#26434;&#27169;&#24335;(&#22914;&#35268;&#21017;&#38388;&#30456;&#20851;&#24615;)&#65292;iii)&#35268;&#21017;&#25366;&#25496;&#12289;&#35268;&#21017;&#25512;&#29702;&#21644;&#23884;&#20837;&#20043;&#38388;&#23384;&#22312;&#20132;&#20114;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#19977;&#20010;&#38454;&#27573;&#30340;&#20855;&#26377;&#35268;&#21017;&#38388;&#30456;&#20851;&#24615;&#30340;&#34394;&#25311;&#37051;&#23621;&#32593;&#32476;(VNC):i)&#35268;&#21017;&#25366;&#25496;&#65292;ii)&#35268;&#21017;&#25512;&#29702;&#65292;&#21644;iii)&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work on knowledge graph completion (KGC) focused on learning embeddings of entities and relations in knowledge graphs. These embedding methods require that all test entities are observed at training time, resulting in a time-consuming retraining process for out-of-knowledge-graph (OOKG) entities. To address this issue, current inductive knowledge embedding methods employ graph neural networks (GNNs) to represent unseen entities by aggregating information of known neighbors. They face three important challenges: (i) data sparsity, (ii) the presence of complex patterns in knowledge graphs (e.g., inter-rule correlations), and (iii) the presence of interactions among rule mining, rule inference, and embedding. In this paper, we propose a virtual neighbor network with inter-rule correlations (VNC) that consists of three stages: (i) rule mining, (ii) rule inference, and (iii) embedding. In the rule mining process, to identify complex patterns in knowledge graphs, both logic rules and 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DELTA&#30340;CTR&#27169;&#22411;&#65292;&#20351;&#29992;&#25130;&#26029;&#24847;&#35782;&#27880;&#24847;&#21147;&#36827;&#34892;&#21160;&#24577;&#23884;&#20837;&#23398;&#20064;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#19978;&#19979;&#25991;&#20013;&#26080;&#25928;&#21644;&#20887;&#20313;&#29305;&#24449;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.04891</link><description>&lt;p&gt;
&#24102;&#26377;&#25130;&#26029;&#24847;&#35782;&#27880;&#24847;&#21147;&#30340;&#21160;&#24577;&#23884;&#20837;&#23398;&#20064;&#27169;&#22411;&#29992;&#20110;CTR&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
DELTA: Dynamic Embedding Learning with Truncated Conscious Attention for CTR Prediction. (arXiv:2305.04891v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04891
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DELTA&#30340;CTR&#27169;&#22411;&#65292;&#20351;&#29992;&#25130;&#26029;&#24847;&#35782;&#27880;&#24847;&#21147;&#36827;&#34892;&#21160;&#24577;&#23884;&#20837;&#23398;&#20064;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#19978;&#19979;&#25991;&#20013;&#26080;&#25928;&#21644;&#20887;&#20313;&#29305;&#24449;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28857;&#20987;&#29575;&#65288;CTR&#65289;&#39044;&#27979;&#26159;&#20135;&#21697;&#21644;&#20869;&#23481;&#25512;&#33616;&#20013;&#20851;&#38190;&#30340;&#20219;&#21153;&#65292;&#23398;&#20064;&#26377;&#25928;&#30340;&#29305;&#24449;&#23884;&#20837;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#20256;&#32479;&#26041;&#27861;&#36890;&#24120;&#23398;&#20064;&#22266;&#23450;&#30340;&#29305;&#24449;&#34920;&#31034;&#65292;&#32780;&#32570;&#20047;&#26681;&#25454;&#19978;&#19979;&#25991;&#20449;&#24687;&#21160;&#24577;&#35843;&#25972;&#29305;&#24449;&#34920;&#31034;&#30340;&#26426;&#21046;&#65292;&#23548;&#33268;&#24615;&#33021;&#19981;&#20339;&#12290;&#19968;&#20123;&#36817;&#26399;&#30340;&#26041;&#27861;&#23581;&#35797;&#36890;&#36807;&#23398;&#20064;&#20301;&#26435;&#37325;&#25110;&#22686;&#24378;&#23884;&#20837;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#26159;&#21463;&#21040;&#19978;&#19979;&#25991;&#20013;&#26080;&#20449;&#24687;&#25110;&#20887;&#20313;&#29305;&#24449;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20511;&#37492;&#20102;&#24847;&#35782;&#21152;&#24037;&#20013;&#20840;&#23616;&#24037;&#20316;&#21306;&#29702;&#35770;&#65292;&#35813;&#29702;&#35770;&#35748;&#20026;&#21482;&#26377;&#29305;&#23450;&#30340;&#20135;&#21697;&#29305;&#24449;&#19982;&#28857;&#20987;&#34892;&#20026;&#30456;&#20851;&#65292;&#20854;&#20313;&#29305;&#24449;&#21487;&#33021;&#20250;&#22122;&#38899;&#24178;&#25200;&#65292;&#29978;&#33267;&#26377;&#23475;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#25130;&#26029;&#24847;&#35782;&#27880;&#24847;&#21147;&#30340;&#21160;&#24577;&#23884;&#20837;&#23398;&#20064;&#27169;&#22411;DELTA&#36827;&#34892;CTR&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Click-Through Rate (CTR) prediction is a pivotal task in product and content recommendation, where learning effective feature embeddings is of great significance. However, traditional methods typically learn fixed feature representations without dynamically refining feature representations according to the context information, leading to suboptimal performance. Some recent approaches attempt to address this issue by learning bit-wise weights or augmented embeddings for feature representations, but suffer from uninformative or redundant features in the context. To tackle this problem, inspired by the Global Workspace Theory in conscious processing, which posits that only a specific subset of the product features are pertinent while the rest can be noisy and even detrimental to human-click behaviors, we propose a CTR model that enables Dynamic Embedding Learning with Truncated Conscious Attention for CTR prediction, termed DELTA. DELTA contains two key components: (I) conscious truncatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MemoNet&#30340;CTR&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#22810;&#21704;&#24076;&#30721;&#26412;&#32593;&#32476;&#65288;HCNet&#65289;&#20316;&#20026;&#35760;&#24518;&#26426;&#21046;&#65292;&#39640;&#25928;&#22320;&#23398;&#20064;&#21644;&#35760;&#24518;&#20132;&#21449;&#29305;&#24449;&#30340;&#34920;&#31034;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;MemoNet&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#23637;&#29616;&#20986;NLP&#20013;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25193;&#23637;&#35268;&#24459;&#12290;</title><link>http://arxiv.org/abs/2211.01334</link><description>&lt;p&gt;
MemoNet: &#36890;&#36807;&#22810;&#21704;&#24076;&#30721;&#26412;&#32593;&#32476;&#39640;&#25928;&#22320;&#35760;&#24518;&#25152;&#26377;&#20132;&#21449;&#29305;&#24449;&#34920;&#31034;&#20197;&#23454;&#29616;CTR&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
MemoNet: Memorizing All Cross Features' Representations Efficiently via Multi-Hash Codebook Network for CTR Prediction. (arXiv:2211.01334v3 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01334
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MemoNet&#30340;CTR&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#22810;&#21704;&#24076;&#30721;&#26412;&#32593;&#32476;&#65288;HCNet&#65289;&#20316;&#20026;&#35760;&#24518;&#26426;&#21046;&#65292;&#39640;&#25928;&#22320;&#23398;&#20064;&#21644;&#35760;&#24518;&#20132;&#21449;&#29305;&#24449;&#30340;&#34920;&#31034;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;MemoNet&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#23637;&#29616;&#20986;NLP&#20013;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25193;&#23637;&#35268;&#24459;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#30340;&#26032;&#21457;&#29616;&#34920;&#26126;&#65292;&#24378;&#22823;&#30340;&#35760;&#24518;&#33021;&#21147;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25104;&#21151;&#36215;&#21040;&#20102;&#24456;&#22823;&#20316;&#29992;&#12290;&#36825;&#21551;&#21457;&#25105;&#20204;&#23558;&#29420;&#31435;&#30340;&#35760;&#24518;&#26426;&#21046;&#24341;&#20837;CTR&#25490;&#21517;&#27169;&#22411;&#65292;&#20197;&#23398;&#20064;&#21644;&#35760;&#24518;&#20132;&#21449;&#29305;&#24449;&#30340;&#34920;&#31034;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22810;&#21704;&#24076;&#30721;&#26412;&#32593;&#32476;&#65288;HCNet&#65289;&#20316;&#20026;CTR&#20219;&#21153;&#20013;&#39640;&#25928;&#23398;&#20064;&#21644;&#35760;&#24518;&#20132;&#21449;&#29305;&#24449;&#34920;&#31034;&#30340;&#35760;&#24518;&#26426;&#21046;&#12290;HCNet&#20351;&#29992;&#22810;&#21704;&#24076;&#30721;&#26412;&#20316;&#20026;&#20027;&#35201;&#30340;&#35760;&#24518;&#20301;&#32622;&#65292;&#24182;&#30001;&#22810;&#21704;&#24076;&#23547;&#22336;&#12289;&#35760;&#24518;&#24674;&#22797;&#21644;&#29305;&#24449;&#32553;&#20943;&#19977;&#20010;&#38454;&#27573;&#32452;&#25104;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MemoNet&#30340;&#26032;&#22411;CTR&#27169;&#22411;&#65292;&#23558;HCNet&#19982;DNN&#39592;&#24178;&#32593;&#32476;&#30456;&#32467;&#21512;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#22312;&#19977;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#21644;&#22312;&#32447;&#27979;&#35797;&#20013;&#34920;&#26126;&#65292;MemoNet&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;MemoNet&#23637;&#29616;&#20986;NLP&#20013;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25193;&#23637;&#35268;&#24459;&#65292;&#36825;&#24847;&#21619;&#30528;&#25105;&#20204;&#21487;&#20197;&#25193;&#22823;&#27169;&#22411;&#35268;&#27169;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
New findings in natural language processing (NLP) demonstrate that the strong memorization capability contributes a lot to the success of Large Language Models (LLM). This inspires us to explicitly bring an independent memory mechanism into CTR ranking model to learn and memorize cross features' representations. In this paper, we propose multi-Hash Codebook NETwork (HCNet) as the memory mechanism for efficiently learning and memorizing representations of cross features in CTR tasks. HCNet uses a multi-hash codebook as the main memory place and the whole memory procedure consists of three phases: multi-hash addressing, memory restoring, and feature shrinking. We also propose a new CTR model named MemoNet which combines HCNet with a DNN backbone. Extensive experimental results on three public datasets and online test show that MemoNet reaches superior performance over state-of-the-art approaches. Besides, MemoNet shows scaling law of large language model in NLP, which means we can enlarg
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#38271;&#23614;&#21830;&#21697;&#25512;&#33616;&#20013;&#30340;&#20559;&#24046;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#21449;&#35299;&#32806;&#32593;&#32476;&#65288;CDN&#65289;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#20445;&#25345;&#25972;&#20307;&#24615;&#33021;&#21644;&#20943;&#23569;&#35757;&#32451;&#21644;&#26381;&#21153;&#25104;&#26412;&#30340;&#26465;&#20214;&#19979;&#25552;&#39640;&#23614;&#37096;&#21830;&#21697;&#30340;&#25512;&#33616;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2210.14309</link><description>&lt;p&gt;
&#36890;&#36807;&#20132;&#21449;&#35299;&#32806;&#32593;&#32476;&#65288;CDN&#65289;&#22686;&#24378;&#38271;&#23614;&#21830;&#21697;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Empowering Long-tail Item Recommendation through Cross Decoupling Network (CDN). (arXiv:2210.14309v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.14309
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#38271;&#23614;&#21830;&#21697;&#25512;&#33616;&#20013;&#30340;&#20559;&#24046;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#21449;&#35299;&#32806;&#32593;&#32476;&#65288;CDN&#65289;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#20445;&#25345;&#25972;&#20307;&#24615;&#33021;&#21644;&#20943;&#23569;&#35757;&#32451;&#21644;&#26381;&#21153;&#25104;&#26412;&#30340;&#26465;&#20214;&#19979;&#25552;&#39640;&#23614;&#37096;&#21830;&#21697;&#30340;&#25512;&#33616;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24037;&#19994;&#25512;&#33616;&#31995;&#32479;&#36890;&#24120;&#36973;&#21463;&#39640;&#24230;&#20542;&#26012;&#30340;&#38271;&#23614;&#21830;&#21697;&#20998;&#24067;&#65292;&#20854;&#20013;&#23569;&#25968;&#21830;&#21697;&#33719;&#24471;&#22823;&#37096;&#20998;&#29992;&#25143;&#21453;&#39304;&#12290;&#36825;&#31181;&#20559;&#24046;&#23545;&#27809;&#26377;&#22826;&#22810;&#29992;&#25143;&#21453;&#39304;&#30340;&#21830;&#21697;&#24433;&#21709;&#25512;&#33616;&#36136;&#37327;&#12290;&#34429;&#28982;&#23398;&#26415;&#30028;&#24050;&#32463;&#21462;&#24471;&#20102;&#35768;&#22810;&#30740;&#31350;&#36827;&#23637;&#65292;&#20294;&#22312;&#29983;&#20135;&#20013;&#37096;&#32626;&#36825;&#20123;&#26041;&#27861;&#38750;&#24120;&#22256;&#38590;&#65292;&#24037;&#19994;&#39046;&#22495;&#20013;&#25913;&#36827;&#30340;&#26041;&#27861;&#24456;&#23569;&#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#39640;&#23614;&#37096;&#21830;&#21697;&#30340;&#25512;&#33616;&#25928;&#26524;&#65292;&#21516;&#26102;&#20943;&#23569;&#22521;&#35757;&#21644;&#26381;&#21153;&#25104;&#26412;&#12290;&#25105;&#20204;&#39318;&#20808;&#21457;&#29616;&#38271;&#23614;&#20998;&#24067;&#19979;&#29992;&#25143;&#20559;&#22909;&#30340;&#39044;&#27979;&#20855;&#26377;&#20559;&#24046;&#12290;&#35813;&#20559;&#35265;&#26469;&#33258;&#20110;&#20004;&#20010;&#26041;&#38754;&#22312;&#35757;&#32451;&#21644;&#26381;&#21153;&#25968;&#25454;&#20043;&#38388;&#30340;&#24046;&#24322;&#65306;1&#65289;&#29289;&#21697;&#20998;&#24067;&#20197;&#21450;2&#65289;&#32473;&#23450;&#29289;&#21697;&#30340;&#29992;&#25143;&#20559;&#22909;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#23581;&#35797;&#20943;&#36731;&#20559;&#24046;&#65292;&#20294;&#40092;&#26377;&#21516;&#26102;&#20445;&#25345;&#25972;&#20307;&#24615;&#33021;&#21644;&#38477;&#20302;&#25104;&#26412;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Industry recommender systems usually suffer from highly-skewed long-tail item distributions where a small fraction of the items receives most of the user feedback. This skew hurts recommender quality especially for the item slices without much user feedback. While there have been many research advances made in academia, deploying these methods in production is very difficult and very few improvements have been made in industry. One challenge is that these methods often hurt overall performance; additionally, they could be complex and expensive to train and serve. In this work, we aim to improve tail item recommendations while maintaining the overall performance with less training and serving cost. We first find that the predictions of user preferences are biased under long-tail distributions. The bias comes from the differences between training and serving data in two perspectives: 1) the item distributions, and 2) user's preference given an item. Most existing methods mainly attempt t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#37325;&#20462;&#27491;&#31574;&#30053;&#65288;DCD&#65289;&#65292;&#29992;&#20110;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#26356;&#26377;&#25928;&#22320;&#23558;&#25945;&#24072;&#27169;&#22411;&#30340;&#25490;&#21517;&#20449;&#24687;&#36716;&#31227;&#21040;&#23398;&#29983;&#27169;&#22411;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#20805;&#20998;&#21033;&#29992;&#20102;&#23398;&#29983;&#27169;&#22411;&#30340;&#39044;&#27979;&#35823;&#24046;&#65292;&#36824;&#25552;&#20379;&#20102;&#26356;&#20840;&#38754;&#30340;&#35270;&#35282;&#65292;&#35299;&#20915;&#20102;&#26494;&#24347;&#25490;&#21517;&#33976;&#39311;&#26041;&#27861;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2109.03459</link><description>&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#20013;&#29992;&#20110;&#25490;&#21517;&#33976;&#39311;&#30340;&#21452;&#37325;&#20462;&#27491;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Dual Correction Strategy for Ranking Distillation in Top-N Recommender System. (arXiv:2109.03459v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.03459
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#37325;&#20462;&#27491;&#31574;&#30053;&#65288;DCD&#65289;&#65292;&#29992;&#20110;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#26356;&#26377;&#25928;&#22320;&#23558;&#25945;&#24072;&#27169;&#22411;&#30340;&#25490;&#21517;&#20449;&#24687;&#36716;&#31227;&#21040;&#23398;&#29983;&#27169;&#22411;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#20805;&#20998;&#21033;&#29992;&#20102;&#23398;&#29983;&#27169;&#22411;&#30340;&#39044;&#27979;&#35823;&#24046;&#65292;&#36824;&#25552;&#20379;&#20102;&#26356;&#20840;&#38754;&#30340;&#35270;&#35282;&#65292;&#35299;&#20915;&#20102;&#26494;&#24347;&#25490;&#21517;&#33976;&#39311;&#26041;&#27861;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#26159;&#23558;&#35757;&#32451;&#20805;&#20998;&#30340;&#22823;&#27169;&#22411;&#65288;&#25945;&#24072;&#65289;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#23567;&#27169;&#22411;&#65288;&#23398;&#29983;&#65289;&#30340;&#37325;&#35201;&#30740;&#31350;&#39046;&#22495;&#65292;&#23545;&#20110;&#25512;&#33616;&#31995;&#32479;&#30340;&#23454;&#38469;&#37096;&#32626;&#32780;&#35328;&#65292;&#23427;&#24050;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#26368;&#36817;&#65292;&#26494;&#24347;&#25490;&#21517;&#33976;&#39311;&#65288;RRD&#65289;&#34920;&#26126;&#65292;&#22312;&#25512;&#33616;&#21015;&#34920;&#20013;&#33976;&#39311;&#25490;&#21517;&#20449;&#24687;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#35813;&#26041;&#27861;&#20173;&#28982;&#23384;&#22312;&#20197;&#19979;&#38480;&#21046;&#65306;1&#65289;&#23427;&#26410;&#20805;&#20998;&#21033;&#29992;&#23398;&#29983;&#27169;&#22411;&#30340;&#39044;&#27979;&#35823;&#24046;&#65292;&#20351;&#24471;&#35757;&#32451;&#25928;&#29575;&#19981;&#39640;&#65307;2&#65289;&#23427;&#21482;&#33976;&#39311;&#29992;&#25143;&#20391;&#30340;&#25490;&#21517;&#20449;&#24687;&#65292;&#22312;&#31232;&#30095;&#30340;&#38544;&#24335;&#21453;&#39304;&#19979;&#25552;&#20379;&#30340;&#35270;&#35282;&#19981;&#36275;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#39640;&#25928;&#30340;&#33976;&#39311;&#26041;&#27861;&#65292;&#21363;&#21452;&#37325;&#20462;&#27491;&#31574;&#30053;&#65288;DCD&#65289;&#65292;&#36890;&#36807;&#25945;&#24072;&#27169;&#22411;&#21644;&#23398;&#29983;&#27169;&#22411;&#39044;&#27979;&#20043;&#38388;&#30340;&#24046;&#24322;&#26469;&#20915;&#23450;&#35201;&#33976;&#39311;&#30340;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge Distillation (KD), which transfers the knowledge of a well-trained large model (teacher) to a small model (student), has become an important area of research for practical deployment of recommender systems. Recently, Relaxed Ranking Distillation (RRD) has shown that distilling the ranking information in the recommendation list significantly improves the performance. However, the method still has limitations in that 1) it does not fully utilize the prediction errors of the student model, which makes the training not fully efficient, and 2) it only distills the user-side ranking information, which provides an insufficient view under the sparse implicit feedback. This paper presents Dual Correction strategy for Distillation (DCD), which transfers the ranking information from the teacher model to the student model in a more efficient manner. Most importantly, DCD uses the discrepancy between the teacher model and the student model predictions to decide which knowledge to be disti
&lt;/p&gt;</description></item></channel></rss>