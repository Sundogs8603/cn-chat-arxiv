<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#37325;&#35299;&#32544;&#21327;&#21516;&#36807;&#28388;&#65288;DDCF&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#23545;&#24847;&#22270;&#21644;&#20559;&#22909;&#22240;&#32032;&#36827;&#34892;&#20998;&#31163;&#65292;&#24182;&#36890;&#36807;&#35299;&#32544;&#34920;&#31034;&#24314;&#31435;&#29420;&#31435;&#30340;&#31232;&#30095;&#20559;&#22909;&#34920;&#31034;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#20934;&#30830;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#20010;&#24615;&#21270;&#25512;&#33616;&#12290;</title><link>http://arxiv.org/abs/2305.11084</link><description>&lt;p&gt;
&#20559;&#22909;&#36824;&#26159;&#24847;&#22270;&#65311;&#21452;&#37325;&#35299;&#32544;&#21327;&#21516;&#36807;&#28388;
&lt;/p&gt;
&lt;p&gt;
Preference or Intent? Double Disentangled Collaborative Filtering. (arXiv:2305.11084v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11084
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#37325;&#35299;&#32544;&#21327;&#21516;&#36807;&#28388;&#65288;DDCF&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#23545;&#24847;&#22270;&#21644;&#20559;&#22909;&#22240;&#32032;&#36827;&#34892;&#20998;&#31163;&#65292;&#24182;&#36890;&#36807;&#35299;&#32544;&#34920;&#31034;&#24314;&#31435;&#29420;&#31435;&#30340;&#31232;&#30095;&#20559;&#22909;&#34920;&#31034;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#20934;&#30830;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#20010;&#24615;&#21270;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20204;&#36873;&#25321;&#29289;&#21697;&#26102;&#36890;&#24120;&#26377;&#19981;&#21516;&#30340;&#24847;&#22270;&#65292;&#32780;&#22312;&#30456;&#21516;&#24847;&#22270;&#19979;&#20182;&#20204;&#30340;&#20559;&#22909;&#20063;&#21487;&#33021;&#19981;&#21516;&#12290;&#20256;&#32479;&#30340;&#21327;&#21516;&#36807;&#28388;&#26041;&#27861;&#36890;&#24120;&#23558;&#24847;&#22270;&#21644;&#20559;&#22909;&#22240;&#32032;&#32416;&#32544;&#22312;&#24314;&#27169;&#36807;&#31243;&#20013;&#65292;&#36825;&#26174;&#33879;&#38480;&#21046;&#20102;&#25512;&#33616;&#24615;&#33021;&#30340;&#31283;&#20581;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21452;&#37325;&#35299;&#32544;&#21327;&#21516;&#36807;&#28388;&#65288;DDCF&#65289;&#30340;&#20010;&#24615;&#21270;&#25512;&#33616;&#26041;&#27861;&#12290;&#19968;&#32423;&#35299;&#32544;&#26159;&#20026;&#20102;&#23558;&#24847;&#22270;&#21644;&#20559;&#22909;&#30340;&#24433;&#21709;&#22240;&#32032;&#20998;&#24320;&#65292;&#32780;&#31532;&#20108;&#32423;&#35299;&#32544;&#26159;&#20026;&#20102;&#26500;&#24314;&#29420;&#31435;&#30340;&#31232;&#30095;&#20559;&#22909;&#34920;&#31034;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DDCF&#26041;&#27861;&#22312;&#25512;&#33616;&#31934;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#22343;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
People usually have different intents for choosing items, while their preferences under the same intent may also different. In traditional collaborative filtering approaches, both intent and preference factors are usually entangled in the modeling process, which significantly limits the robustness and interpretability of recommendation performances. For example, the low-rating items are always treated as negative feedback while they actually could provide positive information about user intent. To this end, in this paper, we propose a two-fold representation learning approach, namely Double Disentangled Collaborative Filtering (DDCF), for personalized recommendations. The first-level disentanglement is for separating the influence factors of intent and preference, while the second-level disentanglement is performed to build independent sparse preference representations under individual intent with limited computational complexity. Specifically, we employ two variational autoencoder net
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#23545;&#27604;&#29366;&#24577;&#22686;&#24378;&#26041;&#27861;&#26469;&#35757;&#32451;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#20854;&#20013;&#21253;&#25324;&#22235;&#31181;&#29366;&#24577;&#22686;&#24378;&#31574;&#30053;&#21644;&#23545;&#27604;&#29366;&#24577;&#34920;&#31034;&#23398;&#20064;&#65292;&#21487;&#20197;&#35299;&#20915;&#29616;&#26377;RL&#25512;&#33616;&#26041;&#27861;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#32988;&#36807;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;RL&#25512;&#33616;&#22120;&#12290;</title><link>http://arxiv.org/abs/2305.11081</link><description>&lt;p&gt;
&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#25512;&#33616;&#31995;&#32479;&#30340;&#23545;&#27604;&#29366;&#24577;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Contrastive State Augmentations for Reinforcement Learning-Based Recommender Systems. (arXiv:2305.11081v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11081
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#23545;&#27604;&#29366;&#24577;&#22686;&#24378;&#26041;&#27861;&#26469;&#35757;&#32451;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#20854;&#20013;&#21253;&#25324;&#22235;&#31181;&#29366;&#24577;&#22686;&#24378;&#31574;&#30053;&#21644;&#23545;&#27604;&#29366;&#24577;&#34920;&#31034;&#23398;&#20064;&#65292;&#21487;&#20197;&#35299;&#20915;&#29616;&#26377;RL&#25512;&#33616;&#26041;&#27861;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#32988;&#36807;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;RL&#25512;&#33616;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#21382;&#21490;&#29992;&#25143;-&#39033;&#30446;&#20132;&#20114;&#24207;&#21015;&#20013;&#23398;&#20064;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#25512;&#33616;&#22120;&#23545;&#20110;&#29983;&#25104;&#39640;&#22238;&#25253;&#24314;&#35758;&#21644;&#25913;&#21892;&#38271;&#26399;&#32047;&#31215;&#25928;&#30410;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;RL&#25512;&#33616;&#26041;&#27861;&#36935;&#21040;&#20197;&#19979;&#22256;&#38590;&#65306;&#65288;i&#65289;&#20026;&#19981;&#21253;&#21547;&#22312;&#31163;&#32447;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#29366;&#24577;&#20272;&#35745;&#20215;&#20540;&#20989;&#25968;&#65307;&#20197;&#21450;&#65288;ii&#65289;&#30001;&#20110;&#32570;&#20047;&#23545;&#27604;&#20449;&#21495;&#65292;&#20174;&#29992;&#25143;&#38544;&#24335;&#21453;&#39304;&#20013;&#23398;&#20064;&#26377;&#25928;&#30340;&#29366;&#24577;&#34920;&#31034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#27604;&#29366;&#24577;&#22686;&#24378;&#65288;CSA&#65289;&#26469;&#35757;&#32451;&#22522;&#20110;RL&#30340;&#25512;&#33616;&#31995;&#32479;&#12290;&#20026;&#20102;&#35299;&#20915;&#31532;&#19968;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22235;&#31181;&#29366;&#24577;&#22686;&#24378;&#31574;&#30053;&#26469;&#25193;&#22823;&#31163;&#32447;&#25968;&#25454;&#30340;&#29366;&#24577;&#31354;&#38388;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#36890;&#36807;&#20351;RL&#20195;&#29702;&#35775;&#38382;&#26412;&#22320;&#29366;&#24577;&#21306;&#22495;&#24182;&#30830;&#20445;&#21407;&#22987;&#21644;&#22686;&#24378;&#29366;&#24577;&#20043;&#38388;&#23398;&#20064;&#30340;&#20215;&#20540;&#20989;&#25968;&#30456;&#20284;&#65292;&#25552;&#39640;&#20102;&#25512;&#33616;&#22120;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#31532;&#20108;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24341;&#20837;&#23545;&#27604;&#29366;&#24577;&#34920;&#31034;&#23398;&#20064;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#27491;&#26679;&#26412;&#30456;&#20284;&#24615;&#21644;&#26368;&#23567;&#21270;&#36127;&#26679;&#26412;&#30456;&#20284;&#24615;&#65292;&#20351;&#20195;&#29702;&#20154;&#23398;&#20064;&#21040;&#20449;&#24687;&#20016;&#23500;&#30340;&#29366;&#24577;&#34920;&#31034;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;RL&#25512;&#33616;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning reinforcement learning (RL)-based recommenders from historical user-item interaction sequences is vital to generate high-reward recommendations and improve long-term cumulative benefits. However, existing RL recommendation methods encounter difficulties (i) to estimate the value functions for states which are not contained in the offline training data, and (ii) to learn effective state representations from user implicit feedback due to the lack of contrastive signals. In this work, we propose contrastive state augmentations (CSA) for the training of RL-based recommender systems. To tackle the first issue, we propose four state augmentation strategies to enlarge the state space of the offline data. The proposed method improves the generalization capability of the recommender by making the RL agent visit the local state regions and ensuring the learned value functions are similar between the original and augmented states. For the second issue, we propose introducing contrastive 
&lt;/p&gt;</description></item><item><title>BERM&#20351;&#29992;&#24179;&#34913;&#30340;&#12289;&#21487;&#25552;&#21462;&#30340;&#29305;&#24449;&#34920;&#31034;&#27861;&#26469;&#25429;&#25417;&#21305;&#37197;&#20449;&#21495;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#23494;&#38598;&#26816;&#32034;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.11052</link><description>&lt;p&gt;
BERM&#65306;&#35757;&#32451;&#24179;&#34913;&#21487;&#25552;&#21462;&#34920;&#31034;&#20197;&#25552;&#39640;&#23494;&#38598;&#26816;&#32034;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
BERM: Training the Balanced and Extractable Representation for Matching to Improve Generalization Ability of Dense Retrieval. (arXiv:2305.11052v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11052
&lt;/p&gt;
&lt;p&gt;
BERM&#20351;&#29992;&#24179;&#34913;&#30340;&#12289;&#21487;&#25552;&#21462;&#30340;&#29305;&#24449;&#34920;&#31034;&#27861;&#26469;&#25429;&#25417;&#21305;&#37197;&#20449;&#21495;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#23494;&#38598;&#26816;&#32034;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23494;&#38598;&#26816;&#32034;&#24050;&#32463;&#34920;&#29616;&#20986;&#22312;&#22495;&#20869;&#26631;&#35760;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#22312;&#31532;&#19968;&#38454;&#27573;&#26816;&#32034;&#36807;&#31243;&#20013;&#26377;&#25152;&#20316;&#20026;&#12290;&#28982;&#32780;&#65292;&#20197;&#21069;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#30001;&#20110;&#23494;&#38598;&#26816;&#32034;&#23545;&#22495;&#19981;&#21464;&#21644;&#21487;&#35299;&#37322;&#30340;&#29305;&#24449;&#30340;&#24314;&#27169;&#36739;&#24369;&#65288;&#21363;&#20004;&#20010;&#25991;&#26412;&#20043;&#38388;&#30340;&#21305;&#37197;&#20449;&#21495;&#65292;&#36825;&#26159;&#20449;&#24687;&#26816;&#32034;&#30340;&#26412;&#36136;&#65289;&#65292;&#22240;&#27492;&#38590;&#20197;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#30340;&#39046;&#22495;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25429;&#25417;&#21305;&#37197;&#20449;&#21495;&#25552;&#20986;&#20102;&#19968;&#31181;&#25552;&#39640;&#23494;&#38598;&#26816;&#32034;&#27867;&#21270;&#24615;&#33021;&#30340;&#26032;&#26041;&#27861;&#65292;&#31216;&#20026;BERM&#12290;&#20840;&#38754;&#30340;&#32454;&#31890;&#24230;&#34920;&#36798;&#21644;&#26597;&#35810;&#23548;&#21521;&#30340;&#26174;&#30528;&#24615;&#26159;&#21305;&#37197;&#20449;&#21495;&#30340;&#20004;&#20010;&#23646;&#24615;&#12290;&#22240;&#27492;&#65292;&#22312;BERM&#20013;&#65292;&#19968;&#20010;&#21333;&#19968;&#30340;Passage&#34987;&#21010;&#20998;&#20026;&#22810;&#20010;&#21333;&#20803;&#65292;&#25552;&#20986;&#20102;&#20004;&#20010;&#21333;&#20803;&#32423;&#35201;&#27714;&#20316;&#20026;&#32422;&#26463;&#36827;&#34892;&#34920;&#31034;&#20197;&#33719;&#24471;&#26377;&#25928;&#30340;&#21305;&#37197;&#20449;&#21495;&#12290;&#19968;&#20010;&#26159;&#35821;&#20041;&#21333;&#20803;&#24179;&#34913;&#65292;&#21478;&#19968;&#20010;&#26159;&#24517;&#38656;&#30340;&#21305;&#37197;&#21333;&#20803;&#21487;&#25552;&#21462;&#24615;&#12290;&#21333;&#20803;&#32423;&#35270;&#22270;&#21644;&#24179;&#34913;&#35821;&#20041;&#20351;&#34920;&#31034;&#20197;&#32454;&#31890;&#24230;&#30340;&#26041;&#24335;&#34920;&#36798;&#25991;&#26412;&#12290;&#24517;&#38656;&#30340;&#21305;&#37197;&#21333;&#20803;&#21487;&#25552;&#21462;&#24615;&#30830;&#20445;&#20445;&#30041;&#20449;&#24687;&#26816;&#32034;&#30340;&#26412;&#36136;&#12290;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;BERM&#22312;&#20445;&#25345;&#22495;&#20869;&#25968;&#25454;&#38598;&#19978;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#25552;&#39640;&#20102;&#31639;&#27861;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dense retrieval has shown promise in the first-stage retrieval process when trained on in-domain labeled datasets. However, previous studies have found that dense retrieval is hard to generalize to unseen domains due to its weak modeling of domain-invariant and interpretable feature (i.e., matching signal between two texts, which is the essence of information retrieval). In this paper, we propose a novel method to improve the generalization of dense retrieval via capturing matching signal called BERM. Fully fine-grained expression and query-oriented saliency are two properties of the matching signal. Thus, in BERM, a single passage is segmented into multiple units and two unit-level requirements are proposed for representation as the constraint in training to obtain the effective matching signal. One is semantic unit balance and the other is essential matching unit extractability. Unit-level view and balanced semantics make representation express the text in a fine-grained manner. Esse
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#36935;&#24615;&#24230;&#37327;&#26041;&#27861;&#26469;&#34913;&#37327;&#25512;&#33616;&#31995;&#32479;&#20013;&#22238;&#22768;&#23460;&#21644;&#21516;&#36136;&#21270;&#30340;&#23384;&#22312;&#65292;&#24182;&#37319;&#29992;&#35789;&#20856;&#26696;&#20363;&#36873;&#25321;&#26469;&#25913;&#21892;&#25512;&#33616;&#25216;&#26415;&#30340;&#22810;&#26679;&#24615;&#65292;&#21462;&#24471;&#20102;&#22312;&#20010;&#24615;&#21270;&#12289;&#35206;&#30422;&#38754;&#21644;&#26426;&#36935;&#24615;&#26041;&#38754;&#30340;&#20248;&#24322;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.11044</link><description>&lt;p&gt;
&#36890;&#36807;&#35789;&#20856;&#26696;&#20363;&#36873;&#25321;&#25552;&#39640;&#25512;&#33616;&#31995;&#32479;&#30340;&#26426;&#36935;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving Recommendation System Serendipity Through Lexicase Selection. (arXiv:2305.11044v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11044
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#36935;&#24615;&#24230;&#37327;&#26041;&#27861;&#26469;&#34913;&#37327;&#25512;&#33616;&#31995;&#32479;&#20013;&#22238;&#22768;&#23460;&#21644;&#21516;&#36136;&#21270;&#30340;&#23384;&#22312;&#65292;&#24182;&#37319;&#29992;&#35789;&#20856;&#26696;&#20363;&#36873;&#25321;&#26469;&#25913;&#21892;&#25512;&#33616;&#25216;&#26415;&#30340;&#22810;&#26679;&#24615;&#65292;&#21462;&#24471;&#20102;&#22312;&#20010;&#24615;&#21270;&#12289;&#35206;&#30422;&#38754;&#21644;&#26426;&#36935;&#24615;&#26041;&#38754;&#30340;&#20248;&#24322;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#24433;&#21709;&#30528;&#25105;&#20204;&#25968;&#23383;&#29983;&#27963;&#30340;&#26041;&#26041;&#38754;&#38754;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#22312;&#21162;&#21147;&#28385;&#36275;&#25105;&#20204;&#38656;&#27714;&#30340;&#36807;&#31243;&#20013;&#65292;&#23427;&#20204;&#38480;&#21046;&#20102;&#25105;&#20204;&#30340;&#24320;&#25918;&#24230;&#12290;&#24403;&#21069;&#30340;&#25512;&#33616;&#31995;&#32479;&#20419;&#36827;&#20102;&#22238;&#22768;&#23460;&#21644;&#21516;&#36136;&#21270;&#65292;&#20351;&#29992;&#25143;&#21482;&#30475;&#21040;&#20182;&#20204;&#24819;&#35201;&#30475;&#21040;&#30340;&#20449;&#24687;&#21644;&#19982;&#20854;&#32972;&#26223;&#30456;&#20284;&#30340;&#20869;&#23481;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#36935;&#24615;&#24230;&#37327;&#26041;&#27861;&#65292;&#20351;&#29992;&#32858;&#31867;&#20998;&#26512;&#26469;&#34913;&#37327;&#25512;&#33616;&#31995;&#32479;&#20013;&#22238;&#22768;&#23460;&#21644;&#21516;&#36136;&#21270;&#30340;&#23384;&#22312;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23581;&#35797;&#37319;&#29992;&#20174;&#28436;&#21270;&#35745;&#31639;&#25991;&#29486;&#20013;&#30693;&#21517;&#30340;&#23478;&#38271;&#36873;&#25321;&#31639;&#27861;&#65292;&#21363;&#35789;&#20856;&#26696;&#20363;&#36873;&#25321;&#65292;&#26469;&#25913;&#21892;&#20445;&#30041;&#22810;&#26679;&#24615;&#30340;&#25512;&#33616;&#25216;&#26415;&#36136;&#37327;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#35789;&#20856;&#26696;&#20363;&#36873;&#25321;&#25110;&#35789;&#20856;&#26696;&#20363;&#36873;&#25321;&#21644;&#25490;&#21517;&#30340;&#28151;&#21512;&#22312;&#20010;&#24615;&#21270;&#12289;&#35206;&#30422;&#38754;&#21644;&#25105;&#20204;&#19987;&#38376;&#35774;&#35745;&#30340;&#26426;&#36935;&#24615;&#22522;&#20934;&#26041;&#38754;&#20248;&#20110;&#20165;&#25490;&#21517;&#30340;&#23545;&#25163;&#65292;&#32780;&#22312;&#20934;&#30830;&#24615;&#65288;&#21629;&#20013;&#29575;&#65289;&#26041;&#38754;&#20165;&#31245;&#26377;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommender systems influence almost every aspect of our digital lives. Unfortunately, in striving to give us what we want, they end up restricting our open-mindedness. Current recommender systems promote echo chambers, where people only see the information they want to see, and homophily, where users of similar background see similar content. We propose a new serendipity metric to measure the presence of echo chambers and homophily in recommendation systems using cluster analysis. We then attempt to improve the diversity-preservation qualities of well known recommendation techniques by adopting a parent selection algorithm from the evolutionary computation literature known as lexicase selection. Our results show that lexicase selection, or a mixture of lexicase selection and ranking, outperforms its purely ranked counterparts in terms of personalization, coverage and our specifically designed serendipity benchmark, while only slightly under-performing in terms of accuracy (hit rate). 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;&#20174;Ad-hoc&#21040;&#20132;&#20114;&#24335;&#25628;&#32034;&#20013;&#26597;&#35810;&#24615;&#33021;&#39044;&#27979;(QPP)&#30340;&#26377;&#25928;&#26041;&#27861;&#65292;&#24182;&#25506;&#32034;&#20102;QPP&#26041;&#27861;&#22312;&#20132;&#20114;&#24335;&#25628;&#32034;&#20013;&#26159;&#21542;&#20855;&#26377;&#25512;&#24191;&#24212;&#29992;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.10923</link><description>&lt;p&gt;
&#26597;&#35810;&#24615;&#33021;&#39044;&#27979;&#65306;&#20174;Ad-hoc&#21040;&#20132;&#20114;&#24335;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Query Performance Prediction: From Ad-hoc to Conversational Search. (arXiv:2305.10923v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10923
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;&#20174;Ad-hoc&#21040;&#20132;&#20114;&#24335;&#25628;&#32034;&#20013;&#26597;&#35810;&#24615;&#33021;&#39044;&#27979;(QPP)&#30340;&#26377;&#25928;&#26041;&#27861;&#65292;&#24182;&#25506;&#32034;&#20102;QPP&#26041;&#27861;&#22312;&#20132;&#20114;&#24335;&#25628;&#32034;&#20013;&#26159;&#21542;&#20855;&#26377;&#25512;&#24191;&#24212;&#29992;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26597;&#35810;&#24615;&#33021;&#39044;&#27979;(QPP)&#26159;&#20449;&#24687;&#26816;&#32034;&#20013;&#30340;&#19968;&#20010;&#26680;&#24515;&#20219;&#21153;&#12290;QPP&#30340;&#20219;&#21153;&#26159;&#22312;&#27809;&#26377;&#30456;&#20851;&#21028;&#26029;&#30340;&#24773;&#20917;&#19979;&#39044;&#27979;&#26597;&#35810;&#30340;&#26816;&#32034;&#36136;&#37327;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;QPP&#22312;Ad-hoc&#25628;&#32034;&#20013;&#38750;&#24120;&#26377;&#25928;&#21644;&#26377;&#29992;&#12290;&#36817;&#24180;&#26469;&#65292;&#23545;&#35805;&#24335;&#25628;&#32034;(CS)&#21462;&#24471;&#20102;&#30456;&#24403;&#22823;&#30340;&#36827;&#23637; &#12290;&#26377;&#25928;&#30340;QPP&#33021;&#22815;&#24110;&#21161;CS&#31995;&#32479;&#22312;&#19979;&#19968;&#36718;&#20915;&#23450;&#36866;&#24403;&#30340;&#34892;&#21160;&#12290;&#23613;&#31649;&#20855;&#26377;&#28508;&#21147;&#65292;&#20294;CS&#30340;QPP&#30740;&#31350;&#36824;&#24456;&#23569;&#12290;&#26412;&#25991;&#36890;&#36807;&#37325;&#29616;&#21644;&#30740;&#31350;&#29616;&#26377;&#30340;QPP&#26041;&#27861;&#22312;CS&#19978;&#30340;&#26377;&#25928;&#24615;&#26469;&#22635;&#34917;&#36825;&#19968;&#30740;&#31350;&#31354;&#30333;&#12290;&#34429;&#28982;&#22312;&#20004;&#31181;&#24773;&#20917;&#19979;&#30340;&#36890;&#36947;&#26816;&#32034;&#20219;&#21153;&#30456;&#21516;&#65292;&#20294;CS&#20013;&#30340;&#29992;&#25143;&#26597;&#35810;&#21462;&#20915;&#20110;&#23545;&#35805;&#21382;&#21490;&#65292;&#24341;&#20837;&#20102;&#26032;&#30340;QPP&#25361;&#25112;&#12290;&#25105;&#20204;&#23588;&#20854;&#26159;&#25506;&#35752;&#20174;Ad-hoc&#25628;&#32034;&#20013;QPP&#26041;&#27861;&#30340;&#30740;&#31350;&#32467;&#26524;&#22312;&#19977;&#20010;CS&#35774;&#32622;&#20013;&#30340;&#25512;&#24191;&#31243;&#24230;:(i) &#35780;&#20272;&#22522;&#20110;&#26597;&#35810;&#37325;&#20889;&#30340;&#26816;&#32034;&#26041;&#27861;&#30340;&#19981;&#21516;&#26597;&#35810;&#30340;&#26816;&#32034;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;
Query performance prediction (QPP) is a core task in information retrieval. The QPP task is to predict the retrieval quality of a search system for a query without relevance judgments. Research has shown the effectiveness and usefulness of QPP for ad-hoc search. Recent years have witnessed considerable progress in conversational search (CS). Effective QPP could help a CS system to decide an appropriate action to be taken at the next turn. Despite its potential, QPP for CS has been little studied. We address this research gap by reproducing and studying the effectiveness of existing QPP methods in the context of CS. While the task of passage retrieval remains the same in the two settings, a user query in CS depends on the conversational history, introducing novel QPP challenges. In particular, we seek to explore to what extent findings from QPP methods for ad-hoc search generalize to three CS settings: (i) estimating the retrieval quality of different query rewriting-based retrieval met
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#22270;&#23545;&#27604;&#23398;&#20064;&#30340;&#25512;&#33616;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#30340;&#26041;&#24335;&#25913;&#36827;&#29992;&#25143;&#21644;&#29289;&#21697;&#30340;&#34920;&#31034;&#65292;&#20851;&#27880;&#25968;&#25454;&#20013;&#30340;&#38590;&#20197;&#21306;&#20998;&#30340;&#36127;&#38754;&#20363;&#23376;&#30340;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2305.10837</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#22270;&#23545;&#27604;&#23398;&#20064;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Adaptive Graph Contrastive Learning for Recommendation. (arXiv:2305.10837v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10837
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#22270;&#23545;&#27604;&#23398;&#20064;&#30340;&#25512;&#33616;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#30340;&#26041;&#24335;&#25913;&#36827;&#29992;&#25143;&#21644;&#29289;&#21697;&#30340;&#34920;&#31034;&#65292;&#20851;&#27880;&#25968;&#25454;&#20013;&#30340;&#38590;&#20197;&#21306;&#20998;&#30340;&#36127;&#38754;&#20363;&#23376;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#24050;&#25104;&#21151;&#22320;&#24212;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#65292;&#25104;&#20026;&#19968;&#31181;&#26377;&#25928;&#30340;&#21327;&#21516;&#36807;&#28388;&#26041;&#27861;&#12290;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25512;&#33616;&#31995;&#32479;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#27839;&#30528;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#36793;&#36882;&#24402;&#22320;&#25191;&#34892;&#28040;&#24687;&#20256;&#36882;&#65292;&#20197;&#23436;&#21892;&#32534;&#30721;&#23884;&#20837;&#65292;&#36825;&#20381;&#36182;&#20110;&#20805;&#36275;&#21644;&#39640;&#36136;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#30001;&#20110;&#23454;&#38469;&#25512;&#33616;&#22330;&#26223;&#20013;&#30340;&#29992;&#25143;&#34892;&#20026;&#25968;&#25454;&#36890;&#24120;&#23384;&#22312;&#22122;&#22768;&#24182;&#21576;&#29616;&#20986;&#20542;&#26012;&#20998;&#24067;&#65292;&#19968;&#20123;&#25512;&#33616;&#26041;&#27861;&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#26469;&#25913;&#21892;&#29992;&#25143;&#34920;&#31034;&#65292;&#20363;&#22914;SGL&#21644;SimGCL&#12290; &#28982;&#32780;&#65292;&#23613;&#31649;&#23427;&#20204;&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#23427;&#20204;&#36890;&#36807;&#21019;&#24314;&#23545;&#27604;&#35270;&#22270;&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#20855;&#26377;&#25968;&#25454;&#22686;&#24378;&#25506;&#32034;&#65292;&#38656;&#35201;&#36827;&#34892;&#32321;&#29712;&#30340;&#35797;&#38169;&#36873;&#25321;&#22686;&#24378;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#36866;&#24212;&#22270;&#23545;&#27604;&#23398;&#20064;&#65288;AdaptiveGCL&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#20294;&#20851;&#27880;&#25968;&#25454;&#20013;&#30340;&#38590;&#20197;&#21306;&#20998;&#30340;&#36127;&#38754;&#20363;&#23376;&#30340;&#20449;&#24687;&#65292;&#29992;&#23545;&#27604;&#23398;&#20064;&#30340;&#26041;&#24335;&#25913;&#36827;&#29992;&#25143;&#21644;&#29289;&#21697;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, graph neural networks (GNNs) have been successfully applied to recommender systems as an effective collaborative filtering (CF) approach. The key idea of GNN-based recommender system is to recursively perform the message passing along the user-item interaction edge for refining the encoded embeddings, relying on sufficient and high-quality training data. Since user behavior data in practical recommendation scenarios is often noisy and exhibits skewed distribution, some recommendation approaches, e.g., SGL and SimGCL, leverage self-supervised learning to improve user representations against the above issues. Despite their effectiveness, however, they conduct self-supervised learning through creating contrastvie views, depending on the exploration of data augmentations with the problem of tedious trial-and-error selection of augmentation methods. In this paper, we propose a novel Adaptive Graph Contrastive Learning (AdaptiveGCL) framework which conducts graph contrastive learni
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#39033;&#30446;&#30456;&#20851;&#24615;&#30340;&#26032;&#22411;&#35757;&#32451;&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#25552;&#39640;&#24207;&#21015;&#25512;&#33616;&#31995;&#32479;&#23545;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.10824</link><description>&lt;p&gt;
&#34701;&#21512;&#39033;&#30446;&#30456;&#20851;&#24615;&#30340;&#24207;&#21015;&#25512;&#33616;&#31995;&#32479;&#35757;&#32451;&#25439;&#22833;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Integrating Item Relevance in Training Loss for Sequential Recommender Systems. (arXiv:2305.10824v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10824
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#39033;&#30446;&#30456;&#20851;&#24615;&#30340;&#26032;&#22411;&#35757;&#32451;&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#25552;&#39640;&#24207;&#21015;&#25512;&#33616;&#31995;&#32479;&#23545;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24207;&#21015;&#25512;&#33616;&#31995;&#32479;&#26159;&#19968;&#31181;&#21463;&#27426;&#36814;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#23427;&#36890;&#36807;&#23398;&#20064;&#29992;&#25143;&#30340;&#21382;&#21490;&#25968;&#25454;&#26469;&#39044;&#27979;&#29992;&#25143;&#19979;&#19968;&#20010;&#21487;&#33021;&#19982;&#20043;&#20132;&#20114;&#30340;&#39033;&#30446;&#12290;&#28982;&#32780;&#65292;&#29992;&#25143;&#30340;&#20132;&#20114;&#21487;&#33021;&#20250;&#21463;&#21040;&#26469;&#33258;&#24080;&#25143;&#20849;&#20139;&#12289;&#19981;&#19968;&#33268;&#30340;&#20559;&#22909;&#25110;&#24847;&#22806;&#28857;&#20987;&#31561;&#22122;&#22768;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#65288;i&#65289;&#25552;&#20986;&#20102;&#19968;&#20010;&#32771;&#34385;&#22810;&#20010;&#26410;&#26469;&#39033;&#30446;&#30340;&#26032;&#30340;&#35780;&#20272;&#21327;&#35758;&#65292;&#65288;ii&#65289;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20851;&#27880;&#30456;&#20851;&#24615;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#35757;&#32451;&#20855;&#26377;&#22810;&#20010;&#26410;&#26469;&#39033;&#30446;&#30340;&#24207;&#21015;&#25512;&#33616;&#31995;&#32479;&#65292;&#20197;&#20351;&#20854;&#23545;&#22122;&#22768;&#26356;&#21152;&#40065;&#26834;&#12290;&#25105;&#20204;&#30340;&#20851;&#27880;&#30456;&#20851;&#24615;&#27169;&#22411;&#22312;&#20256;&#32479;&#35780;&#20272;&#21327;&#35758;&#20013;&#25552;&#39640;&#20102;NDCG@10&#32422;1.2%&#21644;HR&#32422;0.88%&#65292;&#32780;&#22312;&#26032;&#35780;&#20272;&#21327;&#35758;&#20013;&#65292;&#25913;&#36827;&#30340;NDCG@10&#32422;1.63%&#21644;HR&#32422;1.5%&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequential Recommender Systems (SRSs) are a popular type of recommender system that learns from a user's history to predict the next item they are likely to interact with. However, user interactions can be affected by noise stemming from account sharing, inconsistent preferences, or accidental clicks. To address this issue, we (i) propose a new evaluation protocol that takes multiple future items into account and (ii) introduce a novel relevance-aware loss function to train a SRS with multiple future items to make it more robust to noise. Our relevance-aware models obtain an improvement of ~1.2% of NDCG@10 and 0.88% in the traditional evaluation protocol, while in the new evaluation protocol, the improvement is ~1.63% of NDCG@10 and ~1.5% of HR w.r.t the best performing models.
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25628;&#32034;&#22686;&#24378;&#30340;&#39034;&#24207;&#25512;&#33616;&#65288;SESRec&#65289;&#26694;&#26550;&#65292;&#23427;&#21033;&#29992;&#29992;&#25143;&#30340;&#25628;&#32034;&#20852;&#36259;&#36827;&#34892;&#25512;&#33616;&#65292;&#36890;&#36807;&#21306;&#20998;S&#65286;R&#34892;&#20026;&#20013;&#30340;&#30456;&#20284;&#21644;&#19981;&#30456;&#20284;&#34920;&#31034;&#65292;&#20351;S&#65286;R&#29305;&#24449;&#33021;&#22815;&#26356;&#22909;&#22320;&#21457;&#25381;&#20854;&#29420;&#29305;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2305.10822</link><description>&lt;p&gt;
&#24403;&#25628;&#32034;&#36935;&#35265;&#25512;&#33616;&#65306;&#23398;&#20064;&#21306;&#20998;&#25628;&#32034;&#34920;&#31034;&#20197;&#29992;&#20110;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
When Search Meets Recommendation: Learning Disentangled Search Representation for Recommendation. (arXiv:2305.10822v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10822
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25628;&#32034;&#22686;&#24378;&#30340;&#39034;&#24207;&#25512;&#33616;&#65288;SESRec&#65289;&#26694;&#26550;&#65292;&#23427;&#21033;&#29992;&#29992;&#25143;&#30340;&#25628;&#32034;&#20852;&#36259;&#36827;&#34892;&#25512;&#33616;&#65292;&#36890;&#36807;&#21306;&#20998;S&#65286;R&#34892;&#20026;&#20013;&#30340;&#30456;&#20284;&#21644;&#19981;&#30456;&#20284;&#34920;&#31034;&#65292;&#20351;S&#65286;R&#29305;&#24449;&#33021;&#22815;&#26356;&#22909;&#22320;&#21457;&#25381;&#20854;&#29420;&#29305;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#22312;&#32447;&#26381;&#21153;&#25552;&#20379;&#21830;&#65292;&#22914;&#22312;&#32447;&#36141;&#29289;&#24179;&#21488;&#36890;&#24120;&#25552;&#20379;&#25628;&#32034;&#21644;&#25512;&#33616;&#65288;S&#65286;R&#65289;&#26381;&#21153;&#20197;&#28385;&#36275;&#19981;&#21516;&#30340;&#29992;&#25143;&#38656;&#27714;&#12290;&#24456;&#23569;&#26377;&#20219;&#20309;&#26377;&#25928;&#30340;&#25163;&#27573;&#23558;&#26469;&#33258;S&#65286;R&#26381;&#21153;&#30340;&#29992;&#25143;&#34892;&#20026;&#25968;&#25454;&#32467;&#21512;&#36215;&#26469;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#26041;&#27861;&#35201;&#20040;&#20165;&#23558;S&#65286;R&#34892;&#20026;&#21333;&#29420;&#22788;&#29702;&#65292;&#35201;&#20040;&#36890;&#36807;&#32858;&#21512;&#20004;&#20010;&#26381;&#21153;&#30340;&#25968;&#25454;&#26469;&#32852;&#21512;&#20248;&#21270;&#23427;&#20204;&#65292;&#24573;&#30053;&#20102;S&#65286;R&#20013;&#29992;&#25143;&#24847;&#22270;&#21487;&#20197;&#26377;&#25130;&#28982;&#19981;&#21516;&#30340;&#20107;&#23454;&#12290;&#22312;&#25105;&#20204;&#30340;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25628;&#32034;&#22686;&#24378;&#30340;&#39034;&#24207;&#25512;&#33616;&#65288;SESRec&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#21306;&#20998;S&#65286;R&#34892;&#20026;&#20013;&#30340;&#30456;&#20284;&#21644;&#19981;&#30456;&#20284;&#34920;&#31034;&#65292;&#21033;&#29992;&#29992;&#25143;&#30340;&#25628;&#32034;&#20852;&#36259;&#36827;&#34892;&#25512;&#33616;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;SESRec&#39318;&#20808;&#26681;&#25454;&#29992;&#25143;&#30340;&#26597;&#35810;-&#39033;&#30446;&#20132;&#20114;&#26469;&#23545;&#40784;&#26597;&#35810;&#21644;&#39033;&#30446;&#23884;&#20837;&#20197;&#35745;&#31639;&#23427;&#20204;&#30340;&#30456;&#20284;&#24615;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;&#20004;&#20010;&#36716;&#25442;&#22120;&#32534;&#30721;&#22120;&#26469;&#29420;&#31435;&#22320;&#23398;&#20064;S&#65286;R&#34892;&#20026;&#30340;&#19978;&#19979;&#25991;&#34920;&#31034;&#12290;&#26368;&#21518;&#65292;&#35774;&#35745;&#20102;&#23545;&#27604;&#23398;&#20064;&#20219;&#21153;&#20197;&#23398;&#20064;&#25628;&#32032;&#29305;&#24449;&#34920;&#31034;&#21644;&#25512;&#33616;&#29305;&#24449;&#34920;&#31034;&#30340;&#30456;&#20284;&#24230;&#36317;&#31163;&#65292;&#20351;&#24471;S&#65286;R&#29305;&#24449;&#33021;&#22815;&#26356;&#22909;&#22320;&#21457;&#25381;&#20854;&#29420;&#29305;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern online service providers such as online shopping platforms often provide both search and recommendation (S&amp;R) services to meet different user needs. Rarely has there been any effective means of incorporating user behavior data from both S&amp;R services. Most existing approaches either simply treat S&amp;R behaviors separately, or jointly optimize them by aggregating data from both services, ignoring the fact that user intents in S&amp;R can be distinctively different. In our paper, we propose a Search-Enhanced framework for the Sequential Recommendation (SESRec) that leverages users' search interests for recommendation, by disentangling similar and dissimilar representations within S&amp;R behaviors. Specifically, SESRec first aligns query and item embeddings based on users' query-item interactions for the computations of their similarities. Two transformer encoders are used to learn the contextual representations of S&amp;R behaviors independently. Then a contrastive learning task is designed to 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26816;&#32034;&#22686;&#24378;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#28176;&#36827;&#24335;&#23494;&#38598;&#26816;&#32034;&#20174;&#36890;&#29992;&#39046;&#22495;&#30340;&#26080;&#26631;&#31614;&#35821;&#26009;&#24211;&#20013;&#21019;&#24314;&#35757;&#32451;&#25968;&#25454;&#65292;&#23454;&#29616;&#20102;&#38646;&#26679;&#26412;&#25991;&#26412;&#20998;&#31867;&#65292;&#30456;&#36739;&#20110;&#26368;&#24378;&#30340;&#22522;&#32447;&#27169;&#22411;&#25552;&#39640;&#20102;4.3%&#30340;&#24615;&#33021;&#65292;&#19982;&#20351;&#29992;&#22823;&#22411;NLG&#27169;&#22411;&#30340;&#22522;&#32447;&#30456;&#27604;&#33410;&#30465;&#20102;&#32422;70&#65285;&#30340;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2305.10703</link><description>&lt;p&gt;
ReGen: &#36890;&#36807;&#28176;&#36827;&#24335;&#23494;&#38598;&#26816;&#32034;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#30340;&#38646;&#26679;&#26412;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ReGen: Zero-Shot Text Classification via Training Data Generation with Progressive Dense Retrieval. (arXiv:2305.10703v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10703
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26816;&#32034;&#22686;&#24378;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#28176;&#36827;&#24335;&#23494;&#38598;&#26816;&#32034;&#20174;&#36890;&#29992;&#39046;&#22495;&#30340;&#26080;&#26631;&#31614;&#35821;&#26009;&#24211;&#20013;&#21019;&#24314;&#35757;&#32451;&#25968;&#25454;&#65292;&#23454;&#29616;&#20102;&#38646;&#26679;&#26412;&#25991;&#26412;&#20998;&#31867;&#65292;&#30456;&#36739;&#20110;&#26368;&#24378;&#30340;&#22522;&#32447;&#27169;&#22411;&#25552;&#39640;&#20102;4.3%&#30340;&#24615;&#33021;&#65292;&#19982;&#20351;&#29992;&#22823;&#22411;NLG&#27169;&#22411;&#30340;&#22522;&#32447;&#30456;&#27604;&#33410;&#30465;&#20102;&#32422;70&#65285;&#30340;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21457;&#23637;&#65292;&#38646;&#26679;&#26412;&#23398;&#20064;&#22312;&#21508;&#31181;NLP&#20219;&#21153;&#20013;&#21463;&#21040;&#20102;&#35768;&#22810;&#20851;&#27880;&#12290;&#19982;&#20197;&#24448;&#20351;&#29992;&#25968;&#21313;&#20159;&#32423;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#30340;&#26694;&#26550;&#65292;&#20174;&#36890;&#29992;&#39046;&#22495;&#30340;&#26080;&#26631;&#31614;&#35821;&#26009;&#24211;&#20013;&#21019;&#24314;&#35757;&#32451;&#25968;&#25454;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#39318;&#20808;&#36827;&#34892;&#23545;&#27604;&#39044;&#35757;&#32451;&#65292;&#20351;&#29992;&#31867;&#21035;&#25551;&#36848;&#24615;&#35805;&#35821;&#23398;&#20064;&#20102;&#19968;&#20010;&#26080;&#30417;&#30563;&#30340;&#23494;&#38598;&#26816;&#32034;&#22120;&#20197;&#25552;&#21462;&#26368;&#30456;&#20851;&#30340;&#25991;&#26723;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#20004;&#31181;&#31616;&#21333;&#30340;&#31574;&#30053;&#65292;&#21363;&#23637;&#31034;&#22686;&#24378;&#30340;&#35805;&#35821;&#29983;&#25104;&#21644;&#33258;&#19968;&#33268;&#24615;&#24341;&#23548;&#36807;&#28388;&#65292;&#20197;&#25552;&#39640;&#25968;&#25454;&#38598;&#30340;&#20027;&#39064;&#35206;&#30422;&#29575;&#65292;&#21516;&#26102;&#21024;&#38500;&#22122;&#22768;&#26679;&#26412;&#12290;&#23545;&#20061;&#20010;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;REGEN&#30456;&#36739;&#20110;&#26368;&#24378;&#30340;&#22522;&#32447;&#27169;&#22411;&#25552;&#39640;&#20102;4.3%&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#19982;&#20351;&#29992;&#22823;&#22411;NLG&#27169;&#22411;&#30340;&#22522;&#32447;&#30456;&#27604;&#33410;&#30465;&#20102;&#32422;70&#65285;&#30340;&#26102;&#38388;&#12290;&#27492;&#22806;&#65292;REGEN&#21487;&#20197;&#33258;&#28982;&#22320;&#19982;&#26368;&#36817;&#25552;&#20986;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the development of large language models (LLMs), zero-shot learning has attracted much attention for various NLP tasks. Different from prior works that generate training data with billion-scale natural language generation (NLG) models, we propose a retrieval-enhanced framework to create training data from a general-domain unlabeled corpus. To realize this, we first conduct contrastive pretraining to learn an unsupervised dense retriever for extracting the most relevant documents using class-descriptive verbalizers. We then further propose two simple strategies, namely Verbalizer Augmentation with Demonstrations and Self-consistency Guided Filtering to improve the topic coverage of the dataset while removing noisy examples. Experiments on nine datasets demonstrate that REGEN achieves 4.3% gain over the strongest baselines and saves around 70% of the time compared to baselines using large NLG models. Besides, REGEN can be naturally integrated with recently proposed large language mo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;&#29983;&#25104;&#30340;&#25968;&#25454;&#22686;&#24378;&#26694;&#26550;BioAug&#65292;&#29992;&#20110;&#20302;&#36164;&#28304;&#29983;&#29289;&#21307;&#23398;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12290;BioAug&#24314;&#31435;&#22312;BART&#19978;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#30340;&#23631;&#34109;&#21644;&#30693;&#35782;&#22686;&#24378;&#36827;&#34892;&#35757;&#32451;&#12290;&#23454;&#39564;&#23637;&#31034;&#20102;BioAug&#22312;5&#20010;&#22522;&#20934;BioNER&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#19988;&#34920;&#29616;&#20248;&#20110;&#25152;&#26377;&#22522;&#32447;&#12290;</title><link>http://arxiv.org/abs/2305.10647</link><description>&lt;p&gt;
BioAug&#65306;&#22522;&#20110;&#26465;&#20214;&#29983;&#25104;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#29992;&#20110;&#20302;&#36164;&#28304;&#29983;&#29289;&#21307;&#23398;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
BioAug: Conditional Generation based Data Augmentation for Low-Resource Biomedical NER. (arXiv:2305.10647v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10647
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;&#29983;&#25104;&#30340;&#25968;&#25454;&#22686;&#24378;&#26694;&#26550;BioAug&#65292;&#29992;&#20110;&#20302;&#36164;&#28304;&#29983;&#29289;&#21307;&#23398;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12290;BioAug&#24314;&#31435;&#22312;BART&#19978;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#30340;&#23631;&#34109;&#21644;&#30693;&#35782;&#22686;&#24378;&#36827;&#34892;&#35757;&#32451;&#12290;&#23454;&#39564;&#23637;&#31034;&#20102;BioAug&#22312;5&#20010;&#22522;&#20934;BioNER&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#19988;&#34920;&#29616;&#20248;&#20110;&#25152;&#26377;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;(BioNER)&#26159;&#20174;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#20013;&#35782;&#21035;&#21629;&#21517;&#23454;&#20307;&#30340;&#22522;&#26412;&#20219;&#21153;&#12290;&#30001;&#20110;&#27880;&#37322;&#38656;&#35201;&#39640;&#24230;&#19987;&#19994;&#21270;&#21644;&#19987;&#19994;&#30693;&#35782;&#65292;BioNER &#36973;&#21463;&#30528;&#20005;&#37325;&#30340;&#25968;&#25454;&#31232;&#32570;&#21644;&#32570;&#20047;&#39640;&#36136;&#37327;&#26631;&#35760;&#25968;&#25454;&#30340;&#22256;&#25200;&#12290;&#23613;&#31649;&#25968;&#25454;&#22686;&#24378;&#22312;&#20302;&#36164;&#28304;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#26041;&#38754;&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#39640;&#25928;&#30340;&#65292;&#20294;&#29616;&#26377;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#19981;&#33021;&#20026;BioNER&#29983;&#25104;&#30495;&#23454;&#19988;&#22810;&#26679;&#21270;&#30340;&#22686;&#24378;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#22686;&#24378;&#26694;&#26550;BioAug&#65292;&#29992;&#20110;&#20302;&#36164;&#28304;BioNER&#12290;BioAug&#24314;&#31435;&#22312;BART&#19978;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#30340;&#23631;&#34109;&#21644;&#30693;&#35782;&#22686;&#24378;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#19968;&#31181;&#26032;&#30340;&#25991;&#26412;&#37325;&#26500;&#20219;&#21153;&#12290;&#22312;&#35757;&#32451;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#26377;&#26465;&#20214;&#30340;&#29983;&#25104;&#24182;&#22312;&#19982;&#35757;&#32451;&#38454;&#27573;&#31867;&#20284;&#30340;&#26377;&#36873;&#25321;&#24615;&#22320;&#25439;&#22351;&#25991;&#26412;&#30340;&#26465;&#20214;&#19979;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#22686;&#24378;&#12290;&#25105;&#20204;&#22312;5&#20010;&#22522;&#20934;BioNER&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;BioAug&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#34920;&#26126;BioAug&#27604;&#25152;&#26377;&#22522;&#32447;&#37117;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Biomedical Named Entity Recognition (BioNER) is the fundamental task of identifying named entities from biomedical text. However, BioNER suffers from severe data scarcity and lacks high-quality labeled data due to the highly specialized and expert knowledge required for annotation. Though data augmentation has shown to be highly effective for low-resource NER in general, existing data augmentation techniques fail to produce factual and diverse augmentations for BioNER. In this paper, we present BioAug, a novel data augmentation framework for low-resource BioNER. BioAug, built on BART, is trained to solve a novel text reconstruction task based on selective masking and knowledge augmentation. Post training, we perform conditional generation and generate diverse augmentations conditioning BioAug on selectively corrupted text similar to the training stage. We demonstrate the effectiveness of BioAug on 5 benchmark BioNER datasets and show that BioAug outperforms all our baselines by a signi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34394;&#25311;&#37051;&#23621;&#32593;&#32476;(VNC)&#65292;&#29992;&#20110;&#35299;&#20915;&#30693;&#35782;&#22270;&#35889;&#23436;&#25104;&#20013;&#26410;&#30693;&#23454;&#20307;&#34920;&#31034;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35268;&#21017;&#25366;&#25496;&#12289;&#35268;&#21017;&#25512;&#29702;&#21644;&#23884;&#20837;&#19977;&#20010;&#38454;&#27573;&#65292;&#23454;&#29616;&#23545;&#35268;&#21017;&#38388;&#30456;&#20851;&#24615;&#36827;&#34892;&#24314;&#27169;&#12290;</title><link>http://arxiv.org/abs/2305.10531</link><description>&lt;p&gt;
&#36845;&#20195;&#23398;&#20064;&#20855;&#26377;&#35268;&#21017;&#38388;&#30456;&#20851;&#24615;&#30340;&#26410;&#30693;&#23454;&#20307;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Iteratively Learning Representations for Unseen Entities with Inter-Rule Correlations. (arXiv:2305.10531v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10531
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34394;&#25311;&#37051;&#23621;&#32593;&#32476;(VNC)&#65292;&#29992;&#20110;&#35299;&#20915;&#30693;&#35782;&#22270;&#35889;&#23436;&#25104;&#20013;&#26410;&#30693;&#23454;&#20307;&#34920;&#31034;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35268;&#21017;&#25366;&#25496;&#12289;&#35268;&#21017;&#25512;&#29702;&#21644;&#23884;&#20837;&#19977;&#20010;&#38454;&#27573;&#65292;&#23454;&#29616;&#23545;&#35268;&#21017;&#38388;&#30456;&#20851;&#24615;&#36827;&#34892;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#23436;&#25104;(KGC)&#30340;&#26368;&#26032;&#30740;&#31350;&#20391;&#37325;&#20110;&#23398;&#20064;&#30693;&#35782;&#22270;&#35889;&#20013;&#23454;&#20307;&#21644;&#20851;&#31995;&#30340;&#23884;&#20837;&#12290;&#36825;&#20123;&#23884;&#20837;&#26041;&#27861;&#35201;&#27714;&#25152;&#26377;&#27979;&#35797;&#23454;&#20307;&#22312;&#35757;&#32451;&#26102;&#34987;&#35266;&#23519;&#21040;&#65292;&#23548;&#33268;&#23545;&#36229;&#20986;&#30693;&#35782;&#22270;&#35889;&#65288;OOKG&#65289;&#23454;&#20307;&#30340;&#32791;&#26102;&#37325;&#26032;&#35757;&#32451;&#36807;&#31243;&#12290;&#20026;&#35299;&#20915;&#27492;&#38382;&#39064;&#65292;&#24403;&#21069;&#24402;&#32435;&#30693;&#35782;&#23884;&#20837;&#26041;&#27861;&#37319;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#36890;&#36807;&#32858;&#21512;&#24050;&#30693;&#37051;&#23621;&#30340;&#20449;&#24687;&#26469;&#34920;&#31034;&#26410;&#30693;&#23454;&#20307;&#12290;&#20182;&#20204;&#38754;&#20020;&#19977;&#20010;&#37325;&#35201;&#25361;&#25112;:i)&#25968;&#25454;&#31232;&#30095;&#24615;&#65292;ii)&#30693;&#35782;&#22270;&#35889;&#20013;&#23384;&#22312;&#22797;&#26434;&#27169;&#24335;(&#22914;&#35268;&#21017;&#38388;&#30456;&#20851;&#24615;)&#65292;iii)&#35268;&#21017;&#25366;&#25496;&#12289;&#35268;&#21017;&#25512;&#29702;&#21644;&#23884;&#20837;&#20043;&#38388;&#23384;&#22312;&#20132;&#20114;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#19977;&#20010;&#38454;&#27573;&#30340;&#20855;&#26377;&#35268;&#21017;&#38388;&#30456;&#20851;&#24615;&#30340;&#34394;&#25311;&#37051;&#23621;&#32593;&#32476;(VNC):i)&#35268;&#21017;&#25366;&#25496;&#65292;ii)&#35268;&#21017;&#25512;&#29702;&#65292;&#21644;iii)&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work on knowledge graph completion (KGC) focused on learning embeddings of entities and relations in knowledge graphs. These embedding methods require that all test entities are observed at training time, resulting in a time-consuming retraining process for out-of-knowledge-graph (OOKG) entities. To address this issue, current inductive knowledge embedding methods employ graph neural networks (GNNs) to represent unseen entities by aggregating information of known neighbors. They face three important challenges: (i) data sparsity, (ii) the presence of complex patterns in knowledge graphs (e.g., inter-rule correlations), and (iii) the presence of interactions among rule mining, rule inference, and embedding. In this paper, we propose a virtual neighbor network with inter-rule correlations (VNC) that consists of three stages: (i) rule mining, (ii) rule inference, and (iii) embedding. In the rule mining process, to identify complex patterns in knowledge graphs, both logic rules and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;GPT-4&#36827;&#34892;&#22823;&#35268;&#27169;&#25991;&#26412;&#20998;&#26512;&#65292;&#22312;US AI&#19987;&#21033;&#20013;&#21457;&#29616;&#20844;&#20849;&#20215;&#20540;&#34920;&#36798;&#12290;&#37319;&#29992;&#39640;&#32423;&#24067;&#23572;&#26597;&#35810;&#25910;&#38598;&#20102;154,934&#20010;&#19987;&#21033;&#25991;&#26723;&#65292;&#24182;&#19982;USPTO&#30340;&#23436;&#25972;&#19987;&#21033;&#25991;&#26412;&#21512;&#24182;&#12290;&#24471;&#20986;5.4&#30334;&#19975;&#21477;&#23376;&#30340;&#35821;&#26009;&#24211;&#65292;&#20351;&#29992;&#26694;&#26550;&#20197;&#21450;GPT-4&#25552;&#31034;&#36827;&#34892;&#26631;&#35760;&#21644;&#29702;&#24615;&#21270;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#24456;&#20934;&#30830;&#12290;</title><link>http://arxiv.org/abs/2305.10383</link><description>&lt;p&gt;
&#20351;&#29992;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22823;&#35268;&#27169;&#25991;&#26412;&#20998;&#26512;&#65306;&#22312;AI&#19987;&#21033;&#20013;&#21457;&#29616;&#20844;&#20849;&#20215;&#20540;&#34920;&#36798;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Large-Scale Text Analysis Using Generative Language Models: A Case Study in Discovering Public Value Expressions in AI Patents. (arXiv:2305.10383v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10383
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;GPT-4&#36827;&#34892;&#22823;&#35268;&#27169;&#25991;&#26412;&#20998;&#26512;&#65292;&#22312;US AI&#19987;&#21033;&#20013;&#21457;&#29616;&#20844;&#20849;&#20215;&#20540;&#34920;&#36798;&#12290;&#37319;&#29992;&#39640;&#32423;&#24067;&#23572;&#26597;&#35810;&#25910;&#38598;&#20102;154,934&#20010;&#19987;&#21033;&#25991;&#26723;&#65292;&#24182;&#19982;USPTO&#30340;&#23436;&#25972;&#19987;&#21033;&#25991;&#26412;&#21512;&#24182;&#12290;&#24471;&#20986;5.4&#30334;&#19975;&#21477;&#23376;&#30340;&#35821;&#26009;&#24211;&#65292;&#20351;&#29992;&#26694;&#26550;&#20197;&#21450;GPT-4&#25552;&#31034;&#36827;&#34892;&#26631;&#35760;&#21644;&#29702;&#24615;&#21270;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#24456;&#20934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#35760;&#25968;&#25454;&#23545;&#20110;&#35757;&#32451;&#25991;&#26412;&#20998;&#31867;&#22120;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#23545;&#20110;&#22797;&#26434;&#21644;&#25277;&#35937;&#30340;&#27010;&#24565;&#32780;&#35328;&#65292;&#20934;&#30830;&#26631;&#35760;&#24120;&#24120;&#24456;&#38590;&#23454;&#29616;&#12290;&#26412;&#25991;&#37319;&#29992;&#19968;&#31181;&#26032;&#39062;&#26041;&#27861;&#65292;&#20351;&#29992;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#65288;GPT-4&#65289;&#36827;&#34892;&#22823;&#35268;&#27169;&#25991;&#26412;&#20998;&#26512;&#30340;&#26631;&#35760;&#21644;&#29702;&#24615;&#21270;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#24212;&#29992;&#20110;&#22312;&#32654;&#22269;AI&#19987;&#21033;&#20013;&#21457;&#29616;&#20844;&#20849;&#20215;&#20540;&#34920;&#36798;&#30340;&#20219;&#21153;&#19978;&#12290;&#25105;&#20204;&#20351;&#29992;&#22312;InnovationQ+&#19978;&#25552;&#20132;&#30340;&#39640;&#32423;&#24067;&#23572;&#26597;&#35810;&#25910;&#38598;&#20102;&#19968;&#20010;&#21253;&#21547;154,934&#20010;&#19987;&#21033;&#25991;&#26723;&#30340;&#25968;&#25454;&#24211;&#65292;&#36825;&#20123;&#32467;&#26524;&#19982;&#26469;&#33258;USPTO&#30340;&#23436;&#25972;&#19987;&#21033;&#25991;&#26412;&#21512;&#24182;&#65292;&#24635;&#35745;5.4&#30334;&#19975;&#21477;&#23376;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#35782;&#21035;&#21644;&#26631;&#35760;&#36825;&#20123;AI&#19987;&#21033;&#21477;&#23376;&#20013;&#30340;&#20844;&#20849;&#20215;&#20540;&#34920;&#36798;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;GPT-4&#30340;&#25552;&#31034;&#65292;&#20854;&#20013;&#21253;&#25324;&#25991;&#26412;&#20998;&#31867;&#30340;&#23450;&#20041;&#12289;&#25351;&#23548;&#26041;&#38024;&#12289;&#31034;&#20363;&#21644;&#29702;&#24615;&#21270;&#12290;&#25105;&#20204;&#20351;&#29992;BLEU&#20998;&#25968;&#21644;&#20027;&#39064;&#24314;&#27169;&#35780;&#20272;&#20102;GPT-4&#29983;&#25104;&#30340;&#26631;&#31614;&#21644;&#29702;&#24615;&#21270;&#30340;&#36136;&#37327;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#26159;&#20934;&#30830;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Labeling data is essential for training text classifiers but is often difficult to accomplish accurately, especially for complex and abstract concepts. Seeking an improved method, this paper employs a novel approach using a generative language model (GPT-4) to produce labels and rationales for large-scale text analysis. We apply this approach to the task of discovering public value expressions in US AI patents. We collect a database comprising 154,934 patent documents using an advanced Boolean query submitted to InnovationQ+. The results are merged with full patent text from the USPTO, resulting in 5.4 million sentences. We design a framework for identifying and labeling public value expressions in these AI patent sentences. A prompt for GPT-4 is developed which includes definitions, guidelines, examples, and rationales for text classification. We evaluate the quality of the labels and rationales produced by GPT-4 using BLEU scores and topic modeling and find that they are accurate, di
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;PropensityNet&#65292;&#29992;&#20110;&#22312;&#24378;&#26085;&#24535;&#35760;&#24405;&#31574;&#30053;&#19979;&#36827;&#34892;&#26080;&#20559;&#23398;&#20064;&#25490;&#21517;&#65288;ULTR&#65289;&#30340;&#20542;&#21521;&#24615;&#20272;&#35745;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;ULTR&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.09918</link><description>&lt;p&gt;
&#26080;&#20559;&#20542;&#21521;&#20272;&#35745;&#29992;&#20110;&#26080;&#20559;&#25490;&#24207;
&lt;/p&gt;
&lt;p&gt;
Unconfounded Propensity Estimation for Unbiased Ranking. (arXiv:2305.09918v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09918
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;PropensityNet&#65292;&#29992;&#20110;&#22312;&#24378;&#26085;&#24535;&#35760;&#24405;&#31574;&#30053;&#19979;&#36827;&#34892;&#26080;&#20559;&#23398;&#20064;&#25490;&#21517;&#65288;ULTR&#65289;&#30340;&#20542;&#21521;&#24615;&#20272;&#35745;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;ULTR&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#20559;&#23398;&#20064;&#25490;&#21517;&#65288;ULTR&#65289;&#30340;&#30446;&#26631;&#26159;&#21033;&#29992;&#38544;&#21547;&#30340;&#29992;&#25143;&#21453;&#39304;&#26469;&#20248;&#21270;&#23398;&#20064;&#25490;&#24207;&#31995;&#32479;&#12290;&#22312;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#20013;&#65292;&#33258;&#21160;ULTR&#31639;&#27861;&#22312;&#23454;&#36341;&#20013;&#22240;&#20854;&#21331;&#36234;&#30340;&#24615;&#33021;&#21644;&#20302;&#37096;&#32626;&#25104;&#26412;&#32780;&#21463;&#21040;&#20851;&#27880;&#65292;&#35813;&#31639;&#27861;&#21516;&#26102;&#23398;&#20064;&#29992;&#25143;&#20559;&#24046;&#27169;&#22411;&#65288;&#21363;&#20542;&#21521;&#24615;&#27169;&#22411;&#65289;&#21644;&#26080;&#20559;&#25490;&#21517;&#22120;&#12290;&#23613;&#31649;&#35813;&#31639;&#27861;&#22312;&#29702;&#35770;&#19978;&#26159;&#21487;&#38752;&#30340;&#65292;&#20294;&#20854;&#26377;&#25928;&#24615;&#36890;&#24120;&#22312;&#24369;&#26085;&#24535;&#35760;&#24405;&#31574;&#30053;&#19979;&#36827;&#34892;&#39564;&#35777;&#65292;&#20854;&#20013;&#25490;&#21517;&#27169;&#22411;&#20960;&#20046;&#26080;&#27861;&#26681;&#25454;&#19982;&#26597;&#35810;&#30456;&#20851;&#24615;&#26469;&#23545;&#25991;&#26723;&#36827;&#34892;&#25490;&#21517;&#12290;&#28982;&#32780;&#65292;&#24403;&#26085;&#24535;&#35760;&#24405;&#31574;&#30053;&#24456;&#24378;&#26102;&#65292;&#20363;&#22914;&#24037;&#19994;&#37096;&#32626;&#30340;&#25490;&#21517;&#31574;&#30053;&#65292;&#25152;&#25253;&#21578;&#30340;&#26377;&#25928;&#24615;&#26080;&#27861;&#20877;&#29616;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#20174;&#22240;&#26524;&#35282;&#24230;&#35843;&#26597;ULTR&#65292;&#24182;&#25581;&#31034;&#19968;&#20010;&#36127;&#38754;&#32467;&#26524;&#65306;&#29616;&#26377;&#30340;ULTR&#31639;&#27861;&#26410;&#33021;&#35299;&#20915;&#30001;&#26597;&#35810;-&#25991;&#26723;&#30456;&#20851;&#24615;&#28151;&#28102;&#23548;&#33268;&#30340;&#20542;&#21521;&#24615;&#39640;&#20272;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21453;&#38376;&#35843;&#25972;&#30340;&#26032;&#30340;&#23398;&#20064;&#30446;&#26631;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PropensityNet&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#24378;&#26085;&#24535;&#35760;&#24405;&#31574;&#30053;&#19979;&#20026;ULTR&#20272;&#35745;&#26080;&#20559;&#30340;&#20542;&#21521;&#24615;&#20998;&#25968;&#12290;&#22810;&#20010;&#25968;&#25454;&#38598;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;PropensityNet&#22312;&#24378;&#26085;&#24535;&#35760;&#24405;&#31574;&#30053;&#21644;&#24369;&#26085;&#24535;&#35760;&#24405;&#31574;&#30053;&#19979;&#22343;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;ULTR&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of unbiased learning to rank~(ULTR) is to leverage implicit user feedback for optimizing learning-to-rank systems. Among existing solutions, automatic ULTR algorithms that jointly learn user bias models (\ie propensity models) with unbiased rankers have received a lot of attention due to their superior performance and low deployment cost in practice. Despite their theoretical soundness, the effectiveness is usually justified under a weak logging policy, where the ranking model can barely rank documents according to their relevance to the query. However, when the logging policy is strong, e.g., an industry-deployed ranking policy, the reported effectiveness cannot be reproduced. In this paper, we first investigate ULTR from a causal perspective and uncover a negative result: existing ULTR algorithms fail to address the issue of propensity overestimation caused by the query-document relevance confounder. Then, we propose a new learning objective based on backdoor adjustment and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19981;&#21464;&#30340;&#21327;&#21516;&#36807;&#28388;(InvCF)&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#21327;&#21516;&#36807;&#28388;&#27169;&#22411;&#26131;&#21463;&#27969;&#34892;&#24230;&#20998;&#24067;&#21464;&#21270;&#24433;&#21709;&#30340;&#38382;&#39064;&#65292;&#36825;&#19968;&#26041;&#27861;&#19981;&#38656;&#35201;&#20808;&#20102;&#35299;&#27979;&#35797;&#38598;&#30340;&#27969;&#34892;&#24230;&#20998;&#24067;&#65292;&#33021;&#22815;&#24544;&#23454;&#22320;&#25581;&#31034;&#29992;&#25143;&#30340;&#20559;&#22909;&#21644;&#27969;&#34892;&#24230;&#35821;&#20041;&#12290;</title><link>http://arxiv.org/abs/2302.05328</link><description>&lt;p&gt;
&#19981;&#21464;&#30340;&#21327;&#21516;&#36807;&#28388;&#23545;&#25239;&#27969;&#34892;&#24230;&#20998;&#24067;&#30340;&#21464;&#21270;
&lt;/p&gt;
&lt;p&gt;
Invariant Collaborative Filtering to Popularity Distribution Shift. (arXiv:2302.05328v3 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05328
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19981;&#21464;&#30340;&#21327;&#21516;&#36807;&#28388;(InvCF)&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#21327;&#21516;&#36807;&#28388;&#27169;&#22411;&#26131;&#21463;&#27969;&#34892;&#24230;&#20998;&#24067;&#21464;&#21270;&#24433;&#21709;&#30340;&#38382;&#39064;&#65292;&#36825;&#19968;&#26041;&#27861;&#19981;&#38656;&#35201;&#20808;&#20102;&#35299;&#27979;&#35797;&#38598;&#30340;&#27969;&#34892;&#24230;&#20998;&#24067;&#65292;&#33021;&#22815;&#24544;&#23454;&#22320;&#25581;&#31034;&#29992;&#25143;&#30340;&#20559;&#22909;&#21644;&#27969;&#34892;&#24230;&#35821;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#21327;&#21516;&#36807;&#28388;(collaborative filtering, CF)&#27169;&#22411;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#20294;&#30001;&#20110;&#27969;&#34892;&#24230;&#20998;&#24067;&#30340;&#21464;&#21270;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#26222;&#36941;&#19988;&#19981;&#21487;&#36991;&#20813;&#65292;&#22240;&#27492;&#36825;&#20123;&#27169;&#22411;&#23384;&#22312;&#20005;&#37325;&#30340;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#22823;&#22810;&#25968;&#20027;&#27969;&#30340;&#28040;&#38500;&#27969;&#34892;&#24230;&#20559;&#35265;&#30340;&#31574;&#30053;&#38656;&#35201;&#20107;&#20808;&#30693;&#36947;&#27979;&#35797;&#20998;&#24067;&#65292;&#20197;&#30830;&#23450;&#20559;&#35265;&#31243;&#24230;&#24182;&#36827;&#19968;&#27493;&#23398;&#20064;&#19982;&#27969;&#34892;&#24230;&#32416;&#32544;&#22312;&#19968;&#36215;&#30340;&#34920;&#31034;&#26469;&#20943;&#36731;&#20559;&#35265;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#30446;&#26631;&#27979;&#35797;&#38598;&#20013;&#34920;&#29616;&#20986;&#26126;&#26174;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#20294;&#22312;&#19981;&#30693;&#36947;&#27969;&#34892;&#24230;&#20998;&#24067;&#30340;&#24773;&#20917;&#19979;&#65292;&#21364;&#20250;&#22823;&#22823;&#20559;&#31163;&#29992;&#25143;&#30495;&#27491;&#30340;&#20852;&#36259;&#25512;&#33616;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#19981;&#21464;&#30340;&#21327;&#21516;&#36807;&#28388;(InvCF)&#65292;&#29992;&#20110;&#21457;&#29616;&#33021;&#22815;&#24544;&#23454;&#22320;&#25581;&#31034;&#28508;&#22312;&#20559;&#22909;&#21644;&#27969;&#34892;&#24230;&#35821;&#20041;&#30340;&#35299;&#32806;&#34920;&#31034;&#65292;&#32780;&#19981;&#38656;&#35201;&#23545;&#27979;&#35797;&#20998;&#24067;&#20570;&#20986;&#20219;&#20309;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;
Collaborative Filtering (CF) models, despite their great success, suffer from severe performance drops due to popularity distribution shifts, where these changes are ubiquitous and inevitable in real-world scenarios. Unfortunately, most leading popularity debiasing strategies, rather than tackling the vulnerability of CF models to varying popularity distributions, require prior knowledge of the test distribution to identify the degree of bias and further learn the popularity-entangled representations to mitigate the bias. Consequently, these models result in significant performance benefits in the target test set, while dramatically deviating the recommendation from users' true interests without knowing the popularity distribution in advance. In this work, we propose a novel learning framework, Invariant Collaborative Filtering (InvCF), to discover disentangled representations that faithfully reveal the latent preference and popularity semantics without making any assumption about the 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AdaTask&#30340;&#20219;&#21153;&#24863;&#30693;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#22320;&#35843;&#25972;&#19981;&#21516;&#20219;&#21153;&#30340;&#23398;&#20064;&#29575;&#65292;&#20197;&#24179;&#34913;&#19981;&#21516;&#20219;&#21153;&#30340;&#37325;&#35201;&#24615;&#65292;&#20174;&#32780;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#19978;&#22987;&#32456;&#20248;&#20110;&#29616;&#26377;&#30340;MTL&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2211.15055</link><description>&lt;p&gt;
AdaTask: &#19968;&#31181;&#38754;&#21521;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#20219;&#21153;&#24863;&#30693;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
AdaTask: A Task-aware Adaptive Learning Rate Approach to Multi-task Learning. (arXiv:2211.15055v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15055
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AdaTask&#30340;&#20219;&#21153;&#24863;&#30693;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#22320;&#35843;&#25972;&#19981;&#21516;&#20219;&#21153;&#30340;&#23398;&#20064;&#29575;&#65292;&#20197;&#24179;&#34913;&#19981;&#21516;&#20219;&#21153;&#30340;&#37325;&#35201;&#24615;&#65292;&#20174;&#32780;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#19978;&#22987;&#32456;&#20248;&#20110;&#29616;&#26377;&#30340;MTL&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;MTL&#65289;&#27169;&#22411;&#24050;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#25512;&#33616;&#31995;&#32479;&#31561;&#39046;&#22495;&#23637;&#29616;&#20986;&#20196;&#20154;&#30633;&#30446;&#30340;&#32467;&#26524;&#12290;&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#26041;&#27861;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#22914;&#20309;&#22312;&#27599;&#20010;&#21442;&#25968;&#19978;&#24179;&#34913;&#19981;&#21516;&#20219;&#21153;&#20173;&#28982;&#19981;&#28165;&#26970;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#27599;&#20010;&#20219;&#21153;&#23545;&#35813;&#21442;&#25968;&#36827;&#34892;&#30340;&#24635;&#26356;&#26032;&#26469;&#34913;&#37327;&#21442;&#25968;&#30340;&#20219;&#21153;&#20248;&#21183;&#24230;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#25351;&#25968;&#34928;&#20943;&#30340;&#24179;&#22343;&#26356;&#26032;&#65288;AU&#65289;&#26469;&#35745;&#31639;&#27599;&#20010;&#20219;&#21153;&#22312;&#35813;&#21442;&#25968;&#19978;&#30340;&#24635;&#26356;&#26032;&#25968;&#12290;&#22522;&#20110;&#36825;&#19968;&#26032;&#39062;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#29616;&#26377;MTL&#26041;&#27861;&#20013;&#30340;&#35768;&#22810;&#21442;&#25968;&#65292;&#23588;&#20854;&#26159;&#22312;&#36739;&#39640;&#30340;&#20849;&#20139;&#23618;&#20013;&#30340;&#21442;&#25968;&#65292;&#20173;&#28982;&#21463;&#21040;&#19968;&#20010;&#25110;&#20960;&#20010;&#20219;&#21153;&#30340;&#25903;&#37197;&#12290;AU&#30340;&#25903;&#37197;&#20027;&#35201;&#26159;&#30001;&#20110;&#19968;&#20010;&#25110;&#20960;&#20010;&#20219;&#21153;&#30340;&#26799;&#24230;&#32047;&#31215;&#23548;&#33268;&#30340;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AdaTask&#30340;&#20219;&#21153;&#24863;&#30693;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#26041;&#27861;&#65292;&#20197;&#20998;&#31163;&#19981;&#21516;&#20219;&#21153;&#20043;&#38388;&#30340;&#32047;&#31215;&#26799;&#24230;&#65292;&#20174;&#32780;&#24179;&#34913;&#19981;&#21516;&#20219;&#21153;&#30340;&#37325;&#35201;&#24615;&#12290;AdaTask&#26681;&#25454;AU&#20540;&#33258;&#36866;&#24212;&#22320;&#35843;&#25972;&#19981;&#21516;&#20219;&#21153;&#30340;&#23398;&#20064;&#29575;&#65292;&#20197;&#24179;&#34913;&#19981;&#21516;&#20219;&#21153;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#19978;&#35780;&#20272;&#20102;AdaTask&#65292;&#24182;&#35777;&#26126;&#23427;&#22987;&#32456;&#20248;&#20110;&#29616;&#26377;&#30340;MTL&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-task learning (MTL) models have demonstrated impressive results in computer vision, natural language processing, and recommender systems. Even though many approaches have been proposed, how well these approaches balance different tasks on each parameter still remains unclear. In this paper, we propose to measure the task dominance degree of a parameter by the total updates of each task on this parameter. Specifically, we compute the total updates by the exponentially decaying Average of the squared Updates (AU) on a parameter from the corresponding task.Based on this novel metric, we observe that many parameters in existing MTL methods, especially those in the higher shared layers, are still dominated by one or several tasks. The dominance of AU is mainly due to the dominance of accumulative gradients from one or several tasks. Motivated by this, we propose a Task-wise Adaptive learning rate approach, AdaTask in short, to separate the \emph{accumulative gradients} and hence the l
&lt;/p&gt;</description></item></channel></rss>