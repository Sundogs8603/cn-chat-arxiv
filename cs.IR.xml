<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#23618;&#27880;&#24847;&#21147;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#22788;&#29702;&#22810;&#20851;&#31995;&#21644;&#22810;&#23454;&#20307;&#30340;&#22823;&#35268;&#27169;&#24322;&#26500;&#22270;&#12290;&#36890;&#36807;&#22522;&#20110;&#20004;&#20010;&#37325;&#35201;&#24615;&#32423;&#21035;&#30340;&#23618;&#27425;&#22270;&#20851;&#27880;&#26426;&#21046;&#65292;&#20197;&#20010;&#24615;&#21270;&#30340;&#26041;&#24335;&#27169;&#25311;&#33410;&#28857;-&#33410;&#28857;&#21644;&#20851;&#31995;-&#20851;&#31995;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2304.11533</link><description>&lt;p&gt;
&#21452;&#23618;&#27880;&#24847;&#21147;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Bi-Level Attention Graph Neural Networks. (arXiv:2304.11533v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11533
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#23618;&#27880;&#24847;&#21147;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#22788;&#29702;&#22810;&#20851;&#31995;&#21644;&#22810;&#23454;&#20307;&#30340;&#22823;&#35268;&#27169;&#24322;&#26500;&#22270;&#12290;&#36890;&#36807;&#22522;&#20110;&#20004;&#20010;&#37325;&#35201;&#24615;&#32423;&#21035;&#30340;&#23618;&#27425;&#22270;&#20851;&#27880;&#26426;&#21046;&#65292;&#20197;&#20010;&#24615;&#21270;&#30340;&#26041;&#24335;&#27169;&#25311;&#33410;&#28857;-&#33410;&#28857;&#21644;&#20851;&#31995;-&#20851;&#31995;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20855;&#26377;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#22312;&#21382;&#21490;&#19978;&#19968;&#30452;&#23616;&#38480;&#20110;&#23567;&#35268;&#27169;&#21516;&#36136;&#22270;(HoGs)&#12290;&#28982;&#32780;&#65292;&#22788;&#29702;&#24322;&#26500;&#22270;(HeGs)&#30340;GNNs&#65292;&#22312;&#22788;&#29702;&#27880;&#24847;&#21147;&#26041;&#38754;&#23384;&#22312;&#32570;&#38519;&#12290;&#22823;&#22810;&#25968;&#22788;&#29702;HeGs&#30340;GNNs&#21482;&#23398;&#20064;&#33410;&#28857;&#32423;&#21035;&#25110;&#20851;&#31995;&#32423;&#21035;&#30340;&#27880;&#24847;&#21147;&#65292;&#32780;&#19981;&#26159;&#20004;&#32773;&#20860;&#22791;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#39044;&#27979;HeGs&#20013;&#30340;&#37325;&#35201;&#23454;&#20307;&#21644;&#20851;&#31995;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#21363;&#20351;&#26159;&#29616;&#26377;&#23398;&#20064;&#20004;&#31181;&#32423;&#21035;&#27880;&#24847;&#21147;&#30340;&#26368;&#20339;&#26041;&#27861;&#65292;&#20063;&#23384;&#22312;&#20551;&#23450;&#22270;&#20851;&#31995;&#26159;&#29420;&#31435;&#30340;&#65292;&#24182;&#19988;&#20854;&#23398;&#20064;&#30340;&#27880;&#24847;&#21147;&#24573;&#30053;&#20102;&#36825;&#31181;&#20381;&#36182;&#20851;&#32852;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#27169;&#25311;&#22810;&#20851;&#31995;&#21644;&#22810;&#23454;&#20307;&#30340;&#22823;&#35268;&#27169;HeGs&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21452;&#23618;&#27880;&#24847;&#21147;&#22270;&#31070;&#32463;&#32593;&#32476;(BA-GNN)&#65292;&#36825;&#26159;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#31070;&#32463;&#32593;&#32476;(NNs)&#65292;&#36890;&#36807;&#22522;&#20110;&#20004;&#20010;&#37325;&#35201;&#24615;&#32423;&#21035;&#30340;&#23618;&#27425;&#22270;&#20851;&#27880;&#26426;&#21046;&#65292;&#20197;&#20010;&#24615;&#21270;&#30340;&#26041;&#24335;&#27169;&#25311;&#20102;&#33410;&#28857;-&#33410;&#28857;&#21644;&#20851;&#31995;-&#20851;&#31995;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#23398;&#20250;&#20102;&#32771;&#34385;&#22270;&#20851;&#31995;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent graph neural networks (GNNs) with the attention mechanism have historically been limited to small-scale homogeneous graphs (HoGs). However, GNNs handling heterogeneous graphs (HeGs), which contain several entity and relation types, all have shortcomings in handling attention. Most GNNs that learn graph attention for HeGs learn either node-level or relation-level attention, but not both, limiting their ability to predict both important entities and relations in the HeG. Even the best existing method that learns both levels of attention has the limitation of assuming graph relations are independent and that its learned attention disregards this dependency association. To effectively model both multi-relational and multi-entity large-scale HeGs, we present Bi-Level Attention Graph Neural Networks (BA-GNN), scalable neural networks (NNs) that use a novel bi-level graph attention mechanism. BA-GNN models both node-node and relation-relation interactions in a personalized way, by hier
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;TriSIM4Rec&#31639;&#27861;&#65292;&#23427;&#22522;&#20110;&#21160;&#24577;&#20132;&#20114;&#22270;&#65292;&#21516;&#26102;&#21033;&#29992;&#29992;&#25143;-&#29289;&#21697;&#20849;&#29616;&#12289;&#29992;&#25143;&#20132;&#20114;&#26102;&#24207;&#20449;&#24687;&#21644;&#29289;&#21697;&#23545;&#30340;&#36716;&#31227;&#27010;&#29575;&#19977;&#31181;&#32467;&#26500;&#20449;&#24687;&#65292;&#36827;&#32780;&#23454;&#29616;&#20102;&#26356;&#20934;&#30830;&#12289;&#21487;&#35299;&#37322;&#21644;&#20132;&#20114;&#24335;&#30340;&#25512;&#33616;&#12290;</title><link>http://arxiv.org/abs/2304.11528</link><description>&lt;p&gt;
&#19977;&#20803;&#32467;&#26500;&#20449;&#24687;&#24314;&#27169;&#29992;&#20110;&#20934;&#30830;&#12289;&#21487;&#35299;&#37322;&#21644;&#20132;&#20114;&#24335;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Triple Structural Information Modelling for Accurate, Explainable and Interactive Recommendation. (arXiv:2304.11528v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11528
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;TriSIM4Rec&#31639;&#27861;&#65292;&#23427;&#22522;&#20110;&#21160;&#24577;&#20132;&#20114;&#22270;&#65292;&#21516;&#26102;&#21033;&#29992;&#29992;&#25143;-&#29289;&#21697;&#20849;&#29616;&#12289;&#29992;&#25143;&#20132;&#20114;&#26102;&#24207;&#20449;&#24687;&#21644;&#29289;&#21697;&#23545;&#30340;&#36716;&#31227;&#27010;&#29575;&#19977;&#31181;&#32467;&#26500;&#20449;&#24687;&#65292;&#36827;&#32780;&#23454;&#29616;&#20102;&#26356;&#20934;&#30830;&#12289;&#21487;&#35299;&#37322;&#21644;&#20132;&#20114;&#24335;&#30340;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21160;&#24577;&#20132;&#20114;&#22270;&#20013;&#65292;&#29992;&#25143;&#19982;&#29289;&#21697;&#30340;&#20132;&#20114;&#36890;&#24120;&#36981;&#24490;&#24322;&#26500;&#27169;&#24335;&#65292;&#34920;&#31034;&#20026;&#19981;&#21516;&#30340;&#32467;&#26500;&#20449;&#24687;&#65292;&#22914;&#29992;&#25143;-&#29289;&#21697;&#20849;&#29616;&#12289;&#29992;&#25143;&#20132;&#20114;&#30340;&#26102;&#24207;&#20449;&#24687;&#21644;&#29289;&#21697;&#23545;&#30340;&#36716;&#31227;&#27010;&#29575;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#19981;&#33021;&#21516;&#26102;&#21033;&#29992;&#36825;&#19977;&#31181;&#32467;&#26500;&#20449;&#24687;&#65292;&#23548;&#33268;&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TriSIM4Rec&#65292;&#19968;&#31181;&#22522;&#20110;&#19977;&#20803;&#32467;&#26500;&#20449;&#24687;&#24314;&#27169;&#30340;&#21160;&#24577;&#20132;&#20114;&#22270;&#20934;&#30830;&#12289;&#21487;&#35299;&#37322;&#21644;&#20132;&#20114;&#24335;&#25512;&#33616;&#26041;&#27861;&#12290;&#20855;&#20307;&#22320;&#65292;TriSIM4Rec&#21253;&#25324;1)&#19968;&#20010;&#21160;&#24577;&#29702;&#24819;&#20302;&#36890;&#22270;&#28388;&#27874;&#22120;&#65292;&#36890;&#36807;&#22686;&#37327;&#22855;&#24322;&#20540;&#20998;&#35299;&#65288;SVD&#65289;&#21160;&#24577;&#22320;&#25366;&#25496;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#20013;&#30340;&#20849;&#29616;&#20449;&#24687;&#65307;2)&#19968;&#20010;&#26080;&#38656;&#21442;&#25968;&#30340;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#20197;&#26377;&#25928;&#12289;&#39640;&#25928;&#22320;&#25429;&#33719;&#29992;&#25143;&#20132;&#20114;&#30340;&#26102;&#24207;&#20449;&#24687;&#65307;&#21644;3)&#19968;&#20010;&#29289;&#21697;&#36716;&#31227;&#30697;&#38453;&#20197;&#23384;&#20648;&#29289;&#21697;&#23545;&#30340;&#36716;&#31227;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
In dynamic interaction graphs, user-item interactions usually follow heterogeneous patterns, represented by different structural information, such as user-item co-occurrence, sequential information of user interactions and the transition probabilities of item pairs. However, the existing methods cannot simultaneously leverage all three structural information, resulting in suboptimal performance. To this end, we propose TriSIM4Rec, a triple structural information modeling method for accurate, explainable and interactive recommendation on dynamic interaction graphs. Specifically, TriSIM4Rec consists of 1) a dynamic ideal low-pass graph filter to dynamically mine co-occurrence information in user-item interactions, which is implemented by incremental singular value decomposition (SVD); 2) a parameter-free attention module to capture sequential information of user interactions effectively and efficiently; and 3) an item transition matrix to store the transition probabilities of item pairs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20027;&#24352;&#23558;&#20135;&#21697;&#25628;&#32034;&#30475;&#20316;&#31243;&#24207;&#21512;&#25104;&#65292;&#30456;&#27604;&#21521;&#37327;&#31354;&#38388;&#27169;&#22411;&#26377;&#30528;&#37325;&#22823;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2304.11473</link><description>&lt;p&gt;
(&#21521;&#37327;)&#31354;&#38388;&#19981;&#26159;&#26368;&#21518;&#30340;&#30086;&#22495;&#65306;&#23558;&#20135;&#21697;&#25628;&#32034;&#30475;&#20316;&#31243;&#24207;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
(Vector) Space is Not the Final Frontier: Product Search as Program Synthesis. (arXiv:2304.11473v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11473
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20027;&#24352;&#23558;&#20135;&#21697;&#25628;&#32034;&#30475;&#20316;&#31243;&#24207;&#21512;&#25104;&#65292;&#30456;&#27604;&#21521;&#37327;&#31354;&#38388;&#27169;&#22411;&#26377;&#30528;&#37325;&#22823;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#30005;&#23376;&#21830;&#21153;&#30340;&#19981;&#26029;&#22686;&#38271;&#65292;&#24040;&#39069;&#25237;&#36164;&#29992;&#20110;&#20449;&#24687;&#26816;&#32034;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20063;&#38543;&#20043;&#32780;&#26469;&#12290;&#34429;&#28982;&#21521;&#37327;&#31354;&#38388;&#27169;&#22411;&#20027;&#23472;&#20102;&#20135;&#21697;&#25628;&#32034;&#20013;&#30340;&#26816;&#32034;&#27169;&#22411;&#65292;&#20294;&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#30340;&#20986;&#29616;&#65292;&#21521;&#37327;&#21270;&#26412;&#36523;&#20063;&#21457;&#29983;&#20102;&#24040;&#22823;&#21464;&#21270;&#12290;&#25105;&#20204;&#30340;&#31435;&#22330;&#35770;&#25991;&#20197;&#30456;&#21453;&#30340;&#26041;&#24335;&#20027;&#24352;&#65292;&#21363;&#31243;&#24207;&#21512;&#25104;&#23545;&#35768;&#22810;&#26597;&#35810;&#21644;&#24066;&#22330;&#20013;&#30340;&#22823;&#37327;&#21442;&#19982;&#32773;&#25552;&#20379;&#20102;&#37325;&#22823;&#20248;&#21183;&#12290;&#25105;&#20204;&#35814;&#32454;&#35828;&#26126;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#34892;&#19994;&#37325;&#35201;&#24615;&#65292;&#27010;&#36848;&#20102;&#20855;&#20307;&#23454;&#29616;&#32454;&#33410;&#65292;&#24182;&#22522;&#20110;&#25105;&#20204;&#22312;Tooso&#26500;&#24314;&#31867;&#20284;&#31995;&#32479;&#30340;&#32463;&#39564;&#65292;&#22238;&#31572;&#20102;&#19968;&#20123;&#24120;&#35265;&#30340;&#21453;&#23545;&#24847;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
As ecommerce continues growing, huge investments in ML and NLP for Information Retrieval are following. While the vector space model dominated retrieval modelling in product search - even as vectorization itself greatly changed with the advent of deep learning -, our position paper argues in a contrarian fashion that program synthesis provides significant advantages for many queries and a significant number of players in the market. We detail the industry significance of the proposed approach, sketch implementation details, and address common objections drawing from our experience building a similar system at Tooso.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26465;&#20214;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#26465;&#20214;&#33258;&#22238;&#24402;&#30340;&#26041;&#24335;&#23558;&#20248;&#21270;&#21644;&#29983;&#25104;&#36807;&#31243;&#20998;&#35299;&#20026;&#26356;&#23481;&#26131;&#21644;&#21487;&#22788;&#29702;&#30340;&#27493;&#39588;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20248;&#21270;&#27169;&#24335;&#65292;&#32467;&#21512;&#20132;&#21449;&#29109;&#25439;&#22833;&#21644;&#23545;&#25239;&#24615;&#25439;&#22833;&#31283;&#23450;&#35757;&#32451;&#36807;&#31243;&#12290;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#39034;&#24207;&#25512;&#33616;&#26041;&#38754;&#20855;&#26377;&#36739;&#20248;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.11433</link><description>&lt;p&gt;
&#26465;&#20214;&#21435;&#22122;&#25193;&#25955;&#29992;&#20110;&#39034;&#24207;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Conditional Denoising Diffusion for Sequential Recommendation. (arXiv:2304.11433v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11433
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26465;&#20214;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#26465;&#20214;&#33258;&#22238;&#24402;&#30340;&#26041;&#24335;&#23558;&#20248;&#21270;&#21644;&#29983;&#25104;&#36807;&#31243;&#20998;&#35299;&#20026;&#26356;&#23481;&#26131;&#21644;&#21487;&#22788;&#29702;&#30340;&#27493;&#39588;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20248;&#21270;&#27169;&#24335;&#65292;&#32467;&#21512;&#20132;&#21449;&#29109;&#25439;&#22833;&#21644;&#23545;&#25239;&#24615;&#25439;&#22833;&#31283;&#23450;&#35757;&#32451;&#36807;&#31243;&#12290;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#39034;&#24207;&#25512;&#33616;&#26041;&#38754;&#20855;&#26377;&#36739;&#20248;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#33021;&#22815;&#23398;&#20064;&#20869;&#22312;&#30340;&#25968;&#25454;&#20998;&#24067;&#24182;&#22788;&#29702;&#19981;&#30830;&#23450;&#24615;&#65292;&#29983;&#25104;&#27169;&#22411;&#21463;&#21040;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#20004;&#31181;&#20027;&#35201;&#30340;&#29983;&#25104;&#27169;&#22411;&#8212;&#8212;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#21644;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAEs&#65289;&#22312;&#39034;&#24207;&#25512;&#33616;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#23384;&#22312;&#25361;&#25112;&#65292;GANs&#23384;&#22312;&#19981;&#31283;&#23450;&#30340;&#20248;&#21270;&#65292;&#32780;VAEs&#21017;&#23481;&#26131;&#21457;&#29983;&#21518;&#39564;&#23849;&#22604;&#21644;&#36807;&#24230;&#24179;&#28369;&#30340;&#29983;&#25104;&#12290;&#39034;&#24207;&#25512;&#33616;&#30340;&#31232;&#30095;&#21644;&#22024;&#26434;&#30340;&#29305;&#24615;&#36827;&#19968;&#27493;&#21152;&#21095;&#20102;&#36825;&#20123;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26465;&#20214;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#65292;&#21253;&#25324;&#24207;&#21015;&#32534;&#30721;&#22120;&#65292;&#20132;&#21449;&#27880;&#24847;&#21435;&#22122;&#35299;&#30721;&#22120;&#21644;&#36880;&#27493;&#25193;&#25955;&#22120;&#12290;&#36825;&#31181;&#26041;&#27861;&#20197;&#26465;&#20214;&#33258;&#22238;&#24402;&#30340;&#26041;&#24335;&#23558;&#20248;&#21270;&#21644;&#29983;&#25104;&#36807;&#31243;&#20998;&#35299;&#20026;&#26356;&#23481;&#26131;&#21644;&#21487;&#22788;&#29702;&#30340;&#27493;&#39588;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20248;&#21270;&#27169;&#24335;&#65292;&#32467;&#21512;&#20132;&#21449;&#29109;&#25439;&#22833;&#21644;&#23545;&#25239;&#24615;&#25439;&#22833;&#31283;&#23450;&#35757;&#32451;&#36807;&#31243;&#12290;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#39034;&#24207;&#25512;&#33616;&#26041;&#38754;&#20248;&#20110;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#26080;&#35770;&#26159;&#22312;&#23450;&#37327;&#25351;&#26631;&#19978;&#36824;&#26159;&#22312;&#23450;&#24615;&#25351;&#26631;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative models have attracted significant interest due to their ability to handle uncertainty by learning the inherent data distributions. However, two prominent generative models, namely Generative Adversarial Networks (GANs) and Variational AutoEncoders (VAEs), exhibit challenges that impede achieving optimal performance in sequential recommendation tasks. Specifically, GANs suffer from unstable optimization, while VAEs are prone to posterior collapse and over-smoothed generations. The sparse and noisy nature of sequential recommendation further exacerbates these issues. In response to these limitations, we present a conditional denoising diffusion model, which includes a sequence encoder, a cross-attentive denoising decoder, and a step-wise diffuser. This approach streamlines the optimization and generation process by dividing it into easier and tractable steps in a conditional autoregressive manner. Furthermore, we introduce a novel optimization schema that incorporates both cro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;&#24863;&#30693;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;SAILER&#65292;&#38024;&#23545;&#27861;&#24459;&#26696;&#20363;&#26816;&#32034;&#20013;&#30340;&#38271;&#25991;&#26412;&#24207;&#21015;&#21644;&#20851;&#38190;&#27861;&#24459;&#35201;&#32032;&#25935;&#24863;&#38382;&#39064;&#65292;&#37319;&#29992;&#36974;&#34109;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#21644;&#32467;&#26500;&#24863;&#30693;&#36830;&#36143;&#24615;&#39044;&#27979;&#20219;&#21153;&#30456;&#32467;&#21512;&#30340;&#22810;&#20219;&#21153;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#22312;&#20004;&#20010;&#27861;&#24459;&#26696;&#20363;&#26816;&#32034;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26174;&#33879;&#20248;&#20110;&#24378;&#22522;&#32447;&#27169;&#22411;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.11370</link><description>&lt;p&gt;
SAILER: &#38754;&#21521;&#27861;&#24459;&#26696;&#20363;&#26816;&#32034;&#30340;&#32467;&#26500;&#24863;&#30693;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SAILER: Structure-aware Pre-trained Language Model for Legal Case Retrieval. (arXiv:2304.11370v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11370
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;&#24863;&#30693;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;SAILER&#65292;&#38024;&#23545;&#27861;&#24459;&#26696;&#20363;&#26816;&#32034;&#20013;&#30340;&#38271;&#25991;&#26412;&#24207;&#21015;&#21644;&#20851;&#38190;&#27861;&#24459;&#35201;&#32032;&#25935;&#24863;&#38382;&#39064;&#65292;&#37319;&#29992;&#36974;&#34109;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#21644;&#32467;&#26500;&#24863;&#30693;&#36830;&#36143;&#24615;&#39044;&#27979;&#20219;&#21153;&#30456;&#32467;&#21512;&#30340;&#22810;&#20219;&#21153;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#22312;&#20004;&#20010;&#27861;&#24459;&#26696;&#20363;&#26816;&#32034;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26174;&#33879;&#20248;&#20110;&#24378;&#22522;&#32447;&#27169;&#22411;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#26234;&#33021;&#27861;&#24459;&#31995;&#32479;&#20013;&#30340;&#26680;&#24515;&#24037;&#20316;&#8212;&#8212;&#27861;&#24459;&#26696;&#20363;&#26816;&#32034;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32467;&#26500;&#24863;&#30693;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;SAILER&#12290;&#19982;&#36890;&#29992;&#25991;&#26723;&#30456;&#27604;&#65292;&#27861;&#24459;&#26696;&#20363;&#25991;&#20214;&#36890;&#24120;&#20855;&#26377;&#22266;&#26377;&#30340;&#36923;&#36753;&#32467;&#26500;&#65292;&#24182;&#21253;&#21547;&#20851;&#38190;&#30340;&#27861;&#24459;&#35201;&#32032;&#12290;SAILER&#37319;&#29992;&#22810;&#20219;&#21153;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#21253;&#25324;&#36974;&#34109;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#21644;&#36866;&#29992;&#20110;&#27861;&#24459;&#25991;&#26723;&#30340;&#32467;&#26500;&#24863;&#30693;&#36830;&#36143;&#24615;&#39044;&#27979;&#20219;&#21153;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SAILER&#22312;&#20004;&#20010;&#27861;&#24459;&#26696;&#20363;&#26816;&#32034;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#20248;&#20110;&#20960;&#20010;&#24378;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Legal case retrieval, which aims to find relevant cases for a query case, plays a core role in the intelligent legal system. Despite the success that pre-training has achieved in ad-hoc retrieval tasks, effective pre-training strategies for legal case retrieval remain to be explored. Compared with general documents, legal case documents are typically long text sequences with intrinsic logical structures. However, most existing language models have difficulty understanding the long-distance dependencies between different structures. Moreover, in contrast to the general retrieval, the relevance in the legal domain is sensitive to key legal elements. Even subtle differences in key legal elements can significantly affect the judgement of relevance. However, existing pre-trained language models designed for general purposes have not been equipped to handle legal elements.  To address these issues, in this paper, we propose SAILER, a new Structure-Aware pre-traIned language model for LEgal c
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26041;&#27861;&#65292;&#33021;&#22815;&#20174;&#21407;&#22987;&#25968;&#25454;&#20013;&#21512;&#25104;&#26032;&#25968;&#25454;&#38598;&#20197;&#26500;&#24314;&#30693;&#35782;&#22270;&#35889;&#65292;&#23454;&#29616;&#33258;&#28982;&#28798;&#23475;&#24037;&#31243;&#25968;&#25454;&#38598;&#30340;&#30693;&#35782;&#21457;&#29616;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#25171;&#30772;&#20256;&#32479;DesignSafe&#35789;&#27719;&#25628;&#32034;&#30340;&#23616;&#38480;&#24615;&#65292;&#26377;&#25928;&#22320;&#36827;&#34892;&#22797;&#26434;&#26597;&#35810;&#65292;&#20419;&#36827;&#26032;&#30340;&#31185;&#23398;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2304.11273</link><description>&lt;p&gt;
&#22312;DesignSafe&#19978;&#23454;&#29616;&#33258;&#28982;&#28798;&#23475;&#24037;&#31243;&#25968;&#25454;&#30340;&#30693;&#35782;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Enabling knowledge discovery in natural hazard engineering datasets on DesignSafe. (arXiv:2304.11273v1 [physics.geo-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11273
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26041;&#27861;&#65292;&#33021;&#22815;&#20174;&#21407;&#22987;&#25968;&#25454;&#20013;&#21512;&#25104;&#26032;&#25968;&#25454;&#38598;&#20197;&#26500;&#24314;&#30693;&#35782;&#22270;&#35889;&#65292;&#23454;&#29616;&#33258;&#28982;&#28798;&#23475;&#24037;&#31243;&#25968;&#25454;&#38598;&#30340;&#30693;&#35782;&#21457;&#29616;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#25171;&#30772;&#20256;&#32479;DesignSafe&#35789;&#27719;&#25628;&#32034;&#30340;&#23616;&#38480;&#24615;&#65292;&#26377;&#25928;&#22320;&#36827;&#34892;&#22797;&#26434;&#26597;&#35810;&#65292;&#20419;&#36827;&#26032;&#30340;&#31185;&#23398;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#30340;&#21457;&#29616;&#38656;&#35201;&#20174;&#22797;&#26434;&#12289;&#38750;&#32467;&#26500;&#21270;&#21644;&#24322;&#26500;&#30340;&#31185;&#23398;&#25968;&#25454;&#20013;&#35782;&#21035;&#30456;&#20851;&#30340;&#25968;&#25454;&#20851;&#31995;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26041;&#27861;&#65292;&#25552;&#21462;&#20803;&#25968;&#25454;&#24182;&#21033;&#29992;&#31185;&#23398;&#39046;&#22495;&#30693;&#35782;&#20174;&#21407;&#22987;&#25968;&#25454;&#20013;&#21512;&#25104;&#26032;&#25968;&#25454;&#38598;&#20197;&#26500;&#24314;&#30693;&#35782;&#22270;&#35889;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;DesignSafe&#19978;&#30340;&#8220;LEAP&#28082;&#21270;&#8221;&#33258;&#28982;&#28798;&#23475;&#24037;&#31243;&#25968;&#25454;&#38598;&#36827;&#34892;&#26696;&#20363;&#30740;&#31350;&#26469;&#35777;&#26126;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#20256;&#32479;&#30340;DesignSafe&#35789;&#27719;&#25628;&#32034;&#22312;&#25581;&#31034;&#25968;&#25454;&#20869;&#37096;&#38544;&#34255;&#20851;&#31995;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#30340;&#30693;&#35782;&#22270;&#35889;&#33021;&#22815;&#36827;&#34892;&#22797;&#26434;&#26597;&#35810;&#65292;&#24182;&#36890;&#36807;&#20934;&#30830;&#35782;&#21035;&#30456;&#20851;&#23454;&#20307;&#24182;&#24314;&#31435;&#25968;&#25454;&#38598;&#20869;&#30340;&#20851;&#31995;&#65292;&#20419;&#36827;&#26032;&#30340;&#31185;&#23398;&#35265;&#35299;&#12290;&#36825;&#31181;&#21019;&#26032;&#24615;&#30340;&#23454;&#29616;&#21487;&#20197;&#22312;&#21508;&#31181;&#31185;&#23398;&#39046;&#22495;&#20013;&#25913;&#21464;&#25968;&#25454;&#39537;&#21160;&#21457;&#29616;&#30340;&#26684;&#23616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-driven discoveries require identifying relevant data relationships from a sea of complex, unstructured, and heterogeneous scientific data. We propose a hybrid methodology that extracts metadata and leverages scientific domain knowledge to synthesize a new dataset from the original to construct knowledge graphs. We demonstrate our approach's effectiveness through a case study on the natural hazard engineering dataset on ``LEAP Liquefaction'' hosted on DesignSafe. Traditional lexical search on DesignSafe is limited in uncovering hidden relationships within the data. Our knowledge graph enables complex queries and fosters new scientific insights by accurately identifying relevant entities and establishing their relationships within the dataset. This innovative implementation can transform the landscape of data-driven discoveries across various scientific domains.
&lt;/p&gt;</description></item><item><title>CLaMP&#26159;&#19968;&#31181;&#23545;&#27604;&#35821;&#35328;-&#38899;&#20048;&#39044;&#35757;&#32451;&#25216;&#26415;&#65292;&#33021;&#22815;&#23398;&#20064;&#31526;&#21495;&#38899;&#20048;&#21644;&#33258;&#28982;&#35821;&#35328;&#20043;&#38388;&#30340;&#36328;&#27169;&#24577;&#34920;&#31034;&#12290;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#21644;&#20998;&#22359;&#22788;&#29702;&#65292;&#23427;&#23558;&#31526;&#21495;&#38899;&#20048;&#34920;&#31034;&#25104;&#38271;&#24230;&#19981;&#21040;10&#65285;&#30340;&#24207;&#21015;&#65292;&#24182;&#20351;&#29992;&#25513;&#34109;&#38899;&#20048;&#27169;&#22411;&#39044;&#35757;&#32451;&#30446;&#26631;&#26469;&#22686;&#24378;&#38899;&#20048;&#32534;&#30721;&#22120;&#23545;&#38899;&#20048;&#19978;&#19979;&#25991;&#21644;&#32467;&#26500;&#30340;&#29702;&#35299;&#12290;&#36825;&#31181;&#25216;&#26415;&#36229;&#36234;&#20102;&#29616;&#26377;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#21487;&#20197;&#23454;&#29616;&#31526;&#21495;&#38899;&#20048;&#30340;&#35821;&#20041;&#25628;&#32034;&#21644;&#38646;&#26679;&#26412;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2304.11029</link><description>&lt;p&gt;
CLaMP&#65306;&#29992;&#20110;&#36328;&#27169;&#24577;&#31526;&#21495;&#38899;&#20048;&#20449;&#24687;&#26816;&#32034;&#30340;&#23545;&#27604;&#35821;&#35328;-&#38899;&#20048;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
CLaMP: Contrastive Language-Music Pre-training for Cross-Modal Symbolic Music Information Retrieval. (arXiv:2304.11029v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11029
&lt;/p&gt;
&lt;p&gt;
CLaMP&#26159;&#19968;&#31181;&#23545;&#27604;&#35821;&#35328;-&#38899;&#20048;&#39044;&#35757;&#32451;&#25216;&#26415;&#65292;&#33021;&#22815;&#23398;&#20064;&#31526;&#21495;&#38899;&#20048;&#21644;&#33258;&#28982;&#35821;&#35328;&#20043;&#38388;&#30340;&#36328;&#27169;&#24577;&#34920;&#31034;&#12290;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#21644;&#20998;&#22359;&#22788;&#29702;&#65292;&#23427;&#23558;&#31526;&#21495;&#38899;&#20048;&#34920;&#31034;&#25104;&#38271;&#24230;&#19981;&#21040;10&#65285;&#30340;&#24207;&#21015;&#65292;&#24182;&#20351;&#29992;&#25513;&#34109;&#38899;&#20048;&#27169;&#22411;&#39044;&#35757;&#32451;&#30446;&#26631;&#26469;&#22686;&#24378;&#38899;&#20048;&#32534;&#30721;&#22120;&#23545;&#38899;&#20048;&#19978;&#19979;&#25991;&#21644;&#32467;&#26500;&#30340;&#29702;&#35299;&#12290;&#36825;&#31181;&#25216;&#26415;&#36229;&#36234;&#20102;&#29616;&#26377;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#21487;&#20197;&#23454;&#29616;&#31526;&#21495;&#38899;&#20048;&#30340;&#35821;&#20041;&#25628;&#32034;&#21644;&#38646;&#26679;&#26412;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;CLaMP&#65306;&#23545;&#27604;&#35821;&#35328;-&#38899;&#20048;&#39044;&#35757;&#32451;&#65292;&#23427;&#20351;&#29992;&#38899;&#20048;&#32534;&#30721;&#22120;&#21644;&#25991;&#26412;&#32534;&#30721;&#22120;&#36890;&#36807;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#32852;&#21512;&#35757;&#32451;&#26469;&#23398;&#20064;&#33258;&#28982;&#35821;&#35328;&#21644;&#31526;&#21495;&#38899;&#20048;&#20043;&#38388;&#30340;&#36328;&#27169;&#24577;&#34920;&#31034;&#12290;&#20026;&#20102;&#39044;&#35757;&#32451;CLaMP&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;140&#19975;&#20010;&#38899;&#20048;-&#25991;&#26412;&#23545;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#12290;&#23427;&#20351;&#29992;&#20102;&#25991;&#26412;&#38543;&#26426;&#22833;&#27963;&#26469;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#21644;&#20998;&#22359;&#22788;&#29702;&#20197;&#39640;&#25928;&#22320;&#34920;&#31034;&#38899;&#20048;&#25968;&#25454;&#65292;&#20174;&#32780;&#23558;&#24207;&#21015;&#38271;&#24230;&#32553;&#30701;&#21040;&#19981;&#21040;10&#65285;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#25513;&#34109;&#38899;&#20048;&#27169;&#22411;&#39044;&#35757;&#32451;&#30446;&#26631;&#65292;&#20197;&#22686;&#24378;&#38899;&#20048;&#32534;&#30721;&#22120;&#23545;&#38899;&#20048;&#19978;&#19979;&#25991;&#21644;&#32467;&#26500;&#30340;&#29702;&#35299;&#12290;CLaMP&#38598;&#25104;&#20102;&#25991;&#26412;&#20449;&#24687;&#65292;&#20197;&#23454;&#29616;&#31526;&#21495;&#38899;&#20048;&#30340;&#35821;&#20041;&#25628;&#32034;&#21644;&#38646;&#26679;&#26412;&#20998;&#31867;&#65292;&#36229;&#36234;&#20102;&#20808;&#21069;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#20026;&#25903;&#25345;&#35821;&#20041;&#25628;&#32034;&#21644;&#38899;&#20048;&#20998;&#31867;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#20844;&#24320;&#21457;&#24067;&#20102;WikiMusicText&#65288;WikiMT&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;1010&#20010;ABC&#31526;&#21495;&#35889;&#30340;&#25968;&#25454;&#38598;&#65292;&#27599;&#20010;&#35889;&#37117;&#38468;&#24102;&#26377;&#26631;&#39064;&#12289;&#33402;&#26415;&#23478;&#12289;&#27969;&#27966;&#21644;&#25551;&#36848;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce CLaMP: Contrastive Language-Music Pre-training, which learns cross-modal representations between natural language and symbolic music using a music encoder and a text encoder trained jointly with a contrastive loss. To pre-train CLaMP, we collected a large dataset of 1.4 million music-text pairs. It employed text dropout as a data augmentation technique and bar patching to efficiently represent music data which reduces sequence length to less than 10%. In addition, we developed a masked music model pre-training objective to enhance the music encoder's comprehension of musical context and structure. CLaMP integrates textual information to enable semantic search and zero-shot classification for symbolic music, surpassing the capabilities of previous models. To support the evaluation of semantic search and music classification, we publicly release WikiMusicText (WikiMT), a dataset of 1010 lead sheets in ABC notation, each accompanied by a title, artist, genre, and description.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20027;&#39064;&#23884;&#20837;&#26041;&#27861;&#21644;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#26080;&#30417;&#30563;&#22312;&#32447;&#25925;&#20107;&#21457;&#29616;&#26694;&#26550;USTORY&#65292;&#21487;&#20197;&#21160;&#24577;&#34920;&#31034;&#25991;&#31456;&#21644;&#25925;&#20107;&#65292;&#24182;&#32771;&#34385;&#23427;&#20204;&#20849;&#20139;&#30340;&#26102;&#38388;&#20027;&#39064;&#21644;&#26032;&#39062;&#24615;&#65292;&#20197;&#24110;&#21161;&#20154;&#20204;&#28040;&#21270;&#22823;&#37327;&#30340;&#26032;&#38395;&#27969;&#12290;</title><link>http://arxiv.org/abs/2304.04099</link><description>&lt;p&gt;
&#36890;&#36807;&#21487;&#25193;&#23637;&#30340;&#20027;&#39064;&#23884;&#20837;&#20174;&#36830;&#32493;&#26032;&#38395;&#27969;&#20013;&#26080;&#30417;&#30563;&#22320;&#21457;&#29616;&#25925;&#20107;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Story Discovery from Continuous News Streams via Scalable Thematic Embedding. (arXiv:2304.04099v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04099
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20027;&#39064;&#23884;&#20837;&#26041;&#27861;&#21644;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#26080;&#30417;&#30563;&#22312;&#32447;&#25925;&#20107;&#21457;&#29616;&#26694;&#26550;USTORY&#65292;&#21487;&#20197;&#21160;&#24577;&#34920;&#31034;&#25991;&#31456;&#21644;&#25925;&#20107;&#65292;&#24182;&#32771;&#34385;&#23427;&#20204;&#20849;&#20139;&#30340;&#26102;&#38388;&#20027;&#39064;&#21644;&#26032;&#39062;&#24615;&#65292;&#20197;&#24110;&#21161;&#20154;&#20204;&#28040;&#21270;&#22823;&#37327;&#30340;&#26032;&#38395;&#27969;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#22320;&#21457;&#29616;&#23454;&#26102;&#30456;&#20851;&#26032;&#38395;&#25991;&#31456;&#25925;&#20107;&#65292;&#26377;&#21161;&#20110;&#20154;&#20204;&#22312;&#19981;&#38656;&#35201;&#26114;&#36149;&#20154;&#24037;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#28040;&#21270;&#22823;&#37327;&#30340;&#26032;&#38395;&#27969;&#12290;&#29616;&#26377;&#30340;&#26080;&#30417;&#30563;&#22312;&#32447;&#25925;&#20107;&#21457;&#29616;&#30740;&#31350;&#30340;&#26222;&#36941;&#26041;&#27861;&#26159;&#29992;&#31526;&#21495;&#25110;&#22522;&#20110;&#22270;&#30340;&#23884;&#20837;&#26469;&#34920;&#31034;&#26032;&#38395;&#25991;&#31456;&#65292;&#24182;&#23558;&#23427;&#20204;&#36880;&#27493;&#32858;&#31867;&#25104;&#25925;&#20107;&#12290;&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26377;&#26395;&#36827;&#19968;&#27493;&#25913;&#21892;&#23884;&#20837;&#65292;&#20294;&#26159;&#36890;&#36807;&#26080;&#24046;&#21035;&#22320;&#32534;&#30721;&#25991;&#31456;&#20013;&#30340;&#25152;&#26377;&#20449;&#24687;&#26469;&#30452;&#25509;&#37319;&#29992;&#36825;&#20123;&#27169;&#22411;&#26080;&#27861;&#26377;&#25928;&#22788;&#29702;&#23500;&#21547;&#25991;&#26412;&#19988;&#19981;&#26029;&#21457;&#23637;&#30340;&#26032;&#38395;&#27969;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20027;&#39064;&#23884;&#20837;&#26041;&#27861;&#65292;&#20351;&#29992;&#29616;&#25104;&#30340;&#39044;&#35757;&#32451;&#21477;&#23376;&#32534;&#30721;&#22120;&#26469;&#21160;&#24577;&#34920;&#31034;&#25991;&#31456;&#21644;&#25925;&#20107;&#65292;&#24182;&#32771;&#34385;&#23427;&#20204;&#20849;&#20139;&#30340;&#26102;&#38388;&#20027;&#39064;&#12290;&#20026;&#20102;&#23454;&#29616;&#26080;&#30417;&#30563;&#22312;&#32447;&#25925;&#20107;&#21457;&#29616;&#30340;&#24819;&#27861;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#26694;&#26550;USTORY&#65292;&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#25216;&#26415;&#65292;&#21363;&#20027;&#39064;&#21644;&#26102;&#38388;&#24863;&#30693;&#30340;&#21160;&#24577;&#23884;&#20837;&#21644;&#26032;&#39062;&#24615;&#24863;&#30693;&#30340;&#33258;&#36866;&#24212;&#32858;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised discovery of stories with correlated news articles in real-time helps people digest massive news streams without expensive human annotations. A common approach of the existing studies for unsupervised online story discovery is to represent news articles with symbolic- or graph-based embedding and incrementally cluster them into stories. Recent large language models are expected to improve the embedding further, but a straightforward adoption of the models by indiscriminately encoding all information in articles is ineffective to deal with text-rich and evolving news streams. In this work, we propose a novel thematic embedding with an off-the-shelf pretrained sentence encoder to dynamically represent articles and stories by considering their shared temporal themes. To realize the idea for unsupervised online story discovery, a scalable framework USTORY is introduced with two main techniques, theme- and time-aware dynamic embedding and novelty-aware adaptive clustering, fuel
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20174;&#27491;&#24335;&#30340;&#35282;&#24230;&#21453;&#24605;&#20102;&#25490;&#21517;&#20013;&#21484;&#22238;&#29575;&#30340;&#27979;&#37327;&#38382;&#39064;&#65292;&#25552;&#20986;&#21484;&#22238;&#26041;&#21521;&#30340;&#27010;&#24565;&#21644;&#35789;&#20856;&#24335;&#26041;&#27861;&#65292;&#24182;&#20998;&#26512;&#20102;&#20854;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.11370</link><description>&lt;p&gt;
&#21484;&#22238;&#29575;&#12289;&#40065;&#26834;&#24615;&#21644;&#35789;&#20856;&#24335;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Recall, Robustness, and Lexicographic Evaluation. (arXiv:2302.11370v3 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11370
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20174;&#27491;&#24335;&#30340;&#35282;&#24230;&#21453;&#24605;&#20102;&#25490;&#21517;&#20013;&#21484;&#22238;&#29575;&#30340;&#27979;&#37327;&#38382;&#39064;&#65292;&#25552;&#20986;&#21484;&#22238;&#26041;&#21521;&#30340;&#27010;&#24565;&#21644;&#35789;&#20856;&#24335;&#26041;&#27861;&#65292;&#24182;&#20998;&#26512;&#20102;&#20854;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20154;&#21592;&#20351;&#29992;&#21484;&#22238;&#29575;&#26469;&#35780;&#20272;&#21508;&#31181;&#26816;&#32034;&#12289;&#25512;&#33616;&#21644;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#25490;&#21517;&#12290;&#23613;&#31649;&#22312;&#38598;&#21512;&#35780;&#20272;&#20013;&#26377;&#20851;&#21484;&#22238;&#29575;&#30340;&#20439;&#35821;&#35299;&#37322;&#65292;&#20294;&#30740;&#31350;&#31038;&#21306;&#36828;&#26410;&#29702;&#35299;&#25490;&#21517;&#21484;&#22238;&#29575;&#30340;&#21407;&#29702;&#12290;&#23545;&#21484;&#22238;&#29575;&#32570;&#20047;&#21407;&#29702;&#29702;&#35299;&#25110;&#21160;&#26426;&#23548;&#33268;&#20449;&#24687;&#26816;&#32034;&#31038;&#21306;&#25209;&#35780;&#21484;&#22238;&#29575;&#26159;&#21542;&#26377;&#29992;&#20316;&#20026;&#19968;&#20010;&#25351;&#26631;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#20174;&#27491;&#24335;&#30340;&#35282;&#24230;&#21453;&#24605;&#25490;&#21517;&#20013;&#21484;&#22238;&#29575;&#30340;&#27979;&#37327;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#30001;&#19977;&#20010;&#21407;&#21017;&#32452;&#25104;&#65306;&#21484;&#22238;&#29575;&#12289;&#40065;&#26834;&#24615;&#21644;&#35789;&#20856;&#24335;&#35780;&#20272;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#27491;&#24335;&#23450;&#20041;&#8220;&#21484;&#22238;&#26041;&#21521;&#8221;&#20026;&#25935;&#24863;&#20110;&#24213;&#37096;&#25490;&#21517;&#30456;&#20851;&#26465;&#30446;&#31227;&#21160;&#30340;&#24230;&#37327;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20174;&#21487;&#33021;&#30340;&#25628;&#32034;&#32773;&#21644;&#20869;&#23481;&#25552;&#20379;&#32773;&#30340;&#40065;&#26834;&#24615;&#35282;&#24230;&#20998;&#26512;&#20102;&#25105;&#20204;&#30340;&#21484;&#22238;&#26041;&#21521;&#27010;&#24565;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#23454;&#29992;&#30340;&#35789;&#20856;&#24335;&#26041;&#27861;&#26469;&#25193;&#23637;&#23545;&#21484;&#22238;&#30340;&#27010;&#24565;&#21644;&#29702;&#35770;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Researchers use recall to evaluate rankings across a variety of retrieval, recommendation, and machine learning tasks. While there is a colloquial interpretation of recall in set-based evaluation, the research community is far from a principled understanding of recall metrics for rankings. The lack of principled understanding of or motivation for recall has resulted in criticism amongst the retrieval community that recall is useful as a measure at all. In this light, we reflect on the measurement of recall in rankings from a formal perspective. Our analysis is composed of three tenets: recall, robustness, and lexicographic evaluation. First, we formally define `recall-orientation' as sensitivity to movement of the bottom-ranked relevant item. Second, we analyze our concept of recall orientation from the perspective of robustness with respect to possible searchers and content providers. Finally, we extend this conceptual and theoretical treatment of recall by developing a practical pref
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31532;&#19968;&#27425;&#20840;&#38754;&#25506;&#35752;&#20102; DR &#27169;&#22411;&#22312;&#38646;-shot&#26816;&#32034;&#33021;&#21147;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#20998;&#26512;&#20102;&#24433;&#21709;&#34920;&#29616;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#20026;&#24320;&#21457;&#26356;&#22909;&#30340;&#38646;-shot DR &#27169;&#22411;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#35777;&#25454;&#12290;</title><link>http://arxiv.org/abs/2204.12755</link><description>&lt;p&gt;
&#38646;-shot &#23494;&#38598;&#26816;&#32034;&#30340;&#20840;&#38754;&#25506;&#35752;
&lt;/p&gt;
&lt;p&gt;
A Thorough Examination on Zero-shot Dense Retrieval. (arXiv:2204.12755v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.12755
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31532;&#19968;&#27425;&#20840;&#38754;&#25506;&#35752;&#20102; DR &#27169;&#22411;&#22312;&#38646;-shot&#26816;&#32034;&#33021;&#21147;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#20998;&#26512;&#20102;&#24433;&#21709;&#34920;&#29616;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#20026;&#24320;&#21457;&#26356;&#22909;&#30340;&#38646;-shot DR &#27169;&#22411;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#24378;&#22823;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#30340;&#23494;&#38598;&#26816;&#32034;&#65288;DR&#65289;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;DR &#27169;&#22411;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#20294;&#22312;&#38646;-shot&#26816;&#32034;&#35774;&#32622;&#19979;&#65292;&#23427;&#20204;&#26174;&#31034;&#20986;&#30340;&#31454;&#20105;&#21147;&#19981;&#22914;&#20256;&#32479;&#30340;&#31232;&#30095;&#26816;&#32034;&#27169;&#22411;&#65288;&#20363;&#22914;BM25&#65289;&#12290;&#28982;&#32780;&#65292;&#22312;&#30456;&#20851;&#25991;&#29486;&#20013;&#65292;&#20173;&#32570;&#20047;&#23545;&#38646;-shot&#26816;&#32034;&#30340;&#35814;&#32454;&#21644;&#20840;&#38754;&#30340;&#30740;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#23545; DR &#27169;&#22411;&#30340;&#38646;-shot&#33021;&#21147;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#26088;&#22312;&#30830;&#23450;&#20851;&#38190;&#22240;&#32032;&#24182;&#20998;&#26512;&#23427;&#20204;&#22914;&#20309;&#24433;&#21709;&#38646;-shot&#26816;&#32034;&#24615;&#33021;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#19982;&#28304;&#35757;&#32451;&#38598;&#30456;&#20851;&#30340;&#20960;&#20010;&#20851;&#38190;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#20998;&#26512;&#20102;&#30446;&#26631;&#25968;&#25454;&#38598;&#30340;&#28508;&#22312;&#20559;&#24046;&#65292;&#24182;&#22238;&#39038;&#21644;&#27604;&#36739;&#29616;&#26377;&#30340;&#38646;-shot DR &#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#20026;&#26356;&#22909;&#22320;&#29702;&#35299;&#21644;&#24320;&#21457;&#38646;-shot DR &#27169;&#22411;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have witnessed the significant advance in dense retrieval (DR) based on powerful pre-trained language models (PLM). DR models have achieved excellent performance in several benchmark datasets, while they are shown to be not as competitive as traditional sparse retrieval models (e.g., BM25) in a zero-shot retrieval setting. However, in the related literature, there still lacks a detailed and comprehensive study on zero-shot retrieval. In this paper, we present the first thorough examination of the zero-shot capability of DR models. We aim to identify the key factors and analyze how they affect zero-shot retrieval performance. In particular, we discuss the effect of several key factors related to source training set, analyze the potential bias from the target dataset, and review and compare existing zero-shot DR models. Our findings provide important evidence to better understand and develop zero-shot DR models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21453;&#20107;&#23454;&#20132;&#20114;&#24335;&#25512;&#33616;&#31995;&#32479;(CIRS)&#65292;&#20197;&#28040;&#38500;&#36807;&#28388;&#27668;&#27873;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;(offline RL)&#19982;&#22240;&#26524;&#25512;&#26029;&#30456;&#32467;&#21512;&#12290;</title><link>http://arxiv.org/abs/2204.01266</link><description>&lt;p&gt;
&#36890;&#36807;&#21453;&#20107;&#23454;&#20132;&#20114;&#24335;&#25512;&#33616;&#31995;&#32479;&#28040;&#38500;&#36807;&#28388;&#27668;&#27873;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
CIRS: Bursting Filter Bubbles by Counterfactual Interactive Recommender System. (arXiv:2204.01266v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.01266
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21453;&#20107;&#23454;&#20132;&#20114;&#24335;&#25512;&#33616;&#31995;&#32479;(CIRS)&#65292;&#20197;&#28040;&#38500;&#36807;&#28388;&#27668;&#27873;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;(offline RL)&#19982;&#22240;&#26524;&#25512;&#26029;&#30456;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#30340;&#20010;&#24615;&#21270;&#20351;&#24471;&#20854;&#21464;&#24471;&#38750;&#24120;&#26377;&#29992;&#65292;&#20294;&#21516;&#26102;&#20063;&#24102;&#26469;&#20102;&#36807;&#28388;&#27668;&#27873;&#30340;&#38382;&#39064;&#12290;&#26082;&#28982;&#31995;&#32479;&#19968;&#30452;&#22312;&#25512;&#33616;&#29992;&#25143;&#24863;&#20852;&#36259;&#30340;&#29289;&#21697;&#65292;&#37027;&#20040;&#21487;&#33021;&#20250;&#35753;&#29992;&#25143;&#24863;&#21040;&#20047;&#21619;&#24182;&#20943;&#23569;&#28385;&#24847;&#24230;&#12290;&#29616;&#26377;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#38745;&#24577;&#25512;&#33616;&#20013;&#30340;&#36807;&#28388;&#27668;&#27873;&#38382;&#39064;&#65292;&#24456;&#38590;&#25429;&#25417;&#21040;&#36807;&#24230;&#26333;&#20809;&#30340;&#24433;&#21709;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#35748;&#20026;&#30740;&#31350;&#36807;&#28388;&#27668;&#27873;&#38382;&#39064;&#22312;&#20132;&#20114;&#25512;&#33616;&#20013;&#26356;&#21152;&#26377;&#24847;&#20041;&#65292;&#24182;&#19988;&#20248;&#21270;&#38271;&#26399;&#30340;&#29992;&#25143;&#28385;&#24847;&#24230;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20195;&#20215;&#39640;&#65292;&#22240;&#27492;&#22312;&#32447;&#35757;&#32451;&#27169;&#22411;&#24182;&#19981;&#29616;&#23454;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24517;&#39035;&#21033;&#29992;&#31163;&#32447;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#20998;&#31163;&#29992;&#25143;&#28385;&#24847;&#24230;&#30340;&#22240;&#26524;&#25928;&#24212;&#12290;&#20026;&#20102;&#36798;&#21040;&#36825;&#20010;&#30446;&#30340;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21453;&#20107;&#23454;&#20132;&#20114;&#24335;&#25512;&#33616;&#31995;&#32479;(CIRS)&#65292;&#23427;&#23558;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;(offline RL)&#19982;&#22240;&#26524;&#25512;&#26029;&#30456;&#32467;&#21512;&#12290;&#22522;&#26412;&#24605;&#24819;&#26159;&#65306;&#39318;&#20808;&#23398;&#20064;&#19968;&#20010;&#22240;&#26524;&#29992;&#25143;&#27169;&#22411;&#65292;&#20197;&#25429;&#25417;&#29289;&#21697;&#23545;&#29992;&#25143;&#28385;&#24847;&#24230;&#30340;&#36807;&#24230;&#26333;&#20809;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
While personalization increases the utility of recommender systems, it also brings the issue of filter bubbles. E.g., if the system keeps exposing and recommending the items that the user is interested in, it may also make the user feel bored and less satisfied. Existing work studies filter bubbles in static recommendation, where the effect of overexposure is hard to capture. In contrast, we believe it is more meaningful to study the issue in interactive recommendation and optimize long-term user satisfaction. Nevertheless, it is unrealistic to train the model online due to the high cost. As such, we have to leverage offline training data and disentangle the causal effect on user satisfaction.  To achieve this goal, we propose a counterfactual interactive recommender system (CIRS) that augments offline reinforcement learning (offline RL) with causal inference. The basic idea is to first learn a causal user model on historical data to capture the overexposure effect of items on user sat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#33258;&#21160;&#21307;&#30103;&#32534;&#30721;&#39046;&#22495;&#30340;&#21457;&#23637;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#65292;&#24635;&#32467;&#20102;&#26368;&#26032;&#30340;&#39640;&#32423;&#27169;&#22411;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#21457;&#23637;&#30340;&#25361;&#25112;&#21644;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2201.02797</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#33258;&#21160;&#21307;&#30103;&#32534;&#30721;&#20013;&#30340;&#24212;&#29992;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Unified Review of Deep Learning for Automated Medical Coding. (arXiv:2201.02797v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.02797
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#33258;&#21160;&#21307;&#30103;&#32534;&#30721;&#39046;&#22495;&#30340;&#21457;&#23637;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#65292;&#24635;&#32467;&#20102;&#26368;&#26032;&#30340;&#39640;&#32423;&#27169;&#22411;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#21457;&#23637;&#30340;&#25361;&#25112;&#21644;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21307;&#30103;&#32534;&#30721;&#26159;&#21307;&#30103;&#36816;&#33829;&#21644;&#26381;&#21153;&#30340;&#22522;&#26412;&#20219;&#21153;&#65292;&#36890;&#36807;&#20174;&#20020;&#24202;&#25991;&#26723;&#20013;&#39044;&#27979;&#21307;&#30103;&#32534;&#30721;&#26469;&#31649;&#29702;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#12290;&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#36827;&#27493;&#24050;&#24191;&#27867;&#24212;&#29992;&#20110;&#35813;&#20219;&#21153;&#12290;&#20294;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#33258;&#21160;&#21307;&#30103;&#32534;&#30721;&#32570;&#20047;&#23545;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#35774;&#35745;&#30340;&#32479;&#19968;&#35270;&#22270;&#12290;&#26412;&#32508;&#36848;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#65292;&#20197;&#25552;&#20379;&#23545;&#21307;&#30103;&#32534;&#30721;&#27169;&#22411;&#32452;&#20214;&#30340;&#19968;&#33324;&#29702;&#35299;&#65292;&#24182;&#24635;&#32467;&#20102;&#22312;&#27492;&#26694;&#26550;&#19979;&#26368;&#36817;&#30340;&#39640;&#32423;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#32479;&#19968;&#26694;&#26550;&#23558;&#21307;&#30103;&#32534;&#30721;&#20998;&#35299;&#20026;&#22235;&#20010;&#20027;&#35201;&#32452;&#20214;&#65292;&#21363;&#29992;&#20110;&#25991;&#26412;&#29305;&#24449;&#25552;&#21462;&#30340;&#32534;&#30721;&#22120;&#27169;&#22359;&#12289;&#26500;&#24314;&#28145;&#24230;&#32534;&#30721;&#22120;&#26550;&#26500;&#30340;&#26426;&#21046;&#12289;&#29992;&#20110;&#23558;&#38544;&#34255;&#34920;&#31034;&#36716;&#25442;&#25104;&#21307;&#30103;&#20195;&#30721;&#30340;&#35299;&#30721;&#22120;&#27169;&#22359;&#20197;&#21450;&#36741;&#21161;&#20449;&#24687;&#30340;&#20351;&#29992;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#22522;&#20934;&#21644;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#20351;&#29992;&#24773;&#20917;&#65292;&#35752;&#35770;&#20102;&#20851;&#38190;&#30340;&#30740;&#31350;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated medical coding, an essential task for healthcare operation and delivery, makes unstructured data manageable by predicting medical codes from clinical documents. Recent advances in deep learning and natural language processing have been widely applied to this task. However, deep learning-based medical coding lacks a unified view of the design of neural network architectures. This review proposes a unified framework to provide a general understanding of the building blocks of medical coding models and summarizes recent advanced models under the proposed framework. Our unified framework decomposes medical coding into four main components, i.e., encoder modules for text feature extraction, mechanisms for building deep encoder architectures, decoder modules for transforming hidden representations into medical codes, and the usage of auxiliary information. Finally, we introduce the benchmarks and real-world usage and discuss key research challenges and future directions.
&lt;/p&gt;</description></item><item><title>PAIR&#31639;&#27861;&#26159;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#22312;&#23494;&#38598;&#22411;&#27573;&#33853;&#26816;&#32034;&#20013;&#21516;&#26102;&#32771;&#34385;&#26597;&#35810;&#20013;&#24515;&#21644;&#27573;&#33853;&#20013;&#24515;&#30456;&#20284;&#20851;&#31995;&#65292;&#36890;&#36807;&#27491;&#24335;&#20844;&#24335;&#12289;&#30693;&#35782;&#33976;&#39311;&#21644;&#20004;&#38454;&#27573;&#35757;&#32451;&#23454;&#29616;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#20808;&#21069;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2108.06027</link><description>&lt;p&gt;
PAIR&#65306;&#21033;&#29992;&#27573;&#33853;&#20013;&#24515;&#30340;&#30456;&#20284;&#20851;&#31995;&#25913;&#36827;&#23494;&#38598;&#22411;&#27573;&#33853;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
PAIR: Leveraging Passage-Centric Similarity Relation for Improving Dense Passage Retrieval. (arXiv:2108.06027v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2108.06027
&lt;/p&gt;
&lt;p&gt;
PAIR&#31639;&#27861;&#26159;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#22312;&#23494;&#38598;&#22411;&#27573;&#33853;&#26816;&#32034;&#20013;&#21516;&#26102;&#32771;&#34385;&#26597;&#35810;&#20013;&#24515;&#21644;&#27573;&#33853;&#20013;&#24515;&#30456;&#20284;&#20851;&#31995;&#65292;&#36890;&#36807;&#27491;&#24335;&#20844;&#24335;&#12289;&#30693;&#35782;&#33976;&#39311;&#21644;&#20004;&#38454;&#27573;&#35757;&#32451;&#23454;&#29616;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#20808;&#21069;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23494;&#38598;&#22411;&#27573;&#33853;&#26816;&#32034;&#24050;&#25104;&#20026;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#25214;&#21040;&#30456;&#20851;&#20449;&#24687;&#30340;&#19968;&#31181;&#20027;&#27969;&#26041;&#27861;&#12290;&#35768;&#22810;&#30740;&#31350;&#33268;&#21147;&#20110;&#25913;&#36827;&#24191;&#27867;&#37319;&#29992;&#30340;&#21452;&#32534;&#30721;&#22120;&#26550;&#26500;&#12290;&#20294;&#26159;&#65292;&#22823;&#22810;&#25968;&#20197;&#21069;&#30340;&#30740;&#31350;&#22312;&#23398;&#20064;&#21452;&#32534;&#30721;&#22120;&#26816;&#32034;&#22120;&#26102;&#20165;&#32771;&#34385;&#20102;&#26597;&#35810;&#20013;&#24515;&#30340;&#30456;&#20284;&#20851;&#31995;&#12290;&#20026;&#20102;&#25429;&#25417;&#26356;&#20840;&#38754;&#30340;&#30456;&#20284;&#20851;&#31995;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#26597;&#35810;&#20013;&#24515;&#21644;&#27573;&#33853;&#20013;&#24515;&#30340;&#30456;&#20284;&#20851;&#31995;&#65288;&#31216;&#20026;PAIR&#65289;&#36827;&#34892;&#23494;&#38598;&#22411;&#27573;&#33853;&#26816;&#32034;&#12290;&#20026;&#20102;&#23454;&#29616;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#30456;&#20284;&#20851;&#31995;&#30340;&#27491;&#24335;&#20844;&#24335;&#65292;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20266;&#26631;&#35760;&#25968;&#25454;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#20004;&#38454;&#27573;&#35757;&#32451;&#36807;&#31243;&#65292;&#20854;&#20013;&#21253;&#25324;&#27573;&#33853;&#20013;&#24515;&#30456;&#20284;&#20851;&#31995;&#32422;&#26463;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, dense passage retrieval has become a mainstream approach to finding relevant information in various natural language processing tasks. A number of studies have been devoted to improving the widely adopted dual-encoder architecture. However, most of the previous studies only consider query-centric similarity relation when learning the dual-encoder retriever. In order to capture more comprehensive similarity relations, we propose a novel approach that leverages both query-centric and PAssage-centric sImilarity Relations (called PAIR) for dense passage retrieval. To implement our approach, we make three major technical contributions by introducing formal formulations of the two kinds of similarity relations, generating high-quality pseudo labeled data via knowledge distillation, and designing an effective two-stage training procedure that incorporates passage-centric similarity relation constraint. Extensive experiments show that our approach significantly outperforms previous s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#36229;&#20960;&#20309;&#26816;&#39564;&#22312;&#36873;&#23450;&#30340;&#30495;&#23454;&#25968;&#25454;&#25991;&#26723;&#26816;&#32034;&#21644;&#25688;&#35201;&#20219;&#21153;&#20013;&#34920;&#29616;&#19982;&#24120;&#29992;&#30340;TF-IDF&#21464;&#20307;&#30456;&#24403;&#65292;&#36825;&#25552;&#20379;&#20102;TF-IDF&#38271;&#26399;&#26377;&#25928;&#24615;&#30340;&#32479;&#35745;&#26174;&#33879;&#24615;&#35299;&#37322;&#30340;&#31532;&#19968;&#27493;&#12290;</title><link>http://arxiv.org/abs/2002.11844</link><description>&lt;p&gt;
&#36229;&#20960;&#20309;&#26816;&#39564;&#22312;&#26631;&#20934;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#20013;&#34920;&#29616;&#19982;TF-IDF&#21464;&#20307;&#30456;&#24403;
&lt;/p&gt;
&lt;p&gt;
The hypergeometric test performs comparably to a common TF-IDF variant on standard information retrieval tasks. (arXiv:2002.11844v3 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2002.11844
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#36229;&#20960;&#20309;&#26816;&#39564;&#22312;&#36873;&#23450;&#30340;&#30495;&#23454;&#25968;&#25454;&#25991;&#26723;&#26816;&#32034;&#21644;&#25688;&#35201;&#20219;&#21153;&#20013;&#34920;&#29616;&#19982;&#24120;&#29992;&#30340;TF-IDF&#21464;&#20307;&#30456;&#24403;&#65292;&#36825;&#25552;&#20379;&#20102;TF-IDF&#38271;&#26399;&#26377;&#25928;&#24615;&#30340;&#32479;&#35745;&#26174;&#33879;&#24615;&#35299;&#37322;&#30340;&#31532;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35789;&#39057;-&#36870;&#25991;&#26723;&#39057;&#29575;&#65288;TF-IDF&#65289;&#21450;&#20854;&#35768;&#22810;&#21464;&#20307;&#24418;&#25104;&#20102;&#19968;&#31867;&#24120;&#29992;&#30340;&#26415;&#35821;&#21152;&#26435;&#20989;&#25968;&#65292;&#22312;&#20449;&#24687;&#26816;&#32034;&#24212;&#29992;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;&#34429;&#28982;TF-IDF&#26368;&#21021;&#26159;&#19968;&#31181;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#20294;&#24050;&#32463;&#25552;&#20986;&#20102;&#22522;&#20110;&#20449;&#24687;&#29702;&#35770;&#12289;&#27010;&#29575;&#21644;&#19982;&#38543;&#26426;&#24615;&#32972;&#31163;&#30340;&#33539;&#24335;&#30340;&#29702;&#35770;&#35777;&#26126;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#39033;&#23454;&#35777;&#30740;&#31350;&#65292;&#34920;&#26126;&#22312;&#36873;&#23450;&#30340;&#30495;&#23454;&#25968;&#25454;&#25991;&#26723;&#26816;&#32034;&#21644;&#25688;&#35201;&#20219;&#21153;&#20013;&#65292;&#36229;&#20960;&#20309;&#26816;&#39564;&#30340;&#32479;&#35745;&#26174;&#33879;&#24615;&#19982;&#24120;&#29992;&#30340;TF-IDF&#21464;&#20307;&#38750;&#24120;&#25509;&#36817;&#12290;&#36825;&#20123;&#21457;&#29616;&#34920;&#26126;TF-IDF&#21464;&#20307;&#19982;&#36229;&#20960;&#20309;&#26816;&#39564;P&#20540;&#30340;&#36127;&#23545;&#25968;&#65288;&#21363;&#36229;&#20960;&#20309;&#20998;&#24067;&#23614;&#27010;&#29575;&#65289;&#20043;&#38388;&#23384;&#22312;&#26681;&#26412;&#30340;&#25968;&#23398;&#36830;&#32467;&#26377;&#24453;&#38416;&#26126;&#12290;&#25105;&#20204;&#22312;&#27492;&#25552;&#20379;&#23454;&#35777;&#26696;&#20363;&#30740;&#31350;&#65292;&#20316;&#20026;&#20174;&#32479;&#35745;&#26174;&#33879;&#24615;&#35282;&#24230;&#35299;&#37322;TF-IDF&#38271;&#26399;&#26377;&#25928;&#24615;&#30340;&#31532;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Term frequency-inverse document frequency, or tf-idf for short, and its many variants form a class of term weighting functions the members of which are widely used in information retrieval applications. While tf-idf was originally proposed as a heuristic, theoretical justifications grounded in information theory, probability, and the divergence from randomness paradigm have been advanced. In this work, we present an empirical study showing that the hypergeometric test of statistical significance corresponds very nearly with a common tf-idf variant on selected real-data document retrieval and summarization tasks. These findings suggest that a fundamental mathematical connection between the tf-idf variant and the negative logarithm of the hypergeometric test P-value (i.e., a hypergeometric distribution tail probability) remains to be elucidated. We offer the empirical case study herein as a first step toward explaining the long-standing effectiveness of tf-idf from a statistical signific
&lt;/p&gt;</description></item></channel></rss>