<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#23384;&#26723;&#26597;&#35810;&#26085;&#24535;&#26159;&#20114;&#32852;&#32593;&#26723;&#26696;&#39302;&#22312;&#36807;&#21435;25&#24180;&#20013;&#25910;&#38598;&#30340;&#20840;&#38754;&#26597;&#35810;&#26085;&#24535;&#65292;&#21253;&#21547;&#36739;&#22823;&#35268;&#27169;&#12289;&#24191;&#27867;&#33539;&#22260;&#21644;&#22810;&#26679;&#24615;&#65292;&#20854;&#25552;&#20379;&#20102;&#25903;&#25345;&#26032;&#30340;&#26816;&#32034;&#27169;&#22411;&#21644;&#25628;&#32034;&#24341;&#25806;&#20998;&#26512;&#30340;&#26426;&#20250;&#12290;</title><link>http://arxiv.org/abs/2304.00413</link><description>&lt;p&gt;
&#23384;&#26723;&#26597;&#35810;&#26085;&#24535;&#65306;25&#24180;&#32593;&#32476;&#26723;&#26696;&#20013;&#25968;&#30334;&#20010;&#25628;&#32034;&#24341;&#25806;&#30340;&#25968;&#30334;&#19975;&#25628;&#32034;&#32467;&#26524;&#39029;&#38754;&#30340;&#25366;&#25496;
&lt;/p&gt;
&lt;p&gt;
The Archive Query Log: Mining Millions of Search Result Pages of Hundreds of Search Engines from 25 Years of Web Archives. (arXiv:2304.00413v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00413
&lt;/p&gt;
&lt;p&gt;
&#23384;&#26723;&#26597;&#35810;&#26085;&#24535;&#26159;&#20114;&#32852;&#32593;&#26723;&#26696;&#39302;&#22312;&#36807;&#21435;25&#24180;&#20013;&#25910;&#38598;&#30340;&#20840;&#38754;&#26597;&#35810;&#26085;&#24535;&#65292;&#21253;&#21547;&#36739;&#22823;&#35268;&#27169;&#12289;&#24191;&#27867;&#33539;&#22260;&#21644;&#22810;&#26679;&#24615;&#65292;&#20854;&#25552;&#20379;&#20102;&#25903;&#25345;&#26032;&#30340;&#26816;&#32034;&#27169;&#22411;&#21644;&#25628;&#32034;&#24341;&#25806;&#20998;&#26512;&#30340;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23384;&#26723;&#26597;&#35810;&#26085;&#24535;&#65288;AQL&#65289;&#26159;&#20114;&#32852;&#32593;&#26723;&#26696;&#39302;&#22312;&#36807;&#21435;25&#24180;&#25910;&#38598;&#30340;&#20197;&#21069;&#26410;&#20351;&#29992;&#30340;&#20840;&#38754;&#26597;&#35810;&#26085;&#24535;&#12290;&#20854;&#31532;&#19968;&#20010;&#29256;&#26412;&#21253;&#25324;3.56&#20159;&#27425;&#26597;&#35810;&#12289;1.66&#20159;&#20010;&#25628;&#32034;&#32467;&#26524;&#39029;&#38754;&#21644;550&#20010;&#25628;&#32034;&#25552;&#20379;&#21830;&#30340;17&#20159;&#20010;&#25628;&#32034;&#32467;&#26524;&#12290;&#34429;&#28982;&#25991;&#29486;&#20013;&#26377;&#35768;&#22810;&#26597;&#35810;&#26085;&#24535;&#30340;&#30740;&#31350;&#65292;&#20294;&#25317;&#26377;&#36825;&#20123;&#26085;&#24535;&#30340;&#25628;&#32034;&#25552;&#20379;&#21830;&#36890;&#24120;&#19981;&#20250;&#20844;&#24320;&#21457;&#24067;&#20197;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#21644;&#37325;&#35201;&#19994;&#21153;&#25968;&#25454;&#12290;&#22312;&#23569;&#25968;&#20844;&#24320;&#21487;&#29992;&#30340;&#26597;&#35810;&#26085;&#24535;&#20013;&#65292;&#27809;&#26377;&#19968;&#31181;&#32467;&#21512;&#20102;&#35268;&#27169;&#12289;&#33539;&#22260;&#21644;&#22810;&#26679;&#24615;&#12290;AQL&#26159;&#31532;&#19968;&#20010;&#36825;&#26679;&#20570;&#30340;&#65292;&#20026;&#26032;&#30340;&#26816;&#32034;&#27169;&#22411;&#21644;&#65288;&#21382;&#26102;&#30340;&#65289;&#25628;&#32034;&#24341;&#25806;&#20998;&#26512;&#25552;&#20379;&#25903;&#25345;&#12290;&#20197;&#20445;&#25252;&#38544;&#31169;&#30340;&#26041;&#24335;&#25552;&#20379;&#65292;&#23427;&#20419;&#36827;&#20102;&#24320;&#25918;&#30740;&#31350;&#65292;&#24182;&#22686;&#21152;&#20102;&#25628;&#32034;&#34892;&#19994;&#30340;&#36879;&#26126;&#24230;&#21644;&#38382;&#36131;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Archive Query Log (AQL) is a previously unused, comprehensive query log collected at the Internet Archive over the last 25 years. Its first version includes 356 million queries, 166 million search result pages, and 1.7 billion search results across 550 search providers. Although many query logs have been studied in the literature, the search providers that own them generally do not publish their logs to protect user privacy and vital business data. Of the few query logs publicly available, none combines size, scope, and diversity. The AQL is the first to do so, enabling research on new retrieval models and (diachronic) search engine analyses. Provided in a privacy-preserving manner, it promotes open research as well as more transparency and accountability in the search industry.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#33258;1992&#24180;&#20197;&#26469;&#65292;103&#31687;&#20851;&#20110;&#35780;&#23457;&#20154;&#33258;&#21160;&#20998;&#37197;&#30340;&#30740;&#31350;&#25104;&#26524;&#65292;&#24182;&#20171;&#32461;&#20102;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#28909;&#28857;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2304.00353</link><description>&lt;p&gt;
&#35780;&#23457;&#20154;&#20998;&#37197;&#38382;&#39064;&#65306;&#25991;&#29486;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Reviewer Assignment Problem: A Systematic Review of the Literature. (arXiv:2304.00353v1 [cs.DL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00353
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#33258;1992&#24180;&#20197;&#26469;&#65292;103&#31687;&#20851;&#20110;&#35780;&#23457;&#20154;&#33258;&#21160;&#20998;&#37197;&#30340;&#30740;&#31350;&#25104;&#26524;&#65292;&#24182;&#20171;&#32461;&#20102;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#28909;&#28857;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36866;&#24403;&#30340;&#35780;&#23457;&#20154;&#20998;&#37197;&#26174;&#33879;&#24433;&#21709;&#35780;&#20272;&#30340;&#36136;&#37327;&#65292;&#22240;&#20026;&#20934;&#30830;&#21644;&#20844;&#27491;&#30340;&#23457;&#26597;&#21462;&#20915;&#20110;&#23558;&#20854;&#20998;&#37197;&#32473;&#30456;&#20851;&#30340;&#35780;&#23457;&#20154;&#12290;&#23558;&#35780;&#23457;&#20154;&#20998;&#37197;&#32473;&#25552;&#20132;&#30340;&#25552;&#26696;&#26159;&#23457;&#26597;&#36807;&#31243;&#30340;&#36215;&#28857;&#65292;&#20063;&#31216;&#20026;&#35780;&#23457;&#20154;&#20998;&#37197;&#38382;&#39064;&#65288;RAP&#65289;&#12290;&#30001;&#20110;&#25163;&#21160;&#20998;&#37197;&#30340;&#26126;&#26174;&#38480;&#21046;&#65292;&#26399;&#21002;&#32534;&#36753;&#12289;&#20250;&#35758;&#32452;&#32455;&#32773;&#21644;&#25320;&#27454;&#32463;&#29702;&#35201;&#27714;&#33258;&#21160;&#35780;&#23457;&#20154;&#20998;&#37197;&#26041;&#27861;&#12290;&#33258;1992&#24180;&#20197;&#26469;&#65292;&#35768;&#22810;&#30740;&#31350;&#25552;&#20986;&#20102;&#20998;&#37197;&#35299;&#20915;&#26041;&#26696;&#20197;&#21709;&#24212;&#33258;&#21160;&#21270;&#31243;&#24207;&#30340;&#38656;&#27714;&#12290;&#26412;&#27425;&#35843;&#26597;&#25253;&#21578;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#20026;&#23398;&#32773;&#21644;&#23454;&#36341;&#32773;&#25552;&#20379;&#20851;&#20110;RAP&#39046;&#22495;&#20013;&#21487;&#29992;&#30740;&#31350;&#30340;&#20840;&#38754;&#27010;&#36848;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#26412;&#25991;&#23545;&#36807;&#21435;&#19977;&#21313;&#24180;&#20869;&#21457;&#34920;&#22312;Web of Science&#12289;Scopus&#12289;ScienceDirect&#21644;Google Scholar&#31561;&#25968;&#25454;&#24211;&#20013;&#30340;103&#31687;&#35780;&#23457;&#20154;&#20998;&#37197;&#39046;&#22495;&#30340;&#20986;&#29256;&#29289;&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#31995;&#32479;&#32508;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
Appropriate reviewer assignment significantly impacts the quality of proposal evaluation, as accurate and fair reviews are contingent on their assignment to relevant reviewers. The crucial task of assigning reviewers to submitted proposals is the starting point of the review process and is also known as the reviewer assignment problem (RAP). Due to the obvious restrictions of manual assignment, journal editors, conference organizers, and grant managers demand automatic reviewer assignment approaches. Many studies have proposed assignment solutions in response to the demand for automated procedures since 1992. The primary objective of this survey paper is to provide scholars and practitioners with a comprehensive overview of available research on the RAP. To achieve this goal, this article presents an in-depth systematic review of 103 publications in the field of reviewer assignment published in the past three decades and available in the Web of Science, Scopus, ScienceDirect, Google Sc
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#28857;&#23545;&#28857;&#30340;QPP&#26694;&#26550;&#65292;&#36890;&#36807;&#27979;&#37327;&#27599;&#20010;&#39044;&#27979;&#19982;&#30456;&#24212;&#30495;&#23454;&#20540;&#20043;&#38388;&#30340;&#20559;&#24046;&#26469;&#35780;&#20272;QPP&#31995;&#32479;&#30340;&#21333;&#20010;&#26597;&#35810;&#36136;&#37327;&#65292;&#24471;&#21040;&#30340;&#26041;&#27861;&#33021;&#22815;&#22312;&#21508;&#31181;&#30446;&#26631;&#25351;&#26631;&#21644;&#26816;&#32034;&#27169;&#22411;&#19978;&#23548;&#33268;&#26356;&#23567;&#30340;QPP&#35780;&#20272;&#26041;&#24046;&#12290;</title><link>http://arxiv.org/abs/2304.00310</link><description>&lt;p&gt;
&#35770;&#28857;&#32423;&#21035;&#35780;&#20215;&#31995;&#32479;&#30340;&#21487;&#34892;&#24615;&#21644;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Feasibility and Robustness of Pointwise Evaluation of Query Performance Prediction. (arXiv:2304.00310v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00310
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#28857;&#23545;&#28857;&#30340;QPP&#26694;&#26550;&#65292;&#36890;&#36807;&#27979;&#37327;&#27599;&#20010;&#39044;&#27979;&#19982;&#30456;&#24212;&#30495;&#23454;&#20540;&#20043;&#38388;&#30340;&#20559;&#24046;&#26469;&#35780;&#20272;QPP&#31995;&#32479;&#30340;&#21333;&#20010;&#26597;&#35810;&#36136;&#37327;&#65292;&#24471;&#21040;&#30340;&#26041;&#27861;&#33021;&#22815;&#22312;&#21508;&#31181;&#30446;&#26631;&#25351;&#26631;&#21644;&#26816;&#32034;&#27169;&#22411;&#19978;&#23548;&#33268;&#26356;&#23567;&#30340;QPP&#35780;&#20272;&#26041;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#26597;&#35810;&#30340;&#26816;&#32034;&#26377;&#25928;&#24615;&#24444;&#27492;&#29420;&#31435;&#65292;&#20294;&#26159;&#26597;&#35810;&#24615;&#33021;&#39044;&#27979;(QPP)&#31995;&#32479;&#30340;&#35780;&#20272;&#19968;&#30452;&#26159;&#36890;&#36807;&#27979;&#37327;&#25972;&#20010;&#26597;&#35810;&#38598;&#21512;&#19978;&#30340;&#31561;&#32423;&#30456;&#20851;&#24615;&#26469;&#23436;&#25104;&#30340;&#12290;&#36825;&#31181;&#21015;&#34920;&#26041;&#27861;&#26377;&#35768;&#22810;&#32570;&#28857;&#65292;&#23588;&#20854;&#26159;&#23427;&#19981;&#25903;&#25345;&#35780;&#20272;&#21333;&#20010;&#26597;&#35810;&#30340;QPP&#30340;&#24120;&#35265;&#35201;&#27714;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#28857;&#23545;&#28857;&#30340;QPP&#26694;&#26550;&#65292;&#36890;&#36807;&#27979;&#37327;&#27599;&#20010;&#39044;&#27979;&#19982;&#30456;&#24212;&#30495;&#23454;&#20540;&#20043;&#38388;&#30340;&#20559;&#24046;&#65292;&#28982;&#21518;&#22312;&#19968;&#32452;&#26597;&#35810;&#19978;&#32858;&#21512;&#32467;&#26524;&#65292;&#21487;&#20197;&#35780;&#20272;QPP&#31995;&#32479;&#23545;&#21508;&#20010;&#26597;&#35810;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#31181;&#26032;&#26041;&#27861;&#22312;&#21508;&#31181;&#30446;&#26631;&#25351;&#26631;&#21644;&#26816;&#32034;&#27169;&#22411;&#19978;&#22343;&#23548;&#33268;QPP&#35780;&#20272;&#26041;&#24046;&#26356;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the retrieval effectiveness of queries being mutually independent of one another, the evaluation of query performance prediction (QPP) systems has been carried out by measuring rank correlation over an entire set of queries. Such a listwise approach has a number of disadvantages, notably that it does not support the common requirement of assessing QPP for individual queries. In this paper, we propose a pointwise QPP framework that allows us to evaluate the quality of a QPP system for individual queries by measuring the deviations between each prediction versus the corresponding true value, and then aggregating the results over a set of queries. Our experiments demonstrate that this new approach leads to smaller variances in QPP evaluations across a range of different target metrics and retrieval models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;BGCH&#30340;&#31471;&#21040;&#31471;&#21452;&#20998;&#22270;&#22270;&#21367;&#31215;&#21704;&#24076;&#26041;&#27861;&#65292;&#22312;&#27721;&#26126;&#31354;&#38388;&#20013;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;Top-N&#25628;&#32034;&#65292;&#24182;&#20855;&#26377;&#19977;&#20010;&#26032;&#39062;&#32780;&#26377;&#25928;&#30340;&#27169;&#22359;&#65306;&#33258;&#36866;&#24212;&#22270;&#21367;&#31215;&#21704;&#24076;&#65292;&#28508;&#22312;&#29305;&#24449;&#20998;&#25955;&#21644;&#20613;&#37324;&#21494;&#24207;&#21015;&#21270;&#26799;&#24230;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2304.00241</link><description>&lt;p&gt;
&#21452;&#20998;&#22270;&#22270;&#21367;&#31215;&#21704;&#24076;&#22312;&#27721;&#26126;&#31354;&#38388;&#20013;&#26377;&#25928;&#12289;&#39640;&#25928;&#22320;&#36827;&#34892;Top-N&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Bipartite Graph Convolutional Hashing for Effective and Efficient Top-N Search in Hamming Space. (arXiv:2304.00241v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00241
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;BGCH&#30340;&#31471;&#21040;&#31471;&#21452;&#20998;&#22270;&#22270;&#21367;&#31215;&#21704;&#24076;&#26041;&#27861;&#65292;&#22312;&#27721;&#26126;&#31354;&#38388;&#20013;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;Top-N&#25628;&#32034;&#65292;&#24182;&#20855;&#26377;&#19977;&#20010;&#26032;&#39062;&#32780;&#26377;&#25928;&#30340;&#27169;&#22359;&#65306;&#33258;&#36866;&#24212;&#22270;&#21367;&#31215;&#21704;&#24076;&#65292;&#28508;&#22312;&#29305;&#24449;&#20998;&#25955;&#21644;&#20613;&#37324;&#21494;&#24207;&#21015;&#21270;&#26799;&#24230;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#23454;&#38469;&#30340;&#32593;&#32476;&#24212;&#29992;&#20013;&#65292;&#20363;&#22914;&#22312;&#32447;&#25512;&#33616;&#12289;&#25968;&#25454;&#24211;&#26816;&#32034;&#21644;&#26597;&#35810;&#25991;&#26723;&#25628;&#32034;&#65292;&#21452;&#20998;&#22270;&#22270;&#25628;&#32034;&#26159;&#22522;&#30784;&#19988;&#22810;&#29992;&#36884;&#30340;&#12290;&#20256;&#32479;&#26041;&#27861;&#20381;&#36182;&#20110;&#36830;&#32493;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#30690;&#37327;&#21270;&#33410;&#28857;&#23884;&#20837;&#30340;&#30456;&#20284;&#24615;&#21305;&#37197;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#22788;&#29702;&#22823;&#35268;&#27169;&#30340;&#30456;&#20284;&#24230;&#35745;&#31639;&#65292;&#26368;&#36817;&#21457;&#23637;&#20102;&#19968;&#20123;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#21704;&#24076;&#25216;&#26415;&#12290;&#23613;&#31649;&#22312;&#27721;&#26126;&#31354;&#38388;&#20013;&#30340;&#26816;&#32034;&#25928;&#29575;&#24456;&#39640;&#65292;&#20294;&#20808;&#21069;&#30340;&#24037;&#20316;&#20173;&#38754;&#20020;&#28798;&#38590;&#24615;&#30340;&#24615;&#33021;&#34928;&#20943;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#21452;&#20998;&#22270;&#22270;&#21367;&#31215;&#32593;&#32476;&#36827;&#34892;&#21704;&#24076;&#30340;&#38382;&#39064;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;Top-N&#25628;&#32034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#21452;&#20998;&#22270;&#22270;&#21367;&#31215;&#21704;&#24076;&#26041;&#27861;&#65288;BGCH&#65289;&#65292;&#23427;&#30001;&#19977;&#20010;&#26032;&#39062;&#32780;&#26377;&#25928;&#30340;&#27169;&#22359;&#32452;&#25104;&#65306;&#65288;1&#65289;&#33258;&#36866;&#24212;&#22270;&#21367;&#31215;&#21704;&#24076;&#65292;&#65288;2&#65289;&#28508;&#22312;&#29305;&#24449;&#20998;&#25955;&#65292;&#20197;&#21450;&#65288;3&#65289;&#20613;&#37324;&#21494;&#24207;&#21015;&#21270;&#26799;&#24230;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Searching on bipartite graphs is basal and versatile to many real-world Web applications, e.g., online recommendation, database retrieval, and query-document searching. Given a query node, the conventional approaches rely on the similarity matching with the vectorized node embeddings in the continuous Euclidean space. To efficiently manage intensive similarity computation, developing hashing techniques for graph structured data has recently become an emerging research direction. Despite the retrieval efficiency in Hamming space, prior work is however confronted with catastrophic performance decay. In this work, we investigate the problem of hashing with Graph Convolutional Network on bipartite graphs for effective Top-N search. We propose an end-to-end Bipartite Graph Convolutional Hashing approach, namely BGCH, which consists of three novel and effective modules: (1) adaptive graph convolutional hashing, (2) latent feature dispersion, and (3) Fourier serialized gradient estimation. Sp
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#26368;&#26032;&#30340;&#24819;&#27861;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35270;&#20026;&#21487;&#20197;&#35775;&#38382;&#21253;&#21547;&#20851;&#20110;&#32452;&#32455;&#12289;&#26426;&#26500;&#21644;&#20844;&#21496;&#30340;&#26368;&#26032;&#21644;&#20934;&#30830;&#20449;&#24687;&#30340;&#20195;&#29702;&#65292;&#32467;&#21512;&#27668;&#20505;&#21464;&#21270;&#25968;&#25454;&#24211;&#21644;&#24120;&#35268; Google &#25628;&#32034;&#65292;&#20197;&#25552;&#20379;&#26356;&#21487;&#38752;&#21644;&#20934;&#30830;&#30340;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2304.00116</link><description>&lt;p&gt;
&#21033;&#29992;&#27668;&#20505;&#36164;&#28304;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Enhancing Large Language Models with Climate Resources. (arXiv:2304.00116v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00116
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#26368;&#26032;&#30340;&#24819;&#27861;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35270;&#20026;&#21487;&#20197;&#35775;&#38382;&#21253;&#21547;&#20851;&#20110;&#32452;&#32455;&#12289;&#26426;&#26500;&#21644;&#20844;&#21496;&#30340;&#26368;&#26032;&#21644;&#20934;&#30830;&#20449;&#24687;&#30340;&#20195;&#29702;&#65292;&#32467;&#21512;&#27668;&#20505;&#21464;&#21270;&#25968;&#25454;&#24211;&#21644;&#24120;&#35268; Google &#25628;&#32034;&#65292;&#20197;&#25552;&#20379;&#26356;&#21487;&#38752;&#21644;&#20934;&#30830;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#22312;&#29983;&#25104;&#21508;&#31181;&#20027;&#39064;&#30340;&#31867;&#20154;&#25991;&#26412;&#26041;&#38754;&#23637;&#31034;&#20102;&#20854;&#33021;&#21147;&#65292;&#26174;&#33879;&#22320;&#25913;&#21464;&#20102;&#20154;&#24037;&#26234;&#33021;&#30340;&#26684;&#23616;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#23427;&#20204;&#20855;&#26377;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#20294; LLM &#32570;&#20047;&#26368;&#36817;&#30340;&#20449;&#24687;&#65292;&#24182;&#19988;&#24120;&#24120;&#20351;&#29992;&#19981;&#22815;&#20934;&#30830;&#30340;&#35821;&#35328;&#65292;&#36825;&#22312;&#38656;&#35201;&#20934;&#30830;&#24615;&#30340;&#39046;&#22495;&#27604;&#22914;&#27668;&#20505;&#21464;&#21270;&#20013;&#21487;&#33021;&#20250;&#26377;&#23475;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#26368;&#26032;&#30340;&#24819;&#27861;&#26469;&#21033;&#29992; LLM &#30340;&#28508;&#21147;&#65292;&#23558;&#23427;&#20204;&#35270;&#20026;&#21487;&#20197;&#35775;&#38382;&#21253;&#21547;&#20851;&#20110;&#32452;&#32455;&#12289;&#26426;&#26500;&#21644;&#20844;&#21496;&#30340;&#26368;&#26032;&#21644;&#20934;&#30830;&#20449;&#24687;&#30340;&#25968;&#25454;&#24211;&#30340;&#20195;&#29702;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#21407;&#22411;&#20195;&#29702;&#28436;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#35813;&#20195;&#29702;&#20174; ClimateWatch (https://www.climatewatchdata.org/) &#26816;&#32034;&#25490;&#25918;&#25968;&#25454;&#24182;&#21033;&#29992;&#24120;&#35268; Google &#25628;&#32034;&#12290;&#36890;&#36807;&#23558;&#36825;&#20123;&#36164;&#28304;&#19982; LLM &#38598;&#25104;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20811;&#26381;&#20102;&#19982;&#19981;&#31934;&#30830;&#35821;&#35328;&#30456;&#20851;&#30340;&#38480;&#21046;&#65292;&#22312;&#27668;&#20505;&#21464;&#21270;&#31561;&#39046;&#22495;&#25552;&#20379;&#20102;&#26356;&#21487;&#38752;&#21644;&#20934;&#30830;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have significantly transformed the landscape of artificial intelligence by demonstrating their ability in generating human-like text across diverse topics. However, despite their impressive capabilities, LLMs lack recent information and often employ imprecise language, which can be detrimental in domains where accuracy is crucial, such as climate change. In this study, we make use of recent ideas to harness the potential of LLMs by viewing them as agents that access multiple sources, including databases containing recent and precise information about organizations, institutions, and companies. We demonstrate the effectiveness of our method through a prototype agent that retrieves emission data from ClimateWatch (https://www.climatewatchdata.org/) and leverages general Google search. By integrating these resources with LLMs, our approach overcomes the limitations associated with imprecise language and delivers more reliable and accurate information in the cr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20351;&#29992;&#31232;&#30095;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#39640;&#25928;&#31264;&#23494;&#26816;&#32034;&#65292;&#20351;&#29992;Tevatron &#21644;MSMARCO&#12289;NQ&#21644;TriviaQA&#25968;&#25454;&#38598;&#65292;&#21457;&#29616;&#31232;&#30095;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#30452;&#25509;&#26367;&#25442;&#20026;&#21407;&#26377;&#27169;&#22411;&#65292;&#20960;&#20046;&#19981;&#38477;&#20302;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#25512;&#29702;&#36895;&#24230;&#25552;&#39640;&#20102;&#26368;&#22810;4.3&#20493;&#12290;</title><link>http://arxiv.org/abs/2304.00114</link><description>&lt;p&gt;
&#31264;&#23494;&#31232;&#30095;&#26816;&#32034;&#65306;&#20351;&#29992;&#31232;&#30095;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#39640;&#25928;&#31264;&#23494;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Dense Sparse Retrieval: Using Sparse Language Models for Inference Efficient Dense Retrieval. (arXiv:2304.00114v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00114
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20351;&#29992;&#31232;&#30095;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#39640;&#25928;&#31264;&#23494;&#26816;&#32034;&#65292;&#20351;&#29992;Tevatron &#21644;MSMARCO&#12289;NQ&#21644;TriviaQA&#25968;&#25454;&#38598;&#65292;&#21457;&#29616;&#31232;&#30095;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#30452;&#25509;&#26367;&#25442;&#20026;&#21407;&#26377;&#27169;&#22411;&#65292;&#20960;&#20046;&#19981;&#38477;&#20302;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#25512;&#29702;&#36895;&#24230;&#25552;&#39640;&#20102;&#26368;&#22810;4.3&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#21521;&#37327;&#30340;&#26816;&#32034;&#31995;&#32479;&#24050;&#25104;&#20026;&#23398;&#26415;&#21644;&#24037;&#19994;&#25628;&#32034;&#24212;&#29992;&#30340;&#24120;&#35265;&#24037;&#20855;&#65292;&#22240;&#20026;&#23427;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#21487;&#25193;&#23637;&#30340;&#26041;&#24335;&#26469;&#21033;&#29992;&#25991;&#26723;&#21644;&#26597;&#35810;&#30340;&#19978;&#19979;&#25991;&#34920;&#31034;&#36827;&#34892;&#25628;&#32034;&#12290;&#30001;&#20110;&#36825;&#20123;&#21521;&#37327;&#31995;&#32479;&#20381;&#36182;&#20110;&#19978;&#19979;&#25991;&#35821;&#35328;&#27169;&#22411;&#65292;&#22240;&#27492;&#23427;&#20204;&#36890;&#24120;&#38656;&#35201;GPU&#30340;&#20351;&#29992;&#65292;&#36825;&#21487;&#33021;&#20250;&#24456;&#26114;&#36149;&#19988;&#38590;&#20197;&#31649;&#29702;&#12290;&#37492;&#20110;&#36817;&#24180;&#26469;&#24341;&#20837;&#31232;&#30095;&#24615;&#20197;&#25552;&#39640;&#25512;&#29702;&#25928;&#29575;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20351;&#29992;&#31232;&#30095;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#31264;&#23494;&#26816;&#32034;&#20197;&#25552;&#39640;&#25512;&#29702;&#25928;&#29575;&#12290;&#20351;&#29992;&#27969;&#34892;&#30340;&#26816;&#32034;&#24211;Tevatron&#21644;MSMARCO&#12289;NQ&#21644;TriviaQA&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#21457;&#29616;&#31232;&#30095;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#30452;&#25509;&#26367;&#25442;&#20026;&#21407;&#26377;&#27169;&#22411;&#65292;&#20960;&#20046;&#19981;&#38477;&#20302;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#25512;&#29702;&#36895;&#24230;&#25552;&#39640;&#20102;&#26368;&#22810;4.3&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vector-based retrieval systems have become a common staple for academic and industrial search applications because they provide a simple and scalable way of extending the search to leverage contextual representations for documents and queries. As these vector-based systems rely on contextual language models, their usage commonly requires GPUs, which can be expensive and difficult to manage. Given recent advances in introducing sparsity into language models for improved inference efficiency, in this paper, we study how sparse language models can be used for dense retrieval to improve inference efficiency. Using the popular retrieval library Tevatron and the MSMARCO, NQ, and TriviaQA datasets, we find that sparse language models can be used as direct replacements with little to no drop in accuracy and up to 4.3x improved inference speeds
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38754;&#21521;&#26053;&#34892;&#25252;&#29702;&#34892;&#19994;&#30340;&#25307;&#32856;&#26041;&#26696;&#65292;&#37319;&#29992;&#22810;&#27169;&#22411;&#25968;&#25454;&#26381;&#21153;&#21152;&#36895;&#25968;&#25454;&#37319;&#38598;&#65292;&#24182;&#20351;&#29992;&#21452;&#21521;&#24378;&#21270;&#23398;&#20064;&#21644;&#20027;&#21160;&#23398;&#20064;&#25552;&#20379;&#20010;&#24615;&#21270;&#25512;&#33616;&#65292;&#35299;&#20915;&#20102;&#36825;&#19968;&#34892;&#19994;&#26631;&#27880;&#25968;&#25454;&#30701;&#32570;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.00006</link><description>&lt;p&gt;
&#38754;&#21521;&#26053;&#34892;&#25252;&#29702;&#34892;&#19994;&#30340;&#20351;&#29992;&#22810;&#27169;&#22411;&#25968;&#25454;&#26381;&#21153;&#30340;&#21452;&#21521;&#20010;&#24615;&#21270;&#24378;&#21270;&#23398;&#20064;&#26550;&#26500;&#20013;&#30340;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Bi-directional personalization reinforcement learning-based architecture with active learning using a multi-model data service for the travel nursing industry. (arXiv:2304.00006v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00006
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38754;&#21521;&#26053;&#34892;&#25252;&#29702;&#34892;&#19994;&#30340;&#25307;&#32856;&#26041;&#26696;&#65292;&#37319;&#29992;&#22810;&#27169;&#22411;&#25968;&#25454;&#26381;&#21153;&#21152;&#36895;&#25968;&#25454;&#37319;&#38598;&#65292;&#24182;&#20351;&#29992;&#21452;&#21521;&#24378;&#21270;&#23398;&#20064;&#21644;&#20027;&#21160;&#23398;&#20064;&#25552;&#20379;&#20010;&#24615;&#21270;&#25512;&#33616;&#65292;&#35299;&#20915;&#20102;&#36825;&#19968;&#34892;&#19994;&#26631;&#27880;&#25968;&#25454;&#30701;&#32570;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22914;&#20309;&#36890;&#36807;&#20351;&#29992;&#22810;&#27169;&#22411;&#25968;&#25454;&#26381;&#21153;&#21152;&#36895;&#25968;&#25454;&#37319;&#38598;&#24182;&#20351;&#29992;&#21452;&#21521;&#24378;&#21270;&#23398;&#20064;&#21644;&#20027;&#21160;&#23398;&#20064;&#25552;&#20379;&#20010;&#24615;&#21270;&#25512;&#33616;&#65292;&#20174;&#32780;&#22686;&#24378;&#26053;&#34892;&#25252;&#29702;&#34892;&#19994;&#30340;&#25307;&#32856;&#27969;&#31243;&#12290;&#35813;&#26041;&#26696;&#21487;&#24110;&#21161;&#25307;&#32856;&#20154;&#21592;&#25512;&#33616;&#21512;&#26684;&#30003;&#35831;&#20154;&#65292;&#24182;&#20351;&#30003;&#35831;&#20154;&#25509;&#25910;&#21040;&#20010;&#24615;&#21270;&#24037;&#20316;&#25512;&#33616;&#12290;&#26412;&#25991;&#36824;&#35752;&#35770;&#20102;&#20351;&#29992;&#20027;&#21160;&#23398;&#20064;&#26469;&#35299;&#20915;&#36825;&#19968;&#34892;&#19994;&#26631;&#27880;&#25968;&#25454;&#30701;&#32570;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The challenges of using inadequate online recruitment systems can be addressed with machine learning and software engineering techniques. Bi-directional personalization reinforcement learning-based architecture with active learning can get recruiters to recommend qualified applicants and also enable applicants to receive personalized job recommendations. This paper focuses on how machine learning techniques can enhance the recruitment process in the travel nursing industry by helping speed up data acquisition using a multi-model data service and then providing personalized recommendations using bi-directional reinforcement learning with active learning. This need was especially evident when trying to respond to the overwhelming needs of healthcare facilities during the COVID-19 pandemic. The need for traveling nurses and other healthcare professionals was more evident during the lockdown period. A data service was architected for job feed processing using an orchestration of natural la
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#21547;&#22122;&#25216;&#33021;&#26631;&#31614;&#30340;&#26080;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#35757;&#32451;&#20986;&#26377;&#25928;&#30340;&#32844;&#31216;&#30456;&#20284;&#24615;&#27169;&#22411;&#65292;&#23545;&#20110;&#25991;&#26412;&#25490;&#24207;&#21644;&#32844;&#20301;&#26631;&#20934;&#21270;&#31561;&#20219;&#21153;&#38750;&#24120;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2207.00494</link><description>&lt;p&gt;
&#20174;&#21547;&#22122;&#25216;&#33021;&#26631;&#31614;&#20013;&#23398;&#20064;&#32844;&#31216;&#30456;&#20284;&#24230;
&lt;/p&gt;
&lt;p&gt;
Learning Job Titles Similarity from Noisy Skill Labels. (arXiv:2207.00494v3 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.00494
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#21547;&#22122;&#25216;&#33021;&#26631;&#31614;&#30340;&#26080;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#35757;&#32451;&#20986;&#26377;&#25928;&#30340;&#32844;&#31216;&#30456;&#20284;&#24615;&#27169;&#22411;&#65292;&#23545;&#20110;&#25991;&#26412;&#25490;&#24207;&#21644;&#32844;&#20301;&#26631;&#20934;&#21270;&#31561;&#20219;&#21153;&#38750;&#24120;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27979;&#37327;&#32844;&#31216;&#20043;&#38388;&#30340;&#35821;&#20041;&#30456;&#20284;&#24230;&#26159;&#33258;&#21160;&#32844;&#20301;&#25512;&#33616;&#30340;&#37325;&#35201;&#21151;&#33021;&#12290;&#36825;&#20010;&#20219;&#21153;&#36890;&#24120;&#20351;&#29992;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#26469;&#23454;&#29616;&#65292;&#38656;&#35201;&#20197;&#31561;&#25928;&#32844;&#31216;&#23545;&#30340;&#24418;&#24335;&#36827;&#34892;&#35757;&#32451;&#25968;&#25454;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#29992;&#21547;&#22122;&#25216;&#33021;&#26631;&#31614;&#35757;&#32451;&#32844;&#31216;&#30456;&#20284;&#24615;&#27169;&#22411;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#25991;&#26412;&#25490;&#24207;&#21644;&#32844;&#20301;&#26631;&#20934;&#21270;&#31561;&#20219;&#21153;&#20013;&#38750;&#24120;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Measuring semantic similarity between job titles is an essential functionality for automatic job recommendations. This task is usually approached using supervised learning techniques, which requires training data in the form of equivalent job title pairs. In this paper, we instead propose an unsupervised representation learning method for training a job title similarity model using noisy skill labels. We show that it is highly effective for tasks such as text ranking and job normalization.
&lt;/p&gt;</description></item><item><title>ART&#26159;&#19968;&#31181;&#33021;&#22815;&#19981;&#20351;&#29992;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#30340;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#65292;&#24182;&#19988;&#21482;&#38656;&#35201;&#35775;&#38382;&#26410;&#37197;&#23545;&#30340;&#36755;&#20837;&#21644;&#36755;&#20986;&#12290;&#23427;&#20351;&#29992;&#19968;&#20010;&#26032;&#30340;&#25991;&#26723;&#26816;&#32034;&#33258;&#32534;&#30721;&#26041;&#26696;&#65292;&#36890;&#36807;&#38382;&#39064;&#37325;&#26500;&#36827;&#34892;&#26816;&#32034;&#35757;&#32451;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#36827;&#34892;&#26080;&#30417;&#30563;&#23398;&#20064;&#65292;&#24182;&#19988;&#21487;&#20197;&#23558;&#20854;&#21512;&#24182;&#21040;&#23436;&#25972;&#30340;Open QA&#31995;&#32479;&#20013;&#12290;</title><link>http://arxiv.org/abs/2206.10658</link><description>&lt;p&gt;
&#31572;&#26696;&#26377;&#35299;&#65306;&#26080;&#38656;&#26631;&#35760;&#25968;&#25454;&#26469;&#35757;&#32451;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Questions Are All You Need to Train a Dense Passage Retriever. (arXiv:2206.10658v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.10658
&lt;/p&gt;
&lt;p&gt;
ART&#26159;&#19968;&#31181;&#33021;&#22815;&#19981;&#20351;&#29992;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#30340;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#65292;&#24182;&#19988;&#21482;&#38656;&#35201;&#35775;&#38382;&#26410;&#37197;&#23545;&#30340;&#36755;&#20837;&#21644;&#36755;&#20986;&#12290;&#23427;&#20351;&#29992;&#19968;&#20010;&#26032;&#30340;&#25991;&#26723;&#26816;&#32034;&#33258;&#32534;&#30721;&#26041;&#26696;&#65292;&#36890;&#36807;&#38382;&#39064;&#37325;&#26500;&#36827;&#34892;&#26816;&#32034;&#35757;&#32451;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#36827;&#34892;&#26080;&#30417;&#30563;&#23398;&#20064;&#65292;&#24182;&#19988;&#21487;&#20197;&#23558;&#20854;&#21512;&#24182;&#21040;&#23436;&#25972;&#30340;Open QA&#31995;&#32479;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;ART&#65292;&#19968;&#31181;&#26032;&#30340;&#35821;&#26009;&#24211;&#32423;&#33258;&#32534;&#30721;&#26041;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#65292;&#19981;&#38656;&#35201;&#20219;&#20309;&#26631;&#35760;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#22312;&#24320;&#25918;&#22495;&#20219;&#21153;&#20013;&#65292;&#22914;&#24320;&#25918;&#24335;&#38382;&#31572;&#65288;Open QA&#65289;&#20013;&#65292;&#23494;&#38598;&#26816;&#32034;&#26159;&#19968;&#20010;&#20013;&#24515;&#25361;&#25112;&#65292;&#20854;&#20013;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#30417;&#30563;&#25968;&#25454;&#38598;&#65292;&#20855;&#26377;&#33258;&#23450;&#20041;&#30340;&#30828;&#36127;&#38754;&#25366;&#25496;&#21644;&#27491;&#38754;&#31034;&#20363;&#21435;&#22122;&#22768;&#12290;&#19982;&#27492;&#30456;&#21453;&#65292;ART&#21482;&#38656;&#35201;&#35775;&#38382;&#26410;&#37197;&#23545;&#30340;&#36755;&#20837;&#21644;&#36755;&#20986;&#65288;&#20363;&#22914;&#65292;&#38382;&#39064;&#21644;&#28508;&#22312;&#31572;&#26696;&#25991;&#20214;&#65289;&#12290;&#23427;&#20351;&#29992;&#19968;&#31181;&#26032;&#30340;&#25991;&#26723;&#26816;&#32034;&#33258;&#32534;&#30721;&#26041;&#26696;&#65292;&#20854;&#20013;&#65288;1&#65289;&#36755;&#20837;&#38382;&#39064;&#29992;&#20110;&#26816;&#32034;&#19968;&#32452;&#35777;&#25454;&#25991;&#26723;&#65292;&#65288;2&#65289;&#28982;&#21518;&#20351;&#29992;&#25991;&#26723;&#35745;&#31639;&#37325;&#26500;&#21407;&#22987;&#38382;&#39064;&#30340;&#27010;&#29575;&#12290;&#22522;&#20110;&#38382;&#39064;&#37325;&#26500;&#30340;&#26816;&#32034;&#35757;&#32451;&#21487;&#20197;&#26377;&#25928;&#22320;&#36827;&#34892;&#26080;&#30417;&#30563;&#23398;&#20064;&#65292;&#21253;&#25324;&#25991;&#26723;&#21644;&#38382;&#39064;&#32534;&#30721;&#22120;&#65292;&#21487;&#20197;&#31245;&#21518;&#23558;&#20854;&#21512;&#24182;&#21040;&#23436;&#25972;&#30340;Open QA&#31995;&#32479;&#20013;&#65292;&#32780;&#19981;&#38656;&#35201;&#36827;&#19968;&#27493;&#36827;&#34892;&#24494;&#35843;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;ART&#33719;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce ART, a new corpus-level autoencoding approach for training dense retrieval models that does not require any labeled training data. Dense retrieval is a central challenge for open-domain tasks, such as Open QA, where state-of-the-art methods typically require large supervised datasets with custom hard-negative mining and denoising of positive examples. ART, in contrast, only requires access to unpaired inputs and outputs (e.g. questions and potential answer documents). It uses a new document-retrieval autoencoding scheme, where (1) an input question is used to retrieve a set of evidence documents, and (2) the documents are then used to compute the probability of reconstructing the original question. Training for retrieval based on question reconstruction enables effective unsupervised learning of both document and question encoders, which can be later incorporated into complete Open QA systems without any further finetuning. Extensive experiments demonstrate that ART obtain
&lt;/p&gt;</description></item><item><title>&#39057;&#32321;&#35746;&#38405;&#21644;&#20851;&#27880;&#22806;&#38142;&#26159;&#23548;&#33268;&#20855;&#26377;&#21453;&#24863;&#24773;&#32490;&#30340;&#29992;&#25143;&#35266;&#30475;YouTube&#19978;&#30340;&#26497;&#31471;&#35270;&#39057;&#30340;&#20027;&#35201;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2204.10921</link><description>&lt;p&gt;
&#35746;&#38405;&#21644;&#22806;&#37096;&#38142;&#25509;&#20419;&#20351;&#20855;&#26377;&#21453;&#24863;&#24773;&#32490;&#30340;&#29992;&#25143;&#35266;&#30475;YouTube&#19978;&#30340;&#26497;&#31471;&#35270;&#39057;
&lt;/p&gt;
&lt;p&gt;
Subscriptions and external links help drive resentful users to alternative and extremist YouTube videos. (arXiv:2204.10921v2 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.10921
&lt;/p&gt;
&lt;p&gt;
&#39057;&#32321;&#35746;&#38405;&#21644;&#20851;&#27880;&#22806;&#38142;&#26159;&#23548;&#33268;&#20855;&#26377;&#21453;&#24863;&#24773;&#32490;&#30340;&#29992;&#25143;&#35266;&#30475;YouTube&#19978;&#30340;&#26497;&#31471;&#35270;&#39057;&#30340;&#20027;&#35201;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#24179;&#21488;&#26159;&#21542;&#20419;&#36827;&#20102;&#26377;&#23475;&#20869;&#23481;&#30340;&#28040;&#36153;&#65311;&#26412;&#30740;&#31350;&#21033;&#29992;2020&#24180;&#20195;&#34920;&#24615;&#26679;&#26412;&#65288;n=1,181&#65289;&#30340;&#34892;&#20026;&#21644;&#35843;&#26597;&#25968;&#25454;&#65292;&#25581;&#31034;&#20102;&#22312;YouTube&#19978;&#35266;&#30475;&#26367;&#20195;&#21644;&#26497;&#31471;&#28192;&#36947;&#35270;&#39057;&#30340;&#20154;&#32676;&#20027;&#35201;&#38598;&#20013;&#22312;&#39640;&#20808;&#21069;&#24615;&#21035;&#21644;&#31181;&#26063;&#21453;&#24863;&#27700;&#24179;&#30340;&#23569;&#25968;&#20154;&#20043;&#38388;&#12290;&#36825;&#20123;&#35266;&#20247;&#32463;&#24120;&#35746;&#38405;&#36825;&#20123;&#39057;&#36947;&#65288;&#20419;&#20351;&#21521;&#20182;&#20204;&#30340;&#35270;&#39057;&#25512;&#33616;&#65289;&#65292;&#24182;&#20851;&#27880;&#22806;&#37096;&#38142;&#25509;&#12290;&#30456;&#21453;&#65292;&#38750;&#35746;&#38405;&#29992;&#25143;&#24456;&#23569;&#30475;&#21040;&#25110;&#20851;&#27880;&#36825;&#20123;&#39057;&#36947;&#30340;&#35270;&#39057;&#12290;&#36825;&#20123;&#21457;&#29616;&#34920;&#26126;&#65292;&#25105;&#20204;&#35266;&#23519;&#30340;2020&#24180;&#26399;&#38388;&#65292;YouTube&#31639;&#27861;&#27809;&#26377;&#25226;&#20154;&#20204;&#24102;&#20837;&#8220;&#20820;&#23376;&#27934;&#8221;&#65292;&#21487;&#33021;&#26159;&#30001;&#20110;&#35813;&#20844;&#21496;&#22312;2019&#24180;&#23545;&#20854;&#25512;&#33616;&#31995;&#32479;&#36827;&#34892;&#30340;&#26356;&#25913;&#12290;&#28982;&#32780;&#65292;&#35813;&#24179;&#21488;&#22312;&#20419;&#36827;&#19987;&#27880;&#35266;&#20247;&#25509;&#35302;&#26367;&#20195;&#21644;&#26497;&#31471;&#28192;&#36947;&#30340;&#20869;&#23481;&#26041;&#38754;&#32487;&#32493;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Do online platforms facilitate the consumption of potentially harmful content? Using paired behavioral and survey data provided by participants recruited from a representative sample in 2020 (n=1,181), we show that exposure to alternative and extremist channel videos on YouTube is heavily concentrated among a small group of people with high prior levels of gender and racial resentment. These viewers often subscribe to these channels (prompting recommendations to their videos) and follow external links to them. In contrast, non-subscribers rarely see or follow recommendations to videos from these channels. Our findings suggest YouTube's algorithms were not sending people down "rabbit holes" during our observation window in 2020, possibly due to changes that the company made to its recommender system in 2019. However, the platform continues to play a key role in facilitating exposure to content from alternative and extremist channels among dedicated audiences.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38646;&#26679;&#26412;&#38382;&#39064;&#29983;&#25104;&#27169;&#22411;&#30340;&#37325;&#26032;&#25490;&#24207;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#24320;&#25918;&#24335;&#38382;&#39064;&#22238;&#31572;&#20013;&#30340;&#27573;&#33853;&#26816;&#32034;&#65292;&#21487;&#20197;&#36866;&#29992;&#20110;&#20219;&#20309;&#26816;&#32034;&#26041;&#27861;&#65292;&#26080;&#38656;&#29305;&#23450;&#35757;&#32451;&#65292;&#21487;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#20013;&#25552;&#39640;&#26816;&#32034;&#20934;&#30830;&#24615;&#65292;&#21487;&#20197;&#24110;&#21161;&#25552;&#39640;&#24320;&#25918;&#22495;&#38382;&#31572;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2204.07496</link><description>&lt;p&gt;
&#29992;&#38646;&#26679;&#26412;&#38382;&#39064;&#29983;&#25104;&#25552;&#39640;&#27573;&#33853;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Improving Passage Retrieval with Zero-Shot Question Generation. (arXiv:2204.07496v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.07496
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38646;&#26679;&#26412;&#38382;&#39064;&#29983;&#25104;&#27169;&#22411;&#30340;&#37325;&#26032;&#25490;&#24207;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#24320;&#25918;&#24335;&#38382;&#39064;&#22238;&#31572;&#20013;&#30340;&#27573;&#33853;&#26816;&#32034;&#65292;&#21487;&#20197;&#36866;&#29992;&#20110;&#20219;&#20309;&#26816;&#32034;&#26041;&#27861;&#65292;&#26080;&#38656;&#29305;&#23450;&#35757;&#32451;&#65292;&#21487;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#20013;&#25552;&#39640;&#26816;&#32034;&#20934;&#30830;&#24615;&#65292;&#21487;&#20197;&#24110;&#21161;&#25552;&#39640;&#24320;&#25918;&#22495;&#38382;&#31572;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#37325;&#26032;&#25490;&#24207;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#24320;&#25918;&#24335;&#38382;&#39064;&#22238;&#31572;&#20013;&#30340;&#27573;&#33853;&#26816;&#32034;&#12290;&#37325;&#26032;&#25490;&#24207;&#22120;&#20351;&#29992;&#38646;&#26679;&#26412;&#38382;&#39064;&#29983;&#25104;&#27169;&#22411;&#37325;&#26032;&#35780;&#20998;&#65292;&#35813;&#27169;&#22411;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#35745;&#31639;&#22522;&#20110;&#25152;&#26816;&#32034;&#21040;&#30340;&#27573;&#33853;&#30340;&#36755;&#20837;&#38382;&#39064;&#30340;&#27010;&#29575;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#20309;&#26816;&#32034;&#26041;&#27861;&#65288;&#22914;&#31070;&#32463;&#25110;&#22522;&#20110;&#20851;&#38190;&#23383;&#30340;&#26816;&#32034;&#65289;&#65292;&#19981;&#38656;&#35201;&#20219;&#20309;&#22495;&#25110;&#20219;&#21153;&#29305;&#23450;&#30340;&#35757;&#32451;&#65288;&#22240;&#27492;&#65292;&#39044;&#35745;&#26356;&#22909;&#22320;&#25512;&#24191;&#21040;&#25968;&#25454;&#20998;&#24067;&#36716;&#31227;&#65289;&#65292;&#24182;&#25552;&#20379;&#20102;&#26597;&#35810;&#21644;&#27573;&#33853;&#20043;&#38388;&#30340;&#20016;&#23500;&#20132;&#21449;&#27880;&#24847;&#65288;&#21363;&#24517;&#39035;&#35299;&#37322;&#38382;&#39064;&#20013;&#30340;&#27599;&#20010;&#20196;&#29260;&#65289;&#12290;&#22312;&#22810;&#20010;&#24320;&#25918;&#22495;&#26816;&#32034;&#25968;&#25454;&#38598;&#30340;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#30340;&#37325;&#26032;&#25490;&#24207;&#22120;&#23558;&#24378;&#22823;&#30340;&#26080;&#30417;&#30563;&#26816;&#32034;&#27169;&#22411;&#20013;&#25552;&#39640;&#20102;6&#65285;-18&#65285;&#65292;&#24182;&#23558;&#24378;&#22823;&#30340;&#30417;&#30563;&#27169;&#22411;&#20013;&#30340;&#21069;20&#20010;&#27573;&#33853;&#26816;&#32034;&#30340;&#20934;&#30830;&#24615;&#25552;&#39640;&#39640;&#36798;12&#65285;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#31616;&#21333;&#22320;&#28155;&#21152;&#26032;&#30340;&#37325;&#26032;&#25490;&#24207;&#22120;&#26469;&#33719;&#24471;&#20102;&#26032;&#30340;&#24320;&#25918;&#22495;&#38382;&#31572;&#30340;&#26368;&#26032;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a simple and effective re-ranking method for improving passage retrieval in open question answering. The re-ranker re-scores retrieved passages with a zero-shot question generation model, which uses a pre-trained language model to compute the probability of the input question conditioned on a retrieved passage. This approach can be applied on top of any retrieval method (e.g. neural or keyword-based), does not require any domain- or task-specific training (and therefore is expected to generalize better to data distribution shifts), and provides rich cross-attention between query and passage (i.e. it must explain every token in the question). When evaluated on a number of open-domain retrieval datasets, our re-ranker improves strong unsupervised retrieval models by 6%-18% absolute and strong supervised models by up to 12% in terms of top-20 passage retrieval accuracy. We also obtain new state-of-the-art results on full open-domain question answering by simply adding the new r
&lt;/p&gt;</description></item></channel></rss>