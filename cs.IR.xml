<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#27010;&#29575;&#25193;&#23637;&#25511;&#21046;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#31232;&#30095;&#26816;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#26032;&#30340;&#35757;&#32451;&#31639;&#27861;&#26377;&#25928;&#22320;&#20943;&#23569;&#20102;&#39640;&#32500;&#20849;&#28608;&#27963;&#21644;&#35821;&#20041;&#20559;&#24046;&#12290;</title><link>https://arxiv.org/abs/2402.17535</link><description>&lt;p&gt;
&#20855;&#26377;&#27010;&#29575;&#25193;&#23637;&#25511;&#21046;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#31232;&#30095;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Multimodal Learned Sparse Retrieval with Probabilistic Expansion Control
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17535
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#27010;&#29575;&#25193;&#23637;&#25511;&#21046;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#31232;&#30095;&#26816;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#26032;&#30340;&#35757;&#32451;&#31639;&#27861;&#26377;&#25928;&#22320;&#20943;&#23569;&#20102;&#39640;&#32500;&#20849;&#28608;&#27963;&#21644;&#35821;&#20041;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#31232;&#30095;&#26816;&#32034;&#65288;LSR&#65289;&#26159;&#19968;&#31867;&#31070;&#32463;&#26041;&#27861;&#65292;&#23558;&#26597;&#35810;&#21644;&#25991;&#26723;&#32534;&#30721;&#20026;&#31232;&#30095;&#30340;&#35789;&#27719;&#21521;&#37327;&#65292;&#21487;&#20197;&#36890;&#36807;&#20498;&#25490;&#32034;&#24341;&#39640;&#25928;&#32034;&#24341;&#21644;&#26816;&#32034;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;LSR&#22312;&#22810;&#27169;&#24577;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#37325;&#28857;&#20851;&#27880;&#25991;&#26412;-&#22270;&#20687;&#26816;&#32034;&#12290;&#34429;&#28982;LSR&#22312;&#25991;&#26412;&#26816;&#32034;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23427;&#22312;&#22810;&#27169;&#24577;&#26816;&#32034;&#20013;&#30340;&#24212;&#29992;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;&#24403;&#21069;&#30340;&#26041;&#27861;&#22914;LexLIP&#21644;STAIR&#38656;&#35201;&#23545;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#36827;&#34892;&#22797;&#26434;&#30340;&#22810;&#27493;&#35757;&#32451;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#23558;&#26469;&#33258;&#20923;&#32467;&#30340;&#31264;&#23494;&#27169;&#22411;&#30340;&#23494;&#38598;&#21521;&#37327;&#26377;&#25928;&#22320;&#36716;&#25442;&#20026;&#31232;&#30095;&#30340;&#35789;&#27719;&#21521;&#37327;&#12290;&#36890;&#36807;&#20351;&#29992;&#20271;&#21162;&#21033;&#38543;&#26426;&#21464;&#37327;&#25511;&#21046;&#26597;&#35810;&#25193;&#23637;&#65292;&#25105;&#20204;&#36890;&#36807;&#26032;&#30340;&#35757;&#32451;&#31639;&#27861;&#35299;&#20915;&#20102;&#39640;&#32500;&#20849;&#28608;&#27963;&#21644;&#35821;&#20041;&#20559;&#24046;&#30340;&#38382;&#39064;&#12290;&#22312;&#20004;&#20010;&#23494;&#38598;&#27169;&#22411;&#65288;BLIP&#12289;ALBEF&#65289;&#21644;&#20004;&#20010;&#25968;&#25454;&#38598;&#65288;MSCOCO&#12289;Flickr30k&#65289;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#26377;&#25928;&#22320;&#20943;&#23569;&#20102;&#20849;&#28608;&#27963;&#21644;&#35821;&#20041;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17535v1 Announce Type: new  Abstract: Learned sparse retrieval (LSR) is a family of neural methods that encode queries and documents into sparse lexical vectors that can be indexed and retrieved efficiently with an inverted index. We explore the application of LSR to the multi-modal domain, with a focus on text-image retrieval. While LSR has seen success in text retrieval, its application in multimodal retrieval remains underexplored. Current approaches like LexLIP and STAIR require complex multi-step training on massive datasets. Our proposed approach efficiently transforms dense vectors from a frozen dense model into sparse lexical vectors. We address issues of high dimension co-activation and semantic deviation through a new training algorithm, using Bernoulli random variables to control query expansion. Experiments with two dense models (BLIP, ALBEF) and two datasets (MSCOCO, Flickr30k) show that our proposed algorithm effectively reduces co-activation and semantic devia
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29992;&#25143;&#20223;&#30495;&#26694;&#26550;BASES&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#27169;&#25311;&#22823;&#35268;&#27169;&#31867;&#20154;&#31867;&#30340;&#32593;&#32476;&#25628;&#32034;&#34892;&#20026;&#12290;</title><link>https://arxiv.org/abs/2402.17505</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#32593;&#32476;&#25628;&#32034;&#29992;&#25143;&#20223;&#30495;
&lt;/p&gt;
&lt;p&gt;
BASES: Large-scale Web Search User Simulation with Large Language Model based Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17505
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29992;&#25143;&#20223;&#30495;&#26694;&#26550;BASES&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#27169;&#25311;&#22823;&#35268;&#27169;&#31867;&#20154;&#31867;&#30340;&#32593;&#32476;&#25628;&#32034;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20855;&#26377;&#20986;&#33394;&#30340;&#33021;&#21147;&#65292;&#22240;&#27492;&#24320;&#21457;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#20197;&#21487;&#38752;&#22320;&#20223;&#30495;&#29992;&#25143;&#21464;&#24471;&#21487;&#34892;&#12290;&#32771;&#34385;&#21040;&#30495;&#23454;&#29992;&#25143;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#21644;&#38480;&#21046;&#65288;&#20363;&#22914;&#38544;&#31169;&#38382;&#39064;&#65289;&#65292;&#26412;&#25991;&#38024;&#23545;&#32593;&#32476;&#25628;&#32034;&#36827;&#34892;&#22823;&#35268;&#27169;&#29992;&#25143;&#20223;&#30495;&#65292;&#20197;&#25913;&#36827;&#23545;&#29992;&#25143;&#25628;&#32034;&#34892;&#20026;&#30340;&#20998;&#26512;&#21644;&#24314;&#27169;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#29992;&#25143;&#20223;&#30495;&#26694;&#26550;BASES&#65292;&#20854;&#20013;&#21253;&#21547;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#65292;&#26088;&#22312;&#20419;&#36827;&#23545;&#32593;&#32476;&#25628;&#32034;&#29992;&#25143;&#34892;&#20026;&#30340;&#20840;&#38754;&#27169;&#25311;&#12290;&#25105;&#20204;&#30340;&#20223;&#30495;&#26694;&#26550;&#21487;&#20197;&#22823;&#35268;&#27169;&#29983;&#25104;&#29420;&#29305;&#30340;&#29992;&#25143;&#37197;&#32622;&#25991;&#20214;&#65292;&#20174;&#32780;&#23548;&#33268;&#22810;&#26679;&#21270;&#30340;&#25628;&#32034;&#34892;&#20026;&#12290;&#20026;&#20102;&#35777;&#26126;BASES&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#22522;&#20110;&#20013;&#33521;&#25991;&#30340;&#20004;&#20010;&#20154;&#31867;&#22522;&#20934;&#36827;&#34892;&#20102;&#35780;&#20272;&#23454;&#39564;&#65292;&#35777;&#26126;BASES&#33021;&#22815;&#26377;&#25928;&#22320;&#27169;&#25311;&#22823;&#35268;&#27169;&#31867;&#20284;&#20154;&#31867;&#30340;&#25628;&#32034;&#34892;&#20026;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#20419;&#36827;&#32593;&#32476;&#25628;&#32034;&#30740;&#31350;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#30340;&#22823;&#35268;&#27169;&#20223;&#30495;&#27979;&#35797;&#38598;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17505v1 Announce Type: cross  Abstract: Due to the excellent capacities of large language models (LLMs), it becomes feasible to develop LLM-based agents for reliable user simulation. Considering the scarcity and limit (e.g., privacy issues) of real user data, in this paper, we conduct large-scale user simulation for web search, to improve the analysis and modeling of user search behavior. Specially, we propose BASES, a novel user simulation framework with LLM-based agents, designed to facilitate comprehensive simulations of web search user behaviors. Our simulation framework can generate unique user profiles at scale, which subsequently leads to diverse search behaviors. To demonstrate the effectiveness of BASES, we conduct evaluation experiments based on two human benchmarks in both Chinese and English, demonstrating that BASES can effectively simulate large-scale human-like search behaviors. To further accommodate the research on web search, we develop WARRIORS, a new larg
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;REAR&#30340;&#26032;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#20013;&#26080;&#27861;&#20934;&#30830;&#35780;&#20272;&#26816;&#32034;&#25991;&#26723;&#30456;&#20851;&#24615;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#22686;&#24378;&#23545;&#26816;&#32034;&#25991;&#26723;&#30456;&#20851;&#24615;&#30340;&#33258;&#25105;&#24847;&#35782;&#65292;&#33021;&#22815;&#33258;&#36866;&#24212;&#22320;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#12290;</title><link>https://arxiv.org/abs/2402.17497</link><description>&lt;p&gt;
REAR&#65306;&#19968;&#31181;&#38754;&#21521;&#24320;&#25918;&#22495;&#38382;&#31572;&#30340;&#20851;&#27880;&#24230;&#24863;&#30693;&#26816;&#32034;&#22686;&#24378;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
REAR: A Relevance-Aware Retrieval-Augmented Framework for Open-Domain Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17497
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;REAR&#30340;&#26032;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#20013;&#26080;&#27861;&#20934;&#30830;&#35780;&#20272;&#26816;&#32034;&#25991;&#26723;&#30456;&#20851;&#24615;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#22686;&#24378;&#23545;&#26816;&#32034;&#25991;&#26723;&#30456;&#20851;&#24615;&#30340;&#33258;&#25105;&#24847;&#35782;&#65292;&#33021;&#22815;&#33258;&#36866;&#24212;&#22320;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32771;&#34385;&#21040;&#26377;&#38480;&#30340;&#20869;&#37096;&#21442;&#25968;&#21270;&#30693;&#35782;&#65292;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#34987;&#24191;&#27867;&#29992;&#20110;&#25193;&#23637;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#30693;&#35782;&#33539;&#22260;&#12290;&#23613;&#31649;&#22312;RAG&#30740;&#31350;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#21162;&#21147;&#65292;&#20294;&#22312;&#29616;&#26377;&#26041;&#27861;&#20013;&#65292;LLMs &#26080;&#27861;&#20934;&#30830;&#35780;&#20272;&#26816;&#32034;&#25991;&#26723;&#30340;&#30456;&#20851;&#24615;&#65292;&#22240;&#27492;&#24456;&#21487;&#33021;&#23548;&#33268;&#23545;&#22806;&#37096;&#30693;&#35782;&#65288;&#21363;&#26816;&#32034;&#25991;&#26723;&#65289;&#30340;&#35823;&#23548;&#29978;&#33267;&#38169;&#35823;&#21033;&#29992;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102; REAR&#65292;&#19968;&#31181;&#38754;&#21521;&#24320;&#25918;&#22495;&#38382;&#31572;&#65288;QA&#65289;&#30340;&#20851;&#27880;&#24230;&#24863;&#30693;&#26816;&#32034;&#22686;&#24378;&#26041;&#27861;&#12290;&#20316;&#20026;&#20851;&#38190;&#21160;&#26426;&#65292;&#25105;&#20204;&#26088;&#22312;&#22686;&#24378;LLMs&#23545;&#26469;&#28304;&#30456;&#20851;&#24615;&#30340;&#33258;&#25105;&#24847;&#35782;&#65292;&#20197;&#20415;&#22312;RAG&#31995;&#32479;&#20013;&#33258;&#36866;&#24212;&#22320;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;LLM&#30340;RAG&#31995;&#32479;&#26550;&#26500;&#65292;&#36890;&#36807;&#25972;&#21512;&#19968;&#20010;&#31934;&#30830;&#35780;&#20272;&#26816;&#32034;&#25991;&#26723;&#30456;&#20851;&#24615;&#30340;&#29305;&#21035;&#35774;&#35745;&#30340;&#25490;&#21517;&#22836;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#35757;&#32451;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17497v1 Announce Type: new  Abstract: Considering the limited internal parametric knowledge, retrieval-augmented generation (RAG) has been widely used to extend the knowledge scope of large language models (LLMs). Despite the extensive efforts on RAG research, in existing methods, LLMs cannot precisely assess the relevance of retrieved documents, thus likely leading to misleading or even incorrect utilization of external knowledge (i.e., retrieved documents). To address this issue, in this paper, we propose REAR, a RElevance-Aware Retrieval-augmented approach for open-domain question answering (QA). As the key motivation, we aim to enhance the self-awareness of source relevance for LLMs, so as to adaptively utilize external knowledge in RAG systems. Specially, we develop a new architecture for LLM based RAG system, by incorporating a specially designed rank head that precisely assesses the relevance of retrieved documents. Furthermore, we propose an improved training method 
&lt;/p&gt;</description></item><item><title>&#35813;&#35843;&#30740;&#32508;&#36848;&#20102;&#22312;&#31526;&#21495;&#38899;&#20048;&#29983;&#25104;&#21644;&#20449;&#24687;&#26816;&#32034;&#30740;&#31350;&#20013;&#24212;&#29992;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#31526;&#21495;&#38899;&#20048;&#34920;&#31034;&#30340;&#35774;&#35745;&#21644;&#22788;&#29702;&#12290;</title><link>https://arxiv.org/abs/2402.17467</link><description>&lt;p&gt;
&#29992;&#20110;&#31526;&#21495;&#38899;&#20048;&#29983;&#25104;&#21644;&#20449;&#24687;&#26816;&#32034;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#65306;&#19968;&#39033;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
Natural Language Processing Methods for Symbolic Music Generation and Information Retrieval: a Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17467
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35843;&#30740;&#32508;&#36848;&#20102;&#22312;&#31526;&#21495;&#38899;&#20048;&#29983;&#25104;&#21644;&#20449;&#24687;&#26816;&#32034;&#30740;&#31350;&#20013;&#24212;&#29992;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#31526;&#21495;&#38899;&#20048;&#34920;&#31034;&#30340;&#35774;&#35745;&#21644;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;Transformers&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#21462;&#24471;&#31361;&#30772;&#20197;&#26469;&#65292;&#35813;&#27169;&#22411;&#24050;&#22312;&#21508;&#20010;&#39046;&#22495;&#36827;&#34892;&#20102;&#22810;&#31181;&#25913;&#36827;&#12290; &#36825;&#19968;&#36235;&#21183;&#24050;&#32463;&#20256;&#25773;&#21040;&#38899;&#20048;&#20449;&#24687;&#26816;&#32034;&#65288;MIR&#65289;&#39046;&#22495;&#65292;&#21253;&#25324;&#22788;&#29702;&#38899;&#20048;&#25968;&#25454;&#30340;&#30740;&#31350;&#12290; &#20294;&#26159;&#65292;&#22312;MIR&#20013;&#65292;&#21033;&#29992;NLP&#24037;&#20855;&#22788;&#29702;&#31526;&#21495;&#38899;&#20048;&#25968;&#25454;&#30340;&#20570;&#27861;&#24182;&#19981;&#26032;&#39062;&#12290; &#38899;&#20048;&#32463;&#24120;&#34987;&#27604;&#20316;&#35821;&#35328;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#22810;&#20010;&#30456;&#20284;&#20043;&#22788;&#65292;&#21253;&#25324;&#25991;&#26412;&#21644;&#38899;&#20048;&#30340;&#24207;&#21015;&#21270;&#34920;&#31034;&#12290; &#36825;&#20123;&#31867;&#27604;&#36824;&#36890;&#36807;MIR&#21644;NLP&#20013;&#30340;&#31867;&#20284;&#20219;&#21153;&#24471;&#21040;&#20307;&#29616;&#12290; &#26412;&#25991;&#32508;&#36848;&#20102;&#24212;&#29992;&#20110;&#31526;&#21495;&#38899;&#20048;&#29983;&#25104;&#21644;&#20449;&#24687;&#26816;&#32034;&#30740;&#31350;&#30340;NLP&#26041;&#27861;&#65292;&#36981;&#24490;&#20004;&#20010;&#26041;&#38754;&#12290;&#25105;&#20204;&#39318;&#20808;&#27010;&#36848;&#20102;&#20174;&#33258;&#28982;&#35821;&#35328;&#24207;&#21015;&#34920;&#31034;&#20013;&#25913;&#32534;&#32780;&#26469;&#30340;&#31526;&#21495;&#38899;&#20048;&#34920;&#31034;&#12290; &#36890;&#36807;&#32771;&#34385;&#31526;&#21495;&#38899;&#20048;&#30340;&#29305;&#23450;&#24615;&#26469;&#35774;&#35745;&#36825;&#20123;&#34920;&#31034;&#12290; &#28982;&#21518;&#36825;&#20123;&#34920;&#31034;&#34987;&#27169;&#22411;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17467v1 Announce Type: cross  Abstract: Several adaptations of Transformers models have been developed in various domains since its breakthrough in Natural Language Processing (NLP). This trend has spread into the field of Music Information Retrieval (MIR), including studies processing music data. However, the practice of leveraging NLP tools for symbolic music data is not novel in MIR. Music has been frequently compared to language, as they share several similarities, including sequential representations of text and music. These analogies are also reflected through similar tasks in MIR and NLP. This survey reviews NLP methods applied to symbolic music generation and information retrieval studies following two axes. We first propose an overview of representations of symbolic music adapted from natural language sequential representations. Such representations are designed by considering the specificities of symbolic music. These representations are then processed by models. S
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#65292;&#38024;&#23545;&#39135;&#35889;&#25991;&#26412;&#24320;&#21457;&#20102;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#27169;&#22411;&#65292;&#36890;&#36807;&#31995;&#32479;&#30340;&#25968;&#25454;&#22788;&#29702;&#21644;&#20998;&#26512;&#65292;&#26500;&#24314;&#20102;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#26032;&#39135;&#35889;&#30340;&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2402.17447</link><description>&lt;p&gt;
&#39135;&#35889;&#30340;&#28145;&#24230;&#23398;&#20064;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Deep Learning Based Named Entity Recognition Models for Recipes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17447
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#65292;&#38024;&#23545;&#39135;&#35889;&#25991;&#26412;&#24320;&#21457;&#20102;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#27169;&#22411;&#65292;&#36890;&#36807;&#31995;&#32479;&#30340;&#25968;&#25454;&#22788;&#29702;&#21644;&#20998;&#26512;&#65292;&#26500;&#24314;&#20102;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#26032;&#39135;&#35889;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39135;&#29289;&#36890;&#36807;&#21508;&#31181;&#21162;&#21147;&#26041;&#24335;&#24433;&#21709;&#30528;&#25105;&#20204;&#30340;&#29983;&#27963;&#65292;&#21253;&#25324;&#21475;&#21619;&#12289;&#33829;&#20859;&#12289;&#20581;&#24247;&#21644;&#21487;&#25345;&#32493;&#24615;&#12290;&#39135;&#35889;&#26159;&#36890;&#36807;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20195;&#20195;&#30456;&#20256;&#30340;&#25991;&#21270;&#33014;&#22218;&#12290;&#33258;&#21160;&#35782;&#21035;&#21629;&#21517;&#23454;&#20307;&#30340;&#21327;&#35758;&#65292;&#21363;&#39135;&#35889;&#25991;&#26412;&#30340;&#22522;&#26412;&#32452;&#25104;&#37096;&#20998;&#65292;&#23545;&#20110;&#21508;&#31181;&#24212;&#29992;&#26469;&#35828;&#37117;&#20855;&#26377;&#24040;&#22823;&#20215;&#20540;&#65292;&#20174;&#20449;&#24687;&#25552;&#21462;&#21040;&#26032;&#39062;&#39135;&#35889;&#29983;&#25104;&#12290;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#26159;&#19968;&#31181;&#20174;&#24050;&#30693;&#26631;&#31614;&#30340;&#38750;&#32467;&#26500;&#21270;&#25110;&#21322;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#25552;&#21462;&#20449;&#24687;&#30340;&#25216;&#26415;&#12290;&#25105;&#20204;&#20174;&#25163;&#21160;&#27880;&#37322;&#30340;6,611&#20010;&#25104;&#20998;&#30701;&#35821;&#30340;&#25968;&#25454;&#24320;&#22987;&#65292;&#32047;&#31215;&#21019;&#24314;&#20102;26,445&#20010;&#30701;&#35821;&#30340;&#22686;&#24378;&#25968;&#25454;&#38598;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#28165;&#29702;&#21644;&#20998;&#26512;&#20102;&#26469;&#33258;RecipeDB&#30340;&#25104;&#20998;&#30701;&#35821;&#65292;&#36825;&#26159;&#40644;&#37329;&#26631;&#20934;&#30340;&#39135;&#35889;&#25968;&#25454;&#23384;&#20648;&#24211;&#65292;&#24182;&#20351;&#29992;Stanford NER&#36827;&#34892;&#20102;&#26631;&#27880;&#12290;&#22522;&#20110;&#20998;&#26512;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#32858;&#31867;&#30340;&#26041;&#27861;&#23545;88,526&#20010;&#30701;&#35821;&#30340;&#23376;&#38598;&#36827;&#34892;&#20102;&#21462;&#26679;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17447v1 Announce Type: cross  Abstract: Food touches our lives through various endeavors, including flavor, nourishment, health, and sustainability. Recipes are cultural capsules transmitted across generations via unstructured text. Automated protocols for recognizing named entities, the building blocks of recipe text, are of immense value for various applications ranging from information extraction to novel recipe generation. Named entity recognition is a technique for extracting information from unstructured or semi-structured data with known labels. Starting with manually-annotated data of 6,611 ingredient phrases, we created an augmented dataset of 26,445 phrases cumulatively. Simultaneously, we systematically cleaned and analyzed ingredient phrases from RecipeDB, the gold-standard recipe data repository, and annotated them using the Stanford NER. Based on the analysis, we sampled a subset of 88,526 phrases using a clustering-based approach while preserving the diversity
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#26694;&#26550; BiVRec&#65292;&#22312;&#25512;&#33616;&#20219;&#21153;&#20013;&#32852;&#21512;&#35757;&#32451; ID &#21644;&#22810;&#27169;&#24577;&#35270;&#22270;&#65292;&#21452;&#21521;&#22686;&#24378;&#25512;&#33616;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.17334</link><description>&lt;p&gt;
BiVRec: &#21452;&#21521;&#22522;&#20110;&#35270;&#22270;&#30340;&#22810;&#27169;&#24577;&#39034;&#24207;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
BiVRec: Bidirectional View-based Multimodal Sequential Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17334
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#26694;&#26550; BiVRec&#65292;&#22312;&#25512;&#33616;&#20219;&#21153;&#20013;&#32852;&#21512;&#35757;&#32451; ID &#21644;&#22810;&#27169;&#24577;&#35270;&#22270;&#65292;&#21452;&#21521;&#22686;&#24378;&#25512;&#33616;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#20449;&#24687;&#34701;&#20837;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#36817;&#26469;&#24341;&#36215;&#20102;&#30740;&#31350;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#22312;&#22810;&#27169;&#24577;&#39034;&#24207;&#25512;&#33616;&#27169;&#22411;&#30340;&#21021;&#26399;&#38454;&#27573;&#65292;&#20027;&#27969;&#33539;&#24335;&#26159;ID&#20027;&#23548;&#25512;&#33616;&#65292;&#21363;&#22810;&#27169;&#24577;&#20449;&#24687;&#20316;&#20026;&#36741;&#21161;&#20449;&#24687;&#36827;&#34892;&#34701;&#21512;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#22312;&#21487;&#36716;&#31227;&#24615;&#21644;&#20449;&#24687;&#20405;&#20837;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#65292;&#21478;&#19968;&#31181;&#33539;&#24335;&#20986;&#29616;&#20102;&#65292;&#21363;&#30452;&#25509;&#21033;&#29992;&#22810;&#27169;&#24577;&#29305;&#24449;&#36827;&#34892;&#25512;&#33616;&#65292;&#23454;&#29616;&#36328;&#25968;&#25454;&#38598;&#30340;&#25512;&#33616;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#23427;&#24573;&#30053;&#20102;&#29992;&#25143;ID&#20449;&#24687;&#65292;&#23548;&#33268;&#20449;&#24687;&#21033;&#29992;&#29575;&#20302;&#21644;&#35757;&#32451;&#25104;&#26412;&#39640;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#26694;&#26550;&#65292;BiVRec&#65292;&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;ID&#21644;&#22810;&#27169;&#24577;&#35270;&#22270;&#20013;&#30340;&#25512;&#33616;&#20219;&#21153;&#65292;&#21033;&#29992;&#23427;&#20204;&#20043;&#38388;&#30340;&#21327;&#21516;&#20851;&#31995;&#21452;&#21521;&#22686;&#24378;&#25512;&#33616;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#20449;&#24687;&#24322;&#26500;&#24615;&#38382;&#39064;&#65292;&#25105;&#20204;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17334v1 Announce Type: cross  Abstract: The integration of multimodal information into sequential recommender systems has attracted significant attention in recent research. In the initial stages of multimodal sequential recommendation models, the mainstream paradigm was ID-dominant recommendations, wherein multimodal information was fused as side information. However, due to their limitations in terms of transferability and information intrusion, another paradigm emerged, wherein multimodal features were employed directly for recommendation, enabling recommendation across datasets. Nonetheless, it overlooked user ID information, resulting in low information utilization and high training costs. To this end, we propose an innovative framework, BivRec, that jointly trains the recommendation tasks in both ID and multimodal views, leveraging their synergistic relationship to enhance recommendation performance bidirectionally. To tackle the information heterogeneity issue, we fir
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#29983;&#25104;&#24335;&#26381;&#35013;&#25512;&#33616;&#20219;&#21153;&#65288;GOR&#65289;&#65292;&#26088;&#22312;&#21512;&#25104;&#19968;&#32452;&#26102;&#23578;&#22270;&#29255;&#24182;&#32452;&#35013;&#25104;&#35270;&#35273;&#21644;&#35856;&#30340;&#12289;&#23450;&#21046;&#32473;&#20010;&#20154;&#29992;&#25143;&#30340;&#26381;&#35013;&#12290;</title><link>https://arxiv.org/abs/2402.17279</link><description>&lt;p&gt;
DiFashion: &#36808;&#21521;&#20010;&#24615;&#21270;&#26381;&#35013;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
DiFashion: Towards Personalized Outfit Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17279
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#29983;&#25104;&#24335;&#26381;&#35013;&#25512;&#33616;&#20219;&#21153;&#65288;GOR&#65289;&#65292;&#26088;&#22312;&#21512;&#25104;&#19968;&#32452;&#26102;&#23578;&#22270;&#29255;&#24182;&#32452;&#35013;&#25104;&#35270;&#35273;&#21644;&#35856;&#30340;&#12289;&#23450;&#21046;&#32473;&#20010;&#20154;&#29992;&#25143;&#30340;&#26381;&#35013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26381;&#35013;&#25512;&#33616;&#65288;OR&#65289;&#22312;&#26102;&#23578;&#39046;&#22495;&#30340;&#21457;&#23637;&#32463;&#21382;&#20102;&#20004;&#20010;&#19981;&#21516;&#38454;&#27573;&#65306;&#39044;&#23450;&#20041;&#30340;&#26381;&#35013;&#25512;&#33616;&#21644;&#20010;&#24615;&#21270;&#30340;&#26381;&#35013;&#32452;&#21512;&#12290;&#34429;&#28982;&#21462;&#24471;&#20102;&#36825;&#20123;&#36827;&#23637;&#65292;&#20294;&#20004;&#20010;&#38454;&#27573;&#37117;&#38754;&#20020;&#29616;&#26377;&#26102;&#23578;&#20135;&#21697;&#24102;&#26469;&#30340;&#38480;&#21046;&#65292;&#38459;&#30861;&#20102;&#23427;&#20204;&#28385;&#36275;&#29992;&#25143;&#22810;&#26679;&#21270;&#26102;&#23578;&#38656;&#27714;&#30340;&#26377;&#25928;&#24615;&#12290;AI&#29983;&#25104;&#20869;&#23481;&#30340;&#20986;&#29616;&#20026;OR&#20811;&#26381;&#36825;&#20123;&#32422;&#26463;&#38138;&#24179;&#20102;&#36947;&#36335;&#65292;&#23637;&#31034;&#20102;&#20010;&#24615;&#21270;&#26381;&#35013;&#29983;&#25104;&#30340;&#28508;&#21147;&#12290;&#20026;&#20102;&#36861;&#27714;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#39033;&#21517;&#20026;&#29983;&#25104;&#24335;&#26381;&#35013;&#25512;&#33616;&#65288;GOR&#65289;&#30340;&#21019;&#26032;&#20219;&#21153;&#65292;&#20854;&#30446;&#26631;&#26159;&#21512;&#25104;&#19968;&#32452;&#26102;&#23578;&#22270;&#29255;&#65292;&#24182;&#23558;&#23427;&#20204;&#32452;&#35013;&#25104;&#35270;&#35273;&#21644;&#35856;&#30340;&#12289;&#23450;&#21046;&#32473;&#20010;&#20154;&#29992;&#25143;&#30340;&#26381;&#35013;&#12290;GOR&#30340;&#20027;&#35201;&#30446;&#26631;&#38598;&#20013;&#22312;&#23454;&#29616;&#29983;&#25104;&#26381;&#35013;&#30340;&#39640;&#20445;&#30495;&#24230;&#12289;&#20860;&#23481;&#24615;&#21644;&#20010;&#24615;&#21270;&#12290;&#20026;&#23454;&#29616;&#36825;&#20123;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DiFashion&#65292;&#19968;&#20010;&#29983;&#25104;&#24335;&#26381;&#35013;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17279v1 Announce Type: new  Abstract: The evolution of Outfit Recommendation (OR) in the realm of fashion has progressed through two distinct phases: Pre-defined Outfit Recommendation and Personalized Outfit Composition. Despite these advancements, both phases face limitations imposed by existing fashion products, hindering their effectiveness in meeting users' diverse fashion needs. The emergence of AI-generated content has paved the way for OR to overcome these constraints, demonstrating the potential for personalized outfit generation.   In pursuit of this, we introduce an innovative task named Generative Outfit Recommendation (GOR), with the goal of synthesizing a set of fashion images and assembling them to form visually harmonious outfits customized to individual users. The primary objectives of GOR revolve around achieving high fidelity, compatibility, and personalization of the generated outfits. To accomplish these, we propose DiFashion, a generative outfit recommen
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;Prompt-Tuning&#36171;&#33021;&#30340;PromptMM&#22810;&#27169;&#24335;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#29992;&#20110;&#31616;&#21270;&#21644;&#22686;&#24378;&#25512;&#33616;&#31995;&#32479;&#65292;&#23454;&#29616;&#33258;&#36866;&#24212;&#30340;&#36136;&#37327;&#33976;&#39311;&#12290;</title><link>https://arxiv.org/abs/2402.17188</link><description>&lt;p&gt;
PromptMM&#65306;&#22810;&#27169;&#24335;&#30693;&#35782;&#33976;&#39311;&#29992;&#20110;&#22522;&#20110;Prompt-Tuning&#30340;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
PromptMM: Multi-Modal Knowledge Distillation for Recommendation with Prompt-Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17188
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;Prompt-Tuning&#36171;&#33021;&#30340;PromptMM&#22810;&#27169;&#24335;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#29992;&#20110;&#31616;&#21270;&#21644;&#22686;&#24378;&#25512;&#33616;&#31995;&#32479;&#65292;&#23454;&#29616;&#33258;&#36866;&#24212;&#30340;&#36136;&#37327;&#33976;&#39311;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#23186;&#20307;&#22312;&#32447;&#24179;&#21488;&#65288;&#20363;&#22914;&#20122;&#39532;&#36874;&#12289;TikTok&#65289;&#36890;&#36807;&#23558;&#22810;&#23186;&#20307;&#65288;&#20363;&#22914;&#35270;&#35273;&#12289;&#25991;&#26412;&#21644;&#22768;&#23398;&#65289;&#20869;&#23481;&#32435;&#20837;&#20854;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#20013;&#33719;&#30410;&#21290;&#27973;&#12290;&#36825;&#20123;&#27169;&#24577;&#25552;&#20379;&#30452;&#35266;&#35821;&#20041;&#65292;&#26377;&#21161;&#20110;&#36827;&#34892;&#27169;&#24577;&#24863;&#30693;&#30340;&#29992;&#25143;&#20559;&#22909;&#24314;&#27169;&#12290;&#28982;&#32780;&#65292;&#22810;&#27169;&#24335;&#25512;&#33616;&#22120;&#20013;&#23384;&#22312;&#20004;&#20010;&#20851;&#38190;&#25361;&#25112;&#23578;&#26410;&#35299;&#20915;&#65306;i&#65289;&#24341;&#20837;&#20855;&#26377;&#22823;&#37327;&#39069;&#22806;&#21442;&#25968;&#30340;&#22810;&#27169;&#24335;&#32534;&#30721;&#22120;&#20250;&#23548;&#33268;&#36807;&#25311;&#21512;&#65292;&#32771;&#34385;&#21040;&#25552;&#21462;&#22120;&#65288;&#20363;&#22914;ViT&#12289;BERT&#65289;&#25552;&#20379;&#30340;&#39640;&#32500;&#22810;&#27169;&#24335;&#29305;&#24449;&#12290;ii&#65289;&#36741;&#21161;&#20449;&#24687;&#19981;&#21487;&#36991;&#20813;&#22320;&#24341;&#20837;&#19981;&#20934;&#30830;&#24615;&#21644;&#20887;&#20313;&#65292;&#23548;&#33268;&#27169;&#24577;&#20132;&#20114;&#20381;&#36182;&#20559;&#31163;&#30495;&#23454;&#29992;&#25143;&#20559;&#22909;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;Prompt-Tuning&#36171;&#33021;&#12289;&#31616;&#21270;&#25512;&#33616;&#22120;&#30340;PromptMM&#65288;&#22810;&#27169;&#24335;&#30693;&#35782;&#33976;&#39311;&#65289;&#65292;&#23454;&#29616;&#33258;&#36866;&#24212;&#36136;&#37327;&#33976;&#39311;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17188v1 Announce Type: new  Abstract: Multimedia online platforms (e.g., Amazon, TikTok) have greatly benefited from the incorporation of multimedia (e.g., visual, textual, and acoustic) content into their personal recommender systems. These modalities provide intuitive semantics that facilitate modality-aware user preference modeling. However, two key challenges in multi-modal recommenders remain unresolved: i) The introduction of multi-modal encoders with a large number of additional parameters causes overfitting, given high-dimensional multi-modal features provided by extractors (e.g., ViT, BERT). ii) Side information inevitably introduces inaccuracies and redundancies, which skew the modality-interaction dependency from reflecting true user preference. To tackle these problems, we propose to simplify and empower recommenders through Multi-modal Knowledge Distillation (PromptMM) with the prompt-tuning that enables adaptive quality distillation. Specifically, PromptMM cond
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;HSTU&#26550;&#26500;&#65292;&#29992;&#20110;&#39640;&#22522;&#25968;&#12289;&#38750;&#24179;&#31283;&#27969;&#25512;&#33616;&#25968;&#25454;&#65292;&#24615;&#33021;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#39640;&#36798;65.8&#65285;&#30340;NDCG&#65292;&#24182;&#19988;&#27604;&#22522;&#20110;FlashAttention2&#30340;Transformer&#22312;8192&#38271;&#24230;&#24207;&#21015;&#19978;&#24555;5.3&#20493;&#21040;15.2&#20493;&#12290;</title><link>https://arxiv.org/abs/2402.17152</link><description>&lt;p&gt;
&#34892;&#21160;&#32988;&#36807;&#35328;&#36766;&#65306;&#29992;&#20110;&#29983;&#25104;&#25512;&#33616;&#30340;&#21315;&#20159;&#21442;&#25968;&#39034;&#24207;&#36716;&#23548;&#22120;
&lt;/p&gt;
&lt;p&gt;
Actions Speak Louder than Words: Trillion-Parameter Sequential Transducers for Generative Recommendations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17152
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;HSTU&#26550;&#26500;&#65292;&#29992;&#20110;&#39640;&#22522;&#25968;&#12289;&#38750;&#24179;&#31283;&#27969;&#25512;&#33616;&#25968;&#25454;&#65292;&#24615;&#33021;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#39640;&#36798;65.8&#65285;&#30340;NDCG&#65292;&#24182;&#19988;&#27604;&#22522;&#20110;FlashAttention2&#30340;Transformer&#22312;8192&#38271;&#24230;&#24207;&#21015;&#19978;&#24555;5.3&#20493;&#21040;15.2&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#25512;&#33616;&#31995;&#32479;&#30340;&#29305;&#28857;&#26159;&#20381;&#36182;&#20110;&#39640;&#22522;&#25968;&#12289;&#24322;&#26500;&#29305;&#24449;&#65292;&#24182;&#19988;&#38656;&#35201;&#27599;&#22825;&#22788;&#29702;&#25968;&#21313;&#20159;&#29992;&#25143;&#34892;&#20026;&#12290;&#23613;&#31649;&#22312;&#25104;&#21315;&#19978;&#19975;&#20010;&#29305;&#24449;&#19978;&#35757;&#32451;&#20102;&#22823;&#37327;&#25968;&#25454;&#65292;&#20294;&#22823;&#22810;&#25968;&#34892;&#19994;&#20013;&#30340;&#28145;&#24230;&#23398;&#20064;&#25512;&#33616;&#27169;&#22411;(DLRMs)&#22312;&#35745;&#31639;&#26041;&#38754;&#26080;&#27861;&#25193;&#23637;&#12290;&#21463;&#21040;&#22312;&#35821;&#35328;&#21644;&#35270;&#35273;&#39046;&#22495;&#21462;&#24471;&#25104;&#21151;&#30340;Transformer&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#22522;&#26412;&#35774;&#35745;&#36873;&#25321;&#12290;&#25105;&#20204;&#23558;&#25512;&#33616;&#38382;&#39064;&#37325;&#26032;&#26500;&#24314;&#20026;&#29983;&#25104;&#24314;&#27169;&#26694;&#26550;&#20013;&#30340;&#39034;&#24207;&#36716;&#23548;&#20219;&#21153;&#65288;&#8220;&#29983;&#25104;&#25512;&#33616;&#32773;&#8221;&#65289;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#39640;&#22522;&#25968;&#12289;&#38750;&#24179;&#31283;&#27969;&#25512;&#33616;&#25968;&#25454;&#35774;&#35745;&#30340;&#26032;&#26550;&#26500;HSTU&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17152v1 Announce Type: new  Abstract: Large-scale recommendation systems are characterized by their reliance on high cardinality, heterogeneous features and the need to handle tens of billions of user actions on a daily basis. Despite being trained on huge volume of data with thousands of features, most Deep Learning Recommendation Models (DLRMs) in industry fail to scale with compute.   Inspired by success achieved by Transformers in language and vision domains, we revisit fundamental design choices in recommendation systems. We reformulate recommendation problems as sequential transduction tasks within a generative modeling framework (``Generative Recommenders''), and propose a new architecture, HSTU, designed for high cardinality, non-stationary streaming recommendation data.   HSTU outperforms baselines over synthetic and public datasets by up to 65.8\% in NDCG, and is 5.3x to 15.2x faster than FlashAttention2-based Transformers on 8192 length sequences. HSTU-based Gener
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35843;&#26597;&#20840;&#38754;&#22238;&#39038;&#20102;&#22522;&#20110;&#36741;&#21161;&#20449;&#24687;&#39537;&#21160;&#30340;&#22522;&#20110;&#20250;&#35805;&#25512;&#33616;&#30740;&#31350;&#65292;&#25506;&#35752;&#20102;&#22810;&#26679;&#30340;&#36741;&#21161;&#20449;&#24687;&#22914;&#20309;&#22686;&#24378;&#25512;&#33616;&#31995;&#32479;&#24615;&#33021;&#65292;&#20998;&#26512;&#20102;&#26368;&#26032;&#21644;&#20195;&#34920;&#24615;&#30340;&#30740;&#31350;&#36827;&#23637;&#12290;</title><link>https://arxiv.org/abs/2402.17129</link><description>&lt;p&gt;
&#22522;&#20110;&#36741;&#21161;&#20449;&#24687;&#30340;&#22522;&#20110;&#20250;&#35805;&#25512;&#33616;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Side Information-Driven Session-based Recommendation: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17129
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#26597;&#20840;&#38754;&#22238;&#39038;&#20102;&#22522;&#20110;&#36741;&#21161;&#20449;&#24687;&#39537;&#21160;&#30340;&#22522;&#20110;&#20250;&#35805;&#25512;&#33616;&#30740;&#31350;&#65292;&#25506;&#35752;&#20102;&#22810;&#26679;&#30340;&#36741;&#21161;&#20449;&#24687;&#22914;&#20309;&#22686;&#24378;&#25512;&#33616;&#31995;&#32479;&#24615;&#33021;&#65292;&#20998;&#26512;&#20102;&#26368;&#26032;&#21644;&#20195;&#34920;&#24615;&#30340;&#30740;&#31350;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20250;&#35805;&#30340;&#25512;&#33616;&#65288;SBR&#65289;&#30001;&#20110;&#20854;&#22312;&#26377;&#38480;&#20132;&#20114;&#20013;&#39044;&#27979;&#21311;&#21517;&#29992;&#25143;&#24847;&#22270;&#30340;&#33021;&#21147;&#32780;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#26032;&#20852;&#30340;&#24037;&#20316;&#23558;&#21508;&#31181;&#36741;&#21161;&#20449;&#24687;&#32435;&#20837;&#20854;&#26041;&#27861;&#20013;&#65292;&#20197;&#25552;&#39640;&#20219;&#21153;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#20174;&#25968;&#25454;&#20013;&#24515;&#30340;&#35282;&#24230;&#23545;&#22522;&#20110;&#36741;&#21161;&#20449;&#24687;&#39537;&#21160;&#30340;&#22522;&#20110;&#20250;&#35805;&#25512;&#33616;&#36827;&#34892;&#20102;&#20840;&#38754;&#22238;&#39038;&#12290;&#25105;&#20204;&#39318;&#20808;&#38416;&#36848;&#20102;&#36825;&#19968;&#30740;&#31350;&#35838;&#39064;&#32972;&#21518;&#30340;&#21160;&#26426;&#21644;&#24517;&#35201;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35814;&#32454;&#25506;&#35752;&#20102;&#20016;&#23500;&#36741;&#21161;&#20449;&#24687;&#30340;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#36825;&#23545;&#25512;&#36827;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#33267;&#20851;&#37325;&#35201;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#36825;&#20123;&#19981;&#21516;&#31867;&#22411;&#30340;&#36741;&#21161;&#20449;&#24687;&#22914;&#20309;&#22686;&#24378;SBR&#65292;&#24182;&#24378;&#35843;&#23427;&#20204;&#30340;&#29305;&#28857;&#21644;&#23454;&#29992;&#24615;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#23545;&#30740;&#31350;&#36827;&#23637;&#36827;&#34892;&#20102;&#31995;&#32479;&#24615;&#22238;&#39038;&#65292;&#20998;&#26512;&#20102;&#36825;&#19968;&#20027;&#39064;&#20013;&#26368;&#36817;&#21644;&#20855;&#26377;&#20195;&#34920;&#24615;&#30340;&#21457;&#23637;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#23637;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17129v1 Announce Type: new  Abstract: The session-based recommendation (SBR) garners increasing attention due to its ability to predict anonymous user intents within limited interactions. Emerging efforts incorporate various kinds of side information into their methods for enhancing task performance. In this survey, we thoroughly review the side information-driven session-based recommendation from a data-centric perspective. Our survey commences with an illustration of the motivation and necessity behind this research topic. This is followed by a detailed exploration of various benchmarks rich in side information, pivotal for advancing research in this field. Moreover, we delve into how these diverse types of side information enhance SBR, underscoring their characteristics and utility. A systematic review of research progress is then presented, offering an analysis of the most recent and representative developments within this topic. Finally, we present the future prospects 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;&#32463;&#36807;&#24494;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#21521;&#37327;&#25968;&#25454;&#24211;&#38598;&#25104;&#65292;&#24341;&#20837;&#20102;LoRA&#21644;QLoRA&#26041;&#27861;&#65292;&#24182;&#30452;&#25509;&#23558;&#29992;&#25143;&#21453;&#39304;&#32435;&#20837;&#35757;&#32451;&#36807;&#31243;&#65292;&#21516;&#26102;&#24341;&#20837;&#37327;&#21270;&#24433;&#21709;&#24230;&#20316;&#20026;&#8220;Innovative AI Judge&#8221;&#26426;&#21046;&#65292;&#20197;&#25552;&#39640;&#31995;&#32479;&#30340;&#31934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.17081</link><description>&lt;p&gt;
&#36890;&#36807;&#37327;&#21270;&#24433;&#21709;&#24230;&#20316;&#20026;&#20154;&#24037;&#26234;&#33021;&#35780;&#22996;&#30340;&#31934;&#32454;&#35843;&#20248;&#22686;&#24378;&#29256;RAG&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
A Fine-tuning Enhanced RAG System with Quantized Influence Measure as AI Judge
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17081
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;&#32463;&#36807;&#24494;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#21521;&#37327;&#25968;&#25454;&#24211;&#38598;&#25104;&#65292;&#24341;&#20837;&#20102;LoRA&#21644;QLoRA&#26041;&#27861;&#65292;&#24182;&#30452;&#25509;&#23558;&#29992;&#25143;&#21453;&#39304;&#32435;&#20837;&#35757;&#32451;&#36807;&#31243;&#65292;&#21516;&#26102;&#24341;&#20837;&#37327;&#21270;&#24433;&#21709;&#24230;&#20316;&#20026;&#8220;Innovative AI Judge&#8221;&#26426;&#21046;&#65292;&#20197;&#25552;&#39640;&#31995;&#32479;&#30340;&#31934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#22686;&#24378;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#31995;&#32479;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#32463;&#36807;&#24494;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#21521;&#37327;&#25968;&#25454;&#24211;&#26080;&#32541;&#38598;&#25104;&#12290;&#36825;&#31181;&#38598;&#25104;&#21033;&#29992;&#20102;&#32467;&#26500;&#21270;&#25968;&#25454;&#26816;&#32034;&#30340;&#20248;&#21183;&#21644;&#20808;&#36827;LLMs&#25552;&#20379;&#30340;&#32454;&#33268;&#29702;&#35299;&#33021;&#21147;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;LoRA&#21644;QLoRA&#26041;&#27861;&#65292;&#23427;&#20204;&#22788;&#20110;&#27169;&#22411;&#32454;&#21270;&#30340;&#21069;&#27839;&#65292;&#36890;&#36807;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#21644;&#20869;&#23384;&#20248;&#21270;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30740;&#31350;&#30340;&#19968;&#20010;&#26032;&#29305;&#24615;&#26159;&#30452;&#25509;&#23558;&#29992;&#25143;&#21453;&#39304;&#32435;&#20837;&#35757;&#32451;&#36807;&#31243;&#65292;&#30830;&#20445;&#27169;&#22411;&#25345;&#32493;&#36866;&#24212;&#29992;&#25143;&#26399;&#26395;&#65292;&#20174;&#32780;&#25552;&#39640;&#20854;&#24615;&#33021;&#21644;&#36866;&#29992;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#37327;&#21270;&#24433;&#21709;&#24230;&#65288;QIM&#65289;&#20316;&#20026;&#19968;&#31181;&#21019;&#26032;&#30340;&#8220;AI&#35780;&#22996;&#8221;&#26426;&#21046;&#65292;&#22686;&#24378;&#32467;&#26524;&#36873;&#25321;&#30340;&#31934;&#24230;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#31995;&#32479;&#30340;&#20934;&#30830;&#24615;&#12290;&#20276;&#38543;&#30528;&#19968;&#20010;&#25191;&#34892;&#22270;&#21644;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17081v1 Announce Type: new  Abstract: This study presents an innovative enhancement to retrieval-augmented generation (RAG) systems by seamlessly integrating fine-tuned large language models (LLMs) with vector databases. This integration capitalizes on the combined strengths of structured data retrieval and the nuanced comprehension provided by advanced LLMs. Central to our approach are the LoRA and QLoRA methodologies, which stand at the forefront of model refinement through parameter-efficient fine-tuning and memory optimization. A novel feature of our research is the incorporation of user feedback directly into the training process, ensuring the model's continuous adaptation to user expectations and thus, improving its performance and applicability. Additionally, we introduce a Quantized Influence Measure (QIM) as an innovative "AI Judge" mechanism to enhance the precision of result selection, further refining the system's accuracy. Accompanied by an executive diagram and
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#29420;&#29305;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#30446;&#26631;&#65292;&#30740;&#31350;&#32773;&#35774;&#35745;&#20102;&#29992;&#20110;&#25903;&#25345;&#33521;&#35821;&#21644;&#20854;&#20182;&#30446;&#26631;&#35821;&#35328;&#30340;&#26368;&#20808;&#36827;&#30340;&#21452;&#35821;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#22312;STS&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#21516;&#26102;&#22312;&#30446;&#26631;&#35821;&#35328;&#29702;&#35299;&#21644;&#36328;&#35821;&#35328;&#35780;&#20272;&#20219;&#21153;&#20013;&#36229;&#36234;&#20102;&#29616;&#26377;&#22810;&#35821;&#35328;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.17016</link><description>&lt;p&gt;
&#22810;&#20219;&#21153;&#23545;&#27604;&#23398;&#20064;&#29992;&#20110;8192&#26631;&#35760;&#30340;&#21452;&#35821;&#25991;&#26412;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Multi-Task Contrastive Learning for 8192-Token Bilingual Text Embeddings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17016
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#29420;&#29305;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#30446;&#26631;&#65292;&#30740;&#31350;&#32773;&#35774;&#35745;&#20102;&#29992;&#20110;&#25903;&#25345;&#33521;&#35821;&#21644;&#20854;&#20182;&#30446;&#26631;&#35821;&#35328;&#30340;&#26368;&#20808;&#36827;&#30340;&#21452;&#35821;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#22312;STS&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#21516;&#26102;&#22312;&#30446;&#26631;&#35821;&#35328;&#29702;&#35299;&#21644;&#36328;&#35821;&#35328;&#35780;&#20272;&#20219;&#21153;&#20013;&#36229;&#36234;&#20102;&#29616;&#26377;&#22810;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#22871;&#26032;&#39062;&#30340;&#26368;&#20808;&#36827;&#30340;&#21452;&#35821;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#26088;&#22312;&#25903;&#25345;&#33521;&#35821;&#21644;&#21478;&#19968;&#31181;&#30446;&#26631;&#35821;&#35328;&#12290;&#36825;&#20123;&#27169;&#22411;&#33021;&#22815;&#22788;&#29702;&#38271;&#36798;8192&#20010;&#26631;&#35760;&#30340;&#25991;&#26412;&#36755;&#20837;&#65292;&#22240;&#27492;&#38750;&#24120;&#36866;&#29992;&#20110;&#19968;&#31995;&#21015;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65292;&#22914;&#25991;&#26412;&#26816;&#32034;&#12289;&#32858;&#31867;&#21644;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24230;&#65288;STS&#65289;&#35745;&#31639;&#12290;&#36890;&#36807;&#19987;&#27880;&#20110;&#21452;&#35821;&#27169;&#22411;&#24182;&#24341;&#20837;&#29420;&#29305;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#30446;&#26631;&#65292;&#25105;&#20204;&#26174;&#33879;&#25913;&#21892;&#20102;&#22312;STS&#20219;&#21153;&#19978;&#30340;&#27169;&#22411;&#34920;&#29616;&#65292;&#36229;&#36807;&#20102;&#29616;&#26377;&#22810;&#35821;&#35328;&#27169;&#22411;&#22312;&#30446;&#26631;&#35821;&#35328;&#29702;&#35299;&#21644;&#36328;&#35821;&#35328;&#35780;&#20272;&#20219;&#21153;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#21452;&#35821;&#27169;&#22411;&#26356;&#21152;&#39640;&#25928;&#65292;&#38656;&#35201;&#36739;&#23569;&#30340;&#21442;&#25968;&#21644;&#26356;&#23569;&#30340;&#20869;&#23384;&#65292;&#22240;&#20026;&#23427;&#20204;&#38656;&#35201;&#36739;&#23567;&#30340;&#35789;&#27719;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25193;&#23637;&#20102;&#22823;&#35268;&#27169;&#25991;&#26412;&#23884;&#20837;&#22522;&#20934;&#65288;MTEB&#65289;&#65292;&#21253;&#25324;&#24503;&#35821;&#21644;&#35199;&#29677;&#29273;&#35821;&#23884;&#20837;&#30340;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17016v1 Announce Type: cross  Abstract: We introduce a novel suite of state-of-the-art bilingual text embedding models that are designed to support English and another target language. These models are capable of processing lengthy text inputs with up to 8192 tokens, making them highly versatile for a range of natural language processing tasks such as text retrieval, clustering, and semantic textual similarity (STS) calculations.   By focusing on bilingual models and introducing a unique multi-task learning objective, we have significantly improved the model performance on STS tasks, which outperforms the capabilities of existing multilingual models in both target language understanding and cross-lingual evaluation tasks. Moreover, our bilingual models are more efficient, requiring fewer parameters and less memory due to their smaller vocabulary needs. Furthermore, we have expanded the Massive Text Embedding Benchmark (MTEB) to include benchmarks for German and Spanish embed
&lt;/p&gt;</description></item><item><title>&#38271;&#23545;&#35805;&#25688;&#35201;&#30340;&#30740;&#31350;&#24378;&#35843;&#20102;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#20026;&#26377;&#25928;&#27807;&#36890;&#21019;&#36896;&#36830;&#36143;&#21644;&#19978;&#19979;&#25991;&#20016;&#23500;&#25688;&#35201;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.16986</link><description>&lt;p&gt;
&#38271;&#23545;&#35805;&#25688;&#35201;: &#19968;&#39033;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Long Dialog Summarization: An Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16986
&lt;/p&gt;
&lt;p&gt;
&#38271;&#23545;&#35805;&#25688;&#35201;&#30340;&#30740;&#31350;&#24378;&#35843;&#20102;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#20026;&#26377;&#25928;&#27807;&#36890;&#21019;&#36896;&#36830;&#36143;&#21644;&#19978;&#19979;&#25991;&#20016;&#23500;&#25688;&#35201;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Dialog summarization&#22312;&#36328;&#36234;&#19981;&#21516;&#39046;&#22495;&#30340;&#22823;&#35268;&#27169;&#23545;&#35805;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#36825;&#39033;&#20219;&#21153;&#22312;&#25429;&#25417;&#22810;&#36718;&#38271;&#23545;&#35805;&#30340;&#20851;&#38190;&#28857;&#12289;&#19978;&#19979;&#25991;&#21644;&#32454;&#24494;&#24046;&#21035;&#26041;&#38754;&#38754;&#20020;&#30528;&#29420;&#29305;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#24378;&#35843;&#20102;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#20026;&#26377;&#25928;&#27807;&#36890;&#21019;&#36896;&#36830;&#36143;&#21644;&#19978;&#19979;&#25991;&#20016;&#23500;&#25688;&#35201;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#19981;&#21516;&#39046;&#22495;&#38271;&#23545;&#35805;&#25688;&#35201;&#30340;&#24403;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#22522;&#20110;&#22522;&#20934;&#25351;&#26631;&#30340;&#35780;&#20272;&#26174;&#31034;&#19968;&#20010;&#21333;&#19968;&#27169;&#22411;&#22312;&#21508;&#31181;&#22330;&#26223;&#19979;&#30340;&#34920;&#29616;&#24182;&#19981;&#29702;&#24819;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16986v1 Announce Type: new  Abstract: Dialog summarization has become increasingly important in managing and comprehending large-scale conversations across various domains. This task presents unique challenges in capturing the key points, context, and nuances of multi-turn long conversations for summarization. It is worth noting that the summarization techniques may vary based on specific requirements such as in a shopping-chatbot scenario, the dialog summary helps to learn user preferences, whereas in the case of a customer call center, the summary may involve the problem attributes that a user specified, and the final resolution provided. This work emphasizes the significance of creating coherent and contextually rich summaries for effective communication in various applications. We explore current state-of-the-art approaches for long dialog summarization in different domains and benchmark metrics based evaluations show that one single model does not perform well across va
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Cobweb4V&#30340;&#26032;&#39062;&#35270;&#35273;&#20998;&#31867;&#26041;&#27861;&#65292;&#21033;&#29992;&#20154;&#31867;&#31867;&#20284;&#23398;&#20064;&#31995;&#32479;&#65292;&#36991;&#20813;&#20102;&#28798;&#38590;&#24615;&#36951;&#24536;&#25928;&#24212;&#65292;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;&#38656;&#35201;&#26356;&#23569;&#30340;&#25968;&#25454;&#26469;&#23454;&#29616;&#26377;&#25928;&#23398;&#20064;&#25104;&#26524;&#65292;&#24182;&#20445;&#25345;&#31283;&#23450;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.16933</link><description>&lt;p&gt;
&#20351;&#29992;&#20154;&#31867;&#27010;&#24565;&#24418;&#25104;&#36991;&#20813;&#35270;&#35273;&#20998;&#31867;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
Avoiding Catastrophic Forgetting in Visual Classification Using Human Concept Formation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16933
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Cobweb4V&#30340;&#26032;&#39062;&#35270;&#35273;&#20998;&#31867;&#26041;&#27861;&#65292;&#21033;&#29992;&#20154;&#31867;&#31867;&#20284;&#23398;&#20064;&#31995;&#32479;&#65292;&#36991;&#20813;&#20102;&#28798;&#38590;&#24615;&#36951;&#24536;&#25928;&#24212;&#65292;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;&#38656;&#35201;&#26356;&#23569;&#30340;&#25968;&#25454;&#26469;&#23454;&#29616;&#26377;&#25928;&#23398;&#20064;&#25104;&#26524;&#65292;&#24182;&#20445;&#25345;&#31283;&#23450;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#29305;&#21035;&#26159;&#22312;&#35270;&#35273;&#20219;&#21153;&#20013;&#65292;&#28982;&#32780;&#65292;&#24403;&#25353;&#39034;&#24207;&#23398;&#20064;&#26032;&#20219;&#21153;&#26102;&#65292;&#23427;&#20204;&#32463;&#24120;&#38754;&#20020;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Cobweb4V&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#35270;&#35273;&#20998;&#31867;&#26041;&#27861;&#65292;&#23427;&#22522;&#20110;Cobweb&#65292;&#36825;&#26159;&#19968;&#31181;&#20154;&#31867;&#31867;&#20284;&#30340;&#23398;&#20064;&#31995;&#32479;&#65292;&#21463;&#21040;&#20154;&#31867;&#38543;&#26102;&#38388;&#36880;&#28176;&#23398;&#20064;&#26032;&#27010;&#24565;&#30340;&#21551;&#21457;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#23637;&#31034;&#20102;Cobweb4V&#22312;&#23398;&#20064;&#35270;&#35273;&#27010;&#24565;&#26041;&#38754;&#30340;&#29087;&#32451;&#31243;&#24230;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#38656;&#35201;&#26356;&#23569;&#30340;&#25968;&#25454;&#26469;&#23454;&#29616;&#26377;&#25928;&#30340;&#23398;&#20064;&#25104;&#26524;&#65292;&#38543;&#26102;&#38388;&#20445;&#25345;&#31283;&#23450;&#30340;&#24615;&#33021;&#65292;&#24182;&#23454;&#29616;&#20102;&#20196;&#20154;&#31216;&#36190;&#30340;&#28176;&#36817;&#34892;&#20026;&#65292;&#36991;&#20813;&#20102;&#28798;&#38590;&#24615;&#36951;&#24536;&#25928;&#24212;&#12290;&#36825;&#20123;&#29305;&#24449;&#19982;&#20154;&#31867;&#35748;&#30693;&#20013;&#30340;&#23398;&#20064;&#31574;&#30053;&#19968;&#33268;&#65292;&#23558;Cobweb4V&#23450;&#20301;&#20026;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#30340;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16933v1 Announce Type: cross  Abstract: Deep neural networks have excelled in machine learning, particularly in vision tasks, however, they often suffer from catastrophic forgetting when learning new tasks sequentially. In this work, we propose Cobweb4V, a novel visual classification approach that builds on Cobweb, a human like learning system that is inspired by the way humans incrementally learn new concepts over time. In this research, we conduct a comprehensive evaluation, showcasing the proficiency of Cobweb4V in learning visual concepts, requiring less data to achieve effective learning outcomes compared to traditional methods, maintaining stable performance over time, and achieving commendable asymptotic behavior, without catastrophic forgetting effects. These characteristics align with learning strategies in human cognition, positioning Cobweb4V as a promising alternative to neural network approaches.
&lt;/p&gt;</description></item><item><title>&#21521;&#37327;&#25968;&#25454;&#24211;&#21644;&#23884;&#20837;&#27169;&#22411;&#30340;&#24212;&#29992;&#20026;&#25991;&#26412;&#20998;&#31867;&#22120;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#26041;&#24335;&#26469;&#34920;&#36798;&#25968;&#25454;&#27169;&#24335;&#65292;&#29305;&#21035;&#26159;&#22312;&#21307;&#30103;&#39046;&#22495;&#20013;&#24320;&#22987;&#26377;&#30528;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.16886</link><description>&lt;p&gt;
&#20351;&#29992;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#21644;&#21521;&#37327;&#25968;&#25454;&#24211;&#20316;&#20026;&#25991;&#26412;&#20998;&#31867;&#22120;&#30340;&#30740;&#31350;&#65292;&#20197;&#21307;&#30103;&#25968;&#25454;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Using text embedding models and vector databases as text classifiers with the example of medical data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16886
&lt;/p&gt;
&lt;p&gt;
&#21521;&#37327;&#25968;&#25454;&#24211;&#21644;&#23884;&#20837;&#27169;&#22411;&#30340;&#24212;&#29992;&#20026;&#25991;&#26412;&#20998;&#31867;&#22120;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#26041;&#24335;&#26469;&#34920;&#36798;&#25968;&#25454;&#27169;&#24335;&#65292;&#29305;&#21035;&#26159;&#22312;&#21307;&#30103;&#39046;&#22495;&#20013;&#24320;&#22987;&#26377;&#30528;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#26159;&#20196;&#20154;&#20852;&#22859;&#30340;&#65292;&#24182;&#24050;&#22312;&#35768;&#22810;&#39046;&#22495;&#25214;&#21040;&#24212;&#29992;&#65292;&#20294;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#21307;&#23398;&#39046;&#22495;&#30340;&#26631;&#20934;&#35201;&#27714;&#38750;&#24120;&#39640;&#12290;&#19982;LLMs&#37197;&#21512;&#20351;&#29992;&#65292;&#21521;&#37327;&#23884;&#20837;&#27169;&#22411;&#21644;&#21521;&#37327;&#25968;&#25454;&#24211;&#25552;&#20379;&#20102;&#19968;&#31181;&#24378;&#22823;&#30340;&#26041;&#24335;&#26469;&#34920;&#36798;&#21508;&#31181;&#25968;&#25454;&#27169;&#24335;&#65292;&#36825;&#20123;&#25968;&#25454;&#27169;&#24335;&#23481;&#26131;&#34987;&#20856;&#22411;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25152;&#29702;&#35299;&#12290;&#38500;&#20102;&#26041;&#20415;&#22320;&#21521;&#36825;&#20123;&#21521;&#37327;&#25968;&#25454;&#24211;&#28155;&#21152;&#20449;&#24687;&#12289;&#30693;&#35782;&#21644;&#25968;&#25454;&#22806;&#65292;&#23427;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#20196;&#20154;&#20449;&#26381;&#30340;&#29702;&#30001;&#65292;&#21363;&#23558;&#20854;&#24212;&#29992;&#20110;&#36890;&#24120;&#30001;&#20154;&#31867;&#23436;&#25104;&#30340;&#26816;&#32034;&#20449;&#24687;&#20219;&#21153;&#30340;&#21508;&#31181;&#39046;&#22495;&#12290;Google&#30340;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#20102;&#19968;&#20010;&#28165;&#26224;&#30340;&#26367;&#20195;&#27169;&#22411;Med-PaLM&#65292;&#19987;&#38376;&#26088;&#22312;&#19982;&#20020;&#24202;&#21307;&#24072;&#30340;&#21307;&#23398;&#30693;&#35782;&#27700;&#24179;&#21305;&#37197;&#12290;&#22312;&#35757;&#32451;&#20998;&#31867;&#22120;&#21644;&#24320;&#21457;&#27169;&#22411;&#26102;&#65292;&#20445;&#25345;&#20107;&#23454;&#21644;&#20943;&#23569;&#20559;&#35265;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#21521;&#37327;&#25968;&#25454;&#24211;&#21644;&#23884;&#20837;&#27169;&#22411;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16886v1 Announce Type: cross  Abstract: The advent of Large Language Models (LLMs) is promising and has found application in numerous fields, but as it often is with the medical field, the bar is typically quite high [5]. In tandem with LLMs, vector embedding models and vector databases provide a robust way of expressing numerous modes of data that are easily digestible by typical machine learning models. Along with the ease of adding information, knowledge, and data to these vector databases, they provide a compelling reason to apply them in numerous fields where the task of retrieving information is typically done by humans. Researchers at Google have developed a clear alternative model, Med-PaLM [6] specifically designed to match a clinician's level of accuracy when it comes to medical knowledge. When training classifiers, and developing models, it is imperative to maintain factuality and reduce bias [4]. Here, we explore the use of vector databases and embedding models a
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21033;&#29992;&#29983;&#25104;&#33021;&#21147;&#26469;&#21512;&#25104;&#20551;&#35774;&#32451;&#20064;&#65292;&#20197;&#24357;&#21512;&#23398;&#20064;&#32773;&#38656;&#27714;&#19982;&#32451;&#20064;&#20869;&#23481;&#20043;&#38388;&#30340;&#35821;&#20041;&#40511;&#27807;&#65292;&#25552;&#39640;&#20010;&#24615;&#21270;&#35821;&#35328;&#23398;&#20064;&#32451;&#20064;&#26816;&#32034;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.16877</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#30340;&#20010;&#24615;&#21270;&#35821;&#35328;&#23398;&#20064;&#32451;&#20064;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Large Language Model Augmented Exercise Retrieval for Personalized Language Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16877
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21033;&#29992;&#29983;&#25104;&#33021;&#21147;&#26469;&#21512;&#25104;&#20551;&#35774;&#32451;&#20064;&#65292;&#20197;&#24357;&#21512;&#23398;&#20064;&#32773;&#38656;&#27714;&#19982;&#32451;&#20064;&#20869;&#23481;&#20043;&#38388;&#30340;&#35821;&#20041;&#40511;&#27807;&#65292;&#25552;&#39640;&#20010;&#24615;&#21270;&#35821;&#35328;&#23398;&#20064;&#32451;&#20064;&#26816;&#32034;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#32447;&#35821;&#35328;&#23398;&#20064;&#29615;&#22659;&#20013;&#30340;&#38646;&#26679;&#26412;&#32451;&#20064;&#26816;&#32034;&#38382;&#39064;&#65292;&#20197;&#36171;&#20104;&#23398;&#20064;&#32773;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#26126;&#30830;&#35831;&#27714;&#20010;&#24615;&#21270;&#32451;&#20064;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#25910;&#38598;&#33258;&#35821;&#35328;&#23398;&#20064;&#32773;&#30340;&#30495;&#23454;&#25968;&#25454;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#30690;&#37327;&#30456;&#20284;&#24615;&#26041;&#27861;&#24456;&#38590;&#25429;&#25417;&#32451;&#20064;&#20869;&#23481;&#19982;&#23398;&#20064;&#32773;&#29992;&#20110;&#34920;&#36798;&#20182;&#20204;&#24819;&#35201;&#23398;&#20064;&#20869;&#23481;&#30340;&#35821;&#35328;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#26469;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#36890;&#36807;&#22522;&#20110;&#23398;&#20064;&#32773;&#36755;&#20837;&#21512;&#25104;&#20551;&#35774;&#32451;&#20064;&#65292;&#28982;&#21518;&#29992;&#20110;&#25628;&#32034;&#30456;&#20851;&#32451;&#20064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;mHyER&#20811;&#26381;&#20102;&#19977;&#20010;&#25361;&#25112;&#65306;&#65288;1&#65289;&#32570;&#20047;&#29992;&#20110;&#35757;&#32451;&#30340;&#30456;&#20851;&#24615;&#26631;&#31614;&#65292;&#65288;2&#65289;&#21463;&#38480;&#30340;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16877v1 Announce Type: cross  Abstract: We study the problem of zero-shot exercise retrieval in the context of online language learning, to give learners the ability to explicitly request personalized exercises via natural language. Using real-world data collected from language learners, we observe that vector similarity approaches poorly capture the relationship between exercise content and the language that learners use to express what they want to learn. This semantic gap between queries and content dramatically reduces the effectiveness of general-purpose retrieval models pretrained on large scale information retrieval datasets like MS MARCO. We leverage the generative capabilities of large language models to bridge the gap by synthesizing hypothetical exercises based on the learner's input, which are then used to search for relevant exercises. Our approach, which we call mHyER, overcomes three challenges: (1) lack of relevance labels for training, (2) unrestricted learn
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#20219;&#21153;&#65306;&#23398;&#26415;&#22242;&#38431;&#25104;&#21592;&#25512;&#33616;&#27169;&#22411;&#65292;&#36890;&#36807;&#32467;&#21512;&#26597;&#35810;&#21644;&#35770;&#25991;&#19978;&#19979;&#25991;&#19982;&#22270;&#25299;&#25169;&#32467;&#26500;&#65292;&#24418;&#25104;&#36866;&#29992;&#20110;&#29305;&#23450;&#30740;&#31350;&#20852;&#36259;&#21644;&#20219;&#21153;&#30340;&#26032;&#22270;&#65288;CQBG-R&#65289;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.16876</link><description>&lt;p&gt;
&#39640;&#32423;&#23398;&#26415;&#22242;&#38431;&#25104;&#21592;&#25512;&#33616;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Advanced Academic Team Worker Recommendation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16876
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#20219;&#21153;&#65306;&#23398;&#26415;&#22242;&#38431;&#25104;&#21592;&#25512;&#33616;&#27169;&#22411;&#65292;&#36890;&#36807;&#32467;&#21512;&#26597;&#35810;&#21644;&#35770;&#25991;&#19978;&#19979;&#25991;&#19982;&#22270;&#25299;&#25169;&#32467;&#26500;&#65292;&#24418;&#25104;&#36866;&#29992;&#20110;&#29305;&#23450;&#30740;&#31350;&#20852;&#36259;&#21644;&#20219;&#21153;&#30340;&#26032;&#22270;&#65288;CQBG-R&#65289;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#20316;&#20249;&#20276;&#25512;&#33616;&#26159;&#23398;&#26415;&#39046;&#22495;&#20013;&#30340;&#37325;&#35201;&#20219;&#21153;&#12290;&#29616;&#26377;&#26041;&#27861;&#22823;&#22810;&#20551;&#35774;&#25512;&#33616;&#31995;&#32479;&#20165;&#38656;&#20026;&#20219;&#21153;&#25512;&#33616;&#29305;&#23450;&#30340;&#30740;&#31350;&#20154;&#21592;&#12290;&#28982;&#32780;&#65292;&#23398;&#26415;&#25104;&#21151;&#21487;&#33021;&#24402;&#21151;&#20110;&#25972;&#20010;&#23398;&#26415;&#22242;&#38431;&#30340;&#39640;&#25928;&#21512;&#20316;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#65306;&#23398;&#26415;&#22242;&#38431;&#25104;&#21592;&#25512;&#33616;&#12290;&#26681;&#25454;&#32473;&#23450;&#30340;&#36523;&#20221;&#65288;&#23398;&#29983;&#65292;&#21161;&#29702;&#25945;&#25480;&#25110;&#20027;&#25945;&#25480;&#65289;&#12289;&#30740;&#31350;&#20852;&#36259;&#21644;&#29305;&#23450;&#20219;&#21153;&#65292;&#25105;&#20204;&#21487;&#20197;&#25512;&#33616;&#19968;&#20010;&#30001;&#65288;&#20027;&#25945;&#25480;&#65292;&#21161;&#29702;&#25945;&#25480;&#65292;&#23398;&#29983;&#65289;&#32452;&#25104;&#30340;&#23398;&#26415;&#22242;&#38431;&#12290;&#20026;&#20102;&#23436;&#25104;&#36825;&#20010;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CQBG-R&#65288;&#24341;&#29992;-&#26597;&#35810;&#28151;&#21512;&#22270;-&#25490;&#21517;&#65289;&#30340;&#27169;&#22411;&#12290;&#20854;&#20851;&#38190;&#24605;&#24819;&#26159;&#23558;&#26597;&#35810;&#21644;&#35770;&#25991;&#30340;&#19978;&#19979;&#25991;&#19982;&#22270;&#25299;&#25169;&#32467;&#26500;&#32467;&#21512;&#36215;&#26469;&#24418;&#25104;&#19968;&#20010;&#26032;&#30340;&#22270;&#65288;CQBG&#65289;&#65292;&#36825;&#20010;&#22270;&#21487;&#20197;&#38024;&#23545;&#36825;&#27425;&#30340;&#30740;&#31350;&#20852;&#36259;&#21644;&#29305;&#23450;&#30740;&#31350;&#20219;&#21153;&#12290;&#23454;&#39564;&#32467;&#26524;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16876v1 Announce Type: cross  Abstract: Collaborator recommendation is an important task in academic domain. Most of the existing approaches have the assumption that the recommendation system only need to recommend a specific researcher for the task. However, academic successes can be owed to productive collaboration of a whole academic team. In this work, we propose a new task: academic team worker recommendation: with a given status: student, assistant professor or prime professor, research interests and specific task, we can recommend an academic team formed as (prime professor, assistant professor, student). For this task, we propose a model CQBG-R(Citation-Query Blended Graph-Ranking). The key ideas is to combine the context of the query and the papers with the graph topology to form a new graph(CQBG), which can target at the research interests and the specific research task for this time. The experiment results show the effectiveness of the proposed method.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#24433;&#21709;&#26597;&#35810;&#24615;&#33021;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#22240;&#32032;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22810;&#21464;&#37327;&#31163;&#32676;&#20540;&#30340;&#26041;&#27861;&#65292;&#26377;&#25928;&#35782;&#21035;&#20986;QPP&#34920;&#29616;&#19981;&#20339;&#30340;&#26597;&#35810;&#12290;</title><link>https://arxiv.org/abs/2402.16875</link><description>&lt;p&gt;
&#25105;&#20204;&#33021;&#22815;&#39044;&#27979;QPP&#21527;&#65311;&#22522;&#20110;&#22810;&#21464;&#37327;&#31163;&#32676;&#20540;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Can we predict QPP? An approach based on multivariate outliers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16875
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#24433;&#21709;&#26597;&#35810;&#24615;&#33021;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#22240;&#32032;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22810;&#21464;&#37327;&#31163;&#32676;&#20540;&#30340;&#26041;&#27861;&#65292;&#26377;&#25928;&#35782;&#21035;&#20986;QPP&#34920;&#29616;&#19981;&#20339;&#30340;&#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16875v1  &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;  &#25688;&#35201;&#65306;&#26597;&#35810;&#24615;&#33021;&#39044;&#27979;&#65288;QPP&#65289;&#26088;&#22312;&#39044;&#27979;&#25628;&#32034;&#24341;&#25806;&#22312;&#19968;&#31995;&#21015;&#26597;&#35810;&#21644;&#25991;&#26723;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#23613;&#31649;&#26368;&#20808;&#36827;&#30340;&#39044;&#27979;&#22120;&#25552;&#20379;&#20102;&#19968;&#23450;&#27700;&#24179;&#30340;&#31934;&#24230;&#65292;&#20294;&#23427;&#20204;&#30340;&#20934;&#30830;&#24615;&#24182;&#19981;&#23436;&#32654;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#24847;&#35782;&#21040;&#20102;QPP&#22266;&#26377;&#30340;&#25361;&#25112;&#65292;&#20294;&#24448;&#24448;&#32570;&#20047;&#24443;&#24213;&#30340;&#23450;&#24615;&#20998;&#26512;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#30740;&#31350;&#24433;&#21709;&#26597;&#35810;&#24615;&#33021;&#20934;&#30830;&#24615;&#21487;&#39044;&#27979;&#24615;&#30340;&#22240;&#32032;&#26469;&#28145;&#20837;&#25506;&#35752;QPP&#12290;&#25105;&#20204;&#25552;&#20986;&#24037;&#20316;&#20551;&#35774;&#65292;&#21363;&#34429;&#28982;&#26377;&#20123;&#26597;&#35810;&#24456;&#23481;&#26131;&#39044;&#27979;&#65292;&#20294;&#20854;&#20182;&#19968;&#20123;&#21017;&#38754;&#20020;&#26174;&#33879;&#25361;&#25112;&#12290;&#36890;&#36807;&#20851;&#27880;&#31163;&#32676;&#20540;&#65292;&#25105;&#20204;&#26088;&#22312;&#35782;&#21035;&#37027;&#20123;&#29305;&#21035;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26597;&#35810;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#37319;&#29992;&#22810;&#21464;&#37327;&#31163;&#32676;&#20540;&#26816;&#27979;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#35782;&#21035;QPP&#34920;&#29616;&#19981;&#20339;&#30340;&#26597;&#35810;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#65292;&#20135;&#29983;&#20102;&#36739;&#19981;&#21487;&#38752;&#30340;&#39044;&#27979;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#35777;&#25454;&#34920;&#26126;&#25490;&#38500;&#25481;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16875v1 Announce Type: new  Abstract: Query performance prediction (QPP) aims to forecast the effectiveness of a search engine across a range of queries and documents. While state-of-the-art predictors offer a certain level of precision, their accuracy is not flawless. Prior research has recognized the challenges inherent in QPP but often lacks a thorough qualitative analysis. In this paper, we delve into QPP by examining the factors that influence the predictability of query performance accuracy. We propose the working hypothesis that while some queries are readily predictable, others present significant challenges. By focusing on outliers, we aim to identify the queries that are particularly challenging to predict. To this end, we employ multivariate outlier detection method. Our results demonstrate the effectiveness of this approach in identifying queries on which QPP do not perform well, yielding less reliable predictions. Moreover, we provide evidence that excluding the
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#25216;&#26415;&#35299;&#20915;&#20102;&#35821;&#35328;&#29983;&#25104;&#20013;&#8220;&#24187;&#35273;&#8221;&#38382;&#39064;&#65292;&#24182;&#20511;&#21161;&#26597;&#35810;&#20248;&#21270;&#36807;&#31243;&#23558;&#29992;&#25143;&#26597;&#35810;&#19982;&#39640;&#32423;&#35821;&#35328;&#27169;&#22411;&#36830;&#25509;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.16874</link><description>&lt;p&gt;
&#36890;&#36807;&#22686;&#24378;&#26597;&#35810;&#25552;&#21319;&#35821;&#35328;&#29983;&#25104;&#30340;&#26816;&#32034;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Enhancing Retrieval Processes for Language Generation with Augmented Queries
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16874
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#25216;&#26415;&#35299;&#20915;&#20102;&#35821;&#35328;&#29983;&#25104;&#20013;&#8220;&#24187;&#35273;&#8221;&#38382;&#39064;&#65292;&#24182;&#20511;&#21161;&#26597;&#35810;&#20248;&#21270;&#36807;&#31243;&#23558;&#29992;&#25143;&#26597;&#35810;&#19982;&#39640;&#32423;&#35821;&#35328;&#27169;&#22411;&#36830;&#25509;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26234;&#33021;&#25216;&#26415;&#26085;&#26032;&#26376;&#24322;&#30340;&#19990;&#30028;&#20013;&#65292;&#30001;&#20110;&#20808;&#36827;&#35821;&#35328;&#27169;&#22411;&#30340;&#23835;&#36215;&#65292;&#25628;&#32034;&#25991;&#26723;&#21464;&#24471;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#36825;&#20123;&#27169;&#22411;&#26377;&#26102;&#20250;&#38754;&#20020;&#22256;&#38590;&#65292;&#27604;&#22914;&#25552;&#20379;&#19981;&#20934;&#30830;&#30340;&#20449;&#24687;&#65292;&#36890;&#24120;&#34987;&#31216;&#20026;&#8220;&#24187;&#35273;&#8221;&#12290;&#26412;&#30740;&#31350;&#33268;&#21147;&#20110;&#36890;&#36807;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#25216;&#26415;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#35813;&#25216;&#26415;&#25351;&#23548;&#27169;&#22411;&#22522;&#20110;&#30495;&#23454;&#20107;&#23454;&#25552;&#20379;&#20934;&#30830;&#31572;&#22797;&#12290;&#20026;&#20102;&#35299;&#20915;&#21487;&#20280;&#32553;&#24615;&#38382;&#39064;&#65292;&#30740;&#31350;&#25506;&#35752;&#20102;&#23558;&#29992;&#25143;&#26597;&#35810;&#19982;&#35832;&#22914;BERT&#21644;Orca2&#31561;&#22797;&#26434;&#35821;&#35328;&#27169;&#22411;&#36830;&#25509;&#36215;&#26469;&#30340;&#21019;&#26032;&#26597;&#35810;&#20248;&#21270;&#36807;&#31243;&#12290;&#30740;&#31350;&#23637;&#24320;&#22312;&#19977;&#31181;&#24773;&#22659;&#20013;&#65306;&#39318;&#20808;&#65292;&#27809;&#26377;RAG&#65292;&#20854;&#27425;&#65292;&#27809;&#26377;&#39069;&#22806;&#24110;&#21161;&#65292;&#26368;&#21518;&#65292;&#21152;&#20837;&#39069;&#22806;&#24110;&#21161;&#12290;&#36873;&#25321;&#32039;&#20945;&#32780;&#39640;&#25928;&#30340;Orca2 7B&#27169;&#22411;&#23637;&#31034;&#20986;&#23545;&#35745;&#31639;&#36164;&#28304;&#30340;&#26234;&#33021;&#20351;&#29992;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#21021;&#22987;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#26377;&#20102;&#26174;&#33879;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16874v1 Announce Type: cross  Abstract: In the rapidly changing world of smart technology, searching for documents has become more challenging due to the rise of advanced language models. These models sometimes face difficulties, like providing inaccurate information, commonly known as "hallucination." This research focuses on addressing this issue through Retrieval-Augmented Generation (RAG), a technique that guides models to give accurate responses based on real facts. To overcome scalability issues, the study explores connecting user queries with sophisticated language models such as BERT and Orca2, using an innovative query optimization process. The study unfolds in three scenarios: first, without RAG, second, without additional assistance, and finally, with extra help. Choosing the compact yet efficient Orca2 7B model demonstrates a smart use of computing resources. The empirical results indicate a significant improvement in the initial language model's performance unde
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;NFT1000&#30340;&#35270;&#35273;&#25991;&#26412;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;756&#19975;&#20010;&#22270;&#20687;&#25991;&#26412;&#23545;&#65292;&#26469;&#33258;&#38144;&#37327;&#26368;&#39640;&#30340;1000&#20010;PFP NFT&#25910;&#34255;&#21697;&#65292;&#29992;&#20110;&#35299;&#20915;NFT&#26816;&#32034;&#20013;&#30340;&#20934;&#30830;&#21644;&#39640;&#25928;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.16872</link><description>&lt;p&gt;
NFT1000&#65306;&#29992;&#20110;&#38750;&#21516;&#36136;&#21270;&#20195;&#24065;&#26816;&#32034;&#30340;&#35270;&#35273;&#25991;&#26412;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
NFT1000: A Visual Text Dataset For Non-Fungible Token Retrieval
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16872
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;NFT1000&#30340;&#35270;&#35273;&#25991;&#26412;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;756&#19975;&#20010;&#22270;&#20687;&#25991;&#26412;&#23545;&#65292;&#26469;&#33258;&#38144;&#37327;&#26368;&#39640;&#30340;1000&#20010;PFP NFT&#25910;&#34255;&#21697;&#65292;&#29992;&#20110;&#35299;&#20915;NFT&#26816;&#32034;&#20013;&#30340;&#20934;&#30830;&#21644;&#39640;&#25928;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#8220;&#20803;&#23431;&#23449;&#8221;&#21644;&#8220;Web3.0&#8221;&#30340;&#20852;&#36215;&#65292;NFT&#65288;&#38750;&#21516;&#36136;&#21270;&#20195;&#24065;&#65289;&#20316;&#20026;&#19968;&#31181;&#20851;&#38190;&#25968;&#23383;&#36164;&#20135;&#23853;&#38706;&#22836;&#35282;&#65292;&#24341;&#36215;&#20102;&#26497;&#22823;&#20851;&#27880;&#12290;&#21040;2023&#24180;11&#26376;&#24213;&#65292;&#22312;&#21508;&#31181;&#21306;&#22359;&#38142;&#24179;&#21488;&#19978;&#38136;&#36896;&#20102;&#36229;&#36807;14&#20159;&#20010;NFT&#20195;&#24065;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#23450;&#20301;&#19968;&#20010;&#20196;&#20154;&#28385;&#24847;&#30340;NFT&#20195;&#24065;&#65292;&#25628;&#32034;&#24191;&#27867;&#30340;NFT&#25968;&#25454;&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#12290;&#22312;&#21306;&#22495;&#21644;&#35821;&#20041;&#26041;&#38754;&#65292;&#19981;&#21516;NFT&#20195;&#24065;&#20043;&#38388;&#23384;&#22312;&#30456;&#24403;&#39640;&#30340;&#30456;&#20284;&#24230;&#65292;&#36825;&#22686;&#21152;&#20102;NFT&#26816;&#32034;&#30340;&#25361;&#25112;&#12290;&#22312;&#22823;&#35268;&#27169;&#12289;&#39640;&#24230;&#30456;&#20284;&#30340;NFT&#25968;&#25454;&#20013;&#23454;&#29616;&#20934;&#30830;&#21644;&#39640;&#25928;&#30340;&#26816;&#32034;&#23545;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#37117;&#26159;&#19968;&#39033;&#33392;&#24040;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;NFT Top1000&#35270;&#35273;&#25991;&#26412;&#25968;&#25454;&#38598;&#8221;&#65288;&#20197;&#19979;&#31616;&#31216;NFT1000&#65289;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;756&#19975;&#20010;&#22270;&#20687;&#25991;&#26412;&#23545;&#65292;&#25968;&#25454;&#26469;&#33258;&#20197;&#38144;&#21806;&#37327;&#20026;&#26631;&#20934;&#30340;&#21069;1000&#20010;&#26368;&#33879;&#21517;&#30340;PFP NFT&#25910;&#34255;&#21697;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16872v1 Announce Type: new  Abstract: With the rise of 'Metaverse' and 'Web3.0', NFT ( Non-Fungible Token ) has emerged as a kind of pivotal digital asset, garnering significant attention. By the end of November 2023, more than 1.4 billion NFT tokens have been minted across various blockchain platforms. To effectively locate a satisfactory NFT token, conducting searches within the extensive array of NFT data is essential. The challenge in NFT retrieval is heightened due to the high degree of similarity among different NFT tokens, in terms of regional and semantic aspects. Achieving accurate and efficient retrieval within the large-scale, highly similar NFT data presents a formidable challenge for both the academic and industrial communities. In this paper, we will introduce a dataset named 'NFT Top1000 Visual Text Dataset'(henceforth, NFT1000), containing 7.56 million image-text pairs, and being collected from 1000 most famous PFP NFT collections by sales volume on the Ether
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#30340;&#20132;&#20114;&#24335;&#28779;&#26143;&#22270;&#20687;&#20869;&#23481;&#25628;&#32034;&#26041;&#27861;&#65292;&#21487;&#20197;&#24110;&#21161;&#29992;&#25143;&#29702;&#35299;&#21644;&#39564;&#35777;&#20998;&#31867;&#22120;&#20351;&#29992;&#30340;&#35777;&#25454;&#65292;&#24182;&#26367;&#25442;&#29616;&#26377;&#30340;&#19981;&#21487;&#35299;&#37322;&#22270;&#20687;&#25628;&#32034;&#31995;&#32479;&#12290;</title><link>https://arxiv.org/abs/2402.16860</link><description>&lt;p&gt;
&#22522;&#20110;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#30340;&#20132;&#20114;&#24335;&#28779;&#26143;&#22270;&#20687;&#20869;&#23481;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Interactive Mars Image Content-Based Search with Interpretable Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16860
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#30340;&#20132;&#20114;&#24335;&#28779;&#26143;&#22270;&#20687;&#20869;&#23481;&#25628;&#32034;&#26041;&#27861;&#65292;&#21487;&#20197;&#24110;&#21161;&#29992;&#25143;&#29702;&#35299;&#21644;&#39564;&#35777;&#20998;&#31867;&#22120;&#20351;&#29992;&#30340;&#35777;&#25454;&#65292;&#24182;&#26367;&#25442;&#29616;&#26377;&#30340;&#19981;&#21487;&#35299;&#37322;&#22270;&#20687;&#25628;&#32034;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
NASA&#34892;&#26143;&#25968;&#25454;&#31995;&#32479;(PDS)&#25176;&#31649;&#30528;&#25968;&#30334;&#19975;&#24352;&#19981;&#21516;&#20219;&#21153;&#26399;&#38388;&#25910;&#38598;&#30340;&#34892;&#26143;&#12289;&#21355;&#26143;&#21644;&#20854;&#20182;&#22825;&#20307;&#30340;&#22270;&#20687;&#12290;&#25968;&#25454;&#30340;&#19981;&#26029;&#25193;&#23637;&#21644;&#29992;&#25143;&#21442;&#19982;&#30340;&#38656;&#27714;&#20419;&#20351;&#25105;&#20204;&#24314;&#31435;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#20869;&#23481;&#20998;&#31867;&#31995;&#32479;&#65292;&#20197;&#25903;&#25345;&#31185;&#23398;&#21457;&#29616;&#21644;&#20010;&#20154;&#22909;&#22855;&#24515;&#12290;&#26412;&#25991;&#21033;&#29992;&#22522;&#20110;&#21407;&#22411;&#30340;&#26550;&#26500;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#29702;&#35299;&#24182;&#39564;&#35777;&#28779;&#26143;&#31185;&#23398;&#23454;&#39564;&#23460;(MSL)&#22909;&#22855;&#21495;&#20219;&#21153;&#22270;&#20687;&#19978;&#35757;&#32451;&#30340;&#20998;&#31867;&#22120;&#25152;&#29992;&#30340;&#35777;&#25454;&#12290;&#38500;&#20102;&#25552;&#20379;&#35299;&#37322;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#22522;&#20110;&#20869;&#23481;&#30340;&#20998;&#31867;&#22120;&#25152;&#20351;&#29992;&#30340;&#35777;&#25454;&#30340;&#22810;&#26679;&#24615;&#21644;&#27491;&#30830;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#30340;&#24037;&#20316;&#23558;&#37096;&#32626;&#22312;PDS&#22270;&#20687;&#22270;&#38598;&#19978;&#65292;&#21462;&#20195;&#20854;&#19981;&#21487;&#35299;&#37322;&#30340;&#23545;&#24212;&#29289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16860v1 Announce Type: cross  Abstract: The NASA Planetary Data System (PDS) hosts millions of images of planets, moons, and other bodies collected throughout many missions. The ever-expanding nature of data and user engagement demands an interpretable content classification system to support scientific discovery and individual curiosity. In this paper, we leverage a prototype-based architecture to enable users to understand and validate the evidence used by a classifier trained on images from the Mars Science Laboratory (MSL) Curiosity rover mission. In addition to providing explanations, we investigate the diversity and correctness of evidence used by the content-based classifier. The work presented in this paper will be deployed on the PDS Image Atlas, replacing its non- interpretable counterpart.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#22810;&#25968;&#25237;&#31080;&#30340;&#26041;&#24335;&#65292;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32593;&#32476;&#23433;&#20840;&#20027;&#39064;&#20998;&#31867;&#24037;&#20855;&#65292;&#30456;&#27604;&#20110;21&#20010;&#21333;&#29420;&#27169;&#22411;&#65292;&#35813;&#24037;&#20855;&#22312;&#26816;&#27979;&#32593;&#32476;&#23433;&#20840;&#30456;&#20851;&#25991;&#26412;&#26102;&#34920;&#29616;&#20986;&#36739;&#20302;&#30340;&#20551;&#38451;&#24615;&#21644;&#20551;&#38452;&#24615;&#29575;&#65292;&#24182;&#19988;&#33021;&#22815;&#22788;&#29702;&#25968;&#21313;&#19975;&#20221;&#25991;&#26723;&#12290;</title><link>https://arxiv.org/abs/2109.02473</link><description>&lt;p&gt;
&#19968;&#31181;&#24378;&#22823;&#30340;&#32593;&#32476;&#23433;&#20840;&#20027;&#39064;&#20998;&#31867;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
A Robust Cybersecurity Topic Classification Tool
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2109.02473
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#22810;&#25968;&#25237;&#31080;&#30340;&#26041;&#24335;&#65292;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32593;&#32476;&#23433;&#20840;&#20027;&#39064;&#20998;&#31867;&#24037;&#20855;&#65292;&#30456;&#27604;&#20110;21&#20010;&#21333;&#29420;&#27169;&#22411;&#65292;&#35813;&#24037;&#20855;&#22312;&#26816;&#27979;&#32593;&#32476;&#23433;&#20840;&#30456;&#20851;&#25991;&#26412;&#26102;&#34920;&#29616;&#20986;&#36739;&#20302;&#30340;&#20551;&#38451;&#24615;&#21644;&#20551;&#38452;&#24615;&#29575;&#65292;&#24182;&#19988;&#33021;&#22815;&#22788;&#29702;&#25968;&#21313;&#19975;&#20221;&#25991;&#26723;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#26469;&#33258;&#19977;&#20010;&#20114;&#32852;&#32593;&#25991;&#26412;&#20449;&#24687;&#26469;&#28304;&#65288;Reddit&#65292;Stackexchange&#65292;Arxiv&#65289;&#30340;&#29992;&#25143;&#23450;&#20041;&#30340;&#26631;&#31614;&#65292;&#35757;&#32451;&#20102;21&#31181;&#19981;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#26816;&#27979;&#33258;&#28982;&#25991;&#26412;&#20013;&#30340;&#32593;&#32476;&#23433;&#20840;&#35752;&#35770;&#30340;&#20027;&#39064;&#20998;&#31867;&#20219;&#21153;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#27599;&#20010;&#27169;&#22411;&#22312;&#20132;&#21449;&#39564;&#35777;&#23454;&#39564;&#20013;&#30340;&#20551;&#38451;&#24615;&#21644;&#20551;&#38452;&#24615;&#29575;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32593;&#32476;&#23433;&#20840;&#20027;&#39064;&#20998;&#31867;&#65288;CTC&#65289;&#24037;&#20855;&#65292;&#35813;&#24037;&#20855;&#23558;21&#20010;&#35757;&#32451;&#22909;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#22810;&#25968;&#25237;&#31080;&#20316;&#20026;&#26816;&#27979;&#32593;&#32476;&#23433;&#20840;&#30456;&#20851;&#25991;&#26412;&#30340;&#20915;&#31574;&#26426;&#21046;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;CTC&#24037;&#20855;&#30340;&#22810;&#25968;&#25237;&#31080;&#26426;&#21046;&#24179;&#22343;&#25552;&#20379;&#20102;&#27604;21&#20010;&#21333;&#29420;&#27169;&#22411;&#26356;&#20302;&#30340;&#20551;&#38452;&#24615;&#21644;&#20551;&#38451;&#24615;&#29575;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;CTC&#24037;&#20855;&#21487;&#25193;&#23637;&#21040;&#25968;&#21313;&#19975;&#20221;&#25991;&#26723;&#65292;&#32780;&#20854;&#22681;&#38047;&#26102;&#38388;&#22823;&#32422;&#20026;&#20960;&#23567;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2109.02473v4 Announce Type: replace-cross  Abstract: In this research, we use user defined labels from three internet text sources (Reddit, Stackexchange, Arxiv) to train 21 different machine learning models for the topic classification task of detecting cybersecurity discussions in natural text. We analyze the false positive and false negative rates of each of the 21 model's in a cross validation experiment. Then we present a Cybersecurity Topic Classification (CTC) tool, which takes the majority vote of the 21 trained machine learning models as the decision mechanism for detecting cybersecurity related text. We also show that the majority vote mechanism of the CTC tool provides lower false negative and false positive rates on average than any of the 21 individual models. We show that the CTC tool is scalable to the hundreds of thousands of documents with a wall clock time on the order of hours.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;11&#31181;&#25193;&#23637;&#25216;&#26415;&#12289;12&#20010;&#19981;&#21516;&#20998;&#24067;&#21464;&#21270;&#30340;&#25968;&#25454;&#38598;&#21644;24&#20010;&#26816;&#32034;&#27169;&#22411;&#30340;&#20840;&#38754;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#26597;&#35810;&#25110;&#25991;&#26723;&#25193;&#23637;&#30340;&#25928;&#26524;&#19982;&#26816;&#32034;&#22120;&#24615;&#33021;&#30456;&#20851;&#65292;&#23545;&#20110;&#24369;&#27169;&#22411;&#26469;&#35828;&#25193;&#23637;&#25552;&#39640;&#20102;&#20998;&#25968;&#65292;&#20294;&#23545;&#20110;&#24378;&#27169;&#22411;&#26469;&#35828;&#25193;&#23637;&#36890;&#24120;&#20250;&#25439;&#23475;&#20998;&#25968;&#12290;</title><link>http://arxiv.org/abs/2309.08541</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;&#26597;&#35810;&#21644;&#25991;&#26723;&#25193;&#23637;&#20309;&#26102;&#22833;&#36133;&#65311;&#26041;&#27861;&#12289;&#26816;&#32034;&#22120;&#21644;&#25968;&#25454;&#38598;&#30340;&#20840;&#38754;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
When do Generative Query and Document Expansions Fail? A Comprehensive Study Across Methods, Retrievers, and Datasets. (arXiv:2309.08541v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08541
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;11&#31181;&#25193;&#23637;&#25216;&#26415;&#12289;12&#20010;&#19981;&#21516;&#20998;&#24067;&#21464;&#21270;&#30340;&#25968;&#25454;&#38598;&#21644;24&#20010;&#26816;&#32034;&#27169;&#22411;&#30340;&#20840;&#38754;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#26597;&#35810;&#25110;&#25991;&#26723;&#25193;&#23637;&#30340;&#25928;&#26524;&#19982;&#26816;&#32034;&#22120;&#24615;&#33021;&#30456;&#20851;&#65292;&#23545;&#20110;&#24369;&#27169;&#22411;&#26469;&#35828;&#25193;&#23637;&#25552;&#39640;&#20102;&#20998;&#25968;&#65292;&#20294;&#23545;&#20110;&#24378;&#27169;&#22411;&#26469;&#35828;&#25193;&#23637;&#36890;&#24120;&#20250;&#25439;&#23475;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#36827;&#34892;&#26597;&#35810;&#25110;&#25991;&#26723;&#25193;&#23637;&#21487;&#20197;&#25913;&#21892;&#20449;&#24687;&#26816;&#32034;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#36825;&#20123;&#25216;&#26415;&#26159;&#21542;&#26222;&#36941;&#26377;&#30410;&#65292;&#36824;&#26159;&#20165;&#22312;&#29305;&#23450;&#35774;&#32622;&#19979;&#26377;&#25928;&#65292;&#20363;&#22914;&#23545;&#20110;&#29305;&#23450;&#30340;&#26816;&#32034;&#27169;&#22411;&#12289;&#25968;&#25454;&#38598;&#39046;&#22495;&#25110;&#26597;&#35810;&#31867;&#22411;&#12290;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#31532;&#19968;&#27425;&#23545;&#22522;&#20110;LM&#30340;&#25193;&#23637;&#30340;&#20840;&#38754;&#20998;&#26512;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#26816;&#32034;&#22120;&#24615;&#33021;&#19982;&#25193;&#23637;&#30340;&#22686;&#30410;&#20043;&#38388;&#23384;&#22312;&#24378;&#28872;&#30340;&#36127;&#30456;&#20851;&#20851;&#31995;&#65306;&#25193;&#23637;&#25913;&#21892;&#20102;&#36739;&#24369;&#27169;&#22411;&#30340;&#20998;&#25968;&#65292;&#20294;&#36890;&#24120;&#20250;&#25439;&#23475;&#36739;&#24378;&#27169;&#22411;&#30340;&#20998;&#25968;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#19968;&#36235;&#21183;&#22312;11&#31181;&#25193;&#23637;&#25216;&#26415;&#12289;12&#20010;&#20855;&#26377;&#19981;&#21516;&#20998;&#24067;&#21464;&#21270;&#30340;&#25968;&#25454;&#38598;&#21644;24&#20010;&#26816;&#32034;&#27169;&#22411;&#30340;&#19968;&#32452;&#23454;&#39564;&#20013;&#25104;&#31435;&#12290;&#36890;&#36807;&#23450;&#24615;&#38169;&#35823;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20551;&#35774;&#65292;&#21363;&#23613;&#31649;&#25193;&#23637;&#25552;&#20379;&#20102;&#39069;&#22806;&#30340;&#20449;&#24687;&#65288;&#21487;&#33021;&#25913;&#21892;&#20102;&#21484;&#22238;&#29575;&#65289;&#65292;&#20294;&#23427;&#20204;&#20063;&#22686;&#21152;&#20102;&#22122;&#22768;&#65292;&#20351;&#24471;&#24456;&#38590;&#21306;&#20998;&#20986;&#39030;&#32423;&#30456;&#20851;&#25991;&#26723;&#65288;&#20174;&#32780;&#24341;&#20837;&#20102;&#38169;&#35823;&#30340;&#27491;&#20363;&#65289;
&lt;/p&gt;
&lt;p&gt;
Using large language models (LMs) for query or document expansion can improve generalization in information retrieval. However, it is unknown whether these techniques are universally beneficial or only effective in specific settings, such as for particular retrieval models, dataset domains, or query types. To answer this, we conduct the first comprehensive analysis of LM-based expansion. We find that there exists a strong negative correlation between retriever performance and gains from expansion: expansion improves scores for weaker models, but generally harms stronger models. We show this trend holds across a set of eleven expansion techniques, twelve datasets with diverse distribution shifts, and twenty-four retrieval models. Through qualitative error analysis, we hypothesize that although expansions provide extra information (potentially improving recall), they add additional noise that makes it difficult to discern between the top relevant documents (thus introducing false positiv
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#21542;&#23450;&#22312;&#31070;&#32463;&#20449;&#24687;&#26816;&#32034;&#20013;&#30340;&#24433;&#21709;&#65292;&#26500;&#24314;&#20102;&#22522;&#20934;&#27169;&#22411;&#65292;&#32467;&#26524;&#34920;&#26126;&#24403;&#21069;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#22823;&#22810;&#25968;&#37117;&#27809;&#26377;&#32771;&#34385;&#21542;&#23450;&#65292;&#32780;&#20132;&#21449;&#32534;&#30721;&#22120;&#26159;&#30446;&#21069;&#34920;&#29616;&#26368;&#22909;&#30340;&#26550;&#26500;&#12290;</title><link>http://arxiv.org/abs/2305.07614</link><description>&lt;p&gt;
NevIR: &#31070;&#32463;&#20449;&#24687;&#26816;&#32034;&#20013;&#30340;&#21542;&#23450;
&lt;/p&gt;
&lt;p&gt;
NevIR: Negation in Neural Information Retrieval. (arXiv:2305.07614v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07614
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#21542;&#23450;&#22312;&#31070;&#32463;&#20449;&#24687;&#26816;&#32034;&#20013;&#30340;&#24433;&#21709;&#65292;&#26500;&#24314;&#20102;&#22522;&#20934;&#27169;&#22411;&#65292;&#32467;&#26524;&#34920;&#26126;&#24403;&#21069;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#22823;&#22810;&#25968;&#37117;&#27809;&#26377;&#32771;&#34385;&#21542;&#23450;&#65292;&#32780;&#20132;&#21449;&#32534;&#30721;&#22120;&#26159;&#30446;&#21069;&#34920;&#29616;&#26368;&#22909;&#30340;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21542;&#23450;&#26159;&#19968;&#31181;&#24120;&#35265;&#32780;&#26085;&#24120;&#21270;&#30340;&#29616;&#35937;&#65292;&#20063;&#19968;&#30452;&#26159;&#35821;&#35328;&#27169;&#22411;&#30340;&#19968;&#20010;&#24369;&#28857;&#12290;&#34429;&#28982;&#20449;&#24687;&#26816;&#32034;&#39046;&#22495;&#37319;&#29992;&#20102;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#29616;&#20195;&#21270;&#26550;&#26500;&#30340;&#20027;&#24178;&#65292;&#20294;&#20960;&#20046;&#27809;&#26377;&#30740;&#31350;&#28145;&#20837;&#20102;&#35299;&#21542;&#23450;&#23545;&#31070;&#32463;&#20449;&#24687;&#26816;&#32034;&#30340;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#22522;&#20934;&#26469;&#30740;&#31350;&#36825;&#20010;&#20027;&#39064;&#65306;&#35201;&#27714;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#23545;&#20165;&#20165;&#22240;&#20026;&#26159;&#21542;&#23450;&#32780;&#19981;&#21516;&#30340;&#20004;&#20010;&#25991;&#26723;&#36827;&#34892;&#25490;&#21517;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#32467;&#26524;&#26681;&#25454;&#19981;&#21516;&#30340;&#20449;&#24687;&#26816;&#32034;&#26550;&#26500;&#32780;&#26377;&#24456;&#22823;&#24046;&#24322;&#65306;&#20132;&#21449;&#32534;&#30721;&#22120;&#34920;&#29616;&#26368;&#22909;&#65292;&#21518;&#26399;&#20132;&#20114;&#27169;&#22411;&#27425;&#20043;&#65292;&#32780;&#21452;&#32534;&#30721;&#22120;&#21644;&#31232;&#30095;&#31070;&#32463;&#26550;&#26500;&#25490;&#21517;&#26368;&#21518;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22823;&#22810;&#25968;&#24403;&#21069;&#30340;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#37117;&#27809;&#26377;&#32771;&#34385;&#21542;&#23450;&#65292;&#34920;&#29616;&#19982;&#38543;&#26426;&#25490;&#21517;&#30456;&#20284;&#25110;&#26356;&#24046;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#23613;&#31649;&#22312;&#19968;&#20010;&#21253;&#21547;&#21542;&#23450;&#23545;&#29031;&#25991;&#26723;&#30340;&#25968;&#25454;&#38598;&#19978;&#32487;&#32493;&#24494;&#35843;&#26126;&#26174;&#30340;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#65288;&#27169;&#22411;&#22823;&#23567;&#20063;&#26159;&#22914;&#27492;&#65289;&#65292;&#20294;&#26159;&#26426;&#22120;&#21644;&#20154;&#20043;&#38388;&#20173;&#23384;&#22312;&#24456;&#22823;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
Negation is a common everyday phenomena and has been a consistent area of weakness for language models (LMs). Although the Information Retrieval (IR) community has adopted LMs as the backbone of modern IR architectures, there has been little to no research in understanding how negation impacts neural IR. We therefore construct a straightforward benchmark on this theme: asking IR models to rank two documents that differ only by negation. We show that the results vary widely according to the type of IR architecture: cross-encoders perform best, followed by late-interaction models, and in last place are bi-encoder and sparse neural architectures. We find that most current information retrieval models do not consider negation, performing similarly or worse than randomly ranking. We show that although the obvious approach of continued fine-tuning on a dataset of contrastive documents containing negations increases performance (as does model size), there is still a large gap between machine 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26597;&#35810;&#25193;&#20805;&#26469;&#25628;&#32034;&#20887;&#20313;&#20449;&#24687;&#12289;&#24182;&#36890;&#36807;&#26032;&#39062;&#30340;&#32622;&#20449;&#24230;&#26041;&#27861;&#23558;&#20854;&#38598;&#25104;&#21040;&#27169;&#22411;&#20013;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#38450;&#24481;&#24320;&#25918;&#22495;&#38382;&#31572;&#31995;&#32479;&#20013;&#30340;&#27745;&#26579;&#25915;&#20987;&#65292;&#31934;&#30830;&#21305;&#37197;&#29575;&#21487;&#25552;&#39640;&#36817;20%&#12290;</title><link>http://arxiv.org/abs/2212.10002</link><description>&lt;p&gt;
&#22312;&#24320;&#25918;&#22495;&#38382;&#31572;&#20013;&#38450;&#24481;&#35823;&#23548;&#24615;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Defending Against Misinformation Attacks in Open-Domain Question Answering. (arXiv:2212.10002v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10002
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26597;&#35810;&#25193;&#20805;&#26469;&#25628;&#32034;&#20887;&#20313;&#20449;&#24687;&#12289;&#24182;&#36890;&#36807;&#26032;&#39062;&#30340;&#32622;&#20449;&#24230;&#26041;&#27861;&#23558;&#20854;&#38598;&#25104;&#21040;&#27169;&#22411;&#20013;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#38450;&#24481;&#24320;&#25918;&#22495;&#38382;&#31572;&#31995;&#32479;&#20013;&#30340;&#27745;&#26579;&#25915;&#20987;&#65292;&#31934;&#30830;&#21305;&#37197;&#29575;&#21487;&#25552;&#39640;&#36817;20%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#24320;&#25918;&#22495;&#38382;&#31572;&#39046;&#22495;&#20013;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23545;&#20110;&#25628;&#32034;&#38598;&#21512;&#36827;&#34892;&#30340;&#25932;&#23545;&#27745;&#26579;&#21487;&#33021;&#20250;&#23548;&#33268;&#29983;&#20135;&#31995;&#32479;&#30340;&#31934;&#24230;&#22823;&#24133;&#19979;&#38477;&#12290;&#28982;&#32780;&#65292;&#20960;&#20046;&#27809;&#26377;&#24037;&#20316;&#25552;&#20986;&#38450;&#24481;&#36825;&#20123;&#25915;&#20987;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20381;&#36182;&#20110;&#22823;&#22411;&#35821;&#26009;&#24211;&#20013;&#23384;&#22312;&#20887;&#20313;&#20449;&#24687;&#30340;&#30452;&#35273;&#12290;&#20026;&#20102;&#25214;&#21040;&#36825;&#20123;&#20449;&#24687;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20351;&#29992;&#26597;&#35810;&#25193;&#20805;&#26469;&#25628;&#32034;&#21487;&#33021;&#22238;&#31572;&#21407;&#22987;&#38382;&#39064;&#30340;&#22810;&#26679;&#21270;&#27573;&#33853;&#38598;&#21512;&#30340;&#26041;&#27861;&#65292;&#20294;&#26159;&#19981;&#22826;&#21487;&#33021;&#34987;&#27745;&#26579;&#12290;&#25105;&#20204;&#36890;&#36807;&#35774;&#35745;&#19968;&#31181;&#26032;&#22411;&#30340;&#32622;&#20449;&#24230;&#26041;&#27861;&#65288;&#27604;&#36739;&#39044;&#27979;&#31572;&#26696;&#19982;&#20854;&#22312;&#26816;&#32034;&#21040;&#30340;&#19978;&#19979;&#25991;&#20013;&#20986;&#29616;&#30340;&#24773;&#20917;&#8212;&#8212;&#25105;&#20204;&#31216;&#20043;&#20026;&#31572;&#26696;&#20887;&#20313;&#32622;&#20449;&#24230;&#65292;&#21363;CAR&#65289;&#23558;&#36825;&#20123;&#26032;&#27573;&#33853;&#38598;&#25104;&#21040;&#27169;&#22411;&#20013;&#12290;&#36825;&#20123;&#26041;&#27861;&#20849;&#21516;&#26500;&#25104;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26041;&#24335;&#65292;&#29992;&#20110;&#38450;&#24481;&#27745;&#26579;&#25915;&#20987;&#65292;&#21487;&#22312;&#19981;&#21516;&#27700;&#24179;&#30340;&#25968;&#25454;&#27745;&#26579;/&#30693;&#35782;&#20914;&#31361;&#19979;&#25552;&#20379;&#36817;20&#65285;&#30340;&#31934;&#30830;&#21305;&#37197;&#22686;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work in open-domain question answering (ODQA) has shown that adversarial poisoning of the search collection can cause large drops in accuracy for production systems. However, little to no work has proposed methods to defend against these attacks. To do so, we rely on the intuition that redundant information often exists in large corpora. To find it, we introduce a method that uses query augmentation to search for a diverse set of passages that could answer the original question but are less likely to have been poisoned. We integrate these new passages into the model through the design of a novel confidence method, comparing the predicted answer to its appearance in the retrieved contexts (what we call \textit{Confidence from Answer Redundancy}, i.e. CAR). Together these methods allow for a simple but effective way to defend against poisoning attacks that provides gains of nearly 20\% exact match across varying levels of data poisoning/knowledge conflicts.
&lt;/p&gt;</description></item></channel></rss>