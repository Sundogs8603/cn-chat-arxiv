<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#25552;&#20379;&#21830;&#20844;&#24179;&#24615;&#38382;&#39064;&#20197;&#21450;&#30456;&#24212;&#30340;&#36229;&#36234;&#20934;&#30830;&#24615;&#32771;&#34385;&#12290;&#36890;&#36807;&#24341;&#20837;&#21518;&#22788;&#29702;&#37325;&#26032;&#25490;&#24207;&#27169;&#22411;&#65292;&#25105;&#20204;&#22312;&#32500;&#25252;&#29992;&#25143;&#30456;&#20851;&#24615;&#21644;&#25512;&#33616;&#36136;&#37327;&#30340;&#21516;&#26102;&#65292;&#20248;&#20808;&#32771;&#34385;&#20102;&#25552;&#20379;&#21830;&#20844;&#24179;&#24615;&#12290;&#38024;&#23545;&#22810;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#35780;&#20272;&#26174;&#31034;&#35813;&#27169;&#22411;&#22312;&#25512;&#33616;&#36136;&#37327;&#21508;&#20010;&#26041;&#38754;&#26377;&#31215;&#26497;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2309.04250</link><description>&lt;p&gt;
&#25552;&#20379;&#21830;&#20844;&#24179;&#24615;&#19982;&#25512;&#33616;&#31995;&#32479;&#20013;&#36229;&#36234;&#20934;&#30830;&#24615;&#30340;&#26435;&#34913;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Provider Fairness and Beyond-Accuracy Trade-offs in Recommender Systems. (arXiv:2309.04250v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04250
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#25552;&#20379;&#21830;&#20844;&#24179;&#24615;&#38382;&#39064;&#20197;&#21450;&#30456;&#24212;&#30340;&#36229;&#36234;&#20934;&#30830;&#24615;&#32771;&#34385;&#12290;&#36890;&#36807;&#24341;&#20837;&#21518;&#22788;&#29702;&#37325;&#26032;&#25490;&#24207;&#27169;&#22411;&#65292;&#25105;&#20204;&#22312;&#32500;&#25252;&#29992;&#25143;&#30456;&#20851;&#24615;&#21644;&#25512;&#33616;&#36136;&#37327;&#30340;&#21516;&#26102;&#65292;&#20248;&#20808;&#32771;&#34385;&#20102;&#25552;&#20379;&#21830;&#20844;&#24179;&#24615;&#12290;&#38024;&#23545;&#22810;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#35780;&#20272;&#26174;&#31034;&#35813;&#27169;&#22411;&#22312;&#25512;&#33616;&#36136;&#37327;&#21508;&#20010;&#26041;&#38754;&#26377;&#31215;&#26497;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#22312;&#25913;&#21892;&#29992;&#25143;&#22312;&#32447;&#20307;&#39564;&#30340;&#21516;&#26102;&#65292;&#20063;&#24341;&#21457;&#20102;&#23545;&#28508;&#22312;&#30340;&#25552;&#20379;&#21830;&#20844;&#24179;&#24615;&#38382;&#39064;&#30340;&#20851;&#27880;&#12290;&#36825;&#20123;&#31995;&#32479;&#21487;&#33021;&#20250;&#19981;&#32463;&#24847;&#22320;&#20559;&#29233;&#28909;&#38376;&#29289;&#21697;&#65292;&#20174;&#32780;&#20351;&#36739;&#19981;&#27969;&#34892;&#30340;&#29289;&#21697;&#36793;&#32536;&#21270;&#65292;&#24182;&#22949;&#21327;&#20102;&#25552;&#20379;&#21830;&#30340;&#20844;&#24179;&#24615;&#12290;&#34429;&#28982;&#20197;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#24847;&#35782;&#21040;&#25552;&#20379;&#21830;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#20294;&#23545;&#36825;&#20123;&#20559;&#35265;&#22914;&#20309;&#24433;&#21709;&#25512;&#33616;&#31995;&#32479;&#30340;&#36229;&#36234;&#20934;&#30830;&#24615;&#26041;&#38754;&#65288;&#22914;&#22810;&#26679;&#24615;&#12289;&#26032;&#39062;&#24615;&#12289;&#35206;&#30422;&#29575;&#21644;&#20598;&#28982;&#24615;&#65289;&#30340;&#35843;&#26597;&#21364;&#19981;&#22815;&#37325;&#35270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#21518;&#22788;&#29702;&#37325;&#26032;&#25490;&#24207;&#27169;&#22411;&#65292;&#26088;&#22312;&#20248;&#20808;&#32771;&#34385;&#25552;&#20379;&#21830;&#20844;&#24179;&#24615;&#30340;&#21516;&#26102;&#65292;&#20445;&#25345;&#29992;&#25143;&#30456;&#20851;&#24615;&#21644;&#25512;&#33616;&#36136;&#37327;&#65292;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23545;&#27169;&#22411;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23545;&#25512;&#33616;&#36136;&#37327;&#30340;&#21508;&#20010;&#26041;&#38754;&#30340;&#24433;&#21709;&#36827;&#34892;&#28145;&#20837;&#35780;&#20272;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#21518;&#22788;&#29702;&#31639;&#27861;&#24212;&#29992;&#20110;&#22235;&#20010;&#19981;&#21516;&#39046;&#22495;&#25968;&#25454;&#38598;&#19978;&#30340;&#22235;&#20010;&#29420;&#31435;&#30340;&#25512;&#33616;&#27169;&#22411;&#65292;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommender systems, while transformative in online user experiences, have raised concerns over potential provider-side fairness issues. These systems may inadvertently favor popular items, thereby marginalizing less popular ones and compromising provider fairness. While previous research has recognized provider-side fairness issues, the investigation into how these biases affect beyond-accuracy aspects of recommendation systems - such as diversity, novelty, coverage, and serendipity - has been less emphasized. In this paper, we address this gap by introducing a simple yet effective post-processing re-ranking model that prioritizes provider fairness, while simultaneously maintaining user relevance and recommendation quality. We then conduct an in-depth evaluation of the model's impact on various aspects of recommendation quality across multiple datasets. Specifically, we apply the post-processing algorithm to four distinct recommendation models across four varied domain datasets, asses
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35752;&#35770;&#20102;&#22312;&#23384;&#22312;&#28508;&#22312;&#28151;&#28102;&#22240;&#32032;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#31163;&#32447;&#25512;&#33616;&#31995;&#32479;&#35780;&#20272;&#30340;&#38382;&#39064;&#65292;&#24182;&#29305;&#21035;&#20851;&#27880;&#25512;&#33616;&#31995;&#32479;&#29992;&#20363;&#12290;&#36890;&#36807;&#23545;&#22522;&#20110;&#31574;&#30053;&#30340;&#20272;&#35745;&#22120;&#36827;&#34892;&#30740;&#31350;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#30001;&#28151;&#28102;&#22240;&#32032;&#24341;&#36215;&#30340;&#32479;&#35745;&#20559;&#24046;&#12290;</title><link>http://arxiv.org/abs/2309.04222</link><description>&lt;p&gt;
&#26410;&#35266;&#23519;&#21040;&#28508;&#22312;&#28151;&#28102;&#22240;&#32032;&#19979;&#30340;&#31163;&#32447;&#25512;&#33616;&#31995;&#32479;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Offline Recommender System Evaluation under Unobserved Confounding. (arXiv:2309.04222v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04222
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35752;&#35770;&#20102;&#22312;&#23384;&#22312;&#28508;&#22312;&#28151;&#28102;&#22240;&#32032;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#31163;&#32447;&#25512;&#33616;&#31995;&#32479;&#35780;&#20272;&#30340;&#38382;&#39064;&#65292;&#24182;&#29305;&#21035;&#20851;&#27880;&#25512;&#33616;&#31995;&#32479;&#29992;&#20363;&#12290;&#36890;&#36807;&#23545;&#22522;&#20110;&#31574;&#30053;&#30340;&#20272;&#35745;&#22120;&#36827;&#34892;&#30740;&#31350;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#30001;&#28151;&#28102;&#22240;&#32032;&#24341;&#36215;&#30340;&#32479;&#35745;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#25919;&#31574;&#20272;&#35745;&#26041;&#27861;(OPE)&#20801;&#35768;&#25105;&#20204;&#20174;&#35760;&#24405;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#21644;&#35780;&#20272;&#20915;&#31574;&#31574;&#30053;&#65292;&#20351;&#23427;&#20204;&#25104;&#20026;&#31163;&#32447;&#35780;&#20272;&#25512;&#33616;&#31995;&#32479;&#30340;&#21560;&#24341;&#20154;&#36873;&#25321;&#12290;&#26368;&#36817;&#30340;&#19968;&#20123;&#20316;&#21697;&#25253;&#36947;&#20102;&#25104;&#21151;&#37319;&#29992;OPE&#26041;&#27861;&#30340;&#24773;&#20917;&#12290;&#36825;&#39033;&#24037;&#20316;&#30340;&#19968;&#20010;&#37325;&#35201;&#20551;&#35774;&#26159;&#19981;&#23384;&#22312;&#26410;&#35266;&#23519;&#21040;&#30340;&#28151;&#28102;&#22240;&#32032;&#65306;&#22312;&#25968;&#25454;&#25910;&#38598;&#26102;&#24433;&#21709;&#34892;&#21160;&#21644;&#22870;&#21169;&#30340;&#38543;&#26426;&#21464;&#37327;&#12290;&#30001;&#20110;&#25968;&#25454;&#25910;&#38598;&#31574;&#30053;&#36890;&#24120;&#22312;&#20174;&#19994;&#32773;&#30340;&#25511;&#21046;&#20043;&#19979;&#65292;&#22240;&#27492;&#24456;&#23569;&#26126;&#30830;&#22320;&#25552;&#21450;&#26080;&#28151;&#28102;&#20551;&#35774;&#65292;&#24182;&#19988;&#29616;&#26377;&#25991;&#29486;&#20013;&#24456;&#23569;&#22788;&#29702;&#20854;&#36829;&#35268;&#38382;&#39064;&#12290;&#36825;&#39033;&#24037;&#20316;&#26088;&#22312;&#24378;&#35843;&#22312;&#23384;&#22312;&#26410;&#35266;&#23519;&#21040;&#30340;&#28151;&#28102;&#22240;&#32032;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#31163;&#32447;&#31574;&#30053;&#20272;&#35745;&#26102;&#20986;&#29616;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#20851;&#27880;&#25512;&#33616;&#31995;&#32479;&#30340;&#29992;&#20363;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#22522;&#20110;&#31574;&#30053;&#30340;&#20272;&#35745;&#22120;&#65292;&#20854;&#20013;&#26085;&#24535;&#20542;&#21521;&#26159;&#20174;&#35760;&#24405;&#25968;&#25454;&#20013;&#23398;&#20064;&#30340;&#12290;&#25105;&#20204;&#23545;&#30001;&#20110;&#28151;&#28102;&#22240;&#32032;&#24341;&#36215;&#30340;&#32479;&#35745;&#20559;&#24046;&#36827;&#34892;&#20102;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
Off-Policy Estimation (OPE) methods allow us to learn and evaluate decision-making policies from logged data. This makes them an attractive choice for the offline evaluation of recommender systems, and several recent works have reported successful adoption of OPE methods to this end. An important assumption that makes this work is the absence of unobserved confounders: random variables that influence both actions and rewards at data collection time. Because the data collection policy is typically under the practitioner's control, the unconfoundedness assumption is often left implicit, and its violations are rarely dealt with in the existing literature.  This work aims to highlight the problems that arise when performing off-policy estimation in the presence of unobserved confounders, specifically focusing on a recommendation use-case. We focus on policy-based estimators, where the logging propensities are learned from logged data. We characterise the statistical bias that arises due to
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#22522;&#20110;&#32426;&#24405;&#29255;&#21046;&#20316;&#25216;&#26415;&#30340;&#31639;&#27861;&#25512;&#33616;&#22312;T{\"e}nk&#24179;&#21488;&#19978;&#30340;&#25509;&#21463;&#24773;&#20917;&#65292;&#36890;&#36807;&#26500;&#24314;&#19968;&#32452;&#20803;&#25968;&#25454;&#26469;&#25551;&#36848;&#32426;&#24405;&#29255;&#21046;&#20316;&#35774;&#22791;&#30340;&#22810;&#26679;&#24615;&#65292;&#25506;&#35752;&#24179;&#21488;&#30005;&#24433;&#29233;&#22909;&#32773;&#22914;&#20309;&#29702;&#35299;&#21644;&#25509;&#21463;&#31867;&#20284;&#32426;&#24405;&#29255;&#21046;&#20316;&#35774;&#22791;&#30340;&#20010;&#24615;&#21270;&#25512;&#33616;&#12290;&#35752;&#35770;&#20102;&#36825;&#20010;&#27010;&#24565;&#39564;&#35777;&#30340;&#36129;&#29486;&#21644;&#23616;&#38480;&#65292;&#24182;&#25552;&#20986;&#20102;&#25552;&#21319;&#32426;&#24405;&#29255;&#20202;&#22120;&#21270;&#23186;&#20171;&#30340;&#24605;&#32771;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2309.04184</link><description>&lt;p&gt;
&#22522;&#20110;&#32426;&#24405;&#29255;&#21046;&#20316;&#25216;&#26415;&#30340;&#31639;&#27861;&#25512;&#33616;&#30340;&#25509;&#21463;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Receiving an algorithmic recommendation based on documentary filmmaking techniques. (arXiv:2309.04184v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04184
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#22522;&#20110;&#32426;&#24405;&#29255;&#21046;&#20316;&#25216;&#26415;&#30340;&#31639;&#27861;&#25512;&#33616;&#22312;T{\"e}nk&#24179;&#21488;&#19978;&#30340;&#25509;&#21463;&#24773;&#20917;&#65292;&#36890;&#36807;&#26500;&#24314;&#19968;&#32452;&#20803;&#25968;&#25454;&#26469;&#25551;&#36848;&#32426;&#24405;&#29255;&#21046;&#20316;&#35774;&#22791;&#30340;&#22810;&#26679;&#24615;&#65292;&#25506;&#35752;&#24179;&#21488;&#30005;&#24433;&#29233;&#22909;&#32773;&#22914;&#20309;&#29702;&#35299;&#21644;&#25509;&#21463;&#31867;&#20284;&#32426;&#24405;&#29255;&#21046;&#20316;&#35774;&#22791;&#30340;&#20010;&#24615;&#21270;&#25512;&#33616;&#12290;&#35752;&#35770;&#20102;&#36825;&#20010;&#27010;&#24565;&#39564;&#35777;&#30340;&#36129;&#29486;&#21644;&#23616;&#38480;&#65292;&#24182;&#25552;&#20986;&#20102;&#25552;&#21319;&#32426;&#24405;&#29255;&#20202;&#22120;&#21270;&#23186;&#20171;&#30340;&#24605;&#32771;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;T{\"e}nk&#24179;&#21488;&#30005;&#24433;&#35266;&#20247;&#23545;&#32426;&#24405;&#29255;&#30340;&#26032;&#39062;&#31639;&#27861;&#25512;&#33616;&#30340;&#25509;&#21463;&#24773;&#20917;&#12290;&#20026;&#20102;&#25552;&#20986;&#19968;&#31181;&#26367;&#20195;&#22522;&#20110;&#20027;&#39064;&#20998;&#31867;&#12289;&#23548;&#28436;&#25110;&#21046;&#20316;&#26102;&#26399;&#30340;&#25512;&#33616;&#26041;&#27861;&#65292;&#22312;&#36825;&#20010;&#23454;&#39564;&#26694;&#26550;&#20869;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#32452;&#20803;&#25968;&#25454;&#65292;&#20197;&#25551;&#36848;&#8220;&#32426;&#24405;&#29255;&#21046;&#20316;&#35774;&#22791;&#8221;&#30340;&#20016;&#23500;&#22810;&#26679;&#24615;&#12290;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#35843;&#26597;&#24179;&#21488;&#30340;&#30005;&#24433;&#29233;&#22909;&#32773;&#22312;&#20010;&#24615;&#21270;&#25512;&#33616;&#30340;&#24773;&#20917;&#19979;&#22914;&#20309;&#29702;&#35299;&#21644;&#25509;&#21463;&#26377;&#30456;&#20284;&#32426;&#24405;&#29255;&#21046;&#20316;&#35774;&#22791;&#30340;4&#37096;&#32426;&#24405;&#29255;&#12290;&#26368;&#21518;&#65292;&#35752;&#35770;&#20102;&#36825;&#20010;&#27010;&#24565;&#39564;&#35777;&#30340;&#36129;&#29486;&#21644;&#23616;&#38480;&#65292;&#20197;&#21246;&#21202;&#20986;&#25552;&#21319;&#32426;&#24405;&#29255;&#20202;&#22120;&#21270;&#23186;&#20171;&#30340;&#24605;&#32771;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article analyzes the reception of a novel algorithmic recommendation of documentary films by a panel of moviegoers of the T{\"e}nk platform. In order to propose an alternative to recommendations based on a thematic classification, the director or the production period, a set of metadata has been elaborated within the framework of this experimentation in order to characterize the great variety of ``documentary filmmaking dispositifs'' . The goal is to investigate the different ways in which the platform's film lovers appropriate a personalized recommendation of 4 documentaries with similar or similar filmmaking dispositifs. To conclude, the contributions and limits of this proof of concept are discussed in order to sketch out avenues of reflection for improving the instrumented mediation of documentary films.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38271;&#23614;&#21451;&#22909;&#30340;&#34920;&#31034;&#26694;&#26550;&#65292;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#33402;&#26415;&#23478;&#21644;&#38899;&#20048;&#20043;&#38388;&#30340;&#30456;&#20284;&#20851;&#31995;&#65292;&#25913;&#21892;&#20102;&#20256;&#32479;&#26041;&#27861;&#22312;&#31232;&#30095;&#20851;&#31995;&#20013;&#30340;&#34920;&#31034;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.04182</link><description>&lt;p&gt;
&#19968;&#31181;&#38024;&#23545;&#33402;&#26415;&#23478;&#21644;&#38899;&#20048;&#30456;&#20284;&#24615;&#30340;&#38271;&#23614;&#21451;&#22909;&#34920;&#31034;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Long-Tail Friendly Representation Framework for Artist and Music Similarity. (arXiv:2309.04182v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04182
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38271;&#23614;&#21451;&#22909;&#30340;&#34920;&#31034;&#26694;&#26550;&#65292;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#33402;&#26415;&#23478;&#21644;&#38899;&#20048;&#20043;&#38388;&#30340;&#30456;&#20284;&#20851;&#31995;&#65292;&#25913;&#21892;&#20102;&#20256;&#32479;&#26041;&#27861;&#22312;&#31232;&#30095;&#20851;&#31995;&#20013;&#30340;&#34920;&#31034;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38899;&#20048;&#26816;&#32034;&#21644;&#25512;&#33616;&#20013;&#65292;&#30740;&#31350;&#33402;&#26415;&#23478;&#21644;&#38899;&#20048;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#35299;&#20915;&#38271;&#23614;&#29616;&#35937;&#30340;&#25361;&#25112;&#26085;&#30410;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#24314;&#27169;&#30456;&#20284;&#20851;&#31995;&#30340;&#38271;&#23614;&#21451;&#22909;&#34920;&#31034;&#26694;&#26550;(LTFRF)&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#38899;&#20048;&#12289;&#29992;&#25143;&#12289;&#20803;&#25968;&#25454;&#21644;&#20851;&#31995;&#25968;&#25454;&#25972;&#21512;&#21040;&#19968;&#20010;&#32479;&#19968;&#30340;&#24230;&#37327;&#23398;&#20064;&#26694;&#26550;&#20013;&#65292;&#21033;&#29992;&#20803;&#19968;&#33268;&#24615;&#20851;&#31995;&#20316;&#20026;&#27491;&#21017;&#39033;&#65292;&#24341;&#20837;&#22810;&#20851;&#31995;&#25439;&#22833;&#12290;&#19982;&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#30456;&#27604;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#25913;&#21892;&#20102;&#38271;&#23614;&#22330;&#26223;&#20013;&#31232;&#30095;&#30340;&#33402;&#26415;&#23478;&#21644;&#38899;&#20048;&#20043;&#38388;&#30340;&#20851;&#31995;&#34920;&#31034;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;AllMusic&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#21644;&#20998;&#26512;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#26694;&#26550;&#25552;&#20379;&#20102;&#33402;&#26415;&#23478;&#21644;&#38899;&#20048;&#34920;&#31034;&#30340;&#26377;&#21033;&#27867;&#21270;&#25928;&#26524;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#31867;&#20284;&#33402;&#26415;&#23478;/&#38899;&#20048;&#25512;&#33616;&#20219;&#21153;&#19978;&#65292;t
&lt;/p&gt;
&lt;p&gt;
The investigation of the similarity between artists and music is crucial in music retrieval and recommendation, and addressing the challenge of the long-tail phenomenon is increasingly important. This paper proposes a Long-Tail Friendly Representation Framework (LTFRF) that utilizes neural networks to model the similarity relationship. Our approach integrates music, user, metadata, and relationship data into a unified metric learning framework, and employs a meta-consistency relationship as a regular term to introduce the Multi-Relationship Loss. Compared to the Graph Neural Network (GNN), our proposed framework improves the representation performance in long-tail scenarios, which are characterized by sparse relationships between artists and music. We conduct experiments and analysis on the AllMusic dataset, and the results demonstrate that our framework provides a favorable generalization of artist and music representation. Specifically, on similar artist/music recommendation tasks, t
&lt;/p&gt;</description></item><item><title>PRISTA-Net&#26159;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#36845;&#20195;&#32553;&#20943;&#38408;&#20540;&#31639;&#27861;&#30340;&#32593;&#32476;&#65292;&#36890;&#36807;&#23398;&#20064;&#38750;&#32447;&#24615;&#21464;&#25442;&#21644;&#27880;&#24847;&#26426;&#21046;&#26469;&#22788;&#29702;&#30456;&#20301;&#24674;&#22797;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#24555;&#36895;&#20613;&#37324;&#21494;&#21464;&#25442;&#21644;&#22522;&#20110;&#23545;&#25968;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.04171</link><description>&lt;p&gt;
PRISTA-Net:&#29992;&#20110;&#32534;&#30721;&#34893;&#23556;&#22270;&#27169;&#24335;&#30456;&#20301;&#24674;&#22797;&#30340;&#28145;&#24230;&#36845;&#20195;&#32553;&#20943;&#38408;&#20540;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
PRISTA-Net: Deep Iterative Shrinkage Thresholding Network for Coded Diffraction Patterns Phase Retrieval. (arXiv:2309.04171v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04171
&lt;/p&gt;
&lt;p&gt;
PRISTA-Net&#26159;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#36845;&#20195;&#32553;&#20943;&#38408;&#20540;&#31639;&#27861;&#30340;&#32593;&#32476;&#65292;&#36890;&#36807;&#23398;&#20064;&#38750;&#32447;&#24615;&#21464;&#25442;&#21644;&#27880;&#24847;&#26426;&#21046;&#26469;&#22788;&#29702;&#30456;&#20301;&#24674;&#22797;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#24555;&#36895;&#20613;&#37324;&#21494;&#21464;&#25442;&#21644;&#22522;&#20110;&#23545;&#25968;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#20301;&#24674;&#22797;&#65288;PR&#65289;&#38382;&#39064;&#28041;&#21450;&#20174;&#26377;&#38480;&#30340;&#24133;&#24230;&#27979;&#37327;&#25968;&#25454;&#20013;&#24674;&#22797;&#26410;&#30693;&#22270;&#20687;&#65292;&#26159;&#35745;&#31639;&#25104;&#20687;&#21644;&#22270;&#20687;&#22788;&#29702;&#20013;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38750;&#32447;&#24615;&#36870;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;PR&#26041;&#27861;&#22522;&#20110;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#33324;&#37197;&#21644;&#35843;&#21442;&#38656;&#27714;&#30340;&#40657;&#30418;&#32593;&#32476;&#27169;&#22411;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;PRISTA-Net&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#19968;&#38454;&#36845;&#20195;&#32553;&#20943;&#38408;&#20540;&#31639;&#27861;&#65288;ISTA&#65289;&#30340;&#28145;&#24230;&#23637;&#24320;&#32593;&#32476;&#65288;DUN&#65289;&#12290;&#35813;&#32593;&#32476;&#21033;&#29992;&#21487;&#23398;&#20064;&#30340;&#38750;&#32447;&#24615;&#21464;&#25442;&#26469;&#22788;&#29702;&#31232;&#30095;&#20808;&#39564;&#20013;&#19982;&#36817;&#31471;&#28857;&#26144;&#23556;&#23376;&#38382;&#39064;&#26377;&#20851;&#30340;&#30456;&#20301;&#20449;&#24687;&#65292;&#36824;&#20351;&#29992;&#27880;&#24847;&#26426;&#21046;&#26469;&#32858;&#28966;&#21253;&#21547;&#22270;&#20687;&#36793;&#32536;&#12289;&#32441;&#29702;&#21644;&#32467;&#26500;&#30340;&#30456;&#20301;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#36824;&#20351;&#29992;&#24555;&#36895;&#20613;&#37324;&#21494;&#21464;&#25442;&#65288;FFT&#65289;&#26469;&#23398;&#20064;&#20840;&#23616;&#29305;&#24449;&#20197;&#22686;&#24378;&#23616;&#37096;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#35774;&#35745;&#30340;&#22522;&#20110;&#23545;&#25968;&#30340;&#25439;&#22833;&#20989;&#25968;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;
The problem of phase retrieval (PR) involves recovering an unknown image from limited amplitude measurement data and is a challenge nonlinear inverse problem in computational imaging and image processing. However, many of the PR methods are based on black-box network models that lack interpretability and plug-and-play (PnP) frameworks that are computationally complex and require careful parameter tuning. To address this, we have developed PRISTA-Net, a deep unfolding network (DUN) based on the first-order iterative shrinkage thresholding algorithm (ISTA). This network utilizes a learnable nonlinear transformation to address the proximal-point mapping sub-problem associated with the sparse priors, and an attention mechanism to focus on phase information containing image edges, textures, and structures. Additionally, the fast Fourier transform (FFT) is used to learn global features to enhance local information, and the designed logarithmic-based loss function leads to significant improve
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#30340;&#26032;&#22411;&#32039;&#20945;&#23884;&#20837;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#27491;&#21017;&#21270;&#20462;&#21098;&#30340;&#26041;&#24335;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#29615;&#22659;&#20013;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#20869;&#23384;&#25928;&#29575;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#39640;&#20934;&#30830;&#24230;&#30340;&#25512;&#33616;&#12290;</title><link>http://arxiv.org/abs/2309.03518</link><description>&lt;p&gt;
&#36890;&#36807;&#27491;&#21017;&#21270;&#20462;&#21098;&#26469;&#23398;&#20064;&#32039;&#20945;&#30340;&#32452;&#21512;&#23884;&#20837;&#20197;&#29992;&#20110;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Learning Compact Compositional Embeddings via Regularized Pruning for Recommendation. (arXiv:2309.03518v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03518
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#30340;&#26032;&#22411;&#32039;&#20945;&#23884;&#20837;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#27491;&#21017;&#21270;&#20462;&#21098;&#30340;&#26041;&#24335;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#29615;&#22659;&#20013;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#20869;&#23384;&#25928;&#29575;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#39640;&#20934;&#30830;&#24230;&#30340;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28508;&#22312;&#22240;&#32032;&#27169;&#22411;&#26159;&#24403;&#20195;&#25512;&#33616;&#31995;&#32479;&#30340;&#20027;&#35201;&#25903;&#26609;&#65292;&#30001;&#20110;&#23427;&#20204;&#30340;&#24615;&#33021;&#20248;&#21183;&#65292;&#22312;&#36825;&#20123;&#27169;&#22411;&#20013;&#65292;&#27599;&#20010;&#23454;&#20307;&#65288;&#36890;&#24120;&#26159;&#29992;&#25143;/&#29289;&#21697;&#65289;&#38656;&#35201;&#29992;&#19968;&#20010;&#22266;&#23450;&#32500;&#24230;&#65288;&#20363;&#22914;128&#65289;&#30340;&#21807;&#19968;&#21521;&#37327;&#23884;&#20837;&#26469;&#34920;&#31034;&#12290;&#30001;&#20110;&#30005;&#23376;&#21830;&#21153;&#32593;&#31449;&#19978;&#29992;&#25143;&#21644;&#29289;&#21697;&#30340;&#25968;&#37327;&#24040;&#22823;&#65292;&#23884;&#20837;&#34920;&#26684;&#21487;&#20197;&#35828;&#26159;&#25512;&#33616;&#31995;&#32479;&#20013;&#26368;&#19981;&#33410;&#30465;&#20869;&#23384;&#30340;&#32452;&#20214;&#12290;&#23545;&#20110;&#20219;&#20309;&#24076;&#26395;&#33021;&#22815;&#26377;&#25928;&#22320;&#25353;&#27604;&#20363;&#25193;&#23637;&#21040;&#19981;&#26029;&#22686;&#38271;&#30340;&#29992;&#25143;/&#29289;&#21697;&#25968;&#37327;&#25110;&#22312;&#36164;&#28304;&#21463;&#38480;&#29615;&#22659;&#20013;&#20173;&#28982;&#36866;&#29992;&#30340;&#36731;&#37327;&#32423;&#25512;&#33616;&#31995;&#32479;&#65292;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#35201;&#20040;&#36890;&#36807;&#21704;&#24076;&#20943;&#23569;&#25152;&#38656;&#30340;&#23884;&#20837;&#25968;&#37327;&#65292;&#35201;&#20040;&#36890;&#36807;&#31232;&#30095;&#21270;&#23436;&#25972;&#30340;&#23884;&#20837;&#34920;&#26684;&#20197;&#20851;&#38381;&#36873;&#23450;&#30340;&#23884;&#20837;&#32500;&#24230;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21704;&#24076;&#20914;&#31361;&#25110;&#23884;&#20837;&#36807;&#20110;&#31232;&#30095;&#65292;&#23588;&#20854;&#26159;&#22312;&#36866;&#24212;&#26356;&#32039;&#20945;&#30340;&#20869;&#23384;&#39044;&#31639;&#26102;&#65292;&#36825;&#20123;&#36731;&#37327;&#32423;&#25512;&#33616;&#22120;&#19981;&#21487;&#36991;&#20813;&#22320;&#20250;&#29306;&#29298;&#20854;&#20934;&#30830;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32039;&#20945;&#23884;&#20837;&#26694;&#26550;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#65292;&#31216;&#20026;Compos&#12290;
&lt;/p&gt;
&lt;p&gt;
Latent factor models are the dominant backbones of contemporary recommender systems (RSs) given their performance advantages, where a unique vector embedding with a fixed dimensionality (e.g., 128) is required to represent each entity (commonly a user/item). Due to the large number of users and items on e-commerce sites, the embedding table is arguably the least memory-efficient component of RSs. For any lightweight recommender that aims to efficiently scale with the growing size of users/items or to remain applicable in resource-constrained settings, existing solutions either reduce the number of embeddings needed via hashing, or sparsify the full embedding table to switch off selected embedding dimensions. However, as hash collision arises or embeddings become overly sparse, especially when adapting to a tighter memory budget, those lightweight recommenders inevitably have to compromise their accuracy. To this end, we propose a novel compact embedding framework for RSs, namely Compos
&lt;/p&gt;</description></item><item><title>TensorBank&#26159;&#19968;&#20010;&#22522;&#20110;Tensor&#30340;&#28246;&#20179;&#24211;&#65292;&#33021;&#22815;&#20197;&#39640;&#36895;&#20174;&#20113;&#23545;&#35937;&#23384;&#20648;&#27969;&#24335;&#20256;&#36755;&#24352;&#37327;&#21040;GPU&#20869;&#23384;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#20998;&#23618;&#32479;&#35745;&#25351;&#26631;&#36827;&#34892;&#26597;&#35810;&#21152;&#36895;&#12290;</title><link>http://arxiv.org/abs/2309.02094</link><description>&lt;p&gt;
TensorBank: &#22522;&#20110;Tensor&#30340;&#28246;&#20179;&#24211;&#29992;&#20110;&#22522;&#30784;&#27169;&#22411;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
TensorBank:Tensor Lakehouse for Foundation Model Training. (arXiv:2309.02094v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02094
&lt;/p&gt;
&lt;p&gt;
TensorBank&#26159;&#19968;&#20010;&#22522;&#20110;Tensor&#30340;&#28246;&#20179;&#24211;&#65292;&#33021;&#22815;&#20197;&#39640;&#36895;&#20174;&#20113;&#23545;&#35937;&#23384;&#20648;&#27969;&#24335;&#20256;&#36755;&#24352;&#37327;&#21040;GPU&#20869;&#23384;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#20998;&#23618;&#32479;&#35745;&#25351;&#26631;&#36827;&#34892;&#26597;&#35810;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22522;&#30784;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#20043;&#22806;&#30340;&#39046;&#22495;&#30340;&#20852;&#36215;&#65292;&#23384;&#20648;&#21644;&#27969;&#24335;&#22788;&#29702;&#39640;&#32500;&#25968;&#25454;&#25104;&#20026;&#22522;&#30784;&#27169;&#22411;&#35757;&#32451;&#30340;&#20851;&#38190;&#38656;&#27714;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;TensorBank&#65292;&#19968;&#20010;&#33021;&#22815;&#22522;&#20110;&#22797;&#26434;&#20851;&#31995;&#26597;&#35810;&#20174;&#20113;&#23545;&#35937;&#23384;&#20648;&#65288;COS&#65289;&#27969;&#24335;&#20256;&#36755;&#24352;&#37327;&#21040;GPU&#20869;&#23384;&#30340;&#30334;&#20159;&#32423;&#24352;&#37327;&#28246;&#20179;&#24211;&#12290;&#25105;&#20204;&#20351;&#29992;&#20998;&#23618;&#32479;&#35745;&#25351;&#26631;&#65288;HSI&#65289;&#26469;&#21152;&#36895;&#26597;&#35810;&#12290;&#25105;&#20204;&#30340;&#26550;&#26500;&#20801;&#35768;&#20351;&#29992;HTTP&#33539;&#22260;&#35835;&#21462;&#26469;&#30452;&#25509;&#35775;&#38382;&#22359;&#32423;&#21035;&#30340;&#24352;&#37327;&#12290;&#19968;&#26086;&#22312;GPU&#20869;&#23384;&#20013;&#65292;&#25968;&#25454;&#21487;&#20197;&#20351;&#29992;PyTorch&#36716;&#25442;&#36827;&#34892;&#36716;&#25442;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;PyTorch&#25968;&#25454;&#38598;&#31867;&#22411;&#65292;&#37197;&#26377;&#30456;&#24212;&#30340;&#25968;&#25454;&#38598;&#24037;&#21378;&#65292;&#29992;&#20110;&#23558;&#20851;&#31995;&#26597;&#35810;&#21644;&#35831;&#27714;&#30340;&#36716;&#25442;&#20316;&#20026;&#19968;&#20010;&#23454;&#20363;&#36827;&#34892;&#32763;&#35793;&#12290;&#36890;&#36807;&#20351;&#29992;HSI&#65292;&#21487;&#20197;&#36339;&#36807;&#19981;&#30456;&#20851;&#30340;&#22359;&#65292;&#32780;&#26080;&#38656;&#35835;&#21462;&#23427;&#20204;&#65292;&#22240;&#20026;&#36825;&#20123;&#32034;&#24341;&#21253;&#21547;&#19981;&#21516;&#23618;&#27425;&#20998;&#36776;&#29575;&#32423;&#21035;&#19978;&#20869;&#23481;&#30340;&#32479;&#35745;&#20449;&#24687;&#12290;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#24320;&#25918;&#26631;&#20934;&#30340;&#26377;&#20027;&#35266;&#35266;&#28857;&#30340;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Storing and streaming high dimensional data for foundation model training became a critical requirement with the rise of foundation models beyond natural language. In this paper we introduce TensorBank, a petabyte scale tensor lakehouse capable of streaming tensors from Cloud Object Store (COS) to GPU memory at wire speed based on complex relational queries. We use Hierarchical Statistical Indices (HSI) for query acceleration. Our architecture allows to directly address tensors on block level using HTTP range reads. Once in GPU memory, data can be transformed using PyTorch transforms. We provide a generic PyTorch dataset type with a corresponding dataset factory translating relational queries and requested transformations as an instance. By making use of the HSI, irrelevant blocks can be skipped without reading them as those indices contain statistics on their content at different hierarchical resolution levels. This is an opinionated architecture powered by open standards and making h
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102; RecFusion&#65292;&#19968;&#31181;&#29305;&#23450;&#38024;&#23545;1D&#21644;/&#25110;&#20108;&#36827;&#21046;&#35774;&#32622;&#30340;&#25512;&#33616;&#27169;&#22411;&#26041;&#27861;&#65292;&#20854;&#21033;&#29992;&#20102;&#20108;&#39033;&#24335;&#25193;&#25955;&#36807;&#31243;&#23545;&#20108;&#20803;&#29992;&#25143;-&#39033;&#30446;&#20132;&#20114;&#36827;&#34892;&#26174;&#24335;&#24314;&#27169;&#65292;&#24182;&#22312;&#26680;&#24515;&#25512;&#33616;&#35774;&#32622;&#21644;&#26368;&#24120;&#35265;&#30340;&#25968;&#25454;&#38598;&#19978;&#25509;&#36817;&#22797;&#26434;&#30340;VAE&#22522;&#32447;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.08947</link><description>&lt;p&gt;
RecFusion&#65306;&#22522;&#20110;&#20108;&#39033;&#24335;&#25193;&#25955;&#36807;&#31243;&#30340;1D&#25968;&#25454;&#25512;&#33616;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
RecFusion: A Binomial Diffusion Process for 1D Data for Recommendation. (arXiv:2306.08947v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08947
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; RecFusion&#65292;&#19968;&#31181;&#29305;&#23450;&#38024;&#23545;1D&#21644;/&#25110;&#20108;&#36827;&#21046;&#35774;&#32622;&#30340;&#25512;&#33616;&#27169;&#22411;&#26041;&#27861;&#65292;&#20854;&#21033;&#29992;&#20102;&#20108;&#39033;&#24335;&#25193;&#25955;&#36807;&#31243;&#23545;&#20108;&#20803;&#29992;&#25143;-&#39033;&#30446;&#20132;&#20114;&#36827;&#34892;&#26174;&#24335;&#24314;&#27169;&#65292;&#24182;&#22312;&#26680;&#24515;&#25512;&#33616;&#35774;&#32622;&#21644;&#26368;&#24120;&#35265;&#30340;&#25968;&#25454;&#38598;&#19978;&#25509;&#36817;&#22797;&#26434;&#30340;VAE&#22522;&#32447;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;RecFusion&#65292;&#36825;&#26159;&#19968;&#32452;&#29992;&#20110;&#25512;&#33616;&#30340;&#25193;&#25955;&#27169;&#22411;&#12290;&#19981;&#21516;&#20110;&#21253;&#21547;&#31354;&#38388;&#30456;&#20851;&#24615;&#30340;&#22270;&#20687;&#25968;&#25454;&#65292;&#24120;&#29992;&#20110;&#25512;&#33616;&#30340;&#29992;&#25143;-&#39033;&#30446;&#20132;&#20114;&#30697;&#38453;&#32570;&#20047;&#29992;&#25143;&#21644;&#39033;&#30446;&#20043;&#38388;&#30340;&#31354;&#38388;&#20851;&#31995;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#19968;&#32500;&#21521;&#37327;&#19978;&#21046;&#23450;&#20102;&#25193;&#25955;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#20108;&#39033;&#24335;&#25193;&#25955;&#65292;&#36825;&#20010;&#26041;&#27861;&#21033;&#29992;&#20102;&#20271;&#21162;&#21033;&#36807;&#31243;&#26174;&#24335;&#22320;&#23545;&#20108;&#20803;&#29992;&#25143;-&#39033;&#30446;&#20132;&#20114;&#36827;&#34892;&#24314;&#27169;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;RecFusion&#22312;&#26680;&#24515;&#25512;&#33616;&#35774;&#32622;&#65288;&#38024;&#23545;&#20108;&#36827;&#21046;&#38750;&#39034;&#24207;&#21453;&#39304;&#30340;&#21069;n&#39033;&#25512;&#33616;&#65289;&#21644;&#26368;&#24120;&#35265;&#30340;&#25968;&#25454;&#38598;&#65288;MovieLens&#21644;Netflix&#65289;&#19978;&#25509;&#36817;&#20110;&#22797;&#26434;&#30340;VAE&#22522;&#32447;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#19987;&#38376;&#38024;&#23545;1D&#21644;/&#25110;&#20108;&#36827;&#21046;&#35774;&#32622;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#24847;&#20041;&#36229;&#20986;&#20102;&#25512;&#33616;&#31995;&#32479;&#65292;&#20363;&#22914;&#22312;&#21307;&#23398;&#39046;&#22495;&#20013;&#20351;&#29992;MRI&#21644;CT&#25195;&#25551;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we propose RecFusion, which comprise a set of diffusion models for recommendation. Unlike image data which contain spatial correlations, a user-item interaction matrix, commonly utilized in recommendation, lacks spatial relationships between users and items. We formulate diffusion on a 1D vector and propose binomial diffusion, which explicitly models binary user-item interactions with a Bernoulli process. We show that RecFusion approaches the performance of complex VAE baselines on the core recommendation setting (top-n recommendation for binary non-sequential feedback) and the most common datasets (MovieLens and Netflix). Our proposed diffusion models that are specialized for 1D and/or binary setups have implications beyond recommendation systems, such as in the medical domain with MRI and CT scans.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;STIXnet&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#33258;&#21160;&#25552;&#21462;CTI&#25253;&#21578;&#20013;&#25152;&#26377;&#30340;STIX&#23454;&#20307;&#21644;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2303.09999</link><description>&lt;p&gt;
STIXnet: &#19968;&#31181;&#20174;CTI&#25253;&#21578;&#20013;&#25552;&#21462;&#25152;&#26377;STIX&#23545;&#35937;&#30340;&#26032;&#22411;&#27169;&#22359;&#21270;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
STIXnet: A Novel and Modular Solution for Extracting All STIX Objects in CTI Reports. (arXiv:2303.09999v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09999
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;STIXnet&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#33258;&#21160;&#25552;&#21462;CTI&#25253;&#21578;&#20013;&#25152;&#26377;&#30340;STIX&#23454;&#20307;&#21644;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#32593;&#32476;&#23041;&#32961;&#24773;&#25253;(CTI)&#25253;&#21578;&#20013;&#33258;&#21160;&#25552;&#21462;&#20449;&#24687;&#23545;&#20110;&#39118;&#38505;&#31649;&#29702;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;STIXnet&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#25216;&#26415;&#21644;&#20132;&#20114;&#24335;&#23454;&#20307;&#30693;&#35782;&#24211;&#65288;KB&#65289;&#65292;&#21487;&#20197;&#33258;&#21160;&#25552;&#21462;CTI&#25253;&#21578;&#20013;&#25152;&#26377;&#30340;STIX&#23454;&#20307;&#21644;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
The automatic extraction of information from Cyber Threat Intelligence (CTI) reports is crucial in risk management. The increased frequency of the publications of these reports has led researchers to develop new systems for automatically recovering different types of entities and relations from textual data. Most state-of-the-art models leverage Natural Language Processing (NLP) techniques, which perform greatly in extracting a few types of entities at a time but cannot detect heterogeneous data or their relations. Furthermore, several paradigms, such as STIX, have become de facto standards in the CTI community and dictate a formal categorization of different entities and relations to enable organizations to share data consistently. This paper presents STIXnet, the first solution for the automated extraction of all STIX entities and relationships in CTI reports. Through the use of NLP techniques and an interactive Knowledge Base (KB) of entities, our approach obtains F1 scores comparab
&lt;/p&gt;</description></item></channel></rss>