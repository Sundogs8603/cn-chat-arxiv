<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#26412;&#35770;&#25991;&#20013;&#20171;&#32461;&#20102;&#22522;&#20110;LLM&#30340;&#25512;&#33616;&#20013;&#30340;&#25968;&#25454;&#39640;&#25928;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21098;&#26525;&#25968;&#25454;&#26469;&#20943;&#23569;LLM&#30340;&#24494;&#35843;&#25104;&#26412;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#30446;&#26631;&#26469;&#23454;&#29616;&#39640;&#20934;&#30830;&#24615;&#30340;&#25968;&#25454;&#21098;&#26525;&#12290;</title><link>https://arxiv.org/abs/2401.17197</link><description>&lt;p&gt;
&#22522;&#20110;LLM&#30340;&#25512;&#33616;&#30340;&#25968;&#25454;&#39640;&#25928;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Data-efficient Fine-tuning for LLM-based Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17197
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20013;&#20171;&#32461;&#20102;&#22522;&#20110;LLM&#30340;&#25512;&#33616;&#20013;&#30340;&#25968;&#25454;&#39640;&#25928;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21098;&#26525;&#25968;&#25454;&#26469;&#20943;&#23569;LLM&#30340;&#24494;&#35843;&#25104;&#26412;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#30446;&#26631;&#26469;&#23454;&#29616;&#39640;&#20934;&#30830;&#24615;&#30340;&#25968;&#25454;&#21098;&#26525;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#25512;&#33616;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#20854;&#20013;&#24494;&#35843;&#22312;LLM&#30340;&#36866;&#24212;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#24555;&#36895;&#25193;&#23637;&#30340;&#25512;&#33616;&#25968;&#25454;&#19978;&#24494;&#35843;LLM&#30340;&#25104;&#26412;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#23569;&#26679;&#26412;&#24494;&#35843;&#25552;&#20379;&#20102;&#19968;&#31181;&#24555;&#36895;&#36866;&#24212;LLM&#21040;&#26032;&#30340;&#25512;&#33616;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20026;&#39640;&#25928;&#30340;LLM&#25512;&#33616;&#20219;&#21153;&#21098;&#26525;&#25968;&#25454;&#30340;&#20219;&#21153;&#65292;&#26088;&#22312;&#25214;&#21040;&#36866;&#21512;LLM&#30340;&#23569;&#26679;&#26412;&#24494;&#35843;&#30340;&#20195;&#34920;&#26679;&#26412;&#12290;&#34429;&#28982;&#26680;&#24515;&#38598;&#36873;&#25321;&#19982;&#25152;&#25552;&#20986;&#30340;&#20219;&#21153;&#23494;&#20999;&#30456;&#20851;&#65292;&#20294;&#29616;&#26377;&#30340;&#26680;&#24515;&#38598;&#36873;&#25321;&#26041;&#27861;&#24448;&#24448;&#20381;&#36182;&#20110;&#27425;&#20248;&#21551;&#21457;&#24335;&#25351;&#26631;&#25110;&#38656;&#35201;&#22312;&#22823;&#35268;&#27169;&#25512;&#33616;&#25968;&#25454;&#19978;&#36827;&#34892;&#26114;&#36149;&#30340;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Leveraging Large Language Models (LLMs) for recommendation has recently garnered considerable attention, where fine-tuning plays a key role in LLMs' adaptation. However, the cost of fine-tuning LLMs on rapidly expanding recommendation data limits their practical application. To address this challenge, few-shot fine-tuning offers a promising approach to quickly adapt LLMs to new recommendation data. We propose the task of data pruning for efficient LLM-based recommendation, aimed at identifying representative samples tailored for LLMs' few-shot fine-tuning. While coreset selection is closely related to the proposed task, existing coreset selection methods often rely on suboptimal heuristic metrics or entail costly optimization on large-scale recommendation data.   To tackle these issues, we introduce two objectives for the data pruning task in the context of LLM-based recommendation: 1) high accuracy aims to identify the influential samples that can lead to high overall performance; and
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25345;&#32493;&#35821;&#35328;&#23398;&#20064;&#26469;&#25193;&#23637;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;VL-PTMs&#65289;&#30340;&#35821;&#35328;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#22686;&#37327;&#26356;&#26032;&#35821;&#35328;&#30693;&#35782;&#65292;&#36991;&#20813;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#24182;&#22312;&#36328;&#27169;&#24577;&#21644;&#36328;&#35821;&#35328;&#30446;&#26631;&#19979;&#36827;&#34892;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2401.17186</link><description>&lt;p&gt;
&#22312;CLIP&#20013;&#25317;&#25265;&#35821;&#35328;&#21253;&#23481;&#24615;&#21644;&#22810;&#26679;&#24615;&#65306;&#36890;&#36807;&#25345;&#32493;&#35821;&#35328;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Embracing Language Inclusivity and Diversity in CLIP through Continual Language Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17186
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25345;&#32493;&#35821;&#35328;&#23398;&#20064;&#26469;&#25193;&#23637;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;VL-PTMs&#65289;&#30340;&#35821;&#35328;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#22686;&#37327;&#26356;&#26032;&#35821;&#35328;&#30693;&#35782;&#65292;&#36991;&#20813;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#24182;&#22312;&#36328;&#27169;&#24577;&#21644;&#36328;&#35821;&#35328;&#30446;&#26631;&#19979;&#36827;&#34892;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;(VL-PTMs)&#22312;&#22810;&#27169;&#24577;&#30740;&#31350;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#30001;&#20110;&#21482;&#25484;&#25569;&#20102;&#23569;&#25968;&#20960;&#31181;&#35821;&#35328;&#65288;&#27604;&#22914;&#33521;&#35821;&#65289;&#65292;&#38480;&#21046;&#20102;&#20854;&#22312;&#26356;&#24191;&#27867;&#30340;&#31038;&#21306;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#20851;&#27880;&#36890;&#36807;&#32852;&#21512;&#23398;&#20064;&#26469;&#24320;&#21457;&#22810;&#35821;&#35328;VL&#27169;&#22411;&#65292;&#28982;&#32780;&#65292;&#30001;&#20110;&#26114;&#36149;&#30340;&#25104;&#26412;&#21644;&#25968;&#25454;&#21487;&#29992;&#24615;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#33021;&#19981;&#20999;&#23454;&#38469;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#25345;&#32493;&#35821;&#35328;&#23398;&#20064;&#65288;CLL&#65289;&#26469;&#25193;&#23637;VL-PTMs&#30340;&#35821;&#35328;&#33021;&#21147;&#65292;&#21363;&#27169;&#22411;&#38656;&#35201;&#22686;&#37327;&#22320;&#26356;&#26032;&#20854;&#35821;&#35328;&#30693;&#35782;&#65292;&#21516;&#26102;&#36991;&#20813;&#28798;&#38590;&#24615;&#36951;&#24536;&#65288;CF&#65289;&#12290;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;CLL-CLIP&#30340;&#27169;&#22411;&#65292;&#23427;&#26159;&#22312;CLIP&#30340;&#22522;&#30784;&#19978;&#26500;&#24314;&#30340;&#65292;&#32780;CLIP&#26159;&#19968;&#31181;&#24050;&#32463;&#20855;&#22791;&#20102;&#22270;&#20687;-&#33521;&#35821;&#25991;&#26412;&#23545;&#40784;&#33021;&#21147;&#30340;&#27969;&#34892;VL-PTM&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;CLL-CLIP&#21253;&#21547;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#35789;&#23884;&#20837;&#23618;&#65292;&#29992;&#20110;&#22788;&#29702;&#35821;&#35328;&#24046;&#24322;&#12290;&#23427;&#20165;&#35757;&#32451;&#35789;&#23884;&#20837;&#20197;&#25552;&#39640;&#20869;&#23384;&#31283;&#23450;&#24615;&#65292;&#24182;&#22312;&#36328;&#27169;&#24577;&#21644;&#36328;&#35821;&#35328;&#30446;&#26631;&#19979;&#36827;&#34892;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
While vision-language pre-trained models (VL-PTMs) have advanced multimodal research in recent years, their mastery in a few languages like English restricts their applicability in broader communities. To this end, there is an increasing interest in developing multilingual VL models via a joint-learning setup, which, however, could be unrealistic due to expensive costs and data availability. In this work, we propose to extend VL-PTMs' language capacity by continual language learning (CLL), where a model needs to update its linguistic knowledge incrementally without suffering from catastrophic forgetting (CF). We begin our study by introducing a model dubbed CLL-CLIP, which builds upon CLIP, a prevailing VL-PTM that has acquired image-English text alignment. Specifically, CLL-CLIP contains an expandable token embedding layer to handle linguistic differences. It solely trains token embeddings to improve memory stability and is optimized under cross-modal and cross-lingual objectives to l
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#29992;&#25143;&#28385;&#24847;&#24230;&#12289;&#20449;&#24687;&#26816;&#32034;&#24615;&#33021;&#25351;&#26631;&#21644;&#28436;&#31034;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#36890;&#36807;&#23545;&#26080;&#32447;&#26032;&#38395;&#25628;&#32034;&#30028;&#38754;&#30340;&#19981;&#21516;&#32467;&#26524;&#21345;&#24067;&#23616;&#30340;&#26377;&#25928;&#24615;&#36827;&#34892;&#23454;&#35777;&#20998;&#26512;&#65292;&#32467;&#26524;&#34920;&#26126;&#28436;&#31034;&#21644;&#34920;&#29616;&#23545;&#20110;&#29992;&#25143;&#28385;&#24847;&#24230;&#30340;&#24433;&#21709;&#23494;&#20999;&#30456;&#20851;&#12290;</title><link>http://arxiv.org/abs/2401.17100</link><description>&lt;p&gt;
&#28436;&#31034;&#21644;&#34920;&#29616;&#23545;&#29992;&#25143;&#28385;&#24847;&#24230;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
The Influence of Presentation and Performance on User Satisfaction. (arXiv:2401.17100v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.17100
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#29992;&#25143;&#28385;&#24847;&#24230;&#12289;&#20449;&#24687;&#26816;&#32034;&#24615;&#33021;&#25351;&#26631;&#21644;&#28436;&#31034;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#36890;&#36807;&#23545;&#26080;&#32447;&#26032;&#38395;&#25628;&#32034;&#30028;&#38754;&#30340;&#19981;&#21516;&#32467;&#26524;&#21345;&#24067;&#23616;&#30340;&#26377;&#25928;&#24615;&#36827;&#34892;&#23454;&#35777;&#20998;&#26512;&#65292;&#32467;&#26524;&#34920;&#26126;&#28436;&#31034;&#21644;&#34920;&#29616;&#23545;&#20110;&#29992;&#25143;&#28385;&#24847;&#24230;&#30340;&#24433;&#21709;&#23494;&#20999;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#30340;&#26377;&#25928;&#24615;&#19981;&#20165;&#20165;&#21462;&#20915;&#20110;&#20854;&#26816;&#32034;&#30456;&#20851;&#32467;&#26524;&#30340;&#33021;&#21147;&#65292;&#36824;&#21462;&#20915;&#20110;&#23427;&#22914;&#20309;&#21521;&#29992;&#25143;&#21576;&#29616;&#36825;&#20123;&#32467;&#26524;&#65307;&#24341;&#20154;&#20837;&#32988;&#30340;&#28436;&#31034;&#36890;&#24120;&#19982;&#22686;&#21152;&#29992;&#25143;&#28385;&#24847;&#24230;&#30456;&#20851;&#12290;&#23613;&#31649;&#29616;&#26377;&#30740;&#31350;&#24050;&#32463;&#25506;&#35752;&#20102;&#29992;&#25143;&#28385;&#24847;&#24230;&#12289;&#20449;&#24687;&#26816;&#32034;&#24615;&#33021;&#25351;&#26631;&#21644;&#28436;&#31034;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#20294;&#36825;&#20123;&#26041;&#38754;&#36890;&#24120;&#26159;&#23396;&#31435;&#22320;&#36827;&#34892;&#30740;&#31350;&#30340;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26088;&#22312;&#25506;&#35752;&#26597;&#35810;&#24615;&#33021;&#12289;&#28436;&#31034;&#21644;&#29992;&#25143;&#28385;&#24847;&#24230;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#20026;&#20102;&#36827;&#34892;&#20998;&#26512;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#20171;&#20110;&#21463;&#35797;&#32452;&#20043;&#38388;&#30340;&#23454;&#39564;&#65292;&#27604;&#36739;&#20102;&#26080;&#32447;&#20869;&#26032;&#38395;&#25628;&#32034;&#30028;&#38754;&#30340;&#21508;&#31181;&#32467;&#26524;&#21345;&#24067;&#23616;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#20174;TREC WaPo 2018&#25910;&#38598;&#20102;&#25968;&#25454;&#65292;&#22312;&#22235;&#20010;&#29305;&#23450;&#20027;&#39064;&#19978;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#22312;&#36825;&#20123;&#20027;&#39064;&#30340;&#27599;&#19968;&#20010;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#20855;&#26377;&#19981;&#21516;nDCG&#20540;&#30340;&#20845;&#20010;&#19981;&#21516;&#26597;&#35810;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#28041;&#21450;164&#21517;&#21442;&#19982;&#32773;&#65292;&#20182;&#20204;&#25509;&#35302;&#21040;&#20102;&#20116;&#31181;&#19981;&#21516;&#30340;&#21253;&#21547;&#32467;&#26524;&#21345;&#30340;&#24067;&#23616;&#65292;&#20363;&#22914;&#8220;&#26631;&#39064;&#8221;
&lt;/p&gt;
&lt;p&gt;
The effectiveness of an IR system is gauged not just by its ability to retrieve relevant results but also by how it presents these results to users; an engaging presentation often correlates with increased user satisfaction. While existing research has delved into the link between user satisfaction, IR performance metrics, and presentation, these aspects have typically been investigated in isolation. Our research aims to bridge this gap by examining the relationship between query performance, presentation and user satisfaction. For our analysis, we conducted a between-subjects experiment comparing the effectiveness of various result card layouts for an ad-hoc news search interface. Drawing data from the TREC WaPo 2018 collection, we centered our study on four specific topics. Within each of these topics, we assessed six distinct queries with varying nDCG values. Our study involved 164 participants who were exposed to one of five distinct layouts containing result cards, such as "title'
&lt;/p&gt;</description></item><item><title>Re3val&#26159;&#19968;&#20010;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#21644;&#37325;&#26032;&#25490;&#21517;&#25216;&#26415;&#36827;&#34892;&#35757;&#32451;&#30340;&#29983;&#25104;&#26816;&#32034;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#21033;&#29992;&#19978;&#19979;&#25991;&#20449;&#24687;&#26469;&#37325;&#26032;&#25490;&#21517;&#26816;&#32034;&#24471;&#21040;&#30340;&#39029;&#38754;&#26631;&#39064;&#65292;&#20197;&#26368;&#22823;&#21270;&#36890;&#36807;&#21463;&#38480;&#35299;&#30721;&#29983;&#25104;&#30340;&#22870;&#21169;&#12290;&#21516;&#26102;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#29983;&#25104;&#38382;&#39064;&#26469;&#20943;&#23567;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#24357;&#21512;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#39046;&#22495;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2401.16979</link><description>&lt;p&gt;
Re3val: &#24378;&#21270;&#21644;&#37325;&#26032;&#25490;&#21517;&#30340;&#29983;&#25104;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Re3val: Reinforced and Reranked Generative Retrieval. (arXiv:2401.16979v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16979
&lt;/p&gt;
&lt;p&gt;
Re3val&#26159;&#19968;&#20010;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#21644;&#37325;&#26032;&#25490;&#21517;&#25216;&#26415;&#36827;&#34892;&#35757;&#32451;&#30340;&#29983;&#25104;&#26816;&#32034;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#21033;&#29992;&#19978;&#19979;&#25991;&#20449;&#24687;&#26469;&#37325;&#26032;&#25490;&#21517;&#26816;&#32034;&#24471;&#21040;&#30340;&#39029;&#38754;&#26631;&#39064;&#65292;&#20197;&#26368;&#22823;&#21270;&#36890;&#36807;&#21463;&#38480;&#35299;&#30721;&#29983;&#25104;&#30340;&#22870;&#21169;&#12290;&#21516;&#26102;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#29983;&#25104;&#38382;&#39064;&#26469;&#20943;&#23567;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#24357;&#21512;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#39046;&#22495;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#26816;&#32034;&#27169;&#22411;&#23558;&#25991;&#26723;&#20013;&#30340;&#20449;&#24687;&#25351;&#38024;&#32534;&#30721;&#20026;&#27169;&#22411;&#21442;&#25968;&#20013;&#30340;&#32034;&#24341;&#12290;&#36825;&#20123;&#27169;&#22411;&#20316;&#20026;&#26356;&#22823;&#30340;&#27969;&#31243;&#30340;&#19968;&#37096;&#20998;&#65292;&#36890;&#36807;&#26816;&#32034;&#30340;&#20449;&#24687;&#26469;&#20026;&#30693;&#35782;&#23494;&#38598;&#22411;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#29983;&#25104;&#26465;&#20214;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#26377;&#20004;&#20010;&#38480;&#21046;&#65306;&#29983;&#25104;&#26816;&#32034;&#27809;&#26377;&#32771;&#34385;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#20854;&#27425;&#65292;&#26816;&#32034;&#26080;&#27861;&#20026;&#19979;&#28216;&#35835;&#32773;&#36827;&#34892;&#35843;&#25972;&#65292;&#22240;&#20026;&#35299;&#30721;&#39029;&#38754;&#26631;&#39064;&#26159;&#19968;&#20010;&#38750;&#21487;&#24494;&#20998;&#30340;&#25805;&#20316;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#32463;&#36807;&#26377;&#38480;&#25968;&#25454;&#35757;&#32451;&#30340;&#29983;&#25104;&#37325;&#26032;&#25490;&#21517;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340; Re3val&#12290;Re3val&#21033;&#29992;&#36890;&#36807;&#23494;&#38598;&#36890;&#36947;&#26816;&#32034;&#33719;&#24471;&#30340;&#19978;&#19979;&#25991;&#23545;&#24050;&#26816;&#32034;&#39029;&#38754;&#26631;&#39064;&#36827;&#34892;&#37325;&#26032;&#25490;&#21517;&#65292;&#24182;&#21033;&#29992;REINFORCE&#31639;&#27861;&#26368;&#22823;&#21270;&#21463;&#38480;&#35299;&#30721;&#29983;&#25104;&#30340;&#22870;&#21169;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20174;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#29983;&#25104;&#38382;&#39064;&#65292;&#20197;&#20943;&#23567;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#24357;&#21512;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#39046;&#22495;&#24046;&#36317;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#20174;&#20013;&#25552;&#21462;&#21644;&#37325;&#26032;&#25490;&#21517;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative retrieval models encode pointers to information in a corpus as an index within the model's parameters. These models serve as part of a larger pipeline, where retrieved information conditions generation for knowledge-intensive NLP tasks. However, we identify two limitations: the generative retrieval does not account for contextual information. Secondly, the retrieval can't be tuned for the downstream readers as decoding the page title is a non-differentiable operation. This paper introduces Re3val, trained with generative reranking and reinforcement learning using limited data. Re3val leverages context acquired via Dense Passage Retrieval to rerank the retrieved page titles and utilizes REINFORCE to maximize rewards generated by constrained decoding. Additionally, we generate questions from our pre-training dataset to mitigate epistemic uncertainty and bridge the domain gap between the pre-training and fine-tuning datasets. Subsequently, we extract and rerank contexts from th
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24314;&#31435;&#20102;&#25968;&#23398;&#20869;&#23481;&#37325;&#29992;&#30340;&#20998;&#31867;&#27861;&#65292;&#24182;&#20998;&#26512;&#20102;&#21117;&#31363;&#26816;&#27979;&#21644;&#25968;&#23398;&#20869;&#23481;&#30456;&#20284;&#24615;&#30340;&#26368;&#20339;&#26041;&#27861;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#30446;&#21069;&#26368;&#20339;&#26041;&#27861;&#22312;&#21117;&#31363;&#21644;&#25968;&#23398;&#20869;&#23481;&#30456;&#20284;&#24615;&#26041;&#38754;&#30340;&#34920;&#29616;&#20381;&#28982;&#19981;&#29702;&#24819;&#12290;&#36825;&#20123;&#21457;&#29616;&#23558;&#20026;&#21117;&#31363;&#26816;&#27979;&#31995;&#32479;&#12289;&#25512;&#33616;&#31995;&#32479;&#21644;&#38382;&#31572;&#31995;&#32479;&#31561;&#39046;&#22495;&#30340;&#30740;&#31350;&#25552;&#20379;&#24110;&#21161;&#12290;</title><link>http://arxiv.org/abs/2401.16969</link><description>&lt;p&gt;
&#25968;&#23398;&#21117;&#31363;&#20998;&#31867;&#27861;
&lt;/p&gt;
&lt;p&gt;
Taxonomy of Mathematical Plagiarism. (arXiv:2401.16969v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16969
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24314;&#31435;&#20102;&#25968;&#23398;&#20869;&#23481;&#37325;&#29992;&#30340;&#20998;&#31867;&#27861;&#65292;&#24182;&#20998;&#26512;&#20102;&#21117;&#31363;&#26816;&#27979;&#21644;&#25968;&#23398;&#20869;&#23481;&#30456;&#20284;&#24615;&#30340;&#26368;&#20339;&#26041;&#27861;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#30446;&#21069;&#26368;&#20339;&#26041;&#27861;&#22312;&#21117;&#31363;&#21644;&#25968;&#23398;&#20869;&#23481;&#30456;&#20284;&#24615;&#26041;&#38754;&#30340;&#34920;&#29616;&#20381;&#28982;&#19981;&#29702;&#24819;&#12290;&#36825;&#20123;&#21457;&#29616;&#23558;&#20026;&#21117;&#31363;&#26816;&#27979;&#31995;&#32479;&#12289;&#25512;&#33616;&#31995;&#32479;&#21644;&#38382;&#31572;&#31995;&#32479;&#31561;&#39046;&#22495;&#30340;&#30740;&#31350;&#25552;&#20379;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21117;&#31363;&#38382;&#39064;&#26159;&#19968;&#20010;&#32039;&#36843;&#30340;&#20851;&#27880;&#28857;&#65292;&#23588;&#20854;&#26159;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#29992;&#24615;&#19979;&#26356;&#20026;&#31361;&#20986;&#12290;&#29616;&#26377;&#30340;&#21117;&#31363;&#26816;&#27979;&#31995;&#32479;&#21487;&#20197;&#21487;&#38752;&#22320;&#25214;&#21040;&#22797;&#21046;&#21644;&#36866;&#24230;&#25913;&#20889;&#30340;&#25991;&#26412;&#65292;&#20294;&#22312;&#25968;&#23398;&#31185;&#23398;&#20013;&#30340;&#24605;&#24819;&#21117;&#31363;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#65292;&#22240;&#20026;&#25968;&#23398;&#31185;&#23398;&#20013;&#20351;&#29992;&#20102;&#20005;&#26684;&#30340;&#25968;&#23398;&#31526;&#21495;&#12290;&#25105;&#20204;&#20570;&#20986;&#20102;&#20004;&#20010;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#21487;&#33021;&#23384;&#22312;&#21117;&#31363;&#30340;122&#20010;&#31185;&#23398;&#25991;&#26723;&#36827;&#34892;&#27880;&#37322;&#65292;&#24314;&#31435;&#20102;&#25968;&#23398;&#20869;&#23481;&#37325;&#29992;&#30340;&#20998;&#31867;&#27861;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23545;&#21018;&#21018;&#24314;&#31435;&#30340;&#20998;&#31867;&#27861;&#19978;&#26368;&#20339;&#34920;&#29616;&#30340;&#21117;&#31363;&#26816;&#27979;&#26041;&#27861;&#21644;&#25968;&#23398;&#20869;&#23481;&#30456;&#20284;&#24615;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23545;&#20110;&#21117;&#31363;&#21644;&#25968;&#23398;&#20869;&#23481;&#30456;&#20284;&#24615;&#65292;&#34920;&#29616;&#26368;&#20339;&#30340;&#26041;&#27861;&#20998;&#21035;&#36798;&#21040;&#20102;0.06&#21644;0.16&#30340;&#25972;&#20307;&#26816;&#27979;&#20998;&#25968;&#65288;PlagDet&#65289;&#12290;&#36825;&#20123;&#26368;&#20339;&#26041;&#27861;&#26410;&#33021;&#26816;&#27979;&#20986;&#19971;&#31181;&#26032;&#24314;&#31435;&#30340;&#25968;&#23398;&#30456;&#20284;&#24615;&#31867;&#22411;&#20013;&#30340;&#22823;&#37096;&#20998;&#26696;&#20363;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#23558;&#26377;&#21161;&#20110;&#21117;&#31363;&#26816;&#27979;&#31995;&#32479;&#12289;&#25512;&#33616;&#31995;&#32479;&#12289;&#38382;&#31572;&#31995;&#32479;&#21644;&#20854;&#20182;&#30456;&#20851;&#30740;&#31350;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Plagiarism is a pressing concern, even more so with the availability of large language models. Existing plagiarism detection systems reliably find copied and moderately reworded text but fail for idea plagiarism, especially in mathematical science, which heavily uses formal mathematical notation. We make two contributions. First, we establish a taxonomy of mathematical content reuse by annotating potentially plagiarised 122 scientific document pairs. Second, we analyze the best-performing approaches to detect plagiarism and mathematical content similarity on the newly established taxonomy. We found that the best-performing methods for plagiarism and math content similarity achieve an overall detection score (PlagDet) of 0.06 and 0.16, respectively. The best-performing methods failed to detect most cases from all seven newly established math similarity types. Outlined contributions will benefit research in plagiarism detection systems, recommender systems, question-answering systems, an
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35780;&#20272;&#20102;&#22235;&#31181;&#20808;&#36827;&#30340;&#25991;&#26412;&#26816;&#27979;&#22120;&#23545;LLM&#36741;&#21161;&#20889;&#20316;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#23427;&#20204;&#30340;&#24615;&#33021;&#19981;&#22914;&#19968;&#20010;&#31616;&#21333;&#30340;&#26816;&#27979;&#22120;&#12290;&#30740;&#31350;&#35748;&#20026;&#38656;&#35201;&#24320;&#21457;&#19987;&#38376;&#29992;&#20110;LLM&#36741;&#21161;&#20889;&#20316;&#30340;&#29305;&#23450;&#26816;&#27979;&#22120;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#25215;&#35748;&#23454;&#36341;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.16807</link><description>&lt;p&gt;
&#22312;&#31185;&#23398;&#20132;&#27969;&#20013;&#26816;&#27979;LLM&#36741;&#21161;&#20889;&#20316;&#65306;&#25105;&#20204;&#24050;&#32463;&#21040;&#36798;&#20102;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Detecting LLM-Assisted Writing in Scientific Communication: Are We There Yet?. (arXiv:2401.16807v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16807
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35780;&#20272;&#20102;&#22235;&#31181;&#20808;&#36827;&#30340;&#25991;&#26412;&#26816;&#27979;&#22120;&#23545;LLM&#36741;&#21161;&#20889;&#20316;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#23427;&#20204;&#30340;&#24615;&#33021;&#19981;&#22914;&#19968;&#20010;&#31616;&#21333;&#30340;&#26816;&#27979;&#22120;&#12290;&#30740;&#31350;&#35748;&#20026;&#38656;&#35201;&#24320;&#21457;&#19987;&#38376;&#29992;&#20110;LLM&#36741;&#21161;&#20889;&#20316;&#30340;&#29305;&#23450;&#26816;&#27979;&#22120;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#25215;&#35748;&#23454;&#36341;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;ChatGPT&#65292;&#22312;&#25991;&#26412;&#29983;&#25104;&#26041;&#38754;&#20135;&#29983;&#20102;&#37325;&#22823;&#24433;&#21709;&#65292;&#23588;&#20854;&#26159;&#22312;&#20889;&#20316;&#36741;&#21161;&#39046;&#22495;&#12290;&#23613;&#31649;&#20262;&#29702;&#32771;&#34385;&#24378;&#35843;&#20102;&#22312;&#31185;&#23398;&#20132;&#27969;&#20013;&#36879;&#26126;&#22320;&#25215;&#35748;LLM&#30340;&#20351;&#29992;&#30340;&#37325;&#35201;&#24615;&#65292;&#20294;&#30495;&#23454;&#30340;&#25215;&#35748;&#20173;&#28982;&#24456;&#23569;&#35265;&#12290;&#40723;&#21169;&#20934;&#30830;&#25215;&#35748;LLM&#36741;&#21161;&#20889;&#20316;&#30340;&#19968;&#20010;&#28508;&#22312;&#36884;&#24452;&#28041;&#21450;&#20351;&#29992;&#33258;&#21160;&#26816;&#27979;&#22120;&#12290;&#25105;&#20204;&#23545;&#22235;&#20010;&#21069;&#27839;&#30340;LLM&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#22120;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21457;&#29616;&#23427;&#20204;&#30340;&#24615;&#33021;&#19981;&#22914;&#19968;&#20010;&#31616;&#21333;&#30340;&#20020;&#26102;&#26816;&#27979;&#22120;&#65292;&#35813;&#26816;&#27979;&#22120;&#35774;&#35745;&#29992;&#20110;&#35782;&#21035;&#22312;LLM&#22823;&#37327;&#20986;&#29616;&#26102;&#30340;&#31361;&#28982;&#20889;&#20316;&#39118;&#26684;&#21464;&#21270;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#24320;&#21457;&#19987;&#38376;&#29992;&#20110;LLM&#36741;&#21161;&#20889;&#20316;&#26816;&#27979;&#30340;&#19987;&#29992;&#26816;&#27979;&#22120;&#26159;&#24517;&#35201;&#30340;&#12290;&#36825;&#26679;&#30340;&#26816;&#27979;&#22120;&#21487;&#20197;&#22312;&#20419;&#36827;&#23545;LLM&#21442;&#19982;&#31185;&#23398;&#20132;&#27969;&#30340;&#26356;&#30495;&#23454;&#35748;&#21487;&#12289;&#35299;&#20915;&#24403;&#21069;&#25215;&#35748;&#23454;&#36341;&#20013;&#30340;&#25361;&#25112;&#26041;&#38754;&#21457;&#25381;&#20851;&#38190;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), exemplified by ChatGPT, have significantly reshaped text generation, particularly in the realm of writing assistance. While ethical considerations underscore the importance of transparently acknowledging LLM use, especially in scientific communication, genuine acknowledgment remains infrequent. A potential avenue to encourage accurate acknowledging of LLM-assisted writing involves employing automated detectors. Our evaluation of four cutting-edge LLM-generated text detectors reveals their suboptimal performance compared to a simple ad-hoc detector designed to identify abrupt writing style changes around the time of LLM proliferation. We contend that the development of specialized detectors exclusively dedicated to LLM-assisted writing detection is necessary. Such detectors could play a crucial role in fostering more authentic recognition of LLM involvement in scientific communication, addressing the current challenges in acknowledgment practices.
&lt;/p&gt;</description></item><item><title>AutoIE&#26159;&#19968;&#20010;&#33258;&#21160;&#25552;&#21462;&#31185;&#23398;&#25991;&#29486;&#20449;&#24687;&#30340;&#21019;&#26032;&#26694;&#26550;&#65292;&#38598;&#25104;&#20102;&#22810;&#20010;&#20851;&#38190;&#32452;&#20214;&#65292;&#21253;&#25324;PDF&#25991;&#26723;&#24067;&#23616;&#20998;&#26512;&#12289;&#31185;&#23398;&#25991;&#26412;&#21151;&#33021;&#22359;&#35782;&#21035;&#12289;&#20998;&#23376;&#31579;&#21512;&#25104;&#20449;&#24687;&#25552;&#21462;&#21644;&#22312;&#32447;&#23398;&#20064;&#31561;&#65292;&#20855;&#26377;&#39640;&#25928;&#25552;&#21462;&#20851;&#38190;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;&#24212;&#29992;&#20110;&#30707;&#21270;&#39046;&#22495;&#30340;&#23454;&#36341;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.16672</link><description>&lt;p&gt;
AutoIE&#65306;&#19968;&#31181;&#20174;&#31185;&#23398;&#25991;&#29486;&#20013;&#33258;&#21160;&#25552;&#21462;&#20449;&#24687;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
AutoIE: An Automated Framework for Information Extraction from Scientific Literature. (arXiv:2401.16672v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16672
&lt;/p&gt;
&lt;p&gt;
AutoIE&#26159;&#19968;&#20010;&#33258;&#21160;&#25552;&#21462;&#31185;&#23398;&#25991;&#29486;&#20449;&#24687;&#30340;&#21019;&#26032;&#26694;&#26550;&#65292;&#38598;&#25104;&#20102;&#22810;&#20010;&#20851;&#38190;&#32452;&#20214;&#65292;&#21253;&#25324;PDF&#25991;&#26723;&#24067;&#23616;&#20998;&#26512;&#12289;&#31185;&#23398;&#25991;&#26412;&#21151;&#33021;&#22359;&#35782;&#21035;&#12289;&#20998;&#23376;&#31579;&#21512;&#25104;&#20449;&#24687;&#25552;&#21462;&#21644;&#22312;&#32447;&#23398;&#20064;&#31561;&#65292;&#20855;&#26377;&#39640;&#25928;&#25552;&#21462;&#20851;&#38190;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;&#24212;&#29992;&#20110;&#30707;&#21270;&#39046;&#22495;&#30340;&#23454;&#36341;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24555;&#36895;&#21457;&#23637;&#30340;&#31185;&#23398;&#30740;&#31350;&#39046;&#22495;&#20013;&#65292;&#39640;&#25928;&#22320;&#20174;&#22823;&#37327;&#31185;&#23398;&#35770;&#25991;&#20013;&#25552;&#21462;&#20851;&#38190;&#20449;&#24687;&#20173;&#28982;&#26159;&#19968;&#20010;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21019;&#26032;&#26694;&#26550;&#65292;&#26088;&#22312;&#33258;&#21160;&#25552;&#21462;&#31185;&#23398;PDF&#25991;&#26723;&#20013;&#30340;&#37325;&#35201;&#25968;&#25454;&#65292;&#20351;&#30740;&#31350;&#20154;&#21592;&#26356;&#23481;&#26131;&#36776;&#21035;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;AutoIE&#29420;&#29305;&#22320;&#38598;&#25104;&#20102;&#22235;&#20010;&#21019;&#26032;&#32452;&#20214;&#65306;&#65288;1&#65289;&#22522;&#20110;&#22810;&#35821;&#20041;&#29305;&#24449;&#34701;&#21512;&#30340;PDF&#25991;&#26723;&#24067;&#23616;&#20998;&#26512;&#26041;&#27861;&#65307;&#65288;2&#65289;&#31185;&#23398;&#25991;&#26412;&#20013;&#30340;&#39640;&#32423;&#21151;&#33021;&#22359;&#35782;&#21035;&#65307;&#65288;3&#65289;&#19968;&#31181;&#38024;&#23545;&#20998;&#23376;&#31579;&#21512;&#25104;&#30340;&#20449;&#24687;&#25552;&#21462;&#21644;&#30456;&#20851;&#24615;&#30340;&#21327;&#21516;&#25216;&#26415;&#65307;&#65288;4&#65289;&#38024;&#23545;&#20998;&#23376;&#31579;&#25991;&#29486;&#37327;&#36523;&#23450;&#21046;&#30340;&#22312;&#32447;&#23398;&#20064;&#33539;&#24335;&#12290;&#25105;&#20204;&#30340;SBERT&#27169;&#22411;&#22312;CoNLL04&#21644;ADE&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#39640;&#36798;87.19&#21644;89.65&#30340;Marco F1&#20998;&#25968;&#12290;&#27492;&#22806;&#65292;&#23558;AutoIE&#24212;&#29992;&#20110;&#30707;&#21270;&#39046;&#22495;&#30340;&#20998;&#23376;&#31579;&#21512;&#25104;&#23454;&#36341;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#65292;&#36890;&#36807;&#24778;&#20154;&#30340;78%&#30340;...
&lt;/p&gt;
&lt;p&gt;
In the rapidly evolving field of scientific research, efficiently extracting key information from the burgeoning volume of scientific papers remains a formidable challenge. This paper introduces an innovative framework designed to automate the extraction of vital data from scientific PDF documents, enabling researchers to discern future research trajectories more readily. AutoIE uniquely integrates four novel components: (1) A multi-semantic feature fusion-based approach for PDF document layout analysis; (2) Advanced functional block recognition in scientific texts; (3) A synergistic technique for extracting and correlating information on molecular sieve synthesis; (4) An online learning paradigm tailored for molecular sieve literature. Our SBERT model achieves high Marco F1 scores of 87.19 and 89.65 on CoNLL04 and ADE datasets. In addition, a practical application of AutoIE in the petrochemical molecular sieve synthesis domain demonstrates its efficacy, evidenced by an impressive 78\%
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21382;&#21490;&#24863;&#30693;&#30340;&#23545;&#35805;&#24335;&#31264;&#23494;&#26816;&#32034;&#31995;&#32479;&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#21435;&#22122;&#30340;&#26597;&#35810;&#37325;&#26500;&#20197;&#21450;&#26681;&#25454;&#21382;&#21490;&#36718;&#27425;&#30340;&#23454;&#38469;&#24433;&#21709;&#33258;&#21160;&#25366;&#25496;&#30417;&#30563;&#20449;&#21495;&#25913;&#36827;&#20102;&#29616;&#26377;&#30340;&#23545;&#35805;&#24335;&#31264;&#23494;&#26816;&#32034;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.16659</link><description>&lt;p&gt;
&#21382;&#21490;&#24863;&#30693;&#30340;&#23545;&#35805;&#24335;&#31264;&#23494;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
History-Aware Conversational Dense Retrieval. (arXiv:2401.16659v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16659
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21382;&#21490;&#24863;&#30693;&#30340;&#23545;&#35805;&#24335;&#31264;&#23494;&#26816;&#32034;&#31995;&#32479;&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#21435;&#22122;&#30340;&#26597;&#35810;&#37325;&#26500;&#20197;&#21450;&#26681;&#25454;&#21382;&#21490;&#36718;&#27425;&#30340;&#23454;&#38469;&#24433;&#21709;&#33258;&#21160;&#25366;&#25496;&#30417;&#30563;&#20449;&#21495;&#25913;&#36827;&#20102;&#29616;&#26377;&#30340;&#23545;&#35805;&#24335;&#31264;&#23494;&#26816;&#32034;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#25628;&#32034;&#36890;&#36807;&#23454;&#29616;&#29992;&#25143;&#21644;&#31995;&#32479;&#20043;&#38388;&#30340;&#22810;&#36718;&#20132;&#20114;&#65292;&#23454;&#29616;&#20102;&#22797;&#26434;&#20449;&#24687;&#26816;&#32034;&#30340;&#20415;&#21033;&#12290;&#25903;&#25345;&#36825;&#31181;&#20132;&#20114;&#38656;&#35201;&#23545;&#23545;&#35805;&#36755;&#20837;&#26377;&#20840;&#38754;&#30340;&#29702;&#35299;&#65292;&#20197;&#20415;&#26681;&#25454;&#21382;&#21490;&#20449;&#24687;&#21046;&#23450;&#33391;&#22909;&#30340;&#25628;&#32034;&#26597;&#35810;&#12290;&#29305;&#21035;&#26159;&#65292;&#25628;&#32034;&#26597;&#35810;&#24212;&#21253;&#25324;&#26469;&#33258;&#20808;&#21069;&#23545;&#35805;&#22238;&#21512;&#30340;&#30456;&#20851;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#23545;&#35805;&#24335;&#31264;&#23494;&#26816;&#32034;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#23545;&#32463;&#36807;&#31934;&#35843;&#30340;&#39044;&#35757;&#32451;&#19987;&#38376;&#26816;&#32034;&#22120;&#36827;&#34892;&#25972;&#20010;&#23545;&#35805;&#24335;&#25628;&#32034;&#20250;&#35805;&#30340;&#20248;&#21270;&#65292;&#36825;&#21487;&#33021;&#20250;&#21464;&#24471;&#20887;&#38271;&#21644;&#22024;&#26434;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#26041;&#27861;&#21463;&#29616;&#26377;&#25968;&#25454;&#38598;&#20013;&#25163;&#21160;&#30417;&#30563;&#20449;&#21495;&#25968;&#37327;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21382;&#21490;&#24863;&#30693;&#30340;&#23545;&#35805;&#24335;&#31264;&#23494;&#26816;&#32034;(HAConvDR)&#31995;&#32479;&#65292;&#23427;&#32467;&#21512;&#20102;&#20004;&#20010;&#24605;&#24819;&#65306;&#19978;&#19979;&#25991;&#21435;&#22122;&#30340;&#26597;&#35810;&#37325;&#26500;&#21644;&#26681;&#25454;&#21382;&#21490;&#36718;&#27425;&#30340;&#23454;&#38469;&#24433;&#21709;&#36827;&#34892;&#33258;&#21160;&#25366;&#25496;&#30417;&#30563;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conversational search facilitates complex information retrieval by enabling multi-turn interactions between users and the system. Supporting such interactions requires a comprehensive understanding of the conversational inputs to formulate a good search query based on historical information. In particular, the search query should include the relevant information from the previous conversation turns. However, current approaches for conversational dense retrieval primarily rely on fine-tuning a pre-trained ad-hoc retriever using the whole conversational search session, which can be lengthy and noisy. Moreover, existing approaches are limited by the amount of manual supervision signals in the existing datasets. To address the aforementioned issues, we propose a History-Aware Conversational Dense Retrieval (HAConvDR) system, which incorporates two ideas: context-denoised query reformulation and automatic mining of supervision signals based on the actual impact of historical turns. Experime
&lt;/p&gt;</description></item><item><title>FakeClaim&#25968;&#25454;&#38598;&#26159;&#39318;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#29992;&#20110;&#33258;&#21160;&#35782;&#21035;2023&#24180;&#20197;&#33394;&#21015;&#21704;&#39532;&#26031;&#25112;&#20105;&#20013;&#20551;&#26032;&#38395;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20102;&#26469;&#33258;&#19981;&#21516;&#24179;&#21488;&#30340;&#20107;&#23454;&#24615;&#20027;&#24352;&#21644;&#20551;YouTube&#35270;&#39057;&#12290;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#35813;&#30740;&#31350;&#23637;&#31034;&#20102;&#20351;&#29992;&#29992;&#25143;&#35780;&#35770;&#21487;&#20197;&#24110;&#21161;&#36767;&#35875;&#20551;&#35270;&#39057;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.16625</link><description>&lt;p&gt;
FakeClaim: 2023&#24180;&#20197;&#33394;&#21015;&#21704;&#39532;&#26031;&#25112;&#20105;&#20013;&#30340;&#20551;&#26032;&#38395;&#35782;&#21035;&#30340;&#22810;&#24179;&#21488;&#39537;&#21160;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
FakeClaim: A Multiple Platform-driven Dataset for Identification of Fake News on 2023 Israel-Hamas War. (arXiv:2401.16625v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16625
&lt;/p&gt;
&lt;p&gt;
FakeClaim&#25968;&#25454;&#38598;&#26159;&#39318;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#29992;&#20110;&#33258;&#21160;&#35782;&#21035;2023&#24180;&#20197;&#33394;&#21015;&#21704;&#39532;&#26031;&#25112;&#20105;&#20013;&#20551;&#26032;&#38395;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20102;&#26469;&#33258;&#19981;&#21516;&#24179;&#21488;&#30340;&#20107;&#23454;&#24615;&#20027;&#24352;&#21644;&#20551;YouTube&#35270;&#39057;&#12290;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#35813;&#30740;&#31350;&#23637;&#31034;&#20102;&#20351;&#29992;&#29992;&#25143;&#35780;&#35770;&#21487;&#20197;&#24110;&#21161;&#36767;&#35875;&#20551;&#35270;&#39057;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20379;&#20102;&#39318;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#26469;&#33258;&#19981;&#21516;&#24179;&#21488;&#30340;&#20107;&#23454;&#24615;&#20027;&#24352;&#21644;2023&#24180;&#20197;&#33394;&#21015;&#21704;&#39532;&#26031;&#25112;&#20105;&#30340;&#20551;YouTube&#35270;&#39057;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#33258;&#21160;&#35782;&#21035;&#20551;YouTube&#35270;&#39057;&#12290;FakeClaim&#25968;&#25454;&#38598;&#26159;&#20174;60&#20010;&#20107;&#23454;&#26680;&#26597;&#26426;&#26500;&#12289;30&#31181;&#35821;&#35328;&#20013;&#25910;&#38598;&#30340;&#65292;&#24182;&#36890;&#36807;&#35757;&#32451;&#26377;&#32032;&#30340;&#20107;&#23454;&#26680;&#26597;&#19987;&#19994;&#35760;&#32773;&#32500;&#25252;&#30340;&#20107;&#23454;&#26680;&#26597;&#26426;&#26500;&#20803;&#25968;&#25454;&#36827;&#34892;&#20016;&#23500;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#25991;&#26412;&#20449;&#24687;&#21644;&#29992;&#25143;&#35780;&#35770;&#23545;YouTube&#35270;&#39057;&#23376;&#38598;&#20013;&#30340;&#20551;&#35270;&#39057;&#36827;&#34892;&#20998;&#31867;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#26469;&#20351;&#29992;&#19981;&#21516;&#30340;&#29305;&#24449;&#32452;&#21512;&#23545;&#27599;&#20010;&#35270;&#39057;&#36827;&#34892;&#20998;&#31867;&#12290;&#25105;&#20204;&#34920;&#29616;&#26368;&#20339;&#30340;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#65292;&#21363;Universal Sentence Encoder (USE)&#65292;&#36798;&#21040;&#20102;87\%&#30340;&#23439;F1&#65292;&#36825;&#34920;&#26126;&#35757;&#32451;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#29992;&#25143;&#35752;&#35770;&#20013;&#30340;&#35780;&#35770;&#24110;&#21161;&#25581;&#31359;&#20551;&#35270;&#39057;&#12290;&#35813;&#25968;&#25454;&#38598;&#21487;&#22312;Github&#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
We contribute the first publicly available dataset of factual claims from different platforms and fake YouTube videos on the 2023 Israel-Hamas war for automatic fake YouTube video classification. The FakeClaim data is collected from 60 fact-checking organizations in 30 languages and enriched with metadata from the fact-checking organizations curated by trained journalists specialized in fact-checking. Further, we classify fake videos within the subset of YouTube videos using textual information and user comments. We used a pre-trained model to classify each video with different feature combinations. Our best-performing fine-tuned language model, Universal Sentence Encoder (USE), achieves a Macro F1 of 87\%, which shows that the trained model can be helpful for debunking fake videos using the comments from the user discussion. The dataset is available on Github\footnote{https://github.com/Gautamshahi/FakeClaim}
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#29992;&#25143;&#23545;&#25628;&#32034;&#32467;&#26524;&#35299;&#37322;&#30340;&#38656;&#27714;&#65292;&#21457;&#29616;&#29992;&#25143;&#19981;&#24635;&#26159;&#23547;&#27714;&#35299;&#37322;&#65292;&#20294;&#23545;&#20110;&#22797;&#26434;&#21644;&#20851;&#38190;&#30340;&#20219;&#21153;&#65292;&#35299;&#37322;&#26159;&#26377;&#30410;&#22788;&#30340;&#12290;&#35813;&#30740;&#31350;&#36824;&#24635;&#32467;&#20102;&#26377;&#30410;&#30340;&#35299;&#37322;&#30340;&#20851;&#38190;&#29305;&#24449;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#25628;&#32034;&#24341;&#25806;&#21644;&#35299;&#37322;&#30340;&#35774;&#35745;&#24314;&#35758;&#65292;&#20197;&#24110;&#21161;&#29992;&#25143;&#26356;&#22909;&#22320;&#35780;&#20272;&#25628;&#32034;&#32467;&#26524;&#24182;&#25552;&#21319;&#20182;&#20204;&#30340;&#25628;&#32034;&#20307;&#39564;&#12290;</title><link>http://arxiv.org/abs/2401.16509</link><description>&lt;p&gt;
&#25581;&#31034;&#29992;&#25143;&#23545;&#25628;&#32034;&#32467;&#26524;&#35299;&#37322;&#30340;&#38656;&#27714;
&lt;/p&gt;
&lt;p&gt;
Dissecting users' needs for search result explanations. (arXiv:2401.16509v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16509
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#29992;&#25143;&#23545;&#25628;&#32034;&#32467;&#26524;&#35299;&#37322;&#30340;&#38656;&#27714;&#65292;&#21457;&#29616;&#29992;&#25143;&#19981;&#24635;&#26159;&#23547;&#27714;&#35299;&#37322;&#65292;&#20294;&#23545;&#20110;&#22797;&#26434;&#21644;&#20851;&#38190;&#30340;&#20219;&#21153;&#65292;&#35299;&#37322;&#26159;&#26377;&#30410;&#22788;&#30340;&#12290;&#35813;&#30740;&#31350;&#36824;&#24635;&#32467;&#20102;&#26377;&#30410;&#30340;&#35299;&#37322;&#30340;&#20851;&#38190;&#29305;&#24449;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#25628;&#32034;&#24341;&#25806;&#21644;&#35299;&#37322;&#30340;&#35774;&#35745;&#24314;&#35758;&#65292;&#20197;&#24110;&#21161;&#29992;&#25143;&#26356;&#22909;&#22320;&#35780;&#20272;&#25628;&#32034;&#32467;&#26524;&#24182;&#25552;&#21319;&#20182;&#20204;&#30340;&#25628;&#32034;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#25628;&#32034;&#24341;&#25806;&#22914;&#20309;&#35299;&#37322;&#25628;&#32034;&#32467;&#26524;&#30340;&#38382;&#39064;&#65292;&#24050;&#26377;&#30740;&#31350;&#20551;&#35774;&#35299;&#37322;&#20250;&#26377;&#30410;&#22788;&#65292;&#24182;&#24341;&#20837;&#20102;&#25628;&#32034;&#32467;&#26524;&#35299;&#37322;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#20174;&#26681;&#26412;&#19978;&#32771;&#34385;&#20102;&#25628;&#32034;&#35299;&#37322;&#26159;&#21542;&#38656;&#35201;&#20197;&#21450;&#20309;&#26102;&#20250;&#26377;&#30410;&#22788;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#26377;&#30410;&#30340;&#35299;&#37322;&#30340;&#20851;&#38190;&#29305;&#24449;&#65292;&#24182;&#20998;&#20139;&#20102;&#29992;&#25143;&#23545;Google&#21644;Bing&#25552;&#20379;&#30340;&#35299;&#37322;&#21151;&#33021;&#30340;&#30475;&#27861;&#12290;&#23545;&#38750;&#25216;&#26415;&#20154;&#21592;&#30340;&#35775;&#35848;&#26174;&#31034;&#65292;&#29992;&#25143;&#24182;&#19981;&#24635;&#26159;&#23547;&#27714;&#25110;&#29702;&#35299;&#25628;&#32034;&#35299;&#37322;&#65292;&#20294;&#23545;&#20110;&#22797;&#26434;&#21644;&#20851;&#38190;&#30340;&#20219;&#21153;&#65292;&#20182;&#20204;&#35273;&#24471;&#35299;&#37322;&#26377;&#24110;&#21161;&#12290;&#20182;&#20204;&#35748;&#20026;Google&#30340;&#25628;&#32034;&#35299;&#37322;&#22826;&#26126;&#26174;&#65292;&#20294;&#36190;&#36175;&#33021;&#22815;&#36136;&#30097;&#25628;&#32034;&#32467;&#26524;&#30340;&#33021;&#21147;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#35774;&#35745;&#24314;&#35758;&#65292;&#20197;&#24110;&#21161;&#29992;&#25143;&#26356;&#22909;&#22320;&#35780;&#20272;&#25628;&#32034;&#32467;&#26524;&#24182;&#25552;&#21319;&#20182;&#20204;&#30340;&#25628;&#32034;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is a growing demand for transparency in search engines to understand how search results are curated and to enhance users' trust. Prior research has introduced search result explanations with a focus on how to explain, assuming explanations are beneficial. Our study takes a step back to examine if search explanations are needed and when they are likely to provide benefits. Additionally, we summarize key characteristics of helpful explanations and share users' perspectives on explanation features provided by Google and Bing. Interviews with non-technical individuals reveal that users do not always seek or understand search explanations and mostly desire them for complex and critical tasks. They find Google's search explanations too obvious but appreciate the ability to contest search results. Based on our findings, we offer design recommendations for search engines and explanations to help users better evaluate search results and enhance their search experience.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#36951;&#25022;&#30340;&#24191;&#21578;&#29260;&#24191;&#21578;&#26102;&#27573;&#20998;&#37197;&#26041;&#27861;&#65292;&#26088;&#22312;&#26368;&#22823;&#21270;&#24191;&#21578;&#21830;&#19982;&#39038;&#23458;&#20043;&#38388;&#30340;&#24433;&#21709;&#21147;&#65292;&#24182;&#36890;&#36807;&#22235;&#31181;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#20943;&#23569;&#36951;&#25022;&#30340;&#21457;&#29983;&#12290;</title><link>http://arxiv.org/abs/2401.16464</link><description>&lt;p&gt;
&#23454;&#29616;&#26080;&#36951;&#25022;&#30340;&#24191;&#21578;&#29260;&#24191;&#21578;&#26102;&#27573;&#20998;&#37197;
&lt;/p&gt;
&lt;p&gt;
Towards Regret Free Slot Allocation in Billboard Advertisement. (arXiv:2401.16464v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16464
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#36951;&#25022;&#30340;&#24191;&#21578;&#29260;&#24191;&#21578;&#26102;&#27573;&#20998;&#37197;&#26041;&#27861;&#65292;&#26088;&#22312;&#26368;&#22823;&#21270;&#24191;&#21578;&#21830;&#19982;&#39038;&#23458;&#20043;&#38388;&#30340;&#24433;&#21709;&#21147;&#65292;&#24182;&#36890;&#36807;&#22235;&#31181;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#20943;&#23569;&#36951;&#25022;&#30340;&#21457;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24191;&#21578;&#29260;&#24191;&#21578;&#20013;&#65292;&#20026;&#20102;&#21019;&#36896;&#21644;&#26368;&#22823;&#21270;&#23545;&#39038;&#23458;&#30340;&#24433;&#21709;&#21147;&#65292;&#24191;&#21578;&#21830;&#38656;&#35201;&#25214;&#21040;&#20855;&#26377;&#19968;&#23450;&#24433;&#21709;&#21147;&#30340;&#20154;&#25552;&#20379;&#19968;&#23450;&#25968;&#37327;&#30340;&#24191;&#21578;&#25773;&#25918;&#65292;&#24182;&#22522;&#20110;&#35266;&#30475;&#25968;&#37327;&#25910;&#36153;&#12290;&#26412;&#25991;&#38024;&#23545;&#24191;&#21578;&#25773;&#25918;&#32773;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#35813;&#38382;&#39064;&#30340;&#31163;&#25955;&#20248;&#21270;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#22235;&#31181;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#20998;&#26512;&#20102;&#23427;&#20204;&#30340;&#26102;&#38388;&#21644;&#31354;&#38388;&#22797;&#26434;&#24615;&#12290;&#26368;&#32456;&#30446;&#26631;&#26159;&#26368;&#23567;&#21270;&#20915;&#31574;&#24102;&#26469;&#30340;&#25439;&#22833;&#65288;&#36951;&#25022;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Creating and maximizing influence among the customers is one of the central goals of an advertiser, and hence, remains an active area of research in recent times. In this advertisement technique, the advertisers approach an influence provider for a specific number of views of their content on a payment basis. Now, if the influence provider can provide the required number of views or more, he will receive the full, else a partial payment. In the context of an influence provider, it is a loss for him if he offers more or less views. This is formalized as 'Regret', and naturally, in the context of the influence provider, the goal will be to minimize this quantity. In this paper, we solve this problem in the context of billboard advertisement and pose it as a discrete optimization problem. We propose four efficient solution approaches for this problem and analyze them to understand their time and space complexity. We implement all the solution methodologies with real-life datasets and comp
&lt;/p&gt;</description></item><item><title>KAUCUS&#24341;&#20837;&#20102;&#30693;&#35782;&#22686;&#24378;&#29992;&#25143;&#27169;&#25311;&#22120;&#26694;&#26550;&#65292;&#21487;&#20197;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#27169;&#25311;&#22120;&#21161;&#25163;&#20132;&#20114;&#65292;&#24182;&#33021;&#22815;&#24555;&#36895;&#24341;&#20837;&#22806;&#37096;&#30693;&#35782;&#65292;&#20174;&#32780;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#21161;&#25163;&#30340;&#35757;&#32451;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.16454</link><description>&lt;p&gt;
KAUCUS: &#30693;&#35782;&#22686;&#24378;&#29992;&#25143;&#27169;&#25311;&#22120;&#29992;&#20110;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21161;&#25163;
&lt;/p&gt;
&lt;p&gt;
KAUCUS: Knowledge Augmented User Simulators for Training Language Model Assistants. (arXiv:2401.16454v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16454
&lt;/p&gt;
&lt;p&gt;
KAUCUS&#24341;&#20837;&#20102;&#30693;&#35782;&#22686;&#24378;&#29992;&#25143;&#27169;&#25311;&#22120;&#26694;&#26550;&#65292;&#21487;&#20197;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#27169;&#25311;&#22120;&#21161;&#25163;&#20132;&#20114;&#65292;&#24182;&#33021;&#22815;&#24555;&#36895;&#24341;&#20837;&#22806;&#37096;&#30693;&#35782;&#65292;&#20174;&#32780;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#21161;&#25163;&#30340;&#35757;&#32451;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21019;&#24314;&#19968;&#20010;&#33021;&#22815;&#29983;&#25104;&#26377;&#29992;&#20132;&#20114;&#25968;&#25454;&#30340;&#27169;&#25311;&#22120;&#65292;&#21487;&#20197;&#24320;&#21457;&#20986;&#19968;&#20010;&#26377;&#25928;&#30340;&#22810;&#36718;&#25351;&#20196;&#36319;&#38543;&#21161;&#25163;&#12290;&#29702;&#24819;&#30340;&#29992;&#25143;&#27169;&#25311;&#22120;&#38500;&#20102;&#20381;&#38752;&#20854;&#20869;&#22312;&#26435;&#37325;&#22806;&#65292;&#36824;&#24212;&#33021;&#22815;&#24555;&#36895;&#24341;&#20837;&#22806;&#37096;&#30693;&#35782;&#26469;&#27169;&#25311;&#20114;&#32852;&#32593;&#19978;&#22810;&#26679;&#21270;&#30340;&#25991;&#26412;&#12290;&#20197;&#24448;&#30340;&#29992;&#25143;&#27169;&#25311;&#22120;&#36890;&#24120;&#32570;&#20047;&#22810;&#26679;&#24615;&#65292;&#20027;&#35201;&#26159;&#23553;&#38381;&#39046;&#22495;&#30340;&#65292;&#24182;&#19988;&#38656;&#35201;&#20005;&#26684;&#30340;&#27169;&#24335;&#65292;&#20351;&#24471;&#23427;&#20204;&#26080;&#27861;&#24555;&#36895;&#25193;&#23637;&#20197;&#34701;&#20837;&#22806;&#37096;&#30693;&#35782;&#12290;&#22312;&#36825;&#26041;&#38754;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;Kaucus&#30340;&#30693;&#35782;&#22686;&#24378;&#29992;&#25143;&#27169;&#25311;&#22120;&#26694;&#26550;&#65292;&#20197;&#27010;&#36848;&#21019;&#24314;&#22810;&#26679;&#21270;&#29992;&#25143;&#27169;&#25311;&#22120;&#30340;&#36807;&#31243;&#65292;&#24182;&#33021;&#22815;&#26080;&#32541;&#22320;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#65292;&#21516;&#26102;&#21463;&#30410;&#20110;&#19979;&#28216;&#21161;&#25163;&#27169;&#22411;&#30340;&#35757;&#32451;&#12290;&#36890;&#36807;&#20004;&#20010;&#22522;&#20110;GPT-J&#30340;&#27169;&#25311;&#22120;&#65292;&#21363;&#26816;&#32034;&#22686;&#24378;&#27169;&#25311;&#22120;&#21644;&#25688;&#35201;&#25511;&#21046;&#27169;&#25311;&#22120;&#65292;&#25105;&#20204;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#27169;&#25311;&#22120;&#21161;&#25163;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;
An effective multi-turn instruction-following assistant can be developed by creating a simulator that can generate useful interaction data. Apart from relying on its intrinsic weights, an ideal user simulator should also be able to bootstrap external knowledge rapidly in its raw form to simulate the multifarious diversity of text available over the internet. Previous user simulators generally lacked diversity, were mostly closed domain, and necessitated rigid schema making them inefficient to rapidly scale to incorporate external knowledge. In this regard, we introduce, Kaucus, a Knowledge-Augmented User Simulator framework, to outline a process of creating diverse user simulators, that can seamlessly exploit external knowledge as well as benefit downstream assistant model training. Through two GPT-J based simulators viz., a Retrieval Augmented Simulator and a Summary Controlled Simulator we generate diverse simulator-assistant interactions. Through reward and preference model-based ev
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#31070;&#32463;&#27169;&#24335;&#20851;&#32852;&#22120;&#65288;NPA&#65289;&#30340;&#28145;&#24230;&#21830;&#21697;&#20851;&#32852;&#25366;&#25496;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#26126;&#30830;&#22320;&#24314;&#27169;&#36141;&#29289;&#36807;&#31243;&#20013;&#30340;&#22797;&#26434;&#29992;&#25143;&#34892;&#20026;&#65292;&#24182;&#36890;&#36807;&#27880;&#24847;&#21147;&#39537;&#21160;&#30340;&#26597;&#25214;&#26469;&#35782;&#21035;&#29992;&#25143;&#30340;&#36141;&#29289;&#24847;&#22270;&#12290;</title><link>http://arxiv.org/abs/2401.16433</link><description>&lt;p&gt;
&#36890;&#36807;&#31070;&#32463;&#27169;&#24335;&#20851;&#32852;&#22120;&#36827;&#34892;&#31726;&#20869;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Within-basket Recommendation via Neural Pattern Associator. (arXiv:2401.16433v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16433
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#31070;&#32463;&#27169;&#24335;&#20851;&#32852;&#22120;&#65288;NPA&#65289;&#30340;&#28145;&#24230;&#21830;&#21697;&#20851;&#32852;&#25366;&#25496;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#26126;&#30830;&#22320;&#24314;&#27169;&#36141;&#29289;&#36807;&#31243;&#20013;&#30340;&#22797;&#26434;&#29992;&#25143;&#34892;&#20026;&#65292;&#24182;&#36890;&#36807;&#27880;&#24847;&#21147;&#39537;&#21160;&#30340;&#26597;&#25214;&#26469;&#35782;&#21035;&#29992;&#25143;&#30340;&#36141;&#29289;&#24847;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31726;&#20869;&#25512;&#33616;&#65288;WBR&#65289;&#26159;&#25351;&#22312;&#36141;&#29289;&#36807;&#31243;&#20013;&#20026;&#20102;&#23436;&#25104;&#19968;&#20010;&#38750;&#31354;&#36141;&#29289;&#31726;&#32780;&#25512;&#33616;&#21830;&#21697;&#30340;&#20219;&#21153;&#12290;&#23613;&#31649;&#36825;&#20010;&#39046;&#22495;&#30340;&#26368;&#26032;&#21019;&#26032;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#20294;&#23427;&#20204;&#24120;&#24120;&#24573;&#35270;&#20102;&#23454;&#38469;&#29992;&#25143;&#34892;&#20026;&#30340;&#22797;&#26434;&#24615;&#65292;&#27604;&#22914;1&#65289;&#22810;&#20010;&#36141;&#29289;&#24847;&#22270;&#30340;&#20849;&#23384;&#65292;2&#65289;&#36825;&#20123;&#24847;&#22270;&#30340;&#22810;&#31890;&#24230;&#21644;3&#65289;&#36141;&#29289;&#36807;&#31243;&#20013;&#30340;&#20132;&#32455;&#34892;&#20026;&#65288;&#20999;&#25442;&#24847;&#22270;&#65289;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#31070;&#32463;&#27169;&#24335;&#20851;&#32852;&#22120;&#65288;NPA&#65289;&#30340;&#28145;&#24230;&#21830;&#21697;&#20851;&#32852;&#25366;&#25496;&#27169;&#22411;&#65292;&#26126;&#30830;&#22320;&#24314;&#27169;&#20102;&#19978;&#36848;&#22240;&#32032;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#21463;&#21040;&#21521;&#37327;&#37327;&#21270;&#30340;&#21551;&#21457;&#65292;NPA&#27169;&#22411;&#23398;&#20064;&#23558;&#24120;&#35265;&#30340;&#29992;&#25143;&#24847;&#22270;&#65288;&#25110;&#21830;&#21697;&#32452;&#21512;&#27169;&#24335;&#65289;&#32534;&#30721;&#20026;&#37327;&#21270;&#34920;&#31034;&#65288;&#20063;&#31216;&#20026;&#30721;&#26412;&#65289;&#65292;&#36825;&#20801;&#35768;&#22312;&#25512;&#29702;&#38454;&#27573;&#36890;&#36807;&#27880;&#24847;&#21147;&#39537;&#21160;&#30340;&#26597;&#25214;&#26469;&#35782;&#21035;&#29992;&#25143;&#30340;&#36141;&#29289;&#24847;&#22270;&#12290;&#36825;&#26679;&#20135;&#29983;&#30340;&#25512;&#33616;&#32467;&#26524;&#36830;&#36143;&#19988;&#33258;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Within-basket recommendation (WBR) refers to the task of recommending items to the end of completing a non-empty shopping basket during a shopping session. While the latest innovations in this space demonstrate remarkable performance improvement on benchmark datasets, they often overlook the complexity of user behaviors in practice, such as 1) co-existence of multiple shopping intentions, 2) multi-granularity of such intentions, and 3) interleaving behavior (switching intentions) in a shopping session. This paper presents Neural Pattern Associator (NPA), a deep item-association-mining model that explicitly models the aforementioned factors. Specifically, inspired by vector quantization, the NPA model learns to encode common user intentions (or item-combination patterns) as quantized representations (a.k.a. codebook), which permits identification of users's shopping intentions via attention-driven lookup during the reasoning phase. This yields coherent and self-interpretable recommendat
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#25913;&#36827;&#20102;&#22312;&#32447;&#24191;&#21578;&#31995;&#32479;&#20013;&#30340;&#36716;&#21270;&#29575;&#39044;&#27979;&#12290;&#30001;&#20110;&#25968;&#25454;&#31232;&#30095;&#24615;&#30340;&#25361;&#25112;&#65292;&#28155;&#21152;&#38750;&#28857;&#20987;&#24402;&#22240;&#30340;&#36716;&#21270;&#20250;&#25439;&#22351;&#27169;&#22411;&#30340;&#26657;&#20934;&#65292;&#32780;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#33021;&#22815;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.16432</link><description>&lt;p&gt;
&#22312;&#22312;&#32447;&#24191;&#21578;&#20013;&#36890;&#36807;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#25913;&#36827;&#36716;&#21270;&#29575;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Improving conversion rate prediction via self-supervised pre-training in online advertising. (arXiv:2401.16432v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16432
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#25913;&#36827;&#20102;&#22312;&#32447;&#24191;&#21578;&#31995;&#32479;&#20013;&#30340;&#36716;&#21270;&#29575;&#39044;&#27979;&#12290;&#30001;&#20110;&#25968;&#25454;&#31232;&#30095;&#24615;&#30340;&#25361;&#25112;&#65292;&#28155;&#21152;&#38750;&#28857;&#20987;&#24402;&#22240;&#30340;&#36716;&#21270;&#20250;&#25439;&#22351;&#27169;&#22411;&#30340;&#26657;&#20934;&#65292;&#32780;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#33021;&#22815;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#36716;&#21270;&#29575;&#26159;&#22312;&#32447;&#24191;&#21578;&#31995;&#32479;&#20013;&#20248;&#21270;&#25237;&#26631;&#20197;&#28385;&#36275;&#24191;&#21578;&#20027;&#24615;&#33021;&#35201;&#27714;&#30340;&#20851;&#38190;&#20219;&#21153;&#12290;&#23613;&#31649;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#23835;&#36215;&#65292;&#20294;&#36825;&#20123;&#39044;&#27979;&#36890;&#24120;&#30001;&#20998;&#35299;&#26426;&#65288;FM&#65289;&#36827;&#34892;&#65292;&#29305;&#21035;&#26159;&#22312;&#25512;&#29702;&#24310;&#36831;&#33267;&#20851;&#37325;&#35201;&#30340;&#21830;&#19994;&#29615;&#22659;&#20013;&#12290;&#36825;&#20123;&#27169;&#22411;&#20351;&#29992;&#36923;&#36753;&#22238;&#24402;&#26694;&#26550;&#35757;&#32451;&#65292;&#21033;&#29992;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#36807;&#21435;&#29992;&#25143;&#27963;&#21160;&#24418;&#25104;&#30340;&#26631;&#35760;&#34920;&#26684;&#25968;&#25454;&#12290;&#35768;&#22810;&#24191;&#21578;&#20027;&#21482;&#20851;&#24515;&#34987;&#28857;&#20987;&#23646;&#24615;&#30340;&#36716;&#21270;&#12290;&#39044;&#27979;&#32473;&#23450;&#28857;&#20987;&#30340;&#36716;&#21270;&#27169;&#22411;&#35757;&#32451;&#30340;&#20027;&#35201;&#25361;&#25112;&#26469;&#33258;&#25968;&#25454;&#31232;&#30095;&#24615; - &#28857;&#20987;&#24456;&#23569;&#65292;&#28857;&#20987;&#24402;&#22240;&#30340;&#36716;&#21270;&#26356;&#23569;&#12290;&#28982;&#32780;&#65292;&#22312;&#35757;&#32451;&#38598;&#20013;&#28155;&#21152;&#38750;&#28857;&#20987;&#24402;&#22240;&#30340;&#36716;&#21270;&#26469;&#20943;&#36731;&#31232;&#30095;&#24615;&#20250;&#25439;&#22351;&#27169;&#22411;&#30340;&#26657;&#20934;&#12290;&#30001;&#20110;&#26657;&#20934;&#23545;&#23454;&#29616;&#24191;&#21578;&#20027;&#30446;&#26631;&#33267;&#20851;&#37325;&#35201;&#65292;&#36825;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#30340;&#20247;&#25152;&#21608;&#30693;&#30340;&#24605;&#24819;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The task of predicting conversion rates (CVR) lies at the heart of online advertising systems aiming to optimize bids to meet advertiser performance requirements. Even with the recent rise of deep neural networks, these predictions are often made by factorization machines (FM), especially in commercial settings where inference latency is key. These models are trained using the logistic regression framework on labeled tabular data formed from past user activity that is relevant to the task at hand.  Many advertisers only care about click-attributed conversions. A major challenge in training models that predict conversions-given-clicks comes from data sparsity - clicks are rare, conversions attributed to clicks are even rarer. However, mitigating sparsity by adding conversions that are not click-attributed to the training set impairs model calibration. Since calibration is critical to achieving advertiser goals, this is infeasible.  In this work we use the well-known idea of self-supervi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#38598;&#20449;&#24687;&#26816;&#32034;&#21644;&#25552;&#21462;&#20110;&#19968;&#20307;&#30340;&#24037;&#20855;&#65292;&#24212;&#29992;&#20110;COVID-19 Open Research Dataset (CORD-19)&#12290;&#20027;&#35201;&#30446;&#30340;&#26159;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#19968;&#20010;&#26356;&#22909;&#30340;COVID-19&#30456;&#20851;&#35770;&#25991;&#30340;&#25628;&#32034;&#24037;&#20855;&#65292;&#24110;&#21161;&#20182;&#20204;&#25214;&#21040;&#21442;&#32771;&#35770;&#25991;&#24182;&#31361;&#20986;&#26174;&#31034;&#25991;&#26412;&#20013;&#30340;&#30456;&#20851;&#23454;&#20307;&#12290;</title><link>http://arxiv.org/abs/2401.16430</link><description>&lt;p&gt;
&#19968;&#20010;&#38024;&#23545;Covid-19&#30456;&#20851;&#35770;&#25991;&#30340;&#20449;&#24687;&#26816;&#32034;&#21644;&#25552;&#21462;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
An Information Retrieval and Extraction Tool for Covid-19 Related Papers. (arXiv:2401.16430v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16430
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#38598;&#20449;&#24687;&#26816;&#32034;&#21644;&#25552;&#21462;&#20110;&#19968;&#20307;&#30340;&#24037;&#20855;&#65292;&#24212;&#29992;&#20110;COVID-19 Open Research Dataset (CORD-19)&#12290;&#20027;&#35201;&#30446;&#30340;&#26159;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#19968;&#20010;&#26356;&#22909;&#30340;COVID-19&#30456;&#20851;&#35770;&#25991;&#30340;&#25628;&#32034;&#24037;&#20855;&#65292;&#24110;&#21161;&#20182;&#20204;&#25214;&#21040;&#21442;&#32771;&#35770;&#25991;&#24182;&#31361;&#20986;&#26174;&#31034;&#25991;&#26412;&#20013;&#30340;&#30456;&#20851;&#23454;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#65306;COVID-19&#22823;&#27969;&#34892;&#23545;&#20840;&#29699;&#30340;&#21355;&#29983;&#31995;&#32479;&#36896;&#25104;&#20102;&#20005;&#37325;&#24433;&#21709;&#12290;&#20854;&#20005;&#37325;&#24615;&#20197;&#21450;&#20010;&#20154;&#21644;&#32452;&#32455;&#24320;&#23637;&#23545;&#31574;&#30740;&#31350;&#30340;&#20852;&#36259;&#22686;&#21152;&#65292;&#23548;&#33268;&#31185;&#23398;&#26399;&#21002;&#20013;&#20986;&#29616;&#20102;&#22823;&#37327;&#26032;&#30340;&#30740;&#31350;&#12290;&#30446;&#26631;&#65306;&#25105;&#20204;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#23558;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#21644;&#25552;&#21462;&#65288;IE&#65289;&#30340;&#26041;&#38754;&#24212;&#29992;&#20110;COVID-19 Open Research Dataset&#65288;CORD-19&#65289;&#30340;&#26032;&#39062;&#24037;&#20855;&#12290;&#26412;&#25991;&#30340;&#20027;&#35201;&#37325;&#28857;&#26159;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#19968;&#20010;&#26356;&#22909;&#30340;COVID-19&#30456;&#20851;&#35770;&#25991;&#30340;&#25628;&#32034;&#24037;&#20855;&#65292;&#24110;&#21161;&#20182;&#20204;&#25214;&#21040;&#21442;&#32771;&#35770;&#25991;&#24182;&#31361;&#20986;&#26174;&#31034;&#25991;&#26412;&#20013;&#30340;&#30456;&#20851;&#23454;&#20307;&#12290;&#26041;&#27861;&#65306;&#25105;&#20204;&#24212;&#29992;&#38544;&#21547;&#29380;&#21033;&#20811;&#38647;&#20998;&#37197;&#65288;LDA&#65289;&#26469;&#26681;&#25454;&#30740;&#31350;&#26041;&#38754;&#23545;CORD-19&#20013;&#25152;&#26377;&#33521;&#25991;&#25688;&#35201;&#30340;&#20027;&#39064;&#36827;&#34892;&#24314;&#27169;&#12290;&#25552;&#21462;&#27599;&#20010;&#25688;&#35201;&#30340;&#30456;&#20851;&#21629;&#21517;&#23454;&#20307;&#65292;&#24182;&#23558;&#20854;&#38142;&#25509;&#21040;&#30456;&#24212;&#30340;UMLS&#27010;&#24565;&#12290;&#20351;&#29992;&#27491;&#21017;&#34920;&#36798;&#24335;&#21644;K&#26368;&#36817;&#37051;&#31639;&#27861;&#26469;&#23545;&#30456;&#20851;&#35770;&#25991;&#36827;&#34892;&#25490;&#21517;&#12290;&#32467;&#26524;&#65306;&#25105;&#20204;&#30340;&#24037;&#20855;&#24050;&#32463;&#23454;&#29616;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Background: The COVID-19 pandemic has caused severe impacts on health systems worldwide. Its critical nature and the increased interest of individuals and organizations to develop countermeasures to the problem has led to a surge of new studies in scientific journals. Objetive: We sought to develop a tool that incorporates, in a novel way, aspects of Information Retrieval (IR) and Extraction (IE) applied to the COVID-19 Open Research Dataset (CORD-19). The main focus of this paper is to provide researchers with a better search tool for COVID-19 related papers, helping them find reference papers and hightlight relevant entities in text. Method: We applied Latent Dirichlet Allocation (LDA) to model, based on research aspects, the topics of all English abstracts in CORD-19. Relevant named entities of each abstract were extracted and linked to the corresponding UMLS concept. Regular expressions and the K-Nearest Neighbors algorithm were used to rank relevant papers. Results: Our tool has s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#32467;&#21512;&#20027;&#39064;&#24314;&#27169;&#21644;&#24341;&#29992;&#32593;&#32476;&#20998;&#26512;&#30340;&#26041;&#27861;&#65292;&#29992;&#26469;&#30740;&#31350;&#27431;&#27954;&#20154;&#26435;&#27861;&#38498;&#20851;&#20110;&#23562;&#37325;&#31169;&#23494;&#21644;&#23478;&#24237;&#29983;&#27963;&#30340;&#26696;&#20363;&#27861;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#25214;&#21040;&#21644;&#32452;&#32455;&#20855;&#26377;&#30456;&#20284;&#20027;&#39064;&#21644;&#24341;&#29992;&#27169;&#24335;&#30340;&#26696;&#20363;&#27861;&#65292;&#24182;&#19988;&#36890;&#36807;&#32467;&#21512;&#36825;&#20004;&#31181;&#25216;&#26415;&#33021;&#22815;&#24471;&#21040;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.16429</link><description>&lt;p&gt;
&#32467;&#21512;&#20027;&#39064;&#24314;&#27169;&#21644;&#24341;&#29992;&#32593;&#32476;&#20998;&#26512;&#30740;&#31350;&#27431;&#27954;&#20154;&#26435;&#27861;&#38498;&#20851;&#20110;&#23562;&#37325;&#31169;&#20154;&#21644;&#23478;&#24237;&#29983;&#27963;&#26435;&#21033;&#30340;&#26696;&#20363;&#27861;
&lt;/p&gt;
&lt;p&gt;
Combining topic modelling and citation network analysis to study case law from the European Court on Human Rights on the right to respect for private and family life. (arXiv:2401.16429v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16429
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32467;&#21512;&#20027;&#39064;&#24314;&#27169;&#21644;&#24341;&#29992;&#32593;&#32476;&#20998;&#26512;&#30340;&#26041;&#27861;&#65292;&#29992;&#26469;&#30740;&#31350;&#27431;&#27954;&#20154;&#26435;&#27861;&#38498;&#20851;&#20110;&#23562;&#37325;&#31169;&#23494;&#21644;&#23478;&#24237;&#29983;&#27963;&#30340;&#26696;&#20363;&#27861;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#25214;&#21040;&#21644;&#32452;&#32455;&#20855;&#26377;&#30456;&#20284;&#20027;&#39064;&#21644;&#24341;&#29992;&#27169;&#24335;&#30340;&#26696;&#20363;&#27861;&#65292;&#24182;&#19988;&#36890;&#36807;&#32467;&#21512;&#36825;&#20004;&#31181;&#25216;&#26415;&#33021;&#22815;&#24471;&#21040;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;HUDOC&#31561;&#27861;&#24459;&#26696;&#20363;&#27861;&#25968;&#25454;&#24211;&#30340;&#24555;&#36895;&#22686;&#38271;&#65292;&#20026;&#20102;&#22788;&#29702;&#22914;&#27492;&#22823;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#65292;&#27861;&#24459;&#30740;&#31350;&#20154;&#21592;&#25214;&#21040;&#39640;&#25928;&#30340;&#26041;&#27861;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#31181;&#26696;&#20363;&#27861;&#25968;&#25454;&#24211;&#36890;&#24120;&#21253;&#21547;&#26696;&#20214;&#30340;&#25991;&#26412;&#20869;&#23481;&#20197;&#21450;&#23427;&#20204;&#20043;&#38388;&#30340;&#24341;&#29992;&#12290;&#26412;&#25991;&#37325;&#28857;&#30740;&#31350;&#20102;&#26469;&#33258;&#27431;&#27954;&#20154;&#26435;&#27861;&#38498;&#20851;&#20110;&#27431;&#27954;&#20154;&#26435;&#20844;&#32422;&#31532;8&#26465;&#20851;&#20110;&#23562;&#37325;&#31169;&#20154;&#21644;&#23478;&#24237;&#29983;&#27963;&#12289;&#23478;&#24237;&#21644;&#36890;&#20449;&#26435;&#21033;&#30340;&#26696;&#20363;&#27861;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#28436;&#31034;&#24182;&#27604;&#36739;&#20102;&#20027;&#39064;&#24314;&#27169;&#21644;&#24341;&#29992;&#32593;&#32476;&#22312;&#26681;&#25454;&#19968;&#33324;&#20027;&#39064;&#21644;&#24341;&#29992;&#27169;&#24335;&#25214;&#21040;&#21644;&#32452;&#32455;&#31532;8&#26465;&#26696;&#20363;&#27861;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#32034;&#20102;&#32467;&#21512;&#36825;&#20004;&#31181;&#25216;&#26415;&#26159;&#21542;&#27604;&#20165;&#24212;&#29992;&#20854;&#20013;&#19968;&#31181;&#26041;&#27861;&#25928;&#26524;&#26356;&#22909;&#12290;&#25105;&#20204;&#22312;&#19968;&#32452;&#25163;&#24037;&#25910;&#38598;&#21644;&#27880;&#37322;&#30340;&#20851;&#20110;&#39537;&#36880;&#30340;&#31532;8&#26465;&#26696;&#20363;&#27861;&#29420;&#29305;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#32452;&#21512;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#32467;&#21512;&#20351;&#29992;&#36825;&#20004;&#31181;&#26041;&#27861;&#33021;&#22815;&#21462;&#24471;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
As legal case law databases such as HUDOC continue to grow rapidly, it has become essential for legal researchers to find efficient methods to handle such large-scale data sets. Such case law databases usually consist of the textual content of cases together with the citations between them. This paper focuses on case law from the European Court of Human Rights on Article 8 of the European Convention of Human Rights, the right to respect private and family life, home and correspondence. In this study, we demonstrate and compare the potential of topic modelling and citation network to find and organize case law on Article 8 based on their general themes and citation patterns, respectively. Additionally, we explore whether combining these two techniques leads to better results compared to the application of only one of the methods. We evaluate the effectiveness of the combined method on a unique manually collected and annotated dataset of Aricle 8 case law on evictions. The results of our
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#36890;&#36807;&#27491;&#21017;&#21270;&#25216;&#26415;&#20943;&#36731;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#20301;&#32622;&#20559;&#24046;&#38382;&#39064;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#20854;&#20182;&#29616;&#20195;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.16427</link><description>&lt;p&gt;
&#36890;&#36807;&#27491;&#21017;&#21270;&#20943;&#36731;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#20301;&#32622;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Mitigating Position Bias with Regularization for Recommender Systems. (arXiv:2401.16427v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16427
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#36890;&#36807;&#27491;&#21017;&#21270;&#25216;&#26415;&#20943;&#36731;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#20301;&#32622;&#20559;&#24046;&#38382;&#39064;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#20854;&#20182;&#29616;&#20195;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20844;&#27491;&#24615;&#26159;&#36817;&#24180;&#26469;&#30340;&#30740;&#31350;&#28909;&#28857;&#12290;&#19982;&#20844;&#27491;&#24615;&#23494;&#20999;&#30456;&#20851;&#30340;&#30740;&#31350;&#35838;&#39064;&#26159;&#20559;&#24046;&#21644;&#21435;&#20559;&#24046;&#12290;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#20559;&#24046;&#38382;&#39064;&#20013;&#65292;&#20301;&#32622;&#20559;&#24046;&#26159;&#26368;&#24120;&#36935;&#21040;&#30340;&#19968;&#31181;&#24773;&#20917;&#12290;&#20301;&#32622;&#20559;&#24046;&#24847;&#21619;&#30528;&#25512;&#33616;&#21015;&#34920;&#39030;&#37096;&#30340;&#25512;&#33616;&#39033;&#27604;&#30456;&#21516;&#21015;&#34920;&#24213;&#37096;&#30340;&#39033;&#26356;&#21487;&#33021;&#34987;&#28857;&#20987;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#27491;&#21017;&#21270;&#25216;&#26415;&#20943;&#23569;&#20559;&#24046;&#25928;&#24212;&#12290;&#22312;&#23454;&#39564;&#37096;&#20998;&#65292;&#25105;&#20204;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#20854;&#20182;&#29616;&#20195;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fairness is a popular research topic in recent years. A research topic closely related to fairness is bias and debiasing. Among different types of bias problems, position bias is one of the most widely encountered symptoms. Position bias means that recommended items on top of the recommendation list has a higher likelihood to be clicked than items on bottom of the same list. To mitigate this problem, we propose to use regularization technique to reduce the bias effect. In the experiment section, we prove that our method is superior to other modern algorithms.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20301;&#32622;&#25935;&#24863;&#23884;&#20837;&#65288;LSE&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20851;&#31995;&#29305;&#23450;&#30340;&#26144;&#23556;&#26469;&#20462;&#25913;&#22836;&#23454;&#20307;&#65292;&#23558;&#20851;&#31995;&#27010;&#24565;&#21270;&#20026;&#32447;&#24615;&#21464;&#25442;&#12290;LSE&#22312;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#39046;&#22495;&#20855;&#26377;&#29702;&#35770;&#22522;&#30784;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#26356;&#39640;&#25928;&#30340;&#21464;&#20307;LSEd&#12290;&#23454;&#39564;&#35777;&#26126;LSEd&#22312;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#19978;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.10893</link><description>&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#30340;&#20301;&#32622;&#25935;&#24863;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Location Sensitive Embedding for Knowledge Graph Embedding. (arXiv:2401.10893v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10893
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20301;&#32622;&#25935;&#24863;&#23884;&#20837;&#65288;LSE&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20851;&#31995;&#29305;&#23450;&#30340;&#26144;&#23556;&#26469;&#20462;&#25913;&#22836;&#23454;&#20307;&#65292;&#23558;&#20851;&#31995;&#27010;&#24565;&#21270;&#20026;&#32447;&#24615;&#21464;&#25442;&#12290;LSE&#22312;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#39046;&#22495;&#20855;&#26377;&#29702;&#35770;&#22522;&#30784;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#26356;&#39640;&#25928;&#30340;&#21464;&#20307;LSEd&#12290;&#23454;&#39564;&#35777;&#26126;LSEd&#22312;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#19978;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#23558;&#30693;&#35782;&#22270;&#35889;&#36716;&#21270;&#20026;&#36830;&#32493;&#30340;&#12289;&#20302;&#32500;&#24230;&#30340;&#31354;&#38388;&#65292;&#26377;&#21161;&#20110;&#25512;&#29702;&#21644;&#34917;&#20840;&#20219;&#21153;&#12290;&#35813;&#39046;&#22495;&#20027;&#35201;&#20998;&#20026;&#20256;&#32479;&#30340;&#36317;&#31163;&#27169;&#22411;&#21644;&#35821;&#20041;&#21305;&#37197;&#27169;&#22411;&#12290;&#20256;&#32479;&#30340;&#36317;&#31163;&#27169;&#22411;&#38754;&#20020;&#30340;&#20851;&#38190;&#25361;&#25112;&#26159;&#26080;&#27861;&#26377;&#25928;&#21306;&#20998;&#22270;&#35889;&#20013;&#30340;&#8220;&#22836;&#23454;&#20307;&#8221;&#21644;&#8220;&#23614;&#23454;&#20307;&#8221;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;&#20301;&#32622;&#25935;&#24863;&#23884;&#20837;&#65288;LSE&#65289;&#26041;&#27861;&#12290;LSE&#36890;&#36807;&#20851;&#31995;&#29305;&#23450;&#30340;&#26144;&#23556;&#20462;&#25913;&#22836;&#23454;&#20307;&#65292;&#23558;&#20851;&#31995;&#27010;&#24565;&#21270;&#20026;&#32447;&#24615;&#21464;&#25442;&#32780;&#19981;&#20165;&#20165;&#26159;&#24179;&#31227;&#12290;LSE&#30340;&#29702;&#35770;&#22522;&#30784;&#65292;&#21253;&#25324;&#20854;&#34920;&#31034;&#33021;&#21147;&#21644;&#19982;&#29616;&#26377;&#27169;&#22411;&#30340;&#32852;&#31995;&#65292;&#37117;&#36827;&#34892;&#20102;&#35814;&#32454;&#30740;&#31350;&#12290;&#19968;&#31181;&#26356;&#31616;&#21270;&#30340;&#21464;&#20307;LSEd&#21033;&#29992;&#23545;&#35282;&#30697;&#38453;&#36827;&#34892;&#21464;&#25442;&#20197;&#25552;&#39640;&#23454;&#29992;&#24615;&#33021;&#12290;&#22312;&#23545;&#22235;&#20010;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#36827;&#34892;&#38142;&#25509;&#39044;&#27979;&#30340;&#27979;&#35797;&#20013;&#65292;LSEd&#35201;&#20040;&#34920;&#29616;&#26356;&#22909;&#65292;&#35201;&#20040;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graph embedding transforms knowledge graphs into a continuous, low-dimensional space, facilitating inference and completion tasks. This field is mainly divided into translational distance models and semantic matching models. A key challenge in translational distance models is their inability to effectively differentiate between 'head' and 'tail' entities in graphs. To address this, the novel location-sensitive embedding (LSE) method has been developed. LSE innovatively modifies the head entity using relation-specific mappings, conceptualizing relations as linear transformations rather than mere translations. The theoretical foundations of LSE, including its representational capabilities and its connections to existing models, have been thoroughly examined. A more streamlined variant, LSEd, employs a diagonal matrix for transformations to enhance practical efficiency. In tests conducted on four large-scale datasets for link prediction, LSEd either outperforms or is competitive
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25581;&#31034;&#20102;&#22522;&#20110;ChatGPT&#30340;&#25512;&#33616;&#31995;&#32479;&#30340;&#20559;&#35265;&#38382;&#39064;&#65292;&#24182;&#30740;&#31350;&#20102;&#25552;&#31034;&#35774;&#35745;&#31574;&#30053;&#23545;&#25512;&#33616;&#36136;&#37327;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;RecLLMs&#20013;&#24341;&#20837;&#29305;&#23450;&#30340;&#31995;&#32479;&#35282;&#33394;&#21644;&#25552;&#31034;&#31574;&#30053;&#21487;&#20197;&#22686;&#24378;&#25512;&#33616;&#30340;&#20844;&#24179;&#24615;&#21644;&#22810;&#26679;&#24615;&#65292;&#21516;&#26102;GPT-based&#27169;&#22411;&#20542;&#21521;&#20110;&#25512;&#33616;&#26368;&#26032;&#21644;&#26356;&#22810;&#26679;&#21270;&#30340;&#30005;&#24433;&#27969;&#27966;&#12290;</title><link>http://arxiv.org/abs/2401.10545</link><description>&lt;p&gt;
&#29702;&#35299;ChatGPT&#22522;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#20559;&#35265;&#65306;&#20379;&#24212;&#21830;&#20844;&#24179;&#24615;&#12289;&#26102;&#38388;&#31283;&#23450;&#24615;&#21644;&#26368;&#26032;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Understanding Biases in ChatGPT-based Recommender Systems: Provider Fairness, Temporal Stability, and Recency. (arXiv:2401.10545v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10545
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25581;&#31034;&#20102;&#22522;&#20110;ChatGPT&#30340;&#25512;&#33616;&#31995;&#32479;&#30340;&#20559;&#35265;&#38382;&#39064;&#65292;&#24182;&#30740;&#31350;&#20102;&#25552;&#31034;&#35774;&#35745;&#31574;&#30053;&#23545;&#25512;&#33616;&#36136;&#37327;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;RecLLMs&#20013;&#24341;&#20837;&#29305;&#23450;&#30340;&#31995;&#32479;&#35282;&#33394;&#21644;&#25552;&#31034;&#31574;&#30053;&#21487;&#20197;&#22686;&#24378;&#25512;&#33616;&#30340;&#20844;&#24179;&#24615;&#21644;&#22810;&#26679;&#24615;&#65292;&#21516;&#26102;GPT-based&#27169;&#22411;&#20542;&#21521;&#20110;&#25512;&#33616;&#26368;&#26032;&#21644;&#26356;&#22810;&#26679;&#21270;&#30340;&#30005;&#24433;&#27969;&#27966;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;RecLLMs&#65289;&#30340;&#25512;&#33616;&#31995;&#32479;&#30340;&#32454;&#24494;&#33021;&#21147;&#21644;&#22266;&#26377;&#20559;&#35265;&#65292;&#37325;&#28857;&#30740;&#31350;&#20102;&#22522;&#20110;ChatGPT&#30340;&#31995;&#32479;&#12290;&#30740;&#31350;&#20102;&#29983;&#25104;&#27169;&#22411;&#21644;&#20256;&#32479;&#21327;&#21516;&#36807;&#28388;&#27169;&#22411;&#22312;&#30005;&#24433;&#25512;&#33616;&#20013;&#30340;&#24046;&#24322;&#34892;&#20026;&#12290;&#26412;&#30740;&#31350;&#20027;&#35201;&#35843;&#26597;&#20102;&#25552;&#31034;&#35774;&#35745;&#31574;&#30053;&#21450;&#20854;&#23545;&#25512;&#33616;&#36136;&#37327;&#30340;&#21508;&#20010;&#26041;&#38754;&#65288;&#21253;&#25324;&#20934;&#30830;&#24615;&#12289;&#20379;&#24212;&#21830;&#20844;&#24179;&#24615;&#12289;&#22810;&#26679;&#24615;&#12289;&#31283;&#23450;&#24615;&#12289;&#27969;&#34892;&#31867;&#22411;&#21644;&#26102;&#25928;&#24615;&#65289;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#20998;&#26512;&#34920;&#26126;&#65292;&#22312;RecLLMs&#20013;&#24341;&#20837;&#29305;&#23450;&#30340;&#8220;&#31995;&#32479;&#35282;&#33394;&#8221;&#21644;&#8220;&#25552;&#31034;&#31574;&#30053;&#8221;&#26174;&#33879;&#24433;&#21709;&#20854;&#24615;&#33021;&#12290;&#20363;&#22914;&#65292;&#22522;&#20110;&#35282;&#33394;&#30340;&#25552;&#31034;&#21487;&#20197;&#22686;&#24378;&#25512;&#33616;&#30340;&#20844;&#24179;&#24615;&#21644;&#22810;&#26679;&#24615;&#65292;&#20943;&#36731;&#27969;&#34892;&#20559;&#35265;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#34429;&#28982;&#22522;&#20110;GPT&#30340;&#27169;&#22411;&#24182;&#19981;&#24635;&#26159;&#33021;&#19982;&#20256;&#32479;&#21327;&#21516;&#36807;&#28388;&#22522;&#32447;&#27169;&#22411;&#30340;&#24615;&#33021;&#21305;&#37197;&#65292;&#20294;&#23427;&#20204;&#20542;&#21521;&#20110;&#25512;&#33616;&#26356;&#26032;&#12289;&#26356;&#22810;&#26679;&#21270;&#30340;&#30005;&#24433;&#27969;&#27966;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;GPT-base
&lt;/p&gt;
&lt;p&gt;
This study explores the nuanced capabilities and inherent biases of Recommender Systems using Large Language Models (RecLLMs), with a focus on ChatGPT-based systems. It studies into the contrasting behaviors of generative models and traditional collaborative filtering models in movie recommendations. The research primarily investigates prompt design strategies and their impact on various aspects of recommendation quality, including accuracy, provider fairness, diversity, stability, genre dominance, and temporal freshness (recency).  Our experimental analysis reveals that the introduction of specific 'system roles' and 'prompt strategies' in RecLLMs significantly influences their performance. For instance, role-based prompts enhance fairness and diversity in recommendations, mitigating popularity bias. We find that while GPT-based models do not always match the performance of CF baselines, they exhibit a unique tendency to recommend newer and more diverse movie genres. Notably, GPT-base
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24046;&#20998;&#38544;&#31169;&#39034;&#24207;&#25512;&#33616;&#26694;&#26550;&#65292;&#37319;&#29992;&#20102;&#22122;&#22768;&#22270;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#24046;&#20998;&#38544;&#31169;&#25512;&#33616;&#31995;&#32479;&#22312;&#21160;&#24577;&#21644;&#20381;&#36182;&#20851;&#31995;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#65292;&#21516;&#26102;&#20063;&#20851;&#27880;&#20102;&#25935;&#24863;&#29992;&#25143;&#29305;&#24449;&#30340;&#38544;&#31169;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2309.11515</link><description>&lt;p&gt;
&#36816;&#29992;&#22122;&#22768;&#22270;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#23454;&#29616;&#24046;&#20998;&#38544;&#31169;&#30340;&#39034;&#24207;&#25512;&#33616;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Differential Privacy in Sequential Recommendation: A Noisy Graph Neural Network Approach. (arXiv:2309.11515v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11515
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24046;&#20998;&#38544;&#31169;&#39034;&#24207;&#25512;&#33616;&#26694;&#26550;&#65292;&#37319;&#29992;&#20102;&#22122;&#22768;&#22270;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#24046;&#20998;&#38544;&#31169;&#25512;&#33616;&#31995;&#32479;&#22312;&#21160;&#24577;&#21644;&#20381;&#36182;&#20851;&#31995;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#65292;&#21516;&#26102;&#20063;&#20851;&#27880;&#20102;&#25935;&#24863;&#29992;&#25143;&#29305;&#24449;&#30340;&#38544;&#31169;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#21508;&#31181;&#22312;&#32447;&#24179;&#21488;&#20013;&#39640;&#35843;&#30340;&#38544;&#31169;&#27844;&#38706;&#20107;&#20214;&#39057;&#32321;&#21457;&#29983;&#65292;&#29992;&#25143;&#23545;&#38544;&#31169;&#36234;&#26469;&#36234;&#20851;&#27880;&#12290;&#25512;&#33616;&#31995;&#32479;&#20316;&#20026;&#22312;&#32447;&#24179;&#21488;&#25552;&#20379;&#20010;&#24615;&#21270;&#26381;&#21153;&#30340;&#26680;&#24515;&#32452;&#20214;&#65292;&#20854;&#38544;&#31169;&#20445;&#25252;&#24341;&#36215;&#20102;&#26497;&#22823;&#30340;&#20851;&#27880;&#12290;&#20316;&#20026;&#38544;&#31169;&#20445;&#25252;&#30340;&#40644;&#37329;&#26631;&#20934;&#65292;&#24046;&#20998;&#38544;&#31169;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#38544;&#31169;&#20445;&#25252;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24046;&#20998;&#38544;&#31169;&#25512;&#33616;&#31995;&#32479;&#21482;&#32771;&#34385;&#38745;&#24577;&#21644;&#29420;&#31435;&#30340;&#29992;&#25143;&#20132;&#20114;&#65292;&#22240;&#27492;&#26080;&#27861;&#24212;&#29992;&#20110;&#20855;&#26377;&#21160;&#24577;&#21644;&#20381;&#36182;&#20851;&#31995;&#30340;&#39034;&#24207;&#25512;&#33616;&#12290;&#21516;&#26102;&#65292;&#23545;&#20110;&#25935;&#24863;&#29992;&#25143;&#29305;&#24449;&#30340;&#38544;&#31169;&#39118;&#38505;&#20851;&#27880;&#36739;&#23569;&#65292;&#22823;&#22810;&#25968;&#21482;&#20445;&#25252;&#29992;&#25143;&#30340;&#21453;&#39304;&#25968;&#25454;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#24046;&#20998;&#38544;&#31169;&#39034;&#24207;&#25512;&#33616;&#26694;&#26550;&#65292;&#37319;&#29992;&#20102;&#22122;&#22768;&#22270;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65288;&#31216;&#20026;DIPSGNN&#65289;&#26469;&#35299;&#20915;&#36825;&#20123;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
With increasing frequency of high-profile privacy breaches in various online platforms, users are becoming more concerned about their privacy. And recommender system is the core component of online platforms for providing personalized service, consequently, its privacy preservation has attracted great attention. As the gold standard of privacy protection, differential privacy has been widely adopted to preserve privacy in recommender systems. However, existing differentially private recommender systems only consider static and independent interactions, so they cannot apply to sequential recommendation where behaviors are dynamic and dependent. Meanwhile, little attention has been paid on the privacy risk of sensitive user features, most of them only protect user feedbacks. In this work, we propose a novel DIfferentially Private Sequential recommendation framework with a noisy Graph Neural Network approach (denoted as DIPSGNN) to address these limitations. To the best of our knowledge, 
&lt;/p&gt;</description></item><item><title>&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#65292;&#36873;&#25321;&#19982;&#27979;&#35797;&#36755;&#20837;&#35821;&#20041;&#30456;&#20284;&#30340;&#28436;&#31034;&#26377;&#21161;&#20110;&#25552;&#39640;&#19979;&#28216;&#24615;&#33021;&#65292;&#20294;&#26159;&#32771;&#34385;&#21040;&#35821;&#35328;&#27169;&#22411;&#20851;&#20110;&#20219;&#21153;&#30340;&#29616;&#26377;&#30693;&#35782;&#33021;&#22815;&#26356;&#22909;&#22320;&#25351;&#23548;&#28436;&#31034;&#36873;&#25321;&#12290;</title><link>http://arxiv.org/abs/2309.07900</link><description>&lt;p&gt;
&#20855;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#27495;&#20041;&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;
Ambiguity-Aware In-Context Learning with Large Language Models. (arXiv:2309.07900v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07900
&lt;/p&gt;
&lt;p&gt;
&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#65292;&#36873;&#25321;&#19982;&#27979;&#35797;&#36755;&#20837;&#35821;&#20041;&#30456;&#20284;&#30340;&#28436;&#31034;&#26377;&#21161;&#20110;&#25552;&#39640;&#19979;&#28216;&#24615;&#33021;&#65292;&#20294;&#26159;&#32771;&#34385;&#21040;&#35821;&#35328;&#27169;&#22411;&#20851;&#20110;&#20219;&#21153;&#30340;&#29616;&#26377;&#30693;&#35782;&#33021;&#22815;&#26356;&#22909;&#22320;&#25351;&#23548;&#28436;&#31034;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;In-context learning, ICL&#65289;&#20013;&#65292;&#20165;&#21521;LLMs&#23637;&#31034;&#23569;&#37327;&#20219;&#21153;&#29305;&#23450;&#28436;&#31034;&#24050;&#32463;&#23548;&#33268;&#20102;&#19979;&#28216;&#22686;&#30410;&#65292;&#26080;&#38656;&#36827;&#34892;&#20219;&#21153;&#29305;&#23450;&#30340;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;LLMs&#23545;&#20110;&#25552;&#31034;&#36873;&#25321;&#38750;&#24120;&#25935;&#24863;&#65292;&#22240;&#27492;&#19968;&#20010;&#20851;&#38190;&#30340;&#30740;&#31350;&#38382;&#39064;&#26159;&#22914;&#20309;&#20026;ICL&#36873;&#25321;&#22909;&#30340;&#28436;&#31034;&#12290;&#19968;&#31181;&#26377;&#25928;&#30340;&#31574;&#30053;&#26159;&#21033;&#29992;ICL&#28436;&#31034;&#21644;&#27979;&#35797;&#36755;&#20837;&#20043;&#38388;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#65292;&#24182;&#20351;&#29992;&#25991;&#26412;&#26816;&#32034;&#22120;&#65292;&#28982;&#32780;&#36825;&#31181;&#26041;&#27861;&#24182;&#19981;&#32771;&#34385;LLM&#20851;&#20110;&#35813;&#20219;&#21153;&#30340;&#29616;&#26377;&#30693;&#35782;&#65292;&#22240;&#27492;&#24182;&#19981;&#26368;&#20248;&#12290;&#26681;&#25454;&#20043;&#21069;&#30340;&#24037;&#20316;&#65288;Min&#31561;&#65292;2022&#65289;&#65292;&#25105;&#20204;&#24050;&#32463;&#30693;&#36947;&#19982;&#28436;&#31034;&#37197;&#23545;&#30340;&#26631;&#31614;&#20250;&#23545;&#27169;&#22411;&#39044;&#27979;&#36896;&#25104;&#20559;&#35265;&#12290;&#36825;&#24341;&#23548;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20551;&#35774;&#65306;&#32771;&#34385;&#21040;LLM&#20851;&#20110;&#20219;&#21153;&#30340;&#29616;&#26377;&#30693;&#35782;&#65292;&#29305;&#21035;&#26159;&#19982;&#36755;&#20986;&#26631;&#31614;&#31354;&#38388;&#30456;&#20851;&#30340;&#30693;&#35782;&#65292;&#26159;&#21542;&#26377;&#21161;&#20110;&#26356;&#22909;&#30340;&#28436;&#31034;&#36873;&#25321;&#31574;&#30053;&#12290;&#36890;&#36807;&#22312;&#19977;&#20010;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#19978;&#36827;&#34892;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#19981;&#20165;&#36873;&#25321;&#35821;&#20041;&#30456;&#20284;&#30340;ICL&#28436;&#31034;&#26159;&#26377;&#30410;&#30340;&#65292;&#21516;&#26102;&#20063;&#35201;&#32771;&#34385;LLM&#20851;&#20110;&#20219;&#21153;&#30340;&#29616;&#26377;&#30693;&#35782;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#28436;&#31034;&#36873;&#25321;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
In-context learning (ICL) i.e. showing LLMs only a few task-specific demonstrations has led to downstream gains with no task-specific fine-tuning required. However, LLMs are sensitive to the choice of prompts, and therefore a crucial research question is how to select good demonstrations for ICL. One effective strategy is leveraging semantic similarity between the ICL demonstrations and test inputs by using a text retriever, which however is sub-optimal as that does not consider the LLM's existing knowledge about that task. From prior work (Min et al., 2022), we already know that labels paired with the demonstrations bias the model predictions. This leads us to our hypothesis whether considering LLM's existing knowledge about the task, especially with respect to the output label space can help in a better demonstration selection strategy. Through extensive experimentation on three text classification tasks, we find that it is beneficial to not only choose semantically similar ICL demon
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30340;&#21019;&#26032;&#28857;&#26159;&#23558;&#25512;&#33616;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#34701;&#21512;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#22810;&#21151;&#33021;&#20132;&#20114;&#24335;&#25512;&#33616;&#31995;&#32479;&#65292;&#35299;&#20915;&#20102;&#25512;&#33616;&#27169;&#22411;&#22312;&#25552;&#20379;&#35299;&#37322;&#21644;&#21442;&#19982;&#23545;&#35805;&#20219;&#21153;&#26041;&#38754;&#30340;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2308.16505</link><description>&lt;p&gt;
&#25512;&#33616;AI&#20195;&#29702;&#65306;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25972;&#21512;&#21040;&#20132;&#20114;&#24335;&#25512;&#33616;&#20013;
&lt;/p&gt;
&lt;p&gt;
Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations. (arXiv:2308.16505v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16505
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30340;&#21019;&#26032;&#28857;&#26159;&#23558;&#25512;&#33616;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#34701;&#21512;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#22810;&#21151;&#33021;&#20132;&#20114;&#24335;&#25512;&#33616;&#31995;&#32479;&#65292;&#35299;&#20915;&#20102;&#25512;&#33616;&#27169;&#22411;&#22312;&#25552;&#20379;&#35299;&#37322;&#21644;&#21442;&#19982;&#23545;&#35805;&#20219;&#21153;&#26041;&#38754;&#30340;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#27169;&#22411;&#36890;&#36807;&#21033;&#29992;&#24191;&#27867;&#30340;&#29992;&#25143;&#34892;&#20026;&#25968;&#25454;&#26469;&#25552;&#20379;&#39046;&#22495;&#29305;&#23450;&#30340;&#29289;&#21697;&#25512;&#33616;&#65292;&#23637;&#29616;&#20986;&#36731;&#37327;&#32423;&#39046;&#22495;&#19987;&#23478;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#25552;&#20379;&#35299;&#37322;&#21644;&#21442;&#19982;&#23545;&#35805;&#31561;&#22810;&#26679;&#21270;&#20219;&#21153;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20195;&#34920;&#20102;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#30340;&#37325;&#35201;&#36827;&#23637;&#65292;&#22312;&#25351;&#20196;&#29702;&#35299;&#12289;&#24120;&#35782;&#25512;&#29702;&#21644;&#20154;&#31867;&#20132;&#20114;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;LLMs&#32570;&#20047;&#39046;&#22495;&#29305;&#23450;&#29289;&#21697;&#30446;&#24405;&#21644;&#34892;&#20026;&#27169;&#24335;&#30340;&#30693;&#35782;&#65292;&#29305;&#21035;&#26159;&#22312;&#19982;&#19968;&#33324;&#19990;&#30028;&#30693;&#35782;&#19981;&#21516;&#30340;&#39046;&#22495;&#65292;&#22914;&#22312;&#32447;&#30005;&#23376;&#21830;&#21153;&#12290;&#20026;&#27599;&#20010;&#39046;&#22495;&#24494;&#35843;LLMs&#26082;&#19981;&#32463;&#27982;&#21448;&#19981;&#39640;&#25928;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#25512;&#33616;&#27169;&#22411;&#21644;LLMs&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#32467;&#21512;&#21508;&#33258;&#30340;&#20248;&#21183;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#22810;&#21151;&#33021;&#20132;&#20114;&#24335;&#25512;&#33616;&#31995;&#32479;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#26694;&#26550;&#31216;&#20026;RecAgent&#65292;&#35813;&#26694;&#26550;&#20351;&#29992;LLMs
&lt;/p&gt;
&lt;p&gt;
Recommender models excel at providing domain-specific item recommendations by leveraging extensive user behavior data. Despite their ability to act as lightweight domain experts, they struggle to perform versatile tasks such as providing explanations and engaging in conversations. On the other hand, large language models (LLMs) represent a significant step towards artificial general intelligence, showcasing remarkable capabilities in instruction comprehension, commonsense reasoning, and human interaction. However, LLMs lack the knowledge of domain-specific item catalogs and behavioral patterns, particularly in areas that diverge from general world knowledge, such as online e-commerce. Finetuning LLMs for each domain is neither economic nor efficient.  In this paper, we bridge the gap between recommender models and LLMs, combining their respective strengths to create a versatile and interactive recommender system. We introduce an efficient framework called RecAgent, which employs LLMs a
&lt;/p&gt;</description></item><item><title>TransGNN&#26159;&#19968;&#31181;&#23558;Transformer&#21644;GNN&#23618;&#20132;&#26367;&#32467;&#21512;&#20197;&#30456;&#20114;&#22686;&#24378;&#20854;&#33021;&#21147;&#30340;&#26032;&#22411;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;&#24403;&#21069;&#22522;&#20110;GNN&#30340;&#25512;&#33616;&#31995;&#32479;&#38754;&#20020;&#30340;&#24863;&#21463;&#22495;&#26377;&#38480;&#21644;&#23384;&#22312;&#22122;&#38899;&#36830;&#25509;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.14355</link><description>&lt;p&gt;
TransGNN: &#21033;&#29992;Transformer&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#21327;&#21516;&#33021;&#21147;&#26469;&#20570;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
TransGNN: Harnessing the Collaborative Power of Transformers and Graph Neural Networks for Recommender Systems. (arXiv:2308.14355v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14355
&lt;/p&gt;
&lt;p&gt;
TransGNN&#26159;&#19968;&#31181;&#23558;Transformer&#21644;GNN&#23618;&#20132;&#26367;&#32467;&#21512;&#20197;&#30456;&#20114;&#22686;&#24378;&#20854;&#33021;&#21147;&#30340;&#26032;&#22411;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;&#24403;&#21069;&#22522;&#20110;GNN&#30340;&#25512;&#33616;&#31995;&#32479;&#38754;&#20020;&#30340;&#24863;&#21463;&#22495;&#26377;&#38480;&#21644;&#23384;&#22312;&#22122;&#38899;&#36830;&#25509;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#25512;&#33616;&#31995;&#32479;&#20013;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#23545;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#22270;&#36827;&#34892;&#24314;&#27169;&#26469;&#36827;&#34892;&#21327;&#21516;&#36807;&#28388;(CF)&#12290;&#29616;&#26377;&#22522;&#20110;GNN&#30340;&#25512;&#33616;&#31995;&#32479;&#30340;&#26680;&#24515;&#26159;&#36890;&#36807;&#22312;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#36793;&#19978;&#36827;&#34892;&#36882;&#24402;&#28040;&#24687;&#20256;&#36882;&#26469;&#25913;&#36827;&#32534;&#30721;&#23884;&#20837;&#12290;&#23613;&#31649;&#23427;&#20204;&#24050;&#32463;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#65292;&#20294;&#26159;&#24403;&#21069;&#22522;&#20110;GNN&#30340;&#26041;&#27861;&#38754;&#20020;&#30528;&#26377;&#38480;&#30340;&#24863;&#21463;&#22495;&#21644;&#23384;&#22312;&#22122;&#38899; "&#20852;&#36259;&#26080;&#20851;" &#36830;&#25509;&#30340;&#25361;&#25112;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#22522;&#20110;Transformer&#30340;&#26041;&#27861;&#22312;&#33258;&#36866;&#24212;&#21644;&#20840;&#23616;&#20449;&#24687;&#32858;&#21512;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#25429;&#25417;&#22797;&#26434;&#12289;&#32416;&#32544;&#30340;&#32467;&#26500;&#20449;&#24687;&#26041;&#38754;&#22312;&#22823;&#35268;&#27169;&#20132;&#20114;&#22270;&#20013;&#30340;&#24212;&#29992;&#21463;&#21040;&#22256;&#25200;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TransGNN&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#20132;&#26367;&#22320;&#32467;&#21512;Transformer&#21644;GNN&#23618;&#26469;&#30456;&#20114;&#22686;&#24378;&#23427;&#20204;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have emerged as promising solutions for collaborative filtering (CF) through the modeling of user-item interaction graphs. The nucleus of existing GNN-based recommender systems involves recursive message passing along user-item interaction edges to refine encoded embeddings. Despite their demonstrated effectiveness, current GNN-based methods encounter challenges of limited receptive fields and the presence of noisy ``interest-irrelevant'' connections. In contrast, Transformer-based methods excel in aggregating information adaptively and globally. Nevertheless, their application to large-scale interaction graphs is hindered by inherent complexities and challenges in capturing intricate, entangled structural information. In this paper, we propose TransGNN, a novel model that integrates Transformer and GNN layers in an alternating fashion to mutually enhance their capabilities. Specifically, TransGNN leverages Transformer layers to broaden the receptive field 
&lt;/p&gt;</description></item><item><title>SSLRec&#26159;&#19968;&#20010;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#25512;&#33616;&#31995;&#32479;&#24211;&#65292;&#20026;&#35780;&#20272;&#21508;&#31181;SSL&#22686;&#24378;&#25512;&#33616;&#31995;&#32479;&#25552;&#20379;&#20102;&#26631;&#20934;&#21270;&#12289;&#28789;&#27963;&#21644;&#32508;&#21512;&#30340;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2308.05697</link><description>&lt;p&gt;
SSLRec: &#19968;&#20010;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#25512;&#33616;&#31995;&#32479;&#24211;
&lt;/p&gt;
&lt;p&gt;
SSLRec: A Self-Supervised Learning Library for Recommendation. (arXiv:2308.05697v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05697
&lt;/p&gt;
&lt;p&gt;
SSLRec&#26159;&#19968;&#20010;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#25512;&#33616;&#31995;&#32479;&#24211;&#65292;&#20026;&#35780;&#20272;&#21508;&#31181;SSL&#22686;&#24378;&#25512;&#33616;&#31995;&#32479;&#25552;&#20379;&#20102;&#26631;&#20934;&#21270;&#12289;&#28789;&#27963;&#21644;&#32508;&#21512;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#20316;&#20026;&#35299;&#20915;&#25512;&#33616;&#31995;&#32479;&#20013;&#31232;&#30095;&#21644;&#22122;&#22768;&#25968;&#25454;&#25361;&#25112;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#22312;&#26368;&#36817;&#20960;&#24180;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#23613;&#31649;&#35774;&#35745;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;SSL&#31639;&#27861;&#26469;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#25552;&#20379;&#26368;&#20808;&#36827;&#30340;&#25512;&#33616;&#24615;&#33021;&#65288;&#20363;&#22914;&#22270;&#21327;&#21516;&#36807;&#28388;&#12289;&#39034;&#24207;&#25512;&#33616;&#12289;&#31038;&#20132;&#25512;&#33616;&#12289;&#30693;&#35782;&#22270;&#22686;&#24378;&#25512;&#33616;&#65289;&#65292;&#20294;&#30446;&#21069;&#20173;&#32570;&#20047;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#26469;&#25972;&#21512;&#19981;&#21516;&#39046;&#22495;&#30340;&#25512;&#33616;&#31639;&#27861;&#12290;&#36825;&#26679;&#30340;&#26694;&#26550;&#21487;&#20197;&#20316;&#20026;&#33258;&#30417;&#30563;&#25512;&#33616;&#31639;&#27861;&#30340;&#22522;&#30707;&#65292;&#32479;&#19968;&#29616;&#26377;&#26041;&#27861;&#30340;&#39564;&#35777;&#65292;&#24182;&#25512;&#21160;&#26032;&#26041;&#27861;&#30340;&#35774;&#35745;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;SSLRec&#65292;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20934;&#24179;&#21488;&#65292;&#20026;&#35780;&#20272;&#21508;&#31181;SSL&#22686;&#24378;&#25512;&#33616;&#31995;&#32479;&#25552;&#20379;&#20102;&#26631;&#20934;&#21270;&#12289;&#28789;&#27963;&#21644;&#32508;&#21512;&#30340;&#26694;&#26550;&#12290;SSLRec&#24211;&#20855;&#26377;&#27169;&#22359;&#21270;&#26550;&#26500;&#65292;&#21487;&#20197;&#26041;&#20415;&#29992;&#25143;&#35780;&#20272;&#26368;&#20808;&#36827;&#30340;&#25512;&#33616;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) has gained significant interest in recent years as a solution to address the challenges posed by sparse and noisy data in recommender systems. Despite the growing number of SSL algorithms designed to provide state-of-the-art performance in various recommendation scenarios (e.g., graph collaborative filtering, sequential recommendation, social recommendation, KG-enhanced recommendation), there is still a lack of unified frameworks that integrate recommendation algorithms across different domains. Such a framework could serve as the cornerstone for self-supervised recommendation algorithms, unifying the validation of existing methods and driving the design of new ones. To address this gap, we introduce SSLRec, a novel benchmark platform that provides a standardized, flexible, and comprehensive framework for evaluating various SSL-enhanced recommenders. The SSLRec library features a modular architecture that allows users to easily evaluate state-of-the-art m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;FedPDD&#30340;&#38544;&#31169;&#20445;&#25252;&#21452;&#37325;&#33976;&#39311;&#26694;&#26550;&#65292;&#29992;&#20110;&#36328;&#24179;&#21488;&#32852;&#37030;&#25512;&#33616;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#25945;&#24072;&#33976;&#39311;&#21644;&#23398;&#29983;&#33976;&#39311;&#20004;&#20010;&#38454;&#27573;&#65292;&#22312;&#19981;&#20256;&#36755;&#27169;&#22411;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#26377;&#25928;&#22320;&#36716;&#31227;&#30693;&#35782;&#21644;&#20351;&#29992;&#19968;&#31181;&#26032;&#30340;&#33976;&#39311;&#25439;&#22833;&#20989;&#25968;&#26469;&#26500;&#24314;&#20840;&#23616;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.06272</link><description>&lt;p&gt;
FedPDD&#65306;&#29992;&#20110;&#36328;&#24179;&#21488;&#32852;&#37030;&#25512;&#33616;&#30340;&#38544;&#31169;&#20445;&#25252;&#21452;&#37325;&#33976;&#39311;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
FedPDD: A Privacy-preserving Double Distillation Framework for Cross-silo Federated Recommendation. (arXiv:2305.06272v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06272
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;FedPDD&#30340;&#38544;&#31169;&#20445;&#25252;&#21452;&#37325;&#33976;&#39311;&#26694;&#26550;&#65292;&#29992;&#20110;&#36328;&#24179;&#21488;&#32852;&#37030;&#25512;&#33616;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#25945;&#24072;&#33976;&#39311;&#21644;&#23398;&#29983;&#33976;&#39311;&#20004;&#20010;&#38454;&#27573;&#65292;&#22312;&#19981;&#20256;&#36755;&#27169;&#22411;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#26377;&#25928;&#22320;&#36716;&#31227;&#30693;&#35782;&#21644;&#20351;&#29992;&#19968;&#31181;&#26032;&#30340;&#33976;&#39311;&#25439;&#22833;&#20989;&#25968;&#26469;&#26500;&#24314;&#20840;&#23616;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#24179;&#21488;&#25512;&#33616;&#26088;&#22312;&#36890;&#36807;&#20174;&#19981;&#21516;&#24179;&#21488;&#25910;&#38598;&#24322;&#26500;&#29305;&#24449;&#26469;&#25552;&#39640;&#25512;&#33616;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#36234;&#26469;&#36234;&#20005;&#26684;&#30340;&#38544;&#31169;&#20445;&#25252;&#27861;&#35268;&#38480;&#21046;&#20102;&#36825;&#31181;&#24179;&#21488;&#38388;&#30340;&#36328;&#30028;&#21327;&#20316;&#65292;&#22240;&#27492;&#19981;&#33021;&#32858;&#21512;&#25968;&#25454;&#29992;&#20110;&#35757;&#32451;&#12290;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#25512;&#33616;&#22330;&#26223;&#20013;&#22788;&#29702;&#25968;&#25454;&#23396;&#23707;&#38382;&#39064;&#30340;&#23454;&#29992;&#35299;&#20915;&#26041;&#26696;&#12290;&#29616;&#26377;&#30340;&#36328;&#24179;&#21488;FL&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#37325;&#21472;&#29992;&#25143;&#30340;&#25968;&#25454;&#21327;&#21516;&#26500;&#24314;&#20840;&#23616;&#27169;&#22411;&#65292;&#20256;&#36755;&#27169;&#22411;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#20013;&#65292;&#37325;&#21472;&#29992;&#25143;&#30340;&#25968;&#37327;&#24448;&#24448;&#38750;&#24120;&#23567;&#65292;&#20174;&#32780;&#22823;&#22823;&#38480;&#21046;&#20102;&#27492;&#31867;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#35757;&#32451;&#26399;&#38388;&#20256;&#36755;&#27169;&#22411;&#20449;&#24687;&#38656;&#35201;&#39640;&#36890;&#20449;&#25104;&#26412;&#65292;&#21487;&#33021;&#20250;&#36896;&#25104;&#20005;&#37325;&#30340;&#38544;&#31169;&#27844;&#38706;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38544;&#31169;&#20445;&#25252;&#21452;&#37325;&#33976;&#39311;&#26694;&#26550; FedPDD &#29992;&#20110;&#36328;&#24179;&#21488;&#32852;&#37030;&#25512;&#33616;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#26377;&#25928;&#22320;&#36716;&#31227;&#30693;&#35782;&#26469;&#20445;&#25252;&#38544;&#31169;&#12290;FedPDD &#21253;&#25324;&#20004;&#20010;&#38454;&#27573;&#65306;&#25945;&#24072;&#33976;&#39311;&#21644;&#23398;&#29983;&#33976;&#39311;&#12290;&#22312;&#25945;&#24072;&#33976;&#39311;&#38454;&#27573;&#65292;&#27599;&#20010;&#24179;&#21488;&#22312;&#33258;&#24049;&#30340;&#25968;&#25454;&#19978;&#35757;&#32451;&#26412;&#22320;&#27169;&#22411;&#65292;&#24182;&#23558;&#26469;&#33258;&#36825;&#20123;&#27169;&#22411;&#30340;&#30693;&#35782;&#33976;&#39311;&#21040;&#19968;&#20010;&#23567;&#30340;&#12289;&#24102;&#26377;&#22122;&#22768;&#30340;&#25945;&#24072;&#27169;&#22411;&#20013;&#12290;&#28982;&#21518;&#65292;&#22312;&#23398;&#29983;&#33976;&#39311;&#38454;&#27573;&#65292;&#27599;&#20010;&#24179;&#21488;&#36890;&#36807;&#19968;&#31181;&#26032;&#30340;&#33976;&#39311;&#25439;&#22833;&#20989;&#25968;&#65292;&#21516;&#26102;&#20174;&#25945;&#24072;&#27169;&#22411;&#21644;&#26412;&#22320;&#25968;&#25454;&#20013;&#23398;&#20064;&#65292;&#35757;&#32451;&#33258;&#24049;&#30340;&#23398;&#29983;&#27169;&#22411;&#12290;FedPDD &#22312;&#20004;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#36328;&#24179;&#21488;&#32852;&#37030;&#25512;&#33616;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20445;&#25252;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-platform recommendation aims to improve recommendation accuracy by gathering heterogeneous features from different platforms. However, such cross-silo collaborations between platforms are restricted by increasingly stringent privacy protection regulations, thus data cannot be aggregated for training. Federated learning (FL) is a practical solution to deal with the data silo problem in recommendation scenarios. Existing cross-silo FL methods transmit model information to collaboratively build a global model by leveraging the data of overlapped users. However, in reality, the number of overlapped users is often very small, thus largely limiting the performance of such approaches. Moreover, transmitting model information during training requires high communication costs and may cause serious privacy leakage. In this paper, we propose a novel privacy-preserving double distillation framework named FedPDD for cross-silo federated recommendation, which efficiently transfers knowledge wh
&lt;/p&gt;</description></item></channel></rss>