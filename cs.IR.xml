<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#22522;&#20110;&#37325;&#35201;&#24615;&#30340;&#36127;&#26679;&#26412;&#25277;&#26679;&#35757;&#32451;TGNNS&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#35780;&#20272;&#35777;&#26126;&#20102;&#20854;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.09239</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#26368;&#36817;&#37051;&#30340;&#30828;&#36127;&#26679;&#26412;&#30340;&#40065;&#26834;&#24615;&#35757;&#32451;&#26102;&#38388;GNN
&lt;/p&gt;
&lt;p&gt;
Robust Training of Temporal GNNs using Nearest Neighbours based Hard Negatives
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09239
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#22522;&#20110;&#37325;&#35201;&#24615;&#30340;&#36127;&#26679;&#26412;&#25277;&#26679;&#35757;&#32451;TGNNS&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#35780;&#20272;&#35777;&#26126;&#20102;&#20854;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#22270;&#31070;&#32463;&#32593;&#32476;(TGNNS)&#22312;&#26410;&#26469;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#36825;&#20123;TGNNS&#30340;&#35757;&#32451;&#26159;&#36890;&#36807;&#22343;&#21248;&#38543;&#26426;&#25277;&#26679;&#30340;&#26080;&#30417;&#30563;&#25439;&#22833;&#36827;&#34892;&#21015;&#20030;&#30340;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#23545;&#20110;&#27491;&#20363;&#24773;&#20917;&#65292;&#25439;&#22833;&#26159;&#22312;&#26080;&#20449;&#24687;&#30340;&#36127;&#26679;&#26412;&#19978;&#35745;&#31639;&#30340;&#65292;&#36825;&#24341;&#20837;&#20102;&#20887;&#20313;&#21644;&#27425;&#20248;&#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25913;&#36827;&#30340;TGNNS&#26080;&#30417;&#30563;&#23398;&#20064;&#65292;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#37325;&#35201;&#24615;&#30340;&#36127;&#26679;&#26412;&#25277;&#26679;&#26469;&#26367;&#25442;&#22343;&#21248;&#36127;&#26679;&#26412;&#25277;&#26679;&#12290;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#23545;&#36127;&#20363;&#37319;&#26679;&#30340;&#21160;&#24577;&#35745;&#31639;&#20998;&#24067;&#36827;&#34892;&#20102;&#29702;&#35770;&#39564;&#35777;&#21644;&#23450;&#20041;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#23545;&#19977;&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#35777;&#35780;&#20272;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#22522;&#20110;&#25552;&#20986;&#30340;&#36127;&#26679;&#26412;&#25277;&#26679;&#30340;&#25439;&#22833;&#35757;&#32451;&#30340;TGNNS&#25552;&#20379;&#20102;&#19968;&#33268;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09239v1 Announce Type: new Abstract: Temporal graph neural networks Tgnn have exhibited state-of-art performance in future-link prediction tasks. Training of these TGNNs is enumerated by uniform random sampling based unsupervised loss. During training, in the context of a positive example, the loss is computed over uninformative negatives, which introduces redundancy and sub-optimal performance. In this paper, we propose modified unsupervised learning of Tgnn, by replacing the uniform negative sampling with importance-based negative sampling. We theoretically motivate and define the dynamically computed distribution for a sampling of negative examples. Finally, using empirical evaluations over three real-world datasets, we show that Tgnn trained using loss based on proposed negative sampling provides consistent superior performance.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20132;&#20114;&#27169;&#25311;&#22120; (LLM-InS)&#65292;&#29992;&#20110;&#35299;&#20915;&#20919;&#21551;&#21160;&#29289;&#21697;&#25512;&#33616;&#30340;&#38382;&#39064;&#12290;&#35813;&#27169;&#25311;&#22120;&#33021;&#22815;&#27169;&#25311;&#20986;&#36924;&#30495;&#30340;&#20132;&#20114;&#65292;&#24182;&#23558;&#20919;&#21551;&#21160;&#29289;&#21697;&#36716;&#21270;&#20026;&#28909;&#38376;&#29289;&#21697;&#65292;&#20174;&#32780;&#25552;&#39640;&#25512;&#33616;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.09176</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20132;&#20114;&#27169;&#25311;&#22120;&#29992;&#20110;&#20919;&#21551;&#21160;&#29289;&#21697;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Large Language Model Interaction Simulator for Cold-Start Item Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09176
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20132;&#20114;&#27169;&#25311;&#22120; (LLM-InS)&#65292;&#29992;&#20110;&#35299;&#20915;&#20919;&#21551;&#21160;&#29289;&#21697;&#25512;&#33616;&#30340;&#38382;&#39064;&#12290;&#35813;&#27169;&#25311;&#22120;&#33021;&#22815;&#27169;&#25311;&#20986;&#36924;&#30495;&#30340;&#20132;&#20114;&#65292;&#24182;&#23558;&#20919;&#21551;&#21160;&#29289;&#21697;&#36716;&#21270;&#20026;&#28909;&#38376;&#29289;&#21697;&#65292;&#20174;&#32780;&#25552;&#39640;&#25512;&#33616;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#20919;&#21551;&#21160;&#29289;&#21697;&#23545;&#21327;&#21516;&#36807;&#28388;&#27169;&#22411;&#26469;&#35828;&#26159;&#20010;&#38271;&#26399;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#36825;&#20123;&#29289;&#21697;&#32570;&#20047;&#21382;&#21490;&#29992;&#25143;&#20132;&#20114;&#20197;&#24314;&#27169;&#20182;&#20204;&#30340;&#21327;&#21516;&#29305;&#24615;&#12290;&#20919;&#21551;&#21160;&#29289;&#21697;&#30340;&#20869;&#23481;&#19982;&#34892;&#20026;&#27169;&#24335;&#20043;&#38388;&#30340;&#24046;&#36317;&#20351;&#24471;&#24456;&#38590;&#20026;&#20854;&#29983;&#25104;&#20934;&#30830;&#30340;&#34892;&#20026;&#23884;&#20837;&#12290;&#29616;&#26377;&#30340;&#20919;&#21551;&#21160;&#27169;&#22411;&#20351;&#29992;&#26144;&#23556;&#20989;&#25968;&#22522;&#20110;&#20919;&#21551;&#21160;&#29289;&#21697;&#30340;&#20869;&#23481;&#29305;&#24449;&#29983;&#25104;&#34394;&#20551;&#30340;&#34892;&#20026;&#23884;&#20837;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#29983;&#25104;&#30340;&#23884;&#20837;&#19982;&#30495;&#23454;&#30340;&#34892;&#20026;&#23884;&#20837;&#23384;&#22312;&#26174;&#33879;&#30340;&#24046;&#24322;&#65292;&#23545;&#20919;&#21551;&#21160;&#25512;&#33616;&#24615;&#33021;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20869;&#23481;&#26041;&#38754;&#26469;&#27169;&#25311;&#29992;&#25143;&#34892;&#20026;&#27169;&#24335;&#30340;LLM&#20132;&#20114;&#27169;&#25311;&#22120; (LLM-InS)&#12290;&#35813;&#27169;&#25311;&#22120;&#20801;&#35768;&#25512;&#33616;&#31995;&#32479;&#20026;&#27599;&#20010;&#20919;&#21551;&#21160;&#29289;&#21697;&#27169;&#25311;&#29983;&#21160;&#30340;&#20132;&#20114;&#65292;&#24182;&#23558;&#20854;&#30452;&#25509;&#20174;&#20919;&#21551;&#21160;&#29289;&#21697;&#36716;&#21270;&#20026;&#28909;&#38376;&#29289;&#21697;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09176v1 Announce Type: new Abstract: Recommending cold items is a long-standing challenge for collaborative filtering models because these cold items lack historical user interactions to model their collaborative features. The gap between the content of cold items and their behavior patterns makes it difficult to generate accurate behavioral embeddings for cold items. Existing cold-start models use mapping functions to generate fake behavioral embeddings based on the content feature of cold items. However, these generated embeddings have significant differences from the real behavioral embeddings, leading to a negative impact on cold recommendation performance. To address this challenge, we propose an LLM Interaction Simulator (LLM-InS) to model users' behavior patterns based on the content aspect. This simulator allows recommender systems to simulate vivid interactions for each cold item and transform them from cold to warm items directly. Specifically, we outline the desig
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25512;&#33616;&#20250;&#35805;&#30340;&#25512;&#33616;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20351;&#29992;&#20102;&#26469;&#33258;&#19981;&#21516;&#20449;&#24687;&#38598;&#30340;&#25968;&#25454;&#65292;&#21253;&#25324;&#38745;&#24577;&#20449;&#24687;&#31867;&#21035;&#12289;&#23545;&#35937;&#29305;&#24449;&#21644;&#21160;&#24577;&#29992;&#25143;&#34892;&#20026;&#12290;</title><link>https://arxiv.org/abs/2402.09130</link><description>&lt;p&gt;
&#22522;&#20110;&#25512;&#33616;&#20250;&#35805;&#30340;&#25512;&#33616;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Recommendation Algorithm Based on Recommendation Sessions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09130
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25512;&#33616;&#20250;&#35805;&#30340;&#25512;&#33616;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20351;&#29992;&#20102;&#26469;&#33258;&#19981;&#21516;&#20449;&#24687;&#38598;&#30340;&#25968;&#25454;&#65292;&#21253;&#25324;&#38745;&#24577;&#20449;&#24687;&#31867;&#21035;&#12289;&#23545;&#35937;&#29305;&#24449;&#21644;&#21160;&#24577;&#29992;&#25143;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20114;&#32852;&#32593;&#30340;&#24040;&#22823;&#21457;&#23637;&#19981;&#20165;&#22312;&#22320;&#29702;&#33539;&#22260;&#19978;&#65292;&#20063;&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#21033;&#29992;&#20854;&#21487;&#33021;&#24615;&#30340;&#39046;&#22495;&#19978;&#65292;&#20915;&#23450;&#20102;&#24040;&#37327;&#25968;&#25454;&#30340;&#21019;&#24314;&#21644;&#25910;&#38598;&#12290;&#30001;&#20110;&#35268;&#27169;&#30340;&#38382;&#39064;&#65292;&#20256;&#32479;&#26041;&#27861;&#19981;&#33021;&#20998;&#26512;&#36825;&#20123;&#25968;&#25454;&#65292;&#22240;&#27492;&#24517;&#39035;&#20351;&#29992;&#29616;&#20195;&#26041;&#27861;&#21644;&#25216;&#26415;&#12290;&#36825;&#31181;&#26041;&#27861;&#20027;&#35201;&#30001;&#25512;&#33616;&#39046;&#22495;&#25552;&#20379;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#19981;&#21516;&#20449;&#24687;&#38598;&#65288;&#38745;&#24577;&#20449;&#24687;&#31867;&#21035;&#12289;&#23545;&#35937;&#29305;&#24449;&#21644;&#21160;&#24577;&#29992;&#25143;&#34892;&#20026;&#65289;&#30340;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#26032;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09130v1 Announce Type: new Abstract: The enormous development of the Internet, both in the geographical scale and in the area of using its possibilities in everyday life, determines the creation and collection of huge amounts of data. Due to the scale, it is not possible to analyse them using traditional methods, therefore it makes a necessary to use modern methods and techniques. Such methods are provided, among others, by the area of recommendations. The aim of this study is to present a new algorithm in the area of recommendation systems, the algorithm based on data from various sets of information, both static (categories of objects, features of objects) and dynamic (user behaviour).
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;CPFT&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#20013;&#31934;&#32454;&#35843;&#25972;&#36807;&#31243;&#20013;&#32467;&#21512;&#20132;&#21449;&#29109;&#25439;&#22833;&#20989;&#25968;&#21644;&#22522;&#20110;&#31526;&#21512;&#24615;&#39044;&#27979;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#22686;&#24378;&#20102;&#25512;&#33616;&#31995;&#32479;&#30340;&#32622;&#20449;&#24230;&#12290;CPFT&#21160;&#24577;&#29983;&#25104;&#28508;&#22312;&#30495;&#23454;&#20540;&#30340;&#39033;&#30446;&#38598;&#21512;&#65292;&#25552;&#21319;&#20102;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#39640;&#20102;&#25512;&#33616;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#20449;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.08976</link><description>&lt;p&gt;
&#36890;&#36807;&#31526;&#21512;&#24615;&#39044;&#27979;&#23454;&#29616;&#32622;&#20449;&#24230;&#24863;&#30693;&#30340;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#30340;&#31934;&#32454;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Confidence-aware Fine-tuning of Sequential Recommendation Systems via Conformal Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08976
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;CPFT&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#20013;&#31934;&#32454;&#35843;&#25972;&#36807;&#31243;&#20013;&#32467;&#21512;&#20132;&#21449;&#29109;&#25439;&#22833;&#20989;&#25968;&#21644;&#22522;&#20110;&#31526;&#21512;&#24615;&#39044;&#27979;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#22686;&#24378;&#20102;&#25512;&#33616;&#31995;&#32479;&#30340;&#32622;&#20449;&#24230;&#12290;CPFT&#21160;&#24577;&#29983;&#25104;&#28508;&#22312;&#30495;&#23454;&#20540;&#30340;&#39033;&#30446;&#38598;&#21512;&#65292;&#25552;&#21319;&#20102;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#39640;&#20102;&#25512;&#33616;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#20013;&#65292;&#36890;&#24120;&#20351;&#29992;&#20132;&#21449;&#29109;&#25439;&#22833;&#20989;&#25968;&#65292;&#20294;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#26410;&#33021;&#21033;&#29992;&#39033;&#30446;&#32622;&#20449;&#24230;&#20998;&#25968;&#12290;&#20026;&#20102;&#35748;&#35782;&#21040;&#32622;&#20449;&#24230;&#22312;&#23558;&#35757;&#32451;&#30446;&#26631;&#19982;&#35780;&#20272;&#25351;&#26631;&#23545;&#40784;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CPFT&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#21151;&#33021;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#31934;&#32454;&#35843;&#25972;&#36807;&#31243;&#20013;&#23558;&#22522;&#20110;&#31526;&#21512;&#24615;&#39044;&#27979;&#30340;&#25439;&#22833;&#20989;&#25968;&#19982;&#20132;&#21449;&#29109;&#25439;&#22833;&#20989;&#25968;&#30456;&#32467;&#21512;&#65292;&#22686;&#24378;&#20102;&#25512;&#33616;&#31995;&#32479;&#30340;&#32622;&#20449;&#24230;&#12290;CPFT&#21160;&#24577;&#29983;&#25104;&#19968;&#32452;&#20855;&#26377;&#39640;&#27010;&#29575;&#21253;&#21547;&#30495;&#23454;&#20540;&#30340;&#39033;&#30446;&#65292;&#36890;&#36807;&#23558;&#39564;&#35777;&#25968;&#25454;&#32435;&#20837;&#35757;&#32451;&#36807;&#31243;&#32780;&#19981;&#25439;&#23475;&#20854;&#22312;&#27169;&#22411;&#36873;&#25321;&#20013;&#30340;&#20316;&#29992;&#65292;&#20016;&#23500;&#20102;&#35757;&#32451;&#36807;&#31243;&#12290;&#36825;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#19982;&#22522;&#20110;&#31526;&#21512;&#24615;&#39044;&#27979;&#30340;&#25439;&#22833;&#20989;&#25968;&#30456;&#32467;&#21512;&#65292;&#26356;&#19987;&#27880;&#20110;&#25913;&#21892;&#25512;&#33616;&#38598;&#21512;&#65292;&#20174;&#32780;&#25552;&#39640;&#28508;&#22312;&#39033;&#30446;&#39044;&#27979;&#30340;&#32622;&#20449;&#24230;&#12290;&#36890;&#36807;&#36890;&#36807;&#22522;&#20110;&#31526;&#21512;&#24615;&#39044;&#27979;&#30340;&#25439;&#22833;&#20989;&#25968;&#23545;&#39033;&#30446;&#32622;&#20449;&#24230;&#36827;&#34892;&#31934;&#32454;&#35843;&#25972;&#65292;CPFT&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#65292;&#25552;&#20379;&#20102;&#26356;&#31934;&#30830;&#21644;&#21487;&#20449;&#30340;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08976v1 Announce Type: new Abstract: In Sequential Recommendation Systems, Cross-Entropy (CE) loss is commonly used but fails to harness item confidence scores during training. Recognizing the critical role of confidence in aligning training objectives with evaluation metrics, we propose CPFT, a versatile framework that enhances recommendation confidence by integrating Conformal Prediction (CP)-based losses with CE loss during fine-tuning. CPFT dynamically generates a set of items with a high probability of containing the ground truth, enriching the training process by incorporating validation data without compromising its role in model selection. This innovative approach, coupled with CP-based losses, sharpens the focus on refining recommendation sets, thereby elevating the confidence in potential item predictions. By fine-tuning item confidence through CP-based losses, CPFT significantly enhances model performance, leading to more precise and trustworthy recommendations th
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#22312;&#22522;&#20110;&#20250;&#35805;&#30340;&#25512;&#33616;&#20013;&#22686;&#24378;ID&#21644;&#25991;&#26412;&#30340;&#34701;&#21512;&#25928;&#26524;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#31616;&#21333;&#34701;&#21512;&#30340;&#26041;&#27861;&#24182;&#19981;&#33021;&#22987;&#32456;&#36229;&#36234;&#26368;&#20339;&#30340;&#21333;&#27169;&#24577;&#26041;&#27861;&#12290;&#36827;&#19968;&#27493;&#30340;&#35843;&#26597;&#34920;&#26126;&#65292;&#36825;&#21487;&#33021;&#26159;&#30001;&#20110;&#34701;&#21512;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;ID&#21644;&#25991;&#26412;&#27169;&#24577;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.08921</link><description>&lt;p&gt;
&#36890;&#36807;&#26367;&#20195;&#35757;&#32451;&#22686;&#24378;ID&#21644;&#25991;&#26412;&#34701;&#21512;&#22312;&#22522;&#20110;&#20250;&#35805;&#30340;&#25512;&#33616;&#20013;
&lt;/p&gt;
&lt;p&gt;
Enhancing ID and Text Fusion via Alternative Training in Session-based Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08921
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#22312;&#22522;&#20110;&#20250;&#35805;&#30340;&#25512;&#33616;&#20013;&#22686;&#24378;ID&#21644;&#25991;&#26412;&#30340;&#34701;&#21512;&#25928;&#26524;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#31616;&#21333;&#34701;&#21512;&#30340;&#26041;&#27861;&#24182;&#19981;&#33021;&#22987;&#32456;&#36229;&#36234;&#26368;&#20339;&#30340;&#21333;&#27169;&#24577;&#26041;&#27861;&#12290;&#36827;&#19968;&#27493;&#30340;&#35843;&#26597;&#34920;&#26126;&#65292;&#36825;&#21487;&#33021;&#26159;&#30001;&#20110;&#34701;&#21512;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;ID&#21644;&#25991;&#26412;&#27169;&#24577;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#20250;&#35805;&#30340;&#25512;&#33616;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#20854;&#30446;&#26631;&#26159;&#26681;&#25454;&#29992;&#25143;&#22312;&#20250;&#35805;&#20013;&#30340;&#21382;&#21490;&#34892;&#20026;&#25552;&#20379;&#23450;&#21046;&#30340;&#24314;&#35758;&#12290;&#20026;&#20102;&#25512;&#36827;&#36825;&#20010;&#39046;&#22495;&#65292;&#24050;&#32463;&#24320;&#21457;&#20102;&#35768;&#22810;&#26041;&#27861;&#65292;&#20854;&#20013;&#22522;&#20110;ID&#30340;&#26041;&#27861;&#36890;&#24120;&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#38271;&#23614;&#39033;&#30446;&#26041;&#38754;&#32463;&#24120;&#38754;&#20020;&#25361;&#25112;&#65292;&#24182;&#19988;&#24573;&#35270;&#20102;&#20854;&#20182;&#20016;&#23500;&#30340;&#20449;&#24687;&#24418;&#24335;&#65292;&#29305;&#21035;&#26159;&#26377;&#20215;&#20540;&#30340;&#25991;&#26412;&#35821;&#20041;&#20449;&#24687;&#12290;&#20026;&#20102;&#25972;&#21512;&#25991;&#26412;&#20449;&#24687;&#65292;&#24341;&#20837;&#20102;&#21508;&#31181;&#26041;&#27861;&#65292;&#20027;&#35201;&#26159;&#36981;&#24490;&#19968;&#20010;&#31616;&#21333;&#30340;&#34701;&#21512;&#26694;&#26550;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#34701;&#21512;&#36825;&#20004;&#31181;&#27169;&#24577;&#24182;&#19981;&#22987;&#32456;&#20248;&#20110;&#36981;&#24490;&#31616;&#21333;&#34701;&#21512;&#26694;&#26550;&#30340;&#26368;&#20339;&#21333;&#27169;&#24577;&#12290;&#36827;&#19968;&#27493;&#30340;&#35843;&#26597;&#25581;&#31034;&#20102;&#31616;&#21333;&#34701;&#21512;&#20013;&#28508;&#22312;&#30340;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#20854;&#20013;ID&#21344;&#20027;&#23548;&#22320;&#20301;&#65292;&#32780;&#25991;&#26412;&#27169;&#24577;&#21017;&#26410;&#20805;&#20998;&#35757;&#32451;&#12290;&#36825;&#34920;&#26126;&#24847;&#22806;&#35266;&#23519;&#21487;&#33021;&#28304;&#20110;&#31616;&#21333;&#34701;&#21512;&#30340;&#28508;&#22312;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08921v1 Announce Type: cross Abstract: Session-based recommendation has gained increasing attention in recent years, with its aim to offer tailored suggestions based on users' historical behaviors within sessions.   To advance this field, a variety of methods have been developed, with ID-based approaches typically demonstrating promising performance. However, these methods often face challenges with long-tail items and overlook other rich forms of information, notably valuable textual semantic information. To integrate text information, various methods have been introduced, mostly following a naive fusion framework. Surprisingly, we observe that fusing these two modalities does not consistently outperform the best single modality by following the naive fusion framework. Further investigation reveals an potential imbalance issue in naive fusion, where the ID dominates and text modality is undertrained. This suggests that the unexpected observation may stem from naive fusion's
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#24320;&#28304;&#30340;&#22823;&#35268;&#27169;&#39640;&#36136;&#37327;&#25351;&#23548;&#25968;&#25454;&#38598;ECInstruct&#65292;&#36890;&#36807;&#25351;&#23548;&#35843;&#20248;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;&#65292;&#24320;&#21457;&#20102;&#19968;&#31995;&#21015;&#30005;&#23376;&#21830;&#21153;LLMs&#65288;eCeLLM&#65289;&#65292;&#22312;&#30005;&#23376;&#21830;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.08831</link><description>&lt;p&gt;
eCeLLM&#65306;&#20174;&#22823;&#35268;&#27169;&#39640;&#36136;&#37327;&#25351;&#23548;&#25968;&#25454;&#20013;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#24191;&#21040;&#30005;&#23376;&#21830;&#21153;&#20013;
&lt;/p&gt;
&lt;p&gt;
eCeLLM: Generalizing Large Language Models for E-commerce from Large-scale, High-quality Instruction Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08831
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#24320;&#28304;&#30340;&#22823;&#35268;&#27169;&#39640;&#36136;&#37327;&#25351;&#23548;&#25968;&#25454;&#38598;ECInstruct&#65292;&#36890;&#36807;&#25351;&#23548;&#35843;&#20248;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;&#65292;&#24320;&#21457;&#20102;&#19968;&#31995;&#21015;&#30005;&#23376;&#21830;&#21153;LLMs&#65288;eCeLLM&#65289;&#65292;&#22312;&#30005;&#23376;&#21830;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#24320;&#21457;&#26377;&#25928;&#30340;&#30005;&#23376;&#21830;&#21153;&#27169;&#22411;&#26041;&#38754;&#20570;&#20986;&#24040;&#22823;&#21162;&#21147;&#65292;&#20256;&#32479;&#30340;&#30005;&#23376;&#21830;&#21153;&#27169;&#22411;&#22312;&#36890;&#29992;&#30005;&#23376;&#21830;&#21153;&#24314;&#27169;&#19978;&#21462;&#24471;&#20102;&#26377;&#38480;&#30340;&#25104;&#21151;&#65292;&#24182;&#19988;&#22312;&#26032;&#29992;&#25143;&#21644;&#26032;&#20135;&#21697;&#19978;&#30340;&#34920;&#29616;&#19981;&#20339;&#8212;&#8212;&#36825;&#26159;&#19968;&#20010;&#20856;&#22411;&#30340;&#39046;&#22495;&#22806;&#27867;&#21270;&#25361;&#25112;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#35768;&#22810;&#39046;&#22495;&#23637;&#31034;&#20986;&#20102;&#20986;&#33394;&#30340;&#36890;&#29992;&#24314;&#27169;&#21644;&#39046;&#22495;&#22806;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#20805;&#20998;&#21457;&#25381;&#23427;&#20204;&#22312;&#30005;&#23376;&#21830;&#21153;&#20013;&#30340;&#20316;&#29992;&#65292;&#26412;&#25991;&#26500;&#24314;&#20102;ECInstruct&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#38754;&#21521;&#30005;&#23376;&#21830;&#21153;&#30340;&#24320;&#28304;&#12289;&#22823;&#35268;&#27169;&#21644;&#39640;&#36136;&#37327;&#30340;&#25351;&#23548;&#25968;&#25454;&#38598;&#12290;&#21033;&#29992;ECInstruct&#65292;&#25105;&#20204;&#36890;&#36807;&#25351;&#23548;&#35843;&#20248;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;&#24320;&#21457;&#20102;&#19968;&#31995;&#21015;&#30005;&#23376;&#21830;&#21153;LLMs&#65292;&#31216;&#20026;eCeLLM&#12290;&#25105;&#20204;&#30340;&#32508;&#21512;&#23454;&#39564;&#21644;&#35780;&#20272;&#34920;&#26126;&#65292;eCeLLM&#27169;&#22411;&#22312;&#20869;&#37096;&#29615;&#22659;&#20013;&#26126;&#26174;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;&#65292;&#21253;&#25324;&#26368;&#20808;&#36827;&#30340;GPT-4&#21644;&#26368;&#20808;&#36827;&#30340;&#29305;&#23450;&#20219;&#21153;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08831v1 Announce Type: cross Abstract: With tremendous efforts on developing effective e-commerce models, conventional e-commerce models show limited success in generalist e-commerce modeling, and suffer from unsatisfactory performance on new users and new products - a typical out-of-domain generalization challenge. Meanwhile, large language models (LLMs) demonstrate outstanding performance in generalist modeling and out-of-domain generalizability in many fields. Toward fully unleashing their power for e-commerce, in this paper, we construct ECInstruct, the first open-sourced, large-scale, and high-quality benchmark instruction dataset for e-commerce. Leveraging ECInstruct, we develop eCeLLM, a series of e-commerce LLMs, by instruction-tuning general-purpose LLMs. Our comprehensive experiments and evaluation demonstrate that eCeLLM models substantially outperform baseline models, including the most advanced GPT-4, and the state-of-the-art task-specific models in in-domain ev
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22312;&#27809;&#26377;&#26631;&#27880;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#39640;&#25928;&#25552;&#21462;&#26032;&#20135;&#21697;&#20013;&#26410;&#35265;&#36807;&#30340;&#23646;&#24615;&#20540;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#24322;&#26500;&#36229;&#22270;&#24182;&#21033;&#29992;&#24402;&#32435;&#25512;&#29702;&#26469;&#25429;&#25417;&#22797;&#26434;&#30340;&#39640;&#38454;&#20851;&#31995;&#65292;&#20197;&#26356;&#22909;&#22320;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2402.08802</link><description>&lt;p&gt;
&#22810;&#26631;&#31614;&#38646;&#26679;&#26412;&#20135;&#21697;&#23646;&#24615;&#20540;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Multi-Label Zero-Shot Product Attribute-Value Extraction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08802
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22312;&#27809;&#26377;&#26631;&#27880;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#39640;&#25928;&#25552;&#21462;&#26032;&#20135;&#21697;&#20013;&#26410;&#35265;&#36807;&#30340;&#23646;&#24615;&#20540;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#24322;&#26500;&#36229;&#22270;&#24182;&#21033;&#29992;&#24402;&#32435;&#25512;&#29702;&#26469;&#25429;&#25417;&#22797;&#26434;&#30340;&#39640;&#38454;&#20851;&#31995;&#65292;&#20197;&#26356;&#22909;&#22320;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#24212;&#25552;&#20379;&#35814;&#32454;&#30340;&#20135;&#21697;&#25551;&#36848;&#65288;&#23646;&#24615;&#20540;&#65289;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#20135;&#21697;&#25628;&#32034;&#21644;&#25512;&#33616;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#26032;&#20135;&#21697;&#65292;&#36890;&#24120;&#26080;&#27861;&#33719;&#24471;&#23646;&#24615;&#20540;&#20449;&#24687;&#12290;&#20026;&#20102;&#39044;&#27979;&#26410;&#35265;&#36807;&#30340;&#23646;&#24615;&#20540;&#65292;&#38656;&#35201;&#22823;&#37327;&#24102;&#26631;&#31614;&#30340;&#35757;&#32451;&#25968;&#25454;&#26469;&#35757;&#32451;&#20256;&#32479;&#30340;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#12290;&#36890;&#24120;&#65292;&#25163;&#21160;&#26631;&#35760;&#22823;&#37327;&#26032;&#20135;&#21697;&#30340;&#37197;&#32622;&#25991;&#20214;&#26159;&#22256;&#38590;&#12289;&#32791;&#26102;&#19988;&#26114;&#36149;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#20197;&#22312;&#27809;&#26377;&#26631;&#27880;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65288;&#38646;&#26679;&#26412;&#35774;&#32622;&#65289;&#39640;&#25928;&#22320;&#20174;&#26032;&#20135;&#21697;&#20013;&#25552;&#21462;&#26410;&#35265;&#36807;&#30340;&#23646;&#24615;&#20540;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102; HyperPAVE&#65292;&#19968;&#31181;&#21033;&#29992;&#24322;&#26500;&#36229;&#22270;&#36827;&#34892;&#24402;&#32435;&#25512;&#29702;&#30340;&#22810;&#26631;&#31614;&#38646;&#26679;&#26412;&#23646;&#24615;&#20540;&#25552;&#21462;&#27169;&#22411;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#25216;&#26415;&#26500;&#24314;&#20102;&#24322;&#26500;&#36229;&#22270;&#65292;&#20197;&#25429;&#25417;&#22797;&#26434;&#30340;&#39640;&#38454;&#20851;&#31995;&#65288;&#21363;&#29992;&#25143;&#34892;&#20026;&#20449;&#24687;&#65289;&#65292;&#20197;&#26356;&#22909;&#22320;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08802v1 Announce Type: new Abstract: E-commerce platforms should provide detailed product descriptions (attribute values) for effective product search and recommendation. However, attribute value information is typically not available for new products. To predict unseen attribute values, large quantities of labeled training data are needed to train a traditional supervised learning model. Typically, it is difficult, time-consuming, and costly to manually label large quantities of new product profiles. In this paper, we propose a novel method to efficiently and effectively extract unseen attribute values from new products in the absence of labeled data (zero-shot setting). We propose HyperPAVE, a multi-label zero-shot attribute value extraction model that leverages inductive inference in heterogeneous hypergraphs. In particular, our proposed technique constructs heterogeneous hypergraphs to capture complex higher-order relations (i.e. user behavior information) to learn more 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;LoCoV1&#65292;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#38271;&#19978;&#19979;&#25991;&#26816;&#32034;&#24615;&#33021;&#30340;&#26032;&#22411;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#25552;&#20986;&#20102;M2-BERT&#26816;&#32034;&#32534;&#30721;&#22120;&#65292;&#29992;&#20110;&#22788;&#29702;&#38271;&#19978;&#19979;&#25991;&#26816;&#32034;&#65292;&#35299;&#20915;&#20102;&#22914;&#20309;&#35780;&#20272;&#24615;&#33021;&#12289;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20197;&#21450;&#22914;&#20309;&#36827;&#34892;&#24494;&#35843;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.07440</link><description>&lt;p&gt;
&#20351;&#29992;LoCo&#21644;M2-BERT&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#21644;&#26500;&#24314;&#38271;&#19978;&#19979;&#25991;&#26816;&#32034;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Benchmarking and Building Long-Context Retrieval Models with LoCo and M2-BERT
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07440
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;LoCoV1&#65292;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#38271;&#19978;&#19979;&#25991;&#26816;&#32034;&#24615;&#33021;&#30340;&#26032;&#22411;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#25552;&#20986;&#20102;M2-BERT&#26816;&#32034;&#32534;&#30721;&#22120;&#65292;&#29992;&#20110;&#22788;&#29702;&#38271;&#19978;&#19979;&#25991;&#26816;&#32034;&#65292;&#35299;&#20915;&#20102;&#22914;&#20309;&#35780;&#20272;&#24615;&#33021;&#12289;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20197;&#21450;&#22914;&#20309;&#36827;&#34892;&#24494;&#35843;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#31649;&#36947;&#26159;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#20013;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#22312;&#25991;&#26723;&#24456;&#38271;&#65288;&#20363;&#22914;10K&#20010;&#26631;&#35760;&#25110;&#26356;&#22810;&#65289;&#19988;&#38656;&#35201;&#22312;&#25972;&#20010;&#25991;&#26412;&#20013;&#21512;&#25104;&#20449;&#24687;&#26469;&#30830;&#23450;&#30456;&#20851;&#25991;&#26723;&#30340;&#39046;&#22495;&#20013;&#34920;&#29616;&#19981;&#20339;&#12290;&#24320;&#21457;&#36866;&#29992;&#20110;&#36825;&#20123;&#39046;&#22495;&#30340;&#38271;&#19978;&#19979;&#25991;&#26816;&#32034;&#32534;&#30721;&#22120;&#38754;&#20020;&#19977;&#20010;&#25361;&#25112;&#65306;&#65288;1&#65289;&#22914;&#20309;&#35780;&#20272;&#38271;&#19978;&#19979;&#25991;&#26816;&#32034;&#24615;&#33021;&#65292;&#65288;2&#65289;&#22914;&#20309;&#39044;&#35757;&#32451;&#22522;&#26412;&#35821;&#35328;&#27169;&#22411;&#20197;&#34920;&#31034;&#30701;&#19978;&#19979;&#25991;&#65288;&#23545;&#24212;&#26597;&#35810;&#65289;&#21644;&#38271;&#19978;&#19979;&#25991;&#65288;&#23545;&#24212;&#25991;&#26723;&#65289;&#65292;&#20197;&#21450;&#65288;3&#65289;&#22914;&#20309;&#26681;&#25454;GPU&#20869;&#23384;&#38480;&#21046;&#19979;&#30340;&#25209;&#37327;&#22823;&#23567;&#38480;&#21046;&#23545;&#35813;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#20102;LoCoV1&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;12&#20010;&#20219;&#21153;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#27979;&#37327;&#22312;&#19981;&#21487;&#20998;&#22359;&#25110;&#19981;&#26377;&#25928;&#30340;&#24773;&#20917;&#19979;&#30340;&#38271;&#19978;&#19979;&#25991;&#26816;&#32034;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;M2-BERT&#26816;&#32034;&#32534;&#30721;&#22120;&#65292;&#36825;&#26159;&#19968;&#20010;80M&#21442;&#25968;&#29366;&#24577;&#31354;&#38388;&#32534;&#30721;&#22120;&#27169;&#22411;&#65292;&#37319;&#29992;Monarch Mixer&#26550;&#26500;&#26500;&#24314;&#65292;&#33021;&#22815;&#36827;&#34892;&#21487;&#25193;&#23637;&#30340;&#26816;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Retrieval pipelines-an integral component of many machine learning systems-perform poorly in domains where documents are long (e.g., 10K tokens or more) and where identifying the relevant document requires synthesizing information across the entire text. Developing long-context retrieval encoders suitable for these domains raises three challenges: (1) how to evaluate long-context retrieval performance, (2) how to pretrain a base language model to represent both short contexts (corresponding to queries) and long contexts (corresponding to documents), and (3) how to fine-tune this model for retrieval under the batch size limitations imposed by GPU memory constraints. To address these challenges, we first introduce LoCoV1, a novel 12 task benchmark constructed to measure long-context retrieval where chunking is not possible or not effective. We next present the M2-BERT retrieval encoder, an 80M parameter state-space encoder model built from the Monarch Mixer architecture, capable of scali
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#35843;&#26597;&#20102;&#19968;&#31181;&#21517;&#20026;&#26816;&#32034;&#31034;&#33539;&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;&#29305;&#23450;&#20110;&#36755;&#20837;&#26597;&#35810;&#30340;&#31034;&#33539;&#26469;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#23569;&#37327;&#26679;&#26412;&#24773;&#22659;&#23398;&#20064;&#65288;ICL&#65289;&#33021;&#21147;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#25552;&#39640;&#20102;&#23398;&#20064;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#36824;&#20943;&#23569;&#20102;&#25163;&#21160;&#31034;&#20363;&#36873;&#25321;&#20013;&#30340;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2401.11624</link><description>&lt;p&gt;
&#36890;&#36807;&#26816;&#32034;&#31034;&#33539;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#35821;&#35328;&#27169;&#22411;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
In-context Learning with Retrieved Demonstrations for Language Models: A Survey. (arXiv:2401.11624v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11624
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#35843;&#26597;&#20102;&#19968;&#31181;&#21517;&#20026;&#26816;&#32034;&#31034;&#33539;&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;&#29305;&#23450;&#20110;&#36755;&#20837;&#26597;&#35810;&#30340;&#31034;&#33539;&#26469;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#23569;&#37327;&#26679;&#26412;&#24773;&#22659;&#23398;&#20064;&#65288;ICL&#65289;&#33021;&#21147;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#25552;&#39640;&#20102;&#23398;&#20064;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#36824;&#20943;&#23569;&#20102;&#25163;&#21160;&#31034;&#20363;&#36873;&#25321;&#20013;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24050;&#23637;&#31034;&#20986;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#21487;&#20197;&#22312;&#36755;&#20837;&#19978;&#19979;&#25991;&#20013;&#36827;&#34892;&#23569;&#37327;&#26679;&#26412;&#30340;&#24773;&#22659;&#23398;&#20064;&#65288;ICL&#65289;&#65292;&#24182;&#22312;&#26032;&#20219;&#21153;&#19978;&#20855;&#26377;&#36866;&#24212;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#27169;&#22411;&#30340;ICL&#33021;&#21147;&#23545;&#20110;&#23569;&#26679;&#26412;&#31034;&#33539;&#30340;&#36873;&#25321;&#26159;&#25935;&#24863;&#30340;&#12290;&#26368;&#36817;&#30340;&#19968;&#39033;&#30740;&#31350;&#36827;&#23637;&#26159;&#26816;&#32034;&#38024;&#23545;&#27599;&#20010;&#36755;&#20837;&#26597;&#35810;&#23450;&#21046;&#30340;&#31034;&#33539;&#12290;&#31034;&#33539;&#26816;&#32034;&#30340;&#23454;&#29616;&#30456;&#23545;&#31616;&#21333;&#65292;&#21033;&#29992;&#29616;&#26377;&#30340;&#25968;&#25454;&#24211;&#21644;&#26816;&#32034;&#31995;&#32479;&#12290;&#36825;&#19981;&#20165;&#25552;&#39640;&#20102;&#23398;&#20064;&#36807;&#31243;&#30340;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#32780;&#19988;&#24050;&#32463;&#35777;&#26126;&#21487;&#20197;&#20943;&#23569;&#25163;&#21160;&#31034;&#20363;&#36873;&#25321;&#20013;&#30340;&#20559;&#35265;&#12290;&#37492;&#20110;&#20196;&#20154;&#40723;&#33310;&#30340;&#32467;&#26524;&#21644;&#22312;&#26816;&#32034;&#31034;&#33539;&#30340;ICL&#26041;&#38754;&#19981;&#26029;&#22686;&#38271;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#32508;&#36848;&#12290;&#22312;&#36825;&#39033;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#21644;&#27604;&#36739;&#20102;&#26816;&#32034;&#27169;&#22411;&#30340;&#19981;&#21516;&#35774;&#35745;&#36873;&#25321;&#65292;&#26816;&#32034;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Language models, especially pre-trained large language models, have showcased remarkable abilities as few-shot in-context learners (ICL), adept at adapting to new tasks with just a few demonstrations in the input context. However, the model's ability to perform ICL is sensitive to the choice of the few-shot demonstrations. Instead of using a fixed set of demonstrations, one recent development is to retrieve demonstrations tailored to each input query. The implementation of demonstration retrieval is relatively straightforward, leveraging existing databases and retrieval systems. This not only improves the efficiency and scalability of the learning process but also has been shown to reduce biases inherent in manual example selection. In light of the encouraging results and growing research in ICL with retrieved demonstrations, we conduct an extensive review of studies in this area. In this survey, we discuss and compare different design choices for retrieval models, retrieval training p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;CollabContext&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#24039;&#22937;&#22320;&#23558;&#21327;&#21516;&#36807;&#28388;&#20449;&#21495;&#19982;&#24773;&#22659;&#21270;&#34920;&#31034;&#30456;&#32467;&#21512;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#20851;&#38190;&#30340;&#24773;&#22659;&#35821;&#20041;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#25512;&#33616;&#31995;&#32479;&#20013;&#21327;&#21516;&#20449;&#21495;&#21644;&#24773;&#22659;&#21270;&#34920;&#31034;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2310.09400</link><description>&lt;p&gt;
&#21327;&#20316;&#24773;&#22659;&#21270;&#65306;&#22635;&#34917;&#21327;&#21516;&#36807;&#28388;&#21644;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Collaborative Contextualization: Bridging the Gap between Collaborative Filtering and Pre-trained Language Model. (arXiv:2310.09400v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09400
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;CollabContext&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#24039;&#22937;&#22320;&#23558;&#21327;&#21516;&#36807;&#28388;&#20449;&#21495;&#19982;&#24773;&#22659;&#21270;&#34920;&#31034;&#30456;&#32467;&#21512;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#20851;&#38190;&#30340;&#24773;&#22659;&#35821;&#20041;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#25512;&#33616;&#31995;&#32479;&#20013;&#21327;&#21516;&#20449;&#21495;&#21644;&#24773;&#22659;&#21270;&#34920;&#31034;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#25512;&#33616;&#31995;&#32479;&#22312;&#24314;&#27169;&#29992;&#25143;&#21644;&#29289;&#21697;&#26102; heavily relied on identity representations (IDs)&#65292;&#32780;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411; (PLM) &#30340;&#20852;&#36215;&#20016;&#23500;&#20102;&#23545;&#24773;&#22659;&#21270;&#29289;&#21697;&#25551;&#36848;&#30340;&#24314;&#27169;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649; PLM &#22312;&#35299;&#20915; few-shot&#12289;zero-shot &#25110;&#32479;&#19968;&#24314;&#27169;&#22330;&#26223;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#24120;&#24120;&#24573;&#35270;&#20102;&#20851;&#38190;&#30340;&#21327;&#21516;&#36807;&#28388;&#20449;&#21495;&#12290;&#36825;&#31181;&#24573;&#35270;&#24102;&#26469;&#20102;&#20004;&#20010;&#32039;&#36843;&#30340;&#25361;&#25112;&#65306;(1) &#21327;&#20316;&#24773;&#22659;&#21270;&#65292;&#21363;&#21327;&#21516;&#20449;&#21495;&#19982;&#24773;&#22659;&#21270;&#34920;&#31034;&#30340;&#26080;&#32541;&#38598;&#25104;&#12290;(2) &#22312;&#20445;&#30041;&#23427;&#20204;&#30340;&#24773;&#22659;&#35821;&#20041;&#30340;&#21516;&#26102;&#65292;&#24357;&#21512;&#22522;&#20110;ID&#30340;&#34920;&#31034;&#21644;&#24773;&#22659;&#21270;&#34920;&#31034;&#20043;&#38388;&#30340;&#34920;&#31034;&#24046;&#36317;&#30340;&#24517;&#35201;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CollabContext&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#24039;&#22937;&#22320;&#23558;&#21327;&#21516;&#36807;&#28388;&#20449;&#21495;&#19982;&#24773;&#22659;&#21270;&#34920;&#31034;&#32467;&#21512;&#36215;&#26469;&#65292;&#24182;&#23558;&#36825;&#20123;&#34920;&#31034;&#23545;&#40784;&#22312;&#24773;&#22659;&#31354;&#38388;&#20869;&#65292;&#20445;&#30041;&#20102;&#37325;&#35201;&#30340;&#24773;&#22659;&#35821;&#20041;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;...
&lt;/p&gt;
&lt;p&gt;
Traditional recommender systems have heavily relied on identity representations (IDs) to model users and items, while the ascendancy of pre-trained language model (PLM) encoders has enriched the modeling of contextual item descriptions. However, PLMs, although effective in addressing few-shot, zero-shot, or unified modeling scenarios, often neglect the crucial collaborative filtering signal. This neglect gives rise to two pressing challenges: (1) Collaborative Contextualization, the seamless integration of collaborative signals with contextual representations. (2) the imperative to bridge the representation gap between ID-based representations and contextual representations while preserving their contextual semantics. In this paper, we propose CollabContext, a novel model that adeptly combines collaborative filtering signals with contextual representations and aligns these representations within the contextual space, preserving essential contextual semantics. Experimental results acros
&lt;/p&gt;</description></item></channel></rss>