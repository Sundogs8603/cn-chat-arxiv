<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DEPS&#30340;&#26032;&#31639;&#27861;&#65292;&#22312;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#20013;&#36890;&#36807;&#20174;&#29992;&#25143;&#21644;&#39033;&#30446;&#20004;&#20010;&#35270;&#35282;&#21516;&#26102;&#20248;&#21270;IPS&#20272;&#35745;&#22120;&#26469;&#20943;&#23569;&#20559;&#24046;&#65292;&#25552;&#39640;&#25512;&#33616;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.08722</link><description>&lt;p&gt;
&#39034;&#24207;&#25512;&#33616;&#20013;&#21452;&#37325;&#22686;&#24378;&#20542;&#21521;&#20998;&#25968;&#31639;&#27861;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Dually Enhanced Propensity Score Estimation in Sequential Recommendation. (arXiv:2303.08722v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08722
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DEPS&#30340;&#26032;&#31639;&#27861;&#65292;&#22312;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#20013;&#36890;&#36807;&#20174;&#29992;&#25143;&#21644;&#39033;&#30446;&#20004;&#20010;&#35270;&#35282;&#21516;&#26102;&#20248;&#21270;IPS&#20272;&#35745;&#22120;&#26469;&#20943;&#23569;&#20559;&#24046;&#65292;&#25552;&#39640;&#25512;&#33616;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#26159;&#22522;&#20110;&#22823;&#37327;&#38544;&#24335;&#29992;&#25143;&#21453;&#39304;&#25968;&#25454;&#36827;&#34892;&#24314;&#27169;&#30340;&#65292;&#24403;&#29992;&#25143;&#31995;&#32479;&#22320;&#20302;&#20272;/&#39640;&#20272;&#26576;&#20123;&#39033;&#30446;&#26102;&#65292;&#20250;&#21463;&#21040;&#20559;&#35265;&#30340;&#24433;&#21709;&#12290;&#25552;&#20986;&#20102;&#22522;&#20110;&#20498;&#25968;&#20542;&#21521;&#20998;&#25968;&#65288;IPS&#65289;&#30340;&#26080;&#20559;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#22312;&#36825;&#20123;&#26041;&#27861;&#20013;&#65292;&#20542;&#21521;&#20998;&#25968;&#20272;&#35745;&#36890;&#24120;&#20165;&#38480;&#20110;&#39033;&#30446;&#35270;&#22270;&#65292;&#21363;&#23558;&#21453;&#39304;&#25968;&#25454;&#35270;&#20026;&#19982;&#29992;&#25143;&#20114;&#21160;&#30340;&#39033;&#30446;&#24207;&#21015;&#12290;&#28982;&#32780;&#65292;&#21453;&#39304;&#25968;&#25454;&#20063;&#21487;&#20197;&#20174;&#29992;&#25143;&#30340;&#35282;&#24230;&#26469;&#30475;&#24453;&#65292;&#20316;&#20026;&#19982;&#39033;&#30446;&#20114;&#21160;&#30340;&#29992;&#25143;&#24207;&#21015;&#12290;&#27492;&#22806;&#65292;&#20004;&#20010;&#35270;&#35282;&#21487;&#20197;&#20849;&#21516;&#22686;&#24378;&#20542;&#21521;&#20998;&#25968;&#30340;&#20272;&#35745;&#12290;&#21463;&#27492;&#35266;&#23519;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20174;&#29992;&#25143;&#21644;&#39033;&#30446;&#35270;&#35282;&#20272;&#35745;&#20542;&#21521;&#20998;&#25968;&#30340;DEPS&#31639;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;DEPS&#20174;&#29992;&#25143;&#35270;&#35282;&#21644;&#39033;&#30446;&#35270;&#35282;&#20248;&#21270;&#20004;&#20010;IPS&#20272;&#35745;&#22120;&#65292;&#24182;&#36890;&#36807;&#26368;&#23567;&#21270;&#20854;&#21152;&#26435;&#24179;&#22343;&#20540;&#30340;&#26041;&#24046;&#26469;&#20943;&#23569;&#20559;&#24046;&#12290;&#22312;&#20004;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;DEPS&#22312;&#20943;&#23569;&#20559;&#24046;&#21644;&#25552;&#39640;&#25512;&#33616;&#31934;&#24230;&#26041;&#38754;&#20855;&#26377;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequential recommender systems train their models based on a large amount of implicit user feedback data and may be subject to biases when users are systematically under/over-exposed to certain items. Unbiased learning based on inverse propensity scores (IPS), which estimate the probability of observing a user-item pair given the historical information, has been proposed to address the issue. In these methods, propensity score estimation is usually limited to the view of item, that is, treating the feedback data as sequences of items that interacted with the users. However, the feedback data can also be treated from the view of user, as the sequences of users that interact with the items. Moreover, the two views can jointly enhance the propensity score estimation. Inspired by the observation, we propose to estimate the propensity scores from the views of user and item, called Dually Enhanced Propensity Score Estimation (DEPS). Specifically, given a target user-item pair and the corresp
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#32771;&#34385;&#20102;&#33258;&#21160;&#26597;&#35810;&#29983;&#25104;&#65292;&#21363;&#26681;&#25454;&#20107;&#23454;&#38472;&#36848;&#33258;&#21160;&#29983;&#25104;&#25628;&#32034;&#26597;&#35810;&#12290;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#21253;&#21547;390&#20010;&#20107;&#23454;&#38472;&#36848;&#21644;&#30456;&#20851;&#25628;&#32034;&#26597;&#35810;&#21644;&#32467;&#26524;&#30340;&#20013;&#31561;&#35268;&#27169;&#35777;&#25454;&#25910;&#38598;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2303.08652</link><description>&lt;p&gt;
&#33258;&#21160;&#26597;&#35810;&#29983;&#25104;&#29992;&#20110;&#20174;&#32593;&#32476;&#25628;&#32034;&#24341;&#25806;&#20013;&#25910;&#38598;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated Query Generation for Evidence Collection from Web Search Engines. (arXiv:2303.08652v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08652
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32771;&#34385;&#20102;&#33258;&#21160;&#26597;&#35810;&#29983;&#25104;&#65292;&#21363;&#26681;&#25454;&#20107;&#23454;&#38472;&#36848;&#33258;&#21160;&#29983;&#25104;&#25628;&#32034;&#26597;&#35810;&#12290;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#21253;&#21547;390&#20010;&#20107;&#23454;&#38472;&#36848;&#21644;&#30456;&#20851;&#25628;&#32034;&#26597;&#35810;&#21644;&#32467;&#26524;&#30340;&#20013;&#31561;&#35268;&#27169;&#35777;&#25454;&#25910;&#38598;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20204;&#26222;&#36941;&#35748;&#20026;&#65292;&#21487;&#20197;&#36890;&#36807;&#22312;&#20114;&#32852;&#32593;&#19978;&#25628;&#32034;&#20449;&#24687;&#26469;&#39564;&#35777;&#25152;&#35859;&#30340;&#20107;&#23454;&#12290;&#36825;&#20010;&#36807;&#31243;&#38656;&#35201;&#20107;&#23454;&#26680;&#26597;&#21592;&#26681;&#25454;&#20107;&#23454;&#21046;&#23450;&#25628;&#32034;&#26597;&#35810;&#24182;&#21521;&#25628;&#32034;&#24341;&#25806;&#25552;&#20132;&#65292;&#28982;&#21518;&#38656;&#35201;&#22312;&#25628;&#32034;&#32467;&#26524;&#20013;&#35782;&#21035;&#30456;&#20851;&#21644;&#21487;&#20449;&#30340;&#27573;&#33853;&#65292;&#28982;&#21518;&#25165;&#33021;&#20570;&#20986;&#20915;&#31574;&#12290;&#22312;&#35768;&#22810;&#26032;&#38395;&#21644;&#23186;&#20307;&#32452;&#32455;&#20013;&#65292;&#36825;&#20010;&#36807;&#31243;&#30001;&#21103;&#32534;&#36753;&#27599;&#22825;&#23436;&#25104;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#38382;&#19968;&#20010;&#38382;&#39064;&#65292;&#37027;&#23601;&#26159;&#26159;&#21542;&#21487;&#33021;&#33258;&#21160;&#21270;&#31532;&#19968;&#27493;&#65292;&#21363;&#26597;&#35810;&#29983;&#25104;&#12290;&#25105;&#20204;&#26159;&#21542;&#33021;&#22815;&#26681;&#25454;&#31867;&#20284;&#20110;&#20154;&#31867;&#19987;&#23478;&#21046;&#23450;&#30340;&#20107;&#23454;&#38472;&#36848;&#33258;&#21160;&#29983;&#25104;&#25628;&#32034;&#26597;&#35810;&#65311;&#25105;&#20204;&#32771;&#34385;&#30456;&#20284;&#24615;&#65292;&#26080;&#35770;&#26159;&#20174;&#25991;&#26412;&#30456;&#20284;&#24615;&#30340;&#35282;&#24230;&#36824;&#26159;&#20174;&#25628;&#32034;&#24341;&#25806;&#36820;&#22238;&#30456;&#20851;&#25991;&#26723;&#30340;&#35282;&#24230;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;&#19968;&#20010;&#20013;&#31561;&#35268;&#27169;&#30340;&#35777;&#25454;&#25910;&#38598;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#25324;390&#20010;&#20107;&#23454;&#38472;&#36848;&#20197;&#21450;&#30456;&#20851;&#30340;&#20154;&#24037;&#29983;&#25104;&#30340;&#25628;&#32034;&#26597;&#35810;&#21644;&#25628;&#32034;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is widely accepted that so-called facts can be checked by searching for information on the Internet. This process requires a fact-checker to formulate a search query based on the fact and to present it to a search engine. Then, relevant and believable passages need to be identified in the search results before a decision is made. This process is carried out by sub-editors at many news and media organisations on a daily basis. Here, we ask the question as to whether it is possible to automate the first step, that of query generation. Can we automatically formulate search queries based on factual statements which are similar to those formulated by human experts? Here, we consider similarity both in terms of textual similarity and with respect to relevant documents being returned by a search engine. First, we introduce a moderate-sized evidence collection dataset which includes 390 factual statements together with associated human-generated search queries and search results. Then, we i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#22270;&#30340;&#21327;&#21516;&#36807;&#28388;&#27169;&#22411;SimRec&#65292;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#21644;&#23545;&#27604;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#25945;&#24072;GNN&#27169;&#22411;&#19982;&#36731;&#37327;&#32423;&#23398;&#29983;&#32593;&#32476;&#20043;&#38388;&#30340;&#33258;&#36866;&#24212;&#30693;&#35782;&#36716;&#31227;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#29616;&#26377;&#22522;&#20110;GNN&#30340;CF&#27169;&#22411;&#21487;&#33021;&#20986;&#29616;&#30340;&#36807;&#24230;&#24179;&#28369;&#21644;&#22122;&#22768;&#25928;&#24212;&#30340;&#38382;&#39064;&#65292;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#37117;&#36229;&#36807;&#20102;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.08537</link><description>&lt;p&gt;
&#26080;&#22270;&#21327;&#21516;&#36807;&#28388;
&lt;/p&gt;
&lt;p&gt;
Graph-less Collaborative Filtering. (arXiv:2303.08537v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08537
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#22270;&#30340;&#21327;&#21516;&#36807;&#28388;&#27169;&#22411;SimRec&#65292;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#21644;&#23545;&#27604;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#25945;&#24072;GNN&#27169;&#22411;&#19982;&#36731;&#37327;&#32423;&#23398;&#29983;&#32593;&#32476;&#20043;&#38388;&#30340;&#33258;&#36866;&#24212;&#30693;&#35782;&#36716;&#31227;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#29616;&#26377;&#22522;&#20110;GNN&#30340;CF&#27169;&#22411;&#21487;&#33021;&#20986;&#29616;&#30340;&#36807;&#24230;&#24179;&#28369;&#21644;&#22122;&#22768;&#25928;&#24212;&#30340;&#38382;&#39064;&#65292;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#37117;&#36229;&#36807;&#20102;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#24050;&#32463;&#22312;&#21327;&#21516;&#36807;&#28388;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20102;&#20854;&#22312;&#22270;&#32467;&#26500;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#25968;&#25454;&#19978;&#34920;&#31034;&#23398;&#20064;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20302;&#36890;Laplacian&#24179;&#28369;&#31639;&#23376;&#30340;&#36807;&#24230;&#24179;&#28369;&#21644;&#22122;&#22768;&#25928;&#24212;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;GNN&#30340;CF&#27169;&#22411;&#21487;&#33021;&#20250;&#29983;&#25104;&#38590;&#20197;&#21306;&#20998;&#19988;&#19981;&#20934;&#30830;&#30340;&#29992;&#25143;&#65288;&#29289;&#21697;&#65289;&#34920;&#31034;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#21327;&#21516;&#36807;&#28388;&#27169;&#22411;&#65288;SimRec&#65289;&#65292;&#23558;&#30693;&#35782;&#33976;&#39311;&#21644;&#23545;&#27604;&#23398;&#20064;&#30340;&#33021;&#21147;&#34701;&#21512;&#22312;&#19968;&#36215;&#65292;&#23454;&#29616;&#20102;&#25945;&#24072;GNN&#27169;&#22411;&#19982;&#36731;&#37327;&#32423;&#23398;&#29983;&#32593;&#32476;&#20043;&#38388;&#30340;&#33258;&#36866;&#24212;&#30693;&#35782;&#36716;&#31227;&#65292;&#22312;&#19981;&#38656;&#35201;&#26500;&#24314;&#22270;&#30340;&#24773;&#20917;&#19979;&#26356;&#22909;&#22320;&#21457;&#29616;&#29992;&#25143;&#21644;&#29289;&#21697;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) have shown the power in representation learning over graph-structured user-item interaction data for collaborative filtering (CF) task. However, with their inherently recursive message propagation among neighboring nodes, existing GNN-based CF models may generate indistinguishable and inaccurate user (item) representations due to the over-smoothing and noise effect with low-pass Laplacian smoothing operators. In addition, the recursive information propagation with the stacked aggregators in the entire graph structures may result in poor scalability in practical applications. Motivated by these limitations, we propose a simple and effective collaborative filtering model (SimRec) that marries the power of knowledge distillation and contrastive learning. In SimRec, adaptive transferring knowledge is enabled between the teacher GNN model and a lightweight student network, to not only preserve the global collaborative signals, but also address the over-smoothing
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#20083;&#33146;&#30284;&#34920;&#22411;&#25552;&#21462;&#20219;&#21153;&#30340;&#35780;&#20272;&#65292;&#23637;&#31034;&#20102;&#22522;&#20110;BERT&#30340;&#20020;&#24202;NLP&#27169;&#22411;&#22312;&#19981;&#21516;&#20020;&#24202;&#29615;&#22659;&#20013;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#24378;&#35843;&#20102;&#20351;&#29992;&#36716;&#31227;&#23398;&#20064;&#24320;&#21457;&#24191;&#20041;&#20020;&#24202;NLP&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.08448</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#30005;&#23376;&#30149;&#21382;&#30340;&#20083;&#33146;&#30284;&#34920;&#22411;NLP&#31639;&#27861;&#36827;&#34892;&#36328;&#26426;&#26500;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
A Cross-institutional Evaluation on Breast Cancer Phenotyping NLP Algorithms on Electronic Health Records. (arXiv:2303.08448v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08448
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#20083;&#33146;&#30284;&#34920;&#22411;&#25552;&#21462;&#20219;&#21153;&#30340;&#35780;&#20272;&#65292;&#23637;&#31034;&#20102;&#22522;&#20110;BERT&#30340;&#20020;&#24202;NLP&#27169;&#22411;&#22312;&#19981;&#21516;&#20020;&#24202;&#29615;&#22659;&#20013;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#24378;&#35843;&#20102;&#20351;&#29992;&#36716;&#31227;&#23398;&#20064;&#24320;&#21457;&#24191;&#20041;&#20020;&#24202;NLP&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#65306;&#22312;&#27169;&#22411;&#24320;&#21457;&#36807;&#31243;&#20013;&#65292;&#36890;&#24120;&#24573;&#30053;&#20020;&#24202;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#20083;&#33146;&#30284;&#34920;&#22411;&#25552;&#21462;&#20219;&#21153;&#65292;&#35780;&#20272;&#20102;&#22522;&#20110;BERT&#30340;&#20020;&#24202;NLP&#27169;&#22411;&#22312;&#19981;&#21516;&#20020;&#24202;&#29615;&#22659;&#19979;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#26041;&#27861;&#65306;&#20174;&#26126;&#23612;&#33487;&#36798;&#22823;&#23398;&#21644;&#26757;&#22885;&#35786;&#25152;&#30340;&#30005;&#23376;&#30149;&#21382;&#20013;&#25910;&#38598;&#20102;&#20004;&#31181;&#20083;&#33146;&#30284;&#24739;&#32773;&#30340;&#20020;&#24202;&#35821;&#26009;&#24211;&#65292;&#24182;&#25353;&#29031;&#21516;&#19968;&#25351;&#21335;&#36827;&#34892;&#27880;&#37322;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19977;&#31181;&#31867;&#22411;&#30340;NLP&#27169;&#22411;&#65288;&#26465;&#20214;&#38543;&#26426;&#22330;&#12289;&#21452;&#21521;&#38271;&#30701;&#26399;&#35760;&#24518;&#21644;CancerBERT&#65289;&#65292;&#20174;&#20020;&#24202;&#25991;&#26412;&#20013;&#25552;&#21462;&#30284;&#30151;&#34920;&#22411;&#12290;&#20351;&#29992;&#19981;&#21516;&#30340;&#23398;&#20064;&#31574;&#30053;&#65288;&#27169;&#22411;&#36716;&#31227;&#19982;&#26412;&#22320;&#35757;&#32451;&#65289;&#23545;&#27169;&#22411;&#22312;&#19981;&#21516;&#27979;&#35797;&#38598;&#19978;&#36827;&#34892;&#27867;&#21270;&#33021;&#21147;&#35780;&#20272;&#12290;&#35780;&#20272;&#23454;&#20307;&#35206;&#30422;&#29575;&#19982;&#27169;&#22411;&#24615;&#33021;&#30340;&#30456;&#20851;&#24615;&#24471;&#20998;&#12290;&#32467;&#26524;&#65306;&#22312;UMN&#21644;MC&#25163;&#21160;&#27880;&#37322;&#20102;200&#21644;161&#20221;&#20020;&#24202;&#25991;&#26723;&#12290;CancerBERT&#27169;&#22411;&#36798;&#21040;&#20102;&#26368;&#39640;&#30340;F1&#20998;&#25968;&#65288;0.896&#65289;&#21644;&#23454;&#20307;&#35206;&#30422;&#29575;&#65288;98.8%&#65289;&#65292;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;&#27169;&#22411;&#36716;&#31227;&#26041;&#27861;&#22312;&#20004;&#20010;&#26426;&#26500;&#20013;&#20135;&#29983;&#20102;&#31867;&#20284;&#20110;&#26412;&#22320;&#35757;&#32451;&#27169;&#22411;&#30340;&#32467;&#26524;&#65292;&#34920;&#26126;&#36328;&#26426;&#26500;&#23384;&#22312;&#28508;&#22312;&#30340;&#27867;&#21270;&#24615;&#12290;&#32467;&#35770;&#65306;&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22312;&#19981;&#21516;&#20020;&#24202;&#29615;&#22659;&#20013;&#35780;&#20272;NLP&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#24378;&#35843;&#20102;&#20351;&#29992;&#36716;&#31227;&#23398;&#20064;&#24320;&#21457;&#24191;&#20041;&#20020;&#24202;NLP&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Objective: The generalizability of clinical large language models is usually ignored during the model development process. This study evaluated the generalizability of BERT-based clinical NLP models across different clinical settings through a breast cancer phenotype extraction task.  Materials and Methods: Two clinical corpora of breast cancer patients were collected from the electronic health records from the University of Minnesota and the Mayo Clinic, and annotated following the same guideline. We developed three types of NLP models (i.e., conditional random field, bi-directional long short-term memory and CancerBERT) to extract cancer phenotypes from clinical texts. The models were evaluated for their generalizability on different test sets with different learning strategies (model transfer vs. locally trained). The entity coverage score was assessed with their association with the model performances.  Results: We manually annotated 200 and 161 clinical documents at UMN and MC, re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20559;&#22909;&#24341;&#23548;&#30340;&#22270;&#24418;&#31038;&#20132;&#25512;&#33616;&#21435;&#22122;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#25512;&#33616;&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2303.08346</link><description>&lt;p&gt;
&#22522;&#20110;&#20559;&#22909;&#24341;&#23548;&#21435;&#22122;&#30340;&#22270;&#24418;&#31038;&#20132;&#25512;&#33616;&#40065;&#26834;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Robust Preference-Guided Denoising for Graph based Social Recommendation. (arXiv:2303.08346v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08346
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20559;&#22909;&#24341;&#23548;&#30340;&#22270;&#24418;&#31038;&#20132;&#25512;&#33616;&#21435;&#22122;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#25512;&#33616;&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#30340;&#31038;&#20132;&#25512;&#33616;&#27169;&#22411;&#36890;&#36807;&#21033;&#29992;GNN&#22312;&#31038;&#20132;&#20851;&#31995;&#20013;&#30340;&#20559;&#22909;&#30456;&#20284;&#24615;&#26469;&#25552;&#39640;&#29992;&#25143;&#20559;&#22909;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#25512;&#33616;&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#65292;&#24456;&#22823;&#19968;&#37096;&#20998;&#31038;&#20132;&#20851;&#31995;&#21487;&#33021;&#26159;&#20887;&#20313;&#30340;&#29978;&#33267;&#26159;&#22024;&#26434;&#30340;&#65292;&#20363;&#22914;&#65292;&#22312;&#26576;&#20010;&#39046;&#22495;&#20013;&#65292;&#26379;&#21451;&#20043;&#38388;&#19981;&#20849;&#20139;&#20559;&#22909;&#26159;&#24456;&#27491;&#24120;&#30340;&#12290;&#29616;&#26377;&#27169;&#22411;&#27809;&#26377;&#23436;&#20840;&#35299;&#20915;&#36825;&#20010;&#20851;&#31995;&#20887;&#20313;&#21644;&#22122;&#38899;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#20204;&#30452;&#25509;&#34920;&#24449;&#25972;&#20010;&#31038;&#20132;&#32593;&#32476;&#19978;&#30340;&#31038;&#20132;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#20165;&#20445;&#30041;&#20449;&#24687;&#20016;&#23500;&#30340;&#31038;&#20132;&#20851;&#31995;&#26469;&#25913;&#36827;&#22522;&#20110;&#22270;&#30340;&#31038;&#20132;&#25512;&#33616;&#65292;&#20197;&#30830;&#20445;&#19968;&#20010;&#39640;&#25928;&#21644;&#26377;&#25928;&#30340;&#24433;&#21709;&#25193;&#25955;&#65292;&#21363;&#22270;&#24418;&#21435;&#22122;&#12290;&#25105;&#20204;&#35774;&#35745;&#30340;&#21435;&#22122;&#26041;&#27861;&#26159;&#22522;&#20110;&#20559;&#22909;&#24341;&#23548;&#30340;&#65292;&#20197;&#24314;&#27169;&#31038;&#20132;&#20851;&#31995;&#30340;&#20449;&#24515;&#65292;&#24182;&#36890;&#36807;&#25552;&#20379;&#19968;&#20010;&#21435;&#22122;&#20294;&#26356;&#20855;&#20449;&#24687;&#37327;&#30340;&#31038;&#20132;&#22270;&#20026;&#25512;&#33616;&#29992;&#25143;&#20559;&#22909;&#23398;&#20064;&#25552;&#20379;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Network(GNN) based social recommendation models improve the prediction accuracy of user preference by leveraging GNN in exploiting preference similarity contained in social relations. However, in terms of both effectiveness and efficiency of recommendation, a large portion of social relations can be redundant or even noisy, e.g., it is quite normal that friends share no preference in a certain domain. Existing models do not fully solve this problem of relation redundancy and noise, as they directly characterize social influence over the full social network. In this paper, we instead propose to improve graph based social recommendation by only retaining the informative social relations to ensure an efficient and effective influence diffusion, i.e., graph denoising. Our designed denoising method is preference-guided to model social relation confidence and benefits user preference learning in return by providing a denoised but more informative social graph for recommendation 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#39640;&#25928;&#22320;&#25429;&#25417;&#35270;&#35273;&#33402;&#26415;&#30340;&#20803;&#32032;&#65292;&#25552;&#20986;&#20102;&#32467;&#21512;&#25991;&#26412;&#21644;&#35270;&#35273;&#29305;&#24449;&#23398;&#20064;&#25216;&#26415;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#29992;&#20110;&#20010;&#24615;&#21270;&#33402;&#26415;&#21697;&#25512;&#33616;&#65292;&#32467;&#26524;&#26174;&#31034;&#20004;&#32773;&#30340;&#32467;&#21512;&#21487;&#20197;&#25429;&#25417;&#26368;&#21512;&#36866;&#30340;&#38544;&#34255;&#35821;&#20041;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2303.08182</link><description>&lt;p&gt;
&#35270;&#35273;&#33402;&#26415;&#25512;&#33616;&#30340;&#35201;&#32032;&#65306;&#23398;&#20064;&#30011;&#20316;&#30340;&#28508;&#22312;&#35821;&#20041;&#34920;&#24449;
&lt;/p&gt;
&lt;p&gt;
The Elements of Visual Art Recommendation: Learning Latent Semantic Representations of Paintings. (arXiv:2303.08182v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08182
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#39640;&#25928;&#22320;&#25429;&#25417;&#35270;&#35273;&#33402;&#26415;&#30340;&#20803;&#32032;&#65292;&#25552;&#20986;&#20102;&#32467;&#21512;&#25991;&#26412;&#21644;&#35270;&#35273;&#29305;&#24449;&#23398;&#20064;&#25216;&#26415;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#29992;&#20110;&#20010;&#24615;&#21270;&#33402;&#26415;&#21697;&#25512;&#33616;&#65292;&#32467;&#26524;&#26174;&#31034;&#20004;&#32773;&#30340;&#32467;&#21512;&#21487;&#20197;&#25429;&#25417;&#26368;&#21512;&#36866;&#30340;&#38544;&#34255;&#35821;&#20041;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33402;&#26415;&#21697;&#25512;&#33616;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#29702;&#35299;&#29992;&#25143;&#22914;&#20309;&#19982;&#39640;&#24230;&#20027;&#35266;&#30340;&#20869;&#23481;&#20114;&#21160;&#65292;&#33402;&#26415;&#21697;&#20013;&#23884;&#20837;&#30340;&#27010;&#24565;&#30340;&#22797;&#26434;&#24615;&#65292;&#20197;&#21450;&#23427;&#20204;&#21487;&#33021;&#24341;&#36215;&#29992;&#25143;&#30340;&#24773;&#24863;&#21644;&#35748;&#30693;&#21453;&#24212;&#12290;&#26412;&#25991;&#37325;&#28857;&#30740;&#31350;&#22914;&#20309;&#39640;&#25928;&#22320;&#25429;&#25417;&#35270;&#35273;&#33402;&#26415;&#30340;&#20803;&#32032;&#65288;&#21363;&#28508;&#22312;&#35821;&#20041;&#20851;&#31995;&#65289;&#65292;&#20197;&#36827;&#34892;&#20010;&#24615;&#21270;&#25512;&#33616;&#12290;&#25105;&#20204;&#25552;&#20986;&#24182;&#30740;&#31350;&#20102;&#22522;&#20110;&#25991;&#26412;&#21644;&#35270;&#35273;&#29305;&#24449;&#23398;&#20064;&#25216;&#26415;&#20197;&#21450;&#23427;&#20204;&#30340;&#32452;&#21512;&#30340;&#25512;&#33616;&#31995;&#32479;&#12290;&#25105;&#20204;&#23545;&#25512;&#33616;&#36136;&#37327;&#36827;&#34892;&#20102;&#23567;&#35268;&#27169;&#21644;&#22823;&#35268;&#27169;&#30340;&#29992;&#25143;&#20013;&#24515;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25991;&#26412;&#29305;&#24449;&#27604;&#35270;&#35273;&#29305;&#24449;&#34920;&#29616;&#26356;&#22909;&#65292;&#32780;&#20004;&#32773;&#30340;&#32467;&#21512;&#21487;&#20197;&#25429;&#25417;&#33402;&#26415;&#21697;&#25512;&#33616;&#26368;&#21512;&#36866;&#30340;&#38544;&#34255;&#35821;&#20041;&#20851;&#31995;&#12290;&#26368;&#32456;&#65292;&#26412;&#25991;&#26377;&#21161;&#20110;&#29702;&#35299;&#22914;&#20309;&#25552;&#20379;&#36866;&#21512;&#29992;&#25143;&#20852;&#36259;&#21644;&#24863;&#30693;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artwork recommendation is challenging because it requires understanding how users interact with highly subjective content, the complexity of the concepts embedded within the artwork, and the emotional and cognitive reflections they may trigger in users. In this paper, we focus on efficiently capturing the elements (i.e., latent semantic relationships) of visual art for personalized recommendation. We propose and study recommender systems based on textual and visual feature learning techniques, as well as their combinations. We then perform a small-scale and a large-scale user-centric evaluation of the quality of the recommendations. Our results indicate that textual features compare favourably with visual ones, whereas a fusion of both captures the most suitable hidden semantic relationships for artwork recommendation. Ultimately, this paper contributes to our understanding of how to deliver content that suitably matches the user's interests and how they are perceived.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#36807;&#28388;&#22120;&#24863;&#30693;&#30340;&#36890;&#29992;&#36817;&#20284;&#26694;&#26550;&#65292;&#35813;&#26041;&#27861;&#23450;&#20041;&#20102;&#21512;&#36866;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#36816;&#34892;&#26102;&#35757;&#32451;&#20197;&#28385;&#36275;&#32479;&#35745;&#24179;&#31561;&#32422;&#26463;&#65292;&#21516;&#26102;&#26368;&#23567;&#31243;&#24230;&#25200;&#21160;&#21407;&#22987;&#21518;&#39564;&#24773;&#20917;&#19979;&#23454;&#29616;&#27492;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2303.08157</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20844;&#24179;&#22270;&#36807;&#28388;&#26367;&#20195;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Network Surrogates of Fair Graph Filtering. (arXiv:2303.08157v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08157
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#36807;&#28388;&#22120;&#24863;&#30693;&#30340;&#36890;&#29992;&#36817;&#20284;&#26694;&#26550;&#65292;&#35813;&#26041;&#27861;&#23450;&#20041;&#20102;&#21512;&#36866;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#36816;&#34892;&#26102;&#35757;&#32451;&#20197;&#28385;&#36275;&#32479;&#35745;&#24179;&#31561;&#32422;&#26463;&#65292;&#21516;&#26102;&#26368;&#23567;&#31243;&#24230;&#25200;&#21160;&#21407;&#22987;&#21518;&#39564;&#24773;&#20917;&#19979;&#23454;&#29616;&#27492;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36793;&#20256;&#25773;&#23558;&#20808;&#21069;&#30340;&#33410;&#28857;&#20540;&#36716;&#25442;&#20026;&#21518;&#26469;&#30340;&#20998;&#25968;&#30340;&#22270;&#28388;&#27874;&#22120;&#36890;&#24120;&#25903;&#25345;&#24433;&#21709;&#20154;&#31867;&#30340;&#22270;&#25366;&#25496;&#20219;&#21153;&#65292;&#20363;&#22914;&#25512;&#33616;&#21644;&#25490;&#21517;&#12290;&#22240;&#27492;&#65292;&#37325;&#35201;&#30340;&#26159;&#22312;&#28385;&#36275;&#33410;&#28857;&#32452;&#20043;&#38388;&#30340;&#32479;&#35745;&#24179;&#31561;&#32422;&#26463;&#26041;&#38754;&#20351;&#23427;&#20204;&#20844;&#24179;&#65288;&#20363;&#22914;&#65292;&#25353;&#20854;&#20195;&#34920;&#24615;&#23558;&#20998;&#25968;&#36136;&#37327;&#22312;&#24615;&#21035;&#20043;&#38388;&#22343;&#34913;&#20998;&#37197;&#65289;&#12290;&#20026;&#20102;&#22312;&#26368;&#23567;&#31243;&#24230;&#22320;&#25200;&#21160;&#21407;&#22987;&#21518;&#39564;&#24773;&#20917;&#19979;&#23454;&#29616;&#27492;&#30446;&#26631;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#36807;&#28388;&#22120;&#24863;&#30693;&#30340;&#36890;&#29992;&#36817;&#20284;&#26694;&#26550;&#65292;&#29992;&#20110;&#21518;&#39564;&#30446;&#26631;&#12290;&#36825;&#23450;&#20041;&#20102;&#36866;&#24403;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#20854;&#22312;&#36816;&#34892;&#26102;&#35757;&#32451;&#65292;&#31867;&#20284;&#20110;&#36807;&#28388;&#22120;&#65292;&#20294;&#20063;&#22312;&#26412;&#22320;&#20248;&#21270;&#21253;&#25324;&#20844;&#24179;&#24863;&#30693;&#22312;&#20869;&#30340;&#22823;&#31867;&#30446;&#26631;&#12290;&#22312;&#19968;&#32452;8&#20010;&#36807;&#28388;&#22120;&#21644;5&#20010;&#22270;&#24418;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#28385;&#36275;&#32479;&#35745;&#24179;&#31561;&#32422;&#26463;&#26041;&#38754;&#34920;&#29616;&#24471;&#19981;&#20122;&#20110;&#26367;&#20195;&#21697;&#65292;&#21516;&#26102;&#20445;&#30041;&#22522;&#20110;&#20998;&#25968;&#30340;&#31038;&#21306;&#25104;&#21592;&#25512;&#33616;&#30340;AUC&#24182;&#22312;&#20256;&#25773;&#20808;&#21069;&#33410;&#25293;&#26102;&#21019;&#24314;&#26368;&#23567;&#23454;&#29992;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph filters that transform prior node values to posterior scores via edge propagation often support graph mining tasks affecting humans, such as recommendation and ranking. Thus, it is important to make them fair in terms of satisfying statistical parity constraints between groups of nodes (e.g., distribute score mass between genders proportionally to their representation). To achieve this while minimally perturbing the original posteriors, we introduce a filter-aware universal approximation framework for posterior objectives. This defines appropriate graph neural networks trained at runtime to be similar to filters but also locally optimize a large class of objectives, including fairness-aware ones. Experiments on a collection of 8 filters and 5 graphs show that our approach performs equally well or better than alternatives in meeting parity constraints while preserving the AUC of score-based community member recommendation and creating minimal utility loss in prior diffusion.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#22269;&#38469;&#36328;&#23398;&#31185;&#35843;&#26597;&#21457;&#29616;&#65292;&#20316;&#32773;&#32626;&#21517;&#30340;&#20914;&#31361;&#38382;&#39064;&#38750;&#24120;&#26222;&#36941;&#65292;&#24182;&#19988;&#24448;&#24448;&#22312;&#23398;&#32773;&#30340;&#32844;&#19994;&#29983;&#28079;&#26089;&#26399;&#23601;&#24320;&#22987;&#20986;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.00386</link><description>&lt;p&gt;
&#23398;&#26415;&#30028;&#30340;&#20316;&#32773;&#20914;&#31361;&#65306;&#19968;&#39033;&#22269;&#38469;&#36328;&#23398;&#31185;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Authorship Conflicts in Academia: an International Cross-Discipline Survey. (arXiv:2303.00386v2 [cs.DL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00386
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#22269;&#38469;&#36328;&#23398;&#31185;&#35843;&#26597;&#21457;&#29616;&#65292;&#20316;&#32773;&#32626;&#21517;&#30340;&#20914;&#31361;&#38382;&#39064;&#38750;&#24120;&#26222;&#36941;&#65292;&#24182;&#19988;&#24448;&#24448;&#22312;&#23398;&#32773;&#30340;&#32844;&#19994;&#29983;&#28079;&#26089;&#26399;&#23601;&#24320;&#22987;&#20986;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#32773;&#38388;&#30340;&#21512;&#20316;&#24050;&#25104;&#20026;&#24403;&#20195;&#31185;&#23398;&#30340;&#37325;&#35201;&#29305;&#24449;&#65292;&#22240;&#27492;&#22312;&#20986;&#29256;&#29289;&#20013;&#21015;&#20986;&#30340;&#20316;&#32773;&#25968;&#37327;&#19981;&#26029;&#19978;&#21319;&#12290;&#28982;&#32780;&#65292;&#30830;&#23450;&#24212;&#35813;&#21253;&#25324;&#21738;&#20123;&#20316;&#32773;&#20197;&#21450;&#20182;&#20204;&#30340;&#39034;&#24207;&#28041;&#21450;&#22810;&#31181;&#22256;&#38590;&#65292;&#24448;&#24448;&#20250;&#23548;&#33268;&#20914;&#31361;&#12290;&#23613;&#31649;&#20851;&#20110;&#23398;&#26415;&#20914;&#31361;&#30340;&#22823;&#37327;&#25991;&#29486;&#65292;&#20294;&#23427;&#22312;&#20027;&#35201;&#31038;&#20250;&#20154;&#21475;&#23398;&#29305;&#24449;&#20197;&#21450;&#23398;&#26415;&#30028;&#32463;&#21382;&#30340;&#19981;&#21516;&#31867;&#22411;&#30340;&#20132;&#20114;&#26041;&#38754;&#30340;&#20998;&#24067;&#20173;&#19981;&#28165;&#26970;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#22269;&#38469;&#36328;&#23398;&#31185;&#35843;&#26597;&#65292;&#21463;&#21040;&#20102;&#26469;&#33258;41&#20010;&#30740;&#31350;&#39046;&#22495;&#21644;93&#20010;&#22269;&#23478;&#30340;752&#21517;&#23398;&#32773;&#30340;&#32479;&#35745;&#23398;&#20195;&#34920;&#25972;&#20307;&#23398;&#26415;&#21171;&#21160;&#21147;&#30340;&#22238;&#31572;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#20196;&#20154;&#25285;&#24551;&#65292;&#34920;&#26126;&#20316;&#32773;&#32626;&#21517;&#20914;&#31361;&#22312;&#19968;&#20010;&#20154;&#30340;&#23398;&#26415;&#29983;&#28079;&#26089;&#26399;&#21363;&#20135;&#29983;&#65292;&#29978;&#33267;&#22312;&#30805;&#22763;&#21644;&#21338;&#22763;&#27700;&#24179;&#19978;&#23601;&#26222;&#36941;&#20986;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Collaboration among scholars has emerged as a significant characteristic of contemporary science. As a result, the number of authors listed in publications continues to rise steadily. Unfortunately, determining the authors to be included in the byline and their respective order entails multiple difficulties which often lead to conflicts. Despite the large volume of literature about conflicts in academia, it remains unclear how exactly it is distributed over the main socio-demographic properties, as well as the different types of interactions academics experience. To address this gap, we conducted an international and cross-disciplinary survey answered by 752 academics from 41 fields of research and 93 countries that statistically well-represent the overall academic workforce. Our findings are concerning and suggest that authorship credit conflicts arise very early in one's academic career, even at the level of Master and Ph.D., and become increasingly common over time.
&lt;/p&gt;</description></item></channel></rss>