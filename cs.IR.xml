<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>ConFIRM&#26159;&#19968;&#31181;&#20250;&#35805;&#24335;&#37329;&#34701;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#65292;&#36890;&#36807;&#21512;&#25104;&#37329;&#34701;&#39046;&#22495;&#29305;&#23450;&#38382;&#31572;&#23545;&#21644;&#35780;&#20272;&#21442;&#25968;&#24494;&#35843;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#36229;&#36807;90%&#30340;&#20934;&#30830;&#24615;&#65292;&#20026;&#37329;&#34701;&#23545;&#35805;&#31995;&#32479;&#25552;&#20379;&#20102;&#25968;&#25454;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2310.13001</link><description>&lt;p&gt;
&#20250;&#35805;&#24335;&#37329;&#34701;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#65288;ConFIRM&#65289;
&lt;/p&gt;
&lt;p&gt;
Conversational Financial Information Retrieval Model (ConFIRM). (arXiv:2310.13001v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13001
&lt;/p&gt;
&lt;p&gt;
ConFIRM&#26159;&#19968;&#31181;&#20250;&#35805;&#24335;&#37329;&#34701;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#65292;&#36890;&#36807;&#21512;&#25104;&#37329;&#34701;&#39046;&#22495;&#29305;&#23450;&#38382;&#31572;&#23545;&#21644;&#35780;&#20272;&#21442;&#25968;&#24494;&#35843;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#36229;&#36807;90%&#30340;&#20934;&#30830;&#24615;&#65292;&#20026;&#37329;&#34701;&#23545;&#35805;&#31995;&#32479;&#25552;&#20379;&#20102;&#25968;&#25454;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#21033;&#29992;&#23427;&#20204;&#22312;&#37329;&#34701;&#31561;&#19987;&#38376;&#39046;&#22495;&#30340;&#26032;&#20852;&#29305;&#24615;&#20855;&#26377;&#25506;&#32034;&#30340;&#20215;&#20540;&#12290;&#28982;&#32780;&#65292;&#37329;&#34701;&#31561;&#21463;&#30417;&#31649;&#39046;&#22495;&#20855;&#26377;&#29420;&#29305;&#30340;&#32422;&#26463;&#26465;&#20214;&#65292;&#38656;&#35201;&#20855;&#22791;&#38024;&#23545;&#35813;&#39046;&#22495;&#30340;&#20248;&#21270;&#26694;&#26550;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;ConFIRM&#65292;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#20250;&#35805;&#24335;&#37329;&#34701;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#65292;&#29992;&#20110;&#26597;&#35810;&#24847;&#22270;&#20998;&#31867;&#21644;&#30693;&#35782;&#24211;&#26631;&#35760;&#12290;ConFIRM&#21253;&#25324;&#20004;&#20010;&#27169;&#22359;&#65306;1&#65289;&#19968;&#31181;&#21512;&#25104;&#37329;&#34701;&#39046;&#22495;&#29305;&#23450;&#38382;&#31572;&#23545;&#30340;&#26041;&#27861;&#65292;&#20197;&#21450;2&#65289;&#35780;&#20272;&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#26469;&#36827;&#34892;&#26597;&#35810;&#20998;&#31867;&#20219;&#21153;&#12290;&#25105;&#20204;&#29983;&#25104;&#20102;&#19968;&#20010;&#21253;&#21547;4000&#22810;&#20010;&#26679;&#26412;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;&#21333;&#29420;&#30340;&#27979;&#35797;&#38598;&#19978;&#35780;&#20272;&#20102;&#20934;&#30830;&#24615;&#12290;ConFIRM&#23454;&#29616;&#20102;&#36229;&#36807;90%&#30340;&#20934;&#30830;&#24615;&#65292;&#36825;&#23545;&#20110;&#31526;&#21512;&#30417;&#31649;&#35201;&#27714;&#33267;&#20851;&#37325;&#35201;&#12290;ConFIRM&#25552;&#20379;&#20102;&#19968;&#31181;&#25968;&#25454;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#25552;&#21462;&#37329;&#34701;&#23545;&#35805;&#31995;&#32479;&#30340;&#31934;&#30830;&#26597;&#35810;&#24847;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the exponential growth in large language models (LLMs), leveraging their emergent properties for specialized domains like finance merits exploration. However, regulated fields such as finance pose unique constraints, requiring domain-optimized frameworks. We present ConFIRM, an LLM-based conversational financial information retrieval model tailored for query intent classification and knowledge base labeling.  ConFIRM comprises two modules:  1) a method to synthesize finance domain-specific question-answer pairs, and  2) evaluation of parameter efficient fine-tuning approaches for the query classification task. We generate a dataset of over 4000 samples, assessing accuracy on a separate test set.  ConFIRM achieved over 90% accuracy, essential for regulatory compliance. ConFIRM provides a data-efficient solution to extract precise query intent for financial dialog systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;paNNG&#30340;&#31639;&#27861;&#65292;&#23427;&#32467;&#21512;&#20102;&#33258;&#36866;&#24212;kNN&#21644;&#22522;&#20110;&#20998;&#24067;&#30340;&#22270;&#26500;&#24314;&#12290;&#36890;&#36807;&#21253;&#21547;&#20998;&#24067;&#20449;&#24687;&#65292;paNNG&#33021;&#22815;&#26377;&#25928;&#25552;&#21319;&#27169;&#31946;&#26679;&#26412;&#30340;&#24615;&#33021;&#65292;&#24182;&#23454;&#29616;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.02442</link><description>&lt;p&gt;
&#20351;&#29992;&#20998;&#24067;&#24863;&#30693;&#30340;&#33258;&#36866;&#24212;&#20248;&#20808;&#32423;&#38468;&#21152;kNN&#22270;
&lt;/p&gt;
&lt;p&gt;
Adaptive Preferential Attached kNN Graph With Distribution-Awareness. (arXiv:2308.02442v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02442
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;paNNG&#30340;&#31639;&#27861;&#65292;&#23427;&#32467;&#21512;&#20102;&#33258;&#36866;&#24212;kNN&#21644;&#22522;&#20110;&#20998;&#24067;&#30340;&#22270;&#26500;&#24314;&#12290;&#36890;&#36807;&#21253;&#21547;&#20998;&#24067;&#20449;&#24687;&#65292;paNNG&#33021;&#22815;&#26377;&#25928;&#25552;&#21319;&#27169;&#31946;&#26679;&#26412;&#30340;&#24615;&#33021;&#65292;&#24182;&#23454;&#29616;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22270;&#30340;kNN&#31639;&#27861;&#22240;&#20854;&#31616;&#21333;&#24615;&#21644;&#26377;&#25928;&#24615;&#22312;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#24191;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;kNN&#22270;&#23545;&#20110;k&#20540;&#30340;&#22266;&#23450;&#20381;&#36182;&#21487;&#33021;&#20250;&#24433;&#21709;&#20854;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#28041;&#21450;&#22797;&#26434;&#25968;&#25454;&#20998;&#24067;&#30340;&#24773;&#20917;&#19979;&#12290;&#27492;&#22806;&#65292;&#19982;&#20854;&#20182;&#20998;&#31867;&#27169;&#22411;&#31867;&#20284;&#65292;&#20915;&#31574;&#36793;&#30028;&#19978;&#23384;&#22312;&#30340;&#27169;&#31946;&#26679;&#26412;&#24120;&#24120;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#20204;&#26356;&#23481;&#26131;&#34987;&#38169;&#35823;&#20998;&#31867;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20248;&#20808;&#32423;&#38468;&#21152;k-&#26368;&#36817;&#37051;&#22270;&#65288;paNNG&#65289;&#65292;&#23427;&#23558;&#33258;&#36866;&#24212;&#30340;kNN&#19982;&#22522;&#20110;&#20998;&#24067;&#30340;&#22270;&#26500;&#24314;&#30456;&#32467;&#21512;&#12290;&#36890;&#36807;&#32467;&#21512;&#20998;&#24067;&#20449;&#24687;&#65292;paNNG&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#31946;&#26679;&#26412;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#8220;&#25289;&#8221;&#23427;&#20204;&#22238;&#21040;&#21407;&#22987;&#31867;&#21035;&#65292;&#20174;&#32780;&#23454;&#29616;&#25913;&#36827;&#30340;&#25972;&#20307;&#20934;&#30830;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#36890;&#36807;&#22312;&#22810;&#26679;&#21270;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20005;&#26684;&#35780;&#20272;&#65292;paNNG&#30340;&#24615;&#33021;&#36229;&#36234;&#20102;&#29616;&#26377;&#31639;&#27861;&#65292;&#23637;&#31034;&#20102;&#23427;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph-based kNN algorithms have garnered widespread popularity for machine learning tasks, due to their simplicity and effectiveness. However, the conventional kNN graph's reliance on a fixed value of k can hinder its performance, especially in scenarios involving complex data distributions. Moreover, like other classification models, the presence of ambiguous samples along decision boundaries often presents a challenge, as they are more prone to incorrect classification. To address these issues, we propose the Preferential Attached k-Nearest Neighbors Graph (paNNG), which combines adaptive kNN with distribution-based graph construction. By incorporating distribution information, paNNG can significantly improve performance for ambiguous samples by "pulling" them towards their original classes and hence enable enhanced overall accuracy and generalization capability. Through rigorous evaluations on diverse benchmark datasets, paNNG outperforms state-of-the-art algorithms, showcasing its 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#65306;&#21487;&#33021;&#24615;&#35757;&#32451;&#21644;&#19981;&#21487;&#33021;&#24615;&#35757;&#32451;&#65292;&#20197;&#35299;&#37322;&#23884;&#20837;&#21521;&#37327;&#32972;&#21518;&#30340;&#35821;&#20041;&#24182;&#35299;&#20915;&#26631;&#31614;&#31232;&#30095;&#38382;&#39064;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#22270;&#20687;&#21644;&#35270;&#39057;&#20998;&#31867;&#12289;&#26816;&#32034;&#21644;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#25552;&#39640;&#20102;&#23398;&#20064;&#23884;&#20837;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2207.00282</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#23884;&#20837;&#30340;(&#19981;)&#21487;&#33021;&#24615;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
(Un)likelihood Training for Interpretable Embedding. (arXiv:2207.00282v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.00282
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#65306;&#21487;&#33021;&#24615;&#35757;&#32451;&#21644;&#19981;&#21487;&#33021;&#24615;&#35757;&#32451;&#65292;&#20197;&#35299;&#37322;&#23884;&#20837;&#21521;&#37327;&#32972;&#21518;&#30340;&#35821;&#20041;&#24182;&#35299;&#20915;&#26631;&#31614;&#31232;&#30095;&#38382;&#39064;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#22270;&#20687;&#21644;&#35270;&#39057;&#20998;&#31867;&#12289;&#26816;&#32034;&#21644;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#25552;&#39640;&#20102;&#23398;&#20064;&#23884;&#20837;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#24050;&#25104;&#20026;&#24357;&#21512;&#25991;&#26412;&#21644;&#35270;&#35273;&#25968;&#25454;&#35821;&#20041;&#24046;&#36317;&#30340;&#26032;&#24120;&#24577;&#12290;&#28982;&#32780;&#65292;&#22312;&#36830;&#32493;&#28508;&#22312;&#31354;&#38388;&#20013;&#23398;&#20064;&#27169;&#24577;&#19981;&#21487;&#30693;&#34920;&#31034;&#32463;&#24120;&#34987;&#35270;&#20026;&#40657;&#21283;&#23376;&#25968;&#25454;&#39537;&#21160;&#30340;&#35757;&#32451;&#36807;&#31243;&#12290;&#28145;&#24230;&#34920;&#31034;&#23398;&#20064;&#30340;&#26377;&#25928;&#24615;&#20005;&#37325;&#20381;&#36182;&#20110;&#35757;&#32451;&#25968;&#25454;&#30340;&#36136;&#37327;&#21644;&#35268;&#27169;&#12290;&#23545;&#20110;&#35270;&#39057;&#34920;&#31034;&#23398;&#20064;&#65292;&#35201;&#23436;&#25972;&#22320;&#26631;&#27880;&#35270;&#39057;&#20869;&#23481;&#30340;&#25968;&#25454;&#38598;&#26159;&#39640;&#24230;&#22256;&#38590;&#29978;&#33267;&#19981;&#21487;&#33021;&#30340;&#12290;&#36825;&#20123;&#38382;&#39064;&#65292;&#40657;&#21283;&#23376;&#35757;&#32451;&#21644;&#25968;&#25454;&#38598;&#20559;&#24046;&#65292;&#20351;&#24471;&#35299;&#37322;&#24615;&#36739;&#24046;&#21644;&#32467;&#26524;&#38590;&#20197;&#39044;&#27979;&#65292;&#38590;&#20197;&#22312;&#35270;&#39057;&#29702;&#35299;&#26041;&#38754;&#36827;&#34892;&#23454;&#38469;&#24212;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#33021;&#24615;&#21644;&#19981;&#21487;&#33021;&#24615;&#20989;&#25968;&#65292;&#20197;&#23637;&#31034;&#23884;&#20837;&#32972;&#21518;&#30340;&#35821;&#20041;&#65292;&#24182;&#35299;&#20915;&#35757;&#32451;&#20013;&#30340;&#26631;&#31614;&#31232;&#30095;&#38382;&#39064;&#12290;&#21487;&#33021;&#24615;&#35757;&#32451;&#26088;&#22312;&#36890;&#36807;&#23398;&#20064;&#25968;&#25454;&#20998;&#24067;&#26469;&#35299;&#37322;&#23884;&#20837;&#21521;&#37327;&#30340;&#35821;&#20041;&#65292;&#32780;&#19981;&#21487;&#33021;&#24615;&#35757;&#32451;&#21017;&#24378;&#35843;&#27491;&#36127;&#23545;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#23558;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#22270;&#20687;&#21644;&#35270;&#39057;&#20998;&#31867;&#12289;&#26816;&#32034;&#21644;&#29983;&#25104;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#25552;&#39640;&#23398;&#21040;&#30340;&#23884;&#20837;&#30340;&#21487;&#35299;&#37322;&#24615;&#20197;&#21450;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#31454;&#20105;&#24615;&#24615;&#33021;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-modal representation learning has become a new normal for bridging the semantic gap between text and visual data. Learning modality agnostic representations in a continuous latent space, however, is often treated as a black-box data-driven training process. It is well-known that the effectiveness of representation learning depends heavily on the quality and scale of training data. For video representation learning, having a complete set of labels that annotate the full spectrum of video content for training is highly difficult if not impossible. These issues, black-box training and dataset bias, make representation learning practically challenging to be deployed for video understanding due to unexplainable and unpredictable results. In this paper, we propose two novel training objectives, likelihood and unlikelihood functions, to unroll semantics behind embeddings while addressing the label sparsity problem in training. The likelihood training aims to interpret semantics of embed
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Panel-MDP&#30340;&#26032;&#22411;&#27169;&#22411;&#65292;&#36890;&#36807;&#37319;&#29992;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#65292;&#20197;&#29992;&#25143;&#21916;&#22909;&#20026;&#23548;&#21521;&#65292;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#32593;&#26684;&#38754;&#26495;&#25490;&#21015;&#29289;&#21697;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#29992;&#25143;&#20307;&#39564;&#12290;</title><link>http://arxiv.org/abs/2204.04954</link><description>&lt;p&gt;
&#22522;&#20110;2D&#32593;&#26684;&#25512;&#33616;&#38754;&#26495;&#30340;&#24378;&#21270;&#20877;&#25490;&#24207;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Re-ranking with 2D Grid-based Recommendation Panels. (arXiv:2204.04954v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.04954
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Panel-MDP&#30340;&#26032;&#22411;&#27169;&#22411;&#65292;&#36890;&#36807;&#37319;&#29992;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#65292;&#20197;&#29992;&#25143;&#21916;&#22909;&#20026;&#23548;&#21521;&#65292;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#32593;&#26684;&#38754;&#26495;&#25490;&#21015;&#29289;&#21697;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#29992;&#25143;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#25512;&#33616;&#31995;&#32479;&#36890;&#24120;&#20316;&#20026;&#19968;&#20010;&#27969;&#24335;&#30340;&#21333;&#32500;&#25490;&#24207;&#21015;&#34920;&#21576;&#29616;&#29289;&#21697;&#12290;&#36817;&#24180;&#26469;&#65292;&#22312;&#30005;&#23376;&#21830;&#21153;&#20013;&#26377;&#19968;&#31181;&#36235;&#21183;&#65292;&#21363;&#25512;&#33616;&#30340;&#29289;&#21697;&#20197;&#20108;&#32500;&#32593;&#26684;&#38754;&#26495;&#30340;&#24418;&#24335;&#32452;&#32455;&#65292;&#29992;&#25143;&#21487;&#20197;&#22312;&#31446;&#30452;&#21644;&#27700;&#24179;&#26041;&#21521;&#19978;&#26597;&#30475;&#29289;&#21697;&#12290;&#22312;&#32593;&#26684;&#24418;&#24335;&#30340;&#32467;&#26524;&#38754;&#26495;&#20013;&#21576;&#29616;&#29289;&#21697;&#23545;&#20110;&#25512;&#33616;&#31995;&#32479;&#25552;&#20986;&#20102;&#26032;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#29616;&#26377;&#27169;&#22411;&#37117;&#26159;&#35774;&#35745;&#29992;&#20110;&#36755;&#20986;&#24207;&#21015;&#21015;&#34920;&#65292;&#32780;&#32593;&#26684;&#38754;&#26495;&#20013;&#30340;&#25554;&#27133;&#27809;&#26377;&#26126;&#30830;&#30340;&#39034;&#24207;&#12290;&#30452;&#25509;&#23558;&#29289;&#21697;&#25490;&#21517;&#36716;&#25442;&#20026;&#32593;&#26684;&#65288;&#20363;&#22914;&#65292;&#39044;&#23450;&#20041;&#25554;&#27133;&#30340;&#39034;&#24207;&#65289;&#24573;&#30053;&#20102;&#32593;&#26684;&#38754;&#26495;&#19978;&#29992;&#25143;&#29305;&#23450;&#30340;&#34892;&#20026;&#27169;&#24335;&#65292;&#24182;&#19988;&#19981;&#21487;&#36991;&#20813;&#22320;&#24433;&#21709;&#29992;&#25143;&#20307;&#39564;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#65292;&#29992;&#20110;&#22312;&#25512;&#33616;&#31995;&#32479;&#30340;&#26368;&#32456;&#20877;&#25490;&#24207;&#38454;&#27573;&#20013;&#25918;&#32622;&#29289;&#21697;&#21040;&#20108;&#32500;&#32593;&#26684;&#32467;&#26524;&#38754;&#26495;&#20013;&#12290;&#35813;&#27169;&#22411;&#34987;&#31216;&#20026;Panel-MDP&#65292;&#23427;&#20197;&#26089;&#26399;&#38454;&#27573;&#30340;&#21021;&#22987;&#29289;&#21697;&#25490;&#24207;&#20026;&#36755;&#20837;&#12290;&#28982;&#21518;&#65292;&#27169;&#22411;&#23558;&#20197;&#29992;&#25143;&#21916;&#22909;&#20026;&#23548;&#21521;&#65292;&#37319;&#29992;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#26469;&#20915;&#23450;&#22914;&#20309;&#25490;&#21015;&#29289;&#21697;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern recommender systems usually present items as a streaming, one-dimensional ranking list. Recently there is a trend in e-commerce that the recommended items are organized grid-based panels with two dimensions where users can view the items in both vertical and horizontal directions. Presenting items in grid-based result panels poses new challenges to recommender systems because existing models are all designed to output sequential lists while the slots in a grid-based panel have no explicit order. Directly converting the item rankings into grids (e.g., pre-defining an order on the slots) overlooks the user-specific behavioral patterns on grid-based panels and inevitably hurts the user experiences. To address this issue, we propose a novel Markov decision process (MDP) to place the items in 2D grid-based result panels at the final re-ranking stage of the recommender systems. The model, referred to as Panel-MDP, takes an initial item ranking from the early stages as the input. Then,
&lt;/p&gt;</description></item></channel></rss>