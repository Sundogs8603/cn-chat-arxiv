# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Multimodal Learned Sparse Retrieval with Probabilistic Expansion Control](https://arxiv.org/abs/2402.17535) | 提出了一种具有概率扩展控制的多模态学习稀疏检索方法，通过新的训练算法有效地减少了高维共激活和语义偏差。 |
| [^2] | [BASES: Large-scale Web Search User Simulation with Large Language Model based Agents](https://arxiv.org/abs/2402.17505) | 本研究提出了一个基于大型语言模型的用户仿真框架BASES，能够有效地模拟大规模类人类的网络搜索行为。 |
| [^3] | [REAR: A Relevance-Aware Retrieval-Augmented Framework for Open-Domain Question Answering](https://arxiv.org/abs/2402.17497) | 提出了一种名为REAR的新方法，旨在解决大型语言模型在检索增强生成中无法准确评估检索文档相关性的问题，通过增强对检索文档相关性的自我意识，能够自适应地利用外部知识。 |
| [^4] | [Natural Language Processing Methods for Symbolic Music Generation and Information Retrieval: a Survey](https://arxiv.org/abs/2402.17467) | 该调研综述了在符号音乐生成和信息检索研究中应用的自然语言处理方法，重点关注了符号音乐表示的设计和处理。 |
| [^5] | [Deep Learning Based Named Entity Recognition Models for Recipes](https://arxiv.org/abs/2402.17447) | 该研究基于深度学习，针对食谱文本开发了命名实体识别模型，通过系统的数据处理和分析，构建了用于自动生成新食谱的数据集。 |
| [^6] | [BiVRec: Bidirectional View-based Multimodal Sequential Recommendation](https://arxiv.org/abs/2402.17334) | 提出了一个创新框架 BiVRec，在推荐任务中联合训练 ID 和多模态视图，双向增强推荐性能。 |
| [^7] | [DiFashion: Towards Personalized Outfit Generation](https://arxiv.org/abs/2402.17279) | 引入生成式服装推荐任务（GOR），旨在合成一组时尚图片并组装成视觉和谐的、定制给个人用户的服装。 |
| [^8] | [PromptMM: Multi-Modal Knowledge Distillation for Recommendation with Prompt-Tuning](https://arxiv.org/abs/2402.17188) | 提出了一种通过Prompt-Tuning赋能的PromptMM多模式知识蒸馏方法，用于简化和增强推荐系统，实现自适应的质量蒸馏。 |
| [^9] | [Actions Speak Louder than Words: Trillion-Parameter Sequential Transducers for Generative Recommendations](https://arxiv.org/abs/2402.17152) | 提出了HSTU架构，用于高基数、非平稳流推荐数据，性能优于基线方法高达65.8％的NDCG，并且比基于FlashAttention2的Transformer在8192长度序列上快5.3倍到15.2倍。 |
| [^10] | [Side Information-Driven Session-based Recommendation: A Survey](https://arxiv.org/abs/2402.17129) | 这项调查全面回顾了基于辅助信息驱动的基于会话推荐研究，探讨了多样的辅助信息如何增强推荐系统性能，分析了最新和代表性的研究进展。 |
| [^11] | [A Fine-tuning Enhanced RAG System with Quantized Influence Measure as AI Judge](https://arxiv.org/abs/2402.17081) | 本研究通过将经过微调的大型语言模型与向量数据库集成，引入了LoRA和QLoRA方法，并直接将用户反馈纳入训练过程，同时引入量化影响度作为“Innovative AI Judge”机制，以提高系统的精确性。 |
| [^12] | [Multi-Task Contrastive Learning for 8192-Token Bilingual Text Embeddings](https://arxiv.org/abs/2402.17016) | 通过引入独特的多任务学习目标，研究者设计了用于支持英语和其他目标语言的最先进的双语文本嵌入模型，显著提高了模型在STS任务上的表现，同时在目标语言理解和跨语言评估任务中超越了现有多语言模型。 |
| [^13] | [Long Dialog Summarization: An Analysis](https://arxiv.org/abs/2402.16986) | 长对话摘要的研究强调了在各种应用中为有效沟通创造连贯和上下文丰富摘要的重要性。 |
| [^14] | [Avoiding Catastrophic Forgetting in Visual Classification Using Human Concept Formation](https://arxiv.org/abs/2402.16933) | 提出了一种名为Cobweb4V的新颖视觉分类方法，利用人类类似学习系统，避免了灾难性遗忘效应，与传统方法相比，需要更少的数据来实现有效学习成果，并保持稳定性能。 |
| [^15] | [Using text embedding models and vector databases as text classifiers with the example of medical data](https://arxiv.org/abs/2402.16886) | 向量数据库和嵌入模型的应用为文本分类器提供了强大的方式来表达数据模式，特别是在医疗领域中开始有着广泛的应用。 |
| [^16] | [Large Language Model Augmented Exercise Retrieval for Personalized Language Learning](https://arxiv.org/abs/2402.16877) | 大型语言模型利用生成能力来合成假设练习，以弥合学习者需求与练习内容之间的语义鸿沟，提高个性化语言学习练习检索效果。 |
| [^17] | [Advanced Academic Team Worker Recommendation Models](https://arxiv.org/abs/2402.16876) | 提出了一种新任务：学术团队成员推荐模型，通过结合查询和论文上下文与图拓扑结构，形成适用于特定研究兴趣和任务的新图（CQBG-R），实验结果表明这种方法的有效性。 |
| [^18] | [Can we predict QPP? An approach based on multivariate outliers](https://arxiv.org/abs/2402.16875) | 本文通过研究影响查询性能预测准确性的因素，提出了一个基于多变量离群值的方法，有效识别出QPP表现不佳的查询。 |
| [^19] | [Enhancing Retrieval Processes for Language Generation with Augmented Queries](https://arxiv.org/abs/2402.16874) | 本研究通过检索增强生成（RAG）技术解决了语言生成中“幻觉”问题，并借助查询优化过程将用户查询与高级语言模型连接，显著提升了模型性能。 |
| [^20] | [NFT1000: A Visual Text Dataset For Non-Fungible Token Retrieval](https://arxiv.org/abs/2402.16872) | 提出了一个名为NFT1000的视觉文本数据集，其中包含756万个图像文本对，来自销量最高的1000个PFP NFT收藏品，用于解决NFT检索中的准确和高效问题。 |
| [^21] | [Interactive Mars Image Content-Based Search with Interpretable Machine Learning](https://arxiv.org/abs/2402.16860) | 提出了一种基于可解释机器学习的交互式火星图像内容搜索方法，可以帮助用户理解和验证分类器使用的证据，并替换现有的不可解释图像搜索系统。 |
| [^22] | [A Robust Cybersecurity Topic Classification Tool](https://arxiv.org/abs/2109.02473) | 通过使用多数投票的方式，研究提出了一种网络安全主题分类工具，相比于21个单独模型，该工具在检测网络安全相关文本时表现出较低的假阳性和假阴性率，并且能够处理数十万份文档。 |
| [^23] | [When do Generative Query and Document Expansions Fail? A Comprehensive Study Across Methods, Retrievers, and Datasets.](http://arxiv.org/abs/2309.08541) | 通过对11种扩展技术、12个不同分布变化的数据集和24个检索模型的全面分析，我们发现使用大型语言模型进行查询或文档扩展的效果与检索器性能相关，对于弱模型来说扩展提高了分数，但对于强模型来说扩展通常会损害分数。 |
| [^24] | [NevIR: Negation in Neural Information Retrieval.](http://arxiv.org/abs/2305.07614) | 本研究探讨了否定在神经信息检索中的影响，构建了基准模型，结果表明当前信息检索模型大多数都没有考虑否定，而交叉编码器是目前表现最好的架构。 |
| [^25] | [Defending Against Misinformation Attacks in Open-Domain Question Answering.](http://arxiv.org/abs/2212.10002) | 本文提出了一种使用查询扩充来搜索冗余信息、并通过新颖的置信度方法将其集成到模型中的方法，可以有效防御开放域问答系统中的污染攻击，精确匹配率可提高近20%。 |

# 详细

[^1]: 具有概率扩展控制的多模态学习稀疏检索

    Multimodal Learned Sparse Retrieval with Probabilistic Expansion Control

    [https://arxiv.org/abs/2402.17535](https://arxiv.org/abs/2402.17535)

    提出了一种具有概率扩展控制的多模态学习稀疏检索方法，通过新的训练算法有效地减少了高维共激活和语义偏差。

    

    学习稀疏检索（LSR）是一类神经方法，将查询和文档编码为稀疏的词汇向量，可以通过倒排索引高效索引和检索。我们探讨了LSR在多模态领域的应用，重点关注文本-图像检索。虽然LSR在文本检索方面取得了成功，但它在多模态检索中的应用仍未得到充分探讨。当前的方法如LexLIP和STAIR需要对大规模数据集进行复杂的多步训练。我们提出的方法将来自冻结的稠密模型的密集向量有效地转换为稀疏的词汇向量。通过使用伯努利随机变量控制查询扩展，我们通过新的训练算法解决了高维共激活和语义偏差的问题。在两个密集模型（BLIP、ALBEF）和两个数据集（MSCOCO、Flickr30k）上进行的实验表明，我们提出的算法有效地减少了共激活和语义偏差。

    arXiv:2402.17535v1 Announce Type: new  Abstract: Learned sparse retrieval (LSR) is a family of neural methods that encode queries and documents into sparse lexical vectors that can be indexed and retrieved efficiently with an inverted index. We explore the application of LSR to the multi-modal domain, with a focus on text-image retrieval. While LSR has seen success in text retrieval, its application in multimodal retrieval remains underexplored. Current approaches like LexLIP and STAIR require complex multi-step training on massive datasets. Our proposed approach efficiently transforms dense vectors from a frozen dense model into sparse lexical vectors. We address issues of high dimension co-activation and semantic deviation through a new training algorithm, using Bernoulli random variables to control query expansion. Experiments with two dense models (BLIP, ALBEF) and two datasets (MSCOCO, Flickr30k) show that our proposed algorithm effectively reduces co-activation and semantic devia
    
[^2]: 基于大型语言模型的大规模网络搜索用户仿真

    BASES: Large-scale Web Search User Simulation with Large Language Model based Agents

    [https://arxiv.org/abs/2402.17505](https://arxiv.org/abs/2402.17505)

    本研究提出了一个基于大型语言模型的用户仿真框架BASES，能够有效地模拟大规模类人类的网络搜索行为。

    

    由于大型语言模型（LLMs）具有出色的能力，因此开发基于LLM的代理以可靠地仿真用户变得可行。考虑到真实用户数据的稀缺性和限制（例如隐私问题），本文针对网络搜索进行大规模用户仿真，以改进对用户搜索行为的分析和建模。特别是，我们提出了一个新颖的用户仿真框架BASES，其中包含基于LLM的代理，旨在促进对网络搜索用户行为的全面模拟。我们的仿真框架可以大规模生成独特的用户配置文件，从而导致多样化的搜索行为。为了证明BASES的有效性，我们基于中英文的两个人类基准进行了评估实验，证明BASES能够有效地模拟大规模类似人类的搜索行为。为了进一步促进网络搜索研究，我们开发了一个新的大规模仿真测试集

    arXiv:2402.17505v1 Announce Type: cross  Abstract: Due to the excellent capacities of large language models (LLMs), it becomes feasible to develop LLM-based agents for reliable user simulation. Considering the scarcity and limit (e.g., privacy issues) of real user data, in this paper, we conduct large-scale user simulation for web search, to improve the analysis and modeling of user search behavior. Specially, we propose BASES, a novel user simulation framework with LLM-based agents, designed to facilitate comprehensive simulations of web search user behaviors. Our simulation framework can generate unique user profiles at scale, which subsequently leads to diverse search behaviors. To demonstrate the effectiveness of BASES, we conduct evaluation experiments based on two human benchmarks in both Chinese and English, demonstrating that BASES can effectively simulate large-scale human-like search behaviors. To further accommodate the research on web search, we develop WARRIORS, a new larg
    
[^3]: REAR：一种面向开放域问答的关注度感知检索增强框架

    REAR: A Relevance-Aware Retrieval-Augmented Framework for Open-Domain Question Answering

    [https://arxiv.org/abs/2402.17497](https://arxiv.org/abs/2402.17497)

    提出了一种名为REAR的新方法，旨在解决大型语言模型在检索增强生成中无法准确评估检索文档相关性的问题，通过增强对检索文档相关性的自我意识，能够自适应地利用外部知识。

    

    考虑到有限的内部参数化知识，检索增强生成（RAG）被广泛用于扩展大型语言模型（LLMs）的知识范围。尽管在RAG研究上进行了大量努力，但在现有方法中，LLMs 无法准确评估检索文档的相关性，因此很可能导致对外部知识（即检索文档）的误导甚至错误利用。为解决这一问题，本文提出了 REAR，一种面向开放域问答（QA）的关注度感知检索增强方法。作为关键动机，我们旨在增强LLMs对来源相关性的自我意识，以便在RAG系统中自适应地利用外部知识。特别是，我们开发了一种新的基于LLM的RAG系统架构，通过整合一个精确评估检索文档相关性的特别设计的排名头。此外，我们提出了一种改进的训练方法。

    arXiv:2402.17497v1 Announce Type: new  Abstract: Considering the limited internal parametric knowledge, retrieval-augmented generation (RAG) has been widely used to extend the knowledge scope of large language models (LLMs). Despite the extensive efforts on RAG research, in existing methods, LLMs cannot precisely assess the relevance of retrieved documents, thus likely leading to misleading or even incorrect utilization of external knowledge (i.e., retrieved documents). To address this issue, in this paper, we propose REAR, a RElevance-Aware Retrieval-augmented approach for open-domain question answering (QA). As the key motivation, we aim to enhance the self-awareness of source relevance for LLMs, so as to adaptively utilize external knowledge in RAG systems. Specially, we develop a new architecture for LLM based RAG system, by incorporating a specially designed rank head that precisely assesses the relevance of retrieved documents. Furthermore, we propose an improved training method 
    
[^4]: 用于符号音乐生成和信息检索的自然语言处理方法：一项调研

    Natural Language Processing Methods for Symbolic Music Generation and Information Retrieval: a Survey

    [https://arxiv.org/abs/2402.17467](https://arxiv.org/abs/2402.17467)

    该调研综述了在符号音乐生成和信息检索研究中应用的自然语言处理方法，重点关注了符号音乐表示的设计和处理。

    

    自从Transformers模型在自然语言处理（NLP）领域取得突破以来，该模型已在各个领域进行了多种改进。 这一趋势已经传播到音乐信息检索（MIR）领域，包括处理音乐数据的研究。 但是，在MIR中，利用NLP工具处理符号音乐数据的做法并不新颖。 音乐经常被比作语言，因为它们具有多个相似之处，包括文本和音乐的序列化表示。 这些类比还通过MIR和NLP中的类似任务得到体现。 本文综述了应用于符号音乐生成和信息检索研究的NLP方法，遵循两个方面。我们首先概述了从自然语言序列表示中改编而来的符号音乐表示。 通过考虑符号音乐的特定性来设计这些表示。 然后这些表示被模型处理。

    arXiv:2402.17467v1 Announce Type: cross  Abstract: Several adaptations of Transformers models have been developed in various domains since its breakthrough in Natural Language Processing (NLP). This trend has spread into the field of Music Information Retrieval (MIR), including studies processing music data. However, the practice of leveraging NLP tools for symbolic music data is not novel in MIR. Music has been frequently compared to language, as they share several similarities, including sequential representations of text and music. These analogies are also reflected through similar tasks in MIR and NLP. This survey reviews NLP methods applied to symbolic music generation and information retrieval studies following two axes. We first propose an overview of representations of symbolic music adapted from natural language sequential representations. Such representations are designed by considering the specificities of symbolic music. These representations are then processed by models. S
    
[^5]: 食谱的深度学习命名实体识别模型

    Deep Learning Based Named Entity Recognition Models for Recipes

    [https://arxiv.org/abs/2402.17447](https://arxiv.org/abs/2402.17447)

    该研究基于深度学习，针对食谱文本开发了命名实体识别模型，通过系统的数据处理和分析，构建了用于自动生成新食谱的数据集。

    

    食物通过各种努力方式影响着我们的生活，包括口味、营养、健康和可持续性。食谱是通过非结构化文本代代相传的文化胶囊。自动识别命名实体的协议，即食谱文本的基本组成部分，对于各种应用来说都具有巨大价值，从信息提取到新颖食谱生成。命名实体识别是一种从已知标签的非结构化或半结构化数据中提取信息的技术。我们从手动注释的6,611个成分短语的数据开始，累积创建了26,445个短语的增强数据集。同时，我们系统地清理和分析了来自RecipeDB的成分短语，这是黄金标准的食谱数据存储库，并使用Stanford NER进行了标注。基于分析，我们使用基于聚类的方法对88,526个短语的子集进行了取样，同时保留了多样性。

    arXiv:2402.17447v1 Announce Type: cross  Abstract: Food touches our lives through various endeavors, including flavor, nourishment, health, and sustainability. Recipes are cultural capsules transmitted across generations via unstructured text. Automated protocols for recognizing named entities, the building blocks of recipe text, are of immense value for various applications ranging from information extraction to novel recipe generation. Named entity recognition is a technique for extracting information from unstructured or semi-structured data with known labels. Starting with manually-annotated data of 6,611 ingredient phrases, we created an augmented dataset of 26,445 phrases cumulatively. Simultaneously, we systematically cleaned and analyzed ingredient phrases from RecipeDB, the gold-standard recipe data repository, and annotated them using the Stanford NER. Based on the analysis, we sampled a subset of 88,526 phrases using a clustering-based approach while preserving the diversity
    
[^6]: BiVRec: 双向基于视图的多模态顺序推荐

    BiVRec: Bidirectional View-based Multimodal Sequential Recommendation

    [https://arxiv.org/abs/2402.17334](https://arxiv.org/abs/2402.17334)

    提出了一个创新框架 BiVRec，在推荐任务中联合训练 ID 和多模态视图，双向增强推荐性能。

    

    多模态信息融入顺序推荐系统近来引起了研究的广泛关注。在多模态顺序推荐模型的初期阶段，主流范式是ID主导推荐，即多模态信息作为辅助信息进行融合。然而，由于其在可转移性和信息侵入方面的局限性，另一种范式出现了，即直接利用多模态特征进行推荐，实现跨数据集的推荐。尽管如此，它忽略了用户ID信息，导致信息利用率低和训练成本高。为此，我们提出了一个创新框架，BiVRec，通过联合训练ID和多模态视图中的推荐任务，利用它们之间的协同关系双向增强推荐性能。为了解决信息异构性问题，我们...

    arXiv:2402.17334v1 Announce Type: cross  Abstract: The integration of multimodal information into sequential recommender systems has attracted significant attention in recent research. In the initial stages of multimodal sequential recommendation models, the mainstream paradigm was ID-dominant recommendations, wherein multimodal information was fused as side information. However, due to their limitations in terms of transferability and information intrusion, another paradigm emerged, wherein multimodal features were employed directly for recommendation, enabling recommendation across datasets. Nonetheless, it overlooked user ID information, resulting in low information utilization and high training costs. To this end, we propose an innovative framework, BivRec, that jointly trains the recommendation tasks in both ID and multimodal views, leveraging their synergistic relationship to enhance recommendation performance bidirectionally. To tackle the information heterogeneity issue, we fir
    
[^7]: DiFashion: 迈向个性化服装生成

    DiFashion: Towards Personalized Outfit Generation

    [https://arxiv.org/abs/2402.17279](https://arxiv.org/abs/2402.17279)

    引入生成式服装推荐任务（GOR），旨在合成一组时尚图片并组装成视觉和谐的、定制给个人用户的服装。

    

    服装推荐（OR）在时尚领域的发展经历了两个不同阶段：预定义的服装推荐和个性化的服装组合。虽然取得了这些进展，但两个阶段都面临现有时尚产品带来的限制，阻碍了它们满足用户多样化时尚需求的有效性。AI生成内容的出现为OR克服这些约束铺平了道路，展示了个性化服装生成的潜力。为了追求这一目标，我们引入了一项名为生成式服装推荐（GOR）的创新任务，其目标是合成一组时尚图片，并将它们组装成视觉和谐的、定制给个人用户的服装。GOR的主要目标集中在实现生成服装的高保真度、兼容性和个性化。为实现这些目标，我们提出了DiFashion，一个生成式服装推荐

    arXiv:2402.17279v1 Announce Type: new  Abstract: The evolution of Outfit Recommendation (OR) in the realm of fashion has progressed through two distinct phases: Pre-defined Outfit Recommendation and Personalized Outfit Composition. Despite these advancements, both phases face limitations imposed by existing fashion products, hindering their effectiveness in meeting users' diverse fashion needs. The emergence of AI-generated content has paved the way for OR to overcome these constraints, demonstrating the potential for personalized outfit generation.   In pursuit of this, we introduce an innovative task named Generative Outfit Recommendation (GOR), with the goal of synthesizing a set of fashion images and assembling them to form visually harmonious outfits customized to individual users. The primary objectives of GOR revolve around achieving high fidelity, compatibility, and personalization of the generated outfits. To accomplish these, we propose DiFashion, a generative outfit recommen
    
[^8]: PromptMM：多模式知识蒸馏用于基于Prompt-Tuning的推荐

    PromptMM: Multi-Modal Knowledge Distillation for Recommendation with Prompt-Tuning

    [https://arxiv.org/abs/2402.17188](https://arxiv.org/abs/2402.17188)

    提出了一种通过Prompt-Tuning赋能的PromptMM多模式知识蒸馏方法，用于简化和增强推荐系统，实现自适应的质量蒸馏。

    

    多媒体在线平台（例如亚马逊、TikTok）通过将多媒体（例如视觉、文本和声学）内容纳入其个性化推荐系统中获益匪浅。这些模态提供直观语义，有助于进行模态感知的用户偏好建模。然而，多模式推荐器中存在两个关键挑战尚未解决：i）引入具有大量额外参数的多模式编码器会导致过拟合，考虑到提取器（例如ViT、BERT）提供的高维多模式特征。ii）辅助信息不可避免地引入不准确性和冗余，导致模态交互依赖偏离真实用户偏好。为了解决这些问题，我们提出通过Prompt-Tuning赋能、简化推荐器的PromptMM（多模式知识蒸馏），实现自适应质量蒸馏。

    arXiv:2402.17188v1 Announce Type: new  Abstract: Multimedia online platforms (e.g., Amazon, TikTok) have greatly benefited from the incorporation of multimedia (e.g., visual, textual, and acoustic) content into their personal recommender systems. These modalities provide intuitive semantics that facilitate modality-aware user preference modeling. However, two key challenges in multi-modal recommenders remain unresolved: i) The introduction of multi-modal encoders with a large number of additional parameters causes overfitting, given high-dimensional multi-modal features provided by extractors (e.g., ViT, BERT). ii) Side information inevitably introduces inaccuracies and redundancies, which skew the modality-interaction dependency from reflecting true user preference. To tackle these problems, we propose to simplify and empower recommenders through Multi-modal Knowledge Distillation (PromptMM) with the prompt-tuning that enables adaptive quality distillation. Specifically, PromptMM cond
    
[^9]: 行动胜过言辞：用于生成推荐的千亿参数顺序转导器

    Actions Speak Louder than Words: Trillion-Parameter Sequential Transducers for Generative Recommendations

    [https://arxiv.org/abs/2402.17152](https://arxiv.org/abs/2402.17152)

    提出了HSTU架构，用于高基数、非平稳流推荐数据，性能优于基线方法高达65.8％的NDCG，并且比基于FlashAttention2的Transformer在8192长度序列上快5.3倍到15.2倍。

    

    大规模推荐系统的特点是依赖于高基数、异构特征，并且需要每天处理数十亿用户行为。尽管在成千上万个特征上训练了大量数据，但大多数行业中的深度学习推荐模型(DLRMs)在计算方面无法扩展。受到在语言和视觉领域取得成功的Transformer的启发，我们重新审视了推荐系统中的基本设计选择。我们将推荐问题重新构建为生成建模框架中的顺序转导任务（“生成推荐者”），并提出了一种针对高基数、非平稳流推荐数据设计的新架构HSTU。

    arXiv:2402.17152v1 Announce Type: new  Abstract: Large-scale recommendation systems are characterized by their reliance on high cardinality, heterogeneous features and the need to handle tens of billions of user actions on a daily basis. Despite being trained on huge volume of data with thousands of features, most Deep Learning Recommendation Models (DLRMs) in industry fail to scale with compute.   Inspired by success achieved by Transformers in language and vision domains, we revisit fundamental design choices in recommendation systems. We reformulate recommendation problems as sequential transduction tasks within a generative modeling framework (``Generative Recommenders''), and propose a new architecture, HSTU, designed for high cardinality, non-stationary streaming recommendation data.   HSTU outperforms baselines over synthetic and public datasets by up to 65.8\% in NDCG, and is 5.3x to 15.2x faster than FlashAttention2-based Transformers on 8192 length sequences. HSTU-based Gener
    
[^10]: 基于辅助信息的基于会话推荐：一项调查

    Side Information-Driven Session-based Recommendation: A Survey

    [https://arxiv.org/abs/2402.17129](https://arxiv.org/abs/2402.17129)

    这项调查全面回顾了基于辅助信息驱动的基于会话推荐研究，探讨了多样的辅助信息如何增强推荐系统性能，分析了最新和代表性的研究进展。

    

    基于会话的推荐（SBR）由于其在有限交互中预测匿名用户意图的能力而受到越来越多的关注。新兴的工作将各种辅助信息纳入其方法中，以提高任务性能。在这项调查中，我们从数据中心的角度对基于辅助信息驱动的基于会话推荐进行了全面回顾。我们首先阐述了这一研究课题背后的动机和必要性。然后，我们详细探讨了丰富辅助信息的各种基准数据集，这对推进这一领域的研究至关重要。此外，我们深入探讨了这些不同类型的辅助信息如何增强SBR，并强调它们的特点和实用性。随后，我们对研究进展进行了系统性回顾，分析了这一主题中最近和具有代表性的发展。最后，我们提出了未来的展望。

    arXiv:2402.17129v1 Announce Type: new  Abstract: The session-based recommendation (SBR) garners increasing attention due to its ability to predict anonymous user intents within limited interactions. Emerging efforts incorporate various kinds of side information into their methods for enhancing task performance. In this survey, we thoroughly review the side information-driven session-based recommendation from a data-centric perspective. Our survey commences with an illustration of the motivation and necessity behind this research topic. This is followed by a detailed exploration of various benchmarks rich in side information, pivotal for advancing research in this field. Moreover, we delve into how these diverse types of side information enhance SBR, underscoring their characteristics and utility. A systematic review of research progress is then presented, offering an analysis of the most recent and representative developments within this topic. Finally, we present the future prospects 
    
[^11]: 通过量化影响度作为人工智能评委的精细调优增强版RAG系统

    A Fine-tuning Enhanced RAG System with Quantized Influence Measure as AI Judge

    [https://arxiv.org/abs/2402.17081](https://arxiv.org/abs/2402.17081)

    本研究通过将经过微调的大型语言模型与向量数据库集成，引入了LoRA和QLoRA方法，并直接将用户反馈纳入训练过程，同时引入量化影响度作为“Innovative AI Judge”机制，以提高系统的精确性。

    

    本研究提出了一种创新的增强检索增强生成（RAG）系统的方法，通过将经过微调的大型语言模型（LLMs）与向量数据库无缝集成。这种集成利用了结构化数据检索的优势和先进LLMs提供的细致理解能力。我们方法的核心是LoRA和QLoRA方法，它们处于模型细化的前沿，通过参数高效微调和内存优化提高模型的性能。我们研究的一个新特性是直接将用户反馈纳入训练过程，确保模型持续适应用户期望，从而提高其性能和适用性。此外，我们引入了量化影响度（QIM）作为一种创新的“AI评委”机制，增强结果选择的精度，进一步提高系统的准确性。伴随着一个执行图和

    arXiv:2402.17081v1 Announce Type: new  Abstract: This study presents an innovative enhancement to retrieval-augmented generation (RAG) systems by seamlessly integrating fine-tuned large language models (LLMs) with vector databases. This integration capitalizes on the combined strengths of structured data retrieval and the nuanced comprehension provided by advanced LLMs. Central to our approach are the LoRA and QLoRA methodologies, which stand at the forefront of model refinement through parameter-efficient fine-tuning and memory optimization. A novel feature of our research is the incorporation of user feedback directly into the training process, ensuring the model's continuous adaptation to user expectations and thus, improving its performance and applicability. Additionally, we introduce a Quantized Influence Measure (QIM) as an innovative "AI Judge" mechanism to enhance the precision of result selection, further refining the system's accuracy. Accompanied by an executive diagram and
    
[^12]: 多任务对比学习用于8192标记的双语文本嵌入

    Multi-Task Contrastive Learning for 8192-Token Bilingual Text Embeddings

    [https://arxiv.org/abs/2402.17016](https://arxiv.org/abs/2402.17016)

    通过引入独特的多任务学习目标，研究者设计了用于支持英语和其他目标语言的最先进的双语文本嵌入模型，显著提高了模型在STS任务上的表现，同时在目标语言理解和跨语言评估任务中超越了现有多语言模型。

    

    我们引入了一套新颖的最先进的双语文本嵌入模型，旨在支持英语和另一种目标语言。这些模型能够处理长达8192个标记的文本输入，因此非常适用于一系列自然语言处理任务，如文本检索、聚类和语义文本相似度（STS）计算。通过专注于双语模型并引入独特的多任务学习目标，我们显著改善了在STS任务上的模型表现，超过了现有多语言模型在目标语言理解和跨语言评估任务方面的能力。此外，我们的双语模型更加高效，需要较少的参数和更少的内存，因为它们需要较小的词汇量。此外，我们还扩展了大规模文本嵌入基准（MTEB），包括德语和西班牙语嵌入的基准。

    arXiv:2402.17016v1 Announce Type: cross  Abstract: We introduce a novel suite of state-of-the-art bilingual text embedding models that are designed to support English and another target language. These models are capable of processing lengthy text inputs with up to 8192 tokens, making them highly versatile for a range of natural language processing tasks such as text retrieval, clustering, and semantic textual similarity (STS) calculations.   By focusing on bilingual models and introducing a unique multi-task learning objective, we have significantly improved the model performance on STS tasks, which outperforms the capabilities of existing multilingual models in both target language understanding and cross-lingual evaluation tasks. Moreover, our bilingual models are more efficient, requiring fewer parameters and less memory due to their smaller vocabulary needs. Furthermore, we have expanded the Massive Text Embedding Benchmark (MTEB) to include benchmarks for German and Spanish embed
    
[^13]: 长对话摘要: 一项分析

    Long Dialog Summarization: An Analysis

    [https://arxiv.org/abs/2402.16986](https://arxiv.org/abs/2402.16986)

    长对话摘要的研究强调了在各种应用中为有效沟通创造连贯和上下文丰富摘要的重要性。

    

    Dialog summarization在跨越不同领域的大规模对话中变得越来越重要。这项任务在捕捉多轮长对话的关键点、上下文和细微差别方面面临着独特挑战。本研究强调了在各种应用中为有效沟通创造连贯和上下文丰富摘要的重要性。我们探讨了不同领域长对话摘要的当前最先进方法，基于基准指标的评估显示一个单一模型在各种场景下的表现并不理想。

    arXiv:2402.16986v1 Announce Type: new  Abstract: Dialog summarization has become increasingly important in managing and comprehending large-scale conversations across various domains. This task presents unique challenges in capturing the key points, context, and nuances of multi-turn long conversations for summarization. It is worth noting that the summarization techniques may vary based on specific requirements such as in a shopping-chatbot scenario, the dialog summary helps to learn user preferences, whereas in the case of a customer call center, the summary may involve the problem attributes that a user specified, and the final resolution provided. This work emphasizes the significance of creating coherent and contextually rich summaries for effective communication in various applications. We explore current state-of-the-art approaches for long dialog summarization in different domains and benchmark metrics based evaluations show that one single model does not perform well across va
    
[^14]: 使用人类概念形成避免视觉分类中的灾难性遗忘

    Avoiding Catastrophic Forgetting in Visual Classification Using Human Concept Formation

    [https://arxiv.org/abs/2402.16933](https://arxiv.org/abs/2402.16933)

    提出了一种名为Cobweb4V的新颖视觉分类方法，利用人类类似学习系统，避免了灾难性遗忘效应，与传统方法相比，需要更少的数据来实现有效学习成果，并保持稳定性能。

    

    深度神经网络在机器学习中表现出色，特别是在视觉任务中，然而，当按顺序学习新任务时，它们经常面临灾难性遗忘。本研究提出了Cobweb4V，这是一种新颖的视觉分类方法，它基于Cobweb，这是一种人类类似的学习系统，受到人类随时间逐渐学习新概念的启发。我们进行了全面评估，展示了Cobweb4V在学习视觉概念方面的熟练程度，相较于传统方法，需要更少的数据来实现有效的学习成果，随时间保持稳定的性能，并实现了令人称赞的渐近行为，避免了灾难性遗忘效应。这些特征与人类认知中的学习策略一致，将Cobweb4V定位为神经网络方法的一个有前途的替代方案。

    arXiv:2402.16933v1 Announce Type: cross  Abstract: Deep neural networks have excelled in machine learning, particularly in vision tasks, however, they often suffer from catastrophic forgetting when learning new tasks sequentially. In this work, we propose Cobweb4V, a novel visual classification approach that builds on Cobweb, a human like learning system that is inspired by the way humans incrementally learn new concepts over time. In this research, we conduct a comprehensive evaluation, showcasing the proficiency of Cobweb4V in learning visual concepts, requiring less data to achieve effective learning outcomes compared to traditional methods, maintaining stable performance over time, and achieving commendable asymptotic behavior, without catastrophic forgetting effects. These characteristics align with learning strategies in human cognition, positioning Cobweb4V as a promising alternative to neural network approaches.
    
[^15]: 使用文本嵌入模型和向量数据库作为文本分类器的研究，以医疗数据为例

    Using text embedding models and vector databases as text classifiers with the example of medical data

    [https://arxiv.org/abs/2402.16886](https://arxiv.org/abs/2402.16886)

    向量数据库和嵌入模型的应用为文本分类器提供了强大的方式来表达数据模式，特别是在医疗领域中开始有着广泛的应用。

    

    大型语言模型（LLMs）的出现是令人兴奋的，并已在许多领域找到应用，但通常情况下，医学领域的标准要求非常高。与LLMs配合使用，向量嵌入模型和向量数据库提供了一种强大的方式来表达各种数据模式，这些数据模式容易被典型的机器学习模型所理解。除了方便地向这些向量数据库添加信息、知识和数据外，它们还提供了一个令人信服的理由，即将其应用于通常由人类完成的检索信息任务的各种领域。Google的研究人员开发了一个清晰的替代模型Med-PaLM，专门旨在与临床医师的医学知识水平匹配。在训练分类器和开发模型时，保持事实和减少偏见是至关重要的。在本文中，我们探讨了向量数据库和嵌入模型的应用

    arXiv:2402.16886v1 Announce Type: cross  Abstract: The advent of Large Language Models (LLMs) is promising and has found application in numerous fields, but as it often is with the medical field, the bar is typically quite high [5]. In tandem with LLMs, vector embedding models and vector databases provide a robust way of expressing numerous modes of data that are easily digestible by typical machine learning models. Along with the ease of adding information, knowledge, and data to these vector databases, they provide a compelling reason to apply them in numerous fields where the task of retrieving information is typically done by humans. Researchers at Google have developed a clear alternative model, Med-PaLM [6] specifically designed to match a clinician's level of accuracy when it comes to medical knowledge. When training classifiers, and developing models, it is imperative to maintain factuality and reduce bias [4]. Here, we explore the use of vector databases and embedding models a
    
[^16]: 大型语言模型增强的个性化语言学习练习检索

    Large Language Model Augmented Exercise Retrieval for Personalized Language Learning

    [https://arxiv.org/abs/2402.16877](https://arxiv.org/abs/2402.16877)

    大型语言模型利用生成能力来合成假设练习，以弥合学习者需求与练习内容之间的语义鸿沟，提高个性化语言学习练习检索效果。

    

    我们研究了在线语言学习环境中的零样本练习检索问题，以赋予学习者通过自然语言明确请求个性化练习的能力。通过收集自语言学习者的真实数据，我们观察到矢量相似性方法很难捕捉练习内容与学习者用于表达他们想要学习内容的语言之间的关系。我们利用大型语言模型的生成能力来弥合这一差距，通过基于学习者输入合成假设练习，然后用于搜索相关练习。我们的方法mHyER克服了三个挑战：（1）缺乏用于训练的相关性标签，（2）受限的学习

    arXiv:2402.16877v1 Announce Type: cross  Abstract: We study the problem of zero-shot exercise retrieval in the context of online language learning, to give learners the ability to explicitly request personalized exercises via natural language. Using real-world data collected from language learners, we observe that vector similarity approaches poorly capture the relationship between exercise content and the language that learners use to express what they want to learn. This semantic gap between queries and content dramatically reduces the effectiveness of general-purpose retrieval models pretrained on large scale information retrieval datasets like MS MARCO. We leverage the generative capabilities of large language models to bridge the gap by synthesizing hypothetical exercises based on the learner's input, which are then used to search for relevant exercises. Our approach, which we call mHyER, overcomes three challenges: (1) lack of relevance labels for training, (2) unrestricted learn
    
[^17]: 高级学术团队成员推荐模型

    Advanced Academic Team Worker Recommendation Models

    [https://arxiv.org/abs/2402.16876](https://arxiv.org/abs/2402.16876)

    提出了一种新任务：学术团队成员推荐模型，通过结合查询和论文上下文与图拓扑结构，形成适用于特定研究兴趣和任务的新图（CQBG-R），实验结果表明这种方法的有效性。

    

    合作伙伴推荐是学术领域中的重要任务。现有方法大多假设推荐系统仅需为任务推荐特定的研究人员。然而，学术成功可能归功于整个学术团队的高效合作。在这项工作中，我们提出了一个新任务：学术团队成员推荐。根据给定的身份（学生，助理教授或主教授）、研究兴趣和特定任务，我们可以推荐一个由（主教授，助理教授，学生）组成的学术团队。为了完成这个任务，我们提出了一个名为CQBG-R（引用-查询混合图-排名）的模型。其关键思想是将查询和论文的上下文与图拓扑结构结合起来形成一个新的图（CQBG），这个图可以针对这次的研究兴趣和特定研究任务。实验结果展示了该方法的有效性。

    arXiv:2402.16876v1 Announce Type: cross  Abstract: Collaborator recommendation is an important task in academic domain. Most of the existing approaches have the assumption that the recommendation system only need to recommend a specific researcher for the task. However, academic successes can be owed to productive collaboration of a whole academic team. In this work, we propose a new task: academic team worker recommendation: with a given status: student, assistant professor or prime professor, research interests and specific task, we can recommend an academic team formed as (prime professor, assistant professor, student). For this task, we propose a model CQBG-R(Citation-Query Blended Graph-Ranking). The key ideas is to combine the context of the query and the papers with the graph topology to form a new graph(CQBG), which can target at the research interests and the specific research task for this time. The experiment results show the effectiveness of the proposed method.
    
[^18]: 我们能够预测QPP吗？基于多变量离群值的方法

    Can we predict QPP? An approach based on multivariate outliers

    [https://arxiv.org/abs/2402.16875](https://arxiv.org/abs/2402.16875)

    本文通过研究影响查询性能预测准确性的因素，提出了一个基于多变量离群值的方法，有效识别出QPP表现不佳的查询。

    

    arXiv:2402.16875v1  公告类型：新  摘要：查询性能预测（QPP）旨在预测搜索引擎在一系列查询和文档中的有效性。尽管最先进的预测器提供了一定水平的精度，但它们的准确性并不完美。先前的研究已经意识到了QPP固有的挑战，但往往缺乏彻底的定性分析。在本文中，我们通过研究影响查询性能准确性可预测性的因素来深入探讨QPP。我们提出工作假设，即虽然有些查询很容易预测，但其他一些则面临显著挑战。通过关注离群值，我们旨在识别那些特别具有挑战性的查询。为此，我们采用多变量离群值检测方法。我们的结果表明，这种方法在识别QPP表现不佳的查询方面非常有效，产生了较不可靠的预测。此外，我们提供证据表明排除掉

    arXiv:2402.16875v1 Announce Type: new  Abstract: Query performance prediction (QPP) aims to forecast the effectiveness of a search engine across a range of queries and documents. While state-of-the-art predictors offer a certain level of precision, their accuracy is not flawless. Prior research has recognized the challenges inherent in QPP but often lacks a thorough qualitative analysis. In this paper, we delve into QPP by examining the factors that influence the predictability of query performance accuracy. We propose the working hypothesis that while some queries are readily predictable, others present significant challenges. By focusing on outliers, we aim to identify the queries that are particularly challenging to predict. To this end, we employ multivariate outlier detection method. Our results demonstrate the effectiveness of this approach in identifying queries on which QPP do not perform well, yielding less reliable predictions. Moreover, we provide evidence that excluding the
    
[^19]: 通过增强查询提升语言生成的检索过程

    Enhancing Retrieval Processes for Language Generation with Augmented Queries

    [https://arxiv.org/abs/2402.16874](https://arxiv.org/abs/2402.16874)

    本研究通过检索增强生成（RAG）技术解决了语言生成中“幻觉”问题，并借助查询优化过程将用户查询与高级语言模型连接，显著提升了模型性能。

    

    在智能技术日新月异的世界中，由于先进语言模型的崛起，搜索文档变得更具挑战性。这些模型有时会面临困难，比如提供不准确的信息，通常被称为“幻觉”。本研究致力于通过检索增强生成（RAG）技术解决这一问题，该技术指导模型基于真实事实提供准确答复。为了解决可伸缩性问题，研究探讨了将用户查询与诸如BERT和Orca2等复杂语言模型连接起来的创新查询优化过程。研究展开在三种情境中：首先，没有RAG，其次，没有额外帮助，最后，加入额外帮助。选择紧凑而高效的Orca2 7B模型展示出对计算资源的智能使用。实证结果表明，初始语言模型的性能有了显著提升。

    arXiv:2402.16874v1 Announce Type: cross  Abstract: In the rapidly changing world of smart technology, searching for documents has become more challenging due to the rise of advanced language models. These models sometimes face difficulties, like providing inaccurate information, commonly known as "hallucination." This research focuses on addressing this issue through Retrieval-Augmented Generation (RAG), a technique that guides models to give accurate responses based on real facts. To overcome scalability issues, the study explores connecting user queries with sophisticated language models such as BERT and Orca2, using an innovative query optimization process. The study unfolds in three scenarios: first, without RAG, second, without additional assistance, and finally, with extra help. Choosing the compact yet efficient Orca2 7B model demonstrates a smart use of computing resources. The empirical results indicate a significant improvement in the initial language model's performance unde
    
[^20]: NFT1000：用于非同质化代币检索的视觉文本数据集

    NFT1000: A Visual Text Dataset For Non-Fungible Token Retrieval

    [https://arxiv.org/abs/2402.16872](https://arxiv.org/abs/2402.16872)

    提出了一个名为NFT1000的视觉文本数据集，其中包含756万个图像文本对，来自销量最高的1000个PFP NFT收藏品，用于解决NFT检索中的准确和高效问题。

    

    随着“元宇宙”和“Web3.0”的兴起，NFT（非同质化代币）作为一种关键数字资产崭露头角，引起了极大关注。到2023年11月底，在各种区块链平台上铸造了超过14亿个NFT代币。为了有效地定位一个令人满意的NFT代币，搜索广泛的NFT数据是必不可少的。在区域和语义方面，不同NFT代币之间存在相当高的相似度，这增加了NFT检索的挑战。在大规模、高度相似的NFT数据中实现准确和高效的检索对学术界和工业界都是一项艰巨的挑战。本文介绍了一个名为“NFT Top1000视觉文本数据集”（以下简称NFT1000）的数据集，包含756万个图像文本对，数据来自以销售量为标准的前1000个最著名的PFP NFT收藏品。

    arXiv:2402.16872v1 Announce Type: new  Abstract: With the rise of 'Metaverse' and 'Web3.0', NFT ( Non-Fungible Token ) has emerged as a kind of pivotal digital asset, garnering significant attention. By the end of November 2023, more than 1.4 billion NFT tokens have been minted across various blockchain platforms. To effectively locate a satisfactory NFT token, conducting searches within the extensive array of NFT data is essential. The challenge in NFT retrieval is heightened due to the high degree of similarity among different NFT tokens, in terms of regional and semantic aspects. Achieving accurate and efficient retrieval within the large-scale, highly similar NFT data presents a formidable challenge for both the academic and industrial communities. In this paper, we will introduce a dataset named 'NFT Top1000 Visual Text Dataset'(henceforth, NFT1000), containing 7.56 million image-text pairs, and being collected from 1000 most famous PFP NFT collections by sales volume on the Ether
    
[^21]: 基于可解释机器学习的交互式火星图像内容搜索

    Interactive Mars Image Content-Based Search with Interpretable Machine Learning

    [https://arxiv.org/abs/2402.16860](https://arxiv.org/abs/2402.16860)

    提出了一种基于可解释机器学习的交互式火星图像内容搜索方法，可以帮助用户理解和验证分类器使用的证据，并替换现有的不可解释图像搜索系统。

    

    NASA行星数据系统(PDS)托管着数百万张不同任务期间收集的行星、卫星和其他天体的图像。数据的不断扩展和用户参与的需求促使我们建立一个可解释的内容分类系统，以支持科学发现和个人好奇心。本文利用基于原型的架构，使用户能够理解并验证火星科学实验室(MSL)好奇号任务图像上训练的分类器所用的证据。除了提供解释，我们还研究了基于内容的分类器所使用的证据的多样性和正确性。本文提出的工作将部署在PDS图像图集上，取代其不可解释的对应物。

    arXiv:2402.16860v1 Announce Type: cross  Abstract: The NASA Planetary Data System (PDS) hosts millions of images of planets, moons, and other bodies collected throughout many missions. The ever-expanding nature of data and user engagement demands an interpretable content classification system to support scientific discovery and individual curiosity. In this paper, we leverage a prototype-based architecture to enable users to understand and validate the evidence used by a classifier trained on images from the Mars Science Laboratory (MSL) Curiosity rover mission. In addition to providing explanations, we investigate the diversity and correctness of evidence used by the content-based classifier. The work presented in this paper will be deployed on the PDS Image Atlas, replacing its non- interpretable counterpart.
    
[^22]: 一种强大的网络安全主题分类工具

    A Robust Cybersecurity Topic Classification Tool

    [https://arxiv.org/abs/2109.02473](https://arxiv.org/abs/2109.02473)

    通过使用多数投票的方式，研究提出了一种网络安全主题分类工具，相比于21个单独模型，该工具在检测网络安全相关文本时表现出较低的假阳性和假阴性率，并且能够处理数十万份文档。

    

    在这项研究中，我们使用来自三个互联网文本信息来源（Reddit，Stackexchange，Arxiv）的用户定义的标签，训练了21种不同的机器学习模型，用于检测自然文本中的网络安全讨论的主题分类任务。我们分析了每个模型在交叉验证实验中的假阳性和假阴性率。然后，我们提出了一种网络安全主题分类（CTC）工具，该工具将21个训练好的机器学习模型的多数投票作为检测网络安全相关文本的决策机制。我们还展示了CTC工具的多数投票机制平均提供了比21个单独模型更低的假阴性和假阳性率。我们展示了CTC工具可扩展到数十万份文档，而其墙钟时间大约为几小时。

    arXiv:2109.02473v4 Announce Type: replace-cross  Abstract: In this research, we use user defined labels from three internet text sources (Reddit, Stackexchange, Arxiv) to train 21 different machine learning models for the topic classification task of detecting cybersecurity discussions in natural text. We analyze the false positive and false negative rates of each of the 21 model's in a cross validation experiment. Then we present a Cybersecurity Topic Classification (CTC) tool, which takes the majority vote of the 21 trained machine learning models as the decision mechanism for detecting cybersecurity related text. We also show that the majority vote mechanism of the CTC tool provides lower false negative and false positive rates on average than any of the 21 individual models. We show that the CTC tool is scalable to the hundreds of thousands of documents with a wall clock time on the order of hours.
    
[^23]: 生成式查询和文档扩展何时失败？方法、检索器和数据集的全面研究

    When do Generative Query and Document Expansions Fail? A Comprehensive Study Across Methods, Retrievers, and Datasets. (arXiv:2309.08541v1 [cs.IR])

    [http://arxiv.org/abs/2309.08541](http://arxiv.org/abs/2309.08541)

    通过对11种扩展技术、12个不同分布变化的数据集和24个检索模型的全面分析，我们发现使用大型语言模型进行查询或文档扩展的效果与检索器性能相关，对于弱模型来说扩展提高了分数，但对于强模型来说扩展通常会损害分数。

    

    使用大型语言模型（LM）进行查询或文档扩展可以改善信息检索中的泛化能力。然而，目前尚不清楚这些技术是否普遍有益，还是仅在特定设置下有效，例如对于特定的检索模型、数据集领域或查询类型。为了回答这个问题，我们进行了第一次对基于LM的扩展的全面分析。我们发现，检索器性能与扩展的增益之间存在强烈的负相关关系：扩展改善了较弱模型的分数，但通常会损害较强模型的分数。我们展示了这一趋势在11种扩展技术、12个具有不同分布变化的数据集和24个检索模型的一组实验中成立。通过定性错误分析，我们提出了一个假设，即尽管扩展提供了额外的信息（可能改善了召回率），但它们也增加了噪声，使得很难区分出顶级相关文档（从而引入了错误的正例）

    Using large language models (LMs) for query or document expansion can improve generalization in information retrieval. However, it is unknown whether these techniques are universally beneficial or only effective in specific settings, such as for particular retrieval models, dataset domains, or query types. To answer this, we conduct the first comprehensive analysis of LM-based expansion. We find that there exists a strong negative correlation between retriever performance and gains from expansion: expansion improves scores for weaker models, but generally harms stronger models. We show this trend holds across a set of eleven expansion techniques, twelve datasets with diverse distribution shifts, and twenty-four retrieval models. Through qualitative error analysis, we hypothesize that although expansions provide extra information (potentially improving recall), they add additional noise that makes it difficult to discern between the top relevant documents (thus introducing false positiv
    
[^24]: NevIR: 神经信息检索中的否定

    NevIR: Negation in Neural Information Retrieval. (arXiv:2305.07614v1 [cs.IR])

    [http://arxiv.org/abs/2305.07614](http://arxiv.org/abs/2305.07614)

    本研究探讨了否定在神经信息检索中的影响，构建了基准模型，结果表明当前信息检索模型大多数都没有考虑否定，而交叉编码器是目前表现最好的架构。

    

    否定是一种常见而日常化的现象，也一直是语言模型的一个弱点。虽然信息检索领域采用了语言模型作为现代化架构的主干，但几乎没有研究深入了解否定对神经信息检索的影响。因此，我们构建了一个简单的基准来研究这个主题：要求信息检索模型对仅仅因为是否定而不同的两个文档进行排名。我们发现，结果根据不同的信息检索架构而有很大差异：交叉编码器表现最好，后期交互模型次之，而双编码器和稀疏神经架构排名最后。我们发现，大多数当前的信息检索模型都没有考虑否定，表现与随机排名相似或更差。我们证明，尽管在一个包含否定对照文档的数据集上继续微调明显的方法可以提高性能（模型大小也是如此），但是机器和人之间仍存在很大的差距。

    Negation is a common everyday phenomena and has been a consistent area of weakness for language models (LMs). Although the Information Retrieval (IR) community has adopted LMs as the backbone of modern IR architectures, there has been little to no research in understanding how negation impacts neural IR. We therefore construct a straightforward benchmark on this theme: asking IR models to rank two documents that differ only by negation. We show that the results vary widely according to the type of IR architecture: cross-encoders perform best, followed by late-interaction models, and in last place are bi-encoder and sparse neural architectures. We find that most current information retrieval models do not consider negation, performing similarly or worse than randomly ranking. We show that although the obvious approach of continued fine-tuning on a dataset of contrastive documents containing negations increases performance (as does model size), there is still a large gap between machine 
    
[^25]: 在开放域问答中防御误导性攻击

    Defending Against Misinformation Attacks in Open-Domain Question Answering. (arXiv:2212.10002v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.10002](http://arxiv.org/abs/2212.10002)

    本文提出了一种使用查询扩充来搜索冗余信息、并通过新颖的置信度方法将其集成到模型中的方法，可以有效防御开放域问答系统中的污染攻击，精确匹配率可提高近20%。

    

    最近在开放域问答领域中的研究表明，对于搜索集合进行的敌对污染可能会导致生产系统的精度大幅下降。然而，几乎没有工作提出防御这些攻击的方法。为了解决这个问题，我们依赖于大型语料库中存在冗余信息的直觉。为了找到这些信息，我们引入了一种使用查询扩充来搜索可能回答原始问题的多样化段落集合的方法，但是不太可能被污染。我们通过设计一种新型的置信度方法（比较预测答案与其在检索到的上下文中出现的情况——我们称之为答案冗余置信度，即CAR）将这些新段落集成到模型中。这些方法共同构成了一种简单但有效的方式，用于防御污染攻击，可在不同水平的数据污染/知识冲突下提供近20％的精确匹配增益。

    Recent work in open-domain question answering (ODQA) has shown that adversarial poisoning of the search collection can cause large drops in accuracy for production systems. However, little to no work has proposed methods to defend against these attacks. To do so, we rely on the intuition that redundant information often exists in large corpora. To find it, we introduce a method that uses query augmentation to search for a diverse set of passages that could answer the original question but are less likely to have been poisoned. We integrate these new passages into the model through the design of a novel confidence method, comparing the predicted answer to its appearance in the retrieved contexts (what we call \textit{Confidence from Answer Redundancy}, i.e. CAR). Together these methods allow for a simple but effective way to defend against poisoning attacks that provides gains of nearly 20\% exact match across varying levels of data poisoning/knowledge conflicts.
    

