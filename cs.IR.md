# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [SPLADE-v3: New baselines for SPLADE](https://arxiv.org/abs/2403.06789) | SPLADE-v3相对于BM25和SPLADE++在效果上更为显著，同时与交叉编码器重新排序器相比较出色，具有更高的性能表现。 |
| [^2] | [MetaSplit: Meta-Split Network for Limited-Stock Product Recommendation](https://arxiv.org/abs/2403.06747) | 提出了Meta-Split网络（MSN）来解决消费者之间电子商务平台中限量库存产品推荐中的独特挑战，通过分割用户历史序列来有效利用用户历史信息。 |
| [^3] | [Post-Training Attribute Unlearning in Recommender Systems](https://arxiv.org/abs/2403.06737) | 本文提出了一种后训练属性取消学习（PoT-AU）的方法，通过设计两部分损失函数，旨在在推荐系统中保护用户的敏感属性。 |
| [^4] | [Emergency Response Inference Mapping (ERIMap): A Bayesian Network-based Method for Dynamic Observation Processing in Spatially Distributed Emergencies](https://arxiv.org/abs/2403.06716) | ERIMap是一种基于贝叶斯网络的方法，旨在为紧急情况中的决策提供系统性和迅速的信息处理，减少决策者的认知负荷 |
| [^5] | [KELLMRec: Knowledge-Enhanced Large Language Models for Recommendation](https://arxiv.org/abs/2403.06642) | 提出了一种知识增强的大型语言模型用于推荐的方法，通过使用外部知识来帮助生成真实可用的文本，并包括知识为基础的对比学习方案进行训练。 |
| [^6] | [Leveraging Foundation Models for Content-Based Medical Image Retrieval in Radiology](https://arxiv.org/abs/2403.06567) | 基于内容的医学图像检索中，利用基础模型作为特征提取器，无需微调即可取得与专门模型竞争的性能，尤其在检索病理特征方面具有较大困难。 |
| [^7] | [ToolRerank: Adaptive and Hierarchy-Aware Reranking for Tool Retrieval](https://arxiv.org/abs/2403.06551) | ToolRerank是一种自适应和层次感知的重新排名方法，用于工具检索，通过Adaptive Truncation和Hierarchy-Aware Reranking来优化检索结果。 |
| [^8] | [RecAI: Leveraging Large Language Models for Next-Generation Recommender Systems](https://arxiv.org/abs/2403.06465) | 本文介绍了RecAI，一个旨在通过大型语言模型增强推荐系统的实用工具包，新一代由LLMs赋能的推荐系统将更加多才多艺、可解释、对话式和可控，为更智能和用户中心的推荐体验铺平道路。 |
| [^9] | [CoRAL: Collaborative Retrieval-Augmented Large Language Models Improve Long-tail Recommendation](https://arxiv.org/abs/2403.06447) | CoRAL引入了协作检索增强LLMs，将协作证据直接纳入到推理过程中，从而更好地理解用户之间的偏好，改进长尾推荐。 |
| [^10] | [Repeated Padding as Data Augmentation for Sequential Recommendation](https://arxiv.org/abs/2403.06372) | 本文提出了一种名为"RepPad"的简单而有效的填充方法，旨在充分利用填充空间来提高顺序推荐模型的性能和训练效率。 |
| [^11] | [Editing Conceptual Knowledge for Large Language Models](https://arxiv.org/abs/2403.06259) | 该论文首次研究了为大型语言模型编辑概念知识，通过构建基准数据集和建立新评估指标，发现现有方法虽然能一定程度上修改概念定义，但也可能造成LLMs中相关实例知识的扭曲，导致性能下降。 |
| [^12] | [TRAD: Enhancing LLM Agents with Step-Wise Thought Retrieval and Aligned Decision](https://arxiv.org/abs/2403.06221) | 提出了TRAD框架，通过步骤式思维检索和对齐决策解决了利用上下文示例时可能出现的问题。 |
| [^13] | [Can LLM Substitute Human Labeling? A Case Study of Fine-grained Chinese Address Entity Recognition Dataset for UAV Delivery](https://arxiv.org/abs/2403.06097) | 提出了适用于无人机交付系统中地址解析任务的细粒度中文姓名实体识别数据集CNER-UAV，包含五个类别的多样化数据，经过严格的数据清洗和去敏处理，约有12,000个标注样本，评估了传统的实体识别模型并提供了深入分析 |
| [^14] | [Bit-mask Robust Contrastive Knowledge Distillation for Unsupervised Semantic Hashing](https://arxiv.org/abs/2403.06071) | 本文提出了一种创新的位掩码健壮对比度知识蒸馏（BRCD）方法，专门为语义哈希模型的蒸馏而设计，以取得更好的性能。 |
| [^15] | [Hierarchical Query Classification in E-commerce Search](https://arxiv.org/abs/2403.06021) | 提出了一个利用分层信息的新框架，以增强表示学习，解决了分层查询分类中的类别不平衡和查询模糊性带来的挑战。 |
| [^16] | [LEGION: Harnessing Pre-trained Language Models for GitHub Topic Recommendations with Distribution-Balance Loss](https://arxiv.org/abs/2403.05873) | 提出了一种名为Legion的新方法，利用预训练语言模型（PTMs）为GitHub存储库推荐主题，以解决现有技术在主题推荐中的局限性。 |
| [^17] | [CFaiRLLM: Consumer Fairness Evaluation in Large-Language Model Recommender System](https://arxiv.org/abs/2403.05668) | 这项研究引入了一个全面的评估框架 CFaiRLLM，旨在评估和减轻 RecLLMs 中消费者端的偏见 |
| [^18] | [Chaining text-to-image and large language model: A novel approach for generating personalized e-commerce banners](https://arxiv.org/abs/2403.05578) | 本研究提出了一种新方法，利用文本到图像模型和大型语言模型生成个性化网页横幅，根据用户互动动态内容，并且无需人工干预。 |
| [^19] | [Monitoring the evolution of antisemitic discourse on extremist social media using BERT](https://arxiv.org/abs/2403.05548) | 本研究提出了一种使用BERT监测极端社交媒体上反犹太主义话语演变的自动方法，避免了手动监测的不可行性，为干预和防止仇恨升级提供了新途径。 |
| [^20] | [PromptMM: Multi-Modal Knowledge Distillation for Recommendation with Prompt-Tuning](https://arxiv.org/abs/2402.17188) | 提出了一种通过Prompt-Tuning赋能的PromptMM多模式知识蒸馏方法，用于简化和增强推荐系统，实现自适应的质量蒸馏。 |
| [^21] | [Pattern-wise Transparent Sequential Recommendation](https://arxiv.org/abs/2402.11480) | 提出了一种模式透明的顺序推荐框架，通过将项目序列分解为多级模式并在概率空间中量化每个模式对结果的贡献，实现了透明的决策过程。 |
| [^22] | [Cross-Model Comparative Loss for Enhancing Neuronal Utility in Language Understanding](https://arxiv.org/abs/2301.03765) | 本论文提出通过交叉模型比较损失的方法来增强语言理解模型中神经元的效用，实现减少冗余参数和抑制输入噪声的目标。 |
| [^23] | [DemiNet: Dependency-Aware Multi-Interest Network with Self-Supervised Graph Learning for Click-Through Rate Prediction](https://arxiv.org/abs/2109.12512) | 提出了DemiNet模型，通过依赖感知异构注意力、自监督兴趣学习和兴趣专家进行评分，显著提高点击率预测效果。 |
| [^24] | [MuseChat: A Conversational Music Recommendation System for Videos.](http://arxiv.org/abs/2310.06282) | MuseChat是一种创新的对话式音乐推荐系统，通过模拟用户和推荐系统之间的对话交互，利用预训练的音乐标签和艺术家信息，为用户提供定制的音乐推荐，使用户可以个性化选择他们喜欢的音乐。 |
| [^25] | [Accurate Cold-start Bundle Recommendation via Popularity-based Coalescence and Curriculum Heating.](http://arxiv.org/abs/2310.03813) | 本文提出了CoHeat算法，一种准确的冷启动捆绑推荐方法。该算法通过结合历史和关联信息，应对捆绑互动分布的倾斜，并有效地学习潜在表示。 |
| [^26] | [Reformulating Sequential Recommendation: Learning Dynamic User Interest with Content-enriched Language Modeling.](http://arxiv.org/abs/2309.10435) | 本研究提出了一个新的顺序推荐范式 LANCER，利用预训练语言模型的语义理解能力生成更加人性化的个性化推荐。在多个基准数据集上的实验结果表明，该方法有效且有希望，并为了解顺序推荐的影响提供了有价值的见解。 |
| [^27] | [Evaluating the Ebb and Flow: An In-depth Analysis of Question-Answering Trends across Diverse Platforms.](http://arxiv.org/abs/2309.05961) | 本文通过对六个社区问答平台的研究，发现了查询的元数据、问题构成方式和用户互动水平与第一个回答时间之间的关联，并利用机器学习模型预测查询是否能够迅速获得回答。 |
| [^28] | [Improving position bias estimation against sparse and skewed dataset with item embedding.](http://arxiv.org/abs/2305.13931) | 该研究提出了一种利用物品嵌入来缓解广告营销领域中位置偏差稀疏性问题的回归EM算法变体。 |

# 详细

[^1]: SPLADE-v3：SPLADE的新基准

    SPLADE-v3: New baselines for SPLADE

    [https://arxiv.org/abs/2403.06789](https://arxiv.org/abs/2403.06789)

    SPLADE-v3相对于BM25和SPLADE++在效果上更为显著，同时与交叉编码器重新排序器相比较出色，具有更高的性能表现。

    

    这是SPLADE库最新版本发布的伴随物。我们描述了对训练结构的更改，并展示了最新一系列的模型--SPLADE-v3。我们将这个新版本与BM25、SPLADE++以及重新排序器进行比较，并通过对40多个查询集的元分析展示其有效性。SPLADE-v3进一步推动了SPLADE模型的极限：它在统计上显著比BM25和SPLADE++更为有效，同时与交叉编码器重新排序器相比较出色。具体而言，它在MS MARCO dev集上获得了40个以上的MRR@10，并在BEIR基准测试中将域外结果提高了2%。

    arXiv:2403.06789v1 Announce Type: cross  Abstract: A companion to the release of the latest version of the SPLADE library. We describe changes to the training structure and present our latest series of models -- SPLADE-v3. We compare this new version to BM25, SPLADE++, as well as re-rankers, and showcase its effectiveness via a meta-analysis over more than 40 query sets. SPLADE-v3 further pushes the limit of SPLADE models: it is statistically significantly more effective than both BM25 and SPLADE++, while comparing well to cross-encoder re-rankers. Specifically, it gets more than 40 MRR@10 on the MS MARCO dev set, and improves by 2% the out-of-domain results on the BEIR benchmark.
    
[^2]: MetaSplit: 用于限量产品推荐的Meta-Split网络

    MetaSplit: Meta-Split Network for Limited-Stock Product Recommendation

    [https://arxiv.org/abs/2403.06747](https://arxiv.org/abs/2403.06747)

    提出了Meta-Split网络（MSN）来解决消费者之间电子商务平台中限量库存产品推荐中的独特挑战，通过分割用户历史序列来有效利用用户历史信息。

    

    相对于面向消费者的电子商务系统，消费者之间的电子商务平台通常会遇到限量库存问题，即产品在C2C系统中只能销售一次。这为点击率（CTR）预测带来了几个独特的挑战。鉴于每个产品（即商品）的有限用户交互，CTR模型中对应的商品嵌入可能不容易收敛。这使得传统基于序列建模的方法无法有效利用用户历史信息，因为历史用户行为包含了不同库存量的商品混合。特别是，序列模型中的注意力机制倾向于将更多累积用户交互的产品分配更高的分数，导致限量产品被忽视且对最终输出的贡献较少。为此，我们提出了Meta-Split网络（MSN）来分割用户历史序列...

    arXiv:2403.06747v1 Announce Type: new  Abstract: Compared to business-to-consumer (B2C) e-commerce systems, consumer-to-consumer (C2C) e-commerce platforms usually encounter the limited-stock problem, that is, a product can only be sold one time in a C2C system. This poses several unique challenges for click-through rate (CTR) prediction. Due to limited user interactions for each product (i.e. item), the corresponding item embedding in the CTR model may not easily converge. This makes the conventional sequence modeling based approaches cannot effectively utilize user history information since historical user behaviors contain a mixture of items with different volume of stocks. Particularly, the attention mechanism in a sequence model tends to assign higher score to products with more accumulated user interactions, making limited-stock products being ignored and contribute less to the final output. To this end, we propose the Meta-Split Network (MSN) to split user history sequence regar
    
[^3]: 在推荐系统中进行后训练属性遗忘

    Post-Training Attribute Unlearning in Recommender Systems

    [https://arxiv.org/abs/2403.06737](https://arxiv.org/abs/2403.06737)

    本文提出了一种后训练属性取消学习（PoT-AU）的方法，通过设计两部分损失函数，旨在在推荐系统中保护用户的敏感属性。

    

    随着推荐系统中日益增长的隐私问题，推荐取消学习越来越受到关注。现有研究主要使用训练数据，即模型输入，作为取消学习目标。然而，即使模型在训练过程中没有明确遇到，攻击者仍可以从模型中提取私人信息。我们将这些未见信息称为属性，并将其视为取消学习目标。为了保护用户的敏感属性，属性取消学习（AU）旨在使目标属性难以分辨。本文侧重于AU的一个严格但实际的设置，即后训练属性取消学习（PoT-AU），其中取消学习只能在推荐模型训练完成后执行。为了解决推荐系统中的PoT-AU问题，我们提出了一个两部分损失函数。第一部分是可区分性损失，我们设计了一个基于分布的度量

    arXiv:2403.06737v1 Announce Type: new  Abstract: With the growing privacy concerns in recommender systems, recommendation unlearning is getting increasing attention. Existing studies predominantly use training data, i.e., model inputs, as unlearning target. However, attackers can extract private information from the model even if it has not been explicitly encountered during training. We name this unseen information as \textit{attribute} and treat it as unlearning target. To protect the sensitive attribute of users, Attribute Unlearning (AU) aims to make target attributes indistinguishable. In this paper, we focus on a strict but practical setting of AU, namely Post-Training Attribute Unlearning (PoT-AU), where unlearning can only be performed after the training of the recommendation model is completed. To address the PoT-AU problem in recommender systems, we propose a two-component loss function. The first component is distinguishability loss, where we design a distribution-based meas
    
[^4]: 紧急响应推理映射（ERIMap）：一种基于贝叶斯网络的方法，用于空间分布紧急情况的动态观测处理

    Emergency Response Inference Mapping (ERIMap): A Bayesian Network-based Method for Dynamic Observation Processing in Spatially Distributed Emergencies

    [https://arxiv.org/abs/2403.06716](https://arxiv.org/abs/2403.06716)

    ERIMap是一种基于贝叶斯网络的方法，旨在为紧急情况中的决策提供系统性和迅速的信息处理，减少决策者的认知负荷

    

    在紧急情况下，高风险决策通常需要在时间紧迫和压力下做出。为了支持这样的决策，需要快速收集和处理来自各种信息源的信息。可用信息往往在时间和空间上变化，不确定，并且有时存在冲突，这可能导致决策中的潜在偏见。目前，缺乏系统化的信息处理和情况评估方法，以满足紧急情况的特殊需求。为了填补这一空白，我们提出了一种名为ERIMap的基于贝叶斯网络的方法，该方法针对紧急情况中复杂的信息环境进行了定制。该方法能够系统地快速处理异构且可能不确定的观测，并对紧急情况的关键变量进行推理。从而降低了决策者的复杂性和认知负荷。

    arXiv:2403.06716v1 Announce Type: new  Abstract: In emergencies, high stake decisions often have to be made under time pressure and strain. In order to support such decisions, information from various sources needs to be collected and processed rapidly. The information available tends to be temporally and spatially variable, uncertain, and sometimes conflicting, leading to potential biases in decisions. Currently, there is a lack of systematic approaches for information processing and situation assessment which meet the particular demands of emergency situations. To address this gap, we present a Bayesian network-based method called ERIMap that is tailored to the complex information-scape during emergencies. The method enables the systematic and rapid processing of heterogeneous and potentially uncertain observations and draws inferences about key variables of an emergency. It thereby reduces complexity and cognitive load for decision makers. The output of the ERIMap method is a dynami
    
[^5]: KELLMRec: 知识增强大型语言模型用于推荐

    KELLMRec: Knowledge-Enhanced Large Language Models for Recommendation

    [https://arxiv.org/abs/2403.06642](https://arxiv.org/abs/2403.06642)

    提出了一种知识增强的大型语言模型用于推荐的方法，通过使用外部知识来帮助生成真实可用的文本，并包括知识为基础的对比学习方案进行训练。

    

    在推荐系统领域，利用语义信息是一个重要的研究问题，旨在补充主流基于ID的方法的缺失部分。随着LLM的兴起，它作为知识库的能力和推理能力为这一研究领域开辟了新的可能性，使基于LLM的推荐成为新兴研究方向。然而，直接使用LLM来处理推荐场景中的语义信息是不可靠和次优的，由于存在幻觉等问题。应对这一问题的一种有前途的方法是利用外部知识来帮助LLM生成真实可用的文本。受以上动机的启发，我们提出了一种知识增强的LLMRec方法。除了在提示中使用外部知识外，所提出的方法还包括一个基于知识的对比学习方案用于训练。在公共数据集和企业中进行的实验

    arXiv:2403.06642v1 Announce Type: cross  Abstract: The utilization of semantic information is an important research problem in the field of recommender systems, which aims to complement the missing parts of mainstream ID-based approaches. With the rise of LLM, its ability to act as a knowledge base and its reasoning capability have opened up new possibilities for this research area, making LLM-based recommendation an emerging research direction. However, directly using LLM to process semantic information for recommendation scenarios is unreliable and sub-optimal due to several problems such as hallucination. A promising way to cope with this is to use external knowledge to aid LLM in generating truthful and usable text. Inspired by the above motivation, we propose a Knowledge-Enhanced LLMRec method. In addition to using external knowledge in prompts, the proposed method also includes a knowledge-based contrastive learning scheme for training. Experiments on public datasets and in-enter
    
[^6]: 利用基础模型进行放射学中基于内容的医学图像检索

    Leveraging Foundation Models for Content-Based Medical Image Retrieval in Radiology

    [https://arxiv.org/abs/2403.06567](https://arxiv.org/abs/2403.06567)

    基于内容的医学图像检索中，利用基础模型作为特征提取器，无需微调即可取得与专门模型竞争的性能，尤其在检索病理特征方面具有较大困难。

    

    Content-based image retrieval（CBIR）有望显著改善放射学中的诊断辅助和医学研究。我们提出利用视觉基础模型作为强大且多功能的现成特征提取器，用于基于内容的医学图像检索。通过在涵盖四种模态和161种病理学的160万张2D放射图像的全面数据集上对这些模型进行基准测试，我们发现弱监督模型表现优异，P@1可达0.594。这种性能不仅与专门化模型竞争，而且无需进行微调。我们的分析进一步探讨了检索病理学与解剖结构的挑战，表明准确检索病理特征更具挑战性。

    arXiv:2403.06567v1 Announce Type: cross  Abstract: Content-based image retrieval (CBIR) has the potential to significantly improve diagnostic aid and medical research in radiology. Current CBIR systems face limitations due to their specialization to certain pathologies, limiting their utility. In response, we propose using vision foundation models as powerful and versatile off-the-shelf feature extractors for content-based medical image retrieval. By benchmarking these models on a comprehensive dataset of 1.6 million 2D radiological images spanning four modalities and 161 pathologies, we identify weakly-supervised models as superior, achieving a P@1 of up to 0.594. This performance not only competes with a specialized model but does so without the need for fine-tuning. Our analysis further explores the challenges in retrieving pathological versus anatomical structures, indicating that accurate retrieval of pathological features presents greater difficulty. Despite these challenges, our
    
[^7]: ToolRerank：面向工具检索的自适应和层次感知重新排名

    ToolRerank: Adaptive and Hierarchy-Aware Reranking for Tool Retrieval

    [https://arxiv.org/abs/2403.06551](https://arxiv.org/abs/2403.06551)

    ToolRerank是一种自适应和层次感知的重新排名方法，用于工具检索，通过Adaptive Truncation和Hierarchy-Aware Reranking来优化检索结果。

    

    arXiv:2403.06551v1 类型：新 原摘要：工具学习旨在通过外部工具扩展大型语言模型（LLMs）的能力。工具学习面临的主要挑战是如何支持大量工具，包括未见过的工具。为解决这一挑战，先前的研究提出根据用户查询为LLM检索合适的工具。然而，先前提出的方法既没有考虑见过和未见过工具之间的差异，也没有考虑工具库的层次结构，这可能导致工具检索性能不佳。因此，为解决上述问题，我们提出了ToolRerank，这是一种用于工具检索的自适应和层次感知的重新排名方法，以进一步细化检索结果。具体来说，我们提出的ToolRerank包括自适应截断，该方法将与见过和未见过工具相关的检索结果在不同位置截断，并包括层次感知重新排名，该方法让检索结果

    arXiv:2403.06551v1 Announce Type: new  Abstract: Tool learning aims to extend the capabilities of large language models (LLMs) with external tools. A major challenge in tool learning is how to support a large number of tools, including unseen tools. To address this challenge, previous studies have proposed retrieving suitable tools for the LLM based on the user query. However, previously proposed methods do not consider the differences between seen and unseen tools, nor do they take the hierarchy of the tool library into account, which may lead to suboptimal performance for tool retrieval. Therefore, to address the aforementioned issues, we propose ToolRerank, an adaptive and hierarchy-aware reranking method for tool retrieval to further refine the retrieval results. Specifically, our proposed ToolRerank includes Adaptive Truncation, which truncates the retrieval results related to seen and unseen tools at different positions, and Hierarchy-Aware Reranking, which makes retrieval result
    
[^8]: RecAI：利用大型语言模型为下一代推荐系统增添力量

    RecAI: Leveraging Large Language Models for Next-Generation Recommender Systems

    [https://arxiv.org/abs/2403.06465](https://arxiv.org/abs/2403.06465)

    本文介绍了RecAI，一个旨在通过大型语言模型增强推荐系统的实用工具包，新一代由LLMs赋能的推荐系统将更加多才多艺、可解释、对话式和可控，为更智能和用户中心的推荐体验铺平道路。

    

    本文介绍了RecAI，一个实用的工具包，旨在通过大型语言模型（LLMs）的先进能力来增强甚至革新推荐系统。RecAI提供了一系列工具，包括推荐AI代理、面向推荐的语言模型、知识插件、推荐解释器和评估器，以多角度促进LLMs融入推荐系统。LLMs赋能的新一代推荐系统预计将更加多才多艺、可解释、对话式和可控，为更智能和用户中心的推荐体验铺平道路。我们希望RecAI的开源能够加速新一代先进推荐系统的演进。RecAI的源代码可在 \url{https://github.com/microsoft/RecAI} 找到。

    arXiv:2403.06465v1 Announce Type: cross  Abstract: This paper introduces RecAI, a practical toolkit designed to augment or even revolutionize recommender systems with the advanced capabilities of Large Language Models (LLMs). RecAI provides a suite of tools, including Recommender AI Agent, Recommendation-oriented Language Models, Knowledge Plugin, RecExplainer, and Evaluator, to facilitate the integration of LLMs into recommender systems from multifaceted perspectives. The new generation of recommender systems, empowered by LLMs, are expected to be more versatile, explainable, conversational, and controllable, paving the way for more intelligent and user-centric recommendation experiences. We hope the open-source of RecAI can help accelerate evolution of new advanced recommender systems. The source code of RecAI is available at \url{https://github.com/microsoft/RecAI}.
    
[^9]: CoRAL: 协作检索增强大型语言模型改进长尾推荐

    CoRAL: Collaborative Retrieval-Augmented Large Language Models Improve Long-tail Recommendation

    [https://arxiv.org/abs/2403.06447](https://arxiv.org/abs/2403.06447)

    CoRAL引入了协作检索增强LLMs，将协作证据直接纳入到推理过程中，从而更好地理解用户之间的偏好，改进长尾推荐。

    

    长尾推荐对传统推荐系统来说是一个具有挑战性的任务，主要是由于数据稀疏和数据不平衡问题。最近大型语言模型（LLMs）的发展展示了它们在复杂推理方面的能力，可以帮助基于非常少的先前交互来推断用户的偏好。然而，由于大多数基于LLM的系统仅依赖于物品的语义含义作为推理的唯一证据，忽略了用户-物品交互的协作信息，这可能导致LLM的推理与数据集的任务特定协作信息不一致。为了进一步将LLMs的推理与任务特定的用户-物品交互知识相一致，我们引入了协作检索增强LLMs，称为CoRAL，直接将协作证据纳入交互中。基于检索到的用户-物品交互，LLMs可以分析用户之间的共享和不同偏好，并总结

    arXiv:2403.06447v1 Announce Type: cross  Abstract: The long-tail recommendation is a challenging task for traditional recommender systems, due to data sparsity and data imbalance issues. The recent development of large language models (LLMs) has shown their abilities in complex reasoning, which can help to deduce users' preferences based on very few previous interactions. However, since most LLM-based systems rely on items' semantic meaning as the sole evidence for reasoning, the collaborative information of user-item interactions is neglected, which can cause the LLM's reasoning to be misaligned with task-specific collaborative information of the dataset. To further align LLMs' reasoning to task-specific user-item interaction knowledge, we introduce collaborative retrieval-augmented LLMs, CoRAL, which directly incorporate collaborative evidence into the prompts. Based on the retrieved user-item interactions, the LLM can analyze shared and distinct preferences among users, and summariz
    
[^10]: 重复填充作为顺序推荐的数据增强

    Repeated Padding as Data Augmentation for Sequential Recommendation

    [https://arxiv.org/abs/2403.06372](https://arxiv.org/abs/2403.06372)

    本文提出了一种名为"RepPad"的简单而有效的填充方法，旨在充分利用填充空间来提高顺序推荐模型的性能和训练效率。

    

    顺序推荐旨在根据用户的历史互动提供个性化建议。在训练顺序模型时，填充是一种被广泛采用的技术，主要原因有两个：1）绝大多数模型只能处理固定长度的序列；2）基于批处理的训练需要确保每个批次中的序列具有相同的长度。通常使用特殊值0作为填充内容，不包含实际信息并在模型计算中被忽略。这种常识填充策略引出了一个以前从未探讨过的问题：我们能否通过填充其他内容充分利用这一闲置输入空间，进一步提高模型性能和训练效率？ 在本文中，我们提出了一种简单而有效的填充方法，名为RepPad (重复填充)。

    arXiv:2403.06372v1 Announce Type: new  Abstract: Sequential recommendation aims to provide users with personalized suggestions based on their historical interactions. When training sequential models, padding is a widely adopted technique for two main reasons: 1) The vast majority of models can only handle fixed-length sequences; 2) Batching-based training needs to ensure that the sequences in each batch have the same length. The special value \emph{0} is usually used as the padding content, which does not contain the actual information and is ignored in the model calculations. This common-sense padding strategy leads us to a problem that has never been explored before: \emph{Can we fully utilize this idle input space by padding other content to further improve model performance and training efficiency?}   In this paper, we propose a simple yet effective padding method called \textbf{Rep}eated \textbf{Pad}ding (\textbf{RepPad}). Specifically, we use the original interaction sequences as
    
[^11]: 大型语言模型的概念知识编辑

    Editing Conceptual Knowledge for Large Language Models

    [https://arxiv.org/abs/2403.06259](https://arxiv.org/abs/2403.06259)

    该论文首次研究了为大型语言模型编辑概念知识，通过构建基准数据集和建立新评估指标，发现现有方法虽然能一定程度上修改概念定义，但也可能造成LLMs中相关实例知识的扭曲，导致性能下降。

    

    最近，对于大型语言模型（LLMs）的知识编辑引起了越来越多的关注。当前的方法和评估仅探讨了实例级别的编辑，然而LLMs是否具有修改概念的能力仍不清楚。本文首次研究了为LLMs编辑概念知识，通过构建一个新颖的基准数据集ConceptEdit并建立了一套新的评估指标。实验结果表明，尽管现有的编辑方法可以有效地在一定程度上修改概念级别的定义，但它们也有潜力扭曲LLMs中相关的实例知识，导致性能不佳。我们期望这可以激发对更好理解LLMs的进一步进展。我们的项目主页位于https://zjunlp.github.io/project/ConceptEdit。

    arXiv:2403.06259v1 Announce Type: cross  Abstract: Recently, there has been a growing interest in knowledge editing for Large Language Models (LLMs). Current approaches and evaluations merely explore the instance-level editing, while whether LLMs possess the capability to modify concepts remains unclear. This paper pioneers the investigation of editing conceptual knowledge for LLMs, by constructing a novel benchmark dataset ConceptEdit and establishing a suite of new metrics for evaluation. The experimental results reveal that, although existing editing methods can efficiently modify concept-level definition to some extent, they also have the potential to distort the related instantial knowledge in LLMs, leading to poor performance. We anticipate this can inspire further progress in better understanding LLMs. Our project homepage is available at https://zjunlp.github.io/project/ConceptEdit.
    
[^12]: 用步骤式思维检索和对齐决策增强LLM代理

    TRAD: Enhancing LLM Agents with Step-Wise Thought Retrieval and Aligned Decision

    [https://arxiv.org/abs/2403.06221](https://arxiv.org/abs/2403.06221)

    提出了TRAD框架，通过步骤式思维检索和对齐决策解决了利用上下文示例时可能出现的问题。

    

    许多大型语言模型（LLM）代理已经被构建用于不同任务，如网络导航和在线购物，这是因为LLM具有广泛的知识和文本理解能力。在这些研究中，许多利用上下文示例来实现泛化，而无需微调，但少数考虑了如何选择和有效利用这些示例的问题。最近，基于轨迹级检索和使用轨迹作为上下文示例的方法已经提出，以提高代理在一些顺序决策任务中的整体性能。然而，这些方法可能存在问题，因为检索出的可信示例缺乏特定于任务的状态转移动态，且输入长且包含大量无关上下文。在本文中，我们提出了一个新框架（TRAD）来解决这些问题。TRAD首先进行思维检索，实现步骤级演示。

    arXiv:2403.06221v1 Announce Type: new  Abstract: Numerous large language model (LLM) agents have been built for different tasks like web navigation and online shopping due to LLM's wide knowledge and text-understanding ability. Among these works, many of them utilize in-context examples to achieve generalization without the need for fine-tuning, while few of them have considered the problem of how to select and effectively utilize these examples. Recently, methods based on trajectory-level retrieval with task meta-data and using trajectories as in-context examples have been proposed to improve the agent's overall performance in some sequential decision making tasks. However, these methods can be problematic due to plausible examples retrieved without task-specific state transition dynamics and long input with plenty of irrelevant context. In this paper, we propose a novel framework (TRAD) to address these issues. TRAD first conducts Thought Retrieval, achieving step-level demonstration
    
[^13]: 能否用LLM替代人工标注？ 无人机交付任务下的细粒度中文地址实体识别数据集案例研究

    Can LLM Substitute Human Labeling? A Case Study of Fine-grained Chinese Address Entity Recognition Dataset for UAV Delivery

    [https://arxiv.org/abs/2403.06097](https://arxiv.org/abs/2403.06097)

    提出了适用于无人机交付系统中地址解析任务的细粒度中文姓名实体识别数据集CNER-UAV，包含五个类别的多样化数据，经过严格的数据清洗和去敏处理，约有12,000个标注样本，评估了传统的实体识别模型并提供了深入分析

    

    我们提出了CNER-UAV，一个专为无人机交付系统中地址解析任务设计的细粒度中文姓名实体识别数据集。该数据集涵盖了五个类别，可以全面训练和评估实体识别模型。为构建这一数据集，我们从真实无人机交付系统中获取数据，并进行了严格的数据清洗和去敏处理，确保数据的隐私性和完整性。最终的数据集约包含12,000个标注样本，经过人工专家和大型语言模型的注释。我们在我们的数据集上评估了传统的实体识别模型，并提供了深入分析。数据集和模型可以在 \url{https://github.com/zhhvvv/CNER-UAV} 上公开获取。

    arXiv:2403.06097v1 Announce Type: cross  Abstract: We present CNER-UAV, a fine-grained \textbf{C}hinese \textbf{N}ame \textbf{E}ntity \textbf{R}ecognition dataset specifically designed for the task of address resolution in \textbf{U}nmanned \textbf{A}erial \textbf{V}ehicle delivery systems. The dataset encompasses a diverse range of five categories, enabling comprehensive training and evaluation of NER models. To construct this dataset, we sourced the data from a real-world UAV delivery system and conducted a rigorous data cleaning and desensitization process to ensure privacy and data integrity. The resulting dataset, consisting of around 12,000 annotated samples, underwent human experts and \textbf{L}arge \textbf{L}anguage \textbf{M}odel annotation. We evaluated classical NER models on our dataset and provided in-depth analysis. The dataset and models are publicly available at \url{https://github.com/zhhvvv/CNER-UAV}.
    
[^14]: 位掩码健壮对比度知识蒸馏用于无监督语义哈希

    Bit-mask Robust Contrastive Knowledge Distillation for Unsupervised Semantic Hashing

    [https://arxiv.org/abs/2403.06071](https://arxiv.org/abs/2403.06071)

    本文提出了一种创新的位掩码健壮对比度知识蒸馏（BRCD）方法，专门为语义哈希模型的蒸馏而设计，以取得更好的性能。

    

    无监督语义哈希已经成为快速图像搜索的不可或缺的技术，旨在将图像转换为二进制哈希码而不依赖标签。最近在该领域的进展表明，在无监督语义哈希模型中使用大规模骨干（例如 ViT）可以带来显著的改进。然而，推断延迟变得越来越难以忽视。知识蒸馏提供了一种实现模型压缩以缓解此延迟的方法。然而，目前的知识蒸馏方法并未专门针对语义哈希进行设计。它们忽略了语义哈希的独特搜索范式，蒸馏过程的固有必需性以及哈希码的属性。在本文中，我们提出了一种创新的位掩码健壮对比度知识蒸馏（BRCD）方法，专为语义哈希模型的蒸馏而设计。

    arXiv:2403.06071v1 Announce Type: cross  Abstract: Unsupervised semantic hashing has emerged as an indispensable technique for fast image search, which aims to convert images into binary hash codes without relying on labels. Recent advancements in the field demonstrate that employing large-scale backbones (e.g., ViT) in unsupervised semantic hashing models can yield substantial improvements. However, the inference delay has become increasingly difficult to overlook. Knowledge distillation provides a means for practical model compression to alleviate this delay. Nevertheless, the prevailing knowledge distillation approaches are not explicitly designed for semantic hashing. They ignore the unique search paradigm of semantic hashing, the inherent necessities of the distillation process, and the property of hash codes. In this paper, we propose an innovative Bit-mask Robust Contrastive knowledge Distillation (BRCD) method, specifically devised for the distillation of semantic hashing model
    
[^15]: 电子商务搜索中的分层查询分类

    Hierarchical Query Classification in E-commerce Search

    [https://arxiv.org/abs/2403.06021](https://arxiv.org/abs/2403.06021)

    提出了一个利用分层信息的新框架，以增强表示学习，解决了分层查询分类中的类别不平衡和查询模糊性带来的挑战。

    

    电子商务平台通常以层次结构存储和构造产品信息和搜索数据。将用户搜索查询有效分类到类似的层次结构中，在提升电子商务平台用户体验的同时，对于新闻整理和学术研究也至关重要。处理敏感查询分类或关键信息传播时，精确性的重要性得到放大，因为不准确可能带来相当大的负面影响。分层查询分类的固有复杂性受到两个主要挑战的影响：（1）明显的偏向主导类别的类别不平衡性，和（2）搜索查询的固有简洁性和模糊性，阻碍了准确分类。为了解决这些挑战，我们引入了一种新颖的框架，通过（i）增强的表示学习来利用分层信息。

    arXiv:2403.06021v1 Announce Type: cross  Abstract: E-commerce platforms typically store and structure product information and search data in a hierarchy. Efficiently categorizing user search queries into a similar hierarchical structure is paramount in enhancing user experience on e-commerce platforms as well as news curation and academic research. The significance of this task is amplified when dealing with sensitive query categorization or critical information dissemination, where inaccuracies can lead to considerable negative impacts. The inherent complexity of hierarchical query classification is compounded by two primary challenges: (1) the pronounced class imbalance that skews towards dominant categories, and (2) the inherent brevity and ambiguity of search queries that hinder accurate classification.   To address these challenges, we introduce a novel framework that leverages hierarchical information through (i) enhanced representation learning that utilizes the contrastive loss
    
[^16]: LEGION：利用分布平衡损失调整预训练语言模型进行GitHub主题推荐

    LEGION: Harnessing Pre-trained Language Models for GitHub Topic Recommendations with Distribution-Balance Loss

    [https://arxiv.org/abs/2403.05873](https://arxiv.org/abs/2403.05873)

    提出了一种名为Legion的新方法，利用预训练语言模型（PTMs）为GitHub存储库推荐主题，以解决现有技术在主题推荐中的局限性。

    

    开源开发通过促进协作、透明性和社区驱动的创新，彻底改变了软件行业。如今，大量各种类型的开源软件，形成了网络存储库，通常托管在GitHub上-一种流行的软件开发平台。为了增强存储库网络的可发现性，即相似存储库组，GitHub在2017年引入了存储库主题，使用户更容易按类型、技术等浏览相关项目。因此，准确为每个GitHub存储库分配主题至关重要。目前用于自动主题推荐的方法主要依赖于TF-IDF来对文本数据进行编码，存在理解语义细微差别的挑战。本文通过提出Legion，一种利用预训练语言模型（PTMs）推荐主题的新方法，解决了现有技术的局限性。

    arXiv:2403.05873v1 Announce Type: cross  Abstract: Open-source development has revolutionized the software industry by promoting collaboration, transparency, and community-driven innovation. Today, a vast amount of various kinds of open-source software, which form networks of repositories, is often hosted on GitHub - a popular software development platform. To enhance the discoverability of the repository networks, i.e., groups of similar repositories, GitHub introduced repository topics in 2017 that enable users to more easily explore relevant projects by type, technology, and more. It is thus crucial to accurately assign topics for each GitHub repository. Current methods for automatic topic recommendation rely heavily on TF-IDF for encoding textual data, presenting challenges in understanding semantic nuances. This paper addresses the limitations of existing techniques by proposing Legion, a novel approach that leverages Pre-trained Language Models (PTMs) for recommending topics for 
    
[^17]: CFaiRLLM：大型语言模型推荐系统中的消费者公平评估

    CFaiRLLM: Consumer Fairness Evaluation in Large-Language Model Recommender System

    [https://arxiv.org/abs/2403.05668](https://arxiv.org/abs/2403.05668)

    这项研究引入了一个全面的评估框架 CFaiRLLM，旨在评估和减轻 RecLLMs 中消费者端的偏见

    

    在推荐系统不断发展的过程中，像ChatGPT这样的大型语言模型的整合标志着引入了基于语言模型的推荐（RecLLM）的新时代。虽然这些进展承诺提供前所未有的个性化和效率，但也引发了对公平性的重要关切，特别是在推荐可能无意中继续或放大与敏感用户属性相关的偏见的情况下。为了解决这些问题，我们的研究引入了一个全面的评估框架CFaiRLLM，旨在评估（从而减轻）RecLLMs中消费者端的偏见。

    arXiv:2403.05668v1 Announce Type: new  Abstract: In the evolving landscape of recommender systems, the integration of Large Language Models (LLMs) such as ChatGPT marks a new era, introducing the concept of Recommendation via LLM (RecLLM). While these advancements promise unprecedented personalization and efficiency, they also bring to the fore critical concerns regarding fairness, particularly in how recommendations might inadvertently perpetuate or amplify biases associated with sensitive user attributes. In order to address these concerns, our study introduces a comprehensive evaluation framework, CFaiRLLM, aimed at evaluating (and thereby mitigating) biases on the consumer side within RecLLMs.   Our research methodically assesses the fairness of RecLLMs by examining how recommendations might vary with the inclusion of sensitive attributes such as gender, age, and their intersections, through both similarity alignment and true preference alignment. By analyzing recommendations gener
    
[^18]: 将文本到图像和大型语言模型串联：生成个性化电子商务横幅的新方法

    Chaining text-to-image and large language model: A novel approach for generating personalized e-commerce banners

    [https://arxiv.org/abs/2403.05578](https://arxiv.org/abs/2403.05578)

    本研究提出了一种新方法，利用文本到图像模型和大型语言模型生成个性化网页横幅，根据用户互动动态内容，并且无需人工干预。

    

    arXiv:2403.05578v1 公告类型：交叉摘要：稳定扩散等文本到图像模型为生成艺术作品开辟了大量机会。最近的文献调查了文本到图像模型在增强许多创意艺术家工作中的应用。许多电子商务平台采用手动流程生成横幅，这是耗时的且存在可扩展性的局限性。在这项工作中，我们展示了利用文本到图像模型根据在线购物者的互动生成具有动态内容的个性化网页横幅的用途。此方法的新颖之处在于在没有人为干预的情况下将用户互动数据转换为有意义的提示。为此，我们利用大型语言模型（LLM）系统地从项目元信息中提取属性元组。然后通过提示工程将这些属性传递给文本到图像模型，以生成横幅的图像。我们的结果表明，所提出的方法可以创建高-

    arXiv:2403.05578v1 Announce Type: cross  Abstract: Text-to-image models such as stable diffusion have opened a plethora of opportunities for generating art. Recent literature has surveyed the use of text-to-image models for enhancing the work of many creative artists. Many e-commerce platforms employ a manual process to generate the banners, which is time-consuming and has limitations of scalability. In this work, we demonstrate the use of text-to-image models for generating personalized web banners with dynamic content for online shoppers based on their interactions. The novelty in this approach lies in converting users' interaction data to meaningful prompts without human intervention. To this end, we utilize a large language model (LLM) to systematically extract a tuple of attributes from item meta-information. The attributes are then passed to a text-to-image model via prompt engineering to generate images for the banner. Our results show that the proposed approach can create high-
    
[^19]: 使用BERT监测极端社交媒体上反犹太主义话语的演变

    Monitoring the evolution of antisemitic discourse on extremist social media using BERT

    [https://arxiv.org/abs/2403.05548](https://arxiv.org/abs/2403.05548)

    本研究提出了一种使用BERT监测极端社交媒体上反犹太主义话语演变的自动方法，避免了手动监测的不可行性，为干预和防止仇恨升级提供了新途径。

    

    研究表明，社交媒体上的种族主义和不宽容有可能在线下产生仇恨，最终导致身体暴力。本研究考虑的是在线反犹主义，追踪在线讨论中的反犹主题及其相关术语的演变，有助于监测参与者的情绪和演变，并可能提供干预方法，防止仇恨升级。鉴于在线流量庞大且不断变化，手动监测谈话实际上是不现实的。因此，我们提出了一种自动化方法，可以从极端社交媒体中提取反犹主题和术语，跟踪它们的演变。由于监督学习在这样的任务中过于受限，我们开发了一种无监督的在线机器学习方法，使用大型语言模型。

    arXiv:2403.05548v1 Announce Type: cross  Abstract: Racism and intolerance on social media contribute to a toxic online environment which may spill offline to foster hatred, and eventually lead to physical violence. That is the case with online antisemitism, the specific category of hatred considered in this study. Tracking antisemitic themes and their associated terminology over time in online discussions could help monitor the sentiments of their participants and their evolution, and possibly offer avenues for intervention that may prevent the escalation of hatred. Due to the large volume and constant evolution of online traffic, monitoring conversations manually is impractical. Instead, we propose an automated method that extracts antisemitic themes and terminology from extremist social media over time and captures their evolution. Since supervised learning would be too limited for such a task, we created an unsupervised online machine learning approach that uses large language model
    
[^20]: PromptMM：多模式知识蒸馏用于基于Prompt-Tuning的推荐

    PromptMM: Multi-Modal Knowledge Distillation for Recommendation with Prompt-Tuning

    [https://arxiv.org/abs/2402.17188](https://arxiv.org/abs/2402.17188)

    提出了一种通过Prompt-Tuning赋能的PromptMM多模式知识蒸馏方法，用于简化和增强推荐系统，实现自适应的质量蒸馏。

    

    多媒体在线平台（例如亚马逊、TikTok）通过将多媒体（例如视觉、文本和声学）内容纳入其个性化推荐系统中获益匪浅。这些模态提供直观语义，有助于进行模态感知的用户偏好建模。然而，多模式推荐器中存在两个关键挑战尚未解决：i）引入具有大量额外参数的多模式编码器会导致过拟合，考虑到提取器（例如ViT、BERT）提供的高维多模式特征。ii）辅助信息不可避免地引入不准确性和冗余，导致模态交互依赖偏离真实用户偏好。为了解决这些问题，我们提出通过Prompt-Tuning赋能、简化推荐器的PromptMM（多模式知识蒸馏），实现自适应质量蒸馏。

    arXiv:2402.17188v1 Announce Type: new  Abstract: Multimedia online platforms (e.g., Amazon, TikTok) have greatly benefited from the incorporation of multimedia (e.g., visual, textual, and acoustic) content into their personal recommender systems. These modalities provide intuitive semantics that facilitate modality-aware user preference modeling. However, two key challenges in multi-modal recommenders remain unresolved: i) The introduction of multi-modal encoders with a large number of additional parameters causes overfitting, given high-dimensional multi-modal features provided by extractors (e.g., ViT, BERT). ii) Side information inevitably introduces inaccuracies and redundancies, which skew the modality-interaction dependency from reflecting true user preference. To tackle these problems, we propose to simplify and empower recommenders through Multi-modal Knowledge Distillation (PromptMM) with the prompt-tuning that enables adaptive quality distillation. Specifically, PromptMM cond
    
[^21]: 模式透明的顺序推荐

    Pattern-wise Transparent Sequential Recommendation

    [https://arxiv.org/abs/2402.11480](https://arxiv.org/abs/2402.11480)

    提出了一种模式透明的顺序推荐框架，通过将项目序列分解为多级模式并在概率空间中量化每个模式对结果的贡献，实现了透明的决策过程。

    

    透明的决策过程对于开发可靠和值得信赖的推荐系统至关重要。对于顺序推荐来说，意味着模型能够识别关键项目作为其推荐结果的理由。然而，同时实现模型透明度和推荐性能是具有挑战性的，特别是对于将整个项目序列作为输入而不加筛选的模型而言。在本文中，我们提出了一种名为PTSR的可解释框架，它实现了一种模式透明的决策过程。它将项目序列分解为多级模式，这些模式作为整个推荐过程的原子单元。每个模式对结果的贡献在概率空间中得到量化。通过精心设计的模式加权校正，即使在没有真实关键模式的情况下，也能学习模式的贡献。最终推荐

    arXiv:2402.11480v1 Announce Type: new  Abstract: A transparent decision-making process is essential for developing reliable and trustworthy recommender systems. For sequential recommendation, it means that the model can identify critical items asthe justifications for its recommendation results. However, achieving both model transparency and recommendation performance simultaneously is challenging, especially for models that take the entire sequence of items as input without screening. In this paper,we propose an interpretable framework (named PTSR) that enables a pattern-wise transparent decision-making process. It breaks the sequence of items into multi-level patterns that serve as atomic units for the entire recommendation process. The contribution of each pattern to the outcome is quantified in the probability space. With a carefully designed pattern weighting correction, the pattern contribution can be learned in the absence of ground-truth critical patterns. The final recommended
    
[^22]: 通过交叉模型比较损失增强语言理解中神经元效用

    Cross-Model Comparative Loss for Enhancing Neuronal Utility in Language Understanding

    [https://arxiv.org/abs/2301.03765](https://arxiv.org/abs/2301.03765)

    本论文提出通过交叉模型比较损失的方法来增强语言理解模型中神经元的效用，实现减少冗余参数和抑制输入噪声的目标。

    

    当前自然语言理解（NLU）模型在模型规模和输入背景方面不断扩大，引入了更多隐藏神经元和输入神经元，大体上提高了性能。然而，额外的神经元并不能为所有实例带来一致的改进，因为一些隐藏神经元是冗余的，混入输入神经元的噪声往往会分散模型的注意力。之前的工作主要侧重于通过附加的后处理或预处理，如网络修剪和上下文选择，从外部降低低效神经元的数量，以避免这个问题。除此之外，我们是否可以通过增强每个神经元的效用来使模型减少冗余参数并抑制输入噪声？如果一个模型能够有效地利用神经元，那么不管哪些神经元被剥离（禁用），剥离后的子模型的性能都不应该优于原始完整模型。根据这样的比较

    arXiv:2301.03765v2 Announce Type: replace  Abstract: Current natural language understanding (NLU) models have been continuously scaling up, both in terms of model size and input context, introducing more hidden and input neurons. While this generally improves performance on average, the extra neurons do not yield a consistent improvement for all instances. This is because some hidden neurons are redundant, and the noise mixed in input neurons tends to distract the model. Previous work mainly focuses on extrinsically reducing low-utility neurons by additional post- or pre-processing, such as network pruning and context selection, to avoid this problem. Beyond that, can we make the model reduce redundant parameters and suppress input noise by intrinsically enhancing the utility of each neuron? If a model can efficiently utilize neurons, no matter which neurons are ablated (disabled), the ablated submodel should perform no better than the original full model. Based on such a comparison pr
    
[^23]: DemiNet: 基于自监督图学习的依赖感知多兴趣网络用于点击率预测

    DemiNet: Dependency-Aware Multi-Interest Network with Self-Supervised Graph Learning for Click-Through Rate Prediction

    [https://arxiv.org/abs/2109.12512](https://arxiv.org/abs/2109.12512)

    提出了DemiNet模型，通过依赖感知异构注意力、自监督兴趣学习和兴趣专家进行评分，显著提高点击率预测效果。

    

    在本文中，我们提出了一个名为DemiNet（DEpendency-Aware Multi-Interest Network的缩写）的新型模型，用于解决上述两个问题。具体而言，我们首先考虑物品节点之间的各种依赖类型，并为去噪和获取准确的序列物品表示进行依赖感知异构注意力。其次，为了提取多个兴趣，我们在图嵌入之上进行多头注意力操作。为了滤除嘈杂的物品间相关性并增强提取兴趣的鲁棒性，我们将自监督兴趣学习引入到上述两个步骤中。第三，为了聚合多个兴趣，对应不同兴趣路线的兴趣专家分别给出评分，而专门的网络分配每个分数的置信度。在三个真实世界数据集上的实验结果表明，所提出的DemiNet显著提高了整体的推荐效果。

    arXiv:2109.12512v2 Announce Type: replace  Abstract: In this paper, we propose a novel model named DemiNet (short for DEpendency-Aware Multi-Interest Network) to address the above two issues. To be specific, we first consider various dependency types between item nodes and perform dependency-aware heterogeneous attention for denoising and obtaining accurate sequence item representations. Secondly, for multiple interests extraction, multi-head attention is conducted on top of the graph embedding. To filter out noisy inter-item correlations and enhance the robustness of extracted interests, self-supervised interest learning is introduced to the above two steps. Thirdly, to aggregate the multiple interests, interest experts corresponding to different interest routes give rating scores respectively, while a specialized network assigns the confidence of each score. Experimental results on three real-world datasets demonstrate that the proposed DemiNet significantly improves the overall reco
    
[^24]: MuseChat:一种视频对话音乐推荐系统

    MuseChat: A Conversational Music Recommendation System for Videos. (arXiv:2310.06282v1 [cs.LG])

    [http://arxiv.org/abs/2310.06282](http://arxiv.org/abs/2310.06282)

    MuseChat是一种创新的对话式音乐推荐系统，通过模拟用户和推荐系统之间的对话交互，利用预训练的音乐标签和艺术家信息，为用户提供定制的音乐推荐，使用户可以个性化选择他们喜欢的音乐。

    

    我们引入了MuseChat，一种创新的基于对话的音乐推荐系统。这个独特的平台不仅提供互动用户参与，还为输入的视频提供了定制的音乐推荐，使用户可以改进和个性化他们的音乐选择。与之相反，以前的系统主要强调内容的兼容性，往往忽视了用户个体偏好的细微差别。例如，所有的数据集都只提供基本的音乐-视频配对，或者带有音乐描述的配对。为了填补这一空白，我们的研究提供了三个贡献。首先，我们设计了一种对话合成方法，模拟了用户和推荐系统之间的两轮交互，利用预训练的音乐标签和艺术家信息。在这个交互中，用户提交一个视频给系统，系统会提供一个合适的音乐片段，并附带解释。之后，用户会表达他们对音乐的偏好，系统会呈现一个改进后的音乐推荐

    We introduce MuseChat, an innovative dialog-based music recommendation system. This unique platform not only offers interactive user engagement but also suggests music tailored for input videos, so that users can refine and personalize their music selections. In contrast, previous systems predominantly emphasized content compatibility, often overlooking the nuances of users' individual preferences. For example, all the datasets only provide basic music-video pairings or such pairings with textual music descriptions. To address this gap, our research offers three contributions. First, we devise a conversation-synthesis method that simulates a two-turn interaction between a user and a recommendation system, which leverages pre-trained music tags and artist information. In this interaction, users submit a video to the system, which then suggests a suitable music piece with a rationale. Afterwards, users communicate their musical preferences, and the system presents a refined music recomme
    
[^25]: 准确的冷启动捆绑推荐：基于流行度的聚合和课程加热

    Accurate Cold-start Bundle Recommendation via Popularity-based Coalescence and Curriculum Heating. (arXiv:2310.03813v1 [cs.IR])

    [http://arxiv.org/abs/2310.03813](http://arxiv.org/abs/2310.03813)

    本文提出了CoHeat算法，一种准确的冷启动捆绑推荐方法。该算法通过结合历史和关联信息，应对捆绑互动分布的倾斜，并有效地学习潜在表示。

    

    如何准确地向用户推荐冷启动捆绑？捆绑推荐中的冷启动问题在实际场景中至关重要，因为新建捆绑不断出现以满足各种营销目的。尽管其重要性，之前没有研究涉及冷启动捆绑推荐。此外，现有的冷启动物品推荐方法过于依赖历史信息，即使对于不受欢迎的捆绑也是如此，无法应对捆绑互动分布高度倾斜的主要挑战。在这项工作中，我们提出了CoHeat（基于流行度的聚合和课程加热），这是一种准确的冷启动捆绑推荐方法。CoHeat通过结合历史信息和关联信息来估计用户与捆绑之间的关系，以应对捆绑互动分布的高度倾斜问题。此外，CoHeat还通过利用课程学习和聚合特征学习效果地学习潜在表示。

    How can we accurately recommend cold-start bundles to users? The cold-start problem in bundle recommendation is critical in practical scenarios since new bundles are continuously created for various marketing purposes. Despite its importance, no previous studies have addressed cold-start bundle recommendation. Moreover, existing methods for cold-start item recommendation overly rely on historical information, even for unpopular bundles, failing to tackle the primary challenge of the highly skewed distribution of bundle interactions. In this work, we propose CoHeat (Popularity-based Coalescence and Curriculum Heating), an accurate approach for the cold-start bundle recommendation. CoHeat tackles the highly skewed distribution of bundle interactions by incorporating both historical and affiliation information based on the bundle's popularity when estimating the user-bundle relationship. Furthermore, CoHeat effectively learns latent representations by exploiting curriculum learning and co
    
[^26]: 重塑顺序推荐系统：利用内容增强语言建模学习动态用户兴趣

    Reformulating Sequential Recommendation: Learning Dynamic User Interest with Content-enriched Language Modeling. (arXiv:2309.10435v1 [cs.IR])

    [http://arxiv.org/abs/2309.10435](http://arxiv.org/abs/2309.10435)

    本研究提出了一个新的顺序推荐范式 LANCER，利用预训练语言模型的语义理解能力生成更加人性化的个性化推荐。在多个基准数据集上的实验结果表明，该方法有效且有希望，并为了解顺序推荐的影响提供了有价值的见解。

    

    推荐系统对在线应用至关重要，而顺序推荐由于其表达能力强大，能够捕捉到动态用户兴趣而广泛使用。然而，先前的顺序建模方法在捕捉上下文信息方面仍存在局限性。主要的原因是语言模型常常缺乏对领域特定知识和物品相关文本内容的理解。为了解决这个问题，我们采用了一种新的顺序推荐范式，并提出了LANCER，它利用预训练语言模型的语义理解能力生成个性化推荐。我们的方法弥合了语言模型与推荐系统之间的差距，产生了更加人性化的推荐。通过对多个基准数据集上的实验，我们验证了我们的方法的有效性，展示了有希望的结果，并提供了对我们模型对顺序推荐的影响的有价值的见解。

    Recommender systems are essential for online applications, and sequential recommendation has enjoyed significant prevalence due to its expressive ability to capture dynamic user interests. However, previous sequential modeling methods still have limitations in capturing contextual information. The primary reason for this issue is that language models often lack an understanding of domain-specific knowledge and item-related textual content. To address this issue, we adopt a new sequential recommendation paradigm and propose LANCER, which leverages the semantic understanding capabilities of pre-trained language models to generate personalized recommendations. Our approach bridges the gap between language models and recommender systems, resulting in more human-like recommendations. We demonstrate the effectiveness of our approach through experiments on several benchmark datasets, showing promising results and providing valuable insights into the influence of our model on sequential recomm
    
[^27]: 评估潮起潮落：对不同平台间问答趋势的深入分析

    Evaluating the Ebb and Flow: An In-depth Analysis of Question-Answering Trends across Diverse Platforms. (arXiv:2309.05961v1 [cs.SI])

    [http://arxiv.org/abs/2309.05961](http://arxiv.org/abs/2309.05961)

    本文通过对六个社区问答平台的研究，发现了查询的元数据、问题构成方式和用户互动水平与第一个回答时间之间的关联，并利用机器学习模型预测查询是否能够迅速获得回答。

    

    社区问答平台因其快速回答用户查询的能力而越来越受欢迎。这些回答速度的快慢取决于查询特定和用户相关的因素的综合。本文通过研究六个高度流行的社区问答平台，分析了这些因素在其中的作用。我们的调查揭示了问题的第一个回答所花费的时间与元数据、问题的构成方式和用户之间的互动水平之间的关联。此外，通过使用传统的机器学习模型分析这些元数据和用户互动模式，我们试图预测哪些查询将迅速获得初始回答。

    Community Question Answering (CQA) platforms steadily gain popularity as they provide users with fast responses to their queries. The swiftness of these responses is contingent on a mixture of query-specific and user-related elements. This paper scrutinizes these contributing factors within the context of six highly popular CQA platforms, identified through their standout answering speed. Our investigation reveals a correlation between the time taken to yield the first response to a question and several variables: the metadata, the formulation of the questions, and the level of interaction among users. Additionally, by employing conventional machine learning models to analyze these metadata and patterns of user interaction, we endeavor to predict which queries will receive their initial responses promptly.
    
[^28]: 利用物品嵌入改进在稀疏和倾斜数据集中对位置偏差的估计

    Improving position bias estimation against sparse and skewed dataset with item embedding. (arXiv:2305.13931v1 [cs.IR])

    [http://arxiv.org/abs/2305.13931](http://arxiv.org/abs/2305.13931)

    该研究提出了一种利用物品嵌入来缓解广告营销领域中位置偏差稀疏性问题的回归EM算法变体。

    

    在学习排名中，估计位置偏差是一个众所周知的挑战。电子商务应用程序中的点击数据（例如广告定位和搜索引擎）提供了隐含但丰富的反馈，以改进个性化排名。然而，点击数据本质上包括各种偏差，例如位置偏差。点击建模旨在去噪有偏的点击数据并提取可靠的信号。已经提出了随机化结果和回归期望最大化算法来解决位置偏差。但是，这两种方法都需要各种观察值对（项目、位置）。然而，在广告营销的实际情况下，营销人员经常按固定的预定顺序显示广告，估计因此而受到影响。我们将位置偏差估计中的（项目、位置）稀疏性问题作为新问题，并提出了一种利用物品嵌入来缓解稀疏问题的回归EM算法变体。我们首先使用合成数据集评估我们的方法。

    Estimating position bias is a well-known challenge in Learning to rank (L2R). Click data in e-commerce applications, such as advertisement targeting and search engines, provides implicit but abundant feedback to improve personalized rankings. However, click data inherently include various biases like position bias. Click modeling is aimed at denoising biases in click data and extracting reliable signals. Result Randomization and Regression Expectation-maximization algorithm have been proposed to solve position bias. Both methods require various pairs of observations (item, position). However, in real cases of advertising, marketers frequently display advertisements in a fixed pre-determined order, and estimation suffers from it. We propose this sparsity of (item, position) in position bias estimation as a novel problem, and we propose a variant of the Regression EM algorithm which utilizes item embeddings to alleviate the issue of the sparsity. With a synthetic dataset, we first evalua
    

