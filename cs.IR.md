# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Learning from Negative User Feedback and Measuring Responsiveness for Sequential Recommenders.](http://arxiv.org/abs/2308.12256) | 该论文研究了在顺序推荐系统中如何从负面用户反馈中学习以及如何度量响应能力。通过采用“不推荐”损失函数，将显式和隐式的负面用户反馈纳入推荐系统的训练目标，该方法有效提升了推荐系统的性能。 |
| [^2] | [LLMRec: Benchmarking Large Language Models on Recommendation Task.](http://arxiv.org/abs/2308.12241) | LLMRec是一个基于LLMs的推荐系统，用于对LLMs在推荐任务上进行基准测试。研究结果显示，LLMs在顺序推荐和直接推荐等准确性任务方面表现出中等熟练程度，并且在指令遵循能力方面具有与最先进方法相当的性能。 |
| [^3] | [Counterfactual Graph Augmentation for Consumer Unfairness Mitigation in Recommender Systems.](http://arxiv.org/abs/2308.12083) | 本文提出了一种利用反事实解释来增强用户-物品交互集合的方法，以在推荐过程中实现更公平的结果。 |
| [^4] | [Hybrid Retrieval and Multi-stage Text Ranking Solution at TREC 2022 Deep Learning Track.](http://arxiv.org/abs/2308.12039) | 本论文介绍了在 TREC 2022 深度学习赛道中，我们采用的混合文本检索和多阶段文本排名方法，在检索阶段结合了传统稀疏检索和神经稠密检索的两种结构，在排名阶段构建了全交互式排名模型和轻量级子排名模块，评估结果证明了方法的有效性。 |
| [^5] | [LKPNR: LLM and KG for Personalized News Recommendation Framework.](http://arxiv.org/abs/2308.12028) | 该论文提出了一种新型的个性化新闻推荐框架，将大型语言模型（LLM）和知识图谱（KG）结合起来，以提高复杂新闻文本的语义理解能力，并解决传统方法在推荐结果和非活跃用户方面的不足。 |
| [^6] | [Economic Recommender Systems -- A Systematic Review.](http://arxiv.org/abs/2308.11998) | 这篇综述调查了经济推荐系统的现有文献，该领域的研究侧重于推荐系统对终端用户的价值，但现实中推荐系统也能直接用于实现组织的经济目标，例如通过考虑价格意识和盈利能力等因素来改善推荐服务。 |
| [^7] | [Integrating the Wikidata Taxonomy into YAGO.](http://arxiv.org/abs/2308.11884) | 本文介绍了将整个Wikidata分类体系尽可能地合并到YAGO知识库中的工作，为YAGO添加了丰富的信息类别，并保持了知识库的逻辑一致性。 |
| [^8] | [CLIP Multi-modal Hashing: A new baseline CLIPMH.](http://arxiv.org/abs/2308.11797) | CLIP Multi-modal Hashing (CLIPMH) is a new baseline method that improves the retrieval performance of multi-modal hashing by using the CLIP model to extract text and image features and fusing them to generate hash codes. Compared to state-of-the-art methods, CLIPMH significantly enhances performance (maximum increase of 8.38%) and has advantages over text and visual backbone networks. |
| [^9] | [Improving Detection of ChatGPT-Generated Fake Science Using Real Publication Text: Introducing xFakeBibs a Supervised-Learning Network Algorithm.](http://arxiv.org/abs/2308.11767) | 本文介绍了一种能够提高对ChatGPT生成的假科学进行检测的算法。通过使用一种新设计的监督机器学习算法，该算法能够准确地将机器生成的出版物与科学家生成的出版物区分开来。结果表明，ChatGPT在技术术语方面与真实科学存在显著差异。算法在分类过程中取得了较高的准确率。 |
| [^10] | [Knowledge Graph Prompting for Multi-Document Question Answering.](http://arxiv.org/abs/2308.11730) | 这篇论文提出了一种知识图谱引导的方法，用于在多文档问答任务中为大型语言模型（LLMs）提示正确的上下文。通过构建多个文档上的知识图谱，并设计基于语言模型的图遍历器，该方法能够帮助LLMs在MD-QA中进行答案预测。 |
| [^11] | [Invariant representation learning for sequential recommendation.](http://arxiv.org/abs/2308.11728) | 本论文介绍了一种名为Irl4Rec的新颖序列推荐框架，利用不变表示学习和考虑虚假关系，提高了推荐准确性。该框架在比较分析和消融研究中都表现出了优越性能。 |
| [^12] | [Task Relation-aware Continual User Representation Learning.](http://arxiv.org/abs/2306.01792) | 本文提出了一种新的持续用户表示学习方法TERACON，它能够学习通用的用户表示，而不是为每个任务学习任务特定的用户表示，具有很强的实用性和学习能力。 |
| [^13] | [A Tale of Two Graphs: Freezing and Denoising Graph Structures for Multimodal Recommendation.](http://arxiv.org/abs/2211.06924) | 本文研究了多模态推荐中的图结构冻结和去噪问题，提出了一个简单而有效的模型FREEDOM，它同时冻结物品-物品图和去噪用户-物品交互图，取得了相比LATTICE更高的性能。 |

# 详细

[^1]: 从负面用户反馈中学习和度量顺序推荐系统的响应能力

    Learning from Negative User Feedback and Measuring Responsiveness for Sequential Recommenders. (arXiv:2308.12256v1 [cs.IR])

    [http://arxiv.org/abs/2308.12256](http://arxiv.org/abs/2308.12256)

    该论文研究了在顺序推荐系统中如何从负面用户反馈中学习以及如何度量响应能力。通过采用“不推荐”损失函数，将显式和隐式的负面用户反馈纳入推荐系统的训练目标，该方法有效提升了推荐系统的性能。

    

    由于其在建模用户偏好方面的优势，顺序推荐系统在工业中被广泛使用。尽管这些模型擅长学习用户的正面兴趣，但对于从负面用户反馈中学习却付出了较少的关注。负面用户反馈是用户控制的重要手段，并伴随着对推荐系统应该快速响应和减少类似推荐的期望。然而，负面反馈信号在顺序检索模型的训练目标中经常被忽视，这些模型主要旨在预测用户的正面交互。在这项工作中，我们使用“不推荐”损失函数将显式和隐式的负面用户反馈纳入顺序推荐系统在检索阶段的训练目标，优化不推荐带有负面反馈的项目的对数似然。我们通过对大规模工业推荐系统进行实时实验来证明这种方法的有效性。

    Sequential recommenders have been widely used in industry due to their strength in modeling user preferences. While these models excel at learning a user's positive interests, less attention has been paid to learning from negative user feedback. Negative user feedback is an important lever of user control, and comes with an expectation that recommenders should respond quickly and reduce similar recommendations to the user. However, negative feedback signals are often ignored in the training objective of sequential retrieval models, which primarily aim at predicting positive user interactions. In this work, we incorporate explicit and implicit negative user feedback into the training objective of sequential recommenders in the retrieval stage using a "not-to-recommend" loss function that optimizes for the log-likelihood of not recommending items with negative feedback. We demonstrate the effectiveness of this approach using live experiments on a large-scale industrial recommender system
    
[^2]: LLMRec: 在推荐任务上对大型语言模型进行基准测试

    LLMRec: Benchmarking Large Language Models on Recommendation Task. (arXiv:2308.12241v1 [cs.IR])

    [http://arxiv.org/abs/2308.12241](http://arxiv.org/abs/2308.12241)

    LLMRec是一个基于LLMs的推荐系统，用于对LLMs在推荐任务上进行基准测试。研究结果显示，LLMs在顺序推荐和直接推荐等准确性任务方面表现出中等熟练程度，并且在指令遵循能力方面具有与最先进方法相当的性能。

    

    近期，像ChatGPT这样的大型语言模型（LLMs）的快速发展通过增强对话模型的能力，显著提升了自然语言处理任务。然而，LLMs在推荐领域的应用尚未得到深入研究。为了填补这一空白，我们提出了LLMRec，这是一个基于LLMs的推荐系统，旨在对多种推荐任务进行LLMs的基准测试。具体而言，我们对几种热门的现成LLMs进行了基准测试，包括ChatGPT、LLaMA和ChatGLM，涵盖了评分预测、顺序推荐、直接推荐、解释生成和评论摘要等五个推荐任务。此外，我们研究了有监督微调的有效性，以提高LLMs的指令遵循能力。基准测试结果表明，LLMs在基于准确性的任务（如顺序推荐和直接推荐）上只表现出中等的熟练程度。然而，它们在性能上与最先进的方法控制能力相当。

    Recently, the fast development of Large Language Models (LLMs) such as ChatGPT has significantly advanced NLP tasks by enhancing the capabilities of conversational models. However, the application of LLMs in the recommendation domain has not been thoroughly investigated. To bridge this gap, we propose LLMRec, a LLM-based recommender system designed for benchmarking LLMs on various recommendation tasks. Specifically, we benchmark several popular off-the-shelf LLMs, such as ChatGPT, LLaMA, ChatGLM, on five recommendation tasks, including rating prediction, sequential recommendation, direct recommendation, explanation generation, and review summarization. Furthermore, we investigate the effectiveness of supervised finetuning to improve LLMs' instruction compliance ability. The benchmark results indicate that LLMs displayed only moderate proficiency in accuracy-based tasks such as sequential and direct recommendation. However, they demonstrated comparable performance to state-of-the-art me
    
[^3]: 消费者不公平性在推荐系统中的对策：反事实图增强

    Counterfactual Graph Augmentation for Consumer Unfairness Mitigation in Recommender Systems. (arXiv:2308.12083v1 [cs.IR])

    [http://arxiv.org/abs/2308.12083](http://arxiv.org/abs/2308.12083)

    本文提出了一种利用反事实解释来增强用户-物品交互集合的方法，以在推荐过程中实现更公平的结果。

    

    在推荐系统文献中，可解释性和公平性成为了两个重要的考虑角度。然而，先前的研究大多单独处理这两个问题，例如向消费者解释为什么推荐了某个物品或者减轻推荐效用中的差异影响。没有任何一项研究利用可解释性技术来提供关于不公平性的信息。本文提出了一种方法，利用反事实解释来增强用户-物品交互集合，以便在推荐过程中使用它们可以得到更公平的结果。通过将用户-物品交互建模为二分图，我们的方法通过识别新的用户-物品边来增强二分图，这些边不仅可以解释原来的不公平性，并且还可以减轻不公平性。通过在两个公共数据集上进行实验，我们的方法有效地在公平性和推荐效用之间找到了更好的平衡，超越了最先进的减轻过程。

    In recommendation literature, explainability and fairness are becoming two prominent perspectives to consider. However, prior works have mostly addressed them separately, for instance by explaining to consumers why a certain item was recommended or mitigating disparate impacts in recommendation utility. None of them has leveraged explainability techniques to inform unfairness mitigation. In this paper, we propose an approach that relies on counterfactual explanations to augment the set of user-item interactions, such that using them while inferring recommendations leads to fairer outcomes. Modeling user-item interactions as a bipartite graph, our approach augments the latter by identifying new user-item edges that not only can explain the original unfairness by design, but can also mitigate it. Experiments on two public data sets show that our approach effectively leads to a better trade-off between fairness and recommendation utility compared with state-of-the-art mitigation procedure
    
[^4]: TREC 2022深度学习赛道的混合检索和多阶段文本排名解决方案

    Hybrid Retrieval and Multi-stage Text Ranking Solution at TREC 2022 Deep Learning Track. (arXiv:2308.12039v1 [cs.IR])

    [http://arxiv.org/abs/2308.12039](http://arxiv.org/abs/2308.12039)

    本论文介绍了在 TREC 2022 深度学习赛道中，我们采用的混合文本检索和多阶段文本排名方法，在检索阶段结合了传统稀疏检索和神经稠密检索的两种结构，在排名阶段构建了全交互式排名模型和轻量级子排名模块，评估结果证明了方法的有效性。

    

    大规模文本检索技术在各种实际业务场景中被广泛使用。本文介绍了我们在TREC 2022深度学习赛道中的系统。我们解释了我们解决方案中采用的混合文本检索和多阶段文本排名方法。检索阶段结合了传统稀疏检索和神经稠密检索的两种结构。在排名阶段，除了基于大型预训练语言模型构建的全交互式排名模型外，我们还提出了一个轻量级的子排名模块，进一步提升了最终的文本排名性能。评估结果证明了我们提出的方法的有效性。我们的模型在段落排名和文档排名的测试集上分别达到了第1名和第4名。

    Large-scale text retrieval technology has been widely used in various practical business scenarios. This paper presents our systems for the TREC 2022 Deep Learning Track. We explain the hybrid text retrieval and multi-stage text ranking method adopted in our solution. The retrieval stage combined the two structures of traditional sparse retrieval and neural dense retrieval. In the ranking stage, in addition to the full interaction-based ranking model built on large pre-trained language model, we also proposes a lightweight sub-ranking module to further enhance the final text ranking performance. Evaluation results demonstrate the effectiveness of our proposed approach. Our models achieve the 1st and 4th rank on the test set of passage ranking and document ranking respectively.
    
[^5]: LKPNR: 基于LLM和KG的个性化新闻推荐框架

    LKPNR: LLM and KG for Personalized News Recommendation Framework. (arXiv:2308.12028v1 [cs.IR])

    [http://arxiv.org/abs/2308.12028](http://arxiv.org/abs/2308.12028)

    该论文提出了一种新型的个性化新闻推荐框架，将大型语言模型（LLM）和知识图谱（KG）结合起来，以提高复杂新闻文本的语义理解能力，并解决传统方法在推荐结果和非活跃用户方面的不足。

    

    精确地向用户推荐候选新闻文章是个性化新闻推荐系统面临的基本挑战。传统方法通常很难理解新闻文本中复杂的语义信息，导致推荐结果不尽人意。此外，传统方法更适用于具有丰富历史行为的活跃用户，但无法有效解决非活跃用户的“长尾问题”。为了解决这些问题，本研究提出了一个将大型语言模型（LLM）和知识图谱（KG）结合进传统方法的语义表示的新型通用框架。为了提高对复杂新闻文本的语义理解能力，我们利用LLMs强大的文本理解能力生成包含丰富语义信息的新闻表示。此外，我们的方法结合了新闻实体的信息，并通过KG中的多个跳数挖掘高阶结构信息，从而缓解了这个问题。

    Accurately recommending candidate news articles to users is a basic challenge faced by personalized news recommendation systems. Traditional methods are usually difficult to grasp the complex semantic information in news texts, resulting in unsatisfactory recommendation results. Besides, these traditional methods are more friendly to active users with rich historical behaviors. However, they can not effectively solve the "long tail problem" of inactive users. To address these issues, this research presents a novel general framework that combines Large Language Models (LLM) and Knowledge Graphs (KG) into semantic representations of traditional methods. In order to improve semantic understanding in complex news texts, we use LLMs' powerful text understanding ability to generate news representations containing rich semantic information. In addition, our method combines the information about news entities and mines high-order structural information through multiple hops in KG, thus allevia
    
[^6]: 经济推荐系统--一项系统性的综述

    Economic Recommender Systems -- A Systematic Review. (arXiv:2308.11998v1 [cs.IR])

    [http://arxiv.org/abs/2308.11998](http://arxiv.org/abs/2308.11998)

    这篇综述调查了经济推荐系统的现有文献，该领域的研究侧重于推荐系统对终端用户的价值，但现实中推荐系统也能直接用于实现组织的经济目标，例如通过考虑价格意识和盈利能力等因素来改善推荐服务。

    

    当今许多在线服务都为用户提供个性化推荐。这些推荐通常旨在满足用户的需求，例如在信息过载的情况下快速找到相关内容。因此，该领域的学术文献主要关注推荐系统对终端用户的价值。在这种情况下，一个基本的假设是通过推荐所实现的改进服务将积极地影响组织的目标，例如通过增加客户保留率或忠诚度。然而，在现实中，推荐系统可以更直接地用于实现组织的经济目标，通过将价格意识和盈利能力等货币考虑因素纳入基本的推荐模型。本研究通过系统性综述方法对现有关于我们所称的经济推荐系统的文献进行了调查，帮助我们识别出了133个相关的文献。

    Many of today's online services provide personalized recommendations to their users. Such recommendations are typically designed to serve certain user needs, e.g., to quickly find relevant content in situations of information overload. Correspondingly, the academic literature in the field largely focuses on the value of recommender systems for the end user. In this context, one underlying assumption is that the improved service that is achieved through the recommendations will in turn positively impact the organization's goals, e.g., in the form of higher customer retention or loyalty. However, in reality, recommender systems can be used to target organizational economic goals more directly by incorporating monetary considerations such as price awareness and profitability aspects into the underlying recommendation models. In this work, we survey the existing literature on what we call Economic Recommender Systems based on a systematic review approach that helped us identify 133 relevan
    
[^7]: 将Wikidata分类体系集成到YAGO中

    Integrating the Wikidata Taxonomy into YAGO. (arXiv:2308.11884v1 [cs.AI])

    [http://arxiv.org/abs/2308.11884](http://arxiv.org/abs/2308.11884)

    本文介绍了将整个Wikidata分类体系尽可能地合并到YAGO知识库中的工作，为YAGO添加了丰富的信息类别，并保持了知识库的逻辑一致性。

    

    Wikidata是最大的公共通用知识库之一。然而，由于它的合作性质，其模式和分类体系变得复杂。在YAGO 4知识库中，我们将Wikidata与Schema.org的本体论结合起来，减少和清理分类体系和约束条件，并使其能够在数据上运行自动推理器。然而，这也舍弃了大部分的Wikidata分类体系。在本文中，我们展示了将整个Wikidata分类体系尽可能地合并到YAGO知识库中的工作。我们特别关注逻辑约束和类与实例的细致区分。我们的工作创建了YAGO 4.5，为YAGO添加了丰富的信息类别，同时保持了知识库的逻辑一致性。

    Wikidata is one of the largest public general-purpose Knowledge Bases (KBs). Yet, due to its collaborative nature, its schema and taxonomy have become convoluted. For the YAGO 4 KB, we combined Wikidata with the ontology from Schema.org, which reduced and cleaned up the taxonomy and constraints and made it possible to run automated reasoners on the data. However, it also cut away large parts of the Wikidata taxonomy. In this paper, we present our effort to merge the entire Wikidata taxonomy into the YAGO KB as much as possible. We pay particular attention to logical constraints and a careful distinction of classes and instances. Our work creates YAGO 4.5, which adds a rich layer of informative classes to YAGO, while at the same time keeping the KB logically consistent.
    
[^8]: CLIP多模哈希：一种新的基准CLIPMH

    CLIP Multi-modal Hashing: A new baseline CLIPMH. (arXiv:2308.11797v1 [cs.CV])

    [http://arxiv.org/abs/2308.11797](http://arxiv.org/abs/2308.11797)

    CLIP Multi-modal Hashing (CLIPMH) is a new baseline method that improves the retrieval performance of multi-modal hashing by using the CLIP model to extract text and image features and fusing them to generate hash codes. Compared to state-of-the-art methods, CLIPMH significantly enhances performance (maximum increase of 8.38%) and has advantages over text and visual backbone networks.

    

    多模哈希方法被广泛应用于多媒体检索中，可以将多源数据融合生成二进制哈希码。然而，当前的多模方法存在检索精度低的问题，原因在于各个主干网络的特征表达能力有限，并且未经过大规模无监督多模数据的联合预训练。为了解决这个问题，我们提出了一种新的基准CLIP多模哈希（CLIPMH）方法。它利用CLIP模型提取文本和图像特征，然后融合生成哈希码。CLIP改善了每个模态特征的表达能力，从而极大地提高了多模哈希方法的检索性能。与最先进的无监督和有监督多模哈希方法进行比较，实验证明，所提出的CLIPMH可以显著提高性能（最大增加8.38%）。CLIP还在文本和视觉主干网络方面具有很大优势。

    The multi-modal hashing method is widely used in multimedia retrieval. It can fuse multi-source data to generate binary hash code. However, the current multi-modal methods have the problem of low retrieval accuracy. The reason is that the individual backbone networks have limited feature expression capabilities and are not jointly pre-trained on large-scale unsupervised multi-modal data. To solve this problem, we propose a new baseline CLIP Multi-modal Hashing (CLIPMH) method. It uses CLIP model to extract text and image features, and then fuse to generate hash code. CLIP improves the expressiveness of each modal feature. In this way, it can greatly improve the retrieval performance of multi-modal hashing methods. In comparison to state-of-the-art unsupervised and supervised multi-modal hashing methods, experiments reveal that the proposed CLIPMH can significantly enhance performance (Maximum increase of 8.38%). CLIP also has great advantages over the text and visual backbone networks 
    
[^9]: 提高ChatGPT生成的假科学检测的方法：引入xFakeBibs监督学习网络算法

    Improving Detection of ChatGPT-Generated Fake Science Using Real Publication Text: Introducing xFakeBibs a Supervised-Learning Network Algorithm. (arXiv:2308.11767v1 [cs.CL])

    [http://arxiv.org/abs/2308.11767](http://arxiv.org/abs/2308.11767)

    本文介绍了一种能够提高对ChatGPT生成的假科学进行检测的算法。通过使用一种新设计的监督机器学习算法，该算法能够准确地将机器生成的出版物与科学家生成的出版物区分开来。结果表明，ChatGPT在技术术语方面与真实科学存在显著差异。算法在分类过程中取得了较高的准确率。

    

    ChatGPT正在成为现实。本文展示了如何区分ChatGPT生成的出版物与科学家生成的出版物。通过使用一种新设计的监督机器学习算法，我们演示了如何检测机器生成的出版物和科学家生成的出版物。该算法使用100个真实出版物摘要进行训练，然后采用10倍交叉验证方法建立了一个接受范围的下限和上限。与ChatGPT内容进行比较，明显可见ChatGPT仅贡献了23\%的二元组内容，这比其他10个交叉验证中的任何一个都少50\%。这个分析凸显了ChatGPT在技术术语上与真实科学的明显差异。在对每篇文章进行分类时，xFakeBibs算法准确地将98篇出版物识别为假的，有2篇文献错误地分类为真实出版物。尽管这项工作引入了一种算法应用

    ChatGPT is becoming a new reality. In this paper, we show how to distinguish ChatGPT-generated publications from counterparts produced by scientists. Using a newly designed supervised Machine Learning algorithm, we demonstrate how to detect machine-generated publications from those produced by scientists. The algorithm was trained using 100 real publication abstracts, followed by a 10-fold calibration approach to establish a lower-upper bound range of acceptance. In the comparison with ChatGPT content, it was evident that ChatGPT contributed merely 23\% of the bigram content, which is less than 50\% of any of the other 10 calibrating folds. This analysis highlights a significant disparity in technical terms where ChatGPT fell short of matching real science. When categorizing the individual articles, the xFakeBibs algorithm accurately identified 98 out of 100 publications as fake, with 2 articles incorrectly classified as real publications. Though this work introduced an algorithmic app
    
[^10]: 多文档问答中的知识图谱引导

    Knowledge Graph Prompting for Multi-Document Question Answering. (arXiv:2308.11730v1 [cs.CL])

    [http://arxiv.org/abs/2308.11730](http://arxiv.org/abs/2308.11730)

    这篇论文提出了一种知识图谱引导的方法，用于在多文档问答任务中为大型语言模型（LLMs）提示正确的上下文。通过构建多个文档上的知识图谱，并设计基于语言模型的图遍历器，该方法能够帮助LLMs在MD-QA中进行答案预测。

    

    大型语言模型（LLMs）的“预训练、提示、预测”范式在开放域问答（OD-QA）中取得了显著的成功。然而，很少有工作在多文档问答（MD-QA）场景下探索这个范式，这是一个要求对不同文档的内容和结构之间的逻辑关联有深入理解的任务。为了填补这一重要的空白，我们提出了一种知识图谱引导（KGP）方法，用于在MD-QA中为LLMs提示正确的上下文，该方法包括图构建模块和图遍历模块。对于图构建，我们使用节点来表示文段或文档结构（例如，页面/表格），而使用边来表示文段之间的语义/词汇相似性或者文档内的结构关系。对于图遍历，我们设计了一个基于LM的图遍历器，它在节点之间导航并收集支持性的文段，以帮助LLMs在MD-QA中进行答案预测。

    The 'pre-train, prompt, predict' paradigm of large language models (LLMs) has achieved remarkable success in open-domain question answering (OD-QA). However, few works explore this paradigm in the scenario of multi-document question answering (MD-QA), a task demanding a thorough understanding of the logical associations among the contents and structures of different documents. To fill this crucial gap, we propose a Knowledge Graph Prompting (KGP) method to formulate the right context in prompting LLMs for MD-QA, which consists of a graph construction module and a graph traversal module. For graph construction, we create a knowledge graph (KG) over multiple documents with nodes symbolizing passages or document structures (e.g., pages/tables), and edges denoting the semantic/lexical similarity between passages or intra-document structural relations. For graph traversal, we design an LM-guided graph traverser that navigates across nodes and gathers supporting passages assisting LLMs in MD
    
[^11]: 序列推荐的不变表示学习

    Invariant representation learning for sequential recommendation. (arXiv:2308.11728v1 [cs.IR])

    [http://arxiv.org/abs/2308.11728](http://arxiv.org/abs/2308.11728)

    本论文介绍了一种名为Irl4Rec的新颖序列推荐框架，利用不变表示学习和考虑虚假关系，提高了推荐准确性。该框架在比较分析和消融研究中都表现出了优越性能。

    

    序列推荐涉及根据用户的历史物品序列自动推荐下一个物品。虽然大多数先前的研究采用RNN或transformer方法从物品序列中获取信息，为每个用户-物品对生成概率，并推荐前几个物品，但这些方法通常忽视了虚假关系带来的挑战。本文特别解决了这些虚假关系问题。我们介绍了一个新颖的序列推荐框架称为Irl4Rec。该框架利用不变表示学习，并在模型训练过程中考虑了虚假变量和调整变量之间的关系，有助于识别虚假关系。比较分析表明，我们的框架优于三种典型方法，凸显了我们模型的有效性。此外，消融研究进一步证明了我们的模型在检测虚假关系中的关键作用。

    Sequential recommendation involves automatically recommending the next item to users based on their historical item sequence. While most prior research employs RNN or transformer methods to glean information from the item sequence-generating probabilities for each user-item pair and recommending the top items, these approaches often overlook the challenge posed by spurious relationships. This paper specifically addresses these spurious relations. We introduce a novel sequential recommendation framework named Irl4Rec. This framework harnesses invariant learning and employs a new objective that factors in the relationship between spurious variables and adjustment variables during model training. This approach aids in identifying spurious relations. Comparative analyses reveal that our framework outperforms three typical methods, underscoring the effectiveness of our model. Moreover, an ablation study further demonstrates the critical role our model plays in detecting spurious relations.
    
[^12]: 任务关系感知的持续用户表示学习

    Task Relation-aware Continual User Representation Learning. (arXiv:2306.01792v1 [cs.IR])

    [http://arxiv.org/abs/2306.01792](http://arxiv.org/abs/2306.01792)

    本文提出了一种新的持续用户表示学习方法TERACON，它能够学习通用的用户表示，而不是为每个任务学习任务特定的用户表示，具有很强的实用性和学习能力。

    

    用户建模是基于其过去行为学习将用户表示为低维表示空间的方法，它受到了工业界提供个性化服务的兴趣激增。以往的用户建模工作主要集中在学习为单一任务而设计的任务特定用户表示上。然而，由于为每个任务学习任务特定用户表示是不可行的，因此最近的研究引入了通用用户表示的概念，即与多种任务相关的更广义用户表示。尽管这些方法非常有效，但由于数据需求、灾难性遗忘以及为持续添加的任务提供有限的学习能力，现有的学习通用用户表示的方法在实际应用中是不切实际的。本文提出了一种新颖的持续用户表示学习方法TERACON，其学习能力不受任务数量限制。

    User modeling, which learns to represent users into a low-dimensional representation space based on their past behaviors, got a surge of interest from the industry for providing personalized services to users. Previous efforts in user modeling mainly focus on learning a task-specific user representation that is designed for a single task. However, since learning task-specific user representations for every task is infeasible, recent studies introduce the concept of universal user representation, which is a more generalized representation of a user that is relevant to a variety of tasks. Despite their effectiveness, existing approaches for learning universal user representations are impractical in real-world applications due to the data requirement, catastrophic forgetting and the limited learning capability for continually added tasks. In this paper, we propose a novel continual user representation learning method, called TERACON, whose learning capability is not limited as the number 
    
[^13]: 两个图的故事：用于多模态推荐的图结构冻结和去噪

    A Tale of Two Graphs: Freezing and Denoising Graph Structures for Multimodal Recommendation. (arXiv:2211.06924v3 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2211.06924](http://arxiv.org/abs/2211.06924)

    本文研究了多模态推荐中的图结构冻结和去噪问题，提出了一个简单而有效的模型FREEDOM，它同时冻结物品-物品图和去噪用户-物品交互图，取得了相比LATTICE更高的性能。

    

    利用多模态特征（例如图像和文本描述）的多模态推荐系统通常比仅基于用户-物品交互的一般推荐模型具有更好的推荐准确性。然而，先前的研究通常将多模态特征融合到物品ID嵌入中，以丰富物品表示，因此无法捕捉潜在的语义物品-物品结构。在这种情况下，LATTICE提出了显式学习物品之间的潜在结构，并在多模态推荐方面取得了最先进的性能。然而，我们认为LATTICE的潜在图结构学习既低效又不必要。实验证明，在训练之前冻结其物品-物品结构也可以实现竞争性的性能。基于这一发现，我们提出了一个简单而有效的模型，名为FREEDOM，它同时冻结物品-物品图并去噪用户-物品交互图以进行多模态推荐。

    Multimodal recommender systems utilizing multimodal features (e.g., images and textual descriptions) typically show better recommendation accuracy than general recommendation models based solely on user-item interactions. Generally, prior work fuses multimodal features into item ID embeddings to enrich item representations, thus failing to capture the latent semantic item-item structures. In this context, LATTICE proposes to learn the latent structure between items explicitly and achieves state-of-the-art performance for multimodal recommendations. However, we argue the latent graph structure learning of LATTICE is both inefficient and unnecessary. Experimentally, we demonstrate that freezing its item-item structure before training can also achieve competitive performance. Based on this finding, we propose a simple yet effective model, dubbed as FREEDOM, that FREEzes the item-item graph and DenOises the user-item interaction graph simultaneously for Multimodal recommendation. Theoretic
    

