# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [CorpusBrain++: A Continual Generative Pre-Training Framework for Knowledge-Intensive Language Tasks](https://arxiv.org/abs/2402.16767) | 本研究介绍了知识密集型语言任务中的持续文档学习任务，并建立了一个新的评估数据集，旨在探索检索模型有效处理动态检索场景的能力。 |
| [^2] | [LLM-Assisted Multi-Teacher Continual Learning for Visual Question Answering in Robotic Surgery](https://arxiv.org/abs/2402.16664) | LLM辅助的多教师持续学习为机器人手术中的视觉问答系统更新提供了解决新任务需求的方法，同时解决了外科领域中的大领域转变和数据不平衡问题。 |
| [^3] | [BOXREC: Recommending a Box of Preferred Outfits in Online Shopping](https://arxiv.org/abs/2402.16660) | 该论文提出了一个名为BOXREC的盒子推荐框架，首次考虑了用户对单品价格范围的偏好和整体购物预算，以生成一组符合用户喜好的服装推荐。 |
| [^4] | [PAQA: Toward ProActive Open-Retrieval Question Answering](https://arxiv.org/abs/2402.16608) | 本研究针对对话式搜索系统中存在的数据集不足问题，提出了PAQA方法，通过考虑用户查询和文档中的歧义，生成相关澄清问题，从而改善对话搜索系统的效果。 |
| [^5] | [Integrating Large Language Models with Graphical Session-Based Recommendation](https://arxiv.org/abs/2402.16539) | 本文提出了一种名为LLMGR的框架，将大型语言模型与图形会话推荐相结合，有效地弥合了推荐系统中大型语言模型在会话推荐领域的适应性差距 |
| [^6] | [Pre-training Cross-lingual Open Domain Question Answering with Large-scale Synthetic Supervision](https://arxiv.org/abs/2402.16508) | 本研究提出了一种基于自监督方法的单个编码-解码模型来解决跨语言问答问题，通过利用维基百科内的跨语言链接结构合成监督信号，取得了优于其他方法的效果。 |
| [^7] | [Retrouver l'inventeur-auteur : la lev{\'e}e d'homonymies d'autorat entre les brevets et les publications scientifiques](https://arxiv.org/abs/2402.16440) | 通过使用IPC和IPCCAT API，本研究提出了一种方法来解决发明者兼学术作者的身份匹配问题，成功匹配了专利与论文，解决了作者身份消歧的挑战，错误率低于5%。 |
| [^8] | [Effect of utterance duration and phonetic content on speaker identification using second-order statistical methods](https://arxiv.org/abs/2402.16429) | 本文研究了测试语音材料的内容对使用二阶统计方法进行说话人识别的影响，发现液体音和滑音、元音，特别是鼻元音和鼻辅音对说话人识别特别重要。 |
| [^9] | [An Integrated Data Processing Framework for Pretraining Foundation Models](https://arxiv.org/abs/2402.16358) | 提出了一个集成了处理模块和分析模块的数据处理框架，旨在改善数据质量并展示其有效性。 |
| [^10] | [Deep Rating Elicitation for New Users in Collaborative Filtering](https://arxiv.org/abs/2402.16327) | 提出了一种端到端深度学习框架，用于协同过滤中新用户的评分调查，可以一次选择所有种子项目，并通过考虑非线性交互作用来推断用户偏好。 |
| [^11] | [Confidence Calibration for Recommender Systems and Its Applications](https://arxiv.org/abs/2402.16325) | 论文提出了推荐系统的置信度校准框架，通过学习排名分数估计推荐结果的准确置信度，并探讨了置信度在教学和商品呈现等实际应用中的作用。 |
| [^12] | [Top-Personalized-K Recommendation](https://arxiv.org/abs/2402.16304) | 引入了个性化大小的Top-K推荐任务，提出了模型PerK，通过估计用户效用以最大化期望效用来选择推荐大小 |
| [^13] | [Against Filter Bubbles: Diversified Music Recommendation via Weighted Hypergraph Embedding Learning](https://arxiv.org/abs/2402.16299) | 引入了DWHRec算法来解决音乐推荐中准确性和多样性之间的平衡问题，通过加权超图嵌入学习来提高推荐系统的多样性。 |
| [^14] | [PerLTQA: A Personal Long-Term Memory Dataset for Memory Classification, Retrieval, and Synthesis in Question Answering](https://arxiv.org/abs/2402.16288) | PerLTQA是一个结合了语义和情节记忆的创新QA数据集，旨在探索个性化记忆在QA任务中的应用，提供了一个全面的基准和记忆整合、检索、合成的框架 |
| [^15] | [UniRetriever: Multi-task Candidates Selection for Various Context-Adaptive Conversational Retrieval](https://arxiv.org/abs/2402.16261) | 提出了一种UniRetriever框架，利用双编码器架构和两个损失约束实现了多任务候选者选择，适用于不同情境下的对话检索任务。 |
| [^16] | [High-Frequency-aware Hierarchical Contrastive Selective Coding for Representation Learning on Text-attributed Graphs](https://arxiv.org/abs/2402.16240) | 提出了一种名为HASH-CODE的高频感知分层对比选择编码方法，将图神经网络（GNNs）和预训练语言模型（PLMs）相结合，解决了在文本属性图上节点表示学习中的挑战。 |
| [^17] | [IR2: Information Regularization for Information Retrieval](https://arxiv.org/abs/2402.16200) | 介绍了IR2，一种用于在合成数据生成过程中减少过拟合的信息正则化技术，在复杂查询的信息检索任务中表现出优越性能，同时将成本降低高达50%。 |
| [^18] | [Disentangled Graph Variational Auto-Encoder for Multimodal Recommendation with Interpretability](https://arxiv.org/abs/2402.16110) | 提出了一种解缠结图变分自编码器（DGVAE），旨在增强多模态推荐系统的模型和推荐可解释性。 |
| [^19] | [Pfeed: Generating near real-time personalized feeds using precomputed embedding similarities](https://arxiv.org/abs/2402.16073) | 使用预计算的嵌入相似性生成个性化信息流，提高了电子商务平台上的客户参与度和体验，转化率提升4.9％。 |
| [^20] | [MultiContrievers: Analysis of Dense Retrieval Representations](https://arxiv.org/abs/2402.15925) | 该论文对稠密检索器的信息捕获进行了分析，探讨了其与语言模型的比较、信息提取的可行性以及提取性与性能、性别偏见的关系。 |
| [^21] | [ListT5: Listwise Reranking with Fusion-in-Decoder Improves Zero-shot Retrieval](https://arxiv.org/abs/2402.15838) | ListT5通过Fusion-in-Decoder技术实现列表重排，在零-shot检索任务中表现出优越性能，效率高于之前的模型，并解决了以往列表重排器的中间段丢失问题。 |
| [^22] | [Debiased Model-based Interactive Recommendation](https://arxiv.org/abs/2402.15819) | 该论文提出了一种名为iDMIR的模型，通过设计基于因果机制的偏见消除因果世界模型来克服传统基于模型的交互式推荐系统中存在的流行度和抽样偏见的问题。 |
| [^23] | [Cryptanalysis and improvement of multimodal data encryption by machine-learning-based system](https://arxiv.org/abs/2402.15779) | 该论文使用机器学习系统对多模态数据加密进行破解和改进，并通过复杂的数学问题极大地加密通信机制，保护个人信息并减少攻击可能性。 |
| [^24] | [Query Augmentation by Decoding Semantics from Brain Signals](https://arxiv.org/abs/2402.15708) | 提出了一种名为Brain-Aug的方法，通过从脑信号中解码的语义信息增强查询，可以生成更准确的查询，改善文档排序性能，特别适用于模糊查询。 |
| [^25] | [Universal Model in Online Customer Service](https://arxiv.org/abs/2402.15666) | 本文介绍了一种在电子商务中改进在线客户服务的解决方案，即提出了一种基于客户问题预测标签的通用模型，无需进行训练，通过消除个别模型训练和维护的需求，减少了模型开发周期和成本。 |
| [^26] | [Artful Path to Healing: Using Machine Learning for Visual Art Recommendation to Prevent and Reduce Post-Intensive Care](https://arxiv.org/abs/2402.15643) | 使用机器学习开发了视觉艺术推荐系统，为重症监护后患者提供个性化的治疗性视觉艺术体验，并证明这些推荐能够提升时间情感状态 |
| [^27] | [Language-Based User Profiles for Recommendation](https://arxiv.org/abs/2402.15623) | 通过使用以人类可读文本表示的用户偏好，提出了Language-based Factorization Model (LFM)，在冷启动环境中与传统方法相比取得了更好的表现 |
| [^28] | [RecWizard: A Toolkit for Conversational Recommendation with Modular, Portable Models and Interactive User Interface](https://arxiv.org/abs/2402.15591) | RecWizard是一种对话式推荐工具包，具有模块化、便携模型和交互用户界面，可提高CRS研究效率并减少额外工作量 |
| [^29] | [Transforming Norm-based To Graph-based Spatial Representation for Spatio-Temporal Epidemiological Models](https://arxiv.org/abs/2402.14539) | 研究介绍了将基于规范的空间表示转换为基于图的时空流行病学模型的新框架和方法。 |
| [^30] | [Knowledge Graphs Meet Multi-Modal Learning: A Comprehensive Survey](https://arxiv.org/abs/2402.05391) | 知识图谱与多模态学习的综述介绍了KG4MM和MM4KG两个主要方面，包括任务定义、构建进展、评估基准以及关键研究轨迹。 |
| [^31] | [Efficient Title Reranker for Fast and Improved Knowledge-Intense NLP](https://arxiv.org/abs/2312.12430) | 引入了通过广播查询编码器实现的高效标题重新排序器和为标题重新排序定制的Sigmoid Trick损失函数，相结合在KILT知识基准测试的数据集上取得了最先进的结果。 |
| [^32] | [Contextualizing Internet Memes Across Social Media Platforms](https://arxiv.org/abs/2311.11157) | 本研究旨在探究是否可以通过使用知识图来对社交媒体平台上的互联网迷因进行情境化处理，填补了迄今为止对互联网迷因进行全面追踪、识别和映射的空白。 |
| [^33] | [Towards A Unified View of Answer Calibration for Multi-Step Reasoning](https://arxiv.org/abs/2311.09101) | 本文总结了最近答案校准技术的分类法，从统一视角对步级和路径级答案校准进行了彻底评估，结果显示整合两种策略的优势倾向于产生最佳结果。 |
| [^34] | [ITEm: Unsupervised Image-Text Embedding Learning for eCommerce](https://arxiv.org/abs/2311.02084) | ITEm是面向电子商务的无监督学习模型，通过学习文本和图片的嵌入来更好地关注不同模态，扩展了BERT，并在两个任务上取得了良好的表现。 |
| [^35] | [On Bilingual Lexicon Induction with Large Language Models](https://arxiv.org/abs/2310.13995) | 本文研究了利用大型语言模型进行双语词汇识别的潜力，通过研究零次提示和少量上下文提示等方法，探讨了这种方法如何与当前BLI方法相比，并如何进行补充。 |
| [^36] | [RecMind: Large Language Model Powered Agent For Recommendation](https://arxiv.org/abs/2308.14296) | RecMind是一种LLM驱动的自主推荐代理，通过Self-Inspiring算法提高了规划能力，能够为零-shot个性化推荐提供支持。 |
| [^37] | [When Federated Recommendation Meets Cold-Start Problem: Separating Item Attributes and User Interactions](https://arxiv.org/abs/2305.12650) | 本文提出了一种名为Item-aligned Federated Aggregation (IFedRec)的新方法，通过利用物品属性和交互记录同时学习两组物品表示，并设计了物品表示对齐机制，从而在联邦学习框架中解决了冷启动推荐的挑战。 |
| [^38] | [The Klarna Product Page Dataset: Web Element Nomination with Graph Neural Networks and Large Language Models](https://arxiv.org/abs/2111.02168) | Klarna产品页面数据集是一个全面多样的网页集合，为解决网络自动化算法设计中数据集稀缺性问题提供了新的资源。 |
| [^39] | [Next Visit Diagnosis Prediction via Medical Code-Centric Multimodal Contrastive EHR Modelling with Hierarchical Regularisation.](http://arxiv.org/abs/2401.11648) | 通过医学代码中心的多模态对比EHR建模预测下次就诊诊断，并通过分层正则化提高性能。 |
| [^40] | [Knowledge Sharing in Manufacturing using Large Language Models: User Evaluation and Model Benchmarking.](http://arxiv.org/abs/2401.05200) | 使用大型语言模型在制造业中进行知识共享，通过评估实证了该系统的效益，提高了操作员的信息检索速度和问题解决效率，同时强调在有人工专家选项时的偏好。GPT-4是最优秀的模型。 |
| [^41] | [Representation Learning with Large Language Models for Recommendation.](http://arxiv.org/abs/2310.15950) | 这篇论文介绍了一个模型-不可知的框架RLMRec，通过使用大语言模型（LLMs）来增强传统的基于ID的推荐系统，并解决了可扩展性问题、仅依赖文本的限制以及提示输入限制等挑战。 |
| [^42] | [A Survey on Point-of-Interest Recommendations Leveraging Heterogeneous Data.](http://arxiv.org/abs/2308.07426) | 本文针对旅游领域的兴趣点推荐问题进行了调查研究，探讨了利用异构数据解决旅途中兴趣点推荐问题的潜力与挑战。 |
| [^43] | [Revisit and Outstrip Entity Alignment: A Perspective of Generative Models.](http://arxiv.org/abs/2305.14651) | 本文重新从生成模型的角度研究了基于嵌入的实体对齐（EEA）问题，引入基于生成对抗网络的EEA方法及提出的生成的EEA（GEEA）框架，通过互相变分自动编码器（M-VAE）实现实体从一个KG转换到另一个KG，并且从随机噪声向量生成新的实体，具有较好的效果。 |
| [^44] | [Value of Exploration: Measurements, Findings and Algorithms.](http://arxiv.org/abs/2305.07764) | 本研究通过量化探索对内容语料库的影响，证明了探索对用户体验的长期好处，并尝试使用神经线性匀速臂算法构建基于探索的排名系统。 |
| [^45] | [Generative Recommendation: Towards Next-generation Recommender Paradigm.](http://arxiv.org/abs/2304.03516) | 生成式AI可以克服推荐系统中的限制，使其能够生成满足用户特定信息需求的内容，并且用户可以通过自然语言指令来指导内容生成。 |
| [^46] | [CompoDiff: Versatile Composed Image Retrieval With Latent Diffusion.](http://arxiv.org/abs/2303.11916) | CompoDiff 是一种多功能的组合图像检索模型，通过接受各种条件，具有潜在扩散的能力，并在 FashionIQ 上实现了新的零样本最新技术水平。其特征位于完整的 CLIP 嵌入空间中，可以直接用于所有利用 CLIP 空间的模型。 |
| [^47] | [Broad Recommender System: An Efficient Nonlinear Collaborative Filtering Approach.](http://arxiv.org/abs/2204.11602) | 本文提出了一种新的宽泛推荐系统(BroadCF)，使用宽泛学习系统(BLS)作为映射函数来学习用户和项目之间的复杂非线性关系，同时通过用户-项评级协同向量预处理程序将原始数据转换为更适合BLS学习的格式。BroadCF的实验结果表明，在用户推荐准确性和效率方面都优于几种最先进的CF方法。 |

# 详细

[^1]: CorpusBrain++: 知识密集型语言任务的继续生成预训练框架

    CorpusBrain++: A Continual Generative Pre-Training Framework for Knowledge-Intensive Language Tasks

    [https://arxiv.org/abs/2402.16767](https://arxiv.org/abs/2402.16767)

    本研究介绍了知识密集型语言任务中的持续文档学习任务，并建立了一个新的评估数据集，旨在探索检索模型有效处理动态检索场景的能力。

    

    知识密集型语言任务通常需要从可信的语料库（如维基百科）中检索相关文档以生成特定答案。最近，提出了用于知识密集型语言任务的预训练生成式检索模型CorpusBrain，并取得了新的检索性能最优结果。然而，大多数现有的关于知识密集型语言任务的研究，包括CorpusBrain，在很大程度上集中在静态文档集上，忽视了现实场景的动态性质，其中新文档持续地被纳入源语料库。为了填补这一空白，探索检索模型有效处理知识密集型语言任务中固有的动态检索场景的能力至关重要。在本文中，我们首先介绍了知识密集型语言任务的持续文档学习（CDL）任务，并基于原始KILT数据集构建了一个名为KILT++的新型评估基准数据集。

    arXiv:2402.16767v1 Announce Type: cross  Abstract: Knowledge-intensive language tasks (KILTs) typically require retrieving relevant documents from trustworthy corpora, e.g., Wikipedia, to produce specific answers. Very recently, a pre-trained generative retrieval model for KILTs, named CorpusBrain, was proposed and reached new state-of-the-art retrieval performance. However, most existing research on KILTs, including CorpusBrain, has predominantly focused on a static document collection, overlooking the dynamic nature of real-world scenarios, where new documents are continuously being incorporated into the source corpus. To address this gap, it is crucial to explore the capability of retrieval models to effectively handle the dynamic retrieval scenario inherent in KILTs.   In this work, we first introduce the continual document learning (CDL) task for KILTs and build a novel benchmark dataset named KILT++ based on the original KILT dataset for evaluation. Then, we conduct a comprehensi
    
[^2]: LLM辅助的多教师持续学习在机器人手术中的视觉问答

    LLM-Assisted Multi-Teacher Continual Learning for Visual Question Answering in Robotic Surgery

    [https://arxiv.org/abs/2402.16664](https://arxiv.org/abs/2402.16664)

    LLM辅助的多教师持续学习为机器人手术中的视觉问答系统更新提供了解决新任务需求的方法，同时解决了外科领域中的大领域转变和数据不平衡问题。

    

    视觉问答(VQA)在促进机器人辅助手术教育方面可能至关重要。在实践中，学员的需求不断发展，比如学习更多种类的手术，适应不同的机器人，以及为一种手术学习新的外科器械和技术。因此，在机器人手术中需要通过多个资源的顺序数据流持续更新VQA系统，以解决新任务。在外科场景中，存储成本和患者数据隐私通常限制了在更新模型时旧数据的可用性，这需要一个无样本的持续学习(CL)设置。然而，先前的研究忽视了外科领域的两个重要问题：i)来自不同科室或临床中心收集的各种外科手术的大领域转变，ii)由于外科器械或活动的不均匀出现而导致的严重数据不平衡。

    arXiv:2402.16664v1 Announce Type: new  Abstract: Visual question answering (VQA) can be fundamentally crucial for promoting robotic-assisted surgical education. In practice, the needs of trainees are constantly evolving, such as learning more surgical types, adapting to different robots, and learning new surgical instruments and techniques for one surgery. Therefore, continually updating the VQA system by a sequential data stream from multiple resources is demanded in robotic surgery to address new tasks. In surgical scenarios, the storage cost and patient data privacy often restrict the availability of old data when updating the model, necessitating an exemplar-free continual learning (CL) setup. However, prior studies overlooked two vital problems of the surgical domain: i) large domain shifts from diverse surgical operations collected from multiple departments or clinical centers, and ii) severe data imbalance arising from the uneven presence of surgical instruments or activities du
    
[^3]: BOXREC：在线购物中推荐一组喜好的服装

    BOXREC: Recommending a Box of Preferred Outfits in Online Shopping

    [https://arxiv.org/abs/2402.16660](https://arxiv.org/abs/2402.16660)

    该论文提出了一个名为BOXREC的盒子推荐框架，首次考虑了用户对单品价格范围的偏好和整体购物预算，以生成一组符合用户喜好的服装推荐。

    

    在过去几年里，服装搭配的自动化引起了研究界的极大关注。大多数现有的服装推荐系统侧重于预测两两物品的兼容性（使用视觉和文本特征）来评分一个包含多件物品的服装组合，然后推荐前n套服装或一套基于用户时尚品味的服装胶囊。然而，这些系统都没有考虑用户对单个服装类型的价格范围偏好或一组物品的整体购物预算。在本文中，我们提出了一个盒子推荐框架 - BOXREC - 首先收集用户在不同物品类型（即上装、下装和鞋类）上的偏好，包括每种类型的价格范围和特定购物会话的最大购物预算。然后通过从所有类型的偏好物品中检索，生成一组喜好的服装。

    arXiv:2402.16660v1 Announce Type: new  Abstract: Over the past few years, automation of outfit composition has gained much attention from the research community. Most of the existing outfit recommendation systems focus on pairwise item compatibility prediction (using visual and text features) to score an outfit combination having several items, followed by recommendation of top-n outfits or a capsule wardrobe having a collection of outfits based on user's fashion taste. However, none of these consider user's preference of price-range for individual clothing types or an overall shopping budget for a set of items. In this paper, we propose a box recommendation framework - BOXREC - which at first, collects user preferences across different item types (namely, top-wear, bottom-wear and foot-wear) including price-range of each type and a maximum shopping budget for a particular shopping session. It then generates a set of preferred outfits by retrieving all types of preferred items from the
    
[^4]: PAQA：面向积极开放型检索问答的研究

    PAQA: Toward ProActive Open-Retrieval Question Answering

    [https://arxiv.org/abs/2402.16608](https://arxiv.org/abs/2402.16608)

    本研究针对对话式搜索系统中存在的数据集不足问题，提出了PAQA方法，通过考虑用户查询和文档中的歧义，生成相关澄清问题，从而改善对话搜索系统的效果。

    

    会话系统在生成自然语言响应方面取得了显著进展。然而，由于其在信息检索过程中的被动角色，目前作为对话式搜索系统的潜力受到限制。一个主要限制是提供带有支持文档语料库和相关澄清问题的标记模棱两可问题的数据集的稀缺性。本文旨在通过考虑用户查询和文档中存在的固有歧义来解决生成相关澄清问题的挑战。为了实现这一目标，我们提出了PAQA，这是现有AmbiNQ数据集的扩展，其中包含澄清问题。然后，我们评估各种模型，并评估段落检索如何影响模棱两可检测和生成澄清问题。通过解决对话搜索系统中的这一缺陷，我们旨在提供额外的监督以增强其效果。

    arXiv:2402.16608v1 Announce Type: new  Abstract: Conversational systems have made significant progress in generating natural language responses. However, their potential as conversational search systems is currently limited due to their passive role in the information-seeking process. One major limitation is the scarcity of datasets that provide labelled ambiguous questions along with a supporting corpus of documents and relevant clarifying questions. This work aims to tackle the challenge of generating relevant clarifying questions by taking into account the inherent ambiguities present in both user queries and documents. To achieve this, we propose PAQA, an extension to the existing AmbiNQ dataset, incorporating clarifying questions. We then evaluate various models and assess how passage retrieval impacts ambiguity detection and the generation of clarifying questions. By addressing this gap in conversational search systems, we aim to provide additional supervision to enhance their ac
    
[^5]: 将大型语言模型与图形会话推荐相结合

    Integrating Large Language Models with Graphical Session-Based Recommendation

    [https://arxiv.org/abs/2402.16539](https://arxiv.org/abs/2402.16539)

    本文提出了一种名为LLMGR的框架，将大型语言模型与图形会话推荐相结合，有效地弥合了推荐系统中大型语言模型在会话推荐领域的适应性差距

    

    随着大型语言模型（LLMs）的快速发展，出现了各种探索，利用LLMs在推荐系统上的上下文理解能力。虽然开创性的策略主要是将传统推荐任务转变为自然语言生成挑战，但在会话推荐（SBR）领域的探索相对较少，因为其具体性。SBR主要由图神经网络主导，由于其捕获相邻行为之间的内在和显性关系的能力，取得了许多成功结果。图的结构性质与自然语言的本质形成对比，为LLMs提出了重大的适应性差距。本文介绍了将大型语言模型与图形会话推荐相结合的框架LLMGR，这是一个有效的框架，通过和谐地弥合上述差距

    arXiv:2402.16539v1 Announce Type: cross  Abstract: With the rapid development of Large Language Models (LLMs), various explorations have arisen to utilize LLMs capability of context understanding on recommender systems. While pioneering strategies have primarily transformed traditional recommendation tasks into challenges of natural language generation, there has been a relative scarcity of exploration in the domain of session-based recommendation (SBR) due to its specificity. SBR has been primarily dominated by Graph Neural Networks, which have achieved many successful outcomes due to their ability to capture both the implicit and explicit relationships between adjacent behaviors. The structural nature of graphs contrasts with the essence of natural language, posing a significant adaptation gap for LLMs. In this paper, we introduce large language models with graphical Session-Based recommendation, named LLMGR, an effective framework that bridges the aforementioned gap by harmoniously 
    
[^6]: 使用大规模合成监督进行跨语言开放域问答的预训练

    Pre-training Cross-lingual Open Domain Question Answering with Large-scale Synthetic Supervision

    [https://arxiv.org/abs/2402.16508](https://arxiv.org/abs/2402.16508)

    本研究提出了一种基于自监督方法的单个编码-解码模型来解决跨语言问答问题，通过利用维基百科内的跨语言链接结构合成监督信号，取得了优于其他方法的效果。

    

    跨语言问答（CLQA）是一个复杂的问题，包括从多语言知识库进行跨语言检索，然后在英语或查询语言中生成答案。本文展示了可以使用单个编码-解码模型来解决CLQA。为了有效训练这个模型，我们提出了一种基于利用维基百科内跨语言链接结构的自监督方法。我们展示了如何利用链接的维基百科页面来合成跨语言检索的监督信号，通过一种填空查询形式生成更自然的查询以监督答案生成。最后，我们展示了我们的方法CLASS在监督学习和零-shot情况下均优于可比方法。

    arXiv:2402.16508v1 Announce Type: new  Abstract: Cross-lingual question answering (CLQA) is a complex problem, comprising cross-lingual retrieval from a multilingual knowledge base, followed by answer generation either in English or the query language. Both steps are usually tackled by separate models, requiring substantial annotated datasets, and typically auxiliary resources, like machine translation systems to bridge between languages. In this paper, we show that CLQA can be addressed using a single encoder-decoder model. To effectively train this model, we propose a self-supervised method based on exploiting the cross-lingual link structure within Wikipedia. We demonstrate how linked Wikipedia pages can be used to synthesise supervisory signals for cross-lingual retrieval, through a form of cloze query, and generate more natural queries to supervise answer generation. Together, we show our approach, \texttt{CLASS}, outperforms comparable methods on both supervised and zero-shot lan
    
[^7]: 重拾发明者-作者：解决专利与科学出版物之间作者身份的歧义问题

    Retrouver l'inventeur-auteur : la lev{\'e}e d'homonymies d'autorat entre les brevets et les publications scientifiques

    [https://arxiv.org/abs/2402.16440](https://arxiv.org/abs/2402.16440)

    通过使用IPC和IPCCAT API，本研究提出了一种方法来解决发明者兼学术作者的身份匹配问题，成功匹配了专利与论文，解决了作者身份消歧的挑战，错误率低于5%。

    

    专利和科学论文是衡量科学技术产出的重要来源，可作为各种科学计量分析的基础。作者和发明者的姓名是进行这些分析的关键标识符，然而，他们遭遇了消歧的问题。本文提出了一种方法，使用国际专利分类（IPC）和IPCCAT API来评估给定发明者的专利和论文摘要之间的相似程度，以匹配两种类型的文档。该方法基于从国际EPO数据库Espacenet提取的三个专利语料库开发和手动评估。在4679篇专利和7720位发明者中，我们获得了2501位作者。提出的算法解决了消歧的一般问题，错误率低于5%。

    arXiv:2402.16440v1 Announce Type: new  Abstract: Patents and scientific papers provide an essential source for measuring science and technology output, to be used as a basis for the most varied scientometric analyzes. Authors' and inventors' names are the key identifiers to carry out these analyses, which however, run up against the issue of disambiguation. By extension identifying inventors who are also academic authors is a non-trivial challenge. We propose a method using the International Patent Classification (IPC) and the IPCCAT API to assess the degree of similarity of patents and papers abstracts of a given inventor, in order to match both types of documents. The method is developed and manually qualified based on three corpora of patents extracted from the international EPO database Espacenet. Among a set of 4679 patents and 7720 inventors, we obtain 2501 authors. The proposed algorithm solves the general problem of disambiguation with an error rate lower than 5%.
    
[^8]: 使用二阶统计方法进行说话人识别时话语持续时间和语音内容的影响

    Effect of utterance duration and phonetic content on speaker identification using second-order statistical methods

    [https://arxiv.org/abs/2402.16429](https://arxiv.org/abs/2402.16429)

    本文研究了测试语音材料的内容对使用二阶统计方法进行说话人识别的影响，发现液体音和滑音、元音，特别是鼻元音和鼻辅音对说话人识别特别重要。

    

    第二阶统计方法在受控录音条件下自动说话人识别中表现出非常好的结果。这些方法通常用于整个可用的语音材料。在本文中，我们研究了测试语音材料的内容对这些方法表现的影响，即更多的分析性方法。研究的目标是调查这些方法使用的信息类型，以及这些信息在语音信号中的位置。液体音和滑音、元音，特别是鼻元音和鼻辅音，被发现是特定于说话人的：由这些类中大部分声学材料组成的1秒测试话语提供比语音平衡的测试话语更好的说话人识别结果，即使在这两种情况下，训练都是用15秒的语音平衡训练。然而，对于其他情况的结果...

    arXiv:2402.16429v1 Announce Type: new  Abstract: Second-order statistical methods show very good results for automatic speaker identification in controlled recording conditions. These approaches are generally used on the entire speech material available. In this paper, we study the influence of the content of the test speech material on the performances of such methods, i.e. under a more analytical approach. The goal is to investigate on the kind of information which is used by these methods, and where it is located in the speech signal. Liquids and glides together, vowels, and more particularly nasal vowels and nasal consonants, are found to be particularly speaker specific: test utterances of 1 second, composed in majority of acoustic material from one of these classes provide better speaker identification results than phonetically balanced test utterances, even though the training is done, in both cases, with 15 seconds of phonetically balanced speech. Nevertheless, results with oth
    
[^9]: 一个整合的数据处理框架用于预训练基础模型

    An Integrated Data Processing Framework for Pretraining Foundation Models

    [https://arxiv.org/abs/2402.16358](https://arxiv.org/abs/2402.16358)

    提出了一个集成了处理模块和分析模块的数据处理框架，旨在改善数据质量并展示其有效性。

    

    基础模型的能力在很大程度上依赖于大规模、多样化和高质量的预训练数据。为了提高数据质量，研究人员和从业者经常需要手动从不同来源策划数据集，并为每个数据存储库开发专门的数据清洗流程。缺乏统一的数据处理框架，这一过程重复而繁琐。为了缓解这一问题，我们提出了一个集成了处理模块和分析模块的数据处理框架，处理模块包括一系列不同粒度水平的操作符，而分析模块支持对精炼数据进行探查和评估。所提出的框架易于使用且高度灵活。在这篇演示论文中，我们首先介绍如何使用这个框架并展示它在改善数据质量方面的有效性，通过与ChatGPT的自动评估和端到端评估。

    arXiv:2402.16358v1 Announce Type: cross  Abstract: The ability of the foundation models heavily relies on large-scale, diverse, and high-quality pretraining data. In order to improve data quality, researchers and practitioners often have to manually curate datasets from difference sources and develop dedicated data cleansing pipeline for each data repository. Lacking a unified data processing framework, this process is repetitive and cumbersome. To mitigate this issue, we propose a data processing framework that integrates a Processing Module which consists of a series of operators at different granularity levels, and an Analyzing Module which supports probing and evaluation of the refined data. The proposed framework is easy to use and highly flexible. In this demo paper, we first introduce how to use this framework with some example use cases and then demonstrate its effectiveness in improving the data quality with an automated evaluation with ChatGPT and an end-to-end evaluation in 
    
[^10]: 协同过滤中针对新用户的深度评分调查

    Deep Rating Elicitation for New Users in Collaborative Filtering

    [https://arxiv.org/abs/2402.16327](https://arxiv.org/abs/2402.16327)

    提出了一种端到端深度学习框架，用于协同过滤中新用户的评分调查，可以一次选择所有种子项目，并通过考虑非线性交互作用来推断用户偏好。

    

    最近，推荐系统开始使用评分调查来改进初始推荐的质量，这种方法要求新用户对一个小的种子项目集进行评分以推断他们的偏好。评分调查的关键挑战在于选择能够最好推断新用户偏好的种子项目。本文提出了一种新颖的端到端深度学习框架用于评分调查（DRE），该框架一次选择所有种子项目，并考虑非线性交互作用。为此，首先定义了分类分布来从整个项目集中抽样种子项目，然后训练分类分布和一个神经重构网络，从抽样的种子项目的CF信息中推断用户对剩余项目的偏好。通过端到端训练，学习了分类分布以选择最具代表性的种子项目，同时反映了复杂的非线性交互。

    arXiv:2402.16327v1 Announce Type: new  Abstract: Recent recommender systems started to use rating elicitation, which asks new users to rate a small seed itemset for inferring their preferences, to improve the quality of initial recommendations. The key challenge of the rating elicitation is to choose the seed items which can best infer the new users' preference. This paper proposes a novel end-to-end Deep learning framework for Rating Elicitation (DRE), that chooses all the seed items at a time with consideration of the non-linear interactions. To this end, it first defines categorical distributions to sample seed items from the entire itemset, then it trains both the categorical distributions and a neural reconstruction network to infer users' preferences on the remaining items from CF information of the sampled seed items. Through the end-to-end training, the categorical distributions are learned to select the most representative seed items while reflecting the complex non-linear int
    
[^11]: 推荐系统的置信度校准及其应用

    Confidence Calibration for Recommender Systems and Its Applications

    [https://arxiv.org/abs/2402.16325](https://arxiv.org/abs/2402.16325)

    论文提出了推荐系统的置信度校准框架，通过学习排名分数估计推荐结果的准确置信度，并探讨了置信度在教学和商品呈现等实际应用中的作用。

    

    尽管在推荐结果准确度方面受到重视，但在文献中对于拥有一种对推荐结果置信度的衡量方式却出奇的被忽视。在这篇论文中，我提出了一个推荐系统的模型校准框架，用于基于学习到的排名分数估计推荐结果的准确置信度。此外，我随后介绍了置信度在两个现实世界应用中的应用：(1) 通过将大型教师模型的置信度视为附加学习指导，训练一个小型学生模型，(2) 基于校准后的概率估计的预期用户效用调整呈现商品数量。

    arXiv:2402.16325v1 Announce Type: new  Abstract: Despite the importance of having a measure of confidence in recommendation results, it has been surprisingly overlooked in the literature compared to the accuracy of the recommendation. In this dissertation, I propose a model calibration framework for recommender systems for estimating accurate confidence in recommendation results based on the learned ranking scores. Moreover, I subsequently introduce two real-world applications of confidence on recommendations: (1) Training a small student model by treating the confidence of a big teacher model as additional learning guidance, (2) Adjusting the number of presented items based on the expected user utility estimated with calibrated probability.
    
[^12]: 个性化大小的Top-K推荐

    Top-Personalized-K Recommendation

    [https://arxiv.org/abs/2402.16304](https://arxiv.org/abs/2402.16304)

    引入了个性化大小的Top-K推荐任务，提出了模型PerK，通过估计用户效用以最大化期望效用来选择推荐大小

    

    传统的Top-K推荐将呈现具有最高排名得分的前K个物品，这是生成个性化排名列表的常见做法。然而，固定大小的Top-K推荐对于每个用户的满意度是否是最佳方法？未必。我们指出，如果不考虑用户效用而提供固定大小的推荐可能是次优的，因为它可能会不可避免地包含无关项目或限制与相关项目的接触。为了解决这个问题，我们引入了Top-Personalized-K推荐，这是一个旨在生成个性化大小排名列表以最大化个人用户满意度的新推荐任务。作为提出任务的解决方案，我们开发了一个名为PerK的模型无关框架。PerK通过利用校准的交互概率来估计期望的用户效用，随后选择最大化这种期望效用的推荐大小。

    arXiv:2402.16304v1 Announce Type: new  Abstract: The conventional top-K recommendation, which presents the top-K items with the highest ranking scores, is a common practice for generating personalized ranking lists. However, is this fixed-size top-K recommendation the optimal approach for every user's satisfaction? Not necessarily. We point out that providing fixed-size recommendations without taking into account user utility can be suboptimal, as it may unavoidably include irrelevant items or limit the exposure to relevant ones. To address this issue, we introduce Top-Personalized-K Recommendation, a new recommendation task aimed at generating a personalized-sized ranking list to maximize individual user satisfaction. As a solution to the proposed task, we develop a model-agnostic framework named PerK. PerK estimates the expected user utility by leveraging calibrated interaction probabilities, subsequently selecting the recommendation size that maximizes this expected utility. Through
    
[^13]: 对抗筛选气泡：基于加权超图嵌入学习的音乐推荐多样化

    Against Filter Bubbles: Diversified Music Recommendation via Weighted Hypergraph Embedding Learning

    [https://arxiv.org/abs/2402.16299](https://arxiv.org/abs/2402.16299)

    引入了DWHRec算法来解决音乐推荐中准确性和多样性之间的平衡问题，通过加权超图嵌入学习来提高推荐系统的多样性。

    

    推荐系统对用户有着双重作用：过滤不合适或不匹配的信息，同时准确识别符合其偏好的项目。许多推荐算法旨在为用户提供个性化的信息阵列，以满足其偏好。然而，过度个性化可能会将用户限制在“筛选气泡”中。因此，在推荐中获得准确性和多样性之间的平衡是一项迫切关注的问题。为了应对这一挑战，以音乐推荐为例，我们介绍了多样化加权超图音乐推荐算法（DWHRec）。在DWHRec算法中，用户和已听曲目之间的初始连接由加权超图表示。同时，还将艺术家、专辑和标记与曲目的关联也附加到超图中。为了探索用户的潜在偏好，一个超图

    arXiv:2402.16299v1 Announce Type: cross  Abstract: Recommender systems serve a dual purpose for users: sifting out inappropriate or mismatched information while accurately identifying items that align with their preferences. Numerous recommendation algorithms are designed to provide users with a personalized array of information tailored to their preferences. Nevertheless, excessive personalization can confine users within a "filter bubble". Consequently, achieving the right balance between accuracy and diversity in recommendations is a pressing concern. To address this challenge, exemplified by music recommendation, we introduce the Diversified Weighted Hypergraph music Recommendation algorithm (DWHRec). In the DWHRec algorithm, the initial connections between users and listened tracks are represented by a weighted hypergraph. Simultaneously, associations between artists, albums and tags with tracks are also appended to the hypergraph. To explore users' latent preferences, a hypergrap
    
[^14]: PerLTQA: 一个用于问题回答中的记忆分类、检索和合成的个人长期记忆数据集

    PerLTQA: A Personal Long-Term Memory Dataset for Memory Classification, Retrieval, and Synthesis in Question Answering

    [https://arxiv.org/abs/2402.16288](https://arxiv.org/abs/2402.16288)

    PerLTQA是一个结合了语义和情节记忆的创新QA数据集，旨在探索个性化记忆在QA任务中的应用，提供了一个全面的基准和记忆整合、检索、合成的框架

    

    长期记忆在个人互动中起着至关重要的作用，考虑到长期记忆可以更好地利用世界知识、历史信息和对话中的偏好。我们的研究引入了PerLTQA，一个创新的QA数据集，结合了语义和情节记忆，包括世界知识、用户资料、社会关系、事件和对话。这个数据集被收集用于探讨个性化记忆在QA任务中的应用，重点关注社交互动和事件。PerLTQA具有两种记忆类型和一个包含8,593个问题的30个字符的全面基准，促进了在大型语言模型（LLM）中探索和应用个性化记忆。基于PerLTQA，我们提出了一个记忆整合和生成的新框架，包括三个主要组成部分：记忆分类、记忆检索和记忆合成。我们使用五个LLM和三个评估了这个框架。

    arXiv:2402.16288v1 Announce Type: cross  Abstract: Long-term memory plays a critical role in personal interaction, considering long-term memory can better leverage world knowledge, historical information, and preferences in dialogues. Our research introduces PerLTQA, an innovative QA dataset that combines semantic and episodic memories, including world knowledge, profiles, social relationships, events, and dialogues. This dataset is collected to investigate the use of personalized memories, focusing on social interactions and events in the QA task. PerLTQA features two types of memory and a comprehensive benchmark of 8,593 questions for 30 characters, facilitating the exploration and application of personalized memories in Large Language Models (LLMs). Based on PerLTQA, we propose a novel framework for memory integration and generation, consisting of three main components: Memory Classification, Memory Retrieval, and Memory Synthesis. We evaluate this framework using five LLMs and thre
    
[^15]: UniRetriever：各种情境自适应对话检索的多任务候选者选择

    UniRetriever: Multi-task Candidates Selection for Various Context-Adaptive Conversational Retrieval

    [https://arxiv.org/abs/2402.16261](https://arxiv.org/abs/2402.16261)

    提出了一种UniRetriever框架，利用双编码器架构和两个损失约束实现了多任务候选者选择，适用于不同情境下的对话检索任务。

    

    对话检索是指以迭代和交互方式运行的信息检索系统，需要检索各种外部资源（如人设、知识甚至回应）以有效与用户交互并成功完成对话。为了提高效率和性能，我们提出了一个多任务框架，作为三个主要检索任务的通用检索器：人设选择、知识选择和回应选择。为此，我们设计了一个双编码器架构，包括一个情境自适应对话编码器和一个候选者编码器，旨在通过简单的点积关注长对话中的相关上下文并检索合适的候选者。此外，我们引入了两个损失约束以捕捉...

    arXiv:2402.16261v1 Announce Type: new  Abstract: Conversational retrieval refers to an information retrieval system that operates in an iterative and interactive manner, requiring the retrieval of various external resources, such as persona, knowledge, and even response, to effectively engage with the user and successfully complete the dialogue. However, most previous work trained independent retrievers for each specific resource, resulting in sub-optimal performance and low efficiency. Thus, we propose a multi-task framework function as a universal retriever for three dominant retrieval tasks during the conversation: persona selection, knowledge selection, and response selection. To this end, we design a dual-encoder architecture consisting of a context-adaptive dialogue encoder and a candidate encoder, aiming to attention to the relevant context from the long dialogue and retrieve suitable candidates by simply a dot product. Furthermore, we introduce two loss constraints to capture t
    
[^16]: 针对文本属性图的高频感知分层对比选择编码用于表示学习

    High-Frequency-aware Hierarchical Contrastive Selective Coding for Representation Learning on Text-attributed Graphs

    [https://arxiv.org/abs/2402.16240](https://arxiv.org/abs/2402.16240)

    提出了一种名为HASH-CODE的高频感知分层对比选择编码方法，将图神经网络（GNNs）和预训练语言模型（PLMs）相结合，解决了在文本属性图上节点表示学习中的挑战。

    

    我们研究了在文本属性图（TAGs）上的节点表示学习，其中节点关联有文本信息。尽管最近关于图神经网络（GNNs）和预训练语言模型（PLMs）的研究展示了它们在编码网络和文本信号方面的强大能力，但对于精细地将这两种模型耦合在TAGs上的注意力较少。具体而言，现有的GNNs很少以一种情境化的方式对每个节点中的文本进行建模；现有的PLMs由于其序列架构，几乎无法应用于表征图结构。为了解决这些挑战，我们提出了HASH-CODE，一种高频感知的谱分层对比选择编码方法，将GNNs和PLMs整合到统一模型中。与之前的“级联架构”不同，直接在PLM之上添加GNN层的方法不同，我们的HASH-CODE依靠五个自监督优化目标，以促进彻底的相互学习。

    arXiv:2402.16240v1 Announce Type: new  Abstract: We investigate node representation learning on text-attributed graphs (TAGs), where nodes are associated with text information. Although recent studies on graph neural networks (GNNs) and pretrained language models (PLMs) have exhibited their power in encoding network and text signals, respectively, less attention has been paid to delicately coupling these two types of models on TAGs. Specifically, existing GNNs rarely model text in each node in a contextualized way; existing PLMs can hardly be applied to characterize graph structures due to their sequence architecture. To address these challenges, we propose HASH-CODE, a High-frequency Aware Spectral Hierarchical Contrastive Selective Coding method that integrates GNNs and PLMs into a unified model. Different from previous "cascaded architectures" that directly add GNN layers upon a PLM, our HASH-CODE relies on five self-supervised optimization objectives to facilitate thorough mutual e
    
[^17]: IR2：信息正则化用于信息检索

    IR2: Information Regularization for Information Retrieval

    [https://arxiv.org/abs/2402.16200](https://arxiv.org/abs/2402.16200)

    介绍了IR2，一种用于在合成数据生成过程中减少过拟合的信息正则化技术，在复杂查询的信息检索任务中表现出优越性能，同时将成本降低高达50%。

    

    有效地在训练数据有限的情况下进行信息检索（IR），特别是对于复杂查询，仍然是一项具有挑战性的任务。本文介绍了IR2，即信息检索的信息正则化，一种用于在合成数据生成过程中减少过拟合的技术。该方法在具有复杂查询特征的三个最近的IR任务上进行了测试：DORIS-MAE、ArguAna和WhatsThatBook。实验结果表明，我们的正则化技术不仅在所考虑的任务上优于先前的合成查询生成方法，而且还能将成本降低高达50％。此外，本文将不同阶段的三种正则化方法——输入、提示和输出进行了分类和探索，每种方法相对于没有正则化的模型均提供了不同程度的性能改进。

    arXiv:2402.16200v1 Announce Type: cross  Abstract: Effective information retrieval (IR) in settings with limited training data, particularly for complex queries, remains a challenging task. This paper introduces IR2, Information Regularization for Information Retrieval, a technique for reducing overfitting during synthetic data generation. This approach, representing a novel application of regularization techniques in synthetic data creation for IR, is tested on three recent IR tasks characterized by complex queries: DORIS-MAE, ArguAna, and WhatsThatBook. Experimental results indicate that our regularization techniques not only outperform previous synthetic query generation methods on the tasks considered but also reduce cost by up to 50%. Furthermore, this paper categorizes and explores three regularization methods at different stages of the query synthesis pipeline-input, prompt, and output-each offering varying degrees of performance improvement compared to models where no regulariz
    
[^18]: 多模态推荐的解缠结图变分自编码器与可解释性

    Disentangled Graph Variational Auto-Encoder for Multimodal Recommendation with Interpretability

    [https://arxiv.org/abs/2402.16110](https://arxiv.org/abs/2402.16110)

    提出了一种解缠结图变分自编码器（DGVAE），旨在增强多模态推荐系统的模型和推荐可解释性。

    

    多模态推荐系统将多模态信息（例如文本描述、图像）融合到协同过滤框架中，以提供更准确的推荐。然而，当前的多模态模型通过纠缠的数值向量表示用户和商品，使其难以解释。为了解决这个问题，我们提出了一种旨在增强模型和推荐可解释性的解缠结图变分自编码器（DGVAE）。DGVAE首先利用最先进的多模态预训练技术将多模态信息投影到文本内容中，例如将图像转换为文本。然后，它构建一个冻结的商品-商品图，并利用简化的残余图卷积网络将内容和交互编码成两组解缠表示。DGVAE进一步...

    arXiv:2402.16110v1 Announce Type: new  Abstract: Multimodal recommender systems amalgamate multimodal information (e.g., textual descriptions, images) into a collaborative filtering framework to provide more accurate recommendations. While the incorporation of multimodal information could enhance the interpretability of these systems, current multimodal models represent users and items utilizing entangled numerical vectors, rendering them arduous to interpret. To address this, we propose a Disentangled Graph Variational Auto-Encoder (DGVAE) that aims to enhance both model and recommendation interpretability. DGVAE initially projects multimodal information into textual contents, such as converting images to text, by harnessing state-of-the-art multimodal pre-training technologies. It then constructs a frozen item-item graph and encodes the contents and interactions into two sets of disentangled representations utilizing a simplified residual graph convolutional network. DGVAE further re
    
[^19]: 使用预计算的嵌入相似性生成几乎实时个性化信息流

    Pfeed: Generating near real-time personalized feeds using precomputed embedding similarities

    [https://arxiv.org/abs/2402.16073](https://arxiv.org/abs/2402.16073)

    使用预计算的嵌入相似性生成个性化信息流，提高了电子商务平台上的客户参与度和体验，转化率提升4.9％。

    

    在个性化推荐系统中，通常使用嵌入来编码用户动作和项目，然后在嵌入空间中进行检索，使用近似最近邻搜索。然而，这种方法可能会导致两个挑战：1）用户嵌入可能限制所捕获的兴趣多样性，2）保持它们最新需要昂贵的实时基础设施。本文提出了一种在实际工业环境中克服这些挑战的方法。该方法动态更新客户配置文件，并每两分钟组成一个信息流，利用预计算的嵌入及其各自的相似性。我们在荷兰和比利时最大的电子商务平台之一Bol上测试并部署了这种方法，该方法提高了客户参与度和体验，导致转化率显著提高了4.9％。

    arXiv:2402.16073v1 Announce Type: cross  Abstract: In personalized recommender systems, embeddings are often used to encode customer actions and items, and retrieval is then performed in the embedding space using approximate nearest neighbor search. However, this approach can lead to two challenges: 1) user embeddings can restrict the diversity of interests captured and 2) the need to keep them up-to-date requires an expensive, real-time infrastructure. In this paper, we propose a method that overcomes these challenges in a practical, industrial setting. The method dynamically updates customer profiles and composes a feed every two minutes, employing precomputed embeddings and their respective similarities. We tested and deployed this method to personalise promotional items at Bol, one of the largest e-commerce platforms of the Netherlands and Belgium. The method enhanced customer engagement and experience, leading to a significant 4.9% uplift in conversions.
    
[^20]: MultiContrievers: 稠密检索表示的分析

    MultiContrievers: Analysis of Dense Retrieval Representations

    [https://arxiv.org/abs/2402.15925](https://arxiv.org/abs/2402.15925)

    该论文对稠密检索器的信息捕获进行了分析，探讨了其与语言模型的比较、信息提取的可行性以及提取性与性能、性别偏见的关系。

    

    稠密检索器将源文档压缩为（可能是有损的）向量表示，然而目前对于失去和保留的信息以及它们如何影响下游任务的分析较少。我们进行了首次对比稠密检索器捕获的信息与它们基于的语言模型（如BERT与Contriever）之间的分析。我们使用25个MultiBert检查点作为随机初始化来训练MultiContrievers，这是一组25个contriever模型。我们测试特定信息（如性别和职业）是否可以从类似维基百科的文档的contriever向量中提取。我们通过信息论探测来衡量这种可提取性。然后我们研究了可提取性与性能、性别偏见之间的关系，以及这些结果对许多随机初始化和数据洗牌的敏感性。我们发现（1）contriever模型有显著增加的可提取性

    arXiv:2402.15925v1 Announce Type: cross  Abstract: Dense retrievers compress source documents into (possibly lossy) vector representations, yet there is little analysis of what information is lost versus preserved, and how it affects downstream tasks. We conduct the first analysis of the information captured by dense retrievers compared to the language models they are based on (e.g., BERT versus Contriever). We use 25 MultiBert checkpoints as randomized initialisations to train MultiContrievers, a set of 25 contriever models. We test whether specific pieces of information -- such as gender and occupation -- can be extracted from contriever vectors of wikipedia-like documents. We measure this extractability via information theoretic probing. We then examine the relationship of extractability to performance and gender bias, as well as the sensitivity of these results to many random initialisations and data shuffles. We find that (1) contriever models have significantly increased extracta
    
[^21]: ListT5: 基于解码器内融合的列表重排方法改善零-shot检索

    ListT5: Listwise Reranking with Fusion-in-Decoder Improves Zero-shot Retrieval

    [https://arxiv.org/abs/2402.15838](https://arxiv.org/abs/2402.15838)

    ListT5通过Fusion-in-Decoder技术实现列表重排，在零-shot检索任务中表现出优越性能，效率高于之前的模型，并解决了以往列表重排器的中间段丢失问题。

    

    我们提出了ListT5，一种基于在训练和推断时处理多个候选段落的Fusion-in-Decoder（FiD）的新型重排方法。我们还介绍了一个基于m元锦标赛排序和输出缓存的列表排序的高效推断框架。我们在BEIR基准上评估和比较我们的模型，证明了ListT5（1）在平均NDCG@10得分上比最先进的RankT5基线表现出明显的+1.3增益，（2）具有与逐点排名模型可比拟的效率，并超越以前的列表排序模型的效率，（3）克服了以前列表重排器的中间段丢失问题。我们的代码、模型检查点和评估框架完全开源在 \url{https://github.com/soyoung97/ListT5}。

    arXiv:2402.15838v1 Announce Type: new  Abstract: We propose ListT5, a novel reranking approach based on Fusion-in-Decoder (FiD) that handles multiple candidate passages at both train and inference time. We also introduce an efficient inference framework for listwise ranking based on m-ary tournament sort with output caching. We evaluate and compare our model on the BEIR benchmark for zero-shot retrieval task, demonstrating that ListT5 (1) outperforms the state-of-the-art RankT5 baseline with a notable +1.3 gain in the average NDCG@10 score, (2) has an efficiency comparable to pointwise ranking models and surpasses the efficiency of previous listwise ranking models, and (3) overcomes the lost-in-the-middle problem of previous listwise rerankers. Our code, model checkpoints, and the evaluation framework are fully open-sourced at \url{https://github.com/soyoung97/ListT5}.
    
[^22]: 消除偏见的基于模型的交互式推荐

    Debiased Model-based Interactive Recommendation

    [https://arxiv.org/abs/2402.15819](https://arxiv.org/abs/2402.15819)

    该论文提出了一种名为iDMIR的模型，通过设计基于因果机制的偏见消除因果世界模型来克服传统基于模型的交互式推荐系统中存在的流行度和抽样偏见的问题。

    

    现有的基于模型的交互式推荐系统通过查询世界模型来捕捉用户偏好，但从历史记录数据中学习世界模型很容易受到偏见问题的困扰，如流行度偏见和抽样偏见。最近提出了一些消除偏见的方法。然而，仍然存在两个主要缺点：1）忽略时间变化流行度的动态会导致对项目的错误重新加权。 2）将未知样本视为负样本在负采样中会导致抽样偏见。为了克服这两个缺点，我们开发了一种称为可识别去偏见模型的交互式推荐模型（简称iDMIR）。在iDMIR中，针对第一个缺点，我们设计了一个基于因果机制的偏见消除因果世界模型，保证了鉴别性。

    arXiv:2402.15819v1 Announce Type: cross  Abstract: Existing model-based interactive recommendation systems are trained by querying a world model to capture the user preference, but learning the world model from historical logged data will easily suffer from bias issues such as popularity bias and sampling bias. This is why some debiased methods have been proposed recently. However, two essential drawbacks still remain: 1) ignoring the dynamics of the time-varying popularity results in a false reweighting of items. 2) taking the unknown samples as negative samples in negative sampling results in the sampling bias. To overcome these two drawbacks, we develop a model called \textbf{i}dentifiable \textbf{D}ebiased \textbf{M}odel-based \textbf{I}nteractive \textbf{R}ecommendation (\textbf{iDMIR} in short). In iDMIR, for the first drawback, we devise a debiased causal world model based on the causal mechanism of the time-varying recommendation generation process with identification guarantee
    
[^23]: 机器学习系统对多模态数据加密的破解与改进

    Cryptanalysis and improvement of multimodal data encryption by machine-learning-based system

    [https://arxiv.org/abs/2402.15779](https://arxiv.org/abs/2402.15779)

    该论文使用机器学习系统对多模态数据加密进行破解和改进，并通过复杂的数学问题极大地加密通信机制，保护个人信息并减少攻击可能性。

    

    随着互联网日渐流行以及通过云和数据中心广泛使用网络和信息系统，个人和组织的隐私和安全变得极为重要。在这个视角下，加密通过保护公共信息交流来有效满足这些要求。为了实现这些目标，研究人员使用了各种加密算法来满足该领域的不同要求，并在工作过程中专注于复杂的数学问题，从而极大地加密通信机制，以尽可能保护个人信息同时显著减少攻击可能性。

    arXiv:2402.15779v1 Announce Type: cross  Abstract: With the rising popularity of the internet and the widespread use of networks and information systems via the cloud and data centers, the privacy and security of individuals and organizations have become extremely crucial. In this perspective, encryption consolidates effective technologies that can effectively fulfill these requirements by protecting public information exchanges. To achieve these aims, the researchers used a wide assortment of encryption algorithms to accommodate the varied requirements of this field, as well as focusing on complex mathematical issues during their work to substantially complicate the encrypted communication mechanism. as much as possible to preserve personal information while significantly reducing the possibility of attacks. Depending on how complex and distinct the requirements established by these various applications are, the potential of trying to break them continues to occur, and systems for eva
    
[^24]: 从脑信号解码查询语义的查询扩展

    Query Augmentation by Decoding Semantics from Brain Signals

    [https://arxiv.org/abs/2402.15708](https://arxiv.org/abs/2402.15708)

    提出了一种名为Brain-Aug的方法，通过从脑信号中解码的语义信息增强查询，可以生成更准确的查询，改善文档排序性能，特别适用于模糊查询。

    

    查询扩展是用于细化语义不准确查询的关键技术。传统上，查询扩展依赖于从最初检索到的、潜在相关的文档中提取信息。如果最初检索到的文档质量较低，则查询扩展的有效性也会受到限制。我们提出了Brain-Aug，通过将从脑信号解码的语义信息结合到查询中来增强查询。Brain-Aug使用了在脑信号信息构建的提示和面向排名的推理方法生成原始查询的延续部分。对fMRI数据集的实验结果显示，Brain-Aug生成的查询在语义上更准确，导致改进的文档排序性能。脑信号带来的这种改进对于模糊查询特别显著。

    arXiv:2402.15708v1 Announce Type: cross  Abstract: Query augmentation is a crucial technique for refining semantically imprecise queries. Traditionally, query augmentation relies on extracting information from initially retrieved, potentially relevant documents. If the quality of the initially retrieved documents is low, then the effectiveness of query augmentation would be limited as well. We propose Brain-Aug, which enhances a query by incorporating semantic information decoded from brain signals. BrainAug generates the continuation of the original query with a prompt constructed with brain signal information and a ranking-oriented inference approach. Experimental results on fMRI (functional magnetic resonance imaging) datasets show that Brain-Aug produces semantically more accurate queries, leading to improved document ranking performance. Such improvement brought by brain signals is particularly notable for ambiguous queries.
    
[^25]: 在线客户服务中的通用模型

    Universal Model in Online Customer Service

    [https://arxiv.org/abs/2402.15666](https://arxiv.org/abs/2402.15666)

    本文介绍了一种在电子商务中改进在线客户服务的解决方案，即提出了一种基于客户问题预测标签的通用模型，无需进行训练，通过消除个别模型训练和维护的需求，减少了模型开发周期和成本。

    

    建立机器学习模型可能是一个耗时的过程，在 typcial 商业场景中常常需要数月来实现。为了确保模型性能的一致性并考虑到数据分布的变化，定期的重新训练是必需的。本文介绍了一种改进电子商务在线客户服务的解决方案，即提出了一种基于客户问题预测标签的通用模型，无需进行训练。我们的新方法涉及使用机器学习技术在对话中标记客户问题，并创建问题及相应标签的存储库。当客户请求帮助时，一个信息检索模型在存储库中搜索相似问题，并使用统计分析来预测相应的标签。通过消除个别模型训练和维护的需求，我们的方法减少了模型开发周期和成本。

    arXiv:2402.15666v1 Announce Type: cross  Abstract: Building machine learning models can be a time-consuming process that often takes several months to implement in typical business scenarios. To ensure consistent model performance and account for variations in data distribution, regular retraining is necessary. This paper introduces a solution for improving online customer service in e-commerce by presenting a universal model for predict-ing labels based on customer questions, without requiring training. Our novel approach involves using machine learning techniques to tag customer questions in transcripts and create a repository of questions and corresponding labels. When a customer requests assistance, an information retrieval model searches the repository for similar questions, and statistical analysis is used to predict the corresponding label. By eliminating the need for individual model training and maintenance, our approach reduces both the model development cycle and costs. The 
    
[^26]: 精妙的康复之路：利用机器学习进行视觉艺术推荐以预防和减少重症监护后遗症

    Artful Path to Healing: Using Machine Learning for Visual Art Recommendation to Prevent and Reduce Post-Intensive Care

    [https://arxiv.org/abs/2402.15643](https://arxiv.org/abs/2402.15643)

    使用机器学习开发了视觉艺术推荐系统，为重症监护后患者提供个性化的治疗性视觉艺术体验，并证明这些推荐能够提升时间情感状态

    

    待在重症监护室（ICU）里经常会带来创伤，导致重症监护后综合征（PICS），包括身体、心理和认知障碍。目前，对于PICS，可用的干预有限。研究表明，接触视觉艺术可能有助于解决PICS的心理方面，如果个性化定制可能效果更佳。我们开发了基于机器学习的视觉艺术推荐系统（VA RecSys），为重症监护后患者提供个性化的治疗性视觉艺术体验。我们研究了四种最先进的VA RecSys引擎，评估了它们推荐的相关性，以治疗目的而言比较专家策划的推荐。我们进行了专家试点测试和大规模用户研究（n=150），以评估这些推荐的适当性和有效性。研究结果表明所有推荐均提升了时间情感状态。

    arXiv:2402.15643v1 Announce Type: new  Abstract: Staying in the intensive care unit (ICU) is often traumatic, leading to post-intensive care syndrome (PICS), which encompasses physical, psychological, and cognitive impairments. Currently, there are limited interventions available for PICS. Studies indicate that exposure to visual art may help address the psychological aspects of PICS and be more effective if it is personalized. We develop Machine Learning-based Visual Art Recommendation Systems (VA RecSys) to enable personalized therapeutic visual art experiences for post-ICU patients. We investigate four state-of-the-art VA RecSys engines, evaluating the relevance of their recommendations for therapeutic purposes compared to expert-curated recommendations. We conduct an expert pilot test and a large-scale user study (n=150) to assess the appropriateness and effectiveness of these recommendations. Our results suggest all recommendations enhance temporal affective states. Visual and mul
    
[^27]: 基于语言的用户偏好推荐方法

    Language-Based User Profiles for Recommendation

    [https://arxiv.org/abs/2402.15623](https://arxiv.org/abs/2402.15623)

    通过使用以人类可读文本表示的用户偏好，提出了Language-based Factorization Model (LFM)，在冷启动环境中与传统方法相比取得了更好的表现

    

    大多数传统的推荐方法（如矩阵分解）将用户偏好表示为高维向量。不幸的是，这些向量缺乏可解释性和可控性，在冷启动环境下往往表现不佳。为了解决这些缺点，我们探索了使用以人类可读文本表示的用户偏好。我们提出了基于语言的因子分解模型（LFM），它本质上是一个编码器/解码器模型，其中编码器和解码器均为大型语言模型（LLM）。编码器LLM从用户的评分历史生成用户兴趣的简洁自然语言描述。解码器LLM使用这个简要描述来完成预测性的下游任务。我们在MovieLens数据集上评估了LFM方法，将其与矩阵分解和直接从用户评分历史预测的LLM模型进行了比较。在冷启动环境下，我们发现我们的方法能够...

    arXiv:2402.15623v1 Announce Type: new  Abstract: Most conventional recommendation methods (e.g., matrix factorization) represent user profiles as high-dimensional vectors. Unfortunately, these vectors lack interpretability and steerability, and often perform poorly in cold-start settings. To address these shortcomings, we explore the use of user profiles that are represented as human-readable text. We propose the Language-based Factorization Model (LFM), which is essentially an encoder/decoder model where both the encoder and the decoder are large language models (LLMs). The encoder LLM generates a compact natural-language profile of the user's interests from the user's rating history. The decoder LLM uses this summary profile to complete predictive downstream tasks. We evaluate our LFM approach on the MovieLens dataset, comparing it against matrix factorization and an LLM model that directly predicts from the user's rating history. In cold-start settings, we find that our method can h
    
[^28]: RecWizard：一种具有模块化、便携模型和交互用户界面的对话式推荐工具包

    RecWizard: A Toolkit for Conversational Recommendation with Modular, Portable Models and Interactive User Interface

    [https://arxiv.org/abs/2402.15591](https://arxiv.org/abs/2402.15591)

    RecWizard是一种对话式推荐工具包，具有模块化、便携模型和交互用户界面，可提高CRS研究效率并减少额外工作量

    

    我们介绍了一种名为RecWizard的新Python工具包，用于对话式推荐系统（CRS）。RecWizard支持模型开发和交互用户界面，借鉴了Huggingface生态系统的最佳实践。CRS与RecWizard具有模块化、便携、交互和大型语言模型（LLMs）友好性，以简化学习过程并减少CRS研究的额外工作量。有关RecWizard更全面的信息，请查看我们的GitHub https://github.com/McAuley-Lab/RecWizard。

    arXiv:2402.15591v1 Announce Type: cross  Abstract: We present a new Python toolkit called RecWizard for Conversational Recommender Systems (CRS). RecWizard offers support for development of models and interactive user interface, drawing from the best practices of the Huggingface ecosystems. CRS with RecWizard are modular, portable, interactive and Large Language Models (LLMs)-friendly, to streamline the learning process and reduce the additional effort for CRS research. For more comprehensive information about RecWizard, please check our GitHub https://github.com/McAuley-Lab/RecWizard.
    
[^29]: 将基于规范的空间表示转换为基于图的时空流行病学模型

    Transforming Norm-based To Graph-based Spatial Representation for Spatio-Temporal Epidemiological Models

    [https://arxiv.org/abs/2402.14539](https://arxiv.org/abs/2402.14539)

    研究介绍了将基于规范的空间表示转换为基于图的时空流行病学模型的新框架和方法。

    

    疫情大流行以其深远的社会和经济影响，对全球健康、死亡率、经济稳定性和政治格局构成重大威胁。为了应对新兴和再现疫情带来的持续挑战，许多研究采用时空模型来增强我们对这些复杂现象的理解和管理。这些时空模型可以大致分为两种主要空间类别：基于规范和基于图的模型，在准确性、计算负担和可表示性之间进行权衡。在本研究中，我们探讨了将这些模型从基于规范转换为基于图的空间表示的能力。我们引入了一个新颖的框架，以及使用广泛的启发式优化方法的十二种可能实现。我们的研究结果显示，通过利用基于代理的模拟和启发式算法来实现图模型

    arXiv:2402.14539v1 Announce Type: new  Abstract: Pandemics, with their profound societal and economic impacts, pose significant threats to global health, mortality rates, economic stability, and political landscapes. In response to the persistent challenges posed by emerging and reemerging pandemics, numerous studies have employed spatio-temporal models to enhance our understanding and management of these complex phenomena. These spatio-temporal models can be roughly divided into two main spatial categories: norm-based and graph-based trade-offering between accuracy, computational burden, and representational feasibility. In this study, we explore the ability to transform from norm-based to graph-based spatial representation for these models. We introduce a novel framework for this task together with twelve possible implementations using a wide range of heuristic optimization approaches. Our findings show that by leveraging agent-based simulations and heuristic algorithms for the graph
    
[^30]: 知识图谱与多模态学习：综述

    Knowledge Graphs Meet Multi-Modal Learning: A Comprehensive Survey

    [https://arxiv.org/abs/2402.05391](https://arxiv.org/abs/2402.05391)

    知识图谱与多模态学习的综述介绍了KG4MM和MM4KG两个主要方面，包括任务定义、构建进展、评估基准以及关键研究轨迹。

    

    知识图谱在推动各种人工智能应用方面起着关键作用，语义网络社区对多模态维度的探索为创新打开了新的途径。在本综述中，我们仔细审查了300多篇文章，重点关注了两个主要方面的知识图谱感知研究：以知识图谱支持多模态任务的KG驱动多模态（KG4MM）学习，将知识图谱研究扩展到多模态知识图谱（MM4KG）领域。我们从定义知识图谱和多模态知识图谱开始，然后探索它们的构建进展。我们的综述包括两个主要任务类别：KG感知的多模态学习任务，如图像分类和视觉问答，以及内在的多模态知识图谱任务，如多模态知识图谱补全和实体对齐，突出了具体的研究轨迹。对于这些任务中的大部分，我们提供了定义、评估基准，并进一步指出进行相关研究的重要见解。最后，我们讨论了cu

    Knowledge Graphs (KGs) play a pivotal role in advancing various AI applications, with the semantic web community's exploration into multi-modal dimensions unlocking new avenues for innovation. In this survey, we carefully review over 300 articles, focusing on KG-aware research in two principal aspects: KG-driven Multi-Modal (KG4MM) learning, where KGs support multi-modal tasks, and Multi-Modal Knowledge Graph (MM4KG), which extends KG studies into the MMKG realm. We begin by defining KGs and MMKGs, then explore their construction progress. Our review includes two primary task categories: KG-aware multi-modal learning tasks, such as Image Classification and Visual Question Answering, and intrinsic MMKG tasks like Multi-modal Knowledge Graph Completion and Entity Alignment, highlighting specific research trajectories. For most of these tasks, we provide definitions, evaluation benchmarks, and additionally outline essential insights for conducting relevant research. Finally, we discuss cu
    
[^31]: 高效的标题重新排序器，用于快速和改进的知识密集型自然语言处理

    Efficient Title Reranker for Fast and Improved Knowledge-Intense NLP

    [https://arxiv.org/abs/2312.12430](https://arxiv.org/abs/2312.12430)

    引入了通过广播查询编码器实现的高效标题重新排序器和为标题重新排序定制的Sigmoid Trick损失函数，相结合在KILT知识基准测试的数据集上取得了最先进的结果。

    

    在最近的RAG方法中，重新排序器在提升检索准确性方面发挥着关键作用，能够揭示每对查询和文本之间的逻辑关系。然而，现有的重新排序器需要反复对查询和大量长文本进行编码。这导致了较高的计算成本，并限制了检索文本的数量，从而影响了准确性。作为问题的解决方案，我们引入了通过广播查询编码器实现的高效标题重新排序器，这是一种用于标题重新排序的新技术，可以使速度提高20倍至40倍，超过基准通道重新排序器。此外，我们还引入了Sigmoid Trick，一种为标题重新排序定制的新损失函数。将这两种技术结合起来，我们在从KILT知识基准测试中实验的四个数据集上都经验验证了它们的有效性，实现了最先进的结果。

    arXiv:2312.12430v3 Announce Type: replace-cross  Abstract: In recent RAG approaches, rerankers play a pivotal role in refining retrieval accuracy with the ability of revealing logical relations for each pair of query and text. However, existing rerankers are required to repeatedly encode the query and a large number of long retrieved text. This results in high computational costs and limits the number of retrieved text, hindering accuracy. As a remedy of the problem, we introduce the Efficient Title Reranker via Broadcasting Query Encoder, a novel technique for title reranking that achieves a 20x-40x speedup over the vanilla passage reranker. Furthermore, we introduce Sigmoid Trick, a novel loss function customized for title reranking. Combining both techniques, we empirically validated their effectiveness, achieving state-of-the-art results on all four datasets we experimented with from the KILT knowledge benchmark.
    
[^32]: 在社交媒体平台中对互联网迷因进行情境化处理

    Contextualizing Internet Memes Across Social Media Platforms

    [https://arxiv.org/abs/2311.11157](https://arxiv.org/abs/2311.11157)

    本研究旨在探究是否可以通过使用知识图来对社交媒体平台上的互联网迷因进行情境化处理，填补了迄今为止对互联网迷因进行全面追踪、识别和映射的空白。

    

    互联网迷因已成为网络上交流和表达观点的新颖格式。它们的流动性和创造性体现在它们被广泛使用，通常跨平台传播，偶尔也用于不道德或有害的目的。 虽然计算工作已经分析了它们随时间的高级别病毒性，并开发了专门用于检测仇恨言论的分类器，但迄今为止还没有努力旨在全面跟踪、识别和映射在社交媒体上发布的互联网迷因。 为了填补这一空白，我们调查了是否可以通过使用语义知识库，即知识图，对社交媒体平台上的互联网迷因进行情境化处理。我们从两个社交媒体平台Reddit和Discord收集了数千条潜在的互联网迷因帖子，并开发了提取-转换-加载过程，创建了一个带有候选迷因帖子的数据湖。

    arXiv:2311.11157v2 Announce Type: replace-cross  Abstract: Internet memes have emerged as a novel format for communication and expressing ideas on the web. Their fluidity and creative nature are reflected in their widespread use, often across platforms and occasionally for unethical or harmful purposes. While computational work has already analyzed their high-level virality over time and developed specialized classifiers for hate speech detection, there have been no efforts to date that aim to holistically track, identify, and map internet memes posted on social media. To bridge this gap, we investigate whether internet memes across social media platforms can be contextualized by using a semantic repository of knowledge, namely, a knowledge graph. We collect thousands of potential internet meme posts from two social media platforms, namely Reddit and Discord, and develop an extract-transform-load procedure to create a data lake with candidate meme posts. By using vision transformer-bas
    
[^33]: 朝向多步推理的答案校准统一视图

    Towards A Unified View of Answer Calibration for Multi-Step Reasoning

    [https://arxiv.org/abs/2311.09101](https://arxiv.org/abs/2311.09101)

    本文总结了最近答案校准技术的分类法，从统一视角对步级和路径级答案校准进行了彻底评估，结果显示整合两种策略的优势倾向于产生最佳结果。

    

    大型语言模型（LLMs）使用“思维链”提示扩展了改进多步推理能力的范围。我们通常将多步推理分为两个阶段：路径生成以生成推理路径；和答案校准后处理推理路径以获得最终答案。然而，现有文献缺乏对不同答案校准方法的系统分析。本文总结了最近答案校准技术的分类法，并将其分解为步级和路径级策略。然后，我们从统一视角对这些策略进行了彻底评估，系统地审查了多路径上的步级和路径级答案校准。实验结果表明，整合两种策略的优势倾向于产生最佳结果。我们的研究有可能启示优化多步推理系统的关键见解。

    arXiv:2311.09101v2 Announce Type: replace-cross  Abstract: Large Language Models (LLMs) employing Chain-of-Thought (CoT) prompting have broadened the scope for improving multi-step reasoning capabilities. We generally divide multi-step reasoning into two phases: path generation to generate the reasoning path(s); and answer calibration post-processing the reasoning path(s) to obtain a final answer. However, the existing literature lacks systematic analysis on different answer calibration approaches. In this paper, we summarize the taxonomy of recent answer calibration techniques and break them down into step-level and path-level strategies. We then conduct a thorough evaluation on these strategies from a unified view, systematically scrutinizing step-level and path-level answer calibration across multiple paths. Experimental results reveal that integrating the dominance of both strategies tends to derive optimal outcomes. Our study holds the potential to illuminate key insights for opti
    
[^34]: ITEm：面向电子商务的无监督图片文本嵌入学习

    ITEm: Unsupervised Image-Text Embedding Learning for eCommerce

    [https://arxiv.org/abs/2311.02084](https://arxiv.org/abs/2311.02084)

    ITEm是面向电子商务的无监督学习模型，通过学习文本和图片的嵌入来更好地关注不同模态，扩展了BERT，并在两个任务上取得了良好的表现。

    

    产品嵌入是电子商务中广泛应用的基石。通过从多种模态学习的产品嵌入显示出明显的改进，因为不同的模态提供了互补信息。然而，一些模态比其他模态更具信息优势。如何教导模型从不同模态学习嵌入而不忽视较不显著模态的信息是具有挑战性的。我们提出了一个图片文本嵌入模型（ITEm），这是一种设计用来更好地关注图片和文本模态的无监督学习方法。我们通过（1）学习文本和图片的嵌入而不知道感兴趣的区域；（2）训练一个全局表示来预测掩码单词并构建掩码图像补丁而不对它们的单独表示进行过程来扩展BERT。我们在两个任务上评估了预训练的ITEm：寻找扩展。

    arXiv:2311.02084v2 Announce Type: replace-cross  Abstract: Product embedding serves as a cornerstone for a wide range of applications in eCommerce. The product embedding learned from multiple modalities shows significant improvement over that from a single modality, since different modalities provide complementary information. However, some modalities are more informatively dominant than others. How to teach a model to learn embedding from different modalities without neglecting information from the less dominant modality is challenging. We present an image-text embedding model (ITEm), an unsupervised learning method that is designed to better attend to image and text modalities. We extend BERT by (1) learning an embedding from text and image without knowing the regions of interest; (2) training a global representation to predict masked words and to construct masked image patches without their individual representations. We evaluate the pre-trained ITEm on two tasks: the search for ext
    
[^35]: 关于利用大型语言模型进行双语词汇识别

    On Bilingual Lexicon Induction with Large Language Models

    [https://arxiv.org/abs/2310.13995](https://arxiv.org/abs/2310.13995)

    本文研究了利用大型语言模型进行双语词汇识别的潜力，通过研究零次提示和少量上下文提示等方法，探讨了这种方法如何与当前BLI方法相比，并如何进行补充。

    

    双语词汇识别（BLI）是多语言自然语言处理中的核心任务，目前在很大程度上仍然依赖于计算跨语言单词表示。受自然语言处理领域向大型语言模型（LLMs）的全球范式转变的启发，我们探讨了最新一代LLMs在双语词汇开发中的潜力。我们提出了以下研究问题：是否可能促使和微调多语言LLMs（mLLMs）以进行BLI，并且这种方法与当前BLI方法相比如何以及如何补充？为此，我们系统地研究了1）用于无监督BLI的零次提示和2）使用一组种子翻译对进行少量上下文提示，均无需进行任何LLM微调，以及3）对较小LLMs进行标准BLI导向微调。我们在涵盖不同大小（从0.3B到13B参数）的18个开源文本对文本mLLMs上进行实验，涵盖两个标准BLI基准测试。

    arXiv:2310.13995v2 Announce Type: replace-cross  Abstract: Bilingual Lexicon Induction (BLI) is a core task in multilingual NLP that still, to a large extent, relies on calculating cross-lingual word representations. Inspired by the global paradigm shift in NLP towards Large Language Models (LLMs), we examine the potential of the latest generation of LLMs for the development of bilingual lexicons. We ask the following research question: Is it possible to prompt and fine-tune multilingual LLMs (mLLMs) for BLI, and how does this approach compare against and complement current BLI approaches? To this end, we systematically study 1) zero-shot prompting for unsupervised BLI and 2) few-shot in-context prompting with a set of seed translation pairs, both without any LLM fine-tuning, as well as 3) standard BLI-oriented fine-tuning of smaller LLMs. We experiment with 18 open-source text-to-text mLLMs of different sizes (from 0.3B to 13B parameters) on two standard BLI benchmarks covering a rang
    
[^36]: RecMind：大型语言模型驱动的推荐代理

    RecMind: Large Language Model Powered Agent For Recommendation

    [https://arxiv.org/abs/2308.14296](https://arxiv.org/abs/2308.14296)

    RecMind是一种LLM驱动的自主推荐代理，通过Self-Inspiring算法提高了规划能力，能够为零-shot个性化推荐提供支持。

    

    推荐系统（RS）通过深度学习取得了显著进展，但当前RS方法通常在特定任务数据集上训练和微调模型，限制了它们对新推荐任务的泛化能力以及利用外部知识的能力，因为模型规模和数据大小的限制。因此，我们设计了一种LLM驱动的自主推荐代理RecMind，能够利用外部知识，利用谨慎规划的工具为零-shot个性化推荐提供支持。我们提出了一种Self-Inspiring算法来提高规划能力。在每个中间步骤，LLM自我激励以考虑所有先前探索过的状态来规划下一步。这一机制极大地提高了模型理解和利用历史信息规划推荐的能力。我们评估了RecMind在各种推荐场景中的性能。

    arXiv:2308.14296v2 Announce Type: replace-cross  Abstract: While the recommendation system (RS) has advanced significantly through deep learning, current RS approaches usually train and fine-tune models on task-specific datasets, limiting their generalizability to new recommendation tasks and their ability to leverage external knowledge due to model scale and data size constraints. Thus, we designed an LLM-powered autonomous recommender agent, RecMind, which is capable of leveraging external knowledge, utilizing tools with careful planning to provide zero-shot personalized recommendations. We propose a Self-Inspiring algorithm to improve the planning ability. At each intermediate step, the LLM self-inspires to consider all previously explored states to plan for the next step. This mechanism greatly improves the model's ability to comprehend and utilize historical information in planning for recommendation. We evaluate RecMind's performance in various recommendation scenarios. Our exper
    
[^37]: 当联邦推荐遇到冷启动问题：分离物品属性和用户交互

    When Federated Recommendation Meets Cold-Start Problem: Separating Item Attributes and User Interactions

    [https://arxiv.org/abs/2305.12650](https://arxiv.org/abs/2305.12650)

    本文提出了一种名为Item-aligned Federated Aggregation (IFedRec)的新方法，通过利用物品属性和交互记录同时学习两组物品表示，并设计了物品表示对齐机制，从而在联邦学习框架中解决了冷启动推荐的挑战。

    

    arXiv:2305.12650v2 公告类型：替换 摘要：联邦推荐系统通常在服务器上训练全局模型，而无法直接访问用户在自己设备上的私人数据。然而，推荐模型与用户私人数据的分离在提供优质服务方面提出了挑战，特别是在联邦设置中针对新项目，即冷启动推荐。本文介绍了一种名为物品对齐联邦聚合（IFedRec）的新颖方法来解决这一挑战。这是联邦推荐中专门研究冷启动场景的第一项研究工作。所提出的方法通过同时利用物品属性和交互记录学习两组物品表示。此外，设计了一种物品表示对齐机制来对齐两组物品表示，并在联邦学习框架中在服务器端学习元属性网络。在四个基准数据集上进行实验

    arXiv:2305.12650v2 Announce Type: replace  Abstract: Federated recommendation system usually trains a global model on the server without direct access to users' private data on their own devices. However, this separation of the recommendation model and users' private data poses a challenge in providing quality service, particularly when it comes to new items, namely cold-start recommendations in federated settings. This paper introduces a novel method called Item-aligned Federated Aggregation (IFedRec) to address this challenge. It is the first research work in federated recommendation to specifically study the cold-start scenario. The proposed method learns two sets of item representations by leveraging item attributes and interaction records simultaneously. Additionally, an item representation alignment mechanism is designed to align two item representations and learn the meta attribute network at the server within a federated learning framework. Experiments on four benchmark dataset
    
[^38]: Klarna产品页面数据集：利用图神经网络和大型语言模型进行网络元素提名

    The Klarna Product Page Dataset: Web Element Nomination with Graph Neural Networks and Large Language Models

    [https://arxiv.org/abs/2111.02168](https://arxiv.org/abs/2111.02168)

    Klarna产品页面数据集是一个全面多样的网页集合，为解决网络自动化算法设计中数据集稀缺性问题提供了新的资源。

    

    Web自动化有可能彻底改变用户与数字世界的互动方式，通过复杂的计算方法提供无与伦比的帮助，简化任务。在这一演进过程中，网络元素提名任务至关重要，它涉及识别网页上的独特元素。不幸的是，网络自动化算法设计的发展受到全面和真实反映网络应用程序复杂性的数据集的稀缺性的阻碍。为了解决这一问题，我们推出了Klarna产品页面数据集，这是一个全面多样的网页集合，超越了现有数据集的丰富性和多样性。该数据集包含来自8,175个电子商务网站的51,701个手动标记的产品页面，覆盖了八个地理区域，并附带了一组渲染页面截图数据集。为了开始研究Klarna产品页面数据集，我们进行了实证研究

    arXiv:2111.02168v4 Announce Type: replace-cross  Abstract: Web automation holds the potential to revolutionize how users interact with the digital world, offering unparalleled assistance and simplifying tasks via sophisticated computational methods. Central to this evolution is the web element nomination task, which entails identifying unique elements on webpages. Unfortunately, the development of algorithmic designs for web automation is hampered by the scarcity of comprehensive and realistic datasets that reflect the complexity faced by real-world applications on the Web. To address this, we introduce the Klarna Product Page Dataset, a comprehensive and diverse collection of webpages that surpasses existing datasets in richness and variety. The dataset features 51,701 manually labeled product pages from 8,175 e-commerce websites across eight geographic regions, accompanied by a dataset of rendered page screenshots. To initiate research on the Klarna Product Page Dataset, we empirical
    
[^39]: 通过具有分层正则化的医学代码中心的多模态对比EHR建模预测下次就诊诊断

    Next Visit Diagnosis Prediction via Medical Code-Centric Multimodal Contrastive EHR Modelling with Hierarchical Regularisation. (arXiv:2401.11648v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2401.11648](http://arxiv.org/abs/2401.11648)

    通过医学代码中心的多模态对比EHR建模预测下次就诊诊断，并通过分层正则化提高性能。

    

    在医疗保健中，利用电子健康记录（EHR）预测下次就诊的诊断是一项必要的任务，对于制定医疗保健提供者和患者的主动未来计划至关重要。然而，之前的许多研究并没有充分解决EHR数据固有的异构和分层特征，必然导致次优的性能。为此，我们提出了NECHO，一种新颖的医学代码中心的多模态对比EHR学习框架，其中包括分层正则化。首先，我们使用定制的网络设计和一对双模态对比损失融合涵盖医学代码、人口统计数据和临床笔记的多方面信息，所有这些都围绕着医学代码表现。我们还使用医学本体中的父级信息来规范特定模态的编码器，以学习EHR数据的层次结构。对MIMIC-III数据进行的一系列实验证明了我们方法的有效性。

    Predicting next visit diagnosis using Electronic Health Records (EHR) is an essential task in healthcare, critical for devising proactive future plans for both healthcare providers and patients. Nonetheless, many preceding studies have not sufficiently addressed the heterogeneous and hierarchical characteristics inherent in EHR data, inevitably leading to sub-optimal performance. To this end, we propose NECHO, a novel medical code-centric multimodal contrastive EHR learning framework with hierarchical regularisation. First, we integrate multifaceted information encompassing medical codes, demographics, and clinical notes using a tailored network design and a pair of bimodal contrastive losses, all of which pivot around a medical code representation. We also regularise modality-specific encoders using a parental level information in medical ontology to learn hierarchical structure of EHR data. A series of experiments on MIMIC-III data demonstrates effectiveness of our approach.
    
[^40]: 使用大型语言模型在制造业中进行知识共享：用户评估和模型基准测试

    Knowledge Sharing in Manufacturing using Large Language Models: User Evaluation and Model Benchmarking. (arXiv:2401.05200v1 [cs.HC])

    [http://arxiv.org/abs/2401.05200](http://arxiv.org/abs/2401.05200)

    使用大型语言模型在制造业中进行知识共享，通过评估实证了该系统的效益，提高了操作员的信息检索速度和问题解决效率，同时强调在有人工专家选项时的偏好。GPT-4是最优秀的模型。

    

    高效管理知识对组织的成功至关重要。在制造业中，操作工厂变得越来越依赖知识，这给工厂培训和支持新操作员的能力带来了压力。本文介绍了一个基于大型语言模型（LLM）的系统，旨在利用工厂文档中包含的广泛知识，高效回答操作员的查询并促进新知识的共享。为了评估其有效性，我们在一个工厂环境中进行了评估。评估结果表明该系统的好处，即能够更快地检索信息和更高效地解决问题。然而，研究也强调了在有人工专家选项时更倾向于向人工专家学习。此外，我们还对该系统进行了几种闭源和开源语言模型的基准测试。GPT-4表现始终优于其他模型，像StableBe

    Managing knowledge efficiently is crucial for organizational success. In manufacturing, operating factories has become increasing knowledge-intensive putting strain on the factory's capacity to train and support new operators. In this paper, we introduce a Large Language Model (LLM)-based system designed to use the extensive knowledge contained in factory documentation. The system aims to efficiently answer queries from operators and facilitate the sharing of new knowledge. To assess its effectiveness, we conducted an evaluation in a factory setting. The results of this evaluation demonstrated the system's benefits; namely, in enabling quicker information retrieval and more efficient resolution of issues. However, the study also highlighted a preference for learning from a human expert when such an option is available. Furthermore, we benchmarked several closed and open-sourced LLMs for this system. GPT-4 consistently outperformed its counterparts, with open-source models like StableBe
    
[^41]: 用大语言模型进行推荐中的表示学习

    Representation Learning with Large Language Models for Recommendation. (arXiv:2310.15950v3 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2310.15950](http://arxiv.org/abs/2310.15950)

    这篇论文介绍了一个模型-不可知的框架RLMRec，通过使用大语言模型（LLMs）来增强传统的基于ID的推荐系统，并解决了可扩展性问题、仅依赖文本的限制以及提示输入限制等挑战。

    

    推荐系统在深度学习和图神经网络的影响下取得了显著进展，特别是在捕捉复杂的用户-物品关系方面。然而，这些基于图的推荐系统严重依赖于基于ID的数据，可能忽略了与用户和物品相关的有价值的文本信息，导致学到的表示不够富有信息。此外，隐式反馈数据的利用引入了潜在的噪声和偏差，给用户偏好学习的有效性带来了挑战。尽管将大语言模型（LLMs）与传统的基于ID的推荐系统相结合已经引起了人们的关注，但在实际推荐系统中有效实施还需要解决可扩展性问题、仅依赖文本的限制以及提示输入限制等挑战。为了解决这些挑战，我们提出了一个模型不可知的框架RLMRec，旨在通过LLM强化表示来增强现有的推荐系统。

    Recommender systems have seen significant advancements with the influence of deep learning and graph neural networks, particularly in capturing complex user-item relationships. However, these graph-based recommenders heavily depend on ID-based data, potentially disregarding valuable textual information associated with users and items, resulting in less informative learned representations. Moreover, the utilization of implicit feedback data introduces potential noise and bias, posing challenges for the effectiveness of user preference learning. While the integration of large language models (LLMs) into traditional ID-based recommenders has gained attention, challenges such as scalability issues, limitations in text-only reliance, and prompt input constraints need to be addressed for effective implementation in practical recommender systems. To address these challenges, we propose a model-agnostic framework RLMRec that aims to enhance existing recommenders with LLM-empowered representati
    
[^42]: 利用异构数据进行兴趣点推荐的调查

    A Survey on Point-of-Interest Recommendations Leveraging Heterogeneous Data. (arXiv:2308.07426v1 [cs.IR])

    [http://arxiv.org/abs/2308.07426](http://arxiv.org/abs/2308.07426)

    本文针对旅游领域的兴趣点推荐问题进行了调查研究，探讨了利用异构数据解决旅途中兴趣点推荐问题的潜力与挑战。

    

    旅游是推荐系统的一个重要应用领域。在这个领域中，推荐系统主要负责为交通、住宿、兴趣点或旅游服务提供个性化推荐。在这些任务中，尤其是对个体游客可能感兴趣的兴趣点进行推荐的问题近年来引起了越来越多的关注。然而，在游客“旅途中”提供兴趣点推荐可能会面临特殊挑战，因为用户的上下文变化多样。随着互联网的快速发展和当今各种在线服务的大量数据，各种异构数据源的数据已经变得可用，这些异构数据源为解决旅途中兴趣点推荐问题的挑战提供了巨大潜力。在这项工作中，我们从异构数据的角度提供了2017年至2022年间已发表的兴趣点推荐研究的综述。

    Tourism is an important application domain for recommender systems. In this domain, recommender systems are for example tasked with providing personalized recommendations for transportation, accommodation, points-of-interest (POIs), or tourism services. Among these tasks, in particular the problem of recommending POIs that are of likely interest to individual tourists has gained growing attention in recent years. Providing POI recommendations to tourists \emph{during their trip} can however be especially challenging due to the variability of the users' context. With the rapid development of the Web and today's multitude of online services, vast amounts of data from various sources have become available, and these heterogeneous data sources represent a huge potential to better address the challenges of in-trip POI recommendation problems. In this work, we provide a comprehensive survey of published research on POI recommendation between 2017 and 2022 from the perspective of heterogeneou
    
[^43]: 从生成模型的角度重新审视实体对齐及超越：一个视角

    Revisit and Outstrip Entity Alignment: A Perspective of Generative Models. (arXiv:2305.14651v1 [cs.CL])

    [http://arxiv.org/abs/2305.14651](http://arxiv.org/abs/2305.14651)

    本文重新从生成模型的角度研究了基于嵌入的实体对齐（EEA）问题，引入基于生成对抗网络的EEA方法及提出的生成的EEA（GEEA）框架，通过互相变分自动编码器（M-VAE）实现实体从一个KG转换到另一个KG，并且从随机噪声向量生成新的实体，具有较好的效果。

    

    最近，基于嵌入的方法在利用多模态知识图谱（KG）嵌入的实体对齐方面取得了巨大成功。在本文中，我们从生成模型的角度研究了基于嵌入的实体对齐（EEA）。我们表明EEA是一个特殊的问题，其主要目标类似于典型生成模型中的目标，基于这个目标，我们从理论上证明了最近发展的基于生成对抗网络（GAN）的EEA方法的有效性。然后，我们揭示了他们不完整的目标限制了实体对齐和实体合成（即生成新实体）的能力。我们通过引入生成的EEA（abbr.，GEEA）框架和提出的互相变分自动编码器（M-VAE）作为生成模型来缓解这个问题。M-VAE可以将一个实体从一个KG转换到另一个KG，并从随机噪声向量生成新实体。我们通过理论分析和实证实验展示了GEEA的优势。

    Recent embedding-based methods have achieved great successes on exploiting entity alignment from knowledge graph (KG) embeddings of multiple modals. In this paper, we study embedding-based entity alignment (EEA) from a perspective of generative models. We show that EEA is a special problem where the main objective is analogous to that in a typical generative model, based on which we theoretically prove the effectiveness of the recently developed generative adversarial network (GAN)-based EEA methods. We then reveal that their incomplete objective limits the capacity on both entity alignment and entity synthesis (i.e., generating new entities). We mitigate this problem by introducing a generative EEA (abbr., GEEA) framework with the proposed mutual variational autoencoder (M-VAE) as the generative model. M-VAE can convert an entity from one KG to another and generate new entities from random noise vectors. We demonstrate the power of GEEA with theoretical analysis and empirical experime
    
[^44]: 探索的价值：度量、发现和算法

    Value of Exploration: Measurements, Findings and Algorithms. (arXiv:2305.07764v1 [cs.IR])

    [http://arxiv.org/abs/2305.07764](http://arxiv.org/abs/2305.07764)

    本研究通过量化探索对内容语料库的影响，证明了探索对用户体验的长期好处，并尝试使用神经线性匀速臂算法构建基于探索的排名系统。

    

    有效的探索被认为对推荐平台上用户体验的长期影响有积极作用。然而，确定其确切的好处一直是具有挑战性的。探索的常规A/B测试通常测量中性甚至消极的参与度指标，同时未能捕捉其长期效益。为了解决这个问题，我们提出了一项系统性的研究，通过检查探索对内容语料库的影响来正式量化探索的价值，这是推荐系统中直接影响用户体验的关键实体。具体而言，我们引入了新的度量标准和相关实验设计来测量探索对语料库变化的益处，并进一步将语料库变化与长期用户体验联系起来。此外，我们研究了引入神经线性匀速臂算法构建基于探索的排名系统的可能性，并将其用作我们的案例研究的骨干算法。我们在大规模真实场景下进行了广泛的实时实验。

    Effective exploration is believed to positively influence the long-term user experience on recommendation platforms. Determining its exact benefits, however, has been challenging. Regular A/B tests on exploration often measure neutral or even negative engagement metrics while failing to capture its long-term benefits. To address this, we present a systematic study to formally quantify the value of exploration by examining its effects on the content corpus, a key entity in the recommender system that directly affects user experiences. Specifically, we introduce new metrics and the associated experiment design to measure the benefit of exploration on the corpus change, and further connect the corpus change to the long-term user experience. Furthermore, we investigate the possibility of introducing the Neural Linear Bandit algorithm to build an exploration-based ranking system, and use it as the backbone algorithm for our case study. We conduct extensive live experiments on a large-scale 
    
[^45]: 生成式推荐：走向下一代推荐系统范式。

    Generative Recommendation: Towards Next-generation Recommender Paradigm. (arXiv:2304.03516v1 [cs.IR])

    [http://arxiv.org/abs/2304.03516](http://arxiv.org/abs/2304.03516)

    生成式AI可以克服推荐系统中的限制，使其能够生成满足用户特定信息需求的内容，并且用户可以通过自然语言指令来指导内容生成。

    

    推荐系统通常从项目集合中检索项目进行个性化推荐。然而，这种基于检索的推荐范式面临两个限制：1）语料库中的人工生成项目可能无法满足用户的多样化信息需求，2）用户通常通过点击等被动且低效的反馈方式调整推荐内容。近年来，人工智能生成内容在各个领域取得显著成功，具有克服这些限制的潜力：1）生成式人工智能可以生成个性化的内容以满足用户特定的信息需求，2）新兴的ChatGPT通过自然语言指令显著提高了用户准确表达信息需求的能力。在这种情况下，人工智能生成内容的大爆发指引我们走向下一代推荐范式，具有两个新的目标：1）通过生成式人工智能生成个性化内容，2）整合用户指令以指导由人工智能生成的内容。

    Recommender systems typically retrieve items from an item corpus for personalized recommendations. However, such a retrieval-based recommender paradigm faces two limitations: 1) the human-generated items in the corpus might fail to satisfy the users' diverse information needs, and 2) users usually adjust the recommendations via passive and inefficient feedback such as clicks. Nowadays, AI-Generated Content (AIGC) has revealed significant success across various domains, offering the potential to overcome these limitations: 1) generative AI can produce personalized items to meet users' specific information needs, and 2) the newly emerged ChatGPT significantly facilitates users to express information needs more precisely via natural language instructions. In this light, the boom of AIGC points the way towards the next-generation recommender paradigm with two new objectives: 1) generating personalized content through generative AI, and 2) integrating user instructions to guide content gene
    
[^46]: CompoDiff: 基于潜在扩散的多功能组合图像检索

    CompoDiff: Versatile Composed Image Retrieval With Latent Diffusion. (arXiv:2303.11916v1 [cs.CV])

    [http://arxiv.org/abs/2303.11916](http://arxiv.org/abs/2303.11916)

    CompoDiff 是一种多功能的组合图像检索模型，通过接受各种条件，具有潜在扩散的能力，并在 FashionIQ 上实现了新的零样本最新技术水平。其特征位于完整的 CLIP 嵌入空间中，可以直接用于所有利用 CLIP 空间的模型。

    

    本文提出了一种新颖的基于扩散的模型 CompoDiff，用于解决具有潜在扩散的组合图像检索（CIR）问题，并提供了一个由 1800 万个参考图像、条件和相应的目标图像三元组组成的新数据集，用于训练模型。CompoDiff 不仅在像 FashionIQ 这样的 CIR 基准测试上实现了新的零样本最新技术水平，而且还通过接收各种条件（如负文本和图像遮罩条件），使得 CIR 更加多功能，这是现有 CIR 方法所不具备的。此外，CompoDiff 特征位于完整的 CLIP 嵌入空间中，因此它们可以直接用于利用 CLIP 空间的所有现有模型。训练所使用的代码和数据集，以及预训练权重可在 https://github.com/navervision/CompoDiff 上获得。

    This paper proposes a novel diffusion-based model, CompoDiff, for solving Composed Image Retrieval (CIR) with latent diffusion and presents a newly created dataset of 18 million reference images, conditions, and corresponding target image triplets to train the model. CompoDiff not only achieves a new zero-shot state-of-the-art on a CIR benchmark such as FashionIQ but also enables a more versatile CIR by accepting various conditions, such as negative text and image mask conditions, which are unavailable with existing CIR methods. In addition, the CompoDiff features are on the intact CLIP embedding space so that they can be directly used for all existing models exploiting the CLIP space. The code and dataset used for the training, and the pre-trained weights are available at https://github.com/navervision/CompoDiff
    
[^47]: 宽泛推荐系统：一种高效的非线性协同过滤方法

    Broad Recommender System: An Efficient Nonlinear Collaborative Filtering Approach. (arXiv:2204.11602v4 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2204.11602](http://arxiv.org/abs/2204.11602)

    本文提出了一种新的宽泛推荐系统(BroadCF)，使用宽泛学习系统(BLS)作为映射函数来学习用户和项目之间的复杂非线性关系，同时通过用户-项评级协同向量预处理程序将原始数据转换为更适合BLS学习的格式。BroadCF的实验结果表明，在用户推荐准确性和效率方面都优于几种最先进的CF方法。

    

    最近，深度神经网络（DNN）被广泛引入到协同过滤（CF）中，以产生更准确的推荐结果，因为它们具有捕获项目和用户之间复杂非线性关系的能力。然而，基于DNN的模型通常遭受高计算复杂性的问题，即消耗非常长的训练时间并存储大量可训练参数。为了解决这些问题，我们提出了一种新的宽泛推荐系统，称为宽泛协同过滤（BroadCF），它是一种高效的非线性协同过滤方法。宽泛学习系统（BLS）被用作映射函数，以学习用户和项目之间的复杂非线性关系，可以避免上述问题，同时实现非常令人满意的推荐性能。但是，将原始评分数据直接馈送到BLS中并不可行。为此，我们提出了一种用户-项评级协同向量预处理程序，将原始数据转换为更适合BLS学习的格式。三个公共数据集上的实验结果表明，我们提出的BroadCF在推荐准确性和效率方面均优于几种最先进的CF方法。

    Recently, Deep Neural Networks (DNNs) have been widely introduced into Collaborative Filtering (CF) to produce more accurate recommendation results due to their capability of capturing the complex nonlinear relationships between items and users.However, the DNNs-based models usually suffer from high computational complexity, i.e., consuming very long training time and storing huge amount of trainable parameters. To address these problems, we propose a new broad recommender system called Broad Collaborative Filtering (BroadCF), which is an efficient nonlinear collaborative filtering approach. Instead of DNNs, Broad Learning System (BLS) is used as a mapping function to learn the complex nonlinear relationships between users and items, which can avoid the above issues while achieving very satisfactory recommendation performance. However, it is not feasible to directly feed the original rating data into BLS. To this end, we propose a user-item rating collaborative vector preprocessing pro
    

