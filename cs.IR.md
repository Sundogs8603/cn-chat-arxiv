# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [WikiMT++ Dataset Card.](http://arxiv.org/abs/2309.13259) | WikiMT++是一个扩展和精细版本的WikiMusicText数据集，包含了1010个经过策划的ABC记谱法的主题曲。它添加了客观属性和主观情感属性，增强了数据集的应用场景和可用性，并通过CLaMP来纠正属性，提高准确性和完整性。 |
| [^2] | [Document Understanding for Healthcare Referrals.](http://arxiv.org/abs/2309.13184) | 该论文提出了一个混合模型，结合LayoutLMv3和领域特定规则，用于识别传真转诊文档中的关键实体。结果表明，在变换器模型中添加领域特定规则可以显著提高精确度和F1分数，从而提高转诊管理的效率。 |
| [^3] | [American Family Cohort, a data resource description.](http://arxiv.org/abs/2309.13175) | 本文介绍了一个庞大且新颖的美国家庭队列（AFC）的电子健康记录（EHR）数据资源，包含约9000万次就诊记录和750万名患者的数据。 |
| [^4] | [UNICON: A unified framework for behavior-based consumer segmentation in e-commerce.](http://arxiv.org/abs/2309.13068) | UNICON是一个统一的深度学习消费者细分框架，利用丰富的消费者行为数据进行个性化，实现了通过扩大预定义的目标种子细分来获取类似目标的个性化结果，并通过揭示具有相似性倾向的非明显消费者细分来获取数据驱动的个性化结果。 |
| [^5] | [Using Large Language Models to Generate, Validate, and Apply User Intent Taxonomies.](http://arxiv.org/abs/2309.13063) | 通过使用大型语言模型生成用户意图分类，我们提出了一种新方法来分析和验证日志数据中的用户意图，从而解决了手动或基于机器学习的标注方法在大型和不断变化的数据集上的问题。 |
| [^6] | [Decoding the Alphabet Soup of Degrees in the United States Postsecondary Education System Through Hybrid Method: Database and Text Mining.](http://arxiv.org/abs/2309.13050) | 本文提出了一个混合模型，通过数据库和文本挖掘方法，解码了美国高等教育系统中学位的不确定表达，并通过对学生追踪报告进行解释和分类，实现了对学位级别的准确预测。这种分类有助于研究学生成功和流动的模式。 |
| [^7] | [Privacy Preserving Machine Learning for Behavioral Authentication Systems.](http://arxiv.org/abs/2309.13046) | 本文研究了针对行为认证系统的隐私保护机器学习。我们使用随机投影技术来确保神经网络模型中的数据隐私，以防止隐私攻击。这种方法可以消除个人资料数据库的需求，并能有效验证用户的身份。 |
| [^8] | [A Hierarchical Neural Framework for Classification and its Explanation in Large Unstructured Legal Documents.](http://arxiv.org/abs/2309.10563) | 本论文提出了一个名为MESc的分层神经框架，用于分类和解释大型非结构化法律文件。通过将文件分成多个部分并使用大型语言模型的嵌入和无监督聚类，该框架能够实现从长文档中预测判决并提取解释。 |
| [^9] | [Human Action Co-occurrence in Lifestyle Vlogs using Graph Link Prediction.](http://arxiv.org/abs/2309.06219) | 该论文提出了自动识别人类动作共现的任务，并创建了ACE数据集以及相应的代码。通过利用视觉和文本信息的图链接预测模型，可以有效捕捉不同数据域中的人类动作关系。 |
| [^10] | [Multi-Relational Contrastive Learning for Recommendation.](http://arxiv.org/abs/2309.01103) | 本论文介绍了一种多关系对比学习框架（RCL），用于解决个性化推荐系统中单一行为学习的限制。RCL模型通过多关系图编码器捕捉短期偏好的异质性，并使用动态跨关系记忆网络来捕捉用户的长期多行为模式。 |
| [^11] | [Multi-event Video-Text Retrieval.](http://arxiv.org/abs/2308.11551) | 本研究引入了多事件视频文本检索（MeVTR）任务，解决了传统视频文本检索任务中的一种特殊场景，即每个视频包含多个不同事件的情况。 |
| [^12] | [Recommender Systems with Generative Retrieval.](http://arxiv.org/abs/2305.05065) | 本文提出了一种新型的生成式检索模型，将检索和生成组合在一起以产生推荐。 |
| [^13] | [Overview of the TREC 2022 NeuCLIR Track.](http://arxiv.org/abs/2304.12367) | TREC NeuCLIR轨道的第一年，研究神经方法对跨语言信息检索的影响，通过英语查询来ad hoc排名检索中文、波斯语或俄语新闻文档，共有12个团队提交了172次运行。 |
| [^14] | [Committed Private Information Retrieval.](http://arxiv.org/abs/2302.01733) | 该论文提出了一种承诺PIR方案，通过结合线性映射承诺和任意线性PIR方案，实现了$k$-可验证的PIR方案。 |
| [^15] | [Spatio-Temporal Contrastive Learning Enhanced GNNs for Session-based Recommendation.](http://arxiv.org/abs/2209.11461) | 本研究提出了一种名为基于会话的推荐的新框架，通过利用对比学习技术的均匀性和对齐性特性，增强了GNNs对于基于会话的推荐的性能。 |
| [^16] | [Crime Hot-Spot Modeling via Topic Modeling and Relative Density Estimation.](http://arxiv.org/abs/2202.04176) | 本研究提出了一种通过主题建模和相对密度估计来进行犯罪热点建模的方法。实验证明该方法可以捕捉到被调度员忽视的地理热点趋势，这些热点趋势往往与整体事件密度的增加相混淆。 |
| [^17] | [BLM-17m: A Large-Scale Dataset for Black Lives Matter Topic Detection on Twitter.](http://arxiv.org/abs/2105.01331) | 本论文提出了一个用于推特上检测黑人生命至关重要话题的大规模数据集BLM-17m，涵盖了乔治·弗洛伊德事件期间的17百万推文。作者提供了两个基线模型TF-IDF和LDA，并对其进行了评估。 |

# 详细

[^1]: WikiMT++数据集卡片

    WikiMT++ Dataset Card. (arXiv:2309.13259v1 [cs.IR])

    [http://arxiv.org/abs/2309.13259](http://arxiv.org/abs/2309.13259)

    WikiMT++是一个扩展和精细版本的WikiMusicText数据集，包含了1010个经过策划的ABC记谱法的主题曲。它添加了客观属性和主观情感属性，增强了数据集的应用场景和可用性，并通过CLaMP来纠正属性，提高准确性和完整性。

    

    WikiMT++是WikiMusicText（WikiMT）的扩展和精细版本，包含了1010个经过策划的ABC记谱法的主题曲。为了扩展WikiMT的应用场景，我们添加了客观属性（专辑、歌词、视频）和主观情感属性（12个情感形容词）和情感4Q（Russell 4Q），增强了其在音乐信息检索、条件音乐生成、自动作曲和情感分类等方面的可用性。此外，我们还实现了CLaMP来纠正从WikiMT继承的属性，以减少原始数据收集过程中引入的错误，增强了数据集的准确性和完整性。

    WikiMT++ is an expanded and refined version of WikiMusicText (WikiMT), featuring 1010 curated lead sheets in ABC notation. To expand application scenarios of WikiMT, we add both objective (album, lyrics, video) and subjective emotion (12 emotion adjectives) and emo\_4q (Russell 4Q) attributes, enhancing its usability for music information retrieval, conditional music generation, automatic composition, and emotion classification, etc. Additionally, CLaMP is implemented to correct the attributes inherited from WikiMT to reduce errors introduced during original data collection and enhance the accuracy and completeness of our dataset.
    
[^2]: 医疗转诊的文件理解

    Document Understanding for Healthcare Referrals. (arXiv:2309.13184v1 [cs.CL])

    [http://arxiv.org/abs/2309.13184](http://arxiv.org/abs/2309.13184)

    该论文提出了一个混合模型，结合LayoutLMv3和领域特定规则，用于识别传真转诊文档中的关键实体。结果表明，在变换器模型中添加领域特定规则可以显著提高精确度和F1分数，从而提高转诊管理的效率。

    

    依赖于扫描文档和传真通信的医疗转诊导致了高昂的行政成本和可能影响病人护理的错误。在这项工作中，我们提出了一个混合模型，利用LayoutLMv3和领域特定规则来识别传真转诊文档中的关键病人、医生和检查相关实体。我们探讨了将文档理解模型应用于转诊中所面临的一些挑战，这些转诊的格式因医疗实践而异，并使用MUC-5指标评估模型性能，以获得适用于实际用例的适当指标。我们的分析结果显示，将领域特定规则添加到变换器模型中可以大大提高精确度和F1分数，这表明在经过策划的数据集上训练的混合模型可以提高转诊管理的效率。

    Reliance on scanned documents and fax communication for healthcare referrals leads to high administrative costs and errors that may affect patient care. In this work we propose a hybrid model leveraging LayoutLMv3 along with domain-specific rules to identify key patient, physician, and exam-related entities in faxed referral documents. We explore some of the challenges in applying a document understanding model to referrals, which have formats varying by medical practice, and evaluate model performance using MUC-5 metrics to obtain appropriate metrics for the practical use case. Our analysis shows the addition of domain-specific rules to the transformer model yields greatly increased precision and F1 scores, suggesting a hybrid model trained on a curated dataset can increase efficiency in referral management.
    
[^3]: 美国家庭队列研究资源描述

    American Family Cohort, a data resource description. (arXiv:2309.13175v1 [cs.IR])

    [http://arxiv.org/abs/2309.13175](http://arxiv.org/abs/2309.13175)

    本文介绍了一个庞大且新颖的美国家庭队列（AFC）的电子健康记录（EHR）数据资源，包含约9000万次就诊记录和750万名患者的数据。

    

    本文描述了一个庞大且新颖的电子健康记录（EHR）数据资源，名为美国家庭队列（AFC）。AFC数据源自美国家庭医学委员会（ABFM）的PRIME注册表，该注册表是最大的国家合格的临床数据注册表（QCDR）之一。数据已转换为通用的数据模型，即观察性健康数据科学与信息学（OHDSI）观察性医疗结果伙伴关系（OMOP）的共同数据模型（CDM）。该资源包括约9000万次就诊记录和750万名患者。100％的患者提供年龄、性别和地址信息，73％报告种族信息。近93％的患者的实验室数据使用LOINC标准，86％的患者的药物数据使用RxNorm标准，93％的患者的诊断数据使用SNOWMED和ICD，81％的患者的操作数据使用HCPCS或CPT，61％的患者有保险信息。

    This manuscript is a research resource description and presents a large and novel Electronic Health Records (EHR) data resource, American Family Cohort (AFC). The AFC data is derived from Centers for Medicare and Medicaid Services (CMS) certified American Board of Family Medicine (ABFM) PRIME registry. The PRIME registry is the largest national Qualified Clinical Data Registry (QCDR) for Primary Care. The data is converted to a popular common data model, the Observational Health Data Sciences and Informatics (OHDSI) Observational Medical Outcomes Partnership (OMOP) Common Data Model (CDM).  The resource presents approximately 90 million encounters for 7.5 million patients. All 100% of the patients present age, gender, and address information, and 73% report race. Nealy 93% of patients have lab data in LOINC, 86% have medication data in RxNorm, 93% have diagnosis in SNOWMED and ICD, 81% have procedures in HCPCS or CPT, and 61% have insurance information. The richness, breadth, and diver
    
[^4]: UNICON:一种在电子商务中基于行为的消费者细分的统一框架

    UNICON: A unified framework for behavior-based consumer segmentation in e-commerce. (arXiv:2309.13068v1 [cs.IR])

    [http://arxiv.org/abs/2309.13068](http://arxiv.org/abs/2309.13068)

    UNICON是一个统一的深度学习消费者细分框架，利用丰富的消费者行为数据进行个性化，实现了通过扩大预定义的目标种子细分来获取类似目标的个性化结果，并通过揭示具有相似性倾向的非明显消费者细分来获取数据驱动的个性化结果。

    

    数据驱动的个性化是时尚电子商务的关键实践，提高了企业为消费者提供更相关内容的方式。而超级个性化为每个消费者提供高度个定制的体验，但需要大量的个人数据来创建个性化的用户旅程。为了减轻这一问题，基于群体的个性化提供了基于更广泛的共同偏好构建的中度个性化，并且仍能对结果进行个性化。我们引入了UNICON，这是一个统一的深度学习消费者细分框架，利用丰富的消费者行为数据来学习长期的潜在表示，并利用它们提取两种关键类型的细分，以满足不同的个性化使用案例：类似目标，通过与行为相似的消费者扩大预定义的目标种子细分，并且数据驱动，揭示具有相似性倾向的非明显消费者细分。通过详细的实验证明，

    Data-driven personalization is a key practice in fashion e-commerce, improving the way businesses serve their consumers needs with more relevant content. While hyper-personalization offers highly targeted experiences to each consumer, it requires a significant amount of private data to create an individualized journey. To alleviate this, group-based personalization provides a moderate level of personalization built on broader common preferences of a consumer segment, while still being able to personalize the results. We introduce UNICON, a unified deep learning consumer segmentation framework that leverages rich consumer behavior data to learn long-term latent representations and utilizes them to extract two pivotal types of segmentation catering various personalization use-cases: lookalike, expanding a predefined target seed segment with consumers of similar behavior, and data-driven, revealing non-obvious consumer segments with similar affinities. We demonstrate through extensive exp
    
[^5]: 使用大型语言模型生成、验证和应用用户意图分类方法

    Using Large Language Models to Generate, Validate, and Apply User Intent Taxonomies. (arXiv:2309.13063v1 [cs.IR])

    [http://arxiv.org/abs/2309.13063](http://arxiv.org/abs/2309.13063)

    通过使用大型语言模型生成用户意图分类，我们提出了一种新方法来分析和验证日志数据中的用户意图，从而解决了手动或基于机器学习的标注方法在大型和不断变化的数据集上的问题。

    

    日志数据可以揭示用户与网络搜索服务的交互方式、用户的需求以及满意程度等宝贵信息。然而，分析日志数据中的用户意图并不容易，尤其是对于新的网络搜索形式，如人工智能驱动的聊天。为了理解日志数据中的用户意图，我们需要一种能够用有意义的分类方式标记它们的方法，以捕捉其多样性和动态性。现有的方法依赖于手动或基于机器学习的标注，这些方法对于大型且不断变化的数据集而言，要么代价高昂要么不够灵活。我们提出了一种使用大型语言模型(LLM)的新方法，这种模型能够生成丰富且相关的概念、描述和示例来表示用户意图。然而，使用LLM生成用户意图分类并将其应用于日志分析可能存在两个主要问题：这样的分类得不到外部验证，并且可能存在不良的反馈回路。为了克服这些问题，我们提出了一种新的方法，通过人工专家和评估者来验证。

    Log data can reveal valuable information about how users interact with web search services, what they want, and how satisfied they are. However, analyzing user intents in log data is not easy, especially for new forms of web search such as AI-driven chat. To understand user intents from log data, we need a way to label them with meaningful categories that capture their diversity and dynamics. Existing methods rely on manual or ML-based labeling, which are either expensive or inflexible for large and changing datasets. We propose a novel solution using large language models (LLMs), which can generate rich and relevant concepts, descriptions, and examples for user intents. However, using LLMs to generate a user intent taxonomy and apply it to do log analysis can be problematic for two main reasons: such a taxonomy is not externally validated, and there may be an undesirable feedback loop. To overcome these issues, we propose a new methodology with human experts and assessors to verify th
    
[^6]: 通过混合方法：数据库和文本挖掘，解码美国高等教育体系中的学位的字母组合。

    Decoding the Alphabet Soup of Degrees in the United States Postsecondary Education System Through Hybrid Method: Database and Text Mining. (arXiv:2309.13050v1 [cs.IR])

    [http://arxiv.org/abs/2309.13050](http://arxiv.org/abs/2309.13050)

    本文提出了一个混合模型，通过数据库和文本挖掘方法，解码了美国高等教育系统中学位的不确定表达，并通过对学生追踪报告进行解释和分类，实现了对学位级别的准确预测。这种分类有助于研究学生成功和流动的模式。

    

    本文提出了一个模型，用于预测在国家学生清算中心（NSC）的学生追踪报告中含糊不清地表达的高等教育学位（例如学士、硕士等）的级别。该模型是两个模块的混合体。第一个模块通过参考我们编制的近950个美国高等教育机构学位标题缩写的综合数据库，解释NSC报告中嵌入的相关缩写元素。第二个模块是CNN-BiLSTM模型的特征分类和文本挖掘的组合，前面有数个繁重的预处理步骤。本文提出的模型是通过四个不同分辨率的多标签数据集进行训练的，并在最复杂的数据集上返回了97.83％的准确率。这种对学位级别的彻底分类将为研究学生成功和流动的模式提供见解。

    This paper proposes a model to predict the levels (e.g., Bachelor, Master, etc.) of postsecondary degree awards that have been ambiguously expressed in the student tracking reports of the National Student Clearinghouse (NSC). The model will be the hybrid of two modules. The first module interprets the relevant abbreviatory elements embedded in NSC reports by referring to a comprehensive database that we have made of nearly 950 abbreviations for degree titles used by American postsecondary educators. The second module is a combination of feature classification and text mining modeled with CNN-BiLSTM, which is preceded by several steps of heavy pre-processing. The model proposed in this paper was trained with four multi-label datasets of different grades of resolution and returned 97.83\% accuracy with the most sophisticated dataset. Such a thorough classification of degree levels will provide insights into the modeling patterns of student success and mobility. To date, such a classifica
    
[^7]: 针对行为认证系统的隐私保护机器学习

    Privacy Preserving Machine Learning for Behavioral Authentication Systems. (arXiv:2309.13046v1 [cs.IR])

    [http://arxiv.org/abs/2309.13046](http://arxiv.org/abs/2309.13046)

    本文研究了针对行为认证系统的隐私保护机器学习。我们使用随机投影技术来确保神经网络模型中的数据隐私，以防止隐私攻击。这种方法可以消除个人资料数据库的需求，并能有效验证用户的身份。

    

    行为认证系统使用用户的行为特征来验证其身份。通过在用户个人资料上训练神经网络分类器，可以构建一个行为认证验证算法。训练好的神经网络模型对呈现的验证数据进行分类，如果分类结果与声明的身份匹配，则接受该声明。这种基于分类的方法消除了维护个人资料数据库的需求。然而，类似于其他神经网络结构，行为认证系统的神经网络分类器容易受到隐私攻击。为了保护神经网络中使用的训练和测试数据的隐私，广泛使用各种不同的技术。本文主要关注一种非加密的方法，我们使用随机投影来确保神经网络模型中的数据隐私。随机投影是一种基于随机矩阵的距离保持转换。在与验证者共享个人资料之前，用户将通过随机投影对其个人资料进行转换，并保持其隐私性。

    A behavioral authentication (BA) system uses the behavioral characteristics of users to verify their identity claims. A BA verification algorithm can be constructed by training a neural network (NN) classifier on users' profiles. The trained NN model classifies the presented verification data, and if the classification matches the claimed identity, the verification algorithm accepts the claim. This classification-based approach removes the need to maintain a profile database. However, similar to other NN architectures, the NN classifier of the BA system is vulnerable to privacy attacks. To protect the privacy of training and test data used in an NN different techniques are widely used. In this paper, our focus is on a non-crypto-based approach, and we used random projection (RP) to ensure data privacy in an NN model. RP is a distance-preserving transformation based on a random matrix. Before sharing the profiles with the verifier, users will transform their profiles by RP and keep thei
    
[^8]: 一个用于分类和解释大型非结构化法律文件的分层神经框架

    A Hierarchical Neural Framework for Classification and its Explanation in Large Unstructured Legal Documents. (arXiv:2309.10563v1 [cs.IR])

    [http://arxiv.org/abs/2309.10563](http://arxiv.org/abs/2309.10563)

    本论文提出了一个名为MESc的分层神经框架，用于分类和解释大型非结构化法律文件。通过将文件分成多个部分并使用大型语言模型的嵌入和无监督聚类，该框架能够实现从长文档中预测判决并提取解释。

    

    自动法律判决预测及其解释常常面临长达数万字的案例文件和非统一结构的问题。在没有结构标注的文件上预测判决并提取解释变得更具挑战性。本论文将这一问题定义为“稀缺标注法律文件”，并通过一种称为MESc（基于多阶段编码器的带聚类的监督）的深度学习分类框架来探索缺乏结构信息和长文档的特点。具体来说，我们将文档分成多个部分，从自定义微调的大型语言模型的最后四个层中提取它们的嵌入，并试图通过无监督聚类来近似它们的结构。然后，我们利用另一组Transformer编码器层学习部分之间的表示。我们探索了多十亿参数的大型语言模型在这种情况下的适应性。

    Automatic legal judgment prediction and its explanation suffer from the problem of long case documents exceeding tens of thousands of words, in general, and having a non-uniform structure. Predicting judgments from such documents and extracting their explanation becomes a challenging task, more so on documents with no structural annotation. We define this problem as "scarce annotated legal documents" and explore their lack of structural information and their long lengths with a deep learning-based classification framework which we call MESc; "Multi-stage Encoder-based Supervised with-clustering"; for judgment prediction. Specifically, we divide a document into parts to extract their embeddings from the last four layers of a custom fine-tuned Large Language Model, and try to approximate their structure through unsupervised clustering. Which we use in another set of transformer encoder layers to learn the inter-chunk representations. We explore the adaptability of LLMs with multi-billion
    
[^9]: 使用图链接预测在生活方式vlog中的人类动作共现

    Human Action Co-occurrence in Lifestyle Vlogs using Graph Link Prediction. (arXiv:2309.06219v1 [cs.CV])

    [http://arxiv.org/abs/2309.06219](http://arxiv.org/abs/2309.06219)

    该论文提出了自动识别人类动作共现的任务，并创建了ACE数据集以及相应的代码。通过利用视觉和文本信息的图链接预测模型，可以有效捕捉不同数据域中的人类动作关系。

    

    我们介绍了自动识别人类动作共现的任务，即确定两个人类动作是否可以在同一时间间隔内共现。我们创建并公开了ACE（Action Co-occurrencE）数据集，该数据集由约12k个共现的视觉动作对和它们对应的视频片段组成的大型图形。我们描述了利用视觉和文本信息来自动推断两个动作是否共现的图链接预测模型。我们证明了图形特别适合捕捉人类动作之间的关系，并且所学习的图形表示对于我们的任务是有效的，并且在不同的数据域中捕捉到新颖而相关的信息。本文介绍的ACE数据集和代码可在https://github.com/MichiganNLP/vlog_action_co-occurrence公开获取。

    We introduce the task of automatic human action co-occurrence identification, i.e., determine whether two human actions can co-occur in the same interval of time. We create and make publicly available the ACE (Action Co-occurrencE) dataset, consisting of a large graph of ~12k co-occurring pairs of visual actions and their corresponding video clips. We describe graph link prediction models that leverage visual and textual information to automatically infer if two actions are co-occurring. We show that graphs are particularly well suited to capture relations between human actions, and the learned graph representations are effective for our task and capture novel and relevant information across different data domains. The ACE dataset and the code introduced in this paper are publicly available at https://github.com/MichiganNLP/vlog_action_co-occurrence.
    
[^10]: 多关系对比学习用于推荐系统

    Multi-Relational Contrastive Learning for Recommendation. (arXiv:2309.01103v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2309.01103](http://arxiv.org/abs/2309.01103)

    本论文介绍了一种多关系对比学习框架（RCL），用于解决个性化推荐系统中单一行为学习的限制。RCL模型通过多关系图编码器捕捉短期偏好的异质性，并使用动态跨关系记忆网络来捕捉用户的长期多行为模式。

    

    个性化推荐系统在捕捉用户随时间变化的偏好、提供准确有效的推荐方面起着至关重要的作用。然而，许多推荐模型只依赖于一种行为学习，这限制了它们在真实场景中表示用户和物品之间复杂关系的能力。在这种情况下，用户以多种方式与物品互动，包括点击、标记为喜爱、评论和购买。为了解决这个问题，我们提出了关系感知对比学习（RCL）框架，它有效地建模动态交互异质性。RCL模型包括一个多关系图编码器，捕捉短期偏好的异质性，同时保留不同类型用户-物品交互的专用关系语义。此外，我们设计了一个动态跨关系记忆网络，使RCL模型能够捕捉用户的长期多行为模式。

    Personalized recommender systems play a crucial role in capturing users' evolving preferences over time to provide accurate and effective recommendations on various online platforms. However, many recommendation models rely on a single type of behavior learning, which limits their ability to represent the complex relationships between users and items in real-life scenarios. In such situations, users interact with items in multiple ways, including clicking, tagging as favorite, reviewing, and purchasing. To address this issue, we propose the Relation-aware Contrastive Learning (RCL) framework, which effectively models dynamic interaction heterogeneity. The RCL model incorporates a multi-relational graph encoder that captures short-term preference heterogeneity while preserving the dedicated relation semantics for different types of user-item interactions. Moreover, we design a dynamic cross-relational memory network that enables the RCL model to capture users' long-term multi-behavior p
    
[^11]: 多事件视频文本检索

    Multi-event Video-Text Retrieval. (arXiv:2308.11551v1 [cs.CV])

    [http://arxiv.org/abs/2308.11551](http://arxiv.org/abs/2308.11551)

    本研究引入了多事件视频文本检索（MeVTR）任务，解决了传统视频文本检索任务中的一种特殊场景，即每个视频包含多个不同事件的情况。

    

    视频文本检索（VTR）是互联网上海量视频文本数据时代中一项关键的多模态任务。使用双流视觉-语言模型架构学习视频文本对的联合表示成为VTR任务中一种突出的方法。然而，这些模型在假设视频文本对应是双射的情况下运行，并忽视了更实际的情况，即视频内容通常涵盖多个事件，而用户查询或网页元数据等文本往往是具体的，并对应单个事件。这造成了之前的训练目标与实际应用之间的差距，在推理过程中可能导致早期模型的性能下降。本研究引入了多事件视频文本检索（MeVTR）任务，针对每个视频包含多个不同事件的场景，作为传统视频文本检索任务的一个利基场景。

    Video-Text Retrieval (VTR) is a crucial multi-modal task in an era of massive video-text data on the Internet. A plethora of work characterized by using a two-stream Vision-Language model architecture that learns a joint representation of video-text pairs has become a prominent approach for the VTR task. However, these models operate under the assumption of bijective video-text correspondences and neglect a more practical scenario where video content usually encompasses multiple events, while texts like user queries or webpage metadata tend to be specific and correspond to single events. This establishes a gap between the previous training objective and real-world applications, leading to the potential performance degradation of earlier models during inference. In this study, we introduce the Multi-event Video-Text Retrieval (MeVTR) task, addressing scenarios in which each video contains multiple different events, as a niche scenario of the conventional Video-Text Retrieval Task. We pr
    
[^12]: 生成式检索推荐系统

    Recommender Systems with Generative Retrieval. (arXiv:2305.05065v1 [cs.IR])

    [http://arxiv.org/abs/2305.05065](http://arxiv.org/abs/2305.05065)

    本文提出了一种新型的生成式检索模型，将检索和生成组合在一起以产生推荐。

    

    现代推荐系统使用大规模检索模型进行推荐，包括两个阶段：训练双编码模型将查询和候选项嵌入到相同的空间中，然后使用近似最近邻搜索来选择给定查询嵌入的顶部候选项。本文提出了一种新的单阶段范例：生成式检索模型，该模型通过自回归方式在一个阶段中解码目标候选项的标识符。为此，我们不是为每个项目分配随机生成的原子ID，而是生成语义ID：每个项目的语义有意义的元组编码词，它作为其唯一标识符。我们使用称为RQ-VAE的分层方法生成这些编码词。一旦我们对所有项目都有了语义ID，就会训练基于Transformer的序列到序列模型来预测下一个项目的语义ID。由于这个模型以自回归的方式直接预测标识下一个项的编码词元组，因此它可以将检索和生成组合在一起以产生推荐。

    Modern recommender systems leverage large-scale retrieval models consisting of two stages: training a dual-encoder model to embed queries and candidates in the same space, followed by an Approximate Nearest Neighbor (ANN) search to select top candidates given a query's embedding. In this paper, we propose a new single-stage paradigm: a generative retrieval model which autoregressively decodes the identifiers for the target candidates in one phase. To do this, instead of assigning randomly generated atomic IDs to each item, we generate Semantic IDs: a semantically meaningful tuple of codewords for each item that serves as its unique identifier. We use a hierarchical method called RQ-VAE to generate these codewords. Once we have the Semantic IDs for all the items, a Transformer based sequence-to-sequence model is trained to predict the Semantic ID of the next item. Since this model predicts the tuple of codewords identifying the next item directly in an autoregressive manner, it can be c
    
[^13]: TREC 2022 NeuCLIR轨道综述

    Overview of the TREC 2022 NeuCLIR Track. (arXiv:2304.12367v1 [cs.IR])

    [http://arxiv.org/abs/2304.12367](http://arxiv.org/abs/2304.12367)

    TREC NeuCLIR轨道的第一年，研究神经方法对跨语言信息检索的影响，通过英语查询来ad hoc排名检索中文、波斯语或俄语新闻文档，共有12个团队提交了172次运行。

    

    这是TREC神经CLIR（NeuCLIR）轨道的第一年，旨在研究神经方法对跨语言信息检索的影响。今年轨道的主要任务是使用用英语表达的查询，对中文、波斯语或俄语新闻文档进行ad hoc排名检索。话题是使用标准的TREC流程开发的，除了在评估该话题的不同语言上评估一个注释器开发的话题时，由另一个注释器开发的话题。共有12个团队提交了172次运行。

    This is the first year of the TREC Neural CLIR (NeuCLIR) track, which aims to study the impact of neural approaches to cross-language information retrieval. The main task in this year's track was ad hoc ranked retrieval of Chinese, Persian, or Russian newswire documents using queries expressed in English. Topics were developed using standard TREC processes, except that topics developed by an annotator for one language were assessed by a different annotator when evaluating that topic on a different language. There were 172 total runs submitted by twelve teams.
    
[^14]: 承诺私人信息检索

    Committed Private Information Retrieval. (arXiv:2302.01733v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2302.01733](http://arxiv.org/abs/2302.01733)

    该论文提出了一种承诺PIR方案，通过结合线性映射承诺和任意线性PIR方案，实现了$k$-可验证的PIR方案。

    

    私人信息检索（PIR）方案允许客户端在$k$个服务器的$n$个项目$x_1,x_2,\ldots,x_n$中检索出一个数据项目$x_i$，即使当$t<k$个服务器合谋并试图学习$i$时也不会透露$i$是什么。这样的PIR方案被称为$t-$私密。如果客户端可以在$v\leq k$个服务器合谋并试图通过发送篡改的数据来愚弄客户端的情况下验证检索到的$x_i$的正确性，则PIR方案为$v-$可验证。文献中的大多数先前研究假设$v<k$，留下了服务器全部合谋的情况。我们提出了一种通用构造，将线性映射承诺（LMC）和任意线性PIR方案结合起来，以产生一个$k-$可验证的PIR方案，称为承诺PIR方案。即使在最坏的情况下，当所有服务器都在攻击者的控制下，尽管隐私无法避免丢失，客户端也不会被愚弄而接受不正确的$x_i$。

    A private information retrieval (PIR) scheme allows a client to retrieve a data item $x_i$ among $n$ items $x_1,x_2,\ldots,x_n$ from $k$ servers, without revealing what $i$ is even when $t < k$ servers collude and try to learn $i$. Such a PIR scheme is said to be $t$-private. A PIR scheme is $v$-verifiable if the client can verify the correctness of the retrieved $x_i$ even when $v \leq k$ servers collude and try to fool the client by sending manipulated data. Most of the previous works in the literature on PIR assumed that $v < k$, leaving the case of all-colluding servers open. We propose a generic construction that combines a linear map commitment (LMC) and an arbitrary linear PIR scheme to produce a $k$-verifiable PIR scheme, termed a committed PIR scheme. Such a scheme guarantees that even in the worst scenario, when all servers are under the control of an attacker, although the privacy is unavoidably lost, the client won't be fooled into accepting an incorrect $x_i$. We demonstra
    
[^15]: 增强GNNs的时空对比学习用于基于会话的推荐

    Spatio-Temporal Contrastive Learning Enhanced GNNs for Session-based Recommendation. (arXiv:2209.11461v3 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2209.11461](http://arxiv.org/abs/2209.11461)

    本研究提出了一种名为基于会话的推荐的新框架，通过利用对比学习技术的均匀性和对齐性特性，增强了GNNs对于基于会话的推荐的性能。

    

    基于会话的推荐系统旨在利用用户的短期行为序列来预测下一个项目，而不需要详细的用户资料。最近的研究主要通过将会话视为项目之间的转换图，并利用各种图神经网络（GNNs）对项目及其邻居之间的关系进行编码，以建模用户的偏好。一些现有的基于GNN的模型主要关注从空间图结构的视角聚合信息，忽视了在信息传递过程中项目的邻居之间的时间关系，导致信息丢失，从而产生次优问题。其他的工作通过整合额外的时间信息来解决这个问题，但缺乏空间和时间模式之间充分的交互。为了解决这个问题，受对比学习技术的均匀性和对齐性特性的启发，我们提出了一个名为基于会话的推荐的新框架。

    Session-based recommendation (SBR) systems aim to utilize the user's short-term behavior sequence to predict the next item without the detailed user profile. Most recent works try to model the user preference by treating the sessions as between-item transition graphs and utilize various graph neural networks (GNNs) to encode the representations of pair-wise relations among items and their neighbors. Some of the existing GNN-based models mainly focus on aggregating information from the view of spatial graph structure, which ignores the temporal relations within neighbors of an item during message passing and the information loss results in a sub-optimal problem. Other works embrace this challenge by incorporating additional temporal information but lack sufficient interaction between the spatial and temporal patterns. To address this issue, inspired by the uniformity and alignment properties of contrastive learning techniques, we propose a novel framework called Session-based Recommenda
    
[^16]: 通过主题建模和相对密度估计的犯罪热点建模

    Crime Hot-Spot Modeling via Topic Modeling and Relative Density Estimation. (arXiv:2202.04176v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.04176](http://arxiv.org/abs/2202.04176)

    本研究提出了一种通过主题建模和相对密度估计来进行犯罪热点建模的方法。实验证明该方法可以捕捉到被调度员忽视的地理热点趋势，这些热点趋势往往与整体事件密度的增加相混淆。

    

    我们提出了一种方法，通过对犯罪记录叙述的集合进行主题分布的计算，确定相似呼叫的分组以及它们的相对空间分布。我们首先为每个叙述获取一个主题分布，然后提出了一种最近邻相对密度估计（kNN-RDE）方法来获得每个主题的空间相对密度。在亚特兰大警察局的大量叙述文档（$n=475,019$）上进行的实验表明，我们的方法能够捕捉到通常被呼叫调度员一开始没有察觉到的地理热点趋势，这些趋势由于与一般事件密度的混淆而被忽视。

    We present a method to capture groupings of similar calls and determine their relative spatial distribution from a collection of crime record narratives. We first obtain a topic distribution for each narrative, and then propose a nearest neighbors relative density estimation (kNN-RDE) approach to obtain spatial relative densities per topic. Experiments over a large corpus ($n=475,019$) of narrative documents from the Atlanta Police Department demonstrate the viability of our method in capturing geographic hot-spot trends which call dispatchers do not initially pick up on and which go unnoticed due to conflation with elevated event density in general.
    
[^17]: BLM-17m: 一个用于推特上黑人生命至关重要话题检测的大规模数据集

    BLM-17m: A Large-Scale Dataset for Black Lives Matter Topic Detection on Twitter. (arXiv:2105.01331v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2105.01331](http://arxiv.org/abs/2105.01331)

    本论文提出了一个用于推特上检测黑人生命至关重要话题的大规模数据集BLM-17m，涵盖了乔治·弗洛伊德事件期间的17百万推文。作者提供了两个基线模型TF-IDF和LDA，并对其进行了评估。

    

    人权保护是世界上最重要的问题之一。本文旨在提供一个涵盖最近几个月全球影响深远的人权矛盾之一——乔治·弗洛伊德事件的数据集。我们提出了一个带有17百万推文的主题检测标记数据集。这些推文是从2020年5月25日至2020年8月21日收集的，涵盖了这一事件开始后的89天。我们通过监测全球和本地报纸的最热门新闻主题对数据集进行了标记。除此之外，我们还提供了两个基线模型，TF-IDF和LDA。我们使用三个不同的k值对这两种方法的精确度、召回率和F1分数进行了评估。收集到的数据集可以在https://github.com/MeysamAsgariC/BLMT 上找到。

    Protection of human rights is one of the most important problems of our world. In this paper, our aim is to provide a dataset which covers one of the most significant human rights contradiction in recent months affected the whole world, George Floyd incident. We propose a labeled dataset for topic detection that contains 17 million tweets. These Tweets are collected from 25 May 2020 to 21 August 2020 that covers 89 days from start of this incident. We labeled the dataset by monitoring most trending news topics from global and local newspapers. Apart from that, we present two baselines, TF-IDF and LDA. We evaluated the results of these two methods with three different k values for metrics of precision, recall and f1-score. The collected dataset is available at https://github.com/MeysamAsgariC/BLMT.
    

