# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Fresh Content Needs More Attention: Multi-funnel Fresh Content Recommendation.](http://arxiv.org/abs/2306.01720) | 本文介绍了如何在一个大型商业平台上构建一个成功的专门用于新鲜内容推荐系统。他们构建了一个多入口提名系统，有效平衡了覆盖和相关性，同时通过考虑预测不确定性来避免展示平庸内容。 |
| [^2] | [Pretrained Language Model based Web Search Ranking: From Relevance to Satisfaction.](http://arxiv.org/abs/2306.01599) | 该论文提出了一种基于预训练语言模型的框架SAT-Ranker，旨在以统一方式全面建模用户满意度的不同维度，从而在网络搜索中排名用户满意度，并在实验中取得了优异结果。 |
| [^3] | [Influence Maximization with Fairness at Scale (Extended Version).](http://arxiv.org/abs/2306.01587) | 本文针对影响最大化问题中的公平问题，提出了一种基于学习节点表示的方法来实现公平传播，从而实现可扩展的解决方案，适用于具有数百万或数十亿节点的网络。 |
| [^4] | [Syst\`eme de recommandations bas\'e sur les contraintes pour les simulations de gestion de crise.](http://arxiv.org/abs/2306.01504) | 本文提出使用基于约束的推荐系统来推荐驾驶员/车辆配对，在危机管理模拟中实现城市居民的自愿参与。 |
| [^5] | [Hierarchical Reinforcement Learning for Modeling User Novelty-Seeking Intent in Recommender Systems.](http://arxiv.org/abs/2306.01476) | 本文提出了一种基于层次强化学习的方法，用于建模用户的层次新奇寻求意图并调整推荐策略以提高推荐项目准确性和多样性。 |
| [^6] | [Prompt Tuning Large Language Models on Personalized Aspect Extraction for Recommendations.](http://arxiv.org/abs/2306.01475) | 本文提出了一种端到端的方法，将面向方面的提取和推荐相结合，同时引入了个性化基于方面的推荐模型，并利用大型语言模型设计了新的模型来为最终的推荐任务生成方面。 |
| [^7] | [An OPC UA-based industrial Big Data architecture.](http://arxiv.org/abs/2306.01418) | 本文提出了一种基于OPC UA的大数据架构，利用元数据和语义模型提高数据处理和查询效率，使多个处理池能够协同工作而不干扰现场设备和生产过程。 |
| [^8] | [DWT-CompCNN: Deep Image Classification Network for High Throughput JPEG 2000 Compressed Documents.](http://arxiv.org/abs/2306.01359) | 这篇论文提出了一种名为DWT-CompCNN的深度学习模型，它可以直接对使用HTJ2K算法压缩的文档进行分类，从而提高计算效率。 |
| [^9] | [Reducing Popularity Bias in Recommender Systems through AUC-Optimal Negative Sampling.](http://arxiv.org/abs/2306.01348) | 通过最优负采样从根本上缓解流行度偏差，提高模型的性能，同时保持准确性。 |
| [^10] | [LyricSIM: A novel Dataset and Benchmark for Similarity Detection in Spanish Song LyricS.](http://arxiv.org/abs/2306.01325) | 本文介绍了一种针对语义相似性任务的新数据集和基准，由多个本地注释者完成集体注释实验，评估了各种最先进的单语和多语言语言模型的性能，建立了基准结果。 |
| [^11] | [JEPOO: Highly Accurate Joint Estimation of Pitch, Onset and Offset for Music Information Retrieval.](http://arxiv.org/abs/2306.01304) | 本文提出了一种名为JEPOO的音乐信息检索方法，能够准确估计音高、起始和终止，支持单音高和多音高数据，比现有最先进的方法精度提升高达10.6%，8.3%和10.3%。 |
| [^12] | [Self Contrastive Learning for Session-based Recommendation.](http://arxiv.org/abs/2306.01266) | 本文提出了自对比学习方法，简化了会话推荐领域基于对比学习的模型的复杂性，并提高了推荐性能。 |
| [^13] | [How Ready are Pre-trained Abstractive Models and LLMs for Legal Case Judgement Summarization?.](http://arxiv.org/abs/2306.01248) | 这篇论文探讨了是否可以使用预训练的抽象模型和大型语言模型来自动生成法律案例判决的摘要，并在印度的法庭案例判决中进行了相关实验分析。 |
| [^14] | [TimelineQA: A Benchmark for Question Answering over Timelines.](http://arxiv.org/abs/2306.01069) | TimelineQA是一个用于查询生活日志的加速进展的基准测试，涉及时间和地理信息，已经公开发布，并使用两种最先进的模型进行了实验，但这两种模型均未达到人类表现水平。 |
| [^15] | [Quick Dense Retrievers Consume KALE: Post Training Kullback Leibler Alignment of Embeddings for Asymmetrical dual encoders.](http://arxiv.org/abs/2304.01016) | 本文提出了一种通过结构压缩和模型尺寸不对称的双编码器模型 KALE，有效提高密集信息检索的推理效率，同时允许查询编码器的有效压缩，而无需进行全部的再训练或索引生成，此方法能够生成超过DistilBERT性能的模型。 |
| [^16] | [PrefRec: Recommender Systems with Human Preferences for Reinforcing Long-term User Engagement.](http://arxiv.org/abs/2212.02779) | 该论文提出了一种基于人类偏好的推荐系统范式PrefRec，允许强化学习推荐系统从用户历史行为偏好中学习，以优化长期用户参与度。 |
| [^17] | [Self-Supervised Learning for Recommender Systems: A Survey.](http://arxiv.org/abs/2203.15876) | 本文综述了自监督推荐（SSR）的研究进展，提出了四种方法：对比、生成、预测和混合，并介绍了各种方法的优缺点及研究方向。 |
| [^18] | [Classifying YouTube Comments Based on Sentiment and Type of Sentence.](http://arxiv.org/abs/2111.01908) | 本论文提出了一种基于情感和句子类型的方法，将原始的YouTube评论分类，以帮助YouTuber找到更相关的评论，从而增加其观众群。 |

# 详细

[^1]: 新鲜内容需要更多关注: 多入口新鲜内容推荐

    Fresh Content Needs More Attention: Multi-funnel Fresh Content Recommendation. (arXiv:2306.01720v1 [cs.IR])

    [http://arxiv.org/abs/2306.01720](http://arxiv.org/abs/2306.01720)

    本文介绍了如何在一个大型商业平台上构建一个成功的专门用于新鲜内容推荐系统。他们构建了一个多入口提名系统，有效平衡了覆盖和相关性，同时通过考虑预测不确定性来避免展示平庸内容。

    

    推荐系统作为将用户与庞大、多样和不断增长的内容集合进行连接的中介。实际上，需要填补关于新鲜（和尾部）内容的信息缺失，以便让观众得以发现和利用它们。本文分享了我们在一个大型商业平台上构建专用的新鲜内容推荐系统的成功经验。为了提名新鲜内容，我们构建了一个多入口提名系统，它结合了（i）具有广泛泛化能力的两塔模型和（ii）具有接近实时的用户反馈更新的序列模型，以保持覆盖和相关性之间的有效平衡。深入的研究揭示了用户活动水平与其接近新鲜内容的关系，这进一步激发了一个上下文多入口的设置。提名的新鲜候选者然后由系统得分和排名，考虑预测的不确定性，以避免暴露平庸的内容。这个提名和排名系统极大地提高了新鲜内容的整体质量和易于与现有的推荐系统集成。

    Recommendation system serves as a conduit connecting users to an incredibly large, diverse and ever growing collection of contents. In practice, missing information on fresh (and tail) contents needs to be filled in order for them to be exposed and discovered by their audience. We here share our success stories in building a dedicated fresh content recommendation stack on a large commercial platform. To nominate fresh contents, we built a multi-funnel nomination system that combines (i) a two-tower model with strong generalization power for coverage, and (ii) a sequence model with near real-time update on user feedback for relevance. The multi-funnel setup effectively balances between coverage and relevance. An in-depth study uncovers the relationship between user activity level and their proximity toward fresh contents, which further motivates a contextual multi-funnel setup. Nominated fresh candidates are then scored and ranked by systems considering prediction uncertainty to further
    
[^2]: 基于预训练语言模型的网络搜索排名: 从相关性到满足感

    Pretrained Language Model based Web Search Ranking: From Relevance to Satisfaction. (arXiv:2306.01599v1 [cs.IR])

    [http://arxiv.org/abs/2306.01599](http://arxiv.org/abs/2306.01599)

    该论文提出了一种基于预训练语言模型的框架SAT-Ranker，旨在以统一方式全面建模用户满意度的不同维度，从而在网络搜索中排名用户满意度，并在实验中取得了优异结果。

    

    搜索引擎在满足用户多种信息需求方面发挥着关键作用。最近，基于预训练语言模型（PLMs）的文本排名模型在Web搜索中取得了巨大成功。然而，许多最先进的文本排名方法仅关注核心相关性，而忽略了其他有助于用户满意度的维度，例如文档质量、时效性、权威性等。在这项工作中，我们关注的是在网络搜索中排名用户满意度，而不是相关性，并提出了一种基于PLM的框架，即SAT-Ranker，它以统一的方式全面建模用户满意度的不同维度。特别地，我们利用PLM在文本和数字输入方面的能力，并应用多字段输入，将每个用户满意度维度作为输入字段进行模块化。总体而言，SAT-Ranker是一个有效、可扩展和数据中心的框架，具有巨大的工业应用潜力。在严格的线下和线上实验中，SAT-Ranker表现优异，优于强基线，并在MSR-Web30K和TREC Web Track数据集上实现了最先进的结果。

    Search engine plays a crucial role in satisfying users' diverse information needs. Recently, Pretrained Language Models (PLMs) based text ranking models have achieved huge success in web search. However, many state-of-the-art text ranking approaches only focus on core relevance while ignoring other dimensions that contribute to user satisfaction, e.g., document quality, recency, authority, etc. In this work, we focus on ranking user satisfaction rather than relevance in web search, and propose a PLM-based framework, namely SAT-Ranker, which comprehensively models different dimensions of user satisfaction in a unified manner. In particular, we leverage the capacities of PLMs on both textual and numerical inputs, and apply a multi-field input that modularizes each dimension of user satisfaction as an input field. Overall, SAT-Ranker is an effective, extensible, and data-centric framework that has huge potential for industrial applications. On rigorous offline and online experiments, SAT-
    
[^3]: 在规模上考虑公平的影响最大化问题（扩展版）

    Influence Maximization with Fairness at Scale (Extended Version). (arXiv:2306.01587v1 [cs.SI])

    [http://arxiv.org/abs/2306.01587](http://arxiv.org/abs/2306.01587)

    本文针对影响最大化问题中的公平问题，提出了一种基于学习节点表示的方法来实现公平传播，从而实现可扩展的解决方案，适用于具有数百万或数十亿节点的网络。

    

    本文研究关于影响最大化问题中公平的问题，即在选择k个具有影响力的节点来最大化信息传播的同时，确保所选择节点对敏感用户属性的影响是公平的。最近的研究仅关注于极小的网络，因此关键问题在于如何实现可扩展的解决方案，适用于具有数百万或数十亿节点的网络。我们提出了一种基于学习节点表示的方法来实现公平传播，而不是依赖于社交连接，以便我们可以处理非常大的图形。我们提出了两种数据驱动方法：（a）基于公平的参与者抽样（FPS）和（b）公平作为上下文（FAC）。

    In this paper, we revisit the problem of influence maximization with fairness, which aims to select k influential nodes to maximise the spread of information in a network, while ensuring that selected sensitive user attributes are fairly affected, i.e., are proportionally similar between the original network and the affected users. Recent studies on this problem focused only on extremely small networks, hence the challenge remains on how to achieve a scalable solution, applicable to networks with millions or billions of nodes. We propose an approach that is based on learning node representations for fair spread from diffusion cascades, instead of the social connectivity s.t. we can deal with very large graphs. We propose two data-driven approaches: (a) fairness-based participant sampling (FPS), and (b) fairness as context (FAC). Spread related user features, such as the probability of diffusing information to others, are derived from the historical information cascades, using a deep ne
    
[^4]: 基于约束的危机管理模拟推荐系统

    Syst\`eme de recommandations bas\'e sur les contraintes pour les simulations de gestion de crise. (arXiv:2306.01504v1 [cs.IR])

    [http://arxiv.org/abs/2306.01504](http://arxiv.org/abs/2306.01504)

    本文提出使用基于约束的推荐系统来推荐驾驶员/车辆配对，在危机管理模拟中实现城市居民的自愿参与。

    

    在疏散人群的背景下，一些市民/志愿者可能希望并能够通过自己的车辆来帮助应急/疏散车辆疏散处于困境中的人群。本文提出了在危机管理模拟系统中增加推荐驾驶员/车辆配对模块的方法。为此，我们选择建模并开发一个基于本体支持的约束推荐系统，以用于危机管理模拟。

    In the context of the evacuation of populations, some citizens/volunteers may want and be able to participate in the evacuation of populations in difficulty by coming to lend a hand to emergency/evacuation vehicles with their own vehicles. One way of framing these impulses of solidarity would be to be able to list in real-time the citizens/volunteers available with their vehicles (land, sea, air, etc.), to be able to geolocate them according to the risk areas to be evacuated, and adding them to the evacuation/rescue vehicles. Because it is difficult to propose an effective real-time operational system on the field in a real crisis situation, in this work, we propose to add a module for recommending driver/vehicle pairs (with their specificities) to a system of crisis management simulation. To do that, we chose to model and develop an ontology-supported constraint-based recommender system for crisis management simulations.
    
[^5]: 用于建模用户新奇寻求意图的层次强化学习在推荐系统中的应用

    Hierarchical Reinforcement Learning for Modeling User Novelty-Seeking Intent in Recommender Systems. (arXiv:2306.01476v1 [cs.IR])

    [http://arxiv.org/abs/2306.01476](http://arxiv.org/abs/2306.01476)

    本文提出了一种基于层次强化学习的方法，用于建模用户的层次新奇寻求意图并调整推荐策略以提高推荐项目准确性和多样性。

    

    推荐新颖内容可以通过向用户介绍新的兴趣点来改善用户在推荐平台上的长期体验。然而，用户并不总是想要探索新颖的内容。因此，了解他们的寻求新奇的意图并相应地调整推荐策略是至关重要的。本文提出了一种基于层次强化学习的新方法，用于建模用户的层次新奇寻求意图，并根据提取的用户寻求新奇倾向性来调整推荐策略。我们发现，我们提出的方法在推荐项目的准确性和多样性方面优于现有方法。

    Recommending novel content, which expands user horizons by introducing them to new interests, has been shown to improve users' long-term experience on recommendation platforms \cite{chen2021values}. Users however are not constantly looking to explore novel content. It is therefore crucial to understand their novelty-seeking intent and adjust the recommendation policy accordingly. Most existing literature models a user's propensity to choose novel content or to prefer a more diverse set of recommendations at individual interactions. Hierarchical structure, on the other hand, exists in a user's novelty-seeking intent, which is manifested as a static and intrinsic user preference for seeking novelty along with a dynamic session-based propensity. To this end, we propose a novel hierarchical reinforcement learning-based method to model the hierarchical user novelty-seeking intent, and to adapt the recommendation policy accordingly based on the extracted user novelty-seeking propensity. We f
    
[^6]: 面向个性化方面提取的大型语言模型的提示调整用于推荐

    Prompt Tuning Large Language Models on Personalized Aspect Extraction for Recommendations. (arXiv:2306.01475v1 [cs.IR])

    [http://arxiv.org/abs/2306.01475](http://arxiv.org/abs/2306.01475)

    本文提出了一种端到端的方法，将面向方面的提取和推荐相结合，同时引入了个性化基于方面的推荐模型，并利用大型语言模型设计了新的模型来为最终的推荐任务生成方面。

    

    现有的方面提取方法主要依赖于显式或基础事实方面信息，或者使用数据挖掘或机器学习方法从隐含用户反馈（例如用户评论）中提取方面。然而，如何利用提取出的方面生成更有意义的推荐给用户仍然未被充分探索。同时，现有的基于方面的推荐研究通常依赖于单独的方面提取模型或假设方面是已知的，而没有考虑到最佳方面集可能取决于手头的推荐任务。在本文中，我们提出了一种将方面提取与基于方面的推荐结合起来的端到端方法，并在单个框架中同时实现这两个目标。对于方面提取组件，我们利用大型语言模型的最新进展，设计了一种新的提示学习机制来为最终的推荐任务生成方面。对于基于方面的推荐组件，我们引进了一种个性化的基于方面的推荐模型，该模型与方面提取模型一起进行训练。我们的实验证明，所提出的方法在方面提取和基于方面的推荐任务上都优于几种强基准模型。

    Existing aspect extraction methods mostly rely on explicit or ground truth aspect information, or using data mining or machine learning approaches to extract aspects from implicit user feedback such as user reviews. It however remains under-explored how the extracted aspects can help generate more meaningful recommendations to the users. Meanwhile, existing research on aspect-based recommendations often relies on separate aspect extraction models or assumes the aspects are given, without accounting for the fact the optimal set of aspects could be dependent on the recommendation task at hand.  In this work, we propose to combine aspect extraction together with aspect-based recommendations in an end-to-end manner, achieving the two goals together in a single framework. For the aspect extraction component, we leverage the recent advances in large language models and design a new prompt learning mechanism to generate aspects for the end recommendation task. For the aspect-based recommendat
    
[^7]: 基于OPC UA的工业大数据架构

    An OPC UA-based industrial Big Data architecture. (arXiv:2306.01418v1 [cs.IR])

    [http://arxiv.org/abs/2306.01418](http://arxiv.org/abs/2306.01418)

    本文提出了一种基于OPC UA的大数据架构，利用元数据和语义模型提高数据处理和查询效率，使多个处理池能够协同工作而不干扰现场设备和生产过程。

    

    工业4.0工厂很复杂，是基于数据驱动的。数据来自许多来源，包括传感器、PLC和其他设备，还有IT系统，如ERP或CRM系统。本文提出了一种新的基于查询模型的方法，使用大数据架构，使用OPC UA作为基础来捕获来自不同来源的数据。它缓冲并预处理信息，以实现工厂的整体状态空间的协调，并映射到生产现场的当前状态。该信息可以提供给多个处理池，与数据源分离，使其能够处理信息，而不会干扰生产设备、网络设备或对生产过程产生负面影响。元数据和语义模型用于使数据处理和查询更加高效和灵活。该架构在实际工业环境中得到实现和评估。

    Industry 4.0 factories are complex and data-driven. Data is yielded from many sources, including sensors, PLCs, and other devices, but also from IT, like ERP or CRM systems. We ask how to collect and process this data in a way, such that it includes metadata and can be used for industrial analytics or to derive intelligent support systems. This paper describes a new, query model based approach, which uses a big data architecture to capture data from various sources using OPC UA as a foundation. It buffers and preprocesses the information for the purpose of harmonizing and providing a holistic state space of a factory, as well as mappings to the current state of a production site. That information can be made available to multiple processing sinks, decoupled from the data sources, which enables them to work with the information without interfering with devices of the production, disturbing the network devices they are working in, or influencing the production process negatively. Metadat
    
[^8]: DWT-CompCNN：用于高吞吐量JPEG 2000压缩文档的深度图像分类网络

    DWT-CompCNN: Deep Image Classification Network for High Throughput JPEG 2000 Compressed Documents. (arXiv:2306.01359v1 [cs.CV])

    [http://arxiv.org/abs/2306.01359](http://arxiv.org/abs/2306.01359)

    这篇论文提出了一种名为DWT-CompCNN的深度学习模型，它可以直接对使用HTJ2K算法压缩的文档进行分类，从而提高计算效率。

    

    对于任何包含文档图像的数字应用程序，如检索，文档图像的分类成为必要的阶段。传统上，为了达到这个目的，文档的完整版本，即未压缩的文档图像构成输入数据集，这会因数据量大而带来威胁。因此，如果可以使用文档的压缩表示（在部分解压缩的情况下），直接完成相同的分类任务以使整个过程计算效率更高，那将会是一项创新。本研究提出了一种新颖的深度学习模型DWT-CompCNN，用于使用高吞吐量JPEG 2000（HTJ2K）算法压缩的文档的分类。所提出的DWT-CompCNN包括五个卷积层，卷积核大小分别为16、32、64、128和256用于从提取的小波系数中提高学习能力。

    For any digital application with document images such as retrieval, the classification of document images becomes an essential stage. Conventionally for the purpose, the full versions of the documents, that is the uncompressed document images make the input dataset, which poses a threat due to the big volume required to accommodate the full versions of the documents. Therefore, it would be novel, if the same classification task could be accomplished directly (with some partial decompression) with the compressed representation of documents in order to make the whole process computationally more efficient. In this research work, a novel deep learning model, DWT CompCNN is proposed for classification of documents that are compressed using High Throughput JPEG 2000 (HTJ2K) algorithm. The proposed DWT-CompCNN comprises of five convolutional layers with filter sizes of 16, 32, 64, 128, and 256 consecutively for each increasing layer to improve learning from the wavelet coefficients extracted
    
[^9]: 通过AUC-最优负采样降低推荐系统中的流行度偏差

    Reducing Popularity Bias in Recommender Systems through AUC-Optimal Negative Sampling. (arXiv:2306.01348v1 [cs.IR])

    [http://arxiv.org/abs/2306.01348](http://arxiv.org/abs/2306.01348)

    通过最优负采样从根本上缓解流行度偏差，提高模型的性能，同时保持准确性。

    

    流行度偏差是推荐系统中的一个长期问题，对公平性和效率都带来了挑战。现有文献普遍认为，减少流行度偏差通常需要牺牲推荐准确性。在本文中，我们对这种共识提出了质疑。我们在一个通用的偏差-方差分解框架下的分析表明，在某些条件下，减少偏差实际上可以提高模型性能。为了实现这种双赢的局面，我们提出通过负采样干预模型训练，从而修改模型预测，以此来缓解流行度偏差。具体而言，我们提供了一种最优负采样规则，以最大化局部AUC，从而保持任何给定模型的准确性，同时通过修正采样信息和先前信息的方式来灵活和有原则地减少流行度偏差。我们在真实世界数据集上的实验结果显示了我们方法在提高推荐性能和减少流行度偏差方面的卓越性。

    Popularity bias is a persistent issue associated with recommendation systems, posing challenges to both fairness and efficiency. Existing literature widely acknowledges that reducing popularity bias often requires sacrificing recommendation accuracy. In this paper, we challenge this commonly held belief. Our analysis under general bias-variance decomposition framework shows that reducing bias can actually lead to improved model performance under certain conditions. To achieve this win-win situation, we propose to intervene in model training through negative sampling thereby modifying model predictions. Specifically, we provide an optimal negative sampling rule that maximizes partial AUC to preserve the accuracy of any given model, while correcting sample information and prior information to reduce popularity bias in a flexible and principled way. Our experimental results on real-world datasets demonstrate the superiority of our approach in improving recommendation performance and reduc
    
[^10]: LyricSIM：一种用于检测西班牙歌词相似性的新数据集和基准

    LyricSIM: A novel Dataset and Benchmark for Similarity Detection in Spanish Song LyricS. (arXiv:2306.01325v1 [cs.CL])

    [http://arxiv.org/abs/2306.01325](http://arxiv.org/abs/2306.01325)

    本文介绍了一种针对语义相似性任务的新数据集和基准，由多个本地注释者完成集体注释实验，评估了各种最先进的单语和多语言语言模型的性能，建立了基准结果。

    

    本文介绍了一个新的数据集和基准，旨在针对歌词的语义相似性任务。我们的数据集最初由2775对西班牙歌曲组成，由63个本地注释者进行集体注释实验。经过收集和精细化处理数据以确保高度的共识和数据完整性，我们获得了676个高质量的标注对，用于评估各种最先进的单语和多语言语言模型的性能。因此，我们建立了基准结果，希望这些结果对该领域未来的学术和工业应用有所帮助。

    In this paper, we present a new dataset and benchmark tailored to the task of semantic similarity in song lyrics. Our dataset, originally consisting of 2775 pairs of Spanish songs, was annotated in a collective annotation experiment by 63 native annotators. After collecting and refining the data to ensure a high degree of consensus and data integrity, we obtained 676 high-quality annotated pairs that were used to evaluate the performance of various state-of-the-art monolingual and multilingual language models. Consequently, we established baseline results that we hope will be useful to the community in all future academic and industrial applications conducted in this context.
    
[^11]: JEPOO：音乐信息检索中准确估计音高、起始和终止的联合方法

    JEPOO: Highly Accurate Joint Estimation of Pitch, Onset and Offset for Music Information Retrieval. (arXiv:2306.01304v1 [cs.SD])

    [http://arxiv.org/abs/2306.01304](http://arxiv.org/abs/2306.01304)

    本文提出了一种名为JEPOO的音乐信息检索方法，能够准确估计音高、起始和终止，支持单音高和多音高数据，比现有最先进的方法精度提升高达10.6%，8.3%和10.3%。

    

    旋律提取是音乐信息检索中的核心任务，而音高、起始和终止的估计是旋律提取的关键子任务。现有方法的准确性有限，并且只适用于单音高或多音高数据中的一种类型。本文提出了一种名为JEPOO的高度准确的音高、起始和终止联合估计方法。我们通过新颖的模型设计和一种名为帕累托模调损失的优化技术，解决了联合学习优化和处理单音高和多音高数据的挑战。这是第一种能够准确处理单音高和多音高音乐数据，甚至混合类型数据的方法。在广泛的真实数据集上进行的全面实验研究表明，JEPOO在预测音高、起始和终止方面比最先进的方法分别高出10.6％、8.3％和10.3％，同时对于各种类型的数据和乐器具有鲁棒性。

    Melody extraction is a core task in music information retrieval, and the estimation of pitch, onset and offset are key sub-tasks in melody extraction. Existing methods have limited accuracy, and work for only one type of data, either single-pitch or multipitch. In this paper, we propose a highly accurate method for joint estimation of pitch, onset and offset, named JEPOO. We address the challenges of joint learning optimization and handling both single-pitch and multi-pitch data through novel model design and a new optimization technique named Pareto modulated loss with loss weight regularization. This is the first method that can accurately handle both single-pitch and multi-pitch music data, and even a mix of them. A comprehensive experimental study on a wide range of real datasets shows that JEPOO outperforms state-ofthe-art methods by up to 10.6%, 8.3% and 10.3% for the prediction of Pitch, Onset and Offset, respectively, and JEPOO is robust for various types of data and instrument
    
[^12]: 自对比学习用于基于会话的推荐

    Self Contrastive Learning for Session-based Recommendation. (arXiv:2306.01266v1 [cs.IR])

    [http://arxiv.org/abs/2306.01266](http://arxiv.org/abs/2306.01266)

    本文提出了自对比学习方法，简化了会话推荐领域基于对比学习的模型的复杂性，并提高了推荐性能。

    

    基于会话的推荐旨在预测用户对现有项目交互序列的下一个感兴趣的项目，已经吸引了越来越多应用使用对比学习（CL）提高用户和项目的表示。然而，这些对比目标：（1）起到与交叉熵损失类似的作用，同时忽略了项目表示空间优化；（2）通常需要复杂的建模，包括复杂的正/负样本构建和额外的数据增强。在本文中，我们引入了自对比学习（SCL），简化了对比学习的应用，并增强了基于状态的推荐技术的性能。具体而言，SCL被制定为一个目标函数，直接促进项目表示之间的均匀分布，并有效地替换了所有现有的对比目标组件的状态-艺术模型。与以前的工作不同，SCL消除了任何正样本或负样本的需求和SCL消除了任何正样本或负样本的需求和数据增强的需求。

    Session-based recommendation, which aims to predict the next item of users' interest as per an existing sequence interaction of items, has attracted growing applications of Contrastive Learning (CL) with improved user and item representations. However, these contrastive objectives: (1) serve a similar role as the cross-entropy loss while ignoring the item representation space optimisation; and (2) commonly require complicated modelling, including complex positive/negative sample constructions and extra data augmentation. In this work, we introduce Self-Contrastive Learning (SCL), which simplifies the application of CL and enhances the performance of state-of-the-art CL-based recommendation techniques. Specifically, SCL is formulated as an objective function that directly promotes a uniform distribution among item representations and efficiently replaces all the existing contrastive objective components of state-of-the-art models. Unlike previous works, SCL eliminates the need for any p
    
[^13]: 预训练的抽象模型和LLMs在法律案例判决摘要中的应用准备情况？

    How Ready are Pre-trained Abstractive Models and LLMs for Legal Case Judgement Summarization?. (arXiv:2306.01248v1 [cs.CL])

    [http://arxiv.org/abs/2306.01248](http://arxiv.org/abs/2306.01248)

    这篇论文探讨了是否可以使用预训练的抽象模型和大型语言模型来自动生成法律案例判决的摘要，并在印度的法庭案例判决中进行了相关实验分析。

    

    自动摘要法律案例判决一直是采用抽取式摘要方法尝试解决的问题。然而，近年来，具有生成更自然和连贯摘要能力的抽象摘要模型受到越来越多的关注。现在已经有了专门用于法律领域的预训练抽象摘要模型。此外，众所周知，如ChatGPT这样的通用领域预训练大型语言模型(LLMs)能够生成高质量的文本，并具有文本摘要的能力。因此，值得问的是，这些模型是否已准备好用于自动生成案例判决的抽象摘要。为了探讨这个问题，我们将几种最先进的领域特定的抽象性摘要模型和通用领域的LLMs应用于印度法庭案例判决中，并检查所生成摘要的质量。除了摘要质量的标准度量，我们还检查了生成的摘要中可能存在的不一致性和虚构现象。

    Automatic summarization of legal case judgements has traditionally been attempted by using extractive summarization methods. However, in recent years, abstractive summarization models are gaining popularity since they can generate more natural and coherent summaries. Legal domain-specific pre-trained abstractive summarization models are now available. Moreover, general-domain pre-trained Large Language Models (LLMs), such as ChatGPT, are known to generate high-quality text and have the capacity for text summarization. Hence it is natural to ask if these models are ready for off-the-shelf application to automatically generate abstractive summaries for case judgements. To explore this question, we apply several state-of-the-art domain-specific abstractive summarization models and general-domain LLMs on Indian court case judgements, and check the quality of the generated summaries. In addition to standard metrics for summary quality, we check for inconsistencies and hallucinations in the 
    
[^14]: TimelineQA: 一个用于时间线问答的基准测试

    TimelineQA: A Benchmark for Question Answering over Timelines. (arXiv:2306.01069v1 [cs.CL])

    [http://arxiv.org/abs/2306.01069](http://arxiv.org/abs/2306.01069)

    TimelineQA是一个用于查询生活日志的加速进展的基准测试，涉及时间和地理信息，已经公开发布，并使用两种最先进的模型进行了实验，但这两种模型均未达到人类表现水平。

    

    Lifelogs（生活日志）是人们生活经历的描述，这些日志通过结合来自多个数字服务（如在线照片、地图、购物和内容流媒体服务）的数据来创建。问答技术在生活日志上的应用可以为个人助手在提供上下文方面提供关键资源。但是，由于生活日志结合了自由文本与一定程度的结构，如时间和地理信息，因此当前的问答技术无法回答此类问题。为此，我们创建并公开发布了TimelineQA，这是一个用于查询生活日志的加速进展的基准测试。TimelineQA生成虚构人物的生活日志。生活日志中的事件从高中毕业等重大生活事件到日常活动如慢跑都有所覆盖。我们进行了一系列关于TimelineQA的实验，使用了两种最先进的模型，发现这两种模型在这一任务上均未达到人类表现水平。

    Lifelogs are descriptions of experiences that a person had during their life. Lifelogs are created by fusing data from the multitude of digital services, such as online photos, maps, shopping and content streaming services. Question answering over lifelogs can offer personal assistants a critical resource when they try to provide advice in context. However, obtaining answers to questions over lifelogs is beyond the current state of the art of question answering techniques for a variety of reasons, the most pronounced of which is that lifelogs combine free text with some degree of structure such as temporal and geographical information.  We create and publicly release TimelineQA1, a benchmark for accelerating progress on querying lifelogs. TimelineQA generates lifelogs of imaginary people. The episodes in the lifelog range from major life episodes such as high school graduation to those that occur on a daily basis such as going for a run. We describe a set of experiments on TimelineQA w
    
[^15]: 快速密集信息检索器利用KALE进行后置KL对齐的异形双编码器模型训练 (arXiv:2304.01016v2 [cs.CL] UPDATED)

    Quick Dense Retrievers Consume KALE: Post Training Kullback Leibler Alignment of Embeddings for Asymmetrical dual encoders. (arXiv:2304.01016v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2304.01016](http://arxiv.org/abs/2304.01016)

    本文提出了一种通过结构压缩和模型尺寸不对称的双编码器模型 KALE，有效提高密集信息检索的推理效率，同时允许查询编码器的有效压缩，而无需进行全部的再训练或索引生成，此方法能够生成超过DistilBERT性能的模型。

    

    本文提出了一种有结构压缩和模型尺寸不对称的双编码器模型，旨在提高基于语言模型的密集信息检索系统的推理速度。通过对MSMARCO、自然问答、问答游戏等多个数据集进行前后训练压缩实验，研究了压缩对系统推理效率的影响，结果表明密集信息检索器的双编码器结构异形化有助于提高其推理效率。基于此，我们引入了一种名为Kullback Leibler Alignment of Embeddings (KALE)的方法，通过裁剪和对齐查询编码器，提高了密集信息检索的推理效率。KALE扩展了传统的知识蒸馏方法，使得在双编码器训练后可以有效地对查询编码器进行压缩而无需进行完整的再训练或索引生成。使用KALE和不对称训练，我们可以生成超过DistilBERT性能的模型，同时模型尺寸更小。

    In this paper, we consider the problem of improving the inference latency of language model-based dense retrieval systems by introducing structural compression and model size asymmetry between the context and query encoders. First, we investigate the impact of pre and post-training compression on the MSMARCO, Natural Questions, TriviaQA, SQUAD, and SCIFACT, finding that asymmetry in the dual encoders in dense retrieval can lead to improved inference efficiency. Knowing this, we introduce Kullback Leibler Alignment of Embeddings (KALE), an efficient and accurate method for increasing the inference efficiency of dense retrieval methods by pruning and aligning the query encoder after training. Specifically, KALE extends traditional Knowledge Distillation after bi-encoder training, allowing for effective query encoder compression without full retraining or index generation. Using KALE and asymmetric training, we can generate models which exceed the performance of DistilBERT despite having 
    
[^16]: PrefRec：利用人类偏好加强长期用户参与的推荐系统

    PrefRec: Recommender Systems with Human Preferences for Reinforcing Long-term User Engagement. (arXiv:2212.02779v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2212.02779](http://arxiv.org/abs/2212.02779)

    该论文提出了一种基于人类偏好的推荐系统范式PrefRec，允许强化学习推荐系统从用户历史行为偏好中学习，以优化长期用户参与度。

    

    目前，推荐系统在优化即时参与方面取得了令人瞩目的成功。然而，更可取的绩效指标长期用户参与度的提高仍然很难。与此同时，最近的强化学习算法在各种长期目标优化任务中展现了有效性。因此，强化学习被广泛认为是优化推荐中长期用户参与度的有前途的框架。虽然有前途，但应用强化学习在很大程度上依赖于精心设计的奖励，但设计与长期用户参与有关的奖励相当困难。为了缓解这个问题，我们提出了一种新的范式，即以人类偏好为基础的推荐系统，允许强化学习推荐系统从有关用户历史行为的偏好中学习，而不是从明确定义的奖励中学习。这些偏好可以通过众包等技术轻松获得。

    Current advances in recommender systems have been remarkably successful in optimizing immediate engagement. However, long-term user engagement, a more desirable performance metric, remains difficult to improve. Meanwhile, recent reinforcement learning (RL) algorithms have shown their effectiveness in a variety of long-term goal optimization tasks. For this reason, RL is widely considered as a promising framework for optimizing long-term user engagement in recommendation. Though promising, the application of RL heavily relies on well-designed rewards, but designing rewards related to long-term user engagement is quite difficult. To mitigate the problem, we propose a novel paradigm, recommender systems with human preferences (or Preference-based Recommender systems), which allows RL recommender systems to learn from preferences about users historical behaviors rather than explicitly defined rewards. Such preferences are easily accessible through techniques such as crowdsourcing, as they 
    
[^17]: 自监督学习在推荐系统中的应用：综述

    Self-Supervised Learning for Recommender Systems: A Survey. (arXiv:2203.15876v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2203.15876](http://arxiv.org/abs/2203.15876)

    本文综述了自监督推荐（SSR）的研究进展，提出了四种方法：对比、生成、预测和混合，并介绍了各种方法的优缺点及研究方向。

    

    近年来，基于神经网络的推荐系统取得了巨大的成功，但在处理高度稀疏的数据时仍不尽人意。自监督学习作为一种从无标签数据中学习的新兴技术，已经引起了广泛关注，被看作是解决这个问题的潜在方法。本文综述了自监督推荐的研究工作，提出了自监督推荐的定义和分类方法，并详细介绍了各种方法的优劣以及将来的研究方向。

    In recent years, neural architecture-based recommender systems have achieved tremendous success, but they still fall short of expectation when dealing with highly sparse data. Self-supervised learning (SSL), as an emerging technique for learning from unlabeled data, has attracted considerable attention as a potential solution to this issue. This survey paper presents a systematic and timely review of research efforts on self-supervised recommendation (SSR). Specifically, we propose an exclusive definition of SSR, on top of which we develop a comprehensive taxonomy to divide existing SSR methods into four categories: contrastive, generative, predictive, and hybrid. For each category, we elucidate its concept and formulation, the involved methods, as well as its pros and cons. Furthermore, to facilitate empirical comparison, we release an open-source library SELFRec (https://github.com/Coder-Yu/SELFRec), which incorporates a wide range of SSR models and benchmark datasets. Through rigoro
    
[^18]: 基于情感和句子类型对YouTube评论进行分类

    Classifying YouTube Comments Based on Sentiment and Type of Sentence. (arXiv:2111.01908v1 [cs.IR] CROSS LISTED)

    [http://arxiv.org/abs/2111.01908](http://arxiv.org/abs/2111.01908)

    本论文提出了一种基于情感和句子类型的方法，将原始的YouTube评论分类，以帮助YouTuber找到更相关的评论，从而增加其观众群。

    

    随着YouTube频道的增长，每个视频都可能收集大量评论，这些评论是理解观众期望和改善频道参与度的主要手段。然而，这些评论只代表用户关于频道和内容的一般观点。许多评论构造较差，琐碎并且存在拼写和语法错误，因此，识别最能吸引内容创作者的评论是一项繁琐的工作。本文旨在通过情感和句子类型将原始评论分类，以帮助YouTuber找到更相关的评论，从而增加其观众群。

    As a YouTube channel grows, each video can potentially collect enormous amounts of comments that provide direct feedback from the viewers. These comments are a major means of understanding viewer expectations and improving channel engagement. However, the comments only represent a general collection of user opinions about the channel and the content. Many comments are poorly constructed, trivial, and have improper spellings and grammatical errors. As a result, it is a tedious job to identify the comments that best interest the content creators. In this paper, we extract and classify the raw comments into different categories based on both sentiment and sentence types that will help YouTubers find relevant comments for growing their viewership. Existing studies have focused either on sentiment analysis (positive and negative) or classification of sub-types within the same sentence types (e.g., types of questions) on a text corpus. These have limited application on non-traditional text c
    

