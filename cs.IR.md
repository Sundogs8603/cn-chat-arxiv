# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Extracting Blockchain Concepts from Text.](http://arxiv.org/abs/2305.10408) | 本研究旨在通过机器学习模型提取区块链领域的信息并组织，以帮助用户浏览该领域。 |
| [^2] | [Large-Scale Text Analysis Using Generative Language Models: A Case Study in Discovering Public Value Expressions in AI Patents.](http://arxiv.org/abs/2305.10383) | 本文研究使用生成语言模型GPT-4进行大规模文本分析，在US AI专利中发现公共价值表达。采用高级布尔查询收集了154,934个专利文档，并与USPTO的完整专利文本合并。得出5.4百万句子的语料库，使用框架以及GPT-4提示进行标记和理性化。评估结果表明，这种方法很准确。 |
| [^3] | [Knowledge-enhanced Mixed-initiative Dialogue System for Emotional Support Conversations.](http://arxiv.org/abs/2305.10172) | 本文针对情感支持对话系统的混合主动特点，提出了基于知识增强的混合主动对话框架，该框架从大型心理健康知识图谱中检索实际案例知识来生成混合主动响应，并在共情和问题解决能力方面显著优于几个基线模型。 |
| [^4] | [Unconfounded Propensity Estimation for Unbiased Ranking.](http://arxiv.org/abs/2305.09918) | 该论文提出了一种新的算法PropensityNet，用于在强日志记录策略下进行无偏学习排名（ULTR）的倾向性估计，优于现有的最先进ULTR算法。 |
| [^5] | [Semantic Similarity Measure of Natural Language Text through Machine Learning and a Keyword-Aware Cross-Encoder-Ranking Summarizer -- A Case Study Using UCGIS GIS&T Body of Knowledge.](http://arxiv.org/abs/2305.09877) | 本文提出了一种新方法，采用机器学习模型和关键词感知交叉编码器排序摘要程序，从文本内容中提取语义信息，并度量 GIS&T BoK 话题之间的语义相似度，以解决手动定义话题关系带来的不完整评估问题。该方法在准确度量话题关系方面表现良好，对 GIS&T 领域的研究和实践具有重要意义。 |
| [^6] | [Knowledge Graph Completion Models are Few-shot Learners: An Empirical Study of Relation Labeling in E-commerce with LLMs.](http://arxiv.org/abs/2305.09858) | 本文通过对知识图谱中关系标注的实证研究，发现大型语言模型具有强大的学习能力以及在少量标记数据下预测产品类型之间关系的有效性。 |
| [^7] | [Data Bias Management.](http://arxiv.org/abs/2305.09686) | 本文讲述了数据偏差在机器学习中的应用、影响及可能的解决方案 |
| [^8] | [Robust and lightweight audio fingerprint for Automatic Content Recognition.](http://arxiv.org/abs/2305.09559) | 本文提出了一种鲁棒且轻量级的音频指纹识别系统，可用于自动内容识别，具有高度可扩展性和低功耗，并在专有ACR数据集精度、检索速度、内存使用和鲁棒性等所有评估指标上均显著优于基于最小哈希的音频指纹。 |
| [^9] | [Life of PII -- A PII Obfuscation Transformer.](http://arxiv.org/abs/2305.09550) | “Life of PII”是一种新颖的混淆变换器框架，用于将PII转化为人造PII同时尽可能地保留原始信息、意图和上下文，使我们能够有选择地混淆文档中的敏感信息，同时保留文档的统计和语义特性。 |
| [^10] | [Manipulating Visually-aware Federated Recommender Systems and Its Countermeasures.](http://arxiv.org/abs/2305.08183) | 本文研究了可视化信息对联邦推荐系统的影响，发现当加入视觉信息时，现有的恶意推广攻击将变得无效。 |
| [^11] | [Leveraging Large Language Models in Conversational Recommender Systems.](http://arxiv.org/abs/2305.07961) | 本文提出了一种使用大型语言模型构建端到端大规模对话推荐系统的路线图，解决在该系统中有效利用大型语言模型所面临的技术挑战。 |
| [^12] | [Frequency Enhanced Hybrid Attention Network for Sequential Recommendation.](http://arxiv.org/abs/2304.09184) | 本文提出了一种新的序列推荐算法——频率增强的混合注意力网络（FEARec）。该算法通过一个斜坡结构将原有的自注意力从时间域转换到频率域，使得低频和高频信息可以被明确地学习。同时，通过自相关的设计，该算法还可以捕捉到用户行为的固有周期性。在四个真实世界的基准测试中，该算法表现优于现有最先进的方法。 |
| [^13] | [Meta-optimized Contrastive Learning for Sequential Recommendation.](http://arxiv.org/abs/2304.07763) | 本文提出了 MCLRec 模型，该模型在数据增强和可学习模型增强操作的基础上，解决了现有对比学习方法难以推广和训练数据不足的问题。 |
| [^14] | [Variance Tolerance Factors For Interpreting ALL Neural Networks.](http://arxiv.org/abs/2209.13858) | 本文提出一种用于解释黑盒神经网络的方差容忍因子（VTF）理论，通过排名特征的方式探索特征的重要性，同时构建一个基本模型和特征模型的新型架构，来探索所有表现良好的神经网络中特征的重要性，并且经过基准测试和应用于实际环境中的实验验证了方法的可靠性。 |
| [^15] | [(Un)likelihood Training for Interpretable Embedding.](http://arxiv.org/abs/2207.00282) | 该论文提出了两种新的训练方法：可能性训练和不可能性训练，以解释嵌入向量背后的语义并解决标签稀疏问题。这些方法在图像和视频分类、检索和生成任务中表现出色，提高了学习嵌入的可解释性。 |

# 详细

[^1]: 从文本中提取区块链概念

    Extracting Blockchain Concepts from Text. (arXiv:2305.10408v1 [cs.IR])

    [http://arxiv.org/abs/2305.10408](http://arxiv.org/abs/2305.10408)

    本研究旨在通过机器学习模型提取区块链领域的信息并组织，以帮助用户浏览该领域。

    

    区块链提供了一种机制，通过该机制，相互不信任的远程方可以就信息分类账的状态达成共识。随着这个领域的快速发展，需要学习区块链的人也越来越多。由于这是一个技术性的主题，开始学习可能会感到相当不可思议。因此，该项目的主要目标是应用机器学习模型从白皮书和学术论文中提取关于区块链领域的信息，以组织这些信息并帮助用户浏览该领域。

    Blockchains provide a mechanism through which mutually distrustful remote parties can reach consensus on the state of a ledger of information. With the great acceleration with which this space is developed, the demand for those seeking to learn about blockchain also grows. Being a technical subject, it can be quite intimidating to start learning. For this reason, the main objective of this project was to apply machine learning models to extract information from whitepapers and academic articles focused on the blockchain area to organize this information and aid users to navigate the space.
    
[^2]: 使用生成语言模型进行大规模文本分析：在AI专利中发现公共价值表达的案例研究

    Large-Scale Text Analysis Using Generative Language Models: A Case Study in Discovering Public Value Expressions in AI Patents. (arXiv:2305.10383v1 [cs.CL])

    [http://arxiv.org/abs/2305.10383](http://arxiv.org/abs/2305.10383)

    本文研究使用生成语言模型GPT-4进行大规模文本分析，在US AI专利中发现公共价值表达。采用高级布尔查询收集了154,934个专利文档，并与USPTO的完整专利文本合并。得出5.4百万句子的语料库，使用框架以及GPT-4提示进行标记和理性化。评估结果表明，这种方法很准确。

    

    标记数据对于训练文本分类器至关重要，但对于复杂和抽象的概念而言，准确标记常常很难实现。本文采用一种新颖方法，使用生成语言模型（GPT-4）进行大规模文本分析的标记和理性化。我们将这种方法应用于在美国AI专利中发现公共价值表达的任务上。我们使用在InnovationQ+上提交的高级布尔查询收集了一个包含154,934个专利文档的数据库，这些结果与来自USPTO的完整专利文本合并，总计5.4百万句子。我们设计了一个框架来识别和标记这些AI专利句子中的公共价值表达。我们开发了GPT-4的提示，其中包括文本分类的定义、指导方针、示例和理性化。我们使用BLEU分数和主题建模评估了GPT-4生成的标签和理性化的质量，并发现它们是准确的。

    Labeling data is essential for training text classifiers but is often difficult to accomplish accurately, especially for complex and abstract concepts. Seeking an improved method, this paper employs a novel approach using a generative language model (GPT-4) to produce labels and rationales for large-scale text analysis. We apply this approach to the task of discovering public value expressions in US AI patents. We collect a database comprising 154,934 patent documents using an advanced Boolean query submitted to InnovationQ+. The results are merged with full patent text from the USPTO, resulting in 5.4 million sentences. We design a framework for identifying and labeling public value expressions in these AI patent sentences. A prompt for GPT-4 is developed which includes definitions, guidelines, examples, and rationales for text classification. We evaluate the quality of the labels and rationales produced by GPT-4 using BLEU scores and topic modeling and find that they are accurate, di
    
[^3]: 基于知识增强的混合主动对话情感支持系统

    Knowledge-enhanced Mixed-initiative Dialogue System for Emotional Support Conversations. (arXiv:2305.10172v1 [cs.CL])

    [http://arxiv.org/abs/2305.10172](http://arxiv.org/abs/2305.10172)

    本文针对情感支持对话系统的混合主动特点，提出了基于知识增强的混合主动对话框架，该框架从大型心理健康知识图谱中检索实际案例知识来生成混合主动响应，并在共情和问题解决能力方面显著优于几个基线模型。

    

    与共情对话不同，情感支持对话系统需要在安慰求助者的同时主动帮助探索和解决问题。本文研究了混合主动情感支持对话的问题，其中用户和系统都可以在对话中采取主动。我们提出了一个用于评估混合主动情感支持对话的新型模式，并提出了四个情感支持指标来评价混合主动交互。分析揭示了构建混合主动情感支持对话系统的必要性和挑战。在此基础上，我们提出了基于知识增强的混合主动对话框架（KEMI），该框架从大型心理健康知识图谱中检索实际案例知识来生成混合主动响应。实验证明，KEMI在共情和问题解决能力方面显著优于几个基线模型。

    Unlike empathetic dialogues, the system in emotional support conversations (ESC) is expected to not only convey empathy for comforting the help-seeker, but also proactively assist in exploring and addressing their problems during the conversation. In this work, we study the problem of mixed-initiative ESC where the user and system can both take the initiative in leading the conversation. Specifically, we conduct a novel analysis on mixed-initiative ESC systems with a tailor-designed schema that divides utterances into different types with speaker roles and initiative types. Four emotional support metrics are proposed to evaluate the mixed-initiative interactions. The analysis reveals the necessity and challenges of building mixed-initiative ESC systems. In the light of this, we propose a knowledge-enhanced mixed-initiative framework (KEMI) for ESC, which retrieves actual case knowledge from a large-scale mental health knowledge graph for generating mixed-initiative responses. Experimen
    
[^4]: 无偏倾向估计用于无偏排序

    Unconfounded Propensity Estimation for Unbiased Ranking. (arXiv:2305.09918v1 [cs.IR])

    [http://arxiv.org/abs/2305.09918](http://arxiv.org/abs/2305.09918)

    该论文提出了一种新的算法PropensityNet，用于在强日志记录策略下进行无偏学习排名（ULTR）的倾向性估计，优于现有的最先进ULTR算法。

    

    无偏学习排名（ULTR）的目标是利用隐含的用户反馈来优化学习排序系统。在现有解决方案中，自动ULTR算法在实践中因其卓越的性能和低部署成本而受到关注，该算法同时学习用户偏差模型（即倾向性模型）和无偏排名器。尽管该算法在理论上是可靠的，但其有效性通常在弱日志记录策略下进行验证，其中排名模型几乎无法根据与查询相关性来对文档进行排名。然而，当日志记录策略很强时，例如工业部署的排名策略，所报告的有效性无法再现。在本文中，我们首先从因果角度调查ULTR，并揭示一个负面结果：现有的ULTR算法未能解决由查询-文档相关性混淆导致的倾向性高估问题。然后，我们提出了一种基于反门调整的新的学习目标，并提出了一种名为PropensityNet的算法，用于在强日志记录策略下为ULTR估计无偏的倾向性分数。多个数据集的实证结果表明，PropensityNet在强日志记录策略和弱日志记录策略下均优于现有的最先进的ULTR算法。

    The goal of unbiased learning to rank~(ULTR) is to leverage implicit user feedback for optimizing learning-to-rank systems. Among existing solutions, automatic ULTR algorithms that jointly learn user bias models (\ie propensity models) with unbiased rankers have received a lot of attention due to their superior performance and low deployment cost in practice. Despite their theoretical soundness, the effectiveness is usually justified under a weak logging policy, where the ranking model can barely rank documents according to their relevance to the query. However, when the logging policy is strong, e.g., an industry-deployed ranking policy, the reported effectiveness cannot be reproduced. In this paper, we first investigate ULTR from a causal perspective and uncover a negative result: existing ULTR algorithms fail to address the issue of propensity overestimation caused by the query-document relevance confounder. Then, we propose a new learning objective based on backdoor adjustment and 
    
[^5]: 基于机器学习和关键词感知交叉编码器排序摘要程序的自然语言文本语义相似度度量——以UCGIS GIS&T知识体系为案例研究

    Semantic Similarity Measure of Natural Language Text through Machine Learning and a Keyword-Aware Cross-Encoder-Ranking Summarizer -- A Case Study Using UCGIS GIS&T Body of Knowledge. (arXiv:2305.09877v1 [cs.CL])

    [http://arxiv.org/abs/2305.09877](http://arxiv.org/abs/2305.09877)

    本文提出了一种新方法，采用机器学习模型和关键词感知交叉编码器排序摘要程序，从文本内容中提取语义信息，并度量 GIS&T BoK 话题之间的语义相似度，以解决手动定义话题关系带来的不完整评估问题。该方法在准确度量话题关系方面表现良好，对 GIS&T 领域的研究和实践具有重要意义。

    

    GIS&T 知识体系是由地理信息科学与技术相关团体发起的一个社区项目，旨在定义、开发和记录地理信息科学与技术相关话题。本文提出了一种新方法，采用机器学习模型和关键词感知交叉编码器排序摘要程序，从文本内容中提取语义信息，并度量 BoK 话题之间的语义相似度。结果表明，我们的方法在识别 BoK 话题之间的语义相似度方面优于其他 NLP 技术。该方法能够自动且准确地度量话题之间的关系，从而使 GIS&T 领域的研究人员和实践者受益。

    Initiated by the University Consortium of Geographic Information Science (UCGIS), GIS&T Body of Knowledge (BoK) is a community-driven endeavor to define, develop, and document geospatial topics related to geographic information science and technologies (GIS&T). In recent years, GIS&T BoK has undergone rigorous development in terms of its topic re-organization and content updating, resulting in a new digital version of the project. While the BoK topics provide useful materials for researchers and students to learn about GIS, the semantic relationships among the topics, such as semantic similarity, should also be identified so that a better and automated topic navigation can be achieved. Currently, the related topics are either defined manually by editors or authors, which may result in an incomplete assessment of topic relationship. To address this challenge, our research evaluates the effectiveness of multiple natural language processing (NLP) techniques in extracting semantics from te
    
[^6]: 知识图谱补全模型是少样本学习者：以 LLMS 在电商中的关系标注为例的经验研究

    Knowledge Graph Completion Models are Few-shot Learners: An Empirical Study of Relation Labeling in E-commerce with LLMs. (arXiv:2305.09858v1 [cs.IR])

    [http://arxiv.org/abs/2305.09858](http://arxiv.org/abs/2305.09858)

    本文通过对知识图谱中关系标注的实证研究，发现大型语言模型具有强大的学习能力以及在少量标记数据下预测产品类型之间关系的有效性。

    

    知识图谱在增强电子商务系统性能方面发挥着至关重要的作用，提供了关于实体及其关系的结构化信息，例如产品或产品类型之间的互补或替代关系，这些信息可以在推荐系统中利用。然而，由于电子商务领域的动态性和人力成本相关的原因，知识图谱中的关系标注仍然是一个具有挑战性的任务。最近，大型语言模型（LLM）的突破在许多自然语言处理任务中展示了出乎意料的结果。在本文中，我们进行了一个关于 LLM 在电子商务知识图谱中进行关系标注的实证研究，研究它们在自然语言方面强大的学习能力以及在有限标记数据下预测产品类型之间关系的有效性。我们评估了各种 LLM，包括 PaLM 和 GPT-3.5，在基准数据集上，证明它们能够达到与人类相当的关系性能水平。

    Knowledge Graphs (KGs) play a crucial role in enhancing e-commerce system performance by providing structured information about entities and their relationships, such as complementary or substitutable relations between products or product types, which can be utilized in recommender systems. However, relation labeling in KGs remains a challenging task due to the dynamic nature of e-commerce domains and the associated cost of human labor. Recently, breakthroughs in Large Language Models (LLMs) have shown surprising results in numerous natural language processing tasks. In this paper, we conduct an empirical study of LLMs for relation labeling in e-commerce KGs, investigating their powerful learning capabilities in natural language and effectiveness in predicting relations between product types with limited labeled data. We evaluate various LLMs, including PaLM and GPT-3.5, on benchmark datasets, demonstrating their ability to achieve competitive performance compared to humans on relation
    
[^7]: 数据偏差管理

    Data Bias Management. (arXiv:2305.09686v1 [cs.LG])

    [http://arxiv.org/abs/2305.09686](http://arxiv.org/abs/2305.09686)

    本文讲述了数据偏差在机器学习中的应用、影响及可能的解决方案

    

    鉴于数据驱动系统在我们日常生活中的广泛应用，偏差和公平等概念在科研人员和从业人员，无论是在产业界还是学术界中，都受到了重视。这些问题通常源于用于训练机器学习系统的数据质量不同。随着这些系统被商业化和部署，有时被委托做出改变生活的决策，人们正在做出重大努力来确定和消除可能导致数据偏差的来源。本文提供了研究结果，展示数据偏见如何影响最终用户，偏差的起源以及我们应该如何解决该问题。我们认为，不必在所有情况下消除数据偏差，而是应将研究重点转向偏见的识别。

    Due to the widespread use of data-powered systems in our everyday lives, concepts like bias and fairness gained significant attention among researchers and practitioners, in both industry and academia. Such issues typically emerge from the data, which comes with varying levels of quality, used to train supervised machine learning systems. With the commercialization and deployment of such systems that are sometimes delegated to make life-changing decisions, significant efforts are being made towards the identification and removal of possible sources of data bias that may resurface to the final end user or in the decisions being made. In this paper, we present research results that show how bias in data affects end users, where bias is originated, and provide a viewpoint about what we should do about it. We argue that data bias is not something that should necessarily be removed in all cases, and that research attention should instead shift from bias removal towards the identification, m
    
[^8]: 自动内容识别中的鲁棒且轻量级音频指纹识别系统

    Robust and lightweight audio fingerprint for Automatic Content Recognition. (arXiv:2305.09559v1 [cs.SD])

    [http://arxiv.org/abs/2305.09559](http://arxiv.org/abs/2305.09559)

    本文提出了一种鲁棒且轻量级的音频指纹识别系统，可用于自动内容识别，具有高度可扩展性和低功耗，并在专有ACR数据集精度、检索速度、内存使用和鲁棒性等所有评估指标上均显著优于基于最小哈希的音频指纹。

    

    本研究提出了一种新的音频指纹识别系统，用于自动内容识别（ACR）。通过使用信号处理技术和统计变换，我们的方法生成音频片段的紧凑指纹，这些指纹对现实世界中存在的噪声降级具有鲁棒性。该系统具有高度可扩展性，能够使用来自数百万台电视的指纹识别数千小时的内容。指纹的高时间相关性和利用现有的GPU兼容的近似最近邻（ANN）搜索算法使这一点成为可能。此外，指纹生成可以在计算受限的低功耗设备上运行，使其可以被广泛应用。

    This research paper presents a novel audio fingerprinting system for Automatic Content Recognition (ACR). By using signal processing techniques and statistical transformations, our proposed method generates compact fingerprints of audio segments that are robust to noise degradations present in real-world audio. The system is designed to be highly scalable, with the ability to identify thousands of hours of content using fingerprints generated from millions of TVs. The fingerprint's high temporal correlation and utilization of existing GPU-compatible Approximate Nearest Neighbour (ANN) search algorithms make this possible. Furthermore, the fingerprint generation can run on low-power devices with limited compute, making it accessible to a wide range of applications. Experimental results show improvements in our proposed system compared to a min-hash based audio fingerprint on all evaluated metrics, including accuracy on proprietary ACR datasets, retrieval speed, memory usage, and robustn
    
[^9]: PII的生命--一种PII混淆变换器

    Life of PII -- A PII Obfuscation Transformer. (arXiv:2305.09550v1 [cs.CL])

    [http://arxiv.org/abs/2305.09550](http://arxiv.org/abs/2305.09550)

    “Life of PII”是一种新颖的混淆变换器框架，用于将PII转化为人造PII同时尽可能地保留原始信息、意图和上下文，使我们能够有选择地混淆文档中的敏感信息，同时保留文档的统计和语义特性。

    

    在当今大型语言模型和数据驱动服务的世界中，保护敏感信息至关重要。一种常见的方法是使用数据扰动技术来减少(敏感)个人身份识别信息(PII)数据的过度实用性，同时保持其统计和语义特性。数据扰动方法经常导致显着的信息损失，使它们难以使用。在本文中，我们提出了“PII的生命”--一种新颖的混淆变换器框架，用于将PII转化为人造PII同时尽可能地保留原始信息、意图和上下文。我们的方法包括一个API来与给定的文档进行接口，一个基于配置的混淆器和一个基于Transformer架构的模型，在自然语言处理任务和LLMs中表现出高的上下文保存性能。我们的基于Transformer的方法学习了原始PII和其转换后的人造PII对应的映射，使我们能够有选择地混淆文档中的敏感信息，同时保留文档的统计和语义特性。

    Protecting sensitive information is crucial in today's world of Large Language Models (LLMs) and data-driven services. One common method used to preserve privacy is by using data perturbation techniques to reduce overreaching utility of (sensitive) Personal Identifiable Information (PII) data while maintaining its statistical and semantic properties. Data perturbation methods often result in significant information loss, making them impractical for use. In this paper, we propose 'Life of PII', a novel Obfuscation Transformer framework for transforming PII into faux-PII while preserving the original information, intent, and context as much as possible. Our approach includes an API to interface with the given document, a configuration-based obfuscator, and a model based on the Transformer architecture, which has shown high context preservation and performance in natural language processing tasks and LLMs.  Our Transformer-based approach learns mapping between the original PII and its tra
    
[^10]: 可视化信息对联邦推荐系统的影响及其对策

    Manipulating Visually-aware Federated Recommender Systems and Its Countermeasures. (arXiv:2305.08183v1 [cs.IR])

    [http://arxiv.org/abs/2305.08183](http://arxiv.org/abs/2305.08183)

    本文研究了可视化信息对联邦推荐系统的影响，发现当加入视觉信息时，现有的恶意推广攻击将变得无效。

    

    近年来，联邦推荐系统（FedRec）因其保护用户数据隐私的能力而受到广泛关注。在FedRec中，中央服务器通过与客户端共享模型公共参数来协同学习推荐模型，从而提供一种保护隐私的解决方案。然而，模型参数的公开性为攻击者操纵FedRec留下了后门。现有的与FedRec安全相关的研究已经表明，通过模型污染攻击，恶意用户可以轻易地推广项目，但是它们主要集中于只具有协作信息（即用户-项目交互）的FedRec。我们认为这些攻击之所以有效，是因为协作信号的数据稀疏性。在实践中，辅助信息（如产品的视觉描述）用于缓解协作过滤数据的稀疏性。因此，当在FedRec中加入视觉信息时，所有现有的模型污染攻击的有效性都将降低。

    Federated recommender systems (FedRecs) have been widely explored recently due to their ability to protect user data privacy. In FedRecs, a central server collaboratively learns recommendation models by sharing model public parameters with clients, thereby offering a privacy-preserving solution. Unfortunately, the exposure of model parameters leaves a backdoor for adversaries to manipulate FedRecs. Existing works about FedRec security already reveal that items can easily be promoted by malicious users via model poisoning attacks, but all of them mainly focus on FedRecs with only collaborative information (i.e., user-item interactions). We argue that these attacks are effective because of the data sparsity of collaborative signals. In practice, auxiliary information, such as products' visual descriptions, is used to alleviate collaborative filtering data's sparsity. Therefore, when incorporating visual information in FedRecs, all existing model poisoning attacks' effectiveness becomes q
    
[^11]: 在对话推荐系统中利用大型语言模型

    Leveraging Large Language Models in Conversational Recommender Systems. (arXiv:2305.07961v1 [cs.IR])

    [http://arxiv.org/abs/2305.07961](http://arxiv.org/abs/2305.07961)

    本文提出了一种使用大型语言模型构建端到端大规模对话推荐系统的路线图，解决在该系统中有效利用大型语言模型所面临的技术挑战。

    

    对话推荐系统通过启用实时的多轮对话使用户更加透明和掌控。最近，大型语言模型展现了与人类对话自然的能力，并将世界知识和常识推理融入到语言理解中，进一步释放了这一范式的潜力。然而，在对话推荐系统中有效利用大型语言模型引入了新的技术挑战，包括适当地理解和控制复杂的对话和从外部信息源检索。由于大而不断增长的项目语料库和缺乏对话数据进行训练，这些问题加剧了。在本文中，我们提供了使用大型语言模型构建端到端大规模对话推荐系统的路线图。特别地，我们提出了用户偏好理解、灵活的对话管理和可解释的推荐作为整个系统的一部分的新实现方式。

    A Conversational Recommender System (CRS) offers increased transparency and control to users by enabling them to engage with the system through a real-time multi-turn dialogue. Recently, Large Language Models (LLMs) have exhibited an unprecedented ability to converse naturally and incorporate world knowledge and common-sense reasoning into language understanding, unlocking the potential of this paradigm. However, effectively leveraging LLMs within a CRS introduces new technical challenges, including properly understanding and controlling a complex conversation and retrieving from external sources of information. These issues are exacerbated by a large, evolving item corpus and a lack of conversational data for training. In this paper, we provide a roadmap for building an end-to-end large-scale CRS using LLMs. In particular, we propose new implementations for user preference understanding, flexible dialogue management and explainable recommendations as part of an integrated architecture
    
[^12]: 频率增强的混合注意力网络用于序列推荐

    Frequency Enhanced Hybrid Attention Network for Sequential Recommendation. (arXiv:2304.09184v1 [cs.IR])

    [http://arxiv.org/abs/2304.09184](http://arxiv.org/abs/2304.09184)

    本文提出了一种新的序列推荐算法——频率增强的混合注意力网络（FEARec）。该算法通过一个斜坡结构将原有的自注意力从时间域转换到频率域，使得低频和高频信息可以被明确地学习。同时，通过自相关的设计，该算法还可以捕捉到用户行为的固有周期性。在四个真实世界的基准测试中，该算法表现优于现有最先进的方法。

    

    自注意机制是序列推荐领域广泛使用的技术之一，具有建模长程依赖性的强大能力。然而，许多最近的研究表明，目前基于自注意力的模型是低通滤波器，不能捕捉高频信息。此外，由于用户行为中的项目相互交织，这些模型无法区分时间域中模糊的固有周期性。在这项工作中，我们将视角转移到频率域，并提出了一种新颖的序列推荐算法——频率增强的混合注意力网络，即FEARec。在这个模型中，我们首先通过一个斜坡结构将原始时间域自注意力改进到频率域中，使得我们的方法可以明确地学习低频和高频信息。此外，我们还通过自相关设计了类似的注意力机制来捕捉用户行为的固有周期性。在四个真实世界的基准测试中进行了大量实验，结果表明我们提出的方法比现有最先进的方法表现更优秀。

    The self-attention mechanism, which equips with a strong capability of modeling long-range dependencies, is one of the extensively used techniques in the sequential recommendation field. However, many recent studies represent that current self-attention based models are low-pass filters and are inadequate to capture high-frequency information. Furthermore, since the items in the user behaviors are intertwined with each other, these models are incomplete to distinguish the inherent periodicity obscured in the time domain. In this work, we shift the perspective to the frequency domain, and propose a novel Frequency Enhanced Hybrid Attention Network for Sequential Recommendation, namely FEARec. In this model, we firstly improve the original time domain self-attention in the frequency domain with a ramp structure to make both low-frequency and high-frequency information could be explicitly learned in our approach. Moreover, we additionally design a similar attention mechanism via auto-corr
    
[^13]: 序列推荐中的元优化对比学习

    Meta-optimized Contrastive Learning for Sequential Recommendation. (arXiv:2304.07763v1 [cs.IR])

    [http://arxiv.org/abs/2304.07763](http://arxiv.org/abs/2304.07763)

    本文提出了 MCLRec 模型，该模型在数据增强和可学习模型增强操作的基础上，解决了现有对比学习方法难以推广和训练数据不足的问题。

    

    对比学习方法是解决稀疏且含噪声推荐数据的一个新兴方法。然而，现有的对比学习方法要么只针对手工制作的数据进行训练数据和模型增强，要么只使用模型增强方法，这使得模型很难推广。为了更好地训练模型，本文提出了一种称为元优化对比学习的模型。该模型结合了数据增强和可学习模型增强操作。

    Contrastive Learning (CL) performances as a rising approach to address the challenge of sparse and noisy recommendation data. Although having achieved promising results, most existing CL methods only perform either hand-crafted data or model augmentation for generating contrastive pairs to find a proper augmentation operation for different datasets, which makes the model hard to generalize. Additionally, since insufficient input data may lead the encoder to learn collapsed embeddings, these CL methods expect a relatively large number of training data (e.g., large batch size or memory bank) to contrast. However, not all contrastive pairs are always informative and discriminative enough for the training processing. Therefore, a more general CL-based recommendation model called Meta-optimized Contrastive Learning for sequential Recommendation (MCLRec) is proposed in this work. By applying both data augmentation and learnable model augmentation operations, this work innovates the standard 
    
[^14]: 解释所有神经网络的方差容忍因子

    Variance Tolerance Factors For Interpreting ALL Neural Networks. (arXiv:2209.13858v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.13858](http://arxiv.org/abs/2209.13858)

    本文提出一种用于解释黑盒神经网络的方差容忍因子（VTF）理论，通过排名特征的方式探索特征的重要性，同时构建一个基本模型和特征模型的新型架构，来探索所有表现良好的神经网络中特征的重要性，并且经过基准测试和应用于实际环境中的实验验证了方法的可靠性。

    

    黑匣子模型只提供深度学习任务的结果，缺乏有关如何获得这些结果的详细信息。知道输入变量与输出的关系，以及为什么它们相关，可以在将预测转化为实验或在受到审查时维护模型预测的关键时刻起到重要作用。在本文中，我们提出了一个一般性理论，通过定义一个受影响函数启发的方差容忍因子（VTF），从排名特征的角度解释黑匣子神经网络中的特征，并构建一个包含基本模型和特征模型的新型架构，以探索包含所有表现良好的神经网络的瑞士军刀集中的特征重要性。创建并探索了两种Rashomon集中的特征重要性排名方法和基于VTF的特征选择方法。我们提供了对合成数据集和基准数据集的彻底评估，并将该方法应用于基因组学和材料科学中的两个真实世界实验。

    Black box models only provide results for deep learning tasks, and lack informative details about how these results were obtained. Knowing how input variables are related to outputs, in addition to why they are related, can be critical to translating predictions into laboratory experiments, or defending a model prediction under scrutiny. In this paper, we propose a general theory that defines a variance tolerance factor (VTF) inspired by influence function, to interpret features in the context of black box neural networks by ranking the importance of features, and construct a novel architecture consisting of a base model and feature model to explore the feature importance in a Rashomon set that contains all well-performing neural networks. Two feature importance ranking methods in the Rashomon set and a feature selection method based on the VTF are created and explored. A thorough evaluation on synthetic and benchmark datasets is provided, and the method is applied to two real world ex
    
[^15]: 可解释嵌入的(不)可能性训练

    (Un)likelihood Training for Interpretable Embedding. (arXiv:2207.00282v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2207.00282](http://arxiv.org/abs/2207.00282)

    该论文提出了两种新的训练方法：可能性训练和不可能性训练，以解释嵌入向量背后的语义并解决标签稀疏问题。这些方法在图像和视频分类、检索和生成任务中表现出色，提高了学习嵌入的可解释性。

    

    跨模态表示学习已成为弥合文本和视觉数据语义差距的新常态。然而，在连续潜在空间中学习模态不可知表示经常被视为黑匣子数据驱动的训练过程。深度表示学习的有效性严重依赖于训练数据的质量和规模。对于视频表示学习，要完整地标注视频内容的数据集是高度困难甚至不可能的。这些问题，黑匣子训练和数据集偏差，使得解释性较差和结果难以预测，难以在视频理解方面进行实际应用。在本文中，我们提出了两种新的训练方法，可能性和不可能性函数，以展示嵌入背后的语义，并解决训练中的标签稀疏问题。可能性训练旨在通过学习数据分布来解释嵌入向量的语义，而不可能性训练则强调正负对之间的差异。我们将所提出的方法应用于各种任务，包括图像和视频分类、检索和生成，并展示了它们在提高学到的嵌入的可解释性以及在基准数据集上实现竞争性性能方面的有效性。

    Cross-modal representation learning has become a new normal for bridging the semantic gap between text and visual data. Learning modality agnostic representations in a continuous latent space, however, is often treated as a black-box data-driven training process. It is well-known that the effectiveness of representation learning depends heavily on the quality and scale of training data. For video representation learning, having a complete set of labels that annotate the full spectrum of video content for training is highly difficult if not impossible. These issues, black-box training and dataset bias, make representation learning practically challenging to be deployed for video understanding due to unexplainable and unpredictable results. In this paper, we propose two novel training objectives, likelihood and unlikelihood functions, to unroll semantics behind embeddings while addressing the label sparsity problem in training. The likelihood training aims to interpret semantics of embed
    

