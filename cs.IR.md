# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Ambiguity-Aware In-Context Learning with Large Language Models.](http://arxiv.org/abs/2309.07900) | 在上下文学习中，选择与测试输入语义相似的演示有助于提高下游性能，但是考虑到语言模型关于任务的现有知识能够更好地指导演示选择。 |
| [^2] | [NineRec: A Benchmark Dataset Suite for Evaluating Transferable Recommendation.](http://arxiv.org/abs/2309.07705) | NineRec是一个用于评估可迁移推荐的数据集套件，包括一个大规模的源域推荐数据集和九个多样的目标域推荐数据集。每个物品由文本描述和高分辨率封面图像表示。 |
| [^3] | [A Conversation is Worth A Thousand Recommendations: A Survey of Holistic Conversational Recommender Systems.](http://arxiv.org/abs/2309.07682) | 该论文综述了综合对话式推荐系统的方法，传统方法无法应用于真实世界场景，该论文提出了一种新的方法，即使用真实对话数据进行训练。 |
| [^4] | [Feature Engineering in Learning-to-Rank for Community Question Answering Task.](http://arxiv.org/abs/2309.07610) | 本文研究了在社区问答任务中学习到排名中的特征工程的几个方面。首先，引入了基于BERT的特征，捕捉语义相似性；其次，结合问题和答案两种类型的特征；第三，通过经验性研究探索了不同排名算法。 |
| [^5] | [Zero-shot Audio Topic Reranking using Large Language Models.](http://arxiv.org/abs/2309.07606) | 本论文研究了使用大型语言模型的零-shot重新排序方法，以改善基于主题的视频检索性能，无需任何特定任务的训练数据。 |
| [^6] | [Turning Dross Into Gold Loss: is BERT4Rec really better than SASRec?.](http://arxiv.org/abs/2309.07602) | 在比较推荐系统中的两种模型SASRec和BERT4Rec时，我们的研究发现，如果两个模型都使用相同的损失函数进行训练，SASRec在质量和训练速度方面表现明显优于BERT4Rec。同时，我们还发现，即使使用负采样，SASRec仍然能够有效训练并优于BERT4Rec，但需要更多的负样本。 |
| [^7] | [C-Pack: Packaged Resources To Advance General Chinese Embedding.](http://arxiv.org/abs/2309.07597) | C-Pack是一套推进普通汉语嵌入领域的资源，包括全面汉语文本嵌入基准、大规模文本嵌入数据集和涵盖多个尺寸的嵌入模型系列。该资源集在C-MTEB基准上实现了最高+10%的表现，并通过整合和优化一套训练方法进一步提升了效果。此外，C-Pack还发布了英语文本嵌入数据和模型，实现了最先进的性能。该资源集可公开获取。 |
| [^8] | [Neuro-Symbolic Recommendation Model based on Logic Query.](http://arxiv.org/abs/2309.07594) | 本文提出了一个基于逻辑查询的神经符号推荐模型，将用户历史交互转化为逻辑表达式，并通过逻辑查询实现推荐过程。 |
| [^9] | [MMEAD: MS MARCO Entity Annotations and Disambiguations.](http://arxiv.org/abs/2309.07574) | MMEAD是用于MS MARCO数据集的实体链接资源。它提供了实体注释和消歧功能，并改进了使用实体信息的信息检索研究。 |
| [^10] | [Exploring Music Genre Classification: Algorithm Analysis and Deployment Architecture.](http://arxiv.org/abs/2309.04861) | 本文研究了音乐流派分类，使用了数字信号处理和深度学习技术，并提出了一种新颖的算法，可以从音频信号中提取特征并进行分类。该算法在GTZAN数据集上取得高精度，同时还提出了端到端的部署架构，可用于音乐应用程序的集成。 |
| [^11] | [CPMR: Context-Aware Incremental Sequential Recommendation with Pseudo-Multi-Task Learning.](http://arxiv.org/abs/2309.04802) | CPMR是一个基于上下文感知的增量顺序推荐系统，通过创建静态嵌入、历史时间状态和上下文时间状态的三个表示，准确地建模了用户随时间变化的表示和兴趣动态的演化。 |
| [^12] | [A Diffusion model for POI recommendation.](http://arxiv.org/abs/2304.07041) | 本文提出了一种基于扩散算法采样用户空间偏好的POI推荐模型，解决了现有方法只基于用户先前访问位置聚合的缺点，适用于推荐新颖区域的POI。 |
| [^13] | [Reasoning with Language Model Prompting: A Survey.](http://arxiv.org/abs/2212.09597) | 本文提供了使用语言模型提示进行推理的前沿研究综合调查。讨论了新兴推理能力出现的潜在原因，并提供系统资源帮助初学者。 |
| [^14] | [DisenPOI: Disentangling Sequential and Geographical Influence for Point-of-Interest Recommendation.](http://arxiv.org/abs/2210.16591) | 本文提出了DisenPOI，一个新颖的基于双图的POI推荐解开框架，通过利用顺序和地理关系并使用自我监督解开这两种影响，以提高推荐性能和可解释性。 |
| [^15] | [LambdaKG: A Library for Pre-trained Language Model-Based Knowledge Graph Embeddings.](http://arxiv.org/abs/2210.00305) | LambdaKG是一个基于预训练语言模型的知识图谱嵌入库，提供了多个预训练语言模型和支持多种任务，如知识图谱补全、问答、推荐和知识探索。 |
| [^16] | [Modern Baselines for SPARQL Semantic Parsing.](http://arxiv.org/abs/2204.12793) | 本文探讨了从自然语言问题生成SPARQL查询的任务，使用预训练语言模型作为新的基准模型，并在DBpedia和Wikidata知识图谱上进行了实验。我们展示了T5模型在LC-QuAD 1.0和LC-QuAD 2.0数据集上表现出最先进的性能，并且能够解析需要将一部分输入复制到输出查询中的问题，这为知识图谱语义解析带来了新的可能性。 |

# 详细

[^1]: 具有大型语言模型的上下文学习中的歧义感知

    Ambiguity-Aware In-Context Learning with Large Language Models. (arXiv:2309.07900v1 [cs.CL])

    [http://arxiv.org/abs/2309.07900](http://arxiv.org/abs/2309.07900)

    在上下文学习中，选择与测试输入语义相似的演示有助于提高下游性能，但是考虑到语言模型关于任务的现有知识能够更好地指导演示选择。

    

    在上下文学习（In-context learning, ICL）中，仅向LLMs展示少量任务特定演示已经导致了下游增益，无需进行任务特定的微调。然而，LLMs对于提示选择非常敏感，因此一个关键的研究问题是如何为ICL选择好的演示。一种有效的策略是利用ICL演示和测试输入之间的语义相似性，并使用文本检索器，然而这种方法并不考虑LLM关于该任务的现有知识，因此并不最优。根据之前的工作（Min等，2022），我们已经知道与演示配对的标签会对模型预测造成偏见。这引导我们提出了一个假设：考虑到LLM关于任务的现有知识，特别是与输出标签空间相关的知识，是否有助于更好的演示选择策略。通过在三个文本分类任务上进行广泛的实验，我们发现不仅选择语义相似的ICL演示是有益的，同时也要考虑LLM关于任务的现有知识以获得更好的演示选择策略。

    In-context learning (ICL) i.e. showing LLMs only a few task-specific demonstrations has led to downstream gains with no task-specific fine-tuning required. However, LLMs are sensitive to the choice of prompts, and therefore a crucial research question is how to select good demonstrations for ICL. One effective strategy is leveraging semantic similarity between the ICL demonstrations and test inputs by using a text retriever, which however is sub-optimal as that does not consider the LLM's existing knowledge about that task. From prior work (Min et al., 2022), we already know that labels paired with the demonstrations bias the model predictions. This leads us to our hypothesis whether considering LLM's existing knowledge about the task, especially with respect to the output label space can help in a better demonstration selection strategy. Through extensive experimentation on three text classification tasks, we find that it is beneficial to not only choose semantically similar ICL demon
    
[^2]: NineRec: 用于评估可迁移推荐的基准数据集套件

    NineRec: A Benchmark Dataset Suite for Evaluating Transferable Recommendation. (arXiv:2309.07705v1 [cs.IR])

    [http://arxiv.org/abs/2309.07705](http://arxiv.org/abs/2309.07705)

    NineRec是一个用于评估可迁移推荐的数据集套件，包括一个大规模的源域推荐数据集和九个多样的目标域推荐数据集。每个物品由文本描述和高分辨率封面图像表示。

    

    近年来，从物品的原始特征（如图像、文本、音频等）学习推荐系统模型，称为MoRec，引起了越来越多的兴趣。 MoRec的一个关键优势是它可以轻松受益于其他领域的进展，如自然语言处理（NLP）和计算机视觉（CV）。此外，它通过特征自然支持不同系统之间的迁移学习，称为可迁移推荐系统或TransRec。然而，迄今为止，与NLP和CV领域的开创性基础模型相比，TransRec取得了很小的进展。缺乏大规模、高质量的推荐数据集是一个重大障碍。为此，我们介绍了NineRec，这是一个TransRec数据集套件，包括一个大规模的源域推荐数据集和九个多样的目标域推荐数据集。NineRec中的每个物品都由一个文本描述和一个高分辨率的封面图像表示。通过NineRec，我们可以实现Tran

    Learning a recommender system model from an item's raw modality features (such as image, text, audio, etc.), called MoRec, has attracted growing interest recently. One key advantage of MoRec is that it can easily benefit from advances in other fields, such as natural language processing (NLP) and computer vision (CV). Moreover, it naturally supports transfer learning across different systems through modality features, known as transferable recommender systems, or TransRec.  However, so far, TransRec has made little progress, compared to groundbreaking foundation models in the fields of NLP and CV. The lack of large-scale, high-quality recommendation datasets poses a major obstacle. To this end, we introduce NineRec, a TransRec dataset suite that includes a large-scale source domain recommendation dataset and nine diverse target domain recommendation datasets. Each item in NineRec is represented by a text description and a high-resolution cover image. With NineRec, we can implement Tran
    
[^3]: 一次对话胜过千万的推荐：综述综合对话推荐系统

    A Conversation is Worth A Thousand Recommendations: A Survey of Holistic Conversational Recommender Systems. (arXiv:2309.07682v1 [cs.CL])

    [http://arxiv.org/abs/2309.07682](http://arxiv.org/abs/2309.07682)

    该论文综述了综合对话式推荐系统的方法，传统方法无法应用于真实世界场景，该论文提出了一种新的方法，即使用真实对话数据进行训练。

    

    对话式推荐系统通过交互过程生成推荐。然而，并非所有对话式推荐系统方法都使用人类对话作为交互数据源；先前的大部分对话式推荐系统工作通过交换实体级信息来模拟交互。因此，先前的对话式推荐系统工作无法适用于真实世界场景中的对话，其中对话会出现意外转变，或者对话和意图理解并非完美。为了解决这一挑战，研究界已经开始研究综合对话式推荐系统，这些系统使用从真实场景中收集到的对话数据进行训练。尽管这些综合方法已经出现，但其尚未充分探索。我们通过结构化方式总结文献，对综合对话式推荐系统方法进行全面调查。调查将综合对话式推荐系统方法分为三个组成部分：1）骨干语言模型，可选使用2）外部知识和/或3）外部指导。

    Conversational recommender systems (CRS) generate recommendations through an interactive process. However, not all CRS approaches use human conversations as their source of interaction data; the majority of prior CRS work simulates interactions by exchanging entity-level information. As a result, claims of prior CRS work do not generalise to real-world settings where conversations take unexpected turns, or where conversational and intent understanding is not perfect. To tackle this challenge, the research community has started to examine holistic CRS, which are trained using conversational data collected from real-world scenarios. Despite their emergence, such holistic approaches are under-explored.  We present a comprehensive survey of holistic CRS methods by summarizing the literature in a structured manner. Our survey recognises holistic CRS approaches as having three components: 1) a backbone language model, the optional use of 2) external knowledge, and/or 3) external guidance. We
    
[^4]: 学习到排名中的特征工程在社区问答任务中的应用

    Feature Engineering in Learning-to-Rank for Community Question Answering Task. (arXiv:2309.07610v1 [cs.LG])

    [http://arxiv.org/abs/2309.07610](http://arxiv.org/abs/2309.07610)

    本文研究了在社区问答任务中学习到排名中的特征工程的几个方面。首先，引入了基于BERT的特征，捕捉语义相似性；其次，结合问题和答案两种类型的特征；第三，通过经验性研究探索了不同排名算法。

    

    社区问答（CQA）论坛是基于互联网的平台，用户在这里提出问题，其他专家用户试图提供解决方案。许多CQA论坛，如Quora，Stackoverflow，Yahoo！Answer，StackExchange等都有大量用户生成的数据。这些数据在自动化的CQA排名系统中得到利用，以回应用户的查询，呈现类似的问题（和答案）。在这项工作中，我们经验性地调查了该领域的一些方面。首先，除了传统的特征如TF-IDF、BM25等，我们引入了基于BERT的特征，捕捉问题和答案之间的语义相似性。其次，大部分现有研究工作都集中在仅从问题部分提取的特征上，尚未广泛探索从答案中提取的特征。我们以线性方式结合了两种类型的特征。第三，使用我们提出的概念，我们对不同排名算法进行了经验性的研究。

    Community question answering (CQA) forums are Internet-based platforms where users ask questions about a topic and other expert users try to provide solutions. Many CQA forums such as Quora, Stackoverflow, Yahoo!Answer, StackExchange exist with a lot of user-generated data. These data are leveraged in automated CQA ranking systems where similar questions (and answers) are presented in response to the query of the user. In this work, we empirically investigate a few aspects of this domain. Firstly, in addition to traditional features like TF-IDF, BM25 etc., we introduce a BERT-based feature that captures the semantic similarity between the question and answer. Secondly, most of the existing research works have focused on features extracted only from the question part; features extracted from answers have not been explored extensively. We combine both types of features in a linear fashion. Thirdly, using our proposed concepts, we conduct an empirical investigation with different rank-lea
    
[^5]: 使用大型语言模型进行零-shot音频主题重排序

    Zero-shot Audio Topic Reranking using Large Language Models. (arXiv:2309.07606v1 [cs.CL])

    [http://arxiv.org/abs/2309.07606](http://arxiv.org/abs/2309.07606)

    本论文研究了使用大型语言模型的零-shot重新排序方法，以改善基于主题的视频检索性能，无需任何特定任务的训练数据。

    

    多模态视频搜索项目通过使用视频片段作为查询项，而不是传统的文本查询，来研究信息检索。这使得搜索模态更加丰富，例如图像、说话者、内容、主题和情感。这个过程的关键要素是对大型存档的高速、灵活的搜索支持，MVSE通过用嵌入表示视频属性来实现这一点。这项工作旨在通过检查重新排序方法来减少来自快速存档搜索的性能损失。具体而言，研究使用大型语言模型的零-shot 重新排序方法，因为这些方法适用于任何视频存档音频内容。在公开可用的视频存档BBC Rewind语料库上评估了基于主题的检索性能。结果表明，在不需要任何任务特定的训练数据的情况下，重新排序可以实现改进的检索排名。

    The Multimodal Video Search by Examples (MVSE) project investigates using video clips as the query term for information retrieval, rather than the more traditional text query. This enables far richer search modalities such as images, speaker, content, topic, and emotion. A key element for this process is highly rapid, flexible, search to support large archives, which in MVSE is facilitated by representing video attributes by embeddings. This work aims to mitigate any performance loss from this rapid archive search by examining reranking approaches. In particular, zero-shot reranking methods using large language models are investigated as these are applicable to any video archive audio content. Performance is evaluated for topic-based retrieval on a publicly available video archive, the BBC Rewind corpus. Results demonstrate that reranking can achieve improved retrieval ranking without the need for any task-specific training data.
    
[^6]: 将废料变为黄金的损失：BERT4Rec真的比SASRec更好吗？

    Turning Dross Into Gold Loss: is BERT4Rec really better than SASRec?. (arXiv:2309.07602v1 [cs.IR])

    [http://arxiv.org/abs/2309.07602](http://arxiv.org/abs/2309.07602)

    在比较推荐系统中的两种模型SASRec和BERT4Rec时，我们的研究发现，如果两个模型都使用相同的损失函数进行训练，SASRec在质量和训练速度方面表现明显优于BERT4Rec。同时，我们还发现，即使使用负采样，SASRec仍然能够有效训练并优于BERT4Rec，但需要更多的负样本。

    

    最近，在推荐系统领域，顺序推荐和下一个项目预测任务越来越受欢迎。目前，基于Transformer的模型SASRec和BERT4Rec是两种最先进的基准模型。在过去的几年中，有很多发表的论文比较了这两个算法并提出了新的最先进模型。在大多数论文中，BERT4Rec的性能优于SASRec。但是，BERT4Rec对所有项目使用交叉熵，而SASRec使用负采样对一个正样本和一个负样本计算二元交叉熵损失。在我们的工作中，我们展示了如果两个模型都使用BERT4Rec所用的损失进行训练，那么SASRec在质量和训练速度方面将明显优于BERT4Rec。此外，我们还展示了即使使用负采样，SASRec仍然能够有效训练并优于BERT4Rec，但负样本的数量应该比BERT4Rec要大得多。

    Recently sequential recommendations and next-item prediction task has become increasingly popular in the field of recommender systems. Currently, two state-of-the-art baselines are Transformer-based models SASRec and BERT4Rec. Over the past few years, there have been quite a few publications comparing these two algorithms and proposing new state-of-the-art models. In most of the publications, BERT4Rec achieves better performance than SASRec. But BERT4Rec uses cross-entropy over softmax for all items, while SASRec uses negative sampling and calculates binary cross-entropy loss for one positive and one negative item. In our work, we show that if both models are trained with the same loss, which is used by BERT4Rec, then SASRec will significantly outperform BERT4Rec both in terms of quality and training speed. In addition, we show that SASRec could be effectively trained with negative sampling and still outperform BERT4Rec, but the number of negative examples should be much larger than on
    
[^7]: C-Pack: 推进普通汉语嵌入的打包资源

    C-Pack: Packaged Resources To Advance General Chinese Embedding. (arXiv:2309.07597v1 [cs.CL])

    [http://arxiv.org/abs/2309.07597](http://arxiv.org/abs/2309.07597)

    C-Pack是一套推进普通汉语嵌入领域的资源，包括全面汉语文本嵌入基准、大规模文本嵌入数据集和涵盖多个尺寸的嵌入模型系列。该资源集在C-MTEB基准上实现了最高+10%的表现，并通过整合和优化一套训练方法进一步提升了效果。此外，C-Pack还发布了英语文本嵌入数据和模型，实现了最先进的性能。该资源集可公开获取。

    

    我们介绍了C-Pack，这是一套显著推进普通汉语嵌入领域的资源。C-Pack包括三个关键资源。1）C-MTEB是一个涵盖6个任务和35个数据集的全面汉语文本嵌入基准。2）C-MTP是一个从标记和未标记的汉语语料库中策划的大规模文本嵌入数据集，用于训练嵌入模型。3）C-TEM是一个涵盖多个尺寸的嵌入模型系列。我们的模型在C-MTEB上的表现优于之前的所有汉语文本嵌入达到了发布时的最高+10%。我们还整合和优化了C-TEM的整套训练方法。除了我们关于普通汉语嵌入的资源外，我们还发布了我们的英语文本嵌入数据和模型。这些英语模型在MTEB基准上实现了最先进的性能；与此同时，我们发布的英语数据比汉语数据大2倍。所有这些资源都可以在https://github.com/FlagOpen/FlagEmbedding上公开获取。

    We introduce C-Pack, a package of resources that significantly advance the field of general Chinese embeddings. C-Pack includes three critical resources. 1) C-MTEB is a comprehensive benchmark for Chinese text embeddings covering 6 tasks and 35 datasets. 2) C-MTP is a massive text embedding dataset curated from labeled and unlabeled Chinese corpora for training embedding models. 3) C-TEM is a family of embedding models covering multiple sizes. Our models outperform all prior Chinese text embeddings on C-MTEB by up to +10% upon the time of the release. We also integrate and optimize the entire suite of training methods for C-TEM. Along with our resources on general Chinese embedding, we release our data and models for English text embeddings. The English models achieve state-of-the-art performance on MTEB benchmark; meanwhile, our released English data is 2 times larger than the Chinese data. All these resources are made publicly available at https://github.com/FlagOpen/FlagEmbedding.
    
[^8]: 基于逻辑查询的神经符号推荐模型

    Neuro-Symbolic Recommendation Model based on Logic Query. (arXiv:2309.07594v1 [cs.AI])

    [http://arxiv.org/abs/2309.07594](http://arxiv.org/abs/2309.07594)

    本文提出了一个基于逻辑查询的神经符号推荐模型，将用户历史交互转化为逻辑表达式，并通过逻辑查询实现推荐过程。

    

    推荐系统帮助用户找到与他们相关的物品。现有的推荐模型主要基于预测用户和物品之间的关系，并使用复杂的匹配模型或引入大量的外部信息来捕捉数据中的关联模式。然而，推荐不仅是一个利用数据进行归纳统计的问题，也是一个基于从信息中提取的知识进行推理决策的认知任务。因此，在推荐任务中，逻辑系统自然可以用于推理。然而，虽然基于逻辑系统的硬规则方法可以提供强大的推理能力，但它们在处理不一致和不完整的现实任务中很难应对，尤其是对于复杂任务如推荐。因此，在本文中，我们提出了一个神经符号推荐模型，将用户历史交互转化为逻辑表达式，然后将推荐过程变换为逻辑查询。

    A recommendation system assists users in finding items that are relevant to them. Existing recommendation models are primarily based on predicting relationships between users and items and use complex matching models or incorporate extensive external information to capture association patterns in data. However, recommendation is not only a problem of inductive statistics using data; it is also a cognitive task of reasoning decisions based on knowledge extracted from information. Hence, a logic system could naturally be incorporated for the reasoning in a recommendation task. However, although hard-rule approaches based on logic systems can provide powerful reasoning ability, they struggle to cope with inconsistent and incomplete knowledge in real-world tasks, especially for complex tasks such as recommendation. Therefore, in this paper, we propose a neuro-symbolic recommendation model, which transforms the user history interactions into a logic expression and then transforms the recomm
    
[^9]: MMEAD: MS MARCO实体注释和消歧

    MMEAD: MS MARCO Entity Annotations and Disambiguations. (arXiv:2309.07574v1 [cs.IR])

    [http://arxiv.org/abs/2309.07574](http://arxiv.org/abs/2309.07574)

    MMEAD是用于MS MARCO数据集的实体链接资源。它提供了实体注释和消歧功能，并改进了使用实体信息的信息检索研究。

    

    MMEAD是用于MS MARCO数据集的实体链接资源。我们定义了一种格式来存储和共享MS MARCO文档和段落集合的链接。按照此规范，我们发布了MS MARCO v1和v2中文档和段落的实体链接到维基百科。实体链接由REL和BLINK系统生成。MMEAD是一个易于安装的Python包，允许用户轻松加载链接数据和实体嵌入。只需要几行代码即可使用MMEAD。最后，我们展示了如何利用MMEAD来改进使用实体信息的信息检索研究。我们展示了如何通过使用这个资源，在MS MARCO v1段落数据集上对更复杂的查询提高recall@1000和MRR@10。我们还展示了如何利用实体扩展来进行交互式搜索应用。

    MMEAD, or MS MARCO Entity Annotations and Disambiguations, is a resource for entity links for the MS MARCO datasets. We specify a format to store and share links for both document and passage collections of MS MARCO. Following this specification, we release entity links to Wikipedia for documents and passages in both MS MARCO collections (v1 and v2). Entity links have been produced by the REL and BLINK systems. MMEAD is an easy-to-install Python package, allowing users to load the link data and entity embeddings effortlessly. Using MMEAD takes only a few lines of code. Finally, we show how MMEAD can be used for IR research that uses entity information. We show how to improve recall@1000 and MRR@10 on more complex queries on the MS MARCO v1 passage dataset by using this resource. We also demonstrate how entity expansions can be used for interactive search applications.
    
[^10]: 探索音乐流派分类：算法分析与部署架构

    Exploring Music Genre Classification: Algorithm Analysis and Deployment Architecture. (arXiv:2309.04861v1 [cs.SD])

    [http://arxiv.org/abs/2309.04861](http://arxiv.org/abs/2309.04861)

    本文研究了音乐流派分类，使用了数字信号处理和深度学习技术，并提出了一种新颖的算法，可以从音频信号中提取特征并进行分类。该算法在GTZAN数据集上取得高精度，同时还提出了端到端的部署架构，可用于音乐应用程序的集成。

    

    随着各种流媒体应用的出现，音乐流派分类变得越来越重要。如今，在一个复杂的音乐应用程序中，我们无法想象仅通过艺术家的名字和歌曲标题来搜索音乐。正确分类音乐一直很困难，因为与音乐相关的信息，如地区、艺术家、专辑或非专辑，是如此多变。本文提出了一项关于音乐流派分类的研究，使用了数字信号处理（DSP）和深度学习（DL）技术的组合。提出了一种新颖的算法，利用DSP和DL方法从音频信号中提取相关特征，并将其分类到各种流派中。该算法在GTZAN数据集上进行了测试，并取得了高精度。还提出了一种端到端的部署架构，用于集成到与音乐相关的应用程序中。对算法的性能进行了分析，并讨论了改进的未来方向。

    Music genre classification has become increasingly critical with the advent of various streaming applications. Nowadays, we find it impossible to imagine using the artist's name and song title to search for music in a sophisticated music app. It is always difficult to classify music correctly because the information linked to music, such as region, artist, album, or non-album, is so variable. This paper presents a study on music genre classification using a combination of Digital Signal Processing (DSP) and Deep Learning (DL) techniques. A novel algorithm is proposed that utilizes both DSP and DL methods to extract relevant features from audio signals and classify them into various genres. The algorithm was tested on the GTZAN dataset and achieved high accuracy. An end-to-end deployment architecture is also proposed for integration into music-related applications. The performance of the algorithm is analyzed and future directions for improvement are discussed. The proposed DSP and DL-b
    
[^11]: CPMR: 基于上下文感知的增量顺序推荐与伪多任务学习

    CPMR: Context-Aware Incremental Sequential Recommendation with Pseudo-Multi-Task Learning. (arXiv:2309.04802v1 [cs.IR])

    [http://arxiv.org/abs/2309.04802](http://arxiv.org/abs/2309.04802)

    CPMR是一个基于上下文感知的增量顺序推荐系统，通过创建静态嵌入、历史时间状态和上下文时间状态的三个表示，准确地建模了用户随时间变化的表示和兴趣动态的演化。

    

    用户进行互动的动机可以分为静态偏好和动态兴趣。为了准确地建模用户随时间变化的表示，最近的顺序推荐研究利用信息传播和演化从批量到达的互动中进行挖掘。然而，他们忽略了在上下文场景中人们很容易受到其他用户的最近行为的影响，并且在所有历史互动中应用演化会稀释最近互动的重要性，从而无法准确地建模兴趣动态的演化。为了解决这个问题，我们提出了一种基于上下文感知的伪多任务推荐系统（CPMR），通过为每个用户和项目创建三个表示（静态嵌入、历史时间状态和上下文时间状态），来建模历史和上下文情境中的演化。为了同时提高时间状态演化和增量推荐的性能。

    The motivations of users to make interactions can be divided into static preference and dynamic interest. To accurately model user representations over time, recent studies in sequential recommendation utilize information propagation and evolution to mine from batches of arriving interactions. However, they ignore the fact that people are easily influenced by the recent actions of other users in the contextual scenario, and applying evolution across all historical interactions dilutes the importance of recent ones, thus failing to model the evolution of dynamic interest accurately. To address this issue, we propose a Context-Aware Pseudo-Multi-Task Recommender System (CPMR) to model the evolution in both historical and contextual scenarios by creating three representations for each user and item under different dynamics: static embedding, historical temporal states, and contextual temporal states. To dually improve the performance of temporal states evolution and incremental recommenda
    
[^12]: 一种POI推荐的扩散模型

    A Diffusion model for POI recommendation. (arXiv:2304.07041v1 [cs.IR])

    [http://arxiv.org/abs/2304.07041](http://arxiv.org/abs/2304.07041)

    本文提出了一种基于扩散算法采样用户空间偏好的POI推荐模型，解决了现有方法只基于用户先前访问位置聚合的缺点，适用于推荐新颖区域的POI。

    

    下一个兴趣点（POI）的推荐是定位服务中的关键任务，旨在为用户的下一个目的地提供个性化建议。先前关于POI推荐的工作侧重于对用户空间偏好的建模。然而，现有的利用空间信息的方法仅基于用户先前访问位置的聚合，这会使模型不会推荐新颖区域的POI，从而损害其在许多情况下的性能。此外，将时间顺序信息融入用户的空间偏好仍是一个挑战。在本文中，我们提出了Diff-POI：一种基于扩散的模型，用于采样用户的空间偏好，以进行下一步POI推荐。在扩散算法在从分布中进行采样方面的广泛应用的启发下，Diff-POI使用两个量身定制的图编码模块对用户的访问序列和空间特性进行编码。

    Next Point-of-Interest (POI) recommendation is a critical task in location-based services that aim to provide personalized suggestions for the user's next destination. Previous works on POI recommendation have laid focused on modeling the user's spatial preference. However, existing works that leverage spatial information are only based on the aggregation of users' previous visited positions, which discourages the model from recommending POIs in novel areas. This trait of position-based methods will harm the model's performance in many situations. Additionally, incorporating sequential information into the user's spatial preference remains a challenge. In this paper, we propose Diff-POI: a Diffusion-based model that samples the user's spatial preference for the next POI recommendation. Inspired by the wide application of diffusion algorithm in sampling from distributions, Diff-POI encodes the user's visiting sequence and spatial character with two tailor-designed graph encoding modules
    
[^13]: 使用语言模型提示进行推理：一项调查

    Reasoning with Language Model Prompting: A Survey. (arXiv:2212.09597v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.09597](http://arxiv.org/abs/2212.09597)

    本文提供了使用语言模型提示进行推理的前沿研究综合调查。讨论了新兴推理能力出现的潜在原因，并提供系统资源帮助初学者。

    

    推理作为复杂问题解决的重要能力，可以为医疗诊断、谈判等各种实际应用提供后端支持。本文对使用语言模型提示进行推理的前沿研究进行了综合调查。我们介绍了研究成果的比较和总结，并提供了系统资源以帮助初学者。我们还讨论了新兴推理能力出现的潜在原因，并突出了未来的研究方向。资源可在 https://github.com/zjunlp/Prompt4ReasoningPapers 上获取（定期更新）。

    Reasoning, as an essential ability for complex problem-solving, can provide back-end support for various real-world applications, such as medical diagnosis, negotiation, etc. This paper provides a comprehensive survey of cutting-edge research on reasoning with language model prompting. We introduce research works with comparisons and summaries and provide systematic resources to help beginners. We also discuss the potential reasons for emerging such reasoning abilities and highlight future research directions. Resources are available at https://github.com/zjunlp/Prompt4ReasoningPapers (updated periodically).
    
[^14]: DisenPOI: 解开顺序和地理影响的POI推荐

    DisenPOI: Disentangling Sequential and Geographical Influence for Point-of-Interest Recommendation. (arXiv:2210.16591v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2210.16591](http://arxiv.org/abs/2210.16591)

    本文提出了DisenPOI，一个新颖的基于双图的POI推荐解开框架，通过利用顺序和地理关系并使用自我监督解开这两种影响，以提高推荐性能和可解释性。

    

    POI（兴趣点）推荐在各种位置感知服务中起着至关重要的作用。已经观察到POI推荐受到顺序和地理影响的驱动。然而，由于在推荐过程中没有明确指定主导影响的注释标签，现有方法往往会混淆这两种影响，这可能导致次优的推荐性能和差的可解释性。本文通过提出DisenPOI，一个新颖的基于双图的POI推荐解开框架，来解决上述挑战。DisenPOI在两个独立的图上同时利用顺序和地理关系，并通过自我监督解开这两种影响。与现有方法相比，我们的模型的关键创新之处在于使用对比学习提取顺序和地理影响的解开表示。具体而言，我们根据签到序列构建了一个地理图和一个顺序图。

    Point-of-Interest (POI) recommendation plays a vital role in various location-aware services. It has been observed that POI recommendation is driven by both sequential and geographical influences. However, since there is no annotated label of the dominant influence during recommendation, existing methods tend to entangle these two influences, which may lead to sub-optimal recommendation performance and poor interpretability. In this paper, we address the above challenge by proposing DisenPOI, a novel Disentangled dual-graph framework for POI recommendation, which jointly utilizes sequential and geographical relationships on two separate graphs and disentangles the two influences with self-supervision. The key novelty of our model compared with existing approaches is to extract disentangled representations of both sequential and geographical influences with contrastive learning. To be specific, we construct a geographical graph and a sequential graph based on the check-in sequence of a 
    
[^15]: LambdaKG:基于预训练语言模型的知识图谱嵌入库

    LambdaKG: A Library for Pre-trained Language Model-Based Knowledge Graph Embeddings. (arXiv:2210.00305v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2210.00305](http://arxiv.org/abs/2210.00305)

    LambdaKG是一个基于预训练语言模型的知识图谱嵌入库，提供了多个预训练语言模型和支持多种任务，如知识图谱补全、问答、推荐和知识探索。

    

    知识图谱（KG）通常具有异构的图结构和文本丰富的实体/关系信息。基于文本的KG嵌入可以通过使用预训练语言模型对描述进行编码来表示实体，但目前尚无专门为PLM与KG设计的开源库。本文介绍了LambdaKG，一个带有多个预训练语言模型（如BERT，BART，T5，GPT-3）并支持各种任务（如知识图谱补全，问答，推荐和知识探索）的KGE库。LambdaKG在https://github.com/zjunlp/PromptKG/tree/main/lambdaKG上公开开源，并提供了演示视频和长期维护。

    Knowledge Graphs (KGs) often have two characteristics: heterogeneous graph structure and text-rich entity/relation information. Text-based KG embeddings can represent entities by encoding descriptions with pre-trained language models, but no open-sourced library is specifically designed for KGs with PLMs at present. In this paper, we present LambdaKG, a library for KGE that equips with many pre-trained language models (e.g., BERT, BART, T5, GPT-3), and supports various tasks (e.g., knowledge graph completion, question answering, recommendation, and knowledge probing). LambdaKG is publicly open-sourced at https://github.com/zjunlp/PromptKG/tree/main/lambdaKG, with a demo video at this http URL and long-term maintenance.
    
[^16]: SPARQL语义解析的现代基准模型

    Modern Baselines for SPARQL Semantic Parsing. (arXiv:2204.12793v3 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2204.12793](http://arxiv.org/abs/2204.12793)

    本文探讨了从自然语言问题生成SPARQL查询的任务，使用预训练语言模型作为新的基准模型，并在DBpedia和Wikidata知识图谱上进行了实验。我们展示了T5模型在LC-QuAD 1.0和LC-QuAD 2.0数据集上表现出最先进的性能，并且能够解析需要将一部分输入复制到输出查询中的问题，这为知识图谱语义解析带来了新的可能性。

    

    本文关注于从自然语言问题生成SPARQL查询的任务，这些查询可以在知识图谱上执行。我们假设已经提供了黄金实体和关系，剩下的任务是将它们与SPARQL词汇和输入标记一起按正确的顺序排列，以生成正确的SPARQL查询。到目前为止，预训练语言模型（PLMs）在这个任务上尚未深入研究，因此我们尝试了使用BART、T5和PGNs（指针生成网络）与BERT嵌入来寻找这个任务在PLM时代的新基准，在DBpedia和Wikidata知识图谱上进行了实验。我们展示了T5需要特殊的输入标记化，但在LC-QuAD 1.0和LC-QuAD 2.0数据集上表现出最先进的性能，并超过了以前工作中的任务特定模型。此外，这些方法使得对问题进行语义解析成为可能，其中输入的一部分需要复制到输出查询中，从而实现了知识图谱语义解析的新范式。

    In this work, we focus on the task of generating SPARQL queries from natural language questions, which can then be executed on Knowledge Graphs (KGs). We assume that gold entity and relations have been provided, and the remaining task is to arrange them in the right order along with SPARQL vocabulary, and input tokens to produce the correct SPARQL query. Pre-trained Language Models (PLMs) have not been explored in depth on this task so far, so we experiment with BART, T5 and PGNs (Pointer Generator Networks) with BERT embeddings, looking for new baselines in the PLM era for this task, on DBpedia and Wikidata KGs. We show that T5 requires special input tokenisation, but produces state of the art performance on LC-QuAD 1.0 and LC-QuAD 2.0 datasets, and outperforms task-specific models from previous works. Moreover, the methods enable semantic parsing for questions where a part of the input needs to be copied to the output query, thus enabling a new paradigm in KG semantic parsing.
    

