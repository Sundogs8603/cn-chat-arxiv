# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Graph-Based Retriever Captures the Long Tail of Biomedical Knowledge](https://arxiv.org/abs/2402.12352) | 基于Retrieval Augmented Generation (RAG)的图检索器被提议用来克服LLMs的局限，通过从外部数据集检索的上下文来增强提示信息，从而解决生物医学领域长尾知识的捕获挑战。 |
| [^2] | [Explain then Rank: Scale Calibration of Neural Rankers Using Natural Language Explanations from Large Language Models](https://arxiv.org/abs/2402.12276) | 本研究探讨了大型语言模型（LLMs）利用提供与规模校准相关的查询和文档对的不确定性测量的潜力，以解决神经排序器的规模校准问题。 |
| [^3] | [Analysis of Persian News Agencies on Instagram, A Words Co-occurrence Graph-based Approach](https://arxiv.org/abs/2402.12272) | 本研究通过对Instagram上波斯新闻机构的分析，基于词共现图的方法，展示了可以使用无监督方法实现对波斯语Instagram帖子的高精度关键词提取。 |
| [^4] | [Heterogeneity-aware Cross-school Electives Recommendation: a Hybrid Federated Approach](https://arxiv.org/abs/2402.12202) | 提出了一种考虑异质性的混合联邦推荐系统，用于解决跨校选修课程推荐问题，通过构建异构图和设计注意机制来捕捉异质性感知表示，并在联邦方案下训练个别学校模型以推荐量身定制的选修课程。 |
| [^5] | [A Survey on Extractive Knowledge Graph Summarization: Applications, Approaches, Evaluation, and Future Directions](https://arxiv.org/abs/2402.12001) | 本文调查了抽取式知识图谱总结的应用并提出了方法分类，为未来方向提供了重要参考。 |
| [^6] | [FeB4RAG: Evaluating Federated Search in the Context of Retrieval Augmented Generation](https://arxiv.org/abs/2402.11891) | 提出了FeB4RAG，这是一个专门为检索增强生成框架内的联合搜索设计的新数据集。 |
| [^7] | [TriSampler: A Better Negative Sampling Principle for Dense Retrieval](https://arxiv.org/abs/2402.11855) | TriSampler提出了一种新颖的基于准三角原则的负采样框架，能够在稠密检索中选择性地采样更具信息量的负例 |
| [^8] | [Ask Optimal Questions: Aligning Large Language Models with Retriever's Preference in Conversational Search](https://arxiv.org/abs/2402.11827) | 提出了RetPO框架，通过优化语言模型对搜索查询进行重构，以符合目标检索系统的偏好，并构建了一个大型数据集RF Collection，用于收集检索结果作为检索器的偏好。 |
| [^9] | [Microstructures and Accuracy of Graph Recall by Large Language Models](https://arxiv.org/abs/2402.11821) | 本研究首次系统研究了大型语言模型对图形召回的准确性和偏见微结构，探讨了它们与人类的异同以及对其他图形推理任务的影响。 |
| [^10] | [Unveiling the Magic: Investigating Attention Distillation in Retrieval-augmented Generation](https://arxiv.org/abs/2402.11794) | 本文揭示了检索增强生成中的注意力精炼的成功机制，并提出了优化模型训练方法的指标. |
| [^11] | [Large Language Models for Stemming: Promises, Pitfalls and Failures](https://arxiv.org/abs/2402.11757) | 本文研究了使用大型语言模型来进行词干提取的有前途的想法，提出了三种不同的方法，每种方法在计算成本、有效性和稳健性方面具有不同的权衡。 |
| [^12] | [Large Language Models as Data Augmenters for Cold-Start Item Recommendation](https://arxiv.org/abs/2402.11724) | 将大型语言模型用作数据增强器，以解决传统推荐系统在冷启动物品推荐方面的困难。通过结合LLM推断用户对冷启动物品的偏好并将增强的训练信号纳入推荐模型学习，实现对冷启动物品的显著提升。 |
| [^13] | [Search Engines Post-ChatGPT: How Generative Artificial Intelligence Could Make Search Less Reliable](https://arxiv.org/abs/2402.11707) | 讨论了生成式人工智能对搜索引擎的影响，指出了其可能导致的可靠性问题，强调了信息来源不透明和内容正确性等挑战。 |
| [^14] | [Poisoning Federated Recommender Systems with Fake Users](https://arxiv.org/abs/2402.11637) | 本研究介绍了一种无需额外信息的新型基于假用户的毒化攻击方法，用于在联邦推荐系统中执行推广攻击。 |
| [^15] | [Interactive Garment Recommendation with User in the Loop](https://arxiv.org/abs/2402.11627) | 提出一种交互式服装推荐系统，在推荐服装同时实时构建用户资料，通过强化学习Agent吸收用户反馈改善推荐，能够最大化用户满意度。 |
| [^16] | [Metacognitive Retrieval-Augmented Large Language Models](https://arxiv.org/abs/2402.11626) | 本文介绍了MetaRAG，一种结合了检索增强生成过程与元认知的方法，通过元认知调节流程，使模型具有监视、评估和规划其响应策略的能力，增强了其内省推理能力。 |
| [^17] | [Neighborhood-Enhanced Supervised Contrastive Learning for Collaborative Filtering](https://arxiv.org/abs/2402.11523) | 本文提出了一种基于邻域增强的监督对比学习方法，通过将锚节点的协作邻居视为正样本，有效解决了协同过滤中数据稀疏的问题 |
| [^18] | [Pattern-wise Transparent Sequential Recommendation](https://arxiv.org/abs/2402.11480) | 提出了一种模式透明的顺序推荐框架，通过将项目序列分解为多级模式并在概率空间中量化每个模式对结果的贡献，实现了透明的决策过程。 |
| [^19] | [Knowledge Graph-based Session Recommendation with Adaptive Propagation](https://arxiv.org/abs/2402.11302) | 提出了一种基于知识图的会话推荐方法，通过会话自适应传播来跨不同会话捕获全局信息。 |
| [^20] | [Mirror Gradient: Towards Robust Multimodal Recommender Systems via Exploring Flat Local Minima](https://arxiv.org/abs/2402.11262) | 本文从平缓局部最小值的角度分析多模态推荐系统，并提出了一种名为镜像梯度（MG）的梯度策略，可以增强模型的稳健性，缓解来自多模态信息输入的不稳定风险。 |
| [^21] | [Exploring ChatGPT for Next-generation Information Retrieval: Opportunities and Challenges](https://arxiv.org/abs/2402.11203) | ChatGPT作为信息检索领域的关键技术，不断挑战传统范式，带来了新的机遇和挑战，同时超越了之前的GPT-3模型。 |
| [^22] | [Towards Scalability and Extensibility of Query Reformulation Modeling in E-commerce Search](https://arxiv.org/abs/2402.11202) | 本研究致力于克服在电子商务搜索中进行查询重构时面临的可扩展性和可扩展性挑战，构建一个查询重构解决方案，即使在有限的情况下也能有效运行。 |
| [^23] | [A Question Answering Based Pipeline for Comprehensive Chinese EHR Information Extraction](https://arxiv.org/abs/2402.11177) | 提出了一种基于问答的新方法，自动生成训练数据，用于QA模型的迁移学习，通过预处理模块解决了传统方法无法处理的信息提取类型挑战，实现了在电子健康记录中的信息提取任务，表现出色并能有效应对少样本或零样本情况。 |
| [^24] | [Foundation Models for Recommender Systems: A Survey and New Perspectives](https://arxiv.org/abs/2402.11143) | 本文详细分析了基于基础模型的推荐系统，提供了系统分类并讨论了最新研究进展和未来研究方向。 |
| [^25] | [Persona-DB: Efficient Large Language Model Personalization for Response Prediction with Collaborative Data Refinement](https://arxiv.org/abs/2402.11060) | 介绍了 Persona-DB，一个简单却有效的框架，通过层级构建过程和协同优化，改善了大规模语言模型个性化中数据库表示的泛化能力和检索效率。 |
| [^26] | [Retrieval-Augmented Generation: Is Dense Passage Retrieval Retrieving?](https://arxiv.org/abs/2402.11035) | DPR微调预训练网络以增强查询和相关文本数据之间的嵌入对齐，发现训练中知识去中心化，但也揭示了模型内部知识的局限性 |
| [^27] | [LLM-based Federated Recommendation](https://arxiv.org/abs/2402.09959) | 这项研究介绍了一种基于LLM的联邦推荐系统，用于提高推荐系统的性能和隐私保护。面临的挑战是客户端性能不平衡和对计算资源的高需求。 |
| [^28] | [Sequential Recommendation on Temporal Proximities with Contrastive Learning and Self-Attention](https://arxiv.org/abs/2402.09784) | 该论文基于对比学习和自注意力机制，提出了一种考虑垂直和水平时间接近度的顺序推荐方法，以更好地捕捉用户-项目交互中的时间上下文。 |
| [^29] | [Generalizing Conversational Dense Retrieval via LLM-Cognition Data Augmentation](https://arxiv.org/abs/2402.07092) | 本文提出了一种通过LLM-认知数据增强的方法来广义对话密集检索。该方法首先生成多级增强对话，捕捉多样的对话环境。其次，通过认知感知过程减少错误生成情况，并通过难度自适应样本筛选器选择具有挑战性的样本。 |
| [^30] | [Fr\'echet Distance for Offline Evaluation of Information Retrieval Systems with Sparse Labels](https://arxiv.org/abs/2401.17543) | 该论文提出使用Fr\'echet距离来评估稀疏标签信息检索系统的性能。实验证明，在少量标签可用的情况下，Fr\'echet距离是一种有效的评估指标。 |
| [^31] | [RecDCL: Dual Contrastive Learning for Recommendation](https://arxiv.org/abs/2401.15635) | RecDCL提出了一个双重对比学习推荐框架，结合了批次对比学习（BCL）和特征对比学习（FCL），有助于消除冗余的解决方案，但又不会错过最优解。 |
| [^32] | [An Attentive Inductive Bias for Sequential Recommendation beyond the Self-Attention](https://arxiv.org/abs/2312.10325) | 提出了一种名为BSARec的新方法，超越了自注意力，在序列推荐中注入了归纳偏差，并集成了低频和高频信息以减轻过度平滑问题 |
| [^33] | [Enhancing Cross-domain Click-Through Rate Prediction via Explicit Feature Augmentation](https://arxiv.org/abs/2312.00078) | 通过跨领域增强网络（CDA）提出了一种更灵活、更高效的方法，通过显式特征增强来解决跨领域点击率预测中的知识转移问题。 |
| [^34] | [Anomaly detection in cross-country money transfer temporal networks](https://arxiv.org/abs/2311.14778) | 通过时序网络分析，本研究利用网络中心度量在跨国资金转移中实现了异常检测的有效性。 |
| [^35] | [Knowledge-Augmented Large Language Models for Personalized Contextual Query Suggestion](https://arxiv.org/abs/2311.06318) | 提出一种新颖且通用的方法，通过从用户与搜索引擎的交互历史中提取相关上下文来个性化大型语言模型的输出，尤其适用于改进网络搜索体验。 |
| [^36] | [On Image Search in Histopathology.](http://arxiv.org/abs/2401.08699) | 这篇论文综述了组织病理学图像搜索技术的最新发展，为计算病理学研究人员提供了简明的概述，旨在寻求有效、快速和高效的图像搜索方法。 |
| [^37] | [INTERS: Unlocking the Power of Large Language Models in Search with Instruction Tuning.](http://arxiv.org/abs/2401.06532) | 本研究探索了指令调优的方法，以增强大型语言模型在信息检索任务中的能力，通过引入一个新的指令调优数据集INTERS，涵盖了21个IR任务，该方法显著提升了性能。 |
| [^38] | [GraphPro: Graph Pre-training and Prompt Learning for Recommendation.](http://arxiv.org/abs/2311.16716) | GraphPro是一个结合了参数高效和动态图预训练与提示学习的框架，能够有效捕捉长期用户偏好和短期行为动态，从而在真实世界的推荐系统中提供准确和及时的推荐。 |
| [^39] | [Robust Training for Conversational Question Answering Models with Reinforced Reformulation Generation.](http://arxiv.org/abs/2310.13505) | 这项研究提出了一种新的框架REIGN，通过生成训练问题的改写，并使用深度强化学习来指导对话问答模型，增加模型对表面形式变化的鲁棒性，同时在不同的基准上进行零-shot应用。 |
| [^40] | [Decoupled Training: Return of Frustratingly Easy Multi-Domain Learning.](http://arxiv.org/abs/2309.10302) | 这篇论文提出了一种称为解耦训练（D-Train）的令人沮丧的、无超参数的多领域学习方法。该方法采用了一种三阶段的训练策略，首先进行预训练，然后在每个领域上进行后训练，最后进行头部微调，实现解耦训练以获得更好的性能。 |
| [^41] | [ReLLa: Retrieval-enhanced Large Language Models for Lifelong Sequential Behavior Comprehension in Recommendation.](http://arxiv.org/abs/2308.11131) | 本论文提出了一种名为ReLLa的检索增强大型语言模型框架，用于零样本和小样本推荐任务。通过语义用户行为检索（SUBR）来提取上下文中的有用信息，以改善LLMs的推荐性能。 |
| [^42] | [UniRecSys: A Unified Framework for Personalized, Group, Package, and Package-to-Group Recommendations.](http://arxiv.org/abs/2308.04247) | 这篇论文提出了一个新的统一推荐框架，可以处理个性化、群组、套餐和套餐到群组推荐这四种任务，填补了当前研究的空白部分。 |
| [^43] | [Towards Personalized Cold-Start Recommendation with Prompts.](http://arxiv.org/abs/2306.17256) | 本研究旨在解决个性化冷启动推荐问题，通过利用预训练语言模型的能力，将推荐过程转化为自然语言情感分析，提供适用于创业企业和用户参与历史不足的平台的个性化推荐。 |
| [^44] | [SE-PQA: Personalized Community Question Answering.](http://arxiv.org/abs/2306.16261) | 这个论文介绍了SE-PQA（个性化社区问题回答）的新资源，该资源包括超过1百万个查询和2百万个回答，并使用一系列丰富的特征模拟了流行社区问题回答平台的用户之间的社交互动。研究提供了用于社区问题回答任务的可复现基线方法，包括深度学习模型和个性化方法。 |
| [^45] | [GPT4Table: Can Large Language Models Understand Structured Table Data? A Benchmark and Empirical Study.](http://arxiv.org/abs/2305.13062) | 本文设计了一个基准测试来评估大型语言模型（LLMs）对结构化表格数据的理解能力，并发现不同的输入选择会对性能产生影响。在基准测试的基础上，提出了“自我增强”技术以改善理解能力。 |
| [^46] | [Cost-optimal Seeding Strategy During a Botanical Pandemic in Domesticated Fields.](http://arxiv.org/abs/2301.02817) | 这项研究提出了一种基于网格的经济最优播种策略，通过数学模型描述了在植物流行病期间农田作物的经济利润，可为农田主人和决策者提供指导。 |
| [^47] | [Result Diversification in Search and Recommendation: A Survey.](http://arxiv.org/abs/2212.14464) | 这项调研提出了一个统一的分类体系，用于将搜索和推荐中的多样化指标和方法进行分类。调研总结了搜索和推荐中的各种多样性问题，并展示了各种应用在搜索和推荐中的调研成果。未来的研究方向和挑战也被讨论。 |
| [^48] | [Spatio-Temporal Contrastive Learning Enhanced GNNs for Session-based Recommendation.](http://arxiv.org/abs/2209.11461) | 本研究提出了一种名为基于会话的推荐的新框架，通过利用对比学习技术的均匀性和对齐性特性，增强了GNNs对于基于会话的推荐的性能。 |

# 详细

[^1]: 基于图的检索器捕捉生物医学知识的长尾

    Graph-Based Retriever Captures the Long Tail of Biomedical Knowledge

    [https://arxiv.org/abs/2402.12352](https://arxiv.org/abs/2402.12352)

    基于Retrieval Augmented Generation (RAG)的图检索器被提议用来克服LLMs的局限，通过从外部数据集检索的上下文来增强提示信息，从而解决生物医学领域长尾知识的捕获挑战。

    

    大型语言模型(Large language models, LLMs)正在改变信息检索的方式，通过自然语言对话总结和展示大量知识。然而，LLMs倾向于突出训练集中最常见的信息片段，并忽视罕见的信息。在生物医学研究领域，最新的发现对于学术和工业界是至关重要的，但往往被大量不断增长的文献领域所掩盖(信息过载问题)。利用LLMs展现生物医学实体之间的新关联，如药物、基因、疾病，成为捕捉生物医学科学生产的长尾知识的挑战。

    arXiv:2402.12352v1 Announce Type: new  Abstract: Large language models (LLMs) are transforming the way information is retrieved with vast amounts of knowledge being summarized and presented via natural language conversations. Yet, LLMs are prone to highlight the most frequently seen pieces of information from the training set and to neglect the rare ones. In the field of biomedical research, latest discoveries are key to academic and industrial actors and are obscured by the abundance of an ever-increasing literature corpus (the information overload problem). Surfacing new associations between biomedical entities, e.g., drugs, genes, diseases, with LLMs becomes a challenge of capturing the long-tail knowledge of the biomedical scientific production. To overcome this challenge, Retrieval Augmented Generation (RAG) has been proposed to alleviate some of the shortcomings of LLMs by augmenting the prompts with context retrieved from external datasets. RAG methods typically select the conte
    
[^2]: 用大型语言模型的自然语言解释进行神经排序器的规模校准解释和排名

    Explain then Rank: Scale Calibration of Neural Rankers Using Natural Language Explanations from Large Language Models

    [https://arxiv.org/abs/2402.12276](https://arxiv.org/abs/2402.12276)

    本研究探讨了大型语言模型（LLMs）利用提供与规模校准相关的查询和文档对的不确定性测量的潜力，以解决神经排序器的规模校准问题。

    

    排名系统中的规模校准过程涉及调整排序器的输出，以使其与重要品质（如点击率或相关性）相对应，这对于反映现实价值以及提高系统的效果和可靠性至关重要。虽然已经研究了学习排序模型中的校准排序损失，但调整神经排序器的规模的特定问题，这些模型擅长处理文本信息，尚未得到充分研究。神经排序模型擅长处理文本数据，但将现有规模校准技术应用到这些模型会面临重大挑战，因为它们的复杂性和需要大量训练，往往导致次优结果。

    arXiv:2402.12276v1 Announce Type: new  Abstract: The process of scale calibration in ranking systems involves adjusting the outputs of rankers to correspond with significant qualities like click-through rates or relevance, crucial for mirroring real-world value and thereby boosting the system's effectiveness and reliability. Although there has been research on calibrated ranking losses within learning-to-rank models, the particular issue of adjusting the scale for neural rankers, which excel in handling textual information, has not been thoroughly examined. Neural ranking models are adept at processing text data, yet the application of existing scale calibration techniques to these models poses significant challenges due to their complexity and the intensive training they require, often resulting in suboptimal outcomes.   This study delves into the potential of large language models (LLMs) to provide uncertainty measurements for a query and document pair that correlate with the scale-c
    
[^3]: 对Instagram上波斯新闻机构的分析，基于词共现图的方法

    Analysis of Persian News Agencies on Instagram, A Words Co-occurrence Graph-based Approach

    [https://arxiv.org/abs/2402.12272](https://arxiv.org/abs/2402.12272)

    本研究通过对Instagram上波斯新闻机构的分析，基于词共现图的方法，展示了可以使用无监督方法实现对波斯语Instagram帖子的高精度关键词提取。

    

    互联网的崛起和数据的指数增长使得手动数据摘要和分析变得具有挑战性。 Instagram社交网络是伊朗广泛利用的一种重要社交网络，用于各个年龄段之间的信息共享和交流。 Instagram的固有结构，以其文本丰富的内容和类似图形的数据表示为特征，使得可以利用文本和图形处理技术进行数据分析。这些网络的度分布表现出无标度特征，表明了非随机的增长模式。最近，词共现吸引了多个学科的研究人员的注意，因为它的简单性和实用性。关键词提取是自然语言处理中的关键任务。在这项研究中，我们证明了可以使用无监督方法实现对波斯语Instagram帖子的高精度关键词提取。

    arXiv:2402.12272v1 Announce Type: cross  Abstract: The rise of the Internet and the exponential increase in data have made manual data summarization and analysis a challenging task. Instagram social network is a prominent social network widely utilized in Iran for information sharing and communication across various age groups. The inherent structure of Instagram, characterized by its text-rich content and graph-like data representation, enables the utilization of text and graph processing techniques for data analysis purposes. The degree distributions of these networks exhibit scale-free characteristics, indicating non-random growth patterns. Recently, word co-occurrence has gained attention from researchers across multiple disciplines due to its simplicity and practicality. Keyword extraction is a crucial task in natural language processing. In this study, we demonstrated that high-precision extraction of keywords from Instagram posts in the Persian language can be achieved using uns
    
[^4]: 考虑异质性的跨校选修课推荐：一种混合联邦方法

    Heterogeneity-aware Cross-school Electives Recommendation: a Hybrid Federated Approach

    [https://arxiv.org/abs/2402.12202](https://arxiv.org/abs/2402.12202)

    提出了一种考虑异质性的混合联邦推荐系统，用于解决跨校选修课程推荐问题，通过构建异构图和设计注意机制来捕捉异质性感知表示，并在联邦方案下训练个别学校模型以推荐量身定制的选修课程。

    

    在现代教育时代，解决跨校学习者多样性至关重要，尤其是在个性化推荐系统中为选修课程选择。然而，隐私问题经常限制了跨校数据共享，这限制了现有方法对稀疏数据进行建模和有效处理异质性的能力，最终导致子优化的推荐。为此，我们提出了HFRec，一种考虑异质性的混合联邦推荐系统，旨在为跨校选修课程推荐。该模型为每个学校构建了异构图，将学生之间的各种交互和历史行为融入其中，以整合上下文和内容信息。我们设计了一个注意机制来捕捉异质性感知表示。此外，在一个联邦方案下，我们使用自适应学习设置来训练个别学校模型以推荐量身定制的选修课程。

    arXiv:2402.12202v1 Announce Type: cross  Abstract: In the era of modern education, addressing cross-school learner diversity is crucial, especially in personalized recommender systems for elective course selection. However, privacy concerns often limit cross-school data sharing, which hinders existing methods' ability to model sparse data and address heterogeneity effectively, ultimately leading to suboptimal recommendations. In response, we propose HFRec, a heterogeneity-aware hybrid federated recommender system designed for cross-school elective course recommendations. The proposed model constructs heterogeneous graphs for each school, incorporating various interactions and historical behaviors between students to integrate context and content information. We design an attention mechanism to capture heterogeneity-aware representations. Moreover, under a federated scheme, we train individual school-based models with adaptive learning settings to recommend tailored electives. Our HFRec
    
[^5]: 对抽取式知识图谱总结的调查：应用、方法、评估和未来方向

    A Survey on Extractive Knowledge Graph Summarization: Applications, Approaches, Evaluation, and Future Directions

    [https://arxiv.org/abs/2402.12001](https://arxiv.org/abs/2402.12001)

    本文调查了抽取式知识图谱总结的应用并提出了方法分类，为未来方向提供了重要参考。

    

    随着大型知识图谱（KGs）不断增长，抽取式KG总结成为一项热门任务。旨在提炼具有浓缩信息的紧凑子图，有助于各种下游基于KG的任务。在这篇调查论文中，我们是首批对其应用提供系统概述并从跨学科研究中定义现有方法的分类法之一。基于我们广泛而比较的评论，未来方向也已铺开。

    arXiv:2402.12001v1 Announce Type: new  Abstract: With the continuous growth of large Knowledge Graphs (KGs), extractive KG summarization becomes a trending task. Aiming at distilling a compact subgraph with condensed information, it facilitates various downstream KG-based tasks. In this survey paper, we are among the first to provide a systematic overview of its applications and define a taxonomy for existing methods from its interdisciplinary studies. Future directions are also laid out based on our extensive and comparative review.
    
[^6]: 在检索增强生成环境中评估联合搜索系统（FeB4RAG）

    FeB4RAG: Evaluating Federated Search in the Context of Retrieval Augmented Generation

    [https://arxiv.org/abs/2402.11891](https://arxiv.org/abs/2402.11891)

    提出了FeB4RAG，这是一个专门为检索增强生成框架内的联合搜索设计的新数据集。

    

    联合搜索系统聚合来自多个搜索引擎的结果，选择合适的来源以增强结果质量并与用户意图保持一致。随着检索增强生成（RAG）管道的日益流行，联合搜索可以在跨异构数据源中搜索相关信息以生成明智的响应起到关键作用。然而，现有数据集（如过去的TREC FedWeb跟踪中开发的数据集）过时了，缺乏对现代信息检索挑战的代表性。为了弥补这一差距，我们提出了FeB4RAG，这是一个专门为RAG框架内的联合搜索而设计的新数据集。这个数据集源自广泛使用的beir基准集的16个子集合，包括了为聊天机器人应用量身定制的790个信息请求（类似于对话查询），以及每个资源返回的最佳结果以及相关的LLM-der

    arXiv:2402.11891v1 Announce Type: cross  Abstract: Federated search systems aggregate results from multiple search engines, selecting appropriate sources to enhance result quality and align with user intent. With the increasing uptake of Retrieval-Augmented Generation (RAG) pipelines, federated search can play a pivotal role in sourcing relevant information across heterogeneous data sources to generate informed responses. However, existing datasets, such as those developed in the past TREC FedWeb tracks, predate the RAG paradigm shift and lack representation of modern information retrieval challenges. To bridge this gap, we present FeB4RAG, a novel dataset specifically designed for federated search within RAG frameworks. This dataset, derived from 16 sub-collections of the widely used \beir benchmarking collection, includes 790 information requests (akin to conversational queries) tailored for chatbot applications, along with top results returned by each resource and associated LLM-der
    
[^7]: TriSampler: 一种更好的稠密检索负采样原则

    TriSampler: A Better Negative Sampling Principle for Dense Retrieval

    [https://arxiv.org/abs/2402.11855](https://arxiv.org/abs/2402.11855)

    TriSampler提出了一种新颖的基于准三角原则的负采样框架，能够在稠密检索中选择性地采样更具信息量的负例

    

    arXiv:2402.11855v1 公告类型: 新摘要: 负采样作为稠密检索中的一个关键技术，是训练有效检索模型和明显影响检索性能的基本要素。虽然现有的负采样方法通过利用困难的负例取得了可观的进展，但仍然缺乏一个全面的指导原则，用于构建负候选项和设计负采样分布。为填补这一空白，我们对稠密检索中的负采样进行了理论分析。这种探索最终揭示了准三角原则，这是一个阐明查询、正文档和负文档之间类似三角形相互作用的新框架。在这一指导原则的推动下，我们引入了TriSampler，一个简单而高效的负采样方法。TriSampler的关键点在于其能够在规定的约束条件下选择性地采样更具信息量的负例。

    arXiv:2402.11855v1 Announce Type: new  Abstract: Negative sampling stands as a pivotal technique in dense retrieval, essential for training effective retrieval models and significantly impacting retrieval performance. While existing negative sampling methods have made commendable progress by leveraging hard negatives, a comprehensive guiding principle for constructing negative candidates and designing negative sampling distributions is still lacking. To bridge this gap, we embark on a theoretical analysis of negative sampling in dense retrieval. This exploration culminates in the unveiling of the quasi-triangular principle, a novel framework that elucidates the triangular-like interplay between query, positive document, and negative document. Fueled by this guiding principle, we introduce TriSampler, a straightforward yet highly effective negative sampling method. The keypoint of TriSampler lies in its ability to selectively sample more informative negatives within a prescribed constra
    
[^8]: 询问最佳问题：将大型语言模型与检索器偏好在会话搜索中对齐

    Ask Optimal Questions: Aligning Large Language Models with Retriever's Preference in Conversational Search

    [https://arxiv.org/abs/2402.11827](https://arxiv.org/abs/2402.11827)

    提出了RetPO框架，通过优化语言模型对搜索查询进行重构，以符合目标检索系统的偏好，并构建了一个大型数据集RF Collection，用于收集检索结果作为检索器的偏好。

    

    会话式搜索与单轮检索任务不同，需要理解对话上下文中的当前问题。常见的“重写-然后检索”的方法旨在将问题去上下文化，使其对现成的检索器自给自足，但大多数现有方法由于能力有限而产生次优的查询重写，无法充分利用来自检索结果的信号。为了克服这一限制，我们提出了一种新颖的框架RetPO（检索器偏好优化），旨在优化语言模型（LM）以符合目标检索系统的重写搜索查询的偏好。该过程始于提示大型LM生成各种潜在重写，然后收集这些重写的检索性能作为检索器的偏好。通过该过程，我们构建了一个名为RF塑集的大型数据集，其中包含对超过410K个查询的检索器反馈。

    arXiv:2402.11827v1 Announce Type: cross  Abstract: Conversational search, unlike single-turn retrieval tasks, requires understanding the current question within a dialogue context. The common approach of rewrite-then-retrieve aims to decontextualize questions to be self-sufficient for off-the-shelf retrievers, but most existing methods produce sub-optimal query rewrites due to the limited ability to incorporate signals from the retrieval results. To overcome this limitation, we present a novel framework RetPO (Retriever's Preference Optimization), which is designed to optimize a language model (LM) for reformulating search queries in line with the preferences of the target retrieval systems. The process begins by prompting a large LM to produce various potential rewrites and then collects retrieval performance for these rewrites as the retrievers' preferences. Through the process, we construct a large-scale dataset called RF collection, containing Retrievers' Feedback on over 410K quer
    
[^9]: 大型语言模型对图形召回的微结构和准确性

    Microstructures and Accuracy of Graph Recall by Large Language Models

    [https://arxiv.org/abs/2402.11821](https://arxiv.org/abs/2402.11821)

    本研究首次系统研究了大型语言模型对图形召回的准确性和偏见微结构，探讨了它们与人类的异同以及对其他图形推理任务的影响。

    

    图形数据对许多应用至关重要，其中很多数据以文本格式描述关系。因此，准确地召回和编码先前文本中描述的图形是大型语言模型(LLMs)需要展示的基本但关键能力，以执行涉及图形结构信息的推理任务。人类在图形召回方面的表现已被认知科学家研究了几十年，发现其经常呈现与人类处理社会关系一致的某些结构性偏见模式。然而，迄今为止，我们很少了解LLMs在类似图形召回任务中的行为：它们召回的图形是否也呈现某些偏见模式，如果是，它们与人类的表现有何不同并如何影响其他图形推理任务？在这项研究中，我们进行了第一次对LLMs进行图形召回的系统研究，研究其准确性和偏见微结构（局部结构）。

    arXiv:2402.11821v1 Announce Type: cross  Abstract: Graphs data is crucial for many applications, and much of it exists in the relations described in textual format. As a result, being able to accurately recall and encode a graph described in earlier text is a basic yet pivotal ability that LLMs need to demonstrate if they are to perform reasoning tasks that involve graph-structured information. Human performance at graph recall by has been studied by cognitive scientists for decades, and has been found to often exhibit certain structural patterns of bias that align with human handling of social relationships. To date, however, we know little about how LLMs behave in analogous graph recall tasks: do their recalled graphs also exhibit certain biased patterns, and if so, how do they compare with humans and affect other graph reasoning tasks? In this work, we perform the first systematical study of graph recall by LLMs, investigating the accuracy and biased microstructures (local structura
    
[^10]: 揭示魔法：探究检索增强生成中的注意力精炼

    Unveiling the Magic: Investigating Attention Distillation in Retrieval-augmented Generation

    [https://arxiv.org/abs/2402.11794](https://arxiv.org/abs/2402.11794)

    本文揭示了检索增强生成中的注意力精炼的成功机制，并提出了优化模型训练方法的指标.

    

    检索增强生成框架可以通过实时知识更新来解决大型语言模型的局限，以实现更准确的答案。在检索增强模型的训练阶段中，一种高效的方法是注意力精炼，它使用注意力分数作为监督信号，而不是手动注释的查询文档对。尽管注意力精炼越来越受欢迎，但在它成功背后的详细机制仍未被探索，特别是它利用以受益于训练的具体模式。在本文中，我们通过对注意力精炼工作流程的全面回顾，识别影响检索增强语言模型学习质量的关键因素来填补这一空白。我们进一步提出了优化模型训练方法和避免低效训练的指标。

    arXiv:2402.11794v1 Announce Type: new  Abstract: Retrieval-augmented generation framework can address the limitations of large language models by enabling real-time knowledge updates for more accurate answers. An efficient way in the training phase of retrieval-augmented models is attention distillation, which uses attention scores as a supervision signal instead of manually annotated query-document pairs. Despite its growing popularity, the detailed mechanisms behind the success of attention distillation remain unexplored, particularly the specific patterns it leverages to benefit training. In this paper, we address this gap by conducting a comprehensive review of attention distillation workflow and identifying key factors influencing the learning quality of retrieval-augmented language models. We further propose indicators for optimizing models' training methods and avoiding ineffective training.
    
[^11]: 大型语言模型用于词干提取：承诺、风险和失败

    Large Language Models for Stemming: Promises, Pitfalls and Failures

    [https://arxiv.org/abs/2402.11757](https://arxiv.org/abs/2402.11757)

    本文研究了使用大型语言模型来进行词干提取的有前途的想法，提出了三种不同的方法，每种方法在计算成本、有效性和稳健性方面具有不同的权衡。

    

    文本词干提取是一种自然语言处理技术，用于将单词缩减为其基本形式，也称为根形式。传统的词干提取方法仅关注单个术语，忽视了上下文信息的丰富性。因此，在本文中，我们研究了利用大型语言模型（LLMs）通过利用其上下文理解能力来提取词干的有前途的想法。在这方面，我们确定了三种不同的权衡方案：（1）使用LLMs对集合的词汇进行提取，即出现在集合中的唯一单词集合（词汇提取），（2）将LLMs用于单独词根提取 (上下文提取)，(3) 使用LLMs从每个文档中提取

    arXiv:2402.11757v1 Announce Type: cross  Abstract: Text stemming is a natural language processing technique that is used to reduce words to their base form, also known as the root form. The use of stemming in IR has been shown to often improve the effectiveness of keyword-matching models such as BM25. However, traditional stemming methods, focusing solely on individual terms, overlook the richness of contextual information. Recognizing this gap, in this paper, we investigate the promising idea of using large language models (LLMs) to stem words by leveraging its capability of context understanding. With this respect, we identify three avenues, each characterised by different trade-offs in terms of computational cost, effectiveness and robustness : (1) use LLMs to stem the vocabulary for a collection, i.e., the set of unique words that appear in the collection (vocabulary stemming), (2) use LLMs to stem each document separately (contextual stemming), and (3) use LLMs to extract from eac
    
[^12]: 将大型语言模型用作冷启动物品推荐的数据增强器

    Large Language Models as Data Augmenters for Cold-Start Item Recommendation

    [https://arxiv.org/abs/2402.11724](https://arxiv.org/abs/2402.11724)

    将大型语言模型用作数据增强器，以解决传统推荐系统在冷启动物品推荐方面的困难。通过结合LLM推断用户对冷启动物品的偏好并将增强的训练信号纳入推荐模型学习，实现对冷启动物品的显著提升。

    

    arXiv:2402.11724v1 公告类型: 新  摘要: LLM的推理和泛化能力可以帮助我们更好地理解用户偏好和物品特征，为增强推荐系统提供了令人兴奋的前景。虽然在用户物品交互丰富时很有效，但传统的推荐系统在没有历史交互的冷启动物品推荐方面存在困难。为了解决这个问题，我们提出利用LLM作为数据增强器，以在训练过程中弥合对冷启动物品的知识差距。我们利用LLM根据用户历史行为的文本描述和新物品描述推断用户对冷启动物品的偏好。然后将增强的训练信号通过辅助配对损失纳入到学习下游推荐模型中。通过在公共亚马逊数据集上的实验，我们证明LLM可以有效地增强冷启动物品的训练信号，从而显著提高了c

    arXiv:2402.11724v1 Announce Type: new  Abstract: The reasoning and generalization capabilities of LLMs can help us better understand user preferences and item characteristics, offering exciting prospects to enhance recommendation systems. Though effective while user-item interactions are abundant, conventional recommendation systems struggle to recommend cold-start items without historical interactions. To address this, we propose utilizing LLMs as data augmenters to bridge the knowledge gap on cold-start items during training. We employ LLMs to infer user preferences for cold-start items based on textual description of user historical behaviors and new item descriptions. The augmented training signals are then incorporated into learning the downstream recommendation models through an auxiliary pairwise loss. Through experiments on public Amazon datasets, we demonstrate that LLMs can effectively augment the training signals for cold-start items, leading to significant improvements in c
    
[^13]: 搜索引擎在ChatGPT之后：生成式人工智能如何降低搜索的可靠性

    Search Engines Post-ChatGPT: How Generative Artificial Intelligence Could Make Search Less Reliable

    [https://arxiv.org/abs/2402.11707](https://arxiv.org/abs/2402.11707)

    讨论了生成式人工智能对搜索引擎的影响，指出了其可能导致的可靠性问题，强调了信息来源不透明和内容正确性等挑战。

    

    在这篇评论中，我们讨论了搜索引擎的发展性质，因为它们开始生成、索引和分发由生成式人工智能（GenAI）创建的内容。我们的讨论突出了在GenAI整合的早期阶段面临的挑战，特别是围绕事实上的不一致性和偏见。我们讨论了GenAI产生的输出带来了无端的可信度，并降低了透明度和信息来源的能力。此外，搜索引擎已经用含有错误的生成内容回答查询，进一步模糊了信息来源，影响了信息生态系统的完整性。我们认为所有这些因素可能降低搜索引擎的可靠性。最后，我们总结了一些活跃的研究方向和未解问题。

    arXiv:2402.11707v1 Announce Type: cross  Abstract: In this commentary, we discuss the evolving nature of search engines, as they begin to generate, index, and distribute content created by generative artificial intelligence (GenAI). Our discussion highlights challenges in the early stages of GenAI integration, particularly around factual inconsistencies and biases. We discuss how output from GenAI carries an unwarranted sense of credibility, while decreasing transparency and sourcing ability. Furthermore, search engines are already answering queries with error-laden, generated content, further blurring the provenance of information and impacting the integrity of the information ecosystem. We argue how all these factors could reduce the reliability of search engines. Finally, we summarize some of the active research directions and open questions.
    
[^14]: 用假用户对联邦推荐系统进行毒化

    Poisoning Federated Recommender Systems with Fake Users

    [https://arxiv.org/abs/2402.11637](https://arxiv.org/abs/2402.11637)

    本研究介绍了一种无需额外信息的新型基于假用户的毒化攻击方法，用于在联邦推荐系统中执行推广攻击。

    

    联邦推荐是联邦学习中一个显著的用例，但仍然容易受到各种攻击，从用户到服务器端的漏洞。毒化攻击在用户端攻击中特别引人注目，因为参与者上传恶意模型更新来欺骗全局模型，通常意图提升或降低特定目标项。本研究探讨了在联邦推荐系统中执行推广攻击的策略。当前对联邦推荐系统的毒化攻击通常依赖于额外信息，如真实用户的本地训练数据或物品流行度。然而，攻击者很难获得这些信息。因此，有必要开发一种攻击，除了从服务器获取的物品嵌入之外，不需要额外信息。在本文中，我们介绍了一种名为PoisonFRS的新型基于假用户的毒化攻击，用于促销

    arXiv:2402.11637v1 Announce Type: cross  Abstract: Federated recommendation is a prominent use case within federated learning, yet it remains susceptible to various attacks, from user to server-side vulnerabilities. Poisoning attacks are particularly notable among user-side attacks, as participants upload malicious model updates to deceive the global model, often intending to promote or demote specific targeted items. This study investigates strategies for executing promotion attacks in federated recommender systems.   Current poisoning attacks on federated recommender systems often rely on additional information, such as the local training data of genuine users or item popularity. However, such information is challenging for the potential attacker to obtain. Thus, there is a need to develop an attack that requires no extra information apart from item embeddings obtained from the server. In this paper, we introduce a novel fake user based poisoning attack named PoisonFRS to promote the
    
[^15]: 用户参与的交互式服装推荐系统

    Interactive Garment Recommendation with User in the Loop

    [https://arxiv.org/abs/2402.11627](https://arxiv.org/abs/2402.11627)

    提出一种交互式服装推荐系统，在推荐服装同时实时构建用户资料，通过强化学习Agent吸收用户反馈改善推荐，能够最大化用户满意度。

    

    推荐时尚物品通常利用丰富的用户资料，并根据过去的历史和以前的购买做出有针对性的建议。本文假设对用户没有先前知识。我们提出通过集成用户反馈实时构建用户资料，在推荐互补物品以组成服装时。我们提出了一种强化学习Agent，能够建议合适的服装并吸收用户反馈以改善推荐并最大化用户满意度。为了训练这样的模型，我们求助于一个代理模型，以便在训练循环中模拟获得用户反馈。我们在IQON3000时尚数据集上进行实验，发现基于强化学习的Agent能够通过考虑个人偏好来改善其推荐。此外，这样的任务对于无强化的模型来说是困难的，他们无法利用用户反馈。

    arXiv:2402.11627v1 Announce Type: cross  Abstract: Recommending fashion items often leverages rich user profiles and makes targeted suggestions based on past history and previous purchases. In this paper, we work under the assumption that no prior knowledge is given about a user. We propose to build a user profile on the fly by integrating user reactions as we recommend complementary items to compose an outfit. We present a reinforcement learning agent capable of suggesting appropriate garments and ingesting user feedback so to improve its recommendations and maximize user satisfaction. To train such a model, we resort to a proxy model to be able to simulate having user feedback in the training loop. We experiment on the IQON3000 fashion dataset and we find that a reinforcement learning-based agent becomes capable of improving its recommendations by taking into account personal preferences. Furthermore, such task demonstrated to be hard for non-reinforcement models, that cannot exploit
    
[^16]: 元认知检索增强型大型语言模型

    Metacognitive Retrieval-Augmented Large Language Models

    [https://arxiv.org/abs/2402.11626](https://arxiv.org/abs/2402.11626)

    本文介绍了MetaRAG，一种结合了检索增强生成过程与元认知的方法，通过元认知调节流程，使模型具有监视、评估和规划其响应策略的能力，增强了其内省推理能力。

    

    由于其在生成事实内容方面的高效性，检索增强生成已经成为自然语言处理中的核心。 传统方法使用单次检索，而最近更倾向于多次检索以执行多跳推理任务。 然而，这些策略受到预定义推理步骤的限制，可能导致响应生成的不准确性。 本文介绍了MetaRAG，一种结合了检索增强生成过程与元认知的方法。 借鉴认知心理学，元认知使实体能够自我反思并批判性评估其认知过程。 通过整合这一点，MetaRAG使模型能够监视、评估和规划其响应策略，增强其内省推理能力。 通过三步元认知调节流程，模型能够识别初始认知响应中的不足之处，并加以修正。

    arXiv:2402.11626v1 Announce Type: new  Abstract: Retrieval-augmented generation have become central in natural language processing due to their efficacy in generating factual content. While traditional methods employ single-time retrieval, more recent approaches have shifted towards multi-time retrieval for multi-hop reasoning tasks. However, these strategies are bound by predefined reasoning steps, potentially leading to inaccuracies in response generation. This paper introduces MetaRAG, an approach that combines the retrieval-augmented generation process with metacognition. Drawing from cognitive psychology, metacognition allows an entity to self-reflect and critically evaluate its cognitive processes. By integrating this, MetaRAG enables the model to monitor, evaluate, and plan its response strategies, enhancing its introspective reasoning abilities. Through a three-step metacognitive regulation pipeline, the model can identify inadequacies in initial cognitive responses and fixes t
    
[^17]: 基于邻域增强的监督对比学习用于协同过滤

    Neighborhood-Enhanced Supervised Contrastive Learning for Collaborative Filtering

    [https://arxiv.org/abs/2402.11523](https://arxiv.org/abs/2402.11523)

    本文提出了一种基于邻域增强的监督对比学习方法，通过将锚节点的协作邻居视为正样本，有效解决了协同过滤中数据稀疏的问题

    

    尽管在推荐任务中有效，协同过滤（CF）技术面临着数据稀疏的挑战。研究人员已经开始利用对比学习引入额外的自监督信号来解决这一问题。然而，这种方法往往会无意中将目标用户/项目与他们的协作邻居分开，从而限制其有效性。为了应对这一挑战，我们提出了一个解决方案，将锚节点的协作邻居视为最终目标损失函数中的正样本。本文专注于开发两个独特的监督对比损失函数，有效结合了监督信号和对比损失。我们通过梯度视角分析了我们提出的损失函数，证明了不同的正样本同时影响更新锚节点的嵌入。这些样本的影响取决于它们与锚节点和负样本的相似性。

    arXiv:2402.11523v1 Announce Type: cross  Abstract: While effective in recommendation tasks, collaborative filtering (CF) techniques face the challenge of data sparsity. Researchers have begun leveraging contrastive learning to introduce additional self-supervised signals to address this. However, this approach often unintentionally distances the target user/item from their collaborative neighbors, limiting its efficacy. In response, we propose a solution that treats the collaborative neighbors of the anchor node as positive samples within the final objective loss function. This paper focuses on developing two unique supervised contrastive loss functions that effectively combine supervision signals with contrastive loss. We analyze our proposed loss functions through the gradient lens, demonstrating that different positive samples simultaneously influence updating the anchor node's embeddings. These samples' impact depends on their similarities to the anchor node and the negative sample
    
[^18]: 模式透明的顺序推荐

    Pattern-wise Transparent Sequential Recommendation

    [https://arxiv.org/abs/2402.11480](https://arxiv.org/abs/2402.11480)

    提出了一种模式透明的顺序推荐框架，通过将项目序列分解为多级模式并在概率空间中量化每个模式对结果的贡献，实现了透明的决策过程。

    

    透明的决策过程对于开发可靠和值得信赖的推荐系统至关重要。对于顺序推荐来说，意味着模型能够识别关键项目作为其推荐结果的理由。然而，同时实现模型透明度和推荐性能是具有挑战性的，特别是对于将整个项目序列作为输入而不加筛选的模型而言。在本文中，我们提出了一种名为PTSR的可解释框架，它实现了一种模式透明的决策过程。它将项目序列分解为多级模式，这些模式作为整个推荐过程的原子单元。每个模式对结果的贡献在概率空间中得到量化。通过精心设计的模式加权校正，即使在没有真实关键模式的情况下，也能学习模式的贡献。最终推荐

    arXiv:2402.11480v1 Announce Type: new  Abstract: A transparent decision-making process is essential for developing reliable and trustworthy recommender systems. For sequential recommendation, it means that the model can identify critical items asthe justifications for its recommendation results. However, achieving both model transparency and recommendation performance simultaneously is challenging, especially for models that take the entire sequence of items as input without screening. In this paper,we propose an interpretable framework (named PTSR) that enables a pattern-wise transparent decision-making process. It breaks the sequence of items into multi-level patterns that serve as atomic units for the entire recommendation process. The contribution of each pattern to the outcome is quantified in the probability space. With a carefully designed pattern weighting correction, the pattern contribution can be learned in the absence of ground-truth critical patterns. The final recommended
    
[^19]: 基于知识图的自适应传播会话推荐

    Knowledge Graph-based Session Recommendation with Adaptive Propagation

    [https://arxiv.org/abs/2402.11302](https://arxiv.org/abs/2402.11302)

    提出了一种基于知识图的会话推荐方法，通过会话自适应传播来跨不同会话捕获全局信息。

    

    会话型推荐系统（SBRSs）根据用户的历史活动预测用户的下一个互动项目。虽然大多数SBRSs在每个会话中仅局部捕获购买意图，但跨不同会话捕获项目的全局信息对于表征其一般特性至关重要。我们提出了一种基于知识图的会话推荐方法，采用会话自适应传播，以解决先前方法中存在的一些局限性。

    arXiv:2402.11302v1 Announce Type: new  Abstract: Session-based recommender systems (SBRSs) predict users' next interacted items based on their historical activities. While most SBRSs capture purchasing intentions locally within each session, capturing items' global information across different sessions is crucial in characterizing their general properties. Previous works capture this cross-session information by constructing graphs and incorporating neighbor information. However, this incorporation cannot vary adaptively according to the unique intention of each session, and the constructed graphs consist of only one type of user-item interaction. To address these limitations, we propose knowledge graph-based session recommendation with session-adaptive propagation. Specifically, we build a knowledge graph by connecting items with multi-typed edges to characterize various user-item interactions. Then, we adaptively aggregate items' neighbor information considering user intention within
    
[^20]: 镜像梯度：通过探索平缓局部最小值实现鲁棒的多模式推荐系统

    Mirror Gradient: Towards Robust Multimodal Recommender Systems via Exploring Flat Local Minima

    [https://arxiv.org/abs/2402.11262](https://arxiv.org/abs/2402.11262)

    本文从平缓局部最小值的角度分析多模态推荐系统，并提出了一种名为镜像梯度（MG）的梯度策略，可以增强模型的稳健性，缓解来自多模态信息输入的不稳定风险。

    

    多模态推荐系统利用各种信息来建模用户偏好和物品特征，帮助用户发现符合其兴趣的物品。在推荐系统中整合多模态信息可以缓解固有的挑战，例如数据稀疏问题和冷启动问题。然而，它同时会放大来自多模态信息输入的某些风险，如信息调整风险和固有噪声风险。这些风险对推荐模型的稳健性构成重要挑战。在本文中，我们通过平缓局部最小值的新颖视角分析多模态推荐系统，并提出一种简洁而有效的梯度策略，称为镜像梯度（MG）。这种策略可以在优化过程中隐式增强模型的稳健性，缓解由多模态信息输入引起的不稳定风险。

    arXiv:2402.11262v1 Announce Type: cross  Abstract: Multimodal recommender systems utilize various types of information to model user preferences and item features, helping users discover items aligned with their interests. The integration of multimodal information mitigates the inherent challenges in recommender systems, e.g., the data sparsity problem and cold-start issues. However, it simultaneously magnifies certain risks from multimodal information inputs, such as information adjustment risk and inherent noise risk. These risks pose crucial challenges to the robustness of recommendation models. In this paper, we analyze multimodal recommender systems from the novel perspective of flat local minima and propose a concise yet effective gradient strategy called Mirror Gradient (MG). This strategy can implicitly enhance the model's robustness during the optimization process, mitigating instability risks arising from multimodal information inputs. We also provide strong theoretical evide
    
[^21]: 探索ChatGPT在下一代信息检索中的应用：机遇与挑战

    Exploring ChatGPT for Next-generation Information Retrieval: Opportunities and Challenges

    [https://arxiv.org/abs/2402.11203](https://arxiv.org/abs/2402.11203)

    ChatGPT作为信息检索领域的关键技术，不断挑战传统范式，带来了新的机遇和挑战，同时超越了之前的GPT-3模型。

    

    人工智能（AI）的快速发展凸显了ChatGPT作为信息检索（IR）领域中的关键技术。与之前的模型不同，ChatGPT提供了显著的好处，吸引了行业和学术界的关注。一些人认为ChatGPT是一项开创性的创新，而另一些人将其成功归因于产品开发和市场策略的有效整合。ChatGPT的出现，以及与OpenAI的GPT-4一起，标志着生成式AI的新阶段，产生的内容与训练样本有所不同，并超越了以往的GPT-3模型的能力。与信息检索任务中的传统监督学习方法不同，ChatGPT挑战了现有的范式，带来了关于文本质量保证、模型偏差和效率方面的新挑战和机遇。本文旨在研究ChatGPT对信息检索任务的影响，并提供

    arXiv:2402.11203v1 Announce Type: cross  Abstract: The rapid advancement of artificial intelligence (AI) has highlighted ChatGPT as a pivotal technology in the field of information retrieval (IR). Distinguished from its predecessors, ChatGPT offers significant benefits that have attracted the attention of both the industry and academic communities. While some view ChatGPT as a groundbreaking innovation, others attribute its success to the effective integration of product development and market strategies. The emergence of ChatGPT, alongside GPT-4, marks a new phase in Generative AI, generating content that is distinct from training examples and exceeding the capabilities of the prior GPT-3 model by OpenAI. Unlike the traditional supervised learning approach in IR tasks, ChatGPT challenges existing paradigms, bringing forth new challenges and opportunities regarding text quality assurance, model bias, and efficiency. This paper seeks to examine the impact of ChatGPT on IR tasks and offe
    
[^22]: 在电子商务搜索中实现查询重构建模的可扩展性和可扩展性

    Towards Scalability and Extensibility of Query Reformulation Modeling in E-commerce Search

    [https://arxiv.org/abs/2402.11202](https://arxiv.org/abs/2402.11202)

    本研究致力于克服在电子商务搜索中进行查询重构时面临的可扩展性和可扩展性挑战，构建一个查询重构解决方案，即使在有限的情况下也能有效运行。

    

    顾客行为数据在电子商务搜索系统中起着重要作用。然而，在少见查询的情况下，关联的行为数据往往稀疏且嘈杂，无法为搜索机制提供足够的支持。为解决这一挑战，引入了查询重构的概念。它建议少见查询可以利用具有类似含义的热门对应查询的行为模式。在亚马逊产品搜索中，查询重构已显示出在提高搜索相关性和增强整体收入方面的有效性。然而，将这种方法调整为适用于流量较低且复杂的多语言环境下运营的较小或新兴企业在可扩展性和可扩展性方面存在挑战。本研究旨在通过构建一个查询重构方案来克服这一挑战，即使面对有限的交易量也能有效运行。

    arXiv:2402.11202v1 Announce Type: new  Abstract: Customer behavioral data significantly impacts e-commerce search systems. However, in the case of less common queries, the associated behavioral data tends to be sparse and noisy, offering inadequate support to the search mechanism. To address this challenge, the concept of query reformulation has been introduced. It suggests that less common queries could utilize the behavior patterns of their popular counterparts with similar meanings. In Amazon product search, query reformulation has displayed its effectiveness in improving search relevance and bolstering overall revenue. Nonetheless, adapting this method for smaller or emerging businesses operating in regions with lower traffic and complex multilingual settings poses the challenge in terms of scalability and extensibility. This study focuses on overcoming this challenge by constructing a query reformulation solution capable of functioning effectively, even when faced with limited tra
    
[^23]: 基于问答的全面中文电子健康记录信息提取流水线

    A Question Answering Based Pipeline for Comprehensive Chinese EHR Information Extraction

    [https://arxiv.org/abs/2402.11177](https://arxiv.org/abs/2402.11177)

    提出了一种基于问答的新方法，自动生成训练数据，用于QA模型的迁移学习，通过预处理模块解决了传统方法无法处理的信息提取类型挑战，实现了在电子健康记录中的信息提取任务，表现出色并能有效应对少样本或零样本情况。

    

    电子健康记录（EHRs）对研究和应用具有重要价值。作为一种新的信息提取方式，问答（QA）可以提取比传统方法更灵活的信息，且更易于临床研究人员使用，但其进展受到标注数据稀缺的阻碍。在本文中，我们提出了一种新颖的方法，可自动生成训练数据，用于QA模型的迁移学习。我们的流水线集成了一个预处理模块，处理了与提取型QA框架不太兼容的提取类型所带来的挑战，包括具有不连续答案和多对一关系的情况。所得的QA模型在EHRs中的信息提取子任务上表现出色，能够有效处理包含是非问题的少样本或零样本设置。案例研究和消融研究证明了每个组件的必要性。

    arXiv:2402.11177v1 Announce Type: new  Abstract: Electronic health records (EHRs) hold significant value for research and applications. As a new way of information extraction, question answering (QA) can extract more flexible information than conventional methods and is more accessible to clinical researchers, but its progress is impeded by the scarcity of annotated data. In this paper, we propose a novel approach that automatically generates training data for transfer learning of QA models. Our pipeline incorporates a preprocessing module to handle challenges posed by extraction types that are not readily compatible with extractive QA frameworks, including cases with discontinuous answers and many-to-one relationships. The obtained QA model exhibits excellent performance on subtasks of information extraction in EHRs, and it can effectively handle few-shot or zero-shot settings involving yes-no questions. Case studies and ablation studies demonstrate the necessity of each component in 
    
[^24]: 基于基础模型的推荐系统：调查与新视角

    Foundation Models for Recommender Systems: A Survey and New Perspectives

    [https://arxiv.org/abs/2402.11143](https://arxiv.org/abs/2402.11143)

    本文详细分析了基于基础模型的推荐系统，提供了系统分类并讨论了最新研究进展和未来研究方向。

    

    最近，基础模型（FMs）凭借其广泛的知识库和复杂的架构，在推荐系统（RSs）领域提供了独特的机会。 本文试图全面审视基于FM的推荐系统（FM4RecSys）。 我们首先回顾了FM4RecSys的研究背景。 然后，我们提供了现有FM4RecSys研究工作的系统分类，可分为数据特征，表示学习，模型类型和下游任务四个不同部分。 在每个部分中，我们回顾了关键的最新研究进展，概述了代表性模型，并讨论了它们的特点。 此外，我们详细阐述了FM4RecSys的开放问题和机遇，旨在为这一领域的未来研究方向提供启示。 总之，我们总结了研究结果并讨论了该领域的新兴趋势。

    arXiv:2402.11143v1 Announce Type: new  Abstract: Recently, Foundation Models (FMs), with their extensive knowledge bases and complex architectures, have offered unique opportunities within the realm of recommender systems (RSs). In this paper, we attempt to thoroughly examine FM-based recommendation systems (FM4RecSys). We start by reviewing the research background of FM4RecSys. Then, we provide a systematic taxonomy of existing FM4RecSys research works, which can be divided into four different parts including data characteristics, representation learning, model type, and downstream tasks. Within each part, we review the key recent research developments, outlining the representative models and discussing their characteristics. Moreover, we elaborate on the open problems and opportunities of FM4RecSys aiming to shed light on future research directions in this area. In conclusion, we recap our findings and discuss the emerging trends in this field.
    
[^25]: Persona-DB：用于响应预测的高效大规模语言模型个性化与协同数据优化

    Persona-DB: Efficient Large Language Model Personalization for Response Prediction with Collaborative Data Refinement

    [https://arxiv.org/abs/2402.11060](https://arxiv.org/abs/2402.11060)

    介绍了 Persona-DB，一个简单却有效的框架，通过层级构建过程和协同优化，改善了大规模语言模型个性化中数据库表示的泛化能力和检索效率。

    

    随着对大型语言模型（LLMs）个性化交互需求的增加，需要开发能够准确快速识别用户意见和偏好的方法。检索增强作为一种有效策略出现，因为它可以适应大量用户而无需进行微调的成本。然而，现有研究主要集中在增强检索阶段，并对数据库表示的优化进行了有限的探索，这是个性化等任务的关键方面。在这项工作中，我们从一个新的角度研究了这个问题，着重于如何更有效地表示数据，以便在LLM定制的情境下更有效地进行检索。为了解决这一挑战，我们介绍了Persona-DB，这是一个简单而有效的框架，包括一个分层构建过程，以改善跨任务背景的泛化能力，并进行协同优化。

    arXiv:2402.11060v1 Announce Type: cross  Abstract: The increasing demand for personalized interactions with large language models (LLMs) calls for the development of methodologies capable of accurately and efficiently identifying user opinions and preferences. Retrieval augmentation emerges as an effective strategy, as it can accommodate a vast number of users without the costs from fine-tuning. Existing research, however, has largely focused on enhancing the retrieval stage and devoted limited exploration toward optimizing the representation of the database, a crucial aspect for tasks such as personalization. In this work, we examine the problem from a novel angle, focusing on how data can be better represented for more efficient retrieval in the context of LLM customization. To tackle this challenge, we introduce Persona-DB, a simple yet effective framework consisting of a hierarchical construction process to improve generalization across task contexts and collaborative refinement to
    
[^26]: 密集通道检索：密集通道检索是否在检索中？

    Retrieval-Augmented Generation: Is Dense Passage Retrieval Retrieving?

    [https://arxiv.org/abs/2402.11035](https://arxiv.org/abs/2402.11035)

    DPR微调预训练网络以增强查询和相关文本数据之间的嵌入对齐，发现训练中知识去中心化，但也揭示了模型内部知识的局限性

    

    密集通道检索（DPR）是改进大型语言模型（LLM）性能的检索增强生成（RAG）范式中的第一步。 DPR微调预训练网络，以增强查询和相关文本数据之间的嵌入对齐。对DPR微调的深入理解将需要从根本上释放该方法的全部潜力。在这项工作中，我们通过使用探针、层激活分析和模型编辑的组合，机械地探索了DPR训练模型。我们的实验证明，DPR训练使网络中存储知识的方式去中心化，创建了访问相同信息的多个路径。我们还发现了这种训练风格的局限性：预训练模型的内部知识限制了检索模型可以检索的内容。这些发现为密集检索提出了一些可能的方向：（1）暴露DPR训练过程

    arXiv:2402.11035v1 Announce Type: new  Abstract: Dense passage retrieval (DPR) is the first step in the retrieval augmented generation (RAG) paradigm for improving the performance of large language models (LLM). DPR fine-tunes pre-trained networks to enhance the alignment of the embeddings between queries and relevant textual data. A deeper understanding of DPR fine-tuning will be required to fundamentally unlock the full potential of this approach. In this work, we explore DPR-trained models mechanistically by using a combination of probing, layer activation analysis, and model editing. Our experiments show that DPR training decentralizes how knowledge is stored in the network, creating multiple access pathways to the same information. We also uncover a limitation in this training style: the internal knowledge of the pre-trained model bounds what the retrieval model can retrieve. These findings suggest a few possible directions for dense retrieval: (1) expose the DPR training process 
    
[^27]: 基于LLM的联邦推荐系统

    LLM-based Federated Recommendation

    [https://arxiv.org/abs/2402.09959](https://arxiv.org/abs/2402.09959)

    这项研究介绍了一种基于LLM的联邦推荐系统，用于提高推荐系统的性能和隐私保护。面临的挑战是客户端性能不平衡和对计算资源的高需求。

    

    大规模语言模型（LLM）通过微调方法展示了改进推荐系统的巨大潜力，具备先进的上下文理解能力。然而，微调需要用户行为数据，这会带来隐私风险，因为包含了敏感用户信息。这些数据的意外泄露可能侵犯数据保护法，并引发伦理问题。为了减轻这些隐私问题，联邦学习推荐系统（Fed4Rec）被提出作为一种有前景的方法。然而，将Fed4Rec应用于基于LLM的推荐系统面临两个主要挑战：首先，客户端性能不平衡加剧，影响系统的效率；其次，对于本地训练和推理LLM，对客户端的计算和存储资源需求很高。

    arXiv:2402.09959v1 Announce Type: new  Abstract: Large Language Models (LLMs), with their advanced contextual understanding abilities, have demonstrated considerable potential in enhancing recommendation systems via fine-tuning methods. However, fine-tuning requires users' behavior data, which poses considerable privacy risks due to the incorporation of sensitive user information. The unintended disclosure of such data could infringe upon data protection laws and give rise to ethical issues. To mitigate these privacy issues, Federated Learning for Recommendation (Fed4Rec) has emerged as a promising approach. Nevertheless, applying Fed4Rec to LLM-based recommendation presents two main challenges: first, an increase in the imbalance of performance across clients, affecting the system's efficiency over time, and second, a high demand on clients' computational and storage resources for local training and inference of LLMs.   To address these challenges, we introduce a Privacy-Preserving LL
    
[^28]: 基于对比学习和自注意力的时间接近度上的顺序推荐

    Sequential Recommendation on Temporal Proximities with Contrastive Learning and Self-Attention

    [https://arxiv.org/abs/2402.09784](https://arxiv.org/abs/2402.09784)

    该论文基于对比学习和自注意力机制，提出了一种考虑垂直和水平时间接近度的顺序推荐方法，以更好地捕捉用户-项目交互中的时间上下文。

    

    传统的基于深度学习和最新的基于Transformer的模型在先前的研究中捕捉了用户-项目交互中的单向和双向模式，但对于时间上下文的重要性，如个体行为和社会趋势模式，仍未得到很好的探索。最近的模型通常忽略了在类似的时间段内隐含在用户之间发生的用户行为的相似性，我们将其称为垂直时间接近度。这些模型主要通过适应Transformer的自注意机制来考虑用户行为中的时间上下文。同时，这种适应在考虑项目交互中的水平时间接近度方面仍然有限，例如区分在一周内与一个月内购买的连续项目。

    arXiv:2402.09784v1 Announce Type: cross  Abstract: Sequential recommender systems identify user preferences from their past interactions to predict subsequent items optimally. Although traditional deep-learning-based models and modern transformer-based models in previous studies capture unidirectional and bidirectional patterns within user-item interactions, the importance of temporal contexts, such as individual behavioral and societal trend patterns, remains underexplored. Notably, recent models often neglect similarities in users' actions that occur implicitly among users during analogous timeframes-a concept we term vertical temporal proximity. These models primarily adapt the self-attention mechanisms of the transformer to consider the temporal context in individual user actions. Meanwhile, this adaptation still remains limited in considering the horizontal temporal proximity within item interactions, like distinguishing between subsequent item purchases within a week versus a mon
    
[^29]: 通过LLM-认知数据增强广义对话密集检索

    Generalizing Conversational Dense Retrieval via LLM-Cognition Data Augmentation

    [https://arxiv.org/abs/2402.07092](https://arxiv.org/abs/2402.07092)

    本文提出了一种通过LLM-认知数据增强的方法来广义对话密集检索。该方法首先生成多级增强对话，捕捉多样的对话环境。其次，通过认知感知过程减少错误生成情况，并通过难度自适应样本筛选器选择具有挑战性的样本。

    

    对话式搜索利用多轮自然语言环境来检索相关段落。现有的对话密集检索模型大多将对话视为一系列固定的问题和回答，忽视了严重的数据稀疏性问题 - 也就是说，用户可以以不同的方式进行对话，而这些备选对话是未记录的。因此，它们经常难以推广到真实场景中的多样对话。在这项工作中，我们提出了一种通过LLM-认知数据增强广义对话密集检索的框架(ConvAug)。ConvAug首先生成多级增强对话，以捕捉对话环境的多样性。受人类认知方式的启发，我们设计了一种认知感知过程，以减少错误的正例、负例和幻觉的生成。此外，我们还开发了一种难度自适应样本筛选器，用于选择复杂对话的具有挑战性的样本。

    Conversational search utilizes muli-turn natural language contexts to retrieve relevant passages. Existing conversational dense retrieval models mostly view a conversation as a fixed sequence of questions and responses, overlooking the severe data sparsity problem -- that is, users can perform a conversation in various ways, and these alternate conversations are unrecorded. Consequently, they often struggle to generalize to diverse conversations in real-world scenarios. In this work, we propose a framework for generalizing Conversational dense retrieval via LLM-cognition data Augmentation (ConvAug). ConvAug first generates multi-level augmented conversations to capture the diverse nature of conversational contexts. Inspired by human cognition, we devise a cognition-aware process to mitigate the generation of false positives, false negatives, and hallucinations. Moreover, we develop a difficulty-adaptive sample filter that selects challenging samples for complex conversations, thereby g
    
[^30]: 用于稀疏标签信息检索系统离线评估的Fr\'echet距离

    Fr\'echet Distance for Offline Evaluation of Information Retrieval Systems with Sparse Labels

    [https://arxiv.org/abs/2401.17543](https://arxiv.org/abs/2401.17543)

    该论文提出使用Fr\'echet距离来评估稀疏标签信息检索系统的性能。实验证明，在少量标签可用的情况下，Fr\'echet距离是一种有效的评估指标。

    

    自然语言处理、信息检索(IR)、计算机视觉等技术的快速发展，对评估这些系统的性能提出了显著挑战。其中一个主要挑战是人工标记数据的稀缺性，这限制了对这些系统的公平和准确评估。在本研究中，我们特别关注使用稀疏标签评估IR系统，借鉴了最近在评估计算机视觉任务方面的研究成果。受将Fr\'echet Inception Distance (FID)用于评估文本到图像生成系统成功的启发，我们提出利用Fr\'echet距离来衡量相关被判定项和检索结果的分布之间的距离。我们在MS MARCO V1数据集和TREC深度学习轨迹查询集上的实验结果证明了Fr\'echet距离作为评估IR系统的指标的有效性，特别是在少量标签可用的情况下。

    The rapid advancement of natural language processing, information retrieval (IR), computer vision, and other technologies has presented significant challenges in evaluating the performance of these systems. One of the main challenges is the scarcity of human-labeled data, which hinders the fair and accurate assessment of these systems. In this work, we specifically focus on evaluating IR systems with sparse labels, borrowing from recent research on evaluating computer vision tasks. taking inspiration from the success of using Fr\'echet Inception Distance (FID) in assessing text-to-image generation systems. We propose leveraging the Fr\'echet Distance to measure the distance between the distributions of relevant judged items and retrieved results. Our experimental results on MS MARCO V1 dataset and TREC Deep Learning Tracks query sets demonstrate the effectiveness of the Fr\'echet Distance as a metric for evaluating IR systems, particularly in settings where a few labels are available. 
    
[^31]: RecDCL: 用于推荐的双重对比学习

    RecDCL: Dual Contrastive Learning for Recommendation

    [https://arxiv.org/abs/2401.15635](https://arxiv.org/abs/2401.15635)

    RecDCL提出了一个双重对比学习推荐框架，结合了批次对比学习（BCL）和特征对比学习（FCL），有助于消除冗余的解决方案，但又不会错过最优解。

    

    自监督学习（SSL）最近在挖掘协同过滤中的用户-项目交互方面取得了巨大成功。对比学习（CL）是一个重要范式，可以通过对比原始数据和增强数据之间的嵌入来解决网络平台中的数据稀疏性。然而，现有的基于CL的方法主要集中在批次方式对比上，未能充分利用特征维度中的潜在规律，这导致在用户和项目的表示学习过程中出现了冗余的解决方案。在这项工作中，我们研究了如何同时利用批次对比学习（BCL）和特征对比学习（FCL）进行推荐。我们在理论上分析了BCL和FCL之间的关系，并发现结合BCL和FCL有助于消除冗余解决方案，但永远不会错过最优解。我们提出了一个双重对比学习推荐框架-- RecDCL。在RecDCL中，FCL目标旨在消除 ...

    arXiv:2401.15635v2 Announce Type: replace-cross  Abstract: Self-supervised learning (SSL) has recently achieved great success in mining the user-item interactions for collaborative filtering. As a major paradigm, contrastive learning (CL) based SSL helps address data sparsity in Web platforms by contrasting the embeddings between raw and augmented data. However, existing CL-based methods mostly focus on contrasting in a batch-wise way, failing to exploit potential regularity in the feature dimension. This leads to redundant solutions during the representation learning of users and items. In this work, we investigate how to employ both batch-wise CL (BCL) and feature-wise CL (FCL) for recommendation. We theoretically analyze the relation between BCL and FCL, and find that combining BCL and FCL helps eliminate redundant solutions but never misses an optimal solution. We propose a dual contrastive learning recommendation framework -- RecDCL. In RecDCL, the FCL objective is designed to eli
    
[^32]: 超越自注意力的序列推荐中的关注归纳偏差

    An Attentive Inductive Bias for Sequential Recommendation beyond the Self-Attention

    [https://arxiv.org/abs/2312.10325](https://arxiv.org/abs/2312.10325)

    提出了一种名为BSARec的新方法，超越了自注意力，在序列推荐中注入了归纳偏差，并集成了低频和高频信息以减轻过度平滑问题

    

    基于Transformer的序列推荐（SR）模型取得了显著的成功。 Transformer的自注意机制在计算机视觉和自然语言处理中遇到了过度平滑问题，即隐藏表示变得类似于标记。 在SR领域，我们首次展示了相同问题的发生。 我们进行了开创性的研究，揭示了自注意在SR中的低通滤波特性，导致了过度平滑。 为此，我们提出了一种名为$\textbf{B}$eyond $\textbf{S}$elf-$\textbf{A}$ttention for Sequential $\textbf{Rec}$ommendation（BSARec）的新方法，利用傅里叶变换来 i）通过考虑细粒度的序列模式注入归纳偏差和 ii）集成低频和高频信息以减轻过度平滑。 我们的发现在SR领域显示了显著的进展，并有望搭起

    arXiv:2312.10325v2 Announce Type: replace-cross  Abstract: Sequential recommendation (SR) models based on Transformers have achieved remarkable successes. The self-attention mechanism of Transformers for computer vision and natural language processing suffers from the oversmoothing problem, i.e., hidden representations becoming similar to tokens. In the SR domain, we, for the first time, show that the same problem occurs. We present pioneering investigations that reveal the low-pass filtering nature of self-attention in the SR, which causes oversmoothing. To this end, we propose a novel method called $\textbf{B}$eyond $\textbf{S}$elf-$\textbf{A}$ttention for Sequential $\textbf{Rec}$ommendation (BSARec), which leverages the Fourier transform to i) inject an inductive bias by considering fine-grained sequential patterns and ii) integrate low and high-frequency information to mitigate oversmoothing. Our discovery shows significant advancements in the SR domain and is expected to bridge t
    
[^33]: 通过显式特征增强提高跨领域点击率预测

    Enhancing Cross-domain Click-Through Rate Prediction via Explicit Feature Augmentation

    [https://arxiv.org/abs/2312.00078](https://arxiv.org/abs/2312.00078)

    通过跨领域增强网络（CDA）提出了一种更灵活、更高效的方法，通过显式特征增强来解决跨领域点击率预测中的知识转移问题。

    

    跨领域点击率（CDCTR）预测是一个重要的研究课题，研究如何利用相关领域的有意义数据来帮助目标领域的点击率预测。大多数现有的CDCTR作品设计了隐式的方式来跨领域传递知识，比如参数共享，用于规范化目标域中的模型训练。更有效地，最近的研究人员提出了显式技术来提取用户兴趣知识并将此知识转移到目标领域。然而，所提出的方法主要面临两个问题：1）通常需要一个超级领域，即一个非常大的源领域，来涵盖目标领域的大多数用户或项目，2）提取的用户兴趣知识是静态的，无论目标域中的场景如何。这些限制激励我们开发一种更灵活、更高效的技术来显式地转移知识。在这项工作中，我们提出了一种跨领域增强网络（CDA）

    arXiv:2312.00078v2 Announce Type: replace  Abstract: Cross-domain CTR (CDCTR) prediction is an important research topic that studies how to leverage meaningful data from a related domain to help CTR prediction in target domain. Most existing CDCTR works design implicit ways to transfer knowledge across domains such as parameter-sharing that regularizes the model training in target domain. More effectively, recent researchers propose explicit techniques to extract user interest knowledge and transfer this knowledge to target domain. However, the proposed method mainly faces two issues: 1) it usually requires a super domain, i.e. an extremely large source domain, to cover most users or items of target domain, and 2) the extracted user interest knowledge is static no matter what the context is in target domain. These limitations motivate us to develop a more flexible and efficient technique to explicitly transfer knowledge. In this work, we propose a cross-domain augmentation network (CDA
    
[^34]: 跨国资金转移时序网络中的异常检测

    Anomaly detection in cross-country money transfer temporal networks

    [https://arxiv.org/abs/2311.14778](https://arxiv.org/abs/2311.14778)

    通过时序网络分析，本研究利用网络中心度量在跨国资金转移中实现了异常检测的有效性。

    

    本文探讨了通过时序网络分析进行异常检测。与许多传统方法不同，依赖规则算法或一般的机器学习方法，我们的方法利用了时序网络中的演变结构和关系，可用于建模金融交易。我们的方法专注于稳定生态系统中的微小变化，例如大型国际金融机构中发现的那些生态系统，利用网络中心性度量来深入了解个体节点。通过监控基于中心性的节点排名的时序演变，我们的方法有效地识别出特定节点角色的突然变化，促使领域专家进一步调查。

    arXiv:2311.14778v2 Announce Type: replace-cross  Abstract: This paper explores anomaly detection through temporal network analysis. Unlike many conventional methods, relying on rule-based algorithms or general machine learning approaches, our methodology leverages the evolving structure and relationships within temporal networks, that can be used to model financial transactions. Focusing on minimal changes in stable ecosystems, such as those found in large international financial institutions, our approach utilizes network centrality measures to gain insights into individual nodes. By monitoring the temporal evolution of centrality-based node rankings, our method effectively identifies abrupt shifts in the roles of specific nodes, prompting further investigation by domain experts.   To demonstrate its efficacy, our methodology is applied in the Anti-Financial Crime (AFC) domain, analyzing a substantial financial dataset comprising over 80 million cross-country wire transfers. The goal 
    
[^35]: 知识增强的大型语言模型用于个性化上下文查询建议

    Knowledge-Augmented Large Language Models for Personalized Contextual Query Suggestion

    [https://arxiv.org/abs/2311.06318](https://arxiv.org/abs/2311.06318)

    提出一种新颖且通用的方法，通过从用户与搜索引擎的交互历史中提取相关上下文来个性化大型语言模型的输出，尤其适用于改进网络搜索体验。

    

    大型语言模型（LLMs）擅长解决各种自然语言任务。然而，由于重新训练或微调它们所涉及的成本巨大，它们仍然在很大程度上是静态的，并且难以个性化。尽管如此，许多应用程序可以从根据用户的偏好、目标和知识量定制的生成中受益。其中之一是网络搜索，了解用户试图做什么、关心什么以及他们知道什么可以提高搜索体验。在这项工作中，我们提出了一种新颖且通用的方法，该方法使用用户与搜索引擎的交互历史中的相关上下文来增强LLM以个性化其输出。具体而言，我们根据用户在网络上的搜索和浏览活动构建了每个用户的以实体为中心的知识存储，然后利用这些知识为LLM提供具有上下文相关性的提示增强。

    arXiv:2311.06318v2 Announce Type: replace-cross  Abstract: Large Language Models (LLMs) excel at tackling various natural language tasks. However, due to the significant costs involved in re-training or fine-tuning them, they remain largely static and difficult to personalize. Nevertheless, a variety of applications could benefit from generations that are tailored to users' preferences, goals, and knowledge. Among them is web search, where knowing what a user is trying to accomplish, what they care about, and what they know can lead to improved search experiences. In this work, we propose a novel and general approach that augments an LLM with relevant context from users' interaction histories with a search engine in order to personalize its outputs. Specifically, we construct an entity-centric knowledge store for each user based on their search and browsing activities on the web, which is then leveraged to provide contextually relevant LLM prompt augmentations. This knowledge store is 
    
[^36]: 关于组织病理学图像搜索的研究

    On Image Search in Histopathology. (arXiv:2401.08699v1 [eess.IV])

    [http://arxiv.org/abs/2401.08699](http://arxiv.org/abs/2401.08699)

    这篇论文综述了组织病理学图像搜索技术的最新发展，为计算病理学研究人员提供了简明的概述，旨在寻求有效、快速和高效的图像搜索方法。

    

    组织病理学的病理图像可以通过装有摄像头的显微镜或全扫描仪获取。利用相似性计算基于这些图像匹配患者，在研究和临床环境中具有重要潜力。最近搜索技术的进展使得可以对各种组织类型的细胞结构进行微妙的量化，促进比较，并在与诊断和治疗过的病例数据库进行比较时实现关于诊断、预后和新患者预测的推断。本文全面回顾了组织病理学图像搜索技术的最新发展，为计算病理学研究人员提供了简明的概述，以寻求有效、快速和高效的图像搜索方法。

    Pathology images of histopathology can be acquired from camera-mounted microscopes or whole slide scanners. Utilizing similarity calculations to match patients based on these images holds significant potential in research and clinical contexts. Recent advancements in search technologies allow for nuanced quantification of cellular structures across diverse tissue types, facilitating comparisons and enabling inferences about diagnosis, prognosis, and predictions for new patients when compared against a curated database of diagnosed and treated cases. In this paper, we comprehensively review the latest developments in image search technologies for histopathology, offering a concise overview tailored for computational pathology researchers seeking effective, fast and efficient image search methods in their work.
    
[^37]: INTERS: 使用指令调优解锁大型语言模型在搜索中的力量

    INTERS: Unlocking the Power of Large Language Models in Search with Instruction Tuning. (arXiv:2401.06532v1 [cs.CL])

    [http://arxiv.org/abs/2401.06532](http://arxiv.org/abs/2401.06532)

    本研究探索了指令调优的方法，以增强大型语言模型在信息检索任务中的能力，通过引入一个新的指令调优数据集INTERS，涵盖了21个IR任务，该方法显著提升了性能。

    

    大型语言模型（LLMs）在各种自然语言处理任务中展示了令人印象深刻的能力。然而，由于许多与信息检索（IR）具体概念的不经常出现在自然语言中，它们在信息检索任务中的应用仍然具有挑战性。虽然基于提示的方法可以向LLMs提供任务描述，但它们往往在促进全面理解和执行IR任务方面存在不足，从而限制了LLMs的适用性。为了弥补这一差距，本研究探索了指令调优的潜力，以提高LLMs在IR任务中的熟练程度。我们引入了一个新的指令调优数据集INTERS，涵盖了3个基本IR类别中的21个任务：查询理解、文档理解和查询文档关系理解。数据来自43个不同的由手动编写的模板构成的数据集。我们的实证结果表明，INTERS显著提升了各种公开数据集上的性能。

    Large language models (LLMs) have demonstrated impressive capabilities in various natural language processing tasks. Despite this, their application to information retrieval (IR) tasks is still challenging due to the infrequent occurrence of many IR-specific concepts in natural language. While prompt-based methods can provide task descriptions to LLMs, they often fall short in facilitating comprehensive understanding and execution of IR tasks, thereby limiting LLMs' applicability. To address this gap, in this work, we explore the potential of instruction tuning to enhance LLMs' proficiency in IR tasks. We introduce a novel instruction tuning dataset, INTERS, encompassing 21 tasks across three fundamental IR categories: query understanding, document understanding, and query-document relationship understanding. The data are derived from 43 distinct datasets with manually written templates. Our empirical results reveal that INTERS significantly boosts the performance of various publicly a
    
[^38]: GraphPro: 面向推荐系统的图预训练和提示学习

    GraphPro: Graph Pre-training and Prompt Learning for Recommendation. (arXiv:2311.16716v3 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2311.16716](http://arxiv.org/abs/2311.16716)

    GraphPro是一个结合了参数高效和动态图预训练与提示学习的框架，能够有效捕捉长期用户偏好和短期行为动态，从而在真实世界的推荐系统中提供准确和及时的推荐。

    

    基于GNN的推荐系统通过多次消息传递在建模复杂的用户-物品交互方面表现出色。然而，现有方法往往忽视了不断变化的用户-物品交互的动态性，这限制了其在适应用户偏好变化和新到达数据分布变化方面的可扩展性和性能。因此，它们在真实世界的动态环境中的可扩展性和性能受到了限制。在这项研究中，我们提出了GraphPro，这是一个将参数高效和动态图预训练与提示学习相结合的框架。这种新颖的组合能够有效捕捉长期用户偏好和短期行为动态，从而实现准确和及时的推荐。我们的GraphPro框架通过无缝集成临时提示机制和图结构提示学习机制到预训练的GNN模型中来解决用户偏好不断变化的挑战。

    GNN-based recommenders have excelled in modeling intricate user-item interactions through multi-hop message passing. However, existing methods often overlook the dynamic nature of evolving user-item interactions, which impedes the adaption to changing user preferences and distribution shifts in newly arriving data. Thus, their scalability and performances in real-world dynamic environments are limited. In this study, we propose GraphPro, a framework that incorporates parameter-efficient and dynamic graph pre-training with prompt learning. This novel combination empowers GNNs to effectively capture both long-term user preferences and short-term behavior dynamics, enabling the delivery of accurate and timely recommendations. Our GraphPro framework addresses the challenge of evolving user preferences by seamlessly integrating a temporal prompt mechanism and a graph-structural prompt learning mechanism into the pre-trained GNN model. The temporal prompt mechanism encodes time information o
    
[^39]: 具有强化改写生成的对话问答模型的鲁棒训练

    Robust Training for Conversational Question Answering Models with Reinforced Reformulation Generation. (arXiv:2310.13505v1 [cs.CL])

    [http://arxiv.org/abs/2310.13505](http://arxiv.org/abs/2310.13505)

    这项研究提出了一种新的框架REIGN，通过生成训练问题的改写，并使用深度强化学习来指导对话问答模型，增加模型对表面形式变化的鲁棒性，同时在不同的基准上进行零-shot应用。

    

    知识图谱（KG）上的对话问答（ConvQA）模型通常在黄金QA对的基准上进行训练和测试。这意味着训练仅限于在相应数据集中见到的表面形式，评估仅针对一小部分问题。通过我们的提出的框架REIGN，我们采取了几个步骤来解决这个受限的学习设置。首先，我们系统地生成训练问题的改写，以提高模型对表面形式变化的鲁棒性。这是一个特别具有挑战性的问题，因为这些问题的不完整性。其次，我们使用深度强化学习将ConvQA模型引导到更高的性能，只提供那些有助于提高回答质量的改写。第三，我们展示了在一个基准上训练主要模型组件并将其零-shot应用于另一个的可行性。最后，为了对训练模型的鲁棒性进行严格评估，我们使用和重新配置初始的改写、测试语料。

    Models for conversational question answering (ConvQA) over knowledge graphs (KGs) are usually trained and tested on benchmarks of gold QA pairs. This implies that training is limited to surface forms seen in the respective datasets, and evaluation is on a small set of held-out questions. Through our proposed framework REIGN, we take several steps to remedy this restricted learning setup. First, we systematically generate reformulations of training questions to increase robustness of models to surface form variations. This is a particularly challenging problem, given the incomplete nature of such questions. Second, we guide ConvQA models towards higher performance by feeding it only those reformulations that help improve their answering quality, using deep reinforcement learning. Third, we demonstrate the viability of training major model components on one benchmark and applying them zero-shot to another. Finally, for a rigorous evaluation of robustness for trained models, we use and re
    
[^40]: 解耦训练：令人沮丧的简单多领域学习的回归

    Decoupled Training: Return of Frustratingly Easy Multi-Domain Learning. (arXiv:2309.10302v1 [cs.LG])

    [http://arxiv.org/abs/2309.10302](http://arxiv.org/abs/2309.10302)

    这篇论文提出了一种称为解耦训练（D-Train）的令人沮丧的、无超参数的多领域学习方法。该方法采用了一种三阶段的训练策略，首先进行预训练，然后在每个领域上进行后训练，最后进行头部微调，实现解耦训练以获得更好的性能。

    

    多领域学习（MDL）旨在训练一个模型，在多个重叠但非相同的领域中具有最小的平均风险。为了解决数据集偏差和领域优势的挑战，从对齐分布减少领域差距的角度或通过实施领域特定的塔、门甚至专家来保留差异，已经提出了许多MDL方法。MDL模型变得越来越复杂，具有复杂的网络架构或损失函数，引入额外的参数并增加计算成本。在本文中，我们提出了一种令人沮丧的、无超参数的多领域学习方法，命名为解耦训练（D-Train）。D-Train是一种三阶段的从一般到特殊的训练策略，首先在所有领域上进行预训练以热身一个根模型，然后通过将其拆分为多个头部在每个领域上进行后训练，最后通过固定骨干进行头部微调，实现解耦训练以获得更好的性能。

    Multi-domain learning (MDL) aims to train a model with minimal average risk across multiple overlapping but non-identical domains. To tackle the challenges of dataset bias and domain domination, numerous MDL approaches have been proposed from the perspectives of seeking commonalities by aligning distributions to reduce domain gap or reserving differences by implementing domain-specific towers, gates, and even experts. MDL models are becoming more and more complex with sophisticated network architectures or loss functions, introducing extra parameters and enlarging computation costs. In this paper, we propose a frustratingly easy and hyperparameter-free multi-domain learning method named Decoupled Training(D-Train). D-Train is a tri-phase general-to-specific training strategy that first pre-trains on all domains to warm up a root model, then post-trains on each domain by splitting into multi heads, and finally fine-tunes the heads by fixing the backbone, enabling decouple training to ac
    
[^41]: ReLLa: 基于检索增强的大型语言模型的推荐系统中的生命周期序列行为理解

    ReLLa: Retrieval-enhanced Large Language Models for Lifelong Sequential Behavior Comprehension in Recommendation. (arXiv:2308.11131v1 [cs.IR])

    [http://arxiv.org/abs/2308.11131](http://arxiv.org/abs/2308.11131)

    本论文提出了一种名为ReLLa的检索增强大型语言模型框架，用于零样本和小样本推荐任务。通过语义用户行为检索（SUBR）来提取上下文中的有用信息，以改善LLMs的推荐性能。

    

    随着大型语言模型（LLMs）在自然语言处理（NLP）领域取得了显著突破，基于LLM的推荐系统引起了广泛关注并被积极探索。本文专注于适应和增强纯大型语言模型以用于零样本和小样本推荐任务。首先，我们针对推荐领域中LLMs无法从长用户行为序列的文本上下文中提取有用信息的问题，提出并定义了生命周期序列行为理解问题。为了解决这个问题并提高LLMs的推荐性能，我们提出了一种新的框架，即检索增强的大型语言模型（ReLLa）。针对零样本推荐，我们执行语义用户行为检索（SUBR）来提高数据的利用率。

    With large language models (LLMs) achieving remarkable breakthroughs in natural language processing (NLP) domains, LLM-enhanced recommender systems have received much attention and have been actively explored currently. In this paper, we focus on adapting and empowering a pure large language model for zero-shot and few-shot recommendation tasks. First and foremost, we identify and formulate the lifelong sequential behavior incomprehension problem for LLMs in recommendation domains, i.e., LLMs fail to extract useful information from a textual context of long user behavior sequence, even if the length of context is far from reaching the context limitation of LLMs. To address such an issue and improve the recommendation performance of LLMs, we propose a novel framework, namely Retrieval-enhanced Large Language models (ReLLa) for recommendation tasks in both zero-shot and few-shot settings. For zero-shot recommendation, we perform semantic user behavior retrieval (SUBR) to improve the data
    
[^42]: 统一推荐系统：个性化、群组、套餐和套餐到群组的推荐的统一框架

    UniRecSys: A Unified Framework for Personalized, Group, Package, and Package-to-Group Recommendations. (arXiv:2308.04247v1 [cs.IR])

    [http://arxiv.org/abs/2308.04247](http://arxiv.org/abs/2308.04247)

    这篇论文提出了一个新的统一推荐框架，可以处理个性化、群组、套餐和套餐到群组推荐这四种任务，填补了当前研究的空白部分。

    

    推荐系统旨在通过为各种产品和服务提供定制推荐来提高用户体验。这些系统帮助用户做出更明智的决策，提高用户对平台的满意度。然而，这些系统的实施在很大程度上取决于上下文，从向用户或群组推荐项目或套餐中都有所不同。这就需要在部署过程中仔细探索多个模型，因为目前没有一个统一的方法来处理不同层面的推荐。此外，这些个体模型必须根据上下文密切调整其生成的推荐结果，以防止其生成的推荐结果产生显著差异。在本文中，我们提出了一种新颖的统一推荐框架，解决了个性化、群组、套餐或套餐到群组推荐四个任务，填补了当前研究中的空白部分。

    Recommender systems aim to enhance the overall user experience by providing tailored recommendations for a variety of products and services. These systems help users make more informed decisions, leading to greater user satisfaction with the platform. However, the implementation of these systems largely depends on the context, which can vary from recommending an item or package to a user or a group. This requires careful exploration of several models during the deployment, as there is no comprehensive and unified approach that deals with recommendations at different levels. Furthermore, these individual models must be closely attuned to their generated recommendations depending on the context to prevent significant variation in their generated recommendations. In this paper, we propose a novel unified recommendation framework that addresses all four recommendation tasks, namely personalized, group, package, or package-to-group recommendation, filling the gap in the current research lan
    
[^43]: 以提示为基础的个性化冷启动推荐的研究

    Towards Personalized Cold-Start Recommendation with Prompts. (arXiv:2306.17256v1 [cs.IR])

    [http://arxiv.org/abs/2306.17256](http://arxiv.org/abs/2306.17256)

    本研究旨在解决个性化冷启动推荐问题，通过利用预训练语言模型的能力，将推荐过程转化为自然语言情感分析，提供适用于创业企业和用户参与历史不足的平台的个性化推荐。

    

    推荐系统在根据用户过去的行为帮助用户发现与其兴趣相符的信息方面发挥着关键作用。然而，当用户和物品之间的历史交互记录不可用时，开发个性化推荐系统变得具有挑战性，这就是所谓的系统冷启动推荐问题。此问题在创业企业或用户参与历史不足的平台中尤为突出。以往的研究集中在用户或物品的冷启动场景，其中系统仍然通过在同一领域中的历史用户和物品交互进行训练来为新用户或物品提供推荐，而无法解决我们的问题。为了弥合这一鸿沟，我们的研究引入了一种创新且有效的方法，利用预训练语言模型的能力。我们将推荐过程转化为自然语言情感分析，其中包含用户资料和物品属性的信息。

    Recommender systems play a crucial role in helping users discover information that aligns with their interests based on their past behaviors. However, developing personalized recommendation systems becomes challenging when historical records of user-item interactions are unavailable, leading to what is known as the system cold-start recommendation problem. This issue is particularly prominent in start-up businesses or platforms with insufficient user engagement history. Previous studies focus on user or item cold-start scenarios, where systems could make recommendations for new users or items but are still trained with historical user-item interactions in the same domain, which cannot solve our problem. To bridge the gap, our research introduces an innovative and effective approach, capitalizing on the capabilities of pre-trained language models. We transform the recommendation process into sentiment analysis of natural languages containing information of user profiles and item attribu
    
[^44]: SE-PQA: 个性化社区问题回答

    SE-PQA: Personalized Community Question Answering. (arXiv:2306.16261v1 [cs.IR])

    [http://arxiv.org/abs/2306.16261](http://arxiv.org/abs/2306.16261)

    这个论文介绍了SE-PQA（个性化社区问题回答）的新资源，该资源包括超过1百万个查询和2百万个回答，并使用一系列丰富的特征模拟了流行社区问题回答平台的用户之间的社交互动。研究提供了用于社区问题回答任务的可复现基线方法，包括深度学习模型和个性化方法。

    

    个人化的信息检索一直是一个长期研究的课题。然而，目前仍然缺乏高质量、真实的数据集来开展大规模实验，并评估个性化搜索模型。本文通过引入SE-PQA (StackExchange - 个性化问题回答)来填补这一空白，这是一个新的精选资源，用于设计和评估与社区问题回答任务相关的个性化模型。贡献的数据集包括超过1百万个查询和2百万个回答，使用了一系列丰富的特征来模拟一个流行社区问题回答平台的用户之间的社交互动。我们描述了SE-PQA的特点，并详细说明了与问题和回答相关的特征。我们还提供了基于该资源的社区问题回答任务的可复现基线方法，包括深度学习模型和个性化方法。初步实验结果表明了其合适性。

    Personalization in Information Retrieval is a topic studied for a long time. Nevertheless, there is still a lack of high-quality, real-world datasets to conduct large-scale experiments and evaluate models for personalized search. This paper contributes to filling this gap by introducing SE-PQA (StackExchange - Personalized Question Answering), a new curated resource to design and evaluate personalized models related to the task of community Question Answering (cQA). The contributed dataset includes more than 1 million queries and 2 million answers, annotated with a rich set of features modeling the social interactions among the users of a popular cQA platform. We describe the characteristics of SE-PQA and detail the features associated with questions and answers. We also provide reproducible baseline methods for the cQA task based on the resource, including deep learning models and personalization approaches. The results of the preliminary experiments conducted show the appropriateness
    
[^45]: GPT4Table：大型语言模型能理解结构化表格数据吗？一项基准测试和实证研究

    GPT4Table: Can Large Language Models Understand Structured Table Data? A Benchmark and Empirical Study. (arXiv:2305.13062v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.13062](http://arxiv.org/abs/2305.13062)

    本文设计了一个基准测试来评估大型语言模型（LLMs）对结构化表格数据的理解能力，并发现不同的输入选择会对性能产生影响。在基准测试的基础上，提出了“自我增强”技术以改善理解能力。

    

    大型语言模型（LLMs）作为少样本推理器来解决与自然语言相关的任务越来越具吸引力。然而，关于LLMs对结构化数据（例如表格）的理解程度还有很多需要学习的地方。尽管可以使用表格序列化作为LLMs的输入，但目前还缺乏对LLMs是否真正能够理解这类数据的全面研究。本文通过设计一个基准测试来评估LLMs的结构理解能力（SUC）来解决这个问题。我们创建的基准测试包括七个任务，每个任务都有其独特的挑战，例如单元格查找、行检索和大小检测。我们对GPT-3.5和GPT-4进行了一系列评估。我们发现性能因多种输入选择而异，包括表格输入格式、内容顺序、角色提示和分区标记等。根据基准测试评估所得的见解，我们提出了“自我增强”技术以改善性能。

    Large language models (LLMs) are becoming attractive as few-shot reasoners to solve Natural Language (NL)-related tasks. However, there is still much to learn about how well LLMs understand structured data, such as tables. While it is true that tables can be used as inputs to LLMs with serialization, there lack of comprehensive studies examining whether LLMs can truly comprehend such data. In this paper, we try to understand this by designing a benchmark to evaluate the structural understanding capabilities (SUC) of LLMs. The benchmark we create includes seven tasks, each with its own unique challenges, \eg, cell lookup, row retrieval, and size detection. We run a series of evaluations on GPT-3.5 and GPT-4. We discover that the performance varied depending on a number of input choices, including table input format, content order, role prompting, and partition marks. Drawing from the insights gained through the benchmark evaluations, we then propose \textit{self-augmentation} for effect
    
[^46]: 植物流行病在农田中的成本最优播种策略

    Cost-optimal Seeding Strategy During a Botanical Pandemic in Domesticated Fields. (arXiv:2301.02817v3 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2301.02817](http://arxiv.org/abs/2301.02817)

    这项研究提出了一种基于网格的经济最优播种策略，通过数学模型描述了在植物流行病期间农田作物的经济利润，可为农田主人和决策者提供指导。

    

    背景：植物流行病在全球范围内造成了巨大的经济损失和粮食短缺。然而，由于植物流行病在短中期内将继续存在，农田主人可以根据策略性地在自己的农田中播种，以优化每一次作物生产的经济利润。目标：鉴于病原体的流行病学特性，我们旨在为农田主人和决策者寻找一种基于网格的经济最优播种策略。方法：我们提出了一种新颖的流行病学-经济数学模型，描述了在植物流行病期间农田作物的经济利润。我们使用时空扩展的易感-感染-康复流行病学模型以及非线性输出流行病学模型来描述流行病学动态。结果和结论：我们提供了一种算法，用于根据农田和病原体的特性获取最优的网格形成的播种策略，以最大化经济利润。此外，我们还在现实情况下实施了提出的模型。

    Context: Botanical pandemics cause enormous economic damage and food shortage around the globe. However, since botanical pandemics are here to stay in the short-medium term, domesticated field owners can strategically seed their fields to optimize each session's economic profit. Objective: Given the pathogen's epidemiological properties, we aim to find an economically optimal grid-based seeding strategy for field owners and policymakers. Methods: We propose a novel epidemiological-economic mathematical model that describes the economic profit from a field of plants during a botanical pandemic. We describe the epidemiological dynamics using a spatio-temporal extended Susceptible-Infected-Recovered epidemiological model with a non-linear output epidemiological model. Results and Conclusions: We provide an algorithm to obtain an optimal grid-formed seeding strategy to maximize economic profit, given field and pathogen properties. In addition, we implement the proposed model in realistic s
    
[^47]: 搜索和推荐中的结果多样化：一项调研

    Result Diversification in Search and Recommendation: A Survey. (arXiv:2212.14464v3 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2212.14464](http://arxiv.org/abs/2212.14464)

    这项调研提出了一个统一的分类体系，用于将搜索和推荐中的多样化指标和方法进行分类。调研总结了搜索和推荐中的各种多样性问题，并展示了各种应用在搜索和推荐中的调研成果。未来的研究方向和挑战也被讨论。

    

    多样化返回结果对于满足客户的各种兴趣和提供者的市场曝光是重要的研究课题。近年来，对多样化研究的关注不断增加，伴随着对在搜索和推荐中促进多样性的方法的文献大量涌现。然而，检索系统中的多样化研究缺乏系统组织，存在片段化的问题。在这项调研中，我们首次提出了一个统一的分类体系，用于将搜索和推荐中的多样化指标和方法进行分类，这两个领域是检索系统中研究最广泛的领域之一。我们从简要讨论为何多样性在检索系统中重要开始调研，然后总结了搜索和推荐中的各种多样性问题，突出了它们之间的关系和差异。调研的主体部分，我们提供了一个统一的框架，包括描述现有多样化指标和方法的详细内容，展示了各种应用在搜索和推荐中的调研成果。最后，我们对当前的研究趋势进行了讨论，并指出了未来的研究方向和挑战。

    Diversifying return results is an important research topic in retrieval systems in order to satisfy both the various interests of customers and the equal market exposure of providers. There has been growing attention on diversity-aware research during recent years, accompanied by a proliferation of literature on methods to promote diversity in search and recommendation. However, diversity-aware studies in retrieval systems lack a systematic organization and are rather fragmented. In this survey, we are the first to propose a unified taxonomy for classifying the metrics and approaches of diversification in both search and recommendation, which are two of the most extensively researched fields of retrieval systems. We begin the survey with a brief discussion of why diversity is important in retrieval systems, followed by a summary of the various diversity concerns in search and recommendation, highlighting their relationship and differences. For the survey's main body, we present a unifi
    
[^48]: 增强GNNs的时空对比学习用于基于会话的推荐

    Spatio-Temporal Contrastive Learning Enhanced GNNs for Session-based Recommendation. (arXiv:2209.11461v3 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2209.11461](http://arxiv.org/abs/2209.11461)

    本研究提出了一种名为基于会话的推荐的新框架，通过利用对比学习技术的均匀性和对齐性特性，增强了GNNs对于基于会话的推荐的性能。

    

    基于会话的推荐系统旨在利用用户的短期行为序列来预测下一个项目，而不需要详细的用户资料。最近的研究主要通过将会话视为项目之间的转换图，并利用各种图神经网络（GNNs）对项目及其邻居之间的关系进行编码，以建模用户的偏好。一些现有的基于GNN的模型主要关注从空间图结构的视角聚合信息，忽视了在信息传递过程中项目的邻居之间的时间关系，导致信息丢失，从而产生次优问题。其他的工作通过整合额外的时间信息来解决这个问题，但缺乏空间和时间模式之间充分的交互。为了解决这个问题，受对比学习技术的均匀性和对齐性特性的启发，我们提出了一个名为基于会话的推荐的新框架。

    Session-based recommendation (SBR) systems aim to utilize the user's short-term behavior sequence to predict the next item without the detailed user profile. Most recent works try to model the user preference by treating the sessions as between-item transition graphs and utilize various graph neural networks (GNNs) to encode the representations of pair-wise relations among items and their neighbors. Some of the existing GNN-based models mainly focus on aggregating information from the view of spatial graph structure, which ignores the temporal relations within neighbors of an item during message passing and the information loss results in a sub-optimal problem. Other works embrace this challenge by incorporating additional temporal information but lack sufficient interaction between the spatial and temporal patterns. To address this issue, inspired by the uniformity and alignment properties of contrastive learning techniques, we propose a novel framework called Session-based Recommenda
    

