# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Rematch: Robust and Efficient Matching of Local Knowledge Graphs to Improve Structural and Semantic Similarity](https://arxiv.org/abs/2404.02126) | 引入一种新的AMR相似度度量rematch以提高结构和语义相似性，结构相似性排名第二，语义相似性最优，快五倍于下一个最高效度量。 |
| [^2] | [IISAN: Efficiently Adapting Multimodal Representation for Sequential Recommendation with Decoupled PEFT](https://arxiv.org/abs/2404.02059) | IISAN是一种简单的插拔架构，采用解耦PEFT结构，并利用内部和跨模态适应，与全微调和最先进的PEFT性能匹配，显著减少GPU内存使用量，并加速了训练时间。 |
| [^3] | [Where to Move Next: Zero-shot Generalization of LLMs for Next POI Recommendation](https://arxiv.org/abs/2404.01855) | 设计了新颖的提示策略和进行了实证研究以探索LLMs用于下一个POI推荐的零样本泛化能力 |
| [^4] | [CIRP: Cross-Item Relational Pre-training for Multimodal Product Bundling](https://arxiv.org/abs/2404.01735) | 提出了一种名为CIRP的框架，用于产品捆绑中的项目表示学习，旨在解决先前方法中对跨项目关系的不足。 |
| [^5] | [Entity Disambiguation via Fusion Entity Decoding](https://arxiv.org/abs/2404.01626) | 提出了一种通过融合实体描述进行实体消歧的编码-解码模型。 |
| [^6] | [Transforming LLMs into Cross-modal and Cross-lingual RetrievalSystems](https://arxiv.org/abs/2404.01616) | 提出使用LLMs初始化多模态DE检索系统，实现在102种语言中匹配语音和文本的能力，无需在LLM预训练期间使用语音数据，且相比先前系统取得10%的Recall@1绝对改进 |
| [^7] | [BERT-Enhanced Retrieval Tool for Homework Plagiarism Detection System](https://arxiv.org/abs/2404.01582) | 本文提出了一种基于GPT-3.5的抄袭文本数据生成方法和一种基于Faiss和BERT的高效高准确性的抄袭识别方法，填补了高水平抄袭检测研究数据集缺失的空白，实验证明该模型在多个指标上表现优异 |
| [^8] | [Multi-granular Adversarial Attacks against Black-box Neural Ranking Models](https://arxiv.org/abs/2404.01574) | 这项研究聚焦于利用多粒度扰动生成高质量的对抗性示例，通过转化为顺序决策过程来解决组合爆炸问题。 |
| [^9] | [Set-Aligning Framework for Auto-Regressive Event Temporal Graph Generation](https://arxiv.org/abs/2404.01532) | 提出了针对自动回归事件时间图生成的条件集合生成问题的集合对齐框架，用于解决线性化图和语言模型处理序列不匹配的挑战 |
| [^10] | [OpenChemIE: An Information Extraction Toolkit For Chemistry Literature](https://arxiv.org/abs/2404.01462) | OpenChemIE提出了一种用于从化学文献中提取反应数据的工具包，通过整合文本、表格和图像信息以及使用专门神经模型和算法，实现了在文档级别的反应数据提取。 |
| [^11] | [Utilizing AI and Social Media Analytics to Discover Adverse Side Effects of GLP-1 Receptor Agonists](https://arxiv.org/abs/2404.01358) | 通过利用人工智能驱动的社交媒体分析，我们开发了一种数字健康方法，成功检测出与GLP-1受体激动剂相关的21种潜在不良副作用，包括易怒和麻木感，从而革新了对新部署药物未报告ASEs的检测。 |
| [^12] | [Automatic detection of relevant information, predictions and forecasts in financial news through topic modelling with Latent Dirichlet Allocation](https://arxiv.org/abs/2404.01338) | 该研究提出了一种新颖的自然语言处理系统，通过Latent Dirichlet Allocation (LDA)进行相关的主题建模，帮助投资者从非结构化文本源中检测财经事件中的相关信息、预测和预测 |
| [^13] | [Detection of Temporality at Discourse Level on Financial News by Combining Natural Language Processing and Machine Learning](https://arxiv.org/abs/2404.01337) | 通过结合自然语言处理和机器学习技术，提出了一种新颖的系统，旨在在金融新闻中检测篇章级别的关键声明的时间性，以分析句法和语义依赖关系，区分上下文信息和有价值的预测。 |
| [^14] | [Planning and Editing What You Retrieve for Enhanced Tool Learning](https://arxiv.org/abs/2404.00450) | 该论文提出了一种新颖的模型，结合了“规划与检索”和“编辑与确认”范式，通过神经检索模块和LLM-based查询规划器提高了工具利用的效果。 |
| [^15] | [Ink and Individuality: Crafting a Personalised Narrative in the Age of LLMs](https://arxiv.org/abs/2404.00026) | 研究探讨了人们日益依赖的基于LLM的写作助手对创造力和个性可能造成的负面影响，旨在改进人机交互系统和提升写作助手的个性化和个性化功能。 |
| [^16] | [FeatUp: A Model-Agnostic Framework for Features at Any Resolution](https://arxiv.org/abs/2403.10516) | FeatUp是一个任务和模型无关的框架，用于在深度特征中恢复丢失的空间信息，从而使特征可以以任何分辨率重建，在现有应用中取得分辨率和性能的提升。 |
| [^17] | [Towards Trustworthy Reranking: A Simple yet Effective Abstention Mechanism](https://arxiv.org/abs/2402.12997) | 提出了一种适用于现实约束的轻量级弃权机制，特别适用于再排序阶段，通过数据驱动的方法达到有效性，并提供了开源代码以促进其更广泛的应用。 |
| [^18] | [Navigating the Post-API Dilemma Search Engine Results Pages Present a Biased View of Social Media Data](https://arxiv.org/abs/2401.15479) | 搜索引擎结果页面可以作为社交媒体数据的替代方案，但存在对流行帖子偏见较高、情感更积极以及忽视政治、色情和粗俗帖子的问题。 |
| [^19] | [Separating and Learning Latent Confounders to Enhancing User Preferences Modeling](https://arxiv.org/abs/2311.03381) | 通过分离和学习潜在混淆因素，提高了用户偏好建模的准确性 |
| [^20] | [Beyond Yes and No: Improving Zero-Shot LLM Rankers via Scoring Fine-Grained Relevance Labels](https://arxiv.org/abs/2310.14122) | 通过将细粒度相关性标签纳入LLM排序器提示中，可以显著改进零样本LLM排序器的性能 |
| [^21] | [Constructing Cross-lingual Consumer Health Vocabulary with Word-Embedding from Comparable User Generated Content](https://arxiv.org/abs/2206.11612) | 通过使用来自可比用户生成内容的词嵌入，提出了一个跨语言自动术语识别框架，将英语消费者健康词汇扩展为跨语言词汇。 |
| [^22] | [A Survey on Cross-Domain Sequential Recommendation.](http://arxiv.org/abs/2401.04971) | 跨领域序列推荐通过集成和学习多个领域的交互信息，将用户偏好建模从平面转向立体。文章对CDSR问题进行了定义和分析，提供了从宏观和微观两个视角的系统概述。对于不同领域间的模型，总结了多层融合结构和融合桥梁。对于现有模型，讨论了基础技术和辅助学习技术。展示了公开数据集和实验结果，并给出了未来发展的见解。 |
| [^23] | [LLM-Rec: Personalized Recommendation via Prompting Large Language Models.](http://arxiv.org/abs/2307.15780) | 本文通过引导大型语言模型进行个性化推荐的研究，提出了四种不同的引导策略，并通过实验证明了这些策略的有效性。这一发现强调了在个性化内容推荐中，采用多样的引导和输入增强技术可以提高大型语言模型的推荐性能。 |
| [^24] | [Leveraging Recommender Systems to Reduce Content Gaps on Peer Production Platforms.](http://arxiv.org/abs/2307.08669) | 该研究通过在SuggestBot上进行离线分析和为期三个月的对照实验，发现推荐被低估主题的文章可以增加在这些文章上的编辑工作量，同时不会明显降低对推荐内容的接受程度。这一发现对解决对等生产平台上的内容缺口问题具有重要意义。 |
| [^25] | [Few-shot Link Prediction on N-ary Facts.](http://arxiv.org/abs/2305.06104) | 本文提出了一个新任务——少样本N-元事实链接预测，并提出了一个名为FLEN的模型来实现。FLEN由三个模块组成，可以从有限的标记实例中预测N-元事实中的缺失实体。 |

# 详细

[^1]: 重新比赛：改进本地知识图的鲁棒和高效匹配以提高结构和语义相似性

    Rematch: Robust and Efficient Matching of Local Knowledge Graphs to Improve Structural and Semantic Similarity

    [https://arxiv.org/abs/2404.02126](https://arxiv.org/abs/2404.02126)

    引入一种新的AMR相似度度量rematch以提高结构和语义相似性，结构相似性排名第二，语义相似性最优，快五倍于下一个最高效度量。

    

    知识图在各种应用中发挥着关键作用，例如问答和事实核查。抽象意义表示（AMR）将文本表示为知识图。评估这些图的质量涉及将它们结构化匹配到彼此和语义匹配到源文本。现有的AMR度量效率低，难以捕捉语义相似性。我们也缺乏一个系统性的评估基准，用于评估AMR图之间的结构相似性。为了克服这些限制，我们引入了一种新的AMR相似度度量rematch，以及一种用于评估结构相似性的新评估方法RARE。在最先进的度量中，rematch在结构相似性中排名第二；并且在STS-B和SICK-R基准上的语义相似性最高，与下一个最高效度量相比快五倍。

    arXiv:2404.02126v1 Announce Type: new  Abstract: Knowledge graphs play a pivotal role in various applications, such as question-answering and fact-checking. Abstract Meaning Representation (AMR) represents text as knowledge graphs. Evaluating the quality of these graphs involves matching them structurally to each other and semantically to the source text. Existing AMR metrics are inefficient and struggle to capture semantic similarity. We also lack a systematic evaluation benchmark for assessing structural similarity between AMR graphs. To overcome these limitations, we introduce a novel AMR similarity metric, rematch, alongside a new evaluation for structural similarity called RARE. Among state-of-the-art metrics, rematch ranks second in structural similarity; and first in semantic similarity by 1--5 percentage points on the STS-B and SICK-R benchmarks. Rematch is also five times faster than the next most efficient metric.
    
[^2]: IISAN：使用解耦PEFT有效地调整多模态表示以顺序推荐

    IISAN: Efficiently Adapting Multimodal Representation for Sequential Recommendation with Decoupled PEFT

    [https://arxiv.org/abs/2404.02059](https://arxiv.org/abs/2404.02059)

    IISAN是一种简单的插拔架构，采用解耦PEFT结构，并利用内部和跨模态适应，与全微调和最先进的PEFT性能匹配，显著减少GPU内存使用量，并加速了训练时间。

    

    多模态基础模型在顺序推荐系统中具有转变性，利用强大的表示学习能力。虽然参数高效微调（PEFT）通常用于调整基础模型以进行推荐任务，但大多数研究优先考虑参数效率，通常忽略GPU内存效率和训练速度等关键因素。针对这一差距，本文引入了IISAN（多模态表示的内部和跨模态侧面适应网络），一个使用解耦PEFT结构并利用内部和跨模态适应的简单即插即用架构。IISAN与全微调（FFT）和最先进的PEFT的性能相匹配。更重要的是，它显著减少了GPU内存使用量 - 对于多模态顺序推荐任务，从47GB降低到仅3GB。此外，与FFT相比，它将每个时代的训练时间从443秒加速到22秒。

    arXiv:2404.02059v1 Announce Type: new  Abstract: Multimodal foundation models are transformative in sequential recommender systems, leveraging powerful representation learning capabilities. While Parameter-efficient Fine-tuning (PEFT) is commonly used to adapt foundation models for recommendation tasks, most research prioritizes parameter efficiency, often overlooking critical factors like GPU memory efficiency and training speed. Addressing this gap, our paper introduces IISAN (Intra- and Inter-modal Side Adapted Network for Multimodal Representation), a simple plug-and-play architecture using a Decoupled PEFT structure and exploiting both intra- and inter-modal adaptation.   IISAN matches the performance of full fine-tuning (FFT) and state-of-the-art PEFT. More importantly, it significantly reduces GPU memory usage - from 47GB to just 3GB for multimodal sequential recommendation tasks. Additionally, it accelerates training time per epoch from 443s to 22s compared to FFT. This is also
    
[^3]: 下一个去哪里：基于零样本泛化的LLMs用于下一个POI推荐

    Where to Move Next: Zero-shot Generalization of LLMs for Next POI Recommendation

    [https://arxiv.org/abs/2404.01855](https://arxiv.org/abs/2404.01855)

    设计了新颖的提示策略和进行了实证研究以探索LLMs用于下一个POI推荐的零样本泛化能力

    

    下一个兴趣点（POI）推荐为用户提供了探索周边环境的宝贵建议。现有研究依赖于从大规模用户签到数据构建推荐模型，这是任务特定的，并需要大量的计算资源。最近，预训练的大型语言模型（LLMs）在各种NLP任务中取得了显著进展，并且已经被研究用于推荐场景。然而，LLMs的泛化能力在解决下一个POI推荐问题时仍未被探索，其中应提取用户的地理移动模式。虽然有研究利用LLMs进行下一个项目推荐，但它们未能考虑地理影响和顺序转换。因此，它们无法有效解决下一个POI推荐任务。为此，我们设计了新颖的提示策略，并进行了实证研究以验证

    arXiv:2404.01855v1 Announce Type: cross  Abstract: Next Point-of-interest (POI) recommendation provides valuable suggestions for users to explore their surrounding environment. Existing studies rely on building recommendation models from large-scale users' check-in data, which is task-specific and needs extensive computational resources. Recently, the pretrained large language models (LLMs) have achieved significant advancements in various NLP tasks and have also been investigated for recommendation scenarios. However, the generalization abilities of LLMs still are unexplored to address the next POI recommendations, where users' geographical movement patterns should be extracted. Although there are studies that leverage LLMs for next-item recommendations, they fail to consider the geographical influence and sequential transitions. Hence, they cannot effectively solve the next POI recommendation task. To this end, we design novel prompting strategies and conduct empirical studies to ass
    
[^4]: CIRP: 跨项目关系预训练用于多模态产品捆绑

    CIRP: Cross-Item Relational Pre-training for Multimodal Product Bundling

    [https://arxiv.org/abs/2404.01735](https://arxiv.org/abs/2404.01735)

    提出了一种名为CIRP的框架，用于产品捆绑中的项目表示学习，旨在解决先前方法中对跨项目关系的不足。

    

    产品捆绑一直是一种盛行的营销策略，对在线购物场景有益。有效的产品捆绑方法取决于捕获个体项目语义和跨项目关系的高质量项目表示。然而，先前的项目表示学习方法，无论是特征融合还是图形学习，都存在跨模态对齐不足并且难以捕捉冷启动项目的跨项目关系。多模态预训练模型可能是潜在的解决方案，因为它们在各种多模态下游任务上表现出色。然而，当前多模态预训练模型中的跨项目关系尚未得到充分探索。为了弥合这一差距，我们提出了一种新颖且简单的框架Cross-Item Relational Pre-training（CIRP），用于产品捆绑中的项目表示学习。具体来说，我们采用多模态编码器生成图像和文字

    arXiv:2404.01735v1 Announce Type: new  Abstract: Product bundling has been a prevailing marketing strategy that is beneficial in the online shopping scenario. Effective product bundling methods depend on high-quality item representations, which need to capture both the individual items' semantics and cross-item relations. However, previous item representation learning methods, either feature fusion or graph learning, suffer from inadequate cross-modal alignment and struggle to capture the cross-item relations for cold-start items. Multimodal pre-train models could be the potential solutions given their promising performance on various multimodal downstream tasks. However, the cross-item relations have been under-explored in the current multimodal pre-train models. To bridge this gap, we propose a novel and simple framework Cross-Item Relational Pre-training (CIRP) for item representation learning in product bundling. Specifically, we employ a multimodal encoder to generate image and te
    
[^5]: 通过融合实体解码进行实体消歧

    Entity Disambiguation via Fusion Entity Decoding

    [https://arxiv.org/abs/2404.01626](https://arxiv.org/abs/2404.01626)

    提出了一种通过融合实体描述进行实体消歧的编码-解码模型。

    

    实体消歧（ED）是将模糊实体的提及链接到知识库中的指代实体的过程，在实体链接（EL）中起着核心作用。现有的生成式方法在标准化的ZELDA基准下展示出比分类方法更高的准确性。然而，生成式方法需要大规模的预训练且生成效率低下。最重要的是，实体描述经常被忽视，而这些描述可能包含区分相似实体的关键信息。我们提出了一种编码-解码模型，以更详细的实体描述来进行实体消歧。给定文本和候选实体，编码器学习文本与每个候选实体之间的交互，为每个实体候选产生表示。解码器随后将实体候选的表示融合在一起，并选择正确的实体。

    arXiv:2404.01626v1 Announce Type: new  Abstract: Entity disambiguation (ED), which links the mentions of ambiguous entities to their referent entities in a knowledge base, serves as a core component in entity linking (EL). Existing generative approaches demonstrate improved accuracy compared to classification approaches under the standardized ZELDA benchmark. Nevertheless, generative approaches suffer from the need for large-scale pre-training and inefficient generation. Most importantly, entity descriptions, which could contain crucial information to distinguish similar entities from each other, are often overlooked. We propose an encoder-decoder model to disambiguate entities with more detailed entity descriptions. Given text and candidate entities, the encoder learns interactions between the text and each candidate entity, producing representations for each entity candidate. The decoder then fuses the representations of entity candidates together and selects the correct entity. Our 
    
[^6]: 将LLMs转化为跨模态和跨语言检索系统

    Transforming LLMs into Cross-modal and Cross-lingual RetrievalSystems

    [https://arxiv.org/abs/2404.01616](https://arxiv.org/abs/2404.01616)

    提出使用LLMs初始化多模态DE检索系统，实现在102种语言中匹配语音和文本的能力，无需在LLM预训练期间使用语音数据，且相比先前系统取得10%的Recall@1绝对改进

    

    大型语言模型（LLMs）是在仅基于文本数据进行训练的，这超出了具有配对语音和文本数据的语言范围。同时，基于双编码器（DE）的检索系统将查询和文档投影到相同的嵌入空间中，并在检索和双语文本挖掘中展示了成功。为了在许多语言中匹配语音和文本，我们建议使用LLMs初始化多模态DE检索系统。与传统方法不同，我们的系统在LLM预训练期间不需要语音数据，并且可以利用LLM的多语言文本理解能力来匹配检索训练期间看不见的语言中的语音和文本。我们的多模态LLM-based检索系统能够在102种语言中匹配语音和文本，尽管只在21种语言上进行了训练。我们的系统优于先前专门在所有102种语言上训练的系统。在这些语言中，我们在Recall@1上实现了10％的绝对改进。

    arXiv:2404.01616v1 Announce Type: new  Abstract: Large language models (LLMs) are trained on text-only data that go far beyond the languages with paired speech and text data. At the same time, Dual Encoder (DE) based retrieval systems project queries and documents into the same embedding space and have demonstrated their success in retrieval and bi-text mining. To match speech and text in many languages, we propose using LLMs to initialize multi-modal DE retrieval systems. Unlike traditional methods, our system doesn't require speech data during LLM pre-training and can exploit LLM's multilingual text understanding capabilities to match speech and text in languages unseen during retrieval training. Our multi-modal LLM-based retrieval system is capable of matching speech and text in 102 languages despite only training on 21 languages. Our system outperforms previous systems trained explicitly on all 102 languages. We achieve a 10% absolute improvement in Recall@1 averaged across these l
    
[^7]: 基于BERT增强的作业抄袭检测系统

    BERT-Enhanced Retrieval Tool for Homework Plagiarism Detection System

    [https://arxiv.org/abs/2404.01582](https://arxiv.org/abs/2404.01582)

    本文提出了一种基于GPT-3.5的抄袭文本数据生成方法和一种基于Faiss和BERT的高效高准确性的抄袭识别方法，填补了高水平抄袭检测研究数据集缺失的空白，实验证明该模型在多个指标上表现优异

    

    文本抄袭检测任务是一项常见的自然语言处理任务，旨在检测给定文本是否包含从其他文本中抄袭或复制的内容。在现有研究中，由于缺乏高质量的数据集，检测高水平的抄袭仍然是一个挑战。本文提出了一种基于GPT-3.5的抄袭文本数据生成方法，产生了32,927对文本抄袭检测数据集，涵盖了各种抄袭方法，填补了这一研究领域的空白。同时，我们提出了一种基于Faiss和BERT的高效高准确性的抄袭识别方法。我们的实验证明，这种模型在准确率、精确率、召回率和F1分数等多个指标上的表现优于其他模型，分别达到了98.86％、98.90％、98.86％和0.9888。最后，我们还提供了一个用户友好的演示平台，允许用户上传文本。

    arXiv:2404.01582v1 Announce Type: cross  Abstract: Text plagiarism detection task is a common natural language processing task that aims to detect whether a given text contains plagiarism or copying from other texts. In existing research, detection of high level plagiarism is still a challenge due to the lack of high quality datasets. In this paper, we propose a plagiarized text data generation method based on GPT-3.5, which produces 32,927 pairs of text plagiarism detection datasets covering a wide range of plagiarism methods, bridging the gap in this part of research. Meanwhile, we propose a plagiarism identification method based on Faiss with BERT with high efficiency and high accuracy. Our experiments show that the performance of this model outperforms other models in several metrics, including 98.86\%, 98.90%, 98.86%, and 0.9888 for Accuracy, Precision, Recall, and F1 Score, respectively. At the end, we also provide a user-friendly demo platform that allows users to upload a text 
    
[^8]: 目标黑盒神经排序模型的多粒度对抗攻击

    Multi-granular Adversarial Attacks against Black-box Neural Ranking Models

    [https://arxiv.org/abs/2404.01574](https://arxiv.org/abs/2404.01574)

    这项研究聚焦于利用多粒度扰动生成高质量的对抗性示例，通过转化为顺序决策过程来解决组合爆炸问题。

    

    对抗排序攻击由于在发现神经排序模型的脆弱性并增强其鲁棒性方面取得成功而受到越来越多的关注。传统的攻击方法仅在单一粒度上进行扰动，例如单词级或句子级，对目标文档进行攻击。然而，将扰动限制在单一粒度上可能会减少创造对抗性示例的灵活性，从而降低攻击的潜在威胁。因此，我们专注于通过结合不同粒度的扰动生成高质量的对抗性示例。实现这一目标涉及解决组合爆炸问题，需要识别出跨所有可能的粒度、位置和文本片段的最佳组合扰动。为了解决这一挑战，我们将多粒度对抗攻击转化为一个顺序决策过程，其中

    arXiv:2404.01574v1 Announce Type: cross  Abstract: Adversarial ranking attacks have gained increasing attention due to their success in probing vulnerabilities, and, hence, enhancing the robustness, of neural ranking models. Conventional attack methods employ perturbations at a single granularity, e.g., word-level or sentence-level, to a target document. However, limiting perturbations to a single level of granularity may reduce the flexibility of creating adversarial examples, thereby diminishing the potential threat of the attack. Therefore, we focus on generating high-quality adversarial examples by incorporating multi-granular perturbations. Achieving this objective involves tackling a combinatorial explosion problem, which requires identifying an optimal combination of perturbations across all possible levels of granularity, positions, and textual pieces. To address this challenge, we transform the multi-granular adversarial attack into a sequential decision-making process, where 
    
[^9]: 自动回归事件时间图生成的集合对齐框架

    Set-Aligning Framework for Auto-Regressive Event Temporal Graph Generation

    [https://arxiv.org/abs/2404.01532](https://arxiv.org/abs/2404.01532)

    提出了针对自动回归事件时间图生成的条件集合生成问题的集合对齐框架，用于解决线性化图和语言模型处理序列不匹配的挑战

    

    最近的研究使用预训练语言模型自回归生成线性化图，用于构建事件时间图，取得了令人满意的结果。然而，这些方法通常导致次优图生成，因为线性化图表现出集合特征，而语言模型则按顺序处理这些特征。我们重新构思了任务，将其作为条件集合生成问题，并提出了一种针对大型语言模型的集合对齐框架，以解决这些挑战。

    arXiv:2404.01532v1 Announce Type: new  Abstract: Event temporal graphs have been shown as convenient and effective representations of complex temporal relations between events in text. Recent studies, which employ pre-trained language models to auto-regressively generate linearised graphs for constructing event temporal graphs, have shown promising results. However, these methods have often led to suboptimal graph generation as the linearised graphs exhibit set characteristics which are instead treated sequentially by language models. This discrepancy stems from the conventional text generation objectives, leading to erroneous penalisation of correct predictions caused by the misalignment of elements in target sequences. To address these challenges, we reframe the task as a conditional set generation problem, proposing a Set-aligning Framework tailored for the effective utilisation of Large Language Models (LLMs). The framework incorporates data augmentations and set-property regularis
    
[^10]: OpenChemIE：用于化学文献信息提取的工具包

    OpenChemIE: An Information Extraction Toolkit For Chemistry Literature

    [https://arxiv.org/abs/2404.01462](https://arxiv.org/abs/2404.01462)

    OpenChemIE提出了一种用于从化学文献中提取反应数据的工具包，通过整合文本、表格和图像信息以及使用专门神经模型和算法，实现了在文档级别的反应数据提取。

    

    arXiv:2404.01462v1 公告类型：交叉  摘要：从化学文献中提取信息对于构建数据驱动化学的最新反应数据库至关重要。完整的信息提取需要结合文本、表格和图像中的信息，而以往的工作主要研究从单一方式提取反应。在本文中，我们提出了OpenChemIE来解决这一复杂挑战，实现在文档级别提取反应数据。OpenChemIE分两步解决问题：从各个方式中提取相关信息，然后整合结果得到最终的反应列表。对于第一步，我们采用专门的神经模型，每个模型处理化学信息提取的特定任务，比如从文本或图像中解析分子或反应。然后我们使用化学相关的算法整合这些模块的信息，实现精细化反应提取。

    arXiv:2404.01462v1 Announce Type: cross  Abstract: Information extraction from chemistry literature is vital for constructing up-to-date reaction databases for data-driven chemistry. Complete extraction requires combining information across text, tables, and figures, whereas prior work has mainly investigated extracting reactions from single modalities. In this paper, we present OpenChemIE to address this complex challenge and enable the extraction of reaction data at the document level. OpenChemIE approaches the problem in two steps: extracting relevant information from individual modalities and then integrating the results to obtain a final list of reactions. For the first step, we employ specialized neural models that each address a specific task for chemistry information extraction, such as parsing molecules or reactions from text or figures. We then integrate the information from these modules using chemistry-informed algorithms, allowing for the extraction of fine-grained reactio
    
[^11]: 利用人工智能和社交媒体分析发现GLP-1受体激动剂的不良副作用

    Utilizing AI and Social Media Analytics to Discover Adverse Side Effects of GLP-1 Receptor Agonists

    [https://arxiv.org/abs/2404.01358](https://arxiv.org/abs/2404.01358)

    通过利用人工智能驱动的社交媒体分析，我们开发了一种数字健康方法，成功检测出与GLP-1受体激动剂相关的21种潜在不良副作用，包括易怒和麻木感，从而革新了对新部署药物未报告ASEs的检测。

    

    药物的不良副作用（ASEs）在FDA批准后被发现，对患者安全构成威胁。为了及时发现被忽视的ASEs，我们开发了一种数字健康方法，能够分析来自社交媒体、已发表的临床研究、制造商报告和ChatGPT等大量公开数据。我们发现了与肝素样肽1受体激动剂（GLP-1 RA）相关的ASEs，这一市场预计到2030年将呈指数增长至1335亿美元。利用命名实体识别（NER）模型，我们的方法成功检测出FDA批准时被忽视的21种潜在ASEs，包括易怒和麻木感。我们的数据分析方法彻底改变了对新部署药物相关未报告的ASEs的检测，利用前沿的人工智能驱动社交媒体分析。它可以通过释放社交媒体的力量来支持监管机构和制造商在市场上增加新药的安全性。

    arXiv:2404.01358v1 Announce Type: cross  Abstract: Adverse side effects (ASEs) of drugs, revealed after FDA approval, pose a threat to patient safety. To promptly detect overlooked ASEs, we developed a digital health methodology capable of analyzing massive public data from social media, published clinical research, manufacturers' reports, and ChatGPT. We uncovered ASEs associated with the glucagon-like peptide 1 receptor agonists (GLP-1 RA), a market expected to grow exponentially to $133.5 billion USD by 2030. Using a Named Entity Recognition (NER) model, our method successfully detected 21 potential ASEs overlooked upon FDA approval, including irritability and numbness. Our data-analytic approach revolutionizes the detection of unreported ASEs associated with newly deployed drugs, leveraging cutting-edge AI-driven social media analytics. It can increase the safety of new drugs in the marketplace by unlocking the power of social media to support regulators and manufacturers in the ra
    
[^12]: 通过Latent Dirichlet Allocation主题建模自动检测财经新闻中的相关信息、预测和预测

    Automatic detection of relevant information, predictions and forecasts in financial news through topic modelling with Latent Dirichlet Allocation

    [https://arxiv.org/abs/2404.01338](https://arxiv.org/abs/2404.01338)

    该研究提出了一种新颖的自然语言处理系统，通过Latent Dirichlet Allocation (LDA)进行相关的主题建模，帮助投资者从非结构化文本源中检测财经事件中的相关信息、预测和预测

    

    arXiv:2404.01338v1通告类型:新摘要:金融新闻是一种非结构化的信息源，可以开采以从中提取知识，用于市场筛选应用。从持续的金融新闻流中手动提取相关信息是繁琐的，超出了许多投资者的技能范围，他们最多只能关注几个来源和作者。因此，我们专注于对金融新闻的分析，以识别相关文本，并在该文本中进行预测和预测。我们提出了一种新颖的自然语言处理（NLP）系统，帮助投资者通过考虑话语层面上的相关性和时态性，从非结构化文本源中检测相关的财经事件。首先，我们将文本分割以将相关文本归为一组。其次，我们应用共指解析来发现段落内部的依赖关系。最后，我们利用Latent Dirichlet Allocation (LDA)进行相关主题建模。

    arXiv:2404.01338v1 Announce Type: new  Abstract: Financial news items are unstructured sources of information that can be mined to extract knowledge for market screening applications. Manual extraction of relevant information from the continuous stream of finance-related news is cumbersome and beyond the skills of many investors, who, at most, can follow a few sources and authors. Accordingly, we focus on the analysis of financial news to identify relevant text and, within that text, forecasts and predictions. We propose a novel Natural Language Processing (NLP) system to assist investors in the detection of relevant financial events in unstructured textual sources by considering both relevance and temporality at the discursive level. Firstly, we segment the text to group together closely related text. Secondly, we apply co-reference resolution to discover internal dependencies within segments. Finally, we perform relevant topic modelling with Latent Dirichlet Allocation (LDA) to separ
    
[^13]: 结合自然语言处理和机器学习在金融新闻中检测篇章级别的时间性

    Detection of Temporality at Discourse Level on Financial News by Combining Natural Language Processing and Machine Learning

    [https://arxiv.org/abs/2404.01337](https://arxiv.org/abs/2404.01337)

    通过结合自然语言处理和机器学习技术，提出了一种新颖的系统，旨在在金融新闻中检测篇章级别的关键声明的时间性，以分析句法和语义依赖关系，区分上下文信息和有价值的预测。

    

    Finance-related news, such as Bloomberg News, CNN Business, and Forbes, provide valuable real data for market screening systems. Experts in these news articles not only provide technical analyses but also share opinions considering political, sociological, and cultural factors. We propose a novel system that utilizes Natural Language Processing and Machine Learning techniques to detect the temporality of key statements in finance-related news at the discourse level, aiming to differentiate between context information and valuable predictions by analyzing syntactic and semantic dependencies.

    arXiv:2404.01337v1 Announce Type: new  Abstract: Finance-related news such as Bloomberg News, CNN Business and Forbes are valuable sources of real data for market screening systems. In news, an expert shares opinions beyond plain technical analyses that include context such as political, sociological and cultural factors. In the same text, the expert often discusses the performance of different assets. Some key statements are mere descriptions of past events while others are predictions. Therefore, understanding the temporality of the key statements in a text is essential to separate context information from valuable predictions. We propose a novel system to detect the temporality of finance-related news at discourse level that combines Natural Language Processing and Machine Learning techniques, and exploits sophisticated features such as syntactic and semantic dependencies. More specifically, we seek to extract the dominant tenses of the main statements, which may be either explicit 
    
[^14]: 规划和编辑检索以增强工具学习

    Planning and Editing What You Retrieve for Enhanced Tool Learning

    [https://arxiv.org/abs/2404.00450](https://arxiv.org/abs/2404.00450)

    该论文提出了一种新颖的模型，结合了“规划与检索”和“编辑与确认”范式，通过神经检索模块和LLM-based查询规划器提高了工具利用的效果。

    

    最近在将外部工具与大型语言模型（LLMs）集成方面取得的进展打开了新的领域，应用范围涵盖数学推理、代码生成器和智能助手。然而，现有方法依赖简单的一次性检索策略，无法有效准确地筛选相关工具。本文介绍了一种新颖的“规划与检索（P&R）”和“编辑与确认（E&G）”范式的模型，包括了神经检索模块和基于LLM的查询规划器，以增强工具利用的效果。

    arXiv:2404.00450v1 Announce Type: new  Abstract: Recent advancements in integrating external tools with Large Language Models (LLMs) have opened new frontiers, with applications in mathematical reasoning, code generators, and smart assistants. However, existing methods, relying on simple one-time retrieval strategies, fall short on effectively and accurately shortlisting relevant tools. This paper introduces a novel \modelname (\modelmeaning) approach, encompassing ``Plan-and-Retrieve (P\&R)'' and ``Edit-and-Ground (E\&G)'' paradigms. The P\&R paradigm consists of a neural retrieval module for shortlisting relevant tools and an LLM-based query planner that decomposes complex queries into actionable tasks, enhancing the effectiveness of tool utilization. The E\&G paradigm utilizes LLMs to enrich tool descriptions based on user scenarios, bridging the gap between user queries and tool functionalities. Experiment results demonstrate that these paradigms significantly improve the recall an
    
[^15]: 墨水与个性：在LLMs时代塑造个性化叙事

    Ink and Individuality: Crafting a Personalised Narrative in the Age of LLMs

    [https://arxiv.org/abs/2404.00026](https://arxiv.org/abs/2404.00026)

    研究探讨了人们日益依赖的基于LLM的写作助手对创造力和个性可能造成的负面影响，旨在改进人机交互系统和提升写作助手的个性化和个性化功能。

    

    个性和个性化构成了使每个作家独特并影响其文字以有效吸引读者同时传达真实性的独特特征。然而，我们日益依赖基于LLM的写作助手可能会危及我们的创造力和个性。我们经常忽视这一趋势对我们的创造力和独特性的负面影响，尽管可能会造成后果。本研究通过进行简要调查探索不同的观点和概念，以及尝试理解人们的观点，结合以往在该领域的研究，来研究这些问题。解决这些问题对于改进人机交互系统和增强个性化和个性化写作助手至关重要。

    arXiv:2404.00026v1 Announce Type: cross  Abstract: Individuality and personalization comprise the distinctive characteristics that make each writer unique and influence their words in order to effectively engage readers while conveying authenticity. However, our growing reliance on LLM-based writing assistants risks compromising our creativity and individuality over time. We often overlook the negative impacts of this trend on our creativity and uniqueness, despite the possible consequences. This study investigates these concerns by performing a brief survey to explore different perspectives and concepts, as well as trying to understand people's viewpoints, in conjunction with past studies in the area. Addressing these issues is essential for improving human-computer interaction systems and enhancing writing assistants for personalization and individuality.
    
[^16]: FeatUp: 一个与模型无关的特征任意分辨率框架

    FeatUp: A Model-Agnostic Framework for Features at Any Resolution

    [https://arxiv.org/abs/2403.10516](https://arxiv.org/abs/2403.10516)

    FeatUp是一个任务和模型无关的框架，用于在深度特征中恢复丢失的空间信息，从而使特征可以以任何分辨率重建，在现有应用中取得分辨率和性能的提升。

    

    深度特征是计算机视觉研究的基石，捕捉图像语义并使社区能够解决下游任务，即使在零或少样本情况下也能做到。然而，这些特征通常缺乏空间分辨率，无法直接执行像分割和深度预测这样的稠密预测任务，因为模型会过于聚合大范围的信息。在这项工作中，我们介绍了FeatUp，一个任务和模型无关的框架，用于恢复深度特征中丢失的空间信息。我们介绍了FeatUp的两个变体：一个在单次前向传递中引导具有高分辨率信号的特征，另一个适应单个图像并以任何分辨率重构特征的隐式模型。这两种方法都使用了一个具有与 NeRF 类似的深度类比的多视图一致性损失。我们的特征保留其原始语义，并可以替换现有应用程序，即使不重新

    arXiv:2403.10516v1 Announce Type: cross  Abstract: Deep features are a cornerstone of computer vision research, capturing image semantics and enabling the community to solve downstream tasks even in the zero- or few-shot regime. However, these features often lack the spatial resolution to directly perform dense prediction tasks like segmentation and depth prediction because models aggressively pool information over large areas. In this work, we introduce FeatUp, a task- and model-agnostic framework to restore lost spatial information in deep features. We introduce two variants of FeatUp: one that guides features with high-resolution signal in a single forward pass, and one that fits an implicit model to a single image to reconstruct features at any resolution. Both approaches use a multi-view consistency loss with deep analogies to NeRFs. Our features retain their original semantics and can be swapped into existing applications to yield resolution and performance gains even without re-
    
[^17]: 朝着可信的再排序：一种简单但有效的弃权机制

    Towards Trustworthy Reranking: A Simple yet Effective Abstention Mechanism

    [https://arxiv.org/abs/2402.12997](https://arxiv.org/abs/2402.12997)

    提出了一种适用于现实约束的轻量级弃权机制，特别适用于再排序阶段，通过数据驱动的方法达到有效性，并提供了开源代码以促进其更广泛的应用。

    

    神经信息检索（NIR）已经显著改进了基于启发式的IR系统。然而，失败仍然频繁发生，通常所使用的模型无法检索与用户查询相关的文档。我们通过提出一种适用于现实约束的轻量级弃权机制来解决这一挑战，特别强调再排序阶段。我们介绍了一个协议，用于在黑匣子场景中评估弃权策略的效果，并提出了一种简单但有效的数据驱动机制。我们提供了实验复制和弃权实施的开源代码，促进其在不同环境中更广泛的采用和应用。

    arXiv:2402.12997v1 Announce Type: cross  Abstract: Neural Information Retrieval (NIR) has significantly improved upon heuristic-based IR systems. Yet, failures remain frequent, the models used often being unable to retrieve documents relevant to the user's query. We address this challenge by proposing a lightweight abstention mechanism tailored for real-world constraints, with particular emphasis placed on the reranking phase. We introduce a protocol for evaluating abstention strategies in a black-box scenario, demonstrating their efficacy, and propose a simple yet effective data-driven mechanism. We provide open-source code for experiment replication and abstention implementation, fostering wider adoption and application in diverse contexts.
    
[^18]: 应对后API困境：搜索引擎结果页面呈现社交媒体数据的偏见观

    Navigating the Post-API Dilemma Search Engine Results Pages Present a Biased View of Social Media Data

    [https://arxiv.org/abs/2401.15479](https://arxiv.org/abs/2401.15479)

    搜索引擎结果页面可以作为社交媒体数据的替代方案，但存在对流行帖子偏见较高、情感更积极以及忽视政治、色情和粗俗帖子的问题。

    

    最近停止访问社交媒体API的决定对互联网研究和整个计算社会科学领域产生了不利影响。这种对数据的访问缺乏已被称为互联网研究的后API时代。幸运的是，流行的搜索引擎有能力爬取、捕获和展示社交媒体数据在其搜索引擎结果页面(SERP)上，如果提供适当的搜索查询，可能会为这一困境提供解决方案。在当前工作中，我们问：SERP是否提供社交媒体数据的完整和无偏见样本？ SERP是否是直接API访问的可行替代方案？为了回答这些问题，我们对（Google）SERP结果和来自Reddit和Twitter/X的非取样数据进行了比较分析。我们发现，SERP结果在支持流行帖子方面存在高度偏见；反对政治、色情和粗俗帖子；在情感上更为积极；并有大

    arXiv:2401.15479v2 Announce Type: replace-cross  Abstract: Recent decisions to discontinue access to social media APIs are having detrimental effects on Internet research and the field of computational social science as a whole. This lack of access to data has been dubbed the Post-API era of Internet research. Fortunately, popular search engines have the means to crawl, capture, and surface social media data on their Search Engine Results Pages (SERP) if provided the proper search query, and may provide a solution to this dilemma. In the present work we ask: does SERP provide a complete and unbiased sample of social media data? Is SERP a viable alternative to direct API-access? To answer these questions, we perform a comparative analysis between (Google) SERP results and nonsampled data from Reddit and Twitter/X. We find that SERP results are highly biased in favor of popular posts; against political, pornographic, and vulgar posts; are more positive in their sentiment; and have large 
    
[^19]: 分离和学习潜在混淆因素以增强用户偏好建模

    Separating and Learning Latent Confounders to Enhancing User Preferences Modeling

    [https://arxiv.org/abs/2311.03381](https://arxiv.org/abs/2311.03381)

    通过分离和学习潜在混淆因素，提高了用户偏好建模的准确性

    

    推荐模型旨在从历史反馈中捕获用户偏好，然后预测用户对候选项目的特定反馈。然而，各种未测量的混淆因素导致历史反馈中的用户偏好与真实偏好之间存在偏差，进而导致模型未达到预期的性能。现有的去偏模型要么特定于解决特定偏差，要么直接从用户历史反馈中获取辅助信息，这无法确定所学偏好是真实用户偏好还是混入未测量的混淆因素。此外，我们发现以前的推荐系统不仅是未测量的混淆因素的后继者，还会作为影响用户偏好建模的未测量混淆因素，这在先前的研究中一直被忽视。为此，我们将前述推荐系统的影响纳入考虑，并将其视为

    arXiv:2311.03381v2 Announce Type: replace-cross  Abstract: Recommender models aim to capture user preferences from historical feedback and then predict user-specific feedback on candidate items. However, the presence of various unmeasured confounders causes deviations between the user preferences in the historical feedback and the true preferences, resulting in models not meeting their expected performance. Existing debias models either (1) specific to solving one particular bias or (2) directly obtain auxiliary information from user historical feedback, which cannot identify whether the learned preferences are true user preferences or mixed with unmeasured confounders. Moreover, we find that the former recommender system is not only a successor to unmeasured confounders but also acts as an unmeasured confounder affecting user preference modeling, which has always been neglected in previous studies. To this end, we incorporate the effect of the former recommender system and treat it as
    
[^20]: 超越是与否：通过得分细粒度相关性标签来改进零样本LLM排序器

    Beyond Yes and No: Improving Zero-Shot LLM Rankers via Scoring Fine-Grained Relevance Labels

    [https://arxiv.org/abs/2310.14122](https://arxiv.org/abs/2310.14122)

    通过将细粒度相关性标签纳入LLM排序器提示中，可以显著改进零样本LLM排序器的性能

    

    最近LLM技术驱动的零样本文本排序器通过简单的提示获得了显著的排序性能。 现有的逐点LLM排序器提示大多要求模型从“是”和“否”等二元相关性标签中进行选择。然而，缺少中间相关性标签选项可能会导致LLM在部分相关于查询的文档上提供嘈杂或有偏见的答案。我们提出将细粒度相关性标签纳入LLM排序器的提示中，使其能够更好地区分具有不同相关性水平的文档，从而得出更准确的排序。我们研究了两种变体的提示模板，配合不同数量的相关性级别。我们在8个BEIR数据集上的实验表明，添加细粒度相关性标签显著提高了LLM排序器的性能。

    arXiv:2310.14122v3 Announce Type: replace  Abstract: Zero-shot text rankers powered by recent LLMs achieve remarkable ranking performance by simply prompting. Existing prompts for pointwise LLM rankers mostly ask the model to choose from binary relevance labels like "Yes" and "No". However, the lack of intermediate relevance label options may cause the LLM to provide noisy or biased answers for documents that are partially relevant to the query. We propose to incorporate fine-grained relevance labels into the prompt for LLM rankers, enabling them to better differentiate among documents with different levels of relevance to the query and thus derive a more accurate ranking. We study two variants of the prompt template, coupled with different numbers of relevance levels. Our experiments on 8 BEIR data sets show that adding fine-grained relevance labels significantly improves the performance of LLM rankers.
    
[^21]: 使用来自可比用户生成内容的词嵌入构建跨语言消费者健康词汇

    Constructing Cross-lingual Consumer Health Vocabulary with Word-Embedding from Comparable User Generated Content

    [https://arxiv.org/abs/2206.11612](https://arxiv.org/abs/2206.11612)

    通过使用来自可比用户生成内容的词嵌入，提出了一个跨语言自动术语识别框架，将英语消费者健康词汇扩展为跨语言词汇。

    

    在线健康社区（OHC）是普通人分享健康信息的主要渠道。为了分析OHC中消费者生成内容（HCGC），识别普通人使用的口头医学表达是一个关键挑战。开放获取和协作的消费者健康词汇（OAC CHV）是应对这一挑战的受控词汇。然而，OAC CHV仅在英语中可用，限制了其在其他语言中的适用性。本研究提出了一个跨语言自动术语识别框架，用于将英语CHV扩展为跨语言CHV。我们的框架需要一个英语HCGC语料库和一个非英语（本研究中为中文）HCGC语料库作为输入。使用skip-gram算法确定了两个单语词向量空间，使得每个空间编码了语言内普通人之间的常见词关联。根据等距假设，该研究提出了一种评估两种语言之间映射关系的方法。

    arXiv:2206.11612v2 Announce Type: replace  Abstract: The online health community (OHC) is the primary channel for laypeople to share health information. To analyze the health consumer-generated content (HCGC) from the OHCs, identifying the colloquial medical expressions used by laypeople is a critical challenge. The open-access and collaborative consumer health vocabulary (OAC CHV) is the controlled vocabulary for addressing such a challenge. Nevertheless, OAC CHV is only available in English, limiting its applicability to other languages. This research proposes a cross-lingual automatic term recognition framework for extending the English CHV into a cross-lingual one. Our framework requires an English HCGC corpus and a non-English (i.e., Chinese in this study) HCGC corpus as inputs. Two monolingual word vector spaces are determined using the skip-gram algorithm so that each space encodes common word associations from laypeople within a language. Based on the isometry assumption, the f
    
[^22]: 跨领域序列推荐的综述

    A Survey on Cross-Domain Sequential Recommendation. (arXiv:2401.04971v1 [cs.IR])

    [http://arxiv.org/abs/2401.04971](http://arxiv.org/abs/2401.04971)

    跨领域序列推荐通过集成和学习多个领域的交互信息，将用户偏好建模从平面转向立体。文章对CDSR问题进行了定义和分析，提供了从宏观和微观两个视角的系统概述。对于不同领域间的模型，总结了多层融合结构和融合桥梁。对于现有模型，讨论了基础技术和辅助学习技术。展示了公开数据集和实验结果，并给出了未来发展的见解。

    

    跨领域序列推荐（CDSR）通过在不同粒度（从序列间到序列内，从单领域到跨领域）上集成和学习来自多个领域的交互信息，将用户偏好建模从平面转向了立体。本综述文章中，我们首先使用四维张量定义了CDSR问题，并分析了其在多维度降维下的多类型输入表示。接下来，我们从整体和细节两个视角提供了系统的概述。从整体视角，我们总结了各个模型在不同领域间的多层融合结构，并讨论了它们的融合桥梁。从细节视角，我们着重讨论了现有模型的基础技术，并解释了辅助学习技术。最后，我们展示了可用的公开数据集和代表性的实验结果，并提供了对未来发展的一些见解。

    Cross-domain sequential recommendation (CDSR) shifts the modeling of user preferences from flat to stereoscopic by integrating and learning interaction information from multiple domains at different granularities (ranging from inter-sequence to intra-sequence and from single-domain to cross-domain).In this survey, we initially define the CDSR problem using a four-dimensional tensor and then analyze its multi-type input representations under multidirectional dimensionality reductions. Following that, we provide a systematic overview from both macro and micro views. From a macro view, we abstract the multi-level fusion structures of various models across domains and discuss their bridges for fusion. From a micro view, focusing on the existing models, we specifically discuss the basic technologies and then explain the auxiliary learning technologies. Finally, we exhibit the available public datasets and the representative experimental results as well as provide some insights into future d
    
[^23]: LLM-Rec: 通过引导大型语言模型进行个性化推荐

    LLM-Rec: Personalized Recommendation via Prompting Large Language Models. (arXiv:2307.15780v1 [cs.CL])

    [http://arxiv.org/abs/2307.15780](http://arxiv.org/abs/2307.15780)

    本文通过引导大型语言模型进行个性化推荐的研究，提出了四种不同的引导策略，并通过实验证明了这些策略的有效性。这一发现强调了在个性化内容推荐中，采用多样的引导和输入增强技术可以提高大型语言模型的推荐性能。

    

    本文通过输入增强技术，研究了多种不同的引导策略，以提高大型语言模型（LLM）在个性化内容推荐方面的性能。我们提出的方法名为LLM-Rec，包括四种不同的引导策略：（1）基础引导，（2）推荐驱动引导，（3）参与引导引导，和（4）推荐驱动+参与引导引导。实验证明，将原始内容描述与LLM生成的增强输入文本结合起来，采用这些引导策略可以提高推荐性能。这一发现强调了在个性化内容推荐中，通过引入多样的引导和输入增强技术来提升大型语言模型的推荐能力的重要性。

    We investigate various prompting strategies for enhancing personalized content recommendation performance with large language models (LLMs) through input augmentation. Our proposed approach, termed LLM-Rec, encompasses four distinct prompting strategies: (1) basic prompting, (2) recommendation-driven prompting, (3) engagement-guided prompting, and (4) recommendation-driven + engagement-guided prompting. Our empirical experiments show that combining the original content description with the augmented input text generated by LLM using these prompting strategies leads to improved recommendation performance. This finding highlights the importance of incorporating diverse prompts and input augmentation techniques to enhance the recommendation capabilities with large language models for personalized content recommendation.
    
[^24]: 利用推荐系统缩小对等生产平台上的内容差距

    Leveraging Recommender Systems to Reduce Content Gaps on Peer Production Platforms. (arXiv:2307.08669v2 [cs.CY] UPDATED)

    [http://arxiv.org/abs/2307.08669](http://arxiv.org/abs/2307.08669)

    该研究通过在SuggestBot上进行离线分析和为期三个月的对照实验，发现推荐被低估主题的文章可以增加在这些文章上的编辑工作量，同时不会明显降低对推荐内容的接受程度。这一发现对解决对等生产平台上的内容缺口问题具有重要意义。

    

    维基百科等对等生产平台常常存在内容缺口。先前研究表明，推荐系统可以帮助解决这个问题，通过引导编辑人员关注被低估的主题。然而，目前尚不清楚这种方法是否会导致不太相关的推荐，从而降低对推荐内容的整体参与度。为了回答这个问题，我们首先在SuggestBot上进行了离线分析（研究1），然后进行了为期三个月的对照实验（研究2）。我们的结果显示，向用户展示来自被低估主题的文章可以增加在这些文章上的工作量，而不会明显降低对推荐内容的接受程度。我们讨论了结果的意义，包括如何忽视文章发现过程可能会人为地限制推荐。我们以"过滤气泡"的常见问题来展示这一现象，并对任何平台都存在的类似问题进行了类比。

    Peer production platforms like Wikipedia commonly suffer from content gaps. Prior research suggests recommender systems can help solve this problem, by guiding editors towards underrepresented topics. However, it remains unclear whether this approach would result in less relevant recommendations, leading to reduced overall engagement with recommended items. To answer this question, we first conducted offline analyses (Study 1) on SuggestBot, a task-routing recommender system for Wikipedia, then did a three-month controlled experiment (Study 2). Our results show that presenting users with articles from underrepresented topics increased the proportion of work done on those articles without significantly reducing overall recommendation uptake. We discuss the implications of our results, including how ignoring the article discovery process can artificially narrow recommendations. We draw parallels between this phenomenon and the common issue of "filter bubbles" to show how any platform tha
    
[^25]: N-元事实的少样本链接预测

    Few-shot Link Prediction on N-ary Facts. (arXiv:2305.06104v1 [cs.AI])

    [http://arxiv.org/abs/2305.06104](http://arxiv.org/abs/2305.06104)

    本文提出了一个新任务——少样本N-元事实链接预测，并提出了一个名为FLEN的模型来实现。FLEN由三个模块组成，可以从有限的标记实例中预测N-元事实中的缺失实体。

    

    N-元事实由主要三元组（头实体、关系、尾实体）和任意数量的辅助属性值对组成，这在现实世界的知识图谱中很常见。对于N-元事实的链接预测是预测其中一个元素的缺失，填补缺失元素有助于丰富知识图谱并促进许多下游应用程序。以往的研究通常需要大量高质量的数据来理解N-元事实中的元素，但这些研究忽视了少样本关系，在现实世界的场景中却很常见。因此，本文引入一个新任务——少样本N-元事实链接预测，旨在使用有限的标记实例来预测N-元事实中的缺失实体。我们也提出了一个针对N-元事实的少样本链接预测模型FLEN，它由三个模块组成：关系学习模块、支持特定调整模块和查询推理模块。

    N-ary facts composed of a primary triple (head entity, relation, tail entity) and an arbitrary number of auxiliary attribute-value pairs, are prevalent in real-world knowledge graphs (KGs). Link prediction on n-ary facts is to predict a missing element in an n-ary fact. This helps populate and enrich KGs and further promotes numerous downstream applications. Previous studies usually require a substantial amount of high-quality data to understand the elements in n-ary facts. However, these studies overlook few-shot relations, which have limited labeled instances, yet are common in real-world scenarios. Thus, this paper introduces a new task, few-shot link prediction on n-ary facts. It aims to predict a missing entity in an n-ary fact with limited labeled instances. We further propose a model for Few-shot Link prEdict on N-ary facts, thus called FLEN, which consists of three modules: the relation learning, support-specific adjusting, and query inference modules. FLEN captures relation me
    

