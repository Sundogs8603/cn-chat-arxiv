# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Evaluation of GPT-3.5 and GPT-4 for supporting real-world information needs in healthcare delivery.](http://arxiv.org/abs/2304.13714) | 本研究评估了在临床环境中使用GPT-3.5和GPT-4解决医学问题的安全性以及与信息技术咨询服务报告的一致性。研究结果表明，两个LLMs都可以以安全和一致的方式满足医生的信息需求。 |
| [^2] | [Building K-Anonymous User Cohorts with\\ Consecutive Consistent Weighted Sampling (CCWS).](http://arxiv.org/abs/2304.13677) | 本文提出了一种可扩展的 K-匿名组队算法-连续一致加权抽样（CCWS）来构建具有相似特征的用户群组，以检索个性化广告。 |
| [^3] | [A Personalized Dense Retrieval Framework for Unified Information Access.](http://arxiv.org/abs/2304.13654) | 本文提出了一个名为「\framework」的通用且可扩展的稠密检索框架，它可以处理各种个性化的信息访问请求，并通过个性化关注网络将用户的偏好纳入考虑，从而提供更加贴心和准确的个性化信息访问体验。 |
| [^4] | [A Symmetric Dual Encoding Dense Retrieval Framework for Knowledge-Intensive Visual Question Answering.](http://arxiv.org/abs/2304.13649) | 本文提出了一种新的对称双编码稠密检索框架DEDR，以弥合文本和图像之间的空间差距，并进一步引入了MM-FiD，一种多模式融合解码器模型，用于知识密集型视觉问答任务，其效果优于现有最先进方法。 |
| [^5] | [Reformulating CTR Prediction: Learning Invariant Feature Interactions for Recommendation.](http://arxiv.org/abs/2304.13643) | 重塑CTR预测：学习稳定的特征交互，以实现更好的未来预测结果。 |
| [^6] | [Improvements on Recommender System based on Mathematical Principles.](http://arxiv.org/abs/2304.13579) | 本文基于数学原理研究推荐系统算法的实现和改进方法，重要的概率算法是提高算法准确性和速度的关键，同时介绍两种不同数学距离的优缺点。 |
| [^7] | [Key-value information extraction from full handwritten pages.](http://arxiv.org/abs/2304.13530) | 这篇论文提出了一种从手写文档中提取信息的方法，该方法基于Transformer，在同一模型中结合了特征提取、手写识别和命名实体识别步骤。该方法不需要预先分割并在三个公共数据库上表现出色。 |
| [^8] | [Examining the Impact of Uncontrolled Variables on Physiological Signals in User Studies for Information Processing Activities.](http://arxiv.org/abs/2304.13488) | 研究了信息处理活动用户研究中非受控变量对生理信号的影响，发现其中任务持续时间是个体差异的重要指标。 |
| [^9] | [STIR: Siamese Transformer for Image Retrieval Postprocessing.](http://arxiv.org/abs/2304.13393) | 这项工作提出了两部分内容。首先，他们构建了一个基于三元组损失的简单模型，性能达到了最先进水平，但没有复杂模型的缩放问题。其次，他们提出了一种新颖的后处理方法STIR，可在单个前向传递中重新排列多个顶部输出，而不依赖于全局/局部特征提取。 |
| [^10] | [Deep Lifelong Cross-modal Hashing.](http://arxiv.org/abs/2304.13357) | 本文提出了一种深度学习的终身学习跨模态哈希方法，可以在不重复训练哈希函数的情况下实现生命周期哈希检索，而且通过直接训练增量数据来更新哈希函数，避免了灾难性遗忘和非连续哈希检索更新的耗时。 |
| [^11] | [Improving Conversational Passage Re-ranking with View Ensemble.](http://arxiv.org/abs/2304.13290) | 本文提出了ConvRerank，一种采用视图集成的会话段落重新排序方法，提高了会话搜索的效率和效果。 |
| [^12] | [Self-Supervised Multi-Modal Sequential Recommendation.](http://arxiv.org/abs/2304.13277) | 本文提出了一种自监督的多模态顺序推荐方法，采用双塔检索架构和模态特征，能够有效处理项目冷启动和域转移问题，并在多个基准数据集上取得了最优结果。 |
| [^13] | [Generative Relevance Feedback with Large Language Models.](http://arxiv.org/abs/2304.13157) | 本文提出了一种基于大型语言模型的生成式相关反馈方法（GRF），不同于以往的伪相关反馈方法，它从长形文本中构建概率反馈模型，综合实验结果显示，在各种文献检索基准测试中，GRF方法具有显着的优势。 |
| [^14] | [Modeling Spoken Information Queries for Virtual Assistants: Open Problems, Challenges and Opportunities.](http://arxiv.org/abs/2304.13149) | 讨论了虚拟助手中口语信息查询建模的问题和挑战，以及信息检索方法中的机遇；探讨了如何通过查询领域分类、知识图谱等提高语音识别的准确性；简要概述了语音识别中的挑战。 |
| [^15] | [Introducing MBIB -- the first Media Bias Identification Benchmark Task and Dataset Collection.](http://arxiv.org/abs/2304.13148) | 这篇论文介绍了MBIB，一个将不同类型媒体偏见分为共同框架的全面基准测试，并提供了相关数据集以评估媒体偏见检测技术，结果显示没有单一技术可以显著优于其他技术，同时发现研究兴趣和资源分配不均匀分布。 |
| [^16] | [Patterns of gender-specializing query reformulation.](http://arxiv.org/abs/2304.13129) | 本文研究了一类查询重构，即在查询中添加人口统计属性，以探索用户和搜索系统之间的关系。 |
| [^17] | [OFAR: A Multimodal Evidence Retrieval Framework for Illegal Live-streaming Identification.](http://arxiv.org/abs/2304.12608) | OFAR是一种用于非法直播识别的证据检索框架，能够帮助直播平台立即识别直播中的非法行为。 |
| [^18] | [Meta-optimized Contrastive Learning for Sequential Recommendation.](http://arxiv.org/abs/2304.07763) | 本文提出了 MCLRec 模型，该模型在数据增强和可学习模型增强操作的基础上，解决了现有对比学习方法难以推广和训练数据不足的问题。 |
| [^19] | [RecD: Deduplication for End-to-End Deep Learning Recommendation Model Training Infrastructure.](http://arxiv.org/abs/2211.05239) | RecD 是一种为 DLRM 训练提供去重功能的端到端基础设施优化，解决了由于特征重复造成的海量存储、预处理和训练开销，引入了新的张量格式 InverseKeyedJaggedTensors (IKJTs) 来去除特征值的重复，使 DLRM 模型架构能够更好地利用数据的重复性提高训练吞吐量。 |
| [^20] | [Law Article-Enhanced Legal Case Matching: a Causal Learning Approach.](http://arxiv.org/abs/2210.11012) | 本文提出了一种因果学习方法，将法律条款考虑在内以改善法律案件匹配的结果，实验证明该方法优于现有基准方法。 |
| [^21] | [DECONET: an Unfolding Network for Analysis-based Compressed Sensing with Generalization Error Bounds.](http://arxiv.org/abs/2205.07050) | 本文提出了一种名为DECONET的新型深度展开网络，可以用于基于分析稀疏性的压缩感知，能有效地重构向量并优于现有的展开网络，在估计了其泛化误差的基础上得出相关结论。 |

# 详细

[^1]: 评估GPT-3.5和GPT-4在支持医疗保健信息需求方面的实际作用

    Evaluation of GPT-3.5 and GPT-4 for supporting real-world information needs in healthcare delivery. (arXiv:2304.13714v1 [cs.AI])

    [http://arxiv.org/abs/2304.13714](http://arxiv.org/abs/2304.13714)

    本研究评估了在临床环境中使用GPT-3.5和GPT-4解决医学问题的安全性以及与信息技术咨询服务报告的一致性。研究结果表明，两个LLMs都可以以安全和一致的方式满足医生的信息需求。

    

    尽管在医疗保健领域使用大型语言模型(LLMs)越来越受关注，但当前的探索并未评估LLMs在临床环境中的实用性和安全性。我们的目标是确定两个LLM是否可以以安全和一致的方式满足由医生提交的信息需求问题。我们将66个来自信息技术咨询服务的问题通过简单的提示提交给GPT-3.5和GPT-4。12名医生评估了LLM响应对患者造成伤害的可能性以及与信息技术咨询服务的现有报告的一致性。医生的评估基于多数票汇总。对于没有任何问题，大多数医生认为任何一个LLM响应都不会造成伤害。对于GPT-3.5，8个问题的响应与信息技术咨询报告一致，20个不一致，9个无法评估。有29个响应没有多数票表示“同意”、“不同意”和“无法评估”。

    Despite growing interest in using large language models (LLMs) in healthcare, current explorations do not assess the real-world utility and safety of LLMs in clinical settings. Our objective was to determine whether two LLMs can serve information needs submitted by physicians as questions to an informatics consultation service in a safe and concordant manner. Sixty six questions from an informatics consult service were submitted to GPT-3.5 and GPT-4 via simple prompts. 12 physicians assessed the LLM responses' possibility of patient harm and concordance with existing reports from an informatics consultation service. Physician assessments were summarized based on majority vote. For no questions did a majority of physicians deem either LLM response as harmful. For GPT-3.5, responses to 8 questions were concordant with the informatics consult report, 20 discordant, and 9 were unable to be assessed. There were 29 responses with no majority on "Agree", "Disagree", and "Unable to assess". Fo
    
[^2]: 利用连续一致加权采样（CCWS）构建 K-匿名用户组队

    Building K-Anonymous User Cohorts with\\ Consecutive Consistent Weighted Sampling (CCWS). (arXiv:2304.13677v1 [cs.IR])

    [http://arxiv.org/abs/2304.13677](http://arxiv.org/abs/2304.13677)

    本文提出了一种可扩展的 K-匿名组队算法-连续一致加权抽样（CCWS）来构建具有相似特征的用户群组，以检索个性化广告。

    

    为了在保护用户隐私的前提下检索个性化广告系列和创意，数字广告正在从基于成员的身份转向基于小组的身份。在这种身份制度下，需要一种准确高效的组队算法来将具有相似特征的用户分组。本文提出了一种可扩展的 K-匿名组队算法，称为连续一致加权抽样（CCWS）。该方法将一致加权抽样（p -powered）和分层聚类的思想结合起来，从而通过强制限制小组大小的下限来确保 K-匿名性。在由 70 多个百万用户和广告系列组成的 LinkedIn 数据集上的评估显示，CCWS 相对于几种基于哈希的方法（包括 SignRP、MinHash 以及基本的 CWS）具有显着的改进。

    To retrieve personalized campaigns and creatives while protecting user privacy, digital advertising is shifting from member-based identity to cohort-based identity. Under such identity regime, an accurate and efficient cohort building algorithm is desired to group users with similar characteristics. In this paper, we propose a scalable $K$-anonymous cohort building algorithm called {\em consecutive consistent weighted sampling} (CCWS). The proposed method combines the spirit of the ($p$-powered) consistent weighted sampling and hierarchical clustering, so that the $K$-anonymity is ensured by enforcing a lower bound on the size of cohorts. Evaluations on a LinkedIn dataset consisting of $>70$M users and ads campaigns demonstrate that CCWS achieves substantial improvements over several hashing-based methods including sign random projections (SignRP), minwise hashing (MinHash), as well as the vanilla CWS.
    
[^3]: 个性化稠密检索框架：统一信息访问

    A Personalized Dense Retrieval Framework for Unified Information Access. (arXiv:2304.13654v1 [cs.IR])

    [http://arxiv.org/abs/2304.13654](http://arxiv.org/abs/2304.13654)

    本文提出了一个名为「\framework」的通用且可扩展的稠密检索框架，它可以处理各种个性化的信息访问请求，并通过个性化关注网络将用户的偏好纳入考虑，从而提供更加贴心和准确的个性化信息访问体验。

    

    开发一种通用的模型，可以高效、有效地响应从检索到推荐、到回答问题等广泛的信息访问请求，一直是信息检索社区的长期目标。本文认为近年来稠密检索和近邻搜索的发展带来的灵活性、高效性和有效性已经为实现这一目标铺平了道路。我们开发了一个名为「\framework」的通用且可扩展的稠密检索框架，可以处理各种（个性化的）信息访问请求，例如关键字搜索、示例查询和补充物品推荐。我们提出的方法通过开发个性化关注网络，将用户特定的偏好纳入考虑，从而扩展了稠密检索模型在即时检索任务中的功能，使个性化信息访问体验更加贴心和准确。

    Developing a universal model that can efficiently and effectively respond to a wide range of information access requests -- from retrieval to recommendation to question answering -- has been a long-lasting goal in the information retrieval community. This paper argues that the flexibility, efficiency, and effectiveness brought by the recent development in dense retrieval and approximate nearest neighbor search have smoothed the path towards achieving this goal. We develop a generic and extensible dense retrieval framework, called \framework, that can handle a wide range of (personalized) information access requests, such as keyword search, query by example, and complementary item recommendation. Our proposed approach extends the capabilities of dense retrieval models for ad-hoc retrieval tasks by incorporating user-specific preferences through the development of a personalized attentive network. This allows for a more tailored and accurate personalized information access experience. Ou
    
[^4]: 一种用于知识密集型视觉问答的对称双编码稠密检索框架

    A Symmetric Dual Encoding Dense Retrieval Framework for Knowledge-Intensive Visual Question Answering. (arXiv:2304.13649v1 [cs.CV])

    [http://arxiv.org/abs/2304.13649](http://arxiv.org/abs/2304.13649)

    本文提出了一种新的对称双编码稠密检索框架DEDR，以弥合文本和图像之间的空间差距，并进一步引入了MM-FiD，一种多模式融合解码器模型，用于知识密集型视觉问答任务，其效果优于现有最先进方法。

    

    知识密集型视觉问答（KI-VQA）是指回答关于图像的问题，其答案不在图像中。本文提出了一种新的KI-VQA任务流程，包括一个检索器和一个阅读器。首先，我们介绍了DEDR，它是一种对称双编码密集检索框架，其中使用单模（文本）和多模编码器将文档和查询编码为共享嵌入空间。我们引入了一种迭代知识蒸馏方法，以弥合这两个编码器中的表示空间之间的差距。对两个成熟的KI-VQA数据集OK-VQA和FVQA进行广泛的评估表明，DEDR在OK-VQA和FVQA上的性能比最先进的基线方法分别提高了11.6％和30.9％。利用DEDR检索到的段落，我们还进一步介绍了MM-FiD，一种编码器-解码器多模式融合解码器模型，用于为KI-VQA任务生成文本答案。MM-FiD将问题、图像和每个检索到的段落编码为单独的向量，并通过从它们的连接解码生成答案。广泛的实验表明，MM-FiD在OK-VQA和FVQA数据集上均优于最先进的方法。

    Knowledge-Intensive Visual Question Answering (KI-VQA) refers to answering a question about an image whose answer does not lie in the image. This paper presents a new pipeline for KI-VQA tasks, consisting of a retriever and a reader. First, we introduce DEDR, a symmetric dual encoding dense retrieval framework in which documents and queries are encoded into a shared embedding space using uni-modal (textual) and multi-modal encoders. We introduce an iterative knowledge distillation approach that bridges the gap between the representation spaces in these two encoders. Extensive evaluation on two well-established KI-VQA datasets, i.e., OK-VQA and FVQA, suggests that DEDR outperforms state-of-the-art baselines by 11.6% and 30.9% on OK-VQA and FVQA, respectively. Utilizing the passages retrieved by DEDR, we further introduce MM-FiD, an encoder-decoder multi-modal fusion-in-decoder model, for generating a textual answer for KI-VQA tasks. MM-FiD encodes the question, the image, and each retri
    
[^5]: 重构CTR预测：学习推荐系统中不变的特征交互

    Reformulating CTR Prediction: Learning Invariant Feature Interactions for Recommendation. (arXiv:2304.13643v1 [cs.IR])

    [http://arxiv.org/abs/2304.13643](http://arxiv.org/abs/2304.13643)

    重塑CTR预测：学习稳定的特征交互，以实现更好的未来预测结果。

    

    点击率（CTR）预测在推荐系统中起着核心作用，作为用户排名项目的最终过滤器。解决CTR任务的关键是学习对预测有用的特征交互作用，通常通过使用经验风险最小化模式来适配历史点击数据来实现。代表方法包括分解机和深度兴趣网络，在工业应用中取得了广泛成功。然而，这种方法不可避免地学习到不稳定的特征交互作用，即在历史数据中表现出强相关性但在未来交付中的泛化性能差。在这项工作中，我们重构了CTR任务 - 不再关注历史数据上的经验风险最小化，而是将历史数据按时间顺序分成几个期间（称为环境），旨在学习稳定的特征交互作用。这些特征交互作用应该能够更好地推广到预测未来。

    Click-Through Rate (CTR) prediction plays a core role in recommender systems, serving as the final-stage filter to rank items for a user. The key to addressing the CTR task is learning feature interactions that are useful for prediction, which is typically achieved by fitting historical click data with the Empirical Risk Minimization (ERM) paradigm. Representative methods include Factorization Machines and Deep Interest Network, which have achieved wide success in industrial applications. However, such a manner inevitably learns unstable feature interactions, i.e., the ones that exhibit strong correlations in historical data but generalize poorly for future serving. In this work, we reformulate the CTR task -- instead of pursuing ERM on historical data, we split the historical data chronologically into several periods (a.k.a, environments), aiming to learn feature interactions that are stable across periods. Such feature interactions are supposed to generalize better to predict future 
    
[^6]: 基于数学原理的推荐系统改进

    Improvements on Recommender System based on Mathematical Principles. (arXiv:2304.13579v1 [cs.IR])

    [http://arxiv.org/abs/2304.13579](http://arxiv.org/abs/2304.13579)

    本文基于数学原理研究推荐系统算法的实现和改进方法，重要的概率算法是提高算法准确性和速度的关键，同时介绍两种不同数学距离的优缺点。

    

    本文将研究推荐系统的实现原理和算法。我们将基于数学原理解释推荐算法，并寻找可行的改进方法。概率算法在推荐系统中具有重要意义，我们将描述它们如何帮助提高算法的准确性和速度。本文还将详细阐述两种不同数学距离描述相似度的优缺点。

    In this article, we will research the Recommender System's implementation about how it works and the algorithms used. We will explain the Recommender System's algorithms based on mathematical principles, and find feasible methods for improvements. The algorithms based on probability have its significance in Recommender System, we will describe how they help to increase the accuracy and speed of the algorithms. Both the weakness and the strength of two different mathematical distance used to describe the similarity will be detailed illustrated in this article.
    
[^7]: 从全手写页面中提取键值信息

    Key-value information extraction from full handwritten pages. (arXiv:2304.13530v1 [cs.CV])

    [http://arxiv.org/abs/2304.13530](http://arxiv.org/abs/2304.13530)

    这篇论文提出了一种从手写文档中提取信息的方法，该方法基于Transformer，在同一模型中结合了特征提取、手写识别和命名实体识别步骤。该方法不需要预先分割并在三个公共数据库上表现出色。

    

    我们提出了一种基于Transformer的方法，用于从数字化的手写文档中提取信息。我们的方法结合了目前由独立模型执行的不同步骤：特征提取、手写识别和命名实体识别。我们将这种综合方法与传统的两阶段方法进行比较，传统方法在命名实体识别之前进行手写识别，并在不同层次上呈现结果：行、段落和页面。我们的实验表明，在应用于整个页面时，基于注意力的模型特别有趣，因为它们不需要任何预先分割步骤。最后，我们展示了它们能够从键值注释中进行学习：即重要单词和相应命名实体的列表。我们将我们的模型与三个公共数据库（IAM、ESPOSALLES和POPP）的最新方法进行比较，并在所有三个数据集上优于以前的表现。

    We propose a Transformer-based approach for information extraction from digitized handwritten documents. Our approach combines, in a single model, the different steps that were so far performed by separate models: feature extraction, handwriting recognition and named entity recognition. We compare this integrated approach with traditional two-stage methods that perform handwriting recognition before named entity recognition, and present results at different levels: line, paragraph, and page. Our experiments show that attention-based models are especially interesting when applied on full pages, as they do not require any prior segmentation step. Finally, we show that they are able to learn from key-value annotations: a list of important words with their corresponding named entities. We compare our models to state-of-the-art methods on three public databases (IAM, ESPOSALLES, and POPP) and outperform previous performances on all three datasets.
    
[^8]: 研究非受控变量对信息处理活动用户研究中生理信号的影响

    Examining the Impact of Uncontrolled Variables on Physiological Signals in User Studies for Information Processing Activities. (arXiv:2304.13488v1 [cs.IR])

    [http://arxiv.org/abs/2304.13488](http://arxiv.org/abs/2304.13488)

    研究了信息处理活动用户研究中非受控变量对生理信号的影响，发现其中任务持续时间是个体差异的重要指标。

    

    生理信号有潜力作为客观测量指标，用于理解用户与信息访问系统的行为和参与度。然而，这些信号非常敏感，在实验室用户研究中需要许多控制。为了研究任务顺序或持续时间等受控或非受控（即混淆）变量对观察到的信号影响程度，我们进行了一项试点研究，其中每个参与者完成了四种信息处理活动（READ，LISTEN，SPEAK和WRITE），同时我们收集了血容量脉冲、皮肤电活动和瞳孔反应的数据。然后，我们使用机器学习方法来检查用户研究中常见的受控和非受控变量的影响。发现任务持续时间对模型性能有重大影响，表明它代表的是个体差异，而不是提供有关目标变量的洞察。

    Physiological signals can potentially be applied as objective measures to understand the behavior and engagement of users interacting with information access systems. However, the signals are highly sensitive, and many controls are required in laboratory user studies. To investigate the extent to which controlled or uncontrolled (i.e., confounding) variables such as task sequence or duration influence the observed signals, we conducted a pilot study where each participant completed four types of information-processing activities (READ, LISTEN, SPEAK, and WRITE). Meanwhile, we collected data on blood volume pulse, electrodermal activity, and pupil responses. We then used machine learning approaches as a mechanism to examine the influence of controlled and uncontrolled variables that commonly arise in user studies. Task duration was found to have a substantial effect on the model performance, suggesting it represents individual differences rather than giving insight into the target varia
    
[^9]: STIR：用于图像检索后处理的Siamese Transformer（arXiv：2304.13393v1 [cs.IR]）

    STIR: Siamese Transformer for Image Retrieval Postprocessing. (arXiv:2304.13393v1 [cs.IR])

    [http://arxiv.org/abs/2304.13393](http://arxiv.org/abs/2304.13393)

    这项工作提出了两部分内容。首先，他们构建了一个基于三元组损失的简单模型，性能达到了最先进水平，但没有复杂模型的缩放问题。其次，他们提出了一种新颖的后处理方法STIR，可在单个前向传递中重新排列多个顶部输出，而不依赖于全局/局部特征提取。

    

    当前，图像检索的度量学习方法通常基于学习具有信息的潜在表示空间，其中简单的方法如余弦距离将表现良好。最近的最先进方法（如HypViT）转向更复杂的嵌入空间，可能会产生更好的结果，但更难以扩展到生产环境中。在这项工作中，我们首先构建了一个基于三元组损失的简单模型，具有硬负例挖掘，性能达到了最先进水平，但没有这些缺点。其次，我们引入了一种新颖的图像检索后处理方法，称为用于图像检索的Siamese Transformer（STIR），可在单个前向传递中重新排列多个顶部输出。与先前提出的重排变压器不同，STIR不依赖于全局/局部特征提取，而是借助注意机制直接在像素级别比较查询图像和检索到的候选图像。由此得出的方法定义了一个新的最先进水平。

    Current metric learning approaches for image retrieval are usually based on learning a space of informative latent representations where simple approaches such as the cosine distance will work well. Recent state of the art methods such as HypViT move to more complex embedding spaces that may yield better results but are harder to scale to production environments. In this work, we first construct a simpler model based on triplet loss with hard negatives mining that performs at the state of the art level but does not have these drawbacks. Second, we introduce a novel approach for image retrieval postprocessing called Siamese Transformer for Image Retrieval (STIR) that reranks several top outputs in a single forward pass. Unlike previously proposed Reranking Transformers, STIR does not rely on global/local feature extraction and directly compares a query image and a retrieved candidate on pixel level with the usage of attention mechanism. The resulting approach defines a new state of the 
    
[^10]: 深度生命周期跨模态哈希

    Deep Lifelong Cross-modal Hashing. (arXiv:2304.13357v1 [cs.CV])

    [http://arxiv.org/abs/2304.13357](http://arxiv.org/abs/2304.13357)

    本文提出了一种深度学习的终身学习跨模态哈希方法，可以在不重复训练哈希函数的情况下实现生命周期哈希检索，而且通过直接训练增量数据来更新哈希函数，避免了灾难性遗忘和非连续哈希检索更新的耗时。

    

    哈希方法在交叉模态检索任务中取得了重大进展，具有快速的查询速度和低存储成本。其中，基于深度学习的哈希由于其出色的非线性异构特征提取和表示能力，在大规模数据上实现了更好的性能。然而，在数据不断到来时，灾难性遗忘和非连续哈希检索更新的耗时仍然是两个主要挑战。为此，我们在本文中提出了一种新颖的深度生命周期跨模态哈希方法，以实现生命周期哈希检索而不是重复训练哈希函数。具体而言，我们设计了终身学习策略，通过直接训练增量数据来更新哈希函数，而不是使用累计数据重新训练新的哈希函数，这显著减少了训练时间。然后，我们提出了生命周期哈希损失函数，以使原始哈希码可以适应新数据。

    Hashing methods have made significant progress in cross-modal retrieval tasks with fast query speed and low storage cost. Among them, deep learning-based hashing achieves better performance on large-scale data due to its excellent extraction and representation ability for nonlinear heterogeneous features. However, there are still two main challenges in catastrophic forgetting when data with new categories arrive continuously, and time-consuming for non-continuous hashing retrieval to retrain for updating. To this end, we, in this paper, propose a novel deep lifelong cross-modal hashing to achieve lifelong hashing retrieval instead of re-training hash function repeatedly when new data arrive. Specifically, we design lifelong learning strategy to update hash functions by directly training the incremental data instead of retraining new hash functions using all the accumulated data, which significantly reduce training time. Then, we propose lifelong hashing loss to enable original hash cod
    
[^11]: 利用视图集成提高会话段落重新排序的效果

    Improving Conversational Passage Re-ranking with View Ensemble. (arXiv:2304.13290v1 [cs.IR])

    [http://arxiv.org/abs/2304.13290](http://arxiv.org/abs/2304.13290)

    本文提出了ConvRerank，一种采用视图集成的会话段落重新排序方法，提高了会话搜索的效率和效果。

    

    本文介绍了ConvRerank，一种采用新开发的伪标签方法的会话段落重新排序器。我们提出的视图集成方法增强了伪标记数据的质量，从而提高了ConvRerank的微调效果。我们在基准数据集上的实验评估表明，将ConvRerank与会话密集检索器级联使用可以在效率和效果之间取得良好的平衡。与基线方法相比，我们的级联流水线表现出更低的延迟和更高的排名效果。此外，深入分析证实了我们的方法提高会话搜索效果的潜力。

    This paper presents ConvRerank, a conversational passage re-ranker that employs a newly developed pseudo-labeling approach. Our proposed view-ensemble method enhances the quality of pseudo-labeled data, thus improving the fine-tuning of ConvRerank. Our experimental evaluation on benchmark datasets shows that combining ConvRerank with a conversational dense retriever in a cascaded manner achieves a good balance between effectiveness and efficiency. Compared to baseline methods, our cascaded pipeline demonstrates lower latency and higher top-ranking effectiveness. Furthermore, the in-depth analysis confirms the potential of our approach to improving the effectiveness of conversational search.
    
[^12]: 自监督的多模态顺序推荐

    Self-Supervised Multi-Modal Sequential Recommendation. (arXiv:2304.13277v1 [cs.IR])

    [http://arxiv.org/abs/2304.13277](http://arxiv.org/abs/2304.13277)

    本文提出了一种自监督的多模态顺序推荐方法，采用双塔检索架构和模态特征，能够有效处理项目冷启动和域转移问题，并在多个基准数据集上取得了最优结果。

    

    随着电子商务和在线服务的不断发展，个性化推荐系统已成为提高用户满意度和推动业务收入的关键。传统的基于显式物品ID的顺序推荐方法在处理项目冷启动和域转移问题方面面临挑战。最近的方法尝试使用与物品关联的模态特征替代物品ID，使学习的知识能够在不同数据集之间传递。然而，这些方法通常计算模型输出与物品嵌入之间的相关性，可能受到高级特征向量和低级特征嵌入之间不一致的影响，从而阻碍进一步的模型学习。为了解决这个问题，我们提出了一种用于顺序推荐的双塔检索架构。在这个架构中，从用户编码器预测的嵌入被用于检索从物品编码器生成的嵌入，可以与额外的模态嵌入相结合，实现推荐模型的自监督学习。所提出的方法在几个基准数据集上取得了最先进的性能，证明它在处理项目冷启动和域转移问题方面是有效的。

    With the increasing development of e-commerce and online services, personalized recommendation systems have become crucial for enhancing user satisfaction and driving business revenue. Traditional sequential recommendation methods that rely on explicit item IDs encounter challenges in handling item cold start and domain transfer problems. Recent approaches have attempted to use modal features associated with items as a replacement for item IDs, enabling the transfer of learned knowledge across different datasets. However, these methods typically calculate the correlation between the model's output and item embeddings, which may suffer from inconsistencies between high-level feature vectors and low-level feature embeddings, thereby hindering further model learning. To address this issue, we propose a dual-tower retrieval architecture for sequence recommendation. In this architecture, the predicted embedding from the user encoder is used to retrieve the generated embedding from the item 
    
[^13]: 使用大型语言模型的生成式相关反馈

    Generative Relevance Feedback with Large Language Models. (arXiv:2304.13157v1 [cs.IR])

    [http://arxiv.org/abs/2304.13157](http://arxiv.org/abs/2304.13157)

    本文提出了一种基于大型语言模型的生成式相关反馈方法（GRF），不同于以往的伪相关反馈方法，它从长形文本中构建概率反馈模型，综合实验结果显示，在各种文献检索基准测试中，GRF方法具有显着的优势。

    

    当前的查询扩展模型使用伪相关反馈来提高第一遍检索的有效性, 但是当初始结果不相关时则会失败。我们提出了生成式相关反馈（GRF），该模型从大型语言模型生成的长形文本中构建概率反馈模型。我们通过改变零样本生成子任务-查询，实体，事实，新闻文章，文档和文章-来研究生成文本的有效方法。我们在涵盖各种查询和文档集合的文档检索基准测试中评估了GRF，并且结果表明，GRF方法比先前的PRF方法显著提高了效果。特别地，相比RM3扩展，我们提高了5-19%的MAP和17-24%的NDCG@10，并在所有数据集上实现了最佳R @ 1k效果，相比于现有的稀疏、密集和扩展模型。

    Current query expansion models use pseudo-relevance feedback to improve first-pass retrieval effectiveness; however, this fails when the initial results are not relevant. Instead of building a language model from retrieved results, we propose Generative Relevance Feedback (GRF) that builds probabilistic feedback models from long-form text generated from Large Language Models. We study the effective methods for generating text by varying the zero-shot generation subtasks: queries, entities, facts, news articles, documents, and essays. We evaluate GRF on document retrieval benchmarks covering a diverse set of queries and document collections, and the results show that GRF methods significantly outperform previous PRF methods. Specifically, we improve MAP between 5-19% and NDCG@10 17-24% compared to RM3 expansion, and achieve the best R@1k effectiveness on all datasets compared to state-of-the-art sparse, dense, and expansion models.
    
[^14]: 虚拟助手中口语信息查询的建模：未解决的问题，挑战和机遇

    Modeling Spoken Information Queries for Virtual Assistants: Open Problems, Challenges and Opportunities. (arXiv:2304.13149v1 [cs.IR])

    [http://arxiv.org/abs/2304.13149](http://arxiv.org/abs/2304.13149)

    讨论了虚拟助手中口语信息查询建模的问题和挑战，以及信息检索方法中的机遇；探讨了如何通过查询领域分类、知识图谱等提高语音识别的准确性；简要概述了语音识别中的挑战。

    

    虚拟助手正在成为越来越重要的基于语音的信息检索平台，它们可以帮助用户完成各种任务。本论文讨论了关于虚拟助手口语信息查询建模的未解决的问题和挑战，并列出了信息检索方法和研究可以应用于提高虚拟助手语音识别质量的机会。我们讨论了如何通过查询领域分类、知识图谱和用户交互数据以及查询个性化来帮助改善口语信息领域查询的准确识别。最后，我们还简要概述了语音识别中当前存在的问题和挑战。

    Virtual assistants are becoming increasingly important speech-driven Information Retrieval platforms that assist users with various tasks.  We discuss open problems and challenges with respect to modeling spoken information queries for virtual assistants, and list opportunities where Information Retrieval methods and research can be applied to improve the quality of virtual assistant speech recognition.  We discuss how query domain classification, knowledge graphs and user interaction data, and query personalization can be helpful to improve the accurate recognition of spoken information domain queries. Finally, we also provide a brief overview of current problems and challenges in speech recognition.
    
[^15]: MBIB--首个媒体偏见识别基准测试任务和数据集集合的介绍

    Introducing MBIB -- the first Media Bias Identification Benchmark Task and Dataset Collection. (arXiv:2304.13148v1 [cs.IR])

    [http://arxiv.org/abs/2304.13148](http://arxiv.org/abs/2304.13148)

    这篇论文介绍了MBIB，一个将不同类型媒体偏见分为共同框架的全面基准测试，并提供了相关数据集以评估媒体偏见检测技术，结果显示没有单一技术可以显著优于其他技术，同时发现研究兴趣和资源分配不均匀分布。

    

    尽管媒体偏见检测是一个复杂的多任务问题，但目前还没有一个统一的基准来分组这些评估任务。我们引入了媒体偏见识别基准测试（MBIB），这是一个全面的基准测试，将不同类型的媒体偏见（例如，语言、认知、政治）分为一个共同的框架，以测试预测检测技术的概括化程度。在评估了115个数据集后，我们选择了9个任务，仔细提出了22个相关数据集，以评估媒体偏见检测技术。我们使用最先进的Transformer技术（例如T5、BART）评估MBIB。我们的结果表明，尽管仇恨言论、种族偏见和性别偏见更容易检测，但模型难以处理某些偏见类型，例如，认知和政治偏见。但是，我们的结果表明，没有单一技术可以显着优于其他技术。我们还发现研究兴趣和资源分配在媒体偏见识别的个别任务上存在不均匀分布。

    Although media bias detection is a complex multi-task problem, there is, to date, no unified benchmark grouping these evaluation tasks. We introduce the Media Bias Identification Benchmark (MBIB), a comprehensive benchmark that groups different types of media bias (e.g., linguistic, cognitive, political) under a common framework to test how prospective detection techniques generalize. After reviewing 115 datasets, we select nine tasks and carefully propose 22 associated datasets for evaluating media bias detection techniques. We evaluate MBIB using state-of-the-art Transformer techniques (e.g., T5, BART). Our results suggest that while hate speech, racial bias, and gender bias are easier to detect, models struggle to handle certain bias types, e.g., cognitive and political bias. However, our results show that no single technique can outperform all the others significantly. We also find an uneven distribution of research interest and resource allocation to the individual tasks in media 
    
[^16]: 查询重构中的性别专业化模式

    Patterns of gender-specializing query reformulation. (arXiv:2304.13129v1 [cs.IR])

    [http://arxiv.org/abs/2304.13129](http://arxiv.org/abs/2304.13129)

    本文研究了一类查询重构，即在查询中添加人口统计属性，以探索用户和搜索系统之间的关系。

    

    搜索系统的用户经常通过添加查询术语来反映他们日益发展的信息需求，或者当系统无法提供相关内容时更准确地表达他们的信息需求。分析这些查询重构可以为我们提供有关系统和用户行为的信息。在这项工作中，我们研究了一个特殊类别的查询重构，这种查询重构涉及指定人口统计属性，比如性别，作为重构查询的一部分（例如，“奥运会2021年足球比赛结果”变成“奥运会2021年女子足球比赛结果”）。查询、搜索结果和人口统计属性（如性别）之间可能存在许多关系，这导致我们假设不同的原因，如原始结果页面上的欠代表性或标记理论基础。本文报告了一项性别专业化查询重构的观察性研究——它们的上下文和影响——作为探索用户和搜索系统之间关系的一种方法。

    Users of search systems often reformulate their queries by adding query terms to reflect their evolving information need or to more precisely express their information need when the system fails to surface relevant content. Analyzing these query reformulations can inform us about both system and user behavior. In this work, we study a special category of query reformulations that involve specifying demographic group attributes, such as gender, as part of the reformulated query (e.g., "olympic 2021 soccer results" to "olympic 2021 women's soccer results"). There are many ways a query, the search results, and a demographic attribute such as gender may relate, leading us to hypothesize different causes for these reformulation patterns, such as under-representation on the original result page or based on the linguistic theory of markedness. This paper reports on an observational study of gender-specializing query reformulations -- their contexts and effects -- as a lens on the relationship
    
[^17]: OFAR: 一种用于非法直播识别的多模式证据检索框架

    OFAR: A Multimodal Evidence Retrieval Framework for Illegal Live-streaming Identification. (arXiv:2304.12608v1 [cs.IR])

    [http://arxiv.org/abs/2304.12608](http://arxiv.org/abs/2304.12608)

    OFAR是一种用于非法直播识别的证据检索框架，能够帮助直播平台立即识别直播中的非法行为。

    

    非法直播识别是为了帮助直播平台立即识别直播中的非法行为，例如售卖珍贵和濒危动物，对净化网络环境起着至关重要的作用。本文提出了一种名为OFAR的多模式证据检索框架，以利于非法直播识别。OFAR包括三个模块：查询编码器、文档编码器和基于MaxSim的对比晚交集。查询编码器和文档编码器都采用了先进的OFA。

    Illegal live-streaming identification, which aims to help live-streaming platforms immediately recognize the illegal behaviors in the live-streaming, such as selling precious and endangered animals, plays a crucial role in purifying the network environment. Traditionally, the live-streaming platform needs to employ some professionals to manually identify the potential illegal live-streaming. Specifically, the professional needs to search for related evidence from a large-scale knowledge database for evaluating whether a given live-streaming clip contains illegal behavior, which is time-consuming and laborious. To address this issue, in this work, we propose a multimodal evidence retrieval system, named OFAR, to facilitate the illegal live-streaming identification. OFAR consists of three modules: \textit{Query Encoder}, \textit{Document Encoder}, and \textit{MaxSim-based Contrastive Late Intersection}. Both query encoder and document encoder are implemented with the advanced \mbox{OFA} 
    
[^18]: 序列推荐中的元优化对比学习

    Meta-optimized Contrastive Learning for Sequential Recommendation. (arXiv:2304.07763v1 [cs.IR])

    [http://arxiv.org/abs/2304.07763](http://arxiv.org/abs/2304.07763)

    本文提出了 MCLRec 模型，该模型在数据增强和可学习模型增强操作的基础上，解决了现有对比学习方法难以推广和训练数据不足的问题。

    

    对比学习方法是解决稀疏且含噪声推荐数据的一个新兴方法。然而，现有的对比学习方法要么只针对手工制作的数据进行训练数据和模型增强，要么只使用模型增强方法，这使得模型很难推广。为了更好地训练模型，本文提出了一种称为元优化对比学习的模型。该模型结合了数据增强和可学习模型增强操作。

    Contrastive Learning (CL) performances as a rising approach to address the challenge of sparse and noisy recommendation data. Although having achieved promising results, most existing CL methods only perform either hand-crafted data or model augmentation for generating contrastive pairs to find a proper augmentation operation for different datasets, which makes the model hard to generalize. Additionally, since insufficient input data may lead the encoder to learn collapsed embeddings, these CL methods expect a relatively large number of training data (e.g., large batch size or memory bank) to contrast. However, not all contrastive pairs are always informative and discriminative enough for the training processing. Therefore, a more general CL-based recommendation model called Meta-optimized Contrastive Learning for sequential Recommendation (MCLRec) is proposed in this work. By applying both data augmentation and learnable model augmentation operations, this work innovates the standard 
    
[^19]: RecD：为端到端深度学习推荐模型训练基础设施提供去重功能

    RecD: Deduplication for End-to-End Deep Learning Recommendation Model Training Infrastructure. (arXiv:2211.05239v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.05239](http://arxiv.org/abs/2211.05239)

    RecD 是一种为 DLRM 训练提供去重功能的端到端基础设施优化，解决了由于特征重复造成的海量存储、预处理和训练开销，引入了新的张量格式 InverseKeyedJaggedTensors (IKJTs) 来去除特征值的重复，使 DLRM 模型架构能够更好地利用数据的重复性提高训练吞吐量。

    

    我们提出了 RecD（推荐去重），它是一组针对深度学习推荐模型 (DLRM) 训练流程的端到端基础设施优化。RecD解决了由于特征重复造成的海量存储、预处理和训练开销，这是大规模 DLRM 训练数据集内在的问题，因为 DLRM 数据集是从交互中生成的。我们展示了 RecD 如何利用此属性来优化生产数据的流程，减少数据集存储和预处理需求，并最大限度地在训练批次中重复。RecD 引入了一种新的张量格式 InverseKeyedJaggedTensors (IKJTs)，以在每个批次中去除特征值的重复。我们展示了 DLRM 模型架构如何利用 IKJTs 来显著提高训练吞吐量。

    We present RecD (Recommendation Deduplication), a suite of end-to-end infrastructure optimizations across the Deep Learning Recommendation Model (DLRM) training pipeline. RecD addresses immense storage, preprocessing, and training overheads caused by feature duplication inherent in industry-scale DLRM training datasets. Feature duplication arises because DLRM datasets are generated from interactions. While each user session can generate multiple training samples, many features' values do not change across these samples. We demonstrate how RecD exploits this property, end-to-end, across a deployed training pipeline. RecD optimizes data generation pipelines to decrease dataset storage and preprocessing resource demands and to maximize duplication within a training batch. RecD introduces a new tensor format, InverseKeyedJaggedTensors (IKJTs), to deduplicate feature values in each batch. We show how DLRM model architectures can leverage IKJTs to drastically increase training throughput. Re
    
[^20]: 增强法律案件匹配的法律条款：因果学习方法

    Law Article-Enhanced Legal Case Matching: a Causal Learning Approach. (arXiv:2210.11012v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2210.11012](http://arxiv.org/abs/2210.11012)

    本文提出了一种因果学习方法，将法律条款考虑在内以改善法律案件匹配的结果，实验证明该方法优于现有基准方法。

    

    法律案例匹配在智能法律系统中发挥着重要作用，然而目前的语义文本匹配模型只考虑案件文本的相似度，忽略了法律条款在匹配结果中的作用。本文提出了一种法律条款的因果学习方法来改善匹配结果，实验结果表明我们的方法优于现有的基准方法。

    Legal case matching, which automatically constructs a model to estimate the similarities between the source and target cases, has played an essential role in intelligent legal systems. Semantic text matching models have been applied to the task where the source and target legal cases are considered as long-form text documents. These general-purpose matching models make the predictions solely based on the texts in the legal cases, overlooking the essential role of the law articles in legal case matching. In the real world, the matching results (e.g., relevance labels) are dramatically affected by the law articles because the contents and the judgments of a legal case are radically formed on the basis of law. From the causal sense, a matching decision is affected by the mediation effect from the cited law articles by the legal cases, and the direct effect of the key circumstances (e.g., detailed fact descriptions) in the legal cases. In light of the observation, this paper proposes a mod
    
[^21]: DECONET：一种基于分析稀疏性压缩感知的展开网络及其泛化误差界

    DECONET: an Unfolding Network for Analysis-based Compressed Sensing with Generalization Error Bounds. (arXiv:2205.07050v6 [cs.IT] UPDATED)

    [http://arxiv.org/abs/2205.07050](http://arxiv.org/abs/2205.07050)

    本文提出了一种名为DECONET的新型深度展开网络，可以用于基于分析稀疏性的压缩感知，能有效地重构向量并优于现有的展开网络，在估计了其泛化误差的基础上得出相关结论。

    

    我们提出了一种新的深度展开网络DECONET，用于基于分析稀疏性的压缩感知。DECONET联合学习一个解码器，用于从不完整、噪声测量中重构向量，以及一个冗余的稀疏分析算子，该算子在DECONET的各个层之间共享。此外，我们构建了DECONET的假设类并估计其相关的Rademacher复杂度。然后，我们利用这个估计结果为DECONET提供有意义的上限，用于评估DECONET的泛化误差。最后，在合成和真实数据集上评估了我们的理论结果的有效性，并与现有的展开网络进行比较。实验结果表明，我们提出的网络在所有数据集上始终优于基线，并且其行为符合我们的理论发现。

    We present a new deep unfolding network for analysis-sparsity-based Compressed Sensing. The proposed network coined Decoding Network (DECONET) jointly learns a decoder that reconstructs vectors from their incomplete, noisy measurements and a redundant sparsifying analysis operator, which is shared across the layers of DECONET. Moreover, we formulate the hypothesis class of DECONET and estimate its associated Rademacher complexity. Then, we use this estimate to deliver meaningful upper bounds for the generalization error of DECONET. Finally, the validity of our theoretical results is assessed and comparisons to state-of-the-art unfolding networks are made, on both synthetic and real-world datasets. Experimental results indicate that our proposed network outperforms the baselines, consistently for all datasets, and its behaviour complies with our theoretical findings.
    

