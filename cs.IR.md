# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Building Contextual Knowledge Graphs for Personalized Learning Recommendations using Text Mining and Semantic Graph Completion.](http://arxiv.org/abs/2401.13609) | 本研究介绍了使用文本挖掘将分层数据模型转化为知识图模型的方法，并通过评估图质量指标和语义相似性来验证其有效性。结果显示，这种方法可以提供个性化学习路径推荐的基础。 |
| [^2] | [A Cost-Sensitive Meta-Learning Strategy for Fair Provider Exposure in Recommendation.](http://arxiv.org/abs/2401.13566) | 本文提出了一种新颖的成本敏感方法，旨在确保推荐系统中提供者的公平曝光度。该方法量化和减少了不同提供者群体在推荐结果中的差异，以实现公平原则。 |
| [^3] | [Fine-grained Contract NER using instruction based model.](http://arxiv.org/abs/2401.13545) | 该论文介绍了一种使用基于指令的模型进行细粒度的合同命名实体识别的方法。通过将NER任务转化为文本生成任务，该方法能够提高大规模语言模型在NER任务中的性能。 |
| [^4] | [TPRF: A Transformer-based Pseudo-Relevance Feedback Model for Efficient and Effective Retrieval.](http://arxiv.org/abs/2401.13509) | 本文提出一种基于Transformer的伪相关反馈模型（TPRF），适用于资源受限的环境。TPRF相比其他深度语言模型在内存占用和推理时间方面具备更小的开销，并能有效地结合来自稠密文具表示的相关反馈信号。 |
| [^5] | [SciMMIR: Benchmarking Scientific Multi-modal Information Retrieval.](http://arxiv.org/abs/2401.13478) | SciMMIR是一个专门用于科学领域的多模态信息检索基准，通过开放获取的论文集合提取与科学领域相关的图像-文本配对，从而弥补了现有基准在此领域中的差距。 |
| [^6] | [SpeechDPR: End-to-End Spoken Passage Retrieval for Open-Domain Spoken Question Answering.](http://arxiv.org/abs/2401.13463) | SpeechDPR是第一个用于开放领域口语问答的端到端框架，能够从口语存档中检索可能包含答案的段落。通过融合无监督ASR和文本密集检索器的知识，SpeechDPR能够获得较好的性能，并且在UASR性能较差时表现更加鲁棒。 |
| [^7] | [Decentralized Collaborative Learning with Adaptive Reference Data for On-Device POI Recommendation.](http://arxiv.org/abs/2401.13448) | 这项研究提出了一种使用自适应引用数据的去中心化协作学习方法，用于设备上的兴趣点推荐，以解决使用同一引用数据对不同用户产生负面影响的问题。 |
| [^8] | [Query Exposure Prediction for Groups of Documents in Rankings.](http://arxiv.org/abs/2401.13434) | 本论文研究了查询排名中文档组的查询曝光预测问题，指出了重新排名流程的成功与第一阶段检索的性能密切相关，并讨论了特定组文档在最终排名中接受曝光量的影响。 |
| [^9] | [How to Forget Clients in Federated Online Learning to Rank?.](http://arxiv.org/abs/2401.13410) | 本文研究了如何在联邦在线学习排序中删除客户的贡献，提出了一种有效和高效的取消学习方法。 |
| [^10] | [Predicting IR Personalization Performance using Pre-retrieval Query Predictors.](http://arxiv.org/abs/2401.13351) | 在这篇论文中，我们使用预检索查询预测器来预测IR个性化性能。研究发现，禁用个性化对于提高性能和用户满意度是有益的。我们提出了一些新的预测器并使用分类和回归技术改进了结果，最终达到了略高于最理想性能的三分之一。 |
| [^11] | [A Big Data Architecture for Early Identification and Categorization of Dark Web Sites.](http://arxiv.org/abs/2401.13320) | 本文提出了一个用于早期识别和分类暗网站的大数据架构。该架构利用开源技术实现了从多个来源发现洋葱地址、下载HTML并进行内容去重和分类的功能，并成功应对了Tor的不稳定性挑战。在93天内，系统识别了80,049个洋葱服务，并对90%进行了表征。 |
| [^12] | [It's About Time: Incorporating Temporality in Retrieval Augmented Language Models.](http://arxiv.org/abs/2401.13222) | 在大型语言模型中引入时间性是信息检索的关键挑战，目前的检索增强语言模型无法很好地处理时间查询。 |
| [^13] | [Into the crossfire: evaluating the use of a language model to crowdsource gun violence reports.](http://arxiv.org/abs/2401.12989) | 本研究评估了使用语言模型从社交媒体数据中监测枪支暴力事件的可行性。研究团队使用经过微调的BERT模型识别巴西的枪支暴力报告并取得了高准确度。研究结果有助于人权组织收集包含所需数据的全面数据库。 |
| [^14] | [Next Visit Diagnosis Prediction via Medical Code-Centric Multimodal Contrastive EHR Modelling with Hierarchical Regularisation.](http://arxiv.org/abs/2401.11648) | 通过医学代码中心的多模态对比EHR建模预测下次就诊诊断，并通过分层正则化提高性能。 |
| [^15] | [Using LLMs to discover emerging coded antisemitic hate-speech emergence in extremist social media.](http://arxiv.org/abs/2401.10841) | 这项研究提出了一种方法，可以检测新出现的编码恶意术语，为极端社交媒体中的反犹太恶意言论提供了解决方案。 |
| [^16] | [End-to-end Learnable Clustering for Intent Learning in Recommendation.](http://arxiv.org/abs/2401.05975) | 本文提出了一种用于推荐中意图学习的端到端可学习聚类方法ELCRec，该方法解决了现有方法中的复杂优化问题和大规模数据集聚类的可扩展性问题。 |
| [^17] | [Unlock Multi-Modal Capability of Dense Retrieval via Visual Module Plugin.](http://arxiv.org/abs/2310.14037) | 本文介绍了一种名为MARVEL的多模态检索模型，通过视觉模块插件为密集检索器添加图像理解能力，并且在多模态检索任务中取得了显著优于最先进方法的结果。 |
| [^18] | [Pure Message Passing Can Estimate Common Neighbor for Link Prediction.](http://arxiv.org/abs/2309.00976) | 这篇论文提出了一种纯粹的消息传递方法，用于估计共同邻居进行链路预测。该方法通过利用输入向量的正交性来捕捉联合结构特征，提出了一种新的链路预测模型MPLP，该模型利用准正交向量估计链路级结构特征，同时保留了节点级复杂性。 |
| [^19] | [Large Language Models are Zero-Shot Rankers for Recommender Systems.](http://arxiv.org/abs/2305.08845) | 大型语言模型表现出有希望的零-shot排名能力，但在感知历史互动顺序和受到偏见影响方面存在问题。本研究通过特殊设计的提示和引导策略来缓解这些问题。 |
| [^20] | [Towards Better Understanding of User Satisfaction in Open-Domain Conversational Search.](http://arxiv.org/abs/2204.02659) | 在对话式搜索中，如何准确地建模用户满意度是一个重要问题。现有方法要么忽视用户信息需求，要么忽视混合主动属性。传统的IR研究使用Cranfield范式和用户行为建模来估计用户满意度，但实践中具有挑战性。 |

# 详细

[^1]: 使用文本挖掘和语义图完成构建个性化学习推荐的上下文知识图谱

    Building Contextual Knowledge Graphs for Personalized Learning Recommendations using Text Mining and Semantic Graph Completion. (arXiv:2401.13609v1 [cs.IR])

    [http://arxiv.org/abs/2401.13609](http://arxiv.org/abs/2401.13609)

    本研究介绍了使用文本挖掘将分层数据模型转化为知识图模型的方法，并通过评估图质量指标和语义相似性来验证其有效性。结果显示，这种方法可以提供个性化学习路径推荐的基础。

    

    模拟学习对象（LO）在其上下文中可以使学习者从基础的记忆级学习目标发展到更高级别的应用和分析目标。尽管数字学习平台通常使用分层数据模型，但使用基于图的模型可以在这些平台上表示LO的上下文。这为个性化学习路径的推荐提供了基础。本文介绍和评估了使用文本挖掘将分层数据模型转换为LO的知识图（KG）模型的方法。我们利用自定义的文本挖掘流程来挖掘专家策划的分层模型元素之间的语义关系。我们使用图质量控制指标和算法语义相似性与专家定义的语义相似性的比较来评估KG结构和关系提取。结果表明，KG中的关系在语义上与专家定义的关系是可比较的。

    Modelling learning objects (LO) within their context enables the learner to advance from a basic, remembering-level, learning objective to a higher-order one, i.e., a level with an application- and analysis objective. While hierarchical data models are commonly used in digital learning platforms, using graph-based models enables representing the context of LOs in those platforms. This leads to a foundation for personalized recommendations of learning paths. In this paper, the transformation of hierarchical data models into knowledge graph (KG) models of LOs using text mining is introduced and evaluated. We utilize custom text mining pipelines to mine semantic relations between elements of an expert-curated hierarchical model. We evaluate the KG structure and relation extraction using graph quality-control metrics and the comparison of algorithmic semantic-similarities to expert-defined ones. The results show that the relations in the KG are semantically comparable to those defined by d
    
[^2]: 一种关注公平性的成本敏感元学习策略用于推荐系统中的提供者曝光度

    A Cost-Sensitive Meta-Learning Strategy for Fair Provider Exposure in Recommendation. (arXiv:2401.13566v1 [cs.IR])

    [http://arxiv.org/abs/2401.13566](http://arxiv.org/abs/2401.13566)

    本文提出了一种新颖的成本敏感方法，旨在确保推荐系统中提供者的公平曝光度。该方法量化和减少了不同提供者群体在推荐结果中的差异，以实现公平原则。

    

    在设计推荐服务时，考虑到所有内容提供者的利益非常重要，这些提供者包括新进入者和少数民族群体。在某些情况下，特定的提供者群体在物品目录中的代表性较低，这可能会影响推荐结果。因此，平台拥有者通常希望调节这些提供者群体在推荐列表中的曝光度。在本文中，我们提出了一种新颖的成本敏感方法，旨在在成对推荐模型中确保这些目标曝光水平。这种方法量化并减少了分配给群体的推荐量与物品目录中的贡献之间的差异，遵循公平原则。我们的结果表明，这种方法在确保群体曝光度与其分配水平一致的同时，不会损害原始的推荐效果。可以获取源代码和预处理数据。

    When devising recommendation services, it is important to account for the interests of all content providers, encompassing not only newcomers but also minority demographic groups. In various instances, certain provider groups find themselves underrepresented in the item catalog, a situation that can influence recommendation results. Hence, platform owners often seek to regulate the exposure of these provider groups in the recommended lists. In this paper, we propose a novel cost-sensitive approach designed to guarantee these target exposure levels in pairwise recommendation models. This approach quantifies, and consequently mitigate, the discrepancies between the volume of recommendations allocated to groups and their contribution in the item catalog, under the principle of equity. Our results show that this approach, while aligning groups exposure with their assigned levels, does not compromise to the original recommendation utility. Source code and pre-processed data can be retrieved
    
[^3]: 使用基于指令的模型进行细粒度的合同命名实体识别

    Fine-grained Contract NER using instruction based model. (arXiv:2401.13545v1 [cs.IR])

    [http://arxiv.org/abs/2401.13545](http://arxiv.org/abs/2401.13545)

    该论文介绍了一种使用基于指令的模型进行细粒度的合同命名实体识别的方法。通过将NER任务转化为文本生成任务，该方法能够提高大规模语言模型在NER任务中的性能。

    

    最近，基于指令的技术在改善少样本学习场景下的性能方面取得了显著进展。它们通过弥合预训练语言模型和针对特定下游任务的微调之间的差距来实现这一点。尽管取得了这些进展，但大规模语言模型（LLMs）在命名实体识别（NER）等信息提取任务中，使用提示或指令仍然不及受监督的基线。造成这种性能差距的原因可以归因于NER和LLMs之间的基本差异。NER本质上是一个序列标注任务，模型必须为句子中的各个标记分配实体类型标签。相比之下，LLMs被设计为一个文本生成任务。语义标注和文本生成之间的区别导致性能不佳。在本文中，我们将NER任务转化为一个可以被LLMs轻松适应的文本生成任务。这涉及增强源句子的特征。

    Lately, instruction-based techniques have made significant strides in improving performance in few-shot learning scenarios. They achieve this by bridging the gap between pre-trained language models and fine-tuning for specific downstream tasks. Despite these advancements, the performance of Large Language Models (LLMs) in information extraction tasks like Named Entity Recognition (NER), using prompts or instructions, still falls short of supervised baselines. The reason for this performance gap can be attributed to the fundamental disparity between NER and LLMs. NER is inherently a sequence labeling task, where the model must assign entity-type labels to individual tokens within a sentence. In contrast, LLMs are designed as a text generation task. This distinction between semantic labeling and text generation leads to subpar performance. In this paper, we transform the NER task into a text-generation task that can be readily adapted by LLMs. This involves enhancing source sentences wit
    
[^4]: TPRF:一种基于Transformer的伪相关反馈模型，用于高效且有效的检索。

    TPRF: A Transformer-based Pseudo-Relevance Feedback Model for Efficient and Effective Retrieval. (arXiv:2401.13509v1 [cs.IR])

    [http://arxiv.org/abs/2401.13509](http://arxiv.org/abs/2401.13509)

    本文提出一种基于Transformer的伪相关反馈模型（TPRF），适用于资源受限的环境。TPRF相比其他深度语言模型在内存占用和推理时间方面具备更小的开销，并能有效地结合来自稠密文具表示的相关反馈信号。

    

    本文考虑在资源受限的环境中，如廉价云实例或嵌入式系统（如智能手机和智能手表）中，针对稠密检索器的伪相关反馈（PRF）方法，其中内存和CPU受限，没有GPU。为此，我们提出了一种基于Transformer的PRF方法（TPRF），与采用PRF机制的其他深度语言模型相比，具有更小的内存占用和更快的推理时间，较小的效果损失。TPRF学习如何有效地结合来自稠密文具表示的相关反馈信号。具体而言，TPRF提供了一种建模查询和相关反馈信号之间关系和权重的机制。该方法对所使用的具体稠密表示不加偏见，因此可以广泛应用于任何稠密检索器。

    This paper considers Pseudo-Relevance Feedback (PRF) methods for dense retrievers in a resource constrained environment such as that of cheap cloud instances or embedded systems (e.g., smartphones and smartwatches), where memory and CPU are limited and GPUs are not present. For this, we propose a transformer-based PRF method (TPRF), which has a much smaller memory footprint and faster inference time compared to other deep language models that employ PRF mechanisms, with a marginal effectiveness loss. TPRF learns how to effectively combine the relevance feedback signals from dense passage representations. Specifically, TPRF provides a mechanism for modelling relationships and weights between the query and the relevance feedback signals. The method is agnostic to the specific dense representation used and thus can be generally applied to any dense retriever.
    
[^5]: SciMMIR:科学多模态信息检索的基准评测

    SciMMIR: Benchmarking Scientific Multi-modal Information Retrieval. (arXiv:2401.13478v1 [cs.IR])

    [http://arxiv.org/abs/2401.13478](http://arxiv.org/abs/2401.13478)

    SciMMIR是一个专门用于科学领域的多模态信息检索基准，通过开放获取的论文集合提取与科学领域相关的图像-文本配对，从而弥补了现有基准在此领域中的差距。

    

    多模态信息检索（MMIR）是一个快速发展的领域，通过先进的表示学习和跨模态对齐研究，在图像-文本配对方面取得了显著进展。然而，在科学领域内评估图像-文本配对的MMIR性能的当前基准存在明显差距，学术语言中描述的图表和表格图像通常不起重要作用。为了弥补这一差距，我们利用开放获取的论文集合构建了一个专门的科学MMIR（SciMMIR）基准，以提取与科学领域相关的数据。该基准包含了530K个精心策划的从科学文档中提取的图像-文本配对，这些图像-文本配对来自于具有详细标题的科学文档中的图表和表格。我们还使用两级子集-子类别层次注释对图像-文本配对进行了注释，以促进对基准模型的更全面评估。我们对零样本和微调进行了评估。

    Multi-modal information retrieval (MMIR) is a rapidly evolving field, where significant progress, particularly in image-text pairing, has been made through advanced representation learning and cross-modality alignment research. However, current benchmarks for evaluating MMIR performance in image-text pairing within the scientific domain show a notable gap, where chart and table images described in scholarly language usually do not play a significant role. To bridge this gap, we develop a specialised scientific MMIR (SciMMIR) benchmark by leveraging open-access paper collections to extract data relevant to the scientific domain. This benchmark comprises 530K meticulously curated image-text pairs, extracted from figures and tables with detailed captions in scientific documents. We further annotate the image-text pairs with two-level subset-subcategory hierarchy annotations to facilitate a more comprehensive evaluation of the baselines. We conducted zero-shot and fine-tuning evaluations o
    
[^6]: SpeechDPR: 开放领域口语问答的端到端口语段落检索

    SpeechDPR: End-to-End Spoken Passage Retrieval for Open-Domain Spoken Question Answering. (arXiv:2401.13463v1 [cs.CL])

    [http://arxiv.org/abs/2401.13463](http://arxiv.org/abs/2401.13463)

    SpeechDPR是第一个用于开放领域口语问答的端到端框架，能够从口语存档中检索可能包含答案的段落。通过融合无监督ASR和文本密集检索器的知识，SpeechDPR能够获得较好的性能，并且在UASR性能较差时表现更加鲁棒。

    

    口语问答(SQA)是机器通过在给定口语段落中找到答案范围来回答用户问题的关键。过去的SQA方法没有使用ASR，以避免识别错误和词汇外问题。然而，实际的开放领域SQA(openSQA)问题中，机器需要首先从口语存档中检索可能包含答案的段落。本文提出了第一个已知的用于openSQA问题检索组件的端到端框架SpeechDPR。SpeechDPR通过从无监督ASR(UASR)和文本密集检索器(TDR)的级联模型中提炼知识，学习句子级语义表示。不需要手动转录的语音数据。初步实验表明，与级联的UASR和TDR模型相比，性能相当，并且在UASR性能较差时显著提高，验证了这种方法更加鲁棒。

    Spoken Question Answering (SQA) is essential for machines to reply to user's question by finding the answer span within a given spoken passage. SQA has been previously achieved without ASR to avoid recognition errors and Out-of-Vocabulary (OOV) problems. However, the real-world problem of Open-domain SQA (openSQA), in which the machine needs to first retrieve passages that possibly contain the answer from a spoken archive in addition, was never considered. This paper proposes the first known end-to-end framework, Speech Dense Passage Retriever (SpeechDPR), for the retrieval component of the openSQA problem. SpeechDPR learns a sentence-level semantic representation by distilling knowledge from the cascading model of unsupervised ASR (UASR) and text dense retriever (TDR). No manually transcribed speech data is needed. Initial experiments showed performance comparable to the cascading model of UASR and TDR, and significantly better when UASR was poor, verifying this approach is more robus
    
[^7]: 使用自适应引用数据的去中心化协作学习进行设备上的兴趣点推荐

    Decentralized Collaborative Learning with Adaptive Reference Data for On-Device POI Recommendation. (arXiv:2401.13448v1 [cs.IR])

    [http://arxiv.org/abs/2401.13448](http://arxiv.org/abs/2401.13448)

    这项研究提出了一种使用自适应引用数据的去中心化协作学习方法，用于设备上的兴趣点推荐，以解决使用同一引用数据对不同用户产生负面影响的问题。

    

    在基于位置的社交网络中，兴趣点（POI）推荐帮助用户发现有趣的地方。为了保护隐私和减少服务器依赖，从基于云的模型转向设备上的推荐是一个趋势。由于个别设备上的本地用户-项目交互数据稀缺，仅依赖本地数据是不足够的。协作学习（CL）兴起，促进用户之间的模型共享，其中引用数据作为中介，使用户能够在不直接共享私有数据或参数的情况下交换他们的软决策，确保隐私并从协作中受益。然而，现有的基于协作学习的推荐通常为所有用户使用同一个引用数据。对一个用户有价值的引用数据可能对另一个用户有害，鉴于用户偏好的多样性。用户可能不会对他们兴趣范围之外的项目提供有意义的软决策。因此，为所有协作使用相同的引用数据可能会阻碍协作推荐的效果。

    In Location-based Social Networks, Point-of-Interest (POI) recommendation helps users discover interesting places. There is a trend to move from the cloud-based model to on-device recommendations for privacy protection and reduced server reliance. Due to the scarcity of local user-item interactions on individual devices, solely relying on local instances is not adequate. Collaborative Learning (CL) emerges to promote model sharing among users, where reference data is an intermediary that allows users to exchange their soft decisions without directly sharing their private data or parameters, ensuring privacy and benefiting from collaboration. However, existing CL-based recommendations typically use a single reference for all users. Reference data valuable for one user might be harmful to another, given diverse user preferences. Users may not offer meaningful soft decisions on items outside their interest scope. Consequently, using the same reference data for all collaborations can imped
    
[^8]: 查询排名中的文档组的查询曝光预测

    Query Exposure Prediction for Groups of Documents in Rankings. (arXiv:2401.13434v1 [cs.IR])

    [http://arxiv.org/abs/2401.13434](http://arxiv.org/abs/2401.13434)

    本论文研究了查询排名中文档组的查询曝光预测问题，指出了重新排名流程的成功与第一阶段检索的性能密切相关，并讨论了特定组文档在最终排名中接受曝光量的影响。

    

    信息检索系统的主要目标是向用户提供与用户查询最相关的文档。为了做到这一点，现代的信息检索系统通常会部署一个重新排名的流程，其中一组文档由一个轻量级的第一阶段检索过程检索出来，然后由一个更有效但代价更高的模型进行重新排名。然而，重新排名流程的成功在很大程度上取决于第一阶段检索的性能，因为在重新排名阶段通常不会识别出新的文档。此外，这可能会影响特定组文档在最终排名中接受到的曝光量。例如，如果第一阶段检索返回某些组别的文档太少，公平分配曝光变得更加具有挑战性或不可能，因为在排名中的组文档数量对曝光的影响比文档的位置更大。考虑到这一点，

    The main objective of an Information Retrieval system is to provide a user with the most relevant documents to the user's query. To do this, modern IR systems typically deploy a re-ranking pipeline in which a set of documents is retrieved by a lightweight first-stage retrieval process and then re-ranked by a more effective but expensive model. However, the success of a re-ranking pipeline is heavily dependent on the performance of the first stage retrieval, since new documents are not usually identified during the re-ranking stage. Moreover, this can impact the amount of exposure that a particular group of documents, such as documents from a particular demographic group, can receive in the final ranking. For example, the fair allocation of exposure becomes more challenging or impossible if the first stage retrieval returns too few documents from certain groups, since the number of group documents in the ranking affects the exposure more than the documents' positions. With this in mind,
    
[^9]: 如何在联邦在线学习排序中忘记客户？

    How to Forget Clients in Federated Online Learning to Rank?. (arXiv:2401.13410v1 [cs.CR])

    [http://arxiv.org/abs/2401.13410](http://arxiv.org/abs/2401.13410)

    本文研究了如何在联邦在线学习排序中删除客户的贡献，提出了一种有效和高效的取消学习方法。

    

    数据保护法规如欧盟的《通用数据保护条例》（GDPR）建立了“被遗忘的权利”：用户（客户）可以要求将使用他们的数据进行的贡献从学习的模型中删除。本文研究了如何删除参与联邦在线学习排序（FOLTR）系统中客户所做的贡献。在FOLTR系统中，通过聚合局部更新到全局排序模型来学习排序器。局部更新是以在线方式在客户级别上使用查询和隐式交互来学习的，这些查询和交互发生在特定客户内部。通过这样做，每个客户的本地数据不会与其他客户或集中式搜索服务共享，同时客户可以从联邦中的每个客户的贡献中受益。在本文中，我们研究了一种有效和高效的取消学习方法，可以删除客户的贡献。

    Data protection legislation like the European Union's General Data Protection Regulation (GDPR) establishes the \textit{right to be forgotten}: a user (client) can request contributions made using their data to be removed from learned models. In this paper, we study how to remove the contributions made by a client participating in a Federated Online Learning to Rank (FOLTR) system. In a FOLTR system, a ranker is learned by aggregating local updates to the global ranking model. Local updates are learned in an online manner at a client-level using queries and implicit interactions that have occurred within that specific client. By doing so, each client's local data is not shared with other clients or with a centralised search service, while at the same time clients can benefit from an effective global ranking model learned from contributions of each client in the federation.  In this paper, we study an effective and efficient unlearning method that can remove a client's contribution with
    
[^10]: 使用预检索查询预测器预测IR个性化性能

    Predicting IR Personalization Performance using Pre-retrieval Query Predictors. (arXiv:2401.13351v1 [cs.IR])

    [http://arxiv.org/abs/2401.13351](http://arxiv.org/abs/2401.13351)

    在这篇论文中，我们使用预检索查询预测器来预测IR个性化性能。研究发现，禁用个性化对于提高性能和用户满意度是有益的。我们提出了一些新的预测器并使用分类和回归技术改进了结果，最终达到了略高于最理想性能的三分之一。

    

    个性化通常可以提高查询性能，但在某些情况下可能会损害性能。如果我们能够预测并因此在这些情况下禁用个性化，整体性能将会更高，并且用户对个性化系统将更满意。我们使用一些最先进的预检索查询性能预测器，并提出一些新的预测器，包括先前目的的用户配置文件信息。我们研究了这些预测器之间的相关性以及个性化和原始查询之间的差异。我们还使用分类和回归技术来改进结果，并最终达到略高于最理想性能的三分之一。我们认为这是该研究领域一个良好的起点，当然还需要更多的努力和改进。

    Personalization generally improves the performance of queries but in a few cases it may also harms it. If we are able to predict and therefore to disable personalization for those situations, the overall performance will be higher and users will be more satisfied with personalized systems. We use some state-of-the-art pre-retrieval query performance predictors and propose some others including the user profile information for the previous purpose. We study the correlations among these predictors and the difference between the personalized and the original queries. We also use classification and regression techniques to improve the results and finally reach a bit more than one third of the maximum ideal performance. We think this is a good starting point within this research line, which certainly needs more effort and improvements.
    
[^11]: 一个用于早期识别和分类暗网站的大数据架构

    A Big Data Architecture for Early Identification and Categorization of Dark Web Sites. (arXiv:2401.13320v1 [cs.DC])

    [http://arxiv.org/abs/2401.13320](http://arxiv.org/abs/2401.13320)

    本文提出了一个用于早期识别和分类暗网站的大数据架构。该架构利用开源技术实现了从多个来源发现洋葱地址、下载HTML并进行内容去重和分类的功能，并成功应对了Tor的不稳定性挑战。在93天内，系统识别了80,049个洋葱服务，并对90%进行了表征。

    

    暗网因其与非法活动的关联而臭名昭著，需要系统自动监测这一领域。本文提出了一个端到端可扩展的架构，用于早期识别新的Tor网站和对其内容进行日常分析。该解决方案使用开源的大数据技术栈，包括Kubernetes、Kafka、Kubeflow和MinIO，不断从不同来源（威胁情报、代码仓库、Web-Tor网关和Tor仓库）发现洋葱地址，从Tor下载HTML，并使用MinHash LSH进行内容去重，通过BERTopic建模（SBERT嵌入、UMAP降维、HDBSCAN文档聚类和c-TF-IDF主题关键词）进行分类。在93天内，该系统识别了80,049个洋葱服务，并对其中90%进行了表征，解决了Tor的不稳定性挑战。发现了大量重复内容，仅占6.1%。

    The dark web has become notorious for its association with illicit activities and there is a growing need for systems to automate the monitoring of this space. This paper proposes an end-to-end scalable architecture for the early identification of new Tor sites and the daily analysis of their content. The solution is built using an Open Source Big Data stack for data serving with Kubernetes, Kafka, Kubeflow, and MinIO, continuously discovering onion addresses in different sources (threat intelligence, code repositories, web-Tor gateways, and Tor repositories), downloading the HTML from Tor and deduplicating the content using MinHash LSH, and categorizing with the BERTopic modeling (SBERT embedding, UMAP dimensionality reduction, HDBSCAN document clustering and c-TF-IDF topic keywords). In 93 days, the system identified 80,049 onion services and characterized 90% of them, addressing the challenge of Tor volatility. A disproportionate amount of repeated content is found, with only 6.1% u
    
[^12]: 关于时间的重要性：在检索增强语言模型中引入时间性

    It's About Time: Incorporating Temporality in Retrieval Augmented Language Models. (arXiv:2401.13222v1 [cs.IR])

    [http://arxiv.org/abs/2401.13222](http://arxiv.org/abs/2401.13222)

    在大型语言模型中引入时间性是信息检索的关键挑战，目前的检索增强语言模型无法很好地处理时间查询。

    

    网络作为全球的知识存储库，被数十亿人用于搜索信息。确保用户能够获得最相关和最新的信息是信息检索面临的关键挑战，尤其是在存在来自不同时间点的多个版本的网络内容的情况下。最近，这个挑战变得更加复杂，原因是对维基百科或网络内容进行训练的问答工具的增加使用，这些工具由大型语言模型（LLM）驱动，而这些模型被发现会虚构信息，且在处理时间信息方面存在困难。即使是引入文档数据库以减少LLM虚构的检索增强语言模型（RALM）也无法正确处理时间查询。这导致RALM在回答类似“谁赢得了温网冠军？”的查询时，只会检索与温网相关的文档内容，而不完整。

    The web serves as a global repository of knowledge, used by billions of people to search for information. Ensuring that users receive the most relevant and up-to-date information, especially in the presence of multiple versions of web content from different time points remains a critical challenge for information retrieval. This challenge has recently been compounded by the increased use of question answering tools trained on Wikipedia or web content and powered by large language models (LLMs) \citep{chatgpt} which have been found to make up information (or hallucinate), and in addition have been shown to struggle with the temporal dimensions of information. Even Retriever Augmented Language Models (RALMs) which incorporate a document database to reduce LLM hallucination are unable to handle temporal queries correctly. This leads to instances where RALMs respond to queries such as "Who won the Wimbledon Championship?", by retrieving document passages related to Wimbledon but without th
    
[^13]: 在交火中：评估使用语言模型众包枪支暴力报告的效果

    Into the crossfire: evaluating the use of a language model to crowdsource gun violence reports. (arXiv:2401.12989v1 [cs.CL])

    [http://arxiv.org/abs/2401.12989](http://arxiv.org/abs/2401.12989)

    本研究评估了使用语言模型从社交媒体数据中监测枪支暴力事件的可行性。研究团队使用经过微调的BERT模型识别巴西的枪支暴力报告并取得了高准确度。研究结果有助于人权组织收集包含所需数据的全面数据库。

    

    枪支暴力是一个紧迫且不断增长的人权问题，影响着社会的方方面面，从医疗保健和教育到心理学和经济学。可靠的枪支事件数据对于制定更有效的公共政策和应急响应至关重要。然而，缺乏全面的数据库和面对面调查的风险阻止了人权组织在大多数国家收集所需的数据。在这里，我们与一家巴西人权组织合作，对语言模型进行系统评估，以帮助监测来自社交媒体数据的现实世界枪支事件。我们提出了一个在Twitter上经过微调的基于BERT的模型，用于区分枪支暴力报告和普通葡萄牙语文本。我们的模型达到了高达0.97的AUC分数。然后，我们将我们的模型整合到一个Web应用程序中，并在实时干预中对其进行测试。我们研究并采访巴西分析师，他们在持续进行社交媒体事实核查。

    Gun violence is a pressing and growing human rights issue that affects nearly every dimension of the social fabric, from healthcare and education to psychology and the economy. Reliable data on firearm events is paramount to developing more effective public policy and emergency responses. However, the lack of comprehensive databases and the risks of in-person surveys prevent human rights organizations from collecting needed data in most countries. Here, we partner with a Brazilian human rights organization to conduct a systematic evaluation of language models to assist with monitoring real-world firearm events from social media data. We propose a fine-tuned BERT-based model trained on Twitter (now X) texts to distinguish gun violence reports from ordinary Portuguese texts. Our model achieves a high AUC score of 0.97. We then incorporate our model into a web application and test it in a live intervention. We study and interview Brazilian analysts who continuously fact-check social media
    
[^14]: 通过具有分层正则化的医学代码中心的多模态对比EHR建模预测下次就诊诊断

    Next Visit Diagnosis Prediction via Medical Code-Centric Multimodal Contrastive EHR Modelling with Hierarchical Regularisation. (arXiv:2401.11648v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2401.11648](http://arxiv.org/abs/2401.11648)

    通过医学代码中心的多模态对比EHR建模预测下次就诊诊断，并通过分层正则化提高性能。

    

    在医疗保健中，利用电子健康记录（EHR）预测下次就诊的诊断是一项必要的任务，对于制定医疗保健提供者和患者的主动未来计划至关重要。然而，之前的许多研究并没有充分解决EHR数据固有的异构和分层特征，必然导致次优的性能。为此，我们提出了NECHO，一种新颖的医学代码中心的多模态对比EHR学习框架，其中包括分层正则化。首先，我们使用定制的网络设计和一对双模态对比损失融合涵盖医学代码、人口统计数据和临床笔记的多方面信息，所有这些都围绕着医学代码表现。我们还使用医学本体中的父级信息来规范特定模态的编码器，以学习EHR数据的层次结构。对MIMIC-III数据进行的一系列实验证明了我们方法的有效性。

    Predicting next visit diagnosis using Electronic Health Records (EHR) is an essential task in healthcare, critical for devising proactive future plans for both healthcare providers and patients. Nonetheless, many preceding studies have not sufficiently addressed the heterogeneous and hierarchical characteristics inherent in EHR data, inevitably leading to sub-optimal performance. To this end, we propose NECHO, a novel medical code-centric multimodal contrastive EHR learning framework with hierarchical regularisation. First, we integrate multifaceted information encompassing medical codes, demographics, and clinical notes using a tailored network design and a pair of bimodal contrastive losses, all of which pivot around a medical code representation. We also regularise modality-specific encoders using a parental level information in medical ontology to learn hierarchical structure of EHR data. A series of experiments on MIMIC-III data demonstrates effectiveness of our approach.
    
[^15]: 使用LLMs发现极端社交媒体中的编码反犹太恶意言论的出现

    Using LLMs to discover emerging coded antisemitic hate-speech emergence in extremist social media. (arXiv:2401.10841v1 [cs.CL])

    [http://arxiv.org/abs/2401.10841](http://arxiv.org/abs/2401.10841)

    这项研究提出了一种方法，可以检测新出现的编码恶意术语，为极端社交媒体中的反犹太恶意言论提供了解决方案。

    

    网络仇恨言论的蔓延给社交媒体平台带来了一个难题。一个特殊的挑战与使用编码语言的群体有关，这些群体既想为其用户创造归属感，又想回避检测。编码语言发展迅速，并且随着时间的推移使用方式不同。本文提出了一种检测新出现的编码恶意术语的方法论。该方法在在线反犹太言论的环境中进行了测试。该方法考虑了从社交媒体平台上抓取的帖子，通常是极端主义用户使用的。帖子是使用与以前已知的针对犹太人的仇恨言论相关的种子表达式进行抓取的。该方法首先通过识别每个帖子最具代表性的表达式，并计算它们在整个语料库中的频率。过滤掉语法不一致的表达式和之前遇到过的表达式，以便关注新出现的良好形式的术语。然后进行了语义评估。

    Online hate speech proliferation has created a difficult problem for social media platforms. A particular challenge relates to the use of coded language by groups interested in both creating a sense of belonging for its users and evading detection. Coded language evolves quickly and its use varies over time. This paper proposes a methodology for detecting emerging coded hate-laden terminology. The methodology is tested in the context of online antisemitic discourse. The approach considers posts scraped from social media platforms, often used by extremist users. The posts are scraped using seed expressions related to previously known discourse of hatred towards Jews. The method begins by identifying the expressions most representative of each post and calculating their frequency in the whole corpus. It filters out grammatically incoherent expressions as well as previously encountered ones so as to focus on emergent well-formed terminology. This is followed by an assessment of semantic s
    
[^16]: 用于推荐中意图学习的端到端可学习聚类方法

    End-to-end Learnable Clustering for Intent Learning in Recommendation. (arXiv:2401.05975v1 [cs.IR])

    [http://arxiv.org/abs/2401.05975](http://arxiv.org/abs/2401.05975)

    本文提出了一种用于推荐中意图学习的端到端可学习聚类方法ELCRec，该方法解决了现有方法中的复杂优化问题和大规模数据集聚类的可扩展性问题。

    

    挖掘用户的意图在序列推荐中起着关键作用。最近的方法ICLRec使用对比学习和聚类来提取用户的潜在意图。尽管它已经显示出有效性，但现有的方法存在复杂和繁琐的交替优化问题，导致两个主要问题。首先，在广义期望最大化(EM)框架中分离表示学习和聚类优化经常导致次优性能。其次，在整个数据集上进行聚类会影响大规模行业数据的可扩展性。为了解决这些挑战，我们提出了一种新颖的意图学习方法，称为ELCRec，它将表示学习集成到一个端到端可学习聚类框架中进行推荐。

    Mining users' intents plays a crucial role in sequential recommendation. The recent approach, ICLRec, was introduced to extract underlying users' intents using contrastive learning and clustering. While it has shown effectiveness, the existing method suffers from complex and cumbersome alternating optimization, leading to two main issues. Firstly, the separation of representation learning and clustering optimization within a generalized expectation maximization (EM) framework often results in sub-optimal performance. Secondly, performing clustering on the entire dataset hampers scalability for large-scale industry data. To address these challenges, we propose a novel intent learning method called \underline{ELCRec}, which integrates representation learning into an \underline{E}nd-to-end \underline{L}earnable \underline{C}lustering framework for \underline{Rec}ommendation. Specifically, we encode users' behavior sequences and initialize the cluster centers as learnable network parameter
    
[^17]: 通过视觉模块插件解锁密集检索的多模态能力

    Unlock Multi-Modal Capability of Dense Retrieval via Visual Module Plugin. (arXiv:2310.14037v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2310.14037](http://arxiv.org/abs/2310.14037)

    本文介绍了一种名为MARVEL的多模态检索模型，通过视觉模块插件为密集检索器添加图像理解能力，并且在多模态检索任务中取得了显著优于最先进方法的结果。

    

    本文提出了通过视觉模块插件（MARVEL）学习查询和多模态文档的嵌入空间以进行检索的多模态检索模型。MARVEL使用统一的编码器模型对查询和多模态文档进行编码，有助于减小图像和文本之间的模态差距。具体而言，我们通过将视觉模块编码的图像特征作为其输入，使得经过训练的密集检索器T5-ANCE具有图像理解能力。为了促进多模态检索任务，我们基于ClueWeb22数据集构建了ClueWeb22-MM数据集，将锚文本作为查询，并从锚链接的网页中提取相关文本和图像文档。实验证明，MARVEL在多模态检索数据集WebQA和ClueWeb22-MM上明显优于最先进的方法。进一步的分析表明，视觉模块插件方法为实现图像理解能力量身定制。

    This paper proposes Multi-modAl Retrieval model via Visual modulE pLugin (MARVEL) to learn an embedding space for queries and multi-modal documents to conduct retrieval. MARVEL encodes queries and multi-modal documents with a unified encoder model, which helps to alleviate the modality gap between images and texts. Specifically, we enable the image understanding ability of a well-trained dense retriever, T5-ANCE, by incorporating the image features encoded by the visual module as its inputs. To facilitate the multi-modal retrieval tasks, we build the ClueWeb22-MM dataset based on the ClueWeb22 dataset, which regards anchor texts as queries, and exact the related texts and image documents from anchor linked web pages. Our experiments show that MARVEL significantly outperforms the state-of-the-art methods on the multi-modal retrieval dataset WebQA and ClueWeb22-MM. Our further analyses show that the visual module plugin method is tailored to enable the image understanding ability for an 
    
[^18]: 纯粹的消息传递可以估计共同邻居进行链路预测

    Pure Message Passing Can Estimate Common Neighbor for Link Prediction. (arXiv:2309.00976v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2309.00976](http://arxiv.org/abs/2309.00976)

    这篇论文提出了一种纯粹的消息传递方法，用于估计共同邻居进行链路预测。该方法通过利用输入向量的正交性来捕捉联合结构特征，提出了一种新的链路预测模型MPLP，该模型利用准正交向量估计链路级结构特征，同时保留了节点级复杂性。

    

    消息传递神经网络（MPNN）已成为图表示学习中的事实标准。然而，在链路预测方面，它们往往表现不佳，被简单的启发式算法如共同邻居（CN）所超越。这种差异源于一个根本限制：尽管MPNN在节点级表示方面表现出色，但在编码链路预测中至关重要的联合结构特征（如CN）方面则遇到困难。为了弥合这一差距，我们认为通过利用输入向量的正交性，纯粹的消息传递确实可以捕捉到联合结构特征。具体而言，我们研究了MPNN在近似CN启发式算法方面的能力。基于我们的发现，我们引入了一种新的链路预测模型——消息传递链路预测器（MPLP）。MPLP利用准正交向量估计链路级结构特征，同时保留节点级复杂性。此外，我们的方法表明利用消息传递捕捉结构特征能够改善链路预测性能。

    Message Passing Neural Networks (MPNNs) have emerged as the {\em de facto} standard in graph representation learning. However, when it comes to link prediction, they often struggle, surpassed by simple heuristics such as Common Neighbor (CN). This discrepancy stems from a fundamental limitation: while MPNNs excel in node-level representation, they stumble with encoding the joint structural features essential to link prediction, like CN. To bridge this gap, we posit that, by harnessing the orthogonality of input vectors, pure message-passing can indeed capture joint structural features. Specifically, we study the proficiency of MPNNs in approximating CN heuristics. Based on our findings, we introduce the Message Passing Link Predictor (MPLP), a novel link prediction model. MPLP taps into quasi-orthogonal vectors to estimate link-level structural features, all while preserving the node-level complexities. Moreover, our approach demonstrates that leveraging message-passing to capture stru
    
[^19]: 大型语言模型是零-shot推荐系统排名者

    Large Language Models are Zero-Shot Rankers for Recommender Systems. (arXiv:2305.08845v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2305.08845](http://arxiv.org/abs/2305.08845)

    大型语言模型表现出有希望的零-shot排名能力，但在感知历史互动顺序和受到偏见影响方面存在问题。本研究通过特殊设计的提示和引导策略来缓解这些问题。

    

    最近，大型语言模型（例如GPT-4）展示出了令人印象深刻的通用任务解决能力，包括潜力接近推荐任务。在这一研究方向上，本文旨在研究作为推荐系统排名模型的LLMs的能力。

    Recently, large language models (LLMs) (e.g., GPT-4) have demonstrated impressive general-purpose task-solving abilities, including the potential to approach recommendation tasks. Along this line of research, this work aims to investigate the capacity of LLMs that act as the ranking model for recommender systems. We first formalize the recommendation problem as a conditional ranking task, considering sequential interaction histories as conditions and the items retrieved by other candidate generation models as candidates. To solve the ranking task by LLMs, we carefully design the prompting template and conduct extensive experiments on two widely-used datasets. We show that LLMs have promising zero-shot ranking abilities but (1) struggle to perceive the order of historical interactions, and (2) can be biased by popularity or item positions in the prompts. We demonstrate that these issues can be alleviated using specially designed prompting and bootstrapping strategies. Equipped with thes
    
[^20]: 在开放领域的对话式搜索中更好地理解用户满意度

    Towards Better Understanding of User Satisfaction in Open-Domain Conversational Search. (arXiv:2204.02659v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2204.02659](http://arxiv.org/abs/2204.02659)

    在对话式搜索中，如何准确地建模用户满意度是一个重要问题。现有方法要么忽视用户信息需求，要么忽视混合主动属性。传统的IR研究使用Cranfield范式和用户行为建模来估计用户满意度，但实践中具有挑战性。

    

    随着对话式搜索的普及，如何评估对话式搜索系统的性能成为信息检索领域的一个重要问题。现有对话式搜索评估的工作主要可分为两类：基于语义相似性构建度量标准（如BLUE、METEOR和BERTScore），或直接使用传统搜索方法（如nDCG、RBP和nERR）评估系统的响应排名性能。然而，这些方法要么忽视用户的信息需求，要么忽视对话式搜索的混合主动属性。这引发了一个问题：如何在对话式搜索场景中准确地建模用户满意度。由于明确要求用户提供满意度反馈很困难，传统的IR研究往往依赖Cranfield范式（即第三方注释）和用户行为建模来估计搜索中的用户满意度。然而，实现这种模型在实践中很有挑战性。

    With the increasing popularity of conversational search, how to evaluate the performance of conversational search systems has become an important question in the IR community. Existing works on conversational search evaluation can mainly be categorized into two streams: (1) constructing metrics based on semantic similarity (e.g. BLUE, METEOR and BERTScore), or (2) directly evaluating the response ranking performance of the system using traditional search methods (e.g. nDCG, RBP and nERR). However, these methods either ignore the information need of the user or ignore the mixed-initiative property of conversational search. This raises the question of how to accurately model user satisfaction in conversational search scenarios. Since explicitly asking users to provide satisfaction feedback is difficult, traditional IR studies often rely on the Cranfield paradigm (i.e., third-party annotation) and user behavior modeling to estimate user satisfaction in search. However, the feasibility and
    

