# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [DESERE: The 1st Workshop on Decentralised Search and Recommendation](https://arxiv.org/abs/2403.07732) | 探索和分享关于去中心化网络服务的创新想法，主要关注去中心化系统对社会的影响、算法和性能挑战，以及支持去中心化系统和服务的基础设施。 |
| [^2] | [Analyzing Adversarial Attacks on Sequence-to-Sequence Relevance Models](https://arxiv.org/abs/2403.07654) | 对现代序列到序列相关性模型进行了对抗攻击分析，发现恶意文档可以通过注入提示来操纵其相关性得分，这一攻击机制影响了不同相关性模型，但词汇模型BM25不受影响。 |
| [^3] | [Empowering Sequential Recommendation from Collaborative Signals and Semantic Relatedness](https://arxiv.org/abs/2403.07623) | 该论文提出了一种端到端两流架构TSSR，旨在通过有效地将协作信号和语义相关性结合起来，增强顺序推荐任务。 |
| [^4] | [The future of document indexing: GPT and Donut revolutionize table of content processing](https://arxiv.org/abs/2403.07553) | 该论文介绍了一种利用Donut和OpenAI GPT-3.5 Turbo两种前沿AI模型自动提取文件中结构化信息的创新方法，在建筑规格文件的目录处理中取得了显著的准确性，代表了文档索引领域向自动化信息提取迈出的重要一步。 |
| [^5] | [Towards Graph Foundation Models for Personalization](https://arxiv.org/abs/2403.07478) | 本文提出了一种面向个性化的基于图的基础建模方法，其中的Heterogeneous GNN旨在捕捉跨多种可推荐项目类型的多跳内容和消费关系。 |
| [^6] | [LIST: Learning to Index Spatio-Textual Data for Embedding based Spatial Keyword Queries](https://arxiv.org/abs/2403.07331) | 提出了一种名为LIST的新技术，通过学习为基于嵌入的空间关键词查询建立空间文本数据索引，以加速top-k搜索过程。 |
| [^7] | [Self-supervised Contrastive Learning for Implicit Collaborative Filtering](https://arxiv.org/abs/2403.07265) | 提出了一个简单的自监督对比学习框架，通过正特征增强和负标签增强改进自我监督信号，提高了隐式协同过滤准确性，并建立了一种有效的负标签增强技术，实现了有效的增强 |
| [^8] | [Time Series Analysis of Key Societal Events as Reflected in Complex Social Media Data Streams](https://arxiv.org/abs/2403.07090) | 通过时间序列分析探索GAB和Telegram这两个社交媒体平台上的叙事演变，提出了一种新颖的方法来研究多个社交媒体领域，以提取可能被掩盖的关键信息。 |
| [^9] | [MetaSplit: Meta-Split Network for Limited-Stock Product Recommendation](https://arxiv.org/abs/2403.06747) | 提出了Meta-Split网络（MSN）来解决消费者之间电子商务平台中限量库存产品推荐中的独特挑战，通过分割用户历史序列来有效利用用户历史信息。 |
| [^10] | [Navigating the Post-API Dilemma Search Engine Results Pages Present a Biased View of Social Media Data](https://arxiv.org/abs/2401.15479) | 搜索引擎结果页面可以作为社交媒体数据的替代方案，但存在对流行帖子偏见较高、情感更积极以及忽视政治、色情和粗俗帖子的问题。 |
| [^11] | [In-context Learning with Retrieved Demonstrations for Language Models: A Survey.](http://arxiv.org/abs/2401.11624) | 本综述调查了一种名为检索示范的方法，它通过使用特定于输入查询的示范来提高语言模型的少量样本情境学习（ICL）能力。这种方法不仅提高了学习效率和可扩展性，还减少了手动示例选择中的偏见。 |
| [^12] | [Multi-View Variational Autoencoder for Missing Value Imputation in Untargeted Metabolomics.](http://arxiv.org/abs/2310.07990) | 本文提出了一种新的方法，利用多视图变分自动编码器来填充非目标代谢组学中的缺失值，该方法利用了全基因组测序数据和参考代谢物的信息，可以有效地根据基因组信息填充缺失的代谢组学值。 |
| [^13] | [Improving position bias estimation against sparse and skewed dataset with item embedding.](http://arxiv.org/abs/2305.13931) | 该研究提出了一种利用物品嵌入来缓解广告营销领域中位置偏差稀疏性问题的回归EM算法变体。 |

# 详细

[^1]: DESERE：第一届去中心化搜索和推荐研讨会

    DESERE: The 1st Workshop on Decentralised Search and Recommendation

    [https://arxiv.org/abs/2403.07732](https://arxiv.org/abs/2403.07732)

    探索和分享关于去中心化网络服务的创新想法，主要关注去中心化系统对社会的影响、算法和性能挑战，以及支持去中心化系统和服务的基础设施。

    

    arXiv:2403.07732v1 声明类型：新摘要：DESERE研讨会，我们的第一届去中心化搜索和推荐研讨会，为研究人员提供一个探讨和分享关于去中心化网络服务的创新想法的平台，主要关注三个主题：（i）去中心化系统对社会的影响：它们对隐私、政策和监管的影响；（ii）去中心化应用程序：由去中心化带来的算法和性能挑战；和（iii）支持去中心化系统和服务的基础设施：点对点网络、路由和性能评估工具

    arXiv:2403.07732v1 Announce Type: new  Abstract: The DESERE Workshop, our First Workshop on Decentralised Search and Recommendation, offers a platform for researchers to explore and share innovative ideas on decentralised web services, mainly focusing on three major topics: (i) societal impact of decentralised systems: their effect on privacy, policy, and regulation; (ii) decentralising applications: algorithmic and performance challenges that arise from decentralisation; and (iii) infrastructure to support decentralised systems and services: peer-to-peer networks, routing, and performance evaluation tools
    
[^2]: 分析序列到序列相关性模型上的对抗攻击

    Analyzing Adversarial Attacks on Sequence-to-Sequence Relevance Models

    [https://arxiv.org/abs/2403.07654](https://arxiv.org/abs/2403.07654)

    对现代序列到序列相关性模型进行了对抗攻击分析，发现恶意文档可以通过注入提示来操纵其相关性得分，这一攻击机制影响了不同相关性模型，但词汇模型BM25不受影响。

    

    现代序列到序列相关性模型（如monoT5）能够通过交叉编码有效捕捉查询和文档之间的复杂文本交互。然而，在提示中使用自然语言标记，如Query、Document和Relevant对于monoT5而言，为恶意文档开辟了注入攻击向量，通过注入提示（例如添加true等目标词）来操纵其相关性得分。由于检索评估中尚未考虑这样的可能性，我们通过手动构建的模板和基于LLM的文档重写，分析了查询无关提示注入对多个现有相关性模型的影响。我们在TREC深度学习跟踪上的实验表明，对抗性文档可以轻松操纵不同的序列到序列相关性模型，而BM25（作为典型的词汇模型）不受影响。值得注意的是，这些攻击也影响仅编码器相关性模型（）

    arXiv:2403.07654v1 Announce Type: new  Abstract: Modern sequence-to-sequence relevance models like monoT5 can effectively capture complex textual interactions between queries and documents through cross-encoding. However, the use of natural language tokens in prompts, such as Query, Document, and Relevant for monoT5, opens an attack vector for malicious documents to manipulate their relevance score through prompt injection, e.g., by adding target words such as true. Since such possibilities have not yet been considered in retrieval evaluation, we analyze the impact of query-independent prompt injection via manually constructed templates and LLM-based rewriting of documents on several existing relevance models. Our experiments on the TREC Deep Learning track show that adversarial documents can easily manipulate different sequence-to-sequence relevance models, while BM25 (as a typical lexical model) is not affected. Remarkably, the attacks also affect encoder-only relevance models (which
    
[^3]: 从协作信号和语义相关性中增强顺序推荐

    Empowering Sequential Recommendation from Collaborative Signals and Semantic Relatedness

    [https://arxiv.org/abs/2403.07623](https://arxiv.org/abs/2403.07623)

    该论文提出了一种端到端两流架构TSSR，旨在通过有效地将协作信号和语义相关性结合起来，增强顺序推荐任务。

    

    顺序推荐系统(SRS)通过对按时间顺序排列的历史行为进行建模，能够捕捉到动态用户偏好。尽管有效，仅关注行为中的\textit{协作信号}并不能充分把握用户兴趣。在内容特征中反映的\textit{语义相关性}建模也是重要的，例如图片和文本。因此，本文旨在通过有效地将协作信号和语义相关性结合起来，增强SRS任务。值得注意的是，我们实证指出由于语义鸿沟问题，实现这一目标并不容易。因此，我们提出了一种用于顺序推荐的端到端两流架构，命名为TSSR，从基于ID和基于内容的序列中学习用户偏好。具体来说，我们首先提出了新颖的层次对比模块，包括粗用户粒度和细项粒度术语，以对齐表征。

    arXiv:2403.07623v1 Announce Type: new  Abstract: Sequential recommender systems (SRS) could capture dynamic user preferences by modeling historical behaviors ordered in time. Despite effectiveness, focusing only on the \textit{collaborative signals} from behaviors does not fully grasp user interests. It is also significant to model the \textit{semantic relatedness} reflected in content features, e.g., images and text. Towards that end, in this paper, we aim to enhance the SRS tasks by effectively unifying collaborative signals and semantic relatedness together. Notably, we empirically point out that it is nontrivial to achieve such a goal due to semantic gap issues. Thus, we propose an end-to-end two-stream architecture for sequential recommendation, named TSSR, to learn user preferences from ID-based and content-based sequence. Specifically, we first present novel hierarchical contrasting module, including coarse user-grained and fine item-grained terms, to align the representations o
    
[^4]: 文档索引的未来：GPT和Donut革新目录内容处理

    The future of document indexing: GPT and Donut revolutionize table of content processing

    [https://arxiv.org/abs/2403.07553](https://arxiv.org/abs/2403.07553)

    该论文介绍了一种利用Donut和OpenAI GPT-3.5 Turbo两种前沿AI模型自动提取文件中结构化信息的创新方法，在建筑规格文件的目录处理中取得了显著的准确性，代表了文档索引领域向自动化信息提取迈出的重要一步。

    

    工业项目严重依赖冗长、复杂的规格文件，手工提取结构化信息繁琐且效率低下。本文介绍一种创新方法来自动化这一过程，利用两种前沿AI模型的能力：Donut，一种可以直接从扫描文档中提取信息而无需OCR的模型，以及OpenAI GPT-3.5 Turbo，一个强大的大型语言模型。所提出的方法首先通过获取建筑规格文件的目录（ToCs），然后将ToCs文本结构化为JSON数据。尤为显著的准确性被实现，Donut在有效组织ToCs方面达到85%，GPT-3.5 Turbo达到89%。这一里程碑式的成就代表了文档索引领域的重大进步，展示了AI自动化信息提取在各种文档类型中的巨大潜力，提升了效率。

    arXiv:2403.07553v1 Announce Type: cross  Abstract: Industrial projects rely heavily on lengthy, complex specification documents, making tedious manual extraction of structured information a major bottleneck. This paper introduces an innovative approach to automate this process, leveraging the capabilities of two cutting-edge AI models: Donut, a model that extracts information directly from scanned documents without OCR, and OpenAI GPT-3.5 Turbo, a robust large language model. The proposed methodology is initiated by acquiring the table of contents (ToCs) from construction specification documents and subsequently structuring the ToCs text into JSON data. Remarkable accuracy is achieved, with Donut reaching 85% and GPT-3.5 Turbo reaching 89% in effectively organizing the ToCs. This landmark achievement represents a significant leap forward in document indexing, demonstrating the immense potential of AI to automate information extraction tasks across diverse document types, boosting effic
    
[^5]: 面向个性化的图基础模型

    Towards Graph Foundation Models for Personalization

    [https://arxiv.org/abs/2403.07478](https://arxiv.org/abs/2403.07478)

    本文提出了一种面向个性化的基于图的基础建模方法，其中的Heterogeneous GNN旨在捕捉跨多种可推荐项目类型的多跳内容和消费关系。

    

    在个性化领域，整合消费信号和基于内容的表示等多样信息源变得日益关键，以构建最先进的解决方案。在这方面，围绕Graph Neural Networks（GNNs）和Foundation Models（FMs）的研究存在两大趋势。虽然GNNs成为工业界在规模上实现个性化的热门解决方案，但FMs最近才因其在排名和检索等个性化任务中表现出色而受到关注。本文提出了一种针对个性化的基于图的基础建模方法。该方法的核心是一种设计用于捕捉跨各种可推荐项目类型的多跳内容和消费关系的异质GNN（HGNN）。为确保基础模型所需的一般性，我们采用了基于大语言模型（LLM）的文本特征化方法。

    arXiv:2403.07478v1 Announce Type: cross  Abstract: In the realm of personalization, integrating diverse information sources such as consumption signals and content-based representations is becoming increasingly critical to build state-of-the-art solutions. In this regard, two of the biggest trends in research around this subject are Graph Neural Networks (GNNs) and Foundation Models (FMs). While GNNs emerged as a popular solution in industry for powering personalization at scale, FMs have only recently caught attention for their promising performance in personalization tasks like ranking and retrieval. In this paper, we present a graph-based foundation modeling approach tailored to personalization. Central to this approach is a Heterogeneous GNN (HGNN) designed to capture multi-hop content and consumption relationships across a range of recommendable item types. To ensure the generality required from a Foundation Model, we employ a Large Language Model (LLM) text-based featurization of
    
[^6]: LIST: 学习为基于嵌入的空间关键词查询建立空间文本数据索引

    LIST: Learning to Index Spatio-Textual Data for Embedding based Spatial Keyword Queries

    [https://arxiv.org/abs/2403.07331](https://arxiv.org/abs/2403.07331)

    提出了一种名为LIST的新技术，通过学习为基于嵌入的空间关键词查询建立空间文本数据索引，以加速top-k搜索过程。

    

    随着空间文本数据的普及，“Top-k KNN空间关键词查询（TkQs）”已经在许多实际应用中发现，它基于一个评价空间和文本相关性的排名函数返回一个对象列表。现有的用于TkQs的geo-textual索引使用传统的检索模型（如BM25）来计算文本相关性，并通常利用简单的线性函数来计算空间相关性，但其效果有限。为了提高效果，最近提出了几种深度学习模型，但它们存在严重的效率问题。据我们所知，目前没有为加速这些深度学习模型的top-k搜索过程专门设计的有效索引。为了解决这些问题，我们提出了一种新技术，通过学习为回答基于嵌入的空间关键词查询（称为LIST）建立空间文本数据索引。LIST具有两个新颖组件。

    arXiv:2403.07331v1 Announce Type: new  Abstract: With the proliferation of spatio-textual data, Top-k KNN spatial keyword queries (TkQs), which return a list of objects based on a ranking function that evaluates both spatial and textual relevance, have found many real-life applications. Existing geo-textual indexes for TkQs use traditional retrieval models like BM25 to compute text relevance and usually exploit a simple linear function to compute spatial relevance, but its effectiveness is limited. To improve effectiveness, several deep learning models have recently been proposed, but they suffer severe efficiency issues. To the best of our knowledge, there are no efficient indexes specifically designed to accelerate the top-k search process for these deep learning models.   To tackle these issues, we propose a novel technique, which Learns to Index the Spatio-Textual data for answering embedding based spatial keyword queries (called LIST). LIST is featured with two novel components. F
    
[^7]: 基于自监督对比学习的隐式协同过滤

    Self-supervised Contrastive Learning for Implicit Collaborative Filtering

    [https://arxiv.org/abs/2403.07265](https://arxiv.org/abs/2403.07265)

    提出了一个简单的自监督对比学习框架，通过正特征增强和负标签增强改进自我监督信号，提高了隐式协同过滤准确性，并建立了一种有效的负标签增强技术，实现了有效的增强

    

    基于对比学习的推荐算法显著推进了自监督推荐领域的发展，特别是以BPR作为主要代表的掌握隐式协同过滤的排名预测任务。然而，在推荐系统中存在的误正例和误负例阻碍了准确的偏好学习。本研究提出了一个简单的自监督对比学习框架，利用正特征增强和负标签增强来改进自我监督信号。理论分析表明，我们的学习方法等效于利用代表用户兴趣中心的潜在变量最大化似然估计。此外，我们建立了一种有效的负标签增强技术，根据它们的相对排名位置线性相关地采样未标记的示例，实现了有效的增强。

    arXiv:2403.07265v1 Announce Type: new  Abstract: Contrastive learning-based recommendation algorithms have significantly advanced the field of self-supervised recommendation, particularly with BPR as a representative ranking prediction task that dominates implicit collaborative filtering. However, the presence of false-positive and false-negative examples in recommendation systems hampers accurate preference learning. In this study, we propose a simple self-supervised contrastive learning framework that leverages positive feature augmentation and negative label augmentation to improve the self-supervisory signal. Theoretical analysis demonstrates that our learning method is equivalent to maximizing the likelihood estimation with latent variables representing user interest centers. Additionally, we establish an efficient negative label augmentation technique that samples unlabeled examples with a probability linearly dependent on their relative ranking positions, enabling efficient augm
    
[^8]: 反映复杂社交媒体数据流中关键社会事件的时间序列分析

    Time Series Analysis of Key Societal Events as Reflected in Complex Social Media Data Streams

    [https://arxiv.org/abs/2403.07090](https://arxiv.org/abs/2403.07090)

    通过时间序列分析探索GAB和Telegram这两个社交媒体平台上的叙事演变，提出了一种新颖的方法来研究多个社交媒体领域，以提取可能被掩盖的关键信息。

    

    arXiv:2403.07090v1 公告类型: 跨界 摘要: 社交媒体平台蕴藏着宝贵的见解，然而提取关键信息可能具有挑战性。传统的自上而下方法通常难以捕捉快速变化事件中的关键信号。随着全球事件迅速演变，包括虚假信息的社交媒体叙事成为重要的见解来源。为了满足归纳策略的需求，我们探索了一种小众社交媒体平台GAB和一个成熟的消息服务Telegram，以开发适用于更广泛范围的方法。本研究使用定量基于语料库的话语分析技术来研究这些平台上的叙事演变。我们的方法是研究多个社交媒体领域以提炼可能被掩盖的关键信息的新颖方式，从而提供有用且可操作的见解。该论文详细介绍了收集和预处理GAB和Telegram数据的技术和方法论方面。

    arXiv:2403.07090v1 Announce Type: cross  Abstract: Social media platforms hold valuable insights, yet extracting essential information can be challenging. Traditional top-down approaches often struggle to capture critical signals in rapidly changing events. As global events evolve swiftly, social media narratives, including instances of disinformation, become significant sources of insights. To address the need for an inductive strategy, we explore a niche social media platform GAB and an established messaging service Telegram, to develop methodologies applicable on a broader scale. This study investigates narrative evolution on these platforms using quantitative corpus-based discourse analysis techniques. Our approach is a novel mode to study multiple social media domains to distil key information which may be obscured otherwise, allowing for useful and actionable insights. The paper details the technical and methodological aspects of gathering and preprocessing GAB and Telegram data 
    
[^9]: MetaSplit: 用于限量产品推荐的Meta-Split网络

    MetaSplit: Meta-Split Network for Limited-Stock Product Recommendation

    [https://arxiv.org/abs/2403.06747](https://arxiv.org/abs/2403.06747)

    提出了Meta-Split网络（MSN）来解决消费者之间电子商务平台中限量库存产品推荐中的独特挑战，通过分割用户历史序列来有效利用用户历史信息。

    

    相对于面向消费者的电子商务系统，消费者之间的电子商务平台通常会遇到限量库存问题，即产品在C2C系统中只能销售一次。这为点击率（CTR）预测带来了几个独特的挑战。鉴于每个产品（即商品）的有限用户交互，CTR模型中对应的商品嵌入可能不容易收敛。这使得传统基于序列建模的方法无法有效利用用户历史信息，因为历史用户行为包含了不同库存量的商品混合。特别是，序列模型中的注意力机制倾向于将更多累积用户交互的产品分配更高的分数，导致限量产品被忽视且对最终输出的贡献较少。为此，我们提出了Meta-Split网络（MSN）来分割用户历史序列...

    arXiv:2403.06747v1 Announce Type: new  Abstract: Compared to business-to-consumer (B2C) e-commerce systems, consumer-to-consumer (C2C) e-commerce platforms usually encounter the limited-stock problem, that is, a product can only be sold one time in a C2C system. This poses several unique challenges for click-through rate (CTR) prediction. Due to limited user interactions for each product (i.e. item), the corresponding item embedding in the CTR model may not easily converge. This makes the conventional sequence modeling based approaches cannot effectively utilize user history information since historical user behaviors contain a mixture of items with different volume of stocks. Particularly, the attention mechanism in a sequence model tends to assign higher score to products with more accumulated user interactions, making limited-stock products being ignored and contribute less to the final output. To this end, we propose the Meta-Split Network (MSN) to split user history sequence regar
    
[^10]: 应对后API困境：搜索引擎结果页面呈现社交媒体数据的偏见观

    Navigating the Post-API Dilemma Search Engine Results Pages Present a Biased View of Social Media Data

    [https://arxiv.org/abs/2401.15479](https://arxiv.org/abs/2401.15479)

    搜索引擎结果页面可以作为社交媒体数据的替代方案，但存在对流行帖子偏见较高、情感更积极以及忽视政治、色情和粗俗帖子的问题。

    

    最近停止访问社交媒体API的决定对互联网研究和整个计算社会科学领域产生了不利影响。这种对数据的访问缺乏已被称为互联网研究的后API时代。幸运的是，流行的搜索引擎有能力爬取、捕获和展示社交媒体数据在其搜索引擎结果页面(SERP)上，如果提供适当的搜索查询，可能会为这一困境提供解决方案。在当前工作中，我们问：SERP是否提供社交媒体数据的完整和无偏见样本？ SERP是否是直接API访问的可行替代方案？为了回答这些问题，我们对（Google）SERP结果和来自Reddit和Twitter/X的非取样数据进行了比较分析。我们发现，SERP结果在支持流行帖子方面存在高度偏见；反对政治、色情和粗俗帖子；在情感上更为积极；并有大

    arXiv:2401.15479v2 Announce Type: replace-cross  Abstract: Recent decisions to discontinue access to social media APIs are having detrimental effects on Internet research and the field of computational social science as a whole. This lack of access to data has been dubbed the Post-API era of Internet research. Fortunately, popular search engines have the means to crawl, capture, and surface social media data on their Search Engine Results Pages (SERP) if provided the proper search query, and may provide a solution to this dilemma. In the present work we ask: does SERP provide a complete and unbiased sample of social media data? Is SERP a viable alternative to direct API-access? To answer these questions, we perform a comparative analysis between (Google) SERP results and nonsampled data from Reddit and Twitter/X. We find that SERP results are highly biased in favor of popular posts; against political, pornographic, and vulgar posts; are more positive in their sentiment; and have large 
    
[^11]: 通过检索示范进行上下文学习的语言模型：一项综述

    In-context Learning with Retrieved Demonstrations for Language Models: A Survey. (arXiv:2401.11624v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2401.11624](http://arxiv.org/abs/2401.11624)

    本综述调查了一种名为检索示范的方法，它通过使用特定于输入查询的示范来提高语言模型的少量样本情境学习（ICL）能力。这种方法不仅提高了学习效率和可扩展性，还减少了手动示例选择中的偏见。

    

    语言模型，特别是预训练的大型语言模型，已展示出卓越的能力，可以在输入上下文中进行少量样本的情境学习（ICL），并在新任务上具有适应能力。然而，模型的ICL能力对于少样本示范的选择是敏感的。最近的一项研究进展是检索针对每个输入查询定制的示范。示范检索的实现相对简单，利用现有的数据库和检索系统。这不仅提高了学习过程的效率和可扩展性，而且已经证明可以减少手动示例选择中的偏见。鉴于令人鼓舞的结果和在检索示范的ICL方面不断增长的研究，我们进行了广泛的研究综述。在这项综述中，我们讨论和比较了检索模型的不同设计选择，检索训练

    Language models, especially pre-trained large language models, have showcased remarkable abilities as few-shot in-context learners (ICL), adept at adapting to new tasks with just a few demonstrations in the input context. However, the model's ability to perform ICL is sensitive to the choice of the few-shot demonstrations. Instead of using a fixed set of demonstrations, one recent development is to retrieve demonstrations tailored to each input query. The implementation of demonstration retrieval is relatively straightforward, leveraging existing databases and retrieval systems. This not only improves the efficiency and scalability of the learning process but also has been shown to reduce biases inherent in manual example selection. In light of the encouraging results and growing research in ICL with retrieved demonstrations, we conduct an extensive review of studies in this area. In this survey, we discuss and compare different design choices for retrieval models, retrieval training p
    
[^12]: 多视图变分自动编码器在非目标代谢组学中缺失值填充中的应用

    Multi-View Variational Autoencoder for Missing Value Imputation in Untargeted Metabolomics. (arXiv:2310.07990v1 [q-bio.GN])

    [http://arxiv.org/abs/2310.07990](http://arxiv.org/abs/2310.07990)

    本文提出了一种新的方法，利用多视图变分自动编码器来填充非目标代谢组学中的缺失值，该方法利用了全基因组测序数据和参考代谢物的信息，可以有效地根据基因组信息填充缺失的代谢组学值。

    

    背景：在基于质谱的代谢组学中，缺失数据是一个常见的挑战，可能导致偏倚和不完整的分析。将全基因组测序（WGS）数据与代谢组学数据整合起来，已经成为增强代谢组学研究中数据填充准确性的一种有前景的方法。方法：在本研究中，我们提出了一种新的方法，利用来自WGS数据和参考代谢物的信息来填充未知代谢物。我们的方法利用多视图变分自动编码器共同对负担评分、多基因风险评分（PGS）和连锁不平衡（LD）删减的单核苷酸多态性（SNPs）进行特征提取和缺失代谢组学数据的填充。通过学习两种组学数据的潜在表示，我们的方法可以根据基因组信息有效地填充缺失的代谢组学值。结果：我们在具有缺失值和不完整数据的实验代谢组学数据集上评估了我们方法的性能。

    Background: Missing data is a common challenge in mass spectrometry-based metabolomics, which can lead to biased and incomplete analyses. The integration of whole-genome sequencing (WGS) data with metabolomics data has emerged as a promising approach to enhance the accuracy of data imputation in metabolomics studies. Method: In this study, we propose a novel method that leverages the information from WGS data and reference metabolites to impute unknown metabolites. Our approach utilizes a multi-view variational autoencoder to jointly model the burden score, polygenetic risk score (PGS), and linkage disequilibrium (LD) pruned single nucleotide polymorphisms (SNPs) for feature extraction and missing metabolomics data imputation. By learning the latent representations of both omics data, our method can effectively impute missing metabolomics values based on genomic information. Results: We evaluate the performance of our method on empirical metabolomics datasets with missing values and de
    
[^13]: 利用物品嵌入改进在稀疏和倾斜数据集中对位置偏差的估计

    Improving position bias estimation against sparse and skewed dataset with item embedding. (arXiv:2305.13931v1 [cs.IR])

    [http://arxiv.org/abs/2305.13931](http://arxiv.org/abs/2305.13931)

    该研究提出了一种利用物品嵌入来缓解广告营销领域中位置偏差稀疏性问题的回归EM算法变体。

    

    在学习排名中，估计位置偏差是一个众所周知的挑战。电子商务应用程序中的点击数据（例如广告定位和搜索引擎）提供了隐含但丰富的反馈，以改进个性化排名。然而，点击数据本质上包括各种偏差，例如位置偏差。点击建模旨在去噪有偏的点击数据并提取可靠的信号。已经提出了随机化结果和回归期望最大化算法来解决位置偏差。但是，这两种方法都需要各种观察值对（项目、位置）。然而，在广告营销的实际情况下，营销人员经常按固定的预定顺序显示广告，估计因此而受到影响。我们将位置偏差估计中的（项目、位置）稀疏性问题作为新问题，并提出了一种利用物品嵌入来缓解稀疏问题的回归EM算法变体。我们首先使用合成数据集评估我们的方法。

    Estimating position bias is a well-known challenge in Learning to rank (L2R). Click data in e-commerce applications, such as advertisement targeting and search engines, provides implicit but abundant feedback to improve personalized rankings. However, click data inherently include various biases like position bias. Click modeling is aimed at denoising biases in click data and extracting reliable signals. Result Randomization and Regression Expectation-maximization algorithm have been proposed to solve position bias. Both methods require various pairs of observations (item, position). However, in real cases of advertising, marketers frequently display advertisements in a fixed pre-determined order, and estimation suffers from it. We propose this sparsity of (item, position) in position bias estimation as a novel problem, and we propose a variant of the Regression EM algorithm which utilizes item embeddings to alleviate the issue of the sparsity. With a synthetic dataset, we first evalua
    

