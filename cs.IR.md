# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Leveraging High-Resolution Features for Improved Deep Hashing-based Image Retrieval](https://arxiv.org/abs/2403.13747) | 探索利用高分辨率特征进行图像检索，提出了一种利用High-Resolution Networks作为骨干的深度哈希网络（HHNet），在各个基准数据集上表现出优越性能。 |
| [^2] | [No more optimization rules: LLM-enabled policy-based multi-modal query optimizer (version 1)](https://arxiv.org/abs/2403.13597) | LLM启用了基于策略的多模查询优化器，摆脱了传统的基于规则的优化方法，为查询优化带来全新的可能性。 |
| [^3] | [A Large Language Model Enhanced Sequential Recommender for Joint Video and Comment Recommendation](https://arxiv.org/abs/2403.13574) | 提出了一个新颖的推荐方法LSVCR，通过融合用户与视频和评论的交互历史，联合进行个性化视频和评论推荐 |
| [^4] | [A Unified Optimal Transport Framework for Cross-Modal Retrieval with Noisy Labels](https://arxiv.org/abs/2403.13480) | 这项工作提出了UOT-RCL，一个基于最优输运的统一框架，用于解决带有噪声标签的跨模态检索中的两个关键挑战，即强制多模态样本对齐不正确的语义以及扩大异质性差距，从而提升检索性能。 |
| [^5] | [DESIRE-ME: Domain-Enhanced Supervised Information REtrieval using Mixture-of-Experts](https://arxiv.org/abs/2403.13468) | DESIRE-ME是一个神经信息检索模型，利用专家混合框架结合多个专业神经模型，通过训练神经门控机制对不同领域的专家预测进行加权，能够自适应地在多个领域中专门化，并在处理开放领域问题中表现出色。 |
| [^6] | [USE: Dynamic User Modeling with Stateful Sequence Models](https://arxiv.org/abs/2403.13344) | 引入了User Stateful Embedding（USE）来解决动态用户建模中存在的挑战，通过存储先前模型状态，生成用户嵌入并反映用户不断发展的行为。 |
| [^7] | [Harnessing Large Language Models for Text-Rich Sequential Recommendation](https://arxiv.org/abs/2403.13325) | 利用大型语言模型，设计了一种新框架用于文本丰富的顺序推荐，通过对用户历史行为分段并采用LLM摘要工具进行总结，从而解决推荐系统中长文本信息带来的挑战 |
| [^8] | [Flickr30K-CFQ: A Compact and Fragmented Query Dataset for Text-image Retrieval](https://arxiv.org/abs/2403.13317) | 提出了一个新的紧凑碎片化查询挑战数据集（Flickr30K-CFQ），模拟考虑多个查询内容和风格的文本图像检索任务，并提出了一种基于LLM的查询增强文本图像检索方法。 |
| [^9] | [A Semantic Search Engine for Mathlib4](https://arxiv.org/abs/2403.13310) | 提出了一个用于Mathlib4的语义搜索引擎，能够接受非正式查询并找到相关定理，为解决在mathlib4中搜索困难问题提供了新的方法。 |
| [^10] | [An Analysis on Matching Mechanisms and Token Pruning for Late-interaction Models](https://arxiv.org/abs/2403.13291) | 系统分析了后交互模型的匹配机制，揭示了最大值操作严重依赖于文档中的共现信号和重要单词，对于稠密检索模型的发展具有重要意义。 |
| [^11] | [Improving Legal Case Retrieval with Brain Signals](https://arxiv.org/abs/2403.13242) | 通过脑信号改进法律案例检索，解决了传统反馈技术难以解决的问题。 |
| [^12] | [When SMILES have Language: Drug Classification using Text Classification Methods on Drug SMILES Strings](https://arxiv.org/abs/2403.12984) | 将药物SMILES字符串视为句子并利用文本分类方法进行药物分类，证实了通过简单的自然语言处理方法解决复杂问题的可能性 |
| [^13] | [ERASE: Benchmarking Feature Selection Methods for Deep Recommender Systems](https://arxiv.org/abs/2403.12660) | 深度推荐系统中的特征选择方法研究面临着公平比较、选择属性分析缺乏以及过度关注峰值性能等挑战。 |
| [^14] | [An Aligning and Training Framework for Multimodal Recommendations](https://arxiv.org/abs/2403.12384) | 提出了一种名为AlignRec的对齐和训练框架，用于解决多模态推荐中的不对齐问题，通过将推荐目标分解为三个对齐部分，实现内容内部对齐、内容与分类ID之间的对齐以及用户和项目之间的对齐。 |
| [^15] | [Evaluating Large Language Models as Generative User Simulators for Conversational Recommendation](https://arxiv.org/abs/2403.09738) | 大型语言模型作为生成式用户模拟器在对话推荐中展现出潜力，新的协议通过五个任务评估了语言模型模拟人类行为的准确程度，揭示了模型与人类行为的偏差，并提出了如何通过模型选择和提示策略减少这些偏差。 |
| [^16] | [Investigating the Effects of Sparse Attention on Cross-Encoders](https://arxiv.org/abs/2312.17649) | 窗口大小非常小甚至只有4个标记时，仍可保持与以往交叉编码器相当的效果，同时降低内存需求并提高推理速度。 |
| [^17] | [LLatrieval: LLM-Verified Retrieval for Verifiable Generation](https://arxiv.org/abs/2311.07838) | 可验证生成中检索的文件不仅帮助LLM生成正确答案，还作为用户验证LLM输出的证据，但目前广泛使用的检索器已成为性能瓶颈，需要解决。 |
| [^18] | [Workload-aware and Learned Z-Indexes.](http://arxiv.org/abs/2310.04268) | 本文提出了一种基于工作负载和学习的Z-索引变体，通过优化存储布局和搜索结构，改善了范围查询性能，并通过引入页面跳跃机制进一步提升查询性能。实验证明，该索引在范围查询时间、点查询性能和构建时间与索引大小之间保持了良好的平衡。 |
| [^19] | [I^3 Retriever: Incorporating Implicit Interaction in Pre-trained Language Models for Passage Retrieval.](http://arxiv.org/abs/2306.02371) | I^3 Retriever将隐式交互纳入预训练语言模型，提高了段落检索的效果和效率。 |

# 详细

[^1]: 利用高分辨率特征改进基于深度哈希的图像检索

    Leveraging High-Resolution Features for Improved Deep Hashing-based Image Retrieval

    [https://arxiv.org/abs/2403.13747](https://arxiv.org/abs/2403.13747)

    探索利用高分辨率特征进行图像检索，提出了一种利用High-Resolution Networks作为骨干的深度哈希网络（HHNet），在各个基准数据集上表现出优越性能。

    

    深度哈希技术已经成为高效图像检索的主要方法。本研究探讨了利用高分辨率特征进行图像检索任务的有效性，并提出了一种利用高分辨率网络（HRNets）作为深度哈希任务骨干的新方法，称为高分辨率哈希网络（HHNet）。我们的方法相较于现有方法在各个基准数据集（包括CIFAR-10，NUS-WIDE，MS COCO和ImageNet）上表现出显著的优越性能。

    arXiv:2403.13747v1 Announce Type: cross  Abstract: Deep hashing techniques have emerged as the predominant approach for efficient image retrieval. Traditionally, these methods utilize pre-trained convolutional neural networks (CNNs) such as AlexNet and VGG-16 as feature extractors. However, the increasing complexity of datasets poses challenges for these backbone architectures in capturing meaningful features essential for effective image retrieval. In this study, we explore the efficacy of employing high-resolution features learned through state-of-the-art techniques for image retrieval tasks. Specifically, we propose a novel methodology that utilizes High-Resolution Networks (HRNets) as the backbone for the deep hashing task, termed High-Resolution Hashing Network (HHNet). Our approach demonstrates superior performance compared to existing methods across all tested benchmark datasets, including CIFAR-10, NUS-WIDE, MS COCO, and ImageNet. This performance improvement is more pronounced
    
[^2]: 不再有优化规则: 基于LLM的基于策略的多模查询优化器（版本1）

    No more optimization rules: LLM-enabled policy-based multi-modal query optimizer (version 1)

    [https://arxiv.org/abs/2403.13597](https://arxiv.org/abs/2403.13597)

    LLM启用了基于策略的多模查询优化器，摆脱了传统的基于规则的优化方法，为查询优化带来全新的可能性。

    

    大语言模型(LLM)在机器学习和深度学习领域标志着一个重要时刻。最近，人们研究了LLM在查询规划中的能力，包括单模和多模查询。然而，对于LLM的查询优化能力还没有相关研究。作为显著影响查询计划执行性能的关键步骤，不应错过这种分析和尝试。另一方面，现有的查询优化器通常是基于规则或基于规则+基于成本的，即它们依赖于人工创建的规则来完成查询计划重写/转换。鉴于现代优化器包括数百至数千条规则，按照类似方式设计一个多模查询优化器将耗费大量时间，因为我们将不得不列举尽可能多的多模优化规则，而这并没有。

    arXiv:2403.13597v1 Announce Type: cross  Abstract: Large language model (LLM) has marked a pivotal moment in the field of machine learning and deep learning. Recently its capability for query planning has been investigated, including both single-modal and multi-modal queries. However, there is no work on the query optimization capability of LLM. As a critical (or could even be the most important) step that significantly impacts the execution performance of the query plan, such analysis and attempts should not be missed. From another aspect, existing query optimizers are usually rule-based or rule-based + cost-based, i.e., they are dependent on manually created rules to complete the query plan rewrite/transformation. Given the fact that modern optimizers include hundreds to thousands of rules, designing a multi-modal query optimizer following a similar way is significantly time-consuming since we will have to enumerate as many multi-modal optimization rules as possible, which has not be
    
[^3]: 一个大型语言模型增强的序列推荐器，用于联合视频和评论推荐

    A Large Language Model Enhanced Sequential Recommender for Joint Video and Comment Recommendation

    [https://arxiv.org/abs/2403.13574](https://arxiv.org/abs/2403.13574)

    提出了一个新颖的推荐方法LSVCR，通过融合用户与视频和评论的交互历史，联合进行个性化视频和评论推荐

    

    在在线视频平台上，阅读或撰写有趣视频的评论已经成为视频观看体验中不可或缺的一部分。然而，现有视频推荐系统主要对用户与视频的交互行为进行建模，缺乏对评论在用户行为建模中的考虑。本文提出了一种名为LSVCR的新颖推荐方法，通过利用用户与视频和评论的交互历史，共同进行个性化视频和评论推荐。具体而言，我们的方法由两个关键组件组成，即序列推荐（SR）模型和补充大型语言模型（LLM）推荐器。SR模型作为我们方法的主要推荐骨干（在部署中保留），可实现高效的用户偏好建模。与此同时，我们利用LLM推荐器作为一个补充组件（在部署中丢弃），以更好地捕捉潜在

    arXiv:2403.13574v1 Announce Type: new  Abstract: In online video platforms, reading or writing comments on interesting videos has become an essential part of the video watching experience. However, existing video recommender systems mainly model users' interaction behaviors with videos, lacking consideration of comments in user behavior modeling. In this paper, we propose a novel recommendation approach called LSVCR by leveraging user interaction histories with both videos and comments, so as to jointly conduct personalized video and comment recommendation. Specifically, our approach consists of two key components, namely sequential recommendation (SR) model and supplemental large language model (LLM) recommender. The SR model serves as the primary recommendation backbone (retained in deployment) of our approach, allowing for efficient user preference modeling. Meanwhile, we leverage the LLM recommender as a supplemental component (discarded in deployment) to better capture underlying 
    
[^4]: 一个统一的基于最优输运的跨模态检索框架，应对带有噪声标签的情况

    A Unified Optimal Transport Framework for Cross-Modal Retrieval with Noisy Labels

    [https://arxiv.org/abs/2403.13480](https://arxiv.org/abs/2403.13480)

    这项工作提出了UOT-RCL，一个基于最优输运的统一框架，用于解决带有噪声标签的跨模态检索中的两个关键挑战，即强制多模态样本对齐不正确的语义以及扩大异质性差距，从而提升检索性能。

    

    跨模态检索（CMR）旨在建立不同模态之间的交互，其中监督式CMR由于在学习语义类别识别上的灵活性而变得越来越受欢迎。尽管先前的监督式CMR方法表现出色，但它们的成功很大程度上归因于良好注释的数据。然而，即使对于单模态数据，精确的注释也是昂贵且耗时的，而在多模态场景下变得更具挑战性。在实践中，从互联网收集了大量具有粗糙注释的多模态数据，这必然导致有噪声的标签。使用这些误导性标签进行训练会带来两个关键挑战--强制多模态样本\emph{对齐不正确的语义}和\emph{扩大异质性差距}，从而导致检索性能不佳。为了解决这些挑战，本文提出了UOT-RCL，一个基于最优输运（OT）的统一框架

    arXiv:2403.13480v1 Announce Type: cross  Abstract: Cross-modal retrieval (CMR) aims to establish interaction between different modalities, among which supervised CMR is emerging due to its flexibility in learning semantic category discrimination. Despite the remarkable performance of previous supervised CMR methods, much of their success can be attributed to the well-annotated data. However, even for unimodal data, precise annotation is expensive and time-consuming, and it becomes more challenging with the multimodal scenario. In practice, massive multimodal data are collected from the Internet with coarse annotation, which inevitably introduces noisy labels. Training with such misleading labels would bring two key challenges -- enforcing the multimodal samples to \emph{align incorrect semantics} and \emph{widen the heterogeneous gap}, resulting in poor retrieval performance. To tackle these challenges, this work proposes UOT-RCL, a Unified framework based on Optimal Transport (OT) for
    
[^5]: DESIRE-ME：使用专家混合模型的增强领域监督信息检索

    DESIRE-ME: Domain-Enhanced Supervised Information REtrieval using Mixture-of-Experts

    [https://arxiv.org/abs/2403.13468](https://arxiv.org/abs/2403.13468)

    DESIRE-ME是一个神经信息检索模型，利用专家混合框架结合多个专业神经模型，通过训练神经门控机制对不同领域的专家预测进行加权，能够自适应地在多个领域中专门化，并在处理开放领域问题中表现出色。

    

    开放领域问题回答需要检索系统能够处理多样化和变化多样的问题，提供准确的答案涵盖广泛的查询类型和主题。为了通过一个独特的模型处理这种主题异质性，我们提出了DESIRE-ME，这是一个神经信息检索模型，利用专家混合框架结合多个专业神经模型。我们依赖于维基百科数据来训练一个有效的神经门控机制，对传入查询进行分类，并相应地加权不同领域专家的预测。这使得DESIRE-ME能够自适应地在多个领域中专门化。通过对公开可用数据集的大量实验，我们展示了我们的提议能够有效地概括领域增强的神经模型。DESIRE-ME在自适应处理开放领域问题方面表现出色，能将NDCG@10提高高达12%，P@1提高高达22%。

    arXiv:2403.13468v1 Announce Type: new  Abstract: Open-domain question answering requires retrieval systems able to cope with the diverse and varied nature of questions, providing accurate answers across a broad spectrum of query types and topics. To deal with such topic heterogeneity through a unique model, we propose DESIRE-ME, a neural information retrieval model that leverages the Mixture-of-Experts framework to combine multiple specialized neural models. We rely on Wikipedia data to train an effective neural gating mechanism that classifies the incoming query and that weighs the predictions of the different domain-specific experts correspondingly. This allows DESIRE-ME to specialize adaptively in multiple domains. Through extensive experiments on publicly available datasets, we show that our proposal can effectively generalize domain-enhanced neural models. DESIRE-ME excels in handling open-domain questions adaptively, boosting by up to 12% in NDCG@10 and 22% in P@1, the underlying
    
[^6]: 使用：带有有状态序列模型的动态用户建模

    USE: Dynamic User Modeling with Stateful Sequence Models

    [https://arxiv.org/abs/2403.13344](https://arxiv.org/abs/2403.13344)

    引入了User Stateful Embedding（USE）来解决动态用户建模中存在的挑战，通过存储先前模型状态，生成用户嵌入并反映用户不断发展的行为。

    

    用户嵌入在用户参与度预测和个性化服务中起着至关重要的作用。序列建模的最新进展引发了从行为数据中学习用户嵌入的兴趣。然而，基于行为的用户嵌入学习面临动态用户建模的独特挑战。随着用户不断与应用程序交互，用户嵌入应定期更新以考虑用户的最近和长期行为模式。现有方法高度依赖于缺乏历史行为记忆的无状态序列模型。它们必须要么丢弃历史数据仅使用最新数据，要么重新处理旧数据和新数据。两种情况均会产生大量计算开销。为解决这一限制，我们引入了用户有状态嵌入（USE）。USE生成用户嵌入并反映用户不断发展的行为，而无需通过存储先前的模型状态来进行详尽的重新处理。

    arXiv:2403.13344v1 Announce Type: cross  Abstract: User embeddings play a crucial role in user engagement forecasting and personalized services. Recent advances in sequence modeling have sparked interest in learning user embeddings from behavioral data. Yet behavior-based user embedding learning faces the unique challenge of dynamic user modeling. As users continuously interact with the apps, user embeddings should be periodically updated to account for users' recent and long-term behavior patterns. Existing methods highly rely on stateless sequence models that lack memory of historical behavior. They have to either discard historical data and use only the most recent data or reprocess the old and new data jointly. Both cases incur substantial computational overhead. To address this limitation, we introduce User Stateful Embedding (USE). USE generates user embeddings and reflects users' evolving behaviors without the need for exhaustive reprocessing by storing previous model states and
    
[^7]: 利用大型语言模型进行文本丰富的顺序推荐

    Harnessing Large Language Models for Text-Rich Sequential Recommendation

    [https://arxiv.org/abs/2403.13325](https://arxiv.org/abs/2403.13325)

    利用大型语言模型，设计了一种新框架用于文本丰富的顺序推荐，通过对用户历史行为分段并采用LLM摘要工具进行总结，从而解决推荐系统中长文本信息带来的挑战

    

    近年来，大型语言模型（LLMs）的最新进展已经改变了推荐系统（RS）的范式。然而，当推荐场景中的物品包含丰富的文本信息，例如在线购物中的产品描述或社交媒体上的新闻标题时，LLMs需要更长的文本才能全面描述历史用户行为序列。这给基于LLM的推荐系统带来了重大挑战，例如长度限制、庞大的时间和空间开销以及子优化的模型性能。为此，在本文中，我们设计了一个新颖的框架，用于利用大型语言模型进行文本丰富的顺序推荐（LLM-TRSR）。具体而言，我们首先提出对用户历史行为进行分段，随后采用基于LLM的摘要工具对这些用户行为块进行总结。特别地，借鉴卷积神经网络（CNN）在成功应用中的灵感

    arXiv:2403.13325v1 Announce Type: new  Abstract: Recent advances in Large Language Models (LLMs) have been changing the paradigm of Recommender Systems (RS). However, when items in the recommendation scenarios contain rich textual information, such as product descriptions in online shopping or news headlines on social media, LLMs require longer texts to comprehensively depict the historical user behavior sequence. This poses significant challenges to LLM-based recommenders, such as over-length limitations, extensive time and space overheads, and suboptimal model performance. To this end, in this paper, we design a novel framework for harnessing Large Language Models for Text-Rich Sequential Recommendation (LLM-TRSR). Specifically, we first propose to segment the user historical behaviors and subsequently employ an LLM-based summarizer for summarizing these user behavior blocks. Particularly, drawing inspiration from the successful application of Convolutional Neural Networks (CNN) and 
    
[^8]: Flickr30K-CFQ：用于文本图像检索的紧凑碎片化查询数据集

    Flickr30K-CFQ: A Compact and Fragmented Query Dataset for Text-image Retrieval

    [https://arxiv.org/abs/2403.13317](https://arxiv.org/abs/2403.13317)

    提出了一个新的紧凑碎片化查询挑战数据集（Flickr30K-CFQ），模拟考虑多个查询内容和风格的文本图像检索任务，并提出了一种基于LLM的查询增强文本图像检索方法。

    

    随着互联网上多模态信息的爆炸性增长，单模态搜索无法满足互联网应用的需求。需要进行文本图像检索研究，实现不同模态之间高质量高效的检索。现有的文本图像检索研究大多基于通用的视觉-语言数据集（如MS-COCO、Flickr30K），其中查询话语刻板而不自然（即冗长和过于正式）。为了克服这一缺点，我们构建了一个新的紧凑碎片化查询挑战数据集（名为Flickr30K-CFQ），以建模考虑多个查询内容和风格的文本图像检索任务，包括紧凑和细粒度的实体关系语料库。我们提出了一种基于LLM的新型查询增强文本图像检索方法。实验证明，我们提出的Flickr30-CFQ揭示了现有视觉-语言数据集在现实文本图像检索任务中的不足之处。

    arXiv:2403.13317v1 Announce Type: new  Abstract: With the explosive growth of multi-modal information on the Internet, unimodal search cannot satisfy the requirement of Internet applications. Text-image retrieval research is needed to realize high-quality and efficient retrieval between different modalities. Existing text-image retrieval research is mostly based on general vision-language datasets (e.g. MS-COCO, Flickr30K), in which the query utterance is rigid and unnatural (i.e. verbosity and formality). To overcome the shortcoming, we construct a new Compact and Fragmented Query challenge dataset (named Flickr30K-CFQ) to model text-image retrieval task considering multiple query content and style, including compact and fine-grained entity-relation corpus. We propose a novel query-enhanced text-image retrieval method using prompt engineering based on LLM. Experiments show that our proposed Flickr30-CFQ reveals the insufficiency of existing vision-language datasets in realistic text-i
    
[^9]: 一个用于Mathlib4的语义搜索引擎

    A Semantic Search Engine for Mathlib4

    [https://arxiv.org/abs/2403.13310](https://arxiv.org/abs/2403.13310)

    提出了一个用于Mathlib4的语义搜索引擎，能够接受非正式查询并找到相关定理，为解决在mathlib4中搜索困难问题提供了新的方法。

    

    交互式定理证明器Lean使得可以验证正式数学证明，并且得到一个不断扩大的社区的支持。该生态系统的核心是其数学库mathlib4，为扩展范围的数学理论的形式化奠定了基础。然而，在mathlib4中搜索定理可能具有挑战性。为了成功在mathlib4中搜索，用户通常需要熟悉其命名约定或文档字符串。因此，创建一个语义搜索引擎，可以方便地被具有不同熟悉程度的mathlib4的个人使用是非常重要的。在本文中，我们提出了一个用于mathlib4的语义搜索引擎，可以接受非正式查询并找到相关定理。我们还建立了一个用于评估各种mathlib4搜索引擎性能的基准。

    arXiv:2403.13310v1 Announce Type: cross  Abstract: The interactive theorem prover, Lean, enables the verification of formal mathematical proofs and is backed by an expanding community. Central to this ecosystem is its mathematical library, mathlib4, which lays the groundwork for the formalization of an expanding range of mathematical theories. However, searching for theorems in mathlib4 can be challenging. To successfully search in mathlib4, users often need to be familiar with its naming conventions or documentation strings. Therefore, creating a semantic search engine that can be used easily by individuals with varying familiarity with mathlib4 is very important. In this paper, we present a semantic search engine for mathlib4 that accepts informal queries and finds the relevant theorems. We also establish a benchmark for assessing the performance of various search engines for mathlib4.
    
[^10]: 关于后交互模型的匹配机制和标记修剪的分析

    An Analysis on Matching Mechanisms and Token Pruning for Late-interaction Models

    [https://arxiv.org/abs/2403.13291](https://arxiv.org/abs/2403.13291)

    系统分析了后交互模型的匹配机制，揭示了最大值操作严重依赖于文档中的共现信号和重要单词，对于稠密检索模型的发展具有重要意义。

    

    随着预训练语言模型的发展，稠密检索模型已成为传统依赖精确匹配和稀疏词袋表示的检索模型的有希望的替代方案。与大多数稠密检索模型使用双编码器将每个查询或文档编码为稠密向量不同，最近提出的后交互多向量模型（例如ColBERT和COIL）通过使用所有标记嵌入来表示文档和查询，并通过使用最大操作的和来建模它们的相关性，实现了最先进的检索效果。然而，这些细粒度表示可能会导致实际搜索系统无法接受的存储开销。在本研究中，我们系统分析了这些后交互模型的匹配机制，并展示了最大值操作严重依赖于共现信号和文档中的一些重要单词。基于这些发现，

    arXiv:2403.13291v1 Announce Type: new  Abstract: With the development of pre-trained language models, the dense retrieval models have become promising alternatives to the traditional retrieval models that rely on exact match and sparse bag-of-words representations. Different from most dense retrieval models using a bi-encoder to encode each query or document into a dense vector, the recently proposed late-interaction multi-vector models (i.e., ColBERT and COIL) achieve state-of-the-art retrieval effectiveness by using all token embeddings to represent documents and queries and modeling their relevance with a sum-of-max operation. However, these fine-grained representations may cause unacceptable storage overhead for practical search systems. In this study, we systematically analyze the matching mechanism of these late-interaction models and show that the sum-of-max operation heavily relies on the co-occurrence signals and some important words in the document. Based on these findings, w
    
[^11]: 用脑信号改进法律案例检索

    Improving Legal Case Retrieval with Brain Signals

    [https://arxiv.org/abs/2403.13242](https://arxiv.org/abs/2403.13242)

    通过脑信号改进法律案例检索，解决了传统反馈技术难以解决的问题。

    

    arXiv:2403.13242v1 公告类型：新摘要：在过去的十年中，法律案例检索的任务受到了信息检索领域的越来越多的关注。具有隐式用户反馈（例如点击）的相关反馈技术已被证明在传统搜索任务（例如网络搜索）中是有效的。然而，在法律案例检索中，收集相关反馈面临一些难以在现有反馈范式下解决的挑战。首先，法律案例检索是一个复杂的任务，因为用户通常需要详细了解法律案例之间的关系才能正确判断它们的相关性。传统的反馈信号（例如点击）太粗糙了，因为它们不反映任何细粒度的相关信息。其次，法律案例文件通常很长，用户通常需要甚至十几分钟来阅读和理解它们。当用户几乎点击和检查时，简单的行为信号（例如点击和注视跟踪）几乎没有用处。

    arXiv:2403.13242v1 Announce Type: new  Abstract: The tasks of legal case retrieval have received growing attention from the IR community in the last decade. Relevance feedback techniques with implicit user feedback (e.g., clicks) have been demonstrated to be effective in traditional search tasks (e.g., Web search). In legal case retrieval, however, collecting relevance feedback faces a couple of challenges that are difficult to resolve under existing feedback paradigms. First, legal case retrieval is a complex task as users often need to understand the relationship between legal cases in detail to correctly judge their relevance. Traditional feedback signal such as clicks is too coarse to use as they do not reflect any fine-grained relevance information. Second, legal case documents are usually long, users often need even tens of minutes to read and understand them. Simple behavior signal such as clicks and eye-tracking fixations can hardly be useful when users almost click and examine
    
[^12]: 当SMILES拥有语言：使用文本分类方法对药物SMILES字符串进行药物分类

    When SMILES have Language: Drug Classification using Text Classification Methods on Drug SMILES Strings

    [https://arxiv.org/abs/2403.12984](https://arxiv.org/abs/2403.12984)

    将药物SMILES字符串视为句子并利用文本分类方法进行药物分类，证实了通过简单的自然语言处理方法解决复杂问题的可能性

    

    复杂的化学结构，如药物，通常由SMILES字符串来定义，作为分子和键的序列。这些SMILES字符串在不同的基于机器学习的药物相关研究和表示工作中使用。在这项工作中，我们摆脱复杂的表示法，提出了一个问题：如果我们将药物SMILES视为常规句子，并进行文本分类以进行药物分类会怎样？我们的实验证实了这种可能性，获得了非常有竞争力的分数。该研究探讨了将每个原子和键视为句子组件的概念，利用基本的自然语言处理方法对药物类型进行分类，表明复杂的问题也可以用更简单的视角来解决。数据和代码可在此处找到：https://github.com/azminewasi/Drug-Classification-NLP。

    arXiv:2403.12984v1 Announce Type: cross  Abstract: Complex chemical structures, like drugs, are usually defined by SMILES strings as a sequence of molecules and bonds. These SMILES strings are used in different complex machine learning-based drug-related research and representation works. Escaping from complex representation, in this work, we pose a single question: What if we treat drug SMILES as conventional sentences and engage in text classification for drug classification? Our experiments affirm the possibility with very competitive scores. The study explores the notion of viewing each atom and bond as sentence components, employing basic NLP methods to categorize drug types, proving that complex problems can also be solved with simpler perspectives. The data and code are available here: https://github.com/azminewasi/Drug-Classification-NLP.
    
[^13]: ERASE：深度推荐系统特征选择方法的基准测试

    ERASE: Benchmarking Feature Selection Methods for Deep Recommender Systems

    [https://arxiv.org/abs/2403.12660](https://arxiv.org/abs/2403.12660)

    深度推荐系统中的特征选择方法研究面临着公平比较、选择属性分析缺乏以及过度关注峰值性能等挑战。

    

    深度推荐系统(DRS)越来越依赖于大量特征字段来提供更精准的推荐。有效的特征选择方法因此变得至关重要，以进一步提高准确性并优化存储效率，以满足部署需求。研究领域，特别是在DRS的背景下，尚处于初期阶段，面临三个核心挑战：首先，研究论文之间实验设置的差异往往导致不公平比较，遮蔽了实践见解。其次，现有文献缺乏基于大规模数据集的选择属性的详细分析，并且缺乏对选择技术和DRS骨干之间进行全面比较的限制性文章的通用性研究和部署。最后，研究往往专注于比较特征选择方法可达到的峰值性能，这种方法通常在计算方面不足。

    arXiv:2403.12660v1 Announce Type: cross  Abstract: Deep Recommender Systems (DRS) are increasingly dependent on a large number of feature fields for more precise recommendations. Effective feature selection methods are consequently becoming critical for further enhancing the accuracy and optimizing storage efficiencies to align with the deployment demands. This research area, particularly in the context of DRS, is nascent and faces three core challenges. Firstly, variant experimental setups across research papers often yield unfair comparisons, obscuring practical insights. Secondly, the existing literature's lack of detailed analysis on selection attributes, based on large-scale datasets and a thorough comparison among selection techniques and DRS backbones, restricts the generalizability of findings and impedes deployment on DRS. Lastly, research often focuses on comparing the peak performance achievable by feature selection methods, an approach that is typically computationally infe
    
[^14]: 一种用于多模态推荐的对齐和训练框架

    An Aligning and Training Framework for Multimodal Recommendations

    [https://arxiv.org/abs/2403.12384](https://arxiv.org/abs/2403.12384)

    提出了一种名为AlignRec的对齐和训练框架，用于解决多模态推荐中的不对齐问题，通过将推荐目标分解为三个对齐部分，实现内容内部对齐、内容与分类ID之间的对齐以及用户和项目之间的对齐。

    

    随着多媒体应用的发展，多模态推荐正在发挥着重要作用，因为它们可以利用超越用户交互的丰富上下文。现有方法主要将多模态信息视为辅助，用于帮助学习ID特征；然而，多模态内容特征和ID特征之间存在语义差距，直接将多模态信息作为辅助使用会导致用户和项目表示的不对齐。本文首先系统地研究了多模态推荐中的不对齐问题，并提出了一种名为AlignRec的解决方案。在AlignRec中，推荐目标被分解为三个对齐部分，即内容内部对齐，内容与分类ID之间的对齐以及用户和项目之间的对齐。每个对齐部分都由特定的目标函数来表征，并整合到我们的多模态推荐中。

    arXiv:2403.12384v1 Announce Type: cross  Abstract: With the development of multimedia applications, multimodal recommendations are playing an essential role, as they can leverage rich contexts beyond user interactions. Existing methods mainly regard multimodal information as an auxiliary, using them to help learn ID features; however, there exist semantic gaps among multimodal content features and ID features, for which directly using multimodal information as an auxiliary would lead to misalignment in representations of users and items. In this paper, we first systematically investigate the misalignment issue in multimodal recommendations, and propose a solution named AlignRec. In AlignRec, the recommendation objective is decomposed into three alignments, namely alignment within contents, alignment between content and categorical ID, and alignment between users and items. Each alignment is characterized by a specific objective function and is integrated into our multimodal recommendat
    
[^15]: 评估大语言模型作为对话推荐中生成用户模拟器

    Evaluating Large Language Models as Generative User Simulators for Conversational Recommendation

    [https://arxiv.org/abs/2403.09738](https://arxiv.org/abs/2403.09738)

    大型语言模型作为生成式用户模拟器在对话推荐中展现出潜力，新的协议通过五个任务评估了语言模型模拟人类行为的准确程度，揭示了模型与人类行为的偏差，并提出了如何通过模型选择和提示策略减少这些偏差。

    

    合成用户是对话推荐系统评估中成本效益较高的真实用户代理。大型语言模型表现出在模拟类似人类行为方面的潜力，这引发了它们能否代表多样化用户群体的问题。我们引入了一个新的协议，用于衡量语言模型能够准确模拟对话推荐中人类行为的程度。该协议由五个任务组成，每个任务旨在评估合成用户应该表现出的关键特性：选择要谈论的物品，表达二进制偏好，表达开放式偏好，请求推荐以及提供反馈。通过对基准模拟器的评估，我们展示了这些任务有效地揭示了语言模型与人类行为的偏差，并提供了关于如何通过模型选择和提示策略减少这些偏差的见解。

    arXiv:2403.09738v1 Announce Type: cross  Abstract: Synthetic users are cost-effective proxies for real users in the evaluation of conversational recommender systems. Large language models show promise in simulating human-like behavior, raising the question of their ability to represent a diverse population of users. We introduce a new protocol to measure the degree to which language models can accurately emulate human behavior in conversational recommendation. This protocol is comprised of five tasks, each designed to evaluate a key property that a synthetic user should exhibit: choosing which items to talk about, expressing binary preferences, expressing open-ended preferences, requesting recommendations, and giving feedback. Through evaluation of baseline simulators, we demonstrate these tasks effectively reveal deviations of language models from human behavior, and offer insights on how to reduce the deviations with model selection and prompting strategies.
    
[^16]: 研究稀疏注意力对交叉编码器的影响

    Investigating the Effects of Sparse Attention on Cross-Encoders

    [https://arxiv.org/abs/2312.17649](https://arxiv.org/abs/2312.17649)

    窗口大小非常小甚至只有4个标记时，仍可保持与以往交叉编码器相当的效果，同时降低内存需求并提高推理速度。

    

    交叉编码器是有效的段落和文档重新排序器，但效率不如其他神经或经典检索模型。一些先前的研究应用窗口自注意力来使交叉编码器更有效率。然而，这些研究并未探讨不同注意力模式或窗口大小的潜力和限制。我们填补了这一空白，并系统地分析了如何减少标记交互而不损害重新排序的有效性。通过实验使用非对称注意力和不同的窗口大小，我们发现查询标记不需要关注段落或文档标记也能实现有效的重新排序，而非常小的窗口大小就足够了。在我们的实验中，即使是4个标记的窗口仍然能够达到与以前的交叉编码器相当的有效性，同时将内存要求降低至少22% / 59%，并在 passages / documents 的推理时间上快1% / 43%。

    arXiv:2312.17649v2 Announce Type: replace  Abstract: Cross-encoders are effective passage and document re-rankers but less efficient than other neural or classic retrieval models. A few previous studies have applied windowed self-attention to make cross-encoders more efficient. However, these studies did not investigate the potential and limits of different attention patterns or window sizes. We close this gap and systematically analyze how token interactions can be reduced without harming the re-ranking effectiveness. Experimenting with asymmetric attention and different window sizes, we find that the query tokens do not need to attend to the passage or document tokens for effective re-ranking and that very small window sizes suffice. In our experiments, even windows of 4 tokens still yield effectiveness on par with previous cross-encoders while reducing the memory requirements by at least 22% / 59% and being 1% / 43% faster at inference time for passages / documents.
    
[^17]: LLatrieval：LLM-验证检索用于可验证生成

    LLatrieval: LLM-Verified Retrieval for Verifiable Generation

    [https://arxiv.org/abs/2311.07838](https://arxiv.org/abs/2311.07838)

    可验证生成中检索的文件不仅帮助LLM生成正确答案，还作为用户验证LLM输出的证据，但目前广泛使用的检索器已成为性能瓶颈，需要解决。

    

    可验证生成旨在使大型语言模型（LLM）生成具有支撑文件的文本，这使用户能够灵活验证答案，并使LLM的输出更可靠。检索在可验证生成中起着至关重要的作用。具体而言，检索到的文件不仅补充知识以帮助LLM生成正确答案，还作为支持用户验证LLM输出的证据。然而，广泛使用的检索器成为整个流程的瓶颈，并限制了整体性能。由于通常具有的参数比大型语言模型少得多，并且尚未证明能够良好地扩展到LLM的规模，因此它们的能力通常比LLMs差。如果检索器未能正确找到支持文件，则LLM将无法生成正确和可验证的答案，这会掩盖LLM的显著能力。为解决这些问

    arXiv:2311.07838v2 Announce Type: replace  Abstract: Verifiable generation aims to let the large language model (LLM) generate text with supporting documents, which enables the user to flexibly verify the answer and makes the LLM's output more reliable. Retrieval plays a crucial role in verifiable generation. Specifically, the retrieved documents not only supplement knowledge to help the LLM generate correct answers, but also serve as supporting evidence for the user to verify the LLM's output. However, the widely used retrievers become the bottleneck of the entire pipeline and limit the overall performance. Their capabilities are usually inferior to LLMs since they often have much fewer parameters than the large language model and have not been demonstrated to scale well to the size of LLMs. If the retriever does not correctly find the supporting documents, the LLM can not generate the correct and verifiable answer, which overshadows the LLM's remarkable abilities. To address these li
    
[^18]: 基于工作负载和学习的Z-索引

    Workload-aware and Learned Z-Indexes. (arXiv:2310.04268v1 [cs.DB])

    [http://arxiv.org/abs/2310.04268](http://arxiv.org/abs/2310.04268)

    本文提出了一种基于工作负载和学习的Z-索引变体，通过优化存储布局和搜索结构，改善了范围查询性能，并通过引入页面跳跃机制进一步提升查询性能。实验证明，该索引在范围查询时间、点查询性能和构建时间与索引大小之间保持了良好的平衡。

    

    本文提出了一种基于工作负载和学习的Z-索引的变体，该索引同时优化存储布局和搜索结构，作为解决空间索引的挑战的可行解决方案。具体来说，我们首先制定了一个成本函数，用于衡量Z-索引在数据集上的范围查询工作负载下的性能。然后，通过自适应分区和排序优化Z-索引结构，最小化成本函数。此外，我们设计了一种新颖的页面跳跃机制，通过减少对无关数据页面的访问来改善查询性能。我们广泛的实验证明，相比基线，我们的索引平均改善了40%的范围查询时间，同时始终表现得更好或与最先进的空间索引相当。此外，我们的索引在提供有利的构建时间和索引大小权衡的同时，保持良好的点查询性能。

    In this paper, a learned and workload-aware variant of a Z-index, which jointly optimizes storage layout and search structures, as a viable solution for the above challenges of spatial indexing. Specifically, we first formulate a cost function to measure the performance of a Z-index on a dataset for a range-query workload. Then, we optimize the Z-index structure by minimizing the cost function through adaptive partitioning and ordering for index construction. Moreover, we design a novel page-skipping mechanism to improve its query performance by reducing access to irrelevant data pages. Our extensive experiments show that our index improves range query time by 40% on average over the baselines, while always performing better or comparably to state-of-the-art spatial indexes. Additionally, our index maintains good point query performance while providing favourable construction time and index size tradeoffs.
    
[^19]: I^3 Retriever: 将隐式交互纳入预训练语言模型以用于段落检索

    I^3 Retriever: Incorporating Implicit Interaction in Pre-trained Language Models for Passage Retrieval. (arXiv:2306.02371v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2306.02371](http://arxiv.org/abs/2306.02371)

    I^3 Retriever将隐式交互纳入预训练语言模型，提高了段落检索的效果和效率。

    

    段落检索是许多信息系统中的基本任务，例如网络搜索和问答系统，其中效率和效果都是关键问题。近年来，基于预训练语言模型（PLM）的神经检索器，如双编码器，取得了巨大成功。然而，研究发现，双编码器的性能通常受限于忽略查询和候选段落之间的交互信息。因此，提出了各种交互范式来改善纯双编码器的性能。特别是，最近的最先进方法通常在模型推理过程中引入后期交互。然而，这种基于后期交互的方法通常会对大语料库产生大量的计算和存储成本。尽管它们有效，但效率和空间占用的问题仍然是限制交互式神经检索模型应用的重要因素。

    Passage retrieval is a fundamental task in many information systems, such as web search and question answering, where both efficiency and effectiveness are critical concerns. In recent years, neural retrievers based on pre-trained language models (PLM), such as dual-encoders, have achieved huge success. Yet, studies have found that the performance of dual-encoders are often limited due to the neglecting of the interaction information between queries and candidate passages. Therefore, various interaction paradigms have been proposed to improve the performance of vanilla dual-encoders. Particularly, recent state-of-the-art methods often introduce late-interaction during the model inference process. However, such late-interaction based methods usually bring extensive computation and storage cost on large corpus. Despite their effectiveness, the concern of efficiency and space footprint is still an important factor that limits the application of interaction-based neural retrieval models. T
    

