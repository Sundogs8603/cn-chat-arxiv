# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Reinforcement Learning-based Recommender Systems with Large Language Models for State Reward and Action Modeling](https://arxiv.org/abs/2403.16948) | 本文利用大型语言模型(LLM)作为环境(LE)优化了基于强化学习的推荐系统，提高了状态建模和奖励设置的准确性。 |
| [^2] | [Coarse-Tuning for Ad-hoc Document Retrieval Using Pre-trained Language Models](https://arxiv.org/abs/2403.16915) | 本研究引入了粗调优作为一个中间学习阶段，连接了预训练和微调，在专题文档检索中显著改善了效果。 |
| [^3] | [GloSIS: The Global Soil Information System Web Ontology](https://arxiv.org/abs/2403.16778) | GSP提出了全球土壤信息系统(GloSIS)，旨在通过数据协调和交流促进全球可持续土地管理实践，并将语义Web作为其操作化的方向。 |
| [^4] | [ProCQA: A Large-scale Community-based Programming Question Answering Dataset for Code Search](https://arxiv.org/abs/2403.16702) | ProCQA数据集是从StackOverflow社区提取的，为编程问题回答提供了自然结构化的混合模态问答对，并引入了一种模态-不可知的对比预训练方法，显著提高了代码语言模型的性能。 |
| [^5] | [Graph Augmentation for Recommendation](https://arxiv.org/abs/2403.16656) | 提出了一个名为GraphAug的框架，通过引入强大的数据增强程序生成去噪的自监督信号，解决了推荐系统中对比学习中存在的数据噪声和GNN架构平滑问题。 |
| [^6] | [A comparative analysis of embedding models for patent similarity](https://arxiv.org/abs/2403.16630) | 本文比较了不同类型的专利嵌入模型在专利相似性计算任务上的表现，并具体探讨了Sentence Transformers (SBERT) 架构在专利相似性任务中的性能。 |
| [^7] | [LARA: Linguistic-Adaptive Retrieval-Augmented LLMs for Multi-Turn Intent Classification](https://arxiv.org/abs/2403.16504) | LARA是一个Linguistic-Adaptive Retrieval-Augmented Language Models（语言自适应检索增强LLMs），旨在通过结合微调过的较小模型与检索增强机制来提高多语言多轮意图分类任务的准确性，从而改善对话背景的理解。 |
| [^8] | [InstUPR : Instruction-based Unsupervised Passage Reranking with Large Language Models](https://arxiv.org/abs/2403.16435) | InstUPR是一种基于大型语言模型的无监督段落重新排序方法，利用了LLMs的指令跟踪能力，无需额外微调，通过软得分聚合技术和成对重新排序，在BEIR基准测试中表现优秀。 |
| [^9] | [An Experiment with the Use of ChatGPT for LCSH Subject Assignment on Electronic Theses and Dissertations](https://arxiv.org/abs/2403.16424) | 该研究探讨了利用大型语言模型生成美国国会图书馆主题标头的潜力，展示了其对于解决学术图书馆待编目项目积压问题具有战略应对意义，同时也强调了人类编目员仍然在验证和增强生成主题标头方面的重要性。 |
| [^10] | [Play to Your Strengths: Collaborative Intelligence of Conventional Recommender Models and Large Language Models](https://arxiv.org/abs/2403.16378) | LLMs在CRM表现信心和精确性较低的数据段表现优异，而CRM表现优异的样本对LLM而言相对具有挑战性，这表明LLM和CRM之间存在潜在的协同作用。 |
| [^11] | [Uncovering Selective State Space Model's Capabilities in Lifelong Sequential Recommendation](https://arxiv.org/abs/2403.16371) | 研究人员调查了具有选择机制的Mamba在终身序列推荐中的性能，通过有选择地利用Mamba块来模拟用户终身序列，以解决现有模型难以处理终身序列的挑战。 |
| [^12] | [Enhanced Facet Generation with LLM Editing](https://arxiv.org/abs/2403.16345) | 提出了一种通过利用搜索引擎获取的文档和相关查询来增强分面预测的策略，并提出了专注于仅使用查询作为输入来预测分面的框架 |
| [^13] | [Ultra Low-Cost Two-Stage Multimodal System for Non-Normative Behavior Detection](https://arxiv.org/abs/2403.16151) | 提出了一种超低成本两阶段多模式系统，用于以高精确度和召回率识别有害评论和图像，通过CLIP-ViT模型将推文和图像转换为嵌入，并利用常规机器学习分类器进行快速训练和低成本推理。 |
| [^14] | [Complementary Recommendation in E-commerce: Definition, Approaches, and Future Directions](https://arxiv.org/abs/2403.16135) | 本文全面总结和比较了电子商务领域中34项代表性互补推荐研究，包括建模产品之间的互补关系，不同研究问题下的模型分类与比较，以及在同一数据集上进行的实验结果分析。 |
| [^15] | [RankingSHAP -- Listwise Feature Attribution Explanations for Ranking Models](https://arxiv.org/abs/2403.16085) | 本文针对排名模型的特征归因进行了严格定义，并提出了RankingSHAP作为一种逐项排名归因方法，突破了当前解释评估方案的局限，提出了两种新的评估范式。 |
| [^16] | [Knowledge-aware Dual-side Attribute-enhanced Recommendation](https://arxiv.org/abs/2403.16037) | 提出了一种名为“知识感知的双侧属性增强推荐”（KDAR）的方法，通过建模用户偏好和利用偏好-属性连接来增强协同过滤推荐性能 |
| [^17] | [Model, Analyze, and Comprehend User Interactions and Various Attributes within a Social Media Platform](https://arxiv.org/abs/2403.15937) | 本研究提出了一种新颖的基于图的方法，用于模型化和分析社交媒体平台内用户互动，揭示了活跃用户之间的连接模式、对社区动态的贡献率以及用户活动与受欢迎程度之间的相关性。 |
| [^18] | [Towards Human-Like Machine Comprehension: Few-Shot Relational Learning in Visually-Rich Documents](https://arxiv.org/abs/2403.15765) | 该研究聚焦于在视觉丰富文档中进行少样本关系学习，引入了基于现有监督基准数据集构建的两个新的少样本基准，提出了一种包含关系二维空间先验和样本矫正的变分方法 |
| [^19] | [User-Side Realization](https://arxiv.org/abs/2403.15757) | 用户端实现为用户提供了积极的解决方案，通过在用户端运行通用算法来解决常见问题，无需服务提供商改变服务本身。 |
| [^20] | [QueryExplorer: An Interactive Query Generation Assistant for Search and Exploration](https://arxiv.org/abs/2403.15667) | QueryExplorer提出了一个交互式查询生成、重新构造和检索界面，支持多种方法帮助用户创建和修改有效查询，以解决用户缺乏领域专业知识或语言能力时的搜索困难。 |
| [^21] | [Spectral Initialization for High-Dimensional Phase Retrieval with Biased Spatial Directions](https://arxiv.org/abs/2403.15548) | 引入偏向空间方向的方法在高维相位恢复中取得了显著改进，在测量次数少于信号维数时尤为有效，同时揭示了一个依赖于样本大小与信号维数比率的相位转换现象。 |
| [^22] | [GTC: GNN-Transformer Co-contrastive Learning for Self-supervised Heterogeneous Graph Representation](https://arxiv.org/abs/2403.15520) | 该论文提出了一个用于GNN和Transformer的合作学习方案，并构建了GTC架构，通过整合GNN的本地信息聚合和Transformer的全局信息建模能力，解决了过度平滑问题。 |
| [^23] | [Loops On Retrieval Augmented Generation (LoRAG)](https://arxiv.org/abs/2403.15450) | LoRAG是一种新框架，通过引入迭代循环机制提高了检索增强型文本生成的质量，在实验证明在生成的文本连贯性和相关性方面优于当前最先进模型。 |
| [^24] | [No more optimization rules: LLM-enabled policy-based multi-modal query optimizer (version 1)](https://arxiv.org/abs/2403.13597) | LLM启用了基于策略的多模查询优化器，摆脱了传统的基于规则的优化方法，为查询优化带来全新的可能性。 |
| [^25] | [NoteLLM: A Retrievable Large Language Model for Note Recommendation](https://arxiv.org/abs/2403.01744) | 本文提出了一种名为NoteLLM的新颖统一框架，利用大型语言模型(LLMs)来实现物品到物品(I2I)的笔记推荐，通过学习生成哈希标签/类别潜在地增强笔记嵌入，提高了对关键笔记信息的压缩。 |
| [^26] | [Word4Per: Zero-shot Composed Person Retrieval](https://arxiv.org/abs/2311.16515) | 提出了一个新任务：组合人员检索（CPR），旨在联合利用图像和文本信息进行目标人员检索，引入零样本组合人员检索（ZS-CPR）解决了CPR问题，提出了一个两阶段学习框架Word4Per。 |
| [^27] | [Large Language Models for Generative Recommendation: A Survey and Visionary Discussions](https://arxiv.org/abs/2309.01157) | 大型语言模型为推荐系统的生成式推荐提供了新机遇，可以简化推荐流程并直接从完整的项目池中生成推荐。 |
| [^28] | [On the resilience of Collaborative Learning-based Recommender Systems Against Community Detection Attack](https://arxiv.org/abs/2306.08929) | 本文研究了协作学习推荐系统针对一种新型隐私攻击——社区检测攻击（CDA）的韧性。 |
| [^29] | [Spacerini: Plug-and-play Search Engines with Pyserini and Hugging Face](https://arxiv.org/abs/2302.14534) | Spacerini是一个集成了Pyserini和Hugging Face的工具，可以无缝构建和部署交互式搜索引擎，使得非IR从业者可以更轻松地使用最先进的检索模型，对NLP和IR研究人员以及第三方复制研究工作都非常有用。 |
| [^30] | [In-context Learning with Retrieved Demonstrations for Language Models: A Survey.](http://arxiv.org/abs/2401.11624) | 本综述调查了一种名为检索示范的方法，它通过使用特定于输入查询的示范来提高语言模型的少量样本情境学习（ICL）能力。这种方法不仅提高了学习效率和可扩展性，还减少了手动示例选择中的偏见。 |
| [^31] | [Our Model Achieves Excellent Performance on MovieLens: What Does it Mean?.](http://arxiv.org/abs/2307.09985) | 该论文通过对MovieLens数据集的分析，发现用户与该平台的交互在不同阶段存在显著差异，并且用户交互受到平台推荐算法推荐的候选电影的影响。 |

# 详细

[^1]: 利用大语言模型进行基于强化学习的推荐系统，用于状态、奖励和动作建模

    Reinforcement Learning-based Recommender Systems with Large Language Models for State Reward and Action Modeling

    [https://arxiv.org/abs/2403.16948](https://arxiv.org/abs/2403.16948)

    本文利用大型语言模型(LLM)作为环境(LE)优化了基于强化学习的推荐系统，提高了状态建模和奖励设置的准确性。

    

    基于强化学习的推荐系统在从历史用户-物品交互中学习准确的下一个物品推荐方面表现出色，但现有的离线强化学习方法在获取有效用户反馈方面面临挑战。本文利用语言理解能力，将大型语言模型(LLM)调整为环境(LE)以增强基于强化学习的推荐系统，从而减少了对大量训练数据的需求。

    arXiv:2403.16948v1 Announce Type: new  Abstract: Reinforcement Learning (RL)-based recommender systems have demonstrated promising performance in meeting user expectations by learning to make accurate next-item recommendations from historical user-item interactions. However, existing offline RL-based sequential recommendation methods face the challenge of obtaining effective user feedback from the environment. Effectively modeling the user state and shaping an appropriate reward for recommendation remains a challenge. In this paper, we leverage language understanding capabilities and adapt large language models (LLMs) as an environment (LE) to enhance RL-based recommenders. The LE is learned from a subset of user-item interaction data, thus reducing the need for large training data, and can synthesise user feedback for offline data by: (i) acting as a state model that produces high quality states that enrich the user representation, and (ii) functioning as a reward model to accurately 
    
[^2]: 利用预训练语言模型进行粗调优的专题文档检索

    Coarse-Tuning for Ad-hoc Document Retrieval Using Pre-trained Language Models

    [https://arxiv.org/abs/2403.16915](https://arxiv.org/abs/2403.16915)

    本研究引入了粗调优作为一个中间学习阶段，连接了预训练和微调，在专题文档检索中显著改善了效果。

    

    在信息检索系统中，利用预训练语言模型（PLM-based IR）进行微调需要学习查询表示和查询-文档关系，除了下游任务特定的学习。本研究引入了粗调优作为一个中间学习阶段，连接了预训练和微调。通过在粗调优学习查询表示和查询-文档关系，我们旨在减少微调的负担，提高下游IR任务的学习效果。我们提出了用于粗调优的查询-文档对预测（QDPP），其预测查询-文档对的适当性。评估实验显示，所提出的方法显著改善了四个专题文档检索数据集中的MRR和/或nDCG@5。此外，查询预测任务的结果表明，粗调优促进了查询表示和查询-文档关系的学习。

    arXiv:2403.16915v1 Announce Type: cross  Abstract: Fine-tuning in information retrieval systems using pre-trained language models (PLM-based IR) requires learning query representations and query-document relations, in addition to downstream task-specific learning. This study introduces coarse-tuning as an intermediate learning stage that bridges pre-training and fine-tuning. By learning query representations and query-document relations in coarse-tuning, we aim to reduce the load of fine-tuning and improve the learning effect of downstream IR tasks. We propose Query-Document Pair Prediction (QDPP) for coarse-tuning, which predicts the appropriateness of query-document pairs. Evaluation experiments show that the proposed method significantly improves MRR and/or nDCG@5 in four ad-hoc document retrieval datasets. Furthermore, the results of the query prediction task suggested that coarse-tuning facilitated learning of query representation and query-document relations.
    
[^3]: GloSIS：全球土壤信息系统Web本体论

    GloSIS: The Global Soil Information System Web Ontology

    [https://arxiv.org/abs/2403.16778](https://arxiv.org/abs/2403.16778)

    GSP提出了全球土壤信息系统(GloSIS)，旨在通过数据协调和交流促进全球可持续土地管理实践，并将语义Web作为其操作化的方向。

    

    由联合国粮食和农业组织(FAO)成员成立于2012年的全球土壤伙伴关系(GSP)是一个全球利益相关者网络，旨在促进合理的土地和土壤管理实践，实现可持续的世界粮食系统。然而，土壤调查仍然主要是一项地方或区域性活动，受到不同方法和约定的限制。为了推动可持续土地管理实践，GSP选举数据协调和交流作为其主要行动之一。在国际标准和 先前的工作的基础上，发展了一个改进的领域模型，成为全球土壤信息系统(GloSIS)的基础。本文还确定了语义Web作为操作化领域模型的一个可能途径。

    arXiv:2403.16778v1 Announce Type: new  Abstract: Established in 2012 by members of the Food and Agriculture Organisation (FAO), the Global Soil Partnership (GSP) is a global network of stakeholders promoting sound land and soil management practices towards a sustainable world food system. However, soil survey largely remains a local or regional activity, bound to heterogeneous methods and conventions. Recognising the relevance of global and trans-national policies towards sustainable land management practices, the GSP elected data harmonisation and exchange as one of its key lines of action. Building upon international standards and previous work towards a global soil data ontology, an improved domain model was eventually developed within the GSP [54], the basis for a Global Soil Information System (GloSIS). This work also identified the Semantic Web as a possible avenue to operationalise the domain model. This article presents the GloSIS web ontology, an implementation of the GloSIS d
    
[^4]: ProCQA：一个用于代码搜索的大规模基于社区的编程问题回答数据集

    ProCQA: A Large-scale Community-based Programming Question Answering Dataset for Code Search

    [https://arxiv.org/abs/2403.16702](https://arxiv.org/abs/2403.16702)

    ProCQA数据集是从StackOverflow社区提取的，为编程问题回答提供了自然结构化的混合模态问答对，并引入了一种模态-不可知的对比预训练方法，显著提高了代码语言模型的性能。

    

    检索式代码问答旨在将用户在自然语言中的查询与相关代码片段匹配。先前的方法通常依赖于使用精心设计的双模态和单模态数据集进行预训练模型，以对齐文本和代码表示。在本文中，我们介绍了ProCQA，这是一个从StackOverflow社区中提取的大规模编程问题回答数据集，提供自然结构化的混合模态问答对。为了验证其有效性，我们提出了一种模态不可知的对比训练方法，以改善当前代码语言模型的文本和代码表示的对齐。与先前主要使用从CodeSearchNet中提取的双模态和单模态对进行预训练的模型相比，我们的模型在广泛的代码检索基准上表现出显著的性能改进。

    arXiv:2403.16702v1 Announce Type: new  Abstract: Retrieval-based code question answering seeks to match user queries in natural language to relevant code snippets. Previous approaches typically rely on pretraining models using crafted bi-modal and uni-modal datasets to align text and code representations. In this paper, we introduce ProCQA, a large-scale programming question answering dataset extracted from the StackOverflow community, offering naturally structured mixed-modal QA pairs. To validate its effectiveness, we propose a modality-agnostic contrastive pre-training approach to improve the alignment of text and code representations of current code language models. Compared to previous models that primarily employ bimodal and unimodal pairs extracted from CodeSearchNet for pre-training, our model exhibits significant performance improvements across a wide range of code retrieval benchmarks.
    
[^5]: 推荐系统的图增强

    Graph Augmentation for Recommendation

    [https://arxiv.org/abs/2403.16656](https://arxiv.org/abs/2403.16656)

    提出了一个名为GraphAug的框架，通过引入强大的数据增强程序生成去噪的自监督信号，解决了推荐系统中对比学习中存在的数据噪声和GNN架构平滑问题。

    

    Graph augmentation与对比学习在推荐系统领域引起了广泛关注，因为它能够在标记数据有限的情况下学习出富有表现力的用户表示。然而，直接将现有的GCL模型应用于现实推荐环境中存在挑战。主要问题有两个。首先，对比学习中忽略数据噪声可能导致嘈杂的自监督信号，从而降低性能。其次，许多现有的GCL方法依赖于图神经网络（GNN）架构，这可能由于非自适应信息传递而导致过度平滑问题。为解决这些挑战，我们提出了一个被称为GraphAug的原则性框架。该框架引入了一个强大的数据增强程序，生成去噪的自监督信号，提升推荐系统的性能。GraphAug框架还融入了图信息

    arXiv:2403.16656v1 Announce Type: new  Abstract: Graph augmentation with contrastive learning has gained significant attention in the field of recommendation systems due to its ability to learn expressive user representations, even when labeled data is limited. However, directly applying existing GCL models to real-world recommendation environments poses challenges. There are two primary issues to address. Firstly, the lack of consideration for data noise in contrastive learning can result in noisy self-supervised signals, leading to degraded performance. Secondly, many existing GCL approaches rely on graph neural network (GNN) architectures, which can suffer from over-smoothing problems due to non-adaptive message passing. To address these challenges, we propose a principled framework called GraphAug. This framework introduces a robust data augmentor that generates denoised self-supervised signals, enhancing recommender systems. The GraphAug framework incorporates a graph information 
    
[^6]: 专利相似性嵌入模型的比较分析

    A comparative analysis of embedding models for patent similarity

    [https://arxiv.org/abs/2403.16630](https://arxiv.org/abs/2403.16630)

    本文比较了不同类型的专利嵌入模型在专利相似性计算任务上的表现，并具体探讨了Sentence Transformers (SBERT) 架构在专利相似性任务中的性能。

    

    本文对基于文本的专利相似性领域做出了两个贡献。首先，它比较了不同类型的专利特定预训练嵌入模型（如word2vec和doc2vec模型）和上下文词嵌入模型（如基于transformers的模型）在专利相似性计算任务上的表现。其次，它具体比较了具有不同训练阶段的Sentence Transformers（SBERT）架构在专利相似性任务上的性能。为评估模型的性能，我们使用关于专利干涉的信息，即两个或多个专利申请中的专利要求被专利审查员证明存在重叠的现象。因此，我们将这些干涉案例视为两个专利之间的最大相似性的代理，并用它们作为基准来评估不同嵌入模型的性能。

    arXiv:2403.16630v1 Announce Type: new  Abstract: This paper makes two contributions to the field of text-based patent similarity. First, it compares the performance of different kinds of patent-specific pretrained embedding models, namely static word embeddings (such as word2vec and doc2vec models) and contextual word embeddings (such as transformers based models), on the task of patent similarity calculation. Second, it compares specifically the performance of Sentence Transformers (SBERT) architectures with different training phases on the patent similarity task. To assess the models' performance, we use information about patent interferences, a phenomenon in which two or more patent claims belonging to different patent applications are proven to be overlapping by patent examiners. Therefore, we use these interferences cases as a proxy for maximum similarity between two patents, treating them as ground-truth to evaluate the performance of the different embedding models. Our results p
    
[^7]: LARA：语言自适应检索增强LLMs用于多轮意图分类

    LARA: Linguistic-Adaptive Retrieval-Augmented LLMs for Multi-Turn Intent Classification

    [https://arxiv.org/abs/2403.16504](https://arxiv.org/abs/2403.16504)

    LARA是一个Linguistic-Adaptive Retrieval-Augmented Language Models（语言自适应检索增强LLMs），旨在通过结合微调过的较小模型与检索增强机制来提高多语言多轮意图分类任务的准确性，从而改善对话背景的理解。

    

    鉴于大型语言模型(LLMs)取得的显著成就，研究人员已经在文本分类任务中采用了上下文学习。然而，这些研究侧重于单语言、单轮分类任务。本文介绍了LARA（Linguistic-Adaptive Retrieval-Augmented Language Models），旨在增强多语言多轮分类任务的准确性，以适应聊天机器人交互中的众多意图。由于会话背景的复杂性和不断发展的性质，多轮意图分类尤为具有挑战性。LARA通过将微调过的较小模型与检索增强机制结合，嵌入LLMs的架构中来解决这些问题。这种整合使LARA能够动态利用过去的对话和相关意图，从而提高对上下文的理解。此外，我们的自适应检索技术增强了跨语言的能力。

    arXiv:2403.16504v1 Announce Type: new  Abstract: Following the significant achievements of large language models (LLMs), researchers have employed in-context learning for text classification tasks. However, these studies focused on monolingual, single-turn classification tasks. In this paper, we introduce LARA (Linguistic-Adaptive Retrieval-Augmented Language Models), designed to enhance accuracy in multi-turn classification tasks across six languages, accommodating numerous intents in chatbot interactions. Multi-turn intent classification is notably challenging due to the complexity and evolving nature of conversational contexts. LARA tackles these issues by combining a fine-tuned smaller model with a retrieval-augmented mechanism, integrated within the architecture of LLMs. This integration allows LARA to dynamically utilize past dialogues and relevant intents, thereby improving the understanding of the context. Furthermore, our adaptive retrieval techniques bolster the cross-lingual
    
[^8]: 基于大型语言模型的基于指令的无监督段落重新排序方法InstUPR

    InstUPR : Instruction-based Unsupervised Passage Reranking with Large Language Models

    [https://arxiv.org/abs/2403.16435](https://arxiv.org/abs/2403.16435)

    InstUPR是一种基于大型语言模型的无监督段落重新排序方法，利用了LLMs的指令跟踪能力，无需额外微调，通过软得分聚合技术和成对重新排序，在BEIR基准测试中表现优秀。

    

    本文介绍了InstUPR，一种基于大型语言模型（LLMs）的无监督段落重新排序方法。与现有依赖于query-document对进行大量训练或特定于检索的指令的方法不同，我们的方法利用了经过指令调整的LLMs的按照指令进行操作的能力来进行段落重新排序，而无需任何额外的微调。为实现这一目标，我们引入了一种软得分聚合技术，并采用了成对重新排序的无监督段落重新排序。在BEIR基准测试上的实验表明，InstUPR优于无监督基线以及一个经过指令调整的重新排序器，突显了其有效性和优越性。复现所有实验的源代码已在https://github.com/MiuLab/InstUPR 开源。

    arXiv:2403.16435v1 Announce Type: new  Abstract: This paper introduces InstUPR, an unsupervised passage reranking method based on large language models (LLMs). Different from existing approaches that rely on extensive training with query-document pairs or retrieval-specific instructions, our method leverages the instruction-following capabilities of instruction-tuned LLMs for passage reranking without any additional fine-tuning. To achieve this, we introduce a soft score aggregation technique and employ pairwise reranking for unsupervised passage reranking. Experiments on the BEIR benchmark demonstrate that InstUPR outperforms unsupervised baselines as well as an instruction-tuned reranker, highlighting its effectiveness and superiority. Source code to reproduce all experiments is open-sourced at https://github.com/MiuLab/InstUPR
    
[^9]: 使用ChatGPT为电子学位论文指定LCSH主题的实验

    An Experiment with the Use of ChatGPT for LCSH Subject Assignment on Electronic Theses and Dissertations

    [https://arxiv.org/abs/2403.16424](https://arxiv.org/abs/2403.16424)

    该研究探讨了利用大型语言模型生成美国国会图书馆主题标头的潜力，展示了其对于解决学术图书馆待编目项目积压问题具有战略应对意义，同时也强调了人类编目员仍然在验证和增强生成主题标头方面的重要性。

    

    该研究探讨了利用大型语言模型（LLMs）生成美国国会图书馆主题标头（LCSH）的潜力。作者使用ChatGPT根据电子学位论文的标题和摘要生成主题标头。结果显示，尽管一些生成的主题标头是有效的，但存在特定性和详尽性方面的问题。该研究展示了LLMs可以作为学术图书馆待编目项目的战略性应对措施，同时也提供了一种成本效益高且快速生成LCSH的方法。然而，人类编目员仍然是验证和增强LLMs生成的LCSH的有效性、详尽性和特定性的必要条件。

    arXiv:2403.16424v1 Announce Type: new  Abstract: This study delves into the potential use of Large Language Models (LLMs) for generating Library of Congress Subject Headings (LCSH). The authors employed ChatGPT to generate subject headings for electronic theses and dissertations (ETDs) based on their titles and summaries. The results revealed that although some generated subject headings were valid, there were issues regarding specificity and exhaustiveness. The study showcases that LLMs can serve as a strategic response to the backlog of items awaiting cataloging in academic libraries, while also offering a cost-effective approach for promptly generating LCSH. Nonetheless, human catalogers remain essential for verifying and enhancing the validity, exhaustiveness, and specificity of LCSH generated by LLMs.
    
[^10]: 利用各自优势：传统推荐模型和大型语言模型的协作智能

    Play to Your Strengths: Collaborative Intelligence of Conventional Recommender Models and Large Language Models

    [https://arxiv.org/abs/2403.16378](https://arxiv.org/abs/2403.16378)

    LLMs在CRM表现信心和精确性较低的数据段表现优异，而CRM表现优异的样本对LLM而言相对具有挑战性，这表明LLM和CRM之间存在潜在的协同作用。

    

    大型语言模型（LLMs）的崛起为推荐系统（RSs）带来了新的机遇，通过增强用户行为建模和内容理解。然而，当前将LLMs整合到RSs的方法仅仅利用LLM或传统推荐模型(CRM)来生成最终推荐，而没有考虑LLM或CRM在哪些数据段表现优秀。为了填补这一空白，我们在MovieLens-1M和Amazon-Books数据集上进行了实验，比较了代表性CRM（DCNv2）和一个LLM（LLaMA2-7B）在各种数据样本组上的性能。我们发现LLMs在CRM表现信心和精确性较低的数据段表现优异，而CRM表现优异的样本对LLM而言相对具有挑战性，需要大量训练数据和较长的训练时间才能获得可比较的性能。这表明LLM和CRM之间存在潜在的协同作用。受到这些见解的启发，我们...

    arXiv:2403.16378v1 Announce Type: new  Abstract: The rise of large language models (LLMs) has opened new opportunities in Recommender Systems (RSs) by enhancing user behavior modeling and content understanding. However, current approaches that integrate LLMs into RSs solely utilize either LLM or conventional recommender model (CRM) to generate final recommendations, without considering which data segments LLM or CRM excel in. To fill in this gap, we conduct experiments on MovieLens-1M and Amazon-Books datasets, and compare the performance of a representative CRM (DCNv2) and an LLM (LLaMA2-7B) on various groups of data samples. Our findings reveal that LLMs excel in data segments where CRMs exhibit lower confidence and precision, while samples where CRM excels are relatively challenging for LLM, requiring substantial training data and a long training time for comparable performance. This suggests potential synergies in the combination between LLM and CRM. Motivated by these insights, we
    
[^11]: 在终身序列推荐中揭示有选择性的状态空间模型的能力

    Uncovering Selective State Space Model's Capabilities in Lifelong Sequential Recommendation

    [https://arxiv.org/abs/2403.16371](https://arxiv.org/abs/2403.16371)

    研究人员调查了具有选择机制的Mamba在终身序列推荐中的性能，通过有选择地利用Mamba块来模拟用户终身序列，以解决现有模型难以处理终身序列的挑战。

    

    顺序推荐系统已广泛应用于各种在线服务中，旨在从用户的顺序交互中模拟他们的动态兴趣。随着用户越来越多地参与在线平台，生成了大量的终身用户行为序列。然而，现有的顺序推荐模型通常很难处理这样的终身序列。主要挑战源于计算复杂性和捕获序列中长距离依赖的能力。最近，一种具有选择机制（即Mamba）的状态空间模型开始出现。本文研究了Mamba在终身序列推荐（即长度>=2k）中的性能。具体来说，我们利用Mamba块有选择地模拟终身用户序列。我们进行了大量实验，评估了代表性顺序推荐模型在终身序列设置中的性能。

    arXiv:2403.16371v1 Announce Type: new  Abstract: Sequential Recommenders have been widely applied in various online services, aiming to model users' dynamic interests from their sequential interactions. With users increasingly engaging with online platforms, vast amounts of lifelong user behavioral sequences have been generated. However, existing sequential recommender models often struggle to handle such lifelong sequences. The primary challenges stem from computational complexity and the ability to capture long-range dependencies within the sequence. Recently, a state space model featuring a selective mechanism (i.e., Mamba) has emerged. In this work, we investigate the performance of Mamba for lifelong sequential recommendation (i.e., length>=2k). More specifically, we leverage the Mamba block to model lifelong user sequences selectively. We conduct extensive experiments to evaluate the performance of representative sequential recommendation models in the setting of lifelong sequenc
    
[^12]: 基于LLM编辑的增强式分面生成

    Enhanced Facet Generation with LLM Editing

    [https://arxiv.org/abs/2403.16345](https://arxiv.org/abs/2403.16345)

    提出了一种通过利用搜索引擎获取的文档和相关查询来增强分面预测的策略，并提出了专注于仅使用查询作为输入来预测分面的框架

    

    在信息检索中，用户查询的分面识别是一项重要任务。如果搜索服务能够识别用户查询的分面，就有潜力为用户提供更广泛的搜索结果。先前的研究可以通过利用搜索引擎获取的检索文档和相关查询来增强分面预测。然而，在将搜索引擎作为模型的一部分进行扩展到其他应用时存在挑战。第一，搜索引擎不断更新。因此，在训练和测试期间附加信息可能会有变化，这可能会降低性能。第二个挑战是公共搜索引擎无法搜索内部文件。因此，需要构建一个单独的搜索系统来将公司内部文档纳入其中。我们提出了两种策略，专注于一个可以仅通过查询作为输入来预测分面的框架。

    arXiv:2403.16345v1 Announce Type: cross  Abstract: In information retrieval, facet identification of a user query is an important task. If a search service can recognize the facets of a user's query, it has the potential to offer users a much broader range of search results. Previous studies can enhance facet prediction by leveraging retrieved documents and related queries obtained through a search engine. However, there are challenges in extending it to other applications when a search engine operates as part of the model. First, search engines are constantly updated. Therefore, additional information may change during training and test, which may reduce performance. The second challenge is that public search engines cannot search for internal documents. Therefore, a separate search system needs to be built to incorporate documents from private domains within the company. We propose two strategies that focus on a framework that can predict facets by taking only queries as input withou
    
[^13]: 超低成本两阶段多模式系统用于检测非规范行为

    Ultra Low-Cost Two-Stage Multimodal System for Non-Normative Behavior Detection

    [https://arxiv.org/abs/2403.16151](https://arxiv.org/abs/2403.16151)

    提出了一种超低成本两阶段多模式系统，用于以高精确度和召回率识别有害评论和图像，通过CLIP-ViT模型将推文和图像转换为嵌入，并利用常规机器学习分类器进行快速训练和低成本推理。

    

    在线社区日益受到有害评论的泛滥困扰。针对这一日益严峻的挑战，我们介绍了一种两阶段超低成本多模式有害行为检测方法，旨在以高精确度和召回率识别有害评论和图像。我们首先利用CLIP-ViT模型将推文和图像转换为嵌入，有效捕捉文本和图像中语义含义和微妙上下文线索的复杂交互作用。然后在第二阶段，系统将这些嵌入馈送到常规机器学习分类器（如SVM或逻辑回归）中，使系统能够快速训练并以极低成本进行推理。

    arXiv:2403.16151v1 Announce Type: cross  Abstract: The online community has increasingly been inundated by a toxic wave of harmful comments. In response to this growing challenge, we introduce a two-stage ultra-low-cost multimodal harmful behavior detection method designed to identify harmful comments and images with high precision and recall rates. We first utilize the CLIP-ViT model to transform tweets and images into embeddings, effectively capturing the intricate interplay of semantic meaning and subtle contextual clues within texts and images. Then in the second stage, the system feeds these embeddings into a conventional machine learning classifier like SVM or logistic regression, enabling the system to be trained rapidly and to perform inference at an ultra-low cost. By converting tweets into rich multimodal embeddings through the CLIP-ViT model and utilizing them to train conventional machine learning classifiers, our system is not only capable of detecting harmful textual info
    
[^14]: 电子商务中的互补推荐：定义、方法与未来方向

    Complementary Recommendation in E-commerce: Definition, Approaches, and Future Directions

    [https://arxiv.org/abs/2403.16135](https://arxiv.org/abs/2403.16135)

    本文全面总结和比较了电子商务领域中34项代表性互补推荐研究，包括建模产品之间的互补关系，不同研究问题下的模型分类与比较，以及在同一数据集上进行的实验结果分析。

    

    近年来，互补推荐在电子商务领域受到了广泛关注。本文对2009年至2024年间进行的34项代表性研究进行了全面总结和比较。首先，我们比较了用于建模产品之间互补关系的数据和方法，包括简单的互补性以及更复杂的情景，例如非对称互补性、产品之间替代和互补关系共存，以及不同产品对之间的互补程度不同。接下来，我们根据互补推荐的研究问题对模型进行分类并进行比较，如多样性、个性化和冷启动等。此外，我们对不同研究在同一数据集上进行的实验结果进行比较分析，有助于确定研究的优势和劣势。

    arXiv:2403.16135v1 Announce Type: cross  Abstract: In recent years, complementary recommendation has received extensive attention in the e-commerce domain. In this paper, we comprehensively summarize and compare 34 representative studies conducted between 2009 and 2024. Firstly, we compare the data and methods used for modeling complementary relationships between products, including simple complementarity and more complex scenarios such as asymmetric complementarity, the coexistence of substitution and complementarity relationships between products, and varying degrees of complementarity between different pairs of products. Next, we classify and compare the models based on the research problems of complementary recommendation, such as diversity, personalization, and cold-start. Furthermore, we provide a comparative analysis of experimental results from different studies conducted on the same dataset, which helps identify the strengths and weaknesses of the research. Compared to previou
    
[^15]: RankingSHAP -- 针对排名模型的逐项特征归因解释

    RankingSHAP -- Listwise Feature Attribution Explanations for Ranking Models

    [https://arxiv.org/abs/2403.16085](https://arxiv.org/abs/2403.16085)

    本文针对排名模型的特征归因进行了严格定义，并提出了RankingSHAP作为一种逐项排名归因方法，突破了当前解释评估方案的局限，提出了两种新的评估范式。

    

    特征归因是一种常用的解释类型，用于在训练模型后事后解释预测。然而，在信息检索领域，这种方法并没有得到很好的研究。重要的是，特征归因很少被严格定义，除了将最重要的特征归因为最高值之外。什么是比其他特征更重要的特征往往被模糊地描述。因此，大多数方法只关注选择最重要的特征，不充分利用甚至忽视特征内的相对重要性。在这项工作中，我们严格定义了排名模型特征归因的概念，并列出了一个有效归因应具备的基本属性。然后，我们提出RankingSHAP作为逐项排名归因方法的具体实例。与目前关注选择的解释评估方案相反，我们提出了两种用于评估归因的新颖评估范式。

    arXiv:2403.16085v1 Announce Type: new  Abstract: Feature attributions are a commonly used explanation type, when we want to posthoc explain the prediction of a trained model. Yet, they are not very well explored in IR. Importantly, feature attribution has rarely been rigorously defined, beyond attributing the most important feature the highest value. What it means for a feature to be more important than others is often left vague. Consequently, most approaches focus on just selecting the most important features and under utilize or even ignore the relative importance within features. In this work, we rigorously define the notion of feature attribution for ranking models, and list essential properties that a valid attribution should have. We then propose RankingSHAP as a concrete instantiation of a list-wise ranking attribution method. Contrary to current explanation evaluation schemes that focus on selections, we propose two novel evaluation paradigms for evaluating attributions over l
    
[^16]: 知识感知的双侧属性增强推荐

    Knowledge-aware Dual-side Attribute-enhanced Recommendation

    [https://arxiv.org/abs/2403.16037](https://arxiv.org/abs/2403.16037)

    提出了一种名为“知识感知的双侧属性增强推荐”（KDAR）的方法，通过建模用户偏好和利用偏好-属性连接来增强协同过滤推荐性能

    

    基于图神经网络和对比学习的知识感知推荐方法在性能上取得了令人满意的成绩。鉴于它们在建模细粒度用户偏好和利用偏好-属性连接进行预测方面的不足，我们提出了一种名为“知识感知的双侧属性增强推荐”（KDAR）的方法。具体来说，我们在知识图中的属性信息基础上构建了用户偏好表示和属性融合表示，用于增强基于协同过滤的用户和项目表示。为了区分这两种基于属性的表示中每个属性的贡献，我们提出了一个多级协同过滤

    arXiv:2403.16037v1 Announce Type: new  Abstract: \textit{Knowledge-aware} recommendation methods (KGR) based on \textit{graph neural networks} (GNNs) and \textit{contrastive learning} (CL) have achieved promising performance. However, they fall short in modeling fine-grained user preferences and further fail to leverage the \textit{preference-attribute connection} to make predictions, leading to sub-optimal performance. To address the issue, we propose a method named \textit{\textbf{K}nowledge-aware \textbf{D}ual-side \textbf{A}ttribute-enhanced \textbf{R}ecommendation} (KDAR). Specifically, we build \textit{user preference representations} and \textit{attribute fusion representations} upon the attribute information in knowledge graphs, which are utilized to enhance \textit{collaborative filtering} (CF) based user and item representations, respectively. To discriminate the contribution of each attribute in these two types of attribute-based representations, a \textit{multi-level collab
    
[^17]: 模型、分析和理解社交媒体平台内用户互动和各种属性

    Model, Analyze, and Comprehend User Interactions and Various Attributes within a Social Media Platform

    [https://arxiv.org/abs/2403.15937](https://arxiv.org/abs/2403.15937)

    本研究提出了一种新颖的基于图的方法，用于模型化和分析社交媒体平台内用户互动，揭示了活跃用户之间的连接模式、对社区动态的贡献率以及用户活动与受欢迎程度之间的相关性。

    

    本研究提出了一种基于帖子-评论关系的新颖基于图的方法，用于模型化和分析社交媒体平台内用户互动，构建用户互动图并分析以深入了解社区动态、用户行为和内容偏好。研究发现，社区内56.05%的活跃用户强连接在一起，但只有0.8%的用户对其动态有显著贡献。此外，我们观察到社区活动存在时间变化，某些时期经历了更高的参与度。此外，我们的发现强调了用户活动与受欢迎程度之间的相关性，表明更活跃的用户通常更受欢迎。

    arXiv:2403.15937v1 Announce Type: cross  Abstract: How can we effectively model, analyze, and comprehend user interactions and various attributes within a social media platform based on post-comment relationship? In this study, we propose a novel graph-based approach to model and analyze user interactions within a social media platform based on post-comment relationship. We construct a user interaction graph from social media data and analyze it to gain insights into community dynamics, user behavior, and content preferences. Our investigation reveals that while 56.05% of the active users are strongly connected within the community, only 0.8% of them significantly contribute to its dynamics. Moreover, we observe temporal variations in community activity, with certain periods experiencing heightened engagement. Additionally, our findings highlight a correlation between user activity and popularity showing that more active users are generally more popular. Alongside these, a preference f
    
[^18]: 朝向类人机理解的方向：在视觉丰富文档中进行少样本关系学习

    Towards Human-Like Machine Comprehension: Few-Shot Relational Learning in Visually-Rich Documents

    [https://arxiv.org/abs/2403.15765](https://arxiv.org/abs/2403.15765)

    该研究聚焦于在视觉丰富文档中进行少样本关系学习，引入了基于现有监督基准数据集构建的两个新的少样本基准，提出了一种包含关系二维空间先验和样本矫正的变分方法

    

    关键-值关系在视觉丰富文档（VRDs）中普遍存在，通常在不同的空间区域中呈现，伴随特定的颜色和字体风格。这些非文本线索作为重要指示器，极大增强了人类对这种关系三元组的理解和获取。然而，当前的文档AI方法往往未考虑与视觉和空间特征相关的这些有价值的先验信息，导致性能不佳，特别是在处理有限示例时。为了解决这一限制，我们的研究聚焦于少样本关系学习，具体针对在VRDs中提取关键-值关系三元组。鉴于缺乏适用于这一任务的数据集，我们引入了基于现有监督基准数据集构建的两个新的少样本基准。此外，我们提出了一种包含关系二维空间先验和样本矫正的变分方法

    arXiv:2403.15765v1 Announce Type: cross  Abstract: Key-value relations are prevalent in Visually-Rich Documents (VRDs), often depicted in distinct spatial regions accompanied by specific color and font styles. These non-textual cues serve as important indicators that greatly enhance human comprehension and acquisition of such relation triplets. However, current document AI approaches often fail to consider this valuable prior information related to visual and spatial features, resulting in suboptimal performance, particularly when dealing with limited examples. To address this limitation, our research focuses on few-shot relational learning, specifically targeting the extraction of key-value relation triplets in VRDs. Given the absence of a suitable dataset for this task, we introduce two new few-shot benchmarks built upon existing supervised benchmark datasets. Furthermore, we propose a variational approach that incorporates relational 2D-spatial priors and prototypical rectification 
    
[^19]: 用户端实现

    User-Side Realization

    [https://arxiv.org/abs/2403.15757](https://arxiv.org/abs/2403.15757)

    用户端实现为用户提供了积极的解决方案，通过在用户端运行通用算法来解决常见问题，无需服务提供商改变服务本身。

    

    用户对服务感到不满意。由于服务并非量身定制给用户，因此不满意是自然而然的。问题在于，即使用户感到不满意，他们通常也没有解决不满的手段。用户无法修改服务的源代码，也无法强迫服务提供商进行更改。用户别无选择，只能保持不满意或退出服务。用户端实现通过提供通用算法来处理用户端的常见问题，为解决这一问题提供了积极的解决方案。这些算法在用户端运行，并在不需要服务提供商改变服务本身的情况下解决问题。

    arXiv:2403.15757v1 Announce Type: cross  Abstract: Users are dissatisfied with services. Since the service is not tailor-made for a user, it is natural for dissatisfaction to arise. The problem is, that even if users are dissatisfied, they often do not have the means to resolve their dissatisfaction. The user cannot alter the source code of the service, nor can they force the service provider to change. The user has no choice but to remain dissatisfied or quit the service. User-side realization offers proactive solutions to this problem by providing general algorithms to deal with common problems on the user's side. These algorithms run on the user's side and solve the problems without having the service provider change the service itself.
    
[^20]: QueryExplorer：用于搜索和探索的交互式查询生成助手

    QueryExplorer: An Interactive Query Generation Assistant for Search and Exploration

    [https://arxiv.org/abs/2403.15667](https://arxiv.org/abs/2403.15667)

    QueryExplorer提出了一个交互式查询生成、重新构造和检索界面，支持多种方法帮助用户创建和修改有效查询，以解决用户缺乏领域专业知识或语言能力时的搜索困难。

    

    有效制定搜索查询仍然是一项具有挑战性的任务，特别是当用户缺乏特定领域的专业知识或不擅长内容的语言时。提供用户感兴趣的示例文档可能更容易一些。然而，通过示例进行查询的情况容易出现概念漂移，并且检索效果对查询生成方法非常敏感，且没有明确的方法来整合用户反馈。为了实现探索并支持人机实验，我们提出了QueryExplorer -- 一个交互式查询生成、重新构造和检索界面，支持HuggingFace生成模型和PyTerrier的检索管道和数据集，以及对人类反馈的详尽记录。为了让用户创建和修改有效的查询，我们的演示支持使用LLMs的互补方法，协助用户在多个阶段进行编辑和反馈。

    arXiv:2403.15667v1 Announce Type: new  Abstract: Formulating effective search queries remains a challenging task, particularly when users lack expertise in a specific domain or are not proficient in the language of the content. Providing example documents of interest might be easier for a user. However, such query-by-example scenarios are prone to concept drift, and the retrieval effectiveness is highly sensitive to the query generation method, without a clear way to incorporate user feedback. To enable exploration and to support Human-In-The-Loop experiments we propose QueryExplorer -- an interactive query generation, reformulation, and retrieval interface with support for HuggingFace generation models and PyTerrier's retrieval pipelines and datasets, and extensive logging of human feedback. To allow users to create and modify effective queries, our demo supports complementary approaches of using LLMs interactively, assisting the user with edits and feedback at multiple stages of the 
    
[^21]: 具有偏向空间方向的高维相位恢复的光谱初始化

    Spectral Initialization for High-Dimensional Phase Retrieval with Biased Spatial Directions

    [https://arxiv.org/abs/2403.15548](https://arxiv.org/abs/2403.15548)

    引入偏向空间方向的方法在高维相位恢复中取得了显著改进，在测量次数少于信号维数时尤为有效，同时揭示了一个依赖于样本大小与信号维数比率的相位转换现象。

    

    我们探讨了一种在非凸场景中的信号估计的当代研究中起着核心作用的光谱初始化方法。在无噪声相位恢复框架中，我们精确分析了当感知向量遵循多元高斯分布时该方法在高维极限下的表现，适用于协方差矩阵C的两种旋转不变模型。第一种模型中的C是一个投影在一个低维空间上，而第二种模型中的C是一个Wishart矩阵。我们的分析结果扩展了C为单位矩阵时已经被充分确认的情况。我们的研究表明，引入偏向空间方向会显著提高光谱方法的效果，特别是当测量次数少于信号维数时。这一扩展还始终显示了依赖于样本大小与信号维数之间比率的相位转换现象。

    arXiv:2403.15548v1 Announce Type: cross  Abstract: We explore a spectral initialization method that plays a central role in contemporary research on signal estimation in nonconvex scenarios. In a noiseless phase retrieval framework, we precisely analyze the method's performance in the high-dimensional limit when sensing vectors follow a multivariate Gaussian distribution for two rotationally invariant models of the covariance matrix C. In the first model C is a projector on a lower dimensional space while in the second it is a Wishart matrix. Our analytical results extend the well-established case when C is the identity matrix. Our examination shows that the introduction of biased spatial directions leads to a substantial improvement in the spectral method's effectiveness, particularly when the number of measurements is less than the signal's dimension. This extension also consistently reveals a phase transition phenomenon dependent on the ratio between sample size and signal dimension
    
[^22]: GTC：GNN-Transformer自监督异构图表示的共轭对比学习

    GTC: GNN-Transformer Co-contrastive Learning for Self-supervised Heterogeneous Graph Representation

    [https://arxiv.org/abs/2403.15520](https://arxiv.org/abs/2403.15520)

    该论文提出了一个用于GNN和Transformer的合作学习方案，并构建了GTC架构，通过整合GNN的本地信息聚合和Transformer的全局信息建模能力，解决了过度平滑问题。

    

    图神经网络（GNN）由于具有传递信息的机制，能够很好地聚合本地信息，在各种图任务中已经成为最强大的工具。然而，过度平滑一直阻碍着GNN进一步深入和捕获多跳邻居。与GNN不同，Transformer可以通过多头自注意力和适当的Transformer结构来建模全局信息和多跳交互，并且能够更好地解决过度平滑问题。因此，我们是否可以提出一个新颖的框架，将GNN和Transformer结合起来，整合GNN的本地信息聚合和Transformer的全局信息建模能力，以消除过度平滑问题？为了实现这一点，本文提出了一个用于GNN-Transformer的协同学习方案，并构建了GTC架构。GTC利用GNN和Transformer分支分别对来自不同视图的节点信息进行编码，并建立了对比

    arXiv:2403.15520v1 Announce Type: new  Abstract: Graph Neural Networks (GNNs) have emerged as the most powerful weapon for various graph tasks due to the message-passing mechanism's great local information aggregation ability. However, over-smoothing has always hindered GNNs from going deeper and capturing multi-hop neighbors. Unlike GNNs, Transformers can model global information and multi-hop interactions via multi-head self-attention and a proper Transformer structure can show more immunity to the over-smoothing problem. So, can we propose a novel framework to combine GNN and Transformer, integrating both GNN's local information aggregation and Transformer's global information modeling ability to eliminate the over-smoothing problem? To realize this, this paper proposes a collaborative learning scheme for GNN-Transformer and constructs GTC architecture. GTC leverages the GNN and Transformer branch to encode node information from different views respectively, and establishes contrast
    
[^23]: Loops On Retrieval Augmented Generation (LoRAG)

    Loops On Retrieval Augmented Generation (LoRAG)

    [https://arxiv.org/abs/2403.15450](https://arxiv.org/abs/2403.15450)

    LoRAG是一种新框架，通过引入迭代循环机制提高了检索增强型文本生成的质量，在实验证明在生成的文本连贯性和相关性方面优于当前最先进模型。

    

    本文介绍了一种名为Loops On Retrieval Augmented Generation (LoRAG)的新框架，旨在通过引入迭代循环机制来提高检索增强型文本生成的质量。该架构集成了生成模型、检索机制和动态循环模块，允许通过与从输入上下文中检索的相关信息进行交互来对生成的文本进行迭代改进。对基准数据集的实证评估表明，LoRAG在BLEU分数、ROUGE分数和困惑度方面均超过了现有的最先进模型，展示了其在生成文本的连贯性和相关性方面的有效性。定性评估进一步说明了LoRAG产生上下文丰富且连贯的输出的能力。这项研究为迭代循环在缓解文本生成中的挑战方面的潜力提供了有价值的见解。

    arXiv:2403.15450v1 Announce Type: new  Abstract: This paper presents Loops On Retrieval Augmented Generation (LoRAG), a new framework designed to enhance the quality of retrieval-augmented text generation through the incorporation of an iterative loop mechanism. The architecture integrates a generative model, a retrieval mechanism, and a dynamic loop module, allowing for iterative refinement of the generated text through interactions with relevant information retrieved from the input context. Experimental evaluations on benchmark datasets demonstrate that LoRAG surpasses existing state-of-the-art models in terms of BLEU score, ROUGE score, and perplexity, showcasing its effectiveness in achieving both coherence and relevance in generated text. The qualitative assessment further illustrates LoRAG's capability to produce contextually rich and coherent outputs. This research contributes valuable insights into the potential of iterative loops in mitigating challenges in text generation, po
    
[^24]: 不再有优化规则: 基于LLM的基于策略的多模查询优化器（版本1）

    No more optimization rules: LLM-enabled policy-based multi-modal query optimizer (version 1)

    [https://arxiv.org/abs/2403.13597](https://arxiv.org/abs/2403.13597)

    LLM启用了基于策略的多模查询优化器，摆脱了传统的基于规则的优化方法，为查询优化带来全新的可能性。

    

    大语言模型(LLM)在机器学习和深度学习领域标志着一个重要时刻。最近，人们研究了LLM在查询规划中的能力，包括单模和多模查询。然而，对于LLM的查询优化能力还没有相关研究。作为显著影响查询计划执行性能的关键步骤，不应错过这种分析和尝试。另一方面，现有的查询优化器通常是基于规则或基于规则+基于成本的，即它们依赖于人工创建的规则来完成查询计划重写/转换。鉴于现代优化器包括数百至数千条规则，按照类似方式设计一个多模查询优化器将耗费大量时间，因为我们将不得不列举尽可能多的多模优化规则，而这并没有。

    arXiv:2403.13597v1 Announce Type: cross  Abstract: Large language model (LLM) has marked a pivotal moment in the field of machine learning and deep learning. Recently its capability for query planning has been investigated, including both single-modal and multi-modal queries. However, there is no work on the query optimization capability of LLM. As a critical (or could even be the most important) step that significantly impacts the execution performance of the query plan, such analysis and attempts should not be missed. From another aspect, existing query optimizers are usually rule-based or rule-based + cost-based, i.e., they are dependent on manually created rules to complete the query plan rewrite/transformation. Given the fact that modern optimizers include hundreds to thousands of rules, designing a multi-modal query optimizer following a similar way is significantly time-consuming since we will have to enumerate as many multi-modal optimization rules as possible, which has not be
    
[^25]: NoteLLM: 一种可检索的大型语言模型，用于笔记推荐

    NoteLLM: A Retrievable Large Language Model for Note Recommendation

    [https://arxiv.org/abs/2403.01744](https://arxiv.org/abs/2403.01744)

    本文提出了一种名为NoteLLM的新颖统一框架，利用大型语言模型(LLMs)来实现物品到物品(I2I)的笔记推荐，通过学习生成哈希标签/类别潜在地增强笔记嵌入，提高了对关键笔记信息的压缩。

    

    人们喜欢在在线社区内分享“笔记”，包括他们的经验。因此，推荐与用户兴趣相符的笔记已经成为一项关键任务。现有的在线方法只将笔记输入到基于BERT的模型中，用于生成笔记嵌入以评估相似性。然而，它们可能未充分利用一些重要的线索，例如哈希标签或类别，这些代表了笔记的关键概念。事实上，学习生成哈希标签/类别可以潜在地增强笔记嵌入，二者都将重要的笔记信息压缩为有限内容。此外，大型语言模型（LLMs）在理解自然语言方面明显优于BERT。将LLMs引入笔记推荐是很有前途的。在本文中，我们提出了一种名为NoteLLM的新颖统一框架，利用LLMs来处理物品到物品（I2I）笔记推荐。具体来说，我们利用笔记压缩提示来压缩一条笔记

    arXiv:2403.01744v1 Announce Type: new  Abstract: People enjoy sharing "notes" including their experiences within online communities. Therefore, recommending notes aligned with user interests has become a crucial task. Existing online methods only input notes into BERT-based models to generate note embeddings for assessing similarity. However, they may underutilize some important cues, e.g., hashtags or categories, which represent the key concepts of notes. Indeed, learning to generate hashtags/categories can potentially enhance note embeddings, both of which compress key note information into limited content. Besides, Large Language Models (LLMs) have significantly outperformed BERT in understanding natural languages. It is promising to introduce LLMs into note recommendation. In this paper, we propose a novel unified framework called NoteLLM, which leverages LLMs to address the item-to-item (I2I) note recommendation. Specifically, we utilize Note Compression Prompt to compress a note 
    
[^26]: Word4Per: Zero-shot组合人员检索

    Word4Per: Zero-shot Composed Person Retrieval

    [https://arxiv.org/abs/2311.16515](https://arxiv.org/abs/2311.16515)

    提出了一个新任务：组合人员检索（CPR），旨在联合利用图像和文本信息进行目标人员检索，引入零样本组合人员检索（ZS-CPR）解决了CPR问题，提出了一个两阶段学习框架Word4Per。

    

    寻找特定人员具有极大的社会效益和安全价值，通常涉及视觉和文本信息的结合。本文提出了一个全新的任务，称为组合人员检索（CPR），旨在联合利用图像和文本信息进行目标人员检索。然而，监督CPR需要昂贵的手动注释数据集，而目前没有可用资源。为了解决这个问题，我们首先引入了零样本组合人员检索（ZS-CPR），利用现有的领域相关数据解决了CPR问题而不需要昂贵的注释。其次，为了学习ZS-CPR模型，我们提出了一个两阶段学习框架，即Word4Per，其中包含一个轻量级的文本反转网络。

    arXiv:2311.16515v2 Announce Type: replace-cross  Abstract: Searching for specific person has great social benefits and security value, and it often involves a combination of visual and textual information. Conventional person retrieval methods, whether image-based or text-based, usually fall short in effectively harnessing both types of information, leading to the loss of accuracy. In this paper, a whole new task called Composed Person Retrieval (CPR) is proposed to jointly utilize both image and text information for target person retrieval. However, the supervised CPR requires very costly manual annotation dataset, while there are currently no available resources. To mitigate this issue, we firstly introduce the Zero-shot Composed Person Retrieval (ZS-CPR), which leverages existing domain-related data to resolve the CPR problem without expensive annotations. Secondly, to learn ZS-CPR model, we propose a two-stage learning framework, Word4Per, where a lightweight Textual Inversion Netw
    
[^27]: 大型语言模型用于生成式推荐：一项调查和远见讨论

    Large Language Models for Generative Recommendation: A Survey and Visionary Discussions

    [https://arxiv.org/abs/2309.01157](https://arxiv.org/abs/2309.01157)

    大型语言模型为推荐系统的生成式推荐提供了新机遇，可以简化推荐流程并直接从完整的项目池中生成推荐。

    

    大型语言模型（LLM）不仅彻底改变了自然语言处理（NLP）领域，还有潜力重塑许多其他领域，例如推荐系统（RS）。本文调查了基于LLM的生成式推荐的进展、方法和未来方向，着眼于三个问题：1）生成式推荐是什么，2）为什么RS应该发展到生成式推荐，3）如何为各种RS实现基于LLM的生成推荐。

    arXiv:2309.01157v2 Announce Type: replace-cross  Abstract: Large language models (LLM) not only have revolutionized the field of natural language processing (NLP) but also have the potential to reshape many other fields, e.g., recommender systems (RS). However, most of the related work treats an LLM as a component of the conventional recommendation pipeline (e.g., as a feature extractor), which may not be able to fully leverage the generative power of LLM. Instead of separating the recommendation process into multiple stages, such as score computation and re-ranking, this process can be simplified to one stage with LLM: directly generating recommendations from the complete pool of items. This survey reviews the progress, methods, and future directions of LLM-based generative recommendation by examining three questions: 1) What generative recommendation is, 2) Why RS should advance to generative recommendation, and 3) How to implement LLM-based generative recommendation for various RS t
    
[^28]: 关于协作学习推荐系统针对社区检测攻击的韧性研究

    On the resilience of Collaborative Learning-based Recommender Systems Against Community Detection Attack

    [https://arxiv.org/abs/2306.08929](https://arxiv.org/abs/2306.08929)

    本文研究了协作学习推荐系统针对一种新型隐私攻击——社区检测攻击（CDA）的韧性。

    

    协作学习推荐系统源于协作学习技术（如联邦学习和八卦学习）的成功。在这些系统中，用户参与推荐系统的训练同时在其设备上保留已消费项目的历史记录。虽然这些解决方案乍一看似乎有利于保护参与者的隐私，但最近的研究表明，协作学习可能容易受到各种隐私攻击的威胁。本文研究了协作学习推荐系统针对一种称为社区检测攻击（CDA）的新型隐私攻击的韧性。这种攻击使得对手能够基于一个选择的项目集（如识别对特定兴趣点感兴趣的用户）来识别社区成员。通过在三个真实推荐数据集上进行实验，使用两种最先进的推荐

    arXiv:2306.08929v2 Announce Type: replace-cross  Abstract: Collaborative-learning-based recommender systems emerged following the success of collaborative learning techniques such as Federated Learning (FL) and Gossip Learning (GL). In these systems, users participate in the training of a recommender system while maintaining their history of consumed items on their devices. While these solutions seemed appealing for preserving the privacy of the participants at first glance, recent studies have revealed that collaborative learning can be vulnerable to various privacy attacks. In this paper, we study the resilience of collaborative learning-based recommender systems against a novel privacy attack called Community Detection Attack (CDA). This attack enables an adversary to identify community members based on a chosen set of items (eg., identifying users interested in specific points-of-interest). Through experiments on three real recommendation datasets using two state-of-the-art recomme
    
[^29]: Spacerini：使用Pyserini和Hugging Face实现即插即用的搜索引擎

    Spacerini: Plug-and-play Search Engines with Pyserini and Hugging Face

    [https://arxiv.org/abs/2302.14534](https://arxiv.org/abs/2302.14534)

    Spacerini是一个集成了Pyserini和Hugging Face的工具，可以无缝构建和部署交互式搜索引擎，使得非IR从业者可以更轻松地使用最先进的检索模型，对NLP和IR研究人员以及第三方复制研究工作都非常有用。

    

    我们提出了Spacerini，这是一个集成了Pyserini工具包和Hugging Face的工具，可以无缝构建和部署交互式搜索引擎的工具。通过Spacerini，非IR从业者可以更轻松地使用最先进的稀疏和稠密检索模型，并最小化部署工作量。Spacerini对于希望通过对训练语料库进行定性分析来更好地理解和验证研究的NLP研究人员、希望在不断发展的Pyserini生态系统中展示新检索模型的IR研究人员以及复制其他研究人员工作的第三方来说都非常有用。Spacerini是开源的，并包括用于本地和远程加载、预处理、索引和部署搜索引擎的实用程序。我们展示了使用Spacerini创建的13个不同用例的搜索引擎组合。

    arXiv:2302.14534v2 Announce Type: replace-cross  Abstract: We present Spacerini, a tool that integrates the Pyserini toolkit for reproducible information retrieval research with Hugging Face to enable the seamless construction and deployment of interactive search engines. Spacerini makes state-of-the-art sparse and dense retrieval models more accessible to non-IR practitioners while minimizing deployment effort. This is useful for NLP researchers who want to better understand and validate their research by performing qualitative analyses of training corpora, for IR researchers who want to demonstrate new retrieval models integrated into the growing Pyserini ecosystem, and for third parties reproducing the work of other researchers. Spacerini is open source and includes utilities for loading, preprocessing, indexing, and deploying search engines locally and remotely. We demonstrate a portfolio of 13 search engines created with Spacerini for different use cases.
    
[^30]: 通过检索示范进行上下文学习的语言模型：一项综述

    In-context Learning with Retrieved Demonstrations for Language Models: A Survey. (arXiv:2401.11624v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2401.11624](http://arxiv.org/abs/2401.11624)

    本综述调查了一种名为检索示范的方法，它通过使用特定于输入查询的示范来提高语言模型的少量样本情境学习（ICL）能力。这种方法不仅提高了学习效率和可扩展性，还减少了手动示例选择中的偏见。

    

    语言模型，特别是预训练的大型语言模型，已展示出卓越的能力，可以在输入上下文中进行少量样本的情境学习（ICL），并在新任务上具有适应能力。然而，模型的ICL能力对于少样本示范的选择是敏感的。最近的一项研究进展是检索针对每个输入查询定制的示范。示范检索的实现相对简单，利用现有的数据库和检索系统。这不仅提高了学习过程的效率和可扩展性，而且已经证明可以减少手动示例选择中的偏见。鉴于令人鼓舞的结果和在检索示范的ICL方面不断增长的研究，我们进行了广泛的研究综述。在这项综述中，我们讨论和比较了检索模型的不同设计选择，检索训练

    Language models, especially pre-trained large language models, have showcased remarkable abilities as few-shot in-context learners (ICL), adept at adapting to new tasks with just a few demonstrations in the input context. However, the model's ability to perform ICL is sensitive to the choice of the few-shot demonstrations. Instead of using a fixed set of demonstrations, one recent development is to retrieve demonstrations tailored to each input query. The implementation of demonstration retrieval is relatively straightforward, leveraging existing databases and retrieval systems. This not only improves the efficiency and scalability of the learning process but also has been shown to reduce biases inherent in manual example selection. In light of the encouraging results and growing research in ICL with retrieved demonstrations, we conduct an extensive review of studies in this area. In this survey, we discuss and compare different design choices for retrieval models, retrieval training p
    
[^31]: 我们的模型在MovieLens上取得了出色的表现：这意味着什么？

    Our Model Achieves Excellent Performance on MovieLens: What Does it Mean?. (arXiv:2307.09985v1 [cs.IR])

    [http://arxiv.org/abs/2307.09985](http://arxiv.org/abs/2307.09985)

    该论文通过对MovieLens数据集的分析，发现用户与该平台的交互在不同阶段存在显著差异，并且用户交互受到平台推荐算法推荐的候选电影的影响。

    

    推荐系统评估的典型基准数据集是在某一时间段内在平台上生成的用户-物品交互数据。交互生成机制部分解释了为什么用户与物品进行交互（如喜欢、购买、评分）以及特定交互发生的背景。在本研究中，我们对MovieLens数据集进行了细致的分析，并解释了使用该数据集进行评估推荐算法时可能的影响。我们从分析中得出了一些主要发现。首先，在用户与MovieLens平台交互的不同阶段存在显著差异。早期交互在很大程度上定义了用户画像，影响了后续的交互。其次，用户交互受到平台内部推荐算法推荐的候选电影的很大影响。删除靠近最后几次交互的交互会对结果产生较大影响。

    A typical benchmark dataset for recommender system (RecSys) evaluation consists of user-item interactions generated on a platform within a time period. The interaction generation mechanism partially explains why a user interacts with (e.g.,like, purchase, rate) an item, and the context of when a particular interaction happened. In this study, we conduct a meticulous analysis on the MovieLens dataset and explain the potential impact on using the dataset for evaluating recommendation algorithms. We make a few main findings from our analysis. First, there are significant differences in user interactions at the different stages when a user interacts with the MovieLens platform. The early interactions largely define the user portrait which affect the subsequent interactions. Second, user interactions are highly affected by the candidate movies that are recommended by the platform's internal recommendation algorithm(s). Removal of interactions that happen nearer to the last few interactions 
    

