# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Using LLMs to discover emerging coded antisemitic hate-speech emergence in extremist social media.](http://arxiv.org/abs/2401.10841) | 这项研究提出了一种方法，可以检测新出现的编码恶意术语，为极端社交媒体中的反犹太恶意言论提供了解决方案。 |
| [^2] | [Dynamic Q&A of Clinical Documents with Large Language Models.](http://arxiv.org/abs/2401.10733) | 本研究介绍了一种使用大型语言模型进行临床文档动态问答的自然语言接口。通过Langchain和Transformer-based LLMs驱动的聊天机器人，用户可以用自然语言查询临床笔记并获得相关答案。实验结果显示Wizard Vicuna具有出色的准确性，但计算要求较高。模型优化方案提高了约48倍的延迟。然而，模型产生幻象和多样化医疗案例评估的限制仍然存在。解决这些挑战对于发掘临床笔记的价值和推进基于AI的临床决策至关重要。 |
| [^3] | [Beyond RMSE and MAE: Introducing EAUC to unmask hidden bias and unfairness in dyadic regression models.](http://arxiv.org/abs/2401.10690) | 本研究引入EAUC作为一种新的度量标准，用以揭示迪亚德回归模型中隐藏的偏见和不公平问题。传统的全局错误度量标准如RMSE和MAE无法捕捉到这种问题。 |
| [^4] | [Automatic Construction of Multi-faceted User Profiles using Text Clustering and its Application to Expert Recommendation and Filtering Problems.](http://arxiv.org/abs/2401.10634) | 本文介绍了一种使用文本聚类自动构建多维用户个人资料的方法，并将其应用于专家推荐和文档过滤问题中。实验证明，该方法可以提高专家查找和文档过滤的性能。 |
| [^5] | [LDA-based Term Profiles for Expert Finding in a Political Setting.](http://arxiv.org/abs/2401.10617) | 这项研究提出了一种基于LDA的方法，在政治环境下使用主题分配术语配置来帮助查找具有特定领域专业知识的政治家。 |
| [^6] | [Publication venue recommendation using profiles based on clustering.](http://arxiv.org/abs/2401.10611) | 本文研究了出版场所推荐问题，提出了一种基于聚类和信息检索的方法，通过构建基于主题的个人资料来推荐论文的目标出版场所，并考虑使用作者信息来改进推荐的准确性。 |
| [^7] | [Use of topical and temporal profiles and their hybridisation for content-based recommendation.](http://arxiv.org/abs/2401.10607) | 本文研究了如何通过混合时间和主题特征来改进基于内容的推荐系统，并提出了两种不同的混合方法，并与其他方法进行了比较评估。 |
| [^8] | [Understanding Biases in ChatGPT-based Recommender Systems: Provider Fairness, Temporal Stability, and Recency.](http://arxiv.org/abs/2401.10545) | 该研究揭示了基于ChatGPT的推荐系统的偏见问题，并研究了提示设计策略对推荐质量的影响。实验结果显示，在RecLLMs中引入特定的系统角色和提示策略可以增强推荐的公平性和多样性，同时GPT-based模型倾向于推荐最新和更多样化的电影流派。 |
| [^9] | [Generative Dense Retrieval: Memory Can Be a Burden.](http://arxiv.org/abs/2401.10487) | 本文提出了生成式密集检索（GDR）范式，通过在查询和文档之间实现簇间匹配和细粒度的簇内匹配，缓解了生成式检索面临的记忆准确性差、记忆混淆和记忆更新成本高的问题。 |
| [^10] | [Enhancing Scalability in Recommender Systems through Lottery Ticket Hypothesis and Knowledge Distillation-based Neural Network Pruning.](http://arxiv.org/abs/2401.10484) | 本研究通过将彩票票据假设和知识蒸馏框架相结合，提出了一种剪枝神经网络的创新方法，以解决推荐系统在边缘设备上的可扩展性问题。经实验证明，该方法能够有效地降低功耗和模型尺寸，并实现高达66.67%的GPU计算功率减少。此外，本研究还首次应用了彩票票据假设和知识蒸馏技术于推荐系统领域，对该领域作出了重要贡献。 |
| [^11] | [Improving One-class Recommendation with Multi-tasking on Various Preference Intensities.](http://arxiv.org/abs/2401.10316) | 本研究提出了一个多任务框架，考虑了隐式反馈中不同偏好强度的情况，并在其中引入了注意力图卷积层来探索高阶关系。这使得表示更具鲁棒性和泛化性。 |
| [^12] | [A systematic review of geospatial location embedding approaches in large language models: A path to spatial AI systems.](http://arxiv.org/abs/2401.10279) | 这篇论文系统综述了在大型语言模型中的地理位置嵌入方法，提出了四种主要的嵌入主题，并强调了在空间形态和生成模态方面进一步发展的需求。 |
| [^13] | [Knowledge graph driven recommendation model of graph neural network.](http://arxiv.org/abs/2401.10244) | 提出了一种基于知识图谱的图神经网络推荐模型KGLN，通过合并节点特征、调整聚合权重和迭代演化，提高了个性化推荐的准确性和效果。在实验中相对于已有基准方法，KGLN在不同数据集上的AUC提高了0.3%至5.9%和1.1%至8.2%。 |
| [^14] | [Source Code Clone Detection Using Unsupervised Similarity Measures.](http://arxiv.org/abs/2401.09885) | 本研究对使用无监督相似度度量进行源代码克隆检测进行了比较分析，旨在为软件工程师提供指导，以选择适合其特定用例的方法。 |
| [^15] | [A Survey on Cross-Domain Sequential Recommendation.](http://arxiv.org/abs/2401.04971) | 跨领域序列推荐通过集成和学习多个领域的交互信息，将用户偏好建模从平面转向立体。文章对CDSR问题进行了定义和分析，提供了从宏观和微观两个视角的系统概述。对于不同领域间的模型，总结了多层融合结构和融合桥梁。对于现有模型，讨论了基础技术和辅助学习技术。展示了公开数据集和实验结果，并给出了未来发展的见解。 |
| [^16] | [Improving Text Embeddings with Large Language Models.](http://arxiv.org/abs/2401.00368) | 本文介绍了一种使用只用合成数据和少量训练步骤获取高质量文本嵌入的简单方法，并且在没有使用标记数据的情况下，在竞争激烈的文本嵌入基准上取得了强大的性能。 |
| [^17] | [Context-Driven Interactive Query Simulations Based on Generative Large Language Models.](http://arxiv.org/abs/2312.09631) | 本论文介绍了一种基于生成型大规模语言模型的上下文驱动的交互式查询模拟方法，该方法考虑了用户的上下文，并展示了比传统方法更好的有效性和更高效的信息检索会话模拟。 |
| [^18] | [Leveraging Negative Signals with Self-Attention for Sequential Music Recommendation.](http://arxiv.org/abs/2309.11623) | 本研究利用自我注意力机制和负面反馈，提出了用于顺序音乐推荐的Transformer模型，并采用对比学习任务来提高推荐准确性。 |
| [^19] | [Large Language Models for Information Retrieval: A Survey.](http://arxiv.org/abs/2308.07107) | 本综述将大型语言模型（LLMs）在信息检索中的发展进行了综述，探讨了其在捕捉上下文信号和语义细微之处方面的优势和挑战，以及与传统检索方法的结合的重要性。 |

# 详细

[^1]: 使用LLMs发现极端社交媒体中的编码反犹太恶意言论的出现

    Using LLMs to discover emerging coded antisemitic hate-speech emergence in extremist social media. (arXiv:2401.10841v1 [cs.CL])

    [http://arxiv.org/abs/2401.10841](http://arxiv.org/abs/2401.10841)

    这项研究提出了一种方法，可以检测新出现的编码恶意术语，为极端社交媒体中的反犹太恶意言论提供了解决方案。

    

    网络仇恨言论的蔓延给社交媒体平台带来了一个难题。一个特殊的挑战与使用编码语言的群体有关，这些群体既想为其用户创造归属感，又想回避检测。编码语言发展迅速，并且随着时间的推移使用方式不同。本文提出了一种检测新出现的编码恶意术语的方法论。该方法在在线反犹太言论的环境中进行了测试。该方法考虑了从社交媒体平台上抓取的帖子，通常是极端主义用户使用的。帖子是使用与以前已知的针对犹太人的仇恨言论相关的种子表达式进行抓取的。该方法首先通过识别每个帖子最具代表性的表达式，并计算它们在整个语料库中的频率。过滤掉语法不一致的表达式和之前遇到过的表达式，以便关注新出现的良好形式的术语。然后进行了语义评估。

    Online hate speech proliferation has created a difficult problem for social media platforms. A particular challenge relates to the use of coded language by groups interested in both creating a sense of belonging for its users and evading detection. Coded language evolves quickly and its use varies over time. This paper proposes a methodology for detecting emerging coded hate-laden terminology. The methodology is tested in the context of online antisemitic discourse. The approach considers posts scraped from social media platforms, often used by extremist users. The posts are scraped using seed expressions related to previously known discourse of hatred towards Jews. The method begins by identifying the expressions most representative of each post and calculating their frequency in the whole corpus. It filters out grammatically incoherent expressions as well as previously encountered ones so as to focus on emergent well-formed terminology. This is followed by an assessment of semantic s
    
[^2]: 使用大型语言模型进行临床文档的动态问答

    Dynamic Q&A of Clinical Documents with Large Language Models. (arXiv:2401.10733v1 [cs.IR])

    [http://arxiv.org/abs/2401.10733](http://arxiv.org/abs/2401.10733)

    本研究介绍了一种使用大型语言模型进行临床文档动态问答的自然语言接口。通过Langchain和Transformer-based LLMs驱动的聊天机器人，用户可以用自然语言查询临床笔记并获得相关答案。实验结果显示Wizard Vicuna具有出色的准确性，但计算要求较高。模型优化方案提高了约48倍的延迟。然而，模型产生幻象和多样化医疗案例评估的限制仍然存在。解决这些挑战对于发掘临床笔记的价值和推进基于AI的临床决策至关重要。

    

    电子健康记录（EHR）中收录了临床笔记中的重要患者数据。随着这些笔记数量和复杂度的增加，手动提取变得具有挑战性。本研究利用大型语言模型（LLMs）引入了一种自然语言接口，用于对临床笔记进行动态问答。我们的聊天机器人由Langchain和基于Transformer的LLMs驱动，允许用户用自然语言发出查询，并从临床笔记中获得相关答案。通过使用各种嵌入模型和先进的LLMs进行实验，结果表明Wizard Vicuna在准确性方面表现优异，尽管计算要求较高。模型优化，包括权重量化，将延迟提高了约48倍。有希望的结果显示了临床笔记中的价值潜力，但仍存在模型产生幻象和有限的多样化医疗案例评估等挑战。解决这些问题对于发掘临床笔记的价值和推动AI驱动的临床决策至关重要。

    Electronic health records (EHRs) house crucial patient data in clinical notes. As these notes grow in volume and complexity, manual extraction becomes challenging. This work introduces a natural language interface using large language models (LLMs) for dynamic question-answering on clinical notes. Our chatbot, powered by Langchain and transformer-based LLMs, allows users to query in natural language, receiving relevant answers from clinical notes. Experiments, utilizing various embedding models and advanced LLMs, show Wizard Vicuna's superior accuracy, albeit with high compute demands. Model optimization, including weight quantization, improves latency by approximately 48 times. Promising results indicate potential, yet challenges such as model hallucinations and limited diverse medical case evaluations remain. Addressing these gaps is crucial for unlocking the value in clinical notes and advancing AI-driven clinical decision-making.
    
[^3]: 超越RMSE和MAE：引入EAUC来揭示偏见和不公平的迪亚德回归模型中的隐藏因素

    Beyond RMSE and MAE: Introducing EAUC to unmask hidden bias and unfairness in dyadic regression models. (arXiv:2401.10690v1 [cs.LG])

    [http://arxiv.org/abs/2401.10690](http://arxiv.org/abs/2401.10690)

    本研究引入EAUC作为一种新的度量标准，用以揭示迪亚德回归模型中隐藏的偏见和不公平问题。传统的全局错误度量标准如RMSE和MAE无法捕捉到这种问题。

    

    迪亚德回归模型用于预测一对实体的实值结果，在许多领域中都是基础的（例如，在推荐系统中预测用户对产品的评分），在许多其他领域中也有许多潜力但尚未深入探索（例如，在个性化药理学中近似确定患者的适当剂量）。本研究中，我们证明个体实体观察值分布的非均匀性导致了最先进模型中的严重偏见预测，偏向于实体的观察过去值的平均值，并在另类但同样重要的情况下提供比随机预测更差的预测能力。我们表明，全局错误度量标准如均方根误差（RMSE）和平均绝对误差（MAE）不足以捕捉到这种现象，我们将其命名为另类偏见，并引入另类-曲线下面积（EAUC）作为一个新的补充度量，可以在所有研究的模型中量化它。

    Dyadic regression models, which predict real-valued outcomes for pairs of entities, are fundamental in many domains (e.g. predicting the rating of a user to a product in Recommender Systems) and promising and under exploration in many others (e.g. approximating the adequate dosage of a drug for a patient in personalized pharmacology). In this work, we demonstrate that non-uniformity in the observed value distributions of individual entities leads to severely biased predictions in state-of-the-art models, skewing predictions towards the average of observed past values for the entity and providing worse-than-random predictive power in eccentric yet equally important cases. We show that the usage of global error metrics like Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE) is insufficient to capture this phenomenon, which we name eccentricity bias, and we introduce Eccentricity-Area Under the Curve (EAUC) as a new complementary metric that can quantify it in all studied models
    
[^4]: 使用文本聚类自动构建多维用户个人资料及其在专家推荐和过滤问题上的应用

    Automatic Construction of Multi-faceted User Profiles using Text Clustering and its Application to Expert Recommendation and Filtering Problems. (arXiv:2401.10634v1 [cs.IR])

    [http://arxiv.org/abs/2401.10634](http://arxiv.org/abs/2401.10634)

    本文介绍了一种使用文本聚类自动构建多维用户个人资料的方法，并将其应用于专家推荐和文档过滤问题中。实验证明，该方法可以提高专家查找和文档过滤的性能。

    

    在当今的信息时代，我们不仅对访问文档、视频等多媒体对象感兴趣，还对搜索专业专家、人物或名人感兴趣，可能是出于职业需求或只是为了娱乐。信息访问系统需要能够提取和利用关于这些个人的各种信息来源（通常以文本形式），并以适当的方式呈现，通常以个人资料的形式。本文从机器学习的角度，通过对专家文本来源进行聚类来构建个人资料并捕捉专家感兴趣的不同隐藏主题，从而解决基于个人资料的专家推荐和文档过滤问题。实验证明，这是一种有效的技术，可以提高专家查找和文档过滤的性能。

    In the information age we are living in today, not only are we interested in accessing multimedia objects such as documents, videos, etc. but also in searching for professional experts, people or celebrities, possibly for professional needs or just for fun. Information access systems need to be able to extract and exploit various sources of information (usually in text format) about such individuals, and to represent them in a suitable way usually in the form of a profile. In this article, we tackle the problems of profile-based expert recommendation and document filtering from a machine learning perspective by clustering expert textual sources to build profiles and capture the different hidden topics in which the experts are interested. The experts will then be represented by means of multi-faceted profiles. Our experiments show that this is a valid technique to improve the performance of expert finding and document filtering.
    
[^5]: 基于LDA的政治环境下专家查找的术语配置

    LDA-based Term Profiles for Expert Finding in a Political Setting. (arXiv:2401.10617v1 [cs.IR])

    [http://arxiv.org/abs/2401.10617](http://arxiv.org/abs/2401.10617)

    这项研究提出了一种基于LDA的方法，在政治环境下使用主题分配术语配置来帮助查找具有特定领域专业知识的政治家。

    

    许多政治机构（如议会）的常见任务是找到在特定领域内具有专业知识的政治家。为了解决这个问题，第一步是获取政治家的个人资料，其中包括他们的兴趣，这些兴趣可以通过他们的演讲自动学习。由于政治家可能在多个领域有专长，因此一种替代方法是使用一组子资料，每个子资料都涵盖不同的主题。在这项研究中，我们提出了一种新的方法，通过使用潜在狄利克雷分配（LDA）来确定每个政治演讲的主要主题，并将相关术语分配给不同的基于主题的子资料。为了实现这一目标，我们提出使用15种距离和相似性度量来自动确定文档中讨论的主题数，并证明每种度量都收敛为五种策略：欧氏距离、Dice系数、Sorensen系数、余弦相似度和重叠度。我们的实验结果表明

    A common task in many political institutions (i.e. Parliament) is to find politicians who are experts in a particular field. In order to tackle this problem, the first step is to obtain politician profiles which include their interests, and these can be automatically learned from their speeches. As a politician may have various areas of expertise, one alternative is to use a set of subprofiles, each of which covers a different subject. In this study, we propose a novel approach for this task by using latent Dirichlet allocation (LDA) to determine the main underlying topics of each political speech, and to distribute the related terms among the different topic-based subprofiles. With this objective, we propose the use of fifteen distance and similarity measures to automatically determine the optimal number of topics discussed in a document, and to demonstrate that every measure converges into five strategies: Euclidean, Dice, Sorensen, Cosine and Overlap. Our experimental results showed
    
[^6]: 基于聚类的个人资料的出版场所推荐

    Publication venue recommendation using profiles based on clustering. (arXiv:2401.10611v1 [cs.IR])

    [http://arxiv.org/abs/2401.10611](http://arxiv.org/abs/2401.10611)

    本文研究了出版场所推荐问题，提出了一种基于聚类和信息检索的方法，通过构建基于主题的个人资料来推荐论文的目标出版场所，并考虑使用作者信息来改进推荐的准确性。

    

    本文研究了出版场所推荐问题，旨在帮助研究人员找到一个适合提交论文的期刊或会议。解决这个问题的常见方法是建立定义每个场所范围的个人资料，然后将这些个人资料与目标论文进行比较。在我们的方法中，我们将研究如何使用聚类技术构建基于主题的个人资料，并使用信息检索的方法得到最终的推荐。此外，我们还将探讨使用作者信息作为补充信息如何提高推荐的效果。

    In this paper we study the venue recommendation problem in order to help researchers to identify a journal or conference to submit a given paper. A common approach to tackle this problem is to build profiles defining the scope of each venue. Then, these profiles are compared against the target paper. In our approach we will study how clustering techniques can be used to construct topic-based profiles and use an Information Retrieval based approach to obtain the final recommendations. Additionally, we will explore how the use of authorship, representing a complementary piece of information, helps to improve the recommendations.
    
[^7]: 使用主题和时间轴的混合方法进行基于内容的推荐

    Use of topical and temporal profiles and their hybridisation for content-based recommendation. (arXiv:2401.10607v1 [cs.IR])

    [http://arxiv.org/abs/2401.10607](http://arxiv.org/abs/2401.10607)

    本文研究了如何通过混合时间和主题特征来改进基于内容的推荐系统，并提出了两种不同的混合方法，并与其他方法进行了比较评估。

    

    在内容推荐系统的背景下，本文旨在确定如何构建更好的用户和物品特征，并研究这些特征对基于时间和主题的推荐过程的影响。主要贡献是提出了两种不同的混合方法，并与其他替代方案进行评估和比较。

    In the context of content-based recommender systems, the aim of this paper is to determine how better profiles can be built and how these affect the recommendation process based on the incorporation of temporality, i.e. the inclusion of time in the recommendation process, and topicality, i.e. the representation of texts associated with users and items using topics and their combination. The main contribution of the paper is to present two different ways of hybridising these two dimensions and to evaluate and compare them with other alternatives.
    
[^8]: 理解ChatGPT基推荐系统中的偏见：供应商公平性、时间稳定性和最新性研究

    Understanding Biases in ChatGPT-based Recommender Systems: Provider Fairness, Temporal Stability, and Recency. (arXiv:2401.10545v1 [cs.IR])

    [http://arxiv.org/abs/2401.10545](http://arxiv.org/abs/2401.10545)

    该研究揭示了基于ChatGPT的推荐系统的偏见问题，并研究了提示设计策略对推荐质量的影响。实验结果显示，在RecLLMs中引入特定的系统角色和提示策略可以增强推荐的公平性和多样性，同时GPT-based模型倾向于推荐最新和更多样化的电影流派。

    

    该研究探讨了使用大型语言模型（RecLLMs）的推荐系统的细微能力和固有偏见，重点研究了基于ChatGPT的系统。研究了生成模型和传统协同过滤模型在电影推荐中的差异行为。本研究主要调查了提示设计策略及其对推荐质量的各个方面（包括准确性、供应商公平性、多样性、稳定性、流行类型和时效性）的影响。我们的实验分析表明，在RecLLMs中引入特定的“系统角色”和“提示策略”显著影响其性能。例如，基于角色的提示可以增强推荐的公平性和多样性，减轻流行偏见。我们发现，虽然基于GPT的模型并不总是能与传统协同过滤基线模型的性能匹配，但它们倾向于推荐更新、更多样化的电影流派。值得注意的是，GPT-base

    This study explores the nuanced capabilities and inherent biases of Recommender Systems using Large Language Models (RecLLMs), with a focus on ChatGPT-based systems. It studies into the contrasting behaviors of generative models and traditional collaborative filtering models in movie recommendations. The research primarily investigates prompt design strategies and their impact on various aspects of recommendation quality, including accuracy, provider fairness, diversity, stability, genre dominance, and temporal freshness (recency).  Our experimental analysis reveals that the introduction of specific 'system roles' and 'prompt strategies' in RecLLMs significantly influences their performance. For instance, role-based prompts enhance fairness and diversity in recommendations, mitigating popularity bias. We find that while GPT-based models do not always match the performance of CF baselines, they exhibit a unique tendency to recommend newer and more diverse movie genres. Notably, GPT-base
    
[^9]: 生成式密集检索：记忆可以是一个负担

    Generative Dense Retrieval: Memory Can Be a Burden. (arXiv:2401.10487v1 [cs.IR])

    [http://arxiv.org/abs/2401.10487](http://arxiv.org/abs/2401.10487)

    本文提出了生成式密集检索（GDR）范式，通过在查询和文档之间实现簇间匹配和细粒度的簇内匹配，缓解了生成式检索面临的记忆准确性差、记忆混淆和记忆更新成本高的问题。

    

    生成式检索 (GR) 在小规模语料库的情况下表现出色，通过记忆模型参数来隐式地实现查询和文档的深度交互。然而，这种记忆机制面临三个问题：(1) 对文档的细粒度特征的记忆准确性较差；(2) 随着语料库规模的增加，记忆混淆程度越来越严重；(3) 对新文档的记忆更新成本巨大。为了缓解这些问题，我们提出了生成式密集检索（GDR）的范式。具体而言，GDR首先使用有限的内存容量，从查询到相关文档簇实现簇间匹配。然后，引入无记忆的密集检索（DR）匹配机制，从簇到相关文档进行细粒度的簇内匹配。这种从粗到细的过程最大化了GR深度交互和DR的优势。

    Generative Retrieval (GR), autoregressively decoding relevant document identifiers given a query, has been shown to perform well under the setting of small-scale corpora. By memorizing the document corpus with model parameters, GR implicitly achieves deep interaction between query and document. However, such a memorizing mechanism faces three drawbacks: (1) Poor memory accuracy for fine-grained features of documents; (2) Memory confusion gets worse as the corpus size increases; (3) Huge memory update costs for new documents. To alleviate these problems, we propose the Generative Dense Retrieval (GDR) paradigm. Specifically, GDR first uses the limited memory volume to achieve inter-cluster matching from query to relevant document clusters. Memorizing-free matching mechanism from Dense Retrieval (DR) is then introduced to conduct fine-grained intra-cluster matching from clusters to relevant documents. The coarse-to-fine process maximizes the advantages of GR's deep interaction and DR's s
    
[^10]: 基于彩票票据假设和知识蒸馏的神经网络剪枝技术增强推荐系统的可扩展性

    Enhancing Scalability in Recommender Systems through Lottery Ticket Hypothesis and Knowledge Distillation-based Neural Network Pruning. (arXiv:2401.10484v1 [cs.IR])

    [http://arxiv.org/abs/2401.10484](http://arxiv.org/abs/2401.10484)

    本研究通过将彩票票据假设和知识蒸馏框架相结合，提出了一种剪枝神经网络的创新方法，以解决推荐系统在边缘设备上的可扩展性问题。经实验证明，该方法能够有效地降低功耗和模型尺寸，并实现高达66.67%的GPU计算功率减少。此外，本研究还首次应用了彩票票据假设和知识蒸馏技术于推荐系统领域，对该领域作出了重要贡献。

    

    本研究介绍了一种创新的方法，旨在高效地剪枝神经网络，特别关注它们在边缘设备上的部署。我们的方法将彩票票据假设（LTH）与知识蒸馏（KD）框架相结合，形成了三种不同的剪枝模型。这些模型旨在解决推荐系统中的可扩展性问题，其中深度学习模型的复杂性妨碍了它们的实际部署。通过巧妙应用剪枝技术，我们有效地降低了功耗和模型尺寸，同时不影响准确性。我们使用两个真实世界数据集对两个基准进行了实证评估。令人满意的是，我们的方法在GPU计算功率上实现了高达66.67%的降低。值得注意的是，我们的研究通过首次应用LTH和KD技术，对推荐系统领域做出了贡献。

    This study introduces an innovative approach aimed at the efficient pruning of neural networks, with a particular focus on their deployment on edge devices. Our method involves the integration of the Lottery Ticket Hypothesis (LTH) with the Knowledge Distillation (KD) framework, resulting in the formulation of three distinct pruning models. These models have been developed to address scalability issue in recommender systems, whereby the complexities of deep learning models have hindered their practical deployment. With judicious application of the pruning techniques, we effectively curtail the power consumption and model dimensions without compromising on accuracy. Empirical evaluation has been performed using two real world datasets from diverse domains against two baselines. Gratifyingly, our approaches yielded a GPU computation-power reduction of up to 66.67%. Notably, our study contributes to the field of recommendation system by pioneering the application of LTH and KD.
    
[^11]: 在不同偏好强度上通过多任务提升单类推荐系统

    Improving One-class Recommendation with Multi-tasking on Various Preference Intensities. (arXiv:2401.10316v1 [cs.IR])

    [http://arxiv.org/abs/2401.10316](http://arxiv.org/abs/2401.10316)

    本研究提出了一个多任务框架，考虑了隐式反馈中不同偏好强度的情况，并在其中引入了注意力图卷积层来探索高阶关系。这使得表示更具鲁棒性和泛化性。

    

    在单类推荐问题中，需要根据用户的隐式反馈来进行推荐，该反馈是通过用户的行为和不行为进行推断的。现有的方法通过编码来自训练数据中观察到的积极和消极交互来获取用户和物品的表示。然而，这些方法假设隐式反馈中的所有积极信号都反映了固定的偏好强度，这是不现实的。因此，用这些方法学习到的表示通常无法捕捉反映不同偏好强度的信息性实体特征。在本文中，我们提出了一个多任务框架，考虑了隐式反馈中每个信号的不同偏好强度。实体的表示需要同时满足每个子任务的目标，使其更加稳健和泛化。此外，我们还将注意力图卷积层引入到用户-物品的高阶关系中进行探索。

    In the one-class recommendation problem, it's required to make recommendations basing on users' implicit feedback, which is inferred from their action and inaction. Existing works obtain representations of users and items by encoding positive and negative interactions observed from training data. However, these efforts assume that all positive signals from implicit feedback reflect a fixed preference intensity, which is not realistic. Consequently, representations learned with these methods usually fail to capture informative entity features that reflect various preference intensities.  In this paper, we propose a multi-tasking framework taking various preference intensities of each signal from implicit feedback into consideration. Representations of entities are required to satisfy the objective of each subtask simultaneously, making them more robust and generalizable. Furthermore, we incorporate attentive graph convolutional layers to explore high-order relationships in the user-item
    
[^12]: 大型语言模型中地理位置嵌入方法的系统综述：迈向空间人工智能系统的路径

    A systematic review of geospatial location embedding approaches in large language models: A path to spatial AI systems. (arXiv:2401.10279v1 [cs.IR])

    [http://arxiv.org/abs/2401.10279](http://arxiv.org/abs/2401.10279)

    这篇论文系统综述了在大型语言模型中的地理位置嵌入方法，提出了四种主要的嵌入主题，并强调了在空间形态和生成模态方面进一步发展的需求。

    

    地理位置嵌入（GLE）帮助大型语言模型（LLM）吸收和分析空间数据。GLE在地理人工智能（GeoAI）中的出现是由于我们复杂当代空间中对更深入的地理认知的需求以及LLM在生成人工智能中提取深层含义的成功。我们在Google Scholar、Science Direct和arXiv上搜索了关于地理位置嵌入和LLM的论文，并审查了着重于通过LLM实现更深入空间“知识”的文章。我们筛选了304个标题、30个摘要和18篇全文论文，揭示了四个GLE主题 - 实体位置嵌入（ELE）、文档位置嵌入（DLE）、序列位置嵌入（SLE）和令牌位置嵌入（TLE）。综述以表格和叙述的形式呈现，包括“空间”和“LLM”之间的对话。尽管GLE通过叠加空间数据有助于理解空间，但强调了在空间形态和生成模态方面的进一步发展的需求。

    Geospatial Location Embedding (GLE) helps a Large Language Model (LLM) assimilate and analyze spatial data. GLE emergence in Geospatial Artificial Intelligence (GeoAI) is precipitated by the need for deeper geospatial awareness in our complex contemporary spaces and the success of LLMs in extracting deep meaning in Generative AI. We searched Google Scholar, Science Direct, and arXiv for papers on geospatial location embedding and LLM and reviewed articles focused on gaining deeper spatial "knowing" through LLMs. We screened 304 titles, 30 abstracts, and 18 full-text papers that reveal four GLE themes - Entity Location Embedding (ELE), Document Location Embedding (DLE), Sequence Location Embedding (SLE), and Token Location Embedding (TLE). Synthesis is tabular and narrative, including a dialogic conversation between "Space" and "LLM." Though GLEs aid spatial understanding by superimposing spatial data, they emphasize the need to advance in the intricacies of spatial modalities and gener
    
[^13]: 基于知识图谱驱动的图神经网络推荐模型

    Knowledge graph driven recommendation model of graph neural network. (arXiv:2401.10244v1 [cs.IR])

    [http://arxiv.org/abs/2401.10244](http://arxiv.org/abs/2401.10244)

    提出了一种基于知识图谱的图神经网络推荐模型KGLN，通过合并节点特征、调整聚合权重和迭代演化，提高了个性化推荐的准确性和效果。在实验中相对于已有基准方法，KGLN在不同数据集上的AUC提高了0.3%至5.9%和1.1%至8.2%。

    

    提出了一种新的基于图神经网络的推荐模型KGLN，该模型利用知识图谱（KG）信息，提高了个性化推荐的准确性和效果。该模型首先利用单层神经网络将图中的个体节点特征合并，然后通过结合影响因素调整相邻实体的聚合权重。通过迭代，模型从单层逐渐演变为多层，使实体能够获取丰富的多阶关联实体信息。最后，将实体和用户的特征结合起来产生推荐分数。通过比较不同聚合方法和影响因素的效果，评估了模型的性能。在使用MovieLen-1M和Book-Crossing数据集进行测试时，KGLN相对于LibFM和D等已有基准方法，AUC（ROC曲线下的面积）提高了0.3%至5.9%和1.1%至8.2%。

    A new graph neural network-based recommendation model called KGLN, which leverages Knowledge Graph (KG) information, was developed to enhance the accuracy and effectiveness of personalized recommendations. This model begins by using a single-layer neural network to merge individual node features in the graph. It then adjusts the aggregation weights of neighboring entities by incorporating influence factors. The model evolves from a single layer to multiple layers through iteration, enabling entities to access extensive multi-order associated entity information. The final step involves integrating features of entities and users to produce a recommendation score. The model's performance was evaluated by comparing its effects on various aggregation methods and influence factors. In tests using the MovieLen-1M and Book-Crossing datasets, KGLN showed an AUC (Area Under the ROC curve) improvement of 0.3% to 5.9% and 1.1% to 8.2%, respectively, over established benchmark methods like LibFM, D
    
[^14]: 使用无监督相似度度量进行源代码克隆检测

    Source Code Clone Detection Using Unsupervised Similarity Measures. (arXiv:2401.09885v1 [cs.SE])

    [http://arxiv.org/abs/2401.09885](http://arxiv.org/abs/2401.09885)

    本研究对使用无监督相似度度量进行源代码克隆检测进行了比较分析，旨在为软件工程师提供指导，以选择适合其特定用例的方法。

    

    由于在软件工程任务中克隆检测和代码搜索与推荐的重要性，对源代码的相似性进行评估近年来引起了广泛关注。本研究提出了一种比较分析无监督相似度度量用于识别源代码克隆检测的方法。目标是概述目前的最新技术、它们的优点和缺点。为了达到这个目标，我们编译了现有的无监督策略，并评估其在基准数据集上的性能，以指导软件工程师在选择适用于其特定用例的方法时提供指导。本研究的源代码可在\url{https://github.com/jorge-martinez-gil/codesim}上获得。

    Assessing similarity in source code has gained significant attention in recent years due to its importance in software engineering tasks such as clone detection and code search and recommendation. This work presents a comparative analysis of unsupervised similarity measures for identifying source code clone detection. The goal is to overview the current state-of-the-art techniques, their strengths, and weaknesses. To do that, we compile the existing unsupervised strategies and evaluate their performance on a benchmark dataset to guide software engineers in selecting appropriate methods for their specific use cases. The source code of this study is available at \url{https://github.com/jorge-martinez-gil/codesim}
    
[^15]: 跨领域序列推荐的综述

    A Survey on Cross-Domain Sequential Recommendation. (arXiv:2401.04971v1 [cs.IR])

    [http://arxiv.org/abs/2401.04971](http://arxiv.org/abs/2401.04971)

    跨领域序列推荐通过集成和学习多个领域的交互信息，将用户偏好建模从平面转向立体。文章对CDSR问题进行了定义和分析，提供了从宏观和微观两个视角的系统概述。对于不同领域间的模型，总结了多层融合结构和融合桥梁。对于现有模型，讨论了基础技术和辅助学习技术。展示了公开数据集和实验结果，并给出了未来发展的见解。

    

    跨领域序列推荐（CDSR）通过在不同粒度（从序列间到序列内，从单领域到跨领域）上集成和学习来自多个领域的交互信息，将用户偏好建模从平面转向了立体。本综述文章中，我们首先使用四维张量定义了CDSR问题，并分析了其在多维度降维下的多类型输入表示。接下来，我们从整体和细节两个视角提供了系统的概述。从整体视角，我们总结了各个模型在不同领域间的多层融合结构，并讨论了它们的融合桥梁。从细节视角，我们着重讨论了现有模型的基础技术，并解释了辅助学习技术。最后，我们展示了可用的公开数据集和代表性的实验结果，并提供了对未来发展的一些见解。

    Cross-domain sequential recommendation (CDSR) shifts the modeling of user preferences from flat to stereoscopic by integrating and learning interaction information from multiple domains at different granularities (ranging from inter-sequence to intra-sequence and from single-domain to cross-domain).In this survey, we initially define the CDSR problem using a four-dimensional tensor and then analyze its multi-type input representations under multidirectional dimensionality reductions. Following that, we provide a systematic overview from both macro and micro views. From a macro view, we abstract the multi-level fusion structures of various models across domains and discuss their bridges for fusion. From a micro view, focusing on the existing models, we specifically discuss the basic technologies and then explain the auxiliary learning technologies. Finally, we exhibit the available public datasets and the representative experimental results as well as provide some insights into future d
    
[^16]: 用大型语言模型改善文本嵌入

    Improving Text Embeddings with Large Language Models. (arXiv:2401.00368v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2401.00368](http://arxiv.org/abs/2401.00368)

    本文介绍了一种使用只用合成数据和少量训练步骤获取高质量文本嵌入的简单方法，并且在没有使用标记数据的情况下，在竞争激烈的文本嵌入基准上取得了强大的性能。

    

    在本文中，我们介绍了一种新颖且简单的方法，仅使用合成数据和少于1k个训练步骤即可获得高质量的文本嵌入。与现有方法不同，现有方法往往依赖多阶段中间预训练，使用数十亿个弱监督文本对进行训练，然后再使用少量标记数据进行微调，我们的方法不需要构建复杂的训练流程，也不依赖于通常受任务多样性和语言覆盖范围限制的手动收集的数据集。我们利用专有的LLM来为近100种语言的数十万个文本嵌入任务生成多样的合成数据。然后，我们使用标准的对比损失在合成数据上微调开源的只有解码器的LLM。实验证明，我们的方法在竞争激烈的文本嵌入基准上取得了出色的性能，而且没有使用任何标记数据。此外，当与合成数据和标记数据的混合进行微调时，我们的模型创造了新的

    In this paper, we introduce a novel and simple method for obtaining high-quality text embeddings using only synthetic data and less than 1k training steps. Unlike existing methods that often depend on multi-stage intermediate pre-training with billions of weakly-supervised text pairs, followed by fine-tuning with a few labeled datasets, our method does not require building complex training pipelines or relying on manually collected datasets that are often constrained by task diversity and language coverage. We leverage proprietary LLMs to generate diverse synthetic data for hundreds of thousands of text embedding tasks across nearly 100 languages. We then fine-tune open-source decoder-only LLMs on the synthetic data using standard contrastive loss. Experiments demonstrate that our method achieves strong performance on highly competitive text embedding benchmarks without using any labeled data. Furthermore, when fine-tuned with a mixture of synthetic and labeled data, our model sets new
    
[^17]: 基于生成型大规模语言模型的上下文驱动的交互式查询模拟

    Context-Driven Interactive Query Simulations Based on Generative Large Language Models. (arXiv:2312.09631v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2312.09631](http://arxiv.org/abs/2312.09631)

    本论文介绍了一种基于生成型大规模语言模型的上下文驱动的交互式查询模拟方法，该方法考虑了用户的上下文，并展示了比传统方法更好的有效性和更高效的信息检索会话模拟。

    

    模拟用户交互可以更加面向用户评估信息检索(IR)系统。然而，许多方法在真实用户行为方面缺乏真实性。尤其是，当前用户模型忽视了用户的上下文，而上下文是感知相关性和与搜索结果交互的主要驱动因素。为此，本研究引入了上下文驱动的查询重构模拟。所提出的查询生成方法基于最新的大规模语言模型(LLM)方法，并在整个搜索会话的模拟过程中考虑用户的上下文。与简单的无上下文查询生成方法相比，这些方法显示出更好的有效性，并允许模拟更高效的IR会话。类似地，我们的评估考虑了比当前基于会话的度量更多的交互上下文，并在已建立的评估过程中揭示了有趣的补充洞见。

    Simulating user interactions enables a more user-oriented evaluation of information retrieval (IR) systems. While user simulations are cost-efficient and reproducible, many approaches often lack fidelity regarding real user behavior. Most notably, current user models neglect the user's context, which is the primary driver of perceived relevance and the interactions with the search results. To this end, this work introduces the simulation of context-driven query reformulations. The proposed query generation methods build upon recent Large Language Model (LLM) approaches and consider the user's context throughout the simulation of a search session. Compared to simple context-free query generation approaches, these methods show better effectiveness and allow the simulation of more efficient IR sessions. Similarly, our evaluations consider more interaction context than current session-based measures and reveal interesting complementary insights in addition to the established evaluation pro
    
[^18]: 利用自注意力机制对连续音乐推荐进行负面信号的利用

    Leveraging Negative Signals with Self-Attention for Sequential Music Recommendation. (arXiv:2309.11623v1 [cs.IR])

    [http://arxiv.org/abs/2309.11623](http://arxiv.org/abs/2309.11623)

    本研究利用自我注意力机制和负面反馈，提出了用于顺序音乐推荐的Transformer模型，并采用对比学习任务来提高推荐准确性。

    

    音乐流媒体服务仰赖其推荐引擎连续向用户提供内容。因此，顺序推荐已经引起了当前文献的相当关注，而当今最先进的方法主要集中在利用上下文信息（如长期和短期用户历史和项目特征）的自我关注模型上；然而，大多数研究集中在长格式内容领域（零售、电影等）而不是短格式，例如音乐。此外，许多研究未探索在训练过程中如何融入负面会话级反馈。在本研究中，我们研究了基于Transformer的自我关注体系结构，以学习用于顺序音乐推荐的隐式会话级信息。此外，我们还提出了一种对比学习任务，以融入负面反馈（例如跳过的曲目）以促进正面命中并惩罚负面命中。这个任务被形式化为一个简单的损失项，可以加入到训练中。

    Music streaming services heavily rely on their recommendation engines to continuously provide content to their consumers. Sequential recommendation consequently has seen considerable attention in current literature, where state of the art approaches focus on self-attentive models leveraging contextual information such as long and short-term user history and item features; however, most of these studies focus on long-form content domains (retail, movie, etc.) rather than short-form, such as music. Additionally, many do not explore incorporating negative session-level feedback during training. In this study, we investigate the use of transformer-based self-attentive architectures to learn implicit session-level information for sequential music recommendation. We additionally propose a contrastive learning task to incorporate negative feedback (e.g skipped tracks) to promote positive hits and penalize negative hits. This task is formulated as a simple loss term that can be incorporated in
    
[^19]: 信息检索中的大型语言模型：一项综述

    Large Language Models for Information Retrieval: A Survey. (arXiv:2308.07107v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2308.07107](http://arxiv.org/abs/2308.07107)

    本综述将大型语言模型（LLMs）在信息检索中的发展进行了综述，探讨了其在捕捉上下文信号和语义细微之处方面的优势和挑战，以及与传统检索方法的结合的重要性。

    

    作为信息获取的主要手段，信息检索（IR）系统，如搜索引擎，已经融入到我们的日常生活中。这些系统还作为对话、问答和推荐系统的组成部分。IR的发展轨迹从基于词项的方法起步，逐渐发展成与先进的神经模型相融合。尽管神经模型擅长捕捉复杂的上下文信号和语义细微之处，从而改变了IR的格局，但它们仍然面临着数据稀缺、可解释性以及生成上下文合理但潜在不准确响应的挑战。这种演变需要传统方法（如基于词项的稀疏检索方法与快速响应）和现代神经架构（如具有强大语言理解能力的语言模型）的结合。与此同时，大型语言模型（LLMs），如ChatGPT和GPT-4的出现，引起了一场革命

    As a primary means of information acquisition, information retrieval (IR) systems, such as search engines, have integrated themselves into our daily lives. These systems also serve as components of dialogue, question-answering, and recommender systems. The trajectory of IR has evolved dynamically from its origins in term-based methods to its integration with advanced neural models. While the neural models excel at capturing complex contextual signals and semantic nuances, thereby reshaping the IR landscape, they still face challenges such as data scarcity, interpretability, and the generation of contextually plausible yet potentially inaccurate responses. This evolution requires a combination of both traditional methods (such as term-based sparse retrieval methods with rapid response) and modern neural architectures (such as language models with powerful language understanding capacity). Meanwhile, the emergence of large language models (LLMs), typified by ChatGPT and GPT-4, has revolu
    

