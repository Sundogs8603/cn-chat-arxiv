# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Discovering and exploring cases of educational source code plagiarism with Dolos](https://arxiv.org/abs/2402.10853) | Dolos 是一个用于检测和预防教育源代码抄袭的工具生态系统，在最新版本中加强了用户体验，教育工作者可以通过新的 Web 应用程序在浏览器中运行整个抄袭检测流程，无需安装或配置。 |
| [^2] | [Generative Cross-Modal Retrieval: Memorizing Images in Multimodal Language Models for Retrieval and Beyond](https://arxiv.org/abs/2402.10805) | 提出了一种生成式跨模态检索框架，在多模态语言模型中实现了存储和检索图像的能力 |
| [^3] | [Distillation Enhanced Generative Retrieval](https://arxiv.org/abs/2402.10769) | 通过蒸馏方法增强生成式检索系统，提出了一种名为DGR的框架，利用先进排名模型和蒸馏RankNet损失来优化模型。 |
| [^4] | [FairSync: Ensuring Amortized Group Exposure in Distributed Recommendation Retrieval](https://arxiv.org/abs/2402.10628) | FairSync算法同时考虑了推荐系统的检索阶段和排名阶段，确保了分布式推荐检索中的群体曝光均衡。 |
| [^5] | [Are ID Embeddings Necessary? Whitening Pre-trained Text Embeddings for Effective Sequential Recommendation](https://arxiv.org/abs/2402.10602) | 深入研究表明，预训练文本嵌入存在各向异性语义空间，其高余弦相似度妨碍了推荐模型对项目表示进行有效区分，我们提出使用白化处理来解决这个问题 |
| [^6] | [SPAR: Personalized Content-Based Recommendation via Long Engagement Attention](https://arxiv.org/abs/2402.10555) | SPAR是一个基于内容的推荐框架，通过利用PLM、多注意力层和注意力稀疏机制，在会话级别有效地处理长期用户参与历史，提取全面用户兴趣，实现个性化推荐。 |
| [^7] | [Cognitive Personalized Search Integrating Large Language Models with an Efficient Memory Mechanism](https://arxiv.org/abs/2402.10548) | 本文提出了一种认知个性化搜索（CoPS）模型，通过将大型语言模型（LLMs）与启发自人类认知的认知记忆机制相结合，以改善用户建模和用户搜索体验。 |
| [^8] | [Understanding Survey Paper Taxonomy about Large Language Models via Graph Representation Learning](https://arxiv.org/abs/2402.10409) | 通过图结构信息在共类别图上利用图表示学习技术，可以在LLMs的预训练模型微调和零-shot/few-shot分类方面显著优于语言模型，揭示了弱标签微调LLMs的潜力。 |
| [^9] | [UMAIR-FPS: User-aware Multi-modal Animation Illustration Recommendation Fusion with Painting Style](https://arxiv.org/abs/2402.10381) | UMAIR-FPS提出了一种新的用户感知多模态动画插画推荐系统，通过融合图像绘画风格特征和语义特征来增强表示。 |
| [^10] | [Engraving Oriented Joint Estimation of Pitch Spelling and Local and Global Keys](https://arxiv.org/abs/2402.10247) | 这个算法提出了一个联合估计音高符号和调性猜测问题的新方法，不仅可以识别全局调性，还可以在整个分析乐曲中识别局部调性，通过动态规划技术实现高准确度。 |
| [^11] | [Unified Hallucination Detection for Multimodal Large Language Models](https://arxiv.org/abs/2402.03190) | 该论文提出了一个新颖的统一的多模态幻觉检测框架UNIHD，并设计了一个评估基准方法MHaluBench来评估幻觉检测方法的进展。这项工作扩展了幻觉检测的研究范围并提供了有效的解决方案。 |
| [^12] | [CETN: Contrast-enhanced Through Network for CTR Prediction](https://arxiv.org/abs/2312.09715) | 提出了一个简单而有效的新型CTR模型CETN，用于确保特征交互信息的多样性和同质性 |
| [^13] | [Artificial Intelligence Model for Tumoral Clinical Decision Support Systems](https://arxiv.org/abs/2301.03701) | 该研究提出的人工智能模型利用二进制信息生成丰富的图像描述符，从而实现了在肿瘤临床决策支持系统中检测患者特征并推荐最相似病例的目标。 |
| [^14] | [Modeling Attrition in Recommender Systems with Departing Bandits](https://arxiv.org/abs/2203.13423) | 本论文提出了一个新型多臂赌博机设置，捕捉了推荐系统中用户离开的情况，首次证明了在所有用户共享相同类型时，基于UCB的算法是最优的。 |
| [^15] | [History-Aware Conversational Dense Retrieval.](http://arxiv.org/abs/2401.16659) | 该论文提出了一种历史感知的对话式稠密检索系统，通过上下文去噪的查询重构以及根据历史轮次的实际影响自动挖掘监督信号改进了现有的对话式稠密检索方法。 |
| [^16] | [Synthesizing Political Zero-Shot Relation Classification via Codebook Knowledge, NLI, and ChatGPT.](http://arxiv.org/abs/2308.07876) | 该论文通过利用已建立的注释编码本的知识，探索零样本方法用于政治事件本体关系分类，并介绍一种基于自然语言推理的方法，名为ZSP。ZSP采用了一种树查询框架，提高了解释性、效率和对模式更改的适应性。在细粒度根代码分类上，ZSP的性能明显优于ChatGPT，F1得分提高了40%。 |

# 详细

[^1]: 使用Dolos发现和探索教育源代码抄袭案例

    Discovering and exploring cases of educational source code plagiarism with Dolos

    [https://arxiv.org/abs/2402.10853](https://arxiv.org/abs/2402.10853)

    Dolos 是一个用于检测和预防教育源代码抄袭的工具生态系统，在最新版本中加强了用户体验，教育工作者可以通过新的 Web 应用程序在浏览器中运行整个抄袭检测流程，无需安装或配置。

    

    源代码抄袭在教育实践中是一个重要问题，教育工作者需要易于使用的工具来应对这种学术不端行为。本文介绍了 Dolos 的最新版本，一个用于检测和预防教育源代码抄袭的工具生态系统。在这个新版本中，主要侧重于提升用户体验。教育工作者现在可以在他们的浏览器中通过一个新的 Web 应用程序运行整个抄袭检测流程，无需安装或配置。完全重新设计的分析仪表板可以即时评估一组源文件是否包含疑似抄袭案例以及集合中抄袭有多普遍。仪表板支持分层结构化导航，以便轻松缩放到疑似案例。集群是仪表板设计中一个重要的新组件，反映了

    arXiv:2402.10853v1 Announce Type: cross  Abstract: Source code plagiarism is a significant issue in educational practice, and educators need user-friendly tools to cope with such academic dishonesty. This article introduces the latest version of Dolos, a state-of-the-art ecosystem of tools for detecting and preventing plagiarism in educational source code. In this new version, the primary focus has been on enhancing the user experience. Educators can now run the entire plagiarism detection pipeline from a new web app in their browser, eliminating the need for any installation or configuration. Completely redesigned analytics dashboards provide an instant assessment of whether a collection of source files contains suspected cases of plagiarism and how widespread plagiarism is within the collection. The dashboards support hierarchically structured navigation to facilitate zooming in and out of suspect cases. Clusters are an essential new component of the dashboard design, reflecting the 
    
[^2]: 生成式跨模态检索：在多模态语言模型中存储图像用于检索及更多应用

    Generative Cross-Modal Retrieval: Memorizing Images in Multimodal Language Models for Retrieval and Beyond

    [https://arxiv.org/abs/2402.10805](https://arxiv.org/abs/2402.10805)

    提出了一种生成式跨模态检索框架，在多模态语言模型中实现了存储和检索图像的能力

    

    近期生成式语言模型的进展表明其能够记忆文档中的知识并有效地回答用户查询。在此能力基础上，我们提出了使多模态大型语言模型（MLLMs）能够在其参数内存储和检索图像的方法。给定用户对视觉内容的查询，MLLM被期望能够从其参数中“回忆”相关图像作为响应。实现这一目标面临着显著挑战，其中包括MLLM内置的视觉记忆和视觉检索方案。为解决这些挑战，我们引入了一个生成式跨模态检索框架，该框架为图像分配唯一标识符字符串，并涉及两个训练步骤：学习记忆和学习检索。第一步侧重于训练MLLM记忆图像与其标识符之间的关联。

    arXiv:2402.10805v1 Announce Type: cross  Abstract: The recent advancements in generative language models have demonstrated their ability to memorize knowledge from documents and recall knowledge to respond to user queries effectively. Building upon this capability, we propose to enable multimodal large language models (MLLMs) to memorize and recall images within their parameters. Given a user query for visual content, the MLLM is anticipated to "recall" the relevant image from its parameters as the response. Achieving this target presents notable challenges, including inbuilt visual memory and visual recall schemes within MLLMs. To address these challenges, we introduce a generative cross-modal retrieval framework, which assigns unique identifier strings to represent images and involves two training steps: learning to memorize and learning to retrieve. The first step focuses on training the MLLM to memorize the association between images and their respective identifiers. The latter ste
    
[^3]: 蒸馏增强生成式检索

    Distillation Enhanced Generative Retrieval

    [https://arxiv.org/abs/2402.10769](https://arxiv.org/abs/2402.10769)

    通过蒸馏方法增强生成式检索系统，提出了一种名为DGR的框架，利用先进排名模型和蒸馏RankNet损失来优化模型。

    

    生成式检索是文本检索中的一种新兴范式，通过生成相关段落的标识符字符串作为检索目标。该范式利用强大的生成式语言模型，不同于传统的稀疏或密集检索方法。本研究确定了通过蒸馏进一步增强生成式检索的可行方向，并提出了一个名为DGR的可行框架。DGR利用诸如跨编码器等先进排名模型，在教师角色中提供段落排名列表，捕获段落的不同相关程度，而不是二元硬标签；随后，DGR采用一种特别设计的蒸馏RankNet损失来优化生成式检索模型，考虑教师模型提供的段落排名顺序作为标签。该框架仅需要额外的蒸馏步骤来增强当前的生成式检索系统，并不增加任何负担。

    arXiv:2402.10769v1 Announce Type: cross  Abstract: Generative retrieval is a promising new paradigm in text retrieval that generates identifier strings of relevant passages as the retrieval target. This paradigm leverages powerful generative language models, distinct from traditional sparse or dense retrieval methods. In this work, we identify a viable direction to further enhance generative retrieval via distillation and propose a feasible framework, named DGR. DGR utilizes sophisticated ranking models, such as the cross-encoder, in a teacher role to supply a passage rank list, which captures the varying relevance degrees of passages instead of binary hard labels; subsequently, DGR employs a specially designed distilled RankNet loss to optimize the generative retrieval model, considering the passage rank order provided by the teacher model as labels. This framework only requires an additional distillation step to enhance current generative retrieval systems and does not add any burden
    
[^4]: FairSync: 确保在分布式推荐检索中的摊销群体曝光

    FairSync: Ensuring Amortized Group Exposure in Distributed Recommendation Retrieval

    [https://arxiv.org/abs/2402.10628](https://arxiv.org/abs/2402.10628)

    FairSync算法同时考虑了推荐系统的检索阶段和排名阶段，确保了分布式推荐检索中的群体曝光均衡。

    

    为了追求公平和平衡发展，推荐系统（RS）通常优先考虑群体公平，确保特定群体在一定周期内维持最小水平的曝光。本文讨论了如何确保在分布式推荐检索中的群体曝光平衡，提出了FairSync算法，通过同时考虑检索阶段和排名阶段的方法来实现群体曝光的均衡。

    arXiv:2402.10628v1 Announce Type: new  Abstract: In pursuit of fairness and balanced development, recommender systems (RS) often prioritize group fairness, ensuring that specific groups maintain a minimum level of exposure over a given period. For example, RS platforms aim to ensure adequate exposure for new providers or specific categories of items according to their needs. Modern industry RS usually adopts a two-stage pipeline: stage-1 (retrieval stage) retrieves hundreds of candidates from millions of items distributed across various servers, and stage-2 (ranking stage) focuses on presenting a small-size but accurate selection from items chosen in stage-1. Existing efforts for ensuring amortized group exposures focus on stage-2, however, stage-1 is also critical for the task. Without a high-quality set of candidates, the stage-2 ranker cannot ensure the required exposure of groups. Previous fairness-aware works designed for stage-2 typically require accessing and traversing all item
    
[^5]: 是否需要ID Embeddings？为了提高序列推荐的有效性，对预训练文本嵌入进行白化处理

    Are ID Embeddings Necessary? Whitening Pre-trained Text Embeddings for Effective Sequential Recommendation

    [https://arxiv.org/abs/2402.10602](https://arxiv.org/abs/2402.10602)

    深入研究表明，预训练文本嵌入存在各向异性语义空间，其高余弦相似度妨碍了推荐模型对项目表示进行有效区分，我们提出使用白化处理来解决这个问题

    

    最近的序列推荐模型将物品的预训练文本嵌入与物品ID嵌入相结合，以实现更优越的推荐性能。尽管它们很有效，但这些模型中文本特征的表现力仍然大多未被探索。大多数现有模型强调推荐中ID嵌入的重要性，而我们的研究更进一步，研究仅依赖文本特征而不需要ID嵌入的序列推荐模型。通过实验检验预训练文本嵌入，我们发现它们存在于一个各向异性语义空间中，项目之间的平均余弦相似度超过0.8。我们还证明，这种各向异性特性阻碍了推荐模型有效区分项目表示，并导致性能退化。为解决这个问题，我们提出采用一个名为白化的预处理步骤

    arXiv:2402.10602v1 Announce Type: new  Abstract: Recent sequential recommendation models have combined pre-trained text embeddings of items with item ID embeddings to achieve superior recommendation performance. Despite their effectiveness, the expressive power of text features in these models remains largely unexplored. While most existing models emphasize the importance of ID embeddings in recommendations, our study takes a step further by studying sequential recommendation models that only rely on text features and do not necessitate ID embeddings. Upon examining pretrained text embeddings experimentally, we discover that they reside in an anisotropic semantic space, with an average cosine similarity of over 0.8 between items. We also demonstrate that this anisotropic nature hinders recommendation models from effectively differentiating between item representations and leads to degenerated performance. To address this issue, we propose to employ a pre-processing step known as whiten
    
[^6]: SPAR：通过长期参与注意力实现个性化基于内容的推荐

    SPAR: Personalized Content-Based Recommendation via Long Engagement Attention

    [https://arxiv.org/abs/2402.10555](https://arxiv.org/abs/2402.10555)

    SPAR是一个基于内容的推荐框架，通过利用PLM、多注意力层和注意力稀疏机制，在会话级别有效地处理长期用户参与历史，提取全面用户兴趣，实现个性化推荐。

    

    利用用户长期参与历史对个性化内容推荐至关重要。预训练语言模型（PLMs）在自然语言处理领域的成功导致它们被用于编码用户历史和候选项，将内容推荐视为文本语义匹配任务。然而，现有工作仍然在处理非常长的用户历史文本和不足的用户-物品交互方面存在困难。本文介绍了一种基于内容的推荐框架SPAR，有效应对了从长期用户参与历史中提取全面用户兴趣的挑战。它通过利用PLM、多注意力层和注意力稀疏机制以会话为基础对用户的历史进行编码。用户和物品侧特征被充分融合进行参与预测，同时保持双方的独立表示，这对于实际模型部署是有效的。

    arXiv:2402.10555v1 Announce Type: cross  Abstract: Leveraging users' long engagement histories is essential for personalized content recommendations. The success of pretrained language models (PLMs) in NLP has led to their use in encoding user histories and candidate items, framing content recommendations as textual semantic matching tasks. However, existing works still struggle with processing very long user historical text and insufficient user-item interaction. In this paper, we introduce a content-based recommendation framework, SPAR, which effectively tackles the challenges of holistic user interest extraction from the long user engagement history. It achieves so by leveraging PLM, poly-attention layers and attention sparsity mechanisms to encode user's history in a session-based manner. The user and item side features are sufficiently fused for engagement prediction while maintaining standalone representations for both sides, which is efficient for practical model deployment. Mor
    
[^7]: 将大型语言模型与高效记忆机制整合的认知个性化搜索

    Cognitive Personalized Search Integrating Large Language Models with an Efficient Memory Mechanism

    [https://arxiv.org/abs/2402.10548](https://arxiv.org/abs/2402.10548)

    本文提出了一种认知个性化搜索（CoPS）模型，通过将大型语言模型（LLMs）与启发自人类认知的认知记忆机制相结合，以改善用户建模和用户搜索体验。

    

    传统搜索引擎通常为所有用户提供相同的搜索结果，忽视了个体偏好。为了解决这一局限性，发展了个性化搜索，根据查询日志中获取的用户偏好重新排序结果。基于深度学习的个性化搜索方法显示出潜力，但它们严重依赖丰富的训练数据，使其容易受到数据稀疏挑战的影响。本文提出了一种认知个性化搜索（CoPS）模型，该模型将大型语言模型（LLMs）与启发自人类认知的认知记忆机制相结合。CoPS利用LLMs来增强用户建模和用户搜索体验。认知记忆机制包括感觉记忆以快速做出感性反应，工作记忆以进行复杂的认知反应，长期记忆以存储历史互动。CoPS使用三步方法处理新查询：识别重新查找行为

    arXiv:2402.10548v1 Announce Type: new  Abstract: Traditional search engines usually provide identical search results for all users, overlooking individual preferences. To counter this limitation, personalized search has been developed to re-rank results based on user preferences derived from query logs. Deep learning-based personalized search methods have shown promise, but they rely heavily on abundant training data, making them susceptible to data sparsity challenges. This paper proposes a Cognitive Personalized Search (CoPS) model, which integrates Large Language Models (LLMs) with a cognitive memory mechanism inspired by human cognition. CoPS employs LLMs to enhance user modeling and user search experience. The cognitive memory mechanism comprises sensory memory for quick sensory responses, working memory for sophisticated cognitive responses, and long-term memory for storing historical interactions. CoPS handles new queries using a three-step approach: identifying re-finding behav
    
[^8]: 通过图表示学习理解大型语言模型调查论文分类法

    Understanding Survey Paper Taxonomy about Large Language Models via Graph Representation Learning

    [https://arxiv.org/abs/2402.10409](https://arxiv.org/abs/2402.10409)

    通过图结构信息在共类别图上利用图表示学习技术，可以在LLMs的预训练模型微调和零-shot/few-shot分类方面显著优于语言模型，揭示了弱标签微调LLMs的潜力。

    

    随着大型语言模型（LLMs）的新研究持续进行，难以跟上新的研究和模型。为帮助研究人员综合新研究成果，许多人写了调研论文，但即使这些论文也变得越来越多。本文提出了一种自动将调研论文分配到分类法的方法。我们收集了144篇LLM调研论文的元数据，并探讨了三种范例来对分类法内的论文进行分类。我们的工作表明，在共类别图上利用图结构信息可以显著优于两个范例中的语言模型; 使用LLMs进行预训练语言模型的微调和零-shot/few-shot分类。我们发现我们的模型超过了平均人类识别水平，并且利用较小模型生成的弱标签来微调LLMs（本研究中的GCN等）可能比使用地面实况标签更有效，揭示了从弱到强的潜力。

    arXiv:2402.10409v1 Announce Type: cross  Abstract: As new research on Large Language Models (LLMs) continues, it is difficult to keep up with new research and models. To help researchers synthesize the new research many have written survey papers, but even those have become numerous. In this paper, we develop a method to automatically assign survey papers to a taxonomy. We collect the metadata of 144 LLM survey papers and explore three paradigms to classify papers within the taxonomy. Our work indicates that leveraging graph structure information on co-category graphs can significantly outperform the language models in two paradigms; pre-trained language models' fine-tuning and zero-shot/few-shot classifications using LLMs. We find that our model surpasses an average human recognition level and that fine-tuning LLMs using weak labels generated by a smaller model, such as the GCN in this study, can be more effective than using ground-truth labels, revealing the potential of weak-to-stro
    
[^9]: UMAIR-FPS：带绘画风格的用户感知多模态动画插画推荐融合

    UMAIR-FPS: User-aware Multi-modal Animation Illustration Recommendation Fusion with Painting Style

    [https://arxiv.org/abs/2402.10381](https://arxiv.org/abs/2402.10381)

    UMAIR-FPS提出了一种新的用户感知多模态动画插画推荐系统，通过融合图像绘画风格特征和语义特征来增强表示。

    

    高质量基于人工智能的图像生成模型的快速进步产生了大量的动漫插画。在海量数据中向用户推荐插画已成为一项具有挑战性和受欢迎的任务。然而，现有的动漫推荐系统侧重于文本特征，但仍需要整合图像特征。此外，大多数多模态推荐研究受到紧密耦合数据集的限制，限制了其对动漫插画的适用性。我们提出了带绘画风格的用户感知多模态动画插画推荐融合（UMAIR-FPS）来解决这些问题。在特征提取阶段，对于图像特征，我们首次结合图像绘画风格特征与语义特征来构建双输出图像编码器以增强表示。对于文本特征，我们基于Fine-tuning Sentence-Transformers获得文本嵌入，通过整合领域知识

    arXiv:2402.10381v1 Announce Type: cross  Abstract: The rapid advancement of high-quality image generation models based on AI has generated a deluge of anime illustrations. Recommending illustrations to users within massive data has become a challenging and popular task. However, existing anime recommendation systems have focused on text features but still need to integrate image features. In addition, most multi-modal recommendation research is constrained by tightly coupled datasets, limiting its applicability to anime illustrations. We propose the User-aware Multi-modal Animation Illustration Recommendation Fusion with Painting Style (UMAIR-FPS) to tackle these gaps. In the feature extract phase, for image features, we are the first to combine image painting style features with semantic features to construct a dual-output image encoder for enhancing representation. For text features, we obtain text embeddings based on fine-tuning Sentence-Transformers by incorporating domain knowledg
    
[^10]: 雕刻导向的音高符号和局部及全局调性联合估计

    Engraving Oriented Joint Estimation of Pitch Spelling and Local and Global Keys

    [https://arxiv.org/abs/2402.10247](https://arxiv.org/abs/2402.10247)

    这个算法提出了一个联合估计音高符号和调性猜测问题的新方法，不仅可以识别全局调性，还可以在整个分析乐曲中识别局部调性，通过动态规划技术实现高准确度。

    

    我们提出了一个新的算法，用于从包含有关音符边界的MIDI文件中联合估计音高符号和调性猜测的问题。我们的算法不仅可以识别全局调性，还可以在整个分析乐曲中识别局部调性。它使用动态规划技术来搜索在印刷谱表中将显示的意外符号数量方面的最佳符号。对该数量的评估与全局调性以及一些局部调性的估计相结合，每个小节都有一个局部调性。这三种信息中的每一种都用于估计其他信息，在一个多步骤的过程中。对包含总共216464个音符的单音和钢琴数据集进行的评估显示，无论是音高符号（在巴赫作品集中平均为99.5%，在整个数据集上为98.2%）还是全局调性符号估计（平均为93.0%）

    arXiv:2402.10247v1 Announce Type: cross  Abstract: We revisit the problems of pitch spelling and tonality guessing with a new algorithm for their joint estimation from a MIDI file including information about the measure boundaries. Our algorithm does not only identify a global key but also local ones all along the analyzed piece. It uses Dynamic Programming techniques to search for an optimal spelling in term, roughly, of the number of accidental symbols that would be displayed in the engraved score. The evaluation of this number is coupled with an estimation of the global key and some local keys, one for each measure. Each of the three informations is used for the estimation of the other, in a multi-steps procedure. An evaluation conducted on a monophonic and a piano dataset, comprising 216 464 notes in total, shows a high degree of accuracy, both for pitch spelling (99.5% on average on the Bach corpus and 98.2% on the whole dataset) and global key signature estimation (93.0% on avera
    
[^11]: 统一的多模态大型语言模型的幻觉检测

    Unified Hallucination Detection for Multimodal Large Language Models

    [https://arxiv.org/abs/2402.03190](https://arxiv.org/abs/2402.03190)

    该论文提出了一个新颖的统一的多模态幻觉检测框架UNIHD，并设计了一个评估基准方法MHaluBench来评估幻觉检测方法的进展。这项工作扩展了幻觉检测的研究范围并提供了有效的解决方案。

    

    尽管在多模态任务方面取得了重大进展，多模态大型语言模型(MLLMs)仍然存在幻觉的严重问题。因此，可靠地检测MLLMs中的幻觉已成为模型评估和实际应用部署保障的重要方面。之前在这个领域的研究受到了狭窄的任务焦点、不足的幻觉类别涵盖范围以及缺乏详细的细粒度的限制。针对这些挑战，我们的工作扩展了幻觉检测的研究范围。我们提出了一个新颖的元评估基准方法，MHaluBench，精心设计以促进幻觉检测方法的进展评估。此外，我们揭示了一个新颖的统一多模态幻觉检测框架，UNIHD，它利用一套辅助工具来稳健地验证幻觉的发生。我们通过实验证明了UNIHD的有效性。

    Despite significant strides in multimodal tasks, Multimodal Large Language Models (MLLMs) are plagued by the critical issue of hallucination. The reliable detection of such hallucinations in MLLMs has, therefore, become a vital aspect of model evaluation and the safeguarding of practical application deployment. Prior research in this domain has been constrained by a narrow focus on singular tasks, an inadequate range of hallucination categories addressed, and a lack of detailed granularity. In response to these challenges, our work expands the investigative horizons of hallucination detection. We present a novel meta-evaluation benchmark, MHaluBench, meticulously crafted to facilitate the evaluation of advancements in hallucination detection methods. Additionally, we unveil a novel unified multimodal hallucination detection framework, UNIHD, which leverages a suite of auxiliary tools to validate the occurrence of hallucinations robustly. We demonstrate the effectiveness of UNIHD throug
    
[^12]: CETN: 用于CTR预测的增强对比网络

    CETN: Contrast-enhanced Through Network for CTR Prediction

    [https://arxiv.org/abs/2312.09715](https://arxiv.org/abs/2312.09715)

    提出了一个简单而有效的新型CTR模型CETN，用于确保特征交互信息的多样性和同质性

    

    点击率（CTR）预测在个性化信息检索中是至关重要的任务，比如工业推荐系统、在线广告和网络搜索。大多数现有的CTR预测模型利用显式特征交互来克服隐式特征交互的性能瓶颈。因此，基于并行结构（例如DCN、FinalMLP、xDeepFM）的深度CTR模型被提出，以从不同语义空间中获取联合信息。然而，这些并行子组件缺乏有效的监督信号，这使得有效捕捉不同语义空间中有价值的多视图特征交互信息具有挑战性。为解决这一问题，我们提出了一种简单而有效的新型CTR模型：用于CTR的增强对比网络（CETN），以确保特征交互信息的多样性和同质性。具体而言，CETN采用基于产品的特征交互

    arXiv:2312.09715v2 Announce Type: replace  Abstract: Click-through rate (CTR) Prediction is a crucial task in personalized information retrievals, such as industrial recommender systems, online advertising, and web search. Most existing CTR Prediction models utilize explicit feature interactions to overcome the performance bottleneck of implicit feature interactions. Hence, deep CTR models based on parallel structures (e.g., DCN, FinalMLP, xDeepFM) have been proposed to obtain joint information from different semantic spaces. However, these parallel subcomponents lack effective supervisory signals, making it challenging to efficiently capture valuable multi-views feature interaction information in different semantic spaces. To address this issue, we propose a simple yet effective novel CTR model: Contrast-enhanced Through Network for CTR (CETN), so as to ensure the diversity and homogeneity of feature interaction information. Specifically, CETN employs product-based feature interaction
    
[^13]: 肿瘤临床决策支持系统的人工智能模型

    Artificial Intelligence Model for Tumoral Clinical Decision Support Systems

    [https://arxiv.org/abs/2301.03701](https://arxiv.org/abs/2301.03701)

    该研究提出的人工智能模型利用二进制信息生成丰富的图像描述符，从而实现了在肿瘤临床决策支持系统中检测患者特征并推荐最相似病例的目标。

    

    通过比较诊断性脑瘤评估，利用医疗中心的信息来比较类似病例，提出的系统能够利用人工智能模型检索给定查询的最相似的脑瘤病例。主要目标是通过生成更准确的医学图像表示来改善诊断过程，特别关注患者特定正常特征和病理。与先前模型的关键区别在于，它能够仅从二进制信息生成丰富的图像描述符，消除了昂贵且难以获得的肿瘤分割的需求。

    arXiv:2301.03701v2 Announce Type: replace-cross  Abstract: Comparative diagnostic in brain tumor evaluation makes possible to use the available information of a medical center to compare similar cases when a new patient is evaluated. By leveraging Artificial Intelligence models, the proposed system is able of retrieving the most similar cases of brain tumors for a given query. The primary objective is to enhance the diagnostic process by generating more accurate representations of medical images, with a particular focus on patient-specific normal features and pathologies. A key distinction from previous models lies in its ability to produce enriched image descriptors solely from binary information, eliminating the need for costly and difficult to obtain tumor segmentation.   The proposed model uses Artificial Intelligence to detect patient features to recommend the most similar cases from a database. The system not only suggests similar cases but also balances the representation of hea
    
[^14]: 用离开的赌博机模型建议系统中的流失现象

    Modeling Attrition in Recommender Systems with Departing Bandits

    [https://arxiv.org/abs/2203.13423](https://arxiv.org/abs/2203.13423)

    本论文提出了一个新型多臂赌博机设置，捕捉了推荐系统中用户离开的情况，首次证明了在所有用户共享相同类型时，基于UCB的算法是最优的。

    

    在传统的多臂赌博机中，推荐系统的策略影响奖励的获取，但不影响交互的长度。然而，在现实世界中，不满足的用户可能会离开（并永远不再回来）。在这项工作中，我们提出了一个捕捉这种策略依赖性时段的新型多臂赌博机设置。我们的设置包括一个有限的用户类型集合，和多个具有伯努利回报的臂。每个（用户类型，臂）元组对应一个（未知的）奖励概率。每个用户的类型最初是未知的，只能通过其对推荐的响应来推断。此外，如果用户对他们的推荐不满意，他们可能会离开系统。我们首先解决了所有用户共享相同类型的情况，证明了最近基于UCB的算法的最优性。然后，我们转向更具挑战性的情况，即用户分为两类。

    arXiv:2203.13423v2 Announce Type: replace  Abstract: Traditionally, when recommender systems are formalized as multi-armed bandits, the policy of the recommender system influences the rewards accrued, but not the length of interaction. However, in real-world systems, dissatisfied users may depart (and never come back). In this work, we propose a novel multi-armed bandit setup that captures such policy-dependent horizons. Our setup consists of a finite set of user types, and multiple arms with Bernoulli payoffs. Each (user type, arm) tuple corresponds to an (unknown) reward probability. Each user's type is initially unknown and can only be inferred through their response to recommendations. Moreover, if a user is dissatisfied with their recommendation, they might depart the system. We first address the case where all users share the same type, demonstrating that a recent UCB-based algorithm is optimal. We then move forward to the more challenging case, where users are divided among two 
    
[^15]: 历史感知的对话式稠密检索

    History-Aware Conversational Dense Retrieval. (arXiv:2401.16659v1 [cs.IR])

    [http://arxiv.org/abs/2401.16659](http://arxiv.org/abs/2401.16659)

    该论文提出了一种历史感知的对话式稠密检索系统，通过上下文去噪的查询重构以及根据历史轮次的实际影响自动挖掘监督信号改进了现有的对话式稠密检索方法。

    

    对话搜索通过实现用户和系统之间的多轮交互，实现了复杂信息检索的便利。支持这种交互需要对对话输入有全面的理解，以便根据历史信息制定良好的搜索查询。特别是，搜索查询应包括来自先前对话回合的相关信息。然而，目前的对话式稠密检索方法主要依赖于对经过精调的预训练专门检索器进行整个对话式搜索会话的优化，这可能会变得冗长和嘈杂。此外，现有方法受现有数据集中手动监督信号数量的限制。为了解决上述问题，我们提出了一种历史感知的对话式稠密检索(HAConvDR)系统，它结合了两个思想：上下文去噪的查询重构和根据历史轮次的实际影响进行自动挖掘监督信号。

    Conversational search facilitates complex information retrieval by enabling multi-turn interactions between users and the system. Supporting such interactions requires a comprehensive understanding of the conversational inputs to formulate a good search query based on historical information. In particular, the search query should include the relevant information from the previous conversation turns. However, current approaches for conversational dense retrieval primarily rely on fine-tuning a pre-trained ad-hoc retriever using the whole conversational search session, which can be lengthy and noisy. Moreover, existing approaches are limited by the amount of manual supervision signals in the existing datasets. To address the aforementioned issues, we propose a History-Aware Conversational Dense Retrieval (HAConvDR) system, which incorporates two ideas: context-denoised query reformulation and automatic mining of supervision signals based on the actual impact of historical turns. Experime
    
[^16]: 通过编码本知识、自然语言推理和ChatGPT来合成政治零样本关系分类

    Synthesizing Political Zero-Shot Relation Classification via Codebook Knowledge, NLI, and ChatGPT. (arXiv:2308.07876v1 [cs.CL])

    [http://arxiv.org/abs/2308.07876](http://arxiv.org/abs/2308.07876)

    该论文通过利用已建立的注释编码本的知识，探索零样本方法用于政治事件本体关系分类，并介绍一种基于自然语言推理的方法，名为ZSP。ZSP采用了一种树查询框架，提高了解释性、效率和对模式更改的适应性。在细粒度根代码分类上，ZSP的性能明显优于ChatGPT，F1得分提高了40%。

    

    最近的事件编码的监督模型在性能方面远远超过模式匹配方法。然而，它们仅仅依赖于新的注释，忽视了专家数据库中的大量知识，限制了它们在细粒度分类中的适用性。为了解决这些限制，我们通过利用已建立的注释编码本的知识，探索零样本方法用于政治事件本体关系分类。我们的研究涵盖了ChatGPT和一种新颖的基于自然语言推理的方法，名为ZSP。ZSP采用了一种树查询框架，将任务分解为上下文、语态和类别消歧的不同层次。该框架提高了解释性、效率和对模式更改的适应性。通过在我们新策划的数据集上进行大量实验，我们指出了ChatGPT中的不稳定性问题，并突出了ZSP的卓越性能。ZSP在细粒度根代码分类的F1得分上取得了令人印象深刻的提高40%。

    Recent supervised models for event coding vastly outperform pattern-matching methods. However, their reliance solely on new annotations disregards the vast knowledge within expert databases, hindering their applicability to fine-grained classification. To address these limitations, we explore zero-shot approaches for political event ontology relation classification, by leveraging knowledge from established annotation codebooks. Our study encompasses both ChatGPT and a novel natural language inference (NLI) based approach named ZSP. ZSP adopts a tree-query framework that deconstructs the task into context, modality, and class disambiguation levels. This framework improves interpretability, efficiency, and adaptability to schema changes. By conducting extensive experiments on our newly curated datasets, we pinpoint the instability issues within ChatGPT and highlight the superior performance of ZSP. ZSP achieves an impressive 40% improvement in F1 score for fine-grained Rootcode classific
    

