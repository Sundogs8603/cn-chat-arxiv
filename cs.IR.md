# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Automated Attribute Extraction from Legal Proceedings.](http://arxiv.org/abs/2310.12131) | 本论文旨在通过采用结构化表示法和最先进的序列标记框架，实现自动从法律文件中提取属性，并展示了提取的属性在法律判决预测任务中的有效性。 |
| [^2] | [Unveiling the Siren's Song: Towards Reliable Fact-Conflicting Hallucination Detection.](http://arxiv.org/abs/2310.12086) | 该论文介绍了一种为大型语言模型设计的FactCHD事实冲突幻觉检测基准，用于评估LLMs生成文本的事实性。基准包含了多种事实模式，并使用基于事实的证据链进行组合性幻觉的检测。 |
| [^3] | [Simulating Users in Interactive Web Table Retrieval.](http://arxiv.org/abs/2310.11931) | 本文提出了一种模拟用户行为的方法，用于交互式网页表格检索。通过考虑搜索项目的多模态信号，特别是表格的多模态属性，可以提高检索效果。作者还基于Doc2Query引入了一种基于模拟用户知识的交互式查询重构策略。评估结果表明，模拟用户行为的方法可以作为替代真实用户研究的一种更具成本效益和可重复性的方法。在不同的成本范式下考虑用户效果对于评估交互式网页表格检索非常重要。 |
| [^4] | [CIR at the NTCIR-17 ULTRE-2 Task.](http://arxiv.org/abs/2310.11852) | 中国科学院信息检索团队（CIR）在NTCIR-17 ULTRE-2任务中采用双向学习算法（DLA）处理位置偏差，并解决了百度搜索数据中严重的虚假负面问题。他们通过纠正未被点击项目的标签和引入随机文档和具有部分匹配的文档作为负面样本的方法，提高了模型的性能。 |
| [^5] | [DCRNN: A Deep Cross approach based on RNN for Partial Parameter Sharing in Multi-task Learning.](http://arxiv.org/abs/2310.11777) | 这项工作提出了一种基于RNN的深度交叉方法，用于多任务学习中的部分参数共享，旨在提高推荐的成功率并降低计算成本。 |
| [^6] | [From Relevance to Utility: Evidence Retrieval with Feedback for Fact Verification.](http://arxiv.org/abs/2310.11675) | 本论文提出了一种基于反馈的证据检索器(FER)，通过整合声明验证者的反馈来优化事实验证中的证据检索过程。实证研究表明FER优于现有的基准方法。 |
| [^7] | [VKIE: The Application of Key Information Extraction on Video Text.](http://arxiv.org/abs/2310.11650) | 本文提出了一项重要任务，即从视频文本中提取层次化关键信息。研究者们通过拆分任务为四个子任务，并介绍了两种实现方案，即PipVKIE和UniVKIE。两种方案都利用了视觉、文本和坐标的多模态信息进行特征表示。实验证明，这些方案在性能和推理速度方面表现出色。 |
| [^8] | [Open Information Extraction: A Review of Baseline Techniques, Approaches, and Applications.](http://arxiv.org/abs/2310.11644) | 该论文综述了开放信息提取（OIE）的最新方法和应用，通过分析不同领域的关系来改善关系提取技术，避免手动标记预定义的关系，并概述了OIE的挑战和未来工作机会。 |
| [^9] | [Graph Neural Networks for Recommendation: Reproducibility, Graph Topology, and Node Representation.](http://arxiv.org/abs/2310.11270) | 本教程针对推荐中的图神经网络的三个关键方面进行了探讨：最先进方法的可复现性，图拓扑特征对模型性能的影响，以及学习节点表示的策略。 |
| [^10] | [Enhancing Conversational Search: Large Language Model-Aided Informative Query Rewriting.](http://arxiv.org/abs/2310.09716) | 该论文提出了一种利用大型语言模型(LLMs)作为查询重写器的方法，通过指令生成信息丰富的查询重写，以提升对话式搜索的检索性能。实验结果表明，这种方法在QReCC数据集上取得了良好的效果。 |
| [^11] | [Interactive Explanation with Varying Level of Details in an Explainable Scientific Literature Recommender System.](http://arxiv.org/abs/2306.05809) | 本文旨在采用以用户为中心的交互式解释模型，在推荐系统中为用户提供不同细节级别的解释，赋予用户个性化解释的能力。 |
| [^12] | [CLaMP: Contrastive Language-Music Pre-training for Cross-Modal Symbolic Music Information Retrieval.](http://arxiv.org/abs/2304.11029) | CLaMP是一种对比语言-音乐预训练技术，能够学习符号音乐和自然语言之间的跨模态表示。通过数据增强和分块处理，它将符号音乐表示成长度不到10％的序列，并使用掩蔽音乐模型预训练目标来增强音乐编码器对音乐上下文和结构的理解。这种技术超越了现有模型的能力，可以实现符号音乐的语义搜索和零样本分类。 |
| [^13] | [Video-Text Retrieval by Supervised Sparse Multi-Grained Learning.](http://arxiv.org/abs/2302.09473) | 本文提出了一种新的多粒度稀疏学习框架S3MA，用于视频文本检索。该框架通过学习共享的稀疏空间和多粒度相似度来改进检索效果，在多个基准测试中表现优于现有方法。 |
| [^14] | [Solo: Data Discovery Using Natural Language Questions Via A Self-Supervised Approach.](http://arxiv.org/abs/2301.03560) | 提出了一个使用自然语言问题进行数据发现的系统，使用自监督方法来训练系统，克服了昂贵的训练数据的限制，实现了根据自然语言问题进行数据发现的端到端解决方案。 |
| [^15] | [Learning List-Level Domain-Invariant Representations for Ranking.](http://arxiv.org/abs/2212.10764) | 本文提出了一种针对排名问题的列表级别对齐的学习方法，该方法利用列表的结构特性，在领域适应中实现从源领域到目标领域的知识转移。 |
| [^16] | [A Comparative Evaluation of Quantification Methods.](http://arxiv.org/abs/2103.03223) | 本研究通过对24种不同量化方法在超过40个数据集上进行全面实证比较，填补了量化方法比较研究的空白。我们发现在二分类设置中，基于阈值选择的Median Sweep和TSMax方法、DyS框架和弗里德曼的方法表现最佳；而在多分类设置中，Generaliz方法表现良好。 |

# 详细

[^1]: 自动从法律程序中提取属性

    Automated Attribute Extraction from Legal Proceedings. (arXiv:2310.12131v1 [cs.IR])

    [http://arxiv.org/abs/2310.12131](http://arxiv.org/abs/2310.12131)

    本论文旨在通过采用结构化表示法和最先进的序列标记框架，实现自动从法律文件中提取属性，并展示了提取的属性在法律判决预测任务中的有效性。

    

    挂起案件数量不断上升已成为全球关注的焦点。数字化的最新进展为利用人工智能工具处理法律文件提供了可能性。采用结构化表示法来处理法律文件，而不是仅使用平面文本表示，可以显著增强处理能力。为了实现这一目标，我们提出了一套用于刑事案件程序的多样属性。我们使用最先进的序列标记框架来自动从法律文件中提取属性。此外，我们还展示了从提取的属性在下游任务中（即法律判决预测）的有效性。

    The escalating number of pending cases is a growing concern world-wide. Recent advancements in digitization have opened up possibilities for leveraging artificial intelligence (AI) tools in the processing of legal documents. Adopting a structured representation for legal documents, as opposed to a mere bag-of-words flat text representation, can significantly enhance processing capabilities. With the aim of achieving this objective, we put forward a set of diverse attributes for criminal case proceedings. We use a state-of-the-art sequence labeling framework to automatically extract attributes from the legal documents. Moreover, we demonstrate the efficacy of the extracted attributes in a downstream task, namely legal judgment prediction.
    
[^2]: 发现塞壬之歌：可靠的事实冲突幻觉检测

    Unveiling the Siren's Song: Towards Reliable Fact-Conflicting Hallucination Detection. (arXiv:2310.12086v1 [cs.CL])

    [http://arxiv.org/abs/2310.12086](http://arxiv.org/abs/2310.12086)

    该论文介绍了一种为大型语言模型设计的FactCHD事实冲突幻觉检测基准，用于评估LLMs生成文本的事实性。基准包含了多种事实模式，并使用基于事实的证据链进行组合性幻觉的检测。

    

    大型语言模型（LLMs），如ChatGPT/GPT-4，因其广泛的实际应用而受到广泛关注，但其在网络平台上存在事实冲突幻觉的问题限制了其采用。对由LLMs产生的文本的事实性评估仍然未被充分探索，不仅涉及对基本事实的判断，还包括对复杂推理任务（如多跳等）中出现的事实错误的评估。为此，我们引入了FactCHD，一种为LLMs精心设计的事实冲突幻觉检测基准。作为在“查询-响应”上下文中评估事实性的关键工具，我们的基准采用了大规模数据集，涵盖了广泛的事实模式，如基本事实，多跳，比较和集合操作模式。我们基准的一个独特特点是其包含基于事实的证据链，从而便于进行组合性幻觉的检测。

    Large Language Models (LLMs), such as ChatGPT/GPT-4, have garnered widespread attention owing to their myriad of practical applications, yet their adoption has been constrained by issues of fact-conflicting hallucinations across web platforms. The assessment of factuality in text, produced by LLMs, remains inadequately explored, extending not only to the judgment of vanilla facts but also encompassing the evaluation of factual errors emerging in complex inferential tasks like multi-hop, and etc. In response, we introduce FactCHD, a fact-conflicting hallucination detection benchmark meticulously designed for LLMs. Functioning as a pivotal tool in evaluating factuality within "Query-Respons" contexts, our benchmark assimilates a large-scale dataset, encapsulating a broad spectrum of factuality patterns, such as vanilla, multi-hops, comparison, and set-operation patterns. A distinctive feature of our benchmark is its incorporation of fact-based chains of evidence, thereby facilitating com
    
[^3]: 在交互式网页表格检索中模拟用户行为

    Simulating Users in Interactive Web Table Retrieval. (arXiv:2310.11931v1 [cs.IR])

    [http://arxiv.org/abs/2310.11931](http://arxiv.org/abs/2310.11931)

    本文提出了一种模拟用户行为的方法，用于交互式网页表格检索。通过考虑搜索项目的多模态信号，特别是表格的多模态属性，可以提高检索效果。作者还基于Doc2Query引入了一种基于模拟用户知识的交互式查询重构策略。评估结果表明，模拟用户行为的方法可以作为替代真实用户研究的一种更具成本效益和可重复性的方法。在不同的成本范式下考虑用户效果对于评估交互式网页表格检索非常重要。

    

    考虑搜索项目的多模态信号对于提高检索效果是有益的。特别是在网页表格检索（WTR）实验中，考虑到表格的多模态属性可以提升效果。然而，单一模态如何影响用户体验仍然是一个未解决的问题。之前的工作分析了在临时检索基准下的WTR性能，它忽略了交互式搜索行为，并且限制了对于真实用户环境含义的结论。为此，本文提出了模拟交互式WTR搜索会话的全面评估，作为比真实用户研究更具成本效益和可重复性的替代方案。作为首次尝试，我们基于Doc2Query引入了基于模拟用户知识的交互式查询重构策略。我们的评估从两个不同的成本范式考虑用户效果，即按查询和按时间。

    Considering the multimodal signals of search items is beneficial for retrieval effectiveness. Especially in web table retrieval (WTR) experiments, accounting for multimodal properties of tables boosts effectiveness. However, it still remains an open question how the single modalities affect user experience in particular. Previous work analyzed WTR performance in ad-hoc retrieval benchmarks, which neglects interactive search behavior and limits the conclusion about the implications for real-world user environments.  To this end, this work presents an in-depth evaluation of simulated interactive WTR search sessions as a more cost-efficient and reproducible alternative to real user studies. As a first of its kind, we introduce interactive query reformulation strategies based on Doc2Query, incorporating cognitive states of simulated user knowledge. Our evaluations include two perspectives on user effectiveness by considering different cost paradigms, namely query-wise and time-oriented mea
    
[^4]: CIR参与NTCIR-17 ULTRE-2任务

    CIR at the NTCIR-17 ULTRE-2 Task. (arXiv:2310.11852v1 [cs.IR])

    [http://arxiv.org/abs/2310.11852](http://arxiv.org/abs/2310.11852)

    中国科学院信息检索团队（CIR）在NTCIR-17 ULTRE-2任务中采用双向学习算法（DLA）处理位置偏差，并解决了百度搜索数据中严重的虚假负面问题。他们通过纠正未被点击项目的标签和引入随机文档和具有部分匹配的文档作为负面样本的方法，提高了模型的性能。

    

    中国科学院信息检索团队（CIR）参加了NTCIR-17 ULTRE-2任务。本论文描述了我们的方法并报告了我们在ULTRE-2任务中的结果。我们认识到在百度搜索数据中，虚假负面结果的问题非常严重，比位置偏差更加严重。因此，我们采用了双向学习算法（DLA）来解决位置偏差，并将其作为辅助模型研究如何缓解虚假负面问题。我们从两个角度解决这个问题：1）通过从DLA训练的相关性判断模型来纠正未被点击项目的标签，并学习一个新的排序器，该排序器由DLA初始化；2）将随机文档作为真负面和具有部分匹配的文档作为难负面。这两种方法都可以提高模型的性能，我们的最佳方法在nDCG@10上达到了0.5355，比组织者的最佳得分提高了2.66%。

    The Chinese academy of sciences Information Retrieval team (CIR) has participated in the NTCIR-17 ULTRE-2 task. This paper describes our approaches and reports our results on the ULTRE-2 task. We recognize the issue of false negatives in the Baidu search data in this competition is very severe, much more severe than position bias. Hence, we adopt the Dual Learning Algorithm (DLA) to address the position bias and use it as an auxiliary model to study how to alleviate the false negative issue. We approach the problem from two perspectives: 1) correcting the labels for non-clicked items by a relevance judgment model trained from DLA, and learn a new ranker that is initialized from DLA; 2) including random documents as true negatives and documents that have partial matching as hard negatives. Both methods can enhance the model performance and our best method has achieved nDCG@10 of 0.5355, which is 2.66% better than the best score from the organizer.
    
[^5]: DCRNN: 一种基于RNN的深度交叉方法，用于多任务学习中的部分参数共享

    DCRNN: A Deep Cross approach based on RNN for Partial Parameter Sharing in Multi-task Learning. (arXiv:2310.11777v1 [cs.IR])

    [http://arxiv.org/abs/2310.11777](http://arxiv.org/abs/2310.11777)

    这项工作提出了一种基于RNN的深度交叉方法，用于多任务学习中的部分参数共享，旨在提高推荐的成功率并降低计算成本。

    

    最近几年，DL发展迅速，个性化服务正探索使用DL算法来提升推荐系统的性能。对于个性化服务来说，成功的推荐包括吸引用户点击项目和用户愿意消费项目两个方面。如果同时需要预测这两个任务，传统的推荐系统通常会训练两个独立的模型。这种方法繁琐且不能有效地建模“点击-消费”这两个子任务之间的关系。因此，为了提高推荐的成功率并减少计算成本，研究人员正在尝试建模多任务学习。目前，现有的多任务学习模型通常采用硬参数共享或软参数共享架构，但这两种架构各自存在一定的问题。因此，在这项工作中，我们提出了一种基于真实推荐的新型推荐模型。

    In recent years, DL has developed rapidly, and personalized services are exploring using DL algorithms to improve the performance of the recommendation system. For personalized services, a successful recommendation consists of two parts: attracting users to click the item and users being willing to consume the item. If both tasks need to be predicted at the same time, traditional recommendation systems generally train two independent models. This approach is cumbersome and does not effectively model the relationship between the two subtasks of "click-consumption". Therefore, in order to improve the success rate of recommendation and reduce computational costs, researchers are trying to model multi-task learning.  At present, existing multi-task learning models generally adopt hard parameter sharing or soft parameter sharing architecture, but these two architectures each have certain problems. Therefore, in this work, we propose a novel recommendation model based on real recommendation 
    
[^6]: 从相关性到实用性: 基于反馈的证据检索在事实验证中的应用

    From Relevance to Utility: Evidence Retrieval with Feedback for Fact Verification. (arXiv:2310.11675v1 [cs.IR])

    [http://arxiv.org/abs/2310.11675](http://arxiv.org/abs/2310.11675)

    本论文提出了一种基于反馈的证据检索器(FER)，通过整合声明验证者的反馈来优化事实验证中的证据检索过程。实证研究表明FER优于现有的基准方法。

    

    在事实验证中，检索增强方法已成为主要的方法之一；它需要对多个检索到的证据进行推理，以验证声明的真实性。为了检索证据，现有的方法通常使用基于概率排序原则设计的现成检索模型。我们认为，在事实验证中，我们需要关注的是声明验证者从检索到的证据中获得的实用性，而不是相关性。我们引入了基于反馈的证据检索器（FER），通过整合声明验证者的反馈来优化证据检索过程。作为反馈信号，我们使用验证者有效利用检索到的证据和基准证据之间实用性的差异来产生最终的声明标签。实证研究证明FER优于现有的基准方法。

    Retrieval-enhanced methods have become a primary approach in fact verification (FV); it requires reasoning over multiple retrieved pieces of evidence to verify the integrity of a claim. To retrieve evidence, existing work often employs off-the-shelf retrieval models whose design is based on the probability ranking principle. We argue that, rather than relevance, for FV we need to focus on the utility that a claim verifier derives from the retrieved evidence. We introduce the feedback-based evidence retriever(FER) that optimizes the evidence retrieval process by incorporating feedback from the claim verifier. As a feedback signal we use the divergence in utility between how effectively the verifier utilizes the retrieved evidence and the ground-truth evidence to produce the final claim label. Empirical studies demonstrate the superiority of FER over prevailing baselines.
    
[^7]: VKIE:应用关键信息提取于视频文本的研究

    VKIE: The Application of Key Information Extraction on Video Text. (arXiv:2310.11650v1 [cs.IR])

    [http://arxiv.org/abs/2310.11650](http://arxiv.org/abs/2310.11650)

    本文提出了一项重要任务，即从视频文本中提取层次化关键信息。研究者们通过拆分任务为四个子任务，并介绍了两种实现方案，即PipVKIE和UniVKIE。两种方案都利用了视觉、文本和坐标的多模态信息进行特征表示。实验证明，这些方案在性能和推理速度方面表现出色。

    

    从视频中提取结构化信息对于行业中许多下游应用至关重要。本文定义了从视频文本中提取层次化关键信息的重要任务。为了完成这个任务，我们将其拆分为四个子任务，并介绍了两种实现方案，分别称为PipVKIE和UniVKIE。PipVKIE按照连续阶段顺序完成四个子任务，而UniVKIE通过将所有子任务统一到一个主干中进行改进。PipVKIE和UniVKIE都利用了来自视觉、文本和坐标的多模态信息进行特征表示。在一个明确定义的数据集上进行了大量实验证明，我们的解决方案可以取得显著的性能和高效的推理速度。代码和数据集将公开提供。

    Extracting structured information from videos is critical for numerous downstream applications in the industry. In this paper, we define a significant task of extracting hierarchical key information from visual texts on videos. To fulfill this task, we decouples it into four subtasks and introduce two implementation solutions called PipVKIE and UniVKIE. PipVKIE sequentially completes the four subtasks in continuous stages, while UniVKIE is improved by unifying all the subtasks into one backbone. Both PipVKIE and UniVKIE leverage multimodal information from vision, text, and coordinates for feature representation. Extensive experiments on one well-defined dataset demonstrate that our solutions can achieve remarkable performance and efficient inference speed. The code and dataset will be publicly available.
    
[^8]: 开放信息提取：基准技术、方法和应用的综述

    Open Information Extraction: A Review of Baseline Techniques, Approaches, and Applications. (arXiv:2310.11644v1 [cs.IR])

    [http://arxiv.org/abs/2310.11644](http://arxiv.org/abs/2310.11644)

    该论文综述了开放信息提取（OIE）的最新方法和应用，通过分析不同领域的关系来改善关系提取技术，避免手动标记预定义的关系，并概述了OIE的挑战和未来工作机会。

    

    随着大量的在线和离线文本数据的可用性，需要提取短语之间的关系并用几个词汇总每个文档的主要内容变得至关重要。为此，最近一直进行了许多关于开放信息提取（OIE）的研究。OIE通过分析不同领域的关系来改进关系提取技术，并避免在句子中要求手动标记预定义的关系。本文综述了OIE的最新方法以及其在知识图谱（KG）、文本摘要和问答（QA）中的应用。此外，本文描述了OIE基础方法中的关系提取。它简要讨论了每种方法的主要方法和优缺点。最后，它对OIE、关系提取和OIE应用的挑战、开放问题和未来工作机会进行了概述。

    With the abundant amount of available online and offline text data, there arises a crucial need to extract the relation between phrases and summarize the main content of each document in a few words. For this purpose, there have been many studies recently in Open Information Extraction (OIE). OIE improves upon relation extraction techniques by analyzing relations across different domains and avoids requiring hand-labeling pre-specified relations in sentences. This paper surveys recent approaches of OIE and its applications on Knowledge Graph (KG), text summarization, and Question Answering (QA). Moreover, the paper describes OIE basis methods in relation extraction. It briefly discusses the main approaches and the pros and cons of each method. Finally, it gives an overview about challenges, open issues, and future work opportunities for OIE, relation extraction, and OIE applications.
    
[^9]: 推荐系统中的图神经网络: 可复现性、图拓扑和节点表示

    Graph Neural Networks for Recommendation: Reproducibility, Graph Topology, and Node Representation. (arXiv:2310.11270v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2310.11270](http://arxiv.org/abs/2310.11270)

    本教程针对推荐中的图神经网络的三个关键方面进行了探讨：最先进方法的可复现性，图拓扑特征对模型性能的影响，以及学习节点表示的策略。

    

    最近几年，图神经网络（GNNs）在推荐系统中变得越来越重要。通过将用户-物品矩阵表示为一个二部图和无向图，GNN能够捕捉用户-物品之间的近距离和远距离交互，从而比传统推荐方法学习到更准确的偏好模式。与之前的同类教程不同，本教程旨在展示和探讨推荐中GNNs的三个关键方面：（i）最先进方法的可复现性，（ii）图拓扑特征对模型性能的潜在影响，以及（iii）在从零开始训练特征或利用预训练嵌入作为额外物品信息（例如多模态特征）时，学习节点表示的策略。我们的目标是为该领域提供三个新颖的理论和实践视角，目前在图学习中存在争议。

    Graph neural networks (GNNs) have gained prominence in recommendation systems in recent years. By representing the user-item matrix as a bipartite and undirected graph, GNNs have demonstrated their potential to capture short- and long-distance user-item interactions, thereby learning more accurate preference patterns than traditional recommendation approaches. In contrast to previous tutorials on the same topic, this tutorial aims to present and examine three key aspects that characterize GNNs for recommendation: (i) the reproducibility of state-of-the-art approaches, (ii) the potential impact of graph topological characteristics on the performance of these models, and (iii) strategies for learning node representations when training features from scratch or utilizing pre-trained embeddings as additional item information (e.g., multimodal features). The goal is to provide three novel theoretical and practical perspectives on the field, currently subject to debate in graph learning but l
    
[^10]: 提升对话式搜索：基于大型语言模型辅助的信息查询重写

    Enhancing Conversational Search: Large Language Model-Aided Informative Query Rewriting. (arXiv:2310.09716v2 [cs.HC] UPDATED)

    [http://arxiv.org/abs/2310.09716](http://arxiv.org/abs/2310.09716)

    该论文提出了一种利用大型语言模型(LLMs)作为查询重写器的方法，通过指令生成信息丰富的查询重写，以提升对话式搜索的检索性能。实验结果表明，这种方法在QReCC数据集上取得了良好的效果。

    

    查询重写在提升对话式搜索中起着重要作用，通过将上下文相关的用户查询转化为独立形式。现有方法主要利用人工重写的查询作为标签来训练查询重写模型。然而，人工重写可能缺乏足够的信息以实现最佳的检索性能。为了克服这个限制，我们提出利用大型语言模型(LLMs)作为查询重写器，通过精心设计的指令生成信息丰富的查询重写。我们定义了四个重要特性来定义规范的重写，并将其全部纳入指令中。此外，当初始查询重写可用时，我们引入了LLMs的重写编辑器的角色，形成一个“重写-编辑”过程。此外，我们提出将LLMs的重写能力提炼成较小的模型，以减少重写延迟。我们在QReCC数据集上进行的实验评估表明，信息丰富的查询重写可以提高搜索的效果。

    Query rewriting plays a vital role in enhancing conversational search by transforming context-dependent user queries into standalone forms. Existing approaches primarily leverage human-rewritten queries as labels to train query rewriting models. However, human rewrites may lack sufficient information for optimal retrieval performance. To overcome this limitation, we propose utilizing large language models (LLMs) as query rewriters, enabling the generation of informative query rewrites through well-designed instructions. We define four essential properties for well-formed rewrites and incorporate all of them into the instruction. In addition, we introduce the role of rewrite editors for LLMs when initial query rewrites are available, forming a "rewrite-then-edit" process. Furthermore, we propose distilling the rewriting capabilities of LLMs into smaller models to reduce rewriting latency. Our experimental evaluation on the QReCC dataset demonstrates that informative query rewrites can y
    
[^11]: 在可解释的科学文献推荐系统中采用不同细节级别的交互式解释

    Interactive Explanation with Varying Level of Details in an Explainable Scientific Literature Recommender System. (arXiv:2306.05809v1 [cs.IR])

    [http://arxiv.org/abs/2306.05809](http://arxiv.org/abs/2306.05809)

    本文旨在采用以用户为中心的交互式解释模型，在推荐系统中为用户提供不同细节级别的解释，赋予用户个性化解释的能力。

    

    传统上，可解释的推荐系统采用一种“一刀切”的方法，向每个用户提供相同程度的解释，而不考虑他们的个体需求和目标。此外，推荐系统中的解释大多以静态和非交互方式呈现。为填补这些研究空白，本文旨在采用以用户为中心的交互式解释模型，为用户提供不同细节级别的解释，并赋予用户基于其需求和偏好进行交互、控制和个性化解释的能力。我们采用以用户为中心的方法，设计了三个细节级别的交互式解释（基本、中级和高级），并在透明的推荐和兴趣建模应用（RIMA）中实现了它们。我们进行了一个定性用户研究（N=14），以调查提供不同细节级别的交互式解释对用户对系统可解释性的感知的影响。

    Explainable recommender systems (RS) have traditionally followed a one-size-fits-all approach, delivering the same explanation level of detail to each user, without considering their individual needs and goals. Further, explanations in RS have so far been presented mostly in a static and non-interactive manner. To fill these research gaps, we aim in this paper to adopt a user-centered, interactive explanation model that provides explanations with different levels of detail and empowers users to interact with, control, and personalize the explanations based on their needs and preferences. We followed a user-centered approach to design interactive explanations with three levels of detail (basic, intermediate, and advanced) and implemented them in the transparent Recommendation and Interest Modeling Application (RIMA). We conducted a qualitative user study (N=14) to investigate the impact of providing interactive explanations with varying level of details on the users' perception of the e
    
[^12]: CLaMP：用于跨模态符号音乐信息检索的对比语言-音乐预训练

    CLaMP: Contrastive Language-Music Pre-training for Cross-Modal Symbolic Music Information Retrieval. (arXiv:2304.11029v1 [cs.SD])

    [http://arxiv.org/abs/2304.11029](http://arxiv.org/abs/2304.11029)

    CLaMP是一种对比语言-音乐预训练技术，能够学习符号音乐和自然语言之间的跨模态表示。通过数据增强和分块处理，它将符号音乐表示成长度不到10％的序列，并使用掩蔽音乐模型预训练目标来增强音乐编码器对音乐上下文和结构的理解。这种技术超越了现有模型的能力，可以实现符号音乐的语义搜索和零样本分类。

    

    我们介绍了CLaMP：对比语言-音乐预训练，它使用音乐编码器和文本编码器通过对比损失函数联合训练来学习自然语言和符号音乐之间的跨模态表示。为了预训练CLaMP，我们收集了140万个音乐-文本对的大型数据集。它使用了文本随机失活来进行数据增强和分块处理以高效地表示音乐数据，从而将序列长度缩短到不到10％。此外，我们开发了一个掩蔽音乐模型预训练目标，以增强音乐编码器对音乐上下文和结构的理解。CLaMP集成了文本信息，以实现符号音乐的语义搜索和零样本分类，超越了先前模型的能力。为支持语义搜索和音乐分类的评估，我们公开发布了WikiMusicText（WikiMT），这是一个包含1010个ABC符号谱的数据集，每个谱都附带有标题、艺术家、流派和描述信息。

    We introduce CLaMP: Contrastive Language-Music Pre-training, which learns cross-modal representations between natural language and symbolic music using a music encoder and a text encoder trained jointly with a contrastive loss. To pre-train CLaMP, we collected a large dataset of 1.4 million music-text pairs. It employed text dropout as a data augmentation technique and bar patching to efficiently represent music data which reduces sequence length to less than 10%. In addition, we developed a masked music model pre-training objective to enhance the music encoder's comprehension of musical context and structure. CLaMP integrates textual information to enable semantic search and zero-shot classification for symbolic music, surpassing the capabilities of previous models. To support the evaluation of semantic search and music classification, we publicly release WikiMusicText (WikiMT), a dataset of 1010 lead sheets in ABC notation, each accompanied by a title, artist, genre, and description.
    
[^13]: 通过监督稀疏多粒度学习进行视频文本检索

    Video-Text Retrieval by Supervised Sparse Multi-Grained Learning. (arXiv:2302.09473v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.09473](http://arxiv.org/abs/2302.09473)

    本文提出了一种新的多粒度稀疏学习框架S3MA，用于视频文本检索。该框架通过学习共享的稀疏空间和多粒度相似度来改进检索效果，在多个基准测试中表现优于现有方法。

    

    对于视频文本检索，最近在探索更好的表示学习方面取得了进展。本文提出了一个新的多粒度稀疏学习框架S3MA，用于学习视频和文本之间的共享稀疏空间，从而实现视频文本检索。共享稀疏空间通过有限数量的稀疏概念进行初始化，每个概念都对应一些词语。利用现有的文本数据，我们以监督方式学习和更新共享稀疏空间，使用提出的相似度和对齐损失函数。此外，为了实现多粒度对齐，我们将帧表示方法纳入模型，更好地对视频模态进行建模和计算细粒度和粗粒度的相似度。通过学习得到的共享稀疏空间和多粒度相似度，我们在多个视频文本检索基准上进行了广泛实验，实验结果表明S3MA优于现有方法。我们的代码可以在https://github.com/yim上找到。

    While recent progress in video-text retrieval has been advanced by the exploration of better representation learning, in this paper, we present a novel multi-grained sparse learning framework, S3MA, to learn an aligned sparse space shared between the video and the text for video-text retrieval. The shared sparse space is initialized with a finite number of sparse concepts, each of which refers to a number of words. With the text data at hand, we learn and update the shared sparse space in a supervised manner using the proposed similarity and alignment losses. Moreover, to enable multi-grained alignment, we incorporate frame representations for better modeling the video modality and calculating fine-grained and coarse-grained similarities. Benefiting from the learned shared sparse space and multi-grained similarities, extensive experiments on several video-text retrieval benchmarks demonstrate the superiority of S3MA over existing methods. Our code is available at https://github.com/yim
    
[^14]: Solo: 使用自监督方法通过自然语言问题进行数据发现

    Solo: Data Discovery Using Natural Language Questions Via A Self-Supervised Approach. (arXiv:2301.03560v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2301.03560](http://arxiv.org/abs/2301.03560)

    提出了一个使用自然语言问题进行数据发现的系统，使用自监督方法来训练系统，克服了昂贵的训练数据的限制，实现了根据自然语言问题进行数据发现的端到端解决方案。

    

    大多数已部署的数据发现系统，例如Google Datasets和开放数据门户，只支持关键词搜索。关键词搜索适用于普通用户，但限制了系统能够回答的查询类型。我们提出了一个新的系统，允许用户直接编写自然语言问题。使用这个学习的数据发现系统的一个主要障碍是需要昂贵的训练数据，从而限制了其实用性。在本文中，我们介绍了一种自监督方法来组装训练数据集并训练无需人为干预的学习发现系统。它需要解决几个挑战，包括自我监督策略的设计以进行数据发现、用于输入模型的表格表示策略和能与合成生成的问题很好工作的相关性模型。我们将所有上述贡献结合起来构建一个名为Solo的系统，以解决该问题。评估结果表明，新技术表现优于现有方法。

    Most deployed data discovery systems, such as Google Datasets, and open data portals only support keyword search. Keyword search is geared towards general audiences but limits the types of queries the systems can answer. We propose a new system that lets users write natural language questions directly. A major barrier to using this learned data discovery system is it needs expensive-to-collect training data, thus limiting its utility. In this paper, we introduce a self-supervised approach to assemble training datasets and train learned discovery systems without human intervention. It requires addressing several challenges, including the design of self-supervised strategies for data discovery, table representation strategies to feed to the models, and relevance models that work well with the synthetically generated questions. We combine all the above contributions into a system, Solo, that solves the problem end to end. The evaluation results demonstrate the new techniques outperform st
    
[^15]: 学习用于排名的列表级别领域不变表示

    Learning List-Level Domain-Invariant Representations for Ranking. (arXiv:2212.10764v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2212.10764](http://arxiv.org/abs/2212.10764)

    本文提出了一种针对排名问题的列表级别对齐的学习方法，该方法利用列表的结构特性，在领域适应中实现从源领域到目标领域的知识转移。

    

    领域适应旨在将在（数据丰富）源领域学到的知识转移到（资源有限）目标领域，一种常用的方法是不变表示学习，它匹配并对齐特征空间上的数据分布。尽管这种方法在分类和回归问题上得到了广泛研究和应用，但在排名问题上的应用却是零散的，并且现有的几种实现缺乏理论上的证明。本文重新审视了用于排名的不变表示学习。在审查之前的工作时，我们发现他们实施了我们称之为项目级别对齐的方法，该方法在聚合的所有列表中对进行排名的项目分布进行对齐，但忽略了列表的结构。然而，列表的结构应该被利用，因为它是排名问题的固有特性，其中数据和度量是在列表上定义和计算的，而不是在项目本身上。为了解决这一不一致，我们提出了列表级别对齐的学习

    Domain adaptation aims to transfer the knowledge learned on (data-rich) source domains to (low-resource) target domains, and a popular method is invariant representation learning, which matches and aligns the data distributions on the feature space. Although this method is studied extensively and applied on classification and regression problems, its adoption on ranking problems is sporadic, and the few existing implementations lack theoretical justifications. This paper revisits invariant representation learning for ranking. Upon reviewing prior work, we found that they implement what we call item-level alignment, which aligns the distributions of the items being ranked from all lists in aggregate but ignores their list structure. However, the list structure should be leveraged, because it is intrinsic to ranking problems where the data and the metrics are defined and computed on lists, not the items by themselves. To close this discrepancy, we propose list-level alignment -learning
    
[^16]: 量化方法的比较评估

    A Comparative Evaluation of Quantification Methods. (arXiv:2103.03223v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2103.03223](http://arxiv.org/abs/2103.03223)

    本研究通过对24种不同量化方法在超过40个数据集上进行全面实证比较，填补了量化方法比较研究的空白。我们发现在二分类设置中，基于阈值选择的Median Sweep和TSMax方法、DyS框架和弗里德曼的方法表现最佳；而在多分类设置中，Generaliz方法表现良好。

    

    量化是指在数据集中预测类别分布的问题。它也代表着一个在监督式机器学习中不断发展的研究领域，近年来提出了大量不同的算法。然而，目前还没有一份全面的实证比较量化方法的研究，以支持算法选择。在本研究中，我们通过对超过40个数据集进行了24种不同量化方法的彻底实证性性能比较，包括二分类和多分类量化设置，填补了这一研究空白。我们观察到没有单一算法能够在所有竞争对手中始终表现最佳，但我们确定了一组在二分类设置中表现最佳的方法，包括基于阈值选择的Median Sweep和TSMax方法、DyS框架和弗里德曼的方法。对于多分类设置，我们观察到另一组算法表现良好，包括Generaliz方法。

    Quantification represents the problem of predicting class distributions in a dataset. It also represents a growing research field in supervised machine learning, for which a large variety of different algorithms has been proposed in recent years. However, a comprehensive empirical comparison of quantification methods that supports algorithm selection is not available yet. In this work, we close this research gap by conducting a thorough empirical performance comparison of 24 different quantification methods on overall more than 40 data sets, considering binary as well as multiclass quantification settings. We observe that no single algorithm generally outperforms all competitors, but identify a group of methods including the threshold selection-based Median Sweep and TSMax methods, the DyS framework, and Friedman's method that performs best in the binary setting. For the multiclass setting, we observe that a different group of algorithms yields good performance, including the Generaliz
    

