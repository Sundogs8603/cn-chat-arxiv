# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [ERASE: Benchmarking Feature Selection Methods for Deep Recommender Systems](https://arxiv.org/abs/2403.12660) | 深度推荐系统中的特征选择方法研究面临着公平比较、选择属性分析缺乏以及过度关注峰值性能等挑战。 |
| [^2] | [InBox: Recommendation with Knowledge Graph using Interest Box Embedding](https://arxiv.org/abs/2403.12649) | 该论文介绍了一种利用知识图谱和兴趣框Embedding的推荐系统，以提高性能和可解释性。 |
| [^3] | [Context-based Fast Recommendation Strategy for Long User Behavior Sequence in Meituan Waimai](https://arxiv.org/abs/2403.12566) | 美团外卖的推荐系统引入了基于上下文的快速推荐策略，通过识别共享相似用户偏好的上下文，定位相应的PoIs，从而更好地处理长用户行为序列。 |
| [^4] | [Listwise Generative Retrieval Models via a Sequential Learning Process](https://arxiv.org/abs/2403.12499) | 本文提出了一种基于列表的生成式检索模型，通过引入替代的列表方式来优化相关性，超越了传统的点对点方法。 |
| [^5] | [Interpretable User Satisfaction Estimation for Conversational Systems with Large Language Models](https://arxiv.org/abs/2403.12388) | 本文提出了一种名为SPUR的方法，通过LLMs更有效地从自然语言话语中提取用户满意度的可解释信号，并能够利用迭代提示框架进行用户满意度评估。 |
| [^6] | [An Aligning and Training Framework for Multimodal Recommendations](https://arxiv.org/abs/2403.12384) | 提出了一种名为AlignRec的对齐和训练框架，用于解决多模态推荐中的不对齐问题，通过将推荐目标分解为三个对齐部分，实现内容内部对齐、内容与分类ID之间的对齐以及用户和项目之间的对齐。 |
| [^7] | [Methods for Generating Drift in Text Streams](https://arxiv.org/abs/2403.12328) | 文本数据中概念漂移是一个常见现象，而本文提出了四种文本漂移生成方法来帮助产生具有标记漂移的数据集 |
| [^8] | [TnT-LLM: Text Mining at Scale with Large Language Models](https://arxiv.org/abs/2403.12173) | TnT-LLM 提出了一个两阶段框架，利用大规模语言模型自动化生成和分配标签，减少人力成本。 |
| [^9] | [Learning Time Slot Preferences via Mobility Tree for Next POI Recommendation](https://arxiv.org/abs/2403.12100) | 本文引入了“移动树”数据结构，用于学习用户跨不同时间段的偏好，以提升下一个POI推荐任务的性能。 |
| [^10] | [Enriching User Shopping History: Empowering E-commerce with a Hierarchical Recommendation System](https://arxiv.org/abs/2403.12096) | 通过预测缺失的用户购物历史部分并适当丰富它，可以提高推荐系统的准确性。 |
| [^11] | [Methods for Matching English Language Addresses](https://arxiv.org/abs/2403.12092) | 该研究定义并规范了生成英语地址匹配对的框架，并研究了距离基准方法到深度学习模型等各种方法之间的精度、召回率和准确度，以确定最适合地址匹配任务的方法。 |
| [^12] | [Foundation Models and Information Retrieval in Digital Pathology](https://arxiv.org/abs/2403.12090) | 论文回顾了数字病理学中基础模型和信息检索领域的最新进展。 |
| [^13] | [TMU at TREC Clinical Trials Track 2023](https://arxiv.org/abs/2403.12088) | 多伦多都会大学利用自然语言处理技术和神经语言模型参加TREC临床试验跟踪，并展示了其实验结果。 |
| [^14] | [Group Movie Selection using Multi-channel Emotion Recognition](https://arxiv.org/abs/2403.12087) | 该研究提出了一种利用多通道情感识别进行群体电影选择的方法，为群体决策提供了实用工具 |
| [^15] | [Presenting Terrorizer: an algorithm for consolidating company names in patent assignees](https://arxiv.org/abs/2403.12083) | 本文介绍了一种名为Terrorizer的算法，利用自然语言处理、网络理论和基于规则的技术，以解决归因于公司的专利中存在的名称变体问题。 |
| [^16] | [Beyond Beats: A Recipe to Song Popularity? A machine learning approach](https://arxiv.org/abs/2403.12079) | 该研究利用机器学习模型探讨预测歌曲流行度，结果显示流派是影响流行度的主要因素，同时揭示了时间趋势和特征间的复杂关系。 |
| [^17] | [Evaluating Robustness of Generative Search Engine on Adversarial Factual Questions](https://arxiv.org/abs/2403.12077) | 评估生成式搜索引擎对对抗性事实问题的健壮性，通过对多种生成式搜索引擎进行人类评估，展示了对抗性事实问题在诱导不正确响应方面的有效性。 |
| [^18] | [Can LLM Substitute Human Labeling? A Case Study of Fine-grained Chinese Address Entity Recognition Dataset for UAV Delivery](https://arxiv.org/abs/2403.06097) | 提出了适用于无人机交付系统中地址解析任务的细粒度中文姓名实体识别数据集CNER-UAV，包含五个类别的多样化数据，经过严格的数据清洗和去敏处理，约有12,000个标注样本，评估了传统的实体识别模型并提供了深入分析 |
| [^19] | [Exploring the Impact of Large Language Models on Recommender Systems: An Extensive Review](https://arxiv.org/abs/2402.18590) | 大型语言模型在推荐系统中展现出卓越的推荐能力和语言理解，重塑了推荐任务的基础。 |
| [^20] | [RA-Rec: An Efficient ID Representation Alignment Framework for LLM-based Recommendation](https://arxiv.org/abs/2402.04527) | 这篇论文提出了一种基于LLM的推荐系统的高效ID表示对齐框架RA-Rec，通过将预训练的ID嵌入到LLMs中，并设计创新的对齐模块和高效调整方法，实现了在推荐系统中的显著性能优化。 |
| [^21] | [Towards Semantic Consistency: Dirichlet Energy Driven Robust Multi-Modal Entity Alignment](https://arxiv.org/abs/2401.17859) | 本研究提出了基于狄利克雷能量的新方法DESAlign，以解决多模态实体对齐中的语义一致性问题。我们发现语义不一致性导致模型过度拟合模态噪声，而DESAlign通过插值缺失的语义并应对过度平滑问题，实现了语义一致性。 |
| [^22] | [Establishing Performance Baselines in Fine-Tuning, Retrieval-Augmented Generation and Soft-Prompting for Non-Specialist LLM Users](https://arxiv.org/abs/2311.05903) | 通过对GPT 3.5进行微调，并结合基于向量的RAG数据库和非算法软提示，建立了性能基线，发现在特定的测试条件下微调模型表现更好。 |
| [^23] | [EasyEdit: An Easy-to-use Knowledge Editing Framework for Large Language Models](https://arxiv.org/abs/2308.07269) | EasyEdit提出了一种易于使用的知识编辑框架，针对大型语言模型的知识截断或谬误问题，支持各种最新的知识编辑方法，并可应用于多个知名的LLMs。 |

# 详细

[^1]: ERASE：深度推荐系统特征选择方法的基准测试

    ERASE: Benchmarking Feature Selection Methods for Deep Recommender Systems

    [https://arxiv.org/abs/2403.12660](https://arxiv.org/abs/2403.12660)

    深度推荐系统中的特征选择方法研究面临着公平比较、选择属性分析缺乏以及过度关注峰值性能等挑战。

    

    深度推荐系统(DRS)越来越依赖于大量特征字段来提供更精准的推荐。有效的特征选择方法因此变得至关重要，以进一步提高准确性并优化存储效率，以满足部署需求。研究领域，特别是在DRS的背景下，尚处于初期阶段，面临三个核心挑战：首先，研究论文之间实验设置的差异往往导致不公平比较，遮蔽了实践见解。其次，现有文献缺乏基于大规模数据集的选择属性的详细分析，并且缺乏对选择技术和DRS骨干之间进行全面比较的限制性文章的通用性研究和部署。最后，研究往往专注于比较特征选择方法可达到的峰值性能，这种方法通常在计算方面不足。

    arXiv:2403.12660v1 Announce Type: cross  Abstract: Deep Recommender Systems (DRS) are increasingly dependent on a large number of feature fields for more precise recommendations. Effective feature selection methods are consequently becoming critical for further enhancing the accuracy and optimizing storage efficiencies to align with the deployment demands. This research area, particularly in the context of DRS, is nascent and faces three core challenges. Firstly, variant experimental setups across research papers often yield unfair comparisons, obscuring practical insights. Secondly, the existing literature's lack of detailed analysis on selection attributes, based on large-scale datasets and a thorough comparison among selection techniques and DRS backbones, restricts the generalizability of findings and impedes deployment on DRS. Lastly, research often focuses on comparing the peak performance achievable by feature selection methods, an approach that is typically computationally infe
    
[^2]: 使用知识图谱和兴趣框Embedding的推荐系统InBox

    InBox: Recommendation with Knowledge Graph using Interest Box Embedding

    [https://arxiv.org/abs/2403.12649](https://arxiv.org/abs/2403.12649)

    该论文介绍了一种利用知识图谱和兴趣框Embedding的推荐系统，以提高性能和可解释性。

    

    知识图谱(KGs)在现代推荐系统中变得至关重要，有效提高了性能和可解释性。然而，现有研究忽视了两个关键挑战：1）兴趣对应于潜在数量庞大的相关项目集，2）对KG信息和兴趣连接性缺乏明确、细粒度的利用。为了解决这些限制，我们引入了一种新颖的embedding方法。

    arXiv:2403.12649v1 Announce Type: cross  Abstract: Knowledge graphs (KGs) have become vitally important in modern recommender systems, effectively improving performance and interpretability. Fundamentally, recommender systems aim to identify user interests based on historical interactions and recommend suitable items. However, existing works overlook two key challenges: (1) an interest corresponds to a potentially large set of related items, and (2) the lack of explicit, fine-grained exploitation of KG information and interest connectivity. This leads to an inability to reflect distinctions between entities and interests when modeling them in a single way. Additionally, the granularity of concepts in the knowledge graphs used for recommendations tends to be coarse, failing to match the fine-grained nature of user interests. This homogenization limits the precise exploitation of knowledge graph data and interest connectivity. To address these limitations, we introduce a novel embedding-
    
[^3]: 美团外卖中基于上下文的快速推荐策略

    Context-based Fast Recommendation Strategy for Long User Behavior Sequence in Meituan Waimai

    [https://arxiv.org/abs/2403.12566](https://arxiv.org/abs/2403.12566)

    美团外卖的推荐系统引入了基于上下文的快速推荐策略，通过识别共享相似用户偏好的上下文，定位相应的PoIs，从而更好地处理长用户行为序列。

    

    在美团外卖的推荐系统中，我们面对着日益增长的用户行为序列，这给有效建模用户偏好带来了越来越大的挑战。现有的序列推荐模型常常无法捕捉长期依赖关系，或者过于复杂，使得满足美团外卖独特业务需求变得复杂。为了更好地建模用户兴趣，我们考虑基于用户的偏好选择相关子序列从用户广泛的历史行为中。在这种特定场景下，我们注意到用户交互的上下文对他们的偏好有显著影响。为此，我们引入了一种名为基于上下文的快速推荐策略的新方法来解决长序列问题。我们首先识别出与目标上下文具有类似用户偏好的上下文，然后基于这些识别的上下文定位相应的PoI。

    arXiv:2403.12566v1 Announce Type: new  Abstract: In the recommender system of Meituan Waimai, we are dealing with ever-lengthening user behavior sequences, which pose an increasing challenge to modeling user preference effectively. Existing sequential recommendation models often fail to capture long-term dependencies or are too complex, complicating the fulfillment of Meituan Waimai's unique business needs. To better model user interests, we consider selecting relevant sub-sequences from users' extensive historical behaviors based on their preferences. In this specific scenario, we've noticed that the contexts in which users interact have a significant impact on their preferences. For this purpose, we introduce a novel method called Context-based Fast Recommendation Strategy to tackle the issue of long sequences. We first identify contexts that share similar user preferences with the target context and then locate the corresponding PoIs based on these identified contexts. This approach
    
[^4]: 基于序列学习过程的列表生成式检索模型

    Listwise Generative Retrieval Models via a Sequential Learning Process

    [https://arxiv.org/abs/2403.12499](https://arxiv.org/abs/2403.12499)

    本文提出了一种基于列表的生成式检索模型，通过引入替代的列表方式来优化相关性，超越了传统的点对点方法。

    

    最近，提出了一种新颖的生成式检索（GR）范式，其中学习一个单一的序列到序列模型，直接生成一个给定查询的相关文档标识符（docids）列表。现有的GR模型通常采用最大似然估计（MLE）进行优化：这涉及最大化给定输入查询的单个相关docid的可能性，假设列表中的每个docid的可能性与其他docid独立。在本文中，我们称这些模型为点对点方法。虽然点对点方法在GR的背景下已被证明是有效的，但由于它忽略了排序涉及对列表进行预测的基本原则，因此被认为是次优的。在本文中，我们通过介绍一种替代的列表方式来解决这一限制，这种方式使GR模型能够在docid列表级别上优化相关性。

    arXiv:2403.12499v1 Announce Type: new  Abstract: Recently, a novel generative retrieval (GR) paradigm has been proposed, where a single sequence-to-sequence model is learned to directly generate a list of relevant document identifiers (docids) given a query. Existing GR models commonly employ maximum likelihood estimation (MLE) for optimization: this involves maximizing the likelihood of a single relevant docid given an input query, with the assumption that the likelihood for each docid is independent of the other docids in the list. We refer to these models as the pointwise approach in this paper. While the pointwise approach has been shown to be effective in the context of GR, it is considered sub-optimal due to its disregard for the fundamental principle that ranking involves making predictions about lists. In this paper, we address this limitation by introducing an alternative listwise approach, which empowers the GR model to optimize the relevance at the docid list level. Specific
    
[^5]: 基于大型语言模型的可解释对话系统用户满意度估计

    Interpretable User Satisfaction Estimation for Conversational Systems with Large Language Models

    [https://arxiv.org/abs/2403.12388](https://arxiv.org/abs/2403.12388)

    本文提出了一种名为SPUR的方法，通过LLMs更有效地从自然语言话语中提取用户满意度的可解释信号，并能够利用迭代提示框架进行用户满意度评估。

    

    准确而可解释的用户满意度估计对于了解、评估和持续改进对话系统至关重要。本文表明，与基于特征化的机器学习模型或文本嵌入的现有方法相比，LLMs能够更有效地从自然语言话语中提取用户满意度的可解释信号。此外，LLM可以通过一个迭代提示框架，并利用标记示例的监督进行用户满意度评估。

    arXiv:2403.12388v1 Announce Type: cross  Abstract: Accurate and interpretable user satisfaction estimation (USE) is critical for understanding, evaluating, and continuously improving conversational systems. Users express their satisfaction or dissatisfaction with diverse conversational patterns in both general-purpose (ChatGPT and Bing Copilot) and task-oriented (customer service chatbot) conversational systems. Existing approaches based on featurized ML models or text embeddings fall short in extracting generalizable patterns and are hard to interpret. In this work, we show that LLMs can extract interpretable signals of user satisfaction from their natural language utterances more effectively than embedding-based approaches. Moreover, an LLM can be tailored for USE via an iterative prompting framework using supervision from labeled examples. The resulting method, Supervised Prompting for User satisfaction Rubrics (SPUR), not only has higher accuracy but is more interpretable as it sco
    
[^6]: 一种用于多模态推荐的对齐和训练框架

    An Aligning and Training Framework for Multimodal Recommendations

    [https://arxiv.org/abs/2403.12384](https://arxiv.org/abs/2403.12384)

    提出了一种名为AlignRec的对齐和训练框架，用于解决多模态推荐中的不对齐问题，通过将推荐目标分解为三个对齐部分，实现内容内部对齐、内容与分类ID之间的对齐以及用户和项目之间的对齐。

    

    随着多媒体应用的发展，多模态推荐正在发挥着重要作用，因为它们可以利用超越用户交互的丰富上下文。现有方法主要将多模态信息视为辅助，用于帮助学习ID特征；然而，多模态内容特征和ID特征之间存在语义差距，直接将多模态信息作为辅助使用会导致用户和项目表示的不对齐。本文首先系统地研究了多模态推荐中的不对齐问题，并提出了一种名为AlignRec的解决方案。在AlignRec中，推荐目标被分解为三个对齐部分，即内容内部对齐，内容与分类ID之间的对齐以及用户和项目之间的对齐。每个对齐部分都由特定的目标函数来表征，并整合到我们的多模态推荐中。

    arXiv:2403.12384v1 Announce Type: cross  Abstract: With the development of multimedia applications, multimodal recommendations are playing an essential role, as they can leverage rich contexts beyond user interactions. Existing methods mainly regard multimodal information as an auxiliary, using them to help learn ID features; however, there exist semantic gaps among multimodal content features and ID features, for which directly using multimodal information as an auxiliary would lead to misalignment in representations of users and items. In this paper, we first systematically investigate the misalignment issue in multimodal recommendations, and propose a solution named AlignRec. In AlignRec, the recommendation objective is decomposed into three alignments, namely alignment within contents, alignment between content and categorical ID, and alignment between users and items. Each alignment is characterized by a specific objective function and is integrated into our multimodal recommendat
    
[^7]: 生成文本流中漂移的方法

    Methods for Generating Drift in Text Streams

    [https://arxiv.org/abs/2403.12328](https://arxiv.org/abs/2403.12328)

    文本数据中概念漂移是一个常见现象，而本文提出了四种文本漂移生成方法来帮助产生具有标记漂移的数据集

    

    arXiv：2403.12328v1 公告类型：跨越 摘要：系统和个体不断产生数据。 在互联网上，人们分享他们的知识，情感和意见，提供关于服务和产品的评论等。 自动从这些文本数据中学习可以为组织和机构提供见解，从而防止财务影响，例如。 为了随时间学习文本数据，机器学习系统必须考虑概念漂移。 概念漂移是现实世界数据集中的频繁现象，对应于时间上的数据分布更改。 例如，当情感变化或单词含义随时间调整时，就会发生概念漂移。 尽管概念漂移在实际应用中很常见，但具有标记漂移的基准数据集在文献中很少见。 为弥补这一差距，本文提供了四种文本漂移生成方法，以便简化产生具有标记漂移的数据集。 这些方法已应用于Ye

    arXiv:2403.12328v1 Announce Type: cross  Abstract: Systems and individuals produce data continuously. On the Internet, people share their knowledge, sentiments, and opinions, provide reviews about services and products, and so on. Automatically learning from these textual data can provide insights to organizations and institutions, thus preventing financial impacts, for example. To learn from textual data over time, the machine learning system must account for concept drift. Concept drift is a frequent phenomenon in real-world datasets and corresponds to changes in data distribution over time. For instance, a concept drift occurs when sentiments change or a word's meaning is adjusted over time. Although concept drift is frequent in real-world applications, benchmark datasets with labeled drifts are rare in the literature. To bridge this gap, this paper provides four textual drift generation methods to ease the production of datasets with labeled drifts. These methods were applied to Ye
    
[^8]: TnT-LLM：大规模语言模型下的文本挖掘

    TnT-LLM: Text Mining at Scale with Large Language Models

    [https://arxiv.org/abs/2403.12173](https://arxiv.org/abs/2403.12173)

    TnT-LLM 提出了一个两阶段框架，利用大规模语言模型自动化生成和分配标签，减少人力成本。

    

    将非结构化文本转换为结构化有意义的形式，通过有用的类别标签进行组织，是文本挖掘中用于下游分析和应用的基础步骤。然而，大多数现有的生成标签分类法和构建基于文本标签的分类器的方法仍然严重依赖于领域专业知识和手动整理，使得这个过程昂贵且耗时。当标签空间不明确且缺少大规模数据注释时，这一挑战尤为严峻。在本文中，我们用大规模语言模型（LLMs）解决了这些挑战，其基于提示的接口有助于引导和使用大规模伪标签。我们提出了TnT-LLM，这是一个两阶段框架，利用LLMs自动化端到端标签生成和分配的过程，减少人力成本。

    arXiv:2403.12173v1 Announce Type: cross  Abstract: Transforming unstructured text into structured and meaningful forms, organized by useful category labels, is a fundamental step in text mining for downstream analysis and application. However, most existing methods for producing label taxonomies and building text-based label classifiers still rely heavily on domain expertise and manual curation, making the process expensive and time-consuming. This is particularly challenging when the label space is under-specified and large-scale data annotations are unavailable. In this paper, we address these challenges with Large Language Models (LLMs), whose prompt-based interface facilitates the induction and use of large-scale pseudo labels. We propose TnT-LLM, a two-phase framework that employs LLMs to automate the process of end-to-end label generation and assignment with minimal human effort for any given use-case. In the first phase, we introduce a zero-shot, multi-stage reasoning approach w
    
[^9]: 通过移动树学习时间段偏好进行下一个POI推荐

    Learning Time Slot Preferences via Mobility Tree for Next POI Recommendation

    [https://arxiv.org/abs/2403.12100](https://arxiv.org/abs/2403.12100)

    本文引入了“移动树”数据结构，用于学习用户跨不同时间段的偏好，以提升下一个POI推荐任务的性能。

    

    下一个兴趣点（POI）推荐任务旨在根据用户当前的签到轨迹提供POI的动态排名。本任务的推荐性能取决于通过基于位置的社交网络（LBSNs）数据全面了解用户个性化的行为模式。本文介绍了一种名为“移动树”的创新数据结构，用于分层描述用户的签到记录。移动树包含多粒度时间段节点，以学习用户跨不同时间段的偏好。同时，我们提出了移动树网络（MTNet）

    arXiv:2403.12100v1 Announce Type: cross  Abstract: Next Point-of-Interests (POIs) recommendation task aims to provide a dynamic ranking of POIs based on users' current check-in trajectories. The recommendation performance of this task is contingent upon a comprehensive understanding of users' personalized behavioral patterns through Location-based Social Networks (LBSNs) data. While prior studies have adeptly captured sequential patterns and transitional relationships within users' check-in trajectories, a noticeable gap persists in devising a mechanism for discerning specialized behavioral patterns during distinct time slots, such as noon, afternoon, or evening. In this paper, we introduce an innovative data structure termed the ``Mobility Tree'', tailored for hierarchically describing users' check-in records. The Mobility Tree encompasses multi-granularity time slot nodes to learn user preferences across varying temporal periods. Meanwhile, we propose the Mobility Tree Network (MTNet
    
[^10]: 丰富用户购物记录：用层次推荐系统赋能电子商务

    Enriching User Shopping History: Empowering E-commerce with a Hierarchical Recommendation System

    [https://arxiv.org/abs/2403.12096](https://arxiv.org/abs/2403.12096)

    通过预测缺失的用户购物历史部分并适当丰富它，可以提高推荐系统的准确性。

    

    推荐系统可以通过分析用户的购物历史提供准确的推荐。更丰富的用户历史会导致更准确的推荐。然而，在实际应用中，用户更倾向于在物品以最低价格的电子商务平台上购物。换句话说，大多数用户同时从多个电子商务平台购物；用户的不同购物历史部分在不同的电子商务平台之间共享。因此，本研究假设任何电子商务平台都拥有用户历史的完整记录，但只能访问其中的一部分。如果推荐系统能够首先预测缺失的部分并适当丰富用户的购物历史，那么将可以更准确地推荐下一个商品。我们的推荐系统利用用户的购物历史来提高预测准确性。所提出的方法在NDCG@10和其他方面都显示出显著改进。

    arXiv:2403.12096v1 Announce Type: cross  Abstract: Recommendation systems can provide accurate recommendations by analyzing user shopping history. A richer user history results in more accurate recommendations. However, in real applications, users prefer e-commerce platforms where the item they seek is at the lowest price. In other words, most users shop from multiple e-commerce platforms simultaneously; different parts of the user's shopping history are shared between different e-commerce platforms. Consequently, we assume in this study that any e-commerce platform has a complete record of the user's history but can only access some parts of it. If a recommendation system is able to predict the missing parts first and enrich the user's shopping history properly, it will be possible to recommend the next item more accurately. Our recommendation system leverages user shopping history to improve prediction accuracy. The proposed approach shows significant improvements in both NDCG@10 and
    
[^11]: 匹配英语地址的方法

    Methods for Matching English Language Addresses

    [https://arxiv.org/abs/2403.12092](https://arxiv.org/abs/2403.12092)

    该研究定义并规范了生成英语地址匹配对的框架，并研究了距离基准方法到深度学习模型等各种方法之间的精度、召回率和准确度，以确定最适合地址匹配任务的方法。

    

    地址在文本数据中占据着一席之地，因为每个词所具有的位置重要性和它所涉及的地理范围。匹配地址的任务每天都在发生，并且存在于邮件重定向、实体解析等各种领域中。我们的工作定义和规范了一个框架，用于生成英语地址的匹配和不匹配对，并将其用于评估各种方法自动执行地址匹配。这些方法从基于距离的方法到深度学习模型各不相同。通过研究这些方法的精度、召回率和准确度指标，我们可以了解到最适合这种地址匹配任务设置的方法。

    arXiv:2403.12092v1 Announce Type: cross  Abstract: Addresses occupy a niche location within the landscape of textual data, due to the positional importance carried by every word, and the geographical scope it refers to. The task of matching addresses happens everyday and is present in various fields like mail redirection, entity resolution, etc. Our work defines, and formalizes a framework to generate matching and mismatching pairs of addresses in the English language, and use it to evaluate various methods to automatically perform address matching. These methods vary widely from distance based approaches to deep learning models. By studying the Precision, Recall and Accuracy metrics of these approaches, we obtain an understanding of the best suited method for this setting of the address matching task.
    
[^12]: 数字病理学中的基础模型和信息检索

    Foundation Models and Information Retrieval in Digital Pathology

    [https://arxiv.org/abs/2403.12090](https://arxiv.org/abs/2403.12090)

    论文回顾了数字病理学中基础模型和信息检索领域的最新进展。

    

    这篇论文回顾了数字病理学中基础模型、LLM、生成式人工智能、信息检索和内容检索的最新技术。

    arXiv:2403.12090v1 Announce Type: cross  Abstract: The paper reviews the state-of-the-art of foundation models, LLMs, generative AI, information retrieval and CBIR in digital pathology
    
[^13]: TMU参加2023年TREC临床试验跟踪

    TMU at TREC Clinical Trials Track 2023

    [https://arxiv.org/abs/2403.12088](https://arxiv.org/abs/2403.12088)

    多伦多都会大学利用自然语言处理技术和神经语言模型参加TREC临床试验跟踪，并展示了其实验结果。

    

    这篇论文描述了多伦多都会大学参加2023年TREC临床试验跟踪。作为任务的一部分，我们在实验中利用了先进的自然语言处理技术和神经语言模型来检索最相关的临床试验。我们阐述了团队-V-TorontoMU的参与的整体方法论、实验设置和实现结果。

    arXiv:2403.12088v1 Announce Type: new  Abstract: This paper describes Toronto Metropolitan University's participation in the TREC Clinical Trials Track for 2023. As part of the tasks, we utilize advanced natural language processing techniques and neural language models in our experiments to retrieve the most relevant clinical trials. We illustrate the overall methodology, experimental settings, and results of our implementation for the run submission as part of Team - V-TorontoMU.
    
[^14]: 使用多通道情感识别的群体电影选择

    Group Movie Selection using Multi-channel Emotion Recognition

    [https://arxiv.org/abs/2403.12087](https://arxiv.org/abs/2403.12087)

    该研究提出了一种利用多通道情感识别进行群体电影选择的方法，为群体决策提供了实用工具

    

    社交活动中经常包括以群体形式观看电视或电影。选择一个符合不同群体情感倾向的电影可能会很棘手。提出了一种利用多种来源（如电影海报、配乐和文本）的情感分析进行群体电影选择的方法学。研究结合了音乐、文本、彩色图像中的情感识别技术和群体决策，为在群体环境中导航电影选择复杂动态提供了实用工具。

    arXiv:2403.12087v1 Announce Type: new  Abstract: Social activities often done in groups include watching television or movies. Choosing a film that appeals to the emotional inclinations of a varied group can be tricky. One of the most difficult aspects of making group movie suggestions is achieving agreement among members. At the same time, emotion is the most important component that connects the film and the viewer. Current research proposes a methodology for group movie selection that employs emotional analysis from numerous sources, such as film posters, soundtracks, and text. Our research stands at the intersection of emotion recognition technology in music, text, color images, and group decision-making, providing a practical tool for navigating the complex dynamics of film selection in a group setting. The survey participants were given emotion categories and asked to select the emotions that best suited a particular movie. Preliminary comparison results between real and predicte
    
[^15]: 展示 Terrorizer：一种用于整合专利受让人中公司名称的算法

    Presenting Terrorizer: an algorithm for consolidating company names in patent assignees

    [https://arxiv.org/abs/2403.12083](https://arxiv.org/abs/2403.12083)

    本文介绍了一种名为Terrorizer的算法，利用自然语言处理、网络理论和基于规则的技术，以解决归因于公司的专利中存在的名称变体问题。

    

    公司名称消歧的问题在从专利中提取有用信息方面提出了重大挑战。这个问题会导致研究结果存在偏差，因为它在很大程度上低估了归因于公司的专利数量，特别是那些以众多名称（包括相同实体的替代拼写和最终公司的子公司）申请专利的跨国公司。迄今为止，解决这些挑战主要依赖于耗时的基于词典或字符串匹配方法，而大多数情况下解决大型数据集上专利受让人的协调问题仍未得到解决。为填补这一空白，本文描述了 Terrorizer 算法，这是一种文本算法，利用自然语言处理（NLP）、网络理论和基于规则的技术来协调作为专利受让人记录的公司名称的变体。具体而言，该算法遵循其前体的三部分结构

    arXiv:2403.12083v1 Announce Type: cross  Abstract: The problem of disambiguation of company names poses a significant challenge in extracting useful information from patents. This issue biases research outcomes as it mostly underestimates the number of patents attributed to companies, particularly multinational corporations which file patents under a plethora of names, including alternate spellings of the same entity and, eventually, companies' subsidiaries. To date, addressing these challenges has relied on labor-intensive dictionary based or string matching approaches, leaving the problem of patents' assignee harmonization on large datasets mostly unresolved. To bridge this gap, this paper describes the Terrorizer algorithm, a text-based algorithm that leverages natural language processing (NLP), network theory, and rule-based techniques to harmonize the variants of company names recorded as patent assignees. In particular, the algorithm follows the tripartite structure of its antece
    
[^16]: 超越节奏：歌曲流行的秘诀？一种机器学习方法

    Beyond Beats: A Recipe to Song Popularity? A machine learning approach

    [https://arxiv.org/abs/2403.12079](https://arxiv.org/abs/2403.12079)

    该研究利用机器学习模型探讨预测歌曲流行度，结果显示流派是影响流行度的主要因素，同时揭示了时间趋势和特征间的复杂关系。

    

    音乐流行度预测引起了工业界和学术界的极大关注，这得益于数据驱动算法和Spotify等流媒体平台的兴起。本研究旨在探讨使用涵盖1957年至2020年各种流派的30,000首歌曲数据集，探讨各种机器学习模型在预测歌曲流行度方面的预测能力。通过普通最小二乘（OLS）、多元自适应回归样条（MARS）、随机森林和XGBoost算法来分析歌曲特征及其对流行度的影响。研究结果显示，普通最小二乘（OLS）回归分析表明流派是流行度的主要影响因素，而且随时间可见显著的趋势。MARS建模突出了变量之间的复杂关系，尤其是与特征如器乐度和时长相关。随机森林和XGBoost模型强调了流派，尤其是电子舞曲在预测中的重要性。

    arXiv:2403.12079v1 Announce Type: cross  Abstract: Music popularity prediction has garnered significant attention in both industry and academia, fuelled by the rise of data-driven algorithms and streaming platforms like Spotify. This study aims to explore the predictive power of various machine learning models in forecasting song popularity using a dataset comprising 30,000 songs spanning different genres from 1957 to 2020. Methods: We employ Ordinary Least Squares (OLS), Multivariate Adaptive Regression Splines (MARS), Random Forest, and XGBoost algorithms to analyse song characteristics and their impact on popularity. Results: Ordinary Least Squares (OLS) regression analysis reveals genre as the primary influencer of popularity, with notable trends over time. MARS modelling highlights the complex relationship between variables, particularly with features like instrumentalness and duration. Random Forest and XGBoost models underscore the importance of genre, especially EDM, in predict
    
[^17]: 评估生成式搜索引擎对对抗性事实问题的健壮性

    Evaluating Robustness of Generative Search Engine on Adversarial Factual Questions

    [https://arxiv.org/abs/2403.12077](https://arxiv.org/abs/2403.12077)

    评估生成式搜索引擎对对抗性事实问题的健壮性，通过对多种生成式搜索引擎进行人类评估，展示了对抗性事实问题在诱导不正确响应方面的有效性。

    

    生成式搜索引擎有潜力改变人们在线获取信息的方式，但现有大型语言模型（LLMs）支持的生成式搜索引擎生成的响应可能并不总是准确。然而，检索增强生成会加剧安全性问题，因为对手可能通过微妙地操纵声明的最薄弱部分成功规避整个系统。因此，我们提出在对抗性事实问题的现实且高风险设置中评估生成式搜索引擎的健壮性，其中对手仅具有黑盒系统访问权限，并试图欺骗模型返回不正确的响应。通过对必应聊天、PerplexityAI和YouChat等各种生成式搜索引擎进行全面的人类评估，我们展示了对抗性事实问题对诱导不正确响应的有效性。此外，检索增强生成展现出...

    arXiv:2403.12077v1 Announce Type: cross  Abstract: Generative search engines have the potential to transform how people seek information online, but generated responses from existing large language models (LLMs)-backed generative search engines may not always be accurate. Nonetheless, retrieval-augmented generation exacerbates safety concerns, since adversaries may successfully evade the entire system by subtly manipulating the most vulnerable part of a claim. To this end, we propose evaluating the robustness of generative search engines in the realistic and high-risk setting, where adversaries have only black-box system access and seek to deceive the model into returning incorrect responses. Through a comprehensive human evaluation of various generative search engines, such as Bing Chat, PerplexityAI, and YouChat across diverse queries, we demonstrate the effectiveness of adversarial factual questions in inducing incorrect responses. Moreover, retrieval-augmented generation exhibits a
    
[^18]: 能否用LLM替代人工标注？ 无人机交付任务下的细粒度中文地址实体识别数据集案例研究

    Can LLM Substitute Human Labeling? A Case Study of Fine-grained Chinese Address Entity Recognition Dataset for UAV Delivery

    [https://arxiv.org/abs/2403.06097](https://arxiv.org/abs/2403.06097)

    提出了适用于无人机交付系统中地址解析任务的细粒度中文姓名实体识别数据集CNER-UAV，包含五个类别的多样化数据，经过严格的数据清洗和去敏处理，约有12,000个标注样本，评估了传统的实体识别模型并提供了深入分析

    

    我们提出了CNER-UAV，一个专为无人机交付系统中地址解析任务设计的细粒度中文姓名实体识别数据集。该数据集涵盖了五个类别，可以全面训练和评估实体识别模型。为构建这一数据集，我们从真实无人机交付系统中获取数据，并进行了严格的数据清洗和去敏处理，确保数据的隐私性和完整性。最终的数据集约包含12,000个标注样本，经过人工专家和大型语言模型的注释。我们在我们的数据集上评估了传统的实体识别模型，并提供了深入分析。数据集和模型可以在 \url{https://github.com/zhhvvv/CNER-UAV} 上公开获取。

    arXiv:2403.06097v1 Announce Type: cross  Abstract: We present CNER-UAV, a fine-grained \textbf{C}hinese \textbf{N}ame \textbf{E}ntity \textbf{R}ecognition dataset specifically designed for the task of address resolution in \textbf{U}nmanned \textbf{A}erial \textbf{V}ehicle delivery systems. The dataset encompasses a diverse range of five categories, enabling comprehensive training and evaluation of NER models. To construct this dataset, we sourced the data from a real-world UAV delivery system and conducted a rigorous data cleaning and desensitization process to ensure privacy and data integrity. The resulting dataset, consisting of around 12,000 annotated samples, underwent human experts and \textbf{L}arge \textbf{L}anguage \textbf{M}odel annotation. We evaluated classical NER models on our dataset and provided in-depth analysis. The dataset and models are publicly available at \url{https://github.com/zhhvvv/CNER-UAV}.
    
[^19]: 探究大型语言模型对推荐系统的影响：一项广泛综述

    Exploring the Impact of Large Language Models on Recommender Systems: An Extensive Review

    [https://arxiv.org/abs/2402.18590](https://arxiv.org/abs/2402.18590)

    大型语言模型在推荐系统中展现出卓越的推荐能力和语言理解，重塑了推荐任务的基础。

    

    该论文强调了大型语言模型（LLMs）在重塑推荐系统中的重要性，将它们的价值归因于传统推荐系统所缺乏的独特推理能力。不同于缺乏直接用户互动数据的传统系统，LLMs在推荐物品方面表现出卓越的能力，展示了它们在理解语言复杂性方面的熟练程度。这标志着推荐领域的一个根本性范式转变。在充满活力的研究领域中，研究人员积极利用LLMs的语言理解和生成能力重新定义推荐任务的基础。该研究彻底探讨了LLMs在推荐框架内固有的优势，包括细致的语境理解，跨不同领域的平稳过渡，采用统一的方法，利用共享数据池的全面学习策略，透明度

    arXiv:2402.18590v1 Announce Type: cross  Abstract: The paper underscores the significance of Large Language Models (LLMs) in reshaping recommender systems, attributing their value to unique reasoning abilities absent in traditional recommenders. Unlike conventional systems lacking direct user interaction data, LLMs exhibit exceptional proficiency in recommending items, showcasing their adeptness in comprehending intricacies of language. This marks a fundamental paradigm shift in the realm of recommendations. Amidst the dynamic research landscape, researchers actively harness the language comprehension and generation capabilities of LLMs to redefine the foundations of recommendation tasks. The investigation thoroughly explores the inherent strengths of LLMs within recommendation frameworks, encompassing nuanced contextual comprehension, seamless transitions across diverse domains, adoption of unified approaches, holistic learning strategies leveraging shared data reservoirs, transparent
    
[^20]: RA-Rec:基于LLM的推荐系统的高效ID表示对齐框架

    RA-Rec: An Efficient ID Representation Alignment Framework for LLM-based Recommendation

    [https://arxiv.org/abs/2402.04527](https://arxiv.org/abs/2402.04527)

    这篇论文提出了一种基于LLM的推荐系统的高效ID表示对齐框架RA-Rec，通过将预训练的ID嵌入到LLMs中，并设计创新的对齐模块和高效调整方法，实现了在推荐系统中的显著性能优化。

    

    大语言模型(LLM)最近已经成为各种自然语言处理任务的强大工具，为LLM和推荐系统的结合带来了新的潮流，称为LLM-based RS。目前的方法通常分为两种主要范例，即ID直接使用范例和ID翻译范例，指出它们的核心弱点在于缺乏推荐知识和独特性。为了解决这个限制，我们提出了一种新的范例，即ID表示，它以一种互补的方式将预训练的ID嵌入到LLMs中。在这项工作中，我们提出了RA-Rec，一种基于LLM的推荐系统的高效ID表示对齐框架，与多种基于ID的方法和LLM架构兼容。具体而言，我们将ID嵌入视为软提示，并设计了一种创新的对齐模块和一种用于对齐的高效调整方法，以及为对齐定制的数据构建。大量实验证明，RA-Rec在性能上显著优于其他方法。

    Large language models (LLM) have recently emerged as a powerful tool for a variety of natural language processing tasks, bringing a new surge of combining LLM with recommendation systems, termed as LLM-based RS. Current approaches generally fall into two main paradigms, the ID direct usage paradigm and the ID translation paradigm, noting their core weakness stems from lacking recommendation knowledge and uniqueness. To address this limitation, we propose a new paradigm, ID representation, which incorporates pre-trained ID embeddings into LLMs in a complementary manner. In this work, we present RA-Rec, an efficient ID representation alignment framework for LLM-based recommendation, which is compatible with multiple ID-based methods and LLM architectures. Specifically, we treat ID embeddings as soft prompts and design an innovative alignment module and an efficient tuning method with tailored data construction for alignment. Extensive experiments demonstrate RA-Rec substantially outperfo
    
[^21]: 实现语义一致性：基于狄利克雷能量的鲁棒多模态实体对齐

    Towards Semantic Consistency: Dirichlet Energy Driven Robust Multi-Modal Entity Alignment

    [https://arxiv.org/abs/2401.17859](https://arxiv.org/abs/2401.17859)

    本研究提出了基于狄利克雷能量的新方法DESAlign，以解决多模态实体对齐中的语义一致性问题。我们发现语义不一致性导致模型过度拟合模态噪声，而DESAlign通过插值缺失的语义并应对过度平滑问题，实现了语义一致性。

    

    在多模态知识图谱（MMKG）中，多模态实体对齐（MMEA）对于识别不同模态属性间的相同实体至关重要。然而，由于缺失模态属性而导致的语义不一致性是一个重要挑战。传统方法依赖于属性插值，但这往往会引入模态噪声，扭曲原始语义。此外，缺乏一个普适的理论框架限制了对语义一致性的进展。本研究引入了一种新方法DESAlign，通过应用基于狄利克雷能量的理论框架来确保语义一致性来解决这些问题。我们发现，语义不一致性导致模型过度拟合模态噪声，特别是在模态缺失时造成性能波动。DESAlign创新地应对了过度平滑问题，并使用现有模态插值缺失的语义。我们的方法包括一个多模态知识图谱学习的部分。

    In Multi-Modal Knowledge Graphs (MMKGs), Multi-Modal Entity Alignment (MMEA) is crucial for identifying identical entities across diverse modal attributes. However, semantic inconsistency, mainly due to missing modal attributes, poses a significant challenge. Traditional approaches rely on attribute interpolation, but this often introduces modality noise, distorting the original semantics. Moreover, the lack of a universal theoretical framework limits advancements in achieving semantic consistency. This study introduces a novel approach, DESAlign, which addresses these issues by applying a theoretical framework based on Dirichlet energy to ensure semantic consistency. We discover that semantic inconsistency leads to model overfitting to modality noise, causing performance fluctuations, particularly when modalities are missing. DESAlign innovatively combats over-smoothing and interpolates absent semantics using existing modalities. Our approach includes a multi-modal knowledge graph lea
    
[^22]: 在非专业LLM用户中为微调，检索增强生成和软提示建立性能基线

    Establishing Performance Baselines in Fine-Tuning, Retrieval-Augmented Generation and Soft-Prompting for Non-Specialist LLM Users

    [https://arxiv.org/abs/2311.05903](https://arxiv.org/abs/2311.05903)

    通过对GPT 3.5进行微调，并结合基于向量的RAG数据库和非算法软提示，建立了性能基线，发现在特定的测试条件下微调模型表现更好。

    

    通过微调、检索增强生成（RAG）和软提示来改善大型语言模型（LLMs）性能的方法研究往往聚焦于使用高度技术化或高成本的技术，使许多新发现的方法对非技术用户来说相对不易访问。在本文中，我们测试了未经修改的GPT 3.5版本、经微调的版本，以及在单独访问基于向量的RAG数据库时相同未经修改的模型，这两种情况下都配备了基本的非算法软提示。在每种情况下，我们测试了模型回答一组主要涉及2021年9月之后事件的100个问题的能力（这是GPT 3.5的训练数据集结束的时间点）。我们发现，如果使用商业平台并应用默认设置，不进行迭代以建立一组基线输出，那么经过微调的模型表现更好。

    arXiv:2311.05903v2 Announce Type: replace-cross  Abstract: Research into methods for improving the performance of large language models (LLMs) through fine-tuning, retrieval-augmented generation (RAG) and soft-prompting has tended to focus on the use of highly technical or high-cost techniques, making many of the newly discovered approaches comparatively inaccessible to non-technical users. In this paper we tested an unmodified version of GPT 3.5, a fine-tuned version, and the same unmodified model when given access to a vectorised RAG database, both in isolation and in combination with a basic, non-algorithmic soft prompt. In each case we tested the model's ability to answer a set of 100 questions relating primarily to events that occurred after September 2021 (the point at which GPT 3.5's training data set ends). We found that if commercial platforms are used and default settings are applied with no iteration in order to establish a baseline set of outputs, a fine-tuned model outperf
    
[^23]: EasyEdit：一种易于使用的大型语言模型知识编辑框架

    EasyEdit: An Easy-to-use Knowledge Editing Framework for Large Language Models

    [https://arxiv.org/abs/2308.07269](https://arxiv.org/abs/2308.07269)

    EasyEdit提出了一种易于使用的知识编辑框架，针对大型语言模型的知识截断或谬误问题，支持各种最新的知识编辑方法，并可应用于多个知名的LLMs。

    

    大型语言模型（LLMs）通常遭受知识截断或谬误问题，这意味着它们对未见事件不知情或生成具有不正确事实的文本，原因是数据过时/嘈杂。为此，出现了许多针对LLMs的知识编辑方法，旨在微妙地注入/编辑更新的知识或调整不良行为，同时将对不相关输入的影响最小化。然而，由于各种知识编辑方法之间存在显著差异，以及任务设置中的变化，社区中没有可用于知识编辑的标准实施框架，这妨碍了从业者将知识编辑应用于应用程序。为解决这些问题，我们提出了EasyEdit，一种易于使用的LLMs知识编辑框架。它支持各种尖端的知识编辑方法，并可以轻松应用于许多著名的LLMs，如T5、GPT-J、LlaMA等。从经验上来看，我们报告了kno

    arXiv:2308.07269v2 Announce Type: replace-cross  Abstract: Large Language Models (LLMs) usually suffer from knowledge cutoff or fallacy issues, which means they are unaware of unseen events or generate text with incorrect facts owing to outdated/noisy data. To this end, many knowledge editing approaches for LLMs have emerged -- aiming to subtly inject/edit updated knowledge or adjust undesired behavior while minimizing the impact on unrelated inputs. Nevertheless, due to significant differences among various knowledge editing methods and the variations in task setups, there is no standard implementation framework available for the community, which hinders practitioners from applying knowledge editing to applications. To address these issues, we propose EasyEdit, an easy-to-use knowledge editing framework for LLMs. It supports various cutting-edge knowledge editing approaches and can be readily applied to many well-known LLMs such as T5, GPT-J, LlaMA, etc. Empirically, we report the kno
    

