# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Model-Free Approximate Bayesian Learning for Large-Scale Conversion Funnel Optimization.](http://arxiv.org/abs/2401.06710) | 该论文提出了一种解决大规模转化漏斗优化问题的无模型近似贝叶斯学习算法，通过建立转化漏斗模型来捕捉消费者行为，并实现了非常高的准确度。 |
| [^2] | [Improved Learned Sparse Retrieval with Corpus-Specific Vocabularies.](http://arxiv.org/abs/2401.06703) | 本论文研究了利用特定于语料库的词汇库改进了学习稀疏检索系统的效率和效果。通过将底层BERT模型针对不同词汇库大小进行预训练，可以显著提高检索质量并减少延迟。该研究还探讨了自定义词汇库、文档扩展技术和稀疏模型的相互作用。这种方法在不同的检索方法中都能取得效果和效率上的改进。 |
| [^3] | [DQNC2S: DQN-based Cross-stream Crisis event Summarizer.](http://arxiv.org/abs/2401.06683) | 本研究提出了一种基于DQN的在线危机事件摘要生成方法，能够同时总结多个灾害相关的数据流，无需人工标注或内容重新排序，且具有较好的性能表现。 |
| [^4] | [LLMRS: Unlocking Potentials of LLM-Based Recommender Systems for Software Purchase.](http://arxiv.org/abs/2401.06676) | LLMRS是一种基于LLM的零-shot推荐系统，可以将用户评论编码为评论分数并生成个性化推荐。实验证明，LLMRS在软件购买方面的推荐性能超过基准模型，并成功从产品评论中捕捉到有意义的信息，提供更可靠的推荐。 |
| [^5] | [The SemIoE Ontology: A Semantic Model Solution for an IoE-based Industry.](http://arxiv.org/abs/2401.06667) | 该论文提出了一种名为SemIoE的新的语义模型，用于解决在IoE基础上的工业中数据整合和处理的问题。 |
| [^6] | [Ada-Retrieval: An Adaptive Multi-Round Retrieval Paradigm for Sequential Recommendations.](http://arxiv.org/abs/2401.06633) | Ada-Retrieval是一种适应性多轮检索范例，用于提升推荐系统的物品候选者选择过程。它通过迭代地改进用户表示来更好地捕捉完整的物品空间中的潜在候选者，并具有模型无关的设计。 |
| [^7] | [Mapping Transformer Leveraged Embeddings for Cross-Lingual Document Representation.](http://arxiv.org/abs/2401.06583) | 本研究通过使用预训练的变形器模型和映射方法，探索了跨语言文档表示的方法。实验结果表明，通过映射到跨语言领域的变形器技术文档表示（TLDRs），能够有效地实现跨语言的推荐系统。 |
| [^8] | [INTERS: Unlocking the Power of Large Language Models in Search with Instruction Tuning.](http://arxiv.org/abs/2401.06532) | 本研究探索了指令调优的方法，以增强大型语言模型在信息检索任务中的能力，通过引入一个新的指令调优数据集INTERS，涵盖了21个IR任务，该方法显著提升了性能。 |
| [^9] | [UNEX-RL: Reinforcing Long-Term Rewards in Multi-Stage Recommender Systems with UNidirectional EXecution.](http://arxiv.org/abs/2401.06470) | 本论文提出了一种名为UNEX-RL的基于单向执行的多智能体强化学习框架，用于优化多阶段推荐系统中的长期奖励。该框架通过解决观察依赖性和级联效应的挑战，有效地提高了推荐系统的性能。 |
| [^10] | [Improving Graph Convolutional Networks with Transformer Layer in social-based items recommendation.](http://arxiv.org/abs/2401.06436) | 本文提出了一种在社交网络中预测评分的方法，该方法通过在GCN模型中引入Transformer层，在节点嵌入方面取得了更好的性能。 |
| [^11] | [TRACE: A Time-Relational Approximate Cubing Engine for Fast Data Insights.](http://arxiv.org/abs/2401.06336) | TRACE是一个时间关系近似立方引擎，可以以较低成本进行交互式分析，并支持各种用户定义的指标。 |
| [^12] | [Zero-shot Generative Large Language Models for Systematic Review Screening Automation.](http://arxiv.org/abs/2401.06320) | 本研究调查了使用零样本生成式大型语言模型进行系统性综述自动筛选的有效性，结果显示指导微调和校准技术在筛选中起到重要作用，并且与零样本模型的集成相结合可以显著节省筛选时间。 |
| [^13] | [MuGI: Enhancing Information Retrieval through Multi-Text Generation Intergration with Large Language Models.](http://arxiv.org/abs/2401.06311) | MuGI是一个简单而有效的多文本生成集成框架，它通过与大型语言模型合作生成多个伪参考文献，并将其与查询集成以提升信息检索性能。在实验中，MuGI模型在TREC DL数据集上的BM25性能上取得了18%以上的增强，并在BEIR上提高了7.5%。 |
| [^14] | [MultiSlot ReRanker: A Generic Model-based Re-Ranking Framework in Recommendation Systems.](http://arxiv.org/abs/2401.06293) | 多插槽重新排序器是一个通用的基于模型的重新排序框架，在推荐系统中同时优化相关性、多样性和新鲜度。它通过建模物品之间的相互影响和利用多个目标的第二次排序得分来提高离线AUC，并通过离线回放理论在多个目标之间进行权衡，进一步改善离线回放结果。 |
| [^15] | [Learning Unsupervised Semantic Document Representation for Fine-grained Aspect-based Sentiment Analysis.](http://arxiv.org/abs/2401.06210) | 这篇论文研究了学习无监督的语义文档表示以进行细粒度的基于方面的情感分析。通过克服现有方法的困难，实验证明该模型在各个任务上的性能优于最先进方法。 |
| [^16] | [A Comprehensive Survey of Evaluation Techniques for Recommendation Systems.](http://arxiv.org/abs/2312.16015) | 本文介绍了一套综合的推荐系统评估指标，包括相似性指标、候选生成指标、预测指标、排序指标和业务指标。 |
| [^17] | [Leveraging Large Language Models (LLMs) to Empower Training-Free Dataset Condensation for Content-Based Recommendation.](http://arxiv.org/abs/2310.09874) | 本文利用大型语言模型（LLMs）来增强基于内容的推荐中的免训练数据集压缩方法，旨在通过生成文本内容来合成一个小而信息丰富的数据集，使得模型能够达到与在大型数据集上训练的模型相当的性能。 |
| [^18] | [Predicting Group Choices from Group Profiles.](http://arxiv.org/abs/2308.03083) | 本文旨在通过使用机器学习方法和观测到的群体选择数据集，验证了相较于标准偏好聚合策略，可以更好地预测群体的最终选择。 |

# 详细

[^1]: 大规模转化漏斗优化的无模型近似贝叶斯学习

    Model-Free Approximate Bayesian Learning for Large-Scale Conversion Funnel Optimization. (arXiv:2401.06710v1 [cs.LG])

    [http://arxiv.org/abs/2401.06710](http://arxiv.org/abs/2401.06710)

    该论文提出了一种解决大规模转化漏斗优化问题的无模型近似贝叶斯学习算法，通过建立转化漏斗模型来捕捉消费者行为，并实现了非常高的准确度。

    

    在现代营销活动中，根据消费者状态选择广告行动的灵活性至关重要。我们研究了识别最优顺序个性化干预以最大化针对新产品的采纳概率的问题。我们通过一个转化漏斗模型来建模消费者行为，该模型捕捉到每个消费者的状态（例如与公司的互动历史）并允许消费者行为随着其状态和公司的顺序干预而变化。我们展示了我们的模型在真实世界的电子邮件营销数据集中以非常高的准确度（超过0.95的样本外AUC）捕捉到消费者行为。然而，这导致了一个非常大规模的学习问题，公司必须从消费者交互中学习各种干预的状态特定效应。我们提出了一个新的基于归因的决策算法来解决这个问题，我们称之为无模型近似贝叶斯学习。

    The flexibility of choosing the ad action as a function of the consumer state is critical for modern-day marketing campaigns. We study the problem of identifying the optimal sequential personalized interventions that maximize the adoption probability for a new product. We model consumer behavior by a conversion funnel that captures the state of each consumer (e.g., interaction history with the firm) and allows the consumer behavior to vary as a function of both her state and firm's sequential interventions. We show our model captures consumer behavior with very high accuracy (out-of-sample AUC of over 0.95) in a real-world email marketing dataset. However, it results in a very large-scale learning problem, where the firm must learn the state-specific effects of various interventions from consumer interactions. We propose a novel attribution-based decision-making algorithm for this problem that we call model-free approximate Bayesian learning. Our algorithm inherits the interpretability
    
[^2]: 借助特定于语料库的词汇库改进了学习稀疏检索

    Improved Learned Sparse Retrieval with Corpus-Specific Vocabularies. (arXiv:2401.06703v1 [cs.IR])

    [http://arxiv.org/abs/2401.06703](http://arxiv.org/abs/2401.06703)

    本论文研究了利用特定于语料库的词汇库改进了学习稀疏检索系统的效率和效果。通过将底层BERT模型针对不同词汇库大小进行预训练，可以显著提高检索质量并减少延迟。该研究还探讨了自定义词汇库、文档扩展技术和稀疏模型的相互作用。这种方法在不同的检索方法中都能取得效果和效率上的改进。

    

    我们探索了借助特定于语料库的词汇库来改善学习稀疏检索系统的效率和效果。我们发现，将底层BERT模型预训练于目标语料库，并针对文档扩展过程中不同的词汇库大小进行特定调整，可以将检索质量提高最多12％，同时在某些情况下将延迟降低最多50％。我们的实验证明，采用特定于语料库的词汇库并增加词汇库大小可以减少平均倒排列表长度，从而降低延迟。消融研究显示了自定义词汇库、文档扩展技术和稀疏模型的稀疏化目标之间的有趣互动。效果和效率的改进可以迁移到不同的检索方法，如uniCOIL和SPLADE，并提供了一种简单而有效的方法来提供学习稀疏检索系统的新的效率-效果权衡。

    We explore leveraging corpus-specific vocabularies that improve both efficiency and effectiveness of learned sparse retrieval systems. We find that pre-training the underlying BERT model on the target corpus, specifically targeting different vocabulary sizes incorporated into the document expansion process, improves retrieval quality by up to 12% while in some scenarios decreasing latency by up to 50%. Our experiments show that adopting corpus-specific vocabulary and increasing vocabulary size decreases average postings list length which in turn reduces latency. Ablation studies show interesting interactions between custom vocabularies, document expansion techniques, and sparsification objectives of sparse models. Both effectiveness and efficiency improvements transfer to different retrieval approaches such as uniCOIL and SPLADE and offer a simple yet effective approach to providing new efficiency-effectiveness trade-offs for learned sparse retrieval systems.
    
[^3]: DQNC2S：基于DQN的跨流危机事件摘要生成器

    DQNC2S: DQN-based Cross-stream Crisis event Summarizer. (arXiv:2401.06683v1 [cs.IR])

    [http://arxiv.org/abs/2401.06683](http://arxiv.org/abs/2401.06683)

    本研究提出了一种基于DQN的在线危机事件摘要生成方法，能够同时总结多个灾害相关的数据流，无需人工标注或内容重新排序，且具有较好的性能表现。

    

    同时总结多个与灾害相关的数据流尤其具有挑战性，因为现有的检索与重新排序策略在多流数据的固有冗余和多查询环境下的限制可扩展性方面存在问题。本文提出了一种基于弱标注和深度Q网络的在线危机时间轴生成方法。它能够实时选择相关的文本片段，无需人工标注或内容重新排序，从而使推理时间与输入查询的数量无关。该方法还将冗余过滤器融入奖励函数中，以有效处理跨流内容重叠。在CrisisFACTS 2022基准测试中，所达到的ROUGE和BERTScore结果优于最佳性能模型。

    Summarizing multiple disaster-relevant data streams simultaneously is particularly challenging as existing Retrieve&Re-ranking strategies suffer from the inherent redundancy of multi-stream data and limited scalability in a multi-query setting. This work proposes an online approach to crisis timeline generation based on weak annotation with Deep Q-Networks. It selects on-the-fly the relevant pieces of text without requiring neither human annotations nor content re-ranking. This makes the inference time independent of the number of input queries. The proposed approach also incorporates a redundancy filter into the reward function to effectively handle cross-stream content overlaps. The achieved ROUGE and BERTScore results are superior to those of best-performing models on the CrisisFACTS 2022 benchmark.
    
[^4]: LLMRS: 解锁基于LLM的推荐系统对软件购买的潜力

    LLMRS: Unlocking Potentials of LLM-Based Recommender Systems for Software Purchase. (arXiv:2401.06676v1 [cs.IR])

    [http://arxiv.org/abs/2401.06676](http://arxiv.org/abs/2401.06676)

    LLMRS是一种基于LLM的零-shot推荐系统，可以将用户评论编码为评论分数并生成个性化推荐。实验证明，LLMRS在软件购买方面的推荐性能超过基准模型，并成功从产品评论中捕捉到有意义的信息，提供更可靠的推荐。

    

    推荐系统无处不在，从Spotify的歌单推荐到亚马逊的产品推荐。然而，根据方法或数据集的不同，这些系统通常无法捕捉用户的偏好并生成普适的推荐。最近大型语言模型（LLM）的进展为分析用户查询提供了有希望的结果。然而，利用这些模型来捕捉用户的偏好和提高效率仍然是一个未解决的问题。在本文中，我们提出了LLMRS，一种基于LLM的零-shot推荐系统，我们利用预训练的LLM将用户评论编码为评论分数，并生成个性化的推荐。我们在亚马逊产品评论的真实数据集上对LLMRS进行了实验，用于软件购买的使用案例。结果表明，LLMRS优于基于排名的基准模型，同时成功从产品评论中捕捉到有意义的信息，从而提供更可靠的推荐。

    Recommendation systems are ubiquitous, from Spotify playlist suggestions to Amazon product suggestions. Nevertheless, depending on the methodology or the dataset, these systems typically fail to capture user preferences and generate general recommendations. Recent advancements in Large Language Models (LLM) offer promising results for analyzing user queries. However, employing these models to capture user preferences and efficiency remains an open question. In this paper, we propose LLMRS, an LLM-based zero-shot recommender system where we employ pre-trained LLM to encode user reviews into a review score and generate user-tailored recommendations. We experimented with LLMRS on a real-world dataset, the Amazon product reviews, for software purchase use cases. The results show that LLMRS outperforms the ranking-based baseline model while successfully capturing meaningful information from product reviews, thereby providing more reliable recommendations.
    
[^5]: SemIoE本体论: 一种面向基于IoE的工业的语义模型解决方案

    The SemIoE Ontology: A Semantic Model Solution for an IoE-based Industry. (arXiv:2401.06667v1 [cs.IR])

    [http://arxiv.org/abs/2401.06667](http://arxiv.org/abs/2401.06667)

    该论文提出了一种名为SemIoE的新的语义模型，用于解决在IoE基础上的工业中数据整合和处理的问题。

    

    最近，工业5.0作为一种新兴的范式，定义了迈向更智能、环保意识更强、以用户为中心的数字系统的具体步骤。在智能设备在工业领域中越来越复杂和自主的时代，物联网及其演变，即物联网（IoE），涉及人员、机器人、过程和数据在网络中，代表了允许行业将人类的经验和需求置于其生态系统中心的主要推动因素。然而，由于所涉及实体的极端异构性、它们的内在需求和合作能力以及适应动态用户中心环境的目标，对于处理和整合IoE所产生的数据需要特别关注。本文的目标就是提出一种新的语义模型，形式化了基本的行动者，

    Recently, the Industry 5.0 is gaining attention as a novel paradigm, defining the next concrete steps toward more and more intelligent, green-aware and user-centric digital systems. In an era in which smart devices typically adopted in the industry domain are more and more sophisticated and autonomous, the Internet of Things and its evolution, known as the Internet of Everything (IoE, for short), involving also people, robots, processes and data in the network, represent the main driver to allow industries to put the experiences and needs of human beings at the center of their ecosystems. However, due to the extreme heterogeneity of the involved entities, their intrinsic need and capability to cooperate, and the aim to adapt to a dynamic user-centric context, special attention is required for the integration and processing of the data produced by such an IoE. This is the objective of the present paper, in which we propose a novel semantic model that formalizes the fundamental actors, e
    
[^6]: Ada-Retrieval：适应性多轮检索范例用于顺序推荐

    Ada-Retrieval: An Adaptive Multi-Round Retrieval Paradigm for Sequential Recommendations. (arXiv:2401.06633v1 [cs.IR])

    [http://arxiv.org/abs/2401.06633](http://arxiv.org/abs/2401.06633)

    Ada-Retrieval是一种适应性多轮检索范例，用于提升推荐系统的物品候选者选择过程。它通过迭代地改进用户表示来更好地捕捉完整的物品空间中的潜在候选者，并具有模型无关的设计。

    

    检索模型旨在选择与给定用户偏好匹配的一小组物品候选者。它们在大规模推荐系统中起着重要作用，因为后续的模型（如排名器）高度依赖于物品候选者的质量。然而，大多数现有的检索模型采用单轮推理范例，可能无法充分捕捉用户偏好的动态性并固定在物品空间的某个区域。在本文中，我们提出了Ada-Retrieval，一种适用于推荐系统的自适应多轮检索范例，通过迭代地改进用户表示来更好地捕捉完整的物品空间中的潜在候选者。Ada-Retrieval包含两个关键模块：物品表示适配器和用户表示适配器，旨在将上下文信息注入物品和用户的表示中。该框架具有模型无关的设计，可以与各种基础模型（如RNN或Transformer）无缝集成。

    Retrieval models aim at selecting a small set of item candidates which match the preference of a given user. They play a vital role in large-scale recommender systems since subsequent models such as rankers highly depend on the quality of item candidates. However, most existing retrieval models employ a single-round inference paradigm, which may not adequately capture the dynamic nature of user preferences and stuck in one area in the item space. In this paper, we propose Ada-Retrieval, an adaptive multi-round retrieval paradigm for recommender systems that iteratively refines user representations to better capture potential candidates in the full item space. Ada-Retrieval comprises two key modules: the item representation adapter and the user representation adapter, designed to inject context information into items' and users' representations. The framework maintains a model-agnostic design, allowing seamless integration with various backbone models such as RNNs or Transformers. We pe
    
[^7]: 将变形器技术应用于跨语言文档表示的映射

    Mapping Transformer Leveraged Embeddings for Cross-Lingual Document Representation. (arXiv:2401.06583v1 [cs.CL])

    [http://arxiv.org/abs/2401.06583](http://arxiv.org/abs/2401.06583)

    本研究通过使用预训练的变形器模型和映射方法，探索了跨语言文档表示的方法。实验结果表明，通过映射到跨语言领域的变形器技术文档表示（TLDRs），能够有效地实现跨语言的推荐系统。

    

    推荐系统对于文档已经成为在网络上找到相关内容的工具。然而，当推荐非查询语言的文档时，这些系统存在一定限制，可能会忽视非母语的资源。本研究旨在通过使用映射到跨语言领域的变形器技术文档表示（TLDRs）来表示跨语言文档。评估了四个多语言预训练变形器模型（mBERT，mT5 XLM RoBERTa，ErnieM）在20种语言对上使用三种映射方法的效果，这些语言对代表了欧盟选择的五种语言的组合。使用Mate检索率和互惠排序等指标来衡量映射TLDRs与未映射TLDRs的效果。结果强调了通过预训练变形器和映射方法实现的跨语言表示的能力，为扩展跨语言文档表示提供了有希望的方向。

    Recommendation systems, for documents, have become tools to find relevant content on the Web. However, these systems have limitations when it comes to recommending documents in languages different from the query language, which means they might overlook resources in non-native languages. This research focuses on representing documents across languages by using Transformer Leveraged Document Representations (TLDRs) that are mapped to a cross-lingual domain. Four multilingual pre-trained transformer models (mBERT, mT5 XLM RoBERTa, ErnieM) were evaluated using three mapping methods across 20 language pairs representing combinations of five selected languages of the European Union. Metrics like Mate Retrieval Rate and Reciprocal Rank were used to measure the effectiveness of mapped TLDRs compared to non-mapped ones. The results highlight the power of cross-lingual representations achieved through pre-trained transformers and mapping approaches suggesting a promising direction for expanding
    
[^8]: INTERS: 使用指令调优解锁大型语言模型在搜索中的力量

    INTERS: Unlocking the Power of Large Language Models in Search with Instruction Tuning. (arXiv:2401.06532v1 [cs.CL])

    [http://arxiv.org/abs/2401.06532](http://arxiv.org/abs/2401.06532)

    本研究探索了指令调优的方法，以增强大型语言模型在信息检索任务中的能力，通过引入一个新的指令调优数据集INTERS，涵盖了21个IR任务，该方法显著提升了性能。

    

    大型语言模型（LLMs）在各种自然语言处理任务中展示了令人印象深刻的能力。然而，由于许多与信息检索（IR）具体概念的不经常出现在自然语言中，它们在信息检索任务中的应用仍然具有挑战性。虽然基于提示的方法可以向LLMs提供任务描述，但它们往往在促进全面理解和执行IR任务方面存在不足，从而限制了LLMs的适用性。为了弥补这一差距，本研究探索了指令调优的潜力，以提高LLMs在IR任务中的熟练程度。我们引入了一个新的指令调优数据集INTERS，涵盖了3个基本IR类别中的21个任务：查询理解、文档理解和查询文档关系理解。数据来自43个不同的由手动编写的模板构成的数据集。我们的实证结果表明，INTERS显著提升了各种公开数据集上的性能。

    Large language models (LLMs) have demonstrated impressive capabilities in various natural language processing tasks. Despite this, their application to information retrieval (IR) tasks is still challenging due to the infrequent occurrence of many IR-specific concepts in natural language. While prompt-based methods can provide task descriptions to LLMs, they often fall short in facilitating comprehensive understanding and execution of IR tasks, thereby limiting LLMs' applicability. To address this gap, in this work, we explore the potential of instruction tuning to enhance LLMs' proficiency in IR tasks. We introduce a novel instruction tuning dataset, INTERS, encompassing 21 tasks across three fundamental IR categories: query understanding, document understanding, and query-document relationship understanding. The data are derived from 43 distinct datasets with manually written templates. Our empirical results reveal that INTERS significantly boosts the performance of various publicly a
    
[^9]: UNEX-RL: 在多阶段推荐系统中通过单向执行加强长期奖励

    UNEX-RL: Reinforcing Long-Term Rewards in Multi-Stage Recommender Systems with UNidirectional EXecution. (arXiv:2401.06470v1 [cs.IR])

    [http://arxiv.org/abs/2401.06470](http://arxiv.org/abs/2401.06470)

    本论文提出了一种名为UNEX-RL的基于单向执行的多智能体强化学习框架，用于优化多阶段推荐系统中的长期奖励。该框架通过解决观察依赖性和级联效应的挑战，有效地提高了推荐系统的性能。

    

    近年来，越来越多的人开始关注利用强化学习（RL）来优化推荐系统中的长期奖励。由于工业级的推荐系统通常是设计为多阶段系统，使用单个智能体的RL方法在同时优化多个阶段时面临挑战。原因是不同的阶段具有不同的观察空间，因此不能由单个智能体建模。为了解决这个问题，我们提出了一种新的基于UNidirectional-EXecution的多智能体强化学习框架（UNEX-RL），用于加强多阶段推荐系统中的长期奖励。我们展示了单向执行是多阶段推荐系统的一个关键特性，对多智能体强化学习（MARL）的应用带来了新的挑战，即观察依赖性和级联效应。为了解决这些挑战，我们提供了一种级联信息链（CIC）方法来分离独立的观察信息。

    In recent years, there has been a growing interest in utilizing reinforcement learning (RL) to optimize long-term rewards in recommender systems. Since industrial recommender systems are typically designed as multi-stage systems, RL methods with a single agent face challenges when optimizing multiple stages simultaneously. The reason is that different stages have different observation spaces, and thus cannot be modeled by a single agent. To address this issue, we propose a novel UNidirectional-EXecution-based multi-agent Reinforcement Learning (UNEX-RL) framework to reinforce the long-term rewards in multi-stage recommender systems. We show that the unidirectional execution is a key feature of multi-stage recommender systems, bringing new challenges to the applications of multi-agent reinforcement learning (MARL), namely the observation dependency and the cascading effect. To tackle these challenges, we provide a cascading information chain (CIC) method to separate the independent obse
    
[^10]: 在基于社交网络的物品推荐中，用Transformer层改进图卷积网络

    Improving Graph Convolutional Networks with Transformer Layer in social-based items recommendation. (arXiv:2401.06436v1 [cs.LG])

    [http://arxiv.org/abs/2401.06436](http://arxiv.org/abs/2401.06436)

    本文提出了一种在社交网络中预测评分的方法，该方法通过在GCN模型中引入Transformer层，在节点嵌入方面取得了更好的性能。

    

    在这项工作中，我们提出了一种改进GCN在社交网络中预测评分的方法。我们的模型在标准模型的基础上扩展了几层Transformer架构。论文的主要焦点是网络中节点嵌入的编码器架构。使用来自基于图的卷积层的嵌入层，注意机制可以重新排列特征空间，为下游任务获取更高效的嵌入。实验表明，我们提出的架构在传统的链接预测任务上表现优于GCN。

    In this work, we have proposed an approach for improving the GCN for predicting ratings in social networks. Our model is expanded from the standard model with several layers of transformer architecture. The main focus of the paper is on the encoder architecture for node embedding in the network. Using the embedding layer from the graph-based convolution layer, the attention mechanism could rearrange the feature space to get a more efficient embedding for the downstream task. The experiments showed that our proposed architecture achieves better performance than GCN on the traditional link prediction task.
    
[^11]: TRACE:一种用于快速数据洞察的时间关系近似立方引擎

    TRACE: A Time-Relational Approximate Cubing Engine for Fast Data Insights. (arXiv:2401.06336v1 [cs.IR])

    [http://arxiv.org/abs/2401.06336](http://arxiv.org/abs/2401.06336)

    TRACE是一个时间关系近似立方引擎，可以以较低成本进行交互式分析，并支持各种用户定义的指标。

    

    数据问题的大类可以被建模为通过用户定义的指标来确定重要的数据片段。本文介绍了TRACE，一种时间关系近似立方引擎，它可以以较低的前期成本（包括空间和计算）在这些片段上进行交互式分析。通过逐步实现立方体的最重要部分，TRACE支持对一大类分析查询的交互式查询，例如我的业务中哪个部分的收入增长最快（[SubCategory=Sports Equipment, Gender=Female]），哪些片段的每用户收入滞后（[State=CA, Age=20-30]）。TRACE支持许多用户定义的指标，包括常见的聚合（如SUM、COUNT、DISTINCT COUNT）和更复杂的指标（如AVERAGE）。我们针对各种商业用例实施和部署了TRACE。

    A large class of data questions can be modeled as identifying important slices of data driven by user defined metrics. This paper presents TRACE, a Time-Relational Approximate Cubing Engine that enables interactive analysis on such slices with a low upfront cost - both in space and computation. It does this by materializing the most important parts of the cube over time enabling interactive querying for a large class of analytical queries e.g. what part of my business has the highest revenue growth ([SubCategory=Sports Equipment, Gender=Female]), what slices are lagging in revenue per user ([State=CA, Age=20-30]). Many user defined metrics are supported including common aggregations such as SUM, COUNT, DISTINCT COUNT and more complex ones such as AVERAGE. We implemented and deployed TRACE for a variety of business use cases.
    
[^12]: 零样本生成式大型语言模型用于系统性综述筛选自动化

    Zero-shot Generative Large Language Models for Systematic Review Screening Automation. (arXiv:2401.06320v1 [cs.IR])

    [http://arxiv.org/abs/2401.06320](http://arxiv.org/abs/2401.06320)

    本研究调查了使用零样本生成式大型语言模型进行系统性综述自动筛选的有效性，结果显示指导微调和校准技术在筛选中起到重要作用，并且与零样本模型的集成相结合可以显著节省筛选时间。

    

    系统性综述对于基于证据的医学非常重要，它们综合分析了特定问题的已发表研究结果。进行此类综述通常需要大量的资源和时间，特别是在筛选阶段，需要评估出版物摘要是否应包括在综述中。本研究调查了使用零样本大型语言模型（LLM）进行自动筛选的有效性。我们评估了八种不同的LLM的效果，并研究了一种使用预定义的召回阈值的校准技术，用于确定是否应将出版物包括在系统性综述中。我们的全面评估使用了五个标准测试集，结果显示指导微调在筛选中起到了重要作用，校准使LLMs在实现目标召回方面更实用，并且将这两者与零样本模型的集成相结合与现有技术相比节省了大量筛选时间。

    Systematic reviews are crucial for evidence-based medicine as they comprehensively analyse published research findings on specific questions. Conducting such reviews is often resource- and time-intensive, especially in the screening phase, where abstracts of publications are assessed for inclusion in a review. This study investigates the effectiveness of using zero-shot large language models~(LLMs) for automatic screening. We evaluate the effectiveness of eight different LLMs and investigate a calibration technique that uses a predefined recall threshold to determine whether a publication should be included in a systematic review. Our comprehensive evaluation using five standard test collections shows that instruction fine-tuning plays an important role in screening, that calibration renders LLMs practical for achieving a targeted recall, and that combining both with an ensemble of zero-shot models saves significant screening time compared to state-of-the-art approaches.
    
[^13]: MuGI:通过与大型语言模型的多文本生成集成增强信息检索

    MuGI: Enhancing Information Retrieval through Multi-Text Generation Intergration with Large Language Models. (arXiv:2401.06311v1 [cs.IR])

    [http://arxiv.org/abs/2401.06311](http://arxiv.org/abs/2401.06311)

    MuGI是一个简单而有效的多文本生成集成框架，它通过与大型语言模型合作生成多个伪参考文献，并将其与查询集成以提升信息检索性能。在实验中，MuGI模型在TREC DL数据集上的BM25性能上取得了18%以上的增强，并在BEIR上提高了7.5%。

    

    大型语言模型（LLM）已经成为语言技术领域的一个重要力量。它们强大的推理能力和广泛的知识库使其在各个自然语言处理领域，包括信息检索（IR）方面具备了出色的零-shot泛化能力。在本文中，我们对LLM生成的文档在IR中的实用性进行了深入研究。我们引入了一个简单而有效的框架，即多文本生成集成（MuGI），来增强现有的IR方法。具体而言，我们引导LLM生成多个伪参考文献，并将其与查询进行集成以进行检索。无需训练的MuGI模型超越了现有的查询扩展策略，在TREC DL数据集上的BM25上取得了新的标准，并在BEIR上提高了7.5%。通过MuGI，我们构建了一个快速且高保真度的重排序方法。

    Large Language Models (LLMs) have emerged as a pivotal force in language technology. Their robust reasoning capabilities and expansive knowledge repositories have enabled exceptional zero-shot generalization abilities across various facets of the natural language processing field, including information retrieval (IR). In this paper, we conduct an in-depth investigation into the utility of documents generated by LLMs for IR. We introduce a simple yet effective framework, Multi-Text Generation Integration (MuGI), to augment existing IR methodologies. Specifically, we prompt LLMs to generate multiple pseudo references and integrate with query for retrieval. The training-free MuGI model eclipses existing query expansion strategies, setting a new standard in sparse retrieval. It outstrips supervised counterparts like ANCE and DPR, achieving a notable over 18% enhancement in BM25 on the TREC DL dataset and a 7.5% increase on BEIR. Through MuGI, we have forged a rapid and high-fidelity re-ran
    
[^14]: 多插槽重新排序器: 推荐系统中的通用基于模型的重新排序框架

    MultiSlot ReRanker: A Generic Model-based Re-Ranking Framework in Recommendation Systems. (arXiv:2401.06293v1 [cs.AI])

    [http://arxiv.org/abs/2401.06293](http://arxiv.org/abs/2401.06293)

    多插槽重新排序器是一个通用的基于模型的重新排序框架，在推荐系统中同时优化相关性、多样性和新鲜度。它通过建模物品之间的相互影响和利用多个目标的第二次排序得分来提高离线AUC，并通过离线回放理论在多个目标之间进行权衡，进一步改善离线回放结果。

    

    本文提出了一种通用的基于模型的重新排序框架——多插槽重新排序器，它同时优化相关性、多样性和新鲜度。具体而言，我们的顺序贪心算法（SGA）足够高效（线性时间复杂度）用于大规模生产推荐引擎。它在离线AUC（接收器操作特征曲线下面积）上取得了6%至10%的提升，主要是由于在列表中明确建模了物品之间的相互影响，并利用了多个目标的第二次排序得分。此外，我们将离线回放理论推广到多插槽重新排序场景，通过在多个目标之间进行权衡来改善离线回放结果。离线回放结果还可以通过Pareto最优性进一步改进。此外，我们还基于OpenAI Gym和Ray框架构建了一个多插槽重新排序模拟器。它可以根据不同的假设进行简单配置，快速评估强化学习和...

    In this paper, we propose a generic model-based re-ranking framework, MultiSlot ReRanker, which simultaneously optimizes relevance, diversity, and freshness. Specifically, our Sequential Greedy Algorithm (SGA) is efficient enough (linear time complexity) for large-scale production recommendation engines. It achieved a lift of $+6\%$ to $ +10\%$ offline Area Under the receiver operating characteristic Curve (AUC) which is mainly due to explicitly modeling mutual influences among items of a list, and leveraging the second pass ranking scores of multiple objectives. In addition, we have generalized the offline replay theory to multi-slot re-ranking scenarios, with trade-offs among multiple objectives. The offline replay results can be further improved by Pareto Optimality. Moreover, we've built a multi-slot re-ranking simulator based on OpenAI Gym integrated with the Ray framework. It can be easily configured for different assumptions to quickly benchmark both reinforcement learning and s
    
[^15]: 学习无监督的语义文档表示以进行细粒度的基于方面的情感分析

    Learning Unsupervised Semantic Document Representation for Fine-grained Aspect-based Sentiment Analysis. (arXiv:2401.06210v1 [cs.LG])

    [http://arxiv.org/abs/2401.06210](http://arxiv.org/abs/2401.06210)

    这篇论文研究了学习无监督的语义文档表示以进行细粒度的基于方面的情感分析。通过克服现有方法的困难，实验证明该模型在各个任务上的性能优于最先进方法。

    

    文档表示是机器理解中许多自然语言处理任务的核心。以无监督方式学习的一般表示保留了通用性，可用于各种应用。在实践中，情感分析（SA）是一个挑战性的任务，被认为与语义密切相关，并经常用于评估一般表示。现有的无监督文档表示学习方法可以分为两类：序列方法（显式考虑单词的顺序）和非序列方法（不显式考虑顺序）。然而，它们都有各自的缺点。在本文中，我们提出了一个模型，克服了这两类方法遇到的困难。实验证明，我们的模型在流行的SA数据集和细粒度的基于方面的SA上优于现有的最先进方法。

    Document representation is the core of many NLP tasks on machine understanding. A general representation learned in an unsupervised manner reserves generality and can be used for various applications. In practice, sentiment analysis (SA) has been a challenging task that is regarded to be deeply semantic-related and is often used to assess general representations. Existing methods on unsupervised document representation learning can be separated into two families: sequential ones, which explicitly take the ordering of words into consideration, and non-sequential ones, which do not explicitly do so. However, both of them suffer from their own weaknesses. In this paper, we propose a model that overcomes difficulties encountered by both families of methods. Experiments show that our model outperforms state-of-the-art methods on popular SA datasets and a fine-grained aspect-based SA by a large margin.
    
[^16]: 用于推荐系统评估的综合调查

    A Comprehensive Survey of Evaluation Techniques for Recommendation Systems. (arXiv:2312.16015v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2312.16015](http://arxiv.org/abs/2312.16015)

    本文介绍了一套综合的推荐系统评估指标，包括相似性指标、候选生成指标、预测指标、排序指标和业务指标。

    

    推荐系统的有效性对于用户在在线平台上的参与和满意度至关重要。随着这些推荐系统越来越影响用户的选择，它们的评估不仅仅局限于技术性能，而变得对于业务成功至关重要。本文通过引入一套全面的指标来解决推荐系统评估的多方面特性，每个指标专门捕捉系统性能的不同方面。我们讨论了以下几个方面的指标：相似性指标：用于量化基于内容的过滤机制的准确性，并评估协同过滤技术的准确性；候选生成指标：用于评估系统有效地识别广泛但相关的项目的能力；预测指标：用于评估预测的用户偏好的准确性；排序指标：用于评估推荐顺序的有效性；业务指标：用于对齐推荐系统的性能。

    The effectiveness of recommendation systems is pivotal to user engagement and satisfaction in online platforms. As these recommendation systems increasingly influence user choices, their evaluation transcends mere technical performance and becomes central to business success. This paper addresses the multifaceted nature of recommendations system evaluation by introducing a comprehensive suite of metrics, each tailored to capture a distinct aspect of system performance. We discuss  * Similarity Metrics: to quantify the precision of content-based filtering mechanisms and assess the accuracy of collaborative filtering techniques.  * Candidate Generation Metrics: to evaluate how effectively the system identifies a broad yet relevant range of items.  * Predictive Metrics: to assess the accuracy of forecasted user preferences.  * Ranking Metrics: to evaluate the effectiveness of the order in which recommendations are presented.  * Business Metrics: to align the performance of the recommendat
    
[^17]: 利用大型语言模型（LLMs）增强基于内容的推荐的免训练数据集压缩

    Leveraging Large Language Models (LLMs) to Empower Training-Free Dataset Condensation for Content-Based Recommendation. (arXiv:2310.09874v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2310.09874](http://arxiv.org/abs/2310.09874)

    本文利用大型语言模型（LLMs）来增强基于内容的推荐中的免训练数据集压缩方法，旨在通过生成文本内容来合成一个小而信息丰富的数据集，使得模型能够达到与在大型数据集上训练的模型相当的性能。

    

    现代内容推荐（CBR）技术利用物品的内容信息为用户提供个性化服务，但在大型数据集上的资源密集型训练存在问题。为解决这个问题，本文探讨了对文本CBR进行数据集压缩的方法。数据集压缩的目标是合成一个小且信息丰富的数据集，使模型性能可以与在大型数据集上训练的模型相媲美。现有的压缩方法针对连续数据（如图像或嵌入向量）的分类任务而设计，直接应用于CBR存在局限性。为了弥补这一差距，我们研究了基于内容的推荐中高效的数据集压缩方法。受到大型语言模型（LLMs）在文本理解和生成方面出色的能力的启发，我们利用LLMs在数据集压缩期间生成文本内容。为了处理涉及用户和物品的交互数据，我们设计了一个双...

    Modern techniques in Content-based Recommendation (CBR) leverage item content information to provide personalized services to users, but suffer from resource-intensive training on large datasets. To address this issue, we explore the dataset condensation for textual CBR in this paper. The goal of dataset condensation is to synthesize a small yet informative dataset, upon which models can achieve performance comparable to those trained on large datasets. While existing condensation approaches are tailored to classification tasks for continuous data like images or embeddings, direct application of them to CBR has limitations. To bridge this gap, we investigate efficient dataset condensation for content-based recommendation. Inspired by the remarkable abilities of large language models (LLMs) in text comprehension and generation, we leverage LLMs to empower the generation of textual content during condensation. To handle the interaction data involving both users and items, we devise a dua
    
[^18]: 从群体特征预测群体选择

    Predicting Group Choices from Group Profiles. (arXiv:2308.03083v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2308.03083](http://arxiv.org/abs/2308.03083)

    本文旨在通过使用机器学习方法和观测到的群体选择数据集，验证了相较于标准偏好聚合策略，可以更好地预测群体的最终选择。

    

    群体推荐系统通过将群体成员的个人偏好进行聚合，并选择群体配置文件中得分最高的项目来为群体推荐项目。该系统通过假设群体采用与群体推荐系统相同的偏好聚合策略来预测这些推荐项将被群体选择。然而，预测群体的选择更加复杂，因为群体推荐系统无法确切知道群体将使用的偏好聚合策略。因此，本文旨在通过使用机器学习方法和观测到的群体选择数据集，验证以下研究假设：相较于标准偏好聚合策略，可以更好地预测群体的最终选择。受决策方案理论的启发，该理论最早试图解决群体选择预测问题，我们寻找群体配置文件的定义。

    Group recommender systems (GRSs) identify items to recommend to a group of people by aggregating group members' individual preferences into a group profile, and selecting the items that have the largest score in the group profile. The GRS predicts that these recommendations would be chosen by the group, by assuming that the group is applying the same preference aggregation strategy as the one adopted by the GRS. However, predicting the choice of a group is more complex since the GRS is not aware of the exact preference aggregation strategy that is going to be used by the group.  To this end, the aim of this paper is to validate the research hypothesis that, by using a machine learning approach and a data set of observed group choices, it is possible to predict a group's final choice, better than by using a standard preference aggregation strategy. Inspired by the Decision Scheme theory, which first tried to address the group choice prediction problem, we search for a group profile defi
    

