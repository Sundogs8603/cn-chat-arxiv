# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Search and Society: Reimagining Information Access for Radical Futures](https://arxiv.org/abs/2403.17901) | 社区应该重新将研究议程聚焦于社会需求，消除公平性、问责制、透明度和道德研究与信息检索其他领域之间的人为隔离，积极设定研究议程，激励构建明确陈述的社会技术想象力所启发的系统类型。 |
| [^2] | [MIND Your Language: A Multilingual Dataset for Cross-lingual News Recommendation](https://arxiv.org/abs/2403.17876) | 提出了 xMIND，这是一个通过机器翻译从英语 MIND 数据集衍生而来的开放、多语言的新闻推荐数据集，填补了新闻推荐系统在多语言环境和资源匮乏语言方面的基准数据集缺失。 |
| [^3] | [ArabicaQA: A Comprehensive Dataset for Arabic Question Answering](https://arxiv.org/abs/2403.17848) | ArabicaQA是第一个用于阿拉伯语机器阅读理解和开放领域问答的大规模数据集，引入了AraDPR密集段落检索模型和对大型语言模型（LLMs）进行的广泛基准测试，为阿拉伯语自然语言处理资源带来了重要进展。 |
| [^4] | [CaseLink: Inductive Graph Learning for Legal Case Retrieval](https://arxiv.org/abs/2403.17780) | 该论文提出了一种基于归纳图学习的方法，通过充分利用案例间的连接关系，提高了法律案例检索性能。 |
| [^5] | [TWOLAR: a TWO-step LLM-Augmented distillation method for passage Reranking](https://arxiv.org/abs/2403.17759) | TWOLAR引入了新的评分策略和蒸馏过程，创建了多样性训练数据集，显著增强了文档重新排序能力，与三个数量级更多参数的最先进模型相匹配甚至在某些情况下超越。 |
| [^6] | [All-in-One: Heterogeneous Interaction Modeling for Cold-Start Rating Prediction](https://arxiv.org/abs/2403.17740) | 提出了异质交互评分网络（HIRE）框架，通过异质交互模块（HIM）来共同建模异质交互并直接推断重要特征 |
| [^7] | [EulerFormer: Sequential User Behavior Modeling with Complex Vector Attention](https://arxiv.org/abs/2403.17729) | EulerFormer提出了一种具有复杂向量注意力的新型转换器变体，统一了语义差异和位置差异的理论框架。 |
| [^8] | [Large Language Models Enhanced Collaborative Filtering](https://arxiv.org/abs/2403.17688) | 提出了大型语言模型增强协同过滤（LLM-CF）框架，通过LLMs的世界知识和推理能力来提供更好的协同过滤信息 |
| [^9] | [S+t-SNE - Bringing dimensionality reduction to data streams](https://arxiv.org/abs/2403.17643) | S+t-SNE是t-SNE算法的改进版本，在处理数据流时具有增量更新和盲目漂移管理的特点，能够实现高效的降维和信息可视化。 |
| [^10] | [Retentive Decision Transformer with Adaptive Masking for Reinforcement Learning based Recommendation Systems](https://arxiv.org/abs/2403.17634) | 本研究提出了一种新的离线RL推荐系统方法，通过将顺序决策建模为推理任务，利用自适应遮罩配置来重新解释RLRS挑战。 |
| [^11] | [END4Rec: Efficient Noise-Decoupling for Multi-Behavior Sequential Recommendation](https://arxiv.org/abs/2403.17603) | 该研究提出了针对多行为序贯推荐的高效去噪方法，包括开发高效行为序列挖掘器、设计硬与软去噪模块，探索行为与噪音关系，以及引入对比损失函数和引导训练策略。 |
| [^12] | [Document Set Expansion with Positive-Unlabelled Learning Using Intractable Density Estimation](https://arxiv.org/abs/2403.17473) | 本文介绍了一种利用难以估计的密度估计模型的新颖PU学习框架，有效解决了文档集扩展任务中正-未标记学习面临的类先验假设不切实际的问题。 |
| [^13] | [Touch the Core: Exploring Task Dependence Among Hybrid Targets for Recommendation](https://arxiv.org/abs/2403.17442) | 在混合目标推荐中，通过研究多任务学习问题，探讨了离散转化行为与连续转化之间的依赖关系，解决了核心回归任务对其他任务影响较大的问题。 |
| [^14] | [Masked Multi-Domain Network: Multi-Type and Multi-Scenario Conversion Rate Prediction with a Single Model](https://arxiv.org/abs/2403.17425) | 本文提出了一种Masked Multi-Domain Network，解决了使用单一模型进行多类型和多场景转化率预测时的准确性、可扩展性和便利性问题。 |
| [^15] | [MA4DIV: Multi-Agent Reinforcement Learning for Search Result Diversification](https://arxiv.org/abs/2403.17421) | 引入了基于多智能体强化学习的MA4DIV方法，将搜索结果多样化建模为多个智能体之间的合作任务，直接优化多样性指标，如$\alpha$-NDCG，以实现高训练效率。 |
| [^16] | [AFDGCF: Adaptive Feature De-correlation Graph Collaborative Filtering for Recommendations](https://arxiv.org/abs/2403.17416) | 本论文强调了过度相关性问题在降低GNN表示和推荐性能中的关键作用，并致力于解决如何减轻过度关联的影响同时保留协同过滤信号这一重大挑战。 |
| [^17] | [Multi-Domain Recommendation to Attract Users via Domain Preference Modeling](https://arxiv.org/abs/2403.17374) | 本文提出了一个名为DRIP的框架，通过在领域和项目两个层面对用户偏好进行建模，从而解决多领域推荐任务中用户偏好和领域映射的挑战。 |
| [^18] | [An Empirical Study of Training ID-Agnostic Multi-modal Sequential Recommenders](https://arxiv.org/abs/2403.17372) | 通过研究现有的多模态相关的顺序推荐方法，提炼出视觉编码器、文本编码器、多模态融合模块和顺序架构这四个核心组件。 |
| [^19] | [Cognitively Biased Users Interacting with Algorithmically Biased Results in Whole-Session Search on Controversial Topics](https://arxiv.org/abs/2403.17286) | 用户在与算法偏误的搜索结果互动时，受认知偏误影响，特别是在有争议话题上的整个搜索过程中，他们更倾向于选择符合其现有信念的结果，并且确认偏误和结果呈现对搜索行为和结果熟悉度有显著影响。 |
| [^20] | [EXPLORA: A teacher-apprentice methodology for eliciting natural child-computer interactions](https://arxiv.org/abs/2403.17264) | EXPLORA是一个教师-学徒方法，通过预先观察访谈收集数据，帮助研究人员更深入地理解儿童特征和背景，从而设计出更符合儿童需求的技术。 |
| [^21] | [CADGL: Context-Aware Deep Graph Learning for Predicting Drug-Drug Interactions](https://arxiv.org/abs/2403.17210) | 通过CADGL框架，利用上下文感知深度图学习来预测药物-药物相互作用，解决了现有DDI预测模型在泛化、特征提取和现实应用方面的挑战 |
| [^22] | [Generation of Asset Administration Shell with Large Language Model Agents: Interoperability in Digital Twins with Semantic Node](https://arxiv.org/abs/2403.17209) | 通过大型语言模型代理生成AAS实例模型，实现了在数字孪生中的互操作性，降低了手动创建成本和时间。 |
| [^23] | [GOLF: Goal-Oriented Long-term liFe tasks supported by human-AI collaboration](https://arxiv.org/abs/2403.17089) | 该研究提出了GOLF框架，通过目标导向和长期规划增强LLMs的能力，以协助用户处理重要的生活决策。 |
| [^24] | [Coarse-Tuning for Ad-hoc Document Retrieval Using Pre-trained Language Models](https://arxiv.org/abs/2403.16915) | 本研究引入了粗调优作为一个中间学习阶段，连接了预训练和微调，在专题文档检索中显著改善了效果。 |
| [^25] | [Take Care of Your Prompt Bias! Investigating and Mitigating Prompt Bias in Factual Knowledge Extraction](https://arxiv.org/abs/2403.09963) | 本文调查了预训练语言模型在事实知识提取中存在的“提示偏见”，找到了不同类型提示的偏见程度，以及这种偏见对不同基准测试的影响，并提出了一种基于表示的方法来减轻这种提示偏见。 |
| [^26] | [Evaluating Large Language Models as Generative User Simulators for Conversational Recommendation](https://arxiv.org/abs/2403.09738) | 大型语言模型作为生成式用户模拟器在对话推荐中展现出潜力，新的协议通过五个任务评估了语言模型模拟人类行为的准确程度，揭示了模型与人类行为的偏差，并提出了如何通过模型选择和提示策略减少这些偏差。 |
| [^27] | [Graph Signal Diffusion Model for Collaborative Filtering](https://arxiv.org/abs/2311.08744) | 提出了一种用于协同过滤的图信号扩散模型，解决了现有扩散模型在建模隐式反馈数据方面的不足，通过对扩散模型进行创新改进，解决了标准扩散过程导致的个性化信息丢失和图形结构不一致等问题。 |
| [^28] | [Optimizing Feature Set for Click-Through Rate Prediction](https://arxiv.org/abs/2301.10909) | 提出了一种名为OptFS的新方法，用于优化点击率预测模型中特征集的选择。 |
| [^29] | [Understanding Performance of Long-Document Ranking Models through Comprehensive Evaluation and Leaderboarding](https://arxiv.org/abs/2207.01262) | 在标准收集的初步实验中，我们发现长文档模型在MRR或NDCG方面性能不佳，表现低于FirstP，或平均最多超越5％。我们推测这不是因为模型无法处理长上下文，而是由于相关段落具有位置偏见，往往位于前512个文档标记之中。我们找到证据表明这种偏见至少存在于两个测试集中，这促使我们创建了一个新的收集MS MARCO FarRelevant，其中包含 |
| [^30] | [Improving Sequential Recommendations via Bidirectional Temporal Data Augmentation with Pre-training](https://arxiv.org/abs/2112.06460) | 提出了一种名为BARec的新方法，通过双向时间增强和知识增强微调来改进顺序推荐，合成真实的伪先前项目，保留用户偏好并捕捉更深层次的项目语义相关性 |
| [^31] | [Aligning the Capabilities of Large Language Models with the Context of Information Retrieval via Contrastive Feedback.](http://arxiv.org/abs/2309.17078) | 通过对比反馈强化学习框架，有效提升大型语言模型在信息检索中的应用，使其能够生成更具特定性和上下文适应性的回复。 |
| [^32] | [A Decade of Scholarly Research on Open Knowledge Graphs.](http://arxiv.org/abs/2306.13186) | 本文分析了过去十年开放知识图谱的学术研究趋势和主题，并确定了知识图谱构建和增强、评估和复用以及将知识图谱融入NLP系统的三个主要研究主题。 |

# 详细

[^1]: 搜索与社会：重新构想激进未来的信息获取

    Search and Society: Reimagining Information Access for Radical Futures

    [https://arxiv.org/abs/2403.17901](https://arxiv.org/abs/2403.17901)

    社区应该重新将研究议程聚焦于社会需求，消除公平性、问责制、透明度和道德研究与信息检索其他领域之间的人为隔离，积极设定研究议程，激励构建明确陈述的社会技术想象力所启发的系统类型。

    

    arXiv: 2403.17901v1 公告类型：新摘要：信息检索（IR）技术和研究正在经历变革。我们认为社区应该抓住这个机会，重新将研究议程聚焦于社会需求，同时消除IR的公平性、问责制、透明度和道德研究与IR其他领域之间的人为隔离。社区不应采取试图减轻新兴技术可能带来社会害处的反应性策略，而应该积极设定研究议程，激励我们构建各种明确陈述的社会技术想象力所启发的系统类型。支撑信息获取技术设计和开发的社会技术想象力需要明确表达，我们需要在这些不同视角的背景下发展变革理论。我们的指导未来想象力必须受到其他学术领域的启发。

    arXiv:2403.17901v1 Announce Type: new  Abstract: Information retrieval (IR) technologies and research are undergoing transformative changes. It is our perspective that the community should accept this opportunity to re-center our research agendas on societal needs while dismantling the artificial separation between the work on fairness, accountability, transparency, and ethics in IR and the rest of IR research. Instead of adopting a reactionary strategy of trying to mitigate potential social harms from emerging technologies, the community should aim to proactively set the research agenda for the kinds of systems we should build inspired by diverse explicitly stated sociotechnical imaginaries. The sociotechnical imaginaries that underpin the design and development of information access technologies needs to be explicitly articulated, and we need to develop theories of change in context of these diverse perspectives. Our guiding future imaginaries must be informed by other academic field
    
[^2]: MIND 你的语言：用于跨语言新闻推荐的多语言数据集

    MIND Your Language: A Multilingual Dataset for Cross-lingual News Recommendation

    [https://arxiv.org/abs/2403.17876](https://arxiv.org/abs/2403.17876)

    提出了 xMIND，这是一个通过机器翻译从英语 MIND 数据集衍生而来的开放、多语言的新闻推荐数据集，填补了新闻推荐系统在多语言环境和资源匮乏语言方面的基准数据集缺失。

    

    arXiv:2403.17876v1 公告类型：新摘要：数字新闻平台使用新闻推荐作为满足读者个人信息需求的主要工具。尽管网络社区语言日益多样，许多互联网用户使用多种语言阅读新闻，但多数新闻推荐仍集中在主要的、资源丰富的语言上，尤其是英语。此外，几乎所有新闻推荐工作都假设单语言新闻消费，然而越来越多的用户倾向于以至少两种语言来获取信息。因此，现有的新闻推荐研究缺乏公开的多语言基准数据集，这些数据集将促进在多语言环境和对于资源匮乏语言中有效开发新闻推荐系统。为了填补这一空白，我们介绍了 xMIND，这是一个开放的、多语言的新闻推荐数据集，通过机器翻译从英语 MIND 数据集衍生而来，覆盖了一组...

    arXiv:2403.17876v1 Announce Type: new  Abstract: Digital news platforms use news recommenders as the main instrument to cater to the individual information needs of readers. Despite an increasingly language-diverse online community, in which many Internet users consume news in multiple languages, the majority of news recommendation focuses on major, resource-rich languages, and English in particular. Moreover, nearly all news recommendation efforts assume monolingual news consumption, whereas more and more users tend to consume information in at least two languages. Accordingly, the existing body of work on news recommendation suffers from a lack of publicly available multilingual benchmarks that would catalyze development of news recommenders effective in multilingual settings and for low-resource languages. Aiming to fill this gap, we introduce xMIND, an open, multilingual news recommendation dataset derived from the English MIND dataset using machine translation, covering a set of 1
    
[^3]: ArabicaQA：一个用于阿拉伯语问答的综合数据集

    ArabicaQA: A Comprehensive Dataset for Arabic Question Answering

    [https://arxiv.org/abs/2403.17848](https://arxiv.org/abs/2403.17848)

    ArabicaQA是第一个用于阿拉伯语机器阅读理解和开放领域问答的大规模数据集，引入了AraDPR密集段落检索模型和对大型语言模型（LLMs）进行的广泛基准测试，为阿拉伯语自然语言处理资源带来了重要进展。

    

    在本文中，我们通过引入ArabicaQA解决了阿拉伯语自然语言处理资源中的巨大缺口，这是第一个用于阿拉伯语机器阅读理解和开放领域问答的大规模数据集。这个综合数据集由众包工作者创建，包括89,095个可回答的问题和3,701个不可回答的问题，看起来与可回答的问题类似，还附带了额外的开放领域问题标签，标志着阿拉伯语自然语言处理资源的重要进步。我们还介绍了AraDPR，这是第一个在阿拉伯维基百科语料库上训练的密集段落检索模型，专门设计用于解决阿拉伯文本检索的独特挑战。此外，我们的研究还包括对大型语言模型（LLMs）在阿拉伯语问答中的广泛基准测试，批判性地评估它们在阿拉伯语境中的性能。总之，ArabicaQA、AraDPR以及LLMs在阿拉伯语问答中的基准测试等工作将促进阿拉伯语自然语言处理领域的发展。

    arXiv:2403.17848v1 Announce Type: new  Abstract: In this paper, we address the significant gap in Arabic natural language processing (NLP) resources by introducing ArabicaQA, the first large-scale dataset for machine reading comprehension and open-domain question answering in Arabic. This comprehensive dataset, consisting of 89,095 answerable and 3,701 unanswerable questions created by crowdworkers to look similar to answerable ones, along with additional labels of open-domain questions marks a crucial advancement in Arabic NLP resources. We also present AraDPR, the first dense passage retrieval model trained on the Arabic Wikipedia corpus, specifically designed to tackle the unique challenges of Arabic text retrieval. Furthermore, our study includes extensive benchmarking of large language models (LLMs) for Arabic question answering, critically evaluating their performance in the Arabic language context. In conclusion, ArabicaQA, AraDPR, and the benchmarking of LLMs in Arabic question
    
[^4]: CaseLink:法律案例检索的归纳图学习

    CaseLink: Inductive Graph Learning for Legal Case Retrieval

    [https://arxiv.org/abs/2403.17780](https://arxiv.org/abs/2403.17780)

    该论文提出了一种基于归纳图学习的方法，通过充分利用案例间的连接关系，提高了法律案例检索性能。

    

    在案例法中，先例是用来支持法官做出决定以及律师对特定案例的观点的相关案例。为了从大量案例池中高效地找到相关案例，法律从业者广泛使用检索工具。现有的法律案例检索模型主要通过比较单个案例的文本表示来工作。尽管它们获得了不错的检索准确性，但案例之间的固有连接关系未被充分利用于案例编码，从而限制了进一步提高检索性能。在案例池中，有三种案例连接关系：案例引用关系、案例语义关系和案例法律指控关系。由于法律案例检索任务的归纳方式的特点，使用案例引用作为输入

    arXiv:2403.17780v1 Announce Type: new  Abstract: In case law, the precedents are the relevant cases that are used to support the decisions made by the judges and the opinions of lawyers towards a given case. This relevance is referred to as the case-to-case reference relation. To efficiently find relevant cases from a large case pool, retrieval tools are widely used by legal practitioners. Existing legal case retrieval models mainly work by comparing the text representations of individual cases. Although they obtain a decent retrieval accuracy, the intrinsic case connectivity relationships among cases have not been well exploited for case encoding, therefore limiting the further improvement of retrieval performance. In a case pool, there are three types of case connectivity relationships: the case reference relationship, the case semantic relationship, and the case legal charge relationship. Due to the inductive manner in the task of legal case retrieval, using case reference as input 
    
[^5]: TWOLAR：一种基于大型语言模型增强蒸馏方法的段落重新排序的两步骤方法

    TWOLAR: a TWO-step LLM-Augmented distillation method for passage Reranking

    [https://arxiv.org/abs/2403.17759](https://arxiv.org/abs/2403.17759)

    TWOLAR引入了新的评分策略和蒸馏过程，创建了多样性训练数据集，显著增强了文档重新排序能力，与三个数量级更多参数的最先进模型相匹配甚至在某些情况下超越。

    

    在本文中，我们提出了TWOLAR：一种基于大型语言模型（LLM）知识蒸馏的段落重新排序的两阶段流水线。TWOLAR引入了一种新的评分策略和一个蒸馏过程，包括创建一个新颖且多样的训练数据集。该数据集包含20K个查询，每个查询与通过四种不同检索方法检索到的一组文档相关联，以确保多样性，然后通过利用LLM的零-shot重新排序能力进行重新排序。我们的消融研究证明了我们引入的每个新组件的贡献。我们的实验结果表明，TWOLAR显著增强了基础模型的文档重新排序能力，在TREC-DL测试集和零-shot评估基准BEIR上与拥有三个数量级更多参数的最先进模型相匹配甚至在某些情况下超越。为了促进未来工作，我们发布了我们的数据。

    arXiv:2403.17759v1 Announce Type: new  Abstract: In this paper, we present TWOLAR: a two-stage pipeline for passage reranking based on the distillation of knowledge from Large Language Models (LLM). TWOLAR introduces a new scoring strategy and a distillation process consisting in the creation of a novel and diverse training dataset. The dataset consists of 20K queries, each associated with a set of documents retrieved via four distinct retrieval methods to ensure diversity, and then reranked by exploiting the zero-shot reranking capabilities of an LLM. Our ablation studies demonstrate the contribution of each new component we introduced. Our experimental results show that TWOLAR significantly enhances the document reranking ability of the underlying model, matching and in some cases even outperforming state-of-the-art models with three orders of magnitude more parameters on the TREC-DL test sets and the zero-shot evaluation benchmark BEIR. To facilitate future work we release our data 
    
[^6]: 一体化：异质交互建模用于冷启动评分预测

    All-in-One: Heterogeneous Interaction Modeling for Cold-Start Rating Prediction

    [https://arxiv.org/abs/2403.17740](https://arxiv.org/abs/2403.17740)

    提出了异质交互评分网络（HIRE）框架，通过异质交互模块（HIM）来共同建模异质交互并直接推断重要特征

    

    冷启动评分预测是推荐系统中一个基本问题，已得到广泛研究。许多方法已经被提出，利用现有数据之间的显式关系，例如协同过滤、社交推荐和异构信息网络，以缓解冷启动用户和物品的数据不足问题。然而，基于不同角色之间的数据构建的显式关系可能不可靠且无关，从而限制了特定推荐任务的性能上限。受此启发，本文提出了一个灵活的框架，名为异质交互评分网络（HIRE）。HIRE不仅仅依赖于预先定义的交互模式或手动构建的异构信息网络。相反，我们设计了一个异质交互模块（HIM），来共同建模异质交互并直接推断重要特征。

    arXiv:2403.17740v1 Announce Type: cross  Abstract: Cold-start rating prediction is a fundamental problem in recommender systems that has been extensively studied. Many methods have been proposed that exploit explicit relations among existing data, such as collaborative filtering, social recommendations and heterogeneous information network, to alleviate the data insufficiency issue for cold-start users and items. However, the explicit relations constructed based on data between different roles may be unreliable and irrelevant, which limits the performance ceiling of the specific recommendation task. Motivated by this, in this paper, we propose a flexible framework dubbed heterogeneous interaction rating network (HIRE). HIRE dose not solely rely on the pre-defined interaction pattern or the manually constructed heterogeneous information network. Instead, we devise a Heterogeneous Interaction Module (HIM) to jointly model the heterogeneous interactions and directly infer the important in
    
[^7]: EulerFormer：具有复杂向量注意力的顺序用户行为建模

    EulerFormer: Sequential User Behavior Modeling with Complex Vector Attention

    [https://arxiv.org/abs/2403.17729](https://arxiv.org/abs/2403.17729)

    EulerFormer提出了一种具有复杂向量注意力的新型转换器变体，统一了语义差异和位置差异的理论框架。

    

    为了捕捉用户偏好，转换器模型被广泛应用于建模顺序用户行为数据。转换器架构的核心在于自注意力机制，它计算序列中的成对注意力分数。由于排列等变性的特性，位置编码用于增强令牌表示之间的注意力。在这种设定下，成对注意力分数可以通过语义差异和位置差异两者衍生出来。然而，先前的研究经常以不同方式建模两种不同类型的差异测量，这可能限制了序列建模的表达能力。为了解决这个问题，本文提出了一种名为EulerFormer的具有复杂向量注意力的新型转换器变体，提供了一个统一的理论框架来表述语义差异和位置差异。 EulerFormer包含两个关键技术改进。

    arXiv:2403.17729v1 Announce Type: cross  Abstract: To capture user preference, transformer models have been widely applied to model sequential user behavior data. The core of transformer architecture lies in the self-attention mechanism, which computes the pairwise attention scores in a sequence. Due to the permutation-equivariant nature, positional encoding is used to enhance the attention between token representations. In this setting, the pairwise attention scores can be derived by both semantic difference and positional difference. However, prior studies often model the two kinds of difference measurements in different ways, which potentially limits the expressive capacity of sequence modeling. To address this issue, this paper proposes a novel transformer variant with complex vector attention, named EulerFormer, which provides a unified theoretical framework to formulate both semantic difference and positional difference. The EulerFormer involves two key technical improvements. Fi
    
[^8]: 大型语言模型增强协同过滤

    Large Language Models Enhanced Collaborative Filtering

    [https://arxiv.org/abs/2403.17688](https://arxiv.org/abs/2403.17688)

    提出了大型语言模型增强协同过滤（LLM-CF）框架，通过LLMs的世界知识和推理能力来提供更好的协同过滤信息

    

    最近，大型语言模型（LLM）的最新进展吸引了研究人员的广泛兴趣，他们利用这些模型来增强推荐系统（RS）。现有工作主要利用LLM生成知识丰富的文本，或者利用LLM衍生的嵌入作为特征来改进RS。虽然LLM中包含的广泛世界知识通常有利于RS，但该应用只能接受有限数量的用户和项目作为输入，没有充分利用协同过滤信息。考虑到协同过滤在RS中的关键作用，利用LLM增强RS的一个关键挑战在于通过LLM提供更好的协同过滤信息。在本文中，我们从LLM中的上下文学习和思维链推理中汲取灵感，提出了大型语言模型增强协同过滤（LLM-CF）框架，该框架提炼了LLMs的世界知识和推理能力

    arXiv:2403.17688v1 Announce Type: new  Abstract: Recent advancements in Large Language Models (LLMs) have attracted considerable interest among researchers to leverage these models to enhance Recommender Systems (RSs). Existing work predominantly utilizes LLMs to generate knowledge-rich texts or utilizes LLM-derived embeddings as features to improve RSs. Al- though the extensive world knowledge embedded in LLMs generally benefits RSs, the application can only take limited number of users and items as inputs, without adequately exploiting collaborative filtering information. Considering its crucial role in RSs, one key challenge in enhancing RSs with LLMs lies in providing better collaborative filtering information through LLMs. In this paper, drawing inspiration from the in-context learning and chain of thought reasoning in LLMs, we propose the Large Language Models enhanced Collaborative Filtering (LLM-CF) framework, which distils the world knowledge and reasoning capabilities of LLMs
    
[^9]: S+t-SNE - 将降维引入数据流

    S+t-SNE - Bringing dimensionality reduction to data streams

    [https://arxiv.org/abs/2403.17643](https://arxiv.org/abs/2403.17643)

    S+t-SNE是t-SNE算法的改进版本，在处理数据流时具有增量更新和盲目漂移管理的特点，能够实现高效的降维和信息可视化。

    

    我们提出了S+t-SNE，这是t-SNE算法的一种改进，旨在处理无限数据流。S+t-SNE的核心思想是随着新数据的到来逐步更新t-SNE嵌入，确保可扩展性和适应性，以处理流式场景。通过在每一步选择最重要的点，该算法确保可扩展性同时保持信息可视化。采用盲目方法进行漂移管理调整嵌入空间，促进不断可视化不断发展的数据动态。我们的实验评估证明了S+t-SNE的有效性和效率。结果突显了其在流式场景中捕捉模式的能力。我们希望我们的方法为研究人员和从业者提供一个实时工具，用于理解和解释高维数据。

    arXiv:2403.17643v1 Announce Type: new  Abstract: We present S+t-SNE, an adaptation of the t-SNE algorithm designed to handle infinite data streams. The core idea behind S+t-SNE is to update the t-SNE embedding incrementally as new data arrives, ensuring scalability and adaptability to handle streaming scenarios. By selecting the most important points at each step, the algorithm ensures scalability while keeping informative visualisations. Employing a blind method for drift management adjusts the embedding space, facilitating continuous visualisation of evolving data dynamics. Our experimental evaluations demonstrate the effectiveness and efficiency of S+t-SNE. The results highlight its ability to capture patterns in a streaming scenario. We hope our approach offers researchers and practitioners a real-time tool for understanding and interpreting high-dimensional data.
    
[^10]: 具有自适应遮罩的保留决策变压器用于基于强化学习的推荐系统

    Retentive Decision Transformer with Adaptive Masking for Reinforcement Learning based Recommendation Systems

    [https://arxiv.org/abs/2403.17634](https://arxiv.org/abs/2403.17634)

    本研究提出了一种新的离线RL推荐系统方法，通过将顺序决策建模为推理任务，利用自适应遮罩配置来重新解释RLRS挑战。

    

    强化学习推荐系统（RLRS）在一系列应用中显示出潜力，从电子商务平台到流媒体服务。然而，它们在制定奖励函数和利用RL框架中的大型现有数据集方面面临挑战。最近的离线RLRS的技术进步为解决这两个挑战提供了解决方案。然而，现有方法主要依赖于转换器架构，在序列长度增加时可能会引入与计算资源和训练成本相关的挑战。此外，主流方法使用固定长度的输入轨迹，限制了它们捕获不断变化的用户喜好的能力。在本研究中，我们介绍了一种新的离线RLRS方法来解决以上问题。我们通过将顺序决策建模为推理任务，利用自适应遮罩配置来重新解释RLRS挑战。

    arXiv:2403.17634v1 Announce Type: cross  Abstract: Reinforcement Learning-based Recommender Systems (RLRS) have shown promise across a spectrum of applications, from e-commerce platforms to streaming services. Yet, they grapple with challenges, notably in crafting reward functions and harnessing large pre-existing datasets within the RL framework. Recent advancements in offline RLRS provide a solution for how to address these two challenges. However, existing methods mainly rely on the transformer architecture, which, as sequence lengths increase, can introduce challenges associated with computational resources and training costs. Additionally, the prevalent methods employ fixed-length input trajectories, restricting their capacity to capture evolving user preferences. In this study, we introduce a new offline RLRS method to deal with the above problems. We reinterpret the RLRS challenge by modeling sequential decision-making as an inference task, leveraging adaptive masking configurat
    
[^11]: END4Rec：面向多行为序贯推荐的高效去噪方法

    END4Rec: Efficient Noise-Decoupling for Multi-Behavior Sequential Recommendation

    [https://arxiv.org/abs/2403.17603](https://arxiv.org/abs/2403.17603)

    该研究提出了针对多行为序贯推荐的高效去噪方法，包括开发高效行为序列挖掘器、设计硬与软去噪模块，探索行为与噪音关系，以及引入对比损失函数和引导训练策略。

    

    在推荐系统中，用户经常参与多种类型的行为，比如点击、加入购物车和购买。然而，由于具有多样化行为数据，用户行为序列在短期内会变得非常长，这给序列推荐模型的效率带来了挑战。与此同时，一些行为数据也会给用户兴趣建模带来不可避免的噪音。为了解决上述问题，首先，我们开发了高效行为序列挖掘器（EBM），能够高效捕捉用户行为中的复杂模式，同时保持低时间复杂度和参数数量。其次，我们为不同类型的噪音设计了硬性和软性去噪模块，并充分探索行为和噪音之间的关系。最后，我们引入了对比损失函数以及引导训练策略，以比较数据中的有效信息与噪音信号，从而无缝地

    arXiv:2403.17603v1 Announce Type: new  Abstract: In recommendation systems, users frequently engage in multiple types of behaviors, such as clicking, adding to a cart, and purchasing. However, with diversified behavior data, user behavior sequences will become very long in the short term, which brings challenges to the efficiency of the sequence recommendation model. Meanwhile, some behavior data will also bring inevitable noise to the modeling of user interests. To address the aforementioned issues, firstly, we develop the Efficient Behavior Sequence Miner (EBM) that efficiently captures intricate patterns in user behavior while maintaining low time complexity and parameter count. Secondly, we design hard and soft denoising modules for different noise types and fully explore the relationship between behaviors and noise. Finally, we introduce a contrastive loss function along with a guided training strategy to compare the valid information in the data with the noisy signal, and seamles
    
[^12]: 使用难以估计的密度估计进行正-未标记学习进行文档集扩展

    Document Set Expansion with Positive-Unlabelled Learning Using Intractable Density Estimation

    [https://arxiv.org/abs/2403.17473](https://arxiv.org/abs/2403.17473)

    本文介绍了一种利用难以估计的密度估计模型的新颖PU学习框架，有效解决了文档集扩展任务中正-未标记学习面临的类先验假设不切实际的问题。

    

    文档集扩展（DSE）任务涉及根据一组有限的示例文档从大型文档集合中识别相关文档。先前的研究强调了正向和未标记（PU）学习作为这一任务的一种有希望的方法。然而，大多数PU方法依赖于在集合中正样本的类先验的不切实际的假设。为了解决这一限制，本文介绍了一个利用难以估计的密度估计模型的新颖PU学习框架。在PubMed和Covid数据集上进行的转导设置实验展示了所提出方法在DSE中的有效性。代码可从 https://github.com/Beautifuldog01/Document-set-expansion-puDE 获取。

    arXiv:2403.17473v1 Announce Type: new  Abstract: The Document Set Expansion (DSE) task involves identifying relevant documents from large collections based on a limited set of example documents. Previous research has highlighted Positive and Unlabeled (PU) learning as a promising approach for this task. However, most PU methods rely on the unrealistic assumption of knowing the class prior for positive samples in the collection. To address this limitation, this paper introduces a novel PU learning framework that utilizes intractable density estimation models. Experiments conducted on PubMed and Covid datasets in a transductive setting showcase the effectiveness of the proposed method for DSE. Code is available from https://github.com/Beautifuldog01/Document-set-expansion-puDE.
    
[^13]: 触及核心：探索混合目标推荐中任务依赖关系

    Touch the Core: Exploring Task Dependence Among Hybrid Targets for Recommendation

    [https://arxiv.org/abs/2403.17442](https://arxiv.org/abs/2403.17442)

    在混合目标推荐中，通过研究多任务学习问题，探讨了离散转化行为与连续转化之间的依赖关系，解决了核心回归任务对其他任务影响较大的问题。

    

    随着用户行为在商业平台上变得复杂，在线推荐更加关注如何触及核心转化，这些转化与平台的兴趣密切相关。这些核心转化通常是连续的目标，如“观看时间”、“收入”等，它们的预测可以通过之前的离散转化行为来增强。因此，多任务学习（MTL）可以被采用作为学习这些混合目标的范 paradigm。然而，现有的工作主要强调研究离散转化行为之间的顺序依赖关系，而忽视了离散转化与最终连续转化之间的依赖复杂性。此外，同时优化具有更强任务依赖性的混合任务将面临不稳定的问题，其中核心回归任务可能对其他任务产生更大的影响。在本文中，我们研究了具有混合目标的MTL问题。

    arXiv:2403.17442v1 Announce Type: new  Abstract: As user behaviors become complicated on business platforms, online recommendations focus more on how to touch the core conversions, which are highly related to the interests of platforms. These core conversions are usually continuous targets, such as \textit{watch time}, \textit{revenue}, and so on, whose predictions can be enhanced by previous discrete conversion actions. Therefore, multi-task learning (MTL) can be adopted as the paradigm to learn these hybrid targets. However, existing works mainly emphasize investigating the sequential dependence among discrete conversion actions, which neglects the complexity of dependence between discrete conversions and the final continuous conversion. Moreover, simultaneously optimizing hybrid tasks with stronger task dependence will suffer from volatile issues where the core regression task might have a larger influence on other tasks. In this paper, we study the MTL problem with hybrid targets f
    
[^14]: Masked Multi-Domain Network: 一种使用单一模型进行多类型和多场景转化率预测的多域网络

    Masked Multi-Domain Network: Multi-Type and Multi-Scenario Conversion Rate Prediction with a Single Model

    [https://arxiv.org/abs/2403.17425](https://arxiv.org/abs/2403.17425)

    本文提出了一种Masked Multi-Domain Network，解决了使用单一模型进行多类型和多场景转化率预测时的准确性、可扩展性和便利性问题。

    

    在现实世界的广告系统中，转化具有不同的类型，广告可以在不同的展示场景中展示，这两者都极大地影响实际的转化率（CVR）。这导致了多类型和多场景CVR预测问题。解决这一问题的理想模型应满足以下要求：1）准确性：模型应针对任何转化类型在任何展示场景上实现精细的准确性。2）可扩展性：模型参数大小应该是可承受的。3）便利性：模型不应需要大量的数据分区、子集处理和单独存储。现有方法不能同时满足这些要求。例如，为每个（转化类型，展示场景）对构建单独的模型既不具备可扩展性也不便于操作。构建一个统一模型训练所有数据，包括转化类型和展示场景信息。

    arXiv:2403.17425v1 Announce Type: cross  Abstract: In real-world advertising systems, conversions have different types in nature and ads can be shown in different display scenarios, both of which highly impact the actual conversion rate (CVR). This results in the multi-type and multi-scenario CVR prediction problem. A desired model for this problem should satisfy the following requirements: 1) Accuracy: the model should achieve fine-grained accuracy with respect to any conversion type in any display scenario. 2) Scalability: the model parameter size should be affordable. 3) Convenience: the model should not require a large amount of effort in data partitioning, subset processing and separate storage. Existing approaches cannot simultaneously satisfy these requirements. For example, building a separate model for each (conversion type, display scenario) pair is neither scalable nor convenient. Building a unified model trained on all the data with conversion type and display scenario incl
    
[^15]: MA4DIV：用于搜索结果多样化的多智能体强化学习

    MA4DIV: Multi-Agent Reinforcement Learning for Search Result Diversification

    [https://arxiv.org/abs/2403.17421](https://arxiv.org/abs/2403.17421)

    引入了基于多智能体强化学习的MA4DIV方法，将搜索结果多样化建模为多个智能体之间的合作任务，直接优化多样性指标，如$\alpha$-NDCG，以实现高训练效率。

    

    搜索结果多样化（SRD）的目标是确保所选文档涵盖尽可能多的不同子主题。现有方法主要利用“贪婪选择”范式，即一次选择一个具有最高多样性分数的文档。这些方法往往效率低下，容易陷入次优状态。此外，一些其他方法旨在近似优化多样性指标，如$\alpha$-NDCG，但结果仍然不尽如人意。为了解决这些挑战，我们引入了用于搜索结果多样性的多智能体强化学习（MARL）方法，称为MA4DIV。在这种方法中，每个文档都是一个智能体，搜索结果多样化被建模为多个智能体之间的合作任务。该方法允许直接优化多样性指标，如$\alpha$-NDCG，同时实现高训练效率。我们进行了初步实验。

    arXiv:2403.17421v1 Announce Type: cross  Abstract: The objective of search result diversification (SRD) is to ensure that selected documents cover as many different subtopics as possible. Existing methods primarily utilize a paradigm of "greedy selection", i.e., selecting one document with the highest diversity score at a time. These approaches tend to be inefficient and are easily trapped in a suboptimal state. In addition, some other methods aim to approximately optimize the diversity metric, such as $\alpha$-NDCG, but the results still remain suboptimal. To address these challenges, we introduce Multi-Agent reinforcement learning (MARL) for search result DIVersity, which called MA4DIV. In this approach, each document is an agent and the search result diversification is modeled as a cooperative task among multiple agents. This approach allows for directly optimizing the diversity metrics, such as $\alpha$-NDCG, while achieving high training efficiency. We conducted preliminary experi
    
[^16]: AFDGCF：自适应特征去相关图协同过滤推荐

    AFDGCF: Adaptive Feature De-correlation Graph Collaborative Filtering for Recommendations

    [https://arxiv.org/abs/2403.17416](https://arxiv.org/abs/2403.17416)

    本论文强调了过度相关性问题在降低GNN表示和推荐性能中的关键作用，并致力于解决如何减轻过度关联的影响同时保留协同过滤信号这一重大挑战。

    

    基于图神经网络（GNNs）的协同过滤方法在推荐系统（RS）中取得了显著的成功，利用了它们通过消息传递机制捕获复杂用户-物品关系中的协同信号的能力。然而，这些基于GNN的RS不经意地引入了用户和物品嵌入之间过多的线性相关性，违反了提供个性化推荐的目标。尽管现有研究主要将这一缺陷归因于过度平滑的问题，但本文强调了过度相关性问题在降低GNN表示和随后推荐性能的有效性中发挥着关键且常常被忽视的作用。到目前为止，在RS中对过度相关性问题尚未进行探讨。同时，如何减轻过度关联的影响同时保留协同过滤信号是一个重大挑战。为此，本文的目标是

    arXiv:2403.17416v1 Announce Type: new  Abstract: Collaborative filtering methods based on graph neural networks (GNNs) have witnessed significant success in recommender systems (RS), capitalizing on their ability to capture collaborative signals within intricate user-item relationships via message-passing mechanisms. However, these GNN-based RS inadvertently introduce excess linear correlation between user and item embeddings, contradicting the goal of providing personalized recommendations. While existing research predominantly ascribes this flaw to the over-smoothing problem, this paper underscores the critical, often overlooked role of the over-correlation issue in diminishing the effectiveness of GNN representations and subsequent recommendation performance. Up to now, the over-correlation issue remains unexplored in RS. Meanwhile, how to mitigate the impact of over-correlation while preserving collaborative filtering signals is a significant challenge. To this end, this paper aims
    
[^17]: 多领域推荐通过领域偏好建模吸引用户

    Multi-Domain Recommendation to Attract Users via Domain Preference Modeling

    [https://arxiv.org/abs/2403.17374](https://arxiv.org/abs/2403.17374)

    本文提出了一个名为DRIP的框架，通过在领域和项目两个层面对用户偏好进行建模，从而解决多领域推荐任务中用户偏好和领域映射的挑战。

    

    最近，网络平台同时运营各种服务领域。 针对同时运营多个服务领域的平台，我们引入新任务，即通过使用用户在“已见”领域的知识，从多个“未见”领域推荐项目，以吸引用户的多领域推荐（MDRAU）。 本文指出了MDRAU任务的两个挑战。 首先，由于用户通常与不同子集的服务领域互动，因此可能有大量可能的从已见到未见领域的映射组合。 其次，用户可能对每个目标未见领域有不同的偏好，这要求推荐既反映用户对领域的偏好，也反映对项目的偏好。 为了解决这些挑战，我们提出了DRIP框架，该框架在两个级别（即领域和项目）对用户偏好进行建模，并学习各种已见-未见领域映射。

    arXiv:2403.17374v1 Announce Type: new  Abstract: Recently, web platforms have been operating various service domains simultaneously. Targeting a platform that operates multiple service domains, we introduce a new task, Multi-Domain Recommendation to Attract Users (MDRAU), which recommends items from multiple ``unseen'' domains with which each user has not interacted yet, by using knowledge from the user's ``seen'' domains. In this paper, we point out two challenges of MDRAU task. First, there are numerous possible combinations of mappings from seen to unseen domains because users have usually interacted with a different subset of service domains. Second, a user might have different preferences for each of the target unseen domains, which requires that recommendations reflect the user's preferences on domains as well as items. To tackle these challenges, we propose DRIP framework that models users' preferences at two levels (i.e., domain and item) and learns various seen-unseen domain m
    
[^18]: 训练独立于ID的多模态顺序推荐器的实证研究

    An Empirical Study of Training ID-Agnostic Multi-modal Sequential Recommenders

    [https://arxiv.org/abs/2403.17372](https://arxiv.org/abs/2403.17372)

    通过研究现有的多模态相关的顺序推荐方法，提炼出视觉编码器、文本编码器、多模态融合模块和顺序架构这四个核心组件。

    

    顺序推荐旨在基于历史交互来预测未来用户-物品交互。许多顺序推荐方法集中在用户ID和物品ID上，人类通过多模态信号（如文本和图像）感知世界的方式启发了研究人员探索如何构建不使用ID的多模态信息的顺序推荐。然而，多模态学习的复杂性体现在不同的特征提取器、融合方法和预训练模型中。因此，设计一个简单且通用的多模态顺序推荐（MMSR）框架仍然是一个巨大挑战。我们系统总结了现有的多模态相关的顺序推荐方法，并将精华提炼成四个核心组件：视觉编码器、文本编码器、多模态融合模块和顺序架构。沿着这些维度，我们剖析了模型设计，并回答了以下问题

    arXiv:2403.17372v1 Announce Type: new  Abstract: Sequential Recommendation (SR) aims to predict future user-item interactions based on historical interactions. While many SR approaches concentrate on user IDs and item IDs, the human perception of the world through multi-modal signals, like text and images, has inspired researchers to delve into constructing SR from multi-modal information without using IDs. However, the complexity of multi-modal learning manifests in diverse feature extractors, fusion methods, and pre-trained models. Consequently, designing a simple and universal \textbf{M}ulti-\textbf{M}odal \textbf{S}equential \textbf{R}ecommendation (\textbf{MMSR}) framework remains a formidable challenge. We systematically summarize the existing multi-modal related SR methods and distill the essence into four core components: visual encoder, text encoder, multimodal fusion module, and sequential architecture. Along these dimensions, we dissect the model designs, and answer the foll
    
[^19]: 与算法偏误结果互动的认知偏误用户在有争议话题的整个搜索中

    Cognitively Biased Users Interacting with Algorithmically Biased Results in Whole-Session Search on Controversial Topics

    [https://arxiv.org/abs/2403.17286](https://arxiv.org/abs/2403.17286)

    用户在与算法偏误的搜索结果互动时，受认知偏误影响，特别是在有争议话题上的整个搜索过程中，他们更倾向于选择符合其现有信念的结果，并且确认偏误和结果呈现对搜索行为和结果熟悉度有显著影响。

    

    当用户与信息检索(IR)系统交互时，受到确认偏见影响，往往选择确认他们在社会重要争议问题上的现有信念的搜索结果。为了理解在线搜索用户的判断和态度变化，我们的研究考察了认知偏误用户如何与算法性偏误的搜索引擎结果页面(SERPs)互动。我们设计了在不同偏差条件下的争议性话题上的三次查询搜索会话。我们招募了1,321名众包参与者，探讨了他们的态度变化、搜索互动以及确认偏误的影响。我们得出了三个关键发现：1) 大多数态度变化发生在搜索会话的初始查询中；2) 确认偏误和SERPs上的结果呈现影响当前查询中的搜索行为以及之后查询中点击结果的感知熟悉度。偏误立场也影响态度变化。

    arXiv:2403.17286v1 Announce Type: new  Abstract: When interacting with information retrieval (IR) systems, users, affected by confirmation biases, tend to select search results that confirm their existing beliefs on socially significant contentious issues. To understand the judgments and attitude changes of users searching online, our study examined how cognitively biased users interact with algorithmically biased search engine result pages (SERPs). We designed three-query search sessions on debated topics under various bias conditions. We recruited 1,321 crowdsourcing participants and explored their attitude changes, search interactions, and the effects of confirmation bias. Three key findings emerged: 1) most attitude changes occur in the initial query of a search session; 2) confirmation bias and result presentation on SERPs affect search behaviors in the current query and perceived familiarity with clicked results in subsequent queries. The bias position also affect attitude change
    
[^20]: EXPLORA：用于引发自然儿童-计算机交互的教师-学徒方法论

    EXPLORA: A teacher-apprentice methodology for eliciting natural child-computer interactions

    [https://arxiv.org/abs/2403.17264](https://arxiv.org/abs/2403.17264)

    EXPLORA是一个教师-学徒方法，通过预先观察访谈收集数据，帮助研究人员更深入地理解儿童特征和背景，从而设计出更符合儿童需求的技术。

    

    调查儿童-计算机交互在其背景中的情况对于设计满足儿童需求的技术至关重要。然而，确定设计儿童为中心的技术的相关上下文方面仍然是一个挑战。我们介绍了EXPLORA，这是一个多模态、多阶段的在线方法，包括三个关键阶段：（1）建立教师-学徒关系，（2）向儿童教师学习，以及（3）评估和加强研究者-学徒学习。EXPLORA的核心是通过观前访谈收集态度数据，使研究人员更深入地了解儿童的特征和背景。这为随后的在线观察提供了信息，使研究人员可以关注频繁的互动。此外，研究人员可以与儿童验证初步假设。一种手段-目的分析框架有助于对数据进行系统分析，阐明...

    arXiv:2403.17264v1 Announce Type: cross  Abstract: Investigating child-computer interactions within their contexts is vital for designing technology that caters to children's needs. However, determining what aspects of context are relevant for designing child-centric technology remains a challenge. We introduce EXPLORA, a multimodal, multistage online methodology comprising three pivotal stages: (1) building a teacher-apprentice relationship,(2) learning from child-teachers, and (3) assessing and reinforcing researcher-apprentice learning. Central to EXPLORA is the collection of attitudinal data through pre-observation interviews, offering researchers a deeper understanding of children's characteristics and contexts. This informs subsequent online observations, allowing researchers to focus on frequent interactions. Furthermore, researchers can validate preliminary assumptions with children. A means-ends analysis framework aids in the systematic analysis of data, shedding light on cont
    
[^21]: CADGL: 上下文感知深度图学习用于预测药物-药物相互作用

    CADGL: Context-Aware Deep Graph Learning for Predicting Drug-Drug Interactions

    [https://arxiv.org/abs/2403.17210](https://arxiv.org/abs/2403.17210)

    通过CADGL框架，利用上下文感知深度图学习来预测药物-药物相互作用，解决了现有DDI预测模型在泛化、特征提取和现实应用方面的挑战

    

    药物-药物相互作用（DDIs）的研究是药物开发过程中的一个关键元素。DDIs发生在一个药物的性质受其他药物包含的影响时。检测有利的DDIs有可能为在实际设置中应用的创新药物的创造和推进铺平道路。然而，现有的DDI预测模型在极端情况下的泛化、稳健特征提取以及现实应用可能性方面持续面临挑战。我们旨在通过利用上下文感知深度图学习的有效性，引入一种名为CADGL的新颖框架来应对这些挑战。基于定制的变分图自编码器（VGAE），我们利用两个上下文预处理器从两个不同视角：局部邻域和分子上下文，在异质图结构中提取特征，捕获关键的结构和生理化学信息。

    arXiv:2403.17210v1 Announce Type: cross  Abstract: Examining Drug-Drug Interactions (DDIs) is a pivotal element in the process of drug development. DDIs occur when one drug's properties are affected by the inclusion of other drugs. Detecting favorable DDIs has the potential to pave the way for creating and advancing innovative medications applicable in practical settings. However, existing DDI prediction models continue to face challenges related to generalization in extreme cases, robust feature extraction, and real-life application possibilities. We aim to address these challenges by leveraging the effectiveness of context-aware deep graph learning by introducing a novel framework named CADGL. Based on a customized variational graph autoencoder (VGAE), we capture critical structural and physio-chemical information using two context preprocessors for feature extraction from two different perspectives: local neighborhood and molecular context, in a heterogeneous graphical structure. Ou
    
[^22]: 利用大型语言模型代理生成资产管理外壳：数字孪生和语义节点中的互操作性

    Generation of Asset Administration Shell with Large Language Model Agents: Interoperability in Digital Twins with Semantic Node

    [https://arxiv.org/abs/2403.17209](https://arxiv.org/abs/2403.17209)

    通过大型语言模型代理生成AAS实例模型，实现了在数字孪生中的互操作性，降低了手动创建成本和时间。

    

    这项研究介绍了一种新颖的方法，用于协助在工业4.0背景下为数字孪生建模创建资产管理外壳（AAS）实例，旨在增强智能制造中的互操作性，减少手动工作。我们构建了一个“语义节点”数据结构来捕捉文本数据的语义要义。然后，设计并实现了一个由大型语言模型驱动的系统，用于处理“语义节点”并从文本技术数据生成AAS实例模型。我们的评估表明，有效生成率为62-79%，表明相当比例的手动创建工作可以转换为更容易的验证工作，从而减少创建AAS实例模型的时间和成本。在我们的评估中，对不同LLM的比较分析以及检索增强生成（RAG）机制的深入消融研究提供了有关LLM有效性的见解。

    arXiv:2403.17209v1 Announce Type: new  Abstract: This research introduces a novel approach for assisting the creation of Asset Administration Shell (AAS) instances for digital twin modeling within the context of Industry 4.0, aiming to enhance interoperability in smart manufacturing and reduce manual effort. We construct a "semantic node" data structure to capture the semantic essence of textual data. Then, a system powered by large language models is designed and implemented to process "semantic node" and generate AAS instance models from textual technical data. Our evaluation demonstrates a 62-79% effective generation rate, indicating a substantial proportion of manual creation effort can be converted into easier validation effort, thereby reducing the time and cost in creating AAS instance models. In our evaluation, a comparative analysis of different LLMs and an in-depth ablation study of Retrieval-Augmented Generation (RAG) mechanisms provide insights into the effectiveness of LLM
    
[^23]: GOLF：目标导向的长期生活任务，由人工智能协作支持

    GOLF: Goal-Oriented Long-term liFe tasks supported by human-AI collaboration

    [https://arxiv.org/abs/2403.17089](https://arxiv.org/abs/2403.17089)

    该研究提出了GOLF框架，通过目标导向和长期规划增强LLMs的能力，以协助用户处理重要的生活决策。

    

    ChatGPT等大型语言模型（LLMs）的出现彻底改变了人工智能交互和信息获取过程。利用LLMs作为搜索引擎的替代方案，用户现在可以访问根据其查询定制的摘要信息，显著减少了在导航大量信息资源时所带来的认知负荷。这种转变凸显了LLMs在重新定义信息获取范式方面的潜力。基于任务焦点信息检索和LLMs的任务规划能力，本研究将LLMs的能力范围扩展到支持用户导航长期和重要的生活任务。它引入了GOLF框架（目标导向的长期生活任务），侧重于增强LLMs通过目标定向和长期规划来协助用户做出重要的生活决策。该方法论包含了一个全面的类比实验

    arXiv:2403.17089v1 Announce Type: cross  Abstract: The advent of ChatGPT and similar large language models (LLMs) has revolutionized the human-AI interaction and information-seeking process. Leveraging LLMs as an alternative to search engines, users can now access summarized information tailored to their queries, significantly reducing the cognitive load associated with navigating vast information resources. This shift underscores the potential of LLMs in redefining information access paradigms. Drawing on the foundation of task-focused information retrieval and LLMs' task planning ability, this research extends the scope of LLM capabilities beyond routine task automation to support users in navigating long-term and significant life tasks. It introduces the GOLF framework (Goal-Oriented Long-term liFe tasks), which focuses on enhancing LLMs' ability to assist in significant life decisions through goal orientation and long-term planning. The methodology encompasses a comprehensive simul
    
[^24]: 利用预训练语言模型进行粗调优的专题文档检索

    Coarse-Tuning for Ad-hoc Document Retrieval Using Pre-trained Language Models

    [https://arxiv.org/abs/2403.16915](https://arxiv.org/abs/2403.16915)

    本研究引入了粗调优作为一个中间学习阶段，连接了预训练和微调，在专题文档检索中显著改善了效果。

    

    在信息检索系统中，利用预训练语言模型（PLM-based IR）进行微调需要学习查询表示和查询-文档关系，除了下游任务特定的学习。本研究引入了粗调优作为一个中间学习阶段，连接了预训练和微调。通过在粗调优学习查询表示和查询-文档关系，我们旨在减少微调的负担，提高下游IR任务的学习效果。我们提出了用于粗调优的查询-文档对预测（QDPP），其预测查询-文档对的适当性。评估实验显示，所提出的方法显著改善了四个专题文档检索数据集中的MRR和/或nDCG@5。此外，查询预测任务的结果表明，粗调优促进了查询表示和查询-文档关系的学习。

    arXiv:2403.16915v1 Announce Type: cross  Abstract: Fine-tuning in information retrieval systems using pre-trained language models (PLM-based IR) requires learning query representations and query-document relations, in addition to downstream task-specific learning. This study introduces coarse-tuning as an intermediate learning stage that bridges pre-training and fine-tuning. By learning query representations and query-document relations in coarse-tuning, we aim to reduce the load of fine-tuning and improve the learning effect of downstream IR tasks. We propose Query-Document Pair Prediction (QDPP) for coarse-tuning, which predicts the appropriateness of query-document pairs. Evaluation experiments show that the proposed method significantly improves MRR and/or nDCG@5 in four ad-hoc document retrieval datasets. Furthermore, the results of the query prediction task suggested that coarse-tuning facilitated learning of query representation and query-document relations.
    
[^25]: 处理好您的提示偏见！调查和减轻事实知识提取中的提示偏见

    Take Care of Your Prompt Bias! Investigating and Mitigating Prompt Bias in Factual Knowledge Extraction

    [https://arxiv.org/abs/2403.09963](https://arxiv.org/abs/2403.09963)

    本文调查了预训练语言模型在事实知识提取中存在的“提示偏见”，找到了不同类型提示的偏见程度，以及这种偏见对不同基准测试的影响，并提出了一种基于表示的方法来减轻这种提示偏见。

    

    最近的研究表明，预训练语言模型（PLMs）在事实知识提取中存在“提示偏见”，即提示往往会引入对特定标签的偏见。然而，模型内部提示偏见的程度和影响尚未得到充分探讨。为了回应这一点，本文量化了不同类型提示的偏见，并评估了它们对不同基准测试的影响。我们发现：1）实验中的所有提示都表现出不可忽视的偏见，基于梯度的提示如AutoPrompt和OptiPrompt显示出更高水平的偏见；2）提示偏见可以通过过度拟合测试数据集不合理地放大基准测试的准确性，特别是在类似LAMA这样的不平衡数据集上。基于这些发现，我们提出了一种基于表示的方法来减轻提示偏见，在推断时。具体而言，我们首先使用仅提示查询来估计有偏差的表示，然后从中删除。

    arXiv:2403.09963v1 Announce Type: cross  Abstract: Recent research shows that pre-trained language models (PLMs) suffer from "prompt bias" in factual knowledge extraction, i.e., prompts tend to introduce biases toward specific labels. However, the extent and impact of prompt bias within the model remain underexplored. In response, this paper quantifies the bias with various types of prompts and assesses their impact on different benchmarks. We show that: 1) all prompts in the experiments exhibit non-negligible bias, with gradient-based prompts like AutoPrompt and OptiPrompt displaying significantly higher levels of bias; 2) prompt bias can amplify benchmark accuracy unreasonably by overfitting the test datasets, especially on imbalanced datasets like LAMA. Based on these findings, we propose a representation-based approach to mitigate the prompt bias during inference time. Specifically, we first estimate the biased representation using prompt-only querying, and then remove it from the 
    
[^26]: 评估大语言模型作为对话推荐中生成用户模拟器

    Evaluating Large Language Models as Generative User Simulators for Conversational Recommendation

    [https://arxiv.org/abs/2403.09738](https://arxiv.org/abs/2403.09738)

    大型语言模型作为生成式用户模拟器在对话推荐中展现出潜力，新的协议通过五个任务评估了语言模型模拟人类行为的准确程度，揭示了模型与人类行为的偏差，并提出了如何通过模型选择和提示策略减少这些偏差。

    

    合成用户是对话推荐系统评估中成本效益较高的真实用户代理。大型语言模型表现出在模拟类似人类行为方面的潜力，这引发了它们能否代表多样化用户群体的问题。我们引入了一个新的协议，用于衡量语言模型能够准确模拟对话推荐中人类行为的程度。该协议由五个任务组成，每个任务旨在评估合成用户应该表现出的关键特性：选择要谈论的物品，表达二进制偏好，表达开放式偏好，请求推荐以及提供反馈。通过对基准模拟器的评估，我们展示了这些任务有效地揭示了语言模型与人类行为的偏差，并提供了关于如何通过模型选择和提示策略减少这些偏差的见解。

    arXiv:2403.09738v1 Announce Type: cross  Abstract: Synthetic users are cost-effective proxies for real users in the evaluation of conversational recommender systems. Large language models show promise in simulating human-like behavior, raising the question of their ability to represent a diverse population of users. We introduce a new protocol to measure the degree to which language models can accurately emulate human behavior in conversational recommendation. This protocol is comprised of five tasks, each designed to evaluate a key property that a synthetic user should exhibit: choosing which items to talk about, expressing binary preferences, expressing open-ended preferences, requesting recommendations, and giving feedback. Through evaluation of baseline simulators, we demonstrate these tasks effectively reveal deviations of language models from human behavior, and offer insights on how to reduce the deviations with model selection and prompting strategies.
    
[^27]: 协同过滤的图信号扩散模型

    Graph Signal Diffusion Model for Collaborative Filtering

    [https://arxiv.org/abs/2311.08744](https://arxiv.org/abs/2311.08744)

    提出了一种用于协同过滤的图信号扩散模型，解决了现有扩散模型在建模隐式反馈数据方面的不足，通过对扩散模型进行创新改进，解决了标准扩散过程导致的个性化信息丢失和图形结构不一致等问题。

    

    协同过滤是推荐系统中的关键技术之一。在各种方法中，一种越来越受欢迎的范式是基于历史观察重建用户-物品交互。这可以被看作是一个条件生成任务，最近发展的扩散模型显示出巨大潜力。然而，现有的扩散模型研究缺乏对隐式反馈数据建模的有效解决方案。特别是，标准扩散过程的各向同性特性未能考虑物品之间的异质依赖关系，导致与交互空间的图形结构不一致。同时，随机噪声破坏了交互向量中的个性化信息，导致反向重建困难。在这篇论文中，我们对扩散模型进行了新颖的改进，并提出了用于协同过滤的图信号扩散模型（称为GiffCF）。

    arXiv:2311.08744v2 Announce Type: replace-cross  Abstract: Collaborative filtering is a critical technique in recommender systems. Among various methods, an increasingly popular paradigm is to reconstruct user-item interactions based on the historical observations. This can be viewed as a conditional generative task, where recently developed diffusion model demonstrates great potential. However, existing studies on diffusion models lack effective solutions for modeling implicit feedback data. Particularly, the isotropic nature of the standard diffusion process fails to account for the heterogeneous dependencies among items, leading to a misalignment with the graphical structure of the interaction space. Meanwhile, random noise destroying personalized information in interaction vectors, causing difficulty in reverse reconstruction. In this paper, we make novel adaptions of diffusion model and propose Graph Signal Diffusion Model for Collaborative Filtering (named GiffCF). To better repr
    
[^28]: 优化点击率预测的特征集

    Optimizing Feature Set for Click-Through Rate Prediction

    [https://arxiv.org/abs/2301.10909](https://arxiv.org/abs/2301.10909)

    提出了一种名为OptFS的新方法，用于优化点击率预测模型中特征集的选择。

    

    单击率预测（CTR）模型将特征转换为潜在向量，并列举可能的特征交互以改善基于输入特征集的性能。因此，在选择最佳特征集时，我们应考虑特征及其交互的影响。然而，大多数先前的作品侧重于特征字段选择，或仅根据固定特征集选择特征交互以生成特征集。

    arXiv:2301.10909v2 Announce Type: replace  Abstract: Click-through prediction (CTR) models transform features into latent vectors and enumerate possible feature interactions to improve performance based on the input feature set. Therefore, when selecting an optimal feature set, we should consider the influence of both feature and its interaction. However, most previous works focus on either feature field selection or only select feature interaction based on the fixed feature set to produce the feature set. The former restricts search space to the feature field, which is too coarse to determine subtle features. They also do not filter useless feature interactions, leading to higher computation costs and degraded model performance. The latter identifies useful feature interaction from all available features, resulting in many redundant features in the feature set. In this paper, we propose a novel method named OptFS to address these problems. To unify the selection of feature and its int
    
[^29]: 通过全面评估和Leaderboarding理解长文档排名模型的性能

    Understanding Performance of Long-Document Ranking Models through Comprehensive Evaluation and Leaderboarding

    [https://arxiv.org/abs/2207.01262](https://arxiv.org/abs/2207.01262)

    在标准收集的初步实验中，我们发现长文档模型在MRR或NDCG方面性能不佳，表现低于FirstP，或平均最多超越5％。我们推测这不是因为模型无法处理长上下文，而是由于相关段落具有位置偏见，往往位于前512个文档标记之中。我们找到证据表明这种偏见至少存在于两个测试集中，这促使我们创建了一个新的收集MS MARCO FarRelevant，其中包含

    

    我们评估了20多个用于长文档排名的Transformer模型（包括最近使用FlashAttention训练的LongP模型），并将它们与简单的FirstP基线进行了比较（将相同模型应用于输入截断为前512个标记）。我们使用MS MARCO文档v1作为主要训练集，并在零-shot场景下评估了模型，以及在对其他收集进行微调后评估了模型。

    arXiv:2207.01262v2 Announce Type: replace-cross  Abstract: We evaluated 20+ Transformer models for ranking of long documents (including recent LongP models trained with FlashAttention) and compared them with simple FirstP baselines (applying the same model to input truncated to the first 512 tokens). We used MS MARCO Documents v1 as a primary training set and evaluated models in the zero-shot scenario as well as after fine-tuning on other collections.   In our initial experiments with standard collections we found that long-document models underperformed FirstP or outperformed it by at most 5% on average in terms of MRR or NDCG. We then conjectured that this was not due to models inability to process long context but rather due to a positional bias of relevant passages, which tended to be among the first 512 document tokens. We found evidence that this bias was, indeed, present in at least two test sets, which motivated us to create a new collection MS MARCO FarRelevant where the relev
    
[^30]: 通过预训练的双向时间数据增强改进顺序推荐

    Improving Sequential Recommendations via Bidirectional Temporal Data Augmentation with Pre-training

    [https://arxiv.org/abs/2112.06460](https://arxiv.org/abs/2112.06460)

    提出了一种名为BARec的新方法，通过双向时间增强和知识增强微调来改进顺序推荐，合成真实的伪先前项目，保留用户偏好并捕捉更深层次的项目语义相关性

    

    顺序推荐系统对识别用户的时间偏好至关重要。然而，从简化的用户交互序列中学习的任务是一个显著挑战。数据增强被确定为增强这些序列信息丰富性的有效策略。传统的增强技术，如项目随机化，可能破坏固有的时间动态。尽管最近在逆时间伪项目生成方面取得了进展，但在自然时间顺序上评估时，它们可能引入时间差异。为此，我们提出了一种复杂的方法，Bidirectional temporal data Augmentation with pre-training（BARec）。我们的方法利用双向时间增强和知识增强微调来合成真实的伪先前项目，从而保留用户偏好并捕捉更深层次的项目语义相关性。

    arXiv:2112.06460v5 Announce Type: replace  Abstract: Sequential recommendation systems are integral to discerning temporal user preferences. Yet, the task of learning from abbreviated user interaction sequences poses a notable challenge. Data augmentation has been identified as a potent strategy to enhance the informational richness of these sequences. Traditional augmentation techniques, such as item randomization, may disrupt the inherent temporal dynamics. Although recent advancements in reverse chronological pseudo-item generation have shown promise, they can introduce temporal discrepancies when assessed in a natural chronological context. In response, we introduce a sophisticated approach, Bidirectional temporal data Augmentation with pre-training (BARec). Our approach leverages bidirectional temporal augmentation and knowledge-enhanced fine-tuning to synthesize authentic pseudo-prior items that \emph{retain user preferences and capture deeper item semantic correlations}, thus bo
    
[^31]: 通过对比反馈将大型语言模型的能力与信息检索上下文对齐

    Aligning the Capabilities of Large Language Models with the Context of Information Retrieval via Contrastive Feedback. (arXiv:2309.17078v1 [cs.IR])

    [http://arxiv.org/abs/2309.17078](http://arxiv.org/abs/2309.17078)

    通过对比反馈强化学习框架，有效提升大型语言模型在信息检索中的应用，使其能够生成更具特定性和上下文适应性的回复。

    

    信息检索(IR)是寻找满足用户信息需求的过程，在现代人的生活中起着重要作用。近年来，大型语言模型(LLMs)在各种任务中展示了杰出的能力，其中一些任务对于IR来说非常重要。然而，LLMs经常面临生成缺乏特定性回复的问题。这在很多情况下限制了LLMs在IR中的整体效果。为了解决这些问题，我们提出了一种无监督对齐框架，称为对比反馈强化学习(RLCF)，它赋予LLMs生成既具有高质量又与IR任务需求相符的上下文特定回复的能力。具体而言，我们通过将每个文档与其相似文档进行比较构建对比反馈，然后提出了一个名为Batched-MRR的奖励函数，教导LLMs生成能够捕捉区分文档与其相似文档的细粒度信息的回复。

    Information Retrieval (IR), the process of finding information to satisfy user's information needs, plays an essential role in modern people's lives. Recently, large language models (LLMs) have demonstrated remarkable capabilities across various tasks, some of which are important for IR. Nonetheless, LLMs frequently confront the issue of generating responses that lack specificity. This has limited the overall effectiveness of LLMs for IR in many cases. To address these issues, we present an unsupervised alignment framework called Reinforcement Learning from Contrastive Feedback (RLCF), which empowers LLMs to generate both high-quality and context-specific responses that suit the needs of IR tasks. Specifically, we construct contrastive feedback by comparing each document with its similar documents, and then propose a reward function named Batched-MRR to teach LLMs to generate responses that captures the fine-grained information that distinguish documents from their similar ones. To dem
    
[^32]: 开放知识图谱的十年学术研究

    A Decade of Scholarly Research on Open Knowledge Graphs. (arXiv:2306.13186v1 [cs.DL])

    [http://arxiv.org/abs/2306.13186](http://arxiv.org/abs/2306.13186)

    本文分析了过去十年开放知识图谱的学术研究趋势和主题，并确定了知识图谱构建和增强、评估和复用以及将知识图谱融入NLP系统的三个主要研究主题。

    

    过去十年间，开放知识图谱的普及导致了对该话题的学术研究的激增。本文展示了针对2013年至2023年间出版的有关开放知识图谱的学术文献的文献计量分析。该研究旨在识别该领域中的趋势，模式和研究的影响，以及出现的关键主题和研究问题。该作品使用文献计量技术分析了从Scopus检索到的4445篇学术文章的样本。研究结果显示，每年关于开放知识图谱的出版物数量不断增加，特别是在发达国家(+50 per year)。这些成果发表在高度引用的学术期刊和会议上。该研究确定了三个主要研究主题：(1)知识图谱的构建和增强，(2)评估和复用，以及(3)将知识图谱融入NLP系统中。在这些主题中，研究确定了广泛研究的具体任务，例如实体链接，关系提取和本体学习。

    The proliferation of open knowledge graphs has led to a surge in scholarly research on the topic over the past decade. This paper presents a bibliometric analysis of the scholarly literature on open knowledge graphs published between 2013 and 2023. The study aims to identify the trends, patterns, and impact of research in this field, as well as the key topics and research questions that have emerged. The work uses bibliometric techniques to analyze a sample of 4445 scholarly articles retrieved from Scopus. The findings reveal an ever-increasing number of publications on open knowledge graphs published every year, particularly in developed countries (+50 per year). These outputs are published in highly-referred scholarly journals and conferences. The study identifies three main research themes: (1) knowledge graph construction and enrichment, (2) evaluation and reuse, and (3) fusion of knowledge graphs into NLP systems. Within these themes, the study identifies specific tasks that have 
    

