# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [The Impact of Differential Privacy on Recommendation Accuracy and Popularity Bias.](http://arxiv.org/abs/2401.03883) | 本研究探讨了差分隐私对推荐准确性和流行偏差的影响。研究发现，应用差分隐私后推荐准确性显著下降，推荐物品的流行度大幅增加，尤其对于喜欢不受欢迎物品的用户影响更为严重。 |
| [^2] | [Reproducibility Analysis and Enhancements for Multi-Aspect Dense Retriever with Aspect Learning.](http://arxiv.org/abs/2401.03648) | 这项研究提出了多方面密集检索器的可重现性分析与增强。通过对MADRAL模型的复现实验，我们发现从头学习“OTHER”在方面融合中是有害的，而我们提出的替代方案可以大大提高检索性能。 |
| [^3] | [Starling: An I/O-Efficient Disk-Resident Graph Index Framework for High-Dimensional Vector Similarity Search on Data Segment.](http://arxiv.org/abs/2401.02116) | Starling是一种I/O高效的基于磁盘的图索引框架，用于在数据片段上进行高维向量相似性搜索，在准确性、效率和空间成本之间取得平衡。 |
| [^4] | [Poisoning Attacks against Recommender Systems: A Survey.](http://arxiv.org/abs/2401.01527) | 本调查论文系统地、最新地回顾了针对推荐系统的毒化攻击研究领域，提出了一个新颖而全面的分类法，展示了潜在的未来研究方向，并引入了一个开源库ARLib用于比较毒化攻击方法。 |
| [^5] | [RecRanker: Instruction Tuning Large Language Model as Ranker for Top-k Recommendation.](http://arxiv.org/abs/2312.16018) | 本文介绍了RecRanker，专门用于指令调优LLM以作为前k项推荐的排名器。 |
| [^6] | [Session-Based Recommendation by Exploiting Substitutable and Complementary Relationships from Multi-behavior Data.](http://arxiv.org/abs/2312.14957) | 本文提出了一种新的基于会话的推荐方法，通过考虑多个行为类型以及产品之间的可替代和互补关系，以增强推荐效果。 |
| [^7] | [WellFactor: Patient Profiling using Integrative Embedding of Healthcare Data.](http://arxiv.org/abs/2312.14129) | WellFactor是一种使用综合嵌入医疗数据的患者分类方法，并通过使用受约束的低秩逼近、结合标签信息来优化嵌入结果，同时具有即时计算新数据嵌入的特点。在实际医疗数据上得到了验证。 |
| [^8] | [Linguistic and Structural Basis of Engineering Design Knowledge.](http://arxiv.org/abs/2312.06355) | 本文通过分析33881份专利文件的样本，将工程设计知识阐释为知识图谱，从而揭示工程设计知识的语言和结构基础。 |
| [^9] | [A Multi-Granularity-Aware Aspect Learning Model for Multi-Aspect Dense Retrieval.](http://arxiv.org/abs/2312.02538) | 这项研究提出了一种多颗粒度感知的多方面学习模型，用于解决结构化数据的稠密检索问题。与现有方法不同的是，该模型不仅考虑了项目方面值之间的细粒度语义关系，还避免了将方面学习集中到CLS令牌或仅通过值预测目标学习方面嵌入的问题。 |
| [^10] | [TSRankLLM: A Two-Stage Adaptation of LLMs for Text Ranking.](http://arxiv.org/abs/2311.16720) | TSRankLLM提出了一种两阶段适应方法用于文本排序，通过连续预训练和改进的优化策略，实现了更好的性能。 |
| [^11] | [GraphPro: Graph Pre-training and Prompt Learning for Recommendation.](http://arxiv.org/abs/2311.16716) | GraphPro是一个结合了参数高效和动态图预训练与提示学习的框架，能够有效捕捉长期用户偏好和短期行为动态，从而在真实世界的推荐系统中提供准确和及时的推荐。 |
| [^12] | [AI-Generated Images Introduce Invisible Relevance Bias to Text-Image Retrieval.](http://arxiv.org/abs/2311.14084) | 本研究通过构建合适的基准和进行大量实验发现，由AI生成的图像对于文本-图像检索模型引入了一个不可见的相关偏差，即使这些生成的图像在视觉上与查询的相关特征相比并没有更多。这种不可见的相关偏差普遍存在于不同训练数据和架构的检索模型中。 |
| [^13] | [Deep Hashing via Householder Quantization.](http://arxiv.org/abs/2311.04207) | 基于 Householder 量化的深度哈希提出了一种替代的量化策略，通过将学习问题分解为两个阶段来改善哈希方法中相似度学习和量化之间的交互，从而提高嵌入的质量。 |
| [^14] | [LLMs may Dominate Information Access: Neural Retrievers are Biased Towards LLM-Generated Texts.](http://arxiv.org/abs/2310.20501) | 近期的研究发现，大型语言模型（LLMs）对信息检索系统产生了一种偏见，倾向于将LLM生成的文档排名较高。这种“来源偏见”可能对信息访问产生重大影响。 |
| [^15] | [Do as I can, not as I get.](http://arxiv.org/abs/2306.10345) | 该论文提出了一种名为TMR的模型，用于从模拟数据环境中挖掘有价值的信息。 |
| [^16] | [Towards More Robust and Accurate Sequential Recommendation with Cascade-guided Adversarial Training.](http://arxiv.org/abs/2304.05492) | 本研究利用级联指导下的对抗训练方法，增强了串联推荐模型的鲁棒性和准确性，取得了比已有方法更好的结果。 |

# 详细

[^1]: 差分隐私对推荐准确性和流行偏差的影响

    The Impact of Differential Privacy on Recommendation Accuracy and Popularity Bias. (arXiv:2401.03883v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2401.03883](http://arxiv.org/abs/2401.03883)

    本研究探讨了差分隐私对推荐准确性和流行偏差的影响。研究发现，应用差分隐私后推荐准确性显著下降，推荐物品的流行度大幅增加，尤其对于喜欢不受欢迎物品的用户影响更为严重。

    

    基于协同过滤的推荐系统利用大量的用户行为数据，这带来了严重的隐私风险。因此，通常会向数据中添加随机噪声以确保差分隐私（DP）。然而，迄今为止，人们对DP如何影响个性化推荐的方式并不清楚。在这项工作中，我们研究了DP应用于最先进的推荐模型的训练数据时，对推荐准确性和流行偏差的影响。我们的发现有三个：首先，我们发现几乎所有用户的推荐都会在应用DP后发生变化。其次，推荐准确性大幅下降，而推荐物品的流行度急剧增加，表明流行偏差加剧。第三，我们发现，与喜欢流行物品的用户相比，喜欢不受欢迎物品的用户会更严重地受到DP的流行偏差影响。

    Collaborative filtering-based recommender systems leverage vast amounts of behavioral user data, which poses severe privacy risks. Thus, often, random noise is added to the data to ensure Differential Privacy (DP). However, to date, it is not well understood, in which ways this impacts personalized recommendations. In this work, we study how DP impacts recommendation accuracy and popularity bias, when applied to the training data of state-of-the-art recommendation models. Our findings are three-fold: First, we find that nearly all users' recommendations change when DP is applied. Second, recommendation accuracy drops substantially while recommended item popularity experiences a sharp increase, suggesting that popularity bias worsens. Third, we find that DP exacerbates popularity bias more severely for users who prefer unpopular items than for users that prefer popular items.
    
[^2]: 《多方面密集检索器与方面学习的可重现性分析与增强》的翻译标题

    Reproducibility Analysis and Enhancements for Multi-Aspect Dense Retriever with Aspect Learning. (arXiv:2401.03648v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2401.03648](http://arxiv.org/abs/2401.03648)

    这项研究提出了多方面密集检索器的可重现性分析与增强。通过对MADRAL模型的复现实验，我们发现从头学习“OTHER”在方面融合中是有害的，而我们提出的替代方案可以大大提高检索性能。

    

    多方面密集检索旨在将方面信息（例如品牌和类别）与双编码器相结合，以促进相关性匹配。作为一个早期代表性的多方面密集检索器，MADRAL学习了几个额外的方面嵌入，并将显性方面与隐性方面“OTHER”融合以获取最终表示。MADRAL在专有数据上进行了评估，但其代码未发布，这使得难以验证其在其他数据集上的有效性。我们未能在公开的MA-Amazon数据上复现其有效性，因此我们希望探索原因并重新审视其组成部分。我们提出了几种组件替代方案进行比较，包括将“OTHER”替换为“CLS”并使用前几个内容标记表示方面。通过大量实验证实，从头开始学习“OTHER”在方面融合中是有害的。相反，我们提出的变体可以显著提高检索性能。我们的研究不仅揭示了原有模型的问题，还提出了解决方案。

    Multi-aspect dense retrieval aims to incorporate aspect information (e.g., brand and category) into dual encoders to facilitate relevance matching. As an early and representative multi-aspect dense retriever, MADRAL learns several extra aspect embeddings and fuses the explicit aspects with an implicit aspect "OTHER" for final representation. MADRAL was evaluated on proprietary data and its code was not released, making it challenging to validate its effectiveness on other datasets. We failed to reproduce its effectiveness on the public MA-Amazon data, motivating us to probe the reasons and re-examine its components. We propose several component alternatives for comparisons, including replacing "OTHER" with "CLS" and representing aspects with the first several content tokens. Through extensive experiments, we confirm that learning "OTHER" from scratch in aspect fusion is harmful. In contrast, our proposed variants can greatly enhance the retrieval performance. Our research not only shed
    
[^3]: Starling: 一种用于高维向量相似性搜索的I/O高效的基于磁盘的图索引框架，用于数据片段中 (arXiv:2401.02116v1 [cs.DB])

    Starling: An I/O-Efficient Disk-Resident Graph Index Framework for High-Dimensional Vector Similarity Search on Data Segment. (arXiv:2401.02116v1 [cs.DB])

    [http://arxiv.org/abs/2401.02116](http://arxiv.org/abs/2401.02116)

    Starling是一种I/O高效的基于磁盘的图索引框架，用于在数据片段上进行高维向量相似性搜索，在准确性、效率和空间成本之间取得平衡。

    

    高维向量相似性搜索(HVSS)作为数据科学和人工智能应用的强大工具，正受到关注。随着向量数据的增长，内存索引变得非常昂贵，因为它们需要大量扩展主内存资源。一种可能的解决方案是使用基于磁盘的实现，将向量数据存储和搜索在高性能设备(如NVMe SSD)中。然而，对于数据片段的HVSS仍然是向量数据库中的挑战，其中一个机器有多个片段来实现系统功能（如扩展）。在这种情况下，每个片段的内存和磁盘空间有限，因此数据片段上的HVSS需要在准确性，效率和空间成本之间取得平衡。现有的基于磁盘的方法并没有同时考虑到所有这些要求。在本文中，我们提出了Starling，一种I/O高效的基于磁盘的图索引框架，它在片段中优化数据布局和搜索策略。

    High-dimensional vector similarity search (HVSS) is receiving a spotlight as a powerful tool for various data science and AI applications. As vector data grows larger, in-memory indexes become extremely expensive because they necessitate substantial expansion of main memory resources. One possible solution is to use disk-based implementation, which stores and searches vector data in high-performance devices like NVMe SSDs. However, HVSS for data segments is still challenging in vector databases, where one machine has multiple segments for system features (like scaling) purposes. In this setting, each segment has limited memory and disk space, so HVSS on the data segment needs to balance accuracy, efficiency, and space cost. Existing disk-based methods are sub-optimal because they do not consider all these requirements together. In this paper, we present Starling, an I/O-efficient disk-resident graph index framework that optimizes data layout and search strategy in the segment. It has t
    
[^4]: 推荐系统中的恶意攻击：一项调查

    Poisoning Attacks against Recommender Systems: A Survey. (arXiv:2401.01527v1 [cs.IR])

    [http://arxiv.org/abs/2401.01527](http://arxiv.org/abs/2401.01527)

    本调查论文系统地、最新地回顾了针对推荐系统的毒化攻击研究领域，提出了一个新颖而全面的分类法，展示了潜在的未来研究方向，并引入了一个开源库ARLib用于比较毒化攻击方法。

    

    现代推荐系统取得了显著的成功，但它们仍然容易受到恶意活动的攻击，特别是毒化攻击。这些攻击涉及将恶意数据注入到推荐系统的训练数据集中，从而损害其完整性，并通过操纵推荐结果来获取非法利润。本调查论文系统地、最新地回顾了针对推荐系统的毒化攻击研究领域。我们提出了一个新颖而全面的分类法，将现有的毒化攻击方法分为三个不同的类别：组件特定、目标驱动和能力探测。对于每个类别，我们详细讨论了其机制以及相关方法。此外，本文还突出了该领域的潜在未来研究方向。另外，为了促进和基准测试毒化攻击的实证比较，我们引入了一个开源库ARLib，该库包含了一整套毒化攻击模型和常用的测试数据集。

    Modern recommender systems have seen substantial success, yet they remain vulnerable to malicious activities, notably poisoning attacks. These attacks involve injecting malicious data into the training datasets of RS, thereby compromising their integrity and manipulating recommendation outcomes for gaining illicit profits. This survey paper provides a systematic and up-to-date review of the research landscape on Poisoning Attacks against Recommendation (PAR). A novel and comprehensive taxonomy is proposed, categorizing existing PAR methodologies into three distinct categories: Component-Specific, Goal-Driven, and Capability Probing. For each category, we discuss its mechanism in detail, along with associated methods. Furthermore, this paper highlights potential future research avenues in this domain. Additionally, to facilitate and benchmark the empirical comparison of PAR, we introduce an open-source library, ARLib, which encompasses a comprehensive collection of PAR models and common
    
[^5]: RecRanker: 使用大型语言模型作为排名器进行前k项推荐的指令调优

    RecRanker: Instruction Tuning Large Language Model as Ranker for Top-k Recommendation. (arXiv:2312.16018v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2312.16018](http://arxiv.org/abs/2312.16018)

    本文介绍了RecRanker，专门用于指令调优LLM以作为前k项推荐的排名器。

    

    大型语言模型(LLMs)展示了卓越的能力并广泛应用于各个领域，包括推荐系统。许多研究采用专门的“提示”来利用LLMs的上下文学习能力。例如，LLMs被提示为零-shot排名器，用于对由检索模型生成的候选项进行列表排名，以用于推荐。最近的研究还使用指令调优技术，通过与人类偏好的对齐来提供更有前景的推荐。尽管具有潜力，但目前的研究忽视了整合多个排名任务以提高模型性能。此外，传统推荐模型的信号未与LLM整合，限制了当前系统的性能。

    Large language models (LLMs) have demonstrated remarkable capabilities and have been extensively deployed across various domains, including recommender systems. Numerous studies have employed specialized \textit{prompts} to harness the in-context learning capabilities intrinsic to LLMs. For example, LLMs are prompted to act as zero-shot rankers for listwise ranking, evaluating candidate items generated by a retrieval model for recommendation. Recent research further uses instruction tuning techniques to align LLM with human preference for more promising recommendations. Despite its potential, current research overlooks the integration of multiple ranking tasks to enhance model performance. Moreover, the signal from the conventional recommendation model is not integrated into the LLM, limiting the current system performance.  In this paper, we introduce RecRanker, tailored for instruction tuning LLM to serve as the \textbf{Ranker} for top-\textit{k} \textbf{Rec}ommendations. Specificall
    
[^6]: 利用多行为数据中的可替代和互补关系进行基于会话的推荐

    Session-Based Recommendation by Exploiting Substitutable and Complementary Relationships from Multi-behavior Data. (arXiv:2312.14957v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2312.14957](http://arxiv.org/abs/2312.14957)

    本文提出了一种新的基于会话的推荐方法，通过考虑多个行为类型以及产品之间的可替代和互补关系，以增强推荐效果。

    

    基于会话的推荐旨在根据用户最近的用户-物品交互序列动态地向用户推荐物品。大多数现有的基于会话的推荐研究采用先进的深度学习方法。然而，大多数研究只考虑了一种特殊的行为类型（如点击），而那些考虑多种类型行为的研究则忽视了充分利用产品之间关系的机会。因此，本文提出了一种新的方法，称为基于多行为数据的可替代和互补关系（简称SCRM），以更好地探索产品之间的关系进行有效的推荐。具体来说，我们首先根据用户在每个会话中的顺序行为共同考虑“点击”和“购买”行为，构建可替代和互补的图。然后，我们设计了一个去噪网络来消除错误关系，并通过一个特别设计的损失函数进一步考虑这两个关系的约束。

    Session-based recommendation (SR) aims to dynamically recommend items to a user based on a sequence of the most recent user-item interactions. Most existing studies on SR adopt advanced deep learning methods. However, the majority only consider a special behavior type (e.g., click), while those few considering multi-typed behaviors ignore to take full advantage of the relationships between products (items). In this case, the paper proposes a novel approach, called Substitutable and Complementary Relationships from Multi-behavior Data (denoted as SCRM) to better explore the relationships between products for effective recommendation. Specifically, we firstly construct substitutable and complementary graphs based on a user's sequential behaviors in every session by jointly considering `click' and `purchase' behaviors. We then design a denoising network to remove false relationships, and further consider constraints on the two relationships via a particularly designed loss function. Exten
    
[^7]: WellFactor:使用综合嵌入医疗数据的患者分类方法

    WellFactor: Patient Profiling using Integrative Embedding of Healthcare Data. (arXiv:2312.14129v1 [cs.LG] CROSS LISTED)

    [http://arxiv.org/abs/2312.14129](http://arxiv.org/abs/2312.14129)

    WellFactor是一种使用综合嵌入医疗数据的患者分类方法，并通过使用受约束的低秩逼近、结合标签信息来优化嵌入结果，同时具有即时计算新数据嵌入的特点。在实际医疗数据上得到了验证。

    

    在快速发展的医疗行业中，平台现在不仅可以访问传统的医疗记录，还可以获取涵盖各种患者互动的各种数据集，例如来自医疗网站的数据。为了解决这种丰富的数据多样性，我们引入了WellFactor：一种通过整合这些来源信息来得出患者分类的方法。我们方法的核心是利用受约束的低秩逼近。WellFactor被优化为处理医疗数据中经常存在的稀疏性。此外，通过结合特定任务的标签信息，我们的方法改进了嵌入结果，提供了更加明智的患者视角。WellFactor的一个重要特点是能够即时计算新的、以前未观察到的患者数据的嵌入，消除了重新访问整个数据集或重新计算嵌入的需要。对实际医疗数据进行全面评估证明了WellFactor的有效性。

    In the rapidly evolving healthcare industry, platforms now have access to not only traditional medical records, but also diverse data sets encompassing various patient interactions, such as those from healthcare web portals. To address this rich diversity of data, we introduce WellFactor: a method that derives patient profiles by integrating information from these sources. Central to our approach is the utilization of constrained low-rank approximation. WellFactor is optimized to handle the sparsity that is often inherent in healthcare data. Moreover, by incorporating task-specific label information, our method refines the embedding results, offering a more informed perspective on patients. One important feature of WellFactor is its ability to compute embeddings for new, previously unobserved patient data instantaneously, eliminating the need to revisit the entire data set or recomputing the embedding. Comprehensive evaluations on real-world healthcare data demonstrate WellFactor's eff
    
[^8]: 工程设计知识的语言和结构基础

    Linguistic and Structural Basis of Engineering Design Knowledge. (arXiv:2312.06355v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2312.06355](http://arxiv.org/abs/2312.06355)

    本文通过分析33881份专利文件的样本，将工程设计知识阐释为知识图谱，从而揭示工程设计知识的语言和结构基础。

    

    物品描述是工程设计知识的主要载体，既是设计过程的产物，也是驱动设计过程的因素。尽管物品可以以不同的内涵进行描述，但设计过程需要一种描述来体现工程设计知识，这通过实体和关系的复杂安排在文本中表现出来。虽然大型语言模型可以从各种文本中学习，但它们尚未生成体现明确的工程设计事实的文本。现有的本体论设计理论很少能指导目前仅限于构思和学习目的的大型语言模型的应用。本文从33881份专利文件的大样本中将工程设计知识阐释为知识图谱。我们研究这些知识图谱的组成部分，以理解工程设计知识的语言和结构基础。

    Artefact descriptions are the primary carriers of engineering design knowledge that is both an outcome and a driver of the design process. While an artefact could be described in different connotations, the design process requires a description to embody engineering design knowledge, which is expressed in the text through intricate placement of entities and relationships. As large-language models learn from all kinds of text merely as a sequence of characters/tokens, these are yet to generate text that embodies explicit engineering design facts. Existing ontological design theories are less likely to guide the large-language models whose applications are currently limited to ideation and learning purposes. In this article, we explicate engineering design knowledge as knowledge graphs from a large sample of 33,881 patent documents. We examine the constituents of these knowledge graphs to understand the linguistic and structural basis of engineering design knowledge. In terms of linguist
    
[^9]: 多颗粒度感知的多方面学习模型用于多方面稠密检索

    A Multi-Granularity-Aware Aspect Learning Model for Multi-Aspect Dense Retrieval. (arXiv:2312.02538v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2312.02538](http://arxiv.org/abs/2312.02538)

    这项研究提出了一种多颗粒度感知的多方面学习模型，用于解决结构化数据的稠密检索问题。与现有方法不同的是，该模型不仅考虑了项目方面值之间的细粒度语义关系，还避免了将方面学习集中到CLS令牌或仅通过值预测目标学习方面嵌入的问题。

    

    稠密检索方法主要关注非结构化文本，对于具有不同方面的结构化数据(例如，具有类别和品牌等方面的产品)的关注较少。最近的研究提出了两种方法来将方面信息合并到项目表示中，以通过预测与项目方面相关联的值实现有效检索。尽管这些方法非常有效，但它们将值视为孤立的类别(例如，“智能家居”、“家居、花园和工具”和“美容和健康”)，忽略了它们之间的细粒度语义关系。此外，它们要么强迫将方面的学习集中到CLS令牌中，这可能会混淆其作为表达整个内容语义的指定用途，要么仅通过值预测目标学习额外的方面嵌入，这在没有为项目方面注释的值时可能不足够。鉴于这些限制，我们提出了一种多颗粒度感知的多方面学习模型（MUlti-granulaRity-aware Aspect Learning model）

    Dense retrieval methods have been mostly focused on unstructured text and less attention has been drawn to structured data with various aspects, e.g., products with aspects such as category and brand. Recent work has proposed two approaches to incorporate the aspect information into item representations for effective retrieval by predicting the values associated with the item aspects. Despite their efficacy, they treat the values as isolated classes (e.g., "Smart Homes", "Home, Garden & Tools", and "Beauty & Health") and ignore their fine-grained semantic relation. Furthermore, they either enforce the learning of aspects into the CLS token, which could confuse it from its designated use for representing the entire content semantics, or learn extra aspect embeddings only with the value prediction objective, which could be insufficient especially when there are no annotated values for an item aspect. Aware of these limitations, we propose a MUlti-granulaRity-aware Aspect Learning model (
    
[^10]: TSRankLLM: 一种用于文本排序的两阶段LLM适应方法

    TSRankLLM: A Two-Stage Adaptation of LLMs for Text Ranking. (arXiv:2311.16720v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2311.16720](http://arxiv.org/abs/2311.16720)

    TSRankLLM提出了一种两阶段适应方法用于文本排序，通过连续预训练和改进的优化策略，实现了更好的性能。

    

    文本排序是各种信息检索应用中的关键任务，最近预训练语言模型（PLMs），特别是大型语言模型（LLMs）的成功引起了人们对其在文本排序中的应用的兴趣。为了消除PLMs和文本排序之间的不匹配问题，许多学者已经广泛探索了使用有监督排序数据进行微调的方法。然而，以前的研究主要集中在仅编码器和编码器-解码器PLMs上，缺乏对仅解码器LLM的研究。一个例外是RankLLaMA，它建议直接使用有监督的微调（SFT）来全面探索LLaMA。在我们的工作中，我们认为采用两阶段渐进范式会更有益。首先，我们建议使用大规模弱监督语料库对LLMs进行连续预训练（CPT）。其次，我们执行与RankLLaMA一致的SFT，并进一步提出了改进的优化策略。我们在多个基准测试上的实验结果表明我们方法具有卓越的性能。

    Text ranking is a critical task in various information retrieval applications, and the recent success of pre-trained language models (PLMs), especially large language models (LLMs), has sparked interest in their application to text ranking. To eliminate the misalignment between PLMs and text ranking, fine-tuning with supervised ranking data has been widely explored. However, previous studies focus mainly on encoder-only and encoder-decoder PLMs, and decoder-only LLM research is still lacking. An exception to this is RankLLaMA, which suggests direct supervised fine-tuning (SFT) to explore LLaMA fully. In our work, we argue that a two-stage progressive paradigm would be more beneficial. First, we suggest continual pre-training (CPT) on LLMs by using a large-scale weakly-supervised corpus. Second, we perform SFT consistent with RankLLaMA, and propose an improved optimization strategy further. Our experimental results on multiple benchmarks demonstrate the superior performance of our metho
    
[^11]: GraphPro: 面向推荐系统的图预训练和提示学习

    GraphPro: Graph Pre-training and Prompt Learning for Recommendation. (arXiv:2311.16716v3 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2311.16716](http://arxiv.org/abs/2311.16716)

    GraphPro是一个结合了参数高效和动态图预训练与提示学习的框架，能够有效捕捉长期用户偏好和短期行为动态，从而在真实世界的推荐系统中提供准确和及时的推荐。

    

    基于GNN的推荐系统通过多次消息传递在建模复杂的用户-物品交互方面表现出色。然而，现有方法往往忽视了不断变化的用户-物品交互的动态性，这限制了其在适应用户偏好变化和新到达数据分布变化方面的可扩展性和性能。因此，它们在真实世界的动态环境中的可扩展性和性能受到了限制。在这项研究中，我们提出了GraphPro，这是一个将参数高效和动态图预训练与提示学习相结合的框架。这种新颖的组合能够有效捕捉长期用户偏好和短期行为动态，从而实现准确和及时的推荐。我们的GraphPro框架通过无缝集成临时提示机制和图结构提示学习机制到预训练的GNN模型中来解决用户偏好不断变化的挑战。

    GNN-based recommenders have excelled in modeling intricate user-item interactions through multi-hop message passing. However, existing methods often overlook the dynamic nature of evolving user-item interactions, which impedes the adaption to changing user preferences and distribution shifts in newly arriving data. Thus, their scalability and performances in real-world dynamic environments are limited. In this study, we propose GraphPro, a framework that incorporates parameter-efficient and dynamic graph pre-training with prompt learning. This novel combination empowers GNNs to effectively capture both long-term user preferences and short-term behavior dynamics, enabling the delivery of accurate and timely recommendations. Our GraphPro framework addresses the challenge of evolving user preferences by seamlessly integrating a temporal prompt mechanism and a graph-structural prompt learning mechanism into the pre-trained GNN model. The temporal prompt mechanism encodes time information o
    
[^12]: 由AI生成的图像对文本-图像检索引入了不可见的相关偏差

    AI-Generated Images Introduce Invisible Relevance Bias to Text-Image Retrieval. (arXiv:2311.14084v3 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2311.14084](http://arxiv.org/abs/2311.14084)

    本研究通过构建合适的基准和进行大量实验发现，由AI生成的图像对于文本-图像检索模型引入了一个不可见的相关偏差，即使这些生成的图像在视觉上与查询的相关特征相比并没有更多。这种不可见的相关偏差普遍存在于不同训练数据和架构的检索模型中。

    

    随着生成模型的进步，由人工智能生成的内容（AIGC）变得更加逼真，涌入互联网。最近的一项研究表明，这种现象导致了网络搜索中的源偏差。特别是，神经检索模型往往将生成的文本排名高于人工编写的文本。本文将这种偏差的研究扩展到跨模态检索。首先，我们成功构建了一个适合探索偏差存在的基准。随后，在这个基准上进行了大量实验，发现AI生成的图像对于文本-图像检索模型引入了一个不可见的相关偏差。具体来说，我们的实验表明，尽管AI生成的图像与查询相比没有更多的视觉相关特征，但文本-图像检索模型往往将AI生成的图像排名高于真实图像。这种不可见的相关偏差在不同训练数据和架构的检索模型中普遍存在。

    With the advancement of generation models, AI-generated content (AIGC) is becoming more realistic, flooding the Internet. A recent study suggests that this phenomenon causes source bias in text retrieval for web search. Specifically, neural retrieval models tend to rank generated texts higher than human-written texts. In this paper, we extend the study of this bias to cross-modal retrieval. Firstly, we successfully construct a suitable benchmark to explore the existence of the bias. Subsequent extensive experiments on this benchmark reveal that AI-generated images introduce an invisible relevance bias to text-image retrieval models. Specifically, our experiments show that text-image retrieval models tend to rank the AI-generated images higher than the real images, even though the AI-generated images do not exhibit more visually relevant features to the query than real images. This invisible relevance bias is prevalent across retrieval models with varying training data and architectures
    
[^13]: 基于 Householder 量化的深度哈希

    Deep Hashing via Householder Quantization. (arXiv:2311.04207v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2311.04207](http://arxiv.org/abs/2311.04207)

    基于 Householder 量化的深度哈希提出了一种替代的量化策略，通过将学习问题分解为两个阶段来改善哈希方法中相似度学习和量化之间的交互，从而提高嵌入的质量。

    

    哈希是大规模图像相似性搜索的核心，最近的方法通过深度学习技术实现了显著的改进。这些算法通常学习数据的连续嵌入。为了避免后续昂贵的二值化步骤，常见的解决方案是采用将相似度学习项（确保相似的图像被分组到附近的嵌入中）和量化惩罚项（确保嵌入项接近二值化项，例如-1或1）结合的损失函数。然而，这两个项之间的相互作用可能使学习变得更困难，并且嵌入结果更差。我们提出了一种替代的量化策略，将学习问题分解为两个阶段：首先，不进行量化，在嵌入空间上进行相似度学习；其次，找到嵌入的最佳正交变换，使得嵌入的每个坐标都接近其符号，然后通过符号函数对变换后的嵌入进行量化。

    Hashing is at the heart of large-scale image similarity search, and recent methods have been substantially improved through deep learning techniques. Such algorithms typically learn continuous embeddings of the data. To avoid a subsequent costly binarization step, a common solution is to employ loss functions that combine a similarity learning term (to ensure similar images are grouped to nearby embeddings) and a quantization penalty term (to ensure that the embedding entries are close to binarized entries, e.g., -1 or 1). Still, the interaction between these two terms can make learning harder and the embeddings worse. We propose an alternative quantization strategy that decomposes the learning problem in two stages: first, perform similarity learning over the embedding space with no quantization; second, find an optimal orthogonal transformation of the embeddings so each coordinate of the embedding is close to its sign, and then quantize the transformed embedding through the sign func
    
[^14]: LLM可能主导信息访问：神经检索器对LLM生成的文本存在偏见。

    LLMs may Dominate Information Access: Neural Retrievers are Biased Towards LLM-Generated Texts. (arXiv:2310.20501v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2310.20501](http://arxiv.org/abs/2310.20501)

    近期的研究发现，大型语言模型（LLMs）对信息检索系统产生了一种偏见，倾向于将LLM生成的文档排名较高。这种“来源偏见”可能对信息访问产生重大影响。

    

    最近，大型语言模型（LLMs）的出现在信息检索（IR）应用，尤其是在网络搜索方面，彻底改变了范式。由于其在生成类人文本方面的卓越能力，LLMs在互联网上创造了大量的文本。因此，LLMs时代的IR系统面临一个新的挑战：索引的文档不仅是由人类撰写的，而且还包括由LLMs自动生成的文档。这些LLM生成的文档如何影响IR系统是一个紧迫且尚未探索的问题。在这项工作中，我们在涉及人类编写和LLM生成的文本的不同IR模型的场景中进行了定量评估。令人惊讶的是，我们的研究结果表明，神经检索模型倾向于将LLM生成的文档排名较高。我们将这种神经检索模型对LLM生成文本的偏见称为“来源偏见”。此外，我们发现这种偏见不仅限于f方相当的情况，而且在分类任务上也存在。

    Recently, the emergence of large language models (LLMs) has revolutionized the paradigm of information retrieval (IR) applications, especially in web search. With their remarkable capabilities in generating human-like texts, LLMs have created enormous texts on the Internet. As a result, IR systems in the LLMs era are facing a new challenge: the indexed documents now are not only written by human beings but also automatically generated by the LLMs. How these LLM-generated documents influence the IR systems is a pressing and still unexplored question. In this work, we conduct a quantitative evaluation of different IR models in scenarios where both human-written and LLM-generated texts are involved. Surprisingly, our findings indicate that neural retrieval models tend to rank LLM-generated documents higher. We refer to this category of biases in neural retrieval models towards the LLM-generated text as the \textbf{source bias}. Moreover, we discover that this bias is not confined to the f
    
[^15]: 做我能做的，而不是我得到的。(arXiv:2306.10345v2 [cs.AI] UPDATED)

    Do as I can, not as I get. (arXiv:2306.10345v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2306.10345](http://arxiv.org/abs/2306.10345)

    该论文提出了一种名为TMR的模型，用于从模拟数据环境中挖掘有价值的信息。

    

    本文提出了一种名为TMR的模型，用于从模拟数据环境中挖掘有价值的信息。我们打算完成本文的投稿。

    This paper proposes a model called TMR to mine valuable information from simulated data environments. We intend to complete the submission of this paper.
    
[^16]: 改进串联推荐的鲁棒性和准确性: 伴随级联指导的对抗训练方法

    Towards More Robust and Accurate Sequential Recommendation with Cascade-guided Adversarial Training. (arXiv:2304.05492v1 [cs.IR])

    [http://arxiv.org/abs/2304.05492](http://arxiv.org/abs/2304.05492)

    本研究利用级联指导下的对抗训练方法，增强了串联推荐模型的鲁棒性和准确性，取得了比已有方法更好的结果。

    

    串联推荐模型是一种通过学习用户与物品间的时间顺序互动来进行推荐的模型，其已经在许多领域中展现出了良好的表现。然而，近期串联推荐模型的鲁棒性备受质疑。这种模型的两个特性使其容易受到攻击 - 在训练中会产生级联效应，在模型过度依赖时间信息的同时会忽略其他特征。为了解决这些问题，本文提出了一种针对串联推荐模型的级联指导下的对抗训练的方法。我们的方法利用串联建模中的内在级联效应，在训练过程中产生战略性的对抗性扰动来影响物品嵌入。在使用不同的公共数据集训练四种最先进的串联模型实验中，我们的训练方法产生了比现有方法更高的模型鲁棒性，并获得了更好的性能。

    Sequential recommendation models, models that learn from chronological user-item interactions, outperform traditional recommendation models in many settings. Despite the success of sequential recommendation models, their robustness has recently come into question. Two properties unique to the nature of sequential recommendation models may impair their robustness - the cascade effects induced during training and the model's tendency to rely too heavily on temporal information. To address these vulnerabilities, we propose Cascade-guided Adversarial training, a new adversarial training procedure that is specifically designed for sequential recommendation models. Our approach harnesses the intrinsic cascade effects present in sequential modeling to produce strategic adversarial perturbations to item embeddings during training. Experiments on training state-of-the-art sequential models on four public datasets from different domains show that our training approach produces superior model ran
    

