# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Streaming CTR Prediction: Rethinking Recommendation Task for Real-World Streaming Data.](http://arxiv.org/abs/2307.07509) | 这篇论文提出了一种针对实际应用中流数据的流式CTR预测任务，用于解决工业推荐系统中的挑战。研究将CTR预测从静态数据集中的分类任务转变为流式数据的预测任务，并提出了相应的基准设置和度量标准。 |
| [^2] | [PiTL: Cross-modal Retrieval with Weakly-supervised Vision-language Pre-training via Prompting.](http://arxiv.org/abs/2307.07341) | 本文提出了一个名为PiTL的方法，通过从大语言模型中提取的知识来描述图像，以实现弱监督视觉-语言预训练进行跨模态检索。这种方法可以减少对图像-文本对的需求，并且可以使用对象检测器生成的对象标签作为监督。 |
| [^3] | [NS4AR: A new, focused on sampling areas sampling method in graphical recommendation Systems.](http://arxiv.org/abs/2307.07321) | 本文提出了一种新的图形推荐系统采样方法，通过对样本区域进行划分并使用AdaSim对区域赋予不同的权重，形成正负样本集，并提出了一个子集选择模型来缩小核心负样本的数量。 |
| [^4] | [Hybrid moderation in the newsroom: Recommending featured posts to content moderators.](http://arxiv.org/abs/2307.07317) | 本文提出了一种在新闻编辑室中使用的混合审核方法，该方法通过向内容管理员推荐特色帖子来支持他们在选择特色内容方面做出决策。该方法基于概率排序的推荐系统，结合了用户和文本内容特征，取得了较高的分类和排序性能。内容管理员在评估中发现了合适的评论，并在很大程度上接受了推荐结果。 |
| [^5] | [Learning to Retrieve In-Context Examples for Large Language Models.](http://arxiv.org/abs/2307.07164) | 本文提出了一个新颖的框架，通过迭代训练密集检索器来为大型语言模型识别高质量的上下文示例，从而显著提高了上下文学习性能，并展示了在训练期间对未见过任务的泛化能力。 |
| [^6] | [Digital Health Discussion Through Articles Published Until the Year 2021: A Digital Topic Modeling Approach.](http://arxiv.org/abs/2307.07130) | 这里是中文总结出的一句话要点 |
| [^7] | [Making the Most Out of the Limited Context Length: Predictive Power Varies with Clinical Note Type and Note Section.](http://arxiv.org/abs/2307.07051) | 通过分析临床记录的部分，我们发现预测能力在不同类型的记录间存在差异，并且当上下文长度较大时，组合不同类型的记录可以改善性能。我们的研究结果表明，精心选择的采样函数可以使从临床记录中提取信息更加高效。 |
| [^8] | [Towards Populating Generalizable Engineering Design Knowledge.](http://arxiv.org/abs/2307.06985) | 这项研究提出了一种从专利文件中提取工程设计知识的方法，通过构建知识图来填充通用设计知识，并与现有方法进行了比较。 |

# 详细

[^1]: 流式CTR预测：重新思考实际应用中的推荐任务的真实世界流数据

    Streaming CTR Prediction: Rethinking Recommendation Task for Real-World Streaming Data. (arXiv:2307.07509v1 [cs.IR])

    [http://arxiv.org/abs/2307.07509](http://arxiv.org/abs/2307.07509)

    这篇论文提出了一种针对实际应用中流数据的流式CTR预测任务，用于解决工业推荐系统中的挑战。研究将CTR预测从静态数据集中的分类任务转变为流式数据的预测任务，并提出了相应的基准设置和度量标准。

    

    点击率（CTR）预测任务在工业推荐系统中至关重要，其中模型通常在实际应用中部署在动态流数据上。实际推荐系统中的这些流数据面临着许多挑战，例如分布偏移、时间非平稳性和系统偏差，这给推荐模型的训练和利用带来了困难。然而，大多数现有的研究将CTR预测视为静态数据集上的分类任务，假设训练集和测试集是独立且同分布的（即i.i.d.假设）。为了填补这一差距，我们提出了在流式数据情景下将CTR预测问题形式化为流式CTR预测任务。相应地，我们提出了专门的基准设置和度量标准，以评估和分析模型在流数据上的性能。为了更好地理解与传统CTR预测任务的差异，我们深入研究了影响流数据中CTR预测任务的因素。

    The Click-Through Rate (CTR) prediction task is critical in industrial recommender systems, where models are usually deployed on dynamic streaming data in practical applications. Such streaming data in real-world recommender systems face many challenges, such as distribution shift, temporal non-stationarity, and systematic biases, which bring difficulties to the training and utilizing of recommendation models. However, most existing studies approach the CTR prediction as a classification task on static datasets, assuming that the train and test sets are independent and identically distributed (a.k.a, i.i.d. assumption). To bridge this gap, we formulate the CTR prediction problem in streaming scenarios as a Streaming CTR Prediction task. Accordingly, we propose dedicated benchmark settings and metrics to evaluate and analyze the performance of the models in streaming data. To better understand the differences compared to traditional CTR prediction tasks, we delve into the factors that m
    
[^2]: PiTL: 通过提示进行弱监督视觉-语言预训练进行跨模态检索

    PiTL: Cross-modal Retrieval with Weakly-supervised Vision-language Pre-training via Prompting. (arXiv:2307.07341v1 [cs.IR])

    [http://arxiv.org/abs/2307.07341](http://arxiv.org/abs/2307.07341)

    本文提出了一个名为PiTL的方法，通过从大语言模型中提取的知识来描述图像，以实现弱监督视觉-语言预训练进行跨模态检索。这种方法可以减少对图像-文本对的需求，并且可以使用对象检测器生成的对象标签作为监督。

    

    视觉-语言预训练（VLP）已经证明可以很好地在各种视觉-语言下游任务中推广VLP模型，特别是用于跨模态检索。然而，它依赖于大量的图像-文本对，这需要繁琐和昂贵的策划。相反，弱监督VLP（W-VLP）利用预训练的对象检测器（OD）从图像中生成的对象标签来探索方法。然而，它们仍然需要配对的信息，即图像和对象级注释，作为训练OD的监督。为了进一步减少监督的数量，我们提出了在大语言模型（LLM）中提示知识来描述图像的"PiTL"。具体来说，给定一个图像的类别标签，例如炼油厂，由LLM提取的知识，例如炼油厂可以带有大型储罐、管道等，用作语言方面的对应物。这些知识补充了最有可能出现在场景中的实体之间的共同关系。

    Vision-language (VL) Pre-training (VLP) has shown to well generalize VL models over a wide range of VL downstream tasks, especially for cross-modal retrieval. However, it hinges on a huge amount of image-text pairs, which requires tedious and costly curation. On the contrary, weakly-supervised VLP (W-VLP) explores means with object tags generated by a pre-trained object detector (OD) from images. Yet, they still require paired information, i.e. images and object-level annotations, as supervision to train an OD.  To further reduce the amount of supervision, we propose Prompts-in-The-Loop (PiTL) that prompts knowledge from large language models (LLMs) to describe images. Concretely, given a category label of an image, e.g. refinery, the knowledge, e.g. a refinery could be seen with large storage tanks, pipework, and ..., extracted by LLMs is used as the language counterpart. The knowledge supplements, e.g. the common relations among entities most likely appearing in a scene. We create IN
    
[^3]: NS4AR: 一种新的、专注于采样区域的图形推荐系统采样方法

    NS4AR: A new, focused on sampling areas sampling method in graphical recommendation Systems. (arXiv:2307.07321v1 [cs.IR])

    [http://arxiv.org/abs/2307.07321](http://arxiv.org/abs/2307.07321)

    本文提出了一种新的图形推荐系统采样方法，通过对样本区域进行划分并使用AdaSim对区域赋予不同的权重，形成正负样本集，并提出了一个子集选择模型来缩小核心负样本的数量。

    

    图形推荐系统的有效性取决于负采样的数量和质量。本文选择了一些典型的推荐系统模型，并将这些模型上的一些最新的负采样策略作为基线。基于典型的图形推荐模型，我们将样本区域划分为指定的n个区域，并使用AdaSim对这些区域赋予不同的权重，形成正样本集和负样本集。由于负样本的数量和重要性，我们还提出了一个子集选择模型来缩小核心负样本。

    The effectiveness of graphical recommender system depends on the quantity and quality of negative sampling. This paper selects some typical recommender system models, as well as some latest negative sampling strategies on the models as baseline. Based on typical graphical recommender model, we divide sample region into assigned-n areas and use AdaSim to give different weight to these areas to form positive set and negative set. Because of the volume and significance of negative items, we also proposed a subset selection model to narrow the core negative samples.
    
[^4]: 新闻编辑室中的混合审核：向内容管理员推荐特色帖子

    Hybrid moderation in the newsroom: Recommending featured posts to content moderators. (arXiv:2307.07317v1 [cs.IR])

    [http://arxiv.org/abs/2307.07317](http://arxiv.org/abs/2307.07317)

    本文提出了一种在新闻编辑室中使用的混合审核方法，该方法通过向内容管理员推荐特色帖子来支持他们在选择特色内容方面做出决策。该方法基于概率排序的推荐系统，结合了用户和文本内容特征，取得了较高的分类和排序性能。内容管理员在评估中发现了合适的评论，并在很大程度上接受了推荐结果。

    

    在线新闻媒体正努力处理评论区用户生成内容的审核问题。我们提出了一种基于概率排序的推荐系统来支持和授权审核员选择特色帖子，这是一项耗时的任务。通过结合用户和文本内容的特征，我们获得了测试集上的最佳分类F1分数为0.44。此外，我们在大量验证文章上观察到了均值NDCG@5的最佳值为0.87。在专家评估中，内容管理员根据推荐结果选择要推荐的评论，得到了0.83的NDCG分数。我们得出的结论是，首先，添加文本特征可以获得最佳得分；其次，虽然选择特色内容仍然有一定的主观性，但内容管理员在所有被评估的推荐中都找到了合适的评论，除了一个例外。最后，我们通过分析表现最佳的模型，迈向透明和可解释性。

    Online news outlets are grappling with the moderation of user-generated content within their comment section. We present a recommender system based on ranking class probabilities to support and empower the moderator in choosing featured posts, a time-consuming task. By combining user and textual content features we obtain an optimal classification F1-score of 0.44 on the test set. Furthermore, we observe an optimum mean NDCG@5 of 0.87 on a large set of validation articles. As an expert evaluation, content moderators assessed the output of a random selection of articles by choosing comments to feature based on the recommendations, which resulted in a NDCG score of 0.83. We conclude that first, adding text features yields the best score and second, while choosing featured content remains somewhat subjective, content moderators found suitable comments in all but one evaluated recommendations. We end the paper by analyzing our best-performing model, a step towards transparency and explaina
    
[^5]: 学习为大型语言模型检索上下文示例

    Learning to Retrieve In-Context Examples for Large Language Models. (arXiv:2307.07164v1 [cs.CL])

    [http://arxiv.org/abs/2307.07164](http://arxiv.org/abs/2307.07164)

    本文提出了一个新颖的框架，通过迭代训练密集检索器来为大型语言模型识别高质量的上下文示例，从而显著提高了上下文学习性能，并展示了在训练期间对未见过任务的泛化能力。

    

    大型语言模型（LLMs）展示了它们在上下文中学习的能力，使它们能够根据少量的输入-输出示例执行各种任务。然而，上下文学习的有效性在很大程度上依赖于所选示例的质量。在本文中，我们提出了一个新颖的框架，通过迭代训练密集检索器，可以为LLMs识别高质量的上下文示例。我们的框架首先训练基于LLM反馈的奖励模型来评估候选示例的质量，然后通过知识蒸馏训练基于双编码器的密集检索器。我们在30个任务套件上的实验证明，我们的框架显著提高了上下文学习性能。此外，我们还展示了我们的框架在训练期间对未见过任务的泛化能力。深入分析表明，我们的模型通过检索具有相似模式的示例来提高性能，而这种增益在不同规模的LLMs中是一致的。

    Large language models (LLMs) have demonstrated their ability to learn in-context, allowing them to perform various tasks based on a few input-output examples. However, the effectiveness of in-context learning is heavily reliant on the quality of the selected examples. In this paper, we propose a novel framework to iteratively train dense retrievers that can identify high-quality in-context examples for LLMs. Our framework initially trains a reward model based on LLM feedback to evaluate the quality of candidate examples, followed by knowledge distillation to train a bi-encoder based dense retriever. Our experiments on a suite of 30 tasks demonstrate that our framework significantly enhances in-context learning performance. Furthermore, we show the generalization ability of our framework to unseen tasks during training. An in-depth analysis reveals that our model improves performance by retrieving examples with similar patterns, and the gains are consistent across LLMs of varying sizes.
    
[^6]: 这里是翻译过的论文标题

    Digital Health Discussion Through Articles Published Until the Year 2021: A Digital Topic Modeling Approach. (arXiv:2307.07130v1 [stat.AP])

    [http://arxiv.org/abs/2307.07130](http://arxiv.org/abs/2307.07130)

    这里是中文总结出的一句话要点

    

    这里是翻译过的论文摘要

    The digital health industry has grown in popularity since the 2010s, but there has been limited analysis of the topics discussed in the field across academic disciplines. This study aims to analyze the research trends of digital health-related articles published on the Web of Science until 2021, in order to understand the concentration, scope, and characteristics of the research. 15,950 digital health-related papers from the top 10 academic fields were analyzed using the Web of Science. The papers were grouped into three domains: public health, medicine, and electrical engineering and computer science (EECS). Two time periods (2012-2016 and 2017-2021) were compared using Latent Dirichlet Allocation (LDA) for topic modeling. The number of topics was determined based on coherence score, and topic compositions were compared using a homogeneity test. The number of optimal topics varied across domains and time periods. For public health, the first and second halves had 13 and 19 topics, res
    
[^7]: 充分利用有限的上下文长度：预测能力随临床记录类型和记录部分的不同而变化

    Making the Most Out of the Limited Context Length: Predictive Power Varies with Clinical Note Type and Note Section. (arXiv:2307.07051v1 [cs.CL])

    [http://arxiv.org/abs/2307.07051](http://arxiv.org/abs/2307.07051)

    通过分析临床记录的部分，我们发现预测能力在不同类型的记录间存在差异，并且当上下文长度较大时，组合不同类型的记录可以改善性能。我们的研究结果表明，精心选择的采样函数可以使从临床记录中提取信息更加高效。

    

    最近大规模语言模型的进展使得使用临床记录的自由文本进行自然语言处理的兴趣重新燃起。临床记录的一个区别特点是它们跨越多个长文档的长时间跨度。临床记录的独特结构带来了一个新的设计选择：当语言模型预测器的上下文长度有限时，应选择临床记录的哪个部分作为输入？现有研究要么选择具有领域知识的输入，要么简单地截断它们。我们提出了一个分析高预测能力部分的框架。使用MIMIC-III数据集，我们展示了以下发现：1）预测能力分布在护理记录和出院记录之间是不同的；2）当上下文长度较大时，组合不同类型的记录可以提高性能。我们的研究结果表明，精心选择的采样函数可以使从临床记录中提取信息更加高效。

    Recent advances in large language models have led to renewed interest in natural language processing in healthcare using the free text of clinical notes. One distinguishing characteristic of clinical notes is their long time span over multiple long documents. The unique structure of clinical notes creates a new design choice: when the context length for a language model predictor is limited, which part of clinical notes should we choose as the input? Existing studies either choose the inputs with domain knowledge or simply truncate them. We propose a framework to analyze the sections with high predictive power. Using MIMIC-III, we show that: 1) predictive power distribution is different between nursing notes and discharge notes and 2) combining different types of notes could improve performance when the context length is large. Our findings suggest that a carefully selected sampling function could enable more efficient information extraction from clinical notes.
    
[^8]: 迈向填充通用工程设计知识的方法

    Towards Populating Generalizable Engineering Design Knowledge. (arXiv:2307.06985v1 [cs.CL])

    [http://arxiv.org/abs/2307.06985](http://arxiv.org/abs/2307.06985)

    这项研究提出了一种从专利文件中提取工程设计知识的方法，通过构建知识图来填充通用设计知识，并与现有方法进行了比较。

    

    为了填充通用工程设计知识，我们提出了一种从专利文件中提取head entity :: relationship :: tail entity形式事实的方法。这些事实可以在专利文件内部和跨文件之间组合形成知识图，用作表示和存储设计知识的方案。现有的工程设计文献中的方法通常利用一组预定义的关系来填充统计近似而非事实的三元组。在我们的方法中，我们训练一个标记器来识别句子中的实体和关系。在确定了一对实体后，我们训练另一个标记器来识别特定表示这对实体之间关系的关系标记。为了训练这些标记器，我们手动构建了一个包含44,227个句子和相应事实的数据集。我们还将该方法的性能与通常推荐的方法进行了比较，其中我们预.

    Aiming to populate generalizable engineering design knowledge, we propose a method to extract facts of the form head entity :: relationship :: tail entity from sentences found in patent documents. These facts could be combined within and across patent documents to form knowledge graphs that serve as schemes for representing as well as storing design knowledge. Existing methods in engineering design literature often utilise a set of predefined relationships to populate triples that are statistical approximations rather than facts. In our method, we train a tagger to identify both entities and relationships from a sentence. Given a pair of entities thus identified, we train another tagger to identify the relationship tokens that specifically denote the relationship between the pair. For training these taggers, we manually construct a dataset of 44,227 sentences and corresponding facts. We also compare the performance of the method against typically recommended approaches, wherein, we pre
    

