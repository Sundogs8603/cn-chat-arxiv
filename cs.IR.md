# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Crafting Knowledge: Exploring the Creative Mechanisms of Chat-Based Search Engines](https://arxiv.org/abs/2402.19421) | 探索基于聊天的搜索引擎的创造性机制，并解析了大型语言模型如何选择信息源以生成人类一样理解性和创造性的响应。 |
| [^2] | [PaECTER: Patent-level Representation Learning using Citation-informed Transformers](https://arxiv.org/abs/2402.19411) | PaECTER是一个专为专利设计的开放源码文档级编码器，利用引文信息对BERT进行微调，生成专利文档的数值表示，并在专利领域的相似性任务中表现优异。 |
| [^3] | [MENTOR: Multi-level Self-supervised Learning for Multimodal Recommendation](https://arxiv.org/abs/2402.19407) | MENTOR提出了一种多级自监督学习方法，用于解决多模态推荐中的标签稀疏和模态对齐问题。 |
| [^4] | [OpenMedLM: Prompt engineering can out-perform fine-tuning in medical question-answering with open-source large language models](https://arxiv.org/abs/2402.19371) | OpenMedLM 提出了一个提示平台，利用提示工程在医学问答中能够超越对开源大型语言模型进行微调，实现了在医学基准上的 SOTA 性能。 |
| [^5] | [A SOUND APPROACH: Using Large Language Models to generate audio descriptions for egocentric text-audio retrieval](https://arxiv.org/abs/2402.19106) | 使用大型语言模型生成音频描述的方法在自我中心视频设置下的文本-音频检索任务中表现出更高的零样本性能。 |
| [^6] | [Effective Two-Stage Knowledge Transfer for Multi-Entity Cross-Domain Recommendation](https://arxiv.org/abs/2402.19101) | 该研究提出了一种有效的两阶段跨实体跨域推荐知识传输方法，解决了多实体推荐中源实体数据分布不同和特征模式不对齐等重要问题。 |
| [^7] | [Stop Relying on No-Choice and Do not Repeat the Moves: Optimal, Efficient and Practical Algorithms for Assortment Optimization](https://arxiv.org/abs/2402.18917) | 针对带偏好反馈的积极在线分类优化问题，提出了一种既高效又具有最优后悔保证的算法。 |
| [^8] | [Aligning Language Models for Versatile Text-based Item Retrieval](https://arxiv.org/abs/2402.18899) | 本文针对通用文本嵌入模型与项目检索任务之间的差距，通过生成特定领域数据集和微调嵌入模型，实现了在各种检索任务中显着改进性能，同时在会话设置下展示了模型的实际应用。 |
| [^9] | [Exploring the Impact of Large Language Models on Recommender Systems: An Extensive Review](https://arxiv.org/abs/2402.18590) | 大型语言模型在推荐系统中展现出卓越的推荐能力和语言理解，重塑了推荐任务的基础。 |
| [^10] | [Verif.ai: Towards an Open-Source Scientific Generative Question-Answering System with Referenced and Verifiable Answers](https://arxiv.org/abs/2402.18589) | Verif.ai是一个具有引用和可验证答案的开源科学生成式问答系统，通过信息检索、生成模型和验证引擎的结合实现对主张的生成和验证。 |
| [^11] | [DiFashion: Towards Personalized Outfit Generation](https://arxiv.org/abs/2402.17279) | 引入生成式服装推荐任务（GOR），旨在合成一组时尚图片并组装成视觉和谐的、定制给个人用户的服装。 |
| [^12] | [Pattern-wise Transparent Sequential Recommendation](https://arxiv.org/abs/2402.11480) | 提出了一种模式透明的顺序推荐框架，通过将项目序列分解为多级模式并在概率空间中量化每个模式对结果的贡献，实现了透明的决策过程。 |
| [^13] | [Rethinking Cross-Domain Sequential Recommendation under Open-World Assumptions](https://arxiv.org/abs/2311.04590) | 提出了一个适用于跨领域序列推荐的自适应多兴趣去偏见框架（AMID），在开放世界假设下设计，旨在解决现有方法在在线真实平台上由于数据分布转移导致性能下降的问题。 |
| [^14] | [Budgeted Embedding Table For Recommender Systems](https://arxiv.org/abs/2310.14884) | 提出了一种预算嵌入表的方法，解决了传统推荐系统中固定嵌入大小难以扩展的问题，能够有效应对不同用户和项目的多样性嵌入大小。 |
| [^15] | [ReLLa: Retrieval-enhanced Large Language Models for Lifelong Sequential Behavior Comprehension in Recommendation.](http://arxiv.org/abs/2308.11131) | 本论文提出了一种名为ReLLa的检索增强大型语言模型框架，用于零样本和小样本推荐任务。通过语义用户行为检索（SUBR）来提取上下文中的有用信息，以改善LLMs的推荐性能。 |
| [^16] | [web crawler strategies for web pages under robot.txt restriction.](http://arxiv.org/abs/2308.04689) | 本文研究了在robot.txt限制下的网络爬虫策略，讨论了搜索引擎如何确定网页排名以及如何获取数据库中的网页。并介绍了机器人排除协议规则和robot.txt文件的基本格式。 |

# 详细

[^1]: 知识塑造：探索基于聊天的搜索引擎的创造性机制

    Crafting Knowledge: Exploring the Creative Mechanisms of Chat-Based Search Engines

    [https://arxiv.org/abs/2402.19421](https://arxiv.org/abs/2402.19421)

    探索基于聊天的搜索引擎的创造性机制，并解析了大型语言模型如何选择信息源以生成人类一样理解性和创造性的响应。

    

    在数字信息传播领域，搜索引擎扮演着关键的角色，连接信息寻找者和信息提供者。采用大型语言模型和检索增强生成技术的基于聊天的搜索引擎的出现，例如必应聊天，标志着搜索生态系统的进化飞跃。它们展示了元认知能力，能够解释网络信息并具有类似于人类的理解和创造力。然而，大型语言模型的复杂性使得它们的“认知”过程变得不透明，甚至挑战了设计师对其理解。本研究旨在解剖一个基于大型语言模型的基于聊天的搜索引擎（具体为必应聊天）选择信息源以作为其响应的机制。为此，通过与新版必应的互动，编制了一个庞大的数据集，记录了它引用的网站以及传统搜索引擎列出的网站。

    arXiv:2402.19421v1 Announce Type: cross  Abstract: In the domain of digital information dissemination, search engines act as pivotal conduits linking information seekers with providers. The advent of chat-based search engines utilizing Large Language Models (LLMs) and Retrieval Augmented Generation (RAG), exemplified by Bing Chat, marks an evolutionary leap in the search ecosystem. They demonstrate metacognitive abilities in interpreting web information and crafting responses with human-like understanding and creativity. Nonetheless, the intricate nature of LLMs renders their "cognitive" processes opaque, challenging even their designers' understanding. This research aims to dissect the mechanisms through which an LLM-powered chat-based search engine, specifically Bing Chat, selects information sources for its responses. To this end, an extensive dataset has been compiled through engagements with New Bing, documenting the websites it cites alongside those listed by the conventional sea
    
[^2]: PaECTER：使用引文信息的专利级表示学习

    PaECTER: Patent-level Representation Learning using Citation-informed Transformers

    [https://arxiv.org/abs/2402.19411](https://arxiv.org/abs/2402.19411)

    PaECTER是一个专为专利设计的开放源码文档级编码器，利用引文信息对BERT进行微调，生成专利文档的数值表示，并在专利领域的相似性任务中表现优异。

    

    PaECTER是一个公开可用的、面向专利的文档级编码器，我们利用审核员添加的引文信息对BERT进行微调，为专利文档生成数值表示。与专利领域中当前最先进的模型相比，PaECTER在相似性任务中表现更好。具体来说，我们的模型在专利引文预测测试数据集上两种不同的排名评估指标上均优于下一个最佳专利特定的预训练语言模型（专利BERT）。与25个不相关的专利相比，PaECTER在平均排名1.32处预测到至少一个最相似的专利。PaECTER从专利文本生成的数值表示可用于分类、追踪知识流动或语义相似性搜索等下游任务。语义相似性搜索在发明人和专利的先前技术搜索背景中尤为重要。

    arXiv:2402.19411v1 Announce Type: cross  Abstract: PaECTER is a publicly available, open-source document-level encoder specific for patents. We fine-tune BERT for Patents with examiner-added citation information to generate numerical representations for patent documents. PaECTER performs better in similarity tasks than current state-of-the-art models used in the patent domain. More specifically, our model outperforms the next-best patent specific pre-trained language model (BERT for Patents) on our patent citation prediction test dataset on two different rank evaluation metrics. PaECTER predicts at least one most similar patent at a rank of 1.32 on average when compared against 25 irrelevant patents. Numerical representations generated by PaECTER from patent text can be used for downstream tasks such as classification, tracing knowledge flows, or semantic similarity search. Semantic similarity search is especially relevant in the context of prior art search for both inventors and paten
    
[^3]: MENTOR：多级自监督学习用于多模态推荐

    MENTOR: Multi-level Self-supervised Learning for Multimodal Recommendation

    [https://arxiv.org/abs/2402.19407](https://arxiv.org/abs/2402.19407)

    MENTOR提出了一种多级自监督学习方法，用于解决多模态推荐中的标签稀疏和模态对齐问题。

    

    随着多媒体信息的不断增加，多模态推荐受到了广泛关注。它利用多模态信息来缓解推荐系统中的数据稀疏问题，从而提高推荐的准确性。然而，对标记数据的依赖严重限制了多模态推荐模型的性能。最近，自监督学习已被用于多模态推荐以减轻标签稀疏问题。然而，当前的方法在对齐多模态信息时无法避免模态噪声，因为不同模态的分布差异较大。为此，我们提出了一种用于解决标签稀疏问题和模态对齐问题的多级自监督学习用于多模态推荐（MENTOR）方法。

    arXiv:2402.19407v1 Announce Type: new  Abstract: With the increasing multimedia information, multimodal recommendation has received extensive attention. It utilizes multimodal information to alleviate the data sparsity problem in recommendation systems, thus improving recommendation accuracy. However, the reliance on labeled data severely limits the performance of multimodal recommendation models. Recently, self-supervised learning has been used in multimodal recommendations to mitigate the label sparsity problem. Nevertheless, the state-of-the-art methods cannot avoid the modality noise when aligning multimodal information due to the large differences in the distributions of different modalities. To this end, we propose a Multi-level sElf-supervised learNing for mulTimOdal Recommendation (MENTOR) method to address the label sparsity problem and the modality alignment problem. Specifically, MENTOR first enhances the specific features of each modality using the graph convolutional netwo
    
[^4]: OpenMedLM：在医学问答中，提示工程可以胜过对开源大型语言模型进行微调

    OpenMedLM: Prompt engineering can out-perform fine-tuning in medical question-answering with open-source large language models

    [https://arxiv.org/abs/2402.19371](https://arxiv.org/abs/2402.19371)

    OpenMedLM 提出了一个提示平台，利用提示工程在医学问答中能够超越对开源大型语言模型进行微调，实现了在医学基准上的 SOTA 性能。

    

    LLMs 在完成一系列专门任务方面变得越来越有能力，并且可以用来扩大对医学知识的公平访问。大多数医学 LLMs 都涉及大量微调，利用专门的医学数据和大量的计算资源，因此成本高昂。许多表现前列的 LLMs 是专有的，他们的访问仅限于少数研究团体。然而，开源（OS）模型代表了医学 LLMs 的一个重要增长领域，由于性能显著提升以及提供卫生保健所需的透明度和合规性的内在能力。我们提出了 OpenMedLM，这是一个提示平台，为医学基准上的 OS LLMs 提供了最先进的性能。我们在四个医学基准（MedQA、MedMCQA、PubMedQA、MMLU 医学子集）上评估了一系列 OS 基础 LLMs（7B-70B）。我们采用了一系列提示策略，包括零s

    arXiv:2402.19371v1 Announce Type: cross  Abstract: LLMs have become increasingly capable at accomplishing a range of specialized-tasks and can be utilized to expand equitable access to medical knowledge. Most medical LLMs have involved extensive fine-tuning, leveraging specialized medical data and significant, thus costly, amounts of computational power. Many of the top performing LLMs are proprietary and their access is limited to very few research groups. However, open-source (OS) models represent a key area of growth for medical LLMs due to significant improvements in performance and an inherent ability to provide the transparency and compliance required in healthcare. We present OpenMedLM, a prompting platform which delivers state-of-the-art (SOTA) performance for OS LLMs on medical benchmarks. We evaluated a range of OS foundation LLMs (7B-70B) on four medical benchmarks (MedQA, MedMCQA, PubMedQA, MMLU medical-subset). We employed a series of prompting strategies, including zero-s
    
[^5]: 一种声音方法：利用大型语言模型为自我中心文本 - 音频检索生成音频描述

    A SOUND APPROACH: Using Large Language Models to generate audio descriptions for egocentric text-audio retrieval

    [https://arxiv.org/abs/2402.19106](https://arxiv.org/abs/2402.19106)

    使用大型语言模型生成音频描述的方法在自我中心视频设置下的文本-音频检索任务中表现出更高的零样本性能。

    

    互联网视频数据库是文本-音频检索数据集的宝贵来源。然而，考虑到声音和视觉流代表数据的不同“视图”，将视觉描述视为音频描述远非最佳选择。即使存在音频类标签，它们通常也不太详细，使其不适用于文本-音频检索。为了利用视频文本数据集中的相关音频信息，我们引入了一种利用大型语言模型（LLM）生成以音频为中心描述的方法。在这项工作中，我们考虑了自我中心视频设置，并提出了基于EpicMIR和EgoMCQ任务以及EpicSounds数据集的三个新的文本-音频检索基准。我们获得音频为中心的描述的方法比使用原始视觉为中心的描述具有显着更高的零样本性能。此外，我们展示了使用相同提示，我们可以成功地

    arXiv:2402.19106v1 Announce Type: cross  Abstract: Video databases from the internet are a valuable source of text-audio retrieval datasets. However, given that sound and vision streams represent different "views" of the data, treating visual descriptions as audio descriptions is far from optimal. Even if audio class labels are present, they commonly are not very detailed, making them unsuited for text-audio retrieval. To exploit relevant audio information from video-text datasets, we introduce a methodology for generating audio-centric descriptions using Large Language Models (LLMs). In this work, we consider the egocentric video setting and propose three new text-audio retrieval benchmarks based on the EpicMIR and EgoMCQ tasks, and on the EpicSounds dataset. Our approach for obtaining audio-centric descriptions gives significantly higher zero-shot performance than using the original visual-centric descriptions. Furthermore, we show that using the same prompts, we can successfully emp
    
[^6]: 有效的两阶段跨实体跨域推荐知识传输

    Effective Two-Stage Knowledge Transfer for Multi-Entity Cross-Domain Recommendation

    [https://arxiv.org/abs/2402.19101](https://arxiv.org/abs/2402.19101)

    该研究提出了一种有效的两阶段跨实体跨域推荐知识传输方法，解决了多实体推荐中源实体数据分布不同和特征模式不对齐等重要问题。

    

    近年来，电子商务平台上的推荐内容变得越来越丰富 -- 单个用户反馈可能包含多个实体，如销售产品、短视频和内容帖子。为了解决多实体推荐问题，一个直观的解决方案是采用基于共享网络的架构进行联合训练。这一想法是将一个类型实体（源实体）中提取的知识传输到另一个类型实体（目标实体）中。

    arXiv:2402.19101v1 Announce Type: cross  Abstract: In recent years, the recommendation content on e-commerce platforms has become increasingly rich -- a single user feed may contain multiple entities, such as selling products, short videos, and content posts. To deal with the multi-entity recommendation problem, an intuitive solution is to adopt the shared-network-based architecture for joint training. The idea is to transfer the extracted knowledge from one type of entity (source entity) to another (target entity). However, different from the conventional same-entity cross-domain recommendation, multi-entity knowledge transfer encounters several important issues: (1) data distributions of the source entity and target entity are naturally different, making the shared-network-based joint training susceptible to the negative transfer issue, (2) more importantly, the corresponding feature schema of each entity is not exactly aligned (e.g., price is an essential feature for selling product
    
[^7]: 不要依赖无选择，不要重复移动：一种用于分类优化的最佳、高效和实用算法

    Stop Relying on No-Choice and Do not Repeat the Moves: Optimal, Efficient and Practical Algorithms for Assortment Optimization

    [https://arxiv.org/abs/2402.18917](https://arxiv.org/abs/2402.18917)

    针对带偏好反馈的积极在线分类优化问题，提出了一种既高效又具有最优后悔保证的算法。

    

    我们解决了带偏好反馈的积极在线分类优化问题，这是一种用于建模用户选择和子集效用最大化的框架。该框架在各种真实应用中非常有用，包括广告放置、在线零售、推荐系统、微调语言模型等。虽然这个问题过去已经被研究过，但缺乏直观和实用的解决方案，同时需要高效的算法和最优的后悔保证。例如，通常使用的分类选择算法通常需要存在一个始终包含在选择集中的“强参考项”，此外，它们还被设计为重复提供相同的分类，直到参考项被选择 — 所有这些要求对于实际应用而言都非常不现实。在本文中，我们为分类中的后悔最小化问题设计了高效的算法。

    arXiv:2402.18917v1 Announce Type: new  Abstract: We address the problem of active online assortment optimization problem with preference feedback, which is a framework for modeling user choices and subsetwise utility maximization. The framework is useful in various real-world applications including ad placement, online retail, recommender systems, fine-tuning language models, amongst many. The problem, although has been studied in the past, lacks an intuitive and practical solution approach with simultaneously efficient algorithm and optimal regret guarantee. E.g., popularly used assortment selection algorithms often require the presence of a `strong reference' which is always included in the choice sets, further they are also designed to offer the same assortments repeatedly until the reference item gets selected -- all such requirements are quite unrealistic for practical applications. In this paper, we designed efficient algorithms for the problem of regret minimization in assortmen
    
[^8]: 用于多功能基于文本的项目检索的语言模型对齐

    Aligning Language Models for Versatile Text-based Item Retrieval

    [https://arxiv.org/abs/2402.18899](https://arxiv.org/abs/2402.18899)

    本文针对通用文本嵌入模型与项目检索任务之间的差距，通过生成特定领域数据集和微调嵌入模型，实现了在各种检索任务中显着改进性能，同时在会话设置下展示了模型的实际应用。

    

    这篇论文解决了通用文本嵌入和项目检索任务的特定需求之间的差距。我们展示了现有模型在捕捉项目检索任务所需的微妙之处方面的不足。为了克服这些限制，我们提出从专门针对解锁模型表示能力以进行项目检索的十项任务生成领域数据集。我们的实证研究表明，在该数据集上微调嵌入模型会显着改善各种检索任务。我们还展示了我们优化模型在会话设置中的实际应用，它增强了像Chat-Rec这样的基于LLM的推荐代理的功能。我们的代码可在https://github.com/microsoft/RecAI找到。

    arXiv:2402.18899v1 Announce Type: new  Abstract: This paper addresses the gap between general-purpose text embeddings and the specific demands of item retrieval tasks. We demonstrate the shortcomings of existing models in capturing the nuances necessary for zero-shot performance on item retrieval tasks. To overcome these limitations, we propose generate in-domain dataset from ten tasks tailored to unlocking models' representation ability for item retrieval. Our empirical studies demonstrate that fine-tuning embedding models on the dataset leads to remarkable improvements in a variety of retrieval tasks. We also illustrate the practical application of our refined model in a conversational setting, where it enhances the capabilities of LLM-based Recommender Agents like Chat-Rec. Our code is available at https://github.com/microsoft/RecAI.
    
[^9]: 探究大型语言模型对推荐系统的影响：一项广泛综述

    Exploring the Impact of Large Language Models on Recommender Systems: An Extensive Review

    [https://arxiv.org/abs/2402.18590](https://arxiv.org/abs/2402.18590)

    大型语言模型在推荐系统中展现出卓越的推荐能力和语言理解，重塑了推荐任务的基础。

    

    该论文强调了大型语言模型（LLMs）在重塑推荐系统中的重要性，将它们的价值归因于传统推荐系统所缺乏的独特推理能力。不同于缺乏直接用户互动数据的传统系统，LLMs在推荐物品方面表现出卓越的能力，展示了它们在理解语言复杂性方面的熟练程度。这标志着推荐领域的一个根本性范式转变。在充满活力的研究领域中，研究人员积极利用LLMs的语言理解和生成能力重新定义推荐任务的基础。该研究彻底探讨了LLMs在推荐框架内固有的优势，包括细致的语境理解，跨不同领域的平稳过渡，采用统一的方法，利用共享数据池的全面学习策略，透明度

    arXiv:2402.18590v1 Announce Type: cross  Abstract: The paper underscores the significance of Large Language Models (LLMs) in reshaping recommender systems, attributing their value to unique reasoning abilities absent in traditional recommenders. Unlike conventional systems lacking direct user interaction data, LLMs exhibit exceptional proficiency in recommending items, showcasing their adeptness in comprehending intricacies of language. This marks a fundamental paradigm shift in the realm of recommendations. Amidst the dynamic research landscape, researchers actively harness the language comprehension and generation capabilities of LLMs to redefine the foundations of recommendation tasks. The investigation thoroughly explores the inherent strengths of LLMs within recommendation frameworks, encompassing nuanced contextual comprehension, seamless transitions across diverse domains, adoption of unified approaches, holistic learning strategies leveraging shared data reservoirs, transparent
    
[^10]: Verif.ai: 一种具有引用和可验证答案的开源科学生成式问答系统

    Verif.ai: Towards an Open-Source Scientific Generative Question-Answering System with Referenced and Verifiable Answers

    [https://arxiv.org/abs/2402.18589](https://arxiv.org/abs/2402.18589)

    Verif.ai是一个具有引用和可验证答案的开源科学生成式问答系统，通过信息检索、生成模型和验证引擎的结合实现对主张的生成和验证。

    

    在本文中，我们介绍了项目Verif.ai的当前进展，这是一个具有引用和可验证答案的开源科学生成式问答系统。该系统的组成部分包括（1）一个信息检索系统，结合语义和词汇搜索技术对科学论文（PubMed）进行检索，（2）一个经过微调的生成模型（Mistral 7B），获取前几个答案并生成附有从中得出主张的论文引用的答案，以及（3）一个验证引擎，用于交叉检查生成的主张和从中得出主张的摘要或论文，验证生成主张时是否存在任何错觉。我们通过提供上下文中的摘要加强了生成模型，但此外，一个独立的方法和模型集正在验证答案并检查是否存在错觉。因此，我们相信通过使用我们的方法，我们可以使科学家们

    arXiv:2402.18589v1 Announce Type: cross  Abstract: In this paper, we present the current progress of the project Verif.ai, an open-source scientific generative question-answering system with referenced and verified answers. The components of the system are (1) an information retrieval system combining semantic and lexical search techniques over scientific papers (PubMed), (2) a fine-tuned generative model (Mistral 7B) taking top answers and generating answers with references to the papers from which the claim was derived, and (3) a verification engine that cross-checks the generated claim and the abstract or paper from which the claim was derived, verifying whether there may have been any hallucinations in generating the claim. We are reinforcing the generative model by providing the abstract in context, but in addition, an independent set of methods and models are verifying the answer and checking for hallucinations. Therefore, we believe that by using our method, we can make scientis
    
[^11]: DiFashion: 迈向个性化服装生成

    DiFashion: Towards Personalized Outfit Generation

    [https://arxiv.org/abs/2402.17279](https://arxiv.org/abs/2402.17279)

    引入生成式服装推荐任务（GOR），旨在合成一组时尚图片并组装成视觉和谐的、定制给个人用户的服装。

    

    服装推荐（OR）在时尚领域的发展经历了两个不同阶段：预定义的服装推荐和个性化的服装组合。虽然取得了这些进展，但两个阶段都面临现有时尚产品带来的限制，阻碍了它们满足用户多样化时尚需求的有效性。AI生成内容的出现为OR克服这些约束铺平了道路，展示了个性化服装生成的潜力。为了追求这一目标，我们引入了一项名为生成式服装推荐（GOR）的创新任务，其目标是合成一组时尚图片，并将它们组装成视觉和谐的、定制给个人用户的服装。GOR的主要目标集中在实现生成服装的高保真度、兼容性和个性化。为实现这些目标，我们提出了DiFashion，一个生成式服装推荐

    arXiv:2402.17279v1 Announce Type: new  Abstract: The evolution of Outfit Recommendation (OR) in the realm of fashion has progressed through two distinct phases: Pre-defined Outfit Recommendation and Personalized Outfit Composition. Despite these advancements, both phases face limitations imposed by existing fashion products, hindering their effectiveness in meeting users' diverse fashion needs. The emergence of AI-generated content has paved the way for OR to overcome these constraints, demonstrating the potential for personalized outfit generation.   In pursuit of this, we introduce an innovative task named Generative Outfit Recommendation (GOR), with the goal of synthesizing a set of fashion images and assembling them to form visually harmonious outfits customized to individual users. The primary objectives of GOR revolve around achieving high fidelity, compatibility, and personalization of the generated outfits. To accomplish these, we propose DiFashion, a generative outfit recommen
    
[^12]: 模式透明的顺序推荐

    Pattern-wise Transparent Sequential Recommendation

    [https://arxiv.org/abs/2402.11480](https://arxiv.org/abs/2402.11480)

    提出了一种模式透明的顺序推荐框架，通过将项目序列分解为多级模式并在概率空间中量化每个模式对结果的贡献，实现了透明的决策过程。

    

    透明的决策过程对于开发可靠和值得信赖的推荐系统至关重要。对于顺序推荐来说，意味着模型能够识别关键项目作为其推荐结果的理由。然而，同时实现模型透明度和推荐性能是具有挑战性的，特别是对于将整个项目序列作为输入而不加筛选的模型而言。在本文中，我们提出了一种名为PTSR的可解释框架，它实现了一种模式透明的决策过程。它将项目序列分解为多级模式，这些模式作为整个推荐过程的原子单元。每个模式对结果的贡献在概率空间中得到量化。通过精心设计的模式加权校正，即使在没有真实关键模式的情况下，也能学习模式的贡献。最终推荐

    arXiv:2402.11480v1 Announce Type: new  Abstract: A transparent decision-making process is essential for developing reliable and trustworthy recommender systems. For sequential recommendation, it means that the model can identify critical items asthe justifications for its recommendation results. However, achieving both model transparency and recommendation performance simultaneously is challenging, especially for models that take the entire sequence of items as input without screening. In this paper,we propose an interpretable framework (named PTSR) that enables a pattern-wise transparent decision-making process. It breaks the sequence of items into multi-level patterns that serve as atomic units for the entire recommendation process. The contribution of each pattern to the outcome is quantified in the probability space. With a carefully designed pattern weighting correction, the pattern contribution can be learned in the absence of ground-truth critical patterns. The final recommended
    
[^13]: 在开放世界假设下重新思考跨领域序列推荐

    Rethinking Cross-Domain Sequential Recommendation under Open-World Assumptions

    [https://arxiv.org/abs/2311.04590](https://arxiv.org/abs/2311.04590)

    提出了一个适用于跨领域序列推荐的自适应多兴趣去偏见框架（AMID），在开放世界假设下设计，旨在解决现有方法在在线真实平台上由于数据分布转移导致性能下降的问题。

    

    跨领域顺序推荐（CDSR）方法旨在解决单一领域顺序推荐（SDSR）中存在的数据稀疏和冷启动问题。现有的CDSR作品设计其精心的结构，依赖于重叠用户来传播跨领域信息。然而，当前的CDSR方法采用封闭世界假设，假设在多个领域之间完全重叠的用户，并且数据分布从训练环境到测试环境保持不变。因此，这些方法通常因数据分布转移而在在线真实平台上表现较差。为了在开放世界假设下解决这些挑战，我们设计了一个自适应多兴趣去偏见框架，用于跨领域序列推荐（AMID），其中包括一个多兴趣信息模块（MIM）和一个双重鲁

    arXiv:2311.04590v3 Announce Type: replace  Abstract: Cross-Domain Sequential Recommendation (CDSR) methods aim to tackle the data sparsity and cold-start problems present in Single-Domain Sequential Recommendation (SDSR). Existing CDSR works design their elaborate structures relying on overlapping users to propagate the cross-domain information. However, current CDSR methods make closed-world assumptions, assuming fully overlapping users across multiple domains and that the data distribution remains unchanged from the training environment to the test environment. As a result, these methods typically result in lower performance on online real-world platforms due to the data distribution shifts. To address these challenges under open-world assumptions, we design an \textbf{A}daptive \textbf{M}ulti-\textbf{I}nterest \textbf{D}ebiasing framework for cross-domain sequential recommendation (\textbf{AMID}), which consists of a multi-interest information module (\textbf{MIM}) and a doubly robu
    
[^14]: 面向推荐系统的预算嵌入表

    Budgeted Embedding Table For Recommender Systems

    [https://arxiv.org/abs/2310.14884](https://arxiv.org/abs/2310.14884)

    提出了一种预算嵌入表的方法，解决了传统推荐系统中固定嵌入大小难以扩展的问题，能够有效应对不同用户和项目的多样性嵌入大小。

    

    当今推荐系统的核心是提供给用户优质推荐体验的潜在因素模型。这些模型使用嵌入向量来表示用户和项目。最近的轻量级嵌入方法使不同用户和项目能够具有不同的嵌入大小，但通常存在两个主要缺点。首先，它们将嵌入大小搜索限制在优化启发式平衡推荐质量和内存复杂性的范围内，其中折衷系数需要为每个内存预算手动调整。隐式强制的内存复杂性项甚至可能无法限制参数使用量，使得得到的嵌入表无法严格满足内存预算。其次，大多数解决方案，特别是……

    arXiv:2310.14884v4 Announce Type: replace  Abstract: At the heart of contemporary recommender systems (RSs) are latent factor models that provide quality recommendation experience to users. These models use embedding vectors, which are typically of a uniform and fixed size, to represent users and items. As the number of users and items continues to grow, this design becomes inefficient and hard to scale. Recent lightweight embedding methods have enabled different users and items to have diverse embedding sizes, but are commonly subject to two major drawbacks. Firstly, they limit the embedding size search to optimizing a heuristic balancing the recommendation quality and the memory complexity, where the trade-off coefficient needs to be manually tuned for every memory budget requested. The implicitly enforced memory complexity term can even fail to cap the parameter usage, making the resultant embedding table fail to meet the memory budget strictly. Secondly, most solutions, especially 
    
[^15]: ReLLa: 基于检索增强的大型语言模型的推荐系统中的生命周期序列行为理解

    ReLLa: Retrieval-enhanced Large Language Models for Lifelong Sequential Behavior Comprehension in Recommendation. (arXiv:2308.11131v1 [cs.IR])

    [http://arxiv.org/abs/2308.11131](http://arxiv.org/abs/2308.11131)

    本论文提出了一种名为ReLLa的检索增强大型语言模型框架，用于零样本和小样本推荐任务。通过语义用户行为检索（SUBR）来提取上下文中的有用信息，以改善LLMs的推荐性能。

    

    随着大型语言模型（LLMs）在自然语言处理（NLP）领域取得了显著突破，基于LLM的推荐系统引起了广泛关注并被积极探索。本文专注于适应和增强纯大型语言模型以用于零样本和小样本推荐任务。首先，我们针对推荐领域中LLMs无法从长用户行为序列的文本上下文中提取有用信息的问题，提出并定义了生命周期序列行为理解问题。为了解决这个问题并提高LLMs的推荐性能，我们提出了一种新的框架，即检索增强的大型语言模型（ReLLa）。针对零样本推荐，我们执行语义用户行为检索（SUBR）来提高数据的利用率。

    With large language models (LLMs) achieving remarkable breakthroughs in natural language processing (NLP) domains, LLM-enhanced recommender systems have received much attention and have been actively explored currently. In this paper, we focus on adapting and empowering a pure large language model for zero-shot and few-shot recommendation tasks. First and foremost, we identify and formulate the lifelong sequential behavior incomprehension problem for LLMs in recommendation domains, i.e., LLMs fail to extract useful information from a textual context of long user behavior sequence, even if the length of context is far from reaching the context limitation of LLMs. To address such an issue and improve the recommendation performance of LLMs, we propose a novel framework, namely Retrieval-enhanced Large Language models (ReLLa) for recommendation tasks in both zero-shot and few-shot settings. For zero-shot recommendation, we perform semantic user behavior retrieval (SUBR) to improve the data
    
[^16]: 网络爬虫在robot.txt限制下的策略研究

    web crawler strategies for web pages under robot.txt restriction. (arXiv:2308.04689v1 [cs.AI])

    [http://arxiv.org/abs/2308.04689](http://arxiv.org/abs/2308.04689)

    本文研究了在robot.txt限制下的网络爬虫策略，讨论了搜索引擎如何确定网页排名以及如何获取数据库中的网页。并介绍了机器人排除协议规则和robot.txt文件的基本格式。

    

    当今，所有人都了解互联网并每天在互联网上工作。本文介绍了为用户输入的关键字进行搜索的搜索引擎。搜索引擎使用不同的搜索算法，为上网者提供方便的结果。上网者选择排名靠前的搜索结果，但是网页的排名是如何由搜索引擎确定的？搜索引擎如何获取数据库中的所有网页？本文给出了所有这些基本问题的答案。本研究论文还讨论了为搜索引擎工作的网络爬虫和网络爬虫的机器人排除协议规则。网站管理员使用robot.txt文件中的不同限制规则指导网络爬虫，本文还提到了一些基本的robot.txt格式。

    In the present time, all know about World Wide Web and work over the Internet daily. In this paper, we introduce the search engines working for keywords that are entered by users to find something. The search engine uses different search algorithms for convenient results for providing to the net surfer. Net surfers go with the top search results but how did the results of web pages get higher ranks over search engines? how the search engine got that all the web pages in the database? This paper gives the answers to all these kinds of basic questions. Web crawlers working for search engines and robot exclusion protocol rules for web crawlers are also addressed in this research paper. Webmaster uses different restriction facts in robot.txt file to instruct web crawler, some basic formats of robot.txt are also mentioned in this paper.
    

