# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Knowledge Sharing in Manufacturing using Large Language Models: User Evaluation and Model Benchmarking.](http://arxiv.org/abs/2401.05200) | 使用大型语言模型在制造业中进行知识共享，通过评估实证了该系统的效益，提高了操作员的信息检索速度和问题解决效率，同时强调在有人工专家选项时的偏好。GPT-4是最优秀的模型。 |
| [^2] | [Adaptive Hardness Negative Sampling for Collaborative Filtering.](http://arxiv.org/abs/2401.05191) | 自适应难度负采样（AHNS）是一种新的协同过滤方法，通过自适应地选择适当难度的负样本，解决了现有负采样方法中的误判和误负问题，能够提供更好的训练信号并提高性能。 |
| [^3] | [On the Influence of Reading Sequences on Knowledge Gain during Web Search.](http://arxiv.org/abs/2401.05148) | 本论文研究了阅读序列对Web搜索中的知识获取的影响。通过扩展阅读模型，研究发现知识获取量较高的学习者在阅读上花费了更多时间，并且总体上阅读了更多的文字。更快地阅读但回退较多可能是网络学习较好的指标。 |
| [^4] | [SARA: A Collection of Sensitivity-Aware Relevance Assessments.](http://arxiv.org/abs/2401.05144) | 该论文提出了SARA，一个敏感度感知的相关性评估集合，用于开发敏感度感知的搜索引擎，以提供相关的搜索结果，同时确保不返回敏感性文件。 |
| [^5] | [Prompting Large Language Models for Recommender Systems: A Comprehensive Framework and Empirical Analysis.](http://arxiv.org/abs/2401.04997) | 该论文提出了一个通用的框架，通过提示工程将大型语言模型应用于推荐系统。研究中通过分析大型语言模型的输入和提示工程的关键组成部分，总结了大型语言模型作为推荐器的影响因素。 |
| [^6] | [A Survey on Cross-Domain Sequential Recommendation.](http://arxiv.org/abs/2401.04971) | 跨领域序列推荐通过集成和学习多个领域的交互信息，将用户偏好建模从平面转向立体。文章对CDSR问题进行了定义和分析，提供了从宏观和微观两个视角的系统概述。对于不同领域间的模型，总结了多层融合结构和融合桥梁。对于现有模型，讨论了基础技术和辅助学习技术。展示了公开数据集和实验结果，并给出了未来发展的见解。 |
| [^7] | [Improving Tag-Clouds as Visual Information Retrieval Interfaces.](http://arxiv.org/abs/2401.04947) | 本文提出了一种改进标签云作为图形信息检索界面的方法，通过使用聚类算法进行视觉布局，降低了标签集的语义密度，并提高了标签云布局的视觉一致性。 |
| [^8] | [DualVAE: Dual Disentangled Variational AutoEncoder for Recommendation.](http://arxiv.org/abs/2401.04914) | DualVAE是一种用于推荐系统的双重解耦变分自编码器，通过结合解耦表示学习和变分推断，能够生成隐式交互数据，并提高推荐系统的性能和解释性。 |
| [^9] | [User Embedding Model for Personalized Language Prompting.](http://arxiv.org/abs/2401.04858) | 本研究提出了一种新的用户嵌入模块，可以更有效地处理长时间的用户历史记录，并在推荐系统中取得了显著的改进。 |
| [^10] | [Answer Retrieval in Legal Community Question Answering.](http://arxiv.org/abs/2401.04852) | 提出一种在法律领域中进行答案检索的方法，通过利用结构化的输入信息，解决律师与非专业人士之间的知识差距以及法律问答网站上非正式和正式内容的混合问题。实验结果表明，该方法在一个真实世界的法律领域数据集上显著优于现有方法。 |
| [^11] | [Adapting Standard Retrieval Benchmarks to Evaluate Generated Answers.](http://arxiv.org/abs/2401.04842) | 该研究通过将标准检索基准应用于评估大型语言模型生成的答案，解决了评估答案质量的问题。他们探索并比较了两种评估方法，并以信息检索相关性判断作为评估的锚点。 |
| [^12] | [Translate-Distill: Learning Cross-Language Dense Retrieval by Translation and Distillation.](http://arxiv.org/abs/2401.04810) | Translate-Distill 提出了一种使用翻译和蒸馏的方法来训练跨语言稠密检索模型，克服了在不同语言的情况下训练数据不足的挑战，相比现有方法有更高的效率和效果。 |
| [^13] | [A case study of Generative AI in MSX Sales Copilot: Improving seller productivity with a real-time question-answering system for content recommendation.](http://arxiv.org/abs/2401.04732) | 本论文设计了一个实时问答系统，通过LLM嵌入与销售材料进行匹配，提供给销售人员实时推荐，从而提高销售人员的工作效率。这一解决方案可以在几秒钟内返回最相关的内容推荐，即使对于大规模数据集也是如此。这一推荐系统已成功集成到微软销售人员每日使用的Dynamics CRM的生产版本中。 |
| [^14] | [HyperPIE: Hyperparameter Information Extraction from Scientific Publications.](http://arxiv.org/abs/2312.10638) | 本文提出了 HyperPIE 方法，用于从科学论文中提取超参数信息。通过训练和评估多种模型，包括BERT微调模型和五个大型语言模型，我们实现了关系提取和结构化数据提取，并取得了显著的性能改进。 |
| [^15] | [Rethinking Detection Based Table Structure Recognition for Visually Rich Document Images.](http://arxiv.org/abs/2312.00699) | 这项研究重新思考了基于检测的表格结构识别方法，并发现了影响这些模型性能的潜在因素，包括问题定义、度量不匹配、模型特性和局部特征的影响。 |
| [^16] | [Attributes Grouping and Mining Hashing for Fine-Grained Image Retrieval.](http://arxiv.org/abs/2311.06067) | 该论文提出了一种属性分组和挖掘哈希（AGMH）方法，通过将卷积描述符替代注意力引导特征，在细粒度图像检索中生成了多样的特征表示，以捕捉细微的差异和局部特征。这种方法可以有效地提高图像检索的准确性和效率。 |
| [^17] | [Investigating disaster response through social media data and the Susceptible-Infected-Recovered (SIR) model: A case study of 2020 Western U.S. wildfire season.](http://arxiv.org/abs/2308.05281) | 该研究通过社交媒体数据和SIR模型研究了2020年西部美国火灾季的灾害响应。研究发现Twitter用户主要关注健康影响、损失和撤离三个主题，并使用SIR理论探索了这些主题在Twitter上的传播规模和速度。 |

# 详细

[^1]: 使用大型语言模型在制造业中进行知识共享：用户评估和模型基准测试

    Knowledge Sharing in Manufacturing using Large Language Models: User Evaluation and Model Benchmarking. (arXiv:2401.05200v1 [cs.HC])

    [http://arxiv.org/abs/2401.05200](http://arxiv.org/abs/2401.05200)

    使用大型语言模型在制造业中进行知识共享，通过评估实证了该系统的效益，提高了操作员的信息检索速度和问题解决效率，同时强调在有人工专家选项时的偏好。GPT-4是最优秀的模型。

    

    高效管理知识对组织的成功至关重要。在制造业中，操作工厂变得越来越依赖知识，这给工厂培训和支持新操作员的能力带来了压力。本文介绍了一个基于大型语言模型（LLM）的系统，旨在利用工厂文档中包含的广泛知识，高效回答操作员的查询并促进新知识的共享。为了评估其有效性，我们在一个工厂环境中进行了评估。评估结果表明该系统的好处，即能够更快地检索信息和更高效地解决问题。然而，研究也强调了在有人工专家选项时更倾向于向人工专家学习。此外，我们还对该系统进行了几种闭源和开源语言模型的基准测试。GPT-4表现始终优于其他模型，像StableBe

    Managing knowledge efficiently is crucial for organizational success. In manufacturing, operating factories has become increasing knowledge-intensive putting strain on the factory's capacity to train and support new operators. In this paper, we introduce a Large Language Model (LLM)-based system designed to use the extensive knowledge contained in factory documentation. The system aims to efficiently answer queries from operators and facilitate the sharing of new knowledge. To assess its effectiveness, we conducted an evaluation in a factory setting. The results of this evaluation demonstrated the system's benefits; namely, in enabling quicker information retrieval and more efficient resolution of issues. However, the study also highlighted a preference for learning from a human expert when such an option is available. Furthermore, we benchmarked several closed and open-sourced LLMs for this system. GPT-4 consistently outperformed its counterparts, with open-source models like StableBe
    
[^2]: 自适应难度负采样在协同过滤中的应用

    Adaptive Hardness Negative Sampling for Collaborative Filtering. (arXiv:2401.05191v1 [cs.IR])

    [http://arxiv.org/abs/2401.05191](http://arxiv.org/abs/2401.05191)

    自适应难度负采样（AHNS）是一种新的协同过滤方法，通过自适应地选择适当难度的负样本，解决了现有负采样方法中的误判和误负问题，能够提供更好的训练信号并提高性能。

    

    负采样对于隐式协同过滤至关重要，能够提供适当的负训练信号以达到良好的性能。我们实验证明，所有现有的负采样方法都只能选择固定难度级别的负样本，从而导致了误判问题和误负问题。为了解决这个问题，我们提出了一种新的范式——自适应难度负采样（AHNS），并讨论了它的三个关键准则。通过在训练过程中自适应地选择适当难度的负样本，AHNS可以有效降低误判问题和误负问题的影响。接下来，我们提出了具体的AHNS实例——AHNS_{p<0}，并从理论上证明了AHNS_{p<0}可以很好地符合AHNS的三个准则，并实现了更大的归一化折扣累积增益的下界。此外，我们注意到现有的负采样方法可以被看作是AHNS的更宽松的特例。最后，我们c

    Negative sampling is essential for implicit collaborative filtering to provide proper negative training signals so as to achieve desirable performance. We experimentally unveil a common limitation of all existing negative sampling methods that they can only select negative samples of a fixed hardness level, leading to the false positive problem (FPP) and false negative problem (FNP). We then propose a new paradigm called adaptive hardness negative sampling (AHNS) and discuss its three key criteria. By adaptively selecting negative samples with appropriate hardnesses during the training process, AHNS can well mitigate the impacts of FPP and FNP. Next, we present a concrete instantiation of AHNS called AHNS_{p<0}, and theoretically demonstrate that AHNS_{p<0} can fit the three criteria of AHNS well and achieve a larger lower bound of normalized discounted cumulative gain. Besides, we note that existing negative sampling methods can be regarded as more relaxed cases of AHNS. Finally, we c
    
[^3]: 关于阅读序列对Web搜索中的知识获取的影响的研究

    On the Influence of Reading Sequences on Knowledge Gain during Web Search. (arXiv:2401.05148v1 [cs.IR])

    [http://arxiv.org/abs/2401.05148](http://arxiv.org/abs/2401.05148)

    本论文研究了阅读序列对Web搜索中的知识获取的影响。通过扩展阅读模型，研究发现知识获取量较高的学习者在阅读上花费了更多时间，并且总体上阅读了更多的文字。更快地阅读但回退较多可能是网络学习较好的指标。

    

    如今，学习越来越多地涉及到搜索引擎和网络资源的使用。相关的跨学科研究领域“搜索即学习”旨在理解人们在网络上的学习方式。以往的研究已经调查了几种特征类别，以预测例如在Web搜索过程中预期的知识获取量。然而，目前为止，尚未对注视追踪特征进行深入研究。在本文中，我们将之前使用的基于行的阅读模型扩展为可以检测多行阅读序列的模型。我们使用公开可用的基于Web的学习任务的研究数据，来研究我们的特征集与参与者的测试成绩之间的关系。我们的研究结果表明，知识获取量较高的学习者在阅读上花费了更多时间，并且总体上阅读了更多的文字。我们还发现，以牺牲更多回退的方式更快地阅读可能是网络学习较好的指标。我们将我们的代码公开。

    Nowadays, learning increasingly involves the usage of search engines and web resources. The related interdisciplinary research field search as learning aims to understand how people learn on the web. Previous work has investigated several feature classes to predict, for instance, the expected knowledge gain during web search. Therein, eye-tracking features have not been extensively studied so far. In this paper, we extend a previously used reading model from a line-based one to one that can detect reading sequences across multiple lines. We use publicly available study data from a web-based learning task to examine the relationship between our feature set and the participants' test scores. Our findings demonstrate that learners with higher knowledge gain spent significantly more time reading, and processing more words in total. We also find evidence that faster reading at the expense of more backward regressions may be an indicator of better web-based learning. We make our code publicl
    
[^4]: SARA: 一套敏感度感知的相关性评估集合

    SARA: A Collection of Sensitivity-Aware Relevance Assessments. (arXiv:2401.05144v1 [cs.IR])

    [http://arxiv.org/abs/2401.05144](http://arxiv.org/abs/2401.05144)

    该论文提出了SARA，一个敏感度感知的相关性评估集合，用于开发敏感度感知的搜索引擎，以提供相关的搜索结果，同时确保不返回敏感性文件。

    

    大规模的档案收藏，如电子邮件或政府文件，必须在向公众发布之前进行人工审查，以识别任何敏感信息。敏感性分类已受到文献的广泛关注。然而，最近越来越多的人开始对开发敏感度感知的搜索引擎表现出兴趣，这样可以为用户提供相关的搜索结果，同时确保不向用户返回敏感性文件。敏感度感知搜索可以减轻在档案公开之前进行手动敏感度审核的需求。为了开发这样的系统，需要包含一组信息需求的相关性评估以及多个敏感度类别的基准标签的测试集合。著名的恩龙电子邮件收集包含可以用于表示敏感信息的分类基准标签，如“纯个人”和“个人但是专业的”。

    Large archival collections, such as email or government documents, must be manually reviewed to identify any sensitive information before the collection can be released publicly. Sensitivity classification has received a lot of attention in the literature. However, more recently, there has been increasing interest in developing sensitivity-aware search engines that can provide users with relevant search results, while ensuring that no sensitive documents are returned to the user. Sensitivity-aware search would mitigate the need for a manual sensitivity review prior to collections being made available publicly. To develop such systems, there is a need for test collections that contain relevance assessments for a set of information needs as well as ground-truth labels for a variety of sensitivity categories. The well-known Enron email collection contains a classification ground-truth that can be used to represent sensitive information, e.g., the Purely Personal and Personal but in Profes
    
[^5]: 为推荐系统提供大型语言模型的指导：一个全面的框架和实证分析

    Prompting Large Language Models for Recommender Systems: A Comprehensive Framework and Empirical Analysis. (arXiv:2401.04997v1 [cs.IR])

    [http://arxiv.org/abs/2401.04997](http://arxiv.org/abs/2401.04997)

    该论文提出了一个通用的框架，通过提示工程将大型语言模型应用于推荐系统。研究中通过分析大型语言模型的输入和提示工程的关键组成部分，总结了大型语言模型作为推荐器的影响因素。

    

    最近，像ChatGPT这样的大型语言模型展示了在解决一般任务方面的出色能力，展示了在推荐系统中应用的潜力。为了评估大型语言模型在推荐任务中的有效性，我们的研究主要集中在通过提示工程将大型语言模型用作推荐系统。我们提出了一个通用的框架，用于利用大型语言模型在推荐任务中的能力。为了进行分析，我们将大型语言模型的输入形式化为具有两个关键方面的自然语言提示，并解释了我们的框架如何推广到各种推荐场景。至于将大型语言模型用作推荐系统，我们根据大型语言模型的分类分析了公开可用性、调优策略、模型架构、参数规模和上下文长度对推荐结果的影响。至于提示工程，我们进一步分析了四个重要组成部分的影响。

    Recently, large language models such as ChatGPT have showcased remarkable abilities in solving general tasks, demonstrating the potential for applications in recommender systems. To assess how effectively LLMs can be used in recommendation tasks, our study primarily focuses on employing LLMs as recommender systems through prompting engineering. We propose a general framework for utilizing LLMs in recommendation tasks, focusing on the capabilities of LLMs as recommenders. To conduct our analysis, we formalize the input of LLMs for recommendation into natural language prompts with two key aspects, and explain how our framework can be generalized to various recommendation scenarios. As for the use of LLMs as recommenders, we analyze the impact of public availability, tuning strategies, model architecture, parameter scale, and context length on recommendation results based on the classification of LLMs. As for prompt engineering, we further analyze the impact of four important components o
    
[^6]: 跨领域序列推荐的综述

    A Survey on Cross-Domain Sequential Recommendation. (arXiv:2401.04971v1 [cs.IR])

    [http://arxiv.org/abs/2401.04971](http://arxiv.org/abs/2401.04971)

    跨领域序列推荐通过集成和学习多个领域的交互信息，将用户偏好建模从平面转向立体。文章对CDSR问题进行了定义和分析，提供了从宏观和微观两个视角的系统概述。对于不同领域间的模型，总结了多层融合结构和融合桥梁。对于现有模型，讨论了基础技术和辅助学习技术。展示了公开数据集和实验结果，并给出了未来发展的见解。

    

    跨领域序列推荐（CDSR）通过在不同粒度（从序列间到序列内，从单领域到跨领域）上集成和学习来自多个领域的交互信息，将用户偏好建模从平面转向了立体。本综述文章中，我们首先使用四维张量定义了CDSR问题，并分析了其在多维度降维下的多类型输入表示。接下来，我们从整体和细节两个视角提供了系统的概述。从整体视角，我们总结了各个模型在不同领域间的多层融合结构，并讨论了它们的融合桥梁。从细节视角，我们着重讨论了现有模型的基础技术，并解释了辅助学习技术。最后，我们展示了可用的公开数据集和代表性的实验结果，并提供了对未来发展的一些见解。

    Cross-domain sequential recommendation (CDSR) shifts the modeling of user preferences from flat to stereoscopic by integrating and learning interaction information from multiple domains at different granularities (ranging from inter-sequence to intra-sequence and from single-domain to cross-domain).In this survey, we initially define the CDSR problem using a four-dimensional tensor and then analyze its multi-type input representations under multidirectional dimensionality reductions. Following that, we provide a systematic overview from both macro and micro views. From a macro view, we abstract the multi-level fusion structures of various models across domains and discuss their bridges for fusion. From a micro view, focusing on the existing models, we specifically discuss the basic technologies and then explain the auxiliary learning technologies. Finally, we exhibit the available public datasets and the representative experimental results as well as provide some insights into future d
    
[^7]: 提升标签云作为图形信息检索界面的方法

    Improving Tag-Clouds as Visual Information Retrieval Interfaces. (arXiv:2401.04947v1 [cs.IR])

    [http://arxiv.org/abs/2401.04947](http://arxiv.org/abs/2401.04947)

    本文提出了一种改进标签云作为图形信息检索界面的方法，通过使用聚类算法进行视觉布局，降低了标签集的语义密度，并提高了标签云布局的视觉一致性。

    

    基于标签的系统允许用户通过标签（自选的关键词）对网络资源进行分类，以便以后重新找到这些资源。标签也是一种社会编制索引的过程，因为用户共享他们的标签和资源，构建称为云记的社会标志索引。与标签为基础的系统同时流行的是一种名为标签云的界面模型用于视觉信息检索。在这个模型中，最常用的标签按照字母顺序展示。本文提出了一种创新的标签云标签选择方法，并提出使用聚类算法进行视觉布局，旨在提高浏览体验。结果表明，所提出的方法降低了标签集的语义密度，并改善了标签云布局的视觉一致性。

    Tagging-based systems enable users to categorize web resources by means of tags (freely chosen keywords), in order to refinding these resources later. Tagging is implicitly also a social indexing process, since users share their tags and resources, constructing a social tag index, so-called folksonomy. At the same time of tagging-based system, has been popularised an interface model for visual information retrieval known as Tag-Cloud. In this model, the most frequently used tags are displayed in alphabetical order. This paper presents a novel approach to Tag-Cloud's tags selection, and proposes the use of clustering algorithms for visual layout, with the aim of improve browsing experience. The results suggest that presented approach reduces the semantic density of tag set, and improves the visual consistency of Tag-Cloud layout.
    
[^8]: DualVAE: 双重解耦变分自编码器用于推荐系统

    DualVAE: Dual Disentangled Variational AutoEncoder for Recommendation. (arXiv:2401.04914v1 [cs.IR])

    [http://arxiv.org/abs/2401.04914](http://arxiv.org/abs/2401.04914)

    DualVAE是一种用于推荐系统的双重解耦变分自编码器，通过结合解耦表示学习和变分推断，能够生成隐式交互数据，并提高推荐系统的性能和解释性。

    

    学习用户和物品的精确表示以适应观察到的交互数据是协同过滤的基本任务。现有的研究通常推断纠缠表示以适应这种交互数据，忽视了用户和物品之间多样化匹配关系的建模，导致性能受限且解释性较弱。为了解决这个问题，我们提出了一种用于协同推荐的双重解耦变分自编码器(DualVAE)，将解耦表示学习与变分推断相结合，以促进隐式交互数据的生成。具体而言，我们首先通过统一注意力感知的双重解耦和解耦变分自编码器来实现解耦概念，推断用户和物品的解耦潜在表示。此外，为了促进用户和物品的解耦表示的对应性和独立性，我们设计了一个邻近性匹配损失和一个解耦因子化KL散度损失。

    Learning precise representations of users and items to fit observed interaction data is the fundamental task of collaborative filtering. Existing studies usually infer entangled representations to fit such interaction data, neglecting to model the diverse matching relationships between users and items behind their interactions, leading to limited performance and weak interpretability. To address this problem, we propose a Dual Disentangled Variational AutoEncoder (DualVAE) for collaborative recommendation, which combines disentangled representation learning with variational inference to facilitate the generation of implicit interaction data. Specifically, we first implement the disentangling concept by unifying an attention-aware dual disentanglement and disentangled variational autoencoder to infer the disentangled latent representations of users and items. Further, to encourage the correspondence and independence of disentangled representations of users and items, we design a neighbo
    
[^9]: 个性化语言提示的用户嵌入模型

    User Embedding Model for Personalized Language Prompting. (arXiv:2401.04858v1 [cs.CL])

    [http://arxiv.org/abs/2401.04858](http://arxiv.org/abs/2401.04858)

    本研究提出了一种新的用户嵌入模块，可以更有效地处理长时间的用户历史记录，并在推荐系统中取得了显著的改进。

    

    对于提升推荐系统的模型，建模长时间的历史记录起到了关键作用，能够捕捉用户不断演变的偏好，从而得到更准确和个性化的推荐。本研究致力于解决自然语言偏好理解中建模长用户历史记录的挑战。具体地，我们引入了一种新的用户嵌入模块(UEM)，通过将用户历史记录以嵌入形式压缩和表示，将其作为对语言模型的软提示。我们的实验表明，与传统的基于文本的提示方法相比，这种方法在处理显著更长的历史记录方面具有卓越的能力，并在预测性能方面取得了实质性的改进。该研究的主要贡献在于展示了使用表示为嵌入的用户信号来偏置语言模型的能力。

    Modeling long histories plays a pivotal role in enhancing recommendation systems, allowing to capture user's evolving preferences, resulting in more precise and personalized recommendations. In this study we tackle the challenges of modeling long user histories for preference understanding in natural language. Specifically, we introduce a new User Embedding Module (UEM) that efficiently processes user history in free-form text by compressing and representing them as embeddings, to use them as soft prompts to a LM. Our experiments demonstrate the superior capability of this approach in handling significantly longer histories compared to conventional text based prompting methods, yielding substantial improvements in predictive performance. The main contribution of this research is to demonstrate the ability to bias language models with user signals represented as embeddings.
    
[^10]: 在法律社区问答中的答案检索

    Answer Retrieval in Legal Community Question Answering. (arXiv:2401.04852v1 [cs.IR])

    [http://arxiv.org/abs/2401.04852](http://arxiv.org/abs/2401.04852)

    提出一种在法律领域中进行答案检索的方法，通过利用结构化的输入信息，解决律师与非专业人士之间的知识差距以及法律问答网站上非正式和正式内容的混合问题。实验结果表明，该方法在一个真实世界的法律领域数据集上显著优于现有方法。

    

    在法律领域中，答案检索的任务旨在帮助用户从海量的专业回答中寻求相关的法律建议。两个主要挑战阻碍了将现有的答案检索方法应用于法律领域：（1）律师与非专业人士之间的巨大知识差距；（2）法律问答网站上非正式和正式内容的混合。为了解决这些挑战，我们提出了CE_FS，这是一种基于细粒度结构化输入的新型交叉编码器（CE）重排序器。CE_FS利用CQA数据中的额外结构化信息，提高了交叉编码器重排序器的效果。此外，我们提出了LegalQA：一个用于评估法律领域答案检索的真实世界基准数据集。在LegalQA上进行的实验证明，我们提出的方法明显优于在MS MARCO上微调的强交叉编码器重排序器。我们的新发现是，在问题描述之外，加入每个问题的标签能够进一步提升性能。

    The task of answer retrieval in the legal domain aims to help users to seek relevant legal advice from massive amounts of professional responses. Two main challenges hinder applying existing answer retrieval approaches in other domains to the legal domain: (1) a huge knowledge gap between lawyers and non-professionals; and (2) a mix of informal and formal content on legal QA websites. To tackle these challenges, we propose CE_FS, a novel cross-encoder (CE) re-ranker based on the fine-grained structured inputs. CE_FS uses additional structured information in the CQA data to improve the effectiveness of cross-encoder re-rankers. Furthermore, we propose LegalQA: a real-world benchmark dataset for evaluating answer retrieval in the legal domain. Experiments conducted on LegalQA show that our proposed method significantly outperforms strong cross-encoder re-rankers fine-tuned on MS MARCO. Our novel finding is that adding the question tags of each question besides the question description an
    
[^11]: 将标准检索基准应用于评估生成的答案的研究

    Adapting Standard Retrieval Benchmarks to Evaluate Generated Answers. (arXiv:2401.04842v1 [cs.IR])

    [http://arxiv.org/abs/2401.04842](http://arxiv.org/abs/2401.04842)

    该研究通过将标准检索基准应用于评估大型语言模型生成的答案，解决了评估答案质量的问题。他们探索并比较了两种评估方法，并以信息检索相关性判断作为评估的锚点。

    

    目前，大型语言模型能够直接生成许多事实性问题的答案，而无需引用外部来源。然而，对于评估这些答案的质量和正确性，比较一个模型与另一个模型的性能，以及比较一个提示与另一个提示的方法相对较少关注。此外，生成答案的质量很少与检索答案的质量直接进行比较。随着模型的演化和提示的修改，我们没有系统的方法来衡量改进，除非采用昂贵的人为判断。为了解决这个问题，我们将标准检索基准应用于评估大型语言模型生成的答案。受BERTScore摘要评估指标的启发，我们探索了两种方法。第一种方法是基于基准相关性判断进行评估。我们进行了实证实验来研究信息检索相关性判断如何作为评估生成答案的锚点。

    Large language models can now directly generate answers to many factual questions without referencing external sources. Unfortunately, relatively little attention has been paid to methods for evaluating the quality and correctness of these answers, for comparing the performance of one model to another, or for comparing one prompt to another. In addition, the quality of generated answers are rarely directly compared to the quality of retrieved answers. As models evolve and prompts are modified, we have no systematic way to measure improvements without resorting to expensive human judgments. To address this problem we adapt standard retrieval benchmarks to evaluate answers generated by large language models. Inspired by the BERTScore metric for summarization, we explore two approaches. In the first, we base our evaluation on the benchmark relevance judgments. We empirically run experiments on how information retrieval relevance judgments can be utilized as an anchor to evaluating the gen
    
[^12]: Translate-Distill: 通过翻译和蒸馏学习跨语言稠密检索

    Translate-Distill: Learning Cross-Language Dense Retrieval by Translation and Distillation. (arXiv:2401.04810v1 [cs.IR])

    [http://arxiv.org/abs/2401.04810](http://arxiv.org/abs/2401.04810)

    Translate-Distill 提出了一种使用翻译和蒸馏的方法来训练跨语言稠密检索模型，克服了在不同语言的情况下训练数据不足的挑战，相比现有方法有更高的效率和效果。

    

    先前关于英语单语检索的研究表明，使用大量查询-文档相关性判断训练的交互编码器可以用作教师模型来训练更高效但同样有效的双编码器学生模型。然而，在不同语言的查询和文档之间进行跨语言信息检索 (CLIR) 时应用类似的知识蒸馏方法是具有挑战性的，因为查询和文档语言不同时缺乏足够大的训练集合。现有的 CLIR 技术依赖于从庞大的英语 MS MARCO 训练集中翻译查询、文档或两者的方法，称为 Translate-Train。本文提出了一种替代方案 Translate-Distill，其中从单语交互编码器或 CLIR 交互编码器进行知识蒸馏，训练双编码器 CLIR 学生模型。这种更丰富的设计空间使得教师模型能够...

    Prior work on English monolingual retrieval has shown that a cross-encoder trained using a large number of relevance judgments for query-document pairs can be used as a teacher to train more efficient, but similarly effective, dual-encoder student models. Applying a similar knowledge distillation approach to training an efficient dual-encoder model for Cross-Language Information Retrieval (CLIR), where queries and documents are in different languages, is challenging due to the lack of a sufficiently large training collection when the query and document languages differ. The state of the art for CLIR thus relies on translating queries, documents, or both from the large English MS MARCO training set, an approach called Translate-Train. This paper proposes an alternative, Translate-Distill, in which knowledge distillation from either a monolingual cross-encoder or a CLIR cross-encoder is used to train a dual-encoder CLIR student model. This richer design space enables the teacher model to
    
[^13]: MSX销售协同助手中生成式人工智能的案例研究: 通过实时问答系统改善销售人员的工作效率以实现内容推荐

    A case study of Generative AI in MSX Sales Copilot: Improving seller productivity with a real-time question-answering system for content recommendation. (arXiv:2401.04732v1 [cs.IR])

    [http://arxiv.org/abs/2401.04732](http://arxiv.org/abs/2401.04732)

    本论文设计了一个实时问答系统，通过LLM嵌入与销售材料进行匹配，提供给销售人员实时推荐，从而提高销售人员的工作效率。这一解决方案可以在几秒钟内返回最相关的内容推荐，即使对于大规模数据集也是如此。这一推荐系统已成功集成到微软销售人员每日使用的Dynamics CRM的生产版本中。

    

    本文设计了一个实时问答系统，专门为销售人员提供有关材料/文档的实时推荐，以便与客户分享或在电话中参考。通过使用Seismic销售资料的相对较大规模的多样化数据集，我们展示了如何使用卖方查询的LLM嵌入与相关内容相匹配。我们通过以详细的方式设计提示语，并利用可用的丰富的文档和销售者元特征集，实现了这一目标。通过使用具有交叉编码器重排序器架构的双编码器，我们展示了该解决方案在仅几秒钟内即可返回最相关的内容推荐，即使对于大规模数据集也是如此。我们的推荐系统已部署为用于实时推理的AML端点，并已集成到Copilot界面中，该界面现已部署在每日由微软销售人员使用的Dynamics CRM的生产版本中(MSX）。

    In this paper, we design a real-time question-answering system specifically targeted for helping sellers get relevant material/documentation they can share live with their customers or refer to during a call. Taking the Seismic content repository as a relatively large scale example of a diverse dataset of sales material, we demonstrate how LLM embeddings of sellers' queries can be matched with the relevant content. We achieve this by engineering prompts in an elaborate fashion that makes use of the rich set of meta-features available for documents and sellers. Using a bi-encoder with cross-encoder re-ranker architecture, we show how the solution returns the most relevant content recommendations in just a few seconds even for large datasets. Our recommender system is deployed as an AML endpoint for real-time inferencing and has been integrated into a Copilot interface that is now deployed in the production version of the Dynamics CRM, known as MSX, used daily by Microsoft sellers.
    
[^14]: HyperPIE: 从科学论文中提取超参数信息

    HyperPIE: Hyperparameter Information Extraction from Scientific Publications. (arXiv:2312.10638v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2312.10638](http://arxiv.org/abs/2312.10638)

    本文提出了 HyperPIE 方法，用于从科学论文中提取超参数信息。通过训练和评估多种模型，包括BERT微调模型和五个大型语言模型，我们实现了关系提取和结构化数据提取，并取得了显著的性能改进。

    

    从论文中自动提取信息是实现科学知识机器可读化的关键。提取出的信息可以促进学术搜索、决策制定和知识图谱构建。现有方法没有涵盖的一类重要信息是超参数信息。在本文中，我们将超参数信息提取（HyperPIE）形式化为实体识别和关系提取任务，并创建了一个标记数据集来涵盖各种计算机科学学科的论文。利用这个数据集，我们训练和评估了基于BERT的微调模型以及五个大型语言模型：GPT-3.5、GALACTICA、Falcon、Vicuna和WizardLM。对于微调模型，我们提出了一种关系提取方法，相较于最先进的基准模型，F1值提升了29%。对于大型语言模型，我们提出了一种利用YAML输出进行结构化数据提取的方法，取得了

    Automatic extraction of information from publications is key to making scientific knowledge machine readable at a large scale. The extracted information can, for example, facilitate academic search, decision making, and knowledge graph construction. An important type of information not covered by existing approaches is hyperparameters. In this paper, we formalize and tackle hyperparameter information extraction (HyperPIE) as an entity recognition and relation extraction task. We create a labeled data set covering publications from a variety of computer science disciplines. Using this data set, we train and evaluate BERT-based fine-tuned models as well as five large language models: GPT-3.5, GALACTICA, Falcon, Vicuna, and WizardLM. For fine-tuned models, we develop a relation extraction approach that achieves an improvement of 29% F1 over a state-of-the-art baseline. For large language models, we develop an approach leveraging YAML output for structured data extraction, which achieves a
    
[^15]: 对于视觉丰富的文档图像重新思考基于检测的表格结构识别

    Rethinking Detection Based Table Structure Recognition for Visually Rich Document Images. (arXiv:2312.00699v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2312.00699](http://arxiv.org/abs/2312.00699)

    这项研究重新思考了基于检测的表格结构识别方法，并发现了影响这些模型性能的潜在因素，包括问题定义、度量不匹配、模型特性和局部特征的影响。

    

    表格结构识别是一个广泛讨论的任务，旨在将无结构的表格图像转换为结构化格式，如HTML序列，以便使用仅处理文本的模型（如ChatGPT）进一步处理这些表格。一种解决方案是使用检测模型检测表格组件（例如列和行），然后应用基于规则的后处理方法将检测结果转换为HTML序列。然而，现有的基于检测的模型在单元级别的表格结构识别度量（如TEDS）方面通常表现不如其他类型的解决方案，并且这些模型在TSR任务上性能不佳的潜在原因也没有得到很好的探索。因此，我们全面重新审视现有的基于检测的模型，并探索影响这些模型性能的潜在原因，包括问题定义不当、检测和TSR度量的不匹配问题、检测模型的特性以及局部特征的影响。

    Table Structure Recognition (TSR) is a widely discussed task aiming at transforming unstructured table images into structured formats, such as HTML sequences, to make text-only models, such as ChatGPT, that can further process these tables. One type of solution is using detection models to detect table components, such as columns and rows, then applying a rule-based post-processing method to convert detection results into HTML sequences. However, existing detection-based models usually cannot perform as well as other types of solutions regarding cell-level TSR metrics, such as TEDS, and the underlying reasons limiting the performance of these models on the TSR task are also not well-explored. Therefore, we revisit existing detection-based models comprehensively and explore the underlying reasons hindering these models' performance, including the improper problem definition, the mismatch issue of detection and TSR metrics, the characteristics of detection models, and the impact of local
    
[^16]: 属性分组和挖掘哈希在细粒度图像检索中的应用

    Attributes Grouping and Mining Hashing for Fine-Grained Image Retrieval. (arXiv:2311.06067v1 [cs.IR] CROSS LISTED)

    [http://arxiv.org/abs/2311.06067](http://arxiv.org/abs/2311.06067)

    该论文提出了一种属性分组和挖掘哈希（AGMH）方法，通过将卷积描述符替代注意力引导特征，在细粒度图像检索中生成了多样的特征表示，以捕捉细微的差异和局部特征。这种方法可以有效地提高图像检索的准确性和效率。

    

    近年来，哈希方法在大规模媒体搜索中因其低存储和强大的表示能力而受到广泛关注。为了描述具有相似整体外观但细微差异的对象，越来越多的研究聚焦于基于哈希的细粒度图像检索。现有的哈希网络通常通过对相同的深层激活张量进行注意力引导来生成局部和全局特征，这限制了特征表示的多样性。为了解决这个限制，我们将卷积描述符替代注意力引导特征，并提出了一种属性分组和挖掘哈希（AGMH）方法，该方法可以在多个描述符中对类别特定的视觉属性进行分组和嵌入，以生成用于有效细粒度图像检索的综合特征表示。具体来说，我们设计了一种注意力分散损失（ADL）来强制描述符关注各种局部区域，并捕捉多样的细微细节。

    In recent years, hashing methods have been popular in the large-scale media search for low storage and strong representation capabilities. To describe objects with similar overall appearance but subtle differences, more and more studies focus on hashing-based fine-grained image retrieval. Existing hashing networks usually generate both local and global features through attention guidance on the same deep activation tensor, which limits the diversity of feature representations. To handle this limitation, we substitute convolutional descriptors for attention-guided features and propose an Attributes Grouping and Mining Hashing (AGMH), which groups and embeds the category-specific visual attributes in multiple descriptors to generate a comprehensive feature representation for efficient fine-grained image retrieval. Specifically, an Attention Dispersion Loss (ADL) is designed to force the descriptors to attend to various local regions and capture diverse subtle details. Moreover, we propos
    
[^17]: 通过社交媒体数据和易感-感染-康复（SIR）模型研究灾害响应：以2020年西部美国火灾季为案例研究

    Investigating disaster response through social media data and the Susceptible-Infected-Recovered (SIR) model: A case study of 2020 Western U.S. wildfire season. (arXiv:2308.05281v1 [cs.SI])

    [http://arxiv.org/abs/2308.05281](http://arxiv.org/abs/2308.05281)

    该研究通过社交媒体数据和SIR模型研究了2020年西部美国火灾季的灾害响应。研究发现Twitter用户主要关注健康影响、损失和撤离三个主题，并使用SIR理论探索了这些主题在Twitter上的传播规模和速度。

    

    有效的灾害响应对受影响的社区至关重要。应急人员和决策者在灾害期间在了解社区所面临问题的可靠和及时的指标上将受益于社交媒体提供的丰富数据来源。社交媒体可以反映公众关注和需求，为决策者提供有价值的洞见，以了解不断演变的情况并优化资源配置。我们使用双向编码器表示转换（BERT）主题建模对Twitter数据进行主题聚类。然后，我们进行了时间-空间分析，研究了这些主题在2020年美国西部火灾季期间在不同地区的分布情况。我们的结果显示，Twitter用户主要关注三个主题：“健康影响”，“损失”，“撤离”。我们使用易感-感染-康复（SIR）理论来探索主题在Twitter上的传播规模和速度。结果清晰地显示了主题传播的情况。

    Effective disaster response is critical for affected communities. Responders and decision-makers would benefit from reliable, timely measures of the issues impacting their communities during a disaster, and social media offers a potentially rich data source. Social media can reflect public concerns and demands during a disaster, offering valuable insights for decision-makers to understand evolving situations and optimize resource allocation. We used Bidirectional Encoder Representations from Transformers (BERT) topic modeling to cluster topics from Twitter data. Then, we conducted a temporal-spatial analysis to examine the distribution of these topics across different regions during the 2020 western U.S. wildfire season. Our results show that Twitter users mainly focused on three topics:"health impact," "damage," and "evacuation." We used the Susceptible-Infected-Recovered (SIR) theory to explore the magnitude and velocity of topic diffusion on Twitter. The results displayed a clear re
    

