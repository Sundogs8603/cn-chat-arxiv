# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [CODE-ACCORD: A Corpus of Building Regulatory Data for Rule Generation towards Automatic Compliance Checking](https://arxiv.org/abs/2403.02231) | 介绍了一个独特的数据集CODE-ACCORD，旨在解决自动合规检查中解释建筑法规的挑战，成为机器可读规则生成的基础。 |
| [^2] | [Evaluating the Explainability of Neural Rankers](https://arxiv.org/abs/2403.01981) | 本文旨在评估神经排序器的可解释性，提出了一个共同的评估平台，要求每个模型除了返回排名的文档列表外，还需返回每个文档的解释单元或解释理由列表。 |
| [^3] | [Recommending Missed Citations Identified by Reviewers: A New Task, Dataset and Baselines](https://arxiv.org/abs/2403.01873) | 该论文定义了一个新任务——推荐评论者标识的遗漏引用(RMC)，并构建了对应的数据集CitationR，以改进完整论文的引用。 |
| [^4] | [Unleashing Graph Partitioning for Large-Scale Nearest Neighbor Search](https://arxiv.org/abs/2403.01797) | 本文设计了可与任何分区方法一起使用的简单有效的路由方法，并在性能上证明了强大的理论保证。 |
| [^5] | [Towards Self-Contained Answers: Entity-Based Answer Rewriting in Conversational Search](https://arxiv.org/abs/2403.01747) | 本文针对会话搜索中的基于实体的答案重写，提出了两种答案重写策略，以改善用户体验。 |
| [^6] | [NoteLLM: A Retrievable Large Language Model for Note Recommendation](https://arxiv.org/abs/2403.01744) | 本文提出了一种名为NoteLLM的新颖统一框架，利用大型语言模型(LLMs)来实现物品到物品(I2I)的笔记推荐，通过学习生成哈希标签/类别潜在地增强笔记嵌入，提高了对关键笔记信息的压缩。 |
| [^7] | [TweetInfo: An Interactive System to Mitigate Online Harm](https://arxiv.org/abs/2403.01646) | TweetInfo是一个互动系统，通过提供有关帖子的元信息来减轻有害内容的消费，重点关注仇恨言论和不实信息。 |
| [^8] | [Measuring Technological Convergence in Encryption Technologies with Proximity Indices: A Text Mining and Bibliometric Analysis using OpenAlex](https://arxiv.org/abs/2403.01601) | 该研究利用文本挖掘和文献计量分析方法，基于OpenAlex目录，预测了加密技术的技术接近度指数，发现了区块链与公钥密码学之间显著的技术融合。 |
| [^9] | [Logic Rules as Explanations for Legal Case Retrieval](https://arxiv.org/abs/2403.01457) | 本文提出了神经符号增强的法律案例检索（NS-LCR）框架，通过学习案例级别和法律级别的逻辑规则，将规则以神经符号方式集成到检索过程中，以提供逻辑且可解释的解释。 |
| [^10] | [LM4OPT: Unveiling the Potential of Large Language Models in Formulating Mathematical Optimization Problems](https://arxiv.org/abs/2403.01342) | 本研究比较了几种知名的大型语言模型在翻译语言描述为数学优化问题中的表现，发现GPT-4在一次场景中表现出色，并引入了“LM4OPT”框架进行Llama-2-7b的渐进微调。 |
| [^11] | [Supplier Recommendation in Online Procurement](https://arxiv.org/abs/2403.01301) | 本研究提出了一个推荐系统，用于在道路货运在线采购中辅助进行供应商发现，能够提供个性化的供应商推荐，考虑到客户的需求和偏好。 |
| [^12] | [COOL: A Conjoint Perspective on Spatio-Temporal Graph Neural Network for Traffic Forecasting](https://arxiv.org/abs/2403.01091) | 本文提出了一种名为COOL的Conjoint Spatio-Temporal图神经网络，旨在共同捕捉交通预测中的高阶关系。 |
| [^13] | [BasedAI: A decentralized P2P network for Zero Knowledge Large Language Models (ZK-LLMs)](https://arxiv.org/abs/2403.01008) | BasedAI引入了Cerberus Squeezing机制，将标准大型语言模型转化为加密的零知识模型，显著提高了在全同态加密计算环境中的性能。 |
| [^14] | [An Interpretable Ensemble of Graph and Language Models for Improving Search Relevance in E-Commerce](https://arxiv.org/abs/2403.00923) | 该研究提出了一个可解释的图形和语言模型集成方法，用于提高电子商务领域中搜索相关性，弥补了现有模型在实际部署中的泛化能力和可解释性方面的不足。 |
| [^15] | [End-to-end Graph-Sequential Representation Learning for Accurate Recommendations](https://arxiv.org/abs/2403.00895) | 本文提出了一个新颖的多重表示学习框架，有效地结合了基于序列和基于图的推荐方法，显著改善了推荐性能。 |
| [^16] | [Text classification of column headers with a controlled vocabulary: leveraging LLMs for metadata enrichment](https://arxiv.org/abs/2403.00884) | 通过利用LLMs，我们提出了一种方法，使用ChatGPT-3.5、GoogleBard和GoogleGemini对列标题进行主题注释的元数据丰富化，研究它们在对领域特定主题进行分类的能力，并评估其内部一致性、机器对齐性和人机一致性。ChatGPT和GoogleGemini在内部一致性以及与人一致性方面优于GoogleBard。 |
| [^17] | [Dual-Granularity Medication Recommendation Based on Causal Inference](https://arxiv.org/abs/2403.00880) | 提出了DGMed框架，利用因果推断和创新的特征对齐方法进行双粒度药物推荐 |
| [^18] | [Disaggregated Multi-Tower: Topology-aware Modeling Technique for Efficient Large-Scale Recommendation](https://arxiv.org/abs/2403.00877) | Disaggregated Multi-Tower提出了一种面向拓扑感知的建模技术，通过SPTT、TM和TP三个组件实现了高效的大规模推荐，加速性能提升了1.9倍。 |
| [^19] | [LLM-Ensemble: Optimal Large Language Model Ensemble Method for E-commerce Product Attribute Value Extraction](https://arxiv.org/abs/2403.00863) | 提出了一种名为LLM-ensemble的算法，用于集成不同大型语言模型，以提高电子商务产品属性值提取的性能。 |
| [^20] | [Improved Online Learning Algorithms for CTR Prediction in Ad Auctions](https://arxiv.org/abs/2403.00845) | 该研究致力于解决广告拍卖中的在线学习问题，提出了针对广告主两种不同战略行为模型的在线机制，其中一种在最坏情况下取得了紧致的遗憾界限，另一种则解决了更复杂的长期效用优化问题。 |
| [^21] | [Lower-Left Partial AUC: An Effective and Efficient Optimization Metric for Recommendation](https://arxiv.org/abs/2403.00844) | 提出了一种新的优化指标Lower-Left Partial AUC（LLPAUC），在计算效率上类似于AUC，但与Top-K排名指标强相关，能在推荐系统中有效提升性能。 |
| [^22] | [Enhancing Long-Term Recommendation with Bi-level Learnable Large Language Model Planning](https://arxiv.org/abs/2403.00843) | 利用大型语言模型的规划能力来增强长期推荐，使模型在个性化推荐中更有效地理解和应用任务解决原则 |
| [^23] | [Explainable Session-based Recommendation via Path Reasoning](https://arxiv.org/abs/2403.00832) | 通过路径推理的泛化层次强化学习框架提高了基于会话的推荐的可解释性，设计了多目标奖励机制和路径中间点奖励以应对顺序模式的跳过行为和增强知识图的探索效率。 |
| [^24] | [InteraRec: Interactive Recommendations Using Multimodal Large Language Models](https://arxiv.org/abs/2403.00822) | InteraRec引入了一种复杂的交互式推荐框架，与传统方法不同，它不仅依赖Weblog生成推荐，还捕获用户导航时网页的高频截图。 |
| [^25] | [Retrieval Augmented Generation Systems: Automatic Dataset Creation, Evaluation and Boolean Agent Setup](https://arxiv.org/abs/2403.00820) | 本文提出了一种严格的数据集创建和评估工作流程，用于量化比较不同的RAG策略，同时开发和评估了一个布尔代理RAG设置，使得大语言模型可以节省标记来决定是否查询向量数据库。 |
| [^26] | [Doubly Calibrated Estimator for Recommendation on Data Missing Not At Random](https://arxiv.org/abs/2403.00817) | 提出了双重校准估计器，通过校准插补和概率模型来解决推荐系统中缺失数据不随机的挑战 |
| [^27] | [CFRet-DVQA: Coarse-to-Fine Retrieval and Efficient Tuning for Document Visual Question Answering](https://arxiv.org/abs/2403.00816) | 该研究提出了一种名为CFRet-DVQA的方法，通过检索和高效调优，解决了文档视觉问答中定位信息和限制模型输入的长度等问题，进一步提升了答案的生成性能。 |
| [^28] | [RAM-EHR: Retrieval Augmentation Meets Clinical Predictions on Electronic Health Records](https://arxiv.org/abs/2403.00815) | RAM-EHR通过增强检索并利用总结知识，提高了针对电子健康记录的临床预测效果。 |
| [^29] | [Gender Biased Legal Case Retrieval System on Users' Decision Process](https://arxiv.org/abs/2403.00814) | 设计了一个新的用户实验框架来研究法律案例搜索结果中的性别偏见对用户认知的影响 |
| [^30] | [Enhancing Cloud-Based Large Language Model Processing with Elasticsearch and Transformer Models](https://arxiv.org/abs/2403.00807) | 利用Elasticsearch和Transformer模型提升云端基于大型语言模型的处理，尤其在实现语义搜索方面有显著帮助。 |
| [^31] | [Enhanced User Interaction in Operating Systems through Machine Learning Language Models](https://arxiv.org/abs/2403.00806) | 通过结合交互设计和机器学习，提供更高效、个性化的用户体验，满足用户特定需求，持续改善产品质量和性能。 |
| [^32] | [LiMAML: Personalization of Deep Recommender Models via Meta Learning](https://arxiv.org/abs/2403.00803) | 该论文介绍了一种通过元学习实现个性化深度推荐模型的创新解决方案，能够根据最新用户互动信号频繁更新模型，以确保向不同成员提供相关且更新的体验。 |
| [^33] | [Towards a Theoretical Understanding of Two-Stage Recommender Systems](https://arxiv.org/abs/2403.00802) | 这里是中文总结出的一句话要点: 该论文研究了两阶段推荐系统的理论行为，证明了其向最佳推荐系统的强收敛性，同时揭示了它在输入特征的固有维度上实现更快收敛的特性。 |
| [^34] | [Self-Retrieval: Building an Information Retrieval System with One Large Language Model](https://arxiv.org/abs/2403.00801) | 提出了自主检索(Self-Retrieval)，利用一个大型语言模型完全内化信息检索系统的能力，深度利用大型语言模型在信息检索过程中的能力。 |
| [^35] | [Helen: Optimizing CTR Prediction Models with Frequency-wise Hessian Eigenvalue Regularization](https://arxiv.org/abs/2403.00798) | 本文从优化的角度探讨CTR预测问题，揭示了特征频率与最大Hessian特征值之间的强正相关性，提出频繁出现的特征会趋向于收敛到尖锐的局部最小值，从而导致次优性能。 |
| [^36] | [Ad Recommendation in a Collapsed and Entangled World](https://arxiv.org/abs/2403.00793) | 该论文提出了一个行业广告推荐系统，重点关注学习适当表示的挑战和实践，采用多种方法处理特征表示中的关键挑战，包括嵌入的维度坍缩和跨任务或场景的兴趣纠缠。 |
| [^37] | [Leveraging Contrastive Learning for Few-shot Geolocation of Social Posts](https://arxiv.org/abs/2403.00786) | 提出了ContrastGeo框架，利用对比学习加强了少样本社交媒体帖子地理定位，通过引入Tweet-Location对比学习和匹配目标，并采用在线困难负样本挖掘方法来捕捉帖子和位置之间的关联。 |
| [^38] | [Utilizing BERT for Information Retrieval: Survey, Applications, Resources, and Challenges](https://arxiv.org/abs/2403.00784) | BERT的引入为信息检索领域带来了突破，研究者们将其应用于解决实际问题，并通过综合分析其在信息检索中的应用方法，为学术界和工业界提供了有益的参考。 |
| [^39] | [ChatDiet: Empowering Personalized Nutrition-Oriented Food Recommender Chatbots through an LLM-Augmented Framework](https://arxiv.org/abs/2403.00781) | 这项研究介绍了ChatDiet，一个借助LLM技术构建的框架，能够帮助个性化营养导向食品推荐聊天机器人提供个性化和可解释的推荐。 |
| [^40] | [Text mining in education](https://arxiv.org/abs/2403.00769) | 本文系统概述了当前教育文本挖掘领域的现状，旨在回答教育环境中最常用的文本挖掘技术、最常用的教育资源以及主要的应用或教育目标，同时概述了结论和未来趋势。 |
| [^41] | [Exploring the Impact of Large Language Models on Recommender Systems: An Extensive Review](https://arxiv.org/abs/2402.18590) | 大型语言模型在推荐系统中展现出卓越的推荐能力和语言理解，重塑了推荐任务的基础。 |
| [^42] | [A Language Model based Framework for New Concept Placement in Ontologies](https://arxiv.org/abs/2402.17897) | 提出了一种基于语言模型的框架，用于将从文本中提取的新概念插入到本体中，在边搜索、边形成和增强、边选择三个步骤中分别利用神经方法，并在 SNOMED CT 本体和 MedMentions 实体链接基准上进行了评估 |
| [^43] | [JMLR: Joint Medical LLM and Retrieval Training for Enhancing Reasoning and Professional Question Answering Capability](https://arxiv.org/abs/2402.17887) | JMLR通过联合训练信息检索系统和大型语言模型，在医学领域提高问题回答系统性能，降低计算资源需求，增强模型利用医疗知识进行推理和回答问题的能力。 |
| [^44] | [PromptMM: Multi-Modal Knowledge Distillation for Recommendation with Prompt-Tuning](https://arxiv.org/abs/2402.17188) | 提出了一种通过Prompt-Tuning赋能的PromptMM多模式知识蒸馏方法，用于简化和增强推荐系统，实现自适应的质量蒸馏。 |
| [^45] | [Query Augmentation by Decoding Semantics from Brain Signals](https://arxiv.org/abs/2402.15708) | 提出了一种名为Brain-Aug的方法，通过从脑信号中解码的语义信息增强查询，可以生成更准确的查询，改善文档排序性能，特别适用于模糊查询。 |
| [^46] | [Modality-Aware Integration with Large Language Models for Knowledge-based Visual Question Answering](https://arxiv.org/abs/2402.12728) | 提出了一种模态感知的LLM集成方法（MAIL）用于针对KVQA，通过细致地利用多模态知识来处理图像理解和知识推理。 |
| [^47] | [C-RAG: Certified Generation Risks for Retrieval-Augmented Language Models](https://arxiv.org/abs/2402.03181) | C-RAG是第一个用于认证检索增强语言模型生成风险的框架，通过提供符合风险分析和生成风险的上界，确保生成结果的可信性。 |
| [^48] | [NoMIRACL: Knowing When You Don't Know for Robust Multilingual Retrieval-Augmented Generation](https://arxiv.org/abs/2312.11361) | 建立了用于评估大型语言模型在多语言环境中检索增强生成中的鲁棒性的NoMIRACL数据集，并提出了两个衡量模型鲁棒性的指标：幻觉率和错误率。 |
| [^49] | [Explainable Identification of Hate Speech towards Islam using Graph Neural Networks](https://arxiv.org/abs/2311.04916) | 使用图神经网络解释和识别伊斯兰教仇恨言论，模型在保持出色性能的同时能够解释相关性和因果关系。 |
| [^50] | [Large Language Model based Long-tail Query Rewriting in Taobao Search](https://arxiv.org/abs/2311.03758) | 本文提出了基于大型语言模型的BEQUE框架，通过多阶段流程，有效优化长尾查询、弥补语义差距，提高查询重写效果。 |
| [^51] | [Fairness and Diversity in Recommender Systems: A Survey](https://arxiv.org/abs/2307.04644) | 推荐系统研究探索了公平性和多样性之间的联系，通过扩展对用户和物品级别多样性的理解，重新解释了公平性研究，提升了对公平性相关工作的认识 |
| [^52] | [Attention Is Not the Only Choice: Counterfactual Reasoning for Path-Based Explainable Recommendation.](http://arxiv.org/abs/2401.05744) | 这项研究提出了一种基于反事实推理的可解释框架，用于路径推荐，通过学习路径的可解释权重来替代关注权重。这种框架比传统的关注机制更稳定且更符合人类直觉。 |
| [^53] | [Starling: An I/O-Efficient Disk-Resident Graph Index Framework for High-Dimensional Vector Similarity Search on Data Segment.](http://arxiv.org/abs/2401.02116) | Starling是一种I/O高效的基于磁盘的图索引框架，用于在数据片段上进行高维向量相似性搜索，在准确性、效率和空间成本之间取得平衡。 |
| [^54] | [Data Augmentation for Conversational AI.](http://arxiv.org/abs/2309.04739) | 本教程提供了对话式人工智能中数据增强的综述，包括对话增强、开放域和任务导向的对话生成以及评估模型。此外，还讨论了当前的挑战和未来的发展方向，以帮助推动该领域的发展。 |
| [^55] | [Mol-Instructions: A Large-Scale Biomolecular Instruction Dataset for Large Language Models.](http://arxiv.org/abs/2306.08018) | Mol-Instructions是一个专门为生物分子领域设计的综合指令数据集，可以显著提高大语言模型在生物领域中的适应能力和认知敏锐度。 |

# 详细

[^1]: CODE-ACCORD：用于规则生成的建筑法规数据语料库

    CODE-ACCORD: A Corpus of Building Regulatory Data for Rule Generation towards Automatic Compliance Checking

    [https://arxiv.org/abs/2403.02231](https://arxiv.org/abs/2403.02231)

    介绍了一个独特的数据集CODE-ACCORD，旨在解决自动合规检查中解释建筑法规的挑战，成为机器可读规则生成的基础。

    

    自动合规检查（ACC）在建筑、工程和施工（AEC）领域内的自动合规检查需要自动解释建筑法规，以发挥其全部潜力。然而，从文本规则中提取信息以将其转换为机器可读格式由于自然语言的复杂性以及仅能支持先进的机器学习技术的有限资源而成为一个挑战。为了解决这一挑战，我们介绍了一个独特的数据集CODE-ACCORD，这是在欧盟Horizon ACCORD项目下编制的。CODE-ACCORD包含862个来自英格兰和芬兰建筑法规的自包含句子。与我们的核心目标一致，即促进从文本中提取信息以生成机器可读规则，每个句子都注释了实体和关系。实体代表特定组件，如“窗户”和“烟雾探测器”，而re

    arXiv:2403.02231v1 Announce Type: new  Abstract: Automatic Compliance Checking (ACC) within the Architecture, Engineering, and Construction (AEC) sector necessitates automating the interpretation of building regulations to achieve its full potential. However, extracting information from textual rules to convert them to a machine-readable format has been a challenge due to the complexities associated with natural language and the limited resources that can support advanced machine-learning techniques. To address this challenge, we introduce CODE-ACCORD, a unique dataset compiled under the EU Horizon ACCORD project. CODE-ACCORD comprises 862 self-contained sentences extracted from the building regulations of England and Finland. Aligned with our core objective of facilitating information extraction from text for machine-readable rule generation, each sentence was annotated with entities and relations. Entities represent specific components such as "window" and "smoke detectors", while re
    
[^2]: 评估神经排序器的可解释性

    Evaluating the Explainability of Neural Rankers

    [https://arxiv.org/abs/2403.01981](https://arxiv.org/abs/2403.01981)

    本文旨在评估神经排序器的可解释性，提出了一个共同的评估平台，要求每个模型除了返回排名的文档列表外，还需返回每个文档的解释单元或解释理由列表。

    

    信息检索模型已经经历了从无监督统计方法到基于特征的监督方法，再到完全数据驱动的利用大型语言模型预训练的范式转变。虽然搜索模型的增加复杂性已经能够展示在效果方面的改进（以检索结果的相关性为衡量标准），但一个值得彻底检查的问题是 - "这些模型的可解释性有多高？"，这就是本文的评估目标。具体而言，我们提出了一个共同的评估平台，系统地评估任何排序模型的可解释性（说明算法对于所有待评估模型都是相同的）。在我们提出的框架中，每个模型除了返回一个排名的文档列表之外，还需要返回每个文档的解释单元或解释理由的列表。

    arXiv:2403.01981v1 Announce Type: new  Abstract: Information retrieval models have witnessed a paradigm shift from unsupervised statistical approaches to feature-based supervised approaches to completely data-driven ones that make use of the pre-training of large language models. While the increasing complexity of the search models have been able to demonstrate improvements in effectiveness (measured in terms of relevance of top-retrieved results), a question worthy of a thorough inspection is - "how explainable are these models?", which is what this paper aims to evaluate. In particular, we propose a common evaluation platform to systematically evaluate the explainability of any ranking model (the explanation algorithm being identical for all the models that are to be evaluated). In our proposed framework, each model, in addition to returning a ranked list of documents, also requires to return a list of explanation units or rationales for each document. This meta-information from each
    
[^3]: 推荐评论者标识的遗漏引用：一项新任务、数据集和基准

    Recommending Missed Citations Identified by Reviewers: A New Task, Dataset and Baselines

    [https://arxiv.org/abs/2403.01873](https://arxiv.org/abs/2403.01873)

    该论文定义了一个新任务——推荐评论者标识的遗漏引用(RMC)，并构建了对应的数据集CitationR，以改进完整论文的引用。

    

    随着科学出版物数量的爆炸性增长，全面和恰当地引用已经成为一项具有挑战性的任务。目前的引用推荐系统旨在为给定文本环境或草稿推荐一系列科学论文。然而，现有工作都没有着重于已经包含在完整论文中的引用，这些引用并不完美，仍有很大的改进空间。在同行评审的场景中，提交的论文被评论者标识为遗漏重要引用是一种常见现象。这可能会对所呈现的研究的可信度和有效性产生负面影响。为了帮助改进完整论文的引用，我们首先定义了一个新任务——推荐评论者标识的遗漏引用（RMC），并构建了一个相应的专家标记数据集称为CitationR。我们在CitationR上对几种最先进方法进行了广泛评估。此外，我们提出了一种新的

    arXiv:2403.01873v1 Announce Type: new  Abstract: Citing comprehensively and appropriately has become a challenging task with the explosive growth of scientific publications. Current citation recommendation systems aim to recommend a list of scientific papers for a given text context or a draft paper. However, none of the existing work focuses on already included citations of full papers, which are imperfect and still have much room for improvement. In the scenario of peer reviewing, it is a common phenomenon that submissions are identified as missing vital citations by reviewers. This may lead to a negative impact on the credibility and validity of the research presented. To help improve citations of full papers, we first define a novel task of Recommending Missed Citations Identified by Reviewers (RMC) and construct a corresponding expert-labeled dataset called CitationR. We conduct an extensive evaluation of several state-of-the-art methods on CitationR. Furthermore, we propose a new
    
[^4]: 大规模最近邻搜索的图分区技术

    Unleashing Graph Partitioning for Large-Scale Nearest Neighbor Search

    [https://arxiv.org/abs/2403.01797](https://arxiv.org/abs/2403.01797)

    本文设计了可与任何分区方法一起使用的简单有效的路由方法，并在性能上证明了强大的理论保证。

    

    我们考虑将大规模近似最近邻搜索（ANNS）问题分解为较小子问题的基本问题。目标是将输入点分区为保留邻域的碎片，以便任何点的最近邻居仅包含在少数碎片中。当查询到达时，将使用路由算法识别应该搜索其最近邻居的碎片。这种方法构建了分布式ANNS的骨干，其中数据集非常庞大，必须跨多台机器进行拆分。本文设计了简单且高效的路由方法，并证明了它们性能的强大理论保证。我们的路由算法的一个关键特征是它们是固有模块化的，可以与任何分区方法一起使用。这解决了以往方法的一个关键缺点，即路由算法与其关联部分紧密相连。

    arXiv:2403.01797v1 Announce Type: cross  Abstract: We consider the fundamental problem of decomposing a large-scale approximate nearest neighbor search (ANNS) problem into smaller sub-problems. The goal is to partition the input points into neighborhood-preserving shards, so that the nearest neighbors of any point are contained in only a few shards. When a query arrives, a routing algorithm is used to identify the shards which should be searched for its nearest neighbors. This approach forms the backbone of distributed ANNS, where the dataset is so large that it must be split across multiple machines.   In this paper, we design simple and highly efficient routing methods, and prove strong theoretical guarantees on their performance. A crucial characteristic of our routing algorithms is that they are inherently modular, and can be used with any partitioning method. This addresses a key drawback of prior approaches, where the routing algorithms are inextricably linked to their associated
    
[^5]: 朝向自包含答案的方向：会话搜索中基于实体的答案重写

    Towards Self-Contained Answers: Entity-Based Answer Rewriting in Conversational Search

    [https://arxiv.org/abs/2403.01747](https://arxiv.org/abs/2403.01747)

    本文针对会话搜索中的基于实体的答案重写，提出了两种答案重写策略，以改善用户体验。

    

    会话信息检索（CIS）是一种新兴的知识获取和探索性搜索范式。传统的网络搜索界面可以轻松探索实体，但在会话环境中受限于带宽有限的界面。本文探讨了在CIS中重写答案的方法，以便用户可以理解答案而无需求助外部服务或来源。具体而言，我们关注突出的实体--对于理解答案至关重要的实体。作为我们的第一个贡献，我们创建了一个带有突出实体注释的对话数据集。我们对收集到的数据进行分析后发现，大多数答案包含了突出实体。作为我们的第二个贡献，我们提出了两种旨在改善CIS中用户体验的答案重写策略。其一通过内联定义突出实体来扩展答案，使答案自包含。

    arXiv:2403.01747v1 Announce Type: cross  Abstract: Conversational information-seeking (CIS) is an emerging paradigm for knowledge acquisition and exploratory search. Traditional web search interfaces enable easy exploration of entities, but this is limited in conversational settings due to the limited-bandwidth interface. This paper explore ways to rewrite answers in CIS, so that users can understand them without having to resort to external services or sources. Specifically, we focus on salient entities -- entities that are central to understanding the answer. As our first contribution, we create a dataset of conversations annotated with entities for saliency. Our analysis of the collected data reveals that the majority of answers contain salient entities. As our second contribution, we propose two answer rewriting strategies aimed at improving the overall user experience in CIS. One approach expands answers with inline definitions of salient entities, making the answer self-contained
    
[^6]: NoteLLM: 一种可检索的大型语言模型，用于笔记推荐

    NoteLLM: A Retrievable Large Language Model for Note Recommendation

    [https://arxiv.org/abs/2403.01744](https://arxiv.org/abs/2403.01744)

    本文提出了一种名为NoteLLM的新颖统一框架，利用大型语言模型(LLMs)来实现物品到物品(I2I)的笔记推荐，通过学习生成哈希标签/类别潜在地增强笔记嵌入，提高了对关键笔记信息的压缩。

    

    人们喜欢在在线社区内分享“笔记”，包括他们的经验。因此，推荐与用户兴趣相符的笔记已经成为一项关键任务。现有的在线方法只将笔记输入到基于BERT的模型中，用于生成笔记嵌入以评估相似性。然而，它们可能未充分利用一些重要的线索，例如哈希标签或类别，这些代表了笔记的关键概念。事实上，学习生成哈希标签/类别可以潜在地增强笔记嵌入，二者都将重要的笔记信息压缩为有限内容。此外，大型语言模型（LLMs）在理解自然语言方面明显优于BERT。将LLMs引入笔记推荐是很有前途的。在本文中，我们提出了一种名为NoteLLM的新颖统一框架，利用LLMs来处理物品到物品（I2I）笔记推荐。具体来说，我们利用笔记压缩提示来压缩一条笔记

    arXiv:2403.01744v1 Announce Type: new  Abstract: People enjoy sharing "notes" including their experiences within online communities. Therefore, recommending notes aligned with user interests has become a crucial task. Existing online methods only input notes into BERT-based models to generate note embeddings for assessing similarity. However, they may underutilize some important cues, e.g., hashtags or categories, which represent the key concepts of notes. Indeed, learning to generate hashtags/categories can potentially enhance note embeddings, both of which compress key note information into limited content. Besides, Large Language Models (LLMs) have significantly outperformed BERT in understanding natural languages. It is promising to introduce LLMs into note recommendation. In this paper, we propose a novel unified framework called NoteLLM, which leverages LLMs to address the item-to-item (I2I) note recommendation. Specifically, we utilize Note Compression Prompt to compress a note 
    
[^7]: TweetInfo：一个互动系统以减轻在线伤害

    TweetInfo: An Interactive System to Mitigate Online Harm

    [https://arxiv.org/abs/2403.01646](https://arxiv.org/abs/2403.01646)

    TweetInfo是一个互动系统，通过提供有关帖子的元信息来减轻有害内容的消费，重点关注仇恨言论和不实信息。

    

    社交网络站点（SNSs）上活跃用户的增加也观察到社交媒体站点上有害内容的增加。有害内容被描述为一种损害或欺骗个人或一群用户的不当活动。除了现有的检测不实信息和仇恨言论的方法外，用户仍然需要充分了解SNSs上内容的有害性。本研究提出了一个名为TweetInfo的用户互动系统，通过提供有关帖子的元信息来减轻有害内容的消费。它专注于两种类型的有害内容：仇恨言论和不实信息。TweetInfo通过内容分析提供了有关推文的见解。根据以往研究，我们选择了一系列元信息。我们提供了根据元信息Bot、仇恨言论、错误信息、已验证账户、情感、推文类别、语言对内容进行过滤的选项。所提出的用户界面...

    arXiv:2403.01646v1 Announce Type: cross  Abstract: The increase in active users on social networking sites (SNSs) has also observed an increase in harmful content on social media sites. Harmful content is described as an inappropriate activity to harm or deceive an individual or a group of users. Alongside existing methods to detect misinformation and hate speech, users still need to be well-informed about the harmfulness of the content on SNSs. This study proposes a user-interactive system TweetInfo for mitigating the consumption of harmful content by providing metainformation about the posts. It focuses on two types of harmful content: hate speech and misinformation. TweetInfo provides insights into tweets by doing content analysis. Based on previous research, we have selected a list of metainformation. We offer the option to filter content based on metainformation Bot, Hate Speech, Misinformation, Verified Account, Sentiment, Tweet Category, Language. The proposed user interface all
    
[^8]: 使用Proximity Indices测量加密技术中的技术融合：基于OpenAlex的文本挖掘和文献计量分析

    Measuring Technological Convergence in Encryption Technologies with Proximity Indices: A Text Mining and Bibliometric Analysis using OpenAlex

    [https://arxiv.org/abs/2403.01601](https://arxiv.org/abs/2403.01601)

    该研究利用文本挖掘和文献计量分析方法，基于OpenAlex目录，预测了加密技术的技术接近度指数，发现了区块链与公钥密码学之间显著的技术融合。

    

    确定网络安全领域新兴技术之间的技术融合对于推动科学发展和促进创新至关重要。我们的方法不同于以往研究仅关注论文与技术概念之间的二元关系，而是利用归因分数增强研究论文之间的关系，结合关键词、引用率和合作状态等与特定技术概念相关的因素。提出的方法整合了文本挖掘和文献计量分析，利用“OpenAlex”目录制定并预测加密技术的技术接近度指数。我们的案例研究结果突显了区块链与公钥密码学之间显著的融合，表现为接近度指数的增加。这些结果为那些考虑在这些领域进行投资的人提供了宝贵的战略见解。

    arXiv:2403.01601v1 Announce Type: cross  Abstract: Identifying technological convergence among emerging technologies in cybersecurity is crucial for advancing science and fostering innovation. Unlike previous studies focusing on the binary relationship between a paper and the concept it attributes to technology, our approach utilizes attribution scores to enhance the relationships between research papers, combining keywords, citation rates, and collaboration status with specific technological concepts. The proposed method integrates text mining and bibliometric analyses to formulate and predict technological proximity indices for encryption technologies using the "OpenAlex" catalog. Our case study findings highlight a significant convergence between blockchain and public-key cryptography, evidenced by the increasing proximity indices. These results offer valuable strategic insights for those contemplating investments in these domains.
    
[^9]: 逻辑规则作为解释法律案例检索的论文

    Logic Rules as Explanations for Legal Case Retrieval

    [https://arxiv.org/abs/2403.01457](https://arxiv.org/abs/2403.01457)

    本文提出了神经符号增强的法律案例检索（NS-LCR）框架，通过学习案例级别和法律级别的逻辑规则，将规则以神经符号方式集成到检索过程中，以提供逻辑且可解释的解释。

    

    在本文中，我们探讨了使用逻辑规则来解释法律案例检索结果的问题。这项任务对于法律案例检索至关重要，因为用户（如律师或法官）具有高度专业化，需要系统在做出法律决策之前提供逻辑、忠实和可解释的解释。最近，研究工作旨在学习可解释的法律案例检索模型。然而，这些方法通常从法律案例中选择基本原理（关键句）作为解释，未能提供忠实和逻辑正确的解释。在本文中，我们提出了神经符号增强的法律案例检索（NS-LCR）框架，该框架通过学习案例级别和法律级别的逻辑规则来明确地对法律案例的匹配进行推理。然后将学习到的规则以神经符号方式集成到检索过程中。由于逻辑和可解释性的特性...

    arXiv:2403.01457v1 Announce Type: cross  Abstract: In this paper, we address the issue of using logic rules to explain the results from legal case retrieval. The task is critical to legal case retrieval because the users (e.g., lawyers or judges) are highly specialized and require the system to provide logical, faithful, and interpretable explanations before making legal decisions. Recently, research efforts have been made to learn explainable legal case retrieval models. However, these methods usually select rationales (key sentences) from the legal cases as explanations, failing to provide faithful and logically correct explanations. In this paper, we propose Neural-Symbolic enhanced Legal Case Retrieval (NS-LCR), a framework that explicitly conducts reasoning on the matching of legal cases through learning case-level and law-level logic rules. The learned rules are then integrated into the retrieval process in a neuro-symbolic manner. Benefiting from the logic and interpretable natu
    
[^10]: LM4OPT：揭示大型语言模型在制定数学优化问题中潜力

    LM4OPT: Unveiling the Potential of Large Language Models in Formulating Mathematical Optimization Problems

    [https://arxiv.org/abs/2403.01342](https://arxiv.org/abs/2403.01342)

    本研究比较了几种知名的大型语言模型在翻译语言描述为数学优化问题中的表现，发现GPT-4在一次场景中表现出色，并引入了“LM4OPT”框架进行Llama-2-7b的渐进微调。

    

    在自然语言处理这一快速发展的领域中，将语言描述翻译成数学优化问题的数学公式是一项巨大挑战，要求大型语言模型（LLMs）具备复杂的理解和处理能力。本研究比较了几种知名的LLMs，包括GPT-3.5、GPT-4和Llama-2-7b，在零次和一次设置中对这一任务的表现。我们的发现显示出GPT-4在一次场景中的卓越表现。其中心部分是引入了“LM4OPT”，这是一个利用噪声嵌入和专门数据集进行Llama-2-7b渐进微调的框架。然而，这项研究突出了小型模型（如Llama-2-7b）在处理冗长和复杂输入上的上下文理解能力与更大型对应模型之间存在显著差距。我们的实证研究利用了NL4Opt

    arXiv:2403.01342v1 Announce Type: new  Abstract: In the rapidly evolving field of natural language processing, the translation of linguistic descriptions into mathematical formulation of optimization problems presents a formidable challenge, demanding intricate understanding and processing capabilities from Large Language Models (LLMs). This study compares prominent LLMs, including GPT-3.5, GPT-4, and Llama-2-7b, in zero-shot and one-shot settings for this task. Our findings show GPT-4's superior performance, particularly in the one-shot scenario. A central part of this research is the introduction of `LM4OPT,' a progressive fine-tuning framework for Llama-2-7b that utilizes noisy embeddings and specialized datasets. However, this research highlights a notable gap in the contextual understanding capabilities of smaller models such as Llama-2-7b compared to larger counterparts, especially in processing lengthy and complex input contexts. Our empirical investigation, utilizing the NL4Opt
    
[^11]: 在线采购中的供应商推荐

    Supplier Recommendation in Online Procurement

    [https://arxiv.org/abs/2403.01301](https://arxiv.org/abs/2403.01301)

    本研究提出了一个推荐系统，用于在道路货运在线采购中辅助进行供应商发现，能够提供个性化的供应商推荐，考虑到客户的需求和偏好。

    

    供应链优化对于健康和盈利的企业至关重要。许多公司使用在线采购系统与供应商签订合同。邀请最具竞争力的供应商竞标这些合同至关重要。在这项工作中，我们提出了一个推荐系统，以协助在道路货运在线采购中进行供应商发现。我们的系统能够提供个性化的供应商推荐，考虑到客户的需求和偏好。这是推荐系统的一种新颖应用，需要设计选择以符合在线采购的独特要求。我们使用真实数据进行的初步结果令人鼓舞。

    arXiv:2403.01301v1 Announce Type: cross  Abstract: Supply chain optimization is key to a healthy and profitable business. Many companies use online procurement systems to agree contracts with suppliers. It is vital that the most competitive suppliers are invited to bid for such contracts. In this work, we propose a recommender system to assist with supplier discovery in road freight online procurement. Our system is able to provide personalized supplier recommendations, taking into account customer needs and preferences. This is a novel application of recommender systems, calling for design choices that fit the unique requirements of online procurement. Our preliminary results, using real-world data, are promising.
    
[^12]: COOL：一种融合时空图神经网络用于交通预测的共同视角

    COOL: A Conjoint Perspective on Spatio-Temporal Graph Neural Network for Traffic Forecasting

    [https://arxiv.org/abs/2403.01091](https://arxiv.org/abs/2403.01091)

    本文提出了一种名为COOL的Conjoint Spatio-Temporal图神经网络，旨在共同捕捉交通预测中的高阶关系。

    

    本文研究交通预测，旨在根据历史情况预测交通的未来状态。鉴于其对多个场景的持续关注，并促进了许多下游应用程序的发展，例如城市规划和交通管理，该问题已经受到越来越多的关注。然而，由于现有方法倾向于独立地建模时空关系，因此未能充分考虑两者的复杂高阶互动，导致现有方法的效果不佳。此外，交通预测中的过渡模式的多样性使得现有方法难以捕捉，需要更深入地探索这种多样性。为此，本文提出了Conjoint Spatio-Temporal图神经网络（缩写为COOL），它从先前和后续信息中建模异构图，以共同捕捉高阶互动

    arXiv:2403.01091v1 Announce Type: cross  Abstract: This paper investigates traffic forecasting, which attempts to forecast the future state of traffic based on historical situations. This problem has received ever-increasing attention in various scenarios and facilitated the development of numerous downstream applications such as urban planning and transportation management. However, the efficacy of existing methods remains sub-optimal due to their tendency to model temporal and spatial relationships independently, thereby inadequately accounting for complex high-order interactions of both worlds. Moreover, the diversity of transitional patterns in traffic forecasting makes them challenging to capture for existing approaches, warranting a deeper exploration of their diversity. Toward this end, this paper proposes Conjoint Spatio-Temporal graph neural network (abbreviated as COOL), which models heterogeneous graphs from prior and posterior information to conjointly capture high-order sp
    
[^13]: BasedAI：用于零知识大型语言模型（ZK-LLMs）的去中心化P2P网络

    BasedAI: A decentralized P2P network for Zero Knowledge Large Language Models (ZK-LLMs)

    [https://arxiv.org/abs/2403.01008](https://arxiv.org/abs/2403.01008)

    BasedAI引入了Cerberus Squeezing机制，将标准大型语言模型转化为加密的零知识模型，显著提高了在全同态加密计算环境中的性能。

    

    BasedAI是一个由分布式机器构成的网络，引入了去中心化基础设施，能够将全同态加密（FHE）与连接到其网络的任何大型语言模型（LLM）相结合。所提出的框架将一种名为“Cerberus Squeezing”的默认机制嵌入到挖矿过程中，实现了将标准LLM转化为加密的零知识LLM，或“ZK-LLMs”，利用生成对抗网络的见解来实现数据隐私保护。这种新颖的量化机制赋予BasedAI矿工能力，能够处理并响应来自用户与LLMs的交互得到的提示，而无需解密查询或相应内容。引入Cerberus Squeezing显著改善了当前FHE兼容计算环境中由量化函数引起的性能下降问题，通过积极优化用户、矿工和验证者之间的通信调用。

    arXiv:2403.01008v1 Announce Type: cross  Abstract: BasedAI is a distributed network of machines which introduces decentralized infrastructure capable of integrating Fully Homomorphic Encryption (FHE) with any large language model (LLM) connected to its network. The proposed framework embeds a default mechanism, called "Cerberus Squeezing", into the mining process which enables the transformation of a standard LLMs into encrypted zero-knowledge LLMs, or "ZK-LLMs", leveraging insights from generative adversarial networks for data privacy. This novel quantization mechanism empowers BasedAI miners to process and respond to prompts derived from User interaction with LLMs without the need for decrypting ei- ther the queries or their corresponding responses. The introduction of Cerberus Squeezing significantly improves performance degradation caused by quantized functions in current FHE-compliant computing environments by proactively optimizing calls between users, miners, and validators.
    
[^14]: 用于提高电子商务搜索相关性的图形和语言模型可解释的集成

    An Interpretable Ensemble of Graph and Language Models for Improving Search Relevance in E-Commerce

    [https://arxiv.org/abs/2403.00923](https://arxiv.org/abs/2403.00923)

    该研究提出了一个可解释的图形和语言模型集成方法，用于提高电子商务领域中搜索相关性，弥补了现有模型在实际部署中的泛化能力和可解释性方面的不足。

    

    在电子商务领域，搜索相关性的问题是一个具有挑战性的问题，因为它涉及理解用户的简短微妙查询的意图，并将其与目录中的适当产品相匹配。传统上，这个问题是通过使用语言模型（LMs）和图神经网络（GNNs）来应对的，以捕捉语义和产品间行为信号。然而，新架构的快速发展造成了研究和这些技术实际应用之间的鸿沟。评估这些模型在部署中的泛化能力需要对复杂的现实世界数据集进行广泛的实验，这可能并不简单且昂贵。此外，这种模型通常在不为人类所理解的潜在空间表示上运行，这使得评估和比较不同模型的有效性变得困难。这种缺乏可解释性阻碍了开发和ad

    arXiv:2403.00923v1 Announce Type: cross  Abstract: The problem of search relevance in the E-commerce domain is a challenging one since it involves understanding the intent of a user's short nuanced query and matching it with the appropriate products in the catalog. This problem has traditionally been addressed using language models (LMs) and graph neural networks (GNNs) to capture semantic and inter-product behavior signals, respectively. However, the rapid development of new architectures has created a gap between research and the practical adoption of these techniques. Evaluating the generalizability of these models for deployment requires extensive experimentation on complex, real-world datasets, which can be non-trivial and expensive. Furthermore, such models often operate on latent space representations that are incomprehensible to humans, making it difficult to evaluate and compare the effectiveness of different models. This lack of interpretability hinders the development and ad
    
[^15]: 精确推荐的端到端图-序列表示学习

    End-to-end Graph-Sequential Representation Learning for Accurate Recommendations

    [https://arxiv.org/abs/2403.00895](https://arxiv.org/abs/2403.00895)

    本文提出了一个新颖的多重表示学习框架，有效地结合了基于序列和基于图的推荐方法，显著改善了推荐性能。

    

    近年来推荐系统的许多新进展集中在开发基于序列和基于图的方法上。这两种方法在建模行为数据中的复杂关系方面都证明了其有效性，从而在个性化排名和下一个推荐任务中取得了有益的成果，同时保持了良好的可扩展性。然而，它们从数据中捕捉到的信号截然不同。前者直接通过与最近物品的有序交互来表示用户，而后者旨在捕捉交互图中的间接依赖关系。本文提出了一个新颖的多重表示学习框架，利用这两种范式之间的协同作用。我们在几个数据集上的实证评估表明，利用所提出的框架相互训练序列和图组件显著改善了推荐性能。

    arXiv:2403.00895v1 Announce Type: cross  Abstract: Many recent advancements in recommender systems have focused on developing sequence-based and graph-based approaches. Both approaches proved useful in modeling intricate relationships within behavioral data, leading to promising outcomes in personalized ranking and next-item recommendation tasks while maintaining good scalability. However, they capture very different signals from data. While the former approach represents users directly through ordered interactions with recent items, the latter one aims to capture indirect dependencies across the interactions graph. This paper presents a novel multi-representational learning framework that exploits the synergies between these two paradigms. Our empirical evaluation on several datasets demonstrates that mutual training of sequential and graph components with the proposed framework significantly improves recommendations performance.
    
[^16]: 利用LLMs进行元数据丰富化的受控词汇列标题文本分类

    Text classification of column headers with a controlled vocabulary: leveraging LLMs for metadata enrichment

    [https://arxiv.org/abs/2403.00884](https://arxiv.org/abs/2403.00884)

    通过利用LLMs，我们提出了一种方法，使用ChatGPT-3.5、GoogleBard和GoogleGemini对列标题进行主题注释的元数据丰富化，研究它们在对领域特定主题进行分类的能力，并评估其内部一致性、机器对齐性和人机一致性。ChatGPT和GoogleGemini在内部一致性以及与人一致性方面优于GoogleBard。

    

    传统的数据集检索系统主要在元数据信息而非数据值上建立索引。因此主要依赖于手动注释和高质量的元数据，这些过程被认为是耗时且难以自动化的。我们提出了一种方法，利用三种大型语言模型（LLMs）支持对列标题进行主题注释的元数据丰富化：ChatGPT-3.5、GoogleBard和GoogleGemini。我们研究了LLMs基于受控词汇的领域特定主题对列标题进行分类的能力。我们通过评估LLMs的内部一致性、机器间对齐以及人机对主题分类任务的一致性来评估我们的方法。此外，我们还探讨了上下文信息（即数据集描述）对分类结果的影响。我们的结果表明，ChatGPT和GoogleGemini在内部一致性以及LLM与人之间的一致性方面表现优于GoogleBard。

    arXiv:2403.00884v1 Announce Type: cross  Abstract: Traditional dataset retrieval systems index on metadata information rather than on the data values. Thus relying primarily on manual annotations and high-quality metadata, processes known to be labour-intensive and challenging to automate. We propose a method to support metadata enrichment with topic annotations of column headers using three Large Language Models (LLMs): ChatGPT-3.5, GoogleBard and GoogleGemini. We investigate the LLMs ability to classify column headers based on domain-specific topics from a controlled vocabulary. We evaluate our approach by assessing the internal consistency of the LLMs, the inter-machine alignment, and the human-machine agreement for the topic classification task. Additionally, we investigate the impact of contextual information (i.e. dataset description) on the classification outcomes. Our results suggest that ChatGPT and GoogleGemini outperform GoogleBard for internal consistency as well as LLM-hum
    
[^17]: 基于因果推断的双粒度药物推荐

    Dual-Granularity Medication Recommendation Based on Causal Inference

    [https://arxiv.org/abs/2403.00880](https://arxiv.org/abs/2403.00880)

    提出了DGMed框架，利用因果推断和创新的特征对齐方法进行双粒度药物推荐

    

    随着医疗需求增长和机器学习技术的进步，基于人工智能的诊断和治疗系统备受关注。药物推荐旨在将患者的长期健康记录与医学知识整合，为特定疾病推荐准确和安全的药物组合。然而，大多数现有研究将药物推荐系统仅视为传统推荐系统的变体，忽视了药物和疾病之间的异质性。为解决这一挑战，我们提出了DGMed，一个用于药物推荐的框架。DGMed利用因果推断揭示医学实体之间的联系，并提出了一种创新的特征对齐方法来解决异质性问题。具体而言，该研究首先应用因果推断分析历史记录中药物对特定疾病的量化治疗效果，揭示...

    arXiv:2403.00880v1 Announce Type: cross  Abstract: As medical demands grow and machine learning technology advances, AI-based diagnostic and treatment systems are garnering increasing attention. Medication recommendation aims to integrate patients' long-term health records with medical knowledge, recommending accuracy and safe medication combinations for specific conditions. However, most existing researches treat medication recommendation systems merely as variants of traditional recommendation systems, overlooking the heterogeneity between medications and diseases. To address this challenge, we propose DGMed, a framework for medication recommendation. DGMed utilizes causal inference to uncover the connections among medical entities and presents an innovative feature alignment method to tackle heterogeneity issues. Specifically, this study first applies causal inference to analyze the quantified therapeutic effects of medications on specific diseases from historical records, uncoverin
    
[^18]: Disaggregated Multi-Tower: 面向拓扑感知的高效大规模推荐建模技术

    Disaggregated Multi-Tower: Topology-aware Modeling Technique for Efficient Large-Scale Recommendation

    [https://arxiv.org/abs/2403.00877](https://arxiv.org/abs/2403.00877)

    Disaggregated Multi-Tower提出了一种面向拓扑感知的建模技术，通过SPTT、TM和TP三个组件实现了高效的大规模推荐，加速性能提升了1.9倍。

    

    我们研究了深度学习推荐模型的扁平架构、常见的分布式训练模式和分层数据中心拓扑之间的不匹配。为了解决相关的低效性，我们提出了Disaggregated Multi-Tower（DMT），这是一种建模技术，包括（1）语义保留的Tower Transform（SPTT），一个将单片全局嵌入查找过程分解为不相交塔以利用数据中心位置关系的新型训练模式；（2）Tower Module（TM），一个附加到每个塔的协同稠密组件，通过分层特征交互降低模型复杂性和通信量；和（3）Tower Partitioner（TP），一个特征分区器，系统地创建具有有意义特征交互和负载平衡分配的塔，通过学习的嵌入来保持模型质量和训练吞吐量。我们展示了DMT相比于最新的方法可以实现高达1.9倍的加速。

    arXiv:2403.00877v1 Announce Type: new  Abstract: We study a mismatch between the deep learning recommendation models' flat architecture, common distributed training paradigm and hierarchical data center topology. To address the associated inefficiencies, we propose Disaggregated Multi-Tower (DMT), a modeling technique that consists of (1) Semantic-preserving Tower Transform (SPTT), a novel training paradigm that decomposes the monolithic global embedding lookup process into disjoint towers to exploit data center locality; (2) Tower Module (TM), a synergistic dense component attached to each tower to reduce model complexity and communication volume through hierarchical feature interaction; and (3) Tower Partitioner (TP), a feature partitioner to systematically create towers with meaningful feature interactions and load balanced assignments to preserve model quality and training throughput via learned embeddings. We show that DMT can achieve up to 1.9x speedup compared to the state-of-th
    
[^19]: LLM-Ensemble: 用于电子商务产品属性值提取的最佳大型语言模型集成方法

    LLM-Ensemble: Optimal Large Language Model Ensemble Method for E-commerce Product Attribute Value Extraction

    [https://arxiv.org/abs/2403.00863](https://arxiv.org/abs/2403.00863)

    提出了一种名为LLM-ensemble的算法，用于集成不同大型语言模型，以提高电子商务产品属性值提取的性能。

    

    arXiv:2403.00863v1 公告类型:跨领域摘要: 产品属性值提取是自然语言处理（NLP）和当代电子商务行业中至关重要的组成部分。提供精确的产品属性值在确保高质量推荐和提升客户满意度方面至关重要。最近出现的大型语言模型（LLMs）在许多属性提取任务中表现出最新技术水平，而无需进行领域特定的训练数据。然而，由于数据、架构和超参数的多样性，不同LLMs表现出不同的优势和劣势。这种变化使它们彼此互补，没有哪个LLM能完全压倒其他LLM。考虑到LLMs的多样优势和劣势，开发一种利用它们互补潜力的集成方法变得必要。在本文中，我们提出了一种名为LLM-ensemble的新算法，用于集成不同LLMs。

    arXiv:2403.00863v1 Announce Type: cross  Abstract: Product attribute value extraction is a pivotal component in Natural Language Processing (NLP) and the contemporary e-commerce industry. The provision of precise product attribute values is fundamental in ensuring high-quality recommendations and enhancing customer satisfaction. The recently emerging Large Language Models (LLMs) have demonstrated state-of-the-art performance in numerous attribute extraction tasks, without the need for domain-specific training data. Nevertheless, varying strengths and weaknesses are exhibited by different LLMs due to the diversity in data, architectures, and hyperparameters. This variation makes them complementary to each other, with no single LLM dominating all others. Considering the diverse strengths and weaknesses of LLMs, it becomes necessary to develop an ensemble method that leverages their complementary potentials. In this paper, we propose a novel algorithm called LLM-ensemble to ensemble diffe
    
[^20]: 改进的在线学习算法用于广告拍卖中的点击率预测

    Improved Online Learning Algorithms for CTR Prediction in Ad Auctions

    [https://arxiv.org/abs/2403.00845](https://arxiv.org/abs/2403.00845)

    该研究致力于解决广告拍卖中的在线学习问题，提出了针对广告主两种不同战略行为模型的在线机制，其中一种在最坏情况下取得了紧致的遗憾界限，另一种则解决了更复杂的长期效用优化问题。

    

    在这项工作中，我们研究了广告拍卖中收入最大化的在线学习问题，其中卖方需要学习每个广告候选的点击率(CTR)，并通过按点击付费的方式收取获胜者的价格。我们关注广告主的两种战略行为模型。首先，我们假设广告主完全近视；即在每一轮中，他们只针对当前轮次最大化他们的效用。在这种情况下，我们基于置信上界开发了一个在线机制，在最坏情况下实现了严格的$O(\sqrt{T})$遗憾，当值在所有拍卖中静态并且最高期望值(即值乘以他们的CTR)与次高期望值广告之间存在差距时，存在负遗憾。接下来，我们假设广告主是非近视的，并关心他们的长期效用。这种设置要复杂得多，因为广告主有动机

    arXiv:2403.00845v1 Announce Type: cross  Abstract: In this work, we investigate the online learning problem of revenue maximization in ad auctions, where the seller needs to learn the click-through rates (CTRs) of each ad candidate and charge the price of the winner through a pay-per-click manner. We focus on two models of the advertisers' strategic behaviors. First, we assume that the advertiser is completely myopic; i.e.~in each round, they aim to maximize their utility only for the current round. In this setting, we develop an online mechanism based on upper-confidence bounds that achieves a tight $O(\sqrt{T})$ regret in the worst-case and negative regret when the values are static across all the auctions and there is a gap between the highest expected value (i.e.~value multiplied by their CTR) and second highest expected value ad. Next, we assume that the advertiser is non-myopic and cares about their long term utility. This setting is much more complex since an advertiser is incen
    
[^21]: 下-左部分AUC：一种用于推荐系统的有效和高效优化指标

    Lower-Left Partial AUC: An Effective and Efficient Optimization Metric for Recommendation

    [https://arxiv.org/abs/2403.00844](https://arxiv.org/abs/2403.00844)

    提出了一种新的优化指标Lower-Left Partial AUC（LLPAUC），在计算效率上类似于AUC，但与Top-K排名指标强相关，能在推荐系统中有效提升性能。

    

    优化指标对于构建大规模推荐系统至关重要。然而，一种实用的有效和高效指标仍然难以找到。尽管Top-K排名指标是优化的黄金标准，但它们存在着显着的计算开销。相比之下，更高效的准确性和AUC指标往往无法捕捉推荐任务的真正目标，导致性能亚优。为了克服这一困境，我们提出了一种新的优化指标，Lower-Left Partial AUC（LLPAUC），它在计算效率上类似于AUC，但与Top-K排名指标强相关。与AUC相比，LLPAUC仅考虑ROC曲线下方的局部区域，以将优化焦点放在Top-K上。我们提供了LLPAUC与Top-K排名指标之间的相关性的理论验证，并展示了其对嘈杂用户反馈的稳健性。

    arXiv:2403.00844v1 Announce Type: cross  Abstract: Optimization metrics are crucial for building recommendation systems at scale. However, an effective and efficient metric for practical use remains elusive. While Top-K ranking metrics are the gold standard for optimization, they suffer from significant computational overhead. Alternatively, the more efficient accuracy and AUC metrics often fall short of capturing the true targets of recommendation tasks, leading to suboptimal performance. To overcome this dilemma, we propose a new optimization metric, Lower-Left Partial AUC (LLPAUC), which is computationally efficient like AUC but strongly correlates with Top-K ranking metrics. Compared to AUC, LLPAUC considers only the partial area under the ROC curve in the Lower-Left corner to push the optimization focus on Top-K. We provide theoretical validation of the correlation between LLPAUC and Top-K ranking metrics and demonstrate its robustness to noisy user feedback. We further design an 
    
[^22]: 利用双层可学习大型语言模型规划增强长期推荐

    Enhancing Long-Term Recommendation with Bi-level Learnable Large Language Model Planning

    [https://arxiv.org/abs/2403.00843](https://arxiv.org/abs/2403.00843)

    利用大型语言模型的规划能力来增强长期推荐，使模型在个性化推荐中更有效地理解和应用任务解决原则

    

    传统推荐系统倾向于过分迎合用户的即时兴趣而忽视他们的长期参与。 为了解决这个问题，在推荐决策过程中合并规划能力是至关重要的，以开发能够同时考虑即时兴趣和长期参与的策略。本文提出利用大型语言模型（LLMs）对稀疏数据的显著规划能力用于长期推荐。关键在于使语言模型能够在个性化推荐场景中有效理解和应用任务解决原则，因为模型的预训练可能并未自然包含这些内容。

    arXiv:2403.00843v1 Announce Type: cross  Abstract: Traditional recommendation setting tends to excessively cater to users' immediate interests and neglect their long-term engagement. To address it, it is crucial to incorporate planning capabilities into the recommendation decision-making process to develop policies that take into account both immediate interests and long-term engagement. Despite Reinforcement Learning (RL) can learn planning capacity by maximizing cumulative reward, the scarcity of recommendation data presents challenges such as instability and susceptibility to overfitting when training RL models from scratch.   In this context, we propose to leverage the remarkable planning capabilities over sparse data of Large Language Models (LLMs) for long-term recommendation. The key lies in enabling a language model to understand and apply task-solving principles effectively in personalized recommendation scenarios, as the model's pre-training may not naturally encompass these 
    
[^23]: 通过路径推理实现可解释的基于会话的推荐

    Explainable Session-based Recommendation via Path Reasoning

    [https://arxiv.org/abs/2403.00832](https://arxiv.org/abs/2403.00832)

    通过路径推理的泛化层次强化学习框架提高了基于会话的推荐的可解释性，设计了多目标奖励机制和路径中间点奖励以应对顺序模式的跳过行为和增强知识图的探索效率。

    

    本文探讨了通过路径推理为基于会话的推荐（SR）提供可解释性的方法。现有的SR模型强调准确性但缺乏可解释性，而传统的路径推理侧重于知识图探索，忽略了会话历史中存在的顺序模式。因此，我们提出了一种用于SR的泛化层次强化学习框架，通过路径推理（PR4SR）来提高现有SR模型的可解释性。考虑到项目对会话的重要性不同，我们设计了会话级别代理来选择会话中的项目作为路径推理的起点，以及路径级别代理来执行路径推理。特别地，我们设计了多目标奖励机制来适应SR中顺序模式的跳过行为，并引入路径中间点奖励来增强知识图中的探索效率。

    arXiv:2403.00832v1 Announce Type: cross  Abstract: This paper explores providing explainability for session-based recommendation (SR) by path reasoning. Current SR models emphasize accuracy but lack explainability, while traditional path reasoning prioritizes knowledge graph exploration, ignoring sequential patterns present in the session history. Therefore, we propose a generalized hierarchical reinforcement learning framework for SR, which improves the explainability of existing SR models via Path Reasoning, namely PR4SR. Considering the different importance of items to the session, we design the session-level agent to select the items in the session as the starting point for path reasoning and the path-level agent to perform path reasoning. In particular, we design a multi-target reward mechanism to adapt to the skip behaviors of sequential patterns in SR, and introduce path midpoint reward to enhance the exploration efficiency in knowledge graphs. To improve the completeness of the
    
[^24]: InteraRec：使用多模式大型语言模型进行交互式推荐

    InteraRec: Interactive Recommendations Using Multimodal Large Language Models

    [https://arxiv.org/abs/2403.00822](https://arxiv.org/abs/2403.00822)

    InteraRec引入了一种复杂的交互式推荐框架，与传统方法不同，它不仅依赖Weblog生成推荐，还捕获用户导航时网页的高频截图。

    

    Weblog由记录任何网站上用户活动的记录组成，可以为用户偏好、行为和兴趣提供宝贵的见解。许多推荐算法利用通过这些Weblog挖掘的数据，采用协同过滤、基于内容的过滤和混合方法等策略，为用户提供个性化推荐。本研究引入了一种称为InteraRec的复杂交互式推荐框架，它不同于传统方法，后者仅依赖Weblog生成推荐。该框架捕获用户导航时网页的高频截图。

    arXiv:2403.00822v1 Announce Type: cross  Abstract: Weblogs, comprised of records detailing user activities on any website, offer valuable insights into user preferences, behavior, and interests. Numerous recommendation algorithms, employing strategies such as collaborative filtering, content-based filtering, and hybrid methods, leverage the data mined through these weblogs to provide personalized recommendations to users. Despite the abundance of information available in these weblogs, identifying and extracting pertinent information and key features necessitates extensive engineering endeavors. The intricate nature of the data also poses a challenge for interpretation, especially for non-experts. In this study, we introduce a sophisticated and interactive recommendation framework denoted as InteraRec, which diverges from conventional approaches that exclusively depend on weblogs for recommendation generation. This framework captures high-frequency screenshots of web pages as users nav
    
[^25]: 检索增强生成系统：自动数据集创建，评估和布尔代理设置

    Retrieval Augmented Generation Systems: Automatic Dataset Creation, Evaluation and Boolean Agent Setup

    [https://arxiv.org/abs/2403.00820](https://arxiv.org/abs/2403.00820)

    本文提出了一种严格的数据集创建和评估工作流程，用于量化比较不同的RAG策略，同时开发和评估了一个布尔代理RAG设置，使得大语言模型可以节省标记来决定是否查询向量数据库。

    

    检索增强生成（RAG）系统在增强大语言模型（LLM）输出中与领域特定和时间敏感数据流行度极高。最近，从简单的RAG设置每次用户输入都查询向量数据库以获取附加信息的方式，正在转变为更复杂形式的RAG。然而，目前各种具体方法仍主要基于大多是偶然证据竞争。本文提出了一个严格的数据集创建和评估工作流程，以定量比较不同的RAG策略。我们使用以这种方式创建的数据集来开发和评估布尔代理RAG设置：一个系统，其中LLM可以决定是否查询向量数据库，从而节省可以用内部知识回答的问题的标记。我们将我们的代码和生成的数据集在线发布。

    arXiv:2403.00820v1 Announce Type: cross  Abstract: Retrieval Augmented Generation (RAG) systems have seen huge popularity in augmenting Large-Language Model (LLM) outputs with domain specific and time sensitive data. Very recently a shift is happening from simple RAG setups that query a vector database for additional information with every user input to more sophisticated forms of RAG. However, different concrete approaches compete on mostly anecdotal evidence at the moment. In this paper we present a rigorous dataset creation and evaluation workflow to quantitatively compare different RAG strategies. We use a dataset created this way for the development and evaluation of a boolean agent RAG setup: A system in which a LLM can decide whether to query a vector database or not, thus saving tokens on questions that can be answered with internal knowledge. We publish our code and generated dataset online.
    
[^26]: 双重校准估计器用于缺失数据不随机推荐

    Doubly Calibrated Estimator for Recommendation on Data Missing Not At Random

    [https://arxiv.org/abs/2403.00817](https://arxiv.org/abs/2403.00817)

    提出了双重校准估计器，通过校准插补和概率模型来解决推荐系统中缺失数据不随机的挑战

    

    推荐系统往往受到选择偏差的影响，因为用户倾向于评价他们喜欢的物品。在这种条件下收集的数据集表现出不随机缺失的条目，因此不是代表目标人群的随机对照试验。为了解决这一挑战，提出了双重稳健估计器及其增强型变体，因为它们确保在提供准确的插补误差或预测概率时无偏。然而，我们认为现有的估计器依赖于错误校准的插补误差和概率分数，因为它们依赖于估计的基本模型。我们提供理论洞察，说明错误校准的插补和概率模型可能限制双重稳健估计器的有效性，并利用真实世界数据集验证我们的定理。基于此，我们提出了一个包括对插补和概率模型进行校准的双重校准估计器。

    arXiv:2403.00817v1 Announce Type: cross  Abstract: Recommender systems often suffer from selection bias as users tend to rate their preferred items. The datasets collected under such conditions exhibit entries missing not at random and thus are not randomized-controlled trials representing the target population. To address this challenge, a doubly robust estimator and its enhanced variants have been proposed as they ensure unbiasedness when accurate imputed errors or predicted propensities are provided. However, we argue that existing estimators rely on miscalibrated imputed errors and propensity scores as they depend on rudimentary models for estimation. We provide theoretical insights into how miscalibrated imputation and propensity models may limit the effectiveness of doubly robust estimators and validate our theorems using real-world datasets. On this basis, we propose a Doubly Calibrated Estimator that involves the calibration of both the imputation and propensity models. To achi
    
[^27]: CFRet-DVQA：粗到精检索和高效调优用于文档视觉问答

    CFRet-DVQA: Coarse-to-Fine Retrieval and Efficient Tuning for Document Visual Question Answering

    [https://arxiv.org/abs/2403.00816](https://arxiv.org/abs/2403.00816)

    该研究提出了一种名为CFRet-DVQA的方法，通过检索和高效调优，解决了文档视觉问答中定位信息和限制模型输入的长度等问题，进一步提升了答案的生成性能。

    

    文档视觉问答（DVQA）是一个涉及根据图像内容回答查询的任务。现有工作仅限于定位单页内的信息，不支持跨页面问答交互。此外，对模型输入的标记长度限制可能导致与答案相关的部分被截断。在本研究中，我们引入了一种简单但有效的方法学，称为CFRet-DVQA，重点放在检索和高效调优上，以有效解决这一关键问题。为此，我们首先从文档中检索与所提问题相关的多个片段。随后，我们利用大型语言模型（LLM）的先进推理能力，通过指导调优进一步增强其性能。该方法使得生成的答案与文档标签的风格相符。实验演示了...

    arXiv:2403.00816v1 Announce Type: cross  Abstract: Document Visual Question Answering (DVQA) is a task that involves responding to queries based on the content of images. Existing work is limited to locating information within a single page and does not facilitate cross-page question-and-answer interaction. Furthermore, the token length limitation imposed on inputs to the model may lead to truncation of segments pertinent to the answer. In this study, we introduce a simple but effective methodology called CFRet-DVQA, which focuses on retrieval and efficient tuning to address this critical issue effectively. For that, we initially retrieve multiple segments from the document that correlate with the question at hand. Subsequently, we leverage the advanced reasoning abilities of the large language model (LLM), further augmenting its performance through instruction tuning. This approach enables the generation of answers that align with the style of the document labels. The experiments demo
    
[^28]: RAM-EHR: 电子健康记录上的检索增强与临床预测相遇

    RAM-EHR: Retrieval Augmentation Meets Clinical Predictions on Electronic Health Records

    [https://arxiv.org/abs/2403.00815](https://arxiv.org/abs/2403.00815)

    RAM-EHR通过增强检索并利用总结知识，提高了针对电子健康记录的临床预测效果。

    

    我们提出了RAM-EHR，这是一个用于改善电子健康记录（EHR）上临床预测的检索增强（Retrieval Augmentation）流程。RAM-EHR首先收集多个知识来源，将它们转换为文本格式，并使用密集检索来获取与医学概念相关的信息。这一策略解决了与复杂概念名称相关的困难。RAM-EHR然后增广了与一致性正则化代码联合训练的本地EHR预测模型，以捕获来自患者就诊和总结知识的互补信息。在两个EHR数据集上的实验表明，RAM-EHR相对于之前的知识增强基线效果显著（AUROC增益3.4％，AUPR增益7.2％），强调了RAM-EHR的总结知识对临床预测任务的有效性。代码将发布在\url{https://github.com/ritaranx/RAM-EHR}。

    arXiv:2403.00815v1 Announce Type: cross  Abstract: We present RAM-EHR, a Retrieval AugMentation pipeline to improve clinical predictions on Electronic Health Records (EHRs). RAM-EHR first collects multiple knowledge sources, converts them into text format, and uses dense retrieval to obtain information related to medical concepts. This strategy addresses the difficulties associated with complex names for the concepts. RAM-EHR then augments the local EHR predictive model co-trained with consistency regularization to capture complementary information from patient visits and summarized knowledge. Experiments on two EHR datasets show the efficacy of RAM-EHR over previous knowledge-enhanced baselines (3.4% gain in AUROC and 7.2% gain in AUPR), emphasizing the effectiveness of the summarized knowledge from RAM-EHR for clinical prediction tasks. The code will be published at \url{https://github.com/ritaranx/RAM-EHR}.
    
[^29]: 用户决策过程中性别偏见的法律案例检索系统

    Gender Biased Legal Case Retrieval System on Users' Decision Process

    [https://arxiv.org/abs/2403.00814](https://arxiv.org/abs/2403.00814)

    设计了一个新的用户实验框架来研究法律案例搜索结果中的性别偏见对用户认知的影响

    

    在过去的十年中，法律案例搜索已成为法律从业者工作的重要组成部分。在法律案例搜索中，搜索引擎从海量数据中检索出许多相关案例并提供给用户。然而，这些案例是否存在性别偏见，以及这种偏见是否会影响用户的认知，目前尚不确定。我们设计了一个新的用户实验框架来模拟法官阅读相关案例的过程。邀请了72名具有法律背景的参与者进行实验。参与者被要求模拟法官的角色，在3个指定的案例中进行法律案例搜索，并确定这些案例中被告的判决结果。同时对任务和相关案例中被告的性别进行编辑，以统计性地衡量法律案例搜索结果中性别偏见对参与者认知的影响。结果显示，法律案例搜索结果中存在性别偏见

    arXiv:2403.00814v1 Announce Type: new  Abstract: In the last decade, legal case search has become an important part of a legal practitioner's work. During legal case search, search engines retrieval a number of relevant cases from huge amounts of data and serve them to users. However, it is uncertain whether these cases are gender-biased and whether such bias has impact on user perceptions. We designed a new user experiment framework to simulate the judges' reading of relevant cases. 72 participants with backgrounds in legal affairs invited to conduct the experiment. Participants were asked to simulate the role of the judge in conducting a legal case search on 3 assigned cases and determine the sentences of the defendants in these cases. Gender of the defendants in both the task and relevant cases was edited to statistically measure the effect of gender bias in the legal case search results on participants' perceptions. The results showed that gender bias in the legal case search resul
    
[^30]: 利用Elasticsearch和Transformer模型增强基于云的大型语言模型处理

    Enhancing Cloud-Based Large Language Model Processing with Elasticsearch and Transformer Models

    [https://arxiv.org/abs/2403.00807](https://arxiv.org/abs/2403.00807)

    利用Elasticsearch和Transformer模型提升云端基于大型语言模型的处理，尤其在实现语义搜索方面有显著帮助。

    

    大型语言模型(LLMs)是一类利用Transformer网络构建的生成式人工智能模型，能够利用大量数据集识别、总结、翻译、预测和生成语言。LLMs承诺改变社会，然而训练这些基础模型面临巨大挑战。在大型语言模型中进行语义向量搜索是一种强大的技术，可以显著增强搜索结果的准确性和相关性。与传统基于关键词的搜索方法不同，语义搜索利用单词的含义和上下文来理解查询背后的意图，并提供更精确的结果。Elasticsearch是一种最流行的工具之一，用于实现语义搜索，是一个专为索引和搜索大数据集设计的可扩展和稳健的搜索引擎。在本文中，我们深入探讨了语义搜索的基础知识，并探讨了如何利用Elasticsearch和Transformer模型。

    arXiv:2403.00807v1 Announce Type: cross  Abstract: Large Language Models (LLMs) are a class of generative AI models built using the Transformer network, capable of leveraging vast datasets to identify, summarize, translate, predict, and generate language. LLMs promise to revolutionize society, yet training these foundational models poses immense challenges. Semantic vector search within large language models is a potent technique that can significantly enhance search result accuracy and relevance. Unlike traditional keyword-based search methods, semantic search utilizes the meaning and context of words to grasp the intent behind queries and deliver more precise outcomes. Elasticsearch emerges as one of the most popular tools for implementing semantic search an exceptionally scalable and robust search engine designed for indexing and searching extensive datasets. In this article, we delve into the fundamentals of semantic search and explore how to harness Elasticsearch and Transformer m
    
[^31]: 通过机器学习语言模型增强操作系统中的用户交互

    Enhanced User Interaction in Operating Systems through Machine Learning Language Models

    [https://arxiv.org/abs/2403.00806](https://arxiv.org/abs/2403.00806)

    通过结合交互设计和机器学习，提供更高效、个性化的用户体验，满足用户特定需求，持续改善产品质量和性能。

    

    随着大型语言模型展示出类似人类的逻辑推理和理解能力，基于大型语言模型的代理是否能模拟真实用户的交互行为，从而构建一个可靠的虚拟推荐A/B测试场景，帮助推荐研究的应用是一个迫切、重要并具有经济价值的问题。交互设计和机器学习的结合能为产品和服务提供更高效、个性化的用户体验。这种个性化服务可以满足用户的特定需求，提高用户满意度和忠诚度。此外，交互系统可以通过提供良好的用户界面和交互体验来理解用户对产品的看法和需求，然后利用机器学习算法改进和优化产品。这种迭代优化过程可以持续改善产品的质量和性能。

    arXiv:2403.00806v1 Announce Type: cross  Abstract: With the large language model showing human-like logical reasoning and understanding ability, whether agents based on the large language model can simulate the interaction behavior of real users, so as to build a reliable virtual recommendation A/B test scene to help the application of recommendation research is an urgent, important and economic value problem. The combination of interaction design and machine learning can provide a more efficient and personalized user experience for products and services. This personalized service can meet the specific needs of users and improve user satisfaction and loyalty. Second, the interactive system can understand the user's views and needs for the product by providing a good user interface and interactive experience, and then use machine learning algorithms to improve and optimize the product. This iterative optimization process can continuously improve the quality and performance of the produc
    
[^32]: LiMAML: 通过元学习个性化深度推荐模型

    LiMAML: Personalization of Deep Recommender Models via Meta Learning

    [https://arxiv.org/abs/2403.00803](https://arxiv.org/abs/2403.00803)

    该论文介绍了一种通过元学习实现个性化深度推荐模型的创新解决方案，能够根据最新用户互动信号频繁更新模型，以确保向不同成员提供相关且更新的体验。

    

    在推荐系统领域，深度神经网络的普遍采用已经成为建模各种业务目标的主导范式。随着用户基数的持续增长，个性化和频繁的模型更新的必要性已经变得至关重要，以确保向各种成员提供相关且更新的体验。在这项工作中，我们介绍了一种创新的元学习解决方案，用于针对个人成员和其他实体的模型个性化，结合了根据最新用户互动信号进行频繁更新的功能。具体来说，我们利用了模型无关的元学习（MAML）算法，使用最近的用户互动数据来调整每个任务的子网络。考虑到在线推荐系统中生产原始MAML模型几乎不可行，我们提出了一种有效的策略来将元学习的子网络推广应用到生产中。

    arXiv:2403.00803v1 Announce Type: cross  Abstract: In the realm of recommender systems, the ubiquitous adoption of deep neural networks has emerged as a dominant paradigm for modeling diverse business objectives. As user bases continue to expand, the necessity of personalization and frequent model updates have assumed paramount significance to ensure the delivery of relevant and refreshed experiences to a diverse array of members. In this work, we introduce an innovative meta-learning solution tailored to the personalization of models for individual members and other entities, coupled with the frequent updates based on the latest user interaction signals. Specifically, we leverage the Model-Agnostic Meta Learning (MAML) algorithm to adapt per-task sub-networks using recent user interaction data. Given the near infeasibility of productionizing original MAML-based models in online recommendation systems, we propose an efficient strategy to operationalize meta-learned sub-networks in prod
    
[^33]: 朝向理论理解两阶段推荐系统

    Towards a Theoretical Understanding of Two-Stage Recommender Systems

    [https://arxiv.org/abs/2403.00802](https://arxiv.org/abs/2403.00802)

    这里是中文总结出的一句话要点: 该论文研究了两阶段推荐系统的理论行为，证明了其向最佳推荐系统的强收敛性，同时揭示了它在输入特征的固有维度上实现更快收敛的特性。

    

    arXiv:2403.00802v1 公告类型:跨域 摘要:生产级推荐系统在在线媒体服务中广泛使用大规模语料库，包括Netflix、Pinterest和Amazon。这些系统通过学习用户和物品在低维空间中投影的嵌入、通过两阶段模型（两个深度神经网络）丰富推荐，这有助于它们的嵌入构建以预测与物品相关的用户反馈。尽管它在推荐中很受欢迎，但其理论行为仍未得到全面探讨。我们研究了两阶段推荐的渐近行为，这包括对最佳推荐系统的强收敛。我们建立了两阶段推荐的一些理论性质和统计保证。除了渐近行为，我们还展示了两阶段推荐系统通过依赖输入特征的固有维度实现更快的收敛。最后，我们通过数值方法展示

    arXiv:2403.00802v1 Announce Type: cross  Abstract: Production-grade recommender systems rely heavily on a large-scale corpus used by online media services, including Netflix, Pinterest, and Amazon. These systems enrich recommendations by learning users' and items' embeddings projected in a low-dimensional space with two-stage models (two deep neural networks), which facilitate their embedding constructs to predict users' feedback associated with items. Despite its popularity for recommendations, its theoretical behaviors remain comprehensively unexplored. We study the asymptotic behaviors of the two-stage recommender that entail a strong convergence to the optimal recommender system. We establish certain theoretical properties and statistical assurance of the two-stage recommender. In addition to asymptotic behaviors, we demonstrate that the two-stage recommender system attains faster convergence by relying on the intrinsic dimensions of the input features. Finally, we show numerically
    
[^34]: 自主检索：利用一个大型语言模型构建信息检索系统

    Self-Retrieval: Building an Information Retrieval System with One Large Language Model

    [https://arxiv.org/abs/2403.00801](https://arxiv.org/abs/2403.00801)

    提出了自主检索(Self-Retrieval)，利用一个大型语言模型完全内化信息检索系统的能力，深度利用大型语言模型在信息检索过程中的能力。

    

    大型语言模型的兴起改变了信息检索系统在人类获取信息过程中的角色。由于现有信息检索系统具有孤立的架构和有限的相互作用，无法完全适应直接向人类提供信息转变为间接为大型语言模型提供服务的变化。本文提出了自主检索(Self-Retrieval)，这是一个端到端、以大型语言模型驱动的信息检索架构，可以完全内化信息检索系统所需的能力到单个大型语言模型中，并深度利用大型语言模型在信息检索过程中的能力。具体来说，自主检索通过自然语言索引架构将要检索的语料内化为一个大型语言模型。然后整个检索过程被重新定义为文档生成和自我评估的过程，可以使用单个大型语言模型端到端执行。实验结果表明S

    arXiv:2403.00801v1 Announce Type: cross  Abstract: The rise of large language models (LLMs) has transformed the role of information retrieval (IR) systems in the way to humans accessing information. Due to the isolated architecture and the limited interaction, existing IR systems are unable to fully accommodate the shift from directly providing information to humans to indirectly serving large language models. In this paper, we propose Self-Retrieval, an end-to-end, LLM-driven information retrieval architecture that can fully internalize the required abilities of IR systems into a single LLM and deeply leverage the capabilities of LLMs during IR process. Specifically, Self-retrieval internalizes the corpus to retrieve into a LLM via a natural language indexing architecture. Then the entire retrieval process is redefined as a procedure of document generation and self-assessment, which can be end-to-end executed using a single large language model. Experimental results demonstrate that S
    
[^35]: Helen: 使用频率加权Hessian特征值正则化优化CTR预测模型

    Helen: Optimizing CTR Prediction Models with Frequency-wise Hessian Eigenvalue Regularization

    [https://arxiv.org/abs/2403.00798](https://arxiv.org/abs/2403.00798)

    本文从优化的角度探讨CTR预测问题，揭示了特征频率与最大Hessian特征值之间的强正相关性，提出频繁出现的特征会趋向于收敛到尖锐的局部最小值，从而导致次优性能。

    

    单击率(CTR)预测在在线广告和推荐场景中具有至关重要的意义。尽管最近CTR预测模型的广泛增加，但性能改进仍然有限，这一点可以从开源基准评估中得到证实。当前研究人员倾向于针对不同数据集和设置开发新模型，常常忽视一个关键问题：是什么真正使CTR预测如此具有挑战性？

    arXiv:2403.00798v1 Announce Type: cross  Abstract: Click-Through Rate (CTR) prediction holds paramount significance in online advertising and recommendation scenarios. Despite the proliferation of recent CTR prediction models, the improvements in performance have remained limited, as evidenced by open-source benchmark assessments. Current researchers tend to focus on developing new models for various datasets and settings, often neglecting a crucial question: What is the key challenge that truly makes CTR prediction so demanding?   In this paper, we approach the problem of CTR prediction from an optimization perspective. We explore the typical data characteristics and optimization statistics of CTR prediction, revealing a strong positive correlation between the top hessian eigenvalue and feature frequency. This correlation implies that frequently occurring features tend to converge towards sharp local minima, ultimately leading to suboptimal performance. Motivated by the recent advance
    
[^36]: 在一个混乱而纠缠的世界中的广告推荐

    Ad Recommendation in a Collapsed and Entangled World

    [https://arxiv.org/abs/2403.00793](https://arxiv.org/abs/2403.00793)

    该论文提出了一个行业广告推荐系统，重点关注学习适当表示的挑战和实践，采用多种方法处理特征表示中的关键挑战，包括嵌入的维度坍缩和跨任务或场景的兴趣纠缠。

    

    在这篇论文中，我们提出了一个行业广告推荐系统，关注学习适当表示的挑战和实践。我们的研究从展示如何在对各种类型的特征进行嵌入表示时保留先验开始。具体来说，我们讨论了序列特征、数值特征、预训练嵌入特征以及稀疏ID特征。此外，我们深入探讨了与特征表示相关的两个关键挑战：嵌入的维度坍缩和跨多个任务或场景的兴趣纠缠。随后，我们提出了几种实用方法来有效应对这两个挑战。接着，我们探讨了几种训练技术，以促进模型优化，减少偏差并增强探索能力。此外，我们引入了三种分析工具，使我们能够全面研究特征相关性、维度坍缩等问题。

    arXiv:2403.00793v1 Announce Type: cross  Abstract: In this paper, we present an industry ad recommendation system, paying attention to the challenges and practices of learning appropriate representations. Our study begins by showcasing our approaches to preserving priors when encoding features of diverse types into embedding representations. Specifically, we address sequence features, numeric features, pre-trained embedding features, as well as sparse ID features. Moreover, we delve into two pivotal challenges associated with feature representation: the dimensional collapse of embeddings and the interest entanglement across various tasks or scenarios. Subsequently, we propose several practical approaches to effectively tackle these two challenges. We then explore several training techniques to facilitate model optimization, reduce bias, and enhance exploration. Furthermore, we introduce three analysis tools that enable us to comprehensively study feature correlation, dimensional collap
    
[^37]: 利用对比学习进行少样本社交媒体帖子地理定位

    Leveraging Contrastive Learning for Few-shot Geolocation of Social Posts

    [https://arxiv.org/abs/2403.00786](https://arxiv.org/abs/2403.00786)

    提出了ContrastGeo框架，利用对比学习加强了少样本社交媒体帖子地理定位，通过引入Tweet-Location对比学习和匹配目标，并采用在线困难负样本挖掘方法来捕捉帖子和位置之间的关联。

    

    社交地理定位是一个重要问题，即预测社交媒体帖子的来源地点。然而，由于需要大量训练数据，以及完善的标注标签，这个任务很具挑战性。这些问题进一步恶化，因为新的或不太热门的地点缺乏足够的标签，导致数据集不平衡。在本文中，我们提出了ContrastGeo，这是一个利用对比学习增强的框架，用于少样本社交地理定位。具体地，引入了一个Tweet-Location对比学习目标，以在帖子-位置对之间对齐表示。为了捕捉帖子和位置之间的关联，进一步采用了Tweet-Location匹配目标，并通过在线困难负样本挖掘方法进行了改进。我们还开发了三种融合策略，采用不同的融合编码器，以更好地生成联合表示。

    arXiv:2403.00786v1 Announce Type: new  Abstract: Social geolocation is an important problem of predicting the originating locations of social media posts. However, this task is challenging due to the need for a substantial volume of training data, alongside well-annotated labels. These issues are further exacerbated by new or less popular locations with insufficient labels, further leading to an imbalanced dataset. In this paper, we propose \textbf{ContrastGeo}, a \textbf{Contrast}ive learning enhanced framework for few-shot social \textbf{Geo}location. Specifically, a Tweet-Location Contrastive learning objective is introduced to align representations of tweets and locations within tweet-location pairs. To capture the correlations between tweets and locations, a Tweet-Location Matching objective is further adopted into the framework and refined via an online hard negative mining approach. We also develop three fusion strategies with various fusion encoders to better generate joint rep
    
[^38]: 利用BERT进行信息检索：调研、应用、资源和挑战

    Utilizing BERT for Information Retrieval: Survey, Applications, Resources, and Challenges

    [https://arxiv.org/abs/2403.00784](https://arxiv.org/abs/2403.00784)

    BERT的引入为信息检索领域带来了突破，研究者们将其应用于解决实际问题，并通过综合分析其在信息检索中的应用方法，为学术界和工业界提供了有益的参考。

    

    近年来，深度学习在解决各种自然语言处理（NLP）问题方面得到了显著增长。最初的深度学习模型受到它们顺序或单向性质的限制，因此难以捕捉文本输入之间的上下文关系。从变压器（BERT）中引入的双向编码器表征提供了变压器模型的强大编码器，可以理解更广泛的上下文，并在各种NLP任务中获得最先进的性能。这激发了研究人员和从业者将BERT应用于实际问题，如信息检索（IR）。因此，一项关注将预训练的变压器编码器如BERT应用于IR的普遍方法的综合分析的调查对学术界和工业界都有用。鉴于此，本调查重新审视了各种基于BERT的方法，涵盖了各种方法

    arXiv:2403.00784v1 Announce Type: cross  Abstract: Recent years have witnessed a substantial increase in the use of deep learning to solve various natural language processing (NLP) problems. Early deep learning models were constrained by their sequential or unidirectional nature, such that they struggled to capture the contextual relationships across text inputs. The introduction of bidirectional encoder representations from transformers (BERT) leads to a robust encoder for the transformer model that can understand the broader context and deliver state-of-the-art performance across various NLP tasks. This has inspired researchers and practitioners to apply BERT to practical problems, such as information retrieval (IR). A survey that focuses on a comprehensive analysis of prevalent approaches that apply pretrained transformer encoders like BERT to IR can thus be useful for academia and the industry. In light of this, we revisit a variety of BERT-based methods in this survey, cover a wid
    
[^39]: ChatDiet：通过LLM增强框架赋能个性化营养导向食品推荐聊天机器人

    ChatDiet: Empowering Personalized Nutrition-Oriented Food Recommender Chatbots through an LLM-Augmented Framework

    [https://arxiv.org/abs/2403.00781](https://arxiv.org/abs/2403.00781)

    这项研究介绍了ChatDiet，一个借助LLM技术构建的框架，能够帮助个性化营养导向食品推荐聊天机器人提供个性化和可解释的推荐。

    

    食物对健康的深远影响使得先进的营养导向食品推荐服务成为必要。传统方法往往缺乏个性化、可解释性和互动性等关键元素。虽然大型语言模型（LLMs）带来了解释性和可解释性，但它们单独的使用未能实现真正的个性化。本文介绍了ChatDiet，一种新颖的LLM驱动框架，专门设计用于个性化营养导向食品推荐聊天机器人。ChatDiet集成了个人和人群模型，辅以一个协调器，无缝检索和处理相关信息。其结果是动态提供个性化和可解释的食品推荐，根据个人用户喜好定制。我们对ChatDiet进行了评估，包括一个引人入胜的案例研究，在案例研究中建立了一个因果个人模型来估计个人营养效果。

    arXiv:2403.00781v1 Announce Type: cross  Abstract: The profound impact of food on health necessitates advanced nutrition-oriented food recommendation services. Conventional methods often lack the crucial elements of personalization, explainability, and interactivity. While Large Language Models (LLMs) bring interpretability and explainability, their standalone use falls short of achieving true personalization. In this paper, we introduce ChatDiet, a novel LLM-powered framework designed specifically for personalized nutrition-oriented food recommendation chatbots. ChatDiet integrates personal and population models, complemented by an orchestrator, to seamlessly retrieve and process pertinent information. The result is a dynamic delivery of personalized and explainable food recommendations, tailored to individual user preferences. Our evaluation of ChatDiet includes a compelling case study, where we establish a causal personal model to estimate individual nutrition effects. Our assessmen
    
[^40]: 在教育领域的文本挖掘

    Text mining in education

    [https://arxiv.org/abs/2403.00769](https://arxiv.org/abs/2403.00769)

    本文系统概述了当前教育文本挖掘领域的现状，旨在回答教育环境中最常用的文本挖掘技术、最常用的教育资源以及主要的应用或教育目标，同时概述了结论和未来趋势。

    

    在线教育环境的迅猛增长产生了大量数据，特别是来自论坛、聊天、社交网络、评估、论文等文本格式的数据。如何挖掘文本数据以找到对教育相关人员有用的知识，是一个激动人心的挑战。尽管最近已发表了越来越多应用文本挖掘于教育领域的文章，但我们尚未找到任何综述这些工作的论文。因此，本文对当前教育文本挖掘领域的现状进行了系统概述。我们的最终目标是回答三个主要研究问题：在教育环境中最常用的文本挖掘技术是什么？最常用的教育资源是什么？主要应用或教育目标是什么？最后，我们总结了结论和更有趣的未来趋势。

    arXiv:2403.00769v1 Announce Type: cross  Abstract: The explosive growth of online education environments is generating a massive volume of data, specially in text format from forums, chats, social networks, assessments, essays, among others. It produces exciting challenges on how to mine text data in order to find useful knowledge for educational stakeholders. Despite the increasing number of educational applications of text mining published recently, we have not found any paper surveying them. In this line, this work presents a systematic overview of the current status of the Educational Text Mining field. Our final goal is to answer three main research questions: Which are the text mining techniques most used in educational environments? Which are the most used educational resources? And which are the main applications or educational goals? Finally, we outline the conclusions and the more interesting future trends.
    
[^41]: 探究大型语言模型对推荐系统的影响：一项广泛综述

    Exploring the Impact of Large Language Models on Recommender Systems: An Extensive Review

    [https://arxiv.org/abs/2402.18590](https://arxiv.org/abs/2402.18590)

    大型语言模型在推荐系统中展现出卓越的推荐能力和语言理解，重塑了推荐任务的基础。

    

    该论文强调了大型语言模型（LLMs）在重塑推荐系统中的重要性，将它们的价值归因于传统推荐系统所缺乏的独特推理能力。不同于缺乏直接用户互动数据的传统系统，LLMs在推荐物品方面表现出卓越的能力，展示了它们在理解语言复杂性方面的熟练程度。这标志着推荐领域的一个根本性范式转变。在充满活力的研究领域中，研究人员积极利用LLMs的语言理解和生成能力重新定义推荐任务的基础。该研究彻底探讨了LLMs在推荐框架内固有的优势，包括细致的语境理解，跨不同领域的平稳过渡，采用统一的方法，利用共享数据池的全面学习策略，透明度

    arXiv:2402.18590v1 Announce Type: cross  Abstract: The paper underscores the significance of Large Language Models (LLMs) in reshaping recommender systems, attributing their value to unique reasoning abilities absent in traditional recommenders. Unlike conventional systems lacking direct user interaction data, LLMs exhibit exceptional proficiency in recommending items, showcasing their adeptness in comprehending intricacies of language. This marks a fundamental paradigm shift in the realm of recommendations. Amidst the dynamic research landscape, researchers actively harness the language comprehension and generation capabilities of LLMs to redefine the foundations of recommendation tasks. The investigation thoroughly explores the inherent strengths of LLMs within recommendation frameworks, encompassing nuanced contextual comprehension, seamless transitions across diverse domains, adoption of unified approaches, holistic learning strategies leveraging shared data reservoirs, transparent
    
[^42]: 基于语言模型的本体论中新概念放置框架

    A Language Model based Framework for New Concept Placement in Ontologies

    [https://arxiv.org/abs/2402.17897](https://arxiv.org/abs/2402.17897)

    提出了一种基于语言模型的框架，用于将从文本中提取的新概念插入到本体中，在边搜索、边形成和增强、边选择三个步骤中分别利用神经方法，并在 SNOMED CT 本体和 MedMentions 实体链接基准上进行了评估

    

    我们研究了利用语言模型将从文本中提取的新概念插入本体的任务。我们探索了一个三步方法：边搜索，即找到要插入的候选位置集（即概念之间的包含关系），边形成和增强，利用本体结构生成和增强边候选，以及边选择，最终确定要放置的边。在所有步骤中，我们提出利用神经方法，其中应用基于嵌入的方法和对比学习，如BERT用于边搜索，采用基于BERT微调的多标签边交叉编码器，以及GPT系列、FLAN-T5 和 Llama 2 等大型语言模型（LLM）用于边选择。我们在使用 SNOMED CT 本体和 MedMentions 实体链接基准创建的最新数据集上评估了这些方法。

    arXiv:2402.17897v1 Announce Type: new  Abstract: We investigate the task of inserting new concepts extracted from texts into an ontology using language models. We explore an approach with three steps: edge search which is to find a set of candidate locations to insert (i.e., subsumptions between concepts), edge formation and enrichment which leverages the ontological structure to produce and enhance the edge candidates, and edge selection which eventually locates the edge to be placed into. In all steps, we propose to leverage neural methods, where we apply embedding-based methods and contrastive learning with Pre-trained Language Models (PLMs) such as BERT for edge search, and adapt a BERT fine-tuning-based multi-label Edge-Cross-encoder, and Large Language Models (LLMs) such as GPT series, FLAN-T5, and Llama 2, for edge selection. We evaluate the methods on recent datasets created using the SNOMED CT ontology and the MedMentions entity linking benchmark. The best settings in our fram
    
[^43]: JMLR：联合医疗LLM和检索训练以增强推理和专业问题回答能力

    JMLR: Joint Medical LLM and Retrieval Training for Enhancing Reasoning and Professional Question Answering Capability

    [https://arxiv.org/abs/2402.17887](https://arxiv.org/abs/2402.17887)

    JMLR通过联合训练信息检索系统和大型语言模型，在医学领域提高问题回答系统性能，降低计算资源需求，增强模型利用医疗知识进行推理和回答问题的能力。

    

    随着医疗数据的爆炸性增长和人工智能技术的快速发展，精准医学已经成为增强医疗服务质量和效率的关键。在这种背景下，大型语言模型（LLMs）在医疗知识获取和问题回答系统中发挥越来越重要的作用。为了进一步提高这些系统在医学领域的性能，我们介绍了一种创新方法，在微调阶段同时训练信息检索（IR）系统和LLM。我们称之为联合医疗LLM和检索训练（JMLR）的方法旨在克服传统模型在处理医学问题回答任务时面临的挑战。通过采用同步训练机制，JMLR减少了对计算资源的需求，并增强了模型利用医疗知识进行推理和回答问题的能力。

    arXiv:2402.17887v1 Announce Type: new  Abstract: With the explosive growth of medical data and the rapid development of artificial intelligence technology, precision medicine has emerged as a key to enhancing the quality and efficiency of healthcare services. In this context, Large Language Models (LLMs) play an increasingly vital role in medical knowledge acquisition and question-answering systems. To further improve the performance of these systems in the medical domain, we introduce an innovative method that jointly trains an Information Retrieval (IR) system and an LLM during the fine-tuning phase. This approach, which we call Joint Medical LLM and Retrieval Training (JMLR), is designed to overcome the challenges faced by traditional models in handling medical question-answering tasks. By employing a synchronized training mechanism, JMLR reduces the demand for computational resources and enhances the model's ability to leverage medical knowledge for reasoning and answering question
    
[^44]: PromptMM：多模式知识蒸馏用于基于Prompt-Tuning的推荐

    PromptMM: Multi-Modal Knowledge Distillation for Recommendation with Prompt-Tuning

    [https://arxiv.org/abs/2402.17188](https://arxiv.org/abs/2402.17188)

    提出了一种通过Prompt-Tuning赋能的PromptMM多模式知识蒸馏方法，用于简化和增强推荐系统，实现自适应的质量蒸馏。

    

    多媒体在线平台（例如亚马逊、TikTok）通过将多媒体（例如视觉、文本和声学）内容纳入其个性化推荐系统中获益匪浅。这些模态提供直观语义，有助于进行模态感知的用户偏好建模。然而，多模式推荐器中存在两个关键挑战尚未解决：i）引入具有大量额外参数的多模式编码器会导致过拟合，考虑到提取器（例如ViT、BERT）提供的高维多模式特征。ii）辅助信息不可避免地引入不准确性和冗余，导致模态交互依赖偏离真实用户偏好。为了解决这些问题，我们提出通过Prompt-Tuning赋能、简化推荐器的PromptMM（多模式知识蒸馏），实现自适应质量蒸馏。

    arXiv:2402.17188v1 Announce Type: new  Abstract: Multimedia online platforms (e.g., Amazon, TikTok) have greatly benefited from the incorporation of multimedia (e.g., visual, textual, and acoustic) content into their personal recommender systems. These modalities provide intuitive semantics that facilitate modality-aware user preference modeling. However, two key challenges in multi-modal recommenders remain unresolved: i) The introduction of multi-modal encoders with a large number of additional parameters causes overfitting, given high-dimensional multi-modal features provided by extractors (e.g., ViT, BERT). ii) Side information inevitably introduces inaccuracies and redundancies, which skew the modality-interaction dependency from reflecting true user preference. To tackle these problems, we propose to simplify and empower recommenders through Multi-modal Knowledge Distillation (PromptMM) with the prompt-tuning that enables adaptive quality distillation. Specifically, PromptMM cond
    
[^45]: 从脑信号解码查询语义的查询扩展

    Query Augmentation by Decoding Semantics from Brain Signals

    [https://arxiv.org/abs/2402.15708](https://arxiv.org/abs/2402.15708)

    提出了一种名为Brain-Aug的方法，通过从脑信号中解码的语义信息增强查询，可以生成更准确的查询，改善文档排序性能，特别适用于模糊查询。

    

    查询扩展是用于细化语义不准确查询的关键技术。传统上，查询扩展依赖于从最初检索到的、潜在相关的文档中提取信息。如果最初检索到的文档质量较低，则查询扩展的有效性也会受到限制。我们提出了Brain-Aug，通过将从脑信号解码的语义信息结合到查询中来增强查询。Brain-Aug使用了在脑信号信息构建的提示和面向排名的推理方法生成原始查询的延续部分。对fMRI数据集的实验结果显示，Brain-Aug生成的查询在语义上更准确，导致改进的文档排序性能。脑信号带来的这种改进对于模糊查询特别显著。

    arXiv:2402.15708v1 Announce Type: cross  Abstract: Query augmentation is a crucial technique for refining semantically imprecise queries. Traditionally, query augmentation relies on extracting information from initially retrieved, potentially relevant documents. If the quality of the initially retrieved documents is low, then the effectiveness of query augmentation would be limited as well. We propose Brain-Aug, which enhances a query by incorporating semantic information decoded from brain signals. BrainAug generates the continuation of the original query with a prompt constructed with brain signal information and a ranking-oriented inference approach. Experimental results on fMRI (functional magnetic resonance imaging) datasets show that Brain-Aug produces semantically more accurate queries, leading to improved document ranking performance. Such improvement brought by brain signals is particularly notable for ambiguous queries.
    
[^46]: 基于大型语言模型的模态感知集成用于基于知识的视觉问答

    Modality-Aware Integration with Large Language Models for Knowledge-based Visual Question Answering

    [https://arxiv.org/abs/2402.12728](https://arxiv.org/abs/2402.12728)

    提出了一种模态感知的LLM集成方法（MAIL）用于针对KVQA，通过细致地利用多模态知识来处理图像理解和知识推理。

    

    知识驱动的视觉问答（KVQA）已被广泛研究，以利用外部知识如知识图谱（KG）来回答视觉问题。尽管已提出几种尝试利用大型语言模型（LLMs）作为隐含知识源，但由于LLMs可能生成幻觉，因此仍然具有挑战性。此外，多种知识来源，例如图像、知识图谱和LLMs，不能轻易对齐以应对复杂场景。为了解决这些问题，我们提出了一种针对KVQA的新颖的具有模态感知的LLM集成方法（MAIL）。它精心利用多模态知识进行图像理解和知识推理。具体而言，（i）我们提出了一种使用LLMs的两阶段提示策略，将图像密集地融入带有详细视觉特征的场景图中；（ii）我们通过将提到的实体与外部事实联系起来构建一个耦合的概念图；（iii）设计了一个定制的伪孪生图中介融合。

    arXiv:2402.12728v1 Announce Type: cross  Abstract: Knowledge-based visual question answering (KVQA) has been extensively studied to answer visual questions with external knowledge, e.g., knowledge graphs (KGs). While several attempts have been proposed to leverage large language models (LLMs) as an implicit knowledge source, it remains challenging since LLMs may generate hallucinations. Moreover, multiple knowledge sources, e.g., images, KGs and LLMs, cannot be readily aligned for complex scenarios. To tackle these, we present a novel modality-aware integration with LLMs for KVQA (MAIL). It carefully leverages multimodal knowledge for both image understanding and knowledge reasoning. Specifically, (i) we propose a two-stage prompting strategy with LLMs to densely embody the image into a scene graph with detailed visual features; (ii) We construct a coupled concept graph by linking the mentioned entities with external facts. (iii) A tailored pseudo-siamese graph medium fusion is designe
    
[^47]: C-RAG: 针对检索增强语言模型的认证生成风险

    C-RAG: Certified Generation Risks for Retrieval-Augmented Language Models

    [https://arxiv.org/abs/2402.03181](https://arxiv.org/abs/2402.03181)

    C-RAG是第一个用于认证检索增强语言模型生成风险的框架，通过提供符合风险分析和生成风险的上界，确保生成结果的可信性。

    

    尽管大型语言模型（LLMs）在各种应用中具备令人印象深刻的能力，但它们仍然存在可信度问题，如幻觉和错位。检索增强语言模型（RAG）被提出来增强生成结果的可信性，通过引入外部知识。但是，对于RAG模型的生成风险的理论理解尚未被研究。本文回答了以下问题：1）RAG是否确实能够降低生成风险，2）如何对RAG和传统LLM的生成风险提供可证明的保证，以及3）哪些充分条件使得RAG模型能够降低生成风险。我们提出了C-RAG，第一个用于认证RAG模型生成风险的框架。具体而言，我们为RAG模型提供了符合风险分析，并确保了生成风险的上界，我们称之为符合生成风险。我们还对一般有界风险下的符合生成风险提供了理论保证。

    Despite the impressive capabilities of large language models (LLMs) across diverse applications, they still suffer from trustworthiness issues, such as hallucinations and misalignments. Retrieval-augmented language models (RAG) have been proposed to enhance the credibility of generations by grounding external knowledge, but the theoretical understandings of their generation risks remains unexplored. In this paper, we answer: 1) whether RAG can indeed lead to low generation risks, 2) how to provide provable guarantees on the generation risks of RAG and vanilla LLMs, and 3) what sufficient conditions enable RAG models to reduce generation risks. We propose C-RAG, the first framework to certify generation risks for RAG models. Specifically, we provide conformal risk analysis for RAG models and certify an upper confidence bound of generation risks, which we refer to as conformal generation risk. We also provide theoretical guarantees on conformal generation risks for general bounded risk f
    
[^48]: NoMIRACL: 知道自己不知道的鲁棒多语言检索增强生成

    NoMIRACL: Knowing When You Don't Know for Robust Multilingual Retrieval-Augmented Generation

    [https://arxiv.org/abs/2312.11361](https://arxiv.org/abs/2312.11361)

    建立了用于评估大型语言模型在多语言环境中检索增强生成中的鲁棒性的NoMIRACL数据集，并提出了两个衡量模型鲁棒性的指标：幻觉率和错误率。

    

    arXiv:2312.11361v2 公告类型: 替换 摘要: 检索增强生成（RAG）通过利用外部知识源来将大型语言模型（LLM）输出与现实联系起来，以减少事实幻觉。然而，先前的研究缺乏对不同语言族的全面评估，这使得很难评估LLM对外部检索知识错误的鲁棒性。为了克服这一问题，我们建立了NoMIRACL，这是一个人类注释的数据集，用于评估RAG中LLM对18种在类型上多样化的语言的鲁棒性。NoMIRACL包括一个非相关子集和一个相关子集。非相关子集中的查询包含被判断为不相关的段落，而相关子集中的查询至少包含一个被判断为相关的段落。我们使用两个指标来衡量LLM的鲁棒性：（i）幻觉率，衡量模型倾向于在非相关子集的段落中产生幻觉答案的程度，以及（ii）错误率，衡量模型的不准确度。

    arXiv:2312.11361v2 Announce Type: replace  Abstract: Retrieval-augmented generation (RAG) grounds large language model (LLM) output by leveraging external knowledge sources to reduce factual hallucinations. However, prior works lack a comprehensive evaluation of different language families, making it challenging to evaluate LLM robustness against errors in external retrieved knowledge. To overcome this, we establish NoMIRACL, a human-annotated dataset for evaluating LLM robustness in RAG across 18 typologically diverse languages. NoMIRACL includes both a non-relevant and a relevant subset. Queries in the non-relevant subset contain passages judged as non-relevant, whereas queries in the relevant subset include at least a single judged relevant passage. We measure LLM robustness using two metrics: (i) hallucination rate, measuring model tendency to hallucinate an answer, when the answer is not present in passages in the non-relevant subset, and (ii) error rate, measuring model inaccurac
    
[^49]: 使用图神经网络解释伊斯兰教仇恨言论的研究

    Explainable Identification of Hate Speech towards Islam using Graph Neural Networks

    [https://arxiv.org/abs/2311.04916](https://arxiv.org/abs/2311.04916)

    使用图神经网络解释和识别伊斯兰教仇恨言论，模型在保持出色性能的同时能够解释相关性和因果关系。

    

    伊斯兰教仇恨言论在在线社交互动平台上是一个普遍存在的挑战。识别和消除这种仇恨是迈向和谐与和平未来的关键一步。本研究提出了一种新的范例，利用图神经网络来识别和解释针对伊斯兰教的仇恨言论。利用图神经网络发现、提取并利用不同数据点之间的关系的内在能力，我们的模型始终能够在保持出色性能的同时提供对潜在相关性和因果关系的解释。

    arXiv:2311.04916v2 Announce Type: cross  Abstract: Islamophobic language is a prevalent challenge on online social interaction platforms. Identifying and eliminating such hatred is a crucial step towards a future of harmony and peace. This study presents a novel paradigm for identifying and explaining hate speech towards Islam using graph neural networks. Utilizing the intrinsic ability of graph neural networks to find, extract, and use relationships across disparate data points, our model consistently achieves outstanding performance while offering explanations for the underlying correlations and causation.
    
[^50]: 基于大型语言模型的淘宝搜索长尾查询重写

    Large Language Model based Long-tail Query Rewriting in Taobao Search

    [https://arxiv.org/abs/2311.03758](https://arxiv.org/abs/2311.03758)

    本文提出了基于大型语言模型的BEQUE框架，通过多阶段流程，有效优化长尾查询、弥补语义差距，提高查询重写效果。

    

    在电子商务搜索领域，语义匹配的重要性不言而喭，因为它直接影响用户体验和公司收入。在这方面，查询重写作为一个重要的技术，用来弥补语义匹配过程中固有的语义差距，受到了行业和学术界的广泛关注。然而，现有的查询重写方法往往难以有效优化长尾查询，缓解由语义差距引起的“少召回”现象。在本文中，我们提出了BEQUE，一个桥接长尾查询语义差距的综合框架。具体而言，BEQUE包括三个阶段：多指导监督微调（SFT）、离线反馈和客观对齐。我们首先基于拒绝抽样和辅助任务混合构建一个重写数据集，以监督方式微调我们的大型语言模型（LLM）。随后，

    arXiv:2311.03758v3 Announce Type: replace  Abstract: In the realm of e-commerce search, the significance of semantic matching cannot be overstated, as it directly impacts both user experience and company revenue. Along this line, query rewriting, serving as an important technique to bridge the semantic gaps inherent in the semantic matching process, has attached wide attention from the industry and academia. However, existing query rewriting methods often struggle to effectively optimize long-tail queries and alleviate the phenomenon of "few-recall" caused by semantic gap. In this paper, we present BEQUE, a comprehensive framework that Bridges the sEmantic gap for long-tail QUEries. In detail, BEQUE comprises three stages: multi-instruction supervised fine tuning (SFT), offline feedback, and objective alignment. We first construct a rewriting dataset based on rejection sampling and auxiliary tasks mixing to fine-tune our large language model (LLM) in a supervised fashion. Subsequently,
    
[^51]: 推荐系统中的公平性和多样性：一项调查

    Fairness and Diversity in Recommender Systems: A Survey

    [https://arxiv.org/abs/2307.04644](https://arxiv.org/abs/2307.04644)

    推荐系统研究探索了公平性和多样性之间的联系，通过扩展对用户和物品级别多样性的理解，重新解释了公平性研究，提升了对公平性相关工作的认识

    

    推荐系统是减轻信息过载的有效工具，并在各个领域广泛应用。然而，对效用目标的单一关注证明无法解决现实关切，导致对关注公平感知和多样性感知的推荐系统越来越重视。尽管大多数现有研究独立探讨公平和多样性，我们识别出这两个领域之间的紧密连接。在这项调查中，我们首先分别讨论它们，然后深入探讨它们之间的联系。此外，受用户级和物品级公平性概念的启发，我们将多样性的理解扩展到不仅包括物品级别，还包括用户级别。通过对用户和物品级别多样性的扩展观点，我们从多样性的角度重新解释公平性研究。这种新视角增进了我们对与公平有关的工作的理解。

    arXiv:2307.04644v2 Announce Type: replace  Abstract: Recommender systems are effective tools for mitigating information overload and have seen extensive applications across various domains. However, the single focus on utility goals proves to be inadequate in addressing real-world concerns, leading to increasing attention to fairness-aware and diversity-aware recommender systems. While most existing studies explore fairness and diversity independently, we identify strong connections between these two domains. In this survey, we first discuss each of them individually and then dive into their connections. Additionally, motivated by the concepts of user-level and item-level fairness, we broaden the understanding of diversity to encompass not only the item level but also the user level. With this expanded perspective on user and item-level diversity, we re-interpret fairness studies from the viewpoint of diversity. This fresh perspective enhances our understanding of fairness-related work
    
[^52]: 选择并非只有关注力：路径推断在可解释的基于路径的推荐中的应用

    Attention Is Not the Only Choice: Counterfactual Reasoning for Path-Based Explainable Recommendation. (arXiv:2401.05744v1 [cs.IR])

    [http://arxiv.org/abs/2401.05744](http://arxiv.org/abs/2401.05744)

    这项研究提出了一种基于反事实推理的可解释框架，用于路径推荐，通过学习路径的可解释权重来替代关注权重。这种框架比传统的关注机制更稳定且更符合人类直觉。

    

    与仅追求推荐准确性相比，推荐模型的可解释性近年来更受关注。许多基于图的推荐方法使用关注机制来解释推荐过程中的路径。然而，这些关注权重是为了模型的准确性而设计的，而非可解释性。近期，一些研究者开始对基于关注机制的可解释性提出质疑，因为这些关注权重在不同的运行中是不稳定的，且不一定与人类的直觉一致。受因果学习理论中的反事实推理启发，我们提出了一种新颖的可解释框架，用于基于路径的推荐，其中通过学习路径的可解释权重来替代关注权重。具体来说，我们从路径表示和路径拓扑结构的角度设计了两种反事实推理算法。此外，与传统的案例研究不同，我们还提出了...

    Compared with only pursuing recommendation accuracy, the explainability of a recommendation model has drawn more attention in recent years. Many graph-based recommendations resort to informative paths with the attention mechanism for the explanation. Unfortunately, these attention weights are intentionally designed for model accuracy but not explainability. Recently, some researchers have started to question attention-based explainability because the attention weights are unstable for different reproductions, and they may not always align with human intuition. Inspired by the counterfactual reasoning from causality learning theory, we propose a novel explainable framework targeting path-based recommendations, wherein the explainable weights of paths are learned to replace attention weights. Specifically, we design two counterfactual reasoning algorithms from both path representation and path topological structure perspectives. Moreover, unlike traditional case studies, we also propose 
    
[^53]: Starling: 一种用于高维向量相似性搜索的I/O高效的基于磁盘的图索引框架，用于数据片段中 (arXiv:2401.02116v1 [cs.DB])

    Starling: An I/O-Efficient Disk-Resident Graph Index Framework for High-Dimensional Vector Similarity Search on Data Segment. (arXiv:2401.02116v1 [cs.DB])

    [http://arxiv.org/abs/2401.02116](http://arxiv.org/abs/2401.02116)

    Starling是一种I/O高效的基于磁盘的图索引框架，用于在数据片段上进行高维向量相似性搜索，在准确性、效率和空间成本之间取得平衡。

    

    高维向量相似性搜索(HVSS)作为数据科学和人工智能应用的强大工具，正受到关注。随着向量数据的增长，内存索引变得非常昂贵，因为它们需要大量扩展主内存资源。一种可能的解决方案是使用基于磁盘的实现，将向量数据存储和搜索在高性能设备(如NVMe SSD)中。然而，对于数据片段的HVSS仍然是向量数据库中的挑战，其中一个机器有多个片段来实现系统功能（如扩展）。在这种情况下，每个片段的内存和磁盘空间有限，因此数据片段上的HVSS需要在准确性，效率和空间成本之间取得平衡。现有的基于磁盘的方法并没有同时考虑到所有这些要求。在本文中，我们提出了Starling，一种I/O高效的基于磁盘的图索引框架，它在片段中优化数据布局和搜索策略。

    High-dimensional vector similarity search (HVSS) is receiving a spotlight as a powerful tool for various data science and AI applications. As vector data grows larger, in-memory indexes become extremely expensive because they necessitate substantial expansion of main memory resources. One possible solution is to use disk-based implementation, which stores and searches vector data in high-performance devices like NVMe SSDs. However, HVSS for data segments is still challenging in vector databases, where one machine has multiple segments for system features (like scaling) purposes. In this setting, each segment has limited memory and disk space, so HVSS on the data segment needs to balance accuracy, efficiency, and space cost. Existing disk-based methods are sub-optimal because they do not consider all these requirements together. In this paper, we present Starling, an I/O-efficient disk-resident graph index framework that optimizes data layout and search strategy in the segment. It has t
    
[^54]: 对话式人工智能的数据增强

    Data Augmentation for Conversational AI. (arXiv:2309.04739v1 [cs.CL])

    [http://arxiv.org/abs/2309.04739](http://arxiv.org/abs/2309.04739)

    本教程提供了对话式人工智能中数据增强的综述，包括对话增强、开放域和任务导向的对话生成以及评估模型。此外，还讨论了当前的挑战和未来的发展方向，以帮助推动该领域的发展。

    

    对话系统的发展已经彻底改变了信息获取方式，超越了单一查询的限制。然而，开发对话系统需要大量的训练数据，在资源有限的领域和语言中具有挑战性。传统的数据收集方法，如众包，需要大量的人力和时间，因此在此情景下效率低下。数据增强（DA）是一种缓解对话系统中数据稀缺问题的有效方法。本教程全面且最新地概述了在对话系统中使用的DA方法，包括对话增强、开放域和任务导向的对话生成以及不同的评估模型的范式。我们还讨论了当前的挑战和未来的发展方向，以帮助研究人员和从业者进一步推动这一领域的发展。

    Advancements in conversational systems have revolutionized information access, surpassing the limitations of single queries. However, developing dialogue systems requires a large amount of training data, which is a challenge in low-resource domains and languages. Traditional data collection methods like crowd-sourcing are labor-intensive and time-consuming, making them ineffective in this context. Data augmentation (DA) is an affective approach to alleviate the data scarcity problem in conversational systems. This tutorial provides a comprehensive and up-to-date overview of DA approaches in the context of conversational systems. It highlights recent advances in conversation augmentation, open domain and task-oriented conversation generation, and different paradigms of evaluating these models. We also discuss current challenges and future directions in order to help researchers and practitioners to further advance the field in this area.
    
[^55]: Mol-Instructions: 一个大规模生物分子指令数据集，为大语言模型提供支持

    Mol-Instructions: A Large-Scale Biomolecular Instruction Dataset for Large Language Models. (arXiv:2306.08018v1 [q-bio.QM])

    [http://arxiv.org/abs/2306.08018](http://arxiv.org/abs/2306.08018)

    Mol-Instructions是一个专门为生物分子领域设计的综合指令数据集，可以显著提高大语言模型在生物领域中的适应能力和认知敏锐度。

    

    大语言模型（LLM）以其卓越的任务处理能力和创新的输出，在许多领域推动了重大进展。然而，它们在生物分子研究等专业领域的熟练应用还受到限制。为了解决这个挑战，我们介绍了Mol-Instructions，这是一个经过精心策划、专门针对生物分子领域设计的综合指令数据集。Mol-Instructions由三个关键组成部分组成：分子导向指令、蛋白质导向指令和生物分子文本指令，每个部分都被策划用于增强LLM对生物分子特性和行为的理解和预测能力。通过对代表性LLM的广泛指令调整实验，我们强调了Mol-Instructions在增强大模型在生物分子研究复杂领域内的适应能力和认知敏锐度方面的潜力，从而促进生物分子领域的进一步发展。

    Large Language Models (LLMs), with their remarkable task-handling capabilities and innovative outputs, have catalyzed significant advancements across a spectrum of fields. However, their proficiency within specialized domains such as biomolecular studies remains limited. To address this challenge, we introduce Mol-Instructions, a meticulously curated, comprehensive instruction dataset expressly designed for the biomolecular realm. Mol-Instructions is composed of three pivotal components: molecule-oriented instructions, protein-oriented instructions, and biomolecular text instructions, each curated to enhance the understanding and prediction capabilities of LLMs concerning biomolecular features and behaviors. Through extensive instruction tuning experiments on the representative LLM, we underscore the potency of Mol-Instructions to enhance the adaptability and cognitive acuity of large models within the complex sphere of biomolecular studies, thereby promoting advancements in the biomol
    

