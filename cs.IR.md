# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Link Prediction under Heterophily: A Physics-Inspired Graph Neural Network Approach](https://arxiv.org/abs/2402.14802) | 图神经网络在异质图上的链路预测面临学习能力和表达能力方面的挑战，本论文提出了受物理启发的方法以增强节点分类性能。 |
| [^2] | [IEPile: Unearthing Large-Scale Schema-Based Information Extraction Corpus](https://arxiv.org/abs/2402.14710) | 发布了IEPile，一个包含约0.32B个标记的综合双语IE指令语料库，通过收集和清理33个现有IE数据集并引入基于模式的指令生成，可以提高大型语言模型在信息抽取领域的性能，尤其是零样本泛化。 |
| [^3] | [From Keywords to Structured Summaries: Streamlining Scholarly Knowledge Access](https://arxiv.org/abs/2402.14622) | 该论文突出了信息检索引擎在科学界的重要性，并提出了一种通过结构化记录和先进信息技术工具实现的解决方案，以革新研究人员访问和过滤文章的方式。 |
| [^4] | [Improving Assessment of Tutoring Practices using Retrieval-Augmented Generation](https://arxiv.org/abs/2402.14594) | 通过利用生成预训练变换器来自动评估辅导员使用社交情感辅导策略的能力，从而改进辅导实践评估。 |
| [^5] | [Scaling Up LLM Reviews for Google Ads Content Moderation](https://arxiv.org/abs/2402.14590) | 本研究提出了一种用于在Google广告中进行内容管理的方法，通过使用LLMs审核代表性广告并将决策传播回其群集，将审核数目减少了3个数量级以上，同时实现了2倍的召回率。 |
| [^6] | [Transforming Norm-based To Graph-based Spatial Representation for Spatio-Temporal Epidemiological Models](https://arxiv.org/abs/2402.14539) | 研究介绍了将基于规范的空间表示转换为基于图的时空流行病学模型的新框架和方法。 |
| [^7] | [Personalized Behavior-Aware Transformer for Multi-Behavior Sequential Recommendation](https://arxiv.org/abs/2402.14473) | 个性化行为感知Transformer框架用于多行为顺序推荐，旨在更好地探索用户的潜在意图，并解决短序列下推荐性能降低的问题。 |
| [^8] | [Recommender for Its Purpose: Repeat and Exploration in Food Delivery Recommendations](https://arxiv.org/abs/2402.14440) | 本文针对食品配送推荐中的重复和探索消费问题进行了研究，发现现有情境感知方法难以有效解决这两个任务。 |
| [^9] | [Tug-of-War Between Knowledge: Exploring and Resolving Knowledge Conflicts in Retrieval-Augmented Language Models](https://arxiv.org/abs/2402.14409) | 本文探讨了检索增强语言模型中的知识冲突，提出了评估框架，研究了RALMs对内部记忆和外部来源间的冲突，发现了它们会偏向错误的内部记忆。 |
| [^10] | [Ensure Timeliness and Accuracy: A Novel Sliding Window Data Stream Paradigm for Live Streaming Recommendation](https://arxiv.org/abs/2402.14399) | 提出了一种名为Sliver的滑动窗口数据流设计范式，通过减小窗口大小和实现滑动窗口来解决实时推荐系统中标签的及时性和准确性问题 |
| [^11] | [Scalable and Provably Fair Exposure Control for Large-Scale Recommender Systems](https://arxiv.org/abs/2402.14369) | 典型推荐方法忽视了推荐对项目和提供者的影响，而本研究提出了一种可扩展和可证明公平暴露控制方法，解决了工业规模推荐系统中曝光控制的问题。 |
| [^12] | [Towards Efficient Pareto-optimal Utility-Fairness between Groups in Repeated Rankings](https://arxiv.org/abs/2402.14305) | 引入了使用Expohedron解决帕累托最优效用-公平性问题的新方法，避免了Birkhoff-von Neumann分解的高计算复杂度。 |
| [^13] | [GenSERP: Large Language Models for Whole Page Presentation](https://arxiv.org/abs/2402.14301) | 该论文提出了GenSERP框架，利用大型语言模型动态整理搜索结果并根据用户查询生成连贯的搜索引擎结果页面。 |
| [^14] | [MerRec: A Large-scale Multipurpose Mercari Dataset for Consumer-to-Consumer Recommendation Systems](https://arxiv.org/abs/2402.14230) | 提出了MerRec，这是首个专门针对C2C推荐而提出的大规模数据集，填补了C2C推荐数据集中物品属性、用户多样性和规模等方面的缺失。 |
| [^15] | [BIRCO: A Benchmark of Information Retrieval Tasks with Complex Objectives](https://arxiv.org/abs/2402.14151) | BIRCO基准评估基于大型语言模型的信息检索系统对多方面用户目标的检索能力，发现新的检索协议和更强大的模型是解决复杂用户需求的必要条件。 |
| [^16] | [Combining Language and Graph Models for Semi-structured Information Extraction on the Web](https://arxiv.org/abs/2402.14129) | GraphScholarBERT是一种结合语言和图模型的信息抽取方法，能够在Web上半结构化信息中提取目标关系，并在零射领域和零射网站设置中将提取F1得分提高34.8％。 |
| [^17] | [Towards Trustworthy Reranking: A Simple yet Effective Abstention Mechanism](https://arxiv.org/abs/2402.12997) | 提出了一种适用于现实约束的轻量级弃权机制，特别适用于再排序阶段，通过数据驱动的方法达到有效性，并提供了开源代码以促进其更广泛的应用。 |
| [^18] | [Fairly Evaluating Large Language Model-based Recommendation Needs Revisit the Cross-Entropy Loss](https://arxiv.org/abs/2402.06216) | 本文指出现有的基于大规模语言模型的推荐方法在公平性和评估上存在问题，需要重新审视交叉熵损失并替代传统的点对/点对损失函数。并且通过理论和实验结果表明，现有的基于LLM的方法对于预测下一个项目的效果并不像宣称的那样有效。 |
| [^19] | [Knowledge Graphs Meet Multi-Modal Learning: A Comprehensive Survey](https://arxiv.org/abs/2402.05391) | 知识图谱与多模态学习的综述介绍了KG4MM和MM4KG两个主要方面，包括任务定义、构建进展、评估基准以及关键研究轨迹。 |
| [^20] | [RAG-Fusion: a New Take on Retrieval-Augmented Generation](https://arxiv.org/abs/2402.03367) | RAG-Fusion方法通过生成多个查询，并结合互惠排名融合技术，能够从不同角度上下文化原始查询，提供准确和全面的信息。这项研究在人工智能和自然语言处理应用方面有重要进展，并展示了全球和区域之间的转变。 |
| [^21] | [LLMs for Knowledge Graph Construction and Reasoning: Recent Capabilities and Future Opportunities](https://arxiv.org/abs/2305.13168) | 本研究全面评估了LLMs在知识图谱构建和推理领域的性能，发现GPT-4更适合作为推理助手，并在某些情况下超越了精调模型。 |
| [^22] | [Text Embeddings by Weakly-Supervised Contrastive Pre-training](https://arxiv.org/abs/2212.03533) | 本文提出了一种名为E5的文本嵌入模型，通过弱监督对比训练方式，在未经过标记数据的情况下，在多个任务中表现卓越，是第一个在BEIR检索基准测试上击败BM25基线的模型，在微调后在MTEB基准测试上获得最佳结果。 |
| [^23] | [Re4: Learning to Re-contrast, Re-attend, Re-construct for Multi-interest Recommendation](https://arxiv.org/abs/2208.08011) | Re4框架使用反向流重新审视每个兴趣嵌入，包括Re-contrast对兴趣嵌入进行对比学习使其彼此区分开，Re-attend确保兴趣-项目相关性估计 |
| [^24] | [A Survey on Trustworthy Recommender Systems](https://arxiv.org/abs/2207.12515) | 推荐系统在人类中心的人工智能中发挥着重要作用，但可能带来信任问题、不公平待遇和隐私担忧等负面影响，因此有必要开发值得信赖的推荐系统来减轻这些风险。 |
| [^25] | [MuGI: Enhancing Information Retrieval through Multi-Text Generation Intergration with Large Language Models.](http://arxiv.org/abs/2401.06311) | MuGI是一个简单而有效的多文本生成集成框架，它通过与大型语言模型合作生成多个伪参考文献，并将其与查询集成以提升信息检索性能。在实验中，MuGI模型在TREC DL数据集上的BM25性能上取得了18%以上的增强，并在BEIR上提高了7.5%。 |
| [^26] | [TREC iKAT 2023: The Interactive Knowledge Assistance Track Overview.](http://arxiv.org/abs/2401.01330) | TREC iKAT 2023是一个交互式的知识辅助任务，旨在开发适应用户交互和上下文的会话搜索代理。该任务还强调决策搜索任务，用户通过筛选数据和信息来进行决策和执行动作。 |
| [^27] | [Collaborative Large Language Model for Recommender Systems.](http://arxiv.org/abs/2311.01343) | 本研究提出了CLLM4Rec，首个将大型语言模型与推荐系统的 ID 模式紧密集成的协同推荐算法，旨在解决语义差距、虚假相关和低效推荐等问题。通过扩展预训练语言模型的词汇表，并引入软硬提示策略，该算法能够准确地模拟用户和项目的协同与内容语义。 |
| [^28] | [Graph-enhanced Optimizers for Structure-aware Recommendation Embedding Evolution.](http://arxiv.org/abs/2310.03032) | 本文提出了一种新颖的结构感知嵌入演化(SEvo)机制，能够以较低的计算开销将图结构信息注入到嵌入中，从而在现代推荐系统中实现更高效的性能。 |
| [^29] | [SilverRetriever: Advancing Neural Passage Retrieval for Polish Question Answering.](http://arxiv.org/abs/2309.08469) | SilverRetriever是一个特为波兰语问答系统开发的神经检索器，通过训练在多种数据集上取得了显著的改进效果，并且与更大的多语种模型具有竞争力。 |

# 详细

[^1]: 在异质性下的链路预测: 受物理启发的图神经网络方法

    Link Prediction under Heterophily: A Physics-Inspired Graph Neural Network Approach

    [https://arxiv.org/abs/2402.14802](https://arxiv.org/abs/2402.14802)

    图神经网络在异质图上的链路预测面临学习能力和表达能力方面的挑战，本论文提出了受物理启发的方法以增强节点分类性能。

    

    最近几年，由于其在对图表示的真实世界现象建模方面的灵活性，图神经网络（GNNs）已成为各种深度学习领域的事实标准。然而，GNNs的消息传递机制在学习能力和表达能力方面面临挑战，这限制了在异质图上实现高性能的能力，其中相邻节点经常具有不同的标签。大多数现有解决方案主要局限于针对节点分类任务的特定基准。这种狭窄的焦点限制了链路预测在多个应用中的潜在影响，包括推荐系统。例如，在社交网络中，两个用户可能由于某种潜在原因而连接，这使得提前预测这种连接具有挑战性。受物理启发的GNNs（如GRAFF）对提高节点分类性能提供了显著的贡献。

    arXiv:2402.14802v1 Announce Type: new  Abstract: In the past years, Graph Neural Networks (GNNs) have become the `de facto' standard in various deep learning domains, thanks to their flexibility in modeling real-world phenomena represented as graphs. However, the message-passing mechanism of GNNs faces challenges in learnability and expressivity, hindering high performance on heterophilic graphs, where adjacent nodes frequently have different labels. Most existing solutions addressing these challenges are primarily confined to specific benchmarks focused on node classification tasks. This narrow focus restricts the potential impact that link prediction under heterophily could offer in several applications, including recommender systems. For example, in social networks, two users may be connected for some latent reason, making it challenging to predict such connections in advance. Physics-Inspired GNNs such as GRAFF provided a significant contribution to enhance node classification perf
    
[^2]: IEPile: 挖掘大规模基于模式的信息抽取语料库

    IEPile: Unearthing Large-Scale Schema-Based Information Extraction Corpus

    [https://arxiv.org/abs/2402.14710](https://arxiv.org/abs/2402.14710)

    发布了IEPile，一个包含约0.32B个标记的综合双语IE指令语料库，通过收集和清理33个现有IE数据集并引入基于模式的指令生成，可以提高大型语言模型在信息抽取领域的性能，尤其是零样本泛化。

    

    大型语言模型（LLMs）在各个领域展现出了显著的潜力；然而，在信息抽取（IE）方面表现出了显著的性能差距。高质量的指令数据是提升LLMs特定能力的关键，而当前的IE数据集往往规模较小、分散且缺乏标准化的模式。因此，我们介绍了IEPile，一个综合的双语（英文和中文）IE指令语料库，包含约0.32B个标记。我们通过收集和清理33个现有IE数据集构建IEPile，并引入基于模式的指令生成来挖掘大规模语料库。在LLaMA和Baichuan上的实验结果表明，使用IEPile可以提高LLMs在IE方面的性能，尤其是零样本泛化。我们开源了资源和预训练模型，希望为自然语言处理社区提供有价值的支持。

    arXiv:2402.14710v1 Announce Type: cross  Abstract: Large Language Models (LLMs) demonstrate remarkable potential across various domains; however, they exhibit a significant performance gap in Information Extraction (IE). Note that high-quality instruction data is the vital key for enhancing the specific capabilities of LLMs, while current IE datasets tend to be small in scale, fragmented, and lack standardized schema. To this end, we introduce IEPile, a comprehensive bilingual (English and Chinese) IE instruction corpus, which contains approximately 0.32B tokens. We construct IEPile by collecting and cleaning 33 existing IE datasets, and introduce schema-based instruction generation to unearth a large-scale corpus. Experimental results on LLaMA and Baichuan demonstrate that using IEPile can enhance the performance of LLMs for IE, especially the zero-shot generalization. We open-source the resource and pre-trained models, hoping to provide valuable support to the NLP community.
    
[^3]: 从关键词到结构化摘要: 精简学术知识获取

    From Keywords to Structured Summaries: Streamlining Scholarly Knowledge Access

    [https://arxiv.org/abs/2402.14622](https://arxiv.org/abs/2402.14622)

    该论文突出了信息检索引擎在科学界的重要性，并提出了一种通过结构化记录和先进信息技术工具实现的解决方案，以革新研究人员访问和过滤文章的方式。

    

    这篇短文强调了信息检索引擎在科学界日益重要，指出传统基于关键词的搜索引擎由于出版物数量不断增加而效率低下。提出的解决方案涉及结构化记录，支持先进的信息技术工具，包括可视化仪表板，以彻底改变研究人员如何访问和过滤文章，取代传统的文本密集型方法。这一愿景通过一个以“传染病的繁殖数估计”研究主题为中心的概念验证得以体现，使用经过调整的大型语言模型(LLM)自动创建结构化记录以填充一个超越关键词的后端数据库。结果是一个下一代信息检索方法，可在https://orkg.org/usecases/r0-estimates 上访问。

    arXiv:2402.14622v1 Announce Type: cross  Abstract: This short paper highlights the growing importance of information retrieval (IR) engines in the scientific community, addressing the inefficiency of traditional keyword-based search engines due to the rising volume of publications. The proposed solution involves structured records, underpinning advanced information technology (IT) tools, including visualization dashboards, to revolutionize how researchers access and filter articles, replacing the traditional text-heavy approach. This vision is exemplified through a proof of concept centered on the ``reproductive number estimate of infectious diseases'' research theme, using a fine-tuned large language model (LLM) to automate the creation of structured records to populate a backend database that now goes beyond keywords. The result is a next-generation IR method accessible at https://orkg.org/usecases/r0-estimates.
    
[^4]: 利用检索增强生成改进辅导实践评估

    Improving Assessment of Tutoring Practices using Retrieval-Augmented Generation

    [https://arxiv.org/abs/2402.14594](https://arxiv.org/abs/2402.14594)

    通过利用生成预训练变换器来自动评估辅导员使用社交情感辅导策略的能力，从而改进辅导实践评估。

    

    一对一辅导是提高学习效果的有效教学方法，然而其效力取决于辅导员的能力。新手数学辅导员通常优先考虑特定内容的指导，忽视社交情感学习等方面。社交情感学习促进了公平和包容性，并培养与学生的关系，这对于学生整体发展至关重要。准确有效地评估辅导员的能力可以推动定制的辅导员培训计划的发展。然而，在实时辅导中评估新手辅导员的能力仍然具有挑战性，因为通常需要专家参与。为解决这一挑战，本初步研究旨在利用生成预训练变换器（GPT），如GPT-3.5和GPT-4模型，自动评估辅导员使用社交情感辅导策略的能力。此外，本研究还报告了财务维度的内容。

    arXiv:2402.14594v1 Announce Type: cross  Abstract: One-on-one tutoring is an effective instructional method for enhancing learning, yet its efficacy hinges on tutor competencies. Novice math tutors often prioritize content-specific guidance, neglecting aspects such as social-emotional learning. Social-emotional learning promotes equity and inclusion and nurturing relationships with students, which is crucial for holistic student development. Assessing the competencies of tutors accurately and efficiently can drive the development of tailored tutor training programs. However, evaluating novice tutor ability during real-time tutoring remains challenging as it typically requires experts-in-the-loop. To address this challenge, this preliminary study aims to harness Generative Pre-trained Transformers (GPT), such as GPT-3.5 and GPT-4 models, to automatically assess tutors' ability of using social-emotional tutoring strategies. Moreover, this study also reports on the financial dimensions an
    
[^5]: 扩展LLM审核以进行Google广告内容管理

    Scaling Up LLM Reviews for Google Ads Content Moderation

    [https://arxiv.org/abs/2402.14590](https://arxiv.org/abs/2402.14590)

    本研究提出了一种用于在Google广告中进行内容管理的方法，通过使用LLMs审核代表性广告并将决策传播回其群集，将审核数目减少了3个数量级以上，同时实现了2倍的召回率。

    

    大型语言模型（LLMs）是内容管理的强大工具，但它们的推理成本和延迟使它们在大型数据集（如Google Ads存储库）上的临时使用成本过高。本研究提出了一种方法，用于扩展LLM审核以在Google Ads中进行内容管理。首先，我们使用启发式方法通过过滤和重复项删除来选择候选项，并为此创建广告群集，我们选择每个群集的一个代表性广告。然后，我们使用LLMs仅审核代表性广告。最后，我们将代表性广告的LLM决策传播回它们的群集。该方法将审核数目减少了3个数量级以上，同时与基线非LLM模型相比实现了2倍的召回率。该方法的成功在很大程度上取决于聚类和标签传播中使用的表示; 我们发现交叉模态相似性表示产生比单一模态更好的结果。

    arXiv:2402.14590v1 Announce Type: cross  Abstract: Large language models (LLMs) are powerful tools for content moderation, but their inference costs and latency make them prohibitive for casual use on large datasets, such as the Google Ads repository. This study proposes a method for scaling up LLM reviews for content moderation in Google Ads. First, we use heuristics to select candidates via filtering and duplicate removal, and create clusters of ads for which we select one representative ad per cluster. We then use LLMs to review only the representative ads. Finally, we propagate the LLM decisions for the representative ads back to their clusters. This method reduces the number of reviews by more than 3 orders of magnitude while achieving a 2x recall compared to a baseline non-LLM model. The success of this approach is a strong function of the representations used in clustering and label propagation; we found that cross-modal similarity representations yield better results than uni-m
    
[^6]: 将基于规范的空间表示转换为基于图的时空流行病学模型

    Transforming Norm-based To Graph-based Spatial Representation for Spatio-Temporal Epidemiological Models

    [https://arxiv.org/abs/2402.14539](https://arxiv.org/abs/2402.14539)

    研究介绍了将基于规范的空间表示转换为基于图的时空流行病学模型的新框架和方法。

    

    疫情大流行以其深远的社会和经济影响，对全球健康、死亡率、经济稳定性和政治格局构成重大威胁。为了应对新兴和再现疫情带来的持续挑战，许多研究采用时空模型来增强我们对这些复杂现象的理解和管理。这些时空模型可以大致分为两种主要空间类别：基于规范和基于图的模型，在准确性、计算负担和可表示性之间进行权衡。在本研究中，我们探讨了将这些模型从基于规范转换为基于图的空间表示的能力。我们引入了一个新颖的框架，以及使用广泛的启发式优化方法的十二种可能实现。我们的研究结果显示，通过利用基于代理的模拟和启发式算法来实现图模型

    arXiv:2402.14539v1 Announce Type: new  Abstract: Pandemics, with their profound societal and economic impacts, pose significant threats to global health, mortality rates, economic stability, and political landscapes. In response to the persistent challenges posed by emerging and reemerging pandemics, numerous studies have employed spatio-temporal models to enhance our understanding and management of these complex phenomena. These spatio-temporal models can be roughly divided into two main spatial categories: norm-based and graph-based trade-offering between accuracy, computational burden, and representational feasibility. In this study, we explore the ability to transform from norm-based to graph-based spatial representation for these models. We introduce a novel framework for this task together with twelve possible implementations using a wide range of heuristic optimization approaches. Our findings show that by leveraging agent-based simulations and heuristic algorithms for the graph
    
[^7]: 个性化行为感知Transformer用于多行为顺序推荐

    Personalized Behavior-Aware Transformer for Multi-Behavior Sequential Recommendation

    [https://arxiv.org/abs/2402.14473](https://arxiv.org/abs/2402.14473)

    个性化行为感知Transformer框架用于多行为顺序推荐，旨在更好地探索用户的潜在意图，并解决短序列下推荐性能降低的问题。

    

    Sequential Recommendation (SR)通过建模用户在物品之间转换的方式来捕捉用户的动态偏好。然而，仅利用单一类型的行为交互数据的SR模型在序列较短时性能会下降。为了解决这个问题，本文关注多行为顺序推荐(MBSR)，旨在利用时变异构行为依赖关系更好地探索用户在目标行为上的潜在意图。解决MBSR问题具有挑战性。一方面，由于个人特征，用户展现出多样化的多行为模式。另一方面，行为相关性和物品协作之间存在全面的相互影响，其强度深受时间因素影响。为了解决这些挑战，我们提出了一个针对MBSR问题的Personalized Behavior-Aware Transformer框架(PBAT)，该框架可以建模个性化模式

    arXiv:2402.14473v1 Announce Type: cross  Abstract: Sequential Recommendation (SR) captures users' dynamic preferences by modeling how users transit among items. However, SR models that utilize only single type of behavior interaction data encounter performance degradation when the sequences are short. To tackle this problem, we focus on Multi-Behavior Sequential Recommendation (MBSR) in this paper, which aims to leverage time-evolving heterogeneous behavioral dependencies for better exploring users' potential intents on the target behavior. Solving MBSR is challenging. On the one hand, users exhibit diverse multi-behavior patterns due to personal characteristics. On the other hand, there exists comprehensive co-influence between behavior correlations and item collaborations, the intensity of which is deeply affected by temporal factors. To tackle these challenges, we propose a Personalized Behavior-Aware Transformer framework (PBAT) for MBSR problem, which models personalized patterns 
    
[^8]: 针对其目的的推荐系统：食品配送推荐中的重复和探索

    Recommender for Its Purpose: Repeat and Exploration in Food Delivery Recommendations

    [https://arxiv.org/abs/2402.14440](https://arxiv.org/abs/2402.14440)

    本文针对食品配送推荐中的重复和探索消费问题进行了研究，发现现有情境感知方法难以有效解决这两个任务。

    

    推荐系统已被广泛应用于各种场景，例如电子商务、新闻和音乐，提供在线内容以帮助和丰富用户的日常生活。在本文中，我们专注于食品配送推荐，揭示该领域的独特特征，用户在线订购食物，并在送达后不久享用餐点。我们首先对食品配送数据集进行深入分析。分析显示，用户和商家均存在重复订单，并且情境不同影响着食品配送推荐系统中的重复和探索消费。此外，我们重新审视现有情境感知方法分别用于重复和探索推荐，并发现它们无法有效地同时解决这两个任务。

    arXiv:2402.14440v1 Announce Type: new  Abstract: Recommender systems have been widely used for various scenarios, such as e-commerce, news, and music, providing online contents to help and enrich users' daily life. Different scenarios hold distinct and unique characteristics, calling for domain-specific investigations and corresponding designed recommender systems. Therefore, in this paper, we focus on food delivery recommendations to unveil unique features in this domain, where users order food online and enjoy their meals shortly after delivery. We first conduct an in-depth analysis on food delivery datasets. The analysis shows that repeat orders are prevalent for both users and stores, and situations' differently influence repeat and exploration consumption in the food delivery recommender systems. Moreover, we revisit the ability of existing situation-aware methods for repeat and exploration recommendations respectively, and find them unable to effectively solve both tasks simultan
    
[^9]: 知识之间的拉锯战: 探索和解决检索增强语言模型中的知识冲突

    Tug-of-War Between Knowledge: Exploring and Resolving Knowledge Conflicts in Retrieval-Augmented Language Models

    [https://arxiv.org/abs/2402.14409](https://arxiv.org/abs/2402.14409)

    本文探讨了检索增强语言模型中的知识冲突，提出了评估框架，研究了RALMs对内部记忆和外部来源间的冲突，发现了它们会偏向错误的内部记忆。

    

    检索增强语言模型（RALMs）已经在通过从外部来源检索证据来优化和扩展其内部记忆方面表现出重要潜力。然而，RALMs在将内部记忆与外部来源整合时必然会遇到知识冲突。知识冲突会使RALMs陷入知识之间的拉锯战，限制其实际应用。本文着重于探索和解决RALMs中的知识冲突。首先，我们提出了一个评估框架，用于评估不同维度上的知识冲突。然后，我们从以下两个角度研究了RALMs的行为和偏好：（1）内部记忆与外部来源之间的冲突：我们发现，随着邓宁-克鲁格效应的增强，更强大的RALMs会持续偏爱其错误的内部记忆，即使提供了正确的证据。此外，RALMs还表现出一种可用性

    arXiv:2402.14409v1 Announce Type: cross  Abstract: Retrieval-augmented language models (RALMs) have demonstrated significant potential in refining and expanding their internal memory by retrieving evidence from external sources. However, RALMs will inevitably encounter knowledge conflicts when integrating their internal memory with external sources. Knowledge conflicts can ensnare RALMs in a tug-of-war between knowledge, limiting their practical applicability. In this paper, we focus on exploring and resolving knowledge conflicts in RALMs. First, we present an evaluation framework for assessing knowledge conflicts across various dimensions. Then, we investigate the behavior and preference of RALMs from the following two perspectives: (1) Conflicts between internal memory and external sources: We find that stronger RALMs emerge with the Dunning-Kruger effect, persistently favoring their faulty internal memory even when correct evidence is provided. Besides, RALMs exhibit an availability
    
[^10]: 确保及时性和准确性：一种新的滑动窗口数据流范式用于实时推荐

    Ensure Timeliness and Accuracy: A Novel Sliding Window Data Stream Paradigm for Live Streaming Recommendation

    [https://arxiv.org/abs/2402.14399](https://arxiv.org/abs/2402.14399)

    提出了一种名为Sliver的滑动窗口数据流设计范式，通过减小窗口大小和实现滑动窗口来解决实时推荐系统中标签的及时性和准确性问题

    

    Live streaming recommender system专为向用户推荐实时感兴趣的直播流而设计。由于直播内容动态变化，提高推荐系统的及时性是一个关键问题。本文提出了一种新的数据流设计范式，名为Sliver，通过减小窗口大小和相应实现滑动窗口来解决标签的及时性和准确性问题。

    arXiv:2402.14399v1 Announce Type: cross  Abstract: Live streaming recommender system is specifically designed to recommend real-time live streaming of interest to users. Due to the dynamic changes of live content, improving the timeliness of the live streaming recommender system is a critical problem. Intuitively, the timeliness of the data determines the upper bound of the timeliness that models can learn. However, none of the previous works addresses the timeliness problem of the live streaming recommender system from the perspective of data stream design. Employing the conventional fixed window data stream paradigm introduces a trade-off dilemma between labeling accuracy and timeliness. In this paper, we propose a new data stream design paradigm, dubbed Sliver, that addresses the timeliness and accuracy problem of labels by reducing the window size and implementing a sliding window correspondingly. Meanwhile, we propose a time-sensitive re-reco strategy reducing the latency between 
    
[^11]: 大规模推荐系统的可扩展和可证明公平暴露控制

    Scalable and Provably Fair Exposure Control for Large-Scale Recommender Systems

    [https://arxiv.org/abs/2402.14369](https://arxiv.org/abs/2402.14369)

    典型推荐方法忽视了推荐对项目和提供者的影响，而本研究提出了一种可扩展和可证明公平暴露控制方法，解决了工业规模推荐系统中曝光控制的问题。

    

    典型的推荐和排名方法旨在优化用户的满意度，但通常忽视它们对项目（例如产品、工作、新闻、视频）及其提供者的影响。然而，人们日益意识到后者对于许多应用程序至关重要，因为它决定了被推荐人的效用。先前的公平感知推荐方法优化了一个正则化的目标，以平衡用户满意度和基于某些概念（如曝光公平性）的项目公平性。这些现有方法已被证明在控制公平性方面是有效的，但大多数方法在计算上效率低下，限制了它们仅适用于不切实际的小规模情况。这实际上意味着文献尚未提供一种解决方案，以在工业规模的推荐系统中实现对曝光的灵活控制，其中有数以百万计的用户。

    arXiv:2402.14369v1 Announce Type: new  Abstract: Typical recommendation and ranking methods aim to optimize the satisfaction of users, but they are often oblivious to their impact on the items (e.g., products, jobs, news, video) and their providers. However, there has been a growing understanding that the latter is crucial to consider for a wide range of applications, since it determines the utility of those being recommended. Prior approaches to fairness-aware recommendation optimize a regularized objective to balance user satisfaction and item fairness based on some notion such as exposure fairness. These existing methods have been shown to be effective in controlling fairness, however, most of them are computationally inefficient, limiting their applications to only unrealistically small-scale situations. This indeed implies that the literature does not yet provide a solution to enable a flexible control of exposure in the industry-scale recommender systems where millions of users a
    
[^12]: 有效实现在重复排名中群体间帕累托最优的效用—公平性

    Towards Efficient Pareto-optimal Utility-Fairness between Groups in Repeated Rankings

    [https://arxiv.org/abs/2402.14305](https://arxiv.org/abs/2402.14305)

    引入了使用Expohedron解决帕累托最优效用-公平性问题的新方法，避免了Birkhoff-von Neumann分解的高计算复杂度。

    

    在本文中，我们致力于解决计算具有帕累托最优平衡保证的一系列排名的问题，其中考虑了（1）最大化消费者效用和（2）最小化物品生产者之间的不公平性。这样的多目标优化问题通常使用标量化方法和线性规划来解决，基于表示物品可能排名分布的双随机矩阵。然而，上述方法依赖于Birkhoff-von Neumann（BvN）分解，其计算复杂度为$\mathcal{O}(n^5)$，其中$n$是物品数量，这使得在大规模系统中变得不切实际。为解决这一缺陷，我们引入了一种新颖的方法，通过使用Expohedron来解决上述问题 - 一个表征所有可达到物品曝光的排列多面体。在Expohedron上，我们绘制了帕累托曲线，捕捉了在最大化效用和最小化不公平性之间的权衡。

    arXiv:2402.14305v1 Announce Type: cross  Abstract: In this paper, we tackle the problem of computing a sequence of rankings with the guarantee of the Pareto-optimal balance between (1) maximizing the utility of the consumers and (2) minimizing unfairness between producers of the items. Such a multi-objective optimization problem is typically solved using a combination of a scalarization method and linear programming on bi-stochastic matrices, representing the distribution of possible rankings of items. However, the above-mentioned approach relies on Birkhoff-von Neumann (BvN) decomposition, of which the computational complexity is $\mathcal{O}(n^5)$ with $n$ being the number of items, making it impractical for large-scale systems. To address this drawback, we introduce a novel approach to the above problem by using the Expohedron - a permutahedron whose points represent all achievable exposures of items. On the Expohedron, we profile the Pareto curve which captures the trade-off betwee
    
[^13]: GenSERP: 用于整个页面呈现的大型语言模型

    GenSERP: Large Language Models for Whole Page Presentation

    [https://arxiv.org/abs/2402.14301](https://arxiv.org/abs/2402.14301)

    该论文提出了GenSERP框架，利用大型语言模型动态整理搜索结果并根据用户查询生成连贯的搜索引擎结果页面。

    

    大语言模型（LLMs）的出现为最小化搜索引擎结果页面（SERP）的组织工作带来了机会。本文提出了GenSERP，这是一个利用LLMs和视觉在少样本设置中动态组织中间搜索结果的框架，包括生成的聊天答案、网站摘要、多媒体数据、知识面板等，并根据用户的查询以连贯的SERP布局呈现。

    arXiv:2402.14301v1 Announce Type: cross  Abstract: The advent of large language models (LLMs) brings an opportunity to minimize the effort in search engine result page (SERP) organization. In this paper, we propose GenSERP, a framework that leverages LLMs with vision in a few-shot setting to dynamically organize intermediate search results, including generated chat answers, website snippets, multimedia data, knowledge panels into a coherent SERP layout based on a user's query. Our approach has three main stages: (1) An information gathering phase where the LLM continuously orchestrates API tools to retrieve different types of items, and proposes candidate layouts based on the retrieved items, until it's confident enough to generate the final result. (2) An answer generation phase where the LLM populates the layouts with the retrieved content. In this phase, the LLM adaptively optimize the ranking of items and UX configurations of the SERP. Consequently, it assigns a location on the pag
    
[^14]: MerRec：用于消费者对消费者推荐系统的大规模多功能Mercari数据集

    MerRec: A Large-scale Multipurpose Mercari Dataset for Consumer-to-Consumer Recommendation Systems

    [https://arxiv.org/abs/2402.14230](https://arxiv.org/abs/2402.14230)

    提出了MerRec，这是首个专门针对C2C推荐而提出的大规模数据集，填补了C2C推荐数据集中物品属性、用户多样性和规模等方面的缺失。

    

    在不断发展的电子商务领域中，推荐系统至关重要地塑造了用户体验和参与度。消费者对消费者（C2C）推荐系统的崛起，以其灵活性和为客户供应商提供易于访问的特点，标志着一个重要趋势。然而，学术关注主要集中在商家对消费者（B2C）模型上，留下了一个空白，即缺乏物品属性、用户多样性和规模的C2C推荐数据集。C2C推荐系统的复杂性进一步突出了用户扮演卖家和买家两种角色的双重性质，引入了一系列不那么统一和多样化的输入。为解决这一问题，我们引入了MerRec，这是第一个专门用于C2C推荐的大规模数据集，源自Mercari电子商务平台，覆盖了2023年6个月内数百万用户和产品。MerRec不仅包括标准特征，如user_id、item_id和session_id

    arXiv:2402.14230v1 Announce Type: cross  Abstract: In the evolving e-commerce field, recommendation systems crucially shape user experience and engagement. The rise of Consumer-to-Consumer (C2C) recommendation systems, noted for their flexibility and ease of access for customer vendors, marks a significant trend. However, the academic focus remains largely on Business-to-Consumer (B2C) models, leaving a gap filled by the limited C2C recommendation datasets that lack in item attributes, user diversity, and scale. The intricacy of C2C recommendation systems is further accentuated by the dual roles users assume as both sellers and buyers, introducing a spectrum of less uniform and varied inputs. Addressing this, we introduce MerRec, the first large-scale dataset specifically for C2C recommendations, sourced from the Mercari e-commerce platform, covering millions of users and products over 6 months in 2023. MerRec not only includes standard features such as user_id, item_id, and session_id
    
[^15]: BIRCO：具有复杂目标的信息检索任务基准

    BIRCO: A Benchmark of Information Retrieval Tasks with Complex Objectives

    [https://arxiv.org/abs/2402.14151](https://arxiv.org/abs/2402.14151)

    BIRCO基准评估基于大型语言模型的信息检索系统对多方面用户目标的检索能力，发现新的检索协议和更强大的模型是解决复杂用户需求的必要条件。

    

    我们提出了具有复杂目标的信息检索(IR)任务基准(BIRCO)。 BIRCO评估IR系统根据多方面用户目标检索文档的能力。 该基准的复杂性和紧凑大小使其适用于评估基于大型语言模型(LLM)的信息检索系统。 我们提出了一个模块化框架，用于研究可能影响LLM在检索任务上的性能的因素，并确定了一个简单的基线模型，该模型与或优于现有方法和更复杂的替代方案。 没有一种方法在所有基准任务上均达到令人满意的性能，这表明需要更强大的模型和新的检索协议来解决复杂的用户需求。

    arXiv:2402.14151v1 Announce Type: cross  Abstract: We present the Benchmark of Information Retrieval (IR) tasks with Complex Objectives (BIRCO). BIRCO evaluates the ability of IR systems to retrieve documents given multi-faceted user objectives. The benchmark's complexity and compact size make it suitable for evaluating large language model (LLM)-based information retrieval systems. We present a modular framework for investigating factors that may influence LLM performance on retrieval tasks, and identify a simple baseline model which matches or outperforms existing approaches and more complex alternatives. No approach achieves satisfactory performance on all benchmark tasks, suggesting that stronger models and new retrieval protocols are necessary to address complex user needs.
    
[^16]: 结合语言和图模型进行Web上半结构化信息抽取

    Combining Language and Graph Models for Semi-structured Information Extraction on the Web

    [https://arxiv.org/abs/2402.14129](https://arxiv.org/abs/2402.14129)

    GraphScholarBERT是一种结合语言和图模型的信息抽取方法，能够在Web上半结构化信息中提取目标关系，并在零射领域和零射网站设置中将提取F1得分提高34.8％。

    

    关系抽取是在网络上挖掘人类知识的一种高效方式。现有方法依赖于特定领域的训练数据或产生嘈杂的输出。本文着重于从半结构化的网页中提取目标关系，仅给出关系的简短描述。我们提出了GraphScholarBERT，这是一种基于联合图和语言模型结构的开放领域信息提取方法。GraphScholarBERT能够泛化到以前未见过的领域，无需额外数据或训练，并且仅产生与搜索关键字匹配的干净提取结果。实验表明，与零射领域和零射网站设置中的先前工作相比，GraphScholarBERT可以将提取的F1得分提高多达34.8％。

    arXiv:2402.14129v1 Announce Type: cross  Abstract: Relation extraction is an efficient way of mining the extraordinary wealth of human knowledge on the Web. Existing methods rely on domain-specific training data or produce noisy outputs. We focus here on extracting targeted relations from semi-structured web pages given only a short description of the relation. We present GraphScholarBERT, an open-domain information extraction method based on a joint graph and language model structure. GraphScholarBERT can generalize to previously unseen domains without additional data or training and produces only clean extraction results matched to the search keyword. Experiments show that GraphScholarBERT can improve extraction F1 scores by as much as 34.8\% compared to previous work in a zero-shot domain and zero-shot website setting.
    
[^17]: 朝着可信的再排序：一种简单但有效的弃权机制

    Towards Trustworthy Reranking: A Simple yet Effective Abstention Mechanism

    [https://arxiv.org/abs/2402.12997](https://arxiv.org/abs/2402.12997)

    提出了一种适用于现实约束的轻量级弃权机制，特别适用于再排序阶段，通过数据驱动的方法达到有效性，并提供了开源代码以促进其更广泛的应用。

    

    神经信息检索（NIR）已经显著改进了基于启发式的IR系统。然而，失败仍然频繁发生，通常所使用的模型无法检索与用户查询相关的文档。我们通过提出一种适用于现实约束的轻量级弃权机制来解决这一挑战，特别强调再排序阶段。我们介绍了一个协议，用于在黑匣子场景中评估弃权策略的效果，并提出了一种简单但有效的数据驱动机制。我们提供了实验复制和弃权实施的开源代码，促进其在不同环境中更广泛的采用和应用。

    arXiv:2402.12997v1 Announce Type: cross  Abstract: Neural Information Retrieval (NIR) has significantly improved upon heuristic-based IR systems. Yet, failures remain frequent, the models used often being unable to retrieve documents relevant to the user's query. We address this challenge by proposing a lightweight abstention mechanism tailored for real-world constraints, with particular emphasis placed on the reranking phase. We introduce a protocol for evaluating abstention strategies in a black-box scenario, demonstrating their efficacy, and propose a simple yet effective data-driven mechanism. We provide open-source code for experiment replication and abstention implementation, fostering wider adoption and application in diverse contexts.
    
[^18]: 公平评估基于大规模语言模型的推荐方法需要重新审视交叉熵损失

    Fairly Evaluating Large Language Model-based Recommendation Needs Revisit the Cross-Entropy Loss

    [https://arxiv.org/abs/2402.06216](https://arxiv.org/abs/2402.06216)

    本文指出现有的基于大规模语言模型的推荐方法在公平性和评估上存在问题，需要重新审视交叉熵损失并替代传统的点对/点对损失函数。并且通过理论和实验结果表明，现有的基于LLM的方法对于预测下一个项目的效果并不像宣称的那样有效。

    

    大规模语言模型(LLM)在推荐领域引起了广泛关注；一些研究观察到，经过全softmax细调的LLM已经可以达到最先进的性能。然而，这些观点来自于主观和不公平的比较。鉴于现实中的大量物品，传统的推荐方法通常采用点对/点对损失函数进行训练。然而，这种替代方法会导致性能严重下降，低估传统方法的效果，并过高评估LLM的排序能力。本文从理论上证明了交叉熵的优越性，并展示了可以用一些基本近似方法进行适当替代的必要修改。在三个公共数据集上的显著结果证实，即使从实际意义上讲，现有的基于LLM的方法对于预测下一个项目的效果并不像宣称的那样有效。

    Large language models (LLMs) have gained much attention in the recommendation community; some studies have observed that LLMs, fine-tuned by the cross-entropy loss with a full softmax, could achieve state-of-the-art performance already. However, these claims are drawn from unobjective and unfair comparisons. In view of the substantial quantity of items in reality, conventional recommenders typically adopt a pointwise/pairwise loss function instead for training. This substitute however causes severe performance degradation, leading to under-estimation of conventional methods and over-confidence in the ranking capability of LLMs.   In this work, we theoretically justify the superiority of cross-entropy, and showcase that it can be adequately replaced by some elementary approximations with certain necessary modifications. The remarkable results across three public datasets corroborate that even in a practical sense, existing LLM-based methods are not as effective as claimed for next-item 
    
[^19]: 知识图谱与多模态学习：综述

    Knowledge Graphs Meet Multi-Modal Learning: A Comprehensive Survey

    [https://arxiv.org/abs/2402.05391](https://arxiv.org/abs/2402.05391)

    知识图谱与多模态学习的综述介绍了KG4MM和MM4KG两个主要方面，包括任务定义、构建进展、评估基准以及关键研究轨迹。

    

    知识图谱在推动各种人工智能应用方面起着关键作用，语义网络社区对多模态维度的探索为创新打开了新的途径。在本综述中，我们仔细审查了300多篇文章，重点关注了两个主要方面的知识图谱感知研究：以知识图谱支持多模态任务的KG驱动多模态（KG4MM）学习，将知识图谱研究扩展到多模态知识图谱（MM4KG）领域。我们从定义知识图谱和多模态知识图谱开始，然后探索它们的构建进展。我们的综述包括两个主要任务类别：KG感知的多模态学习任务，如图像分类和视觉问答，以及内在的多模态知识图谱任务，如多模态知识图谱补全和实体对齐，突出了具体的研究轨迹。对于这些任务中的大部分，我们提供了定义、评估基准，并进一步指出进行相关研究的重要见解。最后，我们讨论了cu

    Knowledge Graphs (KGs) play a pivotal role in advancing various AI applications, with the semantic web community's exploration into multi-modal dimensions unlocking new avenues for innovation. In this survey, we carefully review over 300 articles, focusing on KG-aware research in two principal aspects: KG-driven Multi-Modal (KG4MM) learning, where KGs support multi-modal tasks, and Multi-Modal Knowledge Graph (MM4KG), which extends KG studies into the MMKG realm. We begin by defining KGs and MMKGs, then explore their construction progress. Our review includes two primary task categories: KG-aware multi-modal learning tasks, such as Image Classification and Visual Question Answering, and intrinsic MMKG tasks like Multi-modal Knowledge Graph Completion and Entity Alignment, highlighting specific research trajectories. For most of these tasks, we provide definitions, evaluation benchmarks, and additionally outline essential insights for conducting relevant research. Finally, we discuss cu
    
[^20]: RAG-Fusion: 检索增强生成的新途径

    RAG-Fusion: a New Take on Retrieval-Augmented Generation

    [https://arxiv.org/abs/2402.03367](https://arxiv.org/abs/2402.03367)

    RAG-Fusion方法通过生成多个查询，并结合互惠排名融合技术，能够从不同角度上下文化原始查询，提供准确和全面的信息。这项研究在人工智能和自然语言处理应用方面有重要进展，并展示了全球和区域之间的转变。

    

    Infineon已经确定工程师、客户经理和客户迅速获取产品信息的需求。传统上，这个问题通过检索增强生成（RAG）聊天机器人来解决，但在这项研究中，我评估了新近流行的RAG-Fusion方法的使用。RAG-Fusion将RAG和互惠排名融合（RRF）相结合，通过生成多个查询，使用互惠分数对其进行再排序，并融合文档和分数。通过对准确性、相关性和全面性进行手动评估，我发现RAG-Fusion能够通过从不同的角度对原始查询进行上下文化，提供准确和全面的回答。然而，当生成的查询与原始查询的相关性不足时，有些答案偏离了主题。这项研究在人工智能（AI）和自然语言处理（NLP）应用方面取得了重要进展，并展示了全球和区域之间的变革。

    Infineon has identified a need for engineers, account managers, and customers to rapidly obtain product information. This problem is traditionally addressed with retrieval-augmented generation (RAG) chatbots, but in this study, I evaluated the use of the newly popularized RAG-Fusion method. RAG-Fusion combines RAG and reciprocal rank fusion (RRF) by generating multiple queries, reranking them with reciprocal scores and fusing the documents and scores. Through manually evaluating answers on accuracy, relevance, and comprehensiveness, I found that RAG-Fusion was able to provide accurate and comprehensive answers due to the generated queries contextualizing the original query from various perspectives. However, some answers strayed off topic when the generated queries' relevance to the original query is insufficient. This research marks significant progress in artificial intelligence (AI) and natural language processing (NLP) applications and demonstrates transformations in a global and m
    
[^21]: LLMs用于知识图谱构建和推理：最新功能与未来机遇

    LLMs for Knowledge Graph Construction and Reasoning: Recent Capabilities and Future Opportunities

    [https://arxiv.org/abs/2305.13168](https://arxiv.org/abs/2305.13168)

    本研究全面评估了LLMs在知识图谱构建和推理领域的性能，发现GPT-4更适合作为推理助手，并在某些情况下超越了精调模型。

    

    本文对大规模语言模型（LLMs）在知识图谱（KG）构建和推理中的数量化和质化评估进行了详尽的研究。我们在八个不同的数据集上进行了实验，重点关注涵盖实体和关系提取、事件提取、链接预测和问答四个典型任务，从而全面探索了LLMs在构建和推理领域的表现。经验性研究发现，以GPT-4为代表的LLMs更适合作为推理助手，而不是少样本信息提取器。具体而言，虽然GPT-4在与KG构建相关的任务中表现出色，但在推理任务中表现更出色，在某些情况下超越了精调模型。此外，我们的调查还扩展到LLMs在信息提取方面的潜在泛化能力，提出了虚拟知识提取的构想。

    arXiv:2305.13168v2 Announce Type: replace-cross  Abstract: This paper presents an exhaustive quantitative and qualitative evaluation of Large Language Models (LLMs) for Knowledge Graph (KG) construction and reasoning. We engage in experiments across eight diverse datasets, focusing on four representative tasks encompassing entity and relation extraction, event extraction, link prediction, and question-answering, thereby thoroughly exploring LLMs' performance in the domain of construction and inference. Empirically, our findings suggest that LLMs, represented by GPT-4, are more suited as inference assistants rather than few-shot information extractors. Specifically, while GPT-4 exhibits good performance in tasks related to KG construction, it excels further in reasoning tasks, surpassing fine-tuned models in certain cases. Moreover, our investigation extends to the potential generalization ability of LLMs for information extraction, leading to the proposition of a Virtual Knowledge Extr
    
[^22]: 用弱监督对比预训练进行文本嵌入

    Text Embeddings by Weakly-Supervised Contrastive Pre-training

    [https://arxiv.org/abs/2212.03533](https://arxiv.org/abs/2212.03533)

    本文提出了一种名为E5的文本嵌入模型，通过弱监督对比训练方式，在未经过标记数据的情况下，在多个任务中表现卓越，是第一个在BEIR检索基准测试上击败BM25基线的模型，在微调后在MTEB基准测试上获得最佳结果。

    

    本文介绍了E5，一种最先进的文本嵌入模型，可以很好地迁移到各种任务中。该模型以对比方式训练，使用我们精心策划的大规模文本配对数据集（名为CCPairs）的弱监督信号。E5可以作为通用嵌入模型用于任何需要单一文本向量表示的任务，如检索、聚类和分类，在零-shot和微调设置下表现出色。我们在BEIR和MTEB基准测试的56个数据集上进行了广泛评估。在零-shot设置下，E5是第一个在BEIR检索基准测试上击败强大的BM25基线且不使用任何标记数据的模型。在微调后，E5在MTEB基准测试上取得了最佳结果，胜过具有40倍参数的现有嵌入模型。

    arXiv:2212.03533v2 Announce Type: replace  Abstract: This paper presents E5, a family of state-of-the-art text embeddings that transfer well to a wide range of tasks. The model is trained in a contrastive manner with weak supervision signals from our curated large-scale text pair dataset (called CCPairs). E5 can be readily used as a general-purpose embedding model for any tasks requiring a single-vector representation of texts such as retrieval, clustering, and classification, achieving strong performance in both zero-shot and fine-tuned settings. We conduct extensive evaluations on 56 datasets from the BEIR and MTEB benchmarks. For zero-shot settings, E5 is the first model that outperforms the strong BM25 baseline on the BEIR retrieval benchmark without using any labeled data. When fine-tuned, E5 obtains the best results on the MTEB benchmark, beating existing embedding models with 40x more parameters.
    
[^23]: Re4: 学习重新对比、重新关注、重新构建以用于多兴趣推荐

    Re4: Learning to Re-contrast, Re-attend, Re-construct for Multi-interest Recommendation

    [https://arxiv.org/abs/2208.08011](https://arxiv.org/abs/2208.08011)

    Re4框架使用反向流重新审视每个兴趣嵌入，包括Re-contrast对兴趣嵌入进行对比学习使其彼此区分开，Re-attend确保兴趣-项目相关性估计

    

    有效地表示用户是现代推荐系统的核心。由于用户的兴趣自然地展现出多个方面，发展多兴趣推荐框架而不是用整体嵌入表示每个用户变得越来越重要。现有方法虽然有效，却仅利用编码器（前向流）来表示兴趣的多个方面。然而，缺乏明确的正则化，兴趣嵌入可能彼此不明显区分，也不能语义上反映代表性历史项目。为此，我们提出了Re4框架，它利用反向流重新审视每个兴趣嵌入。具体来说，Re4包含三个反向流，即1）Re-contrast，利用对比学习驱使每个兴趣嵌入与其他兴趣区分开来；2）Re-attend，确保兴趣-项目相关性估计

    arXiv:2208.08011v2 Announce Type: replace  Abstract: Effectively representing users lie at the core of modern recommender systems. Since users' interests naturally exhibit multiple aspects, it is of increasing interest to develop multi-interest frameworks for recommendation, rather than represent each user with an overall embedding. Despite their effectiveness, existing methods solely exploit the encoder (the forward flow) to represent multiple aspects of interests. However, without explicit regularization, the interest embeddings may not be distinct from each other nor semantically reflect representative historical items. Towards this end, we propose the Re4 framework, which leverages the backward flow to reexamine each interest embedding. Specifically, Re4 encapsulates three backward flows, i.e., 1) Re-contrast, which drives each interest embedding to be distinct from other interests using contrastive learning; 2) Re-attend, which ensures the interest-item correlation estimation in t
    
[^24]: 一项关于值得信赖推荐系统的调查

    A Survey on Trustworthy Recommender Systems

    [https://arxiv.org/abs/2207.12515](https://arxiv.org/abs/2207.12515)

    推荐系统在人类中心的人工智能中发挥着重要作用，但可能带来信任问题、不公平待遇和隐私担忧等负面影响，因此有必要开发值得信赖的推荐系统来减轻这些风险。

    

    推荐系统（RS）作为AI中人类中心的前沿服务，在网络的几乎每个角落被广泛部署，并促进人类决策过程。然而，尽管其巨大的能力和潜力，RS也可能对用户、物品、生产者、平台甚至整个社会产生不良影响，比如由于缺乏透明度而导致用户信任受损、对不同消费者或生产者的不公平对待、由于广泛使用用户私人数据进行个性化而引起的隐私问题，等等。所有这些都迫切需要值得信赖的推荐系统（TRS）来减轻或避免这些负面影响和风险。在本调查中，我们将介绍与值得信赖推荐相关的技术，包括但不限于可解释推荐、公平推荐、隐私感知推荐、推荐的鲁棒性、用户可控制的推荐等。

    arXiv:2207.12515v2 Announce Type: replace  Abstract: Recommender systems (RS), serving at the forefront of Human-centered AI, are widely deployed in almost every corner of the web and facilitate the human decision-making process. However, despite their enormous capabilities and potential, RS may also lead to undesired effects on users, items, producers, platforms, or even the society at large, such as compromised user trust due to non-transparency, unfair treatment of different consumers, or producers, privacy concerns due to extensive use of user's private data for personalization, just to name a few. All of these create an urgent need for Trustworthy Recommender Systems (TRS) so as to mitigate or avoid such adverse impacts and risks. In this survey, we will introduce techniques related to trustworthy recommendation, including but not limited to explainable recommendation, fairness in recommendation, privacy-aware recommendation, robustness in recommendation, user-controllable recomme
    
[^25]: MuGI:通过与大型语言模型的多文本生成集成增强信息检索

    MuGI: Enhancing Information Retrieval through Multi-Text Generation Intergration with Large Language Models. (arXiv:2401.06311v1 [cs.IR])

    [http://arxiv.org/abs/2401.06311](http://arxiv.org/abs/2401.06311)

    MuGI是一个简单而有效的多文本生成集成框架，它通过与大型语言模型合作生成多个伪参考文献，并将其与查询集成以提升信息检索性能。在实验中，MuGI模型在TREC DL数据集上的BM25性能上取得了18%以上的增强，并在BEIR上提高了7.5%。

    

    大型语言模型（LLM）已经成为语言技术领域的一个重要力量。它们强大的推理能力和广泛的知识库使其在各个自然语言处理领域，包括信息检索（IR）方面具备了出色的零-shot泛化能力。在本文中，我们对LLM生成的文档在IR中的实用性进行了深入研究。我们引入了一个简单而有效的框架，即多文本生成集成（MuGI），来增强现有的IR方法。具体而言，我们引导LLM生成多个伪参考文献，并将其与查询进行集成以进行检索。无需训练的MuGI模型超越了现有的查询扩展策略，在TREC DL数据集上的BM25上取得了新的标准，并在BEIR上提高了7.5%。通过MuGI，我们构建了一个快速且高保真度的重排序方法。

    Large Language Models (LLMs) have emerged as a pivotal force in language technology. Their robust reasoning capabilities and expansive knowledge repositories have enabled exceptional zero-shot generalization abilities across various facets of the natural language processing field, including information retrieval (IR). In this paper, we conduct an in-depth investigation into the utility of documents generated by LLMs for IR. We introduce a simple yet effective framework, Multi-Text Generation Integration (MuGI), to augment existing IR methodologies. Specifically, we prompt LLMs to generate multiple pseudo references and integrate with query for retrieval. The training-free MuGI model eclipses existing query expansion strategies, setting a new standard in sparse retrieval. It outstrips supervised counterparts like ANCE and DPR, achieving a notable over 18% enhancement in BM25 on the TREC DL dataset and a 7.5% increase on BEIR. Through MuGI, we have forged a rapid and high-fidelity re-ran
    
[^26]: TREC iKAT 2023: 交互式知识辅助任务概述

    TREC iKAT 2023: The Interactive Knowledge Assistance Track Overview. (arXiv:2401.01330v1 [cs.IR])

    [http://arxiv.org/abs/2401.01330](http://arxiv.org/abs/2401.01330)

    TREC iKAT 2023是一个交互式的知识辅助任务，旨在开发适应用户交互和上下文的会话搜索代理。该任务还强调决策搜索任务，用户通过筛选数据和信息来进行决策和执行动作。

    

    会话式信息查询是一个关键的研究领域，之前的工作也有很大的贡献。TREC交互式知识辅助任务（iKAT）建立在TREC会话辅助任务（CAsT）的基础上。然而，iKAT着重于创建和研究可以根据用户之前的交互和当前情境自适应响应的会话搜索代理。挑战在于使会话搜索代理能够将个性化的上下文信息融入到相应中，以高效地引导用户获取相关信息。iKAT还着重于决策搜索任务，即用户通过数据和信息筛选来衡量各种选择，以达到结论或执行动作。这些任务在日常信息搜索决策中普遍存在，无论是旅游、健康还是购物等，通常涉及一组高级信息操作符，其中查询或问题可能会

    Conversational Information Seeking stands as a pivotal research area with significant contributions from previous works. The TREC Interactive Knowledge Assistance Track (iKAT) builds on the foundational work of the TREC Conversational Assistance Track (CAsT). However, iKAT distinctively emphasizes the creation and research of conversational search agents that adapt responses based on user's prior interactions and present context. The challenge lies in enabling Conversational Search Agents (CSA) to incorporate this personalized context to efficiency and effectively guide users through the relevant information to them. iKAT also emphasizes decisional search tasks, where users sift through data and information to weigh up options in order to reach a conclusion or perform an action. These tasks, prevalent in everyday information-seeking decisions -- be it related to travel, health, or shopping -- often revolve around a subset of high-level information operators where queries or questions a
    
[^27]: 协同大型语言模型用于推荐系统

    Collaborative Large Language Model for Recommender Systems. (arXiv:2311.01343v1 [cs.IR])

    [http://arxiv.org/abs/2311.01343](http://arxiv.org/abs/2311.01343)

    本研究提出了CLLM4Rec，首个将大型语言模型与推荐系统的 ID 模式紧密集成的协同推荐算法，旨在解决语义差距、虚假相关和低效推荐等问题。通过扩展预训练语言模型的词汇表，并引入软硬提示策略，该算法能够准确地模拟用户和项目的协同与内容语义。

    

    最近，越来越多的人对基于预训练的大型语言模型（LLM）开发下一代推荐系统（RS）产生了兴趣，充分利用其编码知识和推理能力。然而，自然语言与推荐任务之间的语义差距仍未得到很好的解决，导致一些问题，如虚假相关的用户/项目描述符、对用户/项目内容的低效语言建模以及通过自动回归进行低效的推荐等。在本文中，我们提出了CLLM4Rec，这是第一个紧密集成LLM范式和RS的ID范式的生成RS，旨在同时解决上述挑战。我们首先使用用户/项目ID标记扩展了预训练LLM的词汇表，以忠实地模拟用户/项目的协同和内容语义。因此，在预训练阶段，提出了一种新颖的软硬提示策略，通过语言建模有效地学习用户/项目的协同/内容标记嵌入。

    Recently, there is a growing interest in developing next-generation recommender systems (RSs) based on pretrained large language models (LLMs), fully utilizing their encoded knowledge and reasoning ability. However, the semantic gap between natural language and recommendation tasks is still not well addressed, leading to multiple issues such as spuriously-correlated user/item descriptors, ineffective language modeling on user/item contents, and inefficient recommendations via auto-regression, etc. In this paper, we propose CLLM4Rec, the first generative RS that tightly integrates the LLM paradigm and ID paradigm of RS, aiming to address the above challenges simultaneously. We first extend the vocabulary of pretrained LLMs with user/item ID tokens to faithfully model the user/item collaborative and content semantics. Accordingly, in the pretraining stage, a novel soft+hard prompting strategy is proposed to effectively learn user/item collaborative/content token embeddings via language m
    
[^28]: 图增强优化器用于结构感知推荐嵌入演化

    Graph-enhanced Optimizers for Structure-aware Recommendation Embedding Evolution. (arXiv:2310.03032v1 [cs.IR])

    [http://arxiv.org/abs/2310.03032](http://arxiv.org/abs/2310.03032)

    本文提出了一种新颖的结构感知嵌入演化(SEvo)机制，能够以较低的计算开销将图结构信息注入到嵌入中，从而在现代推荐系统中实现更高效的性能。

    

    嵌入在现代推荐系统中起着关键作用，因为它们是真实世界实体的虚拟表示，并且是后续决策模型的基础。本文提出了一种新颖的嵌入更新机制，称为结构感知嵌入演化(SEvo)，以鼓励相关节点在每一步中以类似的方式演化。与通常作为中间部分的GNN（图神经网络）不同，SEvo能够直接将图结构信息注入到嵌入中，且在训练过程中计算开销可忽略。本文通过理论分析验证了SEvo的收敛性质及其可能的改进版本，以证明设计的有效性。此外，SEvo可以无缝集成到现有的优化器中，以实现最先进性能。特别是，在矩估计校正的SEvo增强AdamW中，证明了一致的改进效果在多种模型和数据集上，为有效推荐了一种新的技术路线。

    Embedding plays a critical role in modern recommender systems because they are virtual representations of real-world entities and the foundation for subsequent decision models. In this paper, we propose a novel embedding update mechanism, Structure-aware Embedding Evolution (SEvo for short), to encourage related nodes to evolve similarly at each step. Unlike GNN (Graph Neural Network) that typically serves as an intermediate part, SEvo is able to directly inject the graph structure information into embedding with negligible computational overhead in training. The convergence properties of SEvo as well as its possible variants are theoretically analyzed to justify the validity of the designs. Moreover, SEvo can be seamlessly integrated into existing optimizers for state-of-the-art performance. In particular, SEvo-enhanced AdamW with moment estimate correction demonstrates consistent improvements across a spectrum of models and datasets, suggesting a novel technical route to effectively 
    
[^29]: SilverRetriever：提升波兰问答系统的神经通道检索能力

    SilverRetriever: Advancing Neural Passage Retrieval for Polish Question Answering. (arXiv:2309.08469v1 [cs.CL])

    [http://arxiv.org/abs/2309.08469](http://arxiv.org/abs/2309.08469)

    SilverRetriever是一个特为波兰语问答系统开发的神经检索器，通过训练在多种数据集上取得了显著的改进效果，并且与更大的多语种模型具有竞争力。

    

    现代开放领域的问答系统通常依赖于准确和高效的检索组件来找到包含回答问题所需事实的段落。近年来，由于其出色的性能，神经检索器比词汇替代方式更受欢迎。然而，大部分研究都集中在流行语言如英语或中文上，对于其他语言如波兰语，可用的模型很少。在本文中，我们介绍了SilverRetriever，一个基于多种手动标记或弱标记数据集训练的波兰语神经检索器。SilverRetriever在波兰语模型中取得了比其他模型更好的结果，并与更大的多语种模型具有竞争力。与该模型一起，我们还开源了五个新的段落检索数据集。

    Modern open-domain question answering systems often rely on accurate and efficient retrieval components to find passages containing the facts necessary to answer the question. Recently, neural retrievers have gained popularity over lexical alternatives due to their superior performance. However, most of the work concerns popular languages such as English or Chinese. For others, such as Polish, few models are available. In this work, we present SilverRetriever, a neural retriever for Polish trained on a diverse collection of manually or weakly labeled datasets. SilverRetriever achieves much better results than other Polish models and is competitive with larger multilingual models. Together with the model, we open-source five new passage retrieval datasets.
    

