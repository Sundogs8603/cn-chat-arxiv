# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [HimiRec: Modeling Hierarchical Multi-interest for Recommendation](https://rss.arxiv.org/abs/2402.01253) | 本论文提出了一个新颖的两阶段方法，用于显式地建模层次化多兴趣的推荐系统，通过层次聚类和基于Transformer的模型来挖掘层次化的多兴趣信息。 |
| [^2] | [Can Large Language Models Detect Rumors on Social Media?](https://arxiv.org/abs/2402.03916) | 本研究探讨了使用大型语言模型（LLMs）检测社交媒体上的谣言的可行性。通过设计提示来教导LLMs理解新闻和评论中的关键线索，并将传播信息分解为传播链，我们的LeRuD方法相对于其他最先进的模型在谣言检测方面取得了更好的性能，同时具备在少样本或零样本情况下更有潜力的应用。 |
| [^3] | [Learning Metrics that Maximise Power for Accelerated A/B-Tests](https://arxiv.org/abs/2402.03915) | 本论文提出了一种新方法，通过从短期信号中学习指标，直接最大化指标与北极度量标准之间的统计能力，从而减少在线控制实验的成本。 |
| [^4] | [On Practical Diversified Recommendation with Controllable Category Diversity Framework](https://arxiv.org/abs/2402.03801) | 本文提出了一个具有可控制类别多样性框架的实用的多样化推荐系统。该系统通过考虑用户的非交互偏好，扩大用户的兴趣范围，缓解回音室/过滤泡效应。 |
| [^5] | [Retrieval Augmented Cross-Modal Tag Recommendation in Software Q&A Sites](https://arxiv.org/abs/2402.03635) | 提出了一种在软件问答网站中基于检索增强的跨模态标签推荐模型（RACM）。该模型利用外部知识源检索信息来增强不同模态的表示，通过跨模态上下文感知注意力实现有针对性的特征提取，并通过门机制实现精细的特征融合。 |
| [^6] | [Leveraging Large Language Models for Hybrid Workplace Decision Support](https://arxiv.org/abs/2402.03616) | 本论文研究了利用大型语言模型（LLMs）为混合工作场所提供智能决策支持的决策支持模型。通过广泛的用户研究和评估，发现LLMs在提供适当工作区建议方面具有超越提示的推理能力，提高工作者的工作体验。 |
| [^7] | [Understanding and Counteracting Feature-Level Bias in Click-Through Rate Prediction](https://arxiv.org/abs/2402.03600) | 该研究对点击率预测模型中的特征级别偏差进行了分析，并发现线性组件对偏差贡献最大，实验证明正样本比率不平衡是导致特征级别偏差的重要因素。 |
| [^8] | [Identifying Reasons for Contraceptive Switching from Real-World Data Using Large Language Models](https://arxiv.org/abs/2402.03597) | 本研究评估了一种大型语言模型GPT-4在识别避孕药切换原因上的能力，结果表明GPT-4可以准确地从临床记录中提取避孕药切换的原因，相较于基准BERT模型有更好的表现。 |
| [^9] | [Early prediction of onset of sepsis in Clinical Setting](https://arxiv.org/abs/2402.03486) | 本研究提出了使用机器学习模型预测临床数据中脓毒症的早期发作，通过使用有监督学习方法训练XGBoost模型，并利用规范化效用分数评估模型的性能。 |
| [^10] | [Harnessing PubMed User Query Logs for Post Hoc Explanations of Recommended Similar Articles](https://arxiv.org/abs/2402.03484) | 本研究利用PubMed用户查询日志构建了PubCLogs数据集，并采用事后方法解释推荐的相关文章，通过识别类似文章标题中的相关词汇来提供解释。这将帮助研究人员和临床医生在文献搜索中更方便地寻找相关文章。 |
| [^11] | [FINEST: Stabilizing Recommendations by Rank-Preserving Fine-Tuning](https://arxiv.org/abs/2402.03481) | FINEST方法通过从给定的推荐模型获得参考排序列表，并在模拟的扰动场景下进行模型微调，保持排序，以稳定和改善推荐系统的性能。 |
| [^12] | [A Fuzzy Approach to Record Linkages](https://arxiv.org/abs/2402.03464) | 本文描述了一种基于模糊集技术的模糊链接方法，能够有效地处理不同数据源中的不确定性，并解决现有方法的缺点。 |
| [^13] | [Recommendation Fairness in Social Networks Over Time](https://arxiv.org/abs/2402.03450) | 本研究研究了社交网络推荐系统中推荐公平性随时间推移而变化的情况，并与动态网络属性进行了关联分析。结果表明，推荐公平性随时间改善，而少数群体比例和同质性与公平性有关。 |
| [^14] | [Delivery Optimized Discovery in Behavioral User Segmentation under Budget Constrain](https://arxiv.org/abs/2402.03388) | 在预算限制下，我们提出了一种基于随机优化的算法，用于优化传递发现行为用户细分。 |
| [^15] | [Modified K-means with Cluster Assignment -- Application to COVID-19 Data](https://arxiv.org/abs/2402.03380) | 本论文描述了一个改进的K-means聚类算法，应用于COVID-19数据。通过利用两个聚类分配技术与K-means模型，提取文本的本体论被改善，进一步应用矢量平均阻尼技术以灵活移动聚类。实验结果表明，该算法在COVID-19数据上获得了良好的结果。 |
| [^16] | [Entire Chain Uplift Modeling with Context-Enhanced Learning for Intelligent Marketing](https://arxiv.org/abs/2402.03379) | 全链路上升建模方法ECUP旨在解决链路偏差和处理不适应问题，在线营销中有重要的应用价值。 |
| [^17] | [Detection of tortured phrases in scientific literature](https://arxiv.org/abs/2402.03370) | 本文介绍了自动检测科学论文中拙劣短语的方法，通过语言模型和预测分数的传播，可以高效地标记并提取这些拙劣短语，为领域专家验证提供新的数据。 |
| [^18] | [Empirical and Experimental Perspectives on Big Data in Recommendation Systems: A Comprehensive Survey](https://arxiv.org/abs/2402.03368) | 本综合调查论文对推荐系统中的大数据算法进行了全面分析，并提出了一种新颖的、分层的分类法。通过该分类法，研究人员可以全面了解不同算法和技术之间的相互关系。 |
| [^19] | [RAG-Fusion: a New Take on Retrieval-Augmented Generation](https://arxiv.org/abs/2402.03367) | RAG-Fusion方法通过生成多个查询，并结合互惠排名融合技术，能够从不同角度上下文化原始查询，提供准确和全面的信息。这项研究在人工智能和自然语言处理应用方面有重要进展，并展示了全球和区域之间的转变。 |
| [^20] | [Uncertainty-Aware Explainable Recommendation with Large Language Models](https://arxiv.org/abs/2402.03366) | 这项研究开发了一个模型，通过训练用户和项目输入的ID向量作为提示，利用GPT-2实现不确定性感知的可解释推荐系统。该系统采用联合训练机制并在多任务学习框架中进行优化，能够更有效地探索用户的兴趣，提高推荐效果和用户满意度。 |
| [^21] | [Heterophily-Aware Fair Recommendation using Graph Convolutional Networks](https://arxiv.org/abs/2402.03365) | 本文提出了一种利用图卷积网络的公平推荐系统，名为HetroFair，旨在提高项目侧的公平性。HetroFair使用公平注意力和异质性特征加权两个组件来生成具有公平性意识的嵌入。 |
| [^22] | [NanoNER: Named Entity Recognition for nanobiology using experts' knowledge and distant supervision](https://arxiv.org/abs/2402.03362) | 本文介绍了NanoNER，它是一种用于纳米生物学的命名实体识别模型。通过使用领域专家的知识和远程监督学习，NanoNER能够准确地识别先前已知实体，并最大程度地提高注释数据的质量和数量。 |
| [^23] | [EasyInstruct: An Easy-to-use Instruction Processing Framework for Large Language Models](https://arxiv.org/abs/2402.03049) | EasyInstruct是一个易于使用的用于大型语言模型的指令处理框架，通过模块化指令生成、选择和提示，并考虑它们的组合和交互，使指令处理更加方便和高效。 |
| [^24] | [K-PERM: Personalized Response Generation Using Dynamic Knowledge Retrieval and Persona-Adaptive Queries](https://arxiv.org/abs/2312.17748) | K-PERM是一种使用动态知识检索和个人适应性查询来实现个性化响应生成的对话代理，可以提高对话的质量和用户参与度，并在真实个性化对话方面取得了最先进的性能。 |
| [^25] | [Online Recommendations for Agents with Discounted Adaptive Preferences](https://arxiv.org/abs/2302.06014) | 本文研究了具有折扣自适应偏好的代理的在线推荐问题，通过在每一轮中展示一系列物品并考虑代理的偏好演变，以实现对于目标集合的最小化后悔。在长期记忆的情况下，可以实现对于能够在任何时刻实现的分布集合的高效次线性后悔。 |
| [^26] | [Whole Page Unbiased Learning to Rank](https://arxiv.org/abs/2210.10718) | 本论文提出整页无偏学习排序（WP-ULTR）方法处理整页 SERP 特征引发的偏差，该方法面临适合的用户行为模型的挑战和复杂的模型训练难题。 |
| [^27] | [Re3val: Reinforced and Reranked Generative Retrieval.](http://arxiv.org/abs/2401.16979) | Re3val是一个使用强化学习和重新排名技术进行训练的生成检索模型，它通过利用上下文信息来重新排名检索得到的页面标题，以最大化通过受限解码生成的奖励。同时，该模型通过生成问题来减小认识不确定性，并弥合预训练和微调数据集之间的领域差距。 |
| [^28] | [Source Code Clone Detection Using Unsupervised Similarity Measures.](http://arxiv.org/abs/2401.09885) | 本研究对使用无监督相似度度量进行源代码克隆检测进行了比较分析，旨在为软件工程师提供指导，以选择适合其特定用例的方法。 |
| [^29] | [Natural Language is All a Graph Needs.](http://arxiv.org/abs/2308.07134) | 本论文提出了一种名为InstructGLM的结构化语言模型算法，该算法将大型语言模型与图表学习问题相结合，旨在探索是否可以用语言模型取代图神经网络作为图表的基础模型。 |
| [^30] | [An In-depth Investigation of User Response Simulation for Conversational Search.](http://arxiv.org/abs/2304.07944) | 本文研究了对话式搜索中用户响应模拟的方法。当前的模拟系统要么只能对是非问题进行回答，要么无法产生高质量的响应。通过用更小但先进的系统替换当前最先进的用户模拟系统，能够显著改进性能。 |

# 详细

[^1]: HimiRec: 建模层次化多兴趣的推荐系统

    HimiRec: Modeling Hierarchical Multi-interest for Recommendation

    [https://rss.arxiv.org/abs/2402.01253](https://rss.arxiv.org/abs/2402.01253)

    本论文提出了一个新颖的两阶段方法，用于显式地建模层次化多兴趣的推荐系统，通过层次聚类和基于Transformer的模型来挖掘层次化的多兴趣信息。

    

    工业级推荐系统通常包含检索阶段和排名阶段，以处理亿级用户和物品。检索阶段用于检索与用户兴趣相关的候选物品进行推荐，引起了广泛关注。经常情况下，用户展示出层次化的多个兴趣，比如一个在体育中热衷支持金州勇士队的用户，也会对几乎所有动画有兴趣，体育和动画处于同样的层次。然而，大多数现有方法隐式地学习这种层次化差异，导致更细粒度的兴趣信息被平均化，限制了对用户在热门兴趣和其他轻兴趣方面的详细理解。因此，在这项工作中，我们提出了一种新颖的两阶段方法，用于显式地建模层次化多兴趣的推荐系统。在第一阶段，我们使用层次聚类和基于Transformer的模型来挖掘层次化的多兴趣信息。

    Industrial recommender systems usually consist of the retrieval stage and the ranking stage, to handle the billion-scale of users and items. The retrieval stage retrieves candidate items relevant to user interests for recommendations and has attracted much attention. Frequently, users show hierarchical multi-interests reflected in a heavy user of a certain NBA team Golden State Warriors in Sports, who is also a light user of almost the whole Animation. Both Sports and Animation are at the same level. However, most existing methods implicitly learn this hierarchical difference, making more fine-grained interest information to be averaged and limiting detailed understanding of the user's different needs in heavy interests and other light interests. Therefore, we propose a novel two-stage approach to explicitly modeling hierarchical multi-interest for recommendation in this work. In the first hierarchical multi-interest mining stage, the hierarchical clustering and transformer-based model
    
[^2]: 大型语言模型能否检测社交媒体上的谣言？

    Can Large Language Models Detect Rumors on Social Media?

    [https://arxiv.org/abs/2402.03916](https://arxiv.org/abs/2402.03916)

    本研究探讨了使用大型语言模型（LLMs）检测社交媒体上的谣言的可行性。通过设计提示来教导LLMs理解新闻和评论中的关键线索，并将传播信息分解为传播链，我们的LeRuD方法相对于其他最先进的模型在谣言检测方面取得了更好的性能，同时具备在少样本或零样本情况下更有潜力的应用。

    

    在这项工作中，我们研究了使用大型语言模型（LLMs）在社交媒体上进行谣言检测。然而，LLMs在推理整个传播信息时面临挑战，因为该信息包含新闻内容和大量评论，LLMs可能无法集中关注复杂传播信息中的关键线索，并且在面对大量和冗余信息时难以进行推理。因此，我们提出了一种基于LLMs增强的谣言检测（LeRuD）方法，在其中设计提示来教导LLMs关注新闻和评论中的重要线索，并将整个传播信息分解为传播链以减轻LLMs的负担。我们在Twitter和微博数据集上进行了大量实验证明，LeRuD的性能优于几种最先进的谣言检测模型，提升了2.4％至7.6％。同时，通过应用LLMs，LeRuD无需进行训练数据，并且在少样本或零样本情况下展现出更具有潜力的谣言检测能力。

    In this work, we investigate to use Large Language Models (LLMs) for rumor detection on social media. However, it is challenging for LLMs to reason over the entire propagation information on social media, which contains news contents and numerous comments, due to LLMs may not concentrate on key clues in the complex propagation information, and have trouble in reasoning when facing massive and redundant information. Accordingly, we propose an LLM-empowered Rumor Detection (LeRuD) approach, in which we design prompts to teach LLMs to reason over important clues in news and comments, and divide the entire propagation information into a Chain-of-Propagation for reducing LLMs' burden. We conduct extensive experiments on the Twitter and Weibo datasets, and LeRuD outperforms several state-of-the-art rumor detection models by 2.4% to 7.6%. Meanwhile, by applying LLMs, LeRuD requires no data for training, and thus shows more promising rumor detection ability in few-shot or zero-shot scenarios.
    
[^3]: 学习最大化加速A/B测试的指标

    Learning Metrics that Maximise Power for Accelerated A/B-Tests

    [https://arxiv.org/abs/2402.03915](https://arxiv.org/abs/2402.03915)

    本论文提出了一种新方法，通过从短期信号中学习指标，直接最大化指标与北极度量标准之间的统计能力，从而减少在线控制实验的成本。

    

    在技术公司中，在线控制实验是一种重要的工具，可以实现自信的决策。定义了一个北极度量标准（如长期收入或用户保留），在A/B测试中，能够在这个指标上有统计显著提升的系统变体可以被认为是优越的。然而，北极度量标准通常具有时延和不敏感性。因此，实验的成本很高：实验需要长时间运行，即使如此，二类错误（即假阴性）仍然普遍存在。为了解决这个问题，我们提出了一种从短期信号中学习指标的方法，这些指标直接最大化它们相对于北极度量标准所具有的统计能力。我们展示了现有方法容易过拟合的问题，即更高的平均度量敏感性并不意味着改进了二类错误，我们建议通过最小化指标在过去实验的$log$上产生的$p$-value来解决。我们从两个社交媒体应用程序中收集了这样的数据集。

    Online controlled experiments are a crucial tool to allow for confident decision-making in technology companies. A North Star metric is defined (such as long-term revenue or user retention), and system variants that statistically significantly improve on this metric in an A/B-test can be considered superior. North Star metrics are typically delayed and insensitive. As a result, the cost of experimentation is high: experiments need to run for a long time, and even then, type-II errors (i.e. false negatives) are prevalent.   We propose to tackle this by learning metrics from short-term signals that directly maximise the statistical power they harness with respect to the North Star. We show that existing approaches are prone to overfitting, in that higher average metric sensitivity does not imply improved type-II errors, and propose to instead minimise the $p$-values a metric would have produced on a log of past experiments. We collect such datasets from two social media applications with
    
[^4]: 实用的多样化推荐系统与可控制的类别多样性框架

    On Practical Diversified Recommendation with Controllable Category Diversity Framework

    [https://arxiv.org/abs/2402.03801](https://arxiv.org/abs/2402.03801)

    本文提出了一个具有可控制类别多样性框架的实用的多样化推荐系统。该系统通过考虑用户的非交互偏好，扩大用户的兴趣范围，缓解回音室/过滤泡效应。

    

    推荐系统在各个行业取得了重大进展，主要是通过努力提高推荐准确性。然而，这种追求准确性的努力不经意间引发了回音室/过滤泡效应。特别是在工业界，它可能损害用户的体验，阻止用户访问更广泛的项目。其中一个解决方案是考虑多样性。然而，大部分现有研究都着重于用户的显式偏好，很少探索用户的非交互偏好。这些被忽视的非交互偏好对于拓宽用户兴趣、缓解回音室/过滤泡效应尤为重要。因此，在本文中，我们首先基于用户的历史行为将多样性定义为两个不同的定义，即用户显式多样性（U-diversity）和用户-项目非交互多样性（N-diversity）。然后，我们提出了一种简明有效的方法，名为可控制的类别多样化框架（Controllable Category Dive）。

    Recommender systems have made significant strides in various industries, primarily driven by extensive efforts to enhance recommendation accuracy. However, this pursuit of accuracy has inadvertently given rise to echo chamber/filter bubble effects. Especially in industry, it could impair user's experiences and prevent user from accessing a wider range of items. One of the solutions is to take diversity into account. However, most of existing works focus on user's explicit preferences, while rarely exploring user's non-interaction preferences. These neglected non-interaction preferences are especially important for broadening user's interests in alleviating echo chamber/filter bubble effects.Therefore, in this paper, we first define diversity as two distinct definitions, i.e., user-explicit diversity (U-diversity) and user-item non-interaction diversity (N-diversity) based on user historical behaviors. Then, we propose a succinct and effective method, named as Controllable Category Dive
    
[^5]: 在软件问答网站中基于检索增强的跨模态标签推荐

    Retrieval Augmented Cross-Modal Tag Recommendation in Software Q&A Sites

    [https://arxiv.org/abs/2402.03635](https://arxiv.org/abs/2402.03635)

    提出了一种在软件问答网站中基于检索增强的跨模态标签推荐模型（RACM）。该模型利用外部知识源检索信息来增强不同模态的表示，通过跨模态上下文感知注意力实现有针对性的特征提取，并通过门机制实现精细的特征融合。

    

    软件问答网站中的帖子通常包含标题、描述和代码三个主要部分，它们相互关联并共同描述问题。现有的标签推荐方法通常将不同的模态视为整体，或者不充分考虑不同模态之间的交互。此外，它们侧重于直接从帖子本身提取信息，忽略了来自外部知识源的信息。因此，我们提出了一种在软件问答网站中基于检索增强的跨模态标签推荐模型（RACM）。具体而言，我们首先将输入的帖子作为查询，并通过从外部知识源中检索信息来增强不同模态的表示。对于检索增强的表示，我们采用跨模态上下文感知注意力来利用主要模态描述对标题和代码子模态进行有针对性的特征提取。在融合过程中，我们采用了门机制来实现精细的特征融合。

    Posts in software Q\&A sites often consist of three main parts: title, description and code, which are interconnected and jointly describe the question. Existing tag recommendation methods often treat different modalities as a whole or inadequately consider the interaction between different modalities. Additionally, they focus on extracting information directly from the post itself, neglecting the information from external knowledge sources. Therefore, we propose a Retrieval Augmented Cross-Modal (RACM) Tag Recommendation Model in Software Q\&A Sites. Specifically, we first use the input post as a query and enhance the representation of different modalities by retrieving information from external knowledge sources. For the retrieval-augmented representations, we employ a cross-modal context-aware attention to leverage the main modality description for targeted feature extraction across the submodalities title and code. In the fusion process, a gate mechanism is employed to achieve fine
    
[^6]: 利用大型语言模型进行混合工作场所决策支持

    Leveraging Large Language Models for Hybrid Workplace Decision Support

    [https://arxiv.org/abs/2402.03616](https://arxiv.org/abs/2402.03616)

    本论文研究了利用大型语言模型（LLMs）为混合工作场所提供智能决策支持的决策支持模型。通过广泛的用户研究和评估，发现LLMs在提供适当工作区建议方面具有超越提示的推理能力，提高工作者的工作体验。

    

    大型语言模型（LLMs）具有执行各种文本处理任务并为提议的操作或决策提供文本解释的潜力。在混合工作时代，LLMs可以为设计混合工作计划的工作者提供智能决策支持。特别是它们可以为平衡众多决策因素的工作者提供建议和解释，从而增强他们的工作体验。在本文中，我们提出了一个在混合工作环境中工作区决策支持模型，利用LLMs的推理能力。我们首先研究了LLMs对于提供适当工作区建议的能力。我们发现，其推理能力超越了提示中的指导方针，LLMs可以在工作区资源的可用性之间进行权衡。我们进行了广泛的用户研究，以了解工作者在工作区选择上的决策过程，并评估系统的有效性。我们观察到，工作者的决策可能受到影响。

    Large Language Models (LLMs) hold the potential to perform a variety of text processing tasks and provide textual explanations for proposed actions or decisions. In the era of hybrid work, LLMs can provide intelligent decision support for workers who are designing their hybrid work plans. In particular, they can offer suggestions and explanations to workers balancing numerous decision factors, thereby enhancing their work experience. In this paper, we present a decision support model for workspaces in hybrid work environments, leveraging the reasoning skill of LLMs. We first examine LLM's capability of making suitable workspace suggestions. We find that its reasoning extends beyond the guidelines in the prompt and the LLM can manage the trade-off among the available resources in the workspaces. We conduct an extensive user study to understand workers' decision process for workspace choices and evaluate the effectiveness of the system. We observe that a worker's decision could be influe
    
[^7]: 理解和对抗点击率预测中的特征偏差

    Understanding and Counteracting Feature-Level Bias in Click-Through Rate Prediction

    [https://arxiv.org/abs/2402.03600](https://arxiv.org/abs/2402.03600)

    该研究对点击率预测模型中的特征级别偏差进行了分析，并发现线性组件对偏差贡献最大，实验证明正样本比率不平衡是导致特征级别偏差的重要因素。

    

    常见的点击率预测推荐模型往往存在特征级别的偏差，导致不公平的推荐和对用户的不准确推荐。虽然现有方法通过调整CTR模型的学习来解决这个问题，例如通过额外的优化目标，但它们未考虑这些模型内部是如何引起偏差的。为了填补这一研究空白，我们的研究对代表性的CTR模型进行了自上而下的分析。通过逐个阻塞训练好的CTR模型的不同组件，我们确定线性组件对特征级别偏差的关键贡献。我们对线性组件中权重的学习过程进行了理论分析，揭示了训练数据的群组属性如何影响权重。我们的实验和统计分析表明，在物品组之间不平衡的正样本比率与特征级别偏差之间存在强相关性。基于这种理解，

    Common click-through rate (CTR) prediction recommender models tend to exhibit feature-level bias, which leads to unfair recommendations among item groups and inaccurate recommendations for users. While existing methods address this issue by adjusting the learning of CTR models, such as through additional optimization objectives, they fail to consider how the bias is caused within these models. To address this research gap, our study performs a top-down analysis on representative CTR models. Through blocking different components of a trained CTR model one by one, we identify the key contribution of the linear component to feature-level bias. We conduct a theoretical analysis of the learning process for the weights in the linear component, revealing how group-wise properties of training data influence them. Our experimental and statistical analyses demonstrate a strong correlation between imbalanced positive sample ratios across item groups and feature-level bias. Based on this understan
    
[^8]: 使用大型语言模型从实际数据中识别避孕药切换的原因

    Identifying Reasons for Contraceptive Switching from Real-World Data Using Large Language Models

    [https://arxiv.org/abs/2402.03597](https://arxiv.org/abs/2402.03597)

    本研究评估了一种大型语言模型GPT-4在识别避孕药切换原因上的能力，结果表明GPT-4可以准确地从临床记录中提取避孕药切换的原因，相较于基准BERT模型有更好的表现。

    

    处方避孕药在支持妇女生殖健康方面扮演着关键角色。在美国有将近5000万女性使用避孕药，了解导致避孕药选择和切换的因素非常重要。然而，与药物切换相关的许多因素通常只在无结构的临床记录中得到捕获，并且很难提取。在这里，我们评估了最近开发的大型语言模型GPT-4（通过符合HIPAA的Microsoft Azure API）的零-shot能力，以从UCSF信息共享平台的临床记录数据集中识别避孕药类别切换的原因。我们证明了GPT-4可以准确地提取避孕药切换的原因，相较于基准BERT模型，在避孕药开始和停止提取方面的microF1分数分别为0.849和0.881。对于GPT-4提取的切换原因的人工评估显示出91.4%的准确度，出现幻觉的情况很少。

    Prescription contraceptives play a critical role in supporting women's reproductive health. With nearly 50 million women in the United States using contraceptives, understanding the factors that drive contraceptives selection and switching is of significant interest. However, many factors related to medication switching are often only captured in unstructured clinical notes and can be difficult to extract. Here, we evaluate the zero-shot abilities of a recently developed large language model, GPT-4 (via HIPAA-compliant Microsoft Azure API), to identify reasons for switching between classes of contraceptives from the UCSF Information Commons clinical notes dataset. We demonstrate that GPT-4 can accurately extract reasons for contraceptive switching, outperforming baseline BERT-based models with microF1 scores of 0.849 and 0.881 for contraceptive start and stop extraction, respectively. Human evaluation of GPT-4-extracted reasons for switching showed 91.4% accuracy, with minimal hallucin
    
[^9]: 临床环境中早期预测脓毒症的研究

    Early prediction of onset of sepsis in Clinical Setting

    [https://arxiv.org/abs/2402.03486](https://arxiv.org/abs/2402.03486)

    本研究提出了使用机器学习模型预测临床数据中脓毒症的早期发作，通过使用有监督学习方法训练XGBoost模型，并利用规范化效用分数评估模型的性能。

    

    本研究提出了使用机器学习模型，利用来自纽约布朗克斯Montefiore医疗中心的去标识化临床数据，预测脓毒症的早期发作。采用了有监督学习的方法，训练了一个XGBoost模型，使用了80%的训练数据集，包括107个特征（包括原始和衍生特征）。随后，该模型在剩余的20%的测试数据上进行了评估。模型在训练阶段完全未知的前瞻数据上进行了验证。为了评估模型在个体患者水平上的性能和预测的及时性，使用了规范化效用分数，这是脓毒症检测中广泛认可的评分方法，如PhysioNet Sepsis Challenge论文中所述。还设计了F1值、敏感性、特异性和标志率等指标。该模型在测试数据上的规范化效用分数为0.494，在前瞻数据上的规范化效用分数为0.378（阈值为0.3）。

    This study proposes the use of Machine Learning models to predict the early onset of sepsis using deidentified clinical data from Montefiore Medical Center in Bronx, NY, USA. A supervised learning approach was adopted, wherein an XGBoost model was trained utilizing 80\% of the train dataset, encompassing 107 features (including the original and derived features). Subsequently, the model was evaluated on the remaining 20\% of the test data. The model was validated on prospective data that was entirely unseen during the training phase. To assess the model's performance at the individual patient level and timeliness of the prediction, a normalized utility score was employed, a widely recognized scoring methodology for sepsis detection, as outlined in the PhysioNet Sepsis Challenge paper. Metrics such as F1 Score, Sensitivity, Specificity, and Flag Rate were also devised. The model achieved a normalized utility score of 0.494 on test data and 0.378 on prospective data at threshold 0.3. The
    
[^10]: 利用PubMed用户查询日志解释推荐的相关文章

    Harnessing PubMed User Query Logs for Post Hoc Explanations of Recommended Similar Articles

    [https://arxiv.org/abs/2402.03484](https://arxiv.org/abs/2402.03484)

    本研究利用PubMed用户查询日志构建了PubCLogs数据集，并采用事后方法解释推荐的相关文章，通过识别类似文章标题中的相关词汇来提供解释。这将帮助研究人员和临床医生在文献搜索中更方便地寻找相关文章。

    

    在科学研究中，基于引用文章寻找相关文章是一个重要的部分。像许多学术搜索引擎一样，PubMed拥有一个“类似文章”的功能，可以推荐与用户查看的当前文章相关的文章。解释推荐的文章对用户来说非常有用，特别是在文献搜索过程中。鉴于每年有超过一百万篇生物医学论文发表，解释推荐的相关文章将为研究人员和临床医生在寻找相关文章时提供便利。然而，目前大多数文献推荐系统都缺乏对其建议的解释。我们采用事后方法来解释推荐，通过识别类似文章标题中的相关词汇来实现。我们的主要贡献是通过重新利用PubMed用户查询日志中的560万个共点击文章对构建PubCLogs数据集。利用我们的PubCLogs数据集，我们训练了"Highlight Similar Article Title"模型。

    Searching for a related article based on a reference article is an integral part of scientific research. PubMed, like many academic search engines, has a "similar articles" feature that recommends articles relevant to the current article viewed by a user. Explaining recommended items can be of great utility to users, particularly in the literature search process. With more than a million biomedical papers being published each year, explaining the recommended similar articles would facilitate researchers and clinicians in searching for related articles. Nonetheless, the majority of current literature recommendation systems lack explanations for their suggestions. We employ a post hoc approach to explaining recommendations by identifying relevant tokens in the titles of similar articles. Our major contribution is building PubCLogs by repurposing 5.6 million pairs of coclicked articles from PubMed's user query logs. Using our PubCLogs dataset, we train the Highlight Similar Article Title 
    
[^11]: FINEST: 通过保持排序进行微调以稳定推荐

    FINEST: Stabilizing Recommendations by Rank-Preserving Fine-Tuning

    [https://arxiv.org/abs/2402.03481](https://arxiv.org/abs/2402.03481)

    FINEST方法通过从给定的推荐模型获得参考排序列表，并在模拟的扰动场景下进行模型微调，保持排序，以稳定和改善推荐系统的性能。

    

    现代推荐系统可能会因为训练数据的微小扰动而输出大不相同的推荐结果。单个用户数据的改变会改变其他用户的推荐结果。在医疗、住房和金融等应用中，这种敏感性可能对用户体验产生不利影响。我们提出了一种方法来稳定给定的推荐系统，抵抗这种扰动。这是一个具有挑战性的任务，因为(1)缺乏可以用来锚定输出的“参考”排序列表；(2)在保证模型与训练数据的所有可能扰动的排序列表稳定性方面存在计算挑战。我们的方法FINEST通过从给定的推荐模型获得参考排序列表，然后在模拟的扰动场景下进行模型微调，并保持排序的规则化来克服这些挑战。我们在真实世界的数据集上的实验证明FINEST可以稳定推荐系统并保持性能。

    Modern recommender systems may output considerably different recommendations due to small perturbations in the training data. Changes in the data from a single user will alter the recommendations as well as the recommendations of other users. In applications like healthcare, housing, and finance, this sensitivity can have adverse effects on user experience. We propose a method to stabilize a given recommender system against such perturbations. This is a challenging task due to (1) the lack of a ``reference'' rank list that can be used to anchor the outputs; and (2) the computational challenges in ensuring the stability of rank lists with respect to all possible perturbations of training data. Our method, FINEST, overcomes these challenges by obtaining reference rank lists from a given recommendation model and then fine-tuning the model under simulated perturbation scenarios with rank-preserving regularization on sampled items. Our experiments on real-world datasets demonstrate that FIN
    
[^12]: 一个模糊方法用于记录链接

    A Fuzzy Approach to Record Linkages

    [https://arxiv.org/abs/2402.03464](https://arxiv.org/abs/2402.03464)

    本文描述了一种基于模糊集技术的模糊链接方法，能够有效地处理不同数据源中的不确定性，并解决现有方法的缺点。

    

    记录链接是识别和统一来自各种独立数据源的记录的过程。现有的策略，可以是确定性的或概率性的，通常在不确定性下无法令人满意地链接记录。本文描述了一种基于模糊集技术的本地开发的模糊链接方法，可以有效地考虑这些不同数据源中的不确定性，并解决现有方法的缺点。广泛的测试、评估和比较已经证明了这种模糊方法在记录链接中的有效性。

    Record Linkage is the process of identifying and unifying records from various independent data sources. Existing strategies, which can be either deterministic or probabilistic, often fail to link records satisfactorily under uncertainty. This paper describes an indigenously (locally) developed fuzzy linkage method, based on fuzzy set techniques, which can effectively account for this uncertainty prevalent in the disparate data sources and address the shortcomings of the existing approaches. Extensive testing, evaluation and comparisons have demonstrated the efficacy of this fuzzy approach for record linkages.
    
[^13]: 社交网络随时间推移中的推荐公平性

    Recommendation Fairness in Social Networks Over Time

    [https://arxiv.org/abs/2402.03450](https://arxiv.org/abs/2402.03450)

    本研究研究了社交网络推荐系统中推荐公平性随时间推移而变化的情况，并与动态网络属性进行了关联分析。结果表明，推荐公平性随时间改善，而少数群体比例和同质性与公平性有关。

    

    在社交推荐系统中，推荐模型提供不同人口群体（如性别或种族）公平的可见性是至关重要的。现有大多数研究只研究了网络的固定快照，而忽视了网络随时间推移的变化。为了填补这一研究空白，我们研究了推荐公平性随时间的演变及其与动态网络属性的关系。我们通过评估六种推荐算法的公平性，并分析公平性与网络属性随时间的关联，来研究三个真实世界动态网络。我们还通过研究替代演化结果和不同网络属性的对照情景，来研究对网络属性的干预如何影响公平性。我们在实证数据集上的结果表明，不论推荐方法如何，推荐公平性随时间的改善。我们还发现两个网络属性，少数群体比例和同质性，与公平性有关。

    In social recommender systems, it is crucial that the recommendation models provide equitable visibility for different demographic groups, such as gender or race. Most existing research has addressed this problem by only studying individual static snapshots of networks that typically change over time. To address this gap, we study the evolution of recommendation fairness over time and its relation to dynamic network properties. We examine three real-world dynamic networks by evaluating the fairness of six recommendation algorithms and analyzing the association between fairness and network properties over time. We further study how interventions on network properties influence fairness by examining counterfactual scenarios with alternative evolution outcomes and differing network properties. Our results on empirical datasets suggest that recommendation fairness improves over time, regardless of the recommendation method. We also find that two network properties, minority ratio, and homo
    
[^14]: 在预算限制下行为用户分割中的优化传递发现

    Delivery Optimized Discovery in Behavioral User Segmentation under Budget Constrain

    [https://arxiv.org/abs/2402.03388](https://arxiv.org/abs/2402.03388)

    在预算限制下，我们提出了一种基于随机优化的算法，用于优化传递发现行为用户细分。

    

    用户在线行为足迹可以使公司发现基于行为的用户细分，并向用户发送特定细分的信息。在发现细分之后，通过像Facebook和Google这样的首选媒体渠道向用户发送信息可能具有挑战性，因为只有部分行为细分中的用户在媒体上找到匹配，并且只有其中一小部分看到消息（曝光）。即使高质量的发现也会在传递失败时变得无用。许多复杂的算法用于发现行为细分，然而这些算法忽略了传递组件。问题变得复杂是因为（i）发现是在公司数据（例如用户点击）的行为数据空间中进行的，而传递则是基于媒体定义的静态数据空间（例如地理位置，年龄）进行的；（ii）公司在预算限制下运作。我们引入了一种基于随机优化的算法，用于在预算限制下优化传递发现行为用户细分。

    Users' behavioral footprints online enable firms to discover behavior-based user segments (or, segments) and deliver segment specific messages to users. Following the discovery of segments, delivery of messages to users through preferred media channels like Facebook and Google can be challenging, as only a portion of users in a behavior segment find match in a medium, and only a fraction of those matched actually see the message (exposure). Even high quality discovery becomes futile when delivery fails. Many sophisticated algorithms exist for discovering behavioral segments; however, these ignore the delivery component. The problem is compounded because (i) the discovery is performed on the behavior data space in firms' data (e.g., user clicks), while the delivery is predicated on the static data space (e.g., geo, age) as defined by media; and (ii) firms work under budget constraint. We introduce a stochastic optimization based algorithm for delivery optimized discovery of behavioral u
    
[^15]: 修改的K-means聚类算法 - COVID-19数据应用

    Modified K-means with Cluster Assignment -- Application to COVID-19 Data

    [https://arxiv.org/abs/2402.03380](https://arxiv.org/abs/2402.03380)

    本论文描述了一个改进的K-means聚类算法，应用于COVID-19数据。通过利用两个聚类分配技术与K-means模型，提取文本的本体论被改善，进一步应用矢量平均阻尼技术以灵活移动聚类。实验结果表明，该算法在COVID-19数据上获得了良好的结果。

    

    文本提取是一个高度主观的问题，取决于正在处理的数据集和需要提取出的摘要细节的种类。从数据的预处理到选择最优模型进行预测的所有步骤都取决于问题和手头的语料库。在本文中，我们描述了一个文本提取模型，其目标是提取与语义相关的指定单词信息，从而以简洁的格式获取所有相关和有意义的信息。该模型可以获得有意义的结果，并可以增强普遍搜索模型或常规聚类或主题建模算法。通过利用称为两个聚类分配技术的新技术与K-means模型，我们改善了提取文本的本体论。我们进一步应用矢量平均阻尼技术以灵活移动聚类。我们在最近的Covid-19语料库上的实验结果表明，我们获得了良好的结果。

    Text extraction is a highly subjective problem which depends on the dataset that one is working on and the kind of summarization details that needs to be extracted out. All the steps ranging from preprocessing of the data, to the choice of an optimal model for predictions, depends on the problem and the corpus at hand. In this paper, we describe a text extraction model where the aim is to extract word specified information relating to the semantics such that we can get all related and meaningful information about that word in a succinct format. This model can obtain meaningful results and can augment ubiquitous search model or a normal clustering or topic modelling algorithms. By utilizing new technique called two cluster assignment technique with K-means model, we improved the ontology of the retrieved text. We further apply the vector average damping technique for flexible movement of clusters. Our experimental results on a recent corpus of Covid-19 shows that we obtain good results 
    
[^16]: 全链路上升建模与上下文增强学习用于智能营销

    Entire Chain Uplift Modeling with Context-Enhanced Learning for Intelligent Marketing

    [https://arxiv.org/abs/2402.03379](https://arxiv.org/abs/2402.03379)

    全链路上升建模方法ECUP旨在解决链路偏差和处理不适应问题，在线营销中有重要的应用价值。

    

    上升建模在在线营销中非常重要，它旨在通过预测个体处理效果（ITE）来准确衡量不同策略（如优惠券或折扣）对不同用户的影响。在电子商务环境中，用户行为遵循确定的顺序链路，包括展示、点击和转化。营销策略在这个链路中的每个阶段都会产生不同的上升效应，影响着点击率和转化率等指标。尽管其实用性，现有研究忽视了特定处理中所有阶段的相互影响，并未充分利用处理信息，可能给后续的营销决策引入了重大偏差。本文将这两个问题称为链路偏差问题和处理不适应问题。本文介绍了一种用于解决这些问题的具有上下文增强学习的全链路上升方法（ECUP）。ECUP包括两个主要组成部分：

    Uplift modeling, vital in online marketing, seeks to accurately measure the impact of various strategies, such as coupons or discounts, on different users by predicting the Individual Treatment Effect (ITE). In an e-commerce setting, user behavior follows a defined sequential chain, including impression, click, and conversion. Marketing strategies exert varied uplift effects at each stage within this chain, impacting metrics like click-through and conversion rate. Despite its utility, existing research has neglected to consider the inter-task across all stages impacts within a specific treatment and has insufficiently utilized the treatment information, potentially introducing substantial bias into subsequent marketing decisions. We identify these two issues as the chain-bias problem and the treatment-unadaptive problem. This paper introduces the Entire Chain UPlift method with context-enhanced learning (ECUP), devised to tackle these issues. ECUP consists of two primary components: 1)
    
[^17]: 检测科学文献中的拙劣短语

    Detection of tortured phrases in scientific literature

    [https://arxiv.org/abs/2402.03370](https://arxiv.org/abs/2402.03370)

    本文介绍了自动检测科学论文中拙劣短语的方法，通过语言模型和预测分数的传播，可以高效地标记并提取这些拙劣短语，为领域专家验证提供新的数据。

    

    本文介绍了多种自动检测方法，用于从科学论文中提取所谓的拙劣短语。这些拙劣短语，例如将"信号与噪声"替换为"旗帜与喧闹"，是使用改写工具规避抄袭检测的结果。我们构建了一个数据集，并评估了几种策略来标记以前未记录的拙劣短语。提出和测试的方法基于语言模型，要么基于嵌入相似性，要么基于掩码标记的预测。我们发现，一种使用标记预测并将分数传播到块级别的方法效果最好。其召回率为0.87，精确率为0.61，可以检索到新的拙劣短语以供领域专家验证。

    This paper presents various automatic detection methods to extract so called tortured phrases from scientific papers. These tortured phrases, e.g. flag to clamor instead of signal to noise, are the results of paraphrasing tools used to escape plagiarism detection. We built a dataset and evaluated several strategies to flag previously undocumented tortured phrases. The proposed and tested methods are based on language models and either on embeddings similarities or on predictions of masked token. We found that an approach using token prediction and that propagates the scores to the chunk level gives the best results. With a recall value of .87 and a precision value of .61, it could retrieve new tortured phrases to be submitted to domain experts for validation.
    
[^18]: 推荐系统中大数据的实证和实验研究：一项综合调查

    Empirical and Experimental Perspectives on Big Data in Recommendation Systems: A Comprehensive Survey

    [https://arxiv.org/abs/2402.03368](https://arxiv.org/abs/2402.03368)

    本综合调查论文对推荐系统中的大数据算法进行了全面分析，并提出了一种新颖的、分层的分类法。通过该分类法，研究人员可以全面了解不同算法和技术之间的相互关系。

    

    本综合调查论文对推荐系统中的大数据算法进行了全面分析，解决了现有文献中深度和精确性不足的问题。它提出了一种双管齐下的方法：对当前算法进行彻底分析，并提出了一种新颖的、分层的分类法以实现精确归类。该分类法基于三层层次结构，从方法学类别开始，逐步细化为具体技术。这样的框架允许对算法进行结构化和全面的分类，帮助研究人员理解不同算法和技术之间的相互关系。论文涵盖了广泛的算法，首先将算法分为四种主要分析类型：基于用户和项目相似度的方法、混合和综合方法、深度学习和算法方法、数学建模方法，进一步细分为子类别和技术。论文融合了实证和实验视角。

    This survey paper provides a comprehensive analysis of big data algorithms in recommendation systems, addressing the lack of depth and precision in existing literature. It proposes a two-pronged approach: a thorough analysis of current algorithms and a novel, hierarchical taxonomy for precise categorization. The taxonomy is based on a tri-level hierarchy, starting with the methodology category and narrowing down to specific techniques. Such a framework allows for a structured and comprehensive classification of algorithms, assisting researchers in understanding the interrelationships among diverse algorithms and techniques. Covering a wide range of algorithms, this taxonomy first categorizes algorithms into four main analysis types: User and Item Similarity-Based Methods, Hybrid and Combined Approaches, Deep Learning and Algorithmic Methods, and Mathematical Modeling Methods, with further subdivisions into sub-categories and techniques. The paper incorporates both empirical and experim
    
[^19]: RAG-Fusion: 检索增强生成的新途径

    RAG-Fusion: a New Take on Retrieval-Augmented Generation

    [https://arxiv.org/abs/2402.03367](https://arxiv.org/abs/2402.03367)

    RAG-Fusion方法通过生成多个查询，并结合互惠排名融合技术，能够从不同角度上下文化原始查询，提供准确和全面的信息。这项研究在人工智能和自然语言处理应用方面有重要进展，并展示了全球和区域之间的转变。

    

    Infineon已经确定工程师、客户经理和客户迅速获取产品信息的需求。传统上，这个问题通过检索增强生成（RAG）聊天机器人来解决，但在这项研究中，我评估了新近流行的RAG-Fusion方法的使用。RAG-Fusion将RAG和互惠排名融合（RRF）相结合，通过生成多个查询，使用互惠分数对其进行再排序，并融合文档和分数。通过对准确性、相关性和全面性进行手动评估，我发现RAG-Fusion能够通过从不同的角度对原始查询进行上下文化，提供准确和全面的回答。然而，当生成的查询与原始查询的相关性不足时，有些答案偏离了主题。这项研究在人工智能（AI）和自然语言处理（NLP）应用方面取得了重要进展，并展示了全球和区域之间的变革。

    Infineon has identified a need for engineers, account managers, and customers to rapidly obtain product information. This problem is traditionally addressed with retrieval-augmented generation (RAG) chatbots, but in this study, I evaluated the use of the newly popularized RAG-Fusion method. RAG-Fusion combines RAG and reciprocal rank fusion (RRF) by generating multiple queries, reranking them with reciprocal scores and fusing the documents and scores. Through manually evaluating answers on accuracy, relevance, and comprehensiveness, I found that RAG-Fusion was able to provide accurate and comprehensive answers due to the generated queries contextualizing the original query from various perspectives. However, some answers strayed off topic when the generated queries' relevance to the original query is insufficient. This research marks significant progress in artificial intelligence (AI) and natural language processing (NLP) applications and demonstrates transformations in a global and m
    
[^20]: 带有大型语言模型的不确定性感知可解释推荐

    Uncertainty-Aware Explainable Recommendation with Large Language Models

    [https://arxiv.org/abs/2402.03366](https://arxiv.org/abs/2402.03366)

    这项研究开发了一个模型，通过训练用户和项目输入的ID向量作为提示，利用GPT-2实现不确定性感知的可解释推荐系统。该系统采用联合训练机制并在多任务学习框架中进行优化，能够更有效地探索用户的兴趣，提高推荐效果和用户满意度。

    

    在推荐系统内提供解释能够提升用户满意度并建立信任，特别是通过详细说明为用户定制推荐项目的原因。当前领域中主要的方法是生成基于文本的解释，而大型语言模型（LLMs）的应用尤为突出。然而，由于时间和计算资源限制，改进LLMs以实现可解释的推荐在实践上是不可行的。作为替代方案，当前的方法是训练提示而不是LLM。在这项研究中，我们开发了一个模型，利用用户和项目输入的ID向量作为GPT-2的提示。我们在多任务学习框架中采用联合训练机制，优化推荐任务和解释任务。这种策略能够更有效地探索用户的兴趣，提高推荐效果和用户满意度。通过实验，我们的方法表现出...

    Providing explanations within the recommendation system would boost user satisfaction and foster trust, especially by elaborating on the reasons for selecting recommended items tailored to the user. The predominant approach in this domain revolves around generating text-based explanations, with a notable emphasis on applying large language models (LLMs). However, refining LLMs for explainable recommendations proves impractical due to time constraints and computing resource limitations. As an alternative, the current approach involves training the prompt rather than the LLM. In this study, we developed a model that utilizes the ID vectors of user and item inputs as prompts for GPT-2. We employed a joint training mechanism within a multi-task learning framework to optimize both the recommendation task and explanation task. This strategy enables a more effective exploration of users' interests, improving recommendation effectiveness and user satisfaction. Through the experiments, our meth
    
[^21]: 利用图卷积网络的異质友善推荐方法

    Heterophily-Aware Fair Recommendation using Graph Convolutional Networks

    [https://arxiv.org/abs/2402.03365](https://arxiv.org/abs/2402.03365)

    本文提出了一种利用图卷积网络的公平推荐系统，名为HetroFair，旨在提高项目侧的公平性。HetroFair使用公平注意力和异质性特征加权两个组件来生成具有公平性意识的嵌入。

    

    近年来，图神经网络（GNNs）已成为提高推荐系统准确性和性能的流行工具。现代推荐系统不仅设计为为最终用户服务，还要让其他参与者（如项目和项目供应商）从中受益。这些参与者可能具有不同或冲突的目标和利益，这引发了对公平性和流行度偏差考虑的需求。基于GNN的推荐方法也面临不公平性和流行度偏差的挑战，其归一化和聚合过程受到这些挑战的影响。在本文中，我们提出了一种公平的基于GNN的推荐系统，称为HetroFair，旨在提高项目侧的公平性。HetroFair使用两个独立的组件生成具有公平性意识的嵌入：i）公平注意力，它在GNN的归一化过程中结合了点积，以减少节点度数的影响；ii）异质性特征加权，为不同的特征分配不同的权重。

    In recent years, graph neural networks (GNNs) have become a popular tool to improve the accuracy and performance of recommender systems. Modern recommender systems are not only designed to serve the end users, but also to benefit other participants, such as items and items providers. These participants may have different or conflicting goals and interests, which raise the need for fairness and popularity bias considerations. GNN-based recommendation methods also face the challenges of unfairness and popularity bias and their normalization and aggregation processes suffer from these challenges. In this paper, we propose a fair GNN-based recommender system, called HetroFair, to improve items' side fairness. HetroFair uses two separate components to generate fairness-aware embeddings: i) fairness-aware attention which incorporates dot product in the normalization process of GNNs, to decrease the effect of nodes' degrees, and ii) heterophily feature weighting to assign distinct weights to 
    
[^22]: NanoNER: 使用领域专家知识和远程监督进行纳米生物学的命名实体识别

    NanoNER: Named Entity Recognition for nanobiology using experts' knowledge and distant supervision

    [https://arxiv.org/abs/2402.03362](https://arxiv.org/abs/2402.03362)

    本文介绍了NanoNER，它是一种用于纳米生物学的命名实体识别模型。通过使用领域专家的知识和远程监督学习，NanoNER能够准确地识别先前已知实体，并最大程度地提高注释数据的质量和数量。

    

    本文介绍了NanoNER的训练和评估，它是一种用于纳米生物学的命名实体识别（NER）模型。NER是在非结构化文本中识别特定实体的任务，在自然语言处理（NLP）和信息提取中经常是一个主要任务。我们的模型的目的是识别领域专家之前确定为该领域基本知识的实体。我们依靠本体论来提供领域词汇和分类，实现了一个迭代过程，使专家能够确定与当前领域相关的实体。然后我们深入探讨了远程监督学习在NER中的潜力，支持这种方法如何可以通过最少的人力增加注释数据的数量。在包含超过120k实体出现次数的728篇全文纳米生物学文章的完整语料库上，NanoNER在先前已知实体的识别上获得了0.98的F1分数。

    Here we present the training and evaluation of NanoNER, a Named Entity Recognition (NER) model for Nanobiology. NER consists in the identification of specific entities in spans of unstructured texts and is often a primary task in Natural Language Processing (NLP) and Information Extraction. The aim of our model is to recognise entities previously identified by domain experts as constituting the essential knowledge of the domain. Relying on ontologies, which provide us with a domain vocabulary and taxonomy, we implemented an iterative process enabling experts to determine the entities relevant to the domain at hand. We then delve into the potential of distant supervision learning in NER, supporting how this method can increase the quantity of annotated data with minimal additional manpower. On our full corpus of 728 full-text nanobiology articles, containing more than 120k entity occurrences, NanoNER obtained a F1-score of 0.98 on the recognition of previously known entities. Our model 
    
[^23]: EasyInstruct：一个易于使用的用于大型语言模型的指令处理框架

    EasyInstruct: An Easy-to-use Instruction Processing Framework for Large Language Models

    [https://arxiv.org/abs/2402.03049](https://arxiv.org/abs/2402.03049)

    EasyInstruct是一个易于使用的用于大型语言模型的指令处理框架，通过模块化指令生成、选择和提示，并考虑它们的组合和交互，使指令处理更加方便和高效。

    

    近年来，指令调整已经引起了越来越多的关注，并成为增强大型语言模型（LLMs）能力的一种关键技术。为了构建高质量的指令数据集，已经提出了许多指令处理方法，旨在在数据数量和数据质量之间达到精巧的平衡。然而，由于各种指令处理方法之间仍然存在不一致，目前没有标准的开源指令处理实现框架可供社区使用，这使得从业者无法进一步开发和推进。为了促进指令处理的研究和开发，我们提出了EasyInstruct，一个易于使用的用于LLMs的指令处理框架，它将指令生成、选择和提示模块化，并考虑它们的组合和交互。EasyInstruct已经在https://github.com/zjunlp/EasyInstruct上公开发布，并得到了积极维护。

    In recent years, instruction tuning has gained increasing attention and emerged as a crucial technique to enhance the capabilities of Large Language Models (LLMs). To construct high-quality instruction datasets, many instruction processing approaches have been proposed, aiming to achieve a delicate balance between data quantity and data quality. Nevertheless, due to inconsistencies that persist among various instruction processing methods, there is no standard open-source instruction processing implementation framework available for the community, which hinders practitioners from further developing and advancing. To facilitate instruction processing research and development, we present EasyInstruct, an easy-to-use instruction processing framework for LLMs, which modularizes instruction generation, selection, and prompting, while also considering their combination and interaction. EasyInstruct is publicly released and actively maintained at https://github.com/zjunlp/EasyInstruct, along 
    
[^24]: K-PERM: 使用动态知识检索和个人适应性查询实现个性化响应生成

    K-PERM: Personalized Response Generation Using Dynamic Knowledge Retrieval and Persona-Adaptive Queries

    [https://arxiv.org/abs/2312.17748](https://arxiv.org/abs/2312.17748)

    K-PERM是一种使用动态知识检索和个人适应性查询来实现个性化响应生成的对话代理，可以提高对话的质量和用户参与度，并在真实个性化对话方面取得了最先进的性能。

    

    通过个性化对话代理可以提高对话的质量并增加用户的参与度。然而，它们通常缺乏外部知识以适应用户的个性。这对于实际应用，如心理健康支持、饮食计划、文化敏感对话或减少对话代理中的有毒行为尤为重要。为了提高个性化响应的相关性和全面性，我们提出了一个两步方法，包括(1)有选择性地集成用户人物和(2)使用背景知识源补充信息来给响应提供上下文。我们开发了K-PERM(知识引导的个性化与奖励调控)，它是一个动态对话代理，结合了这些元素。K-PERM在流行的FoCus数据集上实现了最先进的性能，该数据集包含有关全球地标的真实个性化对话。我们证明使用K-PERM的响应可以改进对话体验。

    Personalizing conversational agents can enhance the quality of conversations and increase user engagement. However, they often lack external knowledge to appropriately tend to a user's persona. This is particularly crucial for practical applications like mental health support, nutrition planning, culturally sensitive conversations, or reducing toxic behavior in conversational agents. To enhance the relevance and comprehensiveness of personalized responses, we propose using a two-step approach that involves (1) selectively integrating user personas and (2) contextualizing the response with supplementing information from a background knowledge source. We develop K-PERM (Knowledge-guided PErsonalization with Reward Modulation), a dynamic conversational agent that combines these elements. K-PERM achieves state-of-the-art performance on the popular FoCus dataset, containing real-world personalized conversations concerning global landmarks. We show that using responses from K-PERM can improv
    
[^25]: 在具有折扣自适应偏好的代理中的在线推荐问题

    Online Recommendations for Agents with Discounted Adaptive Preferences

    [https://arxiv.org/abs/2302.06014](https://arxiv.org/abs/2302.06014)

    本文研究了具有折扣自适应偏好的代理的在线推荐问题，通过在每一轮中展示一系列物品并考虑代理的偏好演变，以实现对于目标集合的最小化后悔。在长期记忆的情况下，可以实现对于能够在任何时刻实现的分布集合的高效次线性后悔。

    

    我们考虑了一个Bandit推荐问题，其中代理的偏好（代表对推荐物品的选择概率）根据过去的选择作为未知"偏好模型"的函数而演变。在每一轮中，我们向代理展示$k$个物品（共$n$个），代理选择一个物品，我们的目标是在代理的选择上对于某个"目标集合"（物品简单形式的子集）的对抗损失最小化后悔。我们扩展了Agarwal和Brown（2022）的设定，他们考虑了一种均匀记忆的代理，而在这里，我们允许非均匀记忆，在每一轮中对代理的记忆向量应用一个折扣因子。在"长期记忆"的情况下（当有效的记忆时程与$T$的次线性变化），我们证明了可以实现对于"能够在任何时刻实现的分布集合"（即"即时实现分布集合"）的高效次线性后悔。

    We consider a bandit recommendations problem in which an agent's preferences (representing selection probabilities over recommended items) evolve as a function of past selections, according to an unknown $\textit{preference model}$. In each round, we show a menu of $k$ items (out of $n$ total) to the agent, who then chooses a single item, and we aim to minimize regret with respect to some $\textit{target set}$ (a subset of the item simplex) for adversarial losses over the agent's choices. Extending the setting from Agarwal and Brown (2022), where uniform-memory agents were considered, here we allow for non-uniform memory in which a discount factor is applied to the agent's memory vector at each subsequent round. In the "long-term memory" regime (when the effective memory horizon scales with $T$ sublinearly), we show that efficient sublinear regret is obtainable with respect to the set of $\textit{everywhere instantaneously realizable distributions}$ (the "EIRD set", as formulated in pr
    
[^26]: 整页无偏学习排序

    Whole Page Unbiased Learning to Rank

    [https://arxiv.org/abs/2210.10718](https://arxiv.org/abs/2210.10718)

    本论文提出整页无偏学习排序（WP-ULTR）方法处理整页 SERP 特征引发的偏差，该方法面临适合的用户行为模型的挑战和复杂的模型训练难题。

    

    信息检索系统中页面呈现的偏见，尤其是点击行为方面的偏差，是一个众所周知的挑战，阻碍了使用隐式用户反馈来改进排序模型的性能。因此，提出了无偏学习排序(ULTR)算法，通过偏差点击数据来学习一个无偏的排序模型。然而，大多数现有算法特别设计用于减轻与位置相关的偏差，例如信任偏差，并未考虑到搜索结果页面呈现(SERP)中其他特征引发的偏差，例如由多媒体引发的吸引偏差。不幸的是，这些偏差在工业系统中广泛存在，可能导致不令人满意的搜索体验。因此，我们引入了一个新的问题，即整页无偏学习排序(WP-ULTR)，旨在同时处理整页SERP特征引发的偏差。这带来了巨大的挑战：(1) 很难找到适合的用户行为模型 (用户行为假设)；(2) 复杂的模型训练问题。

    The page presentation biases in the information retrieval system, especially on the click behavior, is a well-known challenge that hinders improving ranking models' performance with implicit user feedback. Unbiased Learning to Rank~(ULTR) algorithms are then proposed to learn an unbiased ranking model with biased click data. However, most existing algorithms are specifically designed to mitigate position-related bias, e.g., trust bias, without considering biases induced by other features in search result page presentation(SERP), e.g. attractive bias induced by the multimedia. Unfortunately, those biases widely exist in industrial systems and may lead to an unsatisfactory search experience. Therefore, we introduce a new problem, i.e., whole-page Unbiased Learning to Rank(WP-ULTR), aiming to handle biases induced by whole-page SERP features simultaneously. It presents tremendous challenges: (1) a suitable user behavior model (user behavior hypothesis) can be hard to find; and (2) complex
    
[^27]: Re3val: 强化和重新排名的生成检索

    Re3val: Reinforced and Reranked Generative Retrieval. (arXiv:2401.16979v1 [cs.IR])

    [http://arxiv.org/abs/2401.16979](http://arxiv.org/abs/2401.16979)

    Re3val是一个使用强化学习和重新排名技术进行训练的生成检索模型，它通过利用上下文信息来重新排名检索得到的页面标题，以最大化通过受限解码生成的奖励。同时，该模型通过生成问题来减小认识不确定性，并弥合预训练和微调数据集之间的领域差距。

    

    生成检索模型将文档中的信息指针编码为模型参数中的索引。这些模型作为更大的流程的一部分，通过检索的信息来为知识密集型自然语言处理任务生成条件。然而，我们发现有两个限制：生成检索没有考虑上下文信息。其次，检索无法为下游读者进行调整，因为解码页面标题是一个非可微分的操作。本文介绍了经过有限数据训练的生成重新排名和强化学习的 Re3val。Re3val利用通过密集通道检索获得的上下文对已检索页面标题进行重新排名，并利用REINFORCE算法最大化受限解码生成的奖励。此外，我们从预训练数据集中生成问题，以减小认识不确定性，并弥合预训练和微调数据集之间的领域差距。随后，我们从中提取和重新排名上下文信息。

    Generative retrieval models encode pointers to information in a corpus as an index within the model's parameters. These models serve as part of a larger pipeline, where retrieved information conditions generation for knowledge-intensive NLP tasks. However, we identify two limitations: the generative retrieval does not account for contextual information. Secondly, the retrieval can't be tuned for the downstream readers as decoding the page title is a non-differentiable operation. This paper introduces Re3val, trained with generative reranking and reinforcement learning using limited data. Re3val leverages context acquired via Dense Passage Retrieval to rerank the retrieved page titles and utilizes REINFORCE to maximize rewards generated by constrained decoding. Additionally, we generate questions from our pre-training dataset to mitigate epistemic uncertainty and bridge the domain gap between the pre-training and fine-tuning datasets. Subsequently, we extract and rerank contexts from th
    
[^28]: 使用无监督相似度度量进行源代码克隆检测

    Source Code Clone Detection Using Unsupervised Similarity Measures. (arXiv:2401.09885v1 [cs.SE])

    [http://arxiv.org/abs/2401.09885](http://arxiv.org/abs/2401.09885)

    本研究对使用无监督相似度度量进行源代码克隆检测进行了比较分析，旨在为软件工程师提供指导，以选择适合其特定用例的方法。

    

    由于在软件工程任务中克隆检测和代码搜索与推荐的重要性，对源代码的相似性进行评估近年来引起了广泛关注。本研究提出了一种比较分析无监督相似度度量用于识别源代码克隆检测的方法。目标是概述目前的最新技术、它们的优点和缺点。为了达到这个目标，我们编译了现有的无监督策略，并评估其在基准数据集上的性能，以指导软件工程师在选择适用于其特定用例的方法时提供指导。本研究的源代码可在\url{https://github.com/jorge-martinez-gil/codesim}上获得。

    Assessing similarity in source code has gained significant attention in recent years due to its importance in software engineering tasks such as clone detection and code search and recommendation. This work presents a comparative analysis of unsupervised similarity measures for identifying source code clone detection. The goal is to overview the current state-of-the-art techniques, their strengths, and weaknesses. To do that, we compile the existing unsupervised strategies and evaluate their performance on a benchmark dataset to guide software engineers in selecting appropriate methods for their specific use cases. The source code of this study is available at \url{https://github.com/jorge-martinez-gil/codesim}
    
[^29]: 自然语言是图表所需要的全部内容

    Natural Language is All a Graph Needs. (arXiv:2308.07134v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2308.07134](http://arxiv.org/abs/2308.07134)

    本论文提出了一种名为InstructGLM的结构化语言模型算法，该算法将大型语言模型与图表学习问题相结合，旨在探索是否可以用语言模型取代图神经网络作为图表的基础模型。

    

    大规模预训练语言模型的出现，如ChatGPT，已经在人工智能的各个研究领域中引起了革命。基于Transformer的大型语言模型（LLMs）逐渐取代了CNN和RNN，将计算机视觉和自然语言处理领域统一起来。与相对独立存在的数据（如图像、视频或文本）相比，图表是一种包含丰富结构和关系信息的数据类型。同时，作为最具表现力的媒介之一，自然语言在描述复杂结构方面表现出色。然而，将图表学习问题纳入生成式语言建模框架的现有工作仍然非常有限。随着大型语言模型的重要性不断增长，探索LLMs是否也可以替代GNNs成为图表的基础模型变得至关重要。在本文中，我们提出了InstructGLM（结构化语言模型）算法，系统地设计高度可扩展的模型来处理图表学习问题。

    The emergence of large-scale pre-trained language models, such as ChatGPT, has revolutionized various research fields in artificial intelligence. Transformers-based large language models (LLMs) have gradually replaced CNNs and RNNs to unify fields of computer vision and natural language processing. Compared with the data that exists relatively independently such as images, videos or texts, graph is a type of data that contains rich structural and relational information. Meanwhile, natural language, as one of the most expressive mediums, excels in describing complex structures. However, existing work on incorporating graph learning problems into the generative language modeling framework remains very limited. As the importance of large language models continues to grow, it becomes essential to explore whether LLMs can also replace GNNs as the foundation model for graphs. In this paper, we propose InstructGLM (Instruction-finetuned Graph Language Model), systematically design highly scal
    
[^30]: 论用户响应模拟在对话式搜索中的深入研究

    An In-depth Investigation of User Response Simulation for Conversational Search. (arXiv:2304.07944v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2304.07944](http://arxiv.org/abs/2304.07944)

    本文研究了对话式搜索中用户响应模拟的方法。当前的模拟系统要么只能对是非问题进行回答，要么无法产生高质量的响应。通过用更小但先进的系统替换当前最先进的用户模拟系统，能够显著改进性能。

    

    对话式搜索在信息检索和自然语言处理领域引起了广泛关注。它通过多次自然语言交互来澄清和解决用户的搜索需求。然而，大多数现有系统是通过记录或人工对话日志进行训练和演示的。最终，对话式搜索系统应该在未见过的对话轨迹的开放环境中进行训练、评估和部署。一个关键的挑战是训练和评估这样的系统都需要人工参与，这既昂贵又不可扩展。其中一种策略是模拟用户，以此来减少扩展成本。然而，当前的用户模拟器要么仅限于对对话搜索系统的是非问题进行回答，要么无法产生高质量的响应。本文表明，通过用一个更小但先进的系统来替换当前最先进的用户模拟系统，能够大幅改进其性能。

    Conversational search has seen increased recent attention in both the IR and NLP communities. It seeks to clarify and solve a user's search need through multi-turn natural language interactions. However, most existing systems are trained and demonstrated with recorded or artificial conversation logs. Eventually, conversational search systems should be trained, evaluated, and deployed in an open-ended setting with unseen conversation trajectories. A key challenge is that training and evaluating such systems both require a human-in-the-loop, which is expensive and does not scale. One strategy for this is to simulate users, thereby reducing the scaling costs. However, current user simulators are either limited to only respond to yes-no questions from the conversational search system, or unable to produce high quality responses in general.  In this paper, we show that current state-of-the-art user simulation system could be significantly improved by replacing it with a smaller but advanced
    

