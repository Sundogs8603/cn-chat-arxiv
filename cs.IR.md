# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [GRM: Generative Relevance Modeling Using Relevance-Aware Sample Estimation for Document Retrieval.](http://arxiv.org/abs/2306.09938) | 本文提出了生成关联建模（GRM）方法，使用关联感知样本估计（RASE）进行文本加权，以更准确权衡扩展词项，实验结果表明，GRM方法在三个标准文档排名基准测试中取得了更好的效果。 |
| [^2] | [Smart Sentiment Analysis-based Search Engine Classification Intelligence.](http://arxiv.org/abs/2306.09777) | 本研究通过基于情感分析的搜索功能解决搜索结果中的多义问题，提高了召回率和准确性。所使用的Sentistrength程序在分类搜索结果方面表现优于深度学习和聚类方法。该方法可用于分析互联网上实体的情感和声誉。 |
| [^3] | [Online Distillation for Pseudo-Relevance Feedback.](http://arxiv.org/abs/2306.09657) | 本文提出了一种伪相关反馈的在线蒸馏技术，通过在线逐步建立模型，预测查询与文档的相关得分，并在索引中高效执行，以优化整体检索效果。 |
| [^4] | [I Want This, Not That: Personalized Summarization of Scientific Scholarly Texts.](http://arxiv.org/abs/2306.09604) | 该论文提出了一个名为 P-Summ 的无监督算法，能够生成符合用户个性化需求的学术文本抽取式摘要。该算法的创新之处在于其能够根据用户需求包含所需的知识并消除不需要的知识。同时，论文还提出了一种多粒度评估框架。 |
| [^5] | [h2oGPT: Democratizing Large Language Models.](http://arxiv.org/abs/2306.08161) | 本文介绍了h2oGPT，这是一套开源代码库，用于创建和使用基于GPTs的大语言模型（LLMs），包括100％私有文档搜索。目标是创建真正开源的替代封闭源GPTs，提高人工智能的开发和可靠性。 |
| [^6] | [Modeling Dual Period-Varying Preferences for Takeaway Recommendation.](http://arxiv.org/abs/2306.04370) | 提出了一种DPVP方法，用于外卖推荐，考虑到用户对商店和食品的双重偏好以及在一天中的不同时段偏好的变化。 |
| [^7] | [Graph-Based Model-Agnostic Data Subsampling for Recommendation Systems.](http://arxiv.org/abs/2305.16391) | 本文提出了一种基于图结构的无模型数据子采样方法，通过研究用户-物品图的拓扑结构来估计每个用户-物品交互的重要性，并在网络上进行传播来平滑估计值。该方法结合了无模型和基于模型的子采样方法的优点，在多个基准数据集上表现出较好的实验结果。 |
| [^8] | [Evaluating Search Explainability with Psychometrics and Crowdsourcing.](http://arxiv.org/abs/2210.09430) | 本文主要研究了Web搜索系统中的可解释性，利用心理测量和众包技术，分析了可解释性的多个因素，以期找到解释性与人类因素的关系。 |
| [^9] | [Revisiting DocRED -- Addressing the False Negative Problem in Relation Extraction.](http://arxiv.org/abs/2205.12696) | 该论文主要针对文档层面上的关系抽取中存在假阴性的问题，在重新注释 DocRED 数据集中添加被忽略的关系三元组后，我们得到了一个性能提升约 13 F1 分数的新数据集 Re-DocRED，并发现了有效改进的潜在领域。 |

# 详细

[^1]: 使用关联感知样本估计的生成关联建模来进行文档检索

    GRM: Generative Relevance Modeling Using Relevance-Aware Sample Estimation for Document Retrieval. (arXiv:2306.09938v1 [cs.IR])

    [http://arxiv.org/abs/2306.09938](http://arxiv.org/abs/2306.09938)

    本文提出了生成关联建模（GRM）方法，使用关联感知样本估计（RASE）进行文本加权，以更准确权衡扩展词项，实验结果表明，GRM方法在三个标准文档排名基准测试中取得了更好的效果。

    

    最近的研究表明，使用由大型语言模型生成的文本进行生成关联反馈（GRF）可以增强查询扩展的有效性。然而，语言模型可能会生成不相关的信息，对检索效果有害。为了解决这个问题，我们提出了使用关联感知样本估计（RASE）的生成关联建模（GRM），来更准确地加权扩展词项。具体地，我们为每个生成的文档识别类似的真实文档，并使用神经网络重新排序器估计它们的相关性。在三个标准文档排名基准测试中的实验表明，GRM将均值平均精度（MAP）提高了6-9%和R@1k提高了2-4%，超过了先前的方法。

    Recent studies show that Generative Relevance Feedback (GRF), using text generated by Large Language Models (LLMs), can enhance the effectiveness of query expansion. However, LLMs can generate irrelevant information that harms retrieval effectiveness. To address this, we propose Generative Relevance Modeling (GRM) that uses Relevance-Aware Sample Estimation (RASE) for more accurate weighting of expansion terms. Specifically, we identify similar real documents for each generated document and use a neural re-ranker to estimate their relevance. Experiments on three standard document ranking benchmarks show that GRM improves MAP by 6-9% and R@1k by 2-4%, surpassing previous methods.
    
[^2]: 基于情感分析的智能搜索引擎分类方法研究

    Smart Sentiment Analysis-based Search Engine Classification Intelligence. (arXiv:2306.09777v1 [cs.IR])

    [http://arxiv.org/abs/2306.09777](http://arxiv.org/abs/2306.09777)

    本研究通过基于情感分析的搜索功能解决搜索结果中的多义问题，提高了召回率和准确性。所使用的Sentistrength程序在分类搜索结果方面表现优于深度学习和聚类方法。该方法可用于分析互联网上实体的情感和声誉。

    

    搜索引擎是人们在互联网上寻找信息的常用工具。然而，当前的搜索方法存在一些局限性，例如提供流行但不一定相关的结果。本研究通过实现一个确定检索信息情感色彩的搜索功能来解决搜索结果中的多义问题。该研究利用网络爬虫从英国广播公司（BBC）新闻网站收集数据，使用Sentistrength程序确定新闻文章的情感性。结果表明，所提出的搜索功能在提高召回率的同时，准确地检索到了非多义新闻。此外，Sentistrength在分类搜索结果方面的表现优于深度学习和聚类方法。本文介绍的方法可以应用于分析互联网上实体的情感和声誉。

    Search engines are widely used for finding information on the internet. However, there are limitations in the current search approach, such as providing popular but not necessarily relevant results. This research addresses the issue of polysemy in search results by implementing a search function that determines the sentimentality of the retrieved information. The study utilizes a web crawler to collect data from the British Broadcasting Corporation (BBC) news site, and the sentimentality of the news articles is determined using the Sentistrength program. The results demonstrate that the proposed search function improves recall value while accurately retrieving nonpolysemous news. Furthermore, Sentistrength outperforms deep learning and clustering methods in classifying search results. The methodology presented in this article can be applied to analyze the sentimentality and reputation of entities on the internet.
    
[^3]: Online Distillation for Pseudo-Relevance Feedback（伪相关反馈的在线蒸馏技术）

    Online Distillation for Pseudo-Relevance Feedback. (arXiv:2306.09657v1 [cs.IR])

    [http://arxiv.org/abs/2306.09657](http://arxiv.org/abs/2306.09657)

    本文提出了一种伪相关反馈的在线蒸馏技术，通过在线逐步建立模型，预测查询与文档的相关得分，并在索引中高效执行，以优化整体检索效果。

    

    模型蒸馏是一种提高神经搜索模型效果的重要方法。当前，传统的蒸馏方法采用离线学习的方式，即训练一个新的神经模型预测任意查询与文档之间的相关得分。本文提出一种新的蒸馏方法，通过在线蒸馏逐步建立模型，以此来预测某一查询与文档之间的相关得分，并在索引中执行出色。该技术不仅可以扩大重新排序的文档数量，还能识别在第一阶段检索中被忽略的文档，以优化整体检索效果。

    Model distillation has emerged as a prominent technique to improve neural search models. To date, distillation taken an offline approach, wherein a new neural model is trained to predict relevance scores between arbitrary queries and documents. In this paper, we explore a departure from this offline distillation strategy by investigating whether a model for a specific query can be effectively distilled from neural re-ranking results (i.e., distilling in an online setting). Indeed, we find that a lexical model distilled online can reasonably replicate the re-ranking of a neural model. More importantly, these models can be used as queries that execute efficiently on indexes. This second retrieval stage can enrich the pool of documents for re-ranking by identifying documents that were missed in the first retrieval stage. Empirically, we show that this approach performs favourably when compared with established pseudo relevance feedback techniques, dense retrieval methods, and sparse-dense
    
[^4]: 我需要这个，而不是那个：个性化摘要科学学术文本

    I Want This, Not That: Personalized Summarization of Scientific Scholarly Texts. (arXiv:2306.09604v1 [cs.IR])

    [http://arxiv.org/abs/2306.09604](http://arxiv.org/abs/2306.09604)

    该论文提出了一个名为 P-Summ 的无监督算法，能够生成符合用户个性化需求的学术文本抽取式摘要。该算法的创新之处在于其能够根据用户需求包含所需的知识并消除不需要的知识。同时，论文还提出了一种多粒度评估框架。

    

    本文提出了一个无监督算法 P-Summ，它利用加权非负矩阵分解揭示文档的潜在语义空间，在符合用户知识需求的基础上，对句子进行评分，生成科学学术文本的抽取式摘要以满足用户的个性化需求。该方法的创新之处在于其能够包含所需的知识并消除不需要的知识。我们还提出了一个多粒度评估框架，该框架通过比较生成的个性化摘要与系统生成的通用摘要，包括句子、词语和语义三个层次对该摘要的质量进行评估。评估算法在语义层面的有效性，是通过与参考摘要和用户需求进行比较完成的。

    In this paper, we present a proposal for an unsupervised algorithm, P-Summ, that generates an extractive summary of scientific scholarly text to meet the personal knowledge needs of the user. The method delves into the latent semantic space of the document exposed by Weighted Non-negative Matrix Factorization, and scores sentences in consonance with the knowledge needs of the user. The novelty of the algorithm lies in its ability to include desired knowledge and eliminate unwanted knowledge in the personal summary.  We also propose a multi-granular evaluation framework, which assesses the quality of generated personal summaries at three levels of granularity sentence, terms and semantic. The framework uses system generated generic summary instead of human generated summary as gold standard for evaluating the quality of personal summary generated by the algorithm. The effectiveness of the algorithm at the semantic level is evaluated by taking into account the reference summary and the
    
[^5]: h2oGPT：民主化大语言模型

    h2oGPT: Democratizing Large Language Models. (arXiv:2306.08161v1 [cs.CL])

    [http://arxiv.org/abs/2306.08161](http://arxiv.org/abs/2306.08161)

    本文介绍了h2oGPT，这是一套开源代码库，用于创建和使用基于GPTs的大语言模型（LLMs），包括100％私有文档搜索。目标是创建真正开源的替代封闭源GPTs，提高人工智能的开发和可靠性。

    

    基于生成预训练变压器（GPTs），大语言模型（LLMs）如GPT-4因其在自然语言处理方面的现实应用而成为人工智能革命的一部分。然而，它们也带来了许多重大的风险，如存在有偏见、私人或有害文本和未经授权的版权材料。本文介绍了h2oGPT，这是一套开源代码库，用于创建和使用基于GPTs的大语言模型（LLMs）。该项目的目标是创建世界上最好的真正开源的替代封闭源GPTs。与开源社区合作，作为其一部分，我们开源了几个LLM，其参数从7亿到400亿，可在完全自由的Apache 2.0许可下商用。我们的发布包括使用自然语言的100％私有文档搜索。开源语言模型有助于促进人工智能的发展并使其更加可靠。

    Foundation Large Language Models (LLMs) such as GPT-4 represent a revolution in AI due to their real-world applications though natural language processing. However, they also pose many significant risks such as the presence of biased, private, or harmful text, and the unauthorized inclusion of copyrighted material.  We introduce h2oGPT, a suite of open-source code repositories for the creation and use of Large Language Models (LLMs) based on Generative Pretrained Transformers (GPTs). The goal of this project is to create the world's best truly open-source alternative to closed-source GPTs. In collaboration with and as part of the incredible and unstoppable open-source community, we open-source several fine-tuned h2oGPT models from 7 to 40 Billion parameters, ready for commercial use under fully permissive Apache 2.0 licenses. Included in our release is 100% private document search using natural language.  Open-source language models help boost AI development and make it more accessible
    
[^6]: 基于双周期变化偏好的外卖推荐模型

    Modeling Dual Period-Varying Preferences for Takeaway Recommendation. (arXiv:2306.04370v1 [cs.IR])

    [http://arxiv.org/abs/2306.04370](http://arxiv.org/abs/2306.04370)

    提出了一种DPVP方法，用于外卖推荐，考虑到用户对商店和食品的双重偏好以及在一天中的不同时段偏好的变化。

    

    外卖推荐系统旨在精准提供符合用户兴趣的商店和食物，已经为数十亿用户的日常生活提供服务。与传统推荐不同，外卖推荐面临两个主要挑战：（1）双重交互感知偏好建模。传统的推荐通常关注用户对物品的单一偏好，而外卖推荐需要全面考虑用户对商店和食品的双重偏好。(2) 周期性变化偏好建模。传统的推荐通常从会话级别或日级别的角度来建模用户偏好的连续变化。然而，在实际的外卖系统中，用户的偏好在早晨、中午、晚上和深夜等时段都会有显著的变化。针对这些挑战，我们提出了一种基于双周期变化偏好的外卖推荐建模（DPVP）方法。具体来说，我们设计了一个双重交互感知模块，旨在

    Takeaway recommender systems, which aim to accurately provide stores that offer foods meeting users' interests, have served billions of users in our daily life. Different from traditional recommendation, takeaway recommendation faces two main challenges: (1) Dual Interaction-Aware Preference Modeling. Traditional recommendation commonly focuses on users' single preferences for items while takeaway recommendation needs to comprehensively consider users' dual preferences for stores and foods. (2) Period-Varying Preference Modeling. Conventional recommendation generally models continuous changes in users' preferences from a session-level or day-level perspective. However, in practical takeaway systems, users' preferences vary significantly during the morning, noon, night, and late night periods of the day. To address these challenges, we propose a Dual Period-Varying Preference modeling (DPVP) for takeaway recommendation. Specifically, we design a dual interaction-aware module, aiming to 
    
[^7]: 基于图的无模型数据子采样在推荐系统中的应用研究

    Graph-Based Model-Agnostic Data Subsampling for Recommendation Systems. (arXiv:2305.16391v1 [cs.IR])

    [http://arxiv.org/abs/2305.16391](http://arxiv.org/abs/2305.16391)

    本文提出了一种基于图结构的无模型数据子采样方法，通过研究用户-物品图的拓扑结构来估计每个用户-物品交互的重要性，并在网络上进行传播来平滑估计值。该方法结合了无模型和基于模型的子采样方法的优点，在多个基准数据集上表现出较好的实验结果。

    

    数据子采样广泛用于加速训练大规模推荐系统。大多数子采样方法是基于模型的，常常需要一个预训练的试验模型来通过样本难度等方式测量数据重要性。然而，当试验模型被错误指定时，基于模型的子采样方法将会恶化。鉴于试验模型的错误指定在真实的推荐系统中普遍存在，我们提出了基于数据结构，即图形来探索的无模型数据子采样方法。具体地，我们研究用户-物品图的拓扑结构，通过图导电性来估计每个用户-物品交互（即用户-物品图中的一条边）的重要性，并在网络上进行传播步骤，平滑估计的重要性值。由于我们提出的方法是无模型的，因此我们可以将无模型和基于模型的子采样方法的优点结合起来。我们的实证研究表明，将这两种方法组合使用，在多个基准数据集上均比任何单一方法都要好。

    Data subsampling is widely used to speed up the training of large-scale recommendation systems. Most subsampling methods are model-based and often require a pre-trained pilot model to measure data importance via e.g. sample hardness. However, when the pilot model is misspecified, model-based subsampling methods deteriorate. Since model misspecification is persistent in real recommendation systems, we instead propose model-agnostic data subsampling methods by only exploring input data structure represented by graphs. Specifically, we study the topology of the user-item graph to estimate the importance of each user-item interaction (an edge in the user-item graph) via graph conductance, followed by a propagation step on the network to smooth out the estimated importance value.  Since our proposed method is model-agnostic, we can marry the merits of both model-agnostic and model-based subsampling methods. Empirically, we show that combing the two consistently improves over any single meth
    
[^8]: 通过心理测量和众包评估搜索可解释性

    Evaluating Search Explainability with Psychometrics and Crowdsourcing. (arXiv:2210.09430v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2210.09430](http://arxiv.org/abs/2210.09430)

    本文主要研究了Web搜索系统中的可解释性，利用心理测量和众包技术，分析了可解释性的多个因素，以期找到解释性与人类因素的关系。

    

    信息检索（IR）系统已成为我们日常生活中不可或缺的一部分。由于搜索引擎、推荐系统和对话代理在从娱乐搜索到临床决策支持等各个领域得到应用，因此需要透明和可解释的系统来确保可追溯、公正和无偏见的结果。尽管在可解释的AI和IR技术方面取得了许多近期进展，但仍无法就系统可解释性的含义达成共识。虽然越来越多的文献表明，解释性包含多个子因素，但实际上所有现有方法几乎都将其视为单一概念。在本文中，我们利用心理测量和众包研究了Web搜索系统中的可解释性，以确定人类中心因素与可解释性之间的关系。

    Information retrieval (IR) systems have become an integral part of our everyday lives. As search engines, recommender systems, and conversational agents are employed across various domains from recreational search to clinical decision support, there is an increasing need for transparent and explainable systems to guarantee accountable, fair, and unbiased results. Despite many recent advances towards explainable AI and IR techniques, there is no consensus on what it means for a system to be explainable. Although a growing body of literature suggests that explainability is comprised of multiple subfactors, virtually all existing approaches treat it as a singular notion. In this paper, we examine explainability in Web search systems, leveraging psychometrics and crowdsourcing to identify human-centered factors of explainability.
    
[^9]: 重新审视 DocRED - 解决关系抽取中的假阴性问题

    Revisiting DocRED -- Addressing the False Negative Problem in Relation Extraction. (arXiv:2205.12696v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2205.12696](http://arxiv.org/abs/2205.12696)

    该论文主要针对文档层面上的关系抽取中存在假阴性的问题，在重新注释 DocRED 数据集中添加被忽略的关系三元组后，我们得到了一个性能提升约 13 F1 分数的新数据集 Re-DocRED，并发现了有效改进的潜在领域。

    

    DocRED 数据集是最流行和广泛使用的文档级关系抽取基准数据集之一。它采用了一个推荐-修订的标注方案，以获得大规模的注释数据集。但是，我们发现 DocRED 的标注是不完整的，即假阴性样本很普遍。为了解决这个缺点，我们通过向原始 DocRED 中添加被忽略的关系三元组来重新注释了 4,053 个文档，我们将我们修正后的 DocRED 数据集命名为 Re-DocRED。我们使用最先进的神经模型在这两个数据集上开展了大量的实验，实验结果表明，使用我们的 Re-DocRED 训练和评估的模型性能提高了大约 13 个 F1 分数。此外，我们进行了全面的分析，识别出了进一步改进的潜在领域。我们的数据集公开在 https://github。

    The DocRED dataset is one of the most popular and widely used benchmarks for document-level relation extraction (RE). It adopts a recommend-revise annotation scheme so as to have a large-scale annotated dataset. However, we find that the annotation of DocRED is incomplete, i.e., false negative samples are prevalent. We analyze the causes and effects of the overwhelming false negative problem in the DocRED dataset. To address the shortcoming, we re-annotate 4,053 documents in the DocRED dataset by adding the missed relation triples back to the original DocRED. We name our revised DocRED dataset Re-DocRED. We conduct extensive experiments with state-of-the-art neural models on both datasets, and the experimental results show that the models trained and evaluated on our Re-DocRED achieve performance improvements of around 13 F1 points. Moreover, we conduct a comprehensive analysis to identify the potential areas for further improvement. Our dataset is publicly available at https://github.
    

