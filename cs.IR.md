# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Extracting Information from Twitter Screenshots.](http://arxiv.org/abs/2306.08236) | 本研究提出了从推特截图中提取推文文本、时间戳和推特句柄的方法，旨在开发一个工具，利用实时网络和网络存档中的资源，评估一张推特截图中推文真实性的概率。 |
| [^2] | [Towards Building Voice-based Conversational Recommender Systems: Datasets, Potential Solutions, and Prospects.](http://arxiv.org/abs/2306.08219) | 本文调查了基于语音的对话式推荐系统的潜力，并为电子商务和电影领域创建了两个VCRSs基准数据集，以帮助实现自然、直观、便利和易于访问的用户与RSs互动方式。 |
| [^3] | [Learning on Graphs under Label Noise.](http://arxiv.org/abs/2306.08194) | 该论文介绍了一种新方法 CGNN，它利用一致性图神经网络和基于同质性假设的样本选择技术，在标签噪声的情况下对节点分类进行建模，实现过滤出噪声节点和增强节点表示的鲁棒性。 |
| [^4] | [Contextual Font Recommendations based on User Intent.](http://arxiv.org/abs/2306.08188) | Adobe的20,000多种字体库是个选择恐惧症患者的噩梦，本文通过创造一个基于用户意图的系统来自动给用户提供合适的字体推荐，目前已被数百万 Adobe Express 用户使用，点击率高达 25% 以上。 |
| [^5] | [h2oGPT: Democratizing Large Language Models.](http://arxiv.org/abs/2306.08161) | 本文介绍了h2oGPT，这是一套开源代码库，用于创建和使用基于GPTs的大语言模型（LLMs），包括100％私有文档搜索。目标是创建真正开源的替代封闭源GPTs，提高人工智能的开发和可靠性。 |
| [^6] | [Better Generalization with Semantic IDs: A case study in Ranking for Recommendations.](http://arxiv.org/abs/2306.08121) | 本文提出使用语义ID解决推荐系统中的物品冷启动问题，这些ID是从内容嵌入中学习的，可以捕捉概念的层次关系，相较于完全消除ID特征的方法，语义ID能更好地提高推荐质量。 |
| [^7] | [Mol-Instructions: A Large-Scale Biomolecular Instruction Dataset for Large Language Models.](http://arxiv.org/abs/2306.08018) | Mol-Instructions是一个专门为生物分子领域设计的综合指令数据集，可以显著提高大语言模型在生物领域中的适应能力和认知敏锐度。 |
| [^8] | [Dark web activity classification using deep learning.](http://arxiv.org/abs/2306.07980) | 本文阐述了对于识别和控制暗网非法活动的迫切需求，并提出了一种利用深度学习方法检索 .onion 扩展名的网站上相关图片的搜索引擎，该方法在测试中达到了94% 的准确率。 |
| [^9] | [Utilizing Social Media Attributes for Enhanced Keyword Detection: An IDF-LDA Model Applied to Sina Weibo.](http://arxiv.org/abs/2306.07978) | 本文提出了一种新的方法，将逆文档频率（IDF）和潜在狄利克雷分配（LDA）模型相结合，以更好地应对社交媒体数据的不同属性，通过基于点赞数、评论数和转发数等属性对每个文档的重要性进行加权，从而有效地过滤噪声并识别最相关的关键词。 |
| [^10] | [KuaiSAR: A Unified Search And Recommendation Dataset.](http://arxiv.org/abs/2306.07705) | 这篇论文介绍了一份大规模、真实的数据集KuaiSAR，该数据集记录了快手短视频应用程序中真实的集成搜索和推荐行为。 |
| [^11] | [Incentivizing High-Quality Content in Online Recommender Systems.](http://arxiv.org/abs/2306.07479) | 本文研究了在线推荐系统中激励高质量内容的算法问题，经典的在线学习算法会激励生产者创建低质量的内容，但本文提出的一种算法通过惩罚低质量内容的创建者，成功地激励了生产者创造高质量的内容。 |
| [^12] | [How Ready are Pre-trained Abstractive Models and LLMs for Legal Case Judgement Summarization?.](http://arxiv.org/abs/2306.01248) | 这篇论文探讨了是否可以使用预训练的抽象模型和大型语言模型来自动生成法律案例判决的摘要，并在印度的法庭案例判决中进行了相关实验分析。 |
| [^13] | [Contextual Multilingual Spellchecker for User Queries.](http://arxiv.org/abs/2305.01082) | 本文提出了一个上下文多语种用户查询拼写检查器，它非常快速、可扩展，并根据特定产品的需求调整其词汇表和拼写输出，以满足用户的需求。 |
| [^14] | [LightGCL: Simple Yet Effective Graph Contrastive Learning for Recommendation.](http://arxiv.org/abs/2302.08191) | LightGCL是一种新的图对比学习方法，旨在解决现有方法中存在的不足。该模型采用奇异值分解进行对比增强，更好地保留了内在的语义结构，并提高了模型的通用性和鲁棒性。 |
| [^15] | [Debiasing Recommendation by Learning Identifiable Latent Confounders.](http://arxiv.org/abs/2302.05052) | 本研究提出了一种新方法 iDCF，通过学习可识别的混淆因素来消除推荐偏差，该方法在真实和合成数据集上的实验表明有效性和理论保证。 |
| [^16] | [RLTP: Reinforcement Learning to Pace for Delayed Impression Modeling in Preloaded Ads.](http://arxiv.org/abs/2302.02592) | RLTP算法是一个强化学习算法，用于解决广告预加载过程中的延迟印象现象。 |
| [^17] | [Using Interventions to Improve Out-of-Distribution Generalization of Text-Matching Recommendation Systems.](http://arxiv.org/abs/2210.10636) | 本文提出了使用干预方法来提高文本匹配推荐系统的跨领域泛化能力。研究发现，常用的基于精调模型的方法在具有新领域数据时有反效果，为此，提出了基于干预的重要性度量来解释泛化失败的原因。 |
| [^18] | [PLAtE: A Large-scale Dataset for List Page Web Extraction.](http://arxiv.org/abs/2205.12386) | 这个工作介绍了一个名为PLAtE的大规模列表页网络抽取数据集，用于从产品评论页面中提取商品列表和产品属性。数据集由52,898个项目和156,014个属性组成，是第一个大规模的列表页网络抽取数据集。 |
| [^19] | [Focusing on Potential Named Entities During Active Label Acquisition.](http://arxiv.org/abs/2111.03837) | 本文提出了几个AL句子查询评估函数，关注潜在正面标记，并使用更好的数据驱动的正常化方法，以最小化NER注释成本。 |

# 详细

[^1]: 从推特截图中提取信息

    Extracting Information from Twitter Screenshots. (arXiv:2306.08236v1 [cs.IR])

    [http://arxiv.org/abs/2306.08236](http://arxiv.org/abs/2306.08236)

    本研究提出了从推特截图中提取推文文本、时间戳和推特句柄的方法，旨在开发一个工具，利用实时网络和网络存档中的资源，评估一张推特截图中推文真实性的概率。

    

    在社交媒体上，截图是一种常见的信息分享方法。用户在分享截图之前很少验证其中的帖子是真实的还是虚假的。通过虚假截图进行的信息分享会严重导致社交媒体上的错误信息和误导信息传播。我们的终极目标是开发一个工具，可以通过在实时网络和网络存档中发现的资源，对推特的截图提供推文真实性的概率。本文提供了从推特截图中提取推文文本、时间戳和推特句柄的方法。

    Screenshots are prevalent on social media as a common approach for information sharing. Users rarely verify before sharing a screenshot whether the post it contains is fake or real. Information sharing through fake screenshots can be highly responsible for misinformation and disinformation spread on social media. Our ultimate goal is to develop a tool that could take a screenshot of a tweet and provide a probability that the tweet is real, using resources found on the live web and in web archives. This paper provides methods for extracting the tweet text, timestamp, and Twitter handle from a screenshot of a tweet.
    
[^2]: 构建基于语音的对话式推荐系统：数据集、潜在解决方案和前景探讨

    Towards Building Voice-based Conversational Recommender Systems: Datasets, Potential Solutions, and Prospects. (arXiv:2306.08219v1 [cs.IR])

    [http://arxiv.org/abs/2306.08219](http://arxiv.org/abs/2306.08219)

    本文调查了基于语音的对话式推荐系统的潜力，并为电子商务和电影领域创建了两个VCRSs基准数据集，以帮助实现自然、直观、便利和易于访问的用户与RSs互动方式。

    

    对话式推荐系统(CRSs)由于其通过交互式对话明确获取用户偏好和揭示推荐原因等自然优势而成为推荐系统领域的重要研究主题。然而，当前大多数CRSs都是基于文本的，这不太用户友好，并可能对某些用户造成挑战，例如那些视力受损或读写能力有限的用户。因此，本文首次研究了基于语音的CRS(VCRSs)的潜力，以在自然、直观、便利和易于访问的方式下改革用户与RSs互动的方式。为支持这样的研究，我们在电子商务和电影领域创建了两个VCRSs基准数据集，经过详尽的文献综述后发现了这种数据集的缺乏。具体而言，我们首先在实践中验证了创建这些数据集的好处和必要性。之后，我们将用户-物品交互数据转换为自然语言，形成了基于语音的数据集。

    Conversational recommender systems (CRSs) have become crucial emerging research topics in the field of RSs, thanks to their natural advantages of explicitly acquiring user preferences via interactive conversations and revealing the reasons behind recommendations. However, the majority of current CRSs are text-based, which is less user-friendly and may pose challenges for certain users, such as those with visual impairments or limited writing and reading abilities. Therefore, for the first time, this paper investigates the potential of voice-based CRS (VCRSs) to revolutionize the way users interact with RSs in a natural, intuitive, convenient, and accessible fashion. To support such studies, we create two VCRSs benchmark datasets in the e-commerce and movie domains, after realizing the lack of such datasets through an exhaustive literature review. Specifically, we first empirically verify the benefits and necessity of creating such datasets. Thereafter, we convert the user-item interact
    
[^3]: 在标签噪声下的图上学习

    Learning on Graphs under Label Noise. (arXiv:2306.08194v1 [cs.LG])

    [http://arxiv.org/abs/2306.08194](http://arxiv.org/abs/2306.08194)

    该论文介绍了一种新方法 CGNN，它利用一致性图神经网络和基于同质性假设的样本选择技术，在标签噪声的情况下对节点分类进行建模，实现过滤出噪声节点和增强节点表示的鲁棒性。

    

    图上的节点分类是一项重要的任务，具有广泛的应用，包括社交分析和异常检测。虽然图神经网络（GNN）在这项任务上已经取得了一些有希望的结果，但目前的技术通常假设节点的标签信息是准确的，这在现实应用中可能并不成立。为了解决这个问题，我们研究了如何在标签噪声下的图上学习，并开发了一种名为一致性图神经网络（CGNN）的新方法来解决它。具体而言，我们采用了图对比学习作为正则化项，促进增强节点的两个视角具有一致的表示。由于这个正则化项不能利用标签信息，它可以增强节点表示对标签噪声的鲁棒性。此外，为了在图上检测噪声标签，我们提出了一种基于同质性假设的样本选择技术，通过测量嵌入和它们的邻居之间的一致性来识别噪声节点。各种真实世界数据集上的实验结果表明，CGNN可以有效地缓解标签噪声对节点分类的负面影响，并在对称和非对称标签噪声模型下优于最先进的基线。

    Node classification on graphs is a significant task with a wide range of applications, including social analysis and anomaly detection. Even though graph neural networks (GNNs) have produced promising results on this task, current techniques often presume that label information of nodes is accurate, which may not be the case in real-world applications. To tackle this issue, we investigate the problem of learning on graphs with label noise and develop a novel approach dubbed Consistent Graph Neural Network (CGNN) to solve it. Specifically, we employ graph contrastive learning as a regularization term, which promotes two views of augmented nodes to have consistent representations. Since this regularization term cannot utilize label information, it can enhance the robustness of node representations to label noise. Moreover, to detect noisy labels on the graph, we present a sample selection technique based on the homophily assumption, which identifies noisy nodes by measuring the consisten
    
[^4]: 基于用户意图的上下文字体推荐

    Contextual Font Recommendations based on User Intent. (arXiv:2306.08188v1 [cs.HC])

    [http://arxiv.org/abs/2306.08188](http://arxiv.org/abs/2306.08188)

    Adobe的20,000多种字体库是个选择恐惧症患者的噩梦，本文通过创造一个基于用户意图的系统来自动给用户提供合适的字体推荐，目前已被数百万 Adobe Express 用户使用，点击率高达 25% 以上。

    

    Adobe Fonts 拥有超过 20,000 种独特的字体库，用于创造图形、海报、复合物等。由于字体库的特殊性，选择合适的字体是一项需要大量经验的艰巨任务。对于大多数 Adobe 产品用户，特别是 Adobe Express 的普通用户，这通常意味着选择默认字体而不是使用丰富多样的可用字体。本研究创造了一个基于用户意图的系统，为用户提供上下文字体推荐，以协助他们的创作之旅。我们的系统接受多语言文本输入，并根据用户的意图推荐适当的字体。根据用户的资格，免费和付费字体的混合比例将会做出相应的调整。该功能目前已被数百万 Adobe Express 用户使用，点击率高达 25% 以上。

    Adobe Fonts has a rich library of over 20,000 unique fonts that Adobe users utilize for creating graphics, posters, composites etc. Due to the nature of the large library, knowing what font to select can be a daunting task that requires a lot of experience. For most users in Adobe products, especially casual users of Adobe Express, this often means choosing the default font instead of utilizing the rich and diverse fonts available. In this work, we create an intent-driven system to provide contextual font recommendations to users to aid in their creative journey. Our system takes in multilingual text input and recommends suitable fonts based on the user's intent. Based on user entitlements, the mix of free and paid fonts is adjusted. The feature is currently used by millions of Adobe Express users with a CTR of >25%.
    
[^5]: h2oGPT：民主化大语言模型

    h2oGPT: Democratizing Large Language Models. (arXiv:2306.08161v1 [cs.CL])

    [http://arxiv.org/abs/2306.08161](http://arxiv.org/abs/2306.08161)

    本文介绍了h2oGPT，这是一套开源代码库，用于创建和使用基于GPTs的大语言模型（LLMs），包括100％私有文档搜索。目标是创建真正开源的替代封闭源GPTs，提高人工智能的开发和可靠性。

    

    基于生成预训练变压器（GPTs），大语言模型（LLMs）如GPT-4因其在自然语言处理方面的现实应用而成为人工智能革命的一部分。然而，它们也带来了许多重大的风险，如存在有偏见、私人或有害文本和未经授权的版权材料。本文介绍了h2oGPT，这是一套开源代码库，用于创建和使用基于GPTs的大语言模型（LLMs）。该项目的目标是创建世界上最好的真正开源的替代封闭源GPTs。与开源社区合作，作为其一部分，我们开源了几个LLM，其参数从7亿到400亿，可在完全自由的Apache 2.0许可下商用。我们的发布包括使用自然语言的100％私有文档搜索。开源语言模型有助于促进人工智能的发展并使其更加可靠。

    Foundation Large Language Models (LLMs) such as GPT-4 represent a revolution in AI due to their real-world applications though natural language processing. However, they also pose many significant risks such as the presence of biased, private, or harmful text, and the unauthorized inclusion of copyrighted material.  We introduce h2oGPT, a suite of open-source code repositories for the creation and use of Large Language Models (LLMs) based on Generative Pretrained Transformers (GPTs). The goal of this project is to create the world's best truly open-source alternative to closed-source GPTs. In collaboration with and as part of the incredible and unstoppable open-source community, we open-source several fine-tuned h2oGPT models from 7 to 40 Billion parameters, ready for commercial use under fully permissive Apache 2.0 licenses. Included in our release is 100% private document search using natural language.  Open-source language models help boost AI development and make it more accessible
    
[^6]: 使用语义ID进行更好的泛化：推荐排名的案例研究

    Better Generalization with Semantic IDs: A case study in Ranking for Recommendations. (arXiv:2306.08121v1 [cs.IR])

    [http://arxiv.org/abs/2306.08121](http://arxiv.org/abs/2306.08121)

    本文提出使用语义ID解决推荐系统中的物品冷启动问题，这些ID是从内容嵌入中学习的，可以捕捉概念的层次关系，相较于完全消除ID特征的方法，语义ID能更好地提高推荐质量。

    

    在推荐模型中，训练好的物品表示是至关重要的。通常，一项商品会被分配一个唯一的随机生成的ID，并且通常会通过学习与随机ID值相对应的嵌入来表示。虽然这种方法被广泛使用，但在物品数量大且物品服从幂律分布的情况下——这是真实世界推荐系统的典型特征——会有一定局限性。这会导致物品冷启动问题，模型无法对尾部和以前未见过的物品进行可靠的推荐。完全消除这些ID特征及其学习的嵌入以解决冷启动问题会严重降低推荐质量。基于内容的物品嵌入更为可靠，但对于用户过去的物品交互序列来说，它们成本高且使用困难。本文中，我们使用语义ID来表示离散的物品，这些ID是通过使用RQ-VAE从内容嵌入中学习的，可以捕捉概念的层次关系。

    Training good representations for items is critical in recommender models. Typically, an item is assigned a unique randomly generated ID, and is commonly represented by learning an embedding corresponding to the value of the random ID. Although widely used, this approach have limitations when the number of items are large and items are power-law distributed -- typical characteristics of real-world recommendation systems. This leads to the item cold-start problem, where the model is unable to make reliable inferences for tail and previously unseen items. Removing these ID features and their learned embeddings altogether to combat cold-start issue severely degrades the recommendation quality. Content-based item embeddings are more reliable, but they are expensive to store and use, particularly for users' past item interaction sequence. In this paper, we use Semantic IDs, a compact discrete item representations learned from content embeddings using RQ-VAE that captures hierarchy of concep
    
[^7]: Mol-Instructions: 一个大规模生物分子指令数据集，为大语言模型提供支持

    Mol-Instructions: A Large-Scale Biomolecular Instruction Dataset for Large Language Models. (arXiv:2306.08018v1 [q-bio.QM])

    [http://arxiv.org/abs/2306.08018](http://arxiv.org/abs/2306.08018)

    Mol-Instructions是一个专门为生物分子领域设计的综合指令数据集，可以显著提高大语言模型在生物领域中的适应能力和认知敏锐度。

    

    大语言模型（LLM）以其卓越的任务处理能力和创新的输出，在许多领域推动了重大进展。然而，它们在生物分子研究等专业领域的熟练应用还受到限制。为了解决这个挑战，我们介绍了Mol-Instructions，这是一个经过精心策划、专门针对生物分子领域设计的综合指令数据集。Mol-Instructions由三个关键组成部分组成：分子导向指令、蛋白质导向指令和生物分子文本指令，每个部分都被策划用于增强LLM对生物分子特性和行为的理解和预测能力。通过对代表性LLM的广泛指令调整实验，我们强调了Mol-Instructions在增强大模型在生物分子研究复杂领域内的适应能力和认知敏锐度方面的潜力，从而促进生物分子领域的进一步发展。

    Large Language Models (LLMs), with their remarkable task-handling capabilities and innovative outputs, have catalyzed significant advancements across a spectrum of fields. However, their proficiency within specialized domains such as biomolecular studies remains limited. To address this challenge, we introduce Mol-Instructions, a meticulously curated, comprehensive instruction dataset expressly designed for the biomolecular realm. Mol-Instructions is composed of three pivotal components: molecule-oriented instructions, protein-oriented instructions, and biomolecular text instructions, each curated to enhance the understanding and prediction capabilities of LLMs concerning biomolecular features and behaviors. Through extensive instruction tuning experiments on the representative LLM, we underscore the potency of Mol-Instructions to enhance the adaptability and cognitive acuity of large models within the complex sphere of biomolecular studies, thereby promoting advancements in the biomol
    
[^8]: 深度学习在暗网活动分类中的应用

    Dark web activity classification using deep learning. (arXiv:2306.07980v1 [cs.IR])

    [http://arxiv.org/abs/2306.07980](http://arxiv.org/abs/2306.07980)

    本文阐述了对于识别和控制暗网非法活动的迫切需求，并提出了一种利用深度学习方法检索 .onion 扩展名的网站上相关图片的搜索引擎，该方法在测试中达到了94% 的准确率。

    

    本文强调了识别和控制暗网非法活动的迫切需要。作者提出了一种利用深度学习通过 .onion 扩展名的网站检索非法活动相关图片的新型搜索引擎。在名为 darkoob 的全面数据集的测试中，该方法达到了94% 的准确率。

    The present article highlights the pressing need for identifying and controlling illicit activities on the dark web. While only 4% of the information available on the internet is accessible through regular search engines, the deep web contains a plethora of information, including personal data and online accounts, that is not indexed by search engines. The dark web, which constitutes a subset of the deep web, is a notorious breeding ground for various illegal activities, such as drug trafficking, weapon sales, and money laundering. Against this backdrop, the authors propose a novel search engine that leverages deep learning to identify and extract relevant images related to illicit activities on the dark web. Specifically, the system can detect the titles of illegal activities on the dark web and retrieve pertinent images from websites with a .onion extension. The authors have collected a comprehensive dataset named darkoob and the proposed method achieves an accuracy of 94% on the tes
    
[^9]: 利用社交媒体属性增强关键词检测：基于IDF-LDA模型应用于新浪微博

    Utilizing Social Media Attributes for Enhanced Keyword Detection: An IDF-LDA Model Applied to Sina Weibo. (arXiv:2306.07978v1 [cs.CL])

    [http://arxiv.org/abs/2306.07978](http://arxiv.org/abs/2306.07978)

    本文提出了一种新的方法，将逆文档频率（IDF）和潜在狄利克雷分配（LDA）模型相结合，以更好地应对社交媒体数据的不同属性，通过基于点赞数、评论数和转发数等属性对每个文档的重要性进行加权，从而有效地过滤噪声并识别最相关的关键词。

    

    随着Twitter和微博等社交媒体的快速发展，从大量实时文本数据流中检测关键词已成为一个关键的问题。关键词检测问题旨在从海量文本数据中搜索重要信息以反映最重要的事件或话题。然而，社交媒体数据通常具有独特的特点：文档通常很短，语言口语化，并且数据很可能具有重要的时间模式。因此，从这些文本流中发现关键信息可能是具有挑战性的。在本文中，我们提出了一种新颖的方法来解决社交媒体中的关键词检测问题。我们的模型将逆文档频率（IDF）和潜在狄利克雷分配（LDA）模型结合起来，以更好地应对社交媒体数据的不同属性，如点赞数、评论数和转发数。通过基于这些属性对每个文档的重要性进行加权，我们的方法可以有效地过滤噪声并识别最相关的关键词。我们在中国流行的微博平台上测试了我们的模型，实验结果表明，我们的方法在精确度、召回率和F1得分方面优于基准模型。

    With the rapid development of social media such as Twitter and Weibo, detecting keywords from a huge volume of text data streams in real-time has become a critical problem. The keyword detection problem aims at searching important information from massive text data to reflect the most important events or topics. However, social media data usually has unique features: the documents are usually short, the language is colloquial, and the data is likely to have significant temporal patterns. Therefore, it could be challenging to discover critical information from these text streams. In this paper, we propose a novel method to address the keyword detection problem in social media. Our model combines the Inverse Document Frequency (IDF) and Latent Dirichlet Allocation (LDA) models to better cope with the distinct attributes of social media data, such as the number of likes, comments, and retweets. By weighting the importance of each document based on these attributes, our method can effectiv
    
[^10]: KuaiSAR: 一份统一的搜索与推荐数据集

    KuaiSAR: A Unified Search And Recommendation Dataset. (arXiv:2306.07705v1 [cs.IR])

    [http://arxiv.org/abs/2306.07705](http://arxiv.org/abs/2306.07705)

    这篇论文介绍了一份大规模、真实的数据集KuaiSAR，该数据集记录了快手短视频应用程序中真实的集成搜索和推荐行为。

    

    搜索和推荐服务的融合是像快手和抖音这样的在线内容平台的重要方面。S&R建模的整合是业界实践者采用的高度直观的方法。然而，由于缺乏公开可用的数据集，学术界在这个领域中进行的研究明显不足。因此，在学术界和产业界之间在这个领域进行研究的实践之间出现了实质性的差距。为了弥合这个差距，我们介绍了快手的一个领先短视频应用程序收集的集成搜索与推荐行为的大规模真实世界数据集KuaiSAR。与以前的数据集不同，KuaiSAR记录了真实用户的行为，每个行为的发生时间都被精确记录了。

    The confluence of Search and Recommendation services is a vital aspect of online content platforms like Kuaishou and TikTok. The integration of S&R modeling is a highly intuitive approach adopted by industry practitioners. However, there is a noticeable lack of research conducted in this area within the academia, primarily due to the absence of publicly available datasets. Consequently, a substantial gap has emerged between academia and industry regarding research endeavors in this field. To bridge this gap, we introduce the first large-scale, real-world dataset KuaiSAR of integrated Search And Recommendation behaviors collected from Kuaishou, a leading short-video app in China with over 300 million daily active users. Previous research in this field has predominantly employed publicly available datasets that are semi-synthetic and simulated, with artificially fabricated search behaviors. Distinct from previous datasets, KuaiSAR records genuine user behaviors, the occurrence of each in
    
[^11]: 在在线推荐系统中激励高质量内容

    Incentivizing High-Quality Content in Online Recommender Systems. (arXiv:2306.07479v1 [cs.GT])

    [http://arxiv.org/abs/2306.07479](http://arxiv.org/abs/2306.07479)

    本文研究了在线推荐系统中激励高质量内容的算法问题，经典的在线学习算法会激励生产者创建低质量的内容，但本文提出的一种算法通过惩罚低质量内容的创建者，成功地激励了生产者创造高质量的内容。

    

    对于像TikTok和YouTube这样的内容推荐系统，平台的决策算法塑造了内容生产者的激励，包括生产者在内容质量上投入多少努力。许多平台采用在线学习，这会产生跨时间的激励，因为今天生产的内容会影响未来内容的推荐。在本文中，我们研究了在线学习产生的激励，分析了在纳什均衡下生产的内容质量。我们发现，像Hedge和EXP3这样的经典在线学习算法会激励生产者创建低质量的内容。特别地，内容质量在学习率方面有上限，并且随着典型学习率进展而趋近于零。在这一负面结果的基础上，我们设计了一种不同的学习算法——基于惩罚创建低质量内容的生产者——正确激励生产者创建高质量内容。我们的算法依赖于新颖的策略性赌博机问题，并克服了在组合设置中应用对抗性技术的挑战。在模拟和真实数据的实验中，我们的算法成功地激励生产者创建高质量内容。

    For content recommender systems such as TikTok and YouTube, the platform's decision algorithm shapes the incentives of content producers, including how much effort the content producers invest in the quality of their content. Many platforms employ online learning, which creates intertemporal incentives, since content produced today affects recommendations of future content. In this paper, we study the incentives arising from online learning, analyzing the quality of content produced at a Nash equilibrium. We show that classical online learning algorithms, such as Hedge and EXP3, unfortunately incentivize producers to create low-quality content. In particular, the quality of content is upper bounded in terms of the learning rate and approaches zero for typical learning rate schedules. Motivated by this negative result, we design a different learning algorithm -- based on punishing producers who create low-quality content -- that correctly incentivizes producers to create high-quality co
    
[^12]: 预训练的抽象模型和LLMs在法律案例判决摘要中的应用准备情况？

    How Ready are Pre-trained Abstractive Models and LLMs for Legal Case Judgement Summarization?. (arXiv:2306.01248v1 [cs.CL])

    [http://arxiv.org/abs/2306.01248](http://arxiv.org/abs/2306.01248)

    这篇论文探讨了是否可以使用预训练的抽象模型和大型语言模型来自动生成法律案例判决的摘要，并在印度的法庭案例判决中进行了相关实验分析。

    

    自动摘要法律案例判决一直是采用抽取式摘要方法尝试解决的问题。然而，近年来，具有生成更自然和连贯摘要能力的抽象摘要模型受到越来越多的关注。现在已经有了专门用于法律领域的预训练抽象摘要模型。此外，众所周知，如ChatGPT这样的通用领域预训练大型语言模型(LLMs)能够生成高质量的文本，并具有文本摘要的能力。因此，值得问的是，这些模型是否已准备好用于自动生成案例判决的抽象摘要。为了探讨这个问题，我们将几种最先进的领域特定的抽象性摘要模型和通用领域的LLMs应用于印度法庭案例判决中，并检查所生成摘要的质量。除了摘要质量的标准度量，我们还检查了生成的摘要中可能存在的不一致性和虚构现象。

    Automatic summarization of legal case judgements has traditionally been attempted by using extractive summarization methods. However, in recent years, abstractive summarization models are gaining popularity since they can generate more natural and coherent summaries. Legal domain-specific pre-trained abstractive summarization models are now available. Moreover, general-domain pre-trained Large Language Models (LLMs), such as ChatGPT, are known to generate high-quality text and have the capacity for text summarization. Hence it is natural to ask if these models are ready for off-the-shelf application to automatically generate abstractive summaries for case judgements. To explore this question, we apply several state-of-the-art domain-specific abstractive summarization models and general-domain LLMs on Indian court case judgements, and check the quality of the generated summaries. In addition to standard metrics for summary quality, we check for inconsistencies and hallucinations in the 
    
[^13]: 上下文多语种用户查询拼写检查器

    Contextual Multilingual Spellchecker for User Queries. (arXiv:2305.01082v1 [cs.CL])

    [http://arxiv.org/abs/2305.01082](http://arxiv.org/abs/2305.01082)

    本文提出了一个上下文多语种用户查询拼写检查器，它非常快速、可扩展，并根据特定产品的需求调整其词汇表和拼写输出，以满足用户的需求。

    

    拼写检查是最基本和广泛使用的搜索功能之一。纠正拼写错误的用户查询不仅增强了用户体验，而且用户也期望能够实现。然而，大多数广泛可用的拼写检查解决方案要么比最新的解决方案精度低，要么速度太慢，无法用于延迟是关键要求的搜索用例。此外，大多数最新的创新架构集中在英语上，并且没有以多语言方式进行培训，并且是针对较长文本的拼写纠正进行培训，这是与对用户查询的拼写纠正不同的范式，其中上下文很少(大多数查询只有1-2个单词)。最后，由于大多数企业有独特的词汇，例如产品名称，现成的拼写解决方案无法满足用户的需求。在这项工作中，我们构建了一个多语言拼写检查器，它非常快速和可扩展，并根据特定产品的需求调整其词汇表和拼写输出。

    Spellchecking is one of the most fundamental and widely used search features. Correcting incorrectly spelled user queries not only enhances the user experience but is expected by the user. However, most widely available spellchecking solutions are either lower accuracy than state-of-the-art solutions or too slow to be used for search use cases where latency is a key requirement. Furthermore, most innovative recent architectures focus on English and are not trained in a multilingual fashion and are trained for spell correction in longer text, which is a different paradigm from spell correction for user queries, where context is sparse (most queries are 1-2 words long). Finally, since most enterprises have unique vocabularies such as product names, off-the-shelf spelling solutions fall short of users' needs. In this work, we build a multilingual spellchecker that is extremely fast and scalable and that adapts its vocabulary and hence speller output based on a specific product's needs. Fu
    
[^14]: LightGCL: 简单而有效的用于推荐的图对比学习

    LightGCL: Simple Yet Effective Graph Contrastive Learning for Recommendation. (arXiv:2302.08191v3 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2302.08191](http://arxiv.org/abs/2302.08191)

    LightGCL是一种新的图对比学习方法，旨在解决现有方法中存在的不足。该模型采用奇异值分解进行对比增强，更好地保留了内在的语义结构，并提高了模型的通用性和鲁棒性。

    

    图神经网络是一种强大的基于图的推荐系统学习方法。最近，将对比学习与GNN结合在推荐系统中使用，在处理高度稀疏的数据方面采取数据增强方案，已经显示出超越其他方法的性能。尽管在其成功的基础上，现有的图对比学习方法大多要么在用户-物品交互图上执行随机扰动(例如节点/边扰动)，要么依赖于启发式的增强技术(例如用户聚类)来生成对比视图。本文认为，这些方法不能很好地保持内在的语义结构，并且很容易受到噪音扰动的影响。为此，本文提出了一种称为LightGCL的简单而有效的图对比学习范式，解决了对比学习模型因噪音而失去通用性和鲁棒性的问题。我们的模型仅使用奇异值分解进行对比增强，使其更好地保留了内在的语义结构。

    Graph neural network (GNN) is a powerful learning approach for graph-based recommender systems. Recently, GNNs integrated with contrastive learning have shown superior performance in recommendation with their data augmentation schemes, aiming at dealing with highly sparse data. Despite their success, most existing graph contrastive learning methods either perform stochastic augmentation (e.g., node/edge perturbation) on the user-item interaction graph, or rely on the heuristic-based augmentation techniques (e.g., user clustering) for generating contrastive views. We argue that these methods cannot well preserve the intrinsic semantic structures and are easily biased by the noise perturbation. In this paper, we propose a simple yet effective graph contrastive learning paradigm LightGCL that mitigates these issues impairing the generality and robustness of CL-based recommenders. Our model exclusively utilizes singular value decomposition for contrastive augmentation, which enables the un
    
[^15]: 通过学习可识别的潜在混淆因素消除推荐偏差

    Debiasing Recommendation by Learning Identifiable Latent Confounders. (arXiv:2302.05052v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.05052](http://arxiv.org/abs/2302.05052)

    本研究提出了一种新方法 iDCF，通过学习可识别的混淆因素来消除推荐偏差，该方法在真实和合成数据集上的实验表明有效性和理论保证。

    

    推荐系统旨在预测用户对未被曝光的物品的反馈。混淆偏差是由于存在未测量的变量（例如，用户的社会经济状况）可能会影响用户的曝光和反馈。现有的方法要么对这些未测量变量做出不可行的假设，要么直接推断用户的潜在混淆因素。然而，它们无法保证识别出反事实的反馈，这可能导致预测带有偏见。在本文中，我们提出了一种新方法，即可识别的去混淆（iDCF），它利用一组代理变量（例如，观察到的用户特征）来解决上述的非识别问题。所提出的iDCF是一个通用的去混淆的推荐框架，它应用近端因果推断来推断未测量的混淆因素并识别反事实的反馈，具有理论保证。在各种真实世界和合成数据集上进行的广泛实验证明了我们所提出的方法在减少混淆偏差和提高推荐准确性方面的有效性。

    Recommendation systems aim to predict users' feedback on items not exposed to them.  Confounding bias arises due to the presence of unmeasured variables (e.g., the socio-economic status of a user) that can affect both a user's exposure and feedback. Existing methods either (1) make untenable assumptions about these unmeasured variables or (2) directly infer latent confounders from users' exposure. However, they cannot guarantee the identification of counterfactual feedback, which can lead to biased predictions. In this work, we propose a novel method, i.e., identifiable deconfounder (iDCF), which leverages a set of proxy variables (e.g., observed user features) to resolve the aforementioned non-identification issue. The proposed iDCF is a general deconfounded recommendation framework that applies proximal causal inference to infer the unmeasured confounders and identify the counterfactual feedback with theoretical guarantees. Extensive experiments on various real-world and synthetic da
    
[^16]: RLTP算法：用于预加载广告中的延迟印象建模的强化学习算法

    RLTP: Reinforcement Learning to Pace for Delayed Impression Modeling in Preloaded Ads. (arXiv:2302.02592v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2302.02592](http://arxiv.org/abs/2302.02592)

    RLTP算法是一个强化学习算法，用于解决广告预加载过程中的延迟印象现象。

    

    为了增加品牌知名度，许多广告商与广告平台签订合同购买广告流量，然后将广告投放到目标受众中。在整个广告投放期间，广告商通常希望广告获得特定的印象数，并期望广告展示的效果越好越好（如高点击率）。广告平台通过实时调整流量请求的选择概率来满足需求。然而，发布者的策略也会影响广告投放过程，这是广告平台无法控制的。预加载是许多类型广告（如视频广告）的常用策略，以确保在流量请求后显示的响应时间是合理的，这将导致延迟印象现象。传统的配速算法无法很好地处理预加载的特性，因为它们依赖于即时反馈信号。

    To increase brand awareness, many advertisers conclude contracts with advertising platforms to purchase traffic and then deliver advertisements to target audiences. In a whole delivery period, advertisers usually desire a certain impression count for the ads, and they also expect that the delivery performance is as good as possible (e.g., obtaining high click-through rate). Advertising platforms employ pacing algorithms to satisfy the demands via adjusting the selection probabilities to traffic requests in real-time. However, the delivery procedure is also affected by the strategies from publishers, which cannot be controlled by advertising platforms. Preloading is a widely used strategy for many types of ads (e.g., video ads) to make sure that the response time for displaying after a traffic request is legitimate, which results in delayed impression phenomenon. Traditional pacing algorithms cannot handle the preloading nature well because they rely on immediate feedback signals, and m
    
[^17]: 使用干预方法提高文本匹配推荐系统的跨领域泛化能力

    Using Interventions to Improve Out-of-Distribution Generalization of Text-Matching Recommendation Systems. (arXiv:2210.10636v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2210.10636](http://arxiv.org/abs/2210.10636)

    本文提出了使用干预方法来提高文本匹配推荐系统的跨领域泛化能力。研究发现，常用的基于精调模型的方法在具有新领域数据时有反效果，为此，提出了基于干预的重要性度量来解释泛化失败的原因。

    

    给定用户的输入文本，文本匹配推荐系统通过将输入文本与可用商品的描述进行比较来输出相关商品，例如在电子商务平台上的商品推荐。由于用户的兴趣和物品库存预计会发生变化，因此文本匹配系统具有泛化至数据变化的能力，这是一项称为跨领域（OOD）泛化的任务。然而，我们发现，精调大型基础语言模型相对于已配对的商品相关数据（例如用户点击）的流行方法可能对OOD泛化具有反效果。对于商品推荐任务，在推荐新类别或未来时间段的商品时，微调获得的准确性比基础模型更差。为了解释这种泛化失败，我们考虑了基于干预的重要性指标，该指标显示微调模型捕捉了虚假相关性，并未学习确定任何两个文本之间相关性的因果特征。

    Given a user's input text, text-matching recommender systems output relevant items by comparing the input text to available items' description, such as product-to-product recommendation on e-commerce platforms. As users' interests and item inventory are expected to change, it is important for a text-matching system to generalize to data shifts, a task known as out-of-distribution (OOD) generalization. However, we find that the popular approach of fine-tuning a large, base language model on paired item relevance data (e.g., user clicks) can be counter-productive for OOD generalization. For a product recommendation task, fine-tuning obtains worse accuracy than the base model when recommending items in a new category or for a future time period. To explain this generalization failure, we consider an intervention-based importance metric, which shows that a fine-tuned model captures spurious correlations and fails to learn the causal features that determine the relevance between any two tex
    
[^18]: PLAtE: 一个大规模的列表页网络抽取数据集

    PLAtE: A Large-scale Dataset for List Page Web Extraction. (arXiv:2205.12386v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2205.12386](http://arxiv.org/abs/2205.12386)

    这个工作介绍了一个名为PLAtE的大规模列表页网络抽取数据集，用于从产品评论页面中提取商品列表和产品属性。数据集由52,898个项目和156,014个属性组成，是第一个大规模的列表页网络抽取数据集。

    

    最近，神经模型被利用来显著提高从半结构化网站中提取信息的性能。然而，继续进步的障碍是训练这些模型的数据集数量太少。在这项工作中，我们介绍了 PLAtE （Pages of Lists Attribute Extraction）基准数据集作为一个具有挑战性的新网络抽取任务。PLAtE 主要关注购物数据，特别是从包含多个项目的产品评论页面中提取，包含两个任务：（1）查找产品列表分割边界和（2）提取每个产品的属性。PLAtE由来自6,694个页面的52,898个项目和156,014个属性组成，是第一个大规模的列表页网络抽取数据集。我们使用多阶段方法来收集和注释数据集，并将三个最先进的网络抽取模型适应于两个任务，定量和定性比较它们的优缺点。

    Recently, neural models have been leveraged to significantly improve the performance of information extraction from semi-structured websites. However, a barrier for continued progress is the small number of datasets large enough to train these models. In this work, we introduce the PLAtE (Pages of Lists Attribute Extraction) benchmark dataset as a challenging new web extraction task. PLAtE focuses on shopping data, specifically extractions from product review pages with multiple items encompassing the tasks of: (1) finding product-list segmentation boundaries and (2) extracting attributes for each product. PLAtE is composed of 52, 898 items collected from 6, 694 pages and 156, 014 attributes, making it the first largescale list page web extraction dataset. We use a multi-stage approach to collect and annotate the dataset and adapt three state-of-the-art web extraction models to the two tasks comparing their strengths and weaknesses both quantitatively and qualitatively.
    
[^19]: 集中关注潜在命名实体的主动标注获取

    Focusing on Potential Named Entities During Active Label Acquisition. (arXiv:2111.03837v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2111.03837](http://arxiv.org/abs/2111.03837)

    本文提出了几个AL句子查询评估函数，关注潜在正面标记，并使用更好的数据驱动的正常化方法，以最小化NER注释成本。

    

    命名实体识别(NER)旨在识别结构化文本中命名实体的提及并将其分类到预定义的命名实体类别中。虽然基于深度学习的预训练语言模型有助于在NER中实现良好的预测性能，但许多特定领域的NER应用仍需要大量标记数据。主动学习(AL)是解决标签获取问题的通用框架，已用于NER任务，以最小化注释成本而不牺牲模型性能。然而，标记的严重不均匀类分布引入了设计有效的NER主动学习查询方法的挑战。我们提出了几个AL句子查询评估函数，更多关注潜在的正面标记，并使用基于句子和标记成本评估策略来评估这些提议的函数。我们还提出了更好的数据驱动的正常化方法，以惩罚过长或过短的句子。

    Named entity recognition (NER) aims to identify mentions of named entities in an unstructured text and classify them into predefined named entity classes. While deep learning-based pre-trained language models help to achieve good predictive performances in NER, many domain-specific NER applications still call for a substantial amount of labeled data. Active learning (AL), a general framework for the label acquisition problem, has been used for NER tasks to minimize the annotation cost without sacrificing model performance. However, the heavily imbalanced class distribution of tokens introduces challenges in designing effective AL querying methods for NER. We propose several AL sentence query evaluation functions that pay more attention to potential positive tokens, and evaluate these proposed functions with both sentence-based and token-based cost evaluation strategies. We also propose a better data-driven normalization approach to penalize sentences that are too long or too short. Our
    

