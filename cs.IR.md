# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Automating the Analysis of Institutional Design in International Agreements.](http://arxiv.org/abs/2305.16750) | 本文研究了从国际协议中自动化提取正式制度设计的知识的方法，并通过对于《无形文化遗产保护公约》的测试分析了正式制度设计中参与者的可见性和重要性之间的关系。 |
| [^2] | [The Search for Stability: Learning Dynamics of Strategic Publishers with Initial Documents.](http://arxiv.org/abs/2305.16695) | 本研究在信息检索博弈论模型中提出了相对排名原则（RRP）作为替代排名原则，以达成更稳定的搜索生态系统，并提供了理论和实证证据证明其学习动力学收敛性，同时展示了可能的出版商-用户权衡。 |
| [^3] | [Multiview Identifiers Enhanced Generative Retrieval.](http://arxiv.org/abs/2305.16675) | 该论文提出了一种新型的基于合成标识符的多视角标识符来增强生成式检索，从而提高了检索结果的准确性和多样性。 |
| [^4] | [FARA: Future-aware Ranking Algorithm for Fairness Optimization.](http://arxiv.org/abs/2305.16637) | FARA是一种未来感知的公平优化排名算法，可以通过联合优化多个排名列表并将其保存到未来的会话中，同时最小化公平性和相关性差异。实验证明，FARA在排名相关性和公平性方面优于现有算法。 |
| [^5] | [DataFinder: Scientific Dataset Recommendation from Natural Language Descriptions.](http://arxiv.org/abs/2305.16636) | DataFinder能够根据自然语言描述推荐相关数据集，解决科学家在现有数据集中寻找合适数据集的困难。 |
| [^6] | [CARAMEL: A Succinct Read-Only Lookup Table via Compressed Static Functions.](http://arxiv.org/abs/2305.16545) | CARAMEL是一种基于静态函数构建技术的空间高效的只读查找表，专门用于处理值是多集合的情况。 |
| [^7] | [Graph-Based Model-Agnostic Data Subsampling for Recommendation Systems.](http://arxiv.org/abs/2305.16391) | 本文提出了一种基于图结构的无模型数据子采样方法，通过研究用户-物品图的拓扑结构来估计每个用户-物品交互的重要性，并在网络上进行传播来平滑估计值。该方法结合了无模型和基于模型的子采样方法的优点，在多个基准数据集上表现出较好的实验结果。 |
| [^8] | [Large language models in biomedical natural language processing: benchmarks, baselines, and recommendations.](http://arxiv.org/abs/2305.16326) | 本文研究了GPT-3和GPT-4在生物医学自然语言处理中的表现，分析了它们可能产生的错误类型，并提供了使用这些模型的建议。 |
| [^9] | [ConvGQR: Generative Query Reformulation for Conversational Search.](http://arxiv.org/abs/2305.15645) | 本文提出了一种新的面向会话搜索的ConvGQR框架，通过结合预训练语言模型来重新构造查询，从而提供更好的搜索查询。 |
| [^10] | [Representation Online Matters: Practical End-to-End Diversification in Search and Recommender Systems.](http://arxiv.org/abs/2305.15534) | 为了改善搜索和推荐系统中的代表性，我们提出了一种端到端的多样化方法，并在Pinterest平台上实验和部署了可扩展的多样化机制，以改善美容和时尚类别中不同肤色的代表性。 |
| [^11] | [PALR: Personalization Aware LLMs for Recommendation.](http://arxiv.org/abs/2305.07622) | 本文提出了一个称为PALR的框架，将用户的历史行为与LLMs相结合，生成用户喜欢的物品的推荐。与现有的推荐方法相比，我们的PALR框架实现了最先进的性能。 |
| [^12] | [Automated Data Denoising for Recommendation.](http://arxiv.org/abs/2305.07070) | 本文提出了一个自动数据去噪的推荐框架——AutoDenoise，利用显式数据作为验证集动态 guiding 推荐算法的训练，对隐式数据进行去噪处理，提高推荐系统的准确性。 |
| [^13] | [Boosting Zero-shot Cross-lingual Retrieval by Training on Artificially Code-Switched Data.](http://arxiv.org/abs/2305.05295) | 研究者提出训练排名模型的方法来提高跨语言检索的效率，该模型使用了人工代码切换的数据，并且实验表明在跨语言检索和多语言检索中会带来显著改进，在不影响单语检索的基础上，特别是对于远程语言之间的检索。 |
| [^14] | [Language Agnostic Multilingual Information Retrieval with Contrastive Learning.](http://arxiv.org/abs/2210.06633) | 该论文提出一种使用对比学习的技术，利用平行和非平行语料库来提高多语种信息检索的效果，仅使用英语IR训练数据和一些平行语料库即可在非英语数据上实现显著的检索性能改进。 |
| [^15] | [Everyone's Preference Changes Differently: Weighted Multi-Interest Retrieval Model.](http://arxiv.org/abs/2207.06652) | 本文提出了一种新的加权多兴趣检索模型（Multi-Interest Preference，MIP），通过利用深度学习方法为用户建立多个兴趣嵌入，并将用户在多个兴趣上的偏好进行建模，从而提高候选检索结果的查全率。 |

# 详细

[^1]: 自动化国际协议中制度设计分析

    Automating the Analysis of Institutional Design in International Agreements. (arXiv:2305.16750v1 [cs.CL])

    [http://arxiv.org/abs/2305.16750](http://arxiv.org/abs/2305.16750)

    本文研究了从国际协议中自动化提取正式制度设计的知识的方法，并通过对于《无形文化遗产保护公约》的测试分析了正式制度设计中参与者的可见性和重要性之间的关系。

    

    本文探讨如何自动地从国际协议中提取正式制度设计、规范、规则和参与者等知识。重点是分析规范文化遗产关系关键方面的正式制度设计中参与者的可见性和重要性之间的关系。所开发的工具采用了多种技术，如收集法律文件、用制度语法注释这些文件并使用图分析方法来探索正式制度设计。该系统对2003年《无形文化遗产保护公约》进行了测试。

    This paper explores the automatic knowledge extraction of formal institutional design - norms, rules, and actors - from international agreements. The focus was to analyze the relationship between the visibility and centrality of actors in the formal institutional design in regulating critical aspects of cultural heritage relations. The developed tool utilizes techniques such as collecting legal documents, annotating them with Institutional Grammar, and using graph analysis to explore the formal institutional design. The system was tested against the 2003 UNESCO Convention for the Safeguarding of the Intangible Cultural Heritage.
    
[^2]: 寻求稳定性：具有初始文件的战略出版商的学习动态的研究

    The Search for Stability: Learning Dynamics of Strategic Publishers with Initial Documents. (arXiv:2305.16695v1 [cs.GT])

    [http://arxiv.org/abs/2305.16695](http://arxiv.org/abs/2305.16695)

    本研究在信息检索博弈论模型中提出了相对排名原则（RRP）作为替代排名原则，以达成更稳定的搜索生态系统，并提供了理论和实证证据证明其学习动力学收敛性，同时展示了可能的出版商-用户权衡。

    

    我们研究了一种信息检索的博弈论模型，其中战略出版商旨在在保持原始文档完整性的同时最大化自己排名第一的机会。我们表明，常用的PRP排名方案导致环境不稳定，游戏经常无法达到纯纳什均衡。我们将相对排名原则（RRP）作为替代排名原则，并介绍两个排名函数，它们是RRP的实例。我们提供了理论和实证证据，表明这些方法导致稳定的搜索生态系统，通过提供关于学习动力学收敛的积极结果。我们还定义出版商和用户的福利，并展示了可能的出版商-用户权衡，突显了确定搜索引擎设计师应选择哪种排名函数的复杂性。

    We study a game-theoretic model of information retrieval, in which strategic publishers aim to maximize their chances of being ranked first by the search engine, while maintaining the integrity of their original documents. We show that the commonly used PRP ranking scheme results in an unstable environment where games often fail to reach pure Nash equilibrium. We propose the Relative Ranking Principle (RRP) as an alternative ranking principle, and introduce two ranking functions that are instances of the RRP. We provide both theoretical and empirical evidence that these methods lead to a stable search ecosystem, by providing positive results on the learning dynamics convergence. We also define the publishers' and users' welfare, and demonstrate a possible publisher-user trade-off, which highlights the complexity of determining which ranking function should be selected by the search engine designer.
    
[^3]: 多视角标识增强生成式检索

    Multiview Identifiers Enhanced Generative Retrieval. (arXiv:2305.16675v1 [cs.CL])

    [http://arxiv.org/abs/2305.16675](http://arxiv.org/abs/2305.16675)

    该论文提出了一种新型的基于合成标识符的多视角标识符来增强生成式检索，从而提高了检索结果的准确性和多样性。

    

    与其简单地将查询与现有段落匹配，生成式检索生成段落的标识符字符串作为检索目标。然而，这种标识符必须足够独特以代表一个段落。当前的方法使用数字ID或文本片段（如标题或子字符串）作为标识符。然而，这些标识符不能很好地覆盖一个段落的内容。因此，我们提出了一种新类型的标识符，即基于段落内容生成的合成标识符，可以整合文本片段缺乏的情境信息。此外，我们同时考虑多视角标识符，包括合成标识符、标题和子字符串。这些标识符的视角相互补充，有助于从多个角度综合排名段落。我们在三个公共数据集上进行了一系列实验，结果表明我们提出的方法在生成式检索中表现最佳。

    Instead of simply matching a query to pre-existing passages, generative retrieval generates identifier strings of passages as the retrieval target. At a cost, the identifier must be distinctive enough to represent a passage. Current approaches use either a numeric ID or a text piece (such as a title or substrings) as the identifier. However, these identifiers cannot cover a passage's content well. As such, we are motivated to propose a new type of identifier, synthetic identifiers, that are generated based on the content of a passage and could integrate contextualized information that text pieces lack. Furthermore, we simultaneously consider multiview identifiers, including synthetic identifiers, titles, and substrings. These views of identifiers complement each other and facilitate the holistic ranking of passages from multiple perspectives. We conduct a series of experiments on three public datasets, and the results indicate that our proposed approach performs the best in generative 
    
[^4]: FARA: 未来感知的公平优化排名算法

    FARA: Future-aware Ranking Algorithm for Fairness Optimization. (arXiv:2305.16637v1 [cs.IR])

    [http://arxiv.org/abs/2305.16637](http://arxiv.org/abs/2305.16637)

    FARA是一种未来感知的公平优化排名算法，可以通过联合优化多个排名列表并将其保存到未来的会话中，同时最小化公平性和相关性差异。实验证明，FARA在排名相关性和公平性方面优于现有算法。

    

    排名系统是现代信息检索应用（例如搜索引擎和推荐系统）的关键组件。除了与用户相关的排名相关性外，向项目提供者公平的曝光度也被认为是排名优化的重要因素。许多公平排名算法已被提出以联合优化排名相关性和公平性。然而，我们发现大多数现有的公平排名方法采用贪心算法，仅针对下一个即时会话或请求优化排名。正如本文所示，这种短视的范式可能限制排名优化的上限，并导致长期的次优性能。为此，我们提出了FARA，一种新颖的未来感知排名算法，用于排名相关性和公平优化。FARA不是贪婪地优化下一个会话的排名，而是通过联合优化多个排名列表并将它们保存到未来会话中，来提前规划。特别地，FARA旨在最小化排名列表的公平性和相关性差异，并考虑未来会话对当前排名的影响。我们在真实世界数据集上的实验证明，FARA在排名相关性和公平性方面优于现有的公平排名算法。

    Ranking systems are the key components of modern Information Retrieval (IR) applications, such as search engines and recommender systems. Besides the ranking relevance to users, the exposure fairness to item providers has also been considered an important factor in ranking optimization. Many fair ranking algorithms have been proposed to jointly optimize both ranking relevance and fairness. However, we find that most existing fair ranking methods adopt greedy algorithms that only optimize rankings for the next immediate session or request. As shown in this paper, such a myopic paradigm could limit the upper bound of ranking optimization and lead to suboptimal performance in the long term. To this end, we propose FARA, a novel Future-Aware Ranking Algorithm for ranking relevance and fairness optimization. Instead of greedily optimizing rankings for the next immediate session, FARA plans ahead by jointly optimizing multiple ranklists together and saving them for future sessions. Particula
    
[^5]: DataFinder: 从自然语言描述中推荐科学数据集

    DataFinder: Scientific Dataset Recommendation from Natural Language Descriptions. (arXiv:2305.16636v1 [cs.IR])

    [http://arxiv.org/abs/2305.16636](http://arxiv.org/abs/2305.16636)

    DataFinder能够根据自然语言描述推荐相关数据集，解决科学家在现有数据集中寻找合适数据集的困难。

    

    现代机器学习依赖于数据集来开发和验证研究想法。鉴于公开可用数据的增长，找到合适的数据集变得越来越困难。任何研究问题对能够回答这个问题的数据集的要求都有明确和隐含的限制，例如数据集大小、模态和领域。我们引入了一项新任务，即在给定一个研究想法的简短自然语言描述的情况下推荐相关数据集，以帮助人们找到符合他们需求的相关数据集。数据集推荐存在独特的信息检索问题，数据集很难直接索引进行搜索，也没有现成的语料库用于这个任务。为了实现这个任务，我们构建了DataFinder数据集，其中包括一个自动构建的较大训练集（17500个查询）和一个较小的专家注释的评估集（392个查询）。利用这些数据，我们比较了各种信息检索模型。

    Modern machine learning relies on datasets to develop and validate research ideas. Given the growth of publicly available data, finding the right dataset to use is increasingly difficult. Any research question imposes explicit and implicit constraints on how well a given dataset will enable researchers to answer this question, such as dataset size, modality, and domain. We introduce a new task of recommending relevant datasets given a short natural language description of a research idea, to help people find relevant datasets for their needs. Dataset recommendation poses unique challenges as an information retrieval problem; datasets are hard to directly index for search and there are no corpora readily available for this task. To operationalize this task, we build the DataFinder Dataset which consists of a larger automatically-constructed training set (17.5K queries) and a smaller expert-annotated evaluation set (392 queries). Using this data, we compare various information retrieval 
    
[^6]: CARAMEL：通过压缩静态函数实现的简洁只读查找表

    CARAMEL: A Succinct Read-Only Lookup Table via Compressed Static Functions. (arXiv:2305.16545v1 [cs.DS])

    [http://arxiv.org/abs/2305.16545](http://arxiv.org/abs/2305.16545)

    CARAMEL是一种基于静态函数构建技术的空间高效的只读查找表，专门用于处理值是多集合的情况。

    

    查找表是许多数据处理和系统应用程序中的基本结构。随着网络规模越来越大，这些应用程序经常需要压缩技术来支持快速随机O（1）查找压缩数据中的单个参数（即在RAM中进行块解压缩）。受最近静态函数构建技术的进展启发，我们提出了一种名为CARAMEL的空间高效的不可变键值数据表示，专门针对值是多集合的情况。

    Lookup tables are a fundamental structure in many data processing and systems applications. Examples include tokenized text in NLP, quantized embedding collections in recommendation systems, integer sketches for streaming data, and hash-based string representations in genomics. With the increasing size of web-scale data, such applications often require compression techniques that support fast random $O(1)$ lookup of individual parameters directly on the compressed data (i.e. without blockwise decompression in RAM). While the community has proposd a number of succinct data structures that support queries over compressed representations, these approaches do not fully leverage the low-entropy structure prevalent in real-world workloads to reduce space. Inspired by recent advances in static function construction techniques, we propose a space-efficient representation of immutable key-value data, called CARAMEL, specifically designed for the case where the values are multi-sets. By carefull
    
[^7]: 基于图的无模型数据子采样在推荐系统中的应用研究

    Graph-Based Model-Agnostic Data Subsampling for Recommendation Systems. (arXiv:2305.16391v1 [cs.IR])

    [http://arxiv.org/abs/2305.16391](http://arxiv.org/abs/2305.16391)

    本文提出了一种基于图结构的无模型数据子采样方法，通过研究用户-物品图的拓扑结构来估计每个用户-物品交互的重要性，并在网络上进行传播来平滑估计值。该方法结合了无模型和基于模型的子采样方法的优点，在多个基准数据集上表现出较好的实验结果。

    

    数据子采样广泛用于加速训练大规模推荐系统。大多数子采样方法是基于模型的，常常需要一个预训练的试验模型来通过样本难度等方式测量数据重要性。然而，当试验模型被错误指定时，基于模型的子采样方法将会恶化。鉴于试验模型的错误指定在真实的推荐系统中普遍存在，我们提出了基于数据结构，即图形来探索的无模型数据子采样方法。具体地，我们研究用户-物品图的拓扑结构，通过图导电性来估计每个用户-物品交互（即用户-物品图中的一条边）的重要性，并在网络上进行传播步骤，平滑估计的重要性值。由于我们提出的方法是无模型的，因此我们可以将无模型和基于模型的子采样方法的优点结合起来。我们的实证研究表明，将这两种方法组合使用，在多个基准数据集上均比任何单一方法都要好。

    Data subsampling is widely used to speed up the training of large-scale recommendation systems. Most subsampling methods are model-based and often require a pre-trained pilot model to measure data importance via e.g. sample hardness. However, when the pilot model is misspecified, model-based subsampling methods deteriorate. Since model misspecification is persistent in real recommendation systems, we instead propose model-agnostic data subsampling methods by only exploring input data structure represented by graphs. Specifically, we study the topology of the user-item graph to estimate the importance of each user-item interaction (an edge in the user-item graph) via graph conductance, followed by a propagation step on the network to smooth out the estimated importance value.  Since our proposed method is model-agnostic, we can marry the merits of both model-agnostic and model-based subsampling methods. Empirically, we show that combing the two consistently improves over any single meth
    
[^8]: 生物医学自然语言处理中的大型语言模型: 基准、基线和建议

    Large language models in biomedical natural language processing: benchmarks, baselines, and recommendations. (arXiv:2305.16326v1 [cs.CL])

    [http://arxiv.org/abs/2305.16326](http://arxiv.org/abs/2305.16326)

    本文研究了GPT-3和GPT-4在生物医学自然语言处理中的表现，分析了它们可能产生的错误类型，并提供了使用这些模型的建议。

    

    生物医学文献呈指数级增长，手动筛选和提取知识变得困难。自动从生物医学文献中提取信息的生物医学自然语言处理（BioNLP）技术有助于减轻这种负担。近年来，如GPT-3和GPT-4等大型语言模型（LLMs）因其卓越的性能而受到重视。但是，它们在BioNLP任务中的有效性以及对方法开发和下游用户的影响仍未得到研究。本研究（1）在四个应用程序中在八个BioNLP数据集中建立了GPT-3和GPT-4在零-shot和一-shot设置下的基准表现，包括命名实体识别，关系提取，多标签文档分类和语义相似性和推理；（2）审查了LLMs产生的错误，并将错误分为三种类型：缺失，不一致和不需要的人工内容；（3）提出了使用LLMs的建议。

    Biomedical literature is growing rapidly, making it challenging to curate and extract knowledge manually. Biomedical natural language processing (BioNLP) techniques that can automatically extract information from biomedical literature help alleviate this burden. Recently, large Language Models (LLMs), such as GPT-3 and GPT-4, have gained significant attention for their impressive performance. However, their effectiveness in BioNLP tasks and impact on method development and downstream users remain understudied. This pilot study (1) establishes the baseline performance of GPT-3 and GPT-4 at both zero-shot and one-shot settings in eight BioNLP datasets across four applications: named entity recognition, relation extraction, multi-label document classification, and semantic similarity and reasoning, (2) examines the errors produced by the LLMs and categorized the errors into three types: missingness, inconsistencies, and unwanted artificial content, and (3) provides suggestions for using L
    
[^9]: ConvGQR：面向会话搜索的生成式查询重构

    ConvGQR: Generative Query Reformulation for Conversational Search. (arXiv:2305.15645v1 [cs.IR])

    [http://arxiv.org/abs/2305.15645](http://arxiv.org/abs/2305.15645)

    本文提出了一种新的面向会话搜索的ConvGQR框架，通过结合预训练语言模型来重新构造查询，从而提供更好的搜索查询。

    

    在会话搜索中，用户当前搜索意图依赖于先前的对话历史。从整个对话上下文中确定一个良好的搜索查询是具有挑战性的。为避免查询编码器的昂贵重新训练，大部分现有方法尝试学习一个重写模型，通过模仿手动查询重写来去除当前查询的上下文。然而，手动重写的查询并不总是最好的搜索查询。训练重写模型会限制模型产生良好搜索查询的能力。本文提出一种新的框架ConvGQR，基于预训练语言模型（PLM），一个用于查询重写，另一个用于生成潜在答案，以重新构造会话查询。通过结合两者，ConvGQR可以提供更好的搜索查询。此外，为了将查询重构与检索性能联系起来，我们提出了一种基于特征选择的相似度分数模型，用于验证ConvGQR的有效性。

    In conversational search, the user's real search intent for the current turn is dependent on the previous conversation history. It is challenging to determine a good search query from the whole conversation context. To avoid the expensive re-training of the query encoder, most existing methods try to learn a rewriting model to de-contextualize the current query by mimicking the manual query rewriting. However, manually rewritten queries are not always the best search queries. Training a rewriting model on them would limit the model's ability to produce good search queries. Another useful hint is the potential answer to the question. In this paper, we propose ConvGQR, a new framework to reformulate conversational queries based on generative pre-trained language models (PLMs), one for query rewriting and another for generating potential answers. By combining both, ConvGQR can produce better search queries. In addition, to relate query reformulation to retrieval performance, we propose a 
    
[^10]: 在搜索和推荐系统中，在线表示很重要：实用的端到端多样化方法。

    Representation Online Matters: Practical End-to-End Diversification in Search and Recommender Systems. (arXiv:2305.15534v1 [cs.IR])

    [http://arxiv.org/abs/2305.15534](http://arxiv.org/abs/2305.15534)

    为了改善搜索和推荐系统中的代表性，我们提出了一种端到端的多样化方法，并在Pinterest平台上实验和部署了可扩展的多样化机制，以改善美容和时尚类别中不同肤色的代表性。

    

    随着在线平台在各个人口统计学中的使用不断增长，用户经常表达希望在内容中感受到自己的代表性。为了改善搜索结果和推荐中的代表性，我们引入了端到端的多样化方法，确保多样化内容在这些系统的各个阶段中流动，从检索到排序。我们在多个Pinterest平台的生产界面中开发、实验和部署可扩展的多样化机制，包括搜索、相关产品和新用户主页，以改善美容和时尚内容中不同肤色的代表性。生产系统中的多样化包括三个组成部分：确定会触发多样化的请求，在检索阶段确保从大型内容语料库中检索到多样化的内容，最后，在排名阶段以自我调整的方式平衡多样性和效用的权衡。我们的方法从使用Strong-O开始。

    As the use of online platforms continues to grow across all demographics, users often express a desire to feel represented in the content. To improve representation in search results and recommendations, we introduce end-to-end diversification, ensuring that diverse content flows throughout the various stages of these systems, from retrieval to ranking. We develop, experiment, and deploy scalable diversification mechanisms in multiple production surfaces on the Pinterest platform, including Search, Related Products, and New User Homefeed, to improve the representation of different skin tones in beauty and fashion content. Diversification in production systems includes three components: identifying requests that will trigger diversification, ensuring diverse content is retrieved from the large content corpus during the retrieval stage, and finally, balancing the diversity-utility trade-off in a self-adjusting manner in the ranking stage. Our approaches, which evolved from using Strong-O
    
[^11]: 个性化感知的推荐系统中的LMMs模型

    PALR: Personalization Aware LLMs for Recommendation. (arXiv:2305.07622v1 [cs.IR])

    [http://arxiv.org/abs/2305.07622](http://arxiv.org/abs/2305.07622)

    本文提出了一个称为PALR的框架，将用户的历史行为与LLMs相结合，生成用户喜欢的物品的推荐。与现有的推荐方法相比，我们的PALR框架实现了最先进的性能。

    

    大型语言模型(LLMs)由于其出色的性能而受到越来越多的关注。本文提出了一种新的框架PALR，将用户的历史行为与LLMs相结合，以生成用户喜欢的物品的推荐。我们首先使用用户/物品互动作为候选检索的指导，然后采用基于LLMs的排序模型生成推荐物品。实验结果表明，与现有的推荐方法相比，我们提出的PALR框架实现了最先进的性能。

    Large language models (LLMs) have recently received significant attention for their exceptional capabilities. Despite extensive efforts in developing general-purpose LLMs that can be utilized in various natural language processing (NLP) tasks, there has been less research exploring their potential in recommender systems. In this paper, we propose a novel framework, named PALR, which aiming to combine user history behaviors (such as clicks, purchases, ratings, etc.) with LLMs to generate user preferred items. Specifically, we first use user/item interactions as guidance for candidate retrieval. Then we adopt a LLM-based ranking model to generate recommended items. Unlike existing approaches that typically adopt general-purpose LLMs for zero/few-shot recommendation testing or training on small-sized language models (with less than 1 billion parameters), which cannot fully elicit LLMs' reasoning abilities and leverage rich item side parametric knowledge, we fine-tune a 7 billion parameter
    
[^12]: 自动数据去噪技术用于推荐系统的应用

    Automated Data Denoising for Recommendation. (arXiv:2305.07070v1 [cs.IR])

    [http://arxiv.org/abs/2305.07070](http://arxiv.org/abs/2305.07070)

    本文提出了一个自动数据去噪的推荐框架——AutoDenoise，利用显式数据作为验证集动态 guiding 推荐算法的训练，对隐式数据进行去噪处理，提高推荐系统的准确性。

    

    在现实世界中，大多数平台收集的反馈数据既有大规模、自然嘈杂的隐式反馈，也有小规模但高度相关的显式反馈。由于数据稀缺的问题，隐式反馈通常是训练推荐系统的默认选择，但这种数据可能非常嘈杂，因为用户行为的随机性和多样性。幸运的是，通过利用两种反馈的优势来弥补另一种的弱点，我们可以几乎不花费什么代价来缓解以上问题。本文提出了一个自动数据去噪的框架——AutoDenoise用于推荐系统，它使用少量显式数据作为验证集来指导推荐算法的训练。AutoDenoise受到课程学习（CL）的广义定义的启发，学会自动动态地进行课程学习，进而对数据进行去噪处理。

    In real-world scenarios, most platforms collect both large-scale, naturally noisy implicit feedback and small-scale yet highly relevant explicit feedback. Due to the issue of data sparsity, implicit feedback is often the default choice for training recommender systems (RS), however, such data could be very noisy due to the randomness and diversity of user behaviors. For instance, a large portion of clicks may not reflect true user preferences and many purchases may result in negative reviews or returns. Fortunately, by utilizing the strengths of both types of feedback to compensate for the weaknesses of the other, we can mitigate the above issue at almost no cost. In this work, we propose an Automated Data Denoising framework, \textbf{\textit{AutoDenoise}}, for recommendation, which uses a small number of explicit data as validation set to guide the recommender training. Inspired by the generalized definition of curriculum learning (CL), AutoDenoise learns to automatically and dynamica
    
[^13]: 通过训练人工代码切换数据来提升零样本跨语言检索

    Boosting Zero-shot Cross-lingual Retrieval by Training on Artificially Code-Switched Data. (arXiv:2305.05295v1 [cs.CL])

    [http://arxiv.org/abs/2305.05295](http://arxiv.org/abs/2305.05295)

    研究者提出训练排名模型的方法来提高跨语言检索的效率，该模型使用了人工代码切换的数据，并且实验表明在跨语言检索和多语言检索中会带来显著改进，在不影响单语检索的基础上，特别是对于远程语言之间的检索。

    

    将以英语为代表的高资源语言的信息检索（IR）模型以零样本方式迁移到其他语言已成为被广泛采用的方法。在本研究中，我们表明当查询和文档以不同语言存在时，零样本排名器的有效性会降低。出于这个原因，我们建议使用人工代码切换数据来训练排名模型，而我们生成这些数据是通过利用双语词表。为此，我们尝试了从（1）跨语言词嵌入和（2）平行维基百科页面标题得出的词表。我们使用mMARCO数据集对涵盖单语IR（MoIR）、跨语言IR（CLIR）和多语言IR（MLIR）的36种语言对的重排模型进行了广泛评估。我们的结果表明，代码切换可以在保持MoIR性能稳定的同时，在CLIR中产生5.1 MRR@10的一致和显著增益，以及在MLIR中产生3.9 MRR@10的增益。令人鼓舞的是，远程语言之间的增益特别显著。

    Transferring information retrieval (IR) models from a high-resource language (typically English) to other languages in a zero-shot fashion has become a widely adopted approach. In this work, we show that the effectiveness of zero-shot rankers diminishes when queries and documents are present in different languages. Motivated by this, we propose to train ranking models on artificially code-switched data instead, which we generate by utilizing bilingual lexicons. To this end, we experiment with lexicons induced from (1) cross-lingual word embeddings and (2) parallel Wikipedia page titles. We use the mMARCO dataset to extensively evaluate reranking models on 36 language pairs spanning Monolingual IR (MoIR), Cross-lingual IR (CLIR), and Multilingual IR (MLIR). Our results show that code-switching can yield consistent and substantial gains of 5.1 MRR@10 in CLIR and 3.9 MRR@10 in MLIR, while maintaining stable performance in MoIR. Encouragingly, the gains are especially pronounced for distan
    
[^14]: 无关语言的多语种信息检索与对比学习

    Language Agnostic Multilingual Information Retrieval with Contrastive Learning. (arXiv:2210.06633v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2210.06633](http://arxiv.org/abs/2210.06633)

    该论文提出一种使用对比学习的技术，利用平行和非平行语料库来提高多语种信息检索的效果，仅使用英语IR训练数据和一些平行语料库即可在非英语数据上实现显著的检索性能改进。

    

    多语种信息检索具有挑战性，因为在许多语言中获取经过注释的训练数据成本很高。我们提出了一种有效的方法，在只有英语IR训练数据和英语与其他语言之间的一些平行语料库可用时训练多语种IR系统。我们利用平行和非平行语料库来提高预训练多语种语言模型的跨语言传递能力，并设计了一个语义对比损失，以对齐在不同语言中具有相同语义的平行句子的表示，以及一种新的语言对比损失，利用平行句子对从非平行语料库中的句子表示中删除语言特定信息。在使用这些损失对英语IR数据进行训练并在非英语数据上进行零-shot评估时，我们的模型表现出明显的改进，同时需要较少的计算资源。我们还证明了该方法的实用价值。

    Multilingual information retrieval (IR) is challenging since annotated training data is costly to obtain in many languages. We present an effective method to train multilingual IR systems when only English IR training data and some parallel corpora between English and other languages are available. We leverage parallel and non-parallel corpora to improve the pretrained multilingual language models' cross-lingual transfer ability. We design a semantic contrastive loss to align representations of parallel sentences that share the same semantics in different languages, and a new language contrastive loss to leverage parallel sentence pairs to remove language-specific information in sentence representations from non-parallel corpora. When trained on English IR data with these losses and evaluated zero-shot on non-English data, our model demonstrates significant improvement to prior work on retrieval performance, while it requires much less computational effort. We also demonstrate the valu
    
[^15]: 每个人的偏好变化不同：加权多兴趣检索模型

    Everyone's Preference Changes Differently: Weighted Multi-Interest Retrieval Model. (arXiv:2207.06652v4 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2207.06652](http://arxiv.org/abs/2207.06652)

    本文提出了一种新的加权多兴趣检索模型（Multi-Interest Preference，MIP），通过利用深度学习方法为用户建立多个兴趣嵌入，并将用户在多个兴趣上的偏好进行建模，从而提高候选检索结果的查全率。

    

    用户嵌入（用户的向量化表示）在推荐系统中具有重要作用。已经提出了许多方法来构建用户的多维度表示，以便于检索任务中找到相似的物品，并且已经在工业推荐系统中被证明是有效的。最近人们发现使用多种嵌入（即多个维度的用户表示）来表示用户的兴趣是有用的，每个嵌入表示用户的某个主题兴趣。对于多兴趣表示，重要的是对用户在不同主题上的偏好进行建模，并且了解偏好随时间的变化情况。然而，现有方法要么无法估算用户对每个兴趣的好感度，要么不合理地假设每个用户对每个兴趣的兴趣强度会以相等的速率下降，从而降低了候选检索结果的查全率。在本文中，我们提出加权多兴趣检索模型（Multi-Interest Preference, MIP），通过利用深度学习方法为用户产生多个兴趣嵌入，并且可以对用户在多种兴趣下的偏好进行估计，从而提高候选检索结果的查全率。

    User embeddings (vectorized representations of a user) are essential in recommendation systems. Numerous approaches have been proposed to construct a representation for the user in order to find similar items for retrieval tasks, and they have been proven effective in industrial recommendation systems as well. Recently people have discovered the power of using multiple embeddings to represent a user, with the hope that each embedding represents the user's interest in a certain topic. With multi-interest representation, it's important to model the user's preference over the different topics and how the preference change with time. However, existing approaches either fail to estimate the user's affinity to each interest or unreasonably assume every interest of every user fades with an equal rate with time, thus hurting the recall of candidate retrieval. In this paper, we propose the Multi-Interest Preference (MIP) model, an approach that not only produces multi-interest for users by usin
    

