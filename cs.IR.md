# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [MagicLens: Self-Supervised Image Retrieval with Open-Ended Instructions](https://arxiv.org/abs/2403.19651) | 本研究提出了MagicLens，一系列支持开放式指令的自监督图像检索模型，核心创新在于利用文本指令使得图像检索可以检索到比视觉相似性更丰富关系的图像。 |
| [^2] | [Croissant: A Metadata Format for ML-Ready Datasets](https://arxiv.org/abs/2403.19546) | Croissant是一种面向机器学习数据集的元数据格式，使数据集更易发现、可移植和互操作，有助于解决ML数据管理和负责任AI中的重要挑战。 |
| [^3] | [Breaking the Length Barrier: LLM-Enhanced CTR Prediction in Long Textual User Behaviors](https://arxiv.org/abs/2403.19347) | BAHE提出了行为聚合分层编码（BAHE）来增强LLM-based CTR建模的效率，通过解耦用户行为的编码与行为之间的交互。 |
| [^4] | [Intelligent Classification and Personalized Recommendation of E-commerce Products Based on Machine Learning](https://arxiv.org/abs/2403.19345) | 本论文比较了传统电子商务商品分类系统和个性化推荐系统的运作机制，阐述了个性化推荐系统在电子商务等领域中的重要性和应用，同时深入探讨了相关系统所面临的挑战。 |
| [^5] | [Generate then Retrieve: Conversational Response Retrieval Using LLMs as Answer and Query Generators](https://arxiv.org/abs/2403.19302) | 本文提出了一种利用大型语言模型生成多个查询以增强对话响应检索的方法，并基于不同LLMs实现评估，同时还提出了一个新的TREC iKAT基准。 |
| [^6] | [Enhanced Bayesian Personalized Ranking for Robust Hard Negative Sampling in Recommender Systems](https://arxiv.org/abs/2403.19276) | 该论文提出了一种增强的贝叶斯个性化排序目标Hard-BPR，专门为动态困难负例抽样而设计，以缓解假负例问题。 |
| [^7] | [Are Large Language Models Good at Utility Judgments?](https://arxiv.org/abs/2403.19216) | 大型语言模型（LLMs）在评估段落实用性方面的能力进行了全面研究，实验发现受过良好指导的LLMs可以... |
| [^8] | [Make Large Language Model a Better Ranker](https://arxiv.org/abs/2403.19181) | 本文介绍了一种具有对齐列表排名目标的语言模型框架（ALRO），旨在弥合大型语言模型的能力与推荐系统排名任务的要求之间的差距。 |
| [^9] | [Instruction-based Hypergraph Pretraining](https://arxiv.org/abs/2403.19063) | 引入基于指令的提示到图预训练中，通过文本指令提供具体任务的明确指导，以克服预训练和下游任务之间的差异。 |
| [^10] | [Towards LLM-RecSys Alignment with Textual ID Learning](https://arxiv.org/abs/2403.19021) | 通过提出IDGen，将每个推荐项目表示为独特、简洁、语义丰富的文本ID，从而使得基于大型语言模型的推荐更好地与自然语言生成对齐。 |
| [^11] | [High Recall, Small Data: The Challenges of Within-System Evaluation in a Live Legal Search System](https://arxiv.org/abs/2403.18962) | 高查准率、少数据环境下的现场法律搜索系统内评估面临挑战，尤其是在通常的评估方法可能不够理想的情况下。 |
| [^12] | [Directed Criteria Citation Recommendation and Ranking Through Link Prediction](https://arxiv.org/abs/2403.18855) | 该研究提出使用链接预测作为自动展示相关文献的方法，并通过模型生成的语义表示在推荐和排名任务上取得优越性能。 |
| [^13] | [Lightweight Embeddings for Graph Collaborative Filtering](https://arxiv.org/abs/2403.18479) | 提出了一种轻量级嵌入方法，通过自动学习用户/项目的索引到元嵌入之间的映射，提高了基于GNN的推荐系统的性能。 |
| [^14] | [Improving Pre-trained Language Model Sensitivity via Mask Specific losses: A case study on Biomedical NER](https://arxiv.org/abs/2403.18025) | 提出了Mask Specific Language Modeling（MSLM）方法来改善LM在微调过程中对目标领域知识的敏感性，通过加权领域特定术语的重要性进行学习。 |
| [^15] | [All-in-One: Heterogeneous Interaction Modeling for Cold-Start Rating Prediction](https://arxiv.org/abs/2403.17740) | 提出了异质交互评分网络（HIRE）框架，通过异质交互模块（HIM）来共同建模异质交互并直接推断重要特征 |
| [^16] | [CADGL: Context-Aware Deep Graph Learning for Predicting Drug-Drug Interactions](https://arxiv.org/abs/2403.17210) | 通过CADGL框架，利用上下文感知深度图学习来预测药物-药物相互作用，解决了现有DDI预测模型在泛化、特征提取和现实应用方面的挑战 |
| [^17] | [When SMILES have Language: Drug Classification using Text Classification Methods on Drug SMILES Strings](https://arxiv.org/abs/2403.12984) | 将药物SMILES字符串视为句子并利用文本分类方法进行药物分类，证实了通过简单的自然语言处理方法解决复杂问题的可能性 |
| [^18] | [Dual-Channel Multiplex Graph Neural Networks for Recommendation](https://arxiv.org/abs/2403.11624) | 该研究提出了一种名为双通道多重图神经网络（DCMGNN）的新型推荐框架，能够有效解决现有推荐方法中存在的多通路关系行为模式建模和对目标关系影响忽略的问题。 |
| [^19] | [Towards Unified Multi-Modal Personalization: Large Vision-Language Models for Generative Recommendation and Beyond](https://arxiv.org/abs/2403.10667) | 本文旨在建立一个统一的多模态个性化系统(UniMP)，有效利用多模态数据同时消除与任务和模态特定定制相关的复杂性。 |
| [^20] | [Can Small Language Models be Good Reasoners for Sequential Recommendation?](https://arxiv.org/abs/2403.04260) | 提出了逐步知识提取框架（SLIM），为顺序推荐系统解决了大型语言模型（LLMs）高资源需求的难题，使其能以资源高效的方式享受LLMs的出色推理能力。 |
| [^21] | [Do Similar Entities have Similar Embeddings?](https://arxiv.org/abs/2312.10370) | 本文挑战了实体相似性在图中在嵌入空间中自然反映的主流假设，通过进行广泛的实验来衡量这种关系。 |
| [^22] | [MILL: Mutual Verification with Large Language Models for Zero-Shot Query Expansion](https://arxiv.org/abs/2310.19056) | 该论文提出了一种利用大型语言模型进行相互验证的零-shot查询扩展框架，有效解决了查询扩展中已有方法的限制和缺陷。 |
| [^23] | [Query2GMM: Learning Representation with Gaussian Mixture Model for Reasoning over Knowledge Graphs](https://arxiv.org/abs/2306.10367) | 使用高斯混合模型学习表示，以更好地模拟知识图谱中具有多样化答案的逻辑查询 |
| [^24] | [Enhancing Recommender Systems: A Strategy to Mitigate False Negative Impact](https://arxiv.org/abs/2211.13912) | 本研究提出了一种新颖的负采样策略，即以正样本为主导的负样本合成，可以显著提升推荐系统的预测准确性 |
| [^25] | [Co-Designing Statistical MIMO Radar and In-band Full-Duplex Multi-User MIMO Communications -- Part I: Signal Processing](https://arxiv.org/abs/2006.14774) | 这项研究旨在共同设计一个MRMC框架，解决统计MIMO雷达和基频全双工多用户MIMO通信系统在相同频段内运行时遇到的各种问题。 |
| [^26] | [Large Language Models are Effective Text Rankers with Pairwise Ranking Prompting.](http://arxiv.org/abs/2306.17563) | 本论文提出了一种名为PRP的新技术，通过使用两两排名提示来显著减轻大型语言模型（LLM）的负担，并首次在标准基准测试中实现了最先进的排名性能。 |
| [^27] | [Visual Acuity Prediction on Real-Life Patient Data Using a Machine Learning Based Multistage System.](http://arxiv.org/abs/2204.11970) | 本研究提供了一种使用机器学习技术开发预测模型的多阶段系统，可高精度预测三种眼疾患者的视力变化，并辅助眼科医生进行临床决策和患者咨询。 |

# 详细

[^1]: MagicLens：自监督图像检索与开放式指令

    MagicLens: Self-Supervised Image Retrieval with Open-Ended Instructions

    [https://arxiv.org/abs/2403.19651](https://arxiv.org/abs/2403.19651)

    本研究提出了MagicLens，一系列支持开放式指令的自监督图像检索模型，核心创新在于利用文本指令使得图像检索可以检索到比视觉相似性更丰富关系的图像。

    

    图像检索，即根据参考图像查找所需图像，固有地包含难以仅使用基于图像的度量捕捉到的丰富、多方面的搜索意图。最近的工作利用文本指令允许用户更自由地表达他们的搜索意图。然而，现有工作主要集中在那些视觉上相似和/或可以用一小组预定义关系来表征的图像对上。本文的核心论点是文本指令可以使图像检索能够检索到比视觉相似性更丰富关系的图像。为了证明这一点，我们引入了MagicLens，一系列支持开放式指令的自监督图像检索模型。MagicLens建立在一个重要的新颖见解上：自然发生在同一网页上的图像对包含着大量隐式关系（例如，内部视图），我们可以通过综合指令将这些隐式关系变为显式。

    arXiv:2403.19651v1 Announce Type: cross  Abstract: Image retrieval, i.e., finding desired images given a reference image, inherently encompasses rich, multi-faceted search intents that are difficult to capture solely using image-based measures. Recent work leverages text instructions to allow users to more freely express their search intents. However, existing work primarily focuses on image pairs that are visually similar and/or can be characterized by a small set of pre-defined relations. The core thesis of this paper is that text instructions can enable retrieving images with richer relations beyond visual similarity. To show this, we introduce MagicLens, a series of self-supervised image retrieval models that support open-ended instructions. MagicLens is built on a key novel insight: image pairs that naturally occur on the same web pages contain a wide range of implicit relations (e.g., inside view of), and we can bring those implicit relations explicit by synthesizing instructions
    
[^2]: Croissant：一种面向机器学习数据集的元数据格式

    Croissant: A Metadata Format for ML-Ready Datasets

    [https://arxiv.org/abs/2403.19546](https://arxiv.org/abs/2403.19546)

    Croissant是一种面向机器学习数据集的元数据格式，使数据集更易发现、可移植和互操作，有助于解决ML数据管理和负责任AI中的重要挑战。

    

    数据是机器学习（ML）的关键资源，但处理数据仍然是一个主要的摩擦点。本文介绍了Croissant，一种用于数据集的元数据格式，简化了数据被ML工具和框架使用的方式。Croissant使数据集更易发现、可移植和互操作，从而解决了ML数据管理和负责任AI中的重要挑战。Croissant已得到几个流行数据集库的支持，涵盖数十万个数据集，可以加载到最流行的ML框架中。

    arXiv:2403.19546v1 Announce Type: cross  Abstract: Data is a critical resource for Machine Learning (ML), yet working with data remains a key friction point. This paper introduces Croissant, a metadata format for datasets that simplifies how data is used by ML tools and frameworks. Croissant makes datasets more discoverable, portable and interoperable, thereby addressing significant challenges in ML data management and responsible AI. Croissant is already supported by several popular dataset repositories, spanning hundreds of thousands of datasets, ready to be loaded into the most popular ML frameworks.
    
[^3]: 打破长度限制：LLM增强长文本用户行为中的CTR预测

    Breaking the Length Barrier: LLM-Enhanced CTR Prediction in Long Textual User Behaviors

    [https://arxiv.org/abs/2403.19347](https://arxiv.org/abs/2403.19347)

    BAHE提出了行为聚合分层编码（BAHE）来增强LLM-based CTR建模的效率，通过解耦用户行为的编码与行为之间的交互。

    

    随着大型语言模型（LLMs）的兴起，最近的研究利用LLMs提高了点击率（CTR）预测的性能。然而，我们认为在实际应用中部署LLMs仍然存在一个关键障碍：LLMs在处理长文本用户行为时的效率。随着用户序列变得更长，当前的LLMs效率不足以在数十亿用户和项目上进行训练。为了突破LLMs的效率障碍，我们提出了行为聚合分层编码（BAHE）来增强基于LLM的CTR建模的效率。具体地，BAHE提出了一种新颖的分层架构，将用户行为的编码与行为之间的交互解耦。首先，为了防止由于重复编码相同用户行为而产生的计算冗余，BAHE利用LLM的预训练浅层来提取最粒度的原子用户行为的嵌入。

    arXiv:2403.19347v1 Announce Type: cross  Abstract: With the rise of large language models (LLMs), recent works have leveraged LLMs to improve the performance of click-through rate (CTR) prediction. However, we argue that a critical obstacle remains in deploying LLMs for practical use: the efficiency of LLMs when processing long textual user behaviors. As user sequences grow longer, the current efficiency of LLMs is inadequate for training on billions of users and items. To break through the efficiency barrier of LLMs, we propose Behavior Aggregated Hierarchical Encoding (BAHE) to enhance the efficiency of LLM-based CTR modeling. Specifically, BAHE proposes a novel hierarchical architecture that decouples the encoding of user behaviors from inter-behavior interactions. Firstly, to prevent computational redundancy from repeated encoding of identical user behaviors, BAHE employs the LLM's pre-trained shallow layers to extract embeddings of the most granular, atomic user behaviors from ext
    
[^4]: 基于机器学习的智能电子商务产品分类与个性化推荐

    Intelligent Classification and Personalized Recommendation of E-commerce Products Based on Machine Learning

    [https://arxiv.org/abs/2403.19345](https://arxiv.org/abs/2403.19345)

    本论文比较了传统电子商务商品分类系统和个性化推荐系统的运作机制，阐述了个性化推荐系统在电子商务等领域中的重要性和应用，同时深入探讨了相关系统所面临的挑战。

    

    随着互联网的迅速发展和信息的指数级增长，用户遇到了信息过载和选择困境。个性化推荐系统通过帮助用户筛选和选择符合其偏好和需求的信息，从而在缓解这一负担方面发挥着关键作用。这些系统不仅提升了用户体验和满意度，还为企业和平台提供了增加用户参与度、销售额和广告效果的机会。本文对传统电子商务商品分类系统和个性化推荐系统的运作机制进行了比较分析，阐述了个性化推荐系统在电子商务、内容信息和媒体领域的重要性和应用。此外，还深入探讨了电子商务中个性化推荐系统所面临的挑战。

    arXiv:2403.19345v1 Announce Type: cross  Abstract: With the rapid evolution of the Internet and the exponential proliferation of information, users encounter information overload and the conundrum of choice. Personalized recommendation systems play a pivotal role in alleviating this burden by aiding users in filtering and selecting information tailored to their preferences and requirements. Such systems not only enhance user experience and satisfaction but also furnish opportunities for businesses and platforms to augment user engagement, sales, and advertising efficacy.This paper undertakes a comparative analysis between the operational mechanisms of traditional e-commerce commodity classification systems and personalized recommendation systems. It delineates the significance and application of personalized recommendation systems across e-commerce, content information, and media domains. Furthermore, it delves into the challenges confronting personalized recommendation systems in e-co
    
[^5]: 生成然后检索：使用LLM作为答案和查询生成器的对话响应检索

    Generate then Retrieve: Conversational Response Retrieval Using LLMs as Answer and Query Generators

    [https://arxiv.org/abs/2403.19302](https://arxiv.org/abs/2403.19302)

    本文提出了一种利用大型语言模型生成多个查询以增强对话响应检索的方法，并基于不同LLMs实现评估，同时还提出了一个新的TREC iKAT基准。

    

    CIS是信息检索中的一个重要领域，专注于开发交互式知识助手。这些系统必须能够熟练地理解用户在对话环境中的信息需求，并检索相关信息。为实现这一目标，现有方法使用一个称为重写查询的查询来建模用户的信息需求，并将此查询用于段落检索。在本文中，我们提出了三种用于生成多个查询以增强检索的不同方法。在这些方法中，我们利用大型语言模型（LLMs）的能力来理解用户的信息需求和生成适当的响应，以生成多个查询。我们实现并评估了提出的模型，利用包括GPT-4和Llama-2在零-shot和少-shot设置中的各种LLMs。此外，我们基于gpt 3.5的判断提出了一个针对TREC iKAT的新基准。我们的实验揭示了效果

    arXiv:2403.19302v1 Announce Type: new  Abstract: CIS is a prominent area in IR that focuses on developing interactive knowledge assistants. These systems must adeptly comprehend the user's information requirements within the conversational context and retrieve the relevant information. To this aim, the existing approaches model the user's information needs with one query called rewritten query and use this query for passage retrieval. In this paper, we propose three different methods for generating multiple queries to enhance the retrieval. In these methods, we leverage the capabilities of large language models (LLMs) in understanding the user's information need and generating an appropriate response, to generate multiple queries. We implement and evaluate the proposed models utilizing various LLMs including GPT-4 and Llama-2 chat in zero-shot and few-shot settings. In addition, we propose a new benchmark for TREC iKAT based on gpt 3.5 judgments. Our experiments reveal the effectivenes
    
[^6]: 加强的贝叶斯个性化排序用于推荐系统中的健壮负例抽样

    Enhanced Bayesian Personalized Ranking for Robust Hard Negative Sampling in Recommender Systems

    [https://arxiv.org/abs/2403.19276](https://arxiv.org/abs/2403.19276)

    该论文提出了一种增强的贝叶斯个性化排序目标Hard-BPR，专门为动态困难负例抽样而设计，以缓解假负例问题。

    

    在隐式协同过滤中，发展了困难的负例挖掘技术来加速和增强推荐模型的学习。然而，在困难负例抽样中，意外选择假负例仍然是一个主要问题，因为这些假负例可能提供不正确的信息并误导模型学习。迄今为止，只有少数研究致力于解决假负例问题，主要集中在设计复杂的抽样算法来过滤假负例。相比之下，本文将焦点转向了细化损失函数。我们发现最初设计用于均匀负例抽样的原始贝叶斯个性化排序（BPR）在适应困难抽样场景时是不足的。因此，我们引入了一个增强的贝叶斯个性化排序目标，命名为Hard-BPR，专门为动态困难负例抽样而设计，以减轻影响。

    arXiv:2403.19276v1 Announce Type: new  Abstract: In implicit collaborative filtering, hard negative mining techniques are developed to accelerate and enhance the recommendation model learning. However, the inadvertent selection of false negatives remains a major concern in hard negative sampling, as these false negatives can provide incorrect information and mislead the model learning. To date, only a small number of studies have been committed to solve the false negative problem, primarily focusing on designing sophisticated sampling algorithms to filter false negatives. In contrast, this paper shifts its focus to refining the loss function. We find that the original Bayesian Personalized Ranking (BPR), initially designed for uniform negative sampling, is inadequate in adapting to hard sampling scenarios. Hence, we introduce an enhanced Bayesian Personalized Ranking objective, named as Hard-BPR, which is specifically crafted for dynamic hard negative sampling to mitigate the influence
    
[^7]: 大型语言模型擅长实用性判断吗？

    Are Large Language Models Good at Utility Judgments?

    [https://arxiv.org/abs/2403.19216](https://arxiv.org/abs/2403.19216)

    大型语言模型（LLMs）在评估段落实用性方面的能力进行了全面研究，实验发现受过良好指导的LLMs可以...

    

    检索增强生成（RAG）被认为是缓解大型语言模型（LLMs）幻觉问题的一种有前途的方法，并且近期已受到研究人员的广泛关注。由于检索模型在语义理解上的局限性，RAG的成功在很大程度上取决于LLMs识别具有实用性的段落的能力。最近的研究探讨了LLMs评估检索中段落相关性的能力，但对评估支持问答的段落实用性的工作还很有限。在本工作中，我们进行了一项关于LLMs在开放域QA实用性评估方面能力的全面研究。具体而言，我们引入了一个基准测试程序和不同特征的候选段落集合，促进了与五个代表性LLMs的一系列实验。我们的实验证明：（i）受过良好指导的LLMs可以进行...

    arXiv:2403.19216v1 Announce Type: new  Abstract: Retrieval-augmented generation (RAG) is considered to be a promising approach to alleviate the hallucination issue of large language models (LLMs), and it has received widespread attention from researchers recently. Due to the limitation in the semantic understanding of retrieval models, the success of RAG heavily lies on the ability of LLMs to identify passages with utility. Recent efforts have explored the ability of LLMs to assess the relevance of passages in retrieval, but there has been limited work on evaluating the utility of passages in supporting question answering. In this work, we conduct a comprehensive study about the capabilities of LLMs in utility evaluation for open-domain QA. Specifically, we introduce a benchmarking procedure and collection of candidate passages with different characteristics, facilitating a series of experiments with five representative LLMs. Our experiments reveal that: (i) well-instructed LLMs can di
    
[^8]: 让大型语言模型成为更好的排名器

    Make Large Language Model a Better Ranker

    [https://arxiv.org/abs/2403.19181](https://arxiv.org/abs/2403.19181)

    本文介绍了一种具有对齐列表排名目标的语言模型框架（ALRO），旨在弥合大型语言模型的能力与推荐系统排名任务的要求之间的差距。

    

    大型语言模型（LLMs）的发展显著增强了各个领域的能力，导致推荐系统（RSs）概念和开发方式发生了转变。然而，现有研究主要集中在点对点和成对推荐范式上。这些方法在基于LLM的推荐器中效率低下，因为利用大型语言模型的计算成本很高。一些研究虽然深入研究了列表型方法，但在排名任务中表现不佳。这一不足归因于排名和语言生成目标之间的不匹配。为此，本文介绍了具有对齐列表排名目标的语言模型框架（ALRO）。ALRO旨在弥合LLMs的能力与推荐系统排名任务的微妙要求之间的差距。ALRO的一个关键特性是引入了软lambda值lo

    arXiv:2403.19181v1 Announce Type: cross  Abstract: The evolution of Large Language Models (LLMs) has significantly enhanced capabilities across various fields, leading to a paradigm shift in how Recommender Systems (RSs) are conceptualized and developed. However, existing research primarily focuses on point-wise and pair-wise recommendation paradigms. These approaches prove inefficient in LLM-based recommenders due to the high computational cost of utilizing Large Language Models. While some studies have delved into list-wise approaches, they fall short in ranking tasks. This shortfall is attributed to the misalignment between the objectives of ranking and language generation. To this end, this paper introduces the Language Model Framework with Aligned Listwise Ranking Objectives (ALRO). ALRO is designed to bridge the gap between the capabilities of LLMs and the nuanced requirements of ranking tasks within recommender systems. A key feature of ALRO is the introduction of soft lambda lo
    
[^9]: 基于指令的超图预训练

    Instruction-based Hypergraph Pretraining

    [https://arxiv.org/abs/2403.19063](https://arxiv.org/abs/2403.19063)

    引入基于指令的提示到图预训练中，通过文本指令提供具体任务的明确指导，以克服预训练和下游任务之间的差异。

    

    预训练被广泛探索，以增强图学习模型对从大型数据集传输知识至下游任务（如链接预测或分类）的适应能力。然而，在预训练目标之间的差距以及预训练和下游任务中数据分布的差异阻碍了预训练知识的转移。受预训练语言模型中广泛使用的基于指令的提示启发，我们将指令引入图预训练。在本文中，我们提出了一种名为基于指令的超图预训练的新型预训练框架。为了克服预训练和下游任务之间的差异，应用基于文本的指令来为表示学习中的特定任务提供明确指导。与可学习提示相比，其有效性取决于训练数据的质量和多样性，文本指令固有地蕴含了

    arXiv:2403.19063v1 Announce Type: new  Abstract: Pretraining has been widely explored to augment the adaptability of graph learning models to transfer knowledge from large datasets to a downstream task, such as link prediction or classification. However, the gap between training objectives and the discrepancy between data distributions in pretraining and downstream tasks hinders the transfer of the pretrained knowledge. Inspired by instruction-based prompts widely used in pretrained language models, we introduce instructions into graph pretraining. In this paper, we propose a novel pretraining framework named Instruction-based Hypergraph Pretraining. To overcome the discrepancy between pretraining and downstream tasks, text-based instructions are applied to provide explicit guidance on specific tasks for representation learning. Compared to learnable prompts, whose effectiveness depends on the quality and the diversity of training data, text-based instructions intrinsically encapsulate
    
[^10]: 朝向LLM-RecSys对齐与文本ID学习的方向

    Towards LLM-RecSys Alignment with Textual ID Learning

    [https://arxiv.org/abs/2403.19021](https://arxiv.org/abs/2403.19021)

    通过提出IDGen，将每个推荐项目表示为独特、简洁、语义丰富的文本ID，从而使得基于大型语言模型的推荐更好地与自然语言生成对齐。

    

    基于大型语言模型(LLMs)的生成式推荐已经将传统的基于排名的推荐方式转变为文本生成范例。然而，与固有操作人类词汇的标准NLP任务相反，目前生成式推荐领域的研究在如何在文本生成范式中以简洁而有意义的ID表示有效编码推荐项目方面存在困难。为了更好地对齐LLMs与推荐需求，我们提出了IDGen，使用人类语言标记将每个项目表示为独特、简洁、语义丰富、与平台无关的文本ID。这通过在基于LLM的推荐系统旁训练文本ID生成器来实现，使个性化推荐能够无缝集成到自然语言生成中。值得注意的是，由于用户历史记录以自然语言表达并与原始数据集解耦，我们的方法提出了潜在的

    arXiv:2403.19021v1 Announce Type: cross  Abstract: Generative recommendation based on Large Language Models (LLMs) have transformed the traditional ranking-based recommendation style into a text-to-text generation paradigm. However, in contrast to standard NLP tasks that inherently operate on human vocabulary, current research in generative recommendations struggles to effectively encode recommendation items within the text-to-text framework using concise yet meaningful ID representations. To better align LLMs with recommendation needs, we propose IDGen, representing each item as a unique, concise, semantically rich, platform-agnostic textual ID using human language tokens. This is achieved by training a textual ID generator alongside the LLM-based recommender, enabling seamless integration of personalized recommendations into natural language generation. Notably, as user history is expressed in natural language and decoupled from the original dataset, our approach suggests the potenti
    
[^11]: 高查准率、少数据：现场法律搜索系统中系统内评估的挑战

    High Recall, Small Data: The Challenges of Within-System Evaluation in a Live Legal Search System

    [https://arxiv.org/abs/2403.18962](https://arxiv.org/abs/2403.18962)

    高查准率、少数据环境下的现场法律搜索系统内评估面临挑战，尤其是在通常的评估方法可能不够理想的情况下。

    

    这篇论文阐明了法律信息检索（IR）常见排名评估方法面临的一些挑战。我们通过现场法律搜索系统的日志数据和两项用户研究展示了这些挑战。我们概述了法律IR的各个方面，以及这些方面对于期望中常见评估方法的挑战的影响：基于显式和隐式反馈的测试集、用户调查和A/B测试。接下来，我们使用现场商业法律搜索引擎的数据来阐明常见评估方法的挑战。我们专注于通过单个IR系统随时间连续更改文档排名的有效性监视方法。我们展示了法律IR系统特性与有限用户数据结合可能导致挑战，从而使讨论的常见评估方法不够理想。在我们的未来工作中，我们将专注于较少见的评估

    arXiv:2403.18962v1 Announce Type: new  Abstract: This paper illustrates some challenges of common ranking evaluation methods for legal information retrieval (IR). We show these challenges with log data from a live legal search system and two user studies. We provide an overview of aspects of legal IR, and the implications of these aspects for the expected challenges of common evaluation methods: test collections based on explicit and implicit feedback, user surveys, and A/B testing. Next, we illustrate the challenges of common evaluation methods using data from a live, commercial, legal search engine. We specifically focus on methods for monitoring the effectiveness of (continuous) changes to document ranking by a single IR system over time. We show how the combination of characteristics in legal IR systems and limited user data can lead to challenges that cause the common evaluation methods discussed to be sub-optimal. In our future work we will therefore focus on less common evaluati
    
[^12]: 通过链接预测进行定向标准引文推荐和排名

    Directed Criteria Citation Recommendation and Ranking Through Link Prediction

    [https://arxiv.org/abs/2403.18855](https://arxiv.org/abs/2403.18855)

    该研究提出使用链接预测作为自动展示相关文献的方法，并通过模型生成的语义表示在推荐和排名任务上取得优越性能。

    

    我们探讨使用链接预测作为自动展示与新文档在主题或上下文上可能相关的现有文献的代理。我们的模型使用基于Transformer的图嵌入来编码每个文档的含义，呈现为引文网络中的节点。我们展示了我们的模型生成的语义表示能够在推荐和排名任务中胜过其他基于内容的方法。这为探索引文图提供了一种整体方法，特别是在那些这些文档正确互相引用至关重要的领域，以便最小化任何不一致性的可能性。

    arXiv:2403.18855v1 Announce Type: cross  Abstract: We explore link prediction as a proxy for automatically surfacing documents from existing literature that might be topically or contextually relevant to a new document. Our model uses transformer-based graph embeddings to encode the meaning of each document, presented as a node within a citation network. We show that the semantic representations that our model generates can outperform other content-based methods in recommendation and ranking tasks. This provides a holistic approach to exploring citation graphs in domains where it is critical that these documents properly cite each other, so as to minimize the possibility of any inconsistencies
    
[^13]: 轻量级嵌入用于图协同过滤

    Lightweight Embeddings for Graph Collaborative Filtering

    [https://arxiv.org/abs/2403.18479](https://arxiv.org/abs/2403.18479)

    提出了一种轻量级嵌入方法，通过自动学习用户/项目的索引到元嵌入之间的映射，提高了基于GNN的推荐系统的性能。

    

    图神经网络（GNNs）目前是最有效的协同过滤方法之一。然而，由于使用嵌入表来表示每个用户/项目为不同的向量，基于GNN的推荐系统继承了参数效率低下的长期缺陷。为了解决这个问题，我们提出了一种轻量级嵌入方法，即在学习过程中自动学习用户/项目的索引到对应的元嵌入之间的映射。

    arXiv:2403.18479v1 Announce Type: new  Abstract: Graph neural networks (GNNs) are currently one of the most performant collaborative filtering methods. Meanwhile, owing to the use of an embedding table to represent each user/item as a distinct vector, GNN-based recommenders have inherited the long-standing defect of parameter inefficiency. As a common practice for scalable embeddings, parameter sharing enables the use of fewer embedding vectors (i.e., meta-embeddings). When assigning meta-embeddings, most existing methods are a heuristically designed, predefined mapping from each user's/item's ID to the corresponding meta-embedding indexes, thus simplifying the optimization problem into learning only the meta-embeddings. However, in the context of GNN-based collaborative filtering, such a fixed mapping omits the semantic correlations between entities that are evident in the user-item interaction graph, leading to suboptimal recommendation performance. To this end, we propose Lightweigh
    
[^14]: 通过特定掩码损失改善预训练语言模型的敏感性：以生物医学实体识别为例

    Improving Pre-trained Language Model Sensitivity via Mask Specific losses: A case study on Biomedical NER

    [https://arxiv.org/abs/2403.18025](https://arxiv.org/abs/2403.18025)

    提出了Mask Specific Language Modeling（MSLM）方法来改善LM在微调过程中对目标领域知识的敏感性，通过加权领域特定术语的重要性进行学习。

    

    将语言模型（LMs）调整到新领域通常通过在特定领域数据上微调预训练LM（PLM）来实现。微调将新知识引入LM，使它能够理解和有效执行目标域任务。然而，微调可能会无意中变得不够敏感，如果它忽视了源域和目标域之间的广泛差异（例如在词义上）。为了解决微调不敏感的问题，我们提出了Mask Specific Language Modeling（MSLM），一种通过在微调过程中适当加权领域特定术语（DS-terms）的重要性来有效获取目标领域知识的方法。MSLM同时屏蔽DS术语和通用词，然后通过确保LM受到更大惩罚来学习特定于掩码的损失。

    arXiv:2403.18025v1 Announce Type: cross  Abstract: Adapting language models (LMs) to novel domains is often achieved through fine-tuning a pre-trained LM (PLM) on domain-specific data. Fine-tuning introduces new knowledge into an LM, enabling it to comprehend and efficiently perform a target domain task. Fine-tuning can however be inadvertently insensitive if it ignores the wide array of disparities (e.g in word meaning) between source and target domains. For instance, words such as chronic and pressure may be treated lightly in social conversations, however, clinically, these words are usually an expression of concern. To address insensitive fine-tuning, we propose Mask Specific Language Modeling (MSLM), an approach that efficiently acquires target domain knowledge by appropriately weighting the importance of domain-specific terms (DS-terms) during fine-tuning. MSLM jointly masks DS-terms and generic words, then learns mask-specific losses by ensuring LMs incur larger penalties for in
    
[^15]: 一体化：异质交互建模用于冷启动评分预测

    All-in-One: Heterogeneous Interaction Modeling for Cold-Start Rating Prediction

    [https://arxiv.org/abs/2403.17740](https://arxiv.org/abs/2403.17740)

    提出了异质交互评分网络（HIRE）框架，通过异质交互模块（HIM）来共同建模异质交互并直接推断重要特征

    

    冷启动评分预测是推荐系统中一个基本问题，已得到广泛研究。许多方法已经被提出，利用现有数据之间的显式关系，例如协同过滤、社交推荐和异构信息网络，以缓解冷启动用户和物品的数据不足问题。然而，基于不同角色之间的数据构建的显式关系可能不可靠且无关，从而限制了特定推荐任务的性能上限。受此启发，本文提出了一个灵活的框架，名为异质交互评分网络（HIRE）。HIRE不仅仅依赖于预先定义的交互模式或手动构建的异构信息网络。相反，我们设计了一个异质交互模块（HIM），来共同建模异质交互并直接推断重要特征。

    arXiv:2403.17740v1 Announce Type: cross  Abstract: Cold-start rating prediction is a fundamental problem in recommender systems that has been extensively studied. Many methods have been proposed that exploit explicit relations among existing data, such as collaborative filtering, social recommendations and heterogeneous information network, to alleviate the data insufficiency issue for cold-start users and items. However, the explicit relations constructed based on data between different roles may be unreliable and irrelevant, which limits the performance ceiling of the specific recommendation task. Motivated by this, in this paper, we propose a flexible framework dubbed heterogeneous interaction rating network (HIRE). HIRE dose not solely rely on the pre-defined interaction pattern or the manually constructed heterogeneous information network. Instead, we devise a Heterogeneous Interaction Module (HIM) to jointly model the heterogeneous interactions and directly infer the important in
    
[^16]: CADGL: 上下文感知深度图学习用于预测药物-药物相互作用

    CADGL: Context-Aware Deep Graph Learning for Predicting Drug-Drug Interactions

    [https://arxiv.org/abs/2403.17210](https://arxiv.org/abs/2403.17210)

    通过CADGL框架，利用上下文感知深度图学习来预测药物-药物相互作用，解决了现有DDI预测模型在泛化、特征提取和现实应用方面的挑战

    

    药物-药物相互作用（DDIs）的研究是药物开发过程中的一个关键元素。DDIs发生在一个药物的性质受其他药物包含的影响时。检测有利的DDIs有可能为在实际设置中应用的创新药物的创造和推进铺平道路。然而，现有的DDI预测模型在极端情况下的泛化、稳健特征提取以及现实应用可能性方面持续面临挑战。我们旨在通过利用上下文感知深度图学习的有效性，引入一种名为CADGL的新颖框架来应对这些挑战。基于定制的变分图自编码器（VGAE），我们利用两个上下文预处理器从两个不同视角：局部邻域和分子上下文，在异质图结构中提取特征，捕获关键的结构和生理化学信息。

    arXiv:2403.17210v1 Announce Type: cross  Abstract: Examining Drug-Drug Interactions (DDIs) is a pivotal element in the process of drug development. DDIs occur when one drug's properties are affected by the inclusion of other drugs. Detecting favorable DDIs has the potential to pave the way for creating and advancing innovative medications applicable in practical settings. However, existing DDI prediction models continue to face challenges related to generalization in extreme cases, robust feature extraction, and real-life application possibilities. We aim to address these challenges by leveraging the effectiveness of context-aware deep graph learning by introducing a novel framework named CADGL. Based on a customized variational graph autoencoder (VGAE), we capture critical structural and physio-chemical information using two context preprocessors for feature extraction from two different perspectives: local neighborhood and molecular context, in a heterogeneous graphical structure. Ou
    
[^17]: 当SMILES拥有语言：使用文本分类方法对药物SMILES字符串进行药物分类

    When SMILES have Language: Drug Classification using Text Classification Methods on Drug SMILES Strings

    [https://arxiv.org/abs/2403.12984](https://arxiv.org/abs/2403.12984)

    将药物SMILES字符串视为句子并利用文本分类方法进行药物分类，证实了通过简单的自然语言处理方法解决复杂问题的可能性

    

    复杂的化学结构，如药物，通常由SMILES字符串来定义，作为分子和键的序列。这些SMILES字符串在不同的基于机器学习的药物相关研究和表示工作中使用。在这项工作中，我们摆脱复杂的表示法，提出了一个问题：如果我们将药物SMILES视为常规句子，并进行文本分类以进行药物分类会怎样？我们的实验证实了这种可能性，获得了非常有竞争力的分数。该研究探讨了将每个原子和键视为句子组件的概念，利用基本的自然语言处理方法对药物类型进行分类，表明复杂的问题也可以用更简单的视角来解决。数据和代码可在此处找到：https://github.com/azminewasi/Drug-Classification-NLP。

    arXiv:2403.12984v1 Announce Type: cross  Abstract: Complex chemical structures, like drugs, are usually defined by SMILES strings as a sequence of molecules and bonds. These SMILES strings are used in different complex machine learning-based drug-related research and representation works. Escaping from complex representation, in this work, we pose a single question: What if we treat drug SMILES as conventional sentences and engage in text classification for drug classification? Our experiments affirm the possibility with very competitive scores. The study explores the notion of viewing each atom and bond as sentence components, employing basic NLP methods to categorize drug types, proving that complex problems can also be solved with simpler perspectives. The data and code are available here: https://github.com/azminewasi/Drug-Classification-NLP.
    
[^18]: 双通道多重图神经网络用于推荐

    Dual-Channel Multiplex Graph Neural Networks for Recommendation

    [https://arxiv.org/abs/2403.11624](https://arxiv.org/abs/2403.11624)

    该研究提出了一种名为双通道多重图神经网络（DCMGNN）的新型推荐框架，能够有效解决现有推荐方法中存在的多通路关系行为模式建模和对目标关系影响忽略的问题。

    

    高效的推荐系统在准确捕捉反映个人偏好的用户和项目属性方面发挥着至关重要的作用。一些现有的推荐技术已经开始将重点转向在真实世界的推荐场景中对用户和项目之间的各种类型交互关系进行建模，例如在线购物平台上的点击、标记收藏和购买。然而，这些方法仍然面临两个重要的缺点：(1) 不足的建模和利用用户和项目之间多通路关系形成的各种行为模式对表示学习的影响，以及(2) 忽略了行为模式中不同关系对推荐系统场景中目标关系的影响。在本研究中，我们介绍了一种新颖的推荐框架，即双通道多重图神经网络（DCMGNN），该框架解决了上述挑战。

    arXiv:2403.11624v1 Announce Type: cross  Abstract: Efficient recommender systems play a crucial role in accurately capturing user and item attributes that mirror individual preferences. Some existing recommendation techniques have started to shift their focus towards modeling various types of interaction relations between users and items in real-world recommendation scenarios, such as clicks, marking favorites, and purchases on online shopping platforms. Nevertheless, these approaches still grapple with two significant shortcomings: (1) Insufficient modeling and exploitation of the impact of various behavior patterns formed by multiplex relations between users and items on representation learning, and (2) ignoring the effect of different relations in the behavior patterns on the target relation in recommender system scenarios. In this study, we introduce a novel recommendation framework, Dual-Channel Multiplex Graph Neural Network (DCMGNN), which addresses the aforementioned challenges
    
[^19]: 通向统一多模式个性化：大型视觉语言模型用于生成推荐和更多领域

    Towards Unified Multi-Modal Personalization: Large Vision-Language Models for Generative Recommendation and Beyond

    [https://arxiv.org/abs/2403.10667](https://arxiv.org/abs/2403.10667)

    本文旨在建立一个统一的多模态个性化系统(UniMP)，有效利用多模态数据同时消除与任务和模态特定定制相关的复杂性。

    

    开发一个能够有效利用异构资源并满足各种个性化需求的通用模型一直是社区渴望的目标。我们日常的选择，尤其是在时尚和零售等领域，很大程度上受多模态数据的影响，比如图片和文本描述。这些模态不仅提供直观的指导，还迎合个性化用户偏好。然而，当前主流的个性化方法主要聚焦于基于ID或文本的推荐问题，未能理解涵盖各种任务或模态的信息。本文的目标是建立一个统一的多模态个性化系统(UniMP)，能够有效利用多模态数据，同时消除与任务和模态特定定制相关的复杂性。我们认为基础生成建模的进展提供了

    arXiv:2403.10667v1 Announce Type: cross  Abstract: Developing a universal model that can effectively harness heterogeneous resources and respond to a wide range of personalized needs has been a longstanding community aspiration. Our daily choices, especially in domains like fashion and retail, are substantially shaped by multi-modal data, such as pictures and textual descriptions. These modalities not only offer intuitive guidance but also cater to personalized user preferences. However, the predominant personalization approaches mainly focus on the ID or text-based recommendation problem, failing to comprehend the information spanning various tasks or modalities. In this paper, our goal is to establish a Unified paradigm for Multi-modal Personalization systems (UniMP), which effectively leverages multi-modal data while eliminating the complexities associated with task- and modality-specific customization. We argue that the advancements in foundational generative modeling have provided
    
[^20]: 小型语言模型能成为顺序推荐系统的良好推理者吗？

    Can Small Language Models be Good Reasoners for Sequential Recommendation?

    [https://arxiv.org/abs/2403.04260](https://arxiv.org/abs/2403.04260)

    提出了逐步知识提取框架（SLIM），为顺序推荐系统解决了大型语言模型（LLMs）高资源需求的难题，使其能以资源高效的方式享受LLMs的出色推理能力。

    

    大型语言模型（LLMs）由于其出色的语言理解和生成能力，为顺序推荐开拓了新的领域。然而，要成功实现由LLMs赋能的顺序推荐还有许多挑战需要解决。首先，用户行为模式通常复杂，仅仅依靠LLMs的一步推理可能会导致错误或与任务无关的响应。其次，LLMs（例如ChatGPT-175B）极高的资源需求是难以承受且在实际顺序推荐系统中不切实际的。本文提出了一个新颖的逐步知识提取框架用于推荐（SLIM），为顺序推荐器以“瘦”（即资源高效）的方式享受LLMs出色的推理能力铺平了一条有前途的道路。我们引入基于用户行为序列的CoT提示来实现更好的推荐。

    arXiv:2403.04260v1 Announce Type: cross  Abstract: Large language models (LLMs) open up new horizons for sequential recommendations, owing to their remarkable language comprehension and generation capabilities. However, there are still numerous challenges that should be addressed to successfully implement sequential recommendations empowered by LLMs. Firstly, user behavior patterns are often complex, and relying solely on one-step reasoning from LLMs may lead to incorrect or task-irrelevant responses. Secondly, the prohibitively resource requirements of LLM (e.g., ChatGPT-175B) are overwhelmingly high and impractical for real sequential recommender systems. In this paper, we propose a novel Step-by-step knowLedge dIstillation fraMework for recommendation (SLIM), paving a promising path for sequential recommenders to enjoy the exceptional reasoning capabilities of LLMs in a "slim" (i.e., resource-efficient) manner. We introduce CoT prompting based on user behavior sequences for the larg
    
[^21]: 相似的实体是否具有相似的嵌入？

    Do Similar Entities have Similar Embeddings?

    [https://arxiv.org/abs/2312.10370](https://arxiv.org/abs/2312.10370)

    本文挑战了实体相似性在图中在嵌入空间中自然反映的主流假设，通过进行广泛的实验来衡量这种关系。

    

    为了进行链接预测而开发的知识图嵌入模型（KGEMs）学习知识图中实体的向量表示，即嵌入。一个普遍的默认假设是KGE实体相似性假设，即这些KGEMs在它们的嵌入空间中保留图的结构，即将相似的实体放在图中彼此靠近。这种理想的性质使得KGEMs在推荐系统或药物再利用等下游任务中被广泛使用。然而，实体的相似性与嵌入空间中的相似性之间的关系很少被正式评估。通常，KGEMs是基于其唯一的链接预测能力进行评估的，使用类似Hits@K或Mean Rank的排名指标。本文质疑了图中的实体相似性在嵌入空间中天然反映这一流行假设。因此，我们进行了大量实验来衡量

    arXiv:2312.10370v2 Announce Type: replace  Abstract: Knowledge graph embedding models (KGEMs) developed for link prediction learn vector representations for entities in a knowledge graph, known as embeddings. A common tacit assumption is the KGE entity similarity assumption, which states that these KGEMs retain the graph's structure within their embedding space, \textit{i.e.}, position similar entities within the graph close to one another. This desirable property make KGEMs widely used in downstream tasks such as recommender systems or drug repurposing. Yet, the relation of entity similarity and similarity in the embedding space has rarely been formally evaluated. Typically, KGEMs are assessed based on their sole link prediction capabilities, using ranked-based metrics such as Hits@K or Mean Rank. This paper challenges the prevailing assumption that entity similarity in the graph is inherently mirrored in the embedding space. Therefore, we conduct extensive experiments to measure the 
    
[^22]: MILL：大型语言模型进行零-shot查询扩展的相互验证

    MILL: Mutual Verification with Large Language Models for Zero-Shot Query Expansion

    [https://arxiv.org/abs/2310.19056](https://arxiv.org/abs/2310.19056)

    该论文提出了一种利用大型语言模型进行相互验证的零-shot查询扩展框架，有效解决了查询扩展中已有方法的限制和缺陷。

    

    论文提出了一种新颖的零shot查询扩展框架，利用大型语言模型进行相互验证。具体来说，首先设计了一种查询-查询-文档生成方法，利用LLMs的零-shot推理能力生成多样化的子查询和相应的文档。然后，一个相互验证过程协同生成和检索的文档以实现最佳扩展。我们提出的方法完全是零-shot的。

    arXiv:2310.19056v3 Announce Type: replace-cross  Abstract: Query expansion, pivotal in search engines, enhances the representation of user information needs with additional terms. While existing methods expand queries using retrieved or generated contextual documents, each approach has notable limitations. Retrieval-based methods often fail to accurately capture search intent, particularly with brief or ambiguous queries. Generation-based methods, utilizing large language models (LLMs), generally lack corpus-specific knowledge and entail high fine-tuning costs. To address these gaps, we propose a novel zero-shot query expansion framework utilizing LLMs for mutual verification. Specifically, we first design a query-query-document generation method, leveraging LLMs' zero-shot reasoning ability to produce diverse sub-queries and corresponding documents. Then, a mutual verification process synergizes generated and retrieved documents for optimal expansion. Our proposed method is fully zero
    
[^23]: Query2GMM：使用高斯混合模型学习知识图谱推理表示

    Query2GMM: Learning Representation with Gaussian Mixture Model for Reasoning over Knowledge Graphs

    [https://arxiv.org/abs/2306.10367](https://arxiv.org/abs/2306.10367)

    使用高斯混合模型学习表示，以更好地模拟知识图谱中具有多样化答案的逻辑查询

    

    逻辑查询答案是知识图谱中一个基础且复杂的任务。本文提出Query2GMM来回答知识图谱上的逻辑查询，通过将查询和实体共同嵌入到同一嵌入空间中，以更好地模拟具有多样化答案的查询。

    arXiv:2306.10367v2 Announce Type: replace  Abstract: Logical query answering over Knowledge Graphs (KGs) is a fundamental yet complex task. A promising approach to achieve this is to embed queries and entities jointly into the same embedding space. Research along this line suggests that using multi-modal distribution to represent answer entities is more suitable than uni-modal distribution, as a single query may contain multiple disjoint answer subsets due to the compositional nature of multi-hop queries and the varying latent semantics of relations. However, existing methods based on multi-modal distribution roughly represent each subset without capturing its accurate cardinality, or even degenerate into uni-modal distribution learning during the reasoning process due to the lack of an effective similarity measure. To better model queries with diversified answers, we propose Query2GMM for answering logical queries over knowledge graphs. In Query2GMM, we present the GMM embedding to re
    
[^24]: 加强推荐系统：减轻假阴性影响的策略

    Enhancing Recommender Systems: A Strategy to Mitigate False Negative Impact

    [https://arxiv.org/abs/2211.13912](https://arxiv.org/abs/2211.13912)

    本研究提出了一种新颖的负采样策略，即以正样本为主导的负样本合成，可以显著提升推荐系统的预测准确性

    

    在推荐系统的隐式协同过滤任务中，最近的研究主要集中在模型结构设计，使用了一些很有前景的技术，如图神经网络(GNNs)。然而，适用于这些模型的有效且高效的负采样方法仍未得到充分发展。一个挑战是现有的硬负采样器往往在模型训练中遭受更严重的过拟合问题。在这项工作中，我们首先研究了过拟合的原因，并通过实验证明了这一点，即通过不正确地选择假负实例。此外，我们经验性地观察到了一个逆向现象，即用相当大比例的正样本嵌入来污染硬负样本的嵌入，将导致预测准确性出现明显提升。基于这一发现，我们提出了一种新颖的负采样策略，即以正样本为主导的负样本合成

    arXiv:2211.13912v2 Announce Type: replace  Abstract: In implicit collaborative filtering (CF) task of recommender systems, recent works mainly focus on model structure design with promising techniques like graph neural networks (GNNs). Effective and efficient negative sampling methods that suit these models, however, remain underdeveloped. One challenge is that existing hard negative samplers tend to suffer from severer over-fitting in model training. In this work, we first study the reason behind the over-fitting, and illustrate it with the incorrect selection of false negative instances with the support of experiments. In addition, we empirically observe a counter-intuitive phenomenon, that is, polluting hard negative samples' embeddings with a quite large proportional of positive samples' embeddings will lead to remarkable performance gains for prediction accuracy. On top of this finding, we present a novel negative sampling strategy, i.e., positive-dominated negative synthesizing (
    
[^25]: 统计MIMO雷达和基频全双工多用户MIMO通信的联合设计--第一部分: 信号处理

    Co-Designing Statistical MIMO Radar and In-band Full-Duplex Multi-User MIMO Communications -- Part I: Signal Processing

    [https://arxiv.org/abs/2006.14774](https://arxiv.org/abs/2006.14774)

    这项研究旨在共同设计一个MRMC框架，解决统计MIMO雷达和基频全双工多用户MIMO通信系统在相同频段内运行时遇到的各种问题。

    

    我们考虑了一种频谱共享问题，即在同一频段内同时运行统计（或广泛分布的）多输入多输出（MIMO）雷达和基频全双工（IBFD）多用户MIMO（MU-MIMO）通信系统。以往关于联合MIMO雷达-MIMO通信（MRMC）系统的工作主要集中在共位MIMO雷达、半双工MIMO通信、单用户场景，省略了实际约束（杂波、上行/下行发送功率、上行/下行服务质量和峰均功率比），或者采用分开的发送/接收单元的MRMC共存。本文的目的是共同设计一个MRMC框架，解决所有这些问题。在本文中，我们提出了一种分布式IBFD MRMC的信号处理方法，其中雷达接收机被设计为额外利用下行通信信号的反射。

    arXiv:2006.14774v5 Announce Type: replace-cross  Abstract: We consider a spectral sharing problem in which a statistical (or widely distributed) multiple-input multiple-output (MIMO) radar and an in-band full-duplex (IBFD) multi-user MIMO (MU-MIMO) communications system concurrently operate within the same frequency band. Prior works on joint MIMO-radar-MIMO-communications (MRMC) systems largely focus on either colocated MIMO radars, half-duplex MIMO communications, single-user scenarios, omit practical constraints (clutter, uplink [UL]/downlink [DL] transmit powers, UL/DL quality-of-service, and peak-to-average-power ratio), or MRMC co-existence that employs separate transmit/receive units. The purpose of this and companion papers (Part II and III) is to co-design an MRMC framework that addresses all of these issues. In this paper, we propose signal processing for a distributed IBFD MRMC, where radar receiver is designed to additionally exploit the downlink communications signals refl
    
[^26]: 大型语言模型是有效的文本排序器，具有两两排名提示

    Large Language Models are Effective Text Rankers with Pairwise Ranking Prompting. (arXiv:2306.17563v1 [cs.IR])

    [http://arxiv.org/abs/2306.17563](http://arxiv.org/abs/2306.17563)

    本论文提出了一种名为PRP的新技术，通过使用两两排名提示来显著减轻大型语言模型（LLM）的负担，并首次在标准基准测试中实现了最先进的排名性能。

    

    使用大型语言模型（LLM）通过直接将查询和候选文档输入提示进行文档排序是一个有趣且实用的问题。然而，迄今为止取得了有限的成功，研究人员发现很难在基准数据集上超越精调基准排序器。我们分析了现有方法使用的点对点和列表排序提示，并认为现成的LLM没有完全理解这些排序公式，可能是由于LLM的训练方式的特性。在本文中，我们提出了一种名为两两排名提示（PRP）的新技术，大大减轻了LLM的负担。我们的结果是文献中首次使用中等规模的开源LLM在标准基准测试中实现了最先进的排名性能。在TREC-DL2020上，基于20B参数的Flan-UL2模型的PRP超过了文献中基于商业黑盒GPT-4的最佳方法。

    Ranking documents using Large Language Models (LLMs) by directly feeding the query and candidate documents into the prompt is an interesting and practical problem. However, there has been limited success so far, as researchers have found it difficult to outperform fine-tuned baseline rankers on benchmark datasets. We analyze pointwise and listwise ranking prompts used by existing methods and argue that off-the-shelf LLMs do not fully understand these ranking formulations, possibly due to the nature of how LLMs are trained. In this paper, we propose to significantly reduce the burden on LLMs by using a new technique called Pairwise Ranking Prompting (PRP). Our results are the first in the literature to achieve state-of-the-art ranking performance on standard benchmarks using moderate-sized open-sourced LLMs. On TREC-DL2020, PRP based on the Flan-UL2 model with 20B parameters outperforms the previous best approach in the literature, which is based on the blackbox commercial GPT-4 that ha
    
[^27]: 基于机器学习的多阶段系统对真实患者数据进行视力预测

    Visual Acuity Prediction on Real-Life Patient Data Using a Machine Learning Based Multistage System. (arXiv:2204.11970v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2204.11970](http://arxiv.org/abs/2204.11970)

    本研究提供了一种使用机器学习技术开发预测模型的多阶段系统，可高精度预测三种眼疾患者的视力变化，并辅助眼科医生进行临床决策和患者咨询。

    

    现实生活中，眼科学中的玻璃体手术药物治疗是治疗年龄相关性黄斑变性（AMD）、糖尿病性黄斑水肿（DME）和视网膜静脉阻塞（RVO）相关疾病的一种普遍治疗方法。然而，在真实世界的情况下，由于数据的异质性和不完整性，患者往往会在多年时间内失去视力，尽管接受治疗。本文采用多种IT系统，提出了一种用于研究的数据集成流程，该流程融合了德国一家最佳医疗保健医院的眼科部门的不同IT系统。经过使用机器学习技术开发预测模型，我们实现了对患者视力的预测。我们的结果表明，我们的系统可以为三种疾病的预测提供高准确性。此外，我们还展示了我们的系统可以作为工具，辅助眼科医生进行临床决策和患者咨询。

    In ophthalmology, intravitreal operative medication therapy (IVOM) is a widespread treatment for diseases related to the age-related macular degeneration (AMD), the diabetic macular edema (DME), as well as the retinal vein occlusion (RVO). However, in real-world settings, patients often suffer from loss of vision on time scales of years despite therapy, whereas the prediction of the visual acuity (VA) and the earliest possible detection of deterioration under real-life conditions is challenging due to heterogeneous and incomplete data. In this contribution, we present a workflow for the development of a research-compatible data corpus fusing different IT systems of the department of ophthalmology of a German maximum care hospital. The extensive data corpus allows predictive statements of the expected progression of a patient and his or her VA in each of the three diseases. We found out for the disease AMD a significant deterioration of the visual acuity over time. Within our proposed m
    

