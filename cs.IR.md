# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [PromptCrypt: Prompt Encryption for Secure Communication with Large Language Models](https://arxiv.org/abs/2402.05868) | PromptCrypt是一种使用表情符号对用户输入进行加密的机制，保护了大型语言模型（LLM）中用户的隐私，防止数据泄露和解密。 |
| [^2] | [Natural Language User Profiles for Transparent and Scrutable Recommendations](https://arxiv.org/abs/2402.05810) | 本研究提出了一种新的方法，通过利用用户评价来创建自然语言配置文件，以解决传统推荐系统中的不透明性和可解释性的问题。通过这种方法，我们的系统能够提供透明的自然语言推荐，而且性能水平与传统的推荐系统相当。 |
| [^3] | [CounterCLR: Counterfactual Contrastive Learning with Non-random Missing Data in Recommendation](https://arxiv.org/abs/2402.05740) | CounterCLR是一种处理推荐系统中非随机缺失数据的反事实对比学习框架，通过利用对比学习的进展，采用深度表示网络CauNet来推断缺失数据并进行用户偏好建模。 |
| [^4] | [Multilingual E5 Text Embeddings: A Technical Report](https://arxiv.org/abs/2402.05672) | 这项技术报告介绍了开源的多语言E5文本嵌入模型的训练方法和评估结果，包括三种不同大小的模型和一种新的以指令为导向的嵌入模型。模型在推理效率和嵌入质量上取得了平衡，性能与同等大小的最先进的仅英文模型相当。 |
| [^5] | [Knowledge Graphs Meet Multi-Modal Learning: A Comprehensive Survey](https://arxiv.org/abs/2402.05391) | 知识图谱与多模态学习的综述介绍了KG4MM和MM4KG两个主要方面，包括任务定义、构建进展、评估基准以及关键研究轨迹。 |
| [^6] | [Navigating the Knowledge Sea: Planet-scale answer retrieval using LLMs](https://arxiv.org/abs/2402.05318) | 本文探讨了信息检索技术的演变，特别关注大型语言模型（LLMs）在传统搜索方法与新兴答案检索范式之间的桥梁作用，LLMs的整合标志着用户与信息系统交互方式的范式转变。 |
| [^7] | [CADReN: Contextual Anchor-Driven Relational Network for Controllable Cross-Graphs Node Importance Estimation](https://arxiv.org/abs/2402.05135) | CADReN是一个上下文锚点驱动的关系网络，用于可控的跨图节点重要性估计。它通过引入上下文锚点机制，考虑知识图谱中的结构和语义特征，实现了更好的性能，包括零-shot预测能力，并开源了两个新的数据集RIC200和WK1K。 |
| [^8] | [Quantifying Similarity: Text-Mining Approaches to Evaluate ChatGPT and Google Bard Content in Relation to BioMedical Literature](https://arxiv.org/abs/2402.05116) | 本研究旨在通过文本挖掘方法评估ChatGPT和Google Bard生成的内容与生物医学文献之间的相似性。实验结果显示，在余弦文档相似性方面，ChatGPT表现优于Google Bard。 |
| [^9] | [Can Large Language Models Detect Rumors on Social Media?](https://arxiv.org/abs/2402.03916) | 本研究探讨了使用大型语言模型（LLMs）检测社交媒体上的谣言的可行性。通过设计提示来教导LLMs理解新闻和评论中的关键线索，并将传播信息分解为传播链，我们的LeRuD方法相对于其他最先进的模型在谣言检测方面取得了更好的性能，同时具备在少样本或零样本情况下更有潜力的应用。 |
| [^10] | [Position Paper: Why the Shooting in the Dark Method Dominates Recommender Systems Practice; A Call to Abandon Anti-Utopian Thinking](https://arxiv.org/abs/2402.02152) | 这篇论文质疑了推荐系统实践中目前常用的“摸着石头过河”方法，呼吁摒弃反乌托邦思维。论文提出了使用深度学习堆栈的非标准用法，以解锁奖励优化的推荐系统的潜力。 |
| [^11] | [ParlayANN: Scalable and Deterministic Parallel Graph-Based Approximate Nearest Neighbor Search Algorithms](https://arxiv.org/abs/2305.04359) | ParlayANN是一个具有确定性和并行性的基于图的近似最近邻搜索算法库，提供了用于开发这类算法的一套有用工具。 |
| [^12] | [Real-World Deployment and Evaluation of Kwame for Science, An AI Teaching Assistant for Science Education in West Africa](https://arxiv.org/abs/2302.10786) | 这项研究扩展了AI教辅Kwame，面向西非科学教育，并将其作为一个网络应用程序进行了实际部署和评估。结果显示，在实际环境中，Kwame for Science取得了很高的准确率，并且获得了广泛的用户和问题提问量。 |
| [^13] | [MMRec: Simplifying Multimodal Recommendation](https://arxiv.org/abs/2302.03497) | MMRec是一个开源工具箱，旨在简化和规范多模式推荐模型的实施和比较过程，并提供一个统一的平台来融合来自多个模态的信息。 |
| [^14] | [Regular Expressions with Backreferences: Polynomial-Time Matching Techniques](https://arxiv.org/abs/1903.05896) | 带反向引用的正则表达式可以通过限制参数在多项式时间内进行匹配。此外，引入内存确定性正则表达式和其他相关概念可以用于处理难以检测的属性。 |
| [^15] | [REFORM: Removing False Correlation in Multi-level Interaction for CTR Prediction.](http://arxiv.org/abs/2309.14891) | REFORM是一个CTR预测框架，通过两个流式叠加的循环结构利用了多级高阶特征表示，并消除了误关联。 |

# 详细

[^1]: PromptCrypt: 使用表情符号对大型语言模型进行安全通信的提示加密

    PromptCrypt: Prompt Encryption for Secure Communication with Large Language Models

    [https://arxiv.org/abs/2402.05868](https://arxiv.org/abs/2402.05868)

    PromptCrypt是一种使用表情符号对用户输入进行加密的机制，保护了大型语言模型（LLM）中用户的隐私，防止数据泄露和解密。

    

    基于云的大型语言模型（LLM）如ChatGPT在日常操作中变得越来越重要，成为各种应用程序中的重要工具。虽然这些模型在可访问性和功能性方面带来了重大好处，但它们也引入了重要的隐私问题：在云基础架构中传输和存储用户数据会产生重大的数据泄露和未经授权访问敏感信息的风险；即使数据的传输和存储被加密，LLM服务提供商仍然知道数据的真实内容，从而阻止个人或实体放心使用此类LLM服务。为了解决这些问题，本文提出了一种简单但有效的机制PromptCrypt来保护用户隐私。它使用表情符号对用户输入进行加密，然后将其发送到LLM，有效地使其对人类或LLM的检查无法理解，同时保留原始提示的意图，从而确保用户隐私。

    Cloud-based large language models (LLMs) such as ChatGPT have increasingly become integral to daily operations, serving as vital tools across various applications. While these models offer substantial benefits in terms of accessibility and functionality, they also introduce significant privacy concerns: the transmission and storage of user data in cloud infrastructures pose substantial risks of data breaches and unauthorized access to sensitive information; even if the transmission and storage of data is encrypted, the LLM service provider itself still knows the real contents of the data, preventing individuals or entities from confidently using such LLM services. To address these concerns, this paper proposes a simple yet effective mechanism PromptCrypt to protect user privacy. It uses Emoji to encrypt the user inputs before sending them to LLM, effectively rendering them indecipherable to human or LLM's examination while retaining the original intent of the prompt, thus ensuring the 
    
[^2]: 透明与可解释的推荐系统的自然语言用户配置文件

    Natural Language User Profiles for Transparent and Scrutable Recommendations

    [https://arxiv.org/abs/2402.05810](https://arxiv.org/abs/2402.05810)

    本研究提出了一种新的方法，通过利用用户评价来创建自然语言配置文件，以解决传统推荐系统中的不透明性和可解释性的问题。通过这种方法，我们的系统能够提供透明的自然语言推荐，而且性能水平与传统的推荐系统相当。

    

    当前最先进的推荐系统主要依赖于用户的隐式或显式反馈来建议新项目。虽然在推荐新选项方面有效，但这些传统系统通常使用不可解释的嵌入。这种缺乏透明度不仅限制了用户对为什么建议某些项目的理解，还减少了用户轻松审查和编辑其偏好的能力。为了解决这些限制，我们引入了一种新的方法，利用用户评价来创建描述用户偏好的个性化自然语言配置文件。通过这些描述性配置文件，我们的系统提供透明的自然语言推荐。我们的评估结果表明，这种新方法在与已建立的推荐系统相当的性能水平上。

    Current state-of-the-art recommender systems predominantly rely on either implicit or explicit feedback from users to suggest new items. While effective in recommending novel options, these conventional systems often use uninterpretable embeddings. This lack of transparency not only limits user understanding of why certain items are suggested but also reduces the user's ability to easily scrutinize and edit their preferences. For example, if a user has a change in interests, they would need to make significant changes to their interaction history to adjust the model's recommendations. To address these limitations, we introduce a novel method that utilizes user reviews to craft personalized, natural language profiles describing users' preferences. Through these descriptive profiles, our system provides transparent recommendations in natural language. Our evaluations show that this novel approach maintains a performance level on par with established recommender systems, but with the adde
    
[^3]: CounterCLR: 推荐中具有非随机缺失数据的反事实对比学习

    CounterCLR: Counterfactual Contrastive Learning with Non-random Missing Data in Recommendation

    [https://arxiv.org/abs/2402.05740](https://arxiv.org/abs/2402.05740)

    CounterCLR是一种处理推荐系统中非随机缺失数据的反事实对比学习框架，通过利用对比学习的进展，采用深度表示网络CauNet来推断缺失数据并进行用户偏好建模。

    

    推荐系统的观测反馈通常存在选择偏差和数据稀疏等问题，这严重影响了准确性和排名等性能。本研究提出了一种新颖的反事实对比学习框架——CounterCLR，通过利用对比学习的进展来解决非随机缺失数据的问题。具体而言，CounterCLR采用了一个称为CauNet的深度表示网络，推断推荐中的非随机缺失数据，并进行用户偏好建模。

    Recommender systems are designed to learn user preferences from observed feedback and comprise many fundamental tasks, such as rating prediction and post-click conversion rate (pCVR) prediction. However, the observed feedback usually suffer from two issues: selection bias and data sparsity, where biased and insufficient feedback seriously degrade the performance of recommender systems in terms of accuracy and ranking. Existing solutions for handling the issues, such as data imputation and inverse propensity score, are highly susceptible to additional trained imputation or propensity models. In this work, we propose a novel counterfactual contrastive learning framework for recommendation, named CounterCLR, to tackle the problem of non-random missing data by exploiting the advances in contrast learning. Specifically, the proposed CounterCLR employs a deep representation network, called CauNet, to infer non-random missing data in recommendations and perform user preference modeling by fur
    
[^4]: 多语言E5文本嵌入：一项技术报告

    Multilingual E5 Text Embeddings: A Technical Report

    [https://arxiv.org/abs/2402.05672](https://arxiv.org/abs/2402.05672)

    这项技术报告介绍了开源的多语言E5文本嵌入模型的训练方法和评估结果，包括三种不同大小的模型和一种新的以指令为导向的嵌入模型。模型在推理效率和嵌入质量上取得了平衡，性能与同等大小的最先进的仅英文模型相当。

    

    本技术报告介绍了开源的多语言E5文本嵌入模型的训练方法和评估结果，该模型于2023年中期发布。提供了三种不同大小（小/基础/大）的嵌入模型，平衡了推理效率和嵌入质量。训练过程遵循英文E5模型的配方，涉及10亿个多语言文本对的对比预训练，然后在一系列标记数据集上进行微调。此外，我们还引入了一种新的以指令为导向的嵌入模型，性能与相似大小的最先进的仅英文模型相当。有关模型发布的信息可以在https://github.com/microsoft/unilm/tree/master/e5找到。

    This technical report presents the training methodology and evaluation results of the open-source multilingual E5 text embedding models, released in mid-2023. Three embedding models of different sizes (small / base / large) are provided, offering a balance between the inference efficiency and embedding quality. The training procedure adheres to the English E5 model recipe, involving contrastive pre-training on 1 billion multilingual text pairs, followed by fine-tuning on a combination of labeled datasets. Additionally, we introduce a new instruction-tuned embedding model, whose performance is on par with state-of-the-art, English-only models of similar sizes. Information regarding the model release can be found at https://github.com/microsoft/unilm/tree/master/e5 .
    
[^5]: 知识图谱与多模态学习：综述

    Knowledge Graphs Meet Multi-Modal Learning: A Comprehensive Survey

    [https://arxiv.org/abs/2402.05391](https://arxiv.org/abs/2402.05391)

    知识图谱与多模态学习的综述介绍了KG4MM和MM4KG两个主要方面，包括任务定义、构建进展、评估基准以及关键研究轨迹。

    

    知识图谱在推动各种人工智能应用方面起着关键作用，语义网络社区对多模态维度的探索为创新打开了新的途径。在本综述中，我们仔细审查了300多篇文章，重点关注了两个主要方面的知识图谱感知研究：以知识图谱支持多模态任务的KG驱动多模态（KG4MM）学习，将知识图谱研究扩展到多模态知识图谱（MM4KG）领域。我们从定义知识图谱和多模态知识图谱开始，然后探索它们的构建进展。我们的综述包括两个主要任务类别：KG感知的多模态学习任务，如图像分类和视觉问答，以及内在的多模态知识图谱任务，如多模态知识图谱补全和实体对齐，突出了具体的研究轨迹。对于这些任务中的大部分，我们提供了定义、评估基准，并进一步指出进行相关研究的重要见解。最后，我们讨论了cu

    Knowledge Graphs (KGs) play a pivotal role in advancing various AI applications, with the semantic web community's exploration into multi-modal dimensions unlocking new avenues for innovation. In this survey, we carefully review over 300 articles, focusing on KG-aware research in two principal aspects: KG-driven Multi-Modal (KG4MM) learning, where KGs support multi-modal tasks, and Multi-Modal Knowledge Graph (MM4KG), which extends KG studies into the MMKG realm. We begin by defining KGs and MMKGs, then explore their construction progress. Our review includes two primary task categories: KG-aware multi-modal learning tasks, such as Image Classification and Visual Question Answering, and intrinsic MMKG tasks like Multi-modal Knowledge Graph Completion and Entity Alignment, highlighting specific research trajectories. For most of these tasks, we provide definitions, evaluation benchmarks, and additionally outline essential insights for conducting relevant research. Finally, we discuss cu
    
[^6]: 航行知识之海：利用LLMs进行行星级答案检索

    Navigating the Knowledge Sea: Planet-scale answer retrieval using LLMs

    [https://arxiv.org/abs/2402.05318](https://arxiv.org/abs/2402.05318)

    本文探讨了信息检索技术的演变，特别关注大型语言模型（LLMs）在传统搜索方法与新兴答案检索范式之间的桥梁作用，LLMs的整合标志着用户与信息系统交互方式的范式转变。

    

    信息检索是一个快速发展的信息检索领域，其特点是从基本的超链接导航到复杂的算法驱动搜索引擎的不断改进。本文旨在全面介绍信息检索技术的演变，特别关注大型语言模型（LLMs）在传统搜索方法与新兴答案检索范式之间的桥梁作用。LLMs在响应检索和索引领域的整合标志着用户与信息系统交互方式的范式转变。这种范式转变是由像GPT-4这样的大型语言模型的整合驱动的，它们能够理解和生成类似于人类的文本，从而能够提供更直接和情境相关的答案给用户查询。通过这种探索，我们试图阐明技术里程碑。

    Information retrieval is a rapidly evolving field of information retrieval, which is characterized by a continuous refinement of techniques and technologies, from basic hyperlink-based navigation to sophisticated algorithm-driven search engines. This paper aims to provide a comprehensive overview of the evolution of Information Retrieval Technology, with a particular focus on the role of Large Language Models (LLMs) in bridging the gap between traditional search methods and the emerging paradigm of answer retrieval. The integration of LLMs in the realms of response retrieval and indexing signifies a paradigm shift in how users interact with information systems. This paradigm shift is driven by the integration of large language models (LLMs) like GPT-4, which are capable of understanding and generating human-like text, thus enabling them to provide more direct and contextually relevant answers to user queries. Through this exploration, we seek to illuminate the technological milestones 
    
[^7]: CADReN: 上下文锚点驱动的关系网络用于可控跨图节点重要性估计

    CADReN: Contextual Anchor-Driven Relational Network for Controllable Cross-Graphs Node Importance Estimation

    [https://arxiv.org/abs/2402.05135](https://arxiv.org/abs/2402.05135)

    CADReN是一个上下文锚点驱动的关系网络，用于可控的跨图节点重要性估计。它通过引入上下文锚点机制，考虑知识图谱中的结构和语义特征，实现了更好的性能，包括零-shot预测能力，并开源了两个新的数据集RIC200和WK1K。

    

    节点重要性估计(NIE)对于通过检索增强生成将外部信息整合到大型语言模型中至关重要。传统方法侧重于静态的单一图特征，在新图和用户特定要求方面缺乏适应性。我们提出的CADReN通过引入上下文锚点(CA)机制来解决这些限制。该方法使网络能够相对于CA评估节点的重要性，考虑知识图谱(KGs)中的结构和语义特征。广泛的实验表明，CADReN在跨图NIE任务中取得了更好的性能，并具有零-shot预测能力。CADReN还被证明在单一图NIE任务上与以前的模型性能相匹配。此外，我们还引入并开源了两个新数据集RIC200和WK1K，专门用于跨图NIE研究，为这个领域的未来发展提供了宝贵的资源。

    Node Importance Estimation (NIE) is crucial for integrating external information into Large Language Models through Retriever-Augmented Generation. Traditional methods, focusing on static, single-graph characteristics, lack adaptability to new graphs and user-specific requirements. CADReN, our proposed method, addresses these limitations by introducing a Contextual Anchor (CA) mechanism. This approach enables the network to assess node importance relative to the CA, considering both structural and semantic features within Knowledge Graphs (KGs). Extensive experiments show that CADReN achieves better performance in cross-graph NIE task, with zero-shot prediction ability. CADReN is also proven to match the performance of previous models on single-graph NIE task. Additionally, we introduce and opensource two new datasets, RIC200 and WK1K, specifically designed for cross-graph NIE research, providing a valuable resource for future developments in this domain.
    
[^8]: 量化相似性：使用文本挖掘方法评估ChatGPT和Google Bard生成内容与生物医学文献的关联性

    Quantifying Similarity: Text-Mining Approaches to Evaluate ChatGPT and Google Bard Content in Relation to BioMedical Literature

    [https://arxiv.org/abs/2402.05116](https://arxiv.org/abs/2402.05116)

    本研究旨在通过文本挖掘方法评估ChatGPT和Google Bard生成的内容与生物医学文献之间的相似性。实验结果显示，在余弦文档相似性方面，ChatGPT表现优于Google Bard。

    

    背景：在大语言模型（LLMs）的支持下，生成式人工智能工具的出现展示了强大的生成内容能力。到目前为止，评估通过所谓的提示工程生成的内容的有用性已经成为一个有趣的研究问题。目标：通过提示工程的平均值，我们评估这些内容与科学家产生的真实文献的相似性和接近程度。方法：在这个探索性分析中，（1）我们通过提示工程来生成ChatGPT和Google Bard的临床内容，以便与文献对应内容进行比较，（2）我们通过比较所生成内容与生物医学文献对应内容的相似性来评估它们之间的相似性。我们的方法是使用文本挖掘方法比较文档和相关的二元组，并使用网络分析来评估术语的中心性。结果：实验表明，ChatGPT在余弦文档相似性方面表现优于Google Bard（38%对34%），

    Background: The emergence of generative AI tools, empowered by Large Language Models (LLMs), has shown powerful capabilities in generating content. To date, the assessment of the usefulness of such content, generated by what is known as prompt engineering, has become an interesting research question. Objectives Using the mean of prompt engineering, we assess the similarity and closeness of such contents to real literature produced by scientists. Methods In this exploratory analysis, (1) we prompt-engineer ChatGPT and Google Bard to generate clinical content to be compared with literature counterparts, (2) we assess the similarities of the contents generated by comparing them with counterparts from biomedical literature. Our approach is to use text-mining approaches to compare documents and associated bigrams and to use network analysis to assess the terms' centrality. Results The experiments demonstrated that ChatGPT outperformed Google Bard in cosine document similarity (38% to 34%), 
    
[^9]: 大型语言模型能否检测社交媒体上的谣言？

    Can Large Language Models Detect Rumors on Social Media?

    [https://arxiv.org/abs/2402.03916](https://arxiv.org/abs/2402.03916)

    本研究探讨了使用大型语言模型（LLMs）检测社交媒体上的谣言的可行性。通过设计提示来教导LLMs理解新闻和评论中的关键线索，并将传播信息分解为传播链，我们的LeRuD方法相对于其他最先进的模型在谣言检测方面取得了更好的性能，同时具备在少样本或零样本情况下更有潜力的应用。

    

    在这项工作中，我们研究了使用大型语言模型（LLMs）在社交媒体上进行谣言检测。然而，LLMs在推理整个传播信息时面临挑战，因为该信息包含新闻内容和大量评论，LLMs可能无法集中关注复杂传播信息中的关键线索，并且在面对大量和冗余信息时难以进行推理。因此，我们提出了一种基于LLMs增强的谣言检测（LeRuD）方法，在其中设计提示来教导LLMs关注新闻和评论中的重要线索，并将整个传播信息分解为传播链以减轻LLMs的负担。我们在Twitter和微博数据集上进行了大量实验证明，LeRuD的性能优于几种最先进的谣言检测模型，提升了2.4％至7.6％。同时，通过应用LLMs，LeRuD无需进行训练数据，并且在少样本或零样本情况下展现出更具有潜力的谣言检测能力。

    In this work, we investigate to use Large Language Models (LLMs) for rumor detection on social media. However, it is challenging for LLMs to reason over the entire propagation information on social media, which contains news contents and numerous comments, due to LLMs may not concentrate on key clues in the complex propagation information, and have trouble in reasoning when facing massive and redundant information. Accordingly, we propose an LLM-empowered Rumor Detection (LeRuD) approach, in which we design prompts to teach LLMs to reason over important clues in news and comments, and divide the entire propagation information into a Chain-of-Propagation for reducing LLMs' burden. We conduct extensive experiments on the Twitter and Weibo datasets, and LeRuD outperforms several state-of-the-art rumor detection models by 2.4% to 7.6%. Meanwhile, by applying LLMs, LeRuD requires no data for training, and thus shows more promising rumor detection ability in few-shot or zero-shot scenarios.
    
[^10]: 论文题目：为什么“摸着石头过河”方法主导推荐系统实践；呼吁摒弃反乌托邦思维

    Position Paper: Why the Shooting in the Dark Method Dominates Recommender Systems Practice; A Call to Abandon Anti-Utopian Thinking

    [https://arxiv.org/abs/2402.02152](https://arxiv.org/abs/2402.02152)

    这篇论文质疑了推荐系统实践中目前常用的“摸着石头过河”方法，呼吁摒弃反乌托邦思维。论文提出了使用深度学习堆栈的非标准用法，以解锁奖励优化的推荐系统的潜力。

    

    应用推荐系统研究处于一种奇特的境地。尽管在通过A/B测试来衡量性能方面有一个非常严格的协议，但找到要测试的“B”的最佳方法并没有明确地针对性能，而是针对一个代理指标。因此，一个A/B测试的成功或失败完全取决于所提出的代理指标是否与性能相关性更好。没有原则可以在离线情况下确定一个代理指标是否比另一个更好，这使得从业者们摸不着头脑。本论文的目的是质疑这种反乌托邦思维，并主张深度学习堆栈的非标准用法实际上有潜力解锁优化奖励的推荐系统。

    Applied recommender systems research is in a curious position. While there is a very rigorous protocol for measuring performance by A/B testing, best practice for finding a `B' to test does not explicitly target performance but rather targets a proxy measure. The success or failure of a given A/B test then depends entirely on if the proposed proxy is better correlated to performance than the previous proxy. No principle exists to identify if one proxy is better than another offline, leaving the practitioners shooting in the dark. The purpose of this position paper is to question this anti-Utopian thinking and argue that a non-standard use of the deep learning stacks actually has the potential to unlock reward optimizing recommendation.
    
[^11]: ParlayANN: 可扩展和确定性的并行基于图的近似最近邻搜索算法

    ParlayANN: Scalable and Deterministic Parallel Graph-Based Approximate Nearest Neighbor Search Algorithms

    [https://arxiv.org/abs/2305.04359](https://arxiv.org/abs/2305.04359)

    ParlayANN是一个具有确定性和并行性的基于图的近似最近邻搜索算法库，提供了用于开发这类算法的一套有用工具。

    

    近似最近邻搜索（ANNS）算法是现代深度学习系统的关键部分，因为它们可以在高维向量空间表示（即嵌入）的数据上实现高效的相似性搜索。在各种ANNS算法中，基于图的算法被认为在吞吐量和召回率之间取得了最佳的平衡。尽管现代ANNS数据集具有较大规模，但现有的并行基于图的实现由于大量使用锁和其他顺序瓶颈而存在显著的挑战，这两点阻碍了它们有效扩展到大量处理器，并导致在某些应用中不希望出现的非确定性。

    Approximate nearest-neighbor search (ANNS) algorithms are a key part of the modern deep learning stack due to enabling efficient similarity search over high-dimensional vector space representations (i.e., embeddings) of data. Among various ANNS algorithms, graph-based algorithms are known to achieve the best throughput-recall tradeoffs. Despite the large scale of modern ANNS datasets, existing parallel graph based implementations suffer from significant challenges to scale to large datasets due to heavy use of locks and other sequential bottlenecks, which 1) prevents them from efficiently scaling to a large number of processors, and 2) results in nondeterminism that is undesirable in certain applications.   In this paper, we introduce ParlayANN, a library of deterministic and parallel graph-based approximate nearest neighbor search algorithms, along with a set of useful tools for developing such algorithms. In this library, we develop novel parallel implementations for four state-of-th
    
[^12]: 面向西非科学教育的AI教辅Kwame的现实世界部署和评估

    Real-World Deployment and Evaluation of Kwame for Science, An AI Teaching Assistant for Science Education in West Africa

    [https://arxiv.org/abs/2302.10786](https://arxiv.org/abs/2302.10786)

    这项研究扩展了AI教辅Kwame，面向西非科学教育，并将其作为一个网络应用程序进行了实际部署和评估。结果显示，在实际环境中，Kwame for Science取得了很高的准确率，并且获得了广泛的用户和问题提问量。

    

    非洲教师与学生的比例高，这限制了学生们获取教育问题解答等学习支持的机会。本研究将面向编码教育的AI教辅Kwame扩展为面向科学教育，并将其部署为一个网络应用程序。Kwame for Science通过提供来自精选知识来源的段落以及基于西非高级中学证书考试（WASSCE）的综合科学科目的相关过去的国家考试问题的答案来回答学生们的问题。此外，学生们还可以查看过去的国家考试问题及其答案，并通过我们开发的主题检测模型进行按年份、问题类型和主题的自动分类（91%非加权平均召回率）。我们在实际环境中部署了Kwame for Science超过8个月，有来自32个国家（其中15个在非洲）的750个用户，共提出了1.5K的问题。我们的评估结果显示，87.2%的前三名问题准确率（n=109）。

    Africa has a high student-to-teacher ratio which limits students' access to teachers for learning support such as educational question answering. In this work, we extended Kwame, a bilingual AI teaching assistant for coding education, adapted it for science education, and deployed it as a web app. Kwame for Science provides passages from well-curated knowledge sources and related past national exam questions as answers to questions from students based on the Integrated Science subject of the West African Senior Secondary Certificate Examination (WASSCE). Furthermore, students can view past national exam questions along with their answers and filter by year, question type, and topics that were automatically categorized by a topic detection model which we developed (91% unweighted average recall). We deployed Kwame for Science in the real world over 8 months and had 750 users across 32 countries (15 in Africa) and 1.5K questions asked. Our evaluation showed an 87.2% top 3 accuracy (n=109
    
[^13]: MMRec：简化多模式推荐

    MMRec: Simplifying Multimodal Recommendation

    [https://arxiv.org/abs/2302.03497](https://arxiv.org/abs/2302.03497)

    MMRec是一个开源工具箱，旨在简化和规范多模式推荐模型的实施和比较过程，并提供一个统一的平台来融合来自多个模态的信息。

    

    本文介绍了一个用于多模式推荐的开源工具箱MMRec。MMRec简化和规范了实施和比较多模式推荐模型的过程。MMRec的目标是提供一个统一且可配置的平台，以最小化实施和测试多模式推荐模型的工作量。它使得传统的矩阵分解到现代的基于图的算法等多模式模型能够同时融合多个模态的信息。我们的文档、示例和源代码可在\url{https://github.com/enoche/MMRec}上找到。

    This paper presents an open-source toolbox, MMRec for multimodal recommendation. MMRec simplifies and canonicalizes the process of implementing and comparing multimodal recommendation models. The objective of MMRec is to provide a unified and configurable arena that can minimize the effort in implementing and testing multimodal recommendation models. It enables multimodal models, ranging from traditional matrix factorization to modern graph-based algorithms, capable of fusing information from multiple modalities simultaneously. Our documentation, examples, and source code are available at \url{https://github.com/enoche/MMRec}.
    
[^14]: 带反向引用的正则表达式：多项式时间匹配技术

    Regular Expressions with Backreferences: Polynomial-Time Matching Techniques

    [https://arxiv.org/abs/1903.05896](https://arxiv.org/abs/1903.05896)

    带反向引用的正则表达式可以通过限制参数在多项式时间内进行匹配。此外，引入内存确定性正则表达式和其他相关概念可以用于处理难以检测的属性。

    

    带有反向引用（regex）的正则表达式在大多数现代正则表达式匹配库中得到支持，但它们具有一个NP完全的匹配问题。我们定义了一个复杂度参数，称为活动变量度，在此参数受到常数限制时，可以在多项式时间内匹配正则表达式。此外，我们还制定了一种新颖的确定性类型，用于正则表达式（在自动机理论层面上），这产生了可以在O(|w|p(|r|))时间内匹配的内存确定性正则表达式。这里，r是正则表达式，w是字符串。这些概念的自然扩展导致了正则表达式的一些难以检测的属性。

    Regular expressions with backreferences (regex, for short), as supported by most modern libraries for regular expression matching, have an NP-complete matching problem. We define a complexity parameter of regex, called active variable degree, such that regex with this parameter bounded by a constant can be matched in polynomial-time. Moreover, we formulate a novel type of determinism for regex (on an automaton-theoretic level), which yields the class of memory-deterministic regex that can be matched in time O(|w|p(|r|)) for a polynomial p (where r is the regex and w the word). Natural extensions of these concepts lead to properties of regex that are intractable to check.
    
[^15]: REFORM: 移除CTR预测中的误关联的多级交互

    REFORM: Removing False Correlation in Multi-level Interaction for CTR Prediction. (arXiv:2309.14891v1 [cs.IR])

    [http://arxiv.org/abs/2309.14891](http://arxiv.org/abs/2309.14891)

    REFORM是一个CTR预测框架，通过两个流式叠加的循环结构利用了多级高阶特征表示，并消除了误关联。

    

    点击率（CTR）预测是在线广告和推荐系统中的关键任务，准确的预测对于用户定位和个性化推荐至关重要。最近的一些前沿方法主要关注复杂的隐式和显式特征交互。然而，这些方法忽视了由混淆因子或选择偏差引起的误关联问题。这个问题在这些交互的复杂性和冗余性下变得更加严重。我们提出了一种CTR预测框架，称为REFORM，在多级特征交互中移除了误关联。所提出的REFORM框架通过两个流式叠加的循环结构利用了大量的多级高阶特征表示，并消除了误关联。该框架有两个关键组成部分：I. 多级叠加循环（MSR）结构使模型能够高效地捕捉到来自特征空间的多样非线性交互。

    Click-through rate (CTR) prediction is a critical task in online advertising and recommendation systems, as accurate predictions are essential for user targeting and personalized recommendations. Most recent cutting-edge methods primarily focus on investigating complex implicit and explicit feature interactions. However, these methods neglect the issue of false correlations caused by confounding factors or selection bias. This problem is further magnified by the complexity and redundancy of these interactions. We propose a CTR prediction framework that removes false correlation in multi-level feature interaction, termed REFORM. The proposed REFORM framework exploits a wide range of multi-level high-order feature representations via a two-stream stacked recurrent structure while eliminating false correlations. The framework has two key components: I. The multi-level stacked recurrent (MSR) structure enables the model to efficiently capture diverse nonlinear interactions from feature spa
    

