# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [HimiRec: Modeling Hierarchical Multi-interest for Recommendation](https://rss.arxiv.org/abs/2402.01253) | 本论文提出了一个新颖的两阶段方法，用于显式地建模层次化多兴趣的推荐系统，通过层次聚类和基于Transformer的模型来挖掘层次化的多兴趣信息。 |
| [^2] | [Event-based Product Carousel Recommendation with Query-Click Graph](https://arxiv.org/abs/2402.03277) | 该论文提出了基于事件的产品推荐轮播问题，并提出了一种基于查询-点击二部图的有效推荐系统来解决该问题。 |
| [^3] | [Unified Hallucination Detection for Multimodal Large Language Models](https://arxiv.org/abs/2402.03190) | 该论文提出了一个新颖的统一的多模态幻觉检测框架UNIHD，并设计了一个评估基准方法MHaluBench来评估幻觉检测方法的进展。这项工作扩展了幻觉检测的研究范围并提供了有效的解决方案。 |
| [^4] | [Comparison of Topic Modelling Approaches in the Banking Context](https://arxiv.org/abs/2402.03176) | 本研究在银行业背景下比较了主题建模方法。通过使用KernelPCA和K-means Clustering在BERTopic架构中，我们得到了连贯的主题，连贯性得分为0.8463。 |
| [^5] | [Linguistic features for sentence difficulty prediction in ABSA](https://arxiv.org/abs/2402.03163) | 本研究探讨了面向方面的情感分析中句子困难度的定义。通过实验研究了领域多样性和句法多样性对句子困难度的影响，并使用一组分类器识别了最困难的句子。 |
| [^6] | [EasyInstruct: An Easy-to-use Instruction Processing Framework for Large Language Models](https://arxiv.org/abs/2402.03049) | EasyInstruct是一个易于使用的用于大型语言模型的指令处理框架，通过模块化指令生成、选择和提示，并考虑它们的组合和交互，使指令处理更加方便和高效。 |
| [^7] | [Understanding and Guiding Weakly Supervised Entity Alignment with Potential Isomorphism Propagation](https://arxiv.org/abs/2402.03025) | 本文通过传播视角分析了弱监督的实体对齐任务，并提出一种潜在同构传播操作符来增强知识图谱之间的邻域信息传播。通过验证，发现基于聚合的实体对齐模型中的潜在对齐实体具有同构子图。 |
| [^8] | [Domain Adaptation of Multilingual Semantic Search - Literature Review](https://arxiv.org/abs/2402.02932) | 这篇文献综述对当前在低资源环境下进行领域自适应和多语义搜索的方法进行了概述，提出了一种新的方法来聚类和有效地组合这些方法，并探讨了在低资源环境下将多语义搜索与领域自适应方法进行组合的可能性。 |
| [^9] | [Dynamic Sparse Learning: A Novel Paradigm for Efficient Recommendation](https://arxiv.org/abs/2402.02855) | 本文提出了一种针对推荐模型的新型学习范式，称为动态稀疏学习（DSL），通过从头训练一个轻量级稀疏模型，解决了模型大小和学习效率的问题。 |
| [^10] | [Comparing Knowledge Sources for Open-Domain Scientific Claim Verification](https://arxiv.org/abs/2402.02844) | 在本论文中，我们对开放领域科学主张验证系统的性能进行了测试，通过比较不同的知识来源（PubMed、维基百科、谷歌）和信息检索方法，我们发现在真实世界环境中进行科学主张验证的挑战和问题。 |
| [^11] | [Trinity: Syncretizing Multi-/Long-tail/Long-term Interests All in One](https://arxiv.org/abs/2402.02842) | 本文提出了一种统一的兴趣建模框架“Trinity”，通过利用长期线索来解决兴趣遗忘问题，并改善多兴趣建模任务。通过构建实时聚类系统和计算统计兴趣直方图，Trinity能够识别用户的兴趣并进行个性化推荐。 |
| [^12] | [Intersectional Two-sided Fairness in Recommendation](https://arxiv.org/abs/2402.02816) | 本文针对推荐系统中的交叉双边公平性问题，提出了一种名为交叉双边公平推荐（ITFR）的新方法，通过利用锐度感知损失感知劣势群体，使用协作损失平衡开发不同交叉群体的一致区分能力，并利用预测得分归一化来公平对待不同交叉群体中的正例。实验证明该方法在提高交叉双边公平性方面取得了显著效果。 |
| [^13] | [Large Language Model Distilling Medication Recommendation Model](https://arxiv.org/abs/2402.02803) | 本研究旨在利用大型语言模型改进药物推荐方法，以解决传统模型在医学数据语义理解和新患者处方推荐方面的挑战。 |
| [^14] | [List-aware Reranking-Truncation Joint Model for Search and Retrieval-augmented Generation](https://arxiv.org/abs/2402.02764) | 该论文提出了一个基于列表感知的重新排序-截断联合模型，用于搜索和生成增强检索。该模型旨在捕捉列表级的上下文特征，通过重新排序和截断来返回更好的列表。先前的研究将重新排序和截断视为两个单独的任务，但这种分离不是最优的。因此，该论文提出的联合模型可以解决信息共享和错误积累的问题。 |
| [^15] | [Denoising Time Cycle Modeling for Recommendation](https://arxiv.org/abs/2402.02718) | 本文提出了一种新的方法，降噪时间周期建模（DiCycle），用于在推荐系统中建模用户-物品交互的时间模式。通过选择与目标物品高度相关的用户行为子集，DiCycle能够更好地建模时间周期模式，从而提高推荐性能。 |
| [^16] | [Position bias in features](https://arxiv.org/abs/2402.02626) | 本文提出了一种用于搜索引擎的动态排名系统的特征，该特征可以准确估计文档相关性，但是在存在位置偏差的情况下会有高方差，并强调了准确估计位置偏差的必要性，建议同时使用有偏和无偏的位置偏差特征。 |
| [^17] | [eXplainable Bayesian Multi-Perspective Generative Retrieval](https://arxiv.org/abs/2402.02418) | 本文将不确定性校准和可解释性整合到一个检索流程中，通过引入贝叶斯方法和多透视检索来校准不确定性。使用解释方法分析黑盒模型行为，并将重要性分数作为补充相关性分数，提升基础模型性能。实验证明我们的方法在问答和事实检验任务上显著提高了性能。 |
| [^18] | [ExTTNet: A Deep Learning Algorithm for Extracting Table Texts from Invoice Images](https://arxiv.org/abs/2402.02246) | ExTTNet是一种深度学习算法，能够自动从发票图像中提取表格文字。使用光学字符识别（OCR）技术获取文本，并通过特征提取方法提高准确度。通过多层神经网络模型进行训练，最终获得了0.92的F1分数。 |
| [^19] | [Diffusion Cross-domain Recommendation](https://arxiv.org/abs/2402.02182) | 该论文研究了跨域推荐中的扩散模型，通过从辅助领域中添加数据来解决冷启动用户数据稀疏问题。其中，映射模块起着关键的作用，确定了跨域推荐模型的性能。 |
| [^20] | [Enhancing Complex Question Answering over Knowledge Graphs through Evidence Pattern Retrieval](https://arxiv.org/abs/2402.02175) | 本论文提出了一种名为EPR的方法，通过索引原子邻接模式并将其组合，明确建模结构依赖，并在知识图谱中进行子图提取。实验结果表明，EPR方法在复杂问题回答方面取得了显著的改进，提高了F1得分，并在WebQuestionsSP上表现出竞争力。 |
| [^21] | [PatSTEG: Modeling Formation Dynamics of Patent Citation Networks via The Semantic-Topological Evolutionary Graph](https://arxiv.org/abs/2402.02158) | 本文提出了一种联合语义-拓扑进化图学习方法（PatSTEG），用于建模专利引用网络的形成动态。通过考虑语义和拓扑信息，该方法能够探索专利数据库中复杂的文本和引用网络结构，并为专利数据挖掘提供新的机会。 |
| [^22] | [Position Paper: Why the Shooting in the Dark Method Dominates Recommender Systems Practice; A Call to Abandon Anti-Utopian Thinking](https://arxiv.org/abs/2402.02152) | 这篇论文质疑了推荐系统实践中目前常用的“摸着石头过河”方法，呼吁摒弃反乌托邦思维。论文提出了使用深度学习堆栈的非标准用法，以解锁奖励优化的推荐系统的潜力。 |
| [^23] | [Prototypical Contrastive Learning through Alignment and Uniformity for Recommendation](https://arxiv.org/abs/2402.02079) | 本研究提出了一种基于对齐和一致性的原型对比学习方法（ProtoAU）用于推荐系统中，该方法解决了随机抽样引起的抽样偏差问题，提高了特征表示的有效性。 |
| [^24] | [Locally-Adaptive Quantization for Streaming Vector Search](https://arxiv.org/abs/2402.02044) | 本文研究了局部自适应量化在流式相似度搜索中的应用，引入了Turbo LVQ和multi-means LVQ两种改进方法，分别提升了搜索性能28%和27%。实验证明，LVQ及其新变体使得向量搜索变得极快。 |
| [^25] | [Improving Large-Scale k-Nearest Neighbor Text Categorization with Label Autoencoders](https://arxiv.org/abs/2402.01963) | 本文提出了一种使用标签自编码器改进大规模k最近邻文本分类的方法，通过将大的标签空间映射到一个缩小的潜在空间，并从该潜在空间中重建出预测的标签，以解决在大型文档集合中处理自动语义索引的问题。 |
| [^26] | [Clarifying the Path to User Satisfaction: An Investigation into Clarification Usefulness](https://arxiv.org/abs/2402.01934) | 本研究探讨了澄清问题对用户满意度的重要影响，研究发现具体问题、问题的主观性和情感色彩以及查询长度等特征对澄清的有效性有明显影响。 |
| [^27] | [CoLe and LYS at BioASQ MESINESP8 Task: similarity based descriptor assignment in Spanish](https://arxiv.org/abs/2402.01916) | 这篇论文介绍了基于相似度的描述符分配方法在BioASQ MESINESP8任务中的应用，通过使用传统的信息检索工具，并提出了适用于西班牙语等语言的方法。 |
| [^28] | [Zero-1-to-3: Domain-level Zero-shot Cognitive Diagnosis via One Batch of Early-bird Students towards Three Diagnostic Objectives](https://arxiv.org/abs/2312.13434) | 本论文提出了一种Zero-1-to-3的方法，该方法通过一批早鸟学生关注三个诊断目标，以实现领域级零样本认知诊断。它解决了跨领域诊断模型中可能融入不可转移信息的问题，从而提高了知识传输的效果。 |
| [^29] | [Knowledge graph driven recommendation model of graph neural network.](http://arxiv.org/abs/2401.10244) | 提出了一种基于知识图谱的图神经网络推荐模型KGLN，通过合并节点特征、调整聚合权重和迭代演化，提高了个性化推荐的准确性和效果。在实验中相对于已有基准方法，KGLN在不同数据集上的AUC提高了0.3%至5.9%和1.1%至8.2%。 |
| [^30] | [Spectral-based Graph Neutral Networks for Complementary Item Recommendation.](http://arxiv.org/abs/2401.02130) | 本文提出了一种基于频谱的图神经网络方法（SComGNN）用于模拟和理解商品间的互补关系，以在推荐系统中准确和及时地推荐后续商品。 |
| [^31] | [Evolution of ESG-focused DLT Research: An NLP Analysis of the Literature.](http://arxiv.org/abs/2308.12420) | 本研究通过NLP分析了ESG主导的DLT研究的演化，通过构建引用网络和命名实体识别任务，对DLT在ESG背景下的发展进行了文献综述。 |
| [^32] | [Improving Detection of ChatGPT-Generated Fake Science Using Real Publication Text: Introducing xFakeBibs a Supervised-Learning Network Algorithm.](http://arxiv.org/abs/2308.11767) | 本文介绍了一种能够提高对ChatGPT生成的假科学进行检测的算法。通过使用一种新设计的监督机器学习算法，该算法能够准确地将机器生成的出版物与科学家生成的出版物区分开来。结果表明，ChatGPT在技术术语方面与真实科学存在显著差异。算法在分类过程中取得了较高的准确率。 |
| [^33] | [Natural Language is All a Graph Needs.](http://arxiv.org/abs/2308.07134) | 本论文提出了一种名为InstructGLM的结构化语言模型算法，该算法将大型语言模型与图表学习问题相结合，旨在探索是否可以用语言模型取代图神经网络作为图表的基础模型。 |
| [^34] | [Bringing order into the realm of Transformer-based language models for artificial intelligence and law.](http://arxiv.org/abs/2308.05502) | 本文提供了第一个对基于Transformer的语言模型在法律领域的人工智能问题和任务中的方法的系统概述。文章旨在突出这一领域的研究进展，以进一步了解Transformer在支持法律流程中的AI成功贡献以及当前的局限性。 |
| [^35] | [An In-depth Investigation of User Response Simulation for Conversational Search.](http://arxiv.org/abs/2304.07944) | 本文研究了对话式搜索中用户响应模拟的方法。当前的模拟系统要么只能对是非问题进行回答，要么无法产生高质量的响应。通过用更小但先进的系统替换当前最先进的用户模拟系统，能够显著改进性能。 |

# 详细

[^1]: HimiRec: 建模层次化多兴趣的推荐系统

    HimiRec: Modeling Hierarchical Multi-interest for Recommendation

    [https://rss.arxiv.org/abs/2402.01253](https://rss.arxiv.org/abs/2402.01253)

    本论文提出了一个新颖的两阶段方法，用于显式地建模层次化多兴趣的推荐系统，通过层次聚类和基于Transformer的模型来挖掘层次化的多兴趣信息。

    

    工业级推荐系统通常包含检索阶段和排名阶段，以处理亿级用户和物品。检索阶段用于检索与用户兴趣相关的候选物品进行推荐，引起了广泛关注。经常情况下，用户展示出层次化的多个兴趣，比如一个在体育中热衷支持金州勇士队的用户，也会对几乎所有动画有兴趣，体育和动画处于同样的层次。然而，大多数现有方法隐式地学习这种层次化差异，导致更细粒度的兴趣信息被平均化，限制了对用户在热门兴趣和其他轻兴趣方面的详细理解。因此，在这项工作中，我们提出了一种新颖的两阶段方法，用于显式地建模层次化多兴趣的推荐系统。在第一阶段，我们使用层次聚类和基于Transformer的模型来挖掘层次化的多兴趣信息。

    Industrial recommender systems usually consist of the retrieval stage and the ranking stage, to handle the billion-scale of users and items. The retrieval stage retrieves candidate items relevant to user interests for recommendations and has attracted much attention. Frequently, users show hierarchical multi-interests reflected in a heavy user of a certain NBA team Golden State Warriors in Sports, who is also a light user of almost the whole Animation. Both Sports and Animation are at the same level. However, most existing methods implicitly learn this hierarchical difference, making more fine-grained interest information to be averaged and limiting detailed understanding of the user's different needs in heavy interests and other light interests. Therefore, we propose a novel two-stage approach to explicitly modeling hierarchical multi-interest for recommendation in this work. In the first hierarchical multi-interest mining stage, the hierarchical clustering and transformer-based model
    
[^2]: 基于事件的查询-点击图的产品推荐轮播

    Event-based Product Carousel Recommendation with Query-Click Graph

    [https://arxiv.org/abs/2402.03277](https://arxiv.org/abs/2402.03277)

    该论文提出了基于事件的产品推荐轮播问题，并提出了一种基于查询-点击二部图的有效推荐系统来解决该问题。

    

    当前许多推荐系统主要关注的是产品对产品的推荐和用户对产品的推荐，即使在事件发生时也没有模拟目标事件的典型推荐（例如节日、季节活动或社交活动），而忽略了目标事件的多方面购物需求。通常，针对目标事件的多个方面的产品推荐是由人工策划者生成的，他们手动识别出各个方面，并选择一系列与方面相关的产品（即产品轮播）作为推荐。然而，由于缺乏事件相关方面和相关产品的准确信息，利用机器学习构建推荐系统是非常具有挑战性的。为了弥补这一空白，我们在电子商务领域定义了新问题，即基于事件的产品推荐轮播，并提出了一种基于查询-点击二部图的有效推荐系统。

    Many current recommender systems mainly focus on the product-to-product recommendations and user-to-product recommendations even during the time of events rather than modeling the typical recommendations for the target event (e.g., festivals, seasonal activities, or social activities) without addressing the multiple aspects of the shopping demands for the target event. Product recommendations for the multiple aspects of the target event are usually generated by human curators who manually identify the aspects and select a list of aspect-related products (i.e., product carousel) for each aspect as recommendations. However, building a recommender system with machine learning is non-trivial due to the lack of both the ground truth of event-related aspects and the aspect-related products. To fill this gap, we define the novel problem as the event-based product carousel recommendations in e-commerce and propose an effective recommender system based on the query-click bipartite graph. We app
    
[^3]: 统一的多模态大型语言模型的幻觉检测

    Unified Hallucination Detection for Multimodal Large Language Models

    [https://arxiv.org/abs/2402.03190](https://arxiv.org/abs/2402.03190)

    该论文提出了一个新颖的统一的多模态幻觉检测框架UNIHD，并设计了一个评估基准方法MHaluBench来评估幻觉检测方法的进展。这项工作扩展了幻觉检测的研究范围并提供了有效的解决方案。

    

    尽管在多模态任务方面取得了重大进展，多模态大型语言模型(MLLMs)仍然存在幻觉的严重问题。因此，可靠地检测MLLMs中的幻觉已成为模型评估和实际应用部署保障的重要方面。之前在这个领域的研究受到了狭窄的任务焦点、不足的幻觉类别涵盖范围以及缺乏详细的细粒度的限制。针对这些挑战，我们的工作扩展了幻觉检测的研究范围。我们提出了一个新颖的元评估基准方法，MHaluBench，精心设计以促进幻觉检测方法的进展评估。此外，我们揭示了一个新颖的统一多模态幻觉检测框架，UNIHD，它利用一套辅助工具来稳健地验证幻觉的发生。我们通过实验证明了UNIHD的有效性。

    Despite significant strides in multimodal tasks, Multimodal Large Language Models (MLLMs) are plagued by the critical issue of hallucination. The reliable detection of such hallucinations in MLLMs has, therefore, become a vital aspect of model evaluation and the safeguarding of practical application deployment. Prior research in this domain has been constrained by a narrow focus on singular tasks, an inadequate range of hallucination categories addressed, and a lack of detailed granularity. In response to these challenges, our work expands the investigative horizons of hallucination detection. We present a novel meta-evaluation benchmark, MHaluBench, meticulously crafted to facilitate the evaluation of advancements in hallucination detection methods. Additionally, we unveil a novel unified multimodal hallucination detection framework, UNIHD, which leverages a suite of auxiliary tools to validate the occurrence of hallucinations robustly. We demonstrate the effectiveness of UNIHD throug
    
[^4]: 银行业背景下主题建模方法的比较

    Comparison of Topic Modelling Approaches in the Banking Context

    [https://arxiv.org/abs/2402.03176](https://arxiv.org/abs/2402.03176)

    本研究在银行业背景下比较了主题建模方法。通过使用KernelPCA和K-means Clustering在BERTopic架构中，我们得到了连贯的主题，连贯性得分为0.8463。

    

    主题建模是许多应用程序中自动提取主题的重要任务，例如情感分析和推荐系统。该方法对于服务行业来说是至关重要的，可以用于监控客户讨论。传统方法如潜在狄利克雷分配（Latent Dirichlet Allocation，LDA）在主题发现方面表现出色，但由于数据稀疏性和无法对文档中的单词顺序建模，它们的结果并不一致。因此，本研究提出了在BERTopic架构中使用核主成分分析（Kernel Principal Component Analysis，KernelPCA）和K-means聚类的方法。我们使用尼日利亚银行客户的推文准备了一个新数据集，然后使用该数据集来比较主题建模方法。我们的研究结果显示，在BERTopic架构中使用KernelPCA和K-means可以生成连贯的主题，其中连贯性得分为0.8463。

    Topic modelling is a prominent task for automatic topic extraction in many applications such as sentiment analysis and recommendation systems. The approach is vital for service industries to monitor their customer discussions. The use of traditional approaches such as Latent Dirichlet Allocation (LDA) for topic discovery has shown great performances, however, they are not consistent in their results as these approaches suffer from data sparseness and inability to model the word order in a document. Thus, this study presents the use of Kernel Principal Component Analysis (KernelPCA) and K-means Clustering in the BERTopic architecture. We have prepared a new dataset using tweets from customers of Nigerian banks and we use this to compare the topic modelling approaches. Our findings showed KernelPCA and K-means in the BERTopic architecture-produced coherent topics with a coherence score of 0.8463.
    
[^5]: 用于ABSA中句子困难度预测的语言特征

    Linguistic features for sentence difficulty prediction in ABSA

    [https://arxiv.org/abs/2402.03163](https://arxiv.org/abs/2402.03163)

    本研究探讨了面向方面的情感分析中句子困难度的定义。通过实验研究了领域多样性和句法多样性对句子困难度的影响，并使用一组分类器识别了最困难的句子。

    

    自然语言理解的一个挑战是处理句子的主观性，这些句子可能表达意见和情感，增加了复杂性和细微差别。情感分析是一个旨在从文本中提取和分析这些主观元素的领域，可以应用在不同的粒度级别，如文档、段落、句子或方面。面向方面的情感分析是一个经过研究的课题，有很多可用的数据集和模型。然而，对于面向方面的情感分析来说，什么因素使一个句子变得困难并没有清晰的定义。在本文中，我们通过对三个数据集：“笔记本电脑”、“餐馆”和“MTSC”（多目标依赖情感分类）以及这三个数据集的合并版本进行实验，探讨了这个问题。我们研究了领域多样性和句法多样性对困难度的影响。我们使用一组分类器来识别最困难的句子并分析它们的特征。

    One of the challenges of natural language understanding is to deal with the subjectivity of sentences, which may express opinions and emotions that add layers of complexity and nuance. Sentiment analysis is a field that aims to extract and analyze these subjective elements from text, and it can be applied at different levels of granularity, such as document, paragraph, sentence, or aspect. Aspect-based sentiment analysis is a well-studied topic with many available data sets and models. However, there is no clear definition of what makes a sentence difficult for aspect-based sentiment analysis. In this paper, we explore this question by conducting an experiment with three data sets: "Laptops", "Restaurants", and "MTSC" (Multi-Target-dependent Sentiment Classification), and a merged version of these three datasets. We study the impact of domain diversity and syntactic diversity on difficulty. We use a combination of classifiers to identify the most difficult sentences and analyze their c
    
[^6]: EasyInstruct：一个易于使用的用于大型语言模型的指令处理框架

    EasyInstruct: An Easy-to-use Instruction Processing Framework for Large Language Models

    [https://arxiv.org/abs/2402.03049](https://arxiv.org/abs/2402.03049)

    EasyInstruct是一个易于使用的用于大型语言模型的指令处理框架，通过模块化指令生成、选择和提示，并考虑它们的组合和交互，使指令处理更加方便和高效。

    

    近年来，指令调整已经引起了越来越多的关注，并成为增强大型语言模型（LLMs）能力的一种关键技术。为了构建高质量的指令数据集，已经提出了许多指令处理方法，旨在在数据数量和数据质量之间达到精巧的平衡。然而，由于各种指令处理方法之间仍然存在不一致，目前没有标准的开源指令处理实现框架可供社区使用，这使得从业者无法进一步开发和推进。为了促进指令处理的研究和开发，我们提出了EasyInstruct，一个易于使用的用于LLMs的指令处理框架，它将指令生成、选择和提示模块化，并考虑它们的组合和交互。EasyInstruct已经在https://github.com/zjunlp/EasyInstruct上公开发布，并得到了积极维护。

    In recent years, instruction tuning has gained increasing attention and emerged as a crucial technique to enhance the capabilities of Large Language Models (LLMs). To construct high-quality instruction datasets, many instruction processing approaches have been proposed, aiming to achieve a delicate balance between data quantity and data quality. Nevertheless, due to inconsistencies that persist among various instruction processing methods, there is no standard open-source instruction processing implementation framework available for the community, which hinders practitioners from further developing and advancing. To facilitate instruction processing research and development, we present EasyInstruct, an easy-to-use instruction processing framework for LLMs, which modularizes instruction generation, selection, and prompting, while also considering their combination and interaction. EasyInstruct is publicly released and actively maintained at https://github.com/zjunlp/EasyInstruct, along 
    
[^7]: 通过潜在同构传播理解和引导弱监督的实体对齐

    Understanding and Guiding Weakly Supervised Entity Alignment with Potential Isomorphism Propagation

    [https://arxiv.org/abs/2402.03025](https://arxiv.org/abs/2402.03025)

    本文通过传播视角分析了弱监督的实体对齐任务，并提出一种潜在同构传播操作符来增强知识图谱之间的邻域信息传播。通过验证，发现基于聚合的实体对齐模型中的潜在对齐实体具有同构子图。

    

    弱监督的实体对齐是使用有限数量的种子对齐，在不同知识图谱之间识别等价实体的任务。尽管在基于聚合的弱监督实体对齐方面取得了重大进展，但在这种设置下的基本机制仍未被探索。在本文中，我们提出了一种传播视角来分析弱监督实体对齐，并解释了现有的基于聚合的实体对齐模型。我们的理论分析揭示了这些模型实质上是寻找用于对实体相似度进行传播的操作符。我们进一步证明，尽管不同知识图谱之间存在结构异质性，基于聚合的实体对齐模型中的潜在对齐实体具有同构子图，这是实体对齐的核心前提，但尚未被研究。利用这一洞见，我们引入了潜在同构传播操作符来增强跨知识图谱的邻域信息传播。我们开发了一个通用的实体对齐框架PipEA，实现了效果显著的实验结果。

    Weakly Supervised Entity Alignment (EA) is the task of identifying equivalent entities across diverse knowledge graphs (KGs) using only a limited number of seed alignments. Despite substantial advances in aggregation-based weakly supervised EA, the underlying mechanisms in this setting remain unexplored. In this paper, we present a propagation perspective to analyze weakly supervised EA and explain the existing aggregation-based EA models. Our theoretical analysis reveals that these models essentially seek propagation operators for pairwise entity similarities. We further prove that, despite the structural heterogeneity of different KGs, the potentially aligned entities within aggregation-based EA models have isomorphic subgraphs, which is the core premise of EA but has not been investigated. Leveraging this insight, we introduce a potential isomorphism propagation operator to enhance the propagation of neighborhood information across KGs. We develop a general EA framework, PipEA, inco
    
[^8]: 多语义搜索的领域自适应- 文献综述

    Domain Adaptation of Multilingual Semantic Search - Literature Review

    [https://arxiv.org/abs/2402.02932](https://arxiv.org/abs/2402.02932)

    这篇文献综述对当前在低资源环境下进行领域自适应和多语义搜索的方法进行了概述，提出了一种新的方法来聚类和有效地组合这些方法，并探讨了在低资源环境下将多语义搜索与领域自适应方法进行组合的可能性。

    

    这篇文献综述概述了在低资源环境下进行领域自适应和进行多语义搜索的当前方法。我们开发了一个新的分类方法来对领域自适应方法进行聚类，基于密集文本信息检索系统的部分，并注重如何有效地将它们进行组合。我们还探讨了将多语义搜索与低资源环境下的密集检索器的领域自适应方法进行组合的可能性。

    This literature review gives an overview of current approaches to perform domain adaptation in a low-resource and approaches to perform multilingual semantic search in a low-resource setting. We developed a new typology to cluster domain adaptation approaches based on the part of dense textual information retrieval systems, which they adapt, focusing on how to combine them efficiently. We also explore the possibilities of combining multilingual semantic search with domain adaptation approaches for dense retrievers in a low-resource setting.
    
[^9]: 动态稀疏学习：高效推荐的一种新范式

    Dynamic Sparse Learning: A Novel Paradigm for Efficient Recommendation

    [https://arxiv.org/abs/2402.02855](https://arxiv.org/abs/2402.02855)

    本文提出了一种针对推荐模型的新型学习范式，称为动态稀疏学习（DSL），通过从头训练一个轻量级稀疏模型，解决了模型大小和学习效率的问题。

    

    在基于深度学习的推荐系统领域中，日益增长的用户和物品数量所带来的计算需求增加，给实际部署带来了显著挑战。这个挑战主要有两个方面：在降低模型大小的同时，有效学习用户和物品表示以实现高效推荐。尽管模型压缩和架构搜索方面有了相当大的进展，但现有方法面临着明显的限制。其中包括模型压缩中预训练/重新训练的额外计算开销以及架构设计中广泛的搜索空间。此外，在具有严格时间或空间限制的情况下，管理复杂性和遵守内存限制是有问题的。为了解决这些问题，本文引入了一种新的学习范式，称为动态稀疏学习（DSL），专门用于推荐模型。DSL创新性地从头开始训练一个轻量级稀疏模型，周期的重构。

    In the realm of deep learning-based recommendation systems, the increasing computational demands, driven by the growing number of users and items, pose a significant challenge to practical deployment. This challenge is primarily twofold: reducing the model size while effectively learning user and item representations for efficient recommendations. Despite considerable advancements in model compression and architecture search, prevalent approaches face notable constraints. These include substantial additional computational costs from pre-training/re-training in model compression and an extensive search space in architecture design. Additionally, managing complexity and adhering to memory constraints is problematic, especially in scenarios with strict time or space limitations. Addressing these issues, this paper introduces a novel learning paradigm, Dynamic Sparse Learning (DSL), tailored for recommendation models. DSL innovatively trains a lightweight sparse model from scratch, periodi
    
[^10]: 对开放领域科学主张验证的知识来源的比较

    Comparing Knowledge Sources for Open-Domain Scientific Claim Verification

    [https://arxiv.org/abs/2402.02844](https://arxiv.org/abs/2402.02844)

    在本论文中，我们对开放领域科学主张验证系统的性能进行了测试，通过比较不同的知识来源（PubMed、维基百科、谷歌）和信息检索方法，我们发现在真实世界环境中进行科学主张验证的挑战和问题。

    

    科学知识的发现速度和在线健康主张的分享增加，凸显了为科学主张开发高效事实检核系统的重要性。现有文献中，这项任务的常见设置假设已经提供并注释或包含在有限语料库中的包含证据的文档。这使得该系统在可能需要查询数百万个文档以找到相关证据的现实世界环境中变得不切实际。在本文中，我们进行了一系列实验，以测试开放领域主张验证系统的性能。我们在不同设置下测试了四个数据集中的生物医学和健康主张系统的最终判断预测。在保持流水线的证据选择和判断预测部分不变的情况下，使用三种常见知识来源（PubMed、维基百科、谷歌）和两种不同的信息检索方法进行文档检索。

    The increasing rate at which scientific knowledge is discovered and health claims shared online has highlighted the importance of developing efficient fact-checking systems for scientific claims. The usual setting for this task in the literature assumes that the documents containing the evidence for claims are already provided and annotated or contained in a limited corpus. This renders the systems unrealistic for real-world settings where knowledge sources with potentially millions of documents need to be queried to find relevant evidence. In this paper, we perform an array of experiments to test the performance of open-domain claim verification systems. We test the final verdict prediction of systems on four datasets of biomedical and health claims in different settings. While keeping the pipeline's evidence selection and verdict prediction parts constant, document retrieval is performed over three common knowledge sources (PubMed, Wikipedia, Google) and using two different informati
    
[^11]: Trinity：将多/小众/长期兴趣整合为一个

    Trinity: Syncretizing Multi-/Long-tail/Long-term Interests All in One

    [https://arxiv.org/abs/2402.02842](https://arxiv.org/abs/2402.02842)

    本文提出了一种统一的兴趣建模框架“Trinity”，通过利用长期线索来解决兴趣遗忘问题，并改善多兴趣建模任务。通过构建实时聚类系统和计算统计兴趣直方图，Trinity能够识别用户的兴趣并进行个性化推荐。

    

    在推荐系统中的兴趣建模一直是改善用户体验的一个重要领域，许多现有工作已经研究了典型的兴趣建模任务（例如多兴趣、小众兴趣和长期兴趣）。然而，大多数研究只考虑了其中一个兴趣，并忽略了它们之间的相互关系。在本文中，我们认为这些任务面临共同的“兴趣遗忘”问题，而且存在一种解决方法可以同时减轻这个问题。我们认为长期线索可以成为基石，因为它们揭示了多种兴趣并澄清了小众兴趣。受到这个观察的启发，我们在检索阶段提出了一种新颖的统一框架“Trinity”，来解决兴趣遗忘问题并改善多兴趣建模任务。我们搭建了一个实时聚类系统，可以将物品投影到可枚举的簇中，并在这些簇上计算统计兴趣直方图。基于这些直方图，Trinity可以识别用户的兴趣，并进行个性化的推荐。

    Interest modeling in recommender system has been a constant topic for improving user experience, and typical interest modeling tasks (e.g. multi-interest, long-tail interest and long-term interest) have been investigated in many existing works. However, most of them only consider one interest in isolation, while neglecting their interrelationships. In this paper, we argue that these tasks suffer from a common "interest amnesia" problem, and a solution exists to mitigate it simultaneously. We figure that long-term cues can be the cornerstone since they reveal multi-interest and clarify long-tail interest. Inspired by the observation, we propose a novel and unified framework in the retrieval stage, "Trinity", to solve interest amnesia problem and improve multiple interest modeling tasks. We construct a real-time clustering system that enables us to project items into enumerable clusters, and calculate statistical interest histograms over these clusters. Based on these histograms, Trinity
    
[^12]: 推荐系统中的交叉双边公平性

    Intersectional Two-sided Fairness in Recommendation

    [https://arxiv.org/abs/2402.02816](https://arxiv.org/abs/2402.02816)

    本文针对推荐系统中的交叉双边公平性问题，提出了一种名为交叉双边公平推荐（ITFR）的新方法，通过利用锐度感知损失感知劣势群体，使用协作损失平衡开发不同交叉群体的一致区分能力，并利用预测得分归一化来公平对待不同交叉群体中的正例。实验证明该方法在提高交叉双边公平性方面取得了显著效果。

    

    近年来，推荐系统的公平性引起了越来越多的关注。根据涉及的利益相关者，推荐系统的公平性可分为用户公平性、物品公平性和同时考虑用户和物品公平性的双边公平性。然而，我们认为即使推荐系统是双边公平的，交叉双边不公平仍然可能存在，这在本文使用真实世界数据的实证研究中得到了观察和展示，并且以前尚未得到很好的研究。为了缓解这个问题，我们提出了一种新方法，称为交叉双边公平推荐（ITFR）。我们的方法利用一个锐度感知损失来感知劣势群体，然后使用协作损失平衡来开发不同交叉群体的一致区分能力。此外，我们利用预测得分归一化来调整正面预测得分，以公平地对待不同交叉群体中的正例。广泛的实验结果表明，我们的方法在提高交叉双边公平性方面取得了显著的效果。

    Fairness of recommender systems (RS) has attracted increasing attention recently. Based on the involved stakeholders, the fairness of RS can be divided into user fairness, item fairness, and two-sided fairness which considers both user and item fairness simultaneously. However, we argue that the intersectional two-sided unfairness may still exist even if the RS is two-sided fair, which is observed and shown by empirical studies on real-world data in this paper, and has not been well-studied previously. To mitigate this problem, we propose a novel approach called Intersectional Two-sided Fairness Recommendation (ITFR). Our method utilizes a sharpness-aware loss to perceive disadvantaged groups, and then uses collaborative loss balance to develop consistent distinguishing abilities for different intersectional groups. Additionally, predicted score normalization is leveraged to align positive predicted scores to fairly treat positives in different intersectional groups. Extensive experime
    
[^13]: 大型语言模型蒸馏药物推荐模型

    Large Language Model Distilling Medication Recommendation Model

    [https://arxiv.org/abs/2402.02803](https://arxiv.org/abs/2402.02803)

    本研究旨在利用大型语言模型改进药物推荐方法，以解决传统模型在医学数据语义理解和新患者处方推荐方面的挑战。

    

    药物推荐是智能医疗系统中的重要方面，它根据患者特定的健康需求来推荐最合适的药物。然而，目前使用的许多复杂模型往往忽视医学数据的细微语义，而只是过度依赖标识。此外，这些模型在处理首次访问医院的患者的情况时面临重大挑战，因为它们缺乏之前的处方历史可以参考。为了解决这些问题，我们利用大型语言模型（LLMs）强大的语义理解和输入不可知的特性。我们的研究旨在利用LLMs改进现有的药物推荐方法。在本文中，我们介绍了一种新颖的方法，称为大型语言模型蒸馏药物推荐（LEADER）。我们首先创建合适的提示模板，使LLMs能够有效地推荐药物。

    The recommendation of medication is a vital aspect of intelligent healthcare systems, as it involves prescribing the most suitable drugs based on a patient's specific health needs. Unfortunately, many sophisticated models currently in use tend to overlook the nuanced semantics of medical data, while only relying heavily on identities. Furthermore, these models face significant challenges in handling cases involving patients who are visiting the hospital for the first time, as they lack prior prescription histories to draw upon. To tackle these issues, we harness the powerful semantic comprehension and input-agnostic characteristics of Large Language Models (LLMs). Our research aims to transform existing medication recommendation methodologies using LLMs. In this paper, we introduce a novel approach called Large Language Model Distilling Medication Recommendation (LEADER). We begin by creating appropriate prompt templates that enable LLMs to suggest medications effectively. However, the
    
[^14]: 基于列表感知的重新排序-截断联合模型用于搜索和生成增强检索

    List-aware Reranking-Truncation Joint Model for Search and Retrieval-augmented Generation

    [https://arxiv.org/abs/2402.02764](https://arxiv.org/abs/2402.02764)

    该论文提出了一个基于列表感知的重新排序-截断联合模型，用于搜索和生成增强检索。该模型旨在捕捉列表级的上下文特征，通过重新排序和截断来返回更好的列表。先前的研究将重新排序和截断视为两个单独的任务，但这种分离不是最优的。因此，该论文提出的联合模型可以解决信息共享和错误积累的问题。

    

    信息检索的结果通常以排名列表的形式呈现，例如面向人类的网络搜索和面向大型语言模型的检索增强生成。列表感知检索旨在捕捉列表级的上下文特征，返回更好的列表，主要包括重新排序和截断。重新排序对列表中的文档进行精细重新评分。截断动态确定排名列表的截断点，以在整体相关性和避免无关文档的错误信息之间进行权衡。过去的研究将它们视为两个单独的任务并分别对其进行建模，然而，这种分离是不理想的。首先，很难在两个任务之间共享排名列表的上下文信息。其次，独立的流水线通常会遇到错误积累问题，即重新排序阶段的小错误可能会严重影响截断阶段。为了解决这些问题，我们提出了...（继续描述我们提出的方法）

    The results of information retrieval (IR) are usually presented in the form of a ranked list of candidate documents, such as web search for humans and retrieval-augmented generation for large language models (LLMs). List-aware retrieval aims to capture the list-level contextual features to return a better list, mainly including reranking and truncation. Reranking finely re-scores the documents in the list. Truncation dynamically determines the cut-off point of the ranked list to achieve the trade-off between overall relevance and avoiding misinformation from irrelevant documents. Previous studies treat them as two separate tasks and model them separately. However, the separation is not optimal. First, it is hard to share the contextual information of the ranking list between the two tasks. Second, the separate pipeline usually meets the error accumulation problem, where the small error from the reranking stage can largely affect the truncation stage. To solve these problems, we propose
    
[^15]: 降噪时间周期建模用于推荐

    Denoising Time Cycle Modeling for Recommendation

    [https://arxiv.org/abs/2402.02718](https://arxiv.org/abs/2402.02718)

    本文提出了一种新的方法，降噪时间周期建模（DiCycle），用于在推荐系统中建模用户-物品交互的时间模式。通过选择与目标物品高度相关的用户行为子集，DiCycle能够更好地建模时间周期模式，从而提高推荐性能。

    

    最近，在推荐系统中，建模用户-物品交互的时间模式引起了广泛关注。我们认为现有方法忽略了用户行为的时间模式的多样性。我们将与目标物品无关的用户行为定义为噪声，这限制了与目标相关的时间周期建模的性能，并影响了推荐性能。在本文中，我们提出了一种新颖的方法，称为降噪时间周期建模（DiCycle），用于降噪用户行为并选择与目标物品高度相关的用户行为子集。DiCycle能够明确地建模多样化的时间周期模式以进行推荐。我们在公共基准数据集和真实世界数据集上进行了大量实验，结果表明DiCycle相比最先进的推荐方法具有更好的性能。

    Recently, modeling temporal patterns of user-item interactions have attracted much attention in recommender systems. We argue that existing methods ignore the variety of temporal patterns of user behaviors. We define the subset of user behaviors that are irrelevant to the target item as noises, which limits the performance of target-related time cycle modeling and affect the recommendation performance. In this paper, we propose Denoising Time Cycle Modeling (DiCycle), a novel approach to denoise user behaviors and select the subset of user behaviors that are highly related to the target item. DiCycle is able to explicitly model diverse time cycle patterns for recommendation. Extensive experiments are conducted on both public benchmarks and a real-world dataset, demonstrating the superior performance of DiCycle over the state-of-the-art recommendation methods.
    
[^16]: 特征中的位置偏差

    Position bias in features

    [https://arxiv.org/abs/2402.02626](https://arxiv.org/abs/2402.02626)

    本文提出了一种用于搜索引擎的动态排名系统的特征，该特征可以准确估计文档相关性，但是在存在位置偏差的情况下会有高方差，并强调了准确估计位置偏差的必要性，建议同时使用有偏和无偏的位置偏差特征。

    

    搜索引擎中建模文件相关性的目的是在后续搜索中更好地排名。文档特定的历史点击率可以作为动态排名系统中的重要特征，随着我们积累更多样本，系统会进行更新。本文描述了几种这样的特征的属性，并在控制实验中对其进行了测试。将反向倾向加权方法扩展到文档上可以产生对文档相关性的无偏估计。这个特征可以准确地近似相关性，在理想情况下可以实现接近最优的排名。然而，它具有高方差，且随着位置偏差的程度增加而增加。此外，不准确的位置偏差估计会导致性能不佳。在几种情况下，这个特征的表现可能不如偏倚的点击率。本文强调了准确的位置偏差估计的必要性，并独特地建议同时使用有偏和无偏的位置偏差特征。

    The purpose of modeling document relevance for search engines is to rank better in subsequent searches. Document-specific historical click-through rates can be important features in a dynamic ranking system which updates as we accumulate more sample. This paper describes the properties of several such features, and tests them in controlled experiments. Extending the inverse propensity weighting method to documents creates an unbiased estimate of document relevance. This feature can approximate relevance accurately, leading to near-optimal ranking in ideal circumstances. However, it has high variance that is increasing with respect to the degree of position bias. Furthermore, inaccurate position bias estimation leads to poor performance. Under several scenarios this feature can perform worse than biased click-through rates. This paper underscores the need for accurate position bias estimation, and is unique in suggesting simultaneous use of biased and unbiased position bias features.
    
[^17]: 可解释的贝叶斯多透视生成检索

    eXplainable Bayesian Multi-Perspective Generative Retrieval

    [https://arxiv.org/abs/2402.02418](https://arxiv.org/abs/2402.02418)

    本文将不确定性校准和可解释性整合到一个检索流程中，通过引入贝叶斯方法和多透视检索来校准不确定性。使用解释方法分析黑盒模型行为，并将重要性分数作为补充相关性分数，提升基础模型性能。实验证明我们的方法在问答和事实检验任务上显著提高了性能。

    

    现代确定性检索流水线注重实现最先进的性能，但在决策过程中通常缺乏可解释性。这些模型在评估不确定性时面临挑战，导致过于自信的预测。为了克服这些限制，我们将不确定性校准和可解释性整合到检索流程中。具体而言，我们引入了贝叶斯方法和多透视检索来校准检索流程中的不确定性。我们结合LIME和SHAP等技术来分析黑盒重排序模型的行为。从这些解释方法中得出的重要性分数作为补充相关性分数，以增强基础重排序模型。我们在问答和事实检验任务上评估通过不确定性校准和可解释性重排实现的性能提升。我们的方法在三个KILT数据集上展示出了明显的性能改善。

    Modern deterministic retrieval pipelines prioritize achieving state-of-the-art performance but often lack interpretability in decision-making. These models face challenges in assessing uncertainty, leading to overconfident predictions. To overcome these limitations, we integrate uncertainty calibration and interpretability into a retrieval pipeline. Specifically, we introduce Bayesian methodologies and multi-perspective retrieval to calibrate uncertainty within a retrieval pipeline. We incorporate techniques such as LIME and SHAP to analyze the behavior of a black-box reranker model. The importance scores derived from these explanation methodologies serve as supplementary relevance scores to enhance the base reranker model. We evaluate the resulting performance enhancements achieved through uncertainty calibration and interpretable reranking on Question Answering and Fact Checking tasks. Our methods demonstrate substantial performance improvements across three KILT datasets.
    
[^18]: ExTTNet:一种用于从发票图像中提取表格文字的深度学习算法

    ExTTNet: A Deep Learning Algorithm for Extracting Table Texts from Invoice Images

    [https://arxiv.org/abs/2402.02246](https://arxiv.org/abs/2402.02246)

    ExTTNet是一种深度学习算法，能够自动从发票图像中提取表格文字。使用光学字符识别（OCR）技术获取文本，并通过特征提取方法提高准确度。通过多层神经网络模型进行训练，最终获得了0.92的F1分数。

    

    在这项工作中，通过一个被称为ExTTNet的深度学习模型，自动地从发票图像中提取产品表格。首先使用光学字符识别（OCR）技术从发票图像中获取文本。在此过程中，使用了Tesseract OCR引擎 [37]。然后，通过使用特征提取方法增加现有特征的数量来提高准确度。根据每个OCR获得的文本是否是表格元素，进行标记处理。在本研究中，使用了多层人工神经网络模型进行训练。使用Nvidia RTX 3090显卡进行训练，耗时162分钟。训练的结果，F1分数为0.92。

    In this work, product tables in invoices are obtained autonomously via a deep learning model, which is named as ExTTNet. Firstly, text is obtained from invoice images using Optical Character Recognition (OCR) techniques. Tesseract OCR engine [37] is used for this process. Afterwards, the number of existing features is increased by using feature extraction methods to increase the accuracy. Labeling process is done according to whether each text obtained as a result of OCR is a table element or not. In this study, a multilayer artificial neural network model is used. The training has been carried out with an Nvidia RTX 3090 graphics card and taken $162$ minutes. As a result of the training, the F1 score is $0.92$.
    
[^19]: 跨域推荐中的扩散模型

    Diffusion Cross-domain Recommendation

    [https://arxiv.org/abs/2402.02182](https://arxiv.org/abs/2402.02182)

    该论文研究了跨域推荐中的扩散模型，通过从辅助领域中添加数据来解决冷启动用户数据稀疏问题。其中，映射模块起着关键的作用，确定了跨域推荐模型的性能。

    

    对于推荐系统来说，给与冷启动用户高质量的推荐结果一直是一个挑战。解决目标领域中冷启动用户数据稀疏问题的一个潜在解决方案是从辅助领域添加数据。找到一种合适的方法从辅助领域提取知识并将其转移到目标领域是跨域推荐（CDR）研究的主要目标之一。在现有的方法中，映射方法是一种流行的方法来实现跨域推荐模型（CDRs）。对于这种类型的模型，映射模块起着将数据从一个域转换到另一个域的作用。它主要决定了映射方法CDRs的性能。最近，扩散概率模型（DPMs）在图像合成相关任务中取得了令人印象深刻的成功。它们涉及从添加噪声的样本中恢复图像，这可以看作是具有出色性能的数据转换过程。

    It is always a challenge for recommender systems to give high-quality outcomes to cold-start users. One potential solution to alleviate the data sparsity problem for cold-start users in the target domain is to add data from the auxiliary domain. Finding a proper way to extract knowledge from an auxiliary domain and transfer it into a target domain is one of the main objectives for cross-domain recommendation (CDR) research. Among the existing methods, mapping approach is a popular one to implement cross-domain recommendation models (CDRs). For models of this type, a mapping module plays the role of transforming data from one domain to another. It primarily determines the performance of mapping approach CDRs. Recently, diffusion probability models (DPMs) have achieved impressive success for image synthesis related tasks. They involve recovering images from noise-added samples, which can be viewed as a data transformation process with outstanding performance. To further enhance the perfo
    
[^20]: 通过证据模式检索增强知识图谱上的复杂问题回答

    Enhancing Complex Question Answering over Knowledge Graphs through Evidence Pattern Retrieval

    [https://arxiv.org/abs/2402.02175](https://arxiv.org/abs/2402.02175)

    本论文提出了一种名为EPR的方法，通过索引原子邻接模式并将其组合，明确建模结构依赖，并在知识图谱中进行子图提取。实验结果表明，EPR方法在复杂问题回答方面取得了显著的改进，提高了F1得分，并在WebQuestionsSP上表现出竞争力。

    

    知识图谱问答系统的信息检索方法包括两个阶段：子图提取和答案推理。我们认为当前的子图提取方法低估了证据事实之间的结构依赖的重要性。我们提出了证据模式检索（EPR）来在子图提取过程中明确地建模结构依赖。我们通过索引资源对的原子邻接模式来实现EPR。给定一个问题，我们进行密集检索以获取由资源对形成的原子模式。然后，我们枚举它们的组合以构建候选证据模式。这些证据模式使用神经模型进行打分，并选择最佳模式来提取下游答案推理所需的子图。实验结果表明，基于EPR的方法在ComplexWebQuestions上将IR-KGQA方法的F1得分显著提高了10个百分点以上，并在WebQuestionsSP上取得了有竞争力的性能。

    Information retrieval (IR) methods for KGQA consist of two stages: subgraph extraction and answer reasoning. We argue current subgraph extraction methods underestimate the importance of structural dependencies among evidence facts. We propose Evidence Pattern Retrieval (EPR) to explicitly model the structural dependencies during subgraph extraction. We implement EPR by indexing the atomic adjacency pattern of resource pairs. Given a question, we perform dense retrieval to obtain atomic patterns formed by resource pairs. We then enumerate their combinations to construct candidate evidence patterns. These evidence patterns are scored using a neural model, and the best one is selected to extract a subgraph for downstream answer reasoning. Experimental results demonstrate that the EPR-based approach has significantly improved the F1 scores of IR-KGQA methods by over 10 points on ComplexWebQuestions and achieves competitive performance on WebQuestionsSP.
    
[^21]: PatSTEG：通过语义-拓扑进化图对专利引用网络的形成动态建模

    PatSTEG: Modeling Formation Dynamics of Patent Citation Networks via The Semantic-Topological Evolutionary Graph

    [https://arxiv.org/abs/2402.02158](https://arxiv.org/abs/2402.02158)

    本文提出了一种联合语义-拓扑进化图学习方法（PatSTEG），用于建模专利引用网络的形成动态。通过考虑语义和拓扑信息，该方法能够探索专利数据库中复杂的文本和引用网络结构，并为专利数据挖掘提供新的机会。

    

    专利数据库中的专利文件对于研究、开发和创新至关重要，因为它们包含了宝贵的技术信息。然而，与公开可用的预处理数据库相比，专利数据库存在多方面的挑战，原因是专利文本的复杂性以及专利引用网络内在的稀疏性。尽管专利文本分析和引文分析为探索专利数据挖掘带来了新的机会，但目前不存在利用它们的互补性的研究。为此，我们提出了一种联合语义-拓扑进化图学习方法（PatSTEG）来建模专利引用网络的形成动态。具体而言，我们首先创建了一个名为CNPat的中国专利的实际数据集，并利用其专利文本和引文构建了一个专利引用网络。然后，通过考虑语义和拓扑信息，建立了PatSTEG来研究专利引用形成的进化动态。

    Patent documents in the patent database (PatDB) are crucial for research, development, and innovation as they contain valuable technical information. However, PatDB presents a multifaceted challenge compared to publicly available preprocessed databases due to the intricate nature of the patent text and the inherent sparsity within the patent citation network. Although patent text analysis and citation analysis bring new opportunities to explore patent data mining, no existing work exploits the complementation of them. To this end, we propose a joint semantic-topological evolutionary graph learning approach (PatSTEG) to model the formation dynamics of patent citation networks. More specifically, we first create a real-world dataset of Chinese patents named CNPat and leverage its patent texts and citations to construct a patent citation network. Then, PatSTEG is modeled to study the evolutionary dynamics of patent citation formation by considering the semantic and topological information
    
[^22]: 论文题目：为什么“摸着石头过河”方法主导推荐系统实践；呼吁摒弃反乌托邦思维

    Position Paper: Why the Shooting in the Dark Method Dominates Recommender Systems Practice; A Call to Abandon Anti-Utopian Thinking

    [https://arxiv.org/abs/2402.02152](https://arxiv.org/abs/2402.02152)

    这篇论文质疑了推荐系统实践中目前常用的“摸着石头过河”方法，呼吁摒弃反乌托邦思维。论文提出了使用深度学习堆栈的非标准用法，以解锁奖励优化的推荐系统的潜力。

    

    应用推荐系统研究处于一种奇特的境地。尽管在通过A/B测试来衡量性能方面有一个非常严格的协议，但找到要测试的“B”的最佳方法并没有明确地针对性能，而是针对一个代理指标。因此，一个A/B测试的成功或失败完全取决于所提出的代理指标是否与性能相关性更好。没有原则可以在离线情况下确定一个代理指标是否比另一个更好，这使得从业者们摸不着头脑。本论文的目的是质疑这种反乌托邦思维，并主张深度学习堆栈的非标准用法实际上有潜力解锁优化奖励的推荐系统。

    Applied recommender systems research is in a curious position. While there is a very rigorous protocol for measuring performance by A/B testing, best practice for finding a `B' to test does not explicitly target performance but rather targets a proxy measure. The success or failure of a given A/B test then depends entirely on if the proposed proxy is better correlated to performance than the previous proxy. No principle exists to identify if one proxy is better than another offline, leaving the practitioners shooting in the dark. The purpose of this position paper is to question this anti-Utopian thinking and argue that a non-standard use of the deep learning stacks actually has the potential to unlock reward optimizing recommendation.
    
[^23]: 基于对齐和一致性的原型对比学习用于推荐

    Prototypical Contrastive Learning through Alignment and Uniformity for Recommendation

    [https://arxiv.org/abs/2402.02079](https://arxiv.org/abs/2402.02079)

    本研究提出了一种基于对齐和一致性的原型对比学习方法（ProtoAU）用于推荐系统中，该方法解决了随机抽样引起的抽样偏差问题，提高了特征表示的有效性。

    

    图协同过滤（GCF）是最常使用的推荐系统方法之一，有效地捕捉用户和物品之间复杂的关系。基于图的对比学习（GCL）的GCF已经受到了显著关注，因为它利用自监督技术从现实场景中提取有价值的信号。然而，许多方法通常通过随机抽样学习涉及构建对比对的鉴别任务的实例。GCL方法面临抽样偏差问题，负样本可能具有与正样本类似的语义结构，导致特征表示的有效性下降。为了解决这些问题，我们提出了基于对齐和一致性的原型对比学习用于推荐，称为ProtoAU。具体而言，我们首先提出了原型（聚类中心）作为一个潜在空间，以确保一致性。

    Graph Collaborative Filtering (GCF), one of the most widely adopted recommendation system methods, effectively captures intricate relationships between user and item interactions. Graph Contrastive Learning (GCL) based GCF has gained significant attention as it leverages self-supervised techniques to extract valuable signals from real-world scenarios. However, many methods usually learn the instances of discrimination tasks that involve the construction of contrastive pairs through random sampling. GCL approaches suffer from sampling bias issues, where the negatives might have a semantic structure similar to that of the positives, thus leading to a loss of effective feature representation. To address these problems, we present the \underline{Proto}typical contrastive learning through \underline{A}lignment and \underline{U}niformity for recommendation, which is called \textbf{ProtoAU}. Specifically, we first propose prototypes (cluster centroids) as a latent space to ensure consistency 
    
[^24]: 用于流式向量搜索的局部自适应量化

    Locally-Adaptive Quantization for Streaming Vector Search

    [https://arxiv.org/abs/2402.02044](https://arxiv.org/abs/2402.02044)

    本文研究了局部自适应量化在流式相似度搜索中的应用，引入了Turbo LVQ和multi-means LVQ两种改进方法，分别提升了搜索性能28%和27%。实验证明，LVQ及其新变体使得向量搜索变得极快。

    

    在大量向量集合中找到与给定查询最相似的向量嵌入一直是无数实际应用的关键组成部分。最近引入的检索增强生成就是其中最突出的例子之一。对于许多这些应用，数据库通过插入新数据和删除过时数据而随时间演变。在这些情况下，检索问题被称为流式相似度搜索。虽然局部自适应向量量化（LVQ）作为一种高效的向量压缩方法，在非演化数据库中具有最先进的搜索性能，但其在流式场景中的实用性尚未确定。在这项工作中，我们研究了LVQ在流式相似度搜索中的应用。为了支持我们的评估，我们引入了两种LVQ的改进：Turbo LVQ和multi-means LVQ，分别提升了其搜索性能高达28%和27%。我们的研究表明，LVQ及其新变体使得向量搜索变得极快。

    Retrieving the most similar vector embeddings to a given query among a massive collection of vectors has long been a key component of countless real-world applications. The recently introduced Retrieval-Augmented Generation is one of the most prominent examples. For many of these applications, the database evolves over time by inserting new data and removing outdated data. In these cases, the retrieval problem is known as streaming similarity search. While Locally-Adaptive Vector Quantization (LVQ), a highly efficient vector compression method, yields state-of-the-art search performance for non-evolving databases, its usefulness in the streaming setting has not been yet established. In this work, we study LVQ in streaming similarity search. In support of our evaluation, we introduce two improvements of LVQ: Turbo LVQ and multi-means LVQ that boost its search performance by up to 28% and 27%, respectively. Our studies show that LVQ and its new variants enable blazing fast vector search,
    
[^25]: 使用标签自编码器改进大规模k最近邻文本分类

    Improving Large-Scale k-Nearest Neighbor Text Categorization with Label Autoencoders

    [https://arxiv.org/abs/2402.01963](https://arxiv.org/abs/2402.01963)

    本文提出了一种使用标签自编码器改进大规模k最近邻文本分类的方法，通过将大的标签空间映射到一个缩小的潜在空间，并从该潜在空间中重建出预测的标签，以解决在大型文档集合中处理自动语义索引的问题。

    

    本文介绍了一种多标签惰性学习方法，用于在复杂和结构化标签词汇具有高相关性的大型文档集合中处理自动语义索引。所提出的方法是传统k最近邻算法的演化，它使用一个大的自编码器来将大的标签空间映射到一个缩小的潜在空间，并从该潜在空间中重建出预测的标签。我们在MEDLINE生物医学文档集合的大部分上评估了我们的提议，该集合使用医学主题词（MeSH）词汇表作为控制词汇。在我们的实验中，我们提出并评估了几种文档表示方法和不同的标签自编码器配置。

    In this paper, we introduce a multi-label lazy learning approach to deal with automatic semantic indexing in large document collections in the presence of complex and structured label vocabularies with high inter-label correlation. The proposed method is an evolution of the traditional k-Nearest Neighbors algorithm which uses a large autoencoder trained to map the large label space to a reduced size latent space and to regenerate the predicted labels from this latent space. We have evaluated our proposal in a large portion of the MEDLINE biomedical document collection which uses the Medical Subject Headings (MeSH) thesaurus as a controlled vocabulary. In our experiments we propose and evaluate several document representation approaches and different label autoencoder configurations.
    
[^26]: 澄清路径对用户满意度的影响：对澄清问题有用性的研究

    Clarifying the Path to User Satisfaction: An Investigation into Clarification Usefulness

    [https://arxiv.org/abs/2402.01934](https://arxiv.org/abs/2402.01934)

    本研究探讨了澄清问题对用户满意度的重要影响，研究发现具体问题、问题的主观性和情感色彩以及查询长度等特征对澄清的有效性有明显影响。

    

    澄清问题是现代信息检索系统的重要组成部分，直接影响用户满意度和整体系统性能。错误构建的问题可能导致用户的不满和困惑，进而对系统的性能产生负面影响。本研究旨在迫切需要确定和利用有助于分类澄清问题的关键特征，以提升用户满意度。为了更深入地了解不同特征对用户满意度的影响，我们进行了全面的分析，考虑了广泛的词汇、语义和统计特征，如问题长度和情感极性等。我们的实证结果提供了关于有效查询澄清的三个主要见解：（1）具体的问题比泛化的问题更有效；（2）问题的主观性和情感色彩起到一定作用；（3）较短和更模糊的查询在澄清过程中得益良多。

    Clarifying questions are an integral component of modern information retrieval systems, directly impacting user satisfaction and overall system performance. Poorly formulated questions can lead to user frustration and confusion, negatively affecting the system's performance. This research addresses the urgent need to identify and leverage key features that contribute to the classification of clarifying questions, enhancing user satisfaction. To gain deeper insights into how different features influence user satisfaction, we conduct a comprehensive analysis, considering a broad spectrum of lexical, semantic, and statistical features, such as question length and sentiment polarity. Our empirical results provide three main insights into the qualities of effective query clarification: (1) specific questions are more effective than generic ones; (2) the subjectivity and emotional tone of a question play a role; and (3) shorter and more ambiguous queries benefit significantly from clarificat
    
[^27]: CoLe和LYS在BioASQ MESINESP8任务中的表征分配的相似度方法

    CoLe and LYS at BioASQ MESINESP8 Task: similarity based descriptor assignment in Spanish

    [https://arxiv.org/abs/2402.01916](https://arxiv.org/abs/2402.01916)

    这篇论文介绍了基于相似度的描述符分配方法在BioASQ MESINESP8任务中的应用，通过使用传统的信息检索工具，并提出了适用于西班牙语等语言的方法。

    

    在本文中，我们描述了我们参与了BioASQ生物医学语义指标挑战赛的MESINESP任务。参与的系统仅使用了传统的信息检索工具。我们评估了从IBECS/LILACS文档中提取索引术语的多种方法，以便存储在Apache Lucene索引中。这些索引表示使用要注释的文章内容进行查询，并从检索到的文档创建一个候选标签的排序列表。我们还评估了一种有限的标签全集方法，该方法通过将具有高共现得分的DeCS标签配对并创建元标签，以及一种基于标签个人资料匹配的替代方法。在官方运行中获得的结果似乎证实了这种方法在西班牙语等语言中的适用性。

    In this paper, we describe our participation in the MESINESP Task of the BioASQ biomedical semantic indexing challenge. The participating system follows an approach based solely on conventional information retrieval tools. We have evaluated various alternatives for extracting index terms from IBECS/LILACS documents in order to be stored in an Apache Lucene index. Those indexed representations are queried using the contents of the article to be annotated and a ranked list of candidate labels is created from the retrieved documents. We also have evaluated a sort of limited Label Powerset approach which creates meta-labels joining pairs of DeCS labels with high co-occurrence scores, and an alternative method based on label profile matching. Results obtained in official runs seem to confirm the suitability of this approach for languages like Spanish.
    
[^28]: Zero-1-to-3: 基于领域级零样本认知诊断的一批早鸟学生关于三个诊断目标的方法

    Zero-1-to-3: Domain-level Zero-shot Cognitive Diagnosis via One Batch of Early-bird Students towards Three Diagnostic Objectives

    [https://arxiv.org/abs/2312.13434](https://arxiv.org/abs/2312.13434)

    本论文提出了一种Zero-1-to-3的方法，该方法通过一批早鸟学生关注三个诊断目标，以实现领域级零样本认知诊断。它解决了跨领域诊断模型中可能融入不可转移信息的问题，从而提高了知识传输的效果。

    

    认知诊断通过探索学生的练习测验数据来估计他们的认知状态。它在智能教育系统中的个性化学习指导中起着关键作用。本文关注一个重要但常常被忽视的任务：领域级零样本认知诊断（DZCD），因为在新启动的领域中缺乏学生的练习记录而产生。最近的跨领域诊断模型已经证明是DZCD的一种有前途的策略。这些方法主要关注如何在领域之间传递学生状态。然而，它们可能会无意中将不可转移的信息融入到学生的表示中，从而限制了知识传输的效果。为了解决这个问题，我们提出了Zero-1-to-3，一种基于领域级零样本认知诊断框架，通过一批早鸟学生实现三个诊断目标。我们的方法首先使用预训练的诊断模型进行训练。

    Cognitive diagnosis seeks to estimate the cognitive states of students by exploring their logged practice quiz data. It plays a pivotal role in personalized learning guidance within intelligent education systems. In this paper, we focus on an important, practical, yet often underexplored task: domain-level zero-shot cognitive diagnosis (DZCD), which arises due to the absence of student practice logs in newly launched domains. Recent cross-domain diagnostic models have been demonstrated to be a promising strategy for DZCD. These methods primarily focus on how to transfer student states across domains. However, they might inadvertently incorporate non-transferable information into student representations, thereby limiting the efficacy of knowledge transfer. To tackle this, we propose Zero-1-to-3, a domain-level zero-shot cognitive diagnosis framework via one batch of early-bird students towards three diagnostic objectives. Our approach initiates with pre-training a diagnosis model with d
    
[^29]: 基于知识图谱驱动的图神经网络推荐模型

    Knowledge graph driven recommendation model of graph neural network. (arXiv:2401.10244v1 [cs.IR])

    [http://arxiv.org/abs/2401.10244](http://arxiv.org/abs/2401.10244)

    提出了一种基于知识图谱的图神经网络推荐模型KGLN，通过合并节点特征、调整聚合权重和迭代演化，提高了个性化推荐的准确性和效果。在实验中相对于已有基准方法，KGLN在不同数据集上的AUC提高了0.3%至5.9%和1.1%至8.2%。

    

    提出了一种新的基于图神经网络的推荐模型KGLN，该模型利用知识图谱（KG）信息，提高了个性化推荐的准确性和效果。该模型首先利用单层神经网络将图中的个体节点特征合并，然后通过结合影响因素调整相邻实体的聚合权重。通过迭代，模型从单层逐渐演变为多层，使实体能够获取丰富的多阶关联实体信息。最后，将实体和用户的特征结合起来产生推荐分数。通过比较不同聚合方法和影响因素的效果，评估了模型的性能。在使用MovieLen-1M和Book-Crossing数据集进行测试时，KGLN相对于LibFM和D等已有基准方法，AUC（ROC曲线下的面积）提高了0.3%至5.9%和1.1%至8.2%。

    A new graph neural network-based recommendation model called KGLN, which leverages Knowledge Graph (KG) information, was developed to enhance the accuracy and effectiveness of personalized recommendations. This model begins by using a single-layer neural network to merge individual node features in the graph. It then adjusts the aggregation weights of neighboring entities by incorporating influence factors. The model evolves from a single layer to multiple layers through iteration, enabling entities to access extensive multi-order associated entity information. The final step involves integrating features of entities and users to produce a recommendation score. The model's performance was evaluated by comparing its effects on various aggregation methods and influence factors. In tests using the MovieLen-1M and Book-Crossing datasets, KGLN showed an AUC (Area Under the ROC curve) improvement of 0.3% to 5.9% and 1.1% to 8.2%, respectively, over established benchmark methods like LibFM, D
    
[^30]: 基于频谱的图神经网络用于互补商品推荐

    Spectral-based Graph Neutral Networks for Complementary Item Recommendation. (arXiv:2401.02130v1 [cs.IR])

    [http://arxiv.org/abs/2401.02130](http://arxiv.org/abs/2401.02130)

    本文提出了一种基于频谱的图神经网络方法（SComGNN）用于模拟和理解商品间的互补关系，以在推荐系统中准确和及时地推荐后续商品。

    

    模拟互补关系极大地帮助推荐系统在购买一个商品后准确和及时地推荐后续的商品。与传统的相似关系不同，具有互补关系的商品可能会连续购买（例如iPhone和AirPods Pro），它们不仅共享相关性，还展现出不相似性。由于这两个属性是相反的，建模互补关系具有挑战性。先前尝试利用这些关系的方法要么忽视了或过度简化了不相似性属性，导致建模无效并且无法平衡这两个属性。由于图神经网络（GNNs）可以在频谱域中捕捉节点之间的相关性和不相似性，我们可以利用基于频谱的GNNs有效地理解和建模互补关系。在本研究中，我们提出了一种新方法，称为基于频谱的互补图神经网络（SComGNN），利用这一方法可以比较好地利用互补关系。

    Modeling complementary relationships greatly helps recommender systems to accurately and promptly recommend the subsequent items when one item is purchased. Unlike traditional similar relationships, items with complementary relationships may be purchased successively (such as iPhone and Airpods Pro), and they not only share relevance but also exhibit dissimilarity. Since the two attributes are opposites, modeling complementary relationships is challenging. Previous attempts to exploit these relationships have either ignored or oversimplified the dissimilarity attribute, resulting in ineffective modeling and an inability to balance the two attributes. Since Graph Neural Networks (GNNs) can capture the relevance and dissimilarity between nodes in the spectral domain, we can leverage spectral-based GNNs to effectively understand and model complementary relationships. In this study, we present a novel approach called Spectral-based Complementary Graph Neural Networks (SComGNN) that utilize
    
[^31]: ESG主导的DLT研究的演化：对文献进行NLP分析

    Evolution of ESG-focused DLT Research: An NLP Analysis of the Literature. (arXiv:2308.12420v1 [cs.IR])

    [http://arxiv.org/abs/2308.12420](http://arxiv.org/abs/2308.12420)

    本研究通过NLP分析了ESG主导的DLT研究的演化，通过构建引用网络和命名实体识别任务，对DLT在ESG背景下的发展进行了文献综述。

    

    分布式账本技术(DLT)迅速发展，需要全面了解其各个组成部分。然而，针对DLT的环境、可持续性和治理(ESG)组成部分的系统文献综述还不足。为填补这一空白，我们选择了107篇种子文献，构建了一个包含63,083个参考文献的引用网络，并将其精炼为24,539篇文献的语料库进行分析。然后，我们根据一个已建立的技术分类法从46篇论文中标记了命名实体，并通过找出DLT的ESG要素来完善这个分类法。利用基于transformer的语言模型，我们对一个预先训练的语言模型进行了细化调整，用于命名实体识别任务，使用我们标记的数据集。我们利用我们调整后的语言模型对语料库进行了精简，得到了505篇关键论文，通过命名实体和时间图分析，促进了对DLT在ESG背景下的演化的文献综述。

    Distributed Ledger Technologies (DLTs) have rapidly evolved, necessitating comprehensive insights into their diverse components. However, a systematic literature review that emphasizes the Environmental, Sustainability, and Governance (ESG) components of DLT remains lacking. To bridge this gap, we selected 107 seed papers to build a citation network of 63,083 references and refined it to a corpus of 24,539 publications for analysis. Then, we labeled the named entities in 46 papers according to twelve top-level categories derived from an established technology taxonomy and enhanced the taxonomy by pinpointing DLT's ESG elements. Leveraging transformer-based language models, we fine-tuned a pre-trained language model for a Named Entity Recognition (NER) task using our labeled dataset. We used our fine-tuned language model to distill the corpus to 505 key papers, facilitating a literature review via named entities and temporal graph analysis on DLT evolution in the context of ESG. Our con
    
[^32]: 提高ChatGPT生成的假科学检测的方法：引入xFakeBibs监督学习网络算法

    Improving Detection of ChatGPT-Generated Fake Science Using Real Publication Text: Introducing xFakeBibs a Supervised-Learning Network Algorithm. (arXiv:2308.11767v1 [cs.CL])

    [http://arxiv.org/abs/2308.11767](http://arxiv.org/abs/2308.11767)

    本文介绍了一种能够提高对ChatGPT生成的假科学进行检测的算法。通过使用一种新设计的监督机器学习算法，该算法能够准确地将机器生成的出版物与科学家生成的出版物区分开来。结果表明，ChatGPT在技术术语方面与真实科学存在显著差异。算法在分类过程中取得了较高的准确率。

    

    ChatGPT正在成为现实。本文展示了如何区分ChatGPT生成的出版物与科学家生成的出版物。通过使用一种新设计的监督机器学习算法，我们演示了如何检测机器生成的出版物和科学家生成的出版物。该算法使用100个真实出版物摘要进行训练，然后采用10倍交叉验证方法建立了一个接受范围的下限和上限。与ChatGPT内容进行比较，明显可见ChatGPT仅贡献了23\%的二元组内容，这比其他10个交叉验证中的任何一个都少50\%。这个分析凸显了ChatGPT在技术术语上与真实科学的明显差异。在对每篇文章进行分类时，xFakeBibs算法准确地将98篇出版物识别为假的，有2篇文献错误地分类为真实出版物。尽管这项工作引入了一种算法应用

    ChatGPT is becoming a new reality. In this paper, we show how to distinguish ChatGPT-generated publications from counterparts produced by scientists. Using a newly designed supervised Machine Learning algorithm, we demonstrate how to detect machine-generated publications from those produced by scientists. The algorithm was trained using 100 real publication abstracts, followed by a 10-fold calibration approach to establish a lower-upper bound range of acceptance. In the comparison with ChatGPT content, it was evident that ChatGPT contributed merely 23\% of the bigram content, which is less than 50\% of any of the other 10 calibrating folds. This analysis highlights a significant disparity in technical terms where ChatGPT fell short of matching real science. When categorizing the individual articles, the xFakeBibs algorithm accurately identified 98 out of 100 publications as fake, with 2 articles incorrectly classified as real publications. Though this work introduced an algorithmic app
    
[^33]: 自然语言是图表所需要的全部内容

    Natural Language is All a Graph Needs. (arXiv:2308.07134v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2308.07134](http://arxiv.org/abs/2308.07134)

    本论文提出了一种名为InstructGLM的结构化语言模型算法，该算法将大型语言模型与图表学习问题相结合，旨在探索是否可以用语言模型取代图神经网络作为图表的基础模型。

    

    大规模预训练语言模型的出现，如ChatGPT，已经在人工智能的各个研究领域中引起了革命。基于Transformer的大型语言模型（LLMs）逐渐取代了CNN和RNN，将计算机视觉和自然语言处理领域统一起来。与相对独立存在的数据（如图像、视频或文本）相比，图表是一种包含丰富结构和关系信息的数据类型。同时，作为最具表现力的媒介之一，自然语言在描述复杂结构方面表现出色。然而，将图表学习问题纳入生成式语言建模框架的现有工作仍然非常有限。随着大型语言模型的重要性不断增长，探索LLMs是否也可以替代GNNs成为图表的基础模型变得至关重要。在本文中，我们提出了InstructGLM（结构化语言模型）算法，系统地设计高度可扩展的模型来处理图表学习问题。

    The emergence of large-scale pre-trained language models, such as ChatGPT, has revolutionized various research fields in artificial intelligence. Transformers-based large language models (LLMs) have gradually replaced CNNs and RNNs to unify fields of computer vision and natural language processing. Compared with the data that exists relatively independently such as images, videos or texts, graph is a type of data that contains rich structural and relational information. Meanwhile, natural language, as one of the most expressive mediums, excels in describing complex structures. However, existing work on incorporating graph learning problems into the generative language modeling framework remains very limited. As the importance of large language models continues to grow, it becomes essential to explore whether LLMs can also replace GNNs as the foundation model for graphs. In this paper, we propose InstructGLM (Instruction-finetuned Graph Language Model), systematically design highly scal
    
[^34]: 将顺序带入基于Transformer的语言模型中，用于人工智能和法律的应用

    Bringing order into the realm of Transformer-based language models for artificial intelligence and law. (arXiv:2308.05502v1 [cs.CL])

    [http://arxiv.org/abs/2308.05502](http://arxiv.org/abs/2308.05502)

    本文提供了第一个对基于Transformer的语言模型在法律领域的人工智能问题和任务中的方法的系统概述。文章旨在突出这一领域的研究进展，以进一步了解Transformer在支持法律流程中的AI成功贡献以及当前的局限性。

    

    基于Transformer的语言模型（TLM）被广泛认可是一种先进的技术，能够成功开发出基于深度学习的解决方案，用于需要自然语言处理和理解的问题和应用。与其他文本领域一样，TLM确实推动了法律领域许多感兴趣任务对人工智能方法的最新进展。尽管第一个Transformer模型提出了大约6年时间，但这项技术以前所未有的速度迅猛发展，BERT和相关模型成为主要参考，也在法律领域占有重要地位。本文首次系统概述了TLM在法律领域的人工智能驱动问题和任务中的方法。一个主要目标是突出研究在这一领域的进展，以便一方面了解Transformer在支持法律流程中取得的AI成功贡献是什么，另一方面了解当前的局限性是什么。

    Transformer-based language models (TLMs) have widely been recognized to be a cutting-edge technology for the successful development of deep-learning-based solutions to problems and applications that require natural language processing and understanding. Like for other textual domains, TLMs have indeed pushed the state-of-the-art of AI approaches for many tasks of interest in the legal domain. Despite the first Transformer model being proposed about six years ago, there has been a rapid progress of this technology at an unprecedented rate, whereby BERT and related models represent a major reference, also in the legal domain. This article provides the first systematic overview of TLM-based methods for AI-driven problems and tasks in the legal sphere. A major goal is to highlight research advances in this field so as to understand, on the one hand, how the Transformers have contributed to the success of AI in supporting legal processes, and on the other hand, what are the current limitati
    
[^35]: 论用户响应模拟在对话式搜索中的深入研究

    An In-depth Investigation of User Response Simulation for Conversational Search. (arXiv:2304.07944v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2304.07944](http://arxiv.org/abs/2304.07944)

    本文研究了对话式搜索中用户响应模拟的方法。当前的模拟系统要么只能对是非问题进行回答，要么无法产生高质量的响应。通过用更小但先进的系统替换当前最先进的用户模拟系统，能够显著改进性能。

    

    对话式搜索在信息检索和自然语言处理领域引起了广泛关注。它通过多次自然语言交互来澄清和解决用户的搜索需求。然而，大多数现有系统是通过记录或人工对话日志进行训练和演示的。最终，对话式搜索系统应该在未见过的对话轨迹的开放环境中进行训练、评估和部署。一个关键的挑战是训练和评估这样的系统都需要人工参与，这既昂贵又不可扩展。其中一种策略是模拟用户，以此来减少扩展成本。然而，当前的用户模拟器要么仅限于对对话搜索系统的是非问题进行回答，要么无法产生高质量的响应。本文表明，通过用一个更小但先进的系统来替换当前最先进的用户模拟系统，能够大幅改进其性能。

    Conversational search has seen increased recent attention in both the IR and NLP communities. It seeks to clarify and solve a user's search need through multi-turn natural language interactions. However, most existing systems are trained and demonstrated with recorded or artificial conversation logs. Eventually, conversational search systems should be trained, evaluated, and deployed in an open-ended setting with unseen conversation trajectories. A key challenge is that training and evaluating such systems both require a human-in-the-loop, which is expensive and does not scale. One strategy for this is to simulate users, thereby reducing the scaling costs. However, current user simulators are either limited to only respond to yes-no questions from the conversational search system, or unable to produce high quality responses in general.  In this paper, we show that current state-of-the-art user simulation system could be significantly improved by replacing it with a smaller but advanced
    

