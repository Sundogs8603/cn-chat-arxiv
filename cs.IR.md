# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Drop your Decoder: Pre-training with Bag-of-Word Prediction for Dense Passage Retrieval.](http://arxiv.org/abs/2401.11248) | 本研究介绍了一种使用词袋预测进行预训练的密集通行检索方法，通过替换解码器实现了高效压缩词汇信号，显著改进了输入令牌的条款覆盖。 |
| [^2] | [Prompt-RAG: Pioneering Vector Embedding-Free Retrieval-Augmented Generation in Niche Domains, Exemplified by Korean Medicine.](http://arxiv.org/abs/2401.11246) | Prompt-RAG是一种在小众领域中增强了大型语言模型性能的新方法，与传统的RAG模型不同，它不需要使用嵌入向量。通过问答机器人应用程序的评估，结果表明Prompt-RAG优于现有模型，包括ChatGPT。 |
| [^3] | [Navigating the Thin Line: Examining User Behavior in Search to Detect Engagement and Backfire Effects.](http://arxiv.org/abs/2401.11201) | 本研究调查了有意见的用户在搜索行为中关注度和反效应的问题，并发现暴露于偏见搜索结果会增加他们消费反对态度内容的数量。 |
| [^4] | [A Deep Learning Approach for Selective Relevance Feedback.](http://arxiv.org/abs/2401.11198) | 本文从深度学习的角度重新思考了选择性伪相关反馈问题，并提出了一个完全基于数据驱动训练的模型，通过利用置信度估计来结合原始和扩展的查询信息，从而进一步提高检索效果。 |
| [^5] | [Document Set Expansion with Positive-Unlabeled Learning: A Density Estimation-based Approach.](http://arxiv.org/abs/2401.11145) | 本文提出了一种基于密度估计的正-负学习框架，可以处理文档集扩展任务中存在的问题，并证明其在真实数据集上的有效性。 |
| [^6] | [Exploiting Duality in Open Information Extraction with Predicate Prompt.](http://arxiv.org/abs/2401.11107) | 本文提出了一种新的生成式OpenIE模型DualOIE，通过实现一个双重任务，即将句子中的三元组转化为句子，来更有效地提取复杂的三元组。此方法鼓励模型正确识别句子结构，从而有助于提取所有潜在的三元组。 |
| [^7] | [FedRKG: A Privacy-preserving Federated Recommendation Framework via Knowledge Graph Enhancement.](http://arxiv.org/abs/2401.11089) | FedRKG是一种隐私保护的联邦推荐框架，通过增强知识图，构建全局知识图并利用关系感知的GNN模型，实现高阶用户-项目交互。 |
| [^8] | [On the selection of the correct number of terms for profile construction: theoretical and empirical analysis.](http://arxiv.org/abs/2401.10963) | 本文研究了从一组文档中构建用户配置文件的问题，提出了基于相似性度量的新型选择函数，并证明其具有良好的选择性能。 |
| [^9] | [Positive unlabeled learning for building recommender systems in a parliamentary setting.](http://arxiv.org/abs/2401.10961) | 通过正无标签学习，我们开发了一个推荐/过滤系统，通过挖掘议会活动来了解议员的政治兴趣和偏好，并决定哪些文件应该分发给每位议员。 |
| [^10] | [AI Revolution on Chat Bot: Evidence from a Randomized Controlled Experiment.](http://arxiv.org/abs/2401.10956) | 本文通过一项现场随机对照试验研究了LLM工具在提供无监督信息检索支持服务方面的效果。 |
| [^11] | [Machine Unlearning for Recommendation Systems: An Insight.](http://arxiv.org/abs/2401.10942) | 本文探讨了机器反学习在推荐系统中的应用，解决了适应性、个性化、隐私和偏见等挑战。与传统模型不同，MUL根据用户偏好的变化和伦理考虑动态调整系统知识。通过批判性检验和文献梳理，本文提供了MUL如何改变推荐、用户信任以及未来研究路径的见解。强调个性化和隐私之间的权衡挑战，并鼓励以满足实际需求为目标的贡献，推动MUL在安全和适应性机器学习中的发展。 |
| [^12] | [RELIANCE: Reliable Ensemble Learning for Information and News Credibility Evaluation.](http://arxiv.org/abs/2401.10940) | RELIANCE是一个可靠的集成学习系统，用于评估信息和新闻的可信度。它通过整合多个基本模型的优势，提供了对可信和不可信信息源的准确区分，并在信息和新闻可信度评估方面优于基准模型。 |
| [^13] | [A New Creative Generation Pipeline for Click-Through Rate with Stable Diffusion Model.](http://arxiv.org/abs/2401.10934) | 基于用户信息和艺术设计的新的创意生成流程，通过融合用户信息和考虑用户特征来预测CTR分数，提供更具吸引力的创意设计。 |
| [^14] | [Artificial intelligence to automate the systematic review of scientific literature.](http://arxiv.org/abs/2401.10917) | 本论文介绍了一项调查研究，研究了使用人工智能实现科学文献系统综述的自动化方法。 |
| [^15] | [Location Sensitive Embedding for Knowledge Graph Embedding.](http://arxiv.org/abs/2401.10893) | 这篇论文介绍了一种新颖的位置敏感嵌入（LSE）方法，该方法通过关系特定的映射来修改头实体，将关系概念化为线性变换。LSE在知识图谱嵌入领域具有理论基础，同时提出了更高效的变体LSEd。实验证明LSEd在链接预测任务上具有竞争力。 |
| [^16] | [Knowledge Graph Reasoning Based on Attention GCN.](http://arxiv.org/abs/2312.10049) | 本论文通过将图卷积神经网络（GCN）与注意力机制相结合，提出了一种新颖的技术来增强知识图谱推理，通过检查实体之间及其邻居节点之间的关系以及整合实体的属性和相互作用，生成丰富的隐式特征向量，以提高实体分类和链接预测等任务的性能。 |
| [^17] | [Analytical Modelling of Raw Data for Flow-Guided In-body Nanoscale Localization.](http://arxiv.org/abs/2309.16034) | 本论文研究了基于原始数据的体内纳米尺度定位的分析建模，分析了纳米设备的通信和能源约束对定位性能的影响。 |
| [^18] | [Streamlining Social Media Information Retrieval for Public Health Research with Deep Learning.](http://arxiv.org/abs/2306.16001) | 本研究介绍了一个使用深度学习简化社交媒体信息检索的框架，通过识别医学实体、标准化实体和分配UMLS概念，构建了一个用于COVID-19相关推文的症状词典。 |
| [^19] | [Medication Recommendation via Domain Knowledge Informed Deep Learning.](http://arxiv.org/abs/2305.19604) | 提出一种基于动态领域知识的药物推荐框架DKINet，将领域知识与患者临床表现相结合，此为首次实验。 |
| [^20] | [Large language models in biomedical natural language processing: benchmarks, baselines, and recommendations.](http://arxiv.org/abs/2305.16326) | 本文研究了GPT-3和GPT-4在生物医学自然语言处理中的表现，分析了它们可能产生的错误类型，并提供了使用这些模型的建议。 |
| [^21] | [Learning Graph ODE for Continuous-Time Sequential Recommendation.](http://arxiv.org/abs/2304.07042) | 提出了一个基于图ODE的连续时间序列推荐框架GDERec，它通过连续时间图卷积网络解决协作信号演化和不规则采样问题，并在四个真实数据集上取得显著优于现有方法的效果。 |
| [^22] | [A greedy approach for increased vehicle utilization in ridesharing networks.](http://arxiv.org/abs/2304.01225) | 本文提出了一个基于贪心策略的路线推荐方法，可以增加车辆利用率，缓解拼车服务对环境的影响。 |
| [^23] | [Hierarchical Locality Sensitive Hashing for Structured Data: A Survey.](http://arxiv.org/abs/2204.11209) | 本文调研了结构化数据分层局部敏感哈希（LSH）的研究现状，针对无法保留结构信息的传统LSH算法，提出了有效的解决方案。 |

# 详细

[^1]: 放弃解码器：使用词袋预测进行预训练的密集通行检索研究

    Drop your Decoder: Pre-training with Bag-of-Word Prediction for Dense Passage Retrieval. (arXiv:2401.11248v1 [cs.IR])

    [http://arxiv.org/abs/2401.11248](http://arxiv.org/abs/2401.11248)

    本研究介绍了一种使用词袋预测进行预训练的密集通行检索方法，通过替换解码器实现了高效压缩词汇信号，显著改进了输入令牌的条款覆盖。

    

    掩码自编码器预训练已成为初始化和增强密集检索系统的流行技术。它通常利用额外的Transformer解码块提供可持续的监督信号，并将上下文信息压缩到密集表示中。然而，这种预训练技术有效性的原因尚不清楚。使用基于Transformer的额外解码器也会产生显著的计算成本。本研究旨在通过揭示增强解码的掩码自编码器（MAE）预训练相对于普通BERT检查点在输入令牌的条款覆盖上的显著改进，以解释这个问题。基于这一观察，我们提出了对传统MAE的修改，将掩码自编码器的解码器替换为完全简化的词袋预测任务。这种修改使得词汇信号能够高效地压缩到密集表示中。

    Masked auto-encoder pre-training has emerged as a prevalent technique for initializing and enhancing dense retrieval systems. It generally utilizes additional Transformer decoder blocks to provide sustainable supervision signals and compress contextual information into dense representations. However, the underlying reasons for the effectiveness of such a pre-training technique remain unclear. The usage of additional Transformer-based decoders also incurs significant computational costs. In this study, we aim to shed light on this issue by revealing that masked auto-encoder (MAE) pre-training with enhanced decoding significantly improves the term coverage of input tokens in dense representations, compared to vanilla BERT checkpoints. Building upon this observation, we propose a modification to the traditional MAE by replacing the decoder of a masked auto-encoder with a completely simplified Bag-of-Word prediction task. This modification enables the efficient compression of lexical signa
    
[^2]: Prompt-RAG: 在小众领域中的基于向量嵌入的检索增强生成的开创性研究，以韩医学为例

    Prompt-RAG: Pioneering Vector Embedding-Free Retrieval-Augmented Generation in Niche Domains, Exemplified by Korean Medicine. (arXiv:2401.11246v1 [cs.CL])

    [http://arxiv.org/abs/2401.11246](http://arxiv.org/abs/2401.11246)

    Prompt-RAG是一种在小众领域中增强了大型语言模型性能的新方法，与传统的RAG模型不同，它不需要使用嵌入向量。通过问答机器人应用程序的评估，结果表明Prompt-RAG优于现有模型，包括ChatGPT。

    

    我们提出了一种基于自然语言提示的检索增强生成（Prompt-RAG）的新方法，旨在增强大型语言模型（LLM）在小众领域中的性能。传统的RAG方法大多需要向量嵌入，然而通用的LLM基于嵌入表示对于专业领域的适用性仍然不确定。为了探索和举例说明这一点，我们比较了韩医学（KM）和传统医学（CM）文档的向量嵌入，发现KM文档的嵌入与标记重叠相关性更强，与人工评估的文档相关性较小，而CM文档则相反。Prompt-RAG与传统的RAG模型不同，它不需要嵌入向量。通过问答机器人应用程序对其性能进行了评估，其中回答的相关性、可读性和信息性进行了评估。结果表明，Prompt-RAG优于现有模型，包括ChatGPT和...

    We propose a natural language prompt-based retrieval augmented generation (Prompt-RAG), a novel approach to enhance the performance of generative large language models (LLMs) in niche domains. Conventional RAG methods mostly require vector embeddings, yet the suitability of generic LLM-based embedding representations for specialized domains remains uncertain. To explore and exemplify this point, we compared vector embeddings from Korean Medicine (KM) and Conventional Medicine (CM) documents, finding that KM document embeddings correlated more with token overlaps and less with human-assessed document relatedness, in contrast to CM embeddings. Prompt-RAG, distinct from conventional RAG models, operates without the need for embedding vectors. Its performance was assessed through a Question-Answering (QA) chatbot application, where responses were evaluated for relevance, readability, and informativeness. The results showed that Prompt-RAG outperformed existing models, including ChatGPT and
    
[^3]: 在搜索中探究用户行为以检测参与度和反作用效应的薄线

    Navigating the Thin Line: Examining User Behavior in Search to Detect Engagement and Backfire Effects. (arXiv:2401.11201v1 [cs.IR])

    [http://arxiv.org/abs/2401.11201](http://arxiv.org/abs/2401.11201)

    本研究调查了有意见的用户在搜索行为中关注度和反效应的问题，并发现暴露于偏见搜索结果会增加他们消费反对态度内容的数量。

    

    带有偏见的用户经常寻求与其先前信念一致的信息，同时忽视相矛盾的证据，这是由于确认偏见所致。这种行为阻碍了他们在搜索网络时考虑替代立场的能力。尽管如此，很少有研究分析争议话题的搜索结果多样化如何影响高度有意见的用户的搜索行为。为此，我们进行了一项预注册的用户研究(n = 257)，调查了不同水平(低和高)的偏见指标和搜索结果展示(带或不带基于人工智能预测的立场标签)是否会影响有意见的用户在三个有争议的话题上(即无神论、知识产权和校服)的立场多样性消费和搜索行为。我们的结果表明，将参与者暴露于(反态度)偏见的搜索结果会增加他们消费反对态度内容的数量，但我们也发现偏见与趋向。

    Opinionated users often seek information that aligns with their preexisting beliefs while dismissing contradictory evidence due to confirmation bias. This conduct hinders their ability to consider alternative stances when searching the web. Despite this, few studies have analyzed how the diversification of search results on disputed topics influences the search behavior of highly opinionated users. To this end, we present a preregistered user study (n = 257) investigating whether different levels (low and high) of bias metrics and search results presentation (with or without AI-predicted stances labels) can affect the stance diversity consumption and search behavior of opinionated users on three debated topics (i.e., atheism, intellectual property rights, and school uniforms). Our results show that exposing participants to (counter-attitudinally) biased search results increases their consumption of attitude-opposing content, but we also found that bias was associated with a trend towar
    
[^4]: 深度学习方法用于选择性相关反馈

    A Deep Learning Approach for Selective Relevance Feedback. (arXiv:2401.11198v1 [cs.IR])

    [http://arxiv.org/abs/2401.11198](http://arxiv.org/abs/2401.11198)

    本文从深度学习的角度重新思考了选择性伪相关反馈问题，并提出了一个完全基于数据驱动训练的模型，通过利用置信度估计来结合原始和扩展的查询信息，从而进一步提高检索效果。

    

    通过建立一个完全基于数据驱动训练的模型，本文从深度学习的角度重新思考了选择性伪相关反馈问题，并提出了一个基于transformer双编码器架构的模型。此外，为了进一步提高使用选择性伪相关反馈方法的检索效果，我们利用模型的置信度估计来结合原始和扩展的查询信息。在实验中，我们将此选择性反馈应用于多种不同的组合。

    Pseudo-relevance feedback (PRF) can enhance average retrieval effectiveness over a sufficiently large number of queries. However, PRF often introduces a drift into the original information need, thus hurting the retrieval effectiveness of several queries. While a selective application of PRF can potentially alleviate this issue, previous approaches have largely relied on unsupervised or feature-based learning to determine whether a query should be expanded. In contrast, we revisit the problem of selective PRF from a deep learning perspective, presenting a model that is entirely data-driven and trained in an end-to-end manner. The proposed model leverages a transformer-based bi-encoder architecture. Additionally, to further improve retrieval effectiveness with this selective PRF approach, we make use of the model's confidence estimates to combine the information from the original and expanded queries. In our experiments, we apply this selective feedback on a number of different combinat
    
[^5]: 使用基于密度估计的正-负学习方法的文档集扩展

    Document Set Expansion with Positive-Unlabeled Learning: A Density Estimation-based Approach. (arXiv:2401.11145v1 [cs.LG])

    [http://arxiv.org/abs/2401.11145](http://arxiv.org/abs/2401.11145)

    本文提出了一种基于密度估计的正-负学习框架，可以处理文档集扩展任务中存在的问题，并证明其在真实数据集上的有效性。

    

    文档集扩展旨在基于一组精细主题的小型文档集合，从大型集合中识别相关文档。先前的研究表明，正-负学习是这一任务的一种有希望的方法。然而，一些严重问题仍未解决，例如正-负学习方法所面临的典型挑战，如未知类别先验和数据不平衡，以及需要转导型实验设置。在本文中，我们提出了一种基于密度估计的全新正-负学习框架，称为puDE，它可以处理上述问题。puDE的优势在于它既不受限于SCAR假设，也不需要任何类别先验知识。我们使用一系列真实数据集验证了所提出方法的有效性，并得出结论：我们的方法是DSE任务的一种更好的选择。

    Document set expansion aims to identify relevant documents from a large collection based on a small set of documents that are on a fine-grained topic. Previous work shows that PU learning is a promising method for this task. However, some serious issues remain unresolved, i.e. typical challenges that PU methods suffer such as unknown class prior and imbalanced data, and the need for transductive experimental settings. In this paper, we propose a novel PU learning framework based on density estimation, called puDE, that can handle the above issues. The advantage of puDE is that it neither constrained to the SCAR assumption and nor require any class prior knowledge. We demonstrate the effectiveness of the proposed method using a series of real-world datasets and conclude that our method is a better alternative for the DSE task.
    
[^6]: 使用谓词提示在开放信息提取中利用对偶性

    Exploiting Duality in Open Information Extraction with Predicate Prompt. (arXiv:2401.11107v1 [cs.CL])

    [http://arxiv.org/abs/2401.11107](http://arxiv.org/abs/2401.11107)

    本文提出了一种新的生成式OpenIE模型DualOIE，通过实现一个双重任务，即将句子中的三元组转化为句子，来更有效地提取复杂的三元组。此方法鼓励模型正确识别句子结构，从而有助于提取所有潜在的三元组。

    

    开放信息提取（OpenIE）旨在从给定的句子中提取以（主体，谓词，宾语）形式呈现的无模式三元组。与一般的信息提取（IE）相比，OpenIE对IE模型提出了更多挑战，尤其是在句子中存在多个复杂的三元组时。为了更有效地提取这些复杂的三元组，我们提出了一种新颖的生成式OpenIE模型，名为DualOIE，它在提取句子中的一些三元组的同时实现了另一个任务，即将三元组转化为句子。这种双重任务鼓励模型正确识别给定句子的结构，从而有助于从句子中提取所有潜在的三元组。具体而言，DualOIE分为两个步骤提取三元组：首先提取所有潜在谓词的序列，然后使用谓词序列作为提示诱导三元组的生成。

    Open information extraction (OpenIE) aims to extract the schema-free triplets in the form of (\emph{subject}, \emph{predicate}, \emph{object}) from a given sentence. Compared with general information extraction (IE), OpenIE poses more challenges for the IE models, {especially when multiple complicated triplets exist in a sentence. To extract these complicated triplets more effectively, in this paper we propose a novel generative OpenIE model, namely \emph{DualOIE}, which achieves a dual task at the same time as extracting some triplets from the sentence, i.e., converting the triplets into the sentence.} Such dual task encourages the model to correctly recognize the structure of the given sentence and thus is helpful to extract all potential triplets from the sentence. Specifically, DualOIE extracts the triplets in two steps: 1) first extracting a sequence of all potential predicates, 2) then using the predicate sequence as a prompt to induce the generation of triplets. Our experiments 
    
[^7]: FedRKG：一种通过知识图增强实现隐私保护的联邦推荐框架

    FedRKG: A Privacy-preserving Federated Recommendation Framework via Knowledge Graph Enhancement. (arXiv:2401.11089v1 [cs.CR])

    [http://arxiv.org/abs/2401.11089](http://arxiv.org/abs/2401.11089)

    FedRKG是一种隐私保护的联邦推荐框架，通过增强知识图，构建全局知识图并利用关系感知的GNN模型，实现高阶用户-项目交互。

    

    联邦学习（FL）作为一种在推荐系统中通过本地训练模型来保护数据隐私的有希望的方法已经出现。最近，图神经网络（GNN）因其捕捉用户和项目之间高阶交互的能力而在推荐任务中受到了广泛关注。然而，隐私问题阻碍了整个用户-项目图的全局共享。为了解决这个限制，一些方法在图中创建了伪交互项或用户，以弥补每个客户端缺失信息。不幸的是，这些方法引入了随机噪声并引发隐私问题。在本文中，我们提出了一种新颖的联邦推荐系统FedRKG，在服务器上使用公开的项目信息构建和维护全局知识图（KG），从而实现更高阶的用户-项目交互。在客户端，一个关系感知的GNN模型利用多样的KG关系。为了保护本地交互项目和模糊梯度

    Federated Learning (FL) has emerged as a promising approach for preserving data privacy in recommendation systems by training models locally. Recently, Graph Neural Networks (GNN) have gained popularity in recommendation tasks due to their ability to capture high-order interactions between users and items. However, privacy concerns prevent the global sharing of the entire user-item graph. To address this limitation, some methods create pseudo-interacted items or users in the graph to compensate for missing information for each client. Unfortunately, these methods introduce random noise and raise privacy concerns. In this paper, we propose FedRKG, a novel federated recommendation system, where a global knowledge graph (KG) is constructed and maintained on the server using publicly available item information, enabling higher-order user-item interactions. On the client side, a relation-aware GNN model leverages diverse KG relationships. To protect local interaction items and obscure gradi
    
[^8]: 关于构建用户配置文件所需术语数量的选择：理论和实证分析

    On the selection of the correct number of terms for profile construction: theoretical and empirical analysis. (arXiv:2401.10963v1 [cs.IR])

    [http://arxiv.org/abs/2401.10963](http://arxiv.org/abs/2401.10963)

    本文研究了从一组文档中构建用户配置文件的问题，提出了基于相似性度量的新型选择函数，并证明其具有良好的选择性能。

    

    本文研究了从一组文档中构建用户配置文件的问题。该配置文件由文档中最有代表性的术语子集组成，最好地代表用户的偏好或兴趣。受到离散集中理论的启发，我们对选择函数应满足的七个属性进行了公理研究：最小和最大不确定性原理，对添加零的不变性，对比例变换的不变性，名义增加原则，传递原则和最富有获取更多不等式。我们还提出了一种基于相似性度量的新型选择函数，特别是在信息检索中常用的余弦度量，并证明这种方法符合六个属性以及传递原则的较弱变体，从而代表了一种良好的选择方法。理论研究与实证研究相结合，以比较配置文件构建性能。

    In this paper, we examine the problem of building a user profile from a set of documents. This profile will consist of a subset of the most representative terms in the documents that best represent user preferences or interests. Inspired by the discrete concentration theory we have conducted an axiomatic study of seven properties that a selection function should fulfill: the minimum and maximum uncertainty principle, invariant to adding zeros, invariant to scale transformations, principle of nominal increase, transfer principle and the richest get richer inequality. We also present a novel selection function based on the use of similarity metrics, and more specifically the cosine measure which is commonly used in information retrieval, and demonstrate that this verifies six of the properties in addition to a weaker variant of the transfer principle, thereby representing a good selection approach. The theoretical study was complemented with an empirical study to compare the performance 
    
[^9]: 在议会环境中构建推荐系统的正无标签学习

    Positive unlabeled learning for building recommender systems in a parliamentary setting. (arXiv:2401.10961v1 [cs.IR])

    [http://arxiv.org/abs/2401.10961](http://arxiv.org/abs/2401.10961)

    通过正无标签学习，我们开发了一个推荐/过滤系统，通过挖掘议会活动来了解议员的政治兴趣和偏好，并决定哪些文件应该分发给每位议员。

    

    我们的目标是通过挖掘议会活动来了解议员的政治兴趣和偏好，以开发一个推荐/过滤系统。给定要分配给他们的一系列文件，该系统可以决定每位议员应该收到哪些文件。我们提出使用正无标签学习来解决这个问题，因为我们只有有关相关文件（每位议员在辩论中自己的发言）的信息，但没有关于不相关文件的信息，因此我们无法使用通过正负样本训练的标准二元分类器。我们还开发了一种新的算法，与以下方法相比表现更好：a) 基于假设其他议员的干预都是不相关的基准方法，b) 另一种广为人知的正无标签学习方法，c) 基于信息检索方法将文件和立法进行匹配的方法。

    Our goal is to learn about the political interests and preferences of the Members of Parliament by mining their parliamentary activity, in order to develop a recommendation/filtering system that, given a stream of documents to be distributed among them, is able to decide which documents should receive each Member of Parliament. We propose to use positive unlabeled learning to tackle this problem, because we only have information about relevant documents (the own interventions of each Member of Parliament in the debates) but not about irrelevant documents, so that we cannot use standard binary classifiers trained with positive and negative examples. We have also developed a new algorithm of this type, which compares favourably with: a) the baseline approach assuming that all the interventions of other Members of Parliament are irrelevant, b) another well-known positive unlabeled learning method and c) an approach based on information retrieval methods that matches documents and legislat
    
[^10]: Chat Bot上的AI革命：一项随机对照实验的证据

    AI Revolution on Chat Bot: Evidence from a Randomized Controlled Experiment. (arXiv:2401.10956v1 [cs.HC])

    [http://arxiv.org/abs/2401.10956](http://arxiv.org/abs/2401.10956)

    本文通过一项现场随机对照试验研究了LLM工具在提供无监督信息检索支持服务方面的效果。

    

    近年来，生成式人工智能（generative AI）取得了重大进展，展示出显著的提升人类生产力的潜力。特别是，大型语言模型（LLM），以ChatGPT-4为例，引起了极大关注。许多文章已经研究了LLM工具在实验室环境下和设计任务或观察性研究中对人类生产力的影响。尽管最近取得了进展，但在实际环境中应用LLM工具的现场实验仍然有限。本文介绍了一项现场随机对照试验的研究结果，评估了LLM工具在提供无监督信息检索支持服务方面的有效性。

    In recent years, generative AI has undergone major advancements, demonstrating significant promise in augmenting human productivity. Notably, large language models (LLM), with ChatGPT-4 as an example, have drawn considerable attention. Numerous articles have examined the impact of LLM-based tools on human productivity in lab settings and designed tasks or in observational studies. Despite recent advances, field experiments applying LLM-based tools in realistic settings are limited. This paper presents the findings of a field randomized controlled trial assessing the effectiveness of LLM-based tools in providing unmonitored support services for information retrieval.
    
[^11]: 机器反学习在推荐系统中的应用：一种洞察力

    Machine Unlearning for Recommendation Systems: An Insight. (arXiv:2401.10942v1 [cs.IR])

    [http://arxiv.org/abs/2401.10942](http://arxiv.org/abs/2401.10942)

    本文探讨了机器反学习在推荐系统中的应用，解决了适应性、个性化、隐私和偏见等挑战。与传统模型不同，MUL根据用户偏好的变化和伦理考虑动态调整系统知识。通过批判性检验和文献梳理，本文提供了MUL如何改变推荐、用户信任以及未来研究路径的见解。强调个性化和隐私之间的权衡挑战，并鼓励以满足实际需求为目标的贡献，推动MUL在安全和适应性机器学习中的发展。

    

    本综述探讨了推荐系统中的机器反学习（MUL），解决了适应性、个性化、隐私和偏见等挑战。与传统模型不同，MUL根据用户偏好的变化和伦理考虑动态调整系统知识。本文对MUL的基本原理、现实世界应用和算法透明性等挑战进行了批判性的检验。它梳理了相关文献，提供了MUL如何改变推荐的见解，探讨了用户信任，并提出了未来研究在负责任和用户关注的人工智能领域的路径。本文引导研究人员面对个性化和隐私之间的权衡挑战，鼓励以满足有针对性的数据删除实际需求为目标的贡献。强调MUL在安全和适应性机器学习中的作用，提出了推动其边界的方法。本文的创新之处在于探讨了这些方法的局限性。

    This review explores machine unlearning (MUL) in recommendation systems, addressing adaptability, personalization, privacy, and bias challenges. Unlike traditional models, MUL dynamically adjusts system knowledge based on shifts in user preferences and ethical considerations. The paper critically examines MUL's basics, real-world applications, and challenges like algorithmic transparency. It sifts through literature, offering insights into how MUL could transform recommendations, discussing user trust, and suggesting paths for future research in responsible and user-focused artificial intelligence (AI). The document guides researchers through challenges involving the trade-off between personalization and privacy, encouraging contributions to meet practical demands for targeted data removal. Emphasizing MUL's role in secure and adaptive machine learning, the paper proposes ways to push its boundaries. The novelty of this paper lies in its exploration of the limitations of the methods, w
    
[^12]: RELIANCE: 可靠的集成学习用于信息和新闻可信度评估

    RELIANCE: Reliable Ensemble Learning for Information and News Credibility Evaluation. (arXiv:2401.10940v1 [cs.IR])

    [http://arxiv.org/abs/2401.10940](http://arxiv.org/abs/2401.10940)

    RELIANCE是一个可靠的集成学习系统，用于评估信息和新闻的可信度。它通过整合多个基本模型的优势，提供了对可信和不可信信息源的准确区分，并在信息和新闻可信度评估方面优于基准模型。

    

    在信息泛滥的时代，辨别新闻内容的可信度越来越具有挑战性。本文介绍了RELIANCE，这是一个专为鲁棒信息和虚假新闻可信度评估而设计的先进的集成学习系统。RELIANCE由五个不同的基本模型组成，包括支持向量机（SVM）、朴素贝叶斯、逻辑回归、随机森林和双向长短期记忆网络（BiLSTMs）。RELIANCE采用了创新的方法来整合它们的优势，利用集成的智能提高准确性。实验证明了RELIANCE在区分可信和不可信信息源方面的优越性，表明其在信息和新闻可信度评估方面超过了单个模型，并成为评估信息源可靠性的有效解决方案。

    In the era of information proliferation, discerning the credibility of news content poses an ever-growing challenge. This paper introduces RELIANCE, a pioneering ensemble learning system designed for robust information and fake news credibility evaluation. Comprising five diverse base models, including Support Vector Machine (SVM), naive Bayes, logistic regression, random forest, and Bidirectional Long Short Term Memory Networks (BiLSTMs), RELIANCE employs an innovative approach to integrate their strengths, harnessing the collective intelligence of the ensemble for enhanced accuracy. Experiments demonstrate the superiority of RELIANCE over individual models, indicating its efficacy in distinguishing between credible and non-credible information sources. RELIANCE, also surpasses baseline models in information and news credibility assessment, establishing itself as an effective solution for evaluating the reliability of information sources.
    
[^13]: 一个新的创意生成流程用于点击率稳定扩散模型

    A New Creative Generation Pipeline for Click-Through Rate with Stable Diffusion Model. (arXiv:2401.10934v1 [cs.IR])

    [http://arxiv.org/abs/2401.10934](http://arxiv.org/abs/2401.10934)

    基于用户信息和艺术设计的新的创意生成流程，通过融合用户信息和考虑用户特征来预测CTR分数，提供更具吸引力的创意设计。

    

    在在线广告的场景中，销售商通常会创建多个创意以提供全面的演示，因此呈现出最吸引人的设计以最大化点击率（CTR）至关重要。然而，销售商通常难以考虑用户对创意设计的偏好，导致相对于基于人工智能（AI）的方法来说，美学和数量都较低。传统的基于AI的方法仍然面临同样的问题，即没有考虑用户信息，同时在设计师的美学知识方面有限。事实上，通过融合用户信息，生成的创意可能更具吸引力，因为不同的用户可能有不同的偏好。为了优化结果，传统方法中生成的创意会通过另一个被称为创意排名模型的模块进行排序。排名模型可以考虑用户特征来预测每个创意的CTR分数。然而，上述的两个阶段被视为两个不同的训练任务，使得生成的创意可能不够精准。

    In online advertising scenario, sellers often create multiple creatives to provide comprehensive demonstrations, making it essential to present the most appealing design to maximize the Click-Through Rate (CTR). However, sellers generally struggle to consider users preferences for creative design, leading to the relatively lower aesthetics and quantities compared to Artificial Intelligence (AI)-based approaches. Traditional AI-based approaches still face the same problem of not considering user information while having limited aesthetic knowledge from designers. In fact that fusing the user information, the generated creatives can be more attractive because different users may have different preferences. To optimize the results, the generated creatives in traditional methods are then ranked by another module named creative ranking model. The ranking model can predict the CTR score for each creative considering user features. However, the two above stages are regarded as two different t
    
[^14]: 人工智能实现科学文献系统综述的自动化

    Artificial intelligence to automate the systematic review of scientific literature. (arXiv:2401.10917v1 [cs.IR])

    [http://arxiv.org/abs/2401.10917](http://arxiv.org/abs/2401.10917)

    本论文介绍了一项调查研究，研究了使用人工智能实现科学文献系统综述的自动化方法。

    

    人工智能（AI）在现代计算中变得非常重要，因为它可以有效地解决人们传统上完成的复杂任务。AI提供了表示和推理知识、高效地处理文本和从大量数据中学习的方法。这些特点适用于许多人类找到费力或重复的活动，比如科学文献的分析。手动准备和撰写系统文献综述（SLR）需要相当长的时间和努力，因为它需要策划一个策略，进行文献搜索和分析，并报告结果。根据研究领域的不同，检索到的论文数量可能达到数百或数千篇，这意味着过滤出相关文献并提取关键信息是一个昂贵且容易出错的过程。然而，一些涉及的任务是重复的，因此可以通过AI自动化。在本文中，我们提出了一项调查研究

    Artificial intelligence (AI) has acquired notorious relevance in modern computing as it effectively solves complex tasks traditionally done by humans. AI provides methods to represent and infer knowledge, efficiently manipulate texts and learn from vast amount of data. These characteristics are applicable in many activities that human find laborious or repetitive, as is the case of the analysis of scientific literature. Manually preparing and writing a systematic literature review (SLR) takes considerable time and effort, since it requires planning a strategy, conducting the literature search and analysis, and reporting the findings. Depending on the area under study, the number of papers retrieved can be of hundreds or thousands, meaning that filtering those relevant ones and extracting the key information becomes a costly and error-prone process. However, some of the involved tasks are repetitive and, therefore, subject to automation by means of AI. In this paper, we present a survey
    
[^15]: 知识图谱嵌入的位置敏感嵌入

    Location Sensitive Embedding for Knowledge Graph Embedding. (arXiv:2401.10893v1 [cs.IR])

    [http://arxiv.org/abs/2401.10893](http://arxiv.org/abs/2401.10893)

    这篇论文介绍了一种新颖的位置敏感嵌入（LSE）方法，该方法通过关系特定的映射来修改头实体，将关系概念化为线性变换。LSE在知识图谱嵌入领域具有理论基础，同时提出了更高效的变体LSEd。实验证明LSEd在链接预测任务上具有竞争力。

    

    知识图谱嵌入将知识图谱转化为连续的、低维度的空间，有助于推理和补全任务。该领域主要分为传统的距离模型和语义匹配模型。传统的距离模型面临的关键挑战是无法有效区分图谱中的“头实体”和“尾实体”。为了解决这个问题，提出了新颖的位置敏感嵌入（LSE）方法。LSE通过关系特定的映射修改头实体，将关系概念化为线性变换而不仅仅是平移。LSE的理论基础，包括其表示能力和与现有模型的联系，都进行了详细研究。一种更简化的变体LSEd利用对角矩阵进行变换以提高实用性能。在对四个大规模数据集进行链接预测的测试中，LSEd要么表现更好，要么具有竞争力。

    Knowledge graph embedding transforms knowledge graphs into a continuous, low-dimensional space, facilitating inference and completion tasks. This field is mainly divided into translational distance models and semantic matching models. A key challenge in translational distance models is their inability to effectively differentiate between 'head' and 'tail' entities in graphs. To address this, the novel location-sensitive embedding (LSE) method has been developed. LSE innovatively modifies the head entity using relation-specific mappings, conceptualizing relations as linear transformations rather than mere translations. The theoretical foundations of LSE, including its representational capabilities and its connections to existing models, have been thoroughly examined. A more streamlined variant, LSEd, employs a diagonal matrix for transformations to enhance practical efficiency. In tests conducted on four large-scale datasets for link prediction, LSEd either outperforms or is competitive
    
[^16]: 基于注意力GCN的知识图谱推理

    Knowledge Graph Reasoning Based on Attention GCN. (arXiv:2312.10049v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2312.10049](http://arxiv.org/abs/2312.10049)

    本论文通过将图卷积神经网络（GCN）与注意力机制相结合，提出了一种新颖的技术来增强知识图谱推理，通过检查实体之间及其邻居节点之间的关系以及整合实体的属性和相互作用，生成丰富的隐式特征向量，以提高实体分类和链接预测等任务的性能。

    

    我们提出了一种新颖的技术，通过将图卷积神经网络（GCN）与注意力机制相结合来增强知识图谱推理。该方法利用注意力机制来检查实体之间及其邻居节点之间的关系，从而为每个实体开发详细的特征向量。GCN使用共享参数有效地表示相邻实体的特征。我们首先学习实体之间的相似度，以进行节点表示学习。通过整合实体的属性和它们的相互作用，该方法为每个实体生成了丰富的隐式特征向量，提高了实体分类和链接预测等任务的性能，优于传统的神经网络模型。总之，这项工作为搜索引擎、问答系统、推荐系统和数据整合任务等多个应用领域提供了重要的方法支持。

    We propose a novel technique to enhance Knowledge Graph Reasoning by combining Graph Convolution Neural Network (GCN) with the Attention Mechanism. This approach utilizes the Attention Mechanism to examine the relationships between entities and their neighboring nodes, which helps to develop detailed feature vectors for each entity. The GCN uses shared parameters to effectively represent the characteristics of adjacent entities. We first learn the similarity of entities for node representation learning. By integrating the attributes of the entities and their interactions, this method generates extensive implicit feature vectors for each entity, improving performance in tasks including entity classification and link prediction, outperforming traditional neural network models. To conclude, this work provides crucial methodological support for a range of applications, such as search engines, question-answering systems, recommendation systems, and data integration tasks.
    
[^17]: 基于原始数据的体内纳米尺度定位的分析建模

    Analytical Modelling of Raw Data for Flow-Guided In-body Nanoscale Localization. (arXiv:2309.16034v1 [cs.ET])

    [http://arxiv.org/abs/2309.16034](http://arxiv.org/abs/2309.16034)

    本论文研究了基于原始数据的体内纳米尺度定位的分析建模，分析了纳米设备的通信和能源约束对定位性能的影响。

    

    纳米技术和材料科学的进展为纳米尺度设备的发展铺平了道路，这些设备结合了传感、计算、数据和能源储存以及无线通信。在精密医学中，这些纳米设备对于疾病诊断、治疗和监测呈现出巨大的潜力，而体内纳米尺度定位的流引导定位，即将所感知的生物事件与事件本身的位置关联起来，从精密医学的角度看将具有极大的益处。纳米设备的纳米尺度特性以及血液流动环境的挑战性导致目前的流引导定位方法在通信和能源相关能力方面受到限制。纳米设备的通信和能源约束导致流引导定位的原始数据具有不同的特征，从而影响其性能。本研究通过分析建模研究了这些不完美的影响。

    Advancements in nanotechnology and material science are paving the way toward nanoscale devices that combine sensing, computing, data and energy storage, and wireless communication. In precision medicine, these nanodevices show promise for disease diagnostics, treatment, and monitoring from within the patients' bloodstreams. Assigning the location of a sensed biological event with the event itself, which is the main proposition of flow-guided in-body nanoscale localization, would be immensely beneficial from the perspective of precision medicine. The nanoscale nature of the nanodevices and the challenging environment that the bloodstream represents, result in current flow-guided localization approaches being constrained in their communication and energy-related capabilities. The communication and energy constraints of the nanodevices result in different features of raw data for flow-guided localization, in turn affecting its performance. An analytical modeling of the effects of imperfe
    
[^18]: 用深度学习简化社交媒体信息检索以支持公共卫生研究

    Streamlining Social Media Information Retrieval for Public Health Research with Deep Learning. (arXiv:2306.16001v1 [cs.CL])

    [http://arxiv.org/abs/2306.16001](http://arxiv.org/abs/2306.16001)

    本研究介绍了一个使用深度学习简化社交媒体信息检索的框架，通过识别医学实体、标准化实体和分配UMLS概念，构建了一个用于COVID-19相关推文的症状词典。

    

    社交媒体在流行病监测中的利用已经得到了很好的证实。然而，当使用预定义的词汇表来检索相关语料库时，常常会引入偏见。本研究介绍了一个框架，旨在构建医学俗语和统一医学语言系统（UMLS）概念的广泛字典。该框架由三个模块组成：基于BERT的命名实体识别（NER）模型，用于从社交媒体内容中识别出医学实体；深度学习驱动的标准化模块，用于对提取出的实体进行规范化处理；半监督聚类模块，将最可能的UMLS概念分配给每个规范化实体。我们将该框架应用于从2020年2月1日到2022年4月30日期间与COVID-19相关的推文，生成了一个症状词典（可在https://github.com/ningkko/UMLS_colloquialism/上获取），其中包含9,249个标准化实体，映射到876个UMLS概念和38,175个俚语表达。该框架的演示

    The utilization of social media in epidemic surveillance has been well established. Nonetheless, bias is often introduced when pre-defined lexicons are used to retrieve relevant corpus. This study introduces a framework aimed at curating extensive dictionaries of medical colloquialisms and Unified Medical Language System (UMLS) concepts. The framework comprises three modules: a BERT-based Named Entity Recognition (NER) model that identifies medical entities from social media content, a deep-learning powered normalization module that standardizes the extracted entities, and a semi-supervised clustering module that assigns the most probable UMLS concept to each standardized entity. We applied this framework to COVID-19-related tweets from February 1, 2020, to April 30, 2022, generating a symptom dictionary (available at https://github.com/ningkko/UMLS_colloquialism/) composed of 9,249 standardized entities mapped to 876 UMLS concepts and 38,175 colloquial expressions. This framework demo
    
[^19]: 通过领域知识启示的深度学习进行药物推荐

    Medication Recommendation via Domain Knowledge Informed Deep Learning. (arXiv:2305.19604v1 [cs.AI])

    [http://arxiv.org/abs/2305.19604](http://arxiv.org/abs/2305.19604)

    提出一种基于动态领域知识的药物推荐框架DKINet，将领域知识与患者临床表现相结合，此为首次实验。

    

    药物推荐是医疗保健的基本但至关重要的分支，提供机会为复杂健康状况的患者支持临床医生更精确的药物处方。从电子健康记录（EHR）中学习推荐药物是先前研究中最常见的方法。然而，大多数研究忽视了根据患者的EHR中的临床表现纳入领域知识的问题。为了解决这些问题，我们提出了一种新颖的基于动态领域知识的药物推荐框架，即领域知识启示网络（DKINet），用于将领域知识与可观察的患者临床表现相结合。特别是，我们首先设计了一个基于领域知识的编码器来捕捉领域信息，然后开发了一个数据驱动的编码器将领域知识整合到可观察的EHR中。

    Medication recommendation is a fundamental yet crucial branch of healthcare, which provides opportunities to support clinical physicians with more accurate medication prescriptions for patients with complex health conditions. Learning from electronic health records (EHR) to recommend medications is the most common way in previous studies. However, most of them neglect incorporating domain knowledge according to the clinical manifestations in the EHR of the patient. To address these issues, we propose a novel \textbf{D}omain \textbf{K}nowledge \textbf{I}nformed \textbf{Net}work (DKINet) to integrate domain knowledge with observable clinical manifestations of the patient, which is the first dynamic domain knowledge informed framework toward medication recommendation. In particular, we first design a knowledge-driven encoder to capture the domain information and then develop a data-driven encoder to integrate domain knowledge into the observable EHR. To endow the model with the capability
    
[^20]: 生物医学自然语言处理中的大型语言模型: 基准、基线和建议

    Large language models in biomedical natural language processing: benchmarks, baselines, and recommendations. (arXiv:2305.16326v1 [cs.CL])

    [http://arxiv.org/abs/2305.16326](http://arxiv.org/abs/2305.16326)

    本文研究了GPT-3和GPT-4在生物医学自然语言处理中的表现，分析了它们可能产生的错误类型，并提供了使用这些模型的建议。

    

    生物医学文献呈指数级增长，手动筛选和提取知识变得困难。自动从生物医学文献中提取信息的生物医学自然语言处理（BioNLP）技术有助于减轻这种负担。近年来，如GPT-3和GPT-4等大型语言模型（LLMs）因其卓越的性能而受到重视。但是，它们在BioNLP任务中的有效性以及对方法开发和下游用户的影响仍未得到研究。本研究（1）在四个应用程序中在八个BioNLP数据集中建立了GPT-3和GPT-4在零-shot和一-shot设置下的基准表现，包括命名实体识别，关系提取，多标签文档分类和语义相似性和推理；（2）审查了LLMs产生的错误，并将错误分为三种类型：缺失，不一致和不需要的人工内容；（3）提出了使用LLMs的建议。

    Biomedical literature is growing rapidly, making it challenging to curate and extract knowledge manually. Biomedical natural language processing (BioNLP) techniques that can automatically extract information from biomedical literature help alleviate this burden. Recently, large Language Models (LLMs), such as GPT-3 and GPT-4, have gained significant attention for their impressive performance. However, their effectiveness in BioNLP tasks and impact on method development and downstream users remain understudied. This pilot study (1) establishes the baseline performance of GPT-3 and GPT-4 at both zero-shot and one-shot settings in eight BioNLP datasets across four applications: named entity recognition, relation extraction, multi-label document classification, and semantic similarity and reasoning, (2) examines the errors produced by the LLMs and categorized the errors into three types: missingness, inconsistencies, and unwanted artificial content, and (3) provides suggestions for using L
    
[^21]: 连续时间序列推荐中的图ODE学习

    Learning Graph ODE for Continuous-Time Sequential Recommendation. (arXiv:2304.07042v1 [cs.IR])

    [http://arxiv.org/abs/2304.07042](http://arxiv.org/abs/2304.07042)

    提出了一个基于图ODE的连续时间序列推荐框架GDERec，它通过连续时间图卷积网络解决协作信号演化和不规则采样问题，并在四个真实数据集上取得显著优于现有方法的效果。

    

    连续时间序列推荐旨在通过捕获用户过去交互中的项购买序列，理解用户的偏好。现有方法通常通过建模顺序模式来预测下一个项目。尽管有效，然而存在两个天然不足：（i）用户偏好具有动态性，并且经常忽略协作信号的演化；（ii）观察到的交互通常是不规则采样的，而现有方法则假设均匀间隔来进行项目转换建模。因此，如何有效地对用户偏好的基础动态建模和预测成为一个关键的研究问题。为了解决上述挑战，本文关注于连续时间序列推荐，提出了一种基于图正则微分方程（graph ordinary differential equation, GDERec）的连续时间序列推荐框架。技术上，GDERec由一个自回归图正则微分方程组成，通过连续时间图卷积网络来建模项转移动态。在四个真实数据集上的实证结果表明，GDERec显著优于现有方法，并提供了连续时间用户偏好动态的可解释见解。

    Sequential recommendation aims at understanding user preference by capturing successive behavior correlations, which are usually represented as the item purchasing sequences based on their past interactions. Existing efforts generally predict the next item via modeling the sequential patterns. Despite effectiveness, there exist two natural deficiencies: (i) user preference is dynamic in nature, and the evolution of collaborative signals is often ignored; and (ii) the observed interactions are often irregularly-sampled, while existing methods model item transitions assuming uniform intervals. Thus, how to effectively model and predict the underlying dynamics for user preference becomes a critical research problem. To tackle the above challenges, in this paper, we focus on continuous-time sequential recommendation and propose a principled graph ordinary differential equation framework named GDERec. Technically, GDERec is characterized by an autoregressive graph ordinary differential equa
    
[^22]: 基于贪心策略提高拼车网络中的车辆利用率

    A greedy approach for increased vehicle utilization in ridesharing networks. (arXiv:2304.01225v1 [cs.DS])

    [http://arxiv.org/abs/2304.01225](http://arxiv.org/abs/2304.01225)

    本文提出了一个基于贪心策略的路线推荐方法，可以增加车辆利用率，缓解拼车服务对环境的影响。

    

    近年来，拼车平台已成为城市居民的主要交通方式。对于这些平台来讲，路线推荐是一个至关重要的问题。现有的研究已经建议了具有更高乘客需求的路线。然而，统计数据表明，与私人车辆相比，这些服务会导致增加温室气体排放，因为它们在寻找乘客时四处漫游。本文提供了拼车系统功能的更详细细节，并揭示了在拼车系统蓬勃发展的情况下它们并未有效地利用车辆容量。我们建议克服以上限制，并推荐同时获取多个乘客的路线，从而增加车辆利用率，从而减少这些系统对环境的影响。由于路线推荐是NP-hard问题，我们提出了基于k跳的滑动窗口近似方法。

    In recent years, ridesharing platforms have become a prominent mode of transportation for the residents of urban areas. As a fundamental problem, route recommendation for these platforms is vital for their sustenance. The works done in this direction have recommended routes with higher passenger demand. Despite the existing works, statistics have suggested that these services cause increased greenhouse emissions compared to private vehicles as they roam around in search of riders. This analysis provides finer details regarding the functionality of ridesharing systems and it reveals that in the face of their boom, they have not utilized the vehicle capacity efficiently. We propose to overcome the above limitations and recommend routes that will fetch multiple passengers simultaneously which will result in increased vehicle utilization and thereby decrease the effect of these systems on the environment. As route recommendation is NP-hard, we propose a k-hop-based sliding window approxima
    
[^23]: 结构化数据的分层局部敏感哈希：一项调研

    Hierarchical Locality Sensitive Hashing for Structured Data: A Survey. (arXiv:2204.11209v3 [cs.DS] UPDATED)

    [http://arxiv.org/abs/2204.11209](http://arxiv.org/abs/2204.11209)

    本文调研了结构化数据分层局部敏感哈希（LSH）的研究现状，针对无法保留结构信息的传统LSH算法，提出了有效的解决方案。

    

    数据相似性（或距离）计算是一个基础的研究课题，促进了许多基于相似性的机器学习和数据挖掘应用。在大数据分析中，由于计算成本高昂，计算数据实例的精确相似性是不切实际的。为此，局部敏感哈希（LSH）技术已被提出，以在没有学习过程的情况下以高效的方式提供各种集合或向量之间的各种相似度估计器。结构化数据（例如序列、树和图）在现实世界中很常见，但传统的LSH算法无法保留表示元素之间关系的结构信息。为了解决这个问题，研究人员一直致力于分层LSH算法。本文探讨了分层LSH研究的现有进展。

    Data similarity (or distance) computation is a fundamental research topic which fosters a variety of similarity-based machine learning and data mining applications. In big data analytics, it is impractical to compute the exact similarity of data instances due to high computational cost. To this end, the Locality Sensitive Hashing (LSH) technique has been proposed to provide accurate estimators for various similarity measures between sets or vectors in an efficient manner without the learning process. Structured data (e.g., sequences, trees and graphs), which are composed of elements and relations between the elements, are commonly seen in the real world, but the traditional LSH algorithms cannot preserve the structure information represented as relations between elements. In order to conquer the issue, researchers have been devoted to the family of the hierarchical LSH algorithms. In this paper, we explore the present progress of the research into hierarchical LSH from the following pe
    

