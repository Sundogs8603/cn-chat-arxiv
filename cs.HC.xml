<rss version="2.0"><channel><title>Chat Arxiv cs.HC</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.HC</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#35780;&#20272;&#26694;&#26550;Parachute&#65292;&#29992;&#20110;&#20132;&#20114;&#24335;&#20849;&#21516;&#25776;&#20889;&#31995;&#32479;&#30340;&#35780;&#20272;&#65292;&#35813;&#26694;&#26550;&#21253;&#21547;&#20102;&#20998;&#31867;&#30340;&#23454;&#29992;&#25351;&#26631;&#65292;&#21487;&#20197;&#29992;&#20110;&#35780;&#20272;&#21644;&#27604;&#36739;&#20849;&#21516;&#25776;&#20889;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2303.06333</link><description>&lt;p&gt;
&#38477;&#33853;&#20254;&#65306;&#35780;&#20272;&#20132;&#20114;&#24335;&#20154;&#26426;&#20849;&#21516;&#25776;&#20889;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Parachute: Evaluating Interactive Human-LM Co-writing Systems. (arXiv:2303.06333v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06333
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#35780;&#20272;&#26694;&#26550;Parachute&#65292;&#29992;&#20110;&#20132;&#20114;&#24335;&#20849;&#21516;&#25776;&#20889;&#31995;&#32479;&#30340;&#35780;&#20272;&#65292;&#35813;&#26694;&#26550;&#21253;&#21547;&#20102;&#20998;&#31867;&#30340;&#23454;&#29992;&#25351;&#26631;&#65292;&#21487;&#20197;&#29992;&#20110;&#35780;&#20272;&#21644;&#27604;&#36739;&#20849;&#21516;&#25776;&#20889;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a human-centered evaluation framework, Parachute, for interactive co-writing systems, which includes categorized practical metrics and can be used to evaluate and compare co-writing systems.
&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#39134;&#36895;&#21457;&#23637;&#24341;&#36215;&#20102;&#20154;&#20204;&#23545;&#20110;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#20849;&#21516;&#25776;&#20889;&#31995;&#32479;&#30340;&#26497;&#22823;&#20852;&#36259;&#65292;&#20854;&#20013;&#20154;&#31867;&#21644;&#35821;&#35328;&#27169;&#22411;&#20132;&#20114;&#22320;&#20026;&#20849;&#21516;&#30340;&#20889;&#20316;&#25104;&#26524;&#20570;&#20986;&#36129;&#29486;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#23545;&#20110;&#20132;&#20114;&#24335;&#29615;&#22659;&#19979;&#20849;&#21516;&#25776;&#20889;&#31995;&#32479;&#30340;&#35780;&#20272;&#30740;&#31350;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#35780;&#20272;&#26694;&#26550;Parachute&#65292;&#29992;&#20110;&#20132;&#20114;&#24335;&#20849;&#21516;&#25776;&#20889;&#31995;&#32479;&#30340;&#35780;&#20272;&#12290;Parachute&#23637;&#31034;&#20102;&#20132;&#20114;&#35780;&#20272;&#30340;&#32508;&#21512;&#35270;&#35282;&#65292;&#20854;&#20013;&#27599;&#20010;&#35780;&#20272;&#26041;&#38754;&#37117;&#21253;&#21547;&#20102;&#20998;&#31867;&#30340;&#23454;&#29992;&#25351;&#26631;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#20351;&#29992;&#26696;&#20363;&#26469;&#28436;&#31034;&#22914;&#20309;&#20351;&#29992;Parachute&#35780;&#20272;&#21644;&#27604;&#36739;&#20849;&#21516;&#25776;&#20889;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
A surge of advances in language models (LMs) has led to significant interest in using LMs to build co-writing systems, in which humans and LMs interactively contribute to a shared writing artifact. However, there is a lack of studies assessing co-writing systems in interactive settings. We propose a human-centered evaluation framework, Parachute, for interactive co-writing systems. Parachute showcases an integrative view of interaction evaluation, where each evaluation aspect consists of categorized practical metrics. Furthermore, we present Parachute with a use case to demonstrate how to evaluate and compare co-writing systems using Parachute.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#34394;&#25311;&#21161;&#25163;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#34394;&#25311;&#21161;&#25163;&#26159;&#19968;&#31181;&#33021;&#22815;&#29702;&#35299;&#33258;&#28982;&#35821;&#35328;&#35821;&#38899;&#21629;&#20196;&#24182;&#33021;&#20195;&#34920;&#24744;&#25191;&#34892;&#20219;&#21153;&#30340;&#36719;&#20214;&#65292;&#21487;&#20197;&#23436;&#25104;&#20960;&#20046;&#20219;&#20309;&#24744;&#33258;&#24049;&#21487;&#20197;&#23436;&#25104;&#30340;&#29305;&#23450;&#26234;&#33021;&#25163;&#26426;&#25110;PC&#27963;&#21160;&#65292;&#32780;&#19988;&#21015;&#34920;&#19981;&#26029;&#25193;&#22823;&#12290;</title><link>http://arxiv.org/abs/2303.06309</link><description>&lt;p&gt;
&#34394;&#25311;&#40736;&#26631;&#21644;&#21161;&#25163;&#65306;&#20154;&#24037;&#26234;&#33021;&#30340;&#25216;&#26415;&#38761;&#21629;
&lt;/p&gt;
&lt;p&gt;
Virtual Mouse And Assistant: A Technological Revolution Of Artificial Intelligence. (arXiv:2303.06309v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06309
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#34394;&#25311;&#21161;&#25163;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#34394;&#25311;&#21161;&#25163;&#26159;&#19968;&#31181;&#33021;&#22815;&#29702;&#35299;&#33258;&#28982;&#35821;&#35328;&#35821;&#38899;&#21629;&#20196;&#24182;&#33021;&#20195;&#34920;&#24744;&#25191;&#34892;&#20219;&#21153;&#30340;&#36719;&#20214;&#65292;&#21487;&#20197;&#23436;&#25104;&#20960;&#20046;&#20219;&#20309;&#24744;&#33258;&#24049;&#21487;&#20197;&#23436;&#25104;&#30340;&#29305;&#23450;&#26234;&#33021;&#25163;&#26426;&#25110;PC&#27963;&#21160;&#65292;&#32780;&#19988;&#21015;&#34920;&#19981;&#26029;&#25193;&#22823;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces the performance improvement of virtual assistants, which are software that understands natural language voice commands and can perform tasks on your behalf. They can complete practically any specific smartphone or PC activity that you can complete on your own, and the list is continually expanding.
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#25552;&#39640;&#34394;&#25311;&#21161;&#25163;&#30340;&#24615;&#33021;&#12290;&#37027;&#20040;&#20160;&#20040;&#26159;&#34394;&#25311;&#21161;&#25163;&#65311;&#24212;&#29992;&#36719;&#20214;&#65292;&#36890;&#24120;&#31216;&#20026;&#34394;&#25311;&#21161;&#25163;&#65292;&#20063;&#31216;&#20026;AI&#21161;&#25163;&#25110;&#25968;&#23383;&#21161;&#25163;&#65292;&#26159;&#19968;&#31181;&#33021;&#22815;&#29702;&#35299;&#33258;&#28982;&#35821;&#35328;&#35821;&#38899;&#21629;&#20196;&#24182;&#33021;&#20195;&#34920;&#24744;&#25191;&#34892;&#20219;&#21153;&#30340;&#36719;&#20214;&#12290;&#34394;&#25311;&#21161;&#25163;&#21487;&#20197;&#23436;&#25104;&#20960;&#20046;&#20219;&#20309;&#24744;&#33258;&#24049;&#21487;&#20197;&#23436;&#25104;&#30340;&#29305;&#23450;&#26234;&#33021;&#25163;&#26426;&#25110;PC&#27963;&#21160;&#65292;&#32780;&#19988;&#21015;&#34920;&#19981;&#26029;&#25193;&#22823;&#12290;&#34394;&#25311;&#21161;&#25163;&#36890;&#24120;&#21487;&#20197;&#23436;&#25104;&#21508;&#31181;&#21508;&#26679;&#30340;&#20219;&#21153;&#65292;&#21253;&#25324;&#23433;&#25490;&#20250;&#35758;&#12289;&#21457;&#36865;&#28040;&#24687;&#21644;&#30417;&#25511;&#22825;&#27668;&#12290;&#20197;&#21069;&#30340;&#34394;&#25311;&#21161;&#25163;&#65292;&#22914;Google&#21161;&#25163;&#21644;Cortana&#65292;&#22312;&#26576;&#20123;&#26041;&#38754;&#26377;&#38480;&#21046;&#65292;&#22240;&#20026;&#23427;&#20204;&#21482;&#33021;&#25191;&#34892;&#25628;&#32034;&#65292;&#32780;&#19981;&#26159;&#23436;&#20840;&#33258;&#21160;&#21270;&#12290;&#20363;&#22914;&#65292;&#36825;&#20123;&#24341;&#25806;&#27809;&#26377;&#33021;&#21147;&#21069;&#36827;&#21644;&#20498;&#24102;&#27468;&#26354;&#65292;&#20197;&#20445;&#25345;&#27468;&#26354;&#30340;&#25511;&#21046;&#21151;&#33021;&#65307;&#23427;&#20204;&#21482;&#33021;&#20855;&#26377;&#25628;&#32034;&#27468;&#26354;&#30340;&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;
The purpose of this paper is to enhance the performance of the virtual assistant. So, what exactly is a virtual assistant. Application software, often called virtual assistants, also known as AI assistants or digital assistants, is software that understands natural language voice commands and can perform tasks on your behalf. What does a virtual assistant do. Virtual assistants can complete practically any specific smartphone or PC activity that you can complete on your own, and the list is continually expanding. Virtual assistants typically do an impressive variety of tasks, including scheduling meetings, delivering messages, and monitoring the weather. Previous virtual assistants, like Google Assistant and Cortana, had limits in that they could only perform searches and were not entirely automated. For instance, these engines do not have the ability to forward and rewind the song in order to maintain the control function of the song; they can only have the module to search for songs 
&lt;/p&gt;</description></item><item><title>AVTALER&#26159;&#19968;&#31181;&#20132;&#20114;&#24335;UI&#65292;&#32467;&#21512;&#20102;&#20154;&#31867;&#30340;&#29420;&#29305;&#25216;&#33021;&#21644;&#33258;&#21160;&#21270;&#30340;&#20248;&#21183;&#65292;&#25903;&#25345;&#29992;&#25143;&#23545;&#21487;&#27604;&#36739;&#30340;&#25991;&#26412;&#25688;&#24405;&#36827;&#34892;&#24847;&#20041;&#24314;&#26500;&#21644;&#23545;&#27604;&#12290;</title><link>http://arxiv.org/abs/2303.06264</link><description>&lt;p&gt;
&#19968;&#31181;&#25903;&#25345;&#23545;&#24179;&#34892;&#25991;&#26412;&#38598;&#21512;&#36827;&#34892;&#24847;&#20041;&#24314;&#26500;&#30340;&#20132;&#20114;&#24335;UI
&lt;/p&gt;
&lt;p&gt;
An Interactive UI to Support Sensemaking over Collections of Parallel Texts. (arXiv:2303.06264v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06264
&lt;/p&gt;
&lt;p&gt;
AVTALER&#26159;&#19968;&#31181;&#20132;&#20114;&#24335;UI&#65292;&#32467;&#21512;&#20102;&#20154;&#31867;&#30340;&#29420;&#29305;&#25216;&#33021;&#21644;&#33258;&#21160;&#21270;&#30340;&#20248;&#21183;&#65292;&#25903;&#25345;&#29992;&#25143;&#23545;&#21487;&#27604;&#36739;&#30340;&#25991;&#26412;&#25688;&#24405;&#36827;&#34892;&#24847;&#20041;&#24314;&#26500;&#21644;&#23545;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
AVTALER is an interactive UI that combines human skills and the advantages of automation to support users in sensemaking and contrasting comparable text excerpts.
&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#23478;&#21644;&#31185;&#23398;&#35760;&#32773;&#31561;&#20154;&#32463;&#24120;&#38656;&#35201;&#29702;&#35299;&#22823;&#37327;&#35770;&#25991;&#21450;&#20854;&#22312;&#33539;&#22260;&#12289;&#37325;&#28857;&#12289;&#21457;&#29616;&#25110;&#20854;&#20182;&#37325;&#35201;&#22240;&#32032;&#26041;&#38754;&#30340;&#27604;&#36739;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22823;&#37327;&#30340;&#35770;&#25991;&#65292;&#36880;&#19968;&#36827;&#34892;&#27604;&#36739;&#21644;&#23545;&#27604;&#26159;&#35748;&#30693;&#19978;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#23436;&#20840;&#33258;&#21160;&#21270;&#36825;&#20010;&#23457;&#26597;&#36807;&#31243;&#26159;&#19981;&#21487;&#34892;&#30340;&#65292;&#22240;&#20026;&#23427;&#36890;&#24120;&#38656;&#35201;&#39046;&#22495;&#29305;&#23450;&#30340;&#30693;&#35782;&#65292;&#20197;&#21450;&#29702;&#35299;&#23457;&#26597;&#30340;&#32972;&#26223;&#21644;&#21160;&#26426;&#12290;&#34429;&#28982;&#26377;&#29616;&#26377;&#30340;&#24037;&#20855;&#26469;&#24110;&#21161;&#32452;&#32455;&#21644;&#27880;&#37322;&#25991;&#29486;&#32508;&#36848;&#30340;&#35770;&#25991;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#20381;&#36182;&#20110;&#20154;&#20204;&#36880;&#20010;&#38405;&#35835;&#35770;&#25991;&#24182;&#25163;&#21160;&#29702;&#35299;&#30456;&#20851;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;AVTALER&#65292;&#23427;&#32467;&#21512;&#20102;&#20154;&#20204;&#29420;&#29305;&#30340;&#25216;&#33021;&#12289;&#19978;&#19979;&#25991;&#24847;&#35782;&#21644;&#30693;&#35782;&#65292;&#20197;&#21450;&#33258;&#21160;&#21270;&#30340;&#20248;&#21183;&#12290;&#32473;&#23450;&#19968;&#32452;&#21487;&#27604;&#36739;&#30340;&#25991;&#26412;&#25688;&#24405;&#65292;&#23427;&#25903;&#25345;&#29992;&#25143;&#36827;&#34892;&#24847;&#20041;&#24314;&#26500;&#21644;&#23545;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scientists and science journalists, among others, often need to make sense of a large number of papers and how they compare with each other in scope, focus, findings, or any other important factors. However, with a large corpus of papers, it's cognitively demanding to pairwise compare and contrast them all with each other. Fully automating this review process would be infeasible, because it often requires domain-specific knowledge, as well as understanding what the context and motivations for the review are. While there are existing tools to help with the process of organizing and annotating papers for literature reviews, at the core they still rely on people to serially read through papers and manually make sense of relevant information.  We present AVTALER, which combines peoples' unique skills, contextual awareness, and knowledge, together with the strength of automation. Given a set of comparable text excerpts from a paper corpus, it supports users in sensemaking and contrasting pa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#35780;&#20272;&#65292;&#23558;&#30456;&#23545;&#25104;&#29087;&#30340;&#21487;&#35299;&#37322;AI&#39046;&#22495;&#19982;&#24555;&#36895;&#21457;&#23637;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30740;&#31350;&#36827;&#34892;&#20102;&#31867;&#27604;&#65292;&#35748;&#20026;&#20154;&#31867;&#30340;&#38656;&#27714;&#24212;&#35813;&#25104;&#20026;LLMs&#35780;&#20272;&#30340;&#26680;&#24515;&#12290;</title><link>http://arxiv.org/abs/2303.06223</link><description>&lt;p&gt;
&#35841;&#22312;&#24605;&#32771;&#65311;&#20351;&#29992;XAI Playbook&#25512;&#21160;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;LLMs&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Who's Thinking? A Push for Human-Centered Evaluation of LLMs using the XAI Playbook. (arXiv:2303.06223v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06223
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#35780;&#20272;&#65292;&#23558;&#30456;&#23545;&#25104;&#29087;&#30340;&#21487;&#35299;&#37322;AI&#39046;&#22495;&#19982;&#24555;&#36895;&#21457;&#23637;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30740;&#31350;&#36827;&#34892;&#20102;&#31867;&#27604;&#65292;&#35748;&#20026;&#20154;&#31867;&#30340;&#38656;&#27714;&#24212;&#35813;&#25104;&#20026;LLMs&#35780;&#20272;&#30340;&#26680;&#24515;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores human-centered evaluation of AI-based systems, drawing parallels between the relatively mature field of explainable AI and the rapidly evolving research boom around large language models. The authors argue that humans' needs should be held front and center in evaluating LLMs.
&lt;/p&gt;
&lt;p&gt;
&#37096;&#32626;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#32463;&#24120;&#24433;&#21709;&#20154;&#31867;&#65292;&#32780;&#35780;&#20272;&#36825;&#20123;&#24037;&#20855;&#27809;&#26377;&#19968;&#31181;&#36866;&#21512;&#25152;&#26377;&#24773;&#20917;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;AI&#31995;&#32479;&#35780;&#20272;&#32467;&#21512;&#20102;&#23450;&#37327;&#21644;&#23450;&#24615;&#20998;&#26512;&#20197;&#21450;&#20154;&#31867;&#36755;&#20837;&#12290;&#23427;&#24050;&#32463;&#22312;&#21487;&#35299;&#37322;&#30340;AI&#65288;XAI&#65289;&#21644;&#20154;&#26426;&#20132;&#20114;&#65288;HCI&#65289;&#31038;&#21306;&#20013;&#24471;&#21040;&#20102;&#28145;&#20837;&#25506;&#35752;&#12290;&#20173;&#28982;&#23384;&#22312;&#24046;&#36317;&#65292;&#20294;&#31038;&#21306;&#24050;&#32463;&#25509;&#21463;&#20102;&#20154;&#31867;&#19982;AI&#21450;&#20854;&#38468;&#24102;&#30340;&#35299;&#37322;&#36827;&#34892;&#20132;&#20114;&#65292;&#20197;&#21450;&#24212;&#35813;&#23558;&#20154;&#31867;&#30340;&#38656;&#27714;&#65288;&#21253;&#25324;&#20182;&#20204;&#30340;&#35748;&#30693;&#20559;&#35265;&#21644;&#24618;&#30294;&#65289;&#25918;&#22312;&#39318;&#20301;&#30340;&#22522;&#26412;&#29702;&#35299;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#30456;&#23545;&#25104;&#29087;&#30340;XAI&#39046;&#22495;&#19982;&#24555;&#36895;&#21457;&#23637;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30740;&#31350;&#28909;&#28526;&#20043;&#38388;&#36827;&#34892;&#20102;&#31867;&#27604;&#12290;&#25509;&#21463;&#30340;LLMs&#35780;&#20272;&#25351;&#26631;&#19981;&#26159;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#22312;&#35752;&#35770;LLMs&#26102;&#65292;XAI&#31038;&#21306;&#22312;&#36807;&#21435;&#21313;&#24180;&#20013;&#36208;&#36807;&#30340;&#35768;&#22810;&#30456;&#21516;&#36335;&#24452;&#23558;&#34987;&#37325;&#26032;&#36367;&#19978;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35748;&#20026;&#20154;&#31867;&#30340;&#20542;&#21521; - &#20877;&#27425;&#23436;&#20840;&#21253;&#25324;&#20182;&#20204;&#30340;&#35748;&#30693;&#20559;&#35265;&#21644;&#24618;&#30294; - &#24212;&#35813;&#25104;&#20026;LLMs&#35780;&#20272;&#30340;&#26680;&#24515;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deployed artificial intelligence (AI) often impacts humans, and there is no one-size-fits-all metric to evaluate these tools. Human-centered evaluation of AI-based systems combines quantitative and qualitative analysis and human input. It has been explored to some depth in the explainable AI (XAI) and human-computer interaction (HCI) communities. Gaps remain, but the basic understanding that humans interact with AI and accompanying explanations, and that humans' needs -- complete with their cognitive biases and quirks -- should be held front and center, is accepted by the community. In this paper, we draw parallels between the relatively mature field of XAI and the rapidly evolving research boom around large language models (LLMs). Accepted evaluative metrics for LLMs are not human-centered. We argue that many of the same paths tread by the XAI community over the past decade will be retread when discussing LLMs. Specifically, we argue that humans' tendencies -- again, complete with the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20027;&#24352;&#20174;&#24403;&#21069;&#30340;&#21487;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#27169;&#24335;&#36716;&#21464;&#65292;&#37319;&#29992;&#22522;&#20110;&#20551;&#35774;&#30340;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65292;&#20197;&#26356;&#22909;&#22320;&#25903;&#25345;&#20154;&#31867;&#20915;&#31574;&#12290;</title><link>http://arxiv.org/abs/2302.12389</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#24050;&#27515;&#65292;&#21487;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#19975;&#23681;&#65281;&#22522;&#20110;&#20551;&#35774;&#30340;&#20915;&#31574;&#25903;&#25345;
&lt;/p&gt;
&lt;p&gt;
Explainable AI is Dead, Long Live Explainable AI! Hypothesis-driven decision support. (arXiv:2302.12389v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12389
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20027;&#24352;&#20174;&#24403;&#21069;&#30340;&#21487;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#27169;&#24335;&#36716;&#21464;&#65292;&#37319;&#29992;&#22522;&#20110;&#20551;&#35774;&#30340;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65292;&#20197;&#26356;&#22909;&#22320;&#25903;&#25345;&#20154;&#31867;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper argues for a paradigm shift from the current model of explainable artificial intelligence (XAI) to hypothesis-driven decision support systems to better support human decision making.
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20027;&#24352;&#20174;&#24403;&#21069;&#30340;&#21487;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#27169;&#24335;&#36716;&#21464;&#65292;&#22240;&#20026;&#23427;&#21487;&#33021;&#20250;&#22952;&#30861;&#26356;&#22909;&#30340;&#20154;&#31867;&#20915;&#31574;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#20154;&#20204;&#19981;&#24635;&#26159;&#20250;&#25509;&#21463;&#21644;&#36981;&#24490;&#24314;&#35758;&#65292;&#22240;&#20026;&#20182;&#20204;&#19981;&#20449;&#20219;&#23427;&#20204;&#65292;&#25110;&#32773;&#26356;&#31967;&#31957;&#30340;&#26159;&#65292;&#21363;&#20351;&#24314;&#35758;&#26159;&#38169;&#35823;&#30340;&#65292;&#20154;&#20204;&#20063;&#20250;&#30450;&#30446;&#22320;&#36981;&#24490;&#23427;&#20204;&#12290;&#21487;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#36890;&#36807;&#24110;&#21161;&#20154;&#20204;&#29702;&#35299;&#27169;&#22411;&#20026;&#20160;&#20040;&#20250;&#32473;&#20986;&#26576;&#20123;&#24314;&#35758;&#26469;&#32531;&#35299;&#36825;&#31181;&#24773;&#20917;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20154;&#20204;&#24182;&#19981;&#24635;&#26159;&#36275;&#22815;&#21442;&#19982;&#35299;&#37322;&#24037;&#20855;&#20197;&#24110;&#21161;&#25913;&#21892;&#20915;&#31574;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#36825;&#26159;&#22240;&#20026;&#25105;&#20204;&#27809;&#26377;&#32771;&#34385;&#21040;&#20004;&#20214;&#20107;&#24773;&#12290;&#39318;&#20808;&#65292;&#24314;&#35758;&#65288;&#21450;&#20854;&#35299;&#37322;&#65289;&#21487;&#33021;&#19982;&#20154;&#20204;&#30340;&#20551;&#35774;&#21644;&#20449;&#20208;&#30456;&#20914;&#31361;&#12290;&#20854;&#27425;&#65292;&#20154;&#20204;&#30340;&#20915;&#31574;&#24448;&#24448;&#26159;&#22522;&#20110;&#20551;&#35774;&#30340;&#65292;&#32780;&#19981;&#26159;&#22522;&#20110;&#20107;&#23454;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20027;&#24352;&#37319;&#29992;&#22522;&#20110;&#20551;&#35774;&#30340;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65292;&#20197;&#26356;&#22909;&#22320;&#25903;&#25345;&#20154;&#31867;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we argue for a paradigm shift from the current model of explainable artificial intelligence (XAI), which may be counter-productive to better human decision making. In early decision support systems, we assumed that we could give people recommendations and that they would consider them, and then follow them when required. However, research found that people often ignore recommendations because they do not trust them; or perhaps even worse, people follow them blindly, even when the recommendations are wrong. Explainable artificial intelligence mitigates this by helping people to understand how and why models give certain recommendations. However, recent research shows that people do not always engage with explainability tools enough to help improve decision making. The assumption that people will engage with recommendations and explanations has proven to be unfounded. We argue this is because we have failed to account for two things. First, recommendations (and their expla
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;OpenAI&#30340;GPT3.5&#27169;&#22411;&#37325;&#26032;&#22797;&#21046;&#20102;Many Labs 2&#22797;&#21046;&#39033;&#30446;&#20013;&#30340;14&#39033;&#30740;&#31350;&#65292;&#20854;&#20013;8&#39033;&#30740;&#31350;&#30340;&#32467;&#26524;&#34987;&#25104;&#21151;&#22797;&#21046;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#21097;&#19979;&#30340;6&#39033;&#30740;&#31350;&#65292;GPT3.5&#20197;&#26497;&#20854;&#39044;&#23450;&#30340;&#26041;&#24335;&#22238;&#31572;&#20102;&#35843;&#26597;&#38382;&#39064;&#65292;&#23548;&#33268;&#26080;&#27861;&#20998;&#26512;&#36825;&#20123;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2302.07267</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#24515;&#29702;&#23398;&#20013;&#30340;&#8220;&#27491;&#30830;&#31572;&#26696;&#8221;
&lt;/p&gt;
&lt;p&gt;
"Correct answers" from the psychology of artificial intelligence. (arXiv:2302.07267v3 [cs.HC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07267
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;OpenAI&#30340;GPT3.5&#27169;&#22411;&#37325;&#26032;&#22797;&#21046;&#20102;Many Labs 2&#22797;&#21046;&#39033;&#30446;&#20013;&#30340;14&#39033;&#30740;&#31350;&#65292;&#20854;&#20013;8&#39033;&#30740;&#31350;&#30340;&#32467;&#26524;&#34987;&#25104;&#21151;&#22797;&#21046;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#21097;&#19979;&#30340;6&#39033;&#30740;&#31350;&#65292;GPT3.5&#20197;&#26497;&#20854;&#39044;&#23450;&#30340;&#26041;&#24335;&#22238;&#31572;&#20102;&#35843;&#26597;&#38382;&#39064;&#65292;&#23548;&#33268;&#26080;&#27861;&#20998;&#26512;&#36825;&#20123;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper replicates 14 studies from the Many Labs 2 replication project with OpenAI's text-davinci-003 model, and successfully replicates the results of 8 studies. However, for the remaining 6 studies, GPT3.5 answered survey questions in an extremely predetermined way, making it impossible to analyze these studies.
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#24050;&#32463;&#22823;&#22823;&#22686;&#24378;&#12290;&#36825;&#31181;AI&#31995;&#32479;&#30340;&#19968;&#20010;&#25552;&#20986;&#30340;&#24212;&#29992;&#26159;&#25903;&#25345;&#31038;&#20250;&#21644;&#35748;&#30693;&#31185;&#23398;&#20013;&#30340;&#25968;&#25454;&#25910;&#38598;&#65292;&#30446;&#21069;&#23436;&#32654;&#30340;&#23454;&#39564;&#25511;&#21046;&#26159;&#19981;&#21487;&#34892;&#30340;&#65292;&#32780;&#22823;&#35268;&#27169;&#12289;&#20195;&#34920;&#24615;&#25968;&#25454;&#38598;&#30340;&#25910;&#38598;&#36890;&#24120;&#26159;&#26114;&#36149;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;OpenAI&#30340;text-davinci-003&#27169;&#22411;&#65288;&#20439;&#31216;GPT3.5&#65289;&#37325;&#26032;&#22797;&#21046;&#20102;Many Labs 2&#22797;&#21046;&#39033;&#30446;&#20013;&#30340;14&#39033;&#30740;&#31350;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#27599;&#39033;&#30740;&#31350;&#30340;&#35843;&#26597;&#20316;&#20026;&#25991;&#26412;&#36755;&#20837;&#65292;&#20174;GPT3.5&#30340;&#40664;&#35748;&#35774;&#32622;&#20013;&#25910;&#38598;&#20102;&#21709;&#24212;&#12290;&#22312;&#25105;&#20204;&#21487;&#20197;&#20998;&#26512;&#30340;&#20843;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30340;GPT&#26679;&#26412;&#22797;&#21046;&#20102;&#21407;&#22987;&#32467;&#26524;&#30340;37.5%&#20197;&#21450;Many Labs 2&#32467;&#26524;&#30340;37.5%&#12290;&#20986;&#20046;&#24847;&#26009;&#30340;&#26159;&#65292;&#25105;&#20204;&#26080;&#27861;&#20687;&#39044;&#20808;&#27880;&#20876;&#30340;&#35745;&#21010;&#37027;&#26679;&#20998;&#26512;&#21097;&#19979;&#30340;&#20845;&#39033;&#30740;&#31350;&#12290;&#36825;&#26159;&#22240;&#20026;&#23545;&#20110;&#36825;&#20845;&#39033;&#30740;&#31350;&#20013;&#30340;&#27599;&#19968;&#39033;&#65292;GPT3.5&#20197;&#26497;&#20854;&#39044;&#23450;&#30340;&#26041;&#24335;&#22238;&#31572;&#20102;&#35843;&#26597;&#38382;&#39064;&#65288;&#26080;&#35770;&#26159;&#22240;&#21464;&#37327;&#36824;&#26159;&#26465;&#20214;&#21464;&#37327;&#65289;&#65306;&#19968;&#20010;&#26410;&#30693;&#30340;
&lt;/p&gt;
&lt;p&gt;
Large Language Models have vastly grown in capabilities. One proposed application of such AI systems is to support data collection in the social and cognitive sciences, where perfect experimental control is currently unfeasible and the collection of large, representative datasets is generally expensive. In this paper, we re-replicate 14 studies from the Many Labs 2 replication project with OpenAI's text-davinci-003 model, colloquially known as GPT3.5. We collected responses from the default setting of GPT3.5 by inputting each study's survey as text. Among the eight studies we could analyse, our GPT sample replicated 37.5% of the original results as well as 37.5% of the Many Labs 2 results. Unexpectedly, we could not analyse the remaining six studies as we had planned in our pre-registration. This was because for each of these six studies, GPT3.5 answered at least one of the survey questions (either a dependent variable or a condition variable) in an extremely predetermined way: an unex
&lt;/p&gt;</description></item><item><title>D-Shape&#26159;&#19968;&#31181;&#26032;&#30340;&#32467;&#21512;IL&#21644;RL&#30340;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#22870;&#21169;&#22609;&#24418;&#21644;&#30446;&#26631;&#26465;&#20214;&#21270;RL&#30340;&#24605;&#24819;&#26469;&#35299;&#20915;&#27425;&#20248;&#28436;&#31034;&#19982;&#22238;&#25253;&#26368;&#22823;&#21270;&#30446;&#26631;&#20043;&#38388;&#30340;&#20914;&#31361;&#65292;&#33021;&#22815;&#22312;&#31232;&#30095;&#22870;&#21169;&#32593;&#26684;&#19990;&#30028;&#39046;&#22495;&#20013;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#24182;&#19968;&#33268;&#22320;&#25910;&#25947;&#21040;&#26368;&#20248;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2210.14428</link><description>&lt;p&gt;
D-Shape: &#36890;&#36807;&#30446;&#26631;&#26465;&#20214;&#21270;&#23454;&#29616;&#28436;&#31034;&#24418;&#29366;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
D-Shape: Demonstration-Shaped Reinforcement Learning via Goal Conditioning. (arXiv:2210.14428v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.14428
&lt;/p&gt;
&lt;p&gt;
D-Shape&#26159;&#19968;&#31181;&#26032;&#30340;&#32467;&#21512;IL&#21644;RL&#30340;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#22870;&#21169;&#22609;&#24418;&#21644;&#30446;&#26631;&#26465;&#20214;&#21270;RL&#30340;&#24605;&#24819;&#26469;&#35299;&#20915;&#27425;&#20248;&#28436;&#31034;&#19982;&#22238;&#25253;&#26368;&#22823;&#21270;&#30446;&#26631;&#20043;&#38388;&#30340;&#20914;&#31361;&#65292;&#33021;&#22815;&#22312;&#31232;&#30095;&#22870;&#21169;&#32593;&#26684;&#19990;&#30028;&#39046;&#22495;&#20013;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#24182;&#19968;&#33268;&#22320;&#25910;&#25947;&#21040;&#26368;&#20248;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
D-Shape is a new method that combines imitation learning (IL) and reinforcement learning (RL) using reward shaping and goal-conditioned RL to resolve the conflict between suboptimal demonstrations and return-maximization objective of RL. It improves sample efficiency and consistently converges to the optimal policy in sparse-reward gridworld domains.
&lt;/p&gt;
&lt;p&gt;
&#23558;&#27169;&#20223;&#23398;&#20064;&#65288;IL&#65289;&#21644;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30456;&#32467;&#21512;&#26159;&#35299;&#20915;&#33258;&#20027;&#34892;&#20026;&#33719;&#21462;&#20013;&#26679;&#26412;&#25928;&#29575;&#20302;&#19979;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#20294;&#36825;&#26679;&#20570;&#30340;&#26041;&#27861;&#36890;&#24120;&#20551;&#23450;&#25152;&#38656;&#30340;&#34892;&#20026;&#28436;&#31034;&#30001;&#19987;&#23478;&#25552;&#20379;&#65292;&#35813;&#19987;&#23478;&#30456;&#23545;&#20110;&#20219;&#21153;&#22870;&#21169;&#34920;&#29616;&#26368;&#20339;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#25552;&#20379;&#30340;&#28436;&#31034;&#26159;&#27425;&#20248;&#30340;&#65292;&#21017;&#38754;&#20020;&#19968;&#20010;&#22522;&#26412;&#25361;&#25112;&#65292;&#21363;IL&#30340;&#28436;&#31034;&#21305;&#37197;&#30446;&#26631;&#19982;RL&#30340;&#22238;&#25253;&#26368;&#22823;&#21270;&#30446;&#26631;&#20914;&#31361;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;D-Shape&#65292;&#19968;&#31181;&#26032;&#30340;&#32467;&#21512;IL&#21644;RL&#30340;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#22870;&#21169;&#22609;&#24418;&#21644;&#30446;&#26631;&#26465;&#20214;&#21270;RL&#30340;&#24605;&#24819;&#26469;&#35299;&#20915;&#19978;&#36848;&#20914;&#31361;&#12290;D-Shape&#20801;&#35768;&#20174;&#27425;&#20248;&#28436;&#31034;&#20013;&#23398;&#20064;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#25214;&#21040;&#30456;&#23545;&#20110;&#20219;&#21153;&#22870;&#21169;&#30340;&#26368;&#20248;&#31574;&#30053;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#31232;&#30095;&#22870;&#21169;&#32593;&#26684;&#19990;&#30028;&#39046;&#22495;&#23454;&#39564;&#39564;&#35777;&#20102;D-Shape&#65292;&#32467;&#26524;&#34920;&#26126;&#23427;&#22312;&#26679;&#26412;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;RL&#65292;&#24182;&#19988;&#33021;&#22815;&#19968;&#33268;&#22320;&#25910;&#25947;&#21040;&#26368;&#20248;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
While combining imitation learning (IL) and reinforcement learning (RL) is a promising way to address poor sample efficiency in autonomous behavior acquisition, methods that do so typically assume that the requisite behavior demonstrations are provided by an expert that behaves optimally with respect to a task reward. If, however, suboptimal demonstrations are provided, a fundamental challenge appears in that the demonstration-matching objective of IL conflicts with the return-maximization objective of RL. This paper introduces D-Shape, a new method for combining IL and RL that uses ideas from reward shaping and goal-conditioned RL to resolve the above conflict. D-Shape allows learning from suboptimal demonstrations while retaining the ability to find the optimal policy with respect to the task reward. We experimentally validate D-Shape in sparse-reward gridworld domains, showing that it both improves over RL in terms of sample efficiency and converges consistently to the optimal polic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20449;&#30340;&#20154;&#24037;&#26234;&#33021;&#36890;&#20449;&#32593;&#32476;&#65292;&#23558;&#36890;&#20449;&#21327;&#35758;&#12289;&#21306;&#22359;&#38142;&#25216;&#26415;&#21644;&#20449;&#24687;&#34701;&#21512;&#19982;AI&#38598;&#25104;&#65292;&#20197;&#25913;&#21892;&#20914;&#31361;&#36890;&#20449;&#65292;&#20026;&#20154;&#36947;&#20027;&#20041;&#21033;&#30410;&#25552;&#20379;&#21487;&#38382;&#36131;&#20449;&#24687;&#20132;&#25442;&#12290;</title><link>http://arxiv.org/abs/2112.11191</link><description>&lt;p&gt;
&#20026;&#20154;&#36947;&#20027;&#20041;&#21033;&#30410;&#24320;&#21457;&#21487;&#20449;&#30340;&#20154;&#24037;&#26234;&#33021;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Developing a Trusted Human-AI Network for Humanitarian Benefit. (arXiv:2112.11191v3 [cs.CY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.11191
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20449;&#30340;&#20154;&#24037;&#26234;&#33021;&#36890;&#20449;&#32593;&#32476;&#65292;&#23558;&#36890;&#20449;&#21327;&#35758;&#12289;&#21306;&#22359;&#38142;&#25216;&#26415;&#21644;&#20449;&#24687;&#34701;&#21512;&#19982;AI&#38598;&#25104;&#65292;&#20197;&#25913;&#21892;&#20914;&#31361;&#36890;&#20449;&#65292;&#20026;&#20154;&#36947;&#20027;&#20041;&#21033;&#30410;&#25552;&#20379;&#21487;&#38382;&#36131;&#20449;&#24687;&#20132;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a trusted human-AI communication network that integrates communication protocols, blockchain technology, and information fusion with AI to improve conflict communications for accountable information exchange regarding protected entities, critical infrastructure, and humanitarian signals and status updates for humans and machines in conflicts.
&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#23558;&#36234;&#26469;&#36234;&#22810;&#22320;&#22312;&#20914;&#31361;&#20013;&#20197;&#25968;&#23383;&#21644;&#29289;&#29702;&#26041;&#24335;&#21442;&#19982;&#65292;&#20294;&#32570;&#20047;&#19982;&#20154;&#31867;&#36827;&#34892;&#20154;&#36947;&#20027;&#20041;&#30446;&#30340;&#30340;&#21487;&#20449;&#36890;&#20449;&#12290;&#26412;&#25991;&#32771;&#34385;&#23558;&#36890;&#20449;&#21327;&#35758;&#65288;&#8220;&#30333;&#26071;&#21327;&#35758;&#8221;&#65289;&#12289;&#20998;&#24067;&#24335;&#36134;&#26412;&#8220;&#21306;&#22359;&#38142;&#8221;&#25216;&#26415;&#21644;&#20449;&#24687;&#34701;&#21512;&#19982;AI&#38598;&#25104;&#65292;&#20197;&#25913;&#21892;&#20914;&#31361;&#36890;&#20449;&#65292;&#31216;&#20026;&#8220;&#21463;&#20445;&#25252;&#30340;&#20445;&#35777;&#29702;&#35299;&#24773;&#20917;&#21644;&#23454;&#20307;&#8221;PAUSE&#12290;&#36825;&#26679;&#19968;&#20010;&#21487;&#20449;&#30340;&#20154;&#24037;&#26234;&#33021;&#36890;&#20449;&#32593;&#32476;&#21487;&#20197;&#25552;&#20379;&#20851;&#20110;&#21463;&#20445;&#25252;&#23454;&#20307;&#12289;&#20851;&#38190;&#22522;&#30784;&#35774;&#26045;&#12289;&#20154;&#36947;&#20027;&#20041;&#20449;&#21495;&#21644;&#20154;&#31867;&#21644;&#26426;&#22120;&#22312;&#20914;&#31361;&#20013;&#30340;&#29366;&#24577;&#26356;&#26032;&#30340;&#21487;&#38382;&#36131;&#20449;&#24687;&#20132;&#25442;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20960;&#20010;&#29616;&#23454;&#30340;&#28508;&#22312;&#26696;&#20363;&#30740;&#31350;&#65292;&#23558;&#36825;&#20123;&#25216;&#26415;&#38598;&#25104;&#21040;&#19968;&#20010;&#21487;&#20449;&#30340;&#20154;&#24037;&#26234;&#33021;&#32593;&#32476;&#20013;&#65292;&#20197;&#23454;&#29616;&#20154;&#36947;&#20027;&#20041;&#21033;&#30410;&#65292;&#21253;&#25324;&#23454;&#26102;&#26144;&#23556;&#20914;&#31361;&#21306;&#22495;&#30340;&#24179;&#27665;&#21644;&#25112;&#26007;&#20154;&#21592;&#65292;&#20026;&#36991;&#20813;&#20107;&#25925;&#20570;&#20934;&#22791;&#65292;&#24182;&#20351;&#29992;&#32593;&#32476;&#31649;&#29702;&#38169;&#35823;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligences (AI) will increasingly participate digitally and physically in conflicts, yet there is a lack of trused communications with humans for humanitarian purposes. In this paper we consider the integration of a communications protocol (the 'whiteflag protocol'), distributed ledger 'blockchain' technology, and information fusion with AI, to improve conflict communications called 'protected assurance understanding situation and entitities' PAUSE. Such a trusted human-AI communication network could provide accountable information exchange regarding protected entities, critical infrastructure, humanitiarian signals and status updates for humans and machines in conflicts. We examine several realistic potential case studies for the integration of these technologies into a trusted human-AI network for humanitarian benefit including mapping a conflict zone with civilians and combatants in real time, preparation to avoid incidents and using the network to manage misinformatio
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37319;&#29992;&#36328;&#27169;&#24577;&#20914;&#31361;&#35299;&#20915;&#30340;&#31070;&#32463;&#26426;&#22120;&#20154;&#33539;&#20363;&#65292;&#20351;&#26426;&#22120;&#20154;&#34920;&#29616;&#20986;&#31867;&#20154;&#30340;&#31038;&#20132;&#20851;&#27880;&#65292;&#20026;&#22686;&#24378;&#20154;&#26426;&#31038;&#20132;&#20114;&#21160;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;&#12290;</title><link>http://arxiv.org/abs/2111.01906</link><description>&lt;p&gt;
&#35757;&#32451;&#36807;&#30340;&#20154;&#24418;&#26426;&#22120;&#20154;&#21487;&#20197;&#25191;&#34892;&#31867;&#20154;&#30340;&#36328;&#27169;&#24577;&#31038;&#20132;&#20851;&#27880;&#21644;&#20914;&#31361;&#35299;&#20915;
&lt;/p&gt;
&lt;p&gt;
A trained humanoid robot can perform human-like crossmodal social attention and conflict resolution. (arXiv:2111.01906v5 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.01906
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37319;&#29992;&#36328;&#27169;&#24577;&#20914;&#31361;&#35299;&#20915;&#30340;&#31070;&#32463;&#26426;&#22120;&#20154;&#33539;&#20363;&#65292;&#20351;&#26426;&#22120;&#20154;&#34920;&#29616;&#20986;&#31867;&#20154;&#30340;&#31038;&#20132;&#20851;&#27880;&#65292;&#20026;&#22686;&#24378;&#20154;&#26426;&#31038;&#20132;&#20114;&#21160;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study adopts the neurorobotic paradigm of crossmodal conflict resolution to make a robot express human-like social attention, providing a new approach to enhance human-robot social interaction.
&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22686;&#24378;&#20154;&#26426;&#31038;&#20132;&#20114;&#21160;&#65292;&#26426;&#22120;&#20154;&#22312;&#22797;&#26434;&#30340;&#29616;&#23454;&#29615;&#22659;&#20013;&#22788;&#29702;&#22810;&#20010;&#31038;&#20132;&#32447;&#32034;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#36328;&#27169;&#24577;&#36755;&#20837;&#20449;&#24687;&#30340;&#19981;&#19968;&#33268;&#24615;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#65292;&#36825;&#21487;&#33021;&#23545;&#26426;&#22120;&#20154;&#30340;&#22788;&#29702;&#36896;&#25104;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#37319;&#29992;&#20102;&#36328;&#27169;&#24577;&#20914;&#31361;&#35299;&#20915;&#30340;&#31070;&#32463;&#26426;&#22120;&#20154;&#33539;&#20363;&#65292;&#20351;&#26426;&#22120;&#20154;&#34920;&#29616;&#20986;&#31867;&#20154;&#30340;&#31038;&#20132;&#20851;&#27880;&#12290;&#25105;&#20204;&#23545;37&#21517;&#21442;&#19982;&#32773;&#36827;&#34892;&#20102;&#19968;&#39033;&#34892;&#20026;&#23454;&#39564;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22278;&#26700;&#20250;&#35758;&#22330;&#26223;&#65292;&#26377;&#19977;&#20010;&#21160;&#30011;&#21270;&#30340;&#22836;&#20687;&#65292;&#20197;&#25552;&#39640;&#29983;&#24577;&#25928;&#24230;&#12290;&#27599;&#20010;&#22836;&#20687;&#37117;&#25140;&#30528;&#21307;&#29992;&#21475;&#32617;&#65292;&#36974;&#30422;&#20102;&#40763;&#23376;&#12289;&#22068;&#24052;&#21644;&#19979;&#24052;&#30340;&#38754;&#37096;&#32447;&#32034;&#12290;&#20013;&#22830;&#22836;&#20687;&#31227;&#21160;&#20854;&#30524;&#30555;&#27880;&#35270;&#65292;&#32780;&#22806;&#22260;&#22836;&#20687;&#21017;&#21457;&#20986;&#22768;&#38899;&#12290;&#20957;&#35270;&#26041;&#21521;&#21644;&#22768;&#38899;&#20301;&#32622;&#35201;&#20040;&#26159;&#31354;&#38388;&#19978;&#19968;&#33268;&#30340;&#65292;&#35201;&#20040;&#26159;&#19981;&#19968;&#33268;&#30340;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#20013;&#22830;&#22836;&#20687;&#30340;&#21160;&#24577;&#20957;&#35270;&#21487;&#20197;&#35302;&#21457;&#36328;&#27169;&#24577;&#31038;&#20132;&#20851;&#27880;&#21453;&#24212;&#12290;&#29305;&#21035;&#26159;&#65292;&#20154;&#31867;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
To enhance human-robot social interaction, it is essential for robots to process multiple social cues in a complex real-world environment. However, incongruency of input information across modalities is inevitable and could be challenging for robots to process. To tackle this challenge, our study adopted the neurorobotic paradigm of crossmodal conflict resolution to make a robot express human-like social attention. A behavioural experiment was conducted on 37 participants for the human study. We designed a round-table meeting scenario with three animated avatars to improve ecological validity. Each avatar wore a medical mask to obscure the facial cues of the nose, mouth, and jaw. The central avatar shifted its eye gaze while the peripheral avatars generated sound. Gaze direction and sound locations were either spatially congruent or incongruent. We observed that the central avatar's dynamic gaze could trigger crossmodal social attention responses. In particular, human performances are 
&lt;/p&gt;</description></item></channel></rss>