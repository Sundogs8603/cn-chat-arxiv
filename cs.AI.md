# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Reasoning in Large Language Models Through Symbolic Math Word Problems.](http://arxiv.org/abs/2308.01906) | 本文通过研究数学问题的符号化版本来解决大型语言模型（LLMs）的推理能力问题，该方法探索了一种自我提示的方法，鼓励符号化推理与数值答案保持一致。 |
| [^2] | [Revisiting Deformable Convolution for Depth Completion.](http://arxiv.org/abs/2308.01905) | 本文通过重新思考可变形卷积的思想，提出了一种以可变形核卷积作为一次通过的完善模块的有效架构，并经验性地证明了其优越性。研究表明，可变形卷积在深度完成中具有重要作用。 |
| [^3] | [How many preprints have actually been printed and why: a case study of computer science preprints on arXiv.](http://arxiv.org/abs/2308.01899) | 这个研究通过计算机科学预印本在arXiv上的案例研究，量化了有多少预印本最终被印刷在同行评审的期刊上。为了解决预印本和最终发表版本的对应问题，引入了基于语义的映射方法使用BERT。 |
| [^4] | [Improving Replay Sample Selection and Storage for Less Forgetting in Continual Learning.](http://arxiv.org/abs/2308.01895) | 本文研究了改进持续学习中回放样本选择和存储的方法，通过对蓄水池抽样和其他替代策略进行比较，并提供了寻找最佳存储样本数量的详细分析。 |
| [^5] | [Thespian: Multi-Character Text Role-Playing Game Agents.](http://arxiv.org/abs/2308.01872) | "Thespian"是一种多角色的文本角色扮演游戏智能体框架，具有学习模仿多个角色的能力，并通过柔性提示进行指导。该框架还使用注意机制以少量示例学习新角色，并在多角色学习和少量示例学习方面表现优于现有框架。 |
| [^6] | [ClassEval: A Manually-Crafted Benchmark for Evaluating LLMs on Class-level Code Generation.](http://arxiv.org/abs/2308.01861) | ClassEval是一种手工构建的类级代码生成基准，该研究首次尝试在这一更具挑战性的场景中评估LLMs，并发现现有LLMs在类级代码生成上的性能相对较差。GPT-4和GPT-3.5在类级代码生成方面表现出相对其他LLMs更卓越的优势。 |
| [^7] | [Synthesizing Long-Term Human Motions with Diffusion Models via Coherent Sampling.](http://arxiv.org/abs/2308.01850) | 该论文提出了一种利用扩散模型和连贯采样方法来解决长期人类动作生成中的连贯性问题的新方法。 |
| [^8] | [URET: Universal Robustness Evaluation Toolkit (for Evasion).](http://arxiv.org/abs/2308.01840) | 本论文提出了一个名为URET的通用鲁棒性评估工具包，该工具包可以生成各种类型和任务领域下的对抗性输入，以确保关键AI任务的安全和鲁棒性。 |
| [^9] | [The Capability of Large Language Models to Measure Psychiatric Functioning.](http://arxiv.org/abs/2308.01834) | 本研究调查了大型语言模型在通过患者采访和临床描述预测精神功能方面的能力，结果显示这些模型在预测抑郁症评分方面表现最好，与人类临床评估者的结果相近，揭示了通用临床语言模型在评估精神状况方面的潜力。 |
| [^10] | [Learning beyond sensations: how dreams organize neuronal representations.](http://arxiv.org/abs/2308.01830) | 梦境和想象不仅仅是感觉输入的再现，它们对于塑造大脑皮层的语义表示同样重要，可以通过生成虚拟经验来影响和组织这些表示。 |
| [^11] | [Hard Adversarial Example Mining for Improving Robust Fairness.](http://arxiv.org/abs/2308.01823) | 该研究针对对抗训练模型容易出现不公平问题的限制，提出了通过自适应的困难对抗样本挖掘来改善深度神经网络的鲁棒性和公平性。 |
| [^12] | [Deep Neural Networks Fused with Textures for Image Classification.](http://arxiv.org/abs/2308.01813) | 本论文提出了一种将全局纹理与局部补丁信息相结合的新方法，用于解决细粒度图像分类问题。通过提取深层特征和计算图像级纹理，将两者的优势相结合，实现了更高的分类准确性。 |
| [^13] | [Job Shop Scheduling via Deep Reinforcement Learning: a Sequence to Sequence approach.](http://arxiv.org/abs/2308.01797) | 这篇论文介绍了一种原始的深度强化学习方法，采用序列到序列的方法自动学习调度规则，适用于作业车间调度问题和其他最优作业调度任务。 |
| [^14] | [MAP: A Model-agnostic Pretraining Framework for Click-through Rate Prediction.](http://arxiv.org/abs/2308.01737) | 提出了一个模型无关的预训练框架，用于点击率预测，可以更好地利用多字段分类数据和大量用户点击日志，学习更广义和有效的特征和实例表示。 |
| [^15] | [Towards Self-organizing Personal Knowledge Assistants in Evolving Corporate Memories.](http://arxiv.org/abs/2308.01732) | 近十年来，我们的研究关注于自组织个人知识助手在不断演化的公司记忆中的应用。我们通过实验和结果总结了知识图构建、个人信息管理和知识工作支持等领域的贡献，并提供了相关工业使用案例。这些贡献是新发展的第一步。 |
| [^16] | [Local Large Language Models for Complex Structured Medical Tasks.](http://arxiv.org/abs/2308.01727) | 本文介绍了一种利用本地大型语言模型 (LLMs) 与本地训练相结合的方法，用于处理复杂的结构化医学任务。作者提出的方法通过从病理报告中提取疾病代码来展示其效果，结果表明利用LLaMA模型相较于BERT风格的模型，在各种评估指标上具有明显的优势。而且，LLaMA模型在处理大型数据集方面表现特别出色。 |
| [^17] | [Bees Local Phase Quantization Feature Selection for RGB-D Facial Expressions Recognition.](http://arxiv.org/abs/2308.01700) | 该论文研究了蜜蜂局部相位量化特征选择对RGB-D面部表情识别的影响，并与其他特征选择算法进行了比较。 |
| [^18] | [LiDAR-Camera Panoptic Segmentation via Geometry-Consistent and Semantic-Aware Alignment.](http://arxiv.org/abs/2308.01686) | 本文提出了LCPS，第一个LiDAR-相机全景分割网络，通过异步补偿像素对齐、语义感知区域对齐和点到体素特征传播的融合策略，显著提高了全景分割的性能。 |
| [^19] | [Evaluating Link Prediction Explanations for Graph Neural Networks.](http://arxiv.org/abs/2308.01682) | 本文评估了图神经网络的链路预测解释的质量，并提供了定量指标来衡量解释的质量。我们发现底层假设和技术细节对解释的质量有影响。 |
| [^20] | [NBIAS: A Natural Language Processing Framework for Bias Identification in Text.](http://arxiv.org/abs/2308.01681) | 本论文提出了一个名为NBIAS的自然语言处理框架，旨在识别文本中的偏见。通过收集来自社交媒体、医疗保健和职位招聘等领域的多样化数据构建数据集，并应用基于Transformer的令牌分类模型来识别偏见词/短语。通过定量和定性评估方法来评估模型的效果。 |
| [^21] | [MARLIM: Multi-Agent Reinforcement Learning for Inventory Management.](http://arxiv.org/abs/2308.01649) | 本文提出了一个名为MARLIM的新型多智能体强化学习框架，用于解决具有随机需求和交货时间的单层多产品供应链的库存管理问题，并通过数值实验证明了强化学习方法相对于传统基准方法的优势。 |
| [^22] | [Improving Wind Resistance Performance of Cascaded PID Controlled Quadcopters using Residual Reinforcement Learning.](http://arxiv.org/abs/2308.01648) | 本文提出了一种基于剩余强化学习的方法来改善四旋翼飞行器的风阻性能。通过学习补偿扰动的剩余部分，可以在保持级联PID控制器的简单性和方便性的同时，提高四旋翼飞行器对风扰动的性能。 |
| [^23] | [Interleaving GANs with knowledge graphs to support design creativity for book covers.](http://arxiv.org/abs/2308.01626) | 本文将生成对抗网络与知识图谱相互交错应用于书籍封面设计，通过改变输入标题获得多个选项并利用鉴别器选择最佳图像，表现出比之前方法更好的生成效果和更好的选择。 |
| [^24] | [ReIDTrack: Multi-Object Track and Segmentation Without Motion.](http://arxiv.org/abs/2308.01622) | ReIDTrack提出了一种无需运动的多目标跟踪和分割方法，通过使用高性能的检测和外观模型，大幅度提升了性能，在MOTS基准中取得了第一名。 |
| [^25] | [Assessing Systematic Weaknesses of DNNs using Counterfactuals.](http://arxiv.org/abs/2308.01614) | 通过反事实验证评估DNNs的系统性缺陷。 |
| [^26] | [DOLCE: A Descriptive Ontology for Linguistic and Cognitive Engineering.](http://arxiv.org/abs/2308.01597) | DOLCE是一种描述性本体论，具有广泛的应用领域。它基于认知和语言考虑，旨在建模人类在不同领域中的常识观点。DOLCE不直接涉及特定领域的知识，而是提供了一般的类别和关系，用于指导特定领域的知识建模。 |
| [^27] | [Holy Grail 2.0: From Natural Language to Constraint Models.](http://arxiv.org/abs/2308.01589) | 本文通过利用预训练的大型语言模型从文本问题描述中提取模型的方法，探索了一种使约束编程更易于采用的方法。 |
| [^28] | [Unsupervised Representation Learning for Time Series: A Review.](http://arxiv.org/abs/2308.01578) | 本文对时间序列领域的无监督表示学习方法进行了综述，提出了基于ULTS库的快速实现与统一评估，并为未来的研究提供了参考。 |
| [^29] | [Motion Planning Diffusion: Learning and Planning of Robot Motions with Diffusion Models.](http://arxiv.org/abs/2308.01557) | 本文提出了一种新的机器人运动学习与规划方法，通过学习扩散模型作为先验知识，可以加速运动规划优化过程。扩散模型能够在高维环境中有效地编码数据的多模态性，并可以直接从任务目标条件下的后验轨迹分布中进行采样。 |
| [^30] | [A Global Transport Capacity Risk Prediction Method for Rail Transit Based on Gaussian Bayesian Network.](http://arxiv.org/abs/2308.01556) | 本文提出了一种基于高斯贝叶斯网络的轨道交通全球运输能力风险预测方法，通过构建贝叶斯网络结构并利用最大似然估计方法进行参数学习，实现了对轨道交通网络的运输能力风险的预测和解释。 |
| [^31] | [InterAct: Exploring the Potentials of ChatGPT as a Cooperative Agent.](http://arxiv.org/abs/2308.01552) | 本研究将OpenAI的ChatGPT集成到具身智能体系统中，通过将ChatGPT分配不同角色并与原始语言模型集成，实现了98%的成功率，并在实际环境中展现了ChatGPT在理解和执行复杂任务方面的能力。 |
| [^32] | [Avoidance Navigation Based on Offline Pre-Training Reinforcement Learning.](http://arxiv.org/abs/2308.01551) | 本文提出了一种基于离线预训练强化学习的避障导航方法，通过高效的离线训练策略和收集专家经验的通用数据集，可以减少训练时间并提高强化学习奖励。通过先进的仿真和真实物理建模，缩小了仿真与真实之间的差距。在不同环境中，训练好的模型都能取得相同的效果。 |
| [^33] | [MusicLDM: Enhancing Novelty in Text-to-Music Generation Using Beat-Synchronous Mixup Strategies.](http://arxiv.org/abs/2308.01546) | MusicLDM通过稳定扩散和AudioLDM架构，结合重新训练对比语言-音频预训练模型和Hifi-GAN声码器，以解决音乐生成中的挑战。通过节奏同步的混合策略，对训练数据进行增广，提高新颖性并避免抄袭问题。 |
| [^34] | [Lode Enhancer: Level Co-creation Through Scaling.](http://arxiv.org/abs/2308.01543) | 本文探索了使用AI增强的上采样作为设计辅助工具，在2D游戏关卡设计中实现协同创作。我们使用深度神经网络来将人工降低分辨率的关卡片段上采样，并为此引入了一种能够学习上采样并处理不太常见图块的神经网络架构。经过与设计师的合作研究，我们发现设计师们喜欢这个工具的共同设计过程，并认为它具有潜力推动更多的开发工作。 |
| [^35] | [Non-equilibrium physics: from spin glasses to machine and neural learning.](http://arxiv.org/abs/2308.01538) | 本论文研究了无序系统中的新兴智能行为，并通过统计物理学探索学习机制和物理动力学之间的关系，以此为指导原则设计智能系统。 |
| [^36] | [Quantum Multi-Agent Reinforcement Learning for Autonomous Mobility Cooperation.](http://arxiv.org/abs/2308.01519) | 本论文提出了基于演员-评论家网络概念的量子多智能体强化学习（QMARL）算法，目标是应对多智能体系统中的参数利用和收敛困难问题。QMARL在可扩展性、参数利用效率和快速收敛方面具有优势，并通过定义奖励为多个智能体在计算时间上的任务精度来实现多智能体的协作。另外，论文还提出了一种名为投影价值测量（PVM）的可扩展技术来进一步提高系统的表现。 |
| [^37] | [Implicit Occupancy Flow Fields for Perception and Prediction in Self-Driving.](http://arxiv.org/abs/2308.01471) | 该论文提出了一种统一的感知和未来预测方法，利用隐式占用流场来表示自动驾驶车辆对周围环境的感知并预测其他交通参与者的行为。这种方法避免了不必要的计算和信息丢失的问题。 |
| [^38] | [VertexSerum: Poisoning Graph Neural Networks for Link Inference.](http://arxiv.org/abs/2308.01469) | VertexSerum是一种针对链路推理的图神经网络毒化攻击，通过放大链接连接性泄漏来增加图链接窃取的效果，并提出了一种可以嵌入到链接检测网络中的注意力机制。在实验中，VertexSerum在四个真实世界数据集和三种不同的GNN结构上平均提高了9.8％的AUC分数，且在黑盒和在线学习环境中均表现出有效性。 |
| [^39] | [Novel Physics-Based Machine-Learning Models for Indoor Air Quality Approximations.](http://arxiv.org/abs/2308.01438) | 本研究提出了六种新颖的基于物理学的机器学习模型，用于准确近似室内污染物浓度。所提出的模型结合了物理学中的状态空间概念、门控循环单元和分解技术。通过在加利福尼亚州一栋商业建筑中五个办公室收集的数据进行验证，结果表明所提出的模型具有较低的复杂度。 |
| [^40] | [ChatMOF: An Autonomous AI System for Predicting and Generating Metal-Organic Frameworks.](http://arxiv.org/abs/2308.01423) | ChatMOF是一种自主AI系统，用于预测和生成金属-有机骨架。通过利用大规模语言模型，它能够从文本输入中提取关键细节，并提供适当的回应。该系统通过组合代理、工具包和评估器的核心组件，实现了数据检索、性质预测和结构生成等多个任务。研究进一步展示了在材料科学中使用大型语言模型的优势和潜力。 |
| [^41] | [An Effective Data Creation Pipeline to Generate High-quality Financial Instruction Data for Large Language Model.](http://arxiv.org/abs/2308.01415) | 本文提出了一个精心设计的数据创建流水线，通过与金融专家的对话和反馈，在大型语言模型中生成了一个高质量的金融指令数据集。实验结果表明，该方法在生成准确、相关和金融风格响应方面取得了显著进展。 |
| [^42] | [HouYi: An open-source large language model specially designed for renewable energy and carbon neutrality field.](http://arxiv.org/abs/2308.01414) | 本论文发布了第一个专门为可再生能源领域设计的开源大型语言模型HouYi及其对应的可再生能源学术论文数据集REAP。HouYi展示了在生成可再生能源领域学术论文段落方面的强大能力，与ChatGPT相当，略优于Clau。 |
| [^43] | [LaFiCMIL: Rethinking Large File Classification from the Perspective of Correlated Multiple Instance Learning.](http://arxiv.org/abs/2308.01413) | LaFiCMIL是一个新的方法，从相关多实例学习的角度解决了Transformer模型输入长度限制的问题，可以用于改进大文件分类任务。 |
| [^44] | [Learning to Model the World with Language.](http://arxiv.org/abs/2308.01399) | 本论文提出了一种通过语言学习对世界进行建模的方法，利用语言帮助代理器预测未来并进行行动。通过学习多模态世界模型，代理器可以预测未来的文本和图像表示，并在模型回滚中进行行动。 |
| [^45] | [OpenFlamingo: An Open-Source Framework for Training Large Autoregressive Vision-Language Models.](http://arxiv.org/abs/2308.01390) | OpenFlamingo是一个开源框架，用于训练大型自回归视觉语言模型。它在多个数据集上表现良好，达到了对应模型性能的80%至89%。 |
| [^46] | [CausalOps -- Towards an Industrial Lifecycle for Causal Probabilistic Graphical Models.](http://arxiv.org/abs/2308.01375) | 提出了CausalOps，一个新的因果模型开发和应用的生命周期框架，旨在推动在实际应用中采用因果方法。 |
| [^47] | [An enhanced motion planning approach by integrating driving heterogeneity and long-term trajectory prediction for automated driving systems.](http://arxiv.org/abs/2308.01369) | 本文提出了一种通过集成驾驶异质性和长期轨迹预测来增强运动规划方法的自动驾驶系统。该方法利用层级模型将周围人驾驶车辆的驾驶行为和长期轨迹进行耦合，以提高驾驶安全性。 |
| [^48] | [Empirical Translation Process Research: Past and Possible Future Perspectives.](http://arxiv.org/abs/2308.01368) | 本文追踪了实证翻译过程研究的发展，提出了自由能原理和主动推理作为建模深嵌入式翻译过程的框架，为未来研究提供了激动人心的前景。 |
| [^49] | [EmbeddingTree: Hierarchical Exploration of Entity Features in Embedding.](http://arxiv.org/abs/2308.01329) | 本文提出了一种用于嵌入学习的层次探索算法EmbeddingTree和相应的可视化工具，能够解释具有语义的实体特征和嵌入向量之间的关联，并通过实验验证了其有效性。 |
| [^50] | [DeepSpeed-Chat: Easy, Fast and Affordable RLHF Training of ChatGPT-like Models at All Scales.](http://arxiv.org/abs/2308.01320) | DeepSpeed-Chat是一个新颖的系统，使得ChatGPT-like模型的RLHF培训易于访问，高效且经济实惠。它具有易于使用的训练和推断体验，复制了InstructGPT的训练流程，并集成了各种训练和推断优化，提供了无与伦比的效率和可扩展性。 |
| [^51] | [Recent advancement in Disease Diagnostic using machine learning: Systematic survey of decades, comparisons, and challenges.](http://arxiv.org/abs/2308.01319) | 本文综述了近年来计算机辅助诊断领域在疾病诊断方面使用机器学习的最新进展，探讨了机器学习算法对于疾病检测和诊断的重要性和应用，以及其在分析生物医学数据方面的优势和挑战。 |
| [^52] | [FusionAD: Multi-modality Fusion for Prediction and Planning Tasks of Autonomous Driving.](http://arxiv.org/abs/2308.01006) | FusionAD是第一个将来自相机和激光雷达的信息融合起来用于自动驾驶预测和规划任务的统一框架，在常用数据集上的实验中达到了最先进的性能。 |
| [^53] | [Isolation and Induction: Training Robust Deep Neural Networks against Model Stealing Attacks.](http://arxiv.org/abs/2308.00958) | 这项研究提出了一种名为隔离和诱导（InI）的训练框架，用于对抗模型窃取攻击。该框架通过隔离对手的训练梯度，并直接训练一个防御模型，有效地解决了现有防御方法中推理计算开销高和准确性与防窃鲁棒性之间的不利权衡问题。 |
| [^54] | [On the Trustworthiness Landscape of State-of-the-art Generative Models: A Comprehensive Survey.](http://arxiv.org/abs/2307.16680) | 本文综合调查了大规模生成模型的可信度问题，涵盖了隐私、安全、公平性和责任等多个维度，并提出了实际建议和未来发展方向。 |
| [^55] | [Relation-Oriented: Toward Knowledge-Aligned Causal AI.](http://arxiv.org/abs/2307.16387) | 本研究从创新的关系导向视角出发，探讨了当前的建模范式中的观察模型与实际理解的不对齐问题，并提出了关系定义的表示学习方法作为实现关系导向建模的实践方法。 |
| [^56] | [Data-Driven Modeling with Experimental Augmentation for the Modulation Strategy of the Dual-Active-Bridge Converter.](http://arxiv.org/abs/2307.16173) | 本文介绍了一种基于实验增强的数据驱动建模方法，该方法通过结合仿真数据和实验数据来减轻模型差异，并在实践中提高准确性。 |
| [^57] | [An Effective LSTM-DDPM Scheme for Energy Theft Detection and Forecasting in Smart Grid.](http://arxiv.org/abs/2307.16149) | 这篇论文提出了一种利用LSTM和DDPM相结合的方案来解决智能电网系统中的能量盗窃检测和预测问题。通过重构和预测误差，系统能够准确识别能量盗窃的实例，并在实验中表现出较好的性能。 |
| [^58] | [Freespace Optical Flow Modeling for Automated Driving.](http://arxiv.org/abs/2307.15989) | 本论文提出了一种在碰撞自由空间中建模光流的策略，充分利用了三维驾驶环境中的几何信息，通过对多个数据集进行广泛的实验，实现了光流的明确表示和光流分量与垂直坐标之间的二次关系。 |
| [^59] | [Masked Diffusion Models Are Fast and Privacy-Aware Learners.](http://arxiv.org/abs/2306.11363) | 该论文提出了一种基于先验的去噪训练框架，通过遮蔽学习和扩散模型的结合，实现了更高效的训练和生成更高质量的图像。 |
| [^60] | [Inductive reasoning in humans and large language models.](http://arxiv.org/abs/2306.06548) | 本研究使用GPT-3.5和GPT-4对人类归纳推理中的属性归纳问题进行了实验。结果表明，尽管GPT-3.5有一些困难，但GPT-4的表现与人类相似，除了未能捕捉到前提的非单调性现象。这项工作为人类和机器智能提供了有趣的比较，并提供了用作未来研究基准的两个大型数据集。 |
| [^61] | [Unsupervised Compositional Concepts Discovery with Text-to-Image Generative Models.](http://arxiv.org/abs/2306.05357) | 本论文提出了一种无监督的方法，用于从图像中自动地发现不同的生成概念，并且这些生成概念可以被用于重新组合和生成新的艺术和混合图像，并作为一种表示用于下游的分类任务。 |
| [^62] | [Abnormal Trading Detection in the NFT Market.](http://arxiv.org/abs/2306.04643) | 本文提出了一种通过聚类算法检测非同质化代币（NFT）交易市场中的异常行为的方法，并探讨了监管对减少欺诈行为的影响。 |
| [^63] | [From Latent Graph to Latent Topology Inference: Differentiable Cell Complex Module.](http://arxiv.org/abs/2305.16174) | 该论文研究了一种从潜在图到潜在拓扑推断的方法，通过引入可微分的单复形模块（DCM），学习描述数据点之间多向交互的高阶单复形的稀疏且不规则的拓扑结构，并展示了如何将其与单复形消息传递网络层集成以提高下游任务的效果。 |
| [^64] | [LaDI-VTON: Latent Diffusion Textual-Inversion Enhanced Virtual Try-On.](http://arxiv.org/abs/2305.13501) | 本文介绍了LaDI-VTON，一种利用潜在扩散模型和文本反演组件增强虚拟试穿任务的解决方案，该模型是第一个将这两种方法相结合的模型，能够高质量还原商店服装的细节。 |
| [^65] | [Variational Classification.](http://arxiv.org/abs/2305.10406) | 提出一种新的变分分类方法，通过引入潜变量建模来优化训练，允许灵活的设计选择以改善校准和对抗鲁棒性，实验结果表明其对于域外数据的分类准确性得到了保持。 |
| [^66] | [Mlinear: Rethink the Linear Model for Time-series Forecasting.](http://arxiv.org/abs/2305.04800) | 该论文重新思考了时间序列预测的线性模型，提出了Mlinear方法，通过动态调节通道独立性和通道依赖性属性以实现更好的预测性能。 |
| [^67] | [An automatically discovered chain-of-thought prompt generalizes to novel models and datasets.](http://arxiv.org/abs/2305.02897) | 本文研究了一系列零照顾提示在六个最新发布的语言模型和问题回答数据集的实验中的表现，发现自动提示发现的CoT提示可在新模型和数据集上表现良好，并在应用于GPT-4模型时取得最佳结果。 |
| [^68] | [OpenAGI: When LLM Meets Domain Experts.](http://arxiv.org/abs/2304.04370) | 基于大型语言模型的OpenAGI平台通过整合领域专家模型和自然语言问答形式，实现复杂任务解决。 |
| [^69] | [A closer look at the training dynamics of knowledge distillation.](http://arxiv.org/abs/2303.11098) | 本文对知识蒸馏的训练动态进行了详细研究，实验证明投影器的设计决策、表示的标准化和软最大函数的选择对学生的性能有着重要影响，同时提出了一种解决容量差异问题的简单方法，以及与当前最先进的知识蒸馏技术相媲美的计算效率更高的方法。 |
| [^70] | [Optimal foraging strategies can be learned and outperform L\'evy walks.](http://arxiv.org/abs/2303.06050) | 研究发现，通过强化学习代理，存活的生物可以学习到优于 L\'evy walks 的觅食策略，以提高觅食效率。 |
| [^71] | [SLCA: Slow Learner with Classifier Alignment for Continual Learning on a Pre-trained Model.](http://arxiv.org/abs/2303.05118) | SLCA是一种用于连续学习的简单但极其有效的方法。它通过慢学习和分类器对齐来在预训练模型上提高泛化能力和解决渐进过拟合问题。 |
| [^72] | [Multimodality Helps Unimodality: Cross-Modal Few-Shot Learning with Multimodal Models.](http://arxiv.org/abs/2301.06267) | 通过跨模态适应方法，在多模态模型下利用少样本示例（包括文本和声音）进行狗的视觉分类，并取得了最先进的结果。 |
| [^73] | [BEVBert: Multimodal Map Pre-training for Language-guided Navigation.](http://arxiv.org/abs/2212.04385) | 本论文提出了一种多模态地图预训练方法，用于语言导向导航任务。通过构建局部度量地图和全局拓扑地图，该方法能够准确刻画空间感知和导航依赖关系，从而提高了语言导向导航的性能。 |
| [^74] | [An Unsupervised Machine Learning Approach for Ground-Motion Spectra Clustering and Selection.](http://arxiv.org/abs/2212.03188) | 本论文提出了一种无监督机器学习方法，用于提取地震动谱的定义特征，以辅助地震动选择。它结合了机器发现的潜在特征和传统强度测量，通过聚类分析选择代表性的地震动记录。验证结果表明该方法的有效性。 |
| [^75] | [No Agreement Without Loss: Learning and Social Choice in Peer Review.](http://arxiv.org/abs/2211.02144) | 在本文中，我们挑战了Nothigattu、Shah和Procaccia的框架，该框架旨在通过最小化损失函数来聚合评审人的映射，我们发现了一些负面结果。 |
| [^76] | [Fairness in Recommendation: Foundations, Methods and Applications.](http://arxiv.org/abs/2205.13619) | 这篇论文对推荐系统中的公平性问题进行了系统调查，针对推荐过程中可能出现的数据或算法偏见，提供了一些方法和应用来提升推荐中的公平性。 |
| [^77] | [Successor Feature Neural Episodic Control.](http://arxiv.org/abs/2111.03110) | 本文研究了记忆控制和后继特征两个框架的整合，通过结合这两种方法，提高了强化学习的样本效率和策略重用的优雅性。 |
| [^78] | [Auto-COP: Adaptation Generation in Context-Oriented Programming using Reinforcement Learning Options.](http://arxiv.org/abs/2103.06757) | 运行时生成适应性的新技术Auto-COP在上下文导向编程中使用强化学习选项来生成动作序列，从而实现自适应系统的开发。 |
| [^79] | [ROME: Robustifying Memory-Efficient NAS via Topology Disentanglement and Gradient Accumulation.](http://arxiv.org/abs/2011.11233) | ROME是一种鲁棒的内存高效的NAS方法，通过拓扑解耦和梯度累积解决了单路径DARTS中的性能下降问题。 |

# 详细

[^1]: 大型语言模型通过符号化数学问题进行推理

    Reasoning in Large Language Models Through Symbolic Math Word Problems. (arXiv:2308.01906v1 [cs.CL])

    [http://arxiv.org/abs/2308.01906](http://arxiv.org/abs/2308.01906)

    本文通过研究数学问题的符号化版本来解决大型语言模型（LLMs）的推理能力问题，该方法探索了一种自我提示的方法，鼓励符号化推理与数值答案保持一致。

    

    大型语言模型（LLMs）通过解决几乎没有标记数据的下游任务，改变了自然语言处理（NLP）的方式。尽管它们具有多功能的能力，但对它们的推理能力的问题仍然不太清楚。本文通过研究数学问题的符号化版本来解决数学问题的推理问题，因为符号表达是对数值答案的“简明解释”。我们创建并使用了SVAMP数据集的符号化版本，并发现GPT-3的davinci-002模型在符号化数学问题上也具有良好的零样本准确性。为了评估模型推理的准确性，我们不仅考虑准确率，还评估最终答案和推理结果之间的一致性，分别对应于数值和符号化答案的数学问题。我们探索了一种自我提示的方法，鼓励符号化推理与数值答案保持一致，从而使LLM能够提供简明且可验证的推理。

    Large language models (LLMs) have revolutionized NLP by solving downstream tasks with little to no labeled data. Despite their versatile abilities, the larger question of their ability to reason remains ill-understood. This paper addresses reasoning in math word problems (MWPs) by studying symbolic versions of the numeric problems, since a symbolic expression is a "concise explanation" of the numeric answer. We create and use a symbolic version of the SVAMP dataset and find that GPT-3's davinci-002 model also has good zero-shot accuracy on symbolic MWPs. To evaluate the faithfulness of the model's reasoning, we go beyond accuracy and additionally evaluate the alignment between the final answer and the outputted reasoning, which correspond to numeric and symbolic answers respectively for MWPs. We explore a self-prompting approach to encourage the symbolic reasoning to align with the numeric answer, thus equipping the LLM with the ability to provide a concise and verifiable reasoning and
    
[^2]: 重新思考深度完成的可变形卷积

    Revisiting Deformable Convolution for Depth Completion. (arXiv:2308.01905v1 [cs.CV])

    [http://arxiv.org/abs/2308.01905](http://arxiv.org/abs/2308.01905)

    本文通过重新思考可变形卷积的思想，提出了一种以可变形核卷积作为一次通过的完善模块的有效架构，并经验性地证明了其优越性。研究表明，可变形卷积在深度完成中具有重要作用。

    

    深度完成旨在从稀疏深度图生成高质量的密集深度图，近年来引起了越来越多的关注。以往的工作通常使用RGB图像作为引导，并引入迭代的空间传播来完善估计的粗糙深度图。然而，大多数传播完善方法需要多次迭代，并且存在固定的感受野，可能包含与非常稀疏输入无关和无用的信息。本文通过重新思考可变形卷积的思想，同时解决了这两个挑战。我们提出了一种有效的架构，利用可变形核卷积作为一次通过的完善模块，并经验性地证明了其优越性。为了更好地理解可变形卷积的功能并将其应用于深度完成，我们进一步系统地研究了各种代表性的策略。我们的研究揭示了与以往工作不同，可变形卷积在深度完成中的作用。

    Depth completion, which aims to generate high-quality dense depth maps from sparse depth maps, has attracted increasing attention in recent years. Previous work usually employs RGB images as guidance, and introduces iterative spatial propagation to refine estimated coarse depth maps. However, most of the propagation refinement methods require several iterations and suffer from a fixed receptive field, which may contain irrelevant and useless information with very sparse input. In this paper, we address these two challenges simultaneously by revisiting the idea of deformable convolution. We propose an effective architecture that leverages deformable kernel convolution as a single-pass refinement module, and empirically demonstrate its superiority. To better understand the function of deformable convolution and exploit it for depth completion, we further systematically investigate a variety of representative strategies. Our study reveals that, different from prior work, deformable convol
    
[^3]: 有多少预印本实际上被印刷了，以及为什么：计算机科学预印本在arXiv上的案例研究

    How many preprints have actually been printed and why: a case study of computer science preprints on arXiv. (arXiv:2308.01899v1 [cs.DL])

    [http://arxiv.org/abs/2308.01899](http://arxiv.org/abs/2308.01899)

    这个研究通过计算机科学预印本在arXiv上的案例研究，量化了有多少预印本最终被印刷在同行评审的期刊上。为了解决预印本和最终发表版本的对应问题，引入了基于语义的映射方法使用BERT。

    

    预印本在学术界扮演着越来越重要的角色。研究人员在正式提交到期刊或会议之前将他们的手稿发布到预印本服务器上的原因有很多，但预印本的使用也引发了不少争议，尤其是与优先权的声明有关。本文对2008年至2017年期间提交到arXiv的计算机科学预印本进行了案例研究，以量化最终有多少预印本在同行评审的场合中被印刷。在这些已发表的手稿中，有些以不同的标题发表，且未更新arXiv上的预印本。对于这些手稿，传统的模糊匹配方法无法将预印本与最终发表版本对应起来。鉴于这个问题，我们引入了一种基于语义的映射方法，利用Transformers中的Bidirectional Encoder Representations (BERT)。利用这种新的映射方法和多种数据来源...

    Preprints play an increasingly critical role in academic communities. There are many reasons driving researchers to post their manuscripts to preprint servers before formal submission to journals or conferences, but the use of preprints has also sparked considerable controversy, especially surrounding the claim of priority. In this paper, a case study of computer science preprints submitted to arXiv from 2008 to 2017 is conducted to quantify how many preprints have eventually been printed in peer-reviewed venues. Among those published manuscripts, some are published under different titles and without an update to their preprints on arXiv. In the case of these manuscripts, the traditional fuzzy matching method is incapable of mapping the preprint to the final published version. In view of this issue, we introduce a semantics-based mapping method with the employment of Bidirectional Encoder Representations from Transformers (BERT). With this new mapping method and a plurality of data sou
    
[^4]: 改进回放样本选择和存储以减少持续学习中的遗忘问题

    Improving Replay Sample Selection and Storage for Less Forgetting in Continual Learning. (arXiv:2308.01895v1 [cs.LG])

    [http://arxiv.org/abs/2308.01895](http://arxiv.org/abs/2308.01895)

    本文研究了改进持续学习中回放样本选择和存储的方法，通过对蓄水池抽样和其他替代策略进行比较，并提供了寻找最佳存储样本数量的详细分析。

    

    持续学习旨在使深度学习者能够在一系列未知长度的任务上进行训练，而不会遗忘以前的任务。其中一种有效的解决方案是回放，即将少量先前的经验存储在内存中，并在学习当前任务时重新播放它们。然而，在选择最有信息量的样本进行存储以及确定所需存储样本的最佳数量方面仍有改进的空间。本研究旨在通过对常用的蓄水池抽样与各种替代方法进行比较，提供一个新颖的详细分析，以解决这些问题，并找到最佳的存储样本数。

    Continual learning seeks to enable deep learners to train on a series of tasks of unknown length without suffering from the catastrophic forgetting of previous tasks. One effective solution is replay, which involves storing few previous experiences in memory and replaying them when learning the current task. However, there is still room for improvement when it comes to selecting the most informative samples for storage and determining the optimal number of samples to be stored. This study aims to address these issues with a novel comparison of the commonly used reservoir sampling to various alternative population strategies and providing a novel detailed analysis of how to find the optimal number of stored samples.
    
[^5]: Thespian: 多角色文本角色扮演游戏智能体

    Thespian: Multi-Character Text Role-Playing Game Agents. (arXiv:2308.01872v1 [cs.AI])

    [http://arxiv.org/abs/2308.01872](http://arxiv.org/abs/2308.01872)

    "Thespian"是一种多角色的文本角色扮演游戏智能体框架，具有学习模仿多个角色的能力，并通过柔性提示进行指导。该框架还使用注意机制以少量示例学习新角色，并在多角色学习和少量示例学习方面表现优于现有框架。

    

    文本冒险游戏和文本角色扮演游戏是强化学习游戏智能体的重大挑战。文本角色扮演游戏是开放式环境，智能体必须忠实地扮演特定角色。文章考虑到了字符和演员之间的区别，演员智能体能够扮演多个角色。我们提出了一个名为"Thespian"的框架，使其能够学习模仿多个角色，并使用柔性提示来指导其在任何时候扮演特定角色。此外，我们还描述了一个注意机制，允许智能体以少量示例的方式学习基于先前学习的角色的新角色。我们证明了我们的智能体在多角色学习和少量示例学习方面优于现有的智能体框架。

    Text-adventure games and text role-playing games are grand challenges for reinforcement learning game playing agents. Text role-playing games are open-ended environments where an agent must faithfully play a particular character. We consider the distinction between characters and actors, where an actor agent has the ability to play multiple characters. We present a framework we call a thespian agent that can learn to emulate multiple characters along with a soft prompt that can be used to direct it as to which character to play at any time. We further describe an attention mechanism that allows the agent to learn new characters that are based on previously learned characters in a few-shot fashion. We show that our agent outperforms the state of the art agent framework in multi-character learning and few-shot learning.
    
[^6]: ClassEval: 一种手工构建的用于评估LLMs在类级代码生成上的基准

    ClassEval: A Manually-Crafted Benchmark for Evaluating LLMs on Class-level Code Generation. (arXiv:2308.01861v1 [cs.CL])

    [http://arxiv.org/abs/2308.01861](http://arxiv.org/abs/2308.01861)

    ClassEval是一种手工构建的类级代码生成基准，该研究首次尝试在这一更具挑战性的场景中评估LLMs，并发现现有LLMs在类级代码生成上的性能相对较差。GPT-4和GPT-3.5在类级代码生成方面表现出相对其他LLMs更卓越的优势。

    

    在这项工作中，我们首次尝试在更具挑战性的代码生成场景中评估LLMs，即类级代码生成。我们首先手动构建了第一个类级代码生成基准ClassEval，其中包含100个类级Python代码生成任务，总共耗时约500人小时。在此基础上，我们对11个最先进的LLMs在类级代码生成上进行了第一次研究。根据我们的结果，我们得出以下主要发现。首先，我们发现所有现有的LLMs在类级代码生成上的性能要远低于独立的方法级代码生成基准（如HumanEval）；而方法级的编码能力不能等同地反映LLMs在类级编码能力上的表现。其次，我们发现GPT-4和GPT-3.5在类级代码生成上仍然表现出相对其他LLMs更卓越的优势，而二线模型包括Instruct-Starcoder，Instruct-Codegen和Wizardcoder在性能上非常相似。

    In this work, we make the first attempt to evaluate LLMs in a more challenging code generation scenario, i.e. class-level code generation. We first manually construct the first class-level code generation benchmark ClassEval of 100 class-level Python code generation tasks with approximately 500 person-hours. Based on it, we then perform the first study of 11 state-of-the-art LLMs on class-level code generation. Based on our results, we have the following main findings. First, we find that all existing LLMs show much worse performance on class-level code generation compared to on standalone method-level code generation benchmarks like HumanEval; and the method-level coding ability cannot equivalently reflect the class-level coding ability among LLMs. Second, we find that GPT-4 and GPT-3.5 still exhibit dominate superior than other LLMs on class-level code generation, and the second-tier models includes Instruct-Starcoder, Instruct-Codegen, and Wizardcoder with very similar performance. 
    
[^7]: 通过一致采样使用扩散模型合成长期人类动作

    Synthesizing Long-Term Human Motions with Diffusion Models via Coherent Sampling. (arXiv:2308.01850v1 [cs.CV])

    [http://arxiv.org/abs/2308.01850](http://arxiv.org/abs/2308.01850)

    该论文提出了一种利用扩散模型和连贯采样方法来解决长期人类动作生成中的连贯性问题的新方法。

    

    文本到动作的生成越来越受关注，但大多数现有方法仅限于生成与描述单个动作的单个句子相对应的短期动作。然而，当一个文本流描述一系列连续的动作时，与每个句子相对应的生成的动作可能不是连贯的。现有的长期动作生成方法面临两个主要问题。首先，它们不能直接生成连贯的动作，需要其他操作（如插值）来处理生成的动作。其次，它们以自回归方式生成后续动作，而不考虑未来动作对先前动作的影响。为了解决这些问题，我们提出了一种新的方法，利用过去条件的扩散模型和两种可选的连贯采样方法：过去修补采样和组合转换采样。

    Text-to-motion generation has gained increasing attention, but most existing methods are limited to generating short-term motions that correspond to a single sentence describing a single action. However, when a text stream describes a sequence of continuous motions, the generated motions corresponding to each sentence may not be coherently linked. Existing long-term motion generation methods face two main issues. Firstly, they cannot directly generate coherent motions and require additional operations such as interpolation to process the generated actions. Secondly, they generate subsequent actions in an autoregressive manner without considering the influence of future actions on previous ones. To address these issues, we propose a novel approach that utilizes a past-conditioned diffusion model with two optional coherent sampling methods: Past Inpainting Sampling and Compositional Transition Sampling. Past Inpainting Sampling completes subsequent motions by treating previous motions as
    
[^8]: URET: 通用鲁棒性评估工具包（用于逃避攻击）

    URET: Universal Robustness Evaluation Toolkit (for Evasion). (arXiv:2308.01840v1 [cs.LG])

    [http://arxiv.org/abs/2308.01840](http://arxiv.org/abs/2308.01840)

    本论文提出了一个名为URET的通用鲁棒性评估工具包，该工具包可以生成各种类型和任务领域下的对抗性输入，以确保关键AI任务的安全和鲁棒性。

    

    众所周知，机器学习模型容易受到逃避攻击的影响，如图像分类模型所示。充分了解这种攻击对于确保关键AI任务的安全和鲁棒性至关重要。然而，大多数逃避攻击很难对大多数AI系统进行部署，因为它们仅集中在图像领域并具有少数约束。与实践中使用的其他输入类型不同，图像由均匀的、数值的、连续的和独立的特征组成。此外，某些输入类型包含额外的语义和功能约束，必须遵守以生成逼真的对抗性输入。在这项工作中，我们提出了一个新的框架，可以生成与输入类型和任务领域无关的对抗性输入。给定一个输入和一组预定义的输入转换，我们的框架可以发现一系列转换，从而得到一个符合语义和功能要求的正确结果。

    Machine learning models are known to be vulnerable to adversarial evasion attacks as illustrated by image classification models. Thoroughly understanding such attacks is critical in order to ensure the safety and robustness of critical AI tasks. However, most evasion attacks are difficult to deploy against a majority of AI systems because they have focused on image domain with only few constraints. An image is composed of homogeneous, numerical, continuous, and independent features, unlike many other input types to AI systems used in practice. Furthermore, some input types include additional semantic and functional constraints that must be observed to generate realistic adversarial inputs. In this work, we propose a new framework to enable the generation of adversarial inputs irrespective of the input type and task domain. Given an input and a set of pre-defined input transformations, our framework discovers a sequence of transformations that result in a semantically correct and functi
    
[^9]: 大型语言模型在评估精神状况方面的能力

    The Capability of Large Language Models to Measure Psychiatric Functioning. (arXiv:2308.01834v1 [cs.CL])

    [http://arxiv.org/abs/2308.01834](http://arxiv.org/abs/2308.01834)

    本研究调查了大型语言模型在通过患者采访和临床描述预测精神功能方面的能力，结果显示这些模型在预测抑郁症评分方面表现最好，与人类临床评估者的结果相近，揭示了通用临床语言模型在评估精神状况方面的潜力。

    

    本研究探讨了基于大规模医学知识（Med-PaLM 2）训练的大型语言模型（LLM）在没有经过特定训练的情况下，通过患者采访和临床描述来预测精神功能的能力。通过使用提示来提取估计的临床评分和诊断，分析了145例抑郁症和115例创伤后应激障碍评估以及46例临床案例研究。结果表明，Med-PaLM 2能够在一系列精神疾病中评估精神功能，其中对基于标准评估的抑郁症评分的预测表现最佳（准确率范围= 0.80-0.84），这些预测结果在统计上与人类临床评估者的结果无法区分（t(1,144) = 1.20，p = 0.23）。结果显示了通用临床语言模型在评估精神状况方面的潜力。

    The current work investigates the capability of Large language models (LLMs) that are explicitly trained on large corpuses of medical knowledge (Med-PaLM 2) to predict psychiatric functioning from patient interviews and clinical descriptions without being trained to do so. To assess this, n = 145 depression and n =115 PTSD assessments and n = 46 clinical case studies across high prevalence/high comorbidity disorders (Depressive, Anxiety, Psychotic, trauma and stress, Addictive disorders) were analyzed using prompts to extract estimated clinical scores and diagnoses. Results demonstrate that Med-PaLM 2 is capable of assessing psychiatric functioning across a range of psychiatric conditions with the strongest performance being the prediction of depression scores based on standardized assessments (Accuracy range= 0.80 - 0.84) which were statistically indistinguishable from human clinical raters t(1,144) = 1.20; p = 0.23. Results show the potential for general clinical language models to f
    
[^10]: 超越感觉的学习: 梦如何组织神经表示

    Learning beyond sensations: how dreams organize neuronal representations. (arXiv:2308.01830v1 [q-bio.NC])

    [http://arxiv.org/abs/2308.01830](http://arxiv.org/abs/2308.01830)

    梦境和想象不仅仅是感觉输入的再现，它们对于塑造大脑皮层的语义表示同样重要，可以通过生成虚拟经验来影响和组织这些表示。

    

    高级感觉皮层中的语义表示是稳健而灵活行为的基础。这些表示是在发育过程中以无监督的方式获得的，并在生物的整个生命周期中持续维持。预测性学习理论认为，这些表示是通过预测或重建感觉输入而产生的。然而，众所周知，大脑会产生虚拟经验，例如在想象和梦境中，超越先前经历的输入。在这里，我们提出虚拟经验可能与实际感觉输入同样重要，能够塑造皮层表示。特别是，我们讨论了通过生成虚拟经验来组织表示的两个互补学习原则。首先，“对抗性梦想”提出创造性梦想支持了在皮层中实现对抗性学习，其中反馈和前馈路径参与到相互欺骗的有益游戏中。

    Semantic representations in higher sensory cortices form the basis for robust, yet flexible behavior. These representations are acquired over the course of development in an unsupervised fashion and continuously maintained over an organism's lifespan. Predictive learning theories propose that these representations emerge from predicting or reconstructing sensory inputs. However, brains are known to generate virtual experiences, such as during imagination and dreaming, that go beyond previously experienced inputs. Here, we suggest that virtual experiences may be just as relevant as actual sensory inputs in shaping cortical representations.In particular, we discuss two complementary learning principles that organize representations through the generation of virtual experiences. First, "adversarial dreaming" proposes that creative dreams support a cortical implementation of adversarial learning in which feedback and feedforward pathways engage in a productive game of trying to fool each o
    
[^11]: 提高深度神经网络鲁棒性和公平性的困难对抗样本挖掘

    Hard Adversarial Example Mining for Improving Robust Fairness. (arXiv:2308.01823v1 [cs.LG])

    [http://arxiv.org/abs/2308.01823](http://arxiv.org/abs/2308.01823)

    该研究针对对抗训练模型容易出现不公平问题的限制，提出了通过自适应的困难对抗样本挖掘来改善深度神经网络的鲁棒性和公平性。

    

    对抗训练（AT）被广泛认为是提高深度神经网络（DNNs）对抗性样本（AE）鲁棒性的最先进技术。然而，最近的研究发现，经过对抗训练的模型容易出现不公平问题，限制了它们的适用性。在本文中，我们通过经验观察发现，这个限制可能是由于严重的对抗置信过拟合，即某些具有过度自信的对抗性样本。为了缓解这个问题，我们提出了HAM，这是一个简单但有效的框架，通过自适应的困难对抗样本挖掘。HAM集中于以适应性的方式挖掘困难对抗性样本，同时丢弃容易的样本。具体地，HAM根据计算损失值时需要穿过决策边界的步长来识别困难的AE。此外，还引入了早期丢弃机制来在AE生成的初期丢弃容易的样本，从而使得网络更加鲁棒和公平。

    Adversarial training (AT) is widely considered the state-of-the-art technique for improving the robustness of deep neural networks (DNNs) against adversarial examples (AE). Nevertheless, recent studies have revealed that adversarially trained models are prone to unfairness problems, restricting their applicability. In this paper, we empirically observe that this limitation may be attributed to serious adversarial confidence overfitting, i.e., certain adversarial examples with overconfidence. To alleviate this problem, we propose HAM, a straightforward yet effective framework via adaptive Hard Adversarial example Mining.HAM concentrates on mining hard adversarial examples while discarding the easy ones in an adaptive fashion. Specifically, HAM identifies hard AEs in terms of their step sizes needed to cross the decision boundary when calculating loss value. Besides, an early-dropping mechanism is incorporated to discard the easy examples at the initial stages of AE generation, resulting
    
[^12]: 深度神经网络与纹理相结合的图像分类方法

    Deep Neural Networks Fused with Textures for Image Classification. (arXiv:2308.01813v1 [cs.CV])

    [http://arxiv.org/abs/2308.01813](http://arxiv.org/abs/2308.01813)

    本论文提出了一种将全局纹理与局部补丁信息相结合的新方法，用于解决细粒度图像分类问题。通过提取深层特征和计算图像级纹理，将两者的优势相结合，实现了更高的分类准确性。

    

    细粒度图像分类是计算机视觉中的一个具有挑战性的任务，由于亚类别之间视觉差异较小，但是类内变化较大。深度学习方法在解决细粒度图像分类方面取得了显著的成功。本文提出了一种融合方法来解决细粒度图像分类问题，通过将全局纹理与基于局部补丁信息的特征相结合。第一个步骤从各种固定大小的非重叠补丁中提取深层特征，并使用长短期记忆（LSTM）进行特征编码。另一个步骤使用局部二值模式（LBP）在多个尺度上计算图像级纹理。两个流的优点被整合到表示高效特征向量的方法中，用于图像分类。该方法在代表人脸、皮肤病变、食物盘子、海洋生物等的八个数据集上经过四个标准骨干卷积神经网络的测试。与现有方法相比，我们的方法在分类准确性上取得了更好的结果。

    Fine-grained image classification (FGIC) is a challenging task in computer vision for due to small visual differences among inter-subcategories, but, large intra-class variations. Deep learning methods have achieved remarkable success in solving FGIC. In this paper, we propose a fusion approach to address FGIC by combining global texture with local patch-based information. The first pipeline extracts deep features from various fixed-size non-overlapping patches and encodes features by sequential modelling using the long short-term memory (LSTM). Another path computes image-level textures at multiple scales using the local binary patterns (LBP). The advantages of both streams are integrated to represent an efficient feature vector for image classification. The method is tested on eight datasets representing the human faces, skin lesions, food dishes, marine lives, etc. using four standard backbone CNNs. Our method has attained better classification accuracy over existing methods with no
    
[^13]: 用深度强化学习解决作业车间调度问题: 一种序列到序列的方法。

    Job Shop Scheduling via Deep Reinforcement Learning: a Sequence to Sequence approach. (arXiv:2308.01797v1 [cs.AI])

    [http://arxiv.org/abs/2308.01797](http://arxiv.org/abs/2308.01797)

    这篇论文介绍了一种原始的深度强化学习方法，采用序列到序列的方法自动学习调度规则，适用于作业车间调度问题和其他最优作业调度任务。

    

    作业调度是一个众所周知的组合优化问题，具有无尽的应用。良好规划的调度在自动化系统中带来许多好处：它们限制生产成本和浪费。然而，这个问题的NP难度使得使用启发式算法至关重要，其设计困难，需要专门的知识，并且通常会产生针对特定任务的方法。本文提出了一种原始的端到端深度强化学习方法来进行调度，自动学习调度规则。我们的技术受到自然语言编码器-解码器模型在序列处理中的启发，并据我们所知，从未用于调度目的。我们将我们的方法应用和测试到作业车间问题的一些基准实例上，但该技术足够通用，可以有可能用于处理其他不同的最优作业调度任务，减少干预。结果证明...

    Job scheduling is a well-known Combinatorial Optimization problem with endless applications. Well planned schedules bring many benefits in the context of automated systems: among others, they limit production costs and waste. Nevertheless, the NP-hardness of this problem makes it essential to use heuristics whose design is difficult, requires specialized knowledge and often produces methods tailored to the specific task. This paper presents an original end-to-end Deep Reinforcement Learning approach to scheduling that automatically learns dispatching rules. Our technique is inspired by natural language encoder-decoder models for sequence processing and has never been used, to the best of our knowledge, for scheduling purposes. We applied and tested our method in particular to some benchmark instances of Job Shop Problem, but this technique is general enough to be potentially used to tackle other different optimal job scheduling tasks with minimal intervention. Results demonstrate that 
    
[^14]: MAP: 一个模型无关的预训练框架用于点击率预测

    MAP: A Model-agnostic Pretraining Framework for Click-through Rate Prediction. (arXiv:2308.01737v1 [cs.IR])

    [http://arxiv.org/abs/2308.01737](http://arxiv.org/abs/2308.01737)

    提出了一个模型无关的预训练框架，用于点击率预测，可以更好地利用多字段分类数据和大量用户点击日志，学习更广义和有效的特征和实例表示。

    

    随着个性化在线服务的广泛应用，点击率（CTR）预测越来越受到关注和研究。CTR预测的最突出特点是其多字段分类数据格式和庞大而日益增长的数据量。神经模型的大容量有助于在监督学习范式下消化如此大量的数据，但是它们未能充分利用大量数据的潜力，因为1比特的点击信号不足以指导模型学习功能强大的特征和实例表示。自我监督学习范式提供了更有前景的预训练-微调解决方案，以更好地利用大量用户点击日志并学习更广义和有效的表示。然而，对于CTR预测的自我监督学习仍然是一个开放的问题，因为当前在这方面的工作仅仅是初步和基础的。为此，我们提出了一个模型无关的预训练框架。

    With the widespread application of personalized online services, click-through rate (CTR) prediction has received more and more attention and research. The most prominent features of CTR prediction are its multi-field categorical data format, and vast and daily-growing data volume. The large capacity of neural models helps digest such massive amounts of data under the supervised learning paradigm, yet they fail to utilize the substantial data to its full potential, since the 1-bit click signal is not sufficient to guide the model to learn capable representations of features and instances. The self-supervised learning paradigm provides a more promising pretrain-finetune solution to better exploit the large amount of user click logs, and learn more generalized and effective representations. However, self-supervised learning for CTR prediction is still an open question, since current works on this line are only preliminary and rudimentary. To this end, we propose a Model-agnostic pretrain
    
[^15]: 近十年来我们部门关于自组织个人知识助手在不断演化的公司记忆中的研究的回顾性概述

    Towards Self-organizing Personal Knowledge Assistants in Evolving Corporate Memories. (arXiv:2308.01732v1 [cs.AI])

    [http://arxiv.org/abs/2308.01732](http://arxiv.org/abs/2308.01732)

    近十年来，我们的研究关注于自组织个人知识助手在不断演化的公司记忆中的应用。我们通过实验和结果总结了知识图构建、个人信息管理和知识工作支持等领域的贡献，并提供了相关工业使用案例。这些贡献是新发展的第一步。

    

    本文回顾性地概述了我们部门近十年来关于自组织个人知识助手在不断演化的公司记忆中的研究。我们的研究通常受到现实世界问题的启发，并常常与研究和工业合作伙伴进行跨学科合作。我们总结了过去的实验和结果，包括企业和个人环境中各种知识图构建的方式，Managed Forgetting和(Self-organizing) Context Spaces作为个人信息管理(PIM)和知识工作支持的新方法。过去的结果结合了相关工作的概述和我们迄今未发表的一些最新发现。最后，我们概述了我们相关的工业使用案例，包括对CoMem的详细介绍，它是基于我们提出的研究的公司记忆，已经投入使用并为进一步的研究提供了挑战。许多贡献仅仅是新发展的第一步。

    This paper presents a retrospective overview of a decade of research in our department towards self-organizing personal knowledge assistants in evolving corporate memories. Our research is typically inspired by real-world problems and often conducted in interdisciplinary collaborations with research and industry partners. We summarize past experiments and results comprising topics like various ways of knowledge graph construction in corporate and personal settings, Managed Forgetting and (Self-organizing) Context Spaces as a novel approach to Personal Information Management (PIM) and knowledge work support. Past results are complemented by an overview of related work and some of our latest findings not published so far. Last, we give an overview of our related industry use cases including a detailed look into CoMem, a Corporate Memory based on our presented research already in productive use and providing challenges for further research. Many contributions are only first steps in new d
    
[^16]: 针对复杂结构化医学任务的本地大型语言模型

    Local Large Language Models for Complex Structured Medical Tasks. (arXiv:2308.01727v1 [cs.CL])

    [http://arxiv.org/abs/2308.01727](http://arxiv.org/abs/2308.01727)

    本文介绍了一种利用本地大型语言模型 (LLMs) 与本地训练相结合的方法，用于处理复杂的结构化医学任务。作者提出的方法通过从病理报告中提取疾病代码来展示其效果，结果表明利用LLaMA模型相较于BERT风格的模型，在各种评估指标上具有明显的优势。而且，LLaMA模型在处理大型数据集方面表现特别出色。

    

    本文介绍了一种将大型语言模型 (LLMs) 的语言推理能力与本地训练的优势相结合，以应对复杂的领域特定任务的方法。具体而言，作者通过从病理报告中提取结构化疾病代码来展示他们的方法。提出的方法利用本地LLMs，可以进行针对特定生成指令的微调，并提供结构化输出。作者收集了一个包含超过15万个未编辑的外科病理报告的数据集，其中包含了外观描述、最终诊断和疾病代码。他们训练了不同的模型架构，包括LLaMA、BERT和LongFormer，并评估了它们的性能。结果显示，基于LLaMA的模型在所有评估指标上明显优于BERT风格的模型，即使精确性大幅降低。LLaMA模型在大型数据集上表现特别出色，展示了它们处理复杂的多任务问题的能力。

    This paper introduces an approach that combines the language reasoning capabilities of large language models (LLMs) with the benefits of local training to tackle complex, domain-specific tasks. Specifically, the authors demonstrate their approach by extracting structured condition codes from pathology reports. The proposed approach utilizes local LLMs, which can be fine-tuned to respond to specific generative instructions and provide structured outputs. The authors collected a dataset of over 150k uncurated surgical pathology reports, containing gross descriptions, final diagnoses, and condition codes. They trained different model architectures, including LLaMA, BERT and LongFormer and evaluated their performance. The results show that the LLaMA-based models significantly outperform BERT-style models across all evaluated metrics, even with extremely reduced precision. The LLaMA models performed especially well with large datasets, demonstrating their ability to handle complex, multi-la
    
[^17]: 蜜蜂局部相位量化特征选择对RGB-D面部表情识别的影响

    Bees Local Phase Quantization Feature Selection for RGB-D Facial Expressions Recognition. (arXiv:2308.01700v1 [cs.CV])

    [http://arxiv.org/abs/2308.01700](http://arxiv.org/abs/2308.01700)

    该论文研究了蜜蜂局部相位量化特征选择对RGB-D面部表情识别的影响，并与其他特征选择算法进行了比较。

    

    特征选择可以定义为一个优化问题，并通过生物启发的算法来解决。蜜蜂算法在特征选择优化任务中表现出良好的性能。另一方面，局部相位量化是一种在深度图像上具有优秀性能的频域特征。在该研究中，通过从伊朗Kinect面部数据库（IKFDB）中提取RGB（彩色）和深度图像的局部相位量化特征，蜜蜂特征选择算法应用于选择最终分类任务所需的特征数量。IKFDB使用Kinect传感器V.2进行记录，包括面部和面部微表情识别的彩色和深度图像。在最终验证中，使用了五种面部表情：愤怒，喜悦，惊讶，厌恶和恐惧。提出的蜜蜂局部相位量化方法与粒子群优化、主成分分析、Lasso和仅局部相位量化特征在支持向量机和K最近邻分类任务中进行了比较。

    Feature selection could be defined as an optimization problem and solved by bio-inspired algorithms. Bees Algorithm (BA) shows decent performance in feature selection optimization tasks. On the other hand, Local Phase Quantization (LPQ) is a frequency domain feature which has excellent performance on Depth images. Here, after extracting LPQ features out of RGB (colour) and Depth images from the Iranian Kinect Face Database (IKFDB), the Bees feature selection algorithm applies to select the desired number of features for final classification tasks. IKFDB is recorded with Kinect sensor V.2 and contains colour and depth images for facial and facial micro-expressions recognition purposes. Here five facial expressions of Anger, Joy, Surprise, Disgust and Fear are used for final validation. The proposed Bees LPQ method is compared with Particle Swarm Optimization (PSO) LPQ, PCA LPQ, Lasso LPQ, and just LPQ features for classification tasks with Support Vector Machines (SVM), K-Nearest Neighb
    
[^18]: 基于几何一致性和语义感知对齐的LiDAR-相机全景分割

    LiDAR-Camera Panoptic Segmentation via Geometry-Consistent and Semantic-Aware Alignment. (arXiv:2308.01686v1 [cs.CV])

    [http://arxiv.org/abs/2308.01686](http://arxiv.org/abs/2308.01686)

    本文提出了LCPS，第一个LiDAR-相机全景分割网络，通过异步补偿像素对齐、语义感知区域对齐和点到体素特征传播的融合策略，显著提高了全景分割的性能。

    

    3D全景分割是一个具有挑战性的感知任务，需要同时进行语义分割和实例分割。在这个任务中，我们注意到图像可以提供丰富的纹理、颜色和辨别信息，可以为LiDAR数据提供明显的性能改进，但它们的融合仍然是一个具有挑战性的问题。为此，我们提出了LCPS，第一个LiDAR-相机全景分割网络。在我们的方法中，我们将LiDAR-相机融合分为三个阶段：1）一个异步补偿像素对齐（ACPA）模块，校正传感器之间异步问题导致的坐标错位；2）一个语义感知区域对齐（SARA）模块，将一对一的点-像素映射扩展到一对多的语义关系；3）一个点到体素特征传播（PVP）模块，整合几何和语义融合信息对整个点云进行处理。我们的融合策略相对于仅使用LiDAR的方法，性能提升了约6.9%的PQ。

    3D panoptic segmentation is a challenging perception task that requires both semantic segmentation and instance segmentation. In this task, we notice that images could provide rich texture, color, and discriminative information, which can complement LiDAR data for evident performance improvement, but their fusion remains a challenging problem. To this end, we propose LCPS, the first LiDAR-Camera Panoptic Segmentation network. In our approach, we conduct LiDAR-Camera fusion in three stages: 1) an Asynchronous Compensation Pixel Alignment (ACPA) module that calibrates the coordinate misalignment caused by asynchronous problems between sensors; 2) a Semantic-Aware Region Alignment (SARA) module that extends the one-to-one point-pixel mapping to one-to-many semantic relations; 3) a Point-to-Voxel feature Propagation (PVP) module that integrates both geometric and semantic fusion information for the entire point cloud. Our fusion strategy improves about 6.9% PQ performance over the LiDAR-on
    
[^19]: 评估图神经网络的链路预测解释

    Evaluating Link Prediction Explanations for Graph Neural Networks. (arXiv:2308.01682v1 [cs.LG])

    [http://arxiv.org/abs/2308.01682](http://arxiv.org/abs/2308.01682)

    本文评估了图神经网络的链路预测解释的质量，并提供了定量指标来衡量解释的质量。我们发现底层假设和技术细节对解释的质量有影响。

    

    图机器学习（GML）在现实世界的领域中有许多应用，比如节点/图分类和链路预测。为GML模型提供人类可理解的解释是一项具有挑战性但基础的任务，但对链路预测模型的解释验证却受到了很少的关注。在本文中，我们提供了定量指标来评估链路预测解释的质量，无论是否有基准数据。我们使用这些指标评估了图神经网络的最先进可解释性方法。我们讨论了底层假设和链路预测任务特定的技术细节，比如节点嵌入之间的距离选择，如何影响解释的质量。

    Graph Machine Learning (GML) has numerous applications, such as node/graph classification and link prediction, in real-world domains. Providing human-understandable explanations for GML models is a challenging yet fundamental task to foster their adoption, but validating explanations for link prediction models has received little attention. In this paper, we provide quantitative metrics to assess the quality of link prediction explanations, with or without ground-truth. State-of-the-art explainability methods for Graph Neural Networks are evaluated using these metrics. We discuss how underlying assumptions and technical details specific to the link prediction task, such as the choice of distance between node embeddings, can influence the quality of the explanations.
    
[^20]: NBIAS: 用于文本中偏见识别的自然语言处理框架

    NBIAS: A Natural Language Processing Framework for Bias Identification in Text. (arXiv:2308.01681v1 [cs.CL])

    [http://arxiv.org/abs/2308.01681](http://arxiv.org/abs/2308.01681)

    本论文提出了一个名为NBIAS的自然语言处理框架，旨在识别文本中的偏见。通过收集来自社交媒体、医疗保健和职位招聘等领域的多样化数据构建数据集，并应用基于Transformer的令牌分类模型来识别偏见词/短语。通过定量和定性评估方法来评估模型的效果。

    

    在文本数据中存在偏见可能导致数据使用时产生倾斜的解释和结果。这些偏见可能会持续强化刻板印象、歧视或其他形式的不公平待遇。在有偏见的数据上训练的算法最终会做出不平等影响某个群体的决策。因此，检测和消除这些偏见至关重要，以确保对数据的公平和道德使用。为此，我们开发了一个全面而强大的框架"NBIAS"，它包括数据层、语料库构建、模型开发层和评估层。数据集由从各个领域收集的多样化数据构建，包括社交媒体、医疗保健和职位招聘门户网站。因此，我们应用了基于Transformer的令牌分类模型，通过一个唯一的命名实体能够识别出偏见词/短语。在评估过程中，我们结合了定量和定性评估方法来评估我们模型的效果。

    Bias in textual data can lead to skewed interpretations and outcomes when the data is used. These biases could perpetuate stereotypes, discrimination, or other forms of unfair treatment. An algorithm trained on biased data ends up making decisions that disproportionately impact a certain group of people. Therefore, it is crucial to detect and remove these biases to ensure the fair and ethical use of data. To this end, we develop a comprehensive and robust framework \textsc{Nbias} that consists of a data layer, corpus contruction, model development layer and an evaluation layer. The dataset is constructed by collecting diverse data from various fields, including social media, healthcare, and job hiring portals. As such, we applied a transformer-based token classification model that is able to identify bias words/ phrases through a unique named entity. In the assessment procedure, we incorporate a blend of quantitative and qualitative evaluations to gauge the effectiveness of our models.
    
[^21]: MARLIM: 多智能体强化学习在库存管理中的应用

    MARLIM: Multi-Agent Reinforcement Learning for Inventory Management. (arXiv:2308.01649v1 [cs.LG])

    [http://arxiv.org/abs/2308.01649](http://arxiv.org/abs/2308.01649)

    本文提出了一个名为MARLIM的新型多智能体强化学习框架，用于解决具有随机需求和交货时间的单层多产品供应链的库存管理问题，并通过数值实验证明了强化学习方法相对于传统基准方法的优势。

    

    在供应链行业中，通过优化补货决策来维持产品供需平衡是一个重要的挑战。本文提出了一种名为MARLIM的新型强化学习框架，用于解决具有随机需求和交货时间的单层多产品供应链的库存管理问题。在这个背景下，通过单个或多个智能体在合作环境中开发控制器。对真实数据进行的数值实验证明了强化学习方法相对于传统基准方法的优势。

    Maintaining a balance between the supply and demand of products by optimizing replenishment decisions is one of the most important challenges in the supply chain industry. This paper presents a novel reinforcement learning framework called MARLIM, to address the inventory management problem for a single-echelon multi-products supply chain with stochastic demands and lead-times. Within this context, controllers are developed through single or multiple agents in a cooperative setting. Numerical experiments on real data demonstrate the benefits of reinforcement learning methods over traditional baselines.
    
[^22]: 使用剩余强化学习改进级联PID控制的四旋翼飞行器的风阻性能

    Improving Wind Resistance Performance of Cascaded PID Controlled Quadcopters using Residual Reinforcement Learning. (arXiv:2308.01648v1 [cs.RO])

    [http://arxiv.org/abs/2308.01648](http://arxiv.org/abs/2308.01648)

    本文提出了一种基于剩余强化学习的方法来改善四旋翼飞行器的风阻性能。通过学习补偿扰动的剩余部分，可以在保持级联PID控制器的简单性和方便性的同时，提高四旋翼飞行器对风扰动的性能。

    

    风阻控制是四旋翼飞行器的一个重要特性，用于维持其位置，避免偏离目标位置并防止与障碍物碰撞。传统上，级联PID控制器被用于四旋翼飞行器的控制，因为它简单易用，可以调节其参数。然而，它对风扰动表现较弱，四旋翼飞行器很容易偏离目标位置。在这项工作中，我们提出了一种基于剩余强化学习的方法来构建四旋翼飞行器的风阻控制器。通过仅学习补偿扰动的剩余部分，我们可以继续使用级联PID控制器作为四旋翼飞行器的基本控制器，同时提高其抗风扰动的性能。为了避免四旋翼飞行器意外坠毁和破坏，我们的方法不需要实际硬件进行数据收集和训练。控制器仅在模拟器上进行训练，并直接应用于目标硬件，无需额外微调。

    Wind resistance control is an essential feature for quadcopters to maintain their position to avoid deviation from target position and prevent collisions with obstacles. Conventionally, cascaded PID controller is used for the control of quadcopters for its simplicity and ease of tuning its parameters. However, it is weak against wind disturbances and the quadcopter can easily deviate from target position. In this work, we propose a residual reinforcement learning based approach to build a wind resistance controller of a quadcopter. By learning only the residual that compensates the disturbance, we can continue using the cascaded PID controller as the base controller of the quadcopter but improve its performance against wind disturbances. To avoid unexpected crashes and destructions of quadcopters, our method does not require real hardware for data collection and training. The controller is trained only on a simulator and directly applied to the target hardware without extra finetuning 
    
[^23]: 将知识图谱与生成对抗网络相互交错应用于书籍封面设计的创意支持

    Interleaving GANs with knowledge graphs to support design creativity for book covers. (arXiv:2308.01626v1 [cs.CV])

    [http://arxiv.org/abs/2308.01626](http://arxiv.org/abs/2308.01626)

    本文将生成对抗网络与知识图谱相互交错应用于书籍封面设计，通过改变输入标题获得多个选项并利用鉴别器选择最佳图像，表现出比之前方法更好的生成效果和更好的选择。

    

    书籍封面的吸引力对于一本书的成功至关重要。本文将生成对抗网络（GANs）应用于书籍封面领域，使用不同的训练方法以获得更好的生成图像。我们将知识图谱与GANs相互交错，改变输入标题以获得给定标题的多个可能选项，然后将其作为生成器的增强输入。最后，我们利用训练阶段获得的鉴别器选择使用新标题生成的最佳图像。相比仅使用GANs，我们的方法在生成书籍封面方面表现更好，而知识图谱与书籍作者或编辑相比提供了更好的选择。

    An attractive book cover is important for the success of a book. In this paper, we apply Generative Adversarial Networks (GANs) to the book covers domain, using different methods for training in order to obtain better generated images. We interleave GANs with knowledge graphs to alter the input title to obtain multiple possible options for any given title, which are then used as an augmented input to the generator. Finally, we use the discriminator obtained during the training phase to select the best images generated with new titles. Our method performed better at generating book covers than previous attempts, and the knowledge graph gives better options to the book author or editor compared to using GANs alone.
    
[^24]: ReIDTrack: 无需运动的多目标跟踪和分割

    ReIDTrack: Multi-Object Track and Segmentation Without Motion. (arXiv:2308.01622v1 [cs.CV])

    [http://arxiv.org/abs/2308.01622](http://arxiv.org/abs/2308.01622)

    ReIDTrack提出了一种无需运动的多目标跟踪和分割方法，通过使用高性能的检测和外观模型，大幅度提升了性能，在MOTS基准中取得了第一名。

    

    近年来，主导的多目标跟踪（MOT）和分割（MOTS）方法主要遵循的是跟踪-检测范式。基于Transformer的端到端（E2E）解决方案为MOT和MOTS带来了一些想法，但它们无法在主要的MOT和MOTS基准中达到新的最高性能。检测和关联是跟踪-检测范式的两个主要模块。关联技术主要依赖于运动和外观信息的组合。随着深度学习的最近发展，检测和外观模型的性能得到了快速提升。这些趋势使我们考虑是否可以仅基于高性能的检测和外观模型实现新的最高性能。我们的论文主要关注基于CBNetV2作为检测模型、Swin-B作为检测模型、MoCo-v2作为自监督外观模型的方法。我们在MOTS基准中获得了第一名。

    In recent years, dominant Multi-object tracking (MOT) and segmentation (MOTS) methods mainly follow the tracking-by-detection paradigm. Transformer-based end-to-end (E2E) solutions bring some ideas to MOT and MOTS, but they cannot achieve a new state-of-the-art (SOTA) performance in major MOT and MOTS benchmarks. Detection and association are two main modules of the tracking-by-detection paradigm. Association techniques mainly depend on the combination of motion and appearance information. As deep learning has been recently developed, the performance of the detection and appearance model is rapidly improved. These trends made us consider whether we can achieve SOTA based on only high-performance detection and appearance model. Our paper mainly focuses on exploring this direction based on CBNetV2 with Swin-B as a detection model and MoCo-v2 as a self-supervised appearance model. Motion information and IoU mapping were removed during the association. Our method wins 1st place on the MOTS
    
[^25]: 通过反事实验证评估DNNs的系统性缺陷

    Assessing Systematic Weaknesses of DNNs using Counterfactuals. (arXiv:2308.01614v1 [cs.LG])

    [http://arxiv.org/abs/2308.01614](http://arxiv.org/abs/2308.01614)

    通过反事实验证评估DNNs的系统性缺陷。

    

    随着DNNs进入安全关键应用，对这种模型的测试方法越来越受到关注。目前的一个方向是寻找和识别系统性弱点，这些弱点使基于平均性能值的安全假设处于危险之中。这些弱点可以表现为（语义上连贯的）子集或输入空间中的区域，在这些区域中，DNN的性能比预期的平均性能要差。然而，将观察到的低性能归因于描述该子集的特定语义特征是非常困难的。例如，与其他（未考虑）属性相关的数据不均匀性可能会扭曲结果。然而，考虑所有（可用）属性及其相互作用通常计算成本很高。受反事实解释的启发，我们提出了一种有效且计算成本低的算法来验证现有子集的语义归因，即检查...

    With the advancement of DNNs into safety-critical applications, testing approaches for such models have gained more attention. A current direction is the search for and identification of systematic weaknesses that put safety assumptions based on average performance values at risk. Such weaknesses can take on the form of (semantically coherent) subsets or areas in the input space where a DNN performs systematically worse than its expected average. However, it is non-trivial to attribute the reason for such observed low performances to the specific semantic features that describe the subset. For instance, inhomogeneities within the data w.r.t. other (non-considered) attributes might distort results. However, taking into account all (available) attributes and their interaction is often computationally highly expensive. Inspired by counterfactual explanations, we propose an effective and computationally cheap algorithm to validate the semantic attribution of existing subsets, i.e., to chec
    
[^26]: DOLCE：用于语言和认知工程的描述性本体论

    DOLCE: A Descriptive Ontology for Linguistic and Cognitive Engineering. (arXiv:2308.01597v1 [cs.AI])

    [http://arxiv.org/abs/2308.01597](http://arxiv.org/abs/2308.01597)

    DOLCE是一种描述性本体论，具有广泛的应用领域。它基于认知和语言考虑，旨在建模人类在不同领域中的常识观点。DOLCE不直接涉及特定领域的知识，而是提供了一般的类别和关系，用于指导特定领域的知识建模。

    

    DOLCE是第一个被公理化的顶层（基础性）本体论，在过去的20年中保持稳定，如今被广泛应用于各个领域。DOLCE受认知和语言考虑的启发，旨在建模人类在日常生活中利用的常识观点，涵盖领域包括社会技术系统、制造业、金融交易和文化遗产等各个方面。DOLCE清晰列出了它所基于的本体论选择，依赖于哲学原则，具有丰富的形式化，并根据良好建立的本体论方法（例如OntoClean）进行构建。由于这些特性，它是大多数现有顶层本体论的灵感来源，并已被用于开发或改进标准和公共领域资源（例如CIDOC CRM、DBpedia和WordNet）。作为基础性本体论，DOLCE并不直接涉及领域知识，其目的是提供提供一般的类别和关系，以提供给特定领域的具体知识建模的基础。

    DOLCE, the first top-level (foundational) ontology to be axiomatized, has remained stable for twenty years and today is broadly used in a variety of domains. DOLCE is inspired by cognitive and linguistic considerations and aims to model a commonsense view of reality, like the one human beings exploit in everyday life in areas as diverse as socio-technical systems, manufacturing, financial transactions and cultural heritage. DOLCE clearly lists the ontological choices it is based upon, relies on philosophical principles, is richly formalized, and is built according to well-established ontological methodologies, e.g. OntoClean. Because of these features, it has inspired most of the existing top-level ontologies and has been used to develop or improve standards and public domain resources (e.g. CIDOC CRM, DBpedia and WordNet). Being a foundational ontology, DOLCE is not directly concerned with domain knowledge. Its purpose is to provide the general categories and relations needed to give 
    
[^27]: 圣杯2.0：从自然语言到约束模型

    Holy Grail 2.0: From Natural Language to Constraint Models. (arXiv:2308.01589v1 [cs.AI])

    [http://arxiv.org/abs/2308.01589](http://arxiv.org/abs/2308.01589)

    本文通过利用预训练的大型语言模型从文本问题描述中提取模型的方法，探索了一种使约束编程更易于采用的方法。

    

    27年前，E. Freuder强调了“约束编程代表了计算机科学对于编程的一个最接近圣杯的方法：用户陈述问题，计算机解决问题”。如今，CP用户拥有强大的建模工具（如Minizinc和CPMpy），可以用它们来表达问题，然后让求解器完成剩下的工作，离目标更近了。然而，这仍然要求CP用户了解形式化方法并遵守它。另一个重要的挑战在于需要专业知识才能有效地对组合问题建模。所有这些限制了CP的广泛应用。在这篇立场论文中，我们探讨了一种利用预训练的大型语言模型从文本问题描述中提取模型的可能方法。具体来说，我们从自然语言处理优化（NL4OPT）挑战中获得灵感，并展示了一个基于分解的提示应用的初步结果。

    Twenty-seven years ago, E. Freuder highlighted that "Constraint programming represents one of the closest approaches computer science has yet made to the Holy Grail of programming: the user states the problem, the computer solves it". Nowadays, CP users have great modeling tools available (like Minizinc and CPMpy), allowing them to formulate the problem and then let a solver do the rest of the job, getting closer to the stated goal. However, this still requires the CP user to know the formalism and respect it. Another significant challenge lies in the expertise required to effectively model combinatorial problems. All this limits the wider adoption of CP. In this position paper, we investigate a possible approach to leverage pre-trained Large Language Models to extract models from textual problem descriptions. More specifically, we take inspiration from the Natural Language Processing for Optimization (NL4OPT) challenge and present early results with a decomposition-based prompting app
    
[^28]: 无监督表示学习对时间序列的研究综述

    Unsupervised Representation Learning for Time Series: A Review. (arXiv:2308.01578v1 [cs.LG])

    [http://arxiv.org/abs/2308.01578](http://arxiv.org/abs/2308.01578)

    本文对时间序列领域的无监督表示学习方法进行了综述，提出了基于ULTS库的快速实现与统一评估，并为未来的研究提供了参考。

    

    无监督表示学习方法旨在从无标签数据中学习有区分性的特征表示，而无需对每个样本进行注释。对于时间序列数据来说，实现无监督表示学习非常关键，因为其复杂特性和缺乏与其他数据形态相比的视觉提示导致了标注困境。近年来，无监督表示学习技术在各个领域取得了快速发展。然而，对于时间序列的无监督表示学习方法缺乏系统性的分析。为了填补这个空白，我们进行了对时间序列领域中现有快速发展的无监督表示学习方法的全面文献综述。此外，我们还开发了一个统一和标准化的库，名为ULTS（即无监督时间序列学习），以便对各种模型进行快速实现和统一评估。通过ULTS，我们经验性地评估了无监督表示学习方法在不同任务上的性能，并为未来的研究提供了参考。

    Unsupervised representation learning approaches aim to learn discriminative feature representations from unlabeled data, without the requirement of annotating every sample. Enabling unsupervised representation learning is extremely crucial for time series data, due to its unique annotation bottleneck caused by its complex characteristics and lack of visual cues compared with other data modalities. In recent years, unsupervised representation learning techniques have advanced rapidly in various domains. However, there is a lack of systematic analysis of unsupervised representation learning approaches for time series. To fill the gap, we conduct a comprehensive literature review of existing rapidly evolving unsupervised representation learning approaches for time series. Moreover, we also develop a unified and standardized library, named ULTS (i.e., Unsupervised Learning for Time Series), to facilitate fast implementations and unified evaluations on various models. With ULTS, we empirica
    
[^29]: 运动规划扩散：基于扩散模型的机器人运动学习与规划

    Motion Planning Diffusion: Learning and Planning of Robot Motions with Diffusion Models. (arXiv:2308.01557v1 [cs.RO])

    [http://arxiv.org/abs/2308.01557](http://arxiv.org/abs/2308.01557)

    本文提出了一种新的机器人运动学习与规划方法，通过学习扩散模型作为先验知识，可以加速运动规划优化过程。扩散模型能够在高维环境中有效地编码数据的多模态性，并可以直接从任务目标条件下的后验轨迹分布中进行采样。

    

    学习轨迹分布的先验知识可以加快机器人运动规划的优化。在给定先前成功的规划方案的情况下，学习轨迹生成模型作为新规划问题的先验知识是非常理想的。之前的研究提出了几种利用这种先验知识进行运动规划问题引导的方法。可以通过从先验知识中采样初始化，或者在最大后验优化的过程中使用先验分布。在本文中，我们提出了学习扩散模型作为先验知识的方法。然后，我们可以通过利用扩散模型的逆去噪过程，直接从任务目标条件下的后验轨迹分布中采样。此外，最近的研究表明，扩散在高维环境中可以有效地编码数据的多模态性，这对于大量的轨迹数据集非常适用。为了展示我们的方法的有效性，我们将我们提出的方法-运动规划扩散与几种基准方发进行了比较。

    Learning priors on trajectory distributions can help accelerate robot motion planning optimization. Given previously successful plans, learning trajectory generative models as priors for a new planning problem is highly desirable. Prior works propose several ways on utilizing this prior to bootstrapping the motion planning problem. Either sampling the prior for initializations or using the prior distribution in a maximum-a-posterior formulation for trajectory optimization. In this work, we propose learning diffusion models as priors. We then can sample directly from the posterior trajectory distribution conditioned on task goals, by leveraging the inverse denoising process of diffusion models. Furthermore, diffusion has been recently shown to effectively encode data multimodality in high-dimensional settings, which is particularly well-suited for large trajectory dataset. To demonstrate our method efficacy, we compare our proposed method - Motion Planning Diffusion - against several ba
    
[^30]: 基于高斯贝叶斯网络的轨道交通全球运输能力风险预测方法

    A Global Transport Capacity Risk Prediction Method for Rail Transit Based on Gaussian Bayesian Network. (arXiv:2308.01556v1 [cs.AI])

    [http://arxiv.org/abs/2308.01556](http://arxiv.org/abs/2308.01556)

    本文提出了一种基于高斯贝叶斯网络的轨道交通全球运输能力风险预测方法，通过构建贝叶斯网络结构并利用最大似然估计方法进行参数学习，实现了对轨道交通网络的运输能力风险的预测和解释。

    

    针对轨道交通网络承载能力与乘客流量需求之间不匹配所引起的运输能力风险预测问题，本文提出了一种基于线性高斯贝叶斯网络的轨道交通网络运输能力风险可解释性预测方法。该方法通过包括轨道交通网络、列车流和乘客流的三层结构的轨道交通系统仿真模型获取预测模型的训练数据。提出了一种基于轨道交通网络拓扑结构的贝叶斯网络结构构建方法，并利用MLE（最大似然估计）方法实现了贝叶斯网络的参数学习。最后，通过仿真实例验证了所提出方法的有效性。

    Aiming at the prediction problem of transport capacity risk caused by the mismatch between the carrying capacity of rail transit network and passenger flow demand, this paper proposes an explainable prediction method of rail transit network transport capacity risk based on linear Gaussian Bayesian network. This method obtains the training data of the prediction model based on the simulation model of the rail transit system with a three-layer structure including rail transit network, train flow and passenger flow. A Bayesian network structure construction method based on the topology of the rail transit network is proposed, and the MLE (Maximum Likelihood Estimation) method is used to realize the parameter learning of the Bayesian network. Finally, the effectiveness of the proposed method is verified by simulation examples.
    
[^31]: InterAct: 探索将ChatGPT作为合作代理人的潜力

    InterAct: Exploring the Potentials of ChatGPT as a Cooperative Agent. (arXiv:2308.01552v1 [cs.AI])

    [http://arxiv.org/abs/2308.01552](http://arxiv.org/abs/2308.01552)

    本研究将OpenAI的ChatGPT集成到具身智能体系统中，通过将ChatGPT分配不同角色并与原始语言模型集成，实现了98%的成功率，并在实际环境中展现了ChatGPT在理解和执行复杂任务方面的能力。

    

    本研究论文探讨了将OpenAI的ChatGPT集成到具身智能体系统中，评估其对交互决策基准的影响。我们引入InterAct这一概念，将其类比于人们根据自己独特的优势扮演角色的概念。在这种方法中，我们给ChatGPT提供各种提示，将其分配为像检查员和分类员这样的多种角色，然后将它们与原始语言模型集成。我们的研究在AlfWorld中展示了98%的显著成功率，AlfWorld是一个模拟家庭环境中包含6个不同任务的基准测试，强调了良好的提示工程的重要性。结果强调了ChatGPT在理解和高效地执行复杂任务方面的能力，为任务规划的进一步发展铺平了道路。

    This research paper delves into the integration of OpenAI's ChatGPT into embodied agent systems, evaluating its influence on interactive decision-making benchmark. Drawing a parallel to the concept of people assuming roles according to their unique strengths, we introduce InterAct. In this approach, we feed ChatGPT with varied prompts, assigning it a numerous roles like a checker and a sorter, then integrating them with the original language model. Our research shows a remarkable success rate of 98% in AlfWorld, which consists of 6 different tasks in a simulated household environment, emphasizing the significance of proficient prompt engineering. The results highlight ChatGPT's competence in comprehending and performing intricate tasks effectively in real-world settings, thus paving the way for further advancements in task planning.
    
[^32]: 基于离线预训练强化学习的避障导航

    Avoidance Navigation Based on Offline Pre-Training Reinforcement Learning. (arXiv:2308.01551v1 [cs.RO])

    [http://arxiv.org/abs/2308.01551](http://arxiv.org/abs/2308.01551)

    本文提出了一种基于离线预训练强化学习的避障导航方法，通过高效的离线训练策略和收集专家经验的通用数据集，可以减少训练时间并提高强化学习奖励。通过先进的仿真和真实物理建模，缩小了仿真与真实之间的差距。在不同环境中，训练好的模型都能取得相同的效果。

    

    本文提出了一种针对移动机器人的避障导航的预训练深度强化学习方法，该方法将原始传感器数据映射到控制变量，并在未知环境中进行导航。我们提出了高效的离线训练策略，以加速早期阶段的随机探索，并收集了用于离线训练的包含专家经验的通用数据集，这对于其他导航训练工作具有一定的意义。预训练和优先的专家经验被提出来减少80％的训练时间，并且已经验证可以提高强化学习的奖励2倍。通过先进的仿真Gazebo和真实物理建模以及动态方程，缩小了仿真与真实之间的差距。我们在走廊环境中训练了我们的模型，并在不同环境中评估了模型，得到了相同的效果。与传统的导航方法相比，我们可以确认训练好的模型可以直接应用于不同情境，并取得相同的效果。

    This paper presents a Pre-Training Deep Reinforcement Learning(DRL) for avoidance navigation without map for mobile robots which map raw sensor data to control variable and navigate in an unknown environment. The efficient offline training strategy is proposed to speed up the inefficient random explorations in early stage and we also collect a universal dataset including expert experience for offline training, which is of some significance for other navigation training work. The pre-training and prioritized expert experience are proposed to reduce 80\% training time and has been verified to improve the 2 times reward of DRL. The advanced simulation gazebo with real physical modelling and dynamic equations reduce the gap between sim-to-real. We train our model a corridor environment, and evaluate the model in different environment getting the same effect. Compared to traditional method navigation, we can confirm the trained model can be directly applied into different scenarios and have
    
[^33]: MusicLDM：使用节奏同步的混合策略增强文本转音乐生成中的新颖性

    MusicLDM: Enhancing Novelty in Text-to-Music Generation Using Beat-Synchronous Mixup Strategies. (arXiv:2308.01546v1 [cs.SD])

    [http://arxiv.org/abs/2308.01546](http://arxiv.org/abs/2308.01546)

    MusicLDM通过稳定扩散和AudioLDM架构，结合重新训练对比语言-音频预训练模型和Hifi-GAN声码器，以解决音乐生成中的挑战。通过节奏同步的混合策略，对训练数据进行增广，提高新颖性并避免抄袭问题。

    

    扩散模型在跨模态生成任务中展现了令人期待的结果，包括文本到图像和文本到音频的生成。然而，由于音乐数据的有限可用性以及与版权和抄袭相关的敏感问题，生成音乐作为一种特殊类型的音频，面临着独特的挑战。在本文中，为了解决这些挑战，我们首先构建了一种最先进的文本到音乐模型MusicLDM，将稳定扩散和AudioLDM架构适应到音乐领域。我们通过对一系列音乐数据样本进行重新训练对比语言-音频预训练模型(CLAP)和Hifi-GAN声码器这些MusicLDM的组成部分进行了实现。然后，为了解决训练数据的限制并避免抄袭，我们利用节拍追踪模型并提出了两种不同的数据增广混合策略：节奏同步音频混合和节奏同步潜在混合，分别通过直接重新组合训练音频或通过潜在嵌入空间重新组合。

    Diffusion models have shown promising results in cross-modal generation tasks, including text-to-image and text-to-audio generation. However, generating music, as a special type of audio, presents unique challenges due to limited availability of music data and sensitive issues related to copyright and plagiarism. In this paper, to tackle these challenges, we first construct a state-of-the-art text-to-music model, MusicLDM, that adapts Stable Diffusion and AudioLDM architectures to the music domain. We achieve this by retraining the contrastive language-audio pretraining model (CLAP) and the Hifi-GAN vocoder, as components of MusicLDM, on a collection of music data samples. Then, to address the limitations of training data and to avoid plagiarism, we leverage a beat tracking model and propose two different mixup strategies for data augmentation: beat-synchronous audio mixup and beat-synchronous latent mixup, which recombine training audio directly or via a latent embeddings space, respe
    
[^34]: Lode Enhancer: 通过扩展促进关卡的协同创作

    Lode Enhancer: Level Co-creation Through Scaling. (arXiv:2308.01543v1 [cs.LG])

    [http://arxiv.org/abs/2308.01543](http://arxiv.org/abs/2308.01543)

    本文探索了使用AI增强的上采样作为设计辅助工具，在2D游戏关卡设计中实现协同创作。我们使用深度神经网络来将人工降低分辨率的关卡片段上采样，并为此引入了一种能够学习上采样并处理不太常见图块的神经网络架构。经过与设计师的合作研究，我们发现设计师们喜欢这个工具的共同设计过程，并认为它具有潜力推动更多的开发工作。

    

    我们探索了在创建2D游戏关卡时，使用AI增强的上采样作为设计辅助工具。我们使用深度神经网络将来自谜题平台游戏Lode Runner的人工降低分辨率的关卡片段进行上采样。训练得到的网络被整合到一个基于Web的编辑器中，用户可以以3个不同的分辨率（4x4、8x8和16x16）创建和编辑关卡。在任何分辨率上的编辑都会立即传输到其他分辨率上。由于上采样需要发明在较低分辨率下可能不存在的特征，我们训练神经网络来复制这些特征。我们引入了一种神经网络架构，它不仅能够学习上采样，还能够更高优先级地处理不太常见的图块。为了调查这个工具的潜力并指导进一步的开发，我们进行了一个定性研究，与3位设计师协作，了解他们如何使用它。设计师享受与这个工具的共同设计过程，喜欢它的基本概念，并提供了反馈。

    We explore AI-powered upscaling as a design assistance tool in the context of creating 2D game levels. Deep neural networks are used to upscale artificially downscaled patches of levels from the puzzle platformer game Lode Runner. The trained networks are incorporated into a web-based editor, where the user can create and edit levels at three different levels of resolution: 4x4, 8x8, and 16x16. An edit at any resolution instantly transfers to the other resolutions. As upscaling requires inventing features that might not be present at lower resolutions, we train neural networks to reproduce these features. We introduce a neural network architecture that is capable of not only learning upscaling but also giving higher priority to less frequent tiles. To investigate the potential of this tool and guide further development, we conduct a qualitative study with 3 designers to understand how they use it. Designers enjoyed co-designing with the tool, liked its underlying concept, and provided 
    
[^35]: 非平衡物理学：从自旋玻璃到机器和神经学习

    Non-equilibrium physics: from spin glasses to machine and neural learning. (arXiv:2308.01538v1 [cond-mat.dis-nn])

    [http://arxiv.org/abs/2308.01538](http://arxiv.org/abs/2308.01538)

    本论文研究了无序系统中的新兴智能行为，并通过统计物理学探索学习机制和物理动力学之间的关系，以此为指导原则设计智能系统。

    

    无序多体系统在不同尺度上表现出了各种各样的新兴现象。这些复杂行为可以用于错误修正、学习和优化等各种信息处理任务。尽管利用这些系统进行智能任务的经验成果显著，但其出现的智能行为的基本原则仍然大部分未知。在本论文中，我们旨在通过统计物理学来表征无序系统中的这种新兴智能。我们根据学习机制（长期记忆 vs. 工作记忆）和学习动力学（人工 vs. 自然）这两个方面制定了我们在论文中的努力的路线图。在我们的研究过程中，我们揭示了学习机制和物理动力学之间的关系，这些关系可以作为设计智能系统的指导原则。我们希望通过对看似不相关的学习系统的新兴智能的研究，能够扩展我们当前的认识。

    Disordered many-body systems exhibit a wide range of emergent phenomena across different scales. These complex behaviors can be utilized for various information processing tasks such as error correction, learning, and optimization. Despite the empirical success of utilizing these systems for intelligent tasks, the underlying principles that govern their emergent intelligent behaviors remain largely unknown. In this thesis, we aim to characterize such emergent intelligence in disordered systems through statistical physics. We chart a roadmap for our efforts in this thesis based on two axes: learning mechanisms (long-term memory vs. working memory) and learning dynamics (artificial vs. natural). Throughout our journey, we uncover relationships between learning mechanisms and physical dynamics that could serve as guiding principles for designing intelligent systems. We hope that our investigation into the emergent intelligence of seemingly disparate learning systems can expand our current
    
[^36]: 量子多智能体强化学习用于自主移动协作

    Quantum Multi-Agent Reinforcement Learning for Autonomous Mobility Cooperation. (arXiv:2308.01519v1 [cs.MA])

    [http://arxiv.org/abs/2308.01519](http://arxiv.org/abs/2308.01519)

    本论文提出了基于演员-评论家网络概念的量子多智能体强化学习（QMARL）算法，目标是应对多智能体系统中的参数利用和收敛困难问题。QMARL在可扩展性、参数利用效率和快速收敛方面具有优势，并通过定义奖励为多个智能体在计算时间上的任务精度来实现多智能体的协作。另外，论文还提出了一种名为投影价值测量（PVM）的可扩展技术来进一步提高系统的表现。

    

    在工业4.0革命中，基于多智能体强化学习（MARL）的合作自主移动系统被广泛应用。然而，MARL算法存在着大量参数使用和收敛困难的问题。为了解决这些问题，提出了一种基于演员-评论家网络概念的量子MARL（QMARL）算法，这在可扩展性方面是有益的，以应对噪声中尺度量子（NISQ）时代的限制。此外，我们的QMARL在参数利用效率和快速收敛方面也具有益处，这是由于量子霸权的原因。值得注意的是，我们的QMARL中的奖励定义为多个智能体在计算时间上的任务精度，因此可以实现多智能体的协作。为了进一步提高，提出了一种名为投影价值测量（PVM）的可扩展技术。基于PVM，我们提出的QMARL可以通过减少动作来实现最高的奖励。

    For Industry 4.0 Revolution, cooperative autonomous mobility systems are widely used based on multi-agent reinforcement learning (MARL). However, the MARL-based algorithms suffer from huge parameter utilization and convergence difficulties with many agents. To tackle these problems, a quantum MARL (QMARL) algorithm based on the concept of actor-critic network is proposed, which is beneficial in terms of scalability, to deal with the limitations in the noisy intermediate-scale quantum (NISQ) era. Additionally, our QMARL is also beneficial in terms of efficient parameter utilization and fast convergence due to quantum supremacy. Note that the reward in our QMARL is defined as task precision over computation time in multiple agents, thus, multi-agent cooperation can be realized. For further improvement, an additional technique for scalability is proposed, which is called projection value measure (PVM). Based on PVM, our proposed QMARL can achieve the highest reward, by reducing the action
    
[^37]: 自动驾驶中的隐式占用流场应用于感知和预测

    Implicit Occupancy Flow Fields for Perception and Prediction in Self-Driving. (arXiv:2308.01471v1 [cs.CV])

    [http://arxiv.org/abs/2308.01471](http://arxiv.org/abs/2308.01471)

    该论文提出了一种统一的感知和未来预测方法，利用隐式占用流场来表示自动驾驶车辆对周围环境的感知并预测其他交通参与者的行为。这种方法避免了不必要的计算和信息丢失的问题。

    

    自动驾驶车辆必须能够感知周围环境并预测其他交通参与者的未来行为。现有的方法要么进行目标检测，然后对检测到的目标进行轨迹预测，要么预测整个场景的密集占用和流格。前者由于效率原因需要保持检测数量较少，从而牺牲了对象的回忆率，存在安全问题。后者由于输出格的高维性和完全卷积网络固有的有限感受野而计算成本高。此外，这两种方法都使用了大量的计算资源来预测可能永远不会被运动规划器查询的区域或对象。鉴于这一点，我们提出了一种统一的感知和未来预测方法，利用单个神经网络隐式表示随时间变化的占用和流动。我们的方法避免了不必要的计算，因为它可以直接被运动规划器查询。

    A self-driving vehicle (SDV) must be able to perceive its surroundings and predict the future behavior of other traffic participants. Existing works either perform object detection followed by trajectory forecasting of the detected objects, or predict dense occupancy and flow grids for the whole scene. The former poses a safety concern as the number of detections needs to be kept low for efficiency reasons, sacrificing object recall. The latter is computationally expensive due to the high-dimensionality of the output grid, and suffers from the limited receptive field inherent to fully convolutional networks. Furthermore, both approaches employ many computational resources predicting areas or objects that might never be queried by the motion planner. This motivates our unified approach to perception and future prediction that implicitly represents occupancy and flow over time with a single neural network. Our method avoids unnecessary computation, as it can be directly queried by the mo
    
[^38]: VertexSerum: 针对链路推理的图神经网络毒化攻击

    VertexSerum: Poisoning Graph Neural Networks for Link Inference. (arXiv:2308.01469v1 [cs.LG])

    [http://arxiv.org/abs/2308.01469](http://arxiv.org/abs/2308.01469)

    VertexSerum是一种针对链路推理的图神经网络毒化攻击，通过放大链接连接性泄漏来增加图链接窃取的效果，并提出了一种可以嵌入到链接检测网络中的注意力机制。在实验中，VertexSerum在四个真实世界数据集和三种不同的GNN结构上平均提高了9.8％的AUC分数，且在黑盒和在线学习环境中均表现出有效性。

    

    图神经网络（GNNs）在利用图结构数据的各种应用中（如社交分析和欺诈检测）取得了出色的性能。图的链接（例如社交关系和交易历史）是敏感和有价值的信息，使用GNNs时会引起隐私问题。为了利用这些漏洞，我们提出了VertexSerum，一种新颖的图毒化攻击，通过放大链接连接性泄漏来增加图链接窃取的效果。为了更准确地推断节点邻接关系，我们提出了一种可以嵌入到链接检测网络中的注意力机制。我们的实验结果表明，在四个真实世界数据集和三种不同的GNN结构上，VertexSerum的AUC分数平均提高了9.8％，显著优于SOTA链路推理攻击方法。此外，我们的实验结果还验证了VertexSerum在黑盒和在线学习环境中的有效性。

    Graph neural networks (GNNs) have brought superb performance to various applications utilizing graph structural data, such as social analysis and fraud detection. The graph links, e.g., social relationships and transaction history, are sensitive and valuable information, which raises privacy concerns when using GNNs. To exploit these vulnerabilities, we propose VertexSerum, a novel graph poisoning attack that increases the effectiveness of graph link stealing by amplifying the link connectivity leakage. To infer node adjacency more accurately, we propose an attention mechanism that can be embedded into the link detection network. Our experiments demonstrate that VertexSerum significantly outperforms the SOTA link inference attack, improving the AUC scores by an average of $9.8\%$ across four real-world datasets and three different GNN structures. Furthermore, our experiments reveal the effectiveness of VertexSerum in both black-box and online learning settings, further validating its a
    
[^39]: 室内空气质量近似的新颖基于物理的机器学习模型

    Novel Physics-Based Machine-Learning Models for Indoor Air Quality Approximations. (arXiv:2308.01438v1 [cs.LG])

    [http://arxiv.org/abs/2308.01438](http://arxiv.org/abs/2308.01438)

    本研究提出了六种新颖的基于物理学的机器学习模型，用于准确近似室内污染物浓度。所提出的模型结合了物理学中的状态空间概念、门控循环单元和分解技术。通过在加利福尼亚州一栋商业建筑中五个办公室收集的数据进行验证，结果表明所提出的模型具有较低的复杂度。

    

    成本低廉的传感器能够实时捕捉到不同污染物浓度、室内/室外湿度和温度等与空气质量相关的多种模态。机器学习模型能够提前近似室内空气质量。毫无疑问，准确的室内空气质量近似有助于提供健康的室内环境，优化相关能耗，并提供人体舒适度。然而，设计一个能够捕捉所谓的问题物理学领域知识的机器学习架构至关重要。在本研究中，我们提出了六种新颖的基于物理学的机器学习模型，用于准确近似室内污染物浓度。所提出的模型包括物理学中的状态空间概念、门控循环单元和分解技术的巧妙组合。所提出的模型使用从加利福尼亚州一栋商业建筑中五个办公室收集的数据进行了验证。结果显示，所提出的模型较为简单。

    Cost-effective sensors are capable of real-time capturing a variety of air quality-related modalities from different pollutant concentrations to indoor/outdoor humidity and temperature. Machine learning (ML) models are capable of performing air-quality "ahead-of-time" approximations. Undoubtedly, accurate indoor air quality approximation significantly helps provide a healthy indoor environment, optimize associated energy consumption, and offer human comfort. However, it is crucial to design an ML architecture to capture the domain knowledge, so-called problem physics. In this study, we propose six novel physics-based ML models for accurate indoor pollutant concentration approximations. The proposed models include an adroit combination of state-space concepts in physics, Gated Recurrent Units, and Decomposition techniques. The proposed models were illustrated using data collected from five offices in a commercial building in California. The proposed models are shown to be less complex, 
    
[^40]: ChatMOF: 一种自主人工智能系统用于预测和生成金属-有机骨架

    ChatMOF: An Autonomous AI System for Predicting and Generating Metal-Organic Frameworks. (arXiv:2308.01423v1 [cs.CL])

    [http://arxiv.org/abs/2308.01423](http://arxiv.org/abs/2308.01423)

    ChatMOF是一种自主AI系统，用于预测和生成金属-有机骨架。通过利用大规模语言模型，它能够从文本输入中提取关键细节，并提供适当的回应。该系统通过组合代理、工具包和评估器的核心组件，实现了数据检索、性质预测和结构生成等多个任务。研究进一步展示了在材料科学中使用大型语言模型的优势和潜力。

    

    ChatMOF是一个自主人工智能系统，用于预测和生成金属-有机骨架（MOFs）。通过利用大规模语言模型（gpt-3.5-turbo），ChatMOF从文本输入中提取关键细节并提供适当的回应，从而消除了对刚性结构化查询的需求。该系统由三个核心组件（即代理、工具包和评估器）组成，形成一个强大的流水线，管理多种任务，包括数据检索、性质预测和结构生成。该研究进一步探讨了在材料科学中使用大型语言模型（LLMs）人工智能系统的优点和限制，并展示了其对未来发展的变革潜力。

    ChatMOF is an autonomous Artificial Intelligence (AI) system that is built to predict and generate of metal-organic frameworks (MOFs). By leveraging a large-scale language model (gpt-3.5-turbo), ChatMOF extracts key details from textual inputs and delivers appropriate responses, thus eliminating the necessity for rigid structured queries. The system is comprised of three core components (i.e. an agent, a toolkit, and an evaluator) and it forms a robust pipeline that manages a variety of tasks, including data retrieval, property prediction, and structure generation. The study further explores the merits and constraints of using large language models (LLMs) AI system in material sciences using and showcases its transformative potential for future advancements.
    
[^41]: 一个有效的数据创建流水线用于为大型语言模型生成高质量的金融指令数据

    An Effective Data Creation Pipeline to Generate High-quality Financial Instruction Data for Large Language Model. (arXiv:2308.01415v1 [cs.CL])

    [http://arxiv.org/abs/2308.01415](http://arxiv.org/abs/2308.01415)

    本文提出了一个精心设计的数据创建流水线，通过与金融专家的对话和反馈，在大型语言模型中生成了一个高质量的金融指令数据集。实验结果表明，该方法在生成准确、相关和金融风格响应方面取得了显著进展。

    

    在大型语言模型的初期阶段，为金融相关任务精调大型语言模型生成高质量的金融数据集非常关键。因此，本文提出了一个精心设计的数据创建流水线来实现这一目的。具体而言，我们使用ChatGPT引发了一个AI投资者和金融专家之间的对话，并融入了人工金融专家的反馈，从而改进了数据集。该流水线产生了一个由103k个多轮对话组成的稳定的指令精调数据集。通过采用外部的GPT-4作为评判者，在该数据集上进行了大量实验来评估模型的性能。有希望的实验结果验证了我们的方法在生成准确、相关和金融风格响应方面取得了显著进展，从而为金融领域的应用提供了一个强大的工具。

    At the beginning era of large language model, it is quite critical to generate a high-quality financial dataset to fine-tune a large language model for financial related tasks. Thus, this paper presents a carefully designed data creation pipeline for this purpose. Particularly, we initiate a dialogue between an AI investor and financial expert using ChatGPT and incorporate the feedback of human financial experts, leading to the refinement of the dataset. This pipeline yielded a robust instruction tuning dataset comprised of 103k multi-turn chats. Extensive experiments have been conducted on this dataset to evaluate the model's performance by adopting an external GPT-4 as the judge. The promising experimental results verify that our approach led to significant advancements in generating accurate, relevant, and financial-style responses from AI models, and thus providing a powerful tool for applications within the financial sector.
    
[^42]: HouYi: 一种专门为可再生能源和碳中和领域设计的开源大型语言模型

    HouYi: An open-source large language model specially designed for renewable energy and carbon neutrality field. (arXiv:2308.01414v1 [cs.CL])

    [http://arxiv.org/abs/2308.01414](http://arxiv.org/abs/2308.01414)

    本论文发布了第一个专门为可再生能源领域设计的开源大型语言模型HouYi及其对应的可再生能源学术论文数据集REAP。HouYi展示了在生成可再生能源领域学术论文段落方面的强大能力，与ChatGPT相当，略优于Clau。

    

    可再生能源对于实现碳中和目标至关重要。大型语言模型（LLMs）的成功应用，如ChatGPT在自动内容生成方面的成功，使得LLMs在可再生能源领域扮演越来越重要的角色。然而，目前还没有专门为可再生能源领域设计的LLM，也没有任何可再生能源的数据集用于训练LLM。因此，本论文发布了第一个面向非商业性可再生能源LLM研究的开源可再生能源学术论文（REAP）数据集。REAP数据集通过从Web of Science搜索1168970篇学术文献的标题和摘要进行收集。基于REAP数据集，通过对通用LLMs进行微调，开发了第一个针对可再生能源的LLM模型HouYi。HouYi在可再生能源领域展示了强大的学术论文段落生成能力。实验结果显示，它在生成可再生能源学术论文方面的能力与ChatGPT相当，在某些方面略优于Clau。

    Renewable energy is important for achieving carbon neutrality goal. With the great success of Large Language Models (LLMs) like ChatGPT in automatic content generation, LLMs are playing an increasingly important role. However, there has not been a specially designed LLM for renewable energy. Meanwhile, there has not been any dataset of renewable energy for training LLMs. Therefore, this paper published the first open-source Renewable Energy Academic Paper (REAP) dataset for non-commercial LLM research of renewable energy. REAP dataset is collected through searching the title and abstract of 1,168,970 academic literatures from Web of Science. Based on REAP dataset, HouYi model, the first LLM for renewable energy, is developed through finetuning general LLMs. HouYi demonstrated powerful academic paper paragraph generation ability in renewable energy field. Experiments show that its ability to generate academic papers on renewable energy is comparable to ChatGPT, slightly outperforms Clau
    
[^43]: LaFiCMIL：从相关多实例学习的角度重新思考大文件分类

    LaFiCMIL: Rethinking Large File Classification from the Perspective of Correlated Multiple Instance Learning. (arXiv:2308.01413v1 [cs.CL])

    [http://arxiv.org/abs/2308.01413](http://arxiv.org/abs/2308.01413)

    LaFiCMIL是一个新的方法，从相关多实例学习的角度解决了Transformer模型输入长度限制的问题，可以用于改进大文件分类任务。

    

    基于Transformer的模型在各种语言任务的性能上取得了革命性的突破。直观上，人们可能会期望文本分类，作为不需要像生成任务那样许多高级表示的任务，能够充分利用Transformer强大的表示能力来进行综合性的处理。然而，实际上，在多类别和多标签分类长文本文档和其他大文件的领域仍然存在较大的改进潜力。Transformer模型的性能主要受到一个重要限制的阻碍：有限的输入长度，比如BERT的512个标记。虽然增加GPU内存可以稍微扩展这个限制，但实际应用中往往受限于有限的GPU资源。在这项工作中，我们从相关多实例学习的角度解决了输入限制问题。所提出的方法LaFiCMIL，作为一个多功能的框架，适用于

    Transformer-based models have revolutionized the performance of a wide range of language tasks. Intuitively, one might expect text classification, which does not necessitate as many high-level representations as generative tasks, to be comprehensively addressed with the powerful representation capabilities of Transformers. However, in reality, there remains significant potential for enhancement, particularly in the areas of multi-class and multi-label classification of lengthy textual documents and other large files. The performance of Transformer-based models is mainly hindered by a major limitation: a restricted input length, e.g., 512 tokens for BERT. While an increase in GPU memory can marginally extend this limit, practical real-world applications often operate under constrained GPU resources. In this work, we tackle the input limit problem from the perspective of correlated multiple instance learning. The proposed approach, LaFiCMIL, serves as a versatile framework applicable to 
    
[^44]: 通过语言学习对世界建模

    Learning to Model the World with Language. (arXiv:2308.01399v1 [cs.CL])

    [http://arxiv.org/abs/2308.01399](http://arxiv.org/abs/2308.01399)

    本论文提出了一种通过语言学习对世界进行建模的方法，利用语言帮助代理器预测未来并进行行动。通过学习多模态世界模型，代理器可以预测未来的文本和图像表示，并在模型回滚中进行行动。

    

    为了与人类在世界中相互作用，代理器需要理解人们使用的多样化的语言类型，并将其与视觉世界关联起来，并基于语言行动。虽然当前的代理器可以通过任务奖励学习执行简单的语言指令，但我们的目标是建立可以利用传达一般知识、描述世界状态、提供互动反馈等多样化语言的代理器。我们的核心思想是语言帮助代理器预测未来：将会被观察到什么、世界将如何运行以及哪些情况将获得奖励。这个观点将语言理解与未来预测统一为一个强大的自监督学习目标。我们提出了Dynalang，一种学习多模态世界模型的代理器，它可以预测未来的文本和图像表示，并在想像的模型回滚中学习行动。与只使用语言预测动作的传统代理器不同，Dynalang通过过去的语言还可以获取丰富的语言理解能力。

    To interact with humans in the world, agents need to understand the diverse types of language that people use, relate them to the visual world, and act based on them. While current agents learn to execute simple language instructions from task rewards, we aim to build agents that leverage diverse language that conveys general knowledge, describes the state of the world, provides interactive feedback, and more. Our key idea is that language helps agents predict the future: what will be observed, how the world will behave, and which situations will be rewarded. This perspective unifies language understanding with future prediction as a powerful self-supervised learning objective. We present Dynalang, an agent that learns a multimodal world model that predicts future text and image representations and learns to act from imagined model rollouts. Unlike traditional agents that use language only to predict actions, Dynalang acquires rich language understanding by using past language also to 
    
[^45]: OpenFlamingo: 一个用于训练大型自回归视觉语言模型的开源框架

    OpenFlamingo: An Open-Source Framework for Training Large Autoregressive Vision-Language Models. (arXiv:2308.01390v1 [cs.CV])

    [http://arxiv.org/abs/2308.01390](http://arxiv.org/abs/2308.01390)

    OpenFlamingo是一个开源框架，用于训练大型自回归视觉语言模型。它在多个数据集上表现良好，达到了对应模型性能的80%至89%。

    

    我们介绍了OpenFlamingo，这是一系列自回归的视觉语言模型，参数范围从3B到9B。 OpenFlamingo是一个持续努力的项目，旨在复制DeepMind的Flamingo模型的开源版本。在七个视觉语言数据集上，OpenFlamingo模型的性能介于对应的Flamingo性能的80%至89%之间。本技术报告介绍了我们的模型、训练数据、超参数和评估套件。我们在https://github.com/mlfoundations/open_flamingo上分享我们的模型和代码。

    We introduce OpenFlamingo, a family of autoregressive vision-language models ranging from 3B to 9B parameters. OpenFlamingo is an ongoing effort to produce an open-source replication of DeepMind's Flamingo models. On seven vision-language datasets, OpenFlamingo models average between 80 - 89% of corresponding Flamingo performance. This technical report describes our models, training data, hyperparameters, and evaluation suite. We share our models and code at https://github.com/mlfoundations/open_flamingo.
    
[^46]: CausalOps -- 实现因果概率图模型实际生命周期的方法

    CausalOps -- Towards an Industrial Lifecycle for Causal Probabilistic Graphical Models. (arXiv:2308.01375v1 [cs.AI])

    [http://arxiv.org/abs/2308.01375](http://arxiv.org/abs/2308.01375)

    提出了CausalOps，一个新的因果模型开发和应用的生命周期框架，旨在推动在实际应用中采用因果方法。

    

    因果概率图模型已被广泛应用，可以在不同领域建模因果关系。随着其在汽车系统安全和机器学习等新领域的应用越来越普遍，人们对于类似DevOps和MLOps的集成生命周期框架的需求不断增加。目前，缺乏一个适用于因果工程的组织参考流程。为了填补这个空白并促进广泛的工业应用，我们提出了CausalOps，这是一个面向因果模型开发和应用的全新生命周期框架。通过定义因果工程过程中产生的关键实体、依赖关系和中间产物，我们建立了一个一致的词汇表和工作流模型。本研究将因果模型的使用情境化为不同阶段和利益相关者，概述了创建和维护因果模型的整体视图。CausalOps旨在推动因果方法在实际应用中的采用。

    Causal probabilistic graph-based models have gained widespread utility, enabling the modeling of cause-and-effect relationships across diverse domains. With their rising adoption in new areas, such as automotive system safety and machine learning, the need for an integrated lifecycle framework akin to DevOps and MLOps has emerged. Currently, a process reference for organizations interested in employing causal engineering is missing. To address this gap and foster widespread industrial adoption, we propose CausalOps, a novel lifecycle framework for causal model development and application. By defining key entities, dependencies, and intermediate artifacts generated during causal engineering, we establish a consistent vocabulary and workflow model. This work contextualizes causal model usage across different stages and stakeholders, outlining a holistic view of creating and maintaining them. CausalOps' aim is to drive the adoption of causal methods in practical applications within intere
    
[^47]: 一种通过集成驾驶异质性和长期轨迹预测来增强运动规划方法的自动驾驶系统

    An enhanced motion planning approach by integrating driving heterogeneity and long-term trajectory prediction for automated driving systems. (arXiv:2308.01369v1 [cs.RO])

    [http://arxiv.org/abs/2308.01369](http://arxiv.org/abs/2308.01369)

    本文提出了一种通过集成驾驶异质性和长期轨迹预测来增强运动规划方法的自动驾驶系统。该方法利用层级模型将周围人驾驶车辆的驾驶行为和长期轨迹进行耦合，以提高驾驶安全性。

    

    自动驾驶系统在复杂驾驶环境中的导航是困难的。预测周围人驾驶车辆的驾驶行为是自动驾驶系统的一个关键组成部分。本文提出了一种用于高速公路合流场景的自动驾驶系统的增强运动规划方法。该增强方法利用了两个方面的结果：周围人驾驶车辆的驾驶行为和长期轨迹，通过使用层级模型将它们耦合起来，用于自动驾驶系统的运动规划，以提高驾驶安全性。

    Navigating automated driving systems (ADSs) through complex driving environments is difficult. Predicting the driving behavior of surrounding human-driven vehicles (HDVs) is a critical component of an ADS. This paper proposes an enhanced motion-planning approach for an ADS in a highway-merging scenario. The proposed enhanced approach utilizes the results of two aspects: the driving behavior and long-term trajectory of surrounding HDVs, which are coupled using a hierarchical model that is used for the motion planning of an ADS to improve driving safety.
    
[^48]: 实证翻译过程研究：过去和可能的未来视角

    Empirical Translation Process Research: Past and Possible Future Perspectives. (arXiv:2308.01368v1 [cs.CL])

    [http://arxiv.org/abs/2308.01368](http://arxiv.org/abs/2308.01368)

    本文追踪了实证翻译过程研究的发展，提出了自由能原理和主动推理作为建模深嵌入式翻译过程的框架，为未来研究提供了激动人心的前景。

    

    在过去的四十年里，人们一直致力于开发和评估实证翻译过程研究（TPR）的模型，但一个全面的框架仍然难以捉摸。本文追踪了CRITT TPR-DB传统中实证TPR的发展，并提出了自由能原理（FEP）和主动推理（AIF）作为建模深嵌入式翻译过程的框架。它引入了量化关联理论（相关性，s-mode，i-mode）基本概念的新方法，并建立了它们与监控模型的关系，将关联性最大化定为最小化自由能的特例。FEP/AIF提供了一个数学严谨的基础，可以建模不同时间线上展开的嵌入式翻译过程的深层时间架构。这个框架为预测性TPR的未来研究开辟了令人兴奋的前景，有望丰富我们对人类翻译过程的理解，并为实践提供了宝贵的参考。

    Over the past four decades, efforts have been made to develop and evaluate models for Empirical Translation Process Research (TPR), yet a comprehensive framework remains elusive. This article traces the evolution of empirical TPR within the CRITT TPR-DB tradition and proposes the Free Energy Principle (FEP) and Active Inference (AIF) as a framework for modeling deeply embedded translation processes. It introduces novel approaches for quantifying fundamental concepts of Relevance Theory (relevance, s-mode, i-mode), and establishes their relation to the Monitor Model, framing relevance maximization as a special case of minimizing free energy. FEP/AIF provides a mathematically rigorous foundation that enables modeling of deep temporal architectures in which embedded translation processes unfold on different timelines. This framework opens up exciting prospects for future research in predictive TPR, likely to enrich our comprehension of human translation processes, and making valuable cont
    
[^49]: EmbeddingTree：嵌入式中实体特征的层次探索

    EmbeddingTree: Hierarchical Exploration of Entity Features in Embedding. (arXiv:2308.01329v1 [cs.LG])

    [http://arxiv.org/abs/2308.01329](http://arxiv.org/abs/2308.01329)

    本文提出了一种用于嵌入学习的层次探索算法EmbeddingTree和相应的可视化工具，能够解释具有语义的实体特征和嵌入向量之间的关联，并通过实验验证了其有效性。

    

    嵌入学习将离散的数据实体转换为连续的数值表示，编码了实体的特征/属性。尽管有多种嵌入学习算法报告了出色的性能，但很少有工作投入到对学习到的嵌入空间中的特征如何编码的结构解释。本文提出了EmbeddingTree，一种层次嵌入探索算法，将实体特征的语义与较难解释的嵌入向量相关联。还开发了一种基于EmbeddingTree的交互式可视化工具来探索高维嵌入。该工具可帮助用户发现数据实体的微妙特征，在嵌入训练中执行特征去噪/注入，并为未见实体生成嵌入。我们通过对工业规模的商户数据和公共的30Music听歌/播放列表数据集生成的嵌入来证明EmbeddingTree和我们的可视化工具的有效性。

    Embedding learning transforms discrete data entities into continuous numerical representations, encoding features/properties of the entities. Despite the outstanding performance reported from different embedding learning algorithms, few efforts were devoted to structurally interpreting how features are encoded in the learned embedding space. This work proposes EmbeddingTree, a hierarchical embedding exploration algorithm that relates the semantics of entity features with the less-interpretable embedding vectors. An interactive visualization tool is also developed based on EmbeddingTree to explore high-dimensional embeddings. The tool helps users discover nuance features of data entities, perform feature denoising/injecting in embedding training, and generate embeddings for unseen entities. We demonstrate the efficacy of EmbeddingTree and our visualization tool through embeddings generated for industry-scale merchant data and the public 30Music listening/playlists dataset.
    
[^50]: DeepSpeed-Chat: Easy, Fast and Affordable RLHF Training of ChatGPT-like Models at All Scales.

    DeepSpeed-Chat: Easy, Fast and Affordable RLHF Training of ChatGPT-like Models at All Scales. (arXiv:2308.01320v1 [cs.LG])

    [http://arxiv.org/abs/2308.01320](http://arxiv.org/abs/2308.01320)

    DeepSpeed-Chat是一个新颖的系统，使得ChatGPT-like模型的RLHF培训易于访问，高效且经济实惠。它具有易于使用的训练和推断体验，复制了InstructGPT的训练流程，并集成了各种训练和推断优化，提供了无与伦比的效率和可扩展性。

    

    ChatGPT-like模型在人工智能的各种应用中带来了革命，从摘要和编码到翻译，甚至超越了人类表现。然而，当前环境还缺乏一种易于访问、高效且经济实惠的端到端RLHF（强化学习与人类反馈）训练流程，特别是当训练规模达到数十亿参数时。本文介绍了DeepSpeed-Chat，这是一个新颖的系统，使RLHF培训对AI社区可用。DeepSpeed-Chat提供了三个关键能力：一个易于使用的ChatGPT-like模型的训练和推断体验，一个复制InstructGPT训练流程的DeepSpeed-RLHF流水线，以及一个集成了各种训练和推断优化的强大DeepSpeed-RLHF系统。该系统提供了无与伦比的效率和可扩展性，可以训练具有数千亿参数的模型。

    ChatGPT-like models have revolutionized various applications in artificial intelligence, from summarization and coding to translation, matching or even surpassing human performance. However, the current landscape lacks an accessible, efficient, and cost-effective end-to-end RLHF (Reinforcement Learning with Human Feedback) training pipeline for these powerful models, particularly when training at the scale of billions of parameters. This paper introduces DeepSpeed-Chat, a novel system that democratizes RLHF training, making it accessible to the AI community. DeepSpeed-Chat offers three key capabilities: an easy-to-use training and inference experience for ChatGPT-like models, a DeepSpeed-RLHF pipeline that replicates the training pipeline from InstructGPT, and a robust DeepSpeed-RLHF system that combines various optimizations for training and inference in a unified way. The system delivers unparalleled efficiency and scalability, enabling training of models with hundreds of billions of
    
[^51]: 近年来使用机器学习进行疾病诊断的最新进展: 系统性调查、比较和挑战

    Recent advancement in Disease Diagnostic using machine learning: Systematic survey of decades, comparisons, and challenges. (arXiv:2308.01319v1 [cs.LG])

    [http://arxiv.org/abs/2308.01319](http://arxiv.org/abs/2308.01319)

    本文综述了近年来计算机辅助诊断领域在疾病诊断方面使用机器学习的最新进展，探讨了机器学习算法对于疾病检测和诊断的重要性和应用，以及其在分析生物医学数据方面的优势和挑战。

    

    计算机辅助诊断(CAD)作为一个充满活力的医学成像研究领域，正在迅速发展。由于医学诊断系统的错误可能导致严重误导的医疗治疗，近年来已经付出了重要努力来改进计算机辅助诊断应用。机器学习在计算机辅助诊断中的应用至关重要。一个简单的等式可能会导致关于器官等项目的错误指示。因此，从示例中学习是模式识别的一个重要组成部分。生物医学领域中的模式识别和机器学习有望提高疾病检测和诊断的精确性，同时支持决策过程的客观性。机器学习为创建优雅且自主的算法来分析高维和多模态生物医学数据提供了一种实际方法。本综述文章探讨了用于检测疾病的机器学习算法，包括肝炎、糖尿病、肝脏疾病、登革热等。

    Computer-aided diagnosis (CAD), a vibrant medical imaging research field, is expanding quickly. Because errors in medical diagnostic systems might lead to seriously misleading medical treatments, major efforts have been made in recent years to improve computer-aided diagnostics applications. The use of machine learning in computer-aided diagnosis is crucial. A simple equation may result in a false indication of items like organs. Therefore, learning from examples is a vital component of pattern recognition. Pattern recognition and machine learning in the biomedical area promise to increase the precision of disease detection and diagnosis. They also support the decision-making process's objectivity. Machine learning provides a practical method for creating elegant and autonomous algorithms to analyze high-dimensional and multimodal bio-medical data. This review article examines machine-learning algorithms for detecting diseases, including hepatitis, diabetes, liver disease, dengue fever
    
[^52]: FusionAD: 自动驾驶预测和规划任务的多模态融合

    FusionAD: Multi-modality Fusion for Prediction and Planning Tasks of Autonomous Driving. (arXiv:2308.01006v1 [cs.CV])

    [http://arxiv.org/abs/2308.01006](http://arxiv.org/abs/2308.01006)

    FusionAD是第一个将来自相机和激光雷达的信息融合起来用于自动驾驶预测和规划任务的统一框架，在常用数据集上的实验中达到了最先进的性能。

    

    在自动驾驶感知任务中，构建一个多模态多任务神经网络以实现准确和稳健的性能已成为铁板一块的标准。然而，利用来自多个传感器的数据来联合优化预测和规划任务仍然几乎未被探索。本文提出了FusionAD，据我们所知，这是第一个将来自两个最关键传感器相机和激光雷达的信息融合起来超越感知任务的统一框架。具体来说，我们首先构建了一个基于转换器的多模态融合网络，以有效地产生基于融合的特征。然后，与基于相机的端到端方法UniAD相比，我们建立了一个融合辅助的模态感知预测和状态感知规划模块，称为FMSPnP，充分利用多模态特征的优势。我们在常用的nuScenes数据集上进行了大量实验，结果表明我们的FusionAD达到了最先进的性能，并优于基准线平均15%的性能。

    Building a multi-modality multi-task neural network toward accurate and robust performance is a de-facto standard in perception task of autonomous driving. However, leveraging such data from multiple sensors to jointly optimize the prediction and planning tasks remains largely unexplored. In this paper, we present FusionAD, to the best of our knowledge, the first unified framework that fuse the information from two most critical sensors, camera and LiDAR, goes beyond perception task. Concretely, we first build a transformer based multi-modality fusion network to effectively produce fusion based features. In constrast to camera-based end-to-end method UniAD, we then establish a fusion aided modality-aware prediction and status-aware planning modules, dubbed FMSPnP that take advantages of multi-modality features. We conduct extensive experiments on commonly used benchmark nuScenes dataset, our FusionAD achieves state-of-the-art performance and surpassing baselines on average 15% on perce
    
[^53]: 隔离和诱导：针对模型窃取攻击的鲁棒深度神经网络训练

    Isolation and Induction: Training Robust Deep Neural Networks against Model Stealing Attacks. (arXiv:2308.00958v1 [cs.CR])

    [http://arxiv.org/abs/2308.00958](http://arxiv.org/abs/2308.00958)

    这项研究提出了一种名为隔离和诱导（InI）的训练框架，用于对抗模型窃取攻击。该框架通过隔离对手的训练梯度，并直接训练一个防御模型，有效地解决了现有防御方法中推理计算开销高和准确性与防窃鲁棒性之间的不利权衡问题。

    

    尽管机器学习模型作为服务（MLaaS）广泛应用，但它们容易受到模型窃取攻击的威胁。这些攻击可以通过黑盒查询过程复制模型功能，而不需要任何关于目标受害模型的先前知识。现有的窃取防御方法通过向受害者的后验概率添加欺骗性扰动来误导攻击者。然而，这些防御方法现在面临着推理计算开销高和良好准确性与防窃鲁棒性之间不利权衡的问题，这挑战了在实践中部署这些模型的可行性。为了解决这些问题，本文提出了一种新颖有效的模型窃取防御训练框架Isolation and Induction（InI）。InI不像部署辅助防御模块那样引入冗余推理时间，而是通过将对手的训练梯度与预期梯度隔离来直接训练防御模型，可以有效地减少推理计算开销。

    Despite the broad application of Machine Learning models as a Service (MLaaS), they are vulnerable to model stealing attacks. These attacks can replicate the model functionality by using the black-box query process without any prior knowledge of the target victim model. Existing stealing defenses add deceptive perturbations to the victim's posterior probabilities to mislead the attackers. However, these defenses are now suffering problems of high inference computational overheads and unfavorable trade-offs between benign accuracy and stealing robustness, which challenges the feasibility of deployed models in practice. To address the problems, this paper proposes Isolation and Induction (InI), a novel and effective training framework for model stealing defenses. Instead of deploying auxiliary defense modules that introduce redundant inference time, InI directly trains a defensive model by isolating the adversary's training gradient from the expected gradient, which can effectively reduc
    
[^54]: 关于最先进生成模型的可信度景观：一项综合调查

    On the Trustworthiness Landscape of State-of-the-art Generative Models: A Comprehensive Survey. (arXiv:2307.16680v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.16680](http://arxiv.org/abs/2307.16680)

    本文综合调查了大规模生成模型的可信度问题，涵盖了隐私、安全、公平性和责任等多个维度，并提出了实际建议和未来发展方向。

    

    扩散模型和大规模语言模型已经成为领先的生成模型，并对人类生活的各个方面产生了革命性的影响。然而，这些模型的实际应用也暴露出固有的风险，突显了它们的双重性质，并引发了对它们可信度的担忧。尽管有大量关于这个主题的文献，但针对大规模生成模型及其可信度的综合调查仍然很少见。为了弥补这一空白，本文调查了涉及这些模型的长期和新兴威胁，涵盖了隐私、安全、公平和责任这四个基本维度。通过这种方式，我们构建了一张详尽的地图，概述了这些模型的可信度，并提供了实际建议和未来的发展方向。这些努力对于促进这些模型的可信度部署至关重要。

    Diffusion models and large language models have emerged as leading-edge generative models and have sparked a revolutionary impact on various aspects of human life. However, the practical implementation of these models has also exposed inherent risks, highlighting their dual nature and raising concerns regarding their trustworthiness. Despite the abundance of literature on this subject, a comprehensive survey specifically delving into the intersection of large-scale generative models and their trustworthiness remains largely absent. To bridge this gap, This paper investigates both the long-standing and emerging threats associated with these models across four fundamental dimensions: privacy, security, fairness, and responsibility. In this way, we construct an extensive map outlining the trustworthiness of these models, while also providing practical recommendations and identifying future directions. These efforts are crucial for promoting the trustworthy deployment of these models, ulti
    
[^55]: Relation-Oriented: 迈向与知识对准的因果人工智能

    Relation-Oriented: Toward Knowledge-Aligned Causal AI. (arXiv:2307.16387v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2307.16387](http://arxiv.org/abs/2307.16387)

    本研究从创新的关系导向视角出发，探讨了当前的建模范式中的观察模型与实际理解的不对齐问题，并提出了关系定义的表示学习方法作为实现关系导向建模的实践方法。

    

    在机器学习中，我们自然地应用一个观察导向的原则，其中观察变量先存在并为构建关系奠定基础。虽然对于传统模型来说足够了，但是人工智能与大数据的整合暴露了观察模型与我们的实际理解之间的不对齐。相反，人类塑造了由关系定义的认知实体，使我们能够跨越时间和超维度空间制定知识，而不是被限制在观察构建中。从一种创新的关系导向的视角出发，本研究通过来自计算机视觉和健康信息学的直观例子，分析了在我们当前的建模范式中这种不对齐的根源。我们还介绍了关系定义的表示学习方法作为关系导向建模的一种实际实施，支持广泛的实验验证。

    In machine learning, we naturally apply an Observation-Oriented principle, in which observational variables preexist and set the stage for constructing relationships. While sufficient for traditional models, the integration of AI with big data exposes the misalignment between the observational models and our actual comprehension. Contrarily, humans shape cognitive entities defined by relationships, enabling us to formulate knowledge across temporal and hyper-dimensional spaces, rather than being confined to observational constructs. From an innovative Relation-Oriented perspective, this study examines the roots of this misalignment within our current modeling paradigm, illuminated by intuitive examples from computer vision and health informatics. We also introduce the relation-defined representation learning methodology as a practical implementation of Relation-Oriented modeling, supported by extensive experimental validation.
    
[^56]: 基于实验增强的数据驱动建模方法用于双有源桥变换器的调制策略

    Data-Driven Modeling with Experimental Augmentation for the Modulation Strategy of the Dual-Active-Bridge Converter. (arXiv:2307.16173v1 [eess.SY])

    [http://arxiv.org/abs/2307.16173](http://arxiv.org/abs/2307.16173)

    本文介绍了一种基于实验增强的数据驱动建模方法，该方法通过结合仿真数据和实验数据来减轻模型差异，并在实践中提高准确性。

    

    对于电力转换器的性能建模，主流方法基本上是基于知识的，由于人力负担重和建模精度低而受困。最近出现的数据驱动技术通过从仿真数据中自动建模极大地减轻了对人的依赖。然而，由于未建模的寄生元件、不足的热磁模型、不可预测的环境条件等原因，模型的差异可能会发生。这些仅基于仿真的不准确的数据驱动模型无法代表实际世界中的实际性能，阻碍了它们在电力转换器建模中的应用。为了减轻模型差异并在实践中提高准确性，本文提出了一种新的基于实验增强的数据驱动建模方法(D2EA)，结合了仿真数据和实验数据。在D2EA中，仿真数据旨在建立基本的功能景观，实验数据则专注于匹配实际世界中的实际性能。实例化了D2EA方法

    For the performance modeling of power converters, the mainstream approaches are essentially knowledge-based, suffering from heavy manpower burden and low modeling accuracy. Recent emerging data-driven techniques greatly relieve human reliance by automatic modeling from simulation data. However, model discrepancy may occur due to unmodeled parasitics, deficient thermal and magnetic models, unpredictable ambient conditions, etc. These inaccurate data-driven models based on pure simulation cannot represent the practical performance in physical world, hindering their applications in power converter modeling. To alleviate model discrepancy and improve accuracy in practice, this paper proposes a novel data-driven modeling with experimental augmentation (D2EA), leveraging both simulation data and experimental data. In D2EA, simulation data aims to establish basic functional landscape, and experimental data focuses on matching actual performance in real world. The D2EA approach is instantiated
    
[^57]: 智能电网中一种有效的用于能量盗窃检测和预测的LSTM-DDPM方案

    An Effective LSTM-DDPM Scheme for Energy Theft Detection and Forecasting in Smart Grid. (arXiv:2307.16149v1 [cs.LG])

    [http://arxiv.org/abs/2307.16149](http://arxiv.org/abs/2307.16149)

    这篇论文提出了一种利用LSTM和DDPM相结合的方案来解决智能电网系统中的能量盗窃检测和预测问题。通过重构和预测误差，系统能够准确识别能量盗窃的实例，并在实验中表现出较好的性能。

    

    能量盗窃检测（ETD）和能量消耗预测（ECF）是智能电网系统中两个相互关联的挑战。共同解决这些问题对于确保系统安全至关重要。本论文解决了智能电网系统中的ETD和ECF的相互关联挑战。所提出的解决方案结合了长短期记忆（LSTM）和去噪扩散概率模型（DDPM），用于生成输入重构和预测。通过利用重构和预测误差，系统能够识别能量盗窃的实例，基于重构误差和预测误差的方法相互补充，可以检测不同类型的攻击。通过在真实和合成数据集上进行大量实验，所提出的方案在ETD和ECF问题上表现优于基准方法。集成方法显著提升了ETD性能，能够准确检测到基准方法未能检测到的能量盗窃攻击。该研究提供了一种可行的解决方案来解决智能电网系统中ETD和ECF的挑战。

    Energy theft detection (ETD) and energy consumption forecasting (ECF) are two interconnected challenges in smart grid systems. Addressing these issues collectively is crucial for ensuring system security. This paper addresses the interconnected challenges of ETD and ECF in smart grid systems. The proposed solution combines long short-term memory (LSTM) and a denoising diffusion probabilistic model (DDPM) to generate input reconstruction and forecasting. By leveraging the reconstruction and forecasting errors, the system identifies instances of energy theft, with the methods based on reconstruction error and forecasting error complementing each other in detecting different types of attacks. Through extensive experiments on real-world and synthetic datasets, the proposed scheme outperforms baseline methods in ETD and ECF problems. The ensemble method significantly enhances ETD performance, accurately detecting energy theft attacks that baseline methods fail to detect. The research offers
    
[^58]: 自动驾驶的自由空间光流建模

    Freespace Optical Flow Modeling for Automated Driving. (arXiv:2307.15989v1 [cs.RO])

    [http://arxiv.org/abs/2307.15989](http://arxiv.org/abs/2307.15989)

    本论文提出了一种在碰撞自由空间中建模光流的策略，充分利用了三维驾驶环境中的几何信息，通过对多个数据集进行广泛的实验，实现了光流的明确表示和光流分量与垂直坐标之间的二次关系。

    

    光流和视差是自主驾驶感知中两个信息丰富的视觉特征。它们已广泛应用于各种应用，如障碍物和车道检测。"U-V-Disparity"的概念在文献中得到了广泛探讨，而光流的对应概念却受到了相对较少的关注。传统的运动分析算法通过匹配两个连续视频帧之间的对应点来估计光流，这限制了对环境信息和几何约束的充分利用。因此，我们提出了一种新的策略，在智能车辆的碰撞自由空间（也称为可通行区域或简称为自由空间）中建模光流，充分利用三维驾驶环境中的几何信息。我们提供了光流的明确表示，并推导出光流分量与垂直坐标之间的二次关系。通过对多个数据集进行广泛的实验。

    Optical flow and disparity are two informative visual features for autonomous driving perception. They have been used for a variety of applications, such as obstacle and lane detection. The concept of "U-V-Disparity" has been widely explored in the literature, while its counterpart in optical flow has received relatively little attention. Traditional motion analysis algorithms estimate optical flow by matching correspondences between two successive video frames, which limits the full utilization of environmental information and geometric constraints. Therefore, we propose a novel strategy to model optical flow in the collision-free space (also referred to as drivable area or simply freespace) for intelligent vehicles, with the full utilization of geometry information in a 3D driving environment. We provide explicit representations of optical flow and deduce the quadratic relationship between the optical flow component and the vertical coordinate. Through extensive experiments on severa
    
[^59]: 受遮蔽扩散模型是快速和注重隐私的学习器

    Masked Diffusion Models Are Fast and Privacy-Aware Learners. (arXiv:2306.11363v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2306.11363](http://arxiv.org/abs/2306.11363)

    该论文提出了一种基于先验的去噪训练框架，通过遮蔽学习和扩散模型的结合，实现了更高效的训练和生成更高质量的图像。

    

    扩散模型已成为图像生成的事实上技术，然而它们具有显著的计算开销，限制了该技术在研究社区中的广泛应用。我们提出了一种基于先验的去噪训练框架，首次将预训练和微调范式纳入扩散模型训练过程中，大大提升了训练效率，并在促进各种下游任务方面显示出潜力。我们的方法主要是通过遮蔽输入图像的高比例（例如高达90％），并利用遮蔽去噪得分匹配来去噪可见区域，从而引导扩散模型从训练数据中学习更显著的特征作为先验知识。通过在预训练阶段使用遮蔽学习，我们在CelebA-HQ $256 \times 256$像素空间上高效地训练了基于ViT的扩散模型，实现了4倍加速，并提高了生成图像的质量，与去噪相比。

    Diffusion models have emerged as the \emph{de-facto} technique for image generation, yet they entail significant computational overhead, hindering the technique's broader application in the research community. We propose a prior-based denoising training framework, the first to incorporate the pre-train and fine-tune paradigm into the diffusion model training process, which substantially improves training efficiency and shows potential in facilitating various downstream tasks. Our approach centers on masking a high proportion (e.g., up to 90\%) of the input image and employing masked denoising score matching to denoise the visible areas, thereby guiding the diffusion model to learn more salient features from training data as prior knowledge. By utilizing masked learning in a pre-training stage, we efficiently train the ViT-based diffusion model on CelebA-HQ $256 \times 256$ in the pixel space, achieving a 4x acceleration and enhancing the quality of generated images compared to denoisin
    
[^60]: 人类和大型语言模型中的归纳推理

    Inductive reasoning in humans and large language models. (arXiv:2306.06548v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2306.06548](http://arxiv.org/abs/2306.06548)

    本研究使用GPT-3.5和GPT-4对人类归纳推理中的属性归纳问题进行了实验。结果表明，尽管GPT-3.5有一些困难，但GPT-4的表现与人类相似，除了未能捕捉到前提的非单调性现象。这项工作为人类和机器智能提供了有趣的比较，并提供了用作未来研究基准的两个大型数据集。

    

    大型语言模型的卓越性能引发了人们对其是否能作为普通智能的模型或类似于人类认知的程度的疑问。我们通过将GPT-3.5和GPT-4应用于人类归纳推理中的一个经典问题，即属性归纳，来解决这个问题。通过两个实验，我们获取了人类在多个领域上的属性归纳任务上的判断。尽管GPT-3.5在捕捉人类行为的许多方面上有困难，但GPT-4更加成功：在很大程度上，它的表现与人类的表现在质上相匹配，唯一显著的例外是其未能捕捉到前提的非单调性现象。我们的工作证明了属性归纳可以对人类和机器智能进行有趣的比较，并提供了两个大型数据集，可以作为未来在这一领域中的基准。

    The impressive recent performance of large language models has led many to wonder to what extent they can serve as models of general intelligence or are similar to human cognition. We address this issue by applying GPT-3.5 and GPT-4 to a classic problem in human inductive reasoning known as property induction. Over two experiments, we elicit human judgments on a range of property induction tasks spanning multiple domains. Although GPT-3.5 struggles to capture many aspects of human behaviour, GPT-4 is much more successful: for the most part, its performance qualitatively matches that of humans, and the only notable exception is its failure to capture the phenomenon of premise non-monotonicity. Our work demonstrates that property induction allows for interesting comparisons between human and machine intelligence and provides two large datasets that can serve as benchmarks for future work in this vein.
    
[^61]: 无监督的文本到图像生成模型下的组合式概念发现

    Unsupervised Compositional Concepts Discovery with Text-to-Image Generative Models. (arXiv:2306.05357v1 [cs.CV])

    [http://arxiv.org/abs/2306.05357](http://arxiv.org/abs/2306.05357)

    本论文提出了一种无监督的方法，用于从图像中自动地发现不同的生成概念，并且这些生成概念可以被用于重新组合和生成新的艺术和混合图像，并作为一种表示用于下游的分类任务。

    

    文本到图像生成模型使得在不同领域实现高分辨率的图像合成成为可能，但需要用户指定他们想要生成的内容。本文考虑了相反的问题——在给出的不同图像集合中，我们能否发现代表每个图像的生成概念？我们提出了一种无监督的方法来从一组图像中发现生成的概念，将绘画中不同的艺术风格，对象和照明从厨房场景中分解出来，并通过给定的ImageNet图像发现图像类。我们展示了这样的生成概念能够准确地表示图像的内容，能够重新组合和组成以生成新的艺术和混合图像，并可以作为下游分类任务的一种表示来使用。

    Text-to-image generative models have enabled high-resolution image synthesis across different domains, but require users to specify the content they wish to generate. In this paper, we consider the inverse problem -- given a collection of different images, can we discover the generative concepts that represent each image? We present an unsupervised approach to discover generative concepts from a collection of images, disentangling different art styles in paintings, objects, and lighting from kitchen scenes, and discovering image classes given ImageNet images. We show how such generative concepts can accurately represent the content of images, be recombined and composed to generate new artistic and hybrid images, and be further used as a representation for downstream classification tasks.
    
[^62]: NFT市场中的异常交易检测

    Abnormal Trading Detection in the NFT Market. (arXiv:2306.04643v1 [q-fin.TR])

    [http://arxiv.org/abs/2306.04643](http://arxiv.org/abs/2306.04643)

    本文提出了一种通过聚类算法检测非同质化代币（NFT）交易市场中的异常行为的方法，并探讨了监管对减少欺诈行为的影响。

    

    非同质化代币（NFT）市场近年来呈爆炸性增长。据DappRadar统计，最大的NFT市场OpenSea的总交易额在2023年2月达到了347亿美元。然而，NFT市场大部分是未受监管的，存在着重大的洗钱、欺诈和虚假交易等问题。在本文中，我们试图揭示常见的欺诈行为，如虚拟交易，这可能会误导其他交易者。我们使用市场数据从网络、货币和时间的角度设计量化特征，并将其输入到基于K均值聚类的无监督学习算法中，以对交易者进行分类。最后，我们讨论了聚类结果的重要性以及如何通过监管来减少不良行为。我们的工作可能有助于重新建立交易者的信任。

    The Non-Fungible-Token (NFT) market has experienced explosive growth in recent years. According to DappRadar, the total transaction volume on OpenSea, the largest NFT marketplace, reached 34.7 billion dollars in February 2023. However, the NFT market is mostly unregulated and there are significant concerns about money laundering, fraud and wash trading. Amateur traders and retail investors comprise a significant fraction of the NFT market. Hence it is important that researchers highlight the relevant risks involved in NFT trading. In this paper, we attempt to uncover common fraudulent behaviors such as wash trading that could mislead other traders. Using market data, we design quantitative features from the network, monetary, and temporal perspectives that are fed into K-means clustering unsupervised learning algorithm to sort traders into groups. Lastly, we discuss the clustering results' significance and how regulations can reduce undesired behaviors. Our work can potentially help re
    
[^63]: 从潜在图到潜在拓扑推断：可微分的单复形模块

    From Latent Graph to Latent Topology Inference: Differentiable Cell Complex Module. (arXiv:2305.16174v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.16174](http://arxiv.org/abs/2305.16174)

    该论文研究了一种从潜在图到潜在拓扑推断的方法，通过引入可微分的单复形模块（DCM），学习描述数据点之间多向交互的高阶单复形的稀疏且不规则的拓扑结构，并展示了如何将其与单复形消息传递网络层集成以提高下游任务的效果。

    

    潜在图推断（LGI）通过动态学习来减少图神经网络（GNNs）对给定图拓扑的依赖。然而，大多数LGI方法假设存在（噪声、不完整、可改进的...）输入图来重新连接，并且只能学习常规的图拓扑。在拓扑深度学习（TDL）取得成功之后，我们研究了用于学习描述数据点之间多向交互的高阶单复形（具有稀疏且不规则的拓扑结构）的潜在拓扑推断（LTI）。为此，我们引入了可微分的单复形模块（DCM），一种计算复杂中单元概率的新型可学习函数，用于改进下游任务。我们展示了如何将DCM与单复形消息传递网络层集成，以及如何通过两步推断过程在端到端的方式进行训练，避免对输入中所有可能的单元进行详尽搜索，从而保持可伸缩性。我们的模型在多个高阶拓扑推断任务上进行了测试。

    Latent Graph Inference (LGI) relaxed the reliance of Graph Neural Networks (GNNs) on a given graph topology by dynamically learning it. However, most of LGI methods assume to have a (noisy, incomplete, improvable, ...) input graph to rewire and can solely learn regular graph topologies. In the wake of the success of Topological Deep Learning (TDL), we study Latent Topology Inference (LTI) for learning higher-order cell complexes (with sparse and not regular topology) describing multi-way interactions between data points. To this aim, we introduce the Differentiable Cell Complex Module (DCM), a novel learnable function that computes cell probabilities in the complex to improve the downstream task. We show how to integrate DCM with cell complex message passing networks layers and train it in a end-to-end fashion, thanks to a two-step inference procedure that avoids an exhaustive search across all possible cells in the input, thus maintaining scalability. Our model is tested on several ho
    
[^64]: LaDI-VTON: 潜在扩散文本反演增强虚拟试穿

    LaDI-VTON: Latent Diffusion Textual-Inversion Enhanced Virtual Try-On. (arXiv:2305.13501v1 [cs.CV])

    [http://arxiv.org/abs/2305.13501](http://arxiv.org/abs/2305.13501)

    本文介绍了LaDI-VTON，一种利用潜在扩散模型和文本反演组件增强虚拟试穿任务的解决方案，该模型是第一个将这两种方法相结合的模型，能够高质量还原商店服装的细节。

    

    电子商务和元宇宙这两个不断发展变化的领域继续寻求创新方法来增强消费者体验。同时，最近扩散模型开发的进展使得生成网络能够创造出非常逼真的图像。在这种情况下，基于图像的虚拟试穿，就是生成一个目标模型穿着商店中的某件服装的新图片，尚未充分利用这些强大的生成解决方案的潜力。本研究介绍了LaDI-VTON，这是第一个潜在扩散文本反演增强的虚拟试穿模型。所提出的架构依赖于一个扩散模型，扩展了一个新的附加自动编码器模块，利用可学习的跳过连接来增强生成过程，保持模型的特征。为了有效地保持商店服装的质地和细节，我们提出了一个文本反演组件，可以将视觉特征映射到潜在扩散空间，实现服装的高质量细节还原。

    The rapidly evolving fields of e-commerce and metaverse continue to seek innovative approaches to enhance the consumer experience. At the same time, recent advancements in the development of diffusion models have enabled generative networks to create remarkably realistic images. In this context, image-based virtual try-on, which consists in generating a novel image of a target model wearing a given in-shop garment, has yet to capitalize on the potential of these powerful generative solutions. This work introduces LaDI-VTON, the first Latent Diffusion textual Inversion-enhanced model for the Virtual Try-ON task. The proposed architecture relies on a latent diffusion model extended with a novel additional autoencoder module that exploits learnable skip connections to enhance the generation process preserving the model's characteristics. To effectively maintain the texture and details of the in-shop garment, we propose a textual inversion component that can map the visual features of the 
    
[^65]: 变分分类

    Variational Classification. (arXiv:2305.10406v1 [cs.LG])

    [http://arxiv.org/abs/2305.10406](http://arxiv.org/abs/2305.10406)

    提出一种新的变分分类方法，通过引入潜变量建模来优化训练，允许灵活的设计选择以改善校准和对抗鲁棒性，实验结果表明其对于域外数据的分类准确性得到了保持。

    

    我们提出了一种传统神经网络方法的新型扩展，称为变分分类 (VC)。通过引入潜变量建模，类似于变分自编码器和传统自编码器之间的关系，我们得到了一个基于证据下界 (ELBO) 的训练目标，采用对抗性方法优化。我们的VC模型允许在设计选择方面更加灵活，特别是类条件潜先验，而不是在现成的softmax分类器中做出的隐式假设。在图像和文本分类数据集上的实证评估表明，我们的方法在保持预测准确性的同时，改善了其他良好特性，如校准和对抗鲁棒性，即使应用于域外数据。

    We present a novel extension of the traditional neural network approach to classification tasks, referred to as variational classification (VC). By incorporating latent variable modeling, akin to the relationship between variational autoencoders and traditional autoencoders, we derive a training objective based on the evidence lower bound (ELBO), optimized using an adversarial approach. Our VC model allows for more flexibility in design choices, in particular class-conditional latent priors, in place of the implicit assumptions made in off-the-shelf softmax classifiers. Empirical evaluation on image and text classification datasets demonstrates the effectiveness of our approach in terms of maintaining prediction accuracy while improving other desirable properties such as calibration and adversarial robustness, even when applied to out-of-domain data.
    
[^66]: Mlinear:重新思考时间序列预测的线性模型

    Mlinear: Rethink the Linear Model for Time-series Forecasting. (arXiv:2305.04800v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.04800](http://arxiv.org/abs/2305.04800)

    该论文重新思考了时间序列预测的线性模型，提出了Mlinear方法，通过动态调节通道独立性和通道依赖性属性以实现更好的预测性能。

    

    最近，在时间序列预测研究中取得了显著的进展，越来越关注分析时间序列数据的性质，例如通道独立性（CI）和通道依赖性（CD），而不仅仅关注设计复杂的预测模型。然而，当前研究主要集中在单独的CI或CD上，有效地结合这两个相反的属性以实现协同效应的挑战仍然是一个未解决的问题。在本文中，我们仔细研究了CI和CD的相反属性，并提出了一个迄今未能有效回答的实际问题，即“如何有效地混合时间序列的CI和CD属性以实现更好的预测性能？”为了回答这个问题，我们提出了Mlinear（MIX-Linear），这是一种简单而有效的基于线性层的方法。Mlinear的设计理念主要包括两个方面：（1）基于动态调节CI和CD属性的机制

    Recently, significant advancements have been made in time-series forecasting research, with an increasing focus on analyzing the nature of time-series data, e.g, channel-independence (CI) and channel-dependence (CD), rather than solely focusing on designing sophisticated forecasting models. However, current research has primarily focused on either CI or CD in isolation, and the challenge of effectively combining these two opposing properties to achieve a synergistic effect remains an unresolved issue. In this paper, we carefully examine the opposing properties of CI and CD, and raise a practical question that has not been effectively answered, e.g.,"How to effectively mix the CI and CD properties of time series to achieve better predictive performance?" To answer this question, we propose Mlinear (MIX-Linear), a simple yet effective method based mainly on linear layers. The design philosophy of Mlinear mainly includes two aspects:(1) dynamically tuning the CI and CD properties based on
    
[^67]: 自动发现的思维链提示可以推广到新模型和数据集

    An automatically discovered chain-of-thought prompt generalizes to novel models and datasets. (arXiv:2305.02897v1 [cs.CL])

    [http://arxiv.org/abs/2305.02897](http://arxiv.org/abs/2305.02897)

    本文研究了一系列零照顾提示在六个最新发布的语言模型和问题回答数据集的实验中的表现，发现自动提示发现的CoT提示可在新模型和数据集上表现良好，并在应用于GPT-4模型时取得最佳结果。

    

    新兴的思维链（CoT）推理能力有望提高大型语言模型（LLM）的性能和可解释性。然而，对于先前模型所制定的提示策略如何适用于新模型和不同数据集仍存在不确定性。在这项小型研究中，我们比较了一系列零照顾提示（zero-shot prompts）的性能，以诱导CoT推理，在6个最新发布的LLM（davinci-002，davinci-003，GPT-3.5-turbo，GPT-4，Flan-T5-xxl和Cohere command-xlarge）上与包括科学和医学领域的六个问答数据集混合在一起。我们发现，通过自动提示发现的CoT提示在实验条件下表现出鲁棒性，并在应用于最先进的GPT-4模型时产生最佳结果。

    Emergent chain-of-thought (CoT) reasoning capabilities promise to improve performance and explainability of large language models (LLMs). However, uncertainties remain about how prompting strategies formulated for previous model generations generalize to new model generations and different datasets. In this small-scale study we compare the performance of a range of zero-shot prompts for inducing CoT reasoning across six recently released LLMs (davinci-002, davinci-003, GPT-3.5-turbo, GPT-4, Flan-T5-xxl and Cohere command-xlarge) on a mixture of six question-answering datasets, including datasets from scientific and medical domains. We find that a CoT prompt that was previously discovered through automated prompt discovery shows robust performance across experimental conditions and produces best results when applied to the state-of-the-art model GPT-4.
    
[^68]: OpenAGI：当LLM遇到领域专家

    OpenAGI: When LLM Meets Domain Experts. (arXiv:2304.04370v1 [cs.AI])

    [http://arxiv.org/abs/2304.04370](http://arxiv.org/abs/2304.04370)

    基于大型语言模型的OpenAGI平台通过整合领域专家模型和自然语言问答形式，实现复杂任务解决。

    

    人类具有将基本技能组合成复杂技能以解决复杂任务的显著能力。这种能力对于人工智能同样重要，因此，我们断言，除了开发大型综合智能模型外，将不同领域专家模型应用于复杂任务解决能力同样关键，以在人工智能通用智能的追求中使其具备这种能力。最近的大型语言模型（LLM）的发展证明其具有出色的学习和推理能力，使它们成为选择、综合和执行外部模型以解决复杂任务的控制器的有前途的选择。在这个项目中，我们开发了一个名为OpenAGI的开源AGI研究平台，专门设计为提供复杂的多步骤任务，并配有任务特定的数据集、评估指标和各种可扩展模型。OpenAGI将复杂任务阐释为自然语言问答，旨在促进领域专家和语言模型之间的协同作用。

    Human intelligence has the remarkable ability to assemble basic skills into complex ones so as to solve complex tasks. This ability is equally important for Artificial Intelligence (AI), and thus, we assert that in addition to the development of large, comprehensive intelligent models, it is equally crucial to equip such models with the capability to harness various domain-specific expert models for complex task-solving in the pursuit of Artificial General Intelligence (AGI). Recent developments in Large Language Models (LLMs) have demonstrated remarkable learning and reasoning abilities, making them promising as a controller to select, synthesize, and execute external models to solve complex tasks. In this project, we develop OpenAGI, an open-source AGI research platform, specifically designed to offer complex, multi-step tasks and accompanied by task-specific datasets, evaluation metrics, and a diverse range of extensible models. OpenAGI formulates complex tasks as natural language q
    
[^69]: 对知识蒸馏的训练动态进行详细研究

    A closer look at the training dynamics of knowledge distillation. (arXiv:2303.11098v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2303.11098](http://arxiv.org/abs/2303.11098)

    本文对知识蒸馏的训练动态进行了详细研究，实验证明投影器的设计决策、表示的标准化和软最大函数的选择对学生的性能有着重要影响，同时提出了一种解决容量差异问题的简单方法，以及与当前最先进的知识蒸馏技术相媲美的计算效率更高的方法。

    

    本文重新审视将知识蒸馏作为函数匹配和度量学习问题时的有效性。通过验证三个重要设计决策，即标准化、软最大函数和投影层作为关键要素，我们有理论地显示出投影器隐含地编码了关于过去样本的信息，从而为学生提供了关联梯度。然后，我们展示了表示的标准化与投影器的训练动态密切相关，这可能对学生的性能产生重大影响。最后，我们展示了简单的软最大函数可以用来解决任何显著容量差异的问题。在各种基准数据集上的实验结果表明，利用这些见解可以实现与最先进的知识蒸馏技术相媲美或优于其性能，同时计算效率更高。特别是在图像分类任务上取得了这些结果。

    In this paper we revisit the efficacy of knowledge distillation as a function matching and metric learning problem. In doing so we verify three important design decisions, namely the normalisation, soft maximum function, and projection layers as key ingredients. We theoretically show that the projector implicitly encodes information on past examples, enabling relational gradients for the student. We then show that the normalisation of representations is tightly coupled with the training dynamics of this projector, which can have a large impact on the students performance. Finally, we show that a simple soft maximum function can be used to address any significant capacity gap problems. Experimental results on various benchmark datasets demonstrate that using these insights can lead to superior or comparable performance to state-of-the-art knowledge distillation techniques, despite being much more computationally efficient. In particular, we obtain these results across image classificati
    
[^70]: 最优觅食策略是可学习的，并且优于 L\'evy walks。

    Optimal foraging strategies can be learned and outperform L\'evy walks. (arXiv:2303.06050v2 [cond-mat.stat-mech] UPDATED)

    [http://arxiv.org/abs/2303.06050](http://arxiv.org/abs/2303.06050)

    研究发现，通过强化学习代理，存活的生物可以学习到优于 L\'evy walks 的觅食策略，以提高觅食效率。

    

    L\'evy walks 和其他争议论的最优觅食模型成功地被用于描述现实世界的场景，吸引了经济学、物理学、生态学、进化生物学等多个领域的关注。然而，在大多数情况下仍不清楚哪些策略可以最大化觅食效率，这些策略是否可以被生物学习。为了回答这些问题，我们将觅食者建模为强化学习代理。我们首先理论上证明，在我们的强化学习模型中最大化奖励等同于优化觅食效率。然后，通过数值实验，我们展示了我们的代理学习了优于已知策略如 L\'evy walks 的觅食策略。

    L\'evy walks and other theoretical models of optimal foraging have been successfully used to describe real-world scenarios, attracting attention in several fields such as economy, physics, ecology, and evolutionary biology. However, it remains unclear in most cases which strategies maximize foraging efficiency and whether such strategies can be learned by living organisms. To address these questions, we model foragers as reinforcement learning agents. We first prove theoretically that maximizing rewards in our reinforcement learning model is equivalent to optimizing foraging efficiency. We then show with numerical experiments that our agents learn foraging strategies which outperform the efficiency of known strategies such as L\'evy walks.
    
[^71]: SLCA: 预训练模型上用于连续学习的慢学习者与分类器对齐

    SLCA: Slow Learner with Classifier Alignment for Continual Learning on a Pre-trained Model. (arXiv:2303.05118v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2303.05118](http://arxiv.org/abs/2303.05118)

    SLCA是一种用于连续学习的简单但极其有效的方法。它通过慢学习和分类器对齐来在预训练模型上提高泛化能力和解决渐进过拟合问题。

    

    连续学习的目标是在学习顺序到达的数据中提高识别模型的性能。尽管大部分现有工作都建立在从头学习的前提下，但越来越多的努力已经致力于融入预训练的好处。然而，如何在每个增量任务中自适应地利用预训练的知识，同时保持其泛化能力，仍然是一个未解决的问题。在这项工作中，我们对预训练模型上的连续学习进行了广泛的分析，并将关键挑战归因于渐进过拟合问题。观察到在表征层次上选择性降低学习率几乎可以解决这个问题，我们提出了一种简单但极其有效的方法，名为慢学习者与分类器对齐（SLCA），通过建模类别分布并在事后对齐分类层次，进一步改进了分类层次。在各种实验中，我们证明了SLCA在连续学习任务中的有效性和性能优势。

    The goal of continual learning is to improve the performance of recognition models in learning sequentially arrived data. Although most existing works are established on the premise of learning from scratch, growing efforts have been devoted to incorporating the benefits of pre-training. However, how to adaptively exploit the pre-trained knowledge for each incremental task while maintaining its generalizability remains an open question. In this work, we present an extensive analysis for continual learning on a pre-trained model (CLPM), and attribute the key challenge to a progressive overfitting problem. Observing that selectively reducing the learning rate can almost resolve this issue in the representation layer, we propose a simple but extremely effective approach named Slow Learner with Classifier Alignment (SLCA), which further improves the classification layer by modeling the class-wise distributions and aligning the classification layers in a post-hoc fashion. Across a variety o
    
[^72]: 多模态有助于单模态：多模态模型下的交叉模态少样本学习

    Multimodality Helps Unimodality: Cross-Modal Few-Shot Learning with Multimodal Models. (arXiv:2301.06267v4 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2301.06267](http://arxiv.org/abs/2301.06267)

    通过跨模态适应方法，在多模态模型下利用少样本示例（包括文本和声音）进行狗的视觉分类，并取得了最先进的结果。

    

    快速学习新任务的能力是智能代理的核心要素，也被称为少样本学习。传统的少样本学习基准使用来自单模态的少样本样本，但这些样本可能不足以描述整个概念类。相比之下，人类使用跨模态信息高效地学习新概念。在这项工作中，我们展示了通过阅读关于狗并听它们吠叫的声音来构建更好的视觉狗分类器的可能性。为此，我们利用最近的多模态基础模型（如CLIP）是固有的跨模态的特性，将不同的模态映射到相同的表示空间。具体而言，我们提出了一种简单的跨模态适应方法，从跨越不同模态的少样本示例中进行学习。通过将类名重新用作额外的一次性训练样本，我们使用一个极其简单的线性分类器实现了最先进的结果。

    The ability to quickly learn a new task with minimal instruction - known as few-shot learning - is a central aspect of intelligent agents. Classical few-shot benchmarks make use of few-shot samples from a single modality, but such samples may not be sufficient to characterize an entire concept class. In contrast, humans use cross-modal information to learn new concepts efficiently. In this work, we demonstrate that one can indeed build a better ${\bf visual}$ dog classifier by ${\bf read}$ing about dogs and ${\bf listen}$ing to them bark. To do so, we exploit the fact that recent multimodal foundation models such as CLIP are inherently cross-modal, mapping different modalities to the same representation space. Specifically, we propose a simple cross-modal adaptation approach that learns from few-shot examples spanning different modalities. By repurposing class names as additional one-shot training samples, we achieve SOTA results with an embarrassingly simple linear classifier for visi
    
[^73]: BEVBert: 用于语言导向导航的多模态地图预训练

    BEVBert: Multimodal Map Pre-training for Language-guided Navigation. (arXiv:2212.04385v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2212.04385](http://arxiv.org/abs/2212.04385)

    本论文提出了一种多模态地图预训练方法，用于语言导向导航任务。通过构建局部度量地图和全局拓扑地图，该方法能够准确刻画空间感知和导航依赖关系，从而提高了语言导向导航的性能。

    

    大规模预训练已经在视觉与语言导航（VLN）任务上显示出了很好的结果。然而，大多数现有的预训练方法采用离散的全景图来学习视觉-文本关联。这要求模型隐式地关联全景图中的不完整、重复的观察数据，这可能影响到智能体的空间理解能力。因此，我们提出了一种新的基于地图的预训练范式，以用于VLN中的空间感知。具体而言，我们构建了一个局部度量地图，明确地汇聚不完整的观察数据并消除重复，同时在一个全局拓扑地图中建模导航依赖关系。这种混合设计可以平衡VLN对短期推理和长期规划的需求。然后，基于混合地图，我们设计了一个预训练框架来学习多模态地图表示，从而增强了空间感知跨模态推理，有助于语言导向导航目标的实现。广泛的实验验证了该方法的有效性。

    Large-scale pre-training has shown promising results on the vision-and-language navigation (VLN) task. However, most existing pre-training methods employ discrete panoramas to learn visual-textual associations. This requires the model to implicitly correlate incomplete, duplicate observations within the panoramas, which may impair an agent's spatial understanding. Thus, we propose a new map-based pre-training paradigm that is spatial-aware for use in VLN. Concretely, we build a local metric map to explicitly aggregate incomplete observations and remove duplicates, while modeling navigation dependency in a global topological map. This hybrid design can balance the demand of VLN for both short-term reasoning and long-term planning. Then, based on the hybrid map, we devise a pre-training framework to learn a multimodal map representation, which enhances spatial-aware cross-modal reasoning thereby facilitating the language-guided navigation goal. Extensive experiments demonstrate the effec
    
[^74]: 一种无监督机器学习方法用于地震动谱的聚类和选择

    An Unsupervised Machine Learning Approach for Ground-Motion Spectra Clustering and Selection. (arXiv:2212.03188v2 [physics.geo-ph] UPDATED)

    [http://arxiv.org/abs/2212.03188](http://arxiv.org/abs/2212.03188)

    本论文提出了一种无监督机器学习方法，用于提取地震动谱的定义特征，以辅助地震动选择。它结合了机器发现的潜在特征和传统强度测量，通过聚类分析选择代表性的地震动记录。验证结果表明该方法的有效性。

    

    随着机器学习在应用科学中的快速发展，序列数据的聚类分析在工程设计中仍然具有许多应用。本文提出了一种无监督机器学习算法，用于提取地震动谱的定义特征，也称为潜在特征，以辅助地震动选择。在这个背景下，潜在特征是通过神经网络自动编码器学习的低维度机器发现的谱特征。机器发现的潜在特征可以与传统定义的强度测量相结合，进行聚类分析，从大量的地震动样本中选择一个代表性的子集。高效的地震动选择的目标是选择代表性的记录，以概率性地反映结构在其寿命周期内将经历的情况。本文提供了三个示例来验证该方法，包括使用合成和现场记录。

    Clustering analysis of sequence data continues to address many applications in engineering design, aided with the rapid growth of machine learning in applied science. This paper presents an unsupervised machine learning algorithm to extract defining characteristics of earthquake ground-motion spectra, also called latent features, to aid in ground-motion selection (GMS). In this context, a latent feature is a low-dimensional machine-discovered spectral characteristic learned through nonlinear relationships of a neural network autoencoder. Machine discovered latent features can be combined with traditionally defined intensity measures and clustering can be performed to select a representative subgroup from a large ground-motion suite. The objective of efficient GMS is to choose characteristic records representative of what the structure will probabilistically experience in its lifetime. Three examples are presented to validate this approach, including the use of synthetic and field recor
    
[^75]: 无损失即无协议：学习与同行评审的社会选择

    No Agreement Without Loss: Learning and Social Choice in Peer Review. (arXiv:2211.02144v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2211.02144](http://arxiv.org/abs/2211.02144)

    在本文中，我们挑战了Nothigattu、Shah和Procaccia的框架，该框架旨在通过最小化损失函数来聚合评审人的映射，我们发现了一些负面结果。

    

    在同行评审系统中，评审人经常被要求评估提交稿件的各种特征，如技术质量或新颖性。给定每个预定义特征的评分，评审人必须提供一个整体的定量建议。可以假设每个评审人对特征集合到建议的映射都有自己的看法，并且不同的评审人心中有着不同的映射。这引入了一个称为比例偏差的任选性因素。本文讨论了一个由Noothigattu、Shah和Procaccia引入的框架，并且该框架在AAAI2022会议的组织者中应用。Noothigattu、Shah和Procaccia提出通过最小化特定损失函数来聚合评审人的映射，并研究了这种方法的社会选择理论中的公理性质。我们对他们的工作中使用的结果和假设进行了挑战，并报告了一些负面结果。

    In peer review systems, reviewers are often asked to evaluate various features of submissions, such as technical quality or novelty. A score is given to each of the predefined features and based on these the reviewer has to provide an overall quantitative recommendation. It may be assumed that each reviewer has her own mapping from the set of features to a recommendation, and that different reviewers have different mappings in mind. This introduces an element of arbitrariness known as commensuration bias. In this paper we discuss a framework, introduced by Noothigattu, Shah and Procaccia, and then applied by the organizers of the AAAI 2022 conference. Noothigattu, Shah and Procaccia proposed to aggregate reviewer's mapping by minimizing certain loss functions, and studied axiomatic properties of this approach, in the sense of social choice theory. We challenge several of the results and assumptions used in their work and report a number of negative results. On the one hand, we study a 
    
[^76]: 推荐系统中的公平性：基础、方法和应用

    Fairness in Recommendation: Foundations, Methods and Applications. (arXiv:2205.13619v5 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2205.13619](http://arxiv.org/abs/2205.13619)

    这篇论文对推荐系统中的公平性问题进行了系统调查，针对推荐过程中可能出现的数据或算法偏见，提供了一些方法和应用来提升推荐中的公平性。

    

    作为机器学习最普遍的应用之一，推荐系统在辅助人类决策中起着重要作用。用户的满意度和平台的利益与生成的推荐结果的质量密切相关。然而，作为一个高度数据驱动的系统，推荐系统可能受到数据或算法偏见的影响，从而产生不公平的结果，这可能削弱系统的可信赖性。因此，在推荐设置中解决潜在的不公平问题至关重要。最近，对推荐系统的公平性考虑引起了越来越多的关注，涉及提升推荐中的公平性的方法越来越多。然而，这些研究相对零散且缺乏系统化整理，因此对于新研究人员来说难以深入领域。这促使我们对推荐中现有公平性作品进行系统调查。

    As one of the most pervasive applications of machine learning, recommender systems are playing an important role on assisting human decision making. The satisfaction of users and the interests of platforms are closely related to the quality of the generated recommendation results. However, as a highly data-driven system, recommender system could be affected by data or algorithmic bias and thus generate unfair results, which could weaken the reliance of the systems. As a result, it is crucial to address the potential unfairness problems in recommendation settings. Recently, there has been growing attention on fairness considerations in recommender systems with more and more literature on approaches to promote fairness in recommendation. However, the studies are rather fragmented and lack a systematic organization, thus making it difficult to penetrate for new researchers to the domain. This motivates us to provide a systematic survey of existing works on fairness in recommendation. This
    
[^77]: 后继特征神经记忆控制

    Successor Feature Neural Episodic Control. (arXiv:2111.03110v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2111.03110](http://arxiv.org/abs/2111.03110)

    本文研究了记忆控制和后继特征两个框架的整合，通过结合这两种方法，提高了强化学习的样本效率和策略重用的优雅性。

    

    强化学习中一个长期目标是构建智能代理，展示类似于人类和动物的快速学习和灵活的技能转移。本文研究了解决这些目标的两个框架的整合：记忆控制和后继特征。记忆控制是一种受认知启发的方法，依赖于情节记忆，即代理的经验的基于实例的内存模型。同时，后继特征和广义策略改进（SF&amp;GPI）是一种元学习和迁移学习框架，可以为具有不同奖励函数的后续任务学习策略，并且可以高效地重用先前学到的策略。这两种技术分别在大大提高样本效率和优雅地重用先前学到的策略方面显示出令人印象深刻的结果。因此，我们概述了将这两种方法结合在一个单一的强化学习框架中，并通过实证研究其好处。

    A longstanding goal in reinforcement learning is to build intelligent agents that show fast learning and a flexible transfer of skills akin to humans and animals. This paper investigates the integration of two frameworks for tackling those goals: episodic control and successor features. Episodic control is a cognitively inspired approach relying on episodic memory, an instance-based memory model of an agent's experiences. Meanwhile, successor features and generalized policy improvement (SF&GPI) is a meta and transfer learning framework allowing to learn policies for tasks that can be efficiently reused for later tasks which have a different reward function. Individually, these two techniques have shown impressive results in vastly improving sample efficiency and the elegant reuse of previously learned policies. Thus, we outline a combination of both approaches in a single reinforcement learning framework and empirically illustrate its benefits.
    
[^78]: Auto-COP: 使用强化学习选项在上下文导向编程中生成自适应

    Auto-COP: Adaptation Generation in Context-Oriented Programming using Reinforcement Learning Options. (arXiv:2103.06757v2 [cs.PL] UPDATED)

    [http://arxiv.org/abs/2103.06757](http://arxiv.org/abs/2103.06757)

    运行时生成适应性的新技术Auto-COP在上下文导向编程中使用强化学习选项来生成动作序列，从而实现自适应系统的开发。

    

    自适应软件系统根据执行环境中的内部和外部变化不断进行适应，这些变化以上下文形式捕捉。上下文导向编程（COP）提出了一种用于开发自适应系统的技术，它使用专门的编程语言构造来捕捉其主要特征。COP适应性通过指定独立的模块在基本系统内外进行组合，以响应来自周围环境的感知情况而激活和停用上下文来进行定义。然而，适应性、上下文和相关的特殊行为的定义需要在设计阶段指定。在复杂的CPS中，由于新的不可预测的工作条件，这是难以处理的。我们提出了Auto-COP，一种新的技术，可以在运行时生成适应性。Auto-COP使用RL选项构建动作序列，基于之前系统执行的实例。通过与环境的交互来探索选项，并选择最合适的选项。

    Self-adaptive software systems continuously adapt in response to internal and external changes in their execution environment, captured as contexts. The COP paradigm posits a technique for the development of self-adaptive systems, capturing their main characteristics with specialized programming language constructs. COP adaptations are specified as independent modules composed in and out of the base system as contexts are activated and deactivated in response to sensed circumstances from the surrounding environment. However, the definition of adaptations, their contexts and associated specialized behavior, need to be specified at design time. In complex CPS this is intractable due to new unpredicted operating conditions. We propose Auto-COP, a new technique to enable generation of adaptations at run time. Auto-COP uses RL options to build action sequences, based on the previous instances of the system execution. Options are explored in interaction with the environment, and the most sui
    
[^79]: ROME: 通过拓扑解耦和梯度累积实现鲁棒的内存高效NAS

    ROME: Robustifying Memory-Efficient NAS via Topology Disentanglement and Gradient Accumulation. (arXiv:2011.11233v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2011.11233](http://arxiv.org/abs/2011.11233)

    ROME是一种鲁棒的内存高效的NAS方法，通过拓扑解耦和梯度累积解决了单路径DARTS中的性能下降问题。

    

    尽管可微架构搜索（DARTS）是一种流行的架构搜索方法，但由于整个超网络存放在内存中，它受到了严重的内存开销的限制。这就是单路径DARTS的优势所在，它在每个步骤中只选择一个单路径子模型。虽然它对内存友好，但计算成本较低。然而，我们发现单路径DARTS存在一个关键问题，这一问题尚未得到足够的关注。也就是说，它也会出现严重的性能下降，因为像DARTS一样，它会导出太多无参数操作，例如跳跃连接。在本文中，我们提出了一种名为ROME的新算法来解决这个问题。首先，我们通过拓扑搜索和操作搜索解耦，使搜索和评估保持一致。然后，我们采用Gumbel-Top2重新参数化和梯度累积来增强具有挑战性的双层优化。我们对ROME进行了大量验证。

    Albeit being a prevalent architecture searching approach, differentiable architecture search (DARTS) is largely hindered by its substantial memory cost since the entire supernet resides in the memory. This is where the single-path DARTS comes in, which only chooses a single-path submodel at each step. While being memory-friendly, it also comes with low computational costs. Nonetheless, we discover a critical issue of single-path DARTS that has not been primarily noticed. Namely, it also suffers from severe performance collapse since too many parameter-free operations like skip connections are derived, just like DARTS does. In this paper, we propose a new algorithm called RObustifying Memory-Efficient NAS (ROME) to give a cure. First, we disentangle the topology search from the operation search to make searching and evaluation consistent. We then adopt Gumbel-Top2 reparameterization and gradient accumulation to robustify the unwieldy bi-level optimization. We verify ROME extensively acr
    

