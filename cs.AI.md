# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Recurrent Transformers with Dynamic Halt](https://rss.arxiv.org/abs/2402.00976) | 本文研究了增强Transformer与循环机制的两种方法，并提出了新的扩展和组合方法。在多个诊断任务中进行比较，探索它们的归纳偏好。 |
| [^2] | [NeRF-MAE : Masked AutoEncoders for Self Supervised 3D representation Learning for Neural Radiance Fields](https://arxiv.org/abs/2404.01300) | 通过使用Masked AutoEncoders，本文提出了NeRF-MAE用于自监督三维表示学习，利用标准的三维Vision Transformers适应NeRF的独特公式，将NeRF的体积网格作为密集输入，以产生有效的三维表示。 |
| [^3] | [CausalChaos! Dataset for Comprehensive Causal Action Question Answering Over Longer Causal Chains Grounded in Dynamic Visual Scenes](https://arxiv.org/abs/2404.01299) | 利用卡通图像构建的CausalChaos!数据集，包含更长因果链的因果问答，通过动态互动和视觉展示挑战性因果关系，为模型提供了更多具挑战性且明确定义的因果关系。 |
| [^4] | [Towards Safety and Helpfulness Balanced Responses via Controllable Large Language Models](https://arxiv.org/abs/2404.01295) | 通过控制大语言模型中的属性，实现安全和帮助之间的平衡，并且可以在不同的应用场景中实现这种平衡。 |
| [^5] | [Evaluating Text-to-Visual Generation with Image-to-Text Generation](https://arxiv.org/abs/2404.01291) | 引入VQAScore，利用视觉问答模型生成对齐分数，取得了在图像文本对齐基准上的最先进结果 |
| [^6] | [Mapping the Increasing Use of LLMs in Scientific Papers](https://arxiv.org/abs/2404.01268) | 该论文通过对科学论文中LLM使用增加的映射进行了大规模分析，填补了学术写作中真实LLM修改内容比例缺失的空白。 |
| [^7] | [IsoBench: Benchmarking Multimodal Foundation Models on Isomorphic Representations](https://arxiv.org/abs/2404.01266) | IsoBench提出了一个基准数据集，用于评估基于不同同构表示的多模态基础模型的性能差异，发现大多数模型更偏好文本表示。 |
| [^8] | [FABLES: Evaluating faithfulness and content selection in book-length summarization](https://arxiv.org/abs/2404.01261) | 本文首次对LLM生成的虚构书籍摘要进行了忠实性和内容选择的大规模人类评估，建立了FABLES数据集，通过对26本书的3158个声明进行了注释，成功对LLM摘要进行了基于忠实性的排名 |
| [^9] | [Bridging Remote Sensors with Multisensor Geospatial Foundation Models](https://arxiv.org/abs/2404.01260) | msGFM是一个多传感器地理空间基础模型，能够有效统一四种关键传感器模态的数据，具有处理成对和非成对传感器数据的能力，通过创新的跨传感器预训练方法在蒙版图像建模中合成联合表示，展现出在各种传感器类型下都具有强大性能的综合模型。 |
| [^10] | [Direct Preference Optimization of Video Large Multimodal Models from Language Model Reward](https://arxiv.org/abs/2404.01258) | 本研究提出了一种新的框架，利用详细的视频标题作为视频内容的代理，使得语言模型在评分视频问答（QA）预测时能够融入这些信息作为支持证据。 |
| [^11] | [Feature Splatting: Language-Driven Physics-Based Scene Synthesis and Editing](https://arxiv.org/abs/2404.01223) | 该研究提出了一种名为Feature Splatting的方法，将基于物理的动态场景合成与源自视觉语言的丰富语义统一起来，实现了高质量的对象分解和基于文本查询的物理特性自动合成。 |
| [^12] | [Incorporating Domain Differential Equations into Graph Convolutional Networks to Lower Generalization Discrepancy](https://arxiv.org/abs/2404.01217) | 将领域微分方程纳入图卷积网络可提高其对不匹配训练和测试数据的稳健性。 |
| [^13] | [Capturing Shock Waves by Relaxation Neural Networks](https://arxiv.org/abs/2404.01163) | RelaxNN框架通过弛豫系统解决了在双曲系统解中出现的激波难题，缓解了PINN框架在训练过程中的损失冲突。 |
| [^14] | [SyncMask: Synchronized Attentional Masking for Fashion-centric Vision-Language Pretraining](https://arxiv.org/abs/2404.01156) | 提出了同步注意力遮罩（SyncMask），用于解决时尚领域图像和文本信息不匹配的问题，可以准确对齐细粒度的视觉和文本特征。 |
| [^15] | [Uncovering the Text Embedding in Text-to-Image Diffusion Models](https://arxiv.org/abs/2404.01154) | 本文揭示了文本嵌入在文本到图像扩散模型中的作用，并且提出了关于单词嵌入及其上下文相关性的重要见解，为学习免费图像编辑提供了指导原则。 |
| [^16] | [Condition-Aware Neural Network for Controlled Image Generation](https://arxiv.org/abs/2404.01143) | 提出了一种条件感知神经网络（CAN）用于图像生成模型，通过动态调整神经网络的权重来控制图像生成过程，在ImageNet和COCO上测试表明，CAN与EfficientViT（CaT）相结合取得了显著的改进。 |
| [^17] | [Enhancing Reasoning Capacity of SLM using Cognitive Enhancement](https://arxiv.org/abs/2404.01135) | 利用认知增强技术，本研究提出了一种在网络调查和数字取证中提高SLM推理能力的方法，其中通过处理本地数据并减少参数数量来满足安全性和数据隐私性需求。 |
| [^18] | [GOV-REK: Governed Reward Engineering Kernels for Designing Robust Multi-Agent Reinforcement Learning Systems](https://arxiv.org/abs/2404.01131) | GOV-REK提出了一种动态为多智能体强化学习系统中的代理分配奖励分布的方法，并通过治理内核利用状态或联合行动空间的结构，以解决奖励工程努力无法转化的问题。 |
| [^19] | [Medical Visual Prompting (MVP): A Unified Framework for Versatile and High-Quality Medical Image Segmentation](https://arxiv.org/abs/2404.01127) | 提出了医疗视觉提示（MVP）框架，结合自然语言处理的预训练和提示概念，利用三个关键组件来解决医学图像分割中病变形状信息丢失和手动标记成本高的挑战 |
| [^20] | [What's in Your "Safe" Data?: Identifying Benign Data that Breaks Safety](https://arxiv.org/abs/2404.01099) | 通过双向锚定方法，识别那些在微调后更可能降低模型安全性的良性数据子集，提高模型对有害请求的响应率。 |
| [^21] | [Texture-Preserving Diffusion Models for High-Fidelity Virtual Try-On](https://arxiv.org/abs/2404.01089) | 我们提出了一种Texture-Preserving Diffusion（TPD）模型，通过在空间维度连接遮罩人物和参考服装图像，增强了虚拟试穿的保真度，同时不引入额外的图像编码器。 |
| [^22] | [AILS-NTUA at SemEval-2024 Task 9: Cracking Brain Teasers: Transformer Models for Lateral Thinking Puzzles](https://arxiv.org/abs/2404.01084) | 本研究通过调整多种预训练变压器模型，在SemEval-2024任务9的脑筋急转弯竞赛中取得了竞争力十足的表现，在句子谜题和单词谜题的评估中，最佳提交的准确率分别超过了最佳神经基线ChatGPT超过20%和30%。 |
| [^23] | [Advancing AI with Integrity: Ethical Challenges and Solutions in Neural Machine Translation](https://arxiv.org/abs/2404.01070) | 本文研究了神经机器翻译中的伦理挑战，并通过实证研究和文献综述确定和解决了数据处理、隐私、数据所有权等方面的伦理问题，为确保AI模型的公平性和文化敏感性提供了解决方案。 |
| [^24] | [Regularized Best-of-N Sampling to Mitigate Reward Hacking for Language Model Alignment](https://arxiv.org/abs/2404.01054) | 提出了Regularized Best-of-N (RBoN)，通过引入接近性项来减轻奖励欺骗，提高了算法在解码时与人类偏好对齐的效果。 |
| [^25] | [Can LLMs get help from other LLMs without revealing private information?](https://arxiv.org/abs/2404.01041) | 本研究展示了在级联系统中运用隐私保护技术的可行性，以减少在查询远程模型时泄漏私人信息的风险，并引入了两个隐私度量。 |
| [^26] | [Higher education assessment practice in the era of generative AI tools](https://arxiv.org/abs/2404.01036) | 该研究评估了生成人工智能工具对高等教育评估实践的影响，发现这些工具展示了学科知识、问题解决、分析能力等技能，但在不道德使用时可能限制学习，同时也揭示了部分学科评估中这些工具的局限性。 |
| [^27] | [Survey of Bias In Text-to-Image Generation: Definition, Evaluation, and Mitigation](https://arxiv.org/abs/2404.01030) | 该调查综述了文本到图像生成中的偏见问题，重点讨论了性别、肤色和地域文化这些方面，旨在帮助理解当前进展和研究空白。 |
| [^28] | [Source-Aware Training Enables Knowledge Attribution in Language Models](https://arxiv.org/abs/2404.01019) | 源感知训练使语言模型具备知识归因能力，进而增强了其透明度、可解释性和可验证性。 |
| [^29] | [Teeth-SEG: An Efficient Instance Segmentation Framework for Orthodontic Treatment based on Anthropic Prior Knowledge](https://arxiv.org/abs/2404.01013) | 提出了一种基于ViT的TeethSEG框架，包括多尺度聚合模块和人类先验知识层，以解决牙齿形状差异、位置变化和异常问题。 |
| [^30] | [Query Performance Prediction using Relevance Judgments Generated by Large Language Models](https://arxiv.org/abs/2404.01012) | 提出了一种使用自动生成的相关性判断的查询性能预测框架，能够解决先前方法中对不同IR评估指标准确性和解释性的限制。 |
| [^31] | [LLM-RadJudge: Achieving Radiologist-Level Evaluation for X-Ray Report Generation](https://arxiv.org/abs/2404.00998) | 该研究提出了一个使用大型语言模型进行放射学报告评估的新框架，通过GPT-4实现了与放射科医师评估一致性接近的效果，同时利用知识蒸馏训练的较小模型也达到了类似的评估能力。 |
| [^32] | [360+x: A Panoptic Multi-modal Scene Understanding Dataset](https://arxiv.org/abs/2404.00989) | 该数据集是第一个融合多个视角和多个数据模态以模拟现实世界中日常信息获取方式的数据库。 |
| [^33] | [Continual Learning for Smart City: A Survey](https://arxiv.org/abs/2404.00983) | 该调研综合审查了智慧城市发展中广泛使用的持续学习方法，内容涵盖了方法论分类、多种应用领域和相关数据集。 |
| [^34] | [Nonlinear Impulse Pattern Formulation dynamical social and political prediction algorithm for city planning and public participation](https://arxiv.org/abs/2404.00977) | 提出了一种非线性动力学算法IPF，用于城市规划，能够预测不同社会或政治利益相关参数在规划过程中的发展，已在音乐乐器模拟、脑动力学和人际互动中表现出高预测精度。 |
| [^35] | [Exploring and Evaluating Hallucinations in LLM-Powered Code Generation](https://arxiv.org/abs/2404.00971) | 本研究通过主题分析对LLM生成的代码中的幻觉进行了总结和分类，建立了代码中幻觉的全面分类法。 |
| [^36] | [Evalverse: Unified and Accessible Library for Large Language Model Evaluation](https://arxiv.org/abs/2404.00943) | Evalverse是一个统一和易用的库，简化了大型语言模型（LLMs）的评估，为研究人员和从业者提供了集中且易于访问的评估框架。 |
| [^37] | [Evaluating the Factuality of Large Language Models using Large-Scale Knowledge Graphs](https://arxiv.org/abs/2404.00942) | 使用大型知识图谱创建评估模型以检查大型语言模型在事实性上的表现，有效降低评估成本。 |
| [^38] | [A Survey on Multilingual Large Language Models: Corpora, Alignment, and Bias](https://arxiv.org/abs/2404.00929) | 该论文对多语言大型语言模型进行了全面分析，深入讨论了关键问题，包括多语言语料库、对齐和偏见。 |
| [^39] | [MM3DGS SLAM: Multi-modal 3D Gaussian Splatting for SLAM Using Vision, Depth, and Inertial Measurements](https://arxiv.org/abs/2404.00923) | 使用多模态3D高斯雨滴法SLAM可以结合未定位相机图像和惯性测量，实现准确的地图建立和轨迹跟踪，具有更快的渲染速度、尺度感知性和改进的轨迹追踪能力。 |
| [^40] | [Token-Efficient Leverage Learning in Large Language Models](https://arxiv.org/abs/2404.00914) | 介绍了一种名为Token-Efficient Leverage Learning（TELL）的方法，在大型语言模型中展示了其降低任务数据需求、提高任务性能的潜力，为低资源任务带来了竞争性能。 |
| [^41] | [LLaMA-Excitor: General Instruction Tuning via Indirect Feature Interaction](https://arxiv.org/abs/2404.00913) | LLaMA-Excitor是一种轻量级方法，通过逐渐更多地关注有价值的信息来激发LLM更好地遵循指令，而不直接改变中间隐藏状态，有效保留预训练知识。 |
| [^42] | [Maximizing User Experience with LLMOps-Driven Personalized Recommendation Systems](https://arxiv.org/abs/2404.00903) | 将LLMOps集成到个性化推荐系统中，提升了大规模机器学习模型的效率和可靠性，推动生成与用户偏好一致的个性化推荐，为未来个性化推荐系统的发展带来重要影响。 |
| [^43] | [Machine Learning Robustness: A Primer](https://arxiv.org/abs/2404.00897) | 该章节探讨了机器学习中稳健性的重要概念及关键的技术和因素，以确立人工智能系统的可信度。 |
| [^44] | [MTLight: Efficient Multi-Task Reinforcement Learning for Traffic Signal Control](https://arxiv.org/abs/2404.00886) | MTLight提出了一种用于交通信号控制的高效多任务强化学习算法，通过学习潜在状态和构建多个辅助任务来提高代理的性能。 |
| [^45] | [Self-Demos: Eliciting Out-of-Demonstration Generalizability in Large Language Models](https://arxiv.org/abs/2404.00884) | 提出了自我演示(Self-Demos)方法，在大型语言模型中通过生成查询感知的示范，引出固有的泛化能力，并构建了用于评估该方法有效性的数据集OOD-Toolset。 |
| [^46] | [Bailong: Bilingual Transfer Learning based on QLoRA and Zip-tie Embedding](https://arxiv.org/abs/2404.00862) | 通过结合参数高效微调和先进的嵌入初始化技术，本研究在以英语为主导的开源LLMs上实现了跨语言迁移学习。 |
| [^47] | [Removing Speaker Information from Speech Representation using Variable-Length Soft Pooling](https://arxiv.org/abs/2404.00856) | 该论文旨在利用神经网络预测语音中离散单元边界，实现可变长度池化，从而去除表示中的说话者信息。 |
| [^48] | [TSOM: Small Object Motion Detection Neural Network Inspired by Avian Visual Circuit](https://arxiv.org/abs/2404.00855) | 本论文基于鸟类视觉系统，提出了一种新型的神经网络TSOM，用于小物体运动检测算法。 |
| [^49] | [HeteroMILE: a Multi-Level Graph Representation Learning Framework for Heterogeneous Graphs](https://arxiv.org/abs/2404.00816) | HeteroMILE提出了一种多级嵌入框架，可以将当代图嵌入方法扩展到大型异构图，并通过反复粗化和细化的方式有效降低计算成本。 |
| [^50] | [Towards Realistic Scene Generation with LiDAR Diffusion Models](https://arxiv.org/abs/2404.00815) | 本文提出了LiDAR扩散模型（LiDMs），能够生成具有LiDAR逼真效果的场景，通过引入几何先验，实现了模式逼真、几何逼真和物体逼真。 |
| [^51] | [Algorithmic Collusion by Large Language Models](https://arxiv.org/abs/2404.00806) | 大型语言模型的算法定价代理在寡头市场环境中自主勾结，对消费者利益有害，其说明书中的短语变化可能增加勾结。 |
| [^52] | [Addressing Loss of Plasticity and Catastrophic Forgetting in Continual Learning](https://arxiv.org/abs/2404.00781) | 本文提出了一种新方法，即基于效用的扰动梯度下降（UPGD），通过在梯度更新中应用不同大小的扰动，保护有用单元以防遗忘，同时恢复不太有用单元的可塑性。 |
| [^53] | [Privacy-preserving Optics for Enhancing Protection in Face De-identification](https://arxiv.org/abs/2404.00777) | 本文提出了一种硬件级人脸去标识化方法，通过学习光学编码器和回归模型生成人脸热图，同时隐藏人脸身份，解决了人脸去标识化软件存在的漏洞。 |
| [^54] | [Recover: A Neuro-Symbolic Framework for Failure Detection and Recovery](https://arxiv.org/abs/2404.00756) | 该论文提出了一种神经符号框架，名为Recover，用于在线故障识别和恢复，通过集成符号信息来提升大型语言模型生成恢复计划的能力，降低相关成本。 |
| [^55] | [On the True Distribution Approximation of Minimum Bayes-Risk Decoding](https://arxiv.org/abs/2404.00752) | 本研究提出使用异常检测来衡量最小贝叶斯风险解码中样本接近真实分布的程度，并验证实验结果首次支持了性能与采样近似度之间的联系。 |
| [^56] | [Benchmark Transparency: Measuring the Impact of Data on Evaluation](https://arxiv.org/abs/2404.00748) | 本研究提出了一种自动化框架，可以衡量数据分布对自然语言处理模型性能和评估的影响，揭示了数据分布的重要性和对评估框架的影响。 |
| [^57] | [Mining Weighted Sequential Patterns in Incremental Uncertain Databases](https://arxiv.org/abs/2404.00746) | 在增量不确定数据库中挖掘加权序列模式时，需要处理权重约束，提出了增量挖掘算法，使得挖掘重要信息更加高效。 |
| [^58] | [The Larger the Better? Improved LLM Code-Generation via Budget Reallocation](https://arxiv.org/abs/2404.00725) | 较小的语言模型可以在相同预算下产生可靠的改进，但在无法进行单元测试的情况下，较小的模型选择排名次于较大模型的单个输出。 |
| [^59] | [DRCT: Saving Image Super-resolution away from Information Bottleneck](https://arxiv.org/abs/2404.00722) | 基于Vision Transformer的DRCT方法采用创新的机制解决了图像超分辨率中空间信息衰减的问题，提升了模型性能。 |
| [^60] | [Survey of Computerized Adaptive Testing: A Machine Learning Perspective](https://arxiv.org/abs/2404.00712) | 本文以机器学习视角综述了计算机自适应测试（CAT），重点解析其测试问题选择算法和如何优化认知诊断模型、题库构建和测试控制。 |
| [^61] | [Scaling Properties of Speech Language Models](https://arxiv.org/abs/2404.00685) | 通过研究语音语言模型的尺度特性，可以预测其语言性能的扩展速度，与文本-based大型语言模型相比，语音语言模型的语言性能扩展速度慢三个数量级。 |
| [^62] | [Generative Retrieval as Multi-Vector Dense Retrieval](https://arxiv.org/abs/2404.00684) | 本文填补了这一空白，通过展示生成式检索和多向量密集检索共享相同的框架，用于衡量文档与查询相关性。 |
| [^63] | [LLM meets Vision-Language Models for Zero-Shot One-Class Classification](https://arxiv.org/abs/2404.00675) | 提出了一种两步解决方案，结合大型语言模型和视觉-语言预训练模型，用于零样本单类分类问题，并在实际基准测试中表现出优越性能。 |
| [^64] | [A Survey of Privacy-Preserving Model Explanations: Privacy Risks, Attacks, and Countermeasures](https://arxiv.org/abs/2404.00673) | 本研究是第一个全面调查模型解释中隐私攻击及其对抗措施的论文，通过分类隐私攻击和对抗措施，初步探讨了隐私泄漏原因，提出未解决问题和未来研究方向。 |
| [^65] | [A General and Efficient Training for Transformer via Token Expansion](https://arxiv.org/abs/2404.00672) | 本文提出了一种名为ToE的令牌生长方案，旨在通过加速一致性训练来改善ViT的训练效果。 |
| [^66] | [Observations on Building RAG Systems for Technical Documents](https://arxiv.org/abs/2404.00657) | 研究回顾了对技术文档RAG系统构建的重要因素，并通过实验证明了最佳实践和潜在挑战 |
| [^67] | [WavLLM: Towards Robust and Adaptive Speech Large Language Model](https://arxiv.org/abs/2404.00656) | WavLLM是一个稳健和自适应语音大语言模型，引入了双编码器和Prompt-aware LoRA权重适配器，通过两阶段课程学习方法优化，解耦不同类型的语音信息，为处理语义内容和说话者身份的独特特征提供了新思路 |
| [^68] | [Learning Off-policy with Model-based Intrinsic Motivation For Active Online Exploration](https://arxiv.org/abs/2404.00651) | 本文提出了一种结合预测模型和离线学习元素的强化学习算法，通过内在奖励与模型不确定性的关联，在连续控制任务中实现了样本有效的探索。 |
| [^69] | [Learning to Generate Conditional Tri-plane for 3D-aware Expression Controllable Portrait Animation](https://arxiv.org/abs/2404.00636) | 本文提出了一种一次性的3D感知肖像动画方法Export3D，通过引入三平面生成器和对比预训练框架，实现了控制给定肖像图像的面部表情和摄像机视角，提供了一种新的表达方式。 |
| [^70] | [Learning to Plan for Language Modeling from Unlabeled Data](https://arxiv.org/abs/2404.00614) | 通过自监督学习目标训练一个用于规划未来写作过程的模块，扩展了成功的语言模型公式到更抽象的规划中，改善了语言建模的性能，特别是在文本结构方面，同时新的规划模块可以大规模训练并轻松与社区共享。 |
| [^71] | [Extensive Self-Contrast Enables Feedback-Free Language Model Alignment](https://arxiv.org/abs/2404.00604) | 本论文介绍了一种利用广泛自对比生成负例的无需反馈的大型语言模型对齐方法，该方法在实验中表现优于直接偏好优化方法。 |
| [^72] | [AI Act and Large Language Models (LLMs): When critical issues and privacy impact require human and ethical oversight](https://arxiv.org/abs/2404.00600) | 论文讨论了人类监督、道德监督和隐私影响评估在面对人工智能系统和大型语言模型发展中的重要性。 |
| [^73] | [EvoCodeBench: An Evolving Code Generation Benchmark Aligned with Real-World Code Repositories](https://arxiv.org/abs/2404.00599) | EvoCodeBench 是一个与现实世界代码库对齐的进化代码生成基准，具有完善的注释和评估度量标准，同时避免数据泄露。 |
| [^74] | [LAESI: Leaf Area Estimation with Synthetic Imagery](https://arxiv.org/abs/2404.00593) | LAESI数据集包含10万张合成叶片图像，用于叶形态分析，机器学习模型训练后可预测叶片表面积，并提供了基于3D程序模型和生成式AI的高效框架。 |
| [^75] | [Memory-based Cross-modal Semantic Alignment Network for Radiology Report Generation](https://arxiv.org/abs/2404.00588) | 提出了一种基于记忆的交叉模态语义对齐网络用于减少工作量并生成准确放射学报告 |
| [^76] | [RLGNet: Repeating-Local-Global History Network for Temporal Knowledge Graph Reasoning](https://arxiv.org/abs/2404.00586) | RLGNet提出了重复-局部-全局历史网络，利用全局历史编码器和局部历史编码器分别捕捉历史信息的整体和相关细节，以有效进行时间知识图推理。 |
| [^77] | [A Review of Modern Recommender Systems Using Generative Models (Gen-RecSys)](https://arxiv.org/abs/2404.00579) | 生成模型在现代推荐系统中展现出了强大的潜力，能够同时处理用户-项目交互历史、文本、图像和视频等复杂数据，为推荐系统带来了新的可能性。 |
| [^78] | [Automated Bi-Fold Weighted Ensemble Algorithms and its Application to Brain Tumor Detection and Classification](https://arxiv.org/abs/2404.00576) | 提出了两种新型的自动双向加权集成算法，用于提高脑肿瘤检测和分类的效果 |
| [^79] | [A Theory for Length Generalization in Learning to Reason](https://arxiv.org/abs/2404.00560) | 该论文提出了针对学习推理中长度泛化问题的理论研究，设计了基于DAGs的问题表示，并使用Transformer来实现完美的长度泛化。 |
| [^80] | [Deep Extrinsic Manifold Representation for Vision Tasks](https://arxiv.org/abs/2404.00544) | 提出了一种名为深度外在流形表示（DEMR）的技巧，将外在流形嵌入融入到深度神经网络中，帮助生成流形表示，并通过实证证据支持其在 $SE(3)$ 等流形上的可行性、渐近性质和泛化能力。 |
| [^81] | [Embodied Active Defense: Leveraging Recurrent Feedback to Counter Adversarial Patches](https://arxiv.org/abs/2404.00540) | 利用人类主动感知和循环反馈机制，提出了具身主动防御（EAD）策略，在3D实际环境中主动上下文化环境信息以对抗对抗性贴片。 |
| [^82] | [Comparing Bad Apples to Good Oranges: Aligning Large Language Models via Joint Preference Optimization](https://arxiv.org/abs/2404.00530) | 本研究提出了一种新的偏好获取方法，通过DOVE协议对指令-响应对的联合概率进行优化，以对齐大型语言模型。 |
| [^83] | [The Emotional Impact of Game Duration: A Framework for Understanding Player Emotions in Extended Gameplay Sessions](https://arxiv.org/abs/2404.00526) | 该研究探讨了游戏持续时间对玩家情绪的影响，建立了情感检测框架，并实验结果表明长时间游戏会显著影响玩家的情绪。 |
| [^84] | [Transfer Learning with Reconstruction Loss](https://arxiv.org/abs/2404.00505) | 本文通过引入额外的重建阶段和重建损失，提出了一种具有共享模型参数和特征表示的模型训练方法，建立了共同信息的概念，用于解决相关任务。 |
| [^85] | [Configurable Safety Tuning of Language Models with Synthetic Preference Data](https://arxiv.org/abs/2404.00495) | 提出了一种名为Configurable Safety Tuning（CST）的新方法，通过使用合成偏好数据，在推断时实现对LLMs的灵活安全配置，允许用户根据需要禁用/启用安全偏好，且实验表明CST成功管理不同的安全配置并保留了原始功能，是一种适用于可配置部署的强大方法。 |
| [^86] | [Multi-hop Question Answering under Temporal Knowledge Editing](https://arxiv.org/abs/2404.00492) | 提出了一个新颖的框架TEMPLE-MQA，通过构建时间感知图和推理路径、结构检索和联合推理阶段，有效地识别多跳问题回答中的时间背景，在基准数据集上显著优于基线模型，并贡献了一个新数据集TKEMQA。 |
| [^87] | [PROMPT-SAW: Leveraging Relation-Aware Graphs for Textual Prompt Compression](https://arxiv.org/abs/2404.00489) | 提出了PROMPT-SAW模型，利用关系感知图来实现文本提示的压缩，提高了提示的可读性和可解释性。 |
| [^88] | [Noise-Aware Training of Layout-Aware Language Models](https://arxiv.org/abs/2404.00488) | 本论文提出了一种噪声感知训练方法，可以以可扩展的方式使用弱标记文档训练提取器，从而避免了在企业场景中训练大量不同文档类型的自定义提取器所需的昂贵人工标记成本。 |
| [^89] | [Contextual AI Journaling: Integrating LLM and Time Series Behavioral Sensing Technology to Promote Self-Reflection and Well-being using the MindScape App](https://arxiv.org/abs/2404.00487) | 将时间序列行为与大型语言模型相结合，创建新形式上下文智能日志应用，促进自我反思和幸福感，代表了人工智能新的发展方向。 |
| [^90] | [Dialectical Alignment: Resolving the Tension of 3H and Security Threats of LLMs](https://arxiv.org/abs/2404.00486) | 提出了辩证对齐（DA）框架来解决LLM在面临外部有毒数据时的适应性变色龙问题，增强了其对抗外部攻击的安全性 |
| [^91] | [Cross-lingual Named Entity Corpus for Slavic Languages](https://arxiv.org/abs/2404.00482) | 介绍了一个手动标注的用于六种斯拉夫语言的命名实体语料库，提供了两个训练数据集划分，并使用预训练的多语言模型进行命名实体的识别、分类、引用词化和链接。 |
| [^92] | [Linguistic Calibration of Language Models](https://arxiv.org/abs/2404.00474) | 该论文提出了一种通过语言模型的文本生成来实现语言校准，可以使用户做出校准概率预测的方法。 |
| [^93] | [Shortcuts Arising from Contrast: Effective and Covert Clean-Label Attacks in Prompt-Based Learning](https://arxiv.org/abs/2404.00461) | 基于提示的学习中的干净标签攻击通过特定提示作为触发器，实现成功的同时确保正确标记有毒样本，但仍面临误激活问题和挑战，需要更高比例的毒害。 |
| [^94] | [Interactive Multi-Robot Flocking with Gesture Responsiveness and Musical Accompaniment](https://arxiv.org/abs/2404.00442) | 提出了一个互动多机器人集群任务，目的是通过手势和声音引导人类参与机器人群体，并创造了新颖的群体导航和手势响应算法 |
| [^95] | [Communication Efficient Distributed Training with Distributed Lion](https://arxiv.org/abs/2404.00438) | 分布式狮子是对 Lion 进行了创新性改进，利用符号操作符降低了通信成本，在分布式训练中取得了与标准 Lion 或 AdamW 优化器相当的性能，并显著减少了通信带宽。 |
| [^96] | [Automatic explanation of the classification of Spanish legal judgments in jurisdiction-dependent law categories with tree estimators](https://arxiv.org/abs/2404.00437) | 本研究结合自然语言处理和机器学习，提出了一种可解释的法律文本分类系统，使模型的决策变得可理解给最终用户 |
| [^97] | [From attention to profit: quantitative trading strategy based on transformer](https://arxiv.org/abs/2404.00424) | 该研究介绍了一种基于Transformer的量化交易策略，利用改进的模型架构和情感分析的迁移学习，不仅在捕捉长期依赖关系和建模数据关系方面具有优势，而且能够准确预测未来一段时间内的回报。 |
| [^98] | [Orchestrate Latent Expertise: Advancing Online Continual Learning with Multi-Level Supervision and Reverse Self-Distillation](https://arxiv.org/abs/2404.00417) | 引入了Multi-level Online Sequential Experts (MOSE)方法，通过多层监督和反向自蒸馏，解决了在线持续学习中新旧训练样本学习不足和重复学习的问题。 |
| [^99] | [Language Models are Spacecraft Operators](https://arxiv.org/abs/2404.00413) | 该论文旨在将大型语言模型(LLMs)应用于空间导航和控制领域，通过开发纯LLM解决方案，并在 Kerbal 太空计划差分游戏挑战中取得第二名，首次将LLM代理集成到空间资源中。 |
| [^100] | [TACO -- Twitter Arguments from COnversations](https://arxiv.org/abs/2404.00406) | TACO是第一个利用1,814条推文构建的Twitter Arguments数据集，涵盖200场完整对话，六个主题，具有0.718的Krippendorff's alpha一致性，并提供了一个注释框架来定义和识别论点结构要素。 |
| [^101] | [Aurora-M: The First Open Source Multilingual Language Model Red-teamed according to the U.S. Executive Order](https://arxiv.org/abs/2404.00399) | Aurora-M 是第一个根据美国行政命令进行红队测试的开源多语言模型，通过在英语、芬兰语、印地语、日语、越南语和代码上训练，不断预训练，包括了人工审核的安全说明，总训练 token 数超过 2 万亿个 |
| [^102] | [Constrained Layout Generation with Factor Graphs](https://arxiv.org/abs/2404.00385) | 本文提出了一种基于因子图的方法，用于面向对象的受约束布局生成，可以准确捕捉复杂交互关系，填补了现有方法的不足。 |
| [^103] | [SpikingJET: Enhancing Fault Injection for Fully and Convolutional Spiking Neural Networks](https://arxiv.org/abs/2404.00383) | SpikingJET是一种设计用于全连接和卷积脉冲神经网络的新型故障注入器，通过在关键组件中引入错误和注入故障，为评估SNNs的韧性提供了全面平台。 |
| [^104] | [Worker Robot Cooperation and Integration into the Manufacturing Workcell via the Holonic Control Architecture](https://arxiv.org/abs/2404.00369) | 本文提出通过Holonic控制架构实现工人机器人之间的协作与集成，以实现新的智能制造技术。 |
| [^105] | [Accurate Cutting-point Estimation for Robotic Lychee Harvesting through Geometry-aware Learning](https://arxiv.org/abs/2404.00364) | 提出了一个 Fcaf3d-lychee 网络模型，通过与挤压-激发（SE）模块改进特征提取，实现了对荔枝采摘点的准确定位 |
| [^106] | [Can LLMs Master Math? Investigating Large Language Models on Math Stack Exchange](https://arxiv.org/abs/2404.00344) | LLMs在数学领域表现出色，其中GPT-4在Math Stack Exchange上回答数学问题的表现最佳。 |
| [^107] | [Ontology in Holonic Cooperative Manufacturing: A Solution to Share and Exchange the Knowledge](https://arxiv.org/abs/2404.00341) | 提出了一种使用本体论概念表示合作制造知识的整体控制解决方案，并在合作装配场景中进行了实施。 |
| [^108] | [Memory-Scalable and Simplified Functional Map Learning](https://arxiv.org/abs/2404.00330) | 提出了一种新颖的存储可扩展且高效的功能地图学习管道，该方法可以在不将逐点地图存储在内存中的情况下实现相同结果，并且引入了一种不同iable的地图细化层。 |
| [^109] | [Advancing Multimodal Data Fusion in Pain Recognition: A Strategy Leveraging Statistical Correlation and Human-Centered Perspectives](https://arxiv.org/abs/2404.00320) | 通过结合统计相关性和以人为中心的方法，本研究提出了一种新的方法，改善了多模态数据融合在疼痛识别中的性能，突出了数据多样性和定制化的模态分割对疼痛行为分析的重要性。 |
| [^110] | [Bayesian Exploration of Pre-trained Models for Low-shot Image Classification](https://arxiv.org/abs/2404.00312) | 提出了基于高斯过程的概率模型集成框架，能有效整合不同于CLIP的先验知识，实现了低样本图像分类中的分析推断、不确定性量化和超参数调整。 |
| [^111] | [Leveraging Intelligent Recommender system as a first step resilience measure -- A data-driven supply chain disruption response framework](https://arxiv.org/abs/2404.00306) | 提出了一种基于智能推荐系统技术的数据驱动供应链紧急响应框架，能够作为供应链中断的有效措施，并帮助参与者在危机发生后获得更好的反应表现。 |
| [^112] | [Long-Tailed Recognition on Binary Networks by Calibrating A Pre-trained Model](https://arxiv.org/abs/2404.00285) | 该论文提出了一个校准和蒸馏框架，利用预先训练的全精度模型作为教师，在学习长尾数据集上的二值网络时进行蒸馏，通过对目标函数中项的敌对平衡和多分辨率学习，显著优于以往技术 |
| [^113] | [Survey on Large Language Model-Enhanced Reinforcement Learning: Concept, Taxonomy, and Methods](https://arxiv.org/abs/2404.00282) | 大型语言模型在强化学习中具有潜在优势，通过结构化分类和角色分析，为未来研究提供指导。 |
| [^114] | [Instruction-Driven Game Engines on Large Language Models](https://arxiv.org/abs/2404.00276) | 通过在大型语言模型上开发指令驱动游戏引擎，使用户可以通过简单的自然语言指令创建游戏，从而降低游戏开发的难度。 |
| [^115] | [TG-NAS: Leveraging Zero-Cost Proxies with Transformer and Graph Convolution Networks for Efficient Neural Architecture Search](https://arxiv.org/abs/2404.00271) | TG-NAS提出了一种新型模型通用代理，利用Transformer的运算符嵌入生成器和图卷积网络来预测架构性能，指导神经结构搜索。 |
| [^116] | [A Simple Yet Effective Approach for Diversified Session-Based Recommendation](https://arxiv.org/abs/2404.00261) | 提出了一种简单而有效的多样化类别感知关注SBRS(DCA-SBRS)框架，可以作为插件用于辅助现有的SBRSs在保持推荐准确度的同时生成更多样化的列表 |
| [^117] | [YOLOOC: YOLO-based Open-Class Incremental Object Detection with Novel Class Discovery](https://arxiv.org/abs/2404.00257) | 该论文提出了基于YOLO架构的YOLOOC检测器，针对开放类别设置引入了标签平滑，有效应对新类别检测和增量学习的挑战。 |
| [^118] | [Facilitating Reinforcement Learning for Process Control Using Transfer Learning: Perspectives](https://arxiv.org/abs/2404.00247) | 本文从迁移学习的角度探讨了如何将其与强化学习相结合，为过程控制带来新的可能性。 |
| [^119] | [Your Co-Workers Matter: Evaluating Collaborative Capabilities of Language Models in Blocks World](https://arxiv.org/abs/2404.00246) | 在一个方块世界环境中，论文评估了大型语言模型的协作能力，通过设计不断增加挑战性的设置来评估不同的协作视角，从独立到更复杂的依赖任务。 |
| [^120] | [DeFT: Flash Tree-attention with IO-Awareness for Efficient Tree-search-based LLM Inference](https://arxiv.org/abs/2404.00242) | DeFT提出了一种带IO意识的树注意力算法，通过在QKV准备和注意力计算阶段实现内存高效的计算，降低内存印记，以解决当前树解码策略和推断系统不适配的问题。 |
| [^121] | [Attention-based Shape-Deformation Networks for Artifact-Free Geometry Reconstruction of Lumbar Spine from MR Images](https://arxiv.org/abs/2404.00231) | 这项工作提出了TransDeformer，使用注意力机制实现了对腰椎轮廓的高空间准确性重建，并跨患者实现了网格对应，为医学参数测量提供了可靠性，还设计了变体用于错误估计。 |
| [^122] | [InfLoRA: Interference-Free Low-Rank Adaptation for Continual Learning](https://arxiv.org/abs/2404.00228) | InfLoRA提出了一种新的PEFT方法，名为无干扰低秩自适应（InfLoRA），用于持续学习，旨在消除新任务对旧任务的干扰，帮助模型在稳定性和可塑性之间取得良好平衡。 |
| [^123] | [Is Factuality Decoding a Free Lunch for LLMs? Evaluation on Knowledge Editing Benchmark](https://arxiv.org/abs/2404.00216) | 大型语言模型通过事实解码方法提高了事实准确性，然而，这些方法使模型对已知事实过于自信，进一步评估显示在知识编辑基准上所有解码方法均显著降低了模型性能。 |
| [^124] | [Causal Inference for Human-Language Model Collaboration](https://arxiv.org/abs/2404.00207) | 本文研究了人类和语言模型之间的协作动态，并提出了一个新的因果估计Incremental Stylistic Effect来解释如何改变合作结果。 |
| [^125] | [Multiple-policy Evaluation via Density Estimation](https://arxiv.org/abs/2404.00195) | 该研究提出一种名为 $\mathrm{CAESAR}$ 的算法，通过计算一个近似的最优离线采样分布，同时估计多个策略的价值，以解决多策略评估问题。 |
| [^126] | [DataAgent: Evaluating Large Language Models' Ability to Answer Zero-Shot, Natural Language Queries](https://arxiv.org/abs/2404.00188) | 评估了 OpenAI 的 GPT-3.5 模型作为“语言数据科学家”，成功回答了与基准数据集相关的数据科学查询。 |
| [^127] | [On Inherent Adversarial Robustness of Active Vision Systems](https://arxiv.org/abs/2404.00185) | 主动视觉机制的集成可以提供当前深度学习系统的鲁棒性优势，并且通过实验证明了GFNet和FALcon这两种主动视觉方法在黑盒威胁模型下的固有鲁棒性。 |
| [^128] | [Beyond Suspension: A Two-phase Methodology for Concluding Sports Leagues](https://arxiv.org/abs/2404.00178) | 该研究提出了一种利用预测性和处方性分析的数据驱动模型，为结束现有体育赛季提供了一种新的两阶段方法，以产生类似完整赛季结果的队伍排名。 |
| [^129] | [Universal Bovine Identification via Depth Data and Deep Metric Learning](https://arxiv.org/abs/2404.00172) | 该论文提出了一种利用深度数据和深度度量学习进行通用牛标识的方法，可以在不需要物种特定的外套图案或特写口吻印记的情况下，通过CNN和MLP基础实现学习到良好泛化嵌入空间，从体形区分个体。 |
| [^130] | [Uncovering Bias in Large Vision-Language Models with Counterfactuals](https://arxiv.org/abs/2404.00166) | 通过对输入图像进行反事实变化，我们对不同大规模视觉-语言模型生成的文本进行了研究，以揭示其中的社会偏见。 |
| [^131] | [Accelerating Search-Based Planning for Multi-Robot Manipulation by Leveraging Online-Generated Experiences](https://arxiv.org/abs/2404.00143) | 提出了一种加速基于冲突搜索算法的方法，通过利用其重复性和增量性质，使其适用于涉及多臂协调的复杂场景 |
| [^132] | [Does Faithfulness Conflict with Plausibility? An Empirical Study in Explainable AI across NLP Tasks](https://arxiv.org/abs/2404.00140) | 传统的扰动方法Shapley值和LIME可实现更高的信实性和可信度，建议优化可解释性算法以实现高效的双重目标 |
| [^133] | [Security Risks Concerns of Generative AI in the IoT](https://arxiv.org/abs/2404.00139) | 生成式人工智能在物联网中的集成带来了安全风险，本文分析了数据泄露和技术滥用等风险，并讨论了缓解这些风险的战略方法。 |
| [^134] | [Budget-aware Query Tuning: An AutoML Perspective](https://arxiv.org/abs/2404.00137) | 将传统成本单位视为变量，通过调整值可以获得比默认查询计划更优的查询计划 |
| [^135] | [Robust Ensemble Person Re-Identification via Orthogonal Fusion with Occlusion Handling](https://arxiv.org/abs/2404.00107) | 提出了一种通过正交融合处理遮挡的强健集成人员识别模型，无需手动标记遮挡区域，利用CNN和Transformer架构生成鲁棒特征表示。 |
| [^136] | [Efficient and Sharp Off-Policy Evaluation in Robust Markov Decision Processes](https://arxiv.org/abs/2404.00099) | 在对抗性环境中，本论文提出了一种可修改转移核密度的扰动模型，拓展了传统的边缘敏感性模型，对无限时间RL中策略价值进行了尖锐边界的刻画和估计。 |
| [^137] | [Molecular Generative Adversarial Network with Multi-Property Optimization](https://arxiv.org/abs/2404.00081) | 该研究引入了一种新型的基于演员-评论家强化学习的GAN，即InstGAN，以在令牌级别上生成具有多属性优化的分子，并利用最大化信息熵来缓解模式崩溃。 |
| [^138] | [A Backdoor Approach with Inverted Labels Using Dirty Label-Flipping Attacks](https://arxiv.org/abs/2404.00076) | 提出了一种后门攻击方法，名为“DirtyFlipping”，利用脏标签技术在选定的数据模式中输入触发器，从而实现隐蔽的后门。 |
| [^139] | [Temporal Graph Networks for Graph Anomaly Detection in Financial Networks](https://arxiv.org/abs/2404.00060) | 时间图网络（TGN）在金融网络中的异常检测中表现出显著优势，适应了现代金融系统动态和复杂的特性。 |
| [^140] | [PerOS: Personalized Self-Adapting Operating Systems in the Cloud](https://arxiv.org/abs/2404.00057) | PerOS是个性化自适应操作系统，解决了传统OS在情报和用户体验方面的不足，尤其适应了机器学习和大型语言模型的发展。 |
| [^141] | [Deja vu: Contrastive Historical Modeling with Prefix-tuning for Temporal Knowledge Graph Reasoning](https://arxiv.org/abs/2404.00051) | 提出了ChapTER，一个对比历史建模框架，通过前缀调整实现文本与时间的平衡，用于时间知识图推理。 |
| [^142] | [Policy Optimization finds Nash Equilibrium in Regularized General-Sum LQ Games](https://arxiv.org/abs/2404.00045) | 引入相对熵正则化对一般和总 $N$-agent 游戏的纳什均衡产生影响，证明了NE符合线性高斯策略，并提出了政策优化算法以及增强技术来找到游戏内的NE。 |
| [^143] | [UAlign: Pushing the Limit of Template-free Retrosynthesis Prediction with Unsupervised SMILES Alignment](https://arxiv.org/abs/2404.00044) | 本文提出了UAlign，一种无模板化的图到序列的逆合成预测方法，通过结合图神经网络和Transformer，利用分子的固有图结构，并引入一种简单有效的SMILES对齐技术来促进未改变结构的复用。 |
| [^144] | [Stochastic Optimization with Constraints: A Non-asymptotic Instance-Dependent Analysis](https://arxiv.org/abs/2404.00042) | 该论文研究了具有凸约束的随机凸优化问题，提出了一种非渐近保证的VRPG算法，并展示了其性能受到解以及带凸约束解决的问题的缩放距离控制。 |
| [^145] | [MicroHD: An Accuracy-Driven Optimization of Hyperdimensional Computing Algorithms for TinyML systems](https://arxiv.org/abs/2404.00039) | 提出了MicroHD，一个新颖的面向TinyML系统的精度驱动超高维计算优化方法 |
| [^146] | [Complementarity in Human-AI Collaboration: Concept, Sources, and Evidence](https://arxiv.org/abs/2404.00029) | 本研究建立了全面的理论基础，概念化互补性，并确定了解释人工智能与人类协作中互补团队绩效（CTP）的来源。 |
| [^147] | [LLMs as Writing Assistants: Exploring Perspectives on Sense of Ownership and Reasoning](https://arxiv.org/abs/2404.00027) | 探讨使用大型语言模型作为写作助手引发的写作所有权感和作者身份认知之间的心理困境。 |
| [^148] | [Ink and Individuality: Crafting a Personalised Narrative in the Age of LLMs](https://arxiv.org/abs/2404.00026) | 研究探讨了人们日益依赖的基于LLM的写作助手对创造力和个性可能造成的负面影响，旨在改进人机交互系统和提升写作助手的个性化和个性化功能。 |
| [^149] | [Analysing and Organising Human Communications for AI Fairness-Related Decisions: Use Cases from the Public Sector](https://arxiv.org/abs/2404.00022) | 通过调查与公共部门算法系统相关的从业者进行访谈，研究AI公平相关决策的沟通过程，并通过定性编码分析，确定支撑公平决策的关键沟通元素。 |
| [^150] | [Advancing Explainable Autonomous Vehicle Systems: A Comprehensive Review and Research Roadmap](https://arxiv.org/abs/2404.00019) | 本研究综述了现有自动驾驶车辆解释方法的不确定性，提出了一个全面的未来研究路线图，重点放在了了解交流对象和生成及时解释上。 |
| [^151] | [Can AI Outperform Human Experts in Creating Social Media Creatives?](https://arxiv.org/abs/2404.00018) | 本文评估了人工智能相对于人类专家在创意领域的表现，并提出了使用大型语言模型增强提示来生成社交媒体创意的新方法。 |
| [^152] | [Psittacines of Innovation? Assessing the True Novelty of AI Creations](https://arxiv.org/abs/2404.00017) | AI系统在生成项目标题方面展现出独特的创新能力，即使在任务复杂性增加和计算能力极限的情况下，生成的内容具有表面有效性。 |
| [^153] | [Deep Geometry Handling and Fragment-wise Molecular 3D Graph Generation](https://arxiv.org/abs/2404.00014) | 提出了深度几何处理协议，通过引入片段式生成范式解决了共同挑战，开发了一种几何可靠的分子三维图生成方法 |
| [^154] | [Missing Data Imputation With Granular Semantics and AI-driven Pipeline for Bankruptcy Prediction](https://arxiv.org/abs/2404.00013) | 本文介绍了一种具有精确语义的缺失数据插补方法，通过在粒空间中利用特征语义和可靠观测来预测缺失值，从而解决了破产预测中的重要挑战。 |
| [^155] | [Stress index strategy enhanced with financial news sentiment analysis for the equity markets](https://arxiv.org/abs/2404.00012) | 通过将金融压力指标与财经新闻情感分析相结合，提高了股票市场的风险控制策略表现。 |
| [^156] | [FABind+: Enhancing Molecular Docking through Improved Pocket Prediction and Pose Generation](https://arxiv.org/abs/2403.20261) | FABind+通过改进口袋预测和姿态生成，提升分子对接表现 |
| [^157] | [Sparse Feature Circuits: Discovering and Editing Interpretable Causal Graphs in Language Models](https://arxiv.org/abs/2403.19647) | 该论文介绍了一种新方法，即稀疏特征电路，可以在语言模型中发现和编辑可解释的因果图，为我们提供了对未预料机制的详细理解和包含了用于提高分类器泛化能力的SHIFT方法。 |
| [^158] | [Self-Improved Learning for Scalable Neural Combinatorial Optimization](https://arxiv.org/abs/2403.19561) | 提出一种新颖的自我改进学习(SIL)方法，实现神经组合优化的更好可扩展性，通过自身生成解决方案作为伪标签，设计线性复杂度的注意机制来处理大规模组合优化问题实例。 |
| [^159] | [Compressing Large Language Models by Streamlining the Unimportant Layer](https://arxiv.org/abs/2403.19135) | 通过观察大型语言模型中不同层对隐藏状态的影响程度，提出了LLM-Streamline方法，包括层剪枝和层替换，用于压缩模型并保持性能。 |
| [^160] | [Tiny Machine Learning: Progress and Futures](https://arxiv.org/abs/2403.19076) | TinyML是一种将深度学习模型压缩到物联网设备和微控制器中实现无处不在智能的新方法，需要共同设计算法和系统堆栈以克服硬件限制。 |
| [^161] | [Cross--domain Fiber Cluster Shape Analysis for Language Performance Cognitive Score Prediction](https://arxiv.org/abs/2403.19001) | 本研究通过新颖的框架SFFormer，结合了多头交叉注意力特征融合模块，基于dMRI纤维束追踪，预测了主观语言表现，拓展了脑结构与人类认知功能的关联研究。 |
| [^162] | [Few-Shot Cross-System Anomaly Trace Classification for Microservice-based systems](https://arxiv.org/abs/2403.18998) | 提出了针对微服务系统的少样本异常跟踪分类的新框架，利用多头注意力自编码器构建系统特定的跟踪表示，并应用基于Transformer编码器的模型无关元学习进行高效分类。 |
| [^163] | [Chinese Offensive Language Detection:Current Status and Future Directions](https://arxiv.org/abs/2403.18314) | 总体而言，这篇论文讨论了在中文中检测 offensive 语言的挑战，并强调了开发解决这一问题的特定模型和工具。 |
| [^164] | [Solution for Emotion Prediction Competition of Workshop on Emotionally and Culturally Intelligent AI](https://arxiv.org/abs/2403.17683) | 提出了一种名为ECSP的单多模态情感文化特定提示方法，旨在通过使用单一模态消息增强多模态模型性能，并设计良好的提示来减少文化差异问题。 |
| [^165] | [Transcribing Bengali Text with Regional Dialects to IPA using District Guided Tokens](https://arxiv.org/abs/2403.17407) | 通过引入区域指导标记技术，本文提出了一种将孟加拉文本与地方方言转录为国际音标的方法，为模型提供了关于输入文本的地区方言信息，以理解与每个地区相关的独特音韵模式。 |
| [^166] | [Visual Hallucination: Definition, Quantification, and Prescriptive Remediations](https://arxiv.org/abs/2403.17306) | 本文提出了关于视觉-语言模型中幻觉的细致讨论，对幻觉进行了量化，并提供了一个新的公开数据集VHILT，有助于研究此问题。 |
| [^167] | [A Transformer approach for Electricity Price Forecasting](https://arxiv.org/abs/2403.16108) | 这种独特的Transformer模型在电力价格预测中取得了更好的表现，为可靠和可持续的电力系统运行提供了有前景的解决方案。 |
| [^168] | [Finding needles in a haystack: A Black-Box Approach to Invisible Watermark Detection](https://arxiv.org/abs/2403.15955) | 提出了一种透明水印检测的黑盒方法WMD，在无注释设置下，利用干净无水印数据集检测任意水印，效果显著优于传统方法 |
| [^169] | [Understanding Emergent Abilities of Language Models from the Loss Perspective](https://arxiv.org/abs/2403.15796) | 本文从损失角度重新定义了语言模型的突现能力，发现具有相同预训练损失的模型在不同任务上表现相似，而当预训练损失低于特定阈值时，模型将展现出突现能力。 |
| [^170] | [Introducing an ensemble method for the early detection of Alzheimer's disease through the analysis of PET scan images](https://arxiv.org/abs/2403.15443) | 通过分析PET扫描图像，引入了一种集成方法早期检测阿尔茨海默病，并且在分类阿尔茨海默病时使用了多种深度学习和传统机器学习模型。 |
| [^171] | [LLaVA-PruMerge: Adaptive Token Reduction for Efficient Large Multimodal Models](https://arxiv.org/abs/2403.15388) | PruMerge提出了一种自适应的视觉令牌减少方法，可以有效减少大型多模态模型中的视觉令牌数量，同时保持模型性能。 |
| [^172] | [Large Language Models and User Trust: Focus on Healthcare](https://arxiv.org/abs/2403.14691) | 本文探讨了临床医生对LLMs的信任、数据来源从主要是人类生成到人工智能生成内容的转变，以及随之而来对LLMs精确性和临床医师能力的影响之间的不断发展的关系，强调了LLMs在医疗保健中的整合加深可能带来的挑战和风险。 |
| [^173] | [ReAct Meets ActRe: Autonomous Annotations of Agent Trajectories for Contrastive Self-Training](https://arxiv.org/abs/2403.14589) | 提出了A$^3$T框架，通过ActRe提示代理实现了ReAct风格代理对代理轨迹的自主标注，同时增强了新的轨迹合成能力。 |
| [^174] | [C-TPT: Calibrated Test-Time Prompt Tuning for Vision-Language Models via Text Feature Dispersion](https://arxiv.org/abs/2403.14119) | 本文研究了在测试时提示调整过程中通过利用CLIP的固有属性来探讨校准的方法，发现提示选择显著影响了CLIP中的校准，其中导致更高文本特征离散性的提示会产生更好校准的预测。 |
| [^175] | [ZigMa: Zigzag Mamba Diffusion Model](https://arxiv.org/abs/2403.13802) | 本研究提出了一种名为Zigzag Mamba的零参数方法，通过纠正当前Mamba-based视觉方法中对空间连续性的忽视，实现了更好的速度和内存利用，同时在大分辨率视觉数据集上展示了出色的性能。 |
| [^176] | [Caching-Augmented Lifelong Multi-Agent Path Finding](https://arxiv.org/abs/2403.13421) | CAL-MAPF引入了缓存机制来增强终身多智能体路径规划的性能，通过合适的输入任务分布、高缓存命中率和流畅的交通这三个因素显著提高了解决方案的稳定性。 |
| [^177] | [Incentivizing News Consumption on Social Media Platforms Using Large Language Models and Realistic Bot Accounts](https://arxiv.org/abs/2403.13362) | 通过创建使用 GPT-2 的机器人账户，在社交媒体平台上回复用户的推文，鼓励用户接触和关注验证的、意识形态平衡的新闻，以增加用户接触这些新闻并提高参与度。 |
| [^178] | [STG-Mamba: Spatial-Temporal Graph Learning via Selective State Space Model](https://arxiv.org/abs/2403.12418) | STG-Mamba 是首个利用选择性状态空间模型进行时空图学习的研究，将STG网络视为系统，并采用图选择性状态空间模块（GS3B）精确表征STG的动态演化。 |
| [^179] | [Evaluating Text to Image Synthesis: Survey and Taxonomy of Image Quality Metrics](https://arxiv.org/abs/2403.11821) | 评估文本到图像合成中，提出了针对图像质量的新评估指标，以确保文本和图像内容的对齐，并提出了新的分类法来归纳这些指标 |
| [^180] | [Can LLMs Generate Human-Like Wayfinding Instructions? Towards Platform-Agnostic Embodied Instruction Synthesis](https://arxiv.org/abs/2403.11487) | 提出了一种新方法，利用LLM以及上下文学习，实现了自动生成具身机器人的“行路指示”，并且在多个模拟平台上展示出跨平台特性。 |
| [^181] | [Scaling Data Diversity for Fine-Tuning Language Models in Human Alignment](https://arxiv.org/abs/2403.11124) | 更多的响应但更少的提示可以更好地触发大型语言模型进行人类对齐，此外，提出了一种新的提示多样性公式，可以进一步影响LLMs的最终性能。 |
| [^182] | [Boosting Flow-based Generative Super-Resolution Models via Learned Prior](https://arxiv.org/abs/2403.10988) | 通过引入条件学习的先验知识，成功解决了基于流的超分辨模型中的网格伪影、逆矩阵爆炸和次优结果等问题。 |
| [^183] | [A Conceptual Framework For White Box Neural Networks](https://arxiv.org/abs/2403.09863) | 引入语义特征作为通用概念框架，实现了完全可解释的神经网络层，为白盒神经网络的范式转变开辟了新的可能性。 |
| [^184] | [Abstracting Sparse DNN Acceleration via Structured Sparse Tensor Decomposition](https://arxiv.org/abs/2403.07953) | 本文提出了通过结构化分解张量进一步抽象稀疏DNN加速的方法，实现了将稀疏张量转换成一系列结构化稀疏张量，从而弥合了稀疏DNN模型和硬件之间的差距。 |
| [^185] | [Matrix-Transformation Based Low-Rank Adaptation (MTLoRA): A Brain-Inspired Method for Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2403.07440) | 该论文提出了基于矩阵变换的低秩调整（MTLoRA）方法，受大脑启发，用于提高微调技术的复杂任务适应性、性能、稳定性和算法复杂性。 |
| [^186] | [ParallelPARC: A Scalable Pipeline for Generating Natural-Language Analogies](https://arxiv.org/abs/2403.01139) | 设计了ParallelPARC流水线，利用大型语言模型生成复杂段落类比数据集，评估各种类比类型，并展示出人类在类比识别中的优势。 |
| [^187] | [UrbanGPT: Spatio-Temporal Large Language Models](https://arxiv.org/abs/2403.00813) | 都市GPT旨在建立一个具有强大泛化能力的时空模型，借鉴大型语言模型的成就。 |
| [^188] | [LLM-Resistant Math Word Problem Generation via Adversarial Attacks](https://arxiv.org/abs/2402.17916) | 本研究提出了一种新方法，通过生成保留原问题结构难度但针对LLMs无解的对抗性示例，有效地降低了LLMs的数学问题解决能力。 |
| [^189] | [Q-FOX Learning: Breaking Tradition in Reinforcement Learning](https://arxiv.org/abs/2402.16562) | Q-FOX学习是一种新颖的自动超参数调整方法，结合了FOX优化器和Q-learning算法，提出了使用新的目标函数来解决强化学习中超参数调整的问题。 |
| [^190] | [Triad: A Framework Leveraging a Multi-Role LLM-based Agent to Solve Knowledge Base Question Answering](https://arxiv.org/abs/2402.14320) | Triad框架利用了基于多角色LLM代理来解决知识库问答问题，通过代理的不同角色分别处理KBQA子任务，合作完成KBQA任务，并在多个基准数据集上表现出色。 |
| [^191] | [TofuEval: Evaluating Hallucinations of LLMs on Topic-Focused Dialogue Summarization](https://arxiv.org/abs/2402.13249) | 论文提出了一个新的主题对话摘要评估基准TofuEval，研究发现现有的LLMs在对话领域存在大量事实错误的幻觉，并表明当LLMs充当事实评估器时，其表现不佳。 |
| [^192] | [RS-DPO: A Hybrid Rejection Sampling and Direct Preference Optimization Method for Alignment of Large Language Models](https://arxiv.org/abs/2402.10038) | 本研究提出了一种名为RS-DPO的方法，它将拒绝采样和直接优化偏好结合起来，用于对齐大型语言模型。通过开发一个经过监督微调的策略模型，并从该模型中直接采样响应，RS-DPO能够有效解决基于近端策略优化的不稳定性和高计算成本的问题。通过识别对比样本对，RS-DPO能够更好地进行RLHF。 |
| [^193] | [LlaSMol: Advancing Large Language Models for Chemistry with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset](https://arxiv.org/abs/2402.09391) | 本文介绍了LlaSMol，它是一种推进化学领域大规模语言模型的方法。通过使用一个大规模、全面、高质量的指令调优数据集来训练模型，LlaSMol在化学任务中表现出强大的性能，超过了GPT-4并接近于任务特定模型。 |
| [^194] | [X-LoRA: Mixture of Low-Rank Adapter Experts, a Flexible Framework for Large Language Models with Applications in Protein Mechanics and Design](https://arxiv.org/abs/2402.07148) | X-LoRA是一种灵活的大型语言模型框架，利用低秩适配器专家的混合策略，可以创建精细调整的模型并在蛋白质力学和设计领域应用。该模型利用深层逐层适应的组合来解决特定任务，并受到生物学原理的启发。无需修改底层结构即可应用于任何现有的语言模型。 |
| [^195] | [A Survey on Safe Multi-Modal Learning System](https://arxiv.org/abs/2402.05355) | 这项研究提出了第一个多模态学习系统安全的分类法，对当前发展状态下的关键限制进行了审查，并提出了未来研究的潜在方向。 |
| [^196] | [TinyLLM: Learning a Small Student from Multiple Large Language Models](https://arxiv.org/abs/2402.04616) | TinyLLM是一种从多个大型语言模型中学习小型学生模型的知识蒸馏范式，旨在解决知识多样性有限和缺乏上下文信息等问题，并鼓励学生模型理解答案背后的原理。 |
| [^197] | [Dual-View Visual Contextualization for Web Navigation](https://arxiv.org/abs/2402.04476) | 本文提出了一种通过网页截图中元素的“双视图”来进行 HTML 元素的背景化的方法，这种方法通过将每个元素与其邻居元素进行背景化，使用文本和视觉特征，使得HTML元素的结果表示对于代理执行操作更加信息丰富。 |
| [^198] | [Read to Play (R2-Play): Decision Transformer with Multimodal Game Instruction](https://arxiv.org/abs/2402.04154) | 本论文探索了为智能体提供增强形式的任务指导，使其能够理解游戏指导并实现"读玩游戏"的能力。通过将多模态指导调优的成功应用于视觉任务中的强化学习任务，构建了一组... (内容太长，无法继续显示) |
| [^199] | [Techniques to Detect Crime Leaders within a Criminal Network: A Survey, Experimental, and Comparative Evaluations](https://arxiv.org/abs/2402.03355) | 本论文调查、分析了在犯罪网络中识别犯罪头目的技术和算法，提出了一种新的方法论分类体系，通过实证评估和实验比较，为研究人员提供了全面的决策支持。 |
| [^200] | [Explaining latent representations of generative models with large multimodal models](https://arxiv.org/abs/2402.01858) | 该研究提出了一个框架，利用大型多模态模型全面解释生成模型中的每个潜在因素。对于不同生成模型的解释，我们量化了解释的不确定性，通过多模态模型进行了性能评估，且定性地展示了潜在因素的变化效果。最后，我们讨论了最先进的大型多模态模型的解释能力和局限性。 |
| [^201] | [A PNP ion channel deep learning solver with local neural network and finite element input data](https://arxiv.org/abs/2401.17513) | 该论文提出了一个使用本地神经网络和有限元求解器的PNP离子通道深度学习求解器。该求解器能够较快地训练并生成高准确度的数值解，在处理不同扰动情况和离子通道子区域时表现良好。 |
| [^202] | [Resolving Ethics Trade-offs in Implementing Responsible AI](https://arxiv.org/abs/2401.08103) | 该论文提出了通过权衡处理人工智能伦理中的紧张关系的五种方法，并提出了一个框架来实施全面的人工智能/机器学习系统。 |
| [^203] | [Dr$^2$Net: Dynamic Reversible Dual-Residual Networks for Memory-Efficient Finetuning](https://arxiv.org/abs/2401.04105) | 提出了一种名为Dr$^2$Net的新型网络结构，可以以大大降低内存消耗的方式作为预训练模型的替代网络，通过具有两种类型残差连接的设计实现了可逆性，实现了在训练过程中清除中间激活并减少内存占用 |
| [^204] | [Causal State Distillation for Explainable Reinforcement Learning](https://arxiv.org/abs/2401.00104) | 本文提出了一种因果状态精炼的方法，通过揭示在训练过程中影响智能体目标的奖励的各个方面，解决了强化学习模型不透明性的问题。 |
| [^205] | [ComplexityNet: Increasing LLM Inference Efficiency by Learning Task Complexity](https://arxiv.org/abs/2312.11511) | ComplexityNet通过学习任务复杂性，提高了LLM推理效率，通过预测任务的准确输出概率，成功降低了90%的计算资源使用，并在任务复杂性确定方面取得了79%准确率。 |
| [^206] | [SMILE: Multimodal Dataset for Understanding Laughter in Video with Language Models](https://arxiv.org/abs/2312.09818) | 本论文介绍了一个新任务，即视频笑声推理，旨在解释特定视频中人们笑的原因，并提出了数据集SMILE。通过利用大型语言模型生成文本视频表示，我们的基线能够生成合理的笑声解释。 |
| [^207] | [Collaborating Foundation Models for Domain Generalized Semantic Segmentation](https://arxiv.org/abs/2312.09788) | 提出了一种协作基础模型框架（CLOUDS）用于领域泛化语义分割，集成了CLIP骨干、生成模型和“Segment Anything Model”，以实现对各种目标分布模式的覆盖和预测改进。 |
| [^208] | [LatentEditor: Text Driven Local Editing of 3D Scenes](https://arxiv.org/abs/2312.09313) | LatentEditor 是一个创新框架，通过文本提示实现对神经场进行精确和局部受控编辑，将真实场景嵌入潜在空间，提供比传统方法更快更具适应性的编辑NeRF骨干。引入了增量分数和像素级评分方法以提高编辑精度。 |
| [^209] | [Auto MC-Reward: Automated Dense Reward Design with Large Language Models for Minecraft](https://arxiv.org/abs/2312.09238) | 本文介绍了一种名为Auto MC-Reward的先进学习系统，利用大型语言模型自动设计稠密奖励函数来提高学习效率 |
| [^210] | [Honeybee: Locality-enhanced Projector for Multimodal LLM](https://arxiv.org/abs/2312.06742) | 该研究提出了一种既灵活又增强局部性的新型投影仪设计，有助于连接视觉编码器和语言模型，提高模型的效率和空间理解。 |
| [^211] | [Invariant Representation via Decoupling Style and Spurious Features from Images](https://arxiv.org/abs/2312.06226) | 本文提出了一种新的框架IRSS，通过解耦图像的风格和虚假特征，实现了在缺失域标签情况下的超出分布（OOD）泛化。 |
| [^212] | [Towards Learning a Generalist Model for Embodied Navigation](https://arxiv.org/abs/2312.02010) | 提出了首个面向体验导航的通用模型NaviLLM，通过引入基于模式的指导，将LLMs应用到体验导航中，实现了对不同任务的统一处理。 |
| [^213] | [WATonoBus: An All Weather Autonomous Shuttle](https://arxiv.org/abs/2312.00938) | 提出了一种考虑恶劣天气的多模块和模块化系统架构，在WATonoBus平台上进行了实际测试，证明其能够解决全天候自动驾驶车辆面临的挑战 |
| [^214] | [Contrastive Denoising Score for Text-guided Latent Diffusion Image Editing](https://arxiv.org/abs/2311.18608) | 提出了一种名为Contrastive Denoising Score (CDS) 的简单但强大的修改版本，用于潜在扩散模型 (LDM)，在DDS框架内使用了CUT损失来更好地保留原始图像的特定结构元素。 |
| [^215] | [Sketch Input Method Editor: A Comprehensive Dataset and Methodology for Systematic Input Recognition](https://arxiv.org/abs/2311.18254) | 该研究提出了一个专为专业C4I系统设计的画图输入法编辑器（SketchIME），并通过引入少样本域自适应和增量学习来提高网络性能和适应新用户。 |
| [^216] | [Compositional Chain-of-Thought Prompting for Large Multimodal Models](https://arxiv.org/abs/2311.17076) | 提出了一种新颖的零样式思维提示（CCoT），以克服大型多模态模型难以捕捉到组合视觉推理方面的细节的问题。 |
| [^217] | [Discovering Effective Policies for Land-Use Planning](https://arxiv.org/abs/2311.12304) | 通过学习代理模型并使用进化搜索过程，发现了可定制到不同位置的有效土地利用政策，为土地利用规划提供了一个潜在有用的工具。 |
| [^218] | [OVM, Outcome-supervised Value Models for Planning in Mathematical Reasoning](https://arxiv.org/abs/2311.09724) | 引入Outcome-supervised Value Model (OVM)利用结果监督训练价值模型，以在数学推理中减少错误传播，将任务转化为价值估计问题。 |
| [^219] | [ARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems](https://arxiv.org/abs/2311.09476) | ARES是用于评估检索增强生成系统的自动化评估框架，通过创建合成训练数据和微调评估器，有效评估RAG系统在不同任务中的表现。 |
| [^220] | [AbsPyramid: Benchmarking the Abstraction Ability of Language Models with a Unified Entailment Graph](https://arxiv.org/abs/2311.09174) | 该研究提出了AbsPyramid，一个统一的蕴涵图，用于评估语言模型的抽象能力，实验证明LLMs可以通过在丰富抽象知识上进行训练来获得基本的抽象能力，并泛化到未见的事件。 |
| [^221] | [Identifying Linear Relational Concepts in Large Language Models](https://arxiv.org/abs/2311.08968) | 通过线性关系概念(LRC)技术，可以在大型语言模型的隐藏激活潜在空间中找到与人类可解释概念对应的概念方向，这种技术超越了标准黑盒探测分类器。 |
| [^222] | [Safer-Instruct: Aligning Language Models with Automated Preference Data](https://arxiv.org/abs/2311.08685) | Safer-Instruct通过反向指导调整、指导感应和专家模型评估的方式，实现了自动构建大规模偏好数据的目的，从而在没有人工标注者的情况下高效生成高质量的偏好数据。 |
| [^223] | [Low-Rank Adaptation for Multilingual Summarization: An Empirical Study](https://arxiv.org/abs/2311.08572) | LoRA 在多语言摘要方面竞争力强，特别在低数据情况和跨语言转移方面表现出色。 |
| [^224] | [Evaluating Neighbor Explainability for Graph Neural Networks](https://arxiv.org/abs/2311.08118) | 评价图神经网络中邻居的可解释性，提出新的度量标准并发现基于梯度的方法在GNN领域的解释没有太大差异，同时发现很多技术在没有自环的GNNs下无法准确识别重要邻居。 |
| [^225] | [Flames: Benchmarking Value Alignment of Chinese Large Language Models](https://arxiv.org/abs/2311.06899) | 中国的大型语言模型的价值观契合性需要更加全面的评估，该研究提出了一个名为"火焰"（Flames）的基准测试，涵盖了常见的无害原则和特定中国价值观，以及复杂场景和隐含恶意的提示方法。 |
| [^226] | [Fake Alignment: Are LLMs Really Aligned Well?](https://arxiv.org/abs/2311.05915) | 研究发现大型语言模型（LLMs）在安全性评估中存在假对齐问题，提出了Fake alIgNment Evaluation (FINE)框架和两个新的度量标准，用于验证和修正这一现象 |
| [^227] | [DistillSpec: Improving Speculative Decoding via Knowledge Distillation](https://arxiv.org/abs/2310.08461) | DistillSpec通过知识蒸馏技术改进了投机性解码，在标准基准测试中取得了10-45%的速度提升。 |
| [^228] | [Towards LogiGLUE: A Brief Survey and A Benchmark for Analyzing Logical Reasoning Capabilities of Language Models](https://arxiv.org/abs/2310.00836) | 该研究旨在评估大型语言模型在逻辑推理中的表现，并提供了名为LogiGLUE的基准，以研究逻辑推理数据集、任务和利用语言模型进行推理的方法。 |
| [^229] | [Can I Trust Your Answer? Visually Grounded Video Question Answering](https://arxiv.org/abs/2309.01327) | 通过研究基于视觉的视频问答，发现当前视觉语言模型在做出可靠预测方面存在局限性，并提出了一种改进方法。 |
| [^230] | [An Examination of the Compositionality of Large Generative Vision-Language Models](https://arxiv.org/abs/2308.10509) | 本文检验了生成视觉-语言模型的组合性，发现当前基准中存在句法偏见，提出了新的评估指标SyntaxBias Score和新的基准SyntActically D。 |
| [^231] | [Citation: A Key to Building Responsible and Accountable Large Language Models](https://arxiv.org/abs/2307.02185) | 引文被确定为大型语言模型中关键但缺失的组成部分，其引入可以增强内容的透明性和可验证性以应对知识产权和伦理问题，提议LLMs的全面引文机制应考虑非参数化和参数化内容，尽管实施引文机制复杂且存在潜在缺陷，仍主张推动其发展并概述未来研究问题。 |
| [^232] | [BRAIxDet: Learning to Detect Malignant Breast Lesion with Incomplete Annotations](https://arxiv.org/abs/2301.13418) | 本文提出了一种中间解决方案，即将训练构建为弱监督和半监督学习问题，以解决通过不完整注释数据检测恶性乳腺病变的困境 |
| [^233] | [Self-Organization Towards $1/f$ Noise in Deep Neural Networks](https://arxiv.org/abs/2301.08530) | 本研究发现在深度神经网络中训练的模型会产生类似生物神经网络的1/f噪声，但当神经网络过度容量时，激活模式会转向白噪声。 |
| [^234] | [Finding Nontrivial Minimum Fixed Points in Discrete Dynamical Systems](https://arxiv.org/abs/2301.04090) | 在离散动力系统中，我们提出了一个优化问题，即寻找具有最小受影响节点数量的系统非平凡不动点，并发现了一些特殊情况下可以高效解决这个问题。 |
| [^235] | [G-PECNet: Towards a Generalizable Pedestrian Trajectory Prediction System](https://arxiv.org/abs/2210.09846) | 通过结合周期激活函数的启发和使用隐马尔可夫模型（HMMs）和强化学习（RL）进行合成轨迹数据增强，我们的方法G-PECNet在行人轨迹预测任务中取得了9.5%的最终位移误差（FDE）改进。 |
| [^236] | [Toward the application of XAI methods in EEG-based systems](https://arxiv.org/abs/2210.06554) | 本文旨在将合适的可解释人工智能（XAI）方法应用于解决脑-计算机接口（BCI）中的数据集转移问题，以提高在情绪识别中的EEG信号分类系统的泛化性能。 |
| [^237] | [Regulating eXplainable Artificial Intelligence (XAI) May Harm Consumers](https://arxiv.org/abs/2209.03499) | XAI regulation may be redundant and mandating fully transparent XAI may make firms and consumers worse off, revealing a tradeoff between maximizing welfare and receiving explainable AI outputs. |
| [^238] | [Prompt Tuning with Soft Context Sharing for Vision-Language Models](https://arxiv.org/abs/2208.13474) | 提出了一种名为SoftCPT的新方法，利用软上下文共享来联合调整预训练的视觉-语言模型对多个目标少样本任务进行调整，并设计了一个任务共享的元网络来生成提示上下文。 |
| [^239] | [Multi-Step Deductive Reasoning Over Natural Language: An Empirical Study on Out-of-Distribution Generalisation](https://arxiv.org/abs/2207.14000) | 提出了IMA-GloVe-GA，一个用于自然语言表达的多步推理的迭代神经推理网络，在超领域泛化方面具有更好的性能表现。 |
| [^240] | [Semi-supervised Predictive Clustering Trees for (Hierarchical) Multi-label Classification](https://arxiv.org/abs/2207.09237) | 提出了一种基于半监督学习的预测聚类树的（分层）多标签分类方法。 |
| [^241] | [Embed to Control Partially Observed Systems: Representation Learning with Provable Sample Efficiency](https://arxiv.org/abs/2205.13476) | 论文提出了一种名为Embed to Control（ETC）的强化学习算法，通过在两个级别学习表示的方法来解决部分观察马尔可夫决策问题，以实现样本高效利用。 |
| [^242] | [Aligning Logits Generatively for Principled Black-Box Knowledge Distillation](https://arxiv.org/abs/2205.10490) | 本文提出了一个新的黑盒知识蒸馏方法MEKD，通过将教师和学生模型的低维度对数对齐，实现将一个繁琐模型压缩成轻量级模型。 |
| [^243] | [Distributed Multi-Agent Reinforcement Learning Based on Graph-Induced Local Value Functions](https://arxiv.org/abs/2202.13046) | 通过利用图结构，本文提出了一种通用计算高效的分布式框架，基于局部值函数的分布式RL方法在协作多智能体强化学习中取得了显著的样本复杂性降低。 |
| [^244] | [Communication-Efficient Federated Learning with Accelerated Client Gradient](https://arxiv.org/abs/2201.03172) | 通过使用具有前瞻梯度的全局模型，并通过与超调全局模型对齐来规范本地更新，提出了一种简单但有效的联邦学习框架，以改善参与者之间的一致性，促进服务器模型的收敛。 |
| [^245] | [CGCL: Collaborative Graph Contrastive Learning without Handcrafted Graph Data Augmentations](https://arxiv.org/abs/2111.03262) | 提出了一种新颖的Collaborative Graph Contrastive Learning框架（CGCL），利用多个图编码器观察图形，并避免引入不稳定扰动，保证图的不变性。 |
| [^246] | [Role Similarity Metric Based on Spanning Rooted Forest](https://arxiv.org/abs/2110.07872) | 提出了一种新的角色相似度度量\textsf{ForestSim}，并设计了相应的能够在$O(k)$时间内处理top-k查询的搜索算法\textsf{ForestSimSearch}。 |
| [^247] | [GraphFM: Graph Factorization Machines for Feature Interaction Modeling](https://arxiv.org/abs/2105.11866) | 提出了一种名为GraphFM的图因子分解机方法，通过图结构自然表示特征，并将FM的交互功能集成到GNN的特征聚合策略中，能够模拟任意阶特征交互。 |
| [^248] | [MetaVIM: Meta Variationally Intrinsic Motivated Reinforcement Learning for Decentralized Traffic Signal Control](https://arxiv.org/abs/2101.00746) | 提出了一种新颖的MetaVIM强化学习方法，用于分散式交通信号控制，通过元学习考虑邻居信息，以解决现实世界中交通信号控制面临的挑战。 |
| [^249] | [Health Text Simplification: An Annotated Corpus for Digestive Cancer Education and Novel Strategies for Reinforcement Learning.](http://arxiv.org/abs/2401.15043) | 该论文介绍了一个用于健康文本简化研究的消化癌症教育材料的注释语料库，并探索了基于大型语言模型的简化方法，包括微调、增强学习、增强学习与人类反馈、领域自适应和基于提示的应用。 |
| [^250] | [Topologies of Reasoning: Demystifying Chains, Trees, and Graphs of Thoughts.](http://arxiv.org/abs/2401.14295) | 这篇论文探讨了结合结构的提示工程在提高大型语言模型推理性能方面的前景，通过思维链、思维树或思维图的设计来引导整体推理过程。通过大量实例，这种范式显著增强了模型在多个任务中的能力。总的来说，论文提供了一个通用蓝图，为未来的发展铺平道路。 |
| [^251] | [Can Large Language Models Write Parallel Code?.](http://arxiv.org/abs/2401.12554) | 本文研究了大型语言模型生成并行代码的能力。我们提出了一个基准测试集PCGBench，并使用新指标评估了几个最先进的语言模型在并行编程模型和计算问题类型上的性能。 |
| [^252] | [GI-PIP: Do We Require Impractical Auxiliary Dataset for Gradient Inversion Attacks?.](http://arxiv.org/abs/2401.11748) | 本文提出了一种新颖的梯度反转攻击方法GI-PIP，不需要依赖不切实际的辅助数据集，通过利用异常检测模型从较少的数据中捕获底层分布，并能在图像恢复方面表现出优异的能力，同时在分布泛化方面也更强大。 |
| [^253] | [Quantum Machine Learning: from NISQ to Fault Tolerance.](http://arxiv.org/abs/2401.11351) | 本文提供了对量子机器学习领域的全面回顾，涵盖了在NISQ技术和容错量子计算硬件上使用的技术和算法，并深入讨论了与量子机器学习相关的基本概念和统计学习理论。 |
| [^254] | [DeepEdit: Knowledge Editing as Decoding with Constraints.](http://arxiv.org/abs/2401.10471) | DeepEdit是一种神经符号方法，通过更好的推理一致性和对更新知识的意识，提高了大型语言模型的知识编辑能力，对多跳问题数据集MQuaKE取得了显著的进展。 |
| [^255] | [Incorporating Geo-Diverse Knowledge into Prompting for Increased Geographical Robustness in Object Recognition.](http://arxiv.org/abs/2401.01482) | 本文研究了在目标识别中将地理多样知识融入提示以提高地理鲁棒性的方法，探索了通过大型语言模型获取地理特定对象知识并结合CLIP视觉语言模型的零样本和可学习软提示。通过提出一种地理知识正则化方法，实现了从源地理位置推广到未见目标地理位置的鲁棒性提升。 |
| [^256] | [Leveraging Generative AI for Clinical Evidence Summarization Needs to Ensure Trustworthiness.](http://arxiv.org/abs/2311.11211) | 这项研究讨论了利用生成型人工智能进行临床证据摘要的可靠性问题，尤其关注开发问责制、公平和包容性的模型的挑战。 |
| [^257] | [Learning Unsupervised World Models for Autonomous Driving via Discrete Diffusion.](http://arxiv.org/abs/2311.01017) | 本论文提出了一种通过离散扩散学习无监督的自动驾驶世界模型的新方法，通过使用VQVAE对传感器观察进行标记化并通过离散扩散预测未来，我们的模型在点云观察中实现了显著改进，将1秒预测的SOTA Chamfer距离降低了65%以上。 |
| [^258] | [Robust Offline Policy Evaluation and Optimization with Heavy-Tailed Rewards.](http://arxiv.org/abs/2310.18715) | 本文提出的ROAM和ROOM算法框架通过将中位数法与离线强化学习策略相结合，提供了对重尾奖励的直接不确定性估计，从而增强了离线强化学习在现实应用中的鲁棒性。 |
| [^259] | [Emergent Mixture-of-Experts: Can Dense Pre-trained Transformers Benefit from Emergent Modular Structures?.](http://arxiv.org/abs/2310.10908) | 该论文研究了密集预训练Transformer是否以及如何从自发的模块化结构中获益。 |
| [^260] | [Instruction Tuning with Human Curriculum.](http://arxiv.org/abs/2310.09518) | 本文探讨了在大型语言模型中应用结构化认知学习方法进行指令调整的潜在好处，并提出了一个高度结构化的合成数据集，结果表明该方法优于传统的随机化方法，提高了指令调整的性能。 |
| [^261] | [Mirage: Model-Agnostic Graph Distillation for Graph Classification.](http://arxiv.org/abs/2310.09486) | Mirage是一种模型无关的图蒸馏算法，旨在构建一个更小的合成训练集，以在资源有限的环境中实现图分类，并克服了现有算法对完整数据集的依赖性和对建模流程变化的敏感性。 |
| [^262] | [A Systematic Evaluation of Large Language Models on Out-of-Distribution Logical Reasoning Tasks.](http://arxiv.org/abs/2310.09430) | 通过对大型语言模型在非分布式逻辑推理任务上进行系统评估，我们发现这些模型在处理我们新构建的数据集时都存在困难，尽管它们在其他自然语言处理任务上表现良好。这表明这些模型在逻辑推理方面的泛化和鲁棒性仍需要进一步研究。 |
| [^263] | [LangNav: Language as a Perceptual Representation for Navigation.](http://arxiv.org/abs/2310.07889) | 该论文探索了将语言作为导航的感知表示，并使用现成的视觉系统将每个时间步骤的视图转化为自然语言描述，通过微调预训练的语言模型选择最佳的行动来满足导航指令。有两种用例的实验对这种基于语言的导航方法进行了探索：使用大型语言模型生成合成轨迹以微调较小的语言模型，以及模拟到实际的转换。 |
| [^264] | [Observation-Guided Diffusion Probabilistic Models.](http://arxiv.org/abs/2310.04041) | 提出了观测引导的扩散概率模型（OGDM），通过引入基于条件鉴别器的观测所产生的额外损失项，实现了更准确的负对数似然优化，在函数评估次数有限的推理阶段表现出色。 |
| [^265] | [LightSeq: Sequence Level Parallelism for Distributed Training of Long Context Transformers.](http://arxiv.org/abs/2310.03294) | LightSeq是用于长上下文转换器分布式训练的一种新方法，它通过序列维度进行分区，与不同注意力头数量的模型架构兼容，并且减少了与Megatron-LM相比的通信量，同时还实现了通信和计算的重叠。 |
| [^266] | [SE(3)-Stochastic Flow Matching for Protein Backbone Generation.](http://arxiv.org/abs/2310.02391) | 通过SE(3)-Stochastic Flow Matching，我们提出了一系列新型生成模型FoldFlow，可以准确建模蛋白质主链。这些模型通过无需模拟训练和Riemannian最优传输的结合，具有更好的稳定性和建模能力。 |
| [^267] | [Consistency Trajectory Models: Learning Probability Flow ODE Trajectory of Diffusion.](http://arxiv.org/abs/2310.02279) | 提出了一种一致性轨迹模型（CTM），它可以加速扩散模型的采样，同时通过对抗训练和去噪得分匹配损失的组合来提高性能，并实现了最先进的采样质量。 |
| [^268] | [VDC: Versatile Data Cleanser for Detecting Dirty Samples via Visual-Linguistic Inconsistency.](http://arxiv.org/abs/2309.16211) | VDC是一个多功能数据清洁器，通过检测图像和标签之间的视觉和语言不一致性来解决数据集中的脏样本问题。 |
| [^269] | [Boosting In-Context Learning with Factual Knowledge.](http://arxiv.org/abs/2309.14771) | 本文研究了使用事实知识提升上下文学习的效果，并提出了一个新的知识上下文调优框架来改善学习性能。 |
| [^270] | [Efficient Post-training Quantization with FP8 Formats.](http://arxiv.org/abs/2309.14592) | 本研究研究了FP8格式在后训练量化中的优势，并开发了一个通用的量化工作流程。实验结果表明，FP8相对于INT8具有更好的工作负载覆盖率和模型准确性。 |
| [^271] | [SeaEval for Multilingual Foundation Models: From Cross-Lingual Alignment to Cultural Reasoning.](http://arxiv.org/abs/2309.04766) | SeaEval是一个评估多语言基础模型的基准测试，研究了模型在自然语言理解、推理以及对文化实践、细微差别和价值观的理解能力上的表现。重要发现包括模型在给出改写指令时行为各异，受到暴露偏差的影响，对于语义等价的多语言查询的回答不一致，以及模型在情感相关问题上的一致性不同。 |
| [^272] | [Evaluation of large language models for discovery of gene set function.](http://arxiv.org/abs/2309.04019) | 本研究评估了OpenAI的GPT-4大型语言模型在发现基因集合功能方面的能力，并发现它能根据嵌入的生物医学知识生成与Gene Ontology中具名基因集合非常相似的名称，同时在基因组学数据中发现的基因集合中，GPT-4的命名更具信息量，得到了人工审核的基本验证。 |
| [^273] | [Efficient Benchmarking (of Language Models).](http://arxiv.org/abs/2308.11696) | 本研究提出了一种名为"Efficient Benchmarking"的问题，旨在智能地减少语言模型评估的计算成本而不降低可靠性，并使用一种名为Decision Impact on Reliability（DIoR）的新度量来评估决策的可靠性。通过HELM基准测试的案例研究，发现只需删除一个低排名模型即可改变领先者，并仅需少量示例即可得到正确的基准测试排名。 |
| [^274] | [DiagGPT: An LLM-based Chatbot with Automatic Topic Management for Task-Oriented Dialogue.](http://arxiv.org/abs/2308.08043) | DiagGPT将大型语言模型(LLMs)扩展到任务导向的对话场景，提供了在复杂诊断场景中主动提问和引导用户完成任务的能力。 |
| [^275] | [RAVEN: In-Context Learning with Retrieval Augmented Encoder-Decoder Language Models.](http://arxiv.org/abs/2308.07922) | RAVEN是一种结合了检索增强的蒙特卡洛语言建模和前缀语言建模的模型，通过引入上下文融合学习，它能够在上下文学习方面取得比ATLAS更好的性能。 |
| [^276] | [Deep Neural Networks Fused with Textures for Image Classification.](http://arxiv.org/abs/2308.01813) | 本论文提出了一种将全局纹理与局部补丁信息相结合的新方法，用于解决细粒度图像分类问题。通过提取深层特征和计算图像级纹理，将两者的优势相结合，实现了更高的分类准确性。 |
| [^277] | [XSTest: A Test Suite for Identifying Exaggerated Safety Behaviours in Large Language Models.](http://arxiv.org/abs/2308.01263) | 本文介绍了一个名为XSTest的测试套件，旨在识别大型语言模型中夸大的安全行为。该套件由200个安全提示组成，涵盖十种提示类型，旨在引出模型的系统性问题。 |
| [^278] | [Instructed to Bias: Instruction-Tuned Language Models Exhibit Emergent Cognitive Bias.](http://arxiv.org/abs/2308.00225) | 这项研究发现，经过指导调优的语言模型呈现出新兴的认知偏见，这对于理解和开发更可靠和无偏的语言模型至关重要。 |
| [^279] | [SAS Video-QA: Self-Adaptive Sampling for Efficient Video Question-Answering.](http://arxiv.org/abs/2307.04192) | SAS视频问答通过自适应采样策略解决了视频问答中的问题，提高了效率和准确性 |
| [^280] | [An Interpretable Constructive Algorithm for Incremental Random Weight Neural Networks and Its Application.](http://arxiv.org/abs/2307.00185) | 本文提出了一种可解释的增量随机权重神经网络的构造算法，通过几何信息约束和节点池策略解决了难以解释隐藏参数与残差误差之间关系的问题。这种算法在大规模数据建模任务中表现出了良好的性能。 |
| [^281] | [A Survey on Multimodal Large Language Models.](http://arxiv.org/abs/2306.13549) | 本文追踪和总结了多模态大语言模型（MLLM）的最新进展，包括多模态指令调整、多模态上下文学习、多模态思维链和LLM辅助视觉推理等应用，指出了现有挑战和有前途的研究方向。 |
| [^282] | [Subgraph Networks Based Contrastive Learning.](http://arxiv.org/abs/2306.03506) | 本文提出了一种新的对比学习框架，名为基于子图网络的对比学习(SGNCL)，通过应用子图网络生成策略以产生增强视图，并探究了子结构相互作用对图形表示的影响。 |
| [^283] | [SQL-PaLM: Improved Large Language Model Adaptation for Text-to-SQL.](http://arxiv.org/abs/2306.00739) | 本文提出了一种基于大语言模型的Text-to-SQL模型SQL-PaLM，使用了面向Text-to-SQL的基于执行的自一致提示方法，在Spider上实现了77.3%的测试套件准确度，并显着超越以前的最新技术的方法。 |
| [^284] | [Exploring Phonetic Context in Lip Movement for Authentic Talking Face Generation.](http://arxiv.org/abs/2305.19556) | 本研究探讨了语音上下文对真实说话人脸生成的影响，提出了一种Context-Aware Lip-Sync框架（CALS），可利用语音上下文生成更加准确、稳定的唇部运动。 |
| [^285] | [Enhancing Logical Reasoning of Large Language Models through Logic-Driven Data Augmentation.](http://arxiv.org/abs/2305.12599) | 本论文介绍了一种通过逻辑驱动的数据增强方法来增强大型语言模型的逻辑推理能力。通过将原始文本转换为抽象意义表述图，并对其进行逻辑修改和转换，生成增强数据，从而提升模型性能。该方法适用于各种体系结构的大型语言模型。 |
| [^286] | [Tram: A Token-level Retrieval-augmented Mechanism for Source Code Summarization.](http://arxiv.org/abs/2305.11074) | Tram是一种源代码摘要的令牌级别检索增强机制，它在解码器端精细检索帮助神经模型生成更准确的摘要，并在Java和Python源代码摘要任务上表现优异。 |
| [^287] | [Complex Logical Reasoning over Knowledge Graphs using Large Language Models.](http://arxiv.org/abs/2305.01157) | 本文提出了一种使用大型语言模型的解耦方法，将复杂的知识图谱推理形式化为上下文知识图搜索和抽象逻辑查询推理的组合，与现有方法相比，它在多个逻辑查询结构的标准基准数据集上都表现出更好的性能，并且在更高复杂性的查询中获得了显着的性能提升。 |
| [^288] | [Predicting Human Attention using Computational Attention.](http://arxiv.org/abs/2303.09383) | HAT是一种计算注意力模型，使用新颖的基于变换器的结构和简化的凹视网膜，实现了对于目标存在和目标缺失搜索期间注视行为的扫描路径的最新技术水平。 |
| [^289] | [3D Masked Autoencoding and Pseudo-labeling for Domain Adaptive Segmentation of Heterogeneous Infant Brain MRI.](http://arxiv.org/abs/2303.09373) | 本文提出了一种名为 MAPSeg 的新框架，采用 3D 蒙版自编码和伪标签的方式，实现了跨年龄、跨模态和跨场景下对婴儿脑 MRI 中亚皮质区域的分割，充分考虑不同 MRI 扫描仪、供应商或采集序列以及不同的神经发育阶段所造成的内在异质性，提高了分割结果的鲁棒性。 |
| [^290] | [Shape-Guided Diffusion with Inside-Outside Attention.](http://arxiv.org/abs/2212.00210) | 该论文提出了一种无需训练的形状引导扩散方法，使用一种新颖的内外部注意机制将形状限制应用于跨注意力图和自注意力图上，从而在文本到图像扩散模型中考虑到对象形状，进而可以实现对象形状忠实度更高的图像生成。 |
| [^291] | [Using Persuasive Writing Strategies to Explain and Detect Health Misinformation.](http://arxiv.org/abs/2211.05985) | 本研究旨在通过使用说服性写作技巧的文本段落进行分类来增加自动化虚假信息检测的新层次，以产生可解释的理由。我们提出了一个包含常见说服性写作策略的注释方案和数据集，并使用 RoBERTa 文本分类模型进行实验。 |
| [^292] | [WaveMix: A Resource-efficient Neural Network for Image Analysis.](http://arxiv.org/abs/2205.14375) | WaveMix是一种资源高效的神经网络结构，可以在多项任务上达到可比或更好的准确率，并且需要更少的参数和GPU RAM，实现时间、成本和能量的节省。 |
| [^293] | [Pessimism in the Face of Confounders: Provably Efficient Offline Reinforcement Learning in Partially Observable Markov Decision Processes.](http://arxiv.org/abs/2205.13589) | 本文提出了代理变量悲观策略优化 (P3O) 算法，它通过近端因果推断构建悲观置信区间耦合序列解决了部分可观察马尔可夫决策过程中的混淆偏差和最优策略与行为策略之间的分布偏移问题。 |
| [^294] | [Multi-Label Classification of Thoracic Diseases using Dense Convolutional Network on Chest Radiographs.](http://arxiv.org/abs/2202.03583) | 本研究提出了一种基于密集卷积神经网络和GRADCAM的胸部X光疾病多标签诊断模型，获得了在Cardiomegaly条件下最高的AUC得分0.896，并使用热图提高了模型的可解释性。 |

# 详细

[^1]: 具有动态停止的循环Transformer

    Recurrent Transformers with Dynamic Halt

    [https://rss.arxiv.org/abs/2402.00976](https://rss.arxiv.org/abs/2402.00976)

    本文研究了增强Transformer与循环机制的两种方法，并提出了新的扩展和组合方法。在多个诊断任务中进行比较，探索它们的归纳偏好。

    

    本文研究了两种主要方法在增强Transformer与循环机制方面的归纳偏好——（1）类似于Universal Transformers的深度逐层循环方法；和（2）类似于Temporal Latent Bottleneck的分块时态循环方法。此外，我们提出并研究了扩展和组合上述方法的新方式，例如，我们提出了一种基于全局均值的Universal Transformer动态停止机制，并将Universal Transformer的元素融入到Temporal Latent Bottleneck中。我们通过多个诊断任务（如Long Range Arena（LRA），翻转-翻转语言建模，ListOps和逻辑推理）比较了模型并探索了它们的归纳偏好。

    In this paper, we study the inductive biases of two major approaches to augmenting Transformers with a recurrent mechanism - (1) the approach of incorporating a depth-wise recurrence similar to Universal Transformers; and (2) the approach of incorporating a chunk-wise temporal recurrence like Temporal Latent Bottleneck. Furthermore, we propose and investigate novel ways to extend and combine the above methods - for example, we propose a global mean-based dynamic halting mechanism for Universal Transformer and an augmentation of Temporal Latent Bottleneck with elements from Universal Transformer. We compare the models and probe their inductive biases in several diagnostic tasks such as Long Range Arena (LRA), flip-flop language modeling, ListOps, and Logical Inference.
    
[^2]: NeRF-MAE: 自监督三维表示学习中的Masked AutoEncoders

    NeRF-MAE : Masked AutoEncoders for Self Supervised 3D representation Learning for Neural Radiance Fields

    [https://arxiv.org/abs/2404.01300](https://arxiv.org/abs/2404.01300)

    通过使用Masked AutoEncoders，本文提出了NeRF-MAE用于自监督三维表示学习，利用标准的三维Vision Transformers适应NeRF的独特公式，将NeRF的体积网格作为密集输入，以产生有效的三维表示。

    

    由于神经场在计算机视觉和机器人领域的卓越能力，能够理解三维视觉世界，如推断语义、几何和动态等，本文探讨了神经场在从二维图像中密集表示三维场景的自监督预训练，具体使用Masked AutoEncoders的可能性。我们借鉴了将transformers扩展到新数据模态的令人惊讶的成功，利用标准的三维Vision Transformers来适应NeRF的独特公式。我们将NeRF的体积网格作为transformer的密集输入，与其他三维表示（如点云）进行对比，其信息密度可能不均匀，而表示是不规则的。由于将masked autoencoders应用于类似NeRF这样的隐式表示的困难，我们选择提取一个显式的表示。

    arXiv:2404.01300v1 Announce Type: cross  Abstract: Neural fields excel in computer vision and robotics due to their ability to understand the 3D visual world such as inferring semantics, geometry, and dynamics. Given the capabilities of neural fields in densely representing a 3D scene from 2D images, we ask the question: Can we scale their self-supervised pretraining, specifically using masked autoencoders, to generate effective 3D representations from posed RGB images. Owing to the astounding success of extending transformers to novel data modalities, we employ standard 3D Vision Transformers to suit the unique formulation of NeRFs. We leverage NeRF's volumetric grid as a dense input to the transformer, contrasting it with other 3D representations such as pointclouds where the information density can be uneven, and the representation is irregular. Due to the difficulty of applying masked autoencoders to an implicit representation, such as NeRF, we opt for extracting an explicit repres
    
[^3]: CausalChaos!数据集：基于动态视觉场景中更长因果链的全面因果行动问答

    CausalChaos! Dataset for Comprehensive Causal Action Question Answering Over Longer Causal Chains Grounded in Dynamic Visual Scenes

    [https://arxiv.org/abs/2404.01299](https://arxiv.org/abs/2404.01299)

    利用卡通图像构建的CausalChaos!数据集，包含更长因果链的因果问答，通过动态互动和视觉展示挑战性因果关系，为模型提供了更多具挑战性且明确定义的因果关系。

    

    因果视频问答（QA）越来越受到关注，然而现有数据集在因果推理分析方面往往缺乏深度。为了填补这一空白，我们利用卡通的独特属性构建了CausalChaos!，这是一个新颖且具有挑战性的因果问答（Why-QA）数据集，基于标志性的“猫和老鼠”卡通系列。我们的数据集通过周到的问题和多层次答案，包含着嵌入动态互动和视觉中的更长因果链，同时动画原理允许动画师创造定义明确、明了的因果关系。这些因素使模型能够解决更具挑战性但明确定义的因果关系。我们还引入了硬负采样，包括CausalConfusion版本。虽然模型表现良好，但仍有很大改进空间，特别是在开放式答案方面。我们确定了更为先进/明确的因果关系建模和联合建模等改进方向。

    arXiv:2404.01299v1 Announce Type: cross  Abstract: Causal video question answering (QA) has garnered increasing interest, yet existing datasets often lack depth in causal reasoning analysis. To address this gap, we capitalize on the unique properties of cartoons and construct CausalChaos!, a novel, challenging causal Why-QA dataset built upon the iconic "Tom and Jerry" cartoon series. With thoughtful questions and multi-level answers, our dataset contains much longer causal chains embedded in dynamic interactions and visuals, at the same time principles of animation allows animators to create well-defined, unambiguous causal relationships. These factors allow models to solve more challenging, yet well-defined causal relationships. We also introduce hard negative mining, including CausalConfusion version. While models perform well, there is much room for improvement, especially, on open-ended answers. We identify more advanced/explicit causal relationship modeling and joint modeling of 
    
[^4]: 通过可控大语言模型实现安全和帮助平衡的回应

    Towards Safety and Helpfulness Balanced Responses via Controllable Large Language Models

    [https://arxiv.org/abs/2404.01295](https://arxiv.org/abs/2404.01295)

    通过控制大语言模型中的属性，实现安全和帮助之间的平衡，并且可以在不同的应用场景中实现这种平衡。

    

    随着大型语言模型(LLMs)如今变得容易获得，安全和帮助之间的权衡会显著影响用户体验。一个优先考虑安全的模型会导致用户感到较少参与和被协助，而优先考虑帮助性则可能导致伤害。可能的危害包括教授人们如何制造炸弹，向青少年暴露不当内容以及伤害用户的心理健康。在这项工作中，我们提出通过控制LLMs中的两个属性来在不同的用例中平衡安全性和帮助性。我们探讨了不需要额外人员注释的训练无关和微调方法，并分析了控制LLMs中安全性和帮助性的挑战。我们的实验证明了我们的方法可以重置已学习的模型并解锁其可控性。

    arXiv:2404.01295v1 Announce Type: cross  Abstract: As large language models (LLMs) become easily accessible nowadays, the trade-off between safety and helpfulness can significantly impact user experience. A model that prioritizes safety will cause users to feel less engaged and assisted while prioritizing helpfulness will potentially cause harm. Possible harms include teaching people how to build a bomb, exposing youth to inappropriate content, and hurting users' mental health. In this work, we propose to balance safety and helpfulness in diverse use cases by controlling both attributes in LLM. We explore training-free and fine-tuning methods that do not require extra human annotations and analyze the challenges of controlling safety and helpfulness in LLMs. Our experiments demonstrate that our method can rewind a learned model and unlock its controllability.
    
[^5]: 评估文本到图像生成与图像到文本生成

    Evaluating Text-to-Visual Generation with Image-to-Text Generation

    [https://arxiv.org/abs/2404.01291](https://arxiv.org/abs/2404.01291)

    引入VQAScore，利用视觉问答模型生成对齐分数，取得了在图像文本对齐基准上的最先进结果

    

    尽管生成式人工智能取得了显著进展，但由于缺乏有效的度量标准和标准化基准，综合评估仍然具有挑战性。为了解决这个问题，我们引入了VQAScore，使用视觉问答（VQA）模型通过计算对简单的“这幅图表现出了'{文本}'吗？”问题的“是”答案的概率来生成对齐分数。尽管比先前的方法更简单，但使用现成模型计算的VQAScore在许多（8个）图像文本对齐基准上产生了最先进的结果。

    arXiv:2404.01291v1 Announce Type: cross  Abstract: Despite significant progress in generative AI, comprehensive evaluation remains challenging because of the lack of effective metrics and standardized benchmarks. For instance, the widely-used CLIPScore measures the alignment between a (generated) image and text prompt, but it fails to produce reliable scores for complex prompts involving compositions of objects, attributes, and relations. One reason is that text encoders of CLIP can notoriously act as a "bag of words", conflating prompts such as "the horse is eating the grass" with "the grass is eating the horse". To address this, we introduce the VQAScore, which uses a visual-question-answering (VQA) model to produce an alignment score by computing the probability of a "Yes" answer to a simple "Does this figure show '{text}'?" question. Though simpler than prior art, VQAScore computed with off-the-shelf models produces state-of-the-art results across many (8) image-text alignment benc
    
[^6]: 科学论文中LLM使用增加的映射

    Mapping the Increasing Use of LLMs in Scientific Papers

    [https://arxiv.org/abs/2404.01268](https://arxiv.org/abs/2404.01268)

    该论文通过对科学论文中LLM使用增加的映射进行了大规模分析，填补了学术写作中真实LLM修改内容比例缺失的空白。

    

    科学出版通过传播研究成果、促进合作、鼓励可重复性，并确保科学知识随时间可访问、可验证并不断建立，为科学奠定了基础。近年来，人们对多少人在学术写作中使用大型语言模型（LLMs）如ChatGPT及这种工具对全球科学实践可能产生何种影响进行了大量猜测。然而，我们缺乏对学术写作中实质性修改或生成内容的准确度量。为填补这一空白，我们对在arXiv、bioRxiv和自然学报系列期刊上的950,965篇论文（时间跨度从2020年1月至2024年2月）进行了首次系统性、大规模的分析，利用人群级别的统计框架来测量LLM修改内容随时间的盛行程度。我们的统计估计是在语料库级别及i

    arXiv:2404.01268v1 Announce Type: cross  Abstract: Scientific publishing lays the foundation of science by disseminating research findings, fostering collaboration, encouraging reproducibility, and ensuring that scientific knowledge is accessible, verifiable, and built upon over time. Recently, there has been immense speculation about how many people are using large language models (LLMs) like ChatGPT in their academic writing, and to what extent this tool might have an effect on global scientific practices. However, we lack a precise measure of the proportion of academic writing substantially modified or produced by LLMs. To address this gap, we conduct the first systematic, large-scale analysis across 950,965 papers published between January 2020 and February 2024 on the arXiv, bioRxiv, and Nature portfolio journals, using a population-level statistical framework to measure the prevalence of LLM-modified content over time. Our statistical estimation operates on the corpus level and i
    
[^7]: IsoBench：基于同构表示对多模态基础模型进行基准测试

    IsoBench: Benchmarking Multimodal Foundation Models on Isomorphic Representations

    [https://arxiv.org/abs/2404.01266](https://arxiv.org/abs/2404.01266)

    IsoBench提出了一个基准数据集，用于评估基于不同同构表示的多模态基础模型的性能差异，发现大多数模型更偏好文本表示。

    

    当前的基础模型在仅文本或图像和文本输入同时提示时表现出令人印象深刻的能力。但它们的能力是否会根据输入方式而改变呢？在这项工作中，我们提出了一个名为$\textbf{IsoBench}$的基准数据集，其中包含来自四个主要领域的问题: 数学、科学、算法和游戏。每个示例呈现了多个输入的同构表示，如视觉、文本和数学展示。IsoBench提供了细粒度的反馈，以诊断由表示形式造成的性能差距。在各种基础模型中，我们观察到在相同问题上，模型一贯偏好文本表示。最突出的是，在所有IsoBench问题上进行评估时，Claude-3 Opus在提供图像而不是文本时性能下降28.7分；同样，GPT-4 Turbo性能下降18.7分，Gemini Pro下降14.9分。

    arXiv:2404.01266v1 Announce Type: new  Abstract: Current foundation models exhibit impressive capabilities when prompted either with text only or with both image and text inputs. But do their capabilities change depending on the input modality? In this work, we propose $\textbf{IsoBench}$, a benchmark dataset containing problems from four major areas: math, science, algorithms, and games. Each example is presented with multiple $\textbf{isomorphic representations}$ of inputs, such as visual, textual, and mathematical presentations. IsoBench provides fine-grained feedback to diagnose performance gaps caused by the form of the representation. Across various foundation models, we observe that on the same problem, models have a consistent preference towards textual representations. Most prominently, when evaluated on all IsoBench problems, Claude-3 Opus performs 28.7 points worse when provided with images instead of text; similarly, GPT-4 Turbo is 18.7 points worse and Gemini Pro is 14.9 p
    
[^8]: FABLES：评估书籍摘要中的忠实性和内容选择

    FABLES: Evaluating faithfulness and content selection in book-length summarization

    [https://arxiv.org/abs/2404.01261](https://arxiv.org/abs/2404.01261)

    本文首次对LLM生成的虚构书籍摘要进行了忠实性和内容选择的大规模人类评估，建立了FABLES数据集，通过对26本书的3158个声明进行了注释，成功对LLM摘要进行了基于忠实性的排名

    

    虽然长文本大语言模型（LLMs）在技术上可以总结长达100K个标记的书籍，但迄今为止，文档的长度和复杂性阻碍了对忠实性等输入相关方面的评估。本文在虚构书籍的LLM生成摘要上进行了首次大规模人类评估，通过专注于2023或2024年出版的书籍摘要，雇佣在进行注释任务之前已完全阅读每本书的注释者来减少成本和认知负担，从而缓解了数据污染问题。我们收集了FABLES数据集，对26本书的LLM生成摘要中的3158个声明进行了注释，花费了5200美元，这使我们能够基于忠实性对LLM摘要进行排名：Claude-3-Opus在忠实性方面明显优于所有闭源LLMs，而开源的Mixtral与GPT-3.5-Turbo持平。

    arXiv:2404.01261v1 Announce Type: cross  Abstract: While long-context large language models (LLMs) can technically summarize book-length documents (>100K tokens), the length and complexity of the documents have so far prohibited evaluations of input-dependent aspects like faithfulness. In this paper, we conduct the first large-scale human evaluation of faithfulness and content selection on LLM-generated summaries of fictional books. Our study mitigates the issue of data contamination by focusing on summaries of books published in 2023 or 2024, and we hire annotators who have fully read each book prior to the annotation task to minimize cost and cognitive burden. We collect FABLES, a dataset of annotations on 3,158 claims made in LLM-generated summaries of 26 books, at a cost of $5.2K USD, which allows us to rank LLM summarizers based on faithfulness: Claude-3-Opus significantly outperforms all closed-source LLMs, while the open-source Mixtral is on par with GPT-3.5-Turbo. An analysis o
    
[^9]: 将远程传感器与多传感器地理空间基础模型联系起来

    Bridging Remote Sensors with Multisensor Geospatial Foundation Models

    [https://arxiv.org/abs/2404.01260](https://arxiv.org/abs/2404.01260)

    msGFM是一个多传感器地理空间基础模型，能够有效统一四种关键传感器模态的数据，具有处理成对和非成对传感器数据的能力，通过创新的跨传感器预训练方法在蒙版图像建模中合成联合表示，展现出在各种传感器类型下都具有强大性能的综合模型。

    

    在地理空间分析领域，包括光学和微波技术在内的远程传感器的多样性提供了丰富的独特观测能力。鉴于此，我们提出了msGFM，一个多传感器地理空间基础模型，有效地统一了来自四种关键传感器模态的数据。这种集成涵盖了两百万多传感器图像的庞大数据集。msGFM在处理成对和非成对传感器数据方面独具才能。对于来自相同地理位置的数据，我们的模型采用了一种创新的跨传感器预训练方法，在蒙版图像建模中实现联合表示的合成。msGFM结合了四种远程传感器，具有强大的性能，构建了一个适应各种传感器类型的综合模型。msGFM在一系列单传感器和多传感器下游任务中展现出了卓越的性能。

    arXiv:2404.01260v1 Announce Type: cross  Abstract: In the realm of geospatial analysis, the diversity of remote sensors, encompassing both optical and microwave technologies, offers a wealth of distinct observational capabilities. Recognizing this, we present msGFM, a multisensor geospatial foundation model that effectively unifies data from four key sensor modalities. This integration spans an expansive dataset of two million multisensor images. msGFM is uniquely adept at handling both paired and unpaired sensor data. For data originating from identical geolocations, our model employs an innovative cross-sensor pretraining approach in masked image modeling, enabling the synthesis of joint representations from diverse sensors. msGFM, incorporating four remote sensors, upholds strong performance, forming a comprehensive model adaptable to various sensor types. msGFM has demonstrated enhanced proficiency in a range of both single-sensor and multisensor downstream tasks. These include sce
    
[^10]: 语言模型奖励下的视频大型多模态模型直接偏好优化

    Direct Preference Optimization of Video Large Multimodal Models from Language Model Reward

    [https://arxiv.org/abs/2404.01258](https://arxiv.org/abs/2404.01258)

    本研究提出了一种新的框架，利用详细的视频标题作为视频内容的代理，使得语言模型在评分视频问答（QA）预测时能够融入这些信息作为支持证据。

    

    偏好建模技术，如直接偏好优化（DPO），已证明在增强大型语言模型（LLM）的泛化能力方面是有效的。然而，在涉及视频指令跟随的任务中，提供信息丰富的反馈，特别是用于检测生成的响应中的幻觉，仍然是一个重要挑战。先前的研究已经探讨了使用大型多模态模型（LMM）作为奖励模型来指导偏好建模，但它们准确评估生成响应的事实性与对应视频相比的能力尚未得出结论。本文介绍了一个新颖的框架，利用详细的视频标题作为视频内容的代理，使语言模型能够将这些信息作为支持证据来为视频问答（QA）预测打分。我们的方法表现出与OpenAI GPT-4V模型的奖励机制的稳健对齐

    arXiv:2404.01258v1 Announce Type: cross  Abstract: Preference modeling techniques, such as direct preference optimization (DPO), has shown effective in enhancing the generalization abilities of large language model (LLM). However, in tasks involving video instruction-following, providing informative feedback, especially for detecting hallucinations in generated responses, remains a significant challenge. Previous studies have explored using large large multimodal models (LMMs) as reward models to guide preference modeling, but their ability to accurately assess the factuality of generated responses compared to corresponding videos has not been conclusively established. This paper introduces a novel framework that utilizes detailed video captions as a proxy of video content, enabling language models to incorporate this information as supporting evidence for scoring video Question Answering (QA) predictions. Our approach demonstrates robust alignment with OpenAI GPT-4V model's reward mec
    
[^11]: 特征点切片：基于语言驱动的基于物理的场景合成和编辑

    Feature Splatting: Language-Driven Physics-Based Scene Synthesis and Editing

    [https://arxiv.org/abs/2404.01223](https://arxiv.org/abs/2404.01223)

    该研究提出了一种名为Feature Splatting的方法，将基于物理的动态场景合成与源自视觉语言的丰富语义统一起来，实现了高质量的对象分解和基于文本查询的物理特性自动合成。

    

    使用3D高斯原语表示场景在建模静态和动态3D场景的外观方面取得了出色的结果。 然而，许多图形应用程序需要能够操纵对象的外观和物理特性。 我们介绍了一种称为Feature Splatting的方法，它将基于物理的动态场景合成与源于自然语言的丰富语义统一起来。 我们的第一个贡献是一种方法，可以将高质量的，以对象为中心的视觉语言特征提炼成3D高斯，从而利用文本查询实现半自动场景分解。 我们的第二个贡献是一种方法，通过基于粒子的模拟器从静态场景中合成基于物理的动态场景，在此过程中，材料属性通过文本查询自动分配。 我们剔除了在这个流程中使用的关键技术，以阐明挑战和机会。

    arXiv:2404.01223v1 Announce Type: cross  Abstract: Scene representations using 3D Gaussian primitives have produced excellent results in modeling the appearance of static and dynamic 3D scenes. Many graphics applications, however, demand the ability to manipulate both the appearance and the physical properties of objects. We introduce Feature Splatting, an approach that unifies physics-based dynamic scene synthesis with rich semantics from vision language foundation models that are grounded by natural language. Our first contribution is a way to distill high-quality, object-centric vision-language features into 3D Gaussians, that enables semi-automatic scene decomposition using text queries. Our second contribution is a way to synthesize physics-based dynamics from an otherwise static scene using a particle-based simulator, in which material properties are assigned automatically via text queries. We ablate key techniques used in this pipeline, to illustrate the challenge and opportunit
    
[^12]: 将领域微分方程纳入图卷积网络以降低泛化差异

    Incorporating Domain Differential Equations into Graph Convolutional Networks to Lower Generalization Discrepancy

    [https://arxiv.org/abs/2404.01217](https://arxiv.org/abs/2404.01217)

    将领域微分方程纳入图卷积网络可提高其对不匹配训练和测试数据的稳健性。

    

    确保时间序列预测的准确性和稳健性对许多应用至关重要，从城市规划到疫情管理。本工作表明，在涵盖所有时空模式的充分训练数据的情况下，现有的深度学习模型可以做出相当准确的预测。然而，当训练数据来自与测试数据不同环境（例如，正常天交通模式与自然灾害后交通模式）时，现有方法会失败。在本工作中，我们展示了解决这一挑战的一种方法，即将领域微分方程纳入图卷积网络（GCNs）中。

    arXiv:2404.01217v1 Announce Type: cross  Abstract: Ensuring both accuracy and robustness in time series prediction is critical to many applications, ranging from urban planning to pandemic management. With sufficient training data where all spatiotemporal patterns are well-represented, existing deep-learning models can make reasonably accurate predictions. However, existing methods fail when the training data are drawn from different circumstances (e.g., traffic patterns on regular days) compared to test data (e.g., traffic patterns after a natural disaster). Such challenges are usually classified under domain generalization. In this work, we show that one way to address this challenge in the context of spatiotemporal prediction is by incorporating domain differential equations into Graph Convolutional Networks (GCNs). We theoretically derive conditions where GCNs incorporating such domain differential equations are robust to mismatched training and testing data compared to baseline do
    
[^13]: 通过弛豫神经网络捕捉激波

    Capturing Shock Waves by Relaxation Neural Networks

    [https://arxiv.org/abs/2404.01163](https://arxiv.org/abs/2404.01163)

    RelaxNN框架通过弛豫系统解决了在双曲系统解中出现的激波难题，缓解了PINN框架在训练过程中的损失冲突。

    

    本文提出了一种神经网络框架来解决非线性双曲系统。该框架名为弛豫神经网络（RelaxNN），是物理信息神经网络（PINN）的一种简单可扩展的扩展。研究表明，典型的PINN框架很难处理在双曲系统解中出现的激波，最终导致基于梯度下降的优化在训练过程中失败。弛豫系统为不连续解提供了平滑的渐近解，期望宏观问题能够从微观角度得到解决。基于弛豫系统，RelaxNN框架缓解了PINN框架训练过程中损失冲突的问题。除在数值模拟中展示出的显著结果外，大多数加速技术和改进策略都针对标准PINN框架。

    arXiv:2404.01163v1 Announce Type: cross  Abstract: In this paper, we put forward a neural network framework to solve the nonlinear hyperbolic systems. This framework, named relaxation neural networks(RelaxNN), is a simple and scalable extension of physics-informed neural networks(PINN). It is shown later that a typical PINN framework struggles to handle shock waves that arise in hyperbolic systems' solutions. This ultimately results in the failure of optimization that is based on gradient descent in the training process. Relaxation systems provide a smooth asymptotic to the discontinuity solution, under the expectation that macroscopic problems can be solved from a microscopic perspective. Based on relaxation systems, the RelaxNN framework alleviates the conflict of losses in the training process of the PINN framework. In addition to the remarkable results demonstrated in numerical simulations, most of the acceleration techniques and improvement strategies aimed at the standard PINN fr
    
[^14]: SyncMask：面向时尚视觉语言预训练的同步注意力遮罩

    SyncMask: Synchronized Attentional Masking for Fashion-centric Vision-Language Pretraining

    [https://arxiv.org/abs/2404.01156](https://arxiv.org/abs/2404.01156)

    提出了同步注意力遮罩（SyncMask），用于解决时尚领域图像和文本信息不匹配的问题，可以准确对齐细粒度的视觉和文本特征。

    

    视觉语言模型（VLMs）通过大规模配对数据集在跨模态理解方面取得了重要进展。然而，在时尚领域，数据集中经常存在图像和文本传达信息之间的差异。我们提出了同步注意力遮罩（SyncMask），其生成的遮罩指示了图像块和单词标记在图像和文本中同时出现的位置。

    arXiv:2404.01156v1 Announce Type: cross  Abstract: Vision-language models (VLMs) have made significant strides in cross-modal understanding through large-scale paired datasets. However, in fashion domain, datasets often exhibit a disparity between the information conveyed in image and text. This issue stems from datasets containing multiple images of a single fashion item all paired with one text, leading to cases where some textual details are not visible in individual images. This mismatch, particularly when non-co-occurring elements are masked, undermines the training of conventional VLM objectives like Masked Language Modeling and Masked Image Modeling, thereby hindering the model's ability to accurately align fine-grained visual and textual features. Addressing this problem, we propose Synchronized attentional Masking (SyncMask), which generate masks that pinpoint the image patches and word tokens where the information co-occur in both image and text. This synchronization is accom
    
[^15]: 揭示文本嵌入在文本到图像扩散模型中的作用

    Uncovering the Text Embedding in Text-to-Image Diffusion Models

    [https://arxiv.org/abs/2404.01154](https://arxiv.org/abs/2404.01154)

    本文揭示了文本嵌入在文本到图像扩散模型中的作用，并且提出了关于单词嵌入及其上下文相关性的重要见解，为学习免费图像编辑提供了指导原则。

    

    输入文本与生成图像之间的对应关系表现为不透明性，即轻微的文本修改会导致生成图像的重大偏差。而文本嵌入作为文本与图像之间的关键中间体仍然相对未被深入探讨。本文通过深入探讨文本嵌入空间，释放其对可控图像编辑和可解释的语义方向属性的能力，在无需学习的框架内。具体来说，我们发现了两个关键见解，即单词嵌入的重要性以及它们在文本嵌入中的上下文相关性，为无需学习的图像编辑提供了指导原则。此外，我们发现文本嵌入固有地具有多样的语义潜力，并通过奇异值分解（SVD）的视角进一步揭示了这一属性。这些揭示出来的特性提供了实用价值

    arXiv:2404.01154v1 Announce Type: cross  Abstract: The correspondence between input text and the generated image exhibits opacity, wherein minor textual modifications can induce substantial deviations in the generated image. While, text embedding, as the pivotal intermediary between text and images, remains relatively underexplored. In this paper, we address this research gap by delving into the text embedding space, unleashing its capacity for controllable image editing and explicable semantic direction attributes within a learning-free framework. Specifically, we identify two critical insights regarding the importance of per-word embedding and their contextual correlations within text embedding, providing instructive principles for learning-free image editing. Additionally, we find that text embedding inherently possesses diverse semantic potentials, and further reveal this property through the lens of singular value decomposition (SVD). These uncovered properties offer practical uti
    
[^16]: 条件感知神经网络用于可控图像生成

    Condition-Aware Neural Network for Controlled Image Generation

    [https://arxiv.org/abs/2404.01143](https://arxiv.org/abs/2404.01143)

    提出了一种条件感知神经网络（CAN）用于图像生成模型，通过动态调整神经网络的权重来控制图像生成过程，在ImageNet和COCO上测试表明，CAN与EfficientViT（CaT）相结合取得了显著的改进。

    

    我们提出了一种新的方法，称为条件感知神经网络（CAN），用于向图像生成模型添加控制。与先前的条件控制方法并行，CAN通过动态操纵神经网络的权重来控制图像生成过程。这是通过引入一个条件感知权重生成模块来实现的，根据输入条件为卷积/线性层生成有条件的权重。我们在ImageNet上对类别有条件的图像生成和在COCO上对文本到图像的生成进行了CAN测试。CAN始终为扩散变换器模型提供显着改进，包括DiT和UViT。特别是，CAN与EfficientViT（CaT）相结合，在ImageNet 512x512上实现了2.78 FID，超过了DiT-XL/2，同时每个采样步骤需要的MAC数量减少了52倍。

    arXiv:2404.01143v1 Announce Type: cross  Abstract: We present Condition-Aware Neural Network (CAN), a new method for adding control to image generative models. In parallel to prior conditional control methods, CAN controls the image generation process by dynamically manipulating the weight of the neural network. This is achieved by introducing a condition-aware weight generation module that generates conditional weight for convolution/linear layers based on the input condition. We test CAN on class-conditional image generation on ImageNet and text-to-image generation on COCO. CAN consistently delivers significant improvements for diffusion transformer models, including DiT and UViT. In particular, CAN combined with EfficientViT (CaT) achieves 2.78 FID on ImageNet 512x512, surpassing DiT-XL/2 while requiring 52x fewer MACs per sampling step.
    
[^17]: 利用认知增强技术增强SLM的推理能力

    Enhancing Reasoning Capacity of SLM using Cognitive Enhancement

    [https://arxiv.org/abs/2404.01135](https://arxiv.org/abs/2404.01135)

    利用认知增强技术，本研究提出了一种在网络调查和数字取证中提高SLM推理能力的方法，其中通过处理本地数据并减少参数数量来满足安全性和数据隐私性需求。

    

    大语言模型(LLMs)已被应用于自动化网络安全活动和流程，包括网络调查和数字取证。然而，将这些模型用于网络调查和数字取证时，应该考虑问责制和安全性。问责制确保模型能够提供可解释的推理和结果。这些信息可以通过显式提示请求来提取。对于安全考虑，在数据处理过程中至关重要的是要解决所涉数据的隐私和保密性。一种处理这个考虑的方法是在本地使用模型的本地实例对数据进行处理。由于本地可用资源（主要是内存和GPU容量）的限制，通常会使用较小的大型语言模型(SLM)。这些SLM与LLMs相比，具有明显较少的参数。然而，这种大小缩减…

    arXiv:2404.01135v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have been applied to automate cyber security activities and processes including cyber investigation and digital forensics. However, the use of such models for cyber investigation and digital forensics should address accountability and security considerations. Accountability ensures models have the means to provide explainable reasonings and outcomes. This information can be extracted through explicit prompt requests. For security considerations, it is crucial to address privacy and confidentiality of the involved data during data processing as well. One approach to deal with this consideration is to have the data processed locally using a local instance of the model. Due to limitations of locally available resources, namely memory and GPU capacities, a Smaller Large Language Model (SLM) will typically be used. These SLMs have significantly fewer parameters compared to the LLMs. However, such size reductions
    
[^18]: GOV-REK：用于设计鲁棒多智能体强化学习系统的受管奖励工程核

    GOV-REK: Governed Reward Engineering Kernels for Designing Robust Multi-Agent Reinforcement Learning Systems

    [https://arxiv.org/abs/2404.01131](https://arxiv.org/abs/2404.01131)

    GOV-REK提出了一种动态为多智能体强化学习系统中的代理分配奖励分布的方法，并通过治理内核利用状态或联合行动空间的结构，以解决奖励工程努力无法转化的问题。

    

    对于多智能体强化学习系统（MARLS），问题的制定通常需要投入大量奖励工程工作，针对特定问题。然而，这种努力通常无法转化为其他问题；更糟糕的是，当系统动态发生 drastica 改变时，这种努力就会被浪费。在稀疏奖励场景中，有意义的启发式可以帮助策略收敛任务。我们提出了 GOVerned Reward Engineering Kernels (GOV-REK)，在 MARLS 的学习阶段动态地为代理分配奖励分布。我们还引入了治理内核，利用状态或联合行动空间中的基础结构来分配有意义的代理奖励分布。在代理学习阶段，它使用类似 Hyperband 的算法迭代地探索不同的奖励分布配置，以在问题-不可知 ma 中学习理想的代理奖励模型。

    arXiv:2404.01131v1 Announce Type: cross  Abstract: For multi-agent reinforcement learning systems (MARLS), the problem formulation generally involves investing massive reward engineering effort specific to a given problem. However, this effort often cannot be translated to other problems; worse, it gets wasted when system dynamics change drastically. This problem is further exacerbated in sparse reward scenarios, where a meaningful heuristic can assist in the policy convergence task. We propose GOVerned Reward Engineering Kernels (GOV-REK), which dynamically assign reward distributions to agents in MARLS during its learning stage. We also introduce governance kernels, which exploit the underlying structure in either state or joint action space for assigning meaningful agent reward distributions. During the agent learning stage, it iteratively explores different reward distribution configurations with a Hyperband-like algorithm to learn ideal agent reward models in a problem-agnostic ma
    
[^19]: 医疗视觉提示（MVP）：通用高质量医学图像分割统一框架

    Medical Visual Prompting (MVP): A Unified Framework for Versatile and High-Quality Medical Image Segmentation

    [https://arxiv.org/abs/2404.01127](https://arxiv.org/abs/2404.01127)

    提出了医疗视觉提示（MVP）框架，结合自然语言处理的预训练和提示概念，利用三个关键组件来解决医学图像分割中病变形状信息丢失和手动标记成本高的挑战

    

    淋巴瘤区域的准确分割对于各种疾病的临床诊断和治疗至关重要。尽管深度卷积网络在医学图像分割中取得了令人满意的结果，但由于连续卷积和下采样造成了损失病变形状信息、以及手动标记不同形状和大小病变的高成本等挑战。为了解决这些问题，我们提出了一种新颖的医疗视觉提示（MVP）框架，利用自然语言处理（NLP）中的预训练和提示概念。该框架利用三个关键组件：基于超像素的提示（SPGP）用于对输入图像进行超像素处理，图像嵌入引导提示（IEGP）用于冻结补丁嵌入并与超像素合并以提供视觉提示，以及自适应注意机制引导提示（AAGP）用于确定提示内容并有效地调整所有层

    arXiv:2404.01127v1 Announce Type: cross  Abstract: Accurate segmentation of lesion regions is crucial for clinical diagnosis and treatment across various diseases. While deep convolutional networks have achieved satisfactory results in medical image segmentation, they face challenges such as loss of lesion shape information due to continuous convolution and downsampling, as well as the high cost of manually labeling lesions with varying shapes and sizes. To address these issues, we propose a novel medical visual prompting (MVP) framework that leverages pre-training and prompting concepts from natural language processing (NLP). The framework utilizes three key components: Super-Pixel Guided Prompting (SPGP) for superpixelating the input image, Image Embedding Guided Prompting (IEGP) for freezing patch embedding and merging with superpixels to provide visual prompts, and Adaptive Attention Mechanism Guided Prompting (AAGP) for pinpointing prompt content and efficiently adapting all layer
    
[^20]: 你的“安全”数据中有什么？：识别破坏安全性的良性数据

    What's in Your "Safe" Data?: Identifying Benign Data that Breaks Safety

    [https://arxiv.org/abs/2404.01099](https://arxiv.org/abs/2404.01099)

    通过双向锚定方法，识别那些在微调后更可能降低模型安全性的良性数据子集，提高模型对有害请求的响应率。

    

    当前的大型语言模型（LLMs），即使经过调整以确保安全性和对齐性，也容易被越狱。一些研究表明，只是进一步使用良性数据（即没有有害内容的数据）对一个对齐模型进行微调，会导致安全性大幅下降。我们深入探讨良性微调不经意间导致越狱的数据中心方面。首先，我们通过两种视角表征微调数据：表示和梯度空间。此外，我们提出了一种双向锚定方法，该方法优先考虑靠近有害示例并远离良性示例的数据点。通过这样做，我们的方法有效地识别出更有可能在微调后降低模型安全性的良性数据子集。仅仅训练100个这些看似良性的数据点，就可以使微调模型肯定地回应超过70％的被测试的有害请求，相比之下，...

    arXiv:2404.01099v1 Announce Type: cross  Abstract: Current Large Language Models (LLMs), even those tuned for safety and alignment, are susceptible to jailbreaking. Some have found that just further fine-tuning an aligned model with benign data (i.e., data without harmful content) surprisingly leads to substantial degradation in safety. We delve into the data-centric aspects of why benign fine-tuning inadvertently contributes to jailbreaking. First, we represent fine-tuning data through two lenses: representation and gradient spaces. Furthermore, we propose a bi-directional anchoring method that prioritizes data points that are close to harmful examples and distant from benign ones. By doing so, our approach effectively identifies subsets of benign data that are more likely to degrade the model's safety after fine-tuning. Training on just 100 of these seemingly benign datapoints can lead to the fine-tuned model affirmatively responding to > 70% of tested harmful requests, compared to <
    
[^21]: 高保真虚拟试穿的保持纹理扩散模型

    Texture-Preserving Diffusion Models for High-Fidelity Virtual Try-On

    [https://arxiv.org/abs/2404.01089](https://arxiv.org/abs/2404.01089)

    我们提出了一种Texture-Preserving Diffusion（TPD）模型，通过在空间维度连接遮罩人物和参考服装图像，增强了虚拟试穿的保真度，同时不引入额外的图像编码器。

    

    图像虚拟试穿是在线购物中越来越重要的任务。它旨在合成特定人物穿着指定服装的图像。最近，基于扩散模型的方法变得流行起来，因为它们在图像合成任务中表现出色。然而，这些方法通常使用额外的图像编码器，并依赖交叉注意机制进行从服装到人物图像的纹理传输，这影响了试穿的效率和保真度。为了解决这些问题，我们提出了一种保持纹理扩散（TPD）模型进行虚拟试穿，它增强了结果的保真度并不引入额外的图像编码器。因此，我们从两个方面做出了贡献。首先，我们建议沿空间维度连接遮罩人物和参考服装图像，并利用生成的图像作为扩散模型去噪UNet的输入。

    arXiv:2404.01089v1 Announce Type: cross  Abstract: Image-based virtual try-on is an increasingly important task for online shopping. It aims to synthesize images of a specific person wearing a specified garment. Diffusion model-based approaches have recently become popular, as they are excellent at image synthesis tasks. However, these approaches usually employ additional image encoders and rely on the cross-attention mechanism for texture transfer from the garment to the person image, which affects the try-on's efficiency and fidelity. To address these issues, we propose an Texture-Preserving Diffusion (TPD) model for virtual try-on, which enhances the fidelity of the results and introduces no additional image encoders. Accordingly, we make contributions from two aspects. First, we propose to concatenate the masked person and reference garment images along the spatial dimension and utilize the resulting image as the input for the diffusion model's denoising UNet. This enables the orig
    
[^22]: AILS-NTUA参加SemEval-2024任务9：破解脑筋急转弯：变压器模型用于横向思维谜题

    AILS-NTUA at SemEval-2024 Task 9: Cracking Brain Teasers: Transformer Models for Lateral Thinking Puzzles

    [https://arxiv.org/abs/2404.01084](https://arxiv.org/abs/2404.01084)

    本研究通过调整多种预训练变压器模型，在SemEval-2024任务9的脑筋急转弯竞赛中取得了竞争力十足的表现，在句子谜题和单词谜题的评估中，最佳提交的准确率分别超过了最佳神经基线ChatGPT超过20%和30%。

    

    在本文中，我们概述了我们在SemEval-2024任务9竞赛中的提交:“BRAINTEASER:一个挑战常识的新任务”。我们参与了子任务A-句子谜题和子任务B-单词谜题。我们通过微调评估了大量不同尺寸的预训练变压器模型。随后，我们对它们的得分和响应进行了分析，以帮助未来的研究人员有效地理解和利用这些模型。我们排名靠前的方法在两个子任务的竞赛排行榜上获得了有竞争力的位置。在评估阶段，我们最好的提交在句子谜题中获得了81.7%的平均准确率，在单词谜题中获得了85.4%的准确率，分别比最佳神经基线（ChatGPT）高出超过20%和30%。

    arXiv:2404.01084v1 Announce Type: cross  Abstract: In this paper, we outline our submission for the SemEval-2024 Task 9 competition: 'BRAINTEASER: A Novel Task Defying Common Sense'. We engage in both sub-tasks: Sub-task A-Sentence Puzzle and Sub-task B-Word Puzzle. We evaluate a plethora of pre-trained transformer-based language models of different sizes through fine-tuning. Subsequently, we undertake an analysis of their scores and responses to aid future researchers in understanding and utilizing these models effectively. Our top-performing approaches secured competitive positions on the competition leaderboard across both sub-tasks. In the evaluation phase, our best submission attained an average accuracy score of 81.7% in the Sentence Puzzle, and 85.4% in the Word Puzzle, significantly outperforming the best neural baseline (ChatGPT) by more than 20% and 30% respectively.
    
[^23]: 以诚信推动人工智能发展：神经机器翻译中的伦理挑战与解决方案

    Advancing AI with Integrity: Ethical Challenges and Solutions in Neural Machine Translation

    [https://arxiv.org/abs/2404.01070](https://arxiv.org/abs/2404.01070)

    本文研究了神经机器翻译中的伦理挑战，并通过实证研究和文献综述确定和解决了数据处理、隐私、数据所有权等方面的伦理问题，为确保AI模型的公平性和文化敏感性提供了解决方案。

    

    本文探讨了人工智能在神经机器翻译（NMT）系统中的伦理挑战，强调开发者确保公平性和文化敏感性的必要性。我们调查了NMT中AI模型的伦理素养，审视了NMT开发的每个阶段的伦理考虑，包括数据处理、隐私、数据所有权和同意。我们通过实证研究确定并解决了伦理问题。这些问题包括利用Transformer模型进行卢干达语-英语翻译，以及利用句子迷你批处理提高效率。我们还进行了细化数据标记技术和对卢干达语和英语社交媒体内容进行BERT和Longformer模型的微调等补充研究。我们的第二种方法是来自Google Scholar等数据库和GitHub等平台的文献综述。此外，本文还探讨了AI系统与相关责任之间的分配。

    arXiv:2404.01070v1 Announce Type: cross  Abstract: This paper addresses the ethical challenges of Artificial Intelligence in Neural Machine Translation (NMT) systems, emphasizing the imperative for developers to ensure fairness and cultural sensitivity. We investigate the ethical competence of AI models in NMT, examining the Ethical considerations at each stage of NMT development, including data handling, privacy, data ownership, and consent. We identify and address ethical issues through empirical studies. These include employing Transformer models for Luganda-English translations and enhancing efficiency with sentence mini-batching. And complementary studies that refine data labeling techniques and fine-tune BERT and Longformer models for analyzing Luganda and English social media content. Our second approach is a literature review from databases such as Google Scholar and platforms like GitHub. Additionally, the paper probes the distribution of responsibility between AI systems and 
    
[^24]: 正则化的最佳-N采样以减轻语言模型对齐中的奖励欺骗问题

    Regularized Best-of-N Sampling to Mitigate Reward Hacking for Language Model Alignment

    [https://arxiv.org/abs/2404.01054](https://arxiv.org/abs/2404.01054)

    提出了Regularized Best-of-N (RBoN)，通过引入接近性项来减轻奖励欺骗，提高了算法在解码时与人类偏好对齐的效果。

    

    Best-of-N (BoN)采样与奖励模型已被证明是一种有效的策略，用于在解码时将大型语言模型(LLMs)与人类偏好对齐。然而，BoN采样容易受到奖励欺骗问题的影响。为了防止奖励欺骗，我们提出了一种名为Regularized Best-of-N (RBoN)的变体，通过在响应选择中结合接近性项来减轻奖励欺骗，类似于偏好学习技术。

    arXiv:2404.01054v1 Announce Type: cross  Abstract: Best-of-N (BoN) sampling with a reward model has been shown to be an effective strategy for aligning Large Language Models (LLMs) to human preferences at the time of decoding. BoN sampling is susceptible to a problem known as reward hacking. Because the reward model is an imperfect proxy for the true objective, over-optimizing its value can compromise its performance on the true objective. A common solution to prevent reward hacking in preference learning techniques is to optimize a reward using proximity regularization (e.g., KL regularization), which ensures that the language model remains close to the reference model. In this research, we propose Regularized Best-of-N (RBoN), a variant of BoN that aims to mitigate reward hacking by incorporating a proximity term in response selection, similar to preference learning techniques. We evaluate two variants of RBoN on the AlpacaFarm dataset and find that they outperform BoN, especially wh
    
[^25]: LLM可以在不透露私人信息的情况下获得其他LLM的帮助吗？

    Can LLMs get help from other LLMs without revealing private information?

    [https://arxiv.org/abs/2404.01041](https://arxiv.org/abs/2404.01041)

    本研究展示了在级联系统中运用隐私保护技术的可行性，以减少在查询远程模型时泄漏私人信息的风险，并引入了两个隐私度量。

    

    级联是一种常见类型的机器学习系统，其中如果本地模型无法单独准确标记用户数据，则可以查询一个大型的远程模型。对于大型语言模型（LLMs），由于其在显著降低推断成本的同时保持任务性能的能力，服务堆栈越来越多地使用级联。然而，在本地模型可以访问敏感数据的情况下应用级联系统构成用户的重大隐私风险，因为这些数据可能被转发到远程模型。在这项工作中，我们展示了在此类设置中应用级联系统的可行性，方法是为本地模型配备隐私保护技术，从而减少访问远程模型时泄漏私人信息的风险。为了量化此类设置中的信息泄漏，我们引入了两个隐私度量。然后，我们提出了一个利用最近引入的社交学习范式的系统

    arXiv:2404.01041v1 Announce Type: cross  Abstract: Cascades are a common type of machine learning systems in which a large, remote model can be queried if a local model is not able to accurately label a user's data by itself. Serving stacks for large language models (LLMs) increasingly use cascades due to their ability to preserve task performance while dramatically reducing inference costs. However, applying cascade systems in situations where the local model has access to sensitive data constitutes a significant privacy risk for users since such data could be forwarded to the remote model. In this work, we show the feasibility of applying cascade systems in such setups by equipping the local model with privacy-preserving techniques that reduce the risk of leaking private information when querying the remote model. To quantify information leakage in such setups, we introduce two privacy measures. We then propose a system that leverages the recently introduced social learning paradigm 
    
[^26]: 在生成人工智能工具时代的高等教育评估实践

    Higher education assessment practice in the era of generative AI tools

    [https://arxiv.org/abs/2404.01036](https://arxiv.org/abs/2404.01036)

    该研究评估了生成人工智能工具对高等教育评估实践的影响，发现这些工具展示了学科知识、问题解决、分析能力等技能，但在不道德使用时可能限制学习，同时也揭示了部分学科评估中这些工具的局限性。

    

    高等教育（HE）部门对每个国家的经济和社会都有益处，但它们的贡献受到生成人工智能（GenAI）等先进技术的挑战。本文全面评估了GenAI工具对评估和教学实践的影响，并随后讨论了潜在影响。研究使用了数据科学、数据分析和建筑管理学科的三种评估工具进行实验。研究发现有两个方面：首先，结果显示GenAI工具展示了学科知识、问题解决能力、分析能力、批判性思维和表达能力，因此在不道德使用时可能会限制学习。其次，某些学科评估的设计揭示了GenAI工具的局限性。根据研究结果，我们提出了关于如何在高等教育中利用人工智能工具进行教学与学习的建议。

    arXiv:2404.01036v1 Announce Type: cross  Abstract: The higher education (HE) sector benefits every nation's economy and society at large. However, their contributions are challenged by advanced technologies like generative artificial intelligence (GenAI) tools. In this paper, we provide a comprehensive assessment of GenAI tools towards assessment and pedagogic practice and, subsequently, discuss the potential impacts. This study experimented using three assessment instruments from data science, data analytics, and construction management disciplines. Our findings are two-fold: first, the findings revealed that GenAI tools exhibit subject knowledge, problem-solving, analytical, critical thinking, and presentation skills and thus can limit learning when used unethically. Secondly, the design of the assessment of certain disciplines revealed the limitations of the GenAI tools. Based on our findings, we made recommendations on how AI tools can be utilised for teaching and learning in HE.
    
[^27]: 文本到图像生成中的偏见调查：定义、评估和缓解

    Survey of Bias In Text-to-Image Generation: Definition, Evaluation, and Mitigation

    [https://arxiv.org/abs/2404.01030](https://arxiv.org/abs/2404.01030)

    该调查综述了文本到图像生成中的偏见问题，重点讨论了性别、肤色和地域文化这些方面，旨在帮助理解当前进展和研究空白。

    

    最近强大的大型模型，如OpenAI的DALLE-3和Google的Gemini，使得用户能够从文本提示生成高质量图像。然而，即使是简单的提示也可能导致文本到图像模型在生成的图像中展现明显的社会偏见。这种偏见可能会导致社会中的分配和代表性伤害，进一步边缘化少数群体。鉴于这一问题，最近有大量研究致力于调查文本到图像系统中偏见的不同维度。然而，对这些研究的全面回顾仍然缺乏，阻碍了对当前进展和研究空白的系统性理解。我们提出了关于文本到图像生成模型中偏见的第一次广泛调查。在这项调查中，我们回顾了先前关于偏见维度的研究：性别、肤色和地域文化。

    arXiv:2404.01030v1 Announce Type: cross  Abstract: The recent advancement of large and powerful models with Text-to-Image (T2I) generation abilities -- such as OpenAI's DALLE-3 and Google's Gemini -- enables users to generate high-quality images from textual prompts. However, it has become increasingly evident that even simple prompts could cause T2I models to exhibit conspicuous social bias in generated images. Such bias might lead to both allocational and representational harms in society, further marginalizing minority groups. Noting this problem, a large body of recent works has been dedicated to investigating different dimensions of bias in T2I systems. However, an extensive review of these studies is lacking, hindering a systematic understanding of current progress and research gaps. We present the first extensive survey on bias in T2I generative models. In this survey, we review prior studies on dimensions of bias: Gender, Skintone, and Geo-Culture. Specifically, we discuss how 
    
[^28]: 源感知训练使语言模型具备知识归因能力

    Source-Aware Training Enables Knowledge Attribution in Language Models

    [https://arxiv.org/abs/2404.01019](https://arxiv.org/abs/2404.01019)

    源感知训练使语言模型具备知识归因能力，进而增强了其透明度、可解释性和可验证性。

    

    大型语言模型（LLMs）在预训练期间学到了大量知识，但往往对此类知识的来源毫不在意。本文研究了内在源引用问题，要求LLMs引用支持生成响应的预训练来源。内在源引用可以增强LLMs的透明度、可解释性和可验证性。为赋予LLMs这种能力，我们探索了源感知训练——一个后预训练配方，包括（i）训练LLMs将唯一源文档标识符与每个文档中的知识关联起来，然后（ii）进行指示调整，教导LLMs在被提示时引用支持的预训练来源。源感知训练可以轻松应用于即插即用的预训练LLMs，并与现有的预训练/微调框架的差异最小。通过对精心策划的数据进行实验，我们展示了我们的训练配方可以实现

    arXiv:2404.01019v1 Announce Type: cross  Abstract: Large language models (LLMs) learn a vast amount of knowledge during pretraining, but they are often oblivious to the source(s) of such knowledge. We investigate the problem of intrinsic source citation, where LLMs are required to cite the pretraining source supporting a generated response. Intrinsic source citation can enhance LLM transparency, interpretability, and verifiability. To give LLMs such ability, we explore source-aware training -- a post pretraining recipe that involves (i) training the LLM to associate unique source document identifiers with the knowledge in each document, followed by (ii) an instruction-tuning to teach the LLM to cite a supporting pretraining source when prompted. Source-aware training can easily be applied to pretrained LLMs off the shelf, and diverges minimally from existing pretraining/fine-tuning frameworks. Through experiments on carefully curated data, we demonstrate that our training recipe can en
    
[^29]: Teeth-SEG: 基于人类先验知识的牙齿治疗高效实例分割框架

    Teeth-SEG: An Efficient Instance Segmentation Framework for Orthodontic Treatment based on Anthropic Prior Knowledge

    [https://arxiv.org/abs/2404.01013](https://arxiv.org/abs/2404.01013)

    提出了一种基于ViT的TeethSEG框架，包括多尺度聚合模块和人类先验知识层，以解决牙齿形状差异、位置变化和异常问题。

    

    牙齿在2D图像中的定位、分割和标注在现代牙科学中具有巨大潜力，可以增强牙科诊断、治疗规划以及口腔健康的基于人群的研究。然而，由于一些牙齿形状之间的细微差异（例如，上颌第一前磨牙和第二前磨牙）、牙齿在受试者间的位置和形状变化以及牙齿异常（例如，龋齿和缺失）的存在，通用实例分割框架效能有限。为了解决这些问题，我们提出了一种基于ViT的框架，名为TeethSEG，它由堆叠的多尺度聚合（MSA）模块和一个人类先验知识（APK）层组成。具体而言，为了构建这两个模块，我们设计了1) 一种独特的基于排列的上采样器，以确保高效性的同时建立清晰的分割边界，并具有2) 多头自/交叉门控层来强调特定的语义。

    arXiv:2404.01013v1 Announce Type: cross  Abstract: Teeth localization, segmentation, and labeling in 2D images have great potential in modern dentistry to enhance dental diagnostics, treatment planning, and population-based studies on oral health. However, general instance segmentation frameworks are incompetent due to 1) the subtle differences between some teeth' shapes (e.g., maxillary first premolar and second premolar), 2) the teeth's position and shape variation across subjects, and 3) the presence of abnormalities in the dentition (e.g., caries and edentulism). To address these problems, we propose a ViT-based framework named TeethSEG, which consists of stacked Multi-Scale Aggregation (MSA) blocks and an Anthropic Prior Knowledge (APK) layer. Specifically, to compose the two modules, we design 1) a unique permutation-based upscaler to ensure high efficiency while establishing clear segmentation boundaries with 2) multi-head self/cross-gating layers to emphasize particular semanti
    
[^30]: 使用大型语言模型生成的相关性判断来预测查询性能

    Query Performance Prediction using Relevance Judgments Generated by Large Language Models

    [https://arxiv.org/abs/2404.01012](https://arxiv.org/abs/2404.01012)

    提出了一种使用自动生成的相关性判断的查询性能预测框架，能够解决先前方法中对不同IR评估指标准确性和解释性的限制。

    

    查询性能预测（QPP）旨在估计搜索系统对查询的检索质量，而无需人工相关性判断。先前的QPP方法通常返回单个标量值，并不要求预测值接近特定的信息检索（IR）评估指标，从而导致以下某些缺点：（i）单个标量无法准确表示不同的IR评估指标，特别是当度量不高度相关时，（ii）单个标量限制了QPP方法的可解释性，因为仅使用标量无法解释QPP结果。为解决这些问题，我们提出了一个使用自动生成的相关性判断的QPP框架（QPP-GenRE），将QPP分解为独立的子任务，即对排名列表中每个项目对给定查询的相关性进行判断。这样我们可以使用生成的相关性判断来预测任何IR评估指标。

    arXiv:2404.01012v1 Announce Type: cross  Abstract: Query performance prediction (QPP) aims to estimate the retrieval quality of a search system for a query without human relevance judgments. Previous QPP methods typically return a single scalar value and do not require the predicted values to approximate a specific information retrieval (IR) evaluation measure, leading to certain drawbacks: (i) a single scalar is insufficient to accurately represent different IR evaluation measures, especially when metrics do not highly correlate, and (ii) a single scalar limits the interpretability of QPP methods because solely using a scalar is insufficient to explain QPP results. To address these issues, we propose a QPP framework using automatically generated relevance judgments (QPP-GenRE), which decomposes QPP into independent subtasks of judging the relevance of each item in a ranked list to a given query. This allows us to predict any IR evaluation measure using the generated relevance judgment
    
[^31]: LLM-RadJudge: 实现X射线报告生成的放射科医师级评估

    LLM-RadJudge: Achieving Radiologist-Level Evaluation for X-Ray Report Generation

    [https://arxiv.org/abs/2404.00998](https://arxiv.org/abs/2404.00998)

    该研究提出了一个使用大型语言模型进行放射学报告评估的新框架，通过GPT-4实现了与放射科医师评估一致性接近的效果，同时利用知识蒸馏训练的较小模型也达到了类似的评估能力。

    

    在放射学AI的发展中，评估生成的放射学报告至关重要，但现有指标无法反映任务的临床要求。本研究提出了一种新的评估框架，使用大型语言模型（LLMs）来比较放射学报告以进行评估。我们比较了各种LLMs的性能，并证明，当使用GPT-4时，我们提出的指标实现了接近放射科医师评估一致性的表现。此外，为了降低成本并提高可访问性，使该方法实用化，我们利用LLM评估结果构建数据集，并进行知识蒸馏以训练一个较小的模型。蒸馏模型实现了与GPT-4相当的评估能力。我们的框架和蒸馏模型为放射学报告生成提供了一种可访问且高效的评估方法，促进了开发更具临床相关性模型。该模型将进一步开源。

    arXiv:2404.00998v1 Announce Type: cross  Abstract: Evaluating generated radiology reports is crucial for the development of radiology AI, but existing metrics fail to reflect the task's clinical requirements. This study proposes a novel evaluation framework using large language models (LLMs) to compare radiology reports for assessment. We compare the performance of various LLMs and demonstrate that, when using GPT-4, our proposed metric achieves evaluation consistency close to that of radiologists. Furthermore, to reduce costs and improve accessibility, making this method practical, we construct a dataset using LLM evaluation results and perform knowledge distillation to train a smaller model. The distilled model achieves evaluation capabilities comparable to GPT-4. Our framework and distilled model offer an accessible and efficient evaluation method for radiology report generation, facilitating the development of more clinically relevant models. The model will be further open-sourced 
    
[^32]: 360+x：一个全景多模态场景理解数据集

    360+x: A Panoptic Multi-modal Scene Understanding Dataset

    [https://arxiv.org/abs/2404.00989](https://arxiv.org/abs/2404.00989)

    该数据集是第一个融合多个视角和多个数据模态以模拟现实世界中日常信息获取方式的数据库。

    

    人类对世界的感知受到多种视角和模态的影响。虽然许多现有数据集侧重于从某种视角（例如自我中心或第三人称视角）理解场景，但我们的数据集提供了一个全景视角（即多视角和多数据模态）。具体而言，我们捕捉了第三人称全景和前视图，以及具有视频、多通道音频、定向双耳延迟、位置数据和文本场景描述等丰富模态的自我的单眼/双眼视图，呈现了对世界的全面观察。图1展示了我们的360+x数据集的所有28个场景类别。据我们所知，这是第一个涵盖多个视角和多个数据模态的数据库，模拟了现实世界中日常信息的获取方式。通过我们的基准分析，我们提出了5种不同

    arXiv:2404.00989v1 Announce Type: cross  Abstract: Human perception of the world is shaped by a multitude of viewpoints and modalities. While many existing datasets focus on scene understanding from a certain perspective (e.g. egocentric or third-person views), our dataset offers a panoptic perspective (i.e. multiple viewpoints with multiple data modalities). Specifically, we encapsulate third-person panoramic and front views, as well as egocentric monocular/binocular views with rich modalities including video, multi-channel audio, directional binaural delay, location data and textual scene descriptions within each scene captured, presenting comprehensive observation of the world. Figure 1 offers a glimpse of all 28 scene categories of our 360+x dataset. To the best of our knowledge, this is the first database that covers multiple viewpoints with multiple data modalities to mimic how daily information is accessed in the real world. Through our benchmark analysis, we presented 5 differe
    
[^33]: 智慧城市的持续学习：一项调研

    Continual Learning for Smart City: A Survey

    [https://arxiv.org/abs/2404.00983](https://arxiv.org/abs/2404.00983)

    该调研综合审查了智慧城市发展中广泛使用的持续学习方法，内容涵盖了方法论分类、多种应用领域和相关数据集。

    

    随着现代城市数字化，庞大的数据量和强大的计算资源促使智慧城市中部署的智能模型迅速更新。持续学习（CL）是一种新颖的机器学习范式，不断更新模型以适应变化的环境，其中学习任务、数据和分布可以随时间变化。我们的调查全面审查了在智慧城市发展中广泛使用的持续学习方法。内容分为三个部分：1）方法论。我们将大量基本的CL方法和结合其他学习范例的高级CL框架进行分类，包括图学习、时空学习、多模态学习和联邦学习。2）应用方面。我们介绍了涵盖交通、环境、公共卫生、安全、网络以及与城市计算相关的相关数据集的众多CL应用。3）

    arXiv:2404.00983v1 Announce Type: cross  Abstract: With the digitization of modern cities, large data volumes and powerful computational resources facilitate the rapid update of intelligent models deployed in smart cities. Continual learning (CL) is a novel machine learning paradigm that constantly updates models to adapt to changing environments, where the learning tasks, data, and distributions can vary over time. Our survey provides a comprehensive review of continual learning methods that are widely used in smart city development. The content consists of three parts: 1) Methodology-wise. We categorize a large number of basic CL methods and advanced CL frameworks in combination with other learning paradigms including graph learning, spatial-temporal learning, multi-modal learning, and federated learning. 2) Application-wise. We present numerous CL applications covering transportation, environment, public health, safety, networks, and associated datasets related to urban computing. 3
    
[^34]: 针对城市规划和公众参与的非线性脉冲模式制定动态社会政治预测算法

    Nonlinear Impulse Pattern Formulation dynamical social and political prediction algorithm for city planning and public participation

    [https://arxiv.org/abs/2404.00977](https://arxiv.org/abs/2404.00977)

    提出了一种非线性动力学算法IPF，用于城市规划，能够预测不同社会或政治利益相关参数在规划过程中的发展，已在音乐乐器模拟、脑动力学和人际互动中表现出高预测精度。

    

    提出了一种用于城市规划的非线性动力学算法，即脉冲模式制定（IPF），用于预测不同社会或政治利益相关参数（如健康、艺术自由或财务发展）在规划过程中的发展。IPF已经在音乐乐器模拟、脑动力学和人际互动中表现出高预测精度，而计算成本低廉。社会和政治IPF由三个基本方程组成，用于系统状态发展、利益相关者的自适应、两种自适应互动和适合于不同规划情境的外部冲击项。通过调整一组系统参数，对利益相关者互动和发展的典型情景进行建模，包括利益相关者对外部输入的反应、自适应通过提高系统稳定性、利益相关者因调解互动自适应而收敛等。

    arXiv:2404.00977v1 Announce Type: cross  Abstract: A nonlinear-dynamical algorithm for city planning is proposed as an Impulse Pattern Formulation (IPF) for predicting relevant parameters like health, artistic freedom, or financial developments of different social or political stakeholders over the cause of a planning process. The IPF has already shown high predictive precision at low computational cost in musical instrument simulations, brain dynamics, and human-human interactions. The social and political IPF consists of three basic equations of system state developments, self-adaptation of stakeholders, two adaptive interactions, and external impact terms suitable for respective planning situations. Typical scenarios of stakeholder interactions and developments are modeled by adjusting a set of system parameters. These include stakeholder reaction to external input, enhanced system stability through self-adaptation, stakeholder convergence due to mediative interaction adaptation, as
    
[^35]: 探索和评估LLM驱动的代码生成中的幻觉

    Exploring and Evaluating Hallucinations in LLM-Powered Code Generation

    [https://arxiv.org/abs/2404.00971](https://arxiv.org/abs/2404.00971)

    本研究通过主题分析对LLM生成的代码中的幻觉进行了总结和分类，建立了代码中幻觉的全面分类法。

    

    大型语言模型（LLMs）的崛起已经极大地推动了软件工程任务中许多应用的发展，特别是在代码生成方面。尽管表现出色，LLMs容易产生幻觉，即LLMs可能产生与用户意图偏离、表现出内部不一致或与事实知识不符的输出，使得在广泛应用中部署LLMs可能存在风险。现有研究主要集中在自然语言生成（NLG）领域的幻觉，缺乏对代码生成环境中幻觉类型和程度的理解。为了填补这一空白，我们对LLM生成的代码进行了主题分析，总结和归类其中存在的幻觉。我们的研究建立了LLM生成的代码中幻觉的全面分类法，涵盖了5个主要幻觉类别。

    arXiv:2404.00971v1 Announce Type: cross  Abstract: The rise of Large Language Models (LLMs) has significantly advanced many applications on software engineering tasks, particularly in code generation. Despite the promising performance, LLMs are prone to generate hallucinations, which means LLMs might produce outputs that deviate from users' intent, exhibit internal inconsistencies, or misalign with the factual knowledge, making the deployment of LLMs potentially risky in a wide range of applications. Existing work mainly focuses on investing the hallucination in the domain of natural language generation (NLG), leaving a gap in understanding the types and extent of hallucinations in the context of code generation. To bridge the gap, we conducted a thematic analysis of the LLM-generated code to summarize and categorize the hallucinations present in it. Our study established a comprehensive taxonomy of hallucinations in LLM-generated code, encompassing 5 primary categories of hallucinatio
    
[^36]: Evalverse: 大型语言模型评估的统一和易用库

    Evalverse: Unified and Accessible Library for Large Language Model Evaluation

    [https://arxiv.org/abs/2404.00943](https://arxiv.org/abs/2404.00943)

    Evalverse是一个统一和易用的库，简化了大型语言模型（LLMs）的评估，为研究人员和从业者提供了集中且易于访问的评估框架。

    

    本文介绍了Evalverse，这是一个新颖的库，通过将不同的评估工具统一到一个用户友好的框架中，简化了对大型语言模型（LLMs）的评估。Evalverse使得对人工智能了解有限的个人可以轻松请求LLMs评估并收到详细报告，利用与Slack等通信平台的集成。因此，Evalverse作为LLMs的全面评估强大工具，为研究人员和从业者提供了集中且易于访问的评估框架。最后，我们还提供了Evalverse的演示视频，展示了它的功能和实现方式，以两分钟的格式展示。

    arXiv:2404.00943v1 Announce Type: cross  Abstract: This paper introduces Evalverse, a novel library that streamlines the evaluation of Large Language Models (LLMs) by unifying disparate evaluation tools into a single, user-friendly framework. Evalverse enables individuals with limited knowledge of artificial intelligence to easily request LLM evaluations and receive detailed reports, facilitated by an integration with communication platforms like Slack. Thus, Evalverse serves as a powerful tool for the comprehensive assessment of LLMs, offering both researchers and practitioners a centralized and easily accessible evaluation framework. Finally, we also provide a demo video for Evalverse, showcasing its capabilities and implementation in a two-minute format.
    
[^37]: 使用大规模知识图谱评估大型语言模型的事实性

    Evaluating the Factuality of Large Language Models using Large-Scale Knowledge Graphs

    [https://arxiv.org/abs/2404.00942](https://arxiv.org/abs/2404.00942)

    使用大型知识图谱创建评估模型以检查大型语言模型在事实性上的表现，有效降低评估成本。

    

    大型语言模型（LLMs）的出现显著改变了人工智能领域，增强了机器学习和人工智能的能力。事实性问题对LLMs来说是一个关键问题，因为它们可能生成事实不准确的响应。本文提出了GraphEval，通过大规模测试数据集对LLM的性能进行评估。具体而言，测试数据集是从拥有超过1000万个事实的大型知识图谱中检索而来，无需昂贵的人力成本。与基于生成响应评估LLMs的传统方法不同，GraphEval通过创建一个评估模型简化了评估过程，用于估计LLM给出的答案的正确性。我们的实验表明，评估模型的事实性评估与LLM生成的输出的正确性密切相关，同时显著降低了评估成本。此外，我们的发现为LLM的性能提供了宝贵的洞见。

    arXiv:2404.00942v1 Announce Type: cross  Abstract: The advent of Large Language Models (LLMs) has significantly transformed the AI landscape, enhancing machine learning and AI capabilities. Factuality issue is a critical concern for LLMs, as they may generate factually incorrect responses. In this paper, we propose GraphEval to evaluate an LLM's performance using a substantially large test dataset. Specifically, the test dataset is retrieved from a large knowledge graph with more than 10 million facts without expensive human efforts. Unlike conventional methods that evaluate LLMs based on generated responses, GraphEval streamlines the evaluation process by creating a judge model to estimate the correctness of the answers given by the LLM. Our experiments demonstrate that the judge model's factuality assessment aligns closely with the correctness of the LLM's generated outputs, while also substantially reducing evaluation costs. Besides, our findings offer valuable insights into LLM per
    
[^38]: 多语言大型语言模型：语料库、对齐和偏见综述

    A Survey on Multilingual Large Language Models: Corpora, Alignment, and Bias

    [https://arxiv.org/abs/2404.00929](https://arxiv.org/abs/2404.00929)

    该论文对多语言大型语言模型进行了全面分析，深入讨论了关键问题，包括多语言语料库、对齐和偏见。

    

    基于大型语言模型（LLMs）的基础上，发展了多语言大型语言模型（MLLMs）来解决多语言自然语言处理任务的挑战，希望实现从高资源到低资源语言的知识转移。然而，仍然存在重要限制和挑战，比如语言不平衡、多语言对齐和固有偏见。本文旨在对MLLMs进行全面分析，深入讨论围绕这些关键问题的议题。

    arXiv:2404.00929v1 Announce Type: cross  Abstract: Based on the foundation of Large Language Models (LLMs), Multilingual Large Language Models (MLLMs) have been developed to address the challenges of multilingual natural language processing tasks, hoping to achieve knowledge transfer from high-resource to low-resource languages. However, significant limitations and challenges still exist, such as language imbalance, multilingual alignment, and inherent bias. In this paper, we aim to provide a comprehensive analysis of MLLMs, delving deeply into discussions surrounding these critical issues. First of all, we start by presenting an overview of MLLMs, covering their evolution, key techniques, and multilingual capacities. Secondly, we explore widely utilized multilingual corpora for MLLMs' training and multilingual datasets oriented for downstream tasks that are crucial for enhancing the cross-lingual capability of MLLMs. Thirdly, we survey the existing studies on multilingual representati
    
[^39]: MM3DGS SLAM: 利用视觉、深度和惯性测量的多模态3D高斯雨滴法SLAM

    MM3DGS SLAM: Multi-modal 3D Gaussian Splatting for SLAM Using Vision, Depth, and Inertial Measurements

    [https://arxiv.org/abs/2404.00923](https://arxiv.org/abs/2404.00923)

    使用多模态3D高斯雨滴法SLAM可以结合未定位相机图像和惯性测量，实现准确的地图建立和轨迹跟踪，具有更快的渲染速度、尺度感知性和改进的轨迹追踪能力。

    

    同步定位与建图对于位置跟踪和场景理解至关重要。基于3D高斯的地图表示使得能够利用多姿态相机实现逼真重建和实时渲染场景。我们首次展示，利用3D高斯进行地图表示，并结合未定位相机图像和惯性测量，可以实现准确的SLAM。我们的方法MM3DGS通过实现更快的渲染、尺度感知和改进的轨迹跟踪，解决了先前基于神经辐射场的表示法的局限性。我们的框架实现了基于关键帧的建图和跟踪，利用包含相对姿态变换的损失函数，这些姿态变换来自预积分的惯性测量、深度估计和光度渲染质量的度量。我们还发布了一个多模态数据集UT-MM，该数据集由配备相机和惯性测量单元的移动机器人收集而成。

    arXiv:2404.00923v1 Announce Type: cross  Abstract: Simultaneous localization and mapping is essential for position tracking and scene understanding. 3D Gaussian-based map representations enable photorealistic reconstruction and real-time rendering of scenes using multiple posed cameras. We show for the first time that using 3D Gaussians for map representation with unposed camera images and inertial measurements can enable accurate SLAM. Our method, MM3DGS, addresses the limitations of prior neural radiance field-based representations by enabling faster rendering, scale awareness, and improved trajectory tracking. Our framework enables keyframe-based mapping and tracking utilizing loss functions that incorporate relative pose transformations from pre-integrated inertial measurements, depth estimates, and measures of photometric rendering quality. We also release a multi-modal dataset, UT-MM, collected from a mobile robot equipped with a camera and an inertial measurement unit. Experimen
    
[^40]: 大型语言模型中的高效令牌利用学习

    Token-Efficient Leverage Learning in Large Language Models

    [https://arxiv.org/abs/2404.00914](https://arxiv.org/abs/2404.00914)

    介绍了一种名为Token-Efficient Leverage Learning（TELL）的方法，在大型语言模型中展示了其降低任务数据需求、提高任务性能的潜力，为低资源任务带来了竞争性能。

    

    大型语言模型（LLMs）在各种任务中表现出色，但在高资源场景中表现更佳，这在低资源场景中存在挑战。数据稀缺和LLMs适应特定任务固有的困难加剧了这一挑战。为了解决这两大难题，我们引入了\textbf{Leverage Learning}。我们提出了这一方法的简化实现，称为Token-Efficient Leverage Learning (TELL)。TELL展示了Leverage Learning的潜力，证明它在各种LLMs和低资源任务中的有效性，从$10^4$到$10^6$个令牌不等。与传统的监督微调（SFT）相比，它将任务数据需求降低了近一个数量级，同时提供竞争力的性能。在相同量的任务数据情况下，TELL在改善任务性能方面领先于SFT。我们讨论了Leverage Learning的机制，暗示其符合量化假设。

    arXiv:2404.00914v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have excelled in various tasks but perform better in high-resource scenarios, which presents challenges in low-resource scenarios. Data scarcity and the inherent difficulty of adapting LLMs to specific tasks compound the challenge. To address the twin hurdles, we introduce \textbf{Leverage Learning}. We present a streamlined implement of this methodology called Token-Efficient Leverage Learning (TELL). TELL showcases the potential of Leverage Learning, demonstrating effectiveness across various LLMs and low-resource tasks, ranging from $10^4$ to $10^6$ tokens. It reduces task data requirements by up to nearly an order of magnitude compared to conventional Supervised Fine-Tuning (SFT) while delivering competitive performance. With the same amount of task data, TELL leads in improving task performance compared to SFT. We discuss the mechanism of Leverage Learning, suggesting it aligns with quantization hypoth
    
[^41]: LLaMA-Excitor: 通过间接特征交互进行通用指令调整

    LLaMA-Excitor: General Instruction Tuning via Indirect Feature Interaction

    [https://arxiv.org/abs/2404.00913](https://arxiv.org/abs/2404.00913)

    LLaMA-Excitor是一种轻量级方法，通过逐渐更多地关注有价值的信息来激发LLM更好地遵循指令，而不直接改变中间隐藏状态，有效保留预训练知识。

    

    现有的用于微调LLM（如Adapter、Prefix-tuning和LoRA）的方法引入了额外的模块或附加输入序列，以注入新技能或知识，但可能会损害LLM的固有能力。本文提出了LLaMA-Excitor，这是一种轻量级方法，通过逐渐更多地关注有价值的信息来激发LLM更好地遵循指令。具体来说，LLaMA-Excitor在transformer结构的自注意力计算过程中不直接改变中间隐藏状态。我们设计了Excitor块作为用于LLM自注意力中相似度计算的旁路模块，通过可学习的提示重新构建keys并改变values的重要性。LLaMA-Excitor确保自适应地将额外的注意力分配给输入指令，从而在低质量指令上微调LLM时有效地保留LLM的预训练知识。

    arXiv:2404.00913v1 Announce Type: cross  Abstract: Existing methods to fine-tune LLMs, like Adapter, Prefix-tuning, and LoRA, which introduce extra modules or additional input sequences to inject new skills or knowledge, may compromise the innate abilities of LLMs. In this paper, we propose LLaMA-Excitor, a lightweight method that stimulates the LLMs' potential to better follow instructions by gradually paying more attention to worthwhile information. Specifically, the LLaMA-Excitor does not directly change the intermediate hidden state during the self-attention calculation of the transformer structure. We designed the Excitor block as a bypass module for the similarity score computation in LLMs' self-attention to reconstruct keys and change the importance of values by learnable prompts. LLaMA-Excitor ensures a self-adaptive allocation of additional attention to input instructions, thus effectively preserving LLMs' pre-trained knowledge when fine-tuning LLMs on low-quality instruction-
    
[^42]: 通过LLMOps驱动的个性化推荐系统最大化用户体验

    Maximizing User Experience with LLMOps-Driven Personalized Recommendation Systems

    [https://arxiv.org/abs/2404.00903](https://arxiv.org/abs/2404.00903)

    将LLMOps集成到个性化推荐系统中，提升了大规模机器学习模型的效率和可靠性，推动生成与用户偏好一致的个性化推荐，为未来个性化推荐系统的发展带来重要影响。

    

    将LLMOps集成到个性化推荐系统中，标志着在管理LLM驱动应用方面取得了重大进展。这一创新为企业带来了机遇和挑战，需要专门的团队在优先考虑数据安全和模型可解释性的同时，处理工程技术的复杂性。通过利用LLMOps，企业可以提升大规模机器学习模型的效率和可靠性，推动与用户偏好一致的个性化推荐。尽管存在伦理考量，但LLMOps已准备好被广泛采用，承诺提供更高效、更安全的机器学习服务，提升用户体验并塑造个性化推荐系统的未来。

    arXiv:2404.00903v1 Announce Type: cross  Abstract: The integration of LLMOps into personalized recommendation systems marks a significant advancement in managing LLM-driven applications. This innovation presents both opportunities and challenges for enterprises, requiring specialized teams to navigate the complexity of engineering technology while prioritizing data security and model interpretability. By leveraging LLMOps, enterprises can enhance the efficiency and reliability of large-scale machine learning models, driving personalized recommendations aligned with user preferences. Despite ethical considerations, LLMOps is poised for widespread adoption, promising more efficient and secure machine learning services that elevate user experience and shape the future of personalized recommendation systems.
    
[^43]: 机器学习鲁棒性：入门指南

    Machine Learning Robustness: A Primer

    [https://arxiv.org/abs/2404.00897](https://arxiv.org/abs/2404.00897)

    该章节探讨了机器学习中稳健性的重要概念及关键的技术和因素，以确立人工智能系统的可信度。

    

    这一章节探讨了机器学习（ML）中稳健性的基本概念及其在确立人工智能（AI）系统的可信度中的重要作用。讨论始于稳健性的详细定义，将其描述为ML模型在各种不同和意外的环境条件下保持稳定性能的能力。ML鲁棒性通过多个视角进行了剖析：其与泛化能力的互补性；其作为可信AI的要求；其对抗性与非对抗性方面；其数量化指标；以及其可复现性和可解释性等指标。章节深入探讨了影响鲁棒性的因素，如数据偏差、模型复杂性以及ML流程不明确的风险。它从广泛的视角调查了鲁棒性评估的关键技术，包括对抗性攻击，包括数字和物理领域。

    arXiv:2404.00897v1 Announce Type: cross  Abstract: This chapter explores the foundational concept of robustness in Machine Learning (ML) and its integral role in establishing trustworthiness in Artificial Intelligence (AI) systems. The discussion begins with a detailed definition of robustness, portraying it as the ability of ML models to maintain stable performance across varied and unexpected environmental conditions. ML robustness is dissected through several lenses: its complementarity with generalizability; its status as a requirement for trustworthy AI; its adversarial vs non-adversarial aspects; its quantitative metrics; and its indicators such as reproducibility and explainability. The chapter delves into the factors that impede robustness, such as data bias, model complexity, and the pitfalls of underspecified ML pipelines. It surveys key techniques for robustness assessment from a broad perspective, including adversarial attacks, encompassing both digital and physical realms.
    
[^44]: MTLight：用于交通信号控制的高效多任务强化学习

    MTLight: Efficient Multi-Task Reinforcement Learning for Traffic Signal Control

    [https://arxiv.org/abs/2404.00886](https://arxiv.org/abs/2404.00886)

    MTLight提出了一种用于交通信号控制的高效多任务强化学习算法，通过学习潜在状态和构建多个辅助任务来提高代理的性能。

    

    交通信号控制对于缓解现代城市交通拥堵具有巨大影响。深度强化学习（RL）近年来被广泛用于这一任务，表现出有希望的性能但也面临许多挑战，如性能有限和样本效率低。为了解决这些挑战，提出了MTLight，以从众多交通指标中学习的潜在状态来增强代理的观察。同时，构建了多个辅助和监督任务来学习潜在状态，并使用两种嵌入潜在特征，即任务特定特征和任务共享特征，使潜在状态更加丰富。在CityFlow上进行了大量实验证明，MTLight具有领先的收敛速度和渐近性能。我们在所有不断增加的控制难度场景下进一步模拟高峰小时段，结果表明MTLig

    arXiv:2404.00886v1 Announce Type: new  Abstract: Traffic signal control has a great impact on alleviating traffic congestion in modern cities. Deep reinforcement learning (RL) has been widely used for this task in recent years, demonstrating promising performance but also facing many challenges such as limited performances and sample inefficiency. To handle these challenges, MTLight is proposed to enhance the agent observation with a latent state, which is learned from numerous traffic indicators. Meanwhile, multiple auxiliary and supervisory tasks are constructed to learn the latent state, and two types of embedding latent features, the task-specific feature and task-shared feature, are used to make the latent state more abundant. Extensive experiments conducted on CityFlow demonstrate that MTLight has leading convergence speed and asymptotic performance. We further simulate under peak-hour pattern in all scenarios with increasing control difficulty and the results indicate that MTLig
    
[^45]: 自我演示: 在大型语言模型中引出示范之外的泛化能力

    Self-Demos: Eliciting Out-of-Demonstration Generalizability in Large Language Models

    [https://arxiv.org/abs/2404.00884](https://arxiv.org/abs/2404.00884)

    提出了自我演示(Self-Demos)方法，在大型语言模型中通过生成查询感知的示范，引出固有的泛化能力，并构建了用于评估该方法有效性的数据集OOD-Toolset。

    

    大型语言模型(LLMs)展示了在上下文学习(ICL)方面的良好能力，仅凭借少量示范就能迅速适应新任务。然而，当前的少样本方法严重依赖高质量、特定查询的示范，而这种示范通常缺乏。面对示范之外的查询，依赖手工制定示范或外部检索器的方法可能会失败。为了弥合有限示范和示范之外查询之间的差距，我们提出了自我演示(Self-Demos)，这是一种通过面向查询的示范生成来引出LLMs中固有的泛化能力的新型提示方法。生成的示范策略性地插值了现有示范和给定的查询，将查询从示范之外变为示范内。为了评估我们方法的有效性，我们人工构建了OOD-Toolset数据集，其中包含超过300个真实API和1000个实例，每个实例包括三个工具使用示例作为示范和一个示范之外查询。

    arXiv:2404.00884v1 Announce Type: cross  Abstract: Large language models (LLMs) have shown promising abilities of in-context learning (ICL), adapting swiftly to new tasks with only few-shot demonstrations. However, current few-shot methods heavily depend on high-quality, query-specific demos, which are often lacking. When faced with out-of-demonstration (OOD) queries, methods that rely on hand-crafted demos or external retrievers might fail. To bridge the gap between limited demos and OOD queries, we propose Self-Demos, a novel prompting method that elicits the inherent generalizability in LLMs by query-aware demo generation. The generated demos strategically interpolate between existing demos and the given query, transforming the query from OOD to ID. To evaluate the effectiveness of our approach, we manually constructed OOD-Toolset, a dataset in the tool-using scenario with over 300 real-world APIs and 1000 instances, each consisting of three tool-use cases as demos and an OOD query.
    
[^46]: 基于QLoRA和Zip-tie嵌入的双语迁移学习Bailong

    Bailong: Bilingual Transfer Learning based on QLoRA and Zip-tie Embedding

    [https://arxiv.org/abs/2404.00862](https://arxiv.org/abs/2404.00862)

    通过结合参数高效微调和先进的嵌入初始化技术，本研究在以英语为主导的开源LLMs上实现了跨语言迁移学习。

    

    大型语言模型（LLMs）在各种自然语言处理应用中展现出卓越的性能。然而，现有大多数开源LLMs主要在英语数据上进行预训练，对其他语言的覆盖较少。多语言训练数据的不足导致在应用到资源较少的语言时性能亚优。此外，通过使用额外数据对低资源语言进行全参数微调以提高LLMs性能需要大量计算资源，这给研究机构和个人研究人员带来了计算障碍。因此，提出了一些技术，如参数高效微调和先进的嵌入初始化来解决这些挑战。本文将它们结合起来，以促进对以英语为主导的开源LLMs进行跨语言迁移。

    arXiv:2404.00862v1 Announce Type: cross  Abstract: Large language models (LLMs) have demonstrated exceptional performance in various NLP applications. However, the majority of existing open-source LLMs are pre-trained primarily on English data and little part of other languages. This deficiency in multilingual training data results in suboptimal performance when applied to languages with fewer available resources. Furthermore, enhancing the performance of LLMs on low-resource languages by full-parameter fine-tuning with additional data requires substantial computational resources, posing computational barriers for research organizations and individual researchers. Consequently, several techniques such as parameter-efficient tuning and advanced embedding initialization have been proposed to address these challenges. In this work, we combine them to facilitate cross-lingual transfer on English-dominated open-source LLM. To effectively enhance the model's proficiency in Traditional Chines
    
[^47]: 使用可变长度软池化从语音表示中去除说话者信息

    Removing Speaker Information from Speech Representation using Variable-Length Soft Pooling

    [https://arxiv.org/abs/2404.00856](https://arxiv.org/abs/2404.00856)

    该论文旨在利用神经网络预测语音中离散单元边界，实现可变长度池化，从而去除表示中的说话者信息。

    

    最近，有一些研究致力于利用自监督框架对语音进行编码，以进行语音合成的语言信息。然而，从周围表示预测表示可能会在语音表示中不经意地缠结说话者信息。本文旨在通过利用语音的结构化特性来去除说话者信息，语音由具有清晰边界的离散单元(如音素)组成。神经网络预测这些边界，实现基于事件的表示提取的可变长度池化，而不是固定速率方法。边界预测器输出介于0和1之间的边界概率，使池化软化。模型训练以最小化数据增强的池化表示与时间拉伸和音高变换之间的差异。为了确认学到的表示包含内容信息但独立于说话者信息，模型...

    arXiv:2404.00856v1 Announce Type: cross  Abstract: Recently, there have been efforts to encode the linguistic information of speech using a self-supervised framework for speech synthesis. However, predicting representations from surrounding representations can inadvertently entangle speaker information in the speech representation. This paper aims to remove speaker information by exploiting the structured nature of speech, composed of discrete units like phonemes with clear boundaries. A neural network predicts these boundaries, enabling variable-length pooling for event-based representation extraction instead of fixed-rate methods. The boundary predictor outputs a probability for the boundary between 0 and 1, making pooling soft. The model is trained to minimize the difference with the pooled representation of the data augmented by time-stretch and pitch-shift. To confirm that the learned representation includes contents information but is independent of speaker information, the model
    
[^48]: 基于鸟类视觉回路的小物体运动检测神经网络TSOM

    TSOM: Small Object Motion Detection Neural Network Inspired by Avian Visual Circuit

    [https://arxiv.org/abs/2404.00855](https://arxiv.org/abs/2404.00855)

    本论文基于鸟类视觉系统，提出了一种新型的神经网络TSOM，用于小物体运动检测算法。

    

    从俯视角度在复杂背景中检测小运动物体对于机器视觉系统是一个极具挑战性的任务。受自然界启发，鸟类视觉系统能够处理各种复杂的空中场景中的运动信息，其视网膜-OT-Rt视觉回路高度敏感于从高空捕捉小物体的运动信息。然而，基于鸟类视觉系统的小物体运动检测算法还有待进一步研究。本文通过对视网膜-OT-Rt视觉回路生物机制的广泛研究进行数学建模，并提出了一种新颖的tectum小物体运动检测神经网络（TSOM）。

    arXiv:2404.00855v1 Announce Type: cross  Abstract: Detecting small moving objects in complex backgrounds from an overhead perspective is a highly challenging task for machine vision systems. As an inspiration from nature, the avian visual system is capable of processing motion information in various complex aerial scenes, and its Retina-OT-Rt visual circuit is highly sensitive to capturing the motion information of small objects from high altitudes. However, more needs to be done on small object motion detection algorithms based on the avian visual system. In this paper, we conducted mathematical modeling based on extensive studies of the biological mechanisms of the Retina-OT-Rt visual circuit. Based on this, we proposed a novel tectum small object motion detection neural network (TSOM). The neural network includes the retina, SGC dendritic, SGC Soma, and Rt layers, each layer corresponding to neurons in the visual pathway. The Retina layer is responsible for accurately projecting inp
    
[^49]: HeteroMILE: 用于异构图的多层图表示学习框架

    HeteroMILE: a Multi-Level Graph Representation Learning Framework for Heterogeneous Graphs

    [https://arxiv.org/abs/2404.00816](https://arxiv.org/abs/2404.00816)

    HeteroMILE提出了一种多级嵌入框架，可以将当代图嵌入方法扩展到大型异构图，并通过反复粗化和细化的方式有效降低计算成本。

    

    异构图在现实世界的应用中是普遍存在的，因为它们可以表示不同类型实体之间的各种关系。因此，学习这种图中的嵌入是图机器学习中的一个关键问题。然而，现有的解决方案由于计算复杂度高而无法扩展到大型异构图。为了解决这个问题，我们提出了一个节点在异构图上的多级嵌入框架（HeteroMILE）-一种通用方法，可以使当代图嵌入方法扩展到大图。HeteroMILE将大图反复粗化为较小的大小，同时保留图的主干结构，然后将其嵌入，通过避免耗时的处理操作有效地降低计算成本。然后，它使用异构图卷积神经网络将粗化的嵌入优化到原始图中。

    arXiv:2404.00816v1 Announce Type: cross  Abstract: Heterogeneous graphs are ubiquitous in real-world applications because they can represent various relationships between different types of entities. Therefore, learning embeddings in such graphs is a critical problem in graph machine learning. However, existing solutions for this problem fail to scale to large heterogeneous graphs due to their high computational complexity. To address this issue, we propose a Multi-Level Embedding framework of nodes on a heterogeneous graph (HeteroMILE) - a generic methodology that allows contemporary graph embedding methods to scale to large graphs. HeteroMILE repeatedly coarsens the large sized graph into a smaller size while preserving the backbone structure of the graph before embedding it, effectively reducing the computational cost by avoiding time-consuming processing operations. It then refines the coarsened embedding to the original graph using a heterogeneous graph convolution neural network.
    
[^50]: 基于LiDAR扩散模型的逼真场景生成

    Towards Realistic Scene Generation with LiDAR Diffusion Models

    [https://arxiv.org/abs/2404.00815](https://arxiv.org/abs/2404.00815)

    本文提出了LiDAR扩散模型（LiDMs），能够生成具有LiDAR逼真效果的场景，通过引入几何先验，实现了模式逼真、几何逼真和物体逼真。

    

    扩散模型在逼真图像合成方面表现出色，但是它们在适应LiDAR场景生成方面面临重大障碍。本文提出了LiDAR扩散模型（LiDMs），通过在学习流程中引入几何先验，从而生成具有LiDAR逼真效果的场景。

    arXiv:2404.00815v1 Announce Type: cross  Abstract: Diffusion models (DMs) excel in photo-realistic image synthesis, but their adaptation to LiDAR scene generation poses a substantial hurdle. This is primarily because DMs operating in the point space struggle to preserve the curve-like patterns and 3D geometry of LiDAR scenes, which consumes much of their representation power. In this paper, we propose LiDAR Diffusion Models (LiDMs) to generate LiDAR-realistic scenes from a latent space tailored to capture the realism of LiDAR scenes by incorporating geometric priors into the learning pipeline. Our method targets three major desiderata: pattern realism, geometry realism, and object realism. Specifically, we introduce curve-wise compression to simulate real-world LiDAR patterns, point-wise coordinate supervision to learn scene geometry, and patch-wise encoding for a full 3D object context. With these three core designs, our method achieves competitive performance on unconditional LiDAR g
    
[^51]: 大型语言模型的算法勾结

    Algorithmic Collusion by Large Language Models

    [https://arxiv.org/abs/2404.00806](https://arxiv.org/abs/2404.00806)

    大型语言模型的算法定价代理在寡头市场环境中自主勾结，对消费者利益有害，其说明书中的短语变化可能增加勾结。

    

    arXiv:2404.00806v1 公告类型:交叉摘要:算法定价的兴起引起了对算法勾结的担忧。我们对基于大型语言模型（LLMs）特别是GPT-4的算法定价代理进行实验。我们发现：（1）基于LLM的代理在定价任务上表现出色，（2）基于LLM的定价代理在寡头市场环境中自主勾结，损害消费者利益，（3）LLM说明书中看似无害短语("提示")的变化可能会增加勾结。这些结果也适用于拍卖设置。我们的发现强调了有关算法定价的反垄断监管的必要性，并发现了基于LLM的定价代理所面临的监管挑战。

    arXiv:2404.00806v1 Announce Type: cross  Abstract: The rise of algorithmic pricing raises concerns of algorithmic collusion. We conduct experiments with algorithmic pricing agents based on Large Language Models (LLMs), and specifically GPT-4. We find that (1) LLM-based agents are adept at pricing tasks, (2) LLM-based pricing agents autonomously collude in oligopoly settings to the detriment of consumers, and (3) variation in seemingly innocuous phrases in LLM instructions ("prompts") may increase collusion. These results extend to auction settings. Our findings underscore the need for antitrust regulation regarding algorithmic pricing, and uncover regulatory challenges unique to LLM-based pricing agents.
    
[^52]: 处理连续学习中的可塑性丧失和灾难性遗忘

    Addressing Loss of Plasticity and Catastrophic Forgetting in Continual Learning

    [https://arxiv.org/abs/2404.00781](https://arxiv.org/abs/2404.00781)

    本文提出了一种新方法，即基于效用的扰动梯度下降（UPGD），通过在梯度更新中应用不同大小的扰动，保护有用单元以防遗忘，同时恢复不太有用单元的可塑性。

    

    深度表示学习方法在连续学习中存在困难，既遭受有用单元的灾难性遗忘，又因僵化和无用单元导致可塑性丢失。虽然许多方法分别解决这两个问题，但目前只有少数方法能同时处理这两个问题。本文引入了基于效用的扰动梯度下降（UPGD）作为一种用于表示持续学习的新方法。UPGD结合了梯度更新和扰动，它对更有用的单元应用较小的修改，保护它们免受遗忘，对不太有用的单元应用较大的修改，恢复它们的可塑性。

    arXiv:2404.00781v1 Announce Type: cross  Abstract: Deep representation learning methods struggle with continual learning, suffering from both catastrophic forgetting of useful units and loss of plasticity, often due to rigid and unuseful units. While many methods address these two issues separately, only a few currently deal with both simultaneously. In this paper, we introduce Utility-based Perturbed Gradient Descent (UPGD) as a novel approach for the continual learning of representations. UPGD combines gradient updates with perturbations, where it applies smaller modifications to more useful units, protecting them from forgetting, and larger modifications to less useful units, rejuvenating their plasticity. We use a challenging streaming learning setup where continual learning problems have hundreds of non-stationarities and unknown task boundaries. We show that many existing methods suffer from at least one of the issues, predominantly manifested by their decreasing accuracy over ta
    
[^53]: 隐私保护光学技术增强人脸去标识化的保护

    Privacy-preserving Optics for Enhancing Protection in Face De-identification

    [https://arxiv.org/abs/2404.00777](https://arxiv.org/abs/2404.00777)

    本文提出了一种硬件级人脸去标识化方法，通过学习光学编码器和回归模型生成人脸热图，同时隐藏人脸身份，解决了人脸去标识化软件存在的漏洞。

    

    相机的现代激增，以及广泛应用的计算机视觉技术，引发了重大的隐私和安全担忧。当前人工智能（AI）技术帮助识别相关事件，并在家庭、办公室、医院等领域的日常任务中提供支持。为了这些目的而访问或处理个人信息引发了隐私担忧。虽然软件级解决方案如人脸去标识化提供了很好的隐私/效用权衡，但它们存在嗅探攻击的漏洞。本文提出了一种硬件级人脸去标识化方法来解决这一漏洞。具体地，我们的方法首先学习光学编码器以及回归模型，获取人脸热图，同时隐藏源图像中的人脸身份。我们还提出了一个匿名化框架，通过隐私保护图像、人脸热图和参考人脸生成一个新的人脸。

    arXiv:2404.00777v1 Announce Type: cross  Abstract: The modern surge in camera usage alongside widespread computer vision technology applications poses significant privacy and security concerns. Current artificial intelligence (AI) technologies aid in recognizing relevant events and assisting in daily tasks in homes, offices, hospitals, etc. The need to access or process personal information for these purposes raises privacy concerns. While software-level solutions like face de-identification provide a good privacy/utility trade-off, they present vulnerabilities to sniffing attacks. In this paper, we propose a hardware-level face de-identification method to solve this vulnerability. Specifically, our approach first learns an optical encoder along with a regression model to obtain a face heatmap while hiding the face identity from the source image. We also propose an anonymization framework that generates a new face using the privacy-preserving image, face heatmap, and a reference face i
    
[^54]: Recover：用于故障检测和恢复的神经符号框架

    Recover: A Neuro-Symbolic Framework for Failure Detection and Recovery

    [https://arxiv.org/abs/2404.00756](https://arxiv.org/abs/2404.00756)

    该论文提出了一种神经符号框架，名为Recover，用于在线故障识别和恢复，通过集成符号信息来提升大型语言模型生成恢复计划的能力，降低相关成本。

    

    论文介绍了Recover，这是一个用于在线故障识别和恢复的神经符号框架。Recover通过集成本体论、逻辑规则和基于LLM的规划器，利用符号信息增强LLM生成恢复计划的能力，并降低相关成本。

    arXiv:2404.00756v1 Announce Type: new  Abstract: Recognizing failures during task execution and implementing recovery procedures is challenging in robotics. Traditional approaches rely on the availability of extensive data or a tight set of constraints, while more recent approaches leverage large language models (LLMs) to verify task steps and replan accordingly. However, these methods often operate offline, necessitating scene resets and incurring in high costs. This paper introduces Recover, a neuro-symbolic framework for online failure identification and recovery. By integrating ontologies, logical rules, and LLM-based planners, Recover exploits symbolic information to enhance the ability of LLMs to generate recovery plans and also to decrease the associated costs. In order to demonstrate the capabilities of our method in a simulated kitchen environment, we introduce OntoThor, an ontology describing the AI2Thor simulator setting. Empirical evaluation shows that OntoThor's logical ru
    
[^55]: 关于最小贝叶斯风险解码的真实分布近似

    On the True Distribution Approximation of Minimum Bayes-Risk Decoding

    [https://arxiv.org/abs/2404.00752](https://arxiv.org/abs/2404.00752)

    本研究提出使用异常检测来衡量最小贝叶斯风险解码中样本接近真实分布的程度，并验证实验结果首次支持了性能与采样近似度之间的联系。

    

    最小贝叶斯风险（MBR）解码最近在文本生成中重新引起关注。MBR解码将从模型中采样的文本视为伪参考，并选择与其他文本最相似的文本。因此，采样是MBR解码的关键元素之一，先前的研究报告称性能因采样方法而异。从理论上讲，这种性能变化很可能与样本如何接近参考真实分布有关。然而，这种近似尚未深入研究。在本研究中，我们提出使用异常检测来衡量逼近程度。我们首先仔细研究性能的变化，然后展示以往关于样本的假设与变化的相关性不佳，但我们引入的异常分数则相关。这些结果是首次凭经验证明了性能和采样近似度之间的联系。

    arXiv:2404.00752v1 Announce Type: cross  Abstract: Minimum Bayes-risk (MBR) decoding has recently gained renewed attention in text generation. MBR decoding considers texts sampled from a model as pseudo-references and selects the text with the highest similarity to the others. Therefore, sampling is one of the key elements of MBR decoding, and previous studies reported that the performance varies by sampling methods. From a theoretical standpoint, this performance variation is likely tied to how closely the samples approximate the true distribution of references. However, this approximation has not been the subject of in-depth study. In this study, we propose using anomaly detection to measure the degree of approximation. We first closely examine the performance variation and then show that previous hypotheses about samples do not correlate well with the variation, but our introduced anomaly scores do. The results are the first to empirically support the link between the performance an
    
[^56]: 基准透明度：衡量数据对评估的影响

    Benchmark Transparency: Measuring the Impact of Data on Evaluation

    [https://arxiv.org/abs/2404.00748](https://arxiv.org/abs/2404.00748)

    本研究提出了一种自动化框架，可以衡量数据分布对自然语言处理模型性能和评估的影响，揭示了数据分布的重要性和对评估框架的影响。

    

    在本文中，我们介绍了一项关于量化数据分布对自然语言处理模型性能和评估影响的初步研究。我们提出了一个自动化框架，可以衡量数据点在6个不同维度上的分布：模糊度、困难度、可区分性、长度、噪声和困惑度。我们使用不均衡分层抽样来衡量数据分布对绝对（准确率/F1）和相对（排名）模型性能的影响。我们在两个不同的数据集（SQUAD和MNLI）上进行实验，测试了总共135种不同的模型（125种在SQUAD上，10种在MNLI上）。我们证明了在没有明确控制数据分布的情况下，标准评估框架是不一致和不可靠的。我们发现数据的影响在统计上是显著的，并且通常比更改度量的影响更大。

    arXiv:2404.00748v1 Announce Type: cross  Abstract: In this paper we present an exploratory research on quantifying the impact that data distribution has on the performance and evaluation of NLP models. We propose an automated framework that measures the data point distribution across 6 different dimensions: ambiguity, difficulty, discriminability, length, noise, and perplexity.   We use disproportional stratified sampling to measure how much the data distribution affects absolute (Acc/F1) and relative (Rank) model performance. We experiment on 2 different datasets (SQUAD and MNLI) and test a total of 135 different models (125 on SQUAD and 10 on MNLI). We demonstrate that without explicit control of the data distribution, standard evaluation frameworks are inconsistent and unreliable. We find that the impact of the data is statistically significant and is often larger than the impact of changing the metric.   In a second set of experiments, we demonstrate that the impact of data on eval
    
[^57]: 在增量不确定数据库中挖掘加权序列模式

    Mining Weighted Sequential Patterns in Incremental Uncertain Databases

    [https://arxiv.org/abs/2404.00746](https://arxiv.org/abs/2404.00746)

    在增量不确定数据库中挖掘加权序列模式时，需要处理权重约束，提出了增量挖掘算法，使得挖掘重要信息更加高效。

    

    由于科学技术的快速发展，不精确、噪音和不确定数据的重要性呈指数增长。因此，在不确定数据库中挖掘模式已经引起研究人员的关注。此外，需要发现这些数据库中的项目的频繁序列，以获得具有重大影响的有意义知识。在许多实际情况下，引入项目和模式的权重来发现有趣的序列作为重要性的度量。因此，在挖掘序列模式时需要处理权重约束。此外，由于数据库的动态特性，挖掘重要信息变得更具挑战性。增量挖掘算法不是在每个增量之后从头挖掘模式，而是利用先前挖掘的信息立即更新结果。存在几种算法用于从增量数据库中挖掘频繁模式和加权序列。

    arXiv:2404.00746v1 Announce Type: cross  Abstract: Due to the rapid development of science and technology, the importance of imprecise, noisy, and uncertain data is increasing at an exponential rate. Thus, mining patterns in uncertain databases have drawn the attention of researchers. Moreover, frequent sequences of items from these databases need to be discovered for meaningful knowledge with great impact. In many real cases, weights of items and patterns are introduced to find interesting sequences as a measure of importance. Hence, a constraint of weight needs to be handled while mining sequential patterns. Besides, due to the dynamic nature of databases, mining important information has become more challenging. Instead of mining patterns from scratch after each increment, incremental mining algorithms utilize previously mined information to update the result immediately. Several algorithms exist to mine frequent patterns and weighted sequences from incremental databases. However, t
    
[^58]: 越大越好吗？通过预算重新分配改进LLM代码生成

    The Larger the Better? Improved LLM Code-Generation via Budget Reallocation

    [https://arxiv.org/abs/2404.00725](https://arxiv.org/abs/2404.00725)

    较小的语言模型可以在相同预算下产生可靠的改进，但在无法进行单元测试的情况下，较小的模型选择排名次于较大模型的单个输出。

    

    人们普遍认为，大型语言模型(LLMs)比较小的模型更好。然而，更大的模型在推断过程中也需要更多的时间和计算资源。这就引出了一个问题：当两个模型在相同的预算下运行时会发生什么？（例如，计算资源，运行时间）。为了解决这个问题，我们分析了各种大小的代码生成LLMs，并进行比较，例如运行一个70B模型一次与从13B模型生成五个输出并选择一个的情况。我们的研究结果表明，在标准单元测试设置中，反复使用较小的模型可以产生一致的改进，在五个任务中最高可达15%的增益。另一方面，在无法进行单元测试的情况下，从较小模型中基于排名的候选选择表现不及来自较大模型的单个输出。我们的结果突显了使用较小模型而非较大模型的潜力。

    arXiv:2404.00725v1 Announce Type: cross  Abstract: It is a common belief that large language models (LLMs) are better than smaller-sized ones. However, larger models also require significantly more time and compute during inference. This begs the question: what happens when both models operate under the same budget? (e.g., compute, run-time). To address this question, we analyze code generation LLMs of various sizes and make comparisons such as running a 70B model once vs. generating five outputs from a 13B model and selecting one. Our findings reveal that, in a standard unit-test setup, the repeated use of smaller models can yield consistent improvements, with gains of up to 15% across five tasks. On the other hand, in scenarios where unit-tests are unavailable, a ranking-based selection of candidates from the smaller model falls short of the performance of a single output from larger ones. Our results highlight the potential of using smaller models instead of larger ones, and the imp
    
[^59]: DRCT：将图像超分辨率保存在信息瓶颈之外

    DRCT: Saving Image Super-resolution away from Information Bottleneck

    [https://arxiv.org/abs/2404.00722](https://arxiv.org/abs/2404.00722)

    基于Vision Transformer的DRCT方法采用创新的机制解决了图像超分辨率中空间信息衰减的问题，提升了模型性能。

    

    近年来，基于Vision Transformer的低层视觉任务应用取得了广泛的成功。与基于CNN的模型不同，Transformer更擅长捕捉长距离依赖关系，可以利用非局部区域的信息重建图像。在超分辨率领域，基于Swin Transformer的方法已经成为主流，因为它们能够捕捉全局空间信息，并且具有旋转窗口注意机制，有助于在不同窗口之间交换信息。许多研究人员通过扩大感知野或设计复杂网络来提高图像质量和网络效率，取得了令人称赞的结果。然而，我们观察到在前向传播过程中，由于深度增加，空间信息往往会减少，从而导致空间信息的丢失，并最终限制了模型的潜力。

    arXiv:2404.00722v1 Announce Type: cross  Abstract: In recent years, Vision Transformer-based applications to low-level vision tasks have achieved widespread success. Unlike CNN-based models, Transformers are more adept at capturing long-range dependencies, enabling the reconstruction of images utilizing information from non-local areas. In the domain of super-resolution, Swin-transformer-based approaches have become mainstream due to their capacity to capture global spatial information and their shifting-window attention mechanism that facilitates the interchange of information between different windows. Many researchers have enhanced image quality and network efficiency by expanding the receptive field or designing complex networks, yielding commendable results. However, we observed that spatial information tends to diminish during the forward propagation process due to increased depth, leading to a loss of spatial information and, consequently, limiting the model's potential. To addr
    
[^60]: 计算机自适应测试综述：机器学习视角

    Survey of Computerized Adaptive Testing: A Machine Learning Perspective

    [https://arxiv.org/abs/2404.00712](https://arxiv.org/abs/2404.00712)

    本文以机器学习视角综述了计算机自适应测试（CAT），重点解析其测试问题选择算法和如何优化认知诊断模型、题库构建和测试控制。

    

    计算机自适应测试（CAT）提供了一种高效、量身定制的评估考生熟练程度的方法，通过根据他们的表现动态调整测试问题。CAT广泛应用于教育、医疗、体育和社会学等多个领域，彻底改变了测试实践。然而，随着大规模测试的增加复杂性，CAT已经融合了机器学习技术。本文旨在提供一个以机器学习为重点的CAT综述，从新的角度解读这种自适应测试方法。通过研究CAT适应性核心的测试问题选择算法，我们揭示了其功能。此外，我们探讨了认知诊断模型、题库构建和CAT中的测试控制，探索了机器学习如何优化这些组成部分。通过对当前情况的分析，

    arXiv:2404.00712v1 Announce Type: cross  Abstract: Computerized Adaptive Testing (CAT) provides an efficient and tailored method for assessing the proficiency of examinees, by dynamically adjusting test questions based on their performance. Widely adopted across diverse fields like education, healthcare, sports, and sociology, CAT has revolutionized testing practices. While traditional methods rely on psychometrics and statistics, the increasing complexity of large-scale testing has spurred the integration of machine learning techniques. This paper aims to provide a machine learning-focused survey on CAT, presenting a fresh perspective on this adaptive testing method. By examining the test question selection algorithm at the heart of CAT's adaptivity, we shed light on its functionality. Furthermore, we delve into cognitive diagnosis models, question bank construction, and test control within CAT, exploring how machine learning can optimize these components. Through an analysis of curre
    
[^61]: 语音语言模型的尺度特性

    Scaling Properties of Speech Language Models

    [https://arxiv.org/abs/2404.00685](https://arxiv.org/abs/2404.00685)

    通过研究语音语言模型的尺度特性，可以预测其语言性能的扩展速度，与文本-based大型语言模型相比，语音语言模型的语言性能扩展速度慢三个数量级。

    

    语音语言模型（SLM）旨在从原始音频中学习语言，而无需文本资源。尽管取得了显著进展，我们当前的模型表现出弱的句法和语义能力。然而，如果神经语言模型的尺度特性对语音模态成立，这些能力将随着训练所使用的计算量增加而提高。本文利用模型的尺度行为来估计我们当前方法将产生具有文本-based大型语言模型（LLMs）英语熟练度的SLM的尺度。

    arXiv:2404.00685v1 Announce Type: cross  Abstract: Speech Language Models (SLMs) aim to learn language from raw audio, without textual resources. Despite significant advances, our current models exhibit weak syntax and semantic abilities. However, if the scaling properties of neural language models hold for the speech modality, these abilities will improve as the amount of compute used for training increases. In this paper, we use models of this scaling behavior to estimate the scale at which our current methods will yield a SLM with the English proficiency of text-based Large Language Models (LLMs). We establish a strong correlation between pre-training loss and downstream syntactic and semantic performance in SLMs and LLMs, which results in predictable scaling of linguistic performance. We show that the linguistic performance of SLMs scales up to three orders of magnitude more slowly than that of text-based LLMs. Additionally, we study the benefits of synthetic data designed to boost
    
[^62]: 多向量密集检索中的生成式检索

    Generative Retrieval as Multi-Vector Dense Retrieval

    [https://arxiv.org/abs/2404.00684](https://arxiv.org/abs/2404.00684)

    本文填补了这一空白，通过展示生成式检索和多向量密集检索共享相同的框架，用于衡量文档与查询相关性。

    

    arXiv:2404.00684v1 公告类型: 跨领域  摘要: 生成式检索以端对端方式使用序列到序列架构为给定查询生成相关文档的标识符。生成式检索与其他检索方法之间的关系，特别是基于密集检索模型内匹配的方法，尚未完全理解。先前的研究表明，带有原子标识符的生成式检索等效于单向量密集检索。因此，当使用分层语义标识符时，生成式检索表现出类似于密集检索中树索引内的分层搜索的行为。然而，先前的研究仅专注于检索阶段，未考虑生成式检索解码器内的深层交互。

    arXiv:2404.00684v1 Announce Type: cross  Abstract: Generative retrieval generates identifiers of relevant documents in an end-to-end manner using a sequence-to-sequence architecture for a given query. The relation between generative retrieval and other retrieval methods, especially those based on matching within dense retrieval models, is not yet fully comprehended. Prior work has demonstrated that generative retrieval with atomic identifiers is equivalent to single-vector dense retrieval. Accordingly, generative retrieval exhibits behavior analogous to hierarchical search within a tree index in dense retrieval when using hierarchical semantic identifiers. However, prior work focuses solely on the retrieval stage without considering the deep interactions within the decoder of generative retrieval.   In this paper, we fill this gap by demonstrating that generative retrieval and multi-vector dense retrieval share the same framework for measuring the relevance to a query of a document. Sp
    
[^63]: LLM meets Vision-Language Models用于零样本单类分类

    LLM meets Vision-Language Models for Zero-Shot One-Class Classification

    [https://arxiv.org/abs/2404.00675](https://arxiv.org/abs/2404.00675)

    提出了一种两步解决方案，结合大型语言模型和视觉-语言预训练模型，用于零样本单类分类问题，并在实际基准测试中表现出优越性能。

    

    我们考虑零样本单类视觉分类问题。在这种情况下，仅目标类别的标签可用，并且目标是在不需要来自目标任务的任何验证示例的情况下区分正负查询样本。我们提出了一个两步解决方案，首先查询大型语言模型以查找在视觉上令人困惑的对象，然后依赖于视觉语言预训练模型（例如CLIP）进行分类。通过调整大规模视觉基准，我们展示了所提出的方法在此设置中优于自适应的现成替代方法的能力。具体而言，我们提出了一个实际的基准，其中负查询样本从与正查询样本相同的原始数据集中获取，包括iNaturalist的细粒度控制版本，其中负样本在分类树中与正样本相距固定距离。我们的工作表明，可以通过查询大型语言模型并依赖视觉-语言预训练模型来解决零样本单类分类问题。

    arXiv:2404.00675v1 Announce Type: cross  Abstract: We consider the problem of zero-shot one-class visual classification. In this setting, only the label of the target class is available, and the goal is to discriminate between positive and negative query samples without requiring any validation example from the target task. We propose a two-step solution that first queries large language models for visually confusing objects and then relies on vision-language pre-trained models (e.g., CLIP) to perform classification. By adapting large-scale vision benchmarks, we demonstrate the ability of the proposed method to outperform adapted off-the-shelf alternatives in this setting. Namely, we propose a realistic benchmark where negative query samples are drawn from the same original dataset as positive ones, including a granularity-controlled version of iNaturalist, where negative samples are at a fixed distance in the taxonomy tree from the positive ones. Our work shows that it is possible to 
    
[^64]: 隐私保护型模型解释研究综述：隐私风险、攻击和对抗措施

    A Survey of Privacy-Preserving Model Explanations: Privacy Risks, Attacks, and Countermeasures

    [https://arxiv.org/abs/2404.00673](https://arxiv.org/abs/2404.00673)

    本研究是第一个全面调查模型解释中隐私攻击及其对抗措施的论文，通过分类隐私攻击和对抗措施，初步探讨了隐私泄漏原因，提出未解决问题和未来研究方向。

    

    随着可解释人工智能（XAI）的采用不断扩大，解决其隐私影响的紧迫性变得更加迫切。尽管在人工智能隐私和可解释性方面有越来越多的研究，但对于隐私保护型模型解释却鲜有关注。本文首次全面调查了模型解释的隐私攻击及其对抗措施。我们在这一领域的贡献包括对研究论文进行彻底分析，并提供了一个相互连接的分类法，便于根据目标解释对隐私攻击和对抗措施进行分类。本研究还对隐私泄漏原因进行了初步调查。最后，我们讨论了我们分析中发现的未解决问题和未来研究方向。该调查旨在成为研究界的宝贵资源，并为这一领域的新手提供明确的见解。

    arXiv:2404.00673v1 Announce Type: cross  Abstract: As the adoption of explainable AI (XAI) continues to expand, the urgency to address its privacy implications intensifies. Despite a growing corpus of research in AI privacy and explainability, there is little attention on privacy-preserving model explanations. This article presents the first thorough survey about privacy attacks on model explanations and their countermeasures. Our contribution to this field comprises a thorough analysis of research papers with a connected taxonomy that facilitates the categorisation of privacy attacks and countermeasures based on the targeted explanations. This work also includes an initial investigation into the causes of privacy leaks. Finally, we discuss unresolved issues and prospective research directions uncovered in our analysis. This survey aims to be a valuable resource for the research community and offers clear insights for those new to this domain. To support ongoing research, we have estab
    
[^65]: 通过令牌扩展实现Transformer的一般高效训练

    A General and Efficient Training for Transformer via Token Expansion

    [https://arxiv.org/abs/2404.00672](https://arxiv.org/abs/2404.00672)

    本文提出了一种名为ToE的令牌生长方案，旨在通过加速一致性训练来改善ViT的训练效果。

    

    Vision Transformer（ViT）通常需要极大的训练成本才能取得显著性能。本文提出一种新的令牌生长方案Token Expansion (ToE)，以实现ViT的一致性训练加速。我们引入了一个“初始化-扩展-合并”管道，以保持原始Transformer的中间特征分布的完整性，防止在训练过程中丢失关键的可学习信息。

    arXiv:2404.00672v1 Announce Type: cross  Abstract: The remarkable performance of Vision Transformers (ViTs) typically requires an extremely large training cost. Existing methods have attempted to accelerate the training of ViTs, yet typically disregard method universality with accuracy dropping. Meanwhile, they break the training consistency of the original transformers, including the consistency of hyper-parameters, architecture, and strategy, which prevents them from being widely applied to different Transformer networks. In this paper, we propose a novel token growth scheme Token Expansion (termed ToE) to achieve consistent training acceleration for ViTs. We introduce an "initialization-expansion-merging" pipeline to maintain the integrity of the intermediate feature distribution of original transformers, preventing the loss of crucial learnable information in the training process. ToE can not only be seamlessly integrated into the training and fine-tuning process of transformers (e
    
[^66]: 关于为技术文档构建RAG系统的观察

    Observations on Building RAG Systems for Technical Documents

    [https://arxiv.org/abs/2404.00657](https://arxiv.org/abs/2404.00657)

    研究回顾了对技术文档RAG系统构建的重要因素，并通过实验证明了最佳实践和潜在挑战

    

    RAG（检索增强生成）用于技术文档时存在挑战，因为嵌入通常无法捕捉领域信息。我们回顾了影响RAG的重要因素的先前研究，并进行实验以突出构建技术文档RAG系统的最佳实践和潜在挑战。

    arXiv:2404.00657v1 Announce Type: cross  Abstract: Retrieval augmented generation (RAG) for technical documents creates challenges as embeddings do not often capture domain information. We review prior art for important factors affecting RAG and perform experiments to highlight best practices and potential challenges to build RAG systems for technical documents.
    
[^67]: WavLLM：面向稳健和自适应语音大语言模型

    WavLLM: Towards Robust and Adaptive Speech Large Language Model

    [https://arxiv.org/abs/2404.00656](https://arxiv.org/abs/2404.00656)

    WavLLM是一个稳健和自适应语音大语言模型，引入了双编码器和Prompt-aware LoRA权重适配器，通过两阶段课程学习方法优化，解耦不同类型的语音信息，为处理语义内容和说话者身份的独特特征提供了新思路

    

    近年来，大型语言模型(LLMs)的最新进展彻底改变了自然语言处理领域，逐渐拓宽了它们的范围到多模态感知和生成。然而，有效地将听觉能力整合到LLMs中会带来显著挑战，特别是在泛化跨不同语境和执行复杂听觉任务方面。在这项工作中，我们引入了WavLLM，一个具有双编码器和Prompt-aware LoRA权重适配器的稳健和自适应语音大语言模型，通过两阶段课程学习方法进行优化。利用双编码器，我们解耦不同类型的语音信息，利用Whisper编码器处理语音的语义内容，利用WavLM编码器捕捉说话者身份的独特特征。在课程学习框架内，WavLLM首先通过混合要素进行优化来建立其基础能力

    arXiv:2404.00656v1 Announce Type: cross  Abstract: The recent advancements in large language models (LLMs) have revolutionized the field of natural language processing, progressively broadening their scope to multimodal perception and generation. However, effectively integrating listening capabilities into LLMs poses significant challenges, particularly with respect to generalizing across varied contexts and executing complex auditory tasks. In this work, we introduce WavLLM, a robust and adaptive speech large language model with dual encoders, and a prompt-aware LoRA weight adapter, optimized by a two-stage curriculum learning approach. Leveraging dual encoders, we decouple different types of speech information, utilizing a Whisper encoder to process the semantic content of speech, and a WavLM encoder to capture the unique characteristics of the speaker's identity. Within the curriculum learning framework, WavLLM first builds its foundational capabilities by optimizing on mixed elemen
    
[^68]: 通过基于模型的内在动机学习离线策略以实现主动在线探索

    Learning Off-policy with Model-based Intrinsic Motivation For Active Online Exploration

    [https://arxiv.org/abs/2404.00651](https://arxiv.org/abs/2404.00651)

    本文提出了一种结合预测模型和离线学习元素的强化学习算法，通过内在奖励与模型不确定性的关联，在连续控制任务中实现了样本有效的探索。

    

    在深度强化学习领域，最近的进展在样本效率方面取得了显着进展，涵盖了基于模型的方法和无模型的方法。本文研究了如何在连续控制任务中实现样本有效的探索。我们引入了一种强化学习算法，结合了预测模型和离线学习元素，其中在线规划器通过一种新颖感知的终端价值函数用于样本收集。利用潜在状态空间中的前向预测错误，我们推导出了一种内在奖励，而不会产生参数开销。该奖励建立了与模型不确定性的牢固联系，使代理能够有效地克服渐近性能差距。

    arXiv:2404.00651v1 Announce Type: cross  Abstract: Recent advancements in deep reinforcement learning (RL) have demonstrated notable progress in sample efficiency, spanning both model-based and model-free paradigms. Despite the identification and mitigation of specific bottlenecks in prior works, the agent's exploration ability remains under-emphasized in the realm of sample-efficient RL. This paper investigates how to achieve sample-efficient exploration in continuous control tasks. We introduce an RL algorithm that incorporates a predictive model and off-policy learning elements, where an online planner enhanced by a novelty-aware terminal value function is employed for sample collection. Leveraging the forward predictive error within a latent state space, we derive an intrinsic reward without incurring parameters overhead. This reward establishes a solid connection to model uncertainty, allowing the agent to effectively overcome the asymptotic performance gap. Through extensive expe
    
[^69]: 学习生成条件化三平面用于3D感知表情可控肖像动画

    Learning to Generate Conditional Tri-plane for 3D-aware Expression Controllable Portrait Animation

    [https://arxiv.org/abs/2404.00636](https://arxiv.org/abs/2404.00636)

    本文提出了一种一次性的3D感知肖像动画方法Export3D，通过引入三平面生成器和对比预训练框架，实现了控制给定肖像图像的面部表情和摄像机视角，提供了一种新的表达方式。

    

    在本文中，我们提出了一种一次性的3D感知肖像动画方法Export3D，能够控制给定肖像图像的面部表情和摄像机视角。为了实现这一目标，我们引入了一个三平面生成器，通过将3DMM的表情参数转移到源图像中直接生成3D先验的三平面。然后，通过可微分体积渲染将三平面解码为不同视角的图像。现有的肖像动画方法严重依赖于图像变形来在运动空间中传输表情，挑战在外观和表情的分离上。相比之下，我们提出了一个用于无外观表情参数的对比预训练框架，消除了在传输跨身份表达时不良外观交换。大量实验证明，我们的预训练框架能够学习隐藏在3DMM中的无外观表达表示。

    arXiv:2404.00636v1 Announce Type: cross  Abstract: In this paper, we present Export3D, a one-shot 3D-aware portrait animation method that is able to control the facial expression and camera view of a given portrait image. To achieve this, we introduce a tri-plane generator that directly generates a tri-plane of 3D prior by transferring the expression parameter of 3DMM into the source image. The tri-plane is then decoded into the image of different view through a differentiable volume rendering. Existing portrait animation methods heavily rely on image warping to transfer the expression in the motion space, challenging on disentanglement of appearance and expression. In contrast, we propose a contrastive pre-training framework for appearance-free expression parameter, eliminating undesirable appearance swap when transferring a cross-identity expression. Extensive experiments show that our pre-training framework can learn the appearance-free expression representation hidden in 3DMM, and 
    
[^70]: 从未标记数据中学习语言建模规划

    Learning to Plan for Language Modeling from Unlabeled Data

    [https://arxiv.org/abs/2404.00614](https://arxiv.org/abs/2404.00614)

    通过自监督学习目标训练一个用于规划未来写作过程的模块，扩展了成功的语言模型公式到更抽象的规划中，改善了语言建模的性能，特别是在文本结构方面，同时新的规划模块可以大规模训练并轻松与社区共享。

    

    通过训练来预测未标记语料库中的下一个标记，大型语言模型学会执行许多任务，而无需任何标记数据。然而，它们的下一个标记预测目标可以说限制了它们在需要规划的场景中的性能，比如写作一篇连贯的文章。在这篇论文中，我们通过自监督学习目标训练一个用于规划未来写作过程的模块。通过根据生成的潜在计划进行条件化，我们的模型以无监督的方式将成功的语言模型公式扩展到更抽象的规划中。实验上，我们证明了我们的方法在一般情况下改善了语言建模的性能，特别是在文本结构方面。由于我们的框架使用的是无监督且外部于语言模型的规划模块，因此新的规划模块可以大规模训练，并且能够轻松地与社区共享。

    arXiv:2404.00614v1 Announce Type: cross  Abstract: By training to predict the next token in an unlabeled corpus, large language models learn to perform many tasks without any labeled data. However, their next-token-prediction objective arguably limits their performance in scenarios that require planning, such as writing a coherent article. In this paper, we train a module for planning the future writing process via a self-supervised learning objective. By conditioning on generated latent plans, our model extends the successful language model formula to more abstract planning in an unsupervised way. Empirically, we demonstrate that our method improves language modeling performance in general, particularly with respect to the text structure. Because our framework uses a planner module that is unsupervised and external to the language model, new planner modules can be trained at large scale and easily be shared with the community.
    
[^71]: 广泛的自对比使得无需反馈的语言模型对齐成为可能

    Extensive Self-Contrast Enables Feedback-Free Language Model Alignment

    [https://arxiv.org/abs/2404.00604](https://arxiv.org/abs/2404.00604)

    本论文介绍了一种利用广泛自对比生成负例的无需反馈的大型语言模型对齐方法，该方法在实验中表现优于直接偏好优化方法。

    

    人类反馈的强化学习（RLHF）一直是最近大型语言模型（LLM）对齐的核心技术。然而，其严重依赖昂贵的人类或LLM作为评判者的偏好反馈可能会阻碍其更广泛的应用。在这项工作中，我们引入了Self-Contrast，一种通过利用广泛自动生成的负例来进行无需反馈的大型语言模型对齐方法。仅通过监督的微调（SFT）目标，Self-Contrast利用LLM本身生成大量多样的候选项，并利用预训练的嵌入模型根据文本相似性过滤多个负例。理论上，我们证明了在这种设置中，仅仅扩大负面回应仍然可以有效地近似具有更平衡的正面和负面偏好注释的情况。我们对三个数据集进行了直接偏好优化（DPO）的实验表明，Self-Contrast能够始终优于

    arXiv:2404.00604v1 Announce Type: cross  Abstract: Reinforcement learning from human feedback (RLHF) has been a central technique for recent large language model (LLM) alignment. However, its heavy dependence on costly human or LLM-as-Judge preference feedback could stymie its wider applications. In this work, we introduce Self-Contrast, a feedback-free large language model alignment method via exploiting extensive self-generated negatives. With only supervised fine-tuning (SFT) targets, Self-Contrast leverages the LLM itself to generate massive diverse candidates, and harnesses a pre-trained embedding model to filter multiple negatives according to text similarity. Theoretically, we illustrate that in this setting, merely scaling negative responses can still effectively approximate situations with more balanced positive and negative preference annotations. Our experiments with direct preference optimization (DPO) on three datasets show that, Self-Contrast could consistently outperform
    
[^72]: AI法律和大型语言模型（LLMs）：当关键问题和隐私影响需要人类和道德监督时

    AI Act and Large Language Models (LLMs): When critical issues and privacy impact require human and ethical oversight

    [https://arxiv.org/abs/2404.00600](https://arxiv.org/abs/2404.00600)

    论文讨论了人类监督、道德监督和隐私影响评估在面对人工智能系统和大型语言模型发展中的重要性。

    

    人工智能系统的日益发展，特别是大型语言模型（LLM）的发展，使得有必要对它们在隐私、个人数据保护以及道德层面，尤其是对最脆弱和最弱势群体可能产生的风险和影响进行评估。本文对人类监督、道德监督和隐私影响评估进行了讨论。

    arXiv:2404.00600v1 Announce Type: cross  Abstract: The imposing evolution of artificial intelligence systems and, specifically, of Large Language Models (LLM) makes it necessary to carry out assessments of their level of risk and the impact they may have in the area of privacy, personal data protection and at an ethical level, especially on the weakest and most vulnerable. This contribution addresses human oversight, ethical oversight, and privacy impact assessment.
    
[^73]: EvoCodeBench: 一个与现实世界代码库对齐的进化代码生成基准

    EvoCodeBench: An Evolving Code Generation Benchmark Aligned with Real-World Code Repositories

    [https://arxiv.org/abs/2404.00599](https://arxiv.org/abs/2404.00599)

    EvoCodeBench 是一个与现实世界代码库对齐的进化代码生成基准，具有完善的注释和评估度量标准，同时避免数据泄露。

    

    arXiv:2404.00599v1 公告类型: 跨界 摘要: 如何评估大型语言模型(LLMs)在代码生成中的表现是一个悬而未决的问题。现有的基准测试表现出与现实世界代码库的差距，不足以评估LLMs的编码能力。本文提出了一个新的基准测试 - EvoCodeBench来解决前述问题，其具有三个主要进展。(1) EvoCodeBench在多个维度上与现实世界库对齐，例如代码分布和依赖分布。(2) EvoCodeBench提供了全面的注释(例如需求、参考代码和参考依赖)，以及强大的评估度量标准(例如Pass@k和Recall@k)。(3) EvoCodeBench是一个不断发展的基准测试，以避免数据泄漏。我们构建了一个自动化流水线，从最新的库中更新EvoCodeBench。我们发布了第一个版本 - EvoCodeBench-2403，其中包含来自25个现实世界库的275个样本。基于EvoCodeBench，我们提出repo

    arXiv:2404.00599v1 Announce Type: cross  Abstract: How to evaluate Large Language Models (LLMs) in code generation is an open question. Existing benchmarks demonstrate poor alignment with real-world code repositories and are insufficient to evaluate the coding abilities of LLMs. This paper proposes a new benchmark - EvoCodeBench to address the preceding problems, which has three primary advances. (1) EvoCodeBench aligns with real-world repositories in multiple dimensions, e.g., code distributions and dependency distributions. (2) EvoCodeBench offers comprehensive annotations (e.g., requirements, reference code, and reference dependencies), and robust evaluation metrics (e.g., Pass@k and Recall@k). (3) EvoCodeBench is an evolving benchmark to avoid data leakage. We build an automatic pipeline to update EvoCodeBench from the latest repositories. We release the first version - EvoCodeBench-2403, containing 275 samples from 25 real-world repositories. Based on EvoCodeBench, we propose repo
    
[^74]: LAESI: 使用合成图像进行叶片面积估计

    LAESI: Leaf Area Estimation with Synthetic Imagery

    [https://arxiv.org/abs/2404.00593](https://arxiv.org/abs/2404.00593)

    LAESI数据集包含10万张合成叶片图像，用于叶形态分析，机器学习模型训练后可预测叶片表面积，并提供了基于3D程序模型和生成式AI的高效框架。

    

    我们引入了LAESI，这是一个包含了10万张合成叶片图像的合成叶片数据集，每张图像都有语义分割掩模和表面积标注，图像背景是毫米纸。该数据集主要用于对山毛榉和橡树叶进行形态分析。我们通过训练机器学习模型进行叶片表面积预测和语义分割来评估数据集的适用性，使用真实图像进行验证。验证结果表明，这些模型可以训练出预测叶面积的能力，相对误差不会超过平均人类标注员的误差。LAESI还提供了一个基于3D程序模型和生成式人工智能的高效框架，可用于大规模、可控的数据生成，在农业和生物学等领域具有潜在应用。我们评估了将生成式人工智能纳入我们的程序化数据生成流程中，并展示了基于注释一致性的数据过滤如何导致数据集的生成。

    arXiv:2404.00593v1 Announce Type: cross  Abstract: We introduce LAESI, a Synthetic Leaf Dataset of 100,000 synthetic leaf images on millimeter paper, each with semantic masks and surface area labels. This dataset provides a resource for leaf morphology analysis primarily aimed at beech and oak leaves. We evaluate the applicability of the dataset by training machine learning models for leaf surface area prediction and semantic segmentation, using real images for validation. Our validation shows that these models can be trained to predict leaf surface area with a relative error not greater than an average human annotator. LAESI also provides an efficient framework based on 3D procedural models and generative AI for the large-scale, controllable generation of data with potential further applications in agriculture and biology. We evaluate the inclusion of generative AI in our procedural data generation pipeline and show how data filtering based on annotation consistency results in dataset
    
[^75]: 基于记忆的交叉模态语义对齐网络用于放射学报告生成

    Memory-based Cross-modal Semantic Alignment Network for Radiology Report Generation

    [https://arxiv.org/abs/2404.00588](https://arxiv.org/abs/2404.00588)

    提出了一种基于记忆的交叉模态语义对齐网络用于减少工作量并生成准确放射学报告

    

    生成放射学报告可以自动减少放射科医生的工作量并有助于特定疾病的诊断。然而，由于涉及疾病的关键信息在图像和报告中所占比例很小，模型很难学习放射学图像和报告之间的潜在关系，从而无法生成流畅和准确的放射学报告。为了解决这个问题，我们提出了一种基于记忆的交叉模态语义对齐模型（MCSAM），采用编码-解码范式。MCSAM包括一个良好初始化的长期临床记忆库，用于学习与疾病相关的表示以及不同模态的先验知识，以检索并利用检索到的记忆来进行特征整合。为了确保检索到的跨模态先验知识的语义一致性，提出了一个跨模态语义对齐模

    arXiv:2404.00588v1 Announce Type: cross  Abstract: Generating radiology reports automatically reduces the workload of radiologists and helps the diagnoses of specific diseases. Many existing methods take this task as modality transfer process. However, since the key information related to disease accounts for a small proportion in both image and report, it is hard for the model to learn the latent relation between the radiology image and its report, thus failing to generate fluent and accurate radiology reports. To tackle this problem, we propose a memory-based cross-modal semantic alignment model (MCSAM) following an encoder-decoder paradigm. MCSAM includes a well initialized long-term clinical memory bank to learn disease-related representations as well as prior knowledge for different modalities to retrieve and use the retrieved memory to perform feature consolidation. To ensure the semantic consistency of the retrieved cross modal prior knowledge, a cross-modal semantic alignment m
    
[^76]: RLGNet：用于时间知识图推理的重复-局部-全局历史网络

    RLGNet: Repeating-Local-Global History Network for Temporal Knowledge Graph Reasoning

    [https://arxiv.org/abs/2404.00586](https://arxiv.org/abs/2404.00586)

    RLGNet提出了重复-局部-全局历史网络，利用全局历史编码器和局部历史编码器分别捕捉历史信息的整体和相关细节，以有效进行时间知识图推理。

    

    arXiv:2404.00586v1 公告类型：新 摘要：时间知识图（TKG）推理基于历史信息以预测未来。因此，解析和挖掘历史信息对预测未来至关重要。大多数现有方法未能同时从全局和局部角度处理和理解历史信息。忽视全局视图可能导致忽略宏观趋势和模式，而忽视局部视图可能导致遗漏关键详细信息。此外，一些方法不关注从高频重复事件中学习，这意味着它们可能无法充分掌握频繁发生的历史事件。基于此，我们提出了\textbf{R}epetitive-\textbf{L}ocal-\textbf{G}lobal History \textbf{Net}work（RLGNet）。我们利用全局历史编码器捕捉历史信息的全面性质。随后，局部历史编码器提供与查询时间戳相关的信息。

    arXiv:2404.00586v1 Announce Type: new  Abstract: Temporal Knowledge Graph (TKG) reasoning is based on historical information to predict the future. Therefore, parsing and mining historical information is key to predicting the future. Most existing methods fail to concurrently address and comprehend historical information from both global and local perspectives. Neglecting the global view might result in overlooking macroscopic trends and patterns, while ignoring the local view can lead to missing critical detailed information. Additionally, some methods do not focus on learning from high-frequency repeating events, which means they may not fully grasp frequently occurring historical events. To this end, we propose the \textbf{R}epetitive-\textbf{L}ocal-\textbf{G}lobal History \textbf{Net}work(RLGNet). We utilize a global history encoder to capture the overarching nature of historical information. Subsequently, the local history encoder provides information related to the query timestam
    
[^77]: 使用生成模型的现代推荐系统（Gen-RecSys）综述

    A Review of Modern Recommender Systems Using Generative Models (Gen-RecSys)

    [https://arxiv.org/abs/2404.00579](https://arxiv.org/abs/2404.00579)

    生成模型在现代推荐系统中展现出了强大的潜力，能够同时处理用户-项目交互历史、文本、图像和视频等复杂数据，为推荐系统带来了新的可能性。

    

    传统的推荐系统（RS）通常使用用户-项目评分历史作为主要数据来源，协同过滤是主要方法之一。然而，生成模型最近已经具备了建模和采样复杂数据分布的能力，不仅可以涵盖用户-项目交互历史，还可以包括文本、图像和视频，为新颖的推荐任务解锁了丰富的数据。通过这篇全面的、多学科的调研，我们旨在探讨使用生成模型（Gen-RecSys）在RS中的关键进展，包括：基于交互驱动的生成模型的基础概述；大型语言模型（LLM）在生成推荐、检索和对话推荐中的应用；以及用于处理和生成RS中的图像和视频内容的多模态模型的整合。我们的整体视角使我们能够强调评估所需范式的重要性。

    arXiv:2404.00579v1 Announce Type: cross  Abstract: Traditional recommender systems (RS) have used user-item rating histories as their primary data source, with collaborative filtering being one of the principal methods. However, generative models have recently developed abilities to model and sample from complex data distributions, including not only user-item interaction histories but also text, images, and videos - unlocking this rich data for novel recommendation tasks. Through this comprehensive and multi-disciplinary survey, we aim to connect the key advancements in RS using Generative Models (Gen-RecSys), encompassing: a foundational overview of interaction-driven generative models; the application of large language models (LLM) for generative recommendation, retrieval, and conversational recommendation; and the integration of multimodal models for processing and generating image and video content in RS. Our holistic perspective allows us to highlight necessary paradigms for eval
    
[^78]: 自动双向加权集成算法及其在脑肿瘤检测和分类中的应用

    Automated Bi-Fold Weighted Ensemble Algorithms and its Application to Brain Tumor Detection and Classification

    [https://arxiv.org/abs/2404.00576](https://arxiv.org/abs/2404.00576)

    提出了两种新型的自动双向加权集成算法，用于提高脑肿瘤检测和分类的效果

    

    脑细胞的不受控制和无结构生长被称为脑肿瘤，它在各种癌症中拥有最高的死亡率之一。由于有限的诊断和治疗能力，尤其是在第三世界国家，脑肿瘤带来了重大挑战。早期诊断在有效管理脑肿瘤和减少死亡率方面起着至关重要的作用。然而，由于高成本和长时间的结果获取时间等各种限制，诊断方法的可用性受到阻碍，从而阻碍了疾病的早期检测。在本研究中，我们提出了两种前沿的双向加权投票集成模型，旨在提高加权集成方法的效力。这两种提出的方法结合了来自多个分类器的分类结果，并通过选择第一种方法中具有最高概率的结果，以及最高加权 p

    arXiv:2404.00576v1 Announce Type: cross  Abstract: The uncontrolled and unstructured growth of brain cells is known as brain tumor, which has one of the highest mortality rates among diseases from all types of cancers. Due to limited diagnostic and treatment capabilities, they pose significant challenges, especially in third-world countries. Early diagnosis plays a vital role in effectively managing brain tumors and reducing mortality rates. However, the availability of diagnostic methods is hindered by various limitations, including high costs and lengthy result acquisition times, impeding early detection of the disease. In this study, we present two cutting-edge bi-fold weighted voting ensemble models that aim to boost the effectiveness of weighted ensemble methods. These two proposed methods combine the classification outcomes from multiple classifiers and determine the optimal result by selecting the one with the highest probability in the first approach, and the highest weighted p
    
[^79]: 学习推理中长度泛化的理论

    A Theory for Length Generalization in Learning to Reason

    [https://arxiv.org/abs/2404.00560](https://arxiv.org/abs/2404.00560)

    该论文提出了针对学习推理中长度泛化问题的理论研究，设计了基于DAGs的问题表示，并使用Transformer来实现完美的长度泛化。

    

    长度泛化（LG）是学习推理中的一个挑战性问题。它指的是当在较短长度或大小的推理问题上训练时，所得到的模型在处理较长长度或大小的问题时会遇到困难。尽管许多研究人员已经研究了LG，但挑战依然存在。本文提出了一个针对可以被建模为有向无环图（DAGs）的问题的LG的理论研究。该论文首先确定并证明了在学习推理中可以实现LG的条件。然后基于这一理论设计了问题表示，以学习解决诸如奇偶、加法和乘法等具有挑战性的推理问题，使用Transformer实现完美的LG。

    arXiv:2404.00560v1 Announce Type: new  Abstract: Length generalization (LG) is a challenging problem in learning to reason. It refers to the phenomenon that when trained on reasoning problems of smaller lengths or sizes, the resulting model struggles with problems of larger sizes or lengths. Although LG has been studied by many researchers, the challenge remains. This paper proposes a theoretical study of LG for problems whose reasoning processes can be modeled as DAGs (directed acyclic graphs). The paper first identifies and proves the conditions under which LG can be achieved in learning to reason. It then designs problem representations based on the theory to learn to solve challenging reasoning problems like parity, addition, and multiplication, using a Transformer to achieve perfect LG.
    
[^80]: 视觉任务的深度外在流形表示

    Deep Extrinsic Manifold Representation for Vision Tasks

    [https://arxiv.org/abs/2404.00544](https://arxiv.org/abs/2404.00544)

    提出了一种名为深度外在流形表示（DEMR）的技巧，将外在流形嵌入融入到深度神经网络中，帮助生成流形表示，并通过实证证据支持其在 $SE(3)$ 等流形上的可行性、渐近性质和泛化能力。

    

    非欧几里德数据在不同领域经常遇到，但关于用流形表示作为输出训练神经网络的基本挑战的文献有限。我们在这个背景下引入了一个名为深度外在流形表示（Deep Extrinsic Manifold Representation，DEMR）的技巧，用于视觉任务。DEMR将外在流形嵌入融入到深度神经网络中，有助于生成流形表示。DEMR方法不直接优化复杂的测地线损失，而是专注于优化嵌入欧几里德空间内的计算图，从而适应各种架构需求。我们提供了支持所提概念的实证证据，涉及两种流形，$SE(3)$ 及其关联商流形。这些证据为所提取概念的可行性、渐近性质和泛化能力提供了理论保证。

    arXiv:2404.00544v1 Announce Type: cross  Abstract: Non-Euclidean data is frequently encountered across different fields, yet there is limited literature that addresses the fundamental challenge of training neural networks with manifold representations as outputs. We introduce the trick named Deep Extrinsic Manifold Representation (DEMR) for visual tasks in this context. DEMR incorporates extrinsic manifold embedding into deep neural networks, which helps generate manifold representations. The DEMR approach does not directly optimize the complex geodesic loss. Instead, it focuses on optimizing the computation graph within the embedded Euclidean space, allowing for adaptability to various architectural requirements. We provide empirical evidence supporting the proposed concept on two types of manifolds, $SE(3)$ and its associated quotient manifolds. This evidence offers theoretical assurances regarding feasibility, asymptotic properties, and generalization capability. The experimental re
    
[^81]: 具身主动防御：利用循环反馈对抗对抗性贴片

    Embodied Active Defense: Leveraging Recurrent Feedback to Counter Adversarial Patches

    [https://arxiv.org/abs/2404.00540](https://arxiv.org/abs/2404.00540)

    利用人类主动感知和循环反馈机制，提出了具身主动防御（EAD）策略，在3D实际环境中主动上下文化环境信息以对抗对抗性贴片。

    

    深度神经网络对对抗性贴片的脆弱性促使了许多防御策略的产生，以增强模型的健壮性。然而，现有的防御依赖于单一观察或预先建立的对手信息来对抗对抗性贴片，往往无法应对未知或自适应的对抗性攻击，在动态的3D环境中表现出不尽如人意的性能。受人类主动感知和循环反馈机制的启发，我们开发了具身主动防御（EAD），这是一种积极的防御策略，主动地将环境信息整合到3D实际环境中，以解决对抗性贴片不匹配的问题。为实现这一目标，EAD开发了两个中心循环子模块，即感知模块和策略模块，以实现主动视觉的两个关键功能。这些模型循环处理一系列信念和观察，促进进展。

    arXiv:2404.00540v1 Announce Type: cross  Abstract: The vulnerability of deep neural networks to adversarial patches has motivated numerous defense strategies for boosting model robustness. However, the prevailing defenses depend on single observation or pre-established adversary information to counter adversarial patches, often failing to be confronted with unseen or adaptive adversarial attacks and easily exhibiting unsatisfying performance in dynamic 3D environments. Inspired by active human perception and recurrent feedback mechanisms, we develop Embodied Active Defense (EAD), a proactive defensive strategy that actively contextualizes environmental information to address misaligned adversarial patches in 3D real-world settings. To achieve this, EAD develops two central recurrent sub-modules, i.e., a perception module and a policy module, to implement two critical functions of active vision. These models recurrently process a series of beliefs and observations, facilitating progress
    
[^82]: 将坏苹果与好橘子进行比较：通过联合优化偏好对齐大型语言模型

    Comparing Bad Apples to Good Oranges: Aligning Large Language Models via Joint Preference Optimization

    [https://arxiv.org/abs/2404.00530](https://arxiv.org/abs/2404.00530)

    本研究提出了一种新的偏好获取方法，通过DOVE协议对指令-响应对的联合概率进行优化，以对齐大型语言模型。

    

    一种常见的对齐大型语言模型（LLMs）的技术依赖于通过比较在固定上下文中条件生成的多个生成的人类偏好。然而，当这些生成放置在相同的上下文中时，这仅利用了成对比较。然而，这种条件排名通常无法捕获人类偏好的复杂和多维方面。在这项工作中，我们重新审视偏好获取的传统范式，并提出了一个基于在指令-响应对上联合引发偏好的新轴。虽然先前的偏好优化是针对条件排名协议（例如，DPO）设计的，但我们提出的偏好获取协议引入了DOVE，这是一个新的偏好优化目标，通过提升所选指令-响应对的联合概率来降低所拒绝指令-响应对的概率。

    arXiv:2404.00530v1 Announce Type: cross  Abstract: A common technique for aligning large language models (LLMs) relies on acquiring human preferences by comparing multiple generations conditioned on a fixed context. This only leverages the pairwise comparisons when the generations are placed in an identical context. However, such conditional rankings often fail to capture the complex and multidimensional aspects of human preferences. In this work, we revisit the traditional paradigm of preference acquisition and propose a new axis that is based on eliciting preferences jointly over the instruction-response pairs. While prior preference optimizations are designed for conditional ranking protocols (e.g., DPO), our proposed preference acquisition protocol introduces DOVE, a new preference optimization objective that upweights the joint probability of the chosen instruction-response pair over the rejected instruction-response pair. Interestingly, we find that the LLM trained with joint ins
    
[^83]: 游戏持续时间的情感影响：理解长时间游戏中玩家情感的框架

    The Emotional Impact of Game Duration: A Framework for Understanding Player Emotions in Extended Gameplay Sessions

    [https://arxiv.org/abs/2404.00526](https://arxiv.org/abs/2404.00526)

    该研究探讨了游戏持续时间对玩家情绪的影响，建立了情感检测框架，并实验结果表明长时间游戏会显著影响玩家的情绪。

    

    视频游戏自上世纪70年代以来一直在娱乐中发挥着至关重要的作用，在封锁期间变得更加突出，那时人们正在寻找娱乐方式。然而，当时玩家没有意识到游戏时间对他们情绪的重大影响。这使得设计师和开发人员很难创建新游戏，因为他们必须控制这些游戏对玩家情绪的影响。因此，本研究的目的是研究玩家情绪如何受到游戏持续时间的影响。为实现此目标，建立了一种情感检测框架。根据实验结果，志愿者从20分钟到60分钟的一般表达情感能力有所增加。与较短游戏时间相比，实验发现长时间游戏对玩家的情绪确实产生了显著影响。

    arXiv:2404.00526v1 Announce Type: cross  Abstract: Video games have played a crucial role in entertainment since their development in the 1970s, becoming even more prominent during the lockdown period when people were looking for ways to entertain them. However, at that time, players were unaware of the significant impact that playtime could have on their feelings. This has made it challenging for designers and developers to create new games since they have to control the emotional impact that these games will take on players. Thus, the purpose of this study is to look at how a player's emotions are affected by the duration of the game. In order to achieve this goal, a framework for emotion detection is created. According to the experiment's results, the volunteers' general ability to express emotions increased from 20 to 60 minutes. In comparison to shorter gameplay sessions, the experiment found that extended gameplay sessions did significantly affect the player's emotions. According
    
[^84]: 具有重建损失的迁移学习

    Transfer Learning with Reconstruction Loss

    [https://arxiv.org/abs/2404.00505](https://arxiv.org/abs/2404.00505)

    本文通过引入额外的重建阶段和重建损失，提出了一种具有共享模型参数和特征表示的模型训练方法，建立了共同信息的概念，用于解决相关任务。

    

    在大多数利用神经网络进行数学优化的应用中，通常为每个特定优化目标训练一个专用模型。然而，在许多场景中，同一组问题输入上经常需要优化几个不同但相关的目标或任务。与为每个问题单独训练不同的神经网络相比，更有效的方法是利用这些目标之间的相关性，使用共享模型参数和特征表示训练多个神经网络模型。为实现这一目标，本文首先建立了共同信息的概念：解决相关任务所需的共享知识，然后提出了一种新颖的模型训练方法，通过在模型中添加一个额外的重建阶段以及相关的新重建损失。该损失用于从选择的隐藏状态开始重新构建共同信息。

    arXiv:2404.00505v1 Announce Type: cross  Abstract: In most applications of utilizing neural networks for mathematical optimization, a dedicated model is trained for each specific optimization objective. However, in many scenarios, several distinct yet correlated objectives or tasks often need to be optimized on the same set of problem inputs. Instead of independently training a different neural network for each problem separately, it would be more efficient to exploit the correlations between these objectives and to train multiple neural network models with shared model parameters and feature representations. To achieve this, this paper first establishes the concept of common information: the shared knowledge required for solving the correlated tasks, then proposes a novel approach for model training by adding into the model an additional reconstruction stage associated with a new reconstruction loss. This loss is for reconstructing the common information starting from a selected hidde
    
[^85]: 使用合成偏好数据对语言模型进行可配置安全调优

    Configurable Safety Tuning of Language Models with Synthetic Preference Data

    [https://arxiv.org/abs/2404.00495](https://arxiv.org/abs/2404.00495)

    提出了一种名为Configurable Safety Tuning（CST）的新方法，通过使用合成偏好数据，在推断时实现对LLMs的灵活安全配置，允许用户根据需要禁用/启用安全偏好，且实验表明CST成功管理不同的安全配置并保留了原始功能，是一种适用于可配置部署的强大方法。

    

    最先进的语言模型微调技术，如直接偏好优化（DPO），通过将预定义行为硬编码到模型中，限制了用户的控制权。为了解决这个问题，我们提出了一种新颖的方法，Configurable Safety Tuning（CST），它利用合成偏好数据来增强DPO，以促进在推断时对LLMs进行灵活的安全配置。CST通过引入指定安全配置的系统提示，允许LLM部署者根据需要禁用/启用安全偏好，仅需更改系统提示。我们的实验评估表明，CST成功管理不同的安全配置，并保留了LLMs的原始功能，显示出它是一种适用于可配置部署的强大方法。

    arXiv:2404.00495v1 Announce Type: cross  Abstract: State-of-the-art language model fine-tuning techniques, such as Direct Preference Optimization (DPO), restrict user control by hard-coding predefined behaviors into the model. To address this, we propose a novel method, Configurable Safety Tuning (CST), that augments DPO using synthetic preference data to facilitate flexible safety configuration of LLMs at inference time. CST overcomes the constraints of vanilla DPO by introducing a system prompt specifying safety configurations, enabling LLM deployers to disable/enable safety preferences based on their need, just changing the system prompt. Our experimental evaluations indicate that CST successfully manages different safety configurations and retains the original functionality of LLMs, showing it is a robust method for configurable deployment. Data and models available at https://github.com/vicgalle/configurable-safety-tuning
    
[^86]: 时态知识编辑下的多跳问题回答

    Multi-hop Question Answering under Temporal Knowledge Editing

    [https://arxiv.org/abs/2404.00492](https://arxiv.org/abs/2404.00492)

    提出了一个新颖的框架TEMPLE-MQA，通过构建时间感知图和推理路径、结构检索和联合推理阶段，有效地识别多跳问题回答中的时间背景，在基准数据集上显著优于基线模型，并贡献了一个新数据集TKEMQA。

    

    在大语言模型时代，多跳问题回答 (MQA) 在知识编辑 (KE) 下引起了广泛关注。然而，现有的MQA在处理包含显式时间背景的问题时表现不佳。为了解决这一限制，我们提出了一个新颖的框架，即时态知识增强的多跳问题回答 (TEMPLE-MQA)。不同于以往的方法，TEMPLE-MQA首先构建一个时间感知图 (TAG)，以结构化方式存储编辑知识。然后，通过我们提出的推理路径、结构检索和联合推理阶段，TEMPLE-MQA有效地识别问题查询中的时间背景。对基准数据集的实验表明，TEMPLE-MQA显著优于基线模型。此外，我们贡献了一个新数据集，名为TKEMQA，专门为带有时间约束的MQA量身定制，作为首个基准。

    arXiv:2404.00492v1 Announce Type: cross  Abstract: Multi-hop question answering (MQA) under knowledge editing (KE) has garnered significant attention in the era of large language models. However, existing models for MQA under KE exhibit poor performance when dealing with questions containing explicit temporal contexts. To address this limitation, we propose a novel framework, namely TEMPoral knowLEdge augmented Multi-hop Question Answering (TEMPLE-MQA). Unlike previous methods, TEMPLE-MQA first constructs a time-aware graph (TAG) to store edit knowledge in a structured manner. Then, through our proposed inference path, structural retrieval, and joint reasoning stages, TEMPLE-MQA effectively discerns temporal contexts within the question query. Experiments on benchmark datasets demonstrate that TEMPLE-MQA significantly outperforms baseline models. Additionally, we contribute a new dataset, namely TKEMQA, which serves as the inaugural benchmark tailored specifically for MQA with temporal
    
[^87]: PROMPT-SAW：利用关系感知图进行文本提示压缩

    PROMPT-SAW: Leveraging Relation-Aware Graphs for Textual Prompt Compression

    [https://arxiv.org/abs/2404.00489](https://arxiv.org/abs/2404.00489)

    提出了PROMPT-SAW模型，利用关系感知图来实现文本提示的压缩，提高了提示的可读性和可解释性。

    

    大型语言模型(LLMs)在多种不同的自然语言处理任务中展现出卓越的能力。提示是LLM推理中的基本工具，但我们观察到超长提示会带来显著的成本。现有的压缩长提示的尝试导致压缩提示在可读性和可解释性方面表现不佳，对提示效用产生有害影响。为了解决这一问题，我们提出了PROMPT-SAW：通过关系感知图进行提示压缩，这是一种针对任务不可知和任务感知提示的有效策略。PROMPT-SAW使用提示的文本信息构建图形，在图形中提取关键信息元素，从而得出压缩提示。我们还提出了GSM8K-AUG，即现有GSM8k基准的扩展版本，用于任务不可知提示，以提供全面的评估平台。

    arXiv:2404.00489v1 Announce Type: cross  Abstract: Large language models (LLMs) have shown exceptional abilities for multiple different natural language processing tasks. While prompting is a crucial tool for LLM inference, we observe that there is a significant cost associated with exceedingly lengthy prompts. Existing attempts to compress lengthy prompts lead to sub-standard results in terms of readability and interpretability of the compressed prompt, with a detrimental impact on prompt utility. To address this, we propose PROMPT-SAW: Prompt compresSion via Relation AWare graphs, an effective strategy for prompt compression over task-agnostic and task-aware prompts. PROMPT-SAW uses the prompt's textual information to build a graph, later extracts key information elements in the graph to come up with the compressed prompt. We also propose GSM8K-AUG, i.e., an extended version of the existing GSM8k benchmark for task-agnostic prompts in order to provide a comprehensive evaluation platf
    
[^88]: 噪声感知的布局感知语言模型训练

    Noise-Aware Training of Layout-Aware Language Models

    [https://arxiv.org/abs/2404.00488](https://arxiv.org/abs/2404.00488)

    本论文提出了一种噪声感知训练方法，可以以可扩展的方式使用弱标记文档训练提取器，从而避免了在企业场景中训练大量不同文档类型的自定义提取器所需的昂贵人工标记成本。

    

    一篇视觉丰富的文档（VRD）利用视觉特征和语言线索传播信息。训练一个能够从文档中识别命名实体的自定义提取器需要大量标记为文本和视觉模态的目标文档实例。在企业场景中，我们希望以可扩展的方式为成千上万种不同文档类型训练自定义提取器，这是一个昂贵的瓶颈。在未标记目标文档实例上预训练提取器模型，然后在人工标记实例上进行微调，在这些情况下是行不通的，因为它超出了为提取器分配的最大允许训练时间。本文提出了一种噪声感知训练方法（NAT）来解决这个场景。NAT利用弱标记文档以可扩展的方式训练提取器，而不是获取昂贵的人工标记文档。

    arXiv:2404.00488v1 Announce Type: cross  Abstract: A visually rich document (VRD) utilizes visual features along with linguistic cues to disseminate information. Training a custom extractor that identifies named entities from a document requires a large number of instances of the target document type annotated at textual and visual modalities. This is an expensive bottleneck in enterprise scenarios, where we want to train custom extractors for thousands of different document types in a scalable way. Pre-training an extractor model on unlabeled instances of the target document type, followed by a fine-tuning step on human-labeled instances does not work in these scenarios, as it surpasses the maximum allowable training time allocated for the extractor. We address this scenario by proposing a Noise-Aware Training method or NAT in this paper. Instead of acquiring expensive human-labeled documents, NAT utilizes weakly labeled documents to train an extractor in a scalable way. To avoid degr
    
[^89]: 上下文智能日志：将LLM和时间序列行为感知技术整合，使用MindScape应用促进自我反思和幸福感

    Contextual AI Journaling: Integrating LLM and Time Series Behavioral Sensing Technology to Promote Self-Reflection and Well-being using the MindScape App

    [https://arxiv.org/abs/2404.00487](https://arxiv.org/abs/2404.00487)

    将时间序列行为与大型语言模型相结合，创建新形式上下文智能日志应用，促进自我反思和幸福感，代表了人工智能新的发展方向。

    

    MindScape旨在研究将时间序列行为模式（如对话参与度、睡眠、位置）与大型语言模型（LLMs）相结合，创建一种新形式的上下文智能日志，促进自我反思和幸福感。我们认为在LLMs中整合行为感知很可能会引领人工智能的新领域。在这篇晚报告工作论文中，我们讨论了MindScape上下文日志应用的设计，该应用利用LLMs和行为感知生成上下文化和个性化的日志提示，旨在鼓励自我反思和情感发展。我们还讨论了基于初步用户研究的MindScape对大学生的研究，以及我们即将展开的研究，评估上下文智能日志在促进大学校园中更好幸福感方面的有效性。MindScape代表了一种新的应用类别，将行为智能嵌入到人工智能中。

    arXiv:2404.00487v1 Announce Type: cross  Abstract: MindScape aims to study the benefits of integrating time series behavioral patterns (e.g., conversational engagement, sleep, location) with Large Language Models (LLMs) to create a new form of contextual AI journaling, promoting self-reflection and well-being. We argue that integrating behavioral sensing in LLMs will likely lead to a new frontier in AI. In this Late-Breaking Work paper, we discuss the MindScape contextual journal App design that uses LLMs and behavioral sensing to generate contextual and personalized journaling prompts crafted to encourage self-reflection and emotional development. We also discuss the MindScape study of college students based on a preliminary user study and our upcoming study to assess the effectiveness of contextual AI journaling in promoting better well-being on college campuses. MindScape represents a new application class that embeds behavioral intelligence in AI.
    
[^90]: 辩证对齐：解决3H紧张与LLM安全威胁

    Dialectical Alignment: Resolving the Tension of 3H and Security Threats of LLMs

    [https://arxiv.org/abs/2404.00486](https://arxiv.org/abs/2404.00486)

    提出了辩证对齐（DA）框架来解决LLM在面临外部有毒数据时的适应性变色龙问题，增强了其对抗外部攻击的安全性

    

    随着大型语言模型（LLM）的兴起，确保它们体现有益、诚实和无害（3H）原则，即人类对齐，变得至关重要。现有的对齐方法如RLHF、DPO等有效地微调LLM以匹配偏好数据集中的偏好，但往往会使LLM对高度接受人类输入和外部证据，即使这些信息是有毒的。这导致LLM倾向于成为适应变色龙，当外部证据与其参数性记忆冲突时。这加剧了LLM遭受外部有毒数据攻击的风险，对LLM系统应用（如检索增强生成）构成了重大安全风险。为了解决这一挑战，我们提出了一个新颖的框架：辩证对齐（DA），它（1）利用人工智能反馈来确定LLM导航相互文本冲突和上下文记忆冲突的最佳策略

    arXiv:2404.00486v1 Announce Type: cross  Abstract: With the rise of large language models (LLMs), ensuring they embody the principles of being helpful, honest, and harmless (3H), known as Human Alignment, becomes crucial. While existing alignment methods like RLHF, DPO, etc., effectively fine-tune LLMs to match preferences in the preference dataset, they often lead LLMs to highly receptive human input and external evidence, even when this information is poisoned. This leads to a tendency for LLMs to be Adaptive Chameleons when external evidence conflicts with their parametric memory. This exacerbates the risk of LLM being attacked by external poisoned data, which poses a significant security risk to LLM system applications such as Retrieval-augmented generation (RAG). To address the challenge, we propose a novel framework: Dialectical Alignment (DA), which (1) utilizes AI feedback to identify optimal strategies for LLMs to navigate inter-context conflicts and context-memory conflicts w
    
[^91]: 用于斯拉夫语的跨语言命名实体语料库

    Cross-lingual Named Entity Corpus for Slavic Languages

    [https://arxiv.org/abs/2404.00482](https://arxiv.org/abs/2404.00482)

    介绍了一个手动标注的用于六种斯拉夫语言的命名实体语料库，提供了两个训练数据集划分，并使用预训练的多语言模型进行命名实体的识别、分类、引用词化和链接。

    

    本文介绍了一个手动注释的包含六种斯拉夫语言（保加利亚语、捷克语、波兰语、斯洛文尼亚语、俄语和乌克兰语）命名实体的语料库。这项工作是2017-2023年间斯拉夫自然语言处理研讨会的一系列共享任务的结果。该语料库包含了5017份涵盖七个主题的文档，文档标有五类命名实体，每个实体由类别、引用词和唯一跨语言标识符描述。我们提供了两个训练调整的数据集划分 - 单个主题划分和跨主题划分。对于每个划分，我们使用基于transformer的神经网络架构设置了基准，使用预训练的多语言模型XLM-RoBERTa-large进行命名实体提及识别和分类，以及mT5-large进行命名实体引用词化和链接。

    arXiv:2404.00482v1 Announce Type: cross  Abstract: This paper presents a corpus manually annotated with named entities for six Slavic languages - Bulgarian, Czech, Polish, Slovenian, Russian, and Ukrainian. This work is the result of a series of shared tasks, conducted in 2017-2023 as a part of the Workshops on Slavic Natural Language Processing. The corpus consists of 5 017 documents on seven topics. The documents are annotated with five classes of named entities. Each entity is described by a category, a lemma, and a unique cross-lingual identifier. We provide two train-tune dataset splits - single topic out and cross topics. For each split, we set benchmarks using a transformer-based neural network architecture with the pre-trained multilingual models - XLM-RoBERTa-large for named entity mention recognition and categorization, and mT5-large for named entity lemmatization and linking.
    
[^92]: 语言模型的语言校准

    Linguistic Calibration of Language Models

    [https://arxiv.org/abs/2404.00474](https://arxiv.org/abs/2404.00474)

    该论文提出了一种通过语言模型的文本生成来实现语言校准，可以使用户做出校准概率预测的方法。

    

    语言模型可能会在自信幻觉时导致用户做出次优化的下游决策。通过语言模型口头传达其主张正确概率可以缓解这个问题，但现有模型无法生成具有校准置信度声明的文本。我们通过决策角度，为长篇生成形式的语言校准形式化定义：如果语言模型的生成使其用户能够做出校准概率预测，则该模型是语言上校准的。这个定义使得一个训练框架成为可能，其中一个监督微调步骤引导一个语言模型发出带有置信度声明的长篇生成，诸如“我估计有30%的机会…”或“我确信…”，然后是一个强化学习步骤，奖励使用户能够对相关问题提供校准答案的生成。我们对Llama 2 7B 进行语言校准，并发现在自动化和人类测试中...

    arXiv:2404.00474v1 Announce Type: cross  Abstract: Language models (LMs) may lead their users to make suboptimal downstream decisions when they confidently hallucinate. This issue can be mitigated by having the LM verbally convey the probability that its claims are correct, but existing models cannot produce text with calibrated confidence statements. Through the lens of decision-making, we formalize linguistic calibration for long-form generations: an LM is linguistically calibrated if its generations enable its users to make calibrated probabilistic predictions. This definition enables a training framework where a supervised finetuning step bootstraps an LM to emit long-form generations with confidence statements such as "I estimate a 30% chance of..." or "I am certain that...", followed by a reinforcement learning step which rewards generations that enable a user to provide calibrated answers to related questions. We linguistically calibrate Llama 2 7B and find in automated and huma
    
[^93]: 从对比中出现的快速方法：基于提示的学习中的有效和隐蔽干净标签攻击

    Shortcuts Arising from Contrast: Effective and Covert Clean-Label Attacks in Prompt-Based Learning

    [https://arxiv.org/abs/2404.00461](https://arxiv.org/abs/2404.00461)

    基于提示的学习中的干净标签攻击通过特定提示作为触发器，实现成功的同时确保正确标记有毒样本，但仍面临误激活问题和挑战，需要更高比例的毒害。

    

    Prompt-based learning范式表现出卓越的效力，可以提升预训练语言模型（PLMs）在少样本情况下的适应能力。然而，这种学习范式已被证明容易受到后门攻击的影响。当前的干净标签攻击，利用特定提示作为触发器，可以在不需要外部触发器的情况下成功，并确保对有毒样本的正确标记，相比有毒标签攻击更具隐蔽性，但另一方面，它面临着严重的误激活问题，并提出了更大的挑战，需要更高比例的毒害。通过传统的负数据增强方法，我们发现在干净标签设置中在效力和隐蔽性之间取得平衡是具有挑战性的。在解决这一问题时，我们受到后门充当快捷方式的观念的启发，并假设这一快捷方式源于t。

    arXiv:2404.00461v1 Announce Type: cross  Abstract: Prompt-based learning paradigm has demonstrated remarkable efficacy in enhancing the adaptability of pretrained language models (PLMs), particularly in few-shot scenarios. However, this learning paradigm has been shown to be vulnerable to backdoor attacks. The current clean-label attack, employing a specific prompt as a trigger, can achieve success without the need for external triggers and ensure correct labeling of poisoned samples, which is more stealthy compared to the poisoned-label attack, but on the other hand, it faces significant issues with false activations and poses greater challenges, necessitating a higher rate of poisoning. Using conventional negative data augmentation methods, we discovered that it is challenging to trade off between effectiveness and stealthiness in a clean-label setting. In addressing this issue, we are inspired by the notion that a backdoor acts as a shortcut and posit that this shortcut stems from t
    
[^94]: 具有手势响应和音乐伴奏的互动多机器人集群

    Interactive Multi-Robot Flocking with Gesture Responsiveness and Musical Accompaniment

    [https://arxiv.org/abs/2404.00442](https://arxiv.org/abs/2404.00442)

    提出了一个互动多机器人集群任务，目的是通过手势和声音引导人类参与机器人群体，并创造了新颖的群体导航和手势响应算法

    

    数十年来，机器人研究人员一直致力于多机器人系统的各种任务，从合作操纵到搜索和救援。这些任务是传统机器人任务的多机器人扩展，通常在速度或效率等维度上进行了优化。随着机器人从商业和研究领域过渡到日常环境，社交任务目标，如参与或娱乐，变得越来越重要。本研究提出了一个引人入胜且有趣的多机器人任务，其中主要目标是使人着迷并产生兴趣。在这个任务中，目标是让人类被吸引并与动态、富有表现力的机器人群体一起移动并参与其中。为实现这一目标，研究团队创建了涉及机器人运动和引人注意的交互模式，如手势和声音的算法。贡献如下：（1）一种涉及人类和机器人代理的新颖群体导航算法，（2）用于实时的手势响应算法

    arXiv:2404.00442v1 Announce Type: cross  Abstract: For decades, robotics researchers have pursued various tasks for multi-robot systems, from cooperative manipulation to search and rescue. These tasks are multi-robot extensions of classical robotic tasks and often optimized on dimensions such as speed or efficiency. As robots transition from commercial and research settings into everyday environments, social task aims such as engagement or entertainment become increasingly relevant. This work presents a compelling multi-robot task, in which the main aim is to enthrall and interest. In this task, the goal is for a human to be drawn to move alongside and participate in a dynamic, expressive robot flock. Towards this aim, the research team created algorithms for robot movements and engaging interaction modes such as gestures and sound. The contributions are as follows: (1) a novel group navigation algorithm involving human and robot agents, (2) a gesture responsive algorithm for real-time
    
[^95]: 使用分布式狮子进行高效通信的分布式训练

    Communication Efficient Distributed Training with Distributed Lion

    [https://arxiv.org/abs/2404.00438](https://arxiv.org/abs/2404.00438)

    分布式狮子是对 Lion 进行了创新性改进，利用符号操作符降低了通信成本，在分布式训练中取得了与标准 Lion 或 AdamW 优化器相当的性能，并显著减少了通信带宽。

    

    Lion优化器在训练大型AI模型方面与AdamW有一定竞争力，具有在内存、计算和样本效率上的优势。本文介绍了分布式狮子，这是狮子在分布式训练环境中的创新性改进。利用狮子中的符号操作符，我们的分布式狮子只需要在工作节点和中心服务器之间传递二进制或低精度向量，显著降低了通信成本。我们的理论分析证实了分布式狮子的收敛性质。实证结果表明，它在多种任务、工作者数量和批量大小上表现稳健，在视觉和语言问题上表现出色。值得注意的是，分布式狮子在聚合梯度上达到了与标准狮子或AdamW优化器相当的性能，但通信带宽显著减少。这个特性对于训练大型模型尤为有利。

    arXiv:2404.00438v1 Announce Type: cross  Abstract: The Lion optimizer has been a promising competitor with the AdamW for training large AI models, with advantages on memory, computation, and sample efficiency. In this paper, we introduce Distributed Lion, an innovative adaptation of Lion for distributed training environments. Leveraging the sign operator in Lion, our Distributed Lion only requires communicating binary or lower-precision vectors between workers to the center server, significantly reducing the communication cost. Our theoretical analysis confirms Distributed Lion's convergence properties. Empirical results demonstrate its robustness across a range of tasks, worker counts, and batch sizes, on both vision and language problems. Notably, Distributed Lion attains comparable performance to standard Lion or AdamW optimizers applied on aggregated gradients, but with significantly reduced communication bandwidth. This feature is particularly advantageous for training large model
    
[^96]: 利用树估计器自动解释西班牙法律判决在依赖司法管辖的法律类别中的分类

    Automatic explanation of the classification of Spanish legal judgments in jurisdiction-dependent law categories with tree estimators

    [https://arxiv.org/abs/2404.00437](https://arxiv.org/abs/2404.00437)

    本研究结合自然语言处理和机器学习，提出了一种可解释的法律文本分类系统，使模型的决策变得可理解给最终用户

    

    自动法律文本分类系统已经被提出在文献中，以解决知识从判决中提取并检测其方面。然而，即使它们的模型是可解释的，大多数这些系统都是黑盒的。这可能引发对它们可信度的担忧。因此，本研究提出了一个系统，结合自然语言处理（NLP）和机器学习（ML），以可解释的方式对法律文本进行分类。我们分析了决策中涉及的特征和树结构的决策路径的阈值分叉值，并以自然语言的方式向用户呈现这些信息。这是第一项关于自动分析法律文本的工作，结合NLP和ML以及可解释的人工智能技术，以使模型的决策自动变得可理解给最终用户。此外，法律专家已经验证了我们的解决方案，这些知识也已

    arXiv:2404.00437v1 Announce Type: cross  Abstract: Automatic legal text classification systems have been proposed in the literature to address knowledge extraction from judgments and detect their aspects. However, most of these systems are black boxes even when their models are interpretable. This may raise concerns about their trustworthiness. Accordingly, this work contributes with a system combining Natural Language Processing (NLP) with Machine Learning (ML) to classify legal texts in an explainable manner. We analyze the features involved in the decision and the threshold bifurcation values of the decision paths of tree structures and present this information to the users in natural language. This is the first work on automatic analysis of legal texts combining NLP and ML along with Explainable Artificial Intelligence techniques to automatically make the models' decisions understandable to end users. Furthermore, legal experts have validated our solution, and this knowledge has al
    
[^97]: 从注意力到利润：基于Transformer的量化交易策略

    From attention to profit: quantitative trading strategy based on transformer

    [https://arxiv.org/abs/2404.00424](https://arxiv.org/abs/2404.00424)

    该研究介绍了一种基于Transformer的量化交易策略，利用改进的模型架构和情感分析的迁移学习，不仅在捕捉长期依赖关系和建模数据关系方面具有优势，而且能够准确预测未来一段时间内的回报。

    

    传统量化交易实践中，应对复杂动态的金融市场一直是个持久挑战。先前的机器学习方法往往难以充分捕捉各种市场变量，经常忽视长期信息并且无法捕捉可能带来利润的基本信号。本文引入了改进的Transformer架构，并设计了一个基于该模型的新型因子。通过从情感分析进行迁移学习，所提出的模型不仅发挥了其原有的长距离依赖捕捉和建模复杂数据关系的优势，而且能够处理具有数值输入的任务，并准确预测未来一段时间内的回报。该研究收集了2010年至2019年中国资本市场4,601只股票的5,000,000多条滚动数据。研究结果证明了该模型在预测股票表现方面的卓越性能。

    arXiv:2404.00424v1 Announce Type: cross  Abstract: In traditional quantitative trading practice, navigating the complicated and dynamic financial market presents a persistent challenge. Former machine learning approaches have struggled to fully capture various market variables, often ignore long-term information and fail to catch up with essential signals that may lead the profit. This paper introduces an enhanced transformer architecture and designs a novel factor based on the model. By transfer learning from sentiment analysis, the proposed model not only exploits its original inherent advantages in capturing long-range dependencies and modelling complex data relationships but is also able to solve tasks with numerical inputs and accurately forecast future returns over a period. This work collects more than 5,000,000 rolling data of 4,601 stocks in the Chinese capital market from 2010 to 2019. The results of this study demonstrated the model's superior performance in predicting stock
    
[^98]: 在线持续学习中推进潜在专业知识的编排：多层监督和反向自蒸馏

    Orchestrate Latent Expertise: Advancing Online Continual Learning with Multi-Level Supervision and Reverse Self-Distillation

    [https://arxiv.org/abs/2404.00417](https://arxiv.org/abs/2404.00417)

    引入了Multi-level Online Sequential Experts (MOSE)方法，通过多层监督和反向自蒸馏，解决了在线持续学习中新旧训练样本学习不足和重复学习的问题。

    

    为了适应现实世界的动态变化，人工智能系统需要以在线方式处理连续到达的内容。在正常持续学习（CL）试图通过离线训练每个任务来解决灾难性遗忘的情况之外，在一次数据流中执行CL的在线持续学习（OCL）是一种更具挑战性但更现实的设置。当前的OCL方法主要依赖于旧训练样本的内存重放。然而，从CL到OCL的一个显着差距源于与重演缓冲区的使用相关的过度拟合-欠拟合困境：对新训练样本的学习不足（欠拟合）以及对少量旧训练样本的重复学习（过拟合）。为此，我们引入了一种新颖的方法，即多级在线顺序专家（MOSE），它将模型作为堆叠的子专家进行培育，并整合了多级监督和反向自蒸馏。

    arXiv:2404.00417v1 Announce Type: cross  Abstract: To accommodate real-world dynamics, artificial intelligence systems need to cope with sequentially arriving content in an online manner. Beyond regular Continual Learning (CL) attempting to address catastrophic forgetting with offline training of each task, Online Continual Learning (OCL) is a more challenging yet realistic setting that performs CL in a one-pass data stream. Current OCL methods primarily rely on memory replay of old training samples. However, a notable gap from CL to OCL stems from the additional overfitting-underfitting dilemma associated with the use of rehearsal buffers: the inadequate learning of new training samples (underfitting) and the repeated learning of a few old training samples (overfitting). To this end, we introduce a novel approach, Multi-level Online Sequential Experts (MOSE), which cultivates the model as stacked sub-experts, integrating multi-level supervision and reverse self-distillation. Supervisi
    
[^99]: 语言模型是航天器操作员

    Language Models are Spacecraft Operators

    [https://arxiv.org/abs/2404.00413](https://arxiv.org/abs/2404.00413)

    该论文旨在将大型语言模型(LLMs)应用于空间导航和控制领域，通过开发纯LLM解决方案，并在 Kerbal 太空计划差分游戏挑战中取得第二名，首次将LLM代理集成到空间资源中。

    

    最近出现了一个趋势，即广泛采用大型语言模型(LLMs)作为根据用户文本提示内容采取行动的自主代理。我们打算将这些概念应用于空间导航和控制领域，使LLMs在自主卫星操作的决策过程中扮演重要角色。作为实现这一目标的第一步，我们为 Kerbal 太空计划差分游戏(KSPDG)挑战开发了一种纯LLM方案，这是一个公开的软件设计竞赛，参与者为卫星操纵创建自主代理，在KSP游戏引擎上运行。我们的方法利用提示工程、少样本提示和微调技术创建了一个效果良好的LLM代理，在竞赛中排名第二。据我们所知，这项工作开创了将LLM代理整合到空间资源中的先河。

    arXiv:2404.00413v1 Announce Type: cross  Abstract: Recent trends are emerging in the use of Large Language Models (LLMs) as autonomous agents that take actions based on the content of the user text prompts. We intend to apply these concepts to the field of Guidance, Navigation, and Control in space, enabling LLMs to have a significant role in the decision-making process for autonomous satellite operations. As a first step towards this goal, we have developed a pure LLM-based solution for the Kerbal Space Program Differential Games (KSPDG) challenge, a public software design competition where participants create autonomous agents for maneuvering satellites involved in non-cooperative space operations, running on the KSP game engine. Our approach leverages prompt engineering, few-shot prompting, and fine-tuning techniques to create an effective LLM-based agent that ranked 2nd in the competition. To the best of our knowledge, this work pioneers the integration of LLM agents into space res
    
[^100]: TACO -- Twitter Arguments from COnversations

    TACO -- Twitter Arguments from COnversations

    [https://arxiv.org/abs/2404.00406](https://arxiv.org/abs/2404.00406)

    TACO是第一个利用1,814条推文构建的Twitter Arguments数据集，涵盖200场完整对话，六个主题，具有0.718的Krippendorff's alpha一致性，并提供了一个注释框架来定义和识别论点结构要素。

    

    Twitter已经成为全球参与在线对话的中心，也成为各个学科研究语料库的重要来源，这些学科已经意识到其用户生成的内容的重要性。论点挖掘是处理和理解在线话语的重要分析任务。具体来说，它旨在识别论点的结构要素，表示为信息和推理。然而，这些要素并不是静态的，可能需要在所在对话中设置上下文，然而缺乏解决Twitter上这一动态方面的数据和注释框架。我们贡献了TACO，这是第一个利用1,814条涵盖200场完整对话、涵盖六个异质主题的推文的Twitter Arguments数据集，并在六名专家之间以0.718的Krippendorff's alpha达成一致。其次，我们提供了我们的注释框架，结合了来自剑桥词典的定义，以定义和识别。

    arXiv:2404.00406v1 Announce Type: cross  Abstract: Twitter has emerged as a global hub for engaging in online conversations and as a research corpus for various disciplines that have recognized the significance of its user-generated content. Argument mining is an important analytical task for processing and understanding online discourse. Specifically, it aims to identify the structural elements of arguments, denoted as information and inference. These elements, however, are not static and may require context within the conversation they are in, yet there is a lack of data and annotation frameworks addressing this dynamic aspect on Twitter. We contribute TACO, the first dataset of Twitter Arguments utilizing 1,814 tweets covering 200 entire conversations spanning six heterogeneous topics annotated with an agreement of 0.718 Krippendorff's alpha among six experts. Second, we provide our annotation framework, incorporating definitions from the Cambridge Dictionary, to define and identify
    
[^101]: Aurora-M: 根据美国行政命令，第一个开源的多语言语言模型进行了红队测试

    Aurora-M: The First Open Source Multilingual Language Model Red-teamed according to the U.S. Executive Order

    [https://arxiv.org/abs/2404.00399](https://arxiv.org/abs/2404.00399)

    Aurora-M 是第一个根据美国行政命令进行红队测试的开源多语言模型，通过在英语、芬兰语、印地语、日语、越南语和代码上训练，不断预训练，包括了人工审核的安全说明，总训练 token 数超过 2 万亿个

    

    预训练语言模型支持多种人工智能应用，但是它们在训练时高昂的计算成本限制了可访问性。BLOOM 和 StarCoder 等倡议旨在使预训练模型对于协作社区开发更具民主性。然而，目前存在的模型面临一些挑战：多语言能力有限，持续的预训练会导致灾难性遗忘，而从头开始预训练又具有高昂的计算成本，并且需要遵守人工智能安全和发展法律。本文介绍了 Aurora-M，一个包含 15B 参数的多语言开源模型，训练语言包括英语、芬兰语、印地语、日语、越南语和代码。Aurora-M 不断从 StarCoderPlus 上预训练，额外训练了 4350 亿个 token，总训练 token 数超过了 2 万亿个。它是第一个在人工审核的安全说明上进行微调的开源多语言模型，使其开发与传统

    arXiv:2404.00399v1 Announce Type: cross  Abstract: Pretrained language models underpin several AI applications, but their high computational cost for training limits accessibility. Initiatives such as BLOOM and StarCoder aim to democratize access to pretrained models for collaborative community development. However, such existing models face challenges: limited multilingual capabilities, continual pretraining causing catastrophic forgetting, whereas pretraining from scratch is computationally expensive, and compliance with AI safety and development laws. This paper presents Aurora-M, a 15B parameter multilingual open-source model trained on English, Finnish, Hindi, Japanese, Vietnamese, and code. Continually pretrained from StarCoderPlus on 435 billion additional tokens, Aurora-M surpasses 2 trillion tokens in total training token count. It is the first open-source multilingual model fine-tuned on human-reviewed safety instructions, thus aligning its development not only with conventio
    
[^102]: 基于因子图的受约束布局生成

    Constrained Layout Generation with Factor Graphs

    [https://arxiv.org/abs/2404.00385](https://arxiv.org/abs/2404.00385)

    本文提出了一种基于因子图的方法，用于面向对象的受约束布局生成，可以准确捕捉复杂交互关系，填补了现有方法的不足。

    

    本文解决了在包括平面设计过程在内的多个领域中出现的面向对象的受约束布局生成挑战。现有的作品通常将对象表示为单个节点，缺乏细粒度来准确建模对象之间的复杂交互。为了填补这一空白，我们引入了一种基于因子图的方法，对每个房间引入四个潜变量节点和每个约束引入一个因子节点。因子节点表示与其连接的变量之间的依赖关系，有效捕捉可能是高阶的约束。

    arXiv:2404.00385v1 Announce Type: cross  Abstract: This paper addresses the challenge of object-centric layout generation under spatial constraints, seen in multiple domains including floorplan design process. The design process typically involves specifying a set of spatial constraints that include object attributes like size and inter-object relations such as relative positioning. Existing works, which typically represent objects as single nodes, lack the granularity to accurately model complex interactions between objects. For instance, often only certain parts of an object, like a room's right wall, interact with adjacent objects. To address this gap, we introduce a factor graph based approach with four latent variable nodes for each room, and a factor node for each constraint. The factor nodes represent dependencies among the variables to which they are connected, effectively capturing constraints that are potentially of a higher order. We then develop message-passing on the bipar
    
[^103]: SpikingJET：增强完全和卷积脉冲神经网络的故障注入

    SpikingJET: Enhancing Fault Injection for Fully and Convolutional Spiking Neural Networks

    [https://arxiv.org/abs/2404.00383](https://arxiv.org/abs/2404.00383)

    SpikingJET是一种设计用于全连接和卷积脉冲神经网络的新型故障注入器，通过在关键组件中引入错误和注入故障，为评估SNNs的韧性提供了全面平台。

    

    随着人工神经网络越来越多地集成到自动驾驶车辆、医学诊断设备和工业自动化等安全关键系统中，确保它们在面对随机硬件故障时的可靠性变得至关重要。本文介绍了SpikingJET，一种专为全连接和卷积脉冲神经网络（SNNs）设计的新型故障注入器。我们的工作强调了评估SNNs对硬件故障的韧性的重要性，考虑到它们在现实应用中的日益突出。SpikingJET通过向关键组件（如突触权重、神经元模型参数、内部状态和激活函数）引入错误和注入故障，为评估SNNs的韧性提供了全面平台。本文通过在各种SNN架构上进行广泛的软件级实验展示了Spiking-JET的有效性。

    arXiv:2404.00383v1 Announce Type: cross  Abstract: As artificial neural networks become increasingly integrated into safety-critical systems such as autonomous vehicles, devices for medical diagnosis, and industrial automation, ensuring their reliability in the face of random hardware faults becomes paramount. This paper introduces SpikingJET, a novel fault injector designed specifically for fully connected and convolutional Spiking Neural Networks (SNNs). Our work underscores the critical need to evaluate the resilience of SNNs to hardware faults, considering their growing prominence in real-world applications. SpikingJET provides a comprehensive platform for assessing the resilience of SNNs by inducing errors and injecting faults into critical components such as synaptic weights, neuron model parameters, internal states, and activation functions. This paper demonstrates the effectiveness of Spiking-JET through extensive software-level experiments on various SNN architectures, reveali
    
[^104]: 通过Holonic控制架构实现工人机器人之间的协作与集成

    Worker Robot Cooperation and Integration into the Manufacturing Workcell via the Holonic Control Architecture

    [https://arxiv.org/abs/2404.00369](https://arxiv.org/abs/2404.00369)

    本文提出通过Holonic控制架构实现工人机器人之间的协作与集成，以实现新的智能制造技术。

    

    工人机器人合作是一种新的工业趋势，旨在综合人类和工业机器人的优势，以提供新的智能制造技术。工人和机器人之间的合作制造包含产品零部件和制造工具等其他元素。所有这些生产元素必须在一个制造工作单元中合作，以满足生产要求。制造控制系统是连接所有这些合作元素的手段。由于合作工作单元的性质，这种制造控制系统是分布式和自主的。因此，本文提出了Holonic控制架构作为合作工作单元的制造概念。此外，本文关注该制造概念的可行性，通过将其应用于涉及双臂协作的案例研究中。

    arXiv:2404.00369v1 Announce Type: cross  Abstract: Worker-Robot Cooperation is a new industrial trend, which aims to sum the advantages of both the human and the industrial robot to afford a new intelligent manufacturing techniques. The cooperative manufacturing between the worker and the robot contains other elements such as the product parts and the manufacturing tools. All these production elements must cooperate in one manufacturing workcell to fulfill the production requirements. The manufacturing control system is the mean to connect all these cooperative elements together in one body. This manufacturing control system is distributed and autonomous due to the nature of the cooperative workcell. Accordingly, this article proposes the holonic control architecture as the manufacturing concept of the cooperative workcell. Furthermore, the article focuses on the feasibility of this manufacturing concept, by applying it over a case study that involves the cooperation between a dual-arm
    
[^105]: 通过几何感知学习实现机器人荔枝采摘准确的切割点估计

    Accurate Cutting-point Estimation for Robotic Lychee Harvesting through Geometry-aware Learning

    [https://arxiv.org/abs/2404.00364](https://arxiv.org/abs/2404.00364)

    提出了一个 Fcaf3d-lychee 网络模型，通过与挤压-激发（SE）模块改进特征提取，实现了对荔枝采摘点的准确定位

    

    准确识别结构不规则的果园环境中的荔枝采摘点，并获取它们的坐标位置对于荔枝采摘机器人的成功至关重要。然而，传统的基于二维（2D）图像的目标检测方法通常因为树枝、叶子和果实的复杂几何结构而难以应对，导致对荔枝采摘点的错误确定。本研究提出了一个专门用于准确定位荔枝采摘点的 Fcaf3d-lychee 网络模型。我们使用微软的 Azure Kinect DK 飞行时间（TOF）相机通过多视图拼接获取荔枝采摘点在自然环境中的点云数据。我们将完全卷积无锚点 3D 目标检测（Fcaf3d）模型与一个挤压-激发（SE）模块相结合，利用人类视觉注意机制来改进荔枝采摘点的特征提取。

    arXiv:2404.00364v1 Announce Type: cross  Abstract: Accurately identifying lychee-picking points in unstructured orchard environments and obtaining their coordinate locations is critical to the success of lychee-picking robots. However, traditional two-dimensional (2D) image-based object detection methods often struggle due to the complex geometric structures of branches, leaves and fruits, leading to incorrect determination of lychee picking points. In this study, we propose a Fcaf3d-lychee network model specifically designed for the accurate localisation of lychee picking points. Point cloud data of lychee picking points in natural environments are acquired using Microsoft's Azure Kinect DK time-of-flight (TOF) camera through multi-view stitching. We augment the Fully Convolutional Anchor-Free 3D Object Detection (Fcaf3d) model with a squeeze-and-excitation(SE) module, which exploits human visual attention mechanisms for improved feature extraction of lychee picking points. The traine
    
[^106]: LLMs能够掌握数学吗？在数学堆栈交换上研究大型语言模型

    Can LLMs Master Math? Investigating Large Language Models on Math Stack Exchange

    [https://arxiv.org/abs/2404.00344](https://arxiv.org/abs/2404.00344)

    LLMs在数学领域表现出色，其中GPT-4在Math Stack Exchange上回答数学问题的表现最佳。

    

    大型语言模型（LLMs）在各种自然语言任务中展示出异常能力，通常表现出超越人类的性能。尽管取得了这些进展，数学领域提出了一种独特的挑战，主要是因为其专门的结构和所需的精度。本研究采用了两步方法来调查LLMs在回答数学问题方面的熟练程度。首先，我们采用在数学问题-答案基准测试中表现最佳的LLMs来回答Math Stack Exchange（MSE）中的78个问题。其次，对表现最佳的LLM进行案例分析，重点关注其答案的质量和准确性。我们发现，在为回答数学问题进行微调的现有LLMs中，GPT-4表现最佳（nDCG为0.48，P@10为0.37），在数学问题上表现优异。

    arXiv:2404.00344v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have demonstrated exceptional capabilities in various natural language tasks, often achieving performances that surpass those of humans. Despite these advancements, the domain of mathematics presents a distinctive challenge, primarily due to its specialized structure and the precision it demands. In this study, we adopted a two-step approach for investigating the proficiency of LLMs in answering mathematical questions. First, we employ the most effective LLMs, as identified by their performance on math question-answer benchmarks, to generate answers to 78 questions from the Math Stack Exchange (MSE). Second, a case analysis is conducted on the LLM that showed the highest performance, focusing on the quality and accuracy of its answers through manual evaluation. We found that GPT-4 performs best (nDCG of 0.48 and P@10 of 0.37) amongst existing LLMs fine-tuned for answering mathematics questions and outperfor
    
[^107]: 共同制造中的本体论：分享和交换知识的解决方案

    Ontology in Holonic Cooperative Manufacturing: A Solution to Share and Exchange the Knowledge

    [https://arxiv.org/abs/2404.00341](https://arxiv.org/abs/2404.00341)

    提出了一种使用本体论概念表示合作制造知识的整体控制解决方案，并在合作装配场景中进行了实施。

    

    合作制造是工业的一个新趋势，它依赖于协作机器人的存在。在这项研究中，我们提出了一种使用本体论概念表示合作制造知识的整体控制解决方案。这个整体控制解决方案被实施为一个自主多Agent系统，基于本体模型交换制造知识。最终，研究对一个涉及两名工人和一个协作工人的合作装配场景进行了说明和实施。

    arXiv:2404.00341v1 Announce Type: new  Abstract: Cooperative manufacturing is a new trend in industry, which depends on the existence of a collaborative robot. A collaborative robot is usually a light-weight robot which is capable of operating safely with a human co-worker in a shared work environment. During this cooperation, a vast amount of information is exchanged between the collaborative robot and the worker. This information constructs the cooperative manufacturing knowledge, which describes the production components and environment. In this research, we propose a holonic control solution, which uses the ontology concept to represent the cooperative manufacturing knowledge. The holonic control solution is implemented as an autonomous multi-agent system that exchanges the manufacturing knowledge based on an ontology model. Ultimately, the research illustrates and implements the proposed solution over a cooperative assembly scenario, which involves two workers and one collaborativ
    
[^108]: 存储可扩展且简化的功能地图学习

    Memory-Scalable and Simplified Functional Map Learning

    [https://arxiv.org/abs/2404.00330](https://arxiv.org/abs/2404.00330)

    提出了一种新颖的存储可扩展且高效的功能地图学习管道，该方法可以在不将逐点地图存储在内存中的情况下实现相同结果，并且引入了一种不同iable的地图细化层。

    

    近年来，深度功能地图作为非刚性形状匹配问题的杰出基于学习的框架已经出现。早期在该领域的方法仅关注于在功能领域中的学习，而最新的技术已经证明，通过促进功能地图和逐点地图之间的一致性，可以显著提高准确性。不幸的是，现有方法过分依赖于软逐点地图产生的大型密集矩阵的计算，这损害了它们的效率和可扩展性。为了解决这一限制，我们引入了一种新颖的存储可扩展且高效的功能地图学习管道。通过利用功能地图的具体结构，我们提供了在不将逐点地图存储在内存中的情况下实现相同结果的可能性。此外，基于相同的方法，我们展示了一个可微分的地图细化层，改编自现有的公理性ref

    arXiv:2404.00330v1 Announce Type: cross  Abstract: Deep functional maps have emerged in recent years as a prominent learning-based framework for non-rigid shape matching problems. While early methods in this domain only focused on learning in the functional domain, the latest techniques have demonstrated that by promoting consistency between functional and pointwise maps leads to significant improvements in accuracy. Unfortunately, existing approaches rely heavily on the computation of large dense matrices arising from soft pointwise maps, which compromises their efficiency and scalability. To address this limitation, we introduce a novel memory-scalable and efficient functional map learning pipeline. By leveraging the specific structure of functional maps, we offer the possibility to achieve identical results without ever storing the pointwise map in memory. Furthermore, based on the same approach, we present a differentiable map refinement layer adapted from an existing axiomatic ref
    
[^109]: 在疼痛识别中推进多模态数据融合：利用统计相关性和以人为中心的策略

    Advancing Multimodal Data Fusion in Pain Recognition: A Strategy Leveraging Statistical Correlation and Human-Centered Perspectives

    [https://arxiv.org/abs/2404.00320](https://arxiv.org/abs/2404.00320)

    通过结合统计相关性和以人为中心的方法，本研究提出了一种新的方法，改善了多模态数据融合在疼痛识别中的性能，突出了数据多样性和定制化的模态分割对疼痛行为分析的重要性。

    

    这项研究致力于解决在疼痛识别领域内整合异构数据以进行特定行为识别的挑战，提出了一种将统计相关性与以人为中心方法相结合的新方法。通过利用各种各样的深度学习架构，我们突出了我们的方法在提升模型在各种复杂场景中表现的适应性和有效性。我们方法的新颖之处在于战略性地结合了统计相关性权重，并从以人为中心的角度对模态进行了分割，提高了模型的精度，并提供了对多模态数据可解释性的分析。该研究通过强调数据多样性和定制化的模态分割在增强疼痛行为分析中的作用，超越了传统的模态融合技术。引入了一个框架，将每种模态与适合的分类器进行匹配，基于统计相关性，增加了对多模态数据的解释性分析。

    arXiv:2404.00320v1 Announce Type: new  Abstract: This research tackles the challenge of integrating heterogeneous data for specific behavior recognition within the domain of Pain Recognition, presenting a novel methodology that harmonizes statistical correlations with a human-centered approach. By leveraging a diverse range of deep learning architectures, we highlight the adaptability and efficacy of our approach in improving model performance across various complex scenarios. The novelty of our methodology is the strategic incorporation of statistical relevance weights and the segmentation of modalities from a human-centric perspective, enhancing model precision and providing a explainable analysis of multimodal data. This study surpasses traditional modality fusion techniques by underscoring the role of data diversity and customized modality segmentation in enhancing pain behavior analysis. Introducing a framework that matches each modality with an suited classifier, based on the sta
    
[^110]: 贝叶斯探索预训练模型用于低样本图像分类

    Bayesian Exploration of Pre-trained Models for Low-shot Image Classification

    [https://arxiv.org/abs/2404.00312](https://arxiv.org/abs/2404.00312)

    提出了基于高斯过程的概率模型集成框架，能有效整合不同于CLIP的先验知识，实现了低样本图像分类中的分析推断、不确定性量化和超参数调整。

    

    低样本图像分类是计算机视觉领域的一项基础任务，大规模视觉-语言模型（如CLIP）的出现极大推进了该领域的研究前沿。然而，大多数现有基于CLIP的方法缺乏有效整合其他预训练模型的灵活性，这些模型包含不同于CLIP的知识。为弥补这一差距，本研究提出了一种基于高斯过程的简单高效的概率模型集成框架，高斯过程在处理少量数据时已经显示出显著的效力。我们通过将均值函数指定为CLIP，将核函数指定为基于各种预训练模型构建的深层核，实现了先验知识的整合。通过直接回归分类标签，我们的框架实现了分析推断、直观不确定性量化和合理的超参数调整。

    arXiv:2404.00312v1 Announce Type: cross  Abstract: Low-shot image classification is a fundamental task in computer vision, and the emergence of large-scale vision-language models such as CLIP has greatly advanced the forefront of research in this field. However, most existing CLIP-based methods lack the flexibility to effectively incorporate other pre-trained models that encompass knowledge distinct from CLIP. To bridge the gap, this work proposes a simple and effective probabilistic model ensemble framework based on Gaussian processes, which have previously demonstrated remarkable efficacy in processing small data. We achieve the integration of prior knowledge by specifying the mean function with CLIP and the kernel function with an ensemble of deep kernels built upon various pre-trained models. By regressing the classification label directly, our framework enables analytical inference, straightforward uncertainty quantification, and principled hyper-parameter tuning. Through extensiv
    
[^111]: 利用智能推荐系统作为第一步弹性措施 —— 一种基于数据驱动的供应链紧急响应框架

    Leveraging Intelligent Recommender system as a first step resilience measure -- A data-driven supply chain disruption response framework

    [https://arxiv.org/abs/2404.00306](https://arxiv.org/abs/2404.00306)

    提出了一种基于智能推荐系统技术的数据驱动供应链紧急响应框架，能够作为供应链中断的有效措施，并帮助参与者在危机发生后获得更好的反应表现。

    

    数字技术在提高供应链弹性方面的潜在用途越来越受到关注，尤其是在工业4.0和全球大流行病背景下。尽管推荐系统 (RS) 作为一种能够提升供应链弹性的工具被忽视，但从应变的角度来看，RS 是一种有效的工具。为了解决这一问题，本研究提出了一种基于智能推荐系统技术的全新数据驱动供应链紧急响应框架，并通过一个实际案例验证了概念模型。结果表明，我们的框架可以作为一种有效的供应链紧急响应措施在第一阶段得到实施，并帮助供应链参与者在供应链中断之后获得更好的反应表现。

    arXiv:2404.00306v1 Announce Type: cross  Abstract: Interests in the value of digital technologies for its potential uses to increase supply chain resilience (SCRes) are increasing in light to the industry 4.0 and the global pandemic. Utilization of Recommender systems (RS) as a supply chain (SC) resilience measure is neglected although RS is a capable tool to enhance SC resilience from a reactive aspect. To address this problem, this research proposed a novel data-driven supply chain disruption response framework based on the intelligent recommender system techniques and validated the conceptual model through a practical use case. Results show that our framework can be implemented as an effective SC disruption mitigation measure in the very first response phrase and help SC participants get better reaction performance after the SC disruption.
    
[^112]: 通过校准预训练模型，在二值网络上进行长尾识别

    Long-Tailed Recognition on Binary Networks by Calibrating A Pre-trained Model

    [https://arxiv.org/abs/2404.00285](https://arxiv.org/abs/2404.00285)

    该论文提出了一个校准和蒸馏框架，利用预先训练的全精度模型作为教师，在学习长尾数据集上的二值网络时进行蒸馏，通过对目标函数中项的敌对平衡和多分辨率学习，显著优于以往技术

    

    在真实世界的情景中部署深度模型面临多个挑战，包括计算效率和真实世界（如长尾）数据分布。本文解决了学习长尾分布，利用高度资源有效的二值神经网络作为骨干网的组合挑战。具体来说，我们提出了一个校准和蒸馏框架，使用预先训练的平衡数据集上的全精度模型作为教师，在学习长尾数据集上的二值网络时进行蒸馏。为了更好地泛化到各种数据集，我们进一步提出了一种新颖的目标函数中各项之间的敌对平衡和高效的多分辨率学习方案。我们进行了文献中规模最大的经验研究，使用了15个数据集，包括从现有平衡数据集中新导出的长尾数据集，并展示了我们提出的方法大大优于先前的技术

    arXiv:2404.00285v1 Announce Type: cross  Abstract: Deploying deep models in real-world scenarios entails a number of challenges, including computational efficiency and real-world (e.g., long-tailed) data distributions. We address the combined challenge of learning long-tailed distributions using highly resource-efficient binary neural networks as backbones. Specifically, we propose a calibrate-and-distill framework that uses off-the-shelf pretrained full-precision models trained on balanced datasets to use as teachers for distillation when learning binary networks on long-tailed datasets. To better generalize to various datasets, we further propose a novel adversarial balancing among the terms in the objective function and an efficient multiresolution learning scheme. We conducted the largest empirical study in the literature using 15 datasets, including newly derived long-tailed datasets from existing balanced datasets, and show that our proposed method outperforms prior art by large 
    
[^113]: 基于大型语言模型增强强化学习的调查:概念、分类和方法

    Survey on Large Language Model-Enhanced Reinforcement Learning: Concept, Taxonomy, and Methods

    [https://arxiv.org/abs/2404.00282](https://arxiv.org/abs/2404.00282)

    大型语言模型在强化学习中具有潜在优势，通过结构化分类和角色分析，为未来研究提供指导。

    

    随着大规模语言模型(LLMs)拥有广泛的预训练知识和高级通用能力，它们在增强学习方面如多任务学习、样本效率和任务规划等方面展现出潜力。本调查综述了现有$\textit{LLM增强RL}$文献，总结了其与传统RL方法的特征，旨在澄清研究范围和未来研究方向。利用经典的Agent-环境交互范例，我们提出了一个结构化的分类法，系统地将LLMs在RL中的功能分类，包括四种角色：信息处理器、奖励设计者、决策者和生成器。此外，针对每个角色，我们总结了方法论，分析了缓解的特定RL挑战，并提供了未来方向的见解。最后，潜在应用、前景

    arXiv:2404.00282v1 Announce Type: cross  Abstract: With extensive pre-trained knowledge and high-level general capabilities, large language models (LLMs) emerge as a promising avenue to augment reinforcement learning (RL) in aspects such as multi-task learning, sample efficiency, and task planning. In this survey, we provide a comprehensive review of the existing literature in $\textit{LLM-enhanced RL}$ and summarize its characteristics compared to conventional RL methods, aiming to clarify the research scope and directions for future studies. Utilizing the classical agent-environment interaction paradigm, we propose a structured taxonomy to systematically categorize LLMs' functionalities in RL, including four roles: information processor, reward designer, decision-maker, and generator. Additionally, for each role, we summarize the methodologies, analyze the specific RL challenges that are mitigated, and provide insights into future directions. Lastly, potential applications, prospecti
    
[^114]: 大型语言模型上的指令驱动游戏引擎

    Instruction-Driven Game Engines on Large Language Models

    [https://arxiv.org/abs/2404.00276](https://arxiv.org/abs/2404.00276)

    通过在大型语言模型上开发指令驱动游戏引擎，使用户可以通过简单的自然语言指令创建游戏，从而降低游戏开发的难度。

    

    Instruction-Driven Game Engine (IDGE) 项目旨在通过使大型语言模型（LLM）遵循自由形式的游戏规则并自动生成游戏过程来使游戏开发民主化。IDGE允许用户通过发出简单的自然语言指令来创建游戏，从而显著降低了游戏开发的障碍。我们将IDGE的学习过程视为下一个状态预测任务，模型自回归地预测玩家行动给出的游戏状态。这是一个具有挑战性的任务，因为游戏状态的计算必须准确；否则，轻微的错误可能会破坏游戏过程。为了解决这个问题，我们以课程方式训练IDGE，逐渐增加模型对复杂场景的接触。

    arXiv:2404.00276v1 Announce Type: new  Abstract: The Instruction-Driven Game Engine (IDGE) project aims to democratize game development by enabling a large language model (LLM) to follow free-form game rules and autonomously generate game-play processes. The IDGE allows users to create games by issuing simple natural language instructions, which significantly lowers the barrier for game development. We approach the learning process for IDGEs as a Next State Prediction task, wherein the model autoregressively predicts in-game states given player actions. It is a challenging task because the computation of in-game states must be precise; otherwise, slight errors could disrupt the game-play. To address this, we train the IDGE in a curriculum manner that progressively increases the model's exposure to complex scenarios.   Our initial progress lies in developing an IDGE for Poker, a universally cherished card game. The engine we've designed not only supports a wide range of poker variants b
    
[^115]: TG-NAS：利用Transformer和图卷积网络与零成本代理进行高效神经结构搜索

    TG-NAS: Leveraging Zero-Cost Proxies with Transformer and Graph Convolution Networks for Efficient Neural Architecture Search

    [https://arxiv.org/abs/2404.00271](https://arxiv.org/abs/2404.00271)

    TG-NAS提出了一种新型模型通用代理，利用Transformer的运算符嵌入生成器和图卷积网络来预测架构性能，指导神经结构搜索。

    

    神经结构搜索(NAS)是一种发现新的卷积神经网络(CNN)架构的有效方法。然而，现有方法通常需要耗时的训练或密集的采样和评估。零成本NAS旨在为架构性能预测创建免训练代理。然而，现有代理性能亚优，并且常常被模型参数数量或浮点运算次数等简单指标所超越。此外，现有基于模型的代理无法将泛化到新的搜索空间，其中具有未见新类型运算符且不带有黄金准确度。一个普遍最优的代理仍然难以找到。我们引入了TG-NAS，一种利用基于Transformer的运算符嵌入生成器和图卷积网络(GCN)来预测架构性能的新型模型通用代理。这种方法指导着在任何给定搜索空间内进行神经结构搜索。

    arXiv:2404.00271v1 Announce Type: cross  Abstract: Neural architecture search (NAS) is an effective method for discovering new convolutional neural network (CNN) architectures. However, existing approaches often require time-consuming training or intensive sampling and evaluations. Zero-shot NAS aims to create training-free proxies for architecture performance prediction. However, existing proxies have suboptimal performance, and are often outperformed by simple metrics such as model parameter counts or the number of floating-point operations. Besides, existing model-based proxies cannot be generalized to new search spaces with unseen new types of operators without golden accuracy truth. A universally optimal proxy remains elusive. We introduce TG-NAS, a novel model-based universal proxy that leverages a transformer-based operator embedding generator and a graph convolution network (GCN) to predict architecture performance. This approach guides neural architecture search across any giv
    
[^116]: 一种简单而又有效的多样化基于会话的推荐方法

    A Simple Yet Effective Approach for Diversified Session-Based Recommendation

    [https://arxiv.org/abs/2404.00261](https://arxiv.org/abs/2404.00261)

    提出了一种简单而有效的多样化类别感知关注SBRS(DCA-SBRS)框架，可以作为插件用于辅助现有的SBRSs在保持推荐准确度的同时生成更多样化的列表

    

    会话型推荐系统(SBRSs)在捕捉短期和动态用户偏好的核心能力方面变得极为流行。然而，大多数SBRSs主要在于最大化推荐准确度，但忽略了用户的次要偏好，从而最终导致长期的筛泡。只有少数几项工作致力于提高多样性，依赖于独特的模型设计和校准损失函数，这些方法无法轻易地适应现有的面向准确度的SBRSs。因此，提出一种既简单又有效的设计是值得的，它可以作为插件用于辅助现有的SBRSs在保持推荐准确度的同时生成更多样化的列表。在本文中，我们提出了一种端到端框架，适用于每一个现有的代表性(面向准确度)SBRS，称为多样化类别感知关注SBRS(DCA-SBRS)，以提高推荐多样性的性能。

    arXiv:2404.00261v1 Announce Type: cross  Abstract: Session-based recommender systems (SBRSs) have become extremely popular in view of the core capability of capturing short-term and dynamic user preferences. However, most SBRSs primarily maximize recommendation accuracy but ignore user minor preferences, thus leading to filter bubbles in the long run. Only a handful of works, being devoted to improving diversity, depend on unique model designs and calibrated loss functions, which cannot be easily adapted to existing accuracy-oriented SBRSs. It is thus worthwhile to come up with a simple yet effective design that can be used as a plugin to facilitate existing SBRSs on generating a more diversified list in the meantime preserving the recommendation accuracy. In this case, we propose an end-to-end framework applied for every existing representative (accuracy-oriented) SBRS, called diversified category-aware attentive SBRS (DCA-SBRS), to boost the performance on recommendation diversity. I
    
[^117]: YOLOOC: 基于YOLO的开放类别增量目标检测与新类别发现

    YOLOOC: YOLO-based Open-Class Incremental Object Detection with Novel Class Discovery

    [https://arxiv.org/abs/2404.00257](https://arxiv.org/abs/2404.00257)

    该论文提出了基于YOLO架构的YOLOOC检测器，针对开放类别设置引入了标签平滑，有效应对新类别检测和增量学习的挑战。

    

    最近，由于其在实践中的运用，开放世界目标检测（OWOD）受到了很多关注。挑战在于模型如何检测新类别，然后增量学习它们而不会忘记先前已知的类别。先前的方法依赖于强监督或弱监督的新类别数据用于新类别检测，这可能不适用于实际应用。我们构建了一个新的基准，其中新类别只在推断阶段遇到。我们提出了一种基于YOLO架构的新OWOD检测器YOLOOC，专门针对开放类别的设置。我们引入了标签平滑以防止检测器过于自信地将新类别映射到已知类别并发现新类别。在我们更加现实的设置上进行的大量实验表明，我们的方法在我们的新基准下发现新类别的有效性。

    arXiv:2404.00257v1 Announce Type: cross  Abstract: Because of its use in practice, open-world object detection (OWOD) has gotten a lot of attention recently. The challenge is how can a model detect novel classes and then incrementally learn them without forgetting previously known classes. Previous approaches hinge on strongly-supervised or weakly-supervised novel-class data for novel-class detection, which may not apply to real applications. We construct a new benchmark that novel classes are only encountered at the inference stage. And we propose a new OWOD detector YOLOOC, based on the YOLO architecture yet for the Open-Class setup. We introduce label smoothing to prevent the detector from over-confidently mapping novel classes to known classes and to discover novel classes. Extensive experiments conducted on our more realistic setup demonstrate the effectiveness of our method for discovering novel classes in our new benchmark.
    
[^118]: 利用迁移学习促进过程控制的强化学习：观点

    Facilitating Reinforcement Learning for Process Control Using Transfer Learning: Perspectives

    [https://arxiv.org/abs/2404.00247](https://arxiv.org/abs/2404.00247)

    本文从迁移学习的角度探讨了如何将其与强化学习相结合，为过程控制带来新的可能性。

    

    本文从迁移学习的角度，为过程控制中的深度强化学习（DRL）提供了深入见解。我们分析了在过程工业领域应用DRL所面临的挑战，以及引入迁移学习的必要性。此外，我们为未来研究方向提供了建议和展望，探讨了如何将迁移学习与DRL结合起来加强过程控制。

    arXiv:2404.00247v1 Announce Type: cross  Abstract: This paper provides insights into deep reinforcement learning (DRL) for process control from the perspective of transfer learning. We analyze the challenges of applying DRL in the field of process industries and the necessity of introducing transfer learning. Furthermore, recommendations and prospects are provided for future research directions on how transfer learning can be integrated with DRL to empower process control.
    
[^119]: 你的同事很重要：评估语言模型在方块世界中的协作能力

    Your Co-Workers Matter: Evaluating Collaborative Capabilities of Language Models in Blocks World

    [https://arxiv.org/abs/2404.00246](https://arxiv.org/abs/2404.00246)

    在一个方块世界环境中，论文评估了大型语言模型的协作能力，通过设计不断增加挑战性的设置来评估不同的协作视角，从独立到更复杂的依赖任务。

    

    与世界自行交互的语言代理在自动化数字任务方面具有巨大潜力。虽然大型语言模型代理在理解和执行文本游戏和网页控制等任务方面取得了进展，但许多现实任务也需要与人类或其他同等角色的LLM协作，这涉及意图理解、任务协调和沟通。为测试LLM协作能力，我们设计了一个方块世界环境，在这个环境中，两个代理，每个代理都有独特的目标和技能，一起建造一个目标结构。为实现目标，他们可以在世界中行动并用自然语言进行沟通。在这个环境下，我们设计了越来越具有挑战性的设置，以评估不同协作视角，从独立的到更复杂的依赖任务。我们进一步采用了中间推理步骤来建模合作伙伴的状态。

    arXiv:2404.00246v1 Announce Type: cross  Abstract: Language agents that interact with the world on their own have great potential for automating digital tasks. While large language model (LLM) agents have made progress in understanding and executing tasks such as textual games and webpage control, many real-world tasks also require collaboration with humans or other LLMs in equal roles, which involves intent understanding, task coordination, and communication. To test LLM's ability to collaborate, we design a blocks-world environment, where two agents, each having unique goals and skills, build a target structure together. To complete the goals, they can act in the world and communicate in natural language. Under this environment, we design increasingly challenging settings to evaluate different collaboration perspectives, from independent to more complex, dependent tasks. We further adopt chain-of-thought prompts that include intermediate reasoning steps to model the partner's state a
    
[^120]: DeFT：带IO意识的Flash Tree-attention用于高效的基于树搜索的LLM推断

    DeFT: Flash Tree-attention with IO-Awareness for Efficient Tree-search-based LLM Inference

    [https://arxiv.org/abs/2404.00242](https://arxiv.org/abs/2404.00242)

    DeFT提出了一种带IO意识的树注意力算法，通过在QKV准备和注意力计算阶段实现内存高效的计算，降低内存印记，以解决当前树解码策略和推断系统不适配的问题。

    

    使用树搜索进行解码可以极大地提高基于变压器的大型语言模型（LLMs）的推断质量。根据引导信号，它通过形成LLM输出从根到叶子的最佳路径来提高可控性、推理能力、对齐等。然而，由于计算冗余、内存占用和内存访问，当前的树解码策略及其推断系统互相不适配，导致推断效率低下。为解决这一问题，我们提出了DeFT，一种IO感知树注意力算法，它在两个阶段中保持内存高效的注意力计算，降低内存印记：（1）QKV准备：我们提出了一种KV引导树分裂策略，为GPU的高利用率和尽可能减少GPU全局内存和芯片上共享内存之间的KV缓存的内存读/写; （2）注意力计算...

    arXiv:2404.00242v1 Announce Type: cross  Abstract: Decoding using tree search can greatly enhance the inference quality for transformer-based Large Language Models (LLMs). Depending on the guidance signal, it searches for the best path from root to leaf in the tree by forming LLM outputs to improve controllability, reasoning ability, alignment, et cetera. However, current tree decoding strategies and their inference systems do not suit each other well due to redundancy in computation, memory footprints, and memory access, resulting in inefficient inference. To address this issue, we propose DeFT, an IO-aware tree attention algorithm that maintains memory-efficient attention calculation with low memory footprints in two stages: (1) QKV Preparation: we propose a KV-Guided Tree Split strategy to group QKV wisely for high utilization of GPUs and reduction of memory reads/writes for the KV cache between GPU global memory and on-chip shared memory as much as possible; (2) Attention Calculati
    
[^121]: 基于注意力机制的形状变形网络用于无伪影几何重构骨盆腰椎MR图像

    Attention-based Shape-Deformation Networks for Artifact-Free Geometry Reconstruction of Lumbar Spine from MR Images

    [https://arxiv.org/abs/2404.00231](https://arxiv.org/abs/2404.00231)

    这项工作提出了TransDeformer，使用注意力机制实现了对腰椎轮廓的高空间准确性重建，并跨患者实现了网格对应，为医学参数测量提供了可靠性，还设计了变体用于错误估计。

    

    腰椎椎间盘退变，是腰椎间盘渐进性结构性磨损，被认为在腰部疼痛中发挥重要作用，这是一个重要的全球健康关注焦点。从MR图像中自动重建腰椎几何形状，将使医学参数的快速测量成为可能，以评估腰椎状态，从而确定合适的治疗方案。现有的基于图像分割的技术通常会生成错误的分割或不适合医学参数测量的无结构点云。在这项工作中，我们提出了TransDeformer：一种新颖的基于注意力机制的深度学习方法，以高空间准确度和患者间网格对应的方式重建腰椎轮廓，并且我们还提出了一种TransDeformer的变种用于错误估计。特别是，我们设计了新的注意力模块和新的注意力公式，将图像特征和标记化的轮廓特征集成起来，用于预测...

    arXiv:2404.00231v1 Announce Type: cross  Abstract: Lumbar disc degeneration, a progressive structural wear and tear of lumbar intervertebral disc, is regarded as an essential role on low back pain, a significant global health concern. Automated lumbar spine geometry reconstruction from MR images will enable fast measurement of medical parameters to evaluate the lumbar status, in order to determine a suitable treatment. Existing image segmentation-based techniques often generate erroneous segments or unstructured point clouds, unsuitable for medical parameter measurement. In this work, we present TransDeformer: a novel attention-based deep learning approach that reconstructs the contours of the lumbar spine with high spatial accuracy and mesh correspondence across patients, and we also present a variant of TransDeformer for error estimation. Specially, we devise new attention modules with a new attention formula, which integrates image features and tokenized contour features to predict 
    
[^122]: InfLoRA：无干扰的低秩自适应持续学习方法

    InfLoRA: Interference-Free Low-Rank Adaptation for Continual Learning

    [https://arxiv.org/abs/2404.00228](https://arxiv.org/abs/2404.00228)

    InfLoRA提出了一种新的PEFT方法，名为无干扰低秩自适应（InfLoRA），用于持续学习，旨在消除新任务对旧任务的干扰，帮助模型在稳定性和可塑性之间取得良好平衡。

    

    持续学习要求模型依次学习多个任务。在持续学习中，模型应具备在旧任务上维持性能（稳定性）和不断适应新任务的能力（可塑性）。最近，基于参数高效微调（PEFT）的持续学习方法变得越来越受欢迎。尽管现有基于PEFT的持续学习方法表现出比非PEFT方法更优秀的性能，但大多数方法并未考虑如何消除新任务对旧任务的干扰，从而阻碍模型在稳定性和可塑性之间取得良好平衡。本文提出了一种新的PEFT方法，称为无干扰低秩自适应（InfLoRA）方法，用于持续学习。

    arXiv:2404.00228v1 Announce Type: cross  Abstract: Continual learning requires the model to learn multiple tasks sequentially. In continual learning, the model should possess the ability to maintain its performance on old tasks (stability) and the ability to adapt to new tasks continuously (plasticity). Recently, parameter-efficient fine-tuning (PEFT), which involves freezing a pre-trained model and injecting a small number of learnable parameters to adapt to downstream tasks, has gained increasing popularity in continual learning. Although existing continual learning methods based on PEFT have demonstrated superior performance compared to those not based on PEFT, most of them do not consider how to eliminate the interference of the new task on the old tasks, which inhibits the model from making a good trade-off between stability and plasticity. In this work, we propose a new PEFT method, called interference-free low-rank adaptation (InfLoRA), for continual learning. InfLoRA injects a 
    
[^123]: 大型语言模型中的事实解码：在知识编辑基准上的评估

    Is Factuality Decoding a Free Lunch for LLMs? Evaluation on Knowledge Editing Benchmark

    [https://arxiv.org/abs/2404.00216](https://arxiv.org/abs/2404.00216)

    大型语言模型通过事实解码方法提高了事实准确性，然而，这些方法使模型对已知事实过于自信，进一步评估显示在知识编辑基准上所有解码方法均显著降低了模型性能。

    

    大型语言模型（LLMs）的快速发展使它们能够以更类似于人类的方式传达事实知识。人们已经做出了大量努力来通过修改LLMs并降低事实幻觉来提高事实准确性。然而，这些修改也存在阻碍知识更新的风险，因为它们使模型对已知事实过于自信。本文首先重新审视当前的事实解码方法，并验证了它们在提高事实准确性方面的有效性。随后，我们对几种强大的事实解码方法在知识编辑基准上进行进一步评估。所有这些解码方法与其原始解码相比均显着降低了llama2模型的性能，其中最大的降低幅度达到惊人的81.3\%。这进一步表明，当前的解码方法仍无法完全解决事实幻觉问题，因为它们忽视了先验知识的重要性。

    arXiv:2404.00216v1 Announce Type: cross  Abstract: The rapid development of large language models (LLMs) enables them to convey factual knowledge in a more human-like fashion. Extensive efforts have been made to reduce factual hallucinations by modifying LLMs with factuality decoding. However, they also pose risks of hindering knowledge updates, as they make models overly confident in known facts. In this work, we first revisite the current factuality decoding methods and verified their effectiveness in enhancing factual accuracy. Subsequently, we conduct further evaluation of several strong factuality decoding methods on the knowledge editing benchmark. All these decoding methods significantly diminish the performance of llama2 models compared to their original decoding, with the largest decrease being a staggering 81.3\%. This further indicates that the current existing decoding methods still cannot perfectly address the factual hallucinations, as they overlook the importance of pres
    
[^124]: 人类-语言模型协作的因果推断

    Causal Inference for Human-Language Model Collaboration

    [https://arxiv.org/abs/2404.00207](https://arxiv.org/abs/2404.00207)

    本文研究了人类和语言模型之间的协作动态，并提出了一个新的因果估计Incremental Stylistic Effect来解释如何改变合作结果。

    

    在本文中，我们研究了人类和语言模型（LMs）之间的协作动态，这些互动通常涉及LMs提出文本段落，而人类编辑或回应这些建议。在这种情况下与LMs进行有效的互动要求人类辨别出有效的基于文本的互动策略，例如编辑和回应样式，从历史人类-LM互动中。这个目标本质上是因果关系，受到反事实“如果”问题的驱动:如果人类采用不同的文本编辑/精炼策略，协作的结果会如何改变？回答这个因果推断问题的一个关键挑战是制定一个适当的因果估计:传统的平均处理效应（ATE）估计由于文本的高维度而不适用于基于文本的处理。为了解决这一问题，我们引入了一个新的因果估计 - 增量风格效应

    arXiv:2404.00207v1 Announce Type: cross  Abstract: In this paper, we examine the collaborative dynamics between humans and language models (LMs), where the interactions typically involve LMs proposing text segments and humans editing or responding to these proposals. Productive engagement with LMs in such scenarios necessitates that humans discern effective text-based interaction strategies, such as editing and response styles, from historical human-LM interactions. This objective is inherently causal, driven by the counterfactual `what-if' question: how would the outcome of collaboration change if humans employed a different text editing/refinement strategy? A key challenge in answering this causal inference question is formulating an appropriate causal estimand: the conventional average treatment effect (ATE) estimand is inapplicable to text-based treatments due to their high dimensionality. To address this concern, we introduce a new causal estimand -- Incremental Stylistic Effect (
    
[^125]: 通过密度估计进行多策略评估

    Multiple-policy Evaluation via Density Estimation

    [https://arxiv.org/abs/2404.00195](https://arxiv.org/abs/2404.00195)

    该研究提出一种名为 $\mathrm{CAESAR}$ 的算法，通过计算一个近似的最优离线采样分布，同时估计多个策略的价值，以解决多策略评估问题。

    

    在这项工作中，我们专注于多策略评估问题，给定一组 $K$ 个目标策略，目标是以至少 $1-\delta$ 的概率评估它们的性能（期望总奖励）达到精度 $\epsilon$。我们提出了一种名为 $\mathrm{CAESAR}$ 的算法来解决这个问题。我们的方法基于计算一个近似的最优离线采样分布，并利用从中采样的数据来同时估计策略价值。$\mathrm{CAESAR}$ 包括两个阶段。在第一个阶段，我们以随着 $\tilde{O}(\frac{1}{\epsilon})$ 缩放的低订单采样复杂性率产生目标策略的访问分布的粗略估计。在第二阶段，我们近似最优离线采样分布，并通过最小化一个逐步二次损失函数来计算所有目标策略的重要性权重比例。

    arXiv:2404.00195v1 Announce Type: cross  Abstract: In this work, we focus on the multiple-policy evaluation problem where we are given a set of $K$ target policies and the goal is to evaluate their performance (the expected total rewards) to an accuracy $\epsilon$ with probability at least $1-\delta$. We propose an algorithm named $\mathrm{CAESAR}$ to address this problem. Our approach is based on computing an approximate optimal offline sampling distribution and using the data sampled from it to perform the simultaneous estimation of the policy values. $\mathrm{CAESAR}$ consists of two phases. In the first one we produce coarse estimates of the vistation distributions of the target policies at a low order sample complexity rate that scales with $\tilde{O}(\frac{1}{\epsilon})$. In the second phase, we approximate the optimal offline sampling distribution and compute the importance weighting ratios for all target policies by minimizing a step-wise quadratic loss function inspired by the
    
[^126]: DataAgent: 评估大型语言模型回答零样本自然语言查询的能力

    DataAgent: Evaluating Large Language Models' Ability to Answer Zero-Shot, Natural Language Queries

    [https://arxiv.org/abs/2404.00188](https://arxiv.org/abs/2404.00188)

    评估了 OpenAI 的 GPT-3.5 模型作为“语言数据科学家”，成功回答了与基准数据集相关的数据科学查询。

    

    传统的数据集分析和提取有意义信息的过程往往耗时且繁琐。以前的工作已经确定手动、重复性编码和数据收集是阻碍数据科学家从事更微妙的工作和高水平项目的主要障碍。为了解决这个问题，我们评估了 OpenAI 的 GPT-3.5 作为“语言数据科学家”（LDS），可以从给定数据集中推导出关键发现，包括相关性和基本信息。该模型在多个标准上表现良好，包括基于数据科学代码生成的任务，涉及NumPy、Pandas、Scikit-Learn 和 TensorFlow 等库，并在正确回答与基准数据集相关的给定数据科学查询方面取得了广泛成功。LDS 使用了各种新颖的提示工程技术来有效回答给定问题，包括

    arXiv:2404.00188v1 Announce Type: cross  Abstract: Conventional processes for analyzing datasets and extracting meaningful information are often time-consuming and laborious. Previous work has identified manual, repetitive coding and data collection as major obstacles that hinder data scientists from undertaking more nuanced labor and high-level projects. To combat this, we evaluated OpenAI's GPT-3.5 as a "Language Data Scientist" (LDS) that can extrapolate key findings, including correlations and basic information, from a given dataset. The model was tested on a diverse set of benchmark datasets to evaluate its performance across multiple standards, including data science code-generation based tasks involving libraries such as NumPy, Pandas, Scikit-Learn, and TensorFlow, and was broadly successful in correctly answering a given data science query related to the benchmark dataset. The LDS used various novel prompt engineering techniques to effectively answer a given question, including
    
[^127]: 论主动视觉系统固有对抗鲁棒性

    On Inherent Adversarial Robustness of Active Vision Systems

    [https://arxiv.org/abs/2404.00185](https://arxiv.org/abs/2404.00185)

    主动视觉机制的集成可以提供当前深度学习系统的鲁棒性优势，并且通过实验证明了GFNet和FALcon这两种主动视觉方法在黑盒威胁模型下的固有鲁棒性。

    

    当前的深度神经网络容易受到对抗样本的攻击，这种对抗样本通过添加精心设计的噪声来改变它们的预测。由于人类眼睛对这种输入具有鲁棒性，可能的弱点源于用相同重要性处理每个像素的标准方式。相比之下，神经科学表明人类视觉系统可以通过（1）切换多个注视点（扫视）和（2）以非均匀外部分辨率（视神经）处理周围信息来区分显著特征。本研究提倡将这种主动视觉机制融入当前的深度学习系统中，以提供鲁棒性优势。具体来说，我们在黑盒威胁模型下，经验性地展示了两种主动视觉方法GFNet和FALcon的固有鲁棒性。

    arXiv:2404.00185v1 Announce Type: cross  Abstract: Current Deep Neural Networks are vulnerable to adversarial examples, which alter their predictions by adding carefully crafted noise. Since human eyes are robust to such inputs, it is possible that the vulnerability stems from the standard way of processing inputs in one shot by processing every pixel with the same importance. In contrast, neuroscience suggests that the human vision system can differentiate salient features by (1) switching between multiple fixation points (saccades) and (2) processing the surrounding with a non-uniform external resolution (foveation). In this work, we advocate that the integration of such active vision mechanisms into current deep learning systems can offer robustness benefits. Specifically, we empirically demonstrate the inherent robustness of two active vision methods - GFNet and FALcon - under a black box threat model. By learning and inferencing based on downsampled glimpses obtained from multiple
    
[^128]: 超越停赛：结束体育联盟的两阶段方法论

    Beyond Suspension: A Two-phase Methodology for Concluding Sports Leagues

    [https://arxiv.org/abs/2404.00178](https://arxiv.org/abs/2404.00178)

    该研究提出了一种利用预测性和处方性分析的数据驱动模型，为结束现有体育赛季提供了一种新的两阶段方法，以产生类似完整赛季结果的队伍排名。

    

    问题定义：专业体育联盟可能因各种原因暂停，比如最近的COVID-19大流行。重要问题是联盟在重新开赛时如何适当地选择其余比赛的子集，以在缩短的时间内结束赛季。学术/实践意义：尽管有丰富的文献关于从零开始安排整个赛季，结束现有赛季却截然不同。我们的方法旨在实现团队排名，使其与完整赛季比赛的结果类似。方法：我们提出了一个数据驱动模型，利用预测性和处方性分析，为剩余赛季制定一个由原定安排比赛组成的赛程子集。我们的模型在随机优化模型中引入了基于新颖排名目标，其参数首先使用预测性确定。

    arXiv:2404.00178v1 Announce Type: cross  Abstract: Problem definition: Professional sports leagues may be suspended due to various reasons such as the recent COVID-19 pandemic. A critical question the league must address when re-opening is how to appropriately select a subset of the remaining games to conclude the season in a shortened time frame. Academic/practical relevance: Despite the rich literature on scheduling an entire season starting from a blank slate, concluding an existing season is quite different. Our approach attempts to achieve team rankings similar to that which would have resulted had the season been played out in full. Methodology: We propose a data-driven model which exploits predictive and prescriptive analytics to produce a schedule for the remainder of the season comprised of a subset of originally-scheduled games. Our model introduces novel rankings-based objectives within a stochastic optimization model, whose parameters are first estimated using a predictive 
    
[^129]: 通过深度数据和深度度量学习实现的通用牛标识

    Universal Bovine Identification via Depth Data and Deep Metric Learning

    [https://arxiv.org/abs/2404.00172](https://arxiv.org/abs/2404.00172)

    该论文提出了一种利用深度数据和深度度量学习进行通用牛标识的方法，可以在不需要物种特定的外套图案或特写口吻印记的情况下，通过CNN和MLP基础实现学习到良好泛化嵌入空间，从体形区分个体。

    

    本文首次提出并评估了一种顶部（背部视图）深度数据深度学习系统，用于准确识别个体牛，并提供了相关代码、数据集和训练权重，以便立即复现。畜群规模增加导致牛与人的比例失衡，使得对个体进行手动监测变得更具挑战性。因此，实时牛标识对农场至关重要，是精准畜牧业的关键一步。

    arXiv:2404.00172v1 Announce Type: cross  Abstract: This paper proposes and evaluates, for the first time, a top-down (dorsal view), depth-only deep learning system for accurately identifying individual cattle and provides associated code, datasets, and training weights for immediate reproducibility. An increase in herd size skews the cow-to-human ratio at the farm and makes the manual monitoring of individuals more challenging. Therefore, real-time cattle identification is essential for the farms and a crucial step towards precision livestock farming. Underpinned by our previous work, this paper introduces a deep-metric learning method for cattle identification using depth data from an off-the-shelf 3D camera. The method relies on CNN and MLP backbones that learn well-generalised embedding spaces from the body shape to differentiate individuals -- requiring neither species-specific coat patterns nor close-up muzzle prints for operation. The network embeddings are clustered using a simp
    
[^130]: 通过反事实来揭示大规模视觉-语言模型中的偏见

    Uncovering Bias in Large Vision-Language Models with Counterfactuals

    [https://arxiv.org/abs/2404.00166](https://arxiv.org/abs/2404.00166)

    通过对输入图像进行反事实变化，我们对不同大规模视觉-语言模型生成的文本进行了研究，以揭示其中的社会偏见。

    

    随着拥有越来越令人印象深刻能力的大型语言模型（LLMs）的出现，提出了许多大型视觉-语言模型（LVLMs）来利用视觉输入增强LLMs。这些模型在生成文本时同时条件于输入图像和文本提示，实现了各种用例，如视觉问答和多模态聊天。尽管先前的研究已经考察了LLMs生成的文本中包含的社会偏见，但在LVLMs中相对较少探讨了这个问题。由于文本和视觉模态中包含的信息引起的偏见相互交叉，检查LVLMs中的社会偏见尤其具有挑战性。为了解决这个具有挑战性的问题，我们对不同LVLMs生成的文本进行了大规模研究，通过对输入图像进行反事实变化。具体来说，我们向LVLMs呈现相同的开放式文本提示，同时在不同的图像条件下进行。

    arXiv:2404.00166v1 Announce Type: cross  Abstract: With the advent of Large Language Models (LLMs) possessing increasingly impressive capabilities, a number of Large Vision-Language Models (LVLMs) have been proposed to augment LLMs with visual inputs. Such models condition generated text on both an input image and a text prompt, enabling a variety of use cases such as visual question answering and multimodal chat. While prior studies have examined the social biases contained in text generated by LLMs, this topic has been relatively unexplored in LVLMs. Examining social biases in LVLMs is particularly challenging due to the confounding contributions of bias induced by information contained across the text and visual modalities. To address this challenging problem, we conduct a large-scale study of text generated by different LVLMs under counterfactual changes to input images. Specifically, we present LVLMs with identical open-ended text prompts while conditioning on images from differen
    
[^131]: 利用在线生成的经验加速基于搜索的多机器人操作规划

    Accelerating Search-Based Planning for Multi-Robot Manipulation by Leveraging Online-Generated Experiences

    [https://arxiv.org/abs/2404.00143](https://arxiv.org/abs/2404.00143)

    提出了一种加速基于冲突搜索算法的方法，通过利用其重复性和增量性质，使其适用于涉及多臂协调的复杂场景

    

    机器人操作的一个激动人心的前沿是同时使用多个机械臂。然而，使用当前方法规划并发运动是一项具有挑战性的任务。高维复合状态空间使许多众所周知的运动规划算法变得棘手。最近，多智能体路径规划（MAPF）算法在离散2D领域表现出了潜力，提供了严格的保证。然而，MAPF中广泛使用的基于冲突的方法假定存在高效的单智能体运动规划器。这在调整这些方法以适应规划操作情况时带来了挑战，因为这一假设在这种情况下不成立，由于配置空间的高维度以及与碰撞检测相关的计算瓶颈。为此，我们提出了一种通过利用其重复性和增量性质加速冲突基搜索算法的方法，使其适用于涉及多臂协调的复杂情况。

    arXiv:2404.00143v1 Announce Type: cross  Abstract: An exciting frontier in robotic manipulation is the use of multiple arms at once. However, planning concurrent motions is a challenging task using current methods. The high-dimensional composite state space renders many well-known motion planning algorithms intractable. Recently, Multi-Agent Path-Finding (MAPF) algorithms have shown promise in discrete 2D domains, providing rigorous guarantees. However, widely used conflict-based methods in MAPF assume an efficient single-agent motion planner. This poses challenges in adapting them to manipulation cases where this assumption does not hold, due to the high dimensionality of configuration spaces and the computational bottlenecks associated with collision checking. To this end, we propose an approach for accelerating conflict-based search algorithms by leveraging their repetitive and incremental nature -- making them tractable for use in complex scenarios involving multi-arm coordination 
    
[^132]: 信实性与可信度是否存在冲突？一项关于自然语言处理任务中可解释人工智能的实证研究

    Does Faithfulness Conflict with Plausibility? An Empirical Study in Explainable AI across NLP Tasks

    [https://arxiv.org/abs/2404.00140](https://arxiv.org/abs/2404.00140)

    传统的扰动方法Shapley值和LIME可实现更高的信实性和可信度，建议优化可解释性算法以实现高效的双重目标

    

    旨在解释决策型人工智能系统的可解释性算法通常要平衡两个关键方面：1）\textit{信实性}，即解释必须准确反映模型的推理过程；2）\textit{可信度}，即解释必须与领域专家保持一致。然而，一个问题出现了：信实性和可信度是否从根本上存在冲突？通过在情感分析、意图检测和主题标注三个NLP任务中选定的解释方法与专家级解释之间的全面量化比较，我们证明传统的扰动方法Shapley值和LIME可以实现更高的信实性和可信度。我们的发现表明，我们可以寻求通过双重目标优化可解释性算法来实现高

    arXiv:2404.00140v1 Announce Type: new  Abstract: Explainability algorithms aimed at interpreting decision-making AI systems usually consider balancing two critical dimensions: 1) \textit{faithfulness}, where explanations accurately reflect the model's inference process. 2) \textit{plausibility}, where explanations are consistent with domain experts. However, the question arises: do faithfulness and plausibility inherently conflict? In this study, through a comprehensive quantitative comparison between the explanations from the selected explainability methods and expert-level interpretations across three NLP tasks: sentiment analysis, intent detection, and topic labeling, we demonstrate that traditional perturbation-based methods Shapley value and LIME could attain greater faithfulness and plausibility. Our findings suggest that rather than optimizing for one dimension at the expense of the other, we could seek to optimize explainability algorithms with dual objectives to achieve high l
    
[^133]: 生成式人工智能在物联网中的安全风险问题

    Security Risks Concerns of Generative AI in the IoT

    [https://arxiv.org/abs/2404.00139](https://arxiv.org/abs/2404.00139)

    生成式人工智能在物联网中的集成带来了安全风险，本文分析了数据泄露和技术滥用等风险，并讨论了缓解这些风险的战略方法。

    

    在物联网 (IoT) 与生成式人工智能 (AI) 日益交汇的时代，本文审视了这种整合中潜在的安全风险。我们探讨了生成式人工智能在物联网中推动创新的方式，分析了在使用生成式人工智能时可能发生的数据泄露风险以及在物联网生态系统中滥用生成式人工智能技术的潜力。这些风险不仅威胁着物联网系统的隐私和效率，而且对在以人工智能为驱动的环境中的信任和安全性产生了更广泛的影响。本文讨论延伸至缓解这些风险的战略方法，包括制定健全的安全协议、多层次的安全方法以及采用人工智能技术解决方案。通过全面分析，本文旨在阐明在物联网中拥抱人工智能进步与确保严格安全之间的关键平衡，提供了...

    arXiv:2404.00139v1 Announce Type: cross  Abstract: In an era where the Internet of Things (IoT) intersects increasingly with generative Artificial Intelligence (AI), this article scrutinizes the emergent security risks inherent in this integration. We explore how generative AI drives innovation in IoT and we analyze the potential for data breaches when using generative AI and the misuse of generative AI technologies in IoT ecosystems. These risks not only threaten the privacy and efficiency of IoT systems but also pose broader implications for trust and safety in AI-driven environments. The discussion in this article extends to strategic approaches for mitigating these risks, including the development of robust security protocols, the multi-layered security approaches, and the adoption of AI technological solutions. Through a comprehensive analysis, this article aims to shed light on the critical balance between embracing AI advancements and ensuring stringent security in IoT, providin
    
[^134]: 预算感知查询优化：自动机器学习视角

    Budget-aware Query Tuning: An AutoML Perspective

    [https://arxiv.org/abs/2404.00137](https://arxiv.org/abs/2404.00137)

    将传统成本单位视为变量，通过调整值可以获得比默认查询计划更优的查询计划

    

    现代数据库系统依赖基于成本的查询优化器为输入查询提供良好的执行计划。这种查询优化器依赖成本模型来估算候选查询执行计划的成本。本文挑战了传统观念，将这些成本单位视为变量，表明通过调整成本单位值，可以获得明显优于将成本单位视为常数时查询优化器返回的默认查询计划的查询计划。

    arXiv:2404.00137v1 Announce Type: cross  Abstract: Modern database systems rely on cost-based query optimizers to come up with good execution plans for input queries. Such query optimizers rely on cost models to estimate the costs of candidate query execution plans. A cost model represents a function from a set of cost units to query execution cost, where each cost unit specifies the unit cost of executing a certain type of query processing operation (such as table scan or join). These cost units are traditionally viewed as constants, whose values only depend on the platform configuration where the database system runs on top of but are invariant for queries processed by the database system. In this paper, we challenge this classic view by thinking of these cost units as variables instead. We show that, by varying the cost-unit values one can obtain query plans that significantly outperform the default query plans returned by the query optimizer when viewing the cost units as constants
    
[^135]: 通过具有遮挡处理的正交融合的强健集成人员识别

    Robust Ensemble Person Re-Identification via Orthogonal Fusion with Occlusion Handling

    [https://arxiv.org/abs/2404.00107](https://arxiv.org/abs/2404.00107)

    提出了一种通过正交融合处理遮挡的强健集成人员识别模型，无需手动标记遮挡区域，利用CNN和Transformer架构生成鲁棒特征表示。

    

    遮挡仍然是人员再识别（ReID）中的一个主要挑战，这是由于姿势的多样性和外观的变化。为了改善具有遮挡感知的人员Re-ID的鲁棒性，需要开发新的架构，特别是在低分辨率边缘摄像头上。我们提出了一种深度集成模型，利用CNN和Transformer架构生成强大的特征表示。为了实现鲁棒的Re-ID，而无需手动标记被遮挡的区域，我们提出了一种基于集成学习的方法，从任意形状的被遮挡区域和鲁棒特征表示之间的类比中推导出。利用正交性原理，我们开发的深度CNN模型利用了掩蔽自编码器（MAE）和全局-局部特征融合来进行强大的人员识别。此外，我们提出了一个能够学习鲁棒特征空间的部分遮挡感知变压器。

    arXiv:2404.00107v1 Announce Type: cross  Abstract: Occlusion remains one of the major challenges in person reidentification (ReID) as a result of the diversity of poses and the variation of appearances. Developing novel architectures to improve the robustness of occlusion-aware person Re-ID requires new insights, especially on low-resolution edge cameras. We propose a deep ensemble model that harnesses both CNN and Transformer architectures to generate robust feature representations. To achieve robust Re-ID without the need to manually label occluded regions, we propose to take an ensemble learning-based approach derived from the analogy between arbitrarily shaped occluded regions and robust feature representation. Using the orthogonality principle, our developed deep CNN model makes use of masked autoencoder (MAE) and global-local feature fusion for robust person identification. Furthermore, we present a part occlusion-aware transformer capable of learning feature space that is robust
    
[^136]: 在强健马尔可夫决策过程中高效而尖锐的离线策略评估

    Efficient and Sharp Off-Policy Evaluation in Robust Markov Decision Processes

    [https://arxiv.org/abs/2404.00099](https://arxiv.org/abs/2404.00099)

    在对抗性环境中，本论文提出了一种可修改转移核密度的扰动模型，拓展了传统的边缘敏感性模型，对无限时间RL中策略价值进行了尖锐边界的刻画和估计。

    

    我们研究了在马尔可夫决策过程（MDP）中给定来自原始MDP的转移观察时，在最佳和最坏情况下评估策略，无论是在相同策略还是不同策略下。当存在历史和未来环境之间可能发生转变的可能性时，比如由于未测量的混杂、分布转移或对抗性环境。我们提出了一个扰动模型，可以将转移核密度修改至给定乘法因子或其倒数，这将经典的边际敏感性模型（MSM）扩展到无限时间 RL。我们描述了在这个模型下的策略价值的尖锐边界，即在给定来自原始MDP的转移观测时可能的最严格边界，我们研究了从这些转移观察中估计这些边界。我们开发了一个估计器，具有几个吸引人的特性。

    arXiv:2404.00099v1 Announce Type: new  Abstract: We study evaluating a policy under best- and worst-case perturbations to a Markov decision process (MDP), given transition observations from the original MDP, whether under the same or different policy. This is an important problem when there is the possibility of a shift between historical and future environments, due to e.g. unmeasured confounding, distributional shift, or an adversarial environment. We propose a perturbation model that can modify transition kernel densities up to a given multiplicative factor or its reciprocal, which extends the classic marginal sensitivity model (MSM) for single time step decision making to infinite-horizon RL. We characterize the sharp bounds on policy value under this model, that is, the tightest possible bounds given by the transition observations from the original MDP, and we study the estimation of these bounds from such transition observations. We develop an estimator with several appealing gua
    
[^137]: 具有多属性优化的分子生成对抗网络

    Molecular Generative Adversarial Network with Multi-Property Optimization

    [https://arxiv.org/abs/2404.00081](https://arxiv.org/abs/2404.00081)

    该研究引入了一种新型的基于演员-评论家强化学习的GAN，即InstGAN，以在令牌级别上生成具有多属性优化的分子，并利用最大化信息熵来缓解模式崩溃。

    

    深度生成模型，如生成对抗网络（GANs），已被应用于药物发现中$de~novo$分子生成。大多数先前的研究使用强化学习（RL）算法，特别是蒙特卡罗树搜索（MCTS），来处理GANs中分子表示的离散特性。然而，由于GANs和RL模型的固有训练不稳定性，以及与MCTS采样相关的高计算成本，MCTS RL-based GANs难以扩展到大型化学数据库。为了解决这些挑战，本研究提出了一种基于带即时和全局奖励的演员-评论家RL的新型GAN，称为InstGAN，以在令牌级别上生成具有多属性优化的分子。此外，最大化信息熵被利用来缓解模式崩溃。实验结果表明，InstGAN优于其他基线，达到了可比较的性能。

    arXiv:2404.00081v1 Announce Type: cross  Abstract: Deep generative models, such as generative adversarial networks (GANs), have been employed for $de~novo$ molecular generation in drug discovery. Most prior studies have utilized reinforcement learning (RL) algorithms, particularly Monte Carlo tree search (MCTS), to handle the discrete nature of molecular representations in GANs. However, due to the inherent instability in training GANs and RL models, along with the high computational cost associated with MCTS sampling, MCTS RL-based GANs struggle to scale to large chemical databases. To tackle these challenges, this study introduces a novel GAN based on actor-critic RL with instant and global rewards, called InstGAN, to generate molecules at the token-level with multi-property optimization. Furthermore, maximized information entropy is leveraged to alleviate the mode collapse. The experimental results demonstrate that InstGAN outperforms other baselines, achieves comparable performance
    
[^138]: 使用倒置标签的后门方法：脏标签翻转攻击

    A Backdoor Approach with Inverted Labels Using Dirty Label-Flipping Attacks

    [https://arxiv.org/abs/2404.00076](https://arxiv.org/abs/2404.00076)

    提出了一种后门攻击方法，名为“DirtyFlipping”，利用脏标签技术在选定的数据模式中输入触发器，从而实现隐蔽的后门。

    

    基于声音的机器学习系统经常使用公共或第三方数据，这可能是不准确的。这使得训练在这些数据上的深度神经网络（DNN）模型容易受到潜在的数据毒化攻击。在这种攻击类型中，攻击者可以使用毒化数据来训练DNN模型，可能会降低其性能。另一种对我们的研究非常相关的数据毒化攻击类型是标签翻转，攻击者在其中操纵数据子集的标签。已经证明，即使是能力有限的攻击者，这些攻击也可能极大地降低系统性能。在本研究中，我们提出了一种名为“DirtyFlipping”的后门攻击，使用脏标签技术，“标签对标签”，在与目标类别相关的选定数据模式中输入触发器（拍手），从而实现了隐蔽的后门。

    arXiv:2404.00076v1 Announce Type: cross  Abstract: Audio-based machine learning systems frequently use public or third-party data, which might be inaccurate. This exposes deep neural network (DNN) models trained on such data to potential data poisoning attacks. In this type of assault, attackers can train the DNN model using poisoned data, potentially degrading its performance. Another type of data poisoning attack that is extremely relevant to our investigation is label flipping, in which the attacker manipulates the labels for a subset of data. It has been demonstrated that these assaults may drastically reduce system performance, even for attackers with minimal abilities. In this study, we propose a backdoor attack named 'DirtyFlipping', which uses dirty label techniques, "label-on-label", to input triggers (clapping) in the selected data patterns associated with the target class, thereby enabling a stealthy backdoor.
    
[^139]: 金融网络中的时间图网络用于异常检测

    Temporal Graph Networks for Graph Anomaly Detection in Financial Networks

    [https://arxiv.org/abs/2404.00060](https://arxiv.org/abs/2404.00060)

    时间图网络（TGN）在金融网络中的异常检测中表现出显著优势，适应了现代金融系统动态和复杂的特性。

    

    本文探讨了利用时间图网络（TGN）进行金融异常检测，在金融科技和数字化金融交易时代这是一个迫切需要。我们提出了一个全面的框架，利用TGN捕捉金融网络中边的动态变化，用于欺诈检测。我们比较了TGN与静态图神经网络（GNN）基线以及使用DGraph数据集进行现实金融场景下的前沿超图神经网络基线的性能。结果表明，TGN在AUC指标方面显著优于其他模型。这种优越性能突显了TGN作为检测金融欺诈的有效工具的潜力，展示了其适应现代金融系统动态和复杂性的能力。我们还在TGN框架内尝试了各种图嵌入模块，并比较了它们的有效性。

    arXiv:2404.00060v1 Announce Type: cross  Abstract: This paper explores the utilization of Temporal Graph Networks (TGN) for financial anomaly detection, a pressing need in the era of fintech and digitized financial transactions. We present a comprehensive framework that leverages TGN, capable of capturing dynamic changes in edges within financial networks, for fraud detection. Our study compares TGN's performance against static Graph Neural Network (GNN) baselines, as well as cutting-edge hypergraph neural network baselines using DGraph dataset for a realistic financial context. Our results demonstrate that TGN significantly outperforms other models in terms of AUC metrics. This superior performance underlines TGN's potential as an effective tool for detecting financial fraud, showcasing its ability to adapt to the dynamic and complex nature of modern financial systems. We also experimented with various graph embedding modules within the TGN framework and compared the effectiveness of 
    
[^140]: PerOS: 在云中的个性化自适应操作系统

    PerOS: Personalized Self-Adapting Operating Systems in the Cloud

    [https://arxiv.org/abs/2404.00057](https://arxiv.org/abs/2404.00057)

    PerOS是个性化自适应操作系统，解决了传统OS在情报和用户体验方面的不足，尤其适应了机器学习和大型语言模型的发展。

    

    操作系统（OS）是计算机系统的基础，管理硬件资源并确保多样应用程序的安全环境。然而，尽管它们具有持久重要性，但OS的基本设计目标几十年来几乎没有进化。传统上优先考虑速度、内存效率、安全性和可伸缩性等方面，这些目标常常忽视了情报和个性化用户体验这一关键方面。情报的缺乏在技术革命中变得越发关键，比如机器学习（ML）方面的显著进展。如今的个人设备正在演变成用户的亲密伴侣，对传统OS（如Linux和iOS）提出独特挑战，尤其是随着特殊硬件和异构组件的出现。此外，机器学习中大型语言模型（LLMs）的崛起引入了转变性的能力。

    arXiv:2404.00057v1 Announce Type: cross  Abstract: Operating systems (OSes) are foundational to computer systems, managing hardware resources and ensuring secure environments for diverse applications. However, despite their enduring importance, the fundamental design objectives of OSes have seen minimal evolution over decades. Traditionally prioritizing aspects like speed, memory efficiency, security, and scalability, these objectives often overlook the crucial aspect of intelligence as well as personalized user experience. The lack of intelligence becomes increasingly critical amid technological revolutions, such as the remarkable advancements in machine learning (ML).   Today's personal devices, evolving into intimate companions for users, pose unique challenges for traditional OSes like Linux and iOS, especially with the emergence of specialized hardware featuring heterogeneous components. Furthermore, the rise of large language models (LLMs) in ML has introduced transformative capa
    
[^141]: Deja vu: 使用前缀调整进行对比历史建模，用于时间知识图推理

    Deja vu: Contrastive Historical Modeling with Prefix-tuning for Temporal Knowledge Graph Reasoning

    [https://arxiv.org/abs/2404.00051](https://arxiv.org/abs/2404.00051)

    提出了ChapTER，一个对比历史建模框架，通过前缀调整实现文本与时间的平衡，用于时间知识图推理。

    

    时间知识图推理（TKGR）是在复杂场景中为不完整的TKG推断缺失事实的任务（例如，传导和归纳设置），越来越受到关注。最近，为了减少TKG中结构连接的依赖性，已开发了基于文本的方法，利用实体描述中丰富的语言信息。然而，由于预训练语言模型的巨大参数和不灵活性，现有的基于文本的方法在计算昂贵且目的建立的训练策略上很难平衡文本知识和时间信息。为了发掘文本模型在各种复杂场景中用于TKGR的潜力，我们提出了ChapTER，一个具有前缀调整对比历史建模框架，用于时间推理。

    arXiv:2404.00051v1 Announce Type: new  Abstract: Temporal Knowledge Graph Reasoning (TKGR) is the task of inferring missing facts for incomplete TKGs in complex scenarios (e.g., transductive and inductive settings), which has been gaining increasing attention. Recently, to mitigate dependence on structured connections in TKGs, text-based methods have been developed to utilize rich linguistic information from entity descriptions. However, suffering from the enormous parameters and inflexibility of pre-trained language models, existing text-based methods struggle to balance the textual knowledge and temporal information with computationally expensive purpose-built training strategies. To tap the potential of text-based models for TKGR in various complex scenarios, we propose ChapTER, a Contrastive historical modeling framework with prefix-tuning for TEmporal Reasoning. ChapTER feeds history-contextualized text into the pseudo-Siamese encoders to strike a textual-temporal balance via cont
    
[^142]: 政策优化在正则化广义和总 LQ 游戏中找到纳什均衡

    Policy Optimization finds Nash Equilibrium in Regularized General-Sum LQ Games

    [https://arxiv.org/abs/2404.00045](https://arxiv.org/abs/2404.00045)

    引入相对熵正则化对一般和总 $N$-agent 游戏的纳什均衡产生影响，证明了NE符合线性高斯策略，并提出了政策优化算法以及增强技术来找到游戏内的NE。

    

    在本文中，我们研究了引入相对熵正则化对一般和总 $N$-agent 游戏的纳什均衡 (NE) 的影响，揭示了这类游戏的NE符合线性高斯策略的事实。此外，它描绘了在熵正则化的适当性方面，对游戏内NE独特性的充分条件。由于政策优化是强化学习 (RL) 技术的基础方法，旨在找到 NE，在这项工作中，我们证明了一个政策优化算法的线性收敛性，该算法 (在熵正则化的适当性下) 能够明显地实现 NE。此外，在熵正则化证明不足的情况下，我们提出了一个 $\delta$-增强技术，有助于实现游戏内的 $\epsilon$-NE。

    arXiv:2404.00045v1 Announce Type: cross  Abstract: In this paper, we investigate the impact of introducing relative entropy regularization on the Nash Equilibria (NE) of General-Sum $N$-agent games, revealing the fact that the NE of such games conform to linear Gaussian policies. Moreover, it delineates sufficient conditions, contingent upon the adequacy of entropy regularization, for the uniqueness of the NE within the game. As Policy Optimization serves as a foundational approach for Reinforcement Learning (RL) techniques aimed at finding the NE, in this work we prove the linear convergence of a policy optimization algorithm which (subject to the adequacy of entropy regularization) is capable of provably attaining the NE. Furthermore, in scenarios where the entropy regularization proves insufficient, we present a $\delta$-augmentation technique, which facilitates the achievement of an $\epsilon$-NE within the game.
    
[^143]: UAlign: 无模板化的非监督式SMILES对齐推动无模板化逆合成预测的极限

    UAlign: Pushing the Limit of Template-free Retrosynthesis Prediction with Unsupervised SMILES Alignment

    [https://arxiv.org/abs/2404.00044](https://arxiv.org/abs/2404.00044)

    本文提出了UAlign，一种无模板化的图到序列的逆合成预测方法，通过结合图神经网络和Transformer，利用分子的固有图结构，并引入一种简单有效的SMILES对齐技术来促进未改变结构的复用。

    

    逆合成规划在有机化工行业中，特别是在制药领域，面临着巨大挑战。单步逆合成预测是规划过程中至关重要的一步，近年来由于科学人工智能的进步，这一步骤引起了人们的浓厚兴趣。近年来已经提出了各种基于深度学习的方法来解决这一问题，其中包括不同程度的额外化学知识依赖。本文介绍了UAlign，这是一种基于图到序列的无模板化逆合成预测管线。通过结合图神经网络和Transformer，我们的方法能够更有效地利用分子的固有图结构。基于分子结构在化学反应过程中保持不变的事实，我们提出了一种简单而有效的SMILES对齐技术，以促进未改变结构的复用以生成反应物。大量实验...

    arXiv:2404.00044v1 Announce Type: cross  Abstract: Retrosynthesis planning poses a formidable challenge in the organic chemical industry, particularly in pharmaceuticals. Single-step retrosynthesis prediction, a crucial step in the planning process, has witnessed a surge in interest in recent years due to advancements in AI for science. Various deep learning-based methods have been proposed for this task in recent years, incorporating diverse levels of additional chemical knowledge dependency. This paper introduces UAlign, a template-free graph-to-sequence pipeline for retrosynthesis prediction. By combining graph neural networks and Transformers, our method can more effectively leverage the inherent graph structure of molecules. Based on the fact that the majority of molecule structures remain unchanged during a chemical reaction, we propose a simple yet effective SMILES alignment technique to facilitate the reuse of unchanged structures for reactant generation. Extensive experiments 
    
[^144]: 具有约束的随机优化：非渐近实例相关分析

    Stochastic Optimization with Constraints: A Non-asymptotic Instance-Dependent Analysis

    [https://arxiv.org/abs/2404.00042](https://arxiv.org/abs/2404.00042)

    该论文研究了具有凸约束的随机凸优化问题，提出了一种非渐近保证的VRPG算法，并展示了其性能受到解以及带凸约束解决的问题的缩放距离控制。

    

    我们考虑了随机凸优化在凸约束下的问题。我们分析了一种适用于这个问题的自然方差减少的近端梯度（VRPG）算法的行为。我们的主要结果是VRPG算法的非渐近保证。与极小值最坏情况保证相反，我们的结果是基于实例的。这意味着我们的保证捕捉了损失函数的复杂性，噪声的变异性和约束集的几何性。我们表明，VRPG算法的非渐近性能受给定问题的解和给定凸约束下解决的特定小扰动问题的解之间的缩放距离（由$\sqrt{N}$缩放）的控制，这里，$N$表示样本数。利用局部极小值下界和扰动问题解之间的一种成熟联系，我们表明当$N \rightarrow +\infty$时，极小值存在并且受指定凸约束的约束。

    arXiv:2404.00042v1 Announce Type: cross  Abstract: We consider the problem of stochastic convex optimization under convex constraints. We analyze the behavior of a natural variance reduced proximal gradient (VRPG) algorithm for this problem. Our main result is a non-asymptotic guarantee for VRPG algorithm. Contrary to minimax worst case guarantees, our result is instance-dependent in nature. This means that our guarantee captures the complexity of the loss function, the variability of the noise, and the geometry of the constraint set. We show that the non-asymptotic performance of the VRPG algorithm is governed by the scaled distance (scaled by $\sqrt{N}$) between the solutions of the given problem and that of a certain small perturbation of the given problem -- both solved under the given convex constraints; here, $N$ denotes the number of samples. Leveraging a well-established connection between local minimax lower bounds and solutions to perturbed problems, we show that as $N \right
    
[^145]: MicroHD：面向TinyML系统的高维超计算算法的精度驱动优化

    MicroHD: An Accuracy-Driven Optimization of Hyperdimensional Computing Algorithms for TinyML systems

    [https://arxiv.org/abs/2404.00039](https://arxiv.org/abs/2404.00039)

    提出了MicroHD，一个新颖的面向TinyML系统的精度驱动超高维计算优化方法

    

    超维计算（HDC）作为一种能够有效定位到TinyML应用的有前途的人工智能方法正在兴起，这要归功于其轻量级的计算和内存需求。在HDC的先前研究中，证明将标准的10k维超维空间限制在更低的值是可行的，进一步降低HDC的资源需求。类似地，其他研究表明二进制值可用作生成的超矢量的元素，可以显著提高效率，代价是某种程度的精度降低。然而，当前的优化尝试没有同时协同优化HDC的超参数，并且精度降低不能直接控制，导致HDC模型不够优化，提供了几个质量不可接受的输出。在这项工作中，我们提出MicroHD，一种新颖的精度驱动HDC优化方法，通过迭代调整HDC的超参数，

    arXiv:2404.00039v1 Announce Type: cross  Abstract: Hyperdimensional computing (HDC) is emerging as a promising AI approach that can effectively target TinyML applications thanks to its lightweight computing and memory requirements. Previous works on HDC showed that limiting the standard 10k dimensions of the hyperdimensional space to much lower values is possible, reducing even more HDC resource requirements. Similarly, other studies demonstrated that binary values can be used as elements of the generated hypervectors, leading to significant efficiency gains at the cost of some degree of accuracy degradation. Nevertheless, current optimization attempts do not concurrently co-optimize HDC hyper-parameters, and accuracy degradation is not directly controlled, resulting in sub-optimal HDC models providing several applications with unacceptable output qualities. In this work, we propose MicroHD, a novel accuracy-driven HDC optimization approach that iteratively tunes HDC hyper-parameters, 
    
[^146]: 人工智能与人类协作中的互补性：概念、来源和证据

    Complementarity in Human-AI Collaboration: Concept, Sources, and Evidence

    [https://arxiv.org/abs/2404.00029](https://arxiv.org/abs/2404.00029)

    本研究建立了全面的理论基础，概念化互补性，并确定了解释人工智能与人类协作中互补团队绩效（CTP）的来源。

    

    人工智能（AI）可以提高各个应用领域中人类决策的能力。理想情况下，人类和AI之间的协作应该导致互补的团队绩效（CTP）--一种既不是他们个体所能达到的绩效水平。然而，迄今为止，很少观察到CTP，这表明对人工智能协作中的互补组分的理解不够，这些组分可以促进决策中的CTP。本研究为理解和发展人工智能互补性奠定了全面的理论基础。我们通过引入和形式化互补性潜力及其实现的概念，概念化互补性。此外，我们确定并概述了解释CTP的来源。我们通过在两项实证研究中应用我们的概念来说明我们的概念。在第一项研究中，我们关注信息不对称

    arXiv:2404.00029v1 Announce Type: cross  Abstract: Artificial intelligence (AI) can improve human decision-making in various application areas. Ideally, collaboration between humans and AI should lead to complementary team performance (CTP) -- a level of performance that neither of them can attain individually. So far, however, CTP has rarely been observed, suggesting an insufficient understanding of the complementary constituents in human-AI collaboration that can contribute to CTP in decision-making. This work establishes a holistic theoretical foundation for understanding and developing human-AI complementarity. We conceptualize complementarity by introducing and formalizing the notion of complementarity potential and its realization. Moreover, we identify and outline sources that explain CTP. We illustrate our conceptualization by applying it in two empirical studies exploring two different sources of complementarity potential. In the first study, we focus on information asymmetry 
    
[^147]: LLM作为写作助手：探讨所有权感和推理的视角

    LLMs as Writing Assistants: Exploring Perspectives on Sense of Ownership and Reasoning

    [https://arxiv.org/abs/2404.00027](https://arxiv.org/abs/2404.00027)

    探讨使用大型语言模型作为写作助手引发的写作所有权感和作者身份认知之间的心理困境。

    

    写作中的所有权感限制了我们对思想、时间和贡献的投入，导致对产出物的依恋。然而，使用写作助手引入了一种心理困境，因为一些内容并非直接我们的创作。我们往往更倾向于在创造性任务中更多地归功于大型语言模型（LLMs），尽管它们对所有任务都是平等的。此外，虽然我们可能不会完全声称对由LLM生成的内容拥有所有权，但却自由地声称作者身份。我们进行了一项简短调查来研究这些问题，并了解潜在的认知过程，以更好地了解人机交互在写作中的应用并改进写作辅助系统。

    arXiv:2404.00027v1 Announce Type: cross  Abstract: Sense of ownership in writing confines our investment of thoughts, time, and contribution, leading to attachment to the output. However, using writing assistants introduces a mental dilemma, as some content isn't directly our creation. For instance, we tend to credit Large Language Models (LLMs) more in creative tasks, even though all tasks are equal for them. Additionally, while we may not claim complete ownership of LLM-generated content, we freely claim authorship. We conduct a short survey to examine these issues and understand underlying cognitive processes in order to gain a better knowledge of human-computer interaction in writing and improve writing aid systems.
    
[^148]: 墨水与个性：在LLMs时代塑造个性化叙事

    Ink and Individuality: Crafting a Personalised Narrative in the Age of LLMs

    [https://arxiv.org/abs/2404.00026](https://arxiv.org/abs/2404.00026)

    研究探讨了人们日益依赖的基于LLM的写作助手对创造力和个性可能造成的负面影响，旨在改进人机交互系统和提升写作助手的个性化和个性化功能。

    

    个性和个性化构成了使每个作家独特并影响其文字以有效吸引读者同时传达真实性的独特特征。然而，我们日益依赖基于LLM的写作助手可能会危及我们的创造力和个性。我们经常忽视这一趋势对我们的创造力和独特性的负面影响，尽管可能会造成后果。本研究通过进行简要调查探索不同的观点和概念，以及尝试理解人们的观点，结合以往在该领域的研究，来研究这些问题。解决这些问题对于改进人机交互系统和增强个性化和个性化写作助手至关重要。

    arXiv:2404.00026v1 Announce Type: cross  Abstract: Individuality and personalization comprise the distinctive characteristics that make each writer unique and influence their words in order to effectively engage readers while conveying authenticity. However, our growing reliance on LLM-based writing assistants risks compromising our creativity and individuality over time. We often overlook the negative impacts of this trend on our creativity and uniqueness, despite the possible consequences. This study investigates these concerns by performing a brief survey to explore different perspectives and concepts, as well as trying to understand people's viewpoints, in conjunction with past studies in the area. Addressing these issues is essential for improving human-computer interaction systems and enhancing writing assistants for personalization and individuality.
    
[^149]: 分析和组织人类沟通以支持人工智能与公平决策相关的决策：来自公共部门的使用案例

    Analysing and Organising Human Communications for AI Fairness-Related Decisions: Use Cases from the Public Sector

    [https://arxiv.org/abs/2404.00022](https://arxiv.org/abs/2404.00022)

    通过调查与公共部门算法系统相关的从业者进行访谈，研究AI公平相关决策的沟通过程，并通过定性编码分析，确定支撑公平决策的关键沟通元素。

    

    在公共部门使用的人工智能算法中，例如分配社会福利或预测欺诈，通常涉及多个公共和私人利益相关者在算法生命周期的各个阶段。这些不同利益相关者之间的沟通问题可能导致算法的误解和误用。我们通过与在公共部门算法系统上工作的从业者进行访谈，调查与AI公平相关决策有关的沟通过程。通过应用定性编码分析，我们确定了支撑与公平相关的人类决策的沟通过程的关键元素。我们分析了利益相关者感知的角色、任务、技能和挑战的分工。我们利用一个概念框架来形式化潜在的沟通问题，其中i.代表了沟通模式 ii.概述了缺失元素，比如缺乏为任务提供技能的角色。该框架被用于

    arXiv:2404.00022v1 Announce Type: cross  Abstract: AI algorithms used in the public sector, e.g., for allocating social benefits or predicting fraud, often involve multiple public and private stakeholders at various phases of the algorithm's life-cycle. Communication issues between these diverse stakeholders can lead to misinterpretation and misuse of algorithms. We investigate the communication processes for AI fairness-related decisions by conducting interviews with practitioners working on algorithmic systems in the public sector. By applying qualitative coding analysis, we identify key elements of communication processes that underlie fairness-related human decisions. We analyze the division of roles, tasks, skills, and challenges perceived by stakeholders. We formalize the underlying communication issues within a conceptual framework that i. represents the communication patterns ii. outlines missing elements, such as actors who miss skills for their tasks. The framework is used fo
    
[^150]: 推进可解释自动驾驶车辆系统：综述与研究路线图

    Advancing Explainable Autonomous Vehicle Systems: A Comprehensive Review and Research Roadmap

    [https://arxiv.org/abs/2404.00019](https://arxiv.org/abs/2404.00019)

    本研究综述了现有自动驾驶车辆解释方法的不确定性，提出了一个全面的未来研究路线图，重点放在了了解交流对象和生成及时解释上。

    

    鉴于现有自动驾驶车辆（AVs）解释方法如何满足利益相关者的多样需求存在不确定性，必须进行深入调查以确定需要解释的情境和适当的互动策略。一项全面的综述至关重要，以评估当前方法与AV生态系统内不同利益和期望的一致性。本研究提供了一项综述，讨论了生成和呈现解释所涉及的复杂性，以促进开发更加有效和包容的可解释AV系统。我们的调查将现有文献分类为三个主要主题：解释任务、解释信息和解释信息传达。根据我们的见解，我们提出了一个未来研究的全面路线图，集中在（i）了解交流对象，（ii）生成及时的解释。

    arXiv:2404.00019v1 Announce Type: cross  Abstract: Given the uncertainty surrounding how existing explainability methods for autonomous vehicles (AVs) meet the diverse needs of stakeholders, a thorough investigation is imperative to determine the contexts requiring explanations and suitable interaction strategies. A comprehensive review becomes crucial to assess the alignment of current approaches with the varied interests and expectations within the AV ecosystem. This study presents a review to discuss the complexities associated with explanation generation and presentation to facilitate the development of more effective and inclusive explainable AV systems. Our investigation led to categorising existing literature into three primary topics: explanatory tasks, explanatory information, and explanatory information communication. Drawing upon our insights, we have proposed a comprehensive roadmap for future research centred on (i) knowing the interlocutor, (ii) generating timely explanat
    
[^151]: 人工智能能否超越人类专家在创建社交媒体创意方面？

    Can AI Outperform Human Experts in Creating Social Media Creatives?

    [https://arxiv.org/abs/2404.00018](https://arxiv.org/abs/2404.00018)

    本文评估了人工智能相对于人类专家在创意领域的表现，并提出了使用大型语言模型增强提示来生成社交媒体创意的新方法。

    

    人工智能在象棋和围棋等功能性任务中已经超越了人类专家。那么在创意任务中呢？本文评估了人工智能在创意领域的能力与人类专家相比，迄今为止很少有研究进行。我们提出了一种新颖的Prompt-for-Prompt方法，通过大型语言模型的提示增强生成社交媒体创意。我们选择了顶级品牌Instagram账号中最受欢迎的帖子（获得最多点赞点击）来创建社交媒体创意。我们给GPT 4提供了几个带有文本描述的提示说明，以生成用于最先进的文本到图像生成器（Midjourney、DALL E 3和Stable Diffusion）的最有效提示。通过LLM增强的提示可以通过添加目标、参与策略、灯光和品牌一致性来提升人工智能在社交媒体图像创作方面的能力。我们进行了大规模的人类评估实验，发现人工智能...

    arXiv:2404.00018v1 Announce Type: cross  Abstract: Artificial Intelligence has outperformed human experts in functional tasks such as chess and baduk. How about creative tasks? This paper evaluates AI's capability in the creative domain compared to human experts, which little research has been conducted so far. We propose a novel Prompt-for-Prompt to generate social media creatives via prompt augmentation by Large Language Models. We take the most popular Instagram posts (with the biggest number of like clicks) in top brands' Instagram accounts to create social media creatives. We give GPT 4 several prompt instructions with text descriptions to generate the most effective prompts for cutting-edge text-to-image generators: Midjourney, DALL E 3, and Stable Diffusion. LLM-augmented prompts can boost AI's abilities by adding objectives, engagement strategy, lighting and brand consistency for social media image creation. We conduct an extensive human evaluation experiment, and find that AI 
    
[^152]: 创新的鹦鹉？评估AI创作的真正新颖性

    Psittacines of Innovation? Assessing the True Novelty of AI Creations

    [https://arxiv.org/abs/2404.00017](https://arxiv.org/abs/2404.00017)

    AI系统在生成项目标题方面展现出独特的创新能力，即使在任务复杂性增加和计算能力极限的情况下，生成的内容具有表面有效性。

    

    我们检验人工智能（AI）系统是否生成真正新颖的想法，而不仅仅是在训练期间学习到的模式。利用一种新颖的实验设计，我们让一个AI生成虚构的众筹活动项目标题。我们比较AI生成的项目标题内部，衡量重复性和复杂性。我们利用一种扩展的最大均值差异方法，在AI生成的标题和实际观测的现场数据之间进行比较，该方法是从将统计分布的核均值嵌入应用到高维机器学习（大语言）嵌入向量中推导出来的--从而得到对AI输出新颖性的结构化分析。结果表明：（1）即使在任务复杂性增加，并且在计算能力的极限下，AI也会生成独特内容，（2）生成的内容具有表面有效性，与送入其他生成AI的输入一致。

    arXiv:2404.00017v1 Announce Type: new  Abstract: We examine whether Artificial Intelligence (AI) systems generate truly novel ideas rather than merely regurgitating patterns learned during training. Utilizing a novel experimental design, we task an AI with generating project titles for hypothetical crowdfunding campaigns. We compare within AI-generated project titles, measuring repetition and complexity. We compare between the AI-generated titles and actual observed field data using an extension of maximum mean discrepancy--a metric derived from the application of kernel mean embeddings of statistical distributions to high-dimensional machine learning (large language) embedding vectors--yielding a structured analysis of AI output novelty. Results suggest that (1) the AI generates unique content even under increasing task complexity, and at the limits of its computational capabilities, (2) the generated content has face validity, being consistent with both inputs to other generative AI 
    
[^153]: 深度几何处理与基于片段的分子三维图生成

    Deep Geometry Handling and Fragment-wise Molecular 3D Graph Generation

    [https://arxiv.org/abs/2404.00014](https://arxiv.org/abs/2404.00014)

    提出了深度几何处理协议，通过引入片段式生成范式解决了共同挑战，开发了一种几何可靠的分子三维图生成方法

    

    大多数早期基于3D结构的分子生成方法遵循逐个原子的范式，在蛋白质口袋内逐步添加原子到部分构建的分子片段中。这些方法在设计紧密结合的配体方面有效，但往往忽视了其他重要性质，如可合成性。片段式生成范式提供了一个有前途的解决方案。然而，原子式和片段式方法都面临一个共同的挑战，即在有限的能力下共同设计出合理的化学和几何结构，导致扭曲的构象。为了应对这一挑战，我们介绍了深度几何处理协议，这是一个更抽象的设计，将设计重点延伸到模型架构之外。通过对现有几何相关模型及其协议的全面审查，我们提出了一种新颖的混合策略，最终开发出FragGen - 一种几何可靠的片段

    arXiv:2404.00014v1 Announce Type: cross  Abstract: Most earlier 3D structure-based molecular generation approaches follow an atom-wise paradigm, incrementally adding atoms to a partially built molecular fragment within protein pockets. These methods, while effective in designing tightly bound ligands, often overlook other essential properties such as synthesizability. The fragment-wise generation paradigm offers a promising solution. However, a common challenge across both atom-wise and fragment-wise methods lies in their limited ability to co-design plausible chemical and geometrical structures, resulting in distorted conformations. In response to this challenge, we introduce the Deep Geometry Handling protocol, a more abstract design that extends the design focus beyond the model architecture. Through a comprehensive review of existing geometry-related models and their protocols, we propose a novel hybrid strategy, culminating in the development of FragGen - a geometry-reliable, frag
    
[^154]: 具有精确语义和AI驱动流程的缺失数据插补及破产预测

    Missing Data Imputation With Granular Semantics and AI-driven Pipeline for Bankruptcy Prediction

    [https://arxiv.org/abs/2404.00013](https://arxiv.org/abs/2404.00013)

    本文介绍了一种具有精确语义的缺失数据插补方法，通过在粒空间中利用特征语义和可靠观测来预测缺失值，从而解决了破产预测中的重要挑战。

    

    本文着重设计了一个用于预测破产的流程。缺失值、高维数据以及高度类别不平衡的数据库是该任务中的主要挑战。本文介绍了一种具有精确语义的缺失数据插补新方法。探讨了粒计算的优点以定义此方法。利用特征语义和可靠观测在低维空间、粒空间中预测缺失值。围绕每个缺失条目形成粒子，考虑到一些高度相关的特征和最可靠的最近观测以保持数据库对缺失条目的相关性和可靠性。然后在这些上下文粒子中进行跨粒子预测进行插补。也就是说，上下文粒子使得在巨大数据库中进行一小部分相关的插补成为可能。

    arXiv:2404.00013v1 Announce Type: cross  Abstract: This work focuses on designing a pipeline for the prediction of bankruptcy. The presence of missing values, high dimensional data, and highly class-imbalance databases are the major challenges in the said task. A new method for missing data imputation with granular semantics has been introduced here. The merits of granular computing have been explored here to define this method. The missing values have been predicted using the feature semantics and reliable observations in a low-dimensional space, in the granular space. The granules are formed around every missing entry, considering a few of the highly correlated features and most reliable closest observations to preserve the relevance and reliability, the context, of the database against the missing entries. An intergranular prediction is then carried out for the imputation within those contextual granules. That is, the contextual granules enable a small relevant fraction of the huge 
    
[^155]: 基于财经新闻情感分析的股票市场压力指数策略改进

    Stress index strategy enhanced with financial news sentiment analysis for the equity markets

    [https://arxiv.org/abs/2404.00012](https://arxiv.org/abs/2404.00012)

    通过将金融压力指标与财经新闻情感分析相结合，提高了股票市场的风险控制策略表现。

    

    本文介绍了一种新的股市风险-风险策略，将金融压力指标与通过ChatGPT读取和解释Bloomberg每日市场摘要进行的情感分析相结合。通过将从波动率和信贷利差推导出的市场压力预测与GPT-4推导出的财经新闻情感相结合，改进了策略的表现，表现为更高的夏普比率和降低的最大回撤。改进的表现在纳斯达克、标普500指数和六个主要股票市场中都保持一致，表明该方法在股票市场中具有普遍适用性。

    arXiv:2404.00012v1 Announce Type: cross  Abstract: This paper introduces a new risk-on risk-off strategy for the stock market, which combines a financial stress indicator with a sentiment analysis done by ChatGPT reading and interpreting Bloomberg daily market summaries. Forecasts of market stress derived from volatility and credit spreads are enhanced when combined with the financial news sentiment derived from GPT-4. As a result, the strategy shows improved performance, evidenced by higher Sharpe ratio and reduced maximum drawdowns. The improved performance is consistent across the NASDAQ, the S&P 500 and the six major equity markets, indicating that the method generalises across equities markets.
    
[^156]: FABind+: 通过改进口袋预测和姿态生成增强分子对接

    FABind+: Enhancing Molecular Docking through Improved Pocket Prediction and Pose Generation

    [https://arxiv.org/abs/2403.20261](https://arxiv.org/abs/2403.20261)

    FABind+通过改进口袋预测和姿态生成，提升分子对接表现

    

    分子对接是药物发现中至关重要的过程。传统技术依赖于受物理原理支配的广泛采样和模拟，但这些方法往往速度慢且昂贵。基于深度学习的方法的出现显示出显著的前景，提供了精确性和效率的增长。建立在FABind的基础工作之上，这是一个专注于速度和准确性的模型，我们提出了FABind+，这是一个大大提升其前身性能的增强版。我们确定口袋预测是分子对接中的一个关键瓶颈，并提出了一种显著改进口袋预测的新方法，从而简化了对接过程。此外，我们对对接模块进行了修改，以增强其姿态生成能力。为了缩小与传统采样/生成方法之间的差距，我们结合了一个简单而有效的s

    arXiv:2403.20261v1 Announce Type: cross  Abstract: Molecular docking is a pivotal process in drug discovery. While traditional techniques rely on extensive sampling and simulation governed by physical principles, these methods are often slow and costly. The advent of deep learning-based approaches has shown significant promise, offering increases in both accuracy and efficiency. Building upon the foundational work of FABind, a model designed with a focus on speed and accuracy, we present FABind+, an enhanced iteration that largely boosts the performance of its predecessor. We identify pocket prediction as a critical bottleneck in molecular docking and propose a novel methodology that significantly refines pocket prediction, thereby streamlining the docking process. Furthermore, we introduce modifications to the docking module to enhance its pose generation capabilities. In an effort to bridge the gap with conventional sampling/generative methods, we incorporate a simple yet effective s
    
[^157]: 稀疏特征电路：在语言模型中发现和编辑可解释的因果图

    Sparse Feature Circuits: Discovering and Editing Interpretable Causal Graphs in Language Models

    [https://arxiv.org/abs/2403.19647](https://arxiv.org/abs/2403.19647)

    该论文介绍了一种新方法，即稀疏特征电路，可以在语言模型中发现和编辑可解释的因果图，为我们提供了对未预料机制的详细理解和包含了用于提高分类器泛化能力的SHIFT方法。

    

    我们介绍了用于发现和应用稀疏特征电路的方法。这些电路是人类可解释特征的因果相关子网络，用于解释语言模型行为。 在先前的工作中确定的电路由多义且难以解释的单元组成，例如注意力头或神经元，使它们不适用于许多下游应用。 相比之下，稀疏特征电路实现了对未预料机制的详细理解。 由于它们基于细粒度单元，稀疏特征电路对下游任务非常有用：我们 introduc了SHIFT，通过切除人类判断为任务不相关的特征，从而提高分类器的泛化能力。 最后，我们通过发现成千上万个稀疏特征电路来展示一个完全无监督且可扩展的可解释性管线，用于自动发现的模型行为。

    arXiv:2403.19647v1 Announce Type: cross  Abstract: We introduce methods for discovering and applying sparse feature circuits. These are causally implicated subnetworks of human-interpretable features for explaining language model behaviors. Circuits identified in prior work consist of polysemantic and difficult-to-interpret units like attention heads or neurons, rendering them unsuitable for many downstream applications. In contrast, sparse feature circuits enable detailed understanding of unanticipated mechanisms. Because they are based on fine-grained units, sparse feature circuits are useful for downstream tasks: We introduce SHIFT, where we improve the generalization of a classifier by ablating features that a human judges to be task-irrelevant. Finally, we demonstrate an entirely unsupervised and scalable interpretability pipeline by discovering thousands of sparse feature circuits for automatically discovered model behaviors.
    
[^158]: 自我改进学习用于可扩展神经组合优化

    Self-Improved Learning for Scalable Neural Combinatorial Optimization

    [https://arxiv.org/abs/2403.19561](https://arxiv.org/abs/2403.19561)

    提出一种新颖的自我改进学习(SIL)方法，实现神经组合优化的更好可扩展性，通过自身生成解决方案作为伪标签，设计线性复杂度的注意机制来处理大规模组合优化问题实例。

    

    end-to-end神经组合优化(NCO)方法在解决复杂组合优化问题方面表现出有希望的性能，而不需要专家设计。然而，现有方法在处理大规模问题时存在困难，限制了它们的实际适用性。为了克服这一限制，本研究提出了一种新颖的自我改进学习(SIL)方法，以实现神经组合优化的更好可扩展性。具体来说，我们开发了一种高效的自我改进机制，使模型能够在没有标记数据的情况下直接在大规模问题实例上进行训练。通过一种创新的局部重构方法，该方法可以通过自身迭代生成更好的解决方案作为伪标签，以指导有效的模型训练。此外，我们设计了一种线性复杂度的注意机制，使模型能够有效处理低计算开销的大规模组合优化问题实例。

    arXiv:2403.19561v1 Announce Type: cross  Abstract: The end-to-end neural combinatorial optimization (NCO) method shows promising performance in solving complex combinatorial optimization problems without the need for expert design. However, existing methods struggle with large-scale problems, hindering their practical applicability. To overcome this limitation, this work proposes a novel Self-Improved Learning (SIL) method for better scalability of neural combinatorial optimization. Specifically, we develop an efficient self-improved mechanism that enables direct model training on large-scale problem instances without any labeled data. Powered by an innovative local reconstruction approach, this method can iteratively generate better solutions by itself as pseudo-labels to guide efficient model training. In addition, we design a linear complexity attention mechanism for the model to efficiently handle large-scale combinatorial problem instances with low computation overhead. Comprehens
    
[^159]: 通过简化不重要的层压缩大型语言模型

    Compressing Large Language Models by Streamlining the Unimportant Layer

    [https://arxiv.org/abs/2403.19135](https://arxiv.org/abs/2403.19135)

    通过观察大型语言模型中不同层对隐藏状态的影响程度，提出了LLM-Streamline方法，包括层剪枝和层替换，用于压缩模型并保持性能。

    

    大型语言模型(LLM)已广泛应用于各种自然语言任务和领域，但其适用性受到模型参数的限制。因此，越来越多的人关注表现出高性能的紧凑模型。在这项研究中，我们观察到LLM的不同层对隐藏状态有不同程度的扰动，这使我们能够识别出不那么重要的层。基于这一现象，我们提出了LLM-Streamline，包括两部分：层剪枝，根据目标稀疏度移除模型中一组连续的最不重要的层；层替换，训练一个轻量级模型来替换被剪枝的层，从而缓解由剪枝造成的性能下降。在实验中，我们利用了多层感知器(MLP)和一个transformer层等结构作为轻量级模型。

    arXiv:2403.19135v1 Announce Type: cross  Abstract: Large language models (LLM) have been extensively applied in various natural language tasks and domains, but their applicability is constrained by the large number of parameters of the models. Consequently, there is an increasing emphasis on compact models that exhibit high performance. In this study, we observe that different layers in LLM have varying degrees of perturbation on the hidden states, which allows us to identify less important layers. Based on this phenomenon, we propose LLM-Streamline, which consists of two parts: layer pruning, where we remove a set of consecutive layers with the lowest importance in the model according to the target sparsity; and layer replacement, where we train a lightweight model to substitute the pruned layers, thereby mitigating the performance degradation caused by pruning. In our experiments, we utilize structures such as a multi-layer perceptron (MLP) and a transformer layer as lightweight mode
    
[^160]: 小型机器学习：进展与未来展望

    Tiny Machine Learning: Progress and Futures

    [https://arxiv.org/abs/2403.19076](https://arxiv.org/abs/2403.19076)

    TinyML是一种将深度学习模型压缩到物联网设备和微控制器中实现无处不在智能的新方法，需要共同设计算法和系统堆栈以克服硬件限制。

    

    Tiny Machine Learning（TinyML）是机器学习的一个新领域。通过将深度学习模型压缩到数十亿个物联网设备和微控制器（MCUs）中，我们扩展了人工智能应用的范围，实现了无处不在的智能。然而，由于硬件限制，TinyML具有挑战性：有限的内存资源使得难以容纳为云和移动平台设计的深度学习模型。对于裸机设备，编译器和推断引擎支持也有限。因此，我们需要共同设计算法和系统堆栈以实现TinyML。

    arXiv:2403.19076v1 Announce Type: cross  Abstract: Tiny Machine Learning (TinyML) is a new frontier of machine learning. By squeezing deep learning models into billions of IoT devices and microcontrollers (MCUs), we expand the scope of AI applications and enable ubiquitous intelligence. However, TinyML is challenging due to hardware constraints: the tiny memory resource makes it difficult to hold deep learning models designed for cloud and mobile platforms. There is also limited compiler and inference engine support for bare-metal devices. Therefore, we need to co-design the algorithm and system stack to enable TinyML. In this review, we will first discuss the definition, challenges, and applications of TinyML. We then survey the recent progress in TinyML and deep learning on MCUs. Next, we will introduce MCUNet, showing how we can achieve ImageNet-scale AI applications on IoT devices with system-algorithm co-design. We will further extend the solution from inference to training and in
    
[^161]: 跨领域的纤维簇形状分析用于语言表现认知分数预测

    Cross--domain Fiber Cluster Shape Analysis for Language Performance Cognitive Score Prediction

    [https://arxiv.org/abs/2403.19001](https://arxiv.org/abs/2403.19001)

    本研究通过新颖的框架SFFormer，结合了多头交叉注意力特征融合模块，基于dMRI纤维束追踪，预测了主观语言表现，拓展了脑结构与人类认知功能的关联研究。

    

    形状在计算机图形学中扮演重要角色，提供了有关对象形态和功能的信息特征。脑成像中的形状分析可帮助解释人脑结构和功能的相关性。本研究调查了大脑的3D白质连接的形状及其与人类认知功能的潜在预测关系。我们使用扩散磁共振成像（dMRI）纤维束追踪将大脑连接重建为3D点序列。为了描述每个连接，我们提取了12个形状描述符以及传统的dMRI连接和组织微结构特征。我们引入了一种新颖的框架，形状融合纤维簇变换器（SFFormer），利用多头交叉注意力特征融合模块基于dMRI纤维束追踪来预测特定个体的语言表现。我们在一个大型数据集上评估了该方法的性能。

    arXiv:2403.19001v1 Announce Type: cross  Abstract: Shape plays an important role in computer graphics, offering informative features to convey an object's morphology and functionality. Shape analysis in brain imaging can help interpret structural and functionality correlations of the human brain. In this work, we investigate the shape of the brain's 3D white matter connections and its potential predictive relationship to human cognitive function. We reconstruct brain connections as sequences of 3D points using diffusion magnetic resonance imaging (dMRI) tractography. To describe each connection, we extract 12 shape descriptors in addition to traditional dMRI connectivity and tissue microstructure features. We introduce a novel framework, Shape--fused Fiber Cluster Transformer (SFFormer), that leverages a multi-head cross-attention feature fusion module to predict subject-specific language performance based on dMRI tractography. We assess the performance of the method on a large dataset
    
[^162]: 微服务系统的少样本跨系统异常跟踪分类

    Few-Shot Cross-System Anomaly Trace Classification for Microservice-based systems

    [https://arxiv.org/abs/2403.18998](https://arxiv.org/abs/2403.18998)

    提出了针对微服务系统的少样本异常跟踪分类的新框架，利用多头注意力自编码器构建系统特定的跟踪表示，并应用基于Transformer编码器的模型无关元学习进行高效分类。

    

    微服务系统（MSS）由于其复杂和动态的特性可能在各种故障类别中出现故障。为了有效处理故障，AIOps工具利用基于跟踪的异常检测和根本原因分析。本文提出了一个新颖的框架，用于微服务系统的少样本异常跟踪分类。我们的框架包括两个主要组成部分：（1）多头注意力自编码器用于构建系统特定的跟踪表示，从而实现（2）基于Transformer编码器的模型无关元学习，以进行有效和高效的少样本异常跟踪分类。该框架在两个代表性的MSS，Trainticket和OnlineBoutique上进行了评估，使用开放数据集。结果表明，我们的框架能够调整学到的知识，以对新的、未见的新颖故障类别的异常跟踪进行分类，无论是在最初训练的同一系统内，还是在其他系统中。

    arXiv:2403.18998v1 Announce Type: cross  Abstract: Microservice-based systems (MSS) may experience failures in various fault categories due to their complex and dynamic nature. To effectively handle failures, AIOps tools utilize trace-based anomaly detection and root cause analysis. In this paper, we propose a novel framework for few-shot abnormal trace classification for MSS. Our framework comprises two main components: (1) Multi-Head Attention Autoencoder for constructing system-specific trace representations, which enables (2) Transformer Encoder-based Model-Agnostic Meta-Learning to perform effective and efficient few-shot learning for abnormal trace classification. The proposed framework is evaluated on two representative MSS, Trainticket and OnlineBoutique, with open datasets. The results show that our framework can adapt the learned knowledge to classify new, unseen abnormal traces of novel fault categories both within the same system it was initially trained on and even in the 
    
[^163]: 中国 offensive 语言检测：现状与未来方向

    Chinese Offensive Language Detection:Current Status and Future Directions

    [https://arxiv.org/abs/2403.18314](https://arxiv.org/abs/2403.18314)

    总体而言，这篇论文讨论了在中文中检测 offensive 语言的挑战，并强调了开发解决这一问题的特定模型和工具。

    

    虽然社交媒体平台正在做出相当大的努力监测和规范用户生成内容，但在数字空间中，恶意语言（如仇恨言论或网络欺凌）的普遍存在仍然是一个重要挑战。鉴于维护文明和尊重的在线环境的重要性，迫切需要能够实时检测恶意言论的自动系统。然而，为了开发处理汉语等语言的有效系统，面临着重大挑战，因为这些语言的复杂和微妙性使得自动处理变得困难。本文全面总结了中国 offensive 语言检测情况，审查了当前的基准和方法，并重点介绍了用于解决在这种复杂语言中检测恶意语言的独特挑战的特定模型和工具。

    arXiv:2403.18314v1 Announce Type: cross  Abstract: Despite the considerable efforts being made to monitor and regulate user-generated content on social media platforms, the pervasiveness of offensive language, such as hate speech or cyberbullying, in the digital space remains a significant challenge. Given the importance of maintaining a civilized and respectful online environment, there is an urgent and growing need for automatic systems capable of detecting offensive speech in real time. However, developing effective systems for processing languages such as Chinese presents a significant challenge, owing to the language's complex and nuanced nature, which makes it difficult to process automatically. This paper provides a comprehensive overview of offensive language detection in Chinese, examining current benchmarks and approaches and highlighting specific models and tools for addressing the unique challenges of detecting offensive language in this complex language. The primary object
    
[^164]: 解决情感与文化智能人工智能研讨会情感预测竞赛的方案

    Solution for Emotion Prediction Competition of Workshop on Emotionally and Culturally Intelligent AI

    [https://arxiv.org/abs/2403.17683](https://arxiv.org/abs/2403.17683)

    提出了一种名为ECSP的单多模态情感文化特定提示方法，旨在通过使用单一模态消息增强多模态模型性能，并设计良好的提示来减少文化差异问题。

    

    这篇报告提供了我们在WECIA情感预测竞赛（EPC）中探索和提出的方法的详细描述，该方法通过对带有评论的艺术作品预测一个人的情感。该竞赛的数据集是ArtELingo，旨在鼓励跨语言和文化的多样化工作。该数据集主要面临两个挑战，即模态不平衡问题和语言文化差异问题。为了解决这个问题，我们提出了一种简单而有效的方法，称为单多模态情感文化特定提示（ECSP），其重点是使用单一模态消息来增强多模态模型的性能，并设计良好的提示来减少文化差异问题。澄清一下，我们的方法包含两个主要块：（1）基于XLM-R的单模态模型和基于X$^2$-VLM的多模态模型（2）情感文化特定提示。

    arXiv:2403.17683v1 Announce Type: new  Abstract: This report provide a detailed description of the method that we explored and proposed in the WECIA Emotion Prediction Competition (EPC), which predicts a person's emotion through an artistic work with a comment. The dataset of this competition is ArtELingo, designed to encourage work on diversity across languages and cultures. The dataset has two main challenges, namely modal imbalance problem and language-cultural differences problem. In order to address this issue, we propose a simple yet effective approach called single-multi modal with Emotion-Cultural specific prompt(ECSP), which focuses on using the single modal message to enhance the performance of multimodal models and a well-designed prompt to reduce cultural differences problem. To clarify, our approach contains two main blocks: (1)XLM-R\cite{conneau2019unsupervised} based unimodal model and X$^2$-VLM\cite{zeng2022x} based multimodal model (2) Emotion-Cultural specific prompt.
    
[^165]: 使用区域指导标记将孟加拉文本与地方方言转录为国际音标

    Transcribing Bengali Text with Regional Dialects to IPA using District Guided Tokens

    [https://arxiv.org/abs/2403.17407](https://arxiv.org/abs/2403.17407)

    通过引入区域指导标记技术，本文提出了一种将孟加拉文本与地方方言转录为国际音标的方法，为模型提供了关于输入文本的地区方言信息，以理解与每个地区相关的独特音韵模式。

    

    孟加拉文本到国际音标（IPA）的准确转录是一项具有挑战性的任务，主要是由于语言的复杂音韵学和语境相关的音变。对于区域孟加拉方言来说，由于缺乏针对这些方言的标准拼写约定、当地和外语在这些地区中流行的词汇以及不同地区之间的音韵多样性，这一挑战甚至更为严峻。本文提出了一种方法来解决这个序列到序列的问题，即在覆盖孟加拉国六个地区的新数据集上引入“区域指导标记”（DGT）技术。其关键思想是在生成IPA转录之前向模型提供有关输入文本的区域方言或“地区”的明确信息。这通过在输入序列前添加一个地区标记来实现，有效地引导模型理解与每个地区相关的独特音韵模式。

    arXiv:2403.17407v1 Announce Type: cross  Abstract: Accurate transcription of Bengali text to the International Phonetic Alphabet (IPA) is a challenging task due to the complex phonology of the language and context-dependent sound changes. This challenge is even more for regional Bengali dialects due to unavailability of standardized spelling conventions for these dialects, presence of local and foreign words popular in those regions and phonological diversity across different regions. This paper presents an approach to this sequence-to-sequence problem by introducing the District Guided Tokens (DGT) technique on a new dataset spanning six districts of Bangladesh. The key idea is to provide the model with explicit information about the regional dialect or "district" of the input text before generating the IPA transcription. This is achieved by prepending a district token to the input sequence, effectively guiding the model to understand the unique phonetic patterns associated with each 
    
[^166]: 视觉幻觉：定义、量化和处方修复

    Visual Hallucination: Definition, Quantification, and Prescriptive Remediations

    [https://arxiv.org/abs/2403.17306](https://arxiv.org/abs/2403.17306)

    本文提出了关于视觉-语言模型中幻觉的细致讨论，对幻觉进行了量化，并提供了一个新的公开数据集VHILT，有助于研究此问题。

    

    引发幻觉的不断增加，或许是对负责任人工智能进展的最显著障碍。最近，相当多的研究侧重于检测和缓解大型语言模型（LLMs）中的幻觉。然而，值得注意的是，视觉-语言模型（VLMs）中幻觉也相当普遍。在本文中，我们提供了一个关于基于图像字幕和视觉问答两个任务的VLM幻觉剖析的细致讨论。我们阐明了八个细致的视觉幻觉取向：i) 上下文猜测，ii) 身份不一致，iii) 地理错误，iv) 视觉幻觉，v) 性别异常，vi) VLM作为分类器，vii) 错误阅读，和viii) 数字差异。我们策划了一份名为视觉幻觉诱发（VHILT）的公开数据集，包含了通过两个任务（字幕和VQA）生成的来自八个VLM的2,000个样本。

    arXiv:2403.17306v1 Announce Type: new  Abstract: The troubling rise of hallucination presents perhaps the most significant impediment to the advancement of responsible AI. In recent times, considerable research has focused on detecting and mitigating hallucination in Large Language Models (LLMs). However, it's worth noting that hallucination is also quite prevalent in Vision-Language models (VLMs). In this paper, we offer a fine-grained discourse on profiling VLM hallucination based on two tasks: i) image captioning, and ii) Visual Question Answering (VQA). We delineate eight fine-grained orientations of visual hallucination: i) Contextual Guessing, ii) Identity Incongruity, iii) Geographical Erratum, iv) Visual Illusion, v) Gender Anomaly, vi) VLM as Classifier, vii) Wrong Reading, and viii) Numeric Discrepancy. We curate Visual HallucInation eLiciTation (VHILT), a publicly available dataset comprising 2,000 samples generated using eight VLMs across two tasks of captioning and VQA alo
    
[^167]: 一种用于电力价格预测的Transformer方法

    A Transformer approach for Electricity Price Forecasting

    [https://arxiv.org/abs/2403.16108](https://arxiv.org/abs/2403.16108)

    这种独特的Transformer模型在电力价格预测中取得了更好的表现，为可靠和可持续的电力系统运行提供了有前景的解决方案。

    

    本文提出了一种使用纯Transformer模型进行电力价格预测（EPF）的新方法。与其他方法不同，没有使用其他递归网络结合注意力机制。因此，表明注意力层足以捕捉时间模式。该论文还通过使用开源EPF工具进行了对模型的公平比较，并提供了代码以增强EPF研究的可再现性和透明度。结果表明，Transformer模型优于传统方法，为可靠和可持续的电力系统运行提供了一种有希望的解决方案。

    arXiv:2403.16108v1 Announce Type: cross  Abstract: This paper presents a novel approach to electricity price forecasting (EPF) using a pure Transformer model. As opposed to other alternatives, no other recurrent network is used in combination to the attention mechanism. Hence, showing that the attention layer is enough for capturing the temporal patterns. The paper also provides fair comparison of the models using the open-source EPF toolbox and provide the code to enhance reproducibility and transparency in EPF research. The results show that the Transformer model outperforms traditional methods, offering a promising solution for reliable and sustainable power system operation.
    
[^168]: 在干草堆中寻找针: 一种透明水印检测的黑盒方法

    Finding needles in a haystack: A Black-Box Approach to Invisible Watermark Detection

    [https://arxiv.org/abs/2403.15955](https://arxiv.org/abs/2403.15955)

    提出了一种透明水印检测的黑盒方法WMD，在无注释设置下，利用干净无水印数据集检测任意水印，效果显著优于传统方法

    

    在本文中，我们提出了WaterMark Detection（WMD），这是第一个在黑盒和无注释设置下进行透明水印检测的方法。WMD能够利用一个干净的无水印数据集作为参考，在不依赖特定解码方法或对水印技术的事先了解的情况下，检测给定参考数据集中的任意水印。我们使用偏移学习的基础开发了WMD，干净的无水印数据集使我们能够仅分离出参考数据集中带水印样本的影响。我们进行了全面的评估，证明了WMD的有效性，明显优于仅产生约0.5的AUC得分的简单检测方法。相比之下，WMD在大多数单水印数据集中持续获得令人印象深刻的检测AUC得分，超过0.9，并在更具挑战性的多水印场景中的各种数据集和水印方法中超过0.7。

    arXiv:2403.15955v1 Announce Type: cross  Abstract: In this paper, we propose WaterMark Detection (WMD), the first invisible watermark detection method under a black-box and annotation-free setting. WMD is capable of detecting arbitrary watermarks within a given reference dataset using a clean non-watermarked dataset as a reference, without relying on specific decoding methods or prior knowledge of the watermarking techniques. We develop WMD using foundations of offset learning, where a clean non-watermarked dataset enables us to isolate the influence of only watermarked samples in the reference dataset. Our comprehensive evaluations demonstrate the effectiveness of WMD, significantly outperforming naive detection methods, which only yield AUC scores around 0.5. In contrast, WMD consistently achieves impressive detection AUC scores, surpassing 0.9 in most single-watermark datasets and exceeding 0.7 in more challenging multi-watermark scenarios across diverse datasets and watermarking me
    
[^169]: 从损失角度理解语言模型的突现能力

    Understanding Emergent Abilities of Language Models from the Loss Perspective

    [https://arxiv.org/abs/2403.15796](https://arxiv.org/abs/2403.15796)

    本文从损失角度重新定义了语言模型的突现能力，发现具有相同预训练损失的模型在不同任务上表现相似，而当预训练损失低于特定阈值时，模型将展现出突现能力。

    

    近期研究质疑了传统认为语言模型的突现能力仅存在于大模型中的观点。这种怀疑源自两点观察：1）较小的模型也能展现出对突现能力的高性能；2）质疑用于测量这些能力的不连续性指标。本文提议从预训练损失的角度研究突现能力，而非模型大小或训练计算。我们展示了具有相同预训练损失但不同模型和数据大小的模型，在各种下游任务上表现相同。我们还发现，当某一模型的预训练损失低于特定阈值时，在某些任务上表现出突现能力，而不论指标的连续性如何；而在达到该阈值之前，其性能仍保持在随机猜测水平。这启发我们重新定义突现能力为那些......

    arXiv:2403.15796v1 Announce Type: cross  Abstract: Recent studies have put into question the belief that emergent abilities in language models are exclusive to large models. This skepticism arises from two observations: 1) smaller models can also exhibit high performance on emergent abilities and 2) there is doubt on the discontinuous metrics used to measure these abilities. In this paper, we propose to study emergent abilities in the lens of pre-training loss, instead of model size or training compute. We demonstrate that the models with the same pre-training loss, but different model and data sizes, generate the same performance on various downstream tasks. We also discover that a model exhibits emergent abilities on certain tasks -- regardless of the continuity of metrics -- when its pre-training loss falls below a specific threshold. Before reaching this threshold, its performance remains at the level of random guessing. This inspires us to redefine emergent abilities as those that
    
[^170]: 引入一种集成方法，通过分析PET扫描图像早期检测阿尔茨海默病

    Introducing an ensemble method for the early detection of Alzheimer's disease through the analysis of PET scan images

    [https://arxiv.org/abs/2403.15443](https://arxiv.org/abs/2403.15443)

    通过分析PET扫描图像，引入了一种集成方法早期检测阿尔茨海默病，并且在分类阿尔茨海默病时使用了多种深度学习和传统机器学习模型。

    

    阿尔茨海默病是一种逐渐恶化的神经退行性疾病，主要影响记忆、思维和行为等认知功能。本病存在一个关键阶段，即轻度认知障碍，非常重要尽早诊断，因为一些逐渐发展为病症的MCI患者会发展为这种疾病。本研究探讨了将阿尔茨海默病分类为四个不同组：控制正常（CN）、逐渐发展的轻度认知障碍（pMCI）、稳定的轻度认知障碍（sMCI）和阿尔茨海默病（AD）的具有挑战性的任务。这种分类是基于对从ADNI数据集获得的PET扫描图像的彻底检查，这提供了对疾病进展的彻底理解。已经使用了几种深度学习和传统机器学习模型来检测阿尔茨海默病。在本文中，使用了三种深度学习模型，即VGG16、AlexNet和自定义的卷积神经网络。

    arXiv:2403.15443v1 Announce Type: cross  Abstract: Alzheimer's disease is a progressive neurodegenerative disorder that primarily affects cognitive functions such as memory, thinking, and behavior. In this disease, there is a critical phase, mild cognitive impairment, that is really important to be diagnosed early since some patients with progressive MCI will develop the disease. This study delves into the challenging task of classifying Alzheimer's disease into four distinct groups: control normal (CN), progressive mild cognitive impairment (pMCI), stable mild cognitive impairment (sMCI), and Alzheimer's disease (AD). This classification is based on a thorough examination of PET scan images obtained from the ADNI dataset, which provides a thorough understanding of the disease's progression. Several deep-learning and traditional machine-learning models have been used to detect Alzheimer's disease. In this paper, three deep-learning models, namely VGG16 and AlexNet, and a custom Convolu
    
[^171]: LLaVA-PruMerge: 自适应令牌减少用于高效大型多模态模型

    LLaVA-PruMerge: Adaptive Token Reduction for Efficient Large Multimodal Models

    [https://arxiv.org/abs/2403.15388](https://arxiv.org/abs/2403.15388)

    PruMerge提出了一种自适应的视觉令牌减少方法，可以有效减少大型多模态模型中的视觉令牌数量，同时保持模型性能。

    

    大型多模态模型(LMMs)通过连接视觉编码器和大型语言模型展现了显著的推理能力。最近的LMMs包括了更复杂的视觉输入，如高分辨率图像和视频，这显著增加了视觉令牌的数量。为了解决这个问题，我们探索了一种令牌减少机制，并发现类似于先前的工作，许多视觉令牌在空间上是冗余的。基于此，我们提出了PruMerge，一种新颖的自适应视觉令牌减少方法，大大减少了视觉令牌的数量，同时保持了可比的模型性能。

    arXiv:2403.15388v1 Announce Type: cross  Abstract: Large Multimodal Models (LMMs) have shown significant reasoning capabilities by connecting a visual encoder and a large language model. LMMs typically use a fixed amount of visual tokens, such as the penultimate layer features in the CLIP visual encoder, as the prefix content. Recent LMMs incorporate more complex visual inputs, such as high-resolution images and videos, which increase the number of visual tokens significantly. However, due to the design of the Transformer architecture, computational costs associated with these models tend to increase quadratically with the number of input tokens. To tackle this problem, we explore a token reduction mechanism and find, similar to prior work, that many visual tokens are spatially redundant. Based on this, we propose PruMerge, a novel adaptive visual token reduction approach, which largely reduces the number of visual tokens while maintaining comparable model performance. We first select 
    
[^172]: 大型语言模型和用户信任：以医疗为重点

    Large Language Models and User Trust: Focus on Healthcare

    [https://arxiv.org/abs/2403.14691](https://arxiv.org/abs/2403.14691)

    本文探讨了临床医生对LLMs的信任、数据来源从主要是人类生成到人工智能生成内容的转变，以及随之而来对LLMs精确性和临床医师能力的影响之间的不断发展的关系，强调了LLMs在医疗保健中的整合加深可能带来的挑战和风险。

    

    本文探讨了临床医生对LLMs的信任、数据来源从主要是人类生成到人工智能生成内容的转变，以及随之而来对LLMs精确性和临床医师能力的影响之间的不断发展的关系。其中一个主要问题是随着LLMs在学习过程中越来越依赖它们的输出，可能会导致输出质量下降，并导致临床医师技能减少，因为他们与基本诊断过程的参与减少。尽管目前还处于理论阶段，但这种反馈循环构成了一个重大挑战，因为LLMs在医疗保健中的整合加深，强调了需要积极对话和战略措施，以确保LLM技术的安全有效使用。此外，我们深入探讨了与LLMs自我参考学习循环以及医疗保健专业人员技能下降相关的潜在风险。

    arXiv:2403.14691v1 Announce Type: cross  Abstract: This paper explores the evolving relationship between clinician trust in LLMs, the transformation of data sources from predominantly human-generated to AI-generated content, and the subsequent impact on the precision of LLMs and clinician competence. One of the primary concerns identified is the potential feedback loop that arises as LLMs become more reliant on their outputs for learning, which may lead to a degradation in output quality and a reduction in clinician skills due to decreased engagement with fundamental diagnostic processes. While theoretical at this stage, this feedback loop poses a significant challenge as the integration of LLMs in healthcare deepens, emphasizing the need for proactive dialogue and strategic measures to ensure the safe and effective use of LLM technology. Moreover, we delve into the potential risks associated with LLMs' self-referential learning loops and the deskilling of healthcare professionals. The
    
[^173]: ReAct遇上ActRe：对比性自训练中的代理轨迹自动标注

    ReAct Meets ActRe: Autonomous Annotations of Agent Trajectories for Contrastive Self-Training

    [https://arxiv.org/abs/2403.14589](https://arxiv.org/abs/2403.14589)

    提出了A$^3$T框架，通过ActRe提示代理实现了ReAct风格代理对代理轨迹的自主标注，同时增强了新的轨迹合成能力。

    

    arXiv:2403.14589v1 公告类型：新 文摘：语言代理通过与基础模型推理展示了自主决策能力。最近，人们致力于通过多步推理和行动轨迹作为训练数据来训练语言代理以提高性能。然而，收集这样的轨迹仍需要相当大的人力，无论是通过人工标注还是实施多样化提示框架。在这项工作中，我们提出了A$^3$T，一个允许以ReAct风格自主注释代理轨迹的框架。其中心是一个ActRe提示代理，它解释任意动作的原因。当随机抽取外部动作时，ReAct风格代理可以查询ActRe代理以获取其文本理由。新颖的轨迹然后通过将ActRe的后验推理前置到抽样动作中进行综合合成。通过这种方式，ReAct风格代理可执行

    arXiv:2403.14589v1 Announce Type: new  Abstract: Language agents have demonstrated autonomous decision-making abilities by reasoning with foundation models. Recently, efforts have been made to train language agents for performance improvement, with multi-step reasoning and action trajectories as the training data. However, collecting such trajectories still requires considerable human effort, by either artificial annotations or implementations of diverse prompting frameworks. In this work, we propose A$^3$T, a framework that enables the Autonomous Annotation of Agent Trajectories in the style of ReAct. The central role is an ActRe prompting agent, which explains the reason for an arbitrary action. When randomly sampling an external action, the ReAct-style agent could query the ActRe agent with the action to obtain its textual rationales. Novel trajectories are then synthesized by prepending the posterior reasoning from ActRe to the sampled action. In this way, the ReAct-style agent exe
    
[^174]: C-TPT：通过文本特征离散性的校准测试时提示调整视觉-语言模型

    C-TPT: Calibrated Test-Time Prompt Tuning for Vision-Language Models via Text Feature Dispersion

    [https://arxiv.org/abs/2403.14119](https://arxiv.org/abs/2403.14119)

    本文研究了在测试时提示调整过程中通过利用CLIP的固有属性来探讨校准的方法，发现提示选择显著影响了CLIP中的校准，其中导致更高文本特征离散性的提示会产生更好校准的预测。

    

    在深度学习中，测试时适应已经引起了人们的关注，作为一种在不需要标记数据的情况下对模型进行微调的方法。一个主要的例证是最近提出的用于大规模视觉-语言模型（如CLIP）的测试时提示调整。然而，这些提示主要是为了提高准确性而开发的，忽视了校准的重要性——量化预测不确定性的关键方面。然而，传统的校准方法依赖大量标记数据，这使得它们在测试时场景下不切实际。为此，本文通过利用CLIP的固有属性，在测试时提示调整过程中探讨校准。通过一系列观察，我们发现提示选择显著影响了CLIP中的校准，其中导致更高文本特征离散性的提示会产生更好校准的预测。

    arXiv:2403.14119v1 Announce Type: cross  Abstract: In deep learning, test-time adaptation has gained attention as a method for model fine-tuning without the need for labeled data. A prime exemplification is the recently proposed test-time prompt tuning for large-scale vision-language models such as CLIP. Unfortunately, these prompts have been mainly developed to improve accuracy, overlooking the importance of calibration-a crucial aspect for quantifying prediction uncertainty. However, traditional calibration methods rely on substantial amounts of labeled data, making them impractical for test-time scenarios. To this end, this paper explores calibration during test-time prompt tuning by leveraging the inherent properties of CLIP. Through a series of observations, we find that the prompt choice significantly affects the calibration in CLIP, where the prompts leading to higher text feature dispersion result in better-calibrated predictions. Introducing the Average Text Feature Dispersion
    
[^175]: ZigMa：蜿蜒曼巴扩散模型

    ZigMa: Zigzag Mamba Diffusion Model

    [https://arxiv.org/abs/2403.13802](https://arxiv.org/abs/2403.13802)

    本研究提出了一种名为Zigzag Mamba的零参数方法，通过纠正当前Mamba-based视觉方法中对空间连续性的忽视，实现了更好的速度和内存利用，同时在大分辨率视觉数据集上展示了出色的性能。

    

    扩散模型长期以来一直受到可伸缩性和二次复杂性问题的困扰，特别是在基于变压器的结构内部。在这项研究中，我们旨在利用一种称为曼巴的状态空间模型的长序列建模能力，以扩展其在视觉数据生成中的适用性。首先，我们确定了大多数当前基于曼巴的视觉方法中的一个关键疏忽，即曼巴的扫描方案中缺乏对空间连续性的考虑。其次，基于这一洞察力，我们介绍了一种名为Zigzag Mamba的简单、即插即用、零参数方法，它优于基于曼巴的基线，并表现出比基于变压器的基线更快速和更好的内存利用。最后，我们将Zigzag Mamba集成到随机插值框架中，以研究模型在大分辨率视觉数据集（例如FacesHQ $1024\times 1024$和UCF101，MultiModal-CelebA-HQ）上的可伸缩性。

    arXiv:2403.13802v1 Announce Type: cross  Abstract: The diffusion model has long been plagued by scalability and quadratic complexity issues, especially within transformer-based structures. In this study, we aim to leverage the long sequence modeling capability of a State-Space Model called Mamba to extend its applicability to visual data generation. Firstly, we identify a critical oversight in most current Mamba-based vision methods, namely the lack of consideration for spatial continuity in the scan scheme of Mamba. Secondly, building upon this insight, we introduce a simple, plug-and-play, zero-parameter method named Zigzag Mamba, which outperforms Mamba-based baselines and demonstrates improved speed and memory utilization compared to transformer-based baselines. Lastly, we integrate Zigzag Mamba with the Stochastic Interpolant framework to investigate the scalability of the model on large-resolution visual datasets, such as FacesHQ $1024\times 1024$ and UCF101, MultiModal-CelebA-HQ
    
[^176]: 基于缓存增强的终身多智能体路径规划

    Caching-Augmented Lifelong Multi-Agent Path Finding

    [https://arxiv.org/abs/2403.13421](https://arxiv.org/abs/2403.13421)

    CAL-MAPF引入了缓存机制来增强终身多智能体路径规划的性能，通过合适的输入任务分布、高缓存命中率和流畅的交通这三个因素显著提高了解决方案的稳定性。

    

    多智能体路径规划 (Multi-Agent Path Finding，MAPF) 在各种应用中至关重要，涉及为多个机器人找到无碰撞路径。终身MAPF会在代理完成其初始目标后立即将目标重新分配给代理，提供对实际仓库规划的更准确逼近。本文提出了一种名为缓存增强终身MAPF (CAL-MAPF)的新机制，旨在提高终身MAPF的性能。我们开发了一种称为缓存的临时物品存储和替换的新类型地图网格，并为其设计了一种锁定机制，以提高规划解决方案的稳定性。该缓存机制经过各种缓存替换策略和一系列输入任务分布的评估。我们通过实验确定了三个显著影响CAL-MAPF性能的主要因素：合适的输入任务分布、高缓存命中率和流畅的交通。

    arXiv:2403.13421v2 Announce Type: replace-cross  Abstract: Multi-Agent Path Finding (MAPF), which involves finding collision-free paths for multiple robots, is crucial in various applications. Lifelong MAPF, where targets are reassigned to agents as soon as they complete their initial targets, offers a more accurate approximation of real-world warehouse planning. In this paper, we present a novel mechanism named Caching-Augmented Lifelong MAPF (CAL-MAPF), designed to improve the performance of Lifelong MAPF. We have developed a new type of map grid called cache for temporary item storage and replacement, and designed a locking mechanism for it to improve the stability of the planning solution. This cache mechanism was evaluated using various cache replacement policies and a spectrum of input task distributions. We identified three main factors significantly impacting CAL-MAPF performance through experimentation: suitable input task distribution, high cache hit rate, and smooth traffic.
    
[^177]: 利用大型语言模型和真实机器人账户激励社交媒体平台上的新闻消费

    Incentivizing News Consumption on Social Media Platforms Using Large Language Models and Realistic Bot Accounts

    [https://arxiv.org/abs/2403.13362](https://arxiv.org/abs/2403.13362)

    通过创建使用 GPT-2 的机器人账户，在社交媒体平台上回复用户的推文，鼓励用户接触和关注验证的、意识形态平衡的新闻，以增加用户接触这些新闻并提高参与度。

    

    极化、信任下降以及对民主规范支持动摇是美国民主面临的紧迫威胁。接触验证和优质新闻可能降低个人对这些威胁的易感性，并使公民更具抗击错误信息、民粹主义和极端党派言论的能力。该项目探讨了如何在一个生态有效的环境中增强用户接触和参与验证的、意识形态平衡的新闻。我们依赖于对 28,457 个 Twitter 用户进行的大规模为期两周的田野实验（从 2023 年 1 月 19 日到 2 月 3 日）。我们创建了 28 个利用 GPT-2 的机器人，在用户发表有关体育、娱乐或生活方式的推文时回复一个内容相关的回复，其中包含两个硬代码元素：一个指向优质新闻机构相关主题部分的 URL 和鼓励关注其 Twitter 账户。为进一步测试机器人对性别的差异影响，被试用户被随机分配以接受...

    arXiv:2403.13362v1 Announce Type: cross  Abstract: Polarization, declining trust, and wavering support for democratic norms are pressing threats to U.S. democracy. Exposure to verified and quality news may lower individual susceptibility to these threats and make citizens more resilient to misinformation, populism, and hyperpartisan rhetoric. This project examines how to enhance users' exposure to and engagement with verified and ideologically balanced news in an ecologically valid setting. We rely on a large-scale two-week long field experiment (from 1/19/2023 to 2/3/2023) on 28,457 Twitter users. We created 28 bots utilizing GPT-2 that replied to users tweeting about sports, entertainment, or lifestyle with a contextual reply containing two hardcoded elements: a URL to the topic-relevant section of quality news organization and an encouragement to follow its Twitter account. To further test differential effects by gender of the bots, treated users were randomly assigned to receive re
    
[^178]: STG-Mamba: 通过选择性状态空间模型进行时空图学习

    STG-Mamba: Spatial-Temporal Graph Learning via Selective State Space Model

    [https://arxiv.org/abs/2403.12418](https://arxiv.org/abs/2403.12418)

    STG-Mamba 是首个利用选择性状态空间模型进行时空图学习的研究，将STG网络视为系统，并采用图选择性状态空间模块（GS3B）精确表征STG的动态演化。

    

    Spatial-Temporal Graph（STG）数据具有动态性、异质性和非平稳性特点，导致时空图学习持续面临挑战。近年来，提出了各种基于GNN的方法，主要集中于模拟STG网络中节点个体之间的关系，忽略了随时间存在的STG系统本质特征的建模重要性。相反，现代选择性状态空间模型（SSSMs）提出了一种将STG网络视为系统的新方法，并精心探索了STG系统在时间维度上的动态状态演变。在本工作中，我们引入了Spatial-Temporal Graph Mamba（STG-Mamba），作为首个利用强大的选择性状态空间模型进行STG学习的研究，将STG网络视为系统，并采用图选择性状态空间模块（GS3B）精确表征STG的动态演化。

    arXiv:2403.12418v1 Announce Type: cross  Abstract: Spatial-Temporal Graph (STG) data is characterized as dynamic, heterogenous, and non-stationary, leading to the continuous challenge of spatial-temporal graph learning. In the past few years, various GNN-based methods have been proposed to solely focus on mimicking the relationships among node individuals of the STG network, ignoring the significance of modeling the intrinsic features that exist in STG system over time. In contrast, modern Selective State Space Models (SSSMs) present a new approach which treat STG Network as a system, and meticulously explore the STG system's dynamic state evolution across temporal dimension. In this work, we introduce Spatial-Temporal Graph Mamba (STG-Mamba) as the first exploration of leveraging the powerful selective state space models for STG learning by treating STG Network as a system, and employing the Graph Selective State Space Block (GS3B) to precisely characterize the dynamic evolution of ST
    
[^179]: 评估文本到图像合成：图像质量度量的调查与分类

    Evaluating Text to Image Synthesis: Survey and Taxonomy of Image Quality Metrics

    [https://arxiv.org/abs/2403.11821](https://arxiv.org/abs/2403.11821)

    评估文本到图像合成中，提出了针对图像质量的新评估指标，以确保文本和图像内容的对齐，并提出了新的分类法来归纳这些指标

    

    最近，通过利用语言和视觉结合的基础模型，推动了文本到图像合成方面的进展。这些模型在互联网或其他大规模数据库中的海量文本-图像对上进行了预训练。随着对高质量图像生成的需求转向确保文本与图像之间的内容对齐，已开发了新颖的评估度量标准，旨在模拟人类判断。因此，研究人员开始收集具有越来越复杂注释的数据集，以研究视觉语言模型的组成性及其作为文本与图像内容组成对齐质量度量的其纳入。在这项工作中，我们全面介绍了现有的文本到图像评估指标，并提出了一个新的分类法来对这些指标进行分类。我们还审查了经常采用的文本-图像基准数据集

    arXiv:2403.11821v1 Announce Type: cross  Abstract: Recent advances in text-to-image synthesis have been enabled by exploiting a combination of language and vision through foundation models. These models are pre-trained on tremendous amounts of text-image pairs sourced from the World Wide Web or other large-scale databases. As the demand for high-quality image generation shifts towards ensuring content alignment between text and image, novel evaluation metrics have been developed with the aim of mimicking human judgments. Thus, researchers have started to collect datasets with increasingly complex annotations to study the compositionality of vision-language models and their incorporation as a quality measure of compositional alignment between text and image contents. In this work, we provide a comprehensive overview of existing text-to-image evaluation metrics and propose a new taxonomy for categorizing these metrics. We also review frequently adopted text-image benchmark datasets befor
    
[^180]: LLM能生成类人行路指示吗？走向跨平台的具身指令综合

    Can LLMs Generate Human-Like Wayfinding Instructions? Towards Platform-Agnostic Embodied Instruction Synthesis

    [https://arxiv.org/abs/2403.11487](https://arxiv.org/abs/2403.11487)

    提出了一种新方法，利用LLM以及上下文学习，实现了自动生成具身机器人的“行路指示”，并且在多个模拟平台上展示出跨平台特性。

    

    我们提出了一种新颖的方法，用于自动合成“行路指示”以指导具身机器人。与先前的方法相比，这种方法不再依赖于仅设计用于特定模拟平台的人工注释数据集，而是使用上下文学习来调节LLM，以使用少量参考生成指示。我们使用基于LLM的视觉问答策略收集环境的详细信息，LLM用于指令合成。我们将我们的方法实现在多个模拟平台上，包括Matterport3D、AI Habitat和ThreeDWorld，从而展示了其跨平台特性。我们通过用户研究主观评估了我们的方法，观察到83.3%的用户认为合成的指示准确捕捉了环境的细节，并表现出与人类生成的指示类似的特征。

    arXiv:2403.11487v1 Announce Type: cross  Abstract: We present a novel approach to automatically synthesize "wayfinding instructions" for an embodied robot agent. In contrast to prior approaches that are heavily reliant on human-annotated datasets designed exclusively for specific simulation platforms, our algorithm uses in-context learning to condition an LLM to generate instructions using just a few references. Using an LLM-based Visual Question Answering strategy, we gather detailed information about the environment which is used by the LLM for instruction synthesis. We implement our approach on multiple simulation platforms including Matterport3D, AI Habitat and ThreeDWorld, thereby demonstrating its platform-agnostic nature. We subjectively evaluate our approach via a user study and observe that 83.3% of users find the synthesized instructions accurately capture the details of the environment and show characteristics similar to those of human-generated instructions. Further, we con
    
[^181]: 在人类对齐中扩展数据多样性以微调语言模型

    Scaling Data Diversity for Fine-Tuning Language Models in Human Alignment

    [https://arxiv.org/abs/2403.11124](https://arxiv.org/abs/2403.11124)

    更多的响应但更少的提示可以更好地触发大型语言模型进行人类对齐，此外，提出了一种新的提示多样性公式，可以进一步影响LLMs的最终性能。

    

    与人类偏好对齐可以防止大型语言模型（LLMs）生成误导性或有毒内容，同时需要高成本的人类反馈。假设人工注释资源有限，则可以考虑两种不同的分配方式：更多样化的提示或更多样化的待标记响应。然而，它们对结果的影响的直接比较尚不存在。在这项工作中，我们首先根据微调样本数量控制双方的多样性，这可以直接反映它们的影响。我们发现，与大量提示不同，更多的响应但是更少的提示更能激发LLMs进行人类对齐。此外，提示的多样性概念可能比通常由单个数字量化的响应更复杂。因此，提出了提示多样性的新公式，进一步暗示与微调后LLMs最终性能的线性相关。

    arXiv:2403.11124v1 Announce Type: cross  Abstract: Alignment with human preference prevents large language models (LLMs) from generating misleading or toxic content while requiring high-cost human feedback. Assuming resources of human annotation are limited, there are two different ways of allocating considered: more diverse PROMPTS or more diverse RESPONSES to be labeled. Nonetheless, a straightforward comparison between their impact is absent. In this work, we first control the diversity of both sides according to the number of samples for fine-tuning, which can directly reflect their influence. We find that instead of numerous prompts, more responses but fewer prompts better trigger LLMs for human alignment. Additionally, the concept of diversity for prompts can be more complex than responses that are typically quantified by single digits. Consequently, a new formulation of prompt diversity is proposed, further implying a linear correlation with the final performance of LLMs after f
    
[^182]: 通过学习的先验知识提升基于流的生成超分辨模型

    Boosting Flow-based Generative Super-Resolution Models via Learned Prior

    [https://arxiv.org/abs/2403.10988](https://arxiv.org/abs/2403.10988)

    通过引入条件学习的先验知识，成功解决了基于流的超分辨模型中的网格伪影、逆矩阵爆炸和次优结果等问题。

    

    基于流的超分辨（SR）模型展现了在生成功能高质量图像方面的惊人能力。然而，这些方法在图像生成过程中遇到了几个挑战，比如网格伪影、逆矩阵爆炸和由于固定的采样温度导致的次优结果。为了克服这些问题，本文在基于流的SR模型的推断阶段引入了条件学习的先验知识。这个先验知识是由我们提出的潜在模块根据低分辨率图像预测出的潜在代码，然后通过流模型转换为SR图像。我们的框架旨在无缝地与任何现代基于流的SR模型集成，而无需修改其架构或预训练权重。我们通过大量实验和消融分析评估了提出的框架的有效性。提出的框架成功解决了基于流的SR模型中的所有固有问题。

    arXiv:2403.10988v1 Announce Type: cross  Abstract: Flow-based super-resolution (SR) models have demonstrated astonishing capabilities in generating high-quality images. However, these methods encounter several challenges during image generation, such as grid artifacts, exploding inverses, and suboptimal results due to a fixed sampling temperature. To overcome these issues, this work introduces a conditional learned prior to the inference phase of a flow-based SR model. This prior is a latent code predicted by our proposed latent module conditioned on the low-resolution image, which is then transformed by the flow model into an SR image. Our framework is designed to seamlessly integrate with any contemporary flow-based SR model without modifying its architecture or pre-trained weights. We evaluate the effectiveness of our proposed framework through extensive experiments and ablation analyses. The proposed framework successfully addresses all the inherent issues in flow-based SR models a
    
[^183]: 一个白盒神经网络的概念框架

    A Conceptual Framework For White Box Neural Networks

    [https://arxiv.org/abs/2403.09863](https://arxiv.org/abs/2403.09863)

    引入语义特征作为通用概念框架，实现了完全可解释的神经网络层，为白盒神经网络的范式转变开辟了新的可能性。

    

    本文引入语义特征作为完全可解释神经网络层的通用概念框架。一个充分动机的MNIST相关子问题的概念验证模型包括4个这样的层，总共4800个可学习参数。该模型易于解释，无需任何形式的对抗训练即可实现人类水平的对抗测试准确率，需要较少的超参数调节，并且可以在单个CPU上快速训练。该技术的通用性承诺为彻底民主化和真正通用的白盒神经网络带来了希望。代码可在https://github.com/314-Foundation/white-box-nn找到。

    arXiv:2403.09863v1 Announce Type: cross  Abstract: This paper introduces semantic features as a general conceptual framework for fully explainable neural network layers. A well-motivated proof of concept model for relevant subproblem of MNIST consists of 4 such layers with the total of 4.8K learnable parameters. The model is easily interpretable, achieves human-level adversarial test accuracy with no form of adversarial training, requires little hyperparameter tuning and can be quickly trained on a single CPU. The general nature of the technique bears promise for a paradigm shift towards radically democratised and truly generalizable white box neural networks. The code is available at https://github.com/314-Foundation/white-box-nn
    
[^184]: 通过结构化稀疏张量分解对稀疏DNN加速进行抽象化

    Abstracting Sparse DNN Acceleration via Structured Sparse Tensor Decomposition

    [https://arxiv.org/abs/2403.07953](https://arxiv.org/abs/2403.07953)

    本文提出了通过结构化分解张量进一步抽象稀疏DNN加速的方法，实现了将稀疏张量转换成一系列结构化稀疏张量，从而弥合了稀疏DNN模型和硬件之间的差距。

    

    在深度神经网络（DNNs）中利用稀疏性已成为满足现代DNN日益增长的计算需求的一种具有前景的领域。然而，在实践中，稀疏DNN加速仍然面临一个关键挑战。为了最小化稀疏加速的开销，硬件设计师最近提出了结构化稀疏硬件支持，这提供了有限的灵活性并需要额外的模型微调。此外，为某些结构化稀疏硬件微调的任何稀疏模型无法被其他结构化硬件加速。为了弥合稀疏DNN模型和硬件之间的差距，本文提出了通过结构分解的张量近似（TASD），利用了线性代数中的分配性质将任何稀疏张量转化为一系列结构化稀疏张量。接下来，我们开发了一个软件框架TASDER，通过搜索逐层高质量的结构化分解来加速DNNs的权重和...

    arXiv:2403.07953v1 Announce Type: cross  Abstract: Exploiting sparsity in deep neural networks (DNNs) has been a promising area to meet the growing computation need of modern DNNs. However, in practice, sparse DNN acceleration still faces a key challenge. To minimize the overhead of sparse acceleration, hardware designers have proposed structured sparse hardware support recently, which provides limited flexibility and requires extra model fine-tuning. Moreover, any sparse model fine-tuned for certain structured sparse hardware cannot be accelerated by other structured hardware. To bridge the gap between sparse DNN models and hardware, this paper proposes tensor approximation via structured decomposition (TASD), which leverages the distributive property in linear algebra to turn any sparse tensor into a series of structured sparse tensors. Next, we develop a software framework, TASDER, to accelerate DNNs by searching layer-wise, high-quality structured decomposition for both weight and 
    
[^185]: 基于矩阵变换的低秩调整（MTLoRA）：一种受大脑启发的参数高效微调方法

    Matrix-Transformation Based Low-Rank Adaptation (MTLoRA): A Brain-Inspired Method for Parameter-Efficient Fine-Tuning

    [https://arxiv.org/abs/2403.07440](https://arxiv.org/abs/2403.07440)

    该论文提出了基于矩阵变换的低秩调整（MTLoRA）方法，受大脑启发，用于提高微调技术的复杂任务适应性、性能、稳定性和算法复杂性。

    

    基于大型预训练语言模型（LPLMs）的微调技术已被证明可以显著提高模型在各种下游任务上的性能，并有效控制LPLMs的输出行为。本文受大脑功能受其几何结构塑造的启发，将这一思想融入LoRA技术中，提出了一种新的基于矩阵变换的重新参数化方法，以减少复杂任务适应性、性能、稳定性和算法复杂性方面的改进空间。

    arXiv:2403.07440v1 Announce Type: cross  Abstract: Fine-tuning techniques based on Large Pretrained Language Models (LPLMs) have been proven to significantly enhance model performance on a variety of downstream tasks and effectively control the output behaviors of LPLMs. Recent studies have proposed numerous methods for fine-tuning a small number of parameters based on open-source LPLMs, reducing the demand for computational and storage resources. Among these, reparameterization fine-tuning methods represented by LoRA (Low-Rank Adaptation) have gained popularity. We find that although these methods perform well in many aspects, there is still considerable room for improvement in terms of complex task adaptability, performance, stability, and algorithm complexity. In response to this, inspired by the idea that the functions of the brain are shaped by its geometric structure, this paper integrates this idea into LoRA technology and proposes a new matrix transformation-based reparameteriz
    
[^186]: ParallelPARC: 生成自然语言类比的可扩展流水线

    ParallelPARC: A Scalable Pipeline for Generating Natural-Language Analogies

    [https://arxiv.org/abs/2403.01139](https://arxiv.org/abs/2403.01139)

    设计了ParallelPARC流水线，利用大型语言模型生成复杂段落类比数据集，评估各种类比类型，并展示出人类在类比识别中的优势。

    

    Analogy-making对于人类认知至关重要，使我们能够适应新颖情境--这是当前人工智能系统仍然缺乏的能力。大多数类比数据集今天关注简单的类比（例如，词类比）；包含复杂类型类比的数据集通常是手工策划的，并且非常小。我们认为这限制了计算类比的进展。在这项工作中，我们设计了一个数据生成流水线，ParallelPARC（Parallel Paragraph Creator），利用最先进的大型语言模型（LLM）来创建基于段落的复杂类比，以及简单和具有挑战性的干扰项。我们展示了我们的流水线，并创建了ProPara-Logy，一个关于科学过程间类比的数据集。我们发布了一个由人类验证过的金标准数据集，以及一个自动生成的银标准数据集。我们在二进制和多选环境中测试了LLMs和人类对类比的识别，发现人类胜过最佳模型。

    arXiv:2403.01139v1 Announce Type: cross  Abstract: Analogy-making is central to human cognition, allowing us to adapt to novel situations -- an ability that current AI systems still lack. Most analogy datasets today focus on simple analogies (e.g., word analogies); datasets including complex types of analogies are typically manually curated and very small. We believe that this holds back progress in computational analogy. In this work, we design a data generation pipeline, ParallelPARC (Parallel Paragraph Creator) leveraging state-of-the-art Large Language Models (LLMs) to create complex, paragraph-based analogies, as well as distractors, both simple and challenging. We demonstrate our pipeline and create ProPara-Logy, a dataset of analogies between scientific processes. We publish a gold-set, validated by humans, and a silver-set, generated automatically. We test LLMs' and humans' analogy recognition in binary and multiple-choice settings, and found that humans outperform the best mod
    
[^187]: UrbanGPT: 时空大型语言模型

    UrbanGPT: Spatio-Temporal Large Language Models

    [https://arxiv.org/abs/2403.00813](https://arxiv.org/abs/2403.00813)

    都市GPT旨在建立一个具有强大泛化能力的时空模型，借鉴大型语言模型的成就。

    

    都市GPT旨在预测并洞察城市环境在时间和空间上不断变化的动态。其目的是预测都市生活各个方面的未来模式、趋势和事件，包括交通、人口流动和犯罪率等。尽管已经付出了大量努力开发神经网络技术以准确预测时空数据，但需注意到很多方法在生成精确的时空表示时严重依赖于有足够标记的数据。不幸的是，在实际都市感知场景中，数据稀缺是一个普遍存在的问题。因此，建立一个具有强大泛化能力的时空模型跨越多样时空学习场景是必要的。受大型语言模型(LLM)卓越成就的启发，我们的目标是

    arXiv:2403.00813v1 Announce Type: cross  Abstract: Spatio-temporal prediction aims to forecast and gain insights into the ever-changing dynamics of urban environments across both time and space. Its purpose is to anticipate future patterns, trends, and events in diverse facets of urban life, including transportation, population movement, and crime rates. Although numerous efforts have been dedicated to developing neural network techniques for accurate predictions on spatio-temporal data, it is important to note that many of these methods heavily depend on having sufficient labeled data to generate precise spatio-temporal representations. Unfortunately, the issue of data scarcity is pervasive in practical urban sensing scenarios. Consequently, it becomes necessary to build a spatio-temporal model with strong generalization capabilities across diverse spatio-temporal learning scenarios. Taking inspiration from the remarkable achievements of large language models (LLMs), our objective is 
    
[^188]: 通过对抗攻击实现抗LLM的数学问题生成

    LLM-Resistant Math Word Problem Generation via Adversarial Attacks

    [https://arxiv.org/abs/2402.17916](https://arxiv.org/abs/2402.17916)

    本研究提出了一种新方法，通过生成保留原问题结构难度但针对LLMs无解的对抗性示例，有效地降低了LLMs的数学问题解决能力。

    

    大型语言模型（LLMs）显著改变了教育领域。本文探讨了一种新的范例，生成对抗性示例，以确保公平评估，这些示例保留了原始问题的结构和难度，但LLMs无法解决。我们专注于数学应用领域的词问题，利用抽象语法树结构生成对抗示例，通过简单编辑问题中的数字值，导致LLMs产生错误答案。我们对各种开源和闭源LLMs进行实验，定量和定性地证明我们的方法显著降低了它们的数学问题解决能力。

    arXiv:2402.17916v1 Announce Type: cross  Abstract: Large language models (LLMs) have significantly transformed the educational landscape. As current plagiarism detection tools struggle to keep pace with LLMs' rapid advancements, the educational community faces the challenge of assessing students' true problem-solving abilities in the presence of LLMs. In this work, we explore a new paradigm for ensuring fair evaluation -- generating adversarial examples which preserve the structure and difficulty of the original questions aimed for assessment, but are unsolvable by LLMs. Focusing on the domain of math word problems, we leverage abstract syntax trees to structurally generate adversarial examples that cause LLMs to produce incorrect answers by simply editing the numeric values in the problems. We conduct experiments on various open- and closed-source LLMs, quantitatively and qualitatively demonstrating that our method significantly degrades their math problem-solving ability. We identify
    
[^189]: Q-FOX学习：颠覆传统的强化学习

    Q-FOX Learning: Breaking Tradition in Reinforcement Learning

    [https://arxiv.org/abs/2402.16562](https://arxiv.org/abs/2402.16562)

    Q-FOX学习是一种新颖的自动超参数调整方法，结合了FOX优化器和Q-learning算法，提出了使用新的目标函数来解决强化学习中超参数调整的问题。

    

    强化学习（RL）是人工智能（AI）的一个子集，代理通过与环境的交互来学习最佳动作，因此适用于不需要标记数据或直接监督的任务。 本文提出了一种名为Q-FOX的新颖自动调参方法，该方法使用了FOX优化器和常用的易于实现的RL Q-learning算法解决了调参的问题。此外，还提出了一个新的目标函数，该函数将奖励放在均方误差（MSE）和学习时间之上。

    arXiv:2402.16562v2 Announce Type: replace-cross  Abstract: Reinforcement learning (RL) is a subset of artificial intelligence (AI) where agents learn the best action by interacting with the environment, making it suitable for tasks that do not require labeled data or direct supervision. Hyperparameters (HP) tuning refers to choosing the best parameter that leads to optimal solutions in RL algorithms. Manual or random tuning of the HP may be a crucial process because variations in this parameter lead to changes in the overall learning aspects and different rewards. In this paper, a novel and automatic HP-tuning method called Q-FOX is proposed. This uses both the FOX optimizer, a new optimization method inspired by nature that mimics red foxes' hunting behavior, and the commonly used, easy-to-implement RL Q-learning algorithm to solve the problem of HP tuning. Moreover, a new objective function is proposed which prioritizes the reward over the mean squared error (MSE) and learning time (
    
[^190]: Triad: 一个利用基于多角色LLM代理的框架来解决知识库问答问题

    Triad: A Framework Leveraging a Multi-Role LLM-based Agent to Solve Knowledge Base Question Answering

    [https://arxiv.org/abs/2402.14320](https://arxiv.org/abs/2402.14320)

    Triad框架利用了基于多角色LLM代理来解决知识库问答问题，通过代理的不同角色分别处理KBQA子任务，合作完成KBQA任务，并在多个基准数据集上表现出色。

    

    最近基于LLM代理的进展在各种任务中展现出了令人期待的结果。然而，它们在回答知识库中问题的运用仍然鲜为人知。使用传统方法来实现KBQA系统具有挑战性，因为缺乏特定任务训练数据以及创建以任务为中心的模型结构的复杂性。在本文中，我们提出了Triad，一个利用具有三个角色的LLM代理的统一框架来进行KBQA任务。代理被分配三个角色来处理不同的KBQA子任务：作为掌握各种子任务的通才，作为选择候选者的决策者，以及作为回答带有知识的问题的顾问。我们的KBQA框架在四个阶段中执行，涉及代理的多重角色的协作。我们使用三个基准数据集评估了我们框架的性能，结果显示我们的框架胜过了

    arXiv:2402.14320v1 Announce Type: cross  Abstract: Recent progress with LLM-based agents has shown promising results across various tasks. However, their use in answering questions from knowledge bases remains largely unexplored. Implementing a KBQA system using traditional methods is challenging due to the shortage of task-specific training data and the complexity of creating task-focused model structures. In this paper, we present Triad, a unified framework that utilizes an LLM-based agent with three roles for KBQA tasks. The agent is assigned three roles to tackle different KBQA subtasks: agent as a generalist for mastering various subtasks, as a decision maker for the selection of candidates, and as an advisor for answering questions with knowledge. Our KBQA framework is executed in four phases, involving the collaboration of the agent's multiple roles. We evaluated the performance of our framework using three benchmark datasets, and the results show that our framework outperforms 
    
[^191]: TofuEval：评估LLM在主题对话摘要中的幻觉

    TofuEval: Evaluating Hallucinations of LLMs on Topic-Focused Dialogue Summarization

    [https://arxiv.org/abs/2402.13249](https://arxiv.org/abs/2402.13249)

    论文提出了一个新的主题对话摘要评估基准TofuEval，研究发现现有的LLMs在对话领域存在大量事实错误的幻觉，并表明当LLMs充当事实评估器时，其表现不佳。

    

    单文档新闻摘要在忠实度方面取得了长足进步，这得益于对事实一致性或幻觉评估的研究。我们探讨了这些进展是否能延伸到其他文本摘要领域。我们提出了一个新的主题对话摘要评估基准，由不同规模的LLMs生成。我们提供了关于这些摘要的事实一致性的二元句级人类注释，以及对事实不一致句子的详细解释。我们的分析表明，现有的LLMs在对话领域存在大量事实错误的幻觉，无论模型大小如何。另一方面，当LLMs（包括GPT-4）充当二元事实评估器时，它们表现不佳，且可以被当前最先进的专门事实评估度量所超越。最后，我们对幻觉类型进行了分析

    arXiv:2402.13249v1 Announce Type: cross  Abstract: Single document news summarization has seen substantial progress on faithfulness in recent years, driven by research on the evaluation of factual consistency, or hallucinations. We ask whether these advances carry over to other text summarization domains. We propose a new evaluation benchmark on topic-focused dialogue summarization, generated by LLMs of varying sizes. We provide binary sentence-level human annotations of the factual consistency of these summaries along with detailed explanations of factually inconsistent sentences. Our analysis shows that existing LLMs hallucinate significant amounts of factual errors in the dialogue domain, regardless of the model's size. On the other hand, when LLMs, including GPT-4, serve as binary factual evaluators, they perform poorly and can be outperformed by prevailing state-of-the-art specialized factuality evaluation metrics. Finally, we conducted an analysis of hallucination types with a cu
    
[^192]: RS-DPO：一种用于对齐大型语言模型的混合拒绝采样和直接优化偏好的方法

    RS-DPO: A Hybrid Rejection Sampling and Direct Preference Optimization Method for Alignment of Large Language Models

    [https://arxiv.org/abs/2402.10038](https://arxiv.org/abs/2402.10038)

    本研究提出了一种名为RS-DPO的方法，它将拒绝采样和直接优化偏好结合起来，用于对齐大型语言模型。通过开发一个经过监督微调的策略模型，并从该模型中直接采样响应，RS-DPO能够有效解决基于近端策略优化的不稳定性和高计算成本的问题。通过识别对比样本对，RS-DPO能够更好地进行RLHF。

    

    强化学习从人类反馈中学习（RLHF）已被广泛应用于将大型语言模型与用户意图对齐。然而，基于近端策略优化（PPO）的RLHF有时不稳定，需要显著的超参数微调，并且在对齐过程中计算成本高昂。最近，提出了直接优化偏好（DPO）来解决这些挑战。然而，DPO依赖于从人类标注者和替代LLM生成的对比回复，而不是策略模型，限制了RLHF的效果。本文通过系统地结合拒绝采样（RS）和DPO来解决这两个挑战。我们提出的方法RS-DPO，首先开发出一个经过监督微调的策略模型（SFT）。然后直接从SFT模型中采样每个提示的k个响应。RS-DPO基于其相似度识别对比样本对。

    arXiv:2402.10038v1 Announce Type: cross  Abstract: Reinforcement learning from human feedback (RLHF) has been extensively employed to align large language models with user intent. However, proximal policy optimization (PPO) based RLHF is occasionally unstable requiring significant hyperparameter finetuning, and computationally expensive to maximize the estimated reward during alignment. Recently, direct preference optimization (DPO) is proposed to address those challenges. However, DPO relies on contrastive responses generated from human annotator and alternative LLM, instead of the policy model, limiting the effectiveness of the RLHF. In this paper, we addresses both challenges by systematically combining rejection sampling (RS) and DPO. Our proposed method, RS-DPO, initiates with the development of a supervised fine-tuned policy model (SFT). A varied set of k responses per prompt are sampled directly from the SFT model. RS-DPO identifies pairs of contrastive samples based on their re
    
[^193]: LlaSMol:利用大规模、全面、高质量的指令调优数据集推进化学的大规模语言模型

    LlaSMol: Advancing Large Language Models for Chemistry with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset

    [https://arxiv.org/abs/2402.09391](https://arxiv.org/abs/2402.09391)

    本文介绍了LlaSMol，它是一种推进化学领域大规模语言模型的方法。通过使用一个大规模、全面、高质量的指令调优数据集来训练模型，LlaSMol在化学任务中表现出强大的性能，超过了GPT-4并接近于任务特定模型。

    

    化学在药物研发和材料科学等许多领域中起着至关重要的作用。尽管诸如GPT-4之类的大型语言模型（LLM）在自然语言处理任务上展现出了非凡的能力，但现有工作表明它们在化学任务上的性能令人失望。然而，在本文中，我们展示了我们开发的LLM在一系列化学任务上可以取得非常强大的结果，在所有任务上都显著优于最先进的GPT-4，并接近SoTA任务特定模型。我们取得成功的关键是一个名为SMolInstruct的大规模、全面、高质量的指令调优数据集。它包含了14个经过精心挑选的化学任务和超过三百万个高质量样本，为训练和评估化学LLM奠定了坚实基础。基于SMolInstruct，我们对一组开源LLM进行了微调，其中，我们发现Mistral ser是最佳性能的模型。

    arXiv:2402.09391v1 Announce Type: new Abstract: Chemistry plays a crucial role in many domains, such as drug discovery and material science. While large language models (LLMs) such as GPT-4 exhibit remarkable capabilities on natural language processing tasks, existing work shows their performance on chemistry tasks is discouragingly low. In this paper, however, we demonstrate that our developed LLMs can achieve very strong results on a comprehensive set of chemistry tasks, outperforming the most advanced GPT-4 across all the tasks by a substantial margin and approaching the SoTA task-specific models. The key to our success is a large-scale, comprehensive, high-quality dataset for instruction tuning named SMolInstruct. It contains 14 meticulously selected chemistry tasks and over three million high-quality samples, laying a solid foundation for training and evaluating LLMs for chemistry. Based on SMolInstruct, we fine-tune a set of open-source LLMs, among which, we find that Mistral ser
    
[^194]: X-LoRA: 一种灵活的大型语言模型框架，利用低秩适配器专家的混合策略在蛋白质力学和设计中的应用

    X-LoRA: Mixture of Low-Rank Adapter Experts, a Flexible Framework for Large Language Models with Applications in Protein Mechanics and Design

    [https://arxiv.org/abs/2402.07148](https://arxiv.org/abs/2402.07148)

    X-LoRA是一种灵活的大型语言模型框架，利用低秩适配器专家的混合策略，可以创建精细调整的模型并在蛋白质力学和设计领域应用。该模型利用深层逐层适应的组合来解决特定任务，并受到生物学原理的启发。无需修改底层结构即可应用于任何现有的语言模型。

    

    我们报道了一种使用深层逐层基于低秩适应（LoRA）的新颖预训练适配器的混合专家策略，用于创建精细调整的大型语言模型。我们提出了一种利用隐藏状态动态混合经过适应的层的门控策略，允许得到的X-LoRA模型利用不同的能力并创建以前未使用的深层逐层适应的组合来解决特定任务。该设计受到了生物普遍性和多样性的生物学原理的启发，其中神经网络建模块在不同的分层表示中被重复使用。因此，X-LoRA模型可以轻松用于任何现有的大型语言模型（LLM），无需修改底层结构。我们还开发了一个定制的X-LoRA模型，提供了包括前向/逆向分析任务和增强推理能力在内的科学能力，重点是生物材料分析。

    We report a mixture of expert strategy to create fine-tuned large language models using a deep layer-wise token-level approach based on low-rank adaptation (LoRA). Starting with a set of pre-trained LoRA adapters, we propose a gating strategy that uses the hidden states to dynamically mix adapted layers, allowing the resulting X-LoRA model to draw upon different capabilities and create never-before-used deep layer-wise combinations of adaptations are established to solve specific tasks. The design is inspired by the biological principles of universality and diversity, where neural network building blocks are reused in different hierarchical manifestations. Hence, the X-LoRA model can be easily implemented for any existing large language model (LLM) without a need for modifications of the underlying structure. We develop a tailored X-LoRA model that offers scientific capabilities including forward/inverse analysis tasks and enhanced reasoning capability, focused on biomaterial analysis,
    
[^195]: 安全多模态学习系统调研

    A Survey on Safe Multi-Modal Learning System

    [https://arxiv.org/abs/2402.05355](https://arxiv.org/abs/2402.05355)

    这项研究提出了第一个多模态学习系统安全的分类法，对当前发展状态下的关键限制进行了审查，并提出了未来研究的潜在方向。

    

    随着多模态学习系统在现实场景中的广泛应用，安全问题变得越来越突出。对于这一领域的安全问题缺乏系统性研究已成为一个重要的障碍。为了解决这个问题，我们提出了第一个多模态学习系统安全的分类法，确定了这些问题的四个关键支柱。借助这一分类法，我们对每个支柱进行了深入审查，突出了当前发展状态的关键限制。最后，我们指出了多模态学习系统安全面临的独特挑战，并提供了未来研究的潜在方向。

    With the wide deployment of multimodal learning systems (MMLS) in real-world scenarios, safety concerns have become increasingly prominent. The absence of systematic research into their safety is a significant barrier to progress in this field. To bridge the gap, we present the first taxonomy for MMLS safety, identifying four essential pillars of these concerns. Leveraging this taxonomy, we conduct in-depth reviews for each pillar, highlighting key limitations based on the current state of development. Finally, we pinpoint unique challenges in MMLS safety and provide potential directions for future research.
    
[^196]: TinyLLM: 从多个大型语言模型学习一个小型学生模型

    TinyLLM: Learning a Small Student from Multiple Large Language Models

    [https://arxiv.org/abs/2402.04616](https://arxiv.org/abs/2402.04616)

    TinyLLM是一种从多个大型语言模型中学习小型学生模型的知识蒸馏范式，旨在解决知识多样性有限和缺乏上下文信息等问题，并鼓励学生模型理解答案背后的原理。

    

    将更强大的大型语言模型（LLMs）的推理能力转移到较小的模型上具有吸引力，因为较小的LLMs更灵活，成本更低。在现有的解决方案中，知识蒸馏因其出色的效率和泛化能力而脱颖而出。然而，现有方法存在一些缺点，包括知识多样性有限和缺乏丰富的上下文信息。为了解决这些问题并促进紧凑语言模型的学习，我们提出了TinyLLM，一种从多个大型教师LLMs中学习小型学生LLM的新型知识蒸馏范式。特别地，我们鼓励学生LLM不仅生成正确答案，而且理解这些答案背后的原理。鉴于不同的LLMs具有不同的推理能力，我们引导学生模型吸收来自多个教师LLMs的知识。我们进一步引入了一个上下文示例生成器和一个老师强制模块...

    Transferring the reasoning capability from stronger large language models (LLMs) to smaller ones has been quite appealing, as smaller LLMs are more flexible to deploy with less expense. Among the existing solutions, knowledge distillation stands out due to its outstanding efficiency and generalization. However, existing methods suffer from several drawbacks, including limited knowledge diversity and the lack of rich contextual information. To solve the problems and facilitate the learning of compact language models, we propose TinyLLM, a novel knowledge distillation paradigm to learn a small student LLM from multiple large teacher LLMs. In particular, we encourage the student LLM to not only generate the correct answers but also understand the rationales behind these answers. Given that different LLMs possess diverse reasoning skills, we guide the student model to assimilate knowledge from various teacher LLMs. We further introduce an in-context example generator and a teacher-forcing 
    
[^197]: 双视图视觉背景化的网页导航

    Dual-View Visual Contextualization for Web Navigation

    [https://arxiv.org/abs/2402.04476](https://arxiv.org/abs/2402.04476)

    本文提出了一种通过网页截图中元素的“双视图”来进行 HTML 元素的背景化的方法，这种方法通过将每个元素与其邻居元素进行背景化，使用文本和视觉特征，使得HTML元素的结果表示对于代理执行操作更加信息丰富。

    

    自动网页导航旨在构建一个可以根据语言指令在实际网站上执行复杂和多样任务的网络代理。现有工作主要是以 HTML 文档作为输入，HTML 文档定义了网页的内容和操作空间（即可操作元素和操作）。然而，HTML 文档可能无法为每个元素提供清晰的任务相关背景，使得选择正确的（一系列的）操作变得困难。本文提出通过网页截图中元素的“双视图”来进行 HTML 元素的背景化：每个 HTML 元素在截图中有其对应的边界框和视觉内容。我们基于一个洞察力——网页开发者倾向于在网页上将任务相关元素放置在附近以增强用户体验，并提出将每个元素与其邻居元素进行背景化，使用文本和视觉特征。HTML 元素的结果表示对于代理执行操作更加信息丰富。

    Automatic web navigation aims to build a web agent that can follow language instructions to execute complex and diverse tasks on real-world websites. Existing work primarily takes HTML documents as input, which define the contents and action spaces (i.e., actionable elements and operations) of webpages. Nevertheless, HTML documents may not provide a clear task-related context for each element, making it hard to select the right (sequence of) actions. In this paper, we propose to contextualize HTML elements through their "dual views" in webpage screenshots: each HTML element has its corresponding bounding box and visual content in the screenshot. We build upon the insight -- web developers tend to arrange task-related elements nearby on webpages to enhance user experiences -- and propose to contextualize each element with its neighbor elements, using both textual and visual features. The resulting representations of HTML elements are more informative for the agent to take action. We val
    
[^198]: 《读玩游戏（R2-Play）: 多模态游戏指导下的决策 Transformer》

    Read to Play (R2-Play): Decision Transformer with Multimodal Game Instruction

    [https://arxiv.org/abs/2402.04154](https://arxiv.org/abs/2402.04154)

    本论文探索了为智能体提供增强形式的任务指导，使其能够理解游戏指导并实现"读玩游戏"的能力。通过将多模态指导调优的成功应用于视觉任务中的强化学习任务，构建了一组... (内容太长，无法继续显示)

    

    在人工智能领域，开发一款通用智能体一直是一个长期的目标。先前的研究利用来自各种任务的大量离线数据集，在强化学习的多任务场景中表现出了出色的性能。然而，这些工作在扩展到新任务方面面临挑战。最近的方法将文本指导或视觉轨迹整合到决策网络中，提供任务特定的上下文提示，代表了一个有前途的方向。然而，观察到仅依赖于文本指导或视觉轨迹对于准确传达任务的上下文信息是不足够的。本文探索了增强智能体任务指导的形式，使其能够理解游戏指导，从而实现"读玩游戏"的能力。受到多模态指导调优在视觉任务中的成功启发，我们将基于视觉的强化学习任务视为一个长期视觉任务，并构建了一组... (内容太长，无法继续显示)

    Developing a generalist agent is a longstanding objective in artificial intelligence. Previous efforts utilizing extensive offline datasets from various tasks demonstrate remarkable performance in multitasking scenarios within Reinforcement Learning.However, these works encounter challenges in extending their capabilities to new tasks.Recent approaches integrate textual guidance or visual trajectory into decision networks to provide task-specific contextual cues, representing a promising direction.However, it is observed that relying solely on textual guidance or visual trajectory is insufficient for accurately conveying the contextual information of tasks.This paper explores enhanced forms of task guidance for agents, enabling them to comprehend gameplay instructions, thereby facilitating a "read-to-play" capability.Drawing inspiration from the success of multimodal instruction tuning in visual tasks, we treat the visual-based RL task as a long-horizon vision task and construct a set 
    
[^199]: 在犯罪网络中识别犯罪头目的技术：一项调查、实验和比较评估

    Techniques to Detect Crime Leaders within a Criminal Network: A Survey, Experimental, and Comparative Evaluations

    [https://arxiv.org/abs/2402.03355](https://arxiv.org/abs/2402.03355)

    本论文调查、分析了在犯罪网络中识别犯罪头目的技术和算法，提出了一种新的方法论分类体系，通过实证评估和实验比较，为研究人员提供了全面的决策支持。

    

    本调查论文对在犯罪网络中识别犯罪头目所使用的技术和算法进行了全面分析。针对每种技术，论文讨论了其有效性、局限性、改进潜力和未来前景。现有关注于识别犯罪头目和预测犯罪算法的调查论文所面临的主要挑战是如何有效地对这些算法进行分类。为了解决这个问题，本论文提出了一种新的方法论分类体系，将算法分为更详细的类别和具体技术。论文通过实证评估和实验比较对不同技术进行了排名。方法论分类体系、实证评估和实验比较的结合使人们能够全面而细致地了解识别犯罪头目的技术和算法，为研究人员提供明智的决策支持。此外，论文还提供了其他方面的贡献。

    This survey paper offers a thorough analysis of techniques and algorithms used in the identification of crime leaders within criminal networks. For each technique, the paper examines its effectiveness, limitations, potential for improvement, and future prospects. The main challenge faced by existing survey papers focusing on algorithms for identifying crime leaders and predicting crimes is effectively categorizing these algorithms. To address this limitation, this paper proposes a new methodological taxonomy that hierarchically classifies algorithms into more detailed categories and specific techniques. The paper includes empirical and experimental evaluations to rank the different techniques. The combination of the methodological taxonomy, empirical evaluations, and experimental comparisons allows for a nuanced and comprehensive understanding of the techniques and algorithms for identifying crime leaders, assisting researchers in making informed decisions. Moreover, the paper offers v
    
[^200]: 使用大型多模态模型解释生成模型的潜在表示

    Explaining latent representations of generative models with large multimodal models

    [https://arxiv.org/abs/2402.01858](https://arxiv.org/abs/2402.01858)

    该研究提出了一个框架，利用大型多模态模型全面解释生成模型中的每个潜在因素。对于不同生成模型的解释，我们量化了解释的不确定性，通过多模态模型进行了性能评估，且定性地展示了潜在因素的变化效果。最后，我们讨论了最先进的大型多模态模型的解释能力和局限性。

    

    学习可解释的数据生成潜在因素表示是人工智能发展的重要课题。随着大型多模态模型的兴起，它可以将图像与文本对齐以生成答案。在本研究中，我们提出了一个框架，利用大型多模态模型全面解释生成模型中的每个潜在因素。我们进一步量化评估了我们生成的解释的不确定性，在多个大型多模态模型之间评估了解释生成的性能，并定性地可视化了每个潜在因素的变化，以学习不同生成模型对解释的解缠效果。最后，我们讨论了最先进的大型多模态模型的解释能力和局限性。

    Learning interpretable representations of data generative latent factors is an important topic for the development of artificial intelligence. With the rise of the large multimodal model, it can align images with text to generate answers. In this work, we propose a framework to comprehensively explain each latent factor in the generative models using a large multimodal model. We further measure the uncertainty of our generated explanations, quantitatively evaluate the performance of explanation generation among multiple large multimodal models, and qualitatively visualize the variations of each latent factor to learn the disentanglement effects of different generative models on explanations. Finally, we discuss the explanatory capabilities and limitations of state-of-the-art large multimodal models.
    
[^201]: 一个具有本地神经网络和有限元输入数据的PNP离子通道深度学习求解器

    A PNP ion channel deep learning solver with local neural network and finite element input data

    [https://arxiv.org/abs/2401.17513](https://arxiv.org/abs/2401.17513)

    该论文提出了一个使用本地神经网络和有限元求解器的PNP离子通道深度学习求解器。该求解器能够较快地训练并生成高准确度的数值解，在处理不同扰动情况和离子通道子区域时表现良好。

    

    本文提出了一种用于解决改进的一维Poisson-Nernst-Planck离子通道（PNPic）模型的深度学习方法，称为PNPic深度学习求解器。特别是，它将一种新颖的本地神经网络方案与有效的PNPic有限元求解器相结合。由于神经网络方案的输入数据只涉及粗网格解的一小块局部区域，有限元求解器可以快速生成，因此PNPic深度学习求解器的训练速度比任何对应的传统全局神经网络求解器都要快。经过适当训练，它可以输出一个比低成本粗网格解更高准确度的预测PNPic解，并能反映不同参数扰动情况、离子通道子区域以及界面和边界值等。因此，PNPic深度学习求解器可以为一类PNPic模型生成高准确度的数值解。作为一个初步研究，两个......

    In this paper, a deep learning method for solving an improved one-dimensional Poisson-Nernst-Planck ion channel (PNPic) model, called the PNPic deep learning solver, is presented. In particular, it combines a novel local neural network scheme with an effective PNPic finite element solver. Since the input data of the neural network scheme only involves a small local patch of coarse grid solutions, which the finite element solver can quickly produce, the PNPic deep learning solver can be trained much faster than any corresponding conventional global neural network solvers. After properly trained, it can output a predicted PNPic solution in a much higher degree of accuracy than the low cost coarse grid solutions and can reflect different perturbation cases on the parameters, ion channel subregions, and interface and boundary values, etc. Consequently, the PNPic deep learning solver can generate a numerical solution with high accuracy for a family of PNPic models. As an initial study, two 
    
[^202]: 解决在实施负责任人工智能中的伦理权衡

    Resolving Ethics Trade-offs in Implementing Responsible AI

    [https://arxiv.org/abs/2401.08103](https://arxiv.org/abs/2401.08103)

    该论文提出了通过权衡处理人工智能伦理中的紧张关系的五种方法，并提出了一个框架来实施全面的人工智能/机器学习系统。

    

    虽然把高级人工智能伦理原则应用到实际的人工智能/机器学习系统中已经取得了进展，但在处理底层人工智能伦理方面的紧张关系方面仍存在理论与实践之间的差距。我们提出了五种处理这些关系的方法，从简单到复杂不等。这些方法在考虑的上下文类型、范围、衡量上下文的方法和证明程度上有所不同。这些方法中没有一种适用于所有组织、系统或应用。为了解决这个问题，我们提出了一个框架，包括：（i）积极识别紧张关系，（ii）优先处理和权衡伦理方面，（iii）证明和记录权衡决策。该提议的框架旨在促进实施符合潜在监管要求的全面人工智能/机器学习系统。

    While the operationalisation of high-level AI ethics principles into practical AI/ML systems has made progress, there is still a theory-practice gap in managing tensions between the underlying AI ethics aspects. We cover five approaches for addressing the tensions via trade-offs, ranging from rudimentary to complex. The approaches differ in the types of considered context, scope, methods for measuring contexts, and degree of justification. None of the approaches is likely to be appropriate for all organisations, systems, or applications. To address this, we propose a framework which consists of: (i) proactive identification of tensions, (ii) prioritisation and weighting of ethics aspects, (iii) justification and documentation of trade-off decisions. The proposed framework aims to facilitate the implementation of well-rounded AI/ML systems that are appropriate for potential regulatory requirements.
    
[^203]: Dr$^2$Net：用于内存高效微调的动态可逆双残差网络

    Dr$^2$Net: Dynamic Reversible Dual-Residual Networks for Memory-Efficient Finetuning

    [https://arxiv.org/abs/2401.04105](https://arxiv.org/abs/2401.04105)

    提出了一种名为Dr$^2$Net的新型网络结构，可以以大大降低内存消耗的方式作为预训练模型的替代网络，通过具有两种类型残差连接的设计实现了可逆性，实现了在训练过程中清除中间激活并减少内存占用

    

    大型预训练模型在现代计算机视觉任务中变得越来越重要。这些模型通常通过端对端微调在下游任务中使用，对于具有高分辨率数据的任务（例如视频理解、小目标检测和点云分析），这种微调对内存要求很高。本文提出了一种名为Dr$^2$Net（Dynamic Reversible Dual-Residual Networks）的新型网络结构系列，它充当预训练模型的替代网络，大大降低了内存消耗。Dr$^2$Net包含两种类型的残差连接，一种保持预训练模型中的残差结构，另一种使网络可逆。由于其可逆性，可以从输出中重建中间激活，并在训练过程中将其清除内存。我们分别在两种残差连接类型上引入两个系数，并引入动态tr

    arXiv:2401.04105v2 Announce Type: replace-cross  Abstract: Large pretrained models are increasingly crucial in modern computer vision tasks. These models are typically used in downstream tasks by end-to-end finetuning, which is highly memory-intensive for tasks with high-resolution data, e.g., video understanding, small object detection, and point cloud analysis. In this paper, we propose Dynamic Reversible Dual-Residual Networks, or Dr$^2$Net, a novel family of network architectures that acts as a surrogate network to finetune a pretrained model with substantially reduced memory consumption. Dr$^2$Net contains two types of residual connections, one maintaining the residual structure in the pretrained models, and the other making the network reversible. Due to its reversibility, intermediate activations, which can be reconstructed from output, are cleared from memory during training. We use two coefficients on either type of residual connections respectively, and introduce a dynamic tr
    
[^204]: 因果状态精炼用于可解释强化学习

    Causal State Distillation for Explainable Reinforcement Learning

    [https://arxiv.org/abs/2401.00104](https://arxiv.org/abs/2401.00104)

    本文提出了一种因果状态精炼的方法，通过揭示在训练过程中影响智能体目标的奖励的各个方面，解决了强化学习模型不透明性的问题。

    

    强化学习（RL）是训练智能体的强大技术，但理解这些智能体为何做出特定决策可能非常具有挑战性。RL模型的不透明性一直是一个长期存在的问题，使用户难以理解智能体行为背后的原因。本文提出了一种基于因果状态精炼（Causal State Distillation）的方法，旨在克服奖励分解等其他方法所带来的问题，该方法在训练过程中揭示出对智能体目标产生影响的奖励的各个方面。

    arXiv:2401.00104v2 Announce Type: replace-cross  Abstract: Reinforcement learning (RL) is a powerful technique for training intelligent agents, but understanding why these agents make specific decisions can be quite challenging. This lack of transparency in RL models has been a long-standing problem, making it difficult for users to grasp the reasons behind an agent's behaviour. Various approaches have been explored to address this problem, with one promising avenue being reward decomposition (RD). RD is appealing as it sidesteps some of the concerns associated with other methods that attempt to rationalize an agent's behaviour in a post-hoc manner. RD works by exposing various facets of the rewards that contribute to the agent's objectives during training. However, RD alone has limitations as it primarily offers insights based on sub-rewards and does not delve into the intricate cause-and-effect relationships that occur within an RL agent's neural model. In this paper, we present an e
    
[^205]: ComplexityNet: 通过学习任务复杂性提高LLM推理效率

    ComplexityNet: Increasing LLM Inference Efficiency by Learning Task Complexity

    [https://arxiv.org/abs/2312.11511](https://arxiv.org/abs/2312.11511)

    ComplexityNet通过学习任务复杂性，提高了LLM推理效率，通过预测任务的准确输出概率，成功降低了90%的计算资源使用，并在任务复杂性确定方面取得了79%准确率。

    

    我们提出了ComplexityNet，这是一个专为评估任务复杂性而设计的简化语言模型。该模型通过不同能力的各种语言模型来预测准确输出的可能性。我们首次在Mostly Basic Python Problems（MBPP）数据集上应用了ComplexityNet。我们开创性地创建了第一组标签来定义任务复杂性。ComplexityNet在确定任务复杂性方面取得了显著的79%准确率，较原始、非微调模型的34%准确率有了显著改进。此外，与使用最高复杂性模型相比，ComplexityNet有效地减少了90%的计算资源使用，同时保持了86.7%的高代码生成准确率。这项研究表明，通过微调较小的模型来对任务进行分类，可以在准确性和效率之间取得更平衡的权衡。

    arXiv:2312.11511v2 Announce Type: replace-cross  Abstract: We present ComplexityNet, a streamlined language model designed for assessing task complexity. This model predicts the likelihood of accurate output by various language models, each with different capabilities. Our initial application of ComplexityNet involves the Mostly Basic Python Problems (MBPP) dataset. We pioneered the creation of the first set of labels to define task complexity. ComplexityNet achieved a notable 79% accuracy in determining task complexity, a significant improvement over the 34% accuracy of the original, non fine-tuned model. Furthermore, ComplexityNet effectively reduces computational resource usage by 90% compared to using the highest complexity model, while maintaining a high code generation accuracy of 86.7%. This study demonstrates that fine-tuning smaller models to categorize tasks based on their complexity can lead to a more balanced trade-off between accuracy and efficiency in the use of Large Lan
    
[^206]: SMILE：多模态数据集用于语言模型理解视频中的笑声

    SMILE: Multimodal Dataset for Understanding Laughter in Video with Language Models

    [https://arxiv.org/abs/2312.09818](https://arxiv.org/abs/2312.09818)

    本论文介绍了一个新任务，即视频笑声推理，旨在解释特定视频中人们笑的原因，并提出了数据集SMILE。通过利用大型语言模型生成文本视频表示，我们的基线能够生成合理的笑声解释。

    

    尽管人工智能最近取得了进展，但构建社交智能仍然是一个挑战。其中，笑声是人类社交互动中发生的独特表达之一。在这项工作中，我们面对了机器理解视频中笑声背后理由的新挑战，即视频笑声推理。我们介绍了这一新任务，解释人们在特定视频中为什么会笑的原因，并提出了用于这一任务的数据集SMILE。我们提出了一个基线，通过利用大型语言模型（LLMs）的推理能力和文本视频表示生成合理的笑声解释。实验表明，我们的基线可以生成可信的笑声解释。我们进一步探讨了我们的基线在探测其他视频理解任务和野外视频方面的可扩展性。我们发布了我们的数据集、代码和模型。

    arXiv:2312.09818v2 Announce Type: replace-cross  Abstract: Despite the recent advances of the artificial intelligence, building social intelligence remains a challenge. Among social signals, laughter is one of the distinctive expressions that occurs during social interactions between humans. In this work, we tackle a new challenge for machines to understand the rationale behind laughter in video, Video Laugh Reasoning. We introduce this new task to explain why people laugh in a particular video and a dataset for this task. Our proposed dataset, SMILE, comprises video clips and language descriptions of why people laugh. We propose a baseline by leveraging the reasoning capacity of large language models (LLMs) with textual video representation. Experiments show that our baseline can generate plausible explanations for laughter. We further investigate the scalability of our baseline by probing other video understanding tasks and in-the-wild videos. We release our dataset, code, and model 
    
[^207]: 协作基础模型用于领域泛化的语义分割

    Collaborating Foundation Models for Domain Generalized Semantic Segmentation

    [https://arxiv.org/abs/2312.09788](https://arxiv.org/abs/2312.09788)

    提出了一种协作基础模型框架（CLOUDS）用于领域泛化语义分割，集成了CLIP骨干、生成模型和“Segment Anything Model”，以实现对各种目标分布模式的覆盖和预测改进。

    

    领域泛化语义分割（DGSS）涉及在标记的源域上训练模型，旨在在推断过程中推广到未见过的领域。现有的DGSS方法通常通过域随机化（DR）实现强健特征。这种方法通常存在局限性，因为它只能考虑样式多样性而不是内容。在这项工作中，我们采用了一种正交的DGSS方法，即利用一系列协作基础模型用于领域泛化的语义分割（CLOUDS）。具体而言，CLOUDS是一个集成了各种基础模型的框架：（i）CLIP骨干用于其强健特征表示，（ii）生成模型用于使内容多样化，从而涵盖可能目标分布的各种模式，以及（iii）使用“Segment Anything Model”（SAM）迭代地改进分割模型预测。大量实验表明，我们的CLOUDS表现突出。

    arXiv:2312.09788v2 Announce Type: replace-cross  Abstract: Domain Generalized Semantic Segmentation (DGSS) deals with training a model on a labeled source domain with the aim of generalizing to unseen domains during inference. Existing DGSS methods typically effectuate robust features by means of Domain Randomization (DR). Such an approach is often limited as it can only account for style diversification and not content. In this work, we take an orthogonal approach to DGSS and propose to use an assembly of CoLlaborative FOUndation models for Domain Generalized Semantic Segmentation (CLOUDS). In detail, CLOUDS is a framework that integrates FMs of various kinds: (i) CLIP backbone for its robust feature representation, (ii) generative models to diversify the content, thereby covering various modes of the possible target distribution, and (iii) Segment Anything Model (SAM) for iteratively refining the predictions of the segmentation model. Extensive experiments show that our CLOUDS excels
    
[^208]: LatentEditor: 文本驱动的三维场景局部编辑

    LatentEditor: Text Driven Local Editing of 3D Scenes

    [https://arxiv.org/abs/2312.09313](https://arxiv.org/abs/2312.09313)

    LatentEditor 是一个创新框架，通过文本提示实现对神经场进行精确和局部受控编辑，将真实场景嵌入潜在空间，提供比传统方法更快更具适应性的编辑NeRF骨干。引入了增量分数和像素级评分方法以提高编辑精度。

    

    尽管神经场在视图合成和场景重建方面取得了重要进展，但由于它们隐含地从多视图输入编码几何和纹理信息，编辑它们仍然是一个巨大挑战。在本文中，我们介绍了\textsc{LatentEditor}，这是一个创新性框架，旨在赋予用户使用文本提示执行神经场的精确和局部受控编辑的能力。利用去噪扩散模型，我们成功地将真实世界场景嵌入潜在空间，从而相较于传统方法，对NeRF骨干进行更快更具适应性的编辑。为了增强编辑精度，我们引入了一个增量分数来计算潜在空间中的2D掩码，作为局部修改的指南，同时保留不相关区域。我们的新颖的像素级评分方法利用了InstructPix2Pix (IP2P)的能力，以辨别 IP2 之间的差异。

    arXiv:2312.09313v3 Announce Type: replace-cross  Abstract: While neural fields have made significant strides in view synthesis and scene reconstruction, editing them poses a formidable challenge due to their implicit encoding of geometry and texture information from multi-view inputs. In this paper, we introduce \textsc{LatentEditor}, an innovative framework designed to empower users with the ability to perform precise and locally controlled editing of neural fields using text prompts. Leveraging denoising diffusion models, we successfully embed real-world scenes into the latent space, resulting in a faster and more adaptable NeRF backbone for editing compared to traditional methods. To enhance editing precision, we introduce a delta score to calculate the 2D mask in the latent space that serves as a guide for local modifications while preserving irrelevant regions. Our novel pixel-level scoring approach harnesses the power of InstructPix2Pix (IP2P) to discern the disparity between IP2
    
[^209]: 使用大语言模型为Minecraft自动设计稠密奖励的Auto MC-Reward

    Auto MC-Reward: Automated Dense Reward Design with Large Language Models for Minecraft

    [https://arxiv.org/abs/2312.09238](https://arxiv.org/abs/2312.09238)

    本文介绍了一种名为Auto MC-Reward的先进学习系统，利用大型语言模型自动设计稠密奖励函数来提高学习效率

    

    许多强化学习环境（例如Minecraft）仅提供指示任务完成或失败的稀疏奖励，这些奖励以二进制值表示。在这种环境中探索效率的挑战使得基于强化学习的代理程序难以学习复杂任务。为了解决这个问题，本文介绍了一种名为Auto MC-Reward的先进学习系统，利用大型语言模型（LLMs）自动设计稠密奖励函数，从而提高学习效率。Auto MC-Reward包括三个重要组件：奖励设计者、奖励评论家和轨迹分析器。给定环境信息和任务描述，奖励设计者首先通过编写可执行的Python函数和预定义的观测输入来设计奖励函数。然后，我们的奖励评论家将负责验证代码，检查代码是否自洽且无语法错误。

    arXiv:2312.09238v2 Announce Type: replace  Abstract: Many reinforcement learning environments (e.g., Minecraft) provide only sparse rewards that indicate task completion or failure with binary values. The challenge in exploration efficiency in such environments makes it difficult for reinforcement-learning-based agents to learn complex tasks. To address this, this paper introduces an advanced learning system, named Auto MC-Reward, that leverages Large Language Models (LLMs) to automatically design dense reward functions, thereby enhancing the learning efficiency. Auto MC-Reward consists of three important components: Reward Designer, Reward Critic, and Trajectory Analyzer. Given the environment information and task descriptions, the Reward Designer first design the reward function by coding an executable Python function with predefined observation inputs. Then, our Reward Critic will be responsible for verifying the code, checking whether the code is self-consistent and free of syntax 
    
[^210]: 蜜蜂：多模态LLM的增强局部投影仪

    Honeybee: Locality-enhanced Projector for Multimodal LLM

    [https://arxiv.org/abs/2312.06742](https://arxiv.org/abs/2312.06742)

    该研究提出了一种既灵活又增强局部性的新型投影仪设计，有助于连接视觉编码器和语言模型，提高模型的效率和空间理解。

    

    在多模态大型语言模型（MLLMs）中，视觉投影仪在连接预先训练的视觉编码器与LLMs之间发挥着至关重要的作用，实现深入的视觉理解并利用LLMs的强大能力。尽管视觉投影仪的重要性不言而喻，但研究相对较少。在这项研究中，我们首先确定了两个关键的投影仪属性：（i）灵活性以管理视觉代币的数量，对于MLLMs的整体效率至关重要；（ii）保留来自视觉特征的局部上下文，对于空间理解至关重要。基于这些发现，我们提出了一种既灵活又增强局部性的新型投影仪设计，有效地满足了这两种理想属性。此外，我们提出了全面的策略，以有效利用多个和多方面的指导数据集。通过广泛的实验，我们检验了各种设计选择的影响。

    arXiv:2312.06742v2 Announce Type: replace-cross  Abstract: In Multimodal Large Language Models (MLLMs), a visual projector plays a crucial role in bridging pre-trained vision encoders with LLMs, enabling profound visual understanding while harnessing the LLMs' robust capabilities. Despite the importance of the visual projector, it has been relatively less explored. In this study, we first identify two essential projector properties: (i) flexibility in managing the number of visual tokens, crucial for MLLMs' overall efficiency, and (ii) preservation of local context from visual features, vital for spatial understanding. Based on these findings, we propose a novel projector design that is both flexible and locality-enhanced, effectively satisfying the two desirable properties. Additionally, we present comprehensive strategies to effectively utilize multiple and multifaceted instruction datasets. Through extensive experiments, we examine the impact of individual design choices. Finally, o
    
[^211]: 通过解耦图像的风格和虚假特征来实现不变表示

    Invariant Representation via Decoupling Style and Spurious Features from Images

    [https://arxiv.org/abs/2312.06226](https://arxiv.org/abs/2312.06226)

    本文提出了一种新的框架IRSS，通过解耦图像的风格和虚假特征，实现了在缺失域标签情况下的超出分布（OOD）泛化。

    

    本文考虑当风格分布转移和虚假特征同时存在且缺失域标签的情况下的超出分布（OOD）泛化问题。为应对这一挑战，我们首先提出了一个结构因果模型（SCM）来捕捉风格分布转移和虚假特征，进而设计了一个名为IRSS的新框架，通过引入对抗神经网络和多环境优化逐渐从图像中分离风格分布和虚假特征，实现了OOD泛化。

    arXiv:2312.06226v2 Announce Type: replace-cross  Abstract: This paper considers the out-of-distribution (OOD) generalization problem under the setting that both style distribution shift and spurious features exist and domain labels are missing. This setting frequently arises in real-world applications and is underlooked because previous approaches mainly handle either of these two factors. The critical challenge is decoupling style and spurious features in the absence of domain labels. To address this challenge, we first propose a structural causal model (SCM) for the image generation process, which captures both style distribution shift and spurious features. The proposed SCM enables us to design a new framework called IRSS, which can gradually separate style distribution and spurious features from images by introducing adversarial neural networks and multi-environment optimization, thus achieving OOD generalization. Moreover, it does not require additional supervision (e.g., domain l
    
[^212]: 朝向学习通用模型的体验导航

    Towards Learning a Generalist Model for Embodied Navigation

    [https://arxiv.org/abs/2312.02010](https://arxiv.org/abs/2312.02010)

    提出了首个面向体验导航的通用模型NaviLLM，通过引入基于模式的指导，将LLMs应用到体验导航中，实现了对不同任务的统一处理。

    

    建立一个可以与世界互动的通用代理是人工智能系统的引人注目目标，因此激发了关于体验导航的研究，其中代理需要根据指示进行导航或回答查询。尽管取得了主要进展，先前的工作主要集中在特定任务的代理上，缺乏对未见场景的泛化能力。最近，大型语言模型（LLMs）在各个领域展现出卓越的能力，并为体验导航提供了机会。基于此，我们提出了首个面向体验导航的通用模型NaviLLM。它通过引入基于模式的指导来将LLMs调整到体验导航中。基于模式的指导灵活地将各种任务转化为生成问题，从而统一了各种任务。这种方法使我们能够将来自各个数据集的多样化数据源集成到训练中，为NaviLLM配备了广泛的能力。

    arXiv:2312.02010v3 Announce Type: replace-cross  Abstract: Building a generalist agent that can interact with the world is the intriguing target of AI systems, thus spurring the research for embodied navigation, where an agent is required to navigate according to instructions or respond to queries. Despite the major progress attained, previous works primarily focus on task-specific agents and lack generalizability to unseen scenarios. Recently, LLMs have presented remarkable capabilities across various fields, and provided a promising opportunity for embodied navigation. Drawing on this, we propose the first generalist model for embodied navigation, NaviLLM. It adapts LLMs to embodied navigation by introducing schema-based instruction. The schema-based instruction flexibly casts various tasks into generation problems, thereby unifying a wide range of tasks. This approach allows us to integrate diverse data sources from various datasets into the training, equipping NaviLLM with a wide r
    
[^213]: WATonoBus：一种全天候自动巡航车

    WATonoBus: An All Weather Autonomous Shuttle

    [https://arxiv.org/abs/2312.00938](https://arxiv.org/abs/2312.00938)

    提出了一种考虑恶劣天气的多模块和模块化系统架构，在WATonoBus平台上进行了实际测试，证明其能够解决全天候自动驾驶车辆面临的挑战

    

    自动驾驶车辆在全天候运行中面临显著挑战，涵盖了从感知和决策到路径规划和控制的各个模块。复杂性源于需要解决像雨、雪和雾等恶劣天气条件在自主性堆栈中的问题。传统的基于模型和单模块方法通常缺乏与上游或下游任务的整体集成。我们通过提出一个考虑恶劣天气的多模块和模块化系统架构来解决这个问题，涵盖了从感知水平到决策和安全监测的各个方面，例如覆盖雪的路缘检测。通过在WATonoBus平台上每周日常服务近一年，我们展示了我们提出的方法能够解决恶劣天气条件，并从运营中观察到的极端情况中获得宝贵的经验教训。

    arXiv:2312.00938v1 Announce Type: cross  Abstract: Autonomous vehicle all-weather operation poses significant challenges, encompassing modules from perception and decision-making to path planning and control. The complexity arises from the need to address adverse weather conditions like rain, snow, and fog across the autonomy stack. Conventional model-based and single-module approaches often lack holistic integration with upstream or downstream tasks. We tackle this problem by proposing a multi-module and modular system architecture with considerations for adverse weather across the perception level, through features such as snow covered curb detection, to decision-making and safety monitoring. Through daily weekday service on the WATonoBus platform for almost a year, we demonstrate that our proposed approach is capable of addressing adverse weather conditions and provide valuable learning from edge cases observed during operation.
    
[^214]: 基于对比去噪分数的文本引导潜在扩散图像编辑

    Contrastive Denoising Score for Text-guided Latent Diffusion Image Editing

    [https://arxiv.org/abs/2311.18608](https://arxiv.org/abs/2311.18608)

    提出了一种名为Contrastive Denoising Score (CDS) 的简单但强大的修改版本，用于潜在扩散模型 (LDM)，在DDS框架内使用了CUT损失来更好地保留原始图像的特定结构元素。

    

    随着文本到图像扩散模型的显著出现，图像编辑方法变得更加多样化并不断发展。在这个领域中一种很有前景的最近方法是Delta Denoising Score (DDS) - 一种基于Score Distillation Sampling (SDS)框架的图像编辑技术，它利用了文本到图像扩散模型的丰富生成先验。然而，仅依赖评分函数之间的差异不足以保留原始图像的特定结构元素，这是图像编辑的关键方面之一。为了解决这个问题，我们提出了 DDS 的一个尴尬简单但非常强大的修改版本，称为Contrastive Denoising Score (CDS)，用于潜在扩散模型 (LDM)。受到 DDS 和无配对图像到图像翻译的对比学习 (CUT) 之间的相似性和差异的启发，我们介绍了一种简单直接的方法，在DDS框架内使用CUT损失。

    arXiv:2311.18608v2 Announce Type: replace-cross  Abstract: With the remarkable advent of text-to-image diffusion models, image editing methods have become more diverse and continue to evolve. A promising recent approach in this realm is Delta Denoising Score (DDS) - an image editing technique based on Score Distillation Sampling (SDS) framework that leverages the rich generative prior of text-to-image diffusion models. However, relying solely on the difference between scoring functions is insufficient for preserving specific structural elements from the original image, a crucial aspect of image editing. To address this, here we present an embarrassingly simple yet very powerful modification of DDS, called Contrastive Denoising Score (CDS), for latent diffusion models (LDM). Inspired by the similarities and differences between DDS and the contrastive learning for unpaired image-to-image translation(CUT), we introduce a straightforward approach using CUT loss within the DDS framework. Ra
    
[^215]: 画图输入法编辑器：系统化输入识别的全面数据集和方法论

    Sketch Input Method Editor: A Comprehensive Dataset and Methodology for Systematic Input Recognition

    [https://arxiv.org/abs/2311.18254](https://arxiv.org/abs/2311.18254)

    该研究提出了一个专为专业C4I系统设计的画图输入法编辑器（SketchIME），并通过引入少样本域自适应和增量学习来提高网络性能和适应新用户。

    

    随着触摸屏设备的日益普及，自由手绘已经成为人机交互中一种有前途的方式。尽管先前的研究已经着眼于识别、检索和生成熟悉的日常物体等任务，但本研究旨在创建一个专门为专业C4I系统设计的画图输入法编辑器（SketchIME）。在该系统中，草图被用作低保真原型，用于推荐在创建全面情景地图时的标准化符号。本文还提出了一个包含374种专业草图类型的系统化数据集，并提出了一种具有多级监督的同时识别和分割架构，以改善性能并增强可解释性。通过引入少样本域自适应和增量学习，网络能够适应新用户

    arXiv:2311.18254v2 Announce Type: replace-cross  Abstract: With the recent surge in the use of touchscreen devices, free-hand sketching has emerged as a promising modality for human-computer interaction. While previous research has focused on tasks such as recognition, retrieval, and generation of familiar everyday objects, this study aims to create a Sketch Input Method Editor (SketchIME) specifically designed for a professional C4I system. Within this system, sketches are utilized as low-fidelity prototypes for recommending standardized symbols in the creation of comprehensive situation maps. This paper also presents a systematic dataset comprising 374 specialized sketch types, and proposes a simultaneous recognition and segmentation architecture with multilevel supervision between recognition and segmentation to improve performance and enhance interpretability. By incorporating few-shot domain adaptation and class-incremental learning, the network's ability to adapt to new users and
    
[^216]: 大型多模态模型的组合式思维提示

    Compositional Chain-of-Thought Prompting for Large Multimodal Models

    [https://arxiv.org/abs/2311.17076](https://arxiv.org/abs/2311.17076)

    提出了一种新颖的零样式思维提示（CCoT），以克服大型多模态模型难以捕捉到组合视觉推理方面的细节的问题。

    

    强大的视觉骨干和大规模语言模型(LLM)推理的结合已经导致大型多模态模型(LMM)成为当前广泛视觉和语言(VL)任务的标准。然而，最近的研究表明，即使是最先进的LMM仍然难以捕捉到组合视觉推理方面的细节，比如对象之间的属性和关系。一种解决方案是利用场景图(SGs)——对象及其关系和属性的形式化表达，它已被广泛用作视觉和文本领域之间的桥梁。然而，场景图数据需要场景图注释，这种数据收集成本高昂，因此难以扩展。此外，基于场景图数据微调LMM可能导致预训练目标的灾难性遗忘。为了克服这一问题，受到思维链方法的启发，我们提出了一种新颖的零样式思维提示（CCoT）。

    arXiv:2311.17076v2 Announce Type: replace-cross  Abstract: The combination of strong visual backbones and Large Language Model (LLM) reasoning has led to Large Multimodal Models (LMMs) becoming the current standard for a wide range of vision and language (VL) tasks. However, recent research has shown that even the most advanced LMMs still struggle to capture aspects of compositional visual reasoning, such as attributes and relationships between objects. One solution is to utilize scene graphs (SGs)--a formalization of objects and their relations and attributes that has been extensively used as a bridge between the visual and textual domains. Yet, scene graph data requires scene graph annotations, which are expensive to collect and thus not easily scalable. Moreover, finetuning an LMM based on SG data can lead to catastrophic forgetting of the pretraining objective. To overcome this, inspired by chain-of-thought methods, we propose Compositional Chain-of-Thought (CCoT), a novel zero-sho
    
[^217]: 发现有效的土地利用规划政策

    Discovering Effective Policies for Land-Use Planning

    [https://arxiv.org/abs/2311.12304](https://arxiv.org/abs/2311.12304)

    通过学习代理模型并使用进化搜索过程，发现了可定制到不同位置的有效土地利用政策，为土地利用规划提供了一个潜在有用的工具。

    

    土地被分配给不同的用途，如森林、城市区域和农业，对陆地碳平衡和气候变化有重大影响。基于可用的土地利用变化的历史数据和相关的碳排放和吸收的模拟，可以学习到一个代理模型，从而能够高效评估决策者可选择的不同选项。然后可以使用进化搜索过程来发现特定位置的有效土地利用政策。该系统构建在Project Resilience平台上，并使用Land-Use Harmonization数据集LUH2和簿记模型BLUE进行评估。它生成可定制到不同位置的碳影响和土地利用变化量的帕累托前沿，从而为土地利用规划提供了一个潜在有用的工具。

    How areas of land are allocated for different uses, such as forests, urban areas, and agriculture, has a large effect on the terrestrial carbon balance, and therefore climate change. Based on available historical data on land-use changes and a simulation of the associated carbon emissions and removals, a surrogate model can be learned that makes it possible to evaluate the different options available to decision-makers efficiently. An evolutionary search process can then be used to discover effective land-use policies for specific locations. Such a system was built on the Project Resilience platform and evaluated with the Land-Use Harmonization dataset LUH2 and the bookkeeping model BLUE. It generates Pareto fronts that trade off carbon impact and amount of land-use change customized to different locations, thus providing a potentially useful tool for land-use planning.
    
[^218]: OVM，结果监督价值模型用于数学推理规划

    OVM, Outcome-supervised Value Models for Planning in Mathematical Reasoning

    [https://arxiv.org/abs/2311.09724](https://arxiv.org/abs/2311.09724)

    引入Outcome-supervised Value Model (OVM)利用结果监督训练价值模型，以在数学推理中减少错误传播，将任务转化为价值估计问题。

    

    大型语言模型（LLMs）经常在多个推理步骤中保持准确性方面遇到困难，特别是在数学推理中，早期步骤中的错误可能传播到后续步骤，最终导致错误答案。为了减少错误传播，引入了引导解码以逐步指导LM解码。我们认为，在引导解码中，评估不完整推理路径的潜力可能比仅确保每个步骤的正确性更有优势，因为前一种方法会导向正确的最终答案。这将任务转化为计划中的价值估计问题。受到发现的启发，即$\textit{引导解码的结果监督本质上充当价值模型}$，我们提出了一种Outcome-supervised Value Model (OVM)，它采用结果监督来训练价值模型，优先考虑导致准确性的步骤。

    arXiv:2311.09724v2 Announce Type: replace  Abstract: Large language models (LLMs) often struggle with maintaining accuracy throughout multiple multiple reasoning steps, especially in mathematical reasoning where an error in earlier steps can propagate to subsequent ones and it ultimately leading to an incorrect answer. To reduce error propagation, guided decoding is employed to direct the LM decoding on a step-by-step basis. We argue that in guided decoding, assessing the potential of an incomplete reasoning path can be more advantageous than simply ensuring per-step correctness, as the former approach leads towards a correct final answer. This transforms the task into a $\textit{value estimation}$ problem in planning.   Inspired by the findings that $\textit{outcome supervision for guided decoding essentially acts as a value model}$, we propose Outcome-supervised Value Model (OVM) that employs outcome supervision for training a value model, which prioritizes steps that lead to accurat
    
[^219]: ARES: 用于检索增强生成系统的自动化评估框架

    ARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems

    [https://arxiv.org/abs/2311.09476](https://arxiv.org/abs/2311.09476)

    ARES是用于评估检索增强生成系统的自动化评估框架，通过创建合成训练数据和微调评估器，有效评估RAG系统在不同任务中的表现。

    

    评估检索增强生成（RAG）系统传统上依赖于手动注释输入查询、检索段落和生成响应。我们引入了ARES，一个用于评估RAG系统的自动化评估系统，评估维度包括上下文相关性、答案忠实度和答案相关性。通过创建自己的合成训练数据，ARES微调轻量级LM评估器以评估单个RAG组件的质量。为了减少潜在的预测错误，ARES利用少量人工注释数据集进行预测驱动推理（PPI）。在KILT、SuperGLUE和AIS的八个不同知识密集型任务中，ARES在评估过程中仅使用少量人工注释就准确评估RAG系统。此外，ARES评估器在领域转移中仍然有效，即使在更改用于评估的查询和/或文档类型后仍然准确。

    arXiv:2311.09476v2 Announce Type: replace-cross  Abstract: Evaluating retrieval-augmented generation (RAG) systems traditionally relies on hand annotations for input queries, passages to retrieve, and responses to generate. We introduce ARES, an Automated RAG Evaluation System, for evaluating RAG systems along the dimensions of context relevance, answer faithfulness, and answer relevance. By creating its own synthetic training data, ARES finetunes lightweight LM judges to assess the quality of individual RAG components. To mitigate potential prediction errors, ARES utilizes a small set of human-annotated datapoints for prediction-powered inference (PPI). Across eight different knowledge-intensive tasks in KILT, SuperGLUE, and AIS, ARES accurately evaluates RAG systems while using only a few hundred human annotations during evaluation. Furthermore, ARES judges remain effective across domain shifts, proving accurate even after changing the type of queries and/or documents used in the eva
    
[^220]: AbsPyramid：使用统一的蕴涵图评估语言模型的抽象能力

    AbsPyramid: Benchmarking the Abstraction Ability of Language Models with a Unified Entailment Graph

    [https://arxiv.org/abs/2311.09174](https://arxiv.org/abs/2311.09174)

    该研究提出了AbsPyramid，一个统一的蕴涵图，用于评估语言模型的抽象能力，实验证明LLMs可以通过在丰富抽象知识上进行训练来获得基本的抽象能力，并泛化到未见的事件。

    

    认知研究表明，抽象能力对人类智能至关重要，而这在语言模型中仍然没有得到充分探讨。本文介绍了AbsPyramid，这是一个包含221K个文本描述的统一蕴涵图，用于收集广泛事件三个组成部分的抽象知识，以全面评估语言模型在开放领域中的抽象能力。实验结果表明，目前的LLMs在零短和少量数据情况下面临理解抽象知识的挑战。通过在我们丰富的抽象知识上进行训练，我们发现LLMs可以获得基本的抽象能力，并泛化到未见的事件。同时，我们实证表明我们的基准可以全面提高LLMs在两个先前的抽象任务中的性能。

    arXiv:2311.09174v3 Announce Type: replace-cross  Abstract: Cognitive research indicates that abstraction ability is essential in human intelligence, which remains under-explored in language models. In this paper, we present AbsPyramid, a unified entailment graph of 221K textual descriptions of abstraction knowledge. While existing resources only touch nouns or verbs within simplified events or specific domains, AbsPyramid collects abstract knowledge for three components of diverse events to comprehensively evaluate the abstraction ability of language models in the open domain. Experimental results demonstrate that current LLMs face challenges comprehending abstraction knowledge in zero-shot and few-shot settings. By training on our rich abstraction knowledge, we find LLMs can acquire basic abstraction abilities and generalize to unseen events. In the meantime, we empirically show that our benchmark is comprehensive to enhance LLMs across two previous abstraction tasks.
    
[^221]: 在大型语言模型中识别线性关系概念

    Identifying Linear Relational Concepts in Large Language Models

    [https://arxiv.org/abs/2311.08968](https://arxiv.org/abs/2311.08968)

    通过线性关系概念(LRC)技术，可以在大型语言模型的隐藏激活潜在空间中找到与人类可解释概念对应的概念方向，这种技术超越了标准黑盒探测分类器。

    

    Transformer语言模型(LMs)已被证明可以将概念表示为隐藏激活的潜在空间中的方向。然而，对于任何可由人类解释的概念，我们如何在潜在空间中找到其方向呢？我们提出了一种称为线性关系概念(LRC)的技术，通过首先建模主体和客体之间的关系为线性关系嵌入(LRE)来找到与人类可解释概念对应的概念方向。我们发现，反转LRE并使用较早的客体层会导致一种强大的技术，用于找到胜过标准黑盒探测分类器的概念方向。我们评估了LRC作为概念分类器的性能，以及它们改变模型输出的因果能力。

    arXiv:2311.08968v2 Announce Type: replace-cross  Abstract: Transformer language models (LMs) have been shown to represent concepts as directions in the latent space of hidden activations. However, for any human-interpretable concept, how can we find its direction in the latent space? We present a technique called linear relational concepts (LRC) for finding concept directions corresponding to human-interpretable concepts by first modeling the relation between subject and object as a linear relational embedding (LRE). We find that inverting the LRE and using earlier object layers results in a powerful technique for finding concept directions that outperforms standard black-box probing classifiers. We evaluate LRCs on their performance as concept classifiers as well as their ability to causally change model output.
    
[^222]: Safer-Instruct: 使用自动化偏好数据对齐语言模型

    Safer-Instruct: Aligning Language Models with Automated Preference Data

    [https://arxiv.org/abs/2311.08685](https://arxiv.org/abs/2311.08685)

    Safer-Instruct通过反向指导调整、指导感应和专家模型评估的方式，实现了自动构建大规模偏好数据的目的，从而在没有人工标注者的情况下高效生成高质量的偏好数据。

    

    人工反馈强化学习（RLHF）是增强语言模型能力的重要策略。然而，为RLHF标注偏好数据是一项资源密集且需要创造力的过程，而现有的自动生成方法在数据多样性和质量方面存在局限性。为了应对这一挑战，我们提出了Safer-Instruct，这是一个用于自动构建大规模偏好数据的全新流水线。我们的方法利用了反向指导调整、指导感应和专家模型评估，以高效生成高质量的偏好数据，无需人工标注者。为了验证Safer-Instruct的有效性，我们将该流水线应用于构建一个安全偏好数据集作为案例研究。在这个合成数据集上微调Alpaca模型不仅展示出更好的无害性，还表现出优于在人工标注的安全偏好数据上微调的模型，同时保持

    arXiv:2311.08685v2 Announce Type: replace-cross  Abstract: Reinforcement learning from human feedback (RLHF) is a vital strategy for enhancing model capability in language models. However, annotating preference data for RLHF is a resource-intensive and creativity-demanding process, while existing automatic generation methods face limitations in data diversity and quality. In response, we present Safer-Instruct, a novel pipeline for automatically constructing large-scale preference data. Our approach leverages reversed instruction tuning, instruction induction, and expert model evaluation to efficiently generate high-quality preference data without human annotators. To verify the effectiveness of Safer-Instruct, we apply the pipeline to construct a safety preference dataset as a case study. Finetuning an Alpaca model on this synthetic dataset not only demonstrates improved harmlessness but also outperforms models fine-tuned on human-annotated safety preference data, all the while mainta
    
[^223]: 低秩适应用于多语言摘要：一项实证研究

    Low-Rank Adaptation for Multilingual Summarization: An Empirical Study

    [https://arxiv.org/abs/2311.08572](https://arxiv.org/abs/2311.08572)

    LoRA 在多语言摘要方面竞争力强，特别在低数据情况和跨语言转移方面表现出色。

    

    尽管预训练的大型语言模型的进展显著加速了近年来自然语言处理领域的进步，但它们不断增长的体积对传统的微调提出了重要挑战，特别是在内存密集型任务中。我们调查了参数高效微调的潜力，重点是低秩适应（LoRA），涉及多语言摘要领域，这是一个具有挑战性的任务（因为输入通常很长），且相对未被充分探索。我们进行了一项广泛的研究，涵盖不同数据可用性场景，包括高数据和低数据设置，以及跨语言转移，利用不同规模的模型。我们的发现表明，当使用大量数据训练时，LoRA与完全微调竞争激烈，并且在低数据情况和跨语言转移方面表现出色。我们还研究了不同的少数据点跨语言转移策略，发现持续的LoRA调优...

    arXiv:2311.08572v2 Announce Type: replace-cross  Abstract: Although the advancements of pre-trained Large Language Models have significantly accelerated recent progress in NLP, their ever-increasing size poses significant challenges for conventional fine-tuning, especially in memory-intensive tasks. We investigate the potential of Parameter-Efficient Fine-Tuning, focusing on Low-Rank Adaptation (LoRA), in the domain of multilingual summarization, a task that is both challenging (due to typically long inputs), and relatively unexplored. We conduct an extensive study across different data availability scenarios, including high- and low-data settings, and cross-lingual transfer, leveraging models of different sizes. Our findings reveal that LoRA is competitive with full fine-tuning when trained with high quantities of data, and excels in low-data scenarios and cross-lingual transfer. We also study different strategies for few-shot cross-lingual transfer, finding that continued LoRA tuning
    
[^224]: 评价图神经网络的邻居可解释性

    Evaluating Neighbor Explainability for Graph Neural Networks

    [https://arxiv.org/abs/2311.08118](https://arxiv.org/abs/2311.08118)

    评价图神经网络中邻居的可解释性，提出新的度量标准并发现基于梯度的方法在GNN领域的解释没有太大差异，同时发现很多技术在没有自环的GNNs下无法准确识别重要邻居。

    

    图神经网络（GNNs）中的可解释性是近年来新兴领域。在这篇文章中，我们解决了一个问题，即在对节点进行分类时，确定每个邻居对于 GNN 的重要性以及如何衡量这一特定任务的表现。为此，各种已知的可解释性方法被重新构造以获取邻居重要性，并提出了四种新的度量标准。我们的结果表明，在 GNN 领域，基于梯度的技术提供的解释几乎没有差异。此外，许多可解释性技术在使用没有自环的 GNNs 时未能识别重要的邻居。

    arXiv:2311.08118v2 Announce Type: replace-cross  Abstract: Explainability in Graph Neural Networks (GNNs) is a new field growing in the last few years. In this publication we address the problem of determining how important is each neighbor for the GNN when classifying a node and how to measure the performance for this specific task. To do this, various known explainability methods are reformulated to get the neighbor importance and four new metrics are presented. Our results show that there is almost no difference between the explanations provided by gradient-based techniques in the GNN domain. In addition, many explainability techniques failed to identify important neighbors when GNNs without self-loops are used.
    
[^225]: 火焰: 评估中国大型语言模型与人类价值观的契合性的基准测试

    Flames: Benchmarking Value Alignment of Chinese Large Language Models

    [https://arxiv.org/abs/2311.06899](https://arxiv.org/abs/2311.06899)

    中国的大型语言模型的价值观契合性需要更加全面的评估，该研究提出了一个名为"火焰"（Flames）的基准测试，涵盖了常见的无害原则和特定中国价值观，以及复杂场景和隐含恶意的提示方法。

    

    大型语言模型（LLMs）在各个地区的广泛应用强调了评估它们与人类价值观契合性的迫切性。然而，当前的基准测试未能有效地揭示LLMs中的安全漏洞。尽管许多模型在这些评估中得分很高，且“名列前茅”，但在LLMs与人类价值观的深层契合性和实现真正无害方面仍存在重大差距。为此，本文提出了一个名为"火焰"（Flames）的价值观契合性基准测试，该测试涵盖了常见的无害原则，以及一个整合了特定中国价值观如和谐的独特道德维度。因此，我们精心设计了包含复杂情境和大多带有隐含恶意的破解方法的对抗性提示。通过对17个主流LLMs进行提示，我们获得了模型的回应，并对其进行了详细评估。

    arXiv:2311.06899v2 Announce Type: replace-cross  Abstract: The widespread adoption of large language models (LLMs) across various regions underscores the urgent need to evaluate their alignment with human values. Current benchmarks, however, fall short of effectively uncovering safety vulnerabilities in LLMs. Despite numerous models achieving high scores and 'topping the chart' in these evaluations, there is still a significant gap in LLMs' deeper alignment with human values and achieving genuine harmlessness. To this end, this paper proposes a value alignment benchmark named Flames, which encompasses both common harmlessness principles and a unique morality dimension that integrates specific Chinese values such as harmony. Accordingly, we carefully design adversarial prompts that incorporate complex scenarios and jailbreaking methods, mostly with implicit malice. By prompting 17 mainstream LLMs, we obtain model responses and rigorously annotate them for detailed evaluation. Our findin
    
[^226]: 假对齐：LLMs真的对齐得好吗？

    Fake Alignment: Are LLMs Really Aligned Well?

    [https://arxiv.org/abs/2311.05915](https://arxiv.org/abs/2311.05915)

    研究发现大型语言模型（LLMs）在安全性评估中存在假对齐问题，提出了Fake alIgNment Evaluation (FINE)框架和两个新的度量标准，用于验证和修正这一现象

    

    随着对大型语言模型（LLMs）安全问题的关注日益增强，人们对安全评估产生了相当大的兴趣。本研究探讨了评估LLMs的一个未被充分研究的问题，即多项选择题和开放式问题之间的表现差异。受到越狱攻击模式研究的启发，我们认为这是由于泛化不匹配所引起的。也就是说，LLMs只记住了开放式安全问题的答案风格，这使其无法解决其他形式的安全测试。我们将这种现象称为假对齐，并构建了一个比较基准来在LLMs中经验性地验证其存在。我们引入了一个Fake alIgNment Evaluation (FINE)框架和两个新颖的度量标准--一致性分数（CS）和一致的安全分数（CSS），它们共同评估两种互补形式的评估，以量化假对齐并获得更正

    arXiv:2311.05915v3 Announce Type: replace-cross  Abstract: The growing awareness of safety concerns in large language models (LLMs) has sparked considerable interest in the evaluation of safety. This study investigates an under-explored issue about the evaluation of LLMs, namely the substantial discrepancy in performance between multiple-choice questions and open-ended questions. Inspired by research on jailbreak attack patterns, we argue this is caused by mismatched generalization. That is, LLM only remembers the answer style for open-ended safety questions, which makes it unable to solve other forms of safety tests. We refer to this phenomenon as fake alignment and construct a comparative benchmark to empirically verify its existence in LLMs. We introduce a Fake alIgNment Evaluation (FINE) framework and two novel metrics--Consistency Score (CS) and Consistent Safety Score (CSS), which jointly assess two complementary forms of evaluation to quantify fake alignment and obtain corrected
    
[^227]: DistillSpec：通过知识蒸馏改进投机性解码

    DistillSpec: Improving Speculative Decoding via Knowledge Distillation

    [https://arxiv.org/abs/2310.08461](https://arxiv.org/abs/2310.08461)

    DistillSpec通过知识蒸馏技术改进了投机性解码，在标准基准测试中取得了10-45%的速度提升。

    

    投机性解码（SD）通过使用更快的草稿模型生成多个标记，然后由更大的目标模型并行验证这些标记，从而生成符合目标模型分布的文本，加速大型语言模型推理。然而，找到与目标模型良好对齐的紧凑草稿模型具有挑战性。为了解决这个问题，我们提出了DistillSpec，它使用知识蒸馏来更好地将草稿模型与目标模型对齐，然后应用SD。DistillSpec做出了两个关键设计选择，我们通过系统研究证明这对改进草稿和目标对齐至关重要：利用来自草稿模型的on-policy数据生成，以及将发散函数定制到任务和解码策略。值得注意的是，DistillSpec在一系列标准基准测试上比标准SD获得了令人印象深刻的10-45%的加速，使用贪婪和非贪婪方法。

    arXiv:2310.08461v2 Announce Type: replace-cross  Abstract: Speculative decoding (SD) accelerates large language model inference by employing a faster draft model for generating multiple tokens, which are then verified in parallel by the larger target model, resulting in the text generated according to the target model distribution. However, identifying a compact draft model that is well-aligned with the target model is challenging. To tackle this issue, we propose DistillSpec that uses knowledge distillation to better align the draft model with the target model, before applying SD. DistillSpec makes two key design choices, which we demonstrate via systematic study to be crucial to improving the draft and target alignment: utilizing on-policy data generation from the draft model, and tailoring the divergence function to the task and decoding strategy. Notably, DistillSpec yields impressive 10 - 45% speedups over standard SD on a range of standard benchmarks, using both greedy and non-gr
    
[^228]: 迈向LogiGLUE：对语言模型逻辑推理能力的简要调查和基准的研究

    Towards LogiGLUE: A Brief Survey and A Benchmark for Analyzing Logical Reasoning Capabilities of Language Models

    [https://arxiv.org/abs/2310.00836](https://arxiv.org/abs/2310.00836)

    该研究旨在评估大型语言模型在逻辑推理中的表现，并提供了名为LogiGLUE的基准，以研究逻辑推理数据集、任务和利用语言模型进行推理的方法。

    

    逻辑推理对人类至关重要，但在人工智能领域中却是一个重大挑战。研究人员最初使用的知识表示与推理系统并不能很好地扩展，并且需要大量手动工作。最近，大型语言模型(LLMs)的出现表明了其能够克服形式化知识表示系统的各种局限性。因此，利用自然语言进行逻辑推理的LLMs备受关注。该工作旨在通过对此领域最新进展的简要回顾来了解LLMs在逻辑推理方面的熟练程度；重点关注逻辑推理数据集、任务以及采用的利用LLMs进行推理的方法。为了提供全面的分析，我们编制了一个名为LogiGLUE的基准。其中包括24个不同的数据集，涵盖演绎、进阶和归纳推理等方面。

    arXiv:2310.00836v3 Announce Type: replace-cross  Abstract: Logical reasoning is fundamental for humans yet presents a substantial challenge in the domain of Artificial Intelligence. Initially, researchers used Knowledge Representation and Reasoning (KR) systems that did not scale and required non-trivial manual effort. Recently, the emergence of large language models (LLMs) has demonstrated the ability to overcome various limitations of formal Knowledge Representation (KR) systems. Consequently, there's a growing interest in using LLMs for logical reasoning via natural language. This work strives to understand the proficiency of LLMs in logical reasoning by offering a brief review of the latest progress in this area; with a focus on the logical reasoning datasets, tasks, and the methods adopted to utilize LLMs for reasoning. To offer a thorough analysis, we have compiled a benchmark titled LogiGLUE. This includes 24 varied datasets encompassing deductive, abductive, and inductive reaso
    
[^229]: 我能相信你的回答吗？基于视觉的视频问答

    Can I Trust Your Answer? Visually Grounded Video Question Answering

    [https://arxiv.org/abs/2309.01327](https://arxiv.org/abs/2309.01327)

    通过研究基于视觉的视频问答，发现当前视觉语言模型在做出可靠预测方面存在局限性，并提出了一种改进方法。

    

    我们研究了基于视觉的视频问答，以应对利用预训练技术进行视频语言理解的新趋势。具体来说，通过迫使视觉语言模型（VLMs）回答问题并同时提供视觉证据，我们旨在确定这些技术的预测在多大程度上真正基于相关视频内容，而不是来自语言或无关视觉上下文的虚假相关性。为此，我们构建了NExT-GQA--一个带有10.5K时间定位（或位置）标签与原始QA对相关联的NExT-QA扩展。通过NExT-GQA，我们审查了一系列最先进的VLMs。通过事后注意力分析，我们发现这些模型在证实答案方面非常薄弱，尽管它们的QA性能强劲。这暴露了当前VLM在做出可靠预测方面的局限性。为此，我们进一步探讨并提出了一个解决方案

    arXiv:2309.01327v2 Announce Type: replace-cross  Abstract: We study visually grounded VideoQA in response to the emerging trends of utilizing pretraining techniques for video-language understanding. Specifically, by forcing vision-language models (VLMs) to answer questions and simultaneously provide visual evidence, we seek to ascertain the extent to which the predictions of such techniques are genuinely anchored in relevant video content, versus spurious correlations from language or irrelevant visual context. Towards this, we construct NExT-GQA -- an extension of NExT-QA with 10.5$K$ temporal grounding (or location) labels tied to the original QA pairs. With NExT-GQA, we scrutinize a series of state-of-the-art VLMs. Through post-hoc attention analysis, we find that these models are extremely weak in substantiating the answers despite their strong QA performance. This exposes the limitation of current VLMs in making reliable predictions. As a remedy, we further explore and propose a g
    
[^230]: 对大规模生成视觉-语言模型的组合性的研究

    An Examination of the Compositionality of Large Generative Vision-Language Models

    [https://arxiv.org/abs/2308.10509](https://arxiv.org/abs/2308.10509)

    本文检验了生成视觉-语言模型的组合性，发现当前基准中存在句法偏见，提出了新的评估指标SyntaxBias Score和新的基准SyntActically D。

    

    随着大型语言模型（LLMs）的成功，许多生成视觉-语言模型（GVLMs）通过多模态指导调整得以构建。然而，GVLMs在多模态组合推理中的性能还未得到充分探索。本文旨在检验评估GVLMs组合性的评估指标（VisualGPTScore等）和当前基准。我们发现当前基准中的句法偏见，被GVLMs的语言能力所利用。这种偏见使得VisualGPTScore成为评估GVLMs的不足指标。为了解决这一问题，我们首先引入了一个SyntaxBias Score，利用LLMs量化此类偏见以进行缓解。随后添加了一个具有挑战性的新任务，以评估GVLMs对固有倾向于句法正确性的健壮性。利用缓解偏见的数据集和新任务，我们提出了一个新的基准，即SyntActically D

    arXiv:2308.10509v2 Announce Type: replace-cross  Abstract: With the success of Large Language Models (LLMs), many Generative Vision-Language Models (GVLMs) have been constructed via multimodal instruction tuning. However, the performance of GVLMs in multimodal compositional reasoning remains under-explored. In this paper, we examine both the evaluation metrics (VisualGPTScore, etc.) and current benchmarks for evaluating the compositionality of GVLMs. We identify the syntactical bias in current benchmarks, which is exploited by the linguistic capability of GVLMs. The bias renders VisualGPTScore an insufficient metric for assessing GVLMs. To combat this, we first introduce a SyntaxBias Score, leveraging LLMs to quantify such bias for mitigation. A challenging new task is subsequently added to evaluate the robustness of GVLMs against inherent inclination toward syntactical correctness. Using the bias-mitigated datasets and the new task, we propose a novel benchmark, namely SyntActically D
    
[^231]: 引文：构建负责任和可计算的大型语言模型的关键

    Citation: A Key to Building Responsible and Accountable Large Language Models

    [https://arxiv.org/abs/2307.02185](https://arxiv.org/abs/2307.02185)

    引文被确定为大型语言模型中关键但缺失的组成部分，其引入可以增强内容的透明性和可验证性以应对知识产权和伦理问题，提议LLMs的全面引文机制应考虑非参数化和参数化内容，尽管实施引文机制复杂且存在潜在缺陷，仍主张推动其发展并概述未来研究问题。

    

    大型语言模型（LLMs）带来了变革性的好处，同时也带来了独特的挑战，包括知识产权（IP）和伦理关切。本文探讨了缓解这些风险的新思路，从LLMs和已建立的Web系统之间的相似性出发。我们确定了“引文” - 对来源或证据的承认或引用 - 在LLMs中的关键缺失组成部分。引入引文可以增强内容的透明性和可验证性，从而应对LLMs的知识产权和伦理问题。我们进一步提议，LLMs的全面引文机制应考虑非参数化和参数化内容。尽管实施这样的引文机制的复杂性及潜在缺陷，我们主张推动其发展。基于这一基础，我们概述了该领域中的几个研究问题，旨在引导未来的探索方向。

    arXiv:2307.02185v3 Announce Type: replace-cross  Abstract: Large Language Models (LLMs) bring transformative benefits alongside unique challenges, including intellectual property (IP) and ethical concerns. This position paper explores a novel angle to mitigate these risks, drawing parallels between LLMs and established web systems. We identify "citation" - the acknowledgement or reference to a source or evidence - as a crucial yet missing component in LLMs. Incorporating citation could enhance content transparency and verifiability, thereby confronting the IP and ethical issues in the deployment of LLMs. We further propose that a comprehensive citation mechanism for LLMs should account for both non-parametric and parametric content. Despite the complexity of implementing such a citation mechanism, along with the potential pitfalls, we advocate for its development. Building on this foundation, we outline several research problems in this area, aiming to guide future explorations towards
    
[^232]: BRAIxDet：学习使用不完整注释检测恶性乳腺病变

    BRAIxDet: Learning to Detect Malignant Breast Lesion with Incomplete Annotations

    [https://arxiv.org/abs/2301.13418](https://arxiv.org/abs/2301.13418)

    本文提出了一种中间解决方案，即将训练构建为弱监督和半监督学习问题，以解决通过不完整注释数据检测恶性乳腺病变的困境

    

    从筛查乳房X线照片中检测恶性病变的方法通常是使用具有完全注释的数据集进行训练，其中图像被标记为癌症病变的定位和分类。然而，真实世界的筛查乳房X线照片数据集通常有一个部分是完全注释的，另一个部分只有全局分类的弱注释（即没有病变定位）。鉴于这类数据集的庞大规模，研究人员通常在弱注释子集面临两难选择：要么不使用它，要么完全注释它。第一种选择会降低检测准确性，因为它没有使用整个数据集，而第二种选择则过于昂贵，因为注释需要专业放射科医师完成。在本文中，我们提出了这一困境的一个中间解决方案，即将训练构建为一个我们称之为恶性

    arXiv:2301.13418v3 Announce Type: replace-cross  Abstract: Methods to detect malignant lesions from screening mammograms are usually trained with fully annotated datasets, where images are labelled with the localisation and classification of cancerous lesions. However, real-world screening mammogram datasets commonly have a subset that is fully annotated and another subset that is weakly annotated with just the global classification (i.e., without lesion localisation). Given the large size of such datasets, researchers usually face a dilemma with the weakly annotated subset: to not use it or to fully annotate it. The first option will reduce detection accuracy because it does not use the whole dataset, and the second option is too expensive given that the annotation needs to be done by expert radiologists. In this paper, we propose a middle-ground solution for the dilemma, which is to formulate the training as a weakly- and semi-supervised learning problem that we refer to as malignant
    
[^233]: 深度神经网络中自组织产生1/f噪声

    Self-Organization Towards $1/f$ Noise in Deep Neural Networks

    [https://arxiv.org/abs/2301.08530](https://arxiv.org/abs/2301.08530)

    本研究发现在深度神经网络中训练的模型会产生类似生物神经网络的1/f噪声，但当神经网络过度容量时，激活模式会转向白噪声。

    

    $1/f$噪声，也称为粉红噪声，是生物神经网络中一个被广泛认可的现象，被认为在大脑信息处理中起着重要作用。本研究发现，在训练自然语言的深度神经网络中，也存在类似生物网络的1/f噪声。具体来说，我们在'IMDb' AI基准数据集上训练了长短时记忆（LSTM）网络，然后测量了神经元的激活。对不同神经元时间序列进行去趋势波动分析（DFA）表明明显的1/f模式，而在LSTM输入的时间序列中却不存在这种模式。有趣的是，当神经网络处于过度容量状态，拥有足够多的神经元来完成学习任务时，激活模式将偏离1/f噪声并转向白噪声。这是因为许多神经元未能有效地利用。

    arXiv:2301.08530v2 Announce Type: replace-cross  Abstract: The presence of $1/f$ noise, also known as pink noise, is a well-established phenomenon in biological neural networks, and is thought to play an important role in information processing in the brain. In this study, we find that such $1/f$ noise is also found in deep neural networks trained on natural language, resembling that of their biological counterparts. Specifically, we trained Long Short-Term Memory (LSTM) networks on the `IMDb' AI benchmark dataset, then measured the neuron activations. The detrended fluctuation analysis (DFA) on the time series of the different neurons demonstrate clear $1/f$ patterns, which is absent in the time series of the inputs to the LSTM. Interestingly, when the neural network is at overcapacity, having more than enough neurons to achieve the learning task, the activation patterns deviate from $1/f$ noise and shifts towards white noise. This is because many of the neurons are not effectively us
    
[^234]: 在离散动力系统中找到非平凡的最小不动点

    Finding Nontrivial Minimum Fixed Points in Discrete Dynamical Systems

    [https://arxiv.org/abs/2301.04090](https://arxiv.org/abs/2301.04090)

    在离散动力系统中，我们提出了一个优化问题，即寻找具有最小受影响节点数量的系统非平凡不动点，并发现了一些特殊情况下可以高效解决这个问题。

    

    网络离散动力系统常用于模拟传染病的传播和决策协调游戏中的代理。这种动力系统的不动点代表系统收敛到的配置。在传播不良传染病（如谣言和错误信息）方面，收敛到受影响节点数量较少的不动点是一个值得追求的目标。受这些考虑的启发，我们提出了一个新颖的优化问题，即寻找具有最小受影响节点数量的系统非平凡不动点。我们确定，除非P = NP，否则没有多项式时间算法可以在任意小于 n^1-\epsilon 的因子内近似求解这个问题。为了应对这种计算上的难题，我们确定了几种特殊情况，可以高效地解决这个问题。此外，我们引入了一个整数线性规划来解决这个问题。

    arXiv:2301.04090v4 Announce Type: replace-cross  Abstract: Networked discrete dynamical systems are often used to model the spread of contagions and decision-making by agents in coordination games. Fixed points of such dynamical systems represent configurations to which the system converges. In the dissemination of undesirable contagions (such as rumors and misinformation), convergence to fixed points with a small number of affected nodes is a desirable goal. Motivated by such considerations, we formulate a novel optimization problem of finding a nontrivial fixed point of the system with the minimum number of affected nodes. We establish that, unless P = NP, there is no polynomial time algorithm for approximating a solution to this problem to within the factor n^1-\epsilon for any constant epsilon > 0. To cope with this computational intractability, we identify several special cases for which the problem can be solved efficiently. Further, we introduce an integer linear program to addr
    
[^235]: 朝向通用化的行人轨迹预测系统——G-PECNet

    G-PECNet: Towards a Generalizable Pedestrian Trajectory Prediction System

    [https://arxiv.org/abs/2210.09846](https://arxiv.org/abs/2210.09846)

    通过结合周期激活函数的启发和使用隐马尔可夫模型（HMMs）和强化学习（RL）进行合成轨迹数据增强，我们的方法G-PECNet在行人轨迹预测任务中取得了9.5%的最终位移误差（FDE）改进。

    

    在本研究中，我们通过使用深度生成模型解决了自主无人机导航中的子问题，即预测域外人类和代理人轨迹。我们的方法——General-PECNet或G-PECNet，通过受周期激活函数启发的架构改进和使用隐马尔可夫模型（HMMs）和强化学习（RL）进行合成轨迹数据增强，在2020年基准PECNet上将最终位移误差（FDE）提高了9.5％。此外，我们提出了一个基于简单几何形状的度量方法，用于轨迹非线性和异常值检测，有助于该任务的完成。

    arXiv:2210.09846v3 Announce Type: replace  Abstract: Navigating dynamic physical environments without obstructing or damaging human assets is of quintessential importance for social robots. In this work, we solve autonomous drone navigation's sub-problem of predicting out-of-domain human and agent trajectories using a deep generative model. Our method: General-PECNet or G-PECNet observes an improvement of 9.5\% on the Final Displacement Error (FDE) on 2020's benchmark: PECNet through a combination of architectural improvements inspired by periodic activation functions and synthetic trajectory (data) augmentations using Hidden Markov Models (HMMs) and Reinforcement Learning (RL). Additionally, we propose a simple geometry-inspired metric for trajectory non-linearity and outlier detection, helpful for the task. Code available at https://github.com/Aryan-Garg/PECNet-Pedestrian-Trajectory-Prediction.git
    
[^236]: 运用XAI方法于基于EEG的系统应用研究

    Toward the application of XAI methods in EEG-based systems

    [https://arxiv.org/abs/2210.06554](https://arxiv.org/abs/2210.06554)

    本文旨在将合适的可解释人工智能（XAI）方法应用于解决脑-计算机接口（BCI）中的数据集转移问题，以提高在情绪识别中的EEG信号分类系统的泛化性能。

    

    众所周知，数据集转移问题的一个有趣案例是在脑-计算机接口（BCI）背景下对脑电图（EEG）信号进行分类。 EEG信号的非静止性可能导致BCI分类系统在不同会话中使用时性能泛化差，甚至是同一被试验。 本文的出发点是，通过利用合适的可解释人工智能（XAI）方法来定位和转换输入的相关特征，从而缓解数据集转移问题。 具体来说，我们专注于对几种XAI方法在在典型的用于情绪识别的EEG数据集上训练的ML系统上生成的解释的实验分析。结果表明，XAI方法找到的许多相关组件在会话之间是共享的，可以用来构建一个具有更好泛化能力的系统。

    arXiv:2210.06554v3 Announce Type: replace-cross  Abstract: An interesting case of the well-known Dataset Shift Problem is the classification of Electroencephalogram (EEG) signals in the context of Brain-Computer Interface (BCI). The non-stationarity of EEG signals can lead to poor generalisation performance in BCI classification systems used in different sessions, also from the same subject. In this paper, we start from the hypothesis that the Dataset Shift problem can be alleviated by exploiting suitable eXplainable Artificial Intelligence (XAI) methods to locate and transform the relevant characteristics of the input for the goal of classification. In particular, we focus on an experimental analysis of explanations produced by several XAI methods on an ML system trained on a typical EEG dataset for emotion recognition. Results show that many relevant components found by XAI methods are shared across the sessions and can be used to build a system able to generalise better. However, re
    
[^237]: 规范可解释人工智能（XAI）可能会损害消费者

    Regulating eXplainable Artificial Intelligence (XAI) May Harm Consumers

    [https://arxiv.org/abs/2209.03499](https://arxiv.org/abs/2209.03499)

    XAI regulation may be redundant and mandating fully transparent XAI may make firms and consumers worse off, revealing a tradeoff between maximizing welfare and receiving explainable AI outputs.

    

    最近的人工智能算法是黑匣子模型，其决策难以解释。可解释人工智能（XAI）是一类方法，旨在通过向客户解释他们的人工智能决策来解决缺乏人工智能可解释性和信任。本文通过一个博弈论模型挑战了监管人工智能通过规定完全透明XAI会导致更大社会福利的普遍观念。研究结果显示，XAI规制可能是多余的。事实上，强制规定完全透明XAI可能会使公司和消费者变得更糟。这揭示了在最大化福利和获得可解释人工智能输出之间的权衡。我们在方法和实质范畴上拓展了现有文献，并引入并研究了XAI公平的概念，即使在强制XAI的情况下也可能无法保证。

    arXiv:2209.03499v3 Announce Type: replace  Abstract: Recent AI algorithms are black box models whose decisions are difficult to interpret. eXplainable AI (XAI) is a class of methods that seek to address lack of AI interpretability and trust by explaining to customers their AI decisions. The common wisdom is that regulating AI by mandating fully transparent XAI leads to greater social welfare. Our paper challenges this notion through a game theoretic model of a policy-maker who maximizes social welfare, firms in a duopoly competition that maximize profits, and heterogenous consumers. The results show that XAI regulation may be redundant. In fact, mandating fully transparent XAI may make firms and consumers worse off. This reveals a tradeoff between maximizing welfare and receiving explainable AI outputs. We extend the existing literature on method and substantive fronts, and we introduce and study the notion of XAI fairness, which may be impossible to guarantee even under mandatory XAI.
    
[^238]: 使用软上下文共享的提示调整视觉-语言模型

    Prompt Tuning with Soft Context Sharing for Vision-Language Models

    [https://arxiv.org/abs/2208.13474](https://arxiv.org/abs/2208.13474)

    提出了一种名为SoftCPT的新方法，利用软上下文共享来联合调整预训练的视觉-语言模型对多个目标少样本任务进行调整，并设计了一个任务共享的元网络来生成提示上下文。

    

    视觉-语言模型最近在计算机视觉的许多任务中展现出了巨大潜力。与此同时，先前的研究表明，专为视觉-语言模型设计的提示调整相较于线性探测（一种强基准）在少样本图像识别方面可以获得更优异的性能。在实践中，许多少样本任务在本质上是相关的，特别是在专业领域内。然而，这样的信息先前被忽视了。受到将任务关系建模为多任务学习通常可以提升性能的启发，我们提出了一种新颖的方法SoftCPT（用于提示调整的软上下文共享）来联合调整预训练的视觉-语言模型对多个目标少样本任务进行调整。具体地，我们设计了一个任务共享的元网络，利用任务名称以及可学习的任务上下文作为输入为每个任务生成提示上下文。这个元网络的参数以及任务上下文被调整为

    arXiv:2208.13474v2 Announce Type: replace-cross  Abstract: Vision-language models have recently shown great potential on many tasks in computer vision. Meanwhile, prior work demonstrates prompt tuning designed for vision-language models could acquire superior performance on few-shot image recognition compared to linear probe, a strong baseline. In practice, many few-shot tasks are inherently correlated, particularly within specialized domains. However, such information is overlooked previously. Inspired by the fact that modeling task relationship by multi-task learning can usually boost performance, we propose a novel method SoftCPT (Soft Context Sharing for Prompt Tuning) to tune pre-trained vision-language models on multiple target few-shot tasks jointly. Specifically, we design a task-shared meta network to generate prompt context for each task using task name together with a learnable task context as input. The parameters of this meta network as well as the task context are tuned o
    
[^239]: 自然语言上的多步演绎推理：基于超领域泛化的实证研究

    Multi-Step Deductive Reasoning Over Natural Language: An Empirical Study on Out-of-Distribution Generalisation

    [https://arxiv.org/abs/2207.14000](https://arxiv.org/abs/2207.14000)

    提出了IMA-GloVe-GA，一个用于自然语言表达的多步推理的迭代神经推理网络，在超领域泛化方面具有更好的性能表现。

    

    将深度学习与符号逻辑推理结合起来，旨在充分利用这两个领域的成功，并引起了越来越多的关注。受DeepLogic启发，该模型经过端到端训练，用于执行逻辑程序推理，我们介绍了IMA-GloVe-GA，这是一个用自然语言表达的多步推理的迭代神经推理网络。在我们的模型中，推理是使用基于RNN的迭代内存神经网络进行的，其中包含一个门关注机制。我们在PARARULES、CONCEPTRULES V1和CONCEPTRULES V2三个数据集上评估了IMA-GloVe-GA。实验结果表明，带有门关注机制的DeepLogic比DeepLogic和其他RNN基线模型能够实现更高的测试准确性。我们的模型在规则被打乱时比RoBERTa-Large实现了更好的超领域泛化性能。此外，为了解决当前多步推理数据集中推理深度不平衡的问题

    arXiv:2207.14000v2 Announce Type: replace-cross  Abstract: Combining deep learning with symbolic logic reasoning aims to capitalize on the success of both fields and is drawing increasing attention. Inspired by DeepLogic, an end-to-end model trained to perform inference on logic programs, we introduce IMA-GloVe-GA, an iterative neural inference network for multi-step reasoning expressed in natural language. In our model, reasoning is performed using an iterative memory neural network based on RNN with a gate attention mechanism. We evaluate IMA-GloVe-GA on three datasets: PARARULES, CONCEPTRULES V1 and CONCEPTRULES V2. Experimental results show DeepLogic with gate attention can achieve higher test accuracy than DeepLogic and other RNN baseline models. Our model achieves better out-of-distribution generalisation than RoBERTa-Large when the rules have been shuffled. Furthermore, to address the issue of unbalanced distribution of reasoning depths in the current multi-step reasoning datase
    
[^240]: 半监督预测聚类树用于（分层）多标签分类

    Semi-supervised Predictive Clustering Trees for (Hierarchical) Multi-label Classification

    [https://arxiv.org/abs/2207.09237](https://arxiv.org/abs/2207.09237)

    提出了一种基于半监督学习的预测聚类树的（分层）多标签分类方法。

    

    半监督学习（SSL）是一种常见的学习预测模型的方法，不仅使用标记的示例，还使用未标记的示例。尽管SSL在简单的分类和回归任务上得到了研究界的关注，但对于具有结构相关变量的复杂预测任务尚未得到充分调查。这种情况出现在多标签分类和分层多标签分类任务中，这可能需要额外的信息，可能来自描述空间中由未标记示例提供的基础分布，以更好地面对挑战性的同时预测多个类标签的任务。在本文中，我们研究了这一方面，并提出了一种基于半监督学习的预测聚类树的（分层）多标签分类方法。我们还将该方法扩展到集成学习，并提出

    arXiv:2207.09237v2 Announce Type: replace-cross  Abstract: Semi-supervised learning (SSL) is a common approach to learning predictive models using not only labeled examples, but also unlabeled examples. While SSL for the simple tasks of classification and regression has received a lot of attention from the research community, this is not properly investigated for complex prediction tasks with structurally dependent variables. This is the case of multi-label classification and hierarchical multi-label classification tasks, which may require additional information, possibly coming from the underlying distribution in the descriptive space provided by unlabeled examples, to better face the challenging task of predicting simultaneously multiple class labels.   In this paper, we investigate this aspect and propose a (hierarchical) multi-label classification method based on semi-supervised learning of predictive clustering trees. We also extend the method towards ensemble learning and propose
    
[^241]: 嵌入控制部分观察系统：具有可证明样本效率的表示学习

    Embed to Control Partially Observed Systems: Representation Learning with Provable Sample Efficiency

    [https://arxiv.org/abs/2205.13476](https://arxiv.org/abs/2205.13476)

    论文提出了一种名为Embed to Control（ETC）的强化学习算法，通过在两个级别学习表示的方法来解决部分观察马尔可夫决策问题，以实现样本高效利用。

    

    强化学习在部分观察马尔可夫决策过程（POMDPs）中面临两个挑战。一是通常需要全部历史记录来预测未来，这导致样本复杂度随着时间跨度呈指数级增长。二是观测和状态空间通常是连续的，这导致样本复杂度随外在维数呈指数级增长。为了解决这些挑战，需要通过利用POMDP的结构学习观测和状态历史的最小但足够的表示。为此，我们提出了一种名为Embed to Control (ETC)的强化学习算法，该算法在优化策略的同时学习两个级别的表示。(i)在每一步，ETC学习用低维特征表示状态，这对转移核进行因子分解。(ii)在多个步骤中，ETC学习用低维表示完整历史记录。

    arXiv:2205.13476v2 Announce Type: replace-cross  Abstract: Reinforcement learning in partially observed Markov decision processes (POMDPs) faces two challenges. (i) It often takes the full history to predict the future, which induces a sample complexity that scales exponentially with the horizon. (ii) The observation and state spaces are often continuous, which induces a sample complexity that scales exponentially with the extrinsic dimension. Addressing such challenges requires learning a minimal but sufficient representation of the observation and state histories by exploiting the structure of the POMDP.   To this end, we propose a reinforcement learning algorithm named Embed to Control (ETC), which learns the representation at two levels while optimizing the policy.~(i) For each step, ETC learns to represent the state with a low-dimensional feature, which factorizes the transition kernel. (ii) Across multiple steps, ETC learns to represent the full history with a low-dimensional emb
    
[^242]: 将生成的对数进行准则对齐的黑盒知识蒸馏

    Aligning Logits Generatively for Principled Black-Box Knowledge Distillation

    [https://arxiv.org/abs/2205.10490](https://arxiv.org/abs/2205.10490)

    本文提出了一个新的黑盒知识蒸馏方法MEKD，通过将教师和学生模型的低维度对数对齐，实现将一个繁琐模型压缩成轻量级模型。

    

    黑盒知识蒸馏（B2KD）是一个处理云端到边缘模型压缩的问题，其中数据和模型托管在服务器上且无法看见。B2KD面临的挑战包括互联网交换受限和数据分布在边缘和云端之间的不一致。本文提出了一个包括去隐去和蒸馏两步工作流程，并在理论上提供了一个从对数到单元边界的新优化方向，不同于直接对数对齐。在其指导下，我们提出了一种新方法Mapping-Emulation KD（MEKD），将一个黑盒繁琐模型蒸馏成一个轻量级模型。我们的方法不区分软或硬响应处理，并包括：1）去隐去：通过生成器模拟教师函数的逆映射，和2）蒸馏：通过减小高维图像点之间的距离来对齐教师模型和学生模型的低维度对数。

    arXiv:2205.10490v2 Announce Type: replace-cross  Abstract: Black-Box Knowledge Distillation (B2KD) is a formulated problem for cloud-to-edge model compression with invisible data and models hosted on the server. B2KD faces challenges such as limited Internet exchange and edge-cloud disparity of data distributions. In this paper, we formalize a two-step workflow consisting of deprivatization and distillation, and theoretically provide a new optimization direction from logits to cell boundary different from direct logits alignment. With its guidance, we propose a new method Mapping-Emulation KD (MEKD) that distills a black-box cumbersome model into a lightweight one. Our method does not differentiate between treating soft or hard responses, and consists of: 1) deprivatization: emulating the inverse mapping of the teacher function with a generator, and 2) distillation: aligning low-dimensional logits of the teacher and student models by reducing the distance of high-dimensional image poin
    
[^243]: 基于图诱导的局部值函数的分布式多智能体强化学习

    Distributed Multi-Agent Reinforcement Learning Based on Graph-Induced Local Value Functions

    [https://arxiv.org/abs/2202.13046](https://arxiv.org/abs/2202.13046)

    通过利用图结构，本文提出了一种通用计算高效的分布式框架，基于局部值函数的分布式RL方法在协作多智能体强化学习中取得了显著的样本复杂性降低。

    

    实现大规模协作多智能体系统的分布式强化学习(RL)具有挑战性，因为：(i)每个智能体只能访问有限的信息；(ii)由于维度诅咒，会出现收敛或计算复杂性问题。本文提出了一种通用的计算高效的协作多智能体强化学习(MARL)分布式框架，通过利用该问题中涉及的图结构。我们引入了描述MARL中三种类型智能体耦合的三个耦合图，分别是状态图、观测图和奖励图。通过进一步考虑通信图，我们提出了两种基于耦合图中派生的局部值函数的分布式RL方法。第一种方法在前述四个图上的特定条件下可以显著降低样本复杂性。第二种

    arXiv:2202.13046v4 Announce Type: replace-cross  Abstract: Achieving distributed reinforcement learning (RL) for large-scale cooperative multi-agent systems (MASs) is challenging because: (i) each agent has access to only limited information; (ii) issues on convergence or computational complexity emerge due to the curse of dimensionality. In this paper, we propose a general computationally efficient distributed framework for cooperative multi-agent reinforcement learning (MARL) by utilizing the structures of graphs involved in this problem. We introduce three coupling graphs describing three types of inter-agent couplings in MARL, namely, the state graph, the observation graph and the reward graph. By further considering a communication graph, we propose two distributed RL approaches based on local value-functions derived from the coupling graphs. The first approach is able to reduce sample complexity significantly under specific conditions on the aforementioned four graphs. The second
    
[^244]: 具有加速客户梯度的通信高效联邦学习

    Communication-Efficient Federated Learning with Accelerated Client Gradient

    [https://arxiv.org/abs/2201.03172](https://arxiv.org/abs/2201.03172)

    通过使用具有前瞻梯度的全局模型，并通过与超调全局模型对齐来规范本地更新，提出了一种简单但有效的联邦学习框架，以改善参与者之间的一致性，促进服务器模型的收敛。

    

    联邦学习常常因参与客户数据集的异质性特征而导致收敛缓慢不稳定。当客户参与率较低时，此趋势会加剧，因为从客户收集的信息具有较大的变化。为了解决这一挑战，我们提出了一个简单但有效的联邦学习框架，改善了客户间的一致性，促进了服务器模型的收敛。通过使服务器广播具有前瞻梯度的全局模型，从而实现了该策略，使所提出的方法能够有效地向参与者传达投影的全局更新信息，而无需额外的客户内存和通信成本。我们还通过将每个客户端与超调全局模型对齐来规范本地更新，以减少偏差并改善算法的稳定性。我们提供了理论收敛率。

    arXiv:2201.03172v2 Announce Type: replace-cross  Abstract: Federated learning often suffers from slow and unstable convergence due to the heterogeneous characteristics of participating client datasets. Such a tendency is aggravated when the client participation ratio is low since the information collected from the clients has large variations. To address this challenge, we propose a simple but effective federated learning framework, which improves the consistency across clients and facilitates the convergence of the server model. This is achieved by making the server broadcast a global model with a lookahead gradient. This strategy enables the proposed approach to convey the projected global update information to participants effectively without additional client memory and extra communication costs. We also regularize local updates by aligning each client with the overshot global model to reduce bias and improve the stability of our algorithm. We provide the theoretical convergence ra
    
[^245]: CGCL：无需手工图数据增强的协作式图对比学习

    CGCL: Collaborative Graph Contrastive Learning without Handcrafted Graph Data Augmentations

    [https://arxiv.org/abs/2111.03262](https://arxiv.org/abs/2111.03262)

    提出了一种新颖的Collaborative Graph Contrastive Learning框架（CGCL），利用多个图编码器观察图形，并避免引入不稳定扰动，保证图的不变性。

    

    未监督的图表示学习是一个非常棘手的问题。在结构化数据的无监督表示学习中对比方法的成功启发了对图类似尝试。现有的图对比学习（GCL）旨在学习跨多个增强视图的不变性，这使其严重依赖于手工制作的图增强。然而，不当的图数据增强可能会危害这种不变性。在本文中，我们展示了不当增强的潜在危险，然后提出了一种新颖的协作式图对比学习框架（CGCL）。该框架利用多个图编码器观察图形。来自不同编码器观察到的特征作为对比学习中的对比视图，避免诱发不稳定的扰动并保证不变性。为了确保不同图编码器之间的协作，

    arXiv:2111.03262v2 Announce Type: replace-cross  Abstract: Unsupervised graph representation learning is a non-trivial topic. The success of contrastive methods in the unsupervised representation learning on structured data inspires similar attempts on the graph. Existing graph contrastive learning (GCL) aims to learn the invariance across multiple augmentation views, which renders it heavily reliant on the handcrafted graph augmentations. However, inappropriate graph data augmentations can potentially jeopardize such invariance. In this paper, we show the potential hazards of inappropriate augmentations and then propose a novel Collaborative Graph Contrastive Learning framework (CGCL). This framework harnesses multiple graph encoders to observe the graph. Features observed from different encoders serve as the contrastive views in contrastive learning, which avoids inducing unstable perturbation and guarantees the invariance. To ensure the collaboration among diverse graph encoders, we
    
[^246]: 基于生成根森林的角色相似度度量

    Role Similarity Metric Based on Spanning Rooted Forest

    [https://arxiv.org/abs/2110.07872](https://arxiv.org/abs/2110.07872)

    提出了一种新的角色相似度度量\textsf{ForestSim}，并设计了相应的能够在$O(k)$时间内处理top-k查询的搜索算法\textsf{ForestSimSearch}。

    

    作为网络分析中的一个基本问题，结构节点相似度在学术界受到了广泛关注，并在各种应用中得到了应用。在众多提出的结构节点相似度度量中，由于满足多个公理属性，角色相似度凸显出来。现有的角色相似度度量无法处理大型真实网络上的top-k查询，因为时间和空间成本高。本文提出了一种新的角色相似度度量，即\textsf{ForestSim}。我们证明了\textsf{ForestSim}是一种可容忍的角色相似度度量，并设计了相应的top-k相似度搜索算法，即\textsf{ForestSimSearch}，一旦预计算完成，能够在$O(k)$的时间内处理一个top-k查询。此外，我们通过使用快速近似算法来计算森林矩阵的对角线元素来加速预计算。

    arXiv:2110.07872v2 Announce Type: replace-cross  Abstract: As a fundamental issue in network analysis, structural node similarity has received much attention in academia and is adopted in a wide range of applications. Among these proposed structural node similarity measures, role similarity stands out because of satisfying several axiomatic properties including automorphism conformation. Existing role similarity metrics cannot handle top-k queries on large real-world networks due to the high time and space cost. In this paper, we propose a new role similarity metric, namely \textsf{ForestSim}. We prove that \textsf{ForestSim} is an admissible role similarity metric and devise the corresponding top-k similarity search algorithm, namely \textsf{ForestSimSearch}, which is able to process a top-k query in $O(k)$ time once the precomputation is finished. Moreover, we speed up the precomputation by using a fast approximate algorithm to compute the diagonal entries of the forest matrix, which
    
[^247]: GraphFM：图因子分解机用于特征交互建模

    GraphFM: Graph Factorization Machines for Feature Interaction Modeling

    [https://arxiv.org/abs/2105.11866](https://arxiv.org/abs/2105.11866)

    提出了一种名为GraphFM的图因子分解机方法，通过图结构自然表示特征，并将FM的交互功能集成到GNN的特征聚合策略中，能够模拟任意阶特征交互。

    

    因子分解机（FM）是处理高维稀疏数据时建模成对（二阶）特征交互的一种常见方法。然而，一方面，FM未能捕捉到高阶特征交互，受到组合扩展的影响。另一方面，考虑每对特征之间的交互可能会引入噪声并降低预测准确性。为了解决这些问题，我们提出了一种新方法，称为Graph Factorization Machine（GraphFM），通过将特征自然表示成图结构。具体而言，我们设计了一种机制来选择有益的特征交互，并将其形式化为特征之间的边。然后，所提出的模型将FM的交互功能整合到图神经网络（GNN）的特征聚合策略中，通过堆叠层来模拟图结构特征上的任意阶特征交互。

    arXiv:2105.11866v4 Announce Type: replace-cross  Abstract: Factorization machine (FM) is a prevalent approach to modeling pairwise (second-order) feature interactions when dealing with high-dimensional sparse data. However, on the one hand, FM fails to capture higher-order feature interactions suffering from combinatorial expansion. On the other hand, taking into account interactions between every pair of features may introduce noise and degrade prediction accuracy. To solve the problems, we propose a novel approach, Graph Factorization Machine (GraphFM), by naturally representing features in the graph structure. In particular, we design a mechanism to select the beneficial feature interactions and formulate them as edges between features. Then the proposed model, which integrates the interaction function of FM into the feature aggregation strategy of Graph Neural Network (GNN), can model arbitrary-order feature interactions on the graph-structured features by stacking layers. Experime
    
[^248]: MetaVIM：元变分内在激励强化学习用于分散式交通信号控制

    MetaVIM: Meta Variationally Intrinsic Motivated Reinforcement Learning for Decentralized Traffic Signal Control

    [https://arxiv.org/abs/2101.00746](https://arxiv.org/abs/2101.00746)

    提出了一种新颖的MetaVIM强化学习方法，用于分散式交通信号控制，通过元学习考虑邻居信息，以解决现实世界中交通信号控制面临的挑战。

    

    交通信号控制旨在协调交叉口的交通信号，以改善区域或城市的交通效率。最近，深度强化学习（RL）已被应用于交通信号控制，并表现出有希望的性能，其中每个交通信号被视为一个代理。然而，在现实世界中，仍存在一些可能限制其大规模应用的挑战。为了使从训练场景中学到的策略能够推广到新的未见场景，提出了一种新颖的Meta Variationally Intrinsic Motivated（MetaVIM）RL方法，用于学习考虑邻居信息的每个交叉口的分散式策略。具体来说，我们将策略学习形式化为一个关于一组相关任务的元学习问题，其中每个任务对应于一个交叉口的交通信号控制，其邻居被视为状态的未观察部分。

    arXiv:2101.00746v5 Announce Type: replace-cross  Abstract: Traffic signal control aims to coordinate traffic signals across intersections to improve the traffic efficiency of a district or a city. Deep reinforcement learning (RL) has been applied to traffic signal control recently and demonstrated promising performance where each traffic signal is regarded as an agent. However, there are still several challenges that may limit its large-scale application in the real world. To make the policy learned from a training scenario generalizable to new unseen scenarios, a novel Meta Variationally Intrinsic Motivated (MetaVIM) RL method is proposed to learn the decentralized policy for each intersection that considers neighbor information in a latent way. Specifically, we formulate the policy learning as a meta-learning problem over a set of related tasks, where each task corresponds to traffic signal control at an intersection whose neighbors are regarded as the unobserved part of the state. T
    
[^249]: 健康文本简化：消化癌症教育的注释语料库和增强学习的新策略

    Health Text Simplification: An Annotated Corpus for Digestive Cancer Education and Novel Strategies for Reinforcement Learning. (arXiv:2401.15043v1 [cs.CL])

    [http://arxiv.org/abs/2401.15043](http://arxiv.org/abs/2401.15043)

    该论文介绍了一个用于健康文本简化研究的消化癌症教育材料的注释语料库，并探索了基于大型语言模型的简化方法，包括微调、增强学习、增强学习与人类反馈、领域自适应和基于提示的应用。

    

    目标：健康教育材料的阅读水平显著影响信息的可理解性和可接触性，特别是对于少数族裔人群。许多患者教育资源超过了广泛接受的标准的阅读水平和复杂性。在健康信息中，急需高性能的文本简化模型以增强传播和识字能力。这种需要在癌症教育中尤为迫切，有效的预防和筛查教育可以大大减少发病率和死亡率。方法：我们引入了简化的消化癌症（SimpleDC）并行语料库，用于健康文本简化研究。利用SimpleDC和现有的Med-EASi语料库，我们探索了基于大型语言模型（LLM）的简化方法，包括微调、增强学习（RL）、增强学习与人类反馈（RLHF）、领域自适应和基于提示的应用。

    Objective: The reading level of health educational materials significantly influences information understandability and accessibility, particularly for minoritized populations. Many patient educational resources surpass the reading level and complexity of widely accepted standards. There is a critical need for high-performing text simplification models in health information to enhance dissemination and literacy. This need is particularly acute in cancer education, where effective prevention and screening education can substantially reduce morbidity and mortality.  Methods: We introduce Simplified Digestive Cancer (SimpleDC), a parallel corpus of cancer education materials tailored for health text simplification research. Utilizing SimpleDC alongside the existing Med-EASi corpus, we explore Large Language Model (LLM)-based simplification methods, including fine-tuning, reinforcement learning (RL), reinforcement learning with human feedback (RLHF), domain adaptation, and prompt-based app
    
[^250]: 推理的拓扑学：揭秘思维链、树和图

    Topologies of Reasoning: Demystifying Chains, Trees, and Graphs of Thoughts. (arXiv:2401.14295v1 [cs.CL])

    [http://arxiv.org/abs/2401.14295](http://arxiv.org/abs/2401.14295)

    这篇论文探讨了结合结构的提示工程在提高大型语言模型推理性能方面的前景，通过思维链、思维树或思维图的设计来引导整体推理过程。通过大量实例，这种范式显著增强了模型在多个任务中的能力。总的来说，论文提供了一个通用蓝图，为未来的发展铺平道路。

    

    自然语言处理（NLP）领域近年来取得了显著进展，特别是在通过创新的提示技术提高大型语言模型（LLM）性能方面。其中，与结构相结合的提示工程被视为一种有前途的范式，其设计如思维链、思维树或思维图等，通过结构指导整体LLM推理过程。通过大量实例的说明，这种范式显著增强了LLM在逻辑或数学推理、规划或创造性写作等各种任务中的能力。为了方便理解这个不断发展的领域并为未来的发展铺平道路，我们设计了一个有效和高效的LLM推理方案的通用蓝图。为此，我们对提示执行流程进行了深入分析，澄清并明确定义了不同的概念。然后我们建立第一个分类系统

    The field of natural language processing (NLP) has witnessed significant progress in recent years, with a notable focus on improving large language models' (LLM) performance through innovative prompting techniques. Among these, prompt engineering coupled with structures has emerged as a promising paradigm, with designs such as Chain-of-Thought, Tree of Thoughts, or Graph of Thoughts, in which the overall LLM reasoning is guided by a structure such as a graph. As illustrated with numerous examples, this paradigm significantly enhances the LLM's capability to solve numerous tasks, ranging from logical or mathematical reasoning to planning or creative writing. To facilitate the understanding of this growing field and pave the way for future developments, we devise a general blueprint for effective and efficient LLM reasoning schemes. For this, we conduct an in-depth analysis of the prompt execution pipeline, clarifying and clearly defining different concepts. We then build the first taxon
    
[^251]: 大型语言模型能否编写并行代码?

    Can Large Language Models Write Parallel Code?. (arXiv:2401.12554v1 [cs.DC])

    [http://arxiv.org/abs/2401.12554](http://arxiv.org/abs/2401.12554)

    本文研究了大型语言模型生成并行代码的能力。我们提出了一个基准测试集PCGBench，并使用新指标评估了几个最先进的语言模型在并行编程模型和计算问题类型上的性能。

    

    大型语言模型正变得越来越受软件开发者的欢迎。它们在多种情境下展示了对源代码的建模和生成能力，包括代码补全、摘要、翻译和查找等。然而，它们往往难以为更复杂的任务生成代码。在本文中，我们探讨了最先进的语言模型生成并行代码的能力。我们提出了一个名为PCGBench的基准测试集，其中包含420个任务，用于评估语言模型生成并行代码的能力，并对几个最先进的开源和闭源语言模型在这些任务上的性能进行评估。我们引入了一些用于比较并行代码生成性能的新指标，并使用这些指标来探究每个语言模型在不同的并行编程模型和计算问题类型上的表现。

    Large Language Models are becoming an increasingly popular tool for software development. Their ability to model and generate source code has been demonstrated in a variety of contexts, including code completion, summarization, translation, and lookup. However, they often struggle to generate code for more complex tasks. In this paper, we explore the ability of state-of-the-art language models to generate parallel code. We propose a benchmark, PCGBench, consisting of a set of 420 tasks for evaluating the ability of language models to generate parallel code, and we evaluate the performance of several state-of-the-art open- and closed-source language models on these tasks. We introduce novel metrics for comparing parallel code generation performance and use them to explore how well each LLM performs on various parallel programming models and computational problem types.
    
[^252]: GI-PIP：梯度反转攻击是否需要不切实际的辅助数据集？

    GI-PIP: Do We Require Impractical Auxiliary Dataset for Gradient Inversion Attacks?. (arXiv:2401.11748v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2401.11748](http://arxiv.org/abs/2401.11748)

    本文提出了一种新颖的梯度反转攻击方法GI-PIP，不需要依赖不切实际的辅助数据集，通过利用异常检测模型从较少的数据中捕获底层分布，并能在图像恢复方面表现出优异的能力，同时在分布泛化方面也更强大。

    

    深度梯度反转攻击通过准确地恢复共享梯度中的隐私数据，对联邦学习构成了严重威胁。然而，现有技术在访问过多的辅助数据方面依赖于不切实际的假设，这违反了联邦学习的基本数据分区原则。本文提出了一种新颖的方法，即使用实用图像先验的梯度反转攻击（GI-PIP），在经过修订的威胁模型下。GI-PIP利用异常检测模型从更少的数据中捕获底层分布，而基于GAN的方法需要消耗更多的数据来合成图像。然后，利用提取出的分布来调节攻击过程作为异常得分损失。实验结果表明，GI-PIP只使用了ImageNet数据的3.8%即可实现16.12 dB的PSNR恢复，而基于GAN的方法则需要超过70%的数据。此外，与基于GAN的方法相比，GI-PIP在分布泛化方面表现出更强大的能力。

    Deep gradient inversion attacks expose a serious threat to Federated Learning (FL) by accurately recovering private data from shared gradients. However, the state-of-the-art heavily relies on impractical assumptions to access excessive auxiliary data, which violates the basic data partitioning principle of FL. In this paper, a novel method, Gradient Inversion Attack using Practical Image Prior (GI-PIP), is proposed under a revised threat model. GI-PIP exploits anomaly detection models to capture the underlying distribution from fewer data, while GAN-based methods consume significant more data to synthesize images. The extracted distribution is then leveraged to regulate the attack process as Anomaly Score loss. Experimental results show that GI-PIP achieves a 16.12 dB PSNR recovery using only 3.8% data of ImageNet, while GAN-based methods necessitate over 70%. Moreover, GI-PIP exhibits superior capability on distribution generalization compared to GAN-based methods. Our approach signif
    
[^253]: 量子机器学习：从NISQ到容错。

    Quantum Machine Learning: from NISQ to Fault Tolerance. (arXiv:2401.11351v1 [quant-ph])

    [http://arxiv.org/abs/2401.11351](http://arxiv.org/abs/2401.11351)

    本文提供了对量子机器学习领域的全面回顾，涵盖了在NISQ技术和容错量子计算硬件上使用的技术和算法，并深入讨论了与量子机器学习相关的基本概念和统计学习理论。

    

    量子机器学习是在量子设备上运行机器学习算法的过程，在学术界和商业界引起了广泛关注。本文对量子机器学习领域涌现的各种概念进行了全面而公正的回顾。这包括在噪声中间尺度量子（NISQ）技术中使用的技术，以及与容错量子计算硬件兼容的算法方法。我们的回顾涵盖了与量子机器学习相关的基本概念、算法和统计学习理论。

    Quantum machine learning, which involves running machine learning algorithms on quantum devices, has garnered significant attention in both academic and business circles. In this paper, we offer a comprehensive and unbiased review of the various concepts that have emerged in the field of quantum machine learning. This includes techniques used in Noisy Intermediate-Scale Quantum (NISQ) technologies and approaches for algorithms compatible with fault-tolerant quantum computing hardware. Our review covers fundamental concepts, algorithms, and the statistical learning theory pertinent to quantum machine learning.
    
[^254]: DeepEdit: 带有约束的解码式知识编辑

    DeepEdit: Knowledge Editing as Decoding with Constraints. (arXiv:2401.10471v1 [cs.CL])

    [http://arxiv.org/abs/2401.10471](http://arxiv.org/abs/2401.10471)

    DeepEdit是一种神经符号方法，通过更好的推理一致性和对更新知识的意识，提高了大型语言模型的知识编辑能力，对多跳问题数据集MQuaKE取得了显著的进展。

    

    我们将大型语言模型（LLMs）的知识编辑视为带有约束的解码过程。我们提出了DeepEdit（基于深度优先搜索的渐进式解码知识编辑），这是一种神经符号方法，通过更好的推理一致性、问题相关性和对更新知识的意识来改进知识编辑。DeepEdit可灵活应用于所有黑盒LLMs：不需要访问模型参数、表示或输出词汇分布。DeepEdit逐步产生高质量的推理步骤，以实现有效的知识编辑。它利用深度优先搜索来修改LLMs的输出，从而提高输出对问题的相关性和对更新知识的意识。在知识编辑方面，DeepEdit在控制LLMs产生更简洁的推理方面表现出色。在MQuaKE上，DeepEdit在定量上取得了显著的进展，这是一个具有挑战性的多跳问题数据集。

    We develop a new perspective of knowledge editing for large language models (LLMs) as decoding with constraints. We propose DeepEdit (Depth-first Search based Progressive Decoding for Knowledge Editing), a neuro-symbolic method that improves knowledge editing with better coherence of reasoning, relevance to the question, and awareness of updated knowledge. DeepEdit can be flexibly applied to all black-box LLMs: it does not require any access to the model parameters, representations, or output vocabulary distributions. DeepEdit progressively produces the high-quality reasoning steps towards effective knowledge editing. It utilizes a depth-first search to revise the LLMs' output, which improves the output's informativeness to the input question and awareness of the updated knowledge. Qualitatively, DeepEdit effectively controls LLMs to produce more succinct reasoning in accord with knowledge editing. Quantitatively, DeepEdit yields significant gains on MQuaKE, a challenging multi-hop que
    
[^255]: 在目标识别中将地理多样知识融入提示以提高地理鲁棒性

    Incorporating Geo-Diverse Knowledge into Prompting for Increased Geographical Robustness in Object Recognition. (arXiv:2401.01482v1 [cs.CV])

    [http://arxiv.org/abs/2401.01482](http://arxiv.org/abs/2401.01482)

    本文研究了在目标识别中将地理多样知识融入提示以提高地理鲁棒性的方法，探索了通过大型语言模型获取地理特定对象知识并结合CLIP视觉语言模型的零样本和可学习软提示。通过提出一种地理知识正则化方法，实现了从源地理位置推广到未见目标地理位置的鲁棒性提升。

    

    现有的目标识别模型在不同地理情景下缺乏鲁棒性，这是由于设计和环境中存在重要的领域转移。为了更准确地反映这些转移下的对象概念，需要调整类别表示。在缺乏目标地理位置训练数据的情况下，我们假设可以利用地理特定的对象类别描述性知识来增强鲁棒性。为此，我们探索了通过探测大型语言模型的地理特定对象知识的可行性，并研究了在CLIP视觉语言模型中集成知识的零样本和可学习软提示。特别地，我们提出了一种地理知识正则化方法，以确保在一组源地理位置上训练的软提示能够推广到未见过的目标地理位置集合。当仅依赖来自欧洲的数据进行训练时，我们在DollarStreet上的增益达到了+2.8个国家。

    Existing object recognition models have been shown to lack robustness in diverse geographical scenarios due to significant domain shifts in design and context. Class representations need to be adapted to more accurately reflect an object concept under these shifts. In the absence of training data from target geographies, we hypothesize that geography-specific descriptive knowledge of object categories can be leveraged to enhance robustness. For this purpose, we explore the feasibility of probing a large-language model for geography-specific object knowledge, and we investigate integrating knowledge in zero-shot and learnable soft prompting with the CLIP vision-language model. In particular, we propose a geography knowledge regularization method to ensure that soft prompts trained on a source set of geographies generalize to an unseen target set of geographies. Our gains on DollarStreet when generalizing from a model trained only on data from Europe are as large as +2.8 on countries fro
    
[^256]: 利用生成型人工智能进行临床证据摘要需要确保可靠性

    Leveraging Generative AI for Clinical Evidence Summarization Needs to Ensure Trustworthiness. (arXiv:2311.11211v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2311.11211](http://arxiv.org/abs/2311.11211)

    这项研究讨论了利用生成型人工智能进行临床证据摘要的可靠性问题，尤其关注开发问责制、公平和包容性的模型的挑战。

    

    证据医学承诺通过利用最佳可用证据来增强医疗决策和实践，从而提高医疗质量。医学证据的快速增长，可以从各种来源获取，给收集、评估和综合证据信息带来了挑战。最近生成型人工智能的进展，例如大型语言模型，有望促进这项艰巨的任务。然而，开发出具有问责制、公平和包容性的模型仍然是一项复杂的任务。在这个视角中，我们讨论了生成型人工智能在自动化医疗证据摘要方面的可靠性问题。

    Evidence-based medicine promises to improve the quality of healthcare by empowering medical decisions and practices with the best available evidence. The rapid growth of medical evidence, which can be obtained from various sources, poses a challenge in collecting, appraising, and synthesizing the evidential information. Recent advancements in generative AI, exemplified by large language models, hold promise in facilitating the arduous task. However, developing accountable, fair, and inclusive models remains a complicated undertaking. In this perspective, we discuss the trustworthiness of generative AI in the context of automated summarization of medical evidence.
    
[^257]: 通过离散扩散学习无监督的自动驾驶世界模型

    Learning Unsupervised World Models for Autonomous Driving via Discrete Diffusion. (arXiv:2311.01017v1 [cs.CV])

    [http://arxiv.org/abs/2311.01017](http://arxiv.org/abs/2311.01017)

    本论文提出了一种通过离散扩散学习无监督的自动驾驶世界模型的新方法，通过使用VQVAE对传感器观察进行标记化并通过离散扩散预测未来，我们的模型在点云观察中实现了显著改进，将1秒预测的SOTA Chamfer距离降低了65%以上。

    

    学习世界模型可以以无监督的方式教会智能体世界的运作方式。尽管它可以看作是序列建模的特殊情况，但在自动驾驶等机器人应用中，与使用生成预训练转换器（GPT）扩展语言模型相比，扩展世界模型的进展相对较慢。我们指出了两个主要瓶颈：处理复杂和无结构的观察空间以及具有可扩展性的生成模型。因此，我们提出了一种新颖的世界建模方法，首先使用VQVAE对传感器观察进行标记化，然后通过离散扩散预测未来。为了有效地并行解码和去噪标记，我们将遮蔽生成图像转换器转换为离散扩散框架，并进行了一些简单的改进，取得了显着的改进效果。当应用于点云观察的世界模型学习时，我们的模型将1秒预测的SOTA Chamfer距离降低了65%以上。

    Learning world models can teach an agent how the world works in an unsupervised manner. Even though it can be viewed as a special case of sequence modeling, progress for scaling world models on robotic applications such as autonomous driving has been somewhat less rapid than scaling language models with Generative Pre-trained Transformers (GPT). We identify two reasons as major bottlenecks: dealing with complex and unstructured observation space, and having a scalable generative model. Consequently, we propose a novel world modeling approach that first tokenizes sensor observations with VQVAE, then predicts the future via discrete diffusion. To efficiently decode and denoise tokens in parallel, we recast Masked Generative Image Transformer into the discrete diffusion framework with a few simple changes, resulting in notable improvement. When applied to learning world models on point cloud observations, our model reduces prior SOTA Chamfer distance by more than 65% for 1s prediction, an
    
[^258]: 具有重尾奖励的强化学习离线策略评估和优化的鲁棒性提升

    Robust Offline Policy Evaluation and Optimization with Heavy-Tailed Rewards. (arXiv:2310.18715v1 [cs.LG])

    [http://arxiv.org/abs/2310.18715](http://arxiv.org/abs/2310.18715)

    本文提出的ROAM和ROOM算法框架通过将中位数法与离线强化学习策略相结合，提供了对重尾奖励的直接不确定性估计，从而增强了离线强化学习在现实应用中的鲁棒性。

    

    本文旨在增强离线强化学习在现实世界应用中普遍存在的重尾奖励情况下的鲁棒性。我们提出了两个算法框架，ROAM和ROOM，用于鲁棒的离线策略评估和离线策略优化。我们的框架的核心是将中位数法与离线强化学习策略相结合，能够对值函数估计器进行直接的不确定性估计。这不仅符合离线策略优化中的保守主义原则，而且灵活处理重尾奖励。理论结果和广泛的实验证明，我们的两个框架在记录的数据集中展示了具有重尾奖励分布时超越现有方法的性能。

    This paper endeavors to augment the robustness of offline reinforcement learning (RL) in scenarios laden with heavy-tailed rewards, a prevalent circumstance in real-world applications. We propose two algorithmic frameworks, ROAM and ROOM, for robust off-policy evaluation (OPE) and offline policy optimization (OPO), respectively. Central to our frameworks is the strategic incorporation of the median-of-means method with offline RL, enabling straightforward uncertainty estimation for the value function estimator. This not only adheres to the principle of pessimism in OPO but also adeptly manages heavy-tailed rewards. Theoretical results and extensive experiments demonstrate that our two frameworks outperform existing methods on the logged dataset exhibits heavy-tailed reward distributions.
    
[^259]: 自发性模块化结构：密集预训练Transformer能否从自发模块化结构中获益？

    Emergent Mixture-of-Experts: Can Dense Pre-trained Transformers Benefit from Emergent Modular Structures?. (arXiv:2310.10908v1 [cs.LG])

    [http://arxiv.org/abs/2310.10908](http://arxiv.org/abs/2310.10908)

    该论文研究了密集预训练Transformer是否以及如何从自发的模块化结构中获益。

    

    将模块化设计引入神经网络能够展示出较好的泛化能力和学习效率等优点。现有的模块化神经网络通常是“显式”的，因为它们的模块化架构是预先定义的，每个模块都被期望实现不同的功能。相反，最近的研究表明在标准的预训练Transformer中存在“隐式”的模块化结构，即“自发模块化”。他们表明这样的模块化结构在早期预训练阶段就会出现，并且完全是自发的。然而，大多数Transformer模型仍然被视为单体模型，没有充分利用其模块化的特性。因此，鉴于显式模块化架构的优良特性，我们探索了密集预训练Transformer是否以及如何从自发模块化结构中获益的问题。

    Incorporating modular designs into neural networks demonstrates superior out-of-generalization, learning efficiency, etc. Existing modular neural networks are generally $\textit{explicit}$ because their modular architectures are pre-defined, and individual modules are expected to implement distinct functions. Conversely, recent works reveal that there exist $\textit{implicit}$ modular structures in standard pre-trained transformers, namely $\textit{Emergent Modularity}$. They indicate that such modular structures exhibit during the early pre-training phase and are totally spontaneous. However, most transformers are still treated as monolithic models with their modular natures underutilized. Therefore, given the excellent properties of explicit modular architecture, we explore $\textit{whether and how dense pre-trained transformers can benefit from emergent modular structures.}$ To study this question, we construct \textbf{E}mergent $\textbf{M}$ixture-$\textbf{o}$f-$\textbf{E}$xperts (E
    
[^260]: 人类课程指导下的指令调整

    Instruction Tuning with Human Curriculum. (arXiv:2310.09518v1 [cs.CL])

    [http://arxiv.org/abs/2310.09518](http://arxiv.org/abs/2310.09518)

    本文探讨了在大型语言模型中应用结构化认知学习方法进行指令调整的潜在好处，并提出了一个高度结构化的合成数据集，结果表明该方法优于传统的随机化方法，提高了指令调整的性能。

    

    指令调整的主流范式是随机洗牌训练最大多样化指令-响应对。本文探讨了在当代大型语言模型如ChatGPT和GPT-4中应用结构化认知学习方法进行指令调整的潜在好处。与以往传统的随机化指令数据集不同，我们提出了一个高度结构化的合成数据集，模拟了人类教育的渐进性和有组织性。我们通过将数据集与教育框架对齐来策划我们的数据集，为每个样本包括主题和认知严谨程度等元信息。我们的数据集涵盖了从中学到研究生阶段的全面细粒度主题，每个主题都有各种问题，以利用布鲁姆的认知分级法提高概念深度，该分级法用于区分每个概念的不同人类认知水平。结果表明，这种认知学习方法优于传统的随机化方法，提高了指令调整的性能。

    The dominant paradigm for instruction tuning is the random-shuffled training of maximally diverse instruction-response pairs. This paper explores the potential benefits of applying a structured cognitive learning approach to instruction tuning in contemporary large language models like ChatGPT and GPT-4. Unlike the previous conventional randomized instruction dataset, we propose a highly structured synthetic dataset that mimics the progressive and organized nature of human education. We curate our dataset by aligning it with educational frameworks, incorporating meta information including its topic and cognitive rigor level for each sample. Our dataset covers comprehensive fine-grained topics spanning diverse educational stages (from middle school to graduate school) with various questions for each topic to enhance conceptual depth using Bloom's taxonomy-a classification framework distinguishing various levels of human cognition for each concept. The results demonstrate that this cogni
    
[^261]: Mirage: 图分类的模型无关图蒸馏

    Mirage: Model-Agnostic Graph Distillation for Graph Classification. (arXiv:2310.09486v1 [cs.LG])

    [http://arxiv.org/abs/2310.09486](http://arxiv.org/abs/2310.09486)

    Mirage是一种模型无关的图蒸馏算法，旨在构建一个更小的合成训练集，以在资源有限的环境中实现图分类，并克服了现有算法对完整数据集的依赖性和对建模流程变化的敏感性。

    

    GNNs和其他深度学习模型一样，对数据和计算需求量很大。急需在大型数据集上扩展GNN的训练，以便在资源有限的环境中使用它们。图蒸馏是为此目的而努力，旨在从原始训练数据构建一个更小的合成训练集，而不会显著影响模型性能。虽然初步工作取得了一些进展，但这项工作基于两个关键观察：(1)现有的图蒸馏算法本身依赖于使用完整数据集进行训练，这就破坏了图蒸馏的前提。(2)蒸馏过程对目标GNN架构和超参数具有特异性，因此对建模流程的变化不具备鲁棒性。我们通过设计一种名为Mirage的图分类蒸馏算法来避免这些限制。Mirage建立在一个洞察的基础上，即一个消息传递的GNN将输入图分解为计算的多重集合。

    GNNs, like other deep learning models, are data and computation hungry. There is a pressing need to scale training of GNNs on large datasets to enable their usage on low-resource environments. Graph distillation is an effort in that direction with the aim to construct a smaller synthetic training set from the original training data without significantly compromising model performance. While initial efforts are promising, this work is motivated by two key observations: (1) Existing graph distillation algorithms themselves rely on training with the full dataset, which undermines the very premise of graph distillation. (2) The distillation process is specific to the target GNN architecture and hyper-parameters and thus not robust to changes in the modeling pipeline. We circumvent these limitations by designing a distillation algorithm called Mirage for graph classification. Mirage is built on the insight that a message-passing GNN decomposes the input graph into a multiset of computation 
    
[^262]: 对大型语言模型在非分布式逻辑推理任务上的系统评估

    A Systematic Evaluation of Large Language Models on Out-of-Distribution Logical Reasoning Tasks. (arXiv:2310.09430v1 [cs.CL])

    [http://arxiv.org/abs/2310.09430](http://arxiv.org/abs/2310.09430)

    通过对大型语言模型在非分布式逻辑推理任务上进行系统评估，我们发现这些模型在处理我们新构建的数据集时都存在困难，尽管它们在其他自然语言处理任务上表现良好。这表明这些模型在逻辑推理方面的泛化和鲁棒性仍需要进一步研究。

    

    大型语言模型（LLMs），如GPT-3.5和GPT-4，已经将人工系统在各种自然语言处理任务上的性能提升到接近人类水平。然而，它们在逻辑推理方面的泛化和鲁棒性仍未得到充分评估。为了探索这种能力，我们提出了三个新的逻辑推理数据集，分别名为"ReClor-plus"、"LogiQA-plus"和"LogiQAv2-plus"，每个数据集都包含三个子集：第一个是选项随机打乱，第二个是将正确选项替换为"没有其他选项是正确的"，第三个是前两个子集的组合。我们在这些数据集上进行了实验，使用了鉴别和生成型的LLMs，并表明这些简单的技巧极大地阻碍了语言模型的性能。尽管在原始的公开可用数据集上表现出优秀的性能，但我们发现所有模型都很难回答我们新构建的数据集。我们展示了通过扰动引入任务变化可以提高模型的性能。

    Large language models (LLMs), such as GPT-3.5 and GPT-4, have greatly advanced the performance of artificial systems on various natural language processing tasks to human-like levels. However, their generalisation and robustness to perform logical reasoning remain under-evaluated. To probe this ability, we propose three new logical reasoning datasets named "ReClor-plus", "LogiQA-plus" and "LogiQAv2-plus", each featuring three subsets: the first with randomly shuffled options, the second with the correct choices replaced by "none of the other options are correct", and a combination of the previous two subsets. We carry out experiments on these datasets with both discriminative and generative LLMs and show that these simple tricks greatly hinder the performance of the language models. Despite their superior performance on the original publicly available datasets, we find that all models struggle to answer our newly constructed datasets. We show that introducing task variations by perturb
    
[^263]: LangNav: 语言作为导航的感知表示

    LangNav: Language as a Perceptual Representation for Navigation. (arXiv:2310.07889v1 [cs.CV])

    [http://arxiv.org/abs/2310.07889](http://arxiv.org/abs/2310.07889)

    该论文探索了将语言作为导航的感知表示，并使用现成的视觉系统将每个时间步骤的视图转化为自然语言描述，通过微调预训练的语言模型选择最佳的行动来满足导航指令。有两种用例的实验对这种基于语言的导航方法进行了探索：使用大型语言模型生成合成轨迹以微调较小的语言模型，以及模拟到实际的转换。

    

    我们探索将语言作为视觉与语言导航的感知表示。我们的方法使用现成的视觉系统（用于图像字幕和物体检测）将每个时间步骤中代理人的自我中心全景视图转化为自然语言描述。然后，我们微调预训练的语言模型，根据当前视图和轨迹历史选择最佳的行动来满足导航指令。与标准设置相比，标准设置将预训练的语言模型适应于与预训练的视觉模型连续视觉特征直接配合使用。我们的方法使用（离散的）语言作为感知表示。我们在R2R视觉与语言导航基准测试中探索了两个用例：使用大型语言模型（GPT-4）生成合成轨迹，以便微调较小的语言模型；以及模拟到实际的转换。

    We explore the use of language as a perceptual representation for vision-and-language navigation. Our approach uses off-the-shelf vision systems (for image captioning and object detection) to convert an agent's egocentric panoramic view at each time step into natural language descriptions. We then finetune a pretrained language model to select an action, based on the current view and the trajectory history, that would best fulfill the navigation instructions. In contrast to the standard setup which adapts a pretrained language model to work directly with continuous visual features from pretrained vision models, our approach instead uses (discrete) language as the perceptual representation. We explore two use cases of our language-based navigation (LangNav) approach on the R2R vision-and-language navigation benchmark: generating synthetic trajectories from a prompted large language model (GPT-4) with which to finetune a smaller language model; and sim-to-real transfer where we transfer 
    
[^264]: 观测引导的扩散概率模型

    Observation-Guided Diffusion Probabilistic Models. (arXiv:2310.04041v1 [cs.LG])

    [http://arxiv.org/abs/2310.04041](http://arxiv.org/abs/2310.04041)

    提出了观测引导的扩散概率模型（OGDM），通过引入基于条件鉴别器的观测所产生的额外损失项，实现了更准确的负对数似然优化，在函数评估次数有限的推理阶段表现出色。

    

    我们提出了一种新的扩散模型，称为观测引导的扩散概率模型（OGDM），它有效地解决了质量控制和快速采样之间的权衡问题。我们的方法以原则性的方式将观测过程的引导与马尔可夫链相结合，重新建立了训练目标。通过引入基于条件鉴别器的观测所产生的额外损失项，我们使得优化更准确的负对数似然成为可能，尤其是在函数评估次数有限的推理阶段。这种策略使得我们的训练方法即使只用于微调过程也具有优势，并且与各种快速推理策略兼容，因为我们的方法使用完全相同的推理过程产生更好的去噪网络。

    We propose a novel diffusion model called observation-guided diffusion probabilistic model (OGDM), which effectively addresses the trade-off between quality control and fast sampling. Our approach reestablishes the training objective by integrating the guidance of the observation process with the Markov chain in a principled way. This is achieved by introducing an additional loss term derived from the observation based on the conditional discriminator on noise level, which employs Bernoulli distribution indicating whether its input lies on the (noisy) real manifold or not. This strategy allows us to optimize the more accurate negative log-likelihood induced in the inference stage especially when the number of function evaluations is limited. The proposed training method is also advantageous even when incorporated only into the fine-tuning process, and it is compatible with various fast inference strategies since our method yields better denoising networks using the exactly same inferen
    
[^265]: LightSeq：用于长上下文转换器分布式训练的序列级并行ism

    LightSeq: Sequence Level Parallelism for Distributed Training of Long Context Transformers. (arXiv:2310.03294v1 [cs.LG])

    [http://arxiv.org/abs/2310.03294](http://arxiv.org/abs/2310.03294)

    LightSeq是用于长上下文转换器分布式训练的一种新方法，它通过序列维度进行分区，与不同注意力头数量的模型架构兼容，并且减少了与Megatron-LM相比的通信量，同时还实现了通信和计算的重叠。

    

    增加大型语言模型（LLM）的上下文长度可以解开基本上新的能力，但也显著增加了训练的内存占用。以往的模型并行系统（例如Megatron-LM）对不同的注意力头进行分区和计算，并行处理，导致大量通信量，因此不能在注意力头数量之外扩展，从而阻碍了其采用。本文提出了一种新方法LightSeq，用于长上下文LLM的训练。LightSeq具有许多显著优势。首先，LightSeq通过序列维度进行分区，因此对于具有不同注意力头数量的模型架构是不可知的，适用于Multi-Head，Multi-Query和Grouped-Query attention等模型。其次，LightSeq与Megatron-LM相比，在流行的LLM上不仅需求少至4.7倍的通信，而且还可以将通信与计算重叠。为了进一步减少训练时间，LightSeq还具有一种新的梯度che

    Increasing the context length of large language models (LLMs) unlocks fundamentally new capabilities, but also significantly increases the memory footprints of training. Previous model-parallel systems such as Megatron-LM partition and compute different attention heads in parallel, resulting in large communication volumes, so they cannot scale beyond the number of attention heads, thereby hindering its adoption. In this paper, we introduce a new approach, LightSeq, for long-context LLMs training. LightSeq has many notable advantages. First, LightSeq partitions over the sequence dimension, hence is agnostic to model architectures and readily applicable for models with varying numbers of attention heads, such as Multi-Head, Multi-Query and Grouped-Query attention. Second, LightSeq not only requires up to 4.7x less communication than Megatron-LM on popular LLMs but also overlaps the communication with computation. To further reduce the training time, LightSeq features a novel gradient che
    
[^266]: SE(3)-蛋白质主链生成中的随机流匹配

    SE(3)-Stochastic Flow Matching for Protein Backbone Generation. (arXiv:2310.02391v1 [cs.LG])

    [http://arxiv.org/abs/2310.02391](http://arxiv.org/abs/2310.02391)

    通过SE(3)-Stochastic Flow Matching，我们提出了一系列新型生成模型FoldFlow，可以准确建模蛋白质主链。这些模型通过无需模拟训练和Riemannian最优传输的结合，具有更好的稳定性和建模能力。

    

    通过基于三维刚体运动（即SE(3)群）的流匹配范式，我们引入了一系列具有不断增强建模能力的新型生成模型：FoldFlow，从而实现了对蛋白质主链的准确建模。首先，我们介绍了FoldFlow-Base，一种无需模拟的学习确定性连续时间动力学和匹配不变目标分布的方法。接下来，我们通过引入Riemannian最优传输来加速训练，创建了FoldFlow-OT，从而构建了更简单和稳定的流。最后，我们设计了FoldFlow-SFM，将Riemannian最优传输和无需模拟训练相结合，可以学习SE(3)上的随机连续时间动力学。我们的FoldFlow生成模型家族相比之前的方法具有几个关键优势。

    The computational design of novel protein structures has the potential to impact numerous scientific disciplines greatly. Toward this goal, we introduce $\text{FoldFlow}$ a series of novel generative models of increasing modeling power based on the flow-matching paradigm over $3\text{D}$ rigid motions -i.e. the group $\text{SE(3)}$ -- enabling accurate modeling of protein backbones. We first introduce $\text{FoldFlow-Base}$, a simulation-free approach to learning deterministic continuous-time dynamics and matching invariant target distributions on $\text{SE(3)}$. We next accelerate training by incorporating Riemannian optimal transport to create $\text{FoldFlow-OT}$, leading to the construction of both more simple and stable flows. Finally, we design $\text{FoldFlow-SFM}$ coupling both Riemannian OT and simulation-free training to learn stochastic continuous-time dynamics over $\text{SE(3)}$. Our family of $\text{FoldFlow}$ generative models offer several key advantages over previous
    
[^267]: 一致性轨迹模型：学习扩散的概率流ODE轨迹

    Consistency Trajectory Models: Learning Probability Flow ODE Trajectory of Diffusion. (arXiv:2310.02279v1 [cs.LG])

    [http://arxiv.org/abs/2310.02279](http://arxiv.org/abs/2310.02279)

    提出了一种一致性轨迹模型（CTM），它可以加速扩散模型的采样，同时通过对抗训练和去噪得分匹配损失的组合来提高性能，并实现了最先进的采样质量。

    

    一致性模型（CM）加速基于得分的扩散模型采样，但以牺牲样本质量为代价，缺乏一种自然的方法来权衡速度和质量。为了解决这个限制，我们提出了一致性轨迹模型（CTM），它是包括CM和基于得分模型在内的泛化模型。CTM训练一个单一的神经网络，可以在单次前向传递中输出得分（即对数密度的梯度），并允许在扩散过程中任意初始和最终时间之间进行不受限制的遍历概率流普通微分方程（ODE）。CTM利用对抗训练和去噪得分匹配损失的有效组合来提高性能，并在CIFAR-10（FID 1.73）和64X64分辨率的ImageNet上实现新的最先进FID。CTM还实现了一系列新的采样方案，包括确定性和随机的ODE解中的长跳跃。

    Consistency Models (CM) (Song et al., 2023) accelerate score-based diffusion model sampling at the cost of sample quality but lack a natural way to trade-off quality for speed. To address this limitation, we propose Consistency Trajectory Model (CTM), a generalization encompassing CM and score-based models as special cases. CTM trains a single neural network that can -- in a single forward pass -- output scores (i.e., gradients of log-density) and enables unrestricted traversal between any initial and final time along the Probability Flow Ordinary Differential Equation (ODE) in a diffusion process. CTM enables the efficient combination of adversarial training and denoising score matching loss to enhance performance and achieves new state-of-the-art FIDs for single-step diffusion model sampling on CIFAR-10 (FID 1.73) and ImageNet at 64X64 resolution (FID 2.06). CTM also enables a new family of sampling schemes, both deterministic and stochastic, involving long jumps along the ODE soluti
    
[^268]: VDC: 通过视觉和语言的不一致性检测脏样本的多功能数据清洁器

    VDC: Versatile Data Cleanser for Detecting Dirty Samples via Visual-Linguistic Inconsistency. (arXiv:2309.16211v1 [cs.CV])

    [http://arxiv.org/abs/2309.16211](http://arxiv.org/abs/2309.16211)

    VDC是一个多功能数据清洁器，通过检测图像和标签之间的视觉和语言不一致性来解决数据集中的脏样本问题。

    

    最近，数据中心人工智能的新概念强调了数据在构建AI系统中的作用。不幸的是，在现实世界中，数据集可能会包含脏样本，例如来自后门攻击的毒样本，众包中的噪声标签，甚至是它们的混合体。这些脏样本的存在使得DNNs变得脆弱和不可靠。因此，检测脏样本以提高数据集的质量和可靠性至关重要。现有的检测器只关注检测毒样本或噪声标签，当处理来自其他领域的脏样本时，往往容易出现弱泛化。在本文中，我们发现各种脏样本之间的一个共同性是图像和相关标签之间的视觉和语言之间的不一致性。为了捕捉模态之间的语义不一致性，我们提出了一种多功能数据清洁器(VDC)，利用跨模态对齐和推理的多模态大型语言模型(MLLM)的超过能力。它由...（此处省略）

    The role of data in building AI systems has recently been emphasized by the emerging concept of data-centric AI. Unfortunately, in the real-world, datasets may contain dirty samples, such as poisoned samples from backdoor attack, noisy labels in crowdsourcing, and even hybrids of them. The presence of such dirty samples makes the DNNs vunerable and unreliable.Hence, it is critical to detect dirty samples to improve the quality and realiability of dataset. Existing detectors only focus on detecting poisoned samples or noisy labels, that are often prone to weak generalization when dealing with dirty samples from other domains.In this paper, we find a commonality of various dirty samples is visual-linguistic inconsistency between images and associated labels. To capture the semantic inconsistency between modalities, we propose versatile data cleanser (VDC) leveraging the surpassing capabilities of multimodal large language models (MLLM) in cross-modal alignment and reasoning.It consists o
    
[^269]: 使用事实知识提升上下文学习的效果

    Boosting In-Context Learning with Factual Knowledge. (arXiv:2309.14771v1 [cs.CL])

    [http://arxiv.org/abs/2309.14771](http://arxiv.org/abs/2309.14771)

    本文研究了使用事实知识提升上下文学习的效果，并提出了一个新的知识上下文调优框架来改善学习性能。

    

    在大型语言模型上下文学习（ICL）旨在通过依赖于少量的训练示例解决以前未见过的任务，从而消除参数更新的需求，并实现有竞争力的性能。本文展示了事实知识在ICL的性能中的重要性，包括在LLM中学到的固有知识，从所选的上下文示例中得出的事实知识，以及LLM在输出生成中的知识偏差。为了发挥LLM在少样本学习场景中的能力，我们引入了一种新的知识上下文调优（KICT）框架来进一步提高ICL的性能：1）在持续自监督预训练期间向LLM注入事实知识，2）谨慎选择具有高知识相关性的示例，3）根据先前的知识对预测结果进行校准。我们在自回归LLM（如GPT风格模型）上评估了所提出的方法。

    In-Context Learning (ICL) over Large language models (LLMs) aims at solving previously unseen tasks by conditioning on a few training examples, eliminating the need for parameter updates and achieving competitive performance. In this paper, we demonstrate that factual knowledge is imperative for the performance of ICL in three core facets, i.e., the inherent knowledge learned in LLMs, the factual knowledge derived from the selected in-context examples, and the knowledge biases in LLMs for output generation. To unleash the power of LLMs in few-shot learning scenarios, we introduce a novel Knowledgeable In-Context Tuning (KICT) framework to further improve the performance of ICL: 1) injecting factual knowledge to LLMs during continual self-supervised pre-training, 2) judiciously selecting the examples with high knowledge relevance, and 3) calibrating the prediction results based on prior knowledge. We evaluate the proposed approaches on auto-regressive LLMs (e.g., GPT-style models) over 
    
[^270]: 使用FP8格式的高效后训练量化方法

    Efficient Post-training Quantization with FP8 Formats. (arXiv:2309.14592v1 [cs.LG])

    [http://arxiv.org/abs/2309.14592](http://arxiv.org/abs/2309.14592)

    本研究研究了FP8格式在后训练量化中的优势，并开发了一个通用的量化工作流程。实验结果表明，FP8相对于INT8具有更好的工作负载覆盖率和模型准确性。

    

    最近深度学习方法的进展，如LLMs和扩散模型，提出了对改进的量化方法的需求，以满足这些现代架构的计算需求，同时保持准确性。为了实现这一目标，我们研究了FP8数据格式在75个不同网络架构上进行后训练量化的优势，这些网络架构涵盖了多种任务，包括机器翻译、语言建模、文本生成、图像分类、生成和分割。我们研究了三种不同的FP8表示（E5M2、E4M3和E3M4），以研究在动态范围和精度之间不同权衡程度对模型准确性的影响。基于我们的广泛研究，我们开发了一个量化工作流程，可以概括适用于不同的网络架构。我们的实证结果表明，FP8格式在多个方面优于INT8，包括工作负载覆盖率（92.64％对65.87％）、模型准确性和适用于更广泛的操作范围。

    Recent advances in deep learning methods such as LLMs and Diffusion models have created a need for improved quantization methods that can meet the computational demands of these modern architectures while maintaining accuracy. Towards this goal, we study the advantages of FP8 data formats for post-training quantization across 75 unique network architectures covering a wide range of tasks, including machine translation, language modeling, text generation, image classification, generation, and segmentation. We examine three different FP8 representations (E5M2, E4M3, and E3M4) to study the effects of varying degrees of trade-off between dynamic range and precision on model accuracy. Based on our extensive study, we developed a quantization workflow that generalizes across different network architectures. Our empirical results show that FP8 formats outperform INT8 in multiple aspects, including workload coverage (92.64% vs. 65.87%), model accuracy and suitability for a broader range of ope
    
[^271]: SeaEval多语言基础模型：从跨语言对齐到文化推理

    SeaEval for Multilingual Foundation Models: From Cross-Lingual Alignment to Cultural Reasoning. (arXiv:2309.04766v1 [cs.CL])

    [http://arxiv.org/abs/2309.04766](http://arxiv.org/abs/2309.04766)

    SeaEval是一个评估多语言基础模型的基准测试，研究了模型在自然语言理解、推理以及对文化实践、细微差别和价值观的理解能力上的表现。重要发现包括模型在给出改写指令时行为各异，受到暴露偏差的影响，对于语义等价的多语言查询的回答不一致，以及模型在情感相关问题上的一致性不同。

    

    我们提出了一种用于多语言基础模型的SeaEval基准测试。除了表征这些模型如何理解和推理自然语言外，我们还研究了它们对文化实践、细微差别和价值观的理解能力。除了标准的准确度指标，我们还调查了基础模型在语义和多语言性维度上的脆弱性。我们的分析涵盖了开源和闭源模型，从而得到了在经典的自然语言处理任务、推理和文化理解方面的实证结果。重要发现包括：（1）大多数模型在给出改写指令时的行为各异；（2）许多模型仍然受到暴露偏差的影响（如位置偏差、大多数标签偏差）；（3）对于根源于事实、科学和常识知识的问题，预期在语义上等价的多语言查询应该得到一致的回答。然而，大多数模型在这些查询上表现出令人意外的不一致性；（4）多语言情况下，模型对于情感相关的问题表现出不同程度的一致性。

    We present SeaEval, a benchmark for multilingual foundation models. In addition to characterizing how these models understand and reason with natural language, we also investigate how well they comprehend cultural practices, nuances, and values. Alongside standard accuracy metrics, we investigate the brittleness of foundation models in the dimensions of semantics and multilinguality. Our analyses span both open-sourced and closed models, leading to empirical results across classic NLP tasks, reasoning, and cultural comprehension. Key findings indicate (1) Most models exhibit varied behavior when given paraphrased instructions. (2) Many models still suffer from exposure bias (e.g., positional bias, majority label bias). (3) For questions rooted in factual, scientific, and commonsense knowledge, consistent responses are expected across multilingual queries that are semantically equivalent. Yet, most models surprisingly demonstrate inconsistent performance on these queries. (4) Multilingu
    
[^272]: 评估大型语言模型在发现基因集合功能方面的应用

    Evaluation of large language models for discovery of gene set function. (arXiv:2309.04019v1 [q-bio.GN])

    [http://arxiv.org/abs/2309.04019](http://arxiv.org/abs/2309.04019)

    本研究评估了OpenAI的GPT-4大型语言模型在发现基因集合功能方面的能力，并发现它能根据嵌入的生物医学知识生成与Gene Ontology中具名基因集合非常相似的名称，同时在基因组学数据中发现的基因集合中，GPT-4的命名更具信息量，得到了人工审核的基本验证。

    

    基因集合分析是功能基因组学的重要方法，但它依赖于手动创建的基因功能数据库，这些数据库的不完整和不具备生物学上下文的特点。本文评估了OpenAI的GPT-4大型语言模型的能力，从其嵌入的生物医学知识中发展出有关常见基因功能的假设。我们创建了一个GPT-4流水线，用于用总结其共识功能的名称标记基因集合，并通过分析文本和引文进行证实。在与Gene Ontology中的具名基因集合进行基准测试时，GPT-4在50%的情况下生成了非常相似的名称，而在大多数其他情况下，则恢复了更一般概念的名称。在基因组学数据中发现的基因集合中，与基因集合富集相比，GPT-4的命名更具信息量，其支持性陈述和引文在人工审核中得到了基本验证。快速综合常见基因功能的能力使得大型语言模型成为有价值的功能基因组学助手。

    Gene set analysis is a mainstay of functional genomics, but it relies on manually curated databases of gene functions that are incomplete and unaware of biological context. Here we evaluate the ability of OpenAI's GPT-4, a Large Language Model (LLM), to develop hypotheses about common gene functions from its embedded biomedical knowledge. We created a GPT-4 pipeline to label gene sets with names that summarize their consensus functions, substantiated by analysis text and citations. Benchmarking against named gene sets in the Gene Ontology, GPT-4 generated very similar names in 50% of cases, while in most remaining cases it recovered the name of a more general concept. In gene sets discovered in 'omics data, GPT-4 names were more informative than gene set enrichment, with supporting statements and citations that largely verified in human review. The ability to rapidly synthesize common gene functions positions LLMs as valuable functional genomics assistants.
    
[^273]: 有效的语言模型基准测试

    Efficient Benchmarking (of Language Models). (arXiv:2308.11696v1 [cs.CL])

    [http://arxiv.org/abs/2308.11696](http://arxiv.org/abs/2308.11696)

    本研究提出了一种名为"Efficient Benchmarking"的问题，旨在智能地减少语言模型评估的计算成本而不降低可靠性，并使用一种名为Decision Impact on Reliability（DIoR）的新度量来评估决策的可靠性。通过HELM基准测试的案例研究，发现只需删除一个低排名模型即可改变领先者，并仅需少量示例即可得到正确的基准测试排名。

    

    语言模型的多功能性增加导致了一类全面评估广泛能力的基准测试的出现。这些基准测试与大规模计算成本相关，每个模型需要数千个GPU小时。然而，关于评估效率方面的问题在文献中讨论较少。本文提出了一种名为"Efficient Benchmarking"的问题，即在不损害可靠性的情况下智能地减少语言模型评估的计算成本。通过使用HELM基准测试作为示例，我们研究了不同基准测试设计选择如何影响计算-可靠性权衡。我们提出使用一种名为Decision Impact on Reliability（DIoR）的新度量来评估这些决策的可靠性。例如，我们发现仅通过从基准测试中删除一个低排名模型，当前在HELM上的领先者可能会改变，并且观察到只需一小部分示例即可获得正确的基准测试排名。

    The increasing versatility of language models LMs has given rise to a new class of benchmarks that comprehensively assess a broad range of capabilities. Such benchmarks are associated with massive computational costs reaching thousands of GPU hours per model. However the efficiency aspect of these evaluation efforts had raised little discussion in the literature. In this work we present the problem of Efficient Benchmarking namely intelligently reducing the computation costs of LM evaluation without compromising reliability. Using the HELM benchmark as a test case we investigate how different benchmark design choices affect the computation-reliability tradeoff. We propose to evaluate the reliability of such decisions by using a new measure Decision Impact on Reliability DIoR for short. We find for example that the current leader on HELM may change by merely removing a low-ranked model from the benchmark and observe that a handful of examples suffice to obtain the correct benchmark rank
    
[^274]: DiagGPT:一种基于LLM的任务导向对话的聊天机器人

    DiagGPT: An LLM-based Chatbot with Automatic Topic Management for Task-Oriented Dialogue. (arXiv:2308.08043v1 [cs.CL])

    [http://arxiv.org/abs/2308.08043](http://arxiv.org/abs/2308.08043)

    DiagGPT将大型语言模型(LLMs)扩展到任务导向的对话场景，提供了在复杂诊断场景中主动提问和引导用户完成任务的能力。

    

    大型语言模型(LLMs)如ChatGPT正变得越来越复杂，展示出与人类相似的能力。这些AI模型在日常生活中辅助人类完成各种任务方面发挥着重要作用。AI作为聊天代理人的重要应用是回答人类在各个领域的问题。目前的LLMs在回答一般问题方面已经显示出熟练的能力。然而，在复杂的诊断场景(如法律或医疗咨询)中，基本的问答对话往往表现不佳。这些场景通常需要任务导向对话(TOD)，其中AI聊天代理需要主动提问并引导用户完成特定任务。以前的微调模型在TOD方面表现不佳，而当前的LLMs并未固有这种能力。在本文中，我们介绍了一种名为DiagGPT (Diagnosis GPT)的创新方法，它将LLMs推广到TOD场景中。

    Large Language Models (LLMs), such as ChatGPT, are becoming increasingly sophisticated, demonstrating capabilities that closely resemble those of humans. These AI models are playing an essential role in assisting humans with a wide array of tasks in daily life. A significant application of AI is its use as a chat agent, responding to human inquiries across various domains. Current LLMs have shown proficiency in answering general questions. However, basic question-answering dialogue often falls short in complex diagnostic scenarios, such as legal or medical consultations. These scenarios typically necessitate Task-Oriented Dialogue (TOD), wherein an AI chat agent needs to proactively pose questions and guide users towards specific task completion. Previous fine-tuning models have underperformed in TOD, and current LLMs do not inherently possess this capability. In this paper, we introduce DiagGPT (Dialogue in Diagnosis GPT), an innovative method that extends LLMs to TOD scenarios. Our e
    
[^275]: RAVEN：上下文学习与检索增强的编码器-解码器语言模型

    RAVEN: In-Context Learning with Retrieval Augmented Encoder-Decoder Language Models. (arXiv:2308.07922v1 [cs.CL])

    [http://arxiv.org/abs/2308.07922](http://arxiv.org/abs/2308.07922)

    RAVEN是一种结合了检索增强的蒙特卡洛语言建模和前缀语言建模的模型，通过引入上下文融合学习，它能够在上下文学习方面取得比ATLAS更好的性能。

    

    本文研究了检索增强的编码器-解码器语言模型在上下文学习方面的能力。我们首先对现有的ATLAS模型进行全面分析，发现其在上下文学习方面存在限制，主要原因是预训练和测试之间存在不匹配，以及上下文长度受限。为了解决这些问题，我们提出了RAVEN模型，该模型结合了检索增强的蒙特卡洛语言建模和前缀语言建模。我们还引入了上下文融合学习，通过使模型能够利用更多上下文示例而无需额外训练或模型修改来提高少样本性能。通过大量实验，我们证明了RAVEN在某些场景下明显优于ATLAS，并达到了与最先进的语言模型相当的结果，尽管参数数量显著较少。我们的工作强调了检索增强的编码器-解码器语言模型的潜力。

    In this paper, we investigate the in-context learning ability of retrieval-augmented encoder-decoder language models. We first conduct a comprehensive analysis of the state-of-the-art ATLAS model and identify its limitations in in-context learning, primarily due to a mismatch between pretraining and testing, as well as a restricted context length. To address these issues, we propose RAVEN, a model that combines retrieval-augmented masked language modeling and prefix language modeling. We further introduce Fusion-in-Context Learning to enhance the few-shot performance by enabling the model to leverage more in-context examples without requiring additional training or model modifications. Through extensive experiments, we demonstrate that RAVEN significantly outperforms ATLAS and achieves results comparable to the most advanced language models in certain scenarios, despite having substantially fewer parameters. Our work underscores the potential of retrieval-augmented encoder-decoder lang
    
[^276]: 深度神经网络与纹理相结合的图像分类方法

    Deep Neural Networks Fused with Textures for Image Classification. (arXiv:2308.01813v1 [cs.CV])

    [http://arxiv.org/abs/2308.01813](http://arxiv.org/abs/2308.01813)

    本论文提出了一种将全局纹理与局部补丁信息相结合的新方法，用于解决细粒度图像分类问题。通过提取深层特征和计算图像级纹理，将两者的优势相结合，实现了更高的分类准确性。

    

    细粒度图像分类是计算机视觉中的一个具有挑战性的任务，由于亚类别之间视觉差异较小，但是类内变化较大。深度学习方法在解决细粒度图像分类方面取得了显著的成功。本文提出了一种融合方法来解决细粒度图像分类问题，通过将全局纹理与基于局部补丁信息的特征相结合。第一个步骤从各种固定大小的非重叠补丁中提取深层特征，并使用长短期记忆（LSTM）进行特征编码。另一个步骤使用局部二值模式（LBP）在多个尺度上计算图像级纹理。两个流的优点被整合到表示高效特征向量的方法中，用于图像分类。该方法在代表人脸、皮肤病变、食物盘子、海洋生物等的八个数据集上经过四个标准骨干卷积神经网络的测试。与现有方法相比，我们的方法在分类准确性上取得了更好的结果。

    Fine-grained image classification (FGIC) is a challenging task in computer vision for due to small visual differences among inter-subcategories, but, large intra-class variations. Deep learning methods have achieved remarkable success in solving FGIC. In this paper, we propose a fusion approach to address FGIC by combining global texture with local patch-based information. The first pipeline extracts deep features from various fixed-size non-overlapping patches and encodes features by sequential modelling using the long short-term memory (LSTM). Another path computes image-level textures at multiple scales using the local binary patterns (LBP). The advantages of both streams are integrated to represent an efficient feature vector for image classification. The method is tested on eight datasets representing the human faces, skin lesions, food dishes, marine lives, etc. using four standard backbone CNNs. Our method has attained better classification accuracy over existing methods with no
    
[^277]: XSTest: 用于识别大型语言模型中夸大安全行为的测试套件

    XSTest: A Test Suite for Identifying Exaggerated Safety Behaviours in Large Language Models. (arXiv:2308.01263v1 [cs.CL])

    [http://arxiv.org/abs/2308.01263](http://arxiv.org/abs/2308.01263)

    本文介绍了一个名为XSTest的测试套件，旨在识别大型语言模型中夸大的安全行为。该套件由200个安全提示组成，涵盖十种提示类型，旨在引出模型的系统性问题。

    

    没有适当的保护措施，大型语言模型很容易遵循恶意指令并生成有害内容。这激发了安全工作，如红队测试和大规模反馈学习，旨在使模型既有用又无害。然而，这两个目标之间存在一种紧张关系，因为无害性要求模型拒绝遵从不安全的提示，从而无法提供帮助。最近的一些证据表明，一些模型可能在平衡上存在问题，以至于即使使用类似不安全提示的语言或提及敏感主题的明显安全提示也会被拒绝。本文介绍了一个名为XSTest的新测试套件，以系统化和结构化的方式识别这种夸张的安全行为。目前，XSTest包括200个安全提示，涵盖十种提示类型，良好校准的模型不应该拒绝遵循这些提示。我们描述了XSTest的创建和组成，并使用测试套件突显系统性的问题。

    Without proper safeguards, large language models will readily follow malicious instructions and generate toxic content. This motivates safety efforts such as red-teaming and large-scale feedback learning, which aim to make models both helpful and harmless. However, there is a tension between these two objectives, since harmlessness requires models to refuse complying with unsafe prompts, and thus not be helpful. Recent anecdotal evidence suggests that some models may have struck a poor balance, so that even clearly safe prompts are refused if they use similar language to unsafe prompts or mention sensitive topics. In this paper, we introduce a new test suite called XSTest to identify such eXaggerated Safety behaviours in a structured and systematic way. In its current form, XSTest comprises 200 safe prompts across ten prompt types that well-calibrated models should not refuse to comply with. We describe XSTest's creation and composition, and use the test suite to highlight systematic f
    
[^278]: 被指导的偏见：经过指导调优的语言模型呈现出新兴的认知偏见

    Instructed to Bias: Instruction-Tuned Language Models Exhibit Emergent Cognitive Bias. (arXiv:2308.00225v1 [cs.CL])

    [http://arxiv.org/abs/2308.00225](http://arxiv.org/abs/2308.00225)

    这项研究发现，经过指导调优的语言模型呈现出新兴的认知偏见，这对于理解和开发更可靠和无偏的语言模型至关重要。

    

    最近的研究表明，指导调优和从人类反馈中学习可以显著提高大语言模型（LMs）的能力。虽然这些调优方法可以使模型生成高质量的文本，但我们推测这些经过调优的模型可能会产生更多隐含的认知偏见。我们的研究提供了证据，表明这些经过调优的模型呈现出先前预训练模型中不存在或较不明显的偏见。我们对三种认知偏见进行了研究，包括矛盾效应、确定性效应和信念偏见，这些偏见已被证实对人类的决策和推理有影响。我们的研究结果突显了这些偏见在各种模型中的存在，特别是那些经过指导调优的模型，如Flan-T5、GPT3.5和GPT4。这项研究对于理解指导调优的LMs中的认知偏见是至关重要的，这有助于开发更可靠和无偏的语言模型。

    Recent studies show that instruction tuning and learning from human feedback improve the abilities of large language models (LMs) dramatically. While these tuning methods can make models generate high-quality text, we conjecture that more implicit cognitive biases may arise in these fine-tuned models. Our work provides evidence that these fine-tuned models exhibit biases that were absent or less pronounced in their pretrained predecessors. We examine the extent of this phenomenon in three cognitive biases - the decoy effect, the certainty effect, and the belief bias - all of which are known to influence human decision-making and reasoning. Our findings highlight the presence of these biases in various models, especially those that have undergone instruction tuning, such as Flan-T5, GPT3.5, and GPT4. This research constitutes a step toward comprehending cognitive biases in instruction-tuned LMs, which is crucial for the development of more reliable and unbiased language models.
    
[^279]: SAS视频问答：自适应采样用于高效视频问答

    SAS Video-QA: Self-Adaptive Sampling for Efficient Video Question-Answering. (arXiv:2307.04192v1 [cs.CV])

    [http://arxiv.org/abs/2307.04192](http://arxiv.org/abs/2307.04192)

    SAS视频问答通过自适应采样策略解决了视频问答中的问题，提高了效率和准确性

    

    视频问答是视频理解领域的一项基础任务。尽管当前的视觉-语言模型(VLMs)配备了视频变换器(Video Transformers)，实现了时间建模并取得了优秀的结果，但代价是巨大的计算能力，因此在实时应用场景中过于昂贵。一种经济的解决方法是只对视频的一小部分帧进行采样，来代表视频的主要内容，并在这些采样帧上调整图像-文本模型。最近的视频理解模型通常随机采样一组帧或片段，而不考虑它们的内部关联性和与问题的相关性。我们认为这种无目标的采样可能会遗漏可以推导出正确答案的关键帧，在采样稀疏程度增加时，情况会变得更糟，而随着视频长度的增加，采样稀疏程度也会增加。为了解决这个问题，我们提出了两种帧采样策略，分别是

    Video question--answering is a fundamental task in the field of video understanding. Although current vision--language models (VLMs) equipped with Video Transformers have enabled temporal modeling and yielded superior results, they are at the cost of huge computational power and thus too expensive to deploy in real-time application scenarios. An economical workaround only samples a small portion of frames to represent the main content of that video and tune an image--text model on these sampled frames. Recent video understanding models usually randomly sample a set of frames or clips, regardless of internal correlations between their visual contents, nor their relevance to the problem. We argue that such kinds of aimless sampling may omit the key frames from which the correct answer can be deduced, and the situation gets worse when the sampling sparsity increases, which always happens as the video lengths increase. To mitigate this issue, we propose two frame sampling strategies, namel
    
[^280]: 一种可解释的增量随机权重神经网络及其应用的构造算法

    An Interpretable Constructive Algorithm for Incremental Random Weight Neural Networks and Its Application. (arXiv:2307.00185v1 [cs.LG])

    [http://arxiv.org/abs/2307.00185](http://arxiv.org/abs/2307.00185)

    本文提出了一种可解释的增量随机权重神经网络的构造算法，通过几何信息约束和节点池策略解决了难以解释隐藏参数与残差误差之间关系的问题。这种算法在大规模数据建模任务中表现出了良好的性能。

    

    增量随机权重神经网络(IRWNNs)由于易于实现和快速学习而受到关注。然而，IRWNNs的一个显著缺点是难以解释隐藏参数（节点）与残差误差（模型性能）之间的关系。为了解决这个问题，本文提出了一个具有几何信息约束的可解释的构造算法(ICA)。首先，基于隐藏参数与残差误差之间的几何关系，提出了一个可解释的几何信息约束来随机分配隐藏参数。同时，采用节点池策略获取更有利于收敛的隐藏参数。此外，证明了ICA的通用逼近性质。最后，提出了ICA的轻量级版本用于大规模数据建模任务。在六个基准数据集上的实验结果表明了该算法的有效性。

    Incremental random weight neural networks (IRWNNs) have gained attention in view of its easy implementation and fast learning. However, a significant drawback of IRWNNs is that the elationship between the hidden parameters (node)and the residual error (model performance) is difficult to be interpreted. To address the above issue, this article proposes an interpretable constructive algorithm (ICA) with geometric information constraint. First, based on the geometric relationship between the hidden parameters and the residual error, an interpretable geometric information constraint is proposed to randomly assign the hidden parameters. Meanwhile, a node pool strategy is employed to obtain hidden parameters that is more conducive to convergence from hidden parameters satisfying the proposed constraint. Furthermore, the universal approximation property of the ICA is proved. Finally, a lightweight version of ICA is presented for large-scale data modeling tasks. Experimental results on six ben
    
[^281]: 多模态大语言模型综述

    A Survey on Multimodal Large Language Models. (arXiv:2306.13549v1 [cs.CV])

    [http://arxiv.org/abs/2306.13549](http://arxiv.org/abs/2306.13549)

    本文追踪和总结了多模态大语言模型（MLLM）的最新进展，包括多模态指令调整、多模态上下文学习、多模态思维链和LLM辅助视觉推理等应用，指出了现有挑战和有前途的研究方向。

    

    多模态大语言模型（MLLM）是一种新兴的研究热点，使用强大的大语言模型作为大脑执行多模态任务。MLLM 的惊人能力，如基于图像编写故事和无OCR数学推理等，在传统方法中很少见，表明了通向人工智能的潜在路径。本文旨在追踪和总结 MLLM 的最新进展。首先，我们介绍了 MLLM 的构成，概述了相关概念。然后，讨论了关键技术和应用，包括多模态指令调整（M-IT）、多模态上下文学习（M-ICL）、多模态思维链（M-CoT）和LLM辅助视觉推理（LAVR）。最后，我们讨论了现有的挑战，并指出了有前途的研究方向。鉴于 MLLM 时代才刚刚开始，我们会不断更新这个综述，并希望能激发更多的研究。

    Multimodal Large Language Model (MLLM) recently has been a new rising research hotspot, which uses powerful Large Language Models (LLMs) as a brain to perform multimodal tasks. The surprising emergent capabilities of MLLM, such as writing stories based on images and OCR-free math reasoning, are rare in traditional methods, suggesting a potential path to artificial general intelligence. In this paper, we aim to trace and summarize the recent progress of MLLM. First of all, we present the formulation of MLLM and delineate its related concepts. Then, we discuss the key techniques and applications, including Multimodal Instruction Tuning (M-IT), Multimodal In-Context Learning (M-ICL), Multimodal Chain of Thought (M-CoT), and LLM-Aided Visual Reasoning (LAVR). Finally, we discuss existing challenges and point out promising research directions. In light of the fact that the era of MLLM has only just begun, we will keep updating this survey and hope it can inspire more research. An associated
    
[^282]: 基于子图网络的对比学习

    Subgraph Networks Based Contrastive Learning. (arXiv:2306.03506v1 [cs.LG])

    [http://arxiv.org/abs/2306.03506](http://arxiv.org/abs/2306.03506)

    本文提出了一种新的对比学习框架，名为基于子图网络的对比学习(SGNCL)，通过应用子图网络生成策略以产生增强视图，并探究了子结构相互作用对图形表示的影响。

    

    图对比学习(GCL)是一种自监督学习方法，可解决注释数据稀缺的问题。 它在未注释的图形中挖掘显式特征以生成下游任务的有利图形表示。大多数现有的GCL方法侧重于图形增强策略和相互信息估计操作的设计。 然而，这些方法没有考虑子图中存在的相互作用。为了探索子结构相互作用对图形表示的影响，我们提出了一种名为subgraph network-based contrastive learning (SGNCL)的新框架。SGNCL应用子图网络生成策略以产生增强视图。该策略将原始图转换为具有拓扑和属性特征的边到节点映射网络。单次增强视图是

    Graph contrastive learning (GCL), as a self-supervised learning method, can solve the problem of annotated data scarcity. It mines explicit features in unannotated graphs to generate favorable graph representations for downstream tasks. Most existing GCL methods focus on the design of graph augmentation strategies and mutual information estimation operations. Graph augmentation produces augmented views by graph perturbations. These views preserve a locally similar structure and exploit explicit features. However, these methods have not considered the interaction existing in subgraphs. To explore the impact of substructure interactions on graph representations, we propose a novel framework called subgraph network-based contrastive learning (SGNCL). SGNCL applies a subgraph network generation strategy to produce augmented views. This strategy converts the original graph into an Edge-to-Node mapping network with both topological and attribute features. The single-shot augmented view is a 
    
[^283]: SQL-PaLM：针对Text-to-SQL的改进大语言模型适应性

    SQL-PaLM: Improved Large Language Model Adaptation for Text-to-SQL. (arXiv:2306.00739v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2306.00739](http://arxiv.org/abs/2306.00739)

    本文提出了一种基于大语言模型的Text-to-SQL模型SQL-PaLM，使用了面向Text-to-SQL的基于执行的自一致提示方法，在Spider上实现了77.3%的测试套件准确度，并显着超越以前的最新技术的方法。

    

    大语言模型（LLMs）的一个令人印象深刻的新兴功能是生成代码，包括用于数据库的结构化查询语言（SQL）。对于将自然语言文本转换为SQL查询的任务，即Text-to-SQL，LLMs的适应性至关重要，具体取决于使用的适应性数据量。本文提出了一种基于LLM的Text-to-SQL模型SQL-PaLM，利用了PaLM-2，推动了两种设置的最新进展。Few-shot SQL-PaLM基于面向Text-to-SQL的基于执行的自一致提示方法，可在Spider上实现77.3%的测试套件准确度，据我们所知，这是第一个通过显着较大的微调超越以前的最新技术的方法。此外，我们证明经过精细调整的SQL-PALM可进一步提高1%的性能。为了将SQL-PaLM应用于实际场景，我们进一步评估了其对其他挑战的稳健性。

    One impressive emergent capability of large language models (LLMs) is generation of code, including Structured Query Language (SQL) for databases. For the task of converting natural language text to SQL queries, Text-to-SQL, adaptation of LLMs is of paramount importance, both in in-context learning and fine-tuning settings, depending on the amount of adaptation data used. In this paper, we propose an LLM-based Text-to-SQL model SQL-PaLM, leveraging on PaLM-2, that pushes the state-of-the-art in both settings. Few-shot SQL-PaLM is based on an execution-based self-consistency prompting approach designed for Text-to-SQL, and achieves 77.3% in test-suite accuracy on Spider, which to our best knowledge is the first to outperform previous state-of-the-art with fine-tuning by a significant margin, 4%. Furthermore, we demonstrate that the fine-tuned SQL-PALM outperforms it further by another 1%. Towards applying SQL-PaLM to real-world scenarios we further evaluate its robustness on other chall
    
[^284]: 探索唇部运动的语音上下文对真实说话人脸生成的影响

    Exploring Phonetic Context in Lip Movement for Authentic Talking Face Generation. (arXiv:2305.19556v1 [cs.CV])

    [http://arxiv.org/abs/2305.19556](http://arxiv.org/abs/2305.19556)

    本研究探讨了语音上下文对真实说话人脸生成的影响，提出了一种Context-Aware Lip-Sync框架（CALS），可利用语音上下文生成更加准确、稳定的唇部运动。

    

    说话人脸生成是将自然面部与驱动音频同步合成的任务。尽管在视觉质量、唇形同步和面部动作方面取得了很大进展，但当前的研究仍然难以解决粗糙和异步的唇部运动问题，这可能导致类似木偶动画的效果。本文发现，以往的作品通常将唇部运动与音频在不同的音素级别上进行相关联，然而，由于音素之间的协同发音（co-articulation）现象，即隔离的音素受前一个或下一个音素的影响，因此同一个音素的发音因音素上下文而异。因此，使用音素上下文模型可以生成更加空间和时间上对齐、稳定的唇部运动。基于此，我们研究了唇部运动中的语音上下文对于真实说话人脸生成的影响。我们提出了一种Context-Aware Lip-Sync框架（CALS），利用语音上下文生成更加空间和时间上对齐、稳定的唇部运动。

    Talking face generation is the task of synthesizing a natural face synchronous to driving audio. Although much progress has been made in terms of visual quality, lip synchronization, and facial motion of the talking face, current works still struggle to overcome issues of crude and asynchronous lip movement, which can result in puppetry-like animation. We identify that the prior works commonly correlate lip movement with audio at the phone level. However, due to co-articulation, where an isolated phone is influenced by the preceding or following phones, the articulation of a phone varies upon the phonetic context. Therefore, modeling lip motion with the phonetic context can generate more spatio-temporally aligned and stable lip movement. In this respect, we investigate the phonetic context in lip motion for authentic talking face generation. We propose a Context-Aware Lip-Sync framework (CALS), which leverages phonetic context to generate more spatio-temporally aligned and stable lip m
    
[^285]: 通过逻辑驱动的数据增强增强大型语言模型的逻辑推理能力

    Enhancing Logical Reasoning of Large Language Models through Logic-Driven Data Augmentation. (arXiv:2305.12599v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.12599](http://arxiv.org/abs/2305.12599)

    本论文介绍了一种通过逻辑驱动的数据增强方法来增强大型语言模型的逻辑推理能力。通过将原始文本转换为抽象意义表述图，并对其进行逻辑修改和转换，生成增强数据，从而提升模型性能。该方法适用于各种体系结构的大型语言模型。

    

    将大型语言模型与逻辑推理相结合可以增强它们在问题解决中的能力，使其更加强大和可靠。然而，逻辑推理的复杂性使得从网页上收集可靠的数据来建立全面的训练数据集面临困难，进而影响下游任务的性能。为了解决这个问题，我们提出了一种新颖的逻辑驱动数据增强方法，AMR-LDA。AMR-LDA将原始文本转换成抽象意义表示（AMR）图，这是一种结构化的语义表示，包含了句子的逻辑结构，然后对该图进行操作以生成逻辑修改后的AMR图。修改后的AMR图随后被转换回文本，从而创建增强数据。值得注意的是，我们的方法与体系结构无关，并通过提示增强来增强生成型大型语言模型（如GPT-3.5和GPT-4），并通过微调来增强判别型大型语言模型。

    Combining large language models with logical reasoning enhance their capacity to address problems in a robust and reliable manner. Nevertheless, the intricate nature of logical reasoning poses challenges to gathering reliable data from web for building comprehensive training datasets, subsequently affecting the performance on downstream tasks. To address this, we introduce a novel logic-driven data augmentation approach, AMR-LDA. AMR-LDA converts the original text into an Abstract Meaning Representation (AMR) graph, a structured semantic representation that encapsulates the logic structure of the sentence, upon which operations are performed to generate logically modified AMR graphs. The modified AMR graphs are subsequently converted back into texts to create augmented data. Notably, our methodology is architecture-agnostic and enhances generative large language models, such as GPT-3.5 and GPT-4, through prompt augmentation, and fine-tuning discriminative large language models through 
    
[^286]: Tram：一个源代码摘要的令牌级检索增强机制

    Tram: A Token-level Retrieval-augmented Mechanism for Source Code Summarization. (arXiv:2305.11074v1 [cs.AI])

    [http://arxiv.org/abs/2305.11074](http://arxiv.org/abs/2305.11074)

    Tram是一种源代码摘要的令牌级别检索增强机制，它在解码器端精细检索帮助神经模型生成更准确的摘要，并在Java和Python源代码摘要任务上表现优异。

    

    自动生成人类可读的文本以描述程序的功能是源代码摘要的目标。虽然神经语言模型在这个领域取得了显著的性能，但结合神经模型和外部知识的新趋势正在兴起。大多数先前的方法依赖于句子级别的检索和组合范式（检索类似的代码片段并使用相应的代码和摘要对来编码）。然而，这种范式是粗粒度的，不能直接利用解码器端高质量的检索摘要令牌。本文中，我们探讨了一种精细的令牌级别检索增强机制，在解码器端帮助原始神经模型生成更好的代码摘要。此外，为了缓解令牌级别检索在捕捉上下文代码语义方面的局限性，我们提出将代码语义集成到摘要令牌中。大量的实验和人类评估表明，我们提出的方法Tram在Java和Python源代码摘要任务上均优于最先进的方法。

    Automatically generating human-readable text describing the functionality of a program is the intent of source code summarization. Although Neural Language Models achieve significant performance in this field, an emerging trend is combining neural models with external knowledge. Most previous approaches rely on the sentence-level retrieval and combination paradigm (retrieval of similar code snippets and use of the corresponding code and summary pairs) on the encoder side. However, this paradigm is coarse-grained and cannot directly take advantage of the high-quality retrieved summary tokens on the decoder side. In this paper, we explore a fine-grained token-level retrieval-augmented mechanism on the decoder side to help the vanilla neural model generate a better code summary. Furthermore, to mitigate the limitation of token-level retrieval on capturing contextual code semantics, we propose to integrate code semantics into summary tokens. Extensive experiments and human evaluation revea
    
[^287]: 使用大型语言模型在知识图谱上进行复杂逻辑推理

    Complex Logical Reasoning over Knowledge Graphs using Large Language Models. (arXiv:2305.01157v1 [cs.LO])

    [http://arxiv.org/abs/2305.01157](http://arxiv.org/abs/2305.01157)

    本文提出了一种使用大型语言模型的解耦方法，将复杂的知识图谱推理形式化为上下文知识图搜索和抽象逻辑查询推理的组合，与现有方法相比，它在多个逻辑查询结构的标准基准数据集上都表现出更好的性能，并且在更高复杂性的查询中获得了显着的性能提升。

    

    在知识图谱上进行推理是一项具有挑战性的任务，它需要对实体之间的复杂关系以及它们之间的基础逻辑进行深入理解。当前的方法依赖于学习几何来嵌入实体的向量空间进行逻辑查询操作，但是它们在复杂查询和特定数据集表示方面表现不佳。本文提出了一种新颖的解耦方法，称为基于语言引导的知识图谱抽象推理（LARK），将复杂的知识图谱推理形式化为上下文知识图搜索和抽象逻辑查询推理的组合，以分别利用图形提取算法和大型语言模型的优势。我们的实验表明，所提出的方法在多个逻辑查询结构的标准基准数据集上优于现有的知识图谱推理方法，在更高复杂性的查询中获得了显着的性能提升。

    Reasoning over knowledge graphs (KGs) is a challenging task that requires a deep understanding of the complex relationships between entities and the underlying logic of their relations. Current approaches rely on learning geometries to embed entities in vector space for logical query operations, but they suffer from subpar performance on complex queries and dataset-specific representations. In this paper, we propose a novel decoupled approach, Language-guided Abstract Reasoning over Knowledge graphs (LARK), that formulates complex KG reasoning as a combination of contextual KG search and abstract logical query reasoning, to leverage the strengths of graph extraction algorithms and large language models (LLM), respectively. Our experiments demonstrate that the proposed approach outperforms state-of-the-art KG reasoning methods on standard benchmark datasets across several logical query constructs, with significant performance gain for queries of higher complexity. Furthermore, we show t
    
[^288]: 使用计算注意力预测人类视觉注意力

    Predicting Human Attention using Computational Attention. (arXiv:2303.09383v1 [cs.CV])

    [http://arxiv.org/abs/2303.09383](http://arxiv.org/abs/2303.09383)

    HAT是一种计算注意力模型，使用新颖的基于变换器的结构和简化的凹视网膜，实现了对于目标存在和目标缺失搜索期间注视行为的扫描路径的最新技术水平。

    

    大多数视觉注意力模型旨在预测自上而下或自下而上控制，采用不同的视觉搜索和自由观看任务进行研究。我们提出了一个称为人类注意力变换器的单一模型，可以预测这两种形式的注意力控制。HAT在预测目标存在和目标缺失搜索期间进行注视行为的扫描路径方面是新的最先进技术，在预测无任务自由观看注视路径方面匹配或超过了当前技术水平。HAT通过使用新颖的基于变换器的结构和简化的凹视网膜，共同创建类似于人类动态视觉工作记忆的时空意识，实现了这种新的技术水平。与以前依赖于粗糙的注视单元格网格并由于离散化固定而经历信息丢失的方法不同，HAT具有密集预测架构，并为每个注视输出密集热图，从而避免离散注视。HAT在计算视觉注意力方面设定了一个新的标准。

    Most models of visual attention are aimed at predicting either top-down or bottom-up control, as studied using different visual search and free-viewing tasks. We propose Human Attention Transformer (HAT), a single model predicting both forms of attention control. HAT is the new state-of-the-art (SOTA) in predicting the scanpath of fixations made during target-present and target-absent search, and matches or exceeds SOTA in the prediction of taskless free-viewing fixation scanpaths. HAT achieves this new SOTA by using a novel transformer-based architecture and a simplified foveated retina that collectively create a spatio-temporal awareness akin to the dynamic visual working memory of humans. Unlike previous methods that rely on a coarse grid of fixation cells and experience information loss due to fixation discretization, HAT features a dense-prediction architecture and outputs a dense heatmap for each fixation, thus avoiding discretizing fixations. HAT sets a new standard in computati
    
[^289]: 3D蒙版自编码和伪标签用于异构婴儿脑 MRI 领域间适应性标记

    3D Masked Autoencoding and Pseudo-labeling for Domain Adaptive Segmentation of Heterogeneous Infant Brain MRI. (arXiv:2303.09373v1 [cs.CV])

    [http://arxiv.org/abs/2303.09373](http://arxiv.org/abs/2303.09373)

    本文提出了一种名为 MAPSeg 的新框架，采用 3D 蒙版自编码和伪标签的方式，实现了跨年龄、跨模态和跨场景下对婴儿脑 MRI 中亚皮质区域的分割，充分考虑不同 MRI 扫描仪、供应商或采集序列以及不同的神经发育阶段所造成的内在异质性，提高了分割结果的鲁棒性。

    

    婴儿脑 MRI 在跨年龄、跨模态、跨场景下实现鲁棒的分割仍然是具有挑战性的。本文介绍了一种名为 MAPSeg 的新框架，它使用 3D 蒙版自编码和蒙版伪标签的方式来对婴儿脑MRI的不同亚皮质区域进行分割，并联合学习标记源域数据和未标记目标域数据，以提高分割结果的鲁棒性。

    Robust segmentation of infant brain MRI across multiple ages, modalities, and sites remains challenging due to the intrinsic heterogeneity caused by different MRI scanners, vendors, or acquisition sequences, as well as varying stages of neurodevelopment. To address this challenge, previous studies have explored domain adaptation (DA) algorithms from various perspectives, including feature alignment, entropy minimization, contrast synthesis (style transfer), and pseudo-labeling. This paper introduces a novel framework called MAPSeg (Masked Autoencoding and Pseudo-labelling Segmentation) to address the challenges of cross-age, cross-modality, and cross-site segmentation of subcortical regions in infant brain MRI. Utilizing 3D masked autoencoding as well as masked pseudo-labeling, the model is able to jointly learn from labeled source domain data and unlabeled target domain data. We evaluated our framework on expert-annotated datasets acquired from different ages and sites. MAPSeg consist
    
[^290]: 带有内外部注意力的形状引导扩散

    Shape-Guided Diffusion with Inside-Outside Attention. (arXiv:2212.00210v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2212.00210](http://arxiv.org/abs/2212.00210)

    该论文提出了一种无需训练的形状引导扩散方法，使用一种新颖的内外部注意机制将形状限制应用于跨注意力图和自注意力图上，从而在文本到图像扩散模型中考虑到对象形状，进而可以实现对象形状忠实度更高的图像生成。

    

    在操作对象时，现有的文本到图像扩散模型通常忽略对象的形状并生成错误比例、被截断或被背景内容替换的图像。我们提出了一种无需训练的方法 Shape-Guided Diffusion，该方法修改了预训练扩散模型，使之对用户指定的形状输入或从文本中自动推断的形状敏感。我们使用一种新颖的内外部注意机制，在反演和生成过程中将此形状限制应用于跨注意力图和自注意力图上。我们的机制指定空间区域是对象（内部）还是背景（外部），然后将文本提示指定的编辑与正确的区域相关联。我们在形状引导编辑任务上展示了我们方法的有效性，其中模型必须根据文本提示和对象掩码替换对象。我们创建了一个新的从 MS-COCO 衍生的 ShapePrompts 基准，并在形状忠实度方面实现了 SOTA 的结果，而不需要降级。

    When manipulating an object, existing text-to-image diffusion models often ignore the shape of the object and generate content that is incorrectly scaled, cut off, or replaced with background content. We propose a training-free method, Shape-Guided Diffusion, that modifies pretrained diffusion models to be sensitive to shape input specified by a user or automatically inferred from text. We use a novel Inside-Outside Attention mechanism during the inversion and generation process to apply this shape constraint to the cross- and self-attention maps. Our mechanism designates which spatial region is the object (inside) vs. background (outside) then associates edits specified by text prompts to the correct region. We demonstrate the efficacy of our method on the shape-guided editing task, where the model must replace an object according to a text prompt and object mask. We curate a new ShapePrompts benchmark derived from MS-COCO and achieve SOTA results in shape faithfulness without a degra
    
[^291]: 使用说服性写作策略来解释和检测健康错误信息

    Using Persuasive Writing Strategies to Explain and Detect Health Misinformation. (arXiv:2211.05985v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2211.05985](http://arxiv.org/abs/2211.05985)

    本研究旨在通过使用说服性写作技巧的文本段落进行分类来增加自动化虚假信息检测的新层次，以产生可解释的理由。我们提出了一个包含常见说服性写作策略的注释方案和数据集，并使用 RoBERTa 文本分类模型进行实验。

    

    虚假信息的传播是当今社会的一大问题，许多学术界和工业界的研究人员正在努力解决这个问题。由于每天创造的虚假信息数量巨大，将此任务留给人工事实检查员是不切实际的。数据科学家和研究人员多年来一直致力于自动化虚假信息检测，但今天仍然是一个具有挑战性的问题。我们的研究目标是为自动化虚假信息检测添加一个新层次；使用具有说服性写作技巧的文本段落进行分类，以产生可解释的理由，说明为什么这篇文章可以标记为虚假信息。为此，我们提出了一个包含许多常见说服性写作策略的新注释方案，以及相应的人工注释数据集。我们使用 RoBERTa 文本分类模型来完成此任务，因为它在自然语言处理方面具有高性能。我们开发了几种基于语言模型的基线模型，并提供了结果分析。

    The spread of misinformation is a prominent problem in today's society, and many researchers in academia and industry are trying to combat it. Due to the vast amount of misinformation that is created every day, it is unrealistic to leave this task to human fact-checkers. Data scientists and researchers have been working on automated misinformation detection for years, and it is still a challenging problem today. The goal of our research is to add a new level to automated misinformation detection; classifying segments of text with persuasive writing techniques in order to produce interpretable reasoning for why an article can be marked as misinformation. To accomplish this, we present a novel annotation scheme containing many common persuasive writing tactics, along with a dataset with human annotations accordingly. For this task, we make use of a RoBERTa model for text classification, due to its high performance in NLP. We develop several language model-based baselines and present the 
    
[^292]: WaveMix: 一种用于图像分析的资源高效神经网络

    WaveMix: A Resource-efficient Neural Network for Image Analysis. (arXiv:2205.14375v4 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2205.14375](http://arxiv.org/abs/2205.14375)

    WaveMix是一种资源高效的神经网络结构，可以在多项任务上达到可比或更好的准确率，并且需要更少的参数和GPU RAM，实现时间、成本和能量的节省。

    

    我们提出了WaveMix——一种新颖的神经网络结构，既具有资源效率性，又具有通用性和可扩展性。WaveMix网络在多项任务上达到了可比或更好的准确率，包括Cityscapes中的分割和Places-365、五个EMNIST数据集和iNAT-mini中的分类，并建立了新的基准。令人惊奇的是，与先前的最先进技术相比，WaveMix结构所需的参数更少。此外，当控制参数数量时，WaveMix所需的GPU RAM更少，这意味着节省时间、成本和能量。为了实现这些收益，我们在WaveMix块中使用了多级二维离散小波变换（2D-DWT），它具有以下优点:(1)它基于三种强图像先验条件重新组织空间信息——尺度不变性，位移不变性和边缘的稀疏性,(2) i

    We propose WaveMix -- a novel neural architecture for computer vision that is resource-efficient yet generalizable and scalable. WaveMix networks achieve comparable or better accuracy than the state-of-the-art convolutional neural networks, vision transformers, and token mixers for several tasks, establishing new benchmarks for segmentation on Cityscapes; and for classification on Places-365, five EMNIST datasets, and iNAT-mini. Remarkably, WaveMix architectures require fewer parameters to achieve these benchmarks compared to the previous state-of-the-art. Moreover, when controlled for the number of parameters, WaveMix requires lesser GPU RAM, which translates to savings in time, cost, and energy. To achieve these gains we used multi-level two-dimensional discrete wavelet transform (2D-DWT) in WaveMix blocks, which has the following advantages: (1) It reorganizes spatial information based on three strong image priors -- scale-invariance, shift-invariance, and sparseness of edges, (2) i
    
[^293]: 面对混淆因素的悲观情绪：部分可观察马尔可夫决策过程的证明有效离线强化学习

    Pessimism in the Face of Confounders: Provably Efficient Offline Reinforcement Learning in Partially Observable Markov Decision Processes. (arXiv:2205.13589v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.13589](http://arxiv.org/abs/2205.13589)

    本文提出了代理变量悲观策略优化 (P3O) 算法，它通过近端因果推断构建悲观置信区间耦合序列解决了部分可观察马尔可夫决策过程中的混淆偏差和最优策略与行为策略之间的分布偏移问题。

    

    本文研究了部分可观测马尔可夫决策过程中的离线强化学习。特别地，我们旨在从由行为策略收集的数据集中学习最优策略，该策略可能取决于潜在状态。这样的数据集在混淆意义上同时影响行动和观测值，这对于现有的离线强化学习算法来说是禁止的。为此，我们提出了通过近端因果推断构建的悲观置信区间耦合序列的代理变量悲观策略优化（P3O）算法，该算法在广义函数逼近的上下文中解决了混淆偏差和最优策略与行为策略之间的分布偏移问题。我们证明，在混淆数据集的部分覆盖假设下，P3O可以实现n^{-1/2}的收敛率。

    We study offline reinforcement learning (RL) in partially observable Markov decision processes. In particular, we aim to learn an optimal policy from a dataset collected by a behavior policy which possibly depends on the latent state. Such a dataset is confounded in the sense that the latent state simultaneously affects the action and the observation, which is prohibitive for existing offline RL algorithms. To this end, we propose the \underline{P}roxy variable \underline{P}essimistic \underline{P}olicy \underline{O}ptimization (\texttt{P3O}) algorithm, which addresses the confounding bias and the distributional shift between the optimal and behavior policies in the context of general function approximation. At the core of \texttt{P3O} is a coupled sequence of pessimistic confidence regions constructed via proximal causal inference, which is formulated as minimax estimation. Under a partial coverage assumption on the confounded dataset, we prove that \texttt{P3O} achieves a $n^{-1/2}$-
    
[^294]: 基于密集卷积神经网络的胸部疾病多标签分类方法

    Multi-Label Classification of Thoracic Diseases using Dense Convolutional Network on Chest Radiographs. (arXiv:2202.03583v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2202.03583](http://arxiv.org/abs/2202.03583)

    本研究提出了一种基于密集卷积神经网络和GRADCAM的胸部X光疾病多标签诊断模型，获得了在Cardiomegaly条件下最高的AUC得分0.896，并使用热图提高了模型的可解释性。

    

    传统的X光图像病理识别方法依赖于熟练的人类解释，并且往往耗时。深度学习技术的出现使自动诊断系统的开发成为可能，但这类系统的表现取决于模型的质量和它提供的可解释性水平。本文提出了一种使用密集卷积神经网络（DenseNet）和GRADCAM进行模型可解释性的胸部X光疾病多标签诊断模型。我们使用前置X光训练了我们的模型，并使用各种定量指标（包括受试者操作特征曲线下面积（AUC））评估了模型的性能。我们的模型在Cardiomegaly条件下达到了最高的AUC得分0.896，并获得了0.826的准确度。而在Nodule条件下获得了最低的AUC得分0.655，准确度为0.66。为了提高模型可解释性，并在决策方面建立信任，我们使用GRADCAM生成了热图，突出显示了对诊断最重要的X光区域。

    Traditional methods of identifying pathologies in X-ray images rely heavily on skilled human interpretation and are often time-consuming. The advent of deep learning techniques has enabled the development of automated disease diagnosis systems, but the performance of such systems is dependent on the quality of the model and the level of interpretability it provides. In this paper, we propose a multi-label disease diagnosis model for chest X-rays using a dense convolutional neural network (DenseNet) and model interpretability using GRADCAM. We trained our model using frontal X-rays and evaluated its performance using various quantitative metrics, including the area under the receiver operating characteristic curve (AUC). Our proposed model achieved the highest AUC score of 0.896 for the condition Cardiomegaly with an accuracy of 0.826, while the lowest AUC score was obtained for Nodule, at 0.655 with an accuracy of 0.66. To promote model interpretability and build trust in decision maki
    

