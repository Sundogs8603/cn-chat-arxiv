# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [On the Actionability of Outcome Prediction.](http://arxiv.org/abs/2309.04470) | 提高结果预测准确性有助于确定最适合的干预措施，但在存在多种可能干预的情况下，仅仅依赖结果预测进行干预可能无法达到预期效果。 |
| [^2] | [Subwords as Skills: Tokenization for Sparse-Reward Reinforcement Learning.](http://arxiv.org/abs/2309.04459) | 通过将行动空间离散化并采用分词技术，我们提出了一种在稀疏奖励强化学习中生成技巧的新方法。这种方法能够减少探索的难度，并在连续行动空间中达到良好的性能。 |
| [^3] | [Variations and Relaxations of Normalizing Flows.](http://arxiv.org/abs/2309.04433) | 正规化流是一种模型，采用一系列双射变换将复杂目标分布表示为简单基础分布。然而，由于对微分同胚的限制，其在有效表示复杂拓扑目标分布和处理先验分布与目标分布不同胚的情况方面存在局限性。相关研究通过结合其他生成模型的特点，放宽了正规化流的约束性，以平衡约束和灵活性之间的关系 |
| [^4] | [Create Your World: Lifelong Text-to-Image Diffusion.](http://arxiv.org/abs/2309.04430) | 本文介绍了一种终身文本到图像扩散模型（L2DM），该模型通过解决知识“灾难性遗忘”和文本提示中的语义“灾难性忽视”问题，能够在用户输入的文本提示下生成多样且高质量的图像。 |
| [^5] | [Advanced Computing and Related Applications Leveraging Brain-inspired Spiking Neural Networks.](http://arxiv.org/abs/2309.04426) | 本文总结了基于脑启发的脉冲神经网络在计算速度、实时信息处理和时空信息处理等方面的优势，分析了神经元模型和网络拓扑的特点，并回顾了脉冲神经网络算法和无监督学习算法以及有监督学习算法的应用。 |
| [^6] | [SynthoGestures: A Novel Framework for Synthetic Dynamic Hand Gesture Generation for Driving Scenarios.](http://arxiv.org/abs/2309.04421) | SynthoGestures是一种使用虚幻引擎合成逼真手势的新框架，可以用于驾驶场景下的动态人机界面。该框架通过生成多种变体和模拟不同摄像机类型，提高了手势识别的准确性并节省了数据集创建的时间和精力。 |
| [^7] | [Beyond Static Datasets: A Deep Interaction Approach to LLM Evaluation.](http://arxiv.org/abs/2309.04369) | 该论文提出了一种基于深度交互的LLM评估框架，突破了静态数据集的限制，能够评估LLM在动态实际场景中的能力，并适用于多种实际任务。 |
| [^8] | [Active Learning for Classifying 2D Grid-Based Level Completability.](http://arxiv.org/abs/2309.04367) | 本文提出了使用主动学习方法来学习2D基于网格的关卡可完成性分类，通过对Super Mario Bros.、Kid Icarus和一个类似Zelda的游戏生成的关卡进行标记，获得了更好的分类器性能。 |
| [^9] | [Zero-Shot Robustification of Zero-Shot Models With Foundation Models.](http://arxiv.org/abs/2309.04344) | 提出了一种零样本强化方法RoboShot，通过使用零样本语言模型从任务描述中获取有用的见解，并应用于预训练模型嵌入中以去除有害成分并增强有用成分，从而改善预训练模型的鲁棒性。 |
| [^10] | [Online Submodular Maximization via Online Convex Optimization.](http://arxiv.org/abs/2309.04339) | 本论文研究了在线设置下的一般性子模最大化问题，并将一类大型子模函数归约到在线凸优化问题中。这种归约方式可在组合优化中实现次线性遗憾，并且适用于许多不同版本的在线学习问题。 |
| [^11] | [Graph Neural Networks Use Graphs When They Shouldn't.](http://arxiv.org/abs/2309.04332) | 在图形预测问题中，GNNs倾向于过拟合图结构，即使在忽略图结构的情况下可以获得更好的解决方案。常规图对于这种过拟合更具鲁棒性。 |
| [^12] | [Incremental Learning of Humanoid Robot Behavior from Natural Interaction and Large Language Models.](http://arxiv.org/abs/2309.04316) | 本文提出了一个从自然互动中实现复杂行为增量学习的系统，并演示了在一个人形机器人上的应用。该系统利用大型语言模型对机器人的行为进行高级协调，通过交互式控制台生成Python语句来调用机器人的感知和动作，并通过将人类指令、环境观测和执行结果反馈给语言模型来实现循环交互。 |
| [^13] | [Federated Learning for Early Dropout Prediction on Healthy Ageing Applications.](http://arxiv.org/abs/2309.04311) | 本文提出了一种基于联邦学习的方法，用于预测健康老龄应用中的早期退学情况。该方法通过联合个体和组织进行训练，并实现了隐私保护和分布式学习。 |
| [^14] | [Navigating Out-of-Distribution Electricity Load Forecasting during COVID-19: A Continual Learning Approach Leveraging Human Mobility.](http://arxiv.org/abs/2309.04296) | 本研究提出了一种利用人类移动数据和持续学习技术的方法来解决COVID-19期间非分布期间的电力负荷预测问题，通过保留过去的见解并整合新的数据，提高了模型的准确性和鲁棒性。 |
| [^15] | [FIMO: A Challenge Formal Dataset for Automated Theorem Proving.](http://arxiv.org/abs/2309.04295) | FIMO是一个创新的数据集，用于IMO水平的自动定理证明，包含149个形式化数学问题陈述，目前仍存在挑战和限制。 |
| [^16] | [Fuzzy Fingerprinting Transformer Language-Models for Emotion Recognition in Conversations.](http://arxiv.org/abs/2309.04292) | 本文提出将模糊指纹和大型语言模型相结合，用于对话情感识别，以获得更简单和更可解释的分类器，并在DailyDialog ERC基准数据集上取得最先进的结果。 |
| [^17] | [Sequential Semantic Generative Communication for Progressive Text-to-Image Generation.](http://arxiv.org/abs/2309.04287) | 本文提出了一种通过序列语义生成通信的新框架，通过将图像转换为文本并按照信息量最大的优先级依次发送单词，实现了有效的图像传输和重建。 |
| [^18] | [UQ at #SMM4H 2023: ALEX for Public Health Analysis with Social Media.](http://arxiv.org/abs/2309.04213) | 本文提出了一种名为ALEX的框架，通过采用LLMs解释机制来改进社交媒体上的公共卫生分析性能。该方法通过数据增强和平衡训练解决了数据不平衡问题，并有效利用了LLMs的能力。 |
| [^19] | [Towards Mitigating Architecture Overfitting in Dataset Distillation.](http://arxiv.org/abs/2309.04195) | 本文解决了数据集蒸馏中的架构过度拟合问题，提出了一系列方法来提高不同网络架构在蒸馏训练数据上的泛化性能。 |
| [^20] | [Knowledge-tuning Large Language Models with Structured Medical Knowledge Bases for Reliable Response Generation in Chinese.](http://arxiv.org/abs/2309.04175) | 本研究提出了一种使用结构化医学知识库进行知识调整的方法，以提高大型语言模型在中文可靠响应生成方面的准确性和领域适应能力。 |
| [^21] | [Manifold-based Verbalizer Space Re-embedding for Tuning-free Prompt-based Classification.](http://arxiv.org/abs/2309.04174) | 本研究提出了一种无需调参的基于流形的语言转换器嵌入方法，通过保留同一类中的局部特性来进行分类，实验证明其与自动化的语言转换器效果相当。 |
| [^22] | [Leveraging Prototype Patient Representations with Feature-Missing-Aware Calibration to Mitigate EHR Data Sparsity.](http://arxiv.org/abs/2309.04160) | 本研究提出了一种利用原型患者表示和特征缺失感知校准的间接插补方法，以缓解电子健康记录数据稀疏性问题，通过获取更密集的嵌入来提高预测模型的有效性。 |
| [^23] | [NESTLE: a No-Code Tool for Statistical Analysis of Legal Corpus.](http://arxiv.org/abs/2309.04146) | NESTLE是一个无代码工具，用于进行大规模法律语料库的统计分析。它提供了搜索引擎、端到端的信息提取系统和一个大语言模型，可以通过聊天界面进行操作，使用户可以搜索目标文件、提取信息并可视化数据。 |
| [^24] | [Trustworthy and Synergistic Artificial Intelligence for Software Engineering: Vision and Roadmaps.](http://arxiv.org/abs/2309.04142) | 本文提出了可信和协同的软件工程人工智能的愿景和路线图，旨在提高开发人员的生产力和软件质量。实现这个愿景可能导致软件工程领域的一个重要转变，即软件工程2.0. |
| [^25] | [Proprioceptive External Torque Learning for Floating Base Robot and its Applications to Humanoid Locomotion.](http://arxiv.org/abs/2309.04138) | 本文介绍了一种使用本体感知传感器学习浮动基底机器人外部关节力矩的方法，实验证明该网络在估计外部力矩和接触力矩方面具有较小的误差，并且可用于实现稳定步行。 |
| [^26] | [Weakly Supervised Point Clouds Transformer for 3D Object Detection.](http://arxiv.org/abs/2309.04105) | 本文提出了一个弱监督的点云转换器框架用于3D目标检测，通过无监督投票提议模块和自注意机制，提高了学生网络的表现，减少了对注释数据的需求 |
| [^27] | [Data-driven classification of low-power communication signals by an unauthenticated user using a software-defined radio.](http://arxiv.org/abs/2309.04088) | 本文提出了一种基于神经网络的方法，利用LoRa信号的瞬时频率表示中的结构模式，实现了对低功率通信信号的分类。这种方法能够有效抵御非验证攻击者对LoRa协议的拒绝服务攻击。 |
| [^28] | [Curve Your Attention: Mixed-Curvature Transformers for Graph Representation Learning.](http://arxiv.org/abs/2309.04082) | 本文提出了一种混合曲率Transformer，用于图表示学习。通过将完全乘积立体变换器与标记化的图Transformer相结合，模型能够以端到端的方式学习适应输入图的曲率，从而更好地嵌入图中的分层或循环结构。 |
| [^29] | [SayNav: Grounding Large Language Models for Dynamic Planning to Navigation in New Environments.](http://arxiv.org/abs/2309.04077) | SayNav是一种使用大型语言模型进行动态规划导航的方法，通过使用人类知识和场景图实现对复杂导航任务的高效泛化，动态生成指令并根据新信息不断完善未来步骤。 |
| [^30] | [Computationally Efficient Data-Driven Discovery and Linear Representation of Nonlinear Systems For Control.](http://arxiv.org/abs/2309.04074) | 本论文提出了一种计算高效的数据驱动方法，使用Koopman算子理论来发现和线性表示非线性系统，并应用于控制。与自编码器基准方法相比，我们的方法在训练效率和准确性上都更优秀。 |
| [^31] | [Inferring physical laws by artificial intelligence based causal models.](http://arxiv.org/abs/2309.04069) | 本文提出了一种基于因果学习的物理原理模型，不仅能识别相关性，还能展现因果关系，从而为机器辅助的科学发现提供了新的途径。 |
| [^32] | [3D Denoisers are Good 2D Teachers: Molecular Pretraining via Denoising and Cross-Modal Distillation.](http://arxiv.org/abs/2309.04062) | 本论文提出了一种自监督的分子表示学习框架D&D，通过将3D去噪器的表示蒸馏到2D图形编码器中进行预训练，以解决从大规模无标签数据中预训练分子表示的问题。 |
| [^33] | [Evaluation of large language models for discovery of gene set function.](http://arxiv.org/abs/2309.04019) | 本研究评估了OpenAI的GPT-4大型语言模型在发现基因集合功能方面的能力，并发现它能根据嵌入的生物医学知识生成与Gene Ontology中具名基因集合非常相似的名称，同时在基因组学数据中发现的基因集合中，GPT-4的命名更具信息量，得到了人工审核的基本验证。 |
| [^34] | [ConDA: Contrastive Domain Adaptation for AI-generated Text Detection.](http://arxiv.org/abs/2309.03992) | 创新点：提出了一种基于对比域适应的框架 ConDA，用于检测由大型语言模型生成的新闻文本。这种方法解决了获取标记训练数据的困难，通过利用未标记的目标数据进行无监督域适应。 |
| [^35] | [Noisy Computing of the $\mathsf{OR}$ and $\mathsf{MAX}$ Functions.](http://arxiv.org/abs/2309.03986) | 本研究考虑使用噪声查询计算$\mathsf{OR}$和$\mathsf{MAX}$函数，结果表明，在错误概率收敛时，计算这两个函数所需的期望查询数量与Kullback-Leibler差异之间有密切关系。 |
| [^36] | [Large-Scale Automatic Audiobook Creation.](http://arxiv.org/abs/2309.03926) | 这项工作提出了一个可以自动生成高质量有声读物的系统，通过利用神经文本到语音技术，从在线电子书中创建数千本开放许可的有声读物。该系统允许用户自定义朗读速度和风格，并且提供了匹配所需声音的功能。该工作贡献了5000多本有声读物和一个交互式演示。 |
| [^37] | [Automatic Algorithm Selection for Pseudo-Boolean Optimization with Given Computational Time Limits.](http://arxiv.org/abs/2309.03924) | 提出一种利用机器学习技术自动选择最佳求解器的方法，该方法适用于解决各种问题，并且在考虑了计算时间限制的情况下依然能够预测最佳求解器。 |
| [^38] | [A recommender for the management of chronic pain in patients undergoing spinal cord stimulation.](http://arxiv.org/abs/2309.03918) | 本文提出了一种推荐系统，用于管理进行SCS的慢性疼痛患者。通过使用上下文多臂赌博方法开发的系统，为患者推荐SCS设置，旨在改善其状况。这些推荐通过数字健康生态系统直接发送给患者，并与患者监测系统结合，为慢性疼痛患者的整个治疗过程提供闭环关怀。 |
| [^39] | [The Role of Communication and Reference Songs in the Mixing Process: Insights from Professional Mix Engineers.](http://arxiv.org/abs/2309.03404) | 本论文研究了专业混音师与客户的互动以及他们如何利用客户的反馈指导混音过程。结果表明，混音师通过使用参考曲目和合作沟通等方式，与客户建立起对混音期望音效的默契约定。 |
| [^40] | [BEVTrack: A Simple Baseline for 3D Single Object Tracking in Birds's-Eye-View.](http://arxiv.org/abs/2309.02185) | 本文介绍了一种名为BEVTrack的简单却强大的基线框架，用于解决点云中3D单物体跟踪的挑战。通过将点云转换为鸟瞰图表示，并进行简单的逐元素操作，BEVTrack能够编码空间邻近性和捕捉运动线索，从而实现高效的跟踪。 |
| [^41] | [TensorBank:Tensor Lakehouse for Foundation Model Training.](http://arxiv.org/abs/2309.02094) | TensorBank是一个基于Tensor的湖仓库，能够以高速从云对象存储流式传输张量到GPU内存，并通过使用分层统计指标进行查询加速。 |
| [^42] | [On the Robustness of Post-hoc GNN Explainers to Label Noise.](http://arxiv.org/abs/2309.01706) | 本文对后续GNN解释器在面对标签噪声时的鲁棒性进行了研究，并发现即使是轻微的标签噪声也会严重影响解释质量。 |
| [^43] | [A Survey on Privacy in Graph Neural Networks: Attacks, Preservation, and Applications.](http://arxiv.org/abs/2308.16375) | 这篇综述调查了图神经网络中的隐私问题，包括攻击、保护方法以及应用领域。研究人员着重总结了攻击类型、隐私保护技术分类以及可用于分析和解决GNNs中隐私问题的数据集和应用，同时提出了未来研究的方向，以构建更好的隐私保护GNNs。 |
| [^44] | [When Do Program-of-Thoughts Work for Reasoning?.](http://arxiv.org/abs/2308.15452) | 提出了复杂性影响推理分数（CIRS）来衡量编程语言对推理能力的影响，发现并非所有复杂性的代码数据都可以被学习或理解，适当的复杂性水平对于改善推理能力至关重要。 |
| [^45] | [A Generic Machine Learning Framework for Fully-Unsupervised Anomaly Detection with Contaminated Data.](http://arxiv.org/abs/2308.13352) | 这篇论文介绍了一个完全无监督的机器学习框架，用于处理在训练数据中含有异常样本的异常检测任务。 |
| [^46] | [CyberForce: A Federated Reinforcement Learning Framework for Malware Mitigation.](http://arxiv.org/abs/2308.05978) | CyberForce是一个联邦强化学习框架，用于在物联网设备中协同私密地确定适合缓解各种零日攻击的MTD技术。它整合了设备指纹识别和异常检测，并通过奖励或惩罚FRL agent选择的MTD机制来提高网络安全性。 |
| [^47] | [Developmental Bootstrapping of AIs.](http://arxiv.org/abs/2308.04586) | 传统的符号AI方法和深度学习AI方法无法满足创建强大和可信赖的AI的挑战，然而，发展脱靴法通过模仿人类儿童的能力发展过程，为创建稳健可靠的AI提供了希望。 |
| [^48] | [Accurate Neural Network Pruning Requires Rethinking Sparse Optimization.](http://arxiv.org/abs/2308.02060) | 这项工作研究了高稀疏对神经网络训练的影响，发现使用传统的密集训练策略进行稀疏训练效果不佳，提出了新的方法来解决这个问题，并在视觉和语言模型上都取得了最先进的结果。 |
| [^49] | [Heterogeneous Federated Learning: State-of-the-art and Research Challenges.](http://arxiv.org/abs/2307.10616) | 异构联邦学习是联邦学习领域中的一个重要研究方向，涉及到数据分布、模型架构、网络环境和硬件设备的异质性挑战。本文对异构联邦学习的研究挑战和最新进展进行了综述和分类，为进一步的研究提供了参考。 |
| [^50] | [EENED: End-to-End Neural Epilepsy Detection based on Convolutional Transformer.](http://arxiv.org/abs/2305.10502) | 本文提出了一种名为EENED的端到端神经元癫痫检测模型，结合了CNN和Transformer，能够同时捕捉EEG信号的全局依赖性和局部特征，并有望提高癫痫检测的准确性和可靠性。 |
| [^51] | [Adoption of AI Technology in the Music Mixing Workflow: An Investigation.](http://arxiv.org/abs/2304.03407) | 该研究调查了音乐混音工作流中AI技术的应用情况及不同用户群体的采用情况。结果显示，即使AI混音工具能够为业余爱好者提供不错的结果，职业业余爱好者和专业人员需要更为精确的控制和自定义选项，以及辅助和协作技术。 |
| [^52] | [NeTO:Neural Reconstruction of Transparent Objects with Self-Occlusion Aware Refraction-Tracing.](http://arxiv.org/abs/2303.11219) | 提出了一种名为NeTO的方法，通过体渲染从2D图像中捕捉固体透明物体的3D几何体。通过采用隐式有符号距离函数（SDF）作为表面表示，并通过自遮挡感知的折射追踪通过体渲染来优化SDF场，可以实现高质量的重建结果。 |
| [^53] | [Constrained Reinforcement Learning using Distributional Representation for Trustworthy Quadrotor UAV Tracking Control.](http://arxiv.org/abs/2302.11694) | 提出了一种使用分布式表示进行受限强化学习的方法，用于可信四旋翼无人机的跟踪控制。通过集成分布式强化学习干扰估计器和随机模型预测控制器，能够准确识别气动效应的不确定性，实现最优的全局收敛速率和一定的亚线性收敛速率。 |
| [^54] | [Fairguard: Harness Logic-based Fairness Rules in Smart Cities.](http://arxiv.org/abs/2302.11137) | 本文提出了一种基于时间逻辑的公正智慧城市政策调整和生成方法Fairguard，通过两个阶段的静态生成和动态调节，缓解由多种数据和算法偏见导致的不公正预测结果。 |
| [^55] | [TikTalk: A Video-Based Dialogue Dataset for Multi-Modal Chitchat in Real World.](http://arxiv.org/abs/2301.05880) | TikTalk是一个基于视频的多模态对话数据集，用于研究智能且类似人类的闲聊机器人。数据集包含从流行视频分享平台收集的38K个视频和367K个用户对话。与其他数据集相比，TikTalk提供了更丰富的上下文类型，同时也增加了从复杂的多模态信息中生成个性化回答的难度。数据集中还更频繁地引用了外部知识，为多模态对话模型提供了新的挑战。 |
| [^56] | [Assessing Quality-Diversity Neuro-Evolution Algorithms Performance in Hard Exploration Problems.](http://arxiv.org/abs/2211.13742) | 本文评估了质量多样性神经进化算法在困难探索问题中的性能，并强调了探索在解决控制问题中的重要性。 |
| [^57] | [Leveraging the Potential of Novel Data in Power Line Communication of Electricity Grids.](http://arxiv.org/abs/2209.12693) | 本文提出了两个基于宽带电力线通信测量的首个数据集FiN-1和FiN-2，共收集了130亿个数据点。这些数据可以应用于资产管理、电网状态可视化和预测，以解决电力网在向可再生能源过渡和复杂负载配置的情况下面临的新挑战。 |
| [^58] | [Neuromorphic Visual Scene Understanding with Resonator Networks.](http://arxiv.org/abs/2208.12880) | 本论文提出了一种基于神经形态的解决方案，利用高效的因式分解网络来理解视觉场景并推断物体和姿势。关键创新包括基于复值向量的计算框架VSA、用于处理平移和旋转的分层谐振器网络HRN设计，以及在神经形态硬件上实现复值谐振器网络的多组分脉冲相位神经元模型。 |
| [^59] | [TREE-G: Decision Trees Contesting Graph Neural Networks.](http://arxiv.org/abs/2207.02760) | TREE-G引入了一种专门针对图数据的修改决策树方法，通过新颖的分裂函数和指针机制，使得决策树能够更好地应用于带有拓扑信息的图结构化数据。 |
| [^60] | [NMA: Neural Multi-slot Auctions with Externalities for Online Advertising.](http://arxiv.org/abs/2205.10018) | NMA是一种新颖的在线广告拍卖机制，利用深度神经网络模型考虑广告展示顺序和位置对用户点击的影响，更好地模拟全局的外部性。 |

# 详细

[^1]: 关于结果预测的可行性

    On the Actionability of Outcome Prediction. (arXiv:2309.04470v1 [cs.LG])

    [http://arxiv.org/abs/2309.04470](http://arxiv.org/abs/2309.04470)

    提高结果预测准确性有助于确定最适合的干预措施，但在存在多种可能干预的情况下，仅仅依赖结果预测进行干预可能无法达到预期效果。

    

    在社会影响领域，预测未来结果是机器学习的常见应用。例子从预测教育中学生的成功到预测医疗保健中的疾病风险。从实践者来看，最终目标不仅仅是预测，而是有效地行动。越来越多的证据表明，仅仅依靠结果预测进行下游干预可能不会产生预期结果。在大多数领域中，每个个体存在多种可能的干预措施，这使得采取有效行动的挑战更加严峻。即使连接个体潜在状态与结果的因果机制已经很好地理解，在任何给定的实例（特定的学生或患者）中，实践者仍然需要从预算测量的潜在状态中推断出哪种可能的干预措施对这个个体最有效。考虑到这一点，我们想问：准确的结果预测何时有助于识别最合适的干预措施？

    Predicting future outcomes is a prevalent application of machine learning in social impact domains. Examples range from predicting student success in education to predicting disease risk in healthcare. Practitioners recognize that the ultimate goal is not just to predict but to act effectively. Increasing evidence suggests that relying on outcome predictions for downstream interventions may not have desired results.  In most domains there exists a multitude of possible interventions for each individual, making the challenge of taking effective action more acute. Even when causal mechanisms connecting the individual's latent states to outcomes is well understood, in any given instance (a specific student or patient), practitioners still need to infer -- from budgeted measurements of latent states -- which of many possible interventions will be most effective for this individual. With this in mind, we ask: when are accurate predictors of outcomes helpful for identifying the most suitable
    
[^2]: 子词作为技巧：稀疏奖励强化学习的分词化

    Subwords as Skills: Tokenization for Sparse-Reward Reinforcement Learning. (arXiv:2309.04459v1 [cs.LG])

    [http://arxiv.org/abs/2309.04459](http://arxiv.org/abs/2309.04459)

    通过将行动空间离散化并采用分词技术，我们提出了一种在稀疏奖励强化学习中生成技巧的新方法。这种方法能够减少探索的难度，并在连续行动空间中达到良好的性能。

    

    稀疏奖励强化学习中的探索具有困难，因为需要通过长期的、协调的行动序列才能获得任何奖励。而且，在连续的行动空间中，可能的行动数量是无穷多的，这只会增加探索的难度。为了解决这些问题，一类方法通过在同一领域收集的交互数据中形成时间上延伸的行动，通常称为技巧，并在这个新的行动空间上进行策略的优化。通常这样的方法在连续行动空间中需要一个漫长的预训练阶段，在强化学习开始之前形成技巧。鉴于先前的证据表明在这些任务中并不需要完整的连续行动空间，我们提出了一种新颖的技巧生成方法，包括两个组成部分。首先，我们通过聚类将行动空间离散化，然后我们利用从自然语言处理借鉴来的分词技术。

    Exploration in sparse-reward reinforcement learning is difficult due to the requirement of long, coordinated sequences of actions in order to achieve any reward. Moreover, in continuous action spaces there are an infinite number of possible actions, which only increases the difficulty of exploration. One class of methods designed to address these issues forms temporally extended actions, often called skills, from interaction data collected in the same domain, and optimizes a policy on top of this new action space. Typically such methods require a lengthy pretraining phase, especially in continuous action spaces, in order to form the skills before reinforcement learning can begin. Given prior evidence that the full range of the continuous action space is not required in such tasks, we propose a novel approach to skill-generation with two components. First we discretize the action space through clustering, and second we leverage a tokenization technique borrowed from natural language pro
    
[^3]: 正规化流的变化和放松

    Variations and Relaxations of Normalizing Flows. (arXiv:2309.04433v1 [cs.LG])

    [http://arxiv.org/abs/2309.04433](http://arxiv.org/abs/2309.04433)

    正规化流是一种模型，采用一系列双射变换将复杂目标分布表示为简单基础分布。然而，由于对微分同胚的限制，其在有效表示复杂拓扑目标分布和处理先验分布与目标分布不同胚的情况方面存在局限性。相关研究通过结合其他生成模型的特点，放宽了正规化流的约束性，以平衡约束和灵活性之间的关系

    

    正规化流（NFs）描述了一类将复杂目标分布表示为简单基础分布的一系列双射变换的模型。通过将候选变换空间限制为微分同胚，NFs可以高效地进行精确采样和密度评估，使其既能灵活地作为判别模型，又能作为生成模型。然而，它们对微分同胚的限制强制输入、输出和所有中间空间具有相同的维数，限制了它们有效表示具有复杂拓扑的目标分布的能力。此外，在先验分布和目标分布不同胚的情况下，正规化流可能会将质量泄漏到目标支持之外。本综述涵盖了一系列最近的工作，将其他生成模型类别（如VAEs和基于得分的扩散）的方面结合起来，从而放宽了NFs的严格双射约束，以实现平衡约束与灵活性之间的折中

    Normalizing Flows (NFs) describe a class of models that express a complex target distribution as the composition of a series of bijective transformations over a simpler base distribution. By limiting the space of candidate transformations to diffeomorphisms, NFs enjoy efficient, exact sampling and density evaluation, enabling NFs to flexibly behave as both discriminative and generative models. Their restriction to diffeomorphisms, however, enforces that input, output and all intermediary spaces share the same dimension, limiting their ability to effectively represent target distributions with complex topologies. Additionally, in cases where the prior and target distributions are not homeomorphic, Normalizing Flows can leak mass outside of the support of the target. This survey covers a selection of recent works that combine aspects of other generative model classes, such as VAEs and score-based diffusion, and in doing so loosen the strict bijectivity constraints of NFs to achieve a bal
    
[^4]: 创造你的世界：终身文本到图像扩散

    Create Your World: Lifelong Text-to-Image Diffusion. (arXiv:2309.04430v1 [cs.CV])

    [http://arxiv.org/abs/2309.04430](http://arxiv.org/abs/2309.04430)

    本文介绍了一种终身文本到图像扩散模型（L2DM），该模型通过解决知识“灾难性遗忘”和文本提示中的语义“灾难性忽视”问题，能够在用户输入的文本提示下生成多样且高质量的图像。

    

    文本到图像的生成模型可以通过文本提示产生多样且高质量的概念图像，在图像生成、图像翻译等领域展示出优秀的能力。本研究致力于解决以一种永无止境的方式合成用户自己概念的实例化问题，即创建你的世界，新的用户概念能够通过几个示例快速学习。为了实现这个目标，我们提出了一种终身文本到图像扩散模型（L2DM），旨在解决过去遇到的概念的知识“灾难性遗忘”和文本提示中一个或多个概念的语义“灾难性忽视”问题。在知识“灾难性遗忘”方面，我们的L2DM框架设计了一个任务感知的记忆增强模块和一个弹性概念蒸馏模块，分别保护了先前概念的知识和每个过去个性化概念的知识。在使用用户的文本提示生成图像时，解决方案是

    Text-to-image generative models can produce diverse high-quality images of concepts with a text prompt, which have demonstrated excellent ability in image generation, image translation, etc. We in this work study the problem of synthesizing instantiations of a use's own concepts in a never-ending manner, i.e., create your world, where the new concepts from user are quickly learned with a few examples. To achieve this goal, we propose a Lifelong text-to-image Diffusion Model (L2DM), which intends to overcome knowledge "catastrophic forgetting" for the past encountered concepts, and semantic "catastrophic neglecting" for one or more concepts in the text prompt. In respect of knowledge "catastrophic forgetting", our L2DM framework devises a task-aware memory enhancement module and a elastic-concept distillation module, which could respectively safeguard the knowledge of both prior concepts and each past personalized concept. When generating images with a user text prompt, the solution to 
    
[^5]: 基于脑启发的脉冲神经网络的先进计算和相关应用

    Advanced Computing and Related Applications Leveraging Brain-inspired Spiking Neural Networks. (arXiv:2309.04426v1 [cs.NE])

    [http://arxiv.org/abs/2309.04426](http://arxiv.org/abs/2309.04426)

    本文总结了基于脑启发的脉冲神经网络在计算速度、实时信息处理和时空信息处理等方面的优势，分析了神经元模型和网络拓扑的特点，并回顾了脉冲神经网络算法和无监督学习算法以及有监督学习算法的应用。

    

    在下一代脑启发式人工智能和越来越复杂的电磁环境的快速演化中，脉冲神经网络具有仿生特征和抗干扰性能，在计算速度、实时信息处理和时空信息处理方面显示出巨大潜力。脉冲神经网络是类脑人工智能的核心之一，通过模拟生物神经网络的结构和信息传递模式实现类脑计算。本文总结了五种神经元模型的优点、缺点和适用性，并分析了五种网络拓扑的特点；然后对脉冲神经网络算法进行了回顾，并从无监督学习和有监督学习的角度总结了基于突触可塑性规则的无监督学习算法和四种类型的有监督学习算法。

    In the rapid evolution of next-generation brain-inspired artificial intelligence and increasingly sophisticated electromagnetic environment, the most bionic characteristics and anti-interference performance of spiking neural networks show great potential in terms of computational speed, real-time information processing, and spatio-temporal information processing. Data processing. Spiking neural network is one of the cores of brain-like artificial intelligence, which realizes brain-like computing by simulating the structure and information transfer mode of biological neural networks. This paper summarizes the strengths, weaknesses and applicability of five neuronal models and analyzes the characteristics of five network topologies; then reviews the spiking neural network algorithms and summarizes the unsupervised learning algorithms based on synaptic plasticity rules and four types of supervised learning algorithms from the perspectives of unsupervised learning and supervised learning; 
    
[^6]: SynthoGestures：一种用于驾驶场景的合成动态手势生成的新框架

    SynthoGestures: A Novel Framework for Synthetic Dynamic Hand Gesture Generation for Driving Scenarios. (arXiv:2309.04421v1 [cs.CV])

    [http://arxiv.org/abs/2309.04421](http://arxiv.org/abs/2309.04421)

    SynthoGestures是一种使用虚幻引擎合成逼真手势的新框架，可以用于驾驶场景下的动态人机界面。该框架通过生成多种变体和模拟不同摄像机类型，提高了手势识别的准确性并节省了数据集创建的时间和精力。

    

    在汽车领域中，为动态人机界面创建多样化和全面的手势数据集可能具有挑战性且耗时。为了克服这一挑战，我们提出使用虚拟3D模型生成合成手势数据集。我们的框架利用虚幻引擎合成逼真的手势，提供定制选项并降低过拟合风险。生成多种变体，包括手势速度、性能和手形，以提高泛化能力。此外，我们模拟不同的摄像机位置和类型，如RGB、红外和深度摄像机，而无需额外的时间和费用获取这些摄像机。实验结果表明，我们的提议框架SynthoGestures提高了手势识别准确率，可以替代或增强真手数据集。通过节省数据集创建的时间和精力，我们的工具促进了研究的进展。

    Creating a diverse and comprehensive dataset of hand gestures for dynamic human-machine interfaces in the automotive domain can be challenging and time-consuming. To overcome this challenge, we propose using synthetic gesture datasets generated by virtual 3D models. Our framework utilizes Unreal Engine to synthesize realistic hand gestures, offering customization options and reducing the risk of overfitting. Multiple variants, including gesture speed, performance, and hand shape, are generated to improve generalizability. In addition, we simulate different camera locations and types, such as RGB, infrared, and depth cameras, without incurring additional time and cost to obtain these cameras. Experimental results demonstrate that our proposed framework, SynthoGestures\footnote{\url{https://github.com/amrgomaaelhady/SynthoGestures}}, improves gesture recognition accuracy and can replace or augment real-hand datasets. By saving time and effort in the creation of the data set, our tool acc
    
[^7]: 超越静态数据集：深度交互方法用于LLM评估

    Beyond Static Datasets: A Deep Interaction Approach to LLM Evaluation. (arXiv:2309.04369v1 [cs.CL])

    [http://arxiv.org/abs/2309.04369](http://arxiv.org/abs/2309.04369)

    该论文提出了一种基于深度交互的LLM评估框架，突破了静态数据集的限制，能够评估LLM在动态实际场景中的能力，并适用于多种实际任务。

    

    大型语言模型（LLM）在各种实际任务中取得了进展，这激发了对LLM评估的需求。现有的LLM评估方法主要依赖于静态数据集的监督信号，无法评估LLM在动态实际场景中的能力，而这些场景中深度交互广泛存在。其他的LLM评估方法基于人工评估，成本高且耗时，并且无法进行大规模的LLM评估。为了解决上述问题，我们提出了一种新颖的基于深度交互的LLM评估框架。在我们提出的框架中，通过精心设计的评估任务，可以评估LLM在实际领域中与其他LLM的深度交互中的性能。此外，我们提出的框架是一种通用的评估方法，可应用于诸多实际任务，如机器翻译和代码生成。通过广泛的实验证明了我们提出的方法的有效性。

    Large Language Models (LLMs) have made progress in various real-world tasks, which stimulates requirements for the evaluation of LLMs. Existing LLM evaluation methods are mainly supervised signal-based which depends on static datasets and cannot evaluate the ability of LLMs in dynamic real-world scenarios where deep interaction widely exists. Other LLM evaluation methods are human-based which are costly and time-consuming and are incapable of large-scale evaluation of LLMs. To address the issues above, we propose a novel Deep Interaction-based LLM-evaluation framework. In our proposed framework, LLMs' performances in real-world domains can be evaluated from their deep interaction with other LLMs in elaborately designed evaluation tasks. Furthermore, our proposed framework is a general evaluation method that can be applied to a host of real-world tasks such as machine translation and code generation. We demonstrate the effectiveness of our proposed method through extensive experiments o
    
[^8]: 为分类2D基于网格的关卡可完成性提出的主动学习方法

    Active Learning for Classifying 2D Grid-Based Level Completability. (arXiv:2309.04367v1 [cs.LG])

    [http://arxiv.org/abs/2309.04367](http://arxiv.org/abs/2309.04367)

    本文提出了使用主动学习方法来学习2D基于网格的关卡可完成性分类，通过对Super Mario Bros.、Kid Icarus和一个类似Zelda的游戏生成的关卡进行标记，获得了更好的分类器性能。

    

    确定由程序生成器生成的关卡的可完成性是一项具有挑战性的任务，因为这可能涉及使用求解器代理来分析和解决关卡，而这通常需要大量的时间。尽管主动学习已经成功地应用于自然语言处理、图像和语音识别以及计算机视觉领域，但在游戏评估中，它还没有被广泛采用，尤其是在标记数据有限或昂贵的情况下。在本文中，我们提出了使用主动学习来学习关卡可完成性分类。通过主动学习的方法，我们训练深度学习模型来对Super Mario Bros.、Kid Icarus和一个类似Zelda的游戏中生成的关卡进行可完成性分类。我们将使用主动学习方法标记关卡与使用随机查询标记关卡进行了比较。我们的结果表明，使用主动学习方法来标记关卡可以获得更好的分类器性能，而查询方法相同。

    Determining the completability of levels generated by procedural generators such as machine learning models can be challenging, as it can involve the use of solver agents that often require a significant amount of time to analyze and solve levels. Active learning is not yet widely adopted in game evaluations, although it has been used successfully in natural language processing, image and speech recognition, and computer vision, where the availability of labeled data is limited or expensive. In this paper, we propose the use of active learning for learning level completability classification. Through an active learning approach, we train deep-learning models to classify the completability of generated levels for Super Mario Bros., Kid Icarus, and a Zelda-like game. We compare active learning for querying levels to label with completability against random queries. Our results show using an active learning approach to label levels results in better classifier performance with the same am
    
[^9]: 使用基础模型对零样本模型进行零样本强化

    Zero-Shot Robustification of Zero-Shot Models With Foundation Models. (arXiv:2309.04344v1 [cs.LG])

    [http://arxiv.org/abs/2309.04344](http://arxiv.org/abs/2309.04344)

    提出了一种零样本强化方法RoboShot，通过使用零样本语言模型从任务描述中获取有用的见解，并应用于预训练模型嵌入中以去除有害成分并增强有用成分，从而改善预训练模型的鲁棒性。

    

    零样本推断是一种强大的范式，可以在没有进一步训练的情况下使用预训练模型来进行下游分类任务。然而，这些模型容易受到继承的偏见的影响，从而影响它们的性能。传统的解决方案是微调，但这削弱了预训练模型的主要优势，即可以直接使用的能力。我们提出了RoboShot，一种完全零样本的方法，可以改善预训练模型嵌入的鲁棒性。首先，我们使用零样本语言模型（LMs）从任务描述中获取有用的见解。这些见解被嵌入并用于去除嵌入中的有害成分并增强有用成分--而无需任何监督。从理论上讲，我们提供了一个简单且可计算的模型，用于分析零样本嵌入中的偏见，并给出了在什么条件下我们的方法可以提高性能的结果。在实证上，我们在九个图像和NLP分类任务上评估了RoboShot。

    Zero-shot inference is a powerful paradigm that enables the use of large pretrained models for downstream classification tasks without further training. However, these models are vulnerable to inherited biases that can impact their performance. The traditional solution is fine-tuning, but this undermines the key advantage of pretrained models, which is their ability to be used out-of-the-box. We propose RoboShot, a method that improves the robustness of pretrained model embeddings in a fully zero-shot fashion. First, we use zero-shot language models (LMs) to obtain useful insights from task descriptions. These insights are embedded and used to remove harmful and boost useful components in embeddings -- without any supervision. Theoretically, we provide a simple and tractable model for biases in zero-shot embeddings and give a result characterizing under what conditions our approach can boost performance. Empirically, we evaluate RoboShot on nine image and NLP classification tasks and s
    
[^10]: 通过在线凸优化实现在线子模最大化

    Online Submodular Maximization via Online Convex Optimization. (arXiv:2309.04339v1 [cs.LG])

    [http://arxiv.org/abs/2309.04339](http://arxiv.org/abs/2309.04339)

    本论文研究了在线设置下的一般性子模最大化问题，并将一类大型子模函数归约到在线凸优化问题中。这种归约方式可在组合优化中实现次线性遗憾，并且适用于许多不同版本的在线学习问题。

    

    我们研究了在线设置下的一般性子模最大化问题在一般性模性约束下。我们证明了在线优化一类大型子模函数，即加权阈值势函数，可以归约到在线凸优化(OCO)问题。这是因为这个类别的函数可以进行凹松弛;因此，结合适当的舍入方案，OCO策略可以在组合设置中实现次线性遗憾。我们还展示了我们的简化方式可以应用在许多不同版本的在线学习问题中，包括动态遗憾、强盗和乐观学习等设置。

    We study monotone submodular maximization under general matroid constraints in the online setting. We prove that online optimization of a large class of submodular functions, namely, weighted threshold potential functions, reduces to online convex optimization (OCO). This is precisely because functions in this class admit a concave relaxation; as a result, OCO policies, coupled with an appropriate rounding scheme, can be used to achieve sublinear regret in the combinatorial setting. We show that our reduction extends to many different versions of the online learning problem, including the dynamic regret, bandit, and optimistic-learning settings.
    
[^11]: 图神经网络在不需要的时候仍然使用图形信息

    Graph Neural Networks Use Graphs When They Shouldn't. (arXiv:2309.04332v1 [cs.LG])

    [http://arxiv.org/abs/2309.04332](http://arxiv.org/abs/2309.04332)

    在图形预测问题中，GNNs倾向于过拟合图结构，即使在忽略图结构的情况下可以获得更好的解决方案。常规图对于这种过拟合更具鲁棒性。

    

    在各个领域中，包括社交网络、分子生物学、医学等，对图形进行预测起着至关重要的作用。图神经网络(GNNs)已成为学习图数据的主要方法。图形标注问题的实例包括图结构(即邻接矩阵)和节点特定的特征向量。在某些情况下，这种图结构对于预测任务来说并不具有信息量。例如，分子性质如摩尔质量仅依赖于组成原子(节点特征)，而与分子结构无关。尽管GNNs有能力在这种情况下忽略图结构，但不清楚它们是否会这样做。在这项工作中，我们展示了GNNs实际上倾向于在过拟合图结构，即在忽略它可以获得更好解决方案的情况下仍在使用。我们根据不同的图分布来研究这种现象，发现常规图对这种过拟合更加稳健。

    Predictions over graphs play a crucial role in various domains, including social networks, molecular biology, medicine, and more. Graph Neural Networks (GNNs) have emerged as the dominant approach for learning on graph data. Instances of graph labeling problems consist of the graph-structure (i.e., the adjacency matrix), along with node-specific feature vectors. In some cases, this graph-structure is non-informative for the predictive task. For instance, molecular properties such as molar mass depend solely on the constituent atoms (node features), and not on the molecular structure. While GNNs have the ability to ignore the graph-structure in such cases, it is not clear that they will. In this work, we show that GNNs actually tend to overfit the graph-structure in the sense that they use it even when a better solution can be obtained by ignoring it. We examine this phenomenon with respect to different graph distributions and find that regular graphs are more robust to this overfitting
    
[^12]: 从自然互动和大型语言模型中增量学习人形机器人行为

    Incremental Learning of Humanoid Robot Behavior from Natural Interaction and Large Language Models. (arXiv:2309.04316v1 [cs.RO])

    [http://arxiv.org/abs/2309.04316](http://arxiv.org/abs/2309.04316)

    本文提出了一个从自然互动中实现复杂行为增量学习的系统，并演示了在一个人形机器人上的应用。该系统利用大型语言模型对机器人的行为进行高级协调，通过交互式控制台生成Python语句来调用机器人的感知和动作，并通过将人类指令、环境观测和执行结果反馈给语言模型来实现循环交互。

    

    自然语言对话对于直观的人机交互至关重要。它不仅可以用来表达人类的意图，而且还可以用来传达指令以改进机器人对命令的理解。非常重要的是要赋予机器人从这种交互经验中增量学习的能力，以使它们能够改进自己的行为或避免未来的错误。本文提出了一个从自然互动中实现复杂行为增量学习的系统，并在一个人形机器人上演示其实现。基于最新的进展，我们提出了一个使用大型语言模型（Large Language Models，LLMs）进行机器人行为的高层协调的系统，这个系统的思想是让LLM在交互式控制台中生成Python语句来调用机器人的感知和动作。通过将人类指令、环境观测和执行结果反馈给LLM来关闭交互循环。

    Natural-language dialog is key for intuitive human-robot interaction. It can be used not only to express humans' intents, but also to communicate instructions for improvement if a robot does not understand a command correctly. Of great importance is to endow robots with the ability to learn from such interaction experience in an incremental way to allow them to improve their behaviors or avoid mistakes in the future. In this paper, we propose a system to achieve incremental learning of complex behavior from natural interaction, and demonstrate its implementation on a humanoid robot. Building on recent advances, we present a system that deploys Large Language Models (LLMs) for high-level orchestration of the robot's behavior, based on the idea of enabling the LLM to generate Python statements in an interactive console to invoke both robot perception and action. The interaction loop is closed by feeding back human instructions, environment observations, and execution results to the LLM, 
    
[^13]: 基于联邦学习的健康老龄应用早期退学预测研究

    Federated Learning for Early Dropout Prediction on Healthy Ageing Applications. (arXiv:2309.04311v1 [cs.LG])

    [http://arxiv.org/abs/2309.04311](http://arxiv.org/abs/2309.04311)

    本文提出了一种基于联邦学习的方法，用于预测健康老龄应用中的早期退学情况。该方法通过联合个体和组织进行训练，并实现了隐私保护和分布式学习。

    

    社会护理应用的提供对于改善老年人的生活质量和为运营商提供早期干预是至关重要的。在健康老龄应用中准确预测用户退学是必要的，因为它们直接与个体健康状况相关。机器学习算法已经实现了高度准确的预测，超越了传统统计方法，在处理个体模式时非常困难。然而，机器学习需要大量的数据进行训练，这在存在个人可识别信息(PII)和受到法规碎片化的情况下具有挑战性。本文提出了一种联邦机器学习(FML)方法，该方法最大程度地减少了隐私问题，并实现了分布式训练，而不需要传输个体数据。我们通过考虑FML下的个体和组织来进行协作训练，这样可以对跨设备和跨平台学习场景进行建模。

    The provision of social care applications is crucial for elderly people to improve their quality of life and enables operators to provide early interventions. Accurate predictions of user dropouts in healthy ageing applications are essential since they are directly related to individual health statuses. Machine Learning (ML) algorithms have enabled highly accurate predictions, outperforming traditional statistical methods that struggle to cope with individual patterns. However, ML requires a substantial amount of data for training, which is challenging due to the presence of personal identifiable information (PII) and the fragmentation posed by regulations. In this paper, we present a federated machine learning (FML) approach that minimizes privacy concerns and enables distributed training, without transferring individual data. We employ collaborative training by considering individuals and organizations under FML, which models both cross-device and cross-silo learning scenarios. Our a
    
[^14]: 在COVID-19期间导航不在分布范围内的电力负荷预测：利用人类移动的持续学习方法

    Navigating Out-of-Distribution Electricity Load Forecasting during COVID-19: A Continual Learning Approach Leveraging Human Mobility. (arXiv:2309.04296v1 [cs.LG])

    [http://arxiv.org/abs/2309.04296](http://arxiv.org/abs/2309.04296)

    本研究提出了一种利用人类移动数据和持续学习技术的方法来解决COVID-19期间非分布期间的电力负荷预测问题，通过保留过去的见解并整合新的数据，提高了模型的准确性和鲁棒性。

    

    在传统的深度学习算法中，一个关键假设是数据分布在训练和部署过程中保持不变。然而，在面对非分布期间时，如COVID-19的封锁期，数据分布与模型在训练过程中所见的明显偏离。本文采用双重策略：利用持续学习技术更新模型的新数据，并利用在建筑物外部的保护隐私的行人计数器收集的人类移动数据。与在线学习相比，后者常常会遭受“灾难性遗忘”的困扰，因为新获得的知识常常会抹去先前的信息，持续学习则通过保留过去的见解并整合新的数据，提供了一个整体的方法。本研究将FSNet，一种强大的持续学习算法，应用于墨尔本市13个建筑群的真实数据。

    In traditional deep learning algorithms, one of the key assumptions is that the data distribution remains constant during both training and deployment. However, this assumption becomes problematic when faced with Out-of-Distribution periods, such as the COVID-19 lockdowns, where the data distribution significantly deviates from what the model has seen during training. This paper employs a two-fold strategy: utilizing continual learning techniques to update models with new data and harnessing human mobility data collected from privacy-preserving pedestrian counters located outside buildings. In contrast to online learning, which suffers from 'catastrophic forgetting' as newly acquired knowledge often erases prior information, continual learning offers a holistic approach by preserving past insights while integrating new data. This research applies FSNet, a powerful continual learning algorithm, to real-world data from 13 building complexes in Melbourne, Australia, a city which had the s
    
[^15]: FIMO: 一个用于自动定理证明的挑战形式化数据集

    FIMO: A Challenge Formal Dataset for Automated Theorem Proving. (arXiv:2309.04295v1 [cs.AI])

    [http://arxiv.org/abs/2309.04295](http://arxiv.org/abs/2309.04295)

    FIMO是一个创新的数据集，用于IMO水平的自动定理证明，包含149个形式化数学问题陈述，目前仍存在挑战和限制。

    

    我们提出了FIMO，一个创新的数据集，包括从国际数学奥林匹克竞赛（IMO）的入围问题中获得的形式化数学问题陈述。FIMO旨在促进IMO级别的高级自动定理证明，目前专为Lean形式语言设计。它包括149个形式化问题陈述，同时附带非正式的问题描述和相应的基于LaTeX的非正式证明。通过使用GPT-4进行初步实验，我们的研究结果强调了当前方法的局限性，表明在实现令人满意的IMO级别自动定理证明结果之前还有很长的路要走。

    We present FIMO, an innovative dataset comprising formal mathematical problem statements sourced from the International Mathematical Olympiad (IMO) Shortlisted Problems. Designed to facilitate advanced automated theorem proving at the IMO level, FIMO is currently tailored for the Lean formal language. It comprises 149 formal problem statements, accompanied by both informal problem descriptions and their corresponding LaTeX-based informal proofs. Through initial experiments involving GPT-4, our findings underscore the existing limitations in current methodologies, indicating a substantial journey ahead before achieving satisfactory IMO-level automated theorem proving outcomes.
    
[^16]: 模糊指纹转换器语言模型用于对话情感识别

    Fuzzy Fingerprinting Transformer Language-Models for Emotion Recognition in Conversations. (arXiv:2309.04292v1 [cs.CL])

    [http://arxiv.org/abs/2309.04292](http://arxiv.org/abs/2309.04292)

    本文提出将模糊指纹和大型语言模型相结合，用于对话情感识别，以获得更简单和更可解释的分类器，并在DailyDialog ERC基准数据集上取得最先进的结果。

    

    模糊指纹已成功用作可解释的文本分类技术，但与大多数其他技术一样，在性能上已被大型预训练语言模型（例如BERT或RoBERTa）大大超越。这些模型在几个自然语言处理任务中取得了最先进的结果，即对话情感识别（ERC），但缺乏可解释性和说明性。在本文中，我们提出将这两种方法结合起来进行ERC，以获得更简单和更可解释的基于大型语言模型的分类器。我们提出将话语及其之前的对话转化为预训练的RoBERTa，并获取上下文嵌入的话语表示，然后将其提供给适应的模糊指纹分类模块。我们在广泛使用的DailyDialog ERC基准数据集上验证了我们的方法，结果显示我们使用了更轻量级的模型获得了最先进的水平。

    Fuzzy Fingerprints have been successfully used as an interpretable text classification technique, but, like most other techniques, have been largely surpassed in performance by Large Pre-trained Language Models, such as BERT or RoBERTa. These models deliver state-of-the-art results in several Natural Language Processing tasks, namely Emotion Recognition in Conversations (ERC), but suffer from the lack of interpretability and explainability. In this paper, we propose to combine the two approaches to perform ERC, as a means to obtain simpler and more interpretable Large Language Models-based classifiers. We propose to feed the utterances and their previous conversational turns to a pre-trained RoBERTa, obtaining contextual embedding utterance representations, that are then supplied to an adapted Fuzzy Fingerprint classification module. We validate our approach on the widely used DailyDialog ERC benchmark dataset, in which we obtain state-of-the-art level results using a much lighter mode
    
[^17]: 逐步文本到图像生成的序列语义生成通信

    Sequential Semantic Generative Communication for Progressive Text-to-Image Generation. (arXiv:2309.04287v1 [eess.SP])

    [http://arxiv.org/abs/2309.04287](http://arxiv.org/abs/2309.04287)

    本文提出了一种通过序列语义生成通信的新框架，通过将图像转换为文本并按照信息量最大的优先级依次发送单词，实现了有效的图像传输和重建。

    

    本文提出了一种新的通信系统框架，利用多模态生成模型的有前景的生成能力。针对现今智能应用，通过传达感知意义（即文本提示）可以进行成功的通信。文本作为图像数据的合适语义表示，可以通过多模态技术进行解释、生成图像，类似于人类认知的方式。利用文本还可以减少与传输原始数据相比的负荷。发射机通过多模型生成过程将客观图像转换为文本，接收器使用反向过程重建图像。文本句子中的每个词都有各自的句法角色，负责包含文本的特定信息片段。为了进一步提高通信负载效率，发射机按承载最多信息的优先级顺序依次发送单词。

    This paper proposes new framework of communication system leveraging promising generation capabilities of multi-modal generative models. Regarding nowadays smart applications, successful communication can be made by conveying the perceptual meaning, which we set as text prompt. Text serves as a suitable semantic representation of image data as it has evolved to instruct an image or generate image through multi-modal techniques, by being interpreted in a manner similar to human cognition. Utilizing text can also reduce the overload compared to transmitting the intact data itself. The transmitter converts objective image to text through multi-model generation process and the receiver reconstructs the image using reverse process. Each word in the text sentence has each syntactic role, responsible for particular piece of information the text contains. For further efficiency in communication load, the transmitter sequentially sends words in priority of carrying the most information until re
    
[^18]: UQ在#SMM4H 2023上的论文：利用社交媒体进行公共卫生分析的ALEX方法

    UQ at #SMM4H 2023: ALEX for Public Health Analysis with Social Media. (arXiv:2309.04213v1 [cs.CL])

    [http://arxiv.org/abs/2309.04213](http://arxiv.org/abs/2309.04213)

    本文提出了一种名为ALEX的框架，通过采用LLMs解释机制来改进社交媒体上的公共卫生分析性能。该方法通过数据增强和平衡训练解决了数据不平衡问题，并有效利用了LLMs的能力。

    

    随着社交媒体的普及，与公共卫生相关的活动也越来越多。目前的公共卫生分析技术涉及到流行的模型，如BERT和大型语言模型（LLMs）。然而，为公共卫生域训练LLMs的成本尤其昂贵。此外，社交媒体中这种域内数据集往往存在不平衡的问题。为应对这些挑战，可以通过数据增强和平衡训练来解决数据不平衡的问题。此外，可以通过适当设置模型的引导方式有效利用LLMs的能力。本文提出了一种新颖的ALEX框架，通过采用LLMs解释机制来提高社交媒体上的公共卫生分析性能。结果显示，我们的ALEX模型在Social Media Mining for Health 2023 （SMM4H）的任务2和任务4中获得了最佳性能，并在任务1中得到了较高的评分[1]。我们的代码已在 https:/ /github 上发布。

    As social media becomes increasingly popular, more and more activities related to public health emerge. Current techniques for public health analysis involve popular models such as BERT and large language models (LLMs). However, the costs of training in-domain LLMs for public health are especially expensive. Furthermore, such kinds of in-domain datasets from social media are generally imbalanced. To tackle these challenges, the data imbalance issue can be overcome by data augmentation and balanced training. Moreover, the ability of the LLMs can be effectively utilized by prompting the model properly. In this paper, a novel ALEX framework is proposed to improve the performance of public health analysis on social media by adopting an LLMs explanation mechanism. Results show that our ALEX model got the best performance among all submissions in both Task 2 and Task 4 with a high score in Task 1 in Social Media Mining for Health 2023 (SMM4H)[1]. Our code has been released at https:// github
    
[^19]: 在数据集蒸馏中缓解架构过度拟合的方法

    Towards Mitigating Architecture Overfitting in Dataset Distillation. (arXiv:2309.04195v1 [cs.LG])

    [http://arxiv.org/abs/2309.04195](http://arxiv.org/abs/2309.04195)

    本文解决了数据集蒸馏中的架构过度拟合问题，提出了一系列方法来提高不同网络架构在蒸馏训练数据上的泛化性能。

    

    数据集蒸馏方法在使用极少训练数据进行神经网络训练时表现出了显著的性能。然而，一个重要的挑战是架构过度拟合：由特定网络架构（即训练网络）合成的蒸馏训练数据在其他网络架构（即测试网络）训练时表现出较差的性能。本文解决了这个问题，提出了一系列架构设计和训练方案的方法，可以共同提高不同网络架构在蒸馏训练数据上的泛化性能。我们进行了大量实验证明了我们方法的有效性和普适性。特别地，在不同大小的蒸馏数据涉及的各种场景中，我们的方法在使用容量更大的网络对蒸馏数据进行训练时实现了与现有方法相当或更好的性能。

    Dataset distillation methods have demonstrated remarkable performance for neural networks trained with very limited training data. However, a significant challenge arises in the form of architecture overfitting: the distilled training data synthesized by a specific network architecture (i.e., training network) generates poor performance when trained by other network architectures (i.e., test networks). This paper addresses this issue and proposes a series of approaches in both architecture designs and training schemes which can be adopted together to boost the generalization performance across different network architectures on the distilled training data. We conduct extensive experiments to demonstrate the effectiveness and generality of our methods. Particularly, across various scenarios involving different sizes of distilled data, our approaches achieve comparable or superior performance to existing methods when training on the distilled data using networks with larger capacities.
    
[^20]: 使用结构化医学知识库对大型语言模型进行知识调整，实现中文可靠响应生成

    Knowledge-tuning Large Language Models with Structured Medical Knowledge Bases for Reliable Response Generation in Chinese. (arXiv:2309.04175v1 [cs.CL])

    [http://arxiv.org/abs/2309.04175](http://arxiv.org/abs/2309.04175)

    本研究提出了一种使用结构化医学知识库进行知识调整的方法，以提高大型语言模型在中文可靠响应生成方面的准确性和领域适应能力。

    

    大型语言模型（LLMs）在一般领域的自然语言处理（NLP）任务中取得了显著的成功。然而，由于领域知识的限制，LLMs有时会生成具有关于医学事实的幻觉的响应。这些缺点在医学背景下使用LLMs时存在潜在的风险。为了解决这个挑战，我们提出了知识调整，通过利用结构化医学知识库来使LLMs能够高效掌握领域知识并实现可靠的响应生成。我们还发布了cMedKnowQA，一个从医学知识库构建的中文医学知识问答数据集，以评估LLMs的医学知识水平。实验结果表明，经过cMedKnowQA的知识调整的LLMs，在响应生成方面可以表现出比原始指导调整更高水平的准确性，并为LLMs的领域适应提供了一种可靠的新方式。

    Large Language Models (LLMs) have demonstrated remarkable success in diverse natural language processing (NLP) tasks in general domains. However, LLMs sometimes generate responses with the hallucination about medical facts due to limited domain knowledge. Such shortcomings pose potential risks in the utilization of LLMs within medical contexts. To address this challenge, we propose knowledge-tuning, which leverages structured medical knowledge bases for the LLMs to grasp domain knowledge efficiently and facilitate reliable response generation. We also release cMedKnowQA, a Chinese medical knowledge question-answering dataset constructed from medical knowledge bases to assess the medical knowledge proficiency of LLMs. Experimental results show that the LLMs which are knowledge-tuned with cMedKnowQA, can exhibit higher levels of accuracy in response generation compared with vanilla instruction-tuning and offer a new reliable way for the domain adaptation of LLMs.
    
[^21]: 基于流形的无调参提示分类的重新嵌入方法

    Manifold-based Verbalizer Space Re-embedding for Tuning-free Prompt-based Classification. (arXiv:2309.04174v1 [cs.CL])

    [http://arxiv.org/abs/2309.04174](http://arxiv.org/abs/2309.04174)

    本研究提出了一种无需调参的基于流形的语言转换器嵌入方法，通过保留同一类中的局部特性来进行分类，实验证明其与自动化的语言转换器效果相当。

    

    提示分类通过利用[MASK]标记的遗漏问题形式来适应任务，然后通过预定义的语言转换器将填充的标记映射到标签上。最近的研究已经探索了使用语言转换器嵌入来减少这一过程中的劳动力。然而，所有现有的研究都需要对预训练模型或附加可训练嵌入进行调参过程。同时，由于表示空间中潜在的非线性流形，高维语言转换器嵌入之间的距离不应该使用欧氏距离来衡量。在本研究中，我们提出了一种无调参基于流形的空间重新嵌入方法，称为具有内类近邻约束的局部线性嵌入（LLE-INC），用于语言转换器嵌入，它保留了同一类中的局部特性作为分类的引导。实验结果表明，即使不进行任何参数调优，我们的LLE-INC与自动化的语言转换器相媲美。

    Prompt-based classification adapts tasks to a cloze question format utilizing the [MASK] token and the filled tokens are then mapped to labels through pre-defined verbalizers. Recent studies have explored the use of verbalizer embeddings to reduce labor in this process. However, all existing studies require a tuning process for either the pre-trained models or additional trainable embeddings. Meanwhile, the distance between high-dimensional verbalizer embeddings should not be measured by Euclidean distance due to the potential for non-linear manifolds in the representation space. In this study, we propose a tuning-free manifold-based space re-embedding method called Locally Linear Embedding with Intra-class Neighborhood Constraint (LLE-INC) for verbalizer embeddings, which preserves local properties within the same class as guidance for classification. Experimental results indicate that even without tuning any parameters, our LLE-INC is on par with automated verbalizers with parameter 
    
[^22]: 利用特征缺失感知校准的原型患者表示来缓解电子健康记录数据稀疏性问题

    Leveraging Prototype Patient Representations with Feature-Missing-Aware Calibration to Mitigate EHR Data Sparsity. (arXiv:2309.04160v1 [cs.LG])

    [http://arxiv.org/abs/2309.04160](http://arxiv.org/abs/2309.04160)

    本研究提出了一种利用原型患者表示和特征缺失感知校准的间接插补方法，以缓解电子健康记录数据稀疏性问题，通过获取更密集的嵌入来提高预测模型的有效性。

    

    电子健康记录（EHR）数据经常呈现稀疏特征，给预测建模带来挑战。当前的直接插补方法（如矩阵插补方法）依赖于参考类似行或列来完成原始缺失数据，不区分插补和实际值。因此，模型可能会无意中将与预测目标无关的或具有欺骗性的信息纳入其中，从而损害下游性能的效果。虽然一些方法尝试在直接插补后重新校准或增强EHR嵌入，但它们经常错误地优先考虑插补特征。这种优先错误可能会给模型引入偏见或不准确性。为了解决这些问题，我们的工作采用间接插补，利用类似患者的原型表示获取更密集的嵌入。认识到在衡量时通常将缺失特征与存在特征相同的限制时

    Electronic Health Record (EHR) data frequently exhibits sparse characteristics, posing challenges for predictive modeling. Current direct imputation such as matrix imputation approaches hinge on referencing analogous rows or columns to complete raw missing data and do not differentiate between imputed and actual values. As a result, models may inadvertently incorporate irrelevant or deceptive information with respect to the prediction objective, thereby compromising the efficacy of downstream performance. While some methods strive to recalibrate or augment EHR embeddings after direct imputation, they often mistakenly prioritize imputed features. This misprioritization can introduce biases or inaccuracies into the model. To tackle these issues, our work resorts to indirect imputation, where we leverage prototype representations from similar patients to obtain a denser embedding. Recognizing the limitation that missing features are typically treated the same as present ones when measurin
    
[^23]: NESTLE：一种用于法律语料库统计分析的无代码工具

    NESTLE: a No-Code Tool for Statistical Analysis of Legal Corpus. (arXiv:2309.04146v1 [cs.CL])

    [http://arxiv.org/abs/2309.04146](http://arxiv.org/abs/2309.04146)

    NESTLE是一个无代码工具，用于进行大规模法律语料库的统计分析。它提供了搜索引擎、端到端的信息提取系统和一个大语言模型，可以通过聊天界面进行操作，使用户可以搜索目标文件、提取信息并可视化数据。

    

    大规模法律语料库的统计分析可以提供有价值的法律见解。为了进行这样的分析，需要使用文档检索工具选择语料库的子集，使用信息提取（IE）系统对文本进行结构化，并对数据进行可视化以进行统计分析。每个过程都需要专业工具或编程技能，然而还没有全面的无代码工具可用。尤其是对于IE，如果IE系统的本体中没有预定义的目标信息，那么需要自己构建系统。在这里，我们提供了NESTLE，一种用于大规模法律语料库统计分析的无代码工具。通过NESTLE，用户可以通过聊天界面搜索目标文件、提取信息，并通过辅助GUI进行细致级别的控制来可视化结构化数据。NESTLE由三个主要组件组成：搜索引擎、端到端的IE系统和一个大语言模型（LLM），它将各个组件连接起来。

    The statistical analysis of large scale legal corpus can provide valuable legal insights. For such analysis one needs to (1) select a subset of the corpus using document retrieval tools, (2) structuralize text using information extraction (IE) systems, and (3) visualize the data for the statistical analysis. Each process demands either specialized tools or programming skills whereas no comprehensive unified "no-code" tools have been available. Especially for IE, if the target information is not predefined in the ontology of the IE system, one needs to build their own system. Here we provide NESTLE, a no code tool for large-scale statistical analysis of legal corpus. With NESTLE, users can search target documents, extract information, and visualize the structured data all via the chat interface with accompanying auxiliary GUI for the fine-level control. NESTLE consists of three main components: a search engine, an end-to-end IE system, and a Large Language Model (LLM) that glues the who
    
[^24]: 可信和协同的软件工程人工智能：愿景与路线图

    Trustworthy and Synergistic Artificial Intelligence for Software Engineering: Vision and Roadmaps. (arXiv:2309.04142v1 [cs.SE])

    [http://arxiv.org/abs/2309.04142](http://arxiv.org/abs/2309.04142)

    本文提出了可信和协同的软件工程人工智能的愿景和路线图，旨在提高开发人员的生产力和软件质量。实现这个愿景可能导致软件工程领域的一个重要转变，即软件工程2.0.

    

    数十年来，软件工程研究一直致力于设计自动化解决方案，旨在提高开发人员的生产力和提升软件质量。过去的两个十年见证了专门为软件工程任务量身定制的智能解决方案的不断涌现。这个势头建立了软件工程领域中最活跃和最受欢迎的 Artificial Intelligence for Software Engineering (AI4SE) 领域。本文探讨了几个重点。它首先简要介绍和回顾了AI4SE的历史。随后，它强调了AI4SE固有的核心挑战，特别是强调了实现可信和协同的AI4SE的需求。进一步，本文描绘了一种愿景，即如果克服了AI4SE的核心挑战，将实现可达到的潜在飞跃，建议转向软件工程2.0.

    For decades, much software engineering research has been dedicated to devising automated solutions aimed at enhancing developer productivity and elevating software quality. The past two decades have witnessed an unparalleled surge in the development of intelligent solutions tailored for software engineering tasks. This momentum established the Artificial Intelligence for Software Engineering (AI4SE) area, which has swiftly become one of the most active and popular areas within the software engineering field.  This Future of Software Engineering (FoSE) paper navigates through several focal points. It commences with a succinct introduction and history of AI4SE. Thereafter, it underscores the core challenges inherent to AI4SE, particularly highlighting the need to realize trustworthy and synergistic AI4SE. Progressing, the paper paints a vision for the potential leaps achievable if AI4SE's key challenges are surmounted, suggesting a transition towards Software Engineering 2.0. Two strateg
    
[^25]: 浮动基底机器人的本体感知外力学习及其在人形机器人步态中的应用

    Proprioceptive External Torque Learning for Floating Base Robot and its Applications to Humanoid Locomotion. (arXiv:2309.04138v1 [cs.RO])

    [http://arxiv.org/abs/2309.04138](http://arxiv.org/abs/2309.04138)

    本文介绍了一种使用本体感知传感器学习浮动基底机器人外部关节力矩的方法，实验证明该网络在估计外部力矩和接触力矩方面具有较小的误差，并且可用于实现稳定步行。

    

    对于实现人形机器人和以安全为导向的机器人的稳定步态，估计外部关节力矩和接触力矩是必不可少的。尽管可以使用力矩传感器（FTS）测量人形机器人脚部的接触力矩，但FTS会增加系统的成本、惯性、复杂性和故障可能性。本文介绍了一种使用本体感知传感器（编码器和IMU）学习浮动基底机器人外部关节力矩的方法。学习过程中使用了GRU网络并收集了随机行走数据。真实机器人实验表明，与基于模型的方法（具有摩擦建模的动量观察器）相比，该网络能够更小地估计外部力矩和接触力矩。该研究还验证了估计的接触力矩可以用于零力矩点（ZMP）反馈控制，实现稳定步行。此外，即使机器人的脚部和上半身的惯性存在变化，该方法仍然有效。

    The estimation of external joint torque and contact wrench is essential for achieving stable locomotion of humanoids and safety-oriented robots. Although the contact wrench on the foot of humanoids can be measured using a force-torque sensor (FTS), FTS increases the cost, inertia, complexity, and failure possibility of the system. This paper introduces a method for learning external joint torque solely using proprioceptive sensors (encoders and IMUs) for a floating base robot. For learning, the GRU network is used and random walking data is collected. Real robot experiments demonstrate that the network can estimate the external torque and contact wrench with significantly smaller errors compared to the model-based method, momentum observer (MOB) with friction modeling. The study also validates that the estimated contact wrench can be utilized for zero moment point (ZMP) feedback control, enabling stable walking. Moreover, even when the robot's feet and the inertia of the upper body are
    
[^26]: 弱监督的点云转换器用于3D目标检测

    Weakly Supervised Point Clouds Transformer for 3D Object Detection. (arXiv:2309.04105v1 [cs.CV])

    [http://arxiv.org/abs/2309.04105](http://arxiv.org/abs/2309.04105)

    本文提出了一个弱监督的点云转换器框架用于3D目标检测，通过无监督投票提议模块和自注意机制，提高了学生网络的表现，减少了对注释数据的需求

    

    在场景理解中，需要对3D数据集进行语义分割和目标检测的注释。本文提出了一个弱监督的点云转换器框架用于3D目标检测。其目的是减少训练所需的监督量，因为标注3D数据集的成本较高。我们提出了无监督投票提议模块，该模块学习了随机预设的锚点，并使用投票网络选择高质量的预设锚点。然后，它将信息蒸馏到学生和教师网络中。在学生网络方面，我们应用ResNet网络来有效提取局部特征。然而，这也可能会丢失大量的全局信息。为了提供将全局和局部信息结合为学生网络的输入，我们采用了transformer的自注意机制来提取全局特征，并使用ResNet层提取区域提议

    The annotation of 3D datasets is required for semantic-segmentation and object detection in scene understanding. In this paper we present a framework for the weakly supervision of a point clouds transformer that is used for 3D object detection. The aim is to decrease the required amount of supervision needed for training, as a result of the high cost of annotating a 3D datasets. We propose an Unsupervised Voting Proposal Module, which learns randomly preset anchor points and uses voting network to select prepared anchor points of high quality. Then it distills information into student and teacher network. In terms of student network, we apply ResNet network to efficiently extract local characteristics. However, it also can lose much global information. To provide the input which incorporates the global and local information as the input of student networks, we adopt the self-attention mechanism of transformer to extract global features, and the ResNet layers to extract region proposals
    
[^27]: 低功率通信信号的数据驱动分类方法：使用软件定义无线电的非验证用户 (arXiv:2309.04088v1 [eess.SP])

    Data-driven classification of low-power communication signals by an unauthenticated user using a software-defined radio. (arXiv:2309.04088v1 [eess.SP])

    [http://arxiv.org/abs/2309.04088](http://arxiv.org/abs/2309.04088)

    本文提出了一种基于神经网络的方法，利用LoRa信号的瞬时频率表示中的结构模式，实现了对低功率通信信号的分类。这种方法能够有效抵御非验证攻击者对LoRa协议的拒绝服务攻击。

    

    许多大规模的分布式多智能体系统通过低功率通信网络交换信息。特别是在机器人网络应用中，代理器以有限的功率在未经许可的频谱上间歇性地通信状态和控制信号，容易受到窃听和拒绝服务攻击。本文指出，一种被广泛使用的低功率通信协议LoRa，如果一个非验证的攻击者能够成功识别目标信号的带宽和扩频因子，就容易受到拒绝服务攻击。利用LoRa信号瞬时频率表示中的结构模式，我们将联合推断这两个未知参数问题与分类问题相关联，可以通过神经网络高效实现。

    Many large-scale distributed multi-agent systems exchange information over low-power communication networks. In particular, agents intermittently communicate state and control signals in robotic network applications, often with limited power over an unlicensed spectrum, prone to eavesdropping and denial-of-service attacks. In this paper, we argue that a widely popular low-power communication protocol known as LoRa is vulnerable to denial-of-service attacks by an unauthenticated attacker if it can successfully identify a target signal's bandwidth and spreading factor. Leveraging a structural pattern in the LoRa signal's instantaneous frequency representation, we relate the problem of jointly inferring the two unknown parameters to a classification problem, which can be efficiently implemented using neural networks.
    
[^28]: 对于图表示学习的混合曲率Transformer: 拐弯你的注意力

    Curve Your Attention: Mixed-Curvature Transformers for Graph Representation Learning. (arXiv:2309.04082v1 [cs.LG])

    [http://arxiv.org/abs/2309.04082](http://arxiv.org/abs/2309.04082)

    本文提出了一种混合曲率Transformer，用于图表示学习。通过将完全乘积立体变换器与标记化的图Transformer相结合，模型能够以端到端的方式学习适应输入图的曲率，从而更好地嵌入图中的分层或循环结构。

    

    现实世界中的图往往具有不适合典型欧几里得空间的分层或循环结构。虽然存在能够利用双曲或球面空间学习更准确嵌入这些结构的图神经网络，但这些方法局限于消息传递范式，使得模型容易受到过度平滑和过度压缩等副作用的影响。最近的研究提出了基于全局注意力的图Transformer，可以轻松建模长程交互，但对非欧几里得几何的拓展尚未得到探索。为了弥合这一差距，我们提出了完全乘积立体变换器，这是一种对常曲率空间的变换器的推广。当与标记化的图Transformer结合使用时，我们的模型可以以端到端的方式学习适合输入图的曲率，无需在不同曲率上进行额外调整。

    Real-world graphs naturally exhibit hierarchical or cyclical structures that are unfit for the typical Euclidean space. While there exist graph neural networks that leverage hyperbolic or spherical spaces to learn representations that embed such structures more accurately, these methods are confined under the message-passing paradigm, making the models vulnerable against side-effects such as oversmoothing and oversquashing. More recent work have proposed global attention-based graph Transformers that can easily model long-range interactions, but their extensions towards non-Euclidean geometry are yet unexplored. To bridge this gap, we propose Fully Product-Stereographic Transformer, a generalization of Transformers towards operating entirely on the product of constant curvature spaces. When combined with tokenized graph Transformers, our model can learn the curvature appropriate for the input graph in an end-to-end fashion, without the need of additional tuning on different curvature i
    
[^29]: SayNav：将大型语言模型用于新环境中的动态规划导航

    SayNav: Grounding Large Language Models for Dynamic Planning to Navigation in New Environments. (arXiv:2309.04077v1 [cs.RO])

    [http://arxiv.org/abs/2309.04077](http://arxiv.org/abs/2309.04077)

    SayNav是一种使用大型语言模型进行动态规划导航的方法，通过使用人类知识和场景图实现对复杂导航任务的高效泛化，动态生成指令并根据新信息不断完善未来步骤。

    

    语义推理和动态规划能力对于一个自主代理在未知环境中执行复杂导航任务至关重要。为了在这些任务中取得成功，需要大量的常识知识，这是人类所具备的。我们提出了SayNav，一种新的方法，利用来自大型语言模型（LLMs）的人类知识，以便高效地对未知大规模环境中的复杂导航任务进行泛化。SayNav使用一种新颖的接地机制，逐步构建一个探索环境的3D场景图，并将其作为LLMs的输入，用于生成可行且上下文适当的高层导航计划。然后，由预先训练的低层规划器执行LLM生成的计划，将每个计划的步骤视为短距离点目标导航子任务。SayNav在导航过程中动态生成一步一步的指令，并根据新获取的信息不断完善未来步骤。我们在一个新的多任务机验证环境上评估了SayNav。

    Semantic reasoning and dynamic planning capabilities are crucial for an autonomous agent to perform complex navigation tasks in unknown environments. It requires a large amount of common-sense knowledge, that humans possess, to succeed in these tasks. We present SayNav, a new approach that leverages human knowledge from Large Language Models (LLMs) for efficient generalization to complex navigation tasks in unknown large-scale environments. SayNav uses a novel grounding mechanism, that incrementally builds a 3D scene graph of the explored environment as inputs to LLMs, for generating feasible and contextually appropriate high-level plans for navigation. The LLM-generated plan is then executed by a pre-trained low-level planner, that treats each planned step as a short-distance point-goal navigation sub-task. SayNav dynamically generates step-by-step instructions during navigation and continuously refines future steps based on newly perceived information. We evaluate SayNav on a new mul
    
[^30]: 计算高效的数据驱动非线性系统的发现和线性表示的控制方法

    Computationally Efficient Data-Driven Discovery and Linear Representation of Nonlinear Systems For Control. (arXiv:2309.04074v1 [eess.SY])

    [http://arxiv.org/abs/2309.04074](http://arxiv.org/abs/2309.04074)

    本论文提出了一种计算高效的数据驱动方法，使用Koopman算子理论来发现和线性表示非线性系统，并应用于控制。与自编码器基准方法相比，我们的方法在训练效率和准确性上都更优秀。

    

    该研究着重于使用Koopman算子理论开发一种数据驱动框架，用于非线性系统的系统辨识和线性化以进行控制。我们提出的方法提供了一个具有递归学习的深度学习框架。通过线性二次控制来控制最终的线性系统。使用一个摆动系统的实例来进行演示，并通过对噪声数据的模拟来验证。我们展示了我们的方法比一个自编码器基准方法更高效和更准确。

    This work focuses on developing a data-driven framework using Koopman operator theory for system identification and linearization of nonlinear systems for control. Our proposed method presents a deep learning framework with recursive learning. The resulting linear system is controlled using a linear quadratic control. An illustrative example using a pendulum system is presented with simulations on noisy data. We show that our proposed method is trained more efficiently and is more accurate than an autoencoder baseline.
    
[^31]: 通过基于人工智能的因果模型推断物理定律

    Inferring physical laws by artificial intelligence based causal models. (arXiv:2309.04069v1 [cs.AI])

    [http://arxiv.org/abs/2309.04069](http://arxiv.org/abs/2309.04069)

    本文提出了一种基于因果学习的物理原理模型，不仅能识别相关性，还能展现因果关系，从而为机器辅助的科学发现提供了新的途径。

    

    人工智能（AI）和机器学习（ML）的进步为科学研究打开了许多新的途径，并为知识创造过程增添了新的维度。然而，迄今为止最强大和多功能的ML应用主要在关联分析领域，归结为复杂数据拟合。Judea Pearl指出，人工通用智能必须涉及到干预和想象的行为。因此，任何机器辅助的科学发现都必须包括因果分析和干预。在这个背景下，我们提出了一种基于因果学习的物理原理模型，它不仅能识别相关性，还能展现因果关系。我们使用因果推断和干预原则来研究一些著名物理现象背后的因果关系。我们表明，这种技术不仅能找出数据之间的关联性，而且还能够

    The advances in Artificial Intelligence (AI) and Machine Learning (ML) have opened up many avenues for scientific research, and are adding new dimensions to the process of knowledge creation. However, even the most powerful and versatile of ML applications till date are primarily in the domain of analysis of associations and boil down to complex data fitting. Judea Pearl has pointed out that Artificial General Intelligence must involve interventions involving the acts of doing and imagining. Any machine assisted scientific discovery thus must include casual analysis and interventions. In this context, we propose a causal learning model of physical principles, which not only recognizes correlations but also brings out casual relationships. We use the principles of causal inference and interventions to study the cause-and-effect relationships in the context of some well-known physical phenomena. We show that this technique can not only figure out associations among data, but is also able
    
[^32]: 3D去噪器是好的2D教师：通过去噪和跨模态蒸馏进行分子预训练

    3D Denoisers are Good 2D Teachers: Molecular Pretraining via Denoising and Cross-Modal Distillation. (arXiv:2309.04062v1 [cs.LG])

    [http://arxiv.org/abs/2309.04062](http://arxiv.org/abs/2309.04062)

    本论文提出了一种自监督的分子表示学习框架D&D，通过将3D去噪器的表示蒸馏到2D图形编码器中进行预训练，以解决从大规模无标签数据中预训练分子表示的问题。

    

    从大量无标签数据中预训练分子表示对于分子属性预测至关重要，因为获取地面真实标签的成本很高。虽然存在各种基于2D图形的分子预训练方法，但这些方法难以在预测性能上显示出统计学上的显著提升。因此，最近的工作提出了在去噪任务下基于3D构象的预训练方法，取得了有希望的结果。然而，在下游微调过程中，使用3D构象训练的模型需要准确的先前未见过的分子原子坐标，这在大规模情况下计算成本很高。基于这一限制，我们提出了D&D，一种自监督的分子表示学习框架，通过将3D去噪器的表示蒸馏到2D图形编码器中进行预训练。通过去噪和跨模态知识蒸馏，我们的方法不仅利用了从去噪中获得的知识，而且应用起来很容易。

    Pretraining molecular representations from large unlabeled data is essential for molecular property prediction due to the high cost of obtaining ground-truth labels. While there exist various 2D graph-based molecular pretraining approaches, these methods struggle to show statistically significant gains in predictive performance. Recent work have thus instead proposed 3D conformer-based pretraining under the task of denoising, which led to promising results. During downstream finetuning, however, models trained with 3D conformers require accurate atom-coordinates of previously unseen molecules, which are computationally expensive to acquire at scale. In light of this limitation, we propose D&D, a self-supervised molecular representation learning framework that pretrains a 2D graph encoder by distilling representations from a 3D denoiser. With denoising followed by cross-modal knowledge distillation, our approach enjoys use of knowledge obtained from denoising as well as painless applica
    
[^33]: 评估大型语言模型在发现基因集合功能方面的应用

    Evaluation of large language models for discovery of gene set function. (arXiv:2309.04019v1 [q-bio.GN])

    [http://arxiv.org/abs/2309.04019](http://arxiv.org/abs/2309.04019)

    本研究评估了OpenAI的GPT-4大型语言模型在发现基因集合功能方面的能力，并发现它能根据嵌入的生物医学知识生成与Gene Ontology中具名基因集合非常相似的名称，同时在基因组学数据中发现的基因集合中，GPT-4的命名更具信息量，得到了人工审核的基本验证。

    

    基因集合分析是功能基因组学的重要方法，但它依赖于手动创建的基因功能数据库，这些数据库的不完整和不具备生物学上下文的特点。本文评估了OpenAI的GPT-4大型语言模型的能力，从其嵌入的生物医学知识中发展出有关常见基因功能的假设。我们创建了一个GPT-4流水线，用于用总结其共识功能的名称标记基因集合，并通过分析文本和引文进行证实。在与Gene Ontology中的具名基因集合进行基准测试时，GPT-4在50%的情况下生成了非常相似的名称，而在大多数其他情况下，则恢复了更一般概念的名称。在基因组学数据中发现的基因集合中，与基因集合富集相比，GPT-4的命名更具信息量，其支持性陈述和引文在人工审核中得到了基本验证。快速综合常见基因功能的能力使得大型语言模型成为有价值的功能基因组学助手。

    Gene set analysis is a mainstay of functional genomics, but it relies on manually curated databases of gene functions that are incomplete and unaware of biological context. Here we evaluate the ability of OpenAI's GPT-4, a Large Language Model (LLM), to develop hypotheses about common gene functions from its embedded biomedical knowledge. We created a GPT-4 pipeline to label gene sets with names that summarize their consensus functions, substantiated by analysis text and citations. Benchmarking against named gene sets in the Gene Ontology, GPT-4 generated very similar names in 50% of cases, while in most remaining cases it recovered the name of a more general concept. In gene sets discovered in 'omics data, GPT-4 names were more informative than gene set enrichment, with supporting statements and citations that largely verified in human review. The ability to rapidly synthesize common gene functions positions LLMs as valuable functional genomics assistants.
    
[^34]: ConDA: 基于对比域适应的AI生成文本检测

    ConDA: Contrastive Domain Adaptation for AI-generated Text Detection. (arXiv:2309.03992v1 [cs.CL])

    [http://arxiv.org/abs/2309.03992](http://arxiv.org/abs/2309.03992)

    创新点：提出了一种基于对比域适应的框架 ConDA，用于检测由大型语言模型生成的新闻文本。这种方法解决了获取标记训练数据的困难，通过利用未标记的目标数据进行无监督域适应。

    

    大型语言模型（LLMs）越来越多地被用于各种用途的文本生成，包括新闻报道。鉴于这些LLMs可能被恶意使用来大规模生成虚假信息，构建有效的检测AI生成文本的工具显得尤为重要。由于新的LLMs不断被开发，获取用于监督式检测器的标记训练数据成为一个瓶颈。然而，可能存在大量未标记的文本数据，没有关于其生成器的信息。在这项工作中，我们解决了此数据问题，即检测AI生成的新闻文本，并将问题框架化为无监督域适应任务。这里的域是不同的文本生成器，即LLMs，我们假设只能访问标记的源数据和未标记的目标数据。我们开发了一个名为ConDA的对比域适应框架，将标准的域适应技术与表示能力相结合。

    Large language models (LLMs) are increasingly being used for generating text in a variety of use cases, including journalistic news articles. Given the potential malicious nature in which these LLMs can be used to generate disinformation at scale, it is important to build effective detectors for such AI-generated text. Given the surge in development of new LLMs, acquiring labeled training data for supervised detectors is a bottleneck. However, there might be plenty of unlabeled text data available, without information on which generator it came from. In this work we tackle this data problem, in detecting AI-generated news text, and frame the problem as an unsupervised domain adaptation task. Here the domains are the different text generators, i.e. LLMs, and we assume we have access to only the labeled source data and unlabeled target data. We develop a Contrastive Domain Adaptation framework, called ConDA, that blends standard domain adaptation techniques with the representation power 
    
[^35]: 噪声计算$\mathsf{OR}$和$\mathsf{MAX}$函数

    Noisy Computing of the $\mathsf{OR}$ and $\mathsf{MAX}$ Functions. (arXiv:2309.03986v1 [cs.DS])

    [http://arxiv.org/abs/2309.03986](http://arxiv.org/abs/2309.03986)

    本研究考虑使用噪声查询计算$\mathsf{OR}$和$\mathsf{MAX}$函数，结果表明，在错误概率收敛时，计算这两个函数所需的期望查询数量与Kullback-Leibler差异之间有密切关系。

    

    我们考虑使用含有噪声的查询来计算一个包含$n$个变量的函数的问题，其中每个查询在某个固定的已知概率$p \in (0,1/2)$下是不正确的。具体来说，我们考虑计算$n$个位的$\mathsf{OR}$函数（其中查询对应于位的噪声读数）和$n$个实数的$\mathsf{MAX}$函数（其中查询对应于有噪声的两两比较）。我们证明，在误差概率$\delta = o(1)$迅速收敛的情况下，计算这两个函数所需的期望查询数量为\[ (1 \pm o(1)) \frac{n\log \frac{1}{\delta}}{D_{\mathsf{KL}}(p \| 1-p)} \] ，其中 $D_{\mathsf{KL}}(p \| 1-p)$表示$\mathsf{Bern}(p)$和$\mathsf{Bern}(1-p)$分布之间的Kullback-Leibler差异。与以前的工作相比，我们的结果在两个函数的上下界中都加强了对$p$的依赖关系。

    We consider the problem of computing a function of $n$ variables using noisy queries, where each query is incorrect with some fixed and known probability $p \in (0,1/2)$. Specifically, we consider the computation of the $\mathsf{OR}$ function of $n$ bits (where queries correspond to noisy readings of the bits) and the $\mathsf{MAX}$ function of $n$ real numbers (where queries correspond to noisy pairwise comparisons). We show that an expected number of queries of \[ (1 \pm o(1)) \frac{n\log \frac{1}{\delta}}{D_{\mathsf{KL}}(p \| 1-p)} \] is both sufficient and necessary to compute both functions with a vanishing error probability $\delta = o(1)$, where $D_{\mathsf{KL}}(p \| 1-p)$ denotes the Kullback-Leibler divergence between $\mathsf{Bern}(p)$ and $\mathsf{Bern}(1-p)$ distributions. Compared to previous work, our results tighten the dependence on $p$ in both the upper and lower bounds for the two functions.
    
[^36]: 大规模自动创建有声读物

    Large-Scale Automatic Audiobook Creation. (arXiv:2309.03926v1 [cs.SD])

    [http://arxiv.org/abs/2309.03926](http://arxiv.org/abs/2309.03926)

    这项工作提出了一个可以自动生成高质量有声读物的系统，通过利用神经文本到语音技术，从在线电子书中创建数千本开放许可的有声读物。该系统允许用户自定义朗读速度和风格，并且提供了匹配所需声音的功能。该工作贡献了5000多本有声读物和一个交互式演示。

    

    有声读物可以显著提高文学作品的可访问性和读者参与度。然而，制作、编辑和发布有声读物可能需要数百小时的人力。在这项工作中，我们提出了一个可以从在线电子书自动生成高质量有声读物的系统。特别是，我们利用最近在神经文本到语音方面的进展，从项目古腾堡电子书收藏中创建并发布了数千本高质量、开放许可的有声读物。我们的方法可以针对各种结构多样的图书，识别出适合朗读的电子书内容的合适子集，并可以并行处理数百本书籍。我们的系统允许用户自定义有声读物的朗读速度和风格、情感语调，甚至可以使用少量样本音频来匹配所需的声音。这项工作贡献了5000多本开放许可的有声读物以及一个交互式演示，使用户能够快速创建自己定制的有声读物。

    An audiobook can dramatically improve a work of literature's accessibility and improve reader engagement. However, audiobooks can take hundreds of hours of human effort to create, edit, and publish. In this work, we present a system that can automatically generate high-quality audiobooks from online e-books. In particular, we leverage recent advances in neural text-to-speech to create and release thousands of human-quality, open-license audiobooks from the Project Gutenberg e-book collection. Our method can identify the proper subset of e-book content to read for a wide collection of diversely structured books and can operate on hundreds of books in parallel. Our system allows users to customize an audiobook's speaking speed and style, emotional intonation, and can even match a desired voice using a small amount of sample audio. This work contributed over five thousand open-license audiobooks and an interactive demo that allows users to quickly create their own customized audiobooks. T
    
[^37]: 为给定计算时间限制的假布尔优化问题自动选择算法

    Automatic Algorithm Selection for Pseudo-Boolean Optimization with Given Computational Time Limits. (arXiv:2309.03924v1 [cs.AI])

    [http://arxiv.org/abs/2309.03924](http://arxiv.org/abs/2309.03924)

    提出一种利用机器学习技术自动选择最佳求解器的方法，该方法适用于解决各种问题，并且在考虑了计算时间限制的情况下依然能够预测最佳求解器。

    

    提出了利用机器学习技术从解算器组合中自动选择最佳求解器的方法。这些技术已经应用于布尔满足、旅行商、图着色等各种问题。这些方法被称为元求解器，它们以问题实例和解算器组合作为输入，预测最佳求解器并执行以得到解决方案。通常，解决方案的质量会随着计算时间的增加而提高。因此，开发了任何时间选择器，它考虑了问题实例和用户设定的计算时间限制，并在指定的时间限制内预测最佳求解器。设计带有“任何时间”特性的元求解器比不带此特性的元求解器更具挑战性。本研究侧重于设计针对NP困难问题的任意时间元求解器的任务。

    Machine learning (ML) techniques have been proposed to automatically select the best solver from a portfolio of solvers, based on predicted performance. These techniques have been applied to various problems, such as Boolean Satisfiability, Traveling Salesperson, Graph Coloring, and others.  These methods, known as meta-solvers, take an instance of a problem and a portfolio of solvers as input. They then predict the best-performing solver and execute it to deliver a solution. Typically, the quality of the solution improves with a longer computational time. This has led to the development of anytime selectors, which consider both the instance and a user-prescribed computational time limit. Anytime meta-solvers predict the best-performing solver within the specified time limit.  Constructing an anytime meta-solver is considerably more challenging than building a meta-solver without the "anytime" feature. In this study, we focus on the task of designing anytime meta-solvers for the NP-har
    
[^38]: 管理脊髓刺激术慢性疼痛患者的推荐系统

    A recommender for the management of chronic pain in patients undergoing spinal cord stimulation. (arXiv:2309.03918v1 [cs.AI])

    [http://arxiv.org/abs/2309.03918](http://arxiv.org/abs/2309.03918)

    本文提出了一种推荐系统，用于管理进行SCS的慢性疼痛患者。通过使用上下文多臂赌博方法开发的系统，为患者推荐SCS设置，旨在改善其状况。这些推荐通过数字健康生态系统直接发送给患者，并与患者监测系统结合，为慢性疼痛患者的整个治疗过程提供闭环关怀。

    

    脊髓刺激术（Spinal cord stimulation，SCS）是一种用于治疗慢性疼痛的治疗方法。其通过植入装置向脊髓传递电脉冲，在给予适当的刺激参数时，可以掩盖或阻断疼痛信号。优化刺激参数的选择通常在临床由医生负责，而在家庭中的SCS优化则由患者自己管理。在本文中，我们提出了一种用于管理进行SCS的慢性疼痛患者的推荐系统。具体而言，我们使用了一种上下文多臂赌博（CMAB）方法来开发一个系统，为患者推荐SCS设置，旨在改善其状况。这些推荐通过数字健康生态系统直接发送给患者，并与患者监测系统结合，为慢性疼痛患者的整个治疗过程提供闭环关怀。我们在一组接受SCS植入的ENVISION患者中进行了系统评估。

    Spinal cord stimulation (SCS) is a therapeutic approach used for the management of chronic pain. It involves the delivery of electrical impulses to the spinal cord via an implanted device, which when given suitable stimulus parameters can mask or block pain signals. Selection of optimal stimulation parameters usually happens in the clinic under the care of a provider whereas at-home SCS optimization is managed by the patient. In this paper, we propose a recommender system for the management of pain in chronic pain patients undergoing SCS. In particular, we use a contextual multi-armed bandit (CMAB) approach to develop a system that recommends SCS settings to patients with the aim of improving their condition. These recommendations, sent directly to patients though a digital health ecosystem, combined with a patient monitoring system closes the therapeutic loop around a chronic pain patient over their entire patient journey. We evaluated the system in a cohort of SCS-implanted ENVISION 
    
[^39]: 通讯和参考曲目在混音过程中的作用：来自专业混音师的见解

    The Role of Communication and Reference Songs in the Mixing Process: Insights from Professional Mix Engineers. (arXiv:2309.03404v1 [cs.HC])

    [http://arxiv.org/abs/2309.03404](http://arxiv.org/abs/2309.03404)

    本论文研究了专业混音师与客户的互动以及他们如何利用客户的反馈指导混音过程。结果表明，混音师通过使用参考曲目和合作沟通等方式，与客户建立起对混音期望音效的默契约定。

    

    有效的音乐混音需要技术和创造力的精湛，但与客户的清晰沟通至关重要。混音师必须理解客户的期望和偏好，并共同努力实现所需的音效。通过使用参考曲目和艺术家与工程师之间交换的演示混音等指南，通常可以达成对混音期望音效的默契约定，有时还可以使用语义术语进行言说。本文介绍了一项由两个阶段构成的探索性研究的发现，旨在了解专业混音师如何与客户互动，并利用他们的反馈指导混音过程。在第一阶段，对五名混音师进行了半结构化面谈，旨在了解他们的沟通策略、创造过程和决策标准。基于这些访谈的推论，设计了一个在线问卷，并对22名混音师进行了调查。

    Effective music mixing requires technical and creative finesse, but clear communication with the client is crucial. The mixing engineer must grasp the client's expectations, and preferences, and collaborate to achieve the desired sound. The tacit agreement for the desired sound of the mix is often established using guides like reference songs and demo mixes exchanged between the artist and the engineer and sometimes verbalised using semantic terms. This paper presents the findings of a two-phased exploratory study aimed at understanding how professional mixing engineers interact with clients and use their feedback to guide the mixing process. For phase one, semi-structured interviews were conducted with five mixing engineers with the aim of gathering insights about their communication strategies, creative processes, and decision-making criteria. Based on the inferences from these interviews, an online questionnaire was designed and administered to a larger group of 22 mixing engineers 
    
[^40]: BEVTrack：一种针对鸟瞰图中3D单物体跟踪的简单基线

    BEVTrack: A Simple Baseline for 3D Single Object Tracking in Birds's-Eye-View. (arXiv:2309.02185v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2309.02185](http://arxiv.org/abs/2309.02185)

    本文介绍了一种名为BEVTrack的简单却强大的基线框架，用于解决点云中3D单物体跟踪的挑战。通过将点云转换为鸟瞰图表示，并进行简单的逐元素操作，BEVTrack能够编码空间邻近性和捕捉运动线索，从而实现高效的跟踪。

    

    由于外观变化、干扰物和点云的高稀疏性，点云中的3D单物体跟踪仍然是一个具有挑战性的问题。值得注意的是，在自动驾驶场景中，目标物体通常在连续帧中保持空间邻近性，主要水平移动。这种空间连续性为目标定位提供了宝贵的先验知识。然而，现有的跟踪器通常采用点级表示，难以有效利用这种知识，因为这种表示的不规则格式。因此，它们需要精心设计并解决多个子任务来建立空间对应关系。在本文中，我们介绍BEVTrack，一个简单但强大的用于3D单物体跟踪的基线框架。通过将连续的点云转换为常见的鸟瞰图表示，BEVTrack通过简单的逐元素操作自然地编码了空间邻近性，并灵活地捕捉了跟踪的运动线索。

    3D single object tracking (SOT) in point clouds is still a challenging problem due to appearance variation, distractors, and high sparsity of point clouds. Notably, in autonomous driving scenarios, the target object typically maintains spatial adjacency across consecutive frames, predominantly moving horizontally. This spatial continuity offers valuable prior knowledge for target localization. However, existing trackers, which often employ point-wise representations, struggle to efficiently utilize this knowledge owing to the irregular format of such representations. Consequently, they require elaborate designs and solving multiple subtasks to establish spatial correspondence. In this paper, we introduce BEVTrack, a simple yet strong baseline framework for 3D SOT. After converting consecutive point clouds into the common Bird's-Eye-View representation, BEVTrack inherently encodes spatial proximity and adeptly captures motion cues for tracking via a simple element-wise operation and con
    
[^41]: TensorBank: 基于Tensor的湖仓库用于基础模型训练

    TensorBank:Tensor Lakehouse for Foundation Model Training. (arXiv:2309.02094v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2309.02094](http://arxiv.org/abs/2309.02094)

    TensorBank是一个基于Tensor的湖仓库，能够以高速从云对象存储流式传输张量到GPU内存，并通过使用分层统计指标进行查询加速。

    

    随着基础模型在自然语言之外的领域的兴起，存储和流式处理高维数据成为基础模型训练的关键需求。在本文中，我们介绍了TensorBank，一个能够基于复杂关系查询从云对象存储（COS）流式传输张量到GPU内存的百亿级张量湖仓库。我们使用分层统计指标（HSI）来加速查询。我们的架构允许使用HTTP范围读取来直接访问块级别的张量。一旦在GPU内存中，数据可以使用PyTorch转换进行转换。我们提供了一个通用的PyTorch数据集类型，配有相应的数据集工厂，用于将关系查询和请求的转换作为一个实例进行翻译。通过使用HSI，可以跳过不相关的块，而无需读取它们，因为这些索引包含不同层次分辨率级别上内容的统计信息。这是一个基于开放标准的有主观观点的架构。

    Storing and streaming high dimensional data for foundation model training became a critical requirement with the rise of foundation models beyond natural language. In this paper we introduce TensorBank, a petabyte scale tensor lakehouse capable of streaming tensors from Cloud Object Store (COS) to GPU memory at wire speed based on complex relational queries. We use Hierarchical Statistical Indices (HSI) for query acceleration. Our architecture allows to directly address tensors on block level using HTTP range reads. Once in GPU memory, data can be transformed using PyTorch transforms. We provide a generic PyTorch dataset type with a corresponding dataset factory translating relational queries and requested transformations as an instance. By making use of the HSI, irrelevant blocks can be skipped without reading them as those indices contain statistics on their content at different hierarchical resolution levels. This is an opinionated architecture powered by open standards and making h
    
[^42]: 关于后续GNN解释器对标签噪声的鲁棒性的研究

    On the Robustness of Post-hoc GNN Explainers to Label Noise. (arXiv:2309.01706v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2309.01706](http://arxiv.org/abs/2309.01706)

    本文对后续GNN解释器在面对标签噪声时的鲁棒性进行了研究，并发现即使是轻微的标签噪声也会严重影响解释质量。

    

    后续GNN解释器被提出作为解决图神经网络(GNNs)固有黑盒限制的方案，旨在提供对训练后的GNN表现行为的精确和深刻的解释。尽管在学术和工业环境中最近有显著进展，但后续GNN解释器在面对标签噪声时的鲁棒性尚未被探索。为了填补这一空白，我们进行了系统的实证研究，评估了不同程度标签噪声下各种后续GNN解释器的功效。我们的结果揭示了几个关键见解：首先，后续GNN解释器容易受到标签扰动的影响。其次，即使是对GNN表现无关紧要的轻微标签噪声，也会严重损害生成的解释质量。最后，我们就随着噪声水平的升高逐渐恢复解释效果展开讨论。

    Proposed as a solution to the inherent black-box limitations of graph neural networks (GNNs), post-hoc GNN explainers aim to provide precise and insightful explanations of the behaviours exhibited by trained GNNs. Despite their recent notable advancements in academic and industrial contexts, the robustness of post-hoc GNN explainers remains unexplored when confronted with label noise. To bridge this gap, we conduct a systematic empirical investigation to evaluate the efficacy of diverse post-hoc GNN explainers under varying degrees of label noise. Our results reveal several key insights: Firstly, post-hoc GNN explainers are susceptible to label perturbations. Secondly, even minor levels of label noise, inconsequential to GNN performance, harm the quality of generated explanations substantially. Lastly, we engage in a discourse regarding the progressive recovery of explanation effectiveness with escalating noise levels.
    
[^43]: 图神经网络中的隐私调查：攻击、保护和应用

    A Survey on Privacy in Graph Neural Networks: Attacks, Preservation, and Applications. (arXiv:2308.16375v1 [cs.LG])

    [http://arxiv.org/abs/2308.16375](http://arxiv.org/abs/2308.16375)

    这篇综述调查了图神经网络中的隐私问题，包括攻击、保护方法以及应用领域。研究人员着重总结了攻击类型、隐私保护技术分类以及可用于分析和解决GNNs中隐私问题的数据集和应用，同时提出了未来研究的方向，以构建更好的隐私保护GNNs。

    

    随着处理图结构数据的能力和实际应用的改善，图神经网络（GNNs）引起了人们的极大关注。然而，许多这些模型优先考虑高效能表现，如准确性，而缺乏隐私考虑，这是现代社会隐私攻击盛行的重要问题。为了解决这个问题，研究人员开始开发保护隐私的GNNs。尽管取得了进展，但在图领域缺乏对攻击和隐私保护技术的综合概述。在本调查中，我们旨在通过总结针对图数据的攻击、对GNNs中的隐私保护技术进行分类以及审查可用于分析/解决GNNs中隐私问题的数据集和应用程序，填补这一空白。我们还概述了未来研究的潜在方向，以建立更好的隐私保护GNNs。

    Graph Neural Networks (GNNs) have gained significant attention owing to their ability to handle graph-structured data and the improvement in practical applications. However, many of these models prioritize high utility performance, such as accuracy, with a lack of privacy consideration, which is a major concern in modern society where privacy attacks are rampant. To address this issue, researchers have started to develop privacy-preserving GNNs. Despite this progress, there is a lack of a comprehensive overview of the attacks and the techniques for preserving privacy in the graph domain. In this survey, we aim to address this gap by summarizing the attacks on graph data according to the targeted information, categorizing the privacy preservation techniques in GNNs, and reviewing the datasets and applications that could be used for analyzing/solving privacy issues in GNNs. We also outline potential directions for future research in order to build better privacy-preserving GNNs.
    
[^44]: 什么时候编程思维对推理起作用?

    When Do Program-of-Thoughts Work for Reasoning?. (arXiv:2308.15452v1 [cs.CL])

    [http://arxiv.org/abs/2308.15452](http://arxiv.org/abs/2308.15452)

    提出了复杂性影响推理分数（CIRS）来衡量编程语言对推理能力的影响，发现并非所有复杂性的代码数据都可以被学习或理解，适当的复杂性水平对于改善推理能力至关重要。

    

    大型语言模型（LLM）的推理能力在体现出人工智能领域中起着关键作用。尽管像编程思维提示这样的方法对于使用编程语言来解决复杂推理任务的LLM非常有效，但代码数据对推理能力的具体影响仍未充分探索。为了填补这一空白，我们提出了复杂性影响推理分数（CIRS），它结合了结构和逻辑属性，以衡量代码和推理能力之间的相关性。具体而言，我们使用抽象语法树来编码结构信息，并通过考虑难度和圈复杂度来计算逻辑复杂性。通过实证分析，我们发现并非所有复杂性的代码数据都可以被LLM学习或理解。最佳复杂性水平对于通过编程辅助提示改善推理能力至关重要。然后我们设计了一个自动合成的方法...

    The reasoning capabilities of Large Language Models (LLMs) play a pivotal role in the realm of embodied artificial intelligence. Although there are effective methods like program-of-thought prompting for LLMs which uses programming language to tackle complex reasoning tasks, the specific impact of code data on the improvement of reasoning capabilities remains under-explored. To address this gap, we propose complexity-impacted reasoning score (CIRS), which combines structural and logical attributes, to measure the correlation between code and reasoning abilities. Specifically, we use the abstract syntax tree to encode the structural information and calculate logical complexity by considering the difficulty and the cyclomatic complexity. Through an empirical analysis, we find not all code data of complexity can be learned or understood by LLMs. Optimal level of complexity is critical to the improvement of reasoning abilities by program-aided prompting. Then we design an auto-synthesizing
    
[^45]: 一个用于完全无监督异常检测的通用机器学习框架，适用于污染数据

    A Generic Machine Learning Framework for Fully-Unsupervised Anomaly Detection with Contaminated Data. (arXiv:2308.13352v1 [cs.LG])

    [http://arxiv.org/abs/2308.13352](http://arxiv.org/abs/2308.13352)

    这篇论文介绍了一个完全无监督的机器学习框架，用于处理在训练数据中含有异常样本的异常检测任务。

    

    在各个领域和应用中，机器学习算法已经解决了异常检测（AD）任务。这些算法中大多数使用正常数据对一个基于残差的模型进行训练，并根据未见样本与学习到的正常范围的不相似性来分配异常分数。这些方法的基本假设是可以用无异常的数据进行训练。然而，在真实世界中的操作环境中，训练数据通常会与一定比例的异常样本混合。而利用污染数据进行训练必然会导致基于残差的算法的AD性能下降。本文介绍了一个完全无监督的用于AD任务的污染训练数据的改进框架。该框架是通用的，可应用于任何基于残差的机器学习模型。我们展示了该框架在两个多元时间数据集上的应用。

    Anomaly detection (AD) tasks have been solved using machine learning algorithms in various domains and applications. The great majority of these algorithms use normal data to train a residual-based model, and assign anomaly scores to unseen samples based on their dissimilarity with the learned normal regime. The underlying assumption of these approaches is that anomaly-free data is available for training. This is, however, often not the case in real-world operational settings, where the training data may be contaminated with a certain fraction of abnormal samples. Training with contaminated data, in turn, inevitably leads to a deteriorated AD performance of the residual-based algorithms.  In this paper we introduce a framework for a fully unsupervised refinement of contaminated training data for AD tasks. The framework is generic and can be applied to any residual-based machine learning model. We demonstrate the application of the framework to two public datasets of multivariate time s
    
[^46]: CyberForce: 一个用于恶意软件缓解的联邦强化学习框架

    CyberForce: A Federated Reinforcement Learning Framework for Malware Mitigation. (arXiv:2308.05978v1 [cs.CR])

    [http://arxiv.org/abs/2308.05978](http://arxiv.org/abs/2308.05978)

    CyberForce是一个联邦强化学习框架，用于在物联网设备中协同私密地确定适合缓解各种零日攻击的MTD技术。它整合了设备指纹识别和异常检测，并通过奖励或惩罚FRL agent选择的MTD机制来提高网络安全性。

    

    互联网物联网(IoT)范例的扩展是不可避免的，但是对于IoT设备对恶意软件事件的脆弱性已成为一个越来越关注的问题。最近的研究显示，将强化学习与移动目标防御(MTD)机制相结合，可以增强IoT设备的网络安全性。然而，大量的新恶意软件攻击和代理人学习和选择有效的MTD技术所需的时间使得这种方法在现实世界的IoT场景中不切实际。为解决这个问题，本研究提出了CyberForce，一个采用联邦强化学习(FRL)的框架，用于集体且保密地确定适合缓解各种零日攻击的MTD技术。CyberForce结合了设备指纹识别和异常检测，通过奖励或惩罚FRL agent选择的MTD机制。该框架在一个由十台真实IoT平台设备组成的联邦中进行了评估。通过六个恶意软件样本进行了一系列实验。

    The expansion of the Internet-of-Things (IoT) paradigm is inevitable, but vulnerabilities of IoT devices to malware incidents have become an increasing concern. Recent research has shown that the integration of Reinforcement Learning with Moving Target Defense (MTD) mechanisms can enhance cybersecurity in IoT devices. Nevertheless, the numerous new malware attacks and the time that agents take to learn and select effective MTD techniques make this approach impractical for real-world IoT scenarios. To tackle this issue, this work presents CyberForce, a framework that employs Federated Reinforcement Learning (FRL) to collectively and privately determine suitable MTD techniques for mitigating diverse zero-day attacks. CyberForce integrates device fingerprinting and anomaly detection to reward or penalize MTD mechanisms chosen by an FRL-based agent. The framework has been evaluated in a federation consisting of ten devices of a real IoT platform. A pool of experiments with six malware samp
    
[^47]: AIs的发展脱靴法

    Developmental Bootstrapping of AIs. (arXiv:2308.04586v1 [cs.AI])

    [http://arxiv.org/abs/2308.04586](http://arxiv.org/abs/2308.04586)

    传统的符号AI方法和深度学习AI方法无法满足创建强大和可信赖的AI的挑战，然而，发展脱靴法通过模仿人类儿童的能力发展过程，为创建稳健可靠的AI提供了希望。

    

    尽管当前一些AI在封闭的世界，如棋盘游戏中超越了人类能力，但它们在混乱的现实世界中的表现有限。它们会犯奇怪的错误而且没有意识到。它们很难受到指导，不能运用常识，缺乏好奇心。它们不能成为良好的合作者。传统手动构建的符号AI方法构建的系统和使用生成和深度学习AI方法(包括大规模语言模型)构建的系统都无法应对这些挑战。它们不适合创建强大和可信赖的AI。尽管此方法不属于主流的AI方法，但发展脱靴法显示出希望。在发展脱靴法中，AI像人类儿童一样发展能力。它们从先天能力开始。像人类一样，它们与环境互动，并从互动中学习。它们通过自我发展的能力逐步扩展先天能力。它们互动并逐渐将所学应用于实际操作。

    Although some current AIs surpass human abilities especially in closed worlds such as board games, their performance in the messy real world is limited. They make strange mistakes and do not notice them. They cannot be instructed easily, fail to use common sense, and lack curiosity. They do not make good collaborators. Neither systems built using the traditional manually-constructed symbolic AI approach nor systems built using generative and deep learning AI approaches including large language models (LLMs) can meet the challenges. They are not well suited for creating robust and trustworthy AIs. Although it is outside of mainstream AI approaches, developmental bootstrapping shows promise. In developmental bootstrapping, AIs develop competences like human children do. They start with innate competences. Like humans, they interact with the environment and learn from their interactions. They incrementally extend their innate competences with self-developed competences. They interact and 
    
[^48]: 准确的神经网络剪枝需要重新思考稀疏优化

    Accurate Neural Network Pruning Requires Rethinking Sparse Optimization. (arXiv:2308.02060v1 [cs.LG])

    [http://arxiv.org/abs/2308.02060](http://arxiv.org/abs/2308.02060)

    这项工作研究了高稀疏对神经网络训练的影响，发现使用传统的密集训练策略进行稀疏训练效果不佳，提出了新的方法来解决这个问题，并在视觉和语言模型上都取得了最先进的结果。

    

    在模型压缩领域，获得既高精确又高稀疏的深度神经网络版本是一个主要挑战，社区已经对几种高性能的剪枝技术进行了研究。然而，我们对稀疏性和用于训练稀疏网络的标准随机优化技术的交互了解较少，大多数现有工作使用标准的密集训练计划和超参数来训练稀疏网络。在这项工作中，我们通过使用标准的计算机视觉和自然语言处理稀疏基准来研究高稀疏对模型训练的影响。我们首先展示了使用标准的密集训练策略进行稀疏训练是次优的，导致欠训练。我们提供了新的方法来解决这个问题，既可以用于视觉模型（如ResNet50/ImageNet）的稀疏预训练，也可以用于语言模型（如BERT/GLUE）的稀疏微调，实现了最先进的结果。

    Obtaining versions of deep neural networks that are both highly-accurate and highly-sparse is one of the main challenges in the area of model compression, and several high-performance pruning techniques have been investigated by the community. Yet, much less is known about the interaction between sparsity and the standard stochastic optimization techniques used for training sparse networks, and most existing work uses standard dense schedules and hyperparameters for training sparse networks. In this work, we examine the impact of high sparsity on model training using the standard computer vision and natural language processing sparsity benchmarks. We begin by showing that using standard dense training recipes for sparse training is suboptimal, and results in under-training. We provide new approaches for mitigating this issue for both sparse pre-training of vision models (e.g. ResNet50/ImageNet) and sparse fine-tuning of language models (e.g. BERT/GLUE), achieving state-of-the-art resul
    
[^49]: 异构联邦学习：现状与研究挑战的技术综述

    Heterogeneous Federated Learning: State-of-the-art and Research Challenges. (arXiv:2307.10616v1 [cs.LG])

    [http://arxiv.org/abs/2307.10616](http://arxiv.org/abs/2307.10616)

    异构联邦学习是联邦学习领域中的一个重要研究方向，涉及到数据分布、模型架构、网络环境和硬件设备的异质性挑战。本文对异构联邦学习的研究挑战和最新进展进行了综述和分类，为进一步的研究提供了参考。

    

    联邦学习 (FL) 由于在大规模工业应用中的潜在用途而受到越来越多的关注。现有的联邦学习研究主要针对模型同质的情况。然而，实际的联邦学习通常面临参与方之间的数据分布、模型架构、网络环境和硬件设备的异质性。异构联邦学习 (HFL) 更具挑战性，相应的解决方案多样且复杂。因此，对这个主题进行关于研究挑战和最新进展的系统调查至关重要。在这项调查中，我们首先总结了 HFL 中来自五个方面的各种研究挑战：统计异质性、模型异质性、通信异质性、设备异质性和额外挑战。此外，我们回顾了 HFL 中的最新进展，并提出了对现有 HFL 方法的新分类法，并对其优缺点进行了深入分析。

    Federated learning (FL) has drawn increasing attention owing to its potential use in large-scale industrial applications. Existing federated learning works mainly focus on model homogeneous settings. However, practical federated learning typically faces the heterogeneity of data distributions, model architectures, network environments, and hardware devices among participant clients. Heterogeneous Federated Learning (HFL) is much more challenging, and corresponding solutions are diverse and complex. Therefore, a systematic survey on this topic about the research challenges and state-of-the-art is essential. In this survey, we firstly summarize the various research challenges in HFL from five aspects: statistical heterogeneity, model heterogeneity, communication heterogeneity, device heterogeneity, and additional challenges. In addition, recent advances in HFL are reviewed and a new taxonomy of existing HFL methods is proposed with an in-depth analysis of their pros and cons. We classify
    
[^50]: EENED：基于卷积变压器的端到端神经元癫痫检测

    EENED: End-to-End Neural Epilepsy Detection based on Convolutional Transformer. (arXiv:2305.10502v1 [eess.SP])

    [http://arxiv.org/abs/2305.10502](http://arxiv.org/abs/2305.10502)

    本文提出了一种名为EENED的端到端神经元癫痫检测模型，结合了CNN和Transformer，能够同时捕捉EEG信号的全局依赖性和局部特征，并有望提高癫痫检测的准确性和可靠性。

    

    最近，在EEG信号处理中，基于变压器（Transformer）和卷积神经网络（CNN）的模型表现出了很好的结果。变压器模型可以通过自我注意机制捕捉EEG信号的全局依赖性，而CNN模型可以捕捉如锯齿波之类的局部特征。在本文中，我们提出了一种名为EENED的端到端神经元癫痫检测模型，它结合了CNN和Transformer。具体地，通过在Transformer编码器中引入卷积模块，EENED可以学习患者EEG信号特征的时间依赖关系，并注意到与癫痫密切相关的局部EEG异常突变，如尖锐波的出现和缓慢波的散布。我们提出的框架结合了Transformer和CNN捕捉EEG信号不同尺度的特征的能力，有望提高癫痫检测的准确性和可靠性。我们的源代码将很快在GitHub上发布。

    Recently Transformer and Convolution neural network (CNN) based models have shown promising results in EEG signal processing. Transformer models can capture the global dependencies in EEG signals through a self-attention mechanism, while CNN models can capture local features such as sawtooth waves. In this work, we propose an end-to-end neural epilepsy detection model, EENED, that combines CNN and Transformer. Specifically, by introducing the convolution module into the Transformer encoder, EENED can learn the time-dependent relationship of the patient's EEG signal features and notice local EEG abnormal mutations closely related to epilepsy, such as the appearance of spikes and the sprinkling of sharp and slow waves. Our proposed framework combines the ability of Transformer and CNN to capture different scale features of EEG signals and holds promise for improving the accuracy and reliability of epilepsy detection. Our source code will be released soon on GitHub.
    
[^51]: 音乐混音工作流中的人工智能技术应用调查研究

    Adoption of AI Technology in the Music Mixing Workflow: An Investigation. (arXiv:2304.03407v1 [cs.HC])

    [http://arxiv.org/abs/2304.03407](http://arxiv.org/abs/2304.03407)

    该研究调查了音乐混音工作流中AI技术的应用情况及不同用户群体的采用情况。结果显示，即使AI混音工具能够为业余爱好者提供不错的结果，职业业余爱好者和专业人员需要更为精确的控制和自定义选项，以及辅助和协作技术。

    

    人工智能技术在音乐行业中的应用正在推动音乐创作、制作和混音的重大变化。本研究调查了人工智能在混音工作流程中的当前状态以及不同用户群体对其采纳情况。通过半结构化访谈、基于问卷的研究和分析网络论坛，本研究确认了包括业余爱好者、职业业余爱好者和专业人士在内的三个用户群体。研究结果表明，虽然AI混音工具能够简化过程并为业余爱好者提供不错的结果，职业业余爱好者需要精确的控制和自定义选项，而专业人员除了需要控制和自定义选项外，还需要辅助和协作技术。本研究为设计针对不同用户群体的有效的AI混音工具提供了策略，并概述了未来的方向。

    The integration of artificial intelligence (AI) technology in the music industry is driving a significant change in the way music is being composed, produced and mixed. This study investigates the current state of AI in the mixing workflows and its adoption by different user groups. Through semi-structured interviews, a questionnaire-based study, and analyzing web forums, the study confirms three user groups comprising amateurs, pro-ams, and professionals. Our findings show that while AI mixing tools can simplify the process and provide decent results for amateurs, pro-ams seek precise control and customization options, while professionals desire control and customization options in addition to assistive and collaborative technologies. The study provides strategies for designing effective AI mixing tools for different user groups and outlines future directions.
    
[^52]: NeTO: 透明物体的神经重建与自遮挡感知折射追踪

    NeTO:Neural Reconstruction of Transparent Objects with Self-Occlusion Aware Refraction-Tracing. (arXiv:2303.11219v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2303.11219](http://arxiv.org/abs/2303.11219)

    提出了一种名为NeTO的方法，通过体渲染从2D图像中捕捉固体透明物体的3D几何体。通过采用隐式有符号距离函数（SDF）作为表面表示，并通过自遮挡感知的折射追踪通过体渲染来优化SDF场，可以实现高质量的重建结果。

    

    我们提出了一种新颖的方法，称为NeTO，通过体渲染从2D图像中捕捉固体透明物体的3D几何体。透明物体的重建是一项非常具有挑战性的任务，不适合通用的重建技术，因为其困扰于镜面光传输现象。虽然现有的基于折射追踪的方法在这个任务上取得了令人印象深刻的结果，但它们仍然存在优化不稳定和细节缺失的问题，因为它们采用的显式表面表示难以优化，且忽略了自遮挡问题。在本文中，我们提出利用隐式有符号距离函数（SDF）作为表面表示，并通过自遮挡感知的折射追踪通过体渲染来优化SDF场。隐式表示使得我们的方法能够在有限的图像集合下重建出高质量的重建结果。

    We present a novel method, called NeTO, for capturing 3D geometry of solid transparent objects from 2D images via volume rendering. Reconstructing transparent objects is a very challenging task, which is ill-suited for general-purpose reconstruction techniques due to the specular light transport phenomena. Although existing refraction-tracing based methods, designed specially for this task, achieve impressive results, they still suffer from unstable optimization and loss of fine details, since the explicit surface representation they adopted is difficult to be optimized, and the self-occlusion problem is ignored for refraction-tracing. In this paper, we propose to leverage implicit Signed Distance Function (SDF) as surface representation, and optimize the SDF field via volume rendering with a self-occlusion aware refractive ray tracing. The implicit representation enables our method to be capable of reconstructing high-quality reconstruction even with a limited set of images, and the s
    
[^53]: 受限强化学习在可信四旋翼无人机跟踪控制中的应用

    Constrained Reinforcement Learning using Distributional Representation for Trustworthy Quadrotor UAV Tracking Control. (arXiv:2302.11694v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2302.11694](http://arxiv.org/abs/2302.11694)

    提出了一种使用分布式表示进行受限强化学习的方法，用于可信四旋翼无人机的跟踪控制。通过集成分布式强化学习干扰估计器和随机模型预测控制器，能够准确识别气动效应的不确定性，实现最优的全局收敛速率和一定的亚线性收敛速率。

    

    在复杂的动态环境中，同时实现四旋翼无人机的准确和可靠的跟踪控制是具有挑战性的。由于来自气动力的阻力和力矩变化是混沌的，并且难以精确识别，大多数现有的四旋翼跟踪系统将其视为传统控制方法中的简单“干扰”。本文提出了一种新颖的、可解释的轨迹跟踪器，将分布式强化学习干扰估计器与随机模型预测控制器（SMPC）相结合，用于未知的气动效应。所提出的估计器“受限分布式强化干扰估计器”（ConsDRED）准确地识别真实气动效应与估计值之间的不确定性。采用简化仿射干扰反馈进行控制参数化，以保证凸性，然后将其与SMPC相结合。我们在理论上保证ConsDRED至少实现最优的全局收敛速率和一定的亚线性收敛速率。

    Simultaneously accurate and reliable tracking control for quadrotors in complex dynamic environments is challenging. As aerodynamics derived from drag forces and moment variations are chaotic and difficult to precisely identify, most current quadrotor tracking systems treat them as simple `disturbances' in conventional control approaches. We propose a novel, interpretable trajectory tracker integrating a Distributional Reinforcement Learning disturbance estimator for unknown aerodynamic effects with a Stochastic Model Predictive Controller (SMPC). The proposed estimator `Constrained Distributional Reinforced disturbance estimator' (ConsDRED) accurately identifies uncertainties between true and estimated values of aerodynamic effects. Simplified Affine Disturbance Feedback is used for control parameterization to guarantee convexity, which we then integrate with a SMPC. We theoretically guarantee that ConsDRED achieves at least an optimal global convergence rate and a certain sublinear r
    
[^54]: Fairguard: 在智慧城市中利用基于逻辑的公正规则

    Fairguard: Harness Logic-based Fairness Rules in Smart Cities. (arXiv:2302.11137v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2302.11137](http://arxiv.org/abs/2302.11137)

    本文提出了一种基于时间逻辑的公正智慧城市政策调整和生成方法Fairguard，通过两个阶段的静态生成和动态调节，缓解由多种数据和算法偏见导致的不公正预测结果。

    

    智慧城市运行在计算预测框架上，收集、整合和利用大规模传感器网络的数据。然而，这些框架容易受到多种数据和算法偏见的影响，这经常导致不公正的预测结果。本文首先通过研究田纳西州查塔努加的真实城市数据，展示了偏见在微观层面上在时间和空间上仍然存在。为了缓解这种偏见问题，我们引入了Fairguard，这是一种基于微观层面时间逻辑的方法，用于在复杂的时间空间域中进行公正的智慧城市政策调整和生成。Fairguard框架由两个阶段组成：首先，我们开发了一个静态生成器，能够通过最小化所选属性之间的相关性，基于时间逻辑条件来减少数据偏见。然后，为了确保预测算法的公正性，我们设计了一个动态组件来调节预测结果，并利用逻辑规则生成未来的公正预测。

    Smart cities operate on computational predictive frameworks that collect, aggregate, and utilize data from large-scale sensor networks. However, these frameworks are prone to multiple sources of data and algorithmic bias, which often lead to unfair prediction results. In this work, we first demonstrate that bias persists at a micro-level both temporally and spatially by studying real city data from Chattanooga, TN. To alleviate the issue of such bias, we introduce Fairguard, a micro-level temporal logic-based approach for fair smart city policy adjustment and generation in complex temporal-spatial domains. The Fairguard framework consists of two phases: first, we develop a static generator that is able to reduce data bias based on temporal logic conditions by minimizing correlations between selected attributes. Then, to ensure fairness in predictive algorithms, we design a dynamic component to regulate prediction results and generate future fair predictions by harnessing logic rules. E
    
[^55]: TikTalk: 一个用于多模态真实世界闲聊的基于视频的对话数据集

    TikTalk: A Video-Based Dialogue Dataset for Multi-Modal Chitchat in Real World. (arXiv:2301.05880v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2301.05880](http://arxiv.org/abs/2301.05880)

    TikTalk是一个基于视频的多模态对话数据集，用于研究智能且类似人类的闲聊机器人。数据集包含从流行视频分享平台收集的38K个视频和367K个用户对话。与其他数据集相比，TikTalk提供了更丰富的上下文类型，同时也增加了从复杂的多模态信息中生成个性化回答的难度。数据集中还更频繁地引用了外部知识，为多模态对话模型提供了新的挑战。

    

    为了促进多模态上下文中智能和人类化聊天机器人的研究，我们引入了一个新的基于视频的多模态对话数据集，称为TikTalk。我们从一个流行的视频分享平台收集了38K个视频，以及用户在其下发布的367K个对话。用户根据他们观看视频时的多模态经验进行自发性对话，这有助于重现真实世界的闲聊环境。与之前的多模态对话数据集相比，TikTalk中更丰富的上下文类型导致了更多样化的对话，但也增加了从复杂的多模态信息中捕捉人类兴趣并生成个性化回答的难度。此外，我们的数据集中更频繁地引用了外部知识。这些事实揭示了多模态对话模型面临的新挑战。我们定量地展示了TikTalk的特点，提出了一个基于视频的多模态闲聊任务，并评估了几种对话基线模型。

    To facilitate the research on intelligent and human-like chatbots with multi-modal context, we introduce a new video-based multi-modal dialogue dataset, called TikTalk. We collect 38K videos from a popular video-sharing platform, along with 367K conversations posted by users beneath them. Users engage in spontaneous conversations based on their multi-modal experiences from watching videos, which helps recreate real-world chitchat context. Compared to previous multi-modal dialogue datasets, the richer context types in TikTalk lead to more diverse conversations, but also increase the difficulty in capturing human interests from intricate multi-modal information to generate personalized responses. Moreover, external knowledge is more frequently evoked in our dataset. These facts reveal new challenges for multi-modal dialogue models. We quantitatively demonstrate the characteristics of TikTalk, propose a video-based multi-modal chitchat task, and evaluate several dialogue baselines. Experi
    
[^56]: 在困难的探索问题中评估质量多样性神经进化算法的性能

    Assessing Quality-Diversity Neuro-Evolution Algorithms Performance in Hard Exploration Problems. (arXiv:2211.13742v2 [cs.NE] UPDATED)

    [http://arxiv.org/abs/2211.13742](http://arxiv.org/abs/2211.13742)

    本文评估了质量多样性神经进化算法在困难探索问题中的性能，并强调了探索在解决控制问题中的重要性。

    

    自然界的一个迷人之处在于它能产生一系列在各自领域表现出色的生物体。质量多样性（QD）方法是灵感来自于这一观察的进化算法，在许多应用中取得了巨大的成果，从翅膀设计到机器人适应性。最近，多项研究表明这些方法可以应用于神经进化来解决大搜索空间中的控制问题。在这种问题中，多样性本身可以成为一个目标。多样性也可以成为增强具有欺骗性奖励信号的任务探索的一种方式。虽然QD社区已经深入研究了前者，但后者在文献中相对较少。探索是许多领域如强化学习中试图解决控制问题的核心，QD方法是克服相关挑战的有希望的候选方法。因此，我们相信标准化的基准测试在这些控制问题上是必要的。

    A fascinating aspect of nature lies in its ability to produce a collection of organisms that are all high-performing in their niche. Quality-Diversity (QD) methods are evolutionary algorithms inspired by this observation, that obtained great results in many applications, from wing design to robot adaptation. Recently, several works demonstrated that these methods could be applied to perform neuro-evolution to solve control problems in large search spaces. In such problems, diversity can be a target in itself. Diversity can also be a way to enhance exploration in tasks exhibiting deceptive reward signals. While the first aspect has been studied in depth in the QD community, the latter remains scarcer in the literature. Exploration is at the heart of several domains trying to solve control problems such as Reinforcement Learning and QD methods are promising candidates to overcome the challenges associated. Therefore, we believe that standardized benchmarks exhibiting control problems in 
    
[^57]: 利用新数据在电力网通讯中的潜力

    Leveraging the Potential of Novel Data in Power Line Communication of Electricity Grids. (arXiv:2209.12693v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.12693](http://arxiv.org/abs/2209.12693)

    本文提出了两个基于宽带电力线通信测量的首个数据集FiN-1和FiN-2，共收集了130亿个数据点。这些数据可以应用于资产管理、电网状态可视化和预测，以解决电力网在向可再生能源过渡和复杂负载配置的情况下面临的新挑战。

    

    电力网已成为日常生活中不可或缺的一部分，尽管在日常生活中往往不会注意到。只有当电力网不再可用时，我们通常才会特别意识到这种依赖。然而，重大变化，如向可再生能源（光伏、风力涡轮机等）的过渡以及复杂负载配置（电动汽车、家庭电池系统等）的能源消费者数量的增加，给电力网带来了新的挑战。为了解决这些挑战，我们提出了两个基于宽带电力线通信（PLC）基础设施测量的首个数据集。这两个数据集 FiN-1 和 FiN-2 在德国低压电网的一部分实际使用中收集，向大约440万人提供服务，并显示5100多个传感器收集的超过130亿个数据点。此外，我们提出了不同的用例，用于资产管理、电网状态可视化和预测。

    Electricity grids have become an essential part of daily life, even if they are often not noticed in everyday life. We usually only become particularly aware of this dependence by the time the electricity grid is no longer available. However, significant changes, such as the transition to renewable energy (photovoltaic, wind turbines, etc.) and an increasing number of energy consumers with complex load profiles (electric vehicles, home battery systems, etc.), pose new challenges for the electricity grid. To address these challenges, we propose two first-of-its-kind datasets based on measurements in a broadband powerline communications (PLC) infrastructure. Both datasets FiN-1 and FiN-2, were collected during real practical use in a part of the German low-voltage grid that supplies around 4.4 million people and show more than 13 billion datapoints collected by more than 5100 sensors. In addition, we present different use cases in asset management, grid state visualization, forecasting, 
    
[^58]: 具有谐振器网络的神经形态视觉场景理解

    Neuromorphic Visual Scene Understanding with Resonator Networks. (arXiv:2208.12880v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2208.12880](http://arxiv.org/abs/2208.12880)

    本论文提出了一种基于神经形态的解决方案，利用高效的因式分解网络来理解视觉场景并推断物体和姿势。关键创新包括基于复值向量的计算框架VSA、用于处理平移和旋转的分层谐振器网络HRN设计，以及在神经形态硬件上实现复值谐振器网络的多组分脉冲相位神经元模型。

    

    理解视觉场景并推断其各个物体的身份和姿势仍然是一个未解决的问题。在这里，我们提出了一种神经形态的解决方案，它利用了基于三个关键概念的高效的因式分解网络：（1）基于复值向量的矢量符号体系架构(VSA)的计算框架；（2）用于处理视觉场景中平移和旋转的非可交换性的分层谐振器网络（HRN）的设计，当两者结合使用时；（3）设计了一种多组分脉冲相位神经元模型，用于在神经形态硬件上实现复值谐振器网络。VSA框架使用矢量绑定操作来产生生成式图像模型，其中绑定作为几何变换的等变操作。因此，一个场景可以被描述为向量乘积的和，而这些向量乘积可以通过谐振器网络的因式分解来高效地推断物体和它们的姿势。

    Understanding a visual scene by inferring identities and poses of its individual objects is still and open problem. Here we propose a neuromorphic solution that utilizes an efficient factorization network based on three key concepts: (1) a computational framework based on Vector Symbolic Architectures (VSA) with complex-valued vectors; (2) the design of Hierarchical Resonator Networks (HRN) to deal with the non-commutative nature of translation and rotation in visual scenes, when both are used in combination; (3) the design of a multi-compartment spiking phasor neuron model for implementing complex-valued resonator networks on neuromorphic hardware. The VSA framework uses vector binding operations to produce generative image models in which binding acts as the equivariant operation for geometric transformations. A scene can therefore be described as a sum of vector products, which in turn can be efficiently factorized by a resonator network to infer objects and their poses. The HRN ena
    
[^59]: TREE-G:决策树对抗图神经网络

    TREE-G: Decision Trees Contesting Graph Neural Networks. (arXiv:2207.02760v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.02760](http://arxiv.org/abs/2207.02760)

    TREE-G引入了一种专门针对图数据的修改决策树方法，通过新颖的分裂函数和指针机制，使得决策树能够更好地应用于带有拓扑信息的图结构化数据。

    

    在处理表格数据时，基于决策树的模型是一个流行的选择，因为它们在这些数据类型上具有高准确性、易于应用和可解释性的特点。然而，在处理图结构化数据时，如何有效地应用决策树并将拓扑信息与图的顶点上的表格数据相结合仍不清楚。为了解决这个挑战，我们引入了TREE-G。TREE-G修改了标准决策树，引入了一种专门针对图数据的新型分裂函数。这个分裂函数不仅包括节点特征和拓扑信息，还使用了一种新颖的指针机制，允许分裂节点使用在先前分裂中计算得到的信息。因此，分裂函数能够适应预测任务和当前的图。我们对TREE-G的理论性质进行了分析，并在多个图和顶点预测基准测试上从经验上证明了它的好处。

    When dealing with tabular data, models based on decision trees are a popular choice due to their high accuracy on these data types, their ease of application, and explainability properties. However, when it comes to graph-structured data, it is not clear how to apply them effectively, in a way that incorporates the topological information with the tabular data available on the vertices of the graph. To address this challenge, we introduce TREE-G. TREE-G modifies standard decision trees, by introducing a novel split function that is specialized for graph data. Not only does this split function incorporate the node features and the topological information, but it also uses a novel pointer mechanism that allows split nodes to use information computed in previous splits. Therefore, the split function adapts to the predictive task and the graph at hand. We analyze the theoretical properties of TREE-G and demonstrate its benefits empirically on multiple graph and vertex prediction benchmarks
    
[^60]: NMA: 神经网络多槽广告拍卖带有外部性

    NMA: Neural Multi-slot Auctions with Externalities for Online Advertising. (arXiv:2205.10018v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2205.10018](http://arxiv.org/abs/2205.10018)

    NMA是一种新颖的在线广告拍卖机制，利用深度神经网络模型考虑广告展示顺序和位置对用户点击的影响，更好地模拟全局的外部性。

    

    在线广告拍卖为社交网络服务和电子商务平台带来了数十亿美元的收入。GSP拍卖机制已成为行业中广告拍卖机制的基准，但大多数基于GSP的工业实践假设用户点击仅与广告本身有关，忽视了外部性的影响。本文提出了一种新颖的拍卖机制NMA，它利用深度神经网络模型来考虑广告展示顺序和位置对用户点击的影响，从而更好地模拟全局的外部性。

    Online advertising driven by auctions brings billions of dollars in revenue for social networking services and e-commerce platforms. GSP auctions, which are simple and easy to understand for advertisers, have almost become the benchmark for ad auction mechanisms in the industry. However, most GSP-based industrial practices assume that the user click only relies on the ad itself, which overlook the effect of external items, referred to as externalities. Recently, DNA has attempted to upgrade GSP with deep neural networks and models local externalities to some extent. However, it only considers set-level contexts from auctions and ignores the order and displayed position of ads, which is still suboptimal. Although VCG-based multi-slot auctions (e.g., VCG, WVCG) make it theoretically possible to model global externalities (e.g., the order and positions of ads and so on), they lack an efficient balance of both revenue and social welfare. In this paper, we propose novel auction mechanisms n
    

