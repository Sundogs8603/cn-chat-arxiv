# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [A General-Purpose Self-Supervised Model for Computational Pathology.](http://arxiv.org/abs/2308.15474) | 本论文介绍了一个通用的自监督计算病理学模型，通过在各种组织类型上进行广泛开发和评估，为组织表型分型提供了解决方案。 |
| [^2] | [Multimodal Contrastive Learning and Tabular Attention for Automated Alzheimer's Disease Prediction.](http://arxiv.org/abs/2308.15469) | 本论文提出了一个多模态对比学习的通用框架，结合了图像数据和表格数据，设计了一种新颖的表格注意力模块，并将这些技术应用于阿尔茨海默病的预测。实验证明了该框架的优势，并展示了其在阿尔茨海默病检测方面的高准确率。 |
| [^3] | [A Comparative Study of Loss Functions: Traffic Predictions in Regular and Congestion Scenarios.](http://arxiv.org/abs/2308.15464) | 本文通过研究多种受重尾分析和不平衡分类问题启发的损失函数，提出了改进交通预测中拥堵情况的准确性的方法，并分别提出了针对不同优化目标的最佳选择。 |
| [^4] | [ParaGuide: Guided Diffusion Paraphrasers for Plug-and-Play Textual Style Transfer.](http://arxiv.org/abs/2308.15459) | ParaGuide是一种用于通用风格转移的引导性扩散改写器，可以灵活适应任意目标风格，通过梯度引导和改写条件的扩散模型实现文本的风格转变，同时保留语义信息。 |
| [^5] | [From SMOTE to Mixup for Deep Imbalanced Classification.](http://arxiv.org/abs/2308.15457) | 本研究提出了一种从SMOTE到Mixup的方法，用于深度不平衡分类。通过对SMOTE进行改进，并结合Mixup技术，我们构建了一个统一的数据增强框架。研究表明，Mixup技术通过实现多数类和少数类之间的不平衡间隙来改善泛化能力。我们还提出了一种新颖的基于边界的Mixup技术，更明确地实现了不平衡间隙。实验结果表明我们的方法在多个数据集上取得了最好的性能。 |
| [^6] | [When Do Program-of-Thoughts Work for Reasoning?.](http://arxiv.org/abs/2308.15452) | 提出了复杂性影响推理分数（CIRS）来衡量编程语言对推理能力的影响，发现并非所有复杂性的代码数据都可以被学习或理解，适当的复杂性水平对于改善推理能力至关重要。 |
| [^7] | [Complementing Onboard Sensors with Satellite Map: A New Perspective for HD Map Construction.](http://arxiv.org/abs/2308.15427) | 本研究通过补充卫星地图，增强了车载传感器构建高精度地图的方法，利用卫星地图的广阔覆盖能力。我们释放了卫星地图瓦片作为nuScenes数据集的补充，同时提出了一个分层融合模块来更好地融合车载传感器与卫星地图的信息。 |
| [^8] | [Color Aesthetics: Fuzzy based User-driven Method for Harmony and Preference Prediction.](http://arxiv.org/abs/2308.15397) | 该论文提出了一种基于模糊用户驱动的方法，用于预测协调和偏好，通过结合基本颜色的偏好和色彩协调的评分来预测对色彩方案的偏好，在各种电子商务应用中具有实用价值。 |
| [^9] | [Decentralized Multi-agent Reinforcement Learning based State-of-Charge Balancing Strategy for Distributed Energy Storage System.](http://arxiv.org/abs/2308.15394) | 本文提出了一种基于分散式多智能体强化学习的电荷平衡策略，通过分布式方法解决了分布式能量存储系统中的电荷平衡问题，并通过平均一致性算法来进行优化。 |
| [^10] | [Bayesian Integration of Information Using Top-Down Modulated WTA Networks.](http://arxiv.org/abs/2308.15390) | 本文研究了使用自上而下调节的WTA网络在贝叶斯推理中整合信息的方法，探讨了自上而下的过程在提高WTA网络性能方面的作用。 |
| [^11] | [RED: A Systematic Real-Time Scheduling Approach for Robotic Environmental Dynamics.](http://arxiv.org/abs/2308.15368) | RED是一种系统性实时调度方法，旨在支持资源有限的机器人系统中的多任务深度神经网络工作负载。它采用中间截止时间分配策略，适应性地管理机器人环境动态（RED），以满足实时约束。 |
| [^12] | [Efficient Model Personalization in Federated Learning via Client-Specific Prompt Generation.](http://arxiv.org/abs/2308.15367) | pFedPG is a novel personalized FL framework that generates client-specific prompts to adapt frozen backbones to local data distributions, enabling efficient model personalization for heterogeneous clients in FL. |
| [^13] | [Ego-Motion Estimation and Dynamic Motion Separation from 3D Point Clouds for Accumulating Data and Improving 3D Object Detection.](http://arxiv.org/abs/2308.15357) | 本研究分析了高分辨率雷达传感器在积累雷达点云和改善三维物体检测方面的局限性，并提出了自我运动估计和动态运动校正方法以提高检测性能。 |
| [^14] | [Enhancing Mobile Face Anti-Spoofing: A Robust Framework for Diverse Attack Types under Screen Flash.](http://arxiv.org/abs/2308.15346) | 本论文提出了一种在屏幕闪光下对多样攻击类型鲁棒的人脸防伪框架，通过引入多个网络和双门模块，有效减小了欺骗人脸的类内距离，并提供了更准确的防伪结果。 |
| [^15] | [AI Framework for Early Diagnosis of Coronary Artery Disease: An Integration of Borderline SMOTE, Autoencoders and Convolutional Neural Networks Approach.](http://arxiv.org/abs/2308.15339) | 本研究开发了一个方法来平衡和增强数据，以提高冠状动脉疾病的预测准确性，在数据不平衡和样本量较小的情况下取得了较高的准确率。该方法对于其他数据收集成本高和样本量小的情况也具有潜在的应用价值。 |
| [^16] | [A Framework for Responsible Development of Automated Student Feedback with Generative AI.](http://arxiv.org/abs/2308.15334) | 一种基于生成AI的自动学生反馈框架可以提供丰富的反馈，但引入了伦理问题，并需要解决“多数人的暴政”和忽视长尾中少数群体需求的挑战。 |
| [^17] | [FedLogic: Interpretable Federated Multi-Domain Chain-of-Thought Prompt Selection for Large Language Models.](http://arxiv.org/abs/2308.15324) | 本文提出了一种名为FedLogic的方法，用于解决大型语言模型在多领域思维链选择中的可解释性和平衡性问题。 |
| [^18] | [Elucidating the Exposure Bias in Diffusion Models.](http://arxiv.org/abs/2308.15321) | 本文系统地研究了扩散模型中的曝光偏差问题，并提出了一种名为Epsilon Scaling的免训练方法来减轻这一问题。实验结果验证了该方法的有效性。 |
| [^19] | [On-Device Learning with Binary Neural Networks.](http://arxiv.org/abs/2308.15308) | 本文提出了一种在低功耗设备上使用二进制神经网络进行设备端学习的解决方案，该方法结合了最新的连续学习技术和二进制神经网络的高效性。实验证实了该方法的有效性和适用性。 |
| [^20] | [KGConv, a Conversational Corpus grounded in Wikidata.](http://arxiv.org/abs/2308.15298) | KGConv是一个基于Wikidata的大型对话语料库，每个对话都基于一个事实，并提供了多个变体的问题。它可以用于知识对话问题生成和其他相关任务。 |
| [^21] | [A Hybrid Membership Latent Distance Model for Unsigned and Signed Integer Weighted Networks.](http://arxiv.org/abs/2308.15293) | 本文提出了一种混合成员潜在距离模型（HM-LDM）和有符号混合成员-潜在距离模型（sHM-LDM），通过控制潜在空间的体积，揭示了网络中的社区结构，并引导节点之间的链接关系。 |
| [^22] | [Empowering LLM to use Smartphone for Intelligent Task Automation.](http://arxiv.org/abs/2308.15272) | 本论文提出了AutoDroid，一个移动任务自动化系统，可以在任何Android应用程序上自动处理任意任务。它通过结合LLMs的常识知识和应用的领域特定知识来实现，通过自动化的动态分析来实现功能意识的UI表示方法和基于探索的内存注入技术。 |
| [^23] | [Let There Be Sound: Reconstructing High Quality Speech from Silent Videos.](http://arxiv.org/abs/2308.15256) | 本文介绍了一个重建高质量语音的唇语转语音系统，通过解决一对多映射问题和细节精炼来显著改进生成质量。 |
| [^24] | [Knowledge-based Multiple Adaptive Spaces Fusion for Recommendation.](http://arxiv.org/abs/2308.15244) | 该论文提出了一种基于知识的多重自适应空间融合推荐方法，通过统一的空间来融合双曲、欧几里得和球面空间，并利用注意力机制提高了知识传播的嵌入质量。 |
| [^25] | [Natural language to SQL in low-code platforms.](http://arxiv.org/abs/2308.15239) | 本研究提出了一种自然语言转SQL的管道，允许低代码平台开发人员使用自然语言检索数据库中的数据。通过收集和标注大量数据，训练了一个NL模型来生成SQL，并使用反馈循环不断优化模型。通过A/B测试，观察到特性采用率提高了240%，参与率提高了220%。 |
| [^26] | [PronounFlow: A Hybrid Approach for Calibrating Pronouns in Sentences.](http://arxiv.org/abs/2308.15235) | PronounFlow提出了一种混合方法来校准句子中的代词，以消除歧义。这对于使机器具备常识和推理能力具有重要意义。 |
| [^27] | [Providing Previously Unseen Users Fair Recommendations Using Variational Autoencoders.](http://arxiv.org/abs/2308.15230) | 本论文提出了一种使用变分自动编码器的新方法，通过限制人口统计信息的编码来减少推荐系统中的歧视，从而为以前未出现的用户提供公平推荐。 |
| [^28] | [CLIPTrans: Transferring Visual Knowledge with Pre-trained Models for Multimodal Machine Translation.](http://arxiv.org/abs/2308.15226) | 本研究提出了CLIPTrans，它通过简单地适应独立预训练模型，实现了多模态机器翻译中视觉知识的转移。 |
| [^29] | [From DDMs to DNNs: Using process data and models of decision-making to improve human-AI interactions.](http://arxiv.org/abs/2308.15225) | 本论文提出利用决策过程数据和模型改善人工智能与人类之间的交互。通过详细描述决策过程和建立决策演变模型，可以揭示潜在的偏好，同时追踪决策过程的数据可以提供重要信息，从而改善人工智能的预测能力。 |
| [^30] | [FurChat: An Embodied Conversational Agent using LLMs, Combining Open and Closed-Domain Dialogue with Facial Expressions.](http://arxiv.org/abs/2308.15214) | 本研究开发了一个交互式对话系统，将开放和封闭领域对话、脸部表情结合起来，通过使用LLMs和GPT-3.5模型来生成引人入胜的对话，以提供信息并与访客进行自然交流。 |
| [^31] | [Shared Lexical Items as Triggers of Code Switching.](http://arxiv.org/abs/2308.15209) | 该研究通过使用三种语言对的五个大型数据集，对共享词汇作为代码切换的触发器进行了深入探索。结果表明，共享词汇确实会触发代码切换，切换的倾向取决于触发器与切换点的距离和位置，而不取决于触发词的词源。 |
| [^32] | [Where Would I Go Next? Large Language Models as Human Mobility Predictors.](http://arxiv.org/abs/2308.15197) | 本文研究了大型语言模型在人类移动预测任务中的潜力，提出了一种新方法LLM-Mob，通过利用语言模型的语言理解和推理能力分析人类移动数据，并引入历史停留和上下文停留的概念来捕捉长期和短期依赖关系，实现时态感知预测。 |
| [^33] | [Ensemble of Counterfactual Explainers.](http://arxiv.org/abs/2308.15194) | 该论文提出了一种合奏反事实解释器，可以提升弱解释器的性能，实现对反事实实例的最小化、可操作性、稳定性、多样性、合理性和辨别力的全覆盖。 |
| [^34] | [Enhancing Psychological Counseling with Large Language Model: A Multifaceted Decision-Support System for Non-Professionals.](http://arxiv.org/abs/2308.15192) | 本研究基于大型语言模型构建了一个决策支持系统，可帮助非专业人员在社交媒体上提供心理咨询，从而弥补了专业咨询师短缺的问题。 |
| [^35] | [LTLf Best-Effort Synthesis in Nondeterministic Planning Domains.](http://arxiv.org/abs/2308.15188) | 本研究在完全可观察的非确定性规划领域中，使用线性时态逻辑在有限轨迹上表达目标的最佳努力策略。我们引入了游戏论技术来综合这些策略，该技术利用了非确定性规划领域的特征。实验证明，这种方法比基于重新表达规划领域的直接方法具有更高的可扩展性。 |
| [^36] | [LTLf Synthesis Under Environment Specifications for Reachability and Safety Properties.](http://arxiv.org/abs/2308.15184) | 本文研究了LTLf综合在可达性和安全性属性的环境规范下的问题，并提供了完整的综合算法，其中一些情况是首次研究的。 |
| [^37] | [Symbolic LTLf Best-Effort Synthesis.](http://arxiv.org/abs/2308.15178) | 本文提出并比较了用于有限轨迹线性时态逻辑（LTLf）的最大努力合成的不同符号化方法，这些方法在组件的组合方式上有所不同，对方法的性能有重要影响。 |
| [^38] | [A lightweight 3D dense facial landmark estimation model from position map data.](http://arxiv.org/abs/2308.15170) | 提出了一种基于位置地图数据的轻量级3D密集面部关键点估计模型，通过预测整个脸部的密集3D关键点来解决无需3D数据的面部分析任务中的复杂问题。 |
| [^39] | [Ontologies in Digital Twins: A Systematic Literature Review.](http://arxiv.org/abs/2308.15168) | 本文是一项对数字孪生中本体论的系统性文献综述，通过分析82篇研究论文的途径，探讨了本体论在数字孪生中的应用和贡献，包括知识表示、互操作性和自动推理方面。 |
| [^40] | [Lifelike Agility and Play on Quadrupedal Robots using Reinforcement Learning and Generative Pre-trained Models.](http://arxiv.org/abs/2308.15143) | 该论文提出了一种使用生成模型和强化学习的框架，使四足机器人能够在复杂环境中像真实动物一样具有灵活性和策略。通过预训练生成模型，保留了动物行为的知识，并通过学习适应环境，克服挑战性的障碍。 |
| [^41] | [A Multimodal Visual Encoding Model Aided by Introducing Verbal Semantic Information.](http://arxiv.org/abs/2308.15142) | 本文提出了一种多模态视觉信息编码网络模型，通过整合文字语义信息和图像特征，实现了对视觉信息的编码。该模型通过Transformer网络对图像和文本特征进行对齐，构建了一个多模态特征空间，并通过卷积网络将特征映射到体素空间，从而实现了多模态视觉信息编码。 |
| [^42] | [Abdominal Multi-Organ Segmentation Based on Feature Pyramid Network and Spatial Recurrent Neural Network.](http://arxiv.org/abs/2308.15137) | 这项研究提出了一种新的腹部多器官分割模型，结合了特征金字塔网络（FPN）和空间循环神经网络（SRNN），以加速超声图像分割并减轻超声检查师的负担。 |
| [^43] | [Evaluation and Analysis of Hallucination in Large Vision-Language Models.](http://arxiv.org/abs/2308.15126) | 本文提出了基于大型语言模型的幻觉评估框架HaELM，可以评估大型视觉语言模型中的幻觉问题，并分析了导致幻觉的因素，并提出了缓解幻觉问题的建议。 |
| [^44] | [AI-Based Facial Emotion Recognition Solutions for Education: A Study of Teacher-User and Other Categories.](http://arxiv.org/abs/2308.15119) | 本文提出了基于人工智能的面部情绪识别教育解决方案，将教师用户分为定向、状况和偏好三类，并整理了FER解决方案的技术和应用两类，对教育领域具有重要意义。 |
| [^45] | [Mixup-Augmented Meta-Learning for Sample-Efficient Fine-Tuning of Protein Simulators.](http://arxiv.org/abs/2308.15116) | 本文通过Mixup增强的元学习方法实现了对蛋白质模拟器的高效微调，可以在有限的训练数据下泛化到未见过的场景，并提供了一种通用的模拟连续动态条件的方法。 |
| [^46] | [Probabilistic Dataset Reconstruction from Interpretable Models.](http://arxiv.org/abs/2308.15099) | 本文提出了一个新的框架，实现了从可解释模型中概率重建数据集。在现实假设下，可以高效计算重建的不确定性，以量化信息泄漏的隐私影响。 |
| [^47] | [Sequential annotations for naturally-occurring HRI: first insights.](http://arxiv.org/abs/2308.15097) | 这篇论文介绍了一种用于改善嵌入式对话智能体交互的方法论，并提出了一种基于语言和多模态资源使用理论基础的注释实践。 |
| [^48] | [Can We Rely on AI?.](http://arxiv.org/abs/2308.15092) | 近年来，对抗性攻击算法揭示了人工智能工具的不稳定性，在高风险环境中引发了安全、可靠性和可解释性方面的问题。这篇论文提供了一个对该主题的概述，关注对应用和计算数学领域的研究人员可能感兴趣的方面。 |
| [^49] | [LAMBO: Large Language Model Empowered Edge Intelligence.](http://arxiv.org/abs/2308.15078) | LAMBO是一种基于大型语言模型的边缘智能框架，用于移动边缘计算。它解决了传统深度卸载架构的问题，并提供了高性能的决策模块和强化学习模块。 |
| [^50] | [MadSGM: Multivariate Anomaly Detection with Score-based Generative Models.](http://arxiv.org/abs/2308.15069) | MadSGM是一种基于评分的生成模型的多变量时间序列异常检测器，考虑了重构、密度和梯度等全面的异常测量因素。实验证明MadSGM具有最强大和准确的预测能力。 |
| [^51] | [A Comprehensive Augmentation Framework for Anomaly Detection.](http://arxiv.org/abs/2308.15068) | 本文提出了一种用于异常检测的综合增强框架，该框架通过选择性地利用适当的组合，分析并压缩模拟异常的关键特征，与基于重构的方法相结合，并采用分割训练策略，能够在MVTec异常检测数据集上优于以前的最先进方法。 |
| [^52] | [Learning Cross-modality Information Bottleneck Representation for Heterogeneous Person Re-Identification.](http://arxiv.org/abs/2308.15063) | 本论文介绍了一种名为CMInfoNet的新型网络算法，通过优化相互信息瓶颈折衷，提取具有最具代表性信息的模态不变身份特征，并减少信息冗余和模态补充问题。 |
| [^53] | [Adapting text-based dialogue state tracker for spoken dialogues.](http://arxiv.org/abs/2308.15053) | 这篇论文描述了对构建适应口语对话系统的文本对话状态跟踪器进行的工程工作，利用自动语音识别错误校正和文本对话系统实现了插槽和值的估计。 |
| [^54] | [iBARLE: imBalance-Aware Room Layout Estimation.](http://arxiv.org/abs/2308.15050) | 我们提出了平衡感知式房间布局估计（iBARLE）框架，它包括外观变化生成模块、复杂结构混合模块和梯度-based布局目标函数，旨在解决房间布局估计中的不平衡和泛化问题。 |
| [^55] | [R^3: On-device Real-Time Deep Reinforcement Learning for Autonomous Robotics.](http://arxiv.org/abs/2308.15039) | R^3是一种在自主机器人中应用的基于设备的实时深度强化学习训练方法，它通过动态批量大小和回放缓冲区大小的优化，实现了在时间和算法性能之间的平衡，并有效地管理了内存和算法性能。 |
| [^56] | [Serving MoE Models on Resource-constrained Edge Devices via Dynamic Expert Swapping.](http://arxiv.org/abs/2308.15030) | 本文提出了一种在资源受限的边缘设备上通过动态专家交换为MoE模型提供服务的推理框架，该框架通过分析MoE模型的行为模式，引入了新的数据结构来减少资源消耗，并通过性能分析优化参数配置。 |
| [^57] | [Recursively Summarizing Enables Long-Term Dialogue Memory in Large Language Models.](http://arxiv.org/abs/2308.15022) | 递归总结在大型语言模型中实现长期对话记忆，可以提高对话系统在长对话中记忆重要信息的能力。 |
| [^58] | [Massively Parallel Continuous Local Search for Hybrid SAT Solving on GPUs.](http://arxiv.org/abs/2308.15020) | 提出了一种基于GPU的大规模并行连续局部搜索方法来加速混合SAT求解器，该方法通过使用基于快速傅里叶变换的新型并行算法来计算基本对称多项式，并在搜索中使用重启启发式算法以提高效率。与以前的方法相比，实现了显著的改进。 |
| [^59] | [Generative Model for Models: Rapid DNN Customization for Diverse Tasks and Resource Constraints.](http://arxiv.org/abs/2308.15003) | 该论文提出了一个一站式框架NN-Factory，通过使用生成模型直接生成定制的轻量级模型，实现了快速针对各种边缘场景的资源定制化和任务定制化。 |
| [^60] | [Exploring the Limits of Historical Information for Temporal Knowledge Graph Extrapolation.](http://arxiv.org/abs/2308.15002) | 本研究探索了时间知识图推断中历史信息的限制，并提出了一种新的事件预测模型CENET，通过历史对比学习框架学习历史和非历史依赖关系，以区分最有潜力的实体。 |
| [^61] | [Incorporating Neuro-Inspired Adaptability for Continual Learning in Artificial Intelligence.](http://arxiv.org/abs/2308.14991) | 本研究引入了神经启发的适应性解决方案，以实现人工智能的持续学习。通过模拟果蝇学习系统，我们提出了一种可以灵活适应变化的通用方法，改善了学习可塑性，并确保解决方案的兼容性。 |
| [^62] | [Constructive Incremental Learning for Fault Diagnosis of Rolling Bearings with Ensemble Domain Adaptation.](http://arxiv.org/abs/2308.14983) | 本文提出了一种基于增量构建学习和集成域适应的方法，用于滚动轴承故障诊断。该方法通过在随机配置网络上实施增量学习，结合云特征提取和小波包分解，提高了故障诊断的准确性和适应性。 |
| [^63] | [Efficient labeling of solar flux evolution videos by a deep learning model.](http://arxiv.org/abs/2308.14976) | 通过粗略标注的天文视频训练卷积神经网络（CNN），可以提高数据标注质量和减少人为干预，减少标注过程中的时间消耗。 |
| [^64] | [LLM-Based Human-Robot Collaboration Framework for Manipulation Tasks.](http://arxiv.org/abs/2308.14972) | 本文介绍了一种基于LLM的人机合作操作框架，利用逻辑推理和环境感知，将高级语言命令转化为可执行的运动函数。采用远程操作和动态运动原理进行动作矫正，以提高系统的实用性和普适性。 |
| [^65] | [Robust Open-Set Spoken Language Identification and the CU MultiLang Dataset.](http://arxiv.org/abs/2308.14951) | 本文实现了一种鲁棒的开放集言语识别系统，通过使用MFCC和音高特征，TDNN模型提取特征嵌入，设置置信度阈值，以及使用LDA和pLDA学习新的未知语言分类来实现。在经过训练的语言上，系统准确率达到91.76%，并且具备实时适应未知语言的能力。 |
| [^66] | [Transfusor: Transformer Diffusor for Controllable Human-like Generation of Vehicle Lane Changing Trajectories.](http://arxiv.org/abs/2308.14943) | Transfusor模型是一个使用Transformer和Diffusor模型的创新方法，旨在在高速公路场景中生成高度逼真且可控的类人换道轨迹。 |
| [^67] | [Auto-Prompting SAM for Mobile Friendly 3D Medical Image Segmentation.](http://arxiv.org/abs/2308.14936) | 这项工作提出了一种名为AutoSAM Adapter的方法，用于解决SAM在3D医学图像分割任务上的性能问题。通过参数高效的适应技术，实现了自动提示学习范式，消除了对手动生成提示的需求。 |
| [^68] | [Patient-specific, mechanistic models of tumor growth incorporating artificial intelligence and big data.](http://arxiv.org/abs/2308.14925) | 本文综述了肿瘤生长和治疗模型的不同方法，包括基于“大数据”和人工智能的机械模型以及数据驱动模型。同时指出了当前肿瘤研究中存在的问题和局限性。 |
| [^69] | [Optimal Economic Gas Turbine Dispatch with Deep Reinforcement Learning.](http://arxiv.org/abs/2308.14924) | 本研究通过将西门子能源提供的热力学软件纳入环境模型，并模拟不确定性，揭示了使用深度强化学习进行经济燃气轮机调度优化的好处，并发现Deep Q-Networks (DQN) 在算法和基准方法中获得了最高的奖励。 |
| [^70] | [On Reward Structures of Markov Decision Processes.](http://arxiv.org/abs/2308.14919) | 该论文研究了马尔可夫决策过程中的奖励结构，提出了一种估计器用于估计单个状态值，并通过根据奖励代替常用的基于转移的常数，提供了对强化学习中技巧的理论解释。 |
| [^71] | [RecRec: Algorithmic Recourse for Recommender Systems.](http://arxiv.org/abs/2308.14916) | 本文提出了一个算法性补救框架，用于帮助理解推荐系统的模型并修改推荐结果。 |
| [^72] | [RobustCLEVR: A Benchmark and Framework for Evaluating Robustness in Object-centric Learning.](http://arxiv.org/abs/2308.14899) | RobustCLEVR提出了一种目标中心学习的鲁棒性评估框架和基准数据集。该框架通过在图像生成过程中规定因果依赖关系，并能够产生多种无法在其他评估中出现的图像破坏，以评估目标中心方法的鲁棒性。 |
| [^73] | [Proceedings 39th International Conference on Logic Programming.](http://arxiv.org/abs/2308.14898) | 第39届国际逻辑编程会议论文集包含了在伦敦帝国理工学院举行的技术交流报告，涉及多个专题，包括逻辑编程和机器学习、逻辑编程和可解释性、伦理和可信度等。 |
| [^74] | [Statistically Efficient Variance Reduction with Double Policy Estimation for Off-Policy Evaluation in Sequence-Modeled Reinforcement Learning.](http://arxiv.org/abs/2308.14897) | 本论文提出了一种使用双策略评估的离线序列建模和离线强化学习的RL算法，通过统计方法证明具有方差缩减的特性。 |
| [^75] | [When hard negative sampling meets supervised contrastive learning.](http://arxiv.org/abs/2308.14893) | 当前图像模型在训练时常用交叉熵损失，但其在泛化和稳定性方面存在问题。本文提出了一种新的监督对比学习目标，SCHaNe，通过加权困难的负样本挖掘来提高模型性能。实验结果显示，SCHaNe在各个基准测试中的Top-1准确率优于BEiT-3。 |
| [^76] | [NAS-X: Neural Adaptive Smoothing via Twisting.](http://arxiv.org/abs/2308.14864) | NAS-X是一种基于扭曲的神经自适应平滑方法，通过重新加权的唤醒-睡眠算法来学习和推断顺序潜变量模型，并在离散和连续任务中取得了优于先前方法的推断和参数恢复效果。 |
| [^77] | [Evaluation of Key Spatiotemporal Learners for Print Track Anomaly Classification Using Melt Pool Image Streams.](http://arxiv.org/abs/2308.14861) | 本研究评估了使用熔池图像流进行印刷轨迹异常分类的关键时空学习器，并介绍了一些领先的深度时空学习模型的实践应用。 |
| [^78] | [Attention Visualizer Package: Revealing Word Importance for Deeper Insight into Encoder-Only Transformer Models.](http://arxiv.org/abs/2308.14850) | 这个论文介绍了Attention Visualizer包，通过可视化展示单词在编码器-只有的Transformer模型中的重要性，提高了对神经网络的解释性和可解释性。 |
| [^79] | [Trust in Construction AI-Powered Collaborative Robots: A Qualitative Empirical Analysis.](http://arxiv.org/abs/2308.14846) | 该论文通过分析建筑从业者的半结构化访谈结果，研究了建筑中值得信赖的AI动力协作机器人的特征。研究发现，除了之前已鉴定出的关键信任因素外，财务考虑和不确定性等其他因素也是重要因素。 |
| [^80] | [Robust Activity Recognition for Adaptive Worker-Robot Interaction using Transfer Learning.](http://arxiv.org/abs/2308.14843) | 本文介绍了一种使用迁移学习的鲁棒活动识别方法，对于建筑工人的活动识别，该方法在准确性方面要求更少的数据和计算时间。 |
| [^81] | [Toward Optimized VR/AR Ergonomics: Modeling and Predicting User Neck Muscle Contraction.](http://arxiv.org/abs/2308.14841) | 该研究利用肌电图设备测量、建模和预测了VR用户在与虚拟环境交互时的颈部肌肉收缩水平，开发了一个生物物理启发式的计算模型，可以准确预测不同头部运动状态下的颈部收缩水平，并且可以预测仅通过目标头部姿势的潜在收缩需求。 |
| [^82] | [Identifying and Mitigating the Security Risks of Generative AI.](http://arxiv.org/abs/2308.14840) | 生成式人工智能技术具有巨大的潜力，但也存在安全风险。这篇论文是一个研讨会的综合报道，讨论了生成式人工智能所带来的双重用途困境，提出了社区在这个领域的短期和长期目标。 |
| [^83] | [Distributionally Robust Statistical Verification with Imprecise Neural Networks.](http://arxiv.org/abs/2308.14815) | 本文提出了一种使用不精确神经网络的分布鲁棒统计验证方法，通过结合主动学习、不确定性量化和神经网络验证，可以在大量的分布上提供对黑盒系统行为的保证。 |
| [^84] | [Generating tabular datasets under differential privacy.](http://arxiv.org/abs/2308.14784) | 该论文研究了在差分隐私的约束下生成表格数据集的问题，通过利用生成对抗网络（GAN），它解决了训练数据的记忆重复和隐私泄露的问题，并提出了与传统方法相比更好的解决方案。 |
| [^85] | [Conflict-Aware Active Automata Learning.](http://arxiv.org/abs/2308.14781) | C3AL是一种冲突感知的主动有限状态机学习框架，能够处理观测数据中的冲突，通过将观测树作为学习过程的一等公民并最小化测试次数，具有很好的效果。 |
| [^86] | [May the Force be with You: Unified Force-Centric Pre-Training for 3D Molecular Conformations.](http://arxiv.org/abs/2308.14759) | 本论文提出了一种统一的力导向预训练模型用于3D分子构型，涵盖了平衡和非平衡数据。通过直接从原子力中学习非平衡数据和使用零力正则化和基于力的去噪技术近似近平衡力，我们获得了一个包含超过1500万个多样构型的预训练模型，相比于未预训练的模型，我们将力的准确性提高了大约3倍。 |
| [^87] | [Adaptive mitigation of time-varying quantum noise.](http://arxiv.org/abs/2308.14756) | 提出了一种贝叶斯推断算法来自适应学习和缓解时变量子噪声，并展示其优于非自适应方法的性能提升。 |
| [^88] | [Reinforcement Learning for Generative AI: A Survey.](http://arxiv.org/abs/2308.14328) | 该论文综述了强化学习在生成型人工智能中的应用。通过创建新的训练信号，强化学习展示了其从多个角度引入人类归纳偏好的强大和灵活性，以建立一个性能良好的模型。 |
| [^89] | [Empowering Clinicians and Democratizing Data Science: Large Language Models Automate Machine Learning for Clinical Studies.](http://arxiv.org/abs/2308.14120) | chatGPT ADA是一种能够自主开发临床研究所需的最先进的机器学习模型的大型语言模型，可将高级分析工具民主化，使非数据科学家的临床医生能够轻松应用于医学领域。 |
| [^90] | [Unified and Dynamic Graph for Temporal Character Grouping in Long Videos.](http://arxiv.org/abs/2308.14105) | 本文提出了一种统一动态图（UniDG）框架，通过统一的表示网络学习多模态表示，并保留模态的独特性。采用动态图聚类方法构建可靠的亲和图，并提出了一种渐进式的关联方法。 |
| [^91] | [A Survey of Imbalanced Learning on Graphs: Problems, Techniques, and Future Directions.](http://arxiv.org/abs/2308.13821) | 本综述对图上不平衡学习进行了全面的审视，旨在纠正数据分布偏差，以获得更准确和代表性的学习结果。 |
| [^92] | [Open Gaze: An Open-Source Implementation Replicating Google's Eye Tracking Paper.](http://arxiv.org/abs/2308.13495) | 本论文提出了一个仿制谷歌眼动论文的开源实现，重点是通过整合机器学习技术，在智能手机上实现与谷歌论文相当的准确眼动追踪解决方案。 |
| [^93] | [Reinforcement Learning-assisted Evolutionary Algorithm: A Survey and Research Opportunities.](http://arxiv.org/abs/2308.13420) | 本文调查了强化学习辅助进化算法（RL-EA），该算法将强化学习与进化算法结合，以提高优化性能。对各种RL-EA的结构、操作符和搜索模式进行了分类和概述。 |
| [^94] | [Description Logics Go Second-Order -- Extending EL with Universally Quantified Concepts.](http://arxiv.org/abs/2308.08252) | 这篇论文通过引入全称量化概念扩展了描述逻辑$\mathcal{EL}$，分别提出了模式语义和二阶语义，研究了它们的性质并证明了它们在有用片段中的结论相同。 |
| [^95] | [NBIAS: A Natural Language Processing Framework for Bias Identification in Text.](http://arxiv.org/abs/2308.01681) | 本论文提出了一个名为NBIAS的自然语言处理框架，旨在识别文本中的偏见。通过收集来自社交媒体、医疗保健和职位招聘等领域的多样化数据构建数据集，并应用基于Transformer的令牌分类模型来识别偏见词/短语。通过定量和定性评估方法来评估模型的效果。 |
| [^96] | [ChatGPT for Software Security: Exploring the Strengths and Limitations of ChatGPT in the Security Applications.](http://arxiv.org/abs/2307.12488) | 本文通过对ChatGPT在安全导向的程序分析中的表现进行研究，旨在了解其优势和局限性。研究结果可以帮助我们更好地理解ChatGPT在安全领域的应用潜力。 |
| [^97] | [Anticipating Driving Behavior through Deep Learning-Based Policy Prediction.](http://arxiv.org/abs/2307.11058) | 通过处理视频帧和深度细节，我们开发了一个综合系统来预测驾驶行为，实现了显著的准确性，比单独使用视频帧更好。 |
| [^98] | [What's meant by explainable model: A Scoping Review.](http://arxiv.org/abs/2307.09673) | 这项研究通过范围审查方法调查了应用人工智能模型并采用事后解释方法的论文，探讨了可解释模型这一术语的含义。 |
| [^99] | [Principles and Guidelines for Evaluating Social Robot Navigation Algorithms.](http://arxiv.org/abs/2306.16740) | 本文提出了评估社交机器人导航算法的原则与指南，为解决在人类居住环境中导航的挑战提供了可重复和可比较的基准标准。 |
| [^100] | [DR-HAI: Argumentation-based Dialectical Reconciliation in Human-AI Interactions.](http://arxiv.org/abs/2306.14694) | DR-HAI是一个新颖的基于论证的框架，旨在通过互动调和解决人工智能与人类之间的知识差异，为促进有效的人工智能与人类交互提供了一个有希望的方向。 |
| [^101] | [Large Language Models are Fixated by Red Herrings: Exploring Creative Problem Solving and Einstellung Effect using the Only Connect Wall Dataset.](http://arxiv.org/abs/2306.11167) | 这项研究探索了大型语言模型（LLMs）对创造性问题解决的能力，并发现大型语言模型容易被误导，出现固定效应和Einstellung范式。 |
| [^102] | [Mol-Instructions: A Large-Scale Biomolecular Instruction Dataset for Large Language Models.](http://arxiv.org/abs/2306.08018) | Mol-Instructions是一个专门为生物分子领域设计的综合指令数据集，可以显著提高大语言模型在生物领域中的适应能力和认知敏锐度。 |
| [^103] | [When Do Annotator Demographics Matter? Measuring the Influence of Annotator Demographics with the POPQUORN Dataset.](http://arxiv.org/abs/2306.06826) | 标注者的背景对数据标注的影响很重要。通过POPQUORN数据集的分析，我们发现标注者的背景在他们的判断中起到了显著作用，并且应该考虑以前未考虑的背景因素。我们的研究建议理解标注者的背景，从具有人口统计学平衡的众包工作者中收集标签，以减少数据集的偏差。 |
| [^104] | [On Optimal Caching and Model Multiplexing for Large Model Inference.](http://arxiv.org/abs/2306.02003) | 本文提出了最优缓存与模型复用两种方法来缓解大型模型推理中资源消耗和延迟挑战，经过实证模拟发现这种组合大大提高了传统模型推理方法的性能。 |
| [^105] | [Reinforcement Learning With Reward Machines in Stochastic Games.](http://arxiv.org/abs/2305.17372) | 该论文研究了复杂任务中具有非马尔可夫回报函数的随机博弈的多智能体强化学习，提出了一种基于奖励机制的算法，在纳什均衡下学习每个智能体的最佳应答策略，并证明了学习的Q函数将会收敛于一个纳什均衡的Q函数，前提是阶段博弈具有全局最优点或者鞍点，并且智能体基于这一点进行最佳应答策略的Q函数更新。 |
| [^106] | [Streaming Object Detection on Fisheye Cameras for Automatic Parking.](http://arxiv.org/abs/2305.14713) | 这篇论文提出了一个实时检测框架，使用动态和静态流双流感知模块，能够预测未来并减轻时延问题，解决了以往鱼眼摄像头目标检测中时延差异带来的安全隐患问题。 |
| [^107] | [Walking Your LiDOG: A Journey Through Multiple Domains for LiDAR Semantic Segmentation.](http://arxiv.org/abs/2304.11705) | 本研究通过设计第一个实验设置，探讨了LiDAR语义分割在不同领域间的泛化能力。研究结果表明，现有方法在跨领域设置中存在显著差距。为了解决这个问题，我们提出了一种新的方法，通过结合稀疏-密集卷积网络，实现了在目标领域上的显著优化。 |
| [^108] | [Quantifying Causes of Arctic Amplification via Deep Learning based Time-series Causal Inference.](http://arxiv.org/abs/2303.07122) | 该研究提出了一种基于循环神经网络的时间序列因果推断模型TCINet，用于推断大气过程对海冰融化的因果效应。通过实验证明，该模型能够显著提高量化北极海冰融化的主要原因的能力。 |
| [^109] | [On the Robustness of ChatGPT: An Adversarial and Out-of-distribution Perspective.](http://arxiv.org/abs/2302.12095) | 本研究评估了ChatGPT的鲁棒性，发现其在对抗性和超出分布任务上有一致的优势，但绝对表现仍有提高空间，鲁棒性仍是一个重要的挑战。 |
| [^110] | [Unreliable Partial Label Learning with Recursive Separation.](http://arxiv.org/abs/2302.09891) | 本论文提出了一种名为Unreliable Partial Label Learning with Recursive Separation (UPLLRS)的两阶段框架来解决不可靠部分标签学习问题，在第一阶段采用自适应递归分离策略将训练集分为可靠子集和不可靠子集，在第二阶段利用这些子集的信息来提高学习效果。 |
| [^111] | [Fairness-aware Vision Transformer via Debiased Self-Attention.](http://arxiv.org/abs/2301.13803) | 这篇论文提出了一种基于去偏自注意力的公平感知视觉变换器框架，通过消除与敏感属性相关的虚假特征来减轻偏见，并利用对抗性示例来定位和屏蔽这些特征。 |
| [^112] | [Towards an AI-enabled Connected Industry: AGV Communication and Sensor Measurement Datasets.](http://arxiv.org/abs/2301.03364) | 本文介绍了两个无线测量活动所提供的数据集，并将其与机器学习结合起来用于指纹识别、视线检测、服务质量预测或链路选择等任务。 |
| [^113] | [An Empirical Investigation of the Role of Pre-training in Lifelong Learning.](http://arxiv.org/abs/2112.09153) | 这项研究通过对大型预训练模型在多个任务上的性能评估，发现通用的前期训练可以在终身学习中减轻灾难性遗忘的影响。 |

# 详细

[^1]: 通用的自监督计算病理学模型

    A General-Purpose Self-Supervised Model for Computational Pathology. (arXiv:2308.15474v1 [cs.CV])

    [http://arxiv.org/abs/2308.15474](http://arxiv.org/abs/2308.15474)

    本论文介绍了一个通用的自监督计算病理学模型，通过在各种组织类型上进行广泛开发和评估，为组织表型分型提供了解决方案。

    

    组织表型分型是学习解剖病理标志物客观特征的基本计算病理学任务。然而，整个切片显微镜图像(WSI)存在一个复杂的计算机视觉问题，即WSI的大规模图像分辨率和形态表型的巨大多样性使得大规模数据注释成为不可能。目前的研究工作提出使用预训练的图像编码器，其中包括从自然图像数据集进行的迁移学习或在公开可用的组织病理学数据集上进行自监督预训练，但尚未在各种组织类型上进行广泛开发和评估。我们引入了UNI，这是一个通用的自监督计算病理学模型，使用来自20种主要组织类型的100,000个诊断性血红蛋白和伊红固醇染色切片的一亿多个组织块进行预训练，并在33个代表性的计算病理学临床任务上进行了评估。

    Tissue phenotyping is a fundamental computational pathology (CPath) task in learning objective characterizations of histopathologic biomarkers in anatomic pathology. However, whole-slide imaging (WSI) poses a complex computer vision problem in which the large-scale image resolutions of WSIs and the enormous diversity of morphological phenotypes preclude large-scale data annotation. Current efforts have proposed using pretrained image encoders with either transfer learning from natural image datasets or self-supervised pretraining on publicly-available histopathology datasets, but have not been extensively developed and evaluated across diverse tissue types at scale. We introduce UNI, a general-purpose self-supervised model for pathology, pretrained using over 100 million tissue patches from over 100,000 diagnostic haematoxylin and eosin-stained WSIs across 20 major tissue types, and evaluated on 33 representative CPath clinical tasks in CPath of varying diagnostic difficulties. In addi
    
[^2]: 多模态对比学习和表格注意力在自动化阿尔茨海默病预测中的应用

    Multimodal Contrastive Learning and Tabular Attention for Automated Alzheimer's Disease Prediction. (arXiv:2308.15469v1 [cs.CV])

    [http://arxiv.org/abs/2308.15469](http://arxiv.org/abs/2308.15469)

    本论文提出了一个多模态对比学习的通用框架，结合了图像数据和表格数据，设计了一种新颖的表格注意力模块，并将这些技术应用于阿尔茨海默病的预测。实验证明了该框架的优势，并展示了其在阿尔茨海默病检测方面的高准确率。

    

    随着神经影像学数据如MRI扫描和PET，阿尔茨海默病（AD）数据集中还包含有价值的表格数据，包括AD生物标志物和临床评估。现有的计算机视觉方法难以利用这些额外信息。为了满足这些需求，我们提出了一个多模态对比学习图像数据和表格数据的通用框架，一种用于放大和排序表格中显著特征的新颖表格注意力模块，并将这些技术应用于阿尔茨海默病预测。实验评估通过从ADNI数据库中的882个MR图像切片中检测阿尔茨海默病（AD）来展示我们框架的优势。我们利用表格数据的高可解释性和我们的新颖表格注意力方法，通过对每行的注意力分数进行归因，我们可以确定并排名最主要的特征。结果显示，该模型能够达到超过83.8%的准确率。

    Alongside neuroimaging such as MRI scans and PET, Alzheimer's disease (AD) datasets contain valuable tabular data including AD biomarkers and clinical assessments. Existing computer vision approaches struggle to utilize this additional information. To address these needs, we propose a generalizable framework for multimodal contrastive learning of image data and tabular data, a novel tabular attention module for amplifying and ranking salient features in tables, and the application of these techniques onto Alzheimer's disease prediction. Experimental evaulations demonstrate the strength of our framework by detecting Alzheimer's disease (AD) from over 882 MR image slices from the ADNI database. We take advantage of the high interpretability of tabular data and our novel tabular attention approach and through attribution of the attention scores for each row of the table, we note and rank the most predominant features. Results show that the model is capable of an accuracy of over 83.8%, al
    
[^3]: 损失函数的比较研究：常规和拥堵情景下的交通预测

    A Comparative Study of Loss Functions: Traffic Predictions in Regular and Congestion Scenarios. (arXiv:2308.15464v1 [cs.LG])

    [http://arxiv.org/abs/2308.15464](http://arxiv.org/abs/2308.15464)

    本文通过研究多种受重尾分析和不平衡分类问题启发的损失函数，提出了改进交通预测中拥堵情况的准确性的方法，并分别提出了针对不同优化目标的最佳选择。

    

    时空图神经网络在交通预测方面取得了最先进的性能。然而，由于传统损失函数的局限性，它们往往难以准确预测拥堵情况。准确预测常规交通条件至关重要，但可靠的人工智能系统还必须准确预测拥堵情景，以维持安全和高效的交通。在本文中，我们探讨了受重尾分析和不平衡分类问题启发的各种损失函数，以解决这个问题。我们在交通速度预测方面评估了这些损失函数的有效性，重点关注拥堵情况。通过对真实交通数据集进行广泛实验，我们发现在优化平均绝对误差（MAE）时，MAE-Focal Loss函数表现最为有效。在优化均方误差（MSE）时，Gumbel Loss被证明是更优的选择。这些选择可以有效地预测交通拥堵。

    Spatiotemporal graph neural networks have achieved state-of-the-art performance in traffic forecasting. However, they often struggle to forecast congestion accurately due to the limitations of traditional loss functions. While accurate forecasting of regular traffic conditions is crucial, a reliable AI system must also accurately forecast congestion scenarios to maintain safe and efficient transportation. In this paper, we explore various loss functions inspired by heavy tail analysis and imbalanced classification problems to address this issue. We evaluate the efficacy of these loss functions in forecasting traffic speed, with an emphasis on congestion scenarios. Through extensive experiments on real-world traffic datasets, we discovered that when optimizing for Mean Absolute Error (MAE), the MAE-Focal Loss function stands out as the most effective. When optimizing Mean Squared Error (MSE), Gumbel Loss proves to be the superior choice. These choices effectively forecast traffic conges
    
[^4]: ParaGuide: 用于即插即用文本风格转移的引导性扩散改写器

    ParaGuide: Guided Diffusion Paraphrasers for Plug-and-Play Textual Style Transfer. (arXiv:2308.15459v1 [cs.CL])

    [http://arxiv.org/abs/2308.15459](http://arxiv.org/abs/2308.15459)

    ParaGuide是一种用于通用风格转移的引导性扩散改写器，可以灵活适应任意目标风格，通过梯度引导和改写条件的扩散模型实现文本的风格转变，同时保留语义信息。

    

    文本风格转移是在保留意义的同时转变文本的风格属性的任务。目标风格可以以多种方式定义，从单一属性（例如正式性）到作者（例如莎士比亚）。先前的无监督风格转移方法通常依赖于大量标记数据，仅适用于固定的风格集，或需要大型语言模型。相反，我们引入了一种新的基于扩散的通用风格转移框架，可以在推理时灵活适应任意目标风格。我们的参数高效方法ParaGuide利用了改写条件的扩散模型以及来自现成的分类器和强大的风格嵌入器的梯度引导，以转变文本的风格同时保留语义信息。我们在Enron邮件语料库上进行了验证，包括人工和自动评估，并发现其在正式性和... (内容太多，请参考英文摘要)

    Textual style transfer is the task of transforming stylistic properties of text while preserving meaning. Target "styles" can be defined in numerous ways, ranging from single attributes (e.g, formality) to authorship (e.g, Shakespeare). Previous unsupervised style-transfer approaches generally rely on significant amounts of labeled data for only a fixed set of styles or require large language models. In contrast, we introduce a novel diffusion-based framework for general-purpose style transfer that can be flexibly adapted to arbitrary target styles at inference time. Our parameter-efficient approach, ParaGuide, leverages paraphrase-conditioned diffusion models alongside gradient-based guidance from both off-the-shelf classifiers and strong existing style embedders to transform the style of text while preserving semantic information. We validate the method on the Enron Email Corpus, with both human and automatic evaluations, and find that it outperforms strong baselines on formality, se
    
[^5]: 从SMOTE到Mixup用于深度不平衡分类

    From SMOTE to Mixup for Deep Imbalanced Classification. (arXiv:2308.15457v1 [cs.LG])

    [http://arxiv.org/abs/2308.15457](http://arxiv.org/abs/2308.15457)

    本研究提出了一种从SMOTE到Mixup的方法，用于深度不平衡分类。通过对SMOTE进行改进，并结合Mixup技术，我们构建了一个统一的数据增强框架。研究表明，Mixup技术通过实现多数类和少数类之间的不平衡间隙来改善泛化能力。我们还提出了一种新颖的基于边界的Mixup技术，更明确地实现了不平衡间隙。实验结果表明我们的方法在多个数据集上取得了最好的性能。

    

    鉴于不平衡的数据，使用深度学习训练好的分类器因为少数类的泛化能力差而困难重重。传统上，用于数据增强的知名少数类合成过采样技术（SMOTE），作为一种面向不平衡学习的数据挖掘方法，被用来改善这种泛化。然而，SMOTE在深度学习中是否也有益处仍不清楚。在这项工作中，我们研究了为什么原始的SMOTE对深度学习来说是不足的，并使用软标签增强了SMOTE。将得到的软SMOTE与Mixup，一种现代数据增强技术，连接在一起，形成了一个统一的框架，将传统和现代的数据增强技术纳入同一个范畴。在这个框架中进行系统研究表明，Mixup通过隐式地实现多数类和少数类之间的不平衡间隙来改善泛化能力。然后，我们提出了一种新颖的基于边界的Mixup技术，更明确地实现了不平衡间隙。大量的实验表明，我们的方法在多个数据集上相对于最先进的算法都取得了最好的性能。

    Given imbalanced data, it is hard to train a good classifier using deep learning because of the poor generalization of minority classes. Traditionally, the well-known synthetic minority oversampling technique (SMOTE) for data augmentation, a data mining approach for imbalanced learning, has been used to improve this generalization. However, it is unclear whether SMOTE also benefits deep learning. In this work, we study why the original SMOTE is insufficient for deep learning, and enhance SMOTE using soft labels. Connecting the resulting soft SMOTE with Mixup, a modern data augmentation technique, leads to a unified framework that puts traditional and modern data augmentation techniques under the same umbrella. A careful study within this framework shows that Mixup improves generalization by implicitly achieving uneven margins between majority and minority classes. We then propose a novel margin-aware Mixup technique that more explicitly achieves uneven margins. Extensive experimental r
    
[^6]: 什么时候编程思维对推理起作用?

    When Do Program-of-Thoughts Work for Reasoning?. (arXiv:2308.15452v1 [cs.CL])

    [http://arxiv.org/abs/2308.15452](http://arxiv.org/abs/2308.15452)

    提出了复杂性影响推理分数（CIRS）来衡量编程语言对推理能力的影响，发现并非所有复杂性的代码数据都可以被学习或理解，适当的复杂性水平对于改善推理能力至关重要。

    

    大型语言模型（LLM）的推理能力在体现出人工智能领域中起着关键作用。尽管像编程思维提示这样的方法对于使用编程语言来解决复杂推理任务的LLM非常有效，但代码数据对推理能力的具体影响仍未充分探索。为了填补这一空白，我们提出了复杂性影响推理分数（CIRS），它结合了结构和逻辑属性，以衡量代码和推理能力之间的相关性。具体而言，我们使用抽象语法树来编码结构信息，并通过考虑难度和圈复杂度来计算逻辑复杂性。通过实证分析，我们发现并非所有复杂性的代码数据都可以被LLM学习或理解。最佳复杂性水平对于通过编程辅助提示改善推理能力至关重要。然后我们设计了一个自动合成的方法...

    The reasoning capabilities of Large Language Models (LLMs) play a pivotal role in the realm of embodied artificial intelligence. Although there are effective methods like program-of-thought prompting for LLMs which uses programming language to tackle complex reasoning tasks, the specific impact of code data on the improvement of reasoning capabilities remains under-explored. To address this gap, we propose complexity-impacted reasoning score (CIRS), which combines structural and logical attributes, to measure the correlation between code and reasoning abilities. Specifically, we use the abstract syntax tree to encode the structural information and calculate logical complexity by considering the difficulty and the cyclomatic complexity. Through an empirical analysis, we find not all code data of complexity can be learned or understood by LLMs. Optimal level of complexity is critical to the improvement of reasoning abilities by program-aided prompting. Then we design an auto-synthesizing
    
[^7]: 通过卫星地图补充车载传感器：高精度地图构建的新视角

    Complementing Onboard Sensors with Satellite Map: A New Perspective for HD Map Construction. (arXiv:2308.15427v1 [cs.CV])

    [http://arxiv.org/abs/2308.15427](http://arxiv.org/abs/2308.15427)

    本研究通过补充卫星地图，增强了车载传感器构建高精度地图的方法，利用卫星地图的广阔覆盖能力。我们释放了卫星地图瓦片作为nuScenes数据集的补充，同时提出了一个分层融合模块来更好地融合车载传感器与卫星地图的信息。

    

    高精度（HD）地图对自动驾驶系统至关重要。最近的方法尝试基于车载传感器获取的信息实时构建HD地图。然而，这些方法的性能受到车辆周围环境的显著影响，这是由于车载传感器的固有限制，如对远程探测的能力不足。在本研究中，我们证明了通过补充卫星地图可以增强HD地图构建方法的性能，利用卫星地图的广泛覆盖能力。为了进一步的研究，我们发布了卫星地图瓦片作为nuScenes数据集的补充数据集。与此同时，我们提出了一个分层融合模块，使卫星地图信息与现有方法更好地融合。具体来说，我们设计了基于分割和距离的注意力掩码，应用交叉注意力机制来融合车载传感器与卫星地图的信息。

    High-Definition (HD) maps play a crucial role in autonomous driving systems. Recent methods have attempted to construct HD maps in real-time based on information obtained from vehicle onboard sensors. However, the performance of these methods is significantly susceptible to the environment surrounding the vehicle due to the inherent limitation of onboard sensors, such as weak capacity for long-range detection. In this study, we demonstrate that supplementing onboard sensors with satellite maps can enhance the performance of HD map construction methods, leveraging the broad coverage capability of satellite maps. For the purpose of further research, we release the satellite map tiles as a complementary dataset of nuScenes dataset. Meanwhile, we propose a hierarchical fusion module that enables better fusion of satellite maps information with existing methods. Specifically, we design an attention mask based on segmentation and distance, applying the cross-attention mechanism to fuse onboa
    
[^8]: 色彩美学：基于模糊用户驱动的协调与偏好预测方法

    Color Aesthetics: Fuzzy based User-driven Method for Harmony and Preference Prediction. (arXiv:2308.15397v1 [cs.CV])

    [http://arxiv.org/abs/2308.15397](http://arxiv.org/abs/2308.15397)

    该论文提出了一种基于模糊用户驱动的方法，用于预测协调和偏好，通过结合基本颜色的偏好和色彩协调的评分来预测对色彩方案的偏好，在各种电子商务应用中具有实用价值。

    

    色彩是对产品销售产生强大影响的最重要的内在感觉特征。色彩甚至对我们大脑中的审美感知起到提高的作用。考虑到个体差异在色彩美学中至关重要。对于各种电子商务应用，需要用户驱动的机制。我们提出了一种用于定量评估对颜色的所有类型的知觉反应的方法：明确的颜色偏好、色彩协调和色彩组合偏好。通过结合基本颜色的偏好和色彩协调的评分，可以预测对色彩方案的偏好。使用基于模糊相似性和分组的比较算法从大数据集中提取协调调色板。所提出的模型能够对多色图像的协调和偏好进行有用的预测。例如，在服装协调的背景下，它允许根据服装颜色预测外观的偏好。我们的方法与标准的审美模型有所不同。

    Color is the most important intrinsic sensory feature that has a powerful impact on product sales. Color is even responsible for raising the aesthetic senses in our brains. Account for individual differences is crucial in color aesthetics. It requires user-driven mechanisms for various e-commerce applications. We propose a method for quantitative evaluation of all types of perceptual responses to color(s): distinct color preference, color harmony, and color combination preference. Preference for color schemes can be predicted by combining preferences for the basic colors and ratings of color harmony. Harmonious pallets are extracted from big data set using comparison algorithms based on fuzzy similarity and grouping. The proposed model results in useful predictions of harmony and preference of multicolored images. For example, in the context of apparel coordination, it allows predicting a preference for a look based on clothing colors. Our approach differs from standard aesthetic model
    
[^9]: 分布式能量存储系统的分散式多智能体强化学习电荷平衡策略

    Decentralized Multi-agent Reinforcement Learning based State-of-Charge Balancing Strategy for Distributed Energy Storage System. (arXiv:2308.15394v1 [cs.AI])

    [http://arxiv.org/abs/2308.15394](http://arxiv.org/abs/2308.15394)

    本文提出了一种基于分散式多智能体强化学习的电荷平衡策略，通过分布式方法解决了分布式能量存储系统中的电荷平衡问题，并通过平均一致性算法来进行优化。

    

    本文提出了一种分散式多智能体强化学习（Dec-MARL）方法，用于解决分布式能量存储系统中的电荷平衡问题。首先，将电荷平衡问题转化为一个有限马尔可夫决策过程，并根据需求平衡得到的动作约束，采用Dec-MARL来解决该问题。具体而言，利用一阶平均一致性算法以完全分散的方式扩展分布式能量存储系统状态的观测，并根据这些观测决定初始动作（即输出功率）。为了得到允许范围内的最终动作，提出了一个反事实需求平衡算法来平衡总需求和初始动作。然后，智能体执行最终动作并从环境中获得局部奖励，使系统进入下一个状态。最后，通过一阶平均一致性算法，智能体获得平均奖励，并通过迭代更新策略来不断优化电荷平衡策略。

    This paper develops a Decentralized Multi-Agent Reinforcement Learning (Dec-MARL) method to solve the SoC balancing problem in the distributed energy storage system (DESS). First, the SoC balancing problem is formulated into a finite Markov decision process with action constraints derived from demand balance, which can be solved by Dec-MARL. Specifically, the first-order average consensus algorithm is utilized to expand the observations of the DESS state in a fully-decentralized way, and the initial actions (i.e., output power) are decided by the agents (i.e., energy storage units) according to these observations. In order to get the final actions in the allowable range, a counterfactual demand balance algorithm is proposed to balance the total demand and the initial actions. Next, the agents execute the final actions and get local rewards from the environment, and the DESS steps into the next state. Finally, through the first-order average consensus algorithm, the agents get the avera
    
[^10]: 使用自上向下调节的WTA网络在贝叶斯推理中整合信息的贝叶斯方法

    Bayesian Integration of Information Using Top-Down Modulated WTA Networks. (arXiv:2308.15390v1 [cs.AI])

    [http://arxiv.org/abs/2308.15390](http://arxiv.org/abs/2308.15390)

    本文研究了使用自上而下调节的WTA网络在贝叶斯推理中整合信息的方法，探讨了自上而下的过程在提高WTA网络性能方面的作用。

    

    胜者通吃（WTA）电路作为一种脉冲神经网络（SNN）被建议作为大脑以贝叶斯方式处理信息的能力的促进因素。研究表明，WTA电路能够通过期望最大化（EM）逼近分层贝叶斯模型。到目前为止，这个研究方向主要关注自下而上的过程，这与神经科学证据相矛盾，后者显示除了自下而上的过程外，自上而下的过程也在人脑的信息处理中发挥关键作用。自上而下的过程包括注意力的指向、对期望的调整、学习信息的编码和回忆的促进以及想象等。本文探讨了WTA电路是否适合进一步整合在不同WTA网络中表示的信息。此外，它还探讨了在什么情况下自上而下的过程能够提高WTA网络在推理方面的性能。

    Winner Take All (WTA) circuits a type of Spiking Neural Networks (SNN) have been suggested as facilitating the brain's ability to process information in a Bayesian manner. Research has shown that WTA circuits are capable of approximating hierarchical Bayesian models via Expectation Maximization (EM). So far, research in this direction has focused on bottom up processes. This is contrary to neuroscientific evidence that shows that, besides bottom up processes, top down processes too play a key role in information processing by the human brain. Several functions ascribed to top down processes include direction of attention, adjusting for expectations, facilitation of encoding and recall of learned information, and imagery. This paper explores whether WTA circuits are suitable for further integrating information represented in separate WTA networks. Furthermore, it explores whether, and under what circumstances, top down processes can improve WTA network performance with respect to infere
    
[^11]: RED：一种用于机器人环境动态的系统性实时调度方法

    RED: A Systematic Real-Time Scheduling Approach for Robotic Environmental Dynamics. (arXiv:2308.15368v1 [cs.RO])

    [http://arxiv.org/abs/2308.15368](http://arxiv.org/abs/2308.15368)

    RED是一种系统性实时调度方法，旨在支持资源有限的机器人系统中的多任务深度神经网络工作负载。它采用中间截止时间分配策略，适应性地管理机器人环境动态（RED），以满足实时约束。

    

    智能机器人被设计为能够有效地在充满移动机械元素和物体的动态和不可预测的环境中导航。这种环境诱发的动态，包括移动的障碍物，可以在运行时改变计算需求（例如，创建新任务）和工作负载的结构（例如任务之间的优先约束），从而对系统整体性能产生不利影响。当多任务推理预期在资源和实时约束条件严格的机器人上进行时，这种挑战会被放大。为了解决这种挑战，我们引入了一种名为RED的系统性实时调度方法，旨在支持资源有限的机器人系统中的多任务深度神经网络工作负载。它被设计为在遵守实时约束的同时适应性地管理机器人环境动态（RED）。RED的核心是一个基于截止时间的调度程序，它采用中间截止时间分配策略，从而有效地管理任务的执行顺序。

    Intelligent robots are designed to effectively navigate dynamic and unpredictable environments laden with moving mechanical elements and objects. Such environment-induced dynamics, including moving obstacles, can readily alter the computational demand (e.g., the creation of new tasks) and the structure of workloads (e.g., precedence constraints among tasks) during runtime, thereby adversely affecting overall system performance. This challenge is amplified when multi-task inference is expected on robots operating under stringent resource and real-time constraints. To address such a challenge, we introduce RED, a systematic real-time scheduling approach designed to support multi-task deep neural network workloads in resource-limited robotic systems. It is designed to adaptively manage the Robotic Environmental Dynamics (RED) while adhering to real-time constraints. At the core of RED lies a deadline-based scheduler that employs an intermediate deadline assignment policy, effectively mana
    
[^12]: 通过客户端特定的提示生成，在联邦学习中实现高效的模型个性化

    Efficient Model Personalization in Federated Learning via Client-Specific Prompt Generation. (arXiv:2308.15367v1 [cs.CV])

    [http://arxiv.org/abs/2308.15367](http://arxiv.org/abs/2308.15367)

    pFedPG is a novel personalized FL framework that generates client-specific prompts to adapt frozen backbones to local data distributions, enabling efficient model personalization for heterogeneous clients in FL.

    

    联邦学习（FL）作为一种分散的学习框架，可以在不共享数据以保护隐私的情况下，从多个分布式客户端训练模型。最近，大型预训练模型（如Vision Transformer）展示了从分布式客户端中获得稳健表示的强大能力。然而，客户端之间数据的异质性、有限的计算资源和通信带宽限制了大型模型在FL框架中的部署。为了利用大型模型的稳健表示，同时实现对异构客户端的高效模型个性化，我们提出了一种新的个性化FL框架，即客户端特定的提示生成（pFedPG），它学习在服务器端部署个性化提示生成器，用以产生适应本地数据分布的客户端特定视觉提示，从而有效地将冻结的骨干网络适应到本地数据分布。

    Federated learning (FL) emerges as a decentralized learning framework which trains models from multiple distributed clients without sharing their data to preserve privacy. Recently, large-scale pre-trained models (e.g., Vision Transformer) have shown a strong capability of deriving robust representations. However, the data heterogeneity among clients, the limited computation resources, and the communication bandwidth restrict the deployment of large-scale models in FL frameworks. To leverage robust representations from large-scale models while enabling efficient model personalization for heterogeneous clients, we propose a novel personalized FL framework of client-specific Prompt Generation (pFedPG), which learns to deploy a personalized prompt generator at the server for producing client-specific visual prompts that efficiently adapts frozen backbones to local data distributions. Our proposed framework jointly optimizes the stages of personalized prompt adaptation locally and personal
    
[^13]: 从三维点云中估计自我运动和动态运动的分离以积累数据和改善三维物体检测

    Ego-Motion Estimation and Dynamic Motion Separation from 3D Point Clouds for Accumulating Data and Improving 3D Object Detection. (arXiv:2308.15357v1 [cs.RO])

    [http://arxiv.org/abs/2308.15357](http://arxiv.org/abs/2308.15357)

    本研究分析了高分辨率雷达传感器在积累雷达点云和改善三维物体检测方面的局限性，并提出了自我运动估计和动态运动校正方法以提高检测性能。

    

    新的三维高分辨率雷达传感器在汽车领域的三维物体检测中变得越来越重要，因为它们相对便宜且检测能力比传统低分辨率雷达传感器更好。与激光雷达传感器相比，高分辨率雷达传感器的一个限制是生成的点云稀疏。通过积累连续时间步长的雷达点云，可以部分克服这种稀疏性。本文通过采用不同的自我运动估计方法，分析了View-of-Delft数据集上积累雷达点云的局限性以及可能的解决方案。此外，还采用基于学习的实例运动估计方法，研究了动态运动对累积点云在目标检测中的影响。实验证明，应用自我运动估计和动态运动校正方法可以提高目标检测性能。

    New 3+1D high-resolution radar sensors are gaining importance for 3D object detection in the automotive domain due to their relative affordability and improved detection compared to classic low-resolution radar sensors. One limitation of high-resolution radar sensors, compared to lidar sensors, is the sparsity of the generated point cloud. This sparsity could be partially overcome by accumulating radar point clouds of subsequent time steps. This contribution analyzes limitations of accumulating radar point clouds on the View-of-Delft dataset. By employing different ego-motion estimation approaches, the dataset's inherent constraints, and possible solutions are analyzed. Additionally, a learning-based instance motion estimation approach is deployed to investigate the influence of dynamic motion on the accumulated point cloud for object detection. Experiments document an improved object detection performance by applying an ego-motion estimation and dynamic motion correction approach.
    
[^14]: 提升移动人脸防伪：在屏幕闪光下对多样攻击类型的鲁棒框架

    Enhancing Mobile Face Anti-Spoofing: A Robust Framework for Diverse Attack Types under Screen Flash. (arXiv:2308.15346v1 [cs.CV])

    [http://arxiv.org/abs/2308.15346](http://arxiv.org/abs/2308.15346)

    本论文提出了一种在屏幕闪光下对多样攻击类型鲁棒的人脸防伪框架，通过引入多个网络和双门模块，有效减小了欺骗人脸的类内距离，并提供了更准确的防伪结果。

    

    人脸防伪（FAS）对于保护人脸识别系统至关重要。然而，现有的基于手工制作的二进制或像素级标签的FAS方法由于各种展示攻击（PA）的限制存在局限性。在本文中，我们提出了一种在光闪光下攻击类型鲁棒的人脸防伪框架，称为ATR-FAS。由于各种攻击类型导致的图像差异，基于单一二进制分类网络的传统FAS方法可能导致欺骗人脸的过度类内距离，从而导致决策边界学习的挑战。因此，我们采用多个网络来重建多帧深度图作为辅助监督，并且每个网络专注于一种攻击类型。引入了由类型门和帧注意力门组成的双门模块（DGM），分别用于攻击类型识别和多帧注意力生成。DGM的输出被用作加权混合多个专家网络的结果。

    Face anti-spoofing (FAS) is crucial for securing face recognition systems. However, existing FAS methods with handcrafted binary or pixel-wise labels have limitations due to diverse presentation attacks (PAs). In this paper, we propose an attack type robust face anti-spoofing framework under light flash, called ATR-FAS. Due to imaging differences caused by various attack types, traditional FAS methods based on single binary classification network may result in excessive intra-class distance of spoof faces, leading to a challenge of decision boundary learning. Therefore, we employed multiple networks to reconstruct multi-frame depth maps as auxiliary supervision, and each network experts in one type of attack. A dual gate module (DGM) consisting of a type gate and a frame-attention gate is introduced, which perform attack type recognition and multi-frame attention generation, respectively. The outputs of DGM are utilized as weight to mix the result of multiple expert networks. The multi
    
[^15]: 早期诊断冠状动脉疾病的人工智能框架：边界SMOTE、自动编码器和卷积神经网络的集成方法

    AI Framework for Early Diagnosis of Coronary Artery Disease: An Integration of Borderline SMOTE, Autoencoders and Convolutional Neural Networks Approach. (arXiv:2308.15339v1 [cs.AI])

    [http://arxiv.org/abs/2308.15339](http://arxiv.org/abs/2308.15339)

    本研究开发了一个方法来平衡和增强数据，以提高冠状动脉疾病的预测准确性，在数据不平衡和样本量较小的情况下取得了较高的准确率。该方法对于其他数据收集成本高和样本量小的情况也具有潜在的应用价值。

    

    冠状动脉疾病（CAD）的诊断准确性取决于多种因素，包括人口统计学、症状、医学检查、心电图和超声心动图等数据。在这种情况下，人工智能（AI）可以帮助临床医生在诊断过程的早期阶段识别高风险患者，通过综合多个因素的信息。为了实现这一目标，使用机器学习算法根据CAD疾病风险对患者进行分类。在本研究中，我们通过开发一种用于平衡和增强数据以提高预测准确性的方法，在病例数据不平衡且样本量较小的情况下为这一研究领域作出了贡献。该方法可以在各种情况下使用，特别是在数据收集成本高且样本量较小的情况下。实验结果表明，我们提出的CAD预测方法的平均准确率为95.36，高于随机森林（RF）、决策树（DT）、支持向量机（SVM）等方法。

    The accuracy of coronary artery disease (CAD) diagnosis is dependent on a variety of factors, including demographic, symptom, and medical examination, ECG, and echocardiography data, among others. In this context, artificial intelligence (AI) can help clinicians identify high-risk patients early in the diagnostic process, by synthesizing information from multiple factors. To this aim, Machine Learning algorithms are used to classify patients based on their CAD disease risk. In this study, we contribute to this research filed by developing a methodology for balancing and augmenting data for more accurate prediction when the data is imbalanced and the sample size is small. The methodology can be used in a variety of other situations, particularly when data collection is expensive and the sample size is small. The experimental results revealed that the average accuracy of our proposed method for CAD prediction was 95.36, and was higher than random forest (RF), decision tree (DT), support 
    
[^16]: 一种负责任开发基于生成AI的自动学生反馈框架

    A Framework for Responsible Development of Automated Student Feedback with Generative AI. (arXiv:2308.15334v1 [cs.CY])

    [http://arxiv.org/abs/2308.15334](http://arxiv.org/abs/2308.15334)

    一种基于生成AI的自动学生反馈框架可以提供丰富的反馈，但引入了伦理问题，并需要解决“多数人的暴政”和忽视长尾中少数群体需求的挑战。

    

    提供丰富的反馈对于支持学生学习至关重要。最近生成AI尤其是大规模语言模型的进展，为向学生提供可重复、可扩展和即时生成的自动反馈提供了机会，使得之前稀缺且昂贵的学习资源变得丰富起来。从技术角度而言，这种方法是可行的，得益于最近人工智能和自然语言处理的进步；然而，采用这些技术也引入了一系列潜在的伦理问题，需要认真考虑。人工智能系统的吸引力在于它们可以有效地自动化最乏味的任务；但是这也可能导致“多数人的暴政”，即忽视了长尾中少数群体的需求，因为这些需求很难自动化。因此，开发能够产生有价值和真实的机器学习模型变得至关重要。

    Providing rich feedback to students is essential for supporting student learning. Recent advances in generative AI, particularly within large language modelling (LLM), provide the opportunity to deliver repeatable, scalable and instant automatically generated feedback to students, making abundant a previously scarce and expensive learning resource. Such an approach is feasible from a technical perspective due to these recent advances in Artificial Intelligence (AI) and Natural Language Processing (NLP); while the potential upside is a strong motivator, doing so introduces a range of potential ethical issues that must be considered as we apply these technologies. The attractiveness of AI systems is that they can effectively automate the most mundane tasks; but this risks introducing a "tyranny of the majority", where the needs of minorities in the long tail are overlooked because they are difficult to automate.  Developing machine learning models that can generate valuable and authentic
    
[^17]: FedLogic: 可解释化的联邦多领域思维链选择方法用于大型语言模型

    FedLogic: Interpretable Federated Multi-Domain Chain-of-Thought Prompt Selection for Large Language Models. (arXiv:2308.15324v1 [cs.AI])

    [http://arxiv.org/abs/2308.15324](http://arxiv.org/abs/2308.15324)

    本文提出了一种名为FedLogic的方法，用于解决大型语言模型在多领域思维链选择中的可解释性和平衡性问题。

    

    利用“思维链（CoT）”推理从大型语言模型（LLM）中获取快速精确的回答正迅速引起研究界的兴趣。其中一个重要的挑战是如何设计或选择最佳提示。提示选择的过程依赖于用户根据LLM生成的相应新反应不断调整和组合输入提示的试错。此外，目前还没有研究探讨LLM如何利用从用户交互中学习到的数学问题求解能力来解决叙述写作中的问题。为了改进可解释性并在多领域CoT提示选择场景下探索通用性和个性化之间的平衡原则，我们提出了联邦逻辑规则学习方法（FedLogic）。我们在联邦LLM的背景下引入了多领域CoT提示选择困境的理论形式化和交互模拟。

    Leveraging ``chain-of-thought (CoT)'' reasoning to elicit rapid and precise responses from large language models (LLMs) is rapidly attracting research interest. A notable challenge here is how to design or select optimal prompts. The process of prompt selection relies on trial and error, involving continuous adjustments and combinations of input prompts by users based on the corresponding new responses generated from LLMs. Furthermore, minimal research has been conducted to explore how LLMs employ the mathematical problem-solving capabilities learned from user interactions to address issues in narrative writing. To improve interpretability and explore the balance principle between generality and personalization under a multi-domain CoT prompt selection scenario, we propose the Federated Logic rule learning approach (FedLogic). We introduce a theoretical formalization and interactive emulation of the multi-domain CoT prompt selection dilemma in the context of federated LLMs. We cast the
    
[^18]: 阐明扩散模型中的曝光偏差问题

    Elucidating the Exposure Bias in Diffusion Models. (arXiv:2308.15321v1 [cs.LG])

    [http://arxiv.org/abs/2308.15321](http://arxiv.org/abs/2308.15321)

    本文系统地研究了扩散模型中的曝光偏差问题，并提出了一种名为Epsilon Scaling的免训练方法来减轻这一问题。实验结果验证了该方法的有效性。

    

    扩散模型展示了令人印象深刻的生成能力，但它们的“曝光偏差”问题，即训练和采样之间的输入不匹配，缺乏深入探索。本文通过首先对采样分布进行分析建模，然后将每个采样步骤的预测误差归因为曝光偏差问题的根本原因，系统地研究了扩散模型中的曝光偏差问题。此外，我们讨论了解决这个问题的潜在方法，并提出了一个直观的度量标准。除了阐明曝光偏差问题，我们提出了一种简单但有效的免训练方法，称为Epsilon Scaling，以减轻曝光偏差。我们展示了Epsilon Scaling通过缩小网络输出（Epsilon）明确地将采样轨迹移近训练阶段学习到的向量场，从而减轻了训练和采样之间的输入不匹配。在各种扩散框架上进行了实验。

    Diffusion models have demonstrated impressive generative capabilities, but their 'exposure bias' problem, described as the input mismatch between training and sampling, lacks in-depth exploration. In this paper, we systematically investigate the exposure bias problem in diffusion models by first analytically modelling the sampling distribution, based on which we then attribute the prediction error at each sampling step as the root cause of the exposure bias issue. Furthermore, we discuss potential solutions to this issue and propose an intuitive metric for it. Along with the elucidation of exposure bias, we propose a simple, yet effective, training-free method called Epsilon Scaling to alleviate the exposure bias. We show that Epsilon Scaling explicitly moves the sampling trajectory closer to the vector field learned in the training phase by scaling down the network output (Epsilon), mitigating the input mismatch between training and sampling. Experiments on various diffusion framework
    
[^19]: 使用二进制神经网络进行设备上的学习

    On-Device Learning with Binary Neural Networks. (arXiv:2308.15308v1 [cs.LG])

    [http://arxiv.org/abs/2308.15308](http://arxiv.org/abs/2308.15308)

    本文提出了一种在低功耗设备上使用二进制神经网络进行设备端学习的解决方案，该方法结合了最新的连续学习技术和二进制神经网络的高效性。实验证实了该方法的有效性和适用性。

    

    现有的连续学习解决方案在低功耗嵌入式CPU上部署时，仅部分解决了深度学习模型在功耗、内存和计算方面的限制。本文提出了一种连续学习解决方案，结合了连续学习领域的最新进展和二进制神经网络(BNN)的高效性，BNN使用1位用于权重和激活以高效执行深度学习模型。我们提出了一种CWR*的混合量化方法（一种有效的连续学习方法），该方法在前向传递和反向传递过程中分别考虑，以在梯度更新步骤中保持更高的精度，并同时最大限度地减少延迟开销。选择二进制网络作为骨干网络对于满足低功耗设备的限制至关重要，据作者所知，这是首次尝试证明使用BNN进行设备上的学习。进行的实验验证了该方法的有效性和适用性。

    Existing Continual Learning (CL) solutions only partially address the constraints on power, memory and computation of the deep learning models when deployed on low-power embedded CPUs. In this paper, we propose a CL solution that embraces the recent advancements in CL field and the efficiency of the Binary Neural Networks (BNN), that use 1-bit for weights and activations to efficiently execute deep learning models. We propose a hybrid quantization of CWR* (an effective CL approach) that considers differently forward and backward pass in order to retain more precision during gradient update step and at the same time minimizing the latency overhead. The choice of a binary network as backbone is essential to meet the constraints of low power devices and, to the best of authors' knowledge, this is the first attempt to prove on-device learning with BNN. The experimental validation carried out confirms the validity and the suitability of the proposed method.
    
[^20]: KGConv，基于Wikidata的对话语料库

    KGConv, a Conversational Corpus grounded in Wikidata. (arXiv:2308.15298v1 [cs.CL])

    [http://arxiv.org/abs/2308.15298](http://arxiv.org/abs/2308.15298)

    KGConv是一个基于Wikidata的大型对话语料库，每个对话都基于一个事实，并提供了多个变体的问题。它可以用于知识对话问题生成和其他相关任务。

    

    我们提出了KGConv，一个包含71k个对话的大型对话语料库，每个问题-回答对都基于Wikidata中的一个事实。每个对话平均含有8.6个问题，并为每个Wikidata事实提供多个变体(平均12个)，这些变体使用模板、人工注释、手工规则和问题重写神经模型生成。我们为基于知识的对话问题生成任务提供了基线。KGConv还可用于其他生成和分析任务，例如从Wikidata三元组生成单轮问题、问题重写、从对话或知识图中回答问题以及生成测验题。

    We present KGConv, a large, conversational corpus of 71k conversations where each question-answer pair is grounded in a Wikidata fact. Conversations contain on average 8.6 questions and for each Wikidata fact, we provide multiple variants (12 on average) of the corresponding question using templates, human annotations, hand-crafted rules and a question rewriting neural model. We provide baselines for the task of Knowledge-Based, Conversational Question Generation. KGConv can further be used for other generation and analysis tasks such as single-turn question generation from Wikidata triples, question rewriting, question answering from conversation or from knowledge graphs and quiz generation.
    
[^21]: 一种混合成员潜在距离模型用于无符号和有符号整数加权网络

    A Hybrid Membership Latent Distance Model for Unsigned and Signed Integer Weighted Networks. (arXiv:2308.15293v1 [cs.SI])

    [http://arxiv.org/abs/2308.15293](http://arxiv.org/abs/2308.15293)

    本文提出了一种混合成员潜在距离模型（HM-LDM）和有符号混合成员-潜在距离模型（sHM-LDM），通过控制潜在空间的体积，揭示了网络中的社区结构，并引导节点之间的链接关系。

    

    图表示学习（GRL）已成为进一步理解复杂网络的重要工具，提供了网络嵌入、链接预测和节点分类的技巧。在本文中，我们提出了混合成员-潜在距离模型（HM-LDM），通过探索如何将潜在距离模型（LDM）限制在一个潜在的单纯形上。通过控制单纯形角点的边长，可以系统地控制潜在空间的体积。随着空间越来越受限，社区将被揭示出来，当单纯形体积趋近于零时，也可以恢复硬成员关系。我们进一步探索了一种最近针对有符号网络的似然函数公式，利用Skellam分布考虑了有符号加权网络，并将HM-LDM扩展到有符号混合成员-潜在距离模型（sHM-LDM）。重要的是，引导似然函数明确地吸引那些具有正链接的节点，并阻止节点具有负链接。

    Graph representation learning (GRL) has become a prominent tool for furthering the understanding of complex networks providing tools for network embedding, link prediction, and node classification. In this paper, we propose the Hybrid Membership-Latent Distance Model (HM-LDM) by exploring how a Latent Distance Model (LDM) can be constrained to a latent simplex. By controlling the edge lengths of the corners of the simplex, the volume of the latent space can be systematically controlled. Thereby communities are revealed as the space becomes more constrained, with hard memberships being recovered as the simplex volume goes to zero. We further explore a recent likelihood formulation for signed networks utilizing the Skellam distribution to account for signed weighted networks and extend the HM-LDM to the signed Hybrid Membership-Latent Distance Model (sHM-LDM). Importantly, the induced likelihood function explicitly attracts nodes with positive links and deters nodes from having negative 
    
[^22]: 让LLM能够使用智能手机进行智能任务自动化

    Empowering LLM to use Smartphone for Intelligent Task Automation. (arXiv:2308.15272v1 [cs.AI])

    [http://arxiv.org/abs/2308.15272](http://arxiv.org/abs/2308.15272)

    本论文提出了AutoDroid，一个移动任务自动化系统，可以在任何Android应用程序上自动处理任意任务。它通过结合LLMs的常识知识和应用的领域特定知识来实现，通过自动化的动态分析来实现功能意识的UI表示方法和基于探索的内存注入技术。

    

    移动任务自动化是一种吸引人的技术，旨在实现基于语音的免提用户与智能手机的交互。然而，现有的方法由于语言理解能力有限，以及开发人员或终端用户需要付出非常努力的手动工作而导致可扩展性差。最近大型语言模型（LLMs）在语言理解和推理方面的进展激发了我们从模型中心化的角度重新思考这个问题，即通过统一的语言模型处理任务准备、理解和执行。在这项工作中，我们介绍了AutoDroid，这是一个能够在任何Android应用程序上无需手动工作处理任意任务的移动任务自动化系统。关键洞察力是通过自动化的动态分析将LLMs的常识知识与应用的领域特定知识相结合。主要组件包括功能意识的UI表示方法，桥接了UI和LLM，基于探索的内存注入技术

    Mobile task automation is an attractive technique that aims to enable voice-based hands-free user interaction with smartphones. However, existing approaches suffer from poor scalability due to the limited language understanding ability and the non-trivial manual efforts required from developers or end-users. The recent advance of large language models (LLMs) in language understanding and reasoning inspires us to rethink the problem from a model-centric perspective, where task preparation, comprehension, and execution are handled by a unified language model. In this work, we introduce AutoDroid, a mobile task automation system that can handle arbitrary tasks on any Android application without manual efforts. The key insight is to combine the commonsense knowledge of LLMs and domain-specific knowledge of apps through automated dynamic analysis. The main components include a functionality-aware UI representation method that bridges the UI with the LLM, exploration-based memory injection t
    
[^23]: 让声音存在：从无声视频中重建高质量语音

    Let There Be Sound: Reconstructing High Quality Speech from Silent Videos. (arXiv:2308.15256v1 [eess.AS])

    [http://arxiv.org/abs/2308.15256](http://arxiv.org/abs/2308.15256)

    本文介绍了一个重建高质量语音的唇语转语音系统，通过解决一对多映射问题和细节精炼来显著改进生成质量。

    

    本研究的目标是仅通过唇运动重建高质量的语音，也被称为唇语转语音。唇语转语音系统的一个关键挑战是由于同形异音和多样化语音变化而造成的一对多映射，导致发音错误和过度平滑的语音。在本文中，我们提出了一种新颖的唇语转语音系统，通过从多个角度缓解一对多映射问题，显著改进了生成质量。具体来说，我们结合了（1）自我监督的语音表示来消除同形异音，和（2）声学变异信息来建模多样化的语音风格。此外，为了更好地解决上述问题，我们采用了基于流的后处理网络，捕捉和精炼所生成语音的细节。我们进行了大量实验，并证明我们的方法实现了接近真实人类语音的生成质量，超过了现有方法。

    The goal of this work is to reconstruct high quality speech from lip motions alone, a task also known as lip-to-speech. A key challenge of lip-to-speech systems is the one-to-many mapping caused by (1) the existence of homophenes and (2) multiple speech variations, resulting in a mispronounced and over-smoothed speech. In this paper, we propose a novel lip-to-speech system that significantly improves the generation quality by alleviating the one-to-many mapping problem from multiple perspectives. Specifically, we incorporate (1) self-supervised speech representations to disambiguate homophenes, and (2) acoustic variance information to model diverse speech styles. Additionally, to better solve the aforementioned problem, we employ a flow based post-net which captures and refines the details of the generated speech. We perform extensive experiments and demonstrate that our method achieves the generation quality close to that of real human utterance, outperforming existing methods in term
    
[^24]: 基于知识的多重自适应空间融合推荐方法

    Knowledge-based Multiple Adaptive Spaces Fusion for Recommendation. (arXiv:2308.15244v1 [cs.IR])

    [http://arxiv.org/abs/2308.15244](http://arxiv.org/abs/2308.15244)

    该论文提出了一种基于知识的多重自适应空间融合推荐方法，通过统一的空间来融合双曲、欧几里得和球面空间，并利用注意力机制提高了知识传播的嵌入质量。

    

    鉴于知识图谱包含丰富的语义信息，近年来一些KG-enhanced推荐方法层出不穷。现有方法大多是基于欧几里得空间设计的，没有考虑曲率。然而，最近的研究表明，巨大的图结构数据表现出高度非欧几里得的特性。在这项工作中，我们受到这些观察的启发，提出了一种基于知识的多重自适应空间融合推荐方法，称为MCKG。与现有方法仅采用特定流形的方法不同，我们引入了与双曲、欧几里得和球面空间兼容的统一空间。此外，在注意力方式下，我们融合了多个统一空间，以获取更好的知识传播的高质量嵌入。此外，我们提出了一种几何感知优化策略，使得拉和推过程可以充分利用双曲和球面空间的优势。具体而言，在双曲空间中，

    Since Knowledge Graphs (KGs) contain rich semantic information, recently there has been an influx of KG-enhanced recommendation methods. Most of existing methods are entirely designed based on euclidean space without considering curvature. However, recent studies have revealed that a tremendous graph-structured data exhibits highly non-euclidean properties. Motivated by these observations, in this work, we propose a knowledge-based multiple adaptive spaces fusion method for recommendation, namely MCKG. Unlike existing methods that solely adopt a specific manifold, we introduce the unified space that is compatible with hyperbolic, euclidean and spherical spaces. Furthermore, we fuse the multiple unified spaces in an attention manner to obtain the high-quality embeddings for better knowledge propagation. In addition, we propose a geometry-aware optimization strategy which enables the pull and push processes benefited from both hyperbolic and spherical spaces. Specifically, in hyperbolic 
    
[^25]: 低代码平台中的自然语言转SQL

    Natural language to SQL in low-code platforms. (arXiv:2308.15239v1 [cs.AI])

    [http://arxiv.org/abs/2308.15239](http://arxiv.org/abs/2308.15239)

    本研究提出了一种自然语言转SQL的管道，允许低代码平台开发人员使用自然语言检索数据库中的数据。通过收集和标注大量数据，训练了一个NL模型来生成SQL，并使用反馈循环不断优化模型。通过A/B测试，观察到特性采用率提高了240%，参与率提高了220%。

    

    低代码平台开发人员面临的最大挑战之一是使用SQL查询从数据库中检索数据。我们提出了一种管道，允许开发人员编写自然语言（NL）来检索数据。在这项研究中，我们收集、标注和验证了OutSystems用户最常使用的SQL查询所涉及的数据。我们使用这些数据来训练一个NL模型来生成SQL。同时，我们描述了整个管道，其中包括一个反馈循环，使我们能够快速收集生产数据，并用它来重新训练我们的SQL生成模型。通过众包，我们收集了26k个NL和SQL对，并从生产数据中获得了额外的1k对。最后，我们开发了一个用户界面，允许开发人员在提示中输入NL查询，并获得对应SQL查询的用户友好表示。我们使用A/B测试来比较生产中的四个不同模型，观察到特性采用率提高了240%，参与率提高了220%。

    One of the developers' biggest challenges in low-code platforms is retrieving data from a database using SQL queries. Here, we propose a pipeline allowing developers to write natural language (NL) to retrieve data. In this study, we collect, label, and validate data covering the SQL queries most often performed by OutSystems users. We use that data to train a NL model that generates SQL. Alongside this, we describe the entire pipeline, which comprises a feedback loop that allows us to quickly collect production data and use it to retrain our SQL generation model. Using crowd-sourcing, we collect 26k NL and SQL pairs and obtain an additional 1k pairs from production data. Finally, we develop a UI that allows developers to input a NL query in a prompt and receive a user-friendly representation of the resulting SQL query. We use A/B testing to compare four different models in production and observe a 240% improvement in terms of adoption of the feature, 220% in terms of engagement rate, a
    
[^26]: PronounFlow:一种用于校准句子中代词的混合方法

    PronounFlow: A Hybrid Approach for Calibrating Pronouns in Sentences. (arXiv:2308.15235v1 [cs.CL])

    [http://arxiv.org/abs/2308.15235](http://arxiv.org/abs/2308.15235)

    PronounFlow提出了一种混合方法来校准句子中的代词，以消除歧义。这对于使机器具备常识和推理能力具有重要意义。

    

    翻阅任何一本书或听任何一首歌词，你会遇到在某些情况下会阻碍理解的代词，特别是对于机器来说。随着认知机器在我们生活中的普及，许多系统已经被开发出来以解决各种挑战下的代词歧义。因此，人们认为能够消除句子中的代词歧义的系统将有助于使机器具备与人类相似的常识和推理能力。然而，这些系统在现代英语中面临的一个问题是缺乏性别代词，人们试图通过使用男性、女性或复数来避免整个问题的出现。由于人类的目标是构建出与人类类似的完整意义上的系统，那么当书面文本中的代词(如复数或中性代词)指的是性别不一定已知的未指定实体时会发生什么呢？

    Flip through any book or listen to any song lyrics, and you will come across pronouns that, in certain cases, can hinder meaning comprehension, especially for machines. As the role of having cognitive machines becomes pervasive in our lives, numerous systems have been developed to resolve pronouns under various challenges. Commensurate with this, it is believed that having systems able to disambiguate pronouns in sentences will help towards the endowment of machines with commonsense and reasoning abilities like those found in humans. However, one problem these systems face with modern English is the lack of gender pronouns, where people try to alternate by using masculine, feminine, or plural to avoid the whole issue. Since humanity aims to the building of systems in the full-bodied sense we usually reserve for people, what happens when pronouns in written text, like plural or epicene ones, refer to unspecified entities whose gender is not necessarily known? Wouldn't that put extra bar
    
[^27]: 使用变分自动编码器为以前未出现的用户提供公平推荐

    Providing Previously Unseen Users Fair Recommendations Using Variational Autoencoders. (arXiv:2308.15230v1 [cs.IR])

    [http://arxiv.org/abs/2308.15230](http://arxiv.org/abs/2308.15230)

    本论文提出了一种使用变分自动编码器的新方法，通过限制人口统计信息的编码来减少推荐系统中的歧视，从而为以前未出现的用户提供公平推荐。

    

    机器学习中关于公平性的新定义要求模型对用户的人口统计信息不可见，例如，用户的性别或年龄不应影响模型。个性化推荐系统特别容易通过其显式的用户关注和用户建模来违反这个定义。显式的用户建模也是许多推荐系统无法为以前未出现的用户提供推荐的原因。我们提出了一种限制人口统计信息编码的新方法来减少基于变分自动编码器的推荐系统中的歧视。这些方法能够在评估中为未在训练数据中出现的用户提供公平推荐。

    An emerging definition of fairness in machine learning requires that models are oblivious to demographic user information, e.g., a user's gender or age should not influence the model. Personalized recommender systems are particularly prone to violating this definition through their explicit user focus and user modelling. Explicit user modelling is also an aspect that makes many recommender systems incapable of providing hitherto unseen users with recommendations. We propose novel approaches for mitigating discrimination in Variational Autoencoder-based recommender systems by limiting the encoding of demographic information. The approaches are capable of, and evaluated on, providing users that are not represented in the training data with fair recommendations.
    
[^28]: CLIPTrans：使用预训练模型转移视觉知识进行多模态机器翻译

    CLIPTrans: Transferring Visual Knowledge with Pre-trained Models for Multimodal Machine Translation. (arXiv:2308.15226v1 [cs.CV])

    [http://arxiv.org/abs/2308.15226](http://arxiv.org/abs/2308.15226)

    本研究提出了CLIPTrans，它通过简单地适应独立预训练模型，实现了多模态机器翻译中视觉知识的转移。

    

    近年来，开发增强神经机器翻译（NMT）的多模态机器翻译（MMT）系统以提高翻译质量的兴趣逐渐增长。这一问题设置涉及在训练过程中使用图像作为辅助信息，并且最近在推理过程中消除它们的使用。然而，之前的工作在从头开始训练强大的MMT模型时面临了一个挑战，因为多语言视觉语言数据的标注稀缺，尤其是对于低资源语言。与此同时，针对NMT的多语言预训练模型和针对视觉语言任务的多模态预训练模型大量涌现，主要针对英文，它们展现了出色的泛化能力。然而，这些模型对于MMT并不直接适用，因为它们没有为生成任务提供对齐的多模态多语言特征。为了解决这个问题，我们提出了CLIPTrans，它不像设计复杂的MMT模块，而是简单地适应独立预训练模型，

    There has been a growing interest in developing multimodal machine translation (MMT) systems that enhance neural machine translation (NMT) with visual knowledge. This problem setup involves using images as auxiliary information during training, and more recently, eliminating their use during inference. Towards this end, previous works face a challenge in training powerful MMT models from scratch due to the scarcity of annotated multilingual vision-language data, especially for low-resource languages. Simultaneously, there has been an influx of multilingual pre-trained models for NMT and multimodal pre-trained models for vision-language tasks, primarily in English, which have shown exceptional generalisation ability. However, these are not directly applicable to MMT since they do not provide aligned multimodal multilingual features for generative tasks. To alleviate this issue, instead of designing complex modules for MMT, we propose CLIPTrans, which simply adapts the independently pre-
    
[^29]: 从DDMs到DNNs：利用决策过程的数据和模型来改善人工智能与人类之间的交互

    From DDMs to DNNs: Using process data and models of decision-making to improve human-AI interactions. (arXiv:2308.15225v1 [q-bio.NC])

    [http://arxiv.org/abs/2308.15225](http://arxiv.org/abs/2308.15225)

    本论文提出利用决策过程数据和模型改善人工智能与人类之间的交互。通过详细描述决策过程和建立决策演变模型，可以揭示潜在的偏好，同时追踪决策过程的数据可以提供重要信息，从而改善人工智能的预测能力。

    

    在过去的几十年中，认知神经科学家和行为经济学家已经认识到详细描述决策过程和建立决策随时间演变的模型的价值。例如，决策所需的时间可以揭示一个个体真正的潜在偏好，而不仅仅是决策本身。类似地，追踪决策过程的数据，如眼动或神经记录，包含了关键的信息，即使没有达成决策也可以被利用。在这里，我们认为人工智能研究应更加关注决策如何随时间演变以及如何融入相关的过程数据来改善人工智能的预测，特别是在人与人工智能之间的交互中。首先，我们介绍了一个非常成熟的计算框架，该框架认为决策是从杂音累积的证据中产生的，并介绍了相关的心理学、神经科学和经济学的实证研究。

    Over the past decades, cognitive neuroscientists and behavioral economists have recognized the value of describing the process of decision making in detail and modeling the emergence of decisions over time. For example, the time it takes to decide can reveal more about an agents true hidden preferences than only the decision itself. Similarly, data that track the ongoing decision process such as eye movements or neural recordings contain critical information that can be exploited, even if no decision is made. Here, we argue that artificial intelligence (AI) research would benefit from a stronger focus on insights about how decisions emerge over time and incorporate related process data to improve AI predictions in general and human-AI interactions in particular. First, we introduce a highly established computational framework that assumes decisions to emerge from the noisy accumulation of evidence, and we present related empirical work in psychology, neuroscience, and economics. Next, 
    
[^30]: FurChat: 使用LLMs的具有脸部表情的交互式对话系统，结合开放和封闭领域对话

    FurChat: An Embodied Conversational Agent using LLMs, Combining Open and Closed-Domain Dialogue with Facial Expressions. (arXiv:2308.15214v1 [cs.CL])

    [http://arxiv.org/abs/2308.15214](http://arxiv.org/abs/2308.15214)

    本研究开发了一个交互式对话系统，将开放和封闭领域对话、脸部表情结合起来，通过使用LLMs和GPT-3.5模型来生成引人入胜的对话，以提供信息并与访客进行自然交流。

    

    我们展示了一个交互式对话系统，可以作为接待员，生成结合开放和封闭领域对话以及脸部表情的混合对话。通过使用大型语言模型（LLM）来开发引人入胜的对话，我们将该系统部署到了一个高度表达力的Furhat机器人上，在互动过程中使用了口头和非语言提示。该系统专门为国家机器人实验室设计，通过自然对话与访客进行交互，并向他们提供有关设施、研究、新闻、即将举行的活动等方面的信息。系统利用最先进的GPT-3.5模型根据提示生成这些信息，同时生成领域通用的对话和面部表情。

    We demonstrate an embodied conversational agent that can function as a receptionist and generate a mixture of open and closed-domain dialogue along with facial expressions, by using a large language model (LLM) to develop an engaging conversation. We deployed the system onto a Furhat robot, which is highly expressive and capable of using both verbal and nonverbal cues during interaction. The system was designed specifically for the National Robotarium to interact with visitors through natural conversations, providing them with information about the facilities, research, news, upcoming events, etc. The system utilises the state-of-the-art GPT-3.5 model to generate such information along with domain-general conversations and facial expressions based on prompt engineering.
    
[^31]: 共享词汇作为代码切换的触发器

    Shared Lexical Items as Triggers of Code Switching. (arXiv:2308.15209v1 [cs.CL])

    [http://arxiv.org/abs/2308.15209](http://arxiv.org/abs/2308.15209)

    该研究通过使用三种语言对的五个大型数据集，对共享词汇作为代码切换的触发器进行了深入探索。结果表明，共享词汇确实会触发代码切换，切换的倾向取决于触发器与切换点的距离和位置，而不取决于触发词的词源。

    

    为什么双语者要进行代码切换（混合两种语言）？在解释这种自然和普遍现象的几种理论中，触发假说将代码切换与切换点附近的词汇触发器（特别是同源词和专有名词）联系起来。我们基于三种语言对的五个大型数据集，反映了口语和书面双语交流，对触发假说进行了更全面、更细致、更精细的探索。我们的结果显示，被认为同时存在于两种语言的共享心理词汇中的词确实触发代码切换；切换的倾向取决于触发器与切换点的距离；以及触发器是在切换之前还是之后；但不取决于触发词的词源。因此，我们对词汇触发器与代码切换之间的关系提供了强有力、稳定、以证据为基础的证实。

    Why do bilingual speakers code-switch (mix their two languages)? Among the several theories that attempt to explain this natural and ubiquitous phenomenon, the Triggering Hypothesis relates code-switching to the presence of lexical triggers, specifically cognates and proper names, adjacent to the switch point. We provide a fuller, more nuanced and refined exploration of the triggering hypothesis, based on five large datasets in three language pairs, reflecting both spoken and written bilingual interactions. Our results show that words that are assumed to reside in a mental lexicon shared by both languages indeed trigger code-switching; that the tendency to switch depends on the distance of the trigger from the switch point; and on whether the trigger precedes or succeeds the switch; but not on the etymology of the trigger words. We thus provide strong, robust, evidence-based confirmation to several hypotheses on the relationships between lexical triggers and code-switching.
    
[^32]: 下一个去哪里？大型语言模型作为人类移动预测器。

    Where Would I Go Next? Large Language Models as Human Mobility Predictors. (arXiv:2308.15197v1 [cs.AI])

    [http://arxiv.org/abs/2308.15197](http://arxiv.org/abs/2308.15197)

    本文研究了大型语言模型在人类移动预测任务中的潜力，提出了一种新方法LLM-Mob，通过利用语言模型的语言理解和推理能力分析人类移动数据，并引入历史停留和上下文停留的概念来捕捉长期和短期依赖关系，实现时态感知预测。

    

    准确的人类移动预测在许多重要应用中起着关键作用，包括流行病建模、交通规划和应急响应。由于移动数据的稀疏性和人们日常活动的随机性，实现对人们位置的精确预测仍然是一个挑战。虽然最近开发的大型语言模型（LLMs）在许多与语言相关的任务中展现出卓越性能，但它们在人类移动研究中的适用性尚未被探索。为了填补这一空白，本文探讨了LLMs在人类移动预测任务中的潜力。我们引入了一种新方法LLM-Mob，它利用LLMs的语言理解和推理能力来分析人类移动数据。我们提出了历史停留和上下文停留的概念，以捕捉人类移动中的长期和短期依赖关系，并通过使用时间信息来实现时态感知预测。

    Accurate human mobility prediction underpins many important applications across a variety of domains, including epidemic modelling, transport planning, and emergency responses. Due to the sparsity of mobility data and the stochastic nature of people's daily activities, achieving precise predictions of people's locations remains a challenge. While recently developed large language models (LLMs) have demonstrated superior performance across numerous language-related tasks, their applicability to human mobility studies remains unexplored. Addressing this gap, this article delves into the potential of LLMs for human mobility prediction tasks. We introduce a novel method, LLM-Mob, which leverages the language understanding and reasoning capabilities of LLMs for analysing human mobility data. We present concepts of historical stays and context stays to capture both long-term and short-term dependencies in human movement and enable time-aware prediction by using time information of the predic
    
[^33]: 合奏反事实解释器

    Ensemble of Counterfactual Explainers. (arXiv:2308.15194v1 [cs.AI])

    [http://arxiv.org/abs/2308.15194](http://arxiv.org/abs/2308.15194)

    该论文提出了一种合奏反事实解释器，可以提升弱解释器的性能，实现对反事实实例的最小化、可操作性、稳定性、多样性、合理性和辨别力的全覆盖。

    

    在可解释的人工智能（XAI）领域中，已经提出了几种反事实解释器，每种解释器都关注反事实实例的一些可取特性：最小化、可操作性、稳定性、多样性、合理性、辨别力。我们提出了一种合奏反事实解释器，它能够增强弱解释器的性能，这些弱解释器仅提供这些特性的一个子集，使其成为一种强大的方法涵盖所有特性。该合奏解释器在一些实例和特征的样本上运行弱解释器，并通过利用多样性驱动的选择函数来合并其结果。该方法是模型无关的，并且通过基于自动编码器的封装方法，也是数据无关的。

    In eXplainable Artificial Intelligence (XAI), several counterfactual explainers have been proposed, each focusing on some desirable properties of counterfactual instances: minimality, actionability, stability, diversity, plausibility, discriminative power. We propose an ensemble of counterfactual explainers that boosts weak explainers, which provide only a subset of such properties, to a powerful method covering all of them. The ensemble runs weak explainers on a sample of instances and of features, and it combines their results by exploiting a diversity-driven selection function. The method is model-agnostic and, through a wrapping approach based on autoencoders, it is also data-agnostic.
    
[^34]: 借助大型语言模型增强心理咨询：面向非专业人员的多方面决策支持系统

    Enhancing Psychological Counseling with Large Language Model: A Multifaceted Decision-Support System for Non-Professionals. (arXiv:2308.15192v1 [cs.AI])

    [http://arxiv.org/abs/2308.15192](http://arxiv.org/abs/2308.15192)

    本研究基于大型语言模型构建了一个决策支持系统，可帮助非专业人员在社交媒体上提供心理咨询，从而弥补了专业咨询师短缺的问题。

    

    在当今社交媒体的环境下，大量用户表达负面情绪，其中一些表现为强烈的自杀倾向。这种情况凸显了对训练有素的心理咨询师的迫切需求，他们可以实施有效的心理干预。然而，培养这些专业人员常常是一项必要但耗时的任务。因此，动员非专业人员或志愿者在这方面发挥作用成为一个紧迫的问题。利用人工智能的能力，尤其是大型语言模型的最新进展，为此挑战提供了可行的解决方案。本文介绍了一个基于大型语言模型构建的新型模型，以充分辅助非专业人员在在线用户交流中提供心理干预。该框架使得合理利用非专业咨询师的力量变得可行。进行了一项综合研究。

    In the contemporary landscape of social media, an alarming number of users express negative emotions, some of which manifest as strong suicidal intentions. This situation underscores a profound need for trained psychological counselors who can enact effective mental interventions. However, the development of these professionals is often an imperative but time-consuming task. Consequently, the mobilization of non-professionals or volunteers in this capacity emerges as a pressing concern. Leveraging the capabilities of artificial intelligence, and in particular, the recent advances in large language models, offers a viable solution to this challenge. This paper introduces a novel model constructed on the foundation of large language models to fully assist non-professionals in providing psychological interventions on online user discourses. This framework makes it plausible to harness the power of non-professional counselors in a meaningful way. A comprehensive study was conducted involvi
    
[^35]: 非确定性规划领域中的最佳努力综合研究

    LTLf Best-Effort Synthesis in Nondeterministic Planning Domains. (arXiv:2308.15188v1 [cs.AI])

    [http://arxiv.org/abs/2308.15188](http://arxiv.org/abs/2308.15188)

    本研究在完全可观察的非确定性规划领域中，使用线性时态逻辑在有限轨迹上表达目标的最佳努力策略。我们引入了游戏论技术来综合这些策略，该技术利用了非确定性规划领域的特征。实验证明，这种方法比基于重新表达规划领域的直接方法具有更高的可扩展性。

    

    我们研究了在完全可观察的非确定性领域中，使用线性时态逻辑在有限轨迹上表达目标的最佳努力策略（也称为计划）。最佳努力策略的概念被引入来应对当不存在一个代理策略能满足每个可能的非确定性环境反应的情况。这样的策略在可能的情况下实现目标，并在否则的情况下尽力而为。我们提出了一种利用非确定性规划领域特征综合最佳努力策略的博弈论技术。我们在理论上证明了它的正确性，并通过实验证明了它的有效性，相对于基于将规划领域重新表达为通用环境规范的直接最佳努力综合方法具有更高的可扩展性。

    We study best-effort strategies (aka plans) in fully observable nondeterministic domains (FOND) for goals expressed in Linear Temporal Logic on Finite Traces (LTLf). The notion of best-effort strategy has been introduced to also deal with the scenario when no agent strategy exists that fulfills the goal against every possible nondeterministic environment reaction. Such strategies fulfill the goal if possible, and do their best to do so otherwise. We present a game-theoretic technique for synthesizing best-effort strategies that exploit the specificity of nondeterministic planning domains. We formally show its correctness and demonstrate its effectiveness experimentally, exhibiting a much greater scalability with respect to a direct best-effort synthesis approach based on re-expressing the planning domain as generic environment specifications.
    
[^36]: LTLf综合在环境规范下的可达性和安全性属性

    LTLf Synthesis Under Environment Specifications for Reachability and Safety Properties. (arXiv:2308.15184v1 [cs.LO])

    [http://arxiv.org/abs/2308.15184](http://arxiv.org/abs/2308.15184)

    本文研究了LTLf综合在可达性和安全性属性的环境规范下的问题，并提供了完整的综合算法，其中一些情况是首次研究的。

    

    在本文中，我们研究了LTLf综合在可达性和安全性属性的环境规范下的问题。我们考虑了代理任务和环境规范的两种属性类型，并提供了完整的综合算法。针对每种情况，我们设计了一个特定的算法（问题复杂度最优）并证明了其正确性。这些算法以不同的方式组合了常见的构件。虽然有些情况已经在文献中研究过，但其他情况在本文中首次研究。

    In this paper, we study LTLf synthesis under environment specifications for arbitrary reachability and safety properties. We consider both kinds of properties for both agent tasks and environment specifications, providing a complete landscape of synthesis algorithms. For each case, we devise a specific algorithm (optimal wrt complexity of the problem) and prove its correctness. The algorithms combine common building blocks in different ways. While some cases are already studied in literature others are studied here for the first time.
    
[^37]: 符号化的LTLf最大努力合成

    Symbolic LTLf Best-Effort Synthesis. (arXiv:2308.15178v1 [cs.AI])

    [http://arxiv.org/abs/2308.15178](http://arxiv.org/abs/2308.15178)

    本文提出并比较了用于有限轨迹线性时态逻辑（LTLf）的最大努力合成的不同符号化方法，这些方法在组件的组合方式上有所不同，对方法的性能有重要影响。

    

    我们考虑一个在不确定环境中完成任务的代理。当不存在一种无论环境如何行动都能完成任务的策略时，代理至少应该避免采用阻止完成任务的策略。最大努力合成捕捉了这种直觉。在本文中，我们设计并比较了不同的基于有限轨迹线性时态逻辑（LTLf）的最大努力合成的符号化方法。这些方法基于相同的基本组件，但它们在如何组合这些组件上有所不同，而这对方法的性能有着显著影响，这也得到了我们的经验评估所证实。

    We consider an agent acting to fulfil tasks in a nondeterministic environment. When a strategy that fulfills the task regardless of how the environment acts does not exist, the agent should at least avoid adopting strategies that prevent from fulfilling its task. Best-effort synthesis captures this intuition. In this paper, we devise and compare various symbolic approaches for best-effort synthesis in Linear Temporal Logic on finite traces (LTLf). These approaches are based on the same basic components, however they change in how these components are combined, and this has a significant impact on the performance of the approaches as confirmed by our empirical evaluations.
    
[^38]: 一种基于位置地图数据的轻量级3D密集面部关键点估计模型

    A lightweight 3D dense facial landmark estimation model from position map data. (arXiv:2308.15170v1 [cs.CV])

    [http://arxiv.org/abs/2308.15170](http://arxiv.org/abs/2308.15170)

    提出了一种基于位置地图数据的轻量级3D密集面部关键点估计模型，通过预测整个脸部的密集3D关键点来解决无需3D数据的面部分析任务中的复杂问题。

    

    近年来，将3D数据整合到面部分析任务中变得越来越流行。尽管它提供了人脸更准确、详细的表示，但获取3D人脸数据比2D人脸图像更复杂、更昂贵。人们要么依赖昂贵的3D扫描仪或深度传感器，这些设备容易受到噪声的影响。另一种选择是在没有任何3D数据的基础上，以无监督的方式从非标定的2D图像重建3D人脸。然而，这种方法在计算上会很昂贵，而且所学习的模型尺寸不适合移动设备或其他边缘设备应用。通过预测整个脸部的密集3D关键点可以解决这个问题。由于没有包含密集关键点的公共数据集可用，我们提出了一种流程，从现有的面部位置地图数据中创建一个包含520个关键点的密集关键点训练数据集。我们使用生成的数据集来训练一个基于MobileNet的轻量级回归器模型。

    The incorporation of 3D data in facial analysis tasks has gained popularity in recent years. Though it provides a more accurate and detailed representation of the human face, accruing 3D face data is more complex and expensive than 2D face images. Either one has to rely on expensive 3D scanners or depth sensors which are prone to noise. An alternative option is the reconstruction of 3D faces from uncalibrated 2D images in an unsupervised way without any ground truth 3D data. However, such approaches are computationally expensive and the learned model size is not suitable for mobile or other edge device applications. Predicting dense 3D landmarks over the whole face can overcome this issue. As there is no public dataset available containing dense landmarks, we propose a pipeline to create a dense keypoint training dataset containing 520 key points across the whole face from an existing facial position map data. We train a lightweight MobileNet-based regressor model with the generated da
    
[^39]: 数字孪生中的本体论: 一项系统性文献综述

    Ontologies in Digital Twins: A Systematic Literature Review. (arXiv:2308.15168v1 [cs.AI])

    [http://arxiv.org/abs/2308.15168](http://arxiv.org/abs/2308.15168)

    本文是一项对数字孪生中本体论的系统性文献综述，通过分析82篇研究论文的途径，探讨了本体论在数字孪生中的应用和贡献，包括知识表示、互操作性和自动推理方面。

    

    数字孪生(DT)在网络物理系统中促进监控和推理过程。由于密集的研究活动和工业进展，它们在近年来逐渐受到关注。在DT中，本体和知识图在知识表示、互操作性和自动推理方面的重要性受到了研究。然而，目前还没有对语义技术，特别是本体论，在DT中的具体应用进行全面的分析。本文基于对82篇与DT相关的研究文章的分析，提出了一项系统性文献综述。论文采用不同的分析角度，包括基于参考DT架构的结构分析和基于应用领域的特定分析，以具体的方式处理不同领域的问题。

    Digital Twins (DT) facilitate monitoring and reasoning processes in cyber-physical systems. They have progressively gained popularity over the past years because of intense research activity and industrial advancements. Cognitive Twins is a novel concept, recently coined to refer to the involvement of Semantic Web technology in DTs. Recent studies address the relevance of ontologies and knowledge graphs in the context of DTs, in terms of knowledge representation, interoperability and automatic reasoning. However, there is no comprehensive analysis of how semantic technologies, and specifically ontologies, are utilized within DTs. This Systematic Literature Review (SLR) is based on the analysis of 82 research articles, that either propose or benefit from ontologies with respect to DT. The paper uses different analysis perspectives, including a structural analysis based on a reference DT architecture, and an application-specific analysis to specifically address the different domains, suc
    
[^40]: 使用强化学习和生成预训练模型在四足机器人上实现生动的灵活性和游戏性

    Lifelike Agility and Play on Quadrupedal Robots using Reinforcement Learning and Generative Pre-trained Models. (arXiv:2308.15143v1 [cs.RO])

    [http://arxiv.org/abs/2308.15143](http://arxiv.org/abs/2308.15143)

    该论文提出了一种使用生成模型和强化学习的框架，使四足机器人能够在复杂环境中像真实动物一样具有灵活性和策略。通过预训练生成模型，保留了动物行为的知识，并通过学习适应环境，克服挑战性的障碍。

    

    总结动物和人类的知识启发了机器人创新。在这项工作中，我们提出了一种框架，使四足机器人能够在复杂环境中像真实动物一样拥有生动的灵活性和策略。受到在语言和图像理解方面表现出色的大型预训练模型的启发，我们引入了先进的深度生成模型的能力，以生成模拟真实动物动作的运动控制信号。与传统控制器和端到端强化学习方法只针对特定任务不同，我们提出在动物运动数据集上预训练生成模型，以保留有表达力的动物行为知识。预训练模型拥有足够的原始级知识，但与环境无关。然后，在学习的后续阶段，通过穿越一些以前的方法很少考虑的具有挑战性的障碍，如穿过狭窄的空间等，使其适应环境。

    Summarizing knowledge from animals and human beings inspires robotic innovations. In this work, we propose a framework for driving legged robots act like real animals with lifelike agility and strategy in complex environments. Inspired by large pre-trained models witnessed with impressive performance in language and image understanding, we introduce the power of advanced deep generative models to produce motor control signals stimulating legged robots to act like real animals. Unlike conventional controllers and end-to-end RL methods that are task-specific, we propose to pre-train generative models over animal motion datasets to preserve expressive knowledge of animal behavior. The pre-trained model holds sufficient primitive-level knowledge yet is environment-agnostic. It is then reused for a successive stage of learning to align with the environments by traversing a number of challenging obstacles that are rarely considered in previous approaches, including creeping through narrow sp
    
[^41]: 受文字语义信息辅助的多模态视觉编码模型

    A Multimodal Visual Encoding Model Aided by Introducing Verbal Semantic Information. (arXiv:2308.15142v1 [cs.CV])

    [http://arxiv.org/abs/2308.15142](http://arxiv.org/abs/2308.15142)

    本文提出了一种多模态视觉信息编码网络模型，通过整合文字语义信息和图像特征，实现了对视觉信息的编码。该模型通过Transformer网络对图像和文本特征进行对齐，构建了一个多模态特征空间，并通过卷积网络将特征映射到体素空间，从而实现了多模态视觉信息编码。

    

    生物学研究发现，在大脑皮层中，文字语义信息作为一种额外的来源参与非语言语义任务，比如视觉编码。然而，先前的视觉编码模型没有整合文字语义信息，与这一生物学发现相矛盾。本文针对这个问题提出了一种基于刺激图像和相关文本信息的多模态视觉信息编码网络模型。我们的视觉信息编码网络模型以刺激图像为输入，并利用由文本-图像生成模型生成的文本信息作为文字语义信息。这种方法将新的信息注入到视觉编码模型中。随后，一个Transformer网络对图像和文本特征信息进行对齐，创建一个多模态特征空间。然后，一个卷积网络将从这个多模态特征空间映射到体素空间，构建多模态视觉信息编码网络模型。

    Biological research has revealed that the verbal semantic information in the brain cortex, as an additional source, participates in nonverbal semantic tasks, such as visual encoding. However, previous visual encoding models did not incorporate verbal semantic information, contradicting this biological finding. This paper proposes a multimodal visual information encoding network model based on stimulus images and associated textual information in response to this issue. Our visual information encoding network model takes stimulus images as input and leverages textual information generated by a text-image generation model as verbal semantic information. This approach injects new information into the visual encoding model. Subsequently, a Transformer network aligns image and text feature information, creating a multimodal feature space. A convolutional network then maps from this multimodal feature space to voxel space, constructing the multimodal visual information encoding network model
    
[^42]: 基于特征金字塔网络和空间循环神经网络的腹部多器官分割

    Abdominal Multi-Organ Segmentation Based on Feature Pyramid Network and Spatial Recurrent Neural Network. (arXiv:2308.15137v1 [cs.CV])

    [http://arxiv.org/abs/2308.15137](http://arxiv.org/abs/2308.15137)

    这项研究提出了一种新的腹部多器官分割模型，结合了特征金字塔网络（FPN）和空间循环神经网络（SRNN），以加速超声图像分割并减轻超声检查师的负担。

    

    随着人工智能的进展导致传统诊断方法的衰退，端到端诊断的实现正在迅速接近。超声图像分割是诊断过程中的重要步骤。准确和稳健的分割模型可以加速过程并减轻超声检查师的负担。与以往的研究不同，我们考虑了超声图像的两个内在特征：（1）不同器官和组织的空间尺寸变化，（2）人体内部的解剖结构形成相对恒定的空间关系。基于这两个观点，我们提出了一种新的图像分割模型，结合了特征金字塔网络（FPN）和空间循环神经网络（SRNN）。我们讨论了为什么使用FPN提取不同尺度的解剖结构以及如何实现SRNN来提取腹部超声图像中的空间上下文特征。

    As recent advances in AI are causing the decline of conventional diagnostic methods, the realization of end-to-end diagnosis is fast approaching. Ultrasound image segmentation is an important step in the diagnostic process. An accurate and robust segmentation model accelerates the process and reduces the burden of sonographers. In contrast to previous research, we take two inherent features of ultrasound images into consideration: (1) different organs and tissues vary in spatial sizes, (2) the anatomical structures inside human body form a relatively constant spatial relationship. Based on those two ideas, we propose a new image segmentation model combining Feature Pyramid Network (FPN) and Spatial Recurrent Neural Network (SRNN). We discuss why we use FPN to extract anatomical structures of different scales and how SRNN is implemented to extract the spatial context features in abdominal ultrasound images.
    
[^43]: 大型视觉语言模型中幻觉的评估与分析

    Evaluation and Analysis of Hallucination in Large Vision-Language Models. (arXiv:2308.15126v1 [cs.LG])

    [http://arxiv.org/abs/2308.15126](http://arxiv.org/abs/2308.15126)

    本文提出了基于大型语言模型的幻觉评估框架HaELM，可以评估大型视觉语言模型中的幻觉问题，并分析了导致幻觉的因素，并提出了缓解幻觉问题的建议。

    

    最近，大型视觉语言模型（LVLMs）取得了显著的成功。然而，LVLMs仍然存在幻觉问题，这限制了在许多场景中的实用性。幻觉指的是LVLMs响应中不存在于视觉输入中的信息，这可能导致重大后果的潜在风险。目前对LVLMs中的幻觉评估的研究工作有限。在本文中，我们提出了基于大型语言模型（LLM）的幻觉评估框架HaELM。HaELM的性能近似于ChatGPT的95%，并具有低成本、可复现、保护隐私和本地部署等额外优势。利用HaELM，我们评估了当前LVLMs中的幻觉。此外，我们分析了导致LVLMs中幻觉的因素，并提出了缓解幻觉问题的有用建议。

    Large Vision-Language Models (LVLMs) have recently achieved remarkable success. However, LVLMs are still plagued by the hallucination problem, which limits the practicality in many scenarios. Hallucination refers to the information of LVLMs' responses that does not exist in the visual input, which poses potential risks of substantial consequences. There has been limited work studying hallucination evaluation in LVLMs. In this paper, we propose Hallucination Evaluation based on Large Language Models (HaELM), an LLM-based hallucination evaluation framework. HaELM achieves an approximate 95% performance comparable to ChatGPT and has additional advantages including low cost, reproducibility, privacy preservation and local deployment. Leveraging the HaELM, we evaluate the hallucination in current LVLMs. Furthermore, we analyze the factors contributing to hallucination in LVLMs and offer helpful suggestions to mitigate the hallucination problem. Our training data and human annotation halluci
    
[^44]: 基于人工智能的面部情绪识别教育解决方案：教师用户和其他类别的研究

    AI-Based Facial Emotion Recognition Solutions for Education: A Study of Teacher-User and Other Categories. (arXiv:2308.15119v1 [cs.AI])

    [http://arxiv.org/abs/2308.15119](http://arxiv.org/abs/2308.15119)

    本文提出了基于人工智能的面部情绪识别教育解决方案，将教师用户分为定向、状况和偏好三类，并整理了FER解决方案的技术和应用两类，对教育领域具有重要意义。

    

    现有的关于基于人工智能的面部情绪识别（FER）的信息对于计算机科学领域以外的人来说不易理解，需要跨学科的努力来确定一个促进对这项技术及其对用户的影响的理解的分类框架。大部分支持者将FER按照方法论、实施和分析的角度进行分类，相对较少将其应用于教育领域，而没有将其用户进行分类。本文主要关注教育中FER工具的（潜在）教师用户。它提出了一个基于情感教育目标和相关理论的经典分类法，将这些教师用户分为定向、状况和偏好三类，并将文献中发现或推导出的FER解决方案类型进行整理和归类为“技术”和“应用”两类，作为构建提出的“教师用户”类别的前提。这项工作对支持者，评论者以及教育政策制定者都有着重要的意义。

    Existing information on AI-based facial emotion recognition (FER) is not easily comprehensible by those outside the field of computer science, requiring cross-disciplinary effort to determine a categorisation framework that promotes the understanding of this technology, and its impact on users. Most proponents classify FER in terms of methodology, implementation and analysis; relatively few by its application in education; and none by its users. This paper is concerned primarily with (potential) teacher-users of FER tools for education. It proposes a three-part classification of these teachers, by orientation, condition and preference, based on a classical taxonomy of affective educational objectives, and related theories. It also compiles and organises the types of FER solutions found in or inferred from the literature into "technology" and "applications" categories, as a prerequisite for structuring the proposed "teacher-user" category. This work has implications for proponents', cri
    
[^45]: 通过Mixup增强的元学习方法实现蛋白质模拟器的高效微调

    Mixup-Augmented Meta-Learning for Sample-Efficient Fine-Tuning of Protein Simulators. (arXiv:2308.15116v1 [cs.LG])

    [http://arxiv.org/abs/2308.15116](http://arxiv.org/abs/2308.15116)

    本文通过Mixup增强的元学习方法实现了对蛋白质模拟器的高效微调，可以在有限的训练数据下泛化到未见过的场景，并提供了一种通用的模拟连续动态条件的方法。

    

    分子动力学模拟已经成为研究生物分子的基本工具。与此同时，我们希望在分子能够波动的各种条件下对一组粒子进行模拟。本文中，我们将软提示学习方法应用于分子动力学任务并进行了适应性探索。我们的模型可以在有限的训练数据下非常好地泛化到未见过的和超出分布的场景。虽然我们的工作以温度为测试案例，但我们的方法的多功能性使其可以通过任何连续的动态条件（如压力和体积）进行有效模拟。我们的框架有两个阶段：1）使用数据混合技术进行预训练，增强分子结构数据和温度提示，然后通过逐渐增加比例的方式应用课程学习方法。2）基于元学习的微调框架提高了微调过程的样本效率，并为软提示微调提供更好的表现。

    Molecular dynamics simulations have emerged as a fundamental instrument for studying biomolecules. At the same time, it is desirable to perform simulations of a collection of particles under various conditions in which the molecules can fluctuate. In this paper, we explore and adapt the soft prompt-based learning method to molecular dynamics tasks. Our model can remarkably generalize to unseen and out-of-distribution scenarios with limited training data. While our work focuses on temperature as a test case, the versatility of our approach allows for efficient simulation through any continuous dynamic conditions, such as pressure and volumes. Our framework has two stages: 1) Pre-trains with data mixing technique, augments molecular structure data and temperature prompts, then applies a curriculum learning method by increasing the ratio of them smoothly. 2) Meta-learning-based fine-tuning framework improves sample-efficiency of fine-tuning process and gives the soft prompt-tuning better 
    
[^46]: 从可解释模型中实现概率数据集重建

    Probabilistic Dataset Reconstruction from Interpretable Models. (arXiv:2308.15099v1 [cs.AI])

    [http://arxiv.org/abs/2308.15099](http://arxiv.org/abs/2308.15099)

    本文提出了一个新的框架，实现了从可解释模型中概率重建数据集。在现实假设下，可以高效计算重建的不确定性，以量化信息泄漏的隐私影响。

    

    可解释性经常被认为是可信赖机器学习的关键要求。然而，学习和发布本质上可解释的模型会泄露有关底层训练数据的信息。由于这种披露可能直接与隐私冲突，因此准确量化这种泄漏带来的隐私影响是一个基本问题。例如，以前的研究表明，可以利用决策树的结构构建其训练数据集的概率重建，重建的不确定性是信息泄漏的一个相关度量。在本文中，我们提出了一个新的框架，推广了这些概率重建，使其可以处理其他形式的可解释模型和更一般类型的知识。此外，我们证明在对可解释模型结构进行实际假设的情况下，可以高效地计算重建的不确定性。

    Interpretability is often pointed out as a key requirement for trustworthy machine learning. However, learning and releasing models that are inherently interpretable leaks information regarding the underlying training data. As such disclosure may directly conflict with privacy, a precise quantification of the privacy impact of such breach is a fundamental problem. For instance, previous work have shown that the structure of a decision tree can be leveraged to build a probabilistic reconstruction of its training dataset, with the uncertainty of the reconstruction being a relevant metric for the information leak. In this paper, we propose of a novel framework generalizing these probabilistic reconstructions in the sense that it can handle other forms of interpretable models and more generic types of knowledge. In addition, we demonstrate that under realistic assumptions regarding the interpretable models' structure, the uncertainty of the reconstruction can be computed efficiently. Final
    
[^47]: 自然发生的人机交互的顺序注释:初步见解

    Sequential annotations for naturally-occurring HRI: first insights. (arXiv:2308.15097v1 [cs.AI])

    [http://arxiv.org/abs/2308.15097](http://arxiv.org/abs/2308.15097)

    这篇论文介绍了一种用于改善嵌入式对话智能体交互的方法论，并提出了一种基于语言和多模态资源使用理论基础的注释实践。

    

    我们解释了我们开发的方法论，通过对话分析的顺序和多模态分析来改善嵌入式交互式智能体的交互。使用案例是一个Pepper机器人，预期在图书馆中向用户提供信息和导航。为了提出和学习更好的交互模式，我们正在创建一个自然发生的交互语料库，并将其提供给社区。为此，我们提出了基于有关语言和多模态资源在人机交互中使用的一些理论基础的注释实践。

    We explain the methodology we developed for improving the interactions accomplished by an embedded conversational agent, drawing from Conversation Analytic sequential and multimodal analysis. The use case is a Pepper robot that is expected to inform and orient users in a library. In order to propose and learn better interactive schema, we are creating a corpus of naturally-occurring interactions that will be made available to the community. To do so, we propose an annotation practice based on some theoretical underpinnings about the use of language and multimodal resources in human-robot interaction. CCS CONCEPTS $\bullet$ Computing methodologies $\rightarrow$ Discourse, dialogue and pragmatics; $\bullet$ Human-centered computing $\rightarrow$ Text input; HCI theory, concepts and models; Field studies.
    
[^48]: 我们能依赖人工智能吗？

    Can We Rely on AI?. (arXiv:2308.15092v1 [math.NA])

    [http://arxiv.org/abs/2308.15092](http://arxiv.org/abs/2308.15092)

    近年来，对抗性攻击算法揭示了人工智能工具的不稳定性，在高风险环境中引发了安全、可靠性和可解释性方面的问题。这篇论文提供了一个对该主题的概述，关注对应用和计算数学领域的研究人员可能感兴趣的方面。

    

    在过去的十年里，对抗性攻击算法揭示了深度学习工具的不稳定性。这些算法引发了人工智能在安全性、可靠性和可解释性方面的问题，特别是在高风险环境中。从实际角度来看，攻击和防御策略的发展之间存在着一种升级的战争。在更加理论的层面上，研究人员也研究了攻击的存在性和可计算性等更大的问题。在这里，我们简要概述了这个主题，并重点关注对应用和计算数学领域的研究人员可能感兴趣的方面。

    Over the last decade, adversarial attack algorithms have revealed instabilities in deep learning tools. These algorithms raise issues regarding safety, reliability and interpretability in artificial intelligence; especially in high risk settings. From a practical perspective, there has been a war of escalation between those developing attack and defence strategies. At a more theoretical level, researchers have also studied bigger picture questions concerning the existence and computability of attacks. Here we give a brief overview of the topic, focusing on aspects that are likely to be of interest to researchers in applied and computational mathematics.
    
[^49]: LAMBO: 大型语言模型增强的边缘智能

    LAMBO: Large Language Model Empowered Edge Intelligence. (arXiv:2308.15078v1 [cs.AI])

    [http://arxiv.org/abs/2308.15078](http://arxiv.org/abs/2308.15078)

    LAMBO是一种基于大型语言模型的边缘智能框架，用于移动边缘计算。它解决了传统深度卸载架构的问题，并提供了高性能的决策模块和强化学习模块。

    

    预计下一代边缘智能将为各种应用带来巨大的好处，例如卸载系统。然而，传统的深度卸载架构面临多个问题，包括异构限制、部分感知、不确定的泛化和缺乏可追溯性。在这种背景下，将卸载与大型语言模型（LLMs）集成在一起具有许多优势。因此，我们提出了一种基于LLM的卸载（LAMBO）框架，用于移动边缘计算（MEC），它由四个组成部分组成：（i）输入嵌入（IE），用于用高质量的可学习向量表示具有约束和提示的卸载系统的信息；（ii）非对称编码解码（AED）模型，是一个决策模块，具有深度编码器和浅层解码器。它可以基于多头自注意力机制实现高性能；（iii）演员-评论家强化学习（ACRL）模块，用于进行预训练。

    Next-generation edge intelligence is anticipated to bring huge benefits to various applications, e.g., offloading systems. However, traditional deep offloading architectures face several issues, including heterogeneous constraints, partial perception, uncertain generalization, and lack of tractability. In this context, the integration of offloading with large language models (LLMs) presents numerous advantages. Therefore, we propose an LLM-Based Offloading (LAMBO) framework for mobile edge computing (MEC), which comprises four components: (i) Input embedding (IE), which is used to represent the information of the offloading system with constraints and prompts through learnable vectors with high quality; (ii) Asymmetric encoderdecoder (AED) model, which is a decision-making module with a deep encoder and a shallow decoder. It can achieve high performance based on multi-head self-attention schemes; (iii) Actor-critic reinforcement learning (ACRL) module, which is employed to pre-train th
    
[^50]: MadSGM：基于评分的生成模型的多变量异常检测

    MadSGM: Multivariate Anomaly Detection with Score-based Generative Models. (arXiv:2308.15069v1 [cs.LG])

    [http://arxiv.org/abs/2308.15069](http://arxiv.org/abs/2308.15069)

    MadSGM是一种基于评分的生成模型的多变量时间序列异常检测器，考虑了重构、密度和梯度等全面的异常测量因素。实验证明MadSGM具有最强大和准确的预测能力。

    

    时间序列异常检测是时间序列中最基本的任务之一。与时间序列预测和分类不同，时间序列异常检测通常需要无监督（或自监督）训练，因为收集和标记异常观测是困难的。此外，大多数现有方法采用有限形式的异常测量，因此不清楚它们是否在所有情况下都是最优的。为此，我们提出了一种基于评分的生成模型的多变量时间序列异常检测器，称为MadSGM，它考虑了迄今为止最广泛的异常测量因素：i）基于重构的、ii）基于密度的和iii）基于梯度的异常测量。我们还设计了一个条件评分网络及其去噪评分匹配损失，用于时间序列异常检测。对五个真实世界基准数据集的实验表明，MadSGM实现了最强大和准确的预测。

    The time-series anomaly detection is one of the most fundamental tasks for time-series. Unlike the time-series forecasting and classification, the time-series anomaly detection typically requires unsupervised (or self-supervised) training since collecting and labeling anomalous observations are difficult. In addition, most existing methods resort to limited forms of anomaly measurements and therefore, it is not clear whether they are optimal in all circumstances. To this end, we present a multivariate time-series anomaly detector based on score-based generative models, called MadSGM, which considers the broadest ever set of anomaly measurement factors: i) reconstruction-based, ii) density-based, and iii) gradient-based anomaly measurements. We also design a conditional score network and its denoising score matching loss for the time-series anomaly detection. Experiments on five real-world benchmark datasets illustrate that MadSGM achieves the most robust and accurate predictions.
    
[^51]: 一种用于异常检测的综合增强框架

    A Comprehensive Augmentation Framework for Anomaly Detection. (arXiv:2308.15068v1 [cs.AI])

    [http://arxiv.org/abs/2308.15068](http://arxiv.org/abs/2308.15068)

    本文提出了一种用于异常检测的综合增强框架，该框架通过选择性地利用适当的组合，分析并压缩模拟异常的关键特征，与基于重构的方法相结合，并采用分割训练策略，能够在MVTec异常检测数据集上优于以前的最先进方法。

    

    数据增强方法通常被整合到异常检测模型的训练中。以往的方法主要集中在复制真实世界的异常或增加多样性，而没有考虑到异常的标准在不同类别之间存在差异，这可能导致训练分布的偏差。本文分析了对重构网络训练有贡献的模拟异常的关键特征，并将其压缩成几种方法，从而通过选择性地使用适当的组合来创建一个综合框架。此外，将这个框架与基于重构的方法相结合，并同时提出了一种分割训练策略，既减轻过拟合问题，又避免对重构过程引入干扰。在MVTec异常检测数据集上进行的评估表明，我们的方法在性能上优于以前的最先进方法，特别是在目标相关指标方面。

    Data augmentation methods are commonly integrated into the training of anomaly detection models. Previous approaches have primarily focused on replicating real-world anomalies or enhancing diversity, without considering that the standard of anomaly varies across different classes, potentially leading to a biased training distribution.This paper analyzes crucial traits of simulated anomalies that contribute to the training of reconstructive networks and condenses them into several methods, thus creating a comprehensive framework by selectively utilizing appropriate combinations.Furthermore, we integrate this framework with a reconstruction-based approach and concurrently propose a split training strategy that alleviates the issue of overfitting while avoiding introducing interference to the reconstruction process. The evaluations conducted on the MVTec anomaly detection dataset demonstrate that our method outperforms the previous state-of-the-art approach, particularly in terms of objec
    
[^52]: 学习异构人员再识别的跨模态信息瓶颈表示

    Learning Cross-modality Information Bottleneck Representation for Heterogeneous Person Re-Identification. (arXiv:2308.15063v1 [cs.CV])

    [http://arxiv.org/abs/2308.15063](http://arxiv.org/abs/2308.15063)

    本论文介绍了一种名为CMInfoNet的新型网络算法，通过优化相互信息瓶颈折衷，提取具有最具代表性信息的模态不变身份特征，并减少信息冗余和模态补充问题。

    

    可见光红外人员再识别（VI-ReID）是智能视频监控中一项重要而具有挑战性的任务。现有方法主要集中于学习一个共享特征空间，以减少可见光和红外模态之间的差异，然而仍有两个问题尚未得到充分探索：信息冗余和模态补充。为此，合理地消除与身份无关的信息，同时补充模态特定的信息至关重要，且仍然是一个具有挑战性的任务。为解决上述问题，我们提出了一种新颖的相互信息和模态一致性网络，即CMInfoNet，以提取具有最具代表性信息的模态不变身份特征并减少冗余。我们方法的关键洞见是找到一种最佳表示来捕捉更多与身份相关的信息，并通过优化相互信息瓶颈折衷来压缩无关部分。此外，我们还提出了一种方法来解决模态补充问题。

    Visible-Infrared person re-identification (VI-ReID) is an important and challenging task in intelligent video surveillance. Existing methods mainly focus on learning a shared feature space to reduce the modality discrepancy between visible and infrared modalities, which still leave two problems underexplored: information redundancy and modality complementarity. To this end, properly eliminating the identity-irrelevant information as well as making up for the modality-specific information are critical and remains a challenging endeavor. To tackle the above problems, we present a novel mutual information and modality consensus network, namely CMInfoNet, to extract modality-invariant identity features with the most representative information and reduce the redundancies. The key insight of our method is to find an optimal representation to capture more identity-relevant information and compress the irrelevant parts by optimizing a mutual information bottleneck trade-off. Besides, we propos
    
[^53]: 适应口语对话的文本对话状态跟踪器

    Adapting text-based dialogue state tracker for spoken dialogues. (arXiv:2308.15053v1 [cs.CL])

    [http://arxiv.org/abs/2308.15053](http://arxiv.org/abs/2308.15053)

    这篇论文描述了对构建适应口语对话系统的文本对话状态跟踪器进行的工程工作，利用自动语音识别错误校正和文本对话系统实现了插槽和值的估计。

    

    尽管通过对话系统技术竞赛（DSTC）取得了显著进展，但构建一个具有语音界面的稳健的任务导向对话系统仍然是一个关键挑战。大部分进展都是针对基于文本的对话系统，因为有丰富的书面语料库数据集，而具有口语对话的数据集非常稀缺。然而，正如Siri和Alexa等语音助手系统所展示的，将这种成功转移到口语对话中具有实际重要性。在本文中，我们描述了我们在DSTC11的具有语音感知的对话系统技术挑战赛中的高度成功模型的工程努力。我们的模型由三个主要模块组成：（1）自动语音识别错误校正，以弥合口语和文本话语之间的差距，（2）用于估计插槽和值的基于文本的对话系统（D3ST），该系统使用插槽描述。

    Although there have been remarkable advances in dialogue systems through the dialogue systems technology competition (DSTC), it remains one of the key challenges to building a robust task-oriented dialogue system with a speech interface. Most of the progress has been made for text-based dialogue systems since there are abundant datasets with written corpora while those with spoken dialogues are very scarce. However, as can be seen from voice assistant systems such as Siri and Alexa, it is of practical importance to transfer the success to spoken dialogues. In this paper, we describe our engineering effort in building a highly successful model that participated in the speech-aware dialogue systems technology challenge track in DSTC11. Our model consists of three major modules: (1) automatic speech recognition error correction to bridge the gap between the spoken and the text utterances, (2) text-based dialogue system (D3ST) for estimating the slots and values using slot descriptions, an
    
[^54]: iBARLE：平衡感知式房间布局估计

    iBARLE: imBalance-Aware Room Layout Estimation. (arXiv:2308.15050v1 [cs.CV])

    [http://arxiv.org/abs/2308.15050](http://arxiv.org/abs/2308.15050)

    我们提出了平衡感知式房间布局估计（iBARLE）框架，它包括外观变化生成模块、复杂结构混合模块和梯度-based布局目标函数，旨在解决房间布局估计中的不平衡和泛化问题。

    

    房间布局估计是从单个全景图预测布局。它需要具有大规模和多样化的房间形状的数据集来训练模型。然而，真实世界的数据集中存在显著的不平衡，包括布局复杂度的尺寸、相机位置和场景外观的变化。这些问题显著影响模型的训练性能。在这项工作中，我们提出了平衡感知式房间布局估计（iBARLE）框架来解决这些问题。iBARLE包括（1）外观变化生成（AVG）模块，促进视觉外观领域的泛化，（2）复杂结构混合（CSMix）模块，提高对房间结构的泛化能力，和（3）基于梯度的布局目标函数，更有效地考虑复杂布局中的遮挡。所有模块都是联合训练的，彼此帮助以实现最佳性能。基于ZInD~\cite{cruz2021zillow}数据集的实验和消融研究验证了我们的方法的有效性。

    Room layout estimation predicts layouts from a single panorama. It requires datasets with large-scale and diverse room shapes to train the models. However, there are significant imbalances in real-world datasets including the dimensions of layout complexity, camera locations, and variation in scene appearance. These issues considerably influence the model training performance. In this work, we propose the imBalance-Aware Room Layout Estimation (iBARLE) framework to address these issues. iBARLE consists of (1) Appearance Variation Generation (AVG) module, which promotes visual appearance domain generalization, (2) Complex Structure Mix-up (CSMix) module, which enhances generalizability w.r.t. room structure, and (3) a gradient-based layout objective function, which allows more effective accounting for occlusions in complex layouts. All modules are jointly trained and help each other to achieve the best performance. Experiments and ablation studies based on ZInD~\cite{cruz2021zillow} dat
    
[^55]: R^3: 基于设备的实时深度强化学习在自主机器人中的应用

    R^3: On-device Real-Time Deep Reinforcement Learning for Autonomous Robotics. (arXiv:2308.15039v1 [cs.RO])

    [http://arxiv.org/abs/2308.15039](http://arxiv.org/abs/2308.15039)

    R^3是一种在自主机器人中应用的基于设备的实时深度强化学习训练方法，它通过动态批量大小和回放缓冲区大小的优化，实现了在时间和算法性能之间的平衡，并有效地管理了内存和算法性能。

    

    自主机器人系统，如自动驾驶车辆和机器人搜救系统，需要在动态环境中连续适应深度强化学习(DRL)模型的高效设备上训练。本研究的基本动机是理解和应对基于设备的实时DRL的挑战，这涉及在内存约束下平衡时间和算法性能的能力，通过我们广泛的实证研究揭示。这种微妙的平衡需要共同优化DRL训练的两个关键参数--批量大小和回放缓冲区大小。配置这些参数对时间和算法性能有重要影响，然而两者都需要相当大的内存分配才能达到接近最优的性能。本文提出了R^3，一种在设备上管理时间、内存和算法性能的整体解决方案。R^3采用（i）一个以截止日期为驱动的反馈循环，带有动态批量大小，

    Autonomous robotic systems, like autonomous vehicles and robotic search and rescue, require efficient on-device training for continuous adaptation of Deep Reinforcement Learning (DRL) models in dynamic environments. This research is fundamentally motivated by the need to understand and address the challenges of on-device real-time DRL, which involves balancing timing and algorithm performance under memory constraints, as exposed through our extensive empirical studies. This intricate balance requires co-optimizing two pivotal parameters of DRL training -- batch size and replay buffer size. Configuring these parameters significantly affects timing and algorithm performance, while both (unfortunately) require substantial memory allocation to achieve near-optimal performance.  This paper presents R^3, a holistic solution for managing timing, memory, and algorithm performance in on-device real-time DRL training. R^3 employs (i) a deadline-driven feedback loop with dynamic batch sizing for 
    
[^56]: 在资源受限的边缘设备上通过动态专家交换为MoE模型提供服务

    Serving MoE Models on Resource-constrained Edge Devices via Dynamic Expert Swapping. (arXiv:2308.15030v1 [cs.AI])

    [http://arxiv.org/abs/2308.15030](http://arxiv.org/abs/2308.15030)

    本文提出了一种在资源受限的边缘设备上通过动态专家交换为MoE模型提供服务的推理框架，该框架通过分析MoE模型的行为模式，引入了新的数据结构来减少资源消耗，并通过性能分析优化参数配置。

    

    混合专家(MoE)是深度学习中一种流行的技术，通过有条件激活的并行神经网络模块(专家)提升了模型的容量。然而，在资源受限的延迟关键的边缘场景中提供MoE模型的服务是一项具有挑战性的任务，因为模型的大小和复杂性显著增加。本文首先分析了MoE模型在连续推理场景中的行为模式，得出了关于专家激活的三个关键观察结果，包括时间局部性、可交换性和可跳过计算。基于这些观察，我们引入了PC-MoE，一种适用于资源受限的连续MoE模型服务的推理框架。PC-MoE的核心是一种新的数据结构，称为“参数委员会”，它智能地维护一部分重要的正在使用的专家，以减少资源消耗。通过基于性能分析的委员会规划器，在线找到参数委员会的最佳配置，并进行专家交换和请求处理。

    Mixture of experts (MoE) is a popular technique in deep learning that improves model capacity with conditionally-activated parallel neural network modules (experts). However, serving MoE models in resource-constrained latency-critical edge scenarios is challenging due to the significantly increased model size and complexity. In this paper, we first analyze the behavior pattern of MoE models in continuous inference scenarios, which leads to three key observations about the expert activations, including temporal locality, exchangeability, and skippable computation. Based on these observations, we introduce PC-MoE, an inference framework for resource-constrained continuous MoE model serving. The core of PC-MoE is a new data structure, Parameter Committee, that intelligently maintains a subset of important experts in use to reduce resource consumption. The optimal configuration of Parameter Committee is found offline by a profiling-guided committee planner, and expert swapping and request 
    
[^57]: 递归总结在大型语言模型中实现长期对话记忆

    Recursively Summarizing Enables Long-Term Dialogue Memory in Large Language Models. (arXiv:2308.15022v1 [cs.CL])

    [http://arxiv.org/abs/2308.15022](http://arxiv.org/abs/2308.15022)

    递归总结在大型语言模型中实现长期对话记忆，可以提高对话系统在长对话中记忆重要信息的能力。

    

    大多数开放领域的对话系统在长期对话中容易遗忘重要信息。现有方法通常训练特定的检索器或总结器从过去获取关键信息，这需要耗费时间且高度依赖标记数据的质量。为了缓解这个问题，我们提出使用大型语言模型（LLMs）递归生成总结/记忆，以增强长期记忆能力。具体而言，我们的方法首先刺激LLMs记住小对话上下文，然后递归地使用之前的记忆和随后的对话内容产生新的记忆。最后，LLM可以在最新记忆的帮助下轻松生成高度一致的响应。我们使用ChatGPT和text-davinci-003进行评估，对广泛使用的公共数据集进行的实验证明我们的方法在长对话中可以生成更一致的响应。值得注意的是，我们的方法是实现LLM建模的潜在解决方案。

    Most open-domain dialogue systems suffer from forgetting important information, especially in a long-term conversation. Existing works usually train the specific retriever or summarizer to obtain key information from the past, which is time-consuming and highly depends on the quality of labeled data. To alleviate this problem, we propose to recursively generate summaries/ memory using large language models (LLMs) to enhance long-term memory ability. Specifically, our method first stimulates LLMs to memorize small dialogue contexts and then recursively produce new memory using previous memory and following contexts. Finally, the LLM can easily generate a highly consistent response with the help of the latest memory. We evaluate our method using ChatGPT and text-davinci-003, and the experiments on the widely-used public dataset show that our method can generate more consistent responses in a long-context conversation. Notably, our method is a potential solution to enable the LLM to model
    
[^58]: 基于GPU的混合可满足性问题求解器的大规模并行连续局部搜索

    Massively Parallel Continuous Local Search for Hybrid SAT Solving on GPUs. (arXiv:2308.15020v1 [cs.AI])

    [http://arxiv.org/abs/2308.15020](http://arxiv.org/abs/2308.15020)

    提出了一种基于GPU的大规模并行连续局部搜索方法来加速混合SAT求解器，该方法通过使用基于快速傅里叶变换的新型并行算法来计算基本对称多项式，并在搜索中使用重启启发式算法以提高效率。与以前的方法相比，实现了显著的改进。

    

    针对基于冲突驱动子句学习（CDCL）的最先进可满足性问题（SAT）求解器在工程上取得了显著成功，但其顺序性质限制了在图形处理单元（GPU）等平台上进行加速的并行性。在本研究中，我们提出了FastFourierSAT，一种基于梯度驱动连续局部搜索（CLS）的高度并行混合SAT求解器。这是通过一种受快速傅里叶变换（FFT）卷积启发的新型并行算法来实现的，用于计算以前的CLS方法中的主要计算任务——基本对称多项式（ESP）。我们的算法复杂度与最佳的以前结果相匹配。此外，我们算法固有的大规模并行性能够利用GPU进行加速，相比以前的CLS方法展现出显著的改进。我们还提出了将重启启发式算法融入CLS以提高搜索效率。我们将我们的应用与其他SAT求解器进行了比较。

    Although state-of-the-art (SOTA) SAT solvers based on conflict-driven clause learning (CDCL) have achieved remarkable engineering success, their sequential nature limits the parallelism that may be extracted for acceleration on platforms such as the graphics processing unit (GPU). In this work, we propose FastFourierSAT, a highly parallel hybrid SAT solver based on gradient-driven continuous local search (CLS). This is realized by a novel parallel algorithm inspired by the Fast Fourier Transform (FFT)-based convolution for computing the elementary symmetric polynomials (ESPs), which is the major computational task in previous CLS methods. The complexity of our algorithm matches the best previous result. Furthermore, the substantial parallelism inherent in our algorithm can leverage the GPU for acceleration, demonstrating significant improvement over the previous CLS approaches. We also propose to incorporate the restart heuristics in CLS to improve search efficiency. We compare our app
    
[^59]: 模型的生成模型：针对各种任务和资源限制的快速深度神经网络定制

    Generative Model for Models: Rapid DNN Customization for Diverse Tasks and Resource Constraints. (arXiv:2308.15003v1 [cs.AI])

    [http://arxiv.org/abs/2308.15003](http://arxiv.org/abs/2308.15003)

    该论文提出了一个一站式框架NN-Factory，通过使用生成模型直接生成定制的轻量级模型，实现了快速针对各种边缘场景的资源定制化和任务定制化。

    

    与通常大而统一的基于云的深度学习模型不同，部署在边缘的模型通常需要针对特定领域任务和资源有限环境的定制化。由于边缘场景的多样性和每个场景的训练负载，这种定制化过程可能是昂贵和耗时的。尽管已经提出了各种方法来分别实现快速面向资源的定制化和面向任务的定制化，但同时实现这两者仍然具有挑战性。受到生成型人工智能和神经网络模块可组合性的启示，我们引入了NN-Factory，一个一站式框架，用于为各种边缘场景生成定制轻量级模型。关键思想是使用生成模型直接生成定制的模型，而不是进行训练。NN-Factory的主要组成部分包括一个模块化超网络和可以有条件激活以完成不同任务的预训练模块。

    Unlike cloud-based deep learning models that are often large and uniform, edge-deployed models usually demand customization for domain-specific tasks and resource-limited environments. Such customization processes can be costly and time-consuming due to the diversity of edge scenarios and the training load for each scenario. Although various approaches have been proposed for rapid resource-oriented customization and task-oriented customization respectively, achieving both of them at the same time is challenging. Drawing inspiration from the generative AI and the modular composability of neural networks, we introduce NN-Factory, an one-for-all framework to generate customized lightweight models for diverse edge scenarios. The key idea is to use a generative model to directly produce the customized models, instead of training them. The main components of NN-Factory include a modular supernet with pretrained modules that can be conditionally activated to accomplish different tasks and a g
    
[^60]: 探索历史信息在时间知识图推断中的限制

    Exploring the Limits of Historical Information for Temporal Knowledge Graph Extrapolation. (arXiv:2308.15002v1 [cs.AI])

    [http://arxiv.org/abs/2308.15002](http://arxiv.org/abs/2308.15002)

    本研究探索了时间知识图推断中历史信息的限制，并提出了一种新的事件预测模型CENET，通过历史对比学习框架学习历史和非历史依赖关系，以区分最有潜力的实体。

    

    时间知识图是表示实体之间动态关系和交互的一种方法，被认为是事件预测的有前景的方法。然而，大多数时间知识图推理方法存在一个局限性，就是过于依赖事件的重复性或周期性，这给推断与缺乏历史交互的实体相关的未来事件带来挑战。事实上，当前事态往往是历史信息和不直接可观察的潜在因素的组合结果。为了解决这个问题，我们探讨了历史信息在时间知识图推断中的限制，并提出了一种新的事件预测模型，称为对比事件网络（CENET），基于一种新颖的历史对比学习的训练框架。CENET通过学习历史和非历史的依赖关系，区分最有潜力的实体，以最佳匹配给定的查询。

    Temporal knowledge graphs, representing the dynamic relationships and interactions between entities over time, have been identified as a promising approach for event forecasting. However, a limitation of most temporal knowledge graph reasoning methods is their heavy reliance on the recurrence or periodicity of events, which brings challenges to inferring future events related to entities that lack historical interaction. In fact, the current state of affairs is often the result of a combination of historical information and underlying factors that are not directly observable. To this end, we investigate the limits of historical information for temporal knowledge graph extrapolation and propose a new event forecasting model called Contrastive Event Network (CENET) based on a novel training framework of historical contrastive learning. CENET learns both the historical and non-historical dependency to distinguish the most potential entities that best match the given query. Simultaneously,
    
[^61]: 在人工智能中引入神经启发的适应性，以实现持续学习

    Incorporating Neuro-Inspired Adaptability for Continual Learning in Artificial Intelligence. (arXiv:2308.14991v1 [cs.LG])

    [http://arxiv.org/abs/2308.14991](http://arxiv.org/abs/2308.14991)

    本研究引入了神经启发的适应性解决方案，以实现人工智能的持续学习。通过模拟果蝇学习系统，我们提出了一种可以灵活适应变化的通用方法，改善了学习可塑性，并确保解决方案的兼容性。

    

    持续学习旨在赋予人工智能（AI）对真实世界的强大适应能力。为此，一个理想的解决方案应该在记忆稳定性和学习可塑性之间保持适当平衡，并获得足够的兼容性来捕捉观测到的分布。现有的进展主要集中在保持记忆稳定性以克服灾难性遗忘，但仍难以像生物智能（BI）那样灵活地适应增量变化。通过建模一个能够主动调节遗忘的稳健果蝇学习系统，并利用多个学习模块，我们提出了一种通用方法，通过在参数分布中适当衰减旧记忆来改善学习可塑性，并相应地协调多学习者架构来确保解决方案的兼容性。通过广泛的理论和实证验证，我们的方法不仅明显提高了持续学习的性能，特别是在突触调节方面。

    Continual learning aims to empower artificial intelligence (AI) with strong adaptability to the real world. For this purpose, a desirable solution should properly balance memory stability with learning plasticity, and acquire sufficient compatibility to capture the observed distributions. Existing advances mainly focus on preserving memory stability to overcome catastrophic forgetting, but remain difficult to flexibly accommodate incremental changes as biological intelligence (BI) does. By modeling a robust Drosophila learning system that actively regulates forgetting with multiple learning modules, here we propose a generic approach that appropriately attenuates old memories in parameter distributions to improve learning plasticity, and accordingly coordinates a multi-learner architecture to ensure solution compatibility. Through extensive theoretical and empirical validation, our approach not only clearly enhances the performance of continual learning, especially over synaptic regula
    
[^62]: 基于增量构建学习和集成域适应的滚动轴承故障诊断

    Constructive Incremental Learning for Fault Diagnosis of Rolling Bearings with Ensemble Domain Adaptation. (arXiv:2308.14983v1 [cs.AI])

    [http://arxiv.org/abs/2308.14983](http://arxiv.org/abs/2308.14983)

    本文提出了一种基于增量构建学习和集成域适应的方法，用于滚动轴承故障诊断。该方法通过在随机配置网络上实施增量学习，结合云特征提取和小波包分解，提高了故障诊断的准确性和适应性。

    

    鉴于滚动轴承故障诊断在各种工况下的实际问题普遍存在，样本的有限可用性增加了挑战。此外，外部环境的复杂性和滚动轴承的结构经常表现出具有随机性和模糊性的故障，从而阻碍了故障特征的有效提取，并限制了故障诊断的准确性。为了解决这些问题，本文提出了一种称为构造增量学习的集成域适应（CIL-EDA）方法。具体而言，它在随机配置网络（SCN）上实施，以在多个域中构建其自适应性能。具体地，采用云特征提取方法结合小波包分解（WPD）来捕捉来自多个分辨率方面的故障信息的不确定性。随后，采用增量构建学习的域适应方案。

    Given the prevalence of rolling bearing fault diagnosis as a practical issue across various working conditions, the limited availability of samples compounds the challenge. Additionally, the complexity of the external environment and the structure of rolling bearings often manifests faults characterized by randomness and fuzziness, hindering the effective extraction of fault characteristics and restricting the accuracy of fault diagnosis. To overcome these problems, this paper presents a novel approach termed constructive Incremental learning-based ensemble domain adaptation (CIL-EDA) approach. Specifically, it is implemented on stochastic configuration networks (SCN) to constructively improve its adaptive performance in multi-domains. Concretely, a cloud feature extraction method is employed in conjunction with wavelet packet decomposition (WPD) to capture the uncertainty of fault information from multiple resolution aspects. Subsequently, constructive Incremental learning-based domai
    
[^63]: 一种通过深度学习模型高效对太阳通量演化视频进行标注的方法

    Efficient labeling of solar flux evolution videos by a deep learning model. (arXiv:2308.14976v1 [astro-ph.SR])

    [http://arxiv.org/abs/2308.14976](http://arxiv.org/abs/2308.14976)

    通过粗略标注的天文视频训练卷积神经网络（CNN），可以提高数据标注质量和减少人为干预，减少标注过程中的时间消耗。

    

    机器学习（ML）正成为对大型复杂数据进行问询的关键工具。标注，即添加有意义的注释的过程，是监督式ML的一个关键步骤。然而，标注数据集非常耗时。我们在这里展示了卷积神经网络（CNN）可以利用粗略标注的天文视频来提高数据标注质量，减少人为干预的需求。我们使用太阳磁场的视频，根据它们在太阳盘上首次检测到的双极磁区（BMRs）的出现或非出现将其粗略标记成两个类别。我们使用粗略标签训练CNN，手动验证和纠正CNN与标注的不一致之处，并重复此过程直至收敛。传统上，通量出现的标注是手动完成的。我们发现通过这个迭代过程得到的高质量标注数据集可以将人工验证的需求降低50%。此外，通过逐渐屏蔽掉标签，我们可以逐步提高CNN在准确标注过程中的自信度。

    Machine learning (ML) is becoming a critical tool for interrogation of large complex data. Labeling, defined as the process of adding meaningful annotations, is a crucial step of supervised ML. However, labeling datasets is time consuming. Here we show that convolutional neural networks (CNNs), trained on crudely labeled astronomical videos, can be leveraged to improve the quality of data labeling and reduce the need for human intervention. We use videos of the solar magnetic field, crudely labeled into two classes: emergence or non-emergence of bipolar magnetic regions (BMRs), based on their first detection on the solar disk. We train CNNs using crude labels, manually verify, correct labeling vs. CNN disagreements, and repeat this process until convergence. Traditionally, flux emergence labelling is done manually. We find that a high-quality labeled dataset, derived through this iterative process, reduces the necessary manual verification by 50%. Furthermore, by gradually masking the 
    
[^64]: 基于LLM的人机合作操作框架用于操纵任务

    LLM-Based Human-Robot Collaboration Framework for Manipulation Tasks. (arXiv:2308.14972v1 [cs.RO])

    [http://arxiv.org/abs/2308.14972](http://arxiv.org/abs/2308.14972)

    本文介绍了一种基于LLM的人机合作操作框架，利用逻辑推理和环境感知，将高级语言命令转化为可执行的运动函数。采用远程操作和动态运动原理进行动作矫正，以提高系统的实用性和普适性。

    

    本文提出了一种新的方法来增强自主机器人的操纵能力，利用大型语言模型（LLM）进行逻辑推理，将高级语言命令转化为可执行的运动函数序列。所提出的系统将LLM的优势与基于YOLO的环境感知相结合，使机器人能够根据给定的命令自主进行合理的决策和任务规划。此外，为了解决LLM可能出现的不准确性或不合逻辑的行为，采用了远程操作和动态运动原理（DMP）相结合的方法进行动作矫正。这种集成旨在提高基于LLM的人机合作系统的实用性和普适性。

    This paper presents a novel approach to enhance autonomous robotic manipulation using the Large Language Model (LLM) for logical inference, converting high-level language commands into sequences of executable motion functions. The proposed system combines the advantage of LLM with YOLO-based environmental perception to enable robots to autonomously make reasonable decisions and task planning based on the given commands. Additionally, to address the potential inaccuracies or illogical actions arising from LLM, a combination of teleoperation and Dynamic Movement Primitives (DMP) is employed for action correction. This integration aims to improve the practicality and generalizability of the LLM-based human-robot collaboration system.
    
[^65]: 鲁棒的开放集言语识别和CU MultiLang数据集

    Robust Open-Set Spoken Language Identification and the CU MultiLang Dataset. (arXiv:2308.14951v1 [cs.CL])

    [http://arxiv.org/abs/2308.14951](http://arxiv.org/abs/2308.14951)

    本文实现了一种鲁棒的开放集言语识别系统，通过使用MFCC和音高特征，TDNN模型提取特征嵌入，设置置信度阈值，以及使用LDA和pLDA学习新的未知语言分类来实现。在经过训练的语言上，系统准确率达到91.76%，并且具备实时适应未知语言的能力。

    

    大多数最先进的言语识别模型是闭集的，即它们只能输出它们在训练时使用的类别集合中的语言标签。然而，开放集言语识别系统具备检测输入是否不属于原始语言的能力。本文实现了一种新颖的开放集言语识别方法，该方法使用MFCC和音高特征， TDNN模型提取有意义的特征嵌入，通过对softmax输出进行置信度阈值处理，以及使用LDA和pLDA学习对新的未知语言进行分类。我们提出了一个言语识别系统，其在经过训练的语言上实现了91.76%的准确率，并具备实时适应未知语言的能力。为此，我们还构建了CU MultiLang数据集，这是一个大规模多样化的多语言语音语料库，用于训练和评估我们的系统。

    Most state-of-the-art spoken language identification models are closed-set; in other words, they can only output a language label from the set of classes they were trained on. Open-set spoken language identification systems, however, gain the ability to detect when an input exhibits none of the original languages. In this paper, we implement a novel approach to open-set spoken language identification that uses MFCC and pitch features, a TDNN model to extract meaningful feature embeddings, confidence thresholding on softmax outputs, and LDA and pLDA for learning to classify new unknown languages. We present a spoken language identification system that achieves 91.76% accuracy on trained languages and has the capability to adapt to unknown languages on the fly. To that end, we also built the CU MultiLang Dataset, a large and diverse multilingual speech corpus which was used to train and evaluate our system.
    
[^66]: Transfusor：用于可控人类般生成车辆换道轨迹的Transformer Diffusor

    Transfusor: Transformer Diffusor for Controllable Human-like Generation of Vehicle Lane Changing Trajectories. (arXiv:2308.14943v1 [cs.AI])

    [http://arxiv.org/abs/2308.14943](http://arxiv.org/abs/2308.14943)

    Transfusor模型是一个使用Transformer和Diffusor模型的创新方法，旨在在高速公路场景中生成高度逼真且可控的类人换道轨迹。

    

    随着自动驾驶系统的不断发展和部署需求的增加，研究人员不断寻求可靠的自动驾驶系统的方法。虚拟模拟测试（VST）已经成为测试自动驾驶系统（ADS）和高级驾驶员辅助系统（ADAS）的主要方法，因为它具有快速执行、低成本和高重复性的优势。然而，这些基于模拟的实验的成功程度严重依赖于测试场景的逼真程度。为了增加ADS和ADAS的安全性和可靠性，需要在VST中创建更灵活和高保真度的测试场景。为了应对这一挑战，本文引入了“Transfusor”模型，该模型利用了Transformer和Diffusor模型（两种前沿的深度学习生成技术）。Transfusor模型的主要目标是在高速公路场景中生成高度逼真且可控的类人换道轨迹。

    With ongoing development of autonomous driving systems and increasing desire for deployment, researchers continue to seek reliable approaches for ADS systems. The virtual simulation test (VST) has become a prominent approach for testing autonomous driving systems (ADS) and advanced driver assistance systems (ADAS) due to its advantages of fast execution, low cost, and high repeatability. However, the success of these simulation-based experiments heavily relies on the realism of the testing scenarios. It is needed to create more flexible and high-fidelity testing scenarios in VST in order to increase the safety and reliabilityof ADS and ADAS.To address this challenge, this paper introduces the "Transfusor" model, which leverages the transformer and diffusor models (two cutting-edge deep learning generative technologies). The primary objective of the Transfusor model is to generate highly realistic and controllable human-like lane-changing trajectories in highway scenarios. Extensive exp
    
[^67]: 为移动友好的3D医学图像分割自动提示SAM

    Auto-Prompting SAM for Mobile Friendly 3D Medical Image Segmentation. (arXiv:2308.14936v1 [cs.CV])

    [http://arxiv.org/abs/2308.14936](http://arxiv.org/abs/2308.14936)

    这项工作提出了一种名为AutoSAM Adapter的方法，用于解决SAM在3D医学图像分割任务上的性能问题。通过参数高效的适应技术，实现了自动提示学习范式，消除了对手动生成提示的需求。

    

    Segment Anything Model (SAM)已经被迅速应用于各种自然图像的分割。然而，最近的研究表明，SAM在3D医学图像分割任务上的性能不佳。除了自然图像和医学图像之间的领域差距外，2D和3D图像之间的空间布局差异，强大的GPU服务器所带来的大量计算负担，以及耗时的手动提示生成使得SAM无法扩展到更广泛的医学图像分割应用。为了解决这些挑战，在这项工作中，我们引入了一种新方法AutoSAM Adapter，专为3D多器官CT分割而设计。我们采用参数高效的适应技术开发了一种自动提示学习范式，以促进将SAM模型的能力转化为3D医学图像分割，消除了手动生成提示的需求。

    The Segment Anything Model (SAM) has rapidly been adopted for segmenting a wide range of natural images. However, recent studies have indicated that SAM exhibits subpar performance on 3D medical image segmentation tasks. In addition to the domain gaps between natural and medical images, disparities in the spatial arrangement between 2D and 3D images, the substantial computational burden imposed by powerful GPU servers, and the time-consuming manual prompt generation impede the extension of SAM to a broader spectrum of medical image segmentation applications. To address these challenges, in this work, we introduce a novel method, AutoSAM Adapter, designed specifically for 3D multi-organ CT-based segmentation. We employ parameter-efficient adaptation techniques in developing an automatic prompt learning paradigm to facilitate the transformation of the SAM model's capabilities to 3D medical image segmentation, eliminating the need for manually generated prompts. Furthermore, we effectivel
    
[^68]: 患者特异性的机械模型结合人工智能和大数据的肿瘤生长

    Patient-specific, mechanistic models of tumor growth incorporating artificial intelligence and big data. (arXiv:2308.14925v1 [physics.med-ph])

    [http://arxiv.org/abs/2308.14925](http://arxiv.org/abs/2308.14925)

    本文综述了肿瘤生长和治疗模型的不同方法，包括基于“大数据”和人工智能的机械模型以及数据驱动模型。同时指出了当前肿瘤研究中存在的问题和局限性。

    

    尽管过去十年中癌症诊断、治疗和管理取得了显著进展，但恶性肿瘤仍然是一个重大的公共健康问题。个性化治疗的进一步发展可能通过根据每个患者的预测反应来个性化治疗的方式实现。个性化治疗的设计需要将患者特异性的信息整合到适当的肿瘤反应的数学模型中。实现这一范 Paradigm的一个基本障碍是目前缺乏严格但实用的肿瘤发生、发展、侵袭和治疗反应的数学理论。在本文中，我们首先概述了不同的肿瘤生长和治疗模型的方法，包括基于“大数据”和人工智能的机械模型以及数据驱动模型。接着，我们提供了数学模型的实例，展示了它们的实用性并讨论了其局限性。

    Despite the remarkable advances in cancer diagnosis, treatment, and management that have occurred over the past decade, malignant tumors remain a major public health problem. Further progress in combating cancer may be enabled by personalizing the delivery of therapies according to the predicted response for each individual patient. The design of personalized therapies requires patient-specific information integrated into an appropriate mathematical model of tumor response. A fundamental barrier to realizing this paradigm is the current lack of a rigorous, yet practical, mathematical theory of tumor initiation, development, invasion, and response to therapy. In this review, we begin by providing an overview of different approaches to modeling tumor growth and treatment, including mechanistic as well as data-driven models based on ``big data" and artificial intelligence. Next, we present illustrative examples of mathematical models manifesting their utility and discussing the limitation
    
[^69]: 使用深度强化学习进行经济燃气轮机调度优化

    Optimal Economic Gas Turbine Dispatch with Deep Reinforcement Learning. (arXiv:2308.14924v1 [cs.LG])

    [http://arxiv.org/abs/2308.14924](http://arxiv.org/abs/2308.14924)

    本研究通过将西门子能源提供的热力学软件纳入环境模型，并模拟不确定性，揭示了使用深度强化学习进行经济燃气轮机调度优化的好处，并发现Deep Q-Networks (DQN) 在算法和基准方法中获得了最高的奖励。

    

    在现代电力网络中，燃气轮机的调度策略正在发生变化。与间歇性可再生能源的日益融入相比，燃气轮机需要更频繁地以更短周期和部分负载运行。深度强化学习（DRL）最近被提出作为可以应对这种发展并在经济上调度燃气轮机的工具。DRL的主要优势是无模型优化和处理不确定性的能力，比如由不同负载或可再生能源产生的变化。在本研究中，我们实现了三种流行的DRL算法来解决加拿大阿尔伯塔省的经济燃气轮机调度问题，并通过将西门子能源提供的现有热力学软件纳入环境模型并模拟不确定性（如电价、负载和环境条件的变化）来凸显DRL的优势。

    Dispatching strategies for gas turbines (GTs) are changing in modern electricity grids. A growing incorporation of intermittent renewable energy requires GTs to operate more but shorter cycles and more frequently on partial loads. Deep reinforcement learning (DRL) has recently emerged as a tool that can cope with this development and dispatch GTs economically. The key advantages of DRL are a model-free optimization and the ability to handle uncertainties, such as those introduced by varying loads or renewable energy production. In this study, three popular DRL algorithms are implemented for an economic GT dispatch problem on a case study in Alberta, Canada. We highlight the benefits of DRL by incorporating an existing thermodynamic software provided by Siemens Energy into the environment model and by simulating uncertainty via varying electricity prices, loads, and ambient conditions. Among the tested algorithms and baseline methods, Deep Q-Networks (DQN) obtained the highest rewards w
    
[^70]: 论马尔可夫决策过程的奖励结构

    On Reward Structures of Markov Decision Processes. (arXiv:2308.14919v1 [cs.LG])

    [http://arxiv.org/abs/2308.14919](http://arxiv.org/abs/2308.14919)

    该论文研究了马尔可夫决策过程中的奖励结构，提出了一种估计器用于估计单个状态值，并通过根据奖励代替常用的基于转移的常数，提供了对强化学习中技巧的理论解释。

    

    马尔可夫决策过程可以通过转移核与奖励函数参数化。这两个因素在强化学习研究中起着重要作用，正如它们在贝尔曼方程中的存在所证明的那样。针对机器人应用中的需求，我们研究了与强化学习相关的各种"成本"，奖励是理解马尔可夫决策过程结构的核心，奖励中心概念可以阐明强化学习中的重要概念。具体而言，我们研究了策略评估的样本复杂性，并开发了一种新的估计器，其实例特定的误差界为$\tilde{O}(\sqrt{\frac{\tau_s}{n}})$，用于估计单个状态值。在在线遗憾最小化设置下，我们将基于转移的MDP常数，直径，改进为基于奖励的常数，最大预期到达成本，并通过该常数为一种广为人知的技术，基于潜力的奖励形状提供了理论解释。

    A Markov decision process can be parameterized by a transition kernel and a reward function. Both play essential roles in the study of reinforcement learning as evidenced by their presence in the Bellman equations. In our inquiry of various kinds of ``costs'' associated with reinforcement learning inspired by the demands in robotic applications, rewards are central to understanding the structure of a Markov decision process and reward-centric notions can elucidate important concepts in reinforcement learning. Specifically, we studied the sample complexity of policy evaluation and developed a novel estimator with an instance-specific error bound of $\tilde{O}(\sqrt{\frac{\tau_s}{n}})$ for estimating a single state value. Under the online regret minimization setting, we refined the transition-based MDP constant, diameter, into a reward-based constant, maximum expected hitting cost, and with it, provided a theoretical explanation for how a well-known technique, potential-based reward shap
    
[^71]: RecRec: 推荐系统的算法性补救措施

    RecRec: Algorithmic Recourse for Recommender Systems. (arXiv:2308.14916v1 [cs.IR])

    [http://arxiv.org/abs/2308.14916](http://arxiv.org/abs/2308.14916)

    本文提出了一个算法性补救框架，用于帮助理解推荐系统的模型并修改推荐结果。

    

    推荐系统在娱乐、购物、食物、新闻、就业和教育等领域的决策中起着至关重要的作用。这些推荐系统背后的机器学习模型对于用户、内容提供者和系统开发者来说通常都是巨大且不透明的。对于所有利益相关者来说，理解模型在进行某些预测和推荐时的原理至关重要。对于那些生计依赖于推荐系统的内容提供者来说尤其如此。在本研究中，我们从从实际需求出发，提出了一个面向内容提供者的推荐系统补救框架。推荐设置中的算法性补救是一组操作，如果执行，将以期望的方式修改项目的推荐（或排序）。补救措施提供的操作形式为：“如果一个特征从X变为Y，那么该项目的排名也会相应变化。”

    Recommender systems play an essential role in the choices people make in domains such as entertainment, shopping, food, news, employment, and education. The machine learning models underlying these recommender systems are often enormously large and black-box in nature for users, content providers, and system developers alike. It is often crucial for all stakeholders to understand the model's rationale behind making certain predictions and recommendations. This is especially true for the content providers whose livelihoods depend on the recommender system. Drawing motivation from the practitioners' need, in this work, we propose a recourse framework for recommender systems, targeted towards the content providers. Algorithmic recourse in the recommendation setting is a set of actions that, if executed, would modify the recommendations (or ranking) of an item in the desired manner. A recourse suggests actions of the form: "if a feature changes X to Y, then the ranking of that item for a s
    
[^72]: RobustCLEVR：一种评估目标中心学习的鲁棒性的基准和框架

    RobustCLEVR: A Benchmark and Framework for Evaluating Robustness in Object-centric Learning. (arXiv:2308.14899v1 [cs.CV])

    [http://arxiv.org/abs/2308.14899](http://arxiv.org/abs/2308.14899)

    RobustCLEVR提出了一种目标中心学习的鲁棒性评估框架和基准数据集。该框架通过在图像生成过程中规定因果依赖关系，并能够产生多种无法在其他评估中出现的图像破坏，以评估目标中心方法的鲁棒性。

    

    目标中心表示学习通过明确地将图像场景解析为其组成部分，克服了图像级表示的局限性。然而，图像级表示通常对自然图像破坏缺乏鲁棒性，而目标中心方法的鲁棒性尚未得到广泛测试。为了弥补这一差距，我们提出了RobustCLEVR基准数据集和评估框架。我们的框架通过在专家知识基础上启用图像生成过程中因果依赖性的规定，并能够产生一系列在现有鲁棒性评估中无法获得的图像破坏，以一种全新的方式评估鲁棒性。使用我们的框架，我们定义了几种图像破坏过程的因果模型，这些模型明确编码了每种破坏类型的因果关系和分布的假设。我们为每个因果模型生成数据集变体，并在上面评估各种最先进的方法。

    Object-centric representation learning offers the potential to overcome limitations of image-level representations by explicitly parsing image scenes into their constituent components. While image-level representations typically lack robustness to natural image corruptions, the robustness of object-centric methods remains largely untested. To address this gap, we present the RobustCLEVR benchmark dataset and evaluation framework. Our framework takes a novel approach to evaluating robustness by enabling the specification of causal dependencies in the image generation process grounded in expert knowledge and capable of producing a wide range of image corruptions unattainable in existing robustness evaluations. Using our framework, we define several causal models of the image corruption process which explicitly encode assumptions about the causal relationships and distributions of each corruption type. We generate dataset variants for each causal model on which we evaluate state-of-the-ar
    
[^73]: 第39届国际逻辑编程会议论文集

    Proceedings 39th International Conference on Logic Programming. (arXiv:2308.14898v1 [cs.AI])

    [http://arxiv.org/abs/2308.14898](http://arxiv.org/abs/2308.14898)

    第39届国际逻辑编程会议论文集包含了在伦敦帝国理工学院举行的技术交流报告，涉及多个专题，包括逻辑编程和机器学习、逻辑编程和可解释性、伦理和可信度等。

    

    本卷包含了在2023年7月9日至15日在英国伦敦帝国理工学院举行的第39届国际逻辑编程会议（ICLP 2023）上发表的技术交流报告。这些技术交流报告涉及主题讲座、博士生论坛、应用与系统/演示专题、最新研究进展专题、兴趣小组交流、逻辑编程与机器学习专题以及逻辑编程与可解释性、伦理和可信度专题。

    This volume contains the Technical Communications presented at the 39th International Conference on Logic Programming (ICLP 2023), held at Imperial College London, UK from July 9 to July 15, 2023. Technical Communications included here concern the Main Track, the Doctoral Consortium, the Application and Systems/Demo track, the Recently Published Research Track, the Birds-of-a-Feather track, the Thematic Tracks on Logic Programming and Machine Learning, and Logic Programming and Explainability, Ethics, and Trustworthiness.
    
[^74]: 高效统计方差缩减的双策略评估用于序列建模强化学习中的离线策略评估

    Statistically Efficient Variance Reduction with Double Policy Estimation for Off-Policy Evaluation in Sequence-Modeled Reinforcement Learning. (arXiv:2308.14897v1 [cs.LG])

    [http://arxiv.org/abs/2308.14897](http://arxiv.org/abs/2308.14897)

    本论文提出了一种使用双策略评估的离线序列建模和离线强化学习的RL算法，通过统计方法证明具有方差缩减的特性。

    

    离线强化学习旨在利用先前收集的环境-动作交互记录的数据集来学习一个无需访问真实环境的策略。最近的研究表明，离线强化学习可以被形式化为一个序列建模问题，并通过决策转换器等方法进行监督学习来解决。尽管这些基于序列的方法在回报率方法上取得了有竞争力的结果，尤其是在需要较长的回合或回报稀缺的任务上，但在处理离线数据时并未考虑重要性采样来校正策略偏差，主要原因是缺乏行为策略和使用确定性评估策略。为此，我们提出了DPE：一种将离线序列建模和离线强化学习与统计上证明具有方差缩减性质的双策略评估（DPE）融合在一个统一框架中的RL算法。我们在多个任务中验证了我们的方法。

    Offline reinforcement learning aims to utilize datasets of previously gathered environment-action interaction records to learn a policy without access to the real environment. Recent work has shown that offline reinforcement learning can be formulated as a sequence modeling problem and solved via supervised learning with approaches such as decision transformer. While these sequence-based methods achieve competitive results over return-to-go methods, especially on tasks that require longer episodes or with scarce rewards, importance sampling is not considered to correct the policy bias when dealing with off-policy data, mainly due to the absence of behavior policy and the use of deterministic evaluation policies. To this end, we propose DPE: an RL algorithm that blends offline sequence modeling and offline reinforcement learning with Double Policy Estimation (DPE) in a unified framework with statistically proven properties on variance reduction. We validate our method in multiple tasks 
    
[^75]: 当困难的负样本采样与监督对比学习相遇

    When hard negative sampling meets supervised contrastive learning. (arXiv:2308.14893v1 [cs.CV])

    [http://arxiv.org/abs/2308.14893](http://arxiv.org/abs/2308.14893)

    当前图像模型在训练时常用交叉熵损失，但其在泛化和稳定性方面存在问题。本文提出了一种新的监督对比学习目标，SCHaNe，通过加权困难的负样本挖掘来提高模型性能。实验结果显示，SCHaNe在各个基准测试中的Top-1准确率优于BEiT-3。

    

    目前的图像模型通常遵循两阶段策略：在大数据集上进行预训练，然后使用交叉熵损失进行微调。许多研究表明，使用交叉熵可能导致次优的泛化和稳定性。监督对比损失通过关注类内相似性和类间差异来解决交叉熵损失的一些限制，但它忽视了困难负样本挖掘的重要性。我们提出，通过根据负样本与正样本的不相似程度进行加权，模型将从性能改进中受益。在本文中，我们引入了一种新的监督对比学习目标，SCHaNe，在微调阶段引入了困难负样本采样。实验结果表明，SCHaNe在各个基准测试中的Top-1准确率上优于强基线模型BEiT-3，而无需专门的架构、额外的数据或计算资源。

    State-of-the-art image models predominantly follow a two-stage strategy: pre-training on large datasets and fine-tuning with cross-entropy loss. Many studies have shown that using cross-entropy can result in sub-optimal generalisation and stability. While the supervised contrastive loss addresses some limitations of cross-entropy loss by focusing on intra-class similarities and inter-class differences, it neglects the importance of hard negative mining. We propose that models will benefit from performance improvement by weighting negative samples based on their dissimilarity to positive counterparts. In this paper, we introduce a new supervised contrastive learning objective, SCHaNe, which incorporates hard negative sampling during the fine-tuning phase. Without requiring specialized architectures, additional data, or extra computational resources, experimental results indicate that SCHaNe outperforms the strong baseline BEiT-3 in Top-1 accuracy across various benchmarks, with signific
    
[^76]: NAS-X: 基于扭曲的神经自适应平滑方法

    NAS-X: Neural Adaptive Smoothing via Twisting. (arXiv:2308.14864v1 [cs.LG])

    [http://arxiv.org/abs/2308.14864](http://arxiv.org/abs/2308.14864)

    NAS-X是一种基于扭曲的神经自适应平滑方法，通过重新加权的唤醒-睡眠算法来学习和推断顺序潜变量模型，并在离散和连续任务中取得了优于先前方法的推断和参数恢复效果。

    

    本文提出了一种名为NAS-X的神经自适应平滑方法，该方法基于重新加权的唤醒-睡眠算法进行顺序潜变量模型的学习和推断。NAS-X适用于离散和连续潜变量，并利用平滑SMC方法来拟合比传统的重新加权唤醒-睡眠方法更广泛的模型。我们在离散和连续任务上测试了NAS-X，并发现在推断和参数恢复方面，它明显优于先前的变分和基于重新加权唤醒-睡眠方法。

    We present Neural Adaptive Smoothing via Twisting (NAS-X), a method for learning and inference in sequential latent variable models based on reweighted wake-sleep (RWS). NAS-X works with both discrete and continuous latent variables, and leverages smoothing SMC to fit a broader range of models than traditional RWS methods. We test NAS-X on discrete and continuous tasks and find that it substantially outperforms previous variational and RWS-based methods in inference and parameter recovery.
    
[^77]: 评估使用熔池图像流进行印刷轨迹异常分类的关键时空学习器

    Evaluation of Key Spatiotemporal Learners for Print Track Anomaly Classification Using Melt Pool Image Streams. (arXiv:2308.14861v1 [cs.LG])

    [http://arxiv.org/abs/2308.14861](http://arxiv.org/abs/2308.14861)

    本研究评估了使用熔池图像流进行印刷轨迹异常分类的关键时空学习器，并介绍了一些领先的深度时空学习模型的实践应用。

    

    最近在金属增材制造（MAM）中应用机器学习已经展现出了解决普遍采用MAM技术的关键障碍的巨大潜力。这个领域的最新研究强调了利用熔池特征进行实时缺陷预测的重要性。尽管高质量的熔池图像数据具有实现精确预测的潜力，但对于能够利用增材制造过程的瞬态和时序特征的尖端时空模型的利用仍然有限。本研究介绍并实践了一些领先的深度时空学习模型，可用于分类来自不同材料、系统和应用的熔池图像流。具体而言，它研究了由空间和时间流组成的两流网络、循环空间网络和因式分解3D卷积。

    Recent applications of machine learning in metal additive manufacturing (MAM) have demonstrated significant potential in addressing critical barriers to the widespread adoption of MAM technology. Recent research in this field emphasizes the importance of utilizing melt pool signatures for real-time defect prediction. While high-quality melt pool image data holds the promise of enabling precise predictions, there has been limited exploration into the utilization of cutting-edge spatiotemporal models that can harness the inherent transient and sequential characteristics of the additive manufacturing process. This research introduces and puts into practice some of the leading deep spatiotemporal learning models that can be adapted for the classification of melt pool image streams originating from various materials, systems, and applications. Specifically, it investigates two-stream networks comprising spatial and temporal streams, a recurrent spatial network, and a factorized 3D convoluti
    
[^78]: Attention Visualizer Package:揭示编码器-只有的Transformer模型中单词重要性的注意力可视化工具

    Attention Visualizer Package: Revealing Word Importance for Deeper Insight into Encoder-Only Transformer Models. (arXiv:2308.14850v1 [cs.CL])

    [http://arxiv.org/abs/2308.14850](http://arxiv.org/abs/2308.14850)

    这个论文介绍了Attention Visualizer包，通过可视化展示单词在编码器-只有的Transformer模型中的重要性，提高了对神经网络的解释性和可解释性。

    

    本文介绍了Attention Visualizer包，该包用于视觉化展示编码器-只有的Transformer模型中个别单词的重要性。与其他关注标记和自注意力分数的方法相比，我们的方法将研究单词及其对最终嵌入表示的影响。这样的库在增强神经网络的解释性和可解释性方面起着关键作用。它们提供了了解其内部机制、提高其性能的更好理解的机会。您可以访问以下GitHub存储库获取代码并查看示例：https://github.com/AlaFalaki/AttentionVisualizer。

    This report introduces the Attention Visualizer package, which is crafted to visually illustrate the significance of individual words in encoder-only transformer-based models. In contrast to other methods that center on tokens and self-attention scores, our approach will examine the words and their impact on the final embedding representation. Libraries like this play a crucial role in enhancing the interpretability and explainability of neural networks. They offer the opportunity to illuminate their internal mechanisms, providing a better understanding of how they operate and can be enhanced. You can access the code and review examples on the following GitHub repository: https://github.com/AlaFalaki/AttentionVisualizer.
    
[^79]: 建筑AI动力协作机器人的信任问题：一项定性实证分析

    Trust in Construction AI-Powered Collaborative Robots: A Qualitative Empirical Analysis. (arXiv:2308.14846v1 [cs.HC])

    [http://arxiv.org/abs/2308.14846](http://arxiv.org/abs/2308.14846)

    该论文通过分析建筑从业者的半结构化访谈结果，研究了建筑中值得信赖的AI动力协作机器人的特征。研究发现，除了之前已鉴定出的关键信任因素外，财务考虑和不确定性等其他因素也是重要因素。

    

    建筑技术研究人员和具有前瞻思维的公司正在尝试使用人工智能驱动的协作机器人（即cobots）来探索建筑行业数字化转型的各种自动化场景。智能协作机器人被期望成为未来建筑工作的主导型机器人。然而，AI驱动的cobots的黑箱性质以及引入工作现场的技术和心理方面的未知性，是信任挑战的前兆。本文通过分析使用基于地理理论的半结构化访谈的结果，研究了建筑中值得信赖的AI动力协作机器人的特征。研究发现，尽管之前作者进行的系统文献综述中鉴定出的关键信任因素与领域专家和最终用户产生了共鸣，但财务考虑和不确定性等其他因素也是重要因素。

    Construction technology researchers and forward-thinking companies are experimenting with collaborative robots (aka cobots), powered by artificial intelligence (AI), to explore various automation scenarios as part of the digital transformation of the industry. Intelligent cobots are expected to be the dominant type of robots in the future of work in construction. However, the black-box nature of AI-powered cobots and unknown technical and psychological aspects of introducing them to job sites are precursors to trust challenges. By analyzing the results of semi-structured interviews with construction practitioners using grounded theory, this paper investigates the characteristics of trustworthy AI-powered cobots in construction. The study found that while the key trust factors identified in a systematic literature review -- conducted previously by the authors -- resonated with the field experts and end users, other factors such as financial considerations and the uncertainty associated 
    
[^80]: 使用迁移学习的自适应工人-机器人交互的鲁棒活动识别

    Robust Activity Recognition for Adaptive Worker-Robot Interaction using Transfer Learning. (arXiv:2308.14843v1 [cs.RO])

    [http://arxiv.org/abs/2308.14843](http://arxiv.org/abs/2308.14843)

    本文介绍了一种使用迁移学习的鲁棒活动识别方法，对于建筑工人的活动识别，该方法在准确性方面要求更少的数据和计算时间。

    

    使用机器学习的人类活动识别在检测建筑工人的活动方面表现出巨大的潜力。活动识别在人机交互研究中有许多应用，可以使机器人理解人类同事的活动。然而，许多现有的活动识别方法缺乏鲁棒性、泛化性和适应性。本文提出了一种迁移学习方法，用于建筑工人的活动识别，该方法在相当或更好的分类准确性下，需要比之前更少的数据和计算时间。该算法通过从原作者预训练的模型中转移特征，并对其进行精细调整以进行建筑活动识别的任务。该模型在Kinetics-400上进行了预训练，Kinetics-400是一个大规模的基于视频的人类活动识别数据集，包含400个不同的类别。该模型经过微调，并使用从YouTube上捕获的手动材料处理活动的视频进行了测试。

    Human activity recognition (HAR) using machine learning has shown tremendous promise in detecting construction workers' activities. HAR has many applications in human-robot interaction research to enable robots' understanding of human counterparts' activities. However, many existing HAR approaches lack robustness, generalizability, and adaptability. This paper proposes a transfer learning methodology for activity recognition of construction workers that requires orders of magnitude less data and compute time for comparable or better classification accuracy. The developed algorithm transfers features from a model pre-trained by the original authors and fine-tunes them for the downstream task of activity recognition in construction. The model was pre-trained on Kinetics-400, a large-scale video-based human activity recognition dataset with 400 distinct classes. The model was fine-tuned and tested using videos captured from manual material handling (MMH) activities found on YouTube. Resul
    
[^81]: 优化虚拟现实/增强现实人机工程学：建模和预测用户颈部肌肉收缩

    Toward Optimized VR/AR Ergonomics: Modeling and Predicting User Neck Muscle Contraction. (arXiv:2308.14841v1 [cs.HC])

    [http://arxiv.org/abs/2308.14841](http://arxiv.org/abs/2308.14841)

    该研究利用肌电图设备测量、建模和预测了VR用户在与虚拟环境交互时的颈部肌肉收缩水平，开发了一个生物物理启发式的计算模型，可以准确预测不同头部运动状态下的颈部收缩水平，并且可以预测仅通过目标头部姿势的潜在收缩需求。

    

    对于大规模和长期采用VR/AR体验来说，人体工程学效率是至关重要的。虽然VR/AR头戴显示器在观看时解锁了用户自然的广泛头部运动，但由于增加的硬件重量，不可避免地影响了他们的颈部肌肉舒适度。不幸的是，到目前为止，还没有多少量化的知识来理解和解决这个问题。利用肌电图设备，我们测量、建模和预测VR用户在与虚拟环境交互时头部运动时的颈部肌肉收缩水平（MCL）。具体来说，通过学习收集的生理数据，我们开发了一个生物物理启发式的计算模型，可以预测在不同头部运动状态下的颈部MCL。除了量化完成的头部运动的累积MCL之外，我们的模型还可以预测仅通过目标头部姿势的潜在MCL需求。一系列客观评估和用户研究证明了其预测准确性和普适性。

    Ergonomic efficiency is essential to the mass and prolonged adoption of VR/AR experiences. While VR/AR head-mounted displays unlock users' natural wide-range head movements during viewing, their neck muscle comfort is inevitably compromised by the added hardware weight. Unfortunately, little quantitative knowledge for understanding and addressing such an issue is available so far.  Leveraging electromyography devices, we measure, model, and predict VR users' neck muscle contraction levels (MCL) while they move their heads to interact with the virtual environment. Specifically, by learning from collected physiological data, we develop a bio-physically inspired computational model to predict neck MCL under diverse head kinematic states. Beyond quantifying the cumulative MCL of completed head movements, our model can also predict potential MCL requirements with target head poses only. A series of objective evaluations and user studies demonstrate its prediction accuracy and generality, as
    
[^82]: 识别和减轻生成式人工智能的安全风险

    Identifying and Mitigating the Security Risks of Generative AI. (arXiv:2308.14840v1 [cs.AI])

    [http://arxiv.org/abs/2308.14840](http://arxiv.org/abs/2308.14840)

    生成式人工智能技术具有巨大的潜力，但也存在安全风险。这篇论文是一个研讨会的综合报道，讨论了生成式人工智能所带来的双重用途困境，提出了社区在这个领域的短期和长期目标。

    

    每一项重大技术发明都会带来双重用途的困境 - 新技术既有可能被用于善良，也可能被用于恶意行为。生成式人工智能（GenAI）技术，如大型语言模型（LLM）和扩散模型，展示了卓越的能力（例如上下文学习，代码补全，文本到图像的生成和编辑）。然而，攻击者同样可以利用GenAI生成新的攻击，并增加现有攻击的速度和有效性。本文报告了在Google举办的一个研讨会的发现（由斯坦福大学和威斯康星大学麦迪逊分校共同组织）。本文并不意味着全面，而是试图综合一些有趣的研讨会发现。我们讨论了这个主题的短期和长期目标。我们希望这篇论文既为这个重要主题的讨论提供一个起点，也引起兴趣。

    Every major technical invention resurfaces the dual-use dilemma -- the new technology has the potential to be used for good as well as for harm. Generative AI (GenAI) techniques, such as large language models (LLMs) and diffusion models, have shown remarkable capabilities (e.g., in-context learning, code-completion, and text-to-image generation and editing). However, GenAI can be used just as well by attackers to generate new attacks and increase the velocity and efficacy of existing attacks.  This paper reports the findings of a workshop held at Google (co-organized by Stanford University and the University of Wisconsin-Madison) on the dual-use dilemma posed by GenAI. This paper is not meant to be comprehensive, but is rather an attempt to synthesize some of the interesting findings from the workshop. We discuss short-term and long-term goals for the community on this topic. We hope this paper provides both a launching point for a discussion on this important topic as well as interest
    
[^83]: 使用不精确神经网络的分布鲁棒统计验证

    Distributionally Robust Statistical Verification with Imprecise Neural Networks. (arXiv:2308.14815v1 [cs.AI])

    [http://arxiv.org/abs/2308.14815](http://arxiv.org/abs/2308.14815)

    本文提出了一种使用不精确神经网络的分布鲁棒统计验证方法，通过结合主动学习、不确定性量化和神经网络验证，可以在大量的分布上提供对黑盒系统行为的保证。

    

    在AI安全领域，一个特别具有挑战性的问题是在高维自主系统的行为上提供保证。以可达性分析为中心的验证方法无法扩展，而纯粹的统计方法受到对采样过程的分布假设的限制。相反，我们提出了一个针对黑盒系统的分布鲁棒版本的统计验证问题，其中我们的性能保证适用于大量的分布。本文提出了一种基于主动学习、不确定性量化和神经网络验证的新方法。我们方法的一个核心部分是一种称为不精确神经网络的集成技术，它提供了不确定性以指导主动学习。主动学习使用了一种称为Sherlock的全面神经网络验证工具来收集样本。在openAI gym Mujoco环境中使用多个物理模拟器进行评估。

    A particularly challenging problem in AI safety is providing guarantees on the behavior of high-dimensional autonomous systems. Verification approaches centered around reachability analysis fail to scale, and purely statistical approaches are constrained by the distributional assumptions about the sampling process. Instead, we pose a distributionally robust version of the statistical verification problem for black-box systems, where our performance guarantees hold over a large family of distributions. This paper proposes a novel approach based on a combination of active learning, uncertainty quantification, and neural network verification. A central piece of our approach is an ensemble technique called Imprecise Neural Networks, which provides the uncertainty to guide active learning. The active learning uses an exhaustive neural-network verification tool Sherlock to collect samples. An evaluation on multiple physical simulators in the openAI gym Mujoco environments with reinforcement-
    
[^84]: 在差分隐私下生成表格数据集

    Generating tabular datasets under differential privacy. (arXiv:2308.14784v1 [cs.LG])

    [http://arxiv.org/abs/2308.14784](http://arxiv.org/abs/2308.14784)

    该论文研究了在差分隐私的约束下生成表格数据集的问题，通过利用生成对抗网络（GAN），它解决了训练数据的记忆重复和隐私泄露的问题，并提出了与传统方法相比更好的解决方案。

    

    机器学习在各个领域和行业中推动了进展，但其依赖于可访问和高质量的训练数据。一些最重要的数据集以表格和关系数据库的形式出现在生物医学和金融领域。但这些表格数据通常具有敏感性质。合成数据生成可以揭示敏感数据的潜力，但生成模型往往会记忆和重复训练数据，从而破坏隐私目标。为了解决这个问题，研究人员将差分隐私（DP）的数学框架融入深度神经网络的训练过程中。但这会在生成的数据的质量和隐私之间产生权衡。生成对抗网络（GAN）是在差分隐私下合成表格数据的主要范式，但受到不稳定的对抗训练和模式坍塌的困扰，这些问题在隐私约束和复杂的表格数据模态下更加严重。

    Machine Learning (ML) is accelerating progress across fields and industries, but relies on accessible and high-quality training data. Some of the most important datasets are found in biomedical and financial domains in the form of spreadsheets and relational databases. But this tabular data is often sensitive in nature. Synthetic data generation offers the potential to unlock sensitive data, but generative models tend to memorise and regurgitate training data, which undermines the privacy goal. To remedy this, researchers have incorporated the mathematical framework of Differential Privacy (DP) into the training process of deep neural networks. But this creates a trade-off between the quality and privacy of the resulting data. Generative Adversarial Networks (GANs) are the dominant paradigm for synthesising tabular data under DP, but suffer from unstable adversarial training and mode collapse, which are exacerbated by the privacy constraints and challenging tabular data modality. This 
    
[^85]: 冲突感知的主动有限状态机学习

    Conflict-Aware Active Automata Learning. (arXiv:2308.14781v1 [cs.LG])

    [http://arxiv.org/abs/2308.14781](http://arxiv.org/abs/2308.14781)

    C3AL是一种冲突感知的主动有限状态机学习框架，能够处理观测数据中的冲突，通过将观测树作为学习过程的一等公民并最小化测试次数，具有很好的效果。

    

    主动有限状态机学习算法在处理观测数据中的冲突（同一输入对应不同输出）方面存在困难。这种固有的冲突恢复能力不足，影响了它们在存在噪声或学习中的系统变化场景中的有效应用。我们提出了冲突感知的主动有限状态机学习（C3AL）框架，以在学习过程中处理冲突信息。核心思想是将所谓的观测树视为学习过程的一等公民。尽管这个想法在最近的研究中得到了探索，但我们通过将其与任何现有的学习算法结合，并在面对冲突时最小化对正在学习的系统执行的测试次数，充分发挥了它的作用。我们在大量的基准测试中评估了C3AL，涵盖了30多个不同的真实目标和18,000多个不同的场景。评估结果表明，C3AL是一个合适的替代方法。

    Active automata learning algorithms cannot easily handle \emph{conflict} in the observation data (different outputs observed for the same inputs). This inherent inability to recover after a conflict impairs their effective applicability in scenarios where noise is present or the system under learning is mutating.  We propose the Conflict-Aware Active Automata Learning (C3AL) framework to enable handling conflicting information during the learning process. The core idea is to consider the so-called observation tree as a first-class citizen in the learning process. Though this idea is explored in recent work, we take it to its full effect by enabling its use with any existing learner and minimizing the number of tests performed on the system under learning, specially in the face of conflicts. We evaluate C3AL in a large set of benchmarks, covering over 30 different realistic targets, and over 18,000 different scenarios. The results of the evaluation show that C3AL is a suitable alternati
    
[^86]: 愿原力与你同在：统一的力导向预训练用于3D分子构型

    May the Force be with You: Unified Force-Centric Pre-Training for 3D Molecular Conformations. (arXiv:2308.14759v1 [physics.chem-ph])

    [http://arxiv.org/abs/2308.14759](http://arxiv.org/abs/2308.14759)

    本论文提出了一种统一的力导向预训练模型用于3D分子构型，涵盖了平衡和非平衡数据。通过直接从原子力中学习非平衡数据和使用零力正则化和基于力的去噪技术近似近平衡力，我们获得了一个包含超过1500万个多样构型的预训练模型，相比于未预训练的模型，我们将力的准确性提高了大约3倍。

    

    最近的研究显示了学习预训练模型在3D分子表示方面的潜力。然而，现有的预训练模型主要关注平衡数据，很大程度上忽视了非平衡构型。将这些方法扩展到非平衡数据是具有挑战性的，因为它们的训练目标依赖于构型是局部能量极小值的假设。我们通过提出一种力导向的预训练模型来填补这一差距，用于3D分子构型的平衡和非平衡数据。对于非平衡数据，我们的模型直接从原子力中学习。对于平衡数据，我们引入零力正则化和基于力的去噪技术来近似近平衡力。我们获得了一个具有超过1500万个多样的构型的统一预训练模型。实验表明，相比于未预训练的模型，在我们的预训练目标下，我们将力的准确性提高了大约3倍。

    Recent works have shown the promise of learning pre-trained models for 3D molecular representation. However, existing pre-training models focus predominantly on equilibrium data and largely overlook off-equilibrium conformations. It is challenging to extend these methods to off-equilibrium data because their training objective relies on assumptions of conformations being the local energy minima. We address this gap by proposing a force-centric pretraining model for 3D molecular conformations covering both equilibrium and off-equilibrium data. For off-equilibrium data, our model learns directly from their atomic forces. For equilibrium data, we introduce zero-force regularization and forced-based denoising techniques to approximate near-equilibrium forces. We obtain a unified pre-trained model for 3D molecular representation with over 15 million diverse conformations. Experiments show that, with our pre-training objective, we increase forces accuracy by around 3 times compared to the un
    
[^87]: 可自适应缓解时变量子噪声

    Adaptive mitigation of time-varying quantum noise. (arXiv:2308.14756v1 [quant-ph])

    [http://arxiv.org/abs/2308.14756](http://arxiv.org/abs/2308.14756)

    提出了一种贝叶斯推断算法来自适应学习和缓解时变量子噪声，并展示其优于非自适应方法的性能提升。

    

    当前的量子计算机受到非稳定的高误差率噪声通道的影响，这削弱了它们的可靠性和可重复性。我们提出了一种基于贝叶斯推断的自适应算法，可以学习和缓解随着通道条件变化的量子噪声。我们的研究强调了需要动态推断关键通道参数来提高程序精度。我们使用狄利克雷分布来建模泊松噪声的随机性。这使得我们能够进行贝叶斯推断，从而可以改善在时变噪声下概率性误差抵消（PEC）的性能。我们的工作展示了表征和缓解量子噪声的时间变化的重要性，这对于开发更准确可靠的量子技术至关重要。我们的结果表明，贝叶斯PEC在使用Hellinger距离从理想分布进行测量时可以比非自适应方法提高4.5倍。

    Current quantum computers suffer from non-stationary noise channels with high error rates, which undermines their reliability and reproducibility. We propose a Bayesian inference-based adaptive algorithm that can learn and mitigate quantum noise in response to changing channel conditions. Our study emphasizes the need for dynamic inference of critical channel parameters to improve program accuracy. We use the Dirichlet distribution to model the stochasticity of the Pauli channel. This allows us to perform Bayesian inference, which can improve the performance of probabilistic error cancellation (PEC) under time-varying noise. Our work demonstrates the importance of characterizing and mitigating temporal variations in quantum noise, which is crucial for developing more accurate and reliable quantum technologies. Our results show that Bayesian PEC can outperform non-adaptive approaches by a factor of 4.5x when measured using Hellinger distance from the ideal distribution.
    
[^88]: 强化学习在生成型人工智能中的应用：综述

    Reinforcement Learning for Generative AI: A Survey. (arXiv:2308.14328v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2308.14328](http://arxiv.org/abs/2308.14328)

    该论文综述了强化学习在生成型人工智能中的应用。通过创建新的训练信号，强化学习展示了其从多个角度引入人类归纳偏好的强大和灵活性，以建立一个性能良好的模型。

    

    深度生成型人工智能是机器学习界中一个长期存在的重要主题，可以影响到诸多应用领域，如文本生成和计算机视觉。训练生成模型的主要范式是最大似然估计，通过减小模型分布和目标分布之间的差异来推动学习器捕捉并逼近目标数据分布。这种公式成功地建立了生成任务的目标，然而却无法满足使用者对生成模型的所有需求。强化学习作为一种竞争性选择，通过创建新的目标来注入新的训练信号，展示了它的强大和灵活性，可以从多个角度引入人类归纳偏好，如对抗学习、手动设计规则和学习奖励模型，以建立一个性能良好的模型。

    Deep Generative AI has been a long-standing essential topic in the machine learning community, which can impact a number of application areas like text generation and computer vision. The major paradigm to train a generative model is maximum likelihood estimation, which pushes the learner to capture and approximate the target data distribution by decreasing the divergence between the model distribution and the target distribution. This formulation successfully establishes the objective of generative tasks, while it is incapable of satisfying all the requirements that a user might expect from a generative model. Reinforcement learning, serving as a competitive option to inject new training signals by creating new objectives that exploit novel signals, has demonstrated its power and flexibility to incorporate human inductive bias from multiple angles, such as adversarial learning, hand-designed rules and learned reward model to build a performant model. Thereby, reinforcement learning ha
    
[^89]: 授权临床医生并民主化数据科学：大型语言模型自动化临床研究的机器学习。 (arXiv:2308.14120v2 [cs.LG] 更新版)

    Empowering Clinicians and Democratizing Data Science: Large Language Models Automate Machine Learning for Clinical Studies. (arXiv:2308.14120v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2308.14120](http://arxiv.org/abs/2308.14120)

    chatGPT ADA是一种能够自主开发临床研究所需的最先进的机器学习模型的大型语言模型，可将高级分析工具民主化，使非数据科学家的临床医生能够轻松应用于医学领域。

    

    机器学习（ML）开发者（如数据科学家）和从业者（如临床医生）之间存在知识差距，阻碍了ML在临床数据分析中的充分利用。我们研究了chatGPT Advanced Data Analysis（ADA），即GPT-4的扩展，来弥合这一差距并高效执行ML分析的潜力。我们向chatGPT ADA提供了各种医学专业的大型试验的真实临床数据和研究详细信息，没有给出具体指导。ChatGPT ADA基于原始研究的训练数据自主开发了最先进的ML模型，用于预测临床结果，如癌症发展、癌症进展、疾病并发症或致病基因序列等生物标志物。令人惊讶的是，这些ML模型与其已发表的对应物相匹配甚至表现更好。我们得出结论，chatGPT ADA为民主化医学中的ML提供了一个有前景的途径，使非ML专家能够获得先进的分析工具并推动广泛应用。

    A knowledge gap persists between Machine Learning (ML) developers (e.g., data scientists) and practitioners (e.g., clinicians), hampering the full utilization of ML for clinical data analysis. We investigated the potential of the chatGPT Advanced Data Analysis (ADA), an extension of GPT-4, to bridge this gap and perform ML analyses efficiently. Real-world clinical datasets and study details from large trials across various medical specialties were presented to chatGPT ADA without specific guidance. ChatGPT ADA autonomously developed state-of-the-art ML models based on the original study's training data to predict clinical outcomes such as cancer development, cancer progression, disease complications, or biomarkers such as pathogenic gene sequences. Strikingly, these ML models matched or outperformed their published counterparts. We conclude that chatGPT ADA offers a promising avenue to democratize ML in medicine, making advanced analytics accessible to non-ML experts and promoting broa
    
[^90]: 长视频中的时间性角色分组的统一动态图

    Unified and Dynamic Graph for Temporal Character Grouping in Long Videos. (arXiv:2308.14105v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2308.14105](http://arxiv.org/abs/2308.14105)

    本文提出了一种统一动态图（UniDG）框架，通过统一的表示网络学习多模态表示，并保留模态的独特性。采用动态图聚类方法构建可靠的亲和图，并提出了一种渐进式的关联方法。

    

    视频时间性角色分组根据角色的身份在视频中定位出现的时刻。 为此，最近的研究从无监督聚类发展到基于图的有监督聚类。 然而，图方法建立在固定的亲和图前提下，带来了许多不精确的连接。 此外，它们使用各种模型提取多模态特征，对部署不友好。在本文中，我们提出了一种用于时间性角色分组的统一动态图（UniDG）框架。首先，通过一个统一的表示网络，我们学习了同一空间中多个模态的表示，并同时保留了模态的独特性。其次，我们提出了一种动态图聚类方法，通过循环匹配策略为每个节点动态构建不同数量的邻居，以获得更可靠的亲和图。 第三，我们提出了一个渐进式的关联方法

    Video temporal character grouping locates appearing moments of major characters within a video according to their identities. To this end, recent works have evolved from unsupervised clustering to graph-based supervised clustering. However, graph methods are built upon the premise of fixed affinity graphs, bringing many inexact connections. Besides, they extract multi-modal features with kinds of models, which are unfriendly to deployment. In this paper, we present a unified and dynamic graph (UniDG) framework for temporal character grouping. This is accomplished firstly by a unified representation network that learns representations of multiple modalities within the same space and still preserves the modality's uniqueness simultaneously. Secondly, we present a dynamic graph clustering where the neighbors of different quantities are dynamically constructed for each node via a cyclic matching strategy, leading to a more reliable affinity graph. Thirdly, a progressive association method 
    
[^91]: 图上不平衡学习的综述：问题、技术和未来方向

    A Survey of Imbalanced Learning on Graphs: Problems, Techniques, and Future Directions. (arXiv:2308.13821v1 [cs.LG])

    [http://arxiv.org/abs/2308.13821](http://arxiv.org/abs/2308.13821)

    本综述对图上不平衡学习进行了全面的审视，旨在纠正数据分布偏差，以获得更准确和代表性的学习结果。

    

    图表示与世界各种场景中普遍存在的相互连接的结构。有效的图分析技术，如图学习方法，使用户能够从图数据中获得深刻的洞察力，为节点分类和链路预测等各种任务提供支持。然而，这些方法常常面临数据不平衡的问题，即在图数据中某些片段拥有大量数据而其他数据稀缺，从而导致偏倚的学习结果。这就需要出现了图上不平衡学习的新兴领域，旨在纠正这些数据分布偏差，以获得更准确和代表性的学习结果。在本综述中，我们对图上不平衡学习的文献进行了全面的审视。我们首先提供了对该概念和相关术语的明确理解，为读者建立了扎实的基础知识。随后，我们提出了两个全面的分类法：（1）问题分类法（Problem Taxonomy）。

    Graphs represent interconnected structures prevalent in a myriad of real-world scenarios. Effective graph analytics, such as graph learning methods, enables users to gain profound insights from graph data, underpinning various tasks including node classification and link prediction. However, these methods often suffer from data imbalance, a common issue in graph data where certain segments possess abundant data while others are scarce, thereby leading to biased learning outcomes. This necessitates the emerging field of imbalanced learning on graphs, which aims to correct these data distribution skews for more accurate and representative learning outcomes. In this survey, we embark on a comprehensive review of the literature on imbalanced learning on graphs. We begin by providing a definitive understanding of the concept and related terminologies, establishing a strong foundational understanding for readers. Following this, we propose two comprehensive taxonomies: (1) the problem taxono
    
[^92]: 开放注视：一个仿制谷歌眼动论文的开源实现

    Open Gaze: An Open-Source Implementation Replicating Google's Eye Tracking Paper. (arXiv:2308.13495v1 [cs.CV])

    [http://arxiv.org/abs/2308.13495](http://arxiv.org/abs/2308.13495)

    本论文提出了一个仿制谷歌眼动论文的开源实现，重点是通过整合机器学习技术，在智能手机上实现与谷歌论文相当的准确眼动追踪解决方案。

    

    眼动已经成为视觉研究、语言分析和可用性评估等不同领域的重要工具。然而，大多数先前的研究集中在使用专门的、昂贵的眼动追踪硬件的扩展式桌面显示器上。尽管智能手机的普及率和使用频率很高，但对于智能手机上的眼球移动模式却鲜有见解。在本文中，我们提出了一个基于智能手机的开源注视追踪实现，模拟了谷歌论文提出的方法论（其源代码仍然是专有的）。我们的重点是在不需要额外硬件的情况下达到与谷歌论文方法相当的准确度。通过整合机器学习技术，我们揭示了一种本地于智能手机的准确眼动追踪解决方案。我们的方法展示了与最先进的移动眼动追踪器相当的精度。

    Eye tracking has been a pivotal tool in diverse fields such as vision research, language analysis, and usability assessment. The majority of prior investigations, however, have concentrated on expansive desktop displays employing specialized, costly eye tracking hardware that lacks scalability. Remarkably little insight exists into ocular movement patterns on smartphones, despite their widespread adoption and significant usage. In this manuscript, we present an open-source implementation of a smartphone-based gaze tracker that emulates the methodology proposed by a GooglePaper (whose source code remains proprietary). Our focus is on attaining accuracy comparable to that attained through the GooglePaper's methodology, without the necessity for supplementary hardware. Through the integration of machine learning techniques, we unveil an accurate eye tracking solution that is native to smartphones. Our approach demonstrates precision akin to the state-of-the-art mobile eye trackers, which 
    
[^93]: 强化学习辅助进化算法：调查和研究机会

    Reinforcement Learning-assisted Evolutionary Algorithm: A Survey and Research Opportunities. (arXiv:2308.13420v2 [cs.NE] CROSS LISTED)

    [http://arxiv.org/abs/2308.13420](http://arxiv.org/abs/2308.13420)

    本文调查了强化学习辅助进化算法（RL-EA），该算法将强化学习与进化算法结合，以提高优化性能。对各种RL-EA的结构、操作符和搜索模式进行了分类和概述。

    

    进化算法是一类基于自然进化原理的随机搜索方法，因其在各种实际优化问题中的卓越性能而广受赞誉。尽管全球的研究人员提出了各种各样的进化算法，但仍存在一些限制，如收敛速度慢和泛化能力差。因此，许多学者积极探索改进算法结构、操作符、搜索模式等方法，以提高其优化性能。近年来，将强化学习作为进化算法框架的一个组成部分，已经展示出超越性能。本文综述了将强化学习集成到进化算法中的最新研究进展，被称为强化学习辅助进化算法（RL-EA）。我们首先介绍了强化学习和进化算法的概念。然后，我们提供了一个对RL-EA中不同结构、操作符和搜索模式的分类方法。

    Evolutionary algorithms (EA), a class of stochastic search methods based on the principles of natural evolution, have received widespread acclaim for their exceptional performance in various real-world optimization problems. While researchers worldwide have proposed a wide variety of EAs, certain limitations remain, such as slow convergence speed and poor generalization capabilities. Consequently, numerous scholars actively explore improvements to algorithmic structures, operators, search patterns, etc., to enhance their optimization performance. Reinforcement learning (RL) integrated as a component in the EA framework has demonstrated superior performance in recent years. This paper presents a comprehensive survey on integrating reinforcement learning into the evolutionary algorithm, referred to as reinforcement learning-assisted evolutionary algorithm (RL-EA). We begin with the conceptual outlines of reinforcement learning and the evolutionary algorithm. We then provide a taxonomy of
    
[^94]: 描述逻辑进入二阶--用全称量化概念扩展EL

    Description Logics Go Second-Order -- Extending EL with Universally Quantified Concepts. (arXiv:2308.08252v1 [cs.LO])

    [http://arxiv.org/abs/2308.08252](http://arxiv.org/abs/2308.08252)

    这篇论文通过引入全称量化概念扩展了描述逻辑$\mathcal{EL}$，分别提出了模式语义和二阶语义，研究了它们的性质并证明了它们在有用片段中的结论相同。

    

    历史上，描述逻辑的研究主要集中在可以翻译成可判定的一阶逻辑片段的特征上。在本文中，我们放弃了这个限制，寻找有用且可判定的一阶逻辑以外的扩展。我们引入了全称量化概念，它们以可以被任意概念替换的变量形式出现，并定义了这个扩展的两种语义。模式语义只允许概念变量被特定语言的概念替换，产生类似模态逻辑的公理模式。二阶语义允许概念变量被域的任意子集替换，类似于二阶逻辑中的量化谓词。为了研究所提出的语义，我们重点关注描述逻辑$\mathcal{EL}$的扩展。我们证明了在扩展的有用片段中，不同语义所蕴含的结论是相同的，从而可以使用经典逻辑的推理方法。

    The study of Description Logics have been historically mostly focused on features that can be translated to decidable fragments of first-order logic. In this paper, we leave this restriction behind and look for useful and decidable extensions outside first-order logic. We introduce universally quantified concepts, which take the form of variables that can be replaced with arbitrary concepts, and define two semantics of this extension. A schema semantics allows replacements of concept variables only by concepts from a particular language, giving us axiom schemata similar to modal logics. A second-order semantics allows replacement of concept variables with arbitrary subsets of the domain, which is similar to quantified predicates in second-order logic.  To study the proposed semantics, we focus on the extension of the description logic $\mathcal{EL}$. We show that for a useful fragment of the extension, the conclusions entailed by the different semantics coincide, allowing us to use cla
    
[^95]: NBIAS: 用于文本中偏见识别的自然语言处理框架

    NBIAS: A Natural Language Processing Framework for Bias Identification in Text. (arXiv:2308.01681v1 [cs.CL])

    [http://arxiv.org/abs/2308.01681](http://arxiv.org/abs/2308.01681)

    本论文提出了一个名为NBIAS的自然语言处理框架，旨在识别文本中的偏见。通过收集来自社交媒体、医疗保健和职位招聘等领域的多样化数据构建数据集，并应用基于Transformer的令牌分类模型来识别偏见词/短语。通过定量和定性评估方法来评估模型的效果。

    

    在文本数据中存在偏见可能导致数据使用时产生倾斜的解释和结果。这些偏见可能会持续强化刻板印象、歧视或其他形式的不公平待遇。在有偏见的数据上训练的算法最终会做出不平等影响某个群体的决策。因此，检测和消除这些偏见至关重要，以确保对数据的公平和道德使用。为此，我们开发了一个全面而强大的框架"NBIAS"，它包括数据层、语料库构建、模型开发层和评估层。数据集由从各个领域收集的多样化数据构建，包括社交媒体、医疗保健和职位招聘门户网站。因此，我们应用了基于Transformer的令牌分类模型，通过一个唯一的命名实体能够识别出偏见词/短语。在评估过程中，我们结合了定量和定性评估方法来评估我们模型的效果。

    Bias in textual data can lead to skewed interpretations and outcomes when the data is used. These biases could perpetuate stereotypes, discrimination, or other forms of unfair treatment. An algorithm trained on biased data ends up making decisions that disproportionately impact a certain group of people. Therefore, it is crucial to detect and remove these biases to ensure the fair and ethical use of data. To this end, we develop a comprehensive and robust framework \textsc{Nbias} that consists of a data layer, corpus contruction, model development layer and an evaluation layer. The dataset is constructed by collecting diverse data from various fields, including social media, healthcare, and job hiring portals. As such, we applied a transformer-based token classification model that is able to identify bias words/ phrases through a unique named entity. In the assessment procedure, we incorporate a blend of quantitative and qualitative evaluations to gauge the effectiveness of our models.
    
[^96]: ChatGPT用于软件安全：探索ChatGPT在安全应用中的优点和局限性

    ChatGPT for Software Security: Exploring the Strengths and Limitations of ChatGPT in the Security Applications. (arXiv:2307.12488v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2307.12488](http://arxiv.org/abs/2307.12488)

    本文通过对ChatGPT在安全导向的程序分析中的表现进行研究，旨在了解其优势和局限性。研究结果可以帮助我们更好地理解ChatGPT在安全领域的应用潜力。

    

    作为一个多才多艺的大型语言模型，ChatGPT在各个领域应对问题的潜力得到了显著的展示。它能够分析、理解和综合来自在线资源和用户输入的信息，引起了广泛的关注。先前的研究已经探索了ChatGPT在代码生成和代码审查方面的能力。在本文中，我们深入研究了ChatGPT在面向安全的程序分析中的能力，从攻击者和安全分析师的角度进行了探讨。我们通过一个案例研究来评估ChatGPT在几个安全导向的程序分析任务中的回答质量，并有意地引入挑战来评估其响应能力。通过对ChatGPT提供的答案质量的考察，我们对其在安全导向的程序分析领域的优点和局限性有了更清晰的认识。

    ChatGPT, as a versatile large language model, has demonstrated remarkable potential in addressing inquiries across various domains. Its ability to analyze, comprehend, and synthesize information from both online sources and user inputs has garnered significant attention. Previous research has explored ChatGPT's competence in code generation and code reviews. In this paper, we delve into ChatGPT's capabilities in security-oriented program analysis, focusing on perspectives from both attackers and security analysts. We present a case study involving several security-oriented program analysis tasks while deliberately introducing challenges to assess ChatGPT's responses. Through an examination of the quality of answers provided by ChatGPT, we gain a clearer understanding of its strengths and limitations in the realm of security-oriented program analysis.
    
[^97]: 通过基于深度学习的策略预测来预测驾驶行为

    Anticipating Driving Behavior through Deep Learning-Based Policy Prediction. (arXiv:2307.11058v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2307.11058](http://arxiv.org/abs/2307.11058)

    通过处理视频帧和深度细节，我们开发了一个综合系统来预测驾驶行为，实现了显著的准确性，比单独使用视频帧更好。

    

    在这个研究中，我们开发了一个综合系统，通过处理由普通摄像头拍摄的视频帧衍生出的综合视觉特征以及从点云扫描仪获得的深度细节。该系统旨在预测驾驶行为，包括车辆速度和转向角度。为了确保其可靠性，我们进行了评估，将预测结果与熟练的真实驾驶员遵循的既定规范进行了对比。我们的评估结果表明，预测在至少一半的测试场景中实现了显著的准确性水平（根据具体模型，在50-80%之间）。值得注意的是，使用综合特征相比于只使用视频帧在大多数情况下表现更好。

    In this endeavor, we developed a comprehensive system that processes integrated visual features derived from video frames captured by a regular camera, along with depth details obtained from a point cloud scanner. This system is designed to anticipate driving actions, encompassing both vehicle speed and steering angle. To ensure its reliability, we conducted assessments where we juxtaposed the projected outcomes with the established norms adhered to by skilled real-world drivers. Our evaluation outcomes indicate that the forecasts achieve a noteworthy level of accuracy in a minimum of half the test scenarios (ranging around 50-80%, contingent on the specific model). Notably, the utilization of amalgamated features yielded superior performance in comparison to using video frames in isolation, as demonstrated by most of the cases.
    
[^98]: 什么是可解释模型：一项范围审查

    What's meant by explainable model: A Scoping Review. (arXiv:2307.09673v1 [cs.AI])

    [http://arxiv.org/abs/2307.09673](http://arxiv.org/abs/2307.09673)

    这项研究通过范围审查方法调查了应用人工智能模型并采用事后解释方法的论文，探讨了可解释模型这一术语的含义。

    

    我们经常在描述基于人工智能（AI）的应用的论文标题中看到可解释这个术语。然而，可解释人工智能（XAI）的文献表明，XAI中的解释是特定应用和领域的，因此在用于解释特定应用问题的模型时需要进行评估。此外，文献揭示了事后方法，特别是特征归因方法的性能存在很大差异，暗示它们并不能成为AI可解释性的解决方案。因此，在使用XAI方法时，应在特定应用中评估其信息输出的质量和适用性。基于这些原因，我们使用了范围审查方法来研究应用AI模型和采用事后解释方法的论文，同时将这些模型称为可解释。

    We often see the term explainable in the titles of papers that describe applications based on artificial intelligence (AI). However, the literature in explainable artificial intelligence (XAI) indicates that explanations in XAI are application- and domain-specific, hence requiring evaluation whenever they are employed to explain a model that makes decisions for a specific application problem. Additionally, the literature reveals that the performance of post-hoc methods, particularly feature attribution methods, varies substantially hinting that they do not represent a solution to AI explainability. Therefore, when using XAI methods, the quality and suitability of their information outputs should be evaluated within the specific application. For these reasons, we used a scoping review methodology to investigate papers that apply AI models and adopt methods to generate post-hoc explanations while referring to said models as explainable. This paper investigates whether the term explainabl
    
[^99]: 评估社交机器人导航算法的原则与指南

    Principles and Guidelines for Evaluating Social Robot Navigation Algorithms. (arXiv:2306.16740v1 [cs.RO])

    [http://arxiv.org/abs/2306.16740](http://arxiv.org/abs/2306.16740)

    本文提出了评估社交机器人导航算法的原则与指南，为解决在人类居住环境中导航的挑战提供了可重复和可比较的基准标准。

    

    在人类居住环境中导航是部署机器人广泛应用的主要挑战，通常被称为社交机器人导航。虽然社交导航领域近年来取得了很大进展，但评估解决社交导航的算法仍然困难，因为它不仅涉及机器人在静态环境中移动，还涉及到动态的人类参与者及其对机器人行为的感知适应性。相比之下，清晰、可重复、易于获得的基准在计算机视觉、自然语言处理和传统机器人导航等领域加速了进展，使研究人员能够公平比较算法，揭示现有解决方案的局限性，并呈现有前途的新方向。我们相信相同的方法可以有助于社交导航。在本文中，我们为评估社交机器人导航建立了共同、广泛可用且可重复的基准标准，并提出了自己的创新点。

    A major challenge to deploying robots widely is navigation in human-populated environments, commonly referred to as social robot navigation. While the field of social navigation has advanced tremendously in recent years, the fair evaluation of algorithms that tackle social navigation remains hard because it involves not just robotic agents moving in static environments but also dynamic human agents and their perceptions of the appropriateness of robot behavior. In contrast, clear, repeatable, and accessible benchmarks have accelerated progress in fields like computer vision, natural language processing and traditional robot navigation by enabling researchers to fairly compare algorithms, revealing limitations of existing solutions and illuminating promising new directions. We believe the same approach can benefit social navigation. In this paper, we pave the road towards common, widely accessible, and repeatable benchmarking criteria to evaluate social robot navigation. Our contributio
    
[^100]: DR-HAI: 人工智能与人类交互中基于论证的辩证调和

    DR-HAI: Argumentation-based Dialectical Reconciliation in Human-AI Interactions. (arXiv:2306.14694v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2306.14694](http://arxiv.org/abs/2306.14694)

    DR-HAI是一个新颖的基于论证的框架，旨在通过互动调和解决人工智能与人类之间的知识差异，为促进有效的人工智能与人类交互提供了一个有希望的方向。

    

    我们提出了DR-HAI，这是一个新颖的基于论证的框架，旨在扩展人类感知规划中常用的模型调和方法，以增强人工智能与人类的交互。通过采用基于论证的对话范式，DR-HAI能够进行互动调和，解决解释者和被解释者之间的知识差异。我们对DR-HAI的操作语义进行了形式化描述，提供了理论保证，并对其效果进行了经验评估。我们的研究结果表明，DR-HAI为促进有效的人工智能与人类交互提供了一个具有潜力的方向。

    We present DR-HAI -- a novel argumentation-based framework designed to extend model reconciliation approaches, commonly used in human-aware planning, for enhanced human-AI interaction. By adopting an argumentation-based dialogue paradigm, DR-HAI enables interactive reconciliation to address knowledge discrepancies between an explainer and an explainee. We formally describe the operational semantics of DR-HAI, provide theoretical guarantees, and empirically evaluate its efficacy. Our findings suggest that DR-HAI offers a promising direction for fostering effective human-AI interactions.
    
[^101]: 大型语言模型被误导：使用Only Connect Wall数据集探索创造性问题解决和Einstellung效应。

    Large Language Models are Fixated by Red Herrings: Exploring Creative Problem Solving and Einstellung Effect using the Only Connect Wall Dataset. (arXiv:2306.11167v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2306.11167](http://arxiv.org/abs/2306.11167)

    这项研究探索了大型语言模型（LLMs）对创造性问题解决的能力，并发现大型语言模型容易被误导，出现固定效应和Einstellung范式。

    

    自从人工智能诞生以来，对人类仿真智能的追求一直是人工智能研究的持久话题。最新一代的大型语言模型（LLM）的技术演进和新兴能力将这个主题从学术界带到了文化时代。尽管最近的NLP评估基准任务测试了人类仿真行为的一些方面（例如BIG-bench的“类人行为”任务），但几乎没有一个任务考察创造性问题解决能力。人类的创造性问题解决是认知神经科学中研究较为深入的主题，标准化测试主要使用将线索词之间的（异构）连接能力作为创造性的度量。在这样的任务中，暗示性的误导性刺激-被称为“诱导误解”的干扰因素-通过固定效应和Einstellung范式阻碍了人类的表现。在认知神经科学的研究中，通过事先让参与者接触到有相似拼写的错误因素来实验性地诱导这样的固定。

    The quest for human imitative AI has been an enduring topic in AI research since its inception. The technical evolution and emerging capabilities of the latest cohort of large language models (LLMs) have reinvigorated the subject beyond academia to the cultural zeitgeist. While recent NLP evaluation benchmark tasks test some aspects of human-imitative behaviour (e.g., BIG-bench's 'human-like behavior' tasks), few, if not none, examine creative problem solving abilities. Creative problem solving in humans is a well-studied topic in cognitive neuroscience with standardized tests that predominantly use the ability to associate (heterogeneous) connections among clue words as a metric for creativity. Exposure to misleading stimuli - distractors dubbed red herrings - impede human performance in such tasks via the fixation effect and Einstellung paradigm. In cognitive neuroscience studies, such fixations are experimentally induced by pre-exposing participants to orthographically similar incor
    
[^102]: Mol-Instructions: 一个大规模生物分子指令数据集，为大语言模型提供支持

    Mol-Instructions: A Large-Scale Biomolecular Instruction Dataset for Large Language Models. (arXiv:2306.08018v1 [q-bio.QM])

    [http://arxiv.org/abs/2306.08018](http://arxiv.org/abs/2306.08018)

    Mol-Instructions是一个专门为生物分子领域设计的综合指令数据集，可以显著提高大语言模型在生物领域中的适应能力和认知敏锐度。

    

    大语言模型（LLM）以其卓越的任务处理能力和创新的输出，在许多领域推动了重大进展。然而，它们在生物分子研究等专业领域的熟练应用还受到限制。为了解决这个挑战，我们介绍了Mol-Instructions，这是一个经过精心策划、专门针对生物分子领域设计的综合指令数据集。Mol-Instructions由三个关键组成部分组成：分子导向指令、蛋白质导向指令和生物分子文本指令，每个部分都被策划用于增强LLM对生物分子特性和行为的理解和预测能力。通过对代表性LLM的广泛指令调整实验，我们强调了Mol-Instructions在增强大模型在生物分子研究复杂领域内的适应能力和认知敏锐度方面的潜力，从而促进生物分子领域的进一步发展。

    Large Language Models (LLMs), with their remarkable task-handling capabilities and innovative outputs, have catalyzed significant advancements across a spectrum of fields. However, their proficiency within specialized domains such as biomolecular studies remains limited. To address this challenge, we introduce Mol-Instructions, a meticulously curated, comprehensive instruction dataset expressly designed for the biomolecular realm. Mol-Instructions is composed of three pivotal components: molecule-oriented instructions, protein-oriented instructions, and biomolecular text instructions, each curated to enhance the understanding and prediction capabilities of LLMs concerning biomolecular features and behaviors. Through extensive instruction tuning experiments on the representative LLM, we underscore the potency of Mol-Instructions to enhance the adaptability and cognitive acuity of large models within the complex sphere of biomolecular studies, thereby promoting advancements in the biomol
    
[^103]: Annotator Demographics Matter - Measuring the Influence of Annotator Demographics with the POPQUORN Dataset

    When Do Annotator Demographics Matter? Measuring the Influence of Annotator Demographics with the POPQUORN Dataset. (arXiv:2306.06826v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2306.06826](http://arxiv.org/abs/2306.06826)

    标注者的背景对数据标注的影响很重要。通过POPQUORN数据集的分析，我们发现标注者的背景在他们的判断中起到了显著作用，并且应该考虑以前未考虑的背景因素。我们的研究建议理解标注者的背景，从具有人口统计学平衡的众包工作者中收集标签，以减少数据集的偏差。

    

    标注者的背景和经历会影响他们对数据的标注，然而，自然语言处理领域只近期开始考虑标注者身份对他们决策的影响。本文介绍了POPQUORN（POtato-Prolific 数据集，用于问题回答、冒犯性、文本改写和礼貌评分，包含人口统计学细微差异）。POPQUORN包含1,484个标注者的45,000个标注，采用了美国人口中性别、年龄和种族的代表样本。通过一系列分析，我们展示了标注者背景在他们的判断中起到了重要作用。此外，我们的工作还表明，在自然语言处理中以前未考虑的背景因素（例如教育）是有意义且应该被考虑的。我们的研究表明，理解标注者的背景，并从具有人口统计学平衡的众包工作者中收集标签，对减少数据集偏差非常重要。

    Annotators are not fungible. Their demographics, life experiences, and backgrounds all contribute to how they label data. However, NLP has only recently considered how annotator identity might influence their decisions. Here, we present POPQUORN (the POtato-Prolific dataset for QUestion-Answering, Offensiveness, text Rewriting, and politeness rating with demographic Nuance). POPQUORN contains 45,000 annotations from 1,484 annotators, drawn from a representative sample regarding sex, age, and race as the US population. Through a series of analyses, we show that annotators' background plays a significant role in their judgments. Further, our work shows that backgrounds not previously considered in NLP (e.g., education), are meaningful and should be considered. Our study suggests that understanding the background of annotators and collecting labels from a demographically balanced pool of crowd workers is important to reduce the bias of datasets. The dataset, annotator background, and anno
    
[^104]: 关于大型模型推理中的最优缓存与模型复用

    On Optimal Caching and Model Multiplexing for Large Model Inference. (arXiv:2306.02003v1 [cs.LG])

    [http://arxiv.org/abs/2306.02003](http://arxiv.org/abs/2306.02003)

    本文提出了最优缓存与模型复用两种方法来缓解大型模型推理中资源消耗和延迟挑战，经过实证模拟发现这种组合大大提高了传统模型推理方法的性能。

    

    大型语言模型和其他大型基础模型已经取得了显著的成功，但其尺寸加剧了现有的资源消耗和延迟挑战。本文研究了两种方法来缓解这些挑战：利用缓存存储先前的查询和学习模型复用器来选择用于查询处理的模型。理论上，我们提供了一种最优算法来联合优化这两种方法，从而减少离线和在线制表环境中的推理成本。通过将缓存算法和模型复用器相结合，我们在离线和在线设置下都实现了最优性能。实证模拟表明，我们的缓存和模型复用算法的组合大大提高了传统模型推理方法的性能。

    Large Language Models (LLMs) and other large foundation models have achieved noteworthy success, but their size exacerbates existing resource consumption and latency challenges. In particular, the large-scale deployment of these models is hindered by the significant resource requirements during inference. In this paper, we study two approaches for mitigating these challenges: employing a cache to store previous queries and learning a model multiplexer to choose from an ensemble of models for query processing.  Theoretically, we provide an optimal algorithm for jointly optimizing both approaches to reduce the inference cost in both offline and online tabular settings. By combining a caching algorithm, namely Greedy Dual Size with Frequency (GDSF) or Least Expected Cost (LEC), with a model multiplexer, we achieve optimal rates in both offline and online settings. Empirically, simulations show that the combination of our caching and model multiplexing algorithms greatly improves over the 
    
[^105]: 在随机博弈中使用奖励机制的强化学习

    Reinforcement Learning With Reward Machines in Stochastic Games. (arXiv:2305.17372v2 [cs.MA] UPDATED)

    [http://arxiv.org/abs/2305.17372](http://arxiv.org/abs/2305.17372)

    该论文研究了复杂任务中具有非马尔可夫回报函数的随机博弈的多智能体强化学习，提出了一种基于奖励机制的算法，在纳什均衡下学习每个智能体的最佳应答策略，并证明了学习的Q函数将会收敛于一个纳什均衡的Q函数，前提是阶段博弈具有全局最优点或者鞍点，并且智能体基于这一点进行最佳应答策略的Q函数更新。

    

    我们研究了复杂任务中具有非马尔可夫回报函数的随机博弈的多智能体强化学习。我们利用奖励机制来整合高层次的复杂任务知识。我们开发了一种称为QRM-SG的算法，用于学习每个智能体在纳什均衡下的最佳应答策略。在QRM-SG中，我们在增广状态空间中定义了纳什均衡下的Q函数。增广状态空间整合了随机博弈的状态和奖励机制的状态。每个智能体学习了系统中所有智能体的Q函数。我们证明了在QRM-SG中学习的Q函数将会收敛于一个纳什均衡的Q函数，前提是在学习过程中每个时间步的阶段博弈具有全局最优点或者鞍点，并且智能体基于这一点进行最佳应答策略的Q函数更新。我们使用Lemke-Howson方法来得出给定当前Q函数时的最佳应答策略。

    We investigate multi-agent reinforcement learning for stochastic games with complex tasks, where the reward functions are non-Markovian. We utilize reward machines to incorporate high-level knowledge of complex tasks. We develop an algorithm called Q-learning with reward machines for stochastic games (QRM-SG), to learn the best-response strategy at Nash equilibrium for each agent. In QRM-SG, we define the Q-function at a Nash equilibrium in augmented state space. The augmented state space integrates the state of the stochastic game and the state of reward machines. Each agent learns the Q-functions of all agents in the system. We prove that Q-functions learned in QRM-SG converge to the Q-functions at a Nash equilibrium if the stage game at each time step during learning has a global optimum point or a saddle point, and the agents update Q-functions based on the best-response strategy at this point. We use the Lemke-Howson method to derive the best-response strategy given current Q-func
    
[^106]: 自动泊车的鱼眼摄像头流式目标检测系统

    Streaming Object Detection on Fisheye Cameras for Automatic Parking. (arXiv:2305.14713v1 [cs.CV])

    [http://arxiv.org/abs/2305.14713](http://arxiv.org/abs/2305.14713)

    这篇论文提出了一个实时检测框架，使用动态和静态流双流感知模块，能够预测未来并减轻时延问题，解决了以往鱼眼摄像头目标检测中时延差异带来的安全隐患问题。

    

    鱼眼摄像头广泛应用于自动泊车，而其视频流目标检测是确保车辆安全运行的基本感知功能。以往的研究忽略了深度学习模型输出与当前实际情况存在的时延差异，然而环境在时延时间内不可避免的会发生变化，可能造成潜在的安全隐患。本文提出了一个配备动态和静态流双流感知模块，可以预测未来并减轻时延问题的实时检测框架。同时，我们使用一种新的方案评估延迟和准确性。标准边界框对于鱼眼摄像头图像中的对象不适用，因为鱼眼摄像头的强烈径向畸变，而停车感知的主要检测对象是车辆和行人。

    Fisheye cameras are widely employed in automatic parking, and the video stream object detection (VSOD) of the fisheye camera is a fundamental perception function to ensure the safe operation of vehicles. In past research work, the difference between the output of the deep learning model and the actual situation at the current moment due to the existence of delay of the perception system is generally ignored. But the environment will inevitably change within the delay time which may cause a potential safety hazard. In this paper, we propose a real-time detection framework equipped with a dual-flow perception module (dynamic and static flows) that can predict the future and alleviate the time-lag problem. Meanwhile, we use a new scheme to evaluate latency and accuracy. The standard bounding box is unsuitable for the object in fisheye camera images due to the strong radial distortion of the fisheye camera and the primary detection objects of parking perception are vehicles and pedestrians
    
[^107]: 多领域LiDAR语义分割中的方法研究和探讨

    Walking Your LiDOG: A Journey Through Multiple Domains for LiDAR Semantic Segmentation. (arXiv:2304.11705v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2304.11705](http://arxiv.org/abs/2304.11705)

    本研究通过设计第一个实验设置，探讨了LiDAR语义分割在不同领域间的泛化能力。研究结果表明，现有方法在跨领域设置中存在显著差距。为了解决这个问题，我们提出了一种新的方法，通过结合稀疏-密集卷积网络，实现了在目标领域上的显著优化。

    

    在开发智能机器人的过程中，能够安全地在多样化的环境中运行是至关重要的。我们在同一领域的LiDAR语义分割方面取得了巨大的进展。然而，这些方法是否能够在不同领域之间泛化是一个问题。为了回答这个问题，我们设计了第一个用于研究领域泛化（DG-LSS）的实验设置。我们的结果表明，在跨领域的设置中评估方法之间存在显著差距：例如，在源数据集（SemanticKITTI）上训练的模型在目标数据上获得了26.53的mIoU，而在目标域（nuScenes）上训练的模型则获得了48.49的mIoU。为了解决这个差距，我们提出了第一个专门为DG-LSS设计的方法，在目标领域上获得了34.88的mIoU，超越了所有其他基线模型。我们的方法通过将稀疏卷积编码器-解码器3D分割网络与额外的密集2D卷积相结合，

    The ability to deploy robots that can operate safely in diverse environments is crucial for developing embodied intelligent agents. As a community, we have made tremendous progress in within-domain LiDAR semantic segmentation. However, do these methods generalize across domains? To answer this question, we design the first experimental setup for studying domain generalization (DG) for LiDAR semantic segmentation (DG-LSS). Our results confirm a significant gap between methods, evaluated in a cross-domain setting: for example, a model trained on the source dataset (SemanticKITTI) obtains $26.53$ mIoU on the target data, compared to $48.49$ mIoU obtained by the model trained on the target domain (nuScenes). To tackle this gap, we propose the first method specifically designed for DG-LSS, which obtains $34.88$ mIoU on the target domain, outperforming all baselines. Our method augments a sparse-convolutional encoder-decoder 3D segmentation network with an additional, dense 2D convolutional 
    
[^108]: 基于深度学习的时间序列因果推断量化北极放大的原因

    Quantifying Causes of Arctic Amplification via Deep Learning based Time-series Causal Inference. (arXiv:2303.07122v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2303.07122](http://arxiv.org/abs/2303.07122)

    该研究提出了一种基于循环神经网络的时间序列因果推断模型TCINet，用于推断大气过程对海冰融化的因果效应。通过实验证明，该模型能够显著提高量化北极海冰融化的主要原因的能力。

    

    北极变暖，也称北极放大，由多种大气和海洋因素导致，但其基础热力因素的详细情况仍不清楚。使用固定治疗效应策略推断大气过程对海冰融化的因果效应会导致不现实的反事实估计。这样的模型也容易受到时间变化的混淆的影响而引起偏差。为了解决这些挑战，我们提出了TCINet - 一种基于循环神经网络的时间序列因果推断模型，以连续治疗方式推断因果关系。通过对合成和观测数据的实验，我们展示了我们的研究如何大大提高量化北极海冰融化的主要原因的能力。

    The warming of the Arctic, also known as Arctic amplification, is led by several atmospheric and oceanic drivers, however, the details of its underlying thermodynamic causes are still unknown. Inferring the causal effects of atmospheric processes on sea ice melt using fixed treatment effect strategies leads to unrealistic counterfactual estimations. Such models are also prone to bias due to time-varying confoundedness. In order to tackle these challenges, we propose TCINet - time-series causal inference model to infer causation under continuous treatment using recurrent neural networks. Through experiments on synthetic and observational data, we show how our research can substantially improve the ability to quantify the leading causes of Arctic sea ice melt.
    
[^109]: 论ChatGPT的鲁棒性：对抗性和超出分布的视角。

    On the Robustness of ChatGPT: An Adversarial and Out-of-distribution Perspective. (arXiv:2302.12095v4 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2302.12095](http://arxiv.org/abs/2302.12095)

    本研究评估了ChatGPT的鲁棒性，发现其在对抗性和超出分布任务上有一致的优势，但绝对表现仍有提高空间，鲁棒性仍是一个重要的挑战。

    

    ChatGPT是OpenAI最近发布的聊天机器人服务，并在过去几个月中受到越来越多的关注。虽然已对ChatGPT的各个方面进行了评估，但其鲁棒性，即对于未预期输入的表现，仍不清楚。鲁棒性在负责任的AI中特别受关注，特别是对于安全关键应用程序。在本文中，我们从对抗性和超出分布（OOD）的角度对ChatGPT的鲁棒性进行了彻底评估。为此，我们采用了AdvGLUE和ANLI基准来评估对抗性鲁棒性，采用Flipkart评论和DDXPlus医学诊断数据集进行OOD评估。我们选择了几个流行的基础模型作为基准。结果表明，ChatGPT在大多数对抗性和OOD分类和翻译任务上表现出一致的优势。但是，绝对的表现远非完美，这表明对抗性和OOD鲁棒性仍然是一个重要的威胁。

    ChatGPT is a recent chatbot service released by OpenAI and is receiving increasing attention over the past few months. While evaluations of various aspects of ChatGPT have been done, its robustness, i.e., the performance to unexpected inputs, is still unclear to the public. Robustness is of particular concern in responsible AI, especially for safety-critical applications. In this paper, we conduct a thorough evaluation of the robustness of ChatGPT from the adversarial and out-of-distribution (OOD) perspective. To do so, we employ the AdvGLUE and ANLI benchmarks to assess adversarial robustness and the Flipkart review and DDXPlus medical diagnosis datasets for OOD evaluation. We select several popular foundation models as baselines. Results show that ChatGPT shows consistent advantages on most adversarial and OOD classification and translation tasks. However, the absolute performance is far from perfection, which suggests that adversarial and OOD robustness remains a significant threat 
    
[^110]: 不可靠部分标签学习与递归分离

    Unreliable Partial Label Learning with Recursive Separation. (arXiv:2302.09891v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2302.09891](http://arxiv.org/abs/2302.09891)

    本论文提出了一种名为Unreliable Partial Label Learning with Recursive Separation (UPLLRS)的两阶段框架来解决不可靠部分标签学习问题，在第一阶段采用自适应递归分离策略将训练集分为可靠子集和不可靠子集，在第二阶段利用这些子集的信息来提高学习效果。

    

    部分标签学习（PLL）是一个典型的弱监督学习问题，即每个实例都与一个候选标签集相关，其中只有一个是真实的。然而，假设真实标签总是在候选标签集中是不现实的，在现实应用中，候选标签集的可靠性不能由注释者保证。因此，提出了一种名为Unreliable Partial Label Learning（UPLL）的广义PLL方法，其中真实标签可能不在候选标签集中。由于不可靠标注带来的挑战，先前的PLL方法在应用到UPLL时会遇到明显的性能下降。为了解决这个问题，我们提出了一个名为Unreliable Partial Label Learning with Recursive Separation（UPLLRS）的两阶段框架。在第一阶段，提出了自适应递归分离策略，将训练集分为可靠子集和不可靠子集。在第二个阶段中，我们提出了一种基于递归分离的UPLL学习算法，通过利用可信和不可信子集的信息，以解决不可靠标注问题。

    Partial label learning (PLL) is a typical weakly supervised learning problem in which each instance is associated with a candidate label set, and among which only one is true. However, the assumption that the ground-truth label is always among the candidate label set would be unrealistic, as the reliability of the candidate label sets in real-world applications cannot be guaranteed by annotators. Therefore, a generalized PLL named Unreliable Partial Label Learning (UPLL) is proposed, in which the true label may not be in the candidate label set. Due to the challenges posed by unreliable labeling, previous PLL methods will experience a marked decline in performance when applied to UPLL. To address the issue, we propose a two-stage framework named Unreliable Partial Label Learning with Recursive Separation (UPLLRS). In the first stage, the self-adaptive recursive separation strategy is proposed to separate the training set into a reliable subset and an unreliable subset. In the second st
    
[^111]: 基于去偏自注意力的公平感知视觉变换器

    Fairness-aware Vision Transformer via Debiased Self-Attention. (arXiv:2301.13803v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2301.13803](http://arxiv.org/abs/2301.13803)

    这篇论文提出了一种基于去偏自注意力的公平感知视觉变换器框架，通过消除与敏感属性相关的虚假特征来减轻偏见，并利用对抗性示例来定位和屏蔽这些特征。

    

    最近，由于其提取信息特征和通过自我关注机制建模长距离依赖关系的能力，视觉变换器（ViT）在解决计算机视觉（CV）问题方面引起了广泛关注。为了充分发挥ViT在现实应用中的优势，最近的研究探索了ViT的可靠性和可解释性，包括其鲁棒性和可解释性。然而，另一个需求，公平性，在文献中尚未得到充分解决。我们证明了现有的公平感知算法（主要设计用于CNN）在ViT上表现不佳。这就需要我们通过去偏自注意（DSA）开发我们的新框架。DSA是一种通过盲目方法来强制ViT消除与敏感属性相关的虚假特征以减轻偏见的方法。值得注意的是，对抗性示例被用来定位和屏蔽输入图像块中的虚假特征。

    Vision Transformer (ViT) has recently gained significant interest in solving computer vision (CV) problems due to its capability of extracting informative features and modeling long-range dependencies through the self-attention mechanism. To fully realize the advantages of ViT in real-world applications, recent works have explored the trustworthiness of ViT, including its robustness and explainability. However, another desiderata, fairness has not yet been adequately addressed in the literature. We establish that the existing fairness-aware algorithms (primarily designed for CNNs) do not perform well on ViT. This necessitates the need for developing our novel framework via Debiased Self-Attention (DSA). DSA is a fairness-through-blindness approach that enforces ViT to eliminate spurious features correlated with the sensitive attributes for bias mitigation. Notably, adversarial examples are leveraged to locate and mask the spurious features in the input image patches. In addition, DSA u
    
[^112]: 走向AI-enabled连接产业: AGV通信和传感器测量数据集

    Towards an AI-enabled Connected Industry: AGV Communication and Sensor Measurement Datasets. (arXiv:2301.03364v3 [cs.NI] UPDATED)

    [http://arxiv.org/abs/2301.03364](http://arxiv.org/abs/2301.03364)

    本文介绍了两个无线测量活动所提供的数据集，并将其与机器学习结合起来用于指纹识别、视线检测、服务质量预测或链路选择等任务。

    

    本文介绍了两个工业测试平台上的无线测量活动: 工业车辆间通信(iV2V)和工业车辆到基础设施加传感器(iV2I+)。提供了关于这两个捕获数据集的详细信息。iV2V涵盖了自动引导车(AGVs)之间的侧向链路通信场景，而iV2I+则是在工业设置中进行的，其中自主清洁机器人连接到私有蜂窝网络。不同通信技术的组合，连同共同的测量方法，提供了机器学习(ML)可以利用的洞察力，用于指纹识别、视线检测、服务质量预测或链路选择等任务。此外，数据集已标记和预过滤，以便快速启动和应用。对于两个数据集，还详细介绍了相应的测试平台和测量情况。

    This paper presents two wireless measurement campaigns in industrial testbeds: industrial Vehicle-to-vehicle (iV2V) and industrial Vehicle-to-infrastructure plus Sensor (iV2I+). Detailed information about the two captured datasets is provided as well. iV2V covers sidelink communication scenarios between Automated Guided Vehicles (AGVs), while iV2I+ is conducted at an industrial setting where an autonomous cleaning robot is connected to a private cellular network. The combination of different communication technologies, together with a common measurement methodology, provides insights that can be exploited by Machine Learning (ML) for tasks such as fingerprinting, line-of-sight detection, prediction of quality of service or link selection. Moreover, the datasets are labelled and pre-filtered for fast on-boarding and applicability. The corresponding testbeds and measurements are also presented in detail for both datasets.
    
[^113]: 前期训练在终身学习中的作用的实证研究

    An Empirical Investigation of the Role of Pre-training in Lifelong Learning. (arXiv:2112.09153v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2112.09153](http://arxiv.org/abs/2112.09153)

    这项研究通过对大型预训练模型在多个任务上的性能评估，发现通用的前期训练可以在终身学习中减轻灾难性遗忘的影响。

    

    机器学习中的终身学习范式不仅因其类似生物学习的特性而具有吸引力，而且因其通过避免过多的模型重新训练而减少能源浪费的潜力而备受关注。这一范式面临的关键挑战是灾难性遗忘现象。随着预训练模型在机器学习中的日益流行和成功，我们提出一个问题：在终身学习中，前期训练在灾难性遗忘方面扮演何种角色？我们在大型预训练模型的背景下研究现有方法，并在各种文本和图像分类任务中评估它们的性能，包括使用一个新颖的包含15个不同自然语言处理任务的数据集进行的大规模研究。在所有设置中，我们观察到与随机初始化模型相比，通用的前期训练在学习多个任务时隐含地缓解了灾难性遗忘的影响。

    The lifelong learning paradigm in machine learning is an attractive alternative to the more prominent isolated learning scheme not only due to its resemblance to biological learning but also its potential to reduce energy waste by obviating excessive model re-training. A key challenge to this paradigm is the phenomenon of catastrophic forgetting. With the increasing popularity and success of pre-trained models in machine learning, we pose the question: What role does pre-training play in lifelong learning, specifically with respect to catastrophic forgetting? We investigate existing methods in the context of large, pre-trained models and evaluate their performance on a variety of text and image classification tasks, including a large-scale study using a novel data set of 15 diverse NLP tasks. Across all settings, we observe that generic pre-training implicitly alleviates the effects of catastrophic forgetting when learning multiple tasks sequentially compared to randomly initialized mo
    

