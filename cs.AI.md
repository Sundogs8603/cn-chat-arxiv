# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Do Language Models Exhibit the Same Cognitive Biases in Problem Solving as Human Learners?](https://arxiv.org/abs/2401.18070) | 该研究调查了语言模型在解决算术问题时与人类学习者的认知偏见。研究发现，当前最先进的语言模型在文本理解和解决方案规划阶段表现出与人类类似的偏见。 |
| [^2] | [SpeechComposer: Unifying Multiple Speech Tasks with Prompt Composition](https://arxiv.org/abs/2401.18045) | SpeechComposer是一种能够通过合成固定提示标记来统一多个语音任务的语音语言模型。它提高了任务之间的连接性，并且可以轻松扩展到更多的任务，同时实现了任务之间的知识共享。 |
| [^3] | [Enhancing End-to-End Multi-Task Dialogue Systems: A Study on Intrinsic Motivation Reinforcement Learning Algorithms for Improved Training and Adaptability](https://arxiv.org/abs/2401.18040) | 本研究旨在通过内在动机强化学习算法改进端到端多任务对话系统的训练和适应性。通过教授智能体一个内在奖励系统，可以加速训练并提高其判断行为质量的能力。 |
| [^4] | [Paramanu: A Family of Novel Efficient Indic Generative Foundation Language Models](https://arxiv.org/abs/2401.18034) | Paramanu是一种高效的印度生成式基础语言模型系列，包含多种印度语言模型，并且在单个GPU上进行了从头预训练。它还包括一个先进的印度分词器以及避免多语言诅咒的预训练方法。这些模型在人工评估中展现出良好的语法、连贯性、创造性和事实准确性。 |
| [^5] | [Supporting Anticipatory Governance using LLMs: Evaluating and Aligning Large Language Models with the News Media to Anticipate the Negative Impacts of AI](https://arxiv.org/abs/2401.18028) | 本研究探讨了使用LLMs来增强预测人工智能负面影响的方法，并通过与新闻媒体对齐，建立了一个包含十个类别的人工智能影响分类法。结果表明，基于指令和迁移学习的LLMs模型在生成影响方面具有一定的能力。 |
| [^6] | [Prompt-Driven LLM Safeguarding via Directed Representation Optimization](https://arxiv.org/abs/2401.18018) | 通过研究模型表示的影响，我们发现安全提示并没有明显增强恶意和无害查询之间的区分，并提出了一种名为DRO的方法，用于自动优化安全提示。 |
| [^7] | [Shrub of a thousand faces: an individual segmentation from satellite images using deep learning](https://arxiv.org/abs/2401.17985) | 该研究提出了一种新方法，利用深度学习模型从卫星图像中个别描绘位于西班牙内华达山脉林线上的杜松灌木。研究通过整合遥感影像和实例分割模型，解决了检测和描绘自然对象复杂生长模式的挑战。 |
| [^8] | [Enhancing Multimodal Large Language Models with Vision Detection Models: An Empirical Study](https://arxiv.org/abs/2401.17981) | 本文通过实证研究，将最先进的目标检测和光学字符识别模型与多模态大型语言模型相结合，在提高图像理解和减少回答错误插入方面取得了显著改进。 |
| [^9] | [Circuit Partitioning for Multi-Core Quantum Architectures with Deep Reinforcement Learning](https://arxiv.org/abs/2401.17976) | 本文使用深度强化学习技术，提出了一种适应多核量子架构的电路分区方法，旨在推动量子计算和图分区领域的发展。 |
| [^10] | [Understanding polysemanticity in neural networks through coding theory](https://arxiv.org/abs/2401.17975) | 通过应用编码理论和神经科学工具，本文提出了一种新颖的实践方法来理解神经网络中的多语义性，并对多语义神经元对学习性能的优势进行了解释。 |
| [^11] | [MelNet: A Real-Time Deep Learning Algorithm for Object Detection](https://arxiv.org/abs/2401.17972) | MelNet是一种用于实时目标检测的深度学习算法，经过训练后，在KITTI数据集上表现出良好的性能，同时也展示了迁移学习和定制模型在目标检测中的有效性。 |
| [^12] | [Attention Graph for Multi-Robot Social Navigation with Deep Reinforcement Learning](https://arxiv.org/abs/2401.17914) | 本文提出了一种基于深度强化学习的方法，用于学习多机器人社交导航策略。方法通过使用图神经网络和注意机制来建模机器人和行人之间的交互，创新地考虑了多机器人场景。 |
| [^13] | [ReplaceAnything3D:Text-Guided 3D Scene Editing with Compositional Neural Radiance Fields](https://arxiv.org/abs/2401.17895) | ReplaceAnything3D是一种文本引导的3D场景编辑方法，通过使用Erase-and-Replace方法，能够在保持3D一致性的情况下，替换场景中的特定对象，展示了在各种逼真的3D场景中修改前景对象的多功能性。 |
| [^14] | [Efficient Subseasonal Weather Forecast using Teleconnection-informed Transformers](https://arxiv.org/abs/2401.17870) | 提出了一种使用远程连接通知的Transformer模型来实现高效的次季节天气预报，该方法通过利用预训练模型和集成远程连接通知的时间模块来改进预测能力。 |
| [^15] | [Making Sense of Knowledge Intensive Processes: an Oil & Gas Industry Scenario](https://arxiv.org/abs/2401.17866) | 本研究在油气工业的知识密集型过程中找到了不同类型的知识可以组合来形成协作决策过程的理解结果。我们还讨论了利用人工智能驱动的系统来实现这些想法的可能性。 |
| [^16] | [Manipulating Predictions over Discrete Inputs in Machine Teaching](https://arxiv.org/abs/2401.17865) | 本文研究离散域中的机器教学，在操纵学生模型的预测方面具有显著的数值优势，可用于矫正错误的预测或恶意操纵模型实现个人利益。 |
| [^17] | [Explainable Benchmarking for Iterative Optimization Heuristics](https://arxiv.org/abs/2401.17842) | 本文介绍了一种称为可解释基准测试的新方法，并提出了IOH-Xplainer软件框架，用于分析和理解优化算法的性能和影响。通过该框架，研究人员可以评估和解释迭代优化启发式方法在不同场景下的行为和效率。 |
| [^18] | [Global-Liar: Factuality of LLMs over Time and Geographic Regions](https://arxiv.org/abs/2401.17839) | 本论文评估了GPT模型的事实准确性、稳定性和偏见，并引入了一个平衡数据集"全球说谎者"，结果显示较新的GPT模型并不总是意味着性能的提升，并且观察到一个全球南方陈述被偏袒的问题。 |
| [^19] | [A Cross-View Hierarchical Graph Learning Hypernetwork for Skill Demand-Supply Joint Prediction](https://arxiv.org/abs/2401.17838) | 本文提出了一个跨视角分层图学习超网络（CHGH）框架，用于联合预测技能需求和供应。框架包括跨视角图编码器、层次图编码器和条件超解码器，能够捕捉不同技能之间的关系和需求供应的内在联系。 |
| [^20] | [Leveraging Swin Transformer for Local-to-Global Weakly Supervised Semantic Segmentation](https://arxiv.org/abs/2401.17828) | 本研究利用Swin Transformer提出了"SWTformer"，通过从局部到全局的视角来增强弱监督语义分割的准确性。 |
| [^21] | [Neural Machine Translation for Malayalam Paraphrase Generation](https://arxiv.org/abs/2401.17827) | 本研究针对马拉亚拉姆语探索了四种生成改写的方法，并使用了英语改写和预训练的神经机器翻译模型的资源。研究发现，自动化评估指标不完全适用于马拉亚拉姆语，强调了对于高度合词性语言更细致的改写评估方法的需求。 |
| [^22] | [Deterministic Computing Power Networking: Architecture, Technologies and Prospects](https://arxiv.org/abs/2401.17812) | 提出了一种名为确定性计算能力网络（Det-CPN）的新网络范式，旨在为计算密集型和延迟敏感的任务等新型互联网服务提供端到端传输确定性和计算确定性。该研究对于确保服务的安全高效运行具有重要意义。 |
| [^23] | [SWEA: Changing Factual Knowledge in Large Language Models via Subject Word Embedding Altering](https://arxiv.org/abs/2401.17809) | 提出了一种主题词嵌入修改框架（SWEA），通过在推理阶段修改主题的表示来编辑知识，保护模型的原始权重，避免不可逆的损害和额外的推理开销。 |
| [^24] | [Biospheric AI](https://arxiv.org/abs/2401.17805) | 生物圈人工智能是一种新的范式，以生态中心主义为基础，旨在捕捉生物圈的复杂性并确保人工智能不会对其造成损害。 |
| [^25] | [Distillation Enhanced Time Series Forecasting Network with Momentum Contrastive Learning](https://arxiv.org/abs/2401.17802) | 本论文提出了一种创新的蒸馏增强框架，用于长序列时间序列预测。通过设计可学习的数据增强机制和带有动量更新的对比学习任务，能够充分利用时间序列数据的复杂性，并获得更鲁棒的表示。 |
| [^26] | [Graph Transformers without Positional Encodings](https://arxiv.org/abs/2401.17791) | 本文介绍了一种不需要位置编码的图变压器模型，该模型通过注意机制本身包含图结构信息，并通过实验证明了其有效性。 |
| [^27] | [SDRDPy: An application to graphically visualize the knowledge obtained with supervised descriptive rule algorithms](https://arxiv.org/abs/2401.17783) | SDRDPy是一款可图形化显示通过监督性描述性规则算法获得的知识的应用程序，通过直观的图表和表格帮助专家分析数据集的相关信息和规则之间的关系，以及导出报告。 |
| [^28] | [Double InfoGAN for Contrastive Analysis](https://arxiv.org/abs/2401.17776) | 这篇论文提出了双重InfoGAN方法用于对比分析，通过结合GAN的高质量合成和InfoGAN的分离能力，有效地解决了当前基于VAE的方法在处理共同因素和特殊因素时的不足，并在不同的视觉数据集上展现出优越的性能。 |
| [^29] | [PF-GNN: Differentiable particle filtering based approximation of universal graph representations](https://arxiv.org/abs/2401.17752) | PF-GNN是一种可微的基于粒子滤波的通用图表示逼近算法，通过引入精确同构求解技术指导学习过程，从而提高了图神经网络的表达能力。 |
| [^30] | [SwarmBrain: Embodied agent for real-time strategy game StarCraft II via large language models](https://arxiv.org/abs/2401.17749) | SwarmBrain是一个利用LLM在星际争霸II游戏环境中实现实时策略的具身化智能体，由Overmind Intelligence Matrix和Swarm ReflexNet两个关键组件组成。Overmind Intelligence Matrix负责宏观策略的决策和协调，而Swarm ReflexNet处理微观级别的战术决策。 |
| [^31] | [Haris: an Advanced Autonomous Mobile Robot for Smart Parking Assistance](https://arxiv.org/abs/2401.17741) | 本文介绍了Haris，一个先进的自主移动机器人系统，通过车牌识别追踪拥挤停车场中车辆的位置，消除了对GPS的依赖，并提供了便捷的用户界面来帮助用户确定车辆位置和缓解停车场拥堵。 |
| [^32] | [Operator learning without the adjoint](https://arxiv.org/abs/2401.17739) | 本论文提出了一种不需要探测伴随算子的算子学习方法，通过在Fourier基上进行投影来逼近一类非自伴随的无限维紧算子，并应用于恢复椭圆型偏微分算子的格林函数。这是第一个试图填补算子学习理论与实践差距的无需伴随算子分析。 |
| [^33] | [Towards Physical Plausibility in Neuroevolution Systems](https://arxiv.org/abs/2401.17733) | 本研究关注神经进化系统中物理合理性的问题，提出了一种旨在最大化人工神经网络模型准确性同时最小化功耗的方法，通过引入新的突变策略和训练技术来优化模型表现和节能效果。 |
| [^34] | [Prediction of multitasking performance post-longitudinal tDCS via EEG-based functional connectivity and machine learning methods](https://arxiv.org/abs/2401.17711) | 本研究使用脑电功能连接和机器学习方法，预测经过长期tDCS干预后的多任务认知表现的变化。 |
| [^35] | [Aesthetic Preference Prediction in Interior Design: Fuzzy Approach](https://arxiv.org/abs/2401.17710) | 本论文介绍了一种基于模糊逻辑和图像处理的方法，用于量化和预测室内设计中的审美偏好。研究结合了颜色和谐度、亮度和复杂度等视觉属性，通过加权平均法计算总体审美得分，并考虑个人颜色偏好。 |
| [^36] | [Classification of executive functioning performance post-longitudinal tDCS using functional connectivity and machine learning methods](https://arxiv.org/abs/2401.17700) | 本研究使用功能连接和机器学习方法对长期经颅直流电刺激（tDCS）后的执行功能表现进行了分类，为理解和研究该干预方式对人类认知过程的影响提供了重要的参考。 |
| [^37] | [EnCLAP: Combining Neural Audio Codec and Audio-Text Joint Embedding for Automated Audio Captioning](https://arxiv.org/abs/2401.17690) | EnCLAP是一种自动音频字幕生成框架，利用神经音频编解码器和音频文本联合嵌入技术，通过引入masked codec modeling训练目标提高了预训练语言模型的声学意识，实验证明其在AudioCaps和Clotho数据集上优于基准模型的性能。 |
| [^38] | [Contextual Feature Extraction Hierarchies Converge in Large Language Models and the Brain](https://arxiv.org/abs/2401.17671) | 在研究中发现，随着大型语言模型在基准任务上的性能提高，模型不仅在预测神经响应时表现出更高的类脑性能，而且其分层特征提取路径与大脑的映射更接近，并且使用更少的层次来进行相同的编码。 |
| [^39] | [Towards the implementation of Industry 4.0: A methodology-based approach oriented to the customer life cycle](https://arxiv.org/abs/2401.17661) | 这项研究提出了一种以客户生命周期为导向的方法论，可以帮助中小型制造企业的软件工程师在实施工业4.0技术方面取得成功。 |
| [^40] | [An attempt to generate new bridge types from latent space of energy-based model](https://arxiv.org/abs/2401.17657) | 使用能量模型进行桥梁创新，通过博弈论解释损失函数，并利用朗之万动力学技术生成能量值较低的新样本，建立基于能量的桥梁生成模型。 |
| [^41] | [ReSLLM: Large Language Models are Strong Resource Selectors for Federated Search](https://arxiv.org/abs/2401.17645) | 大型语言模型在联邦搜索中展现出强大的资源选择能力，相比于传统的基于特征的学习方法具有更高的效果和更低的成本。 |
| [^42] | [Navigating the OverKill in Large Language Models](https://arxiv.org/abs/2401.17633) | 本研究调查了大型语言模型中过度杀伤的因素，并发现了其中存在的捷径和对有害词语的过度关注。我们提出了自对比解码（Self-CD）策略来缓解过度杀伤现象，该策略无需训练且适用于各种模型。 |
| [^43] | [Generative AI to Generate Test Data Generators](https://arxiv.org/abs/2401.17626) | 本研究评估了生成式人工智能在不同领域生成测试数据的能力，并证明它能够在不同的集成级别上生成逼真的测试数据生成器。 |
| [^44] | [Unveiling the Power of Self-supervision for Multi-view Multi-human Association and Tracking](https://arxiv.org/abs/2401.17617) | 本研究提出了一种自监督学习感知的网络，通过考虑反身性、对称性和传递性属性，解决了多视角多人物关联与跟踪的问题。 |
| [^45] | [Good at captioning, bad at counting: Benchmarking GPT-4V on Earth observation data](https://arxiv.org/abs/2401.17600) | 在这项研究中，我们对GPT-4V模型在地球观测数据上的性能进行了评估。结果显示，尽管GPT-4V在开放式任务如位置理解和图像标注方面表现良好，但其空间推理能力不足，限制了其在目标定位和计数方面的实用性。 |
| [^46] | [Local Feature Matching Using Deep Learning: A Survey](https://arxiv.org/abs/2401.17592) | 本文调查了使用深度学习进行局部特征匹配的方法。局部特征匹配在计算机视觉中的各个领域具有广泛应用，但是由于视角和光照变化等因素，匹配的准确性和鲁棒性仍然存在挑战。近年来，深度学习模型的引入使得局部特征匹配技术得到了广泛研究。本文对局部特征匹配方法进行了全面概述，并对之前的工作进行了评估。 |
| [^47] | [Propagation and Pitfalls: Reasoning-based Assessment of Knowledge Editing through Counterfactual Tasks](https://arxiv.org/abs/2401.17585) | 该论文针对现有的知识编辑方法在推理能力方面的限制，通过引入ReCoE数据集进行了深入分析。研究发现所有的模型编辑方法在该数据集上表现较差，尤其在特定的推理方案中。此外，通过对编辑模型思维链生成的分析，揭示了现有方法的不足之处，包括对事实编辑、事实回忆能力和连贯性的考量。 |
| [^48] | [Agile But Safe: Learning Collision-Free High-Speed Legged Locomotion](https://arxiv.org/abs/2401.17583) | 本文介绍了一种名为敏捷但安全（ABS）的学习控制框架，能够实现四足机器人的敏捷且无碰撞行走。该框架通过一个学习得到的控制论到达-避免值网络来实现策略切换，并通过协作运行的敏捷策略和恢复策略，使机器人能够高速且安全地导航。 |
| [^49] | [Rethinking Channel Dependence for Multivariate Time Series Forecasting: Learning from Leading Indicators](https://arxiv.org/abs/2401.17548) | 本文提出了一种新方法，LIFT，通过利用通道相关性和领先指标，为多元时间序列预测提供准确的预测。LIFT方法可以无缝与任意时间序列预测方法协作，大量实验证明了其有效性。 |
| [^50] | [Data-Effective Learning: A Comprehensive Medical Benchmark](https://arxiv.org/abs/2401.17542) | 这项研究引入了一个综合基准，用于评估医学领域的数据有效学习。该基准包括大量的医疗数据样本、基准方法和新的评估指标，能够准确评估数据有效学习的性能。 |
| [^51] | [Learning to Stop Cut Generation for Efficient Mixed-Integer Linear Programming](https://arxiv.org/abs/2401.17527) | 这篇论文提出了一种学习停止割生成的方法，以提高混合整数线性规划的效率。通过将问题形式化为强化学习问题，并使用混合图表示模型进行学习，该方法能够动态地决策何时停止割生成，并有效捕捉混合整数线性规划的动态和静态特征。 |
| [^52] | [A PNP ion channel deep learning solver with local neural network and finite element input data](https://arxiv.org/abs/2401.17513) | 该论文提出了一个使用本地神经网络和有限元求解器的PNP离子通道深度学习求解器。该求解器能够较快地训练并生成高准确度的数值解，在处理不同扰动情况和离子通道子区域时表现良好。 |
| [^53] | [Linguistically Communicating Uncertainty in Patient-Facing Risk Prediction Models](https://arxiv.org/abs/2401.17511) | 本文讨论了在医疗保健领域中面向患者的风险预测模型中不确定性量化的挑战，并提出了一种设计来应对这些挑战，重点关注体外受精结果预测的具体应用。 |
| [^54] | [Arrows of Time for Large Language Models](https://arxiv.org/abs/2401.17505) | 这篇论文通过研究自回归大型语言模型的时间方向性，发现了模型在建模自然语言能力上存在时间上的不对称性。从信息理论的角度来看，这种差异理论上是不应该存在的。通过稀疏性和计算复杂性的考虑，提供了一个理论框架来解释这种不对称性的出现。 |
| [^55] | [LeTO: Learning Constrained Visuomotor Policy with Differentiable Trajectory Optimization](https://arxiv.org/abs/2401.17500) | LeTO 是一种通过可微分轨迹优化实现受限视觉运动策略的学习方法，它将优化层表示为轨迹优化问题，使模型能以安全可控的方式端到端生成动作。通过引入约束信息，实现了平衡满足约束、平滑轨迹和最小化演示误差的训练目标。在仿真和实际机器人中进行了评估，表明LeTO方法在成功率上与最先进的模仿学习方法相当。 |
| [^56] | [Detecting mental disorder on social media: a ChatGPT-augmented explainable approach](https://arxiv.org/abs/2401.17477) | 本文提出了一种利用大型语言模型，可解释人工智能和对话代理器ChatGPT相结合的新方法，以解决通过社交媒体检测抑郁症的可解释性挑战。通过将Twitter特定变体BERTweet与自解释模型BERT-XDD相结合，并借助ChatGPT将技术解释转化为人类可读的评论，实现了解释能力的同时提高了可解释性。这种方法可以为发展社会负责任的数字平台，促进早期干预做出贡献。 |
| [^57] | [Synthetic Dialogue Dataset Generation using LLM Agents](https://arxiv.org/abs/2401.17461) | 本文提出了使用LLM智能体生成合成对话数据集的方法，通过对话智能体和用户进行交流来获取生成线性模型所需的关键信息，并提出了对话的外部评估方法。 |
| [^58] | [A Preliminary Study on Using Large Language Models in Software Pentesting](https://arxiv.org/abs/2401.17459) | 本研究探索了在软件渗透测试中使用大型语言模型（LLM）的潜力，并证明了通过改进提示来提高模型准确性的可行性。 |
| [^59] | [Multiscale Parallel Tempering for Fast Sampling on Redistricting Plans](https://arxiv.org/abs/2401.17455) | 本研究提出了一种多尺度并行调温方法，用于快速采样角色分割方案。该方法通过局部移动在不同尺度进行采样，可以满足各种基于政策的指标，并在康涅狄格州进行了评估。 |
| [^60] | [Liquid Democracy for Low-Cost Ensemble Pruning](https://arxiv.org/abs/2401.17443) | 本文介绍了一种利用液态民主实现低成本集合剪枝的方法。通过液态民主的委派机制识别和移除冗余分类器，成功降低了集合训练的计算成本，并比某些增强方法具有更高的准确性。同时，本文还展示了计算社会选择文献框架在非传统领域问题中的应用。 |
| [^61] | [Explaining Predictive Uncertainty by Exposing Second-Order Effects](https://arxiv.org/abs/2401.17441) | 该论文研究发现，预测不确定性主要受到单个特征或特征之间乘积相互作用的二阶影响的影响。作者提出了一种基于这些二阶影响来解释预测不确定性的新方法。该方法通过简单的协方差计算对一阶解释进行处理，可以将常见的归因技术转化为强大的二阶不确定性解释器。作者通过量化评估验证了该方法解释的准确性，并展示了整体实用性。 |
| [^62] | [Difficulty Modelling in Mobile Puzzle Games: An Empirical Study on Different Methods to Combine Player Analytics and Simulated Data](https://arxiv.org/abs/2401.17436) | 本文通过对不同方法结合玩家分析和模拟数据的实证研究，揭示了在移动拼图游戏中难度的准确估算方法。结果显示，使用基于队列统计和模拟数据训练的人工神经网络模型能够在各种场景下产生最准确的估算结果。 |
| [^63] | [Can Large Language Models Replace Economic Choice Prediction Labs?](https://arxiv.org/abs/2401.17435) | 该论文研究大型语言模型是否能够取代经济实验室进行选择预测，并通过相关实验证明了其可行性。 |
| [^64] | [Integrating Generative AI in Hackathons: Opportunities, Challenges, and Educational Implications](https://arxiv.org/abs/2401.17434) | 引入生成式人工智能的黑客马拉松在软件行业中发挥重要作用，并在教育领域带来了机遇和挑战。 |
| [^65] | [Superiority of Multi-Head Attention in In-Context Linear Regression](https://arxiv.org/abs/2401.17426) | 多头注意力在上下文线性回归任务中表现出优于单头注意力的性能，通过理论分析证明了多头注意力在大嵌入维度情况下有更小的预测损失，并且在各种数据分布设置下都显示出优势。 |
| [^66] | [Application of Neural Networks for the Reconstruction of Supernova Neutrino Energy Spectra Following Fast Neutrino Flavor Conversions](https://arxiv.org/abs/2401.17424) | 本研究利用物理信息驱动的神经网络（PINNs），基于多能量中微子气体中中微子角分布的前两个矩，预测了快速 flavor 转换的结果，取得了较低的预测误差。 |
| [^67] | [Through-Wall Imaging based on WiFi Channel State Information](https://arxiv.org/abs/2401.17417) | 本研究提出了一种通过WiFi信道状态信息实现穿墙成像的创新方法，可以将室内环境可视化监测到房间边界之外，无需摄像机，具有广泛的实际应用潜力。 |
| [^68] | [Step-size Optimization for Continual Learning](https://arxiv.org/abs/2401.17401) | 本文研究了连续学习中的步长优化问题，指出传统算法忽视了对整体目标函数的影响，而随机元梯度下降算法能够明确优化步长向量，在简单问题中表现更优。 |
| [^69] | [Fine-tuning Transformer-based Encoder for Turkish Language Understanding Tasks](https://arxiv.org/abs/2401.17396) | 在本研究中，我们提供了一个基于Transformer的模型和一个土耳其语基准测试，成功地对名为BERTurk的土耳其BERT模型进行了微调，实现了许多下游任务的理解和评估。 |
| [^70] | [Customizing Language Model Responses with Contrastive In-Context Learning](https://arxiv.org/abs/2401.17390) | 本论文提出了一种使用对比示例来定制语言模型回复的方法，通过提供正面示例和负面示例，使模型学会如何回避负面特征，从而更好地满足用户需求。 |
| [^71] | [Infini-gram: Scaling Unbounded n-gram Language Models to a Trillion Tokens](https://arxiv.org/abs/2401.17377) | 这项研究展示了n-gram语言模型的价值，并介绍了一个名为infini-gram的引擎，它可以以毫秒级的延迟计算任意n的n-gram概率，使得在神经大型语言模型中对文本进行更准确的分析成为可能。 |
| [^72] | [Arabic Tweet Act: A Weighted Ensemble Pre-Trained Transformer Model for Classifying Arabic Speech Acts on Twitter](https://arxiv.org/abs/2401.17373) | 本文提出了一种用于阿拉伯语推特语言行为分类的加权集成预训练Transformer模型。通过整合不同的BERT模型，我们实现了对阿拉伯方言的精确分类，为理解用户观点和态度提供了有力的工具。 |
| [^73] | [Timeseries Suppliers Allocation Risk Optimization via Deep Black Litterman Model](https://arxiv.org/abs/2401.17350) | 通过深度黑石贝莱曼模型和时空图神经网络，我们优化了供应商选择和订单分配，同时解决了零阶情况下的可信度问题，实现了准确的预测和精确的置信区间。 |
| [^74] | [YTCommentQA: Video Question Answerability in Instructional Videos](https://arxiv.org/abs/2401.17343) | 本研究提出了YTCommentQA数据集，通过收集来自YouTube的自然问题，分类其可回答性和所需模态，以解决指导视频中的问题可回答性问题。 |
| [^75] | [A Latent Space Metric for Enhancing Prediction Confidence in Earth Observation Data](https://arxiv.org/abs/2401.17342) | 这项研究提出了一种新的方法来估计利用地球观测数据进行回归任务时机器学习模型预测的置信度。通过利用潜在空间表示来推导置信度度量，建立了潜在表示中的欧几里得距离与个体蚊子种群预测的绝对误差之间的相关性。该方法在意大利威尼托地区和德国上莱茵河谷的地区得到了验证，并表现出较高的可靠性和可信度。 |
| [^76] | [Decentralized Federated Learning: A Survey on Security and Privacy](https://arxiv.org/abs/2401.17319) | 去中心化联邦学习架构允许保护隐私，但也引入了新的安全和隐私威胁，该综述对去中心化联邦学习中的威胁、对手和防御机制进行了研究。 |
| [^77] | [HEQuant: Marrying Homomorphic Encryption and Quantization for Communication-Efficient Private Inference](https://arxiv.org/abs/2401.15970) | HEQuant提出了一种结合同态加密和量化的方法，以实现通信高效的私有推断。通过低精度量化感知优化和一系列其他优化措施，HEQuant相对于先前的HE协议有效地降低了数据传输的数量和精度。 |
| [^78] | [Baichuan2-Sum: Instruction Finetune Baichuan2-7B Model for Dialogue Summarization](https://arxiv.org/abs/2401.15496) | 本文提出了Baichuan2-Sum模型，通过指导微调Baichuan2-7B模型进行对话摘要，并应用NEFTune技术改进训练过程。实验证明该模型在CSDS和SAMSUM数据集上取得了新的最先进结果。 |
| [^79] | [Wind speed super-resolution and validation: from ERA5 to CERRA via diffusion models](https://arxiv.org/abs/2401.15469) | 本论文提出了一种利用扩散模型以数据驱动的方式近似CERRA的降尺度的方法，通过利用ERA5数据集进行风速超分辨率任务。 |
| [^80] | [Survey of Natural Language Processing for Education: Taxonomy, Systematic Review, and Future Trends](https://arxiv.org/abs/2401.07518) | 这篇论文调查了教育领域自然语言处理的最新进展，提出了分类体系，并总结了挑战和未来研究方向。 |
| [^81] | [Code Security Vulnerability Repair Using Reinforcement Learning with Large Language Models](https://arxiv.org/abs/2401.07031) | 本论文研究了使用大型语言模型的强化学习方法，以修复代码中的安全漏洞。目前的模型主要通过监督微调生成修复代码，但存在安全措施代码和功能代码之间的不平衡问题。 |
| [^82] | [Separate-and-Enhance: Compositional Finetuning for Text2Image Diffusion Models](https://arxiv.org/abs/2312.06712) | 本研究提出了一种名为"Separate-and-Enhance"的方法，通过分离损失和增强损失来解决现有文本到图像模型中的错位问题。这种方法通过减少对象掩码重叠和最大化注意力得分来提高图像生成的质量和与文本提示的对齐。评估结果显示，该方法在图像逼真度、文本-图像对齐和适应性方面表现优于其他方法。 |
| [^83] | [Efficient Large Language Models: A Survey](https://arxiv.org/abs/2312.03863) | 这篇综述论文对高效大型语言模型进行了系统和全面的调查，提供了从模型为中心、数据为中心和框架为中心的三个主要角度的分类和总结。此外，还创建了一个GitHub存储库来收集和更新相关论文。 |
| [^84] | [Injecting linguistic knowledge into BERT for Dialogue State Tracking](https://arxiv.org/abs/2311.15623) | 本文提出了一种方法，在对话状态跟踪任务中，通过无监督的知识提取方法将语言知识注入到BERT中，以提高性能和可解释性。这种方法无需额外的训练数据，通过简单的神经模块实现。该方法使用的特征提取工具与对话的句法和语义模式相关，有助于理解DST模型的决策过程。 |
| [^85] | [GeoSAM: Fine-tuning SAM with Sparse and Dense Visual Prompting for Automated Segmentation of Mobility Infrastructure](https://arxiv.org/abs/2311.11319) | GeoSAM是一个基于SAM的新框架，使用了来自零样本学习和预训练CNN分割模型的视觉提示，提高了地理图像分割的性能。 |
| [^86] | [Large Trajectory Models are Scalable Motion Predictors and Planners](https://arxiv.org/abs/2310.19620) | 大规模轨迹模型（LTMs）采用State Transformer (STR)模型，将运动预测和规划问题统一建模，有效应对自动驾驶中的挑战。 |
| [^87] | [Do self-supervised speech and language models extract similar representations as human brain?](https://arxiv.org/abs/2310.04645) | 通过评估Wav2Vec2.0和GPT-2模型的大脑预测能力，我们发现自我监督的语音和语言模型能够准确预测语音反应，其大脑预测之间存在显著相关性，且共享的语音上下文信息是解释大脑活动中变异的主要因素。 |
| [^88] | [Monolingual or Multilingual Instruction Tuning: Which Makes a Better Alpaca](https://arxiv.org/abs/2309.08958) | 通过实证分析比较了单语和多语指导调整的成本效益，发现在多语言场景下，多语指导调整可以达到或超越单独调整每种语言的效果，并且采用下采样的数据进行多语调整可以提供更强的效果和更好的鲁棒性。 |
| [^89] | [SCRAPS: Speech Contrastive Representations of Acoustic and Phonetic Spaces](https://arxiv.org/abs/2307.12445) | 本文提出了一个基于CLIP的模型，旨在学习语音和声学空间的共享表征。实验证明该模型对语音变化敏感，具有鲁棒性并适用于多种下游应用。 |
| [^90] | [On-the-fly Denoising for Data Augmentation in Natural Language Understanding](https://arxiv.org/abs/2212.10558) | 本文提出了一种即时去噪的数据增强技术，利用软增强标签和自我正则化模块，通过从更干净的原始数据学习来保证增强数据的质量。 |
| [^91] | [On the Generalizability of ECG-based Stress Detection Models](https://arxiv.org/abs/2210.06225) | 本文研究了基于心电图(ECG)的压力检测模型的泛化能力，探讨了深度学习模型和基于心电图特征的模型在不同压力场景下的应用程度。 |
| [^92] | [An Empathetic AI Coach for Self-Attachment Therapy](https://arxiv.org/abs/2209.08316) | 本文介绍了一个用于自述疗法的共情人工智能辅导系统，通过深度学习和规则进行情绪识别和生成流畅、共情的对话，达到更高的用户参与度和实用性。通过非临床试验验证了框架的有效性，并提供改进设计和性能的指导方针。 |
| [^93] | [Variational Transfer Learning using Cross-Domain Latent Modulation](https://arxiv.org/abs/2205.15523) | 本研究提出了一种变分自编码器框架中的跨领域潜在调制机制，通过从一领域获取深层表示并影响另一领域的潜在变量，实现了有效的转移学习。在多个转移学习基准任务中，我们的模型展示了竞争性能。 |
| [^94] | [Secure and Efficient Federated Learning Through Layering and Sharding Blockchain](https://arxiv.org/abs/2104.13130) | 本论文提出了一种名为ChainFL的创新联邦学习系统，通过分层和分片的方式结合区块链技术，解决了传统区块链系统处理大规模联邦学习任务时面临的挑战，并定制化了联邦学习过程与区块链的整合。 |
| [^95] | [BlockFusion: Expandable 3D Scene Generation using Latent Tri-plane Extrapolation.](http://arxiv.org/abs/2401.17053) | BlockFusion是一种使用扩散和外推技术生成三维场景的模型，能无缝地添加新的块以扩展场景。采用混合神经场和潜在三平面空间来保证高质量和多样化的生成结果。 |
| [^96] | [DiffuserLite: Towards Real-time Diffusion Planning.](http://arxiv.org/abs/2401.15443) | DiffuserLite是一个快速轻量级的扩散规划框架，通过引入计划细化过程（PRP）来提高决策频率，相比之前的框架，它只产生了很小的运行时间成本，并在D4RL基准测试中达到了最先进的性能。 |
| [^97] | [A RAG-based Question Answering System Proposal for Understanding Islam: MufassirQAS LLM.](http://arxiv.org/abs/2401.15378) | 基于RAG的MufassirQAS问答系统利用NLP技术建立联系并准确回答复杂问题，提高了LLMs的准确性和透明度，帮助理解伊斯兰教的复杂性和教义深度。 |
| [^98] | [The Case for Co-Designing Model Architectures with Hardware.](http://arxiv.org/abs/2401.14489) | 本文提供了一组指南，通过考虑模型超参数对GPU上执行的计算内核的效率的影响，来最大化变换器模型的运行时性能。相比于具有相似参数数量但形状未经优化的模型，使用高效模型形状的模型可以提高39%的吞吐量且保持准确性。 |
| [^99] | [Semantic Sensitivities and Inconsistent Predictions: Measuring the Fragility of NLI Models.](http://arxiv.org/abs/2401.14440) | 这份论文研究发现，最先进的NLI模型对微小的语义保持表面形式变化非常敏感，导致推断结果不一致。其行为与对组合语义的有效理解不同，这对当前NLI模型的可靠性提出了挑战。 |
| [^100] | [Generative Design of Crystal Structures by Point Cloud Representations and Diffusion Model.](http://arxiv.org/abs/2401.13192) | 本研究提出了一种基于点云和扩散模型的晶体结构生成设计框架，并通过重建输入结构和生成全新材料的实验证明了其有效性和潜力。 |
| [^101] | [GRATH: Gradual Self-Truthifying for Large Language Models.](http://arxiv.org/abs/2401.12292) | GRATH是一种逐步自我真实化的方法，用于提高大型语言模型的真实性。它通过使用领域外问题提示生成答案，并通过直接偏好优化进行自适应模型优化。GRATH在没有标注答案的情况下以自我监督的方式学习真实性，并通过迭代优化来逐步提升模型真实性。 |
| [^102] | [Improving Small Language Models' Mathematical Reasoning via Equation-of-Thought Distillation.](http://arxiv.org/abs/2401.11864) | 本研究提出了思维方程蒸馏（EoTD）技术和集合思维蒸馏（ETD）框架，通过构建基于方程的表示和使用多个思维过程的推理数据集来改进小型语言模型（SLMs）的数学推理能力，实验结果表明，EoTD和ETD显著提升了SLMs的推理能力。 |
| [^103] | [Gaussian Adaptive Attention is All You Need: Robust Contextual Representations Across Multiple Modalities.](http://arxiv.org/abs/2401.11143) | 该论文提出了一个名为GAAM的多头高斯自适应注意力机制，用于增强跨多个模态的信息聚合。通过将可学习的均值和方差纳入注意力机制中，GAAM能够动态地重新调整特征的重要性，从而在处理非平稳数据时取得了显著的性能提升，超过了目前现有的注意力技术。该方法的适应性强且参数数量较少，具有改进现有注意力框架的潜力。 |
| [^104] | [Combining Confidence Elicitation and Sample-based Methods for Uncertainty Quantification in Misinformation Mitigation.](http://arxiv.org/abs/2401.08694) | 本研究提出了一种结合置信度引导和基于样本的方法的不确定性量化框架，用于解决误信息消除中的幻觉和过度自信的预测问题，并提出了混合框架以提供更好的不确定性估计。 |
| [^105] | [Ada-Retrieval: An Adaptive Multi-Round Retrieval Paradigm for Sequential Recommendations.](http://arxiv.org/abs/2401.06633) | Ada-Retrieval是一种适应性多轮检索范例，用于提升推荐系统的物品候选者选择过程。它通过迭代地改进用户表示来更好地捕捉完整的物品空间中的潜在候选者，并具有模型无关的设计。 |
| [^106] | [Multimodal Learning for detecting urban functional zones using remote sensing image and multi-semantic information.](http://arxiv.org/abs/2401.06550) | 本研究提出了一种利用遥感图像和多语义信息进行城市功能区检测的多模态学习算法，能够满足移动互联网在线到离线业务的精确要求。 |
| [^107] | [Vanishing Gradients in Reinforcement Finetuning of Language Models.](http://arxiv.org/abs/2310.20703) | 本研究发现在强化微调（RFT）中存在梯度消失的问题，当模型下奖励的标准差较小时，输入的期望梯度会消失，导致奖励最大化缓慢。初始监督微调（SFT）阶段是克服这个问题的最有希望的方法。 |
| [^108] | [A Comprehensive Study of GPT-4V's Multimodal Capabilities in Medical Imaging.](http://arxiv.org/abs/2310.20381) | 本文对GPT-4V在医学影像中的多模态能力进行了全面研究和评估，发现其在生成描述性报告和医学VQA方面有潜力，但在某些评估指标上仍需改进。 |
| [^109] | [Fundamental Limits of Membership Inference Attacks on Machine Learning Models.](http://arxiv.org/abs/2310.13786) | 本文探讨了机器学习模型上成员推断攻击的基本限制，包括推导了效果和成功率的统计量，并提供了几种情况下的界限。这使得我们能够根据样本数量和其他结构参数推断潜在攻击的准确性。 |
| [^110] | [Interactive Navigation in Environments with Traversable Obstacles Using Large Language and Vision-Language Models.](http://arxiv.org/abs/2310.08873) | 本文提出了一个使用大型语言和视觉-语言模型的交互式导航框架，使机器人能够在带有可通行障碍的环境中进行导航。通过使用这些模型，我们可以实现从文本指令到动作感知边界框的端到端系统，无需微调和额外的训练数据。同时，我们还使用大型模型划分激光雷达点云，生成动作感知成本地图以生成可行路径。 |
| [^111] | [MAPLE: Mobile App Prediction Leveraging Large Language model Embeddings.](http://arxiv.org/abs/2309.08648) | MAPLE是一个利用大型语言模型嵌入进行移动应用预测的模型，通过严格测试验证了其在解密复杂模式和理解用户环境方面的能力，并强调了语言模型在不同领域中的广泛适用性。 |
| [^112] | [Combining deep learning and street view imagery to map smallholder crop types.](http://arxiv.org/abs/2309.05930) | 本研究利用深度学习和街景图像开发了一个自动化系统，可以生成农作物类型地面参考，解决了在低收入和中等收入国家创建农作物类型地图时的挑战。 |
| [^113] | [Dual Relation Alignment for Composed Image Retrieval.](http://arxiv.org/abs/2309.02169) | 本研究提出了双重关系对齐的方法，用于组合图像检索任务。通过利用显性和隐性关系，可以更好地学习网络并提升检索性能。 |
| [^114] | [A Generic Machine Learning Framework for Fully-Unsupervised Anomaly Detection with Contaminated Data.](http://arxiv.org/abs/2308.13352) | 这篇论文介绍了一个完全无监督的机器学习框架，用于处理在训练数据中含有异常样本的异常检测任务。 |
| [^115] | [Traffic Flow Optimisation for Lifelong Multi-Agent Path Finding.](http://arxiv.org/abs/2308.11234) | 本文提出了一种新的终身多智能体路径规划方法，通过引导智能体避开拥堵路径来优化交通流量，显著提高解决方案质量和总体吞吐量。 |
| [^116] | [ConcatPlexer: Additional Dim1 Batching for Faster ViTs.](http://arxiv.org/abs/2308.11199) | 本文提出了一种名为ConcatPlexer的方法，通过在视觉识别中使用附加的Dim1批处理（即连接）来提高吞吐量，同时准确性受到的影响较小。 |
| [^117] | [Semantics-guided Transformer-based Sensor Fusion for Improved Waypoint Prediction.](http://arxiv.org/abs/2308.02126) | 该论文提出一种基于语义引导的传感器融合方法，通过融合多个传感器的特征和使用辅助任务来改进自动驾驶代理的航点预测。 |
| [^118] | [RCT Rejection Sampling for Causal Estimation Evaluation.](http://arxiv.org/abs/2307.15176) | 该论文提出了一种名为RCT拒绝抽样的新抽样算法，用于因果估计评估。该方法通过子抽样随机控制试验(RCT)创建混淆的观测数据集，并使用RCT的平均因果效应作为基准真实值，以进行有效比较。 |
| [^119] | [GEAR: Augmenting Language Models with Generalizable and Efficient Tool Resolution.](http://arxiv.org/abs/2307.08775) | GEAR是一种通用且高效的工具解决方案，通过将工具对应和执行分别委托给小型语言模型和大型语言模型，在不依赖任务示范的情况下实现了更高的性能和精确度。 |
| [^120] | [What Is Fairness? Philosophical Considerations and Implications For FairML.](http://arxiv.org/abs/2205.09622) | 本文探讨了公平性的哲学概念，提出了公平性和预测性能不是不可调和的对立面，并强调从数据收集到最终模型评估都需纳入伦理考虑。 |

# 详细

[^1]: 语言模型在解决问题时是否表现出与人类学习者相同的认知偏见？

    Do Language Models Exhibit the Same Cognitive Biases in Problem Solving as Human Learners?

    [https://arxiv.org/abs/2401.18070](https://arxiv.org/abs/2401.18070)

    该研究调查了语言模型在解决算术问题时与人类学习者的认知偏见。研究发现，当前最先进的语言模型在文本理解和解决方案规划阶段表现出与人类类似的偏见。

    

    越来越多的人对使用大型语言模型（LLMs）作为认知模型感兴趣。为了达到这个目的，了解LLMs能够模拟哪些认知特性以及哪些不能模拟是至关重要的。在这项研究中，我们研究了LLMs在解决算术问题时与儿童已知认知偏见的相关性。通过调查学习科学文献，我们提出问题解决过程可以分为三个明确的步骤：文本理解、解决方案规划和解决方案执行。我们为每个步骤构建了测试，以了解当前最先进的LLMs可以如何忠实地模拟这个过程的哪些部分。我们使用一种神经符号方法为每个测试生成了一组新的单词问题，该方法可以对问题特征进行精细控制。我们发现，LLMs在文本理解和解决方案规划两个解决过程的步骤中，不论是否经过指导调整，都表现出与人类类似的偏见。

    There is increasing interest in employing large language models (LLMs) as cognitive models. For such purposes, it is central to understand which cognitive properties are well-modeled by LLMs, and which are not. In this work, we study the biases of LLMs in relation to those known in children when solving arithmetic word problems. Surveying the learning science literature, we posit that the problem-solving process can be split into three distinct steps: text comprehension, solution planning and solution execution. We construct tests for each one in order to understand which parts of this process can be faithfully modeled by current state-of-the-art LLMs. We generate a novel set of word problems for each of these tests, using a neuro-symbolic method that enables fine-grained control over the problem features. We find evidence that LLMs, with and without instruction-tuning, exhibit human-like biases in both the text-comprehension and the solution-planning steps of the solving process, but 
    
[^2]: SpeechComposer: 使用提示合成统一多个语音任务

    SpeechComposer: Unifying Multiple Speech Tasks with Prompt Composition

    [https://arxiv.org/abs/2401.18045](https://arxiv.org/abs/2401.18045)

    SpeechComposer是一种能够通过合成固定提示标记来统一多个语音任务的语音语言模型。它提高了任务之间的连接性，并且可以轻松扩展到更多的任务，同时实现了任务之间的知识共享。

    

    最近语言模型的进展大大提升了多种语音相关任务的性能。现有的语音语言模型通常利用任务相关的提示标记将各种语音任务统一在一个模型中。然而，这种设计忽略了不同语音任务之间的内在联系，这可能会提高每个任务的性能。在这项工作中，我们提出了一种新颖的只解码器的语音语言模型SpeechComposer，它可以通过组合一组固定的提示标记统一常见的语音任务。SpeechComposer建立在四个主要任务的基础上--语音合成、语音识别、语音语言建模和文本语言建模--通过设计良好的提示标记的组合，如声音转换和语音增强，可以轻松扩展到更多的语音任务。提示标记的统一也使得不同语音任务之间的知识共享更加结构化。实验结果表明我们的模型提供了更好的性能和更大的灵活性。

    Recent advancements in language models have significantly enhanced performance in multiple speech-related tasks. Existing speech language models typically utilize task-dependent prompt tokens to unify various speech tasks in a single model. However, this design omits the intrinsic connections between different speech tasks, which can potentially boost the performance of each task. In this work, we propose a novel decoder-only speech language model, SpeechComposer, that can unify common speech tasks by composing a fixed set of prompt tokens. Built upon four primary tasks -- speech synthesis, speech recognition, speech language modeling, and text language modeling -- SpeechComposer can easily extend to more speech tasks via compositions of well-designed prompt tokens, like voice conversion and speech enhancement. The unification of prompt tokens also makes it possible for knowledge sharing among different speech tasks in a more structured manner. Experimental results demonstrate that our
    
[^3]: 加强端到端多任务对话系统：基于内在动机强化学习算法的改进训练和适应性研究

    Enhancing End-to-End Multi-Task Dialogue Systems: A Study on Intrinsic Motivation Reinforcement Learning Algorithms for Improved Training and Adaptability

    [https://arxiv.org/abs/2401.18040](https://arxiv.org/abs/2401.18040)

    本研究旨在通过内在动机强化学习算法改进端到端多任务对话系统的训练和适应性。通过教授智能体一个内在奖励系统，可以加速训练并提高其判断行为质量的能力。

    

    端到端多任务对话系统通常通过对话流水线的独立模块进行设计。其中，策略模块是决定对用户输入如何响应的关键。这个策略是通过强化学习算法进行训练的，通过利用一个智能体在一个反馈信号形式的环境中接收反馈。然而，当前的对话系统只提供了稀缺且简单的奖励。本研究的目标是研究内在动机强化学习算法。通过这种算法，智能体可以快速加速训练，并通过教授一个内在奖励系统来提高判断其行为质量的能力。具体而言，我们将随机网络蒸馏和好奇驱动强化学习技术应用于测量状态访问频率，并通过使用话语之间的语义相似性来鼓励探索。在一个异构数据集MultiWOZ上进行的实验结果显示...

    End-to-end multi-task dialogue systems are usually designed with separate modules for the dialogue pipeline. Among these, the policy module is essential for deciding what to do in response to user input. This policy is trained by reinforcement learning algorithms by taking advantage of an environment in which an agent receives feedback in the form of a reward signal. The current dialogue systems, however, only provide meagre and simplistic rewards. Investigating intrinsic motivation reinforcement learning algorithms is the goal of this study. Through this, the agent can quickly accelerate training and improve its capacity to judge the quality of its actions by teaching it an internal incentive system. In particular, we adapt techniques for random network distillation and curiosity-driven reinforcement learning to measure the frequency of state visits and encourage exploration by using semantic similarity between utterances. Experimental results on MultiWOZ, a heterogeneous dataset, sho
    
[^4]: Paramanu: 一种高效的印度生成式基础语言模型系列

    Paramanu: A Family of Novel Efficient Indic Generative Foundation Language Models

    [https://arxiv.org/abs/2401.18034](https://arxiv.org/abs/2401.18034)

    Paramanu是一种高效的印度生成式基础语言模型系列，包含多种印度语言模型，并且在单个GPU上进行了从头预训练。它还包括一个先进的印度分词器以及避免多语言诅咒的预训练方法。这些模型在人工评估中展现出良好的语法、连贯性、创造性和事实准确性。

    

    我们介绍了Gyan AI Paramanu（“原子”），一种适用于印度语言的新型语言模型系列。它是一个在单个GPU上从头开始预训练的包含单语、双语和多语印度语言模型的集合，涵盖了10种印度语言（阿萨姆语、孟加拉语、印地语、康坎尼语、迈蒂利语、马拉地语、奥迪亚语、梵语、泰米尔语和泰卢固语）以及5种不同大小的字母表（孟加拉语、天城体、奥迪亚语、泰米尔语和泰卢固语）。这些模型以1024的上下文大小在单个GPU上预训练，非常高效、小巧、快速且强大。我们还开发了一种高效的先进的印度语分词器，甚至可以标记未知语言。为了避免我们的多语言mParamanu模型中的“多语言诅咒”，我们使用相同的字母表按语言类型进行了可比较语料库的预训练。我们对我们预训练模型进行了人工评估，评估指标包括语法、连贯性、创造性和事实准确性。

    We present Gyan AI Paramanu ("atom"), a family of novel language models for Indian languages. It is a collection of auto-regressive monolingual, bilingual, and multilingual Indic language models pretrained from scratch on a single GPU for 10 Indian languages (Assamese, Bangla, Hindi, Konkani, Maithili, Marathi, Odia, Sanskrit, Tamil, Telugu) across 5 scripts (Bangla, Devanagari, Odia, Tamil, Telugu) of varying sizes ranging from 13.29M to 367.5M.The models are pretrained with a context size of 1024 on a single GPU. The models are very efficient, small, fast, and powerful. We have also developed an efficient most advanced Indic tokenizer that can even tokenize unseen languages. In order to avoid the "curse of multi-linguality" in our multilingual mParamanu model, we pretrained on comparable corpora by typological grouping using the same script. We performed human evaluation of our pretrained models for open end text generation on grammar, coherence, creativity, and factuality metrics fo
    
[^5]: 使用LLMs支持预期治理: 通过与新闻媒体对齐，评估大型语言模型以预测人工智能的负面影响

    Supporting Anticipatory Governance using LLMs: Evaluating and Aligning Large Language Models with the News Media to Anticipate the Negative Impacts of AI

    [https://arxiv.org/abs/2401.18028](https://arxiv.org/abs/2401.18028)

    本研究探讨了使用LLMs来增强预测人工智能负面影响的方法，并通过与新闻媒体对齐，建立了一个包含十个类别的人工智能影响分类法。结果表明，基于指令和迁移学习的LLMs模型在生成影响方面具有一定的能力。

    

    在人工智能技术发展的早期阶段，预测其可能带来的负面影响是一个挑战。使用LLMs增强和指导这一过程是一种不太被研究的预测方法。尽管LLMs和评估指标在生成文本中考虑偏差方面有所进展，但目前尚不清楚这些模型在预测任务中表现如何。具体而言，使用LLMs预测人工智能影响引发了关于模型能够生成的负面影响类别的质量和范围的问题。在本文中，我们利用丰富的包含对新兴技术的规范性评估的数据来源——新闻媒体，制定了一个基准，用于比较不同类别的影响。通过计算分析全球数百个在线新闻媒体发布的数千篇新闻文章，我们建立了一个包含十个类别的人工智能影响分类法。然后，我们评估了基于指令的LLMs模型（GPT-4等）和基于迁移学习的模型在根据我们的分类法生成的影响方面的表现。

    Anticipating the negative impacts of emerging AI technologies is a challenge, especially in the early stages of development. An understudied approach to such anticipation is the use of LLMs to enhance and guide this process. Despite advancements in LLMs and evaluation metrics to account for biases in generated text, it is unclear how well these models perform in anticipatory tasks. Specifically, the use of LLMs to anticipate AI impacts raises questions about the quality and range of categories of negative impacts these models are capable of generating. In this paper we leverage news media, a diverse data source that is rich with normative assessments of emerging technologies, to formulate a taxonomy of impacts to act as a baseline for comparing against. By computationally analyzing thousands of news articles published by hundreds of online news domains around the world, we develop a taxonomy consisting of ten categories of AI impacts. We then evaluate both instruction-based (GPT-4 and 
    
[^6]: 通过定向表示优化实现的安全提示驱动的大型语言模型(LLM)保护

    Prompt-Driven LLM Safeguarding via Directed Representation Optimization

    [https://arxiv.org/abs/2401.18018](https://arxiv.org/abs/2401.18018)

    通过研究模型表示的影响，我们发现安全提示并没有明显增强恶意和无害查询之间的区分，并提出了一种名为DRO的方法，用于自动优化安全提示。

    

    在大型语言模型(LLM)中，使用安全提示在模型输入之前是一种常见的保护实践，以使其不遵从包含恶意意图的查询。然而，安全提示的工作机制尚未完全理解，这妨碍了自动优化其以改善LLM安全性的潜力。针对这个问题，我们从模型表示的角度调查了安全提示的影响。我们发现在模型的表示空间中，有害和无害的查询可以在很大程度上区分开来，但安全提示并没有明显增强这一区分。相反，不同安全提示导致查询的表示朝着相似的方向移动，使得模型即使在查询无害时也更容易拒绝提供协助。受到这些发现的启发，我们提出了一种名为DRO（定向表示优化）的方法，用于自动安全提示优化。DRO将安全提示视为要优化的表示方向。

    Prepending model inputs with safety prompts is a common practice of safeguarding large language models (LLMs) from complying with queries that contain harmful intents. However, the working mechanisms of safety prompts have not yet been fully understood, which hinders the potential for automatically optimizing them for improved LLM safety. Motivated by this problem, we investigate the impact of safety prompts from the perspective of model representations. We find that in models' representation space, harmful and harmless queries can be largely distinguished, but this is not noticeably enhanced by safety prompts. Instead, the queries' representations are moved by different safety prompts in similar directions, where models become more prone to refusal (i.e., refusing to provide assistance) even when the queries are harmless. Inspired by these findings, we propose a method called DRO (Directed Representation Optimization) for automatic safety prompt optimization. DRO treats safety prompts
    
[^7]: 千面灌木：利用深度学习从卫星图像中进行个体分割

    Shrub of a thousand faces: an individual segmentation from satellite images using deep learning

    [https://arxiv.org/abs/2401.17985](https://arxiv.org/abs/2401.17985)

    该研究提出了一种新方法，利用深度学习模型从卫星图像中个别描绘位于西班牙内华达山脉林线上的杜松灌木。研究通过整合遥感影像和实例分割模型，解决了检测和描绘自然对象复杂生长模式的挑战。

    

    监测长寿灌木（如常见的杜松）的分布和大小结构可以用于估计气候变化对高山和高纬度生态系统的长期影响。历史航空超高分辨率影像提供了一种回顾性工具，可以高精度监测灌木的生长和分布。目前，深度学习模型在检测和描绘具有明确定义形状的对象的轮廓方面取得了令人印象深刻的结果。然而，将这些模型适应于检测表达复杂生长模式的自然对象（例如杜松）仍然是一项具有挑战性的任务。本研究提出了一种新方法，利用遥感RGB影像与基于Mask R-CNN的实例分割模型相结合，个别描绘西班牙内华达山脉林线上的杜松灌木。在这项研究中，我们提出了一种新的数据构造设计，其中使用的是光解译数据（PI）和实地调查数据（FW）分别进行操作。

    Monitoring the distribution and size structure of long-living shrubs, such as Juniperus communis, can be used to estimate the long-term effects of climate change on high-mountain and high latitude ecosystems. Historical aerial very-high resolution imagery offers a retrospective tool to monitor shrub growth and distribution at high precision. Currently, deep learning models provide impressive results for detecting and delineating the contour of objects with defined shapes. However, adapting these models to detect natural objects that express complex growth patterns, such as junipers, is still a challenging task.   This research presents a novel approach that leverages remotely sensed RGB imagery in conjunction with Mask R-CNN-based instance segmentation models to individually delineate Juniperus shrubs above the treeline in Sierra Nevada (Spain). In this study, we propose a new data construction design that consists in using photo interpreted (PI) and field work (FW) data to respectivel
    
[^8]: 通过视觉检测模型增强多模态大型语言模型：一项实证研究

    Enhancing Multimodal Large Language Models with Vision Detection Models: An Empirical Study

    [https://arxiv.org/abs/2401.17981](https://arxiv.org/abs/2401.17981)

    本文通过实证研究，将最先进的目标检测和光学字符识别模型与多模态大型语言模型相结合，在提高图像理解和减少回答错误插入方面取得了显著改进。

    

    尽管多模态大型语言模型（MLLMs）在集成文本和图像模态方面具有令人印象深刻的能力，但在准确解释细节视觉元素方面仍存在挑战。本文通过将最先进的目标检测和光学字符识别模型与MLLMs结合，进行实证研究，旨在提高对细粒度图像理解，并减少回答中的错误插入。我们的研究探讨了基于嵌入的检测信息的融合，这种融合对MLLMs的原始能力的影响，以及检测模型的可互换性。我们对LLaVA-1.5、DINO和PaddleOCRv2等模型进行了系统实验，发现我们的方法不仅改善了MLLMs在特定视觉任务中的性能，而且保持了它们的原始优势。通过在10个基准测试中，增强的MLLMs在9个测试中超越了最先进模型，标准化平均得分提升高达12.99%，取得了显著的改进。

    Despite the impressive capabilities of Multimodal Large Language Models (MLLMs) in integrating text and image modalities, challenges remain in accurately interpreting detailed visual elements. This paper presents an empirical study on enhancing MLLMs with state-of-the-art (SOTA) object detection and Optical Character Recognition models to improve fine-grained image understanding and reduce hallucination in responses. Our research investigates the embedding-based infusion of detection information, the impact of such infusion on the MLLMs' original abilities, and the interchangeability of detection models. We conduct systematic experiments with models such as LLaVA-1.5, DINO, and PaddleOCRv2, revealing that our approach not only refines MLLMs' performance in specific visual tasks but also maintains their original strengths. The resulting enhanced MLLMs outperform SOTA models on 9 out of 10 benchmarks, achieving an improvement of up to 12.99% on the normalized average score, marking a not
    
[^9]: 使用深度强化学习对多核量子架构进行电路分区

    Circuit Partitioning for Multi-Core Quantum Architectures with Deep Reinforcement Learning

    [https://arxiv.org/abs/2401.17976](https://arxiv.org/abs/2401.17976)

    本文使用深度强化学习技术，提出了一种适应多核量子架构的电路分区方法，旨在推动量子计算和图分区领域的发展。

    

    量子计算利用量子力学的独特性质，具有解决经典难题的巨大潜力。量子架构的可扩展性仍然是一个重要挑战。多核量子架构被提出来解决可扩展性问题，引发了硬件、通信和编译等一系列新的挑战。其中之一是将量子算法适应到量子计算机的不同核心中。本文提出了一种使用深度强化学习进行电路分区的新方法，为量子计算和图分区的进展做出了贡献。这项工作是将深度强化学习技术整合到量子电路映射中的第一步，为解决此类问题开辟了一种新的范式。

    Quantum computing holds immense potential for solving classically intractable problems by leveraging the unique properties of quantum mechanics. The scalability of quantum architectures remains a significant challenge. Multi-core quantum architectures are proposed to solve the scalability problem, arising a new set of challenges in hardware, communications and compilation, among others. One of these challenges is to adapt a quantum algorithm to fit within the different cores of the quantum computer. This paper presents a novel approach for circuit partitioning using Deep Reinforcement Learning, contributing to the advancement of both quantum computing and graph partitioning. This work is the first step in integrating Deep Reinforcement Learning techniques into Quantum Circuit Mapping, opening the door to a new paradigm of solutions to such problems.
    
[^10]: 通过编码理论了解神经网络中的多语义性

    Understanding polysemanticity in neural networks through coding theory

    [https://arxiv.org/abs/2401.17975](https://arxiv.org/abs/2401.17975)

    通过应用编码理论和神经科学工具，本文提出了一种新颖的实践方法来理解神经网络中的多语义性，并对多语义神经元对学习性能的优势进行了解释。

    

    尽管付出了大量努力，神经网络可解释性仍然是一个难以捉摸的目标，以前的研究未能对大多数单个神经元对网络输出的影响提供简洁的解释。这个限制是由于大多数神经元的多语义性，即一个给定的神经元参与多个不相关的网络状态，使解释该神经元变得复杂。在本文中，我们应用神经科学和信息论中开发的工具，提出了一种新颖的网络可解释性实践方法，并对多语义性和编码密度提出了理论见解。我们通过检查激活的协方差矩阵的特征谱来推断网络代码的冗余水平。此外，我们展示了随机投影如何揭示网络是否具有平滑的或不可微的代码，从而解释了代码的可解释性。这个相同的框架解释了多语义神经元对学习性能的优势并解释了

    Despite substantial efforts, neural network interpretability remains an elusive goal, with previous research failing to provide succinct explanations of most single neurons' impact on the network output. This limitation is due to the polysemantic nature of most neurons, whereby a given neuron is involved in multiple unrelated network states, complicating the interpretation of that neuron. In this paper, we apply tools developed in neuroscience and information theory to propose both a novel practical approach to network interpretability and theoretical insights into polysemanticity and the density of codes. We infer levels of redundancy in the network's code by inspecting the eigenspectrum of the activation's covariance matrix. Furthermore, we show how random projections can reveal whether a network exhibits a smooth or non-differentiable code and hence how interpretable the code is. This same framework explains the advantages of polysemantic neurons to learning performance and explains
    
[^11]: MelNet:一种用于实时目标检测的深度学习算法

    MelNet: A Real-Time Deep Learning Algorithm for Object Detection

    [https://arxiv.org/abs/2401.17972](https://arxiv.org/abs/2401.17972)

    MelNet是一种用于实时目标检测的深度学习算法，经过训练后，在KITTI数据集上表现出良好的性能，同时也展示了迁移学习和定制模型在目标检测中的有效性。

    

    本研究介绍了一种名为MelNet的新型深度学习算法，用于目标检测。MelNet利用KITTI数据集进行了目标检测的训练。经过300个训练轮次，MelNet达到了0.732的mAP（平均精度）。此外，还对三个备选模型（YOLOv5、EfficientDet和Faster-RCNN-MobileNetv3）在KITTI数据集上进行了训练，并与MelNet进行了对比。结果表明，在某些情况下，采用迁移学习是有效的。值得注意的是，预先在知名数据集（如ImageNet、COCO和Pascal VOC）上训练的现有模型产生了更好的结果。另一个发现强调了根据特定情景创建新模型并在特定数据集上进行训练的可行性。这项研究表明，仅在KITTI数据集上训练的MelNet在150个轮次后也超过了EfficientDet。因此，训练后，MelNet的性能与EfficientDet接近。

    In this study, a novel deep learning algorithm for object detection, named MelNet, was introduced. MelNet underwent training utilizing the KITTI dataset for object detection. Following 300 training epochs, MelNet attained an mAP (mean average precision) score of 0.732. Additionally, three alternative models -YOLOv5, EfficientDet, and Faster-RCNN-MobileNetv3- were trained on the KITTI dataset and juxtaposed with MelNet for object detection.   The outcomes underscore the efficacy of employing transfer learning in certain instances. Notably, preexisting models trained on prominent datasets (e.g., ImageNet, COCO, and Pascal VOC) yield superior results. Another finding underscores the viability of creating a new model tailored to a specific scenario and training it on a specific dataset. This investigation demonstrates that training MelNet exclusively on the KITTI dataset also surpasses EfficientDet after 150 epochs. Consequently, post-training, MelNet's performance closely aligns with that
    
[^12]: 基于深度强化学习的多机器人社交导航中的关注图

    Attention Graph for Multi-Robot Social Navigation with Deep Reinforcement Learning

    [https://arxiv.org/abs/2401.17914](https://arxiv.org/abs/2401.17914)

    本文提出了一种基于深度强化学习的方法，用于学习多机器人社交导航策略。方法通过使用图神经网络和注意机制来建模机器人和行人之间的交互，创新地考虑了多机器人场景。

    

    学习机器人在行人群体中的导航策略对于基于领域的应用至关重要。结合感知、规划和预测可以模拟机器人和行人之间的互动，尤其是在基于深度强化学习的最近方法中取得了令人印象深刻的成果。然而，这些工作没有考虑多机器人场景。在本文中，我们提出了一种新的方法，使用强化学习来学习多智能体社交感知导航策略。受到近期多智能体深度强化学习研究的启发，我们的方法利用了基于图的表示来描述智能体之间的相互作用，结合了实体的位置和视野。每个智能体使用两个图神经网络模型结合注意机制。首先，边选择器生成稀疏图，然后众包协调器应用节点注意力来生成表示每个实体对其他实体影响的图。这被加入到一个模型中。

    Learning robot navigation strategies among pedestrian is crucial for domain based applications. Combining perception, planning and prediction allows us to model the interactions between robots and pedestrians, resulting in impressive outcomes especially with recent approaches based on deep reinforcement learning (RL). However, these works do not consider multi-robot scenarios. In this paper, we present MultiSoc, a new method for learning multi-agent socially aware navigation strategies using RL. Inspired by recent works on multi-agent deep RL, our method leverages graph-based representation of agent interactions, combining the positions and fields of view of entities (pedestrians and agents). Each agent uses a model based on two Graph Neural Network combined with attention mechanisms. First an edge-selector produces a sparse graph, then a crowd coordinator applies node attention to produce a graph representing the influence of each entity on the others. This is incorporated into a mode
    
[^13]: ReplaceAnything3D：具有组合神经辐射场的文本引导的3D场景编辑

    ReplaceAnything3D:Text-Guided 3D Scene Editing with Compositional Neural Radiance Fields

    [https://arxiv.org/abs/2401.17895](https://arxiv.org/abs/2401.17895)

    ReplaceAnything3D是一种文本引导的3D场景编辑方法，通过使用Erase-and-Replace方法，能够在保持3D一致性的情况下，替换场景中的特定对象，展示了在各种逼真的3D场景中修改前景对象的多功能性。

    

    我们引入了ReplaceAnything3D模型（RAM3D），一种新颖的文本引导的3D场景编辑方法，可以替换场景中的特定对象。给定一个场景的多视角图像、描述要替换对象的文本提示和描述新对象的文本提示，我们的Erase-and-Replace方法可以有效地用新生成的内容交换场景中的对象，同时保持多视角的3D一致性。我们展示了ReplaceAnything3D的多功能性，将其应用于各种逼真的3D场景，并展示了修改后的前景对象与场景的其他部分融合得很好，同时不影响其整体完整性的结果。

    We introduce ReplaceAnything3D model (RAM3D), a novel text-guided 3D scene editing method that enables the replacement of specific objects within a scene. Given multi-view images of a scene, a text prompt describing the object to replace, and a text prompt describing the new object, our Erase-and-Replace approach can effectively swap objects in the scene with newly generated content while maintaining 3D consistency across multiple viewpoints. We demonstrate the versatility of ReplaceAnything3D by applying it to various realistic 3D scenes, showcasing results of modified foreground objects that are well-integrated with the rest of the scene without affecting its overall integrity.
    
[^14]: 使用远程连接通知的Transformer进行高效的次季节天气预报

    Efficient Subseasonal Weather Forecast using Teleconnection-informed Transformers

    [https://arxiv.org/abs/2401.17870](https://arxiv.org/abs/2401.17870)

    提出了一种使用远程连接通知的Transformer模型来实现高效的次季节天气预报，该方法通过利用预训练模型和集成远程连接通知的时间模块来改进预测能力。

    

    次季节预报对农业、水资源管理和灾害预警至关重要，但由于大气的混沌性，面临着挑战。最近机器学习领域的进展通过实现与数值模型相当的预测能力，革新了天气预报。然而，训练这些基础模型需要数千个GPU的计算时间，这导致相当多的碳排放，并限制了其更广泛的应用。此外，机器学习模型往往通过产生平滑的结果来愚弄像素误差评分，这些结果缺乏物理一致性和气象意义。为了解决上述问题，我们提出了一种远程连接通知的Transformer。我们的架构利用预训练的Pangu模型来获得良好的初始权重，并集成了一个远程连接通知的时间模块，以提高在延长的时间范围内的可预测性。值得注意的是，通过调整Pangu模型的1.1%参数，我们的方法改进了预测能力。

    Subseasonal forecasting, which is pivotal for agriculture, water resource management, and early warning of disasters, faces challenges due to the chaotic nature of the atmosphere. Recent advances in machine learning (ML) have revolutionized weather forecasting by achieving competitive predictive skills to numerical models. However, training such foundation models requires thousands of GPU days, which causes substantial carbon emissions and limits their broader applicability. Moreover, ML models tend to fool the pixel-wise error scores by producing smoothed results which lack physical consistency and meteorological meaning. To deal with the aforementioned problems, we propose a teleconnection-informed transformer. Our architecture leverages the pretrained Pangu model to achieve good initial weights and integrates a teleconnection-informed temporal module to improve predictability in an extended temporal range. Remarkably, by adjusting 1.1% of the Pangu model's parameters, our method enh
    
[^15]: 理解知识密集型过程：以油气工业为例

    Making Sense of Knowledge Intensive Processes: an Oil & Gas Industry Scenario

    [https://arxiv.org/abs/2401.17866](https://arxiv.org/abs/2401.17866)

    本研究在油气工业的知识密集型过程中找到了不同类型的知识可以组合来形成协作决策过程的理解结果。我们还讨论了利用人工智能驱动的系统来实现这些想法的可能性。

    

    理解是一种不断进行的过程，人们通过经验给事物赋予意义。它可以是个别的过程，被称为“猜测”，也可以是一种群体过程，人们通过它给集体经验赋予意义。群体的理解受到每个人对经验的猜测过程的影响。每个协作过程都需要一定程度的理解才能取得结果。对于知识密集型过程，理解是中心和与大多数任务相关的。我们通过在油气工业的知识密集型过程中进行的实地调查提出了研究结果。我们的研究结果表明，可以组合不同类型的知识来组成理解过程的结果（如决策、需要更多讨论等）。本文提出了一组初始的知识类型，可以组合起来形成协作决策过程的理解结果。同时，我们还讨论了利用人工智能驱动的系统来实现这些想法的可能性。

    Sensemaking is a constant and ongoing process by which people associate meaning to experiences. It can be an individual process, known as abduction, or a group process by which people give meaning to collective experiences. The sensemaking of a group is influenced by the abduction process of each person about the experience. Every collaborative process needs some level of sensemaking to show results. For a knowledge intensive process, sensemaking is central and related to most of its tasks. We present findings from a fieldwork executed in knowledge intensive process from the Oil and Gas industry. Our findings indicated that different types of knowledge can be combined to compose the result of a sensemaking process (e.g. decision, the need for more discussion, etc.). This paper presents an initial set of knowledge types that can be combined to compose the result of the sensemaking of a collaborative decision making process. We also discuss ideas for using systems powered by Artificial I
    
[^16]: 在机器教学中操纵离散输入的预测

    Manipulating Predictions over Discrete Inputs in Machine Teaching

    [https://arxiv.org/abs/2401.17865](https://arxiv.org/abs/2401.17865)

    本文研究离散域中的机器教学，在操纵学生模型的预测方面具有显著的数值优势，可用于矫正错误的预测或恶意操纵模型实现个人利益。

    

    机器教学通常涉及创建一个最优（通常是最小的）数据集，以帮助模型（被称为“学生”）根据教师给出的特定目标实现特定目标。尽管在连续域中广泛存在，但在离散域中对机器教学的有效性的研究相对较少。本文主要研究离散域中的机器教学，具体地说，是通过有效地改变训练数据来操纵学生模型的预测，以实现教师的目标。我们将这个任务形式化为一个组合优化问题，并通过提出一个迭代搜索算法来解决它。我们的算法在教师试图纠正错误的预测以改善学生模型，或恶意操纵模型以错误分类某些特定样本到与其个人利益一致的目标类别的场景中具有显著的数值优势。实验结果表明，我们提出的算法具有超级

    Machine teaching often involves the creation of an optimal (typically minimal) dataset to help a model (referred to as the `student') achieve specific goals given by a teacher. While abundant in the continuous domain, the studies on the effectiveness of machine teaching in the discrete domain are relatively limited. This paper focuses on machine teaching in the discrete domain, specifically on manipulating student models' predictions based on the goals of teachers via changing the training data efficiently. We formulate this task as a combinatorial optimization problem and solve it by proposing an iterative searching algorithm. Our algorithm demonstrates significant numerical merit in the scenarios where a teacher attempts at correcting erroneous predictions to improve the student's models, or maliciously manipulating the model to misclassify some specific samples to the target class aligned with his personal profits. Experimental results show that our proposed algorithm can have super
    
[^17]: 迭代优化启发式方法可解释性基准测试

    Explainable Benchmarking for Iterative Optimization Heuristics

    [https://arxiv.org/abs/2401.17842](https://arxiv.org/abs/2401.17842)

    本文介绍了一种称为可解释基准测试的新方法，并提出了IOH-Xplainer软件框架，用于分析和理解优化算法的性能和影响。通过该框架，研究人员可以评估和解释迭代优化启发式方法在不同场景下的行为和效率。

    

    启发式算法的基准测试对于理解在什么条件下以及在何种问题上某些算法表现良好至关重要。目前大部分对启发式优化算法的研究只探索了非常有限的场景、算法配置和超参数设置，导致了不完整且常常有偏见的见解和结果。本文提出了一种新的方法，称之为可解释基准测试。介绍了IOH-Xplainer软件框架，用于分析和理解各种优化算法的性能以及它们不同组件和超参数的影响。我们在两个模块化优化框架的背景下展示了该框架。通过该框架，我们研究不同算法组件和配置的影响，提供了它们在不同场景下的性能见解。我们提供了一种系统的方法来评估和解释迭代优化启发式方法的行为和效率。

    Benchmarking heuristic algorithms is vital to understand under which conditions and on what kind of problems certain algorithms perform well. In most current research into heuristic optimization algorithms, only a very limited number of scenarios, algorithm configurations and hyper-parameter settings are explored, leading to incomplete and often biased insights and results. This paper presents a novel approach we call explainable benchmarking. Introducing the IOH-Xplainer software framework, for analyzing and understanding the performance of various optimization algorithms and the impact of their different components and hyper-parameters. We showcase the framework in the context of two modular optimization frameworks. Through this framework, we examine the impact of different algorithmic components and configurations, offering insights into their performance across diverse scenarios. We provide a systematic method for evaluating and interpreting the behaviour and efficiency of iterativ
    
[^18]: 全球说谎者：LLMs在时间和地理区域上的事实性

    Global-Liar: Factuality of LLMs over Time and Geographic Regions

    [https://arxiv.org/abs/2401.17839](https://arxiv.org/abs/2401.17839)

    本论文评估了GPT模型的事实准确性、稳定性和偏见，并引入了一个平衡数据集"全球说谎者"，结果显示较新的GPT模型并不总是意味着性能的提升，并且观察到一个全球南方陈述被偏袒的问题。

    

    越来越多地依赖于人工智能驱动的解决方案，特别是像GPT系列这样的大型语言模型（LLMs）在信息检索中的使用，突显了对它们的事实准确性和公正性的重要性，尤其是在网络上虚假信息和误导信息猖獗传播的背景下。我们的研究评估了广泛采用的GPT模型（包括GPT-3.5和GPT-4）的事实准确性、稳定性和偏见，以提高人工智能介导信息传播的可靠性和完整性。我们引入了一个独特平衡的数据集“全球说谎者”，其在地理和时间表征方面有助于更细致地评估LLM的偏见。我们的分析结果表明，较新的GPT模型并不总是意味着性能的提升。值得注意的是，3月发布的GPT-4版本显示出比其后续6月发布版本更高的事实准确性。此外，还观察到一个令人担忧的偏见，即对全球南方的陈述给予了特权，可能加剧了不平等。

    The increasing reliance on AI-driven solutions, particularly Large Language Models (LLMs) like the GPT series, for information retrieval highlights the critical need for their factuality and fairness, especially amidst the rampant spread of misinformation and disinformation online. Our study evaluates the factual accuracy, stability, and biases in widely adopted GPT models, including GPT-3.5 and GPT-4, contributing to reliability and integrity of AI-mediated information dissemination.   We introduce 'Global-Liar,' a dataset uniquely balanced in terms of geographic and temporal representation, facilitating a more nuanced evaluation of LLM biases. Our analysis reveals that newer iterations of GPT models do not always equate to improved performance. Notably, the GPT-4 version from March demonstrates higher factual accuracy than its subsequent June release. Furthermore, a concerning bias is observed, privileging statements from the Global North over the Global South, thus potentially exace
    
[^19]: 跨视角分层图学习超网络用于技能需求供应联合预测

    A Cross-View Hierarchical Graph Learning Hypernetwork for Skill Demand-Supply Joint Prediction

    [https://arxiv.org/abs/2401.17838](https://arxiv.org/abs/2401.17838)

    本文提出了一个跨视角分层图学习超网络（CHGH）框架，用于联合预测技能需求和供应。框架包括跨视角图编码器、层次图编码器和条件超解码器，能够捕捉不同技能之间的关系和需求供应的内在联系。

    

    技术和产业的迅速变化导致技能需求动态变化，因此员工和雇主预测这种变化以在劳动市场保持竞争优势至关重要。现有的研究要么依赖于领域专家知识，要么将技能演变视为简化的时间序列预测问题。然而，这两种方法都忽视了不同技能之间复杂的关系以及技能需求和供应变化之间的内在联系。本文提出了一种用于联合技能需求和供应预测的跨视角分层图学习超网络（CHGH）框架。具体而言，CHGH是一个编码器-解码器网络，包括：i) 一个跨视角图编码器用于捕捉技能需求和供应之间的相互关联，ii) 一个层次图编码器用于从集群角度建模技能的共同演变，iii) 一个条件超解码器用于共同预测需求和供应变量。

    The rapidly changing landscape of technology and industries leads to dynamic skill requirements, making it crucial for employees and employers to anticipate such shifts to maintain a competitive edge in the labor market. Existing efforts in this area either rely on domain-expert knowledge or regarding skill evolution as a simplified time series forecasting problem. However, both approaches overlook the sophisticated relationships among different skills and the inner-connection between skill demand and supply variations. In this paper, we propose a Cross-view Hierarchical Graph learning Hypernetwork (CHGH) framework for joint skill demand-supply prediction. Specifically, CHGH is an encoder-decoder network consisting of i) a cross-view graph encoder to capture the interconnection between skill demand and supply, ii) a hierarchical graph encoder to model the co-evolution of skills from a cluster-wise perspective, and iii) a conditional hyper-decoder to jointly predict demand and supply va
    
[^20]: 利用Swin Transformer进行从局部到全局的弱监督语义分割

    Leveraging Swin Transformer for Local-to-Global Weakly Supervised Semantic Segmentation

    [https://arxiv.org/abs/2401.17828](https://arxiv.org/abs/2401.17828)

    本研究利用Swin Transformer提出了"SWTformer"，通过从局部到全局的视角来增强弱监督语义分割的准确性。

    

    最近几年，利用图像级标签作为监督的弱监督语义分割在计算机视觉领域引起了极大关注。大多数现有方法通过从类激活图(CAM)中生成伪标签来解决这些标签中缺乏空间信息的挑战。由于卷积神经网络(CNNs)的局部模式检测，CAMs通常只强调对象的最具区分度的部分，使得准确区分前景对象与背景以及彼此之间变得困难。最近的研究表明，由于其全局视角，Vision Transformer (ViT)特征在捕捉场景布局方面比CNNs更有效。然而，层次化的ViTs在该领域中的应用尚未得到广泛探索。本文通过提出“SWTformer”来探索使用Swin Transformer来提高初始准确性。

    In recent years, weakly supervised semantic segmentation using image-level labels as supervision has received significant attention in the field of computer vision. Most existing methods have addressed the challenges arising from the lack of spatial information in these labels by focusing on facilitating supervised learning through the generation of pseudo-labels from class activation maps (CAMs). Due to the localized pattern detection of Convolutional Neural Networks (CNNs), CAMs often emphasize only the most discriminative parts of an object, making it challenging to accurately distinguish foreground objects from each other and the background. Recent studies have shown that Vision Transformer (ViT) features, due to their global view, are more effective in capturing the scene layout than CNNs. However, the use of hierarchical ViTs has not been extensively explored in this field. This work explores the use of Swin Transformer by proposing "SWTformer" to enhance the accuracy of the init
    
[^21]: 马拉亚拉姆语改写生成的神经机器翻译

    Neural Machine Translation for Malayalam Paraphrase Generation

    [https://arxiv.org/abs/2401.17827](https://arxiv.org/abs/2401.17827)

    本研究针对马拉亚拉姆语探索了四种生成改写的方法，并使用了英语改写和预训练的神经机器翻译模型的资源。研究发现，自动化评估指标不完全适用于马拉亚拉姆语，强调了对于高度合词性语言更细致的改写评估方法的需求。

    

    本研究探讨了四种在马拉亚拉姆语中生成改写的方法，利用了英语改写和预训练的神经机器翻译（NMT）模型的资源。我们使用自动化指标（如BLEU，METEOR和余弦相似度）以及人工标注来评估所得到的改写。我们的研究发现，自动化评估指标对于马拉亚拉姆语可能不完全适用，因为它们与人类判断不一致。这种差异突显了对于高度合词性语言尤其需要更细致的改写评估方法的需求。

    This study explores four methods of generating paraphrases in Malayalam, utilizing resources available for English paraphrasing and pre-trained Neural Machine Translation (NMT) models. We evaluate the resulting paraphrases using both automated metrics, such as BLEU, METEOR, and cosine similarity, as well as human annotation. Our findings suggest that automated evaluation measures may not be fully appropriate for Malayalam, as they do not consistently align with human judgment. This discrepancy underscores the need for more nuanced paraphrase evaluation approaches especially for highly agglutinative languages.
    
[^22]: 确定性计算能力网络：架构、技术与前景

    Deterministic Computing Power Networking: Architecture, Technologies and Prospects

    [https://arxiv.org/abs/2401.17812](https://arxiv.org/abs/2401.17812)

    提出了一种名为确定性计算能力网络（Det-CPN）的新网络范式，旨在为计算密集型和延迟敏感的任务等新型互联网服务提供端到端传输确定性和计算确定性。该研究对于确保服务的安全高效运行具有重要意义。

    

    随着计算密集型和延迟敏感的任务等新型互联网服务的发展，传统的“尽力而为”网络传输方式受到了极大的挑战。网络系统迫切需要为新应用提供端到端传输确定性和计算确定性，以确保服务的安全高效运行。在计算和网络的融合研究基础上，提出了一种名为确定性计算能力网络（Det-CPN）的新网络范 paradigm。本文首先介绍了计算能力网络的研究进展。然后分析了Det-CPN的动机和应用场景。接着，我们给出了Det-CPN的系统架构、技术能力、工作流程以及关键技术。最后，分析和讨论了Det-CPN面临的挑战和未来趋势。

    With the development of new Internet services such as computation-intensive and delay-sensitive tasks, the traditional "Best Effort" network transmission mode has been greatly challenged. The network system is urgently required to provide end-to-end transmission determinacy and computing determinacy for new applications to ensure the safe and efficient operation of services. Based on the research of the convergence of computing and networking, a new network paradigm named deterministic computing power networking (Det-CPN) is proposed. In this article, we firstly introduce the research advance of computing power networking. And then the motivations and scenarios of Det-CPN are analyzed. Following that, we present the system architecture, technological capabilities, workflow as well as key technologies for Det-CPN. Finally, the challenges and future trends of Det-CPN are analyzed and discussed.
    
[^23]: SWEA:通过主题词嵌入修改改变大型语言模型中的事实知识

    SWEA: Changing Factual Knowledge in Large Language Models via Subject Word Embedding Altering

    [https://arxiv.org/abs/2401.17809](https://arxiv.org/abs/2401.17809)

    提出了一种主题词嵌入修改框架（SWEA），通过在推理阶段修改主题的表示来编辑知识，保护模型的原始权重，避免不可逆的损害和额外的推理开销。

    

    模型编辑近来引起了广泛关注。目前的模型编辑方法主要涉及修改模型参数或向现有模型添加附加模块。然而，前者会对LLM造成不可逆的影响，而后者会产生额外的推理开销，并且模糊的向量匹配并不总是可靠的。为了解决这些问题，我们提出了一种可扩展的主题词嵌入修改（SWEA）框架，它在推理阶段修改主题的表示，并实现编辑知识的目标。SWEA在模型外部使用精确的关键匹配，并进行可靠的主题词嵌入修改，从而保护模型的原始权重而不增加推理开销。然后，我们提出优化抑制融合方法，首先优化编辑目标的嵌入向量，然后抑制知识嵌入维度（KED）以获得最终融合的嵌入。我们因此提出了SWEAOS元方法。

    Model editing has recently gained widespread attention. Current model editing methods primarily involve modifying model parameters or adding additional modules to the existing model. However, the former causes irreversible damage to LLMs, while the latter incurs additional inference overhead and fuzzy vector matching is not always reliable. To address these issues, we propose an expandable Subject Word Embedding Altering (SWEA) framework, which modifies the representation of subjects and achieve the goal of editing knowledge during the inference stage. SWEA uses precise key matching outside the model and performs reliable subject word embedding altering, thus protecting the original weights of the model without increasing inference overhead. We then propose optimizing then suppressing fusion method, which first optimizes the embedding vector for the editing target and then suppresses the Knowledge Embedding Dimension (KED) to obtain the final fused embedding. We thus propose SWEAOS met
    
[^24]: 生物圈人工智能

    Biospheric AI

    [https://arxiv.org/abs/2401.17805](https://arxiv.org/abs/2401.17805)

    生物圈人工智能是一种新的范式，以生态中心主义为基础，旨在捕捉生物圈的复杂性并确保人工智能不会对其造成损害。

    

    在人工智能伦理和价值调整领域中，人类中心主义占据主导地位。这些学科的关注重点仅限于人类价值观，导致其洞察力的深度和广度受限。最近，一些学者已开始尝试扩大到以感知者为中心的视角。我们认为，这两种观点都不足以捕捉生物圈的实际复杂性，并确保人工智能不会对其造成损害。因此，我们提出了一种新的范式——生物圈人工智能，它采用了一个生态中心主义的视角。我们讨论了这种人工智能可能被设计的假设性方式。此外，我们还提出了与生物圈利益一致的现代人工智能模型研究和应用的方向。总的来说，这项工作试图迈出首要步骤，研究人工智能与生物圈之间的相互作用。

    The dominant paradigm in AI ethics and value alignment is highly anthropocentric. The focus of these disciplines is strictly on human values which limits the depth and breadth of their insights. Recently, attempts to expand to a sentientist perspective have been initiated. We argue that neither of these outlooks is sufficient to capture the actual complexity of the biosphere and ensure that AI does not damage it. Thus, we propose a new paradigm -- Biospheric AI that assumes an ecocentric perspective. We discuss hypothetical ways in which such an AI might be designed. Moreover, we give directions for research and application of the modern AI models that would be consistent with the biospheric interests. All in all, this work attempts to take first steps towards a comprehensive program of research that focuses on the interactions between AI and the biosphere.
    
[^25]: 带有动量对比学习的蒸馏增强时间序列预测网络

    Distillation Enhanced Time Series Forecasting Network with Momentum Contrastive Learning

    [https://arxiv.org/abs/2401.17802](https://arxiv.org/abs/2401.17802)

    本论文提出了一种创新的蒸馏增强框架，用于长序列时间序列预测。通过设计可学习的数据增强机制和带有动量更新的对比学习任务，能够充分利用时间序列数据的复杂性，并获得更鲁棒的表示。

    

    对比表示学习在时间序列分析中非常重要，可以缓解数据噪声、不完整性以及监督信号稀疏性等问题。然而，现有的对比学习框架通常聚焦于时间内部特征，未能充分利用时间序列数据的复杂性。为解决这个问题，我们提出了DE-TSMCL，一种创新的用于长序列时间序列预测的蒸馏增强框架。具体来说，我们设计了一种可学习的数据增强机制，用于自适应地学习是否屏蔽时间戳以获得优化的子序列。然后，我们提出了一种带有动量更新的对比学习任务，探索时间序列的样本间和时间内部的相关性，从而学习未标记时间序列的潜在结构特征。同时，我们设计了一个监督任务，以学习更鲁棒的表示并促进对比学习过程。最后，我们联合优化上述两个任务。

    Contrastive representation learning is crucial in time series analysis as it alleviates the issue of data noise and incompleteness as well as sparsity of supervision signal. However, existing constrastive learning frameworks usually focus on intral-temporal features, which fails to fully exploit the intricate nature of time series data. To address this issue, we propose DE-TSMCL, an innovative distillation enhanced framework for long sequence time series forecasting. Specifically, we design a learnable data augmentation mechanism which adaptively learns whether to mask a timestamp to obtain optimized sub-sequences. Then, we propose a contrastive learning task with momentum update to explore inter-sample and intra-temporal correlations of time series to learn the underlying structure feature on the unlabeled time series. Meanwhile, we design a supervised task to learn more robust representations and facilitate the contrastive learning process. Finally, we jointly optimize the above two 
    
[^26]: 不带位置编码的图变压器

    Graph Transformers without Positional Encodings

    [https://arxiv.org/abs/2401.17791](https://arxiv.org/abs/2401.17791)

    本文介绍了一种不需要位置编码的图变压器模型，该模型通过注意机制本身包含图结构信息，并通过实验证明了其有效性。

    

    最近，用于图表示学习的变压器越来越受欢迎，在各种数据集上取得了最先进的性能，无论是单独使用还是与消息传递图神经网络（MP-GNN）结合。将图归纳偏见融入天然与结构无关的变压器架构中，以结构或位置编码（PEs）的形式，是实现这些令人印象深刻的结果的关键。然而，设计这样的编码是棘手的，人们已经提出了不同的尝试来设计这样的编码，包括拉普拉斯特征向量、相对随机行走概率（RRWP）、空间编码、中心度编码、边缘编码等。在这项工作中，我们认为这些编码可能根本不需要，只要注意机制本身包含有关图结构的信息。我们介绍了Eigenformer，它使用一种新颖的谱感知注意机制，了解图的拉普拉斯谱，并通过实验证明

    Recently, Transformers for graph representation learning have become increasingly popular, achieving state-of-the-art performance on a wide-variety of datasets, either alone or in combination with message-passing graph neural networks (MP-GNNs). Infusing graph inductive-biases in the innately structure-agnostic transformer architecture in the form of structural or positional encodings (PEs) is key to achieving these impressive results. However, designing such encodings is tricky and disparate attempts have been made to engineer such encodings including Laplacian eigenvectors, relative random-walk probabilities (RRWP), spatial encodings, centrality encodings, edge encodings etc. In this work, we argue that such encodings may not be required at all, provided the attention mechanism itself incorporates information about the graph structure. We introduce Eigenformer, which uses a novel spectrum-aware attention mechanism cognizant of the Laplacian spectrum of the graph, and empirically show
    
[^27]: SDRDPy：一款可图形化显示通过监督性描述性规则算法获得知识的应用程序

    SDRDPy: An application to graphically visualize the knowledge obtained with supervised descriptive rule algorithms

    [https://arxiv.org/abs/2401.17783](https://arxiv.org/abs/2401.17783)

    SDRDPy是一款可图形化显示通过监督性描述性规则算法获得的知识的应用程序，通过直观的图表和表格帮助专家分析数据集的相关信息和规则之间的关系，以及导出报告。

    

    SDRDPy是一款桌面应用程序，允许专家直观地图形化和表格化表示通过任何监督性描述性规则发现算法提取的知识。该应用程序能够提供数据分析，显示数据集的相关信息以及规则、数据和每个规则的质量度量之间的关系，无论算法在何处执行。所有信息都以用户友好的方式呈现，以便于专家分析，同时还可导出不同格式的报告。

    SDRDPy is a desktop application that allows experts an intuitive graphic and tabular representation of the knowledge extracted by any supervised descriptive rule discovery algorithm. The application is able to provide an analysis of the data showing the relevant information of the data set and the relationship between the rules, data and the quality measures associated for each rule regardless of the tool where algorithm has been executed. All of the information is presented in a user-friendly application in order to facilitate expert analysis and also the exportation of reports in different formats.
    
[^28]: 双重InfoGAN用于对比分析

    Double InfoGAN for Contrastive Analysis

    [https://arxiv.org/abs/2401.17776](https://arxiv.org/abs/2401.17776)

    这篇论文提出了双重InfoGAN方法用于对比分析，通过结合GAN的高质量合成和InfoGAN的分离能力，有效地解决了当前基于VAE的方法在处理共同因素和特殊因素时的不足，并在不同的视觉数据集上展现出优越的性能。

    

    对比分析（CA）处理的是发现目标领域与背景领域相比的共同点和特殊点。这在许多应用中具有极大的兴趣，例如医学成像。目前的最新方法是基于VAE的潜在变量模型（CA-VAEs）。然而，它们要么忽略重要的约束条件，要么没有强制执行基本假设。这可能导致次优解，其中将特殊因素误认为共同因素（或反之亦然）。此外，生成的图像具有VAE的较差质量，降低了其可解释性和实用性。在这里，我们提出了双重InfoGAN，这是第一个基于GAN的CA方法，它充分利用了GAN的高质量合成和InfoGAN的分离能力。在从简单的合成示例到复杂的医学图像的四个视觉数据集上的实验结果表明，所提出的方法在潜在分离和图像质量方面优于最新的CA-VAEs。

    Contrastive Analysis (CA) deals with the discovery of what is common and what is distinctive of a target domain compared to a background one. This is of great interest in many applications, such as medical imaging. Current state-of-the-art (SOTA) methods are latent variable models based on VAE (CA-VAEs). However, they all either ignore important constraints or they don't enforce fundamental assumptions. This may lead to sub-optimal solutions where distinctive factors are mistaken for common ones (or viceversa). Furthermore, the generated images have a rather poor quality, typical of VAEs, decreasing their interpretability and usefulness. Here, we propose Double InfoGAN, the first GAN based method for CA that leverages the high-quality synthesis of GAN and the separation power of InfoGAN. Experimental results on four visual datasets, from simple synthetic examples to complex medical images, show that the proposed method outperforms SOTA CA-VAEs in terms of latent separation and image qu
    
[^29]: PF-GNN: 可微的基于粒子滤波的通用图表示逼近

    PF-GNN: Differentiable particle filtering based approximation of universal graph representations

    [https://arxiv.org/abs/2401.17752](https://arxiv.org/abs/2401.17752)

    PF-GNN是一种可微的基于粒子滤波的通用图表示逼近算法，通过引入精确同构求解技术指导学习过程，从而提高了图神经网络的表达能力。

    

    消息传递的图神经网络（GNN）在表示图同构性的1-WL颜色精炼测试方面受到了限制。其他更具表达能力的模型要么计算成本高，要么需要预处理来提取结构特征。在这项工作中，我们提出了一种通过使用精确同构求解技术来指导学习过程，以使GNN成为通用模型。该技术在个体化和精炼（IR）范式下操作，通过人为引入不对称性并进一步精炼着色，当1-WL停止时。同构求解器生成一个颜色着色的搜索树，其叶子节点能唯一标识图。然而，搜索树的大小呈指数级增长，并且需要手工设计的修剪技术，这在学习角度上是不可取的。我们采用概率视角，并通过从根节点到叶子节点的搜索树中采样多条路径，来近似颜色着色的搜索树（即嵌入）。以学习更有辨别性的表示。

    Message passing Graph Neural Networks (GNNs) are known to be limited in expressive power by the 1-WL color-refinement test for graph isomorphism. Other more expressive models either are computationally expensive or need preprocessing to extract structural features from the graph. In this work, we propose to make GNNs universal by guiding the learning process with exact isomorphism solver techniques which operate on the paradigm of Individualization and Refinement (IR), a method to artificially introduce asymmetry and further refine the coloring when 1-WL stops. Isomorphism solvers generate a search tree of colorings whose leaves uniquely identify the graph. However, the tree grows exponentially large and needs hand-crafted pruning techniques which are not desirable from a learning perspective. We take a probabilistic view and approximate the search tree of colorings (i.e. embeddings) by sampling multiple paths from root to leaves of the search tree. To learn more discriminative represe
    
[^30]: SwarmBrain: 大型语言模型在实时战略游戏星际争霸II中的应用

    SwarmBrain: Embodied agent for real-time strategy game StarCraft II via large language models

    [https://arxiv.org/abs/2401.17749](https://arxiv.org/abs/2401.17749)

    SwarmBrain是一个利用LLM在星际争霸II游戏环境中实现实时策略的具身化智能体，由Overmind Intelligence Matrix和Swarm ReflexNet两个关键组件组成。Overmind Intelligence Matrix负责宏观策略的决策和协调，而Swarm ReflexNet处理微观级别的战术决策。

    

    近期，大型语言模型（LLMs）在各种探索性任务上取得了显著成就，甚至超过了历史上主导代理系统领域的传统强化学习方法的性能。本文旨在研究LLMs在星际争霸II游戏环境中执行实时战略战争任务的有效性。我们介绍了SwarmBrain，一个利用LLM在星际争霸II游戏环境中实现实时策略的具身化智能体。SwarmBrain包括两个关键组件：1）Overmind Intelligence Matrix，由最先进的LLMs驱动，旨在从高层次视角组织宏观策略。该矩阵模拟虫族智能大脑的总体意识，综合战略预见力，旨在分配资源，指导扩张和协调多线攻击。2）Swarm ReflexNet，是一个灵活的协同体，用于处理微观级别的战术决策，它通过感知环境变化和敌人动态来生成即时反应。

    Large language models (LLMs) have recently garnered significant accomplishments in various exploratory tasks, even surpassing the performance of traditional reinforcement learning-based methods that have historically dominated the agent-based field. The purpose of this paper is to investigate the efficacy of LLMs in executing real-time strategy war tasks within the StarCraft II gaming environment. In this paper, we introduce SwarmBrain, an embodied agent leveraging LLM for real-time strategy implementation in the StarCraft II game environment. The SwarmBrain comprises two key components: 1) a Overmind Intelligence Matrix, powered by state-of-the-art LLMs, is designed to orchestrate macro-level strategies from a high-level perspective. This matrix emulates the overarching consciousness of the Zerg intelligence brain, synthesizing strategic foresight with the aim of allocating resources, directing expansion, and coordinating multi-pronged assaults. 2) a Swarm ReflexNet, which is agile co
    
[^31]: Haris:一个用于智能停车辅助的先进自主移动机器人

    Haris: an Advanced Autonomous Mobile Robot for Smart Parking Assistance

    [https://arxiv.org/abs/2401.17741](https://arxiv.org/abs/2401.17741)

    本文介绍了Haris，一个先进的自主移动机器人系统，通过车牌识别追踪拥挤停车场中车辆的位置，消除了对GPS的依赖，并提供了便捷的用户界面来帮助用户确定车辆位置和缓解停车场拥堵。

    

    本文介绍了Haris，一个先进的自主移动机器人系统，通过车牌识别追踪拥挤停车场中车辆的位置。该系统采用同时定位与建图（SLAM）进行自主导航和精确建图，消除了对GPS的依赖。此外，该系统利用计算机视觉技术实现了物体检测和自动车牌识别（ALPR），可以读取和关联车牌号与位置数据。然后，这些信息与后端服务同步，并通过一个用户友好的移动应用程序提供给用户，方便确定车辆位置并缓解停车场的拥堵。提出的系统有潜力改善拥挤场所如体育场等大型室外临时停车区域的管理。Demo视频可以在https://youtu.be/ZkTCM35f中找到。

    This paper presents Haris, an advanced autonomous mobile robot system for tracking the location of vehicles in crowded car parks using license plate recognition. The system employs simultaneous localization and mapping (SLAM) for autonomous navigation and precise mapping of the parking area, eliminating the need for GPS dependency. In addition, the system utilizes a sophisticated framework using computer vision techniques for object detection and automatic license plate recognition (ALPR) for reading and associating license plate numbers with location data. This information is subsequently synchronized with a back-end service and made accessible to users via a user-friendly mobile app, offering effortless vehicle location and alleviating congestion within the parking facility. The proposed system has the potential to improve the management of short-term large outdoor parking areas in crowded places such as sports stadiums. The demo of the robot can be found on https://youtu.be/ZkTCM35f
    
[^32]: 不需要伴随算子的算子学习

    Operator learning without the adjoint

    [https://arxiv.org/abs/2401.17739](https://arxiv.org/abs/2401.17739)

    本论文提出了一种不需要探测伴随算子的算子学习方法，通过在Fourier基上进行投影来逼近一类非自伴随的无限维紧算子，并应用于恢复椭圆型偏微分算子的格林函数。这是第一个试图填补算子学习理论与实践差距的无需伴随算子分析。

    

    算子学习中存在一个谜团：如何在没有探测伴随算子的情况下从数据中恢复非自伴随算子？目前的实际方法表明，在仅使用由算子的正向作用生成的数据的情况下，可以准确地恢复算子，而不需要访问伴随算子。然而，以直观的方式看，似乎有必要采样伴随算子的作用。在本文中，我们部分解释了这个谜团，通过证明在不查询伴随算子的情况下，可以通过在Fourier基上进行投影来逼近一类非自伴随的无限维紧算子。然后，我们将该结果应用于恢复椭圆型偏微分算子的格林函数，并导出一个无需伴随算子的样本复杂度界限。虽然现有的理论证明了算子学习的低样本复杂度，但我们的是第一个试图填补理论与实践差距的无需伴随算子的分析。

    There is a mystery at the heart of operator learning: how can one recover a non-self-adjoint operator from data without probing the adjoint? Current practical approaches suggest that one can accurately recover an operator while only using data generated by the forward action of the operator without access to the adjoint. However, naively, it seems essential to sample the action of the adjoint. In this paper, we partially explain this mystery by proving that without querying the adjoint, one can approximate a family of non-self-adjoint infinite-dimensional compact operators via projection onto a Fourier basis. We then apply the result to recovering Green's functions of elliptic partial differential operators and derive an adjoint-free sample complexity bound. While existing theory justifies low sample complexity in operator learning, ours is the first adjoint-free analysis that attempts to close the gap between theory and practice.
    
[^33]: 在神经进化系统中迈向物理合理性

    Towards Physical Plausibility in Neuroevolution Systems

    [https://arxiv.org/abs/2401.17733](https://arxiv.org/abs/2401.17733)

    本研究关注神经进化系统中物理合理性的问题，提出了一种旨在最大化人工神经网络模型准确性同时最小化功耗的方法，通过引入新的突变策略和训练技术来优化模型表现和节能效果。

    

    使用人工智能模型，特别是深度神经网络，越来越多地增加了训练和推理过程中的功耗，引发了对更加节能算法和硬件解决方案的需求，并引起了环境担忧。本研究致力于解决机器学习中推理阶段日益增长的能源消耗问题。即使稍微减少电力使用也可能导致显著的能源节约，使用户、公司和环境都受益。我们的方法着重于在神经进化框架中最大化人工神经网络模型的准确性，同时最小化其功耗。为此，我们在适应度函数中考虑了功耗。我们引入一种新的突变策略，以随机方式重新引入层模块，具有节能模块的选择机会更高。我们还引入了一种训练两个独立模型的新技术。

    The increasing usage of Artificial Intelligence (AI) models, especially Deep Neural Networks (DNNs), is increasing the power consumption during training and inference, posing environmental concerns and driving the need for more energy-efficient algorithms and hardware solutions. This work addresses the growing energy consumption problem in Machine Learning (ML), particularly during the inference phase. Even a slight reduction in power usage can lead to significant energy savings, benefiting users, companies, and the environment. Our approach focuses on maximizing the accuracy of Artificial Neural Network (ANN) models using a neuroevolutionary framework whilst minimizing their power consumption. To do so, power consumption is considered in the fitness function. We introduce a new mutation strategy that stochastically reintroduces modules of layers, with power-efficient modules having a higher chance of being chosen. We introduce a novel technique that allows training two separate models
    
[^34]: 使用基于脑电功能连接和机器学习方法预测经过长期tDCS干预的多任务表现

    Prediction of multitasking performance post-longitudinal tDCS via EEG-based functional connectivity and machine learning methods

    [https://arxiv.org/abs/2401.17711](https://arxiv.org/abs/2401.17711)

    本研究使用脑电功能连接和机器学习方法，预测经过长期tDCS干预后的多任务认知表现的变化。

    

    预测和理解认知表现的变化，特别是在长期干预之后，是神经科学的一个基本目标。纵向基于脑刺激的干预方法，如经皮直流电刺激（tDCS），会引起静息膜电位的短期变化，并影响认知过程。然而，目前对于预测干预后认知表现变化的研究很少。本研究旨在通过采用不同的基于脑电功能连接分析和机器学习算法，预测复杂多任务任务中认知表现的变化，来填补文献中的这一空白。将40名受试者分为实验组和主动对照组。在第一天，所有受试者执行了一个多任务任务，并同时获取到32通道的脑电信号。从第二天到第七天，实验组受试者进行了15分钟的2mA阴极tDCS刺激。

    Predicting and understanding the changes in cognitive performance, especially after a longitudinal intervention, is a fundamental goal in neuroscience. Longitudinal brain stimulation-based interventions like transcranial direct current stimulation (tDCS) induce short-term changes in the resting membrane potential and influence cognitive processes. However, very little research has been conducted on predicting these changes in cognitive performance post-intervention. In this research, we intend to address this gap in the literature by employing different EEG-based functional connectivity analyses and machine learning algorithms to predict changes in cognitive performance in a complex multitasking task. Forty subjects were divided into experimental and active-control conditions. On Day 1, all subjects executed a multitasking task with simultaneous 32-channel EEG being acquired. From Day 2 to Day 7, subjects in the experimental condition undertook 15 minutes of 2mA anodal tDCS stimulation
    
[^35]: 室内设计中的审美偏好预测：模糊方法

    Aesthetic Preference Prediction in Interior Design: Fuzzy Approach

    [https://arxiv.org/abs/2401.17710](https://arxiv.org/abs/2401.17710)

    本论文介绍了一种基于模糊逻辑和图像处理的方法，用于量化和预测室内设计中的审美偏好。研究结合了颜色和谐度、亮度和复杂度等视觉属性，通过加权平均法计算总体审美得分，并考虑个人颜色偏好。

    

    室内设计是关于创造看起来和感觉良好的空间的。然而，审美偏好的主观性质在定义和量化何为室内设计视觉吸引力方面提出了重大挑战。本论文通过引入一种新的方法论来量化和预测室内设计的审美偏好来填补这一差距。我们的研究将模糊逻辑与图像处理技术相结合。我们从社交媒体平台收集了一组室内设计图像数据集，关注颜色和谐度、亮度和复杂度等基本视觉属性。我们使用加权平均法整合这些特征以计算一个总体审美得分。我们的方法在计算整体审美偏好时考虑个人颜色偏好。我们首先收集用户对红色、棕色等主要颜色的评分，以了解他们的偏好。然后，我们使用图像中前五个主导颜色的像素计数来得到

    Interior design is all about creating spaces that look and feel good. However, the subjective nature of aesthetic preferences presents a significant challenge in defining and quantifying what makes an interior design visually appealing. The current paper addresses this gap by introducing a novel methodology for quantifying and predicting aesthetic preferences in interior design. Our study combines fuzzy logic with image processing techniques. We collected a dataset of interior design images from social media platforms, focusing on essential visual attributes such as color harmony, lightness, and complexity. We integrate these features using weighted average to compute a general aesthetic score. Our approach considers individual color preferences in calculating the overall aesthetic preference. We initially gather user ratings for primary colors like red, brown, and others to understand their preferences. Then, we use the pixel count of the top five dominant colors in the image to get t
    
[^36]: 长期经颅直流电刺激后使用功能连接和机器学习方法对执行功能表现的分类

    Classification of executive functioning performance post-longitudinal tDCS using functional connectivity and machine learning methods

    [https://arxiv.org/abs/2401.17700](https://arxiv.org/abs/2401.17700)

    本研究使用功能连接和机器学习方法对长期经颅直流电刺激（tDCS）后的执行功能表现进行了分类，为理解和研究该干预方式对人类认知过程的影响提供了重要的参考。

    

    执行功能是一种认知过程，使人类能够以目标导向的方式计划、组织和调节行为。目前还没有对经过长期干预（如经颅直流电刺激（tDCS））后执行功能变化进行研究和分类的文献。本研究利用功能连接和机器学习算法对tDCS后的执行功能表现进行分类。将五十名受试者分为实验组和安慰剂对照组。在第一天，受试者在执行功能任务时收集了脑电数据。实验组在第2天到第8天进行任务训练时接受tDCS，而对照组接受了安慰剂tDCS。第10天，受试者重复了第一天指定的任务。从脑电数据中提取了不同的功能连接指标，最终使用不同的机器学习算法对执行功能表现进行分类。

    Executive functioning is a cognitive process that enables humans to plan, organize, and regulate their behavior in a goal-directed manner. Understanding and classifying the changes in executive functioning after longitudinal interventions (like transcranial direct current stimulation (tDCS)) has not been explored in the literature. This study employs functional connectivity and machine learning algorithms to classify executive functioning performance post-tDCS. Fifty subjects were divided into experimental and placebo control groups. EEG data was collected while subjects performed an executive functioning task on Day 1. The experimental group received tDCS during task training from Day 2 to Day 8, while the control group received sham tDCS. On Day 10, subjects repeated the tasks specified on Day 1. Different functional connectivity metrics were extracted from EEG data and eventually used for classifying executive functioning performance using different machine learning algorithms. Resu
    
[^37]: EnCLAP：将神经音频编解码器和音频文本联合嵌入结合的自动音频字幕生成方法

    EnCLAP: Combining Neural Audio Codec and Audio-Text Joint Embedding for Automated Audio Captioning

    [https://arxiv.org/abs/2401.17690](https://arxiv.org/abs/2401.17690)

    EnCLAP是一种自动音频字幕生成框架，利用神经音频编解码器和音频文本联合嵌入技术，通过引入masked codec modeling训练目标提高了预训练语言模型的声学意识，实验证明其在AudioCaps和Clotho数据集上优于基准模型的性能。

    

    我们提出了EnCLAP，一种新颖的自动音频字幕生成框架。EnCLAP采用了两种声学表示模型：EnCodec和CLAP，还使用了一个预训练语言模型BART。我们还引入了一种名为masked codec modeling的新的训练目标，以提高预训练语言模型的声学意识。在AudioCaps和Clotho数据集上的实验结果表明，我们的模型超过了基准模型的性能。源代码可在https://github.com/jaeyeonkim99/EnCLAP获取。在线演示可在https://huggingface.co/spaces/enclap-team/enclap 上进行。

    We propose EnCLAP, a novel framework for automated audio captioning. EnCLAP employs two acoustic representation models, EnCodec and CLAP, along with a pretrained language model, BART. We also introduce a new training objective called masked codec modeling that improves acoustic awareness of the pretrained language model. Experimental results on AudioCaps and Clotho demonstrate that our model surpasses the performance of baseline models. Source code will be available at https://github.com/jaeyeonkim99/EnCLAP . An online demo is available at https://huggingface.co/spaces/enclap-team/enclap .
    
[^38]: 在大型语言模型和大脑中，上下文特征提取层次收敛

    Contextual Feature Extraction Hierarchies Converge in Large Language Models and the Brain

    [https://arxiv.org/abs/2401.17671](https://arxiv.org/abs/2401.17671)

    在研究中发现，随着大型语言模型在基准任务上的性能提高，模型不仅在预测神经响应时表现出更高的类脑性能，而且其分层特征提取路径与大脑的映射更接近，并且使用更少的层次来进行相同的编码。

    

    最近在人工智能方面的进展引发了对大型语言模型（LLM）和人类神经处理之间相似性的兴趣，特别是在语言理解方面。虽然之前的研究已经确定了LLM表示和大脑之间的相似之处，但在演化的LLM背景下引发这种收敛的潜在计算原理仍然不清楚。在这里，我们对一系列性能较高的LLM进行了研究，这些模型的参数大小相似，以探究导致其与大脑语言处理机制一致的因素。我们发现，当LLM在基准任务上达到更高的性能时，它们不仅在预测LLM嵌入的神经响应时表现出更高的类脑性能，而且它们的分层特征提取路径与大脑的映射更接近，并且使用更少的层次来进行相同的编码。

    Recent advancements in artificial intelligence have sparked interest in the parallels between large language models (LLMs) and human neural processing, particularly in language comprehension. While prior research has established similarities in the representation of LLMs and the brain, the underlying computational principles that cause this convergence, especially in the context of evolving LLMs, remain elusive. Here, we examined a diverse selection of high-performance LLMs with similar parameter sizes to investigate the factors contributing to their alignment with the brain's language processing mechanisms. We find that as LLMs achieve higher performance on benchmark tasks, they not only become more brain-like as measured by higher performance when predicting neural responses from LLM embeddings, but also their hierarchical feature extraction pathways map more closely onto the brain's while using fewer layers to do the same encoding. We also compare the feature extraction pathways of 
    
[^39]: 朝着工业4.0的实现: 以客户生命周期为导向的基于方法论的方法

    Towards the implementation of Industry 4.0: A methodology-based approach oriented to the customer life cycle

    [https://arxiv.org/abs/2401.17661](https://arxiv.org/abs/2401.17661)

    这项研究提出了一种以客户生命周期为导向的方法论，可以帮助中小型制造企业的软件工程师在实施工业4.0技术方面取得成功。

    

    许多不同的全球倡议正在推动从机器主导型制造转向数字化制造。为了实现成功转型到工业4.0标准，制造企业需要实施清晰的路线图。然而，中小型制造企业在工业4.0的实施过程中遇到许多障碍和困难（经济、技术、文化等）。虽然已有一些研究讨论了如何将工业4.0技术应用于产品生命周期和供应链生命周期，为中小型制造企业提供了参考，但对于客户生命周期来说情况并非如此。因此，我们提出了两个可以帮助中小型制造企业的软件工程师在客户生命周期中引入工业4.0技术的贡献。第一个贡献是一种方法论，可以帮助这些软件工程师创造与工业4.0相一致的新软件服务。

    Many different worldwide initiatives are promoting the transformation from machine dominant manufacturing to digital manufacturing. Thus, to achieve a successful transformation to Industry 4.0 standard, manufacturing enterprises are required to implement a clear roadmap. However, Small and Medium Manufacturing Enterprises (SMEs) encounter many barriers and difficulties (economical, technical, cultural, etc.) in the implementation of Industry 4.0. Although several works deal with the incorporation of Industry 4.0 technologies in the area of the product and supply chain life cycles, which SMEs could use as reference, this is not the case for the customer life cycle. Thus, we present two contributions that can help the software engineers of those SMEs to incorporate Industry 4.0 technologies in the context of the customer life cycle. The first contribution is a methodology that can help those software engineers in the task of creating new software services, aligned with Industry 4.0, that
    
[^40]: 从能量模型的潜在空间生成新的桥梁类型的尝试

    An attempt to generate new bridge types from latent space of energy-based model

    [https://arxiv.org/abs/2401.17657](https://arxiv.org/abs/2401.17657)

    使用能量模型进行桥梁创新，通过博弈论解释损失函数，并利用朗之万动力学技术生成能量值较低的新样本，建立基于能量的桥梁生成模型。

    

    使用能量模型进行桥梁创新。通过博弈论解释损失函数，逻辑清晰，公式简单明了。因此避免使用最大似然估计来解释损失函数，并消除了解决标准化分母的蒙特卡洛方法的需求。假设桥梁类型的种群符合玻尔兹曼分布，构建神经网络表示能量函数。利用朗之万动力学技术生成能量值较低的新样本，从而建立基于能量的桥梁生成模型。通过对三跨梁桥、拱桥、斜拉桥和悬索桥的对称结构图像数据集进行能量函数训练，精确计算真实和伪造样本的能量值。从潜在空间进行采样，利用梯度下降算法，能量函数将采样点转化为能量得分低的样本，从而生成新的桥梁。

    Use energy-based model for bridge-type innovation. The loss function is explained by the game theory, the logic is clear and the formula is simple and clear. Thus avoid the use of maximum likelihood estimation to explain the loss function and eliminate the need for Monte Carlo methods to solve the normalized denominator. Assuming that the bridge-type population follows a Boltzmann distribution, a neural network is constructed to represent the energy function. Use Langevin dynamics technology to generate a new sample with low energy value, thus a generative model of bridge-type based on energy is established. Train energy function on symmetric structured image dataset of three span beam bridge, arch bridge, cable-stayed bridge, and suspension bridge to accurately calculate the energy values of real and fake samples. Sampling from latent space, using gradient descent algorithm, the energy function transforms the sampling points into low energy score samples, thereby generating new bridge
    
[^41]: ReSLLM: 大型语言模型是联邦搜索强大的资源选择器

    ReSLLM: Large Language Models are Strong Resource Selectors for Federated Search

    [https://arxiv.org/abs/2401.17645](https://arxiv.org/abs/2401.17645)

    大型语言模型在联邦搜索中展现出强大的资源选择能力，相比于传统的基于特征的学习方法具有更高的效果和更低的成本。

    

    联邦搜索是将多个独立搜索引擎的结果整合起来的过程，在增强检索生成流水线中具有越来越重要的作用，为聊天机器人等基于LLM的应用提供支持。这些系统通常根据用户的话语性质，将查询分发到各种搜索引擎中，从专门的（如PubMed）到通用的（如Google）。联邦搜索的一个关键方面是资源选择，即在发出查询之前选择适当的资源，以确保高质量和快速响应，并降低调用外部搜索引擎的成本。然而，当前的SOTA资源选择方法主要依赖于基于特征的学习方法。这些方法通常涉及人力密集和昂贵的训练标签的创建。相比之下，LLM在NLP和IR任务中表现出了强大的零-shot方法的效果。我们假设在这篇论文中...

    Federated search, which involves integrating results from multiple independent search engines, will become increasingly pivotal in the context of Retrieval-Augmented Generation pipelines empowering LLM-based applications such as chatbots. These systems often distribute queries among various search engines, ranging from specialized (e.g., PubMed) to general (e.g., Google), based on the nature of user utterances. A critical aspect of federated search is resource selection - the selection of appropriate resources prior to issuing the query to ensure high-quality and rapid responses, and contain costs associated with calling the external search engines. However, current SOTA resource selection methodologies primarily rely on feature-based learning approaches. These methods often involve the labour intensive and expensive creation of training labels for each resource. In contrast, LLMs have exhibited strong effectiveness as zero-shot methods across NLP and IR tasks. We hypothesise that in t
    
[^42]: 在大型语言模型中解决过度杀伤问题的导航

    Navigating the OverKill in Large Language Models

    [https://arxiv.org/abs/2401.17633](https://arxiv.org/abs/2401.17633)

    本研究调查了大型语言模型中过度杀伤的因素，并发现了其中存在的捷径和对有害词语的过度关注。我们提出了自对比解码（Self-CD）策略来缓解过度杀伤现象，该策略无需训练且适用于各种模型。

    

    大型语言模型被精心调整，以既有助益又无害。然而，最近的研究指出存在潜在的过度杀伤问题，这意味着模型可能会拒绝回答无害查询。本文通过探索模型如何处理和确定查询的安全性，来研究过度杀伤的因素。我们的发现揭示了模型内部存在捷径，导致对“杀伤”等有害词语过度关注，而强调安全性的提示将加剧过度杀伤。基于这些发现，我们提出了一种无需训练且适用于各种模型的策略，名为自对比解码（Self-Contrastive Decoding，Self-CD），来缓解这一现象。首先，我们通过放大模型在回应系统提示时输出分布的差异，提取这种过度关注。然后，通过对比解码来减弱模型对这种过度关注的影响，以确定最终的下一个标记预测。实证结果表明，我们的方法具有

    Large language models are meticulously aligned to be both helpful and harmless. However, recent research points to a potential overkill which means models may refuse to answer benign queries. In this paper, we investigate the factors for overkill by exploring how models handle and determine the safety of queries. Our findings reveal the presence of shortcuts within models, leading to an over-attention of harmful words like 'kill' and prompts emphasizing safety will exacerbate overkill. Based on these insights, we introduce Self-Contrastive Decoding (Self-CD), a training-free and model-agnostic strategy, to alleviate this phenomenon. We first extract such over-attention by amplifying the difference in the model's output distributions when responding to system prompts that either include or omit an emphasis on safety. Then we determine the final next-token predictions by downplaying the over-attention from the model via contrastive decoding. Empirical results indicate that our method has
    
[^43]: 生成式人工智能生成测试数据生成器

    Generative AI to Generate Test Data Generators

    [https://arxiv.org/abs/2401.17626](https://arxiv.org/abs/2401.17626)

    本研究评估了生成式人工智能在不同领域生成测试数据的能力，并证明它能够在不同的集成级别上生成逼真的测试数据生成器。

    

    生成假数据是现代软件测试的重要维度之一，众多数据伪造库的数量和重要性证明了这一点。然而，伪造库的开发者无法满足不同自然语言和领域所需生成的广泛数据范围。本文评估了生成式人工智能在不同领域生成测试数据的能力。我们设计了三种类型的大型语言模型（LLMs）提示，它们在不同的集成级别上执行测试数据生成任务：1）原始测试数据生成，2）合成特定语言的程序以生成有用的测试数据，3）生成使用尖端伪造库的程序。我们通过提示LLMs为11个领域生成测试数据来评估我们的方法。结果表明，在所有三个集成级别上，LLMs能够成功地生成逼真的测试数据生成器。

    Generating fake data is an essential dimension of modern software testing, as demonstrated by the number and significance of data faking libraries. Yet, developers of faking libraries cannot keep up with the wide range of data to be generated for different natural languages and domains. In this paper, we assess the ability of generative AI for generating test data in different domains. We design three types of prompts for Large Language Models (LLMs), which perform test data generation tasks at different levels of integrability: 1) raw test data generation, 2) synthesizing programs in a specific language that generate useful test data, and 3) producing programs that use state-of-the-art faker libraries. We evaluate our approach by prompting LLMs to generate test data for 11 domains. The results show that LLMs can successfully generate realistic test data generators in a wide range of domains at all three levels of integrability.
    
[^44]: 揭示自监督在多视角多人物关联与跟踪中的能力

    Unveiling the Power of Self-supervision for Multi-view Multi-human Association and Tracking

    [https://arxiv.org/abs/2401.17617](https://arxiv.org/abs/2401.17617)

    本研究提出了一种自监督学习感知的网络，通过考虑反身性、对称性和传递性属性，解决了多视角多人物关联与跟踪的问题。

    

    多视角多人物关联与跟踪(MvMHAT)是多人场景视频监控中的一个新而重要的问题，旨在在每个视角中跟踪一组人，并且同时在不同视角中识别相同的人，这与先前仅考虑跨时间人物跟踪的MOT和多摄像头MOT任务不同。因此，MvMHAT的视频需要更复杂的注释，并包含更多的信息供自学习使用。在这项工作中，我们采用了一种自监督学习感知的端到端网络来解决这个问题。具体而言，我们提出利用空间-时间自一致性推理来考虑反身性、对称性和传递性三个属性。除了自然满足的反身性属性外，我们还设计了基于对称性和传递性属性的自监督学习损失，用于外观特征学习和分配矩阵优化，以进行关联和跟踪的优化。

    Multi-view multi-human association and tracking (MvMHAT), is a new but important problem for multi-person scene video surveillance, aiming to track a group of people over time in each view, as well as to identify the same person across different views at the same time, which is different from previous MOT and multi-camera MOT tasks only considering the over-time human tracking. This way, the videos for MvMHAT require more complex annotations while containing more information for self learning. In this work, we tackle this problem with a self-supervised learning aware end-to-end network. Specifically, we propose to take advantage of the spatial-temporal self-consistency rationale by considering three properties of reflexivity, symmetry and transitivity. Besides the reflexivity property that naturally holds, we design the self-supervised learning losses based on the properties of symmetry and transitivity, for both appearance feature learning and assignment matrix optimization, to associ
    
[^45]: 在地球观测数据上对GPT-4V进行标注任务评估：对语言视觉模型在视觉任务上的一项基准测试

    Good at captioning, bad at counting: Benchmarking GPT-4V on Earth observation data

    [https://arxiv.org/abs/2401.17600](https://arxiv.org/abs/2401.17600)

    在这项研究中，我们对GPT-4V模型在地球观测数据上的性能进行了评估。结果显示，尽管GPT-4V在开放式任务如位置理解和图像标注方面表现良好，但其空间推理能力不足，限制了其在目标定位和计数方面的实用性。

    

    大型语言视觉模型（VLMs）在涉及自然语言指令和视觉输入的复杂任务上展示出令人印象深刻的性能。然而，目前尚不清楚这些模型在以卫星和航空图像为主的地球观测（EO）数据上的能力，这类数据在VLMs的训练数据中较为罕见。在这项研究中，我们提出了一个全面的基准测试，通过评估VLMs在场景理解、定位和计数以及变化检测任务上的能力，以衡量它们在EO数据上作为有效工具的进展。受现实世界应用的启发，我们的基准测试包括了城市监测、灾害救援、土地利用和保护等场景。我们发现，尽管像GPT-4V这样的最新VLMs具有丰富的世界知识，导致在位置理解和图像标注等开放式任务上表现强劲，但是它们的空间推理能力不足，限制了它们在目标定位和计数方面的实用性。

    Large Vision-Language Models (VLMs) have demonstrated impressive performance on complex tasks involving visual input with natural language instructions. However, it remains unclear to what extent capabilities on natural images transfer to Earth observation (EO) data, which are predominantly satellite and aerial images less common in VLM training data. In this work, we propose a comprehensive benchmark to gauge the progress of VLMs toward being useful tools for EO data by assessing their abilities on scene understanding, localization and counting, and change detection tasks. Motivated by real-world applications, our benchmark includes scenarios like urban monitoring, disaster relief, land use, and conservation. We discover that, although state-of-the-art VLMs like GPT-4V possess extensive world knowledge that leads to strong performance on open-ended tasks like location understanding and image captioning, their poor spatial reasoning limits usefulness on object localization and counting
    
[^46]: 使用深度学习进行局部特征匹配：一项调查

    Local Feature Matching Using Deep Learning: A Survey

    [https://arxiv.org/abs/2401.17592](https://arxiv.org/abs/2401.17592)

    本文调查了使用深度学习进行局部特征匹配的方法。局部特征匹配在计算机视觉中的各个领域具有广泛应用，但是由于视角和光照变化等因素，匹配的准确性和鲁棒性仍然存在挑战。近年来，深度学习模型的引入使得局部特征匹配技术得到了广泛研究。本文对局部特征匹配方法进行了全面概述，并对之前的工作进行了评估。

    

    局部特征匹配在计算机视觉领域中具有广泛的应用，包括图像检索、三维重建和物体识别等领域。然而，由于视角和光照变化等因素，提高匹配的准确性和鲁棒性仍然面临挑战。近年来，深度学习模型的引入引发了局部特征匹配技术的广泛探索。本文旨在提供局部特征匹配方法的全面概述。这些方法基于是否存在检测器被分为两个主要类别。基于检测器的类别包括检测然后描述、联合检测和描述、描述然后检测以及基于图的技术。相反，不需要检测器的类别包括基于CNN的方法、基于Transformer的方法和基于块的方法。我们的研究超越了方法论分析，还包括对先前工作的评估。

    Local feature matching enjoys wide-ranging applications in the realm of computer vision, encompassing domains such as image retrieval, 3D reconstruction, and object recognition. However, challenges persist in improving the accuracy and robustness of matching due to factors like viewpoint and lighting variations. In recent years, the introduction of deep learning models has sparked widespread exploration into local feature matching techniques. The objective of this endeavor is to furnish a comprehensive overview of local feature matching methods. These methods are categorized into two key segments based on the presence of detectors. The Detector-based category encompasses models inclusive of Detect-then-Describe, Joint Detection and Description, Describe-then-Detect, as well as Graph Based techniques. In contrast, the Detector-free category comprises CNN Based, Transformer Based, and Patch Based methods. Our study extends beyond methodological analysis, incorporating evaluations of prev
    
[^47]: 传播与陷阱：通过反事实任务评估基于推理的知识编辑的困境

    Propagation and Pitfalls: Reasoning-based Assessment of Knowledge Editing through Counterfactual Tasks

    [https://arxiv.org/abs/2401.17585](https://arxiv.org/abs/2401.17585)

    该论文针对现有的知识编辑方法在推理能力方面的限制，通过引入ReCoE数据集进行了深入分析。研究发现所有的模型编辑方法在该数据集上表现较差，尤其在特定的推理方案中。此外，通过对编辑模型思维链生成的分析，揭示了现有方法的不足之处，包括对事实编辑、事实回忆能力和连贯性的考量。

    

    当前的知识编辑方法在有效传播更新的相互关联事实方面面临困难。在这项工作中，我们深入探讨了阻碍准确推理模型中更新知识适当传播的障碍。为了支持我们的分析，我们引入了一种新颖的基于推理的基准——ReCoE（基于推理的反事实编辑数据集），涵盖了现实世界中的六种常见推理方案。我们对现有的知识编辑技术进行了全面分析，包括输入增强、微调和定位编辑。我们发现所有模型编辑方法在这个数据集上的表现都明显较低，尤其是在某些推理方案中。我们通过对编辑模型的思维链生成的分析，从推理的角度揭示了现有知识编辑方法不足的关键原因，包括对事实编辑、事实回忆能力以及生成的连贯性的方面。我们将公开我们的基准数据集和代码。

    Current approaches of knowledge editing struggle to effectively propagate updates to interconnected facts. In this work, we delve into the barriers that hinder the appropriate propagation of updated knowledge within these models for accurate reasoning. To support our analysis, we introduce a novel reasoning-based benchmark -- ReCoE (Reasoning-based Counterfactual Editing dataset) -- which covers six common reasoning schemes in real world. We conduct a thorough analysis of existing knowledge editing techniques, including input augmentation, finetuning, and locate-and-edit. We found that all model editing methods show notably low performance on this dataset, especially in certain reasoning schemes. Our analysis over the chain-of-thought generation of edited models further uncover key reasons behind the inadequacy of existing knowledge editing methods from a reasoning standpoint, involving aspects on fact-wise editing, fact recall ability, and coherence in generation. We will make our ben
    
[^48]: 敏捷但安全：学习无碰撞高速四足机器人行走

    Agile But Safe: Learning Collision-Free High-Speed Legged Locomotion

    [https://arxiv.org/abs/2401.17583](https://arxiv.org/abs/2401.17583)

    本文介绍了一种名为敏捷但安全（ABS）的学习控制框架，能够实现四足机器人的敏捷且无碰撞行走。该框架通过一个学习得到的控制论到达-避免值网络来实现策略切换，并通过协作运行的敏捷策略和恢复策略，使机器人能够高速且安全地导航。

    

    在杂乱环境中行走的四足机器人必须既敏捷以提高任务执行效率，又要确保安全，避免与障碍物或人碰撞。现有的研究要么开发保守的控制器（速度小于1.0 m/s）以确保安全，要么专注于敏捷性而未考虑潜在致命的碰撞。本文介绍了敏捷但安全（ABS）的学习控制框架，为四足机器人实现了敏捷且无碰撞的行走。ABS包括一个敏捷策略来在障碍物中执行灵活的动作技能，并且有一个恢复策略来避免失败，共同实现高速且无碰撞的导航。ABS中的策略切换由一个学习得到的控制论到达-避免值网络控制，该网络也指导恢复策略作为目标函数，从而在闭环中保护机器人。训练过程涉及敏捷策略、到达-避免值网络、恢复策略和外感知表征的学习。

    Legged robots navigating cluttered environments must be jointly agile for efficient task execution and safe to avoid collisions with obstacles or humans. Existing studies either develop conservative controllers (< 1.0 m/s) to ensure safety, or focus on agility without considering potentially fatal collisions. This paper introduces Agile But Safe (ABS), a learning-based control framework that enables agile and collision-free locomotion for quadrupedal robots. ABS involves an agile policy to execute agile motor skills amidst obstacles and a recovery policy to prevent failures, collaboratively achieving high-speed and collision-free navigation. The policy switch in ABS is governed by a learned control-theoretic reach-avoid value network, which also guides the recovery policy as an objective function, thereby safeguarding the robot in a closed loop. The training process involves the learning of the agile policy, the reach-avoid value network, the recovery policy, and an exteroception repre
    
[^49]: 重新思考多元时间序列预测的通道相关性：从领先指标中学习

    Rethinking Channel Dependence for Multivariate Time Series Forecasting: Learning from Leading Indicators

    [https://arxiv.org/abs/2401.17548](https://arxiv.org/abs/2401.17548)

    本文提出了一种新方法，LIFT，通过利用通道相关性和领先指标，为多元时间序列预测提供准确的预测。LIFT方法可以无缝与任意时间序列预测方法协作，大量实验证明了其有效性。

    

    最近，独立于通道的方法在多元时间序列（MTS）预测中取得了最先进的性能。尽管这些方法减少了过拟合的风险，但它们错过了利用通道相关性进行准确预测的潜在机会。我们认为，在变量之间存在局部平稳的领先-滞后关系，即一些滞后变量在短时间内可能遵循领先指标。利用这种通道相关性是有益的，因为领先指标提供了先进信息，可以用来减少滞后变量的预测难度。在本文中，我们提出了一种名为LIFT的新方法，该方法首先在每个时间步骤高效地估计领先指标及其领先步骤，然后巧妙地允许滞后变量利用来自领先指标的先进信息。LIFT作为一个插件，可以与任意时间序列预测方法无缝协作。进行了大量实验证明了LIFT方法的有效性。

    Recently, channel-independent methods have achieved state-of-the-art performance in multivariate time series (MTS) forecasting. Despite reducing overfitting risks, these methods miss potential opportunities in utilizing channel dependence for accurate predictions. We argue that there exist locally stationary lead-lag relationships between variates, i.e., some lagged variates may follow the leading indicators within a short time period. Exploiting such channel dependence is beneficial since leading indicators offer advance information that can be used to reduce the forecasting difficulty of the lagged variates. In this paper, we propose a new method named LIFT that first efficiently estimates leading indicators and their leading steps at each time step and then judiciously allows the lagged variates to utilize the advance information from leading indicators. LIFT plays as a plugin that can be seamlessly collaborated with arbitrary time series forecasting methods. Extensive experiments o
    
[^50]: 数据有效学习：一项综合医学基准研究

    Data-Effective Learning: A Comprehensive Medical Benchmark

    [https://arxiv.org/abs/2401.17542](https://arxiv.org/abs/2401.17542)

    这项研究引入了一个综合基准，用于评估医学领域的数据有效学习。该基准包括大量的医疗数据样本、基准方法和新的评估指标，能够准确评估数据有效学习的性能。

    

    数据有效学习旨在以最有效的方式利用数据来训练AI模型，其涉及关注数据质量而非数量的策略，确保用于训练的数据具有高信息价值。数据有效学习在加快AI训练、减少计算成本和节省数据存储方面发挥着重要作用，这在近年来医学数据的数量超出了许多人的预期时尤为重要。然而，由于缺乏标准和综合的基准研究，医学领域的数据有效学习研究还不够深入。为了填补这一空白，本文引入了一个专门用于评估医学领域数据有效学习的综合基准。该基准包括来自31个医疗中心数百万个数据样本的数据集(DataDEL)，用于比较的基准方法(MedDEL)，以及一个用于客观衡量数据有效学习性能的新评估指标(NormDEL)。我们进行了广泛的实证实验和比较，证明了我们的基准在评估数据有效学习方面的有效性和适用性。

    Data-effective learning aims to use data in the most impactful way to train AI models, which involves strategies that focus on data quality rather than quantity, ensuring the data used for training has high informational value. Data-effective learning plays a profound role in accelerating AI training, reducing computational costs, and saving data storage, which is very important as the volume of medical data in recent years has grown beyond many people's expectations. However, due to the lack of standards and comprehensive benchmark, research on medical data-effective learning is poorly studied. To address this gap, our paper introduces a comprehensive benchmark specifically for evaluating data-effective learning in the medical field. This benchmark includes a dataset with millions of data samples from 31 medical centers (DataDEL), a baseline method for comparison (MedDEL), and a new evaluation metric (NormDEL) to objectively measure data-effective learning performance. Our extensive e
    
[^51]: 学习停止割生成以提高混合整数线性规划的效率

    Learning to Stop Cut Generation for Efficient Mixed-Integer Linear Programming

    [https://arxiv.org/abs/2401.17527](https://arxiv.org/abs/2401.17527)

    这篇论文提出了一种学习停止割生成的方法，以提高混合整数线性规划的效率。通过将问题形式化为强化学习问题，并使用混合图表示模型进行学习，该方法能够动态地决策何时停止割生成，并有效捕捉混合整数线性规划的动态和静态特征。

    

    切割平面在解决混合整数线性规划中起着重要作用，它们显著加紧了对偶界限并改善了求解性能。割生成停止问题是割的重要问题，对于提高混合整数线性规划的效率至关重要。然而，许多现代混合整数线性规划求解器使用硬编码启发式策略来解决这个问题，这往往忽视了来自某些应用中的混合整数线性规划的潜在模式。为了解决这个挑战，我们将割生成停止问题形式化为强化学习问题，并提出了一种新颖的混合图表示模型（HYGRO），以学习有效的停止策略。HYGRO的一个吸引人的特点是它可以有效捕捉到混合整数线性规划的动态和静态特征，从而实现对停止策略的动态决策。据我们所知，HYGRO是解决割生成停止问题的第一个数据驱动方法。通过将我们的方法与模型整合起来，我们在 实验证明了我们的方法的有效性。

    Cutting planes (cuts) play an important role in solving mixed-integer linear programs (MILPs), as they significantly tighten the dual bounds and improve the solving performance. A key problem for cuts is when to stop cuts generation, which is important for the efficiency of solving MILPs. However, many modern MILP solvers employ hard-coded heuristics to tackle this problem, which tends to neglect underlying patterns among MILPs from certain applications. To address this challenge, we formulate the cuts generation stopping problem as a reinforcement learning problem and propose a novel hybrid graph representation model (HYGRO) to learn effective stopping strategies. An appealing feature of HYGRO is that it can effectively capture both the dynamic and static features of MILPs, enabling dynamic decision-making for the stopping strategies. To the best of our knowledge, HYGRO is the first data-driven method to tackle the cuts generation stopping problem. By integrating our approach with mod
    
[^52]: 一个具有本地神经网络和有限元输入数据的PNP离子通道深度学习求解器

    A PNP ion channel deep learning solver with local neural network and finite element input data

    [https://arxiv.org/abs/2401.17513](https://arxiv.org/abs/2401.17513)

    该论文提出了一个使用本地神经网络和有限元求解器的PNP离子通道深度学习求解器。该求解器能够较快地训练并生成高准确度的数值解，在处理不同扰动情况和离子通道子区域时表现良好。

    

    本文提出了一种用于解决改进的一维Poisson-Nernst-Planck离子通道（PNPic）模型的深度学习方法，称为PNPic深度学习求解器。特别是，它将一种新颖的本地神经网络方案与有效的PNPic有限元求解器相结合。由于神经网络方案的输入数据只涉及粗网格解的一小块局部区域，有限元求解器可以快速生成，因此PNPic深度学习求解器的训练速度比任何对应的传统全局神经网络求解器都要快。经过适当训练，它可以输出一个比低成本粗网格解更高准确度的预测PNPic解，并能反映不同参数扰动情况、离子通道子区域以及界面和边界值等。因此，PNPic深度学习求解器可以为一类PNPic模型生成高准确度的数值解。作为一个初步研究，两个......

    In this paper, a deep learning method for solving an improved one-dimensional Poisson-Nernst-Planck ion channel (PNPic) model, called the PNPic deep learning solver, is presented. In particular, it combines a novel local neural network scheme with an effective PNPic finite element solver. Since the input data of the neural network scheme only involves a small local patch of coarse grid solutions, which the finite element solver can quickly produce, the PNPic deep learning solver can be trained much faster than any corresponding conventional global neural network solvers. After properly trained, it can output a predicted PNPic solution in a much higher degree of accuracy than the low cost coarse grid solutions and can reflect different perturbation cases on the parameters, ion channel subregions, and interface and boundary values, etc. Consequently, the PNPic deep learning solver can generate a numerical solution with high accuracy for a family of PNPic models. As an initial study, two 
    
[^53]: 在面向患者的风险预测模型中，语言表达不确定性的问题

    Linguistically Communicating Uncertainty in Patient-Facing Risk Prediction Models

    [https://arxiv.org/abs/2401.17511](https://arxiv.org/abs/2401.17511)

    本文讨论了在医疗保健领域中面向患者的风险预测模型中不确定性量化的挑战，并提出了一种设计来应对这些挑战，重点关注体外受精结果预测的具体应用。

    

    本文讨论了在医疗保健领域中，应用于面向患者环境中的人工智能模型中不确定性量化所面临的独特挑战。与为模型开发者或领域专家量身定制的传统可解释人工智能（XAI）方法不同，这里需要考虑自然语言的表达、展示和评估可理解性的附加因素。我们在风险预测的语境下识别了在自然语言中沟通模型性能、置信度、推理和未知已知的挑战。我们提出了一个旨在应对这些挑战的设计，重点关注体外受精结果预测的具体应用。

    This paper addresses the unique challenges associated with uncertainty quantification in AI models when applied to patient-facing contexts within healthcare. Unlike traditional eXplainable Artificial Intelligence (XAI) methods tailored for model developers or domain experts, additional considerations of communicating in natural language, its presentation and evaluating understandability are necessary. We identify the challenges in communication model performance, confidence, reasoning and unknown knowns using natural language in the context of risk prediction. We propose a design aimed at addressing these challenges, focusing on the specific application of in-vitro fertilisation outcome prediction.
    
[^54]: 大型语言模型中的时间箭头

    Arrows of Time for Large Language Models

    [https://arxiv.org/abs/2401.17505](https://arxiv.org/abs/2401.17505)

    这篇论文通过研究自回归大型语言模型的时间方向性，发现了模型在建模自然语言能力上存在时间上的不对称性。从信息理论的角度来看，这种差异理论上是不应该存在的。通过稀疏性和计算复杂性的考虑，提供了一个理论框架来解释这种不对称性的出现。

    

    我们通过时间方向性的视角研究了自回归大型语言模型的概率建模。我们在实证上发现这类模型在建模自然语言能力上存在时间上的不对称性：预测下一个记号和预测前一个记号时的平均对数困惑度存在差异。这种差异既微妙又在不同的模态（语言、模型大小、训练时间等）下非常一致。从信息理论的角度来看，这在理论上是令人惊讶的，不应该存在这样的差异。我们提供了一个理论框架，解释了这种不对称性如何出现在稀疏性和计算复杂性考虑中，并概述了我们的结果带来的一些展望。

    We study the probabilistic modeling performed by Autoregressive Large Language Models through the angle of time directionality. We empirically find a time asymmetry exhibited by such models in their ability to model natural language: a difference in the average log-perplexity when trying to predict the next token versus when trying to predict the previous one. This difference is at the same time subtle and very consistent across various modalities (language, model size, training time, ...). Theoretically, this is surprising: from an information-theoretic point of view, there should be no such difference. We provide a theoretical framework to explain how such an asymmetry can appear from sparsity and computational complexity considerations, and outline a number of perspectives opened by our results.
    
[^55]: LeTO：通过可微分轨迹优化学习受限视觉运动策略

    LeTO: Learning Constrained Visuomotor Policy with Differentiable Trajectory Optimization

    [https://arxiv.org/abs/2401.17500](https://arxiv.org/abs/2401.17500)

    LeTO 是一种通过可微分轨迹优化实现受限视觉运动策略的学习方法，它将优化层表示为轨迹优化问题，使模型能以安全可控的方式端到端生成动作。通过引入约束信息，实现了平衡满足约束、平滑轨迹和最小化演示误差的训练目标。在仿真和实际机器人中进行了评估，表明LeTO方法在成功率上与最先进的模仿学习方法相当。

    

    本文介绍了一种名为LeTO的方法，通过可微分轨迹优化实现受限视觉运动策略的学习。我们的方法独特地将一个可微分优化层整合到神经网络中。通过将优化层表示为一个轨迹优化问题，我们能够使模型以安全和可控的方式端到端生成动作，而无需额外的模块。我们的方法允许在训练过程中引入约束信息，从而平衡满足约束、平滑轨迹和最小化演示误差的训练目标。这种“灰盒”方法将基于优化的安全性和可解释性与神经网络的强大表达能力结合在一起。我们在仿真和实际机器人上对LeTO进行了定量评估。在仿真中，LeTO的成功率与最先进的模仿学习方法相当，但生成的轨迹的不一致性较小。

    This paper introduces LeTO, a method for learning constrained visuomotor policy via differentiable trajectory optimization. Our approach uniquely integrates a differentiable optimization layer into the neural network. By formulating the optimization layer as a trajectory optimization problem, we enable the model to end-to-end generate actions in a safe and controlled fashion without extra modules. Our method allows for the introduction of constraints information during the training process, thereby balancing the training objectives of satisfying constraints, smoothing the trajectories, and minimizing errors with demonstrations. This "gray box" method marries the optimization-based safety and interpretability with the powerful representational abilities of neural networks. We quantitatively evaluate LeTO in simulation and on the real robot. In simulation, LeTO achieves a success rate comparable to state-of-the-art imitation learning methods, but the generated trajectories are of less un
    
[^56]: 在社交媒体上检测心理障碍：基于ChatGPT的可解释方法

    Detecting mental disorder on social media: a ChatGPT-augmented explainable approach

    [https://arxiv.org/abs/2401.17477](https://arxiv.org/abs/2401.17477)

    本文提出了一种利用大型语言模型，可解释人工智能和对话代理器ChatGPT相结合的新方法，以解决通过社交媒体检测抑郁症的可解释性挑战。通过将Twitter特定变体BERTweet与自解释模型BERT-XDD相结合，并借助ChatGPT将技术解释转化为人类可读的评论，实现了解释能力的同时提高了可解释性。这种方法可以为发展社会负责任的数字平台，促进早期干预做出贡献。

    

    在数字时代，社交媒体上表达的抑郁症状的频率引起了严重关注，迫切需要先进的方法来及时检测。本文通过提出一种新颖的方法，将大型语言模型（LLM）与可解释的人工智能（XAI）和ChatGPT等对话代理器有效地结合起来，以应对可解释性抑郁症检测的挑战。在我们的方法中，通过将Twitter特定变体BERTweet与一种新型的自解释模型BERT-XDD相结合，实现了解释能力，该模型能够通过掩码注意力提供分类和解释。使用ChatGPT将技术解释转化为可读性强的评论，进一步增强了可解释性。通过引入一种有效且模块化的可解释抑郁症检测方法，我们的方法可以为发展社会负责任的数字平台做出贡献，促进早期干预。

    In the digital era, the prevalence of depressive symptoms expressed on social media has raised serious concerns, necessitating advanced methodologies for timely detection. This paper addresses the challenge of interpretable depression detection by proposing a novel methodology that effectively combines Large Language Models (LLMs) with eXplainable Artificial Intelligence (XAI) and conversational agents like ChatGPT. In our methodology, explanations are achieved by integrating BERTweet, a Twitter-specific variant of BERT, into a novel self-explanatory model, namely BERT-XDD, capable of providing both classification and explanations via masked attention. The interpretability is further enhanced using ChatGPT to transform technical explanations into human-readable commentaries. By introducing an effective and modular approach for interpretable depression detection, our methodology can contribute to the development of socially responsible digital platforms, fostering early intervention and
    
[^57]: 使用LLM智能体生成合成对话数据集

    Synthetic Dialogue Dataset Generation using LLM Agents

    [https://arxiv.org/abs/2401.17461](https://arxiv.org/abs/2401.17461)

    本文提出了使用LLM智能体生成合成对话数据集的方法，通过对话智能体和用户进行交流来获取生成线性模型所需的关键信息，并提出了对话的外部评估方法。

    

    线性规划问题在现实生活中应用广泛。然而，尽管表面上简单，但未经训练的用户可能难以确定其特定问题的线性模型。我们设想创建一个目标导向的对话智能体，与用户进行交流，以获取生成线性模型所需的所有信息，从而建立一个后续的智能体。在本文中，我们提出了一种用于生成可以用于开发和训练这样一个对话智能体的示例对话的方法。通过启发式设计，我们开发了两个相互“对话”的智能体，一个充当对话智能体，另一个充当用户。使用仅对用户可见的一组线性问题文本描述，智能体和用户进行对话，直到智能体从原始问题描述中获取到所有关键信息。我们还提出了对话的外部评估方法。

    Linear programming (LP) problems are pervasive in real-life applications. However, despite their apparent simplicity, an untrained user may find it difficult to determine the linear model of their specific problem. We envisage the creation of a goal-oriented conversational agent that will engage in conversation with the user to elicit all information required so that a subsequent agent can generate the linear model. In this paper, we present an approach for the generation of sample dialogues that can be used to develop and train such a conversational agent. Using prompt engineering, we develop two agents that "talk" to each other, one acting as the conversational agent, and the other acting as the user. Using a set of text descriptions of linear problems from NL4Opt available to the user only, the agent and the user engage in conversation until the agent has retrieved all key information from the original problem description. We also propose an extrinsic evaluation of the dialogues by 
    
[^58]: 使用大型语言模型在软件渗透测试中的初步研究

    A Preliminary Study on Using Large Language Models in Software Pentesting

    [https://arxiv.org/abs/2401.17459](https://arxiv.org/abs/2401.17459)

    本研究探索了在软件渗透测试中使用大型语言模型（LLM）的潜力，并证明了通过改进提示来提高模型准确性的可行性。

    

    大型语言模型（LLM）被认为具有自动化安全任务的潜在潜力，例如安全操作中心（SOC）中的任务。作为评估这种潜力的第一步，我们研究了在软件渗透测试中使用LLMs，其中主要任务是自动识别源代码中的软件安全漏洞。我们假设基于LLM的人工智能代理可以随着人员的互动而不断改进特定安全任务。作为第一步，通过根据生成的响应，将相关上下文和结构包含在LLM中的提示之中，来改进AI代理。如果这样的工程努力可以在当前任务上产生更好的结果，并且在未来的未知任务上也能产生更好的结果，则这样的工程努力将具有可持续性。为了检验这个假设，我们利用包含2,740个手工制作的测试用例的OWASP基准项目1.2进行研究。

    Large language models (LLM) are perceived to offer promising potentials for automating security tasks, such as those found in security operation centers (SOCs). As a first step towards evaluating this perceived potential, we investigate the use of LLMs in software pentesting, where the main task is to automatically identify software security vulnerabilities in source code. We hypothesize that an LLM-based AI agent can be improved over time for a specific security task as human operators interact with it. Such improvement can be made, as a first step, by engineering prompts fed to the LLM based on the responses produced, to include relevant contexts and structures so that the model provides more accurate results. Such engineering efforts become sustainable if the prompts that are engineered to produce better results on current tasks, also produce better results on future unknown tasks. To examine this hypothesis, we utilize the OWASP Benchmark Project 1.2 which contains 2,740 hand-craft
    
[^59]: 多尺度并行调温用于快速采样角色分割方案

    Multiscale Parallel Tempering for Fast Sampling on Redistricting Plans

    [https://arxiv.org/abs/2401.17455](https://arxiv.org/abs/2401.17455)

    本研究提出了一种多尺度并行调温方法，用于快速采样角色分割方案。该方法通过局部移动在不同尺度进行采样，可以满足各种基于政策的指标，并在康涅狄格州进行了评估。

    

    在审查一个角色分割方案时，一个具有说服力的方法是将该方案与供参考的中立角色分割方案集合进行比较。这些集合通过对平衡图分区进行分布采样算法生成。为了审查集合与给定方案之间的党派差异，必须确保匹配非党派标准，以便可以得出党派差异来自于偏见，而不是来自于紧凑性水平或社区保护的差异等。某些采样算法允许明确规定基于政策的方案概率分布，然而，这些算法在大型图（即角色分割空间）中的混合时间表现出较差，除了一些专门化的指标。在这项工作中，我们生成了一种多尺度并行调温方法，该方法在每个尺度上进行局部移动。局部移动使我们能够采用多种基于政策的指标。我们在康涅狄格州对我们的方法进行了评估。

    When auditing a redistricting plan, a persuasive method is to compare the plan with an ensemble of neutrally drawn redistricting plans. Ensembles are generated via algorithms that sample distributions on balanced graph partitions. To audit the partisan difference between the ensemble and a given plan, one must ensure that the non-partisan criteria are matched so that we may conclude that partisan differences come from bias rather than, for example, levels of compactness or differences in community preservation. Certain sampling algorithms allow one to explicitly state the policy-based probability distribution on plans, however, these algorithms have shown poor mixing times for large graphs (i.e. redistricting spaces) for all but a few specialized measures. In this work, we generate a multiscale parallel tempering approach that makes local moves at each scale. The local moves allow us to adopt a wide variety of policy-based measures. We examine our method in the state of Connecticut and
    
[^60]: 低成本集合剪枝的液态民主

    Liquid Democracy for Low-Cost Ensemble Pruning

    [https://arxiv.org/abs/2401.17443](https://arxiv.org/abs/2401.17443)

    本文介绍了一种利用液态民主实现低成本集合剪枝的方法。通过液态民主的委派机制识别和移除冗余分类器，成功降低了集合训练的计算成本，并比某些增强方法具有更高的准确性。同时，本文还展示了计算社会选择文献框架在非传统领域问题中的应用。

    

    我们认为，集合学习和一种代理投票模式——液态民主之间存在着强烈的联系，可以通过液态民主来降低集合训练的成本。我们提出了一种增量训练的过程，通过液态民主的启发，通过委派机制来识别和移除集合中的冗余分类器。通过分析和大量实验，我们展示了这个过程大大降低了训练的计算成本，相比于训练一个完整的集合。通过精选底层的委派机制，避免了分类器群体的权重集中，提高了准确性，而且，这项工作也展示了计算社会选择文献中的框架如何应用于非传统领域的问题。

    We argue that there is a strong connection between ensemble learning and a delegative voting paradigm -- liquid democracy -- that can be leveraged to reduce ensemble training costs. We present an incremental training procedure that identifies and removes redundant classifiers from an ensemble via delegation mechanisms inspired by liquid democracy. Through both analysis and extensive experiments we show that this process greatly reduces the computational cost of training compared to training a full ensemble. By carefully selecting the underlying delegation mechanism, weight centralization in the classifier population is avoided, leading to higher accuracy than some boosting methods. Furthermore, this work serves as an exemplar of how frameworks from computational social choice literature can be applied to problems in nontraditional domains.
    
[^61]: 揭示二阶影响来解释预测不确定性

    Explaining Predictive Uncertainty by Exposing Second-Order Effects

    [https://arxiv.org/abs/2401.17441](https://arxiv.org/abs/2401.17441)

    该论文研究发现，预测不确定性主要受到单个特征或特征之间乘积相互作用的二阶影响的影响。作者提出了一种基于这些二阶影响来解释预测不确定性的新方法。该方法通过简单的协方差计算对一阶解释进行处理，可以将常见的归因技术转化为强大的二阶不确定性解释器。作者通过量化评估验证了该方法解释的准确性，并展示了整体实用性。

    

    可解释的人工智能（Explainable AI）使复杂的机器学习黑箱变得透明，特别是可以确定模型用来进行预测的特征。然而，关于解释预测不确定性，即为什么模型“不确定”，目前研究较少。我们的研究发现，预测不确定性主要由涉及单个特征或特征之间的乘积相互作用的二阶影响所主导。我们提出了一种基于这些二阶影响来解释预测不确定性的新方法。在计算上，我们的方法简化成对一组一阶解释进行简单协方差计算。我们的方法具有普遍适用性，可以将常见的归因技术（LRP，Gradient x Input等）转化为强大的二阶不确定性解释器，称为CovLRP，CovGI等。我们通过系统量化评估验证了我们方法产生解释的准确性，展示了我们方法的整体实用性。

    Explainable AI has brought transparency into complex ML blackboxes, enabling, in particular, to identify which features these models use for their predictions. So far, the question of explaining predictive uncertainty, i.e. why a model 'doubts', has been scarcely studied. Our investigation reveals that predictive uncertainty is dominated by second-order effects, involving single features or product interactions between them. We contribute a new method for explaining predictive uncertainty based on these second-order effects. Computationally, our method reduces to a simple covariance computation over a collection of first-order explanations. Our method is generally applicable, allowing for turning common attribution techniques (LRP, Gradient x Input, etc.) into powerful second-order uncertainty explainers, which we call CovLRP, CovGI, etc. The accuracy of the explanations our method produces is demonstrated through systematic quantitative evaluations, and the overall usefulness of our m
    
[^62]: 移动拼图游戏中的难度建模：对玩家分析和模拟数据结合的不同方法进行实证研究

    Difficulty Modelling in Mobile Puzzle Games: An Empirical Study on Different Methods to Combine Player Analytics and Simulated Data

    [https://arxiv.org/abs/2401.17436](https://arxiv.org/abs/2401.17436)

    本文通过对不同方法结合玩家分析和模拟数据的实证研究，揭示了在移动拼图游戏中难度的准确估算方法。结果显示，使用基于队列统计和模拟数据训练的人工神经网络模型能够在各种场景下产生最准确的估算结果。

    

    难度是玩家参与度的关键驱动因素之一，同时也是设计师优化玩家体验的重要方面之一；因此，对游戏开发工作室来说，难度操作是一项关键任务。一种常见做法是根据玩家与内容的互动收集的数据来创建度量标准；然而，这种方法仅允许在内容发布后进行估算，并且不考虑潜在未来玩家的特征。本文提出了一些潜在解决方案，可以在这种条件下估算难度，并展示了一项比较研究的结果，旨在了解不同场景下哪种方法和哪种类型的数据表现更好。结果表明，在所有场景中，基于队列统计和模拟数据训练的模型能够产生最准确的难度估算。此外，在这些模型中，人工神经网络表现最为一致。

    Difficulty is one of the key drivers of player engagement and it is often one of the aspects that designers tweak most to optimise the player experience; operationalising it is, therefore, a crucial task for game development studios. A common practice consists of creating metrics out of data collected by player interactions with the content; however, this allows for estimation only after the content is released and does not consider the characteristics of potential future players.   In this article, we present a number of potential solutions for the estimation of difficulty under such conditions, and we showcase the results of a comparative study intended to understand which method and which types of data perform better in different scenarios.   The results reveal that models trained on a combination of cohort statistics and simulated data produce the most accurate estimations of difficulty in all scenarios. Furthermore, among these models, artificial neural networks show the most cons
    
[^63]: 大型语言模型能否取代经济选择预测实验室？

    Can Large Language Models Replace Economic Choice Prediction Labs?

    [https://arxiv.org/abs/2401.17435](https://arxiv.org/abs/2401.17435)

    该论文研究大型语言模型是否能够取代经济实验室进行选择预测，并通过相关实验证明了其可行性。

    

    经济选择预测是一项具有挑战性的重要任务，往往受限于获取人类选择数据的困难。实验经济学研究在很大程度上专注于简单的选择环境。最近，人工智能界以两种方式为该努力做出了贡献：考虑大型语言模型是否可以代替人类在上述简单选择预测环境中，以及通过机器学习视角研究更复杂但仍严格的实验经济学环境，包括不完全信息、重复博弈和基于自然语言交流的说服游戏。这引发了一个重要的灵感：大型语言模型是否能够完全模拟经济环境，并生成用于高效人类选择预测的数据，替代复杂的经济实验室研究？我们在这个主题上开创了研究，并展示了其可行性。特别是，我们表明仅在大型语言模型生成的数据上训练的模型可以有效地进行预测。

    Economic choice prediction is an essential challenging task, often constrained by the difficulties in acquiring human choice data. Indeed, experimental economics studies had focused mostly on simple choice settings. The AI community has recently contributed to that effort in two ways: considering whether LLMs can substitute for humans in the above-mentioned simple choice prediction settings, and the study through ML lens of more elaborated but still rigorous experimental economics settings, employing incomplete information, repetitive play, and natural language communication, notably language-based persuasion games. This leaves us with a major inspiration: can LLMs be used to fully simulate the economic environment and generate data for efficient human choice prediction, substituting for the elaborated economic lab studies? We pioneer the study of this subject, demonstrating its feasibility. In particular, we show that a model trained solely on LLM-generated data can effectively predic
    
[^64]: 在黑客马拉松中集成生成式人工智能: 机遇，挑战和教育影响

    Integrating Generative AI in Hackathons: Opportunities, Challenges, and Educational Implications

    [https://arxiv.org/abs/2401.17434](https://arxiv.org/abs/2401.17434)

    引入生成式人工智能的黑客马拉松在软件行业中发挥重要作用，并在教育领域带来了机遇和挑战。

    

    黑客马拉松和软件竞赛在软件行业中变得越来越重要，它们对组织和学生的创新和技能发展起到重要推动作用。这些平台使公司能够迅速原型化想法，而学生则获得丰富的学习经验，增强他们的实践技能。多年来，黑客马拉松已经从简单的竞争活动转变为重要的教育工具，将理论知识与实际问题解决相结合。将黑客马拉松纳入计算机科学和软件工程课程的整合旨在在合作的环境中对齐教育能力，通过产学合作促进同行之间的连接和丰富学习。然而，高级技术，特别是人工智能（AI）和机器学习的融合进黑客马拉松正在改变它们的结构和结果。这种演变带来了机遇，如增强的学习体验，

    Hackathons and software competitions, increasingly pivotal in the software industry, serve as vital catalysts for innovation and skill development for both organizations and students. These platforms enable companies to prototype ideas swiftly, while students gain enriched learning experiences, enhancing their practical skills. Over the years, hackathons have transitioned from mere competitive events to significant educational tools, fusing theoretical knowledge with real-world problem-solving. The integration of hackathons into computer science and software engineering curricula aims to align educational proficiencies within a collaborative context, promoting peer connectivity and enriched learning via industry-academia collaborations. However, the infusion of advanced technologies, notably artificial intelligence (AI), and machine learning, into hackathons is revolutionizing their structure and outcomes. This evolution brings forth both opportunities, like enhanced learning experienc
    
[^65]: 多头注意力在上下文线性回归中的优势

    Superiority of Multi-Head Attention in In-Context Linear Regression

    [https://arxiv.org/abs/2401.17426](https://arxiv.org/abs/2401.17426)

    多头注意力在上下文线性回归任务中表现出优于单头注意力的性能，通过理论分析证明了多头注意力在大嵌入维度情况下有更小的预测损失，并且在各种数据分布设置下都显示出优势。

    

    我们通过理论分析在线性回归任务的上下文学习中，使用softmax注意力的transformer的性能。与现有文献主要关注单头/多头注意力的收敛性不同，我们的研究着重比较它们的性能。我们进行了精确的理论分析，证明了具有较大嵌入维度的多头注意力比单头注意力表现更好。当上下文示例数量D增加时，使用单头/多头注意力的预测损失为O(1/D)，而多头注意力的乘法常数较小。除了最简单的数据分布设置，我们考虑了更多情景，例如噪声标签，局部示例，相关特征和先验知识。我们观察到，总的来说，多头注意力优于单头注意力。我们的结果验证了多头注意力设计的有效性。

    We present a theoretical analysis of the performance of transformer with softmax attention in in-context learning with linear regression tasks. While the existing literature predominantly focuses on the convergence of transformers with single-/multi-head attention, our research centers on comparing their performance. We conduct an exact theoretical analysis to demonstrate that multi-head attention with a substantial embedding dimension performs better than single-head attention. When the number of in-context examples D increases, the prediction loss using single-/multi-head attention is in O(1/D), and the one for multi-head attention has a smaller multiplicative constant. In addition to the simplest data distribution setting, we consider more scenarios, e.g., noisy labels, local examples, correlated features, and prior knowledge. We observe that, in general, multi-head attention is preferred over single-head attention. Our results verify the effectiveness of the design of multi-head at
    
[^66]: 应用神经网络重建快速中微子换 flavor 后的能谱

    Application of Neural Networks for the Reconstruction of Supernova Neutrino Energy Spectra Following Fast Neutrino Flavor Conversions

    [https://arxiv.org/abs/2401.17424](https://arxiv.org/abs/2401.17424)

    本研究利用物理信息驱动的神经网络（PINNs），基于多能量中微子气体中中微子角分布的前两个矩，预测了快速 flavor 转换的结果，取得了较低的预测误差。

    

    中微子在极密度的天体环境中，如核心坍缩超新星和中子星合并中，可以进行快速的 flavor 转换。本研究探索了多能量中微子气体中的快速 flavor 转换，发现当快速 flavor 转换的增长率显著超过真空哈密顿量的增长率时，所有中微子（无论其能量如何）都共享一个由能量积分中微子谱决定的存活概率。然后，我们使用物理信息驱动的神经网络（PINNs）来预测在这种多能量中微子气体中快速 flavor 转换的渐近结果。这些预测基于每个能量 bin 的中微子角分布的前两个矩，通常可以从最先进的超新星和中子星模拟中获得。我们的 PINNs 对于预测电子通道中的中微子数量和中微子矩的相对绝对误差分别达到不到6％和不到18％的误差。

    Neutrinos can undergo fast flavor conversions (FFCs) within extremely dense astrophysical environments such as core-collapse supernovae (CCSNe) and neutron star mergers (NSMs). In this study, we explore FFCs in a \emph{multi-energy} neutrino gas, revealing that when the FFC growth rate significantly exceeds that of the vacuum Hamiltonian, all neutrinos (regardless of energy) share a common survival probability dictated by the energy-integrated neutrino spectrum. We then employ physics-informed neural networks (PINNs) to predict the asymptotic outcomes of FFCs within such a multi-energy neutrino gas. These predictions are based on the first two moments of neutrino angular distributions for each energy bin, typically available in state-of-the-art CCSN and NSM simulations. Our PINNs achieve errors as low as $\lesssim6\%$ and $\lesssim 18\%$ for predicting the number of neutrinos in the electron channel and the relative absolute error in the neutrino moments, respectively.
    
[^67]: 基于WiFi信道状态信息的穿墙成像

    Through-Wall Imaging based on WiFi Channel State Information

    [https://arxiv.org/abs/2401.17417](https://arxiv.org/abs/2401.17417)

    本研究提出了一种通过WiFi信道状态信息实现穿墙成像的创新方法，可以将室内环境可视化监测到房间边界之外，无需摄像机，具有广泛的实际应用潜力。

    

    本研究提出了一种创新的方法，通过WiFi信道状态信息（CSI）在穿墙场景中合成图像。利用WiFi的优势，如成本效益，光照不变性和穿墙能力，我们的方法实现了对室内环境的可视化监测，越过房间边界，无需摄像机。更一般地，它通过解锁执行基于图像的下游任务（例如，视觉活动识别）的选项，提高了WiFi CSI的可解释性。为了实现从WiFi CSI到图像的跨模态转换，我们依赖于一个适应我们问题特定的多模态变分自编码器（VAE）。我们通过架构配置的剔除研究和重建图像的定量/定性评估对我们提出的方法进行了广泛评估。我们的结果证明了我们方法的可行性，并突显了其在实际应用中的潜力。

    This work presents a seminal approach for synthesizing images from WiFi Channel State Information (CSI) in through-wall scenarios. Leveraging the strengths of WiFi, such as cost-effectiveness, illumination invariance, and wall-penetrating capabilities, our approach enables visual monitoring of indoor environments beyond room boundaries and without the need for cameras. More generally, it improves the interpretability of WiFi CSI by unlocking the option to perform image-based downstream tasks, e.g., visual activity recognition. In order to achieve this crossmodal translation from WiFi CSI to images, we rely on a multimodal Variational Autoencoder (VAE) adapted to our problem specifics. We extensively evaluate our proposed methodology through an ablation study on architecture configuration and a quantitative/qualitative assessment of reconstructed images. Our results demonstrate the viability of our method and highlight its potential for practical applications.
    
[^68]: 连续学习的步长优化

    Step-size Optimization for Continual Learning

    [https://arxiv.org/abs/2401.17401](https://arxiv.org/abs/2401.17401)

    本文研究了连续学习中的步长优化问题，指出传统算法忽视了对整体目标函数的影响，而随机元梯度下降算法能够明确优化步长向量，在简单问题中表现更优。

    

    在连续学习中，学习者需要在整个生命周期内不断学习数据。一个关键问题是决定要保留什么知识和放弃什么知识。在神经网络中，可以通过使用步长向量来缩放梯度样本对网络权重的改变程度来实现。常见的算法，如RMSProp和Adam，使用启发式方法，特别是标准化，来适应这个步长向量。在本文中，我们展示了这些启发式方法忽视了它们对整体目标函数的适应效果，例如将步长向量远离更好的步长向量。另一方面，像IDBD（Sutton，1992）这样的随机元梯度下降算法，明确地针对整体目标函数优化步长向量。在简单问题上，我们展示了IDBD能够持续改善步长向量，而RMSProp和Adam则不行。我们解释了这两种方法以及它们各自的局限性之间的差异。

    In continual learning, a learner has to keep learning from the data over its whole life time. A key issue is to decide what knowledge to keep and what knowledge to let go. In a neural network, this can be implemented by using a step-size vector to scale how much gradient samples change network weights. Common algorithms, like RMSProp and Adam, use heuristics, specifically normalization, to adapt this step-size vector. In this paper, we show that those heuristics ignore the effect of their adaptation on the overall objective function, for example by moving the step-size vector away from better step-size vectors. On the other hand, stochastic meta-gradient descent algorithms, like IDBD (Sutton, 1992), explicitly optimize the step-size vector with respect to the overall objective function. On simple problems, we show that IDBD is able to consistently improve step-size vectors, where RMSProp and Adam do not. We explain the differences between the two approaches and their respective limitat
    
[^69]: 对土耳其语理解任务进行基于Transformer编码器的微调

    Fine-tuning Transformer-based Encoder for Turkish Language Understanding Tasks

    [https://arxiv.org/abs/2401.17396](https://arxiv.org/abs/2401.17396)

    在本研究中，我们提供了一个基于Transformer的模型和一个土耳其语基准测试，成功地对名为BERTurk的土耳其BERT模型进行了微调，实现了许多下游任务的理解和评估。

    

    深度学习和最近的Transformer语言模型在自然语言处理领域的研究中占据主导地位。由于其准确和快速的微调特性，它们已经超越了传统的基于机器学习的方法，并在许多具有挑战性的自然语言理解（NLU）问题上取得了最先进的结果。最近的研究表明，基于Transformer的模型，如BERT（双向Transformer编码器表示），在许多任务上取得了令人瞩目的成果。此外，由于它们的迁移学习能力，这些架构允许我们将预先构建好的模型转移并针对特定的NLU任务进行微调，如问答。在这项研究中，我们为土耳其语提供了一个基于Transformer的模型和一个基准测试。我们成功地对一种名为BERTurk的土耳其BERT模型进行了微调，该模型是使用基本设置进行训练的，并用于许多下游任务进行了评估。

    Deep learning-based and lately Transformer-based language models have been dominating the studies of natural language processing in the last years. Thanks to their accurate and fast fine-tuning characteristics, they have outperformed traditional machine learning-based approaches and achieved state-of-the-art results for many challenging natural language understanding (NLU) problems. Recent studies showed that the Transformer-based models such as BERT, which is Bidirectional Encoder Representations from Transformers, have reached impressive achievements on many tasks. Moreover, thanks to their transfer learning capacity, these architectures allow us to transfer pre-built models and fine-tune them to specific NLU tasks such as question answering. In this study, we provide a Transformer-based model and a baseline benchmark for the Turkish Language. We successfully fine-tuned a Turkish BERT model, namely BERTurk that is trained with base settings, to many downstream tasks and evaluated wit
    
[^70]: 使用对比式上下文学习定制语言模型的回复

    Customizing Language Model Responses with Contrastive In-Context Learning

    [https://arxiv.org/abs/2401.17390](https://arxiv.org/abs/2401.17390)

    本论文提出了一种使用对比示例来定制语言模型回复的方法，通过提供正面示例和负面示例，使模型学会如何回避负面特征，从而更好地满足用户需求。

    

    大型语言模型 (LLMs) 对于机器学习应用变得越来越重要。然而，将LLMs与我们的意图对齐可能会具有挑战性，特别是当我们希望生成优于其他内容的内容，或者当我们希望LLMs以一种难以描述的风格或语气进行回应时。为了解决这个问题，我们提出了一种使用对比示例来更好地描述我们的意图的方法。这涉及提供正面示例来说明真实的意图，以及负面示例来展示我们希望LLMs避免的特征。负面示例可以从标记数据中检索，由人工编写，或由LLMs自动生成。在生成答案之前，我们要求模型分析这些示例，以教会自己避免什么。这个推理步骤为模型提供了与用户需求相关的适当表达，并引导其生成更好的答案。我们在合成和真实数据上测试了我们的方法。

    Large language models (LLMs) are becoming increasingly important for machine learning applications. However, it can be challenging to align LLMs with our intent, particularly when we want to generate content that is preferable over others or when we want the LLM to respond in a certain style or tone that is hard to describe. To address this challenge, we propose an approach that uses contrastive examples to better describe our intent. This involves providing positive examples that illustrate the true intent, along with negative examples that show what characteristics we want LLMs to avoid. The negative examples can be retrieved from labeled data, written by a human, or generated by the LLM itself. Before generating an answer, we ask the model to analyze the examples to teach itself what to avoid. This reasoning step provides the model with the appropriate articulation of the user's need and guides it towards generting a better answer. We tested our approach on both synthesized and real
    
[^71]: 无限-gram：将无限n-gram语言模型扩展到万亿标记

    Infini-gram: Scaling Unbounded n-gram Language Models to a Trillion Tokens

    [https://arxiv.org/abs/2401.17377](https://arxiv.org/abs/2401.17377)

    这项研究展示了n-gram语言模型的价值，并介绍了一个名为infini-gram的引擎，它可以以毫秒级的延迟计算任意n的n-gram概率，使得在神经大型语言模型中对文本进行更准确的分析成为可能。

    

    在神经大型语言模型（LLM）时代，n-gram语言模型还具有相关性吗？我们的答案是肯定的，并且我们展示了它们在文本分析和改进神经LLM方面的价值。然而，这需要在两个方面对n-gram模型进行现代化。首先，我们将它们与神经LLM相同的数据规模训练- 1.4万亿个标记。这是迄今为止构建的最大的n-gram模型。其次，现有的n-gram模型使用的n很小，这妨碍了它们的性能；相反，我们允许n可以是任意大的，通过引入一个新的无限-gram LM与回退。我们开发了一个名为infini-gram的引擎，它可以通过后缀数组计算无限-gram（以及任意n的n-gram）概率，并且具有毫秒级的延迟，而无需预先计算n-gram计数表（这将非常昂贵）。无限-gram框架和infini-gram引擎使我们能够对人类写作和机器生成的文本进行许多新颖和有意思的分析：我们发现无限-gram LM...

    Are n-gram language models still relevant in this era of neural large language models (LLMs)? Our answer is yes, and we show their values in both text analysis and improving neural LLMs. Yet this necessitates modernizing n-gram models in two aspects. First, we train them at the same data scale as neural LLMs -- 1.4 trillion tokens. This is the largest n-gram model ever built. Second, existing n-gram models use small n which hinders their performance; we instead allow n to be arbitrarily large, by introducing a new $\infty$-gram LM with backoff. Instead of pre-computing n-gram count tables (which would be very expensive), we develop an engine named infini-gram -- powered by suffix arrays -- that can compute $\infty$-gram (as well as n-gram with arbitrary n) probabilities with millisecond-level latency. The $\infty$-gram framework and infini-gram engine enable us to conduct many novel and interesting analyses of human-written and machine-generated text: we find that the $\infty$-gram LM 
    
[^72]: 阿拉伯推文行为：基于加权集成预训练Transformer模型的阿拉伯语推特语言行为分类

    Arabic Tweet Act: A Weighted Ensemble Pre-Trained Transformer Model for Classifying Arabic Speech Acts on Twitter

    [https://arxiv.org/abs/2401.17373](https://arxiv.org/abs/2401.17373)

    本文提出了一种用于阿拉伯语推特语言行为分类的加权集成预训练Transformer模型。通过整合不同的BERT模型，我们实现了对阿拉伯方言的精确分类，为理解用户观点和态度提供了有力的工具。

    

    语言行为是说话者在对话中表达意思时的行为，例如询问、推荐、问候、道谢、表达想法或提出建议。理解语言行为有助于解释说话者或作者言语背后的意图和行为。本文提出了一种基于Transformer深度学习神经网络的阿拉伯语推特语言行为分类方法。推特和社交媒体越来越融入日常生活。因此，它们已经演变成了表达用户观点和态度的重要信息来源。我们提出了一种基于BERT的加权集成学习方法，以整合方言阿拉伯语语言行为分类中各种BERT模型的优势。我们将所提出的模型与几个阿拉伯BERT模型和基于序列的模型进行了比较。通过对现有大型数据集的子集进行注释，我们开发了一个方言阿拉伯推特行为数据集。

    Speech acts are a speakers actions when performing an utterance within a conversation, such as asking, recommending, greeting, or thanking someone, expressing a thought, or making a suggestion. Understanding speech acts helps interpret the intended meaning and actions behind a speakers or writers words. This paper proposes a Twitter dialectal Arabic speech act classification approach based on a transformer deep learning neural network. Twitter and social media, are becoming more and more integrated into daily life. As a result, they have evolved into a vital source of information that represents the views and attitudes of their users. We proposed a BERT based weighted ensemble learning approach to integrate the advantages of various BERT models in dialectal Arabic speech acts classification. We compared the proposed model against several variants of Arabic BERT models and sequence-based models. We developed a dialectal Arabic tweet act dataset by annotating a subset of a large existing
    
[^73]: 通过深度黑石贝莱曼模型优化时间序列供应商分配风险

    Timeseries Suppliers Allocation Risk Optimization via Deep Black Litterman Model

    [https://arxiv.org/abs/2401.17350](https://arxiv.org/abs/2401.17350)

    通过深度黑石贝莱曼模型和时空图神经网络，我们优化了供应商选择和订单分配，同时解决了零阶情况下的可信度问题，实现了准确的预测和精确的置信区间。

    

    我们介绍了BL模型和Perspective矩阵，以优化供应商选择和订单分配，重点关注时间和空间动态。我们使用时空图神经网络开发了供应商关系网络，增强了对复杂供应商相互依赖关系的理解。此外，我们还通过Masked Ranking机制解决了零阶情况下的可信度问题，提高了供应商排序效率。与传统模型相比，我们的模型在两个数据集上展现了优越的结果。我们使用真实数据集进行的评估突出了DBLM在提供准确预测和精确置信区间方面的优势，特别是在高分辨率情景下。

    We introduce the BL model and the Perspective Matrix to optimize supplier selection and order allocation, focusing on both temporal and spatial dynamics. Our development of a Supplier Relationship Network, using a Spatio-Temporal Graph Neural Network, enhances the understanding of complex supplier interdependencies. Additionally, we address credibility issues in zero-order scenarios with a Masked Ranking Mechanism, improving supplier ranking efficiency. Our model demonstrates superior results on two datasets compared to the traditional models. Our evaluations using real-world datasets highlight DBLM's superiority in providing accurate predictions and precise confidence intervals, particularly in high-resolution scenarios.
    
[^74]: YTCommentQA: 指导视频中的问题可回答性

    YTCommentQA: Video Question Answerability in Instructional Videos

    [https://arxiv.org/abs/2401.17343](https://arxiv.org/abs/2401.17343)

    本研究提出了YTCommentQA数据集，通过收集来自YouTube的自然问题，分类其可回答性和所需模态，以解决指导视频中的问题可回答性问题。

    

    指导视频为各种任务提供了详细的如何操作指南，观众通常会就内容提出问题。解答这些问题对于理解内容至关重要，但是很难立即获得答案。目前已经开发了许多用于视频问题回答（Video QA）任务的计算模型，但它们主要是根据视频内容生成问题，旨在从内容中产生答案。然而，在现实世界的情况下，用户可能提出超出视频信息边界的问题，这突显出确定视频是否能提供答案的必要性。由于视频具有多模态性，视觉和口头信息交织在一起，因此判断一个问题是否可以通过视频内容回答是具有挑战性的。为了弥补这一差距，我们提出了YTCommentQA数据集，其中包含从YouTube生成的自然问题，根据其可回答性和所需模态进行了分类。

    Instructional videos provide detailed how-to guides for various tasks, with viewers often posing questions regarding the content. Addressing these questions is vital for comprehending the content, yet receiving immediate answers is difficult. While numerous computational models have been developed for Video Question Answering (Video QA) tasks, they are primarily trained on questions generated based on video content, aiming to produce answers from within the content. However, in real-world situations, users may pose questions that go beyond the video's informational boundaries, highlighting the necessity to determine if a video can provide the answer. Discerning whether a question can be answered by video content is challenging due to the multi-modal nature of videos, where visual and verbal information are intertwined. To bridge this gap, we present the YTCommentQA dataset, which contains naturally-generated questions from YouTube, categorized by their answerability and required modali
    
[^75]: 提高地球观测数据预测置信度的潜在空间度量

    A Latent Space Metric for Enhancing Prediction Confidence in Earth Observation Data

    [https://arxiv.org/abs/2401.17342](https://arxiv.org/abs/2401.17342)

    这项研究提出了一种新的方法来估计利用地球观测数据进行回归任务时机器学习模型预测的置信度。通过利用潜在空间表示来推导置信度度量，建立了潜在表示中的欧几里得距离与个体蚊子种群预测的绝对误差之间的相关性。该方法在意大利威尼托地区和德国上莱茵河谷的地区得到了验证，并表现出较高的可靠性和可信度。

    

    本研究提出了一种新的方法，用于估计机器学习模型预测的置信度，特别是在利用地球观测数据进行回归任务时，重点关注蚊子种群（MA）估计。我们利用变分自动编码器的架构，通过EO数据的潜在空间表示来推导置信度度量。这种方法对于建立潜在表示中的欧几里得距离与个体MA预测的绝对误差（AE）之间的相关性至关重要。我们的研究重点关注了意大利威尼托地区和德国上莱茵河谷的EO数据集，这些地区受蚊子种群的影响显著。一个重要的发现是MA预测的AE与所提出的置信度度量之间存在0.46的显著相关性。这个相关性意味着这是一个稳健的、新的度量方法，用于量化AI模型在该背景下的预测可靠性和提高可信度。

    This study presents a new approach for estimating confidence in machine learning model predictions, specifically in regression tasks utilizing Earth Observation (EO) data, with a particular focus on mosquito abundance (MA) estimation. We take advantage of a Variational AutoEncoder architecture, to derive a confidence metric by the latent space representations of EO datasets. This methodology is pivotal in establishing a correlation between the Euclidean distance in latent representations and the Absolute Error (AE) in individual MA predictions. Our research focuses on EO datasets from the Veneto region in Italy and the Upper Rhine Valley in Germany, targeting areas significantly affected by mosquito populations. A key finding is a notable correlation of 0.46 between the AE of MA predictions and the proposed confidence metric. This correlation signifies a robust, new metric for quantifying the reliability and enhancing the trustworthiness of the AI model's predictions in the context of 
    
[^76]: 去中心化联邦学习：安全与隐私综述

    Decentralized Federated Learning: A Survey on Security and Privacy

    [https://arxiv.org/abs/2401.17319](https://arxiv.org/abs/2401.17319)

    去中心化联邦学习架构允许保护隐私，但也引入了新的安全和隐私威胁，该综述对去中心化联邦学习中的威胁、对手和防御机制进行了研究。

    

    联邦学习由于其保护隐私等优势，在近年来迅速发展并受到广泛关注。然而，在这种架构中，模型更新和梯度的交换为网络中的恶意用户提供了新的攻击面，可能危及模型性能以及用户和数据的隐私。因此，去中心化联邦学习的主要动机之一是通过去除服务器并通过区块链等技术进行补偿来消除与服务器相关的威胁。然而，这种优势却以挑战系统面临新的隐私威胁为代价。因此，对这种新范 paradigm，并进行全面的安全分析是必要的。这项调查研究了去中心化联邦学习中可能存在的威胁和对手变化，并概述了潜在的防御机制。还考虑了去中心化联邦学习的可信度和验证性。

    Federated learning has been rapidly evolving and gaining popularity in recent years due to its privacy-preserving features, among other advantages. Nevertheless, the exchange of model updates and gradients in this architecture provides new attack surfaces for malicious users of the network which may jeopardize the model performance and user and data privacy. For this reason, one of the main motivations for decentralized federated learning is to eliminate server-related threats by removing the server from the network and compensating for it through technologies such as blockchain. However, this advantage comes at the cost of challenging the system with new privacy threats. Thus, performing a thorough security analysis in this new paradigm is necessary. This survey studies possible variations of threats and adversaries in decentralized federated learning and overviews the potential defense mechanisms. Trustability and verifiability of decentralized federated learning are also considered 
    
[^77]: HEQuant: 结合同态加密和量化以实现通信高效的私有推断

    HEQuant: Marrying Homomorphic Encryption and Quantization for Communication-Efficient Private Inference

    [https://arxiv.org/abs/2401.15970](https://arxiv.org/abs/2401.15970)

    HEQuant提出了一种结合同态加密和量化的方法，以实现通信高效的私有推断。通过低精度量化感知优化和一系列其他优化措施，HEQuant相对于先前的HE协议有效地降低了数据传输的数量和精度。

    

    带有同态加密（HE）的安全双方计算使用形式化的安全保证保护数据隐私，但通信开销较高。虽然先前的研究，如Cheetah、Iron等，已经提出了用于不同神经网络（NN）操作的高效HE协议，但它们仍假设NN操作需要高精度，例如固定点37位，并忽视了NN对量化误差的固有鲁棒性。在本文中，我们提出了HEQuant，它具有针对HE协议的低精度量化感知优化。我们观察到，在位精度降低时，量化和HE的简单组合好处迅速饱和。因此，为了进一步提高通信效率，我们提出了一系列优化措施，包括一种内部系数打包算法和一种量化感知瓦片算法，以同时减少传输数据的数量和精度。与先前的HE协议（如CrypTFlow2）相比，

    Secure two-party computation with homomorphic encryption (HE) protects data privacy with a formal security guarantee but suffers from high communication overhead. While previous works, e.g., Cheetah, Iron, etc, have proposed efficient HE-based protocols for different neural network (NN) operations, they still assume high precision, e.g., fixed point 37 bit, for the NN operations and ignore NNs' native robustness against quantization error. In this paper, we propose HEQuant, which features low-precision-quantization-aware optimization for the HE-based protocols. We observe the benefit of a naive combination of quantization and HE quickly saturates as bit precision goes down. Hence, to further improve communication efficiency, we propose a series of optimizations, including an intra-coefficient packing algorithm and a quantization-aware tiling algorithm, to simultaneously reduce the number and precision of the transferred data. Compared with prior-art HE-based protocols, e.g., CrypTFlow2
    
[^78]: Baichuan2-Sum: 使用指导微调Baichuan2-7B模型进行对话摘要

    Baichuan2-Sum: Instruction Finetune Baichuan2-7B Model for Dialogue Summarization

    [https://arxiv.org/abs/2401.15496](https://arxiv.org/abs/2401.15496)

    本文提出了Baichuan2-Sum模型，通过指导微调Baichuan2-7B模型进行对话摘要，并应用NEFTune技术改进训练过程。实验证明该模型在CSDS和SAMSUM数据集上取得了新的最先进结果。

    

    巨大的语言模型（LLM）如Llama、Baichuan和Bloom模型在许多自然语言任务中展现出了令人瞩目的能力。然而，对于对话摘要任务，该任务旨在为对话中的不同角色生成摘要，大多数最先进的方法都是基于小模型（例如Bart和Bert）进行的。现有方法尝试在小模型上添加任务指定的优化，如向模型添加全局-局部中心度得分。在本文中，我们提出了一种指导微调模型：Baichuan2-Sum，用于面向角色的对话摘要。通过为不同角色设置不同的指令，模型可以从对话交互中学习并输出期望的摘要。此外，我们还应用了NEFTune技术，在训练过程中添加合适的噪声以提高结果。实验证明，所提出的模型在两个公开的对话摘要数据集CSDS和SAMSUM上取得了新的最先进结果。

    Large language models (LLMs) like Llama, Baichuan and Bloom models show remarkable ability with instruction fine-tuning in many natural language tasks. Nevertheless, for the dialogue summarization task, which aims to generate summaries for different roles in dialogue, most of the state-of-the-art methods conduct on small models (e.g Bart and Bert). Existing methods try to add task specified optimization on small models like adding global-local centrality score to models. In this paper, we propose an instruction fine-tuning model: Baichuan2-Sum, for role-oriented diaglouge summarization. By setting different instructions for different roles, the model can learn from the dialogue interactions and output the expected summaries. Furthermore, we applied NEFTune technique to add suitable noise during training to improve the results. The experiments demonstrate that the proposed model achieves the new state-of-the-art results on two public dialogue summarization datasets: CSDS and SAMSUM. We 
    
[^79]: 风速超分辨率与验证：从ERA5到CERRA通过扩散模型

    Wind speed super-resolution and validation: from ERA5 to CERRA via diffusion models

    [https://arxiv.org/abs/2401.15469](https://arxiv.org/abs/2401.15469)

    本论文提出了一种利用扩散模型以数据驱动的方式近似CERRA的降尺度的方法，通过利用ERA5数据集进行风速超分辨率任务。

    

    Copernicus区域再分析数据集CERRA是一个高分辨率的欧洲区域再分析数据集。近年来，在各种与气候相关的任务中它显示出了显著的实用性，包括天气预报、气候变化研究、可再生能源预测、资源管理、空气质量风险评估以及罕见事件的预测等。不幸的是，由于获取所需外部数据和生成过程中计算量大，CERRA的可用性滞后于当前日期两年。作为解决方案，本文提出了一种新的方法，使用扩散模型以数据驱动的方式近似CERRA的降尺度，而无需额外信息。通过利用边界条件由低分辨率ERA5数据集提供的CERRA数据，我们将其视为超分辨率任务。以意大利周围的风速为重点，我们的模型在现有CERRA数据上进行训练。

    The Copernicus Regional Reanalysis for Europe, CERRA, is a high-resolution regional reanalysis dataset for the European domain. In recent years it has shown significant utility across various climate-related tasks, ranging from forecasting and climate change research to renewable energy prediction, resource management, air quality risk assessment, and the forecasting of rare events, among others. Unfortunately, the availability of CERRA is lagging two years behind the current date, due to constraints in acquiring the requisite external data and the intensive computational demands inherent in its generation. As a solution, this paper introduces a novel method using diffusion models to approximate CERRA downscaling in a data-driven manner, without additional informations. By leveraging the lower resolution ERA5 dataset, which provides boundary conditions for CERRA, we approach this as a super-resolution task. Focusing on wind speed around Italy, our model, trained on existing CERRA data,
    
[^80]: 教育领域自然语言处理的调查：分类体系、系统综述和未来趋势

    Survey of Natural Language Processing for Education: Taxonomy, Systematic Review, and Future Trends

    [https://arxiv.org/abs/2401.07518](https://arxiv.org/abs/2401.07518)

    这篇论文调查了教育领域自然语言处理的最新进展，提出了分类体系，并总结了挑战和未来研究方向。

    

    自然语言处理（NLP）旨在通过计算机科学领域的技术分析文本，应用于医疗保健、商业和教育领域。特别是，在教育领域，NLP已经被应用于教学和学习方面的帮助。本调查研究主要关注解决与教育领域相关的问题，并回顾了NLP的最新进展。具体来说，我们从介绍相关背景开始，然后提出教育领域NLP的分类系统。接着，我们根据上述分类系统说明任务定义、挑战和相应的技术。之后，我们展示了该领域中的一些现有演示，并总结了未来的研究方向。

    Natural Language Processing (NLP) aims to analyze the text via techniques in the computer science field. It serves the applications in healthcare, commerce, and education domains. Particularly, NLP has been applied to the education domain to help teaching and learning. In this survey, we review recent advances in NLP with a focus on solving problems related to the education domain. In detail, we begin with introducing the relevant background. Then, we present the taxonomy of NLP in the education domain. Next, we illustrate the task definition, challenges, and corresponding techniques based on the above taxonomy. After that, we showcase some off-the-shelf demonstrations in this domain and conclude with future directions.
    
[^81]: 使用大型语言模型的强化学习修复代码安全漏洞

    Code Security Vulnerability Repair Using Reinforcement Learning with Large Language Models

    [https://arxiv.org/abs/2401.07031](https://arxiv.org/abs/2401.07031)

    本论文研究了使用大型语言模型的强化学习方法，以修复代码中的安全漏洞。目前的模型主要通过监督微调生成修复代码，但存在安全措施代码和功能代码之间的不平衡问题。

    

    随着大型语言模型（LLMs）的最新进展，对于各种开发者来说，生成功能正确的代码变得不再那么复杂。虽然使用LLMs加速了功能开发过程，但对代码安全性构成了重大风险。在使用LLMs进行带有安全措施的代码生成时，相较于功能代码生成，任务更为困难。安全措施可能包括在原始代码中添加一对代码行，包括空指针检查或准备好的语句以防止SQL注入。目前，可用的代码修复LLMs通过监督微调生成代码修复，模型通过交叉熵损失进行训练。然而，原始代码和修复后的代码在功能和语法上大致相似，除了少数（1-2）行作为安全措施。这种安全措施所需行数与功能代码之间的不平衡导致监督微调模型优先考虑生成功能代码。

    With the recent advancement of Large Language Models (LLMs), generating functionally correct code has become less complicated for a wide array of developers. While using LLMs has sped up the functional development process, it poses a heavy risk to code security. Code generation with proper security measures using LLM is a significantly more challenging task than functional code generation. Security measures may include adding a pair of lines of code with the original code, consisting of null pointer checking or prepared statements for SQL injection prevention. Currently, available code repair LLMs generate code repair by supervised fine-tuning, where the model looks at cross-entropy loss. However, the original and repaired codes are mostly similar in functionality and syntactically, except for a few (1-2) lines, which act as security measures. This imbalance between the lines needed for security measures and the functional code enforces the supervised fine-tuned model to prioritize gen
    
[^82]: Separate-and-Enhance: 文本到图像扩散模型的组合微调方法

    Separate-and-Enhance: Compositional Finetuning for Text2Image Diffusion Models

    [https://arxiv.org/abs/2312.06712](https://arxiv.org/abs/2312.06712)

    本研究提出了一种名为"Separate-and-Enhance"的方法，通过分离损失和增强损失来解决现有文本到图像模型中的错位问题。这种方法通过减少对象掩码重叠和最大化注意力得分来提高图像生成的质量和与文本提示的对齐。评估结果显示，该方法在图像逼真度、文本-图像对齐和适应性方面表现优于其他方法。

    

    尽管基于扩散的文本到图像(T2I)模型在近期取得了重大进展，但是目前的系统仍然不太能够确保与文本提示对齐的良好组合生成，特别是在多对象生成方面。本研究阐明了这种错位的基本原因，指出了与低注意力激活得分和掩码重叠有关的问题。尽管先前的研究工作已经分别解决了这些问题，但我们认为整体方法至关重要。因此，我们提出了两个新的目标，即分离损失和增强损失，分别减少对象掩码重叠和最大化注意力得分。我们的方法不同于常规的测试时调适技术，着重于微调关键参数，从而提高了可伸缩性和普适性。全面的评估表明，我们的模型在图像逼真度、文本-图像对齐和适应性方面表现卓越，尤其是在实现上述方面优于其他方法。

    Despite recent significant strides achieved by diffusion-based Text-to-Image (T2I) models, current systems are still less capable of ensuring decent compositional generation aligned with text prompts, particularly for the multi-object generation. This work illuminates the fundamental reasons for such misalignment, pinpointing issues related to low attention activation scores and mask overlaps. While previous research efforts have individually tackled these issues, we assert that a holistic approach is paramount. Thus, we propose two novel objectives, the Separate loss and the Enhance loss, that reduce object mask overlaps and maximize attention scores, respectively. Our method diverges from conventional test-time-adaptation techniques, focusing on finetuning critical parameters, which enhances scalability and generalizability. Comprehensive evaluations demonstrate the superior performance of our model in terms of image realism, text-image alignment, and adaptability, notably outperform
    
[^83]: 高效大型语言模型：一项调查

    Efficient Large Language Models: A Survey

    [https://arxiv.org/abs/2312.03863](https://arxiv.org/abs/2312.03863)

    这篇综述论文对高效大型语言模型进行了系统和全面的调查，提供了从模型为中心、数据为中心和框架为中心的三个主要角度的分类和总结。此外，还创建了一个GitHub存储库来收集和更新相关论文。

    

    大型语言模型（LLMs）在重要任务如自然语言理解、语言生成和复杂推理中展示了卓越的能力，并且有潜力对我们的社会产生重大影响。然而，这种能力伴随着它们所需的相当大的资源，突显了解决效率挑战的有效技术的强烈需求。在这项调查中，我们提供了对高效LLMs研究的系统和全面的综述。我们将文献按照模型为中心、数据为中心和框架为中心的三个主要分类进行组织，涵盖了不同但相互关联的高效LLMs主题。我们还创建了一个GitHub存储库，其中收集了本调查中列出的论文，并将积极维护该存储库，并随着新的研究的出现而更新。

    Large Language Models (LLMs) have demonstrated remarkable capabilities in important tasks such as natural language understanding, language generation, and complex reasoning and have the potential to make a substantial impact on our society. Such capabilities, however, come with the considerable resources they demand, highlighting the strong need to develop effective techniques for addressing their efficiency challenges.In this survey, we provide a systematic and comprehensive review of efficient LLMs research. We organize the literature in a taxonomy consisting of three main categories, covering distinct yet interconnected efficient LLMs topics from model-centric, data-centric, and framework-centric perspective, respectively. We have also created a GitHub repository where we compile the papers featured in this survey at https://github.com/AIoT-MLSys-Lab/Efficient-LLMs-Survey, and will actively maintain this repository and incorporate new research as it emerges. We hope our survey can s
    
[^84]: 将语言知识注入到BERT中用于对话状态跟踪

    Injecting linguistic knowledge into BERT for Dialogue State Tracking

    [https://arxiv.org/abs/2311.15623](https://arxiv.org/abs/2311.15623)

    本文提出了一种方法，在对话状态跟踪任务中，通过无监督的知识提取方法将语言知识注入到BERT中，以提高性能和可解释性。这种方法无需额外的训练数据，通过简单的神经模块实现。该方法使用的特征提取工具与对话的句法和语义模式相关，有助于理解DST模型的决策过程。

    

    对话状态跟踪(DST)模型通常采用复杂的神经网络架构，需要大量的训练数据，其推理过程缺乏透明性。本文提出了一种方法，通过无监督框架提取语言知识，然后利用这些知识来增强BERT在DST任务中的性能和可解释性。知识提取过程计算经济高效，不需要注释或额外的训练数据。注入提取的知识只需要添加简单的神经模块。我们使用凸多面体模型(CPM)作为DST任务的特征提取工具，并表明所获取的特征与对话中的句法和语义模式相关。这种相关性有助于全面理解影响DST模型决策过程的语言特征。我们在不同的DST任务上对这个框架进行基准测试，并展示了其效果。

    Dialogue State Tracking (DST) models often employ intricate neural network architectures, necessitating substantial training data, and their inference processes lack transparency. This paper proposes a method that extracts linguistic knowledge via an unsupervised framework and subsequently utilizes this knowledge to augment BERT's performance and interpretability in DST tasks. The knowledge extraction procedure is computationally economical and does not necessitate annotations or additional training data. The injection of the extracted knowledge necessitates the addition of only simple neural modules. We employ the Convex Polytopic Model (CPM) as a feature extraction tool for DST tasks and illustrate that the acquired features correlate with the syntactic and semantic patterns in the dialogues. This correlation facilitates a comprehensive understanding of the linguistic features influencing the DST model's decision-making process. We benchmark this framework on various DST tasks and ob
    
[^85]: GeoSAM: 使用稀疏和密集的视觉提示对SAM进行改进，实现自动化的移动基础设施分割

    GeoSAM: Fine-tuning SAM with Sparse and Dense Visual Prompting for Automated Segmentation of Mobility Infrastructure

    [https://arxiv.org/abs/2311.11319](https://arxiv.org/abs/2311.11319)

    GeoSAM是一个基于SAM的新框架，使用了来自零样本学习和预训练CNN分割模型的视觉提示，提高了地理图像分割的性能。

    

    当应用于自然图像分割时，Segment Anything Model (SAM)已经展现出了令人印象深刻的性能。然而，它在地理图像（如航拍和卫星图像）中面临困难，特别是在分割道路、人行道和人行横道等移动基础设施时。这种较差的性能源于这些对象的窄小特征，它们的纹理融入环境中，以及树木、建筑物、车辆和行人等物体的干扰，这些都可能使模型失去定向产生不准确的分割图。为了解决这些挑战，我们提出了地理SAM（GeoSAM），这是一个基于SAM的新框架，它使用来自零样本学习的密集视觉提示和预训练CNN分割模型的稀疏视觉提示实施了细调策略。所提出的GeoSAM在地理图像分割方面优于现有方法，特别是对于道路基础设施、行人基础设施的分割性能提升了26％、7％和17％。

    The Segment Anything Model (SAM) has shown impressive performance when applied to natural image segmentation. However, it struggles with geographical images like aerial and satellite imagery, especially when segmenting mobility infrastructure including roads, sidewalks, and crosswalks. This inferior performance stems from the narrow features of these objects, their textures blending into the surroundings, and interference from objects like trees, buildings, vehicles, and pedestrians - all of which can disorient the model to produce inaccurate segmentation maps. To address these challenges, we propose Geographical SAM (GeoSAM), a novel SAM-based framework that implements a fine-tuning strategy using the dense visual prompt from zero-shot learning, and the sparse visual prompt from a pre-trained CNN segmentation model. The proposed GeoSAM outperforms existing approaches for geographical image segmentation, specifically by 26%, 7%, and 17% for road infrastructure, pedestrian infrastructur
    
[^86]: 大规模轨迹模型是可扩展的运动预测和规划器

    Large Trajectory Models are Scalable Motion Predictors and Planners

    [https://arxiv.org/abs/2310.19620](https://arxiv.org/abs/2310.19620)

    大规模轨迹模型（LTMs）采用State Transformer (STR)模型，将运动预测和规划问题统一建模，有效应对自动驾驶中的挑战。

    

    运动预测和规划是自动驾驶中的重要任务，最近的研究工作已经转向基于机器学习的方法。面临的挑战包括理解多样化的道路拓扑，推理长时间范围内的交通动态，解释异质行为，并在大规模连续状态空间生成策略。受到大型语言模型在通过模型扩展解决类似复杂性方面的成功启发，我们引入一种可扩展的轨迹模型 State Transformer (STR)。STR通过将观察、状态和动作排列成一个统一的序列建模任务来重新定义运动预测和运动规划问题。我们的方法将轨迹生成问题与其他序列建模问题结合起来，借助领域间的突破性进展（如语言建模），实现快速迭代。引人注目的是，实验结果表明，大规模轨迹模型（如STR）遵循扩展定律，展示了

    Motion prediction and planning are vital tasks in autonomous driving, and recent efforts have shifted to machine learning-based approaches. The challenges include understanding diverse road topologies, reasoning traffic dynamics over a long time horizon, interpreting heterogeneous behaviors, and generating policies in a large continuous state space. Inspired by the success of large language models in addressing similar complexities through model scaling, we introduce a scalable trajectory model called State Transformer (STR). STR reformulates the motion prediction and motion planning problems by arranging observations, states, and actions into one unified sequence modeling task. Our approach unites trajectory generation problems with other sequence modeling problems, powering rapid iterations with breakthroughs in neighbor domains such as language modeling. Remarkably, experimental results reveal that large trajectory models (LTMs), such as STR, adhere to the scaling laws by presenting
    
[^87]: 自我监督的语音和语言模型是否提取了与人类大脑类似的表示？

    Do self-supervised speech and language models extract similar representations as human brain?

    [https://arxiv.org/abs/2310.04645](https://arxiv.org/abs/2310.04645)

    通过评估Wav2Vec2.0和GPT-2模型的大脑预测能力，我们发现自我监督的语音和语言模型能够准确预测语音反应，其大脑预测之间存在显著相关性，且共享的语音上下文信息是解释大脑活动中变异的主要因素。

    

    通过自我监督学习（SSL）训练的语音和语言模型在语音和语言感知期间展现出与大脑活动的强大对齐性。然而，由于它们的不同训练方式，它们是否与相同的神经方面相关仍然不清楚。我们通过评估两种代表性的SSL模型（Wav2Vec2.0和GPT-2）在语音和语言任务中的大脑预测性能来直接回答这个问题。我们的研究结果显示，这两种模型都能准确预测听觉皮层中的语音响应，并且它们的大脑预测之间存在显著相关性。值得注意的是，Wav2Vec2.0和GPT-2之间的共享语音上下文信息解释了大脑活动中的大部分变异，超过了静态语义和较低级的声音-音位信息。这些结果强调了SSL模型中语音上下文表示的收敛性，以及它们与语音知觉底层神经网络的对齐。

    Speech and language models trained through self-supervised learning (SSL) demonstrate strong alignment with brain activity during speech and language perception. However, given their distinct training modalities, it remains unclear whether they correlate with the same neural aspects. We directly address this question by evaluating the brain prediction performance of two representative SSL models, Wav2Vec2.0 and GPT-2, designed for speech and language tasks. Our findings reveal that both models accurately predict speech responses in the auditory cortex, with a significant correlation between their brain predictions. Notably, shared speech contextual information between Wav2Vec2.0 and GPT-2 accounts for the majority of explained variance in brain activity, surpassing static semantic and lower-level acoustic-phonetic information. These results underscore the convergence of speech contextual representations in SSL models and their alignment with the neural network underlying speech percept
    
[^88]: 单语或多语指导调整：哪种方式更适合alpaca？

    Monolingual or Multilingual Instruction Tuning: Which Makes a Better Alpaca

    [https://arxiv.org/abs/2309.08958](https://arxiv.org/abs/2309.08958)

    通过实证分析比较了单语和多语指导调整的成本效益，发现在多语言场景下，多语指导调整可以达到或超越单独调整每种语言的效果，并且采用下采样的数据进行多语调整可以提供更强的效果和更好的鲁棒性。

    

    基础的大型语言模型（LLM）可以通过指导调整来执行开放域的问答任务，从而实现聊天助手等应用。虽然这类努力通常只在单一语言中进行，但我们实证分析了多语言场景下的成本效益策略。我们使用Alpaca数据集和其中的机器翻译数据形成多语言调整的训练集，然后采用低秩调整或完全参数训练的方式对LLM进行调整。在受控算力预算下的比较结果表明，多语言调整可以达到或超越每种语言单独调整的效果。此外，通过下采样的数据进行多语言调整可以达到相同甚至更强的效果。我们的研究结果为通过指导调整来扩展语言支持提供了指导。

    Foundational large language models (LLMs) can be instruction-tuned to perform open-domain question answering, facilitating applications like chat assistants. While such efforts are often carried out in a single language, we empirically analyze cost-efficient strategies for multilingual scenarios. Our study employs the Alpaca dataset and machine translations of it to form multilingual data, which is then used to tune LLMs through either low-rank adaptation or full-parameter training. Under a controlled computation budget, comparisons show that multilingual tuning is on par or better than tuning a model for each language. Furthermore, multilingual tuning with downsampled data can be as powerful and more robust. Our findings serve as a guide for expanding language support through instruction tuning.
    
[^89]: SCRAPS: 声音和语音空间的对比表征

    SCRAPS: Speech Contrastive Representations of Acoustic and Phonetic Spaces

    [https://arxiv.org/abs/2307.12445](https://arxiv.org/abs/2307.12445)

    本文提出了一个基于CLIP的模型，旨在学习语音和声学空间的共享表征。实验证明该模型对语音变化敏感，具有鲁棒性并适用于多种下游应用。

    

    文献中的众多例子证明了深度学习模型在多模态数据上的良好表现。最近，CLIP使得深度学习系统能够学习图像和文本描述之间的共享潜在空间，在下游任务中具有杰出的零次或少次测试结果。在本文中，我们探索了CLIP提出的相同思想，但应用于语音领域，其中语音和声学空间通常共存。我们训练了一个基于CLIP的模型，旨在学习语音和声学空间的共享表征。结果表明，所提出的模型对语音变化敏感，将20％随机替换的音素得分下降了91％，同时对不同类型的噪声提供了实质性的鲁棒性，将音频与75％的高斯噪声混合时性能下降了10％。我们还提供了经验证据表明，所得到的嵌入对于各种下游应用是有用的，比如可懂度。

    Numerous examples in the literature proved that deep learning models have the ability to work well with multimodal data. Recently, CLIP has enabled deep learning systems to learn shared latent spaces between images and text descriptions, with outstanding zero- or few-shot results in downstream tasks. In this paper we explore the same idea proposed by CLIP but applied to the speech domain, where the phonetic and acoustic spaces usually coexist. We train a CLIP-based model with the aim to learn shared representations of phonetic and acoustic spaces. The results show that the proposed model is sensible to phonetic changes, with a 91% of score drops when replacing 20% of the phonemes at random, while providing substantial robustness against different kinds of noise, with a 10% performance drop when mixing the audio with 75% of Gaussian noise. We also provide empirical evidence showing that the resulting embeddings are useful for a variety of downstream applications, such as intelligibility
    
[^90]: 自然语言理解中的即时去噪数据增强

    On-the-fly Denoising for Data Augmentation in Natural Language Understanding

    [https://arxiv.org/abs/2212.10558](https://arxiv.org/abs/2212.10558)

    本文提出了一种即时去噪的数据增强技术，利用软增强标签和自我正则化模块，通过从更干净的原始数据学习来保证增强数据的质量。

    

    数据增强（DA）经常被用来在没有额外的人工注释的情况下提供额外的训练数据。然而，数据增强可能引入噪声数据来干扰训练。为了保证增强数据的质量，现有方法要么假设增强数据中没有噪声，并采用一致性训练，要么使用简单的启发式方法（如训练损失和多样性约束）来过滤掉“嘈杂”的数据。然而，这些被过滤的示例可能仍然包含有用的信息，并且完全丢弃它们会导致监督信号的丧失。在本文中，基于原始数据集比增强数据更干净的假设，我们提出了一种即时去噪的数据增强技术，该技术利用了在更干净的原始数据上训练的有机教师模型提供的软增强标签进行学习。为了进一步防止过度拟合噪声标签，我们还应用了简单的自我正则化模块，强制模型预测保持一致。

    Data Augmentation (DA) is frequently used to provide additional training data without extra human annotation automatically. However, data augmentation may introduce noisy data that impairs training. To guarantee the quality of augmented data, existing methods either assume no noise exists in the augmented data and adopt consistency training or use simple heuristics such as training loss and diversity constraints to filter out "noisy" data. However, those filtered examples may still contain useful information, and dropping them completely causes a loss of supervision signals. In this paper, based on the assumption that the original dataset is cleaner than the augmented data, we propose an on-the-fly denoising technique for data augmentation that learns from soft augmented labels provided by an organic teacher model trained on the cleaner original data. To further prevent overfitting on noisy labels, a simple self-regularization module is applied to force the model prediction to be consi
    
[^91]: 关于基于心电图(ECG)的压力检测模型的泛化能力研究

    On the Generalizability of ECG-based Stress Detection Models

    [https://arxiv.org/abs/2210.06225](https://arxiv.org/abs/2210.06225)

    本文研究了基于心电图(ECG)的压力检测模型的泛化能力，探讨了深度学习模型和基于心电图特征的模型在不同压力场景下的应用程度。

    

    压力在日常生活的许多方面都很普遍，包括工作、医疗和社交互动。许多研究已经研究了各种生物信号的手工特征，这些特征是压力的指标。最近，也提出了使用深度学习模型检测压力。通常，压力模型在相同的数据集上进行训练和验证，通常涉及一个压力场景。然而，为每个场景收集压力数据是不实际的。因此，研究这些模型的泛化能力，并确定它们在其他场景中的使用程度非常重要。在本文中，我们探索了基于心电图(ECG)的深度学习模型和基于心电图特征（即心率变异性(HRV)特征）的模型的泛化能力。为此，我们训练了三个HRV模型和两个使用ECG信号作为输入的深度学习模型。我们使用了两个流行的压力数据集（WESAD和SWELL-KW）的ECG信号。

    Stress is prevalent in many aspects of everyday life including work, healthcare, and social interactions. Many works have studied handcrafted features from various bio-signals that are indicators of stress. Recently, deep learning models have also been proposed to detect stress. Typically, stress models are trained and validated on the same dataset, often involving one stressful scenario. However, it is not practical to collect stress data for every scenario. So, it is crucial to study the generalizability of these models and determine to what extent they can be used in other scenarios. In this paper, we explore the generalization capabilities of Electrocardiogram (ECG)-based deep learning models and models based on handcrafted ECG features, i.e., Heart Rate Variability (HRV) features. To this end, we train three HRV models and two deep learning models that use ECG signals as input. We use ECG signals from two popular stress datasets - WESAD and SWELL-KW - differing in terms of stresso
    
[^92]: 一种自述疗法的共情人工智能辅导系统

    An Empathetic AI Coach for Self-Attachment Therapy

    [https://arxiv.org/abs/2209.08316](https://arxiv.org/abs/2209.08316)

    本文介绍了一个用于自述疗法的共情人工智能辅导系统，通过深度学习和规则进行情绪识别和生成流畅、共情的对话，达到更高的用户参与度和实用性。通过非临床试验验证了框架的有效性，并提供改进设计和性能的指导方针。

    

    本文提出了一个用于指导用户进行自述疗法的数字辅导系统的新数据集和计算策略。我们的框架通过将基于规则的对话代理与深度学习分类器相结合，可以识别用户文本回复中的潜在情绪，并采用深度学习辅助检索方法生成新颖、流畅和共情的话语。我们还设计了一组类似人类的角色供用户选择互动。我们的目标是在虚拟疗法会话中实现高水平的参与度。我们在一项非临床试验中对N=16名参与者进行了框架的有效性评估，这些参与者在五天内至少与代理进行了四次互动。结果显示，与简单的基于规则的框架相比，我们的平台在共情度、用户参与度和实用性方面被评价得更高。最后，我们提供了进一步改进设计和性能的指导方针。

    In this work, we present a new dataset and a computational strategy for a digital coach that aims to guide users in practicing the protocols of self-attachment therapy. Our framework augments a rule-based conversational agent with a deep-learning classifier for identifying the underlying emotion in a user's text response, as well as a deep-learning assisted retrieval method for producing novel, fluent and empathetic utterances. We also craft a set of human-like personas that users can choose to interact with. Our goal is to achieve a high level of engagement during virtual therapy sessions. We evaluate the effectiveness of our framework in a non-clinical trial with N=16 participants, all of whom have had at least four interactions with the agent over the course of five days. We find that our platform is consistently rated higher for empathy, user engagement and usefulness than the simple rule-based framework. Finally, we provide guidelines to further improve the design and performance 
    
[^93]: 变分转移学习中的跨领域潜在调制机制

    Variational Transfer Learning using Cross-Domain Latent Modulation

    [https://arxiv.org/abs/2205.15523](https://arxiv.org/abs/2205.15523)

    本研究提出了一种变分自编码器框架中的跨领域潜在调制机制，通过从一领域获取深层表示并影响另一领域的潜在变量，实现了有效的转移学习。在多个转移学习基准任务中，我们的模型展示了竞争性能。

    

    为了成功地将训练好的神经网络模型应用到新领域，强大的转移学习解决方案至关重要。我们提出了一种新的跨领域潜在调制机制，将其引入到变分自编码器框架中，以实现有效的转移学习。我们的关键思想是从一个数据领域获取深层表示，并用它来影响另一个领域的潜在变量的重新参数化。具体地说，通过统一的推理模型来提取源领域和目标领域的深层表示，并通过梯度反转进行对齐。然后将学习到的深层表示跨调制到另一领域的潜在编码中，并应用一致性约束。在包括一些无监督领域自适应和图像到图像转换的转移学习基准任务的实证验证中，我们的模型展示了竞争性能，并且得到了支持的证据。

    To successfully apply trained neural network models to new domains, powerful transfer learning solutions are essential. We propose to introduce a novel cross-domain latent modulation mechanism to a variational autoencoder framework so as to achieve effective transfer learning. Our key idea is to procure deep representations from one data domain and use it to influence the reparameterization of the latent variable of another domain. Specifically, deep representations of the source and target domains are first extracted by a unified inference model and aligned by employing gradient reversal. The learned deep representations are then cross-modulated to the latent encoding of the alternative domain, where consistency constraints are also applied. In the empirical validation that includes a number of transfer learning benchmark tasks for unsupervised domain adaptation and image-to-image translation, our model demonstrates competitive performance, which is also supported by evidence obtained
    
[^94]: 通过分层和分片区块链实现安全高效的联邦学习

    Secure and Efficient Federated Learning Through Layering and Sharding Blockchain

    [https://arxiv.org/abs/2104.13130](https://arxiv.org/abs/2104.13130)

    本论文提出了一种名为ChainFL的创新联邦学习系统，通过分层和分片的方式结合区块链技术，解决了传统区块链系统处理大规模联邦学习任务时面临的挑战，并定制化了联邦学习过程与区块链的整合。

    

    将区块链引入联邦学习中，构建一个可信的边缘计算环境来进行传输和学习已经引起了广泛关注，作为一种新的分布式学习模式。然而，传统的区块链系统的共识机制和架构在处理大规模联邦学习任务，特别是在物联网设备上，面临着重大挑战，由于它们的资源消耗、交易吞吐量有限和复杂的通信需求。为了解决这些挑战，本文提出了ChainFL，一个创新的基于两层区块链的联邦学习系统。它将物联网网络分割成多个分片，并在子链层内有效地减少了信息交换规模，并使用基于有向无环图（DAG）的主链作为主链层，实现了并行和异步的跨分片验证。此外，联邦学习过程被定制化以与区块链技术深度整合，还提出了一种修改过的DAG...

    Introducing blockchain into Federated Learning (FL) to build a trusted edge computing environment for transmission and learning has attracted widespread attention as a new decentralized learning pattern. However, traditional consensus mechanisms and architectures of blockchain systems face significant challenges in handling large-scale FL tasks, especially on Internet of Things (IoT) devices, due to their substantial resource consumption, limited transaction throughput, and complex communication requirements. To address these challenges, this paper proposes ChainFL, a novel two-layer blockchain-driven FL system. It splits the IoT network into multiple shards within the subchain layer, effectively reducing the scale of information exchange, and employs a Direct Acyclic Graph (DAG)-based mainchain as the mainchain layer, enabling parallel and asynchronous cross-shard validation. Furthermore, the FL procedure is customized to integrate deeply with blockchain technology, and a modified DAG
    
[^95]: BlockFusion: 使用潜在三平面外推扩展的可扩展三维场景生成模型

    BlockFusion: Expandable 3D Scene Generation using Latent Tri-plane Extrapolation. (arXiv:2401.17053v1 [cs.CV])

    [http://arxiv.org/abs/2401.17053](http://arxiv.org/abs/2401.17053)

    BlockFusion是一种使用扩散和外推技术生成三维场景的模型，能无缝地添加新的块以扩展场景。采用混合神经场和潜在三平面空间来保证高质量和多样化的生成结果。

    

    我们提出了BlockFusion，一种基于扩散的模型，以单位块形式生成三维场景，并无缝地添加新的块以扩展场景。BlockFusion使用从完整的三维场景中随机裁剪的3D块数据集进行训练。通过块拟合，将所有训练块转换为混合神经场：它包含几何特征的三平面，以及用于解码有符号距离值的多层感知机(MLP)。采用变分自动编码器将三平面压缩到潜在三平面空间，并在其上执行去噪扩散过程。对潜在表示应用扩散，可以实现高质量和多样化的三维场景生成。在生成过程中扩展场景时，只需将空块添加到与当前场景重叠，并外推现有的潜在三平面以填充新块。外推过程通过使用特征对生成过程进行约束来完成。

    We present BlockFusion, a diffusion-based model that generates 3D scenes as unit blocks and seamlessly incorporates new blocks to extend the scene. BlockFusion is trained using datasets of 3D blocks that are randomly cropped from complete 3D scene meshes. Through per-block fitting, all training blocks are converted into the hybrid neural fields: with a tri-plane containing the geometry features, followed by a Multi-layer Perceptron (MLP) for decoding the signed distance values. A variational auto-encoder is employed to compress the tri-planes into the latent tri-plane space, on which the denoising diffusion process is performed. Diffusion applied to the latent representations allows for high-quality and diverse 3D scene generation. To expand a scene during generation, one needs only to append empty blocks to overlap with the current scene and extrapolate existing latent tri-planes to populate new blocks. The extrapolation is done by conditioning the generation process with the feature 
    
[^96]: DiffuserLite: 实时扩散规划的研究

    DiffuserLite: Towards Real-time Diffusion Planning. (arXiv:2401.15443v1 [cs.AI])

    [http://arxiv.org/abs/2401.15443](http://arxiv.org/abs/2401.15443)

    DiffuserLite是一个快速轻量级的扩散规划框架，通过引入计划细化过程（PRP）来提高决策频率，相比之前的框架，它只产生了很小的运行时间成本，并在D4RL基准测试中达到了最先进的性能。

    

    扩散规划被认为是各个领域中有效的决策范式。长时间跨度轨迹的高质量条件生成能力使其成为一个有前途的研究方向。然而，现有的扩散规划方法由于迭代抽样成本昂贵而导致决策频率低。为了解决这个问题，我们引入了DiffuserLite，一个快速而轻量级的扩散规划框架。DiffuserLite使用了一个计划细化过程（PRP）来生成粗到细粒度的轨迹，这显著减少了冗余信息的建模，从而显著提高了决策频率。我们的实验结果表明，与之前的框架相比，DiffuserLite仅产生了$0.88\%$的运行时间成本，平均决策频率达到了122Hz，并在D4RL基准测试上达到了最先进的性能。此外，我们的干净DiffuserLite框架可以提供...

    Diffusion planning has been recognized as an effective decision-making paradigm in various domains. The high-quality conditional generation capability of long-horizon trajectories makes it a promising research direction. However, existing diffusion planning methods suffer from low decision-making frequencies because of the expensive iterative sampling cost. To address this issue, we introduce DiffuserLite, a fast and lightweight diffusion planning framework. DiffuserLite employs a planning refinement process (PRP) to generate coarse-to-fine-grained trajectories, which significantly reduces the modeling of redundant information and leads to notable increases in decision-making frequency. Our experimental results demonstrate that DiffuserLite incurs only $0.88\%$ of the runtime cost compared to previous frameworks, achieves an average decision-making frequency of $122$Hz, and reaches state-of-the-art performance on D4RL benchmarks. In addition, our clean DiffuserLite framework can serve 
    
[^97]: 基于RAG的理解伊斯兰教问题回答系统提案：MufassirQAS LLM

    A RAG-based Question Answering System Proposal for Understanding Islam: MufassirQAS LLM. (arXiv:2401.15378v1 [cs.CL])

    [http://arxiv.org/abs/2401.15378](http://arxiv.org/abs/2401.15378)

    基于RAG的MufassirQAS问答系统利用NLP技术建立联系并准确回答复杂问题，提高了LLMs的准确性和透明度，帮助理解伊斯兰教的复杂性和教义深度。

    

    学习和理解宗教存在复杂性和教义深度的挑战。问答机器人作为解决这些挑战的问题回答系统，可以帮助。LLM聊天机器人利用自然语言处理技术建立主题之间的联系，准确回答复杂问题。这些能力使其成为用于宗教启蒙的问题回答聊天机器人的理想选择。然而，LLM也有生成虚假信息的倾向，称为幻觉。聊天机器人的回答可能包含侮辱个人宗教信仰、跨宗派冲突和有争议或敏感的话题的内容。它需要避免这种情况，而不会宣扬仇恨言论或冒犯某些群体的人或他们的信仰。本研究使用基于向量数据库的检索增强生成（RAG）方法来提高LLMs的准确性和透明度。我们的问答系统称为"MufassirQAS"。我们创建了一个模型来评估该系统并证明其在解决宗教行业问题中的效果。

    There exist challenges in learning and understanding religions as the presence of complexity and depth of religious doctrines and teachings. Chatbots as question-answering systems can help in solving these challenges. LLM chatbots use NLP techniques to establish connections between topics and accurately respond to complex questions. These capabilities make it perfect to be used in enlightenment on religion as a question answering chatbot. However, LLMs also have a tendency to generate false information, known as hallucination. The responses of the chatbots can include content that insults personal religious beliefs, interfaith conflicts, and controversial or sensitive topics. It needs to avoid such cases without promoting hate speech or offending certain groups of people or their beliefs. This study uses a vector database-based Retrieval Augmented Generation (RAG) approach to enhance the accuracy and transparency of LLMs. Our question-answering system is called as "MufassirQAS". We cre
    
[^98]: 与硬件共同设计模型架构的理由

    The Case for Co-Designing Model Architectures with Hardware. (arXiv:2401.14489v1 [cs.DC])

    [http://arxiv.org/abs/2401.14489](http://arxiv.org/abs/2401.14489)

    本文提供了一组指南，通过考虑模型超参数对GPU上执行的计算内核的效率的影响，来最大化变换器模型的运行时性能。相比于具有相似参数数量但形状未经优化的模型，使用高效模型形状的模型可以提高39%的吞吐量且保持准确性。

    

    虽然GPU负责训练大部分最先进的深度学习模型，但在设计新的深度学习模型时往往忽视了其架构的影响。因此，将深度学习模型修改为更适合目标硬件可以显著提高深度学习训练和推理的运行时性能。本文提供了一组指南，用于使用户最大程度地提高他们的变换器模型的运行时性能。这些指南是通过仔细考虑控制模型形状的各种模型超参数对GPU上执行的底层计算内核的效率的影响而创建的。我们发现具有高效模型形状的模型的吞吐量比具有相似参数数量但形状未经优化的模型高出39％，同时保持准确性。

    While GPUs are responsible for training the vast majority of state-of-the-art deep learning models, the implications of their architecture are often overlooked when designing new deep learning (DL) models. As a consequence, modifying a DL model to be more amenable to the target hardware can significantly improve the runtime performance of DL training and inference. In this paper, we provide a set of guidelines for users to maximize the runtime performance of their transformer models. These guidelines have been created by carefully considering the impact of various model hyperparameters controlling model shape on the efficiency of the underlying computation kernels executed on the GPU. We find the throughput of models with efficient model shapes is up to 39\% higher while preserving accuracy compared to models with a similar number of parameters but with unoptimized shapes.
    
[^99]: 语义敏感性和不一致的预测：衡量NLI模型的脆弱性

    Semantic Sensitivities and Inconsistent Predictions: Measuring the Fragility of NLI Models. (arXiv:2401.14440v1 [cs.CL])

    [http://arxiv.org/abs/2401.14440](http://arxiv.org/abs/2401.14440)

    这份论文研究发现，最先进的NLI模型对微小的语义保持表面形式变化非常敏感，导致推断结果不一致。其行为与对组合语义的有效理解不同，这对当前NLI模型的可靠性提出了挑战。

    

    最近对基于transformer的自然语言理解（NLU）模型的新能力进行的研究表明，它们具备对词汇和组合语义的理解。然而，我们提供了证据表明这些说法应该持保留态度：我们发现目前最先进的自然语言推理（NLI）模型对微小的保留语义的表面形式变化敏感，这导致推断过程中出现大量不一致的模型决策。值得注意的是，这种行为与对组合语义的有效和深入理解不同，而在标准基准测试中评估模型准确度或探究句法、单调性和逻辑鲁棒性推理时均不会出现。我们提出了一个新颖的框架来衡量语义敏感性的程度。为此，我们使用含有微小保留语义的表面形式输入噪声的对抗生成样例来评估NLI模型。

    Recent studies of the emergent capabilities of transformer-based Natural Language Understanding (NLU) models have indicated that they have an understanding of lexical and compositional semantics. We provide evidence that suggests these claims should be taken with a grain of salt: we find that state-of-the-art Natural Language Inference (NLI) models are sensitive towards minor semantics preserving surface-form variations, which lead to sizable inconsistent model decisions during inference. Notably, this behaviour differs from valid and in-depth comprehension of compositional semantics, however does neither emerge when evaluating model accuracy on standard benchmarks nor when probing for syntactic, monotonic, and logically robust reasoning. We propose a novel framework to measure the extent of semantic sensitivity. To this end, we evaluate NLI models on adversarially generated examples containing minor semantics-preserving surface-form input noise. This is achieved using conditional text
    
[^100]: 基于点云表示和扩散模型的晶体结构生成设计

    Generative Design of Crystal Structures by Point Cloud Representations and Diffusion Model. (arXiv:2401.13192v1 [cs.AI])

    [http://arxiv.org/abs/2401.13192](http://arxiv.org/abs/2401.13192)

    本研究提出了一种基于点云和扩散模型的晶体结构生成设计框架，并通过重建输入结构和生成全新材料的实验证明了其有效性和潜力。

    

    在材料设计中，高效地生成能量稳定的晶体结构一直是个挑战，主要是因为晶格中原子的巨大排列。为了促进稳定材料的发现，我们提出了一个用于生成可合成材料的框架，利用点云表示来编码复杂的结构信息。在这个框架的核心是引入扩散模型作为基础支柱。为了评估我们方法的有效性，我们使用它来重建训练数据集中的输入结构，并严格验证其高重建性能。此外，我们通过生成全新的材料，重点强调了基于点云的晶体扩散(PCCD)的巨大潜力，并展示了其可合成性。我们的研究在材料设计和合成的推进中，通过先进的生成设计方法，做出了显著贡献。

    Efficiently generating energetically stable crystal structures has long been a challenge in material design, primarily due to the immense arrangement of atoms in a crystal lattice. To facilitate the discovery of stable material, we present a framework for the generation of synthesizable materials, leveraging a point cloud representation to encode intricate structural information. At the heart of this framework lies the introduction of a diffusion model as its foundational pillar. To gauge the efficacy of our approach, we employ it to reconstruct input structures from our training datasets, rigorously validating its high reconstruction performance. Furthermore, we demonstrate the profound potential of Point Cloud-Based Crystal Diffusion (PCCD) by generating entirely new materials, emphasizing their synthesizability. Our research stands as a noteworthy contribution to the advancement of materials design and synthesis through the cutting-edge avenue of generative design instead of the con
    
[^101]: GRATH: 大型语言模型的逐渐自我真实化方法

    GRATH: Gradual Self-Truthifying for Large Language Models. (arXiv:2401.12292v1 [cs.CL])

    [http://arxiv.org/abs/2401.12292](http://arxiv.org/abs/2401.12292)

    GRATH是一种逐步自我真实化的方法，用于提高大型语言模型的真实性。它通过使用领域外问题提示生成答案，并通过直接偏好优化进行自适应模型优化。GRATH在没有标注答案的情况下以自我监督的方式学习真实性，并通过迭代优化来逐步提升模型真实性。

    

    随着大型语言模型（LLMs）在真实世界应用中的部署越来越多，真实性对它们来说至关重要。然而，现有的LLMs在生成真实答案和内容方面仍然存在困难，如在TruthfulQA等基准上的表现不佳。为了解决这个问题，我们提出了GRAdual self-truTHifying (GRATH)，一种通过后处理方法提高LLMs真实性的新方法。GRATH利用领域外的问题提示生成相应的答案，并通过直接偏好优化进行自适应模型优化。在这个过程中，GRATH以无需标注答案的自我监督方式学习真实性。具体而言，GRATH首先通过提示LLM自身生成成对真实性训练数据，每对包含一个问题及其正确和错误答案。然后，使用直接偏好优化来微调模型，从答案对的差异中学习。随后，GRATH迭代地优化模型以逐渐提高真实性。

    Truthfulness is paramount for large language models (LLMs) as they are increasingly deployed in real-world applications. However, existing LLMs still struggle with generating truthful answers and content, as evidenced by their modest performance on benchmarks like TruthfulQA. To address this issue, we propose GRAdual self-truTHifying (GRATH), a novel post-processing method to enhance truthfulness of LLMs. GRATH utilizes out-of-domain question prompts to generate corresponding answers and adaptively optimizes the model via direct preference optimization (DPO). Note that during this process, GRATH learns truthfulness in a self-supervised manner without requiring annotated answers. In particular, GRATH first generates pairwise truthfulness training data by prompting the LLM itself, with each pair containing a question and its correct and incorrect answers. The model is then fine-tuned using DPO to learn from the difference between answer pairs. Subsequently, GRATH iteratively refines the 
    
[^102]: 通过思维方程蒸馏改进小型语言模型的数学推理能力

    Improving Small Language Models' Mathematical Reasoning via Equation-of-Thought Distillation. (arXiv:2401.11864v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2401.11864](http://arxiv.org/abs/2401.11864)

    本研究提出了思维方程蒸馏（EoTD）技术和集合思维蒸馏（ETD）框架，通过构建基于方程的表示和使用多个思维过程的推理数据集来改进小型语言模型（SLMs）的数学推理能力，实验结果表明，EoTD和ETD显著提升了SLMs的推理能力。

    

    本研究解决了将先进的大型语言模型（LLMs）的数学推理能力压缩到具有小于十亿参数的小型语言模型（SLMs）中的挑战，同时不损害性能。我们引入了一种新颖的思维方程蒸馏（EoTD）技术，将推理过程封装为基于方程的表示，构建了一个EoTD数据集来对SLMs进行微调。此外，我们提出了集合思维蒸馏（ETD）框架，以提升SLMs的推理性能。这包括创建一个包含多个思维过程（包括思维链、思维程序和思维方程）的推理数据集，并将其用于微调。我们的实验证明，EoTD显著提升了SLMs的推理能力，而ETD使这些模型实现了最先进的推理性能。

    This work addresses the challenge of democratizing advanced Large Language Models (LLMs) by compressing their mathematical reasoning capabilities into sub-billion parameter Small Language Models (SLMs) without compromising performance. We introduce Equation-of-Thought Distillation (EoTD), a novel technique that encapsulates the reasoning process into equation-based representations to construct an EoTD dataset for fine-tuning SLMs. Additionally, we propose the Ensemble Thoughts Distillation (ETD) framework to enhance the reasoning performance of SLMs. This involves creating a reasoning dataset with multiple thought processes, including Chain-of-Thought (CoT), Program-of-Thought (PoT), and Equation-of-Thought (EoT), and using it for fine-tuning. Our experimental findings demonstrate that EoTD significantly boosts the reasoning abilities of SLMs, while ETD enables these models to achieve state-of-the-art reasoning performance.
    
[^103]: 高斯自适应注意力是唯一所需的：跨多个模态的健壮上下文表示

    Gaussian Adaptive Attention is All You Need: Robust Contextual Representations Across Multiple Modalities. (arXiv:2401.11143v1 [cs.LG])

    [http://arxiv.org/abs/2401.11143](http://arxiv.org/abs/2401.11143)

    该论文提出了一个名为GAAM的多头高斯自适应注意力机制，用于增强跨多个模态的信息聚合。通过将可学习的均值和方差纳入注意力机制中，GAAM能够动态地重新调整特征的重要性，从而在处理非平稳数据时取得了显著的性能提升，超过了目前现有的注意力技术。该方法的适应性强且参数数量较少，具有改进现有注意力框架的潜力。

    

    我们提出了多头高斯自适应注意力机制（GAAM），一种新颖的概率注意力框架，并设计了高斯自适应变压器（GAT），旨在增强跨多个模态（包括语音、文本和视觉）的信息聚合。GAAM将可学习的均值和方差融入其注意力机制中，采用多头框架实现，使其能够集体建模任何概率分布，以动态重新调整特征重要性。该方法在处理高度非平稳数据时表现出显著改进，通过识别特征空间中的关键元素，超越了现有的注意力技术在模型性能上的状态（精度增加约20%）。GAAM与基于点积的注意力模型兼容，并具有相对较低的参数数量，展示了其适应性和提升现有注意力框架的潜力。在实证方面，GAAM表现出卓越的适应性和功效。

    We propose the Multi-Head Gaussian Adaptive Attention Mechanism (GAAM), a novel probabilistic attention framework, and the Gaussian Adaptive Transformer (GAT), designed to enhance information aggregation across multiple modalities, including Speech, Text and Vision. GAAM integrates learnable mean and variance into its attention mechanism, implemented in a Multi-Headed framework enabling it to collectively model any Probability Distribution for dynamic recalibration of feature significance. This method demonstrates significant improvements, especially with highly non-stationary data, surpassing the state-of-the-art attention techniques in model performance (up to approximately +20% in accuracy) by identifying key elements within the feature space. GAAM's compatibility with dot-product-based attention models and relatively low number of parameters showcases its adaptability and potential to boost existing attention frameworks. Empirically, GAAM exhibits superior adaptability and efficacy
    
[^104]: 结合置信度引导和基于样本的方法用于消除误信息中的不确定性量化

    Combining Confidence Elicitation and Sample-based Methods for Uncertainty Quantification in Misinformation Mitigation. (arXiv:2401.08694v1 [cs.CL])

    [http://arxiv.org/abs/2401.08694](http://arxiv.org/abs/2401.08694)

    本研究提出了一种结合置信度引导和基于样本的方法的不确定性量化框架，用于解决误信息消除中的幻觉和过度自信的预测问题，并提出了混合框架以提供更好的不确定性估计。

    

    大型语言模型已经成为解决误信息消除的主要候选方案。然而，现有方法在幻觉和过度自信的预测方面存在问题。我们提出了一种不确定性量化框架，利用直接置信度引导和基于样本的一致性方法，为自然语言处理误信息消除解决方案提供更好的校准。首先，我们研究基于样本一致性方法的校准性，该方法利用样本规模和随机水平的一致性的不同特征。接下来，我们评估了鲁棒的数字化口头提示在单步和两步置信度引导过程中的性能和分布变化。我们还比较了相同提示在不同版本的GPT和不同数字尺度下的性能。最后，我们结合基于样本一致性和数字化方法，提出了一个混合框架，为GPT模型提供更好的不确定性估计。

    Large Language Models have emerged as prime candidates to tackle misinformation mitigation. However, existing approaches struggle with hallucinations and overconfident predictions. We propose an uncertainty quantification framework that leverages both direct confidence elicitation and sampled-based consistency methods to provide better calibration for NLP misinformation mitigation solutions. We first investigate the calibration of sample-based consistency methods that exploit distinct features of consistency across sample sizes and stochastic levels. Next, we evaluate the performance and distributional shift of a robust numeric verbalization prompt across single vs. two-step confidence elicitation procedure. We also compare the performance of the same prompt with different versions of GPT and different numerical scales. Finally, we combine the sample-based consistency and verbalized methods to propose a hybrid framework that yields a better uncertainty estimation for GPT models. Overal
    
[^105]: Ada-Retrieval：适应性多轮检索范例用于顺序推荐

    Ada-Retrieval: An Adaptive Multi-Round Retrieval Paradigm for Sequential Recommendations. (arXiv:2401.06633v1 [cs.IR])

    [http://arxiv.org/abs/2401.06633](http://arxiv.org/abs/2401.06633)

    Ada-Retrieval是一种适应性多轮检索范例，用于提升推荐系统的物品候选者选择过程。它通过迭代地改进用户表示来更好地捕捉完整的物品空间中的潜在候选者，并具有模型无关的设计。

    

    检索模型旨在选择与给定用户偏好匹配的一小组物品候选者。它们在大规模推荐系统中起着重要作用，因为后续的模型（如排名器）高度依赖于物品候选者的质量。然而，大多数现有的检索模型采用单轮推理范例，可能无法充分捕捉用户偏好的动态性并固定在物品空间的某个区域。在本文中，我们提出了Ada-Retrieval，一种适用于推荐系统的自适应多轮检索范例，通过迭代地改进用户表示来更好地捕捉完整的物品空间中的潜在候选者。Ada-Retrieval包含两个关键模块：物品表示适配器和用户表示适配器，旨在将上下文信息注入物品和用户的表示中。该框架具有模型无关的设计，可以与各种基础模型（如RNN或Transformer）无缝集成。

    Retrieval models aim at selecting a small set of item candidates which match the preference of a given user. They play a vital role in large-scale recommender systems since subsequent models such as rankers highly depend on the quality of item candidates. However, most existing retrieval models employ a single-round inference paradigm, which may not adequately capture the dynamic nature of user preferences and stuck in one area in the item space. In this paper, we propose Ada-Retrieval, an adaptive multi-round retrieval paradigm for recommender systems that iteratively refines user representations to better capture potential candidates in the full item space. Ada-Retrieval comprises two key modules: the item representation adapter and the user representation adapter, designed to inject context information into items' and users' representations. The framework maintains a model-agnostic design, allowing seamless integration with various backbone models such as RNNs or Transformers. We pe
    
[^106]: 通过遥感图像和多语义信息检测城市功能区的多模态学习

    Multimodal Learning for detecting urban functional zones using remote sensing image and multi-semantic information. (arXiv:2401.06550v1 [cs.CV])

    [http://arxiv.org/abs/2401.06550](http://arxiv.org/abs/2401.06550)

    本研究提出了一种利用遥感图像和多语义信息进行城市功能区检测的多模态学习算法，能够满足移动互联网在线到离线业务的精确要求。

    

    城市兴趣区（AOI）是指具有定义边界的整合的城市功能区域。城市商业的迅速发展导致了对定义AOI的更精确要求的增加。然而，现有研究主要集中于城市规划或区域经济分析的广泛AOI挖掘，未能满足移动互联网在线到离线业务的精确要求。这些业务需要到具体的社区、学校或医院的准确度。在本文中，我们提出了一种端到端的多模态深度学习算法，用于使用遥感图像和多语义参考信息检测AOI围栏多边形。然后，我们通过包含动态人员流动和物流地址信息的级联模块来评估其时效性。具体而言，我们从选择特定类别的兴趣点（POI）开始，并用它来召回相应的遥感图像、附近的POI、道路n

    Urban area-of-interest (AOI) refers to an integrated urban functional zone with defined boundaries. The rapid development of urban commerce has resulted in an increased demand for more precise requirements in defining AOIs. However, existing research primarily concentrates on broad AOI mining for urban planning or regional economic analysis, failing to cater to the precise requirements of mobile Internet online-to-offline businesses. These businesses necessitate accuracy down to a specific community, school, or hospital. In this paper, we propose an end-to-end multimodal deep learning algorithm for detecting AOI fence polygon using remote sensing images and multi-semantics reference information. We then evaluate its timeliness through a cascaded module that incorporates dynamic human mobility and logistics address information. Specifically, we begin by selecting a point-of-interest (POI) of specific category, and use it to recall corresponding remote sensing images, nearby POIs, road n
    
[^107]: 强化微调语言模型中的梯度消失问题

    Vanishing Gradients in Reinforcement Finetuning of Language Models. (arXiv:2310.20703v1 [cs.LG])

    [http://arxiv.org/abs/2310.20703](http://arxiv.org/abs/2310.20703)

    本研究发现在强化微调（RFT）中存在梯度消失的问题，当模型下奖励的标准差较小时，输入的期望梯度会消失，导致奖励最大化缓慢。初始监督微调（SFT）阶段是克服这个问题的最有希望的方法。

    

    预训练的语言模型通过强化微调（RFT）与人类偏好和下游任务对齐，即使用策略梯度算法最大化（可能是学习得到的）奖励函数。本研究发现了RFT中的一个基本的优化障碍：我们证明了当模型下的奖励标准差较小时，输入的期望梯度会消失，即使期望奖励远离最优解。通过在RFT基准和控制环境中进行实验，以及理论分析，我们证明了由于小的奖励标准差导致的梯度消失问题普遍存在且有害，导致奖励最大化极其缓慢。最后，我们探索了克服RFT中梯度消失的方法。我们发现初始监督微调（SFT）阶段是最有希望的候选方法，并且揭示了它在RFT流程中的重要性。此外，我们还表明相对较小的训练数据集的SFT阶段可以有效克服梯度消失问题。

    Pretrained language models are commonly aligned with human preferences and downstream tasks via reinforcement finetuning (RFT), which entails maximizing a (possibly learned) reward function using policy gradient algorithms. This work highlights a fundamental optimization obstacle in RFT: we prove that the expected gradient for an input vanishes when its reward standard deviation under the model is small, even if the expected reward is far from optimal. Through experiments on an RFT benchmark and controlled environments, as well as a theoretical analysis, we then demonstrate that vanishing gradients due to small reward standard deviation are prevalent and detrimental, leading to extremely slow reward maximization. Lastly, we explore ways to overcome vanishing gradients in RFT. We find the common practice of an initial supervised finetuning (SFT) phase to be the most promising candidate, which sheds light on its importance in an RFT pipeline. Moreover, we show that a relatively small num
    
[^108]: GPT-4V在医学影像中的多模态能力的全面研究

    A Comprehensive Study of GPT-4V's Multimodal Capabilities in Medical Imaging. (arXiv:2310.20381v1 [cs.CV])

    [http://arxiv.org/abs/2310.20381](http://arxiv.org/abs/2310.20381)

    本文对GPT-4V在医学影像中的多模态能力进行了全面研究和评估，发现其在生成描述性报告和医学VQA方面有潜力，但在某些评估指标上仍需改进。

    

    本文对GPT-4V在不同医学影像任务中的能力进行了全面评估，包括放射学报告生成、医学视觉问答(VQA)和视觉定位。尽管先前的研究探索了GPT-4V在医学影像中的性能，但据我们所知，我们的研究是首个基于公开可用基准的定量评估。我们的研究发现，当给出结构良好的提示时，GPT-4V在胸部X射线图像的生成描述性报告方面具有潜力。然而，在MIMIC-CXR数据集基准上的表现揭示了某些评估指标(如CIDEr)的改进空间。在医学VQA领域，GPT-4V在区分问题类型方面表现出熟练，但在准确度方面不及现有基准。此外，我们的分析发现常规评估指标如BLEU分数的局限性，呼吁开发更好的评价指标。

    This paper presents a comprehensive evaluation of GPT-4V's capabilities across diverse medical imaging tasks, including Radiology Report Generation, Medical Visual Question Answering (VQA), and Visual Grounding. While prior efforts have explored GPT-4V's performance in medical imaging, to the best of our knowledge, our study represents the first quantitative evaluation on publicly available benchmarks. Our findings highlight GPT-4V's potential in generating descriptive reports for chest X-ray images, particularly when guided by well-structured prompts. However, its performance on the MIMIC-CXR dataset benchmark reveals areas for improvement in certain evaluation metrics, such as CIDEr. In the domain of Medical VQA, GPT-4V demonstrates proficiency in distinguishing between question types but falls short of prevailing benchmarks in terms of accuracy. Furthermore, our analysis finds the limitations of conventional evaluation metrics like the BLEU score, advocating for the development of m
    
[^109]: 机器学习模型的成员推断攻击的基本限制

    Fundamental Limits of Membership Inference Attacks on Machine Learning Models. (arXiv:2310.13786v1 [stat.ML])

    [http://arxiv.org/abs/2310.13786](http://arxiv.org/abs/2310.13786)

    本文探讨了机器学习模型上成员推断攻击的基本限制，包括推导了效果和成功率的统计量，并提供了几种情况下的界限。这使得我们能够根据样本数量和其他结构参数推断潜在攻击的准确性。

    

    成员推断攻击（MIA）可以揭示特定数据点是否是训练数据集的一部分，可能暴露个人的敏感信息。本文探讨了关于机器学习模型上MIA的基本统计限制。具体而言，我们首先推导了统计量，该统计量决定了这种攻击的有效性和成功率。然后，我们研究了几种情况，并对这个感兴趣的统计量提供了界限。这使我们能够根据样本数量和学习模型的其他结构参数推断潜在攻击的准确性，在某些情况下可以直接从数据集中估计。

    Membership inference attacks (MIA) can reveal whether a particular data point was part of the training dataset, potentially exposing sensitive information about individuals. This article explores the fundamental statistical limitations associated with MIAs on machine learning models. More precisely, we first derive the statistical quantity that governs the effectiveness and success of such attacks. Then, we investigate several situations for which we provide bounds on this quantity of interest. This allows us to infer the accuracy of potential attacks as a function of the number of samples and other structural parameters of learning models, which in some cases can be directly estimated from the dataset.
    
[^110]: 在带有可通行障碍的环境中使用大型语言和视觉-语言模型进行交互式导航

    Interactive Navigation in Environments with Traversable Obstacles Using Large Language and Vision-Language Models. (arXiv:2310.08873v1 [cs.RO] CROSS LISTED)

    [http://arxiv.org/abs/2310.08873](http://arxiv.org/abs/2310.08873)

    本文提出了一个使用大型语言和视觉-语言模型的交互式导航框架，使机器人能够在带有可通行障碍的环境中进行导航。通过使用这些模型，我们可以实现从文本指令到动作感知边界框的端到端系统，无需微调和额外的训练数据。同时，我们还使用大型模型划分激光雷达点云，生成动作感知成本地图以生成可行路径。

    

    本文提出了一种使用大型语言和视觉-语言模型的交互式导航框架，使机器人能够在带有可通行障碍的环境中进行导航。我们利用大型语言模型(GPT-3.5)和开放式视觉-语言模型(基于Grounding DINO)创建了一个动作感知成本地图，用于进行有效的路径规划而无需微调。通过大型模型，我们可以实现从文本指令（例如“你能通过窗帘给我送药吗？”）到具有动作感知属性的边界框（例如窗帘）的端到端系统。它们可以用于将激光雷达点云分成两部分：可通行和不可通行部分，然后构建一个动作感知成本地图用于生成可行路径。预训练的大型模型具有很强的泛化能力，不需要额外的注释数据进行训练，可以快速部署于交互式导航任务中。我们选择使用多个可通行对象。

    This paper proposes an interactive navigation framework by using large language and vision-language models, allowing robots to navigate in environments with traversable obstacles. We utilize the large language model (GPT-3.5) and the open-set Vision-language Model (Grounding DINO) to create an action-aware costmap to perform effective path planning without fine-tuning. With the large models, we can achieve an end-to-end system from textual instructions like "Can you pass through the curtains to deliver medicines to me?", to bounding boxes (e.g., curtains) with action-aware attributes. They can be used to segment LiDAR point clouds into two parts: traversable and untraversable parts, and then an action-aware costmap is constructed for generating a feasible path. The pre-trained large models have great generalization ability and do not require additional annotated data for training, allowing fast deployment in the interactive navigation tasks. We choose to use multiple traversable object
    
[^111]: MAPLE: 基于大型语言模型嵌入的移动应用预测

    MAPLE: Mobile App Prediction Leveraging Large Language model Embeddings. (arXiv:2309.08648v1 [cs.CL])

    [http://arxiv.org/abs/2309.08648](http://arxiv.org/abs/2309.08648)

    MAPLE是一个利用大型语言模型嵌入进行移动应用预测的模型，通过严格测试验证了其在解密复杂模式和理解用户环境方面的能力，并强调了语言模型在不同领域中的广泛适用性。

    

    尽管移动应用的发展迅速，但由于复杂的用户行为和不断演变的环境，预测应用的使用仍然是一个严峻的挑战。为了解决这些问题，本文介绍了Mobile App Prediction Leveraging Large Language Model Embeddings (MAPLE)模型。这种创新的方法利用大型语言模型(LLM)来准确预测应用的使用情况。通过对两个公开数据集进行严格测试，MAPLE的能力在解密复杂模式和理解用户环境方面得到了验证。这些强大的结果证实了MAPLE在不同场景中的多功能性和弹性。尽管其主要设计面向应用预测，但结果也强调了LLM在不同领域中的广泛适用性。通过这项研究，我们强调了LLM在应用使用预测中的潜力，并建议在建模各种领域中的人类行为方面，它们具有变革能力。

    Despite the rapid advancement of mobile applications, predicting app usage remains a formidable challenge due to intricate user behaviours and ever-evolving contexts. To address these issues, this paper introduces the Mobile App Prediction Leveraging Large Language Model Embeddings (MAPLE) model. This innovative approach utilizes Large Language Models (LLMs) to predict app usage accurately. Rigorous testing on two public datasets highlights MAPLE's capability to decipher intricate patterns and comprehend user contexts. These robust results confirm MAPLE's versatility and resilience across various scenarios. While its primary design caters to app prediction, the outcomes also emphasize the broader applicability of LLMs in different domains. Through this research, we emphasize the potential of LLMs in app usage prediction and suggest their transformative capacity in modelling human behaviours across diverse fields.
    
[^112]: 结合深度学习和街景图像来绘制小农户农作物类型的地图

    Combining deep learning and street view imagery to map smallholder crop types. (arXiv:2309.05930v1 [cs.CV])

    [http://arxiv.org/abs/2309.05930](http://arxiv.org/abs/2309.05930)

    本研究利用深度学习和街景图像开发了一个自动化系统，可以生成农作物类型地面参考，解决了在低收入和中等收入国家创建农作物类型地图时的挑战。

    

    准确的农作物类型地图对于监测规模的产量进展、预测全球农作物生产和制定有效政策是一种必要的信息来源。然而，由于在训练机器学习模型时缺乏地面真实标签，迄今为止，低收入和中等收入国家的农作物类型地图制作仍具有挑战性。田间调查在准确性方面是黄金标准，但需要大量的时间、金钱和统计能力。近年来，全球范围内提供了街景图像（如Google Street View，KartaView和Mapillary）。这些图像包含了特定位置和时间的农作物种类的丰富信息。在这项工作中，我们开发了一个使用深度学习和Google Street View图像生成农作物类型地面参考的自动化系统。该方法有效地筛选了一组包含农田的街景图像，通过利用弱标签训练模型来预测农作物类型。

    Accurate crop type maps are an essential source of information for monitoring yield progress at scale, projecting global crop production, and planning effective policies. To date, however, crop type maps remain challenging to create in low and middle-income countries due to a lack of ground truth labels for training machine learning models. Field surveys are the gold standard in terms of accuracy but require an often-prohibitively large amount of time, money, and statistical capacity. In recent years, street-level imagery, such as Google Street View, KartaView, and Mapillary, has become available around the world. Such imagery contains rich information about crop types grown at particular locations and times. In this work, we develop an automated system to generate crop type ground references using deep learning and Google Street View imagery. The method efficiently curates a set of street view images containing crop fields, trains a model to predict crop type by utilizing weakly-label
    
[^113]: 双重关系对齐用于组合图像检索

    Dual Relation Alignment for Composed Image Retrieval. (arXiv:2309.02169v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2309.02169](http://arxiv.org/abs/2309.02169)

    本研究提出了双重关系对齐的方法，用于组合图像检索任务。通过利用显性和隐性关系，可以更好地学习网络并提升检索性能。

    

    组合图像检索是一项通过使用参考图像和补充文本作为查询，来搜索目标图像的任务。随着跨模态建模的进展，这一任务取得了显著的进展。与仅存在一种对齐关系的一般图像-文本检索问题不同，我们认为在组合图像检索中存在着两种类型的关系。显性关系涉及参考图像 & 补充文本-目标图像，这是现有方法常用的关系。除了这种直观关系之外，我们在实践中观察到另一种隐含但关键的关系，即参考图像 & 目标图像-补充文本。因为我们发现，通过研究目标图像和参考图像之间的关系，可以推断出补充文本。可惜的是，现有方法主要关注利用显性关系来学习网络，而忽视了隐性关系。

    Composed image retrieval, a task involving the search for a target image using a reference image and a complementary text as the query, has witnessed significant advancements owing to the progress made in cross-modal modeling. Unlike the general image-text retrieval problem with only one alignment relation, i.e., image-text, we argue for the existence of two types of relations in composed image retrieval. The explicit relation pertains to the reference image & complementary text-target image, which is commonly exploited by existing methods. Besides this intuitive relation, the observations during our practice have uncovered another implicit yet crucial relation, i.e., reference image & target image-complementary text, since we found that the complementary text can be inferred by studying the relation between the target image and the reference image. Regrettably, existing methods largely focus on leveraging the explicit relation to learn their networks, while overlooking the implicit re
    
[^114]: 一个用于完全无监督异常检测的通用机器学习框架，适用于污染数据

    A Generic Machine Learning Framework for Fully-Unsupervised Anomaly Detection with Contaminated Data. (arXiv:2308.13352v1 [cs.LG])

    [http://arxiv.org/abs/2308.13352](http://arxiv.org/abs/2308.13352)

    这篇论文介绍了一个完全无监督的机器学习框架，用于处理在训练数据中含有异常样本的异常检测任务。

    

    在各个领域和应用中，机器学习算法已经解决了异常检测（AD）任务。这些算法中大多数使用正常数据对一个基于残差的模型进行训练，并根据未见样本与学习到的正常范围的不相似性来分配异常分数。这些方法的基本假设是可以用无异常的数据进行训练。然而，在真实世界中的操作环境中，训练数据通常会与一定比例的异常样本混合。而利用污染数据进行训练必然会导致基于残差的算法的AD性能下降。本文介绍了一个完全无监督的用于AD任务的污染训练数据的改进框架。该框架是通用的，可应用于任何基于残差的机器学习模型。我们展示了该框架在两个多元时间数据集上的应用。

    Anomaly detection (AD) tasks have been solved using machine learning algorithms in various domains and applications. The great majority of these algorithms use normal data to train a residual-based model, and assign anomaly scores to unseen samples based on their dissimilarity with the learned normal regime. The underlying assumption of these approaches is that anomaly-free data is available for training. This is, however, often not the case in real-world operational settings, where the training data may be contaminated with a certain fraction of abnormal samples. Training with contaminated data, in turn, inevitably leads to a deteriorated AD performance of the residual-based algorithms.  In this paper we introduce a framework for a fully unsupervised refinement of contaminated training data for AD tasks. The framework is generic and can be applied to any residual-based machine learning model. We demonstrate the application of the framework to two public datasets of multivariate time s
    
[^115]: 交通流量优化的终身多智能体路径规划

    Traffic Flow Optimisation for Lifelong Multi-Agent Path Finding. (arXiv:2308.11234v1 [cs.AI])

    [http://arxiv.org/abs/2308.11234](http://arxiv.org/abs/2308.11234)

    本文提出了一种新的终身多智能体路径规划方法，通过引导智能体避开拥堵路径来优化交通流量，显著提高解决方案质量和总体吞吐量。

    

    多智能体路径规划(MAPF)是机器人领域的一个基本问题，要求为一个团队的智能体计算无碰撞路径，所有智能体都在共享地图上移动。尽管有许多相关研究，但当前的算法在智能体数量增加时都会遇到困难。主要原因是现有方法通常规划自由流动的最优路径，这会导致拥堵。为了解决这个问题，我们提出了一种新的MAPF方法，通过跟随避免拥堵的路径来引导智能体到达目的地。我们在两个大规模场景中评估了这个想法：一次性MAPF，每个智能体只有一个目的地，以及终身MAPF，智能体不断被分配新任务。对于一次性MAPF，我们展示了我们的方法大大提高了解决方案的质量。对于终身MAPF，我们报告了总体吞吐量的大幅提升。

    Multi-Agent Path Finding (MAPF) is a fundamental problem in robotics that asks us to compute collision-free paths for a team of agents, all moving across a shared map. Although many works appear on this topic, all current algorithms struggle as the number of agents grows. The principal reason is that existing approaches typically plan free-flow optimal paths, which creates congestion. To tackle this issue we propose a new approach for MAPF where agents are guided to their destination by following congestion-avoiding paths. We evaluate the idea in two large-scale settings: one-shot MAPF, where each agent has a single destination, and lifelong MAPF, where agents are continuously assigned new tasks. For one-shot MAPF we show that our approach substantially improves solution quality. For Lifelong MAPF we report large improvements in overall throughput.
    
[^116]: ConcatPlexer：通过附加Dim1批处理以加快ViTs速度

    ConcatPlexer: Additional Dim1 Batching for Faster ViTs. (arXiv:2308.11199v1 [cs.CV])

    [http://arxiv.org/abs/2308.11199](http://arxiv.org/abs/2308.11199)

    本文提出了一种名为ConcatPlexer的方法，通过在视觉识别中使用附加的Dim1批处理（即连接）来提高吞吐量，同时准确性受到的影响较小。

    

    Transformer不仅在自然语言处理领域，还在计算机视觉领域取得了巨大成功，引发了各种创新的方法和应用。然而，Transformer卓越的性能和建模灵活性带来了计算成本的严重增加，因此有几项工作提出了减少这种负担的方法。受语言模型的一种减少成本的方法Data Multiplexing (DataMUX)的启发，我们提出了一种用于高效视觉识别的新方法，它采用了附加的Dim1批处理（即连接），在保证准确性的基础上大大提高了吞吐量。我们首先为视觉模型引入了DataMux的一种天然适应方法，图像多路复用器（Image Multiplexer），并设计了新的组件来克服其缺点，进而形成了我们最终的模型ConcatPlexer，在推理速度和准确度之间找到了平衡点。ConcatPlexer在ImageNet1K和CIFAR100数据集上进行了训练。

    Transformers have demonstrated tremendous success not only in the natural language processing (NLP) domain but also the field of computer vision, igniting various creative approaches and applications. Yet, the superior performance and modeling flexibility of transformers came with a severe increase in computation costs, and hence several works have proposed methods to reduce this burden. Inspired by a cost-cutting method originally proposed for language models, Data Multiplexing (DataMUX), we propose a novel approach for efficient visual recognition that employs additional dim1 batching (i.e., concatenation) that greatly improves the throughput with little compromise in the accuracy. We first introduce a naive adaptation of DataMux for vision models, Image Multiplexer, and devise novel components to overcome its weaknesses, rendering our final model, ConcatPlexer, at the sweet spot between inference speed and accuracy. The ConcatPlexer was trained on ImageNet1K and CIFAR100 dataset and
    
[^117]: 基于语义引导的基于Transformer的传感器融合方法改进航点预测

    Semantics-guided Transformer-based Sensor Fusion for Improved Waypoint Prediction. (arXiv:2308.02126v1 [cs.RO])

    [http://arxiv.org/abs/2308.02126](http://arxiv.org/abs/2308.02126)

    该论文提出一种基于语义引导的传感器融合方法，通过融合多个传感器的特征和使用辅助任务来改进自动驾驶代理的航点预测。

    

    对于智能自动驾驶代理来说，传感器融合方法仍然是驾驶场景理解的关键，通过从输入传感器获取的视觉全局环境。具体而言，对于局部航点预测任务，单模态网络仍然受限于对输入传感器的灵敏度的强依赖性，因此最近的工作推广了在特征级别上融合多个传感器的使用。虽然众所周知多个数据模态能促进相互的上下文交互，但在实际驾驶场景中实时进行全局三维场景理解并进行最小计算，因此在给定有限数量的可实际使用的传感器的情况下，对训练策略的重要性更加突出。在这一背景下，我们通过融合与目标任务（如交通灯识别和语义分割）高度相关的精心选取的辅助任务特征，并使用辅助头为航点预测进行了利用。

    Sensor fusion approaches for intelligent self-driving agents remain key to driving scene understanding given visual global contexts acquired from input sensors. Specifically, for the local waypoint prediction task, single-modality networks are still limited by strong dependency on the sensitivity of the input sensor, and thus recent works promote the use of multiple sensors in fusion in feature level. While it is well known that multiple data modalities promote mutual contextual exchange, deployment to practical driving scenarios requires global 3D scene understanding in real-time with minimal computations, thus placing greater significance on training strategies given a limited number of practically usable sensors. In this light, we exploit carefully selected auxiliary tasks that are highly correlated with the target task of interest (e.g., traffic light recognition and semantic segmentation) by fusing auxiliary task features and also using auxiliary heads for waypoint prediction base
    
[^118]: RCT拒绝抽样用于因果估计评估

    RCT Rejection Sampling for Causal Estimation Evaluation. (arXiv:2307.15176v1 [cs.AI])

    [http://arxiv.org/abs/2307.15176](http://arxiv.org/abs/2307.15176)

    该论文提出了一种名为RCT拒绝抽样的新抽样算法，用于因果估计评估。该方法通过子抽样随机控制试验(RCT)创建混淆的观测数据集，并使用RCT的平均因果效应作为基准真实值，以进行有效比较。

    

    混淆是从观测数据中无偏估计因果效应的一个重要障碍。对于高维协变量的情况，如文本数据、基因组学或行为社会科学，研究人员提出了适应机器学习方法进行因果估计的调整方法。然而，这些调整方法的经验评估一直存在困难和限制。在这项工作中，我们基于一种有前景的经验评估策略，简化了评估设计，并使用真实数据：对随机控制试验(RCT)进行子抽样，以创建混淆的观测数据集，同时使用RCT的平均因果效应作为基准真实值。我们提出了一种新的抽样算法，称为RCT拒绝抽样，并提供了理论保证，以确保观测数据的因果识别成立，从而可以与基准RCT进行有效比较。通过使用合成数据，我们展示了我们的算法在...

    Confounding is a significant obstacle to unbiased estimation of causal effects from observational data. For settings with high-dimensional covariates -- such as text data, genomics, or the behavioral social sciences -researchers have proposed methods to adjust for confounding by adapting machine learning methods to the goal of causal estimation. However, empirical evaluation of these adjustment methods has been challenging and limited. In this work, we build on a promising empirical evaluation strategy that simplifies evaluation design and uses real data: subsampling randomized controlled trials (RCTs) to create confounded observational datasets while using the average causal effects from the RCTs as ground-truth. We contribute a new sampling algorithm, which we call RCT rejection sampling, and provide theoretical guarantees that causal identification holds in the observational data to allow for valid comparisons to the ground-truth RCT. Using synthetic data, we show our algorithm in
    
[^119]: GEAR: 与通用化和高效解决方案增强语言模型

    GEAR: Augmenting Language Models with Generalizable and Efficient Tool Resolution. (arXiv:2307.08775v1 [cs.AI])

    [http://arxiv.org/abs/2307.08775](http://arxiv.org/abs/2307.08775)

    GEAR是一种通用且高效的工具解决方案，通过将工具对应和执行分别委托给小型语言模型和大型语言模型，在不依赖任务示范的情况下实现了更高的性能和精确度。

    

    通过增加外部工具来增强大型语言模型（LLM）可以提高其在各种任务中的性能。然而，先前的研究过于依赖特定任务的工具使用示范，限制了其通用性，并且由于对大规模LLM进行多次调用而增加了计算成本。我们引入了GEAR，一种计算效率高的查询-工具对应算法，可以适用于不依赖特定任务示范的各种需要使用工具的任务。GEAR通过将工具对应和执行分别委托给小型语言模型（SLM）和LLM来实现更高的效率；同时利用语义和基于模式的评估在问题和答案级别上进行通用化的工具对应。我们在6个下游任务的14个数据集上评估了GEAR，证明了它对于新任务、新工具和不同SLM的强大通用性。尽管提供更高的效率，但GEAR在工具对应中的精确性比使用LLM的先前策略更高。

    Augmenting large language models (LLM) to use external tools enhances their performance across a variety of tasks. However, prior works over-rely on task-specific demonstration of tool use that limits their generalizability and computational cost due to making many calls to large-scale LLMs. We introduce GEAR, a computationally efficient query-tool grounding algorithm that is generalizable to various tasks that require tool use while not relying on task-specific demonstrations. GEAR achieves better efficiency by delegating tool grounding and execution to small language models (SLM) and LLM, respectively; while leveraging semantic and pattern-based evaluation at both question and answer levels for generalizable tool grounding. We evaluate GEAR on 14 datasets across 6 downstream tasks, demonstrating its strong generalizability to novel tasks, tools and different SLMs. Despite offering more efficiency, GEAR achieves higher precision in tool grounding compared to prior strategies using LLM
    
[^120]: 什么是公平性？哲学的思考与对fairML的影响

    What Is Fairness? Philosophical Considerations and Implications For FairML. (arXiv:2205.09622v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.09622](http://arxiv.org/abs/2205.09622)

    本文探讨了公平性的哲学概念，提出了公平性和预测性能不是不可调和的对立面，并强调从数据收集到最终模型评估都需纳入伦理考虑。

    

    在公平性人工智能(fairML)领域，通过定义衡量模型公平性的度量和提出确保训练模型数据具有低公平性度量值的方法，来减轻人工智能(ML)产生的相关不公平性问题。然而，公平的基本概念，即"公平是什么"，很少被讨论，这造成了公平性研究在哲学领域几个世纪的讨论与近期被应用于机器学习领域之间的鸿沟。本文试图通过形式化一致性公平概念和将哲学思考转化为ADM系统中ML模型训练和评估的形式框架，来架起这一鸿沟。我们指出，不公平性问题可能已经存在，即使没有受保护性属性的存在，强调公平性和预测性能不是不可调和的对立面，而是前者实现的必要条件。我们提出的框架强调将伦理考虑纳入ML管道的所有阶段，从数据收集到最终部署模型的评估。

    A growing body of literature in fairness-aware ML (fairML) aspires to mitigate machine learning (ML)-related unfairness in automated decision making (ADM) by defining metrics that measure fairness of an ML model and by proposing methods that ensure that trained ML models achieve low values in those measures. However, the underlying concept of fairness, i.e., the question of what fairness is, is rarely discussed, leaving a considerable gap between centuries of philosophical discussion and recent adoption of the concept in the ML community. In this work, we try to bridge this gap by formalizing a consistent concept of fairness and by translating the philosophical considerations into a formal framework for the training and evaluation of ML models in ADM systems. We derive that fairness problems can already arise without the presence of protected attributes, pointing out that fairness and predictive performance are not irreconcilable counterparts, but rather that the latter is necessary to
    

