# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [The Definitive Guide to Policy Gradients in Deep Reinforcement Learning: Theory, Algorithms and Implementations.](http://arxiv.org/abs/2401.13662) | 本文提供了一个深度强化学习中策略梯度算法的综合指南，包括理论基础、实际实现和比较结果，并对正则化的益处进行了讨论。 |
| [^2] | [Inadequacy of common stochastic neural networks for reliable clinical decision support.](http://arxiv.org/abs/2401.13657) | 本研究调查了随机神经网络在临床应用中的可靠性，并发现常见的深度学习方法在数据转移情况下过于自信。这强调了对本地不确定性可靠估计及其向最终用户传达的重要性。 |
| [^3] | [Graph-Informed Neural Networks for Sparse Grid-Based Discontinuity Detectors.](http://arxiv.org/abs/2401.13652) | 本文提出了一种利用图信息神经网络和稀疏网格来检测不连续函数不连续界面的新方法，该方法在维度大于3的情况下表现出高效且准确的不连续性检测能力，在维度n = 2和n = 4的函数上进行的实验验证了其高效性和泛化能力，并具有可移植性和多功能性。 |
| [^4] | [How Good is ChatGPT at Face Biometrics? A First Look into Recognition, Soft Biometrics, and Explainability.](http://arxiv.org/abs/2401.13641) | 本研究初步探索了基于GPT-4的ChatGPT在面部生物识别中的表现。研究分析了ChatGPT在面部验证、软生物特征估计和结果可解释性方面的能力。ChatGPT的应用有望提高自动决策在人类场景中的解释性和透明度。 |
| [^5] | [Enhancing Image Retrieval : A Comprehensive Study on Photo Search using the CLIP Mode.](http://arxiv.org/abs/2401.13613) | CLIP模型为照片搜索带来显著的进展，通过学习图像和文本的共享表示空间，实现了跨模态理解，并实现了基于自然语言查询的高效准确的图像检索。它具有强大的泛化能力，可应用于零样本学习和少样本分类等任务。 |
| [^6] | [Non-Intrusive Speech Intelligibility Prediction for Hearing-Impaired Users using Intermediate ASR Features and Human Memory Models.](http://arxiv.org/abs/2401.13611) | 本研究使用中间ASR特征和人类记忆模型预测听障用户的语音可懂度，取得了显著的性能提升并降低了均方根误差。 |
| [^7] | [Stream-based perception for cognitive agents in mobile ecosystems.](http://arxiv.org/abs/2401.13604) | 这篇论文提出了一种基于流式感知的方法，使移动设备中的认知代理能够感知有意义的情境。研究通过众包案例展示了手机传感器数据如何被用于触发和指导自主、自利的代理协作送货到目的地。 |
| [^8] | [Graph Guided Question Answer Generation for Procedural Question-Answering.](http://arxiv.org/abs/2401.13594) | 本文提出了一种生成详尽的高质量训练数据的方法，用于训练紧凑的、任务特定的QA模型，从而在特定问答任务中与GPT变体模型具有竞争力。这种方法利用过程性文本的结构化特性，通过将每个步骤和整个流程表示为图形，并以图节点为条件，在详尽和可控的方式下自动生成QA对。 |
| [^9] | [Evaluation of General Large Language Models in Contextually Assessing Semantic Concepts Extracted from Adult Critical Care Electronic Health Record Notes.](http://arxiv.org/abs/2401.13588) | 在实际医疗领域中对大型语言模型（LLMs）进行更深入和实用的评估是必要的，该研究旨在评估LLMs在成人重症护理医学复杂环境中的表现。 |
| [^10] | [Prompt Weight Experiments for LLM Instruction Fine-Tuning.](http://arxiv.org/abs/2401.13586) | LLM指令微调中，对于短提示完成数据集，提示词标记分类损失加权（PLW）与性能呈负二次关系，而长提示完成数据集则不受PLW影响。 |
| [^11] | [Benchmarking the Fairness of Image Upsampling Methods.](http://arxiv.org/abs/2401.13555) | 这项工作提出了一个评估有条件生成模型性能和公平性的框架，并针对图像上采样应用创建了一个涵盖现代方法的基准测试。实证研究发现使用无偏训练集对结果至关重要，并揭示了不同算法对该问题的响应变化。 |
| [^12] | [Expressive Acoustic Guitar Sound Synthesis with an Instrument-Specific Input Representation and Diffusion Outpainting.](http://arxiv.org/abs/2401.13498) | 本文提出了一种采用乐器特定输入表示和扩散外扩技术的表现力丰富的声学吉他声音合成模型，通过定量和定性评估，展示了该模型比其他模型具有更高的音频质量和更加逼真的音色声音。 |
| [^13] | [Separable Physics-Informed Neural Networks for the solution of elasticity problems.](http://arxiv.org/abs/2401.13486) | 本文提出了一种基于分离的物理信息神经网络（SPINN）和深度能量方法（DEM）的弹性问题求解方法，该方法在收敛速度和精度上显著优于传统的物理信息神经网络（PINN），并且在复杂几何体上的线性弹性理论问题方面具有应用潜力。 |
| [^14] | [How AI Ideas Affect the Creativity, Diversity, and Evolution of Human Ideas: Evidence From a Large, Dynamic Experiment.](http://arxiv.org/abs/2401.13481) | AI思想对个体创造力没有影响，但增加了整体思想多样性的数量和变化速率。 |
| [^15] | [Growing from Exploration: A self-exploring framework for robots based on foundation models.](http://arxiv.org/abs/2401.13462) | 该论文提出了一个名为GExp的框架，可以使机器人在没有人类干预的情况下自主探索和学习。基于基础模型，GExp鼓励机器人通过自我生成的任务来理解和探索环境，并从有益的经验中获取技能。这使得机器人能够通过自我探索解决复杂任务。 |
| [^16] | [Multi-Agent Diagnostics for Robustness via Illuminated Diversity.](http://arxiv.org/abs/2401.13460) | MADRID是一种新方法，通过生成多样化的对抗场景来揭示预训练多Agent策略的战略漏洞，并通过遗憾值衡量漏洞的程度。 |
| [^17] | [Clue-Guided Path Exploration: An Efficient Knowledge Base Question-Answering Framework with Low Computational Resource Consumption.](http://arxiv.org/abs/2401.13444) | 该论文介绍了一种以低计算资源消耗为中心的高效知识库问答框架，通过引入线索引导路径探索的方式，将知识库与大型语言模型高效地融合，从而降低了对模型能力的要求，并在实验证明了其优越性能。 |
| [^18] | [Semi-Supervised Coupled Thin-Plate Spline Model for Rotation Correction and Beyond.](http://arxiv.org/abs/2401.13432) | 本文提出了一种半监督耦合薄板样条模型，用于处理旋转校正等图像变形任务。通过耦合多个薄板样条变换，有效消除了插值误差，突破了控制点数量瓶颈，并且通过半监督学习方法，可以在有限的带标注样本条件下，充分利用未标注样本来提高模型性能。 |
| [^19] | [Causal Perception.](http://arxiv.org/abs/2401.13408) | 这项研究提出了因果感知的概念，并将其应用于自动决策系统中。感知对决策的公平性有重要影响，因为公平性是与背景相关的，并且其解释取决于评判人是谁。 |
| [^20] | [Past, Present, Future: A Comprehensive Exploration of AI Use Cases in the UMBRELLA IoT Testbed.](http://arxiv.org/abs/2401.13346) | UMBRELLA是一个大规模的物联网试验平台，具有多个应用案例，包括自动街灯监控、数字孪生环境、联邦学习框架和容器化应用入侵检测。未来，UMBRELLA还有潜力用于智能城市和多机器人群感知应用。 |
| [^21] | [Full Bayesian Significance Testing for Neural Networks.](http://arxiv.org/abs/2401.13335) | 该论文提出了一种全贝叶斯神经网络显著性检验方法（nFBST），通过利用贝叶斯神经网络拟合非线性和多维关系，并计算证据值来替代传统方法中的理论推导，该方法能够测试全局、局部和实例级的显著性。 |
| [^22] | [Explainable Bayesian Optimization.](http://arxiv.org/abs/2401.13334) | 本论文介绍了一种可解释性贝叶斯优化的方法，通过TNTRules生成高质量的解释，填补了贝叶斯优化和可解释人工智能之间的间隙。 |
| [^23] | [Information That Matters: Exploring Information Needs of People Affected by Algorithmic Decisions.](http://arxiv.org/abs/2401.13324) | 本研究探讨了受算法决策影响的人的信息需求，发现解释往往不能满足他们的关注点，导致对监管框架的理解和遵守产生障碍。为了解决这个问题，研究团队提出了XAI初学者问题库，涵盖了就业预测和健康监测两个领域中受影响利益相关者的信息需求。 |
| [^24] | [Deep Learning for Improved Polyp Detection from Synthetic Narrow-Band Imaging.](http://arxiv.org/abs/2401.13315) | 本文提出了一种基于CycleGAN的框架，将使用常规白光成像（WLI）捕获的图像转化为合成窄带成像（SNBI），用于改进没有窄带成像（NBI）时的WLI上的息肉检测效果。研究结果表明，使用合成NBI图像进行目标检测可以实现比原始WLI图像更好的息肉检测。 |
| [^25] | [ConTextual: Evaluating Context-Sensitive Text-Rich Visual Reasoning in Large Multimodal Models.](http://arxiv.org/abs/2401.13311) | 本文介绍了一个新颖的基准ConTextual，用于评估能够进行上下文敏感的文本富有视觉推理的大型多模态模型。研究发现，目前最好的模型GPT-4V在抽象类别表现出色，但在整体性能上仍然落后于人类，存在改进的空间。 |
| [^26] | [Towards Explainable Harmful Meme Detection through Multimodal Debate between Large Language Models.](http://arxiv.org/abs/2401.13298) | 本文提出了一种通过大型语言模型之间的多模态辩论实现可解释的有害模因检测方法。通过推理有害和无害立场之间的相互矛盾理由，生成可读的解释，提升有害模因检测的效果。 |
| [^27] | [RefreshNet: Learning Multiscale Dynamics through Hierarchical Refreshing.](http://arxiv.org/abs/2401.13282) | RefreshNet是一个多尺度框架，通过分层刷新机制来学习复杂系统的动态，实现了计算效率和预测准确性的平衡。 |
| [^28] | [Can AI Assistants Know What They Don't Know?.](http://arxiv.org/abs/2401.13275) | 本文研究了AI助手是否能知道自己不知道的事情，并通过自然语言表达出来的问题。为了回答这个问题，我们构建了一个特定模型的"I don't know"（Idk）数据集，并与AI助手进行对齐。 |
| [^29] | [Audio-Infused Automatic Image Colorization by Exploiting Audio Scene Semantics.](http://arxiv.org/abs/2401.13270) | 本论文提出了一种使用音频场景语义进行自动图像上色的方法，通过引入音频作为辅助信息，降低了对场景语义理解的难度，并通过三个阶段的网络进行了实现。 |
| [^30] | [Designing Redistribution Mechanisms for Reducing Transaction Fees in Blockchains.](http://arxiv.org/abs/2401.13262) | 本研究设计了一种交易费重新分配机制，在分布式账本中减少交易手续费。这种机制使用折扣的形式重新分配VCG支付，以最小化交易手续费，并同时保证优化分配效率和用户激励兼容性。 |
| [^31] | [UniMS-RAG: A Unified Multi-source Retrieval-Augmented Generation for Personalized Dialogue Systems.](http://arxiv.org/abs/2401.13256) | 这项研究提出了一种统一多源检索增强生成系统（UniMS-RAG），通过统一知识源选择、知识检索和回复生成三个子任务，使语言模型能够根据需求自适应地检索证据和评估关联性，从而生成个性化的回复。 |
| [^32] | [From Random to Informed Data Selection: A Diversity-Based Approach to Optimize Human Annotation and Few-Shot Learning.](http://arxiv.org/abs/2401.13229) | 该论文介绍了一种基于多样性的方法，从而优化人类注释和少样本学习。传统的随机选择数据方法忽视了数据的特征和模型的需求，而该方法将考虑这些因素，以提高数据选择的效率。 |
| [^33] | [Scalable Link Prediction on Large-Scale Heterogeneous Graphs with Large Language Models.](http://arxiv.org/abs/2401.13227) | 本研究探索了在大规模异构图上应用大型语言模型进行图学习的方法，提出了LPNL框架用于可扩展链接预测。通过创新的提示语和采样流程，以及分而治之的策略，成功解决了大规模图中的信息过载问题，并在实验中表现出了优越的性能。 |
| [^34] | [TAT-LLM: A Specialized Language Model for Discrete Reasoning over Tabular and Textual Data.](http://arxiv.org/abs/2401.13223) | TAT-LLM是一种专门用于离散推理的语言模型，针对混合表格和文本数据上的问答任务。该模型通过分步流水线的方式，包括提取器、推理器和执行器，利用LLMs的强大能力来解决问题。而为了应对成本、延迟和数据安全风险等挑战，我们开发了TAT-LLM，一个专门针对此任务的较小LLM。 |
| [^35] | [TEPI: Taxonomy-aware Embedding and Pseudo-Imaging for Scarcely-labeled Zero-shot Genome Classification.](http://arxiv.org/abs/2401.13219) | 本研究提出了一种针对稀缺标记的零样本基因组分类问题的解决方法，称为TEPI。通过将基因组表示为伪图像并将其映射到分类感知的嵌入空间，我们能够捕捉物种的组成和系统分类关系，实现了零样本学习。 |
| [^36] | [AMANet: Advancing SAR Ship Detection with Adaptive Multi-Hierarchical Attention Network.](http://arxiv.org/abs/2401.13214) | 本文提出了一种自适应多层次注意力网络(AMANet)，用于合成孔径雷达(SAR)图像中的船舶检测。该方法通过学习多尺度特征和自适应聚合显著特征，解决了海岸环境中小型和沿海船舶检测的挑战。 |
| [^37] | [AdCorDA: Classifier Refinement via Adversarial Correction and Domain Adaptation.](http://arxiv.org/abs/2401.13212) | AdCorDA方法通过对抗修正和领域适应改进预训练分类器网络，实验证明在CIFAR-100数据集上能够显著提升准确率，并且在权重量化的神经网络上也表现出显著的性能提升。 |
| [^38] | [Boosting the Transferability of Adversarial Examples via Local Mixup and Adaptive Step Size.](http://arxiv.org/abs/2401.13205) | 本文提出了一个黑盒对抗性生成框架，通过本地混合和自适应步长的设计，增强了输入多样性，并且能够生成可迁移的对抗性样本。 |
| [^39] | [MLLMReID: Multimodal Large Language Model-based Person Re-identification.](http://arxiv.org/abs/2401.13201) | MLLMReID是一种基于多模态大语言模型的人物再识别方法，通过微调模型并将其视觉编码器作为主干进行优化，解决了MLLM在ReID任务中的设计指令和特征学习效果的问题。 |
| [^40] | [Catch-Up Mix: Catch-Up Class for Struggling Filters in CNN.](http://arxiv.org/abs/2401.13193) | 该论文提出了Catch-Up Mix方法，用于解决深度学习模型过于依赖特定滤波器的问题，并通过观察到慢学习滤波器因为快学习滤波器而失去学习机会，借鉴图像增强方法，提出了该方法来缓解这个问题。 |
| [^41] | [Generative Design of Crystal Structures by Point Cloud Representations and Diffusion Model.](http://arxiv.org/abs/2401.13192) | 本研究提出了一种基于点云和扩散模型的晶体结构生成设计框架，并通过重建输入结构和生成全新材料的实验证明了其有效性和潜力。 |
| [^42] | [AgentBoard: An Analytical Evaluation Board of Multi-turn LLM Agents.](http://arxiv.org/abs/2401.13178) | AgentBoard是一个综合的基准测试和评估框架，专为分析评估LLM智能体而设计，解决了在多轮交互和部分可观察环境中对智能体性能进行基准测试的挑战，并提供了细粒度的进展率指标和评估工具包。 |
| [^43] | [Compositional Generative Inverse Design.](http://arxiv.org/abs/2401.13171) | 逆向设计起到优化底层目标函数的作用，最近的研究利用了学习的动力学模型进行优化。通过优化扩散模型捕获的学习能量函数，可以避免对抗示例，并显著提高设计性能。这一设计系统是组合性的，使得可以设计具有每个指定组件的系统。 |
| [^44] | [Time-Aware Knowledge Representations of Dynamic Objects with Multidimensional Persistence.](http://arxiv.org/abs/2401.13157) | 本论文提出了一种新的时间感知知识表示机制，聚焦于多维拓扑信息和隐含的时间相关信息。该方法通过使用单参数拓扑摘要生成数据的多维拓扑指纹。 |
| [^45] | [Visibility into AI Agents.](http://arxiv.org/abs/2401.13138) | 本文评估了三类增加对AI代理可见性的措施：代理标识符、实时监控和活动记录，并提出了可能的实施方式。这些措施在集中化和去中心化部署环境中应用广泛，有助于理解和减轻AI代理带来的风险。 |
| [^46] | [The Language Barrier: Dissecting Safety Challenges of LLMs in Multilingual Contexts.](http://arxiv.org/abs/2401.13136) | 本文研究了在多语言环境中，LLMs面临的安全挑战以及缓解这些挑战的方法。在不同语言中，LLMs对恶意提示的响应存在差异，低资源语言下的响应更容易产生不安全、不相关的结果。对于这种差异的原因，本文还研究了使用强化学习和有监督微调等方法对数据进行调节的影响。 |
| [^47] | [DISCOUNT: Distributional Counterfactual Explanation With Optimal Transport.](http://arxiv.org/abs/2401.13112) | 本文提出了使用最优传输进行分布式对抗解释的方法DISCOUNT，将对抗解释的概念扩展到整个输入输出分布，并通过统计置信度来支撑这一方法。 |
| [^48] | [XAI for All: Can Large Language Models Simplify Explainable AI?.](http://arxiv.org/abs/2401.13110) | 本文介绍了一种名为“x-[plAIn]”的自定义大型语言模型，通过该模型能够使得可解释的人工智能（XAI）更易于访问，并为不同用户群体生成清晰、简明的解释。该模型具有适应性，能够根据用户的知识水平和兴趣来生成解释。实验结果表明，这种适应性提高了XAI的可访问性，弥补了差距。 |
| [^49] | [Sparse identification of nonlinear dynamics in the presence of library and system uncertainty.](http://arxiv.org/abs/2401.13099) | 针对系统变量和函数库不确定性，本文提出了增强的SINDy算法，可以更好地识别稀疏非线性动力学。 |
| [^50] | [Gravity-Informed Deep Learning Framework for Predicting Ship Traffic Flow and Invasion Risk of Non-Indigenous Species via Ballast Water Discharge.](http://arxiv.org/abs/2401.13098) | 通过考虑航运通量密度、港口距离、贸易流量和交通枢纽的中心性指标等因素，本研究开发了一个受物理启发的模型来预测海事航运流量，并用于指导全球交通网络中入侵物种的风险评估和管理。 |
| [^51] | [Digital Divides in Scene Recognition: Uncovering Socioeconomic Biases in Deep Learning Systems.](http://arxiv.org/abs/2401.13097) | 该研究研究了深度学习系统中的社会经济偏见对场景识别的影响，发现了预训练的卷积神经网络在低社会经济地位的家庭照片中显示出更低的分类准确度和分类置信度，并更容易分配具有冒犯性的标签。 |
| [^52] | [Towards Trustable Language Models: Investigating Information Quality of Large Language Models.](http://arxiv.org/abs/2401.13086) | 这项研究探讨了大规模语言模型的信息质量问题，发现标记化不可靠、偏见以及信息质量下降可能导致幻觉、捏造信息，从而对企业决策产生错误影响。 |
| [^53] | [IndiText Boost: Text Augmentation for Low Resource India Languages.](http://arxiv.org/abs/2401.13085) | 本论文研究了针对低资源印度语言的文本增强方法，包括Easy Data Augmentation、Back Translation、Paraphrasing、使用LLMs进行文本生成以及使用LLMs进行文本扩展等技术。通过二元和多类文本分类实验，研究发现基本的数据增强技术可以显著提升性能。 |
| [^54] | [Free Form Medical Visual Question Answering in Radiology.](http://arxiv.org/abs/2401.13081) | 这项研究在医学领域的视觉问答中取得了重要进展，创新性地增强了数据集，并实现了与当前最先进模型相当的性能。 |
| [^55] | [Local Background Estimation for Improved Gas Plume Identification in Hyperspectral Images.](http://arxiv.org/abs/2401.13068) | 使用图像分割和迭代背景估计算法改进了高光谱图像中气溶胶识别的效果。 |
| [^56] | [TCE at Qur'an QA 2023 Shared Task: Low Resource Enhanced Transformer-based Ensemble Approach for Qur'anic QA.](http://arxiv.org/abs/2401.13060) | 本文介绍了我们在《古兰经问答2023》共享任务中的应用，通过利用迁移学习和投票集成来提高预测稳定性，使用不同的Transformer模型和阈值机制来解决低资源训练数据的挑战。我们的最佳系统在隐藏数据集上取得了显著提升。 |
| [^57] | [CIS-UNet: Multi-Class Segmentation of the Aorta in Computed Tomography Angiography via Context-Aware Shifted Window Self-Attention.](http://arxiv.org/abs/2401.13049) | 本研究提出了一种称为CIS-UNet的深度学习模型，用于主动脉和主动脉分支的多类分割。该模型结合了CNN和Swin transformers的优势，采用了上下文感知的平移窗口自注意力，能够准确地识别主动脉的各个分支。 |
| [^58] | [Locality Sensitive Sparse Encoding for Learning World Models Online.](http://arxiv.org/abs/2401.13034) | 本文提出了一种基于局部敏感稀疏编码的线性回归模型，通过非线性随机特征实现对复杂环境的拟合。这种模型能够高效地进行稀疏更新，实现了优化拟合先前经验的Follow-The-Leader（FTL）世界模型。 |
| [^59] | [CIMGEN: Controlled Image Manipulation by Finetuning Pretrained Generative Models on Limited Data.](http://arxiv.org/abs/2401.13006) | 本文提出的方法利用预训练生成模型对受限数据进行微调，实现了可控的图像操作。通过修改语义地图，可以方便地插入、删除或替换图像中的对象。该方法在图像伪造和图像编辑领域具有潜在的应用价值和有效性。 |
| [^60] | [Theorem Discovery Amongst Cyclic Polygons.](http://arxiv.org/abs/2401.13002) | 该论文介绍了关于循环2n边形的一个几何定理类别，证明了当取n个不相交的边对时，这些边之间角度的线性组合是恒定的，并提供了一个公式来表示这个线性组合。论文还介绍了一个利用这个结果生成新的几何证明问题及其解法的程序。 |
| [^61] | [PatternPortrait: Draw Me Like One of Your Scribbles.](http://arxiv.org/abs/2401.13001) | 本论文介绍了一种从图片生成抽象肖像绘画的方法，通过利用单一的手绘图案素描和图形神经网络架构，实现了生成多样化的笔触变化，创造出风格独特的充满喜悦的抽象绘画。 |
| [^62] | [Quantum-Inspired Machine Learning for Molecular Docking.](http://arxiv.org/abs/2401.12999) | 量子启发的机器学习方法在分子对接中取得了显著的改进，通过结合量子特性和深度学习在编码的分子空间中学习的梯度，提高了盲目对接的成功率。 |
| [^63] | [Evaluating and Enhancing Large Language Models Performance in Domain-specific Medicine: Osteoarthritis Management with DocOA.](http://arxiv.org/abs/2401.12998) | 本研究评估了大型语言模型在特定领域医学中的表现，并以骨关节炎管理为例进行了增强。研究结果发现，通用语言模型相对于领域特定的骨关节炎管理模型在提供个性化治疗建议方面表现不佳，而专门模型表现有明显提升。 |
| [^64] | [A Comparison of Veterans with Problematic Opioid Use Identified through Natural Language Processing of Clinical Notes versus Using Diagnostic Codes.](http://arxiv.org/abs/2401.12996) | 通过翻译了的临床记录进行自然语言处理，发现了存在问题的鸦片使用的退伍军人。与仅通过诊断代码识别的鸦片使用障碍患者相比，这些患者具有不同的人口统计学和临床特征。 |
| [^65] | [Few-Shot Learning for Chronic Disease Management: Leveraging Large Language Models and Multi-Prompt Engineering with Medical Knowledge Injection.](http://arxiv.org/abs/2401.12988) | 本研究提出了慢性病管理的少样本学习框架，利用大型语言模型和多Prompt工程进行精神障碍的检测，通过个性化的提示和医疗知识注入来解决数据挑战，实现慢性病管理的目标。 |
| [^66] | [Crowdsourced Adaptive Surveys.](http://arxiv.org/abs/2401.12986) | 众包自适应调查方法（CSAS）结合自然语言处理和自适应算法，能够根据用户输入演变问题库，并在调查中适应新的问题，应用在拉丁裔信息环境和议题重要性领域，能够识别难以通过传统方法跟踪的主张或问题。 |
| [^67] | [Assessing Large Language Models in Mechanical Engineering Education: A Study on Mechanics-Focused Conceptual Understanding.](http://arxiv.org/abs/2401.12983) | 本研究通过对三个大型语言模型（LLMs）在机械工程领域中解决概念性问题的能力进行评估，发现GPT-4在各个力学主题的问题回答方面表现优于其他两个模型和人类对照组，显示出潜在的未来改进空间。 |
| [^68] | [Quantitative Analysis of Molecular Transport in the Extracellular Space Using Physics-Informed Neural Network.](http://arxiv.org/abs/2401.12435) | 本文提出了一种使用物理信息神经网络对细胞外间隙中分子传输进行定量分析的新方法，解决了对分子传输形式不清楚的挑战，并实现了自动计算扩散系数和分子速度的优化功能。 |
| [^69] | [Knowledge Distillation on Spatial-Temporal Graph Convolutional Network for Traffic Prediction.](http://arxiv.org/abs/2401.11798) | 本论文研究了在交通预测中应用空间-时间图卷积网络和知识蒸馏的方法。知识蒸馏的思想能够实现在减少参数和保持准确性的同时提高执行效率。通过引入教师网络的空间-时间相关性，我们的方法能够使学生网络学习到复杂的交通模式。 |
| [^70] | [Safe and Generalized end-to-end Autonomous Driving System with Reinforcement Learning and Demonstrations.](http://arxiv.org/abs/2401.11792) | 本文介绍了一种安全且广义的端到端自主驾驶系统 (SGADS)，使用强化学习和示范相结合的方法解决了现有方法的低安全性、泛化能力差和采样效率低的问题，同时引入了变分推理和归一化流以准确预测驾驶轨迹，并提出了鲁棒性安全约束的制定方法。 |
| [^71] | [Next Visit Diagnosis Prediction via Medical Code-Centric Multimodal Contrastive EHR Modelling with Hierarchical Regularisation.](http://arxiv.org/abs/2401.11648) | 通过医学代码中心的多模态对比EHR建模预测下次就诊诊断，并通过分层正则化提高性能。 |
| [^72] | [Enhancing Large Language Models for Clinical Decision Support by Incorporating Clinical Practice Guidelines.](http://arxiv.org/abs/2401.11120) | 该论文研究了将临床实践指南纳入大型语言模型以增强临床决策支持的方法。他们开发了三种方法，并对四个大型语言模型进行了评估，在COVID-19门诊治疗方面取得了较高的性能。 |
| [^73] | [Using LLMs to discover emerging coded antisemitic hate-speech emergence in extremist social media.](http://arxiv.org/abs/2401.10841) | 这项研究提出了一种方法，可以检测新出现的编码恶意术语，为极端社交媒体中的反犹太恶意言论提供了解决方案。 |
| [^74] | [DiConStruct: Causal Concept-based Explanations through Black-Box Distillation.](http://arxiv.org/abs/2401.08534) | DiConStruct是一种基于黑盒模型的因果概念解释方法，通过创建结构性因果模型和概念归因方式提供更具可解释性的局部解释。 |
| [^75] | [Supporting Student Decisions on Learning Recommendations: An LLM-Based Chatbot with Knowledge Graph Contextualization for Conversational Explainability and Mentoring.](http://arxiv.org/abs/2401.08517) | 这个论文研究了一种基于知识图谱情境化的LLM聊天机器人，作为学生学习推荐的解释工具和指导。通过定义上下文并利用人工策划的信息源来调控LLM的生成，聊天机器人能在与学生对话中提供解释和指导。 |
| [^76] | [Hidden Flaws Behind Expert-Level Accuracy of GPT-4 Vision in Medicine.](http://arxiv.org/abs/2401.08396) | GPT-4 Vision在医学领域中具有专家级准确度，但在图像理解方面存在缺陷。 |
| [^77] | [Formal Logic Enabled Personalized Federated Learning Through Property Inference.](http://arxiv.org/abs/2401.07448) | 本论文提出了一种通过引入时态逻辑推理来实现个性化的形式逻辑启用的联邦学习，以解决异质性客户设备带来的挑战，并提出了聚合群集的概念。 |
| [^78] | [Between Lines of Code: Unraveling the Distinct Patterns of Machine and Human Programmers.](http://arxiv.org/abs/2401.06461) | 本文通过分析代码的属性，揭示了机器和人类代码之间的独特模式，尤其是结构分割对于识别代码来源很关键。基于这些发现，我们提出了一种名为DetectCodeGPT的新方法来检测机器生成的代码。 |
| [^79] | [How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs.](http://arxiv.org/abs/2401.06373) | 本文通过将LLMs视为人类交流者，探索了每天语言互动和AI安全之间忽视的交叉点，并提出了一种通过说服LLMs进行越狱的方法。研究结果表明，说服显著提高了越狱性能，在多个风险类别上均取得了超过92%的攻击成功率。 |
| [^80] | [End-to-end Learnable Clustering for Intent Learning in Recommendation.](http://arxiv.org/abs/2401.05975) | 本文提出了一种用于推荐中意图学习的端到端可学习聚类方法ELCRec，该方法解决了现有方法中的复杂优化问题和大规模数据集聚类的可扩展性问题。 |
| [^81] | [Improving the Accuracy and Interpretability of Random Forests via Forest Pruning.](http://arxiv.org/abs/2401.05535) | 通过森林修剪方法，本研究提出了一种兼顾随机森林准确性和决策树可解释性的方法。实验证明，在大多数情景下，这种方法能够显著提高随机森林的性能。 |
| [^82] | [Fine-tuning and Utilization Methods of Domain-specific LLMs.](http://arxiv.org/abs/2401.02981) | 本研究调查了领域特定LLM的微调和利用方法，以金融领域为例，详细介绍了数据集选择、预处理、模型选择以及在金融领域LLM微调中的关键因素。研究探讨了领域特定词汇的构建和安全合规性的考虑因素，并提供了在金融领域生成领域特定LLM的过程和实施方法。多种金融案例被涵盖在内，包括股票价格预测、金融新闻情绪分析、自动文档处理、研究和信息提取等。 |
| [^83] | [SAR-RARP50: Segmentation of surgical instrumentation and Action Recognition on Robot-Assisted Radical Prostatectomy Challenge.](http://arxiv.org/abs/2401.00496) | 该论文介绍了SAR-RARP50挑战，该挑战提供了第一个多模态、公开的、体内的手术动作识别和语义仪器分割数据集，旨在让研究人员开发出稳健且准确的单任务动作识别方法。 |
| [^84] | [Proportional Representation in Metric Spaces and Low-Distortion Committee Selection.](http://arxiv.org/abs/2312.10369) | 在这篇论文中，我们提出了一种新的定义，用于度量空间中小集合对大集合的“代表性”。给定代表的集合和可能的代表集合，我们的标准要求对于大集合的任意子集，该子集与最好的代表点的平均距离与在所有代表点中的最好点的平均距离之间的差距不超过一个因子gamma。与已有的比例公平和核心公平的概念不同的是，我们的定义要求以比例表示大的凝聚簇。在资源增强框架中研究了该定义，同时也考虑了选举应用的情况。 |
| [^85] | [A Compact LSTM-SVM Fusion Model for Long-Duration Cardiovascular Diseases Detection.](http://arxiv.org/abs/2312.09442) | 本研究提出了一个紧凑型的LSTM-SVM融合模型，实现了对心血管疾病的早期检测。该模型采用了一种流程优化方法将心电图信号预处理为一致的10秒持续时间，同时利用LSTM和SVM实现了最先进的结果。 |
| [^86] | [An Incremental Unified Framework for Small Defect Inspection.](http://arxiv.org/abs/2312.08917) | 我们提出了一种增量统一框架（IUF），用于解决工业制造中的小缺陷检测问题。通过引入对象感知自注意力（OASA）和语义压缩损失（SCL），我们的方法可以在不断整合新物体时减少特征冲突问题，并展现出卓越的性能。 |
| [^87] | [Privacy Issues in Large Language Models: A Survey.](http://arxiv.org/abs/2312.06717) | 这项调研关注大型语言模型中的隐私问题。主要总结了红队模型揭示隐私风险、将隐私纳入训练和推理过程、高效删除训练模型中的数据以符合隐私法规、以及减轻版权问题等技术研究。法律和政策研究虽然从不同角度解决了相关挑战，但不是本调研的重点。 |
| [^88] | [Linear Log-Normal Attention with Unbiased Concentration.](http://arxiv.org/abs/2311.13541) | 本论文研究了自注意机制，并分析了注意力矩阵的分布和集中能力。通过引入线性对数正态注意力来模拟原始自注意力的分布和集中行为，提高了Transformer模型的可扩展性。 |
| [^89] | [Applying Large Language Models to Power Systems: Potential Security Threats.](http://arxiv.org/abs/2311.13361) | 将大型语言模型应用于电力系统可能带来潜在安全威胁，需要进行迫切的研究和开发相应对策。 |
| [^90] | [Formally Specifying the High-Level Behavior of LLM-Based Agents.](http://arxiv.org/abs/2310.08535) | 本文介绍了一个最小生成框架，通过在高级声明中定义所需的代理人行为，然后构建解码监视器，从而实现了基于LLM的代理人的快速设计和实施。 |
| [^91] | [Batch Calibration: Rethinking Calibration for In-Context Learning and Prompt Engineering.](http://arxiv.org/abs/2309.17249) | 本研究提出了一种名为批量校准（BC）的方法，用于解决大型语言模型中提示脆弱性和偏见因素导致的性能下降问题。BC通过控制批量输入的上下文偏见，统一了现有的校准方法，并具有零-shot和仅推理的特点。 |
| [^92] | [Revisiting Softmax Masking for Stability in Continual Learning.](http://arxiv.org/abs/2309.14808) | 本文重新审视了用于连续学习中的Softmax掩码的影响，并提出了一种利用其置信度保持效果的方法，通过增加稳定性同时保持准确性。 |
| [^93] | [How Transferable are Attribute Controllers on Pretrained Multilingual Translation Models?.](http://arxiv.org/abs/2309.08565) | 本文研究了如何将预训练的多语言翻译模型中的属性控制器迁移到没有监督数据的语言。通过全面分析不同数据场景下的训练和推断时控制技术，揭示了它们在零样本性能和领域鲁棒性上的相对优势和劣势。 |
| [^94] | [Peering Through Preferences: Unraveling Feedback Acquisition for Aligning Large Language Models.](http://arxiv.org/abs/2308.15812) | 本研究分析了对于对齐和评估大型语言模型而言，设计反馈选择是评分还是排名对结果的影响。研究发现评分和排名所推断出的偏好存在不一致问题，并且注释者的偏见也会影响结果。同时，研究还发现反馈协议的选择也对评估结果有显著影响。 |
| [^95] | [CALM : A Multi-task Benchmark for Comprehensive Assessment of Language Model Bias.](http://arxiv.org/abs/2308.12539) | CALM是一个用于量化语言模型偏见的多任务基准数据集，相比先前数据集更加多样和可靠，能更好地捕捉评估模型偏见所需的语言变化的广度。 |
| [^96] | [Distributed Dynamic Programming and an O.D.E. Framework of Distributed TD-Learning for Networked Multi-Agent Markov Decision Processes.](http://arxiv.org/abs/2307.16706) | 本文研究了网络多智能体马尔可夫决策问题中的分布式动态规划和分布式TD学习算法。其中，我们通过引入新的分布式DP算法和分布式TD学习算法，并证明了它们的收敛性，提出了两个关键点。该分布式DP算法具有两个独立的动态系统的特点。 |
| [^97] | [Knapsack: Connectedness, Path, and Shortest-Path.](http://arxiv.org/abs/2307.12547) | 该论文研究了带有图论约束的背包问题，证明了问题的复杂性并提出了近似算法。 |
| [^98] | [VELMA: Verbalization Embodiment of LLM Agents for Vision and Language Navigation in Street View.](http://arxiv.org/abs/2307.06082) | VELMA是一个口头化的LLM智能体，利用人类写作的导航指令中的地标并结合CLIP来进行视觉环境的理解，以实现在街景中的视觉和语言导航。 |
| [^99] | [Surge Routing: Event-informed Multiagent Reinforcement Learning for Autonomous Rideshare.](http://arxiv.org/abs/2307.02637) | 这项研究提出了一种基于事件信息的多智能体强化学习框架，用于自动拼车，通过预测和适应需求激增，并生成协同的路由和接乘策略来服务更多请求。 |
| [^100] | [Graph Neural Networks based Log Anomaly Detection and Explanation.](http://arxiv.org/abs/2307.00527) | 提出了一种基于图神经网络的无监督日志异常检测方法，该方法将事件日志转换为带属性、有向和加权的图，并利用图神经网络进行图级别的异常检测。引入了一种新的图神经网络模型OCDiGCN来检测一组带属性、有向和加权的图中的图级别异常，并提供对异常的解释能力。 |
| [^101] | [TopP\&R: Robust Support Estimation Approach for Evaluating Fidelity and Diversity in Generative Models.](http://arxiv.org/abs/2306.08013) | 本文提出了一种鲁棒可靠的生成模型评估指标TopP\&R，通过引入拓扑和统计处理进行严格的支持估计。TopP\&R仅保留具有一定置信水平的具有拓扑和统计上重要性的特征，对于噪声特征具有强大的鲁棒性，并提供了统计一致性。 |
| [^102] | [Learning DAGs from Data with Few Root Causes.](http://arxiv.org/abs/2305.15936) | 该论文提出了一种新的算法，能够从仅有少量根因的数据中学习DAGs，并证明了其可识别性，并在性能上优于以前的方法。 |
| [^103] | [Rethinking GNN-based Entity Alignment on Heterogeneous Knowledge Graphs: New Datasets and A New Method.](http://arxiv.org/abs/2304.03468) | 文章重新考虑了基于GNN的异构知识图谱实体对齐。为探究EA实际场景中的表现，提出了更接近现实的高度异构知识图谱数据集，并提出了新方法。 |
| [^104] | [Digital Over-the-Air Federated Learning in Multi-Antenna Systems.](http://arxiv.org/abs/2302.14648) | 本文研究了在多天线系统中采用数字调制和空中计算的情况下，联邦学习的性能优化问题。通过结合数字调制和AirComp，提出了一种改进的联邦平均算法，以解决无线信道衰落导致的总体失真问题。 |
| [^105] | [Adversarial Detection by Approximation of Ensemble Boundary.](http://arxiv.org/abs/2211.10227) | 本论文提出了一种使用Walsh系数逼近决策边界的对抗攻击检测方法，通过观察清晰图像和对抗图像之间的Walsh系数逼近差异，实现了对对抗攻击的检测。 |
| [^106] | [TE2Rules: Explaining Tree Ensembles using Rules.](http://arxiv.org/abs/2206.14359) | 本文介绍了一种将二元分类任务中的树集合模型转换为可解释规则列表的方法，该方法可以有效解释模型对于少数类别的预测。实验证明，TE2Rules方法生成的规则列表准确性较高，并且运行时间与其他基线方法相当。 |
| [^107] | [Relative Policy-Transition Optimization for Fast Policy Transfer.](http://arxiv.org/abs/2206.06009) | 本论文介绍了相对策略过渡优化的方法，通过引入一个基于强化学习的引理衡量两个MDP之间的相对差距，并提出了相对策略优化和相对转移优化两个算法来实现快速策略转移和动态建模。同时，将这两个算法集成在一起形成完整的相对策略过渡优化算法。 |
| [^108] | [Multi-modal Misinformation Detection: Approaches, Challenges and Opportunities.](http://arxiv.org/abs/2203.13883) | 这项研究总结了多模式虚假信息检测的方法、挑战和机遇。由于社交媒体平台的转变，虚假信息的性质也发生了变化。研究人员已经开发出自动检测跨模态不协调的技术，但仍面临挑战和不足之处，进一步的研究机会也在等待着挖掘。 |
| [^109] | [AI Ethics Principles in Practice: Perspectives of Designers and Developers.](http://arxiv.org/abs/2112.07467) | 这篇论文研究了设计师和开发人员的实践和经验，分析其如何遵循澳大利亚政府提出的高层次AI伦理原则，并总结出原则之间存在的张力和权衡。 |
| [^110] | [A New Sentence Extraction Strategy for Unsupervised Extractive Summarization Methods.](http://arxiv.org/abs/2112.03203) | 本文提出了一种新的句子提取策略，用于改善特征分布和降低摘要句子之间的相互信息，该策略适用于现有的无监督抽取式摘要方法，并在实验证明了其有效性。 |

# 详细

[^1]: 深度强化学习中策略梯度的终极指南：理论、算法和实现

    The Definitive Guide to Policy Gradients in Deep Reinforcement Learning: Theory, Algorithms and Implementations. (arXiv:2401.13662v1 [cs.LG])

    [http://arxiv.org/abs/2401.13662](http://arxiv.org/abs/2401.13662)

    本文提供了一个深度强化学习中策略梯度算法的综合指南，包括理论基础、实际实现和比较结果，并对正则化的益处进行了讨论。

    

    最近几年，在深度强化学习中提出了各种强大的策略梯度算法。虽然所有这些算法都建立在策略梯度定理的基础上，但具体的设计选择在算法之间有很大的差异。我们提供了一个整体的视角来概述在线策略梯度算法，以便理解它们的理论基础和实际实现。在这个概述中，我们包括了连续版本的策略梯度定理的详细证明、收敛结果和对实际算法的全面讨论。我们比较了连续控制环境中最重要的算法，并对正则化的益处提供了深入的见解。所有的代码都可以在https://github.com/Matt00n/PolicyGradientsJax获得。

    In recent years, various powerful policy gradient algorithms have been proposed in deep reinforcement learning. While all these algorithms build on the Policy Gradient Theorem, the specific design choices differ significantly across algorithms. We provide a holistic overview of on-policy policy gradient algorithms to facilitate the understanding of both their theoretical foundations and their practical implementations. In this overview, we include a detailed proof of the continuous version of the Policy Gradient Theorem, convergence results and a comprehensive discussion of practical algorithms. We compare the most prominent algorithms on continuous control environments and provide insights on the benefits of regularization. All code is available at https://github.com/Matt00n/PolicyGradientsJax.
    
[^2]: 常见的随机神经网络在可靠性临床决策支持方面的不足

    Inadequacy of common stochastic neural networks for reliable clinical decision support. (arXiv:2401.13657v1 [cs.LG])

    [http://arxiv.org/abs/2401.13657](http://arxiv.org/abs/2401.13657)

    本研究调查了随机神经网络在临床应用中的可靠性，并发现常见的深度学习方法在数据转移情况下过于自信。这强调了对本地不确定性可靠估计及其向最终用户传达的重要性。

    

    由于伦理和安全相关的关切，人工智能的广泛应用于医学决策仍然受阻。对于基于人工智能的医疗决策支持系统来说，可靠性和可信度至关重要。然而，常见的深度学习方法在数据转移下往往过于自信。这种在基于证据的场景之外不恰当的推理可能会产生严重后果。这凸显了对本地不确定性可靠估计及其向最终用户传达的重要性。尽管随机神经网络被誉为这些问题的潜在解决方案，但本研究调查了其在临床应用中的实际可靠性。我们以从MIMIC3研究中使用的EHR的ICU住院病死率预测为例来进行分析。对于EHR时间序列的预测，我们采用了仅编码器的Transformer模型。通过纳入常见方法来实现模型函数的随机性。

    Widespread adoption of AI for medical decision making is still hindered due to ethical and safety-related concerns. For AI-based decision support systems in healthcare settings it is paramount to be reliable and trustworthy. Common deep learning approaches, however, have the tendency towards overconfidence under data shift. Such inappropriate extrapolation beyond evidence-based scenarios may have dire consequences. This highlights the importance of reliable estimation of local uncertainty and its communication to the end user. While stochastic neural networks have been heralded as a potential solution to these issues, this study investigates their actual reliability in clinical applications. We centered our analysis on the exemplary use case of mortality prediction for ICU hospitalizations using EHR from MIMIC3 study. For predictions on the EHR time series, Encoder-Only Transformer models were employed. Stochasticity of model functions was achieved by incorporating common methods such 
    
[^3]: 基于稀疏网格的不连续性检测的图信息神经网络

    Graph-Informed Neural Networks for Sparse Grid-Based Discontinuity Detectors. (arXiv:2401.13652v1 [cs.LG])

    [http://arxiv.org/abs/2401.13652](http://arxiv.org/abs/2401.13652)

    本文提出了一种利用图信息神经网络和稀疏网格来检测不连续函数不连续界面的新方法，该方法在维度大于3的情况下表现出高效且准确的不连续性检测能力，在维度n = 2和n = 4的函数上进行的实验验证了其高效性和泛化能力，并具有可移植性和多功能性。

    

    本文提出了一种新颖的方法来检测不连续函数的不连续界面。该方法利用了基于图的神经网络（GINNs）和稀疏网格来解决维度大于3的情况下的不连续性检测。训练过的GINNs在稀疏网格上识别有问题的点，并利用构建在网格上的图结构实现高效准确的不连续性检测性能。我们还引入了一种递归算法用于一般的基于稀疏网格的检测器，具有收敛性和易于应用性。在维度n=2和n=4的函数上进行的数值实验证明了GINNs在检测不连续界面方面的高效性和鲁棒泛化能力。值得注意的是，经过训练的GINNs具有可移植性和多功能性，可以集成到各种算法中并共享给用户。

    In this paper, we present a novel approach for detecting the discontinuity interfaces of a discontinuous function. This approach leverages Graph-Informed Neural Networks (GINNs) and sparse grids to address discontinuity detection also in domains of dimension larger than 3. GINNs, trained to identify troubled points on sparse grids, exploit graph structures built on the grids to achieve efficient and accurate discontinuity detection performances. We also introduce a recursive algorithm for general sparse grid-based detectors, characterized by convergence properties and easy applicability. Numerical experiments on functions with dimensions n = 2 and n = 4 demonstrate the efficiency and robust generalization of GINNs in detecting discontinuity interfaces. Notably, the trained GINNs offer portability and versatility, allowing integration into various algorithms and sharing among users.
    
[^4]: ChatGPT在面部生物识别中的表现有多好？对识别、软生物特征和可解释性的初步探索。

    How Good is ChatGPT at Face Biometrics? A First Look into Recognition, Soft Biometrics, and Explainability. (arXiv:2401.13641v1 [cs.CV])

    [http://arxiv.org/abs/2401.13641](http://arxiv.org/abs/2401.13641)

    本研究初步探索了基于GPT-4的ChatGPT在面部生物识别中的表现。研究分析了ChatGPT在面部验证、软生物特征估计和结果可解释性方面的能力。ChatGPT的应用有望提高自动决策在人类场景中的解释性和透明度。

    

    诸如OpenAI开发的GPT这样的大型语言模型已经展现出令人惊讶的结果，为我们的社会引入了快速变革。ChatGPT的发布进一步加强了这一影响，它使任何人都能以简单的对话方式与语言模型进行交互，不需要任何领域经验。因此，ChatGPT已被迅速应用于许多不同的任务，如代码和歌曲创作、教育、虚拟助手等，展示了对于未经过训练的任务而言令人印象深刻的结果（零样本学习）。本研究旨在探讨基于最新的GPT-4多模态语言模型的ChatGPT在面部生物识别任务中的能力。具体而言，我们分析了ChatGPT在面部验证、软生物特征估计和结果可解释性方面的能力。ChatGPT对于进一步增加人类场景中自动决策的解释性和透明度非常有价值。实验被进行以评估ChatGPT的表现。

    Large Language Models (LLMs) such as GPT developed by OpenAI, have already shown astonishing results, introducing quick changes in our society. This has been intensified by the release of ChatGPT which allows anyone to interact in a simple conversational way with LLMs, without any experience in the field needed. As a result, ChatGPT has been rapidly applied to many different tasks such as code- and song-writer, education, virtual assistants, etc., showing impressive results for tasks for which it was not trained (zero-shot learning).  The present study aims to explore the ability of ChatGPT, based on the recent GPT-4 multimodal LLM, for the task of face biometrics. In particular, we analyze the ability of ChatGPT to perform tasks such as face verification, soft-biometrics estimation, and explainability of the results. ChatGPT could be very valuable to further increase the explainability and transparency of the automatic decisions in human scenarios. Experiments are carried out in order
    
[^5]: 提升图像检索：使用CLIP模型进行照片搜索的综合研究

    Enhancing Image Retrieval : A Comprehensive Study on Photo Search using the CLIP Mode. (arXiv:2401.13613v1 [cs.CV])

    [http://arxiv.org/abs/2401.13613](http://arxiv.org/abs/2401.13613)

    CLIP模型为照片搜索带来显著的进展，通过学习图像和文本的共享表示空间，实现了跨模态理解，并实现了基于自然语言查询的高效准确的图像检索。它具有强大的泛化能力，可应用于零样本学习和少样本分类等任务。

    

    照片搜索是根据文本查询检索图像的任务，在CLIP（对比学习语言与图像预训练）模型的引入下取得了显著的进展。CLIP利用了视觉语言预训练方法，学习了图像和文本的共享表示空间，实现了跨模态理解。该模型展示了理解不同图像和文本对之间语义关系的能力，实现了基于自然语言查询的图像高效准确的检索。通过在包含图像及其相关文本描述的大规模数据集上进行训练，CLIP实现了显著的泛化能力，为零样本学习和少样本分类等任务提供了强大的工具。这篇摘要总结了CLIP的基本原理，并强调了其对推进照片搜索领域的潜在影响，促进了自然语言理解和计算机视觉无缝融合的发展。

    Photo search, the task of retrieving images based on textual queries, has witnessed significant advancements with the introduction of CLIP (Contrastive Language-Image Pretraining) model. CLIP leverages a vision-language pre training approach, wherein it learns a shared representation space for images and text, enabling cross-modal understanding. This model demonstrates the capability to understand the semantic relationships between diverse image and text pairs, allowing for efficient and accurate retrieval of images based on natural language queries. By training on a large-scale dataset containing images and their associated textual descriptions, CLIP achieves remarkable generalization, providing a powerful tool for tasks such as zero-shot learning and few-shot classification. This abstract summarizes the foundational principles of CLIP and highlights its potential impact on advancing the field of photo search, fostering a seamless integration of natural language understanding and comp
    
[^6]: 使用中间ASR特征和人类记忆模型对听障用户进行非侵入性语音可懂度预测

    Non-Intrusive Speech Intelligibility Prediction for Hearing-Impaired Users using Intermediate ASR Features and Human Memory Models. (arXiv:2401.13611v1 [cs.SD])

    [http://arxiv.org/abs/2401.13611](http://arxiv.org/abs/2401.13611)

    本研究使用中间ASR特征和人类记忆模型预测听障用户的语音可懂度，取得了显著的性能提升并降低了均方根误差。

    

    神经网络已成功用于非侵入性语音可懂度预测。最近，使用来自预训练的自监督和弱监督模型的中间层特征表示在这个任务中被发现特别有用。本研究将Whisper ASR解码器层的表示作为神经网络的输入特征，结合了一种基于示范的、心理学模型的人类记忆，以预测助听器用户的人类可懂度评分。与已建立的侵入性HASPI基线系统相比，我们实验结果显示出了显著的性能提升，包括在训练数据中未见过的增强系统和听者，均方根误差从28.7降至25.3。

    Neural networks have been successfully used for non-intrusive speech intelligibility prediction. Recently, the use of feature representations sourced from intermediate layers of pre-trained self-supervised and weakly-supervised models has been found to be particularly useful for this task. This work combines the use of Whisper ASR decoder layer representations as neural network input features with an exemplar-based, psychologically motivated model of human memory to predict human intelligibility ratings for hearing-aid users. Substantial performance improvement over an established intrusive HASPI baseline system is found, including on enhancement systems and listeners unseen in the training data, with a root mean squared error of 25.3 compared with the baseline of 28.7.
    
[^7]: 移动生态系统中基于流式感知的认知代理

    Stream-based perception for cognitive agents in mobile ecosystems. (arXiv:2401.13604v1 [cs.AI])

    [http://arxiv.org/abs/2401.13604](http://arxiv.org/abs/2401.13604)

    这篇论文提出了一种基于流式感知的方法，使移动设备中的认知代理能够感知有意义的情境。研究通过众包案例展示了手机传感器数据如何被用于触发和指导自主、自利的代理协作送货到目的地。

    

    认知代理抽象可以帮助在移动设备中构建智能系统。在智能手机上，来自机载传感器的数据可以为用户的当前情况提供有价值的洞察。然而，当前的认知代理框架无法很好地应对传感器数据的挑战性特征。传感器数据位于低抽象级别上，单个数据元素在孤立观察时没有意义。相反，认知代理在高级感知上运行，并且缺乏有效地检测多个感知序列中复杂时空模式的方法。在本文中，我们提出了一种基于流式感知的方法，使代理能够在低级传感器数据流中感知有意义的情境。我们通过一个代表性的众包案例研究展示了手机传感器数据导出的情境如何触发和指导自主、自利的代理协作送货到目的地。

    Cognitive agent abstractions can help to engineer intelligent systems across mobile devices. On smartphones, the data obtained from onboard sensors can give valuable insights into the user's current situation. Unfortunately, today's cognitive agent frameworks cannot cope well with the challenging characteristics of sensor data. Sensor data is located on a low abstraction level and the individual data elements are not meaningful when observed in isolation. In contrast, cognitive agents operate on high-level percepts and lack the means to effectively detect complex spatio-temporal patterns in sequences of multiple percepts. In this paper, we present a stream-based perception approach that enables the agents to perceive meaningful situations in low-level sensor data streams. We present a crowdshipping case study where autonomous, self-interested agents collaborate to deliver parcels to their destinations. We show how situations derived from smartphone sensor data can trigger and guide auc
    
[^8]: 用于程序性问答的图形引导问题生成方法

    Graph Guided Question Answer Generation for Procedural Question-Answering. (arXiv:2401.13594v1 [cs.CL])

    [http://arxiv.org/abs/2401.13594](http://arxiv.org/abs/2401.13594)

    本文提出了一种生成详尽的高质量训练数据的方法，用于训练紧凑的、任务特定的QA模型，从而在特定问答任务中与GPT变体模型具有竞争力。这种方法利用过程性文本的结构化特性，通过将每个步骤和整个流程表示为图形，并以图节点为条件，在详尽和可控的方式下自动生成QA对。

    

    本文针对特定任务的问答(QA)问题进行研究，提出了一种生成详尽高质量训练数据的方法，使我们能够训练紧凑的、针对特定任务的QA模型，并与GPT变体模型竞争。关键的技术支持是一种从过程文本中自动生成问题-答案的新机制，它可以处理大量的文本指令，并产生详尽的领域内QA训练数据。目前的QA数据生成方法会产生形式良好且多样化的数据，但其非详尽性不利于训练QA模型。相反，我们利用过程性文本的高度结构化特性，将每个步骤和整个流程表示为图形，并以图节点为条件，自动以详尽和可控的方式生成QA对。对我们方法的全面评估表明：1) 使用我们的方法训练的小型模型能够达到竞争水平。

    In this paper, we focus on task-specific question answering (QA). To this end, we introduce a method for generating exhaustive and high-quality training data, which allows us to train compact (e.g., run on a mobile device), task-specific QA models that are competitive against GPT variants. The key technological enabler is a novel mechanism for automatic question-answer generation from procedural text which can ingest large amounts of textual instructions and produce exhaustive in-domain QA training data. While current QA data generation methods can produce well-formed and varied data, their non-exhaustive nature is sub-optimal for training a QA model. In contrast, we leverage the highly structured aspect of procedural text and represent each step and the overall flow of the procedure as graphs. We then condition on graph nodes to automatically generate QA pairs in an exhaustive and controllable manner. Comprehensive evaluations of our method show that: 1) small models trained with our 
    
[^9]: 评估大型语言模型在从成人重症护理电子病历中提取语义概念的背景下的应用

    Evaluation of General Large Language Models in Contextually Assessing Semantic Concepts Extracted from Adult Critical Care Electronic Health Record Notes. (arXiv:2401.13588v1 [cs.CL])

    [http://arxiv.org/abs/2401.13588](http://arxiv.org/abs/2401.13588)

    在实际医疗领域中对大型语言模型（LLMs）进行更深入和实用的评估是必要的，该研究旨在评估LLMs在成人重症护理医学复杂环境中的表现。

    

    鉴于大型语言模型（LLMs）的出色表现，该研究将焦点转向了医疗健康领域。然而，它们在实际临床应用中的表现尚未得到充分探讨。传统的问答任务评估不能完全捕捉到复杂的上下文信息。这种差距凸显了在实际医疗领域中对LLMs进行更深入和实用的评估的需求。该研究旨在使用系统化和易于理解的分析方法，包括临床医生注释和裁定，评估LLMs在成人重症护理医学复杂环境中的表现。

    The field of healthcare has increasingly turned its focus towards Large Language Models (LLMs) due to their remarkable performance. However, their performance in actual clinical applications has been underexplored. Traditional evaluations based on question-answering tasks don't fully capture the nuanced contexts. This gap highlights the need for more in-depth and practical assessments of LLMs in real-world healthcare settings. Objective: We sought to evaluate the performance of LLMs in the complex clinical context of adult critical care medicine using systematic and comprehensible analytic methods, including clinician annotation and adjudication. Methods: We investigated the performance of three general LLMs in understanding and processing real-world clinical notes. Concepts from 150 clinical notes were identified by MetaMap and then labeled by 9 clinicians. Each LLM's proficiency was evaluated by identifying the temporality and negation of these concepts using different prompts for an
    
[^10]: LLM指令微调中的提示权重实验

    Prompt Weight Experiments for LLM Instruction Fine-Tuning. (arXiv:2401.13586v1 [cs.LG])

    [http://arxiv.org/abs/2401.13586](http://arxiv.org/abs/2401.13586)

    LLM指令微调中，对于短提示完成数据集，提示词标记分类损失加权（PLW）与性能呈负二次关系，而长提示完成数据集则不受PLW影响。

    

    我们进行了一项小型研究，分析了提示词标记分类损失加权（PLW）如何影响在指令任务上进行微调的7B大小的LLaMA模型的性能。我们使用多个指令数据集重现了斯坦福大学的Alpaca实验，其中包括LLaMA 1和LLaMA 2。我们发现，在我们的短提示完成数据集上微调的模型与PLW之间存在负二次关系，而在长提示完成数据集上微调的模型不受PLW的影响。

    We present a small study analyzing how prompt token classification loss weighting (PLW) affects the performance of 7B-size LLaMA models fine-tuned on instruction tasks. We recreated Stanford's Alpaca experiment with both LLaMA 1 and LLaMA 2 using multiple instruction datasets. We found that models fine-tuned on our short-completion dataset have a negative quadratic relationship with PLW while models fine-tuned on long-completion datasets were unaffected by PLW.
    
[^11]: 图像上采样方法的公平性基准测试

    Benchmarking the Fairness of Image Upsampling Methods. (arXiv:2401.13555v1 [cs.CV])

    [http://arxiv.org/abs/2401.13555](http://arxiv.org/abs/2401.13555)

    这项工作提出了一个评估有条件生成模型性能和公平性的框架，并针对图像上采样应用创建了一个涵盖现代方法的基准测试。实证研究发现使用无偏训练集对结果至关重要，并揭示了不同算法对该问题的响应变化。

    

    近年来，深度生成模型在创建合成媒体（如图像和视频）方面取得了快速发展。虽然这些模型在日常任务中的实际应用非常诱人，但评估其公平性相关的潜在风险至关重要。在这项工作中，我们引入了一个全面的框架，用于评估有条件生成模型的性能和公平性。我们开发了一套度量标准——受监督公平性的灵感来源——来评估模型的公平性和多样性。我们针对图像上采样这个特定应用，创建了一个涵盖各种现代上采样方法的基准测试。作为基准测试的一部分，我们引入了UnfairFace，这是FairFace的一个子集，复制了常见大规模人脸数据集的种族分布。我们的实证研究凸显了使用无偏训练集的重要性，并揭示了算法对该问题的响应变化。

    Recent years have witnessed a rapid development of deep generative models for creating synthetic media, such as images and videos. While the practical applications of these models in everyday tasks are enticing, it is crucial to assess the inherent risks regarding their fairness. In this work, we introduce a comprehensive framework for benchmarking the performance and fairness of conditional generative models. We develop a set of metrics$\unicode{x2013}$inspired by their supervised fairness counterparts$\unicode{x2013}$to evaluate the models on their fairness and diversity. Focusing on the specific application of image upsampling, we create a benchmark covering a wide variety of modern upsampling methods. As part of the benchmark, we introduce UnfairFace, a subset of FairFace that replicates the racial distribution of common large-scale face datasets. Our empirical study highlights the importance of using an unbiased training set and reveals variations in how the algorithms respond to 
    
[^12]: 采用乐器特定输入表示和扩散外扩技术的表现力丰富的声学吉他声音合成

    Expressive Acoustic Guitar Sound Synthesis with an Instrument-Specific Input Representation and Diffusion Outpainting. (arXiv:2401.13498v1 [cs.SD])

    [http://arxiv.org/abs/2401.13498](http://arxiv.org/abs/2401.13498)

    本文提出了一种采用乐器特定输入表示和扩散外扩技术的表现力丰富的声学吉他声音合成模型，通过定量和定性评估，展示了该模型比其他模型具有更高的音频质量和更加逼真的音色声音。

    

    由于复音和表现的高度变异，合成演奏吉他声音是一项极具挑战性的任务。最近，深度生成模型在从音乐乐谱中合成表现力丰富的多音乐器声音方面显示出了有希望的结果，通常使用通用的MIDI输入。在这项工作中，我们提出了一种具有自定义乐器输入表示的表现力丰富的声学吉他声音合成模型，我们称之为guitarroll。我们使用基于扩散的外扩技术实现了所提出的方法，该技术可以生成具有长期一致性的音频。为了克服缺乏MIDI/音频配对数据集的问题，我们不仅使用了现有的吉他数据集，还从高质量的基于样本的吉他合成器中收集了数据。通过定量和定性评估，我们展示了我们提出的模型比基准模型具有更高的音频质量，并且比先前的领先工作生成更加逼真的音色声音。

    Synthesizing performing guitar sound is a highly challenging task due to the polyphony and high variability in expression. Recently, deep generative models have shown promising results in synthesizing expressive polyphonic instrument sounds from music scores, often using a generic MIDI input. In this work, we propose an expressive acoustic guitar sound synthesis model with a customized input representation to the instrument, which we call guitarroll. We implement the proposed approach using diffusion-based outpainting which can generate audio with long-term consistency. To overcome the lack of MIDI/audio-paired datasets, we used not only an existing guitar dataset but also collected data from a high quality sample-based guitar synthesizer. Through quantitative and qualitative evaluations, we show that our proposed model has higher audio quality than the baseline model and generates more realistic timbre sounds than the previous leading work.
    
[^13]: 分离的物理信息神经网络用于弹性问题的求解

    Separable Physics-Informed Neural Networks for the solution of elasticity problems. (arXiv:2401.13486v1 [math.NA])

    [http://arxiv.org/abs/2401.13486](http://arxiv.org/abs/2401.13486)

    本文提出了一种基于分离的物理信息神经网络（SPINN）和深度能量方法（DEM）的弹性问题求解方法，该方法在收敛速度和精度上显著优于传统的物理信息神经网络（PINN），并且在复杂几何体上的线性弹性理论问题方面具有应用潜力。

    

    本文提出了一种基于分离的物理信息神经网络（SPINN）和深度能量方法（DEM）的弹性问题求解方法。通过数值实验，证明该方法的收敛速度和精度显著高于传统的物理信息神经网络（PINN）以及基于偏微分方程（PDEs）的SPINN。此外，利用SPINN在DEM方法框架下，还可以解决复杂几何体上的线性弹性理论问题，而传统的PINN在偏微分方程框架下无法实现。所考虑的问题在几何形状、加载和材料参数方面非常接近工业问题。

    A method for solving elasticity problems based on separable physics-informed neural networks (SPINN) in conjunction with the deep energy method (DEM) is presented. Numerical experiments have been carried out for a number of problems showing that this method has a significantly higher convergence rate and accuracy than the vanilla physics-informed neural networks (PINN) and even SPINN based on a system of partial differential equations (PDEs). In addition, using the SPINN in the framework of DEM approach it is possible to solve problems of the linear theory of elasticity on complex geometries, which is unachievable with the help of PINNs in frames of partial differential equations. Considered problems are very close to the industrial problems in terms of geometry, loading, and material parameters.
    
[^14]: AI思想如何影响人类思想的创造力、多样性和进化：来自一个大规模动态实验的证据

    How AI Ideas Affect the Creativity, Diversity, and Evolution of Human Ideas: Evidence From a Large, Dynamic Experiment. (arXiv:2401.13481v1 [cs.CY])

    [http://arxiv.org/abs/2401.13481](http://arxiv.org/abs/2401.13481)

    AI思想对个体创造力没有影响，但增加了整体思想多样性的数量和变化速率。

    

    大规模语言模型输出的接触正在迅速增加。观看到AI生成的思想将如何影响人类思想？我们进行了一个实验（800+参与者，40+个国家），参与者观看了来自ChatGPT或之前实验参与者的创意思想，然后进行了自己的创意思考。我们变化了AI生成示例的数量（无、低、高曝光）以及示例是否标记为“AI”（披露）。我们的动态实验设计 - 在同一实验条件下，使用之前参与者的思想作为未来参与者的刺激 - 模拟了文化创造的相互依赖过程：创造性思想建立在之前的思想基础上。因此，我们捕捉到了LLM“在文化循环中”的复合效应。我们发现高AI曝光（但不是低AI曝光）并没有影响个人思想的创造力，但增加了整体思想多样性的平均数量和变化速率。AI使思想多样性的累积效应增强了。

    Exposure to large language model output is rapidly increasing. How will seeing AI-generated ideas affect human ideas? We conducted an experiment (800+ participants, 40+ countries) where participants viewed creative ideas that were from ChatGPT or prior experimental participants and then brainstormed their own idea. We varied the number of AI-generated examples (none, low, or high exposure) and if the examples were labeled as 'AI' (disclosure). Our dynamic experiment design -- ideas from prior participants in an experimental condition are used as stimuli for future participants in the same experimental condition -- mimics the interdependent process of cultural creation: creative ideas are built upon prior ideas. Hence, we capture the compounding effects of having LLMs 'in the culture loop'. We find that high AI exposure (but not low AI exposure) did not affect the creativity of individual ideas but did increase the average amount and rate of change of collective idea diversity. AI made 
    
[^15]: 从探索中成长: 基于基础模型的自我探索机器人框架

    Growing from Exploration: A self-exploring framework for robots based on foundation models. (arXiv:2401.13462v1 [cs.RO])

    [http://arxiv.org/abs/2401.13462](http://arxiv.org/abs/2401.13462)

    该论文提出了一个名为GExp的框架，可以使机器人在没有人类干预的情况下自主探索和学习。基于基础模型，GExp鼓励机器人通过自我生成的任务来理解和探索环境，并从有益的经验中获取技能。这使得机器人能够通过自我探索解决复杂任务。

    

    智能机器人是机器人领域的最终目标。现有工作通过基于学习或优化的方法来完成人类定义的任务。然而，使机器人能够自主地探索各种环境的挑战仍未解决。在本研究中，我们提出了一个名为GExp的框架，可以使机器人在没有人类干预的情况下自主探索和学习。为了实现这个目标，我们设计了包括自我探索、知识库构建和闭环反馈在内的基于基础模型的模块。受到婴儿与世界互动方式的启发，GExp鼓励机器人通过一系列自动生成的任务来理解和探索环境。在探索过程中，机器人将从有益的经验中获得有用于将来的技能。GExp使机器人通过自我探索解决复杂任务。

    Intelligent robot is the ultimate goal in the robotics field. Existing works leverage learning-based or optimization-based methods to accomplish human-defined tasks. However, the challenge of enabling robots to explore various environments autonomously remains unresolved. In this work, we propose a framework named GExp, which enables robots to explore and learn autonomously without human intervention. To achieve this goal, we devise modules including self-exploration, knowledge-base-building, and close-loop feedback based on foundation models. Inspired by the way that infants interact with the world, GExp encourages robots to understand and explore the environment with a series of self-generated tasks. During the process of exploration, the robot will acquire skills from beneficial experiences that are useful in the future. GExp provides robots with the ability to solve complex tasks through self-exploration. GExp work is independent of prior interactive knowledge and human interventio
    
[^16]: 通过多样性启示的多Agent诊断方法用于稳健性

    Multi-Agent Diagnostics for Robustness via Illuminated Diversity. (arXiv:2401.13460v1 [cs.LG])

    [http://arxiv.org/abs/2401.13460](http://arxiv.org/abs/2401.13460)

    MADRID是一种新方法，通过生成多样化的对抗场景来揭示预训练多Agent策略的战略漏洞，并通过遗憾值衡量漏洞的程度。

    

    在快速发展的多Agent系统领域中，确保在陌生和敌对环境中的稳健性至关重要。尽管这些系统在熟悉环境中表现出色，但在新情况下往往会因为训练阶段的过拟合而失败。在既包含合作又包含竞争行为的环境中，这一问题尤为突出，体现了过拟合和泛化挑战的双重性质。为了解决这个问题，我们提出了通过多样性启示的多Agent稳健性诊断（MADRID），这是一种生成多Agent策略中暴露战略漏洞的多样化对抗场景的新方法。MADRID利用开放式学习的概念，导航对抗环境的广阔空间，使用目标策略的遗憾值来衡量这些环境的漏洞。我们在11vs11版的Google Research Football上评估了MADRID的有效性。

    In the rapidly advancing field of multi-agent systems, ensuring robustness in unfamiliar and adversarial settings is crucial. Notwithstanding their outstanding performance in familiar environments, these systems often falter in new situations due to overfitting during the training phase. This is especially pronounced in settings where both cooperative and competitive behaviours are present, encapsulating a dual nature of overfitting and generalisation challenges. To address this issue, we present Multi-Agent Diagnostics for Robustness via Illuminated Diversity (MADRID), a novel approach for generating diverse adversarial scenarios that expose strategic vulnerabilities in pre-trained multi-agent policies. Leveraging the concepts from open-ended learning, MADRID navigates the vast space of adversarial settings, employing a target policy's regret to gauge the vulnerabilities of these settings. We evaluate the effectiveness of MADRID on the 11vs11 version of Google Research Football, one o
    
[^17]: 以低计算资源消耗为中心的高效知识库问答框架：基于线索引导路径探索

    Clue-Guided Path Exploration: An Efficient Knowledge Base Question-Answering Framework with Low Computational Resource Consumption. (arXiv:2401.13444v1 [cs.CL])

    [http://arxiv.org/abs/2401.13444](http://arxiv.org/abs/2401.13444)

    该论文介绍了一种以低计算资源消耗为中心的高效知识库问答框架，通过引入线索引导路径探索的方式，将知识库与大型语言模型高效地融合，从而降低了对模型能力的要求，并在实验证明了其优越性能。

    

    在最近的研究中，大型语言模型（LLMs）展示了出色的能力。然而，更新它们的知识面会带来挑战，当面对不熟悉的查询时可能导致不准确性。虽然已经研究了将知识图谱与LLMs集成的方法，但现有方法将LLMs视为主要的决策者，对其能力提出了较高的要求。对于计算成本较低且性能相对较差的LLMs来说，这是不太合适的。本文介绍了一种以线索引导路径探索为核心的知识库问答框架（CGPE），它将知识库与LLMs高效地融合，对模型的能力要求较低。受人类手动检索知识的方法启发，CGPE利用问题中的信息作为线索，系统地探索知识库中所需的知识路径。开源数据集上的实验证明，CGPE优于先前的方法，并且非常适用于计算成本较低且性能较差的LLMs。

    In recent times, large language models (LLMs) have showcased remarkable capabilities. However, updating their knowledge poses challenges, potentially leading to inaccuracies when confronted with unfamiliar queries. While integrating knowledge graphs with LLMs has been explored, existing approaches treat LLMs as primary decision-makers, imposing high demands on their capabilities. This is particularly unsuitable for LLMs with lower computational costs and relatively poorer performance. In this paper, we introduce a Clue-Guided Path Exploration framework (CGPE) that efficiently merges a knowledge base with an LLM, placing less stringent requirements on the model's capabilities. Inspired by the method humans use to manually retrieve knowledge, CGPE employs information from the question as clues to systematically explore the required knowledge path within the knowledge base. Experiments on open-source datasets reveal that CGPE outperforms previous methods and is highly applicable to LLMs w
    
[^18]: 半监督耦合薄板样条模型用于旋转校正及更多应用

    Semi-Supervised Coupled Thin-Plate Spline Model for Rotation Correction and Beyond. (arXiv:2401.13432v1 [cs.CV])

    [http://arxiv.org/abs/2401.13432](http://arxiv.org/abs/2401.13432)

    本文提出了一种半监督耦合薄板样条模型，用于处理旋转校正等图像变形任务。通过耦合多个薄板样条变换，有效消除了插值误差，突破了控制点数量瓶颈，并且通过半监督学习方法，可以在有限的带标注样本条件下，充分利用未标注样本来提高模型性能。

    

    薄板样条（TPS）是一种允许使用控制点运动表示弹性非线性变换的主要变形方式。随着控制点数量的增加，变形越来越灵活，但常常会遇到由不希望的问题（如内容畸变）导致的瓶颈。本文探索了TPS在基于单幅图像的变形任务中的通用应用，如旋转校正、矩形化和肖像校正。为了突破这个瓶颈，我们提出了耦合薄板样条模型（CoupledTPS），它将有限控制点的多个TPS迭代耦合成一个更灵活和强大的变换。具体来说，我们首先设计了一个迭代搜索来根据当前潜在条件预测新的控制点。然后，我们以变形流作为连接不同TPS变换的桥梁，有效消除了多个变形引起的插值误差。此外，鉴于繁琐的注释成为制约深度学习方法效果的重要因素，我们还提出了一种半监督学习方法，使得在有限带标注样本条件下，能充分利用未标注样本来提高模型性能。

    Thin-plate spline (TPS) is a principal warp that allows for representing elastic, nonlinear transformation with control point motions. With the increase of control points, the warp becomes increasingly flexible but usually encounters a bottleneck caused by undesired issues, e.g., content distortion. In this paper, we explore generic applications of TPS in single-image-based warping tasks, such as rotation correction, rectangling, and portrait correction. To break this bottleneck, we propose the coupled thin-plate spline model (CoupledTPS), which iteratively couples multiple TPS with limited control points into a more flexible and powerful transformation. Concretely, we first design an iterative search to predict new control points according to the current latent condition. Then, we present the warping flow as a bridge for the coupling of different TPS transformations, effectively eliminating interpolation errors caused by multiple warps. Besides, in light of the laborious annotation co
    
[^19]: 因果感知

    Causal Perception. (arXiv:2401.13408v1 [cs.AI])

    [http://arxiv.org/abs/2401.13408](http://arxiv.org/abs/2401.13408)

    这项研究提出了因果感知的概念，并将其应用于自动决策系统中。感知对决策的公平性有重要影响，因为公平性是与背景相关的，并且其解释取决于评判人是谁。

    

    当两个个体对相同的信息进行不同解读时，感知会发生。尽管这是一个已知现象，对决策中偏见有影响，但是感知在自动决策系统中仍然被忽视。感知对于ADM系统的公平性或公平使用具有重要影响，因为公平本身是与背景相关的，其解释取决于评判人是谁。本文将感知在因果推理中形式化，以捕捉个体的解释行为。我们还将个体经验形式化为额外的因果知识，个体会使用这些知识。此外，我们定义和讨论了易引发感知的属性，即易引发感知的属性。敏感属性，如性别和种族，就是易引发感知的明确示例。我们根据因果原则定义了两种感知，即不忠实感知和不一致感知。

    Perception occurs when two individuals interpret the same information differently. Despite being a known phenomenon with implications for bias in decision-making, as individuals' experience determines interpretation, perception remains largely overlooked in automated decision-making (ADM) systems. In particular, it can have considerable effects on the fairness or fair usage of an ADM system, as fairness itself is context-specific and its interpretation dependent on who is judging. In this work, we formalize perception under causal reasoning to capture the act of interpretation by an individual. We also formalize individual experience as additional causal knowledge that comes with and is used by an individual. Further, we define and discuss loaded attributes, which are attributes prone to evoke perception. Sensitive attributes, such as gender and race, are clear examples of loaded attributes. We define two kinds of causal perception, unfaithful and inconsistent, based on the causal prop
    
[^20]: 过去、现在、未来：对UMBRELLA物联网试验平台中人工智能应用案例的全面探索

    Past, Present, Future: A Comprehensive Exploration of AI Use Cases in the UMBRELLA IoT Testbed. (arXiv:2401.13346v1 [cs.NI])

    [http://arxiv.org/abs/2401.13346](http://arxiv.org/abs/2401.13346)

    UMBRELLA是一个大规模的物联网试验平台，具有多个应用案例，包括自动街灯监控、数字孪生环境、联邦学习框架和容器化应用入侵检测。未来，UMBRELLA还有潜力用于智能城市和多机器人群感知应用。

    

    UMBRELLA是一个大规模的开放式物联网生态系统，包括200多个多传感器多无线节点、20个协作机器人和支持边缘智能的设备。本文提供了UMBRELLA在实际物联网系统中已实现和潜在的人工智能能力的指南。详细介绍了四个现有的UMBRELLA应用程序：1）用于检测问题并触发维护警报的自动街灯监控；2）提供增强的空气质量感知和降低成本的建筑环境数字孪生；3）用于减少通信开销的大规模联邦学习框架；4）用于识别恶意活动的容器化应用入侵检测。此外，还概述了UMBRELLA在未来智能城市和多机器人群感知应用中的潜力，增强了语义通信和多智能体规划。最后，为了实现上述用例，我们讨论了UMBRELLA的要求。

    UMBRELLA is a large-scale, open-access Internet of Things (IoT) ecosystem incorporating over 200 multi-sensor multi-wireless nodes, 20 collaborative robots, and edge-intelligence-enabled devices. This paper provides a guide to the implemented and prospective artificial intelligence (AI) capabilities of UMBRELLA in real-world IoT systems. Four existing UMBRELLA applications are presented in detail: 1) An automated streetlight monitoring for detecting issues and triggering maintenance alerts; 2) A Digital twin of building environments providing enhanced air quality sensing with reduced cost; 3) A large-scale Federated Learning framework for reducing communication overhead; and 4) An intrusion detection for containerised applications identifying malicious activities. Additionally, the potential of UMBRELLA is outlined for future smart city and multi-robot crowdsensing applications enhanced by semantic communications and multi-agent planning. Finally, to realise the above use-cases we disc
    
[^21]: 全贝叶斯神经网络显著性检验

    Full Bayesian Significance Testing for Neural Networks. (arXiv:2401.13335v1 [stat.ML])

    [http://arxiv.org/abs/2401.13335](http://arxiv.org/abs/2401.13335)

    该论文提出了一种全贝叶斯神经网络显著性检验方法（nFBST），通过利用贝叶斯神经网络拟合非线性和多维关系，并计算证据值来替代传统方法中的理论推导，该方法能够测试全局、局部和实例级的显著性。

    

    显著性检验旨在确定给定观测结果，关于总体分布的命题是否为真。然而，传统的显著性检验通常需要推导出检验统计量的分布，无法处理复杂的非线性关系。本文提出了一种用于神经网络的全贝叶斯显著性检验方法，称为nFBST，旨在克服传统方法在关系表征方面的局限性。利用贝叶斯神经网络拟合非线性和多维关系，并通过计算证据值而不是进行繁琐的理论推导来避免错误。此外，nFBST还可以测试全局、局部和实例级的显著性，这是之前的检验方法所不关注的。此外，nFBST是一个通用框架，可以根据所选的度量进行扩展，如Grad-nFBST，LRP-nFBST，DeepLIFT-nFBST。

    Significance testing aims to determine whether a proposition about the population distribution is the truth or not given observations. However, traditional significance testing often needs to derive the distribution of the testing statistic, failing to deal with complex nonlinear relationships. In this paper, we propose to conduct Full Bayesian Significance Testing for neural networks, called \textit{n}FBST, to overcome the limitation in relationship characterization of traditional approaches. A Bayesian neural network is utilized to fit the nonlinear and multi-dimensional relationships with small errors and avoid hard theoretical derivation by computing the evidence value. Besides, \textit{n}FBST can test not only global significance but also local and instance-wise significance, which previous testing methods don't focus on. Moreover, \textit{n}FBST is a general framework that can be extended based on the measures selected, such as Grad-\textit{n}FBST, LRP-\textit{n}FBST, DeepLIFT-\t
    
[^22]: 可解释性贝叶斯优化

    Explainable Bayesian Optimization. (arXiv:2401.13334v1 [cs.LG])

    [http://arxiv.org/abs/2401.13334](http://arxiv.org/abs/2401.13334)

    本论文介绍了一种可解释性贝叶斯优化的方法，通过TNTRules生成高质量的解释，填补了贝叶斯优化和可解释人工智能之间的间隙。

    

    在工业领域，贝叶斯优化（BO）被广泛应用于人工智能协作参数调优的控制系统中。然而，由于近似误差和简化目标，BO的解决方案可能偏离人类专家的真实目标，需要后续调整。BO的黑盒特性限制了协作调优过程，因为专家不信任BO的建议。目前的可解释人工智能（XAI）方法不适用于优化问题，因此无法解决此间隙。为了填补这一间隙，我们提出了TNTRules（TUNE-NOTUNE规则），一种事后基于规则的可解释性方法，通过多目标优化生成高质量的解释。我们对基准优化问题和实际超参数优化任务的评估表明，TNTRules在生成高质量解释方面优于最先进的XAI方法。这项工作对BO和XAI的交叉领域做出了贡献，提供了可解释的优化方法。

    In industry, Bayesian optimization (BO) is widely applied in the human-AI collaborative parameter tuning of cyber-physical systems. However, BO's solutions may deviate from human experts' actual goal due to approximation errors and simplified objectives, requiring subsequent tuning. The black-box nature of BO limits the collaborative tuning process because the expert does not trust the BO recommendations. Current explainable AI (XAI) methods are not tailored for optimization and thus fall short of addressing this gap. To bridge this gap, we propose TNTRules (TUNE-NOTUNE Rules), a post-hoc, rule-based explainability method that produces high quality explanations through multiobjective optimization. Our evaluation of benchmark optimization problems and real-world hyperparameter optimization tasks demonstrates TNTRules' superiority over state-of-the-art XAI methods in generating high quality explanations. This work contributes to the intersection of BO and XAI, providing interpretable opt
    
[^23]: 有关算法决策的信息：探索受到算法决策影响的人的信息需求。

    Information That Matters: Exploring Information Needs of People Affected by Algorithmic Decisions. (arXiv:2401.13324v1 [cs.HC])

    [http://arxiv.org/abs/2401.13324](http://arxiv.org/abs/2401.13324)

    本研究探讨了受算法决策影响的人的信息需求，发现解释往往不能满足他们的关注点，导致对监管框架的理解和遵守产生障碍。为了解决这个问题，研究团队提出了XAI初学者问题库，涵盖了就业预测和健康监测两个领域中受影响利益相关者的信息需求。

    

    AI系统的解释很少涉及到受算法决策影响的人的信息需求。这种传达信息与受影响利益相关者所关心的信息之间的差距可能阻碍对监管框架（如AI法案）的理解和遵守。为了解决这个差距，我们提出了“XAI初学者问题库”：这是一个涵盖两个算法决策应用领域（就业预测和健康监测）中受影响利益相关者信息需求的目录，包括数据、系统背景、系统使用和系统规范等类别。信息需求是通过访谈研究收集的，参与者根据自己的问题获得解释。参与者还报告了他们的理解和决策信心，结果显示，尽管在接受解释后信心倾向于增加，但参与者也面临着理解上的挑战，如无法解释为什么自己的理解感觉不完整。解释还对理解产生了影响。

    Explanations of AI systems rarely address the information needs of people affected by algorithmic decision-making (ADM). This gap between conveyed information and information that matters to affected stakeholders can impede understanding and adherence to regulatory frameworks such as the AI Act. To address this gap, we present the "XAI Novice Question Bank": A catalog of affected stakeholders' information needs in two ADM use cases (employment prediction and health monitoring), covering the categories data, system context, system usage, and system specifications. Information needs were gathered in an interview study where participants received explanations in response to their inquiries. Participants further reported their understanding and decision confidence, showing that while confidence tended to increase after receiving explanations, participants also met understanding challenges, such as being unable to tell why their understanding felt incomplete. Explanations further influenced
    
[^24]: 深度学习用于改进合成窄带成像中的息肉检测

    Deep Learning for Improved Polyp Detection from Synthetic Narrow-Band Imaging. (arXiv:2401.13315v1 [eess.IV])

    [http://arxiv.org/abs/2401.13315](http://arxiv.org/abs/2401.13315)

    本文提出了一种基于CycleGAN的框架，将使用常规白光成像（WLI）捕获的图像转化为合成窄带成像（SNBI），用于改进没有窄带成像（NBI）时的WLI上的息肉检测效果。研究结果表明，使用合成NBI图像进行目标检测可以实现比原始WLI图像更好的息肉检测。

    

    为了应对结直肠癌（CRC）日益增长的患病率，息肉检测和切除的筛查计划已经证明了它们的有效性。结肠镜被认为是CRC筛查的最佳方法。为了简化检查过程，已经开发了基于深度学习的自动息肉检测方法，适用于传统的白光成像（WLI）。与WLI相比，窄带成像（NBI）在结肠镜检查过程中可以改善息肉分类，但需要特殊设备。我们提出了一种基于CycleGAN的框架，将使用常规WLI捕获的图像转化为合成NBI（SNBI），作为在没有NBI时改进WLI上的目标检测的预处理方法。本文首先展示了与相对相似的WLI数据集相比，NBI上可以取得更好的息肉检测结果。其次，实验结果表明，从WLI生成的SNBI图像上进行的我们提出的模态转换可以实现比原始WLI上更好的息肉检测效果。

    To cope with the growing prevalence of colorectal cancer (CRC), screening programs for polyp detection and removal have proven their usefulness. Colonoscopy is considered the best-performing procedure for CRC screening. To ease the examination, deep learning based methods for automatic polyp detection have been developed for conventional white-light imaging (WLI). Compared with WLI, narrow-band imaging (NBI) can improve polyp classification during colonoscopy but requires special equipment. We propose a CycleGAN-based framework to convert images captured with regular WLI to synthetic NBI (SNBI) as a pre-processing method for improving object detection on WLI when NBI is unavailable. This paper first shows that better results for polyp detection can be achieved on NBI compared to a relatively similar dataset of WLI. Secondly, experimental results demonstrate that our proposed modality translation can achieve improved polyp detection on SNBI images generated from WLI compared to the orig
    
[^25]: ConTextual: 在大型多模态模型中评估上下文敏感的文本富有视觉推理

    ConTextual: Evaluating Context-Sensitive Text-Rich Visual Reasoning in Large Multimodal Models. (arXiv:2401.13311v1 [cs.CV])

    [http://arxiv.org/abs/2401.13311](http://arxiv.org/abs/2401.13311)

    本文介绍了一个新颖的基准ConTextual，用于评估能够进行上下文敏感的文本富有视觉推理的大型多模态模型。研究发现，目前最好的模型GPT-4V在抽象类别表现出色，但在整体性能上仍然落后于人类，存在改进的空间。

    

    最近人工智能的进步导致了大型多模态模型（LMMs）的发展，这些模型能够处理涉及文本和图像内容的复杂任务，例如在公共场所导航地图。本文介绍了ConTextual，这是一个新颖的基准，包括专门设计的指令，用于评估LMMs在执行上下文敏感的文本富有视觉推理方面的能力。ConTextual强调了多样的现实世界场景（例如时间阅读、导航、购物等），要求更深入地理解文本和视觉元素之间的相互作用。我们的研究结果显示，最佳表现的LMM，GPT-4V(ision)，与人类能力之间存在30.8%的性能差距，使用人类评估指出在上下文敏感的文本富有视觉推理方面还有很大的改进空间。值得注意的是，虽然GPT-4V在抽象类别（如模因和引文解释）中表现出色，但其整体性能仍然落后于人类。

    Recent advancements in AI have led to the development of large multimodal models (LMMs) capable of processing complex tasks involving joint reasoning over text and visual content in the image (e.g., navigating maps in public places). This paper introduces ConTextual, a novel benchmark comprising instructions designed explicitly to evaluate LMMs' ability to perform context-sensitive text-rich visual reasoning. ConTextual emphasizes diverse real-world scenarios (e.g., time-reading, navigation, shopping and more) demanding a deeper understanding of the interactions between textual and visual elements. Our findings reveal a significant performance gap of 30.8% between the best-performing LMM, GPT-4V(ision), and human capabilities using human evaluation indicating substantial room for improvement in context-sensitive text-rich visual reasoning. Notably, while GPT-4V excelled in abstract categories like meme and quote interpretation, its overall performance still lagged behind humans. In add
    
[^26]: 通过大型语言模型之间的多模态辩论实现可解释的有害模因检测

    Towards Explainable Harmful Meme Detection through Multimodal Debate between Large Language Models. (arXiv:2401.13298v1 [cs.CL])

    [http://arxiv.org/abs/2401.13298](http://arxiv.org/abs/2401.13298)

    本文提出了一种通过大型语言模型之间的多模态辩论实现可解释的有害模因检测方法。通过推理有害和无害立场之间的相互矛盾理由，生成可读的解释，提升有害模因检测的效果。

    

    社交媒体时代充斥着互联网模因，需要明确掌握和有效识别有害模因。由于模因中蕴含的隐含含义不能通过表面文本和图像明确传达，这一任务带来了重大挑战。然而，现有的有害模因检测方法未提供可读的解释以揭示这种隐含含义以支持其检测决策。在本文中，我们提出了一种可解释的有害模因检测方法，通过推理有害和无害立场之间的相互矛盾理由来实现。具体而言，受大型语言模型在文本生成和推理方面的强大能力启发，我们首先引发大型语言模型之间的多模态辩论，生成基于矛盾论据的解释。然后，我们提出使用精调的小型语言模型作为辩论裁判来推断有害性，以促进有害和无害信息的多模态融合。

    The age of social media is flooded with Internet memes, necessitating a clear grasp and effective identification of harmful ones. This task presents a significant challenge due to the implicit meaning embedded in memes, which is not explicitly conveyed through the surface text and image. However, existing harmful meme detection methods do not present readable explanations that unveil such implicit meaning to support their detection decisions. In this paper, we propose an explainable approach to detect harmful memes, achieved through reasoning over conflicting rationales from both harmless and harmful positions. Specifically, inspired by the powerful capacity of Large Language Models (LLMs) on text generation and reasoning, we first elicit multimodal debate between LLMs to generate the explanations derived from the contradictory arguments. Then we propose to fine-tune a small language model as the debate judge for harmfulness inference, to facilitate multimodal fusion between the harmfu
    
[^27]: RefreshNet: 通过分层刷新学习多尺度动态

    RefreshNet: Learning Multiscale Dynamics through Hierarchical Refreshing. (arXiv:2401.13282v1 [cs.LG])

    [http://arxiv.org/abs/2401.13282](http://arxiv.org/abs/2401.13282)

    RefreshNet是一个多尺度框架，通过分层刷新机制来学习复杂系统的动态，实现了计算效率和预测准确性的平衡。

    

    预测复杂系统动力学，特别是对于长期预测，始终受到误差累积和计算负担的限制。本研究提出了RefreshNet，这是一个多尺度框架，旨在克服这些挑战，提供了计算效率和预测准确性之间的前所未有的平衡。RefreshNet结合了卷积自动编码器，用于识别捕捉动态的基本特征的降阶潜在空间，并在潜在空间内策略性地使用多个时间分辨率不同的递归神经网络（RNN）块，从而允许多个时间尺度上捕捉潜在动态。RefreshNet中的独特的“刷新”机制允许较粗糙的块重新设置较细块的输入，有效控制和减轻误差累积。这种设计在计算效率和预测准确性方面表现出优越性，特别是在长期预测中。

    Forecasting complex system dynamics, particularly for long-term predictions, is persistently hindered by error accumulation and computational burdens. This study presents RefreshNet, a multiscale framework developed to overcome these challenges, delivering an unprecedented balance between computational efficiency and predictive accuracy. RefreshNet incorporates convolutional autoencoders to identify a reduced order latent space capturing essential features of the dynamics, and strategically employs multiple recurrent neural network (RNN) blocks operating at varying temporal resolutions within the latent space, thus allowing the capture of latent dynamics at multiple temporal scales. The unique "refreshing" mechanism in RefreshNet allows coarser blocks to reset inputs of finer blocks, effectively controlling and alleviating error accumulation. This design demonstrates superiority over existing techniques regarding computational efficiency and predictive accuracy, especially in long-term
    
[^28]: AI助手是否能知道自己不知道的事情?

    Can AI Assistants Know What They Don't Know?. (arXiv:2401.13275v1 [cs.CL])

    [http://arxiv.org/abs/2401.13275](http://arxiv.org/abs/2401.13275)

    本文研究了AI助手是否能知道自己不知道的事情，并通过自然语言表达出来的问题。为了回答这个问题，我们构建了一个特定模型的"I don't know"（Idk）数据集，并与AI助手进行对齐。

    

    最近，基于大型语言模型（LLMs）的AI助手在对话、解决数学问题、编写代码和使用工具等许多任务中表现出令人惊讶的性能。尽管LLMs具有深入的世界知识，但在面对某些知识密集型任务（如开放领域问答）时仍然会出现事实错误。AI助手的这种不真实回答可能在实际应用中造成重大风险。我们认为，AI助手拒绝回答自己不知道的问题是减少幻觉和使助手真实的关键方法。因此，在本文中，我们提出问题“AI助手是否能知道自己不知道的事情，并通过自然语言表达出来？”为了回答这个问题，我们为助手构建了一个特定模型的“I don't know”(Idk)数据集，其中包含了已知和未知的问题，基于现有的开放领域问答数据集。然后我们将助手与其相应的Idk数据进行对齐。

    Recently, AI assistants based on large language models (LLMs) show surprising performance in many tasks, such as dialogue, solving math problems, writing code, and using tools. Although LLMs possess intensive world knowledge, they still make factual errors when facing some knowledge intensive tasks, like open-domain question answering. These untruthful responses from the AI assistant may cause significant risks in practical applications. We believe that an AI assistant's refusal to answer questions it does not know is a crucial method for reducing hallucinations and making the assistant truthful. Therefore, in this paper, we ask the question "Can AI assistants know what they don't know and express them through natural language?" To answer this question, we construct a model-specific "I don't know" (Idk) dataset for an assistant, which contains its known and unknown questions, based on existing open-domain question answering datasets. Then we align the assistant with its corresponding I
    
[^29]: 通过利用音频场景语义实现自动图像上色

    Audio-Infused Automatic Image Colorization by Exploiting Audio Scene Semantics. (arXiv:2401.13270v1 [cs.CV])

    [http://arxiv.org/abs/2401.13270](http://arxiv.org/abs/2401.13270)

    本论文提出了一种使用音频场景语义进行自动图像上色的方法，通过引入音频作为辅助信息，降低了对场景语义理解的难度，并通过三个阶段的网络进行了实现。

    

    自动图像上色是一个具有不确定性的问题，需要对场景进行准确的语义理解，以估计灰度图像的合理颜色。虽然最近的基于交互的方法取得了令人印象深刻的性能，但对于自动上色来说，推断出逼真和准确的颜色仍然是一个非常困难的任务。为了降低对灰度场景的语义理解难度，本文尝试利用相应的音频，音频自然地包含了关于同一场景的额外语义信息。具体而言，提出了一种新颖的音频注入自动图像上色（AIAIC）网络，该网络分为三个阶段。首先，我们将彩色图像的语义作为桥梁，通过彩色图像的语义引导预训练上色网络。其次，利用音频与视频的自然共现来学习音频和视觉场景之间的颜色语义相关性。第三，隐式音频语义表示被利用以引导图像上色。

    Automatic image colorization is inherently an ill-posed problem with uncertainty, which requires an accurate semantic understanding of scenes to estimate reasonable colors for grayscale images. Although recent interaction-based methods have achieved impressive performance, it is still a very difficult task to infer realistic and accurate colors for automatic colorization. To reduce the difficulty of semantic understanding of grayscale scenes, this paper tries to utilize corresponding audio, which naturally contains extra semantic information about the same scene. Specifically, a novel audio-infused automatic image colorization (AIAIC) network is proposed, which consists of three stages. First, we take color image semantics as a bridge and pretrain a colorization network guided by color image semantics. Second, the natural co-occurrence of audio and video is utilized to learn the color semantic correlations between audio and visual scenes. Third, the implicit audio semantic representati
    
[^30]: 设计分布式账本中减少交易手续费的重新分配机制

    Designing Redistribution Mechanisms for Reducing Transaction Fees in Blockchains. (arXiv:2401.13262v1 [cs.GT])

    [http://arxiv.org/abs/2401.13262](http://arxiv.org/abs/2401.13262)

    本研究设计了一种交易费重新分配机制，在分布式账本中减少交易手续费。这种机制使用折扣的形式重新分配VCG支付，以最小化交易手续费，并同时保证优化分配效率和用户激励兼容性。

    

    区块链采用交易费机制（TFMs）来确定哪些用户交易包含在区块中并确定其支付（即交易手续费）。需求增加和稀缺的区块资源导致用户交易手续费高涨。由于这些区块链是公共资源，降低交易手续费可能更为可取。为此，我们引入了交易费重新分配机制（TFRMs）-通过将TFMs所收集的VCG支付以折扣的形式重新分配，以最小化交易手续费。经典的重新分配机制（RMs）在确保优化分配效率（AE）和用户激励兼容性（UIC）的同时实现了这一目标。

    Blockchains deploy Transaction Fee Mechanisms (TFMs) to determine which user transactions to include in blocks and determine their payments (i.e., transaction fees). Increasing demand and scarce block resources have led to high user transaction fees. As these blockchains are a public resource, it may be preferable to reduce these transaction fees. To this end, we introduce Transaction Fee Redistribution Mechanisms (TFRMs) -- redistributing VCG payments collected from such TFM as rebates to minimize transaction fees. Classic redistribution mechanisms (RMs) achieve this while ensuring Allocative Efficiency (AE) and User Incentive Compatibility (UIC). Our first result shows the non-triviality of applying RM in TFMs. More concretely, we prove that it is impossible to reduce transaction fees when (i) transactions that are not confirmed do not receive rebates and (ii) the miner can strategically manipulate the mechanism. Driven by this, we propose \emph{Robust} TFRM (\textsf{R-TFRM}): a mech
    
[^31]: UniMS-RAG: 用于个性化对话系统的统一多源检索增强生成模型

    UniMS-RAG: A Unified Multi-source Retrieval-Augmented Generation for Personalized Dialogue Systems. (arXiv:2401.13256v1 [cs.CL])

    [http://arxiv.org/abs/2401.13256](http://arxiv.org/abs/2401.13256)

    这项研究提出了一种统一多源检索增强生成系统（UniMS-RAG），通过统一知识源选择、知识检索和回复生成三个子任务，使语言模型能够根据需求自适应地检索证据和评估关联性，从而生成个性化的回复。

    

    大型语言模型在许多自然语言理解和生成任务中展示出了非凡的能力。然而，在对话系统中涉及到多个信息源时，个性化问题仍然是一个令人向往的属性。为了更好地计划和整合多个信息源在生成个性化回复中的使用，我们首先将其分解为三个子任务：知识源选择、知识检索和回复生成。然后，我们提出了一种新颖的统一多源检索增强生成系统（UniMS-RAG）。具体来说，我们在训练期间使用相同的序列到序列范式将这三个子任务统一起来，通过使用特殊的令牌，即行动令牌和评估令牌，能够自适应地检索证据并评估关联性。使语言模型能够生成行动令牌有助于与各种知识源进行交互，使其能够适应其上下文和生成个性化的回复。

    Large Language Models (LLMs) has shown exceptional capabilities in many natual language understanding and generation tasks. However, the personalization issue still remains a much-coveted property, especially when it comes to the multiple sources involved in the dialogue system. To better plan and incorporate the use of multiple sources in generating personalized response, we firstly decompose it into three sub-tasks: Knowledge Source Selection, Knowledge Retrieval, and Response Generation. We then propose a novel Unified Multi-Source Retrieval-Augmented Generation system (UniMS-RAG) Specifically, we unify these three sub-tasks with different formulations into the same sequence-to-sequence paradigm during the training, to adaptively retrieve evidences and evaluate the relevance on-demand using special tokens, called acting tokens and evaluation tokens. Enabling language models to generate acting tokens facilitates interaction with various knowledge sources, allowing them to adapt their
    
[^32]: 从随机到有信息选择数据：基于多样性的方法优化人类注释和少样本学习

    From Random to Informed Data Selection: A Diversity-Based Approach to Optimize Human Annotation and Few-Shot Learning. (arXiv:2401.13229v1 [cs.CL])

    [http://arxiv.org/abs/2401.13229](http://arxiv.org/abs/2401.13229)

    该论文介绍了一种基于多样性的方法，从而优化人类注释和少样本学习。传统的随机选择数据方法忽视了数据的特征和模型的需求，而该方法将考虑这些因素，以提高数据选择的效率。

    

    自然语言处理中的一个主要挑战是获取用于监督学习的注释数据。一种选择是使用众包平台进行数据注释。然而，众包引入了与注释者的经验、一致性和偏见相关的问题。另一种选择是使用零样本方法，但与少样本或完全监督的方法相比，零样本方法有其局限性。最近由大型语言模型推动的最新进展显示出潜力，但在数据严重受限的专业领域中，它们往往难以适应。因此，最常见的方法是人类随机注释一组数据点来构建初始数据集。然而，随机抽样数据进行注释通常效率低下，因为它忽视了数据的特征和模型的特定需求。当处理不平衡数据集时，情况更加糟糕，因为随机抽样倾向于严重偏向多数类别，导致过多的注释数据。

    A major challenge in Natural Language Processing is obtaining annotated data for supervised learning. An option is the use of crowdsourcing platforms for data annotation. However, crowdsourcing introduces issues related to the annotator's experience, consistency, and biases. An alternative is to use zero-shot methods, which in turn have limitations compared to their few-shot or fully supervised counterparts. Recent advancements driven by large language models show potential, but struggle to adapt to specialized domains with severely limited data. The most common approaches therefore involve the human itself randomly annotating a set of datapoints to build initial datasets. But randomly sampling data to be annotated is often inefficient as it ignores the characteristics of the data and the specific needs of the model. The situation worsens when working with imbalanced datasets, as random sampling tends to heavily bias towards the majority classes, leading to excessive annotated data. To
    
[^33]: 大规模异构图上基于大型语言模型的链接预测的可扩展性研究

    Scalable Link Prediction on Large-Scale Heterogeneous Graphs with Large Language Models. (arXiv:2401.13227v1 [cs.CL])

    [http://arxiv.org/abs/2401.13227](http://arxiv.org/abs/2401.13227)

    本研究探索了在大规模异构图上应用大型语言模型进行图学习的方法，提出了LPNL框架用于可扩展链接预测。通过创新的提示语和采样流程，以及分而治之的策略，成功解决了大规模图中的信息过载问题，并在实验中表现出了优越的性能。

    

    探索将大规模语言模型应用于图学习是一项新颖的努力。然而，大图中蕴含的大量信息给这一过程带来了重大挑战。本文侧重于链接预测任务，并介绍了LPNL（Link Prediction via Natural Language），这是一个基于大型语言模型的框架，用于大规模异构图上的可扩展链接预测。我们设计了能以自然语言表达图细节的创新提示语。我们提出了一个两阶段的采样流程，从大规模异构图中提取关键信息，并采用分而治之的策略来控制输入令牌数量在预定限制内，解决了信息过载的挑战。我们还通过自监督学习设计了一个用于链接预测的T5模型进行微调。在大型公共异构图上进行的广泛实验表明，LPNL的性能超过了各种先进的基准模型。

    Exploring the application of large-scale language models to graph learning is a novel endeavor. However, the vast amount of information inherent in large graphs poses significant challenges to this process. This paper focuses on the link prediction task and introduces LPNL (Link Prediction via Natural Language), a framework based on a large language model designed for scalable link prediction on large-scale heterogeneous graphs.We design novel prompts for link prediction that articulate graph details in natural language. We propose a two-stage sampling pipeline to extract crucial information from large-scale heterogeneous graphs, and a divide-and-conquer strategy to control the input token count within predefined limits, addressing the challenge of overwhelming information. We fine-tune a T5 model based on our self-supervised learning designed for for link prediction. Extensive experiments on a large public heterogeneous graphs demonstrate that LPNL outperforms various advanced baselin
    
[^34]: TAT-LLM: 一种针对表格和文本数据的专用语言模型用于离散推理

    TAT-LLM: A Specialized Language Model for Discrete Reasoning over Tabular and Textual Data. (arXiv:2401.13223v1 [cs.CL])

    [http://arxiv.org/abs/2401.13223](http://arxiv.org/abs/2401.13223)

    TAT-LLM是一种专门用于离散推理的语言模型，针对混合表格和文本数据上的问答任务。该模型通过分步流水线的方式，包括提取器、推理器和执行器，利用LLMs的强大能力来解决问题。而为了应对成本、延迟和数据安全风险等挑战，我们开发了TAT-LLM，一个专门针对此任务的较小LLM。

    

    在这项工作中，我们解决了在混合表格和文本数据上进行问答的问题，这在Web上非常常见（如SEC文件），通常需要离散推理能力。最近，像GPT-4这样的大型语言模型展示了强大的多步骤推理能力。我们考虑利用LLMs的强大能力来解决我们的任务。我们提出了面向表格和文本问答的分步流水线的抽象，包括提取器、推理器和执行器三个关键步骤，并首先设计了一份指令来实例化该流水线并验证GPT-4优于所有现有方法。然而，利用像GPT-4这样的在线LLM存在成本、延迟和数据安全风险等各种挑战，这促使我们专门针对此任务开发较小的LLM。我们通过对现有专家标注数据集自动生成的训练数据对LLaMA 2进行微调，开发了TAT-LLM语言模型。

    In this work, we address question answering (QA) over a hybrid of tabular and textual data that are very common content on the Web (e.g. SEC filings), where discrete reasoning capabilities are often required. Recently, large language models (LLMs) like GPT-4 have demonstrated strong multi-step reasoning capabilities. We then consider harnessing the amazing power of LLMs to solve our task. We abstract a Step-wise Pipeline for tabular and textual QA, which consists of three key steps, including Extractor, Reasoner and Executor, and initially design an instruction to instantiate the pipeline and validate that GPT-4 outperforms all existing methods. However, utilizing an online LLM like GPT-4 holds various challenges in terms of cost, latency, and data security risk, which motivates us to specialize smaller LLMs in this task. We develop a TAT-LLM language model by fine-tuning LLaMA 2 with the training data generated automatically from existing expert-annotated datasets following the Step-w
    
[^35]: TEPI: 对稀缺标记的零样本基因组分类进行分类感知嵌入和伪成像

    TEPI: Taxonomy-aware Embedding and Pseudo-Imaging for Scarcely-labeled Zero-shot Genome Classification. (arXiv:2401.13219v1 [q-bio.GN])

    [http://arxiv.org/abs/2401.13219](http://arxiv.org/abs/2401.13219)

    本研究提出了一种针对稀缺标记的零样本基因组分类问题的解决方法，称为TEPI。通过将基因组表示为伪图像并将其映射到分类感知的嵌入空间，我们能够捕捉物种的组成和系统分类关系，实现了零样本学习。

    

    物种的基因组编码了有价值的进化、生物和系统分类信息，有助于物种识别、分类和理解基因易感性，如药物抗性和毒力。然而，巨大的物种数量给开发通用的全基因组分类工具带来了重大挑战。传统的生物信息学工具取得了显著进展，但缺乏可扩展性并且计算成本高。基于机器学习的框架显示出潜力，但必须解决具有长尾分布的大分类词汇的问题。在本研究中，我们通过使用TEPI，即分类感知嵌入和伪成像，来解决这个问题。我们将每个基因组表示为伪图像，并将其映射到分类感知的嵌入空间进行推理和分类。这个嵌入空间捕捉了物种的组成和系统分类关系，实现了零样本学习。

    A species' genetic code or genome encodes valuable evolutionary, biological, and phylogenetic information that aids in species recognition, taxonomic classification, and understanding genetic predispositions like drug resistance and virulence. However, the vast number of potential species poses significant challenges in developing a general-purpose whole genome classification tool. Traditional bioinformatics tools have made notable progress but lack scalability and are computationally expensive. Machine learning-based frameworks show promise but must address the issue of large classification vocabularies with long-tail distributions. In this study, we propose addressing this problem through zero-shot learning using TEPI, Taxonomy-aware Embedding and Pseudo-Imaging. We represent each genome as pseudo-images and map them to a taxonomy-aware embedding space for reasoning and classification. This embedding space captures compositional and phylogenetic relationships of species, enabling pre
    
[^36]: AMANet：利用自适应多层次注意力网络提升SAR船舶检测

    AMANet: Advancing SAR Ship Detection with Adaptive Multi-Hierarchical Attention Network. (arXiv:2401.13214v1 [cs.CV])

    [http://arxiv.org/abs/2401.13214](http://arxiv.org/abs/2401.13214)

    本文提出了一种自适应多层次注意力网络(AMANet)，用于合成孔径雷达(SAR)图像中的船舶检测。该方法通过学习多尺度特征和自适应聚合显著特征，解决了海岸环境中小型和沿海船舶检测的挑战。

    

    最近，基于深度学习的方法已成功应用于合成孔径雷达(SAR)图像的船舶检测。尽管已经开发了许多船舶检测方法，但由于海岸环境中特征有限和杂乱干扰的原因，检测小型和沿海船舶仍然是一个重要挑战。为此，提出了一种新颖的自适应多层次注意力模块(AMAM)，用于学习多尺度特征，并在复杂环境中自适应地聚合显著特征。具体而言，我们首先融合相邻特征层的信息，增强对较小目标的检测，从而实现多尺度特征增强。然后，为了滤除复杂背景的负面效果，我们在通道上解剖之前融合的多级特征，单独挖掘显著区域，并自适应地汇集来自不同通道的特征。第三，我们提出了一种新颖的自适应多层次注意力网络(AMANet)，用于综合建模和船舶检测。

    Recently, methods based on deep learning have been successfully applied to ship detection for synthetic aperture radar (SAR) images. Despite the development of numerous ship detection methodologies, detecting small and coastal ships remains a significant challenge due to the limited features and clutter in coastal environments. For that, a novel adaptive multi-hierarchical attention module (AMAM) is proposed to learn multi-scale features and adaptively aggregate salient features from various feature layers, even in complex environments. Specifically, we first fuse information from adjacent feature layers to enhance the detection of smaller targets, thereby achieving multi-scale feature enhancement. Then, to filter out the adverse effects of complex backgrounds, we dissect the previously fused multi-level features on the channel, individually excavate the salient regions, and adaptively amalgamate features originating from different channels. Thirdly, we present a novel adaptive multi-h
    
[^37]: AdCorDA: 通过对抗修正和领域适应进行分类器改进

    AdCorDA: Classifier Refinement via Adversarial Correction and Domain Adaptation. (arXiv:2401.13212v1 [cs.CV])

    [http://arxiv.org/abs/2401.13212](http://arxiv.org/abs/2401.13212)

    AdCorDA方法通过对抗修正和领域适应改进预训练分类器网络，实验证明在CIFAR-100数据集上能够显著提升准确率，并且在权重量化的神经网络上也表现出显著的性能提升。

    

    本文描述了一种简单而有效的技术，用于改进预训练分类器网络。所提出的AdCorDA方法基于修改训练集，并利用网络权重和层输入之间的对偶性。我们称之为输入空间训练。该方法由两个阶段组成 - 对抗修正，然后进行领域适应。对抗修正使用对抗性攻击来修正错误的训练集分类。将训练集中错误分类的样本移除，并用对抗修正的样本替换，形成新的训练集，然后在第二阶段，进行领域适应回到原始训练集。大量的实验证明，在CIFAR-100数据集上，准确率提升超过5%。该技术可以直接应用于权重量化的神经网络的改进，实验证明在基线上性能得到了显著提升。

    This paper describes a simple yet effective technique for refining a pretrained classifier network. The proposed AdCorDA method is based on modification of the training set and making use of the duality between network weights and layer inputs. We call this input space training. The method consists of two stages - adversarial correction followed by domain adaptation. Adversarial correction uses adversarial attacks to correct incorrect training-set classifications. The incorrectly classified samples of the training set are removed and replaced with the adversarially corrected samples to form a new training set, and then, in the second stage, domain adaptation is performed back to the original training set. Extensive experimental validations show significant accuracy boosts of over 5% on the CIFAR-100 dataset. The technique can be straightforwardly applied to refinement of weight-quantized neural networks, where experiments show substantial enhancement in performance over the baseline. T
    
[^38]: 通过本地混合和自适应步长提高对抗性样本的可迁移性

    Boosting the Transferability of Adversarial Examples via Local Mixup and Adaptive Step Size. (arXiv:2401.13205v1 [cs.CV])

    [http://arxiv.org/abs/2401.13205](http://arxiv.org/abs/2401.13205)

    本文提出了一个黑盒对抗性生成框架，通过本地混合和自适应步长的设计，增强了输入多样性，并且能够生成可迁移的对抗性样本。

    

    对抗性样本是各种视觉应用中一个关键的安全威胁，注入人眼无法察觉的扰动可以混淆输出。在黑盒环境下生成可迁移的对抗性样本在实践中至关重要但具有挑战性。现有的基于输入多样性的方法采用不同的图像变换，但由于输入多样性不足和相同的扰动步长可能效率低下。受到不同图像区域在分类中具有不同权重的启发，本文通过同时设计增强的输入多样性和自适应的步长，提出了一个黑盒对抗生成框架。我们设计了本地混合来随机混合一组变换后的对抗性图像，增强了输入多样性。为了精确生成对抗性样本，我们将扰动投影到$tanh$空间以放松边界约束。此外，不同区域的步长可以通过整合适应性调整。

    Adversarial examples are one critical security threat to various visual applications, where injected human-imperceptible perturbations can confuse the output.Generating transferable adversarial examples in the black-box setting is crucial but challenging in practice. Existing input-diversity-based methods adopt different image transformations, but may be inefficient due to insufficient input diversity and an identical perturbation step size. Motivated by the fact that different image regions have distinctive weights in classification, this paper proposes a black-box adversarial generative framework by jointly designing enhanced input diversity and adaptive step sizes. We design local mixup to randomly mix a group of transformed adversarial images, strengthening the input diversity. For precise adversarial generation, we project the perturbation into the $tanh$ space to relax the boundary constraint. Moreover, the step sizes of different regions can be dynamically adjusted by integratin
    
[^39]: MLLMReID: 基于多模态大语言模型的人物再识别

    MLLMReID: Multimodal Large Language Model-based Person Re-identification. (arXiv:2401.13201v1 [cs.CV])

    [http://arxiv.org/abs/2401.13201](http://arxiv.org/abs/2401.13201)

    MLLMReID是一种基于多模态大语言模型的人物再识别方法，通过微调模型并将其视觉编码器作为主干进行优化，解决了MLLM在ReID任务中的设计指令和特征学习效果的问题。

    

    多模态大语言模型（MLLM）在许多任务中取得了令人满意的结果。然而，它们在人物再识别（ReID）任务中的表现尚未被研究。本文将研究如何将它们适应于ReID任务。一种直观的想法是使用ReID图像-文本数据集对MLLM进行微调，然后将它们的视觉编码器作为ReID的主干。然而，仍存在两个明显的问题：（1）为ReID设计指令时，MLLM可能过度拟合特定指令，而设计各种指令将导致更高的成本。（2）LLM的潜在图像特征向量没有参与损失计算。指令学习，对齐图像-文本特征，导致间接优化和学习目标不充分利用特征，限制了人物特征学习的效果。为了解决这些问题，本文提出了MLLMReID：基于多模态大语言模型的ReID。首先，我们提出了公共指令。

    Multimodal large language models (MLLM) have achieved satisfactory results in many tasks. However, their performance in the task of person re-identification (ReID) has not been explored to date. This paper will investigate how to adapt them for the task of ReID. An intuitive idea is to fine-tune MLLM with ReID image-text datasets, and then use their visual encoder as a backbone for ReID. However, there still exist two apparent issues: (1) Designing instructions for ReID, MLLMs may overfit specific instructions, and designing a variety of instructions will lead to higher costs. (2) Latent image feature vectors from LLMs are not involved in loss computation. Instructional learning, aligning image-text features, results in indirect optimization and a learning objective that inadequately utilizes features, limiting effectiveness in person feature learning. To address these problems, this paper proposes MLLMReID: Multimodal Large Language Model-based ReID. Firstly, we proposed Common Instru
    
[^40]: Catch-Up Mix: 计算机视觉中针对CNN滤波器的Catch-Up Class

    Catch-Up Mix: Catch-Up Class for Struggling Filters in CNN. (arXiv:2401.13193v1 [cs.CV])

    [http://arxiv.org/abs/2401.13193](http://arxiv.org/abs/2401.13193)

    该论文提出了Catch-Up Mix方法，用于解决深度学习模型过于依赖特定滤波器的问题，并通过观察到慢学习滤波器因为快学习滤波器而失去学习机会，借鉴图像增强方法，提出了该方法来缓解这个问题。

    

    深度学习在计算机视觉中取得了显著的进展，尤其在图像分类任务中。尽管深度学习模型在训练数据上具有很高的准确性，但也经常面临与复杂性和过拟合相关的挑战。一个值得关注的问题是，模型通常过于依赖有限的一部分滤波器进行预测。这种依赖性可能导致泛化能力受损，并增加对微小变化的脆弱性。虽然常见的正则化技术如权重衰减、dropout和数据增强可以解决这个问题，但它们可能并没有直接解决对特定滤波器的依赖。我们观察到，当慢学习的滤波器由于快学习的滤波器而失去了学习机会时，这个依赖性问题会变得严重。受图像增强研究的启发，该研究通过删除和替换图像的部分来解决对特定图像区域的过于依赖问题，我们的想法是缓解过于依赖特定滤波器的问题。

    Deep learning has made significant advances in computer vision, particularly in image classification tasks. Despite their high accuracy on training data, deep learning models often face challenges related to complexity and overfitting. One notable concern is that the model often relies heavily on a limited subset of filters for making predictions. This dependency can result in compromised generalization and an increased vulnerability to minor variations. While regularization techniques like weight decay, dropout, and data augmentation are commonly used to address this issue, they may not directly tackle the reliance on specific filters. Our observations reveal that the heavy reliance problem gets severe when slow-learning filters are deprived of learning opportunities due to fast-learning filters. Drawing inspiration from image augmentation research that combats over-reliance on specific image regions by removing and replacing parts of images, our idea is to mitigate the problem of ove
    
[^41]: 基于点云表示和扩散模型的晶体结构生成设计

    Generative Design of Crystal Structures by Point Cloud Representations and Diffusion Model. (arXiv:2401.13192v1 [cs.AI])

    [http://arxiv.org/abs/2401.13192](http://arxiv.org/abs/2401.13192)

    本研究提出了一种基于点云和扩散模型的晶体结构生成设计框架，并通过重建输入结构和生成全新材料的实验证明了其有效性和潜力。

    

    在材料设计中，高效地生成能量稳定的晶体结构一直是个挑战，主要是因为晶格中原子的巨大排列。为了促进稳定材料的发现，我们提出了一个用于生成可合成材料的框架，利用点云表示来编码复杂的结构信息。在这个框架的核心是引入扩散模型作为基础支柱。为了评估我们方法的有效性，我们使用它来重建训练数据集中的输入结构，并严格验证其高重建性能。此外，我们通过生成全新的材料，重点强调了基于点云的晶体扩散(PCCD)的巨大潜力，并展示了其可合成性。我们的研究在材料设计和合成的推进中，通过先进的生成设计方法，做出了显著贡献。

    Efficiently generating energetically stable crystal structures has long been a challenge in material design, primarily due to the immense arrangement of atoms in a crystal lattice. To facilitate the discovery of stable material, we present a framework for the generation of synthesizable materials, leveraging a point cloud representation to encode intricate structural information. At the heart of this framework lies the introduction of a diffusion model as its foundational pillar. To gauge the efficacy of our approach, we employ it to reconstruct input structures from our training datasets, rigorously validating its high reconstruction performance. Furthermore, we demonstrate the profound potential of Point Cloud-Based Crystal Diffusion (PCCD) by generating entirely new materials, emphasizing their synthesizability. Our research stands as a noteworthy contribution to the advancement of materials design and synthesis through the cutting-edge avenue of generative design instead of the con
    
[^42]: AgentBoard: 一种多轮LLM智能体的分析评估板

    AgentBoard: An Analytical Evaluation Board of Multi-turn LLM Agents. (arXiv:2401.13178v1 [cs.CL])

    [http://arxiv.org/abs/2401.13178](http://arxiv.org/abs/2401.13178)

    AgentBoard是一个综合的基准测试和评估框架，专为分析评估LLM智能体而设计，解决了在多轮交互和部分可观察环境中对智能体性能进行基准测试的挑战，并提供了细粒度的进展率指标和评估工具包。

    

    评估大型语言模型（LLM）作为通用智能体对于理解其能力并促进其融入实际应用至关重要。然而，评估过程面临重大挑战。主要障碍之一是在统一框架内对智能体在不同场景下的性能进行基准测试，特别是在维护部分可观察环境和确保多轮交互方面。此外，当前的评估框架主要关注最终成功率，过程中提供的见解很少，无法深入理解模型的能力。为了解决这些挑战，我们引入了AgentBoard，这是一个创新的综合基准和伴随的开源评估框架，专为LLM智能体的分析评估而设计。AgentBoard提供了一种细粒度的进展率指标，捕捉逐步的进展，以及一个综合的评估工具包，具有易于评估和分析模型能力的功能。

    Evaluating large language models (LLMs) as general-purpose agents is essential for understanding their capabilities and facilitating their integration into practical applications. However, the evaluation process presents substantial challenges. A primary obstacle is the benchmarking of agent performance across diverse scenarios within a unified framework, especially in maintaining partially-observable environments and ensuring multi-round interactions. Moreover, current evaluation frameworks mostly focus on the final success rate, revealing few insights during the process and failing to provide a deep understanding of the model abilities. To address these challenges, we introduce AgentBoard, a pioneering comprehensive benchmark and accompanied open-source evaluation framework tailored to analytical evaluation of LLM agents. AgentBoard offers a fine-grained progress rate metric that captures incremental advancements as well as a comprehensive evaluation toolkit that features easy assess
    
[^43]: 组合式生成逆设计

    Compositional Generative Inverse Design. (arXiv:2401.13171v1 [cs.LG])

    [http://arxiv.org/abs/2401.13171](http://arxiv.org/abs/2401.13171)

    逆向设计起到优化底层目标函数的作用，最近的研究利用了学习的动力学模型进行优化。通过优化扩散模型捕获的学习能量函数，可以避免对抗示例，并显著提高设计性能。这一设计系统是组合性的，使得可以设计具有每个指定组件的系统。

    

    逆设计是一种寻求设计输入变量以优化底层目标函数的重要问题，在机械工程到航天工程等领域都有应用。逆设计通常被构建成一个优化问题，最近的研究利用了学习的动力学模型进行优化。然而，由于模型的优化往往会陷入对抗模式，阻碍有效的抽样。我们证明，通过优化扩散模型捕获的学习能量函数，我们可以避免这种对抗性示例，并显著提高设计性能。我们进一步展示了这样一个设计系统是组合性的，使我们能够结合多个不同的扩散模型来代表所需系统的子组件，从而设计具有每个指定组件的系统。在一个N体相互作用任务和一个具有挑战性的二维多翼型设计任务中，我们证明通过组合学习的能量函数，可以实现更好的设计性能。

    Inverse design, where we seek to design input variables in order to optimize an underlying objective function, is an important problem that arises across fields such as mechanical engineering to aerospace engineering. Inverse design is typically formulated as an optimization problem, with recent works leveraging optimization across learned dynamics models. However, as models are optimized they tend to fall into adversarial modes, preventing effective sampling. We illustrate that by instead optimizing over the learned energy function captured by the diffusion model, we can avoid such adversarial examples and significantly improve design performance. We further illustrate how such a design system is compositional, enabling us to combine multiple different diffusion models representing subcomponents of our desired system to design systems with every specified component. In an N-body interaction task and a challenging 2D multi-airfoil design task, we demonstrate that by composing the learn
    
[^44]: 具有多维持久性的动态对象的时间感知知识表示

    Time-Aware Knowledge Representations of Dynamic Objects with Multidimensional Persistence. (arXiv:2401.13157v1 [cs.LG])

    [http://arxiv.org/abs/2401.13157](http://arxiv.org/abs/2401.13157)

    本论文提出了一种新的时间感知知识表示机制，聚焦于多维拓扑信息和隐含的时间相关信息。该方法通过使用单参数拓扑摘要生成数据的多维拓扑指纹。

    

    学习多变量时间序列和动态网络等时间变化的对象需要开发新的知识表示机制和神经网络架构，以捕捉数据中的隐含的时间相关信息。这些信息通常不直接观察到，但在学习任务性能中起着关键作用。然而，对于时间相关数据而言，知识编码机制中缺少时间维度会导致频繁的模型更新，学习性能差，从而影响决策的质量。在这里，我们提出了一种新的时间感知知识表示机制的方法，特别关注多个几何维度上的隐含时间相关拓扑信息。具体而言，我们提出了一种名为 \textit{Temporal MultiPersistence} (TMP) 的新方法，通过使用现有的单参数拓扑摘要，生成数据的多维拓扑指纹。

    Learning time-evolving objects such as multivariate time series and dynamic networks requires the development of novel knowledge representation mechanisms and neural network architectures, which allow for capturing implicit time-dependent information contained in the data. Such information is typically not directly observed but plays a key role in the learning task performance. In turn, lack of time dimension in knowledge encoding mechanisms for time-dependent data leads to frequent model updates, poor learning performance, and, as a result, subpar decision-making. Here we propose a new approach to a time-aware knowledge representation mechanism that notably focuses on implicit time-dependent topological information along multiple geometric dimensions. In particular, we propose a new approach, named \textit{Temporal MultiPersistence} (TMP), which produces multidimensional topological fingerprints of the data by using the existing single parameter topological summaries. The main idea be
    
[^45]: 对AI代理的可见性

    Visibility into AI Agents. (arXiv:2401.13138v1 [cs.CY])

    [http://arxiv.org/abs/2401.13138](http://arxiv.org/abs/2401.13138)

    本文评估了三类增加对AI代理可见性的措施：代理标识符、实时监控和活动记录，并提出了可能的实施方式。这些措施在集中化和去中心化部署环境中应用广泛，有助于理解和减轻AI代理带来的风险。

    

    将商业、科学、政府和个人活动委托给具有有限监督能力的AI代理系统，可能会加剧现有的社会风险并引入新的风险。理解和减轻这些风险涉及对现有治理结构进行批判性评估，根据需要进行修订和调整，并确保关键利益相关者的问责制。我们将AI代理的使用地点、原因、方式以及使用者等信息称为“可见性”，这对于实现上述目标至关重要。在本文中，我们评估了三类增加对AI代理可见性的措施：代理标识符、实时监控和活动记录。对于每一种措施，我们概述了可能的实施方式，这些方式在侵入性和信息性方面有所差异。我们分析了这些措施在集中化和去中心化部署环境中的应用情况，考虑了不同变量的影响。

    Increased delegation of commercial, scientific, governmental, and personal activities to AI agents -- systems capable of pursuing complex goals with limited supervision -- may exacerbate existing societal risks and introduce new risks. Understanding and mitigating these risks involves critically evaluating existing governance structures, revising and adapting these structures where needed, and ensuring accountability of key stakeholders. Information about where, why, how, and by whom certain AI agents are used, which we refer to as \textbf{visibility}, is critical to these objectives. In this paper, we assess three categories of measures to increase visibility into AI agents: \textbf{agent identifiers}, \textbf{real-time monitoring}, and \textbf{activity logging}. For each, we outline potential implementations that vary in intrusiveness and informativeness. We analyze how the measures apply across a spectrum of centralized through decentralized deployment contexts, accounting for vario
    
[^46]: 语言障碍：剖析LLMs在多语言环境中的安全挑战

    The Language Barrier: Dissecting Safety Challenges of LLMs in Multilingual Contexts. (arXiv:2401.13136v1 [cs.CL])

    [http://arxiv.org/abs/2401.13136](http://arxiv.org/abs/2401.13136)

    本文研究了在多语言环境中，LLMs面临的安全挑战以及缓解这些挑战的方法。在不同语言中，LLMs对恶意提示的响应存在差异，低资源语言下的响应更容易产生不安全、不相关的结果。对于这种差异的原因，本文还研究了使用强化学习和有监督微调等方法对数据进行调节的影响。

    

    随着大型语言模型（LLMs）对全球社区的影响不断扩大，它们在多语言环境中的安全挑战对齐研究变得至关重要。本文研究了LLMs在不同语言中面临的安全挑战的变化，并讨论了缓解这些问题的方法。通过比较当前最先进的LLMs在高资源语言和低资源语言中对同一组恶意提示的响应情况，我们观察到（1）当恶意提示用低资源语言编写时，LLMs往往更容易生成不安全的响应，（2）LLMs在低资源语言下更容易生成与恶意提示无关的响应。为了理解造成这种差异的原因，我们研究了使用与人类反馈的强化学习（RLHF）或有监督微调（SFT）进行指导调节对HH-RLHF数据集的影响。令人惊讶的是，虽然使用高资源语言进行训练可以改善模型的对齐性，但通过训练仍会产生不一致的结果。

    As the influence of large language models (LLMs) spans across global communities, their safety challenges in multilingual settings become paramount for alignment research. This paper examines the variations in safety challenges faced by LLMs across different languages and discusses approaches to alleviating such concerns. By comparing how state-of-the-art LLMs respond to the same set of malicious prompts written in higher- vs. lower-resource languages, we observe that (1) LLMs tend to generate unsafe responses much more often when a malicious prompt is written in a lower-resource language, and (2) LLMs tend to generate more irrelevant responses to malicious prompts in lower-resource languages. To understand where the discrepancy can be attributed, we study the effect of instruction tuning with reinforcement learning from human feedback (RLHF) or supervised finetuning (SFT) on the HH-RLHF dataset. Surprisingly, while training with high-resource languages improves model alignment, traini
    
[^47]: DISCOUNT: 使用最优传输进行分布式对抗解释

    DISCOUNT: Distributional Counterfactual Explanation With Optimal Transport. (arXiv:2401.13112v1 [cs.AI])

    [http://arxiv.org/abs/2401.13112](http://arxiv.org/abs/2401.13112)

    本文提出了使用最优传输进行分布式对抗解释的方法DISCOUNT，将对抗解释的概念扩展到整个输入输出分布，并通过统计置信度来支撑这一方法。

    

    对抗解释是在黑盒决策模型中提供洞察力和可解释性的事实方法，通过确定导致不同结果的替代输入实例来实现。本文将对抗解释的概念扩展到分布上下文，从个体数据点扩大到整个输入输出分布，命名为分布式对抗解释。在分布式对抗解释中，我们的重点转向分析事实和对抗的分布属性，类似于评估个体实例及其结果决策的经典方法。我们利用最优传输来构建一个机会约束优化问题，旨在导出与事实对应的对抗分布，以统计置信度做支撑。我们提出的优化方法DISCOUNT在输入和输出分布之间平衡这种置信度。

    Counterfactual Explanations (CE) is the de facto method for providing insight and interpretability in black-box decision-making models by identifying alternative input instances that lead to different outcomes. This paper extends the concept of CEs to a distributional context, broadening the scope from individual data points to entire input and output distributions, named Distributional Counterfactual Explanation (DCE). In DCE, our focus shifts to analyzing the distributional properties of the factual and counterfactual, drawing parallels to the classical approach of assessing individual instances and their resulting decisions. We leverage Optimal Transport (OT) to frame a chance-constrained optimization problem, aiming to derive a counterfactual distribution that closely aligns with its factual counterpart, substantiated by statistical confidence. Our proposed optimization method, DISCOUNT, strategically balances this confidence across both input and output distributions. This algorit
    
[^48]: XAI普及化：大型语言模型是否能简化可解释的人工智能？

    XAI for All: Can Large Language Models Simplify Explainable AI?. (arXiv:2401.13110v1 [cs.AI])

    [http://arxiv.org/abs/2401.13110](http://arxiv.org/abs/2401.13110)

    本文介绍了一种名为“x-[plAIn]”的自定义大型语言模型，通过该模型能够使得可解释的人工智能（XAI）更易于访问，并为不同用户群体生成清晰、简明的解释。该模型具有适应性，能够根据用户的知识水平和兴趣来生成解释。实验结果表明，这种适应性提高了XAI的可访问性，弥补了差距。

    

    可解释的人工智能（XAI）领域通常关注具有较强技术背景的用户，这使得非专家难以理解XAI方法。本文介绍了“x-[plAIn]”，一种通过使用ChatGPT Builder开发的自定义大型语言模型（LLM），使XAI对更广泛的受众更易于访问的新方法。我们的目标是设计一个模型，可以针对不同的用户群体，包括商业专业人士和学术界人士，生成清晰、简明的各种XAI方法的总结。我们模型的关键特点是其能够根据每个用户群体的知识水平和兴趣来适应解释。我们的方法仍然能够提供及时的见解，为最终用户的决策过程提供便利。我们的使用案例研究结果表明，我们的模型在提供易于理解的、针对特定受众的解释方面非常有效，不论使用何种XAI方法，这种适应性提高了XAI的可访问性，弥补了差距。

    The field of Explainable Artificial Intelligence (XAI) often focuses on users with a strong technical background, making it challenging for non-experts to understand XAI methods. This paper presents "x-[plAIn]", a new approach to make XAI more accessible to a wider audience through a custom Large Language Model (LLM), developed using ChatGPT Builder. Our goal was to design a model that can generate clear, concise summaries of various XAI methods, tailored for different audiences, including business professionals and academics. The key feature of our model is its ability to adapt explanations to match each audience group's knowledge level and interests. Our approach still offers timely insights, facilitating the decision-making process by the end users. Results from our use-case studies show that our model is effective in providing easy-to-understand, audience-specific explanations, regardless of the XAI method used. This adaptability improves the accessibility of XAI, bridging the gap 
    
[^49]: 在库和系统不确定性存在的情况下，稀疏非线性动力学的识别

    Sparse identification of nonlinear dynamics in the presence of library and system uncertainty. (arXiv:2401.13099v1 [cs.LG])

    [http://arxiv.org/abs/2401.13099](http://arxiv.org/abs/2401.13099)

    针对系统变量和函数库不确定性，本文提出了增强的SINDy算法，可以更好地识别稀疏非线性动力学。

    

    SINDy算法已成功地用于从时间序列数据中识别动力系统的控制方程。然而，SINDy假设用户对系统中的变量和能够作为系统基础的函数库具有先验知识。在本文中，我们通过真实世界数据演示了增强SINDy算法在系统变量不确定性存在的情况下优于SINDy的性能。然后，我们展示了当两种不确定性同时存在时，SINDy可以进一步增强以实现稳健性能。

    The SINDy algorithm has been successfully used to identify the governing equations of dynamical systems from time series data. However, SINDy assumes the user has prior knowledge of the variables in the system and of a function library that can act as a basis for the system. In this paper, we demonstrate on real world data how the Augmented SINDy algorithm outperforms SINDy in the presence of system variable uncertainty. We then show SINDy can be further augmented to perform robustly when both kinds of uncertainty are present.
    
[^50]: 通过重力信息驱动的深度学习框架预测非本地物种船舶交通流量和入侵风险

    Gravity-Informed Deep Learning Framework for Predicting Ship Traffic Flow and Invasion Risk of Non-Indigenous Species via Ballast Water Discharge. (arXiv:2401.13098v1 [cs.LG])

    [http://arxiv.org/abs/2401.13098](http://arxiv.org/abs/2401.13098)

    通过考虑航运通量密度、港口距离、贸易流量和交通枢纽的中心性指标等因素，本研究开发了一个受物理启发的模型来预测海事航运流量，并用于指导全球交通网络中入侵物种的风险评估和管理。

    

    水体中的入侵物种对全球环境和生物多样性构成了重大威胁。由于交通和贸易增加，非本土物种已经引入了新的环境，导致生态系统破坏，并导致农业、林业和渔业方面的经济损失。因此，迫切需要风险评估和管理技术以减轻这些入侵的影响。本研究旨在开发一种新的受物理启发的模型，用于预测海事航运交通流量，并以此指导通过全球交通网络传播的入侵物种风险评估。受国际贸易重力模型的启发，我们的模型考虑了影响船舶活动可能性和影响的各种因素，如航运通量密度、港口之间的距离、贸易流量和交通枢纽的中心性指标。此外，通过分析入侵物种的风险网络，我们为评估和管理入侵提供了全面的框架。

    Invasive species in water bodies pose a major threat to the environment and biodiversity globally. Due to increased transportation and trade, non-native species have been introduced to new environments, causing damage to ecosystems and leading to economic losses in agriculture, forestry, and fisheries. Therefore, there is a pressing need for risk assessment and management techniques to mitigate the impact of these invasions. This study aims to develop a new physics-inspired model to forecast maritime shipping traffic and thus inform risk assessment of invasive species spread through global transportation networks. Inspired by the gravity model for international trades, our model considers various factors that influence the likelihood and impact of vessel activities, such as shipping flux density, distance between ports, trade flow, and centrality measures of transportation hubs. Additionally, by analyzing the risk network of invasive species, we provide a comprehensive framework for as
    
[^51]: 场景识别中的数字鸿沟：揭示深度学习系统中的社会经济偏见

    Digital Divides in Scene Recognition: Uncovering Socioeconomic Biases in Deep Learning Systems. (arXiv:2401.13097v1 [cs.CV])

    [http://arxiv.org/abs/2401.13097](http://arxiv.org/abs/2401.13097)

    该研究研究了深度学习系统中的社会经济偏见对场景识别的影响，发现了预训练的卷积神经网络在低社会经济地位的家庭照片中显示出更低的分类准确度和分类置信度，并更容易分配具有冒犯性的标签。

    

    基于计算机的场景理解影响了从城市规划到自动驾驶的领域，然而我们对这些技术在社会差异中的表现了解甚少。我们研究了深度卷积神经网络（dCNNs）在场景分类中的偏见，使用了来自全球和美国的近百万张图片，包括用户提交的家庭照片和Airbnb的房源照片。我们运用了统计模型，对家庭收入、人类发展指数（HDI）等社会经济指标以及公开数据来源（CIA和美国人口普查）的人口统计因素对dCNNs的表现影响进行了量化。我们的分析发现了显著的社会经济偏见，预训练的dCNNs表现出更低的分类准确度、更低的分类置信度，以及更高的倾向性在低社会经济地位的家庭（例如“废墟”，“贫民窟”）的图片中分配具有冒犯性的标签。这种趋势是持续的。

    Computer-based scene understanding has influenced fields ranging from urban planning to autonomous vehicle performance, yet little is known about how well these technologies work across social differences. We investigate the biases of deep convolutional neural networks (dCNNs) in scene classification, using nearly one million images from global and US sources, including user-submitted home photographs and Airbnb listings. We applied statistical models to quantify the impact of socioeconomic indicators such as family income, Human Development Index (HDI), and demographic factors from public data sources (CIA and US Census) on dCNN performance. Our analyses revealed significant socioeconomic bias, where pretrained dCNNs demonstrated lower classification accuracy, lower classification confidence, and a higher tendency to assign labels that could be offensive when applied to homes (e.g., "ruin", "slum"), especially in images from homes with lower socioeconomic status (SES). This trend is c
    
[^52]: 向可信赖的语言模型迈进：探究大规模语言模型的信息质量

    Towards Trustable Language Models: Investigating Information Quality of Large Language Models. (arXiv:2401.13086v1 [cs.CL])

    [http://arxiv.org/abs/2401.13086](http://arxiv.org/abs/2401.13086)

    这项研究探讨了大规模语言模型的信息质量问题，发现标记化不可靠、偏见以及信息质量下降可能导致幻觉、捏造信息，从而对企业决策产生错误影响。

    

    大规模语言模型（LLM）正在迅速生成大量信息，用户越来越依赖和信任这些数据。尽管LLM取得了显著进展，但由于信息质量的挑战，LLM生成的信息并不完全可信。具体而言，LLM在预训练过程中的标记化不可靠、存在偏见，导致信息质量的完整性下降。此外，由于信息质量下降的问题，LLM可能会产生幻觉、捏造信息。不可靠的信息可能导致企业做出错误决策，影响经济活动。在这项工作中，我们引入了LLM的新颖数学信息质量评估方法，进一步分析和突出了信息质量挑战，以系统地扩展语言模型。

    Large language models (LLM) are generating information at a rapid pace, requiring users to increasingly rely and trust the data. Despite remarkable advances of LLM, Information generated by LLM is not completely trustworthy, due to challenges in information quality. Specifically, integrity of Information quality decreases due to unreliable, biased, tokenization during pre-training of LLM. Moreover, due to decreased information quality issues, has led towards hallucination, fabricated information. Unreliable information can lead towards flawed decisions in businesses, which impacts economic activity. In this work, we introduce novel mathematical information quality evaluation of LLM, we furthermore analyze and highlight information quality challenges, scaling laws to systematically scale language models.
    
[^53]: IndiText Boost: 低资源条件下印度语言的文本增强

    IndiText Boost: Text Augmentation for Low Resource India Languages. (arXiv:2401.13085v1 [cs.CL])

    [http://arxiv.org/abs/2401.13085](http://arxiv.org/abs/2401.13085)

    本论文研究了针对低资源印度语言的文本增强方法，包括Easy Data Augmentation、Back Translation、Paraphrasing、使用LLMs进行文本生成以及使用LLMs进行文本扩展等技术。通过二元和多类文本分类实验，研究发现基本的数据增强技术可以显著提升性能。

    

    文本增强是低资源语言中重要的任务，它有助于解决数据稀缺问题。通过数据增强策略来应对数据稀缺问题。多年来，已经对英语进行了大量的数据增强工作，而在印度语言方面却做得很少。这与使用数据增强来处理数据稀缺的事实相反。在这项工作中，我们重点实施Easy Data Augmentation、Back Translation、Paraphrasing、使用LLMs进行文本生成以及使用LLMs进行文本扩展等技术，用于不同语言的文本分类。我们重点关注6种印度语言：信德语、马拉地语、印地语、古吉拉特语、泰卢固语和梵语。据我们所知，目前没有类似的工作用于印度语言的文本增强。我们进行了二元和多类文本分类，以使我们的结果更具可比性。我们得到了令人惊讶的结果，基本的数据增强技术可显著提升性能。

    Text Augmentation is an important task for low-resource languages. It helps deal with the problem of data scarcity. A data augmentation strategy is used to deal with the problem of data scarcity. Through the years, much work has been done on data augmentation for the English language. In contrast, very less work has been done on Indian languages. This is contrary to the fact that data augmentation is used to deal with data scarcity. In this work, we focus on implementing techniques like Easy Data Augmentation, Back Translation, Paraphrasing, Text Generation using LLMs, and Text Expansion using LLMs for text classification on different languages. We focus on 6 Indian languages namely: Sindhi, Marathi, Hindi, Gujarati, Telugu, and Sanskrit. According to our knowledge, no such work exists for text augmentation on Indian languages. We carry out binary as well as multi-class text classification to make our results more comparable. We get surprising results as basic data augmentation techniq
    
[^54]: 医学影像中的自由形式问答

    Free Form Medical Visual Question Answering in Radiology. (arXiv:2401.13081v1 [cs.CV])

    [http://arxiv.org/abs/2401.13081](http://arxiv.org/abs/2401.13081)

    这项研究在医学领域的视觉问答中取得了重要进展，创新性地增强了数据集，并实现了与当前最先进模型相当的性能。

    

    在医学领域，视觉问答（VQA）面临着一个独特的、跨学科的挑战，结合了计算机视觉、自然语言处理和知识表示等领域。尽管其重要性，医学VQA的研究一直很少，直到2018年才开始蓬勃发展。我们的研究填补了这一空白，深入探讨了放射学图像的有效表示以及多模态表示的联合学习，超越了现有的方法。我们创新地增强了SLAKE数据集，使我们的模型能够回答更多样化的问题，不仅限于放射学或病理学图像的内容。我们的模型在更简单的架构下实现了79.55％的top-1准确率，展示了与当前最先进模型相当的性能。这项研究不仅推动了医学VQA的发展，也为诊断设置中的实际应用开辟了道路。

    Visual Question Answering (VQA) in the medical domain presents a unique, interdisciplinary challenge, combining fields such as Computer Vision, Natural Language Processing, and Knowledge Representation. Despite its importance, research in medical VQA has been scant, only gaining momentum since 2018. Addressing this gap, our research delves into the effective representation of radiology images and the joint learning of multimodal representations, surpassing existing methods. We innovatively augment the SLAKE dataset, enabling our model to respond to a more diverse array of questions, not limited to the immediate content of radiology or pathology images. Our model achieves a top-1 accuracy of 79.55\% with a less complex architecture, demonstrating comparable performance to current state-of-the-art models. This research not only advances medical VQA but also opens avenues for practical applications in diagnostic settings.
    
[^55]: 基于图像分割的本地背景估计，提高高光谱图像中气溶胶识别的效果。

    Local Background Estimation for Improved Gas Plume Identification in Hyperspectral Images. (arXiv:2401.13068v1 [cs.CV])

    [http://arxiv.org/abs/2401.13068](http://arxiv.org/abs/2401.13068)

    使用图像分割和迭代背景估计算法改进了高光谱图像中气溶胶识别的效果。

    

    深度学习鉴别模型在城市场景的长波红外高光谱图像中识别气溶胶方面表现出了潜力，特别是在考虑了大量气体的情况下。由于许多气体具有相似的光谱特征，正确估计检测到的气溶胶的信号十分重要。通常情况下，会估计场景的全局均值光谱和协方差矩阵，以对气溶胶的信号进行白化处理，从气溶胶的光谱中去除背景的签名。然而，城市场景中可能存在许多不同的背景材料，这些材料在空间和光谱上具有异质性。如果全局背景估计不能代表给定的本地背景材料，这可能导致识别性能较差。我们使用图像分割和迭代背景估计算法，为气溶胶下方的各种背景材料创建本地估计值。我们的方法在气溶胶识别方面优于全局背景估计。

    Deep learning identification models have shown promise for identifying gas plumes in Longwave IR hyperspectral images of urban scenes, particularly when a large library of gases are being considered. Because many gases have similar spectral signatures, it is important to properly estimate the signal from a detected plume. Typically, a scene's global mean spectrum and covariance matrix are estimated to whiten the plume's signal, which removes the background's signature from the gas signature. However, urban scenes can have many different background materials that are spatially and spectrally heterogeneous. This can lead to poor identification performance when the global background estimate is not representative of a given local background material. We use image segmentation, along with an iterative background estimation algorithm, to create local estimates for the various background materials that reside underneath a gas plume. Our method outperforms global background estimation on a se
    
[^56]: TCE在《古兰经问答2023》共享任务中的应用: 低资源增强Transformer-基于集成方法的古兰经问答

    TCE at Qur'an QA 2023 Shared Task: Low Resource Enhanced Transformer-based Ensemble Approach for Qur'anic QA. (arXiv:2401.13060v1 [cs.CL])

    [http://arxiv.org/abs/2401.13060](http://arxiv.org/abs/2401.13060)

    本文介绍了我们在《古兰经问答2023》共享任务中的应用，通过利用迁移学习和投票集成来提高预测稳定性，使用不同的Transformer模型和阈值机制来解决低资源训练数据的挑战。我们的最佳系统在隐藏数据集上取得了显著提升。

    

    本文介绍了我们在《古兰经问答2023》共享任务A和B中的方法。为了解决训练数据资源稀缺的挑战，我们依靠迁移学习和投票集成来提高在多次运行中的预测稳定性。此外，我们采用不同的体系结构和学习机制，针对两个任务使用一系列的阿拉伯语预训练Transformer模型。为了识别无法回答的问题，我们提出使用一个阈值机制。我们的最佳系统在隐藏数据集上的性能大大超过基准性能，任务A的MAP得分为25.05%，任务B的局部平均准确率(pAP)为57.11%。

    In this paper, we present our approach to tackle Qur'an QA 2023 shared tasks A and B. To address the challenge of low-resourced training data, we rely on transfer learning together with a voting ensemble to improve prediction stability across multiple runs. Additionally, we employ different architectures and learning mechanisms for a range of Arabic pre-trained transformer-based models for both tasks. To identify unanswerable questions, we propose using a thresholding mechanism. Our top-performing systems greatly surpass the baseline performance on the hidden split, achieving a MAP score of 25.05% for task A and a partial Average Precision (pAP) of 57.11% for task B.
    
[^57]: CIS-UNet: 基于上下文感知的平移窗口自注意力的CTA主动脉多类分割

    CIS-UNet: Multi-Class Segmentation of the Aorta in Computed Tomography Angiography via Context-Aware Shifted Window Self-Attention. (arXiv:2401.13049v1 [eess.IV])

    [http://arxiv.org/abs/2401.13049](http://arxiv.org/abs/2401.13049)

    本研究提出了一种称为CIS-UNet的深度学习模型，用于主动脉和主动脉分支的多类分割。该模型结合了CNN和Swin transformers的优势，采用了上下文感知的平移窗口自注意力，能够准确地识别主动脉的各个分支。

    

    医学成像和内衬血管膜技术的进步促进了主动脉疾病的微创治疗。准确地对主动脉及其分支进行3D分割对于干预至关重要，因为不准确的分割可能导致错误的手术规划和内部支架构造。之前的方法将主动脉分割简化为二值图像分割问题，忽视了区分各个主动脉分支的必要性。在本文中，我们介绍了一种称为Context Infused Swin-UNet（CIS-UNet）的深度学习模型，用于主动脉和十三个主动脉分支的多类分割。CIS-UNet结合了卷积神经网络（CNNs）和Swin transformers的优势，采用了层次化的编码器-解码器结构，包括CNN编码器、对称解码器、跳跃连接，以及一种新颖的基于上下文感知的平移窗口自注意力（CSW-SA）作为瓶颈模块。值得注意的是，CSW-SA引入了一种独特的方法来利用图像中的补丁信息。

    Advancements in medical imaging and endovascular grafting have facilitated minimally invasive treatments for aortic diseases. Accurate 3D segmentation of the aorta and its branches is crucial for interventions, as inaccurate segmentation can lead to erroneous surgical planning and endograft construction. Previous methods simplified aortic segmentation as a binary image segmentation problem, overlooking the necessity of distinguishing between individual aortic branches. In this paper, we introduce Context Infused Swin-UNet (CIS-UNet), a deep learning model designed for multi-class segmentation of the aorta and thirteen aortic branches. Combining the strengths of Convolutional Neural Networks (CNNs) and Swin transformers, CIS-UNet adopts a hierarchical encoder-decoder structure comprising a CNN encoder, symmetric decoder, skip connections, and a novel Context-aware Shifted Window Self-Attention (CSW-SA) as the bottleneck block. Notably, CSW-SA introduces a unique utilization of the patch
    
[^58]: 在线学习世界模型的局部敏感稀疏编码

    Locality Sensitive Sparse Encoding for Learning World Models Online. (arXiv:2401.13034v1 [cs.LG])

    [http://arxiv.org/abs/2401.13034](http://arxiv.org/abs/2401.13034)

    本文提出了一种基于局部敏感稀疏编码的线性回归模型，通过非线性随机特征实现对复杂环境的拟合。这种模型能够高效地进行稀疏更新，实现了优化拟合先前经验的Follow-The-Leader（FTL）世界模型。

    

    为了解决神经网络在在线学习中遇到的数据非平稳性问题，本文提出了一种基于局部敏感稀疏编码的线性回归模型，该模型通过非线性随机特征实现了对复杂环境的拟合。通过引入局部敏感稀疏编码，我们能够进行高效的稀疏更新，在平衡模型容量和计算效率的同时实现优化拟合所有先前经验的Follow-The-Leader（FTL）世界模型。

    Acquiring an accurate world model online for model-based reinforcement learning (MBRL) is challenging due to data nonstationarity, which typically causes catastrophic forgetting for neural networks (NNs). From the online learning perspective, a Follow-The-Leader (FTL) world model is desirable, which optimally fits all previous experiences at each round. Unfortunately, NN-based models need re-training on all accumulated data at every interaction step to achieve FTL, which is computationally expensive for lifelong agents. In this paper, we revisit models that can achieve FTL with incremental updates. Specifically, our world model is a linear regression model supported by nonlinear random features. The linear part ensures efficient FTL update while the nonlinear random feature empowers the fitting of complex environments. To best trade off model capacity and computation efficiency, we introduce a locality sensitive sparse encoding, which allows us to conduct efficient sparse updates even 
    
[^59]: CIMGEN: 受限数据上对预训练生成模型进行微调的可控图像操作

    CIMGEN: Controlled Image Manipulation by Finetuning Pretrained Generative Models on Limited Data. (arXiv:2401.13006v1 [cs.AI])

    [http://arxiv.org/abs/2401.13006](http://arxiv.org/abs/2401.13006)

    本文提出的方法利用预训练生成模型对受限数据进行微调，实现了可控的图像操作。通过修改语义地图，可以方便地插入、删除或替换图像中的对象。该方法在图像伪造和图像编辑领域具有潜在的应用价值和有效性。

    

    内容创作和图像编辑可以受益于用户的灵活控制。条件图像生成的常见中间表示是语义地图，其中包含图像中存在的对象的信息。与原始RGB像素相比，修改语义地图要容易得多。可以采用语义地图并轻松修改地图以选择性地插入、删除或替换地图中的对象。本文提出的方法接受修改后的语义地图，并根据修改后的地图调整原始图像。该方法利用传统的预训练图像到图像转换生成对抗网络（GAN），如CycleGAN或Pix2Pix GAN，在与语义地图相关的参考图像的有限数据集上进行微调。我们讨论了我们的技术的定性和定量性能，以说明它在图像伪造和图像编辑领域的能力和可能的应用。我们还展示了提议的图像伪造方法的有效性。

    Content creation and image editing can benefit from flexible user controls. A common intermediate representation for conditional image generation is a semantic map, that has information of objects present in the image. When compared to raw RGB pixels, the modification of semantic map is much easier. One can take a semantic map and easily modify the map to selectively insert, remove, or replace objects in the map. The method proposed in this paper takes in the modified semantic map and alter the original image in accordance to the modified map. The method leverages traditional pre-trained image-to-image translation GANs, such as CycleGAN or Pix2Pix GAN, that are fine-tuned on a limited dataset of reference images associated with the semantic maps. We discuss the qualitative and quantitative performance of our technique to illustrate its capacity and possible applications in the fields of image forgery and image editing. We also demonstrate the effectiveness of the proposed image forgery
    
[^60]: 循环多边形中的定理发现

    Theorem Discovery Amongst Cyclic Polygons. (arXiv:2401.13002v1 [cs.CG])

    [http://arxiv.org/abs/2401.13002](http://arxiv.org/abs/2401.13002)

    该论文介绍了关于循环2n边形的一个几何定理类别，证明了当取n个不相交的边对时，这些边之间角度的线性组合是恒定的，并提供了一个公式来表示这个线性组合。论文还介绍了一个利用这个结果生成新的几何证明问题及其解法的程序。

    

    我们研究了关于循环2n边形的几何定理类别。我们证明，如果我们取n个不相交的边对，每个对之间有偶数个多边形边分隔，则这些边之间的角度的线性组合是恒定的。我们提出了一个线性组合的公式，它以这些角度的方式给出了一个定理陈述。我们描述了一个利用这个结果生成新的几何证明问题及其解法的程序。

    We examine a class of geometric theorems on cyclic 2n-gons. We prove that if we take n disjoint pairs of sides, each pair separated by an even number of polygon sides, then there is a linear combination of the angles between those sides which is constant. We present a formula for the linear combination, which provides a theorem statement in terms of those angles. We describe a program which uses this result to generate new geometry proof problems and their solutions.
    
[^61]: PatternPortrait：用你的涂鸦画出我。 (arXiv:2401.13001v1 [cs.GR])

    PatternPortrait: Draw Me Like One of Your Scribbles. (arXiv:2401.13001v1 [cs.GR])

    [http://arxiv.org/abs/2401.13001](http://arxiv.org/abs/2401.13001)

    本论文介绍了一种从图片生成抽象肖像绘画的方法，通过利用单一的手绘图案素描和图形神经网络架构，实现了生成多样化的笔触变化，创造出风格独特的充满喜悦的抽象绘画。

    

    本论文介绍了一种从图片生成抽象肖像绘画的方法。其独特的风格是通过利用单一的手绘图案素描作为参考来生成用于阴影的独特图案。该方法涉及从图像中提取面部和身体特征，并将其转化为向量线条。研究的一个关键方面是开发一种图形神经网络架构，旨在学习向量形式的素描笔触表示，从而实现生成多样化的笔触变化。这两种方法的组合创造了充满喜悦的抽象绘画，通过钢笔绘图仪实现。所介绍的过程在大约280名参与者中获得了积极的反馈。

    This paper introduces a process for generating abstract portrait drawings from pictures. Their unique style is created by utilizing single freehand pattern sketches as references to generate unique patterns for shading. The method involves extracting facial and body features from images and transforming them into vector lines. A key aspect of the research is the development of a graph neural network architecture designed to learn sketch stroke representations in vector form, enabling the generation of diverse stroke variations. The combination of these two approaches creates joyful abstract drawings that are realized via a pen plotter. The presented process garnered positive feedback from an audience of approximately 280 participants.
    
[^62]: 量子启发的机器学习用于分子对接

    Quantum-Inspired Machine Learning for Molecular Docking. (arXiv:2401.12999v1 [physics.chem-ph])

    [http://arxiv.org/abs/2401.12999](http://arxiv.org/abs/2401.12999)

    量子启发的机器学习方法在分子对接中取得了显著的改进，通过结合量子特性和深度学习在编码的分子空间中学习的梯度，提高了盲目对接的成功率。

    

    分子对接是构建基于结构的药物设计的重要工具，可以加快药物开发的效率。蛋白质和小分子之间的复杂和动态结合过程需要在广泛的空间范围内进行搜索和采样。传统的对接方法通过搜索可能的结合位点和构象来实现，计算复杂度高，在盲目对接中效果不佳。受到这一点的启发，我们通过将量子启发算法与通过深度学习在编码的分子空间中学习的梯度相结合，实现了在盲目对接中的改进。数值仿真结果表明，我们的方法在传统的对接算法和基于深度学习的算法上表现出了超过10%的提升。与目前最先进的基于深度学习的对接算法DiffDock相比，Top-1（RMSD<2）的成功率从33%提高到35%。

    Molecular docking is an important tool for structure-based drug design, accelerating the efficiency of drug development. Complex and dynamic binding processes between proteins and small molecules require searching and sampling over a wide spatial range. Traditional docking by searching for possible binding sites and conformations is computationally complex and results poorly under blind docking. Quantum-inspired algorithms combining quantum properties and annealing show great advantages in solving combinatorial optimization problems. Inspired by this, we achieve an improved in blind docking by using quantum-inspired combined with gradients learned by deep learning in the encoded molecular space. Numerical simulation shows that our method outperforms traditional docking algorithms and deep learning-based algorithms over 10\%. Compared to the current state-of-the-art deep learning-based docking algorithm DiffDock, the success rate of Top-1 (RMSD<2) achieves an improvement from 33\% to 35
    
[^63]: 评估和提升大型语言模型在特定领域医学中的表现：以DocOA为例的骨关节炎管理

    Evaluating and Enhancing Large Language Models Performance in Domain-specific Medicine: Osteoarthritis Management with DocOA. (arXiv:2401.12998v1 [cs.CL])

    [http://arxiv.org/abs/2401.12998](http://arxiv.org/abs/2401.12998)

    本研究评估了大型语言模型在特定领域医学中的表现，并以骨关节炎管理为例进行了增强。研究结果发现，通用语言模型相对于领域特定的骨关节炎管理模型在提供个性化治疗建议方面表现不佳，而专门模型表现有明显提升。

    

    大型语言模型（LLMs）在特定领域医学中的效能，尤其是在管理骨关节炎（OA）等复杂疾病方面，仍然大部分未被探索。本研究聚焦于评估和提升LLMs在特定领域的临床能力，以骨关节炎（OA）管理为案例研究。开发了一个特定领域的基准框架，评估LLMs在领域特定知识和真实临床场景中的临床应用之间的表现。开发了一种针对OA管理的专门LLM，名为DocOA，它结合了检索增强生成（RAG）和指令提示。通过客观和人工评估，比较了GPT-3.5、GPT-4和专门助手DocOA的性能。结果显示，通用LLMs如GPT-3.5和GPT-4在OA管理这种专门领域中的表现较差，尤其是在提供个性化治疗建议方面。然而，DocOA显示了显著的表现提升。

    The efficacy of large language models (LLMs) in domain-specific medicine, particularly for managing complex diseases such as osteoarthritis (OA), remains largely unexplored. This study focused on evaluating and enhancing the clinical capabilities of LLMs in specific domains, using osteoarthritis (OA) management as a case study. A domain specific benchmark framework was developed, which evaluate LLMs across a spectrum from domain-specific knowledge to clinical applications in real-world clinical scenarios. DocOA, a specialized LLM tailored for OA management that integrates retrieval-augmented generation (RAG) and instruction prompts, was developed. The study compared the performance of GPT-3.5, GPT-4, and a specialized assistant, DocOA, using objective and human evaluations. Results showed that general LLMs like GPT-3.5 and GPT-4 were less effective in the specialized domain of OA management, particularly in providing personalized treatment recommendations. However, DocOA showed signifi
    
[^64]: 通过临床记录的自然语言处理与使用诊断代码对比发现存在问题的疗效性鸦片使用的退伍军人：一项比较研究

    A Comparison of Veterans with Problematic Opioid Use Identified through Natural Language Processing of Clinical Notes versus Using Diagnostic Codes. (arXiv:2401.12996v1 [cs.CL])

    [http://arxiv.org/abs/2401.12996](http://arxiv.org/abs/2401.12996)

    通过翻译了的临床记录进行自然语言处理，发现了存在问题的鸦片使用的退伍军人。与仅通过诊断代码识别的鸦片使用障碍患者相比，这些患者具有不同的人口统计学和临床特征。

    

    背景：电子健康记录是鸦片类药物研究的数据来源。众所周知，鸦片类药物滥用难以通过诊断代码进行编码，但是存在问题的鸦片使用可以记录在临床记录中。目标：我们的目标是1）从各种临床记录中识别存在问题的鸦片使用；2）比较仅通过临床记录记录存在问题鸦片使用的患者与使用ICD鸦片使用障碍诊断代码的患者的特征。材料与方法：我们开发并使用自然语言处理（NLP）工具，对来自两个退伍军人事务所区域的患者队列（n=222,371）的临床记录进行分析以识别存在问题的鸦片使用的患者。我们还使用一组ICD诊断代码来识别来自相同队列的患有鸦片使用障碍的患者。我们比较了仅通过NLP识别出的患者与通过诊断代码识别出的患者的人口统计学和临床特征。

    Background: Electronic health records (EHRs) are a data source for opioid research. Opioid use disorder is known to be under-coded as a diagnosis, yet problematic opioid use can be documented in clinical notes.  Objectives: Our goals were 1) to identify problematic opioid use from a full range of clinical notes; and 2) to compare the characteristics of patients identified as having problematic opioid use, exclusively documented in clinical notes, to those having documented ICD opioid use disorder diagnostic codes.  Materials and Methods: We developed and applied a natural language processing (NLP) tool to the clinical notes of a patient cohort (n=222,371) from two Veteran Affairs service regions to identify patients with problematic opioid use. We also used a set of ICD diagnostic codes to identify patients with opioid use disorder from the same cohort. We compared the demographic and clinical characteristics of patients identified only through NLP, to those of patients identified thro
    
[^65]: 慢性病管理的少样本学习：利用大型语言模型和多Prompt工程与医疗知识注入

    Few-Shot Learning for Chronic Disease Management: Leveraging Large Language Models and Multi-Prompt Engineering with Medical Knowledge Injection. (arXiv:2401.12988v1 [cs.CL])

    [http://arxiv.org/abs/2401.12988](http://arxiv.org/abs/2401.12988)

    本研究提出了慢性病管理的少样本学习框架，利用大型语言模型和多Prompt工程进行精神障碍的检测，通过个性化的提示和医疗知识注入来解决数据挑战，实现慢性病管理的目标。

    

    本研究利用最先进的人工智能技术进行慢性病管理，特别是通过用户生成的文本内容来检测各种精神障碍。现有研究通常依赖于全监督机器学习，这带来了一些挑战，比如注释庞大的训练数据对于每种疾病的费时费力的手动过程，以及需要为每个问题设计专门的深度学习架构。为了解决这些挑战，我们提出了一个新颖的框架，利用了先进的人工智能技术，包括大型语言模型和多Prompt工程。具体而言，我们解决了数据驱动慢性病管理中的两个关键技术挑战：（1）开发个性化的提示来表示每个用户的独特性，以及（2）将医疗知识融入到提示中，为慢性病检测提供上下文，指导学习目标，并实现预测目标。我们使用四种精神障碍来评估我们的方法。

    This study harnesses state-of-the-art AI technology for chronic disease management, specifically in detecting various mental disorders through user-generated textual content. Existing studies typically rely on fully supervised machine learning, which presents challenges such as the labor-intensive manual process of annotating extensive training data for each disease and the need to design specialized deep learning architectures for each problem. To address such challenges, we propose a novel framework that leverages advanced AI techniques, including large language models and multi-prompt engineering. Specifically, we address two key technical challenges in data-driven chronic disease management: (1) developing personalized prompts to represent each user's uniqueness and (2) incorporating medical knowledge into prompts to provide context for chronic disease detection, instruct learning objectives, and operationalize prediction goals. We evaluate our method using four mental disorders, w
    
[^66]: 众包自适应调查

    Crowdsourced Adaptive Surveys. (arXiv:2401.12986v1 [cs.CL])

    [http://arxiv.org/abs/2401.12986](http://arxiv.org/abs/2401.12986)

    众包自适应调查方法（CSAS）结合自然语言处理和自适应算法，能够根据用户输入演变问题库，并在调查中适应新的问题，应用在拉丁裔信息环境和议题重要性领域，能够识别难以通过传统方法跟踪的主张或问题。

    

    公众舆论调查对于民主决策至关重要，但对于传统调查方法来说，快速变化的信息环境和在小众社区中衡量观点可能是具有挑战性的。本文介绍了一种众包自适应调查方法（CSAS），它将自然语言处理和自适应算法的进展结合起来，生成随着用户输入不断演变的问题库。CSAS方法将参与者提供的开放式文本转换为Likert式项目，并应用多臂赌博算法来确定应优先考虑在调查中的用户提供问题。该方法的自适应性允许探索新的调查问题，同时在调查长度上施加最小的成本。在拉丁裔信息环境和议题重要性领域的应用展示了CSAS识别可能难以通过标准方法跟踪的主张或问题的能力。最后，我提出 Conclusion by di的结束语。

    Public opinion surveys are vital for informing democratic decision-making, but responding to rapidly changing information environments and measuring beliefs within niche communities can be challenging for traditional survey methods. This paper introduces a crowdsourced adaptive survey methodology (CSAS) that unites advances in natural language processing and adaptive algorithms to generate question banks that evolve with user input. The CSAS method converts open-ended text provided by participants into Likert-style items and applies a multi-armed bandit algorithm to determine user-provided questions that should be prioritized in the survey. The method's adaptive nature allows for the exploration of new survey questions, while imposing minimal costs in survey length. Applications in the domains of Latino information environments and issue importance showcase CSAS's ability to identify claims or issues that might otherwise be difficult to track using standard approaches. I conclude by di
    
[^67]: 在机械工程教育中评估大型语言模型：关于以力学为重点的概念理解的研究

    Assessing Large Language Models in Mechanical Engineering Education: A Study on Mechanics-Focused Conceptual Understanding. (arXiv:2401.12983v1 [cs.CL])

    [http://arxiv.org/abs/2401.12983](http://arxiv.org/abs/2401.12983)

    本研究通过对三个大型语言模型（LLMs）在机械工程领域中解决概念性问题的能力进行评估，发现GPT-4在各个力学主题的问题回答方面表现优于其他两个模型和人类对照组，显示出潜在的未来改进空间。

    

    本研究是对大型语言模型（LLMs）在机械工程领域中解决概念性问题的能力进行的开创性研究，重点是力学。我们使用包含126个多项选择题的手工制作的考试来进行考察，涵盖了力学课程的各个方面，包括流体力学、机械振动、工程静力学和动力学、材料力学、弹性理论和连续介质力学。我们对三个LLM进行了评估，包括ChatGPT（GPT-3.5）、ChatGPT（GPT-4）和Claude（Claude-2.1），并将其与具有或没有机械工程背景的工程教职员和学生进行了比较。研究结果显示，在各种力学主题的问题回答方面，除了连续介质力学外，GPT-4的表现优于其他两个LLM和人类对照组。这表明GPT模型在处理符号计算和张量分析方面有潜在的未来改进空间。

    This study is a pioneering endeavor to investigate the capabilities of Large Language Models (LLMs) in addressing conceptual questions within the domain of mechanical engineering with a focus on mechanics. Our examination involves a manually crafted exam encompassing 126 multiple-choice questions, spanning various aspects of mechanics courses, including Fluid Mechanics, Mechanical Vibration, Engineering Statics and Dynamics, Mechanics of Materials, Theory of Elasticity, and Continuum Mechanics. Three LLMs, including ChatGPT (GPT-3.5), ChatGPT (GPT-4), and Claude (Claude-2.1), were subjected to evaluation against engineering faculties and students with or without mechanical engineering background. The findings reveal GPT-4's superior performance over the other two LLMs and human cohorts in answering questions across various mechanics topics, except for Continuum Mechanics. This signals the potential future improvements for GPT models in handling symbolic calculations and tensor analyses
    
[^68]: 使用物理信息神经网络对细胞外间隙中的分子传输进行定量分析

    Quantitative Analysis of Molecular Transport in the Extracellular Space Using Physics-Informed Neural Network. (arXiv:2401.12435v1 [cs.AI])

    [http://arxiv.org/abs/2401.12435](http://arxiv.org/abs/2401.12435)

    本文提出了一种使用物理信息神经网络对细胞外间隙中分子传输进行定量分析的新方法，解决了对分子传输形式不清楚的挑战，并实现了自动计算扩散系数和分子速度的优化功能。

    

    大脑的细胞外间隙 (ECS)是位于细胞之间或细胞与血管之间的不规则、极其迂回的纳米级空间，对神经细胞的生存至关重要。它在记忆、情绪和感觉等高级脑功能中起着关键作用。然而，ECS内分子传输的具体形式仍然不清楚。为了解决这个问题，本文提出了一种新的方法，通过使用物理信息神经网络 (PINN) 解决从对流-扩散方程 (ADE) 导出的一个逆问题，定量分析ECS内的分子传输。PINN为ADE提供了一个简化的解决方案，而无需复杂的数学公式或网格设置。此外，PINN的优化功能可自动计算决定长期分子传输的扩散系数和由对流驱动的分子速度。因此，所提出的方法允许进行定量分析。

    The brain extracellular space (ECS), an irregular, extremely tortuous nanoscale space located between cells or between cells and blood vessels, is crucial for nerve cell survival. It plays a pivotal role in high-level brain functions such as memory, emotion, and sensation. However, the specific form of molecular transport within the ECS remain elusive. To address this challenge, this paper proposes a novel approach to quantitatively analyze the molecular transport within the ECS by solving an inverse problem derived from the advection-diffusion equation (ADE) using a physics-informed neural network (PINN). PINN provides a streamlined solution to the ADE without the need for intricate mathematical formulations or grid settings. Additionally, the optimization of PINN facilitates the automatic computation of the diffusion coefficient governing long-term molecule transport and the velocity of molecules driven by advection. Consequently, the proposed method allows for the quantitative analy
    
[^69]: 空间-时间图卷积网络在交通预测上的知识蒸馏

    Knowledge Distillation on Spatial-Temporal Graph Convolutional Network for Traffic Prediction. (arXiv:2401.11798v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2401.11798](http://arxiv.org/abs/2401.11798)

    本论文研究了在交通预测中应用空间-时间图卷积网络和知识蒸馏的方法。知识蒸馏的思想能够实现在减少参数和保持准确性的同时提高执行效率。通过引入教师网络的空间-时间相关性，我们的方法能够使学生网络学习到复杂的交通模式。

    

    高效实时交通预测对减少交通时间至关重要。为了预测交通状况，我们采用了空间-时间图神经网络（ST-GNN）将实时交通数据建模为时间图。尽管ST-GNN具有强大的能力，但在为实际交通数据进行高效实时预测时经常面临挑战。鉴于实时数据动态性的重要性，我们采用知识蒸馏（KD）作为解决方案，以提高ST-GNN在交通预测中的执行时间。本文介绍了一个成本函数，旨在使用复杂网络（教师）的蒸馏数据来训练具有较少参数的网络（学生），同时保持其准确性接近教师的准确性。我们使用知识蒸馏，将教师网络的空间-时间相关性融入学生网络，使学生能够学习到教师感知的复杂模式。然而，面临一个挑战。

    Efficient real-time traffic prediction is crucial for reducing transportation time. To predict traffic conditions, we employ a spatio-temporal graph neural network (ST-GNN) to model our real-time traffic data as temporal graphs. Despite its capabilities, it often encounters challenges in delivering efficient real-time predictions for real-world traffic data. Recognizing the significance of timely prediction due to the dynamic nature of real-time data, we employ knowledge distillation (KD) as a solution to enhance the execution time of ST-GNNs for traffic prediction. In this paper, We introduce a cost function designed to train a network with fewer parameters (the student) using distilled data from a complex network (the teacher) while maintaining its accuracy close to that of the teacher. We use knowledge distillation, incorporating spatial-temporal correlations from the teacher network to enable the student to learn the complex patterns perceived by the teacher. However, a challenge a
    
[^70]: 安全且广义的端到端自主驾驶系统：基于强化学习和示范的研究

    Safe and Generalized end-to-end Autonomous Driving System with Reinforcement Learning and Demonstrations. (arXiv:2401.11792v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2401.11792](http://arxiv.org/abs/2401.11792)

    本文介绍了一种安全且广义的端到端自主驾驶系统 (SGADS)，使用强化学习和示范相结合的方法解决了现有方法的低安全性、泛化能力差和采样效率低的问题，同时引入了变分推理和归一化流以准确预测驾驶轨迹，并提出了鲁棒性安全约束的制定方法。

    

    一个智能驾驶系统应该能够根据当前环境和车辆状态动态制定适当的驾驶策略，同时确保系统的安全性和可靠性。然而，基于强化学习和模仿学习的现有方法存在安全性低、泛化能力差和采样效率低的问题。此外，它们无法准确预测未来的驾驶轨迹，而准确预测未来的驾驶轨迹是做出最优决策的前提。为了解决这些问题，本文引入了一种复杂而多样场景下的安全且广义的端到端自主驾驶系统 (SGADS)。我们的SGADS与变分推理和归一化流结合，使智能车辆能够准确预测未来的驾驶轨迹。此外，我们提出了鲁棒性安全约束的制定。此外，我们将强化学习与示范相结合进行增强学习。

    An intelligent driving system should be capable of dynamically formulating appropriate driving strategies based on the current environment and vehicle status, while ensuring the security and reliability of the system. However, existing methods based on reinforcement learning and imitation learning suffer from low safety, poor generalization, and inefficient sampling. Additionally, they cannot accurately predict future driving trajectories, and the accurate prediction of future driving trajectories is a precondition for making optimal decisions. To solve these problems, in this paper, we introduce a Safe and Generalized end-to-end Autonomous Driving System (SGADS) for complex and various scenarios. Our SGADS incorporates variational inference with normalizing flows, enabling the intelligent vehicle to accurately predict future driving trajectories. Moreover, we propose the formulation of robust safety constraints. Furthermore, we combine reinforcement learning with demonstrations to aug
    
[^71]: 通过具有分层正则化的医学代码中心的多模态对比EHR建模预测下次就诊诊断

    Next Visit Diagnosis Prediction via Medical Code-Centric Multimodal Contrastive EHR Modelling with Hierarchical Regularisation. (arXiv:2401.11648v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2401.11648](http://arxiv.org/abs/2401.11648)

    通过医学代码中心的多模态对比EHR建模预测下次就诊诊断，并通过分层正则化提高性能。

    

    在医疗保健中，利用电子健康记录（EHR）预测下次就诊的诊断是一项必要的任务，对于制定医疗保健提供者和患者的主动未来计划至关重要。然而，之前的许多研究并没有充分解决EHR数据固有的异构和分层特征，必然导致次优的性能。为此，我们提出了NECHO，一种新颖的医学代码中心的多模态对比EHR学习框架，其中包括分层正则化。首先，我们使用定制的网络设计和一对双模态对比损失融合涵盖医学代码、人口统计数据和临床笔记的多方面信息，所有这些都围绕着医学代码表现。我们还使用医学本体中的父级信息来规范特定模态的编码器，以学习EHR数据的层次结构。对MIMIC-III数据进行的一系列实验证明了我们方法的有效性。

    Predicting next visit diagnosis using Electronic Health Records (EHR) is an essential task in healthcare, critical for devising proactive future plans for both healthcare providers and patients. Nonetheless, many preceding studies have not sufficiently addressed the heterogeneous and hierarchical characteristics inherent in EHR data, inevitably leading to sub-optimal performance. To this end, we propose NECHO, a novel medical code-centric multimodal contrastive EHR learning framework with hierarchical regularisation. First, we integrate multifaceted information encompassing medical codes, demographics, and clinical notes using a tailored network design and a pair of bimodal contrastive losses, all of which pivot around a medical code representation. We also regularise modality-specific encoders using a parental level information in medical ontology to learn hierarchical structure of EHR data. A series of experiments on MIMIC-III data demonstrates effectiveness of our approach.
    
[^72]: 将临床实践指南纳入大型语言模型以增强临床决策支持

    Enhancing Large Language Models for Clinical Decision Support by Incorporating Clinical Practice Guidelines. (arXiv:2401.11120v1 [cs.CL])

    [http://arxiv.org/abs/2401.11120](http://arxiv.org/abs/2401.11120)

    该论文研究了将临床实践指南纳入大型语言模型以增强临床决策支持的方法。他们开发了三种方法，并对四个大型语言模型进行了评估，在COVID-19门诊治疗方面取得了较高的性能。

    

    大型语言模型（LLM），搭配临床实践指南（CPGs），可以显著提高临床决策支持（CDS）。然而，将CPGs纳入LLMs的方法并未得到充分研究。我们开发了三种不同的方法将CPGs纳入LLMs：二元决策树（BDT），程序辅助图构建（PAGC），以及思维链少样本提示（CoT-FSP）。为评估所提方法的有效性，我们创建了一组合成患者描述，并对由四个LLMs生成的响应进行自动和人工评估：GPT-4，GPT-3.5 Turbo，LLaMA和PaLM 2。零样本提示（ZSP）被用作基线方法。我们以COVID-19门诊治疗的临床决策支持为案例研究。四个LLMs在增加了CPGs后相对于基线ZSP展现了提高的性能。BDT在自动评估中表现优于CoT-FSP和PAGC。所有提出的方法都表现出了较高的性能。

    Background Large Language Models (LLMs), enhanced with Clinical Practice Guidelines (CPGs), can significantly improve Clinical Decision Support (CDS). However, methods for incorporating CPGs into LLMs are not well studied. Methods We develop three distinct methods for incorporating CPGs into LLMs: Binary Decision Tree (BDT), Program-Aided Graph Construction (PAGC), and Chain-of-Thought-Few-Shot Prompting (CoT-FSP). To evaluate the effectiveness of the proposed methods, we create a set of synthetic patient descriptions and conduct both automatic and human evaluation of the responses generated by four LLMs: GPT-4, GPT-3.5 Turbo, LLaMA, and PaLM 2. Zero-Shot Prompting (ZSP) was used as the baseline method. We focus on CDS for COVID-19 outpatient treatment as the case study. Results All four LLMs exhibit improved performance when enhanced with CPGs compared to the baseline ZSP. BDT outperformed both CoT-FSP and PAGC in automatic evaluation. All of the proposed methods demonstrated high per
    
[^73]: 使用LLMs发现极端社交媒体中的编码反犹太恶意言论的出现

    Using LLMs to discover emerging coded antisemitic hate-speech emergence in extremist social media. (arXiv:2401.10841v1 [cs.CL])

    [http://arxiv.org/abs/2401.10841](http://arxiv.org/abs/2401.10841)

    这项研究提出了一种方法，可以检测新出现的编码恶意术语，为极端社交媒体中的反犹太恶意言论提供了解决方案。

    

    网络仇恨言论的蔓延给社交媒体平台带来了一个难题。一个特殊的挑战与使用编码语言的群体有关，这些群体既想为其用户创造归属感，又想回避检测。编码语言发展迅速，并且随着时间的推移使用方式不同。本文提出了一种检测新出现的编码恶意术语的方法论。该方法在在线反犹太言论的环境中进行了测试。该方法考虑了从社交媒体平台上抓取的帖子，通常是极端主义用户使用的。帖子是使用与以前已知的针对犹太人的仇恨言论相关的种子表达式进行抓取的。该方法首先通过识别每个帖子最具代表性的表达式，并计算它们在整个语料库中的频率。过滤掉语法不一致的表达式和之前遇到过的表达式，以便关注新出现的良好形式的术语。然后进行了语义评估。

    Online hate speech proliferation has created a difficult problem for social media platforms. A particular challenge relates to the use of coded language by groups interested in both creating a sense of belonging for its users and evading detection. Coded language evolves quickly and its use varies over time. This paper proposes a methodology for detecting emerging coded hate-laden terminology. The methodology is tested in the context of online antisemitic discourse. The approach considers posts scraped from social media platforms, often used by extremist users. The posts are scraped using seed expressions related to previously known discourse of hatred towards Jews. The method begins by identifying the expressions most representative of each post and calculating their frequency in the whole corpus. It filters out grammatically incoherent expressions as well as previously encountered ones so as to focus on emergent well-formed terminology. This is followed by an assessment of semantic s
    
[^74]: DiConStruct: 基于黑盒精华的因果概念解释

    DiConStruct: Causal Concept-based Explanations through Black-Box Distillation. (arXiv:2401.08534v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2401.08534](http://arxiv.org/abs/2401.08534)

    DiConStruct是一种基于黑盒模型的因果概念解释方法，通过创建结构性因果模型和概念归因方式提供更具可解释性的局部解释。

    

    模型可解释性在人工智能决策系统中起着核心作用。理想情况下，解释应该使用人可解释的语义概念来表达。此外，解释器应该捕捉这些概念之间的因果关系，以便对解释进行推理。最后，解释方法应该高效，并不损害预测任务的性能。尽管近年来AI解释性取得了快速进展，但据我们所知，至今没有一种方法满足这三个条件。事实上，主流的局部概念可解释性方法不产生因果解释，并在解释性和预测性能之间存在权衡。我们提出了DiConStruct，一种既基于概念又具有因果性的解释方法，旨在通过结构性因果模型和概念归因方式创建更具可解释性的局部解释。我们的解释器作为一个精华模型适用于任何黑盒机器学习模型。

    Model interpretability plays a central role in human-AI decision-making systems. Ideally, explanations should be expressed using human-interpretable semantic concepts. Moreover, the causal relations between these concepts should be captured by the explainer to allow for reasoning about the explanations. Lastly, explanation methods should be efficient and not compromise the performance of the predictive task. Despite the rapid advances in AI explainability in recent years, as far as we know to date, no method fulfills these three properties. Indeed, mainstream methods for local concept explainability do not produce causal explanations and incur a trade-off between explainability and prediction performance. We present DiConStruct, an explanation method that is both concept-based and causal, with the goal of creating more interpretable local explanations in the form of structural causal models and concept attributions. Our explainer works as a distillation model to any black-box machine l
    
[^75]: 支持学生决策的学习推荐：基于知识图谱情境化的LLM聊天机器人，实现对话解释和指导

    Supporting Student Decisions on Learning Recommendations: An LLM-Based Chatbot with Knowledge Graph Contextualization for Conversational Explainability and Mentoring. (arXiv:2401.08517v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2401.08517](http://arxiv.org/abs/2401.08517)

    这个论文研究了一种基于知识图谱情境化的LLM聊天机器人，作为学生学习推荐的解释工具和指导。通过定义上下文并利用人工策划的信息源来调控LLM的生成，聊天机器人能在与学生对话中提供解释和指导。

    

    学生对学习推荐的决策与其理解推荐原因的能力是不可分割的；他们能否根据这种理解进行修改。在各种解释性方法中，聊天机器人具有与同行或导师讨论类似的潜力来与学生进行对话。然而，尽管生成式人工智能（GenAI）和大型语言模型（LLM）的进展，聊天机器人的能力仍然不足以取代人类导师。因此，我们提出了一种方法，利用聊天机器人作为对话的中介和解释的有限和受控生成的来源，以利用LLM的潜力的同时减少其潜在风险。所提出的基于LLM的聊天机器人支持学生理解学习路径推荐。我们使用知识图谱（KG）作为人工策划的信息源，通过定义其提示的上下文来调控LLM的输出。

    Student commitment towards a learning recommendation is not separable from their understanding of the reasons it was recommended to them; and their ability to modify it based on that understanding. Among explainability approaches, chatbots offer the potential to engage the student in a conversation, similar to a discussion with a peer or a mentor. The capabilities of chatbots, however, are still not sufficient to replace a human mentor, despite the advancements of generative AI (GenAI) and large language models (LLM). Therefore, we propose an approach to utilize chatbots as mediators of the conversation and sources of limited and controlled generation of explanations, to harvest the potential of LLMs while reducing their potential risks at the same time. The proposed LLM-based chatbot supports students in understanding learning-paths recommendations. We use a knowledge graph (KG) as a human-curated source of information, to regulate the LLM's output through defining its prompt's contex
    
[^76]: GPT-4 Vision在医学领域中专家级准确度背后的隐藏缺陷

    Hidden Flaws Behind Expert-Level Accuracy of GPT-4 Vision in Medicine. (arXiv:2401.08396v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2401.08396](http://arxiv.org/abs/2401.08396)

    GPT-4 Vision在医学领域中具有专家级准确度，但在图像理解方面存在缺陷。

    

    最近的研究表明，具有Vision功能的GPT-4在医学挑战任务中表现优于人类医生。然而，这些评估主要关注多项选择题的准确度。本研究通过对GPT-4V在解决新英格兰医学杂志图像挑战中的图像理解、医学知识回忆和逐步多模态推理的原理进行全面分析，扩展了当前的研究范围。评估结果证实，GPT-4V在多项选择准确度上优于人类医生（88.0% vs. 77.0%，p=0.034）。GPT-4V在医生回答错误的情况下，也能表现出超过80%的准确度。然而，我们发现，GPT-4V在最终做出正确选择的情况下，经常提供有缺陷的推理（27.3%），其中最突出的是图像理解（21.6%）。

    Recent studies indicate that Generative Pre-trained Transformer 4 with Vision (GPT-4V) outperforms human physicians in medical challenge tasks. However, these evaluations primarily focused on the accuracy of multi-choice questions alone. Our study extends the current scope by conducting a comprehensive analysis of GPT-4V's rationales of image comprehension, recall of medical knowledge, and step-by-step multimodal reasoning when solving New England Journal of Medicine (NEJM) Image Challenges - an imaging quiz designed to test the knowledge and diagnostic capabilities of medical professionals. Evaluation results confirmed that GPT-4V outperforms human physicians regarding multi-choice accuracy (88.0% vs. 77.0%, p=0.034). GPT-4V also performs well in cases where physicians incorrectly answer, with over 80% accuracy. However, we discovered that GPT-4V frequently presents flawed rationales in cases where it makes the correct final choices (27.3%), most prominent in image comprehension (21.6
    
[^77]: 借助属性推理实现个性化的形式逻辑启用的联邦学习

    Formal Logic Enabled Personalized Federated Learning Through Property Inference. (arXiv:2401.07448v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2401.07448](http://arxiv.org/abs/2401.07448)

    本论文提出了一种通过引入时态逻辑推理来实现个性化的形式逻辑启用的联邦学习，以解决异质性客户设备带来的挑战，并提出了聚合群集的概念。

    

    近期对联邦学习（FL）的不断研究进展极大地促进了分布式协作应用的发展，特别是在物联网人工智能（AIoT）领域。然而，当前研究领域中一个缺失的关键方面是使具有符号推理能力的数据驱动客户模型能够工作。具体而言，参与联邦学习的客户设备的异质性构成了一个重要挑战，因为每个客户都具有独特的逻辑推理特性。忽视这些设备特定的规范可能会导致客户端预测中遗漏关键属性，从而导致性能不佳。在本研究中，我们提出了一种新的训练范式，利用时态逻辑推理来解决这个问题。我们的方法涉及通过为每个FL客户端加入机械生成的逻辑表达式来增强训练过程。此外，我们引入了聚合群集的概念。

    Recent advancements in federated learning (FL) have greatly facilitated the development of decentralized collaborative applications, particularly in the domain of Artificial Intelligence of Things (AIoT). However, a critical aspect missing from the current research landscape is the ability to enable data-driven client models with symbolic reasoning capabilities. Specifically, the inherent heterogeneity of participating client devices poses a significant challenge, as each client exhibits unique logic reasoning properties. Failing to consider these device-specific specifications can result in critical properties being missed in the client predictions, leading to suboptimal performance. In this work, we propose a new training paradigm that leverages temporal logic reasoning to address this issue. Our approach involves enhancing the training process by incorporating mechanically generated logic expressions for each FL client. Additionally, we introduce the concept of aggregation clusters 
    
[^78]: 代码之间的界限：揭示机器和人类程序员之间不同的模式

    Between Lines of Code: Unraveling the Distinct Patterns of Machine and Human Programmers. (arXiv:2401.06461v1 [cs.SE])

    [http://arxiv.org/abs/2401.06461](http://arxiv.org/abs/2401.06461)

    本文通过分析代码的属性，揭示了机器和人类代码之间的独特模式，尤其是结构分割对于识别代码来源很关键。基于这些发现，我们提出了一种名为DetectCodeGPT的新方法来检测机器生成的代码。

    

    大型语言模型在代码生成方面取得了显著的进展，但它们模糊了机器和人类源代码之间的区别，导致软件产物的完整性和真实性问题。本文通过对代码长度、词汇多样性和自然性等属性的严格分析，揭示了机器和人类代码固有的独特模式。在我们的研究中特别注意到，代码的结构分割是识别其来源的关键因素。基于我们的发现，我们提出了一种名为DetectCodeGPT的新型机器生成代码检测方法，该方法改进了DetectGPT。

    Large language models have catalyzed an unprecedented wave in code generation. While achieving significant advances, they blur the distinctions between machine-and human-authored source code, causing integrity and authenticity issues of software artifacts. Previous methods such as DetectGPT have proven effective in discerning machine-generated texts, but they do not identify and harness the unique patterns of machine-generated code. Thus, its applicability falters when applied to code. In this paper, we carefully study the specific patterns that characterize machine and human-authored code. Through a rigorous analysis of code attributes such as length, lexical diversity, and naturalness, we expose unique pat-terns inherent to each source. We particularly notice that the structural segmentation of code is a critical factor in identifying its provenance. Based on our findings, we propose a novel machine-generated code detection method called DetectCodeGPT, which improves DetectGPT by cap
    
[^79]: 如何使Johnny说服LLMs越狱：通过人性化LLMs重新思考对AI安全的挑战

    How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs. (arXiv:2401.06373v1 [cs.CL])

    [http://arxiv.org/abs/2401.06373](http://arxiv.org/abs/2401.06373)

    本文通过将LLMs视为人类交流者，探索了每天语言互动和AI安全之间忽视的交叉点，并提出了一种通过说服LLMs进行越狱的方法。研究结果表明，说服显著提高了越狱性能，在多个风险类别上均取得了超过92%的攻击成功率。

    

    大多数传统的AI安全研究将AI模型视为机器，并集中在由安全专家开发的基于算法的攻击上。随着大型语言模型（LLMs）的普及和竞争力越来越强，非专家用户在日常互动中也可能产生风险。本文介绍了一种新的视角，将LLMs作为类似人类的交流者来越狱，以探索每天语言互动和AI安全之间被忽视的交叉点。具体而言，我们研究如何说服LLMs越狱。首先，我们提出了一个从几十年的社会科学研究中得出的说服分类法。然后，我们应用这个分类法来自动生成可解释的说服对抗提示（PAP）来越狱LLMs。结果显示，说服显著提高了越狱性能，在所有风险类别上PAP在Llama 2-7b Chat、GPT-3.5和GPT-4上的攻击成功率在10次试验中均超过92%，超过了最近的基于算法的攻击。

    Most traditional AI safety research has approached AI models as machines and centered on algorithm-focused attacks developed by security experts. As large language models (LLMs) become increasingly common and competent, non-expert users can also impose risks during daily interactions. This paper introduces a new perspective to jailbreak LLMs as human-like communicators, to explore this overlooked intersection between everyday language interaction and AI safety. Specifically, we study how to persuade LLMs to jailbreak them. First, we propose a persuasion taxonomy derived from decades of social science research. Then, we apply the taxonomy to automatically generate interpretable persuasive adversarial prompts (PAP) to jailbreak LLMs. Results show that persuasion significantly increases the jailbreak performance across all risk categories: PAP consistently achieves an attack success rate of over $92\%$ on Llama 2-7b Chat, GPT-3.5, and GPT-4 in $10$ trials, surpassing recent algorithm-focu
    
[^80]: 用于推荐中意图学习的端到端可学习聚类方法

    End-to-end Learnable Clustering for Intent Learning in Recommendation. (arXiv:2401.05975v1 [cs.IR])

    [http://arxiv.org/abs/2401.05975](http://arxiv.org/abs/2401.05975)

    本文提出了一种用于推荐中意图学习的端到端可学习聚类方法ELCRec，该方法解决了现有方法中的复杂优化问题和大规模数据集聚类的可扩展性问题。

    

    挖掘用户的意图在序列推荐中起着关键作用。最近的方法ICLRec使用对比学习和聚类来提取用户的潜在意图。尽管它已经显示出有效性，但现有的方法存在复杂和繁琐的交替优化问题，导致两个主要问题。首先，在广义期望最大化(EM)框架中分离表示学习和聚类优化经常导致次优性能。其次，在整个数据集上进行聚类会影响大规模行业数据的可扩展性。为了解决这些挑战，我们提出了一种新颖的意图学习方法，称为ELCRec，它将表示学习集成到一个端到端可学习聚类框架中进行推荐。

    Mining users' intents plays a crucial role in sequential recommendation. The recent approach, ICLRec, was introduced to extract underlying users' intents using contrastive learning and clustering. While it has shown effectiveness, the existing method suffers from complex and cumbersome alternating optimization, leading to two main issues. Firstly, the separation of representation learning and clustering optimization within a generalized expectation maximization (EM) framework often results in sub-optimal performance. Secondly, performing clustering on the entire dataset hampers scalability for large-scale industry data. To address these challenges, we propose a novel intent learning method called \underline{ELCRec}, which integrates representation learning into an \underline{E}nd-to-end \underline{L}earnable \underline{C}lustering framework for \underline{Rec}ommendation. Specifically, we encode users' behavior sequences and initialize the cluster centers as learnable network parameter
    
[^81]: 通过森林修剪提高随机森林的准确性和可解释性

    Improving the Accuracy and Interpretability of Random Forests via Forest Pruning. (arXiv:2401.05535v1 [stat.ML])

    [http://arxiv.org/abs/2401.05535](http://arxiv.org/abs/2401.05535)

    通过森林修剪方法，本研究提出了一种兼顾随机森林准确性和决策树可解释性的方法。实验证明，在大多数情景下，这种方法能够显著提高随机森林的性能。

    

    接近几十年的发展之后，随机森林仍然在各种学习问题中提供最先进的准确性，在这方面超越了决策树甚至神经网络等替代机器学习算法。然而，作为一种集成方法，随机森林在解释性方面往往比决策树表现不佳。在本研究中，我们提出了一种事后方法，旨在兼顾随机森林的准确性和决策树的可解释性。为此，我们提出了两种森林修剪方法，以在给定的随机森林内找到最佳子森林，然后在适用的情况下将选定的树合并为一棵。我们的第一种方法依赖于约束穷举搜索，而第二种方法基于LASSO方法的改进。在合成和真实世界数据集上进行的大量实验证明，在大多数情景下，这两种方法中至少有一种能够显著提高随机森林的准确性和可解释性。

    Decades after their inception, random forests continue to provide state-of-the-art accuracy in a variety of learning problems, outperforming in this respect alternative machine learning algorithms such as decision trees or even neural networks. However, being an ensemble method, the one aspect where random forests tend to severely underperform decision trees is interpretability. In the present work, we propose a post-hoc approach that aims to have the best of both worlds: the accuracy of random forests and the interpretability of decision trees. To this end, we present two forest-pruning methods to find an optimal sub-forest within a given random forest, and then, when applicable, combine the selected trees into one. Our first method relies on constrained exhaustive search, while our second method is based on an adaptation of the LASSO methodology. Extensive experiments over synthetic and real world datasets show that, in the majority of scenarios, at least one of the two methods propo
    
[^82]: 领域特定LLM的微调和利用方法

    Fine-tuning and Utilization Methods of Domain-specific LLMs. (arXiv:2401.02981v1 [cs.CL])

    [http://arxiv.org/abs/2401.02981](http://arxiv.org/abs/2401.02981)

    本研究调查了领域特定LLM的微调和利用方法，以金融领域为例，详细介绍了数据集选择、预处理、模型选择以及在金融领域LLM微调中的关键因素。研究探讨了领域特定词汇的构建和安全合规性的考虑因素，并提供了在金融领域生成领域特定LLM的过程和实施方法。多种金融案例被涵盖在内，包括股票价格预测、金融新闻情绪分析、自动文档处理、研究和信息提取等。

    

    最近发布的预训练的大型语言模型（LLM）引起了相当大的关注，但关于领域特定LLM的微调和应用的研究仍然很少。本研究调查了领域特定LLM的微调和利用方法，重点关注LLM的趋势、基础模型和用于领域特定预训练的方法。以金融行业为例，详细介绍了数据集选择、预处理、模型选择以及在金融领域LLM微调中关键的考虑因素。针对金融数据的独特特点，本研究探讨了领域特定词汇的构建，以及安全性和合规性的考虑因素。在LLM微调的实际应用中，本研究概述了在金融领域生成领域特定LLM的过程和实施方法。包括股票价格预测、金融新闻情绪分析、自动文档处理、研究和信息提取等多种金融案例被涵盖在内。

    Recent releases of pre-trained Large Language Models (LLMs) have gained considerable traction, yet research on fine-tuning and employing domain-specific LLMs remains scarce. This study investigates approaches for fine-tuning and leveraging domain-specific LLMs, highlighting trends in LLMs, foundational models, and methods for domain-specific pre-training. Focusing on the financial sector, it details dataset selection, preprocessing, model choice, and considerations crucial for LLM fine-tuning in finance. Addressing the unique characteristics of financial data, the study explores the construction of domain-specific vocabularies and considerations for security and regulatory compliance. In the practical application of LLM fine-tuning, the study outlines the procedure and implementation for generating domain-specific LLMs in finance. Various financial cases, including stock price prediction, sentiment analysis of financial news, automated document processing, research, information extract
    
[^83]: SAR-RARP50:机器人辅助根治性前列腺切除术中手术器械的分割和动作识别挑战

    SAR-RARP50: Segmentation of surgical instrumentation and Action Recognition on Robot-Assisted Radical Prostatectomy Challenge. (arXiv:2401.00496v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2401.00496](http://arxiv.org/abs/2401.00496)

    该论文介绍了SAR-RARP50挑战，该挑战提供了第一个多模态、公开的、体内的手术动作识别和语义仪器分割数据集，旨在让研究人员开发出稳健且准确的单任务动作识别方法。

    

    手术工具分割和动作识别是许多计算机辅助干预应用的基本构建模块，从手术技能评估到决策支持系统。现在，基于学习的动作识别和分割方法在性能上优于传统方法，但依赖于大规模的注释数据集。此外，动作识别和工具分割算法通常是在彼此孤立地训练和预测，没有利用潜在的交叉任务关系。通过EndoVis 2022 SAR-RARP50挑战，我们发布了第一个多模态的、公开的、体内的手术动作识别和语义仪器分割数据集，其中包含50个机器人辅助根治性前列腺切除术（RARP）的缝合视频片段。挑战的目标是两方面的。首先，让研究人员利用提供的数据集规模，开发出稳健且高准确性的单任务动作识别方法。

    Surgical tool segmentation and action recognition are fundamental building blocks in many computer-assisted intervention applications, ranging from surgical skills assessment to decision support systems. Nowadays, learning-based action recognition and segmentation approaches outperform classical methods, relying, however, on large, annotated datasets. Furthermore, action recognition and tool segmentation algorithms are often trained and make predictions in isolation from each other, without exploiting potential cross-task relationships. With the EndoVis 2022 SAR-RARP50 challenge, we release the first multimodal, publicly available, in-vivo, dataset for surgical action recognition and semantic instrumentation segmentation, containing 50 suturing video segments of Robotic Assisted Radical Prostatectomy (RARP). The aim of the challenge is twofold. First, to enable researchers to leverage the scale of the provided dataset and develop robust and highly accurate single-task action recognitio
    
[^84]: 在度量空间中的比例表示和低失真委员会选择

    Proportional Representation in Metric Spaces and Low-Distortion Committee Selection. (arXiv:2312.10369v2 [cs.GT] UPDATED)

    [http://arxiv.org/abs/2312.10369](http://arxiv.org/abs/2312.10369)

    在这篇论文中，我们提出了一种新的定义，用于度量空间中小集合对大集合的“代表性”。给定代表的集合和可能的代表集合，我们的标准要求对于大集合的任意子集，该子集与最好的代表点的平均距离与在所有代表点中的最好点的平均距离之间的差距不超过一个因子gamma。与已有的比例公平和核心公平的概念不同的是，我们的定义要求以比例表示大的凝聚簇。在资源增强框架中研究了该定义，同时也考虑了选举应用的情况。

    

    我们引入了一个新的定义，即在度量空间中，对于一个包含k个点的小集合R来说，“代表”更大集合的定义。给定要表示的集合V（例如，文档或选民），以及可能的代表集合C，我们的标准要求对于V的任意子集S，该子集包含V的theta分数，在R中的theta*k个最佳点的平均距离不应超过与它们与C中所有点的最佳theta*k点的平均距离相比超过一个因子gamma。该定义强化了比例公平和核心公平的概念，但与这些概念不同的是，它要求大的凝聚簇以比例表示它们的大小。由于存在实例，除非gamma是多项式级别的大，否则没有解存在，我们在资源增强框架中研究了这个概念，隐含地将大小为k的集合R的约束说明为其大小仅为k/alpha，其中alpha > 1。此外，受到选举应用的启发，我们最...

    We introduce a novel definition for a small set R of k points being "representative" of a larger set in a metric space. Given a set V (e.g., documents or voters) to represent, and a set C of possible representatives, our criterion requires that for any subset S comprising a theta fraction of V, the average distance of S to their best theta*k points in R should not be more than a factor gamma compared to their average distance to the best theta*k points among all of C. This definition is a strengthening of proportional fairness and core fairness, but - different from those notions - requires that large cohesive clusters be represented proportionally to their size.  Since there are instances for which - unless gamma is polynomially large - no solutions exist, we study this notion in a resource augmentation framework, implicitly stating the constraints for a set R of size k as though its size were only k/alpha, for alpha > 1. Furthermore, motivated by the application to elections, we most
    
[^85]: 一种用于长时间心血管疾病检测的紧凑型LSTM-SVM融合模型

    A Compact LSTM-SVM Fusion Model for Long-Duration Cardiovascular Diseases Detection. (arXiv:2312.09442v2 [eess.SP] UPDATED)

    [http://arxiv.org/abs/2312.09442](http://arxiv.org/abs/2312.09442)

    本研究提出了一个紧凑型的LSTM-SVM融合模型，实现了对心血管疾病的早期检测。该模型采用了一种流程优化方法将心电图信号预处理为一致的10秒持续时间，同时利用LSTM和SVM实现了最先进的结果。

    

    全球范围内，心血管疾病是致死的主要原因，每年约有1790万人死于该病。早期检测心血管疾病是关键的临床目标，电心图（ECG）数据是一个受到研究界关注的领域。最近基于机器学习和深度学习的进展在该领域取得了巨大的进展。然而，现有的方法存在固有的局限性，包括不适当的模型评估和数据泄漏的实例。在本研究中，我们提出了一个简化的工作流程范式，将ECG信号预处理为一致的10秒持续时间，消除了手动特征提取/心搏检测的需要。我们还提出了一种混合模型，利用长短期记忆网络（LSTM）与支持向量机（SVM）进行欺诈检测。该架构包括两个LSTM层和一个SVM分类器，实现了SOTA结果，平均精确度评分为...

    Globally, cardiovascular diseases (CVDs) are the leading cause of mortality, accounting for an estimated 17.9 million deaths annually. One critical clinical objective is the early detection of CVDs using electrocardiogram (ECG) data, an area that has received significant attention from the research community. Recent advancements based on machine learning and deep learning have achieved great progress in this domain. However, existing methodologies exhibit inherent limitations, including inappropriate model evaluations and instances of data leakage. In this study, we present a streamlined workflow paradigm for preprocessing ECG signals into consistent 10-second durations, eliminating the need for manual feature extraction/beat detection. We also propose a hybrid model of Long Short-Term Memory (LSTM) with Support Vector Machine (SVM) for fraud detection. This architecture consists of two LSTM layers and an SVM classifier, which achieves a SOTA results with an Average precision score of 
    
[^86]: 一种用于小缺陷检测的增量统一框架

    An Incremental Unified Framework for Small Defect Inspection. (arXiv:2312.08917v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2312.08917](http://arxiv.org/abs/2312.08917)

    我们提出了一种增量统一框架（IUF），用于解决工业制造中的小缺陷检测问题。通过引入对象感知自注意力（OASA）和语义压缩损失（SCL），我们的方法可以在不断整合新物体时减少特征冲突问题，并展现出卓越的性能。

    

    在工业制造中，基于人工智能的缺陷检测至关重要。然而，许多方法针对特定的流水线，在面对各种产品组合和不断变化的流程时往往面临特征冲突问题。为解决这个问题，我们提出了增量统一框架（IUF），可以在不断整合新物体时减少特征冲突问题，这在面临物体增量学习场景中具有优势。我们使用最先进的transformer引入了对象感知自注意力（OASA）来划分明确的语义边界。集成了语义压缩损失（SCL）来优化非主要语义空间，增强了网络对新物体的适应性。此外，在权重更新时，我们优先保留已建立物体的特征。通过在图像和像素级缺陷检测方面展示出卓越性能，我们的方法实现了最先进的性能，对于动态和可扩展的工业检测至关重要。

    Artificial Intelligence (AI)-driven defect inspection is pivotal in industrial manufacturing. Yet, many methods, tailored to specific pipelines, grapple with diverse product portfolios and evolving processes. Addressing this, we present the Incremental Unified Framework (IUF), which can reduce the feature conflict problem when continuously integrating new objects in the pipeline, making it advantageous in object-incremental learning scenarios. Employing a state-of-the-art transformer, we introduce Object-Aware Self-Attention (OASA) to delineate distinct semantic boundaries. Semantic Compression Loss (SCL) is integrated to optimize non-primary semantic space, enhancing network adaptability for novel objects. Additionally, we prioritize retaining the features of established objects during weight updates. Demonstrating prowess in both image and pixel-level defect inspection, our approach achieves state-of-the-art performance, proving indispensable for dynamic and scalable industrial inspe
    
[^87]: 大型语言模型中的隐私问题：一项调研

    Privacy Issues in Large Language Models: A Survey. (arXiv:2312.06717v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2312.06717](http://arxiv.org/abs/2312.06717)

    这项调研关注大型语言模型中的隐私问题。主要总结了红队模型揭示隐私风险、将隐私纳入训练和推理过程、高效删除训练模型中的数据以符合隐私法规、以及减轻版权问题等技术研究。法律和政策研究虽然从不同角度解决了相关挑战，但不是本调研的重点。

    

    这是首个关注大型语言模型（LLMs）隐私问题的AI研究领域调研。具体而言，我们关注红队模型以突出隐私风险的工作，尝试将隐私纳入训练或推理过程中的工作，使得数据可以从训练模型中高效删除以符合现有的隐私法规，并试图减轻版权问题。我们的重点在于总结开发算法、证明定理和进行实证评估的技术研究。虽然有大量的法律和政策研究从不同角度解决这些挑战，但这不是我们调研的重点。然而，这些作品以及最近的法律进展确实影响了这些技术问题的形式化处理方式，因此我们在第一节中对其进行了简要讨论。尽管我们已经尽力包含所有相关研究，但由于这一领域的研究进展迅速，我们可能会漏掉一些最新的研究成果。

    This is the first survey of the active area of AI research that focuses on privacy issues in Large Language Models (LLMs). Specifically, we focus on work that red-teams models to highlight privacy risks, attempts to build privacy into the training or inference process, enables efficient data deletion from trained models to comply with existing privacy regulations, and tries to mitigate copyright issues. Our focus is on summarizing technical research that develops algorithms, proves theorems, and runs empirical evaluations. While there is an extensive body of legal and policy work addressing these challenges from a different angle, that is not the focus of our survey. Nevertheless, these works, along with recent legal developments do inform how these technical problems are formalized, and so we discuss them briefly in Section 1. While we have made our best effort to include all the relevant work, due to the fast moving nature of this research we may have missed some recent work. If we h
    
[^88]: 线性对数正态注意力与无偏集中力

    Linear Log-Normal Attention with Unbiased Concentration. (arXiv:2311.13541v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2311.13541](http://arxiv.org/abs/2311.13541)

    本论文研究了自注意机制，并分析了注意力矩阵的分布和集中能力。通过引入线性对数正态注意力来模拟原始自注意力的分布和集中行为，提高了Transformer模型的可扩展性。

    

    Transformer模型在各种应用中取得了显著的成果。然而，由于自注意机制的时间和内存复杂度与序列长度的二次关系，其可扩展性受到限制。当处理长文档或高分辨率图像时，这一限制构成了重大障碍。本研究通过分析注意力矩阵的分布和集中能力，对自注意机制进行了研究。此外，我们提出了衡量这些数量的工具，并引入了一种新的自注意机制，即线性对数正态注意力，旨在模拟原始自注意力的分布和集中行为。我们在常用的自然语言基准测试上的实验证明，我们提出的线性对数正态注意力优于其他线性化注意力替代方法，为增强Transformer模型的可扩展性提供了一个有前途的途径。我们的代码附在补充材料中。

    Transformer models have achieved remarkable results in a wide range of applications. However, their scalability is hampered by the quadratic time and memory complexity of the self-attention mechanism concerning the sequence length. This limitation poses a substantial obstacle when dealing with long documents or high-resolution images. In this work, we study the self-attention mechanism by analyzing the distribution of the attention matrix and its concentration ability. Furthermore, we propose instruments to measure these quantities and introduce a novel self-attention mechanism, Linear Log-Normal Attention, designed to emulate the distribution and concentration behavior of the original self-attention. Our experimental results on popular natural language benchmarks reveal that our proposed Linear Log-Normal Attention outperforms other linearized attention alternatives, offering a promising avenue for enhancing the scalability of transformer models. Our code is available in supplementary
    
[^89]: 将大型语言模型应用于电力系统：潜在的安全威胁

    Applying Large Language Models to Power Systems: Potential Security Threats. (arXiv:2311.13361v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2311.13361](http://arxiv.org/abs/2311.13361)

    将大型语言模型应用于电力系统可能带来潜在安全威胁，需要进行迫切的研究和开发相应对策。

    

    将大型语言模型（LLMs）应用于现代电力系统是提高决策和运营效率的一种有前景的途径。然而，这一行动也可能带来潜在的安全威胁，迄今尚未完全认识到。因此，本文分析了将LLMs应用于电力系统可能带来的潜在威胁，并强调了迫切需要研究和开发相应对策的重要性。

    Applying large language models (LLMs) to modern power systems presents a promising avenue for enhancing decision-making and operational efficiency. However, this action may also incur potential security threats, which have not been fully recognized so far. To this end, this article analyzes potential threats incurred by applying LLMs to power systems, emphasizing the need for urgent research and development of countermeasures.
    
[^90]: 正式规定基于LLM的代理人的高级行为

    Formally Specifying the High-Level Behavior of LLM-Based Agents. (arXiv:2310.08535v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2310.08535](http://arxiv.org/abs/2310.08535)

    本文介绍了一个最小生成框架，通过在高级声明中定义所需的代理人行为，然后构建解码监视器，从而实现了基于LLM的代理人的快速设计和实施。

    

    最近，由LLM驱动的自主目标驱动型代理人已成为解决具有挑战性问题的有前途的工具，而无需获得昂贵的任务特定的精细调整模型。目前，这类代理人的设计和实施是临时性的，因为LLM-based代理人可能应用于的各种任务的广泛性质意味着不能有一种适用于所有情况的代理人设计方法。在这项工作中，我们旨在通过提出一个简化代理人构建过程的最小生成框架来减轻设计和实施新代理人的难度。我们引入的框架允许用户以高级声明的规范方式定义所需的代理人行为，然后使用这个规范构建解码监视器，以确保LLM会产生具有所需行为的输出。我们的声明性方法，即描述行为而不考虑如何实施或强制执行，可以实现快速设计

    Autonomous, goal-driven agents powered by LLMs have recently emerged as promising tools for solving challenging problems without the need for task-specific finetuned models that can be expensive to procure. Currently, the design and implementation of such agents is ad hoc, as the wide variety of tasks that LLM-based agents may be applied to naturally means there can be no one-size-fits-all approach to agent design. In this work we aim to alleviate the difficulty of designing and implementing new agents by proposing a minimalistic generation framework that simplifies the process of building agents. The framework we introduce allows the user to define desired agent behaviors in a high-level, declarative specification that is then used to construct a decoding monitor which guarantees the LLM will produce an output exhibiting the desired behavior. Our declarative approach, in which the behavior is described without concern for how it should be implemented or enforced, enables rapid design,
    
[^91]: 批量校准：重新思考上下文学习和提示工程的校准方法

    Batch Calibration: Rethinking Calibration for In-Context Learning and Prompt Engineering. (arXiv:2309.17249v1 [cs.CL])

    [http://arxiv.org/abs/2309.17249](http://arxiv.org/abs/2309.17249)

    本研究提出了一种名为批量校准（BC）的方法，用于解决大型语言模型中提示脆弱性和偏见因素导致的性能下降问题。BC通过控制批量输入的上下文偏见，统一了现有的校准方法，并具有零-shot和仅推理的特点。

    

    提示和上下文学习已成为大型语言模型（LLM）的高效学习范式。然而，LLM存在提示脆弱性和各种偏见因素，包括但不限于格式、选择性的表达方式和上下文学习示例。为解决这个导致性能下降的问题，已经开发了校准方法来减轻这些偏见的影响并恢复LLM的性能。在这项工作中，我们首先对现有的校准方法进行了系统分析，提供了统一的观点并揭示了失败案例。受这些分析的启发，我们提出了批量校准（BC），这是一种简单而直观的方法，可以从批量输入中控制上下文偏见，统一了各种先前的方法，并有效地解决了上述问题。BC是零-shot、仅推理和额外成本可忽略。在少-shot设置中，我们进一步扩展BC以实现全部翻译

    Prompting and in-context learning (ICL) have become efficient learning paradigms for large language models (LLMs). However, LLMs suffer from prompt brittleness and various bias factors in the prompt, including but not limited to the formatting, the choice verbalizers, and the ICL examples. To address this problem that results in unexpected performance degradation, calibration methods have been developed to mitigate the effects of these biases while recovering LLM performance. In this work, we first conduct a systematic analysis of the existing calibration methods, where we both provide a unified view and reveal the failure cases. Inspired by these analyses, we propose Batch Calibration (BC), a simple yet intuitive method that controls the contextual bias from the batched input, unifies various prior approaches, and effectively addresses the aforementioned issues. BC is zero-shot, inference-only, and incurs negligible additional costs. In the few-shot setup, we further extend BC to allo
    
[^92]: 重新审视用于连续学习中的Softmax掩码以提高稳定性

    Revisiting Softmax Masking for Stability in Continual Learning. (arXiv:2309.14808v1 [cs.LG])

    [http://arxiv.org/abs/2309.14808](http://arxiv.org/abs/2309.14808)

    本文重新审视了用于连续学习中的Softmax掩码的影响，并提出了一种利用其置信度保持效果的方法，通过增加稳定性同时保持准确性。

    

    在连续学习中，许多分类器使用Softmax函数来学习置信度。然而，许多研究指出其无法准确确定离群值的置信度分布，通常称为认识不确定性。这种固有限制还限制了在连续学习过程中选择何时忘记和保留先前训练的置信度分布的准确决策。为了解决这个问题，我们重新审视了掩码Softmax函数的影响。尽管这种方法在文献中既简单又普遍，但对于在连续学习过程中保持置信度分布（也称为稳定性）的影响尚未得到充分调查。在本文中，我们重新审视了Softmax掩码的影响，并引入了一种利用其置信度保持效果的方法。在具有和不具有记忆重放的类-和任务增量学习基准测试中，我们的方法显著增加了稳定性同时保持了足够大的准确性。

    In continual learning, many classifiers use softmax function to learn confidence. However, numerous studies have pointed out its inability to accurately determine confidence distributions for outliers, often referred to as epistemic uncertainty. This inherent limitation also curtails the accurate decisions for selecting what to forget and keep in previously trained confidence distributions over continual learning process. To address the issue, we revisit the effects of masking softmax function. While this method is both simple and prevalent in literature, its implication for retaining confidence distribution during continual learning, also known as stability, has been under-investigated. In this paper, we revisit the impact of softmax masking, and introduce a methodology to utilize its confidence preservation effects. In class- and task-incremental learning benchmarks with and without memory replay, our approach significantly increases stability while maintaining sufficiently large pla
    
[^93]: 预训练多语言翻译模型上的属性控制器能否迁移到其他语言？

    How Transferable are Attribute Controllers on Pretrained Multilingual Translation Models?. (arXiv:2309.08565v1 [cs.CL])

    [http://arxiv.org/abs/2309.08565](http://arxiv.org/abs/2309.08565)

    本文研究了如何将预训练的多语言翻译模型中的属性控制器迁移到没有监督数据的语言。通过全面分析不同数据场景下的训练和推断时控制技术，揭示了它们在零样本性能和领域鲁棒性上的相对优势和劣势。

    

    最近，将机器翻译模型定制为符合细粒度属性（如形式）已取得了巨大进展。然而，当前方法大多依赖于至少一些带有属性注释的监督数据。因此，数据稀缺仍然是将此定制能力普及到更广泛语言范围，尤其是低资源语言的一个瓶颈。鉴于最近在预训练大规模多语言翻译模型方面取得的进展，我们将它们作为对没有监督数据的语言进行属性控制能力迁移的基础。在这项工作中，我们基于预训练的NLLB-200模型对属性控制器的迁移进行了全面分析。我们研究了在各种数据场景下的训练和推断时控制技术，并揭示了它们在零样本性能和领域鲁棒性上的相对优势和劣势。我们显示出两种范式是互补的，通过一致的改进来证明。

    Customizing machine translation models to comply with fine-grained attributes such as formality has seen tremendous progress recently. However, current approaches mostly rely on at least some supervised data with attribute annotation. Data scarcity therefore remains a bottleneck to democratizing such customization possibilities to a wider range of languages, lower-resource ones in particular. Given recent progress in pretrained massively multilingual translation models, we use them as a foundation to transfer the attribute controlling capabilities to languages without supervised data. In this work, we present a comprehensive analysis of transferring attribute controllers based on a pretrained NLLB-200 model. We investigate both training- and inference-time control techniques under various data scenarios, and uncover their relative strengths and weaknesses in zero-shot performance and domain robustness. We show that both paradigms are complementary, as shown by consistent improvements o
    
[^94]: 透过偏好看大型语言模型的反馈获取：揭示对齐的重要性

    Peering Through Preferences: Unraveling Feedback Acquisition for Aligning Large Language Models. (arXiv:2308.15812v1 [cs.LG])

    [http://arxiv.org/abs/2308.15812](http://arxiv.org/abs/2308.15812)

    本研究分析了对于对齐和评估大型语言模型而言，设计反馈选择是评分还是排名对结果的影响。研究发现评分和排名所推断出的偏好存在不一致问题，并且注释者的偏见也会影响结果。同时，研究还发现反馈协议的选择也对评估结果有显著影响。

    

    大型语言模型（LLMs）与人类价值观和意图的对齐承诺涉及使用人工智能或人类反馈。稠密的反馈注释获取和整合成本较高，而稀疏的反馈则涉及结构性设计选择，即评分（例如，在1-7的范围内对回答A进行评分）和排名（例如，回答A是否比回答B更好？）。在这项工作中，我们分析了这种设计选择对LLMs的对齐和评估的影响。我们发现，评分和排名所推断出的偏好在人类和AI注释者中都存在严重的不一致问题，达到了60%。我们的后续分析确定了解释这个现象的各种注释者偏见方面，比如人类注释者更喜欢密集回答并在两个选项之间更青睐准确性。令我们惊讶的是，我们还观察到反馈协议的选择对对齐的LLMs的评估也有显著影响。特别是，我们发现LLMs的评估结果因为反馈协议的选择而有所不同。

    Aligning large language models (LLMs) with human values and intents critically involves the use of human or AI feedback. While dense feedback annotations are expensive to acquire and integrate, sparse feedback presents a structural design choice between ratings (e.g., score Response A on a scale of 1-7) and rankings (e.g., is Response A better than Response B?). In this work, we analyze the effect of this design choice for the alignment and evaluation of LLMs. We uncover an inconsistency problem wherein the preferences inferred from ratings and rankings significantly disagree 60% for both human and AI annotators. Our subsequent analysis identifies various facets of annotator biases that explain this phenomena, such as human annotators would rate denser responses higher while preferring accuracy during pairwise judgments. To our surprise, we also observe that the choice of feedback protocol also has a significant effect on the evaluation of aligned LLMs. In particular, we find that LLMs
    
[^95]: CALM: 一种用于全面评估语言模型偏见的多任务基准数据集

    CALM : A Multi-task Benchmark for Comprehensive Assessment of Language Model Bias. (arXiv:2308.12539v1 [cs.CL])

    [http://arxiv.org/abs/2308.12539](http://arxiv.org/abs/2308.12539)

    CALM是一个用于量化语言模型偏见的多任务基准数据集，相比先前数据集更加多样和可靠，能更好地捕捉评估模型偏见所需的语言变化的广度。

    

    随着语言模型（LMs）的不断增强，量化和比较它们在社会和人口学偏见方面的能力以及潜在的危害变得越来越重要。先前的偏见测量数据集对于人工设计模板的扰动敏感，因此不可靠。为了保证可靠性，我们引入了全面评估语言模型偏见（CALM）的基准数据集，用于量化LMs在三个任务上的偏见。我们整合了来自不同领域（如维基百科和新闻文章）的16个现有数据集，过滤出224个模板，并构建了一个包含78,400个示例的数据集。我们通过平均语义相似性和模板长度的变异程度等指标，比较CALM与先前数据集的多样性，并测试其对细微扰动的敏感性。我们展示了我们的数据集相对于先前数据集更加多样和可靠，因此能更好地捕捉评估模型偏见所需的语言变化的广度。我们评估了20个大型语言模型的偏见。

    As language models (LMs) become increasingly powerful, it is important to quantify and compare them for sociodemographic bias with potential for harm. Prior bias measurement datasets are sensitive to perturbations in their manually designed templates, therefore unreliable. To achieve reliability, we introduce the Comprehensive Assessment of Language Model bias (CALM), a benchmark dataset to quantify bias in LMs across three tasks. We integrate 16 existing datasets across different domains, such as Wikipedia and news articles, to filter 224 templates from which we construct a dataset of 78,400 examples. We compare the diversity of CALM with prior datasets on metrics such as average semantic similarity, and variation in template length, and test the sensitivity to small perturbations. We show that our dataset is more diverse and reliable than previous datasets, thus better capture the breadth of linguistic variation required to reliably evaluate model bias. We evaluate 20 large language 
    
[^96]: 分布式动态规划和分布式TD-Learning的网络多智能体马尔可夫决策过程的ODE框架

    Distributed Dynamic Programming and an O.D.E. Framework of Distributed TD-Learning for Networked Multi-Agent Markov Decision Processes. (arXiv:2307.16706v2 [eess.SY] UPDATED)

    [http://arxiv.org/abs/2307.16706](http://arxiv.org/abs/2307.16706)

    本文研究了网络多智能体马尔可夫决策问题中的分布式动态规划和分布式TD学习算法。其中，我们通过引入新的分布式DP算法和分布式TD学习算法，并证明了它们的收敛性，提出了两个关键点。该分布式DP算法具有两个独立的动态系统的特点。

    

    本文主要研究网络多智能体马尔可夫决策问题中的分布式动态规划（DP）和分布式时序差分（TD）学习算法。我们采用分布式多智能体框架，其中各个智能体只能访问自己的奖励，缺乏对其他智能体奖励的了解。此外，每个智能体都能通过一个由图表示的通信网络与相邻智能体共享其参数。我们的贡献可以总结为两个关键点：1）我们引入了一个新的受连续时间区间内的平均一致性方法启发的分布式DP。通过控制理论的视角评估了该DP的收敛性。2）基于上述DP，我们设计了一个新的分布式TD学习算法并证明了其收敛性。我们提出的分布式DP的一个显著特点是其包含了两个独立的动态系统。

    The primary objective of this paper is to investigate distributed dynamic programming (DP) and distributed temporal difference (TD) learning algorithms for networked multi-agent Markov decision problems (MAMDPs). In our study, we adopt a distributed multi-agent framework where individual agents have access only to their own rewards, lacking insights into the rewards of other agents. Additionally, each agent has the ability to share its parameters with neighboring agents through a communication network, represented by a graph. Our contributions can be summarized in two key points: 1) We introduce a novel distributed DP, inspired by the averaging consensus method in the continuous-time domain. The convergence of this DP is assessed through control theory perspectives. 2) Building upon the aforementioned DP, we devise a new distributed TD-learning algorithm and prove its convergence. A standout feature of our proposed distributed DP is its incorporation of two independent dynamic systems,
    
[^97]: 背包问题：连通性、路径和最短路径

    Knapsack: Connectedness, Path, and Shortest-Path. (arXiv:2307.12547v2 [cs.DS] UPDATED)

    [http://arxiv.org/abs/2307.12547](http://arxiv.org/abs/2307.12547)

    该论文研究了带有图论约束的背包问题，证明了问题的复杂性并提出了近似算法。

    

    我们研究了带有图论约束的背包问题。也就是说，我们假设背包项目集上存在一个图结构，并且解决方案还需要满足背包约束之上的某些图论性质。特别是，在连通背包问题中，我们需要计算一个连通的项目子集，其价值最大，且满足背包约束的大小。我们证明了即使对于最大度为四的图，这个问题也是强NP完全的，对于星形图，这个问题也是NP完全的。另一方面，我们开发了一个运行时间为$O\left(2^{tw\log tw}\cdot\text{poly}(\min\{s^2,d^2\})\right)$的算法，在其中$tw,s,d$分别是图的树宽度，背包的大小和目标值。我们进一步展示了一个$(1-\epsilon)$近似算法，其运行时间为$O\left(2^{tw\log tw}\cdot\text{poly}(n,1/\epsilon)\right)$，对于每个$\epsilon>0$都成立。我们还展示了对几个其他图论性质的类似结果。

    We study the knapsack problem with graph theoretic constraints. That is, we assume that there exists a graph structure on the set of items of knapsack and the solution also needs to satisfy certain graph theoretic properties on top of knapsack constraints. In particular, we need to compute in the connected knapsack problem a connected subset of items which has maximum value subject to the size of knapsack constraint. We show that this problem is strongly NP-complete even for graphs of maximum degree four and NP-complete even for star graphs. On the other hand, we develop an algorithm running in time $O\left(2^{tw\log tw}\cdot\text{poly}(\min\{s^2,d^2\})\right)$ where $tw,s,d$ are respectively treewidth of the graph, size, and target value of the knapsack. We further exhibit a $(1-\epsilon)$ factor approximation algorithm running in time $O\left(2^{tw\log tw}\cdot\text{poly}(n,1/\epsilon)\right)$ for every $\epsilon>0$. We show similar results for several other graph theoretic propertie
    
[^98]: VELMA: LLM智能体在街景中进行视觉和语言导航的口头化体现

    VELMA: Verbalization Embodiment of LLM Agents for Vision and Language Navigation in Street View. (arXiv:2307.06082v1 [cs.AI])

    [http://arxiv.org/abs/2307.06082](http://arxiv.org/abs/2307.06082)

    VELMA是一个口头化的LLM智能体，利用人类写作的导航指令中的地标并结合CLIP来进行视觉环境的理解，以实现在街景中的视觉和语言导航。

    

    在现实世界环境中的增量决策是具有挑战性的以体现人工智能的任务之一。其中最具挑战性的场景之一是视觉和语言导航(VLN)，它需要视觉和自然语言理解以及空间和时间推理能力。这个体现智能体需要在街景等真实世界环境的观察基础上准确理解导航指令。尽管LLM在其他研究领域取得了令人印象深刻的结果，但如何最好地将它们与交互式视觉环境连接起来仍然是一个持续的问题。在这项工作中，我们提出了VELMA，一种使用轨迹和视觉环境观察的口头化作为下一步操作的上下文提示的LLM智能体。视觉信息通过一个流程进行口头化，该流程从人类编写的导航指令中提取地标，并使用CLIP来确定它们在当前全景视图中的可见性。

    Incremental decision making in real-world environments is one of the most challenging tasks in embodied artificial intelligence. One particularly demanding scenario is Vision and Language Navigation~(VLN) which requires visual and natural language understanding as well as spatial and temporal reasoning capabilities. The embodied agent needs to ground its understanding of navigation instructions in observations of a real-world environment like Street View. Despite the impressive results of LLMs in other research areas, it is an ongoing problem of how to best connect them with an interactive visual environment. In this work, we propose VELMA, an embodied LLM agent that uses a verbalization of the trajectory and of visual environment observations as contextual prompt for the next action. Visual information is verbalized by a pipeline that extracts landmarks from the human written navigation instructions and uses CLIP to determine their visibility in the current panorama view. We show that
    
[^99]: Surge Routing：基于事件信息的多智能体强化学习用于自动拼车

    Surge Routing: Event-informed Multiagent Reinforcement Learning for Autonomous Rideshare. (arXiv:2307.02637v1 [cs.AI])

    [http://arxiv.org/abs/2307.02637](http://arxiv.org/abs/2307.02637)

    这项研究提出了一种基于事件信息的多智能体强化学习框架，用于自动拼车，通过预测和适应需求激增，并生成协同的路由和接乘策略来服务更多请求。

    

    大型事件通常会引发拼车服务需求的激增，而这些需求不会被平均需求模式所捕捉到，给路由算法带来了独特的挑战。我们提出了一个学习框架，用于自动化出租车车队，从互联网上获取事件数据以预测和适应需求激增，并生成协同的路由和接乘策略，以服务更多的请求比其他路由协议。我们通过以下方式实现这一目标：(i)一个事件处理框架，用于从互联网上抓取事件信息并生成密集的向量表示，作为神经网络的输入特征来预测需求；(ii)一个双神经网络系统，使用这些密集的向量表示来预测整个地图上的每小时需求；(iii)一种基于概率的方法，利用区域占用时间表将公开的需求数据映射到离散化的街道交叉口上。

    Large events such as conferences, concerts and sports games, often cause surges in demand for ride services that are not captured in average demand patterns, posing unique challenges for routing algorithms. We propose a learning framework for an autonomous fleet of taxis that scrapes event data from the internet to predict and adapt to surges in demand and generates cooperative routing and pickup policies that service a higher number of requests than other routing protocols. We achieve this through a combination of (i) an event processing framework that scrapes the internet for event information and generates dense vector representations that can be used as input features for a neural network that predicts demand; (ii) a two neural network system that predicts hourly demand over the entire map, using these dense vector representations; (iii) a probabilistic approach that leverages locale occupancy schedules to map publicly available demand data over sectors to discretized street inters
    
[^100]: 基于图神经网络的日志异常检测与解释

    Graph Neural Networks based Log Anomaly Detection and Explanation. (arXiv:2307.00527v2 [cs.SE] UPDATED)

    [http://arxiv.org/abs/2307.00527](http://arxiv.org/abs/2307.00527)

    提出了一种基于图神经网络的无监督日志异常检测方法，该方法将事件日志转换为带属性、有向和加权的图，并利用图神经网络进行图级别的异常检测。引入了一种新的图神经网络模型OCDiGCN来检测一组带属性、有向和加权的图中的图级别异常，并提供对异常的解释能力。

    

    事件日志被广泛用于记录高科技系统的状态，因此日志异常检测对于监控这些系统非常重要。大多数现有的日志异常检测方法将日志事件计数矩阵或日志事件序列作为输入，利用日志事件之间的定量和/或顺序关系来检测异常。然而，仅考虑定量或顺序关系可能导致检测准确性较低。为了缓解这个问题，我们提出了一种基于图的无监督日志异常检测方法，称为Logs2Graphs，它首先将事件日志转换为带属性、有向和加权的图，然后利用图神经网络进行图级别的异常检测。具体而言，我们提出了一种全新的图神经网络模型One-Class Digraph Inception Convolutional Networks（OCDiGCN），用于在一组带属性、有向和加权的图中检测图级别的异常。通过将图表示与属性表示耦合起来，在图级别上进行异常检测的同时提供了对异常的解释能力。

    Event logs are widely used to record the status of high-tech systems, making log anomaly detection important for monitoring those systems. Most existing log anomaly detection methods take a log event count matrix or log event sequences as input, exploiting quantitative and/or sequential relationships between log events to detect anomalies. Unfortunately, only considering quantitative or sequential relationships may result in low detection accuracy. To alleviate this problem, we propose a graph-based method for unsupervised log anomaly detection, dubbed Logs2Graphs, which first converts event logs into attributed, directed, and weighted graphs, and then leverages graph neural networks to perform graph-level anomaly detection. Specifically, we introduce One-Class Digraph Inception Convolutional Networks, abbreviated as OCDiGCN, a novel graph neural network model for detecting graph-level anomalies in a collection of attributed, directed, and weighted graphs. By coupling the graph represe
    
[^101]: TopP\&R: 具有鲁棒性的支持估计方法，用于评估生成模型中的保真度和多样性

    TopP\&R: Robust Support Estimation Approach for Evaluating Fidelity and Diversity in Generative Models. (arXiv:2306.08013v1 [cs.LG])

    [http://arxiv.org/abs/2306.08013](http://arxiv.org/abs/2306.08013)

    本文提出了一种鲁棒可靠的生成模型评估指标TopP\&R，通过引入拓扑和统计处理进行严格的支持估计。TopP\&R仅保留具有一定置信水平的具有拓扑和统计上重要性的特征，对于噪声特征具有强大的鲁棒性，并提供了统计一致性。

    

    本文提出了一种鲁棒可靠的生成模型评估指标，通过引入拓扑和统计处理进行严格的支持估计。现有的度量标准，如Inception Score（IS），Fr\'echet Inception Distance（FID）以及Precision and Recall（P\&R）的变体，严重依赖于从样本特征估计的支持。然而，尽管评估的质量完全取决于其可靠性，但其估计的可靠性并没有得到严肃的讨论（并被忽视）。本文提出了拓扑精度和召回率（TopP\&R，发音为“topper”），它提供了一种系统的方法来估计支持，仅保留具有一定置信水平的具有拓扑和统计上重要性的特征。这不仅使TopP\&R对于噪声特征具有强大的鲁棒性，而且还提供了统计一致性。我们的理论和实验结果表明，TopP\&R对于离群值和非独立同分布具有鲁棒性。

    We propose a robust and reliable evaluation metric for generative models by introducing topological and statistical treatments for rigorous support estimation. Existing metrics, such as Inception Score (IS), Fr\'echet Inception Distance (FID), and the variants of Precision and Recall (P\&R), heavily rely on supports that are estimated from sample features. However, the reliability of their estimation has not been seriously discussed (and overlooked) even though the quality of the evaluation entirely depends on it. In this paper, we propose Topological Precision and Recall (TopP\&R, pronounced 'topper'), which provides a systematic approach to estimating supports, retaining only topologically and statistically important features with a certain level of confidence. This not only makes TopP\&R strong for noisy features, but also provides statistical consistency. Our theoretical and experimental results show that TopP\&R is robust to outliers and non-independent and identically distributed
    
[^102]: 从少量根因的数据中学习DAGs

    Learning DAGs from Data with Few Root Causes. (arXiv:2305.15936v1 [cs.LG])

    [http://arxiv.org/abs/2305.15936](http://arxiv.org/abs/2305.15936)

    该论文提出了一种新的算法，能够从仅有少量根因的数据中学习DAGs，并证明了其可识别性，并在性能上优于以前的方法。

    

    我们提出了一种新的角度和算法，用于从线性结构方程模型(SEM)生成的数据中学习有向无环图(DAGs)。我们首先展示线性SEM可以被视为一种线性变换，先前的工作使用一个由与节点关联的随机值根因(我们将其称为)的稠密输入向量计算数据。我们考虑仅存在几个根因(近似)的情况，并在数据的测量中引入噪声。从直觉上讲，这意味着DAG数据是由少数数据生成事件产生的，其效果通过DAG传播。我们在这种新设置中证明可识别性，并表明真正的DAG是根因向量L0-范数的全局最小化者。对于具有少量根因的数据，有和没有噪音的情况下，我们表现出比以前的DAG学习方法更优异的性能。

    We present a novel perspective and algorithm for learning directed acyclic graphs (DAGs) from data generated by a linear structural equation model (SEM). First, we show that a linear SEM can be viewed as a linear transform that, in prior work, computes the data from a dense input vector of random valued root causes (as we will call them) associated with the nodes. Instead, we consider the case of (approximately) few root causes and also introduce noise in the measurement of the data. Intuitively, this means that the DAG data is produced by few data-generating events whose effect percolates through the DAG. We prove identifiability in this new setting and show that the true DAG is the global minimizer of the $L^0$-norm of the vector of root causes. For data with few root causes, with and without noise, we show superior performance compared to prior DAG learning methods.
    
[^103]: 重新考虑基于GNN的异构知识图谱实体对齐：新数据集和新方法

    Rethinking GNN-based Entity Alignment on Heterogeneous Knowledge Graphs: New Datasets and A New Method. (arXiv:2304.03468v1 [cs.LG])

    [http://arxiv.org/abs/2304.03468](http://arxiv.org/abs/2304.03468)

    文章重新考虑了基于GNN的异构知识图谱实体对齐。为探究EA实际场景中的表现，提出了更接近现实的高度异构知识图谱数据集，并提出了新方法。

    

    知识图谱（KG）应用的发展导致了需要从各种来源提取的异构KG之间的实体对齐（EA）的不断增长需求。近来，由于GNN的出色结构信息捕捉能力，在EA任务中广泛采用GNN。然而，我们观察到现有常见EA数据集的过于简单化的设置与现实场景相距甚远，这妨碍了对最近方法所取得进展的全面理解。这种现象使我们深思：现有基于GNN的EA方法是否真的取得了伟大进展？为了研究EA方法在现实情况下的性能，本文聚焦于高度异构的KG（HHKG）（例如，事件KG和通用KG）的对齐，这些KG在规模和结构上不同，并共享更少的重叠实体。首先，我们清理了不合理的设置，并提出了两个新的HHKG数据集，其密切地模拟了现实世界场景。

    The development of knowledge graph (KG) applications has led to a rising need for entity alignment (EA) between heterogeneous KGs that are extracted from various sources. Recently, graph neural networks (GNNs) have been widely adopted in EA tasks due to GNNs' impressive ability to capture structure information. However, we have observed that the oversimplified settings of the existing common EA datasets are distant from real-world scenarios, which obstructs a full understanding of the advancements achieved by recent methods. This phenomenon makes us ponder: Do existing GNN-based EA methods really make great progress?  In this paper, to study the performance of EA methods in realistic settings, we focus on the alignment of highly heterogeneous KGs (HHKGs) (e.g., event KGs and general KGs) which are different with regard to the scale and structure, and share fewer overlapping entities. First, we sweep the unreasonable settings, and propose two new HHKG datasets that closely mimic real-wo
    
[^104]: 多天线系统中数字无线协作联邦学习的研究

    Digital Over-the-Air Federated Learning in Multi-Antenna Systems. (arXiv:2302.14648v2 [cs.IT] UPDATED)

    [http://arxiv.org/abs/2302.14648](http://arxiv.org/abs/2302.14648)

    本文研究了在多天线系统中采用数字调制和空中计算的情况下，联邦学习的性能优化问题。通过结合数字调制和AirComp，提出了一种改进的联邦平均算法，以解决无线信道衰落导致的总体失真问题。

    

    本文研究了在实际的无线多输入多输出（MIMO）通信系统中，采用数字调制和空中计算（AirComp）的情况下，联邦学习（FL）的性能优化。特别考虑了一个MIMO系统，在该系统中，边缘设备使用波束形成将其本地收集的数据训练的本地FL模型传输给参数服务器（PS），以最大化可调度传输的设备数量。PS作为中央控制器，使用接收到的本地FL模型生成一个全局FL模型并广播给所有设备。由于无线网络中的带宽有限，采用了AirComp以实现高效的无线数据聚合。然而，无线信道的衰落会产生基于AirComp的FL方案中的总体失真。为了克服这一挑战，我们提出了一种改进的联邦平均（FedAvg）算法，将数字调制与AirComp相结合以减轻失真问题。

    In this paper, the performance optimization of federated learning (FL), when deployed over a realistic wireless multiple-input multiple-output (MIMO) communication system with digital modulation and over-the-air computation (AirComp) is studied. In particular, a MIMO system is considered in which edge devices transmit their local FL models (trained using their locally collected data) to a parameter server (PS) using beamforming to maximize the number of devices scheduled for transmission. The PS, acting as a central controller, generates a global FL model using the received local FL models and broadcasts it back to all devices. Due to the limited bandwidth in a wireless network, AirComp is adopted to enable efficient wireless data aggregation. However, fading of wireless channels can produce aggregate distortions in an AirComp-based FL scheme. To tackle this challenge, we propose a modified federated averaging (FedAvg) algorithm that combines digital modulation with AirComp to mitigate
    
[^105]: 使用集成边界逼近的对抗检测方法

    Adversarial Detection by Approximation of Ensemble Boundary. (arXiv:2211.10227v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.10227](http://arxiv.org/abs/2211.10227)

    本论文提出了一种使用Walsh系数逼近决策边界的对抗攻击检测方法，通过观察清晰图像和对抗图像之间的Walsh系数逼近差异，实现了对对抗攻击的检测。

    

    本论文提出了一种新的对抗攻击检测方法，针对解决两类模式识别问题的深度神经网络（DNN）集成。该集成使用Walsh系数进行组合，能够逼近布尔函数并控制集成决策边界的复杂性。本文的假设是高曲率的决策边界允许找到对抗扰动，但会改变决策边界的曲率，而与清晰图像相比，使用Walsh系数对其进行逼近的方式也有所不同。通过观察清晰图像和对抗图像之间的Walsh系数逼近差异，实验证明了攻击的可迁移性可用于检测。此外，逼近决策边界可能有助于理解DNN的学习和可迁移性特性。尽管本文的实验使用图像，所提出的方法可以用于建模两类模式识别问题的集成边界逼近。

    A new method of detecting adversarial attacks is proposed for an ensemble of Deep Neural Networks (DNNs) solving two-class pattern recognition problems. The ensemble is combined using Walsh coefficients which are capable of approximating Boolean functions and thereby controlling the complexity of the ensemble decision boundary. The hypothesis in this paper is that decision boundaries with high curvature allow adversarial perturbations to be found, but change the curvature of the decision boundary, which is then approximated in a different way by Walsh coefficients compared to the clean images. By observing the difference in Walsh coefficient approximation between clean and adversarial images, it is shown experimentally that transferability of attack may be used for detection. Furthermore, approximating the decision boundary may aid in understanding the learning and transferability properties of DNNs. While the experiments here use images, the proposed approach of modelling two-class en
    
[^106]: TE2Rules: 使用规则解释树集合模型

    TE2Rules: Explaining Tree Ensembles using Rules. (arXiv:2206.14359v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.14359](http://arxiv.org/abs/2206.14359)

    本文介绍了一种将二元分类任务中的树集合模型转换为可解释规则列表的方法，该方法可以有效解释模型对于少数类别的预测。实验证明，TE2Rules方法生成的规则列表准确性较高，并且运行时间与其他基线方法相当。

    

    树集合（如梯度提升树）通常相比单棵决策树具有更高的预测性能，然而，树集合模型通常缺乏透明度和可解释性，因为人类难以理解它们的决策逻辑。本文提出一种新颖的方法，将用于二元分类任务的树集合模型转换为接近树集合的可解释规则列表，并且有效地解释模型对于模型预测的少数类别。在基准数据集上的实验表明，TE2Rules生成的规则列表相对于现有方法具有更高的准确性，TE2Rules的运行时间与其他类似基线方法相当，TE2Rules算法的运行时间可以以稍微降低的准确性为代价进行权衡。

    Tree Ensemble (TE) models (like Gradient Boosted Trees) often provide higher prediction performance compared to single decision trees. However, TE models generally lack transparency and interpretability, as humans have difficulty understanding their decision logic. This paper presents a novel approach to convert a TE trained for a binary classification task, to a rule list (RL) that closely approximates the TE and is interpretable for a human. This RL can effectively explain the model even on the minority class predicted by the model. Experiments on benchmark datasets demonstrate that, (i) predictions from the RL generated by TE2Rules have higher fidelity (with respect to the original TE) compared to state-of-the-art methods, (ii) the run-time of TE2Rules is comparable to that of some other similar baselines and (iii) the run-time of TE2Rules algorithm can be traded off at the cost of a slightly lower fidelity.
    
[^107]: 快速策略转移的相对策略过渡优化

    Relative Policy-Transition Optimization for Fast Policy Transfer. (arXiv:2206.06009v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.06009](http://arxiv.org/abs/2206.06009)

    本论文介绍了相对策略过渡优化的方法，通过引入一个基于强化学习的引理衡量两个MDP之间的相对差距，并提出了相对策略优化和相对转移优化两个算法来实现快速策略转移和动态建模。同时，将这两个算法集成在一起形成完整的相对策略过渡优化算法。

    

    本文考虑了在两个马尔可夫决策过程之间的策略转移问题。我们引入了一个基于强化学习中现有理论结果的引理，用于衡量任意两个MDP之间的相对差距，即不同策略和环境动态下累积预期回报的差异。基于该引理，我们提出了两个新算法，分别为相对策略优化（RPO）和相对转移优化（RTO），分别提供了快速策略转移和动态建模。RPO通过将在一个环境中评估的策略转移以最大化在另一个环境中的回报，而RTO则通过更新参数化的动态模型来减小两个环境动态之间的差距。将这两个算法集成在一起可以得到完整的相对策略过渡优化（RPTO）算法，其中策略同时与两个环境进行交互，从而从两个环境中收集数据。

    We consider the problem of policy transfer between two Markov Decision Processes (MDPs). We introduce a lemma based on existing theoretical results in reinforcement learning to measure the relativity gap between two arbitrary MDPs, that is the difference between any two cumulative expected returns defined on different policies and environment dynamics. Based on this lemma, we propose two new algorithms referred to as Relative Policy Optimization (RPO) and Relative Transition Optimization (RTO), which offer fast policy transfer and dynamics modelling, respectively. RPO transfers the policy evaluated in one environment to maximize the return in another, while RTO updates the parameterized dynamics model to reduce the gap between the dynamics of the two environments. Integrating the two algorithms results in the complete Relative Policy-Transition Optimization (RPTO) algorithm, in which the policy interacts with the two environments simultaneously, such that data collections from two envi
    
[^108]: 多模式的虚假信息检测：方法、挑战与机遇

    Multi-modal Misinformation Detection: Approaches, Challenges and Opportunities. (arXiv:2203.13883v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2203.13883](http://arxiv.org/abs/2203.13883)

    这项研究总结了多模式虚假信息检测的方法、挑战和机遇。由于社交媒体平台的转变，虚假信息的性质也发生了变化。研究人员已经开发出自动检测跨模态不协调的技术，但仍面临挑战和不足之处，进一步的研究机会也在等待着挖掘。

    

    随着社交媒体平台从文本为主的论坛转向多模式环境，社交媒体中的虚假信息的性质也相应发生了变化。利用图像和视频等视觉模态更受用户青睐和吸引力的事实，以及文本内容有时被粗略浏览的情况，虚假信息传播者最近开始针对模态之间的上下文连接，例如文本和图像之间的关系。因此，许多研究人员已经开发出自动检测网页内容中可能存在的跨模态不协调的技术。我们分析、分类和识别现有的方法，以及它们面临的挑战和不足之处，以揭示多模式虚假信息检测领域的新研究机会。

    As social media platforms are evolving from text-based forums into multi-modal environments, the nature of misinformation in social media is also transforming accordingly. Taking advantage of the fact that visual modalities such as images and videos are more favorable and attractive to the users and textual contents are sometimes skimmed carelessly, misinformation spreaders have recently targeted contextual connections between the modalities e.g., text and image. Hence many researchers have developed automatic techniques for detecting possible cross-modal discordance in web-based content. We analyze, categorize and identify existing approaches in addition to challenges and shortcomings they face in order to unearth new research opportunities in the field of multi-modal misinformation detection.
    
[^109]: 实践中的AI伦理原则：设计师和开发者的视角

    AI Ethics Principles in Practice: Perspectives of Designers and Developers. (arXiv:2112.07467v6 [cs.CY] UPDATED)

    [http://arxiv.org/abs/2112.07467](http://arxiv.org/abs/2112.07467)

    这篇论文研究了设计师和开发人员的实践和经验，分析其如何遵循澳大利亚政府提出的高层次AI伦理原则，并总结出原则之间存在的张力和权衡。

    

    随着各种已发布的AI伦理原则的共识逐渐形成，高层次的原则和可立即采用的实际技术之间存在巨大差距，而这些技术可用于设计和开发负责任的AI系统。我们研究了来自澳大利亚国家科学研究机构(CSIRO)的研究人员和工程师的实践和经验，他们参与设计和开发许多应用领域的AI系统。我们使用半结构化访谈来研究参与者的实践如何与澳大利亚政府提出的一组高层次AI伦理原则相联系和一致。这些原则包括：(1)隐私保护和安全、(2)可靠性和安全性、(3)透明度和可解释性、(4)公正性、(5)可争议性、(6)责任制、(7)以人为中心的价值观、(8)人类、社会和环境的福祉。通过对访谈所获得的洞见的讨论，包括原则之间的各种张力和权衡。

    As consensus across the various published AI ethics principles is approached, a gap remains between high-level principles and practical techniques that can be readily adopted to design and develop responsible AI systems. We examine the practices and experiences of researchers and engineers from Australia's national scientific research agency (CSIRO), who are involved in designing and developing AI systems for many application areas. Semi-structured interviews were used to examine how the practices of the participants relate to and align with a set of high-level AI ethics principles proposed by the Australian Government. The principles comprise: (1) privacy protection and security, (2) reliability and safety, (3) transparency and explainability, (4) fairness, (5) contestability, (6) accountability, (7) human-centred values, (8) human, social and environmental wellbeing. Discussions on the gained insights from the interviews include various tensions and trade-offs between the principles,
    
[^110]: 一种新的用于无监督抽取式摘要方法的句子提取策略

    A New Sentence Extraction Strategy for Unsupervised Extractive Summarization Methods. (arXiv:2112.03203v5 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2112.03203](http://arxiv.org/abs/2112.03203)

    本文提出了一种新的句子提取策略，用于改善特征分布和降低摘要句子之间的相互信息，该策略适用于现有的无监督抽取式摘要方法，并在实验证明了其有效性。

    

    最近几年，由于神经网络模型的研究，文本摘要方法再次引起了广泛关注。当前大部分基于神经网络模型的文本摘要方法都是需要大规模数据集的监督方法。然而，在实际应用中，获取大规模数据集是困难的。本文从信息论的角度对抽取式文本摘要方法进行建模，并使用统一的框架描述了无监督抽取方法。为了改善特征分布并降低摘要句子之间的相互信息，我们提出了一种新的句子提取策略，可以应用于现有的无监督抽取方法。我们在不同的数据集上进行了实验，结果表明我们的策略确实有效并符合预期。

    In recent years, text summarization methods have attracted much attention again thanks to the researches on neural network models. Most of the current text summarization methods based on neural network models are supervised methods which need large-scale datasets. However, large-scale datasets are difficult to obtain in practical applications. In this paper, we model the task of extractive text summarization methods from the perspective of Information Theory, and then describe the unsupervised extractive methods with a uniform framework. To improve the feature distribution and to decrease the mutual information of summarization sentences, we propose a new sentence extraction strategy which can be applied to existing unsupervised extractive methods. Experiments are carried out on different datasets, and results show that our strategy is indeed effective and in line with expectations.
    

