# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Sam-Guided Enhanced Fine-Grained Encoding with Mixed Semantic Learning for Medical Image Captioning.](http://arxiv.org/abs/2311.01004) | 本论文提出了一种使用混合语义学习的Sam引导增强细粒度编码的医学图像字幕生成方法，有效地捕捉了医学图像的详细特征，并在评估指标上优于预训练的BLIP2模型。 |
| [^2] | [Optimizing Inventory Routing: A Decision-Focused Learning Approach using Neural Networks.](http://arxiv.org/abs/2311.00983) | 本文提出了一种基于决策导向学习的方法来解决库存配送问题，通过直接集成库存预测和路径优化，可能确保一个强大的供应链策略。 |
| [^3] | [In Search of Lost Online Test-time Adaptation: A Survey.](http://arxiv.org/abs/2310.20199) | 本文展示了在线测试时间适应性（OTTA）的调研结果，重点研究了解决模糊设置、过时骨干结构和不一致超参数调整的挑战，并提出了有效的策略和新的评估指标。 |
| [^4] | [Split-and-Denoise: Protect large language model inference with local differential privacy.](http://arxiv.org/abs/2310.09130) | 本文提出了一种名为SnD的创新框架，用于保护大型语言模型推断阶段的隐私。该方法通过在客户端上执行令牌嵌入层和引入噪声来优化隐私-效用权衡，无需修改模型参数。 |
| [^5] | [Hard View Selection for Contrastive Learning.](http://arxiv.org/abs/2310.03940) | 本文提出了一种Easy、无需学习但强大的Hard View Selection策略，通过选择更难的样本，提高了对比学习模型的性能。 |
| [^6] | [Q-Bench: A Benchmark for General-Purpose Foundation Models on Low-level Vision.](http://arxiv.org/abs/2309.14181) | Q-Bench是一个综合性的基准测试，用于评估多模态大型语言模型在低级别视觉感知和理解方面的能力。 |
| [^7] | [Solving Satisfiability Modulo Counting for Symbolic and Statistical AI Integration With Provable Guarantees.](http://arxiv.org/abs/2309.08883) | 我们提出了XOR-SMC，一个具有访问NP-oracles权限的多项式算法，以解决高度难解的可满足性模数问题，并具有恒定的近似保证。 |
| [^8] | [Information Processing by Neuron Populations in the Central Nervous System: Mathematical Structure of Data and Operations.](http://arxiv.org/abs/2309.02332) | 神经群体在中枢神经系统中使用数学结构精确地表示和操作信息，实现了特化、泛化、新奇检测等多种功能。 |
| [^9] | [Federated Two Stage Decoupling With Adaptive Personalization Layers.](http://arxiv.org/abs/2308.15821) | 该论文提出了一种具有自适应个性化层的联邦化两阶段解耦方法，通过将同质客户端聚类到同一组的方式来提高联邦学习的性能，并解决了数据异质性和聚类时间选择的问题。 |
| [^10] | [Distilled Feature Fields Enable Few-Shot Language-Guided Manipulation.](http://arxiv.org/abs/2308.07931) | 本论文通过精简特征场，将精确的3D几何与2D基础模型的丰富语义相结合，实现了对未见过的物体的少样本操作的泛化能力。 |
| [^11] | [When Foundation Model Meets Federated Learning: Motivations, Challenges, and Future Directions.](http://arxiv.org/abs/2306.15546) | 基础模型与联邦学习的交叉提供了解锁新可能性的独特机会，扩展了数据可用性，促进了协作式模型发展，并提高了性能和隐私保护。 |
| [^12] | [Can Large Language Models Infer Causation from Correlation?.](http://arxiv.org/abs/2306.05836) | 本文提出了一个新的任务（Corr2Cause），用于测量大型语言模型的因果推断能力，并通过实验发现这些模型在这个任务上表现很差。 |
| [^13] | [Addressing Negative Transfer in Diffusion Models.](http://arxiv.org/abs/2306.00354) | 本文从多任务学习角度出发，研究了扩散训练中的负迁移现象，提出了利用正则化技术增强扩散训练的方法，以减轻负迁移并提高去噪任务的性能。 |
| [^14] | [UDTIRI: An Open-Source Road Pothole Detection Benchmark Suite.](http://arxiv.org/abs/2304.08842) | 该论文介绍了一个开源的道路坑洞检测基准套件UDTIRI，包含了标记齐全的1000张道路坑洞图像，可以用于深度学习方法在城市道路检查中的目标检测、语义分割和实例分割任务。 |
| [^15] | [Precise localization of corneal reflections in eye images using deep learning trained on synthetic data.](http://arxiv.org/abs/2304.05673) | 该论文提出了一种使用深度学习在眼部图像中准确定位角膜反射的方法，无需对真实眼部图像进行注释，仅使用模拟数据进行训练，该方法表现出色且提供了一种可行的解决方案。 |
| [^16] | [Robust Decision-Focused Learning for Reward Transfer.](http://arxiv.org/abs/2304.03365) | 本文介绍了一种稳健决策重点（RDF）算法，利用非识别性的DF解，学习同时最大化期望回报和抵御奖励函数变化的模型，可以显著提高DF对奖励函数变化的稳健性，而不会降低智能体的总回报。 |
| [^17] | [Exploring AI-Generated Text in Student Writing: How Does AI Help?.](http://arxiv.org/abs/2304.02478) | 研究发现，在学生写作中使用人工智能生成文本有一定好处，但过度依赖此类工具也存在潜在风险。 |
| [^18] | [An EMO Joint Pruning with Multiple Sub-networks: Fast and Effect.](http://arxiv.org/abs/2303.16212) | 本文提出了一种基于多子网络的EMO联合剪枝算法，该算法可以减少空间和资源消耗。该算法采用分治的EMO网络剪枝框架，将整个网络上复杂的EMO剪枝任务分解为多个子网络上更简单的子任务。基于跨网络约束的子网络训练方法可以进一步提高性能。 |
| [^19] | [Unraveling the Connections between Privacy and Certified Robustness in Federated Learning Against Poisoning Attacks.](http://arxiv.org/abs/2209.04030) | 该论文研究了联邦学习中隐私和认证稳定性之间的联系，并探讨了如何利用差分隐私提供认证稳定性以及如何改进隐私以提高稳定性认证。 |
| [^20] | [Byzantines can also Learn from History: Fall of Centered Clipping in Federated Learning.](http://arxiv.org/abs/2208.09894) | 本文研究了中心化剪裁在面对不同恶意代理时的脆弱性，提出了一种称为多引用点剪裁 (MRPC) 的算法来解决这个问题。MRPC 框架利用多个参考点有效地中和专门设计的 Byzantine attacks。实验结果表明，在各种类型的 Byzantine attacks 下，MRPC 显著优于最先进的 FL 方法。 |
| [^21] | [SuperAnimal models pretrained for plug-and-play analysis of animal behavior.](http://arxiv.org/abs/2203.07436) | SuperAnimal是一种能够开发和部署深度学习模型的方法，不需要额外的人工标注和模型训练，同时可用于超过45个物种进行视频推断，并具有微调模型效率高的优点。 |

# 详细

[^1]: 使用混合语义学习的Sam引导增强细粒度编码的医学图像字幕生成

    Sam-Guided Enhanced Fine-Grained Encoding with Mixed Semantic Learning for Medical Image Captioning. (arXiv:2311.01004v1 [cs.CV])

    [http://arxiv.org/abs/2311.01004](http://arxiv.org/abs/2311.01004)

    本论文提出了一种使用混合语义学习的Sam引导增强细粒度编码的医学图像字幕生成方法，有效地捕捉了医学图像的详细特征，并在评估指标上优于预训练的BLIP2模型。

    

    随着多模态和大型语言模型的发展，基于深度学习的医学图像字幕生成技术具有提供有价值的诊断建议的潜力。然而，当前的通用文本和图像预训练模型在描述医学图像中的复杂细节时无法提供令人满意的结果。在本文中，我们提出了一种由段落任意模型（SAM）引导的新型医学图像字幕生成方法，以实现对一般和详细特征的增强编码。此外，我们的方法采用独特的混合语义学习预训练策略，同时捕捉医学图像的整体信息和更细节的细节。我们展示了这种方法的有效性，它在生成医学图像描述的各种评估指标上优于预训练的BLIP2模型。

    With the development of multimodality and large language models, the deep learning-based technique for medical image captioning holds the potential to offer valuable diagnostic recommendations. However, current generic text and image pre-trained models do not yield satisfactory results when it comes to describing intricate details within medical images. In this paper, we present a novel medical image captioning method guided by the segment anything model (SAM) to enable enhanced encoding with both general and detailed feature extraction. In addition, our approach employs a distinctive pre-training strategy with mixed semantic learning to simultaneously capture both the overall information and finer details within medical images. We demonstrate the effectiveness of this approach, as it outperforms the pre-trained BLIP2 model on various evaluation metrics for generating descriptions of medical images.
    
[^2]: 优化库存配送：一种基于神经网络的决策导向学习方法

    Optimizing Inventory Routing: A Decision-Focused Learning Approach using Neural Networks. (arXiv:2311.00983v1 [cs.LG])

    [http://arxiv.org/abs/2311.00983](http://arxiv.org/abs/2311.00983)

    本文提出了一种基于决策导向学习的方法来解决库存配送问题，通过直接集成库存预测和路径优化，可能确保一个强大的供应链策略。

    

    库存配送问题（IRP）是供应链管理中的一个关键挑战，它涉及在考虑库存需求规划的不确定性的情况下优化有效的路径选择。为了解决IRP问题，通常采用两阶段的方法，首先使用机器学习技术预测需求，然后使用优化算法来最小化配送成本。我们的实验表明，机器学习模型在实现完美准确度方面存在不足，因为库存水平受动态业务环境的影响，进而影响到下一阶段的优化问题，导致次优决策。在本文中，我们提出了一种基于决策导向学习的方法来解决现实世界的IRP问题。这种方法在一个端到端的系统中直接集成了库存预测和路径优化，可能确保一个强大的供应链策略。

    Inventory Routing Problem (IRP) is a crucial challenge in supply chain management as it involves optimizing efficient route selection while considering the uncertainty of inventory demand planning. To solve IRPs, usually a two-stage approach is employed, where demand is predicted using machine learning techniques first, and then an optimization algorithm is used to minimize routing costs. Our experiment shows machine learning models fall short of achieving perfect accuracy because inventory levels are influenced by the dynamic business environment, which, in turn, affects the optimization problem in the next stage, resulting in sub-optimal decisions. In this paper, we formulate and propose a decision-focused learning-based approach to solving real-world IRPs. This approach directly integrates inventory prediction and routing optimization within an end-to-end system potentially ensuring a robust supply chain strategy.
    
[^3]: 追寻失落的在线测试时间适应性：一项调研

    In Search of Lost Online Test-time Adaptation: A Survey. (arXiv:2310.20199v1 [cs.AI])

    [http://arxiv.org/abs/2310.20199](http://arxiv.org/abs/2310.20199)

    本文展示了在线测试时间适应性（OTTA）的调研结果，重点研究了解决模糊设置、过时骨干结构和不一致超参数调整的挑战，并提出了有效的策略和新的评估指标。

    

    本文详细调研了在线测试时间适应性（OTTA）的综合概况，该范式专注于在批量到达时将机器学习模型调整到新数据分布上。尽管最近OTTA方法的增加，但该领域存在模糊的设置、过时的骨干结构和不一致的超参数调整等问题，这使得真正的挑战变得难以复现。为了清晰和严格的比较，我们将OTTA技术分为三个主要类别，并使用功能强大的Vision Transformer（ViT）骨干架构对它们进行基准测试，以发现真正有效的策略。我们的基准测试不仅涵盖传统的受损数据集，如CIFAR-10/100-C和ImageNet-C，还包括体现在CIFAR-10.1和CIFAR-10-Warehouse中的现实世界转变，涵盖了搜索引擎的变化和扩散模型合成数据的变化。为了衡量在线场景中的效率，我们引入了新的评估指标。

    In this paper, we present a comprehensive survey on online test-time adaptation (OTTA), a paradigm focused on adapting machine learning models to novel data distributions upon batch arrival. Despite the proliferation of OTTA methods recently, the field is mired in issues like ambiguous settings, antiquated backbones, and inconsistent hyperparameter tuning, obfuscating the real challenges and making reproducibility elusive. For clarity and a rigorous comparison, we classify OTTA techniques into three primary categories and subject them to benchmarks using the potent Vision Transformer (ViT) backbone to discover genuinely effective strategies. Our benchmarks span not only conventional corrupted datasets such as CIFAR-10/100-C and ImageNet-C but also real-world shifts embodied in CIFAR-10.1 and CIFAR-10-Warehouse, encapsulating variations across search engines and synthesized data by diffusion models. To gauge efficiency in online scenarios, we introduce novel evaluation metrics, inclusiv
    
[^4]: 使用本地差分隐私保护大型语言模型推断：拆分与去噪

    Split-and-Denoise: Protect large language model inference with local differential privacy. (arXiv:2310.09130v1 [cs.AI])

    [http://arxiv.org/abs/2310.09130](http://arxiv.org/abs/2310.09130)

    本文提出了一种名为SnD的创新框架，用于保护大型语言模型推断阶段的隐私。该方法通过在客户端上执行令牌嵌入层和引入噪声来优化隐私-效用权衡，无需修改模型参数。

    

    大型语言模型（LLMs）通过捕捉向量空间中的隐藏语义，展示了在自然语言理解方面的强大能力。这一过程丰富了文本嵌入的价值，从而促进了作为服务（EaaS）的嵌入模型商业模式。然而，直接将文本传输到服务器面临着较大的隐私泄露风险，这是一个尚未得到有效解决的问题。为了缓解这个问题，我们引入了Split-N-Denoise（SnD），一种创新的框架，通过在客户端上以最小的计算成本执行令牌嵌入层来拆分模型。这使得客户端能够在将嵌入传输到服务器之前引入噪声，并随后接收和去噪后的扰动输出嵌入用于下游任务。我们的方法专为LLMs的推断阶段设计，不需要修改模型参数。广泛的实验证明了SnD在各种LLM中优化隐私-效用权衡方面的有效性。

    Large Language Models (LLMs) shows powerful capability in natural language understanding by capturing hidden semantics in vector space. This process enriches the value of the text embeddings for various downstream tasks, thereby fostering the Embedding-as-a-Service (EaaS) business model. However, the direct transmission of text to servers poses a largely unaddressed risk of privacy leakage. To mitigate this issue, we introduce Split-N-Denoise (SnD), an innovative framework that split the model to execute the token embedding layer on the client side at minimal computational cost. This allows the client to introduce noise prior to transmitting the embeddings to the server, and subsequently receive and denoise the perturbed output embeddings for downstream tasks. Our approach is designed for the inference stage of LLMs and requires no modifications to the model parameters. Extensive experiments demonstrate SnD's effectiveness in optimizing the privacy-utility tradeoff across various LLM a
    
[^5]: 对比学习的难视图选择

    Hard View Selection for Contrastive Learning. (arXiv:2310.03940v1 [cs.CV])

    [http://arxiv.org/abs/2310.03940](http://arxiv.org/abs/2310.03940)

    本文提出了一种Easy、无需学习但强大的Hard View Selection策略，通过选择更难的样本，提高了对比学习模型的性能。

    

    许多对比学习方法训练模型对图像输入的不同“视图”具有不变性，而一个好的数据增强流程对此至关重要。然而，大多数方法仍然依赖于对图像增强流程中的操作进行随机抽样，如随机裁剪或颜色扭曲操作。本文认为视图生成及其对性能的影响在目前研究中尚未得到足够的关注。为了解决这个问题，我们提出了一种易于实施但强大的“难视图选择”策略，该策略通过将训练过程中的随机视图生成扩展到更难的样本，提高了模型的性能。策略包括以下迭代步骤：1）随机选择多个视图并创建两个视图的配对，2）进行向前传递...

    Many Contrastive Learning (CL) methods train their models to be invariant to different "views" of an image input for which a good data augmentation pipeline is crucial. While considerable efforts were directed towards improving pre-text tasks, architectures, or robustness (e.g., Siamese networks or teacher-softmax centering), the majority of these methods remain strongly reliant on the random sampling of operations within the image augmentation pipeline, such as the random resized crop or color distortion operation. In this paper, we argue that the role of the view generation and its effect on performance has so far received insufficient attention. To address this, we propose an easy, learning-free, yet powerful Hard View Selection (HVS) strategy designed to extend the random view generation to expose the pretrained model to harder samples during CL training. It encompasses the following iterative steps: 1) randomly sample multiple views and create pairs of two views, 2) run forward pa
    
[^6]: Q-Bench: 一种用于低级别视觉通用基础模型的基准测试

    Q-Bench: A Benchmark for General-Purpose Foundation Models on Low-level Vision. (arXiv:2309.14181v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2309.14181](http://arxiv.org/abs/2309.14181)

    Q-Bench是一个综合性的基准测试，用于评估多模态大型语言模型在低级别视觉感知和理解方面的能力。

    

    多模态大型语言模型（MLLMs）的快速发展引发了计算机视觉从专门模型向通用基础模型的转变。然而，评估MLLMs在低级别视觉感知和理解方面的能力仍然不足。为了填补这一差距，我们提出了Q-Bench，这是一个综合性的基准测试，用于系统评估MLLMs在三个领域的潜在能力：低级别视觉感知、低级别视觉描述和整体视觉质量评估。

    The rapid evolution of Multi-modality Large Language Models (MLLMs) has catalyzed a shift in computer vision from specialized models to general-purpose foundation models. Nevertheless, there is still an inadequacy in assessing the abilities of MLLMs on low-level visual perception and understanding. To address this gap, we present Q-Bench, a holistic benchmark crafted to systematically evaluate potential abilities of MLLMs on three realms: low-level visual perception, low-level visual description, and overall visual quality assessment. a) To evaluate the low-level perception ability, we construct the LLVisionQA dataset, consisting of 2,990 diverse-sourced images, each equipped with a human-asked question focusing on its low-level attributes. We then measure the correctness of MLLMs on answering these questions. b) To examine the description ability of MLLMs on low-level information, we propose the LLDescribe dataset consisting of long expert-labelled golden low-level text descriptions o
    
[^7]: 解决符号和统计人工智能集成的可证明保证的可满足性模数问题

    Solving Satisfiability Modulo Counting for Symbolic and Statistical AI Integration With Provable Guarantees. (arXiv:2309.08883v1 [cs.AI])

    [http://arxiv.org/abs/2309.08883](http://arxiv.org/abs/2309.08883)

    我们提出了XOR-SMC，一个具有访问NP-oracles权限的多项式算法，以解决高度难解的可满足性模数问题，并具有恒定的近似保证。

    

    可满足性模数问题涵盖了需要符号决策和统计推理的问题。它的一般形式捕捉了符号和统计人工智能交叉领域的许多实际问题。可满足性模数问题寻找策略干预以控制概率性结果。解决可满足性模数问题具有高度难解的特性（$\text{NP}^{\text{PP}}$-complete），融合了统计推理和符号推理。先前对可满足性模数问题的研究缺乏可证明的保证和/或在组合约束存在时表现出次优的经验性能力。我们提出了XOR-SMC，一个具有访问NP-oracles权限的多项式算法，以解决高度难解的可满足性模数问题，并具有恒定的近似保证。XOR-SMC通过用随机的XOR约束替换SMC中的模型计数，将高度难解的SMC转化为可满足性问题。实验表明...

    Satisfiability Modulo Counting (SMC) encompasses problems that require both symbolic decision-making and statistical reasoning. Its general formulation captures many real-world problems at the intersection of symbolic and statistical Artificial Intelligence. SMC searches for policy interventions to control probabilistic outcomes. Solving SMC is challenging because of its highly intractable nature($\text{NP}^{\text{PP}}$-complete), incorporating statistical inference and symbolic reasoning. Previous research on SMC solving lacks provable guarantees and/or suffers from sub-optimal empirical performance, especially when combinatorial constraints are present. We propose XOR-SMC, a polynomial algorithm with access to NP-oracles, to solve highly intractable SMC problems with constant approximation guarantees. XOR-SMC transforms the highly intractable SMC into satisfiability problems, by replacing the model counting in SMC with SAT formulae subject to randomized XOR constraints. Experiments o
    
[^8]: 神经群体在中枢神经系统中的信息处理：数据和操作的数学结构

    Information Processing by Neuron Populations in the Central Nervous System: Mathematical Structure of Data and Operations. (arXiv:2309.02332v1 [q-bio.NC] CROSS LISTED)

    [http://arxiv.org/abs/2309.02332](http://arxiv.org/abs/2309.02332)

    神经群体在中枢神经系统中使用数学结构精确地表示和操作信息，实现了特化、泛化、新奇检测等多种功能。

    

    在哺乳动物中枢神经系统的复杂结构中，神经元形成群体。轴索束通过脉冲列作为媒介在这些群集之间进行通信。然而，这些神经群体的精确编码和操作还有待发现。在我们的分析中，出发点是一个具有可塑性的通用神经元的先进的机械模型。从这个简单的框架中出现了一个深刻的数学构造：通过有限凸锥的代数可以准确地描述信息的表示和操作。此外，这些神经群体不仅仅是被动传输者。它们在这个代数结构中扮演着运算符的角色，反映了低级编程语言的功能。当这些群体互连时，它们具有简洁而强大的代数表达式。这些网络使它们能够实现许多操作，如特化、泛化、新奇检测、维度降低等。

    In the intricate architecture of the mammalian central nervous system, neurons form populations. Axonal bundles communicate between these clusters using spike trains as their medium. However, these neuron populations' precise encoding and operations have yet to be discovered. In our analysis, the starting point is a state-of-the-art mechanistic model of a generic neuron endowed with plasticity. From this simple framework emerges a profound mathematical construct: The representation and manipulation of information can be precisely characterized by an algebra of finite convex cones. Furthermore, these neuron populations are not merely passive transmitters. They act as operators within this algebraic structure, mirroring the functionality of a low-level programming language. When these populations interconnect, they embody succinct yet potent algebraic expressions. These networks allow them to implement many operations, such as specialization, generalization, novelty detection, dimensiona
    
[^9]: 具有自适应个性化层的联邦化两阶段解耦

    Federated Two Stage Decoupling With Adaptive Personalization Layers. (arXiv:2308.15821v1 [cs.LG])

    [http://arxiv.org/abs/2308.15821](http://arxiv.org/abs/2308.15821)

    该论文提出了一种具有自适应个性化层的联邦化两阶段解耦方法，通过将同质客户端聚类到同一组的方式来提高联邦学习的性能，并解决了数据异质性和聚类时间选择的问题。

    

    由于分布式设备间的数据异质性，联邦学习在保持隐私约束的同时实现分布式学习的突破性能力引起了广泛关注。然而，由于数据异质性导致了显著的学习降级和慢收敛速度。因此，自然地采用将同质客户端聚类到同一组的概念，只允许在每个组内聚合模型权重。尽管大多数现有的聚类联邦学习方法采用模型梯度或推理输出作为客户端分区的度量标准，目的是将相似设备组合在一起，但每个聚类内部仍可能存在异质性。此外，缺乏研究探索确定聚类的适当时间的根本原因，导致常见做法是将每个客户端分配到其自己的独立聚类中，特别是在高度非ind情况下。

    Federated learning has gained significant attention due to its groundbreaking ability to enable distributed learning while maintaining privacy constraints. However, as a consequence of data heterogeneity among decentralized devices, it inherently experiences significant learning degradation and slow convergence speed. Therefore, it is natural to employ the concept of clustering homogeneous clients into the same group, allowing only the model weights within each group to be aggregated. While most existing clustered federated learning methods employ either model gradients or inference outputs as metrics for client partitioning, with the goal of grouping similar devices together, may still have heterogeneity within each cluster. Moreover, there is a scarcity of research exploring the underlying reasons for determining the appropriate timing for clustering, resulting in the common practice of assigning each client to its own individual cluster, particularly in the context of highly non ind
    
[^10]: 精简特征场使得语言引导的少样本操作成为可能

    Distilled Feature Fields Enable Few-Shot Language-Guided Manipulation. (arXiv:2308.07931v1 [cs.CV])

    [http://arxiv.org/abs/2308.07931](http://arxiv.org/abs/2308.07931)

    本论文通过精简特征场，将精确的3D几何与2D基础模型的丰富语义相结合，实现了对未见过的物体的少样本操作的泛化能力。

    

    自监督和语言监督的图像模型包含了世界的丰富知识，对于泛化很重要。然而，许多机器人任务需要对 3D 几何的详细理解，这在 2D 图像特征中往往缺乏。本研究通过利用精简特征场，将精确的 3D 几何与 2D 基础模型的丰富语义相结合，来弥合机器人操作中的 2D 到 3D 的差距。我们提出一种针对 6 自由度抓取和放置的少样本学习方法，利用这些强大的空间和语义先验，实现对未见过的物体的自然泛化。通过从视觉语言模型 CLIP 中精简的特征，我们展示了一种通过自由文本自然语言指定新颖对象进行操作的方式，并展示了它在未见过的表达和新颖类别的物体上的泛化能力。

    Self-supervised and language-supervised image models contain rich knowledge of the world that is important for generalization. Many robotic tasks, however, require a detailed understanding of 3D geometry, which is often lacking in 2D image features. This work bridges this 2D-to-3D gap for robotic manipulation by leveraging distilled feature fields to combine accurate 3D geometry with rich semantics from 2D foundation models. We present a few-shot learning method for 6-DOF grasping and placing that harnesses these strong spatial and semantic priors to achieve in-the-wild generalization to unseen objects. Using features distilled from a vision-language model, CLIP, we present a way to designate novel objects for manipulation via free-text natural language, and demonstrate its ability to generalize to unseen expressions and novel categories of objects.
    
[^11]: 当基础模型遇到联邦学习：动机、挑战和未来方向

    When Foundation Model Meets Federated Learning: Motivations, Challenges, and Future Directions. (arXiv:2306.15546v1 [cs.LG])

    [http://arxiv.org/abs/2306.15546](http://arxiv.org/abs/2306.15546)

    基础模型与联邦学习的交叉提供了解锁新可能性的独特机会，扩展了数据可用性，促进了协作式模型发展，并提高了性能和隐私保护。

    

    基础模型（FM）与联邦学习（FL）的交叉提供了相互的好处，在AI研究中提供了解锁新可能性的独特机会，解决了AI和现实世界应用中的关键挑战。FL扩展了FM的数据可用性，并实现了计算共享，分散了训练过程，并减轻了FL参与者的负担。它促进了协作式FM发展，民主化了这一过程，促进了包容性和创新。另一方面，FM以其庞大的规模、预训练的知识和出色的性能，为FL提供了一个强大的起点，促进了在非独立同分布数据下更快的收敛和更好的性能。此外，利用FM生成合成数据可以丰富数据多样性，减少过拟合，保护隐私。通过研究FL和FM之间的相互作用，本文旨在加深对它们协同关系的理解，强调动机和挑战。

    The intersection of the Foundation Model (FM) and Federated Learning (FL) provides mutual benefits, presents a unique opportunity to unlock new possibilities in AI research, and address critical challenges in AI and real-world applications. FL expands the availability of data for FMs and enables computation sharing, distributing the training process and reducing the burden on FL participants. It promotes collaborative FM development, democratizing the process and fostering inclusivity and innovation. On the other hand, FM, with its enormous size, pre-trained knowledge, and exceptional performance, serves as a robust starting point for FL, facilitating faster convergence and better performance under non-iid data. Additionally, leveraging FM to generate synthetic data enriches data diversity, reduces overfitting, and preserves privacy. By examining the interplay between FL and FM, this paper aims to deepen the understanding of their synergistic relationship, highlighting the motivations,
    
[^12]: 大型语言模型能否从相关性中推断出因果关系?

    Can Large Language Models Infer Causation from Correlation?. (arXiv:2306.05836v1 [cs.CL])

    [http://arxiv.org/abs/2306.05836](http://arxiv.org/abs/2306.05836)

    本文提出了一个新的任务（Corr2Cause），用于测量大型语言模型的因果推断能力，并通过实验发现这些模型在这个任务上表现很差。

    

    因果推断是人类智慧的标志之一。虽然CausalNLP领域近年来引起了广泛关注，但NLP中现有的因果推断数据集主要依赖于从经验知识（例如常识知识）中发现因果关系。在本文中，我们提出了第一个基准数据集，用于测试大型语言模型（LLM）的纯因果推断能力。具体而言，我们制定了一个新的任务Corr2Cause，它采用一组相关语句并确定变量之间的因果关系。我们策划了一个大规模的数据集，其中包含超过400K个样本，我们在其中评估了17个现有的LLMs。通过我们的实验，我们确定了LLMs在因果推断技能方面的一个关键缺陷，并表明这些模型在该任务上的表现几乎接近随机。当我们尝试通过微调将LLMs重新用于这种技能时，这种缺陷在某种程度上得到了缓解，但我们发现这些模型仍然失败了。

    Causal inference is one of the hallmarks of human intelligence. While the field of CausalNLP has attracted much interest in the recent years, existing causal inference datasets in NLP primarily rely on discovering causality from empirical knowledge (e.g., commonsense knowledge). In this work, we propose the first benchmark dataset to test the pure causal inference skills of large language models (LLMs). Specifically, we formulate a novel task Corr2Cause, which takes a set of correlational statements and determines the causal relationship between the variables. We curate a large-scale dataset of more than 400K samples, on which we evaluate seventeen existing LLMs. Through our experiments, we identify a key shortcoming of LLMs in terms of their causal inference skills, and show that these models achieve almost close to random performance on the task. This shortcoming is somewhat mitigated when we try to re-purpose LLMs for this skill via finetuning, but we find that these models still fa
    
[^13]: 解决扩散模型中的负迁移问题

    Addressing Negative Transfer in Diffusion Models. (arXiv:2306.00354v1 [cs.CV])

    [http://arxiv.org/abs/2306.00354](http://arxiv.org/abs/2306.00354)

    本文从多任务学习角度出发，研究了扩散训练中的负迁移现象，提出了利用正则化技术增强扩散训练的方法，以减轻负迁移并提高去噪任务的性能。

    

    基于扩散的生成模型在各个领域都取得了显著的成功。它在同时涵盖不同噪声水平的去噪任务上训练模型，代表了一种多任务学习（MTL）的形式。然而，从MTL的角度分析和改善扩散模型仍然未被充分探索。特别地，MTL有时会导致众所周知的$\textit{负迁移}$现象，这种现象是由于任务之间存在冲突而导致某些任务的性能降低。本文旨在从MTL的角度分析扩散训练，提出了两个关键观察：$\textbf{(O1)}$ 随着噪声水平之间的差距加大，去噪任务之间的任务亲和力减弱， $\textbf{(O2)}$ 在扩散训练的背景下，负迁移也可能会出现。基于这些观察结果，我们的目标是通过减轻负迁移来增强扩散训练。为了实现这一目标，我们提出了利用现有的MTL方法、具体是正则化技术，来鼓励任务特定的特征提取并减少任务干扰。实验结果表明，我们提出的方法有效地减轻了负迁移，提高了扩散模型在一系列去噪任务上的性能。

    Diffusion-based generative models have achieved remarkable success in various domains. It trains a model on denoising tasks that encompass different noise levels simultaneously, representing a form of multi-task learning (MTL). However, analyzing and improving diffusion models from an MTL perspective remains under-explored. In particular, MTL can sometimes lead to the well-known phenomenon of $\textit{negative transfer}$, which results in the performance degradation of certain tasks due to conflicts between tasks. In this paper, we aim to analyze diffusion training from an MTL standpoint, presenting two key observations: $\textbf{(O1)}$ the task affinity between denoising tasks diminishes as the gap between noise levels widens, and $\textbf{(O2)}$ negative transfer can arise even in the context of diffusion training. Building upon these observations, our objective is to enhance diffusion training by mitigating negative transfer. To achieve this, we propose leveraging existing MTL metho
    
[^14]: UDTIRI:一个开源的道路坑洞检测基准套件

    UDTIRI: An Open-Source Road Pothole Detection Benchmark Suite. (arXiv:2304.08842v1 [cs.CV])

    [http://arxiv.org/abs/2304.08842](http://arxiv.org/abs/2304.08842)

    该论文介绍了一个开源的道路坑洞检测基准套件UDTIRI，包含了标记齐全的1000张道路坑洞图像，可以用于深度学习方法在城市道路检查中的目标检测、语义分割和实例分割任务。

    

    看到在城市数字孪生领域中利用强大的深度学习方法的巨大潜力。特别是在智能道路检查领域，目前研究和数据有限。为了促进这一领域的进展，我们开发了一个名为Urban Digital Twins Intelligent Road Inspection (UDTIRI)数据集的标记齐全的道路坑洞数据集。我们希望这个数据集能够让强大的深度学习方法在城市道路检查中发挥作用，让算法更全面地理解场景并最大化其潜力。我们的数据集包括1000张道路坑洞图像，拍摄于不同的情境中，具有不同的光照和湿度条件。我们的意图是将这个数据集应用于目标检测、语义分割和实例分割任务。我们的团队花费了大量精力进行了详细的统计分析，并对UDTIRI数据集的一些代表性深度学习模型进行了基准测试。

    It is seen that there is enormous potential to leverage powerful deep learning methods in the emerging field of urban digital twins. It is particularly in the area of intelligent road inspection where there is currently limited research and data available. To facilitate progress in this field, we have developed a well-labeled road pothole dataset named Urban Digital Twins Intelligent Road Inspection (UDTIRI) dataset. We hope this dataset will enable the use of powerful deep learning methods in urban road inspection, providing algorithms with a more comprehensive understanding of the scene and maximizing their potential. Our dataset comprises 1000 images of potholes, captured in various scenarios with different lighting and humidity conditions. Our intention is to employ this dataset for object detection, semantic segmentation, and instance segmentation tasks. Our team has devoted significant effort to conducting a detailed statistical analysis, and benchmarking a selection of represent
    
[^15]: 使用深度学习在眼部图像中准确定位角膜反射

    Precise localization of corneal reflections in eye images using deep learning trained on synthetic data. (arXiv:2304.05673v1 [cs.CV])

    [http://arxiv.org/abs/2304.05673](http://arxiv.org/abs/2304.05673)

    该论文提出了一种使用深度学习在眼部图像中准确定位角膜反射的方法，无需对真实眼部图像进行注释，仅使用模拟数据进行训练，该方法表现出色且提供了一种可行的解决方案。

    

    我们提出了一种深度学习方法，用于准确地定位单个眼部图像中角膜反射的中心。与以往的方法不同，我们使用了一个纯粹使用模拟数据训练的卷积神经网络（CNN）。使用只有模拟数据的方法的好处是完全避开了需要对真实眼部图像进行监督训练的繁琐注释过程。为了系统地评估我们方法的准确性，我们首先对放置在不同背景中和嵌入不同噪声水平的图像进行了测试。其次，我们对从真实眼睛中拍摄的高质量视频测试了该方法。我们的方法在真实眼部图像上表现出色，在空间精度方面降低了35％，并在模拟图像方面以空间准确性与最先进的方法相当。我们得出结论，我们的方法提供了一种精确的角膜反射中心定位方法，并提供了一种解决方案。

    We present a deep learning method for accurately localizing the center of a single corneal reflection (CR) in an eye image. Unlike previous approaches, we use a convolutional neural network (CNN) that was trained solely using simulated data. Using only simulated data has the benefit of completely sidestepping the time-consuming process of manual annotation that is required for supervised training on real eye images. To systematically evaluate the accuracy of our method, we first tested it on images with simulated CRs placed on different backgrounds and embedded in varying levels of noise. Second, we tested the method on high-quality videos captured from real eyes. Our method outperformed state-of-the-art algorithmic methods on real eye images with a 35% reduction in terms of spatial precision, and performed on par with state-of-the-art on simulated images in terms of spatial accuracy.We conclude that our method provides a precise method for CR center localization and provides a solutio
    
[^16]: 奖励转移的稳健决策重点学习

    Robust Decision-Focused Learning for Reward Transfer. (arXiv:2304.03365v1 [cs.LG])

    [http://arxiv.org/abs/2304.03365](http://arxiv.org/abs/2304.03365)

    本文介绍了一种稳健决策重点（RDF）算法，利用非识别性的DF解，学习同时最大化期望回报和抵御奖励函数变化的模型，可以显著提高DF对奖励函数变化的稳健性，而不会降低智能体的总回报。

    

    最近，决策重点（Decision-focused，DF）的基于模型的强化学习被介绍为一种强有力的算法，它可以专注于学习最有利于获得高报酬的MDP动态。虽然这种方法通过专注于直接优化报酬来提高智能体的性能，但从MLE的角度来看，它学习的动力学不够准确，因此可能对奖励函数的变化很脆弱。在这项工作中，我们开发了稳健决策重点（RDF）算法，它利用DF解的非识别性，学习同时最大化期望回报和抵御奖励函数变化的模型。我们在各种玩具示例和医疗模拟器上展示了RDF显着增加了DF对奖励函数变化的稳健性，而不会降低智能体的总回报。

    Decision-focused (DF) model-based reinforcement learning has recently been introduced as a powerful algorithm which can focus on learning the MDP dynamics which are most relevant for obtaining high rewards. While this approach increases the performance of agents by focusing the learning towards optimizing for the reward directly, it does so by learning less accurate dynamics (from a MLE standpoint), and may thus be brittle to changes in the reward function. In this work, we develop the robust decision-focused (RDF) algorithm which leverages the non-identifiability of DF solutions to learn models which maximize expected returns while simultaneously learning models which are robust to changes in the reward function. We demonstrate on a variety of toy example and healthcare simulators that RDF significantly increases the robustness of DF to changes in the reward function, without decreasing the overall return the agent obtains.
    
[^17]: 探索学生写作中的人工智能生成文本：AI能起到什么作用？

    Exploring AI-Generated Text in Student Writing: How Does AI Help?. (arXiv:2304.02478v1 [cs.CL])

    [http://arxiv.org/abs/2304.02478](http://arxiv.org/abs/2304.02478)

    研究发现，在学生写作中使用人工智能生成文本有一定好处，但过度依赖此类工具也存在潜在风险。

    

    以英语作为外语的学生使用人工智能自然语言生成工具生成的文本可能会提高他们的写作质量。然而，目前尚不清楚这些学生的写作中使用人工智能生成文本在多大程度上会导致更高质量的写作。我们探索了23名香港中学生撰写故事（包含自己的文字和人工智能生成的文本）的尝试。人类专家对这些故事进行了内容、语言和组织方面的评分。我们分析了故事中的AI生成文本的基本组织结构和句法复杂度，并执行了多元线性回归和聚类分析。结果表明，人类词语的数量和人工智能生成词语的数量对分数有重要贡献。此外，与同龄人相比，学生的写作可以分为擅长和不擅长使用更多或更少人工智能生成文本的两组。聚类比较显示，使用人工智能生成文本在学生写作中有一定好处，但同时也强调了过度依赖这种工具的潜在风险。

    English as foreign language_EFL_students' use of text generated from artificial intelligence_AI_natural language generation_NLG_tools may improve their writing quality. However, it remains unclear to what extent AI-generated text in these students' writing might lead to higher-quality writing. We explored 23 Hong Kong secondary school students' attempts to write stories comprising their own words and AI-generated text. Human experts scored the stories for dimensions of content, language and organization. We analyzed the basic organization and structure and syntactic complexity of the stories' AI-generated text and performed multiple linear regression and cluster analyses. The results show the number of human words and the number of AI-generated words contribute significantly to scores. Besides, students can be grouped into competent and less competent writers who use more AI-generated text or less AI-generated text compared to their peers. Comparisons of clusters reveal some benefit of
    
[^18]: 基于多子网络的EMO联合剪枝：高效快速

    An EMO Joint Pruning with Multiple Sub-networks: Fast and Effect. (arXiv:2303.16212v1 [cs.LG])

    [http://arxiv.org/abs/2303.16212](http://arxiv.org/abs/2303.16212)

    本文提出了一种基于多子网络的EMO联合剪枝算法，该算法可以减少空间和资源消耗。该算法采用分治的EMO网络剪枝框架，将整个网络上复杂的EMO剪枝任务分解为多个子网络上更简单的子任务。基于跨网络约束的子网络训练方法可以进一步提高性能。

    

    基于进化多目标（EMO）的网络剪枝算法可以平衡网络的剪枝率和性能。然而，由于基于种群的特性，它经常受到复杂的剪枝优化空间和高度资源消耗的剪枝结构验证过程的限制，从而限制了它的应用。为此，本文提出了一种基于多子网络的EMO联合剪枝（EMO-PMS），以减少空间复杂度和资源消耗。首先，提出了一种分治的EMO网络剪枝框架，将整个网络上复杂的EMO剪枝任务分解为多个子网络上更简单的子任务。一方面，这种分解减少了剪枝优化空间并降低了优化难度；另一方面，较小的网络结构收敛更快，因此所提出的算法的计算资源消耗较低。其次，基于跨网络约束的子网络训练方法。

    The network pruning algorithm based on evolutionary multi-objective (EMO) can balance the pruning rate and performance of the network. However, its population-based nature often suffers from the complex pruning optimization space and the highly resource-consuming pruning structure verification process, which limits its application. To this end, this paper proposes an EMO joint pruning with multiple sub-networks (EMO-PMS) to reduce space complexity and resource consumption. First, a divide-and-conquer EMO network pruning framework is proposed, which decomposes the complex EMO pruning task on the whole network into easier sub-tasks on multiple sub-networks. On the one hand, this decomposition reduces the pruning optimization space and decreases the optimization difficulty; on the other hand, the smaller network structure converges faster, so the computational resource consumption of the proposed algorithm is lower. Secondly, a sub-network training method based on cross-network constraint
    
[^19]: 揭示隐私与认证稳定性之间在联邦学习中对抗毒化攻击中的联系

    Unraveling the Connections between Privacy and Certified Robustness in Federated Learning Against Poisoning Attacks. (arXiv:2209.04030v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2209.04030](http://arxiv.org/abs/2209.04030)

    该论文研究了联邦学习中隐私和认证稳定性之间的联系，并探讨了如何利用差分隐私提供认证稳定性以及如何改进隐私以提高稳定性认证。

    

    联邦学习（FL）提供了一种有效的范式，可以利用分布式用户的数据共同训练全局模型。由于本地训练数据来自不可信任的不同用户，一些研究表明FL容易受到毒化攻击。与此同时，为了保护本地用户的隐私，FL通常采用差分隐私的方式进行训练（DPFL）。因此，在本文中，我们提出以下问题：差分隐私和联邦学习中的认证稳定性之间存在何种联系？我们能否利用DPFL的隐私属性为FL提供认证稳定性？我们如何进一步改进FL的隐私以提高这种稳定性认证？我们首先研究FL的用户级和实例级隐私，并提供正式的隐私分析以实现提高实例级隐私。然后，我们提供了两个稳定性认证指标：认证预测和认证攻击无效性，用于用户和实例级DPFL的认证。

    Federated learning (FL) provides an efficient paradigm to jointly train a global model leveraging data from distributed users. As local training data comes from different users who may not be trustworthy, several studies have shown that FL is vulnerable to poisoning attacks. Meanwhile, to protect the privacy of local users, FL is usually trained in a differentially private way (DPFL). Thus, in this paper, we ask: What are the underlying connections between differential privacy and certified robustness in FL against poisoning attacks? Can we leverage the innate privacy property of DPFL to provide certified robustness for FL? Can we further improve the privacy of FL to improve such robustness certification? We first investigate both user-level and instance-level privacy of FL and provide formal privacy analysis to achieve improved instance-level privacy. We then provide two robustness certification criteria: certified prediction and certified attack inefficacy for DPFL on both user and i
    
[^20]: 拜占庭人也能从历史中学习：联邦学习中心化剪裁的衰落

    Byzantines can also Learn from History: Fall of Centered Clipping in Federated Learning. (arXiv:2208.09894v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.09894](http://arxiv.org/abs/2208.09894)

    本文研究了中心化剪裁在面对不同恶意代理时的脆弱性，提出了一种称为多引用点剪裁 (MRPC) 的算法来解决这个问题。MRPC 框架利用多个参考点有效地中和专门设计的 Byzantine attacks。实验结果表明，在各种类型的 Byzantine attacks 下，MRPC 显著优于最先进的 FL 方法。

    

    联邦学习 (FL) 框架由于在广泛的协作学习任务中的成功而越来越受欢迎，但也引起了某些安全问题。其中，拜占庭攻击的风险是特别关注的问题，这指的是恶意客户参与学习过程的可能性。因此，FL 中的一个关键目标是消除 Byzantine attacks 的潜在影响，确保最终模型是可信的。已经观察到，客户端的模型/更新之间的方差越大，隐藏 Byzantine attacks 的空间就越大。因此，通过使用动量，从而减少方差，可以削弱已知 Byzantine attacks 的力量。中心化剪裁 (CC) 框架进一步表明，上一次的动量项除了减少方差外，还可以作为一个参考点更好地消除 Byzantine attacks。在本文中，我们研究了在不同的恶意代理有不同目标时 CC 的脆弱性。我们提出了一种改进的剪裁算法称为多引用点剪裁 (MRPC)，以克服这种脆弱性。MRPC 框架有效地利用多个参考点来消除专门设计以绕过 CC 方法的 Byzantine attacks。实验结果表明，在各种类型的 Byzantine attacks 下，MRPC 显著优于最先进的 FL 方法。

    The increasing popularity of the federated learning (FL) framework due to its success in a wide range of collaborative learning tasks also induces certain security concerns. Among many vulnerabilities, the risk of Byzantine attacks is of particular concern, which refers to the possibility of malicious clients participating in the learning process. Hence, a crucial objective in FL is to neutralize the potential impact of Byzantine attacks, and to ensure that the final model is trustable. It has been observed that the higher the variance among the clients' models/updates, the more space there is for Byzantine attacks to be hidden. As a consequence, by utilizing momentum, and thus, reducing the variance, it is possible to weaken the strength of known Byzantine attacks. The centered clipping (CC) framework has further shown that, the momentum term from the previous iteration, besides reducing the variance, can be used as a reference point to neutralize Byzantine attacks better. In this wor
    
[^21]: 超级动物模型的预训练，用于动物行为的即插即用分析

    SuperAnimal models pretrained for plug-and-play analysis of animal behavior. (arXiv:2203.07436v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2203.07436](http://arxiv.org/abs/2203.07436)

    SuperAnimal是一种能够开发和部署深度学习模型的方法，不需要额外的人工标注和模型训练，同时可用于超过45个物种进行视频推断，并具有微调模型效率高的优点。

    

    行为量化在神经科学、兽医和动物保护等应用中至关重要。行为分析的关键步骤是首先提取与动物相关的关键点，即姿势估计。然而，可靠的姿势推断目前需要领域知识和手动标注来构建监督模型。我们提出了一系列技术创新，使一种名为SuperAnimal的新方法能够开发和部署深度学习模型，这些模型不需要额外的人工标注和模型训练。SuperAnimal允许对45多个物种进行视频推断，同时只使用两种全局动物姿势模型。如果需要微调模型，我们展示了SuperAnimal模型具有10倍的数据效率，并且胜过先前的迁移学习方法。此外，我们提供了一种新的视频自适应方法来执行无监督的视频细化，并且我们展示了我们的模型在行为分类中的实用性。

    Quantification of behavior is critical in applications ranging from neuroscience, veterinary medicine and animal conservation efforts. A common key step for behavioral analysis is first extracting relevant keypoints on animals, known as pose estimation. However, reliable inference of poses currently requires domain knowledge and manual labeling effort to build supervised models. We present a series of technical innovations that enable a new method, collectively called SuperAnimal, to develop and deploy deep learning models that require zero additional human labels and model training. SuperAnimal allows video inference on over 45 species with only two global classes of animal pose models. If the models need fine-tuning, we show SuperAnimal models are 10$\times$ more data efficient and outperform prior transfer learning approaches. Moreover, we provide a new video-adaptation method to perform unsupervised refinement of videos, and we illustrate the utility of our model in behavioral clas
    

