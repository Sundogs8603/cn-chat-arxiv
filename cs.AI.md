# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [CFGPT: Chinese Financial Assistant with Large Language Model.](http://arxiv.org/abs/2309.10654) | CFGPT是一个具有大型语言模型的中国金融助手，包括CFData用于预训练和监督微调，以及CFLLM用于处理金融文本，CFAPP用于实际金融应用。这个框架在金融领域的各个方面展现出了巨大的潜力。 |
| [^2] | [Towards Energy-Aware Federated Traffic Prediction for Cellular Networks.](http://arxiv.org/abs/2309.10645) | 本文提出了一种能够考虑准确性和能源消耗之间权衡的可持续性指标，针对蜂窝网络中的交通预测问题提出了面向能源感知的联合交通预测技术，通过联合学习的方式实现了高精度的预测。 |
| [^3] | [Geometric structure of Deep Learning networks and construction of global ${\mathcal L}^2$ minimizers.](http://arxiv.org/abs/2309.10639) | 本文提供了深度学习网络结构的几何解释，并且构建了全局最小化器族，该族能够全局最小化成本函数。此外，还确定了各种退化局部最小值。 |
| [^4] | [Exploring the Influence of Information Entropy Change in Learning Systems.](http://arxiv.org/abs/2309.10625) | 本研究探索了在深度学习系统中引入噪声对性能的影响，证明了特定噪声可以在降低任务复杂性的条件下提升深度架构的性能，通过实验证明了在大规模图像数据集中的显著性能提升。 |
| [^5] | [Large language models can accurately predict searcher preferences.](http://arxiv.org/abs/2309.10621) | 大型语言模型可以通过从真实用户那里获取高质量的第一方数据来准确预测搜索者的偏好。 |
| [^6] | [A Dynamic Linear Bias Incorporation Scheme for Nonnegative Latent Factor Analysis.](http://arxiv.org/abs/2309.10618) | 本文提出了一种动态线性偏置引入方案，以提高非负潜在因子分析（NLFA）模型对高维不完整数据的表示学习能力。 |
| [^7] | [Decentralized Online Learning in Task Assignment Games for Mobile Crowdsensing.](http://arxiv.org/abs/2309.10594) | 本文研究了移动群智感知系统中的任务分配问题，提出了一种新颖的结合匹配理论和在线学习的去中心化方法。该方法通过考虑移动单元的个体目标，同时在线学习移动单元的努力，解决了感知平台和移动单元之间的目标冲突和不确定性问题。创新的"无感知"机制提高了学习过程并减少了冲突。 |
| [^8] | [PDRL: Multi-Agent based Reinforcement Learning for Predictive Monitoring.](http://arxiv.org/abs/2309.10576) | 本文提出了一种基于多智能体强化学习的预测监控系统（PDRL），该系统可以在复杂环境中监测预测未来状态，并通过奖励策略来学习现有知识和最大化奖励。 |
| [^9] | [A multimodal deep learning architecture for smoking detection with a small data approach.](http://arxiv.org/abs/2309.10561) | 本文提出了一个多模态深度学习架构，利用少量训练数据可以以高准确率检测出各种媒体中的吸烟行为。 |
| [^10] | [A Neighbourhood-Aware Differential Privacy Mechanism for Static Word Embeddings.](http://arxiv.org/abs/2309.10551) | 该论文提出了一种邻域感知差分隐私机制，通过考虑预训练的词嵌入空间中单词的邻域来确定所需的最小噪声量，实验证明该机制在多个下游任务中优于其他机制，同时保证更高的隐私级别。 |
| [^11] | [Towards Generative Modeling of Urban Flow through Knowledge-enhanced Denoising Diffusion.](http://arxiv.org/abs/2309.10547) | 本文研究了一种新颖的城市流动生成问题，即为没有历史流动数据的地区生成动态城市流动。通过采用扩散模型和构建城市知识图谱，搭建了一个能够捕捉区域特征和城市环境等多种因素对城市流动影响的方法。 |
| [^12] | [Mean Absolute Directional Loss as a New Loss Function for Machine Learning Problems in Algorithmic Investment Strategies.](http://arxiv.org/abs/2309.10546) | 这项研究提出了一种新的损失函数(Mean Absolute Directional Loss，MADL)，用于解决在机器学习模型中应用于算法投资策略中的金融时间序列预测问题。通过在两种不同资产类别的数据上验证，MADL函数能够提供更好的超参数选择，并获得更有效的投资策略。 |
| [^13] | [Model Leeching: An Extraction Attack Targeting LLMs.](http://arxiv.org/abs/2309.10544) | 模型吸取是一种针对大型语言模型的提取攻击，能够将目标模型的任务特定知识提取到一个参数较少的模型中，并且具有较高的准确率和攻击成功率。 |
| [^14] | [OpenMSD: Towards Multilingual Scientific Documents Similarity Measurement.](http://arxiv.org/abs/2309.10539) | 这项研究开发并评估了多语言科学文档相似度测量模型，提出了第一个多语言科学文档数据集OpenMSD，利用该数据集训练了科学专门化语言模型，并探索了多种策略来提高模型性能。 |
| [^15] | [A Cognitively-Inspired Neural Architecture for Visual Abstract Reasoning Using Contrastive Perceptual and Conceptual Processing.](http://arxiv.org/abs/2309.10532) | 基于人类认知启发，我们提出了一种新的神经结构，用于解决视觉抽象推理任务。该架构通过追求感知和概念处理之间的一致性，采用迭代、自对比的学习过程来模拟人类的认知过程。实验证明，该网络在RAVEN数据集上实现了比之前所有模型更高的准确性，并且使用了最弱的归纳偏置。我们还指出了原始数据集中存在的类不平衡问题，并提出了解决方案。 |
| [^16] | [Visible and NIR Image Fusion Algorithm Based on Information Complementarity.](http://arxiv.org/abs/2309.10522) | 这篇论文提出了一种基于信息互补的可见光和近红外图像融合算法，该算法通过使用权重引导滤波器和引导滤波器来获取纹理和边缘信息，并通过差值图和夜间补偿来生成互补权重图，从而解决了当前可见光和近红外融合算法中存在的色彩失真和伪影问题。 |
| [^17] | [Partially-Specified Causal Simulations.](http://arxiv.org/abs/2309.10514) | 本文介绍了因果推断方法不当模拟设计的问题，并提出了一个满足期望的模拟框架PARCS，该框架合成了基于因果模型和可调参数的数据，使用户能够生成符合条件的数据来评估因果推断方法。 |
| [^18] | [A Configurable Library for Generating and Manipulating Maze Datasets.](http://arxiv.org/abs/2309.10498) | 这个论文介绍了一个可配置的库，用于生成和处理迷宫数据集，研究人员可以通过该库生成不同分布的迷宫数据集，并对生成参数和生成规则进行自定义控制。可以支持多种输出格式，适用于不同类型的模型。 |
| [^19] | [An Evaluation of GPT-4 on the ETHICS Dataset.](http://arxiv.org/abs/2309.10492) | 本研究评估了GPT-4在ETHICS数据集上的性能，结果表明GPT-4的表现优于之前的模型，表明AI伦理中与共同人类价值观的合作学习并不是一个难题。 |
| [^20] | [Fully automated landmarking and facial segmentation on 3D photographs.](http://arxiv.org/abs/2309.10472) | 本研究开发了一种基于深度学习的自动头部测量注释方法，使用三维面部立体摄影术进行了评估。该方法能够准确地进行地标标记和面部分割，避免了手动注释的耗时和容易出错的问题。 |
| [^21] | [Exploring the Dark Side of AI: Advanced Phishing Attack Design and Deployment Using ChatGPT.](http://arxiv.org/abs/2309.10463) | 本文探讨使用ChatGPT开发高级网络钓鱼攻击并自动进行大规模部署的可能性，突出了这些攻击的快速生成和与目标网站的密切相似性，强调了在网络钓鱼攻击中滥用人工智能的风险，以及对AI系统加强对策的必要性。 |
| [^22] | [Human-AI Interactions and Societal Pitfalls.](http://arxiv.org/abs/2309.10448) | 本研究研究了人工智能与人类互动中面临的同质化和偏见问题，提出了改善人工智能与人类互动的解决办法，实现个性化输出而不牺牲生产力。 |
| [^23] | [Exploring Self-Reinforcement for Improving Learnersourced Multiple-Choice Question Explanations with Large Language Models.](http://arxiv.org/abs/2309.10444) | 本文提出了一个自我强化大型语言模型框架，自动生成和评估学生生成的解释，用于改进学生资源共享中学生生成的多项选择题的解释质量。 |
| [^24] | [Rethinking Imitation-based Planner for Autonomous Driving.](http://arxiv.org/abs/2309.10443) | 本论文通过提供大规模实际数据集和标准化闭环基准测试，重新思考了基于模仿的自动驾驶规划器，并对关键特征和数据增强技术进行了研究，发现了模仿差距，并提出了强大的基线模型-PlanTF。 |
| [^25] | [Multi-Object Graph Affordance Network: Enabling Goal-Oriented Planning through Compound Object Affordances.](http://arxiv.org/abs/2309.10426) | 多对象图形可用性网络（MOGAN）模型化了复合对象的可用性，预测了将新对象放置在复合对象上的效果。在构建具有特定高度或属性的塔等任务中，我们使用基于搜索的规划找到具有适当可用性的对象堆叠操作的序列。我们的系统能够正确建模非常复杂的复合对象的可用性，并在模拟和真实环境中展示了其适用性。 |
| [^26] | [Functional requirements to mitigate the Risk of Harm to Patients from Artificial Intelligence in Healthcare.](http://arxiv.org/abs/2309.10424) | 提出了14个功能要求，人工智能系统可以通过实施这些要求来降低医疗领域人工智能可能带来的风险。 |
| [^27] | [Learning from Teaching Assistants to Program with Subgoals: Exploring the Potential for AI Teaching Assistants.](http://arxiv.org/abs/2309.10419) | 通过研究初学者在子目标学习环境中与AI助教的互动，我们发现生成型AI助教在初学者编程教育中具有与人类助教相当的实用性和效果。这为我们提供了设计和利用生成型AI助教的指导方针。 |
| [^28] | [Unsupervised Learning via Network-Aware Embeddings.](http://arxiv.org/abs/2309.10408) | 本研究提出了一种通过估计数值节点属性之间的网络距离来创建网络感知嵌入的无监督学习方法，解决了传统聚类方法难以处理复杂网络结构的问题。 |
| [^29] | [Exploiting Causality Signals in Medical Images: A Pilot Study with Empirical Results.](http://arxiv.org/abs/2309.10399) | 本研究提出了一种利用医学图像中的因果信号进行自动分类的新方法，通过模型化图像中一个部分特征的存在如何影响另一个部分特征的外观，改善了分类性能并产生了更稳健的预测，聚焦于图像中的相关部分。 |
| [^30] | [Adaptive questionnaires for facilitating patient data entry in clinical decision support systems: Methods and application to STOPP/START v2.](http://arxiv.org/abs/2309.10398) | 本研究提出了一种适应性问卷的设计与应用方法，用于简化临床决策支持系统中病人数据的录入过程，并解决了数据质量和可用性等问题。 |
| [^31] | [Graph Contrastive Learning Meets Graph Meta Learning: A Unified Method for Few-shot Node Tasks.](http://arxiv.org/abs/2309.10376) | COLA是一种统一的方法，整合了图对比学习和元学习的优势，用于少样本节点分类任务。它利用图扩增来识别语义相似的节点，从而使得能够在少样本情况下进行快速泛化。 |
| [^32] | [Generative AI vs. AGI: The Cognitive Strengths and Weaknesses of Modern LLMs.](http://arxiv.org/abs/2309.10371) | 本文考察了生成式AI（LLMs）作为认知系统的优势与劣势。研究发现，这些系统的实际弱点可以追溯到其基本认知架构的缺陷。逐步改进LLMs并不能实现人类水平的AGI。然而，研究和实验LLMs仍然有助于理解人类级AGI，并且LLMs可以成为人类级AGI架构的重要组成部分。 |
| [^33] | [Geometric structure of shallow neural networks and constructive ${\mathcal L}^2$ cost minimization.](http://arxiv.org/abs/2309.10370) | 本文提供了浅层神经网络的几何结构解释，并通过基于${\mathcal L}^2$代价最小化的构造方法获得了一个具有优越性能的网络。 |
| [^34] | [Toward efficient resource utilization at edge nodes in federated learning.](http://arxiv.org/abs/2309.10367) | 本论文提出了一种受迁移学习启发的策略，旨在通过减少设备上的资源利用、以及减少服务器和网络的负载，提高联邦学习中边缘节点的效率。 |
| [^35] | [OccluTrack: Rethinking Awareness of Occlusion for Enhancing Multiple Pedestrian Tracking.](http://arxiv.org/abs/2309.10360) | 本文提出了一种自适应遮挡感知多目标行人跟踪器OccluTrack，通过引入异常运动抑制机制、姿势导向的再识别模块和全局相关性优化方法，解决了多目标行人跟踪中遮挡导致的问题。 |
| [^36] | [Explaining Agent Behavior with Large Language Models.](http://arxiv.org/abs/2309.10346) | 使用大规模语言模型解释智能体行为的方法，通过学习智能体行为的紧凑表示，并与用户进行交互，能够生成合理的解释，具备与人类专家相似的帮助性。 |
| [^37] | [FedWOA: A Federated Learning Model that uses the Whale Optimization Algorithm for Renewable Energy Prediction.](http://arxiv.org/abs/2309.10337) | 本论文提出了一种名为FedWOA的新型联邦学习模型，该模型利用鲸鱼优化算法从家庭能源数据训练的局部LTSM神经网络模型的权重中聚合出全局预测模型，解决了数据异质性和参数数量增加导致的预测精度降低的问题。 |
| [^38] | [Learning based 2D Irregular Shape Packing.](http://arxiv.org/abs/2309.10329) | 这个论文提出了一种基于学习的二维不规则形状装箱方法，通过迭代选择和分组UV贴图补丁，将问题转化为装箱问题并使用联合优化来提高装箱比例。 |
| [^39] | [QASnowball: An Iterative Bootstrapping Framework for High-Quality Question-Answering Data Generation.](http://arxiv.org/abs/2309.10326) | QASnowball是一个迭代自举框架，可以根据有监督的样本种子集生成大规模高质量的QA数据，并通过重新种子化进行自我增强。在高资源英文场景中进行了实验。 |
| [^40] | [Metastatic Breast Cancer Prognostication Through Multimodal Integration of Dimensionality Reduction Algorithms and Classification Algorithms.](http://arxiv.org/abs/2309.10324) | 通过多模态集成降维算法和分类算法来预测转移性乳腺癌，提高转移性癌症的正确识别，并节省资源和时间。 |
| [^41] | [Who to Trust, How and Why: Untangling AI Ethics Principles, Trustworthiness and Trust.](http://arxiv.org/abs/2309.10318) | 人工智能信任不仅取决于系统本身，还包括对开发者的信任。人工智能伦理原则如可解释性和透明度对用户的信任影响尚不清晰。人工智能系统应被认识为社会技术系统，人与系统同样重要来确定其可信度。 |
| [^42] | [Investigating the Catastrophic Forgetting in Multimodal Large Language Models.](http://arxiv.org/abs/2309.10313) | 本论文针对多模态大规模语言模型中的灾难性遗忘问题进行研究，引入了EMT方法来评估灾难性遗忘，并发现在标准图像分类任务上，几乎所有评估的模型都无法保持与视觉编码器相同的性能水平。研究结果表明，早期微调阶段对性能至关重要。 |
| [^43] | [Leveraging Speech PTM, Text LLM, and Emotional TTS for Speech Emotion Recognition.](http://arxiv.org/abs/2309.10294) | 本文利用语音预训练模型、文本生成技术和语音合成技术提升了语音情感识别的效果，并通过实验证明了方法的有效性。 |
| [^44] | [QXAI: Explainable AI Framework for Quantitative Analysis in Patient Monitoring Systems.](http://arxiv.org/abs/2309.10293) | 该论文提出了一种QXAI框架，用于患者监测系统中的定量分析。该框架具有可解释性，对于决策者在医疗应用中基于非线性模型结果进行决策非常重要。 |
| [^45] | [Koopman Invertible Autoencoder: Leveraging Forward and Backward Dynamics for Temporal Modeling.](http://arxiv.org/abs/2309.10291) | Koopman可逆自编码器（KIA）是一种基于Koopman算子理论的机器学习模型，通过建模正向和反向动力学来提高长期预测的准确性。它能够有效地学习低维表示，并保证了正向和逆向操作的可逆性和一致性。 |
| [^46] | [FRAMU: Attention-based Machine Unlearning using Federated Reinforcement Learning.](http://arxiv.org/abs/2309.10283) | FRAMU是一种基于注意力和联邦强化学习的机器遗忘框架，通过自适应学习机制、隐私保护技术和优化策略，处理各种数据源并保持准确性和隐私，适应波动的数据环境，支持持续模型演进。 |
| [^47] | [Crowd-Aware Multi-Agent Pathfinding With Boosted Curriculum Reinforcement Learning.](http://arxiv.org/abs/2309.10275) | 该论文介绍了CRAMP，一种基于增强式课程强化学习的众包感知分散式路径规划方法，旨在解决拥挤环境下多智能体路径规划的困难。 |
| [^48] | [Using an Uncrewed Surface Vehicle to Create a Volumetric Model of Non-Navigable Rivers and Other Shallow Bodies of Water.](http://arxiv.org/abs/2309.10269) | 使用无人表面船舶将浅水体的水下地形图与数字地表图合并，创建了一种统一的体积模型，以获取关于非可航运河流和其他浅水体所能承载的水体积的数据。 |
| [^49] | [Correlation between morphological evolution of splashing drop and exerted impact force revealed by interpretation of explainable artificial intelligence.](http://arxiv.org/abs/2309.10266) | 本研究通过解释性人工智能分析，揭示了飞溅液滴形态演化与液滴对固体表面施加的冲击力之间的相关性，并发现权重矩阵元素的值与液滴形态的时间演化而变化。 |
| [^50] | [LLM Platform Security: Applying a Systematic Evaluation Framework to OpenAI's ChatGPT Plugins.](http://arxiv.org/abs/2309.10254) | 本文提出了一个框架，用于分析和改进当前和未来与插件集成的LLM平台的安全性、隐私和安全性。在应用框架于OpenAI的插件生态系统时，我们发现了一些具体证明了潜在问题的插件。 |
| [^51] | [GPTFUZZER : Red Teaming Large Language Models with Auto-Generated Jailbreak Prompts.](http://arxiv.org/abs/2309.10253) | GPTFUZZER是一种黑盒越狱模糊测试框架，自动生成用于红队测试大型语言模型的越狱模板。这种自动化方法避免了手工工程，并通过种子选择策略提高了效率。 |
| [^52] | [On Explicit Curvature Regularization in Deep Generative Models.](http://arxiv.org/abs/2309.10237) | 该论文提出了一种基于曲率的正则化方法，用于深度生成模型的学习。在任意数据流形的情况下，推导出了明确的坐标不变公式来计算内在和外在曲率。实验证明，基于曲率的正则化方法优于现有的自动编码器方法，其中内在曲率度量略优于外在曲率度量。 |
| [^53] | [Drive as You Speak: Enabling Human-Like Interaction with Large Language Models in Autonomous Vehicles.](http://arxiv.org/abs/2309.10228) | 本文介绍了一个框架，利用大型语言模型（LLMs）提升自动驾驶汽车的决策过程，实现与人类类似的交互。该框架通过整合LLMs的自然语言能力和上下文理解、专业工具使用等，为自动驾驶汽车提供个性化辅助、持续学习和透明决策，从而改变自动驾驶汽车的操作方式。 |
| [^54] | [Multi-level feature fusion network combining attention mechanisms for polyp segmentation.](http://arxiv.org/abs/2309.10219) | 提出了一种名为 MLFF-Net 的多层次特征融合网络，结合注意力机制实现了对息肉分割的改进。对编码器提取的特征进行了滤波和利用，并解决了特征融合引起的语义冲突和信息冗余问题。 |
| [^55] | [An Empirical Study of Attention Networks for Semantic Segmentation.](http://arxiv.org/abs/2309.10217) | 本研究对语义分割中的注意力网络进行了实证研究，分析了它们的计算复杂性和性能，总结了适用的方法。 |
| [^56] | [Safe POMDP Online Planning via Shielding.](http://arxiv.org/abs/2309.10216) | 通过计算屏蔽动作来实现安全的POMDP在线规划。四种屏蔽方法。 |
| [^57] | [Towards Effective Semantic OOD Detection in Unseen Domains: A Domain Generalization Perspective.](http://arxiv.org/abs/2309.10209) | 本研究提出了一个新的问题，即面向未知领域的语义OOD检测，通过引入域泛化正则化和OOD检测正则化策略，同时解决了协变量偏移和语义偏移问题。实验证明，该框架在三个标准域泛化基准上表现优于传统方法。 |
| [^58] | [Stabilizing RLHF through Advantage Model and Selective Rehearsal.](http://arxiv.org/abs/2309.10202) | 本论文提出了通过优势模型和选择性回放来稳定RLHF训练的两种创新方法，成功地解决了奖励欺骗和灾难性遗忘等不稳定性问题，并在实验中取得了更高的奖励得分和胜率。 |
| [^59] | [Graph-enabled Reinforcement Learning for Time Series Forecasting with Adaptive Intelligence.](http://arxiv.org/abs/2309.10186) | 本研究提出了一种使用图神经网络和强化学习监控的新方法，以更准确地预测时间序列数据。该方法能够克服深度学习的限制，并捕捉复杂的时序结构中的时间依赖关系，适用于医疗、交通和天气预测等领域。 |
| [^60] | [QoS-Aware Service Prediction and Orchestration in Cloud-Network Integrated Beyond 5G.](http://arxiv.org/abs/2309.10185) | 本论文提出了一个针对网络-云集成环境中的服务部署和资源分配的非线性规划模型，考虑了容量限制、动态用户和端到端延迟，并解决了超越5G中的服务连续性问题。 |
| [^61] | [Positive and Risky Message Assessment for Music Products.](http://arxiv.org/abs/2309.10182) | 这项研究提出了一个新的问题：如何评估音乐产品中的正面和风险信息。研究者提出了一个多任务预测模型，通过序数约束解决这个问题，并且取得了显著优于其他方法的结果。 |
| [^62] | [Double Deep Q-Learning-based Path Selection and Service Placement for Latency-Sensitive Beyond 5G Applications.](http://arxiv.org/abs/2309.10180) | 通过研究通信和计算资源分配的联合问题，本文提出了两种方法，即B&B-CCRA和WF-CCRA，以最小化总成本。 |
| [^63] | [Self-Sustaining Multiple Access with Continual Deep Reinforcement Learning for Dynamic Metaverse Applications.](http://arxiv.org/abs/2309.10177) | 本文研究了动态元宇宙应用中的多重接入问题，在采用自适应人工智能和深度强化学习的基础上，通过连续训练来实现自我维持的策略。该研究填补了当前文献中对于适应非稳态环境的代理机制问题的研究空白。 |
| [^64] | [One ACT Play: Single Demonstration Behavior Cloning with Action Chunking Transformers.](http://arxiv.org/abs/2309.10175) | 本研究提出了一种通过行为克隆学习任务的方法，只需要一次人类演示，并使用线性变换生成多样化的轨迹，成功完成了三个块操作任务。此外，还提出了一种新的暂存方法，用于动作块代理的行动预测。 |
| [^65] | [Asynchronous Perception-Action-Communication with Graph Neural Networks.](http://arxiv.org/abs/2309.10164) | 该论文提出了使用图神经网络实现异步感知-动作-通信的方法，解决了在大型机器人群体中协作和通信的挑战。现有的框架假设顺序执行，该方法是完全分散的，但在评估和部署方面仍存在一些限制。 |
| [^66] | [RadOnc-GPT: A Large Language Model for Radiation Oncology.](http://arxiv.org/abs/2309.10160) | RadOnc-GPT是一种专门用于放射肿瘤学的大型语言模型，通过先进的调整方法进行微调，在生成治疗方案、确定疗法和提供诊断描述等关键任务上具有显著改进，展示了利用领域特定知识进行微调的大型语言模型在高度专业化的医疗领域具有的潜力。 |
| [^67] | [Analysis of the Memorization and Generalization Capabilities of AI Agents: Are Continual Learners Robust?.](http://arxiv.org/abs/2309.10149) | 本文提出了一种新的连续学习框架，旨在在动态环境下实现鲁棒的泛化能力并保留过去的知识。通过使用有限容量的内存来保存先前观察到的环境信息，并从内存中采样数据点来获得对未知变化鲁棒的预测器。该分析展示了记忆和泛化之间的权衡，而实验证明了所提出的算法的优越性。 |
| [^68] | [Human Gait Recognition using Deep Learning: A Comprehensive Review.](http://arxiv.org/abs/2309.10144) | 这篇论文总结了使用深度学习进行人体步态识别的研究，探讨了其在不同环境中的应用、优势以及可能遇到的挑战。 |
| [^69] | [Efficient Low-Rank GNN Defense Against Structural Attacks.](http://arxiv.org/abs/2309.10136) | 提出了一种高效的低秩图神经网络（ELR-GNN）防御方法，通过学习低秩和稀疏的图结构抵御对抗攻击，并在更高的效率下实现有效的防御。 |
| [^70] | [GDM: Dual Mixup for Graph Classification with Limited Supervision.](http://arxiv.org/abs/2309.10134) | GDM是一种双重的mixup方法，利用图实例的功能和结构信息生成新的标记图样本。 |
| [^71] | [Adaptive Liquidity Provision in Uniswap V3 with Deep Reinforcement Learning.](http://arxiv.org/abs/2309.10129) | 本研究介绍了一个使用深度强化学习的方法，能够使Uniswap V3中的流动性提供者能够自适应地调整价格范围，以实现最大化利润和减轻市场风险。 |
| [^72] | [AR-TTA: A Simple Method for Real-World Continual Test-Time Adaptation.](http://arxiv.org/abs/2309.10109) | AR-TTA提出了一种简单的方法用于真实世界连续测试时间自适应。通过将内存缓冲区纳入自训练框架，并根据数据流的强度进行动态适应，提高了模型的稳定性。 |
| [^73] | [Data Formulator: AI-powered Concept-driven Visualization Authoring.](http://arxiv.org/abs/2309.10094) | Data Formulator是一种基于人工智能的可视化创作工具，采用概念绑定范式，通过将高级可视化意图和低级数据转换步骤分离，利用AI代理自动转换输入数据，并生成所需的可视化。 |
| [^74] | [Conformal Temporal Logic Planning using Large Language Models: Knowing When to Do What and When to Ask for Help.](http://arxiv.org/abs/2309.10092) | 本文提出了一个使用大型语言模型的一致时间逻辑规划方法，用于解决多个高级子任务的移动机器人运动规划问题。其中的一个关键挑战是如何以正确性的角度推理机器人计划与基于自然语言的逻辑任务的关系。 |
| [^75] | [Unified Coarse-to-Fine Alignment for Video-Text Retrieval.](http://arxiv.org/abs/2309.10091) | 提出了一种统一粗到细对齐模型UCoFiA，用于视频-文本检索，该模型能够在不同粒度级别上捕捉跨模态相似性信息，并通过交互式相似性聚合模块有效考虑不同视觉特征的重要性，最终解决了视频-文本检索中的精确匹配问题。 |
| [^76] | [HTEC: Human Transcription Error Correction.](http://arxiv.org/abs/2309.10089) | HTEC是一种用于人类转录错误修正的方法，包括错误检测和填充两个阶段，提出了一种综合的修正操作列表，并针对删除错误提出了四种新操作。 |
| [^77] | [GAME: Generalized deep learning model towards multimodal data integration for early screening of adolescent mental disorders.](http://arxiv.org/abs/2309.10077) | 本研究设计了一个广义深度学习模型GAME，通过整合多模态特征并采用新颖的关注机制，能够对青少年的心理状况进行高准确度的评估。研究发现每种模态都对心理障碍的筛查和共病状况有动态的贡献。 |
| [^78] | [Sex-based Disparities in Brain Aging: A Focus on Parkinson's Disease.](http://arxiv.org/abs/2309.10069) | 本研究旨在探讨帕金森病患者中性别在大脑衰老中的作用。通过分析373名帕金森病患者的大脑预测年龄差异，并按性别进行分层分析，发现性别差异会导致帕金森病的症状和进展速度的差异。 |
| [^79] | [Automatic Personalized Impression Generation for PET Reports Using Large Language Models.](http://arxiv.org/abs/2309.10066) | 本研究旨在使用fine-tuned大型语言模型实现自动个性化生成全身PET报告的准确印象。通过训练语言模型并引入阅读医生的身份信息，模型能够学习医生特定的报告风格。研究结果经过专家评估和核医学医生的质量评分认可，证明该方法在实践中具有潜在的应用价值。 |
| [^80] | [Toward collision-free trajectory for autonomous and pilot-controlled unmanned aerial vehicles.](http://arxiv.org/abs/2309.10064) | 该研究旨在实现无人机的无碰撞轨迹，并安全高效地将无人机交通管理系统与空中交通管理系统进行整合。 |
| [^81] | [Survey of Consciousness Theory from Computational Perspective.](http://arxiv.org/abs/2309.10063) | 本文调查了从计算角度出发，解释人类大脑中意识现象的多种理论。解开意识之谜是构建通用人工智能的重要一步。 |
| [^82] | [Evaluation of GPT-3 for Anti-Cancer Drug Sensitivity Prediction.](http://arxiv.org/abs/2309.10016) | 本研究评估了GPT-3在抗癌药物敏感性预测任务中的潜力，并发现药物的SMILES表示和细胞系的基因组突变特征对药物反应具有预测能力。这些结果有助于在精准肿瘤学中设计更有效的治疗方案。 |
| [^83] | [SYNDICOM: Improving Conversational Commonsense with Error-Injection and Natural Language Feedback.](http://arxiv.org/abs/2309.10015) | SYNDICOM是一种改进对话常识的方法，包含了一个常识对话数据集和一个基于自然语言反馈的模型，可用于训练对话应答生成模型。 |
| [^84] | [Looking through the past: better knowledge retention for generative replay in continual learning.](http://arxiv.org/abs/2309.10012) | 本文改进了不断学习环境中的生成回放方法，以在复杂场景下表现更好。通过蒸馏潜在空间、改善生成特征对齐和周期性生成以增强知识保留。 |
| [^85] | [Multi-Agent Deep Reinforcement Learning for Cooperative and Competitive Autonomous Vehicles using AutoDRIVE Ecosystem.](http://arxiv.org/abs/2309.10007) | 本研究提出了一个模块化和并行化的多智能体深度强化学习框架，在AutoDRIVE生态系统中培养合作与竞争行为。我们利用该生态系统开发了准确物理和逼真图形的数字孪生体，并使用它来训练和部署多智能体强化学习策略，实现了在自主车辆中的合作和竞争行为。 |
| [^86] | [The Optimized path for the public transportation of Incheon in South Korea.](http://arxiv.org/abs/2309.10006) | 本论文研究了基于乘客需求寻找韩国仁川市公交系统的最优路径，通过修改A*算法，在性能上优于其他常用路径规划算法，能够实时找到最短路径。 |
| [^87] | [OpenAI Cribbed Our Tax Example, But Can GPT-4 Really Do Tax?.](http://arxiv.org/abs/2309.09992) | GPT-4在处理税务方面存在问题，无法可靠地计算税务。 |
| [^88] | [Molecular Conformation Generation via Shifting Scores.](http://arxiv.org/abs/2309.09985) | 该论文提出了一种新颖的分子构象生成方法，通过将分子解离视为对其组成原子施加逐渐增大的力场，从而改变原子间距离的分布，该方法在分子构象生成方面取得了显著改进。 |
| [^89] | [Exploration and Comparison of Deep Learning Architectures to Predict Brain Response to Realistic Pictures.](http://arxiv.org/abs/2309.09983) | 本研究探索和比较了多种深度学习架构，用于预测大脑对真实图片的反应。最终发现，使用多个简单模型，每个模型专注于受试者大脑每个半球的每个ROI，可以获得更好的预测结果。 |
| [^90] | [Introspective Deep Metric Learning.](http://arxiv.org/abs/2309.09982) | 本文提出了一种内省式深度度量学习 (IDML) 框架，通过考虑图像中的不确定性，以更好地处理模糊图像，实现更鲁棒的训练。 |
| [^91] | [Code Representation Pre-training with Complements from Program Executions.](http://arxiv.org/abs/2309.09980) | 本论文提出了一种名为FuzzPretrain的方法，用于在代码表示预训练中探索由程序的测试用例揭示的动态信息，并解决从代码中直接学习功能语义的挑战。 |
| [^92] | [MindAgent: Emergent Gaming Interaction.](http://arxiv.org/abs/2309.09971) | MindAgent是一种新颖的基础设施，用于评估游戏互动中的规划和协调新能力，它可以在多智能体系统中协调完成复杂任务，并与人类玩家合作。研究还引入了一个新的游戏场景和相关基准。 |
| [^93] | [Bias of AI-Generated Content: An Examination of News Produced by Large Language Models.](http://arxiv.org/abs/2309.09825) | 这项研究调查了七个代表性大型语言模型生成的AI生成内容的偏见。研究发现这些模型生成的内容存在显著的性别和种族偏见。 |
| [^94] | [Harnessing Collective Intelligence Under a Lack of Cultural Consensus.](http://arxiv.org/abs/2309.09787) | 在缺乏文化共识的情况下，通过无限深度潜在结构的文化共识理论（iDLC-CCT）模型，扩展了文化共识理论（CCT）的能力，提高了对共识信念多样性的建模能力。 |
| [^95] | [How to Data in Datathons.](http://arxiv.org/abs/2309.09770) | 本文提供了关于如何处理数据马拉松中的数据的指导方针和建议，通过10个案例研究验证了提出的框架的有效性。 |
| [^96] | [A Quantum Optimization Case Study for a Transport Robot Scheduling Problem.](http://arxiv.org/abs/2309.09736) | 本研究通过比较D-Waves的量子-经典混合框架、富士通的量子启发式数字退火器和Gurobi的经典求解器的性能，提供了解决运输机器人调度问题的指导，发现数字退火器有希望的结果，并为混合量子退火器提供了一些机会。 |
| [^97] | [LLM4Jobs: Unsupervised occupation extraction and standardization leveraging Large Language Models.](http://arxiv.org/abs/2309.09708) | 本文介绍了LLM4Jobs无监督的职业提取和规范化方法，通过利用大型语言模型，它展现了超越最新基准的灵活性和多功能性。此外，本研究还提供了合成和真实数据集，可用于相关研究。研究结果表明，现代语言模型有望为复杂的职业提取和规范化任务提供强有力的基础。 |
| [^98] | [A performance characteristic curve for model evaluation: the application in information diffusion prediction.](http://arxiv.org/abs/2309.09537) | 本研究提出了一种模型的性能特征曲线，用于评估其在不同复杂度任务中的表现。通过使用基于信息熵的度量方法，我们确定了随机性与模型预测准确性之间的关系，并发现不同条件下的数据点都可以合并成一条曲线，捕捉了模型在面对不确定性时的正确预测能力。 |
| [^99] | [FedGKD: Unleashing the Power of Collaboration in Federated Graph Neural Networks.](http://arxiv.org/abs/2309.09517) | FedGKD是一种新颖的联邦图神经网络框架，通过利用客户端图数据集蒸馏方法提取更好的任务特征并引入感知全局协作结构的服务器端聚合机制，解决了联邦GNN系统中图异构性问题，提高了效率和准确性。 |
| [^100] | [Exploring and Learning in Sparse Linear MDPs without Computationally Intractable Oracles.](http://arxiv.org/abs/2309.09457) | 本文研究了在稀疏线性MDP中探索和学习的问题，通过特征选择提出了一个多项式时间算法，以在与环境的交互中学习出近似最优策略。 |
| [^101] | [GenDOM: Generalizable One-shot Deformable Object Manipulation with Parameter-Aware Policy.](http://arxiv.org/abs/2309.09051) | GenDOM是一个框架，通过条件化操作策略和多样化模拟训练，使操作策略能够仅通过一个真实世界演示来处理不同的可变形物体，并通过最小化真实世界演示和模拟之间的点云格密度差异来估计新物体的可变形物体参数。 |
| [^102] | [TextBind: Multi-turn Interleaved Multimodal Instruction-following.](http://arxiv.org/abs/2309.08637) | TextBind是一个注释极少的框架，用于将较大规模的语言模型赋予多轮交错多模态指令跟随能力，并通过图像-标题对生成多轮多模态指令-回应对话。这个框架对于解决实际任务具有重要意义，并为未来的研究提供了数据集、模型和演示。 |
| [^103] | [Representation Learning in Low-rank Slate-based Recommender Systems.](http://arxiv.org/abs/2309.08622) | 该论文提出了在推荐系统中使用低秩MDP将推荐问题视为在线RL问题，并通过提出的算法实现了高效的表示学习。 |
| [^104] | [BEA: Revisiting anchor-based object detection DNN using Budding Ensemble Architecture.](http://arxiv.org/abs/2309.08036) | 本文提出了一种新型简化的集合结构BEA用于锚点目标检测模型，并通过改进置信度得分的校准与降低不确定性误差的损失函数，提高了模型准确性。 |
| [^105] | [The Rise and Potential of Large Language Model Based Agents: A Survey.](http://arxiv.org/abs/2309.07864) | 基于大型语言模型的代理的崛起和潜力：一项调查。大型语言模型被认为是构建通用人工智能代理的潜在催化剂，许多研究已经取得重要进展。 |
| [^106] | [Traveling Words: A Geometric Interpretation of Transformers.](http://arxiv.org/abs/2309.07315) | 本文提出了一种几何视角来解释变压器的内部机制，主要贡献在于阐明了层归一化如何限制潜在特征并在超球面上塑造注意力机制，通过探测预训练的GPT-2模型验证了该视角的有效性，并提供了对变压器的直观理解。 |
| [^107] | [Safe and Accelerated Deep Reinforcement Learning-based O-RAN Slicing: A Hybrid Transfer Learning Approach.](http://arxiv.org/abs/2309.07265) | 本文提出了一种混合迁移学习方法，用于解决在O-RAN网络中使用DRL算法进行闭环控制时遇到的收敛速度慢和性能不稳定的问题。 |
| [^108] | [An Empirical Study of NetOps Capability of Pre-Trained Large Language Models.](http://arxiv.org/abs/2309.05557) | 本文通过对预训练大型语言模型（LLMs）进行系统评估，发现LLMs在网络运维（NetOps）领域具有强大的潜力应用，能够提升自动化和智能化的NetOps能力。 |
| [^109] | [FOLLOWUPQG: Towards Information-Seeking Follow-up Question Generation.](http://arxiv.org/abs/2309.05007) | 本文引入了一项真实世界的信息获取跟进问题生成任务，通过生成跟进问题来更深入地理解初始问题和答案。构建了数据集FOLLOWUPQG，评估了当前的问题生成模型在生成跟进问题方面的效果，并展示了其作为一个具有挑战性的基准任务的验证。 |
| [^110] | [Characterizing Lipschitz Stability of GNN for Fairness.](http://arxiv.org/abs/2309.03648) | 论文通过研究GNN的Lipschitz界限表征了GNN对公平性稳定性的影响，尤其是在处理非欧几里得数据和固有偏倚的情况下。同时，该研究对于限制GNN输出的扰动以保障公平性训练提出了挑战。 |
| [^111] | [A Survey on Privacy in Graph Neural Networks: Attacks, Preservation, and Applications.](http://arxiv.org/abs/2308.16375) | 这篇综述调查了图神经网络中的隐私问题，包括攻击、保护方法以及应用领域。研究人员着重总结了攻击类型、隐私保护技术分类以及可用于分析和解决GNNs中隐私问题的数据集和应用，同时提出了未来研究的方向，以构建更好的隐私保护GNNs。 |
| [^112] | [Complementing Onboard Sensors with Satellite Map: A New Perspective for HD Map Construction.](http://arxiv.org/abs/2308.15427) | 本研究通过补充卫星地图，增强了车载传感器构建高精度地图的方法，利用卫星地图的广阔覆盖能力。我们释放了卫星地图瓦片作为nuScenes数据集的补充，同时提出了一个分层融合模块来更好地融合车载传感器与卫星地图的信息。 |
| [^113] | [Evaluation and Analysis of Hallucination in Large Vision-Language Models.](http://arxiv.org/abs/2308.15126) | 本文提出了基于大型语言模型的幻觉评估框架HaELM，可以评估大型视觉语言模型中的幻觉问题，并分析了导致幻觉的因素，并提出了缓解幻觉问题的建议。 |
| [^114] | [A multiobjective continuation method to compute the regularization path of deep neural networks.](http://arxiv.org/abs/2308.12044) | 本文提出了一种多目标延续方法，用于计算深度神经网络的正则化路径，以解决DNNs中稀疏性和数值效率之间的冲突。 |
| [^115] | [Semantic RGB-D Image Synthesis.](http://arxiv.org/abs/2308.11356) | 本文提出了语义RGB-D图像合成方法，用于解决RGB-D语义图像分割训练集缺乏多样性的问题。通过生成逼真的RGB-D图像来实现给定语义标签图的合成。本文的主要创新是提出了一种生成器，可以处理多模态数据，将与模态无关的信息与与模态相关的信息分离开来。 |
| [^116] | [FusionPlanner: A Multi-task Motion Planner for Mining Trucks using Multi-sensor Fusion Method.](http://arxiv.org/abs/2308.06931) | 本研究提出了一种面向无人运输的综合方案，包括FusionPlanner运动规划算法、MiningNav评估工具和Parallel Mining Simulator模拟器，以应对露天采矿复杂环境下的运输挑战。 |
| [^117] | [Why We Don't Have AGI Yet.](http://arxiv.org/abs/2308.03598) | 本论文探讨了为什么尚未实现人工通用智能（AGI），并指出了纯粹的统计方法和资金推广不足是制约AGI发展的原因之一，同时还分析了实现人类适应能力和自主学习所需的关键认知能力。 |
| [^118] | [VQGraph: Graph Vector-Quantization for Bridging GNNs and MLPs.](http://arxiv.org/abs/2308.02117) | VQGraph是一个框架，通过学习一个强大的图形表示空间，用于连接GNN和MLPs。它采用矢量量化变分自编码器（VQ-VAE）的编码器作为结构感知图标记器，有效地表示底层图的多样化局部结构。通过 VQGraph，可以实现从GNN到MLP的知识转移。 |
| [^119] | [Hierarchical Skeleton Meta-Prototype Contrastive Learning with Hard Skeleton Mining for Unsupervised Person Re-Identification.](http://arxiv.org/abs/2307.12917) | 本文提出了一种无监督的层次骨架元-原型对比学习（Hi-MPC）方法，结合硬骨架挖掘，用于无标签3D骨架的人物重新识别。通过构建层次骨架表示并利用元-原型对比学习进行特征提取和聚类，实现了更多信息丰富的骨架特征的利用。 |
| [^120] | [Extended Graph Assessment Metrics for Graph Neural Networks.](http://arxiv.org/abs/2307.10112) | 本论文提出了扩展的图评估指标（GAMs），适用于回归任务和连续邻接矩阵。主要关注的两个GAMs是同质性和跨类邻域相似度（CCNS）。这些扩展的指标能够在图神经网络中评估图结构，提高模型性能。 |
| [^121] | [Creating a Dataset Supporting Translation Between OpenMP Fortran and C++ Code.](http://arxiv.org/abs/2307.07686) | 本研究创建了一个数据集，用于训练机器学习模型在OpenMP Fortran和C++代码之间进行翻译。这个数据集通过精细的代码相似性测试确保了可靠性和适用性，并且能够显著提升大规模语言模型的翻译能力。 |
| [^122] | [Weakly-supervised positional contrastive learning: application to cirrhosis classification.](http://arxiv.org/abs/2307.04617) | 本研究提出了一种弱监督定位对比学习策略，在医学应用中使用大量弱标签图像进行肝硬化预测。这种策略将每个2D切片的空间上下文和弱标签整合起来，通过使用通用基于核的损失函数实现高效学习。 |
| [^123] | [Recent Advancements in End-to-End Autonomous Driving using Deep Learning: A Survey.](http://arxiv.org/abs/2307.04370) | 本文调研了深度学习在端到端自动驾驶中的最新进展，并提供了一种基于神经网络的自动驾驶任务分类体系。研究分析了端到端自动驾驶的关键挑战，并列举了不同的研究方法和核心功能。 |
| [^124] | [ElectroCardioGuard: Preventing Patient Misidentification in Electrocardiogram Databases through Neural Networks.](http://arxiv.org/abs/2306.06196) | 本研究提出了一种基于神经网络的小而高效模型，用于确定两个心电图(ECG)是否来自同一患者。同时，还提出了一种方法，利用该模型检测记录-分配错误，实现了在现实场景中的应用，并在新的ECG数据集上进行了评估。 |
| [^125] | [ChatGPT Informed Graph Neural Network for Stock Movement Prediction.](http://arxiv.org/abs/2306.03763) | 该研究介绍了一种新的框架，利用ChatGPT技术增强图神经网络，能够从财经新闻中提取出不断变化的网络结构，并用于股票价格预测，获得了超过基于深度学习的最新基准的表现，提示了ChatGPT在文本推断和金融预测方面的潜力。 |
| [^126] | [ChatGraph: Interpretable Text Classification by Converting ChatGPT Knowledge to Graphs.](http://arxiv.org/abs/2305.03513) | ChatGraph通过将ChatGPT的知识转换为图形，提高了文本分类的可解释性和性能 |
| [^127] | [A preferential interpretation of MultiLayer Perceptrons in a conditional logic with typicality.](http://arxiv.org/abs/2305.00304) | 本文探究了缺陷推理的多优选语义和多层神经网络模型之间的关系，并利用提出的多优先语义，对多层感知器(MLPs)进行了优先解释，并验证了其条件属性。 |
| [^128] | [Conformal Off-Policy Evaluation in Markov Decision Processes.](http://arxiv.org/abs/2304.02574) | 本论文提出了一种基于合规预测的异策评估方法，能够以一定的确定性水平输出包含目标策略的真实奖励的区间，并提出了不同的处理分布偏移方法，其中一些方法在保证合规性的前提下实现了最先进的性能。 |
| [^129] | [Probe: Learning Users' Personalized Projection Bias in Intertemporal Bundle Choices.](http://arxiv.org/abs/2303.06016) | 本文提出了一种新的偏差嵌入式偏好模型——Probe，旨在解决用户在时间跨度的购物选择中的投影偏差和参照点效应，提高决策的有效性和个性化。 |
| [^130] | [Elliptic PDE learning is provably data-efficient.](http://arxiv.org/abs/2302.12888) | 该论文提供了椭圆型偏微分方程学习中的数据效率理论保证，通过利用随机数值线性代数和PDE理论，实现了对于3D均匀椭圆型PDE解算符的数据高效恢复，并在训练数据集大小上以指数收敛率的误差。 |
| [^131] | [Algorithmic Hallucinations of Near-Surface Winds: Statistical Downscaling with Generative Adversarial Networks to Convection-Permitting Scales.](http://arxiv.org/abs/2302.08720) | 本论文将新兴的图像超分辨率技术应用于统计降尺度任务，具体探索了基于生成对抗网络的算法在模拟北美地区地表风中的应用。通过使用非理想化的低分辨率和高分辨率输入数据，该方法克服了共享尺度不匹配的问题，并通过评估空间功率谱等指标来评估模型的技能。 |
| [^132] | [Generalization through Diversity: Improving Unsupervised Environment Design.](http://arxiv.org/abs/2301.08025) | 这项研究改进了无监督环境设计，通过训练RL智能体适应各种环境变化，提高了在分布外测试场景上的性能。 |
| [^133] | [Graphical House Allocation.](http://arxiv.org/abs/2301.01323) | 这篇论文探讨了图形房屋分配问题，将个体放置在图的顶点上，开发了几种结构和计算方法，以最小化个体之间的嫉妒程度作为公平目标。 |
| [^134] | [Towards AI-Empowered Crowdsourcing.](http://arxiv.org/abs/2212.14676) | 本文调查了如何将人工智能赋能于众包以提高效率的方法，提出了以人工智能为驱动的众包（AIEC）的分类法，并讨论了其中的限制因素和挑战。 |
| [^135] | [RouteNet-Fermi: Network Modeling with Graph Neural Networks.](http://arxiv.org/abs/2212.12070) | RouteNet-Fermi是一种基于图神经网络的自定义模型，与传统的排队理论相比，在存在真实流量模型的情况下具有更高的准确性。它能够准确预测网络的延迟、抖动和丢包情况。 |
| [^136] | [Unsupervised Unlearning of Concept Drift with Autoencoders.](http://arxiv.org/abs/2211.12989) | 本论文提出了一种基于自编码器的无监督的概念漂移取消学习方法，通过在全局级别引入自编码器，可以避免重新训练或调整学习模型，从而实现对概念漂移的适应。 |
| [^137] | [Why people judge humans differently from machines: The role of perceived agency and experience.](http://arxiv.org/abs/2210.10081) | 本研究探讨了人们为什么在对人类和机器的判断上存在差异，并发现当人们认为机器具有更多代理能力时，他们对机器的评判更像对待人类的方式。 |
| [^138] | [Decoupling Knowledge from Memorization: Retrieval-augmented Prompt Learning.](http://arxiv.org/abs/2205.14704) | 本论文提出了一种检索增强的提示学习方法，通过将知识从记忆中解耦，帮助模型在泛化和记忆之间取得平衡。 |
| [^139] | [Explainable Deep Learning Methods in Medical Image Classification: A Survey.](http://arxiv.org/abs/2205.04766) | 这项调查提供了关于可解释深度学习方法在医学图像分类中的应用的全面概述，包括各种解释方法的比较和性能评估。 |
| [^140] | [Relation Extraction as Open-book Examination: Retrieval-enhanced Prompt Tuning.](http://arxiv.org/abs/2205.02355) | 提出了一种新的半参数学习范式，即检索增强的提示调优，用于关系抽取。通过构建开放式存储库，并使用线性插值的方式，模型能够在推断过程中根据存储库中的记忆信息推断关系。 |
| [^141] | [Contrastive Demonstration Tuning for Pre-trained Language Models.](http://arxiv.org/abs/2204.04392) | 本论文提出了一种名为对比演示调优的方法，可以在低数据场景下有效激发预训练语言模型的能力。实验结果表明，该方法与先前的提示调优方法相结合可以取得更好的性能。 |
| [^142] | [Artificial Intelligence and Statistical Collusion.](http://arxiv.org/abs/2202.05946) | 本研究提出了一个可处理的模型来研究学习算法之间的战略互动，揭示了一种导致算法勾结出现的机制。通过自发耦合，算法周期性地协调行动，达到更高利润。该模型的参数可预测统计关联的出现和有利于算法勾结的市场结构，进一步展示了自发耦合如何在价格和市场份额上维持勾结，并应用于设计算法市场。 |
| [^143] | [D-HAN: Dynamic News Recommendation with Hierarchical Attention Network.](http://arxiv.org/abs/2112.10085) | D-HAN是一种动态新闻推荐模型，采用分层注意力网络将连续时间信息无缝整合，有效表示新闻信息，并引入动态负采样方法优化用户的隐式反馈。 |
| [^144] | [Bridging the Gap between Spatial and Spectral Domains: A Unified Framework for Graph Neural Networks.](http://arxiv.org/abs/2107.10234) | 该论文提出了一种统一框架，用于将基于空间和谱域的图神经网络进行整合，并紧密关联各自域内的方法。 |

# 详细

[^1]: CFGPT: 具有大型语言模型的中国金融助手

    CFGPT: Chinese Financial Assistant with Large Language Model. (arXiv:2309.10654v1 [cs.CL])

    [http://arxiv.org/abs/2309.10654](http://arxiv.org/abs/2309.10654)

    CFGPT是一个具有大型语言模型的中国金融助手，包括CFData用于预训练和监督微调，以及CFLLM用于处理金融文本，CFAPP用于实际金融应用。这个框架在金融领域的各个方面展现出了巨大的潜力。

    

    大型语言模型（LLM）已经在金融领域的自然语言处理任务中展现出巨大的潜力。在这项工作中，我们提出了一个名为CFGPT的中国金融生成式预训练Transformer框架，该框架包括用于预训练和监督微调的数据集（CFData），用于熟练处理金融文本的金融LLM（CFLLM），以及用于实际金融应用的部署框架（CFAPP）。CFData包括一个预训练数据集和一个监督微调数据集，其中预训练数据集汇集了中国金融数据和分析，以及总共584M个文件和141B个标记的较小的通用文本子集，并且监督微调数据集针对六个不同的金融任务进行了定制，内容涵盖了金融分析和决策的各个方面，包括1.5M个指令对和总计1.5B个标记。CFLLM基于InternLM-7B进行了平衡模型能力的调整。

    Large language models (LLMs) have demonstrated great potential in natural language processing tasks within the financial domain. In this work, we present a Chinese Financial Generative Pre-trained Transformer framework, named CFGPT, which includes a dataset~(CFData) for pre-training and supervised fine-tuning, a financial LLM~(CFLLM) to adeptly manage financial texts, and a deployment framework~(CFAPP) designed to navigate real-world financial applications. The CFData comprising both a pre-training dataset and a supervised fine-tuning dataset, where the pre-training dataset collates Chinese financial data and analytics, alongside a smaller subset of general-purpose text with 584M documents and 141B tokens in total, and the supervised fine-tuning dataset is tailored for six distinct financial tasks, embodying various facets of financial analysis and decision-making with 1.5M instruction pairs and 1.5B tokens in total. The CFLLM, which is based on InternLM-7B to balance the model capabil
    
[^2]: 面向能源感知的联合交通预测技术在蜂窝网络中的应用

    Towards Energy-Aware Federated Traffic Prediction for Cellular Networks. (arXiv:2309.10645v1 [cs.LG])

    [http://arxiv.org/abs/2309.10645](http://arxiv.org/abs/2309.10645)

    本文提出了一种能够考虑准确性和能源消耗之间权衡的可持续性指标，针对蜂窝网络中的交通预测问题提出了面向能源感知的联合交通预测技术，通过联合学习的方式实现了高精度的预测。

    

    蜂窝网络中的交通预测对于优化第五代(5G)及更高版本的网络至关重要，准确的预测对于智能网络设计、资源分配和异常缓解是必不可少的。虽然机器学习(ML)是一种有效预测网络流量的方法，但大规模数据的集中存储在单个数据中心中，涉及到机密性、隐私和数据传输的问题。为了解决这些挑战，联合学习(FL)作为一种吸引人的ML训练框架应运而生，通过并行分布式计算提供高精度的预测。然而，这些方法的环境影响常常被忽视，这引发了对于其可持续性的质疑。本文通过提出一种新的可持续性指标，解决了联合学习中准确性和能源消耗之间的折衷问题，并对最先进的方法进行了全面评估。

    Cellular traffic prediction is a crucial activity for optimizing networks in fifth-generation (5G) networks and beyond, as accurate forecasting is essential for intelligent network design, resource allocation and anomaly mitigation. Although machine learning (ML) is a promising approach to effectively predict network traffic, the centralization of massive data in a single data center raises issues regarding confidentiality, privacy and data transfer demands. To address these challenges, federated learning (FL) emerges as an appealing ML training framework which offers high accurate predictions through parallel distributed computations. However, the environmental impact of these methods is often overlooked, which calls into question their sustainability. In this paper, we address the trade-off between accuracy and energy consumption in FL by proposing a novel sustainability indicator that allows assessing the feasibility of ML models. Then, we comprehensively evaluate state-of-the-art d
    
[^3]: 深度学习网络的几何结构和全局${\mathcal L}^2$最小化器的构建

    Geometric structure of Deep Learning networks and construction of global ${\mathcal L}^2$ minimizers. (arXiv:2309.10639v1 [cs.LG])

    [http://arxiv.org/abs/2309.10639](http://arxiv.org/abs/2309.10639)

    本文提供了深度学习网络结构的几何解释，并且构建了全局最小化器族，该族能够全局最小化成本函数。此外，还确定了各种退化局部最小值。

    

    本文提供了对深度学习（DL）网络结构的几何解释，该网络具有$L$个隐藏层，斜坡激活函数，${\mathcal L}^2$ Schatten类（或Hilbert-Schmidt）成本函数，以及相等维度$Q\geq1$的输入和输出空间${\mathbb R}^Q$。隐藏层也定义在${\mathbb R}^{Q}$的空间上。我们利用我们最新的关于浅层神经网络的结果，在$L\geq Q$的情况下构造了一个明确的最小化器族，该族能够全局最小化成本函数，并且我们证明这个族是退化的。在这里提到的上下文中，DL网络的隐藏层通过对训练输入的递归截断映射的应用来“整理”训练输入，以最小化噪声与信号的比率。此外，我们确定了$2^Q-1$个不同的退化局部最小值。

    In this paper, we provide a geometric interpretation of the structure of Deep Learning (DL) networks, characterized by $L$ hidden layers, a ramp activation function, an ${\mathcal L}^2$ Schatten class (or Hilbert-Schmidt) cost function, and input and output spaces ${\mathbb R}^Q$ with equal dimension $Q\geq1$. The hidden layers are defined on spaces ${\mathbb R}^{Q}$, as well. We apply our recent results on shallow neural networks to construct an explicit family of minimizers for the global minimum of the cost function in the case $L\geq Q$, which we show to be degenerate. In the context presented here, the hidden layers of the DL network "curate" the training inputs by recursive application of a truncation map that minimizes the noise to signal ratio of the training inputs. Moreover, we determine a set of $2^Q-1$ distinct degenerate local minima of the cost function.
    
[^4]: 探索学习系统中信息熵变化的影响

    Exploring the Influence of Information Entropy Change in Learning Systems. (arXiv:2309.10625v1 [cs.AI])

    [http://arxiv.org/abs/2309.10625](http://arxiv.org/abs/2309.10625)

    本研究探索了在深度学习系统中引入噪声对性能的影响，证明了特定噪声可以在降低任务复杂性的条件下提升深度架构的性能，通过实验证明了在大规模图像数据集中的显著性能提升。

    

    在本研究中，我们通过向输入/隐含特征添加噪声来探索深度学习系统中熵变化的影响。本文的应用重点是计算机视觉中的深度学习任务，但所提出的理论可以进一步应用于其他领域。噪声通常被视为各种深度学习架构（如卷积神经网络和视觉变换器）以及图像分类和迁移学习等不同学习任务中的有害扰动。然而，本文旨在重新思考传统命题是否总是成立。我们证明了在特定条件下，特定噪声可以提升各种深度架构的性能。我们在信息熵定义的任务复杂性减少方面从理论上证明了正噪声的增强效果，并在大规模图像数据集（如ImageNet）中实验证明了显著的性能提升。

    In this work, we explore the influence of entropy change in deep learning systems by adding noise to the inputs/latent features. The applications in this paper focus on deep learning tasks within computer vision, but the proposed theory can be further applied to other fields. Noise is conventionally viewed as a harmful perturbation in various deep learning architectures, such as convolutional neural networks (CNNs) and vision transformers (ViTs), as well as different learning tasks like image classification and transfer learning. However, this paper aims to rethink whether the conventional proposition always holds. We demonstrate that specific noise can boost the performance of various deep architectures under certain conditions. We theoretically prove the enhancement gained from positive noise by reducing the task complexity defined by information entropy and experimentally show the significant performance gain in large image datasets, such as the ImageNet. Herein, we use the informat
    
[^5]: 大型语言模型能够准确预测搜索者的偏好

    Large language models can accurately predict searcher preferences. (arXiv:2309.10621v1 [cs.IR])

    [http://arxiv.org/abs/2309.10621](http://arxiv.org/abs/2309.10621)

    大型语言模型可以通过从真实用户那里获取高质量的第一方数据来准确预测搜索者的偏好。

    

    相关性标签是评估和优化搜索系统的关键。获取大量相关性标签通常需要第三方标注人员，但存在低质量数据的风险。本论文介绍了一种改进标签质量的替代方法，通过从真实用户那里获得仔细反馈来获取高质量的第一方数据。

    Relevance labels, which indicate whether a search result is valuable to a searcher, are key to evaluating and optimising search systems. The best way to capture the true preferences of users is to ask them for their careful feedback on which results would be useful, but this approach does not scale to produce a large number of labels. Getting relevance labels at scale is usually done with third-party labellers, who judge on behalf of the user, but there is a risk of low-quality data if the labeller doesn't understand user needs. To improve quality, one standard approach is to study real users through interviews, user studies and direct feedback, find areas where labels are systematically disagreeing with users, then educate labellers about user needs through judging guidelines, training and monitoring. This paper introduces an alternate approach for improving label quality. It takes careful feedback from real users, which by definition is the highest-quality first-party gold data that 
    
[^6]: 非负潜在因子分析的动态线性偏置引入方案

    A Dynamic Linear Bias Incorporation Scheme for Nonnegative Latent Factor Analysis. (arXiv:2309.10618v1 [cs.AI])

    [http://arxiv.org/abs/2309.10618](http://arxiv.org/abs/2309.10618)

    本文提出了一种动态线性偏置引入方案，以提高非负潜在因子分析（NLFA）模型对高维不完整数据的表示学习能力。

    

    高维度和不完整（HDI）数据在大数据相关应用中很常见，比如社交网络服务系统，这些系统涉及与众多节点之间的有限交互。从HDI数据中获取知识是数据科学领域的一个重要问题，因为HDI数据中嵌入了丰富的模式，如节点行为。非负潜在因子分析（NLFA）模型已被证明在解决这个问题方面具有优势，在这个模型中，线性偏置引入（LBI）方案对于处理训练过度摆动、波动以及防止模型过早收敛非常重要。然而，现有的LBI方案都是统计型的，线性偏置是固定的，这严重限制了产生的NLFA模型的可扩展性，并导致了对HDI数据的表示学习能力的损失。在以上发现的基础上，本文创新地提出了一种动态线性偏置引入方案。

    High-Dimensional and Incomplete (HDI) data is commonly encountered in big data-related applications like social network services systems, which are concerning the limited interactions among numerous nodes. Knowledge acquisition from HDI data is a vital issue in the domain of data science due to their embedded rich patterns like node behaviors, where the fundamental task is to perform HDI data representation learning. Nonnegative Latent Factor Analysis (NLFA) models have proven to possess the superiority to address this issue, where a linear bias incorporation (LBI) scheme is important in present the training overshooting and fluctuation, as well as preventing the model from premature convergence. However, existing LBI schemes are all statistic ones where the linear biases are fixed, which significantly restricts the scalability of the resultant NLFA model and results in loss of representation learning ability to HDI data. Motivated by the above discoveries, this paper innovatively pres
    
[^7]: 移动群智感知中的任务分配游戏中的去中心化在线学习

    Decentralized Online Learning in Task Assignment Games for Mobile Crowdsensing. (arXiv:2309.10594v1 [cs.SI])

    [http://arxiv.org/abs/2309.10594](http://arxiv.org/abs/2309.10594)

    本文研究了移动群智感知系统中的任务分配问题，提出了一种新颖的结合匹配理论和在线学习的去中心化方法。该方法通过考虑移动单元的个体目标，同时在线学习移动单元的努力，解决了感知平台和移动单元之间的目标冲突和不确定性问题。创新的"无感知"机制提高了学习过程并减少了冲突。

    

    本文研究了移动群智感知系统中协调数据收集的问题。移动群智感知平台逐步发布感知任务给可用移动单元，并通过回传感知报价来表示它们参与任务的意愿。根据所收到的报价，感知平台决定任务分配。稳定的任务分配需要解决两个挑战：感知平台和移动单元之间的目标冲突，以及对移动单元所需努力和偏好的不确定性。为了克服这些挑战，提出了一种结合匹配理论和在线学习的新颖去中心化方法，称为"碰撞避免多臂老虎机-策略无感知"（CA-MAB-SFS）。将任务分配问题建模为考虑感知平台和移动单元的个体目标的匹配游戏，而移动单元在线学习其努力。我们的创新的"无感知"机制显著提高了移动单元的学习过程，同时减少了冲突。

    The problem of coordinated data collection is studied for a mobile crowdsensing (MCS) system. A mobile crowdsensing platform (MCSP) sequentially publishes sensing tasks to the available mobile units (MUs) that signal their willingness to participate in a task by sending sensing offers back to the MCSP. From the received offers, the MCSP decides the task assignment. A stable task assignment must address two challenges: the MCSP's and MUs' conflicting goals, and the uncertainty about the MUs' required efforts and preferences. To overcome these challenges a novel decentralized approach combining matching theory and online learning, called collision-avoidance multi-armed bandit with strategic free sensing (CA-MAB-SFS), is proposed. The task assignment problem is modeled as a matching game considering the MCSP's and MUs' individual goals while the MUs learn their efforts online. Our innovative "free-sensing" mechanism significantly improves the MU's learning process while reducing collision
    
[^8]: PDRL：基于多智能体强化学习的预测监控

    PDRL: Multi-Agent based Reinforcement Learning for Predictive Monitoring. (arXiv:2309.10576v1 [cs.LG])

    [http://arxiv.org/abs/2309.10576](http://arxiv.org/abs/2309.10576)

    本文提出了一种基于多智能体强化学习的预测监控系统（PDRL），该系统可以在复杂环境中监测预测未来状态，并通过奖励策略来学习现有知识和最大化奖励。

    

    强化学习因其能够从以往经验中学习并做出自适应决策的能力，在监控应用中越来越被应用。然而，现有的基于机器学习的健康监控应用大多是基于监督学习算法，训练标签数据，无法在不确定的复杂环境中做出自适应决策。本研究提出了一种新颖的通用系统，即具有多个强化学习智能体的预测深度强化学习（PDRL），应用于时间序列预测环境。该提出的通用框架可以容纳虚拟深度 Q 网络（DQN）智能体，以监测复杂环境的预测未来状态，并根据明确定义的奖励策略使智能体在最大化奖励的同时学习现有知识。在评估该框架的过程中，部署了三个强化学习智能体以监测通过 BiLSTM 模型预测的受试者未来的心率、呼吸率和体温。随着每次迭代，智能体根据实际反馈调整其行为策略。

    Reinforcement learning has been increasingly applied in monitoring applications because of its ability to learn from previous experiences and can make adaptive decisions. However, existing machine learning-based health monitoring applications are mostly supervised learning algorithms, trained on labels and they cannot make adaptive decisions in an uncertain complex environment. This study proposes a novel and generic system, predictive deep reinforcement learning (PDRL) with multiple RL agents in a time series forecasting environment. The proposed generic framework accommodates virtual Deep Q Network (DQN) agents to monitor predicted future states of a complex environment with a well-defined reward policy so that the agent learns existing knowledge while maximizing their rewards. In the evaluation process of the proposed framework, three DRL agents were deployed to monitor a subject's future heart rate, respiration, and temperature predicted using a BiLSTM model. With each iteration, t
    
[^9]: 一个多模态深度学习架构用于小样本方法下的吸烟检测

    A multimodal deep learning architecture for smoking detection with a small data approach. (arXiv:2309.10561v1 [cs.CV])

    [http://arxiv.org/abs/2309.10561](http://arxiv.org/abs/2309.10561)

    本文提出了一个多模态深度学习架构，利用少量训练数据可以以高准确率检测出各种媒体中的吸烟行为。

    

    引言：隐蔽的烟草广告常常引起监管措施。本文提出，人工智能尤其是深度学习在检测隐藏广告中具有巨大潜力，并可以对烟草相关媒体内容进行无偏见、可重复性和公正的量化。方法：我们提出了一种基于深度学习、生成方法和人工强化的综合文本和图像处理模型，可以在文本和视觉格式下检测吸烟案例，即使有很少的可用训练数据。结果：我们的模型在图像上可以达到74%的准确率，在文本上可以达到98%的准确率。此外，我们的系统还集成了专家干预的可能性，以人工强化的形式。结论：使用深度学习提供的预训练多模态、图像和文本处理模型，即使有少量的训练数据，也可以检测出不同媒体中的吸烟行为。

    Introduction: Covert tobacco advertisements often raise regulatory measures. This paper presents that artificial intelligence, particularly deep learning, has great potential for detecting hidden advertising and allows unbiased, reproducible, and fair quantification of tobacco-related media content. Methods: We propose an integrated text and image processing model based on deep learning, generative methods, and human reinforcement, which can detect smoking cases in both textual and visual formats, even with little available training data. Results: Our model can achieve 74\% accuracy for images and 98\% for text. Furthermore, our system integrates the possibility of expert intervention in the form of human reinforcement. Conclusions: Using the pre-trained multimodal, image, and text processing models available through deep learning makes it possible to detect smoking in different media even with few training data.
    
[^10]: 用于静态词嵌入的邻域感知差分隐私机制

    A Neighbourhood-Aware Differential Privacy Mechanism for Static Word Embeddings. (arXiv:2309.10551v1 [cs.LG])

    [http://arxiv.org/abs/2309.10551](http://arxiv.org/abs/2309.10551)

    该论文提出了一种邻域感知差分隐私机制，通过考虑预训练的词嵌入空间中单词的邻域来确定所需的最小噪声量，实验证明该机制在多个下游任务中优于其他机制，同时保证更高的隐私级别。

    

    我们提出了一种考虑预训练的静态词嵌入空间中单词邻域的邻域感知差分隐私（NADP）机制，以确定保证指定隐私级别所需的最小噪声量。我们首先使用它们的嵌入构建单词的最近邻图，并将其分解为一组连通分量（即邻域）。然后，在每个邻域中根据该邻域中的单词集合分别对单词应用不同水平的高斯噪声。实验表明，我们提出的NADP机制在多个下游任务中始终优于多个先前提出的DP机制，如拉普拉斯、高斯和马氏距离，同时保证更高的隐私级别。

    We propose a Neighbourhood-Aware Differential Privacy (NADP) mechanism considering the neighbourhood of a word in a pretrained static word embedding space to determine the minimal amount of noise required to guarantee a specified privacy level. We first construct a nearest neighbour graph over the words using their embeddings, and factorise it into a set of connected components (i.e. neighbourhoods). We then separately apply different levels of Gaussian noise to the words in each neighbourhood, determined by the set of words in that neighbourhood. Experiments show that our proposed NADP mechanism consistently outperforms multiple previously proposed DP mechanisms such as Laplacian, Gaussian, and Mahalanobis in multiple downstream tasks, while guaranteeing higher levels of privacy.
    
[^11]: 通过知识增强去噪扩散实现城市流动的生成模型

    Towards Generative Modeling of Urban Flow through Knowledge-enhanced Denoising Diffusion. (arXiv:2309.10547v1 [cs.AI])

    [http://arxiv.org/abs/2309.10547](http://arxiv.org/abs/2309.10547)

    本文研究了一种新颖的城市流动生成问题，即为没有历史流动数据的地区生成动态城市流动。通过采用扩散模型和构建城市知识图谱，搭建了一个能够捕捉区域特征和城市环境等多种因素对城市流动影响的方法。

    

    尽管生成人工智能在许多领域取得了成功，但其对地理空间数据的建模能力仍未被充分探索。城市流动作为一种典型的地理空间数据，在各种城市应用中至关重要。现有的研究大多集中在对城市流动进行预测建模，即基于历史流动数据预测未来流动情况，但这些数据在数据稀疏的地区或新规划的地区可能无法获取。其他一些研究旨在预测地区之间的OD流量，但未能对城市流动随时间动态变化进行建模。在这项工作中，我们研究了一种新颖的城市流动生成问题，即为没有历史流动数据的地区生成动态城市流动。为了捕捉区域特征和城市环境等多种因素对城市流动的影响，我们采用扩散模型为不同条件下的地区生成城市流动。首先，我们构建了一个城市知识图谱（UKG）来建模城市环境和地区之间的关系。

    Although generative AI has been successful in many areas, its ability to model geospatial data is still underexplored. Urban flow, a typical kind of geospatial data, is critical for a wide range of urban applications. Existing studies mostly focus on predictive modeling of urban flow that predicts the future flow based on historical flow data, which may be unavailable in data-sparse areas or newly planned regions. Some other studies aim to predict OD flow among regions but they fail to model dynamic changes of urban flow over time. In this work, we study a new problem of urban flow generation that generates dynamic urban flow for regions without historical flow data. To capture the effect of multiple factors on urban flow, such as region features and urban environment, we employ diffusion model to generate urban flow for regions under different conditions. We first construct an urban knowledge graph (UKG) to model the urban environment and relationships between regions, based on which 
    
[^12]: 机器学习问题中用于算法投资策略的新损失函数——平均绝对方向损失

    Mean Absolute Directional Loss as a New Loss Function for Machine Learning Problems in Algorithmic Investment Strategies. (arXiv:2309.10546v1 [q-fin.CP])

    [http://arxiv.org/abs/2309.10546](http://arxiv.org/abs/2309.10546)

    这项研究提出了一种新的损失函数(Mean Absolute Directional Loss，MADL)，用于解决在机器学习模型中应用于算法投资策略中的金融时间序列预测问题。通过在两种不同资产类别的数据上验证，MADL函数能够提供更好的超参数选择，并获得更有效的投资策略。

    

    本文研究了在算法投资策略（AIS）构建中，用于金融时间序列预测的机器学习模型的适当损失函数的问题。我们提出了平均绝对方向损失（MADL）函数，解决了传统预测误差函数在从预测中提取信息以创建有效的买卖信号方面的重要问题。最后，我们基于两种不同资产类别（加密货币：比特币和大宗商品：原油）的数据表明，新的损失函数使我们能够选择更好的LSTM模型超参数，并在样本外数据上获得更有效的投资策略，相对于风险调整回报指标。

    This paper investigates the issue of an adequate loss function in the optimization of machine learning models used in the forecasting of financial time series for the purpose of algorithmic investment strategies (AIS) construction. We propose the Mean Absolute Directional Loss (MADL) function, solving important problems of classical forecast error functions in extracting information from forecasts to create efficient buy/sell signals in algorithmic investment strategies. Finally, based on the data from two different asset classes (cryptocurrencies: Bitcoin and commodities: Crude Oil), we show that the new loss function enables us to select better hyperparameters for the LSTM model and obtain more efficient investment strategies, with regard to risk-adjusted return metrics on the out-of-sample data.
    
[^13]: 模型吸取: 针对LLMs的一种提取攻击

    Model Leeching: An Extraction Attack Targeting LLMs. (arXiv:2309.10544v1 [cs.LG])

    [http://arxiv.org/abs/2309.10544](http://arxiv.org/abs/2309.10544)

    模型吸取是一种针对大型语言模型的提取攻击，能够将目标模型的任务特定知识提取到一个参数较少的模型中，并且具有较高的准确率和攻击成功率。

    

    模型吸取是一种针对大型语言模型(LLMs)的新型提取攻击，能够将目标LLM的任务特定知识提炼到一个参数较少的模型中。我们通过从ChatGPT-3.5-Turbo中提取任务能力来演示我们攻击的有效性，实现了73%的准确匹配(EM)相似性以及75%的SQuAD EM准确率和87%的F1得分，仅需50美元的API费用。我们进一步展示了通过模型吸取提取的模型在对目标LLM进行机器学习攻击时的可行性，当应用于ChatGPT-3.5-Turbo时，攻击成功率提高了11%。

    Model Leeching is a novel extraction attack targeting Large Language Models (LLMs), capable of distilling task-specific knowledge from a target LLM into a reduced parameter model. We demonstrate the effectiveness of our attack by extracting task capability from ChatGPT-3.5-Turbo, achieving 73% Exact Match (EM) similarity, and SQuAD EM and F1 accuracy scores of 75% and 87%, respectively for only $50 in API cost. We further demonstrate the feasibility of adversarial attack transferability from an extracted model extracted via Model Leeching to perform ML attack staging against a target LLM, resulting in an 11% increase to attack success rate when applied to ChatGPT-3.5-Turbo.
    
[^14]: OpenMSD:面向多语言科学文档相似度测量的研究

    OpenMSD: Towards Multilingual Scientific Documents Similarity Measurement. (arXiv:2309.10539v1 [cs.CL])

    [http://arxiv.org/abs/2309.10539](http://arxiv.org/abs/2309.10539)

    这项研究开发并评估了多语言科学文档相似度测量模型，提出了第一个多语言科学文档数据集OpenMSD，利用该数据集训练了科学专门化语言模型，并探索了多种策略来提高模型性能。

    

    在这项工作中，我们开发并评估了多语言科学文档相似度测量模型。这些模型可以用来找到不同语言的相关作品，帮助多语言研究人员更有效地找到和探索论文。我们提出了第一个多语言科学文档数据集OpenMSD，其中包含103种语言的7400万篇论文和7780万个引用对。利用OpenMSD，我们预训练了科学专门化语言模型，并探索了不同策略来导出“相关”的论文对以调整模型，包括使用引用、共引用和文献耦合的混合对。为了进一步提高非英文论文的模型性能，我们还探索了使用生成式语言模型来用英文摘要丰富非英文论文。这使我们能够利用模型的英文能力为非英文论文创建更好的表示。我们的最佳模型显著地超过了其他模型的表现。

    We develop and evaluate multilingual scientific documents similarity measurement models in this work. Such models can be used to find related works in different languages, which can help multilingual researchers find and explore papers more efficiently. We propose the first multilingual scientific documents dataset, Open-access Multilingual Scientific Documents (OpenMSD), which has 74M papers in 103 languages and 778M citation pairs. With OpenMSD, we pretrain science-specialized language models, and explore different strategies to derive "related" paper pairs to fine-tune the models, including using a mixture of citation, co-citation, and bibliographic-coupling pairs. To further improve the models' performance for non-English papers, we explore the use of generative language models to enrich the non-English papers with English summaries. This allows us to leverage the models' English capabilities to create better representations for non-English papers. Our best model significantly outp
    
[^15]: 基于对比感知和概念处理的认知启发神经结构用于视觉抽象推理

    A Cognitively-Inspired Neural Architecture for Visual Abstract Reasoning Using Contrastive Perceptual and Conceptual Processing. (arXiv:2309.10532v1 [cs.AI])

    [http://arxiv.org/abs/2309.10532](http://arxiv.org/abs/2309.10532)

    基于人类认知启发，我们提出了一种新的神经结构，用于解决视觉抽象推理任务。该架构通过追求感知和概念处理之间的一致性，采用迭代、自对比的学习过程来模拟人类的认知过程。实验证明，该网络在RAVEN数据集上实现了比之前所有模型更高的准确性，并且使用了最弱的归纳偏置。我们还指出了原始数据集中存在的类不平衡问题，并提出了解决方案。

    

    我们引入了一种新的神经结构，用于解决视觉抽象推理任务，受人类认知的启发，具体来说是由人类抽象推理通常将感知和概念处理交替进行作为灵活、迭代和动态认知过程的一部分的观察所启发。受此原理的启发，我们的架构将视觉抽象推理建模为一种迭代的、自对比的学习过程，追求视觉刺激的感知和概念处理之间的一致性。我们解释了这个新的对比感知-概念网络（CPCNet）如何通过模拟鸦文进阶矩阵智力测试的矩阵推理问题来工作。在机器学习数据集RAVEN上进行的实验证明，CPCNet在使用最弱的归纳偏置的同时实现了比之前所有已发表模型更高的精度。我们还指出了原始RAVEN数据集中存在的大量且以前没有被注意到的类不平衡问题，并提出了一个新的解决方案。

    We introduce a new neural architecture for solving visual abstract reasoning tasks inspired by human cognition, specifically by observations that human abstract reasoning often interleaves perceptual and conceptual processing as part of a flexible, iterative, and dynamic cognitive process. Inspired by this principle, our architecture models visual abstract reasoning as an iterative, self-contrasting learning process that pursues consistency between perceptual and conceptual processing of visual stimuli. We explain how this new Contrastive Perceptual-Conceptual Network (CPCNet) works using matrix reasoning problems in the style of the well-known Raven's Progressive Matrices intelligence test. Experiments on the machine learning dataset RAVEN show that CPCNet achieves higher accuracy than all previously published models while also using the weakest inductive bias. We also point out a substantial and previously unremarked class imbalance in the original RAVEN dataset, and we propose a new
    
[^16]: 基于信息互补的可见光和近红外图像融合算法

    Visible and NIR Image Fusion Algorithm Based on Information Complementarity. (arXiv:2309.10522v1 [cs.CV])

    [http://arxiv.org/abs/2309.10522](http://arxiv.org/abs/2309.10522)

    这篇论文提出了一种基于信息互补的可见光和近红外图像融合算法，该算法通过使用权重引导滤波器和引导滤波器来获取纹理和边缘信息，并通过差值图和夜间补偿来生成互补权重图，从而解决了当前可见光和近红外融合算法中存在的色彩失真和伪影问题。

    

    可见光和近红外（NIR）频带传感器提供了从场景中捕捉到的互补光谱辐射的图像。可见光和近红外图像的融合旨在利用它们的光谱特性来提高图像质量。然而，目前的可见光和近红外融合算法无法很好地利用光谱特性，同时也缺乏信息互补，导致色彩失真和伪影。因此，本文从物理信号的层面设计了一种互补融合模型。首先，为了区分噪声和有用信息，我们使用了两层权重引导滤波器和引导滤波器来分别获得纹理和边缘图像。其次，为了生成初始的可见光-NIR互补权重图，通过扩展-DoG滤波器对可见光和近红外的差值图进行滤波。之后，NIR夜间补偿的显著区域通过arctanI函数来指导初始互补权重图。

    Visible and near-infrared(NIR) band sensors provide images that capture complementary spectral radiations from a scene. And the fusion of the visible and NIR image aims at utilizing their spectrum properties to enhance image quality. However, currently visible and NIR fusion algorithms cannot well take advantage of spectrum properties, as well as lack information complementarity, which results in color distortion and artifacts. Therefore, this paper designs a complementary fusion model from the level of physical signals. First, in order to distinguish between noise and useful information, we use two layers of the weight-guided filter and guided filter to obtain texture and edge layers, respectively. Second, to generate the initial visible-NIR complementarity weight map, the difference maps of visible and NIR are filtered by the extend-DoG filter. After that, the significant region of NIR night-time compensation guides the initial complementarity weight map by the arctanI function. Fina
    
[^17]: 部分指定的因果模拟

    Partially-Specified Causal Simulations. (arXiv:2309.10514v1 [stat.ME])

    [http://arxiv.org/abs/2309.10514](http://arxiv.org/abs/2309.10514)

    本文介绍了因果推断方法不当模拟设计的问题，并提出了一个满足期望的模拟框架PARCS，该框架合成了基于因果模型和可调参数的数据，使用户能够生成符合条件的数据来评估因果推断方法。

    

    模拟研究在验证因果推断方法中起着关键作用。只有在研究根据被测试方法所承诺的操作条件进行设计时，模拟结果才是可靠的。然而，很多因果推断文献往往设计了过于受限或错误指定的研究。本文详细介绍了因果方法不当模拟设计的问题，并编制了有效的模拟框架的一系列期望。然后，我们介绍了部分随机化的因果模拟（PARCS），这是一个满足这些期望的模拟框架。PARCS基于图形化的因果模型和一系列可调参数合成数据。通常的因果假设与参数之间有明确的映射，因此用户可以确定并指定相关参数的子集，并随机化其余参数以生成符合条件的数据生成过程，用于其因果方法。结果是更全面和准确地评估因果推断方法的数据。

    Simulation studies play a key role in the validation of causal inference methods. The simulation results are reliable only if the study is designed according to the promised operational conditions of the method-in-test. Still, many causal inference literature tend to design over-restricted or misspecified studies. In this paper, we elaborate on the problem of improper simulation design for causal methods and compile a list of desiderata for an effective simulation framework. We then introduce partially-randomized causal simulation (PARCS), a simulation framework that meets those desiderata. PARCS synthesizes data based on graphical causal models and a wide range of adjustable parameters. There is a legible mapping from usual causal assumptions to the parameters, thus, users can identify and specify the subset of related parameters and randomize the remaining ones to generate a range of complying data-generating processes for their causal method. The result is a more comprehensive and i
    
[^18]: 一个可配置的库用于生成和操作迷宫数据集

    A Configurable Library for Generating and Manipulating Maze Datasets. (arXiv:2309.10498v1 [cs.LG])

    [http://arxiv.org/abs/2309.10498](http://arxiv.org/abs/2309.10498)

    这个论文介绍了一个可配置的库，用于生成和处理迷宫数据集，研究人员可以通过该库生成不同分布的迷宫数据集，并对生成参数和生成规则进行自定义控制。可以支持多种输出格式，适用于不同类型的模型。

    

    理解机器学习模型对分布偏移的响应方式是一个重要的研究挑战。由于不同的生成算法提供了一个细致的平台来模拟微妙和显著的分布偏移，迷宫作为一个优秀的测试基准。为了支持对模型在分布偏离数据上行为的系统性研究，我们提出了“maze-dataset”，一个包含迷宫求解任务的生成、处理和可视化数据集的综合库。借助这个库，研究人员可以轻松创建数据集，可以对使用的生成算法、传递给选择算法的参数和生成的迷宫必须满足的筛选器进行广泛的控制。此外，它支持多种输出格式，包括栅格化和基于文本的格式，适用于卷积神经网络和自回归变换模型。这些格式以及用于可视化和转换的工具确保了灵活性和适应性。

    Understanding how machine learning models respond to distributional shifts is a key research challenge. Mazes serve as an excellent testbed due to varied generation algorithms offering a nuanced platform to simulate both subtle and pronounced distributional shifts. To enable systematic investigations of model behavior on out-of-distribution data, we present $\texttt{maze-dataset}$, a comprehensive library for generating, processing, and visualizing datasets consisting of maze-solving tasks. With this library, researchers can easily create datasets, having extensive control over the generation algorithm used, the parameters fed to the algorithm of choice, and the filters that generated mazes must satisfy. Furthermore, it supports multiple output formats, including rasterized and text-based, catering to convolutional neural networks and autoregressive transformer models. These formats, along with tools for visualizing and converting between them, ensure versatility and adaptability in re
    
[^19]: 对GPT-4在ETHICS数据集上的评估

    An Evaluation of GPT-4 on the ETHICS Dataset. (arXiv:2309.10492v1 [cs.CL])

    [http://arxiv.org/abs/2309.10492](http://arxiv.org/abs/2309.10492)

    本研究评估了GPT-4在ETHICS数据集上的性能，结果表明GPT-4的表现优于之前的模型，表明AI伦理中与共同人类价值观的合作学习并不是一个难题。

    

    本报告总结了对GPT-4在ETHICS数据集上性能的短期研究。ETHICS数据集包含五个分数据集，涵盖了伦理学的不同领域：正义、道德、德性伦理学、功利主义和常识伦理学。这些道德判断被精选，以尽可能高的一致性来代表共享的人类价值观，而不是道德困境。GPT-4的表现比之前的模型好得多，表明AI伦理中与共同人类价值观的合作学习并不是一个难题。

    This report summarizes a short study of the performance of GPT-4 on the ETHICS dataset. The ETHICS dataset consists of five sub-datasets covering different fields of ethics: Justice, Deontology, Virtue Ethics, Utilitarianism, and Commonsense Ethics. The moral judgments were curated so as to have a high degree of agreement with the aim of representing shared human values rather than moral dilemmas. GPT-4's performance is much better than that of previous models and suggests that learning to work with common human values is not the hard problem for AI ethics.
    
[^20]: 三维照片上完全自动的地标标记和面部分割

    Fully automated landmarking and facial segmentation on 3D photographs. (arXiv:2309.10472v1 [cs.CV])

    [http://arxiv.org/abs/2309.10472](http://arxiv.org/abs/2309.10472)

    本研究开发了一种基于深度学习的自动头部测量注释方法，使用三维面部立体摄影术进行了评估。该方法能够准确地进行地标标记和面部分割，避免了手动注释的耗时和容易出错的问题。

    

    三维面部立体摄影术提供了详细的颅面软组织表示，无需使用电离辐射。虽然手动注释标记作为头部测量分析的当前金标准，但这是一个耗时的过程，容易出错。本研究的目的是开发和评估一种基于深度学习的自动头部测量注释方法。一个观察者手动在2897张三维面部照片上注释了十个地标。自动标记工作流涉及两个连续的DiffusionNet模型和额外的面部分割算法。数据集被随机分为训练和测试数据集。训练数据集用于训练深度学习网络，测试数据集用于评估自动工作流的性能。通过计算自动和手动标记之间的欧氏距离来评估工作流的精度。

    Three-dimensional facial stereophotogrammetry provides a detailed representation of craniofacial soft tissue without the use of ionizing radiation. While manual annotation of landmarks serves as the current gold standard for cephalometric analysis, it is a time-consuming process and is prone to human error. The aim in this study was to develop and evaluate an automated cephalometric annotation method using a deep learning-based approach. Ten landmarks were manually annotated on 2897 3D facial photographs by a single observer. The automated landmarking workflow involved two successive DiffusionNet models and additional algorithms for facial segmentation. The dataset was randomly divided into a training and test dataset. The training dataset was used to train the deep learning networks, whereas the test dataset was used to evaluate the performance of the automated workflow. The precision of the workflow was evaluated by calculating the Euclidean distances between the automated and manual
    
[^21]: 探索人工智能的黑暗面: 使用ChatGPT设计和部署高级网络钓鱼攻击

    Exploring the Dark Side of AI: Advanced Phishing Attack Design and Deployment Using ChatGPT. (arXiv:2309.10463v1 [cs.CR])

    [http://arxiv.org/abs/2309.10463](http://arxiv.org/abs/2309.10463)

    本文探讨使用ChatGPT开发高级网络钓鱼攻击并自动进行大规模部署的可能性，突出了这些攻击的快速生成和与目标网站的密切相似性，强调了在网络钓鱼攻击中滥用人工智能的风险，以及对AI系统加强对策的必要性。

    

    本文探讨使用ChatGPT开发高级网络钓鱼攻击并自动进行大规模部署的可能性。我们让ChatGPT生成网络钓鱼攻击的以下部分：i) 克隆目标网站，ii) 集成窃取凭据的代码，iii) 混淆代码，iv) 在托管提供商上自动化部署网站，v) 注册网络钓鱼域名，vi) 将网站与反向代理集成。对自动生成的网络钓鱼工具包的初步评估突出了它们快速生成和部署的过程，以及生成页面与目标网站的密切相似性。更广泛地说，我们展示了最近在人工智能方面的进展强调了在网络钓鱼攻击中滥用其潜力的风险，这可能导致它们的普遍性和严重性增加。这突显了AI系统中增强对策的必要性。

    This paper explores the possibility of using ChatGPT to develop advanced phishing attacks and automate their large-scale deployment. We make ChatGPT generate the following parts of a phishing attack: i) cloning a targeted website, ii) integrating code for stealing credentials, iii) obfuscating code, iv) automating website deployment on a hosting provider, v) registering a phishing domain name, and vi) integrating the website with a reverse proxy. The initial assessment of the automatically generated phishing kits highlights their rapid generation and deployment process as well as the close resemblance of the resulting pages to the target website. More broadly, we demonstrate that recent advances in AI underscore the potential risks of its misuse in phishing attacks, which can lead to their increased prevalence and severity. This highlights the necessity for enhanced countermeasures within AI systems.
    
[^22]: 人工智能与人类互动以及社会陷阱

    Human-AI Interactions and Societal Pitfalls. (arXiv:2309.10448v1 [cs.AI])

    [http://arxiv.org/abs/2309.10448](http://arxiv.org/abs/2309.10448)

    本研究研究了人工智能与人类互动中面临的同质化和偏见问题，提出了改善人工智能与人类互动的解决办法，实现个性化输出而不牺牲生产力。

    

    当与生成式人工智能（AI）合作时，用户可能会看到生产力的提升，但AI生成的内容可能不完全符合他们的偏好。为了研究这种影响，我们引入了一个贝叶斯框架，其中异质用户选择与AI共享多少信息，面临输出保真度和通信成本之间的权衡。我们展示了这些个体决策与AI训练之间的相互作用可能导致社会挑战。输出可能变得更加同质化，特别是当AI在AI生成的内容上进行训练时。而任何AI的偏见可能成为社会偏见。解决同质化和偏见问题的办法是改进人工智能与人类的互动，实现个性化输出而不牺牲生产力。

    When working with generative artificial intelligence (AI), users may see productivity gains, but the AI-generated content may not match their preferences exactly. To study this effect, we introduce a Bayesian framework in which heterogeneous users choose how much information to share with the AI, facing a trade-off between output fidelity and communication cost. We show that the interplay between these individual-level decisions and AI training may lead to societal challenges. Outputs may become more homogenized, especially when the AI is trained on AI-generated content. And any AI bias may become societal bias. A solution to the homogenization and bias issues is to improve human-AI interactions, enabling personalized outputs without sacrificing productivity.
    
[^23]: 利用大型语言模型探索自我强化以改进学生生成的多项选择题解释

    Exploring Self-Reinforcement for Improving Learnersourced Multiple-Choice Question Explanations with Large Language Models. (arXiv:2309.10444v1 [cs.AI])

    [http://arxiv.org/abs/2309.10444](http://arxiv.org/abs/2309.10444)

    本文提出了一个自我强化大型语言模型框架，自动生成和评估学生生成的解释，用于改进学生资源共享中学生生成的多项选择题的解释质量。

    

    学生资源共享涉及学生生成和分享学习资源。在学生生成多项选择题时，创建解释是一个关键步骤，因为它有助于对相关概念的深入理解。然而，学生往往由于主题理解有限和仅仅重申问题、干扰因素和正确答案的倾向而难以编写有效的解释。为了帮助支撑这个任务，在这项工作中，我们提出了一个自我强化的大型语言模型框架，旨在自动生成和评估解释。该框架由三个模块组成，生成与学生对齐的解释，评估这些解释以确保其质量，并迭代增强解释。如果一个解释的评估分数低于定义的阈值，框架会迭代地优化和重新评估解释。重要的是，我们的框架模拟了一个学生学习的过程。

    Learnersourcing involves students generating and sharing learning resources with their peers. When learnersourcing multiple-choice questions, creating explanations for the generated questions is a crucial step as it facilitates a deeper understanding of the related concepts. However, it is often difficult for students to craft effective explanations due to limited subject understanding and a tendency to merely restate the question stem, distractors, and correct answer. To help scaffold this task, in this work we propose a self-reinforcement large-language-model framework, with the goal of generating and evaluating explanations automatically. Comprising three modules, the framework generates student-aligned explanations, evaluates these explanations to ensure their quality and iteratively enhances the explanations. If an explanation's evaluation score falls below a defined threshold, the framework iteratively refines and reassesses the explanation. Importantly, our framework emulates th
    
[^24]: 重新思考基于模仿的自动驾驶规划器

    Rethinking Imitation-based Planner for Autonomous Driving. (arXiv:2309.10443v1 [cs.RO])

    [http://arxiv.org/abs/2309.10443](http://arxiv.org/abs/2309.10443)

    本论文通过提供大规模实际数据集和标准化闭环基准测试，重新思考了基于模仿的自动驾驶规划器，并对关键特征和数据增强技术进行了研究，发现了模仿差距，并提出了强大的基线模型-PlanTF。

    

    近年来，基于模仿的驾驶规划器取得了相当大的成功。然而，由于缺乏统一的基准测试，不同设计的有效性仍然不清楚。新发布的nuPlan通过提供大规模的实际数据集和标准化闭环基准测试来解决这个问题，以进行公平比较。利用这个平台，我们对基于模仿的规划器的两个基本但未被充分研究的方面进行了全面的研究：自我规划所需的关键特征和降低复合误差的有效数据增强技术。此外，我们强调了当前学习系统忽视的模仿差距。最后，整合我们的发现，我们提出了一个强大的基线模型-PlanTF。我们的结果表明，一个设计良好的纯模仿规划器可以与涉及手工制定规则的现有方法实现高竞争性性能，并展示出更好的推广能力。

    In recent years, imitation-based driving planners have reported considerable success. However, due to the absence of a standardized benchmark, the effectiveness of various designs remains unclear. The newly released nuPlan addresses this issue by offering a large-scale real-world dataset and a standardized closed-loop benchmark for equitable comparisons. Utilizing this platform, we conduct a comprehensive study on two fundamental yet underexplored aspects of imitation-based planners: the essential features for ego planning and the effective data augmentation techniques to reduce compounding errors. Furthermore, we highlight an imitation gap that has been overlooked by current learning systems. Finally, integrating our findings, we propose a strong baseline model-PlanTF. Our results demonstrate that a well-designed, purely imitation-based planner can achieve highly competitive performance compared to state-of-the-art methods involving hand-crafted rules and exhibit superior generalizati
    
[^25]: 多对象图形可用性网络：通过复合对象可用性实现目标导向规划

    Multi-Object Graph Affordance Network: Enabling Goal-Oriented Planning through Compound Object Affordances. (arXiv:2309.10426v1 [cs.RO])

    [http://arxiv.org/abs/2309.10426](http://arxiv.org/abs/2309.10426)

    多对象图形可用性网络（MOGAN）模型化了复合对象的可用性，预测了将新对象放置在复合对象上的效果。在构建具有特定高度或属性的塔等任务中，我们使用基于搜索的规划找到具有适当可用性的对象堆叠操作的序列。我们的系统能够正确建模非常复杂的复合对象的可用性，并在模拟和真实环境中展示了其适用性。

    

    学习对象的可用性是机器人学习领域的有效工具。虽然数据驱动的模型深入探讨了单个或成对对象的可用性，但在调查由复杂形状的任意数量的对象组成的复合对象的可用性方面存在明显差距。在本研究中，我们提出了多对象图形可用性网络（MOGAN），它建模了复合对象的可用性，并预测将新对象放置在现有复合对象上的效果。给定不同的任务，例如构建具有特定高度或属性的塔，我们使用基于搜索的规划找到具有适当可用性的对象堆叠操作的序列。我们展示了我们的系统能够正确建模包括堆叠的球体和杯子、杆和包围杆的环等非常复杂的复合对象的可用性。我们在模拟和真实环境中展示了我们系统的适用性。

    Learning object affordances is an effective tool in the field of robot learning. While the data-driven models delve into the exploration of affordances of single or paired objects, there is a notable gap in the investigation of affordances of compound objects that are composed of an arbitrary number of objects with complex shapes. In this study, we propose Multi-Object Graph Affordance Network (MOGAN) that models compound object affordances and predicts the effect of placing new objects on top of the existing compound. Given different tasks, such as building towers of specific heights or properties, we used a search based planning to find the sequence of stack actions with the objects of suitable affordances. We showed that our system was able to correctly model the affordances of very complex compound objects that include stacked spheres and cups, poles, and rings that enclose the poles. We demonstrated the applicability of our system in both simulated and real-world environments, com
    
[^26]: 减少医疗人工智能对患者造成伤害的功能要求

    Functional requirements to mitigate the Risk of Harm to Patients from Artificial Intelligence in Healthcare. (arXiv:2309.10424v1 [cs.AI])

    [http://arxiv.org/abs/2309.10424](http://arxiv.org/abs/2309.10424)

    提出了14个功能要求，人工智能系统可以通过实施这些要求来降低医疗领域人工智能可能带来的风险。

    

    欧洲议会议员研究服务总司为欧洲议会议员准备了一份报告，其中列举了医疗和医疗保健领域人工智能的七个主要风险：由于人工智能错误导致患者受伤、医用人工智能工具的滥用、人工智能的偏见以及现有不平等的延续、缺乏透明度、隐私和安全问题、责任缺失和实施障碍。本研究提出了14个功能要求，人工智能系统可以通过实施这些要求来降低与其医疗目的相关的风险：人工智能护照、用户管理、法规检查、仅供学术使用的免责声明、数据质量评估、临床医生的双重检查、持续性能评估、审计跟踪、持续可用性测试、回顾性/模拟案例的审查、偏见检查、可解释的人工智能、加密和使用经过实地测试的库以及语义互操作性。我们的目的是提供一种方法来减少医疗人工智能可能带来的风险。

    The Directorate General for Parliamentary Research Services of the European Parliament has prepared a report to the Members of the European Parliament where they enumerate seven main risks of Artificial Intelligence (AI) in medicine and healthcare: patient harm due to AI errors, misuse of medical AI tools, bias in AI and the perpetuation of existing inequities, lack of transparency, privacy and security issues, gaps in accountability, and obstacles in implementation.  In this study, we propose fourteen functional requirements that AI systems may implement to reduce the risks associated with their medical purpose: AI passport, User management, Regulation check, Academic use only disclaimer, data quality assessment, Clinicians double check, Continuous performance evaluation, Audit trail, Continuous usability test, Review of retrospective/simulated cases, Bias check, eXplainable AI, Encryption and use of field-tested libraries, and Semantic interoperability.  Our intention here is to prov
    
[^27]: 从助教中学习编程子目标：探索AI助教的潜力

    Learning from Teaching Assistants to Program with Subgoals: Exploring the Potential for AI Teaching Assistants. (arXiv:2309.10419v1 [cs.HC])

    [http://arxiv.org/abs/2309.10419](http://arxiv.org/abs/2309.10419)

    通过研究初学者在子目标学习环境中与AI助教的互动，我们发现生成型AI助教在初学者编程教育中具有与人类助教相当的实用性和效果。这为我们提供了设计和利用生成型AI助教的指导方针。

    

    随着生成型AI的最新进展，像ChatGPT这样的对话模型成为了助教的可行候选人。我们通过研究初学者在子目标学习环境中与助教的互动来探讨使用生成型AI作为编程教育助教的实用性。为了比较学习者与AI助教和人类助教之间的互动和感知，我们进行了一项20名初学者编程学习者的组间研究。学习者通过产生子目标和子解决方案，并在助教的指导下解决编程任务。我们的研究表明，学习者能够以相当的得分更快地解决任务，并且对AI助教的认识与人类助教在回复的速度和详尽程度、对话的有用性、难度和满意度等方面相当。最后，我们提出了根据聊天记录分析结果更好地设计和利用生成型AI作为编程教育助教的指导方针。

    With recent advances in generative AI, conversational models like ChatGPT have become feasible candidates for TAs. We investigate the practicality of using generative AI as TAs in introductory programming education by examining novice learners' interaction with TAs in a subgoal learning environment. To compare the learners' interaction and perception of the AI and human TAs, we conducted a between-subject study with 20 novice programming learners. Learners solve programming tasks by producing subgoals and subsolutions with the guidance of a TA. Our study shows that learners can solve tasks faster with comparable scores with AI TAs. Learners' perception of the AI TA is on par with that of human TAs in terms of speed and comprehensiveness of the replies and helpfulness, difficulty, and satisfaction of the conversation. Finally, we suggest guidelines to better design and utilize generative AI as TAs in programming education from the result of our chat log analysis.
    
[^28]: 通过网络感知嵌入进行无监督学习

    Unsupervised Learning via Network-Aware Embeddings. (arXiv:2309.10408v1 [cs.LG])

    [http://arxiv.org/abs/2309.10408](http://arxiv.org/abs/2309.10408)

    本研究提出了一种通过估计数值节点属性之间的网络距离来创建网络感知嵌入的无监督学习方法，解决了传统聚类方法难以处理复杂网络结构的问题。

    

    数据聚类是无监督学习的关键组成部分，其任务是根据相似性对观测结果进行分组，广泛应用于生物学、医学和社会科学等多个领域。然而，在这些领域中，数据的不同维度之间存在着复杂的相互依赖关系，例如人们在复杂的社交网络中可能具有的各种特征和观点。当前的聚类方法很难处理这种复杂性：深度学习可以近似这些依赖关系，但不能将其显式地作为分析的输入。本文旨在解决无监督学习文献中的这个盲点。我们通过估计数值节点属性之间的网络距离来创建网络感知嵌入，通过广义欧氏距离计算。与我们所知道的所有文献中的方法不同，我们不是对网络的节点进行聚类，而是对其节点属性进行聚类。在我们的实验中，还展示了我们方法的性能。

    Data clustering, the task of grouping observations according to their similarity, is a key component of unsupervised learning -- with real world applications in diverse fields such as biology, medicine, and social science. Often in these fields the data comes with complex interdependencies between the dimensions of analysis, for instance the various characteristics and opinions people can have live on a complex social network. Current clustering methods are ill-suited to tackle this complexity: deep learning can approximate these dependencies, but not take their explicit map as the input of the analysis. In this paper, we aim at fixing this blind spot in the unsupervised learning literature. We can create network-aware embeddings by estimating the network distance between numeric node attributes via the generalized Euclidean distance. Differently from all methods in the literature that we know of, we do not cluster the nodes of the network, but rather its node attributes. In our experi
    
[^29]: 在医学图像中利用因果信号的研究：一项带有实证结果的试点研究

    Exploiting Causality Signals in Medical Images: A Pilot Study with Empirical Results. (arXiv:2309.10399v1 [cs.CV])

    [http://arxiv.org/abs/2309.10399](http://arxiv.org/abs/2309.10399)

    本研究提出了一种利用医学图像中的因果信号进行自动分类的新方法，通过模型化图像中一个部分特征的存在如何影响另一个部分特征的外观，改善了分类性能并产生了更稳健的预测，聚焦于图像中的相关部分。

    

    我们提出了一种新的方法，用于自动分类医学图像，该方法利用场景中的弱因果信号来建模图像中一个部分特征的存在如何影响另一个部分特征的外观。我们的方法由两个组成部分组成：卷积神经网络骨干和因果因子提取模块。后者计算特征图的权重，根据其对图像场景的因果影响增强每个特征图。我们可以通过使用两个外部信号来修改因果模块的功能，从而获得我们方法的不同变体。我们使用定量实验、定性评估和削弱实验在公开数据集上对前列腺MRI图像进行前列腺癌诊断评估。我们的结果表明，我们的方法改善了分类性能，并产生了更稳健的预测，聚焦于图像中的相关部分。这在医疗图像中尤为重要。

    We present a new method for automatically classifying medical images that uses weak causal signals in the scene to model how the presence of a feature in one part of the image affects the appearance of another feature in a different part of the image. Our method consists of two components: a convolutional neural network backbone and a causality-factors extractor module. The latter computes weights for the feature maps to enhance each feature map according to its causal influence in the image's scene. We can modify the functioning of the causality module by using two external signals, thus obtaining different variants of our method. We evaluate our method on a public dataset of prostate MRI images for prostate cancer diagnosis, using quantitative experiments, qualitative assessment, and ablation studies. Our results show that our method improves classification performance and produces more robust predictions, focusing on relevant parts of the image. That is especially important in medic
    
[^30]: 支持临床决策系统中适应性问卷的设计与应用：以STOPP/START v2为例

    Adaptive questionnaires for facilitating patient data entry in clinical decision support systems: Methods and application to STOPP/START v2. (arXiv:2309.10398v1 [cs.AI])

    [http://arxiv.org/abs/2309.10398](http://arxiv.org/abs/2309.10398)

    本研究提出了一种适应性问卷的设计与应用方法，用于简化临床决策支持系统中病人数据的录入过程，并解决了数据质量和可用性等问题。

    

    临床决策支持系统是帮助临床医生进行医疗决策的软件工具。然而，临床医生对这些系统的接受程度通常较低。已知的问题是它们通常需要临床医生手动输入大量病人数据，而这一过程既冗长又繁琐。现有的解决方案，如从电子健康记录中自动提取数据，尚未完全满足要求，因为数据质量和可用性较低。实际上，许多系统仍然包括需要填写长问卷的数据输入方式。本文提出了一种简化病人数据输入的创新解决方案，使用适应性问卷，即在用户交互过程中动态显示或隐藏问题的问卷。针对基于规则的决策支持系统，我们设计了将系统的临床规则转化为显示规则的方法，以确定在问卷中显示的项目，并设计了确定项目优先顺序的方法。

    Clinical decision support systems are software tools that help clinicians to make medical decisions. However, their acceptance by clinicians is usually rather low. A known problem is that they often require clinicians to manually enter lots of patient data, which is long and tedious. Existing solutions, such as the automatic data extraction from electronic health record, are not fully satisfying, because of low data quality and availability. In practice, many systems still include long questionnaire for data entry.  In this paper, we propose an original solution to simplify patient data entry, using an adaptive questionnaire, i.e. a questionnaire that evolves during user interaction, showing or hiding questions dynamically. Considering a rule-based decision support systems, we designed methods for translating the system's clinical rules into display rules that determine the items to show in the questionnaire, and methods for determining the optimal order of priority among the items in 
    
[^31]: 图对比学习与图元学习相遇：一种统一的用于少样本节点任务的方法

    Graph Contrastive Learning Meets Graph Meta Learning: A Unified Method for Few-shot Node Tasks. (arXiv:2309.10376v1 [cs.LG])

    [http://arxiv.org/abs/2309.10376](http://arxiv.org/abs/2309.10376)

    COLA是一种统一的方法，整合了图对比学习和元学习的优势，用于少样本节点分类任务。它利用图扩增来识别语义相似的节点，从而使得能够在少样本情况下进行快速泛化。

    

    图神经网络（GNNs）已经在图表示学习（GRL）中变得流行起来。其中一个基本应用是少样本节点分类。大多数现有的方法都遵循元学习范式，展示了对少样本任务快速泛化的能力。然而，最近的研究表明，图对比学习结合微调可以明显优于元学习方法。尽管在经验上取得了成功，但对其背后原因的理解有限。在我们的研究中，我们首先确定了图对比学习相对于元学习的两个关键优势，包括（1）全面利用图节点和（2）图扩增的能力。为了将图对比学习和元学习的优势结合到少样本节点分类任务中，我们引入了一种新的范式：对比式少样本节点分类（COLA）。具体来说，COLA利用图扩增来识别语义相似的节点，从而使得

    Graph Neural Networks (GNNs) have become popular in Graph Representation Learning (GRL). One fundamental application is few-shot node classification. Most existing methods follow the meta learning paradigm, showing the ability of fast generalization to few-shot tasks. However, recent works indicate that graph contrastive learning combined with fine-tuning can significantly outperform meta learning methods. Despite the empirical success, there is limited understanding of the reasons behind it. In our study, we first identify two crucial advantages of contrastive learning compared to meta learning, including (1) the comprehensive utilization of graph nodes and (2) the power of graph augmentations. To integrate the strength of both contrastive learning and meta learning on the few-shot node classification tasks, we introduce a new paradigm: Contrastive Few-Shot Node Classification (COLA). Specifically, COLA employs graph augmentations to identify semantically similar nodes, which enables 
    
[^32]: 生成式AI与AGI：现代LLMs的认知优势与劣势

    Generative AI vs. AGI: The Cognitive Strengths and Weaknesses of Modern LLMs. (arXiv:2309.10371v1 [cs.AI])

    [http://arxiv.org/abs/2309.10371](http://arxiv.org/abs/2309.10371)

    本文考察了生成式AI（LLMs）作为认知系统的优势与劣势。研究发现，这些系统的实际弱点可以追溯到其基本认知架构的缺陷。逐步改进LLMs并不能实现人类水平的AGI。然而，研究和实验LLMs仍然有助于理解人类级AGI，并且LLMs可以成为人类级AGI架构的重要组成部分。

    

    本文对互动LLMs作为认知系统进行了适度详细的考察，重点关注于2023年左右的LLMs，如ChatGPT、GPT-4、Bard、Llama等。回顾了这些系统的认知优势，然后仔细研究了这些LLMs与人类认知系统之间的实质性差异。研究发现，这些AI系统的许多实际弱点可以具体归因于这些系统构建所依赖的基本认知架构的缺陷。论文认为，在可实现的计算资源数量范围内，逐步改进此类LLMs并不是朝着达到人类级AGI的可行方法。这并不意味着在研究和实验LLMs方面不能从中获得关于人类级AGI的经验，也不意味着LLMs不能成为包含其他想法的人类级AGI架构的重要组成部分。社会和伦理问题。

    A moderately detailed consideration of interactive LLMs as cognitive systems is given, focusing on LLMs circa mid-2023 such as ChatGPT, GPT-4, Bard, Llama, etc.. Cognitive strengths of these systems are reviewed, and then careful attention is paid to the substantial differences between the sort of cognitive system these LLMs are, and the sort of cognitive systems human beings are. It is found that many of the practical weaknesses of these AI systems can be tied specifically to lacks in the basic cognitive architectures according to which these systems are built. It is argued that incremental improvement of such LLMs is not a viable approach to working toward human-level AGI, in practical terms given realizable amounts of compute resources. This does not imply there is nothing to learn about human-level AGI from studying and experimenting with LLMs, nor that LLMs cannot form significant parts of human-level AGI architectures that also incorporate other ideas. Social and ethical matters 
    
[^33]: 浅层神经网络的几何结构和基于${\mathcal L}^2$代价最小化的构造方法

    Geometric structure of shallow neural networks and constructive ${\mathcal L}^2$ cost minimization. (arXiv:2309.10370v1 [cs.LG])

    [http://arxiv.org/abs/2309.10370](http://arxiv.org/abs/2309.10370)

    本文提供了浅层神经网络的几何结构解释，并通过基于${\mathcal L}^2$代价最小化的构造方法获得了一个具有优越性能的网络。

    

    本文给出了一个几何解释：浅层神经网络的结构由一个隐藏层、一个斜坡激活函数、一个${\mathcal L}^2$谱范类（或者Hilbert-Schmidt）的代价函数、输入空间${\mathbb R}^M$、输出空间${\mathbb R}^Q$（其中$Q\leq M$），以及训练输入样本数量$N>QM$所特征。我们证明了代价函数的最小值具有$O(\delta_P)$的上界，其中$\delta_P$衡量了训练输入的信噪比。我们使用适应于属于同一输出向量$y_j$的训练输入向量$\overline{x_{0,j}}$的投影来获得近似的优化器，其中$j=1,\dots,Q$。在特殊情况$M=Q$下，我们明确确定了代价函数的一个确切退化局部最小值；这个尖锐的值与对于$Q\leq M$所获得的上界之间有一个相对误差$O(\delta_P^2)$。上界证明的方法提供了一个构造性训练的网络；我们证明它测度了$Q$维空间中的给定输出。

    In this paper, we provide a geometric interpretation of the structure of shallow neural networks characterized by one hidden layer, a ramp activation function, an ${\mathcal L}^2$ Schatten class (or Hilbert-Schmidt) cost function, input space ${\mathbb R}^M$, output space ${\mathbb R}^Q$ with $Q\leq M$, and training input sample size $N>QM$. We prove an upper bound on the minimum of the cost function of order $O(\delta_P$ where $\delta_P$ measures the signal to noise ratio of training inputs. We obtain an approximate optimizer using projections adapted to the averages $\overline{x_{0,j}}$ of training input vectors belonging to the same output vector $y_j$, $j=1,\dots,Q$. In the special case $M=Q$, we explicitly determine an exact degenerate local minimum of the cost function; the sharp value differs from the upper bound obtained for $Q\leq M$ by a relative error $O(\delta_P^2)$. The proof of the upper bound yields a constructively trained network; we show that it metrizes the $Q$-dimen
    
[^34]: 边缘节点在联邦学习中的资源利用效率

    Toward efficient resource utilization at edge nodes in federated learning. (arXiv:2309.10367v1 [cs.LG])

    [http://arxiv.org/abs/2309.10367](http://arxiv.org/abs/2309.10367)

    本论文提出了一种受迁移学习启发的策略，旨在通过减少设备上的资源利用、以及减少服务器和网络的负载，提高联邦学习中边缘节点的效率。

    

    联邦学习（FL）使得边缘节点能够共同构建全局模型，而无需共享他们的数据。这是通过设备计算本地私有模型更新，然后由服务器进行聚合来实现的。然而，计算资源限制和网络通信对于大型深度学习应用中的较大模型大小可能成为严重瓶颈。边缘节点往往具有有限的硬件资源（RAM、CPU），而边缘的网络带宽和可靠性对于扩展联邦车队应用来说是一个问题。在本文中，我们提出并评估了一种受迁移学习启发的FL策略，以减少设备上的资源利用，以及每个全局训练轮次中服务器和网络的负载。对于每个本地模型更新，我们随机选择要训练的层，冻结模型的其余部分。通过这样做，我们可以通过排除所有未训练的部分来减少每轮的服务器负载和通信成本。

    Federated learning (FL) enables edge nodes to collaboratively contribute to constructing a global model without sharing their data. This is accomplished by devices computing local, private model updates that are then aggregated by a server. However, computational resource constraints and network communication can become a severe bottleneck for larger model sizes typical for deep learning applications. Edge nodes tend to have limited hardware resources (RAM, CPU), and the network bandwidth and reliability at the edge is a concern for scaling federated fleet applications. In this paper, we propose and evaluate a FL strategy inspired by transfer learning in order to reduce resource utilization on devices, as well as the load on the server and network in each global training round. For each local model update, we randomly select layers to train, freezing the remaining part of the model. In doing so, we can reduce both server load and communication costs per round by excluding all untrained
    
[^35]: OccluTrack: 重新思考增强多目标行人跟踪中对遮挡感知的方法

    OccluTrack: Rethinking Awareness of Occlusion for Enhancing Multiple Pedestrian Tracking. (arXiv:2309.10360v1 [cs.CV])

    [http://arxiv.org/abs/2309.10360](http://arxiv.org/abs/2309.10360)

    本文提出了一种自适应遮挡感知多目标行人跟踪器OccluTrack，通过引入异常运动抑制机制、姿势导向的再识别模块和全局相关性优化方法，解决了多目标行人跟踪中遮挡导致的问题。

    

    多目标行人跟踪在遮挡的情况下面临着挑战。现有方法在遮挡情况下由于不准确的运动估计、外观特征提取和关联而受到影响，导致身份F1得分（IDF1）不够准确，ID切换过多（IDSw），以及关联准确性和召回率（AssA和AssR）不足。我们发现主要原因是部分遮挡引起的异常检测。本文认为明确的运动估计、可靠的外观特征和公平的关联是解决遮挡场景下问题的关键。具体而言，我们提出了一种自适应遮挡感知多目标行人跟踪器OccluTrack。首先，我们将异常运动抑制机制引入到卡尔曼滤波器中，以自适应地检测和抑制部分遮挡引起的异常运动。其次，我们提出了一种姿势导向的再识别模块，用于提取部分遮挡行人的判别性部分特征。最后，我们设计了一个全局相关性优化方法，以提高遮挡场景下的关联准确性。

    Multiple pedestrian tracking faces the challenge of tracking pedestrians in the presence of occlusion. Existing methods suffer from inaccurate motion estimation, appearance feature extraction, and association due to occlusion, leading to inadequate Identification F1-Score (IDF1), excessive ID switches (IDSw), and insufficient association accuracy and recall (AssA and AssR). We found that the main reason is abnormal detections caused by partial occlusion. In this paper, we suggest that the key insight is explicit motion estimation, reliable appearance features, and fair association in occlusion scenes. Specifically, we propose an adaptive occlusion-aware multiple pedestrian tracker, OccluTrack. We first introduce an abnormal motion suppression mechanism into the Kalman Filter to adaptively detect and suppress outlier motions caused by partial occlusion. Second, we propose a pose-guided re-ID module to extract discriminative part features for partially occluded pedestrians. Last, we desi
    
[^36]: 使用大规模语言模型解释智能体行为

    Explaining Agent Behavior with Large Language Models. (arXiv:2309.10346v1 [cs.LG])

    [http://arxiv.org/abs/2309.10346](http://arxiv.org/abs/2309.10346)

    使用大规模语言模型解释智能体行为的方法，通过学习智能体行为的紧凑表示，并与用户进行交互，能够生成合理的解释，具备与人类专家相似的帮助性。

    

    智能体如机器人越来越多地被部署在真实世界中的安全关键环境中。重要的是，这些智能体能够向人类对等体解释他们决策背后的推理，然而，他们的行为通常是由不可解释的模型（如深度神经网络）产生的。我们提出了一种方法，基于状态和动作的观察，不考虑底层模型表示，生成智能体行为的自然语言解释。我们展示了如何学习智能体行为的简洁表示，并用其生成合理的解释，同时保证了与预训练的大规模语言模型的用户交互。通过用户研究和实证实验，我们证明了我们的方法生成的解释与人类领域专家生成的解释一样有用，同时具备有益的交互，如澄清和反事实查询。

    Intelligent agents such as robots are increasingly deployed in real-world, safety-critical settings. It is vital that these agents are able to explain the reasoning behind their decisions to human counterparts, however, their behavior is often produced by uninterpretable models such as deep neural networks. We propose an approach to generate natural language explanations for an agent's behavior based only on observations of states and actions, agnostic to the underlying model representation. We show how a compact representation of the agent's behavior can be learned and used to produce plausible explanations with minimal hallucination while affording user interaction with a pre-trained large language model. Through user studies and empirical experiments, we show that our approach generates explanations as helpful as those generated by a human domain expert while enabling beneficial interactions such as clarification and counterfactual queries.
    
[^37]: FedWOA:一种利用鲸鱼优化算法进行可再生能源预测的联邦学习模型

    FedWOA: A Federated Learning Model that uses the Whale Optimization Algorithm for Renewable Energy Prediction. (arXiv:2309.10337v1 [cs.LG])

    [http://arxiv.org/abs/2309.10337](http://arxiv.org/abs/2309.10337)

    本论文提出了一种名为FedWOA的新型联邦学习模型，该模型利用鲸鱼优化算法从家庭能源数据训练的局部LTSM神经网络模型的权重中聚合出全局预测模型，解决了数据异质性和参数数量增加导致的预测精度降低的问题。

    

    隐私在处理敏感个人信息的机器学习模型中非常重要，这些模型需要大规模的数据集进行训练。在能源领域，访问家庭生产者和消费者的能源数据对于能源预测以支持能源网格管理和可再生能源的大规模采用至关重要，然而公民通常不愿意授予基于云的机器学习模型访问权限。联邦学习被提出作为解决隐私挑战的方法，但由于数据异质性、发电模式的变化和参数数量的增加导致全局预测模型的生成存在问题，进而降低了预测精度。本文通过引入FedWOA这种新颖的联邦学习模型来解决这些挑战，该模型利用鲸鱼优化算法从基于家庭能源数据训练的局部LTSM神经网络模型的权重中聚合出全局预测模型。所提出的解决方案能够识别出最优的权重向量。

    Privacy is important when dealing with sensitive personal information in machine learning models, which require large data sets for training. In the energy field, access to household prosumer energy data is crucial for energy predictions to support energy grid management and large-scale adoption of renewables however citizens are often hesitant to grant access to cloud-based machine learning models. Federated learning has been proposed as a solution to privacy challenges however report issues in generating the global prediction model due to data heterogeneity, variations in generation patterns, and the high number of parameters leading to even lower prediction accuracy. This paper addresses these challenges by introducing FedWOA a novel federated learning model that employs the Whale Optimization Algorithm to aggregate global prediction models from the weights of local LTSM neural network models trained on prosumer energy data. The proposed solution identifies the optimal vector of wei
    
[^38]: 基于学习的二维不规则形状装箱

    Learning based 2D Irregular Shape Packing. (arXiv:2309.10329v1 [cs.GR])

    [http://arxiv.org/abs/2309.10329](http://arxiv.org/abs/2309.10329)

    这个论文提出了一种基于学习的二维不规则形状装箱方法，通过迭代选择和分组UV贴图补丁，将问题转化为装箱问题并使用联合优化来提高装箱比例。

    

    二维不规则形状装箱是计算机图形学中在纹理图集内排列3D模型的UV贴图补丁以实现内存高效渲染的必要步骤。这个问题是一个联合的、组合的决策问题，涉及所有补丁的位置和方向，已知其NP难度。先前的解决方案要么假设启发式的排列顺序，要么修改上游的网格切割和UV映射以简化问题，这两种方法都限制了装箱比例或引起了鲁棒性或通用性问题。相反，我们提出了一种基于学习的二维不规则形状装箱方法，它能够在最小的输入要求下实现高质量的装箱效果。我们的方法通过迭代地将部分UV贴图补丁选择和分组成近矩形的超级补丁，从而将问题转化为装箱问题，接着使用联合优化来进一步提高装箱比例。为了有效处理包含数百个补丁的大型问题实例，我们还引入了一个特殊的启发式选择和分组算法。

    2D irregular shape packing is a necessary step to arrange UV patches of a 3D model within a texture atlas for memory-efficient appearance rendering in computer graphics. Being a joint, combinatorial decision-making problem involving all patch positions and orientations, this problem has well-known NP-hard complexity. Prior solutions either assume a heuristic packing order or modify the upstream mesh cut and UV mapping to simplify the problem, which either limits the packing ratio or incurs robustness or generality issues. Instead, we introduce a learning-assisted 2D irregular shape packing method that achieves a high packing quality with minimal requirements from the input. Our method iteratively selects and groups subsets of UV patches into near-rectangular super patches, essentially reducing the problem to bin-packing, based on which a joint optimization is employed to further improve the packing ratio. In order to efficiently deal with large problem instances with hundreds of patche
    
[^39]: QASnowball: 一个用于高质量问答数据生成的迭代自举框架

    QASnowball: An Iterative Bootstrapping Framework for High-Quality Question-Answering Data Generation. (arXiv:2309.10326v1 [cs.CL])

    [http://arxiv.org/abs/2309.10326](http://arxiv.org/abs/2309.10326)

    QASnowball是一个迭代自举框架，可以根据有监督的样本种子集生成大规模高质量的QA数据，并通过重新种子化进行自我增强。在高资源英文场景中进行了实验。

    

    近年来，问题回答（QA）取得了成功，特别是其在应对各种自然语言处理任务中的潜力。然而，获取足够的数据来构建一个有效稳定的QA系统仍然是一个未解决的问题。针对这个问题，我们引入了一个迭代自举框架QASnowball，用于QA数据增强，它可以根据有监督的样本种子集迭代地生成大规模高质量的QA数据。具体而言，QASnowball包括三个模块：回答提取器，用于从无标签的文档中提取候选答案的核心短语；问题生成器，根据文档和候选答案生成问题；QA数据过滤器，用于过滤出高质量的QA数据。此外，QASnowball可以通过重新种子化种子集在不同迭代中进行自我增强，从而不断提高生成质量。我们在高资源英文场景中进行了实验。

    Recent years have witnessed the success of question answering (QA), especially its potential to be a foundation paradigm for tackling diverse NLP tasks. However, obtaining sufficient data to build an effective and stable QA system still remains an open problem. For this problem, we introduce an iterative bootstrapping framework for QA data augmentation (named QASnowball), which can iteratively generate large-scale high-quality QA data based on a seed set of supervised examples. Specifically, QASnowball consists of three modules, an answer extractor to extract core phrases in unlabeled documents as candidate answers, a question generator to generate questions based on documents and candidate answers, and a QA data filter to filter out high-quality QA data. Moreover, QASnowball can be self-enhanced by reseeding the seed set to fine-tune itself in different iterations, leading to continual improvements in the generation quality. We conduct experiments in the high-resource English scenario
    
[^40]: 通过多模态集成降维算法和分类算法对转移性乳腺癌进行预测

    Metastatic Breast Cancer Prognostication Through Multimodal Integration of Dimensionality Reduction Algorithms and Classification Algorithms. (arXiv:2309.10324v1 [cs.AI])

    [http://arxiv.org/abs/2309.10324](http://arxiv.org/abs/2309.10324)

    通过多模态集成降维算法和分类算法来预测转移性乳腺癌，提高转移性癌症的正确识别，并节省资源和时间。

    

    机器学习是人工智能的一个分支，计算机通过分析数据并发现数据中的模式。本研究侧重于使用机器学习来检测转移性癌症。转移性癌症是癌症扩散到身体其他部位的点，大约导致90%的癌症相关死亡。通常，病理学家每天要花几个小时手动分类肿瘤是良性还是恶性。这项繁琐的任务导致转移性肿瘤的错误标记高达60%以上，强调了意识到人为错误和其他低效性的重要性。机器学习是提高转移性癌症正确识别的一个很好的候选方法，能够挽救成千上万的生命，还能提高过程的速度和效率，从而节省资源和时间。到目前为止，深度学习方法已被用于研究中检测癌症。本研究是一种新颖的方法，旨在确定使用预处理算法与分类算法结合的潜力。

    Machine learning (ML) is a branch of Artificial Intelligence (AI) where computers analyze data and find patterns in the data. The study focuses on the detection of metastatic cancer using ML. Metastatic cancer is the point where the cancer has spread to other parts of the body and is the cause of approximately 90% of cancer related deaths. Normally, pathologists spend hours each day to manually classify whether tumors are benign or malignant. This tedious task contributes to mislabeling metastasis being over 60% of time and emphasizes the importance to be aware of human error, and other inefficiencies. ML is a good candidate to improve the correct identification of metastatic cancer saving thousands of lives and can also improve the speed and efficiency of the process thereby taking less resources and time. So far, deep learning methodology of AI has been used in the research to detect cancer. This study is a novel approach to determine the potential of using preprocessing algorithms c
    
[^41]: 谁可以信任，如何以及为什么：梳理人工智能伦理原则、可信度和信任

    Who to Trust, How and Why: Untangling AI Ethics Principles, Trustworthiness and Trust. (arXiv:2309.10318v1 [cs.AI])

    [http://arxiv.org/abs/2309.10318](http://arxiv.org/abs/2309.10318)

    人工智能信任不仅取决于系统本身，还包括对开发者的信任。人工智能伦理原则如可解释性和透明度对用户的信任影响尚不清晰。人工智能系统应被认识为社会技术系统，人与系统同样重要来确定其可信度。

    

    我们概述了关于人工智能信任和可信度的文献，并主张需要更清晰地区分这些概念，并收集更多关于人们信任行为的实证证据。我们讨论了信任人工智能不仅涉及对系统本身的依赖，还包括对人工智能系统的开发者的信任。人工智能伦理原则，如可解释性和透明度，经常被认为可以促进用户的信任，但这些特性实际上如何影响用户对系统可信度的感知的经验证据并不丰富或者不那么明确。人工智能系统应该被认识为社会技术系统，其中参与设计、开发、部署和使用系统的人与系统本身一样重要，以确定系统是否值得信赖。如果不认识到这些细微差别，对人工智能的信任和可信的人工智能就有可能成为任何对人工智能系统有益特性的模糊术语。

    We present an overview of the literature on trust in AI and AI trustworthiness and argue for the need to distinguish these concepts more clearly and to gather more empirically evidence on what contributes to people s trusting behaviours. We discuss that trust in AI involves not only reliance on the system itself, but also trust in the developers of the AI system. AI ethics principles such as explainability and transparency are often assumed to promote user trust, but empirical evidence of how such features actually affect how users perceive the system s trustworthiness is not as abundance or not that clear. AI systems should be recognised as socio-technical systems, where the people involved in designing, developing, deploying, and using the system are as important as the system for determining whether it is trustworthy. Without recognising these nuances, trust in AI and trustworthy AI risk becoming nebulous terms for any desirable feature for AI systems.
    
[^42]: 对多模态大规模语言模型中的灾难性遗忘进行的研究

    Investigating the Catastrophic Forgetting in Multimodal Large Language Models. (arXiv:2309.10313v1 [cs.CL])

    [http://arxiv.org/abs/2309.10313](http://arxiv.org/abs/2309.10313)

    本论文针对多模态大规模语言模型中的灾难性遗忘问题进行研究，引入了EMT方法来评估灾难性遗忘，并发现在标准图像分类任务上，几乎所有评估的模型都无法保持与视觉编码器相同的性能水平。研究结果表明，早期微调阶段对性能至关重要。

    

    在GPT4的成功之后，多模态大规模语言模型（MLLM）研究引起了广泛关注。这一研究方向侧重于通过微调预训练的LLM和视觉模型来开发通用的LLM。然而，灾难性遗忘，即微调模型无法保持与预训练模型相似的性能水平，仍然是多模态LLM（MLLM）中的一个固有问题。本文介绍了EMT：用于评估MLLM中灾难性遗忘的评估方法，将每个MLLM作为一个图像分类器进行评估。我们首先应用EMT来评估几个开源的微调MLLM，并发现几乎所有评估的MLLM在标准图像分类任务上无法保持与他们的视觉编码器相同的性能水平。此外，我们继续微调LLaVA，一种MLLM，并利用EMT来评估整个微调过程中的性能。有趣的是，我们的结果表明，早期的微调阶段是关键的，过早停止微调可能导致低性能的模型。

    Following the success of GPT4, there has been a surge in interest in multimodal large language model (MLLM) research. This line of research focuses on developing general-purpose LLMs through fine-tuning pre-trained LLMs and vision models. However, catastrophic forgetting, a notorious phenomenon where the fine-tuned model fails to retain similar performance compared to the pre-trained model, still remains an inherent problem in multimodal LLMs (MLLM). In this paper, we introduce EMT: Evaluating MulTimodality for evaluating the catastrophic forgetting in MLLMs, by treating each MLLM as an image classifier. We first apply EMT to evaluate several open-source fine-tuned MLLMs and we discover that almost all evaluated MLLMs fail to retain the same performance levels as their vision encoders on standard image classification tasks. Moreover, we continue fine-tuning LLaVA, an MLLM and utilize EMT to assess performance throughout the fine-tuning. Interestingly, our results suggest that early-sta
    
[^43]: 利用语音PTM、文本LLM和情感TTS进行语音情感识别

    Leveraging Speech PTM, Text LLM, and Emotional TTS for Speech Emotion Recognition. (arXiv:2309.10294v1 [cs.CL])

    [http://arxiv.org/abs/2309.10294](http://arxiv.org/abs/2309.10294)

    本文利用语音预训练模型、文本生成技术和语音合成技术提升了语音情感识别的效果，并通过实验证明了方法的有效性。

    

    本文探讨如何利用最先进的语音预训练模型(PTM)、data2vec、文本生成技术GPT-4以及语音合成技术Azure TTS来提升语音情感识别(SER)。首先，我们研究了不同的语音自监督预训练模型的表示能力，发现data2vec在SER任务上具有良好的表示能力。其次，我们采用了强大的大语言模型(GPT-4)和情感文本到语音(TTS)模型(Azure TTS)来生成情感一致的文本和语音。我们精心设计了文本提示和数据集构建，以获得质量高的合成情感语音数据。第三，我们研究了不同的数据增强方法来促进使用合成语音的SER任务，包括随机混合、对抗训练、迁移学习和课程学习。在IEMOCAP数据集上的实验和消融研究表明了我们方法的有效性。

    In this paper, we explored how to boost speech emotion recognition (SER) with the state-of-the-art speech pre-trained model (PTM), data2vec, text generation technique, GPT-4, and speech synthesis technique, Azure TTS. First, we investigated the representation ability of different speech self-supervised pre-trained models, and we found that data2vec has a good representation ability on the SER task. Second, we employed a powerful large language model (LLM), GPT-4, and emotional text-to-speech (TTS) model, Azure TTS, to generate emotionally congruent text and speech. We carefully designed the text prompt and dataset construction, to obtain the synthetic emotional speech data with high quality. Third, we studied different ways of data augmentation to promote the SER task with synthetic speech, including random mixing, adversarial training, transfer learning, and curriculum learning. Experiments and ablation studies on the IEMOCAP dataset demonstrate the effectiveness of our method, compar
    
[^44]: QXAI：可解释的人工智能框架用于患者监测系统中的定量分析

    QXAI: Explainable AI Framework for Quantitative Analysis in Patient Monitoring Systems. (arXiv:2309.10293v1 [cs.AI])

    [http://arxiv.org/abs/2309.10293](http://arxiv.org/abs/2309.10293)

    该论文提出了一种QXAI框架，用于患者监测系统中的定量分析。该框架具有可解释性，对于决策者在医疗应用中基于非线性模型结果进行决策非常重要。

    

    人工智能技术可用于对患者的身体活动进行分类和预测远程患者监测的生命体征。基于非线性模型（如深度学习模型）的回归分析由于其黑盒性质而具有有限的可解释性。这可能需要决策者在医疗应用中基于非线性模型结果盲目决策。在非侵入式监测中，跟踪传感器获取的患者数据和其相关临床特征作为预测未来生命体征的输入特征。解释各种特征对监测应用整体输出的贡献对于临床医师的决策至关重要。本研究提出了一种QXAI框架，用于监督学习方法中回归和分类任务的事后模型可解释性和内在可解释性。这是通过利用Shapley值进行实现的。

    Artificial Intelligence techniques can be used to classify a patient's physical activities and predict vital signs for remote patient monitoring. Regression analysis based on non-linear models like deep learning models has limited explainability due to its black-box nature. This can require decision-makers to make blind leaps of faith based on non-linear model results, especially in healthcare applications. In non-invasive monitoring, patient data from tracking sensors and their predisposing clinical attributes act as input features for predicting future vital signs. Explaining the contributions of various features to the overall output of the monitoring application is critical for a clinician's decision-making. In this study, an Explainable AI for Quantitative analysis (QXAI) framework is proposed with post-hoc model explainability and intrinsic explainability for regression and classification tasks in a supervised learning approach. This was achieved by utilizing the Shapley values c
    
[^45]: Koopman可逆自编码器：利用正向和反向动力学进行时间建模

    Koopman Invertible Autoencoder: Leveraging Forward and Backward Dynamics for Temporal Modeling. (arXiv:2309.10291v1 [cs.LG])

    [http://arxiv.org/abs/2309.10291](http://arxiv.org/abs/2309.10291)

    Koopman可逆自编码器（KIA）是一种基于Koopman算子理论的机器学习模型，通过建模正向和反向动力学来提高长期预测的准确性。它能够有效地学习低维表示，并保证了正向和逆向操作的可逆性和一致性。

    

    准确的长期预测是许多机器学习应用和决策过程的基础。然而，由于现有的时间模型（如循环神经网络）只能捕捉到训练数据中的统计关系，难以学习目标系统的潜在动力学，因此建立准确的长期预测模型仍然具有挑战性。为了解决这个问题，我们提出了一种基于Koopman算子理论的新型机器学习模型，称为Koopman可逆自编码器（KIA），它在无穷维希尔伯特空间中建模了正向和反向动力学，从而能够有效地学习低维表示，实现对长期系统行为的更准确预测。此外，我们方法的可逆设计保证了正向和逆向操作的可逆性和一致性。我们通过实验表明，KIA在多个时间序列预测任务中具有优异的性能。

    Accurate long-term predictions are the foundations for many machine learning applications and decision-making processes. However, building accurate long-term prediction models remains challenging due to the limitations of existing temporal models like recurrent neural networks (RNNs), as they capture only the statistical connections in the training data and may fail to learn the underlying dynamics of the target system. To tackle this challenge, we propose a novel machine learning model based on Koopman operator theory, which we call Koopman Invertible Autoencoders (KIA), that captures the inherent characteristic of the system by modeling both forward and backward dynamics in the infinite-dimensional Hilbert space. This enables us to efficiently learn low-dimensional representations, resulting in more accurate predictions of long-term system behavior. Moreover, our method's invertibility design guarantees reversibility and consistency in both forward and inverse operations. We illustra
    
[^46]: FRAMU: 基于注意力的联邦强化学习机器遗忘

    FRAMU: Attention-based Machine Unlearning using Federated Reinforcement Learning. (arXiv:2309.10283v1 [cs.LG])

    [http://arxiv.org/abs/2309.10283](http://arxiv.org/abs/2309.10283)

    FRAMU是一种基于注意力和联邦强化学习的机器遗忘框架，通过自适应学习机制、隐私保护技术和优化策略，处理各种数据源并保持准确性和隐私，适应波动的数据环境，支持持续模型演进。

    

    机器遗忘是一个新兴领域，通过允许从机器学习过程中删除私有或无关数据，解决数据隐私问题。使用过时的、私有的和无关的数据会引发与隐私和模型效率相关的挑战。这些问题不仅影响模型在机器学习和遗忘中的准确性和计算效率，还会对数据隐私造成威胁。为了解决这些挑战，我们引入了一种新颖的框架，即基于注意力的联邦强化学习机器遗忘（FRAMU）。该框架融合了自适应学习机制、隐私保护技术和优化策略，是处理各种数据源（单模态或多模态）同时保持准确性和隐私性的综合解决方案。FRAMU的优势在于其适应波动的数据环境、遗忘过时、私有或无关数据的能力，以及支持模型持续演进的支持。

    Machine Unlearning is an emerging field that addresses data privacy issues by enabling the removal of private or irrelevant data from the Machine Learning process. Challenges related to privacy and model efficiency arise from the use of outdated, private, and irrelevant data. These issues compromise both the accuracy and the computational efficiency of models in both Machine Learning and Unlearning. To mitigate these challenges, we introduce a novel framework, Attention-based Machine Unlearning using Federated Reinforcement Learning (FRAMU). This framework incorporates adaptive learning mechanisms, privacy preservation techniques, and optimization strategies, making it a well-rounded solution for handling various data sources, either single-modality or multi-modality, while maintaining accuracy and privacy. FRAMU's strength lies in its adaptability to fluctuating data landscapes, its ability to unlearn outdated, private, or irrelevant data, and its support for continual model evolution
    
[^47]: 具有增强课程强化学习的众包感知多智能体路径规划研究

    Crowd-Aware Multi-Agent Pathfinding With Boosted Curriculum Reinforcement Learning. (arXiv:2309.10275v1 [cs.RO])

    [http://arxiv.org/abs/2309.10275](http://arxiv.org/abs/2309.10275)

    该论文介绍了CRAMP，一种基于增强式课程强化学习的众包感知分散式路径规划方法，旨在解决拥挤环境下多智能体路径规划的困难。

    

    在拥挤环境中进行的多智能体路径规划是一个具有挑战性的运动规划问题，旨在为系统中的所有智能体找到无碰撞路径。多智能体路径规划在各个领域中都有广泛的应用，包括空中群体、自动化仓储机器人和自动驾驶车辆。当前的多智能体路径规划方法可以大致分为两种主要类别：集中式规划和分散式规划。集中式规划受到维度灾难的困扰，因此在大型和复杂环境中不具备良好的可扩展性。另一方面，分散式规划使智能体能够在部分可观察环境中进行实时路径规划，展示了隐式的协调能力。然而，在密集环境中它们的收敛速度较慢且性能下降。本文介绍了一种名为CRAMP的众包感知分散式方法，通过增强式课程引导的强化学习来解决这个问题。

    Multi-Agent Path Finding (MAPF) in crowded environments presents a challenging problem in motion planning, aiming to find collision-free paths for all agents in the system. MAPF finds a wide range of applications in various domains, including aerial swarms, autonomous warehouse robotics, and self-driving vehicles. The current approaches for MAPF can be broadly categorized into two main categories: centralized and decentralized planning. Centralized planning suffers from the curse of dimensionality and thus does not scale well in large and complex environments. On the other hand, decentralized planning enables agents to engage in real-time path planning within a partially observable environment, demonstrating implicit coordination. However, they suffer from slow convergence and performance degradation in dense environments. In this paper, we introduce CRAMP, a crowd-aware decentralized approach to address this problem by leveraging reinforcement learning guided by a boosted curriculum-b
    
[^48]: 使用无人表面船舶创建非可航运河流和其他浅水体的体积模型

    Using an Uncrewed Surface Vehicle to Create a Volumetric Model of Non-Navigable Rivers and Other Shallow Bodies of Water. (arXiv:2309.10269v1 [cs.RO])

    [http://arxiv.org/abs/2309.10269](http://arxiv.org/abs/2309.10269)

    使用无人表面船舶将浅水体的水下地形图与数字地表图合并，创建了一种统一的体积模型，以获取关于非可航运河流和其他浅水体所能承载的水体积的数据。

    

    非可航运河流和持水池在预防洪水方面发挥着重要作用，但是紧急规划人员常常没有关于它们在洪水前所能承载的水体积的数据。本文描述了一种实用的方法，利用无人海洋表面船只（USV）将浅水体的水下地形图与数字地表图合并，形成统一的体积模型。水下网格是通过将稀疏声纳深度读数应用于泊松表面重建算法来开发的。使用商业化的结构运动（SfM）软件包创建了河岸的密集的水上网格。合并是具有挑战性的，最重要的原因是传感器覆盖的间隙，即USV无法收集声纳深度数据或看到沙滩，导致兩個网格可能不相交。该方法在Hydronalix EMILY USV上进行了演示。

    Non-navigable rivers and retention ponds play important roles in buffering communities from flooding, yet emergency planners often have no data as to the volume of water that they can carry before flooding the surrounding. This paper describes a practical approach for using an uncrewed marine surface vehicle (USV) to collect and merge bathymetric maps with digital surface maps of the banks of shallow bodies of water into a unified volumetric model. The below-waterline mesh is developed by applying the Poisson surface reconstruction algorithm to the sparse sonar depth readings of the underwater surface. Dense above-waterline meshes of the banks are created using commercial structure from motion (SfM) packages. Merging is challenging for many reasons, the most significant is gaps in sensor coverage, i.e., the USV cannot collect sonar depth data or visually see sandy beaches leading to a bank thus the two meshes may not intersect. The approach is demonstrated on a Hydronalix EMILY USV wit
    
[^49]: 解释性人工智能解释揭示了飞溅液滴形态演化与施加冲击力之间的相关性

    Correlation between morphological evolution of splashing drop and exerted impact force revealed by interpretation of explainable artificial intelligence. (arXiv:2309.10266v1 [physics.flu-dyn])

    [http://arxiv.org/abs/2309.10266](http://arxiv.org/abs/2309.10266)

    本研究通过解释性人工智能分析，揭示了飞溅液滴形态演化与液滴对固体表面施加的冲击力之间的相关性，并发现权重矩阵元素的值与液滴形态的时间演化而变化。

    

    本研究通过解释性人工智能（XAI）视频分类器对飞溅和非飞溅液滴分类的特征提取方法和解释方法，揭示了飞溅形态与液滴对固体表面施加的标准化冲击力之间可能存在的相关性。值得注意的是，XAI的权重矩阵元素的值与液滴形态的时间演化而变化。我们计算了每帧对视频分类值的贡献的变化率，作为量化提取的飞溅和非飞溅特征在不同冲击时间对XAI模型分类的贡献的重要指标。值得注意的是，计算得到的飞溅特征的变化率与标准化冲击力的曲线非常匹配。

    This study reveals a possible correlation between splashing morphology and the normalized impact force exerted by an impacting drop on a solid surface. This finding is obtained from a newly proposed feature extraction method and a subsequent interpretation of the classification of splashing and non-splashing drops performed by an explainable artificial intelligence (XAI) video classifier. Notably, the values of the weight matrix elements of the XAI that correspond to the extracted features are found to change with the temporal evolution of the drop morphology. We compute the rate of change of the contributions of each frame with respect to the classification value of a video as an important index to quantify the contributions of the extracted splashing and non-splashing features at different impact times to the classification of the XAI model. Remarkably, the rate computed for the extracted splashing features is found to closely match the profile of the normalized impact force, where t
    
[^50]: LLM平台安全：将系统评估框架应用于OpenAI的ChatGPT插件

    LLM Platform Security: Applying a Systematic Evaluation Framework to OpenAI's ChatGPT Plugins. (arXiv:2309.10254v1 [cs.CR])

    [http://arxiv.org/abs/2309.10254](http://arxiv.org/abs/2309.10254)

    本文提出了一个框架，用于分析和改进当前和未来与插件集成的LLM平台的安全性、隐私和安全性。在应用框架于OpenAI的插件生态系统时，我们发现了一些具体证明了潜在问题的插件。

    

    近期，如ChatGPT等大型语言模型（LLM）平台开始提供插件生态系统，以与互联网上的第三方服务进行交互。虽然这些插件扩展了LLM平台的功能，但它们是由任意的第三方开发的，因此不能隐式信任。插件还使用自然语言与LLM平台和用户进行交互，这可能导致模糊的解释。本文提出了一个框架，为LLM平台设计者分析和改进当前和未来与插件集成的LLM平台的安全性、隐私和安全性奠定了基础。我们的框架是一个攻击分类法的表述，通过迭代地探索LLM平台相关方如何利用他们的能力和责任对彼此进行攻击来开发的。作为我们迭代过程的一部分，我们将我们的框架应用于OpenAI的插件生态系统。我们揭示了一些具体证明了潜在问题的插件。

    Large language model (LLM) platforms, such as ChatGPT, have recently begun offering a plugin ecosystem to interface with third-party services on the internet. While these plugins extend the capabilities of LLM platforms, they are developed by arbitrary third parties and thus cannot be implicitly trusted. Plugins also interface with LLM platforms and users using natural language, which can have imprecise interpretations. In this paper, we propose a framework that lays a foundation for LLM platform designers to analyze and improve the security, privacy, and safety of current and future plugin-integrated LLM platforms. Our framework is a formulation of an attack taxonomy that is developed by iteratively exploring how LLM platform stakeholders could leverage their capabilities and responsibilities to mount attacks against each other. As part of our iterative process, we apply our framework in the context of OpenAI's plugin ecosystem. We uncover plugins that concretely demonstrate the poten
    
[^51]: GPTFUZZER : 使用自动生成的越狱提示对大型语言模型进行红队测试

    GPTFUZZER : Red Teaming Large Language Models with Auto-Generated Jailbreak Prompts. (arXiv:2309.10253v1 [cs.AI])

    [http://arxiv.org/abs/2309.10253](http://arxiv.org/abs/2309.10253)

    GPTFUZZER是一种黑盒越狱模糊测试框架，自动生成用于红队测试大型语言模型的越狱模板。这种自动化方法避免了手工工程，并通过种子选择策略提高了效率。

    

    大型语言模型（LLMs）最近非常受欢迎，广泛用于日常对话到基于人工智能的编程。然而，尽管取得了巨大的成功，LLMs并不完全可靠，可能会提供有关进行有害或非法活动的详细指导。虽然安全措施可以减少这些输出的风险，但对抗性的"越狱"攻击仍然可以利用LLMs生成有害内容。这些越狱模板通常是手工精心制作的，使大规模测试具有挑战性。在本文中，我们介绍了一种新颖的黑盒越狱模糊测试框架\fuzzer，受AFL模糊测试框架的启发。与手工工程不同，\fuzzer自动化生成用于红队测试LLMs的越狱模板。在核心部分，\fuzzer从人工编写的模板作为种子开始，然后使用变异操作对其进行变异以生成新的模板。我们详细介绍了\fuzzer的三个关键组成部分：用于平衡效率的种子选择策略

    Large language models (LLMs) have recently experienced tremendous popularity and are widely used from casual conversations to AI-driven programming. However, despite their considerable success, LLMs are not entirely reliable and can give detailed guidance on how to conduct harmful or illegal activities. While safety measures can reduce the risk of such outputs, adversarial "jailbreak" attacks can still exploit LLMs to produce harmful content. These jailbreak templates are typically manually crafted, making large-scale testing challenging. In this paper, we introduce \fuzzer, a novel black-box jailbreak fuzzing framework inspired by AFL fuzzing framework. Instead of manual engineering, \fuzzer automates the generation of jailbreak templates for red-teaming LLMs. At its core, \fuzzer starts with human-written templates as seeds, then mutates them using mutate operators to produce new templates. We detail three key components of \fuzzer: a seed selection strategy for balancing efficiency 
    
[^52]: 关于深度生成模型中显式曲率正则化的研究

    On Explicit Curvature Regularization in Deep Generative Models. (arXiv:2309.10237v1 [cs.AI])

    [http://arxiv.org/abs/2309.10237](http://arxiv.org/abs/2309.10237)

    该论文提出了一种基于曲率的正则化方法，用于深度生成模型的学习。在任意数据流形的情况下，推导出了明确的坐标不变公式来计算内在和外在曲率。实验证明，基于曲率的正则化方法优于现有的自动编码器方法，其中内在曲率度量略优于外在曲率度量。

    

    我们提出了一族基于曲率的正则化项，用于深度生成模型的学习。针对嵌入在高维欧几里得空间中的任意数据流形，推导出了在内在和外在曲率度量下的明确的坐标不变公式。由于计算曲率是一个涉及到二阶导数求值的计算密集型过程，我们导出了用于近似评估内在和外在曲率的有效公式。通过比较研究了内在曲率和外在曲率的正则化度量的相对有效性，以及与现有自动编码器训练方法的性能比较。实验证明，基于曲率的方法优于现有的自动编码器正则化方法，且内在曲率度量略优于外在曲率度量。

    We propose a family of curvature-based regularization terms for deep generative model learning. Explicit coordinate-invariant formulas for both intrinsic and extrinsic curvature measures are derived for the case of arbitrary data manifolds embedded in higher-dimensional Euclidean space. Because computing the curvature is a highly computation-intensive process involving the evaluation of second-order derivatives, efficient formulas are derived for approximately evaluating intrinsic and extrinsic curvatures. Comparative studies are conducted that compare the relative efficacy of intrinsic versus extrinsic curvature-based regularization measures, as well as performance comparisons against existing autoencoder training methods. Experiments involving noisy motion capture data confirm that curvature-based methods outperform existing autoencoder regularization methods, with intrinsic curvature measures slightly more effective than extrinsic curvature measures.
    
[^53]: 在自动驾驶汽车中实现与人类类似的交互：以语言模型为基础

    Drive as You Speak: Enabling Human-Like Interaction with Large Language Models in Autonomous Vehicles. (arXiv:2309.10228v1 [cs.HC])

    [http://arxiv.org/abs/2309.10228](http://arxiv.org/abs/2309.10228)

    本文介绍了一个框架，利用大型语言模型（LLMs）提升自动驾驶汽车的决策过程，实现与人类类似的交互。该框架通过整合LLMs的自然语言能力和上下文理解、专业工具使用等，为自动驾驶汽车提供个性化辅助、持续学习和透明决策，从而改变自动驾驶汽车的操作方式。

    

    自动驾驶汽车的未来在于人本设计和先进的人工智能能力的融合。未来的自动驾驶汽车不仅会运送乘客，还会与其互动并适应他们的需求，使旅程舒适、高效、愉快。本文提出了一个新的框架，利用大型语言模型（LLMs）增强自动驾驶汽车的决策过程。通过整合LLMs的自然语言能力和上下文理解、专业工具使用、协同推理以及与自动驾驶汽车上的各种模块一起行动，该框架旨在无缝集成LLMs的高级语言和推理能力到自动驾驶汽车中。所提出的框架具有改变自动驾驶汽车操作方式的潜力，提供个性化辅助、持续学习和透明决策，从而为更安全、更高效的自动驾驶做出贡献。

    The future of autonomous vehicles lies in the convergence of human-centric design and advanced AI capabilities. Autonomous vehicles of the future will not only transport passengers but also interact and adapt to their desires, making the journey comfortable, efficient, and pleasant. In this paper, we present a novel framework that leverages Large Language Models (LLMs) to enhance autonomous vehicles' decision-making processes. By integrating LLMs' natural language capabilities and contextual understanding, specialized tools usage, synergizing reasoning, and acting with various modules on autonomous vehicles, this framework aims to seamlessly integrate the advanced language and reasoning capabilities of LLMs into autonomous vehicles. The proposed framework holds the potential to revolutionize the way autonomous vehicles operate, offering personalized assistance, continuous learning, and transparent decision-making, ultimately contributing to safer and more efficient autonomous driving t
    
[^54]: 多层次特征融合网络结合注意力机制的息肉分割

    Multi-level feature fusion network combining attention mechanisms for polyp segmentation. (arXiv:2309.10219v1 [cs.CV])

    [http://arxiv.org/abs/2309.10219](http://arxiv.org/abs/2309.10219)

    提出了一种名为 MLFF-Net 的多层次特征融合网络，结合注意力机制实现了对息肉分割的改进。对编码器提取的特征进行了滤波和利用，并解决了特征融合引起的语义冲突和信息冗余问题。

    

    临床上，自动化的息肉分割技术有潜力显著提高医学诊断的效率和准确性，从而降低患者患结直肠癌的风险。然而，现有方法存在两个显著弱点可能影响分割的准确性。首先，编码器提取的特征未经充分滤波和利用。其次，由特征融合引起的语义冲突和信息冗余没有得到关注。为了克服这些局限性，我们提出了一种新的息肉分割方法，名为 MLFF-Net，它利用多层次特征融合和注意力机制。具体而言，MLFF-Net 包括三个模块：多尺度注意力模块（MAM）、高层特征增强模块（HFEM）和全局注意力模块（GAM）。其中，MAM 用于从编码器的浅层输出中提取多尺度信息和息肉细节。在 HFEM 中，编码器的深层特征可以被进一步增强。

    Clinically, automated polyp segmentation techniques have the potential to significantly improve the efficiency and accuracy of medical diagnosis, thereby reducing the risk of colorectal cancer in patients. Unfortunately, existing methods suffer from two significant weaknesses that can impact the accuracy of segmentation. Firstly, features extracted by encoders are not adequately filtered and utilized. Secondly, semantic conflicts and information redundancy caused by feature fusion are not attended to. To overcome these limitations, we propose a novel approach for polyp segmentation, named MLFF-Net, which leverages multi-level feature fusion and attention mechanisms. Specifically, MLFF-Net comprises three modules: Multi-scale Attention Module (MAM), High-level Feature Enhancement Module (HFEM), and Global Attention Module (GAM). Among these, MAM is used to extract multi-scale information and polyp details from the shallow output of the encoder. In HFEM, the deep features of the encoders
    
[^55]: 一项关于语义分割注意力网络的实证研究

    An Empirical Study of Attention Networks for Semantic Segmentation. (arXiv:2309.10217v1 [cs.CV])

    [http://arxiv.org/abs/2309.10217](http://arxiv.org/abs/2309.10217)

    本研究对语义分割中的注意力网络进行了实证研究，分析了它们的计算复杂性和性能，总结了适用的方法。

    

    语义分割是计算机视觉中一个重要的问题。最近，用于语义分割的端到端卷积神经网络成为了一种常见的解决方案，其准确性比传统方法更高。近期，基于注意力的解码器在各种数据集上取得了最先进的性能。然而，这些网络经常与以前的最先进网络的mIoU进行比较，以证明它们的优越性，而忽视了它们在各个类别中的计算复杂性和精度等特性，这对工程应用来说是至关重要的。此外，不同网络之间对FLOPs和内存进行分析的方法不一致，使得比较难以利用。此外，各种方法在语义分割中利用了注意力，但这些方法的结论却不明确。本文首先进行实验，分析它们的计算复杂性并比较它们的性能。然后总结了适合的方法。

    Semantic segmentation is a vital problem in computer vision. Recently, a common solution to semantic segmentation is the end-to-end convolution neural network, which is much more accurate than traditional methods.Recently, the decoders based on attention achieve state-of-the-art (SOTA) performance on various datasets. But these networks always are compared with the mIoU of previous SOTA networks to prove their superiority and ignore their characteristics without considering the computation complexity and precision in various categories, which is essential for engineering applications. Besides, the methods to analyze the FLOPs and memory are not consistent between different networks, which makes the comparison hard to be utilized. What's more, various methods utilize attention in semantic segmentation, but the conclusion of these methods is lacking. This paper first conducts experiments to analyze their computation complexity and compare their performance. Then it summarizes suitable sc
    
[^56]: 通过屏蔽实现安全的POMDP在线规划

    Safe POMDP Online Planning via Shielding. (arXiv:2309.10216v1 [cs.AI])

    [http://arxiv.org/abs/2309.10216](http://arxiv.org/abs/2309.10216)

    通过计算屏蔽动作来实现安全的POMDP在线规划。四种屏蔽方法。

    

    部分可观察的马尔可夫决策过程（POMDP）广泛应用于许多机器人应用中，用于在不确定性下进行序列决策。POMDP在线规划算法，如部分可观察的蒙特卡洛规划（POMCP），可以解决目标为最大化预期回报的大型POMDP。但是，由此产生的策略无法提供对于现实世界中安全关键任务（如自动驾驶）至关重要的安全保证。在本文中，我们考虑安全要求，将其表示为几乎一定的到达-避免规范（即，达到一组目标状态的概率为1，达到一组不安全状态的概率为0）。我们计算限制违反几乎一定到达-避免规范的不安全动作的屏蔽。然后，将这些屏蔽集成到POMCP算法中以实现安全的POMDP在线规划。我们提出了四种不同的屏蔽方法，根据屏蔽的计算和集成方式的不同，包括分解方式。

    Partially observable Markov decision processes (POMDPs) have been widely used in many robotic applications for sequential decision-making under uncertainty. POMDP online planning algorithms such as Partially Observable Monte-Carlo Planning (POMCP) can solve very large POMDPs with the goal of maximizing the expected return. But the resulting policies cannot provide safety guarantees that are imperative for real-world safety-critical tasks (e.g., autonomous driving). In this work, we consider safety requirements represented as almost-sure reach-avoid specifications (i.e., the probability to reach a set of goal states is one and the probability to reach a set of unsafe states is zero). We compute shields that restrict unsafe actions violating almost-sure reach-avoid specifications. We then integrate these shields into the POMCP algorithm for safe POMDP online planning. We propose four distinct shielding methods, differing in how the shields are computed and integrated, including factored 
    
[^57]: 面向未知领域的有效语义OOD检测：域泛化视角

    Towards Effective Semantic OOD Detection in Unseen Domains: A Domain Generalization Perspective. (arXiv:2309.10209v1 [cs.AI])

    [http://arxiv.org/abs/2309.10209](http://arxiv.org/abs/2309.10209)

    本研究提出了一个新的问题，即面向未知领域的语义OOD检测，通过引入域泛化正则化和OOD检测正则化策略，同时解决了协变量偏移和语义偏移问题。实验证明，该框架在三个标准域泛化基准上表现优于传统方法。

    

    机器学习中两种常见的分布偏移是协变量偏移（观察到不同领域之间的差异）和语义偏移（观察到不同类别之间的差异）。传统的OOD检测技术通常只处理其中一种偏移。然而，真实世界的测试环境通常同时存在协变量偏移和语义偏移。在本研究中，我们引入了一个新的问题，即跨领域的语义OOD检测，同时解决了这两种分布偏移。为此，我们引入了两种正则化策略：域泛化正则化，确保域间语义不变以对抗协变量偏移，以及OOD检测正则化，通过能量限制来增强对语义偏移的OOD检测能力。通过在三个标准域泛化基准上进行严格测试，我们的提出的框架展示了其优于传统域泛化应用的优越性。

    Two prevalent types of distributional shifts in machine learning are the covariate shift (as observed across different domains) and the semantic shift (as seen across different classes). Traditional OOD detection techniques typically address only one of these shifts. However, real-world testing environments often present a combination of both covariate and semantic shifts. In this study, we introduce a novel problem, semantic OOD detection across domains, which simultaneously addresses both distributional shifts. To this end, we introduce two regularization strategies: domain generalization regularization, which ensures semantic invariance across domains to counteract the covariate shift, and OOD detection regularization, designed to enhance OOD detection capabilities against the semantic shift through energy bounding. Through rigorous testing on three standard domain generalization benchmarks, our proposed framework showcases its superiority over conventional domain generalization app
    
[^58]: 通过优势模型和选择性回放稳定RLHF

    Stabilizing RLHF through Advantage Model and Selective Rehearsal. (arXiv:2309.10202v1 [cs.CL])

    [http://arxiv.org/abs/2309.10202](http://arxiv.org/abs/2309.10202)

    本论文提出了通过优势模型和选择性回放来稳定RLHF训练的两种创新方法，成功地解决了奖励欺骗和灾难性遗忘等不稳定性问题，并在实验中取得了更高的奖励得分和胜率。

    

    大型语言模型（LLM）已经在自然语言处理领域产生了革命性的影响，然而，通过RLHF将这些模型与人类价值观和偏好对齐仍然是一个重大挑战。这个挑战表现为各种不稳定性，比如奖励欺骗和灾难性遗忘。在这个技术报告中，我们提出了两项创新来稳定RLHF训练: 1) 优势模型，直接建模优势得分，即相对于期望奖励的额外奖励，并通过调节得分分布来防止奖励欺骗。2) 选择性回放，通过有策略地选择数据进行PPO训练和知识回放，从而减轻灾难性遗忘。我们在公开和专有数据集上进行的实验分析表明，所提出的方法不仅在RLHF训练中增加了稳定性，而且实现了更高的奖励得分和胜率。

    Large Language Models (LLMs) have revolutionized natural language processing, yet aligning these models with human values and preferences using RLHF remains a significant challenge. This challenge is characterized by various instabilities, such as reward hacking and catastrophic forgetting. In this technical report, we propose two innovations to stabilize RLHF training: 1) Advantage Model, which directly models advantage score i.e., extra reward compared to the expected rewards and regulates score distributions across tasks to prevent reward hacking. 2) Selective Rehearsal, which mitigates catastrophic forgetting by strategically selecting data for PPO training and knowledge rehearsing. Our experimental analysis on public and proprietary datasets reveals that the proposed methods not only increase stability in RLHF training but also achieve higher reward scores and win rates.
    
[^59]: 基于图增强的自适应智能时间序列预测强化学习

    Graph-enabled Reinforcement Learning for Time Series Forecasting with Adaptive Intelligence. (arXiv:2309.10186v1 [cs.LG])

    [http://arxiv.org/abs/2309.10186](http://arxiv.org/abs/2309.10186)

    本研究提出了一种使用图神经网络和强化学习监控的新方法，以更准确地预测时间序列数据。该方法能够克服深度学习的限制，并捕捉复杂的时序结构中的时间依赖关系，适用于医疗、交通和天气预测等领域。

    

    强化学习以其能够自适应地建模序列任务和学习潜在数据模式的能力而闻名。深度学习模型在回归和分类任务中得到了广泛的探索和应用。然而，深度学习存在一些限制，例如假设数据等间隔有序以及无法充分融入图结构等。图神经网络(GNN)能够克服这些挑战并捕捉时间序列数据的时间依赖关系。 在本研究中，我们提出了一种使用GNN和强化学习监控预测时间序列数据的新方法。GNN能够显式地将数据的图结构纳入模型，使其能够以更自然的方式捕捉时间依赖关系。该方法能够在复杂的时序结构中进行更准确的预测，例如医疗、交通和天气预测中的时序数据。

    Reinforcement learning is well known for its ability to model sequential tasks and learn latent data patterns adaptively. Deep learning models have been widely explored and adopted in regression and classification tasks. However, deep learning has its limitations such as the assumption of equally spaced and ordered data, and the lack of ability to incorporate graph structure in terms of time-series prediction. Graphical neural network (GNN) has the ability to overcome these challenges and capture the temporal dependencies in time-series data. In this study, we propose a novel approach for predicting time-series data using GNN and monitoring with Reinforcement Learning (RL). GNNs are able to explicitly incorporate the graph structure of the data into the model, allowing them to capture temporal dependencies in a more natural way. This approach allows for more accurate predictions in complex temporal structures, such as those found in healthcare, traffic and weather forecasting. We also 
    
[^60]: 基于云-网络集成的超越5G中的QoS感知服务预测与编排

    QoS-Aware Service Prediction and Orchestration in Cloud-Network Integrated Beyond 5G. (arXiv:2309.10185v1 [cs.NI])

    [http://arxiv.org/abs/2309.10185](http://arxiv.org/abs/2309.10185)

    本论文提出了一个针对网络-云集成环境中的服务部署和资源分配的非线性规划模型，考虑了容量限制、动态用户和端到端延迟，并解决了超越5G中的服务连续性问题。

    

    诸如元宇宙等新型应用突显了超越5G网络的潜力，这需要超低延迟通信和大规模的宽带连接。此外，伴随着用户数量不断波动的这类服务的蓬勃需求，使得在B5G中需要更加关注服务连续性。为了实现这些服务，边缘云模式是利用云容量并实时有效地管理用户在网络中移动的潜在解决方案。然而，边缘云网络面临着诸多限制，包括必须集体管理网络和计算资源以释放其全部潜力。本文在考虑容量限制、动态用户和端到端延迟的情况下，解决了网络-云集成环境中服务部署和资源分配的联合问题。我们提出了一个非线性规划模型，该模型制定了优化问题的目标。

    Novel applications such as the Metaverse have highlighted the potential of beyond 5G networks, which necessitate ultra-low latency communications and massive broadband connections. Moreover, the burgeoning demand for such services with ever-fluctuating users has engendered a need for heightened service continuity consideration in B5G. To enable these services, the edge-cloud paradigm is a potential solution to harness cloud capacity and effectively manage users in real time as they move across the network. However, edge-cloud networks confront a multitude of limitations, including networking and computing resources that must be collectively managed to unlock their full potential. This paper addresses the joint problem of service placement and resource allocation in a network-cloud integrated environment while considering capacity constraints, dynamic users, and end-to-end delays. We present a non-linear programming model that formulates the optimization problem with the aiming objectiv
    
[^61]: 音乐产品的正面和风险信息评估

    Positive and Risky Message Assessment for Music Products. (arXiv:2309.10182v1 [cs.CL])

    [http://arxiv.org/abs/2309.10182](http://arxiv.org/abs/2309.10182)

    这项研究提出了一个新的问题：如何评估音乐产品中的正面和风险信息。研究者提出了一个多任务预测模型，通过序数约束解决这个问题，并且取得了显著优于其他方法的结果。

    

    在这项工作中，我们提出了一个新颖的研究问题：评估音乐产品中的正面和风险信息。我们首先建立了一个多角度多级音乐内容评估的基准，然后提出了一种有效的多任务预测模型，并通过序数约束来解决这个问题。我们的结果显示，所提出的方法不仅明显优于强大的针对特定任务的对应方法，而且可以同时评估多个方面。

    In this work, we propose a novel research problem: assessing positive and risky messages from music products. We first establish a benchmark for multi-angle multi-level music content assessment and then present an effective multi-task prediction model with ordinality-enforcement to solve this problem. Our result shows the proposed method not only significantly outperforms strong task-specific counterparts but can concurrently evaluate multiple aspects.
    
[^62]: 基于双深度Q学习的低延迟5G应用路径选择和服务部署

    Double Deep Q-Learning-based Path Selection and Service Placement for Latency-Sensitive Beyond 5G Applications. (arXiv:2309.10180v1 [cs.NI])

    [http://arxiv.org/abs/2309.10180](http://arxiv.org/abs/2309.10180)

    通过研究通信和计算资源分配的联合问题，本文提出了两种方法，即B&B-CCRA和WF-CCRA，以最小化总成本。

    

    如今，随着对容量的需求不断增长，全新的服务正在涌现。为了以实时响应和可扩展的方式提供这些服务，需要一个坚实的云网络集成基础设施。由于通信和计算资源的多样性和有限容量，必须协同管理这些资源以发挥它们的全部潜力。尽管已经提出了一些创新方法来编排这些资源，但大多数方法都忽略了网络资源或将网络简化为简单的图，只关注云资源。本文通过研究通信和计算资源分配的联合问题，称为CCRA，包括功能部署和分配、流量优先级和路径选择，考虑容量限制和质量要求，以最小化总成本。我们将问题形式化为非线性规划模型，并提出了两种方法，即B&B-CCRA和WF-CCRA，基于Br...

    Nowadays, as the need for capacity continues to grow, entirely novel services are emerging. A solid cloud-network integrated infrastructure is necessary to supply these services in a real-time responsive, and scalable way. Due to their diverse characteristics and limited capacity, communication and computing resources must be collaboratively managed to unleash their full potential. Although several innovative methods have been proposed to orchestrate the resources, most ignored network resources or relaxed the network as a simple graph, focusing only on cloud resources. This paper fills the gap by studying the joint problem of communication and computing resource allocation, dubbed CCRA, including function placement and assignment, traffic prioritization, and path selection considering capacity constraints and quality requirements, to minimize total cost. We formulate the problem as a non-linear programming model and propose two approaches, dubbed B\&B-CCRA and WF-CCRA, based on the Br
    
[^63]: 自我维持的连续深度强化学习在动态元宇宙应用中的多重接入

    Self-Sustaining Multiple Access with Continual Deep Reinforcement Learning for Dynamic Metaverse Applications. (arXiv:2309.10177v1 [cs.NI])

    [http://arxiv.org/abs/2309.10177](http://arxiv.org/abs/2309.10177)

    本文研究了动态元宇宙应用中的多重接入问题，在采用自适应人工智能和深度强化学习的基础上，通过连续训练来实现自我维持的策略。该研究填补了当前文献中对于适应非稳态环境的代理机制问题的研究空白。

    

    元宇宙是一个旨在创建由众多世界组成的虚拟环境的新范 Paradigm，其中每个世界将提供不同的服务。为了应对如此动态和复杂的场景，并考虑到针对第六代通信系统（6G）的严格服务质量要求，一种潜在的方法是采用自我维持策略，通过使用自适应人工智能（自适应AI）来实现，其中模型将不断地根据新的数据和条件进行重新训练。自我维持的一个方面是对频谱的多重接入进行管理。虽然已经提出了几种创新的方法来解决这个挑战，大多数方法使用深度强化学习（DRL），但是适应非稳态环境的代理机制问题尚未得到准确解决。本文通过研究多通道环境中的多重接入问题，填补了当前文献中的空白。

    The Metaverse is a new paradigm that aims to create a virtual environment consisting of numerous worlds, each of which will offer a different set of services. To deal with such a dynamic and complex scenario, considering the stringent quality of service requirements aimed at the 6th generation of communication systems (6G), one potential approach is to adopt self-sustaining strategies, which can be realized by employing Adaptive Artificial Intelligence (Adaptive AI) where models are continually re-trained with new data and conditions. One aspect of self-sustainability is the management of multiple access to the frequency spectrum. Although several innovative methods have been proposed to address this challenge, mostly using Deep Reinforcement Learning (DRL), the problem of adapting agents to a non-stationary environment has not yet been precisely addressed. This paper fills in the gap in the current literature by investigating the problem of multiple access in multi-channel environment
    
[^64]: 一次性演示行为克隆与动作分块转换的一人剧本

    One ACT Play: Single Demonstration Behavior Cloning with Action Chunking Transformers. (arXiv:2309.10175v1 [cs.RO])

    [http://arxiv.org/abs/2309.10175](http://arxiv.org/abs/2309.10175)

    本研究提出了一种通过行为克隆学习任务的方法，只需要一次人类演示，并使用线性变换生成多样化的轨迹，成功完成了三个块操作任务。此外，还提出了一种新的暂存方法，用于动作块代理的行动预测。

    

    从人类示范中学习（行为克隆）是机器人学习的基石。然而，大多数行为克隆算法需要大量示范来学习一个任务，尤其是对于具有多样化初始条件的一般任务。然而，人类只需看到一两个示范就能学会完成任务，即使是复杂的任务。我们的工作旨在模拟这种能力，仅凭一次人类演示就使用行为克隆学习一个任务。我们通过使用线性变换来增强单个演示，生成一系列适用于各种初始条件的轨迹，实现了这个目标。通过这些示范，我们能够训练一个行为克隆代理成功完成三个块操作任务。此外，在推理过程中，我们还开发了一种新的暂存方法，用于动作块代理的行动预测中综合标准差。

    Learning from human demonstrations (behavior cloning) is a cornerstone of robot learning. However, most behavior cloning algorithms require a large number of demonstrations to learn a task, especially for general tasks that have a large variety of initial conditions. Humans, however, can learn to complete tasks, even complex ones, after only seeing one or two demonstrations. Our work seeks to emulate this ability, using behavior cloning to learn a task given only a single human demonstration. We achieve this goal by using linear transforms to augment the single demonstration, generating a set of trajectories for a wide range of initial conditions. With these demonstrations, we are able to train a behavior cloning agent to successfully complete three block manipulation tasks. Additionally, we developed a novel addition to the temporal ensembling method used by action chunking agents during inference. By incorporating the standard deviation of the action predictions into the ensembling m
    
[^65]: 异步感知-动作-通信与图神经网络

    Asynchronous Perception-Action-Communication with Graph Neural Networks. (arXiv:2309.10164v1 [cs.RO])

    [http://arxiv.org/abs/2309.10164](http://arxiv.org/abs/2309.10164)

    该论文提出了使用图神经网络实现异步感知-动作-通信的方法，解决了在大型机器人群体中协作和通信的挑战。现有的框架假设顺序执行，该方法是完全分散的，但在评估和部署方面仍存在一些限制。

    

    在大型机器人群体中实现共同的全局目标的协作是一个具有挑战性的问题，因为机器人的感知和通信能力有限。机器人必须执行感知-动作-通信（PAC）循环-它们感知局部环境，与其他机器人通信，并实时采取行动。分散的PAC系统面临的一个基本挑战是决定与相邻机器人通信的信息以及如何在利用邻居共享的信息的同时采取行动。最近，使用图神经网络（GNNs）来解决这个问题已经取得了一些进展，比如在群集和覆盖控制等应用中。虽然在概念上，GNN策略是完全分散的，但评估和部署这样的策略主要仍然是集中式的或具有限制性的分散式。此外，现有的框架假设感知和动作推理的顺序执行，这在现实世界的应用中非常限制性。

    Collaboration in large robot swarms to achieve a common global objective is a challenging problem in large environments due to limited sensing and communication capabilities. The robots must execute a Perception-Action-Communication (PAC) loop -- they perceive their local environment, communicate with other robots, and take actions in real time. A fundamental challenge in decentralized PAC systems is to decide what information to communicate with the neighboring robots and how to take actions while utilizing the information shared by the neighbors. Recently, this has been addressed using Graph Neural Networks (GNNs) for applications such as flocking and coverage control. Although conceptually, GNN policies are fully decentralized, the evaluation and deployment of such policies have primarily remained centralized or restrictively decentralized. Furthermore, existing frameworks assume sequential execution of perception and action inference, which is very restrictive in real-world applica
    
[^66]: RadOnc-GPT：一种用于放射肿瘤学的大型语言模型

    RadOnc-GPT: A Large Language Model for Radiation Oncology. (arXiv:2309.10160v1 [physics.med-ph])

    [http://arxiv.org/abs/2309.10160](http://arxiv.org/abs/2309.10160)

    RadOnc-GPT是一种专门用于放射肿瘤学的大型语言模型，通过先进的调整方法进行微调，在生成治疗方案、确定疗法和提供诊断描述等关键任务上具有显著改进，展示了利用领域特定知识进行微调的大型语言模型在高度专业化的医疗领域具有的潜力。

    

    本论文介绍了一种名为RadOnc-GPT的大型语言模型，通过先进的调整方法专门用于放射肿瘤学。RadOnc-GPT在Mayo Clinic的大量放射肿瘤学患者记录和临床笔记数据集上进行了微调。该模型通过三个关键任务进行指令调整，包括生成放射治疗方案、确定最佳放射疗法以及基于患者诊断细节提供诊断描述/ICD代码。通过将放射肿瘤学医生比较RadOnc-GPT的印象与通用大型语言模型的印象进行评估，研究表明RadOnc-GPT生成的输出在清晰度、特异度和临床相关性方面显著提高。该研究证明了利用像RadOnc-GPT这样利用领域特定知识进行微调的大型语言模型在放射肿瘤学等高度专业化的医疗领域实现变革能力的潜力。

    This paper presents RadOnc-GPT, a large language model specialized for radiation oncology through advanced tuning methods. RadOnc-GPT was finetuned on a large dataset of radiation oncology patient records and clinical notes from the Mayo Clinic. The model employs instruction tuning on three key tasks generating radiotherapy treatment regimens, determining optimal radiation modalities, and providing diagnostic descriptions/ICD codes based on patient diagnostic details. Evaluations conducted by having radiation oncologists compare RadOnc-GPT impressions to general large language model impressions showed that RadOnc-GPT generated outputs with significantly improved clarity, specificity, and clinical relevance. The study demonstrated the potential of using large language models fine-tuned using domain-specific knowledge like RadOnc-GPT to achieve transformational capabilities in highly specialized healthcare fields such as radiation oncology.
    
[^67]: AI代理的记忆和泛化能力分析：连续学习者是否具有鲁棒性？

    Analysis of the Memorization and Generalization Capabilities of AI Agents: Are Continual Learners Robust?. (arXiv:2309.10149v1 [cs.LG])

    [http://arxiv.org/abs/2309.10149](http://arxiv.org/abs/2309.10149)

    本文提出了一种新的连续学习框架，旨在在动态环境下实现鲁棒的泛化能力并保留过去的知识。通过使用有限容量的内存来保存先前观察到的环境信息，并从内存中采样数据点来获得对未知变化鲁棒的预测器。该分析展示了记忆和泛化之间的权衡，而实验证明了所提出的算法的优越性。

    

    在连续学习中，AI代理（例如自动驾驶车辆或机器人）在动态环境下从非稳态数据流中学习。对于这类应用的实际部署，保证对未知环境的鲁棒性以及保留过去的经验是非常重要的。本文提出了一种新的连续学习框架，旨在在动态环境下实现鲁棒的泛化能力并保留过去的知识。考虑到连续学习代理使用有限容量的内存来保存先前观察到的环境信息，以减轻遗忘问题。然后，从内存中采样数据点，以估计环境变化的风险分布，从而得到对未知变化具有鲁棒性的预测器。对所提出的框架的泛化和记忆性能进行了理论分析。该分析展示了随内存大小的记忆和泛化之间的权衡。实验证明了所提出的算法的优越性。

    In continual learning (CL), an AI agent (e.g., autonomous vehicles or robotics) learns from non-stationary data streams under dynamic environments. For the practical deployment of such applications, it is important to guarantee robustness to unseen environments while maintaining past experiences. In this paper, a novel CL framework is proposed to achieve robust generalization to dynamic environments while retaining past knowledge. The considered CL agent uses a capacity-limited memory to save previously observed environmental information to mitigate forgetting issues. Then, data points are sampled from the memory to estimate the distribution of risks over environmental change so as to obtain predictors that are robust with unseen changes. The generalization and memorization performance of the proposed framework are theoretically analyzed. This analysis showcases the tradeoff between memorization and generalization with the memory size. Experiments show that the proposed algorithm outpe
    
[^68]: 使用深度学习的人体步态识别：综述

    Human Gait Recognition using Deep Learning: A Comprehensive Review. (arXiv:2309.10144v1 [cs.CV])

    [http://arxiv.org/abs/2309.10144](http://arxiv.org/abs/2309.10144)

    这篇论文总结了使用深度学习进行人体步态识别的研究，探讨了其在不同环境中的应用、优势以及可能遇到的挑战。

    

    步态识别（GR）是通过视觉摄像头从远处识别人员的生物测定模式。GR提供了一个安全可靠的替代方案，用于指纹和面部识别，因为很难区分假象和真实信号。此外，它对欺诈的抵抗力使得GR适用于各种环境。随着深度学习的兴起，GR技术在各种环境中取得了令人期待的成果。随着视频监控的普及，新的障碍不断出现，例如确保在不同协议中进行统一性能评估，可靠的识别尽管光照条件变化，步态模式波动以及保护隐私。本综述旨在概述GR并分析可能影响其环境要素和复杂性，相对于其他生物测定系统。主要目标是研究现有的深度学习（DL）技术。

    Gait recognition (GR) is a growing biometric modality used for person identification from a distance through visual cameras. GR provides a secure and reliable alternative to fingerprint and face recognition, as it is harder to distinguish between false and authentic signals. Furthermore, its resistance to spoofing makes GR suitable for all types of environments. With the rise of deep learning, steadily improving strides have been made in GR technology with promising results in various contexts. As video surveillance becomes more prevalent, new obstacles arise, such as ensuring uniform performance evaluation across different protocols, reliable recognition despite shifting lighting conditions, fluctuations in gait patterns, and protecting privacy.This survey aims to give an overview of GR and analyze the environmental elements and complications that could affect it in comparison to other biometric recognition systems. The primary goal is to examine the existing deep learning (DL) techni
    
[^69]: 高效的低秩图神经网络抵御结构攻击

    Efficient Low-Rank GNN Defense Against Structural Attacks. (arXiv:2309.10136v1 [cs.LG])

    [http://arxiv.org/abs/2309.10136](http://arxiv.org/abs/2309.10136)

    提出了一种高效的低秩图神经网络（ELR-GNN）防御方法，通过学习低秩和稀疏的图结构抵御对抗攻击，并在更高的效率下实现有效的防御。

    

    图神经网络（GNNs）在图数据上展现出强大的表示能力。然而，GNNs容易受到对抗攻击的影响，即使对图结构进行微小扰动也会严重影响它们的性能。现有方法要么对复杂攻击无效，要么需要优化稠密邻接矩阵，这非常耗时且容易陷入局部最优。为了解决这个问题，我们提出了一种高效的低秩图神经网络（ELR-GNN）防御方法，旨在学习低秩和稀疏的图结构来抵御对抗攻击，保证有效的防御与更高的效率。具体而言，ELR-GNN包括两个模块：粗糙低秩估计模块和细粒度估计模块。第一个模块采用截断奇异值分解（SVD）来初始化低秩邻接矩阵的估计，这作为优化低秩邻接矩阵的起点。

    Graph Neural Networks (GNNs) have been shown to possess strong representation abilities over graph data. However, GNNs are vulnerable to adversarial attacks, and even minor perturbations to the graph structure can significantly degrade their performance. Existing methods either are ineffective against sophisticated attacks or require the optimization of dense adjacency matrices, which is time-consuming and prone to local minima. To remedy this problem, we propose an Efficient Low-Rank Graph Neural Network (ELR-GNN) defense method, which aims to learn low-rank and sparse graph structures for defending against adversarial attacks, ensuring effective defense with greater efficiency. Specifically, ELR-GNN consists of two modules: a Coarse Low-Rank Estimation Module and a Fine-Grained Estimation Module. The first module adopts the truncated Singular Value Decomposition (SVD) to initialize the low-rank adjacency matrix estimation, which serves as a starting point for optimizing the low-rank 
    
[^70]: GDM: 双重Mixup用于有限标注的图分类

    GDM: Dual Mixup for Graph Classification with Limited Supervision. (arXiv:2309.10134v1 [cs.LG])

    [http://arxiv.org/abs/2309.10134](http://arxiv.org/abs/2309.10134)

    GDM是一种双重的mixup方法，利用图实例的功能和结构信息生成新的标记图样本。

    

    图神经网络(GNNs)对于图分类任务需要大量的标记图样本以获得良好的性能。随着标记图样本数量的减少，GNNs的性能显著下降。为了降低标注成本，开发能够生成新的图实例以增加可用标记图样本数量和多样性的图增强方法非常重要。在这项工作中，我们提出了一种新颖的基于mixup的图增强方法，称为Graph Dual Mixup (GDM)，它利用图实例的功能和结构信息来生成新的标记图样本。GDM使用图结构自动编码器来学习图样本的结构嵌入，然后在学到的结构嵌入空间中应用mixup到图的结构信息，并从mixup结构嵌入中生成新的图结构。至于功能信息，GDM应用...

    Graph Neural Networks (GNNs) require a large number of labeled graph samples to obtain good performance on the graph classification task. The performance of GNNs degrades significantly as the number of labeled graph samples decreases. To reduce the annotation cost, it is therefore important to develop graph augmentation methods that can generate new graph instances to increase the size and diversity of the limited set of available labeled graph samples. In this work, we propose a novel mixup-based graph augmentation method, Graph Dual Mixup (GDM), that leverages both functional and structural information of the graph instances to generate new labeled graph samples. GDM employs a graph structural auto-encoder to learn structural embeddings of the graph samples, and then applies mixup to the structural information of the graphs in the learned structural embedding space and generates new graph structures from the mixup structural embeddings. As for the functional information, GDM applies 
    
[^71]: Uniswap V3中的深度强化学习适应性流动性提供

    Adaptive Liquidity Provision in Uniswap V3 with Deep Reinforcement Learning. (arXiv:2309.10129v1 [cs.AI])

    [http://arxiv.org/abs/2309.10129](http://arxiv.org/abs/2309.10129)

    本研究介绍了一个使用深度强化学习的方法，能够使Uniswap V3中的流动性提供者能够自适应地调整价格范围，以实现最大化利润和减轻市场风险。

    

    去中心化交易所（DEX）是去中心化金融（DeFi）的基石，允许用户在无需第三方授权的情况下交易加密货币。投资者有动力将资产存入流动性池，用户可以直接与流动性提供者进行交易，并支付费用。然而，与资本效率和市场风险相关的一些未解决问题阻碍了DeFi的进一步发展。Uniswap V3是一项领先而开创性的DEX项目，通过使流动性提供者将流动性集中在特定价格范围内，解决了资本效率问题。然而，这种方法加剧了市场风险，因为只有当资产价格在这些预定的价格范围内时，流动性提供者才能获得交易费用。为了减轻这个问题，本文引入了一种深度强化学习（DRL）解决方案，旨在自适应调整价格范围，最大化利润并减轻市场风险。我们的方法还能够抵消价格变动。

    Decentralized exchanges (DEXs) are a cornerstone of decentralized finance (DeFi), allowing users to trade cryptocurrencies without the need for third-party authorization. Investors are incentivized to deposit assets into liquidity pools, against which users can trade directly, while paying fees to liquidity providers (LPs). However, a number of unresolved issues related to capital efficiency and market risk hinder DeFi's further development. Uniswap V3, a leading and groundbreaking DEX project, addresses capital efficiency by enabling LPs to concentrate their liquidity within specific price ranges for deposited assets. Nevertheless, this approach exacerbates market risk, as LPs earn trading fees only when asset prices are within these predetermined brackets. To mitigate this issue, this paper introduces a deep reinforcement learning (DRL) solution designed to adaptively adjust these price ranges, maximizing profits and mitigating market risks. Our approach also neutralizes price-change
    
[^72]: AR-TTA: 一种用于真实世界连续测试时间自适应的简单方法

    AR-TTA: A Simple Method for Real-World Continual Test-Time Adaptation. (arXiv:2309.10109v1 [cs.CV])

    [http://arxiv.org/abs/2309.10109](http://arxiv.org/abs/2309.10109)

    AR-TTA提出了一种简单的方法用于真实世界连续测试时间自适应。通过将内存缓冲区纳入自训练框架，并根据数据流的强度进行动态适应，提高了模型的稳定性。

    

    测试时间自适应是一种有前景的研究方向，它允许源模型在没有任何监督的情况下适应数据分布的变化。然而，当前的方法通常在只是实际场景简化版本的基准测试中进行评估。因此，我们建议使用最近推出的自动驾驶数据集CLAD-C和SHIFT来验证测试时间自适应方法。我们观察到，当前的测试时间自适应方法往往难以有效处理不同程度的域偏移，常常导致性能下降，低于源模型。我们注意到问题的根源在于无法保留源模型的知识，并且无法适应动态变化、时间相关的数据流。因此，我们通过将一个小的内存缓冲区纳入到成熟的自训练框架中，增加模型的稳定性，并同时根据数据流的强度进行动态适应。

    Test-time adaptation is a promising research direction that allows the source model to adapt itself to changes in data distribution without any supervision. Yet, current methods are usually evaluated on benchmarks that are only a simplification of real-world scenarios. Hence, we propose to validate test-time adaptation methods using the recently introduced datasets for autonomous driving, namely CLAD-C and SHIFT. We observe that current test-time adaptation methods struggle to effectively handle varying degrees of domain shift, often resulting in degraded performance that falls below that of the source model. We noticed that the root of the problem lies in the inability to preserve the knowledge of the source model and adapt to dynamically changing, temporally correlated data streams. Therefore, we enhance well-established self-training framework by incorporating a small memory buffer to increase model stability and at the same time perform dynamic adaptation based on the intensity of 
    
[^73]: 数据公式化: 基于人工智能驱动的概念驱动可视化创作

    Data Formulator: AI-powered Concept-driven Visualization Authoring. (arXiv:2309.10094v1 [cs.HC])

    [http://arxiv.org/abs/2309.10094](http://arxiv.org/abs/2309.10094)

    Data Formulator是一种基于人工智能的可视化创作工具，采用概念绑定范式，通过将高级可视化意图和低级数据转换步骤分离，利用AI代理自动转换输入数据，并生成所需的可视化。

    

    大多数现代可视化工具需要作者将其数据转换为整洁格式以创建所需的可视化。由于这需要具备编程或单独的数据处理工具的经验，数据转换仍然是可视化创作的一个障碍。为了解决这一挑战，我们提出了一种新的可视化范式，即概念绑定，它将高级可视化意图和低级数据转换步骤分开，并利用人工智能代理来实现。我们在Data Formulator中实现了这个范式，这是一个交互式的可视化创作工具。使用Data Formulator，作者首先使用自然语言或示例定义他们计划可视化的数据概念，然后将它们绑定到可视通道。Data Formulator然后将其人工智能代理派遣出去，自动将输入数据转换为呈现这些概念和生成所需可视化的数据。在向Data Formulator展示来自人工智能代理的结果(转换后的表格和输出的可视化)时，Data Formulator提供反馈。

    With most modern visualization tools, authors need to transform their data into tidy formats to create visualizations they want. Because this requires experience with programming or separate data processing tools, data transformation remains a barrier in visualization authoring. To address this challenge, we present a new visualization paradigm, concept binding, that separates high-level visualization intents and low-level data transformation steps, leveraging an AI agent. We realize this paradigm in Data Formulator, an interactive visualization authoring tool. With Data Formulator, authors first define data concepts they plan to visualize using natural languages or examples, and then bind them to visual channels. Data Formulator then dispatches its AI-agent to automatically transform the input data to surface these concepts and generate desired visualizations. When presenting the results (transformed table and output visualizations) from the AI agent, Data Formulator provides feedback
    
[^74]: 使用大型语言模型的一致时间逻辑规划：知道何时做什么和何时寻求帮助。

    Conformal Temporal Logic Planning using Large Language Models: Knowing When to Do What and When to Ask for Help. (arXiv:2309.10092v1 [cs.RO])

    [http://arxiv.org/abs/2309.10092](http://arxiv.org/abs/2309.10092)

    本文提出了一个使用大型语言模型的一致时间逻辑规划方法，用于解决多个高级子任务的移动机器人运动规划问题。其中的一个关键挑战是如何以正确性的角度推理机器人计划与基于自然语言的逻辑任务的关系。

    

    本文解决了一个新的移动机器人运动规划问题，任务是以自然语言（NL）表达并以时间和逻辑顺序完成多个高级子任务。为了正式定义这样的任务，我们利用基于NL的原子谓词在LTL上定义了模型。这与相关的规划方法形成对比，这些方法在原子谓词上定义了捕捉所需低级系统配置的LTL任务。我们的目标是设计机器人计划，满足基于NL的原子命题定义的LTL任务。在这个设置中出现的一个新的技术挑战在于推理机器人计划的正确性与这些LTL编码的任务的关系。为了解决这个问题，我们提出了HERACLEs，一个分层一致的自然语言规划器，它依赖于现有工具的新型整合，包括（i）自动机理论，以确定机器人应该完成的NL指定的子任务以推进任务进展；

    This paper addresses a new motion planning problem for mobile robots tasked with accomplishing multiple high-level sub-tasks, expressed using natural language (NL), in a temporal and logical order. To formally define such missions, we leverage LTL defined over NL-based atomic predicates modeling the considered NL-based sub-tasks. This is contrast to related planning approaches that define LTL tasks over atomic predicates capturing desired low-level system configurations. Our goal is to design robot plans that satisfy LTL tasks defined over NL-based atomic propositions. A novel technical challenge arising in this setup lies in reasoning about correctness of a robot plan with respect to such LTL-encoded tasks. To address this problem, we propose HERACLEs, a hierarchical conformal natural language planner, that relies on a novel integration of existing tools that include (i) automata theory to determine the NL-specified sub-task the robot should accomplish next to make mission progress; (
    
[^75]: 视频-文本检索的统一粗到细对齐方法

    Unified Coarse-to-Fine Alignment for Video-Text Retrieval. (arXiv:2309.10091v1 [cs.CV])

    [http://arxiv.org/abs/2309.10091](http://arxiv.org/abs/2309.10091)

    提出了一种统一粗到细对齐模型UCoFiA，用于视频-文本检索，该模型能够在不同粒度级别上捕捉跨模态相似性信息，并通过交互式相似性聚合模块有效考虑不同视觉特征的重要性，最终解决了视频-文本检索中的精确匹配问题。

    

    视频-文本检索通常利用视觉和文本信息之间的粗粒度或细粒度对齐。然而，根据文本查询检索正确的视频通常具有挑战性，因为它需要能够推理出高级（场景）和低级（对象）视觉线索及其与文本查询的关系。为此，我们提出了一种名为UCoFiA的统一粗到细对齐模型。具体而言，我们的模型在不同粒度级别上捕捉跨模态相似性信息。为减轻无关视觉线索的影响，我们还应用了交互式相似性聚合模块（ISA）来考虑不同视觉特征的重要性，同时聚合跨模态相似性以获得每个粒度的相似度得分。最后，我们应用Sinkhorn-Knopp算法对每个级别的相似性进行标准化，以减轻不同级别上的过度或不足表示问题。

    The canonical approach to video-text retrieval leverages a coarse-grained or fine-grained alignment between visual and textual information. However, retrieving the correct video according to the text query is often challenging as it requires the ability to reason about both high-level (scene) and low-level (object) visual clues and how they relate to the text query. To this end, we propose a Unified Coarse-to-fine Alignment model, dubbed UCoFiA. Specifically, our model captures the cross-modal similarity information at different granularity levels. To alleviate the effect of irrelevant visual clues, we also apply an Interactive Similarity Aggregation module (ISA) to consider the importance of different visual features while aggregating the cross-modal similarity to obtain a similarity score for each granularity. Finally, we apply the Sinkhorn-Knopp algorithm to normalize the similarities of each level before summing them, alleviating over- and under-representation issues at different l
    
[^76]: HTEC: 人类转录错误修正

    HTEC: Human Transcription Error Correction. (arXiv:2309.10089v1 [eess.AS])

    [http://arxiv.org/abs/2309.10089](http://arxiv.org/abs/2309.10089)

    HTEC是一种用于人类转录错误修正的方法，包括错误检测和填充两个阶段，提出了一种综合的修正操作列表，并针对删除错误提出了四种新操作。

    

    高质量的人类转录对于训练和改进自动语音识别（ASR）模型至关重要。最近的研究发现，每增加1%的转录错误率（WER），使用这些转录来训练ASR模型将增加约2%的ASR WER。即使是经过高度培训的注释员也难免出现转录错误。然而，很少有研究探讨人类转录错误的修正方法。其他问题的错误修正方法，如ASR错误修正和语法错误修正，对这个问题的表现不够好。因此，我们提出了HTEC用于人类转录错误修正。HTEC包括两个阶段：Trans-Checker，一种错误检测模型，用于预测和屏蔽错误单词，和Trans-Filler，一种序列到序列的生成模型，用于填充屏蔽位置。我们提出了一个综合的修正操作列表，其中包括四种处理删除错误的新操作。

    High-quality human transcription is essential for training and improving Automatic Speech Recognition (ASR) models. Recent study~\cite{libricrowd} has found that every 1% worse transcription Word Error Rate (WER) increases approximately 2% ASR WER by using the transcriptions to train ASR models. Transcription errors are inevitable for even highly-trained annotators. However, few studies have explored human transcription correction. Error correction methods for other problems, such as ASR error correction and grammatical error correction, do not perform sufficiently for this problem. Therefore, we propose HTEC for Human Transcription Error Correction. HTEC consists of two stages: Trans-Checker, an error detection model that predicts and masks erroneous words, and Trans-Filler, a sequence-to-sequence generative model that fills masked positions. We propose a holistic list of correction operations, including four novel operations handling deletion errors. We further propose a variant of e
    
[^77]: GAME: 一种用于早期筛查青少年心理障碍的多模态数据整合的广义深度学习模型

    GAME: Generalized deep learning model towards multimodal data integration for early screening of adolescent mental disorders. (arXiv:2309.10077v1 [cs.LG])

    [http://arxiv.org/abs/2309.10077](http://arxiv.org/abs/2309.10077)

    本研究设计了一个广义深度学习模型GAME，通过整合多模态特征并采用新颖的关注机制，能够对青少年的心理状况进行高准确度的评估。研究发现每种模态都对心理障碍的筛查和共病状况有动态的贡献。

    

    及时发现青少年心理障碍是一个全球公共卫生挑战。由于其复杂而微妙的性质，单一因素很难检测到异常。此外，目前还没有广义的多模态图像与机器人相互作用的计算机辅助筛查系统用于检测青少年心理障碍。本研究设计了一个在便携机器人上部署的安卓应用程序，采用迷你游戏和聊天记录，对3,783名中学生进行筛查，并构建了包括面部图像、生理指标、语音记录和文本转录的多模态筛查数据集。我们开发了一个名为GAME（具有关注机制和多模态EmbraceNet的广义模型）的模型，该模型通过将跨模态特征整合到模型中，对青少年的心理状况进行高准确度（73.34%-92.77%）和F1-Score（71.32%-91.06%）的评估。我们发现每种模态动态地对心理障碍筛查和共病状况作出贡献。

    The timely identification of mental disorders in adolescents is a global public health challenge.Single factor is difficult to detect the abnormality due to its complex and subtle nature. Additionally, the generalized multimodal Computer-Aided Screening (CAS) systems with interactive robots for adolescent mental disorders are not available. Here, we design an android application with mini-games and chat recording deployed in a portable robot to screen 3,783 middle school students and construct the multimodal screening dataset, including facial images, physiological signs, voice recordings, and textual transcripts.We develop a model called GAME (Generalized Model with Attention and Multimodal EmbraceNet) with novel attention mechanism that integrates cross-modal features into the model. GAME evaluates adolescent mental conditions with high accuracy (73.34%-92.77%) and F1-Score (71.32%-91.06%).We find each modality contributes dynamically to the mental disorders screening and comorbiditi
    
[^78]: 基于性别的差异在大脑衰老中的作用：以帕金森病为重点

    Sex-based Disparities in Brain Aging: A Focus on Parkinson's Disease. (arXiv:2309.10069v1 [q-bio.NC])

    [http://arxiv.org/abs/2309.10069](http://arxiv.org/abs/2309.10069)

    本研究旨在探讨帕金森病患者中性别在大脑衰老中的作用。通过分析373名帕金森病患者的大脑预测年龄差异，并按性别进行分层分析，发现性别差异会导致帕金森病的症状和进展速度的差异。

    

    帕金森病与大脑衰老速度加快有关。性别被认为是帕金森病的一个重要因素，男性患病的可能性是女性的两倍，且症状更严重，进展更快。尽管以前的研究已经有所进展，但对帕金森病患者中性别在大脑衰老过程中的功能理解仍存在显著差距。本研究使用基于T1加权MRI的大脑预测年龄差异，在PPMI数据库的373名帕金森病患者中使用经过训练的鲁棒大脑年龄估计框架进行计算，该框架在949名健康受试者上进行了训练。使用线性回归模型研究了帕金森病中大脑预测年龄差异与临床变量之间的关联，按性别分层。所有女性帕金森病患者都参与了相关性分析，而相同数量的男性根据年龄、教育水平、症状发病年龄和临床症状严重程度使用倾向评分匹配方法进行了筛选。尽管两组患者的人口统计学参数相匹配，动作的演变却因性别而有所不同。

    PD is linked to faster brain aging. Sex is recognized as an important factor in PD, such that males are twice as likely as females to have the disease and have more severe symptoms and a faster progression rate. Despite previous research, there remains a significant gap in understanding the function of sex in the process of brain aging in PD patients. The T1-weighted MRI-driven brain-predicted age difference was computed in a group of 373 PD patients from the PPMI database using a robust brain-age estimation framework that was trained on 949 healthy subjects. Linear regression models were used to investigate the association between brain-PAD and clinical variables in PD, stratified by sex. All female PD patients were used in the correlational analysis while the same number of males were selected based on propensity score matching method considering age, education level, age of symptom onset, and clinical symptom severity. Despite both patient groups being matched for demographics, moto
    
[^79]: 使用大型语言模型的自动个性化印象生成PET报告

    Automatic Personalized Impression Generation for PET Reports Using Large Language Models. (arXiv:2309.10066v1 [cs.AI])

    [http://arxiv.org/abs/2309.10066](http://arxiv.org/abs/2309.10066)

    本研究旨在使用fine-tuned大型语言模型实现自动个性化生成全身PET报告的准确印象。通过训练语言模型并引入阅读医生的身份信息，模型能够学习医生特定的报告风格。研究结果经过专家评估和核医学医生的质量评分认可，证明该方法在实践中具有潜在的应用价值。

    

    目的：确定通过fine-tuned大型语言模型(LLMs)是否可以为全身PET报告生成准确的个性化印象。材料和方法：使用teacher-forcing算法在PET报告语料库上训练了12个语言模型，输入是报告发现，参考是临床印象。额外的输入标记编码了阅读医生的身份，使模型能够学习医生特定的报告风格。我们的语料库包括2010年至2022年间从我们机构收集的37,370份回顾性PET报告。通过与两名核医学（NM）医生的质量评分进行30个评估指标的基准测试，最匹配的指标选择了用于专家评估的模型。在部分数据子集中，根据6个质量维度和一个总体实用性评分（5分制），三名核医学医生评估了模型生成的印象和原始临床印象。

    Purpose: To determine if fine-tuned large language models (LLMs) can generate accurate, personalized impressions for whole-body PET reports. Materials and Methods: Twelve language models were trained on a corpus of PET reports using the teacher-forcing algorithm, with the report findings as input and the clinical impressions as reference. An extra input token encodes the reading physician's identity, allowing models to learn physician-specific reporting styles. Our corpus comprised 37,370 retrospective PET reports collected from our institution between 2010 and 2022. To identify the best LLM, 30 evaluation metrics were benchmarked against quality scores from two nuclear medicine (NM) physicians, with the most aligned metrics selecting the model for expert evaluation. In a subset of data, model-generated impressions and original clinical impressions were assessed by three NM physicians according to 6 quality dimensions and an overall utility score (5-point scale). Each physician reviewe
    
[^80]: 为自主和操纵员控制的无人机实现无碰撞轨迹

    Toward collision-free trajectory for autonomous and pilot-controlled unmanned aerial vehicles. (arXiv:2309.10064v1 [cs.RO])

    [http://arxiv.org/abs/2309.10064](http://arxiv.org/abs/2309.10064)

    该研究旨在实现无人机的无碰撞轨迹，并安全高效地将无人机交通管理系统与空中交通管理系统进行整合。

    

    对于安全关键系统的无人机，需要在机载上安装避障技术，以便察觉即将发生的非合作性威胁或冲突交通。根据自主等级，采取合适的行动以避免碰撞。将无人机交通管理系统和空中交通管理系统安全高效地整合起来成为一个新的需求，因为无人机应用的数量在拥挤空中交通环境中迅速增加。

    For drones, as safety-critical systems, there is an increasing need for onboard detect & avoid (DAA) technology i) to see, sense or detect conflicting traffic or imminent non-cooperative threats due to their high mobility with multiple degrees of freedom and the complexity of deployed unstructured environments, and subsequently ii) to take the appropriate actions to avoid collisions depending upon the level of autonomy. The safe and efficient integration of UAV traffic management (UTM) systems with air traffic management (ATM) systems, using intelligent autonomous approaches, is an emerging requirement where the number of diverse UAV applications is increasing on a large scale in dense air traffic environments for completing swarms of multiple complex missions flexibly and simultaneously. Significant progress over the past few years has been made in detecting UAVs present in aerospace, identifying them, and determining their existing flight path. This study makes greater use of electro
    
[^81]: 从计算角度的意识理论调查

    Survey of Consciousness Theory from Computational Perspective. (arXiv:2309.10063v1 [q-bio.NC])

    [http://arxiv.org/abs/2309.10063](http://arxiv.org/abs/2309.10063)

    本文调查了从计算角度出发，解释人类大脑中意识现象的多种理论。解开意识之谜是构建通用人工智能的重要一步。

    

    人类意识已经是几个世纪以来的长期谜团，而机器智能和意识是一项艰巨的追求。研究人员从不同的角度和层次上发展了各种理论来解释人类大脑中的意识现象。本文从计算角度概述了几个主要的意识理论分支，包括信息论、量子物理学、认知心理学、生理学和计算机科学，并旨在从计算角度来连接这些理论。它还讨论了意识的现有评估指标以及当前计算模型能否具有意识。解开意识之谜可以是构建具有计算机的通用人工智能的重要一步。

    Human consciousness has been a long-lasting mystery for centuries, while machine intelligence and consciousness is an arduous pursuit. Researchers have developed diverse theories for interpreting the consciousness phenomenon in human brains from different perspectives and levels. This paper surveys several main branches of consciousness theories originating from different subjects including information theory, quantum physics, cognitive psychology, physiology and computer science, with the aim of bridging these theories from a computational perspective. It also discusses the existing evaluation metrics of consciousness and possibility for current computational models to be conscious. Breaking the mystery of consciousness can be an essential step in building general artificial intelligence with computing machines.
    
[^82]: GPT-3用于抗癌药物敏感性预测的评估

    Evaluation of GPT-3 for Anti-Cancer Drug Sensitivity Prediction. (arXiv:2309.10016v1 [cs.LG])

    [http://arxiv.org/abs/2309.10016](http://arxiv.org/abs/2309.10016)

    本研究评估了GPT-3在抗癌药物敏感性预测任务中的潜力，并发现药物的SMILES表示和细胞系的基因组突变特征对药物反应具有预测能力。这些结果有助于在精准肿瘤学中设计更有效的治疗方案。

    

    本研究使用结构化的药物基因组数据，在五种组织类型中探究了GPT-3在抗癌药物敏感性预测任务中的潜力，并分别采用零样本提示和微调范式对其性能进行了评估。药物的SMILES表示和细胞系的基因组突变特征对药物反应具有预测能力。本研究的结果有望为精准肿瘤学中设计更有效的治疗方案铺平道路。

    In this study, we investigated the potential of GPT-3 for the anti-cancer drug sensitivity prediction task using structured pharmacogenomics data across five tissue types and evaluated its performance with zero-shot prompting and fine-tuning paradigms. The drug's smile representation and cell line's genomic mutation features were predictive of the drug response. The results from this study have the potential to pave the way for designing more efficient treatment protocols in precision oncology.
    
[^83]: SYNDICOM: 错误注入和自然语言反馈改进对话常识研究

    SYNDICOM: Improving Conversational Commonsense with Error-Injection and Natural Language Feedback. (arXiv:2309.10015v1 [cs.CL])

    [http://arxiv.org/abs/2309.10015](http://arxiv.org/abs/2309.10015)

    SYNDICOM是一种改进对话常识的方法，包含了一个常识对话数据集和一个基于自然语言反馈的模型，可用于训练对话应答生成模型。

    

    常识推理是人类交流的关键方面。尽管近年来由大型语言模型驱动的对话人工智能取得了进展，但常识推理仍然是一个具有挑战性的任务。在这项工作中，我们介绍了SYNDICOM - 一种改进对话应答生成中常识的方法。SYNDICOM由两个部分组成。第一个组件是一个由知识图创建的常识对话数据集，并以自然语言形式合成。该数据集包括对话环境中的有效和无效回答，以及对无效回答的自然语言反馈（NLF）。第二个贡献是一个两步的过程：训练一个模型来预测无效回答的自然语言反馈（NLF），然后根据预测的NLF、无效回答和对话条件训练一个应答生成模型。SYNDICOM具有可伸缩性，不需要强化学习。通过对三个任务的经验结果进行评估。

    Commonsense reasoning is a critical aspect of human communication. Despite recent advances in conversational AI driven by large language models, commonsense reasoning remains a challenging task. In this work, we introduce SYNDICOM - a method for improving commonsense in dialogue response generation. SYNDICOM consists of two components. The first component is a dataset composed of commonsense dialogues created from a knowledge graph and synthesized into natural language. This dataset includes both valid and invalid responses to dialogue contexts, along with natural language feedback (NLF) for the invalid responses. The second contribution is a two-step procedure: training a model to predict natural language feedback (NLF) for invalid responses, and then training a response generation model conditioned on the predicted NLF, the invalid response, and the dialogue. SYNDICOM is scalable and does not require reinforcement learning. Empirical results on three tasks are evaluated using a broad
    
[^84]: 窥探过去：改进不断学习中生成回放的知识保留

    Looking through the past: better knowledge retention for generative replay in continual learning. (arXiv:2309.10012v1 [cs.LG])

    [http://arxiv.org/abs/2309.10012](http://arxiv.org/abs/2309.10012)

    本文改进了不断学习环境中的生成回放方法，以在复杂场景下表现更好。通过蒸馏潜在空间、改善生成特征对齐和周期性生成以增强知识保留。

    

    在这项工作中，我们改进了不断学习环境中的生成回放，以在具有挑战性的场景中表现良好。当前的生成回放方法通常在小型和简单的数据集上进行基准测试，因为它们不足以生成更复杂的数据和更多的类别。我们注意到，在基于VAE的生成回放中，这可能归因于当生成的特征映射到潜在空间时与原始特征之间存在较大差异。因此，我们提出了三种修改方法，使模型能够学习和生成复杂的数据。具体而言，我们将当前模型与先前模型之间的潜在空间进行蒸馏，以减少特征漂移。此外，我们提出了一种用于改善生成特征对齐的重建和原始数据的潜在匹配。进一步地，基于对保存知识的重建效果更好的观察，我们通过先前训练过的模型周期性生成的方式进一步增强了知识保留。

    In this work, we improve the generative replay in a continual learning setting to perform well on challenging scenarios. Current generative rehearsal methods are usually benchmarked on small and simple datasets as they are not powerful enough to generate more complex data with a greater number of classes. We notice that in VAE-based generative replay, this could be attributed to the fact that the generated features are far from the original ones when mapped to the latent space. Therefore, we propose three modifications that allow the model to learn and generate complex data. More specifically, we incorporate the distillation in latent space between the current and previous models to reduce feature drift. Additionally, a latent matching for the reconstruction and original data is proposed to improve generated features alignment. Further, based on the observation that the reconstructions are better for preserving knowledge, we add the cycling of generations through the previously trained
    
[^85]: 自主车辆间的多智能体深度强化学习在AutoDRIVE生态系统中的合作与竞争

    Multi-Agent Deep Reinforcement Learning for Cooperative and Competitive Autonomous Vehicles using AutoDRIVE Ecosystem. (arXiv:2309.10007v1 [cs.RO])

    [http://arxiv.org/abs/2309.10007](http://arxiv.org/abs/2309.10007)

    本研究提出了一个模块化和并行化的多智能体深度强化学习框架，在AutoDRIVE生态系统中培养合作与竞争行为。我们利用该生态系统开发了准确物理和逼真图形的数字孪生体，并使用它来训练和部署多智能体强化学习策略，实现了在自主车辆中的合作和竞争行为。

    

    本研究提出了一个模块化和可并行化的多智能体深度强化学习框架，用于在自主车辆中培养合作和竞争行为。我们引入了AutoDRIVE生态系统作为一个工具，开发出与真实的Nigel和F1TENTH两种比例自主车辆平台具有独特特性和能力的准确物理和逼真图形的数字孪生体，并利用这个生态系统来训练和部署多智能体强化学习策略。我们首先研究了一个交叉路口穿越问题，使用一组合作车辆（Nigel）在单个或多个智能体学习环境中共享有限状态信息，采用一种公共策略方法。然后我们研究了一个对抗性的头对头自主赛车问题，使用另一组车辆（F1TENTH）在多个智能体学习环境中采用个体策略方法。在任何一组实验中，都采用了分散学习架构。

    This work presents a modular and parallelizable multi-agent deep reinforcement learning framework for imbibing cooperative as well as competitive behaviors within autonomous vehicles. We introduce AutoDRIVE Ecosystem as an enabler to develop physically accurate and graphically realistic digital twins of Nigel and F1TENTH, two scaled autonomous vehicle platforms with unique qualities and capabilities, and leverage this ecosystem to train and deploy multi-agent reinforcement learning policies. We first investigate an intersection traversal problem using a set of cooperative vehicles (Nigel) that share limited state information with each other in single as well as multi-agent learning settings using a common policy approach. We then investigate an adversarial head-to-head autonomous racing problem using a different set of vehicles (F1TENTH) in a multi-agent learning setting using an individual policy approach. In either set of experiments, a decentralized learning architecture was adopted
    
[^86]: 韩国仁川市公共交通的优化路径

    The Optimized path for the public transportation of Incheon in South Korea. (arXiv:2309.10006v1 [physics.soc-ph])

    [http://arxiv.org/abs/2309.10006](http://arxiv.org/abs/2309.10006)

    本论文研究了基于乘客需求寻找韩国仁川市公交系统的最优路径，通过修改A*算法，在性能上优于其他常用路径规划算法，能够实时找到最短路径。

    

    路径规划是计算机科学领域中最受欢迎的研究课题之一。路径规划策略确定了从给定坐标到另一点的路径。本文的重点是基于乘客需求寻找公交系统的最优路径。该研究是基于韩国仁川市的公交站点进行的，我们展示了我们修改后的A*算法在性能上优于其他基本路径规划算法，如遗传算法和Dijkstra算法。我们提出的方法可以实时找到最短路径，即使对于大量的数据点。

    Path-finding is one of the most popular subjects in the field of computer science. Pathfinding strategies determine a path from a given coordinate to another. The focus of this paper is on finding the optimal path for the bus transportation system based on passenger demand. This study is based on bus stations in Incheon, South Korea, and we show that our modified A* algorithm performs better than other basic pathfinding algorithms such as the Genetic and Dijkstra. Our proposed approach can find the shortest path in real-time even for large amounts of data(points).
    
[^87]: OpenAI抄袭了我们的税务案例，但GPT-4真的能够处理税务吗？

    OpenAI Cribbed Our Tax Example, But Can GPT-4 Really Do Tax?. (arXiv:2309.09992v1 [cs.AI])

    [http://arxiv.org/abs/2309.09992](http://arxiv.org/abs/2309.09992)

    GPT-4在处理税务方面存在问题，无法可靠地计算税务。

    

    作者解释了OpenAI在GPT-4的直播演示中使用税法案例的来源，以及为什么GPT-4得到了错误的答案，以及它如何无法可靠地计算税务。

    The authors explain where OpenAI got the tax law example in its livestream demonstration of GPT-4, why GPT-4 got the wrong answer, and how it fails to reliably calculate taxes.
    
[^88]: 分子构象生成的位移得分法

    Molecular Conformation Generation via Shifting Scores. (arXiv:2309.09985v1 [physics.comp-ph])

    [http://arxiv.org/abs/2309.09985](http://arxiv.org/abs/2309.09985)

    该论文提出了一种新颖的分子构象生成方法，通过将分子解离视为对其组成原子施加逐渐增大的力场，从而改变原子间距离的分布，该方法在分子构象生成方面取得了显著改进。

    

    分子构象生成是计算化学中的一个关键方面，涉及为给定的分子生成三维构象几何。通过扩散生成分子构象需要学习逆向噪声过程。使用原子间距离扩散而不是构象来保持SE(3)-等价性，并且与其他技术相比表现出更好的性能，而相关的生成模型主要基于启发式假设。针对这一问题，我们提出了一种新颖的分子构象生成方法，其动机是认为分子的解离可以被视为对其组成原子施加逐渐增大的力场，从而使得原子间距离的变化分布从高斯分布转变为麦克斯韦-玻尔兹曼分布。相应的生成模型确保了可行的原子间距离几何结构，并呈现时间可逆性。实验结果表明，该方法在分子构象生成上取得了显著的改进。

    Molecular conformation generation, a critical aspect of computational chemistry, involves producing the three-dimensional conformer geometry for a given molecule. Generating molecular conformation via diffusion requires learning to reverse a noising process. Diffusion on inter-atomic distances instead of conformation preserves SE(3)-equivalence and shows superior performance compared to alternative techniques, whereas related generative modelings are predominantly based upon heuristical assumptions. In response to this, we propose a novel molecular conformation generation approach driven by the observation that the disintegration of a molecule can be viewed as casting increasing force fields to its composing atoms, such that the distribution of the change of inter-atomic distance shifts from Gaussian to Maxwell-Boltzmann distribution. The corresponding generative modeling ensures a feasible inter-atomic distance geometry and exhibits time reversibility. Experimental results on molecula
    
[^89]: 探索和比较用于预测大脑对真实图片的深度学习架构

    Exploration and Comparison of Deep Learning Architectures to Predict Brain Response to Realistic Pictures. (arXiv:2309.09983v1 [q-bio.NC])

    [http://arxiv.org/abs/2309.09983](http://arxiv.org/abs/2309.09983)

    本研究探索和比较了多种深度学习架构，用于预测大脑对真实图片的反应。最终发现，使用多个简单模型，每个模型专注于受试者大脑每个半球的每个ROI，可以获得更好的预测结果。

    

    我们展示了一项关于预测大脑对真实图像反应的机器学习架构的研究，以应对Algonauts Challenge 2023。我们的研究涉及对各种预训练模型进行了广泛实验。最初，我们采用了较简单的模型来预测大脑活动，但逐渐引入了更复杂的架构，利用可用的数据和大规模预训练模型生成的嵌入。我们遇到了与机器学习问题相关的典型困难，比如正则化和过拟合，以及与挑战特定的问题，如难以结合多个输入编码，以及输出的高维度、不明确的结构和嘈杂性。为了克服这些问题，我们测试了基于单边三维位置、多感兴趣区域(ROI)和半球的预测模型，但我们发现使用多个简单的模型，每个模型专注于受试者大脑每个半球的每个ROI，可以获得更好的结果。

    We present an exploration of machine learning architectures for predicting brain responses to realistic images on occasion of the Algonauts Challenge 2023. Our research involved extensive experimentation with various pretrained models. Initially, we employed simpler models to predict brain activity but gradually introduced more complex architectures utilizing available data and embeddings generated by large-scale pre-trained models. We encountered typical difficulties related to machine learning problems, e.g. regularization and overfitting, as well as issues specific to the challenge, such as difficulty in combining multiple input encodings, as well as the high dimensionality, unclear structure, and noisy nature of the output. To overcome these issues we tested single edge 3D position-based, multi-region of interest (ROI) and hemisphere predictor models, but we found that employing multiple simple models, each dedicated to a ROI in each hemisphere of the brain of each subject, yielded
    
[^90]: 内省式深度度量学习

    Introspective Deep Metric Learning. (arXiv:2309.09982v1 [cs.CV])

    [http://arxiv.org/abs/2309.09982](http://arxiv.org/abs/2309.09982)

    本文提出了一种内省式深度度量学习 (IDML) 框架，通过考虑图像中的不确定性，以更好地处理模糊图像，实现更鲁棒的训练。

    

    本文提出了一种内省式深度度量学习 (IDML) 框架，用于对图像进行不确定性感知的比较。传统的深度度量学习方法着重于学习一个具有区分度的嵌入来描述图像的语义特征，忽略了由于噪声或语义模糊性而导致的每个图像的不确定性存在。在没有意识到这些不确定性的情况下进行训练会导致模型在训练期间过度拟合注释标签，并在推理期间产生不令人满意的判断。受此启发，我们认为一个好的相似度模型应考虑到不确定性的语义差异，以更好地处理模糊图像，从而实现更鲁棒的训练。为了实现这一点，我们提出了一种表示图像的方法，不仅使用语义嵌入，还使用伴随的不确定性嵌入，分别描述图像的语义特征和模糊性。我们进一步提出了一种内省式相似度度量方法，用于进行内省式比较，以更好地处理图像的不确定性。

    This paper proposes an introspective deep metric learning (IDML) framework for uncertainty-aware comparisons of images. Conventional deep metric learning methods focus on learning a discriminative embedding to describe the semantic features of images, which ignore the existence of uncertainty in each image resulting from noise or semantic ambiguity. Training without awareness of these uncertainties causes the model to overfit the annotated labels during training and produce unsatisfactory judgments during inference. Motivated by this, we argue that a good similarity model should consider the semantic discrepancies with awareness of the uncertainty to better deal with ambiguous images for more robust training. To achieve this, we propose to represent an image using not only a semantic embedding but also an accompanying uncertainty embedding, which describes the semantic characteristics and ambiguity of an image, respectively. We further propose an introspective similarity metric to make
    
[^91]: 通过程序执行补充进行代码表示预训练

    Code Representation Pre-training with Complements from Program Executions. (arXiv:2309.09980v1 [cs.SE])

    [http://arxiv.org/abs/2309.09980](http://arxiv.org/abs/2309.09980)

    本论文提出了一种名为FuzzPretrain的方法，用于在代码表示预训练中探索由程序的测试用例揭示的动态信息，并解决从代码中直接学习功能语义的挑战。

    

    大型语言模型（LLMs）已被应用于编程语言建模，以推进代码智能化。尽管代码可以以文本格式表示，但为了正确编译或解释以执行一组期望的行为，代码在语法上更加严格。在这种情况下，现有的工作通过抽象语法树、控制流图等形式的句法表示，从代码中以较少的歧义性学习。然而，具有相同目的的程序可以用各种方式实现，显示出不同的句法表示，而具有类似实现的程序可能具有不同的行为。虽然在执行过程中可以轻易地演示这种语义，但功能上的这些语义很难直接从代码中学习，特别是在无监督的情况下。因此，在本文中，我们提出了FuzzPretrain来探索由测试用例揭示的程序的动态信息，并嵌入到代码表示的预训练中。

    Large language models (LLMs) for natural language processing have been grafted onto programming language modeling for advancing code intelligence. Although it can be represented in the text format, code is syntactically more rigorous in order to be properly compiled or interpreted to perform a desired set of behaviors given any inputs. In this case, existing works benefit from syntactic representations to learn from code less ambiguously in the forms of abstract syntax tree, control-flow graph, etc. However, programs with the same purpose can be implemented in various ways showing different syntactic representations while the ones with similar implementations can have distinct behaviors. Though trivially demonstrated during executions, such semantics about functionality are challenging to be learned directly from code, especially in an unsupervised manner. Hence, in this paper, we propose FuzzPretrain to explore the dynamic information of programs revealed by their test cases and embed
    
[^92]: MindAgent: 新兴游戏互动

    MindAgent: Emergent Gaming Interaction. (arXiv:2309.09971v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2309.09971](http://arxiv.org/abs/2309.09971)

    MindAgent是一种新颖的基础设施，用于评估游戏互动中的规划和协调新能力，它可以在多智能体系统中协调完成复杂任务，并与人类玩家合作。研究还引入了一个新的游戏场景和相关基准。

    

    大型语言模型（LLM）具有在多智能体系统中执行复杂调度并将这些智能体协调完成需要广泛协作的复杂任务的能力。然而，尽管已经引入了许多游戏框架，但社区在构建涵盖LLM和人类-NPC协作的通用多智能体协作基础设施方面仍然缺乏基准。在这项工作中，我们提出了一种新颖的基础设施- MindAgent-来评估游戏互动的规划和协调新能力。特别是，我们的基础设施利用现有的游戏框架，i）需要对多智能体系统的协调员进行理解，ii）通过未经调优的正确指示与人类玩家合作，iii）建立在少量提示和反馈的情境学习。此外，我们引入了一个新的游戏场景和相关的基准- CUISINEWORLD，该场景派遣了一个多智能体协作系统。

    Large Language Models (LLMs) have the capacity of performing complex scheduling in a multi-agent system and can coordinate these agents into completing sophisticated tasks that require extensive collaboration. However, despite the introduction of numerous gaming frameworks, the community has insufficient benchmarks towards building general multi-agents collaboration infrastructure that encompass both LLM and human-NPCs collaborations. In this work, we propose a novel infrastructure - MindAgent - to evaluate planning and coordination emergent capabilities for gaming interaction. In particular, our infrastructure leverages existing gaming framework, to i) require understanding of the coordinator for a multi-agent system, ii) collaborate with human players via un-finetuned proper instructions, and iii) establish an in-context learning on few-shot prompt with feedback. Furthermore, we introduce CUISINEWORLD, a new gaming scenario and related benchmark that dispatch a multi-agent collaborat
    
[^93]: AI生成内容的偏见：对大型语言模型生成的新闻的考察

    Bias of AI-Generated Content: An Examination of News Produced by Large Language Models. (arXiv:2309.09825v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2309.09825](http://arxiv.org/abs/2309.09825)

    这项研究调查了七个代表性大型语言模型生成的AI生成内容的偏见。研究发现这些模型生成的内容存在显著的性别和种族偏见。

    

    大型语言模型（LLM）通过生成的内容（即AI生成内容）具有改变我们生活和工作的潜力。为了利用这种转变，我们需要了解LLM的局限性。在这里，我们调查了由七种代表性LLM（包括ChatGPT和LLaMA）生成的AIGC的偏见。我们收集了《纽约时报》和路透社的新闻文章，这两家媒体以提供公正无偏的新闻而闻名。然后，我们将每个被调查的LLM应用于生成新闻内容，以这些新闻文章的标题作为提示，并通过比较AIGC与原始新闻文章来评估LLM生成的AIGC的性别和种族偏见。我们进一步分析了各个LLM在带有偏见的提示下的性别偏见，通过向从这些新闻标题构建的提示中添加性别偏见信息。我们的研究揭示了每个被调查的LLM生成的AIGC存在明显的性别和种族偏见。

    Large language models (LLMs) have the potential to transform our lives and work through the content they generate, known as AI-Generated Content (AIGC). To harness this transformation, we need to understand the limitations of LLMs. Here, we investigate the bias of AIGC produced by seven representative LLMs, including ChatGPT and LLaMA. We collect news articles from The New York Times and Reuters, both known for their dedication to provide unbiased news. We then apply each examined LLM to generate news content with headlines of these news articles as prompts, and evaluate the gender and racial biases of the AIGC produced by the LLM by comparing the AIGC and the original news articles. We further analyze the gender bias of each LLM under biased prompts by adding gender-biased messages to prompts constructed from these news headlines. Our study reveals that the AIGC produced by each examined LLM demonstrates substantial gender and racial biases. Moreover, the AIGC generated by each LLM ex
    
[^94]: 在缺乏文化共识的情况下利用集体智慧

    Harnessing Collective Intelligence Under a Lack of Cultural Consensus. (arXiv:2309.09787v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2309.09787](http://arxiv.org/abs/2309.09787)

    在缺乏文化共识的情况下，通过无限深度潜在结构的文化共识理论（iDLC-CCT）模型，扩展了文化共识理论（CCT）的能力，提高了对共识信念多样性的建模能力。

    

    利用集体智慧来推动有效的决策和合作受益于能够检测和描述共识信念的多样性。文化共识理论（CCT）提供了一种统计框架，用于检测和描述这些不同的共识信念。然而，它在现代应用中不可行，因为它缺乏对相似信念的概括能力，对稀疏数据无效，并且无法利用外部知识库或学习到的机器表示。在这里，我们通过无限深度潜在结构的文化共识理论（iDLC-CCT）来克服这些限制。该模型是一个非参数贝叶斯模型，通过一个潜在结构来扩展CCT。该结构允许我们将文化共识看作是一系列无限深度的概念构建块，从而提高了对信念多样性的建模能力。

    Harnessing collective intelligence to drive effective decision-making and collaboration benefits from the ability to detect and characterize heterogeneity in consensus beliefs. This is particularly true in domains such as technology acceptance or leadership perception, where a consensus defines an intersubjective truth, leading to the possibility of multiple "ground truths" when subsets of respondents sustain mutually incompatible consensuses. Cultural Consensus Theory (CCT) provides a statistical framework for detecting and characterizing these divergent consensus beliefs. However, it is unworkable in modern applications because it lacks the ability to generalize across even highly similar beliefs, is ineffective with sparse data, and can leverage neither external knowledge bases nor learned machine representations. Here, we overcome these limitations through Infinite Deep Latent Construct Cultural Consensus Theory (iDLC-CCT), a nonparametric Bayesian model that extends CCT with a lat
    
[^95]: 如何在数据马拉松中处理数据

    How to Data in Datathons. (arXiv:2309.09770v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2309.09770](http://arxiv.org/abs/2309.09770)

    本文提供了关于如何处理数据马拉松中的数据的指导方针和建议，通过10个案例研究验证了提出的框架的有效性。

    

    数据马拉松的兴起提供了一个在短时间内合作、学习和创新的平台。尽管它们具有重要的潜在好处，但组织往往因缺乏明确的指导方针和最佳实践而难以有效处理数据。根据我们自己的经验以及自2016年以来组织了超过80个数据马拉松挑战赛与60个合作伙伴组织的见解，我们提供了指导方针和建议，作为组织者在处理数据相关复杂性时的资源。我们将我们提出的框架应用于10个案例研究。

    The rise of datathons, also known as data or data science hackathons, has provided a platform to collaborate, learn, and innovate in a short timeframe. Despite their significant potential benefits, organizations often struggle to effectively work with data due to a lack of clear guidelines and best practices for potential issues that might arise. Drawing on our own experiences and insights from organizing >80 datathon challenges with >60 partnership organizations since 2016, we provide guidelines and recommendations that serve as a resource for organizers to navigate the data-related complexities of datathons. We apply our proposed framework to 10 case studies.
    
[^96]: 一个用于运输机器人调度问题的量子优化案例研究

    A Quantum Optimization Case Study for a Transport Robot Scheduling Problem. (arXiv:2309.09736v2 [quant-ph] UPDATED)

    [http://arxiv.org/abs/2309.09736](http://arxiv.org/abs/2309.09736)

    本研究通过比较D-Waves的量子-经典混合框架、富士通的量子启发式数字退火器和Gurobi的经典求解器的性能，提供了解决运输机器人调度问题的指导，发现数字退火器有希望的结果，并为混合量子退火器提供了一些机会。

    

    本文提出了一个综合的案例研究，比较了D-Waves的量子-经典混合框架、富士通的量子启发式数字退火器和Gurobi的最先进经典求解器在解决运输机器人调度问题方面的性能。这个问题源于一个具有工业相关性的现实场景。我们为我们的问题提供了三种不同的模型，采用不同的设计理念。在我们的基准测试中，我们关注不同模型和求解器组合的解决方案质量和端到端运行时间。我们发现，与Gurobi直接比较，数字退火器有希望的结果，并为混合量子退火器提供了一些机会。我们的研究提供了有关使用不同策略解决应用导向优化问题的工作流程的见解，并可以用于评估不同方法的优势和劣势。

    We present a comprehensive case study comparing the performance of D-Waves' quantum-classical hybrid framework, Fujitsu's quantum-inspired digital annealer, and Gurobi's state-of-the-art classical solver in solving a transport robot scheduling problem. This problem originates from an industrially relevant real-world scenario. We provide three different models for our problem following different design philosophies. In our benchmark, we focus on the solution quality and end-to-end runtime of different model and solver combinations. We find promising results for the digital annealer and some opportunities for the hybrid quantum annealer in direct comparison with Gurobi. Our study provides insights into the workflow for solving an application-oriented optimization problem with different strategies, and can be useful for evaluating the strengths and weaknesses of different approaches.
    
[^97]: LLM4Jobs: 无监督的职业提取和规范化，借助大型语言模型

    LLM4Jobs: Unsupervised occupation extraction and standardization leveraging Large Language Models. (arXiv:2309.09708v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2309.09708](http://arxiv.org/abs/2309.09708)

    本文介绍了LLM4Jobs无监督的职业提取和规范化方法，通过利用大型语言模型，它展现了超越最新基准的灵活性和多功能性。此外，本研究还提供了合成和真实数据集，可用于相关研究。研究结果表明，现代语言模型有望为复杂的职业提取和规范化任务提供强有力的基础。

    

    自动从自由文本的招聘信息和简历中提取和规范化职业是诸如职位推荐和劳动力市场政策制定等应用中至关重要的。本文介绍了LLM4Jobs，一种利用大型语言模型（LLM）进行职业编码的新型无监督方法。LLM4Jobs独特地利用了LLMs的自然语言理解和生成能力。通过对合成和真实数据集的严格实验评估，我们展示了LLM4Jobs始终超越无监督的最新基准，证明了其在不同数据集和粒度上的多功能性。作为我们工作的一个附带结果，我们提供了合成和真实数据集，这对于该领域的后续研究可能具有重要意义。总体而言，这项调查突显了当代LLMs在复杂的职业提取和规范化任务中的潜力，为建立一个强大的基础。

    Automated occupation extraction and standardization from free-text job postings and resumes are crucial for applications like job recommendation and labor market policy formation. This paper introduces LLM4Jobs, a novel unsupervised methodology that taps into the capabilities of large language models (LLMs) for occupation coding. LLM4Jobs uniquely harnesses both the natural language understanding and generation capacities of LLMs. Evaluated on rigorous experimentation on synthetic and real-world datasets, we demonstrate that LLM4Jobs consistently surpasses unsupervised state-of-the-art benchmarks, demonstrating its versatility across diverse datasets and granularities. As a side result of our work, we present both synthetic and real-world datasets, which may be instrumental for subsequent research in this domain. Overall, this investigation highlights the promise of contemporary LLMs for the intricate task of occupation extraction and standardization, laying the foundation for a robust
    
[^98]: 一种模型评估的性能特征曲线：在信息扩散预测中的应用

    A performance characteristic curve for model evaluation: the application in information diffusion prediction. (arXiv:2309.09537v2 [cs.SI] UPDATED)

    [http://arxiv.org/abs/2309.09537](http://arxiv.org/abs/2309.09537)

    本研究提出了一种模型的性能特征曲线，用于评估其在不同复杂度任务中的表现。通过使用基于信息熵的度量方法，我们确定了随机性与模型预测准确性之间的关系，并发现不同条件下的数据点都可以合并成一条曲线，捕捉了模型在面对不确定性时的正确预测能力。

    

    社交网络上的信息扩散预测旨在预测未来消息的接收者，在市场营销和社交媒体等实际应用中具有实用价值。尽管不同的预测模型都声称表现良好，但性能评估的通用框架仍然有限。本文旨在识别模型的性能特征曲线，该曲线捕获了模型在不同复杂度任务上的表现。我们提出了一种基于信息熵的度量方法来量化扩散数据中的随机性，然后确定了随机性与模型预测准确性之间的缩放模式。不同序列长度、系统大小和随机性下的数据点都合并成一条曲线，捕捉了模型在面对增加的不确定性时作出正确预测的内在能力。考虑到这条曲线具有评估模型的重要属性，我们将其定义为模型的性能特征曲线。

    The information diffusion prediction on social networks aims to predict future recipients of a message, with practical applications in marketing and social media. While different prediction models all claim to perform well, general frameworks for performance evaluation remain limited. Here, we aim to identify a performance characteristic curve for a model, which captures its performance on tasks of different complexity. We propose a metric based on information entropy to quantify the randomness in diffusion data, then identify a scaling pattern between the randomness and the prediction accuracy of the model. Data points in the patterns by different sequence lengths, system sizes, and randomness all collapse into a single curve, capturing a model's inherent capability of making correct predictions against increased uncertainty. Given that this curve has such important properties that it can be used to evaluate the model, we define it as the performance characteristic curve of the model.
    
[^99]: FedGKD:在联邦图神经网络中释放协作的力量

    FedGKD: Unleashing the Power of Collaboration in Federated Graph Neural Networks. (arXiv:2309.09517v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2309.09517](http://arxiv.org/abs/2309.09517)

    FedGKD是一种新颖的联邦图神经网络框架，通过利用客户端图数据集蒸馏方法提取更好的任务特征并引入感知全局协作结构的服务器端聚合机制，解决了联邦GNN系统中图异构性问题，提高了效率和准确性。

    

    最近几年来，由于联邦图神经网络（GNN）能够在数据隔离场景下执行与图相关的任务并保护数据隐私，联邦训练已经变得流行起来。然而，联邦GNN系统中的图异构性问题仍然存在挑战。现有的框架通过使用不同的统计量来表示局部任务，并通过简单的聚合机制将它们联系起来来解决这个问题。然而，这些方法在两个方面都效率有限：任务相关性量化的质量低和利用协作结构的无效性。为了解决这些问题，我们提出了FedGKD，一种新颖的联邦GNN框架，它利用一种新颖的客户端图数据集蒸馏方法提取更好地描述任务相关性的任务特征，并引入一个新颖的服务器端聚合机制，该机制能够感知到全局的协作结构。我们在六个真实世界的数据集上进行了大量实验证明了FedGKD框架的有效性。

    Federated training of Graph Neural Networks (GNN) has become popular in recent years due to its ability to perform graph-related tasks under data isolation scenarios while preserving data privacy. However, graph heterogeneity issues in federated GNN systems continue to pose challenges. Existing frameworks address the problem by representing local tasks using different statistics and relating them through a simple aggregation mechanism. However, these approaches suffer from limited efficiency from two aspects: low quality of task-relatedness quantification and inefficacy of exploiting the collaboration structure. To address these issues, we propose FedGKD, a novel federated GNN framework that utilizes a novel client-side graph dataset distillation method to extract task features that better describe task-relatedness, and introduces a novel server-side aggregation mechanism that is aware of the global collaboration structure. We conduct extensive experiments on six real-world datasets of
    
[^100]: 不需要计算复杂性无法解决的预言机，在稀疏线性MDP中探索和学习。

    Exploring and Learning in Sparse Linear MDPs without Computationally Intractable Oracles. (arXiv:2309.09457v1 [cs.LG])

    [http://arxiv.org/abs/2309.09457](http://arxiv.org/abs/2309.09457)

    本文研究了在稀疏线性MDP中探索和学习的问题，通过特征选择提出了一个多项式时间算法，以在与环境的交互中学习出近似最优策略。

    

    线性马尔可夫决策过程（MDPs）的基本假设是学习者可以访问已知的特征映射$ \phi（x，a）$，该映射将状态-动作对映射到$d$维向量，并且奖励和转换是此表示中的线性函数。但是这些特征从哪里来？在没有专家领域知识的情况下，一种诱人的策略是使用“厨房水槽”方法，并希望真实特征包含在一个更大的潜在特征集中。在本文中，我们从特征选择的角度重新审视线性MDP。在$k$-稀疏线性MDP中，存在一个未知的大小为$k$的子集$S \subset [d]$，其中包含所有相关特征，目标是在与环境的交互中仅经过poly$(k,\log d)$次学习，学习出近似最优策略。我们的主要结果是这个问题的第一个多项式时间算法。与此相反，早期的研究要么做出了明显的假设，使得探索无关紧要，要么提供了指数复杂度的算法。

    The key assumption underlying linear Markov Decision Processes (MDPs) is that the learner has access to a known feature map $\phi(x, a)$ that maps state-action pairs to $d$-dimensional vectors, and that the rewards and transitions are linear functions in this representation. But where do these features come from? In the absence of expert domain knowledge, a tempting strategy is to use the ``kitchen sink" approach and hope that the true features are included in a much larger set of potential features. In this paper we revisit linear MDPs from the perspective of feature selection. In a $k$-sparse linear MDP, there is an unknown subset $S \subset [d]$ of size $k$ containing all the relevant features, and the goal is to learn a near-optimal policy in only poly$(k,\log d)$ interactions with the environment. Our main result is the first polynomial-time algorithm for this problem. In contrast, earlier works either made prohibitively strong assumptions that obviated the need for exploration, o
    
[^101]: GenDOM：具有参数感知的泛化性一次变形物体操作

    GenDOM: Generalizable One-shot Deformable Object Manipulation with Parameter-Aware Policy. (arXiv:2309.09051v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2309.09051](http://arxiv.org/abs/2309.09051)

    GenDOM是一个框架，通过条件化操作策略和多样化模拟训练，使操作策略能够仅通过一个真实世界演示来处理不同的可变形物体，并通过最小化真实世界演示和模拟之间的点云格密度差异来估计新物体的可变形物体参数。

    

    由于在运动中存在固有的变形不确定性，以往的可变形物体操作方法（如绳子和布料）通常需要数百个真实世界演示来训练每个物体的操作策略，这限制了它们在不断变化的世界中的应用。为了解决这个问题，我们引入了GenDOM，这是一个框架，允许操作策略只需一个真实世界演示来处理不同的可变形物体。为了实现这一点，我们通过将操作策略与可变形物体参数联系起来，并使用多样化的模拟可变形物体对其进行训练，使策略能够根据不同的物体参数调整动作。在推理时，给定一个新的物体，GenDOM可以通过最小化真实世界演示和模拟之间点云的格密度差异来估计可变形物体参数，而只需一个真实世界演示。

    Due to the inherent uncertainty in their deformability during motion, previous methods in deformable object manipulation, such as rope and cloth, often required hundreds of real-world demonstrations to train a manipulation policy for each object, which hinders their applications in our ever-changing world. To address this issue, we introduce GenDOM, a framework that allows the manipulation policy to handle different deformable objects with only a single real-world demonstration. To achieve this, we augment the policy by conditioning it on deformable object parameters and training it with a diverse range of simulated deformable objects so that the policy can adjust actions based on different object parameters. At the time of inference, given a new object, GenDOM can estimate the deformable object parameters with only a single real-world demonstration by minimizing the disparity between the grid density of point clouds of real-world demonstrations and simulations in a differentiable phys
    
[^102]: TextBind: 多轮交错多模态指令跟随

    TextBind: Multi-turn Interleaved Multimodal Instruction-following. (arXiv:2309.08637v1 [cs.CL])

    [http://arxiv.org/abs/2309.08637](http://arxiv.org/abs/2309.08637)

    TextBind是一个注释极少的框架，用于将较大规模的语言模型赋予多轮交错多模态指令跟随能力，并通过图像-标题对生成多轮多模态指令-回应对话。这个框架对于解决实际任务具有重要意义，并为未来的研究提供了数据集、模型和演示。

    

    具有指令跟随能力的大型语言模型已经在人工智能领域产生了革命性的影响。这些模型通过其自然语言界面展示了卓越的泛化能力，可以解决各种实际任务。然而，它们的性能在很大程度上依赖于高质量的示例数据，而这往往很难获得。当涉及到多模态指令跟随时，这个挑战变得更加严峻。我们引入了TextBind，这是一个几乎不需要注释的框架，用于赋予较大规模的语言模型多轮交错多模态指令跟随能力。我们的方法仅需要图像-标题对，并从语言模型生成多轮多模态指令-回应对话。我们发布了我们的数据集、模型和演示，以促进未来在多模态指令跟随领域的研究。

    Large language models with instruction-following abilities have revolutionized the field of artificial intelligence. These models show exceptional generalizability to tackle various real-world tasks through their natural language interfaces. However, their performance heavily relies on high-quality exemplar data, which is often difficult to obtain. This challenge is further exacerbated when it comes to multimodal instruction following. We introduce TextBind, an almost annotation-free framework for empowering larger language models with the multi-turn interleaved multimodal instruction-following capabilities. Our approach requires only image-caption pairs and generates multi-turn multimodal instruction-response conversations from a language model. We release our dataset, model, and demo to foster future research in the area of multimodal instruction following.
    
[^103]: 基于低秩Slate的推荐系统中的表示学习

    Representation Learning in Low-rank Slate-based Recommender Systems. (arXiv:2309.08622v1 [cs.IR])

    [http://arxiv.org/abs/2309.08622](http://arxiv.org/abs/2309.08622)

    该论文提出了在推荐系统中使用低秩MDP将推荐问题视为在线RL问题，并通过提出的算法实现了高效的表示学习。

    

    在推荐系统中应用强化学习（RL）可以优化长期用户参与度的推荐。然而，该环境通常包含大量的状态和动作空间，这使得学习和探索变得困难。在这项工作中，我们提出了一种样本高效的表示学习算法，使用标准的slate推荐设置，将其视为低秩马尔可夫决策过程（MDP）的在线RL问题。我们还通过提出的设置和采样方法构建了推荐模拟环境。

    Reinforcement learning (RL) in recommendation systems offers the potential to optimize recommendations for long-term user engagement. However, the environment often involves large state and action spaces, which makes it hard to efficiently learn and explore. In this work, we propose a sample-efficient representation learning algorithm, using the standard slate recommendation setup, to treat this as an online RL problem with low-rank Markov decision processes (MDPs). We also construct the recommender simulation environment with the proposed setup and sampling method.
    
[^104]: BEA: 重新审视使用Budding Ensemble Architecture的基于锚点的目标检测DNN

    BEA: Revisiting anchor-based object detection DNN using Budding Ensemble Architecture. (arXiv:2309.08036v1 [cs.CV])

    [http://arxiv.org/abs/2309.08036](http://arxiv.org/abs/2309.08036)

    本文提出了一种新型简化的集合结构BEA用于锚点目标检测模型，并通过改进置信度得分的校准与降低不确定性误差的损失函数，提高了模型准确性。

    

    本文介绍了Budding Ensemble Architecture（BEA），一种用于锚点目标检测模型的新型简化集合结构。目标检测模型在基于视觉的任务中非常重要，特别是在自主系统中。它们应该提供精确的边界框检测，并校准其预测的置信度得分，以获得更高质量的不确定性估计。然而，由于假阳性接收到高分或真阳性由于低分而被丢弃，当前的模型可能会做出错误的决策。BEA旨在解决这些问题。BEA的提出的损失函数改善了置信度得分的校准和降低了不确定性误差，从而更好地区分真阳性和假阳性，最终提高了目标检测模型的准确性。使用BEA方法和其提出的损失函数，对Base-YOLOv3和SSD模型进行了改进。BEA在KITTI数据集上训练的Base-YOLOv3结果中，精度分别提高了6%和3.7%。

    This paper introduces the Budding Ensemble Architecture (BEA), a novel reduced ensemble architecture for anchor-based object detection models. Object detection models are crucial in vision-based tasks, particularly in autonomous systems. They should provide precise bounding box detections while also calibrating their predicted confidence scores, leading to higher-quality uncertainty estimates. However, current models may make erroneous decisions due to false positives receiving high scores or true positives being discarded due to low scores. BEA aims to address these issues. The proposed loss functions in BEA improve the confidence score calibration and lower the uncertainty error, which results in a better distinction of true and false positives and, eventually, higher accuracy of the object detection models. Both Base-YOLOv3 and SSD models were enhanced using the BEA method and its proposed loss functions. The BEA on Base-YOLOv3 trained on the KITTI dataset results in a 6% and 3.7% i
    
[^105]: 基于大型语言模型的代理的崛起和潜力：一项调查

    The Rise and Potential of Large Language Model Based Agents: A Survey. (arXiv:2309.07864v1 [cs.AI])

    [http://arxiv.org/abs/2309.07864](http://arxiv.org/abs/2309.07864)

    基于大型语言模型的代理的崛起和潜力：一项调查。大型语言模型被认为是构建通用人工智能代理的潜在催化剂，许多研究已经取得重要进展。

    

    长期以来，人类一直追求人工智能（AI）达到或超越人类水平的目标，而被认为是实现这一目标的有望方式的AI代理。AI代理是能感知环境、做出决策和采取行动的人工实体。自20世纪中叶以来，人们为开发智能AI代理进行了许多努力。然而，这些努力主要集中在算法或训练策略的进步上，以增强特定能力或在特定任务上的性能。实际上，社区所缺乏的是一个足够通用和强大的模型，作为设计能适应各种场景的AI代理的起点。由于展示出的多功能和显著能力，大型语言模型（LLMs）被视为人工通用智能（AGI）的潜在催化剂，为构建通用AI代理提供了希望。许多研究工作利用LLMs作为构建AI代理的基础，并且已经取得重要的进展。

    For a long time, humanity has pursued artificial intelligence (AI) equivalent to or surpassing the human level, with AI agents considered a promising vehicle for this pursuit. AI agents are artificial entities that sense their environment, make decisions, and take actions. Many efforts have been made to develop intelligent AI agents since the mid-20th century. However, these efforts have mainly focused on advancement in algorithms or training strategies to enhance specific capabilities or performance on particular tasks. Actually, what the community lacks is a sufficiently general and powerful model to serve as a starting point for designing AI agents that can adapt to diverse scenarios. Due to the versatile and remarkable capabilities they demonstrate, large language models (LLMs) are regarded as potential sparks for Artificial General Intelligence (AGI), offering hope for building general AI agents. Many research efforts have leveraged LLMs as the foundation to build AI agents and ha
    
[^106]: 旅行词：一种变压器的几何解释。

    Traveling Words: A Geometric Interpretation of Transformers. (arXiv:2309.07315v1 [cs.CL])

    [http://arxiv.org/abs/2309.07315](http://arxiv.org/abs/2309.07315)

    本文提出了一种几何视角来解释变压器的内部机制，主要贡献在于阐明了层归一化如何限制潜在特征并在超球面上塑造注意力机制，通过探测预训练的GPT-2模型验证了该视角的有效性，并提供了对变压器的直观理解。

    

    变压器在自然语言处理领域取得了显著的进展，但理解其内部机制仍然是一个挑战。本文介绍了一种新颖的几何视角，阐明了变压器操作的内部机制。我们的主要贡献是说明了层归一化如何将潜在特征限制在一个超球面上，从而使注意力能够在该表面上塑造单词的语义表示。这种几何视点无缝地连接了迭代改进和上下文嵌入等已知属性。我们通过探测一个预训练的124M参数的GPT-2模型验证了我们的见解。我们的发现揭示了早期层中清晰的查询-键注意力模式，并在更深的层次上建立在先前关于注意头的专门性的观察基础上。利用这些几何见解，我们提出了对变压器的直观理解，将其描绘为塑造轨迹的过程。

    Transformers have significantly advanced the field of natural language processing, but comprehending their internal mechanisms remains a challenge. In this paper, we introduce a novel geometric perspective that elucidates the inner mechanisms of transformer operations. Our primary contribution is illustrating how layer normalization confines the latent features to a hyper-sphere, subsequently enabling attention to mold the semantic representation of words on this surface. This geometric viewpoint seamlessly connects established properties such as iterative refinement and contextual embeddings. We validate our insights by probing a pre-trained 124M parameter GPT-2 model. Our findings reveal clear query-key attention patterns in early layers and build upon prior observations regarding the subject-specific nature of attention heads at deeper layers. Harnessing these geometric insights, we present an intuitive understanding of transformers, depicting them as processes that model the trajec
    
[^107]: 安全且加速的基于深度强化学习的O-RAN切片: 一种混合迁移学习方法

    Safe and Accelerated Deep Reinforcement Learning-based O-RAN Slicing: A Hybrid Transfer Learning Approach. (arXiv:2309.07265v1 [cs.NI])

    [http://arxiv.org/abs/2309.07265](http://arxiv.org/abs/2309.07265)

    本文提出了一种混合迁移学习方法，用于解决在O-RAN网络中使用DRL算法进行闭环控制时遇到的收敛速度慢和性能不稳定的问题。

    

    开放无线接入网络（O-RAN）架构支持智能网络控制算法作为其核心能力之一。数据驱动应用程序利用这些算法通过无线接入网络智能控制器（RIC）来优化无线接入网络（RAN）功能。在O-RAN文献中，深度强化学习（DRL）算法是解决动态无线资源管理问题的主要方法之一。然而，尽管O-RAN RIC引入了诸多好处，但在真实网络部署中，DRL算法的实际采用却落后。这主要是因为DRL代理在部署和面对之前未见过的网络条件时收敛速度慢、性能不稳定。在本文中，我们通过将迁移学习（TL）作为O-RAN功能的DRL基于闭环控制的训练和部署流程的核心组成部分来解决这些挑战。为此，我们提出并设计了一个混合TL辅助的方法

    The open radio access network (O-RAN) architecture supports intelligent network control algorithms as one of its core capabilities. Data-driven applications incorporate such algorithms to optimize radio access network (RAN) functions via RAN intelligent controllers (RICs). Deep reinforcement learning (DRL) algorithms are among the main approaches adopted in the O-RAN literature to solve dynamic radio resource management problems. However, despite the benefits introduced by the O-RAN RICs, the practical adoption of DRL algorithms in real network deployments falls behind. This is primarily due to the slow convergence and unstable performance exhibited by DRL agents upon deployment and when facing previously unseen network conditions. In this paper, we address these challenges by proposing transfer learning (TL) as a core component of the training and deployment workflows for the DRL-based closed-loop control of O-RAN functionalities. To this end, we propose and design a hybrid TL-aided a
    
[^108]: 预训练大型语言模型的网络运维能力的实证研究

    An Empirical Study of NetOps Capability of Pre-Trained Large Language Models. (arXiv:2309.05557v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2309.05557](http://arxiv.org/abs/2309.05557)

    本文通过对预训练大型语言模型（LLMs）进行系统评估，发现LLMs在网络运维（NetOps）领域具有强大的潜力应用，能够提升自动化和智能化的NetOps能力。

    

    大型语言模型（LLMs）可以回答人类语言查询，并在网络运维（NetOps）领域展现出强大的潜力应用。由于具备大量常识知识，LLMs在推理准确性上比传统模型更好，并具有更强的泛化能力、推理能力和代码生成能力，这些能力可能对自动化和智能化的NetOps有巨大的提升。然而，LLMs在各种NetOps任务中的表现仍然未被充分探索。本文系统评估了选择的几种LLMs在NetOps领域的能力、优势和限制。评估针对5732个关于NetOps的问题进行，涵盖了26个公开可用的通用领域LLMs，包括ChatGPT、LLaMA、Falcon等。我们还对其中一些LLMs进行了NetOps语料库的微调，并评估了结果模型的性能。评估方法遵循广泛采用的用于生成任务基准测试的方法。

    Large language models (LLMs) can respond to human language queries and have shown powerful potential applications in network operations (NetOps). Thanks to the large amount of commonsense knowledge inherent, LLMs achieve much better inference accuracy than traditional models and emerge with strong abilities in generalization, reasoning, and code generation. These abilities may have a crucial boost to automated and intelligent NetOps. However, it remains under-explored how well LLMs perform in various NetOps tasks. In this work, we make a systematic assessment of the capabilities, strengths, and limitations of selected LLMs in the field of NetOps. The evaluation is conducted on a collection of 5,732 questions about NetOps, encompassing 26 publicly available general-domain LLMs, including ChatGPT, LLaMA, Falcon, etc. We also finetune some of these LLMs with our collected NetOps corpus and evaluate the resulting models. The evaluation method follows the widely adopted benchmarks for gener
    
[^109]: FOLLOWUPQG:面向信息获取的跟进问题生成

    FOLLOWUPQG: Towards Information-Seeking Follow-up Question Generation. (arXiv:2309.05007v1 [cs.CL])

    [http://arxiv.org/abs/2309.05007](http://arxiv.org/abs/2309.05007)

    本文引入了一项真实世界的信息获取跟进问题生成任务，通过生成跟进问题来更深入地理解初始问题和答案。构建了数据集FOLLOWUPQG，评估了当前的问题生成模型在生成跟进问题方面的效果，并展示了其作为一个具有挑战性的基准任务的验证。

    

    人类出于好奇心而提出跟进问题，这反映了人类创造性的认知过程。我们引入了一个真实世界的信息获取跟进问题生成（FQG）任务，旨在生成能够更深入理解初始问题和答案的跟进问题。我们构建了FOLLOWUPQG数据集，包含了来自Reddit论坛的超过3K个真实世界的（初始问题，答案，跟进问题）元组，提供了对开放性问题的非专业人士友好的解释。与现有数据集相比，FOLLOWUPQG中的问题使用更多样化的实用策略来寻求信息，并展示了更高层次的认知技能（如应用和关联）。我们评估了当前的问题生成模型在生成跟进问题方面的效果，探索如何基于逐步演示生成特定类型的跟进问题。我们的结果验证了FOLLOWUPQG作为一个具有挑战性的基准任务。

    Humans ask follow-up questions driven by curiosity, which reflects a creative human cognitive process. We introduce the task of real-world information-seeking follow-up question generation (FQG), which aims to generate follow-up questions seeking a more in-depth understanding of an initial question and answer. We construct FOLLOWUPQG, a dataset of over 3K real-world (initial question, answer, follow-up question) tuples collected from a Reddit forum providing layman-friendly explanations for open-ended questions. In contrast to existing datasets, questions in FOLLOWUPQG use more diverse pragmatic strategies to seek information, and they also show higher-order cognitive skills (such as applying and relating). We evaluate current question generation models on their efficacy for generating follow-up questions, exploring how to generate specific types of follow-up questions based on step-by-step demonstrations. Our results validate FOLLOWUPQG as a challenging benchmark, as model-generated q
    
[^110]: GNN对公平性稳定性的Lipschitz特性表征

    Characterizing Lipschitz Stability of GNN for Fairness. (arXiv:2309.03648v1 [cs.LG])

    [http://arxiv.org/abs/2309.03648](http://arxiv.org/abs/2309.03648)

    论文通过研究GNN的Lipschitz界限表征了GNN对公平性稳定性的影响，尤其是在处理非欧几里得数据和固有偏倚的情况下。同时，该研究对于限制GNN输出的扰动以保障公平性训练提出了挑战。

    

    Lipschitz界限是从鲁棒统计学中借鉴的一种技术，可以限制输出相对于输入的最大变化，考虑到相关的非关键偏倚因素。这是一种高效且可证明的方法，可以检查机器学习模型的输出稳定性，而不会增加额外的计算成本。最近，对于在非欧几里得数据上操作的图神经网络（GNN）引起了广泛的关注。然而，之前没有研究调查GNN的Lipschitz界限以揭示模型输出的稳定性，特别是在处理具有固有偏倚的非欧几里得数据时。由于常见图形数据在GNN训练中存在固有偏差，这给限制由输入偏差引起的GNN输出扰动，从而在训练期间保障公平性，带来了严峻的挑战。最近，尽管Lipschitz常数在控制欧几里得神经网络的稳定性方面有所应用，但精确Lipschitz常数的计算十分困难。

    The Lipschitz bound, a technique from robust statistics, can limit the maximum changes in the output concerning the input, taking into account associated irrelevant biased factors. It is an efficient and provable method for examining the output stability of machine learning models without incurring additional computation costs. Recently, Graph Neural Networks (GNNs), which operate on non-Euclidean data, have gained significant attention. However, no previous research has investigated the GNN Lipschitz bounds to shed light on stabilizing model outputs, especially when working on non-Euclidean data with inherent biases. Given the inherent biases in common graph data used for GNN training, it poses a serious challenge to constraining the GNN output perturbations induced by input biases, thereby safeguarding fairness during training. Recently, despite the Lipschitz constant's use in controlling the stability of Euclideanneural networks, the calculation of the precise Lipschitz constant rem
    
[^111]: 图神经网络中的隐私调查：攻击、保护和应用

    A Survey on Privacy in Graph Neural Networks: Attacks, Preservation, and Applications. (arXiv:2308.16375v1 [cs.LG])

    [http://arxiv.org/abs/2308.16375](http://arxiv.org/abs/2308.16375)

    这篇综述调查了图神经网络中的隐私问题，包括攻击、保护方法以及应用领域。研究人员着重总结了攻击类型、隐私保护技术分类以及可用于分析和解决GNNs中隐私问题的数据集和应用，同时提出了未来研究的方向，以构建更好的隐私保护GNNs。

    

    随着处理图结构数据的能力和实际应用的改善，图神经网络（GNNs）引起了人们的极大关注。然而，许多这些模型优先考虑高效能表现，如准确性，而缺乏隐私考虑，这是现代社会隐私攻击盛行的重要问题。为了解决这个问题，研究人员开始开发保护隐私的GNNs。尽管取得了进展，但在图领域缺乏对攻击和隐私保护技术的综合概述。在本调查中，我们旨在通过总结针对图数据的攻击、对GNNs中的隐私保护技术进行分类以及审查可用于分析/解决GNNs中隐私问题的数据集和应用程序，填补这一空白。我们还概述了未来研究的潜在方向，以建立更好的隐私保护GNNs。

    Graph Neural Networks (GNNs) have gained significant attention owing to their ability to handle graph-structured data and the improvement in practical applications. However, many of these models prioritize high utility performance, such as accuracy, with a lack of privacy consideration, which is a major concern in modern society where privacy attacks are rampant. To address this issue, researchers have started to develop privacy-preserving GNNs. Despite this progress, there is a lack of a comprehensive overview of the attacks and the techniques for preserving privacy in the graph domain. In this survey, we aim to address this gap by summarizing the attacks on graph data according to the targeted information, categorizing the privacy preservation techniques in GNNs, and reviewing the datasets and applications that could be used for analyzing/solving privacy issues in GNNs. We also outline potential directions for future research in order to build better privacy-preserving GNNs.
    
[^112]: 通过卫星地图补充车载传感器：高精度地图构建的新视角

    Complementing Onboard Sensors with Satellite Map: A New Perspective for HD Map Construction. (arXiv:2308.15427v1 [cs.CV])

    [http://arxiv.org/abs/2308.15427](http://arxiv.org/abs/2308.15427)

    本研究通过补充卫星地图，增强了车载传感器构建高精度地图的方法，利用卫星地图的广阔覆盖能力。我们释放了卫星地图瓦片作为nuScenes数据集的补充，同时提出了一个分层融合模块来更好地融合车载传感器与卫星地图的信息。

    

    高精度（HD）地图对自动驾驶系统至关重要。最近的方法尝试基于车载传感器获取的信息实时构建HD地图。然而，这些方法的性能受到车辆周围环境的显著影响，这是由于车载传感器的固有限制，如对远程探测的能力不足。在本研究中，我们证明了通过补充卫星地图可以增强HD地图构建方法的性能，利用卫星地图的广泛覆盖能力。为了进一步的研究，我们发布了卫星地图瓦片作为nuScenes数据集的补充数据集。与此同时，我们提出了一个分层融合模块，使卫星地图信息与现有方法更好地融合。具体来说，我们设计了基于分割和距离的注意力掩码，应用交叉注意力机制来融合车载传感器与卫星地图的信息。

    High-Definition (HD) maps play a crucial role in autonomous driving systems. Recent methods have attempted to construct HD maps in real-time based on information obtained from vehicle onboard sensors. However, the performance of these methods is significantly susceptible to the environment surrounding the vehicle due to the inherent limitation of onboard sensors, such as weak capacity for long-range detection. In this study, we demonstrate that supplementing onboard sensors with satellite maps can enhance the performance of HD map construction methods, leveraging the broad coverage capability of satellite maps. For the purpose of further research, we release the satellite map tiles as a complementary dataset of nuScenes dataset. Meanwhile, we propose a hierarchical fusion module that enables better fusion of satellite maps information with existing methods. Specifically, we design an attention mask based on segmentation and distance, applying the cross-attention mechanism to fuse onboa
    
[^113]: 大型视觉语言模型中幻觉的评估与分析

    Evaluation and Analysis of Hallucination in Large Vision-Language Models. (arXiv:2308.15126v1 [cs.LG])

    [http://arxiv.org/abs/2308.15126](http://arxiv.org/abs/2308.15126)

    本文提出了基于大型语言模型的幻觉评估框架HaELM，可以评估大型视觉语言模型中的幻觉问题，并分析了导致幻觉的因素，并提出了缓解幻觉问题的建议。

    

    最近，大型视觉语言模型（LVLMs）取得了显著的成功。然而，LVLMs仍然存在幻觉问题，这限制了在许多场景中的实用性。幻觉指的是LVLMs响应中不存在于视觉输入中的信息，这可能导致重大后果的潜在风险。目前对LVLMs中的幻觉评估的研究工作有限。在本文中，我们提出了基于大型语言模型（LLM）的幻觉评估框架HaELM。HaELM的性能近似于ChatGPT的95%，并具有低成本、可复现、保护隐私和本地部署等额外优势。利用HaELM，我们评估了当前LVLMs中的幻觉。此外，我们分析了导致LVLMs中幻觉的因素，并提出了缓解幻觉问题的有用建议。

    Large Vision-Language Models (LVLMs) have recently achieved remarkable success. However, LVLMs are still plagued by the hallucination problem, which limits the practicality in many scenarios. Hallucination refers to the information of LVLMs' responses that does not exist in the visual input, which poses potential risks of substantial consequences. There has been limited work studying hallucination evaluation in LVLMs. In this paper, we propose Hallucination Evaluation based on Large Language Models (HaELM), an LLM-based hallucination evaluation framework. HaELM achieves an approximate 95% performance comparable to ChatGPT and has additional advantages including low cost, reproducibility, privacy preservation and local deployment. Leveraging the HaELM, we evaluate the hallucination in current LVLMs. Furthermore, we analyze the factors contributing to hallucination in LVLMs and offer helpful suggestions to mitigate the hallucination problem. Our training data and human annotation halluci
    
[^114]: 用于计算深度神经网络正则化路径的多目标延续方法

    A multiobjective continuation method to compute the regularization path of deep neural networks. (arXiv:2308.12044v1 [cs.LG])

    [http://arxiv.org/abs/2308.12044](http://arxiv.org/abs/2308.12044)

    本文提出了一种多目标延续方法，用于计算深度神经网络的正则化路径，以解决DNNs中稀疏性和数值效率之间的冲突。

    

    稀疏性是深度神经网络(DNNs)中非常理想的特征，因为它确保了数值效率，提高了模型的可解释性(由于相关特征的数量较少)和鲁棒性。在基于线性模型的机器学习方法中，众所周知在$\ell^1$范数(即零权重)的最稀疏解和非正则化解之间存在一条连接路径，这条路径被称为正则化路径。最近，通过将经验损失和稀疏性($\ell^1$范数)作为两个冲突的标准，并解决由此产生的多目标优化问题，首次尝试将正则化路径的概念扩展到DNNs。然而，由于$\ell^1$范数的不光滑性和参数数量的高度，从计算的角度来看，这种方法并不是很有效。为了克服这个限制，我们提出了一种算法，可以近似计算整个帕累托曲线

    Sparsity is a highly desired feature in deep neural networks (DNNs) since it ensures numerical efficiency, improves the interpretability of models (due to the smaller number of relevant features), and robustness. In machine learning approaches based on linear models, it is well known that there exists a connecting path between the sparsest solution in terms of the $\ell^1$ norm (i.e., zero weights) and the non-regularized solution, which is called the regularization path. Very recently, there was a first attempt to extend the concept of regularization paths to DNNs by means of treating the empirical loss and sparsity ($\ell^1$ norm) as two conflicting criteria and solving the resulting multiobjective optimization problem. However, due to the non-smoothness of the $\ell^1$ norm and the high number of parameters, this approach is not very efficient from a computational perspective. To overcome this limitation, we present an algorithm that allows for the approximation of the entire Pareto
    
[^115]: 语义RGB-D图像合成

    Semantic RGB-D Image Synthesis. (arXiv:2308.11356v1 [cs.CV])

    [http://arxiv.org/abs/2308.11356](http://arxiv.org/abs/2308.11356)

    本文提出了语义RGB-D图像合成方法，用于解决RGB-D语义图像分割训练集缺乏多样性的问题。通过生成逼真的RGB-D图像来实现给定语义标签图的合成。本文的主要创新是提出了一种生成器，可以处理多模态数据，将与模态无关的信息与与模态相关的信息分离开来。

    

    在RGB-D语义图像分割的训练集中收集各种各样的图像并不总是可能的。尤其是当机器人需要在隐私敏感区域如家庭中操作时，收集受限于一小部分地点。因此，标注图像在外观上缺乏多样性，RGB-D语义图像分割的方法往往对训练数据过度拟合。因此本文提出了语义RGB-D图像合成来解决这个问题。这需要为给定的语义标签图合成一个逼真的RGB-D图像。然而，目前的方法是单模态的，不能处理多模态数据。事实上，我们展示了将单模态方法扩展到多模态数据时效果不佳。因此，在本文中，我们提出了一个用于多模态数据的生成器，将语义布局中与模态无关的信息与生成RGB和深度图像所需的与模态相关的信息分离开来。

    Collecting diverse sets of training images for RGB-D semantic image segmentation is not always possible. In particular, when robots need to operate in privacy-sensitive areas like homes, the collection is often limited to a small set of locations. As a consequence, the annotated images lack diversity in appearance and approaches for RGB-D semantic image segmentation tend to overfit the training data. In this paper, we thus introduce semantic RGB-D image synthesis to address this problem. It requires synthesising a realistic-looking RGB-D image for a given semantic label map. Current approaches, however, are uni-modal and cannot cope with multi-modal data. Indeed, we show that extending uni-modal approaches to multi-modal data does not perform well. In this paper, we therefore propose a generator for multi-modal data that separates modal-independent information of the semantic layout from the modal-dependent information that is needed to generate an RGB and a depth image, respectively. 
    
[^116]: FusionPlanner：一种使用多传感器融合方法的多任务运动规划器，用于矿用卡车

    FusionPlanner: A Multi-task Motion Planner for Mining Trucks using Multi-sensor Fusion Method. (arXiv:2308.06931v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2308.06931](http://arxiv.org/abs/2308.06931)

    本研究提出了一种面向无人运输的综合方案，包括FusionPlanner运动规划算法、MiningNav评估工具和Parallel Mining Simulator模拟器，以应对露天采矿复杂环境下的运输挑战。

    

    近年来，在智能车辆的运动规划方面取得了重要成果。然而，作为一个典型的非结构化环境，露天采矿由于其复杂的操作条件和不利的环境因素而吸引了有限的关注。本研究提出了一种面向无人运输的综合范式，包括一个仿真平台、一个测试基准和一个可靠稳健的运动规划器。首先，我们提出了一种名为FusionPlanner的多任务运动规划算法，通过多传感器融合方法适应无人运输的横向和纵向控制任务。然后，我们开发了一个新颖的基准测试工具MiningNav，用于评估在露天采矿交通道路上经过良好训练的算法的可靠性和稳健性。最后，我们介绍了并行采矿模拟器（PMS），这是一个新的高保真度模拟器。

    In recent years, significant achievements have been made in motion planning for intelligent vehicles. However, as a typical unstructured environment, open-pit mining attracts limited attention due to its complex operational conditions and adverse environmental factors. A comprehensive paradigm for unmanned transportation in open-pit mines is proposed in this research, including a simulation platform, a testing benchmark, and a trustworthy and robust motion planner. Firstly, we propose a multi-task motion planning algorithm, called FusionPlanner, for autonomous mining trucks by the Multi-sensor fusion method to adapt both lateral and longitudinal control tasks for unmanned transportation. Then, we develop a novel benchmark called MiningNav, which offers three validation approaches to evaluate the trustworthiness and robustness of well-trained algorithms in transportation roads of open-pit mines. Finally, we introduce the Parallel Mining Simulator (PMS), a new high-fidelity simulator spe
    
[^117]: 为什么我们尚未拥有AGI

    Why We Don't Have AGI Yet. (arXiv:2308.03598v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2308.03598](http://arxiv.org/abs/2308.03598)

    本论文探讨了为什么尚未实现人工通用智能（AGI），并指出了纯粹的统计方法和资金推广不足是制约AGI发展的原因之一，同时还分析了实现人类适应能力和自主学习所需的关键认知能力。

    

    2002年重新阐述了AI的原始愿景，称之为“人工通用智能”或AGI。这一愿景是构建能够像人类一样学习、推理和解决问题的“思考机器”计算机系统。这与几十年来几乎所有人在该领域实践的“狭义AI”方法形成鲜明对比。虽然有几个大规模的项目名义上在致力于AGI的研发（尤其是DeepMind），但在纯粹专注的AGI发展领域，资金和推广并不充足。这令人惊讶，因为真正的AGI可以为人类带来巨大的价值。除了在这个领域缺乏努力之外，还存在亏欠的理论和方法上的错误。我们强调纯粹的统计方法无法实现AGI，并确定了实现类似人类适应能力和自主学习的关键认知能力。最终，我们概述了社会技术发展方面的一些问题。

    The original vision of AI was re-articulated in 2002 via the term 'Artificial General Intelligence' or AGI. This vision is to build 'Thinking Machines' computer systems that can learn, reason, and solve problems similar to the way humans do. This is in stark contrast to the 'Narrow AI' approach practiced by almost everyone in the field over the many decades. While several large-scale efforts have nominally been working on AGI (most notably DeepMind), the field of pure focused AGI development has not been well funded or promoted. This is surprising given the fantastic value that true AGI can bestow on humanity. In addition to the dearth of effort in this field, there are also several theoretical and methodical missteps that are hampering progress. We highlight why purely statistical approaches are unlikely to lead to AGI, and identify several crucial cognitive abilities required to achieve human-like adaptability and autonomous learning. We conclude with a survey of socio-technical fa
    
[^118]: VQGraph: 图形向量量化用于连接GNN和MLPs

    VQGraph: Graph Vector-Quantization for Bridging GNNs and MLPs. (arXiv:2308.02117v1 [cs.LG])

    [http://arxiv.org/abs/2308.02117](http://arxiv.org/abs/2308.02117)

    VQGraph是一个框架，通过学习一个强大的图形表示空间，用于连接GNN和MLPs。它采用矢量量化变分自编码器（VQ-VAE）的编码器作为结构感知图标记器，有效地表示底层图的多样化局部结构。通过 VQGraph，可以实现从GNN到MLP的知识转移。

    

    图神经网络（GNNs）进行信息传递，聚合局部邻居以更新节点表示。这种信息传递导致在实际的延迟约束应用程序中存在可扩展性问题。为了解决这个问题，最近的方法采用知识蒸馏（KD）通过模仿GNN的输出来学习计算效率高的多层感知机（MLP）。然而，现有的GNN表示空间可能不足以表示底层图的多样化局部结构，这限制了从GNN到MLP的知识转移。在这里，我们提出了一个新颖的框架VQGraph，用于学习一个强大的图形表示空间，用于连接GNN和MLPs。我们采用一种变体的矢量量化变分自编码器（VQ-VAE）的编码器作为结构感知图标记器，它将多样化的局部结构节点明确表示为大量离散令牌，并构成一个有意义的代码书。配备了学习的代码书，我们提出

    Graph Neural Networks (GNNs) conduct message passing which aggregates local neighbors to update node representations. Such message passing leads to scalability issues in practical latency-constrained applications. To address this issue, recent methods adopt knowledge distillation (KD) to learn computationally-efficient multi-layer perceptron (MLP) by mimicking the output of GNN. However, the existing GNN representation space may not be expressive enough for representing diverse local structures of the underlying graph, which limits the knowledge transfer from GNN to MLP. Here we present a novel framework VQGraph to learn a powerful graph representation space for bridging GNNs and MLPs. We adopt the encoder of a variant of a vector-quantized variational autoencoder (VQ-VAE) as a structure-aware graph tokenizer, which explicitly represents the nodes of diverse local structures as numerous discrete tokens and constitutes a meaningful codebook. Equipped with the learned codebook, we propos
    
[^119]: 层次骨架元-原型对比学习与硬骨架挖掘相结合的无监督人物重新识别方法

    Hierarchical Skeleton Meta-Prototype Contrastive Learning with Hard Skeleton Mining for Unsupervised Person Re-Identification. (arXiv:2307.12917v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2307.12917](http://arxiv.org/abs/2307.12917)

    本文提出了一种无监督的层次骨架元-原型对比学习（Hi-MPC）方法，结合硬骨架挖掘，用于无标签3D骨架的人物重新识别。通过构建层次骨架表示并利用元-原型对比学习进行特征提取和聚类，实现了更多信息丰富的骨架特征的利用。

    

    随着深度传感器和深度学习的快速发展，基于骨架的人物重新识别模型近年来取得了显著进展，并具有许多优势。然而，大多数现有方法仅从身体关节学习单层次的骨架特征，并假设骨架的重要性相等，因此通常无法利用更多来自不同层次（如肢体层次）的信息丰富的骨架特征。此外，这些方法的标签依赖性也限制了它们在学习更一般的骨架表示方面的灵活性。本文提出了一种通用的无监督层次骨架元-原型对比学习（Hi-MPC）方法，结合硬骨架挖掘（HSM）用于无标签3D骨架的人物重新识别。首先，我们构建了骨架的层次表示，以模拟身体关节、组件和肢体层次的粗到细的身体和动作特征。然后，我们使用层次化的元-原型对比学习方法进行特征提取和聚类。

    With rapid advancements in depth sensors and deep learning, skeleton-based person re-identification (re-ID) models have recently achieved remarkable progress with many advantages. Most existing solutions learn single-level skeleton features from body joints with the assumption of equal skeleton importance, while they typically lack the ability to exploit more informative skeleton features from various levels such as limb level with more global body patterns. The label dependency of these methods also limits their flexibility in learning more general skeleton representations. This paper proposes a generic unsupervised Hierarchical skeleton Meta-Prototype Contrastive learning (Hi-MPC) approach with Hard Skeleton Mining (HSM) for person re-ID with unlabeled 3D skeletons. Firstly, we construct hierarchical representations of skeletons to model coarse-to-fine body and motion features from the levels of body joints, components, and limbs. Then a hierarchical meta-prototype contrastive learni
    
[^120]: 扩展图神经网络的图评估指标

    Extended Graph Assessment Metrics for Graph Neural Networks. (arXiv:2307.10112v1 [cs.SI])

    [http://arxiv.org/abs/2307.10112](http://arxiv.org/abs/2307.10112)

    本论文提出了扩展的图评估指标（GAMs），适用于回归任务和连续邻接矩阵。主要关注的两个GAMs是同质性和跨类邻域相似度（CCNS）。这些扩展的指标能够在图神经网络中评估图结构，提高模型性能。

    

    当将患者队列重组为所谓的人口图时，最初独立的数据点可以合并成一个相互连接的图结构。利用图神经网络（GNNs），可以使用这种人口图进行医学的下游任务。适合的图结构的构建是学习过程中的一个具有挑战性的步骤，它对模型的性能有着重要的影响。为此，已经引入了不同的图评估指标来评估图结构。然而，这些指标仅适用于分类任务和离散的邻接矩阵，只覆盖了一小部分实际应用。在这项工作中，我们引入了针对回归任务和连续邻接矩阵的扩展图评估指标（GAMs）。我们重点关注两个具体的GAMs：同质性和跨类邻域相似度（CCNS）。我们将GAMs的概念扩展到多个跳跃，并为回归任务定义了同质性。

    When re-structuring patient cohorts into so-called population graphs, initially independent data points can be incorporated into one interconnected graph structure. This population graph can then be used for medical downstream tasks using graph neural networks (GNNs). The construction of a suitable graph structure is a challenging step in the learning pipeline that can have severe impact on model performance. To this end, different graph assessment metrics have been introduced to evaluate graph structures. However, these metrics are limited to classification tasks and discrete adjacency matrices, only covering a small subset of real-world applications. In this work, we introduce extended graph assessment metrics (GAMs) for regression tasks and continuous adjacency matrices. We focus on two GAMs in specific: \textit{homophily} and \textit{cross-class neighbourhood similarity} (CCNS). We extend the notion of GAMs to more than one hop, define homophily for regression tasks, as well as con
    
[^121]: 创建一个支持OpenMP Fortran和C++代码相互翻译的数据集

    Creating a Dataset Supporting Translation Between OpenMP Fortran and C++ Code. (arXiv:2307.07686v1 [cs.SE])

    [http://arxiv.org/abs/2307.07686](http://arxiv.org/abs/2307.07686)

    本研究创建了一个数据集，用于训练机器学习模型在OpenMP Fortran和C++代码之间进行翻译。这个数据集通过精细的代码相似性测试确保了可靠性和适用性，并且能够显著提升大规模语言模型的翻译能力。

    

    在本研究中，我们提出了一个新颖的数据集，用于训练在OpenMP Fortran和C++代码之间进行翻译的机器学习模型。通过精细的代码相似性测试，我们确保了数据集的可靠性和适用性。我们使用定量（CodeBLEU）和定性（人工评估）方法评估了我们数据集的有效性。我们展示了这个数据集如何显著提高大规模语言模型的翻译能力，对于没有先前编码知识的模型，提高了5.1倍，对于具有一定编码熟悉度的模型，提高了9.9倍。我们的工作突显了这个数据集在高性能计算的代码翻译领域的潜力。

    In this study, we present a novel dataset for training machine learning models translating between OpenMP Fortran and C++ code. To ensure reliability and applicability, the dataset is initially refined using a meticulous code similarity test. The effectiveness of our dataset is assessed using both quantitative (CodeBLEU) and qualitative (human evaluation) methods. We demonstrate how this dataset can significantly improve the translation capabilities of large-scale language models, with improvements of \times 5.1 for models with no prior coding knowledge and \times 9.9 for models with some coding familiarity. Our work highlights the potential of this dataset to advance the field of code translation for high-performance computing.
    
[^122]: 弱监督定位对比学习：肝硬化分类的应用

    Weakly-supervised positional contrastive learning: application to cirrhosis classification. (arXiv:2307.04617v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2307.04617](http://arxiv.org/abs/2307.04617)

    本研究提出了一种弱监督定位对比学习策略，在医学应用中使用大量弱标签图像进行肝硬化预测。这种策略将每个2D切片的空间上下文和弱标签整合起来，通过使用通用基于核的损失函数实现高效学习。

    

    大型医学影像数据集可以通过低置信度的弱标签（例如放射学评分）进行廉价快速的注释。而高置信度的标签（如基于组织学的诊断）很少且昂贵。预训练策略，如对比学习方法，可以利用未标记或弱标注的数据集。然而，这些方法通常需要较大的批处理大小，这在大型3D图像的全分辨率情况下存在困难，因为GPU内存有限。尽管如此，关于每个2D切片的空间上下文的体积信息对于某些医学应用非常重要。在这项研究中，我们提出了一种高效的弱监督定位对比学习策略，通过使用通用基于核的损失函数将每个2D切片的空间上下文和弱标签进行整合。我们通过使用大量弱标签图像（即放射学低置信度标注）来说明我们的方法在肝硬化预测中的应用。

    Large medical imaging datasets can be cheaply and quickly annotated with low-confidence, weak labels (e.g., radiological scores). Access to high-confidence labels, such as histology-based diagnoses, is rare and costly. Pretraining strategies, like contrastive learning (CL) methods, can leverage unlabeled or weakly-annotated datasets. These methods typically require large batch sizes, which poses a difficulty in the case of large 3D images at full resolution, due to limited GPU memory. Nevertheless, volumetric positional information about the spatial context of each 2D slice can be very important for some medical applications. In this work, we propose an efficient weakly-supervised positional (WSP) contrastive learning strategy where we integrate both the spatial context of each 2D slice and a weak label via a generic kernel-based loss function. We illustrate our method on cirrhosis prediction using a large volume of weakly-labeled images, namely radiological low-confidence annotations,
    
[^123]: 深度学习在端到端自动驾驶中的最新进展：一项调研

    Recent Advancements in End-to-End Autonomous Driving using Deep Learning: A Survey. (arXiv:2307.04370v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2307.04370](http://arxiv.org/abs/2307.04370)

    本文调研了深度学习在端到端自动驾驶中的最新进展，并提供了一种基于神经网络的自动驾驶任务分类体系。研究分析了端到端自动驾驶的关键挑战，并列举了不同的研究方法和核心功能。

    

    端到端驾驶是一种有前景的范例，它避免了模块化系统的缺点，如复杂性过高和误差传播的倾向。自动驾驶通过预先主动识别关键事件来超越传统交通模式，确保乘客的安全并为他们提供舒适的交通，特别是在高度随机和多变的交通环境中。本文对端到端自动驾驶技术进行了全面的回顾，提供了一种在端到端方式中使用神经网络的自动驾驶任务分类体系，包括从感知到控制的整个驾驶过程，并解决了实际应用中遇到的关键挑战。本文对端到端自动驾驶的最新发展进行了分析，并根据基本原理、方法论和核心功能对研究进行了分类。

    End-to-End driving is a promising paradigm as it circumvents the drawbacks associated with modular systems, such as their overwhelming complexity and propensity for error propagation. Autonomous driving transcends conventional traffic patterns by proactively recognizing critical events in advance, ensuring passengers' safety and providing them with comfortable transportation, particularly in highly stochastic and variable traffic settings. This paper presents a comprehensive review of the End-to-End autonomous driving stack. It provides a taxonomy of automated driving tasks wherein neural networks have been employed in an End-to-End manner, encompassing the entire driving process from perception to control, while addressing key challenges encountered in real-world applications. Recent developments in End-to-End autonomous driving are analyzed, and research is categorized based on underlying principles, methodologies, and core functionality. These categories encompass sensorial input, m
    
[^124]: ElectroCardioGuard：通过神经网络防止心电图数据库中患者误识别

    ElectroCardioGuard: Preventing Patient Misidentification in Electrocardiogram Databases through Neural Networks. (arXiv:2306.06196v1 [cs.LG])

    [http://arxiv.org/abs/2306.06196](http://arxiv.org/abs/2306.06196)

    本研究提出了一种基于神经网络的小而高效模型，用于确定两个心电图(ECG)是否来自同一患者。同时，还提出了一种方法，利用该模型检测记录-分配错误，实现了在现实场景中的应用，并在新的ECG数据集上进行了评估。

    

    心电图(ECG)通常被心脏病专家用于检测与心脏相关的病理情况，而可靠的ECG集合对于确诊非常重要。然而，在临床实践中，将记录的ECG分配给错误的患者可能会不经意地发生。本文与一家临床和研究机构合作，该机构认识到这一挑战并联系我们，我们提出了一项研究来解决这个问题。我们提出了一种小巧高效的基于神经网络的模型，用于确定两个ECG是否来自同一患者。我们的模型展现了很强的泛化能力，并在利用760倍更少的参数的情况下，在PTB-XL上实现了最新的画廊探针患者识别表现。此外，我们提出了一种技术，利用我们的模型来检测记录-分配错误，展示了它在一个现实场景中的适用性。最后，我们对新收集的ECG数据集进行了评估。

    Electrocardiograms (ECGs) are commonly used by cardiologists to detect heart-related pathological conditions. Reliable collections of ECGs are crucial for precise diagnosis. However, in clinical practice, the assignment of captured ECG recordings to incorrect patients can occur inadvertently. In collaboration with a clinical and research facility which recognized this challenge and reached out to us, we present a study that addresses this issue. In this work, we propose a small and efficient neural-network based model for determining whether two ECGs originate from the same patient. Our model demonstrates great generalization capabilities and achieves state-of-the-art performance in gallery-probe patient identification on PTB-XL while utilizing 760x fewer parameters. Furthermore, we present a technique leveraging our model for detection of recording-assignment mistakes, showcasing its applicability in a realistic scenario. Finally, we evaluate our model on a newly collected ECG dataset
    
[^125]: ChatGPT信息的图神经网络用于股票价格预测

    ChatGPT Informed Graph Neural Network for Stock Movement Prediction. (arXiv:2306.03763v1 [q-fin.ST])

    [http://arxiv.org/abs/2306.03763](http://arxiv.org/abs/2306.03763)

    该研究介绍了一种新的框架，利用ChatGPT技术增强图神经网络，能够从财经新闻中提取出不断变化的网络结构，并用于股票价格预测，获得了超过基于深度学习的最新基准的表现，提示了ChatGPT在文本推断和金融预测方面的潜力。

    

    ChatGPT已在各种自然语言处理（NLP）任务中展示了出色的能力。然而，它从时间文本数据（尤其是财经新闻）推断动态网络结构的潜力仍是一个未开发的领域。在这项研究中，我们介绍了一个新的框架，利用ChatGPT的图推断能力来增强图神经网络（GNN）。我们的框架巧妙地从文本数据中提取出不断变化的网络结构，并将这些网络结构融合到图神经网络中，进行后续的预测任务。股票价格预测的实验结果表明，我们的模型始终优于基于深度学习的最新基准。此外，基于我们模型的产出构建的组合展示出更高的年化累计回报、更低的波动性和最大回撤。这种卓越表现突显了ChatGPT用于基于文本的网络推断和金融预测应用的潜力。

    ChatGPT has demonstrated remarkable capabilities across various natural language processing (NLP) tasks. However, its potential for inferring dynamic network structures from temporal textual data, specifically financial news, remains an unexplored frontier. In this research, we introduce a novel framework that leverages ChatGPT's graph inference capabilities to enhance Graph Neural Networks (GNN). Our framework adeptly extracts evolving network structures from textual data, and incorporates these networks into graph neural networks for subsequent predictive tasks. The experimental results from stock movement forecasting indicate our model has consistently outperformed the state-of-the-art Deep Learning-based benchmarks. Furthermore, the portfolios constructed based on our model's outputs demonstrate higher annualized cumulative returns, alongside reduced volatility and maximum drawdown. This superior performance highlights the potential of ChatGPT for text-based network inferences and 
    
[^126]: ChatGraph: 通过将ChatGPT的知识转换为图形来实现可解释的文本分类

    ChatGraph: Interpretable Text Classification by Converting ChatGPT Knowledge to Graphs. (arXiv:2305.03513v1 [cs.CL])

    [http://arxiv.org/abs/2305.03513](http://arxiv.org/abs/2305.03513)

    ChatGraph通过将ChatGPT的知识转换为图形，提高了文本分类的可解释性和性能

    

    ChatGPT作为最近推出的大型语言模型（LLM），在各种自然语言处理（NLP）任务中展现出卓越的性能。然而，存在两个主要限制阻碍了它的潜在应用：（1）在下游任务上微调的不灵活性和（2）在决策过程中缺乏可解释性。为了解决这些限制，我们提出了一种新颖的框架，利用ChatGPT的能力来进行特定任务（如文本分类），同时提高其可解释性。

    ChatGPT, as a recently launched large language model (LLM), has shown superior performance in various natural language processing (NLP) tasks. However, two major limitations hinder its potential applications: (1) the inflexibility of finetuning on downstream tasks and (2) the lack of interpretability in the decision-making process. To tackle these limitations, we propose a novel framework that leverages the power of ChatGPT for specific tasks, such as text classification, while improving its interpretability. The proposed framework conducts a knowledge graph extraction task to extract refined and structural knowledge from the raw data using ChatGPT. The rich knowledge is then converted into a graph, which is further used to train an interpretable linear classifier to make predictions. To evaluate the effectiveness of our proposed method, we conduct experiments on four datasets. The result shows that our method can significantly improve the performance compared to directly utilizing Cha
    
[^127]: 一种具有典型性的条件逻辑中多层感知器的优先解释

    A preferential interpretation of MultiLayer Perceptrons in a conditional logic with typicality. (arXiv:2305.00304v1 [cs.AI])

    [http://arxiv.org/abs/2305.00304](http://arxiv.org/abs/2305.00304)

    本文探究了缺陷推理的多优选语义和多层神经网络模型之间的关系，并利用提出的多优先语义，对多层感知器(MLPs)进行了优先解释，并验证了其条件属性。

    

    本文研究了知识表示中缺陷推理的多优选语义与多层神经网络模型之间的关系。考虑了一种具有典型性的简单描述逻辑的加权知识库，在“概念层面”的多优先语义下进行。该语义被用来提供多层感知器(MLPs)的优先解释。利用模型检查和蕴含关系的方法验证MLPs的条件属性。

    In this paper we investigate the relationships between a multipreferential semantics for defeasible reasoning in knowledge representation and a multilayer neural network model. Weighted knowledge bases for a simple description logic with typicality are considered under a (many-valued) ``concept-wise" multipreference semantics. The semantics is used to provide a preferential interpretation of MultiLayer Perceptrons (MLPs). A model checking and an entailment based approach are exploited in the verification of conditional properties of MLPs.
    
[^128]: 马尔可夫决策过程中的合规异策评估

    Conformal Off-Policy Evaluation in Markov Decision Processes. (arXiv:2304.02574v1 [cs.LG])

    [http://arxiv.org/abs/2304.02574](http://arxiv.org/abs/2304.02574)

    本论文提出了一种基于合规预测的异策评估方法，能够以一定的确定性水平输出包含目标策略的真实奖励的区间，并提出了不同的处理分布偏移方法，其中一些方法在保证合规性的前提下实现了最先进的性能。

    

    强化学习旨在从数据中识别和评估有效的控制策略。在许多实际应用中，学习者不能进行实验，也不能以在线方式获取数据（在实验费用高昂、风险高或不道德的情况下，就会出现这种情况）。针对这种应用，必须使用在不同策略下收集的历史数据（行为策略）来估计给定策略（目标策略）的奖励。大多数针对这种学习任务的方法，即异策评估（OPE），都没有准确性和确定性保证。我们提出了一种基于合规预测的新型OPE方法，该方法输出一个包含目标策略的真实奖励的区间，同时具有一定的确定性水平。OPE中的主要挑战来自于目标策略和行为策略之间的差异引起的分布偏移。我们提出并经验性地评估了不同处理这种偏移的方法。其中一些方法在保证估计的奖励区间的合规性的同时，在基准环境中实现了最先进的性能。

    Reinforcement Learning aims at identifying and evaluating efficient control policies from data. In many real-world applications, the learner is not allowed to experiment and cannot gather data in an online manner (this is the case when experimenting is expensive, risky or unethical). For such applications, the reward of a given policy (the target policy) must be estimated using historical data gathered under a different policy (the behavior policy). Most methods for this learning task, referred to as Off-Policy Evaluation (OPE), do not come with accuracy and certainty guarantees. We present a novel OPE method based on Conformal Prediction that outputs an interval containing the true reward of the target policy with a prescribed level of certainty. The main challenge in OPE stems from the distribution shift due to the discrepancies between the target and the behavior policies. We propose and empirically evaluate different ways to deal with this shift. Some of these methods yield conform
    
[^129]: Probe：学习用户在时间跨度的捆绑选择中的个性化投影偏差

    Probe: Learning Users' Personalized Projection Bias in Intertemporal Bundle Choices. (arXiv:2303.06016v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2303.06016](http://arxiv.org/abs/2303.06016)

    本文提出了一种新的偏差嵌入式偏好模型——Probe，旨在解决用户在时间跨度的购物选择中的投影偏差和参照点效应，提高决策的有效性和个性化。

    

    时间跨度的选择需要权衡现在的成本和未来的收益。其中一种具体的选择是决定购买单个物品还是选择包含该物品的捆绑销售方式。以往的研究假设个人对这些选择中涉及的因素有准确的期望。然而，在现实中，用户对这些因素的感知往往存在偏差，导致了非理性和次优的决策。本文重点关注两种常见的偏差：投影偏差和参照点效应，并为此提出了一种新颖的偏差嵌入式偏好模型——Probe。该模型利用加权函数来捕捉用户的投影偏差，利用价值函数来考虑参照点效应，并引入行为经济学中的前景理论来组合加权和价值函数。这使得我们能够确定用户购买捆绑销售的概率，从而提高决策的有效性和个性化。

    Intertemporal choices involve making decisions that require weighing the costs in the present against the benefits in the future. One specific type of intertemporal choice is the decision between purchasing an individual item or opting for a bundle that includes that item. Previous research assumes that individuals have accurate expectations of the factors involved in these choices. However, in reality, users' perceptions of these factors are often biased, leading to irrational and suboptimal decision-making. In this work, we specifically focus on two commonly observed biases: projection bias and the reference-point effect. To address these biases, we propose a novel bias-embedded preference model called Probe. The Probe incorporates a weight function to capture users' projection bias and a value function to account for the reference-point effect, and introduce prospect theory from behavioral economics to combine the weight and value functions. This allows us to determine the probabili
    
[^130]: 椭圆型偏微分方程学习在数据效率上是可靠的

    Elliptic PDE learning is provably data-efficient. (arXiv:2302.12888v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.12888](http://arxiv.org/abs/2302.12888)

    该论文提供了椭圆型偏微分方程学习中的数据效率理论保证，通过利用随机数值线性代数和PDE理论，实现了对于3D均匀椭圆型PDE解算符的数据高效恢复，并在训练数据集大小上以指数收敛率的误差。

    

    PDE学习是一个将物理学和机器学习相结合的新兴领域，旨在从实验数据中恢复未知的物理系统。虽然深度学习模型通常需要大量的训练数据，但最近的PDE学习技术在数据有限的情况下取得了令人瞩目的结果。然而，这些结果仍然是经验性的。我们的工作在PDE学习中提供了对所需的输入-输出训练对数量的理论保证。具体而言，我们利用随机数值线性代数和PDE理论推导出了一种具有可靠数据效率的算法，该算法从输入-输出数据中恢复3D均匀椭圆型PDE的解算符，并以极高的成功概率实现了与训练数据集大小指数收敛率的误差。

    PDE learning is an emerging field that combines physics and machine learning to recover unknown physical systems from experimental data. While deep learning models traditionally require copious amounts of training data, recent PDE learning techniques achieve spectacular results with limited data availability. Still, these results are empirical. Our work provides theoretical guarantees on the number of input-output training pairs required in PDE learning. Specifically, we exploit randomized numerical linear algebra and PDE theory to derive a provably data-efficient algorithm that recovers solution operators of 3D uniformly elliptic PDEs from input-output data and achieves an exponential convergence rate of the error with respect to the size of the training dataset with an exceptionally high probability of success.
    
[^131]: 近地表风的算法幻觉：使用生成对抗网络进行统计降尺度的研究

    Algorithmic Hallucinations of Near-Surface Winds: Statistical Downscaling with Generative Adversarial Networks to Convection-Permitting Scales. (arXiv:2302.08720v2 [physics.ao-ph] UPDATED)

    [http://arxiv.org/abs/2302.08720](http://arxiv.org/abs/2302.08720)

    本论文将新兴的图像超分辨率技术应用于统计降尺度任务，具体探索了基于生成对抗网络的算法在模拟北美地区地表风中的应用。通过使用非理想化的低分辨率和高分辨率输入数据，该方法克服了共享尺度不匹配的问题，并通过评估空间功率谱等指标来评估模型的技能。

    

    本论文探讨了将图像超分辨率（SR）中新兴的机器学习方法应用于统计降尺度任务。我们特别关注卷积神经网络的生成对抗网络（GANs）。我们的GANs是通过对低分辨率（LR）输入进行条件训练来生成模拟北美地区 Weather Research and Forecasting（WRF）模型的高分辨率（HR）地表风。与传统的SR模型不同，LR输入在WRF模拟中使用了非理想化的LR和HR配对，导致由于内部变异引起的共享尺度不匹配。我们的研究基于当前基于SR的统计降尺度，并尝试了计算机视觉领域的新颖频率分离（FS）方法。为了评估SR模型的技能，我们精选评估指标，并关注基于空间功率谱的性能度量。我们的分析揭示了GAN配置如何影响模型的性能。

    This paper explores the application of emerging machine learning methods from image super-resolution (SR) to the task of statistical downscaling. We specifically focus on convolutional neural network-based Generative Adversarial Networks (GANs). Our GANs are conditioned on low-resolution (LR) inputs to generate high-resolution (HR) surface winds emulating Weather Research and Forecasting (WRF) model simulations over North America. Unlike traditional SR models, where LR inputs are idealized coarsened versions of the HR images, WRF emulation involves using non-idealized LR and HR pairs resulting in shared-scale mismatches due to internal variability. Our study builds upon current SR-based statistical downscaling by experimenting with a novel frequency-separation (FS) approach from the computer vision field. To assess the skill of SR models, we carefully select evaluation metrics, and focus on performance measures based on spatial power spectra. Our analyses reveal how GAN configurations 
    
[^132]: 多样性推广：改善无监督环境设计

    Generalization through Diversity: Improving Unsupervised Environment Design. (arXiv:2301.08025v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2301.08025](http://arxiv.org/abs/2301.08025)

    这项研究改进了无监督环境设计，通过训练RL智能体适应各种环境变化，提高了在分布外测试场景上的性能。

    

    使用强化学习（RL）进行智能决策主要依赖于环境的模型或模拟器（例如，在一个8x8大小的迷宫中移动，或者在一个8x8大小的棋盘上下棋）。由于这种依赖性，环境的微小变化（例如，迷宫中障碍物的位置，棋盘的大小）可能严重影响智能体学习到的策略的有效性。为此，已有研究提出在自适应课程环境（自动生成）上训练RL智能体，以改善在分布外（OOD）的测试场景上的性能。具体而言，现有研究将智能体在环境中学习的潜力（通过广义优势估计，GAE）作为选择下一个环境进行训练的关键因素。然而，这样的机制可能选择相似的环境（具有高学习潜力），从而使智能体在除其中一个环境外的所有环境上的训练变得多余。为此，我们...

    Agent decision making using Reinforcement Learning (RL) heavily relies on either a model or simulator of the environment (e.g., moving in an 8x8 maze with three rooms, playing Chess on an 8x8 board). Due to this dependence, small changes in the environment (e.g., positions of obstacles in the maze, size of the board) can severely affect the effectiveness of the policy learned by the agent. To that end, existing work has proposed training RL agents on an adaptive curriculum of environments (generated automatically) to improve performance on out-of-distribution (OOD) test scenarios. Specifically, existing research has employed the potential for the agent to learn in an environment (captured using Generalized Advantage Estimation, GAE) as the key factor to select the next environment(s) to train the agent. However, such a mechanism can select similar environments (with a high potential to learn) thereby making agent training redundant on all but one of those environments. To that end, we 
    
[^133]: 图形房屋分配

    Graphical House Allocation. (arXiv:2301.01323v2 [cs.GT] UPDATED)

    [http://arxiv.org/abs/2301.01323](http://arxiv.org/abs/2301.01323)

    这篇论文探讨了图形房屋分配问题，将个体放置在图的顶点上，开发了几种结构和计算方法，以最小化个体之间的嫉妒程度作为公平目标。

    

    经典的房屋分配问题涉及根据个体的偏好分配n套房子（或物品）给n个个体。这类问题中的一个关键准则是满足一些公平性约束，如无嫉妒性。我们考虑这个问题的一个推广，即将个体放置在图的顶点上（对应社交网络），每个个体只能对其邻居感到嫉妒。我们的目标是将个体之间的总嫉妒最小化，作为一种自然的公平目标，即在社交图中所有边上的所有成对嫉妒值的总和。当个体具有相同且均匀分布的估价时，我们的问题归结为已研究的线性排列问题。对于可能不均匀分布的相同估价，我们展示了我们的设置与这一经典问题的几个深入和令人惊讶的不同之处。更广泛地说，我们为各类图的结构和计算结果做出了几项贡献

    The classical house allocation problem involves assigning $n$ houses (or items) to $n$ agents according to their preferences. A key criterion in such problems is satisfying some fairness constraints such as envy-freeness. We consider a generalization of this problem wherein the agents are placed along the vertices of a graph (corresponding to a social network), and each agent can only experience envy towards its neighbors. Our goal is to minimize the aggregate envy among the agents as a natural fairness objective, i.e., the sum of all pairwise envy values over all edges in a social graph.  When agents have identical and evenly-spaced valuations, our problem reduces to the well-studied problem of linear arrangements. For identical valuations with possibly uneven spacing, we show a number of deep and surprising ways in which our setting is a departure from this classical problem. More broadly, we contribute several structural and computational results for various classes of graphs, inclu
    
[^134]: 走向以人工智能为驱动的众包

    Towards AI-Empowered Crowdsourcing. (arXiv:2212.14676v2 [cs.CY] UPDATED)

    [http://arxiv.org/abs/2212.14676](http://arxiv.org/abs/2212.14676)

    本文调查了如何将人工智能赋能于众包以提高效率的方法，提出了以人工智能为驱动的众包（AIEC）的分类法，并讨论了其中的限制因素和挑战。

    

    众包是将人类智力和生产力动态组织起来处理自动化无法解决的复杂任务的一种方式，它已成为一个重要的研究课题，并激发了新的商业模式（如Uber、Airbnb）。多年来，众包不再仅仅是提供一个平台来手动匹配工作者和任务，而是借助数据驱动的人工智能算法管理方法，实现日益复杂的优化目标。本文提供了一份调查报告，对如何将人工智能赋能于众包以提高效率进行了系统概述，我们将其称为以人工智能为驱动的众包（AIEC）。我们提出了一个将AIEC划分为三个主要领域的分类法：1）任务委托，2）激励工作者，3）质量控制，重点关注需要实现的主要目标。我们讨论了限制因素和洞见，并梳理了进行研究的挑战。

    Crowdsourcing, in which human intelligence and productivity is dynamically mobilized to tackle tasks too complex for automation alone to handle, has grown to be an important research topic and inspired new businesses (e.g., Uber, Airbnb). Over the years, crowdsourcing has morphed from providing a platform where workers and tasks can be matched up manually into one which leverages data-driven algorithmic management approaches powered by artificial intelligence (AI) to achieve increasingly sophisticated optimization objectives. In this paper, we provide a survey presenting a unique systematic overview on how AI can empower crowdsourcing to improve its efficiency - which we refer to as AI-Empowered Crowdsourcing(AIEC). We propose a taxonomy which divides AIEC into three major areas: 1) task delegation, 2) motivating workers, and 3) quality control, focusing on the major objectives which need to be accomplished. We discuss the limitations and insights, and curate the challenges of doing re
    
[^135]: RouteNet-Fermi: 使用图神经网络进行网络建模

    RouteNet-Fermi: Network Modeling with Graph Neural Networks. (arXiv:2212.12070v2 [cs.NI] UPDATED)

    [http://arxiv.org/abs/2212.12070](http://arxiv.org/abs/2212.12070)

    RouteNet-Fermi是一种基于图神经网络的自定义模型，与传统的排队理论相比，在存在真实流量模型的情况下具有更高的准确性。它能够准确预测网络的延迟、抖动和丢包情况。

    

    网络模型是现代网络的重要组成部分，广泛用于网络规划和优化。然而，随着网络规模和复杂性的增加，一些模型存在限制，如排队理论模型中对马尔可夫流量的假设，以及网络模拟器的高计算成本。机器学习的最新进展，如图神经网络（GNN），正在推动一代新的数据驱动网络模型，能够学习复杂的非线性行为。在本文中，我们提出了一种名为RouteNet-Fermi的自定义GNN模型，它与排队理论具有相同的目标，并且在存在真实流量模型的情况下准确性更高。该模型可以准确预测网络的延迟、抖动和丢包情况。我们在不断增长的网络规模（最大达到300个节点）和包括具有混合流量特性的样本（如复杂的非马尔可夫模型）中测试了RouteNet-Fermi模型。

    Network models are an essential block of modern networks. For example, they are widely used in network planning and optimization. However, as networks increase in scale and complexity, some models present limitations, such as the assumption of Markovian traffic in queuing theory models, or the high computational cost of network simulators. Recent advances in machine learning, such as Graph Neural Networks (GNN), are enabling a new generation of network models that are data-driven and can learn complex non-linear behaviors. In this paper, we present RouteNet-Fermi, a custom GNN model that shares the same goals as Queuing Theory, while being considerably more accurate in the presence of realistic traffic models. The proposed model predicts accurately the delay, jitter, and packet loss of a network. We have tested RouteNet-Fermi in networks of increasing size (up to 300 nodes), including samples with mixed traffic profiles -- e.g., with complex non-Markovian models -- and arbitrary routin
    
[^136]: 使用自编码器进行无监督的概念漂移取消学习

    Unsupervised Unlearning of Concept Drift with Autoencoders. (arXiv:2211.12989v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.12989](http://arxiv.org/abs/2211.12989)

    本论文提出了一种基于自编码器的无监督的概念漂移取消学习方法，通过在全局级别引入自编码器，可以避免重新训练或调整学习模型，从而实现对概念漂移的适应。

    

    概念漂移是指影响未来样本数据流的数据分布变化。因此，对数据流进行操作的学习模型可能会变得过时，需要昂贵且困难的调整，如重新训练或适应。现有方法通常实施局部概念漂移适应方案，其中要么使用增量学习模型，要么在漂移检测机制触发警报时完全重新训练模型。本文提出了一种替代方法，介绍了一种基于自编码器的无监督和模型无关的全局级别概念漂移适应方法。具体而言，所提出的方法旨在“取消学习”概念漂移，而无需重新训练或调整任何在数据上操作的学习模型。在两个应用领域进行了广泛的实验评估。我们考虑了一个具有30个以上模型的真实水配送网络，其中我们...

    Concept drift refers to a change in the data distribution affecting the data stream of future samples. Consequently, learning models operating on the data stream might become obsolete, and need costly and difficult adjustments such as retraining or adaptation. Existing methods usually implement a local concept drift adaptation scheme, where either incremental learning of the models is used, or the models are completely retrained when a drift detection mechanism triggers an alarm. This paper proposes an alternative approach in which an unsupervised and model-agnostic concept drift adaptation method at the global level is introduced, based on autoencoders. Specifically, the proposed method aims to ``unlearn'' the concept drift without having to retrain or adapt any of the learning models operating on the data. An extensive experimental evaluation is conducted in two application domains. We consider a realistic water distribution network with more than 30 models in-place, from which we cr
    
[^137]: 为什么人们对人类和机器的判断不同：感知代理和经验的作用

    Why people judge humans differently from machines: The role of perceived agency and experience. (arXiv:2210.10081v2 [cs.CY] UPDATED)

    [http://arxiv.org/abs/2210.10081](http://arxiv.org/abs/2210.10081)

    本研究探讨了人们为什么在对人类和机器的判断上存在差异，并发现当人们认为机器具有更多代理能力时，他们对机器的评判更像对待人类的方式。

    

    人们已知使用功利主义的道德哲学来评判人工智能，而使用强调被感知意图的道德哲学来评判人类。但为什么人们对人类和机器的判断不同呢？心理学认为人们对人类和机器可能有不同的心理感知模型，因此，他们会把类人机器更像对待人类的方式对待。在这里，我们通过一个随机实验来探索人们如何判断那些在代理（例如，规划和行动能力）和经验（例如，感知能力）上与人类更相似的机器。我们发现，当人们感知机器具有更多代理能力时，他们对机器的评判更接近于对人类的评判，但并不受经验的影响。我们的研究结果表明，人们对人类和机器使用不同的道德哲学进行判断可以得到解释。

    People are known to judge artificial intelligence using a utilitarian moral philosophy and humans using a moral philosophy emphasizing perceived intentions. But why do people judge humans and machines differently? Psychology suggests that people may have different mind perception models of humans and machines, and thus, will treat human-like robots more similarly to the way they treat humans. Here we present a randomized experiment where we manipulated people's perception of machine agency (e.g., ability to plan, act) and experience (e.g., ability to feel) to explore whether people judge machines that are perceived to be more similar to humans along these two dimensions more similarly to the way they judge humans. We find that people's judgments of machines become more similar to that of humans when they perceive machines as having more agency but not more experience. Our findings indicate that people's use of different moral philosophies to judge humans and machines can be explained b
    
[^138]: 将知识从记忆中解耦：检索增强的提示学习

    Decoupling Knowledge from Memorization: Retrieval-augmented Prompt Learning. (arXiv:2205.14704v4 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2205.14704](http://arxiv.org/abs/2205.14704)

    本论文提出了一种检索增强的提示学习方法，通过将知识从记忆中解耦，帮助模型在泛化和记忆之间取得平衡。

    

    提示学习方法在自然语言处理领域取得了显著的突破，提高了少样本学习的性能，但仍然遵循参数化学习范式；在学习过程中，遗忘和机械记忆问题可能导致不稳定的泛化问题。为了缓解这些限制，我们开发了RetroPrompt，旨在从记忆中将知识解耦，帮助模型在泛化和记忆之间取得平衡。与传统的提示学习方法相比，RetroPrompt从训练实例构建了一个开放式知识库，并在输入、训练和推断过程中实施检索机制，使模型具备了从训练语料库中检索相关上下文用于增强的能力。大量实验证明了RetroPrompt的效果。

    Prompt learning approaches have made waves in natural language processing by inducing better few-shot performance while they still follow a parametric-based learning paradigm; the oblivion and rote memorization problems in learning may encounter unstable generalization issues. Specifically, vanilla prompt learning may struggle to utilize atypical instances by rote during fully-supervised training or overfit shallow patterns with low-shot data. To alleviate such limitations, we develop RetroPrompt with the motivation of decoupling knowledge from memorization to help the model strike a balance between generalization and memorization. In contrast with vanilla prompt learning, RetroPrompt constructs an open-book knowledge-store from training instances and implements a retrieval mechanism during the process of input, training and inference, thus equipping the model with the ability to retrieve related contexts from the training corpus as cues for enhancement. Extensive experiments demonstra
    
[^139]: 医学图像分类中的可解释深度学习方法：一项调查

    Explainable Deep Learning Methods in Medical Image Classification: A Survey. (arXiv:2205.04766v3 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2205.04766](http://arxiv.org/abs/2205.04766)

    这项调查提供了关于可解释深度学习方法在医学图像分类中的应用的全面概述，包括各种解释方法的比较和性能评估。

    

    深度学习在医学影像诊断中取得了显著的成功，但由于其缺乏可解释性，这些模型很难在临床工作流程中得到采用。这引发了对解释深度学习模型决策过程的需求，进而形成了可解释人工智能（XAI）的研究领域。本文全面调查了XAI在医学影像诊断中的应用，包括视觉、文本、基于示例和基于概念的解释方法。此外，本文还回顾了现有的医学影像数据集和用于评估解释质量的指标。此外，我们还对一组基于报告生成的方法进行了性能比较。

    The remarkable success of deep learning has prompted interest in its application to medical imaging diagnosis. Even though state-of-the-art deep learning models have achieved human-level accuracy on the classification of different types of medical data, these models are hardly adopted in clinical workflows, mainly due to their lack of interpretability. The black-box-ness of deep learning models has raised the need for devising strategies to explain the decision process of these models, leading to the creation of the topic of eXplainable Artificial Intelligence (XAI). In this context, we provide a thorough survey of XAI applied to medical imaging diagnosis, including visual, textual, example-based and concept-based explanation methods. Moreover, this work reviews the existing medical imaging datasets and the existing metrics for evaluating the quality of the explanations. In addition, we include a performance comparison among a set of report generation-based methods. Finally, the major 
    
[^140]: 关系抽取作为开书考试：检索增强的提示调优

    Relation Extraction as Open-book Examination: Retrieval-enhanced Prompt Tuning. (arXiv:2205.02355v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2205.02355](http://arxiv.org/abs/2205.02355)

    提出了一种新的半参数学习范式，即检索增强的提示调优，用于关系抽取。通过构建开放式存储库，并使用线性插值的方式，模型能够在推断过程中根据存储库中的记忆信息推断关系。

    

    预训练语言模型通过展示出卓越的少样本学习能力，在关系抽取方面做出了重要贡献。然而，关系抽取的提示调优方法可能仍然无法推广到那些罕见或困难的模式中。我们将关系抽取视为一种开放式考试，并提出了一种新的检索增强的提示调优的半参数学习范式。我们构建了一个开放式存储库，用于检索基于提示的实例表示和相应的关系标签作为记忆的键值对。在推断过程中，模型可以通过线性插值基于PLM的基本输出与存储库上的非参数最近邻分布来推断关系。

    Pre-trained language models have contributed significantly to relation extraction by demonstrating remarkable few-shot learning abilities. However, prompt tuning methods for relation extraction may still fail to generalize to those rare or hard patterns. Note that the previous parametric learning paradigm can be viewed as memorization regarding training data as a book and inference as the close-book test. Those long-tailed or hard patterns can hardly be memorized in parameters given few-shot instances. To this end, we regard RE as an open-book examination and propose a new semiparametric paradigm of retrieval-enhanced prompt tuning for relation extraction. We construct an open-book datastore for retrieval regarding prompt-based instance representations and corresponding relation labels as memorized key-value pairs. During inference, the model can infer relations by linearly interpolating the base output of PLM with the non-parametric nearest neighbor distribution over the datastore. In
    
[^141]: 面向预训练语言模型的对比演示调优

    Contrastive Demonstration Tuning for Pre-trained Language Models. (arXiv:2204.04392v4 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2204.04392](http://arxiv.org/abs/2204.04392)

    本论文提出了一种名为对比演示调优的方法，可以在低数据场景下有效激发预训练语言模型的能力。实验结果表明，该方法与先前的提示调优方法相结合可以取得更好的性能。

    

    在低数据场景中，使用文本提示或演示可以有效地激发预训练语言模型的能力。最近的研究主要集中在自动搜索离散或连续提示或优化语言表达者，但对于演示的研究仍然有限。具体来说，演示示例对于最终的提示调优性能至关重要。本文提出了一种新颖的可插拔、可扩展和高效的方法，称为对比演示调优，它不需要进行演示采样。此外，该方法能够：（i）嵌入到任何先前的提示调优方法中；（ii）扩展到具有大量类别的广泛分类任务中。在16个数据集上的实验结果表明，我们的方法与先前的LM-BFF和P-tuning方法相结合可以得到更好的性能。代码可在https://github.com/zjunlp/PromptKG/tree/main/research/Demo-Tuning中获得。

    Pretrained language models can be effectively stimulated by textual prompts or demonstrations, especially in low-data scenarios. Recent works have focused on automatically searching discrete or continuous prompts or optimized verbalizers, yet studies for the demonstration are still limited. Concretely, the demonstration examples are crucial for an excellent final performance of prompt-tuning. In this paper, we propose a novel pluggable, extensible, and efficient approach named contrastive demonstration tuning, which is free of demonstration sampling. Furthermore, the proposed approach can be: (i) Plugged into any previous prompt-tuning approaches; (ii) Extended to widespread classification tasks with a large number of categories. Experimental results on 16 datasets illustrate that our method integrated with previous approaches LM-BFF and P-tuning can yield better performance. Code is available in https://github.com/zjunlp/PromptKG/tree/main/research/Demo-Tuning.
    
[^142]: 人工智能与统计勾结

    Artificial Intelligence and Statistical Collusion. (arXiv:2202.05946v4 [econ.TH] UPDATED)

    [http://arxiv.org/abs/2202.05946](http://arxiv.org/abs/2202.05946)

    本研究提出了一个可处理的模型来研究学习算法之间的战略互动，揭示了一种导致算法勾结出现的机制。通过自发耦合，算法周期性地协调行动，达到更高利润。该模型的参数可预测统计关联的出现和有利于算法勾结的市场结构，进一步展示了自发耦合如何在价格和市场份额上维持勾结，并应用于设计算法市场。

    

    我们开发了一个可处理的模型来研究学习算法之间的战略互动。我们发现了一种导致算法勾结出现的机制。我们观察到算法周期性地协调行动，这些行动比静态纳什均衡更具利润性。这种新的勾结渠道依赖于算法估计中的内生统计关联，我们称之为自发耦合。模型的参数预测了统计关联是否会出现，以及什么市场结构有助于算法勾结。我们展示了自发耦合如何维持价格和市场份额上的勾结，这与文献中的实验证据相补充。最后，我们将结果应用于设计算法市场。

    We develop a tractable model for studying strategic interactions between learning algorithms. We uncover a mechanism responsible for the emergence of algorithmic collusion. We observe that algorithms periodically coordinate on actions that are more profitable than static Nash equilibria. This novel collusive channel relies on an endogenous statistical linkage in the algorithms' estimates which we call spontaneous coupling. The model's parameters predict whether the statistical linkage will appear, and what market structures facilitate algorithmic collusion. We show that spontaneous coupling can sustain collusion in prices and market shares, complementing experimental findings in the literature. Finally, we apply our results to design algorithmic markets.
    
[^143]: D-HAN: 动态新闻推荐模型与分层注意力网络

    D-HAN: Dynamic News Recommendation with Hierarchical Attention Network. (arXiv:2112.10085v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2112.10085](http://arxiv.org/abs/2112.10085)

    D-HAN是一种动态新闻推荐模型，采用分层注意力网络将连续时间信息无缝整合，有效表示新闻信息，并引入动态负采样方法优化用户的隐式反馈。

    

    由于静态的用户新闻交互方式，新闻推荐模型往往难以准确捕捉用户的偏好。为了解决这个问题，我们提出了一种新颖的动态新闻推荐模型，它能够无缝地将连续时间信息整合到分层注意力网络中，有效地表示句子、元素和序列级别的新闻信息。此外，我们引入了一种动态负采样方法来优化用户的隐式反馈。为了验证我们模型的有效性，我们在三个实际数据集上进行了大量实验，结果证明了我们提出的方法的有效性。

    News recommendation models often fall short in capturing users' preferences due to their static approach to user-news interactions. To address this limitation, we present a novel dynamic news recommender model that seamlessly integrates continuous time information to a hierarchical attention network that effectively represents news information at the sentence, element, and sequence levels. Moreover, we introduce a dynamic negative sampling method to optimize users' implicit feedback. To validate our model's effectiveness, we conduct extensive experiments on three real-world datasets. The results demonstrate the effectiveness of our proposed approach.
    
[^144]: 跨越空间和光谱域的鸿沟：一种用于图神经网络的统一框架。

    Bridging the Gap between Spatial and Spectral Domains: A Unified Framework for Graph Neural Networks. (arXiv:2107.10234v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2107.10234](http://arxiv.org/abs/2107.10234)

    该论文提出了一种统一框架，用于将基于空间和谱域的图神经网络进行整合，并紧密关联各自域内的方法。

    

    近年来，深度学习的性能得到了广泛的认可。图神经网络（GNN）旨在处理经典深度学习难以处理的图结构数据。由于大多数GNN是使用不同的理论创建的，因此无法直接进行比较。先前的研究主要集中在对现有模型进行分类，对它们的内在连接关系关注甚少。本研究的目的是建立一个基于谱图和近似论的统一框架，集成基于空间和谱域的GNN，并紧密关联各自域内的方法。

    Deep learning's performance has been extensively recognized recently. Graph neural networks (GNNs) are designed to deal with graph-structural data that classical deep learning does not easily manage. Since most GNNs were created using distinct theories, direct comparisons are impossible. Prior research has primarily concentrated on categorizing existing models, with little attention paid to their intrinsic connections. The purpose of this study is to establish a unified framework that integrates GNNs based on spectral graph and approximation theory. The framework incorporates a strong integration between spatial- and spectral-based GNNs while tightly associating approaches that exist within each respective domain.
    

