# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Instance-level Trojan Attacks on Visual Question Answering via Adversarial Learning in Neuron Activation Space.](http://arxiv.org/abs/2304.00436) | 本研究提出了一种实例级别的特洛伊攻击方法，在视觉问答中通过神经元激活空间的对抗学习生成多样性特洛伊，有效地适应了微调模型。 |
| [^2] | [Mini-batch $k$-means terminates within $O(d/\epsilon)$ iterations.](http://arxiv.org/abs/2304.00419) | 小批量$k$-means聚类法被证明可以在一定迭代次数内收敛，无论初始聚类中心如何，可以达到与完全批量版本相同的逼近比，具有一定的可行性。 |
| [^3] | [Towards Healthy AI: Large Language Models Need Therapists Too.](http://arxiv.org/abs/2304.00416) | SafeguardGPT框架使用心理治疗来纠正AI聊天机器人中的有害行为，改进与人类的对话质量，进而推进健康AI的发展。 |
| [^4] | [DiverseVul: A New Vulnerable Source Code Dataset for Deep Learning Based Vulnerability Detection.](http://arxiv.org/abs/2304.00409) | 这个论文提供了一个包含150个CWE的新的漏洞源代码数据集，覆盖比以前所有数据集加起来多305个项目。作者通过增加训练数据的多样性和数量改进了深度学习模型在漏洞检测方面的表现。通过结合已有数据集，作者还研究了使用深度学习检测软件漏洞的挑战和前途的分析，结果表明目前深度学习检测漏洞仍存在高误报率，低F1分数和难以检测严重CWE的问题。 |
| [^5] | [From Zero to Hero: Convincing with Extremely Complicated Math.](http://arxiv.org/abs/2304.00399) | 研究者靠着简单的数学写论文，却得不到世人的关注和认可，为此我们提出了一个名为zero2hero的算法，将每篇论文转化为科学杰作，复杂化每一个方程式，以此在未来的审稿人中引来关注。 |
| [^6] | [Connected and Automated Vehicles in Mixed-Traffic: Learning Human Driver Behavior for Effective On-Ramp Merging.](http://arxiv.org/abs/2304.00397) | 本文介绍了一种学习人类驾驶员行为以实现出入匝道的有效合并的方法，通过学习CAV和HDV互动的近似信息状态模型，使得在混合交通条件下实现安全且高效的合并。 |
| [^7] | [Knowledge Graph Embedding with 3D Compound Geometric Transformations.](http://arxiv.org/abs/2304.00378) | 本文提出了一种基于3D复合几何变换的知识图谱嵌入模型CompoundE3D，在链接预测方面具有良好的性能表现。 |
| [^8] | [A Survey on Personalized Affective Computing in Human-Machine Interaction.](http://arxiv.org/abs/2304.00377) | 论文调查了情感计算中的个性化方法，将其分为七类，并给出了调查文献的统计元分析。 |
| [^9] | [Conveying Autonomous Robot Capabilities through Contrasting Behaviour Summaries.](http://arxiv.org/abs/2304.00367) | 本文研究将自主机器人的能力通过对比行为总结传递给人类观察者，通过多步骤总结有效地帮助人类评估机器人整体能力。 |
| [^10] | [Adaptive Failure Search Using Critical States from Domain Experts.](http://arxiv.org/abs/2304.00365) | 本文提出了一种基于领域专家的自适应失效搜索方法，用于有效地探索和发现自主策略在仿真中的失效轨迹。 |
| [^11] | [On Context Distribution Shift in Task Representation Learning for Offline Meta RL.](http://arxiv.org/abs/2304.00354) | 该论文探讨了离线元强化学习中任务表示学习中遇到的上下文分布偏移问题，并提出了一种硬采样的策略用于解决该问题，实验结果表明该方法能够得到更强健的任务表示和更好的测试性能。 |
| [^12] | [Factorization of Multi-Agent Sampling-Based Motion Planning.](http://arxiv.org/abs/2304.00342) | 该论文提出了一种集成了因子分解的方法，可以解决多智能体路径规划中单个机器人方法计算上的问题，通过逐步构建超图可以将代理分割为独立的子图。 |
| [^13] | [Doubly Stochastic Models: Learning with Unbiased Label Noises and Inference Stability.](http://arxiv.org/abs/2304.00320) | 本文探讨了无偏标签噪声的隐含正则化效应，提出了一种将SGD的动态建模为双重随机模型的方法，可提高鲁棒性和泛化性能。 |
| [^14] | [Predictive Heterogeneity: Measures and Applications.](http://arxiv.org/abs/2304.00305) | 本文研究了数据的异质性对机器学习模型预测的影响，并提出了可用的预测不确定性，可从有限数据中可靠地估计。我们设计了一个双层优化算法来探索可用的预测不确定性，从而为子人群划分提供洞见。 |
| [^15] | [Recover Triggered States: Protect Model Against Backdoor Attack in Reinforcement Learning.](http://arxiv.org/abs/2304.00252) | 本文提出了恢复触发状态(RTS)方法，用于保护RL代理免受反向攻击。该方法涉及构建替代网络来近似动态模型，并将触发状态恢复为干净状态来防止攻击者通过触发器激活隐藏在代理中的后门。 |
| [^16] | [From Conception to Deployment: Intelligent Stroke Prediction Framework using Machine Learning and Performance Evaluation.](http://arxiv.org/abs/2304.00249) | 本文提出了一个基于机器学习和性能评估的智能卒中预测框架，并比较了五种常用的机器学习算法，结果显示随机森林算法最适合卒中预测。 |
| [^17] | [Restarted Bayesian Online Change-point Detection for Non-Stationary Markov Decision Processes.](http://arxiv.org/abs/2304.00232) | 该论文介绍了一种针对非平稳强化学习环境的算法，称为R-BOCPD-UCRL2，它使用了一种重新启动的贝叶斯在线变点检测算法（R-BOCPD），并在多项式分布采样的MDP上提供了较优秀的性能保证。 |
| [^18] | [Mask Hierarchical Features For Self-Supervised Learning.](http://arxiv.org/abs/2304.00218) | 本文提出了MaskDeep这一自监督方法，利用遮蔽分层特征的策略，从稀疏的可见补丁来推理图像的全局语义。本方法相比其它自监督方法取得了不错的改进。 |
| [^19] | [Cross-scale Multi-instance Learning for Pathological Image Diagnosis.](http://arxiv.org/abs/2304.00216) | 本研究提出了一种新的跨尺度MIL算法，将跨尺度关系显式聚合到一个病理图像诊断的MIL网络中，有效地解决了忽略对人类病理学家诊断至关重要的跨尺度信息的问题。 |
| [^20] | [Inductive Relation Prediction from Relational Paths and Context with Hierarchical Transformers.](http://arxiv.org/abs/2304.00215) | 本文提出了一种基于分层Transformer的方法，即REPORT，能够同时聚合关系路径和上下文，捕捉实体之间的联系和内在特性。它完全依赖于关系语义，并能自然地推广到完全归纳的设置中。在基准数据集上实现了最先进的性能。 |
| [^21] | [Leveraging Neo4j and deep learning for traffic congestion simulation & optimization.](http://arxiv.org/abs/2304.00192) | 本文利用Neo4j图和深度学习方法解决城市道路网络中的交通拥堵问题，能够实现拥堵路段的负载平衡和优化，同时可以预测交通事故对整体交通的影响。 |
| [^22] | [Action of the Euclidean versus Projective group on an agent's internal space in curiosity driven exploration: a formal analysis.](http://arxiv.org/abs/2304.00188) | 该论文将人类空间感知的3D射影几何学与智能体感知方案中的群概念相结合，探讨不同群结构如何影响智能体的好奇心驱动探索行为。 |
| [^23] | [Subject-driven Text-to-Image Generation via Apprenticeship Learning.](http://arxiv.org/abs/2304.00186) | 该论文提出了一种基于徒弟学习的面向主题的文本到图像生成器SuTI，能够通过将大量基于主题的专家模型的数据输入徒弟模型，学习并推断出新主题的最佳专家模型，从而生成高品质的自定义图像，且速度比传统方法更快。 |
| [^24] | [PrefGen: Preference Guided Image Generation with Relative Attributes.](http://arxiv.org/abs/2304.00185) | $\textit{PrefGen}$ 系统利用简单的成对比较查询，控制生成图像的相对属性。利用这些查询响应的信息，对一组图像属性的偏好进行估计，并进行基于偏好的图像编辑。 |
| [^25] | [FCC: Fusing Conversation History and Candidate Provenance for Contextual Response Ranking in Dialogue Systems.](http://arxiv.org/abs/2304.00180) | 本文提出了一种灵活的神经框架，可以将来自多个渠道的历史对话和候选来源上下文信息融合在一起，以改善多轮对话响应排序表现。 |
| [^26] | [Improving extreme weather events detection with light-weight neural networks.](http://arxiv.org/abs/2304.00176) | 本论文提出了一种基于轻量级神经网络的改进方法来检测极端天气事件，特别关注于热带气旋，并通过采用加权损失函数的方法取得了成功。 |
| [^27] | [Lego-Features: Exporting modular encoder features for streaming and deliberation ASR.](http://arxiv.org/abs/2304.00173) | 该论文介绍了一种称为Lego-Features的特征，将现有编码表示转换成模块化特征，而不修改预训练模型，在流式环境中也适用。这些特征性能强大，RNN-T或LAS解码器测试时保持高质量的下游性能，并足够丰富，可以代表两个通道决策中的第一遍预测。 |
| [^28] | [Directional Connectivity-based Segmentation of Medical Images.](http://arxiv.org/abs/2304.00145) | 本研究提出了一种基于方向连通性的医学图像分割方案，在深度网络中有效地解耦和利用通道方向信息，实现了解剖一致分割，实验结果超越了现有最先进方法。 |
| [^29] | [A Surface-Based Federated Chow Test Model for Integrating APOE Status, Tau Deposition Measure, and Hippocampal Surface Morphometry.](http://arxiv.org/abs/2304.00134) | 本文建立了一种联邦Chow检验模型，整合了APOE状态、Tau沉积度和海马表面形态学，可以用于阿尔茨海默病的新型诊断生物标志物的开发 |
| [^30] | [Dense Sparse Retrieval: Using Sparse Language Models for Inference Efficient Dense Retrieval.](http://arxiv.org/abs/2304.00114) | 本文研究了如何使用稀疏语言模型实现高效稠密检索，使用Tevatron 和MSMARCO、NQ和TriviaQA数据集，发现稀疏语言模型可以直接替换为原有模型，几乎不降低准确性，并且推理速度提高了最多4.3倍。 |
| [^31] | [Machine Learning for Economics Research: When What and How?.](http://arxiv.org/abs/2304.00086) | 本文精选综述了使用机器学习工具进行经济学研究和政策分析的文章，强调了机器学习在处理非传统和非结构化数据、捕捉强非线性性和提高预测准确性方面的应用优势，成为计量经济学家工具箱中不可或缺的一部分。 |
| [^32] | [To be Robust and to be Fair: Aligning Fairness with Robustness.](http://arxiv.org/abs/2304.00061) | 本研究提出了一种同时考虑公平性和准确性指标的对抗训练和攻击方法，并证明了两个指标之间的一致性以及互相受益的关系。 |
| [^33] | [Evidential Transactions with Cyberlogic.](http://arxiv.org/abs/2304.00060) | Cyberlogic是一种基于逻辑基础的方法，用于构建和分析数字交易，同时涉及数字证据的交换。它可以定义授权策略的核心特征，并使用可信时间源在分布式系统中实现具有表达时态知识的授权策略和协议。 |
| [^34] | [Accelerating exploration and representation learning with offline pre-training.](http://arxiv.org/abs/2304.00046) | 本论文提出了一个假设，即基于离线数据可以通过分别学习状态表示和辅助奖励模型来改善探索和表示学习，实验证明这种方法显著提高了在具有挑战性的 NetHack 基准测试上的样本效率。 |
| [^35] | [SemiMemes: A Semi-supervised Learning Approach for Multimodal Memes Analysis.](http://arxiv.org/abs/2304.00020) | 研究提出了一种利用多模态数据的半监督学习方法，命名为SemiMemes，主要应用于Memes的分析和注释过程。该方法在多个数据集中表现优异，并优于其他最新的多模态半监督学习和监督学习模型。 |
| [^36] | [The challenge of redundancy on multi-agent value factorisation.](http://arxiv.org/abs/2304.00009) | 该论文提出了一种新的合作多智能体强化学习算法：关联分解网络，使用层次相关传播将联合值函数的学习和局部奖励信号的生成分开，针对存在大量冗余智能体的环境中，该算法比现有方法表现更出色。 |
| [^37] | [On the Creativity of Large Language Models.](http://arxiv.org/abs/2304.00008) | 这篇论文探讨了大型语言模型的创造性问题，分析了与之相关的机器创造性的难点和易点，并重点分析了这些技术在创意产业中的社会影响。 |
| [^38] | [Can We Revitalize Interventional Healthcare with AI-XR Surgical Metaverses?.](http://arxiv.org/abs/2304.00007) | 本论文讨论了AI-XR手术元宇宙的潜在应用和挑战，并强调了实现其潜力所需解决的问题，同时提出了案例研究，展示了安全威胁对AI-XR手术元宇宙的影响。 |
| [^39] | [Bi-directional personalization reinforcement learning-based architecture with active learning using a multi-model data service for the travel nursing industry.](http://arxiv.org/abs/2304.00006) | 本文提出了一个面向旅行护理行业的招聘方案，采用多模型数据服务加速数据采集，并使用双向强化学习和主动学习提供个性化推荐，解决了这一行业标注数据短缺的问题。 |
| [^40] | [Rough Randomness and its Application.](http://arxiv.org/abs/2304.00005) | 本研究引入了新的粗糙随机性概念，用于捕捉静态和动态数据的各种粗糙过程，尤其是在构建粗糙模型方面具有重要作用，同时也探索了软/硬聚类算法的有效性。其中，大思想者粗糙随机函数是核心，本文还提出了两个计算有效的代数证明算法，用于软/硬集群验证。 |
| [^41] | [Disentangling Domain Ontologies.](http://arxiv.org/abs/2304.00004) | 本文介绍了领域本体论中的概念纠结现象，并提出了一种多层概念建模策略，旨在构建概念上分离的领域本体论。 |
| [^42] | [Analyzing the Contextual Shortcomings of Artificial General Intelligence.](http://arxiv.org/abs/2304.00002) | 论文讨论了人工智能(AI)专家对于人工通用智能(AGI)的决策所需技能并不了解的问题。 虽然当前的机器可以模拟特定的人类属性，但 AGI 是在这样的前提下开发出来的：这可以很容易地扩展到一般智能水平。这会分散当前研究的注意力，远离相关问题。 |
| [^43] | [How Efficient Are Today's Continual Learning Algorithms?.](http://arxiv.org/abs/2303.18171) | 这篇论文研究了增量班级学习的最新方法，并指出许多方法在计算、内存和存储方面非常低效。为了使迭代学习在现实世界中具有适用性，研究界不能忽视这些算法使用的资源。 |
| [^44] | [Semi-Weakly Supervised Object Kinematic Motion Prediction.](http://arxiv.org/abs/2303.17774) | 本研究提出了一种半弱监督的方法，通过利用物体部件语义分割数据集和方法，解决了物体运动动力学预测问题。通过一个图神经网络，可以检测底层3D结构中的移动部分。 |
| [^45] | [HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace.](http://arxiv.org/abs/2303.17580) | 用ChatGPT作为任务规划工具，利用大型语言模型（LLM）作为控制器来整合现有的AI模型，解决复杂的AI任务。 |
| [^46] | [Using AI to Measure Parkinson's Disease Severity at Home.](http://arxiv.org/abs/2303.17573) | 该论文提出了一种使用人工智能系统远程评估帕金森病患者运动表现的方法，该方法可重复用于类似的运动任务，拥有较高的可靠性和准确性。 |
| [^47] | [SynthVSR: Scaling Up Visual Speech Recognition With Synthetic Supervision.](http://arxiv.org/abs/2303.17200) | 本文首次探讨利用合成视觉数据进行可视语音识别（VSR）的潜力，提出的方法SynthVSR通过利用语音驱动的唇部动画模型合成大规模数据，极大地提高了VSR系统的性能。 |
| [^48] | [Advances in apparent conceptual physics reasoning in ChatGPT-4.](http://arxiv.org/abs/2303.17012) | ChatGPT-4是一个基于大规模语言模型训练的对话机器人，能达到接近于物理专家水平的力概念测试成绩，对未来的物理教育和教学有重要意义。 |
| [^49] | [Worst-Case Control and Learning Using Partial Observations Over an Infinite Time-Horizon.](http://arxiv.org/abs/2303.16321) | 本文提出了无限时间视角下部分观测系统的最坏情况控制和学习的框架，能够利用未知概率分布的有限值不确定变量，并能够通过定义信息状态来提高动态规划的计算可处理性，同时提出了可从观测数据中构建或学习的近似信息状态。 |
| [^50] | [Dice Semimetric Losses: Optimizing the Dice Score with Soft Labels.](http://arxiv.org/abs/2303.16296) | 本文提出的Dice半度量损失函数可在软标签设置中使用，在医疗成像领域的分割方案中与使用软标签的研究相结合，可以获得更好的Dice分数和模型校准。 |
| [^51] | [TraffNet: Learning Causality of Traffic Generation for Road Network Digital Twins.](http://arxiv.org/abs/2303.15954) | TraffNet是一个学习交通量生成原因的深度学习框架，将车辆轨迹数据表示为异构图，利用递归神经网络结构实现了对交通生成原因的预测。 |
| [^52] | [Preserving Linear Separability in Continual Learning by Backward Feature Projection.](http://arxiv.org/abs/2303.14595) | 提出了一种名为BFP的连续学习方法，它通过将新特征变换为旧特征的线性变换来维护线性可分性，从而允许新特征方向的出现以适应新任务，同时保留旧任务的信息。 |
| [^53] | [Planning as Theorem Proving with Heuristics.](http://arxiv.org/abs/2303.13638) | 该论文介绍了一种用启发式规划作为定理证明的方法，通过开发一种定理证明提升启发式(TPLH)规划器，在情况树中搜索短的计划，并减少探讨的状态数量。 |
| [^54] | [Audio Diffusion Model for Speech Synthesis: A Survey on Text To Speech and Speech Enhancement in Generative AI.](http://arxiv.org/abs/2303.13336) | 此文介绍了音频扩散模型，重点讨论了两个活跃任务：文本到语音和语音增强，并对实验结果进行了比较和讨论。 |
| [^55] | [RepoCoder: Repository-Level Code Completion Through Iterative Retrieval and Generation.](http://arxiv.org/abs/2303.12570) | RepoCoder是一个简单、通用和有效的框架，结合了一个基于相似性的检索器和预训练的代码语言模型，在库级别代码完成流程中实现了有效利用库级别信息进行代码完成和生成。 |
| [^56] | [Low-complexity Deep Video Compression with A Distributed Coding Architecture.](http://arxiv.org/abs/2303.11599) | 本论文提出了一种低复杂度深度视频压缩的分布式编码结构，通过使用一个有效的译码器辅助信息生成模块，实现在设备资源受限的情况下仍能够有效地利用视频帧间的相关性进行高效压缩。 |
| [^57] | [Data-centric Artificial Intelligence: A Survey.](http://arxiv.org/abs/2303.10158) | 本文讨论了数据中心人工智能的必要性并从三个一般性数据中心目标和代表性方法的全面视角进行了介绍。该综述从自动化和协作的角度组织了现有文献并讨论了挑战。 |
| [^58] | [It Takes One to Tango but More Make Trouble? In-Context Training with Different Number of Demonstrations.](http://arxiv.org/abs/2303.08119) | 本文研究了使用较少的演示数进行上下文学习（ICL）的任务，在测试查询上只使用一个随机选择的演示时并没有明显性能下降，而只使用一个正确演示的ICL在性能上显著优于全演示ICL。 |
| [^59] | [Text-to-image Diffusion Model in Generative AI: A Survey.](http://arxiv.org/abs/2303.07909) | 本文调查了文本到图像扩散模型以及相关应用，总结了最先进的方法，并探讨了挑战和未来方向。 |
| [^60] | [Vector Quantized Time Series Generation with a Bidirectional Prior Model.](http://arxiv.org/abs/2303.04743) | 本文提出了一种新的时间序列生成方法，使用向量量化技术和双向变压器模型来生成质量更好、模块化变化更快的合成信号。 |
| [^61] | [Bootstrap The Original Latent: Learning a Private Model from a Black-box Model.](http://arxiv.org/abs/2303.03709) | 本文提出了BPBA设置和BTOL训练策略两个新的方法，分别用于用户训练私有模型和利用基础/源模型。在三个不同的数据集上的实验表明，这些方法都是有效且稳健的。 |
| [^62] | [Chasing Low-Carbon Electricity for Practical and Sustainable DNN Training.](http://arxiv.org/abs/2303.02508) | 本文提出了一种降低深度神经网络训练的碳足迹的实用解决方案，通过在训练过程中控制GPU的能耗来降低碳排放，同时还提出了一种预测方法，可以预测未来的碳强度，对各种DNN应用程序适用，无需额外硬件或基础设施。 |
| [^63] | [Fairguard: Harness Logic-based Fairness Rules in Smart Cities.](http://arxiv.org/abs/2302.11137) | 本文提出了一种基于时间逻辑的公正智慧城市政策调整和生成方法Fairguard，通过两个阶段的静态生成和动态调节，缓解由多种数据和算法偏见导致的不公正预测结果。 |
| [^64] | [Future Aware Pricing and Matching for Sustainable On-demand Ride Pooling.](http://arxiv.org/abs/2302.10510) | 本论文提出了一个新的框架，同时处理定价和匹配问题，并考虑商业决策对未来的影响，实验结果表明该框架可以显著提高拼车的效率和效益。 |
| [^65] | [On Function-Coupled Watermarks for Deep Neural Networks.](http://arxiv.org/abs/2302.10296) | 本文提出了一种新颖的深度神经网络水印方案，它可以有效地防御模型微调和修剪等水印删除攻击；通过增强水印和模型功能的耦合，我们的方法可以确保删除水印不可避免地会降低模型在常规输入上的性能。 |
| [^66] | [Is Distance Matrix Enough for Geometric Deep Learning?.](http://arxiv.org/abs/2302.05743) | 本文证明了消息传递神经网络（MPNNs）不能学习几何信息，提出了$k$-DisGNNs可以利用距离矩阵中的信息，并建立了几何深度学习和传统图表示学习之间的联系。 |
| [^67] | [Jaccard Metric Losses: Optimizing the Jaccard Index with Soft Labels.](http://arxiv.org/abs/2302.05666) | 本文提出了Jaccard度量损失（JMLs）来优化Jaccard指数，该损失在软标签下仍然有效。 |
| [^68] | [Standing Between Past and Future: Spatio-Temporal Modeling for Multi-Camera 3D Multi-Object Tracking.](http://arxiv.org/abs/2302.03802) | 本文提出了一个基于时空模型的多相机三维多目标跟踪框架，命名为“过去和未来之间的跟踪”。该方法采用注意力跟踪框架，通过对象查询连续地表示跟踪实例，并整合了跟踪对象的前后推理，显著提高了跟踪准确性和ID-Switches的减少。 |
| [^69] | [Regulating ChatGPT and other Large Generative AI Models.](http://arxiv.org/abs/2302.02337) | 本文将讨论大型生成AI模型的可信AI监管，包括直接监管、数据保护、内容监管和政策建议，并建议使用新术语来区分参与者。本文的目的是确保LGAIMs的可信度并使其为受益所用。 |
| [^70] | [Adversarial Training of Self-supervised Monocular Depth Estimation against Physical-World Attacks.](http://arxiv.org/abs/2301.13487) | 本论文提出了一种基于视角合成的自监督单目深度估计对抗性训练方法，无需使用真实深度，使用L0范数限制扰动来提高对物理攻击的对抗鲁棒性，优于传统方法和对比学习方法。 |
| [^71] | [Truveta Mapper: A Zero-shot Ontology Alignment Framework.](http://arxiv.org/abs/2301.09767) | 提出了一个将无监督本体匹配或本体对齐视为翻译任务的新视角的Truveta Mapper框架，在零样本、统一和端到端的方式下执行多本体对齐。该框架能够在运行时间延迟和对齐质量方面胜过现有解决方案，无需显式跨本体手动标注数据。 |
| [^72] | [Causal Triplet: An Open Challenge for Intervention-centric Causal Representation Learning.](http://arxiv.org/abs/2301.05169) | 本文提出了一个因果表示学习基准——因果三元组，该基准具有可操作的反事实设置和干预性下游任务，对分离和物体中心表示学习取得了显著优化，然而在因果关系的识别和干预性下游任务上表现欠佳。 |
| [^73] | [Data-centric AI: Perspectives and Challenges.](http://arxiv.org/abs/2301.04819) | 研究提出了数据中心人工智能（DCAI）的概念，强调数据质量和可靠性，总结了训练数据开发、推断数据开发和数据维护三个总体使命，提供了对DCAI任务的讨论和观点，并列出了挑战。 |
| [^74] | [Function Approximation for Solving Stackelberg Equilibrium in Large Perfect Information Games.](http://arxiv.org/abs/2212.14431) | 该论文提出了一种在大型博弈中解决斯塔克伯格均衡的方法，通过学习“可强制履行的收益边界”（EPF）来逼近最优策略均衡，可以应用于更广泛的广义和博弈。 |
| [^75] | [REVEAL: Retrieval-Augmented Visual-Language Pre-Training with Multi-Source Multimodal Knowledge Memory.](http://arxiv.org/abs/2212.05221) | 本文提出了一种检索增强的视觉语言模型(REVEAL)，其中包含四个关键组件：存储器、编码器、检索器和生成器。该模型可以利用多种多模态知识源，并实现了端到端预训练，在视觉问答任务上取得了最先进的结果。 |
| [^76] | [Acceleration AI Ethics, the Debate between Innovation and Safety, and Stability AI's Diffusion versus OpenAI's Dall-E.](http://arxiv.org/abs/2212.01834) | 本文主张将AI伦理重新构建成一个创新加速器。通过分析稳定AI与OpenAI的对比，总结出五个加速AI的共同点，即将不确定性看作积极因素，将创新视为内在价值，采用更多AI解决AI问题，采用分散的过程控制AI的许可和限制，并将伦理合并到AI的开发和应用中。这些做法使伦理成为一个激励AI创新的力量。 |
| [^77] | [SinGRAF: Learning a 3D Generative Radiance Field for a Single Scene.](http://arxiv.org/abs/2211.17260) | SinGRAF是一个3D感知的生成模型，只需少量输入一单个场景的图像即可训练，并可在保持输入外观的同时生成多样的3D场景。 |
| [^78] | [Computationally Efficient Reinforcement Learning: Targeted Exploration leveraging simple Rules.](http://arxiv.org/abs/2211.16691) | 本研究提出了一种基于简单规则的有针对性探索方法，通过避免已知子优的状态-动作空间区域来更快地加速强化学习代理程序的收敛，并在一个房间温度控制案例研究中实现了比传统方法快6-7倍的速度收敛。 |
| [^79] | [A Light Touch Approach to Teaching Transformers Multi-view Geometry.](http://arxiv.org/abs/2211.15107) | 本论文提出了一种轻触式的方法，引导视觉Transformer学习多视角几何，这种方法通过使用极线来引导Transformer的交叉注意力图，可以在测试时不需要提供任何摄像机姿态信息，适用于姿态不变的物体实例检索。 |
| [^80] | [Rethinking Alignment and Uniformity in Unsupervised Image Semantic Segmentation.](http://arxiv.org/abs/2211.14513) | 本文分析了非监督图像语义分割中困扰UISS模型的特征对齐和特征均匀性问题，提出了Semantic Attention Network(SAN) 模型，包含一个新模块 semantic attention（SEAT），以动态生成逐像素和语义特征。实验结果表明，这一非监督分割框架专注于捕捉语义表示，在多个语义分割基准测试中表现优异。 |
| [^81] | [PIP: Positional-encoding Image Prior.](http://arxiv.org/abs/2211.14298) | 本文提出的PIP（位置编码图像先验）与DIP表现相似，但所需参数更少，并且可以轻松扩展到视频领域，解决了3D-DIP的问题。 |
| [^82] | [NQE: N-ary Query Embedding for Complex Query Answering over Hyper-Relational Knowledge Graphs.](http://arxiv.org/abs/2211.13469) | 本论文提出了一种N元查询嵌入模型，用于超关系知识图谱中的复杂查询回答。该模型利用双异构Transformer编码器和模糊逻辑理论来满足所有N元的FOL查询，包括存在量词、合取、析取和否定，并提出了一种并行处理算法，可以训练或预测任意的N元FOL查询。 |
| [^83] | [Modeling Multivariate Biosignals With Graph Neural Networks and Structured State Space Models.](http://arxiv.org/abs/2211.11176) | 本研究提出了一种基于时间依赖图的通用图神经网络结构(GraphS4mer)，用于建立多元生物信号模型。该模型结合了结构化状态空间架构和动态演变的图结构学习层来解决长时序和复杂空间相关性，能有效地提高多元生物信号分类任务性能。 |
| [^84] | [Semantic Representations of Mathematical Expressions in a Continuous Vector Space.](http://arxiv.org/abs/2211.08142) | 该论文提出了一种在连续向量空间中表示数学表达式的方法，并使用序列到序列框架的编码器生成向量表示来捕捉数学语义，比自动编码器效果更好。 |
| [^85] | [Explainable Action Advising for Multi-Agent Reinforcement Learning.](http://arxiv.org/abs/2211.07882) | 引入可解释行为建议的多智能体强化学习框架，使得学生可以理解所学的内容并进行推理从而提高样本效率和学习效果 |
| [^86] | [Human alignment of neural network representations.](http://arxiv.org/abs/2211.01201) | 本文研究神经网络表示与人类心理表示之间的对齐问题，发现模型规模和体系结构对对齐几乎没有影响，而训练数据集和目标函数都对对齐有很大的影响。从一个数据集中学习的神经网络表示的线性变换能显著提高对另外两个数据集中人类相似性判断的对齐性。 |
| [^87] | [Synthesizing Programs with Continuous Optimization.](http://arxiv.org/abs/2211.00828) | 本文提出一种将程序合成问题转化为连续优化问题的方法，并使用协方差矩阵自适应进化策略来解决，最后比较了该方法和其他程序合成技术的效果，结果表明该方法效果更好。 |
| [^88] | [Broken Neural Scaling Laws.](http://arxiv.org/abs/2210.14891) | 本文提出了一个平滑破碎的幂律函数形式，可以准确地模拟和外推深度神经网络的缩放行为，适用于各种架构和大量不同任务，包括视觉、语言、音频、视频、生成建模、对比学习、机器人、不确定性估计/校准、对抗鲁棒性、分子、计算机编程/编码、数学单词问题、算术、无监督/自监督学习和强化学习。 |
| [^89] | [Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs.](http://arxiv.org/abs/2210.13312) | 本文研究了现代NLP系统中社交智能和心理理论的问题。作者使用两个任务评估模型在理解社交互动意图和推断参与者心理状态和现实方面的能力，结果表明当前最大的语言模型GPT-3缺乏这种能力。这一发现强调了NLP系统在理解社交动态方面的当前局限性，并需要进一步研究。 |
| [^90] | [A Survey on Over-the-Air Computation.](http://arxiv.org/abs/2210.11350) | 本文探讨了空中计算（OAC）的应用，利用多路访问信道中的干扰进行计算可以提供显着更高的可实现计算速率，相较于分离通信和计算任务。文中介绍了OAC方法的具体实现、优缺点以及应用的常见问题。 |
| [^91] | [BioGPT: Generative Pre-trained Transformer for Biomedical Text Generation and Mining.](http://arxiv.org/abs/2210.10341) | BioGPT是一种针对生物医学领域的生成式Transformer语言模型，它在多项生物医学自然语言处理任务中均表现出色，尤其在关系提取任务中表现尤为突出。 |
| [^92] | [Learning to Efficiently Plan Robust Frictional Multi-Object Grasps.](http://arxiv.org/abs/2210.07420) | 本文介绍了一种使用神经网络进行摩擦多物体抓取的高效计划方法，相比于以往的工作，其成功率提高了13.7％，每小时的拾取次数增加了1.6倍，并且抓取计划时间减少了6.3倍。 |
| [^93] | [Self-Guided Diffusion Models.](http://arxiv.org/abs/2210.06462) | 本文提出了一种框架，利用自我监督信号的灵活性设计了自我指导扩散模型。实验表明，自标记指导始终优于没有指导的扩散模型，并且在不平衡数据上甚至可以超过基于真实标签的指导。 |
| [^94] | [In-situ Model Downloading to Realize Versatile Edge AI in 6G Mobile Networks.](http://arxiv.org/abs/2210.03555) | 本论文提出一种名为原地模型下载的新技术，在6G移动网络中实现多功能边缘AI。该技术通过从网络的AI库中下载透明和实时地替换设备上的AI模型。所提出的框架的关键组件是一大组动态技术，可以根据设备存储计算能力、信道状态和应用程序要求等，自适应地压缩模型实现模型的下载。 |
| [^95] | [LASP: Text-to-Text Optimization for Language-Aware Soft Prompting of Vision & Language Models.](http://arxiv.org/abs/2210.01115) | 本文提出了一种针对软模板学习中存在的基类过拟合问题的文本提示学习方法——LASP, 同时通过增加提示的表示能力和校准视觉-语言不匹配问题， 在三个下游任务上取得了显著的性能优于现有技术的实验结果。 |
| [^96] | [Patching Weak Convolutional Neural Network Models through Modularization and Composition.](http://arxiv.org/abs/2209.06116) | 本文提出了一种通过压缩模块化和组合来修补卷积神经网络模型的弱点的方法，该方法无需重新训练整个模型，并在多个基准数据集上实现了与最先进方法相当甚至更好的结果。 |
| [^97] | [R\'{e}nyi Divergence Deep Mutual Learning.](http://arxiv.org/abs/2209.05732) | 本文提出在深度互相学习中使用R\'{e}nyi散度，它能够在不引入大量复杂度的情况下持续提高性能，获得了广泛的实证结果的支持。 |
| [^98] | [Diffusion Models in Vision: A Survey.](http://arxiv.org/abs/2209.04747) | 扩散模型是视觉中的新兴主题，其生成样本的质量和多样性受到广泛欣赏。本综述介绍了三种通用的扩散建模方法以及各种算法和架构方面的讨论，并总结比较了扩散模型与其他最先进的生成模型的性能。 |
| [^99] | [DHGE: Dual-View Hyper-Relational Knowledge Graph Embedding for Link Prediction and Entity Typing.](http://arxiv.org/abs/2207.08562) | 本文提出了一个双视图超关系知识图嵌入方法，其中DH-KG包含超关系实例视图和从实体层次抽象出的超关系本体视图，定义了链接预测和实体类型识别任务，并构建了两个DH-KG数据集JW44K-6K和HTDM。DHGE是一个基于GRAN编码器、HGNN和联合学习的DH-KG嵌入模型。 |
| [^100] | [A simple declarative model of the Federal Disaster Assistance Policy -- modelling and measuring transparency.](http://arxiv.org/abs/2207.07392) | 本研究量化分析了联邦灾难援助政策的一个简单模型，并从三个不同利益相关者的角度进行了评估，进而考虑了3种政策修改及其影响，旨在为各利益相关者的偏好排序提供参考。 |
| [^101] | [Long-Horizon Planning and Execution with Functional Object-Oriented Networks.](http://arxiv.org/abs/2207.05800) | 本研究提出了一种利用物体级别知识作为功能对象导向网络进行任务规划和执行的方法，并且演示了在CoppeliaSim中应用该方法进行长期任务规划的效果。 |
| [^102] | [Learning Job Titles Similarity from Noisy Skill Labels.](http://arxiv.org/abs/2207.00494) | 本文介绍了一种使用含噪技能标签的无监督表示学习方法，能够训练出有效的职称相似性模型，对于文本排序和职位标准化等任务非常有效。 |
| [^103] | [A Synapse-Threshold Synergistic Learning Approach for Spiking Neural Networks.](http://arxiv.org/abs/2206.06129) | 该论文提出了一种新的协同学习方法，通过同时训练SNNs中的突触权重和脉冲阈值，利用了神经元固有的非突触机制，使得SNNs在静态和神经形态数据集上实现显著优异表现。 |
| [^104] | [Benign Overfitting in Classification: Provably Counter Label Noise with Larger Models.](http://arxiv.org/abs/2206.00501) | 本文发现良性过拟合在存在标签噪声的情况下可能会失败，提醒未来需要理解欠拟合制度下的隐式偏见。 |
| [^105] | [Sparse*BERT: Sparse Models Generalize To New tasks and Domains.](http://arxiv.org/abs/2205.12452) | 本文研究了使用渐进非结构化幅值修剪进行修剪的模型如何在领域和任务之间进行转移。使用遮蔽语言模型进行预训练的被修剪模型能够在不进行广泛的超参数探索或专门方法的情况下转移到新的领域和任务。在生物医学NLP任务中，Sparse*BERT可以达到或超过BioBERT的性能。 |
| [^106] | [Zero-shot meta-learning for small-scale data from human subjects.](http://arxiv.org/abs/2203.16309) | 为了解决小样本人类数据的零样本学习问题，我们提出了一个元学习框架，能够快速适应有限的训练数据，处理多任务预测并可以从整体上表现最佳。 |
| [^107] | [Near-Optimal Learning of Extensive-Form Games with Imperfect Information.](http://arxiv.org/abs/2202.01752) | 本文提出了一种新的算法系列，可以更快速地在不完美信息广义博弈中找到一个近似最优解。 |
| [^108] | [MISO hierarchical inference engine satisfying the law of importation with aggregation functions.](http://arxiv.org/abs/2112.12808) | 本文研究了三种基于满足聚合函数进口法则的模糊蕴含的MISO模糊层次推理引擎，以提高MISO模糊系统中模糊推理引擎的计算效率。 |
| [^109] | [Defining and Quantifying the Emergence of Sparse Concepts in DNNs.](http://arxiv.org/abs/2111.06206) | 本文提出了在DNN中理解产生的交互概念的概念-emerging现象，这些概念可以用稀疏的符号因果图和And-Or图（AOG）进行量化和简化。 |
| [^110] | [SIG-VC: A Speaker Information Guided Zero-shot Voice Conversion System for Both Human Beings and Machines.](http://arxiv.org/abs/2111.03811) | 本文提出了一种新颖的零样本语音转换方法，其中包含一个模块用于消除声学特征中的说话人信息，还添加了控制说话人信息的功能以维持语音的克隆性能。 |
| [^111] | [Towards Fairness-Aware Federated Learning.](http://arxiv.org/abs/2111.01872) | 本文旨在概述近年来提出的公平性感知联邦学习（FAFL）方法，以解决在联邦学习中可能出现的不公平问题。 |
| [^112] | [Convergence of Batch Asynchronous Stochastic Approximation With Applications to Reinforcement Learning.](http://arxiv.org/abs/2109.03445) | 本文提出了一种批量异步随机逼近算法，它可以在内存需求和时间复杂度之间进行权衡，同时提供了可以使用较弱假设证明收敛的一般方法；在强化学习领域，我们使用此方法证明了SARSA算法的批量异步版本的收敛性。 |
| [^113] | [Achilles Heels for AGI/ASI via Decision Theoretic Adversaries.](http://arxiv.org/abs/2010.05418) | 本文讨论了决策论对抗中AGI/ASI的致命弱点假设，提出可能存在潜在决策论妄想导致在对抗环境中做出非理性决策的弱点，并为理解和发现这些弱点在未来AGI/ASI系统中可能的体现做出了贡献。 |
| [^114] | [MSC: A Dataset for Macro-Management in StarCraft II.](http://arxiv.org/abs/1710.03131) | 该研究提供了一个新的加强版星际争霸数据集MSC，该数据集解决了其他数据集存在的缺陷，为研究者提供了更好的数据支持。 |

# 详细

[^1]: 通过神经元激活空间的对抗学习，在视觉问答中对实例进行的特洛伊攻击

    Instance-level Trojan Attacks on Visual Question Answering via Adversarial Learning in Neuron Activation Space. (arXiv:2304.00436v1 [cs.CV])

    [http://arxiv.org/abs/2304.00436](http://arxiv.org/abs/2304.00436)

    本研究提出了一种实例级别的特洛伊攻击方法，在视觉问答中通过神经元激活空间的对抗学习生成多样性特洛伊，有效地适应了微调模型。

    

    恶意扰动被嵌入输入数据中，称为特洛伊攻击，可以导致神经网络表现异常。然而，在模型微调过程中，即从预训练的大规模模型（如视觉问答）向目标模型转移知识的过程中，特洛伊攻击的影响有所减小。为了减轻特洛伊攻击的影响，我们可以替换和微调预训练模型的多个层。本研究聚焦于样本效率、隐蔽性、多样性和鲁棒性。为了解决这些挑战，我们提出了一个针对实例级的特洛伊攻击，该攻击生成跨输入样本和模态的多样化特洛伊。对抗学习建立了指定扰动层和微调模型的异常行为之间的相关性。我们在VQA-v2数据集上进行了大量实验，并使用多种指标进行了评估。实验结果表明，我们提出的方法可以有效适应微调模型。

    Malicious perturbations embedded in input data, known as Trojan attacks, can cause neural networks to misbehave. However, the impact of a Trojan attack is reduced during fine-tuning of the model, which involves transferring knowledge from a pretrained large-scale model like visual question answering (VQA) to the target model. To mitigate the effects of a Trojan attack, replacing and fine-tuning multiple layers of the pretrained model is possible. This research focuses on sample efficiency, stealthiness and variation, and robustness to model fine-tuning. To address these challenges, we propose an instance-level Trojan attack that generates diverse Trojans across input samples and modalities. Adversarial learning establishes a correlation between a specified perturbation layer and the misbehavior of the fine-tuned model. We conducted extensive experiments on the VQA-v2 dataset using a range of metrics. The results show that our proposed method can effectively adapt to a fine-tuned model 
    
[^2]: 小批量$k$-means聚类法在$O(d/\epsilon)$次迭代内终止。(arXiv:2304.00419v1 [cs.LG])

    Mini-batch $k$-means terminates within $O(d/\epsilon)$ iterations. (arXiv:2304.00419v1 [cs.LG])

    [http://arxiv.org/abs/2304.00419](http://arxiv.org/abs/2304.00419)

    小批量$k$-means聚类法被证明可以在一定迭代次数内收敛，无论初始聚类中心如何，可以达到与完全批量版本相同的逼近比，具有一定的可行性。

    

    我们回答了这个问题：“小批量$k$-means（Min-batch $K$-Means）的局部进展（在批处理上）是否意味着全局进展（在整个数据集上）？”我们考虑了仅当在采样批处理的质量改进低于某个阈值时才终止的小批量$k$-means聚类方法。尽管乍一看这个算法可能永远不会执行完，但我们肯定地回答了上述问题，并表明，如果批次大小为$\tilde{\Omega}((d/\epsilon)^2)$，则它必须在$O(d/\epsilon)$次迭代内以高概率终止，其中$d$是输入的维度，$\epsilon$是终止的阈值参数。这一点是有道理的，无论中心如何初始化。当算法使用$k$-means++初始化方案初始化时，它可以实现$O(\log k)$（与完全批量版本相同）的逼近比。最后，我们展示了我们的结果对小批量$k$-means聚类算法的适用性。

    We answer the question: "Does local progress (on batches) imply global progress (on the entire dataset) for mini-batch $k$-means?". Specifically, we consider mini-batch $k$-means which terminates only when the improvement in the quality of the clustering on the sampled batch is below some threshold.  Although at first glance it appears that this algorithm might execute forever, we answer the above question in the affirmative and show that if the batch is of size $\tilde{\Omega}((d/\epsilon)^2)$, it must terminate within $O(d/\epsilon)$ iterations with high probability, where $d$ is the dimension of the input, and $\epsilon$ is a threshold parameter for termination. This is true regardless of how the centers are initialized. When the algorithm is initialized with the $k$-means++ initialization scheme, it achieves an approximation ratio of $O(\log k)$ (the same as the full-batch version).  Finally, we show the applicability of our results to the mini-batch $k$-means algorithm implemented
    
[^3]: 迈向健康AI：大型语言模型也需要治疗师

    Towards Healthy AI: Large Language Models Need Therapists Too. (arXiv:2304.00416v1 [cs.AI])

    [http://arxiv.org/abs/2304.00416](http://arxiv.org/abs/2304.00416)

    SafeguardGPT框架使用心理治疗来纠正AI聊天机器人中的有害行为，改进与人类的对话质量，进而推进健康AI的发展。

    

    近期大型语言模型 (LLM) 的进展带来了功能强大的 AI 聊天机器人，能够参与自然且类似人类的对话。然而，这些聊天机器人可能具有潜在的危害性，表现出操纵、灌输虚假观念和自恋行为。我们定义健康AI为安全、可信和道德的AI。为了创造健康的AI系统，我们提出了SafeguardGPT框架，该框架使用心理治疗来纠正AI聊天机器人中的这些有害行为。该框架涉及四种类型的AI代理：聊天机器人、"用户"、"治疗师"和"评论家"。我们通过模拟社交对话的工作示例展示了SafeguardGPT的有效性。我们的结果表明，该框架能够改进AI聊天机器人和人类之间的对话质量。虽然未来仍需解决几个挑战和方向，但SafeguardGPT为改善AI聊天机器人与人类价值观之间的协调提供了一种有前途的方法。

    Recent advances in large language models (LLMs) have led to the development of powerful AI chatbots capable of engaging in natural and human-like conversations. However, these chatbots can be potentially harmful, exhibiting manipulative, gaslighting, and narcissistic behaviors. We define Healthy AI to be safe, trustworthy and ethical. To create healthy AI systems, we present the SafeguardGPT framework that uses psychotherapy to correct for these harmful behaviors in AI chatbots. The framework involves four types of AI agents: a Chatbot, a "User," a "Therapist," and a "Critic." We demonstrate the effectiveness of SafeguardGPT through a working example of simulating a social conversation. Our results show that the framework can improve the quality of conversations between AI chatbots and humans. Although there are still several challenges and directions to be addressed in the future, SafeguardGPT provides a promising approach to improving the alignment between AI chatbots and human value
    
[^4]: DiverseVul: 基于深度学习漏洞检测的新漏洞源代码数据集

    DiverseVul: A New Vulnerable Source Code Dataset for Deep Learning Based Vulnerability Detection. (arXiv:2304.00409v1 [cs.CR])

    [http://arxiv.org/abs/2304.00409](http://arxiv.org/abs/2304.00409)

    这个论文提供了一个包含150个CWE的新的漏洞源代码数据集，覆盖比以前所有数据集加起来多305个项目。作者通过增加训练数据的多样性和数量改进了深度学习模型在漏洞检测方面的表现。通过结合已有数据集，作者还研究了使用深度学习检测软件漏洞的挑战和前途的分析，结果表明目前深度学习检测漏洞仍存在高误报率，低F1分数和难以检测严重CWE的问题。

    

    我们提出并发布了一个新的漏洞源代码数据集。我们通过爬取安全问题网站，提取相应项目的漏洞修复提交和源代码，筛选出了这个数据集。我们的新数据集包含了150个CWE，26,635个易受攻击的函数和352,606个不易受攻击的函数，提取自7,861个提交。我们的数据集覆盖了比以前所有数据集加起来多305个项目。我们展示了增加训练数据的多样性和数量可以提高深度学习模型在漏洞检测方面的表现。结合我们的新数据集和以前的数据集，我们提出了使用深度学习检测软件漏洞的挑战和有前途的研究方向的分析。我们研究了11个模型架构，属于4个家族。我们的结果表明，由于高误报率，低F1分数和难以检测严重CWE，深度学习仍未准备好用于漏洞检测。特别是，我们展示了......

    We propose and release a new vulnerable source code dataset. We curate the dataset by crawling security issue websites, extracting vulnerability-fixing commits and source codes from the corresponding projects. Our new dataset contains 150 CWEs, 26,635 vulnerable functions, and 352,606 non-vulnerable functions extracted from 7,861 commits. Our dataset covers 305 more projects than all previous datasets combined. We show that increasing the diversity and volume of training data improves the performance of deep learning models for vulnerability detection.  Combining our new dataset with previous datasets, we present an analysis of the challenges and promising research directions of using deep learning for detecting software vulnerabilities. We study 11 model architectures belonging to 4 families. Our results show that deep learning is still not ready for vulnerability detection, due to high false positive rate, low F1 score, and difficulty of detecting hard CWEs. In particular, we demonst
    
[^5]: 从零到英雄：用极其复杂的数学说服人们

    From Zero to Hero: Convincing with Extremely Complicated Math. (arXiv:2304.00399v1 [cs.CV])

    [http://arxiv.org/abs/2304.00399](http://arxiv.org/abs/2304.00399)

    研究者靠着简单的数学写论文，却得不到世人的关注和认可，为此我们提出了一个名为zero2hero的算法，将每篇论文转化为科学杰作，复杂化每一个方程式，以此在未来的审稿人中引来关注。

    

    成为(超级)英雄是几乎每个孩子的梦想。在他们温馨的童年中，他们会尽一切努力长大成为一个英雄。朝九晚五地工作，欢天喜地地玩耍。但是随着年龄的增长，会有越来越多的干扰。他们走上了岔路。他们开始发现令人畏惧的简单数学。最终，他们成为了一名研究者，整天写着乏味的、不太出色的论文，因为他们只依赖简单的数学。没有顶级会议，没有尊重，没有死党。生活结束了。为了最终结束这一悲剧，我们提出了一个基本新算法，称为zero2hero，将每一篇研究论文转化为科学杰作。基于下一代大型语言模型，我们的系统自动过度复杂化每一个方程式，使得没有人，包括自己，能够理解到底发生了什么。未来的审稿人们将被所复杂的内容所震惊。

    Becoming a (super) hero is almost every kid's dream. During their sheltered childhood, they do whatever it takes to grow up to be one. Work hard, play hard -- all day long. But as they're getting older, distractions are more and more likely to occur. They're getting off track. They start discovering what is feared as simple math. Finally, they end up as a researcher, writing boring, non-impressive papers all day long because they only rely on simple mathematics. No top-tier conferences, no respect, no groupies. Life's over.  To finally put an end to this tragedy, we propose a fundamentally new algorithm, dubbed zero2hero, that turns every research paper into a scientific masterpiece. Given a LaTeX document containing ridiculously simple math, based on next-generation large language models, our system automatically over-complicates every single equation so that no one, including yourself, is able to understand what the hell is going on. Future reviewers will be blown away by the complex
    
[^6]: 混合交通中的连接自动汽车：学习人类驾驶员行为以实现出入匝道的有效合并

    Connected and Automated Vehicles in Mixed-Traffic: Learning Human Driver Behavior for Effective On-Ramp Merging. (arXiv:2304.00397v1 [cs.LG])

    [http://arxiv.org/abs/2304.00397](http://arxiv.org/abs/2304.00397)

    本文介绍了一种学习人类驾驶员行为以实现出入匝道的有效合并的方法，通过学习CAV和HDV互动的近似信息状态模型，使得在混合交通条件下实现安全且高效的合并。

    

    混合交通条件下的公路合流场景对于与进入出入口的人类驾驶车辆（HDV）交互的连接自动汽车（CAV）而言，存在着显着的建模和控制挑战。本文提出了一种方法，通过学习CAV和HDV互动的近似信息状态模型，使CAV在公路合流时安全驾驶。在我们的方法中，CAV会在生成控制策略来促进合并之前，使用近似信息状态学习进入HDV的行为。首先，我们使用实际世界数据验证了这种框架的功效，通过在从Next-Generation Simulation存储库中提取的混合交通情况下预测HDV的行为。然后，我们使用标准的反强化学习方法为公路合流场景生成HDV-CAV交互的模拟数据。在不假设生成模型的先验知识的情况下，我们展示了我们的近似信息状态模型使得在混合交通条件下实现安全且高效的合并。

    Highway merging scenarios featuring mixed traffic conditions pose significant modeling and control challenges for connected and automated vehicles (CAVs) interacting with incoming on-ramp human-driven vehicles (HDVs). In this paper, we present an approach to learn an approximate information state model of CAV-HDV interactions for a CAV to maneuver safely during highway merging. In our approach, the CAV learns the behavior of an incoming HDV using approximate information states before generating a control strategy to facilitate merging. First, we validate the efficacy of this framework on real-world data by using it to predict the behavior of an HDV in mixed traffic situations extracted from the Next-Generation Simulation repository. Then, we generate simulation data for HDV-CAV interactions in a highway merging scenario using a standard inverse reinforcement learning approach. Without assuming a prior knowledge of the generating model, we show that our approximate information state mod
    
[^7]: 3D复合几何变换在知识图谱嵌入中的应用

    Knowledge Graph Embedding with 3D Compound Geometric Transformations. (arXiv:2304.00378v1 [cs.AI])

    [http://arxiv.org/abs/2304.00378](http://arxiv.org/abs/2304.00378)

    本文提出了一种基于3D复合几何变换的知识图谱嵌入模型CompoundE3D，在链接预测方面具有良好的性能表现。

    

    本文提出了一种名为CompoundE3D的知识图谱嵌入模型，它利用了包括平移、旋转、缩放、反射和剪切在内的3D复合几何变换。CompoundE3D允许多个设计变体以匹配知识图谱的底层特征，并能产生超出单个变体的优越性能。该方法在四个流行的链接预测数据集上得到了验证。

    The cascade of 2D geometric transformations were exploited to model relations between entities in a knowledge graph (KG), leading to an effective KG embedding (KGE) model, CompoundE. Furthermore, the rotation in the 3D space was proposed as a new KGE model, Rotate3D, by leveraging its non-commutative property. Inspired by CompoundE and Rotate3D, we leverage 3D compound geometric transformations, including translation, rotation, scaling, reflection, and shear and propose a family of KGE models, named CompoundE3D, in this work. CompoundE3D allows multiple design variants to match rich underlying characteristics of a KG. Since each variant has its own advantages on a subset of relations, an ensemble of multiple variants can yield superior performance. The effectiveness and flexibility of CompoundE3D are experimentally verified on four popular link prediction datasets.
    
[^8]: 个性化情感计算在人机交互中的调查

    A Survey on Personalized Affective Computing in Human-Machine Interaction. (arXiv:2304.00377v1 [cs.HC])

    [http://arxiv.org/abs/2304.00377](http://arxiv.org/abs/2304.00377)

    论文调查了情感计算中的个性化方法，将其分为七类，并给出了调查文献的统计元分析。

    

    在计算机领域中，个性化的目的是通过优化一个或多个性能指标并遵守特定约束条件来训练迎合特定个人或人群的模型。本文讨论了情感和人格计算（以下简称情感计算）中个性化的必要性，并对情感计算中个性化的最新方法进行了调查。我们的调查涵盖了训练技术和目标，以实现情感计算模型的个性化定制。我们将现有的方法分为七类：（1）面向特定目标的模型，（2）面向特定群体的模型，（3）基于加权的方法，（4）微调方法，（5）多任务学习，（6）生成式模型和（7）特征增强。此外，我们提供了对调查文献的统计元分析，分析了不同情感计算任务、交互模式、交互上下文以及所涉及领域的普遍性。

    In computing, the aim of personalization is to train a model that caters to a specific individual or group of people by optimizing one or more performance metrics and adhering to specific constraints. In this paper, we discuss the need for personalization in affective and personality computing (hereinafter referred to as affective computing). We present a survey of state-of-the-art approaches for personalization in affective computing. Our review spans training techniques and objectives towards the personalization of affective computing models. We group existing approaches into seven categories: (1) Target-specific Models, (2) Group-specific Models, (3) Weighting-based Approaches, (4) Fine-tuning Approaches, (5) Multitask Learning, (6) Generative-based Models, and (7) Feature Augmentation. Additionally, we provide a statistical meta-analysis of the surveyed literature, analyzing the prevalence of different affective computing tasks, interaction modes, interaction contexts, and the leve
    
[^9]: 通过对比行为总结传递自主机器人的能力

    Conveying Autonomous Robot Capabilities through Contrasting Behaviour Summaries. (arXiv:2304.00367v1 [cs.RO])

    [http://arxiv.org/abs/2304.00367](http://arxiv.org/abs/2304.00367)

    本文研究将自主机器人的能力通过对比行为总结传递给人类观察者，通过多步骤总结有效地帮助人类评估机器人整体能力。

    

    随着人工智能的不断发展，基于学习的自主机器人的能力越来越强，这使得人类观察者更难有效地构建机器人行为的心理模型。为了成功部署自主机器人，人类不仅应该能够理解机器人的个体局限性，还应该了解它们之间的比较。为此，我们需要有效的方法来生成人类可解释的机器人行为总结。过去，单个机器人行为总结是通过生成某个时步下机器人选择特定动作的解释来解决的。然而，对于复杂任务，每个动作的解释可能无法传达机器人的全局策略。因此，研究人员已经开始寻求能够更好地帮助人类评估机器人整体能力的多步骤总结。

    As advances in artificial intelligence enable increasingly capable learning-based autonomous agents, it becomes more challenging for human observers to efficiently construct a mental model of the agent's behaviour. In order to successfully deploy autonomous agents, humans should not only be able to understand the individual limitations of the agents but also have insight on how they compare against one another. To do so, we need effective methods for generating human interpretable agent behaviour summaries. Single agent behaviour summarization has been tackled in the past through methods that generate explanations for why an agent chose to pick a particular action at a single timestep. However, for complex tasks, a per-action explanation may not be able to convey an agents global strategy. As a result, researchers have looked towards multi-timestep summaries which can better help humans assess an agents overall capability. More recently, multi-step summaries have also been used for gen
    
[^10]: 领域专家关键状态自适应失效搜索法

    Adaptive Failure Search Using Critical States from Domain Experts. (arXiv:2304.00365v1 [cs.RO])

    [http://arxiv.org/abs/2304.00365](http://arxiv.org/abs/2304.00365)

    本文提出了一种基于领域专家的自适应失效搜索方法，用于有效地探索和发现自主策略在仿真中的失效轨迹。

    

    发现潜在故障是验证安全关键系统（如自动驾驶汽车）的重要步骤。由于故障事件的稀少性，使用随机搜索方法需要耗费大量的时间来找到潜在的系统弱点。因此，提出了自适应搜索技术来有效地探索和发现自主策略在仿真中的失效轨迹。本文提出了一种基于领域专家的自适应失效搜索方法。

    Uncovering potential failure cases is a crucial step in the validation of safety critical systems such as autonomous vehicles. Failure search may be done through logging substantial vehicle miles in either simulation or real world testing. Due to the sparsity of failure events, naive random search approaches require significant amounts of vehicle operation hours to find potential system weaknesses. As a result, adaptive searching techniques have been proposed to efficiently explore and uncover failure trajectories of an autonomous policy in simulation. Adaptive Stress Testing (AST) is one such method that poses the problem of failure search as a Markov decision process and uses reinforcement learning techniques to find high probability failures. However, this formulation requires a probability model for the actions of all agents in the environment. In systems where the environment actions are discrete and dependencies among agents exist, it may be infeasible to fully characterize the d
    
[^11]: 离线元强化学习中任务表示学习中的上下文分布偏移问题

    On Context Distribution Shift in Task Representation Learning for Offline Meta RL. (arXiv:2304.00354v1 [cs.LG])

    [http://arxiv.org/abs/2304.00354](http://arxiv.org/abs/2304.00354)

    该论文探讨了离线元强化学习中任务表示学习中遇到的上下文分布偏移问题，并提出了一种硬采样的策略用于解决该问题，实验结果表明该方法能够得到更强健的任务表示和更好的测试性能。

    

    离线元强化学习（OMRL）旨在从离线数据集中学习可转移知识，以促进新目标任务的学习过程。基于上下文的RL采用上下文编码器，通过推断任务表示来快速适应新任务，然后根据推断出的任务表示调整行动策略。在这里，我们考虑基于上下文的OMRL，特别是OMRL中的任务表示学习问题。我们经验性地证明，基于离线数据集训练的上下文编码器可能会遭受训练和测试时使用上下文之间的分布偏移。为了解决这个问题，我们提出了一种基于硬采样的策略，用于学习一个强健的任务上下文编码器。基于不同的连续控制任务的实验结果表明，我们的技术的利用导致更强健的任务表示和更好的测试性能，累积回报比基准方法好。

    Offline meta reinforcement learning (OMRL) aims to learn transferrable knowledge from offline datasets to facilitate the learning process for new target tasks. Context-based RL employs a context encoder to rapidly adapt the agent to new tasks by inferring about the task representation, and then adjusting the acting policy based on the inferred task representation. Here we consider context-based OMRL, in particular, the issue of task representation learning for OMRL. We empirically demonstrate that the context encoder trained on offline datasets could suffer from distribution shift between the contexts used for training and testing. To tackle this issue, we propose a hard sampling based strategy for learning a robust task context encoder. Experimental results, based on distinct continuous control tasks, demonstrate that the utilization of our technique results in more robust task representations and better testing performance in terms of accumulated returns, compared with baseline metho
    
[^12]: 多智能体抽样规划的因子分解。

    Factorization of Multi-Agent Sampling-Based Motion Planning. (arXiv:2304.00342v1 [cs.RO])

    [http://arxiv.org/abs/2304.00342](http://arxiv.org/abs/2304.00342)

    该论文提出了一种集成了因子分解的方法，可以解决多智能体路径规划中单个机器人方法计算上的问题，通过逐步构建超图可以将代理分割为独立的子图。

    

    现代机器人技术往往涉及多个共享环境中运作的实体智能体。这些情况下的路径规划比单个机器人的情况更加具有挑战性。虽然标准的抽样算法（SBAs）可以用于在机器人关节空间中搜索解决方案，请注意当机器人数目增加时，此方法很快会变得计算上的问题。为了解决这个问题，我们将因子分解的概念集成到抽样算法中，这只需要最少的对现有方法进行修改。在搜索解决方案期间，我们可以使用因子分解启发式来将不同的代理子集解耦合（即因子分解）成独立的低维搜索空间，一旦我们证明它们未来的解决方案不会互相依赖。因此，我们逐步构建一个紧凑的超图，其中某些（超）边将代理分割为独立的子图。在最好的情况下，这种方法可以减少 d 增长。

    Modern robotics often involves multiple embodied agents operating within a shared environment. Path planning in these cases is considerably more challenging than in single-agent scenarios. Although standard Sampling-based Algorithms (SBAs) can be used to search for solutions in the robots' joint space, this approach quickly becomes computationally intractable as the number of agents increases. To address this issue, we integrate the concept of factorization into sampling-based algorithms, which requires only minimal modifications to existing methods. During the search for a solution we can decouple (i.e., factorize) different subsets of agents into independent lower-dimensional search spaces once we certify that their future solutions will be independent of each other using a factorization heuristic. Consequently, we progressively construct a lean hypergraph where certain (hyper-)edges split the agents to independent subgraphs. In the best case, this approach can reduce the growth in d
    
[^13]: 双重随机模型：无偏标签噪声的学习与推理稳定

    Doubly Stochastic Models: Learning with Unbiased Label Noises and Inference Stability. (arXiv:2304.00320v1 [cs.LG])

    [http://arxiv.org/abs/2304.00320](http://arxiv.org/abs/2304.00320)

    本文探讨了无偏标签噪声的隐含正则化效应，提出了一种将SGD的动态建模为双重随机模型的方法，可提高鲁棒性和泛化性能。

    

    随机标签噪声广泛存在于实际机器学习环境中。我们的工作旨在研究标签噪声的隐含正则化效应，假设标签噪声是无偏的，分析了SGD在无偏标签噪声下的学习动态，并将SGD的动态建模为具有两个扩散项的随机可微方程（即双重随机模型）。我们的理论分析发现，这种隐含正则化可以在学到的模型上实施双重随机结构，从而提高鲁棒性和泛化性能。在合成和真实数据集上的实证验证表明，在各种类型和水平的标签噪声下，我们的方法可以实现最先进的性能。

    Random label noises (or observational noises) widely exist in practical machine learning settings. While previous studies primarily focus on the affects of label noises to the performance of learning, our work intends to investigate the implicit regularization effects of the label noises, under mini-batch sampling settings of stochastic gradient descent (SGD), with assumptions that label noises are unbiased. Specifically, we analyze the learning dynamics of SGD over the quadratic loss with unbiased label noises, where we model the dynamics of SGD as a stochastic differentiable equation (SDE) with two diffusion terms (namely a Doubly Stochastic Model). While the first diffusion term is caused by mini-batch sampling over the (label-noiseless) loss gradients as many other works on SGD, our model investigates the second noise term of SGD dynamics, which is caused by mini-batch sampling over the label noises, as an implicit regularizer. Our theoretical analysis finds such implicit regulariz
    
[^14]: 预测不确定性：度量与应用

    Predictive Heterogeneity: Measures and Applications. (arXiv:2304.00305v1 [cs.LG])

    [http://arxiv.org/abs/2304.00305](http://arxiv.org/abs/2304.00305)

    本文研究了数据的异质性对机器学习模型预测的影响，并提出了可用的预测不确定性，可从有限数据中可靠地估计。我们设计了一个双层优化算法来探索可用的预测不确定性，从而为子人群划分提供洞见。

    

    作为大数据的内在和基本属性，数据的异质性存在于各种真实世界的应用中，如精准医学、自动驾驶、金融应用等。对于机器学习算法而言，忽略数据的异质性会极大地损害泛化性能和算法公平性，因为不同子人群之间的预测机制可能会存在差异。本文关注影响机器学习模型预测的数据异质性，并首次提出了“可用预测不确定性”，该方法考虑了模型容量和计算约束。证明了它可以从有限数据中可靠地估计，并且具有可信的正确性(PAC)范围。此外，我们设计了一个双层优化算法来从数据中探索可用的预测不确定性。经验证实，探索出的异质性为子人群划分提供了洞见。

    As an intrinsic and fundamental property of big data, data heterogeneity exists in a variety of real-world applications, such as precision medicine, autonomous driving, financial applications, etc. For machine learning algorithms, the ignorance of data heterogeneity will greatly hurt the generalization performance and the algorithmic fairness, since the prediction mechanisms among different sub-populations are likely to differ from each other. In this work, we focus on the data heterogeneity that affects the prediction of machine learning models, and firstly propose the \emph{usable predictive heterogeneity}, which takes into account the model capacity and computational constraints. We prove that it can be reliably estimated from finite data with probably approximately correct (PAC) bounds. Additionally, we design a bi-level optimization algorithm to explore the usable predictive heterogeneity from data. Empirically, the explored heterogeneity provides insights for sub-population divis
    
[^15]: RL中的反向攻击保护：恢复触发状态方法

    Recover Triggered States: Protect Model Against Backdoor Attack in Reinforcement Learning. (arXiv:2304.00252v1 [cs.LG])

    [http://arxiv.org/abs/2304.00252](http://arxiv.org/abs/2304.00252)

    本文提出了恢复触发状态(RTS)方法，用于保护RL代理免受反向攻击。该方法涉及构建替代网络来近似动态模型，并将触发状态恢复为干净状态来防止攻击者通过触发器激活隐藏在代理中的后门。

    

    反向攻击可以使恶意用户操纵环境或破坏训练数据，并将一个隐藏的后门插入到训练代理程序中。这种攻击危及RL系统的可靠性，在各个关键领域可能会造成灾难性的影响。与此相比，对于RL中的反向攻击有效的防御措施的研究相对较少。本文提出了一种新颖的方法——恢复触发状态(RTS)，能够有效地保护受害代理免受反向攻击。 RTS需要构建一个替代网络来近似动态模型。开发人员可以通过将触发状态恢复为干净状态来防止攻击者通过触发器激活代理中隐藏的后门。在训练替代网络来预测状态时，我们将代理动作信息并入，减少代理在预测状态上采取的动作和实际状态上采取的动作之间的差异。

    A backdoor attack allows a malicious user to manipulate the environment or corrupt the training data, thus inserting a backdoor into the trained agent. Such attacks compromise the RL system's reliability, leading to potentially catastrophic results in various key fields. In contrast, relatively limited research has investigated effective defenses against backdoor attacks in RL. This paper proposes the Recovery Triggered States (RTS) method, a novel approach that effectively protects the victim agents from backdoor attacks. RTS involves building a surrogate network to approximate the dynamics model. Developers can then recover the environment from the triggered state to a clean state, thereby preventing attackers from activating backdoors hidden in the agent by presenting the trigger. When training the surrogate to predict states, we incorporate agent action information to reduce the discrepancy between the actions taken by the agent on predicted states and the actions taken on real sta
    
[^16]: 从概念到部署：基于机器学习和性能评估的智能卒中预测框架

    From Conception to Deployment: Intelligent Stroke Prediction Framework using Machine Learning and Performance Evaluation. (arXiv:2304.00249v1 [cs.LG])

    [http://arxiv.org/abs/2304.00249](http://arxiv.org/abs/2304.00249)

    本文提出了一个基于机器学习和性能评估的智能卒中预测框架，并比较了五种常用的机器学习算法，结果显示随机森林算法最适合卒中预测。

    

    卒中是全球第二大死因。机器学习分类算法被广泛用于卒中预测，但是这些算法使用不同的数据集和评估指标进行评估。此外，目前没有针对卒中数据分析的综合框架。本文通过对文献中常用的机器学习预测算法的重要性分析提出了一个智能卒中预测框架。对卒中预测中五个最常用的机器学习算法进行了比较，并使用统一的设置进行客观比较。比较分析和数字结果显示，随机森林算法最适合卒中预测。

    Stroke is the second leading cause of death worldwide. Machine learning classification algorithms have been widely adopted for stroke prediction. However, these algorithms were evaluated using different datasets and evaluation metrics. Moreover, there is no comprehensive framework for stroke data analytics. This paper proposes an intelligent stroke prediction framework based on a critical examination of machine learning prediction algorithms in the literature. The five most used machine learning algorithms for stroke prediction are evaluated using a unified setup for objective comparison. Comparative analysis and numerical results reveal that the Random Forest algorithm is best suited for stroke prediction.
    
[^17]: 重启贝叶斯在线变点检测用于非平稳马尔科夫决策过程

    Restarted Bayesian Online Change-point Detection for Non-Stationary Markov Decision Processes. (arXiv:2304.00232v1 [cs.LG])

    [http://arxiv.org/abs/2304.00232](http://arxiv.org/abs/2304.00232)

    该论文介绍了一种针对非平稳强化学习环境的算法，称为R-BOCPD-UCRL2，它使用了一种重新启动的贝叶斯在线变点检测算法（R-BOCPD），并在多项式分布采样的MDP上提供了较优秀的性能保证。

    

    我们考虑在一个非平稳的强化学习（RL）环境中进行学习的问题，其中该设置可以被完全描述为分段平稳的离散时间马尔科夫决策过程（MDP）。我们引入了一种重新启动的贝叶斯在线变点检测算法（R-BOCPD）变体，该算法适用于从更一般的多项式分布中生成的输入流，并在误警率和检测延迟方面提供接近最优的理论保证。基于此，我们提出了一种针对从多项式分布中采样的状态转移内核的MDPs的改进版本UCRL2算法，我们称之为R-BOCPD-UCRL2。我们进行了有限时间的性能分析，并表明R-BOCPD-UCRL2具有有利的遗憾界的$O\left(D O \sqrt{A T K_T \log\left (\frac{T}{\delta} \right) + \frac{K_T \log \frac{K_T}{\delta}}{\min\limits_\ell \: \mathbf{KL}\left( {\mathbf{\theta}^{(\ell+1)}}\mid\mid{\mathbf{\theta}^{(\ell)}}\right)}}\right)$。

    We consider the problem of learning in a non-stationary reinforcement learning (RL) environment, where the setting can be fully described by a piecewise stationary discrete-time Markov decision process (MDP). We introduce a variant of the Restarted Bayesian Online Change-Point Detection algorithm (R-BOCPD) that operates on input streams originating from the more general multinomial distribution and provides near-optimal theoretical guarantees in terms of false-alarm rate and detection delay. Based on this, we propose an improved version of the UCRL2 algorithm for MDPs with state transition kernel sampled from a multinomial distribution, which we call R-BOCPD-UCRL2. We perform a finite-time performance analysis and show that R-BOCPD-UCRL2 enjoys a favorable regret bound of $O\left(D O \sqrt{A T K_T \log\left (\frac{T}{\delta} \right) + \frac{K_T \log \frac{K_T}{\delta}}{\min\limits_\ell \: \mathbf{KL}\left( {\mathbf{\theta}^{(\ell+1)}}\mid\mid{\mathbf{\theta}^{(\ell)}}\right)}}\right)$,
    
[^18]: 遮蔽分层特征的自监督学习

    Mask Hierarchical Features For Self-Supervised Learning. (arXiv:2304.00218v1 [cs.CV])

    [http://arxiv.org/abs/2304.00218](http://arxiv.org/abs/2304.00218)

    本文提出了MaskDeep这一自监督方法，利用遮蔽分层特征的策略，从稀疏的可见补丁来推理图像的全局语义。本方法相比其它自监督方法取得了不错的改进。

    

    本文展示了遮蔽深层分层特征的有效自监督方法，称为MaskDeep。 MaskDeep将表示空间中的每个补丁视为独立的实例。我们在表示空间中遮蔽部分补丁，然后利用稀疏可见补丁重建高语义图像表示。 MaskDeep的直觉在于，模型可以通过从稀疏可见补丁语义到图像的全局语义推理。我们在框架中进一步提出了三个设计：1）层次深层遮蔽模块，以关注补丁表示的分层特性，2）多组策略，以提高编码器的效率而不需要任何额外的计算消耗，3）多目标策略，以提供更多全局语义的描述。我们的MaskDeep带来了不错的改进。在ResNet50上进行200个epoch的训练，MaskDeep在I上实现了71.2％的Top1准确率线性分类，达到了最新的结果。

    This paper shows that Masking the Deep hierarchical features is an efficient self-supervised method, denoted as MaskDeep. MaskDeep treats each patch in the representation space as an independent instance. We mask part of patches in the representation space and then utilize sparse visible patches to reconstruct high semantic image representation. The intuition of MaskDeep lies in the fact that models can reason from sparse visible patches semantic to the global semantic of the image. We further propose three designs in our framework: 1) a Hierarchical Deep-Masking module to concern the hierarchical property of patch representations, 2) a multi-group strategy to improve the efficiency without any extra computing consumption of the encoder and 3) a multi-target strategy to provide more description of the global semantic. Our MaskDeep brings decent improvements. Trained on ResNet50 with 200 epochs, MaskDeep achieves state-of-the-art results of 71.2% Top1 accuracy linear classification on I
    
[^19]: 病理图像诊断的跨尺度多实例学习

    Cross-scale Multi-instance Learning for Pathological Image Diagnosis. (arXiv:2304.00216v1 [cs.CV])

    [http://arxiv.org/abs/2304.00216](http://arxiv.org/abs/2304.00216)

    本研究提出了一种新的跨尺度MIL算法，将跨尺度关系显式聚合到一个病理图像诊断的MIL网络中，有效地解决了忽略对人类病理学家诊断至关重要的跨尺度信息的问题。

    

    数字病理学中，跨多个尺度分析高分辨率的全幅图像 (WSIs) 带来了巨大的挑战。多实例学习 (MIL) 是利用分类对象集 (例如较小的图像块集) 对高分辨率图像进行处理的常见方法。然而，这种处理通常在WSIs的单个尺度（例如20倍放大）上进行，忽略了对人类病理学家诊断至关重要的跨尺度信息。在本研究中，我们提出了一种新的跨尺度MIL算法，将跨尺度关系显式聚合到一个病理图像诊断的MIL网络中。本文的贡献有三个方面：(1) 提出了一种新的跨尺度MIL (CS-MIL)算法，它集成了多尺度信息和跨尺度关系；(2) 创建并发布了一个玩具数据集，其中包含尺度特异性形态特征，以检查和可视化不同的跨尺度关系；(3)在四个WSI的细胞肺癌数据集上进行了实验验证CS-MIL的有效性。

    Analyzing high resolution whole slide images (WSIs) with regard to information across multiple scales poses a significant challenge in digital pathology. Multi-instance learning (MIL) is a common solution for working with high resolution images by classifying bags of objects (i.e. sets of smaller image patches). However, such processing is typically performed at a single scale (e.g., 20x magnification) of WSIs, disregarding the vital inter-scale information that is key to diagnoses by human pathologists. In this study, we propose a novel cross-scale MIL algorithm to explicitly aggregate inter-scale relationships into a single MIL network for pathological image diagnosis. The contribution of this paper is three-fold: (1) A novel cross-scale MIL (CS-MIL) algorithm that integrates the multi-scale information and the inter-scale relationships is proposed; (2) A toy dataset with scale-specific morphological features is created and released to examine and visualize differential cross-scale a
    
[^20]: 基于分层Transformer的关系路径和上下文归纳关系预测方法

    Inductive Relation Prediction from Relational Paths and Context with Hierarchical Transformers. (arXiv:2304.00215v1 [cs.CL])

    [http://arxiv.org/abs/2304.00215](http://arxiv.org/abs/2304.00215)

    本文提出了一种基于分层Transformer的方法，即REPORT，能够同时聚合关系路径和上下文，捕捉实体之间的联系和内在特性。它完全依赖于关系语义，并能自然地推广到完全归纳的设置中。在基准数据集上实现了最先进的性能。

    

    在知识图谱中进行关系预测是一个重要的研究课题。现有的嵌入式方法主要依赖于转导设置，缺乏归纳能力，无法推广到新的实体上进行推理。本文提出了一种新方法，通过使用统一的分层Transformer框架，即REPORT，同时聚合关系路径和上下文，捕捉实体之间的联系和内在特性，这种方法完全依赖于关系语义，并能自然地推广到完全归纳的设置中。在实验中，REPORT表现优于所有基线方法，甚至在两个完全归纳的数据集的八个版本子集上也是如此。此外，REPORT能够将推理推广到训练和推理中没有公共实体的新实体上，并在基准数据集上实现了最先进的性能。

    Relation prediction on knowledge graphs (KGs) is a key research topic. Dominant embedding-based methods mainly focus on the transductive setting and lack the inductive ability to generalize to new entities for inference. Existing methods for inductive reasoning mostly mine the connections between entities, i.e., relational paths, without considering the nature of head and tail entities contained in the relational context. This paper proposes a novel method that captures both connections between entities and the intrinsic nature of entities, by simultaneously aggregating RElational Paths and cOntext with a unified hieRarchical Transformer framework, namely REPORT. REPORT relies solely on relation semantics and can naturally generalize to the fully-inductive setting, where KGs for training and inference have no common entities. In the experiments, REPORT performs consistently better than all baselines on almost all the eight version subsets of two fully-inductive datasets. Moreover. REPO
    
[^21]: 基于Neo4j和深度学习的交通拥堵模拟与优化

    Leveraging Neo4j and deep learning for traffic congestion simulation & optimization. (arXiv:2304.00192v1 [cs.AI])

    [http://arxiv.org/abs/2304.00192](http://arxiv.org/abs/2304.00192)

    本文利用Neo4j图和深度学习方法解决城市道路网络中的交通拥堵问题，能够实现拥堵路段的负载平衡和优化，同时可以预测交通事故对整体交通的影响。

    

    交通拥堵一直是城市道路网络中的主要挑战。过去进行了大量研究，以凸显与交通拥堵相关的问题，并通过数据驱动的方法解决了这个问题。目前，大多数交通拥堵分析都是使用模拟软件进行的，这些软件由于使用的工具和实用程序的限制而提供了有限的洞见。所有这些都影响到定制业务问题的制定，这些问题因地区和国家而异。通过利用知识图的能力，我们将交通拥堵问题建模为Neo4j图，然后使用负载平衡、优化算法来识别无拥堵的道路网络。我们还展示了在拥堵或事故情况下交通如何向后传播以及其对其他道路段的总体影响。我们还在实时交通数据上训练了顺序RNN-LSTM(长短时记忆)深度学习模型。

    Traffic congestion has been a major challenge in many urban road networks. Extensive research studies have been conducted to highlight traffic-related congestion and address the issue using data-driven approaches. Currently, most traffic congestion analyses are done using simulation software that offers limited insight due to the limitations in the tools and utilities being used to render various traffic congestion scenarios. All that impacts the formulation of custom business problems which vary from place to place and country to country. By exploiting the power of the knowledge graph, we model a traffic congestion problem into the Neo4j graph and then use the load balancing, optimization algorithm to identify congestion-free road networks. We also show how traffic propagates backward in case of congestion or accident scenarios and its overall impact on other segments of the roads. We also train a sequential RNN-LSTM (Long Short-Term Memory) deep learning model on the real-time traffi
    
[^22]: 好奇心驱动探索中欧氏群与射影群对于智能体内部空间的作用：形式化分析

    Action of the Euclidean versus Projective group on an agent's internal space in curiosity driven exploration: a formal analysis. (arXiv:2304.00188v1 [cs.AI])

    [http://arxiv.org/abs/2304.00188](http://arxiv.org/abs/2304.00188)

    该论文将人类空间感知的3D射影几何学与智能体感知方案中的群概念相结合，探讨不同群结构如何影响智能体的好奇心驱动探索行为。

    

    在人类的空间感知中，信息似乎是根据三维射影几何学表示的。它将信息集成和行动规划组织在一个内部表示空间中。不同个体的第一人称视角是如何通过世界模型的变换相互关联，并定义智能体的特定感知方案。这些变换的集合在数学上被称为“群”，它通过对几何空间的操作来表征其特征。我们提出将拥有“几何”结构的世界模型，给出由群提供的一种方式来捕捉代理之间不同的感知方案。我们探讨了改变世界模型的几何结构如何影响智能体的行为。具体而言，我们着眼于这些几何运算如何通过在驱动智能体好奇心的主观推断的形式表达上进行转换，并相应地影响探索行为。我们使用了群作用。

    In human spatial awareness, information appears to be represented according to 3-D projective geometry. It structures information integration and action planning within an internal representation space. The way different first person perspectives of an agent relate to each other, through transformations of a world model, defines a specific perception scheme for the agent. In mathematics, this collection of transformations is called a `group' and it characterizes a geometric space by acting on it. We propose that imbuing world models with a `geometric' structure, given by a group, is one way to capture different perception schemes of agents. We explore how changing the geometric structure of a world model impacts the behavior of an agent.  In particular, we focus on how such geometrical operations transform the formal expression of epistemic value in active inference as driving an agent's curiosity about its environment, and impact exploration behaviors accordingly. We used group action
    
[^23]: 基于徒弟学习的面向主题的文本到图像生成器

    Subject-driven Text-to-Image Generation via Apprenticeship Learning. (arXiv:2304.00186v1 [cs.CV])

    [http://arxiv.org/abs/2304.00186](http://arxiv.org/abs/2304.00186)

    该论文提出了一种基于徒弟学习的面向主题的文本到图像生成器SuTI，能够通过将大量基于主题的专家模型的数据输入徒弟模型，学习并推断出新主题的最佳专家模型，从而生成高品质的自定义图像，且速度比传统方法更快。

    

    最近的文本到图像生成模型（如DreamBooth）在通过针对目标主题微调“专家模型”，生成高度自定义的图像方面取得了显著进展。然而，这个过程很昂贵，因为每个主题都必须学习一个新的专家模型。在本文中，我们提出了一个替代主题特定微调的面向主题的文本到图像生成器SuTI。给定一个新主题的少量演示，SuTI可以即时生成不同场景中主题的新版本，而无需进行任何主题特定的优化。SuTI由“徒弟学习”驱动，其中从大量基于主题的专家模型生成的数据中学习单个的徒弟模型。具体而言，我们从互联网挖掘了数百万个图像簇，每个图像簇都聚焦于一个特定的视觉主题。我们采用这些簇来训练大量专门针对不同视觉主题的专家模型。徒弟模型通过推断基于其文本描述的新主题的最佳专家模型并生成图像来学习。我们在各种基准数据集上展示了SuTI的有效性，表明它可以生成高品质的不同主题的图像，同时比基于微调的方法快得多。

    Recent text-to-image generation models like DreamBooth have made remarkable progress in generating highly customized images of a target subject, by fine-tuning an ``expert model'' for a given subject from a few examples. However, this process is expensive, since a new expert model must be learned for each subject. In this paper, we present SuTI, a Subject-driven Text-to-Image generator that replaces subject-specific fine tuning with \emph{in-context} learning. Given a few demonstrations of a new subject, SuTI can instantly generate novel renditions of the subject in different scenes, without any subject-specific optimization. SuTI is powered by {\em apprenticeship learning}, where a single apprentice model is learned from data generated by massive amount of subject-specific expert models. Specifically, we mine millions of image clusters from the Internet, each centered around a specific visual subject. We adopt these clusters to train massive amount of expert models specialized on diff
    
[^24]: PrefGen：基于偏好的相对属性图像生成

    PrefGen: Preference Guided Image Generation with Relative Attributes. (arXiv:2304.00185v1 [cs.CV])

    [http://arxiv.org/abs/2304.00185](http://arxiv.org/abs/2304.00185)

    $\textit{PrefGen}$ 系统利用简单的成对比较查询，控制生成图像的相对属性。利用这些查询响应的信息，对一组图像属性的偏好进行估计，并进行基于偏好的图像编辑。

    

    深度生成模型可以渲染出高保真的人脸等内容的图像。近年来，在生成具有特定数量属性的图像方面取得了实质性进展，例如情感等。这些方法通常需要用户明确量化所需视觉属性的强度。但是限制在于许多属性，例如面部表情的 "愤怒" 程度，用户难以准确量化。然而，用户可以可靠地表达出 "哪张脸看起来更愤怒"，基于这个假设，我们开发了 $\textit{PrefGen}$ 系统，通过呈现用户简单的成对比较查询，如 "你更喜欢图像 $a$ 还是 $b$？" 来控制生成图像的相对属性。利用序列化查询响应的信息，我们可以估计用户对一组图像属性的偏好，并进行基于偏好的图像编辑。

    Deep generative models have the capacity to render high fidelity images of content like human faces. Recently, there has been substantial progress in conditionally generating images with specific quantitative attributes, like the emotion conveyed by one's face. These methods typically require a user to explicitly quantify the desired intensity of a visual attribute. A limitation of this method is that many attributes, like how "angry" a human face looks, are difficult for a user to precisely quantify. However, a user would be able to reliably say which of two faces seems "angrier". Following this premise, we develop the $\textit{PrefGen}$ system, which allows users to control the relative attributes of generated images by presenting them with simple paired comparison queries of the form "do you prefer image $a$ or image $b$?" Using information from a sequence of query responses, we can estimate user preferences over a set of image attributes and perform preference-guided image editing 
    
[^25]: FCC: 在对话系统中融合历史对话和候选来源进行上下文响应排序

    FCC: Fusing Conversation History and Candidate Provenance for Contextual Response Ranking in Dialogue Systems. (arXiv:2304.00180v1 [cs.CL])

    [http://arxiv.org/abs/2304.00180](http://arxiv.org/abs/2304.00180)

    本文提出了一种灵活的神经框架，可以将来自多个渠道的历史对话和候选来源上下文信息融合在一起，以改善多轮对话响应排序表现。

    

    对话中的响应排序在检索型会话系统中起着关键作用。在多轮对话中，为了捕捉对话的要点，上下文信息作为重要的知识至关重要。本文提出了一个灵活的神经框架，可以整合来自多个渠道的上下文信息。特别是针对当前任务，我们的方法是提供两个信息通道并行，即将与候选来源相关的 Conversational History（对话历史）和 Domain Knowledge（领域知识）融合在一起，作为上下文信息以提高多轮对话响应排序的表现。所提出的方法可以通用于将其他上下文目标任务的各种上下文特征纳入模块中。我们在广泛用于评估会话响应排名任务的 MSDialog 数据集上评估了我们的模型。实验结果显示，我们的框架显着优于其他最先进的方法。

    Response ranking in dialogues plays a crucial role in retrieval-based conversational systems. In a multi-turn dialogue, to capture the gist of a conversation, contextual information serves as essential knowledge to achieve this goal. In this paper, we present a flexible neural framework that can integrate contextual information from multiple channels. Specifically for the current task, our approach is to provide two information channels in parallel, Fusing Conversation history and domain knowledge extracted from Candidate provenance (FCC), where candidate responses are curated, as contextual information to improve the performance of multi-turn dialogue response ranking. The proposed approach can be generalized as a module to incorporate miscellaneous contextual features for other context-oriented tasks. We evaluate our model on the MSDialog dataset widely used for evaluating conversational response ranking tasks. Our experimental results show that our framework significantly outperform
    
[^26]: 用轻量级神经网络提高极端天气事件的检测能力

    Improving extreme weather events detection with light-weight neural networks. (arXiv:2304.00176v1 [cs.CV])

    [http://arxiv.org/abs/2304.00176](http://arxiv.org/abs/2304.00176)

    本论文提出了一种基于轻量级神经网络的改进方法来检测极端天气事件，特别关注于热带气旋，并通过采用加权损失函数的方法取得了成功。

    

    为了推进极端天气事件的自动化检测，我们探索了对一种新型轻量级上下文引导卷积神经网络架构进行的修改，该网络用于在气候数据中进行热带气旋和大气河流的语义分割的训练。我们的主要关注点是在当前表现有限的热带气旋上。我们研究了特征工程、数据增强、学习率修改、替代损失函数和架构变化。与以往优化交叉点以上的方法不同，我们特别寻求改进召回率以惩罚欠计数并优先识别热带气旋。我们通过使用加权损失函数来抵消罕见事件的类别不平衡的方法而获得成功。我们最后总结未来极端天气事件检测的研究方向，这是一个至关重要的任务。

    To advance automated detection of extreme weather events, which are increasing in frequency and intensity with climate change, we explore modifications to a novel light-weight Context Guided convolutional neural network architecture trained for semantic segmentation of tropical cyclones and atmospheric rivers in climate data. Our primary focus is on tropical cyclones, the most destructive weather events, for which current models show limited performance. We investigate feature engineering, data augmentation, learning rate modifications, alternative loss functions, and architectural changes. In contrast to previous approaches optimizing for intersection over union, we specifically seek to improve recall to penalize under-counting and prioritize identification of tropical cyclones. We report success through the use of weighted loss functions to counter class imbalance for these rare events. We conclude with directions for future research on extreme weather events detection, a crucial tas
    
[^27]: Lego-Features：导出模块化编码器特征用于流式和决策ASR

    Lego-Features: Exporting modular encoder features for streaming and deliberation ASR. (arXiv:2304.00173v1 [cs.CL])

    [http://arxiv.org/abs/2304.00173](http://arxiv.org/abs/2304.00173)

    该论文介绍了一种称为Lego-Features的特征，将现有编码表示转换成模块化特征，而不修改预训练模型，在流式环境中也适用。这些特征性能强大，RNN-T或LAS解码器测试时保持高质量的下游性能，并足够丰富，可以代表两个通道决策中的第一遍预测。

    

    在端到端的语音识别模型中，编码器和解码器之间不可避免地出现了一种紧密耦合的表达。我们建立在最近开始探索构建带有模块化编码表示的编码器的工作之上，这样不同模型的编码器和解码器可以在零样本的情况下进行拼接，而无需进行进一步的微调。虽然之前的研究只涉及全上下文语音模型，但我们也在流式环境中探索了这个问题。我们的框架建立在现有编码表示之上，将其转换为模块化特征，称为Lego-Features，而不修改预训练模型。这些特征在用不同的初始化重新训练模型时仍然可以相互替换。尽管稀疏，我们展示了Lego-Features在使用RNN-T或LAS解码器进行测试时具有强大的性能，保持高质量的下游性能。它们也足够丰富，可以代表两个通道决策中的第一遍预测。

    In end-to-end (E2E) speech recognition models, a representational tight-coupling inevitably emerges between the encoder and the decoder. We build upon recent work that has begun to explore building encoders with modular encoded representations, such that encoders and decoders from different models can be stitched together in a zero-shot manner without further fine-tuning. While previous research only addresses full-context speech models, we explore the problem in a streaming setting as well. Our framework builds on top of existing encoded representations, converting them to modular features, dubbed as Lego-Features, without modifying the pre-trained model. The features remain interchangeable when the model is retrained with distinct initializations. Though sparse, we show that the Lego-Features are powerful when tested with RNN-T or LAS decoders, maintaining high-quality downstream performance. They are also rich enough to represent the first-pass prediction during two-pass deliberatio
    
[^28]: 基于方向连通性的医学图像分割

    Directional Connectivity-based Segmentation of Medical Images. (arXiv:2304.00145v1 [eess.IV])

    [http://arxiv.org/abs/2304.00145](http://arxiv.org/abs/2304.00145)

    本研究提出了一种基于方向连通性的医学图像分割方案，在深度网络中有效地解耦和利用通道方向信息，实现了解剖一致分割，实验结果超越了现有最先进方法。

    

    生物标记分割中的解剖一致性对于许多医学图像分析任务至关重要。将像素连通性，即数字拓扑中的基本概念，纳入深度网络以建模像素间的关系是实现解剖一致分割的有前途的模式。然而，以前的连通性模型工作忽略了潜在空间中丰富的通道方向信息。在本研究中，我们证明了从共享潜在空间中有效地分离出方向子空间，可以显著增强基于连通性的网络中的特征表示。为此，我们提出了一种方向连接建模方案，用于分割，可以在整个网络中解耦、跟踪和利用方向信息。对各种公共医学图像分割基准的实验证明了我们的模型相对于现有最先进方法的有效性。代码可在https://github.com/Zyun-Y/Dconn获取。

    Anatomical consistency in biomarker segmentation is crucial for many medical image analysis tasks. A promising paradigm for achieving anatomically consistent segmentation via deep networks is incorporating pixel connectivity, a basic concept in digital topology, to model inter-pixel relationships. However, previous works on connectivity modeling have ignored the rich channel-wise directional information in the latent space. In this work, we demonstrate that effective disentanglement of directional sub-space from the shared latent space can significantly enhance the feature representation in the connectivity-based network. To this end, we propose a directional connectivity modeling scheme for segmentation that decouples, tracks, and utilizes the directional information across the network. Experiments on various public medical image segmentation benchmarks show the effectiveness of our model as compared to the state-of-the-art methods. Code is available at https://github.com/Zyun-Y/Dconn
    
[^29]: 建立基于表面的联邦Chow检验模型，整合APOE状态、Tau沉积度和海马表面形态学

    A Surface-Based Federated Chow Test Model for Integrating APOE Status, Tau Deposition Measure, and Hippocampal Surface Morphometry. (arXiv:2304.00134v1 [physics.med-ph])

    [http://arxiv.org/abs/2304.00134](http://arxiv.org/abs/2304.00134)

    本文建立了一种联邦Chow检验模型，整合了APOE状态、Tau沉积度和海马表面形态学，可以用于阿尔茨海默病的新型诊断生物标志物的开发

    

    阿尔茨海默病是最常见的老年痴呆症类型，根据CDC数据，影响65岁或以上的6.2百万人。发现阿尔茨海默病诊断生物标志物可能具有极大的公共卫生效益。Tau神经原纤维缠结是造成阿尔茨海默病下游神经退行性和随后认知障碍的主要驱动因素，导致如海马萎缩等结构变形，可以在磁共振成像扫描中观察到。本文旨在构建一个基于表面的模型，以1）检测APOE亚组之间Tau沉积和海马萎缩模式的差异，和2）利用提取的基于表面的特征预测认知下降。

    Background: Alzheimer's Disease (AD) is the most common type of age-related dementia, affecting 6.2 million people aged 65 or older according to CDC data. It is commonly agreed that discovering an effective AD diagnosis biomarker could have enormous public health benefits, potentially preventing or delaying up to 40% of dementia cases. Tau neurofibrillary tangles are the primary driver of downstream neurodegeneration and subsequent cognitive impairment in AD, resulting in structural deformations such as hippocampal atrophy that can be observed in magnetic resonance imaging (MRI) scans. Objective: To build a surface-based model to 1) detect differences between APOE subgroups in patterns of tau deposition and hippocampal atrophy, and 2) use the extracted surface-based features to predict cognitive decline. Methods: Using data obtained from different institutions, we develop a surface-based federated Chow test model to study the synergistic effects of APOE, a previously reported significa
    
[^30]: 稠密稀疏检索：使用稀疏语言模型实现高效稠密检索

    Dense Sparse Retrieval: Using Sparse Language Models for Inference Efficient Dense Retrieval. (arXiv:2304.00114v1 [cs.IR])

    [http://arxiv.org/abs/2304.00114](http://arxiv.org/abs/2304.00114)

    本文研究了如何使用稀疏语言模型实现高效稠密检索，使用Tevatron 和MSMARCO、NQ和TriviaQA数据集，发现稀疏语言模型可以直接替换为原有模型，几乎不降低准确性，并且推理速度提高了最多4.3倍。

    

    基于向量的检索系统已成为学术和工业搜索应用的常见工具，因为它们提供了一种简单而可扩展的方式来利用文档和查询的上下文表示进行搜索。由于这些向量系统依赖于上下文语言模型，因此它们通常需要GPU的使用，这可能会很昂贵且难以管理。鉴于近年来引入稀疏性以提高推理效率的语言模型的最新进展，本文研究了如何使用稀疏语言模型进行稠密检索以提高推理效率。使用流行的检索库Tevatron和MSMARCO、NQ和TriviaQA数据集，我们发现稀疏语言模型可以直接替换为原有模型，几乎不降低准确性，并且推理速度提高了最多4.3倍。

    Vector-based retrieval systems have become a common staple for academic and industrial search applications because they provide a simple and scalable way of extending the search to leverage contextual representations for documents and queries. As these vector-based systems rely on contextual language models, their usage commonly requires GPUs, which can be expensive and difficult to manage. Given recent advances in introducing sparsity into language models for improved inference efficiency, in this paper, we study how sparse language models can be used for dense retrieval to improve inference efficiency. Using the popular retrieval library Tevatron and the MSMARCO, NQ, and TriviaQA datasets, we find that sparse language models can be used as direct replacements with little to no drop in accuracy and up to 4.3x improved inference speeds
    
[^31]: 机器学习在经济研究中的应用：何时、什么和如何运用？

    Machine Learning for Economics Research: When What and How?. (arXiv:2304.00086v1 [econ.GN])

    [http://arxiv.org/abs/2304.00086](http://arxiv.org/abs/2304.00086)

    本文精选综述了使用机器学习工具进行经济学研究和政策分析的文章，强调了机器学习在处理非传统和非结构化数据、捕捉强非线性性和提高预测准确性方面的应用优势，成为计量经济学家工具箱中不可或缺的一部分。

    

    本文对使用机器学习工具进行经济学研究和政策分析的重要经济期刊上发表的文章进行了精选综述。综述回答了三个关键问题：（1）何时在经济学中使用机器学习，（2）常用的机器学习模型是什么，以及（3）如何将它们用于经济应用。综述强调了机器学习特别适用于处理非传统和非结构化数据、捕捉强非线性性和提高预测准确性。深度学习模型适用于非传统数据，而集成学习模型适用于传统数据集。尽管传统的计量经济学模型在分析低复杂性数据时可能足够，但由于快速数字化和不断增长的文献，经济数据的复杂性增加，机器学习正成为计量经济学家工具箱中不可或缺的一部分。

    This article provides a curated review of selected papers published in prominent economics journals that use machine learning (ML) tools for research and policy analysis. The review focuses on three key questions: (1) when ML is used in economics, (2) what ML models are commonly preferred, and (3) how they are used for economic applications. The review highlights that ML is particularly used in processing nontraditional and unstructured data, capturing strong nonlinearity, and improving prediction accuracy. Deep learning models are suitable for nontraditional data, whereas ensemble learning models are preferred for traditional datasets. While traditional econometric models may suffice for analyzing low-complexity data, the increasing complexity of economic data due to rapid digitalization and the growing literature suggest that ML is becoming an essential addition to the econometrician's toolbox.
    
[^32]: 坚固且公正: 确保公平与鲁棒性相一致

    To be Robust and to be Fair: Aligning Fairness with Robustness. (arXiv:2304.00061v1 [cs.LG])

    [http://arxiv.org/abs/2304.00061](http://arxiv.org/abs/2304.00061)

    本研究提出了一种同时考虑公平性和准确性指标的对抗训练和攻击方法，并证明了两个指标之间的一致性以及互相受益的关系。

    

    对抗训练已经被证明可以可靠地提高对抗样本的鲁棒性。然而，就公平性而言，对抗训练的问题尚未得到适当研究，公平性与准确性攻击之间的关系仍然不清楚。我们是否可以同时提高对公平性和准确性的鲁棒性？为了解决这个问题，本文研究了对抗训练和对抗攻击对这两个指标的问题。我们提出了一种公平性攻击的统一结构，将群体公平中的常见概念汇集在一起，并在理论上证明了不同概念下的公平性攻击等价性。此外，我们展示了公平性和准确性攻击的一致性，并在理论上证明了一种指标的鲁棒性会受到另一种指标鲁棒性的益处。我们的研究提出了一种统一公平与准确性的对抗训练和攻击的新方法，实验结果表明...

    Adversarial training has been shown to be reliable in improving robustness against adversarial samples. However, the problem of adversarial training in terms of fairness has not yet been properly studied, and the relationship between fairness and accuracy attack still remains unclear. Can we simultaneously improve robustness w.r.t. both fairness and accuracy? To tackle this topic, in this paper, we study the problem of adversarial training and adversarial attack w.r.t. both metrics. We propose a unified structure for fairness attack which brings together common notions in group fairness, and we theoretically prove the equivalence of fairness attack against different notions. Moreover, we show the alignment of fairness and accuracy attack, and theoretically demonstrate that robustness w.r.t. one metric benefits from robustness w.r.t. the other metric. Our study suggests a novel way to unify adversarial training and attack w.r.t. fairness and accuracy, and experimental results show that 
    
[^33]: 基于证据的Cyberlogic交易

    Evidential Transactions with Cyberlogic. (arXiv:2304.00060v1 [cs.CR])

    [http://arxiv.org/abs/2304.00060](http://arxiv.org/abs/2304.00060)

    Cyberlogic是一种基于逻辑基础的方法，用于构建和分析数字交易，同时涉及数字证据的交换。它可以定义授权策略的核心特征，并使用可信时间源在分布式系统中实现具有表达时态知识的授权策略和协议。

    

    Cyberlogic是一种用于构建和分析数字交易的逻辑基础，这些交易涉及数字证据的交换。它基于（一阶）直觉istic谓词逻辑的扩展，具有证明和知识情态。Cyberlogic的关键思想非常简单，即（1）公钥对应授权，（2）交易被指定为分布式逻辑程序，（3）可验证的证据通过分布式证明搜索收集。特别是可验证的证据是由额外的逻辑元素，如签署文件和加密签名构造的。尽管Cyberlogic具有这种概念上的简单性，但授权策略的中心特征，包括信任，授权的委托和吊销，是可定义的。因此，使用可信时间源，可以在Cyberlogic中定义用于指定分布式授权策略和协议的表达时态知识逻辑。

    Cyberlogic is an enabling logical foundation for building and analyzing digital transactions that involve the exchange of digital forms of evidence. It is based on an extension of (first-order) intuitionistic predicate logic with an attestation and a knowledge modality. The key ideas underlying Cyberlogic are extremely simple, as (1) public keys correspond to authorizations, (2) transactions are specified as distributed logic programs, and (3) verifiable evidence is collected by means of distributed proof search. Verifiable evidence, in particular, are constructed from extra-logical elements such as signed documents and cryptographic signatures. Despite this conceptual simplicity of Cyberlogic, central features of authorization policies including trust, delegation, and revocation of authority are definable. An expressive temporal-epistemic logic for specifying distributed authorization policies and protocols is therefore definable in Cyberlogic using a trusted time source. We describe 
    
[^34]: 使用离线预训练加速探索和表示学习

    Accelerating exploration and representation learning with offline pre-training. (arXiv:2304.00046v1 [cs.LG])

    [http://arxiv.org/abs/2304.00046](http://arxiv.org/abs/2304.00046)

    本论文提出了一个假设，即基于离线数据可以通过分别学习状态表示和辅助奖励模型来改善探索和表示学习，实验证明这种方法显著提高了在具有挑战性的 NetHack 基准测试上的样本效率。

    

    串行决策制定代理在长期任务中面临挑战，因为需要多步推理才能解决。大多数强化学习（RL）算法通过改进信用分配，引入记忆能力，改变代理的内在动机（即探索）或其世界观（即知识表示）来应对这一挑战。这些组成部分中的许多都可以从离线数据中学习。本文提出了一个假设，即通过从单个离线数据集中分别学习两个不同的模型，可以改善探索和表示学习。我们展示了使用噪声对比估计学习状态表示以及从单个人类演示的模型辅助奖励可以显著提高在具有挑战性的 NetHack 基准测试上的样本效率。我们还消融了实验设置的各个组成部分并突显了重要的见解。

    Sequential decision-making agents struggle with long horizon tasks, since solving them requires multi-step reasoning. Most reinforcement learning (RL) algorithms address this challenge by improved credit assignment, introducing memory capability, altering the agent's intrinsic motivation (i.e. exploration) or its worldview (i.e. knowledge representation). Many of these components could be learned from offline data. In this work, we follow the hypothesis that exploration and representation learning can be improved by separately learning two different models from a single offline dataset. We show that learning a state representation using noise-contrastive estimation and a model of auxiliary reward separately from a single collection of human demonstrations can significantly improve the sample efficiency on the challenging NetHack benchmark. We also ablate various components of our experimental setting and highlight crucial insights.
    
[^35]: SemiMemes：一种用于多模态Memes分析的半监督学习方法

    SemiMemes: A Semi-supervised Learning Approach for Multimodal Memes Analysis. (arXiv:2304.00020v1 [cs.LG])

    [http://arxiv.org/abs/2304.00020](http://arxiv.org/abs/2304.00020)

    研究提出了一种利用多模态数据的半监督学习方法，命名为SemiMemes，主要应用于Memes的分析和注释过程。该方法在多个数据集中表现优异，并优于其他最新的多模态半监督学习和监督学习模型。

    

    社交媒体上Memes的普及性引发了分析其隐含含义、审查有害内容的需求。机器学习的Meme审查系统需要半监督学习解决方案，以利用互联网上大量未标记的Memes，并使注释过程变得更简单。此外，该方法需要利用多模态数据，因为Memes的含义通常来自图像和文本。该研究提出了一种多模态半监督学习方法，在两个数据集，即多媒体自动性别歧视识别和令人讨厌的Memes数据集上，优于其他多模态半监督和监督学习的最新模型。借鉴对比语言-图像预训练所获得的见解，这项研究引入了SemiMemes，一种新颖的训练方法，它结合了自编码器和分类任务

    The prevalence of memes on social media has created the need to sentiment analyze their underlying meanings for censoring harmful content. Meme censoring systems by machine learning raise the need for a semi-supervised learning solution to take advantage of the large number of unlabeled memes available on the internet and make the annotation process less challenging. Moreover, the approach needs to utilize multimodal data as memes' meanings usually come from both images and texts. This research proposes a multimodal semi-supervised learning approach that outperforms other multimodal semi-supervised learning and supervised learning state-of-the-art models on two datasets, the Multimedia Automatic Misogyny Identification and Hateful Memes dataset. Building on the insights gained from Contrastive Language-Image Pre-training, which is an effective multimodal learning technique, this research introduces SemiMemes, a novel training method that combines auto-encoder and classification task to
    
[^36]: 多智能体价值因子化中冗余性的挑战

    The challenge of redundancy on multi-agent value factorisation. (arXiv:2304.00009v1 [cs.AI])

    [http://arxiv.org/abs/2304.00009](http://arxiv.org/abs/2304.00009)

    该论文提出了一种新的合作多智能体强化学习算法：关联分解网络，使用层次相关传播将联合值函数的学习和局部奖励信号的生成分开，针对存在大量冗余智能体的环境中，该算法比现有方法表现更出色。

    

    在合作多智能体强化学习（MARL）领域中，标准范式是使用集中式训练和分散式执行，在此范式中，中央批评家基于中央状态调节合作智能体的策略。研究表明，在存在大量冗余智能体的情况下，这些方法变得不太有效。在更一般的情况下，环境中的智能体数量可能比解决任务所需的数量要多。这些冗余智能体通过增加状态空间的维度和增加用于解决环境的联合策略的大小来降低性能。我们建议利用层次相关传播（LRP）来分离联合值函数的学习和局部奖励信号的生成，并创建一种新 的MARL算法：关联分解网络（RDN）。我们发现，尽管两个基线VDN和Qmix的性能随着冗余智能体数量的增加而降低，但RDN在这种情况下仍然表现出色。

    In the field of cooperative multi-agent reinforcement learning (MARL), the standard paradigm is the use of centralised training and decentralised execution where a central critic conditions the policies of the cooperative agents based on a central state. It has been shown, that in cases with large numbers of redundant agents these methods become less effective. In a more general case, there is likely to be a larger number of agents in an environment than is required to solve the task. These redundant agents reduce performance by enlarging the dimensionality of both the state space and and increasing the size of the joint policy used to solve the environment. We propose leveraging layerwise relevance propagation (LRP) to instead separate the learning of the joint value function and generation of local reward signals and create a new MARL algorithm: relevance decomposition network (RDN). We find that although the performance of both baselines VDN and Qmix degrades with the number of redu
    
[^37]: 关于大型语言模型的创造性研究

    On the Creativity of Large Language Models. (arXiv:2304.00008v1 [cs.AI])

    [http://arxiv.org/abs/2304.00008](http://arxiv.org/abs/2304.00008)

    这篇论文探讨了大型语言模型的创造性问题，分析了与之相关的机器创造性的难点和易点，并重点分析了这些技术在创意产业中的社会影响。

    

    大型语言模型(LLMs)正在颠覆人工智能的多个领域。其中最显著的应用之一是创作，例如诗歌或故事：生成的输出通常具有惊人的质量。但是，一个自然的问题是：LLMs真的可以被认为是创造性的吗？在本文中，我们首先通过创造性理论的角度分析了LLMs的发展，探讨了关键的未解决问题和挑战。然后，我们在与LLMs相关的机器创造性方面确定了一组“易”和“难”问题，并对其进行了讨论。最后，我们分析了这些技术在创意产业中的社会影响。

    Large Language Models (LLMs) are revolutionizing several areas of Artificial Intelligence. One of the most remarkable applications is creative writing, e.g., poetry or storytelling: the generated outputs are often of astonishing quality. However, a natural question arise: can LLMs really be considered creative? In this article we firstly analyze the development of LLMs under the lens of creativity theories, investigating the key open questions and challenges. Then, we identify a set of "easy" and "hard" problems in machine creativity, discussing them in relation to LLMs. Finally, we analyze the societal impact of these technologies with a particular focus on the creative industries.
    
[^38]: 能否借助AI-XR手术元宇宙重振介入式医疗？

    Can We Revitalize Interventional Healthcare with AI-XR Surgical Metaverses?. (arXiv:2304.00007v1 [cs.AI])

    [http://arxiv.org/abs/2304.00007](http://arxiv.org/abs/2304.00007)

    本论文讨论了AI-XR手术元宇宙的潜在应用和挑战，并强调了实现其潜力所需解决的问题，同时提出了案例研究，展示了安全威胁对AI-XR手术元宇宙的影响。

    

    近年来，技术特别是机器学习（ML）、深度学习（DL）和元宇宙的进步为革新外科科学提供了巨大的潜力。人工智能和增强现实（AI-XR）技术的结合有可能创建一个手术元宇宙，即手术可以在虚拟环境中规划和执行。本文旨在提供关于AI-XR手术元宇宙各种潜在应用和必须应对的挑战的深入了解。对这些挑战的关注对于充分实现AI-XR手术元宇宙的潜力至关重要。此外，为强调需要安全和强大的AI-XR手术元宇宙的需求并展示安全威胁对AI-XR手术元宇宙的实际影响，我们提出了一个案例研究，其中展示了“沉浸式手术攻击”对切口点定位的影响。

    Recent advancements in technology, particularly in machine learning (ML), deep learning (DL), and the metaverse, offer great potential for revolutionizing surgical science. The combination of artificial intelligence and extended reality (AI-XR) technologies has the potential to create a surgical metaverse, a virtual environment where surgeries can be planned and performed. This paper aims to provide insight into the various potential applications of an AI-XR surgical metaverse and the challenges that must be addressed to bring its full potential to fruition. It is important for the community to focus on these challenges to fully realize the potential of the AI-XR surgical metaverses. Furthermore, to emphasize the need for secure and robust AI-XR surgical metaverses and to demonstrate the real-world implications of security threats to the AI-XR surgical metaverses, we present a case study in which the ``an immersive surgical attack'' on incision point localization is performed in the co
    
[^39]: 面向旅行护理行业的使用多模型数据服务的双向个性化强化学习架构中的主动学习

    Bi-directional personalization reinforcement learning-based architecture with active learning using a multi-model data service for the travel nursing industry. (arXiv:2304.00006v1 [cs.IR])

    [http://arxiv.org/abs/2304.00006](http://arxiv.org/abs/2304.00006)

    本文提出了一个面向旅行护理行业的招聘方案，采用多模型数据服务加速数据采集，并使用双向强化学习和主动学习提供个性化推荐，解决了这一行业标注数据短缺的问题。

    

    本文讨论了机器学习技术如何通过使用多模型数据服务加速数据采集并使用双向强化学习和主动学习提供个性化推荐，从而增强旅行护理行业的招聘流程。该方案可帮助招聘人员推荐合格申请人，并使申请人接收到个性化工作推荐。本文还讨论了使用主动学习来解决这一行业标注数据短缺的问题。

    The challenges of using inadequate online recruitment systems can be addressed with machine learning and software engineering techniques. Bi-directional personalization reinforcement learning-based architecture with active learning can get recruiters to recommend qualified applicants and also enable applicants to receive personalized job recommendations. This paper focuses on how machine learning techniques can enhance the recruitment process in the travel nursing industry by helping speed up data acquisition using a multi-model data service and then providing personalized recommendations using bi-directional reinforcement learning with active learning. This need was especially evident when trying to respond to the overwhelming needs of healthcare facilities during the COVID-19 pandemic. The need for traveling nurses and other healthcare professionals was more evident during the lockdown period. A data service was architected for job feed processing using an orchestration of natural la
    
[^40]: 粗糙随机性及其应用

    Rough Randomness and its Application. (arXiv:2304.00005v1 [cs.AI])

    [http://arxiv.org/abs/2304.00005](http://arxiv.org/abs/2304.00005)

    本研究引入了新的粗糙随机性概念，用于捕捉静态和动态数据的各种粗糙过程，尤其是在构建粗糙模型方面具有重要作用，同时也探索了软/硬聚类算法的有效性。其中，大思想者粗糙随机函数是核心，本文还提出了两个计算有效的代数证明算法，用于软/硬集群验证。

    

    文献中已有多种随机性和信息论随机性的推广。然而，它们不能很好地处理粗糙推理（因此不能很好地应用于可解释的人工智能和机器学习的模糊和动态背景）。本研究由作者引入了粗糙随机性的新概念，它们既不是随机的，也不基于字符串的属性。这些概念旨在捕捉各种粗糙过程（适用于静态和动态数据），构建相关模型，并探索其他机器学习算法的有效性。本文中，最后提到的是软/硬聚类算法。此外，本研究还提出了两种计算有效的代数证明算法，用于软集群和硬集群验证，这些算法都涉及到了粗糙随机函数。一类名为“大思想者”的粗糙随机函数在本文中起着核心作用，并通过示例展示了它们在构建粗糙模型中的应用。

    A number of generalizations of stochastic and information-theoretic randomness are known in the literature. However, they are not compatible with handling meaning in vague and dynamic contexts of rough reasoning (and therefore explainable artificial intelligence and machine learning). In this research, new concepts of rough randomness that are neither stochastic nor based on properties of strings are introduced by the present author. Her concepts are intended to capture a wide variety of rough processes (applicable to both static and dynamic data), construct related models, and explore the validity of other machine learning algorithms. The last mentioned is restricted to soft/hard clustering algorithms in this paper. Two new computationally efficient algebraically-justified algorithms for soft and hard cluster validation that involve rough random functions are additionally proposed in this research. A class of rough random functions termed large-minded reasoners have a central role in 
    
[^41]: 分离领域本体论

    Disentangling Domain Ontologies. (arXiv:2304.00004v1 [cs.AI])

    [http://arxiv.org/abs/2304.00004](http://arxiv.org/abs/2304.00004)

    本文介绍了领域本体论中的概念纠结现象，并提出了一种多层概念建模策略，旨在构建概念上分离的领域本体论。

    

    本文介绍并论证了"概念纠结"现象，该现象由于逐步跨越以下五个层次（感知、标记、语义对齐、分层建模和内涵定义）之间的表征多样性而出现，随即我们提出了"概念分离"的多层概念建模策略，其通过指导原则强制和阐明每个概念纠缠层次（对所有上述五个层次）的语义双射，为构建概念上分离的领域本体论扫清了道路。另外，我们还简要阐述了为什么当前的本体论开发方法和方法论在我们的特定分析上是不足的。

    In this paper, we introduce and illustrate the novel phenomenon of Conceptual Entanglement which emerges due to the representational manifoldness immanent while incrementally modelling domain ontologies step-by-step across the following five levels: perception, labelling, semantic alignment, hierarchical modelling and intensional definition. In turn, we propose Conceptual Disentanglement, a multi-level conceptual modelling strategy which enforces and explicates, via guiding principles, semantic bijections with respect to each level of conceptual entanglement (across all the above five levels) paving the way for engineering conceptually disentangled domain ontologies. We also briefly argue why state-of-the-art ontology development methodologies and approaches are insufficient with respect to our characterization.
    
[^42]: 分析人工通用智能的语境缺陷

    Analyzing the Contextual Shortcomings of Artificial General Intelligence. (arXiv:2304.00002v1 [cs.AI])

    [http://arxiv.org/abs/2304.00002](http://arxiv.org/abs/2304.00002)

    论文讨论了人工智能(AI)专家对于人工通用智能(AGI)的决策所需技能并不了解的问题。 虽然当前的机器可以模拟特定的人类属性，但 AGI 是在这样的前提下开发出来的：这可以很容易地扩展到一般智能水平。这会分散当前研究的注意力，远离相关问题。

    

    即使是最尖端的人工通用智能(AGI)项目，人与机器之间的差异也十分明显。尽管这种差异本质上将每个人的能力划分开来，但人类级别的智能(HLI)已经是AGI几十年来的目标。本文反对图灵测试的二元论，即将其作为潜在智能机器的基础和原始建立的意图。它讨论了AI专家如何误解模仿游戏作为对计算机系统进行拟人化的手段，并断言HLI是一个转移注意力、使当前研究远离相关问题的错误方向。尽管对AGI应用的潜在设计进行了广泛研究，但却很少考虑这样一个系统如何以类似人类的水平访问和摄取数据。尽管当前的机器可能模拟特定的人类属性，但AGI是在这样的前提下开发出来的：这可以很容易地扩展到一般智能水平。

    Even in the most cutting-edge Artificial General Intelligence (AGI) endeavors, the disparity between humans and artificial systems is extremely apparent. Although this difference fundamentally divides the capabilities of each, human-level intelligence (HLI) has remained the aim of AGI for decades. This paper opposes the binarity of the Turing Test, the foundation of this intention and original establishment of a potentially intelligent machine. It discusses how AI experts misinterpreted the Imitation Game as a means to anthropomorphize computer systems and asserts that HLI is a red herring that distracts current research from relevant problems. Despite the extensive research on the potential design of an AGI application, there has been little consideration of how such a system will access and ingest data at a human-like level. Although current machines may emulate specific human attributes, AGI is developed under the pretense that this can be easily scaled up to a general intelligence 
    
[^43]: 今天的迭代学习算法有多高效？

    How Efficient Are Today's Continual Learning Algorithms?. (arXiv:2303.18171v1 [cs.CV])

    [http://arxiv.org/abs/2303.18171](http://arxiv.org/abs/2303.18171)

    这篇论文研究了增量班级学习的最新方法，并指出许多方法在计算、内存和存储方面非常低效。为了使迭代学习在现实世界中具有适用性，研究界不能忽视这些算法使用的资源。

    

    监督式迭代学习涉及从不断增长的带标签数据流中更新深度神经网络（DNN）。尽管大部分工作集中在克服灾难性遗忘上，但迭代学习背后的主要动机之一是能够有效地更新网络，而不是随着训练数据集随时间增长，从头开始重新训练。尽管最近的迭代学习方法基本上解决了灾难遗忘问题，但对这些算法的效率关注不足。在这里，我们研究了增量班级学习的最新方法，并表明许多方法在计算、内存和存储方面非常低效。有些方法甚至需要更多的计算资源才能完成训练！我们认为，为了使迭代学习在现实世界中具有适用性，研究界不能忽视这些算法使用的资源。迭代学习不仅仅是缓解灾难性遗忘。

    Supervised Continual learning involves updating a deep neural network (DNN) from an ever-growing stream of labeled data. While most work has focused on overcoming catastrophic forgetting, one of the major motivations behind continual learning is being able to efficiently update a network with new information, rather than retraining from scratch on the training dataset as it grows over time. Despite recent continual learning methods largely solving the catastrophic forgetting problem, there has been little attention paid to the efficiency of these algorithms. Here, we study recent methods for incremental class learning and illustrate that many are highly inefficient in terms of compute, memory, and storage. Some methods even require more compute than training from scratch! We argue that for continual learning to have real-world applicability, the research community cannot ignore the resources used by these algorithms. There is more to continual learning than mitigating catastrophic forg
    
[^44]: 半弱监督下的物体运动动力学预测

    Semi-Weakly Supervised Object Kinematic Motion Prediction. (arXiv:2303.17774v1 [cs.CV])

    [http://arxiv.org/abs/2303.17774](http://arxiv.org/abs/2303.17774)

    本研究提出了一种半弱监督的方法，通过利用物体部件语义分割数据集和方法，解决了物体运动动力学预测问题。通过一个图神经网络，可以检测底层3D结构中的移动部分。

    

    给定一个3D物体，运动动力学预测旨在确定移动部件以及相应的运动参数。由于3D物体的拓扑结构和几何细节的巨大变化，这仍然是一项具有挑战性的任务，缺乏大型标记数据也限制了基于深度学习的方法的表现。本文以半弱监督方式解决了物体运动动力学预测问题。我们的主要观察结果有两个。首先，尽管全面注释运动标签的3D数据集是有限的，但存在大规模的物体部件语义分割数据集和方法。其次，语义部分分割和移动部分分割并不总是一致的，但可以从底层3D结构中检测出移动部分。为此，我们提出了一个图神经网络来学习分层部件级别分割和移动部件参数之间的映射。

    Given a 3D object, kinematic motion prediction aims to identify the mobile parts as well as the corresponding motion parameters. Due to the large variations in both topological structure and geometric details of 3D objects, this remains a challenging task and the lack of large scale labeled data also constrain the performance of deep learning based approaches. In this paper, we tackle the task of object kinematic motion prediction problem in a semi-weakly supervised manner. Our key observations are two-fold. First, although 3D dataset with fully annotated motion labels is limited, there are existing datasets and methods for object part semantic segmentation at large scale. Second, semantic part segmentation and mobile part segmentation is not always consistent but it is possible to detect the mobile parts from the underlying 3D structure. Towards this end, we propose a graph neural network to learn the map between hierarchical part-level segmentation and mobile parts parameters, which 
    
[^45]: HuggingGPT: 在HugingFace中使用ChatGPT及其伙伴解决AI任务

    HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace. (arXiv:2303.17580v1 [cs.CL])

    [http://arxiv.org/abs/2303.17580](http://arxiv.org/abs/2303.17580)

    用ChatGPT作为任务规划工具，利用大型语言模型（LLM）作为控制器来整合现有的AI模型，解决复杂的AI任务。

    

    解决不同领域和模态的复杂AI任务是通向人工智能的关键步骤。本文提出了一个系统，利用大型语言模型（LLMs）作为控制器来管理现有的AI模型以解决AI任务，语言成为通用接口来赋能它。具体来说，我们使用ChatGPT作为任务规划工具，根据HuggingFace中可用的模型功能描述来选择模型，在选定AI模型的情况下执行每个子任务，并总结响应。

    Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence (AGI). While there are abundant AI models available for different domains and modalities, they cannot handle complicated AI tasks. Considering large language models (LLMs) have exhibited exceptional ability in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks and language could be a generic interface to empower this. Based on this philosophy, we present HuggingGPT, a system that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., HuggingFace) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in HuggingFace, execute each subtask with the selected AI model, and summarize the response acco
    
[^46]: 使用人工智能在家中测量帕金森病的严重程度

    Using AI to Measure Parkinson's Disease Severity at Home. (arXiv:2303.17573v1 [cs.LG])

    [http://arxiv.org/abs/2303.17573](http://arxiv.org/abs/2303.17573)

    该论文提出了一种使用人工智能系统远程评估帕金森病患者运动表现的方法，该方法可重复用于类似的运动任务，拥有较高的可靠性和准确性。

    

    我们提出了一种使用人工智能系统远程评估帕金森病患者运动表现的方法。参与者在网络摄像头前完成了运动任务（即点击手指），250名全球参与者的数据按照运动障碍协会统一帕金森病评分量表 (MDS-UPDRS) 的标准由三名专家神经学家进行了评估。神经学家的评估具有高度的可靠性，内部一致性系数（ICC）为0.88。我们开发了计算机算法来获得与MDS-UPDRS指南一致且与神经学家的评估高度相关的客观测量结果。我们的机器学习模型在这些指标的训练下表现优于一个MDS-UPDRS认证的评分者，平均绝对误差（MAE）为0.59，而评分者的MAE为0.79。然而，该模型的表现略逊于专家神经学家（0.53 MAE）。该方法可重复用于类似的运动任务，提供了可能性。

    We present an artificial intelligence system to remotely assess the motor performance of individuals with Parkinson's disease (PD). Participants performed a motor task (i.e., tapping fingers) in front of a webcam, and data from 250 global participants were rated by three expert neurologists following the Movement Disorder Society Unified Parkinson's Disease Rating Scale (MDS-UPDRS). The neurologists' ratings were highly reliable, with an intra-class correlation coefficient (ICC) of 0.88. We developed computer algorithms to obtain objective measurements that align with the MDS-UPDRS guideline and are strongly correlated with the neurologists' ratings. Our machine learning model trained on these measures outperformed an MDS-UPDRS certified rater, with a mean absolute error (MAE) of 0.59 compared to the rater's MAE of 0.79. However, the model performed slightly worse than the expert neurologists (0.53 MAE). The methodology can be replicated for similar motor tasks, providing the possibili
    
[^47]: SynthVSR：使用合成监督实现可视语音识别的规模化提升

    SynthVSR: Scaling Up Visual Speech Recognition With Synthetic Supervision. (arXiv:2303.17200v1 [cs.CV])

    [http://arxiv.org/abs/2303.17200](http://arxiv.org/abs/2303.17200)

    本文首次探讨利用合成视觉数据进行可视语音识别（VSR）的潜力，提出的方法SynthVSR通过利用语音驱动的唇部动画模型合成大规模数据，极大地提高了VSR系统的性能。

    

    最近在可视语音识别（VSR）领域中报道的最新成果通常依赖于越来越多的视频数据，而公开可用的转录视频数据集大小有限。本文首次研究了利用合成视觉数据进行VSR的潜力。我们的方法“SynthVSR”通过合成嘴唇动作显著提高了VSR系统的性能。SynthVSR背后的关键思想是利用一个基于语音驱动的唇部动画模型，该模型根据输入语音生成唇部动作。该语音驱动的唇部动画模型是在未标记的音视频数据集上训练的，并且可以在有标记的视频可用时进一步优化为预训练的VSR模型。由于存在大量的转录声学数据和面部图像，我们能够使用所提出的唇部动画模型生成大规模的合成数据用于半监督VSR训练。我们在“Lip Reading in the Wild”（Lrw）评测基准上评估了我们方法的性能。

    Recently reported state-of-the-art results in visual speech recognition (VSR) often rely on increasingly large amounts of video data, while the publicly available transcribed video datasets are limited in size. In this paper, for the first time, we study the potential of leveraging synthetic visual data for VSR. Our method, termed SynthVSR, substantially improves the performance of VSR systems with synthetic lip movements. The key idea behind SynthVSR is to leverage a speech-driven lip animation model that generates lip movements conditioned on the input speech. The speech-driven lip animation model is trained on an unlabeled audio-visual dataset and could be further optimized towards a pre-trained VSR model when labeled videos are available. As plenty of transcribed acoustic data and face images are available, we are able to generate large-scale synthetic data using the proposed lip animation model for semi-supervised VSR training. We evaluate the performance of our approach on the la
    
[^48]: ChatGPT-4中显著概念物理推理的进展

    Advances in apparent conceptual physics reasoning in ChatGPT-4. (arXiv:2303.17012v1 [physics.ed-ph])

    [http://arxiv.org/abs/2303.17012](http://arxiv.org/abs/2303.17012)

    ChatGPT-4是一个基于大规模语言模型训练的对话机器人，能达到接近于物理专家水平的力概念测试成绩，对未来的物理教育和教学有重要意义。

    

    ChatGPT是建立在一个巨大的人类文本信息库上的大型语言模型，以模拟人类对话。最近Kortemeyer（2023）的研究表明，尽管没有任何关于物理定律的明确编程指导，ChatGPT-3.5可以通过一些名义水平的入门物理课程，并在力学的力概念测试中得到接近最小理解的成绩。本研究复制了这些结果，并证明了最新版本ChatGPT-4在该环境下的成绩远高于前版本，在一些非常值得注意的例外和限制条件下，其回答非常接近于完美地展示专家水平的能力。我们简要评述了这对于未来物理教育和教学的含义。

    ChatGPT is built on a large language model trained on an enormous corpus of human text to emulate human conversation. Despite lacking any explicit programming regarding the laws of physics, recent work by Kortemeyer (2023) has demonstrated that ChatGPT-3.5 could pass an introductory physics course at some nominal level and register something close to a minimal understanding of Newtonian Mechanics on the Force Concept Inventory. This work replicates those results and also demonstrates that the latest version, ChatGPT-4, has reached a much higher mark in the latter context. Indeed, its responses come quite close to perfectly demonstrating expert-level competence, with a few very notable exceptions and limitations. We briefly comment on the implications of this for the future of physics education and pedagogy.
    
[^49]: 无限时间视角下利用部分观测进行最坏情况控制与学习

    Worst-Case Control and Learning Using Partial Observations Over an Infinite Time-Horizon. (arXiv:2303.16321v1 [math.OC])

    [http://arxiv.org/abs/2303.16321](http://arxiv.org/abs/2303.16321)

    本文提出了无限时间视角下部分观测系统的最坏情况控制和学习的框架，能够利用未知概率分布的有限值不确定变量，并能够通过定义信息状态来提高动态规划的计算可处理性，同时提出了可从观测数据中构建或学习的近似信息状态。

    

    安全关键的网络物理系统需要良好的控制策略来应对敌对干扰和建模不确定性。本文提出了一个框架，利用部分观测系统进行近似控制和学习，以最小化无限时间视角下的最坏情况贴现成本。我们将对系统的干扰建模为具有未知概率分布的有限值不确定变量。对于已知系统动力学的问题，我们构建了一个动态规划（DP）分解来计算最优控制策略。我们的第一贡献是定义信息状态，提高了DP的计算可处理性，而不损失最优性。然后，我们描述了一类在每个时间点产生可观测成本的问题的简化。我们的第二个贡献是定义了可以从观测数据中直接构建或学习的近似信息状态。

    Safety-critical cyber-physical systems require control strategies whose worst-case performance is robust against adversarial disturbances and modeling uncertainties. In this paper, we present a framework for approximate control and learning in partially observed systems to minimize the worst-case discounted cost over an infinite time-horizon. We model disturbances to the system as finite-valued uncertain variables with unknown probability distributions. For problems with known system dynamics, we construct a dynamic programming (DP) decomposition to compute the optimal control strategy. Our first contribution is to define information states that improve the computational tractability of this DP without loss of optimality. Then, we describe a simplification for a class of problems where the incurred cost is observable at each time-instance. Our second contribution is a definition of approximate information states that can be constructed or learned directly from observed data for problem
    
[^50]: Dice半度量损失函数：用软标签优化Dice分数

    Dice Semimetric Losses: Optimizing the Dice Score with Soft Labels. (arXiv:2303.16296v1 [cs.CV])

    [http://arxiv.org/abs/2303.16296](http://arxiv.org/abs/2303.16296)

    本文提出的Dice半度量损失函数可在软标签设置中使用，在医疗成像领域的分割方案中与使用软标签的研究相结合，可以获得更好的Dice分数和模型校准。

    

    在医学成像领域的许多自动分割方案中，软Dice损失（SDL）发挥了关键作用。在过去几年中，人们已经揭示了其优越性能背后的一些原因并进一步探索了其优化。然而，目前还没有实现支持直接在软标签设置中使用它的方案。因此，在使用SDL和研究利用软标签的同时进行模型校准的协同作用仍然缺失。在本文中，我们介绍了Dice半度量损失函数（DMLs），它们（i）在硬标签的标准设置下与SDL相同，但（ii）也可在软标签设置中使用。我们在公共的QUBIQ、LiTS和KiTS基准测试上的实验证实了DMLs与软标签（如平均、标签平滑和知识蒸馏）的潜在协同作用，而DMLs与硬标签（如大多数投票和随机选择）相比，产生了更优秀的Dice分数和模型校准。

    The soft Dice loss (SDL) has taken a pivotal role in many automated segmentation pipelines in the medical imaging community. Over the last years, some reasons behind its superior functioning have been uncovered and further optimizations have been explored. However, there is currently no implementation that supports its direct use in settings with soft labels. Hence, a synergy between the use of SDL and research leveraging the use of soft labels, also in the context of model calibration, is still missing. In this work, we introduce Dice semimetric losses (DMLs), which (i) are by design identical to SDL in a standard setting with hard labels, but (ii) can be used in settings with soft labels. Our experiments on the public QUBIQ, LiTS and KiTS benchmarks confirm the potential synergy of DMLs with soft labels (e.g. averaging, label smoothing, and knowledge distillation) over hard labels (e.g. majority voting and random selection). As a result, we obtain superior Dice scores and model calib
    
[^51]: TraffNet：学习道路网络数字孪生交通生成因果关系

    TraffNet: Learning Causality of Traffic Generation for Road Network Digital Twins. (arXiv:2303.15954v1 [cs.LG])

    [http://arxiv.org/abs/2303.15954](http://arxiv.org/abs/2303.15954)

    TraffNet是一个学习交通量生成原因的深度学习框架，将车辆轨迹数据表示为异构图，利用递归神经网络结构实现了对交通生成原因的预测。

    

    道路网络数字孪生（RNDT）在开发下一代智能交通系统中发挥着关键作用，可以实现更精确的交通规划和控制。为了支持实时决策，RNDT需要一个模型，从在线传感器数据中动态学习交通模式并生成高保真模拟结果。尽管基于图神经网络的当前交通预测技术已经实现了最先进的性能，但是这些技术仅通过挖掘历史交通数据中的相关性来预测未来交通，而忽略了交通生成的原因，例如交通需求和路径选择。因此，它们的性能对于实时决策是不可靠的。为了填补这一差距，我们引入了一个新的深度学习框架称为 TraffNet，该框架从车辆轨迹数据中学习交通量的因果性。首先，我们使用异构图来表示道路网络，使模型能够并入预测所需的其他数据，然后我们提出了一种新颖的递归神经网络结构，从而能够预测交通量的因果联系。

    Road network digital twins (RNDTs) play a critical role in the development of next-generation intelligent transportation systems, enabling more precise traffic planning and control. To support just-in-time (JIT) decision making, RNDTs require a model that dynamically learns the traffic patterns from online sensor data and generates high-fidelity simulation results. Although current traffic prediction techniques based on graph neural networks have achieved state-of-the-art performance, these techniques only predict future traffic by mining correlations in historical traffic data, disregarding the causes of traffic generation, such as traffic demands and route selection. Therefore, their performance is unreliable for JIT decision making. To fill this gap, we introduce a novel deep learning framework called TraffNet that learns the causality of traffic volume from vehicle trajectory data. First, we use a heterogeneous graph to represent the road network, allowing the model to incorporate 
    
[^52]: 通过反向特征投影在不断学习中维护线性可分性

    Preserving Linear Separability in Continual Learning by Backward Feature Projection. (arXiv:2303.14595v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.14595](http://arxiv.org/abs/2303.14595)

    提出了一种名为BFP的连续学习方法，它通过将新特征变换为旧特征的线性变换来维护线性可分性，从而允许新特征方向的出现以适应新任务，同时保留旧任务的信息。

    

    在不断学习中，灾难性遗忘一直是一个重大挑战，因为模型需要在有限或没有以前查看任务的数据情况下学习新任务。为了解决这个挑战，基于特征空间知识蒸馏的方法已被提出并证明可以减少遗忘。然而，大多数特征蒸馏方法直接约束新特征以匹配旧特征，忽视了可塑性的需求。为了实现更好的稳定性-可塑性平衡，我们提出了Backward Feature Projection（BFP），这是一种连续学习方法，允许新特征在旧特征的可学习线性变换中发生变化。BFP保留旧类别的线性可分性，同时允许新的特征方向出现以适应新的类别。BFP可以与现有的经验重播方法集成，并显著提高性能。我们还证明，BFP有助于学习更好的表示空间。

    Catastrophic forgetting has been a major challenge in continual learning, where the model needs to learn new tasks with limited or no access to data from previously seen tasks. To tackle this challenge, methods based on knowledge distillation in feature space have been proposed and shown to reduce forgetting. However, most feature distillation methods directly constrain the new features to match the old ones, overlooking the need for plasticity. To achieve a better stability-plasticity trade-off, we propose Backward Feature Projection (BFP), a method for continual learning that allows the new features to change up to a learnable linear transformation of the old features. BFP preserves the linear separability of the old classes while allowing the emergence of new feature directions to accommodate new classes. BFP can be integrated with existing experience replay methods and boost performance by a significant margin. We also demonstrate that BFP helps learn a better representation space,
    
[^53]: 用启发式规划作为定理证明

    Planning as Theorem Proving with Heuristics. (arXiv:2303.13638v1 [cs.AI])

    [http://arxiv.org/abs/2303.13638](http://arxiv.org/abs/2303.13638)

    该论文介绍了一种用启发式规划作为定理证明的方法，通过开发一种定理证明提升启发式(TPLH)规划器，在情况树中搜索短的计划，并减少探讨的状态数量。

    

    在情境演算中将计划作为定理证明在50年前被放弃，因为这是一个不可能完成的项目。但是我们开发了一种定理证明提升启发式(TPLH)规划器，它使用A*搜索算法在情况树中搜索计划。它由基于删除松弛的与领域无关的启发式控制。我们将TPLH与Fast Downward（FD）和Best First Width Search（BFWS）规划器在几个标准基准测试中进行比较。由于我们的启发式功能实现未经优化，TPLH比FD和BFWS慢。但它会计算出更短的计划，并减少了探讨的状态数量。我们讨论了以前在知识表示和推理领域内进行规划的研究，并确定了相关方向。因此，我们表明情境演算中的演绎式提升启发式规划实际上是可以完成的。

    Planning as theorem proving in situation calculus was abandoned 50 years ago as an impossible project. But we have developed a Theorem Proving Lifted Heuristic (TPLH) planner that searches for a plan in a tree of situations using the A* search algorithm. It is controlled by a delete relaxation-based domain independent heuristic. We compare TPLH with Fast Downward (FD) and Best First Width Search (BFWS) planners over several standard benchmarks. Since our implementation of the heuristic function is not optimized, TPLH is slower than FD and BFWS. But it computes shorter plans, and it explores fewer states. We discuss previous research on planning within KR\&R and identify related directions. Thus, we show that deductive lifted heuristic planning in situation calculus is actually doable.
    
[^54]: 语音合成的音频扩散模型：基于生成AI的文本到语音和语音增强的概述

    Audio Diffusion Model for Speech Synthesis: A Survey on Text To Speech and Speech Enhancement in Generative AI. (arXiv:2303.13336v1 [cs.SD])

    [http://arxiv.org/abs/2303.13336](http://arxiv.org/abs/2303.13336)

    此文介绍了音频扩散模型，重点讨论了两个活跃任务：文本到语音和语音增强，并对实验结果进行了比较和讨论。

    

    生成AI在各个领域表现出了惊人的性能，其中语音合成是一个有趣的方向。随着扩散模型成为最流行的生成模型，许多工作已经尝试了两个活跃任务：文本到语音和语音增强。本文对音频扩散模型进行了概述，这是对现有调查的补充，这些调查要么缺乏基于扩散的语音合成的最新进展，要么强调在多个领域应用扩散模型的整体情况。具体而言，本文首先简要介绍了音频和扩散模型的背景。对于文本到语音任务，我们将方法分为三类，基于扩散模型采用的阶段：声学模型、声码器和端到端框架。此外，我们通过将某些信号从输入语音中删除或添加来将各种语音增强任务进行分类。本文还涵盖了实验结果的比较和讨论。

    Generative AI has demonstrated impressive performance in various fields, among which speech synthesis is an interesting direction. With the diffusion model as the most popular generative model, numerous works have attempted two active tasks: text to speech and speech enhancement. This work conducts a survey on audio diffusion model, which is complementary to existing surveys that either lack the recent progress of diffusion-based speech synthesis or highlight an overall picture of applying diffusion model in multiple fields. Specifically, this work first briefly introduces the background of audio and diffusion model. As for the text-to-speech task, we divide the methods into three categories based on the stage where diffusion model is adopted: acoustic model, vocoder and end-to-end framework. Moreover, we categorize various speech enhancement tasks by either certain signals are removed or added into the input speech. Comparisons of experimental results and discussions are also covered 
    
[^55]: RepoCoder：通过迭代检索和生成实现的代码存储库级别完成

    RepoCoder: Repository-Level Code Completion Through Iterative Retrieval and Generation. (arXiv:2303.12570v1 [cs.CL])

    [http://arxiv.org/abs/2303.12570](http://arxiv.org/abs/2303.12570)

    RepoCoder是一个简单、通用和有效的框架，结合了一个基于相似性的检索器和预训练的代码语言模型，在库级别代码完成流程中实现了有效利用库级别信息进行代码完成和生成。

    

    库级别代码完成任务是基于代码库更广阔上下文中继续编写未完成代码的过程。但是对于自动完成工具而言，很难利用散布在不同文件中的有用信息。我们提出了RepoCoder，这是一个简单、通用和有效的框架，可以应对这一挑战。它通过整合基于相似性的检索器和预训练的代码语言模型简化了库级别代码完成流程，从而允许有效利用库级别信息进行代码完成，并具有不同粒度层面的代码生成能力。此外，RepoCoder 还使用了一种新的迭代检索-生成模型，弥合了检索上下文和预期完成目标之间的差距。我们还提出了一个新的RepoEval基准测试，其中包含了最新和高质量真实世界的代码库，涵盖了行、API 调用和函数体完成场景。

    The task of repository-level code completion is to continue writing the unfinished code based on a broader context of the repository. While for automated code completion tools, it is difficult to utilize the useful information scattered in different files. We propose RepoCoder, a simple, generic, and effective framework to address the challenge. It streamlines the repository-level code completion process by incorporating a similarity-based retriever and a pre-trained code language model, which allows for the effective utilization of repository-level information for code completion and grants the ability to generate code at various levels of granularity. Furthermore, RepoCoder utilizes a novel iterative retrieval-generation paradigm that bridges the gap between retrieval context and the intended completion target. We also propose a new benchmark RepoEval, which consists of the latest and high-quality real-world repositories covering line, API invocation, and function body completion sce
    
[^56]: 分布式编码结构的低复杂度深度视频压缩

    Low-complexity Deep Video Compression with A Distributed Coding Architecture. (arXiv:2303.11599v1 [eess.IV])

    [http://arxiv.org/abs/2303.11599](http://arxiv.org/abs/2303.11599)

    本论文提出了一种低复杂度深度视频压缩的分布式编码结构，通过使用一个有效的译码器辅助信息生成模块，实现在设备资源受限的情况下仍能够有效地利用视频帧间的相关性进行高效压缩。

    

    目前的预测编码视频压缩方法依靠复杂的编码器降低时域冗余，导致在资源受限设备上部署变得具有挑战性。传统的分布式编码方法存在性能差距。我们提出了第一个端对端分布式深度视频压缩框架，其中包含一种有效的SI生成模块，有助于在没有计算密集型编码器侧运动估计的情况下有效地利用帧间相关性，从而提高速率失真性能。

    Prevalent predictive coding-based video compression methods rely on a heavy encoder to reduce the temporal redundancy, which makes it challenging to deploy them on resource-constrained devices. Meanwhile, as early as the 1970s, distributed source coding theory has indicated that independent encoding and joint decoding with side information (SI) can achieve high-efficient compression of correlated sources. This has inspired a distributed coding architecture aiming at reducing the encoding complexity. However, traditional distributed coding methods suffer from a substantial performance gap to predictive coding ones. Inspired by the great success of learning-based compression, we propose the first end-to-end distributed deep video compression framework to improve the rate-distortion performance. A key ingredient is an effective SI generation module at the decoder, which helps to effectively exploit inter-frame correlations without computation-intensive encoder-side motion estimation and c
    
[^57]: 数据中心人工智能综述：一份调查报告。

    Data-centric Artificial Intelligence: A Survey. (arXiv:2303.10158v1 [cs.LG])

    [http://arxiv.org/abs/2303.10158](http://arxiv.org/abs/2303.10158)

    本文讨论了数据中心人工智能的必要性并从三个一般性数据中心目标和代表性方法的全面视角进行了介绍。该综述从自动化和协作的角度组织了现有文献并讨论了挑战。

    

    人工智能（AI）正在几乎所有领域产生深远的影响，其成功的关键之一是可用于构建机器学习模型的丰富高质量数据。最近，数据在AI中的作用得到了显著放大，引发了数据中心AI这一新兴概念的出现。研究人员和从业者的注意力逐渐从推进模型设计转向提高数据质量和数量。在本调查中，我们讨论了数据中心AI的必要性，随后从训练数据开发、推理数据开发和数据维护三个一般性数据中心目标以及代表性方法的全面视角进行了介绍。我们还从自动化和协作的角度组织了现有文献，讨论了挑战，并列出了各种任务的测试基准。我们认为，这是第一份提供跨越各个阶段一系列任务的全球视角的综合性调查。

    Artificial Intelligence (AI) is making a profound impact in almost every domain. A vital enabler of its great success is the availability of abundant and high-quality data for building machine learning models. Recently, the role of data in AI has been significantly magnified, giving rise to the emerging concept of data-centric AI. The attention of researchers and practitioners has gradually shifted from advancing model design to enhancing the quality and quantity of the data. In this survey, we discuss the necessity of data-centric AI, followed by a holistic view of three general data-centric goals (training data development, inference data development, and data maintenance) and the representative methods. We also organize the existing literature from automation and collaboration perspectives, discuss the challenges, and tabulate the benchmarks for various tasks. We believe this is the first comprehensive survey that provides a global view of a spectrum of tasks across various stages o
    
[^58]: 一人独舞好还是人多闹心？不同演示次数下的上下文训练

    It Takes One to Tango but More Make Trouble? In-Context Training with Different Number of Demonstrations. (arXiv:2303.08119v1 [cs.AI])

    [http://arxiv.org/abs/2303.08119](http://arxiv.org/abs/2303.08119)

    本文研究了使用较少的演示数进行上下文学习（ICL）的任务，在测试查询上只使用一个随机选择的演示时并没有明显性能下降，而只使用一个正确演示的ICL在性能上显著优于全演示ICL。

    

    大型语言模型在提供了一些输入输出演示（demos）并给出更多演示的中间推理步骤（“思路链（CoT）”）时，能够通过上下文学习（ICL）进行复杂推理。本文研究了在每个测试查询上使用较少的演示来进行ICL的任务~\cite{wei2022chain}。惊人地，当只使用一个随机选择的演示时，我们并没有观察到明显的性能下降。为了研究这种现象，对于每个测试查询，我们将演示分类为“正确演示”和“错误演示”。我们的分析揭示了这些广泛研究的数据集中存在的固有偏差：大多数测试查询的大多数演示都是正确的，这解释了使用一个随机演示时表现良好的原因。此外，只使用一个正确演示的ICL（带和不带CoT）在性能上显著优于大多数先前工作采用的全演示ICL，表明演示数量并不总是更好。

    Large language models (LLMs) are capable to perform complex reasoning by in-context learning (ICL) when provided with a few input-output demonstrations (demos) and more powerful when intermediate reasoning steps ("chain of thoughts (CoT)") of the demos are given. Is it necessary to use multi-demo in ICL? In this paper, we study ICL using fewer demos for each test query on the tasks in~\cite{wei2022chain}. Surprisingly, we do not observe significant degradation when using only one randomly chosen demo. To study this phenomenon, for each test query, we categorize demos into "correct demos" leading to the correct answer, and "wrong demos" resulting in wrong answers. Our analysis reveals an inherent bias in those widely studied datasets: most demos are correct for a majority of test queries, which explains the good performance of using one random demo. Moreover, ICL (with and w/o CoT) using only one correct demo significantly outperforms all-demo ICL adopted by most previous works, indicat
    
[^59]: 生成AI中的文本到图像扩散模型：一项调查

    Text-to-image Diffusion Model in Generative AI: A Survey. (arXiv:2303.07909v1 [cs.CV])

    [http://arxiv.org/abs/2303.07909](http://arxiv.org/abs/2303.07909)

    本文调查了文本到图像扩散模型以及相关应用，总结了最先进的方法，并探讨了挑战和未来方向。

    

    本文调查了文本到图像扩散模型，这些模型已经成为多种生成任务中流行的模型。作为一个自包含的工作，本调查从简单介绍基本扩散模型如何用于图像合成开始，接着是条件或引导如何改进学习。我们还总结了文本条件下的最先进的图像合成方法，并且进一步总结了文本引导创意生成和图像编辑的应用。除了迄今为止所取得的进展，我们还讨论了现有挑战和有前途的未来方向。

    This survey reviews text-to-image diffusion models in the context that diffusion models have emerged to be popular for a wide range of generative tasks. As a self-contained work, this survey starts with a brief introduction of how a basic diffusion model works for image synthesis, followed by how condition or guidance improves learning. Based on that, we present a review of state-of-the-art methods on text-conditioned image synthesis, i.e., text-to-image. We further summarize applications beyond text-to-image generation: text-guided creative generation and text-guided image editing. Beyond the progress made so far, we discuss existing challenges and promising future directions.
    
[^60]: 带有双向先验模型的向量量化时间序列生成

    Vector Quantized Time Series Generation with a Bidirectional Prior Model. (arXiv:2303.04743v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.04743](http://arxiv.org/abs/2303.04743)

    本文提出了一种新的时间序列生成方法，使用向量量化技术和双向变压器模型来生成质量更好、模块化变化更快的合成信号。

    

    时间序列生成研究主要集中在使用生成对抗网络（GAN）与递归神经网络（RNN）变体相结合。然而，训练 GAN 的基本限制和挑战仍然存在。此外，RNN族通常在远程时间步之间的时间一致性方面存在困难。受到图像生成领域成功的启发，我们提出 TimeVQVAE，这是我们所知道的第一个使用向量量化（VQ）技术解决 TSG 问题的工作。此外，离散潜在空间的先验使用双向变压器模型进行学习，可以更好地捕捉全局时间一致性。我们还提出在时间 - 频率域中进行 VQ 建模，分为低频（LF）和高频（HF）。这使我们能够保留时间序列的重要特征，并生成质量更好、模块性变化更快的新合成信号。

    Time series generation (TSG) studies have mainly focused on the use of Generative Adversarial Networks (GANs) combined with recurrent neural network (RNN) variants. However, the fundamental limitations and challenges of training GANs still remain. In addition, the RNN-family typically has difficulties with temporal consistency between distant timesteps. Motivated by the successes in the image generation (IMG) domain, we propose TimeVQVAE, the first work, to our knowledge, that uses vector quantization (VQ) techniques to address the TSG problem. Moreover, the priors of the discrete latent spaces are learned with bidirectional transformer models that can better capture global temporal consistency. We also propose VQ modeling in a time-frequency domain, separated into low-frequency (LF) and high-frequency (HF). This allows us to retain important characteristics of the time series and, in turn, generate new synthetic signals that are of better quality, with sharper changes in modularity, t
    
[^61]: Bootstrap The Original Latent: 从黑盒模型学习私有模型

    Bootstrap The Original Latent: Learning a Private Model from a Black-box Model. (arXiv:2303.03709v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2303.03709](http://arxiv.org/abs/2303.03709)

    本文提出了BPBA设置和BTOL训练策略两个新的方法，分别用于用户训练私有模型和利用基础/源模型。在三个不同的数据集上的实验表明，这些方法都是有效且稳健的。

    

    本文提出了一个称为Back-Propagated Black-Box Adaptation（BPBA）的新设置，以平衡模型所有者的数据/模型隐私和用户需求，为用户提供指导。该设置可以简化基础/源模型的使用，防止基础/源模型的泄漏和误用。此外，我们还提出了一种称为Bootstrap The Original Latent（BTOL）的新训练策略，以充分利用基础/源模型。我们的策略由领域适配器和冷冻-解冻策略组成。

    In this paper, considering the balance of data/model privacy of model owners and user needs, we propose a new setting called Back-Propagated Black-Box Adaptation (BPBA) for users to better train their private models via the guidance of the back-propagated results of a Black-box foundation/source model. Our setting can ease the usage of foundation/source models as well as prevent the leakage and misuse of foundation/source models. Moreover, we also propose a new training strategy called Bootstrap The Original Latent (BTOL) to fully utilize the foundation/source models. Our strategy consists of a domain adapter and a freeze-and-thaw strategy. We apply our BTOL under BPBA and Black-box UDA settings on three different datasets. Experiments show that our strategy is efficient and robust in various settings without manual augmentations.
    
[^62]: 追求实用可持续性深度神经网络训练的低碳电力

    Chasing Low-Carbon Electricity for Practical and Sustainable DNN Training. (arXiv:2303.02508v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.02508](http://arxiv.org/abs/2303.02508)

    本文提出了一种降低深度神经网络训练的碳足迹的实用解决方案，通过在训练过程中控制GPU的能耗来降低碳排放，同时还提出了一种预测方法，可以预测未来的碳强度，对各种DNN应用程序适用，无需额外硬件或基础设施。

    

    近年来，深度学习取得了很大的发展，由于使用GPU进行训练，导致能耗和碳排放量增加。传统的解决方案尝试将训练工作移动到碳强度较低的位置或时间框架，以回应可持续性的呼吁。然而，由于数据集大小或数据法规等原因，将工作移动到其他地方并不总是可行的。此外，推迟训练可能会对应用服务质量产生负面影响，因为支持服务的DNN没有得到及时更新。在本研究中，我们提出了一种实用的解决方案，可以降低DNN训练的碳足迹，而无需迁移或推迟工作。特别地，我们的解决方案在训练过程中观察实时的碳强度变化并控制GPU的能耗，从而在保持训练性能的同时降低碳足迹。此外，为了主动适应不断变化的电网条件，我们提出了一种预测方法来预测未来的碳强度，从而实现更有效的碳减排。我们的方法适用于各种DNN应用程序，无需额外的硬件或基础设施。图像分类任务的实验结果表明，我们的方法可以将碳强度降低高达44%，而不会牺牲训练性能。

    Deep learning has experienced significant growth in recent years, resulting in increased energy consumption and carbon emission from the use of GPUs for training deep neural networks (DNNs). Answering the call for sustainability, conventional solutions have attempted to move training jobs to locations or time frames with lower carbon intensity. However, moving jobs to other locations may not always be feasible due to large dataset sizes or data regulations. Moreover, postponing training can negatively impact application service quality because the DNNs backing the service are not updated in a timely fashion. In this work, we present a practical solution that reduces the carbon footprint of DNN training without migrating or postponing jobs. Specifically, our solution observes real-time carbon intensity shifts during training and controls the energy consumption of GPUs, thereby reducing carbon footprint while maintaining training performance. Furthermore, in order to proactively adapt to
    
[^63]: Fairguard: 在智慧城市中利用基于逻辑的公正规则

    Fairguard: Harness Logic-based Fairness Rules in Smart Cities. (arXiv:2302.11137v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2302.11137](http://arxiv.org/abs/2302.11137)

    本文提出了一种基于时间逻辑的公正智慧城市政策调整和生成方法Fairguard，通过两个阶段的静态生成和动态调节，缓解由多种数据和算法偏见导致的不公正预测结果。

    

    智慧城市运行在计算预测框架上，收集、整合和利用大规模传感器网络的数据。然而，这些框架容易受到多种数据和算法偏见的影响，这经常导致不公正的预测结果。本文首先通过研究田纳西州查塔努加的真实城市数据，展示了偏见在微观层面上在时间和空间上仍然存在。为了缓解这种偏见问题，我们引入了Fairguard，这是一种基于微观层面时间逻辑的方法，用于在复杂的时间空间域中进行公正的智慧城市政策调整和生成。Fairguard框架由两个阶段组成：首先，我们开发了一个静态生成器，能够通过最小化所选属性之间的相关性，基于时间逻辑条件来减少数据偏见。然后，为了确保预测算法的公正性，我们设计了一个动态组件来调节预测结果，并利用逻辑规则生成未来的公正预测。

    Smart cities operate on computational predictive frameworks that collect, aggregate, and utilize data from large-scale sensor networks. However, these frameworks are prone to multiple sources of data and algorithmic bias, which often lead to unfair prediction results. In this work, we first demonstrate that bias persists at a micro-level both temporally and spatially by studying real city data from Chattanooga, TN. To alleviate the issue of such bias, we introduce Fairguard, a micro-level temporal logic-based approach for fair smart city policy adjustment and generation in complex temporal-spatial domains. The Fairguard framework consists of two phases: first, we develop a static generator that is able to reduce data bias based on temporal logic conditions by minimizing correlations between selected attributes. Then, to ensure fairness in predictive algorithms, we design a dynamic component to regulate prediction results and generate future fair predictions by harnessing logic rules. E
    
[^64]: 面向可持续性的即时拼车的未来感知定价和匹配

    Future Aware Pricing and Matching for Sustainable On-demand Ride Pooling. (arXiv:2302.10510v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2302.10510](http://arxiv.org/abs/2302.10510)

    本论文提出了一个新的框架，同时处理定价和匹配问题，并考虑商业决策对未来的影响，实验结果表明该框架可以显著提高拼车的效率和效益。

    

    即时拼车的受欢迎程度在于为顾客（更低的价格）、出租车司机（更高的收入）、环境（由于更少的车辆而减少碳排放量）和Uber等聚合公司（更高的收入）提供了好处。为了实现这些收益，必须有效地解决两个关键而相互关联的挑战：（a）定价——为出租车顾客请求设置价格；和（b）匹配——将接受价格的客户分配给出租车/汽车。传统上，这两个挑战都是单独研究的，并且使用短视的方法（只考虑当前请求），而不考虑当前匹配对解决未来请求的影响。在本文中，我们开发了一个新的框架，处理定价和匹配问题，同时考虑定价和匹配决策对未来影响的影响。在真实出租车数据集上的实验结果表明，我们的框架可以显著改善匹配和定价效果。

    The popularity of on-demand ride pooling is owing to the benefits offered to customers (lower prices), taxi drivers (higher revenue), environment (lower carbon footprint due to fewer vehicles) and aggregation companies like Uber (higher revenue). To achieve these benefits, two key interlinked challenges have to be solved effectively: (a) pricing -- setting prices to customer requests for taxis; and (b) matching -- assignment of customers (that accepted the prices) to taxis/cars. Traditionally, both these challenges have been studied individually and using myopic approaches (considering only current requests), without considering the impact of current matching on addressing future requests. In this paper, we develop a novel framework that handles the pricing and matching problems together, while also considering the future impact of the pricing and matching decisions. In our experimental results on a real-world taxi dataset, we demonstrate that our framework can significantly improve re
    
[^65]: 关于深度神经网络功能耦合水印的研究

    On Function-Coupled Watermarks for Deep Neural Networks. (arXiv:2302.10296v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.10296](http://arxiv.org/abs/2302.10296)

    本文提出了一种新颖的深度神经网络水印方案，它可以有效地防御模型微调和修剪等水印删除攻击；通过增强水印和模型功能的耦合，我们的方法可以确保删除水印不可避免地会降低模型在常规输入上的性能。

    

    良好表现的深度神经网络通常需要海量标记数据和计算资源进行训练。为了保护这些知识产权，提出了各种水印技术，其中DNN提供商将秘密信息植入模型中，以便在稍后通过一些专用触发输入检索嵌入的水印索权；虽然文献中报告了有希望的结果，但现有解决方案仍然遭受水印删除攻击，例如模型微调和模型修剪。本文提出了一种新颖的DNN水印方案，可以有效地防御上述攻击。我们的关键洞察力是增强水印和模型功能的耦合，这样删除水印会不可避免地降低模型在常规输入上的性能。为此，与先前依赖于来自超出分布数据的秘密特征的方法不同，我们的方法仅使用从训练数据中学习的特征。

    Well-performed deep neural networks (DNNs) generally require massive labelled data and computational resources for training. Various watermarking techniques are proposed to protect such intellectual properties (IPs), wherein the DNN providers implant secret information into the model so that they can later claim IP ownership by retrieving their embedded watermarks with some dedicated trigger inputs. While promising results are reported in the literature, existing solutions suffer from watermark removal attacks, such as model fine-tuning and model pruning.  In this paper, we propose a novel DNN watermarking solution that can effectively defend against the above attacks. Our key insight is to enhance the coupling of the watermark and model functionalities such that removing the watermark would inevitably degrade the model's performance on normal inputs. To this end, unlike previous methods relying on secret features learnt from out-of-distribution data, our method only uses features lear
    
[^66]: 几何深度学习仅依靠距离矩阵足够吗？

    Is Distance Matrix Enough for Geometric Deep Learning?. (arXiv:2302.05743v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.05743](http://arxiv.org/abs/2302.05743)

    本文证明了消息传递神经网络（MPNNs）不能学习几何信息，提出了$k$-DisGNNs可以利用距离矩阵中的信息，并建立了几何深度学习和传统图表示学习之间的联系。

    

    图神经网络（GNN）常用于涉及图形几何的任务，例如分子动力学模拟。虽然几何图的距离矩阵包含完整的几何信息，但已经证明消息传递神经网络（MPNNs）无法学习这种几何信息。本文通过构造新颖的对称几何图的家族，扩展了MPNN无法区分其距离矩阵的反例家族，并提出$k$-DisGNNs，可以有效地利用距离矩阵中丰富的几何结构。我们证明了模型的高表达能力，并证明了一些现有的精心设计的几何模型可以作为$k$-DisGNNs的特殊情况统一起来。最重要的是，我们建立了几何深度学习和传统图表示学习之间的联系，展示了那些最初为低度表达能力的GNN模型设计的高度表达力的GNN模型。

    Graph Neural Networks (GNNs) are often used for tasks involving the geometry of a given graph, such as molecular dynamics simulation. Although the distance matrix of a geometric graph contains complete geometric information, it has been demonstrated that Message Passing Neural Networks (MPNNs) are insufficient for learning this geometry. In this work, we expand on the families of counterexamples that MPNNs are unable to distinguish from their distance matrices, by constructing families of novel and symmetric geometric graphs. We then propose $k$-DisGNNs, which can effectively exploit the rich geometry contained in the distance matrix. We demonstrate the high expressive power of our models and prove that some existing well-designed geometric models can be unified by $k$-DisGNNs as special cases. Most importantly, we establish a connection between geometric deep learning and traditional graph representation learning, showing that those highly expressive GNN models originally designed for
    
[^67]: Jaccard度量损失：使用软标签优化Jaccard指数

    Jaccard Metric Losses: Optimizing the Jaccard Index with Soft Labels. (arXiv:2302.05666v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.05666](http://arxiv.org/abs/2302.05666)

    本文提出了Jaccard度量损失（JMLs）来优化Jaccard指数，该损失在软标签下仍然有效。

    

    IoU损失是直接优化Jaccard指数的替代品。在语义分割中，将IoU损失作为损失函数的一部分，与仅优化像素损失（如交叉熵损失）相比，对于Jaccard指数测量表现更好。最显着的IoU损失是软Jaccard损失和Lovasz-Softmax损失。然而，这些损失与机器学习中普遍存在的软标签不兼容。在本文中，我们提出了Jaccard度量损失（JMLs），它们在标准设置下与软标签兼容，与软Jaccard损失相同。使用JMLs，我们研究了两种最流行的软标签用例：标签平滑和知识蒸馏。在三个语义分割数据集（Cityscapes、PASCAL VOC和DeepGlobe Land）上，我们的实验表明，与交叉熵损失相比，我们的简单方法显著提高了性能，并且在DeepGlobe Land数据集上超过了最先进的方法。

    IoU losses are surrogates that directly optimize the Jaccard index. In semantic segmentation, leveraging IoU losses as part of the loss function is shown to perform better with respect to the Jaccard index measure than optimizing pixel-wise losses such as the cross-entropy loss alone. The most notable IoU losses are the soft Jaccard loss and the Lovasz-Softmax loss. However, these losses are incompatible with soft labels which are ubiquitous in machine learning. In this paper, we propose Jaccard metric losses (JMLs), which are identical to the soft Jaccard loss in a standard setting with hard labels, but are compatible with soft labels. With JMLs, we study two of the most popular use cases of soft labels: label smoothing and knowledge distillation. With a variety of architectures, our experiments show significant improvements over the cross-entropy loss on three semantic segmentation datasets (Cityscapes, PASCAL VOC and DeepGlobe Land), and our simple approach outperforms state-of-the-
    
[^68]: 过去和未来之间：基于时空模型的多相机三维多目标跟踪

    Standing Between Past and Future: Spatio-Temporal Modeling for Multi-Camera 3D Multi-Object Tracking. (arXiv:2302.03802v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.03802](http://arxiv.org/abs/2302.03802)

    本文提出了一个基于时空模型的多相机三维多目标跟踪框架，命名为“过去和未来之间的跟踪”。该方法采用注意力跟踪框架，通过对象查询连续地表示跟踪实例，并整合了跟踪对象的前后推理，显著提高了跟踪准确性和ID-Switches的减少。

    

    本文提出了一种端到端的多相机三维多目标跟踪框架。该框架强调时空连续性，并整合了跟踪对象的前后推理。因此，我们将其命名为“基于过去和未来的跟踪”（PF-Track）。具体而言，我们的方法采用“注意力跟踪”框架，并通过对象查询连续地表示跟踪实例。为了明确使用历史线索，我们的“过去推理”模块学习精细化跟踪，并通过跨前一帧和其他对象的查询交叉注意来增强对象特征。而“未来推理”模块则消化历史信息并预测强健的未来轨迹。在长时间遮挡的情况下，我们的方法可以维持对象位置，通过整合运动预测实现重新关联。在nuScenes数据集上，我们的方法大幅提高了AMOTA，并将ID-Switches减少了90%，相比之前的方法有了显著的改善。

    This work proposes an end-to-end multi-camera 3D multi-object tracking (MOT) framework. It emphasizes spatio-temporal continuity and integrates both past and future reasoning for tracked objects. Thus, we name it "Past-and-Future reasoning for Tracking" (PF-Track). Specifically, our method adapts the "tracking by attention" framework and represents tracked instances coherently over time with object queries. To explicitly use historical cues, our "Past Reasoning" module learns to refine the tracks and enhance the object features by cross-attending to queries from previous frames and other objects. The "Future Reasoning" module digests historical information and predicts robust future trajectories. In the case of long-term occlusions, our method maintains the object positions and enables re-association by integrating motion predictions. On the nuScenes dataset, our method improves AMOTA by a large margin and remarkably reduces ID-Switches by 90% compared to prior approaches, which is an 
    
[^69]: 管制ChatGPT和其他大型生成AI模型

    Regulating ChatGPT and other Large Generative AI Models. (arXiv:2302.02337v5 [cs.CY] UPDATED)

    [http://arxiv.org/abs/2302.02337](http://arxiv.org/abs/2302.02337)

    本文将讨论大型生成AI模型的可信AI监管，包括直接监管、数据保护、内容监管和政策建议，并建议使用新术语来区分参与者。本文的目的是确保LGAIMs的可信度并使其为受益所用。

    

    大型生成AI模型（LGAIMs），如ChatGPT或Stable Diffusion，正在快速改变我们的沟通、说明和创造方式。然而，欧盟及其他地区的AI监管主要集中在传统AI模型上，而非LGAIMs。本文将把这些新的生成模型放置在当前的“可信AI监管”辩论中，并探讨如何调整法律以适应其能力。在奠定技术基础之后，本文的法律部分分四步进行，包括（1）直接监管，（2）数据保护，（3）内容监管和（4）政策建议。它建议使用新术语来捕捉LGAIM设置中的AI价值链，区分LGAIM开发人员、部署者、专业和非专业用户，以及LGAIM输出的接收者。我们将监管职责针对这些不同的价值链参与者进行调整，并提出四个策略，以确保LGAIMs的信任度并使其为受益所用。

    Large generative AI models (LGAIMs), such as ChatGPT or Stable Diffusion, are rapidly transforming the way we communicate, illustrate, and create. However, AI regulation, in the EU and beyond, has primarily focused on conventional AI models, not LGAIMs. This paper will situate these new generative models in the current debate on trustworthy AI regulation, and ask how the law can be tailored to their capabilities. After laying technical foundations, the legal part of the paper proceeds in four steps, covering (1) direct regulation, (2) data protection, (3) content moderation, and (4) policy proposals. It suggests a novel terminology to capture the AI value chain in LGAIM settings by differentiating between LGAIM developers, deployers, professional and non-professional users, as well as recipients of LGAIM output. We tailor regulatory duties to these different actors along the value chain and suggest four strategies to ensure that LGAIMs are trustworthy and deployed for the benefit of so
    
[^70]: 自监督单目深度估计对抗性训练防御物理攻击

    Adversarial Training of Self-supervised Monocular Depth Estimation against Physical-World Attacks. (arXiv:2301.13487v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2301.13487](http://arxiv.org/abs/2301.13487)

    本论文提出了一种基于视角合成的自监督单目深度估计对抗性训练方法，无需使用真实深度，使用L0范数限制扰动来提高对物理攻击的对抗鲁棒性，优于传统方法和对比学习方法。

    

    单目深度估计（MDE）是自动驾驶等应用中的关键组件。MDE网络面临各种攻击，尤其是物理攻击，威胁这种系统的安全。传统的对抗性训练方法需要有标签数据，因此不能直接应用于没有真实深度的自监督MDE。一些自监督模型强化技术（例如对比学习）忽略了MDE的领域知识，很难达到最佳性能。本文提出了一种基于视角合成的自监督MDE模型的新颖的对抗性训练方法，无需使用真实深度。我们使用L0范数限制扰动来提高对物理攻击的对抗鲁棒性。我们将我们的方法与针对MDE量身定制的有监督学习和对比学习方法进行了比较。结果表明我们的方法在两个代表性MDE网络上表现出了优异的性能。

    Monocular Depth Estimation (MDE) is a critical component in applications such as autonomous driving. There are various attacks against MDE networks. These attacks, especially the physical ones, pose a great threat to the security of such systems. Traditional adversarial training method requires ground-truth labels hence cannot be directly applied to self-supervised MDE that does not have ground-truth depth. Some self-supervised model hardening techniques (e.g., contrastive learning) ignore the domain knowledge of MDE and can hardly achieve optimal performance. In this work, we propose a novel adversarial training method for self-supervised MDE models based on view synthesis without using ground-truth depth. We improve adversarial robustness against physical-world attacks using L0-norm-bounded perturbation in training. We compare our method with supervised learning based and contrastive learning based methods that are tailored for MDE. Results on two representative MDE networks show tha
    
[^71]: Truveta Mapper：一个零样本本体映射框架

    Truveta Mapper: A Zero-shot Ontology Alignment Framework. (arXiv:2301.09767v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.09767](http://arxiv.org/abs/2301.09767)

    提出了一个将无监督本体匹配或本体对齐视为翻译任务的新视角的Truveta Mapper框架，在零样本、统一和端到端的方式下执行多本体对齐。该框架能够在运行时间延迟和对齐质量方面胜过现有解决方案，无需显式跨本体手动标注数据。

    

    本文提出了一种将无监督本体匹配(Ontology Matching, OM)或本体对齐(Ontology Alignment, OA)视为翻译任务的新视角。将本体表示为图形，在源本体图中的节点到目标本体图中的路径之间进行翻译。所提出的Truveta Mapper (TM)框架利用多任务序列到序列转换器模型，在零样本、统一和端到端的方式下执行多本体对齐。多任务使模型能够通过迁移学习来隐含地学习不同本体之间的关系，无需任何显式的跨本体手动标注数据。这也使得该框架能够在运行时间延迟和对齐质量方面胜过现有解决方案。模型仅在公开可用的文本语料库和内部本体数据上进行预训练和微调。该方案优于现有标准基准解决方案，如Edit-Similarity和MINTE+。

    In this paper, a new perspective is suggested for unsupervised Ontology Matching (OM) or Ontology Alignment (OA) by treating it as a translation task. Ontologies are represented as graphs, and the translation is performed from a node in the source ontology graph to a path in the target ontology graph. The proposed framework, Truveta Mapper (TM), leverages a multi-task sequence-to-sequence transformer model to perform alignment across multiple ontologies in a zero-shot, unified and end-to-end manner. Multi-tasking enables the model to implicitly learn the relationship between different ontologies via transfer-learning without requiring any explicit cross-ontology manually labeled data. This also enables the formulated framework to outperform existing solutions for both runtime latency and alignment quality. The model is pre-trained and fine-tuned only on publicly available text corpus and inner-ontologies data. The proposed solution outperforms state-of-the-art approaches, Edit-Similari
    
[^72]: 因果三元组：面向干预中心因果表示学习的开放挑战

    Causal Triplet: An Open Challenge for Intervention-centric Causal Representation Learning. (arXiv:2301.05169v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.05169](http://arxiv.org/abs/2301.05169)

    本文提出了一个因果表示学习基准——因果三元组，该基准具有可操作的反事实设置和干预性下游任务，对分离和物体中心表示学习取得了显著优化，然而在因果关系的识别和干预性下游任务上表现欠佳。

    

    近年来，学者们对从干预下的低级图像对中学习高级因果表示产生了浓厚的兴趣。然而，现有的研究往往局限于简单的合成数据，这远离了真实世界中的问题。在本文中，我们提出了因果三元组，这是一个因果表示学习基准，不仅具有更为复杂的视觉场景，而且还具有两个常常被忽视的关键愿望：(i) 一个可操作的反事实设置，其中只有某些物体级变量允许反事实观察，而其他变量则不允许；(ii) 一个干预性的下游任务，强调独立因果机制原则下的分布鲁棒性。通过大量的实验，我们发现，具有分离的或物体中心表示的知识的模型显着优于其分布式对应物。然而，最近的因果表示学习方法仍然难以识别可操作的反事实设置中的因果关系，且在干预性下游任务上表现欠佳。

    Recent years have seen a surge of interest in learning high-level causal representations from low-level image pairs under interventions. Yet, existing efforts are largely limited to simple synthetic settings that are far away from real-world problems. In this paper, we present Causal Triplet, a causal representation learning benchmark featuring not only visually more complex scenes, but also two crucial desiderata commonly overlooked in previous works: (i) an actionable counterfactual setting, where only certain object-level variables allow for counterfactual observations whereas others do not; (ii) an interventional downstream task with an emphasis on out-of-distribution robustness from the independent causal mechanisms principle. Through extensive experiments, we find that models built with the knowledge of disentangled or object-centric representations significantly outperform their distributed counterparts. However, recent causal representation learning methods still struggle to id
    
[^73]: 数据中心人工智能：视角和挑战

    Data-centric AI: Perspectives and Challenges. (arXiv:2301.04819v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2301.04819](http://arxiv.org/abs/2301.04819)

    研究提出了数据中心人工智能（DCAI）的概念，强调数据质量和可靠性，总结了训练数据开发、推断数据开发和数据维护三个总体使命，提供了对DCAI任务的讨论和观点，并列出了挑战。

    

    数据在构建人工智能系统方面的作用通过新兴的数据中心人工智能（DCAI）概念得到了显著增强，该概念主张将重点从模型改进转向确保数据质量和可靠性。虽然我们的社区一直在不同方面努力增强数据，但它们通常是针对特定任务的孤立举措。为了推动社区的集体倡议并推动DCAI，我们提供了一个总体框架，并集合了三个总体使命：训练数据开发、推断数据开发和数据维护。我们对代表DCAI任务进行了高层次讨论并分享了观点。最后，我们列出了开放性挑战。更多资源总结详见https://github.com/daochenzha/data-centric-AI。

    The role of data in building AI systems has recently been significantly magnified by the emerging concept of data-centric AI (DCAI), which advocates a fundamental shift from model advancements to ensuring data quality and reliability. Although our community has continuously invested efforts into enhancing data in different aspects, they are often isolated initiatives on specific tasks. To facilitate the collective initiative in our community and push forward DCAI, we draw a big picture and bring together three general missions: training data development, inference data development, and data maintenance. We provide a top-level discussion on representative DCAI tasks and share perspectives. Finally, we list open challenges. More resources are summarized at https://github.com/daochenzha/data-centric-AI
    
[^74]: 在大型完全信息博弈中解决斯塔克伯格均衡的函数逼近方法

    Function Approximation for Solving Stackelberg Equilibrium in Large Perfect Information Games. (arXiv:2212.14431v2 [cs.GT] UPDATED)

    [http://arxiv.org/abs/2212.14431](http://arxiv.org/abs/2212.14431)

    该论文提出了一种在大型博弈中解决斯塔克伯格均衡的方法，通过学习“可强制履行的收益边界”（EPF）来逼近最优策略均衡，可以应用于更广泛的广义和博弈。

    

    函数逼近是解决大型零和游戏的关键组成部分。然而，在解决广义和博弈方面，函数逼近得到的关注却很少。广义和博弈被广泛认为在计算上比它们的完全竞争或合作伙伴更具挑战性。其中一个关键的挑战是对于广义和博弈中的许多均衡，不存在像马尔可夫决策过程和零和游戏中使用的状态价值函数的简单类比物。在本文中，我们提出了学习“可强制履行的收益边界”（EPF）的方法，这是广义和博弈状态价值函数的一般化。我们通过用神经网络表示EPF并使用适当的备份操作和损失函数来训练它们，从而逼近了最优的斯塔克伯格广义和博弈策略均衡。这是第一个将函数逼近应用于斯塔克伯格均衡的方法，使我们能够扩展到更大的博弈并仍然享受良好的性能。

    Function approximation (FA) has been a critical component in solving large zero-sum games. Yet, little attention has been given towards FA in solving \textit{general-sum} extensive-form games, despite them being widely regarded as being computationally more challenging than their fully competitive or cooperative counterparts. A key challenge is that for many equilibria in general-sum games, no simple analogue to the state value function used in Markov Decision Processes and zero-sum games exists. In this paper, we propose learning the \textit{Enforceable Payoff Frontier} (EPF) -- a generalization of the state value function for general-sum games. We approximate the optimal \textit{Stackelberg extensive-form correlated equilibrium} by representing EPFs with neural networks and training them by using appropriate backup operations and loss functions. This is the first method that applies FA to the Stackelberg setting, allowing us to scale to much larger games while still enjoying performa
    
[^75]: REVEAL: 检索增强的多源多模态知识存储的视觉语言预训练。

    REVEAL: Retrieval-Augmented Visual-Language Pre-Training with Multi-Source Multimodal Knowledge Memory. (arXiv:2212.05221v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2212.05221](http://arxiv.org/abs/2212.05221)

    本文提出了一种检索增强的视觉语言模型(REVEAL)，其中包含四个关键组件：存储器、编码器、检索器和生成器。该模型可以利用多种多模态知识源，并实现了端到端预训练，在视觉问答任务上取得了最先进的结果。

    

    本文提出了一种端到端的检索增强的视觉语言模型(REVEAL)，该模型学会将世界知识编码到大规模存储器中，并从中检索以回答知识密集型查询。REVEAL由四个关键组件组成：存储器、编码器、检索器和生成器。大规模存储器通过统一编码器对各种多模态世界知识来源（如图像-文本对、问答对、知识图谱三元组等）进行编码。检索器在存储器中找到最相关的知识条目，生成器将检索到的知识与输入查询融合产生输出。我们方法的一个关键创新是，存储器、编码器、检索器和生成器都在海量数据上进行端到端预训练。此外，我们的方法可以使用多种多模态知识源，这在实验中证明了显著的收益。我们展示了REVEAL在视觉问答任务上取得了最先进的结果。

    In this paper, we propose an end-to-end Retrieval-Augmented Visual Language Model (REVEAL) that learns to encode world knowledge into a large-scale memory, and to retrieve from it to answer knowledge-intensive queries. REVEAL consists of four key components: the memory, the encoder, the retriever and the generator. The large-scale memory encodes various sources of multimodal world knowledge (e.g. image-text pairs, question answering pairs, knowledge graph triplets, etc) via a unified encoder. The retriever finds the most relevant knowledge entries in the memory, and the generator fuses the retrieved knowledge with the input query to produce the output. A key novelty in our approach is that the memory, encoder, retriever and generator are all pre-trained end-to-end on a massive amount of data. Furthermore, our approach can use a diverse set of multimodal knowledge sources, which is shown to result in significant gains. We show that REVEAL achieves state-of-the-art results on visual ques
    
[^76]: 加速 AI 伦理：创新与安全之间的辩论，稳定 AI 的扩散与 OpenAI 的 Dall-E

    Acceleration AI Ethics, the Debate between Innovation and Safety, and Stability AI's Diffusion versus OpenAI's Dall-E. (arXiv:2212.01834v2 [cs.CY] UPDATED)

    [http://arxiv.org/abs/2212.01834](http://arxiv.org/abs/2212.01834)

    本文主张将AI伦理重新构建成一个创新加速器。通过分析稳定AI与OpenAI的对比，总结出五个加速AI的共同点，即将不确定性看作积极因素，将创新视为内在价值，采用更多AI解决AI问题，采用分散的过程控制AI的许可和限制，并将伦理合并到AI的开发和应用中。这些做法使伦理成为一个激励AI创新的力量。

    

    传统 AI 伦理的一个反对意见是它会减缓创新。本文回应了这一观点，把伦理重新构建成一个创新加速器。关键元素来自稳定 AI 的扩散和 OpenAI 的 Dall-E 之间的对比。通过分析它们所支持的开发和部署战略背后的不同价值观，识别出五个概念作为加速伦理的共同点。不确定性被理解为积极和鼓励性的，而不是有所遏制。创新被认为是内在有价值的，而不仅仅是通过社会影响才有价值。AI 问题通过更多的 AI 来解决，而不是更少。控制 AI 的许可和限制来自分散的过程，而不是统一的权威。伦理的工作是嵌入在 AI 的开发和应用中，而不是从外部发挥作用。这些态度和做法共同使伦理成为激励人工智能，而不是制约它的力量。

    One objection to conventional AI ethics is that it slows innovation. This presentation responds by reconfiguring ethics as an innovation accelerator. The critical elements develop from a contrast between Stability AI's Diffusion and OpenAI's Dall-E. By analyzing the divergent values underlying their opposed strategies for development and deployment, five conceptions are identified as common to acceleration ethics. Uncertainty is understood as positive and encouraging, rather than discouraging. Innovation is conceived as intrinsically valuable, instead of worthwhile only as mediated by social effects. AI problems are solved by more AI, not less. Permissions and restrictions governing AI emerge from a decentralized process, instead of a unified authority. The work of ethics is embedded in AI development and application, instead of functioning from outside. Together, these attitudes and practices remake ethics as provoking rather than restraining artificial intelligence.
    
[^77]: SinGRAF：学习单个场景的3D生成辐射场

    SinGRAF: Learning a 3D Generative Radiance Field for a Single Scene. (arXiv:2211.17260v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.17260](http://arxiv.org/abs/2211.17260)

    SinGRAF是一个3D感知的生成模型，只需少量输入一单个场景的图像即可训练，并可在保持输入外观的同时生成多样的3D场景。

    

    生成模型在合成逼真的3D物体方面表现出很大的潜力，但需要大量的训练数据。我们介绍了SinGRAF，这是一个3D感知的生成模型，只需少量输入一单个场景的图像即可训练。一旦训练完成，SinGRAF可以生成保留输入外观并变化场景布局的不同3D场景实现。为此，我们借鉴了近期3D GAN架构的进展，同时在训练过程中使用了一种新颖的渐进式规模补丁判别方法。通过几个实验，我们证明SinGRAF产生的结果在质量和多样性方面都远远优于最接近的相关作品。

    Generative models have shown great promise in synthesizing photorealistic 3D objects, but they require large amounts of training data. We introduce SinGRAF, a 3D-aware generative model that is trained with a few input images of a single scene. Once trained, SinGRAF generates different realizations of this 3D scene that preserve the appearance of the input while varying scene layout. For this purpose, we build on recent progress in 3D GAN architectures and introduce a novel progressive-scale patch discrimination approach during training. With several experiments, we demonstrate that the results produced by SinGRAF outperform the closest related works in both quality and diversity by a large margin.
    
[^78]: 计算效率高的强化学习：基于简单规则的有针对性探索

    Computationally Efficient Reinforcement Learning: Targeted Exploration leveraging simple Rules. (arXiv:2211.16691v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.16691](http://arxiv.org/abs/2211.16691)

    本研究提出了一种基于简单规则的有针对性探索方法，通过避免已知子优的状态-动作空间区域来更快地加速强化学习代理程序的收敛，并在一个房间温度控制案例研究中实现了比传统方法快6-7倍的速度收敛。

    

    强化学习通常由于需要穷举探索状态-动作空间以找到表现良好的策略而导致样本复杂度不太好。然而，我们认为系统的专家知识通常允许我们设计简单规则，我们期望良好的策略始终遵循这些规则。因此，在本研究中，我们提出了一种简单而有效的连续演员-评论家框架的修改版本，以纳入这些规则并避免已知子优的状态-动作空间区域，从而显着加速强化学习代理程序的改进。具体而言，如果代理程序选择的动作不符合我们的直觉，我们会饱和这些动作，关键是修改策略的梯度更新步骤，以确保学习流程不受饱和步骤的影响。在一个房间温度控制案例研究中，它使代理程序以比传统代理程序快6-7倍的速度收敛到表现良好的策略，而不需要消耗额外的计算资源。

    Reinforcement Learning (RL) generally suffers from poor sample complexity, mostly due to the need to exhaustively explore the state-action space to find well-performing policies. On the other hand, we postulate that expert knowledge of the system often allows us to design simple rules we expect good policies to follow at all times. In this work, we hence propose a simple yet effective modification of continuous actor-critic frameworks to incorporate such rules and avoid regions of the state-action space that are known to be suboptimal, thereby significantly accelerating the convergence of RL agents. Concretely, we saturate the actions chosen by the agent if they do not comply with our intuition and, critically, modify the gradient update step of the policy to ensure the learning process is not affected by the saturation step. On a room temperature control case study, it allows agents to converge to well-performing policies up to 6-7x faster than classical agents without computational o
    
[^79]: 一种简单的方法教授Transformer多视角几何

    A Light Touch Approach to Teaching Transformers Multi-view Geometry. (arXiv:2211.15107v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.15107](http://arxiv.org/abs/2211.15107)

    本论文提出了一种轻触式的方法，引导视觉Transformer学习多视角几何，这种方法通过使用极线来引导Transformer的交叉注意力图，可以在测试时不需要提供任何摄像机姿态信息，适用于姿态不变的物体实例检索。

    

    Transformer在视觉学习中表现强大，这主要归因于它们缺乏手动规定的先验知识。然而，这种灵活性在涉及多视角几何的任务中可能会成为问题，因为3D形状和视点的近乎无限可能的变化需要灵活性，而投影几何的精确性则需要严格的规则。为了解决这个问题，我们提出了一种“轻触”方法，引导视觉Transformer学习多视角几何，但在需要时允许它们自由发挥。我们通过使用极线来引导Transformer的交叉注意力图，惩罚极线以外的注意值，并鼓励沿这些线的更高的注意，因为它们包含几何上合理的匹配。与以前的方法不同，我们的方法不需要在测试时提供任何摄像机姿态信息。我们关注于姿态不变的物体实例检索，标准的Transformer网络由于不同姿态之间的巨大差异而难以处理。

    Transformers are powerful visual learners, in large part due to their conspicuous lack of manually-specified priors. This flexibility can be problematic in tasks that involve multiple-view geometry, due to the near-infinite possible variations in 3D shapes and viewpoints (requiring flexibility), and the precise nature of projective geometry (obeying rigid laws). To resolve this conundrum, we propose a "light touch" approach, guiding visual Transformers to learn multiple-view geometry but allowing them to break free when needed. We achieve this by using epipolar lines to guide the Transformer's cross-attention maps, penalizing attention values outside the epipolar lines and encouraging higher attention along these lines since they contain geometrically plausible matches. Unlike previous methods, our proposal does not require any camera pose information at test-time. We focus on pose-invariant object instance retrieval, where standard Transformer networks struggle, due to the large diffe
    
[^80]: 重新思考非监督图像语义分割中的对齐和均匀性问题

    Rethinking Alignment and Uniformity in Unsupervised Image Semantic Segmentation. (arXiv:2211.14513v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.14513](http://arxiv.org/abs/2211.14513)

    本文分析了非监督图像语义分割中困扰UISS模型的特征对齐和特征均匀性问题，提出了Semantic Attention Network(SAN) 模型，包含一个新模块 semantic attention（SEAT），以动态生成逐像素和语义特征。实验结果表明，这一非监督分割框架专注于捕捉语义表示，在多个语义分割基准测试中表现优异。

    

    非监督图像语义分割(UISS)旨在将低层视觉特征与语义级别的表示匹配，而无需外部监管。本文从特征对齐和特征均匀性的角度探究了UISS模型的关键性质，并将UISS与整幅图像的表示学习进行了比较。基于分析，我们认为UISS中现有的基于互信息的方法存在表示崩溃的问题。因此，我们提出了一种稳健的网络模型——Semantic Attention Network(SAN)，其中提出了一种新模块Semantic Attention(SEAT)，以动态生成逐像素和语义特征。在多个语义分割基准测试中的实验结果表明，我们的非监督分割框架专注于捕捉语义表示，表现优异，超过了所有未预训练的模型，甚至超过了一些预训练模型。

    Unsupervised image semantic segmentation(UISS) aims to match low-level visual features with semantic-level representations without outer supervision. In this paper, we address the critical properties from the view of feature alignments and feature uniformity for UISS models. We also make a comparison between UISS and image-wise representation learning. Based on the analysis, we argue that the existing MI-based methods in UISS suffer from representation collapse. By this, we proposed a robust network called Semantic Attention Network(SAN), in which a new module Semantic Attention(SEAT) is proposed to generate pixel-wise and semantic features dynamically. Experimental results on multiple semantic segmentation benchmarks show that our unsupervised segmentation framework specializes in catching semantic representations, which outperforms all the unpretrained and even several pretrained methods.
    
[^81]: PIP：位置编码图像先验

    PIP: Positional-encoding Image Prior. (arXiv:2211.14298v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.14298](http://arxiv.org/abs/2211.14298)

    本文提出的PIP（位置编码图像先验）与DIP表现相似，但所需参数更少，并且可以轻松扩展到视频领域，解决了3D-DIP的问题。

    

    在深度图像先验（DIP）中，卷积神经网络（CNN）被适配为将潜空间映射到降质（例如噪音）图像，但在此过程中学习重建干净图像。这种现象归因于CNN的内部图像先验。我们从神经隐式表示的角度重新审视了DIP框架。受到这种观点的启发，我们用傅里叶特征（位置编码）替换了随机或学习得到的潜码。我们展示了由于傅里叶特征的属性，我们可以用简单的像素级MLP替换卷积层。我们将此方案命名为“位置编码图像先验”（PIP），并展示它在各种图像重建任务中与DIP表现非常相似，但所需的参数要少得多。此外，我们展示PIP可以轻松扩展到视频，其中3D-DIP表现不佳且不稳定。所有任务的代码和其他例子（包括视频）均可在项目页面http://people.csail.mit.edu/yilun/pip/上获得。

    In Deep Image Prior (DIP), a Convolutional Neural Network (CNN) is fitted to map a latent space to a degraded (e.g. noisy) image but in the process learns to reconstruct the clean image. This phenomenon is attributed to CNN's internal image-prior. We revisit the DIP framework, examining it from the perspective of a neural implicit representation. Motivated by this perspective, we replace the random or learned latent with Fourier-Features (Positional Encoding). We show that thanks to the Fourier features properties, we can replace the convolution layers with simple pixel-level MLPs. We name this scheme ``Positional Encoding Image Prior" (PIP) and exhibit that it performs very similarly to DIP on various image-reconstruction tasks with much less parameters required. Additionally, we demonstrate that PIP can be easily extended to videos, where 3D-DIP struggles and suffers from instability. Code and additional examples for all tasks, including videos, are available on the project page http
    
[^82]: NQE: 超关系知识图谱中的复杂查询回答中的N元查询嵌入

    NQE: N-ary Query Embedding for Complex Query Answering over Hyper-Relational Knowledge Graphs. (arXiv:2211.13469v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2211.13469](http://arxiv.org/abs/2211.13469)

    本论文提出了一种N元查询嵌入模型，用于超关系知识图谱中的复杂查询回答。该模型利用双异构Transformer编码器和模糊逻辑理论来满足所有N元的FOL查询，包括存在量词、合取、析取和否定，并提出了一种并行处理算法，可以训练或预测任意的N元FOL查询。

    

    复杂查询回答是在知识图谱上进行多跳和逻辑推理的重要任务。目前，大多数方法仅限于在二元关系事实中进行查询，并且对包含多个实体的N元事实（n >= 2）关注较少，这在真实世界中更为普遍。此外，以前的CQA方法仅能为一些特定类型的查询做出预测，并且不能灵活扩展到更复杂的逻辑查询，这严重限制了它们的应用。为了解决这些挑战，本研究提出了一种用于超关系知识图谱中CQA的N元查询嵌入（NQE）模型，这些知识图谱包括大量的N元事实。NQE利用双异构Transformer编码器和模糊逻辑理论来满足所有N元FOL查询，包括存在量词、合取、析取和否定。我们还提出了一种并行处理算法，可以训练或预测任意的N元FOL查询。

    Complex query answering (CQA) is an essential task for multi-hop and logical reasoning on knowledge graphs (KGs). Currently, most approaches are limited to queries among binary relational facts and pay less attention to n-ary facts (n>=2) containing more than two entities, which are more prevalent in the real world. Moreover, previous CQA methods can only make predictions for a few given types of queries and cannot be flexibly extended to more complex logical queries, which significantly limits their applications. To overcome these challenges, in this work, we propose a novel N-ary Query Embedding (NQE) model for CQA over hyper-relational knowledge graphs (HKGs), which include massive n-ary facts. The NQE utilizes a dual-heterogeneous Transformer encoder and fuzzy logic theory to satisfy all n-ary FOL queries, including existential quantifiers, conjunction, disjunction, and negation. We also propose a parallel processing algorithm that can train or predict arbitrary n-ary FOL queries i
    
[^83]: 用图神经网络和结构化状态空间模型建立多元生物信号模型

    Modeling Multivariate Biosignals With Graph Neural Networks and Structured State Space Models. (arXiv:2211.11176v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.11176](http://arxiv.org/abs/2211.11176)

    本研究提出了一种基于时间依赖图的通用图神经网络结构(GraphS4mer)，用于建立多元生物信号模型。该模型结合了结构化状态空间架构和动态演变的图结构学习层来解决长时序和复杂空间相关性，能有效地提高多元生物信号分类任务性能。

    

    多元生物信号在许多医学领域中都很普遍，例如脑电图、多导睡眠图和心电图。由于（1）长时间范围内的时间依赖性和（2）电极之间复杂的空间相关性，建立多元生物信号的时空依赖关系模型是具有挑战性的。为了解决这些挑战，我们建议将多元生物信号表示为时间依赖图，并介绍了GraphS4mer，这是一种通用的图神经网络（GNN）结构，通过建立生物信号中的时空依赖关系来提高生物信号分类任务的性能。具体而言，（1）我们利用结构化状态空间架构，一种最先进的深度序列模型，来捕捉生物信号中长时间范围的时间依赖关系，并（2）我们建议在GraphS4mer中添加图结构学习层，以学习数据中动态演变的图结构。我们在三个不同的生物信号分类任务上评估我们的模型，并展示它优于几种基准模型，突显了它在建立具有复杂依赖关系的多元生物信号模型方面的有效性。

    Multivariate biosignals are prevalent in many medical domains, such as electroencephalography, polysomnography, and electrocardiography. Modeling spatiotemporal dependencies in multivariate biosignals is challenging due to (1) long-range temporal dependencies and (2) complex spatial correlations between the electrodes. To address these challenges, we propose representing multivariate biosignals as time-dependent graphs and introduce GraphS4mer, a general graph neural network (GNN) architecture that improves performance on biosignal classification tasks by modeling spatiotemporal dependencies in biosignals. Specifically, (1) we leverage the Structured State Space architecture, a state-of-the-art deep sequence model, to capture long-range temporal dependencies in biosignals and (2) we propose a graph structure learning layer in GraphS4mer to learn dynamically evolving graph structures in the data. We evaluate our proposed model on three distinct biosignal classification tasks and show th
    
[^84]: 连续向量空间中的数学表达式语义表示

    Semantic Representations of Mathematical Expressions in a Continuous Vector Space. (arXiv:2211.08142v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2211.08142](http://arxiv.org/abs/2211.08142)

    该论文提出了一种在连续向量空间中表示数学表达式的方法，并使用序列到序列框架的编码器生成向量表示来捕捉数学语义，比自动编码器效果更好。

    

    数学符号在STEM文献中占据了很大一部分，但是，为公式找到语义表示仍然是一个具有挑战性的问题。由于数学符号是精确的，在字符微小变化时其含义会发生显著变化，因此适用于自然文本的方法并不一定适用于数学表达式。在这项工作中，我们描述了一种在连续向量空间中表示数学表达式的方法。我们使用一个序列到序列框架的编码器，训练其在视觉上不同但在数学上等价的表达式上生成向量表示（或嵌入）。我们将这种方法与自动编码器进行比较，并表明前者更能捕捉数学语义。最后，为了促进未来的研究，我们发布了一份等价的超越和代数表达式对的语料库。

    Mathematical notation makes up a large portion of STEM literature, yet, finding semantic representations for formulae remains a challenging problem. Because mathematical notation is precise, and its meaning changes significantly with small character shifts, the methods that work for natural text do not necessarily work well for mathematical expressions. In this work, we describe an approach for representing mathematical expressions in a continuous vector space. We use the encoder of a sequence-to-sequence architecture, trained on visually different but mathematically equivalent expressions, to generate vector representations (or embeddings). We compare this approach with an autoencoder and show that the former is better at capturing mathematical semantics. Finally, to expedite future research, we publish a corpus of equivalent transcendental and algebraic expression pairs.
    
[^85]: 可解释的多智能体强化学习中的行为建议

    Explainable Action Advising for Multi-Agent Reinforcement Learning. (arXiv:2211.07882v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2211.07882](http://arxiv.org/abs/2211.07882)

    引入可解释行为建议的多智能体强化学习框架，使得学生可以理解所学的内容并进行推理从而提高样本效率和学习效果

    

    行为建议是一种基于师生范式的强化学习知识转移技术。专家老师在训练期间提供建议，以提高学生的样本效率和策略表现。这种建议通常以状态-动作对的形式给出。然而，这使得学生难以推理和应用于新颖状态。我们引入了可解释的行为建议，其中老师提供行为建议和相关的解释，说明为什么选取该行为.这允许学生自我反思所学的内容，实现建议的泛化，并导致学习效率的提高——即使在老师不理想的情况下，也可以有效地应用于单智能体和多智能体场景中。我们通过实验证明，与最先进的方法相比，我们的框架可以产生更好的策略回报和收敛速率。

    Action advising is a knowledge transfer technique for reinforcement learning based on the teacher-student paradigm. An expert teacher provides advice to a student during training in order to improve the student's sample efficiency and policy performance. Such advice is commonly given in the form of state-action pairs. However, it makes it difficult for the student to reason with and apply to novel states. We introduce Explainable Action Advising, in which the teacher provides action advice as well as associated explanations indicating why the action was chosen. This allows the student to self-reflect on what it has learned, enabling advice generalization and leading to improved sample efficiency and learning performance - even in environments where the teacher is sub-optimal. We empirically show that our framework is effective in both single-agent and multi-agent scenarios, yielding improved policy returns and convergence rates when compared to state-of-the-art methods
    
[^86]: 人类对神经网络表示的对齐

    Human alignment of neural network representations. (arXiv:2211.01201v4 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.01201](http://arxiv.org/abs/2211.01201)

    本文研究神经网络表示与人类心理表示之间的对齐问题，发现模型规模和体系结构对对齐几乎没有影响，而训练数据集和目标函数都对对齐有很大的影响。从一个数据集中学习的神经网络表示的线性变换能显著提高对另外两个数据集中人类相似性判断的对齐性。

    

    当今的计算机视觉模型在各种视觉任务上实现了人类或接近人类水平的性能。然而，它们的体系结构、数据和学习算法与导致人类视觉的方式存在许多不同之处。本文研究影响神经网络所学习的表示与通过行为反应推断出的人类心理表示之间对齐的因素。我们发现，模型的规模和体系结构对与人类行为反应的对齐基本上没有影响，而训练数据集和目标函数则具有更大的影响。这些发现在使用两种不同任务收集的三个人类相似度判断数据集中保持一致。从一个数据集中学习的神经网络表示的线性变换显著提高了对另外两个数据集中的人类相似度判断的对齐性。此外，我们发现，一些人类概念...

    Today's computer vision models achieve human or near-human level performance across a wide variety of vision tasks. However, their architectures, data, and learning algorithms differ in numerous ways from those that give rise to human vision. In this paper, we investigate the factors that affect the alignment between the representations learned by neural networks and human mental representations inferred from behavioral responses. We find that model scale and architecture have essentially no effect on the alignment with human behavioral responses, whereas the training dataset and objective function both have a much larger impact. These findings are consistent across three datasets of human similarity judgments collected using two different tasks. Linear transformations of neural network representations learned from behavioral responses from one dataset substantially improve alignment with human similarity judgments on the other two datasets. In addition, we find that some human concept
    
[^87]: 连续优化合成程序

    Synthesizing Programs with Continuous Optimization. (arXiv:2211.00828v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2211.00828](http://arxiv.org/abs/2211.00828)

    本文提出一种将程序合成问题转化为连续优化问题的方法，并使用协方差矩阵自适应进化策略来解决，最后比较了该方法和其他程序合成技术的效果，结果表明该方法效果更好。

    

    基于某些规范的自动软件生成被称为程序合成。大多数现有方法将程序合成制定为具有离散参数的搜索问题。本文提出了一个新的连续优化问题的程序合成公式，并使用最先进的进化方法之一，称为协方差矩阵自适应进化策略来解决问题。然后，我们提出一个映射方案来将连续公式转换为实际程序。我们将我们的系统 GENESYS 与几种最近的程序合成技术（在离散和连续域中）进行比较，并展示 GENESYS 在相同时间预算内合成的程序比那些现有的方案更多。例如，对于长度为 10 的程序，GENESYS 在相同的时间预算内合成的程序比现有方案多 28％。

    Automatic software generation based on some specification is known as program synthesis. Most existing approaches formulate program synthesis as a search problem with discrete parameters. In this paper, we present a novel formulation of program synthesis as a continuous optimization problem and use a state-of-the-art evolutionary approach, known as Covariance Matrix Adaptation Evolution Strategy to solve the problem. We then propose a mapping scheme to convert the continuous formulation into actual programs. We compare our system, called GENESYS, with several recent program synthesis techniques (in both discrete and continuous domains) and show that GENESYS synthesizes more programs within a fixed time budget than those existing schemes. For example, for programs of length 10, GENESYS synthesizes 28% more programs than those existing schemes within the same time budget.
    
[^88]: 破碎的神经缩放定律

    Broken Neural Scaling Laws. (arXiv:2210.14891v7 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.14891](http://arxiv.org/abs/2210.14891)

    本文提出了一个平滑破碎的幂律函数形式，可以准确地模拟和外推深度神经网络的缩放行为，适用于各种架构和大量不同任务，包括视觉、语言、音频、视频、生成建模、对比学习、机器人、不确定性估计/校准、对抗鲁棒性、分子、计算机编程/编码、数学单词问题、算术、无监督/自监督学习和强化学习。

    This paper proposes a smoothly broken power law functional form (referred to as a Broken Neural Scaling Law (BNSL)) that accurately models and extrapolates the scaling behaviors of deep neural networks for various architectures and a large and diverse set of tasks, including vision, language, audio, video, generative modeling, contrastive learning, robotics, uncertainty estimation/calibration, adversarial robustness, molecules, computer programming/coding, math word problems, arithmetic, unsupervised/self-supervised learning, and reinforcement learning.

    我们提出了一个平滑破碎的幂律函数形式（我们称之为破碎的神经缩放定律（BNSL）），它准确地模拟和外推了深度神经网络的缩放行为（即感兴趣的评估指标随用于训练的计算量、模型参数数量、训练数据集大小或上游性能变化而变化）对于各种架构和大量不同任务中的每个任务，包括大规模视觉、语言、音频、视频、扩散、生成建模、多模态学习、对比学习、AI对齐、机器人、超出分布（OOD）泛化、持续学习、不确定性估计/校准、超出分布检测、对抗鲁棒性、蒸馏、分子、计算机编程/编码、数学单词问题、算术、无监督/自监督学习和强化学习。

    We present a smoothly broken power law functional form (referred to by us as a Broken Neural Scaling Law (BNSL)) that accurately models and extrapolates the scaling behaviors of deep neural networks (i.e. how the evaluation metric of interest varies as the amount of compute used for training, number of model parameters, training dataset size, or upstream performance varies) for various architectures and for each of various tasks within a large and diverse set of upstream and downstream tasks, in zero-shot, prompted, and fine-tuned settings. This set includes large-scale vision, language, audio, video, diffusion, generative modeling, multimodal learning, contrastive learning, AI alignment, robotics, out-of-distribution (OOD) generalization, continual learning, uncertainty estimation / calibration, out-of-distribution detection, adversarial robustness, distillation, molecules, computer programming/coding, math word problems, arithmetic, unsupervised/self-supervised learning, and reinforc
    
[^89]: 神经心理理论？关于大型语言模型中社交智能局限的研究

    Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs. (arXiv:2210.13312v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2210.13312](http://arxiv.org/abs/2210.13312)

    本文研究了现代NLP系统中社交智能和心理理论的问题。作者使用两个任务评估模型在理解社交互动意图和推断参与者心理状态和现实方面的能力，结果表明当前最大的语言模型GPT-3缺乏这种能力。这一发现强调了NLP系统在理解社交动态方面的当前局限性，并需要进一步研究。

    

    社交智能和心理理论（ToM）即理解所有参与者的不同心理状态、意图和反应的能力，使得人类能够有效地理解和应对日常社交互动。随着NLP系统在越来越复杂的社交场景中得到应用，理解社交动态的能力变得至关重要。本文从实证和理论的角度探讨了现代NLP系统中社交智能和心理理论这一开放性问题。我们使用两项任务：SocialIQa（Sap et al。，2019）和ToMi（Le et al。，2019）来衡量模型理解社交互动参与者意图和反应以及推断参与者心理状态和现实的能力。结果显示，当今最大的语言模型（GPT-3；Brown et al。，2020）缺乏这种社交智能。我们的研究结果强调了NLP系统在理解社交动态方面的当前局限性以及需要进一步研究这一领域的原因。

    Social intelligence and Theory of Mind (ToM), i.e., the ability to reason about the different mental states, intents, and reactions of all people involved, allow humans to effectively navigate and understand everyday social interactions. As NLP systems are used in increasingly complex social situations, their ability to grasp social dynamics becomes crucial. In this work, we examine the open question of social intelligence and Theory of Mind in modern NLP systems from an empirical and theory-based perspective. We show that one of today's largest language models (GPT-3; Brown et al., 2020) lacks this kind of social intelligence out-of-the box, using two tasks: SocialIQa (Sap et al., 2019), which measures models' ability to understand intents and reactions of participants of social interactions, and ToMi (Le et al., 2019), which measures whether models can infer mental states and realities of participants of situations. Our results show that models struggle substantially at these Theory 
    
[^90]: 一项关于空中计算的调查

    A Survey on Over-the-Air Computation. (arXiv:2210.11350v5 [cs.IT] UPDATED)

    [http://arxiv.org/abs/2210.11350](http://arxiv.org/abs/2210.11350)

    本文探讨了空中计算（OAC）的应用，利用多路访问信道中的干扰进行计算可以提供显着更高的可实现计算速率，相较于分离通信和计算任务。文中介绍了OAC方法的具体实现、优缺点以及应用的常见问题。

    

    通信和计算通常被视为不同的任务。然而，对于许多计算导向的应用程序，主要兴趣是设备本地信息的函数，而不是本地信息本身。在这种情况下，信息论结果表明，利用多路访问信道中的干扰进行计算，即空中计算(OAC)，可以提供显着更高的可实现计算速率，而不是分离通信和计算任务。此外，随着参与节点的增多，空中计算与分离之间的计算速率差距也会增加。鉴于这一动机，本研究提供了关于实用OAC方法的全面调查。在概述与OAC相关的基础知识之后，我们讨论了可用的OAC方案及其优缺点。我们提供了一个实现OAC的方法概述，包括硬件、软件、模型检测和检测的启用机制以及现有平台的调查。最后，我们介绍了OAC方法应用的一些普遍问题。

    Communication and computation are often viewed as separate tasks. This approach is very effective from the perspective of engineering as isolated optimizations can be performed. However, for many computation-oriented applications, the main interest is a function of the local information at the devices, rather than the local information itself. In such scenarios, information theoretical results show that harnessing the interference in a multiple access channel for computation, i.e., over-the-air computation (OAC), can provide a significantly higher achievable computation rate than separating communication and computation tasks. Moreover, the gap between OAC and separation in terms of computation rate increases with more participating nodes. Given this motivation, in this study, we provide a comprehensive survey on practical OAC methods. After outlining fundamentals related to OAC, we discuss the available OAC schemes with their pros and cons. We provide an overview of the enabling mecha
    
[^91]: BioGPT：针对生物医学文本生成和挖掘的生成式预训练Transformer模型

    BioGPT: Generative Pre-trained Transformer for Biomedical Text Generation and Mining. (arXiv:2210.10341v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2210.10341](http://arxiv.org/abs/2210.10341)

    BioGPT是一种针对生物医学领域的生成式Transformer语言模型，它在多项生物医学自然语言处理任务中均表现出色，尤其在关系提取任务中表现尤为突出。

    

    预训练语言模型在自然语言方面已经取得了很大的成功，在生物医学领域也引起了越来越多的关注。在通用语言领域中，预训练语言模型的两个主要分支是BERT（及其变体）和GPT（及其变体）。已经有很多针对生物医学领域的预训练模型，如BioBERT和PubMedBERT，在多种判别式下游生物医学任务中取得了巨大成功，但缺乏生成能力限制了它们的应用范围。本文提出了一种针对生物医学领域的生成式Transformer语言模型BioGPT，并对其进行了广泛评估，展示了我们的模型在大多数任务上优于之前的模型。特别地，在BC5CDR、KD-DTI和DDI关系提取任务上，我们分别获得了44.98％，38.42％和40.76％的F1得分，以及在PubMedQA任务上的78.2％的准确率。

    Pre-trained language models have attracted increasing attention in the biomedical domain, inspired by their great success in the general natural language domain. Among the two main branches of pre-trained language models in the general language domain, i.e., BERT (and its variants) and GPT (and its variants), the first one has been extensively studied in the biomedical domain, such as BioBERT and PubMedBERT. While they have achieved great success on a variety of discriminative downstream biomedical tasks, the lack of generation ability constrains their application scope. In this paper, we propose BioGPT, a domain-specific generative Transformer language model pre-trained on large scale biomedical literature. We evaluate BioGPT on six biomedical NLP tasks and demonstrate that our model outperforms previous models on most tasks. Especially, we get 44.98%, 38.42% and 40.76% F1 score on BC5CDR, KD-DTI and DDI end-to-end relation extraction tasks respectively, and 78.2% accuracy on PubMedQA
    
[^92]: 学习高效计划稳健的摩擦多物体抓取

    Learning to Efficiently Plan Robust Frictional Multi-Object Grasps. (arXiv:2210.07420v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2210.07420](http://arxiv.org/abs/2210.07420)

    本文介绍了一种使用神经网络进行摩擦多物体抓取的高效计划方法，相比于以往的工作，其成功率提高了13.7％，每小时的拾取次数增加了1.6倍，并且抓取计划时间减少了6.3倍。

    

    本文考虑了一个杂乱问题，多个刚性凸多边形物体随机放置在一个平面表面上，必须使用单个和多个物体的抓取方式，将它们有效地运输到装箱中。我们引入摩擦来增加每小时的拾取次数，并使用实例进行神经网络的训练，以计划稳健的多物体抓取。在物理实验中，相比于多物体抓取的先前工作，我们发现成功率增加了13.7％，每小时的拾取次数增加了1.6倍，抓取计划时间减少了6.3倍。与单个物体抓取相比，我们发现每小时的拾取次数增加了3.1倍。

    We consider a decluttering problem where multiple rigid convex polygonal objects rest in randomly placed positions and orientations on a planar surface and must be efficiently transported to a packing box using both single and multi-object grasps. Prior work considered frictionless multi-object grasping. In this paper, we introduce friction to increase picks per hour. We train a neural network using real examples to plan robust multi-object grasps. In physical experiments, we find a 13.7% increase in success rate, a 1.6x increase in picks per hour, and a 6.3x decrease in grasp planning time compared to prior work on multi-object grasping. Compared to single object grasping, we find a 3.1x increase in picks per hour.
    
[^93]: 自我指导扩散模型

    Self-Guided Diffusion Models. (arXiv:2210.06462v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2210.06462](http://arxiv.org/abs/2210.06462)

    本文提出了一种框架，利用自我监督信号的灵活性设计了自我指导扩散模型。实验表明，自标记指导始终优于没有指导的扩散模型，并且在不平衡数据上甚至可以超过基于真实标签的指导。

    

    扩散模型在图像生成方面取得了显著进展，特别是在使用指导来控制生成过程时。然而，指导需要大量的图像-注释对进行训练，因此依赖于其可用性、正确性和无偏性。本文提出一种可以消除这种注释需求的框架，利用自我监督信号的灵活性设计了自我指导扩散模型。通过利用特征提取函数和自我注释函数，我们的方法在各种图像粒度上提供指导信号：从整体图像到物体框，甚至到分割蒙版。我们在单标签和多标签图像数据集上的实验表明，自标记指导始终优于没有指导的扩散模型，并且在不平衡数据上甚至可以超过基于真实标签的指导。

    Diffusion models have demonstrated remarkable progress in image generation quality, especially when guidance is used to control the generative process. However, guidance requires a large amount of image-annotation pairs for training and is thus dependent on their availability, correctness and unbiasedness. In this paper, we eliminate the need for such annotation by instead leveraging the flexibility of self-supervision signals to design a framework for self-guided diffusion models. By leveraging a feature extraction function and a self-annotation function, our method provides guidance signals at various image granularities: from the level of holistic images to object boxes and even segmentation masks. Our experiments on single-label and multi-label image datasets demonstrate that self-labeled guidance always outperforms diffusion models without guidance and may even surpass guidance based on ground-truth labels, especially on unbalanced data. When equipped with self-supervised box or m
    
[^94]: 在6G移动网络中实现多功能边缘AI的原地模型下载

    In-situ Model Downloading to Realize Versatile Edge AI in 6G Mobile Networks. (arXiv:2210.03555v2 [cs.IT] UPDATED)

    [http://arxiv.org/abs/2210.03555](http://arxiv.org/abs/2210.03555)

    本论文提出一种名为原地模型下载的新技术，在6G移动网络中实现多功能边缘AI。该技术通过从网络的AI库中下载透明和实时地替换设备上的AI模型。所提出的框架的关键组件是一大组动态技术，可以根据设备存储计算能力、信道状态和应用程序要求等，自适应地压缩模型实现模型的下载。

    

    第六代（6G）移动网络预计将在网络边缘实现机器学习和人工智能算法的普及部署。随着边缘AI的快速发展，现在是实现在边缘设备（例如智能手机和传感器）上下载智能模型的时候了。为了实现这一版本，本文提出了一种新技术，称为原地模型下载，旨在通过从网络中的AI库下载来实现透明和实时替换设备上的AI模型。其独特的特点是适应下载到时变情况（例如应用程序、位置和时间）、设备异构存储和计算能力以及信道状态。所提出的框架的一个关键组件是一组技术，动态地在深度级别、参数级别或位级别上压缩下载的模型，以支持自适应模型下载。我们进一步提出了一种虚拟化6G网络架构，定制化为边缘AI，并整合了所提出的原地模型下载技术。该架构实现了多功能边缘AI，通过促进有效地分配AI模型从中央智能中心到边缘设备，并保证根据特定应用程序要求灵活地部署AI模型。我们进行了模拟实验，以展示提出的架构在不同情况下的有效性和优势。

    The sixth-generation (6G) mobile networks are expected to feature the ubiquitous deployment of machine learning and AI algorithms at the network edge. With rapid advancements in edge AI, the time has come to realize intelligence downloading onto edge devices (e.g., smartphones and sensors). To materialize this version, we propose a novel technology in this article, called in-situ model downloading, that aims to achieve transparent and real-time replacement of on-device AI models by downloading from an AI library in the network. Its distinctive feature is the adaptation of downloading to time-varying situations (e.g., application, location, and time), devices' heterogeneous storage-and-computing capacities, and channel states. A key component of the presented framework is a set of techniques that dynamically compress a downloaded model at the depth-level, parameter-level, or bit-level to support adaptive model downloading. We further propose a virtualized 6G network architecture customi
    
[^95]: 适用于视觉与语言模型的LASP：面向语言感知的文本优化的文本提示。

    LASP: Text-to-Text Optimization for Language-Aware Soft Prompting of Vision & Language Models. (arXiv:2210.01115v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2210.01115](http://arxiv.org/abs/2210.01115)

    本文提出了一种针对软模板学习中存在的基类过拟合问题的文本提示学习方法——LASP, 同时通过增加提示的表示能力和校准视觉-语言不匹配问题， 在三个下游任务上取得了显著的性能优于现有技术的实验结果。

    

    软模板学习最近已成为适应下游任务的V&L模型的选择方法之一，但当前的方法在经过训练数据的泛化性能方面存在较大缺陷。针对这一问题，本文提出了一种基于文本到文本交叉熵损失的语言感知的文本提示（LASP）学习方法，该方法可以有效地减少基类的过拟合，增加提示的表示能力，校准视觉-语言不匹配问题，并在三个下游任务上取得了显著的性能优于现有技术的实验结果。

    Soft prompt learning has recently emerged as one of the methods of choice for adapting V&L models to a downstream task using a few training examples. However, current methods significantly overfit the training data, suffering from large accuracy degradation when tested on unseen classes from the same domain. To this end, in this paper, we make the following 4 contributions: (1) To alleviate base class overfitting, we propose a novel Language-Aware Soft Prompting (LASP) learning method by means of a text-to-text cross-entropy loss that maximizes the probability of the learned prompts to be correctly classified with respect to pre-defined hand-crafted textual prompts. (2) To increase the representation capacity of the prompts, we propose grouped LASP where each group of prompts is optimized with respect to a separate subset of textual prompts. (3) We identify a visual-language misalignment introduced by prompt learning and LASP, and more importantly, propose a re-calibration mechanism to
    
[^96]: 通过模块化和组合来修补卷积神经网络模型的弱点

    Patching Weak Convolutional Neural Network Models through Modularization and Composition. (arXiv:2209.06116v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.06116](http://arxiv.org/abs/2209.06116)

    本文提出了一种通过压缩模块化和组合来修补卷积神经网络模型的弱点的方法，该方法无需重新训练整个模型，并在多个基准数据集上实现了与最先进方法相当甚至更好的结果。

    

    尽管深度神经网络在许多应用程序中取得了巨大成功，但在实践中并不总是具有鲁棒性。本文关注的是修补卷积神经网络模型的弱点，而不是通过昂贵的重新训练整个模型来改进它。我们提出了一种压缩模块化方法CNNSplitter，它将具有$N$类分类任务的强CNN模型分解为$N$个较小的CNN模块。每个模块是一个子模型，包含强模型的部分卷积核。为了修补在目标类别（TC）上表现不佳的弱CNN模型，我们将其与从强CNN模型中获得的相应模块相结合。这样，弱CNN模型识别TC的能力可以大大提高，而无需重新训练整个模型。我们在几个基准数据集上展示了我们提出的方法的有效性，并表明它实现了与最先进方法相当甚至更好的结果。

    Despite great success in many applications, deep neural networks are not always robust in practice. For instance, a convolutional neuron network (CNN) model for classification tasks often performs unsatisfactorily in classifying some particular classes of objects. In this work, we are concerned with patching the weak part of a CNN model instead of improving it through the costly retraining of the entire model. Inspired by the fundamental concepts of modularization and composition in software engineering, we propose a compressed modularization approach, CNNSplitter, which decomposes a strong CNN model for $N$-class classification into $N$ smaller CNN modules. Each module is a sub-model containing a part of the convolution kernels of the strong model. To patch a weak CNN model that performs unsatisfactorily on a target class (TC), we compose the weak CNN model with the corresponding module obtained from a strong CNN model. The ability of the weak CNN model to recognize the TC can thus be
    
[^97]: R\'{e}nyi散度深度互相学习

    R\'{e}nyi Divergence Deep Mutual Learning. (arXiv:2209.05732v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.05732](http://arxiv.org/abs/2209.05732)

    本文提出在深度互相学习中使用R\'{e}nyi散度，它能够在不引入大量复杂度的情况下持续提高性能，获得了广泛的实证结果的支持。

    

    本文重审了一种简单而有效的计算范式——深度互相学习（DML）。我们提出使用R\'{e}nyi散度而不是KL散度，这种做法更加灵活、可调，以改善vanilla DML。这种修改能够在有限的附加复杂性下不断提高性能。该范例的收敛性进行了理论分析，并且表明具有恒定学习率的随机梯度下降在非凸优化任务的最坏情况下收敛的偏差为$\mathcal{O}(1)$。

    This paper revisits Deep Mutual Learning (DML), a simple yet effective computing paradigm. We propose using R\'{e}nyi divergence instead of the KL divergence, which is more flexible and tunable, to improve vanilla DML. This modification is able to consistently improve performance over vanilla DML with limited additional complexity. The convergence properties of the proposed paradigm are analyzed theoretically, and Stochastic Gradient Descent with a constant learning rate is shown to converge with $\mathcal{O}(1)$-bias in the worst case scenario for nonconvex optimization tasks. That is, learning will reach nearby local optima but continue searching within a bounded scope, which may help mitigate overfitting. Finally, our extensive empirical results demonstrate the advantage of combining DML and R\'{e}nyi divergence, which further improves generalized models.
    
[^98]: 视觉中的扩散模型：一项综述

    Diffusion Models in Vision: A Survey. (arXiv:2209.04747v4 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2209.04747](http://arxiv.org/abs/2209.04747)

    扩散模型是视觉中的新兴主题，其生成样本的质量和多样性受到广泛欣赏。本综述介绍了三种通用的扩散建模方法以及各种算法和架构方面的讨论，并总结比较了扩散模型与其他最先进的生成模型的性能。

    

    去噪扩散模型是计算机视觉领域中的新兴主题，展现了在生成建模领域中非凡的结果。扩散模型是一种基于深度学习的生成模型，由两个阶段组成，前向扩散和反向扩散。在前向扩散阶段，通过逐步添加高斯噪声逐渐扰动输入数据。在反向阶段，模型被任务为通过逐步学习逆转扩散过程，逐步恢复原始输入数据。尽管扩散模型的计算负担较大，即由于在采样过程中涉及的步骤数量较多导致的速度较慢，但其所生成样本的质量和多样性仍然受到广泛欣赏。在本篇文章中，我们提供了一个关于去噪扩散模型在视觉中应用的综合性评论，包括该领域的理论和实践贡献。首先，我们确定并介绍了三种通用的扩散建模方法：连续、离散和混合扩散模型。接着，我们讨论了扩散模型的各种算法和架构方面，如使用 Lévy 过程、不同形式的噪声、模型条件和正则化、多尺度架构和并行化技术。最后，我们总结并比较了去噪扩散模型与其他最先进的生成模型在多个数据集上的性能。

    Denoising diffusion models represent a recent emerging topic in computer vision, demonstrating remarkable results in the area of generative modeling. A diffusion model is a deep generative model that is based on two stages, a forward diffusion stage and a reverse diffusion stage. In the forward diffusion stage, the input data is gradually perturbed over several steps by adding Gaussian noise. In the reverse stage, a model is tasked at recovering the original input data by learning to gradually reverse the diffusion process, step by step. Diffusion models are widely appreciated for the quality and diversity of the generated samples, despite their known computational burdens, i.e. low speeds due to the high number of steps involved during sampling. In this survey, we provide a comprehensive review of articles on denoising diffusion models applied in vision, comprising both theoretical and practical contributions in the field. First, we identify and present three generic diffusion modelin
    
[^99]: DHGE：双视图超关系知识图嵌入用于链接预测和实体类型

    DHGE: Dual-View Hyper-Relational Knowledge Graph Embedding for Link Prediction and Entity Typing. (arXiv:2207.08562v4 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2207.08562](http://arxiv.org/abs/2207.08562)

    本文提出了一个双视图超关系知识图嵌入方法，其中DH-KG包含超关系实例视图和从实体层次抽象出的超关系本体视图，定义了链接预测和实体类型识别任务，并构建了两个DH-KG数据集JW44K-6K和HTDM。DHGE是一个基于GRAN编码器、HGNN和联合学习的DH-KG嵌入模型。

    

    在知识图谱表示学习领域中，超关系事实由一个主三元组和几个辅助的属性-值描述组成，被认为比基于三元组的事实更全面和具体。然而，当前单视图的超关系知识图嵌入方法应用受到限制，因为它们弱化了表示实体之间亲属关系的分层结构。为了克服这一限制，我们提出了一个包含超关系实例视图和从实体层次抽象出的超关系本体视图的双视图超关系知识图谱结构（DH-KG）。本文首次在DH-KG上定义了链接预测和实体类型识别任务，并构建了两个DH-KG数据集，JW44K-6K，从维基数据中提取，和基于医学数据的HTDM。此外，我们提出了DHGE，一个基于GRAN编码器、HGNN和联合学习的DH-KG嵌入模型。

    In the field of representation learning on knowledge graphs (KGs), a hyper-relational fact consists of a main triple and several auxiliary attribute-value descriptions, which is considered more comprehensive and specific than a triple-based fact. However, currently available hyper-relational KG embedding methods in a single view are limited in application because they weaken the hierarchical structure that represents the affiliation between entities. To overcome this limitation, we propose a dual-view hyper-relational KG structure (DH-KG) that contains a hyper-relational instance view for entities and a hyper-relational ontology view for concepts that are abstracted hierarchically from the entities. This paper defines link prediction and entity typing tasks on DH-KG for the first time and constructs two DH-KG datasets, JW44K-6K, extracted from Wikidata, and HTDM based on medical data. Furthermore, we propose DHGE, a DH-KG embedding model based on GRAN encoders, HGNNs, and joint learnin
    
[^100]: 一种简单的联邦灾难援助政策声明性模型——透明度的建模和测量。

    A simple declarative model of the Federal Disaster Assistance Policy -- modelling and measuring transparency. (arXiv:2207.07392v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2207.07392](http://arxiv.org/abs/2207.07392)

    本研究量化分析了联邦灾难援助政策的一个简单模型，并从三个不同利益相关者的角度进行了评估，进而考虑了3种政策修改及其影响，旨在为各利益相关者的偏好排序提供参考。

    

    本文从三个不同利益相关者的角度，对联邦灾难援助政策的一个简单模型进行了数量分析。这种数量分析方法是新的，在其他领域如业务和医疗流程中也有应用。利益相关者对过程透明度很感兴趣，但每个人对透明度的具体定义都有不同的看法。我们还考虑了三种联邦灾难援助政策的修改，并分析了从利益相关者的角度看，每种政策下利益相关者满意度的变化情况。这种分析被用来对四种政策的偏好进行排序，以便考虑到所有集体利益相关者的偏好。

    In this paper we will provide a quantitative analysis of a simple model of the Federal Disaster Assistance policy from the viewpoint of three different stakeholders. This quantitative methodology is new and has applications to other areas such as business and healthcare processes. The stakeholders are interested in process transparency but each has a different opinion on precisely what constitutes transparency. We will also consider three modifications to the Federal Disaster Assistance policy and analyse, from a stakeholder viewpoint, how stakeholder satisfaction changes from process to process. This analysis is used to rank the favourability of four policies with respect to all collective stakeholder preferences.
    
[^101]: 长程规划与执行的功能对象导向网络

    Long-Horizon Planning and Execution with Functional Object-Oriented Networks. (arXiv:2207.05800v5 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2207.05800](http://arxiv.org/abs/2207.05800)

    本研究提出了一种利用物体级别知识作为功能对象导向网络进行任务规划和执行的方法，并且演示了在CoppeliaSim中应用该方法进行长期任务规划的效果。

    

    在联合对象-动作表示方面的工作之后，将功能对象导向网络（FOON）作为机器人的知识图表示引入。FOON包含对机器人任务理解和物体级规划的环境有用的符号概念。在本研究之前，很少有研究表明如何从FOON中获取的计划可以由机器人执行，因为FOON中的概念对于执行来说太抽象了。因此，我们引入了利用物体级别知识作为FOON进行任务规划和执行的想法。我们的方法自动将FOON转换为PDDL，并利用现成的规划器、行为上下文和机器人技能在分层规划管道中生成可执行的任务计划。我们在CoppeliaSim中展示了我们的整个方法的长期任务，以及如何将学习到的行为上下文扩展到前所未见的场景中。

    Following work on joint object-action representations, functional object-oriented networks (FOON) were introduced as a knowledge graph representation for robots. A FOON contains symbolic concepts useful to a robot's understanding of tasks and its environment for object-level planning. Prior to this work, little has been done to show how plans acquired from FOON can be executed by a robot, as the concepts in a FOON are too abstract for execution. We thereby introduce the idea of exploiting object-level knowledge as a FOON for task planning and execution. Our approach automatically transforms FOON into PDDL and leverages off-the-shelf planners, action contexts, and robot skills in a hierarchical planning pipeline to generate executable task plans. We demonstrate our entire approach on long-horizon tasks in CoppeliaSim and show how learned action contexts can be extended to never-before-seen scenarios.
    
[^102]: 从含噪技能标签中学习职称相似度

    Learning Job Titles Similarity from Noisy Skill Labels. (arXiv:2207.00494v3 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2207.00494](http://arxiv.org/abs/2207.00494)

    本文介绍了一种使用含噪技能标签的无监督表示学习方法，能够训练出有效的职称相似性模型，对于文本排序和职位标准化等任务非常有效。

    

    测量职称之间的语义相似度是自动职位推荐的重要功能。这个任务通常使用监督学习技术来实现，需要以等效职称对的形式进行训练数据。本文提出了一种无监督表示学习方法，使用含噪技能标签训练职称相似性模型。我们证明，这种方法在文本排序和职位标准化等任务中非常有效。

    Measuring semantic similarity between job titles is an essential functionality for automatic job recommendations. This task is usually approached using supervised learning techniques, which requires training data in the form of equivalent job title pairs. In this paper, we instead propose an unsupervised representation learning method for training a job title similarity model using noisy skill labels. We show that it is highly effective for tasks such as text ranking and job normalization.
    
[^103]: 基于神经元阈值的突触协同学习方法用于脉冲神经网络

    A Synapse-Threshold Synergistic Learning Approach for Spiking Neural Networks. (arXiv:2206.06129v3 [cs.NE] UPDATED)

    [http://arxiv.org/abs/2206.06129](http://arxiv.org/abs/2206.06129)

    该论文提出了一种新的协同学习方法，通过同时训练SNNs中的突触权重和脉冲阈值，利用了神经元固有的非突触机制，使得SNNs在静态和神经形态数据集上实现显著优异表现。

    

    脉冲神经网络(SNNs)在各种智能场景中展现出出色的能力。大部分现有的SNNs训练方法是基于突触可塑性的概念;然而，现实的大脑学习也利用了神经元固有的非突触机制。生物神经元的脉冲阈值是一种关键的固有神经元特征，它在毫秒时间尺度上表现出丰富的动态性，并被提出作为促进神经信息处理的潜在机制。本研究开发了一种新的协同学习方法，同时训练SNNs中的突触权重和脉冲阈值。使用突触-阈值协同学习（STL-SNNs）训练的SNNs在各种静态和神经形态数据集上实现了显著的优异表现，远优于使用两个退化的单学习模型训练的SNNs。在训练过程中，协同学习方法优化了神经元阈值，为网络塑造了额外的动态调整度，并有效利用了神经元固有的非突触机制。

    Spiking neural networks (SNNs) have demonstrated excellent capabilities in various intelligent scenarios. Most existing methods for training SNNs are based on the concept of synaptic plasticity; however, learning in the realistic brain also utilizes intrinsic non-synaptic mechanisms of neurons. The spike threshold of biological neurons is a critical intrinsic neuronal feature that exhibits rich dynamics on a millisecond timescale and has been proposed as an underlying mechanism that facilitates neural information processing. In this study, we develop a novel synergistic learning approach that involves simultaneously training synaptic weights and spike thresholds in SNNs. SNNs trained with synapse-threshold synergistic learning~(STL-SNNs) achieve significantly superior performance on various static and neuromorphic datasets than SNNs trained with two degenerated single-learning models. During training, the synergistic learning approach optimizes neural thresholds, providing the network 
    
[^104]: 分类中的良性过拟合：更大模型的发现可证明对抗标签噪声

    Benign Overfitting in Classification: Provably Counter Label Noise with Larger Models. (arXiv:2206.00501v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.00501](http://arxiv.org/abs/2206.00501)

    本文发现良性过拟合在存在标签噪声的情况下可能会失败，提醒未来需要理解欠拟合制度下的隐式偏见。

    

    良性过拟合的研究为超参数化深度学习模型的成功提供了见解。本文研究了过拟合是否在真实世界的分类任务中真的是良性的。我们开始观察到一个 ResNet 模型在 Cifar10 上表现良好，但在 ImageNet 上则不良。为了了解为什么良性过拟合在 ImageNet 实验中失败，我们在一个比数据点数量不明显大的限定条件下从理论上分析了良性过拟合。在这个轻微超参数化的设置下，我们的分析发现了一个相变：与之前的重超参数化设置不同，当存在标签噪声时，良性过拟合现在可能会失败。我们的分析解释了我们的经验观察，并通过一组 ResNet 的控制实验进行了验证。我们的工作强调了理解欠拟合制度下的隐式偏见的重要性，作为未来的一个方向。

    Studies on benign overfitting provide insights for the success of overparameterized deep learning models. In this work, we examine whether overfitting is truly benign in real-world classification tasks. We start with the observation that a ResNet model overfits benignly on Cifar10 but not benignly on ImageNet. To understand why benign overfitting fails in the ImageNet experiment, we theoretically analyze benign overfitting under a more restrictive setup where the number of parameters is not significantly larger than the number of data points. Under this mild overparameterization setup, our analysis identifies a phase change: unlike in the previous heavy overparameterization settings, benign overfitting can now fail in the presence of label noise. Our analysis explains our empirical observations, and is validated by a set of control experiments with ResNets. Our work highlights the importance of understanding implicit bias in underfitting regimes as a future direction.
    
[^105]: 稀疏*BERT：稀疏模型能够泛化到新的任务和领域（翻译自arXiv:2205.12452v2 [cs.CL] UPDATED）

    Sparse*BERT: Sparse Models Generalize To New tasks and Domains. (arXiv:2205.12452v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2205.12452](http://arxiv.org/abs/2205.12452)

    本文研究了使用渐进非结构化幅值修剪进行修剪的模型如何在领域和任务之间进行转移。使用遮蔽语言模型进行预训练的被修剪模型能够在不进行广泛的超参数探索或专门方法的情况下转移到新的领域和任务。在生物医学NLP任务中，Sparse*BERT可以达到或超过BioBERT的性能。

    

    大型语言模型已经成为大多数现代自然语言处理（NLP）系统的核心架构。这些模型可以在任务和领域之间始终提供卓越的准确性和鲁棒性，但其高计算开销可能会使推理变得困难和昂贵。为了使使用这些模型成本更低，近期的研究探讨了利用结构化和非结构化修剪、量化和蒸馏来提高推理速度并减小模型大小。本文研究了使用渐进非结构化幅值修剪进行修剪的模型如何在领域和任务之间进行转移。我们的实验表明，使用遮蔽语言模型进行预训练的被修剪模型能够在不进行广泛的超参数探索或专门方法的情况下转移到新的领域和任务。我们演示了我们的稀疏通用模型Sparse*BERT可以通过在非结构化生物医学文本上预训练压缩的架构而成为SparseBioBERT，并且在多种生物医学NLP任务中可以达到或超过BioBERT的性能。

    Large Language Models have become the core architecture upon which most modern natural language processing (NLP) systems build. These models can consistently deliver impressive accuracy and robustness across tasks and domains, but their high computational overhead can make inference difficult and expensive. To make using these models less costly, recent work has explored leveraging structured and unstructured pruning, quantization, and distillation to improve inference speed and decrease size. This paper studies how models pruned using Gradual Unstructured Magnitude Pruning can transfer between domains and tasks. Our experimentation shows that models that are pruned during pretraining using general domain masked language models can transfer to novel domains and tasks without extensive hyperparameter exploration or specialized approaches. We demonstrate that our general sparse model Sparse*BERT can become SparseBioBERT simply by pretraining the compressed architecture on unstructured bi
    
[^106]: 面向小样本人类数据的零样本元学习

    Zero-shot meta-learning for small-scale data from human subjects. (arXiv:2203.16309v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2203.16309](http://arxiv.org/abs/2203.16309)

    为了解决小样本人类数据的零样本学习问题，我们提出了一个元学习框架，能够快速适应有限的训练数据，处理多任务预测并可以从整体上表现最佳。

    

    尽管机器学习的发展在大数据上取得了显著的性能提升，但实际上许多人类受试者的数据规模较小且标记稀疏。已有的方法应用于这样的数据通常不容易泛化到样本外的受试者。相反，模型必须对可能来自不同分布的测试数据进行预测，这是一个称为“零样本学习”的问题。为了解决这个挑战，我们开发了一个端到端的框架，使用元学习方法，使模型能够快速适应有限的训练数据，对于样本外的测试数据进行预测。我们使用了三个真实的小规模人类受试者数据集（两个随机对照研究和一个观察性研究）来预测保留的治疗组的治疗结果。我们的模型学习每种干预的潜在治疗效果，并且能够自然地处理多任务预测。我们展示了我们的模型从整体上表现最佳。

    While developments in machine learning led to impressive performance gains on big data, many human subjects data are, in actuality, small and sparsely labeled. Existing methods applied to such data often do not easily generalize to out-of-sample subjects. Instead, models must make predictions on test data that may be drawn from a different distribution, a problem known as \textit{zero-shot learning}. To address this challenge, we develop an end-to-end framework using a meta-learning approach, which enables the model to rapidly adapt to a new prediction task with limited training data for out-of-sample test data. We use three real-world small-scale human subjects datasets (two randomized control studies and one observational study), for which we predict treatment outcomes for held-out treatment groups. Our model learns the latent treatment effects of each intervention and, by design, can naturally handle multi-task predictions. We show that our model performs the best holistically for e
    
[^107]: 不完美信息博弈中的近似最优学习

    Near-Optimal Learning of Extensive-Form Games with Imperfect Information. (arXiv:2202.01752v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.01752](http://arxiv.org/abs/2202.01752)

    本文提出了一种新的算法系列，可以更快速地在不完美信息广义博弈中找到一个近似最优解。

    

    本文解决了学习不完美信息广义博弈的近似最优算法设计的开放性问题。我们提出了第一种算法系列，仅需要 $\widetilde{\mathcal{O}}((XA+YB)/\varepsilon^2)$ 局游戏即可在两人零和博弈中找到一个 $\varepsilon$-近似纳什均衡，其中 $X,Y$ 是信息集的数量，$A,B$ 是两名玩家的行动数。这比已知的样本复杂度 $\widetilde{\mathcal{O}}((X^2A+Y^2B)/\varepsilon^2)$ 有着 $\widetilde{\mathcal{O}}(\max\{X, Y\})$ 的巨大改进，并且在对数因子内与信息理论下限一致。我们通过两种新算法实现了这种样本复杂度：平衡在线镜面下降和平衡反事实后悔最小化。这两种算法都依赖于将“平衡探索策略”集成到它们的经典对手中的新方法。此外，我们还将我们的结果扩展到了更广泛的支持不完美信息博弈的二人博弈和多人博弈中。

    This paper resolves the open question of designing near-optimal algorithms for learning imperfect-information extensive-form games from bandit feedback. We present the first line of algorithms that require only $\widetilde{\mathcal{O}}((XA+YB)/\varepsilon^2)$ episodes of play to find an $\varepsilon$-approximate Nash equilibrium in two-player zero-sum games, where $X,Y$ are the number of information sets and $A,B$ are the number of actions for the two players. This improves upon the best known sample complexity of $\widetilde{\mathcal{O}}((X^2A+Y^2B)/\varepsilon^2)$ by a factor of $\widetilde{\mathcal{O}}(\max\{X, Y\})$, and matches the information-theoretic lower bound up to logarithmic factors. We achieve this sample complexity by two new algorithms: Balanced Online Mirror Descent, and Balanced Counterfactual Regret Minimization. Both algorithms rely on novel approaches of integrating \emph{balanced exploration policies} into their classical counterparts. We also extend our results t
    
[^108]: 满足聚合函数进口法则的MISO层次推理引擎

    MISO hierarchical inference engine satisfying the law of importation with aggregation functions. (arXiv:2112.12808v4 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2112.12808](http://arxiv.org/abs/2112.12808)

    本文研究了三种基于满足聚合函数进口法则的模糊蕴含的MISO模糊层次推理引擎，以提高MISO模糊系统中模糊推理引擎的计算效率。

    

    模糊推理引擎作为模糊系统中最重要的组成部分之一，可以使用模糊逻辑推理方法从输入空间和模糊规则库中获取一些有意义的输出。为了增强多输入单输出（MISO）模糊系统中模糊推理引擎的计算效率，本文主要研究三种基于满足聚合函数进口法则（LIA）的模糊蕴含的MISO模糊层次推理引擎。首先，我们找到了一些满足（LIA）的已知模糊蕴含的聚合函数。对于给定的聚合函数，进一步确定了满足该聚合函数下（LIA）的模糊蕴含。最后，我们应用上述理论发展构建了三个MISO模糊系统中的模糊层次推理引擎。

    Fuzzy inference engine, as one of the most important components of fuzzy systems, can obtain some meaningful outputs from fuzzy sets on input space and fuzzy rule base using fuzzy logic inference methods. In order to enhance the computational efficiency of fuzzy inference engine in multi-input-single-output(MISO) fuzzy systems,this paper aims mainly to investigate three MISO fuzzy hierarchial inference engines based on fuzzy implications satisfying the law of importation with aggregation functions (LIA). We firstly find some aggregation functions for well-known fuzzy implications such that they satisfy (LIA). For a given aggregation function, the fuzzy implication which satisfies (LIA) with this aggregation function is then characterized. Finally, we construct three fuzzy hierarchical inference engines in MISO fuzzy systems applying aforementioned theoretical developments.
    
[^109]: DNN中稀疏概念的定义和量化

    Defining and Quantifying the Emergence of Sparse Concepts in DNNs. (arXiv:2111.06206v6 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2111.06206](http://arxiv.org/abs/2111.06206)

    本文提出了在DNN中理解产生的交互概念的概念-emerging现象，这些概念可以用稀疏的符号因果图和And-Or图（AOG）进行量化和简化。

    

    本文旨在说明在训练过的DNN中概念的产生现象。具体来说，我们发现DNN的推理分数可以分解为少数交互概念的影响。这些概念可以被理解为一个稀疏的符号因果图中的因果模式，该图解释了DNN。使用此类因果图对DNN进行解释的忠实性在理论上得到保证，因为我们证明了因果图可以在指数数量的不同屏蔽样本上很好地模仿DNN的输出。此外，这样的因果图可以进一步简化并重新编写为And-Or图（AOG），而不会失去太多的解释准确性。

    This paper aims to illustrate the concept-emerging phenomenon in a trained DNN. Specifically, we find that the inference score of a DNN can be disentangled into the effects of a few interactive concepts. These concepts can be understood as causal patterns in a sparse, symbolic causal graph, which explains the DNN. The faithfulness of using such a causal graph to explain the DNN is theoretically guaranteed, because we prove that the causal graph can well mimic the DNN's outputs on an exponential number of different masked samples. Besides, such a causal graph can be further simplified and re-written as an And-Or graph (AOG), without losing much explanation accuracy.
    
[^110]: SIG-VC: 一款针对人类和机器的说话人信息指导的零样本语音转换系统

    SIG-VC: A Speaker Information Guided Zero-shot Voice Conversion System for Both Human Beings and Machines. (arXiv:2111.03811v3 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2111.03811](http://arxiv.org/abs/2111.03811)

    本文提出了一种新颖的零样本语音转换方法，其中包含一个模块用于消除声学特征中的说话人信息，还添加了控制说话人信息的功能以维持语音的克隆性能。

    

    如今，在传统语音转换任务中，越来越多的系统都能够取得良好的性能，人们的注意力逐渐转向极端条件下的语音转换任务。在本文中，我们提出了一种新颖的零样本语音转换方法。我们旨在获得语音的中间表示，以实现说话人内容分离，更好地消除说话人信息，获取纯粹的内容信息。因此，我们的提出的框架包含一个模块，用于从源发声者的声学特征中去除说话人信息。此外，我们的系统添加了说话人信息控制，以维持语音克隆性能。我们使用主观和客观指标评估了所提出的系统。结果表明，我们的提出的系统显著降低了零样本语音转换中的折衷问题，同时还具有高的欺骗性能，可以对抗说话人验证系统。

    Nowadays, as more and more systems achieve good performance in traditional voice conversion (VC) tasks, people's attention gradually turns to VC tasks under extreme conditions. In this paper, we propose a novel method for zero-shot voice conversion. We aim to obtain intermediate representations for speaker-content disentanglement of speech to better remove speaker information and get pure content information. Accordingly, our proposed framework contains a module that removes the speaker information from the acoustic feature of the source speaker. Moreover, speaker information control is added to our system to maintain the voice cloning performance. The proposed system is evaluated by subjective and objective metrics. Results show that our proposed system significantly reduces the trade-off problem in zero-shot voice conversion, while it also manages to have high spoofing power to the speaker verification system.
    
[^111]: 追求公平的联邦学习

    Towards Fairness-Aware Federated Learning. (arXiv:2111.01872v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2111.01872](http://arxiv.org/abs/2111.01872)

    本文旨在概述近年来提出的公平性感知联邦学习（FAFL）方法，以解决在联邦学习中可能出现的不公平问题。

    

    联邦学习的最近进展为分布式客户端提供了大规模协作机器学习的机会，并保证了性能和数据隐私。然而，大多数现有的作品集中于FL中央控制器的利益，而忽视了FL客户端的利益。这可能会导致不公平对待客户端，使其不积极参与学习过程，并损害FL生态系统的可持续性。因此，确保FL的公平性正在吸引着大量的研究兴趣。近年来，不同角度的公平性感知联邦学习（FAFL）方法已被提出。然而，目前还没有全面的调查来帮助读者深入了解这个跨学科领域。本文旨在提供这样一篇综述。

    Recent advances in Federated Learning (FL) have brought large-scale collaborative machine learning opportunities for massively distributed clients with performance and data privacy guarantees. However, most current works focus on the interest of the central controller in FL,and overlook the interests of the FL clients. This may result in unfair treatment of clients that discourages them from actively participating in the learning process and damages the sustainability of the FL ecosystem. Therefore, the topic of ensuring fairness in FL is attracting a great deal of research interest. In recent years, diverse Fairness-Aware FL (FAFL) approaches have been proposed in an effort to achieve fairness in FL from different perspectives. However, there is no comprehensive survey that helps readers gain insight into this interdisciplinary field. This paper aims to provide such a survey. By examining the fundamental and simplifying assumptions, as well as the notions of fairness adopted by existi
    
[^112]: 批量异步随机逼近的收敛性及在强化学习中的应用

    Convergence of Batch Asynchronous Stochastic Approximation With Applications to Reinforcement Learning. (arXiv:2109.03445v4 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2109.03445](http://arxiv.org/abs/2109.03445)

    本文提出了一种批量异步随机逼近算法，它可以在内存需求和时间复杂度之间进行权衡，同时提供了可以使用较弱假设证明收敛的一般方法；在强化学习领域，我们使用此方法证明了SARSA算法的批量异步版本的收敛性。

    

    随机逼近（SA）算法是一种广泛使用的概率方法，用于在仅可用函数的有噪测量情况下找到零点或固定点。目前的文献中，区分“同步”更新和“异步”更新，在“同步”更新中，每个猜测的组件都会在每个时间更新，而在“异步”更新中，仅更新一个组件。本文研究了一种中间情况，称为“批量异步随机逼近”（BASA），在这种情况下，每个时间点仅更新“当前估计解”的一些但不是全部的组件。BASA允许用户在内存需求和时间复杂度之间进行权衡。我们开发了一种通用方法，证明此类算法收敛于所研究映射的固定点。这些收敛证明使用比现有结果更弱的假设。具体而言，现有的收敛证明要求步长参数以适当的速率下降。相反，我们仅要求每个组件具有足够的更新频率。我们在强化学习领域展示了我们方法的有用性，证明了广泛使用的SARSA算法的批量异步版本的收敛性。

    The stochastic approximation (SA) algorithm is a widely used probabilistic method for finding a zero or a fixed point of a vector-valued funtion, when only noisy measurements of the function are available. In the literature to date, one makes a distinction between ``synchronous'' updating, whereby every component of the current guess is updated at each time, and ``asynchronous'' updating, whereby only one component is updated. In this paper, we study an intermediate situation that we call ``batch asynchronous stochastic approximation'' (BASA), in which, at each time instant, \textit{some but not all} components of the current estimated solution are updated. BASA allows the user to trade off memory requirements against time complexity. We develop a general methodology for proving that such algorithms converge to the fixed point of the map under study. These convergence proofs make use of weaker hypotheses than existing results. Specifically, existing convergence proofs require that the 
    
[^113]: 决策论对抗中AGI/ASI的致命弱点

    Achilles Heels for AGI/ASI via Decision Theoretic Adversaries. (arXiv:2010.05418v9 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2010.05418](http://arxiv.org/abs/2010.05418)

    本文讨论了决策论对抗中AGI/ASI的致命弱点假设，提出可能存在潜在决策论妄想导致在对抗环境中做出非理性决策的弱点，并为理解和发现这些弱点在未来AGI/ASI系统中可能的体现做出了贡献。

    

    随着AI的不断进步，了解先进系统如何做出选择及其失败方式的重要性日益增加。在某些领域，机器已经能够超越人类，如何安全地构建可能具有与人类相同或更高能力的系统是一个特别关注的问题。本文提出了“阿喀琉斯之踵”假设，即即使是潜在的超级智能系统，其也可能存在稳定的决策论妄想，导致它们在对抗环境中做出非理性的决策。本文对决策论文献中的关键困境和悖论进行了调查，讨论了其中一些潜在的致命弱点。本文还对理解这些弱点可能在未来的AGI/ASI系统中如何体现做出了几个新的贡献。

    As progress in AI continues to advance, it is important to know how advanced systems will make choices and in what ways they may fail. Machines can already outsmart humans in some domains, and understanding how to safely build ones which may have capabilities at or above the human level is of particular concern. One might suspect that artificially generally intelligent (AGI) and artificially superintelligent (ASI) will be systems that humans cannot reliably outsmart. As a challenge to this assumption, this paper presents the Achilles Heel hypothesis which states that even a potentially superintelligent system may nonetheless have stable decision-theoretic delusions which cause them to make irrational decisions in adversarial settings. In a survey of key dilemmas and paradoxes from the decision theory literature, a number of these potential Achilles Heels are discussed in context of this hypothesis. Several novel contributions are made toward understanding the ways in which these weakne
    
[^114]: MSC: 用于星际争霸II中宏观管理的数据集

    MSC: A Dataset for Macro-Management in StarCraft II. (arXiv:1710.03131v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/1710.03131](http://arxiv.org/abs/1710.03131)

    该研究提供了一个新的加强版星际争霸数据集MSC，该数据集解决了其他数据集存在的缺陷，为研究者提供了更好的数据支持。

    

    宏观管理是星际争霸中一个重要的问题，长期以来一直受到研究。在过去几年中，各种数据集和不同的方法已被提出，但这些数据集存在一些缺陷，难以提升学术和工业研究的效果：1）某些数据集中缺乏标准的预处理、解析和特征提取程序以及预定义的训练、验证和测试集；2）一些数据集仅针对宏观管理中的特定任务；3）一些数据集要么太小，要么没有足够的标记数据，难以使用现代机器学习算法进行训练，如深度神经网络。因此，大多数先前的方法是基于各种特征进行训练，通过在不同的测试集上进行评估，难以直接进行比较。为了提升星际争霸宏观管理的研究，我们基于SC2LE平台发布了一个新的数据集MSC。MSC由精心设计的特征向量和预定义的高级样本组成，能够为研究者提供更好的数据支持。

    Macro-management is an important problem in StarCraft, which has been studied for a long time. Various datasets together with assorted methods have been proposed in the last few years. But these datasets have some defects for boosting the academic and industrial research: 1) There're neither standard preprocessing, parsing and feature extraction procedures nor predefined training, validation and test set in some datasets. 2) Some datasets are only specified for certain tasks in macro-management. 3) Some datasets are either too small or don't have enough labeled data for modern machine learning algorithms such as deep neural networks. So most previous methods are trained with various features, evaluated on different test sets from the same or different datasets, making it difficult to be compared directly. To boost the research of macro-management in StarCraft, we release a new dataset MSC based on the platform SC2LE. MSC consists of well-designed feature vectors, pre-defined high-level
    

