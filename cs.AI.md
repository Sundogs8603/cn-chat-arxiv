# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [UrbanVLP: A Multi-Granularity Vision-Language Pre-Trained Foundation Model for Urban Indicator Prediction](https://arxiv.org/abs/2403.16831) | UrbanVLP是一种多粒度信息集成的视觉语言预训练模型，旨在克服目前城市指标预测中预训练模型的局限性，提高了可解释性和精度 |
| [^2] | [SegICL: A Universal In-context Learning Framework for Enhanced Segmentation in Medical Imaging](https://arxiv.org/abs/2403.16578) | SegICL引入了一种利用上下文学习的图像分割新方法，能够在新任务中适应医学图像分割，无需从头训练模型或进行复杂微调。 |
| [^3] | [Text clustering with LLM embeddings](https://arxiv.org/abs/2403.15112) | 研究表明，LLM嵌入能够捕捉结构化语言的细微差别，BERT在性能上领先于轻量级选项，增加嵌入维度和摘要技术并不一致地提高聚类效率 |
| [^4] | [Boosting Flow-based Generative Super-Resolution Models via Learned Prior](https://arxiv.org/abs/2403.10988) | 通过引入条件学习的先验知识，成功解决了基于流的超分辨模型中的网格伪影、逆矩阵爆炸和次优结果等问题。 |
| [^5] | [Decoupled Data Consistency with Diffusion Purification for Image Restoration](https://arxiv.org/abs/2403.06054) | 通过分离反向过程和数据一致性步骤，提出了一种新颖的基于扩散的图像恢复求解器。 |
| [^6] | [Unfamiliar Finetuning Examples Control How Language Models Hallucinate](https://arxiv.org/abs/2403.05612) | 本文研究了大型语言模型如何产生幻觉，并提出通过调整微调示例的监督来控制其对不熟悉输入的预测。作者开发了一种基于RL的方法，更可靠地减轻了长篇生成任务中的幻觉。 |
| [^7] | [From One to Many: Expanding the Scope of Toxicity Mitigation in Language Models](https://arxiv.org/abs/2403.03893) | 该研究拓展了语言模型中毒性缓解的范围，涵盖了多语言环境，通过翻译数据评估和增强缓解技术，比较了不同缓解方法，并探讨了模型大小和数据量对缓解效果的影响。 |
| [^8] | [Spatio-Temporal Field Neural Networks for Air Quality Inference](https://arxiv.org/abs/2403.02354) | 该研究提出了基于时空场神经网络的新模型和金字塔推断框架，在空气质量推断中取得了最先进的性能。 |
| [^9] | [Learning Topological Representations with Bidirectional Graph Attention Network for Solving Job Shop Scheduling Problem](https://arxiv.org/abs/2402.17606) | 本文提出了拓扑感知的双向图注意力网络（TBGAT），在解决车间作业调度问题中，通过嵌入并发图并利用双向视图嵌入、图注意力聚合等技术，实现了对拓扑结构的更好建模和利用。 |
| [^10] | [Pandora's White-Box: Increased Training Data Leakage in Open LLMs](https://arxiv.org/abs/2402.17012) | 本文对开源大型语言模型（LLMs）进行了隐私攻击研究，提出了首个能同时实现高真正率和低误分类率的预训练LLMs会员推理攻击（MIAs），以及展示了在自然环境中可以从微调LLM中提取超过50%的微调数据集。 |
| [^11] | [Double-I Watermark: Protecting Model Copyright for LLM Fine-tuning](https://arxiv.org/abs/2402.14883) | 提出了一种名为“双I水印”的水印方法，通过引入两种backdoor数据范例并利用LLM的学习能力，有效地保护了LLM微调定制模型的版权。 |
| [^12] | [Rethinking Invariance Regularization in Adversarial Training to Improve Robustness-Accuracy Trade-off](https://arxiv.org/abs/2402.14648) | 重新审视了基于表示的不变性正则化方法，提出了Asymmetrically Representation-regularized Adversarial Training (AR-AT)来解决“梯度冲突”和混合分布问题，改善鲁棒性-准确性权衡。 |
| [^13] | [Linear bandits with polylogarithmic minimax regret](https://arxiv.org/abs/2402.12042) | 该研究提出了一种新的线性赌博机算法，解决了线性随机赌博机中最小极小遗憾的多对数缩放问题，通过加权最小二乘估计实现对设计矩阵特征值关系的控制，实现了累积遗憾的对数缩放。 |
| [^14] | [RAG-Driver: Generalisable Driving Explanations with Retrieval-Augmented In-Context Learning in Multi-Modal Large Language Model](https://arxiv.org/abs/2402.10828) | RAG-Driver 提出了一种适用于自动驾驶的通用化驾驶解释系统，通过检索增强上下文学习，解决了多模态大语言模型训练成本高、数据稀缺和泛化能力限制等问题。 |
| [^15] | [Entropy-Regularized Token-Level Policy Optimization for Large Language Models](https://arxiv.org/abs/2402.06700) | 本文提出了一种熵正则化的令牌级策略优化方法（ETPO），用于优化大规模语言模型（LLMs）。该方法能够通过直接与任务特定环境进行交互，并解决在如何分配令牌级学分和最大化奖励之间的冲突问题。 |
| [^16] | [Generalized Preference Optimization: A Unified Approach to Offline Alignment](https://arxiv.org/abs/2402.05749) | 广义偏好优化（GPO）是一种离线损失函数，通过参数化一类凸函数来实现统一的偏好优化视角，并提供了新的算法工具和实证洞见。 |
| [^17] | [Improving Token-Based World Models with Parallel Observation Prediction](https://arxiv.org/abs/2402.05643) | 该论文提出了一种改进基于令牌的世界模型的方法，通过引入并行观测预测机制（POP）来解决想象过程中出现的瓶颈问题。通过在一个新型TBWM代理中应用POP，想象速度提高了15.4倍，在不到12小时的训练时间内在Atari 100K基准测试中取得了超人类的表现。 |
| [^18] | [A Graph is Worth $K$ Words: Euclideanizing Graph using Pure Transformer](https://arxiv.org/abs/2402.02464) | 这篇论文介绍了GraphsGPT，它使用纯Transformer将非欧几里德图形转换为在欧几里德空间中可学习的图形单词，并通过解码器将图形单词重新构建为原始图形，保证了信息的等价性。预训练的GraphsGPT在图形表示学习和图形生成方面取得了突出成果。 |
| [^19] | [Building Guardrails for Large Language Models](https://arxiv.org/abs/2402.01822) | 本文旨在为大型语言模型构建防护措施，并倡导采用系统化方法，通过与多学科团队合作来确定精确的技术要求，以减轻LLM的风险，并全面考虑不同LLM应用的多样化上下文。 |
| [^20] | [InstructRetro: Instruction Tuning post Retrieval-Augmented Pretraining](https://arxiv.org/abs/2310.07713) | InstructRetro是目前规模最大的使用检索预训练的LLM，扩展了基础模型Retro 48B，通过指令调优在各种零样例任务上取得显著改进。 |
| [^21] | [Black-Box Access is Insufficient for Rigorous AI Audits.](http://arxiv.org/abs/2401.14446) | 本文探讨了黑盒审计的局限性以及白盒和超越框架审计的优势，黑盒访问对于严格的人工智能审计是不充分的。 |
| [^22] | [Evolutionary Computation in the Era of Large Language Model: Survey and Roadmap.](http://arxiv.org/abs/2401.10034) | 该论文调查了大语言模型和进化计算之间的相互作用，并提出了在黑盒设置下进一步提升大语言模型性能的优化框架，以及将大语言模型与进化算法结合应用于各种任务的方法。 |
| [^23] | [Code Simulation Challenges for Large Language Models.](http://arxiv.org/abs/2401.09074) | 大型语言模型在模拟计算机代码和算法执行方面遇到挑战，性能随着代码长度的增加而迅速下降。在处理短程序或标准过程时，它们能以低错误率按顺序执行指令，但对于复杂的程序，特别是包含关键路径和冗余指令的程序，模拟效果较差。我们提出了一种逐行模拟代码执行的方法来解决这个问题。 |
| [^24] | [SynHIN: Generating Synthetic Heterogeneous Information Network for Explainable AI.](http://arxiv.org/abs/2401.04133) | 该论文提出了一种生成合成异构信息网络的方法，用于可解释人工智能。该方法通过识别现实世界数据集中的模式，构建合成网络，并确保生成的合成图数据与真实数据接近。这提供了用于节点分类任务的合成异构图数据集。 |
| [^25] | [Do Concept Bottleneck Models Obey Locality?.](http://arxiv.org/abs/2401.01259) | 本文研究了概念瓶颈模型（CBMs）是否能够正确捕捉到概念之间的条件独立程度，通过分析对于概念局部性之外特征的变化如何影响概念的预测。 |
| [^26] | [Automated Model Selection for Tabular Data.](http://arxiv.org/abs/2401.00961) | 本文介绍了一种自动化模型选择算法，用于表格数据的预测。该算法考虑了特征之间的交互，并包含了基于优先级的随机网格搜索和贪婪搜索两种不同的特征选择方法。 |
| [^27] | [Learn to Refuse: Making Large Language Models More Controllable and Reliable through Knowledge Scope Limitation and Refusal Mechanism.](http://arxiv.org/abs/2311.01041) | 本文提出了一种学会拒绝（L2R）的简单而有效的解决方案，通过引入拒绝机制，使大型语言模型（LLMs）能够识别和拒绝难以回答的问题，从而提高模型的可控性和可靠性。 |
| [^28] | [Debiasing Algorithm through Model Adaptation.](http://arxiv.org/abs/2310.18913) | 本论文提出了一种通过模型适应来检测和减轻语言模型中性别偏见的方法，并证明了该方法能够显著减少偏见同时保持模型性能。 |
| [^29] | [A Wireless AI-Generated Content (AIGC) Provisioning Framework Empowered by Semantic Communication.](http://arxiv.org/abs/2310.17705) | 一种由语义通信增强的无线AI生成内容（AIGC）供应框架，通过使用语义信息而不是所有的二进制位提取和传输内容，以解决在无线网络中提供最优AIGC服务的挑战。 |
| [^30] | [ACES: generating diverse programming puzzles with autotelic language models and semantic descriptors.](http://arxiv.org/abs/2310.10692) | ACES是一种使用自我目标语言模型和语义描述符生成多样化的编程难题的方法，能够优化有趣的多样性和少样本生成。 |
| [^31] | [ParFam -- Symbolic Regression Based on Continuous Global Optimization.](http://arxiv.org/abs/2310.05537) | ParFam是一种新的符号回归方法，利用参数化的符号函数族将离散问题转化为连续问题，并结合全局优化器，能够有效解决符号回归问题。 |
| [^32] | [Continual Contrastive Spoken Language Understanding.](http://arxiv.org/abs/2310.02699) | COCONUT是一种类别增量学习方法，结合了经验重播和对比式学习，在语音理解领域中解决了模型在持续学习新任务时难以保持之前知识的问题。 |
| [^33] | [Sherlock Holmes Doesn't Play Dice: The significance of Evidence Theory for the Social and Life Sciences.](http://arxiv.org/abs/2309.03222) | 本文强调了Evidence Theory在社会与生命科学中的潜力，可以表达来自未确定事件可能实现的不确定性，而与之对比的Probability Theory只能限于决策者当前设想的可能性。本文还讨论了Evidence Theory与Probability Theory的关系以及Evidence Theory在信息论中的应用增强效果，并通过审计练习案例进一步说明了Evidence Theory的应用。 |
| [^34] | [Serving MoE Models on Resource-constrained Edge Devices via Dynamic Expert Swapping.](http://arxiv.org/abs/2308.15030) | 本文提出了一种在资源受限的边缘设备上通过动态专家交换为MoE模型提供服务的推理框架，该框架通过分析MoE模型的行为模式，引入了新的数据结构来减少资源消耗，并通过性能分析优化参数配置。 |
| [^35] | [Shifting Attention to Relevance: Towards the Uncertainty Estimation of Large Language Models.](http://arxiv.org/abs/2307.01379) | 本论文研究了大型语言模型（LLMs）自动生成的关键词不平等问题，发现在估计不确定性时，重要的令牌和含有有限语义的句子被同等或更加重视。为了解决这个问题，提出了共同转移关注点来更好地估计不确定性。 |
| [^36] | [Decom--CAM: Tell Me What You See, In Details! Feature-Level Interpretation via Decomposition Class Activation Map.](http://arxiv.org/abs/2306.04644) | 本文提出了一种名为Decom-CAM的两阶段可解释性方法，可用于对深度学习模型的预测结果进行特征级解释。实验结果表明该方法可有效定位重要区域和显著特征，并改善了模型的决策质量。 |
| [^37] | [Stochastic Collapse: How Gradient Noise Attracts SGD Dynamics Towards Simpler Subnetworks.](http://arxiv.org/abs/2306.04251) | SGD在训练过度表达的网络时，会随机地将动态吸引到更简单的子网络，这种随机吸引性能够提高泛化能力。 |
| [^38] | [Think Before You Act: Decision Transformers with Internal Working Memory.](http://arxiv.org/abs/2305.16338) | 该论文提出了具有内部工作记忆模块的决策Transformer方法，以解决使用大型语言模型的决策代理在处理新任务上性能低下的问题。所提出的方法改善了训练效率和泛化能力，并进一步增强了转化决策制定代理对新任务的适应性。 |
| [^39] | [UP5: Unbiased Foundation Model for Fairness-aware Recommendation.](http://arxiv.org/abs/2305.12090) | 本研究提出了一种新颖的基础模型UP5，它采用反事实公平促进技术来消除大型语言模型中的偏见，从而实现面向公平性的推荐。 |
| [^40] | [Proof Number Based Monte-Carlo Tree Search.](http://arxiv.org/abs/2303.09449) | 本文提出了一种将蒙特卡罗树搜索和证明数搜索相结合的PN-MCTS算法，该算法在多个游戏中均表现出优越的性能，可用于最终走步选择、解决子树以及UCT公式。 |
| [^41] | [PARIS: Personalized Activity Recommendation for Improving Sleep Quality.](http://arxiv.org/abs/2110.13745) | 该论文利用机器学习技术，结合可穿戴设备监测的数据，通过时间序列聚类找到与指定主题相关的行为模型并生成相应的睡眠质量活动建议，为提高睡眠质量提供了一种个性化解决方案。 |

# 详细

[^1]: UrbanVLP：用于城市指标预测的多粒度视觉语言预训练基础模型

    UrbanVLP: A Multi-Granularity Vision-Language Pre-Trained Foundation Model for Urban Indicator Prediction

    [https://arxiv.org/abs/2403.16831](https://arxiv.org/abs/2403.16831)

    UrbanVLP是一种多粒度信息集成的视觉语言预训练模型，旨在克服目前城市指标预测中预训练模型的局限性，提高了可解释性和精度

    

    城市指标预测旨在利用数据驱动方法推断不同城市景观中的社会经济指标。然而，目前流行的预训练模型，特别是依赖卫星图像的模型，面临着双重挑战。首先，仅集中在卫星数据中的宏观级别模式可能引入偏见，在微观级别缺乏细致的细节，例如某地的建筑细节。其次，预训练模型缺乏可解释性，限制了它们在提供城市规划透明证据方面的实用性。针对这些问题，本文设计了一种新颖的Vision-Language Pre-Trained Model（UrbanVLP）。我们的UrbanVLP无缝整合来自宏观（卫星）和微观（街景）级别的多粒度信息，克服了先前预训练模型的局限性。此外，它引入了自动生成文本和校准，提高了在下游应用中的可解释性。

    arXiv:2403.16831v1 Announce Type: cross  Abstract: Urban indicator prediction aims to infer socio-economic metrics in diverse urban landscapes using data-driven methods. However, prevalent pre-trained models, particularly those reliant on satellite imagery, face dual challenges. Firstly, concentrating solely on macro-level patterns from satellite data may introduce bias, lacking nuanced details at micro levels, such as architectural details at a place. Secondly, the lack of interpretability in pre-trained models limits their utility in providing transparent evidence for urban planning. In response to these issues, we devise a novel Vision-Language Pre-Trained Model (UrbanVLP) in this paper. Our UrbanVLP seamlessly integrates multi-granularity information from both macro (satellite) and micro (street-view) levels, overcoming the limitations of prior pre-trained models. Moreover, it introduces automatic text generation and calibration, elevating interpretability in downstream application
    
[^2]: SegICL：一种用于增强医学成像分割的通用上下文学习框架

    SegICL: A Universal In-context Learning Framework for Enhanced Segmentation in Medical Imaging

    [https://arxiv.org/abs/2403.16578](https://arxiv.org/abs/2403.16578)

    SegICL引入了一种利用上下文学习的图像分割新方法，能够在新任务中适应医学图像分割，无需从头训练模型或进行复杂微调。

    

    通过上下文学习以新任务中适应的医学图像分割模型是一个令人兴奋的进展。通用分割模型旨在横跨医学图像的不同模态进行概括，然而，它们的效果在应用于分布之外的（OOD）数据模态和任务时通常会减弱，需要对模型进行复杂微调以获得最佳性能。为解决这一挑战，我们引入了SegICL，一种利用上下文学习（ICL）进行图像分割的新方法。与现有方法不同，SegICL能够利用文本引导分割并使用一小组图像-掩码对进行上下文学习，消除了从头开始训练模型或为OOD任务（包括OOD模态和数据集）进行微调的需要。SegICL的大量实验验证表明，提示示例数量与分割之间存在正相关关系。

    arXiv:2403.16578v1 Announce Type: cross  Abstract: Medical image segmentation models adapting to new tasks in a training-free manner through in-context learning is an exciting advancement. Universal segmentation models aim to generalize across the diverse modality of medical images, yet their effectiveness often diminishes when applied to out-of-distribution (OOD) data modalities and tasks, requiring intricate fine-tuning of model for optimal performance. For addressing this challenge, we introduce SegICL, a novel approach leveraging In-Context Learning (ICL) for image segmentation. Unlike existing methods, SegICL has the capability to employ text-guided segmentation and conduct in-context learning with a small set of image-mask pairs, eliminating the need for training the model from scratch or fine-tuning for OOD tasks (including OOD modality and dataset). Extensive experimental validation of SegICL demonstrates a positive correlation between the number of prompt samples and segmentat
    
[^3]: 使用LLM嵌入进行文本聚类

    Text clustering with LLM embeddings

    [https://arxiv.org/abs/2403.15112](https://arxiv.org/abs/2403.15112)

    研究表明，LLM嵌入能够捕捉结构化语言的细微差别，BERT在性能上领先于轻量级选项，增加嵌入维度和摘要技术并不一致地提高聚类效率

    

    文本聚类是组织不断增长的数字内容的重要方法，有助于结构化和发现未分类数据中的隐藏模式。在这项研究中，我们调查了不同文本嵌入（特别是大型语言模型LLMs中使用的）和聚类算法如何影响文本数据集的聚类方式。进行了一系列实验以评估嵌入是如何影响聚类结果的，以及通过摘要进行降维和嵌入大小调整的作用。结果显示，LLM嵌入在捕获结构化语言的细微差别方面表现出色，而BERT在性能上领先于轻量级选项。此外，我们发现增加嵌入维度和摘要技术并不一致地提高聚类效率，这表明这些策略需要仔细分析才能在实际模型中使用。这些结果突出了一种

    arXiv:2403.15112v1 Announce Type: cross  Abstract: Text clustering is an important approach for organising the growing amount of digital content, helping to structure and find hidden patterns in uncategorised data. In this research, we investigated how different textual embeddings - particularly those used in large language models (LLMs) - and clustering algorithms affect how text datasets are clustered. A series of experiments were conducted to assess how embeddings influence clustering results, the role played by dimensionality reduction through summarisation, and embedding size adjustment. Results reveal that LLM embeddings excel at capturing the nuances of structured language, while BERT leads the lightweight options in performance. In addition, we find that increasing embedding dimensionality and summarisation techniques do not uniformly improve clustering efficiency, suggesting that these strategies require careful analysis to use in real-life models. These results highlight a co
    
[^4]: 通过学习的先验知识提升基于流的生成超分辨模型

    Boosting Flow-based Generative Super-Resolution Models via Learned Prior

    [https://arxiv.org/abs/2403.10988](https://arxiv.org/abs/2403.10988)

    通过引入条件学习的先验知识，成功解决了基于流的超分辨模型中的网格伪影、逆矩阵爆炸和次优结果等问题。

    

    基于流的超分辨（SR）模型展现了在生成功能高质量图像方面的惊人能力。然而，这些方法在图像生成过程中遇到了几个挑战，比如网格伪影、逆矩阵爆炸和由于固定的采样温度导致的次优结果。为了克服这些问题，本文在基于流的SR模型的推断阶段引入了条件学习的先验知识。这个先验知识是由我们提出的潜在模块根据低分辨率图像预测出的潜在代码，然后通过流模型转换为SR图像。我们的框架旨在无缝地与任何现代基于流的SR模型集成，而无需修改其架构或预训练权重。我们通过大量实验和消融分析评估了提出的框架的有效性。提出的框架成功解决了基于流的SR模型中的所有固有问题。

    arXiv:2403.10988v1 Announce Type: cross  Abstract: Flow-based super-resolution (SR) models have demonstrated astonishing capabilities in generating high-quality images. However, these methods encounter several challenges during image generation, such as grid artifacts, exploding inverses, and suboptimal results due to a fixed sampling temperature. To overcome these issues, this work introduces a conditional learned prior to the inference phase of a flow-based SR model. This prior is a latent code predicted by our proposed latent module conditioned on the low-resolution image, which is then transformed by the flow model into an SR image. Our framework is designed to seamlessly integrate with any contemporary flow-based SR model without modifying its architecture or pre-trained weights. We evaluate the effectiveness of our proposed framework through extensive experiments and ablation analyses. The proposed framework successfully addresses all the inherent issues in flow-based SR models a
    
[^5]: 具有扩散净化的分离数据一致性的图像恢复

    Decoupled Data Consistency with Diffusion Purification for Image Restoration

    [https://arxiv.org/abs/2403.06054](https://arxiv.org/abs/2403.06054)

    通过分离反向过程和数据一致性步骤，提出了一种新颖的基于扩散的图像恢复求解器。

    

    最近，扩散模型作为一种强大的深度生成先验类别已经引起了人们的关注，由于其出色地建模数据分布的能力，在各种图像恢复任务中表现出色。为了解决图像恢复问题，许多现有技术通过将额外的似然梯度步骤纳入到扩散模型的反向采样过程中来实现数据一致性。然而，这些额外的梯度步骤对于实际应用中存在挑战，因为它们造成了巨大的计算开销，从而增加了推理时间。当使用加速的扩散模型采样器时，这些额外的步骤还会导致额外的困难，因为数据一致性步骤的数量受限于反向采样步骤的数量。在这项工作中，我们提出了一种新颖的基于扩散的图像恢复求解器，通过将反向过程与数据一致性步骤分离来解决这些问题。我们的方法涉及

    arXiv:2403.06054v1 Announce Type: cross  Abstract: Diffusion models have recently gained traction as a powerful class of deep generative priors, excelling in a wide range of image restoration tasks due to their exceptional ability to model data distributions. To solve image restoration problems, many existing techniques achieve data consistency by incorporating additional likelihood gradient steps into the reverse sampling process of diffusion models. However, the additional gradient steps pose a challenge for real-world practical applications as they incur a large computational overhead, thereby increasing inference time. They also present additional difficulties when using accelerated diffusion model samplers, as the number of data consistency steps is limited by the number of reverse sampling steps. In this work, we propose a novel diffusion-based image restoration solver that addresses these issues by decoupling the reverse process from the data consistency steps. Our method involv
    
[^6]: 不熟悉的微调示例控制语言模型如何产生幻觉

    Unfamiliar Finetuning Examples Control How Language Models Hallucinate

    [https://arxiv.org/abs/2403.05612](https://arxiv.org/abs/2403.05612)

    本文研究了大型语言模型如何产生幻觉，并提出通过调整微调示例的监督来控制其对不熟悉输入的预测。作者开发了一种基于RL的方法，更可靠地减轻了长篇生成任务中的幻觉。

    

    大型语言模型（LLMs）倾向于生成听起来令人信服但事实不正确的响应，特别是当在不熟悉的概念上进行查询时。本文探讨了调整后的LLMs如何产生幻觉的基本机制。我们的调查揭示了一个有趣的模式：随着输入变得更不熟悉，LLMs的输出倾向于默认为"含糊其词"的预测，其形式受微调数据中不熟悉示例监督方式的影响。因此，通过策略性地修改这些示例的监督，我们可以控制LLM对不熟悉输入的预测（例如，教会它们说“我不知道”）。基于这些原则，我们开发了一种RL方法，通过解决奖励模型幻觉带来的挑战，更可靠地减轻长篇生成任务的幻觉。我们通过在MMLU上的多选QA中进行一系列受控实验来验证我们的发现。

    arXiv:2403.05612v1 Announce Type: cross  Abstract: Large language models (LLMs) have a tendency to generate plausible-sounding yet factually incorrect responses, especially when queried on unfamiliar concepts. In this work, we explore the underlying mechanisms that govern how finetuned LLMs hallucinate. Our investigation reveals an interesting pattern: as inputs become more unfamiliar, LLM outputs tend to default towards a ``hedged'' prediction, whose form is determined by how the unfamiliar examples in the finetuning data are supervised. Thus, by strategically modifying these examples' supervision, we can control LLM predictions for unfamiliar inputs (e.g., teach them to say ``I don't know''). Based on these principles, we develop an RL approach that more reliably mitigates hallucinations for long-form generation tasks, by tackling the challenges presented by reward model hallucinations. We validate our findings with a series of controlled experiments in multiple-choice QA on MMLU, as
    
[^7]: 从单一到多样：拓展语言模型中毒性缓解的范围

    From One to Many: Expanding the Scope of Toxicity Mitigation in Language Models

    [https://arxiv.org/abs/2403.03893](https://arxiv.org/abs/2403.03893)

    该研究拓展了语言模型中毒性缓解的范围，涵盖了多语言环境，通过翻译数据评估和增强缓解技术，比较了不同缓解方法，并探讨了模型大小和数据量对缓解效果的影响。

    

    迄今为止，语言模型中的毒性缓解几乎完全集中在单语言环境中。随着语言模型拥抱多语言能力，我们的安全措施跟上步伐至关重要。我们意识到了这一研究空白，我们的方法将传统的毒性缓解范围扩展到应对多语言带来的复杂性。在缺乏跨语言的足够标注数据集的情况下，我们使用翻译数据来评估和增强我们的缓解技术。我们还在静态和持续毒性缓解场景下比较了微调缓解方法和检索增强技术。这使我们能够检验翻译质量和跨语言转移对毒性缓解的影响。我们还探讨了模型大小和数据数量如何影响这些缓解工作的成功。涵盖了九种语言，我们的研究代表了广泛的语言学领域。

    arXiv:2403.03893v1 Announce Type: cross  Abstract: To date, toxicity mitigation in language models has almost entirely been focused on single-language settings. As language models embrace multilingual capabilities, it's crucial our safety measures keep pace. Recognizing this research gap, our approach expands the scope of conventional toxicity mitigation to address the complexities presented by multiple languages. In the absence of sufficient annotated datasets across languages, we employ translated data to evaluate and enhance our mitigation techniques. We also compare finetuning mitigation approaches against retrieval-augmented techniques under both static and continual toxicity mitigation scenarios. This allows us to examine the effects of translation quality and the cross-lingual transfer on toxicity mitigation. We also explore how model size and data quantity affect the success of these mitigation efforts. Covering nine languages, our study represents a broad array of linguistic f
    
[^8]: 基于时空场神经网络的空气质量推断

    Spatio-Temporal Field Neural Networks for Air Quality Inference

    [https://arxiv.org/abs/2403.02354](https://arxiv.org/abs/2403.02354)

    该研究提出了基于时空场神经网络的新模型和金字塔推断框架，在空气质量推断中取得了最先进的性能。

    

    空气质量推断问题旨在利用来自有限观测站的历史数据推断未知位置的空气质量指数。考虑到观测站高昂的维护成本导致数据稀疏性，良好的推断算法可以有效节约成本并细化数据粒度。尽管时空图神经网络在这个问题上取得了显著进展，但它们对现实的非欧几里得和离散数据结构建模限制了潜力。本文首次尝试通过提出一个新模型，即时空场神经网络，及其对应的新框架，金字塔推断，将两种不同的时空观点，场和图，相结合。大量实验证实我们的模型在中国大陆全国范围内的空气质量推断中实现了最新技术水平，展示了我们提出的模型的优越性。

    arXiv:2403.02354v1 Announce Type: cross  Abstract: The air quality inference problem aims to utilize historical data from a limited number of observation sites to infer the air quality index at an unknown location. Considering the sparsity of data due to the high maintenance cost of the stations, good inference algorithms can effectively save the cost and refine the data granularity. While spatio-temporal graph neural networks have made excellent progress on this problem, their non-Euclidean and discrete data structure modeling of reality limits its potential. In this work, we make the first attempt to combine two different spatio-temporal perspectives, fields and graphs, by proposing a new model, Spatio-Temporal Field Neural Network, and its corresponding new framework, Pyramidal Inference. Extensive experiments validate that our model achieves state-of-the-art performance in nationwide air quality inference in the Chinese Mainland, demonstrating the superiority of our proposed model 
    
[^9]: 使用双向图注意力网络学习拓扑表示解决车间作业调度问题

    Learning Topological Representations with Bidirectional Graph Attention Network for Solving Job Shop Scheduling Problem

    [https://arxiv.org/abs/2402.17606](https://arxiv.org/abs/2402.17606)

    本文提出了拓扑感知的双向图注意力网络（TBGAT），在解决车间作业调度问题中，通过嵌入并发图并利用双向视图嵌入、图注意力聚合等技术，实现了对拓扑结构的更好建模和利用。

    

    现有的基于学习的方法通常使用针对无向图的现成GNN模型解决车间作业调度问题（JSSP），并忽略了并发图（DGs）的丰富而有意义的拓扑结构。本文提出了拓扑感知的双向图注意力网络（TBGAT），这是一种基于注意力机制的新颖GNN架构，用于在本地搜索框架中嵌入DG以解决JSSP。具体而言，TBGAT分别从正向和反向视图嵌入DG，消息通过遵循不同视图的拓扑结构传播，并通过图注意力进行汇总。然后，我们提出一种基于消息传递机制的新操作符，用于计算DG的前向和后向拓扑排序，这些特征用于表征拓扑结构并被我们的模型利用。此外，我们从理论和实验上展示了TBGAT的...

    arXiv:2402.17606v1 Announce Type: cross  Abstract: Existing learning-based methods for solving job shop scheduling problem (JSSP) usually use off-the-shelf GNN models tailored to undirected graphs and neglect the rich and meaningful topological structures of disjunctive graphs (DGs). This paper proposes the topology-aware bidirectional graph attention network (TBGAT), a novel GNN architecture based on the attention mechanism, to embed the DG for solving JSSP in a local search framework. Specifically, TBGAT embeds the DG from a forward and a backward view, respectively, where the messages are propagated by following the different topologies of the views and aggregated via graph attention. Then, we propose a novel operator based on the message-passing mechanism to calculate the forward and backward topological sorts of the DG, which are the features for characterizing the topological structures and exploited by our model. In addition, we theoretically and experimentally show that TBGAT h
    
[^10]: Pandora's White-Box：开放LLMs中训练数据泄漏的增加

    Pandora's White-Box: Increased Training Data Leakage in Open LLMs

    [https://arxiv.org/abs/2402.17012](https://arxiv.org/abs/2402.17012)

    本文对开源大型语言模型（LLMs）进行了隐私攻击研究，提出了首个能同时实现高真正率和低误分类率的预训练LLMs会员推理攻击（MIAs），以及展示了在自然环境中可以从微调LLM中提取超过50%的微调数据集。

    

    在本文中，我们对开源的大型语言模型（LLMs）遭受的隐私攻击进行了系统研究，其中对手可以访问模型权重、梯度或损失，试图利用它们来了解底层训练数据。我们的主要结果是针对预训练LLMs的第一个会员推理攻击（MIAs），能够同时实现高TPR和低FPR，并展示了在自然环境中可以从微调LLM中提取超过50%的微调数据集。我们考虑了对底层模型的不同访问程度、语言模型的定制化以及攻击者可以使用的资源。在预训练设置中，我们提出了三种新的白盒MIAs：基于梯度范数的攻击、监督神经网络分类器和单步损失比攻击。所有这些都优于现有的黑盒基线，并且我们的.....

    arXiv:2402.17012v1 Announce Type: cross  Abstract: In this paper we undertake a systematic study of privacy attacks against open source Large Language Models (LLMs), where an adversary has access to either the model weights, gradients, or losses, and tries to exploit them to learn something about the underlying training data. Our headline results are the first membership inference attacks (MIAs) against pre-trained LLMs that are able to simultaneously achieve high TPRs and low FPRs, and a pipeline showing that over $50\%$ (!) of the fine-tuning dataset can be extracted from a fine-tuned LLM in natural settings. We consider varying degrees of access to the underlying model, customization of the language model, and resources available to the attacker. In the pre-trained setting, we propose three new white-box MIAs: an attack based on the gradient norm, a supervised neural network classifier, and a single step loss ratio attack. All outperform existing black-box baselines, and our supervi
    
[^11]: 双I水印：保护LLM微调模型版权

    Double-I Watermark: Protecting Model Copyright for LLM Fine-tuning

    [https://arxiv.org/abs/2402.14883](https://arxiv.org/abs/2402.14883)

    提出了一种名为“双I水印”的水印方法，通过引入两种backdoor数据范例并利用LLM的学习能力，有效地保护了LLM微调定制模型的版权。

    

    为了支持各种应用，业主经常通过LLM所有者或云服务器提供的API对预训练的LLM进行微调，以获取定制模型。然而，这一过程存在着模型被滥用的风险，可能会给业主带来严重的经济后果。因此，在LLM微调过程中保护这些定制模型的版权已成为紧迫的实际需求，但现有的解决方案有限。为了解决这一紧迫问题，我们提出了一种名为“双I水印”的新型水印方法。具体地，基于指导微调数据，引入了两种backdoor数据范例，分别在指令和输入中触发。通过利用LLM的学习能力将定制的后门样本纳入数据集，所提出的方法有效地注入了特定的水印。

    arXiv:2402.14883v1 Announce Type: cross  Abstract: To support various applications, business owners often seek the customized models that are obtained by fine-tuning a pre-trained LLM through the API provided by LLM owners or cloud servers. However, this process carries a substantial risk of model misuse, potentially resulting in severe economic consequences for business owners. Thus, safeguarding the copyright of these customized models during LLM fine-tuning has become an urgent practical requirement, but there are limited existing solutions to provide such protection. To tackle this pressing issue, we propose a novel watermarking approach named "Double-I watermark". Specifically, based on the instruct-tuning data, two types of backdoor data paradigms are introduced with trigger in the instruction and the input, respectively. By leveraging LLM's learning capability to incorporate customized backdoor samples into the dataset, the proposed approach effectively injects specific watermar
    
[^12]: 在对抗训练中重新思考不变性正则化以改善鲁棒性-准确性权衡

    Rethinking Invariance Regularization in Adversarial Training to Improve Robustness-Accuracy Trade-off

    [https://arxiv.org/abs/2402.14648](https://arxiv.org/abs/2402.14648)

    重新审视了基于表示的不变性正则化方法，提出了Asymmetrically Representation-regularized Adversarial Training (AR-AT)来解决“梯度冲突”和混合分布问题，改善鲁棒性-准确性权衡。

    

    尽管对抗训练一直是抵抗对抗性样本（AEs）的最先进方法，但它们存在鲁棒性-准确性权衡问题。在这项研究中，我们重新审视基于表示的不变性正则化，学习具有辨别性却对抗性不变的表示，旨在缓解这种权衡。我们在经验上确定了妨碍不变性正则化的两个关键问题：（1）不变性损失和分类目标之间的“梯度冲突”，表明存在“崩溃解”，以及（2）由于干净和对抗性输入的分布发散而出现的混合分布问题。为了解决这些问题，我们提出了一种不对称表示正则化的对抗训练（AR-AT），该方法结合了一个停止梯度操作和一个预测器来避免“崩溃解”，灵感来自最近的非对比自监督学习。

    arXiv:2402.14648v1 Announce Type: cross  Abstract: Although adversarial training has been the state-of-the-art approach to defend against adversarial examples (AEs), they suffer from a robustness-accuracy trade-off. In this work, we revisit representation-based invariance regularization to learn discriminative yet adversarially invariant representations, aiming to mitigate this trade-off. We empirically identify two key issues hindering invariance regularization: (1) a "gradient conflict" between invariance loss and classification objectives, indicating the existence of "collapsing solutions," and (2) the mixture distribution problem arising from diverged distributions of clean and adversarial inputs. To address these issues, we propose Asymmetrically Representation-regularized Adversarial Training (AR-AT), which incorporates a stop-gradient operation and a pre-dictor in the invariance loss to avoid "collapsing solutions," inspired by a recent non-contrastive self-supervised learning a
    
[^13]: 具有多对数极小极小遗憾的线性赌博机

    Linear bandits with polylogarithmic minimax regret

    [https://arxiv.org/abs/2402.12042](https://arxiv.org/abs/2402.12042)

    该研究提出了一种新的线性赌博机算法，解决了线性随机赌博机中最小极小遗憾的多对数缩放问题，通过加权最小二乘估计实现对设计矩阵特征值关系的控制，实现了累积遗憾的对数缩放。

    

    我们研究了一种线性随机赌博机的噪声模型，对于该模型，当我们选择越来越接近未知向量的单位球上的动作时，亚高斯噪声参数以线性方式消失。我们针对这个问题引入了一种算法，其在时间长度$T$的情况下呈对数$^3（T）$的最小遗憾缩放，与典型赌博机算法的平方根遗憾缩放形成鲜明对比。我们的策略基于加权最小二乘估计，通过几何论证实现了设计矩阵$V_t$在每个时间步骤$t$处的特征值关系$\lambda_{\min} ( V_t ) = \Omega (\sqrt{\lambda_{\max}(V_t ) })$，这些几何论证与噪声模型无关，并可能具有独立的兴趣。这使我们能够严格控制每个时间步骤的期望遗憾为$O(\frac1{t})$的数量级，从而导致累积遗憾的对数缩放。

    arXiv:2402.12042v1 Announce Type: cross  Abstract: We study a noise model for linear stochastic bandits for which the subgaussian noise parameter vanishes linearly as we select actions on the unit sphere closer and closer to the unknown vector. We introduce an algorithm for this problem that exhibits a minimax regret scaling as $\log^3(T)$ in the time horizon $T$, in stark contrast the square root scaling of this regret for typical bandit algorithms. Our strategy, based on weighted least-squares estimation, achieves the eigenvalue relation $\lambda_{\min} ( V_t ) = \Omega (\sqrt{\lambda_{\max}(V_t ) })$ for the design matrix $V_t$ at each time step $t$ through geometrical arguments that are independent of the noise model and might be of independent interest. This allows us to tightly control the expected regret in each time step to be of the order $O(\frac1{t})$, leading to the logarithmic scaling of the cumulative regret.
    
[^14]: RAG-Driver：在多模态大语言模型中通过检索增强上下文学习实现可泛化的驾驶解释

    RAG-Driver: Generalisable Driving Explanations with Retrieval-Augmented In-Context Learning in Multi-Modal Large Language Model

    [https://arxiv.org/abs/2402.10828](https://arxiv.org/abs/2402.10828)

    RAG-Driver 提出了一种适用于自动驾驶的通用化驾驶解释系统，通过检索增强上下文学习，解决了多模态大语言模型训练成本高、数据稀缺和泛化能力限制等问题。

    

    由“黑匣子”模型驱动的机器人需要提供人类可信赖的可理解解释。因此，可解释性在可信任的自主决策中扮演着关键角色，以促进透明度和最终用户的接受度，特别是在复杂的自动驾驶场景中。最近多模态大语言模型（MLLMs）的进展显示出了增强解释性作为驾驶代理的潜力，通过产生控制预测以及自然语言解释。然而，由于昂贵的注释成本和不同数据集之间的显著领域差异，导致的严重数据稀缺使得开发一个强大且具有通用性的系统变得异常具有挑战性。此外，MLLM的训练要求昂贵，而灾难性遗忘问题的尚未解决也限制了它们在部署后的泛化能力。为了解决这些挑战，我们提出了RAG-Driver。

    arXiv:2402.10828v1 Announce Type: cross  Abstract: Robots powered by 'blackbox' models need to provide human-understandable explanations which we can trust. Hence, explainability plays a critical role in trustworthy autonomous decision-making to foster transparency and acceptance among end users, especially in complex autonomous driving. Recent advancements in Multi-Modal Large Language models (MLLMs) have shown promising potential in enhancing the explainability as a driving agent by producing control predictions along with natural language explanations. However, severe data scarcity due to expensive annotation costs and significant domain gaps between different datasets makes the development of a robust and generalisable system an extremely challenging task. Moreover, the prohibitively expensive training requirements of MLLM and the unsolved problem of catastrophic forgetting further limit their generalisability post-deployment. To address these challenges, we present RAG-Driver, a n
    
[^15]: 熵正则化的令牌级策略优化用于大规模语言模型

    Entropy-Regularized Token-Level Policy Optimization for Large Language Models

    [https://arxiv.org/abs/2402.06700](https://arxiv.org/abs/2402.06700)

    本文提出了一种熵正则化的令牌级策略优化方法（ETPO），用于优化大规模语言模型（LLMs）。该方法能够通过直接与任务特定环境进行交互，并解决在如何分配令牌级学分和最大化奖励之间的冲突问题。

    

    大规模语言模型（LLMs）在交互式决策任务中表现出了智能代理的潜力。传统方法通常依赖于精心设计的提示、高质量的示例或额外的奖励模型进行上下文学习、监督微调或RLHF。强化学习（RL）提供了一种动态的解决方案，使LLMs能够通过直接与任务特定环境进行交互来克服这些依赖关系。尽管如此，它面临着重重困难：1）由于巨大的动作空间需要探索而产生的不稳定性；2）基于动作级奖励信号分配令牌级学分的挑战，导致最大化奖励和准确建模语料库数据之间的冲突。为了应对这些挑战，我们引入了熵正则化的令牌级策略优化（ETPO），这是一种专为在令牌级优化LLMs而设计的熵增强强化学习方法。ETPO的核心是我们的一种新颖的逐令牌软Bellman更新算法，

    Large Language Models (LLMs) have shown promise as intelligent agents in interactive decision-making tasks. Traditional approaches often depend on meticulously designed prompts, high-quality examples, or additional reward models for in-context learning, supervised fine-tuning, or RLHF. Reinforcement learning (RL) presents a dynamic alternative for LLMs to overcome these dependencies by engaging directly with task-specific environments. Nonetheless, it faces significant hurdles: 1) instability stemming from the exponentially vast action space requiring exploration; 2) challenges in assigning token-level credit based on action-level reward signals, resulting in discord between maximizing rewards and accurately modeling corpus data. In response to these challenges, we introduce Entropy-Regularized Token-level Policy Optimization (ETPO), an entropy-augmented RL method tailored for optimizing LLMs at the token level. At the heart of ETPO is our novel per-token soft Bellman update, designed 
    
[^16]: 广义偏好优化：离线对齐的统一方法

    Generalized Preference Optimization: A Unified Approach to Offline Alignment

    [https://arxiv.org/abs/2402.05749](https://arxiv.org/abs/2402.05749)

    广义偏好优化（GPO）是一种离线损失函数，通过参数化一类凸函数来实现统一的偏好优化视角，并提供了新的算法工具和实证洞见。

    

    离线偏好优化允许直接从离线数据中对大型模型进行微调，并在最近的对齐实践中证明了其有效性。我们提出了广义偏好优化（GPO），这是一类通过一般的凸函数参数化的离线损失函数。GPO提供了对偏好优化的统一视角，涵盖了现有算法（DPO、IPO和SLiC）作为特殊情况，同时自然引入了新的变体。GPO框架还揭示了离线算法如何通过定义损失的凸函数来实施正则化。我们的分析和实验揭示了离线正则化和规范的RLHF公式所意图的KL散度正则化之间的联系和微妙差异。总的来说，我们的结果为对齐实践者提供了新的算法工具和实证洞见。

    Offline preference optimization allows fine-tuning large models directly from offline data, and has proved effective in recent alignment practices. We propose generalized preference optimization (GPO), a family of offline losses parameterized by a general class of convex functions. GPO enables a unified view over preference optimization, encompassing existing algorithms such as DPO, IPO and SLiC as special cases, while naturally introducing new variants. The GPO framework also sheds light on how offline algorithms enforce regularization, through the design of the convex function that defines the loss. Our analysis and experiments reveal the connections and subtle differences between the offline regularization and the KL divergence regularization intended by the canonical RLHF formulation. In all, our results present new algorithmic toolkits and empirical insights to alignment practitioners.
    
[^17]: 通过并行观测预测改进基于令牌的世界模型

    Improving Token-Based World Models with Parallel Observation Prediction

    [https://arxiv.org/abs/2402.05643](https://arxiv.org/abs/2402.05643)

    该论文提出了一种改进基于令牌的世界模型的方法，通过引入并行观测预测机制（POP）来解决想象过程中出现的瓶颈问题。通过在一个新型TBWM代理中应用POP，想象速度提高了15.4倍，在不到12小时的训练时间内在Atari 100K基准测试中取得了超人类的表现。

    

    受到将Transformer应用于离散符号序列的成功启发，最近提出了基于令牌的世界模型（TBWMs）作为高效样本方法。在TBWMs中，世界模型将代理经验作为一种类似语言的令牌序列进行消耗，其中每个观测构成一个子序列。然而，在想象过程中，通过令牌逐个生成下一个观测的串行方式导致了严重的瓶颈问题，导致训练时间长、GPU利用率低和表示能力有限。为了解决这个瓶颈问题，我们设计了一种新颖的并行观测预测（POP）机制。POP通过一种针对我们的强化学习环境设计的新型前向模式来扩充了保持网络（RetNet）。我们将POP集成到一种名为REM（保持环境模型）的新型TBWM代理中，展示了比以前的TBWMs快15.4倍的想象能力。REM在Atari 100K基准测试的26个游戏中的12个游戏中达到超越人类水平的性能，并且在不到12小时的训练时间内完成训练。

    Motivated by the success of Transformers when applied to sequences of discrete symbols, token-based world models (TBWMs) were recently proposed as sample-efficient methods. In TBWMs, the world model consumes agent experience as a language-like sequence of tokens, where each observation constitutes a sub-sequence. However, during imagination, the sequential token-by-token generation of next observations results in a severe bottleneck, leading to long training times, poor GPU utilization, and limited representations. To resolve this bottleneck, we devise a novel Parallel Observation Prediction (POP) mechanism. POP augments a Retentive Network (RetNet) with a novel forward mode tailored to our reinforcement learning setting. We incorporate POP in a novel TBWM agent named REM (Retentive Environment Model), showcasing a 15.4x faster imagination compared to prior TBWMs. REM attains superhuman performance on 12 out of 26 games of the Atari 100K benchmark, while training in less than 12 hours.
    
[^18]: 一张图值千言：使用纯Transformer将图形欧拉化

    A Graph is Worth $K$ Words: Euclideanizing Graph using Pure Transformer

    [https://arxiv.org/abs/2402.02464](https://arxiv.org/abs/2402.02464)

    这篇论文介绍了GraphsGPT，它使用纯Transformer将非欧几里德图形转换为在欧几里德空间中可学习的图形单词，并通过解码器将图形单词重新构建为原始图形，保证了信息的等价性。预训练的GraphsGPT在图形表示学习和图形生成方面取得了突出成果。

    

    我们能否将非欧几里德图形建模为纯语言甚至欧几里德向量，同时保留其固有信息？非欧几里德性质一直是图形建模中的长期挑战。尽管最近的GNN和Graphformer努力将图形编码为欧几里德向量，但从向量中恢复出原始图形仍然是一个挑战。我们引入了GraphsGPT，它具有一个将非欧几里德图形转换为在欧几里德空间中可学习图形单词的Graph2Seq编码器，以及一个从图形单词重构原始图形以确保信息等价性的GraphGPT解码器。我们在100M个分子上预训练了GraphsGPT，并得到一些有趣的发现：(1) 预训练的Graph2Seq在图形表示学习方面表现出色，在8/9个图形分类和回归任务上取得了最新成果。(2) 预训练的GraphGPT作为一个强大的图形生成器，其能够进行无条件和有条件的图形生成。(3) Graph2Seq+Gr

    Can we model non-Euclidean graphs as pure language or even Euclidean vectors while retaining their inherent information? The non-Euclidean property have posed a long term challenge in graph modeling. Despite recent GNN and Graphformer efforts encoding graphs as Euclidean vectors, recovering original graph from the vectors remains a challenge. We introduce GraphsGPT, featuring a Graph2Seq encoder that transforms non-Euclidean graphs into learnable graph words in a Euclidean space, along with a GraphGPT decoder that reconstructs the original graph from graph words to ensure information equivalence. We pretrain GraphsGPT on 100M molecules and yield some interesting findings: (1) Pretrained Graph2Seq excels in graph representation learning, achieving state-of-the-art results on 8/9 graph classification and regression tasks. (2) Pretrained GraphGPT serves as a strong graph generator, demonstrated by its ability to perform both unconditional and conditional graph generation. (3) Graph2Seq+Gr
    
[^19]: 为大型语言模型构建防护措施

    Building Guardrails for Large Language Models

    [https://arxiv.org/abs/2402.01822](https://arxiv.org/abs/2402.01822)

    本文旨在为大型语言模型构建防护措施，并倡导采用系统化方法，通过与多学科团队合作来确定精确的技术要求，以减轻LLM的风险，并全面考虑不同LLM应用的多样化上下文。

    

    随着大型语言模型（LLM）越来越多地融入我们的日常生活中，识别和减轻它们的风险变得至关重要，特别是当这些风险对人类用户和社会产生深远影响时。防护措施，即过滤LLM的输入或输出，已经成为一种核心的安全技术。本文深入研究了当前的开源解决方案（Llama Guard，Nvidia NeMo，Guardrails AI），讨论了构建更完整解决方案的挑战和路径。基于前期研究的有力证据，我们倡导采用系统化方法构建LLM的防护措施，全面考虑不同LLM应用的多样化上下文。我们建议通过与多学科团队的合作，采用社会技术方法来确定精确的技术要求，探索面向需求复杂性的先进神经符号实现，并开展验证和测试。

    As Large Language Models (LLMs) become more integrated into our daily lives, it is crucial to identify and mitigate their risks, especially when the risks can have profound impacts on human users and societies. Guardrails, which filter the inputs or outputs of LLMs, have emerged as a core safeguarding technology. This position paper takes a deep look at current open-source solutions (Llama Guard, Nvidia NeMo, Guardrails AI), and discusses the challenges and the road towards building more complete solutions. Drawing on robust evidence from previous research, we advocate for a systematic approach to construct guardrails for LLMs, based on comprehensive consideration of diverse contexts across various LLMs applications. We propose employing socio-technical methods through collaboration with a multi-disciplinary team to pinpoint precise technical requirements, exploring advanced neural-symbolic implementations to embrace the complexity of the requirements, and developing verification and t
    
[^20]: InstructRetro: 检索增强的预训练中指令调优

    InstructRetro: Instruction Tuning post Retrieval-Augmented Pretraining

    [https://arxiv.org/abs/2310.07713](https://arxiv.org/abs/2310.07713)

    InstructRetro是目前规模最大的使用检索预训练的LLM，扩展了基础模型Retro 48B，通过指令调优在各种零样例任务上取得显著改进。

    

    使用检索增强技术对自回归大型语言模型（LLM）进行预训练可以提高困惑度和事实准确性。然而，现有的预训练检索增强LLM的规模仍然有限（如Retro具有75亿个参数），这限制了指令调优和零样例泛化的效果。本文介绍了Retro 48B，这是目前规模最大的使用检索预训练的LLM。具体来说，我们使用检索技术从1.2万亿个标记中继续预训练一个43B的GPT模型，并借助Retro方法将其扩展到4800亿个参数。值得注意的是，所得到的基础模型Retro 48B在困惑度方面显著优于仅使用1.2万亿个标记进行训练的43B GPT模型，且只增加了2.58%的GPU使用时间，展示了该方法的显著扩展潜力。在对Retro进行指令调优后，InstructRetro在各种零样例任务上表现出显著的改进。

    Pretraining auto-regressive large language models (LLMs) with retrieval demonstrates better perplexity and factual accuracy by leveraging external databases. However, the size of existing pretrained retrieval-augmented LLM is still limited (e.g., Retro has 7.5B parameters), which limits the effectiveness of instruction tuning and zero-shot generalization. In this work, we introduce Retro 48B, the largest LLM pretrained with retrieval. Specifically, we continue to pretrain a 43B GPT model on additional 100 billion tokens using the Retro augmentation method by retrieving from 1.2 trillion tokens. Notably, the obtained foundation model, Retro 48B, largely outperforms the counterpart GPT 43B trained on 1.2T tokens in terms of perplexity with only 2.58% additional GPU hours, demonstrating the significant scaling potential of the method. After instruction tuning on Retro, InstructRetro demonstrates significant improvement over the instruction tuned GPT on a wide range of zero-shot tasks. Spe
    
[^21]: 黑盒访问对于严格的人工智能审计是不充分的

    Black-Box Access is Insufficient for Rigorous AI Audits. (arXiv:2401.14446v1 [cs.CY])

    [http://arxiv.org/abs/2401.14446](http://arxiv.org/abs/2401.14446)

    本文探讨了黑盒审计的局限性以及白盒和超越框架审计的优势，黑盒访问对于严格的人工智能审计是不充分的。

    

    外部对人工智能系统的审计被越来越多地认为是人工智能治理的一个关键机制。审计的有效性取决于审计员被授予的系统访问程度。近期对最先进的人工智能系统的审计主要依赖于黑盒访问，审计员只能查询系统并观察其输出。然而，对系统内部工作方式（例如权重、激活、梯度）的透明访问允许审计员进行更强的攻击，更全面地解释模型，并进行精细调整。同时，对其培训和部署信息的超越框架访问（例如方法论、代码、文档、超参数、数据、部署细节、内部评估结果）允许审计员审查开发过程，并设计更具针对性的评估。在本文中，我们研究了黑盒审计的局限性以及白盒和超越框架审计的优势。我们还讨论了技术和生理上的优势。

    External audits of AI systems are increasingly recognized as a key mechanism for AI governance. The effectiveness of an audit, however, depends on the degree of system access granted to auditors. Recent audits of state-of-the-art AI systems have primarily relied on black-box access, in which auditors can only query the system and observe its outputs. However, white-box access to the system's inner workings (e.g., weights, activations, gradients) allows an auditor to perform stronger attacks, more thoroughly interpret models, and conduct fine-tuning. Meanwhile, outside-the-box access to its training and deployment information (e.g., methodology, code, documentation, hyperparameters, data, deployment details, findings from internal evaluations) allows for auditors to scrutinize the development process and design more targeted evaluations. In this paper, we examine the limitations of black-box audits and the advantages of white- and outside-the-box audits. We also discuss technical, physi
    
[^22]: 大语言模型时代的进化计算：调查与路线图

    Evolutionary Computation in the Era of Large Language Model: Survey and Roadmap. (arXiv:2401.10034v1 [cs.NE])

    [http://arxiv.org/abs/2401.10034](http://arxiv.org/abs/2401.10034)

    该论文调查了大语言模型和进化计算之间的相互作用，并提出了在黑盒设置下进一步提升大语言模型性能的优化框架，以及将大语言模型与进化算法结合应用于各种任务的方法。

    

    大型语言模型（LLMs）是基于Transformer架构，在多样的数据上进行大规模预训练的，它们不仅在自然语言处理领域引起了革命，还将其能力扩展到了各个领域，迈向了人工通用智能的重要一步。尽管进化算法（EAs）与LLMs在目标和方法论上存在差异，但它们之间的相互作用揭示了有趣的相似之处，特别是在他们共同的优化性质、黑盒特性和处理复杂问题的能力方面。与此同时，进化算法不仅可以为LLM在黑盒设置下提供优化框架，还可以在应用中为LLM赋予灵活的全局搜索和迭代机制。另一方面，LLM丰富的领域知识使得进化算法可以进行更智能的搜索，而其文本处理能力则有助于将进化算法应用于各种任务。基于它们的互补优势，本文提出了一份调查和路线图。

    Large Language Models (LLMs), built upon Transformer-based architectures with massive pretraining on diverse data, have not only revolutionized natural language processing but also extended their prowess to various domains, marking a significant stride towards artificial general intelligence. The interplay between LLMs and Evolutionary Algorithms (EAs), despite differing in objectives and methodologies, reveals intriguing parallels, especially in their shared optimization nature, black-box characteristics, and proficiency in handling complex problems. Meanwhile, EA can not only provide an optimization framework for LLM's further enhancement under black-box settings but also empower LLM with flexible global search and iterative mechanism in applications. On the other hand, LLM's abundant domain knowledge enables EA to perform smarter searches, while its text processing capability assist in deploying EA across various tasks. Based on their complementary advantages, this paper presents a 
    
[^23]: 大型语言模型中的代码模拟挑战

    Code Simulation Challenges for Large Language Models. (arXiv:2401.09074v1 [cs.LG])

    [http://arxiv.org/abs/2401.09074](http://arxiv.org/abs/2401.09074)

    大型语言模型在模拟计算机代码和算法执行方面遇到挑战，性能随着代码长度的增加而迅速下降。在处理短程序或标准过程时，它们能以低错误率按顺序执行指令，但对于复杂的程序，特别是包含关键路径和冗余指令的程序，模拟效果较差。我们提出了一种逐行模拟代码执行的方法来解决这个问题。

    

    我们调查了大型语言模型（LLMs）在模拟计算机代码和算法执行方面的能力。我们首先研究了直线程序，并展示了当前LLMs在处理这样简单的程序时表现出的性能较差——性能随着代码长度的增加而迅速下降。接着，我们研究了LLMs在模拟包含关键路径和冗余指令的程序方面的能力。我们还通过排序算法和嵌套循环超越了直线程序的模拟，并展示了程序的计算复杂性直接影响LLMs模拟其执行的能力。我们观察到LLMs只有在处理短程序或标准过程时才能以低错误率按顺序执行指令。LLMs的代码模拟与它们的模式识别和记忆能力存在矛盾：在记忆对任务有害的情况下，我们提出了一种新的提示方法，逐行模拟代码的执行。

    We investigate the extent to which Large Language Models (LLMs) can simulate the execution of computer code and algorithms. We begin by looking straight line programs, and show that current LLMs demonstrate poor performance even with such simple programs -- performance rapidly degrades with the length of code. We then investigate the ability of LLMs to simulate programs that contain critical paths and redundant instructions. We also go beyond straight line program simulation with sorting algorithms and nested loops, and we show the computational complexity of a routine directly affects the ability of an LLM to simulate its execution. We observe that LLMs execute instructions sequentially and with a low error margin only for short programs or standard procedures. LLMs' code simulation is in tension with their pattern recognition and memorisation capabilities: on tasks where memorisation is detrimental, we propose a novel prompting method to simulate code execution line by line. Empirica
    
[^24]: SynHIN: 生成用于可解释人工智能的合成异构信息网络

    SynHIN: Generating Synthetic Heterogeneous Information Network for Explainable AI. (arXiv:2401.04133v1 [cs.LG])

    [http://arxiv.org/abs/2401.04133](http://arxiv.org/abs/2401.04133)

    该论文提出了一种生成合成异构信息网络的方法，用于可解释人工智能。该方法通过识别现实世界数据集中的模式，构建合成网络，并确保生成的合成图数据与真实数据接近。这提供了用于节点分类任务的合成异构图数据集。

    

    图神经网络在各个领域有着优秀的表现，从检测电子商务垃圾邮件到社交网络分类问题。然而，缺乏公共图数据集阻碍了研究进展，尤其是在异构信息网络（HIN）方面。由于图神经网络解释模型的进展，对于公平的HIN比较而言，需要数据集的需求越来越大。为此，我们提出了SynHIN，一种生成合成异构信息网络的独特方法。SynHIN识别现实世界数据集中的模式，总结图统计数据，并构建合成网络。我们的方法利用In-Cluster和Out-Cluster Merge模块从主要的模式集群构建合成HIN。在In/Out-Cluster合并和符合真实数据集约束的后修剪过程后，我们确保合成的图统计数据与参考数据接近。SynHIN生成用于节点分类任务的合成异构图数据集，使用主要的模式作为输入。

    Graph Neural Networks (GNNs) excel in various domains, from detecting e-commerce spam to social network classification problems. However, the lack of public graph datasets hampers research progress, particularly in heterogeneous information networks (HIN). The demand for datasets for fair HIN comparisons is growing due to advancements in GNN interpretation models. In response, we propose SynHIN, a unique method for generating synthetic heterogeneous information networks. SynHIN identifies motifs in real-world datasets, summarizes graph statistics, and constructs a synthetic network. Our approach utilizes In-Cluster and Out-Cluster Merge modules to build the synthetic HIN from primary motif clusters. After In/Our-Cluster mergers and a post-pruning process fitting the real dataset constraints, we ensure the synthetic graph statistics align closely with the reference one. SynHIN generates a synthetic heterogeneous graph dataset for node classification tasks, using the primary motif as the
    
[^25]: 概念瓶颈模型是否遵循局部性？

    Do Concept Bottleneck Models Obey Locality?. (arXiv:2401.01259v1 [cs.LG])

    [http://arxiv.org/abs/2401.01259](http://arxiv.org/abs/2401.01259)

    本文研究了概念瓶颈模型（CBMs）是否能够正确捕捉到概念之间的条件独立程度，通过分析对于概念局部性之外特征的变化如何影响概念的预测。

    

    概念基础学习通过解释其预测结果使用人可理解的概念，改善了深度学习模型的可解释性。在这种范式下训练的深度学习模型严重依赖于神经网络能够学习独立于其他概念的给定概念的存在或不存在。然而，最近的研究强烈暗示这种假设可能在概念瓶颈模型（CBMs）这一典型的基于概念的可解释架构中不能成立。本文中，我们研究了当这些概念既在空间上（通过它们的值完全由固定子集的特征定义）又在语义上（通过它们的值仅与预定义的固定子集的概念相关联）定位时，CBMs是否正确捕捉到概念之间的条件独立程度。为了理解局部性，我们分析了概念之外的特征变化对概念预测的影响。

    Concept-based learning improves a deep learning model's interpretability by explaining its predictions via human-understandable concepts. Deep learning models trained under this paradigm heavily rely on the assumption that neural networks can learn to predict the presence or absence of a given concept independently of other concepts. Recent work, however, strongly suggests that this assumption may fail to hold in Concept Bottleneck Models (CBMs), a quintessential family of concept-based interpretable architectures. In this paper, we investigate whether CBMs correctly capture the degree of conditional independence across concepts when such concepts are localised both spatially, by having their values entirely defined by a fixed subset of features, and semantically, by having their values correlated with only a fixed subset of predefined concepts. To understand locality, we analyse how changes to features outside of a concept's spatial or semantic locality impact concept predictions. Our
    
[^26]: 自动化模型选择算法用于表格数据

    Automated Model Selection for Tabular Data. (arXiv:2401.00961v1 [cs.LG])

    [http://arxiv.org/abs/2401.00961](http://arxiv.org/abs/2401.00961)

    本文介绍了一种自动化模型选择算法，用于表格数据的预测。该算法考虑了特征之间的交互，并包含了基于优先级的随机网格搜索和贪婪搜索两种不同的特征选择方法。

    

    表格数据中的结构化数据包含独特且离散的特征，并且这些特征对目标的重要性各不相同。单个特征的组合可能比简单的单个特征贡献更具预测性和意义。R的混合效应线性模型库允许用户在模型设计中提供这种交互式特征组合。然而，鉴于有许多特征和可能的交互选择，模型选择变得非常困难。我们的目标是通过保持计算成本较小，自动化表格数据预测中的模型选择过程，并同时考虑特征之间的交互。该框架包括两种不同的特征选择方法：基于优先级的随机网格搜索和贪婪搜索方法。基于优先级的方法利用先验概率来引导搜索，高效地探索特征组合。贪婪方法通过迭代地添加特征构建解决方案。

    Structured data in the form of tabular datasets contain features that are distinct and discrete, with varying individual and relative importances to the target. Combinations of one or more features may be more predictive and meaningful than simple individual feature contributions. R's mixed effect linear models library allows users to provide such interactive feature combinations in the model design. However, given many features and possible interactions to select from, model selection becomes an exponentially difficult task. We aim to automate the model selection process for predictions on tabular datasets incorporating feature interactions while keeping computational costs small. The framework includes two distinct approaches for feature selection: a Priority-based Random Grid Search and a Greedy Search method. The Priority-based approach efficiently explores feature combinations using prior probabilities to guide the search. The Greedy method builds the solution iteratively by addin
    
[^27]: 学会拒绝：通过知识范围限制和拒绝机制使大型语言模型更可控和可靠

    Learn to Refuse: Making Large Language Models More Controllable and Reliable through Knowledge Scope Limitation and Refusal Mechanism. (arXiv:2311.01041v1 [cs.CL])

    [http://arxiv.org/abs/2311.01041](http://arxiv.org/abs/2311.01041)

    本文提出了一种学会拒绝（L2R）的简单而有效的解决方案，通过引入拒绝机制，使大型语言模型（LLMs）能够识别和拒绝难以回答的问题，从而提高模型的可控性和可靠性。

    

    大型语言模型（LLMs）展示了令人印象深刻的语言理解和生成能力，使它们能够回答各个领域的广泛问题。然而，这些模型并不完美，经常产生含有错误或错误信息的回答。这些不准确性，通常称为幻觉，使得LLMs在许多场景中不可靠甚至不可用。本文的重点是在LLMs中缓解幻觉问题，特别是在问答环境中。我们探索了一种拒绝机制，指导LLMs拒绝回答具有挑战性的问题以避免错误。我们提出了一个简单而有效的解决方案Learn to Refuse (L2R)，它将拒绝机制纳入到LLMs中，使其能够识别和拒绝那些它们难以回答的问题。为了实现这一点，我们利用结构化知识库来表示所有LLMs所需要的知识。

    Large language models (LLMs) have demonstrated impressive language understanding and generation capabilities, enabling them to answer a wide range of questions across various domains. However, these models are not flawless and often produce responses that contain errors or misinformation. These inaccuracies, commonly referred to as hallucinations, render LLMs unreliable and even unusable in many scenarios. In this paper, our focus is on mitigating the issue of hallucination in LLMs, particularly in the context of question-answering. Instead of attempting to answer all questions, we explore a refusal mechanism that instructs LLMs to refuse to answer challenging questions in order to avoid errors. We then propose a simple yet effective solution called Learn to Refuse (L2R), which incorporates the refusal mechanism to enable LLMs to recognize and refuse to answer questions that they find difficult to address. To achieve this, we utilize a structured knowledge base to represent all the LLM
    
[^28]: 通过模型适应来去除偏见算法

    Debiasing Algorithm through Model Adaptation. (arXiv:2310.18913v1 [cs.CL])

    [http://arxiv.org/abs/2310.18913](http://arxiv.org/abs/2310.18913)

    本论文提出了一种通过模型适应来检测和减轻语言模型中性别偏见的方法，并证明了该方法能够显著减少偏见同时保持模型性能。

    

    大型语言模型正在成为各种语言任务的首选解决方案。然而，随着容量的增长，模型很容易依赖训练数据中存在的偏见和刻板印象所产生的虚假相关性。本研究提出了一种新颖的方法来检测和减轻语言模型中的性别偏见。我们进行因果分析，以识别问题模型组件，并发现中上层前馈层最容易传递偏见。根据分析结果，我们通过线性投影将这些层乘以模型进行适应。我们的方法DAMA通过各种度量指标明显减少了偏见，同时保持模型在后续任务中的性能。我们发布了我们的方法和模型的代码，通过重新训练，保持了LLaMA的最先进性能，同时偏见显著减少。

    Large language models are becoming the go-to solution for various language tasks. However, with growing capacity, models are prone to rely on spurious correlations stemming from biases and stereotypes present in the training data. This work proposes a novel method for detecting and mitigating gender bias in language models. We perform causal analysis to identify problematic model components and discover that mid-upper feed-forward layers are most prone to convey biases. Based on the analysis results, we adapt the model by multiplying these layers by a linear projection. Our titular method, DAMA, significantly decreases bias as measured by diverse metrics while maintaining the model's performance on downstream tasks. We release code for our method and models, which retrain LLaMA's state-of-the-art performance while being significantly less biased.
    
[^29]: 一种由语义通信增强的无线AI生成内容（AIGC）供应框架

    A Wireless AI-Generated Content (AIGC) Provisioning Framework Empowered by Semantic Communication. (arXiv:2310.17705v1 [cs.NI])

    [http://arxiv.org/abs/2310.17705](http://arxiv.org/abs/2310.17705)

    一种由语义通信增强的无线AI生成内容（AIGC）供应框架，通过使用语义信息而不是所有的二进制位提取和传输内容，以解决在无线网络中提供最优AIGC服务的挑战。

    

    近期，生成式AI应用通过创建多样化且高质量的AI生成内容（AIGC）来满足广大用户群体的需求。随着移动设备的普及和移动流量的快速增长，通过无线通信网络提供对高质量AIGC服务的无处不在的访问已成为AIGC产品的未来方向。然而，在不稳定的信道、有限的带宽资源和分布不均匀的计算资源的无线网络中提供最优的AIGC服务是具有挑战性的。为了解决这些挑战，我们提出了一个由语义通信（SemCom）增强的AIGC（SemAIGC）生成和传输框架，其中只需提取和传输内容的语义信息而不是所有的二进制位。具体而言，SemAIGC在语义编码器和解码器中集成了基于扩散的模型，以实现高效的内容生成和灵活调整计算工作负载的目的。

    Generative AI applications are recently catering to a vast user base by creating diverse and high-quality AI-generated content (AIGC). With the proliferation of mobile devices and rapid growth of mobile traffic, providing ubiquitous access to high-quality AIGC services via wireless communication networks is becoming the future direction for AIGC products. However, it is challenging to provide optimal AIGC services in wireless networks with unstable channels, limited bandwidth resources, and unevenly distributed computational resources. To tackle these challenges, we propose a semantic communication (SemCom)-empowered AIGC (SemAIGC) generation and transmission framework, where only semantic information of the content rather than all the binary bits should be extracted and transmitted by using SemCom. Specifically, SemAIGC integrates diffusion-based models within the semantic encoder and decoder for efficient content generation and flexible adjustment of the computing workload of both tr
    
[^30]: ACES: 使用自我目标语言模型和语义描述符生成多样的编程难题

    ACES: generating diverse programming puzzles with autotelic language models and semantic descriptors. (arXiv:2310.10692v1 [cs.LG])

    [http://arxiv.org/abs/2310.10692](http://arxiv.org/abs/2310.10692)

    ACES是一种使用自我目标语言模型和语义描述符生成多样化的编程难题的方法，能够优化有趣的多样性和少样本生成。

    

    寻找和选择新颖有趣的问题是好奇心、科学和创新的核心。在Python编程难题的无限空间中，我们研究了自动问题生成。现有的生成模型通常旨在建模参考分布，没有明确的多样性优化。其他方法在有限的手工编码表示空间或不可解释的学习嵌入空间中明确优化多样性，这些嵌入空间可能与人类对有趣变化的感知不符。通过ACES（自我目标代码探索与语义描述符），我们引入了一种新的自我目标生成方法，利用大型语言模型（LLM）生成语义描述符，直接优化有趣的多样性，以及少样本生成。每个难题都标记有10个维度，每个维度捕捉了解决它所需的编程技能。ACES生成并追求新颖可行的目标。

    Finding and selecting new and interesting problems to solve is at the heart of curiosity, science and innovation. We here study automated problem generation in the context of the open-ended space of python programming puzzles. Existing generative models often aim at modeling a reference distribution without any explicit diversity optimization. Other methods explicitly optimizing for diversity do so either in limited hand-coded representation spaces or in uninterpretable learned embedding spaces that may not align with human perceptions of interesting variations. With ACES (Autotelic Code Exploration via Semantic descriptors), we introduce a new autotelic generation method that leverages semantic descriptors produced by a large language model (LLM) to directly optimize for interesting diversity, as well as few-shot-based generation. Each puzzle is labeled along 10 dimensions, each capturing a programming skill required to solve it. ACES generates and pursues novel and feasible goals to 
    
[^31]: ParFam - 基于连续全局优化的符号回归

    ParFam -- Symbolic Regression Based on Continuous Global Optimization. (arXiv:2310.05537v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2310.05537](http://arxiv.org/abs/2310.05537)

    ParFam是一种新的符号回归方法，利用参数化的符号函数族将离散问题转化为连续问题，并结合全局优化器，能够有效解决符号回归问题。

    

    符号回归（SR）问题在许多不同的应用中出现，比如从给定数据中识别物理定律或推导描述金融市场行为的数学方程。目前存在多种解决SR问题的方法，通常基于遗传编程。然而，这些方法通常非常复杂，需要大量超参数调整和计算资源。本文介绍了我们提出的新方法ParFam，它利用适合的符号函数的参数化族将离散的符号回归问题转化为连续问题，相比当前最先进的方法，这种方法的设置更加直观。结合强大的全局优化器，这种方法可以有效地解决SR问题。此外，它可以轻松扩展到更高级的算法，例如添加深度神经网络以找到适合的参数化族。我们证明了这种方法的性能。

    The problem of symbolic regression (SR) arises in many different applications, such as identifying physical laws or deriving mathematical equations describing the behavior of financial markets from given data. Various methods exist to address the problem of SR, often based on genetic programming. However, these methods are usually quite complicated and require a lot of hyperparameter tuning and computational resources. In this paper, we present our new method ParFam that utilizes parametric families of suitable symbolic functions to translate the discrete symbolic regression problem into a continuous one, resulting in a more straightforward setup compared to current state-of-the-art methods. In combination with a powerful global optimizer, this approach results in an effective method to tackle the problem of SR. Furthermore, it can be easily extended to more advanced algorithms, e.g., by adding a deep neural network to find good-fitting parametric families. We prove the performance of 
    
[^32]: 持续对比式语音理解

    Continual Contrastive Spoken Language Understanding. (arXiv:2310.02699v1 [eess.AS])

    [http://arxiv.org/abs/2310.02699](http://arxiv.org/abs/2310.02699)

    COCONUT是一种类别增量学习方法，结合了经验重播和对比式学习，在语音理解领域中解决了模型在持续学习新任务时难以保持之前知识的问题。

    

    最近，神经网络在各个领域取得了令人印象深刻的进展，其中包括语音处理。然而，这个领域的最新突破通常需要使用大规模数据集和庞大的计算资源进行离线训练。不幸的是，这些模型在持续学习新任务时往往难以保持之前获得的知识，并且重新训练几乎总是不可行的。在本文中，我们研究了一种在类别增量学习（CIL）设置下学习序列到序列模型用于语音理解的问题，并提出了一种称为COCONUT的CIL方法，该方法依赖于经验重播和对比式学习的组合。通过对仅对回放样本应用改进版本的标准有监督对比损失，COCONUT通过将同一类别的样本拉近并将其他样本推开，保留了学习到的表示。此外，我们利用了一种多模态对比损失。

    Recently, neural networks have shown impressive progress across diverse fields, with speech processing being no exception. However, recent breakthroughs in this area require extensive offline training using large datasets and tremendous computing resources. Unfortunately, these models struggle to retain their previously acquired knowledge when learning new tasks continually, and retraining from scratch is almost always impractical. In this paper, we investigate the problem of learning sequence-to-sequence models for spoken language understanding in a class-incremental learning (CIL) setting and we propose COCONUT, a CIL method that relies on the combination of experience replay and contrastive learning. Through a modified version of the standard supervised contrastive loss applied only to the rehearsal samples, COCONUT preserves the learned representations by pulling closer samples from the same class and pushing away the others. Moreover, we leverage a multimodal contrastive loss that
    
[^33]: Sherlock Holmes不玩骰子：Evidence Theory对社会与生命科学的意义

    Sherlock Holmes Doesn't Play Dice: The significance of Evidence Theory for the Social and Life Sciences. (arXiv:2309.03222v1 [cs.AI])

    [http://arxiv.org/abs/2309.03222](http://arxiv.org/abs/2309.03222)

    本文强调了Evidence Theory在社会与生命科学中的潜力，可以表达来自未确定事件可能实现的不确定性，而与之对比的Probability Theory只能限于决策者当前设想的可能性。本文还讨论了Evidence Theory与Probability Theory的关系以及Evidence Theory在信息论中的应用增强效果，并通过审计练习案例进一步说明了Evidence Theory的应用。

    

    虽然Evidence Theory（Demster-Shafer Theory，Belief Functions Theory）在数据融合中得到越来越多的应用，但其在社会与生命科学中的潜力常常被人们对其独特特点的缺乏认识所掩盖。本文强调，Evidence Theory可以表达来自于人们未能确定的事件可能实现的不确定性，而Probability Theory必须仅限于决策者当前所设想的可能性。随后，我们说明了Dempster-Shafer的组合规则如何与各种版本的Probability Theory的贝叶斯定理相关，并讨论了哪些信息论的应用可以通过Evidence Theory得到增强。最后，我们通过一个案例阐述了使用Evidence Theory来理解审计练习中出现的部分重叠、部分相互矛盾的解决方案的情况。

    While Evidence Theory (Demster-Shafer Theory, Belief Functions Theory) is being increasingly used in data fusion, its potentialities in the Social and Life Sciences are often obscured by lack of awareness of its distinctive features. With this paper we stress that Evidence Theory can express the uncertainty deriving from the fear that events may materialize, that one has not been able to figure out. By contrast, Probability Theory must limit itself to the possibilities that a decision-maker is currently envisaging.  Subsequently, we illustrate how Dempster-Shafer's combination rule relates to Bayes' Theorem for various versions of Probability Theory and discuss which applications of Information Theory can be enhanced by Evidence Theory. Finally, we illustrate our claims with an example where Evidence Theory is used to make sense of the partially overlapping, partially contradictory solutions that appear in an auditing exercise.
    
[^34]: 在资源受限的边缘设备上通过动态专家交换为MoE模型提供服务

    Serving MoE Models on Resource-constrained Edge Devices via Dynamic Expert Swapping. (arXiv:2308.15030v1 [cs.AI])

    [http://arxiv.org/abs/2308.15030](http://arxiv.org/abs/2308.15030)

    本文提出了一种在资源受限的边缘设备上通过动态专家交换为MoE模型提供服务的推理框架，该框架通过分析MoE模型的行为模式，引入了新的数据结构来减少资源消耗，并通过性能分析优化参数配置。

    

    混合专家(MoE)是深度学习中一种流行的技术，通过有条件激活的并行神经网络模块(专家)提升了模型的容量。然而，在资源受限的延迟关键的边缘场景中提供MoE模型的服务是一项具有挑战性的任务，因为模型的大小和复杂性显著增加。本文首先分析了MoE模型在连续推理场景中的行为模式，得出了关于专家激活的三个关键观察结果，包括时间局部性、可交换性和可跳过计算。基于这些观察，我们引入了PC-MoE，一种适用于资源受限的连续MoE模型服务的推理框架。PC-MoE的核心是一种新的数据结构，称为“参数委员会”，它智能地维护一部分重要的正在使用的专家，以减少资源消耗。通过基于性能分析的委员会规划器，在线找到参数委员会的最佳配置，并进行专家交换和请求处理。

    Mixture of experts (MoE) is a popular technique in deep learning that improves model capacity with conditionally-activated parallel neural network modules (experts). However, serving MoE models in resource-constrained latency-critical edge scenarios is challenging due to the significantly increased model size and complexity. In this paper, we first analyze the behavior pattern of MoE models in continuous inference scenarios, which leads to three key observations about the expert activations, including temporal locality, exchangeability, and skippable computation. Based on these observations, we introduce PC-MoE, an inference framework for resource-constrained continuous MoE model serving. The core of PC-MoE is a new data structure, Parameter Committee, that intelligently maintains a subset of important experts in use to reduce resource consumption. The optimal configuration of Parameter Committee is found offline by a profiling-guided committee planner, and expert swapping and request 
    
[^35]: 将关注点转移到相关性上: 探索大型语言模型的不确定性估计

    Shifting Attention to Relevance: Towards the Uncertainty Estimation of Large Language Models. (arXiv:2307.01379v1 [cs.CL])

    [http://arxiv.org/abs/2307.01379](http://arxiv.org/abs/2307.01379)

    本论文研究了大型语言模型（LLMs）自动生成的关键词不平等问题，发现在估计不确定性时，重要的令牌和含有有限语义的句子被同等或更加重视。为了解决这个问题，提出了共同转移关注点来更好地估计不确定性。

    

    虽然大型语言模型（LLMs）在自然语言生成方面表现出了巨大的潜力，但是对于模型生成的不确定性的特征化仍然具有挑战性，即用户何时可以信任模型的输出。我们的研究基于一些启发性的事实，即在自回归的LLMs中，令牌在反映生成的含义方面是不平等的，即一些令牌比其他令牌更相关（或更具代表性），然而在估计不确定性时所有的令牌被等值对待。这是由于语言冗余，其中大部分情况下，只需要几个关键词就足以传达一个长句的含义。我们将这些不平等称为生成的不平等，并研究它们如何影响不确定性的估计。我们的结果揭示，相当数量的令牌和包含有限语义的句子，在估计不确定性时被同等或甚至更加重视。为了解决由生成的不平等引起的这些偏差，我们提出了共同转移关注点来更好地估计不确定性。

    Although Large Language Models (LLMs) have shown great potential in Natural Language Generation, it is still challenging to characterize the uncertainty of model generations, i.e., when users could trust model outputs. Our research is derived from the heuristic facts that tokens are created unequally in reflecting the meaning of generations by auto-regressive LLMs, i.e., some tokens are more relevant (or representative) than others, yet all the tokens are equally valued when estimating uncertainty. It is because of the linguistic redundancy where mostly a few keywords are sufficient to convey the meaning of a long sentence. We name these inequalities as generative inequalities and investigate how they affect uncertainty estimation. Our results reveal that considerable tokens and sentences containing limited semantics are weighted equally or even heavily when estimating uncertainty. To tackle these biases posed by generative inequalities, we propose to jointly Shifting Attention to more
    
[^36]: 基于分解类激活图的特征级解释方法：Decom-CAM

    Decom--CAM: Tell Me What You See, In Details! Feature-Level Interpretation via Decomposition Class Activation Map. (arXiv:2306.04644v1 [cs.CV])

    [http://arxiv.org/abs/2306.04644](http://arxiv.org/abs/2306.04644)

    本文提出了一种名为Decom-CAM的两阶段可解释性方法，可用于对深度学习模型的预测结果进行特征级解释。实验结果表明该方法可有效定位重要区域和显著特征，并改善了模型的决策质量。

    

    解释深度学习模型一直是一个非常具有挑战性的问题。虽然类激活图（CAM）被广泛用于通过突出对象位置来解释深度模型的预测结果，但它未能提供有关模型用于做出决策的显著特征的见解。此外，现有的评估协议常常忽略了可解释性表现与模型决策质量之间的关联，这是一个更基本的问题。本文提出了一种新的分解类激活图（Decom-CAM）两阶段可解释性方法，用于对模型的预测结果进行特征级解释。Decom-CAM使用奇异值分解将中间激活图分解为正交特征，并通过集成这些特征生成显著性图。特征的正交性使CAM能够捕捉本地特征，并可以用于定位输入图像中的语义组件，如眼睛、鼻子和面部，使其更易于理解和解释。实验结果表明，我们的方法可以有效地定位重要区域和显著特征，在解释准确性方面优于以前的方法，并提高了模型的决策质量。

    Interpretation of deep learning remains a very challenging problem. Although the Class Activation Map (CAM) is widely used to interpret deep model predictions by highlighting object location, it fails to provide insight into the salient features used by the model to make decisions. Furthermore, existing evaluation protocols often overlook the correlation between interpretability performance and the model's decision quality, which presents a more fundamental issue. This paper proposes a new two-stage interpretability method called the Decomposition Class Activation Map (Decom-CAM), which offers a feature-level interpretation of the model's prediction. Decom-CAM decomposes intermediate activation maps into orthogonal features using singular value decomposition and generates saliency maps by integrating them. The orthogonality of features enables CAM to capture local features and can be used to pinpoint semantic components such as eyes, noses, and faces in the input image, making it more 
    
[^37]: 随机坍缩：如何利用梯度噪声使SGD动态趋向更简单的子网络

    Stochastic Collapse: How Gradient Noise Attracts SGD Dynamics Towards Simpler Subnetworks. (arXiv:2306.04251v1 [cs.LG])

    [http://arxiv.org/abs/2306.04251](http://arxiv.org/abs/2306.04251)

    SGD在训练过度表达的网络时，会随机地将动态吸引到更简单的子网络，这种随机吸引性能够提高泛化能力。

    

    本文揭示了随机梯度下降（SGD）的一个强烈隐式偏好，它将过度表达的网络驱动到更简单的子网络，从而大大减少了独立参数的数量，并提高了泛化能力。为了揭示这个偏好，我们识别了不变集，或者说是SGD未修改的参数空间的子集。我们专注于两类不变集，它们对应于现代架构中常见的更简单的子网络。我们的分析揭示了SGD在这些简单不变集方面具有随机吸引性的特性。我们根据损失景观在不变集周围的曲率和随机梯度引入的噪声之间的竞争建立了一种随机吸引性的充分条件。值得注意的是，我们发现增加噪声水平会增强吸引力，导致与鞍点或训练损失的局部极大值相关的吸引不变集的出现。

    In this work, we reveal a strong implicit bias of stochastic gradient descent (SGD) that drives overly expressive networks to much simpler subnetworks, thereby dramatically reducing the number of independent parameters, and improving generalization. To reveal this bias, we identify invariant sets, or subsets of parameter space that remain unmodified by SGD. We focus on two classes of invariant sets that correspond to simpler subnetworks and commonly appear in modern architectures. Our analysis uncovers that SGD exhibits a property of stochastic attractivity towards these simpler invariant sets. We establish a sufficient condition for stochastic attractivity based on a competition between the loss landscape's curvature around the invariant set and the noise introduced by stochastic gradients. Remarkably, we find that an increased level of noise strengthens attractivity, leading to the emergence of attractive invariant sets associated with saddle-points or local maxima of the train loss.
    
[^38]: 深思熟虑：具有内部工作记忆的决策Transformer

    Think Before You Act: Decision Transformers with Internal Working Memory. (arXiv:2305.16338v1 [cs.LG])

    [http://arxiv.org/abs/2305.16338](http://arxiv.org/abs/2305.16338)

    该论文提出了具有内部工作记忆模块的决策Transformer方法，以解决使用大型语言模型的决策代理在处理新任务上性能低下的问题。所提出的方法改善了训练效率和泛化能力，并进一步增强了转化决策制定代理对新任务的适应性。

    

    基于大型语言模型（LLM）的决策制定代理已经展示了跨越多个任务的泛化能力。然而，它们的性能依赖于大规模的数据和计算。我们认为，这种低效性源于遗忘现象，即模型通过参数记忆其行为，在训练过程中。因此，新任务的训练可能会降低模型在先前任务上的性能。与LLM的隐式记忆机制不同，人脑利用分布式存储器存储记忆，以有效地管理和组织多种技能，减轻了遗忘现象。因此，我们建议使用内部工作记忆模块来存储、融合和检索不同下游任务的信息。评估结果表明，所提出的方法改善了Atari游戏和元世界物体操作任务的训练效率和泛化能力。此外，我们证明了记忆微调进一步增强了转化决策制定代理对新任务的适应性。

    Large language model (LLM)-based decision-making agents have shown the ability to generalize across multiple tasks. However, their performance relies on massive data and compute. We argue that this inefficiency stems from the forgetting phenomenon, in which a model memorizes its behaviors in parameters throughout training. As a result, training on a new task may deteriorate the model's performance on previous tasks. In contrast to LLMs' implicit memory mechanism, the human brain utilizes distributed memory storage, which helps manage and organize multiple skills efficiently, mitigating the forgetting phenomenon. Thus inspired, we propose an internal working memory module to store, blend, and retrieve information for different downstream tasks. Evaluation results show that the proposed method improves training efficiency and generalization in both Atari games and meta-world object manipulation tasks. Moreover, we demonstrate that memory fine-tuning further enhances the adaptability of t
    
[^39]: UP5: 面向公平性推荐的无偏基础模型

    UP5: Unbiased Foundation Model for Fairness-aware Recommendation. (arXiv:2305.12090v1 [cs.IR])

    [http://arxiv.org/abs/2305.12090](http://arxiv.org/abs/2305.12090)

    本研究提出了一种新颖的基础模型UP5，它采用反事实公平促进技术来消除大型语言模型中的偏见，从而实现面向公平性的推荐。

    

    基于大型语言模型（LLM）等基础模型的最新进展，已将它们推到了推荐系统（RS）的前沿。此外，RS中的公平性很关键，因为许多用户将其用于决策和需求履行。然而，目前尚缺乏对推荐基础模型展示公平性水平和公平处理不同用户群组的适当方法的理解。本文侧重于用户方面的不公平问题，并通过彻底检查表明，LLMs中存在不公平性，导致不公平的推荐结果。为了消除LLM中的偏差以实现面向公平性的推荐，我们引入了一种基于反事实公平促进技术的新型无偏P5（UP5）基础模型。CFP包括两个子模块：个性化前缀提示和Prompt混合，从而增强了个体敏感属性的公平性。

    Recent advancements in foundation models such as large language models (LLM) have propelled them to the forefront of recommender systems (RS). Moreover, fairness in RS is critical since many users apply it for decision-making and demand fulfillment. However, at present, there is a lack of understanding regarding the level of fairness exhibited by recommendation foundation models and the appropriate methods for equitably treating different groups of users in foundation models. In this paper, we focus on user-side unfairness problem and show through a thorough examination that there is unfairness involved in LLMs that lead to unfair recommendation results. To eliminate bias from LLM for fairness-aware recommendation, we introduce a novel Unbiased P5 (UP5) foundation model based on Counterfactually-Fair-Prompting (CFP) techniques. CFP includes two sub-modules: a personalized prefix prompt that enhances fairness with respect to individual sensitive attributes, and a Prompt Mixture that int
    
[^40]: 基于证明数的蒙特卡罗树搜索

    Proof Number Based Monte-Carlo Tree Search. (arXiv:2303.09449v1 [cs.AI])

    [http://arxiv.org/abs/2303.09449](http://arxiv.org/abs/2303.09449)

    本文提出了一种将蒙特卡罗树搜索和证明数搜索相结合的PN-MCTS算法，该算法在多个游戏中均表现出优越的性能，可用于最终走步选择、解决子树以及UCT公式。

    

    本文提出了一种新的游戏搜索算法PN-MCTS，它将蒙特卡罗树搜索（MCTS）和证明数搜索（PNS）相结合。这两种算法已成功应用于各种领域的决策问题。我们定义了三个领域，即最终走步选择，解决子树以及UCT公式，这些领域可以利用在MCTS树中收集到的证明数和证伪数提供的附加知识。我们在不同的时间设置下对所有可能的组合进行了测试，并在几个游戏中与vanilla UCT MCTS进行对决：动作线（$7$$\times$$7$和$8$$\times$$8$），MiniShogi， Knightthrough， Awari和Gomoku。此外，我们对该算法进行扩展，以适当地处理出现平局的游戏，如Awari，通过在MCTS树的顶部添加一个附加的PNS层。实验结果表明，PN-MCTS在6个游戏领域中有5个的胜率优于MCTS（除了Gomoku），其中在动作线游戏中获得了高达96.2%的胜率。

    This paper proposes a new game search algorithm, PN-MCTS, that combines Monte-Carlo Tree Search (MCTS) and Proof-Number Search (PNS). These two algorithms have been successfully applied for decision making in a range of domains. We define three areas where the additional knowledge provided by the proof and disproof numbers gathered in MCTS trees might be used: final move selection, solving subtrees, and the UCT formula. We test all possible combinations on different time settings, playing against vanilla UCT MCTS on several games: Lines of Action ($7$$\times$$7$ and $8$$\times$$8$), MiniShogi, Knightthrough, Awari, and Gomoku. Furthermore, we extend this new algorithm to properly address games with draws, like Awari, by adding an additional layer of PNS on top of the MCTS tree. The experiments show that PN-MCTS confidently outperforms MCTS in 5 out of 6 game domains (all except Gomoku), achieving win rates up to 96.2% for Lines of Action.
    
[^41]: PARIS：用于改善睡眠质量的个性化活动推荐系统

    PARIS: Personalized Activity Recommendation for Improving Sleep Quality. (arXiv:2110.13745v1 [cs.LG] CROSS LISTED)

    [http://arxiv.org/abs/2110.13745](http://arxiv.org/abs/2110.13745)

    该论文利用机器学习技术，结合可穿戴设备监测的数据，通过时间序列聚类找到与指定主题相关的行为模型并生成相应的睡眠质量活动建议，为提高睡眠质量提供了一种个性化解决方案。

    

    睡眠质量对人们的身体和心理健康有深远影响。睡眠不足的人更容易报告身体和心理困扰、活动受限、焦虑和疼痛。此外，过去几年中，活动监测和健康跟踪的应用和设备方兴未艾。从这些可穿戴设备收集到的信号可用于研究和改善睡眠质量。本文利用实体活动和睡眠质量之间的关系，利用机器学习技术找到协助人们改善睡眠的方法。对活动数据进行时间序列聚类，我们找到与特定主题最显着的行为模式相关的簇中心。然后为每个行为模式中的每个簇生成有助于良好睡眠质量的活动建议。这些活动建议供应给每位用户的个性化活动推荐系统。

    The quality of sleep has a deep impact on people's physical and mental health. People with insufficient sleep are more likely to report physical and mental distress, activity limitation, anxiety, and pain. Moreover, in the past few years, there has been an explosion of applications and devices for activity monitoring and health tracking. Signals collected from these wearable devices can be used to study and improve sleep quality. In this paper, we utilize the relationship between physical activity and sleep quality to find ways of assisting people improve their sleep using machine learning techniques. People usually have several behavior modes that their bio-functions can be divided into. Performing time series clustering on activity data, we find cluster centers that would correlate to the most evident behavior modes for a specific subject. Activity recipes are then generated for good sleep quality for each behavior mode within each cluster. These activity recipes are supplied to an a
    

