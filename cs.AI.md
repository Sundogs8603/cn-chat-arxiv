# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Segment Anything.](http://arxiv.org/abs/2304.02643) | 这篇论文介绍了一个新的图像分割任务、模型和数据集，该模型具有高效的分割能力和任务迁移能力，可以在新的图像分布和任务中进行零-shot 任务迁移，使其零-shot表现令人印象深刻，并发布了Segment Anything Model（SAM）和相应的数据集（SA-1B），以促进计算机视觉基础模型研究的发展。 |
| [^2] | [GenPhys: From Physical Processes to Generative Models.](http://arxiv.org/abs/2304.02637) | 本文介绍了一个新的生成模型家族：GenPhys，它将从物理过程中描述的偏微分方程翻译为生成模型，并探索了更广泛的生成模型设计空间。 |
| [^3] | [What Affects Learned Equivariance in Deep Image Recognition Models?.](http://arxiv.org/abs/2304.02628) | 本文研究了神经网络中学习到的等变性，提出了一种改进的等变性测量方法，并发现数据增强、减少模型容量和卷积操作可提高神经网络中学习到的等变性。 |
| [^4] | [Generative Novel View Synthesis with 3D-Aware Diffusion Models.](http://arxiv.org/abs/2304.02602) | 我们提出了一个基于扩散的模型，用于从仅有的一个输入图像生成3D感知的新视角插图，通过整合3D几何先验项，使生成结果更加逼真，能够生成视角一致的新渲染，且具有生成3D一致的序列的能力。 |
| [^5] | [Bayesian neural networks via MCMC: a Python-based tutorial.](http://arxiv.org/abs/2304.02595) | 本文提供了一个基于Python的教程，介绍了贝叶斯神经网络的MCMC方法应用，通过教程使得深度学习开发者能够更好地应用贝叶斯推断进行参数估计和不确定性量化。 |
| [^6] | [Competitive plasticity to reduce the energetic costs of learning.](http://arxiv.org/abs/2304.02594) | 该论文探讨了大脑如何通过竞争性可塑性降低学习的能量成本，并提出了两个省能的可塑性限制算法，结合起来可以大幅降低能量消耗。 |
| [^7] | [Conformal Off-Policy Evaluation in Markov Decision Processes.](http://arxiv.org/abs/2304.02574) | 本论文提出了一种基于合规预测的异策评估方法，能够以一定的确定性水平输出包含目标策略的真实奖励的区间，并提出了不同的处理分布偏移方法，其中一些方法在保证合规性的前提下实现了最先进的性能。 |
| [^8] | [Evaluation of ChatGPT Family of Models for Biomedical Reasoning and Classification.](http://arxiv.org/abs/2304.02496) | 本论文探讨了ChatGPT家族模型在生物医学任务中的性能，虽然Fine-tuned的生物医学NLP任务并未表现出一致的性能增益，但显示出大型语言模型在生物医学应用中超越问答的潜力，需要更定制化的模型设计和Fine-tuning策略。 |
| [^9] | [Quantifying the Roles of Visual, Linguistic, and Visual-Linguistic Complexity in Verb Acquisition.](http://arxiv.org/abs/2304.02492) | 本研究探讨了早期动词学习不对称性的原因，并量化地证明，与名词相比，动词更难习得，需要整合视觉和语言信息。另外，多语言和多模式输入可以改善语言和视觉信息之间的不匹配。 |
| [^10] | ["It's Weird That it Knows What I Want": Usability and Interactions with Copilot for Novice Programmers.](http://arxiv.org/abs/2304.02491) | 本研究是首次观察初学编程者使用代码自动生成工具 "Github Copilot" 完成入门级编程作业。通过研究，探讨了初学者对于这种技术的利弊的感知，介绍了新的交互模式，并讨论了学生面临的认知和元认知困难。 |
| [^11] | [Exploring AI-Generated Text in Student Writing: How Does AI Help?.](http://arxiv.org/abs/2304.02478) | 研究发现，在学生写作中使用人工智能生成文本有一定好处，但过度依赖此类工具也存在潜在风险。 |
| [^12] | [Quiz-based Knowledge Tracing.](http://arxiv.org/abs/2304.02413) | 本文介绍了一种基于测验的知识追踪模型（QKT），该模型可以根据学生基于测验的学习交互来监测其知识状态。 |
| [^13] | [Explaining Multimodal Data Fusion: Occlusion Analysis for Wilderness Mapping.](http://arxiv.org/abs/2304.02407) | 本研究提出了一个用于多模态数据融合的深度学习框架，采用可解释的机器学习方法来研究模态的影响，结果显示在野区地图制作任务中，利用土地覆盖和夜间光数据能够大大提高模型准确性。 |
| [^14] | [AutoRL Hyperparameter Landscapes.](http://arxiv.org/abs/2304.02396) | 本文提出了一种方法，在训练期间多次建立和分析AutoRL超参数的景观，证明代表算法（DQN和SAC）在不同环境下的超参数景观会随时间而变化。 |
| [^15] | [Physics-Inspired Interpretability Of Machine Learning Models.](http://arxiv.org/abs/2304.02381) | 本文提出了一种以能量景观方法为启发的机器学习模型可解释性研究方法，可以识别决策的驱动因素，有助于解决机器学习模型在高度敏感领域应用的难题。 |
| [^16] | [Efficient CNNs via Passive Filter Pruning.](http://arxiv.org/abs/2304.02319) | 本论文提出了一种被动滤波剪枝方法来达到高效CNN的目的，解决了高剪枝率下逐元素范数方法存在的性能下降问题。 |
| [^17] | [How to choose your best allies for a transferable attack?.](http://arxiv.org/abs/2304.02312) | 本文提出了一种新方法来评估可转移性，通过将畸变放置于中心位置并提出了一种新的选择机制FiT，该机制旨在通过只进行几个初步查询即可选择最佳的源模型。 |
| [^18] | [Trap-Based Pest Counting: Multiscale and Deformable Attention CenterNet Integrating Internal LR and HR Joint Feature Learning.](http://arxiv.org/abs/2304.02291) | 本文介绍了一种新的害虫计数模型，使用多尺度和可变形注意力中心点网络进行内部低分辨率和高分辨率联合特征学习。为了解决害虫图像中严重遮挡、广泛姿态变化和尺度变化等问题，所提出的模型采用了两步式的多尺度热图生成方法和可变形卷积。在三个害虫数据集上的实验结果表明，该模型表现出优异的害虫计数准确性。 |
| [^19] | [About optimal loss function for training physics-informed neural networks under respecting causality.](http://arxiv.org/abs/2304.02282) | 本文提出了一种新的方法用于在尊重因果关系的情况下训练物理学知识的神经网络，该方法可以消除调整缩放系数的需要，并在数值实验中证明了其准确性。 |
| [^20] | [Persuading to Prepare for Quitting Smoking with a Virtual Coach: Using States and User Characteristics to Predict Behavior.](http://arxiv.org/abs/2304.02264) | 研究探讨了如何利用被劝说者的条件和特征选择更有效的劝说信息和预测行为，并进行了一项长期研究，结果表明这个方法能成功地促使日常吸烟者进行戒烟准备活动。 |
| [^21] | [Ericson: An Interactive Open-Domain Conversational Search Agent.](http://arxiv.org/abs/2304.02233) | 本文介绍了Ericson，一个交互式的开放领域会话搜索代理人，其中包括最先进的问答和信息检索组件，以及用于主动问题细化和推荐的意图推断和对话管理模型，在Alexa Prize中经过了压力测试，并发现了现有搜索技术的局限性。 |
| [^22] | [Large Language Models as Master Key: Unlocking the Secrets of Materials Science with GPT.](http://arxiv.org/abs/2304.02213) | 本文介绍了一个新的自然语言处理任务——结构化信息推理（SIS），利用GPT-3模型能够准确提取材料科学设备层面的信息，并通过实验预测PCE和反向预测参数，展示了大型语言模型在材料学中的巨大潜力。 |
| [^23] | [Document-Level Machine Translation with Large Language Models.](http://arxiv.org/abs/2304.02210) | 本文以文档级机器翻译为试验场，深入评估了大型语言模型在语篇建模方面的性能。研究发现利用LLMs强大的长文本建模能力可以提高翻译质量，在提示方面进行改进也可以显着提高翻译质量，并且LLMs有潜力编码丰富的语篇知识。 |
| [^24] | [MoocRadar: A Fine-grained and Multi-aspect Knowledge Repository for Improving Cognitive Student Modeling in MOOCs.](http://arxiv.org/abs/2304.02205) | 本文介绍了 MoocRadar，一种多方面的、细粒度的知识库，用于提高 MOOC 中认知学生建模的精度。 |
| [^25] | [Globalizing Fairness Attributes in Machine Learning: A Case Study on Health in Africa.](http://arxiv.org/abs/2304.02190) | 该论文以非洲卫生领域为案例研究，提出了机器学习中全球化的公正属性的体系，并应用于医疗模式，为促进全球卫生中的公平研究提供了基础和行动的呼吁。 |
| [^26] | [Bodily expressed emotion understanding through integrating Laban movement analysis.](http://arxiv.org/abs/2304.02187) | 该研究通过开发基于拉班动作分析运动编码系统的高质量人类运动要素数据集，并利用此数据集共同学习运动要素和情绪，提高机器理解身体语言表达的情绪能力，为精神科专业人员提供定量诊断和预后辅助，以及帮助执法机构识别欺骗行为。 |
| [^27] | [Blaming Humans and Machines: What Shapes People's Reactions to Algorithmic Harm.](http://arxiv.org/abs/2304.02176) | 本篇论文探究了人们对算法伤害的反应，发现人们对机器、设计师和用户的责备分配有所不同，并且人们对机器发起责备的决策取决于他们是否认为这是一个合适的反应。 |
| [^28] | [ChartReader: A Unified Framework for Chart Derendering and Comprehension without Heuristic Rules.](http://arxiv.org/abs/2304.02173) | ChartReader 是一个统一框架，可以无启发式规则进行图表去渲染和理解，通过从注释数据集中自动学习图表规则，消除了手动规则制定的需求，提高了准确性，在多项任务中表现优于现有算法。 |
| [^29] | [Synthesize Extremely High-dimensional Longitudinal Electronic Health Records via Hierarchical Autoregressive Language Model.](http://arxiv.org/abs/2304.02169) | 此论文提出了一种名为HALO的方法，它是一个层级自回归模型，可以生成高保真、细粒度电子健康记录数据，而这些数据可以用于训练准确的ML模型，且无需涉及隐私问题。 |
| [^30] | [I2I: Initializing Adapters with Improvised Knowledge.](http://arxiv.org/abs/2304.02168) | 本文提出了一种称为ImprovisetoInitialize(I2I)的连续学习算法，通过提取先前学习的任务适配器的知识来为即将到来的任务初始化适配器。这使得从一个任务到另一个任务的知识传递更加高效。 |
| [^31] | [GINA-3D: Learning to Generate Implicit Neural Assets in the Wild.](http://arxiv.org/abs/2304.02163) | GINA-3D是一种学习从真实场景中生成3D隐式神经资产的生成模型，这对于自主驾驶等机器人学习问题的测试和验证环境是重要的创新。 |
| [^32] | [GUTS: Generalized Uncertainty-Aware Thompson Sampling for Multi-Agent Active Search.](http://arxiv.org/abs/2304.02075) | 本文提出了 GUTS 算法，能够在异构多机器人系统中部署，解决大型非结构化环境下的主动搜索问题，考虑了传感器不确定性、遮挡问题、异构搜索团队和硬件、通信故障的鲁棒性等问题，并在仿真中超过现有算法高达80%。 |
| [^33] | [Multimodal Garment Designer: Human-Centric Latent Diffusion Models for Fashion Image Editing.](http://arxiv.org/abs/2304.02051) | 本文提出了一种基于人体的多模态面部图像编辑方法，通过潜在扩散模型来生成服装设计，实现了时尚插图的自动化。 |
| [^34] | [Multi-Class Explainable Unlearning for Image Classification via Weight Filtering.](http://arxiv.org/abs/2304.02049) | 本论文提出一种基于权重滤波的多类可解释性卸载图像分类方法，可以在单个未训练轮中取消学习网络的所有类别，并且恢复可解释的类别表示。 |
| [^35] | [How well do Large Language Models perform in Arithmetic tasks?.](http://arxiv.org/abs/2304.02015) | 这项工作提出了一个算术数据集 MATH 401 以评估大型语言模型的计算能力，并具体测试了 GPT-4、ChatGPT、InstrctGPT、Galactica 和 LLaMA 等模型的表现。 |
| [^36] | [Sociocultural knowledge is needed for selection of shots in hate speech detection tasks.](http://arxiv.org/abs/2304.01890) | HATELEXICON是一个包含巴西，德国，印度和肯尼亚仇恨言论的词汇表，利用其可以提高模型在训练中的性能表现。 |
| [^37] | [HyperCUT: Video Sequence from a Single Blurry Image using Unsupervised Ordering.](http://arxiv.org/abs/2304.01686) | 本文提出了一种新的无监督排序方法来解决单模糊图像到视频序列的重建问题，显式地分配每个序列的顺序，有效提高了去模糊模型的训练质量，并在合成和真实数据集上获得了最好的结果。 |
| [^38] | [To ChatGPT, or not to ChatGPT: That is the question!.](http://arxiv.org/abs/2304.01487) | 研究评估了聊天GPT检测中的最新技术和其他AI生成文本检测工具的表现，并提出区分人工生成和AI生成文本的重要性。 |
| [^39] | [Spam-T5: Benchmarking Large Language Models for Few-Shot Email Spam Detection.](http://arxiv.org/abs/2304.01238) | 本文通过比较不同类型的大型语言模型和传统机器学习技术在邮件垃圾检测中的表现，发现大多数情况下，大型语言模型优于传统技术，特别是在样本有限的情况下。同时，本文还介绍了经过改进和微调的Spam-T5模型，该模型具有出色的性能表现。 |
| [^40] | [POLAR-Express: Efficient and Precise Formal Reachability Analysis of Neural-Network Controlled Systems.](http://arxiv.org/abs/2304.01218) | POLAR-Express 是一种高效且准确的形式可达性分析工具，用于验证神经网络控制系统的安全性。它使用 Taylor 模型算术和逐层传播技术，可以分析具有连续激活功能的前馈神经网络，并在 ReLU 激活函数上提供了一种更有效的精确传播 TM 的新方法。 |
| [^41] | [AUDIT: Audio Editing by Following Instructions with Latent Diffusion Models.](http://arxiv.org/abs/2304.00830) | 研究提出了基于潜在扩散模型的指令引导音频编辑模型AUDIT，实现了在各种音频编辑任务上的最先进性能，并通过自适应方案和文本到片段匹配模块解决了先前扩散-based方法存在的问题。 |
| [^42] | [A Survey on Federated Learning for the Healthcare Metaverse: Concepts, Applications, Challenges, and Future Directions.](http://arxiv.org/abs/2304.00524) | 本综述介绍了联邦学习在医疗元宇宙中的应用。在介绍了联邦学习、物联网中的医疗系统和元宇宙医疗的基础上，讨论了联邦学习在元宇宙医疗中的优点和挑战。未来的研究方向和问题也被确定。 |
| [^43] | [DAMO-StreamNet: Optimizing Streaming Perception in Autonomous Driving.](http://arxiv.org/abs/2303.17144) | DAMO-StreamNet是一个优化自动驾驶中流式感知的框架，它融合了YOLO系列的最新进展，并通过颈部结构、双分支结构、蒸馏机制和实时预测机制等关键创新点，提供了尖端的解决方案。 |
| [^44] | [TabRet: Pre-training Transformer-based Tabular Models for Unseen Columns.](http://arxiv.org/abs/2303.15747) | 提出了一种可预训练的Transformer-based表格模型：TabRet，能够支持未知列，并在医疗保健分类任务上表现优秀。重新标记化和随机洗牌增强对性能提升有贡献。 |
| [^45] | [A Survey on Causal Discovery Methods for Temporal and Non-Temporal Data.](http://arxiv.org/abs/2303.15027) | 本文综述了时序和非时序数据因果发现的方法，探讨了不同算法在不同情况下识别因果边缘的能力。同时还介绍了可用于评估因果发现方法性能的基准数据集和常见指标。 |
| [^46] | [Vibration Signal Denoising Using Deep Learning.](http://arxiv.org/abs/2303.11413) | 本文研究了基于深度学习的去除脚步引起的振动信号的噪声的方法，该方法适用于高斯噪声和非平稳噪声。 |
| [^47] | [Highly Personalized Text Embedding for Image Manipulation by Stable Diffusion.](http://arxiv.org/abs/2303.08767) | 本论文提出了一种简单但高效的个性化方法——使用高度个性化文本嵌入来进行图像操作，可以运用于图像的背景、纹理和动作的编辑，不需要多个参考图像或复杂训练，能实现复杂语义图像编辑。 |
| [^48] | [Do we need entire training data for adversarial training?.](http://arxiv.org/abs/2303.06241) | 本文提出了一种新的对抗训练方法，通过对训练数据集进行筛选，仅使用易受对抗攻击的样本进行训练，从而减少训练时间。 |
| [^49] | [Rule-based Out-Of-Distribution Detection.](http://arxiv.org/abs/2303.01860) | 本文提出了一种基于XAI的方法，通过不同指标鉴别in-distribution和out of-distribution之间的相似性，从而实现非参数统计模型的OoD检测。该方法在多个复杂场景下表现出良好的检测准确度和评估训练和操作条件的接近程度。 |
| [^50] | [KHAN: Knowledge-Aware Hierarchical Attention Networks for Accurate Political Stance Prediction.](http://arxiv.org/abs/2302.12126) | 本文提出了一种新颖的知识感知政治立场预测方法，采用层次化注意力网络、外部知识库和知识感知损失函数，可以有效地捕捉新闻文章中的关键信息，优于现有方法。 |
| [^51] | [Topological Feature Selection: A Graph-Based Filter Feature Selection Approach.](http://arxiv.org/abs/2302.09543) | 本文提出了一种基于图论的特征选择方法，利用拓扑学和依赖关系，具有高度灵活性和解释性，在16个基准数据集上显示出优于或匹配于当前最先进技术的表现。 |
| [^52] | [Vision Learners Meet Web Image-Text Pairs.](http://arxiv.org/abs/2301.07088) | 本论文提出了一种基于网络数据的新型视觉学习方法MUlti-modal Generator (MUG)。在视觉数据集的转移学习任务上取得了最先进的表现，是之前最佳结果的3.4%和2.2%的提升。 |
| [^53] | [PIVOT: Prompting for Video Continual Learning.](http://arxiv.org/abs/2212.04842) | PIVOT 是一种针对视频数据的基于提示的持续学习方法，利用预训练模型减少了可训练参数的数量以及相应的遗忘，并且是第一个能够在无域内预训练情况下有效使用提示机制的方法。 |
| [^54] | [A Visual Active Search Framework for Geospatial Exploration.](http://arxiv.org/abs/2211.15788) | 本论文提出了基于视觉主动搜索框架的强化学习方法，以协助地理空间探索问题。该框架以广阔区域的图像为输入，通过有限的查询来验证目标对象示例的存在，并提供关于目标对象空间分布的信息。 |
| [^55] | [Collective Intelligence for 2D Push Manipulation with Mobile Robots.](http://arxiv.org/abs/2211.15136) | 本研究利用基于软体物理模拟器的规划器和基于注意力的神经网络，实现了移动机器人2D协作推动操作中的集体智能，比传统方法具有更好的性能并具备环境自适应能力。 |
| [^56] | [A Policy-Guided Imitation Approach for Offline Reinforcement Learning.](http://arxiv.org/abs/2210.08323) | 该论文提出了一种策略指导模仿方法，它将传统的奖励最大化策略分解成引导策略和执行策略。在训练期间，引导策略和执行策略在仅使用数据集中的数据的情况下进行学习。在评估期间，引导策略通过指引执行策略以最大化奖励，起到“先知”的作用，允许合理的超出分布泛化。 |
| [^57] | [Untargeted Backdoor Watermark: Towards Harmless and Stealthy Dataset Copyright Protection.](http://arxiv.org/abs/2210.00875) | 本文提出了一种针对数据集版权保护的无害和隐蔽的无目标后门水印方案，可以达到与最先进方案相当或更好的水印效果，并证明对模型性能无害且隐蔽。 |
| [^58] | [Adversarial robustness of VAEs through the lens of local geometry.](http://arxiv.org/abs/2208.03923) | 本文证明了对手攻击VAEs的最佳方法是利用由编码器和解码器网络引起的随机回溯度规张量的方向偏差。 |
| [^59] | [DeepIPC: Deeply Integrated Perception and Control for an Autonomous Vehicle in Real Environments.](http://arxiv.org/abs/2207.09934) | 本文介绍了DeepIPC，一个端到端自动驾驶模型，能够同时处理感知和控制任务。实验结果表明，DeepIPC具有最佳的可驾性和多任务性能，甚至比其他模型所需的参数更少。 |
| [^60] | [PatchCensor: Patch Robustness Certification for Transformers via Exhaustive Testing.](http://arxiv.org/abs/2111.10481) | PatchCensor是一种用于视觉Transformer的补丁鲁棒性认证方法，基于全面测试并考虑最坏的补丁攻击情境，能够提供受保证的准确性。 |
| [^61] | [From SLAM to Situational Awareness: Challenges and Survey.](http://arxiv.org/abs/2110.00273) | 本文研究旨在连接广泛的多学科现有知识，为移动机器人构建完整的情境感知（SA）系统，以提升其自主能力。 |
| [^62] | [Federated Submodel Optimization for Hot and Cold Data Features.](http://arxiv.org/abs/2109.07704) | 本文提出了一种针对联邦学习稀疏数据特征的联邦子模型平均算法(FedSubAvg)，该算法可以有效避免数据稀疏问题导致的计算下降，并保证每个模型参数的全局更新期望等于涉及它的客户端的本地更新平均值。该算法的新度量元素梯度范数可以更好地表征在稀疏数据上的联邦优化收敛。 |
| [^63] | [Improving Semiconductor Device Modeling for Electronic Design Automation by Machine Learning Techniques.](http://arxiv.org/abs/2105.11453) | 本文提出一种利用机器学习技术改进半导体器件建模的方法，通过自我增强策略和变分自编码器技术，只需少量实验数据点即可实现高精度预测，有效降低平均绝对误差，具有广泛应用价值。 |
| [^64] | [Undivided Attention: Are Intermediate Layers Necessary for BERT?.](http://arxiv.org/abs/2012.11881) | 本论文调查了中间层对于BERT在下游任务表现的作用，表明删除中间层数量可以减少模型参数和训练时间，同时对下游任务的影响较小，学习到的表示不受影响。 |
| [^65] | [CokeBERT: Contextual Knowledge Selection and Embedding towards Enhanced Pre-Trained Language Models.](http://arxiv.org/abs/2009.13964) | 本文提出了一种名为Coke的新框架，用于将上下文知识动态选择和嵌入到预训练语言模型中，以避免对输入文本匹配效果差的冗余和模糊知识的影响，并在知识驱动的自然语言处理任务上取得了优异表现。 |

# 详细

[^1]: 完成分割任务的万能模型

    Segment Anything. (arXiv:2304.02643v1 [cs.CV])

    [http://arxiv.org/abs/2304.02643](http://arxiv.org/abs/2304.02643)

    这篇论文介绍了一个新的图像分割任务、模型和数据集，该模型具有高效的分割能力和任务迁移能力，可以在新的图像分布和任务中进行零-shot 任务迁移，使其零-shot表现令人印象深刻，并发布了Segment Anything Model（SAM）和相应的数据集（SA-1B），以促进计算机视觉基础模型研究的发展。

    

    我们介绍了“Segment Anything（SA）”项目，这是一个新的图像分割任务、模型和数据集。我们使用高效的模型在数据收集循环中构建了到目前为止最大的分割数据集，涵盖1100万个许可和隐私尊重的图像，其中包含10亿个掩膜。该模型经过设计和训练，可以快速响应，因此可以在新的图像分布和任务中进行零-shot 任务迁移。我们对其在众多任务中的表现进行了评估，并发现其零-shot 性能令人印象深刻 -- 通常是竞争对手或甚至超过之前的完全监督结果。我们正在 https://segment-anything.com 上发布 Segment Anything Model（SAM）和相应的数据集（SA-1B），以促进计算机视觉基础模型研究的发展。

    We introduce the Segment Anything (SA) project: a new task, model, and dataset for image segmentation. Using our efficient model in a data collection loop, we built the largest segmentation dataset to date (by far), with over 1 billion masks on 11M licensed and privacy respecting images. The model is designed and trained to be promptable, so it can transfer zero-shot to new image distributions and tasks. We evaluate its capabilities on numerous tasks and find that its zero-shot performance is impressive -- often competitive with or even superior to prior fully supervised results. We are releasing the Segment Anything Model (SAM) and corresponding dataset (SA-1B) of 1B masks and 11M images at https://segment-anything.com to foster research into foundation models for computer vision.
    
[^2]: GenPhys：从物理过程到生成模型

    GenPhys: From Physical Processes to Generative Models. (arXiv:2304.02637v1 [cs.LG])

    [http://arxiv.org/abs/2304.02637](http://arxiv.org/abs/2304.02637)

    本文介绍了一个新的生成模型家族：GenPhys，它将从物理过程中描述的偏微分方程翻译为生成模型，并探索了更广泛的生成模型设计空间。

    

    由于扩散模型(DM)和最近的泊松流生成模型(PFGM)都受到物理过程的启发，因此合理地问一下：物理过程能否提供更多新的生成模型？我们展示了答案是肯定的。我们介绍了一个通用的家族，即从物理过程到生成模型(GenPhys)，其中我们将描述物理过程的偏微分方程(PDEs)翻译为生成模型。我们展示了可以从s-生成的PDEs (s代表平滑)构建生成模型。GenPhys包含了两个现有生成模型(DM和PFGM)，并甚至引出了新的生成模型家族，例如，受弱相互作用启发的“Yukawa生成模型”。另一方面，某些物理过程默认不属于GenPhys家族，例如波动方程和薛定谔方程，但可以通过一些修改加入到GenPhys家族中。我们的目标是通过GenPhys来探索和扩展生成模型的设计空间。

    Since diffusion models (DM) and the more recent Poisson flow generative models (PFGM) are inspired by physical processes, it is reasonable to ask: Can physical processes offer additional new generative models? We show that the answer is yes. We introduce a general family, Generative Models from Physical Processes (GenPhys), where we translate partial differential equations (PDEs) describing physical processes to generative models. We show that generative models can be constructed from s-generative PDEs (s for smooth). GenPhys subsume the two existing generative models (DM and PFGM) and even give rise to new families of generative models, e.g., "Yukawa Generative Models" inspired from weak interactions. On the other hand, some physical processes by default do not belong to the GenPhys family, e.g., the wave equation and the Schr\"{o}dinger equation, but could be made into the GenPhys family with some modifications. Our goal with GenPhys is to explore and expand the design space of gener
    
[^3]: 影响深度图像识别模型中学习等变性的因素是什么？

    What Affects Learned Equivariance in Deep Image Recognition Models?. (arXiv:2304.02628v1 [cs.CV])

    [http://arxiv.org/abs/2304.02628](http://arxiv.org/abs/2304.02628)

    本文研究了神经网络中学习到的等变性，提出了一种改进的等变性测量方法，并发现数据增强、减少模型容量和卷积操作可提高神经网络中学习到的等变性。

    

    神经网络中与几何变换相关的等变性可以提高数据效率、参数效率和对域外透视变换的鲁棒性。当等变性没有被设计到神经网络中时，网络仍然可以从数据中学习等变函数。本文提出了一种改进的等变性测量方法来量化这种学习到的等变性，并发现学习到的翻译等变性与在ImageNet上的验证准确性之间存在相关性。因此，我们研究了如何增加神经网络中学习到的等变性，并发现数据增强、减少模型容量以及卷积形式的归纳偏差可以在神经网络中引入更高的学习到的等变性。

    Equivariance w.r.t. geometric transformations in neural networks improves data efficiency, parameter efficiency and robustness to out-of-domain perspective shifts. When equivariance is not designed into a neural network, the network can still learn equivariant functions from the data. We quantify this learned equivariance, by proposing an improved measure for equivariance. We find evidence for a correlation between learned translation equivariance and validation accuracy on ImageNet. We therefore investigate what can increase the learned equivariance in neural networks, and find that data augmentation, reduced model capacity and inductive bias in the form of convolutions induce higher learned equivariance in neural networks.
    
[^4]: 基于3D感知扩散模型的生成新视角插图

    Generative Novel View Synthesis with 3D-Aware Diffusion Models. (arXiv:2304.02602v1 [cs.CV])

    [http://arxiv.org/abs/2304.02602](http://arxiv.org/abs/2304.02602)

    我们提出了一个基于扩散的模型，用于从仅有的一个输入图像生成3D感知的新视角插图，通过整合3D几何先验项，使生成结果更加逼真，能够生成视角一致的新渲染，且具有生成3D一致的序列的能力。

    

    我们提出了一个基于扩散的模型，用于从仅有的一个输入图像生成3D感知的新视角插图。我们的模型从与输入一致的可能渲染分布中进行采样，即使存在歧义，也能够生成多样化且逼真的新视角。为了实现这一点，我们的方法利用现有的二维扩散主干，但关键地，将3D特征体积作为几何先验项加入其中。这个潜在的特征场捕捉了可能的场景表示分布，并提高了我们的方法生成视角一致的新渲染的能力。除了生成新视角，我们的方法还能自回归地合成3D一致的序列。我们展示了对于合成渲染和房间规模的场景的最先进的结果。我们也展示了对于具有挑战性的真实对象的令人信服的结果。

    We present a diffusion-based model for 3D-aware generative novel view synthesis from as few as a single input image. Our model samples from the distribution of possible renderings consistent with the input and, even in the presence of ambiguity, is capable of rendering diverse and plausible novel views. To achieve this, our method makes use of existing 2D diffusion backbones but, crucially, incorporates geometry priors in the form of a 3D feature volume. This latent feature field captures the distribution over possible scene representations and improves our method's ability to generate view-consistent novel renderings. In addition to generating novel views, our method has the ability to autoregressively synthesize 3D-consistent sequences. We demonstrate state-of-the-art results on synthetic renderings and room-scale scenes; we also show compelling results for challenging, real-world objects.
    
[^5]: 基于MCMC的贝叶斯神经网络：基于Python的教程

    Bayesian neural networks via MCMC: a Python-based tutorial. (arXiv:2304.02595v1 [stat.ML])

    [http://arxiv.org/abs/2304.02595](http://arxiv.org/abs/2304.02595)

    本文提供了一个基于Python的教程，介绍了贝叶斯神经网络的MCMC方法应用，通过教程使得深度学习开发者能够更好地应用贝叶斯推断进行参数估计和不确定性量化。

    

    贝叶斯推断为机器学习和深度学习提供了参数估计和不确定性量化的方法。变分推断和马尔科夫链蒙特卡罗（MCMC）采样技术用于实现贝叶斯推断。在过去三十年中，MCMC方法在适应更大的模型（如深度学习）和大数据问题方面面临了许多挑战。包括梯度的高级提议（例如Langevin提议分布）提供了一种解决MCMC采样中的一些限制的方法，此外，MCMC方法通常被限制在统计学家的使用范围内，并且仍不是深度学习研究人员的主流方法。我们提供了一个MCMC方法的教程，涵盖了简单的贝叶斯线性和逻辑模型，以及贝叶斯神经网络。这个教程的目的是通过编码来弥合理论和实现之间的差距，鉴于当前MCMC方法的普及程度仍然较低。

    Bayesian inference provides a methodology for parameter estimation and uncertainty quantification in machine learning and deep learning methods. Variational inference and Markov Chain Monte-Carlo (MCMC) sampling techniques are used to implement Bayesian inference. In the past three decades, MCMC methods have faced a number of challenges in being adapted to larger models (such as in deep learning) and big data problems. Advanced proposals that incorporate gradients, such as a Langevin proposal distribution, provide a means to address some of the limitations of MCMC sampling for Bayesian neural networks. Furthermore, MCMC methods have typically been constrained to use by statisticians and are still not prominent among deep learning researchers. We present a tutorial for MCMC methods that covers simple Bayesian linear and logistic models, and Bayesian neural networks. The aim of this tutorial is to bridge the gap between theory and implementation via coding, given a general sparsity of li
    
[^6]: 降低学习的能量成本的竞争性可塑性

    Competitive plasticity to reduce the energetic costs of learning. (arXiv:2304.02594v1 [cs.NE])

    [http://arxiv.org/abs/2304.02594](http://arxiv.org/abs/2304.02594)

    该论文探讨了大脑如何通过竞争性可塑性降低学习的能量成本，并提出了两个省能的可塑性限制算法，结合起来可以大幅降低能量消耗。

    

    大脑不仅在支持计算方面受能量限制，而且在形成记忆的能量方面也受限。实验表明，学习简单的条件反射任务已经产生了很大的代谢成本。然而，学习像MNIST这样的任务到95％的准确度似乎至少需要10^8个突触更新。因此，大脑可能已经进化出能够以尽可能少的能量学习的能力。我们探讨了前馈神经网络的学习能量需求。基于一个简约的能量模型，我们提议了两个节能的可塑性限制算法：1）仅修改大更新的突触，2）将可塑性限制在形成网络路径的突触子集上。将这两种方法结合起来可以大大节省能量，同时只增加很少的学习时间。在生物网络中，网络通常比任务要求的大得多。特别是在这种情况下，可以实现大幅度的节能。

    The brain is not only constrained by energy needed to fuel computation, but it is also constrained by energy needed to form memories. Experiments have shown that learning simple conditioning tasks already carries a significant metabolic cost. Yet, learning a task like MNIST to 95% accuracy appears to require at least 10^{8} synaptic updates. Therefore the brain has likely evolved to be able to learn using as little energy as possible. We explored the energy required for learning in feedforward neural networks. Based on a parsimonious energy model, we propose two plasticity restricting algorithms that save energy: 1) only modify synapses with large updates, and 2) restrict plasticity to subsets of synapses that form a path through the network. Combining these two methods leads to substantial energy savings while only incurring a small increase in learning time. In biology networks are often much larger than the task requires. In particular in that case, large savings can be achieved. Th
    
[^7]: 马尔可夫决策过程中的合规异策评估

    Conformal Off-Policy Evaluation in Markov Decision Processes. (arXiv:2304.02574v1 [cs.LG])

    [http://arxiv.org/abs/2304.02574](http://arxiv.org/abs/2304.02574)

    本论文提出了一种基于合规预测的异策评估方法，能够以一定的确定性水平输出包含目标策略的真实奖励的区间，并提出了不同的处理分布偏移方法，其中一些方法在保证合规性的前提下实现了最先进的性能。

    

    强化学习旨在从数据中识别和评估有效的控制策略。在许多实际应用中，学习者不能进行实验，也不能以在线方式获取数据（在实验费用高昂、风险高或不道德的情况下，就会出现这种情况）。针对这种应用，必须使用在不同策略下收集的历史数据（行为策略）来估计给定策略（目标策略）的奖励。大多数针对这种学习任务的方法，即异策评估（OPE），都没有准确性和确定性保证。我们提出了一种基于合规预测的新型OPE方法，该方法输出一个包含目标策略的真实奖励的区间，同时具有一定的确定性水平。OPE中的主要挑战来自于目标策略和行为策略之间的差异引起的分布偏移。我们提出并经验性地评估了不同处理这种偏移的方法。其中一些方法在保证估计的奖励区间的合规性的同时，在基准环境中实现了最先进的性能。

    Reinforcement Learning aims at identifying and evaluating efficient control policies from data. In many real-world applications, the learner is not allowed to experiment and cannot gather data in an online manner (this is the case when experimenting is expensive, risky or unethical). For such applications, the reward of a given policy (the target policy) must be estimated using historical data gathered under a different policy (the behavior policy). Most methods for this learning task, referred to as Off-Policy Evaluation (OPE), do not come with accuracy and certainty guarantees. We present a novel OPE method based on Conformal Prediction that outputs an interval containing the true reward of the target policy with a prescribed level of certainty. The main challenge in OPE stems from the distribution shift due to the discrepancies between the target and the behavior policies. We propose and empirically evaluate different ways to deal with this shift. Some of these methods yield conform
    
[^8]: ChatGPT家族模型在生物医学推理和分类中的评估

    Evaluation of ChatGPT Family of Models for Biomedical Reasoning and Classification. (arXiv:2304.02496v1 [cs.CL])

    [http://arxiv.org/abs/2304.02496](http://arxiv.org/abs/2304.02496)

    本论文探讨了ChatGPT家族模型在生物医学任务中的性能，虽然Fine-tuned的生物医学NLP任务并未表现出一致的性能增益，但显示出大型语言模型在生物医学应用中超越问答的潜力，需要更定制化的模型设计和Fine-tuning策略。

    

    最近大型语言模型的不断提升展现了其在生物医学问答方面的卓越能力，但尚未充分研究其在更具体的生物医学应用中的表现。这项研究探讨了ChatGPT家族模型（GPT-3.5s，GPT-4）等大型语言模型在生物医学任务中的性能，在OpenAI API公共接口中不能传递患者数据的情况下，我们使用超过10000个样本作为两个基本临床任务分类和推理的代理进行模型性能评估。第一个任务是将科学文献中的临床和政策建议陈述归类为健康建议。第二个任务是从生物医学文献中检测因果关系。我们将大型语言模型与简单模型（如逻辑回归的词袋模型）和Fine-tuned的BioBERT模型进行了比较。尽管ChatGPT非常受欢迎，但我们发现，Fine-tuned的生物医学NLP任务并未表现出一致的性能增益。然而，本研究揭示了大型语言模型在生物医学应用中超越问答的潜力，强调了需要更定制化的模型设计和Fine-tuning策略。

    Recent advances in large language models (LLMs) have shown impressive ability in biomedical question-answering, but have not been adequately investigated for more specific biomedical applications. This study investigates the performance of LLMs such as the ChatGPT family of models (GPT-3.5s, GPT-4) in biomedical tasks beyond question-answering. Because no patient data can be passed to the OpenAI API public interface, we evaluated model performance with over 10000 samples as proxies for two fundamental tasks in the clinical domain classification and reasoning. The first task is classifying whether statements of clinical and policy recommendations in scientific literature constitute health advice. The second task is causal relation detection from the biomedical literature. We compared LLMs with simpler models, such as bag-of-words (BoW) with logistic regression, and fine-tuned BioBERT models. Despite the excitement around viral ChatGPT, we found that fine-tuning for two fundamental NLP
    
[^9]: 量化视觉、语言和视觉-语言复杂度在动词习得中的作用

    Quantifying the Roles of Visual, Linguistic, and Visual-Linguistic Complexity in Verb Acquisition. (arXiv:2304.02492v1 [cs.CL])

    [http://arxiv.org/abs/2304.02492](http://arxiv.org/abs/2304.02492)

    本研究探讨了早期动词学习不对称性的原因，并量化地证明，与名词相比，动词更难习得，需要整合视觉和语言信息。另外，多语言和多模式输入可以改善语言和视觉信息之间的不匹配。

    

    幼儿学习名词的意义通常比学习动词的意义早。但是，不清楚这种不对称性是由于语言所指的世界中类别的视觉结构的复杂性，语言本身的结构，还是两种信息来源之间的相互作用所致。我们通过使用大规模预训练的人工神经网络中来源于视觉和语言的单词表示，定量地测试了关于早期动词学习的这三个假说。通过检查视觉和语言嵌入空间的结构，我们发现，首先，与名词的表示相比，动词的表示在域内通常更加变化和不可辨识。其次，我们发现，如果每个类别只有一个学习实例，那么动词系统中的视觉和语言表示比名词系统中的不太吻合。然而，与人类语言发展的过程类似，如果在学习期间有多语言和多模式输入，那么名词和动词的语言和视觉信息都变得更加吻合。我们的研究结果表明，早期动词学习的不对称性至少部分归因于更复杂的动词含义，这需要集成视觉和语言信息，而不仅仅是视觉复杂度或语言结构。

    Children typically learn the meanings of nouns earlier than the meanings of verbs. However, it is unclear whether this asymmetry is a result of complexity in the visual structure of categories in the world to which language refers, the structure of language itself, or the interplay between the two sources of information. We quantitatively test these three hypotheses regarding early verb learning by employing visual and linguistic representations of words sourced from large-scale pre-trained artificial neural networks. Examining the structure of both visual and linguistic embedding spaces, we find, first, that the representation of verbs is generally more variable and less discriminable within domain than the representation of nouns. Second, we find that if only one learning instance per category is available, visual and linguistic representations are less well aligned in the verb system than in the noun system. However, in parallel with the course of human language development, if mult
    
[^10]: “它知道我想要什么”：初学者与 Copilot 的易用性和交互

    "It's Weird That it Knows What I Want": Usability and Interactions with Copilot for Novice Programmers. (arXiv:2304.02491v1 [cs.HC])

    [http://arxiv.org/abs/2304.02491](http://arxiv.org/abs/2304.02491)

    本研究是首次观察初学编程者使用代码自动生成工具 "Github Copilot" 完成入门级编程作业。通过研究，探讨了初学者对于这种技术的利弊的感知，介绍了新的交互模式，并讨论了学生面临的认知和元认知困难。

    

    深度学习的最新发展已经产生了能够高精度地将自然语言和基于代码提示生成源代码的代码生成模型。这可能会在课堂上产生深刻的影响，在那里初学者学习编程可以使用免费工具自动建议编程练习和作业的解决方案。然而，目前对初学者如何实际操作这些工具还知之甚少。我们提出了第一项研究，观察了初学者使用一种代码自动生成工具 Github Copilot 在典型的入门级编程（CS1）作业上的情况。通过观察和采访，我们探讨了学生对于这种学习技术的利弊的感知，介绍了新的观察到的交互模式，并讨论了学生面临的认知和元认知困难。我们考虑了这些发现的设计影响，特别是在如何更好地支持 Copilot 等工具方面。

    Recent developments in deep learning have resulted in code-generation models that produce source code from natural language and code-based prompts with high accuracy. This is likely to have profound effects in the classroom, where novices learning to code can now use free tools to automatically suggest solutions to programming exercises and assignments. However, little is currently known about how novices interact with these tools in practice. We present the first study that observes students at the introductory level using one such code auto-generating tool, Github Copilot, on a typical introductory programming (CS1) assignment. Through observations and interviews we explore student perceptions of the benefits and pitfalls of this technology for learning, present new observed interaction patterns, and discuss cognitive and metacognitive difficulties faced by students. We consider design implications of these findings, specifically in terms of how tools like Copilot can better support 
    
[^11]: 探索学生写作中的人工智能生成文本：AI能起到什么作用？

    Exploring AI-Generated Text in Student Writing: How Does AI Help?. (arXiv:2304.02478v1 [cs.CL])

    [http://arxiv.org/abs/2304.02478](http://arxiv.org/abs/2304.02478)

    研究发现，在学生写作中使用人工智能生成文本有一定好处，但过度依赖此类工具也存在潜在风险。

    

    以英语作为外语的学生使用人工智能自然语言生成工具生成的文本可能会提高他们的写作质量。然而，目前尚不清楚这些学生的写作中使用人工智能生成文本在多大程度上会导致更高质量的写作。我们探索了23名香港中学生撰写故事（包含自己的文字和人工智能生成的文本）的尝试。人类专家对这些故事进行了内容、语言和组织方面的评分。我们分析了故事中的AI生成文本的基本组织结构和句法复杂度，并执行了多元线性回归和聚类分析。结果表明，人类词语的数量和人工智能生成词语的数量对分数有重要贡献。此外，与同龄人相比，学生的写作可以分为擅长和不擅长使用更多或更少人工智能生成文本的两组。聚类比较显示，使用人工智能生成文本在学生写作中有一定好处，但同时也强调了过度依赖这种工具的潜在风险。

    English as foreign language_EFL_students' use of text generated from artificial intelligence_AI_natural language generation_NLG_tools may improve their writing quality. However, it remains unclear to what extent AI-generated text in these students' writing might lead to higher-quality writing. We explored 23 Hong Kong secondary school students' attempts to write stories comprising their own words and AI-generated text. Human experts scored the stories for dimensions of content, language and organization. We analyzed the basic organization and structure and syntactic complexity of the stories' AI-generated text and performed multiple linear regression and cluster analyses. The results show the number of human words and the number of AI-generated words contribute significantly to scores. Besides, students can be grouped into competent and less competent writers who use more AI-generated text or less AI-generated text compared to their peers. Comparisons of clusters reveal some benefit of
    
[^12]: 基于测验的知识追踪

    Quiz-based Knowledge Tracing. (arXiv:2304.02413v1 [cs.AI])

    [http://arxiv.org/abs/2304.02413](http://arxiv.org/abs/2304.02413)

    本文介绍了一种基于测验的知识追踪模型（QKT），该模型可以根据学生基于测验的学习交互来监测其知识状态。

    

    知识追踪（KT）旨在根据学习者与在线学习系统（OIS）中不同练习的学习交互来评估其不断发展的知识状态，这对于支持随后的智能服务，如个性化学习资源推荐，是至关重要的决策。现有的研究人员广泛研究了KT并开发了许多有效的方法。然而，大多数方法假设学生的历史交互在连续序列中均匀分布，忽略了实际交互序列是基于一系列具有清晰边界的测验组织的事实，在一个测验内的交互连续完成，但是跨不同测验的交互是离散的，可能会间隔数天。在本文中，我们提出了基于测验的知识追踪（QKT）模型，根据学生基于测验的学习交互来监测其知识状态。

    Knowledge tracing (KT) aims to assess individuals' evolving knowledge states according to their learning interactions with different exercises in online learning systems (OIS), which is critical in supporting decision-making for subsequent intelligent services, such as personalized learning source recommendation. Existing researchers have broadly studied KT and developed many effective methods. However, most of them assume that students' historical interactions are uniformly distributed in a continuous sequence, ignoring the fact that actual interaction sequences are organized based on a series of quizzes with clear boundaries, where interactions within a quiz are consecutively completed, but interactions across different quizzes are discrete and may be spaced over days. In this paper, we present the Quiz-based Knowledge Tracing (QKT) model to monitor students' knowledge states according to their quiz-based learning interactions. Specifically, as students' interactions within a quiz ar
    
[^13]: 解释多模态数据融合：野区地图制作中的遮挡分析

    Explaining Multimodal Data Fusion: Occlusion Analysis for Wilderness Mapping. (arXiv:2304.02407v1 [cs.CV])

    [http://arxiv.org/abs/2304.02407](http://arxiv.org/abs/2304.02407)

    本研究提出了一个用于多模态数据融合的深度学习框架，采用可解释的机器学习方法来研究模态的影响，结果显示在野区地图制作任务中，利用土地覆盖和夜间光数据能够大大提高模型准确性。

    

    早在很久以前人们就发现，在一个共同的潜在空间中共同利用多模态输入数据的互补特征是有益的。然而，每种模态对模型决策的影响仍然令人困惑。本研究提出了一种用于多模态地球观测数据的模态级解释的深度学习框架，该框架利用了可解释的机器学习方法，即遮挡敏感性，并研究了在早期融合情况下模态的影响。我们表明，野区地图制作这一任务很大程度上受益于诸如土地覆盖和夜间光数据等辅助数据。

    Jointly harnessing complementary features of multi-modal input data in a common latent space has been found to be beneficial long ago. However, the influence of each modality on the models decision remains a puzzle. This study proposes a deep learning framework for the modality-level interpretation of multimodal earth observation data in an end-to-end fashion. While leveraging an explainable machine learning method, namely Occlusion Sensitivity, the proposed framework investigates the influence of modalities under an early-fusion scenario in which the modalities are fused before the learning process. We show that the task of wilderness mapping largely benefits from auxiliary data such as land cover and night time light data.
    
[^14]: AutoRL超参数景观

    AutoRL Hyperparameter Landscapes. (arXiv:2304.02396v1 [cs.LG])

    [http://arxiv.org/abs/2304.02396](http://arxiv.org/abs/2304.02396)

    本文提出了一种方法，在训练期间多次建立和分析AutoRL超参数的景观，证明代表算法（DQN和SAC）在不同环境下的超参数景观会随时间而变化。

    

    强化学习（RL）在取得令人瞩目成果的同时，其超参数对性能的影响限制了其应用范围。这经常使得在实践中难以获得良好的结果。自动化RL（AutoRL）解决了这个难题，但有关超参数优化（HPO）方法在搜索最佳配置时所遍历的超参数景观动态变化的信息很少。鉴于现有AutoRL方法动态调整超参数配置的情况，我们提出了一种方法，在训练期间不仅在一个时间点，而且在多个时间点上建立和分析这些超参数景观。针对关于这种动态AutoRL方法合法性的一个重要开放问题，我们提供了充分的证据，表明在不同种类的环境（Cartpole和Pendulum）中，来自RL文献的代表算法（DQN和SAC）的超参数景观会随时间而强烈变化。

    Although Reinforcement Learning (RL) has shown to be capable of producing impressive results, its use is limited by the impact of its hyperparameters on performance. This often makes it difficult to achieve good results in practice. Automated RL (AutoRL) addresses this difficulty, yet little is known about the dynamics of the hyperparameter landscapes that hyperparameter optimization (HPO) methods traverse in search of optimal configurations. In view of existing AutoRL approaches dynamically adjusting hyperparameter configurations, we propose an approach to build and analyze these hyperparameter landscapes not just for one point in time but at multiple points in time throughout training. Addressing an important open question on the legitimacy of such dynamic AutoRL approaches, we provide thorough empirical evidence that the hyperparameter landscapes strongly vary over time across representative algorithms from RL literature (DQN and SAC) in different kinds of environments (Cartpole and
    
[^15]: 机器学习模型的物理启发式可解释性研究

    Physics-Inspired Interpretability Of Machine Learning Models. (arXiv:2304.02381v1 [cs.LG])

    [http://arxiv.org/abs/2304.02381](http://arxiv.org/abs/2304.02381)

    本文提出了一种以能量景观方法为启发的机器学习模型可解释性研究方法，可以识别决策的驱动因素，有助于解决机器学习模型在高度敏感领域应用的难题。

    

    机器学习模型的决策解释能力一直是限制其在高度敏感领域如医学、网络安全或自动驾驶中广泛应用的主要障碍之一。本文提出了一种新颖的方法，受物理学领域的能量景观研究方法启发，以识别输入数据的相关特征以促进模型决策。通过识别损失景观局部极小值组中的守恒权重，我们可以确定模型决策的驱动因素。在分子科学中存在类似的思想，使用坐标不变量或有序参数来确定分子的关键特征。然而，在机器学习损失景观中没有这样的方法。本文将展示能量景观方法在机器学习模型中的适用性，并举例说明其应用。

    The ability to explain decisions made by machine learning models remains one of the most significant hurdles towards widespread adoption of AI in highly sensitive areas such as medicine, cybersecurity or autonomous driving. Great interest exists in understanding which features of the input data prompt model decision making. In this contribution, we propose a novel approach to identify relevant features of the input data, inspired by methods from the energy landscapes field, developed in the physical sciences. By identifying conserved weights within groups of minima of the loss landscapes, we can identify the drivers of model decision making. Analogues to this idea exist in the molecular sciences, where coordinate invariants or order parameters are employed to identify critical features of a molecule. However, no such approach exists for machine learning loss landscapes. We will demonstrate the applicability of energy landscape methods to machine learning models and give examples, both 
    
[^16]: 通过被动滤波剪枝实现高效CNN

    Efficient CNNs via Passive Filter Pruning. (arXiv:2304.02319v1 [cs.LG])

    [http://arxiv.org/abs/2304.02319](http://arxiv.org/abs/2304.02319)

    本论文提出了一种被动滤波剪枝方法来达到高效CNN的目的，解决了高剪枝率下逐元素范数方法存在的性能下降问题。

    

    卷积神经网络在各种应用中已经展示出最先进的性能，但是由于对高计算复杂度和存储器的要求，CNN是资源密集型的。最近实现CNN计算效率的努力包括滤波剪枝方法，其根据滤波器的“重要性”消除CNN中的某些滤波器。大多数现有的滤波剪枝方法要么是“主动型”，即使用数据集和生成特征映射来量化滤波器的重要性；要么是“被动型”，即使用滤波器的逐元素范数计算滤波器的重要性而不涉及数据。在高剪枝率下，即需要从网络中剪枝大量的滤波器时，逐元素范数方法消除相对较小的范数滤波器而不考虑滤波器在产生节点输出方面的重要性，导致性能下降。为了解决这个问题，我们提出了一种被动滤波剪枝方法。

    Convolutional neural networks (CNNs) have shown state-of-the-art performance in various applications. However, CNNs are resource-hungry due to their requirement of high computational complexity and memory storage. Recent efforts toward achieving computational efficiency in CNNs involve filter pruning methods that eliminate some of the filters in CNNs based on the \enquote{importance} of the filters. The majority of existing filter pruning methods are either "active", which use a dataset and generate feature maps to quantify filter importance, or "passive", which compute filter importance using entry-wise norm of the filters without involving data. Under a high pruning ratio where large number of filters are to be pruned from the network, the entry-wise norm methods eliminate relatively smaller norm filters without considering the significance of the filters in producing the node output, resulting in degradation in the performance. To address this, we present a passive filter pruning me
    
[^17]: 如何选择最佳的盟友进行可转移攻击？

    How to choose your best allies for a transferable attack?. (arXiv:2304.02312v1 [cs.CR])

    [http://arxiv.org/abs/2304.02312](http://arxiv.org/abs/2304.02312)

    本文提出了一种新方法来评估可转移性，通过将畸变放置于中心位置并提出了一种新的选择机制FiT，该机制旨在通过只进行几个初步查询即可选择最佳的源模型。

    

    对抗样本的可转移性是深度神经网络安全中的一个关键问题。一个为源模型而制造的对抗样本可以欺骗另一个目标模型，使对抗攻击的威胁更加真实。衡量可转移性是一个关键问题，但攻击成功率本身并不能提供坚实的评估。本文提出了一种评估可转移性的新方法，将畸变放置于中心位置。这个新工具显示，如果攻击者随机选择源模型，那么可转移攻击的表现可能远远不及黑盒攻击。为了解决这个问题，我们提出了一种新的选择机制，称为FiT，该机制旨在通过只进行几个初步查询即可选择最佳的源模型。我们的实验结果表明，FiT在选择多个攻击情境下的最佳源模型方面非常有效，例如单一模型攻击、集成模型攻击和多攻击（代码可在https://github.com/weny1choi/FiT中找到）。

    The transferability of adversarial examples is a key issue in the security of deep neural networks. The possibility of an adversarial example crafted for a source model fooling another targeted model makes the threat of adversarial attacks more realistic. Measuring transferability is a crucial problem, but the Attack Success Rate alone does not provide a sound evaluation. This paper proposes a new methodology for evaluating transferability by putting distortion in a central position. This new tool shows that transferable attacks may perform far worse than a black box attack if the attacker randomly picks the source model. To address this issue, we propose a new selection mechanism, called FiT, which aims at choosing the best source model with only a few preliminary queries to the target. Our experimental results show that FiT is highly effective at selecting the best source model for multiple scenarios such as single-model attacks, ensemble-model attacks and multiple attacks (Code avai
    
[^18]: 基于捕获陷阱的害虫计数：内部低分辨率和高分辨率联合特征学习的多尺度可变形注意力中心点网络

    Trap-Based Pest Counting: Multiscale and Deformable Attention CenterNet Integrating Internal LR and HR Joint Feature Learning. (arXiv:2304.02291v1 [cs.CV])

    [http://arxiv.org/abs/2304.02291](http://arxiv.org/abs/2304.02291)

    本文介绍了一种新的害虫计数模型，使用多尺度和可变形注意力中心点网络进行内部低分辨率和高分辨率联合特征学习。为了解决害虫图像中严重遮挡、广泛姿态变化和尺度变化等问题，所提出的模型采用了两步式的多尺度热图生成方法和可变形卷积。在三个害虫数据集上的实验结果表明，该模型表现出优异的害虫计数准确性。

    

    害虫计数是非常重要的，因为它能够预测早期害虫数量，从而实现快速害虫控制，减少对作物的损害，提高生产力。近年来，光陷阱越来越多地用于引诱和拍摄害虫进行害虫计数。然而，由于严重的遮挡、广泛的姿态变化甚至尺度变化，害虫图像的外观差异很大，这使得害虫计数更具挑战性。为了解决这些问题，本研究提出了一种新的害虫计数模型，被称为多尺度可变形注意力中心点网络 (Mada-CenterNet)，用于内部低分辨率和高分辨率联合特征学习。与传统的CenterNet相比，所提出的Mada-CenterNet采用了两步式的多尺度热图生成方法来自适应地学习适应于尺度变化（即害虫数变化）的低分辨率和高分辨率热图。此外，为了克服姿态和遮挡的挑战，采用可变形卷积来增强所提出模型的鲁棒性。在三个害虫数据集上的实验表明，所提出的Mada-CenterNet在害虫计数准确性方面优于现有的最先进方法。

    Pest counting, which predicts the number of pests in the early stage, is very important because it enables rapid pest control, reduces damage to crops, and improves productivity. In recent years, light traps have been increasingly used to lure and photograph pests for pest counting. However, pest images have a wide range of variability in pest appearance owing to severe occlusion, wide pose variation, and even scale variation. This makes pest counting more challenging. To address these issues, this study proposes a new pest counting model referred to as multiscale and deformable attention CenterNet (Mada-CenterNet) for internal low-resolution (LR) and high-resolution (HR) joint feature learning. Compared with the conventional CenterNet, the proposed Mada-CenterNet adopts a multiscale heatmap generation approach in a two-step fashion to predict LR and HR heatmaps adaptively learned to scale variations, that is, changes in the number of pests. In addition, to overcome the pose and occlus
    
[^19]: 关于在尊重因果关系的情况下训练物理学知识的神经网络的最优损失函数研究

    About optimal loss function for training physics-informed neural networks under respecting causality. (arXiv:2304.02282v1 [math.NA])

    [http://arxiv.org/abs/2304.02282](http://arxiv.org/abs/2304.02282)

    本文提出了一种新的方法用于在尊重因果关系的情况下训练物理学知识的神经网络，该方法可以消除调整缩放系数的需要，并在数值实验中证明了其准确性。

    

    本文提出了一种方法，可以将具有初始和边界条件的微分方程问题简化为仅由微分方程描述的问题。修改后的物理启发式神经网络（PINNs）方法的优势在于，可以将损失函数表示为与微分方程相关的单个项的形式，从而消除了调整与边界和初始条件相关的项的缩放系数的需要。尊重因果关系的加权损失函数被修改，基于广义函数推导出了新的加权损失函数。对于多个问题进行了数值实验，证明了所提出方法的准确性。

    A method is presented that allows to reduce a problem described by differential equations with initial and boundary conditions to the problem described only by differential equations. The advantage of using the modified problem for physics-informed neural networks (PINNs) methodology is that it becomes possible to represent the loss function in the form of a single term associated with differential equations, thus eliminating the need to tune the scaling coefficients for the terms related to boundary and initial conditions. The weighted loss functions respecting causality were modified and new weighted loss functions based on generalized functions are derived. Numerical experiments have been carried out for a number of problems, demonstrating the accuracy of the proposed methods.
    
[^20]: 借助虚拟教练劝导戒烟准备：基于状态和用户特征预测行为

    Persuading to Prepare for Quitting Smoking with a Virtual Coach: Using States and User Characteristics to Predict Behavior. (arXiv:2304.02264v1 [cs.AI])

    [http://arxiv.org/abs/2304.02264](http://arxiv.org/abs/2304.02264)

    研究探讨了如何利用被劝说者的条件和特征选择更有效的劝说信息和预测行为，并进行了一项长期研究，结果表明这个方法能成功地促使日常吸烟者进行戒烟准备活动。

    

    尽管在行为变革的eHealth应用中非常普遍，但劝说信息对行为的影响往往很小。被劝说者的条件或状态（例如信心，知识和动机）和特征（例如性别，年龄和个性）是选择劝说信息更有效算法的两个有前途的组成部分。然而，至今还不太清楚在进行劝说尝试后，特别是在长期内，考虑这些组件能否很好地预测行为。由于收集许多算法组件的数据是昂贵的并且会给用户带来负担，因此欢迎更好地了解个别组件在实践中的影响，这有助于在使用哪些组件上做出明智的决策。因此，我们进行了一项纵向研究，在这项研究中，虚拟教练劝说671名日常吸烟者做准备活动，如设想自己所希望的未来自我，以准备戒烟和变得更加身体活跃。

    Despite their prevalence in eHealth applications for behavior change, persuasive messages tend to have small effects on behavior. Conditions or states (e.g., confidence, knowledge, motivation) and characteristics (e.g., gender, age, personality) of persuadees are two promising components for more effective algorithms for choosing persuasive messages. However, it is not yet sufficiently clear how well considering these components allows one to predict behavior after persuasive attempts, especially in the long run. Since collecting data for many algorithm components is costly and places a burden on users, a better understanding of the impact of individual components in practice is welcome. This can help to make an informed decision on which components to use. We thus conducted a longitudinal study in which a virtual coach persuaded 671 daily smokers to do preparatory activities for quitting smoking and becoming more physically active, such as envisioning one's desired future self. Based 
    
[^21]: Ericson: 一个交互式的开放领域会话搜索代理人

    Ericson: An Interactive Open-Domain Conversational Search Agent. (arXiv:2304.02233v1 [cs.CL])

    [http://arxiv.org/abs/2304.02233](http://arxiv.org/abs/2304.02233)

    本文介绍了Ericson，一个交互式的开放领域会话搜索代理人，其中包括最先进的问答和信息检索组件，以及用于主动问题细化和推荐的意图推断和对话管理模型，在Alexa Prize中经过了压力测试，并发现了现有搜索技术的局限性。

    

    开放式领域会话搜索（ODCS）旨在提供有价值的、最新的信息，同时保持自然的对话，以帮助用户细化并最终回答信息需求。然而，创建一个有效和强大的ODCS代理人是具有挑战性的。在本文中，我们介绍了一个完全功能的ODCS系统Ericson，其中包括最先进的问答和信息检索组件，以及用于主动问题细化和推荐的意图推断和对话管理模型。我们的系统在亚马逊Alexa Prize中经过了压力测试，与数千名Alexa用户进行了实时对话，从而为分析ODCS系统在真实环境中的基础提供了经验依据。我们的交互数据分析表明，精确的意图分类、鼓励用户参与和仔细的主动推荐对用户满意度做出了最大的贡献。我们的研究进一步确定了现有搜索技术的局限性。

    Open-domain conversational search (ODCS) aims to provide valuable, up-to-date information, while maintaining natural conversations to help users refine and ultimately answer information needs. However, creating an effective and robust ODCS agent is challenging. In this paper, we present a fully functional ODCS system, Ericson, which includes state-of-the-art question answering and information retrieval components, as well as intent inference and dialogue management models for proactive question refinement and recommendations. Our system was stress-tested in the Amazon Alexa Prize, by engaging in live conversations with thousands of Alexa users, thus providing empirical basis for the analysis of the ODCS system in real settings. Our interaction data analysis revealed that accurate intent classification, encouraging user engagement, and careful proactive recommendations contribute most to the users satisfaction. Our study further identifies limitations of the existing search techniques, 
    
[^22]: 大型语言模型作为钥匙：用GPT解密材料科学的秘密。

    Large Language Models as Master Key: Unlocking the Secrets of Materials Science with GPT. (arXiv:2304.02213v1 [cs.CL])

    [http://arxiv.org/abs/2304.02213](http://arxiv.org/abs/2304.02213)

    本文介绍了一个新的自然语言处理任务——结构化信息推理（SIS），利用GPT-3模型能够准确提取材料科学设备层面的信息，并通过实验预测PCE和反向预测参数，展示了大型语言模型在材料学中的巨大潜力。

    

    本文介绍了一个新的自然语言处理（NLP）任务——结构化信息推理（SIS），以解决材料科学设备层面信息提取的复杂性。我们使用现有的钙钛矿太阳能电池FAIR数据集对GPT-3进行微调，获得了91.8 F1得分，并更新了数据集，包括迄今为止所有相关科学论文。所生成的数据集已被格式化和标准化，使得它可以直接作为后续数据分析的输入。这个特性将使材料科学家通过选择高质量的领域评论文章来开发其自己的模型。此外，我们设计了实验来预测PCE和反向预测参数，并获得了与DFT相当的性能，这证明了大型语言模型能够像材料学家一样评判材料和设计新材料。

    This article presents a new NLP task called structured information inference (SIS) to address the complexities of information extraction at the device level in materials science. We accomplished this task by finetuning GPT-3 on a exsiting perovskite solar cell FAIR dataset with 91.8 F1-score and we updated the dataset with all related scientific papers up to now. The produced dataset is formatted and normalized, enabling its direct utilization as input in subsequent data analysis. This feature will enable materials scientists to develop their own models by selecting high-quality review papers within their domain. Furthermore, we designed experiments to predict PCE and reverse-predict parameters and obtained comparable performance with DFT, which demonstrates the potential of large language models to judge materials and design new materials like a materials scientist.
    
[^23]: 大型语言模型在文档级机器翻译中的应用研究

    Document-Level Machine Translation with Large Language Models. (arXiv:2304.02210v1 [cs.CL])

    [http://arxiv.org/abs/2304.02210](http://arxiv.org/abs/2304.02210)

    本文以文档级机器翻译为试验场，深入评估了大型语言模型在语篇建模方面的性能。研究发现利用LLMs强大的长文本建模能力可以提高翻译质量，在提示方面进行改进也可以显着提高翻译质量，并且LLMs有潜力编码丰富的语篇知识。

    

    大型语言模型（LLMs）如Chat-GPT可以为各种自然语言处理（NLP）任务生成连贯，连贯，相关和流畅的答案。本文以文档级机器翻译为试验场，提供了LLMs在语篇建模方面的深入评估。本研究着重关注三个方面：1）语篇感知提示的影响，我们调查不同提示对文档级翻译质量和语篇现象的影响；2）翻译模型的比较，我们比较Chat-GPT与商业MT系统和高级文档级MT方法的翻译性能；3）语篇建模能力分析，我们进一步探究LLMs中编码的语篇知识，并研究培训技术对语篇建模的影响。通过评估许多基准测试，我们惊讶地发现，1）利用强大的长文本建模能力，ChatGPT在文档级翻译质量方面优于商业MT系统和高级文档级MT方法；2）修改明确针对语篇现象的提示可以显着提高翻译质量；3）LLMs有潜力编码丰富的语篇知识，培训技术可以进一步增强这种能力。

    Large language models (LLMs) such as Chat-GPT can produce coherent, cohesive, relevant, and fluent answers for various natural language processing (NLP) tasks. Taking document-level machine translation (MT) as a testbed, this paper provides an in-depth evaluation of LLMs' ability on discourse modeling. The study fo-cuses on three aspects: 1) Effects of Discourse-Aware Prompts, where we investigate the impact of different prompts on document-level translation quality and discourse phenomena; 2) Comparison of Translation Models, where we compare the translation performance of Chat-GPT with commercial MT systems and advanced document-level MT methods; 3) Analysis of Discourse Modelling Abilities, where we further probe discourse knowledge encoded in LLMs and examine the impact of training techniques on discourse modeling. By evaluating a number of benchmarks, we surprisingly find that 1) leveraging their powerful long-text mod-eling capabilities, ChatGPT outperforms commercial MT systems 
    
[^24]: MoocRadar: 一种多方面的、细粒度的知识库，用于提高 MOOC 中认知学生建模的精度

    MoocRadar: A Fine-grained and Multi-aspect Knowledge Repository for Improving Cognitive Student Modeling in MOOCs. (arXiv:2304.02205v1 [cs.AI])

    [http://arxiv.org/abs/2304.02205](http://arxiv.org/abs/2304.02205)

    本文介绍了 MoocRadar，一种多方面的、细粒度的知识库，用于提高 MOOC 中认知学生建模的精度。

    

    学生建模是智能教育中推断学生学习特征的一项基本任务。尽管从知识跟踪和认知诊断的最近尝试提出了几个有希望改进当前模型可用性和有效性的方向，但现有公共数据集仍然不足以满足这些潜在解决方案的需求，因为它们忽略了完整的练习情境、细粒度的概念和认知标签。本文介绍了 MoocRadar，它是一个由 2,513 个练习问题、5,600 个知识概念和超过 1200 万行为记录组成的细粒度、多方面的知识库。具体而言，我们提出了一个框架，以保证细粒度概念和认知标签的高质量和全面性注释。统计和实验结果表明，我们的数据集为未来改进智能教育模型提供了一个基础。

    Student modeling, the task of inferring a student's learning characteristics through their interactions with coursework, is a fundamental issue in intelligent education. Although the recent attempts from knowledge tracing and cognitive diagnosis propose several promising directions for improving the usability and effectiveness of current models, the existing public datasets are still insufficient to meet the need for these potential solutions due to their ignorance of complete exercising contexts, fine-grained concepts, and cognitive labels. In this paper, we present MoocRadar, a fine-grained, multi-aspect knowledge repository consisting of 2,513 exercise questions, 5,600 knowledge concepts, and over 12 million behavioral records. Specifically, we propose a framework to guarantee a high-quality and comprehensive annotation of fine-grained concepts and cognitive labels. The statistical and experimental results indicate that our dataset provides the basis for the future improvements of e
    
[^25]: 机器学习中全球化的公正属性：以非洲健康为例的案例研究

    Globalizing Fairness Attributes in Machine Learning: A Case Study on Health in Africa. (arXiv:2304.02190v1 [cs.LG])

    [http://arxiv.org/abs/2304.02190](http://arxiv.org/abs/2304.02190)

    该论文以非洲卫生领域为案例研究，提出了机器学习中全球化的公正属性的体系，并应用于医疗模式，为促进全球卫生中的公平研究提供了基础和行动的呼吁。

    

    随着机器学习在医疗保健中应用日益增加，有人呼吁在机器学习中实现公平，以了解和缓解这些系统可能带来的伦理问题。公平对非洲的全球卫生具有影响，因为全球南北之间已经存在不公平的权力失衡。本文旨在探讨非洲健康的公平性，并提出了非洲语境下考虑公平性的属性，并勾画了这些属性可能在不同的机器学习医学模式中发挥作用的范围。这项工作为促进全球卫生中的公平研究提供了基础和行动的呼吁。

    With growing machine learning (ML) applications in healthcare, there have been calls for fairness in ML to understand and mitigate ethical concerns these systems may pose. Fairness has implications for global health in Africa, which already has inequitable power imbalances between the Global North and South. This paper seeks to explore fairness for global health, with Africa as a case study. We propose fairness attributes for consideration in the African context and delineate where they may come into play in different ML-enabled medical modalities. This work serves as a basis and call for action for furthering research into fairness in global health.
    
[^26]: 通过整合拉班动作分析实现身体表达情绪理解

    Bodily expressed emotion understanding through integrating Laban movement analysis. (arXiv:2304.02187v1 [cs.AI])

    [http://arxiv.org/abs/2304.02187](http://arxiv.org/abs/2304.02187)

    该研究通过开发基于拉班动作分析运动编码系统的高质量人类运动要素数据集，并利用此数据集共同学习运动要素和情绪，提高机器理解身体语言表达的情绪能力，为精神科专业人员提供定量诊断和预后辅助，以及帮助执法机构识别欺骗行为。

    

    身体动作传递着人们情绪和精神状态的重要信息，是日常交流中必不可少的。增强机器理解身体语言表达的情绪能力可以提高辅助机器人与儿童和老年用户的交流，为精神科专业人员提供定量诊断和预后辅助，以及帮助执法机构识别欺骗行为。本研究开发了基于拉班动作分析运动编码系统的高质量人类运动要素数据集，并利用此数据集共同学习运动要素和情绪。我们的长期目标是整合计算机、心理学和表演艺术的知识，通过身体语言实现自动理解和分析情绪和精神状态。这项工作为进一步研究人类运动分析中的情绪识别提供了良好的起点。

    Body movements carry important information about a person's emotions or mental state and are essential in daily communication. Enhancing the ability of machines to understand emotions expressed through body language can improve the communication of assistive robots with children and elderly users, provide psychiatric professionals with quantitative diagnostic and prognostic assistance, and aid law enforcement in identifying deception. This study develops a high-quality human motor element dataset based on the Laban Movement Analysis movement coding system and utilizes that to jointly learn about motor elements and emotions. Our long-term ambition is to integrate knowledge from computing, psychology, and performing arts to enable automated understanding and analysis of emotion and mental state through body language. This work serves as a launchpad for further research into recognizing emotions through analysis of human movement.
    
[^27]: 归咎于人和机器：什么决定人们对算法伤害的反应

    Blaming Humans and Machines: What Shapes People's Reactions to Algorithmic Harm. (arXiv:2304.02176v1 [cs.CY])

    [http://arxiv.org/abs/2304.02176](http://arxiv.org/abs/2304.02176)

    本篇论文探究了人们对算法伤害的反应，发现人们对机器、设计师和用户的责备分配有所不同，并且人们对机器发起责备的决策取决于他们是否认为这是一个合适的反应。

    

    人工智能系统可能会对人们造成伤害。本研究通过责备的视角研究了个体对此类伤害的反应。在前人有人们归咎于AI系统的研究基础上，我们探究了几个因素对个体反应机器、设计师和用户的责备风险的影响。三项研究结果（N = 1,153）表明，在归咎于这些行为者的过程中存在差异。AI系统是否可解释并不影响人们将责备放在他们、他们的开发者和用户的身上。公平和有害的考虑增加了对设计师和用户的责备，但对AI系统的判断几乎没有任何影响。相反，决定人们对机器的反应态度的是人们认为将责备他们作为对算法伤害的合适反应。我们讨论了一些含义，例如未来关于将AI系统纳入社会和道德范畴的决策将如何塑造普通人对AI的反应。

    Artificial intelligence (AI) systems can cause harm to people. This research examines how individuals react to such harm through the lens of blame. Building upon research suggesting that people blame AI systems, we investigated how several factors influence people's reactive attitudes towards machines, designers, and users. The results of three studies (N = 1,153) indicate differences in how blame is attributed to these actors. Whether AI systems were explainable did not impact blame directed at them, their developers, and their users. Considerations about fairness and harmfulness increased blame towards designers and users but had little to no effect on judgments of AI systems. Instead, what determined people's reactive attitudes towards machines was whether people thought blaming them would be a suitable response to algorithmic harm. We discuss implications, such as how future decisions about including AI systems in the social and moral spheres will shape laypeople's reactions to AI-
    
[^28]: ChartReader：一个无启发式规则的图表去渲染与理解的统一框架

    ChartReader: A Unified Framework for Chart Derendering and Comprehension without Heuristic Rules. (arXiv:2304.02173v1 [cs.CV])

    [http://arxiv.org/abs/2304.02173](http://arxiv.org/abs/2304.02173)

    ChartReader 是一个统一框架，可以无启发式规则进行图表去渲染和理解，通过从注释数据集中自动学习图表规则，消除了手动规则制定的需求，提高了准确性，在多项任务中表现优于现有算法。

    

    图表是一种传达复杂数据的强大工具，但由于图表类型的多样性和复杂的组件，其理解对于我们来说是一个挑战。现有的图表理解方法要么依赖启发式规则，要么过度依赖OCR系统，结果表现不佳。为了解决这些问题，我们提出了ChartReader，一个无缝集成图表去渲染和理解任务的统一框架。我们的方法包括基于Transformer的图表组件检测模块和一个扩展的预训练视觉语言模型，用于图表到X任务。通过从注释数据集中自动学习图表规则，我们的方法消除了手动规则制定的需求，减少了工作量并提高了准确性。我们还引入了一种数据变量替换技术，并扩展了预训练模型的输入和位置嵌入，实现跨任务训练。我们对ChartReader进行了Chart-to-Table、ChartQA和Chart-to-Text任务的评估，表明其性能优于现有的图表理解方法。

    Charts are a powerful tool for visually conveying complex data, but their comprehension poses a challenge due to the diverse chart types and intricate components. Existing chart comprehension methods suffer from either heuristic rules or an over-reliance on OCR systems, resulting in suboptimal performance. To address these issues, we present ChartReader, a unified framework that seamlessly integrates chart derendering and comprehension tasks. Our approach includes a transformer-based chart component detection module and an extended pre-trained vision-language model for chart-to-X tasks. By learning the rules of charts automatically from annotated datasets, our approach eliminates the need for manual rule-making, reducing effort and enhancing accuracy.~We also introduce a data variable replacement technique and extend the input and position embeddings of the pre-trained model for cross-task training. We evaluate ChartReader on Chart-to-Table, ChartQA, and Chart-to-Text tasks, demonstrat
    
[^29]: 基于层级自回归语言模型合成极高维长期电子健康记录

    Synthesize Extremely High-dimensional Longitudinal Electronic Health Records via Hierarchical Autoregressive Language Model. (arXiv:2304.02169v1 [cs.LG])

    [http://arxiv.org/abs/2304.02169](http://arxiv.org/abs/2304.02169)

    此论文提出了一种名为HALO的方法，它是一个层级自回归模型，可以生成高保真、细粒度电子健康记录数据，而这些数据可以用于训练准确的ML模型，且无需涉及隐私问题。

    

    合成的电子健康记录(EHRs)能够在机器学习(ML)和统计分析中作为真实EHRs的替代品，既真实又保护隐私。然而，由于高维数据的内在复杂性，以其原始高度维形式生成高保真、细粒度电子健康记录(EHR)数据对现有方法构成了挑战。本文提出了一种名为“Hierarchical Autoregressive Language mOdel (HALO)”的方法，用于生成纵向高维EHR数据，该方法保留了真实EHR的统计特性，可以用于训练准确的ML模型而不涉及隐私问题。我们的HALO方法被设计为一个层级自回归模型，生成一组针对医学代码、临床就诊和病人记录的概率密度函数，可以在其原始未聚合形式下生成真实的EHR数据，无需进行变量选择或聚合。此外，我们的模型还产生大量的随机样本，以提供复杂度较低但仍有意义的EHR数据。

    Synthetic electronic health records (EHRs) that are both realistic and preserve privacy can serve as an alternative to real EHRs for machine learning (ML) modeling and statistical analysis. However, generating high-fidelity and granular electronic health record (EHR) data in its original, highly-dimensional form poses challenges for existing methods due to the complexities inherent in high-dimensional data. In this paper, we propose Hierarchical Autoregressive Language mOdel (HALO) for generating longitudinal high-dimensional EHR, which preserve the statistical properties of real EHR and can be used to train accurate ML models without privacy concerns. Our HALO method, designed as a hierarchical autoregressive model, generates a probability density function of medical codes, clinical visits, and patient records, allowing for the generation of realistic EHR data in its original, unaggregated form without the need for variable selection or aggregation. Additionally, our model also produc
    
[^30]: I2I: 用改进的知识初始化转接器

    I2I: Initializing Adapters with Improvised Knowledge. (arXiv:2304.02168v1 [cs.CL])

    [http://arxiv.org/abs/2304.02168](http://arxiv.org/abs/2304.02168)

    本文提出了一种称为ImprovisetoInitialize(I2I)的连续学习算法，通过提取先前学习的任务适配器的知识来为即将到来的任务初始化适配器。这使得从一个任务到另一个任务的知识传递更加高效。

    

    转接器是延续学习中解决灾难性遗忘问题的一种有前途的解决方案。然而，为每个新任务训练独立的适配器模块错失了跨任务知识转移的机会。我们提出了一种称为 Improvise to Initialize (I2I) 的连续学习算法，通过提取先前学习的任务适配器的知识，为即将到来的任务初始化适配器。我们通过对视觉问答任务序列进行实验，评估了 I2I 在 CLiMB，一个多模态的连续学习基准上的表现。使用 I2I 训练的适配器始终比独立训练的适配器具有更好的任务精度，证明了我们的算法促进了任务适配器之间的知识转移，并且相对于先进的 AdapterFusion，I2I 也能实现更好的跨任务知识转移而不产生相关的参数成本。

    Adapters present a promising solution to the catastrophic forgetting problem in continual learning. However, training independent Adapter modules for every new task misses an opportunity for cross-task knowledge transfer. We propose Improvise to Initialize (I2I), a continual learning algorithm that initializes Adapters for incoming tasks by distilling knowledge from previously-learned tasks' Adapters. We evaluate I2I on CLiMB, a multimodal continual learning benchmark, by conducting experiments on sequences of visual question answering tasks. Adapters trained with I2I consistently achieve better task accuracy than independently-trained Adapters, demonstrating that our algorithm facilitates knowledge transfer between task Adapters. I2I also results in better cross-task knowledge transfer than the state-of-the-art AdapterFusion without incurring the associated parametric cost.
    
[^31]: GINA-3D：在真实场景中学习生成隐式神经资产

    GINA-3D: Learning to Generate Implicit Neural Assets in the Wild. (arXiv:2304.02163v1 [cs.CV])

    [http://arxiv.org/abs/2304.02163](http://arxiv.org/abs/2304.02163)

    GINA-3D是一种学习从真实场景中生成3D隐式神经资产的生成模型，这对于自主驾驶等机器人学习问题的测试和验证环境是重要的创新。

    

    从传感器数据中建模3D世界以进行仿真是开发自动驾驶等机器人学习问题的测试和验证环境的可扩展方法。然而，手动创建或重新创建类似真实世界的环境是困难，昂贵且不可扩展的。最近的生成模型技术，通过仅使用丰富的2D图像来学习3D资产，已经显示出解决这些挑战的有希望的进展 -- 但仍然存在局限性，因为它们利用人类策划的图像数据集或手动创建的合成3D环境的渲染。在本文中，我们介绍GINA-3D，这是一个生成模型，它使用来自相机和LiDAR传感器的真实驾驶数据创建真实3D隐式神经资产，包括各种车辆和行人。与现有的图像数据集相比，真实驾驶环境由于遮挡，光照变化和长尾分布而面临新的挑战。GINA-3D通过解耦从单个视角生成3D达到了解决这些挑战的目的，进而使3D场景的自动化成为可能。

    Modeling the 3D world from sensor data for simulation is a scalable way of developing testing and validation environments for robotic learning problems such as autonomous driving. However, manually creating or re-creating real-world-like environments is difficult, expensive, and not scalable. Recent generative model techniques have shown promising progress to address such challenges by learning 3D assets using only plentiful 2D images -- but still suffer limitations as they leverage either human-curated image datasets or renderings from manually-created synthetic 3D environments. In this paper, we introduce GINA-3D, a generative model that uses real-world driving data from camera and LiDAR sensors to create realistic 3D implicit neural assets of diverse vehicles and pedestrians. Compared to the existing image datasets, the real-world driving setting poses new challenges due to occlusions, lighting-variations and long-tail distributions. GINA-3D tackles these challenges by decoupling re
    
[^32]: GUTS：多智能体主动搜索的广义不确定性感知 Thompson Sampling算法

    GUTS: Generalized Uncertainty-Aware Thompson Sampling for Multi-Agent Active Search. (arXiv:2304.02075v1 [cs.RO])

    [http://arxiv.org/abs/2304.02075](http://arxiv.org/abs/2304.02075)

    本文提出了 GUTS 算法，能够在异构多机器人系统中部署，解决大型非结构化环境下的主动搜索问题，考虑了传感器不确定性、遮挡问题、异构搜索团队和硬件、通信故障的鲁棒性等问题，并在仿真中超过现有算法高达80%。

    

    快速灾难响应的机器人解决方案对确保最小生命损失至关重要，特别是当搜索区域对于人类救援者而言过于危险或过于广阔时。本文将这个问题建模为一个异步多智能体主动搜索任务，在此任务中，每个机器人旨在高效地在未知环境中寻找感兴趣的对象（OOI）。这种表述解决了搜索任务应该专注于快速恢复OOI而不是对搜索区域进行全覆盖的要求。先前的方法未能准确建模传感器不确定性，考虑到由于植被或地形的遮挡而导致的遮挡问题，或者考虑到异构搜索团队和硬件、通信故障的鲁棒性要求。我们提出了广义不确定性感知Thompson抽样（GUTS）算法，它解决了这些问题，并适用于在大型非结构化环境中部署异构多机器人系统进行主动搜索。我们通过仿真实验表明，GUTS算法在性能上超过现有算法高达80%。

    Robotic solutions for quick disaster response are essential to ensure minimal loss of life, especially when the search area is too dangerous or too vast for human rescuers. We model this problem as an asynchronous multi-agent active-search task where each robot aims to efficiently seek objects of interest (OOIs) in an unknown environment. This formulation addresses the requirement that search missions should focus on quick recovery of OOIs rather than full coverage of the search region. Previous approaches fail to accurately model sensing uncertainty, account for occlusions due to foliage or terrain, or consider the requirement for heterogeneous search teams and robustness to hardware and communication failures. We present the Generalized Uncertainty-aware Thompson Sampling (GUTS) algorithm, which addresses these issues and is suitable for deployment on heterogeneous multi-robot systems for active search in large unstructured environments. We show through simulation experiments that GU
    
[^33]: 多模态服装设计师：面向人的潜在扩散模型用于时尚图像编辑

    Multimodal Garment Designer: Human-Centric Latent Diffusion Models for Fashion Image Editing. (arXiv:2304.02051v1 [cs.CV])

    [http://arxiv.org/abs/2304.02051](http://arxiv.org/abs/2304.02051)

    本文提出了一种基于人体的多模态面部图像编辑方法，通过潜在扩散模型来生成服装设计，实现了时尚插图的自动化。

    

    时尚插图是设计师用来传达他们的视觉和将设计理念从构思到实现，展示服装如何与人体交互的方式。在这个背景下，计算机视觉可以用于改进时尚设计过程。本文提出了一个新的架构，基于潜在扩散模型，提出了多模态条件的时尚图像编辑任务，通过跟随多模态提示，如文本、人体姿势和服装草图，指导生成以人为中心的时尚图像。由于缺乏适合这项任务的现有数据集，我们还展开了两个现有时尚数据集 Dress Code 和 VITON-HD，用半自动的方法补充了多模态注释。新数据集上的实验结果表明了该方法的有效性。

    Fashion illustration is used by designers to communicate their vision and to bring the design idea from conceptualization to realization, showing how clothes interact with the human body. In this context, computer vision can thus be used to improve the fashion design process. Differently from previous works that mainly focused on the virtual try-on of garments, we propose the task of multimodal-conditioned fashion image editing, guiding the generation of human-centric fashion images by following multimodal prompts, such as text, human body poses, and garment sketches. We tackle this problem by proposing a new architecture based on latent diffusion models, an approach that has not been used before in the fashion domain. Given the lack of existing datasets suitable for the task, we also extend two existing fashion datasets, namely Dress Code and VITON-HD, with multimodal annotations collected in a semi-automatic manner. Experimental results on these new datasets demonstrate the effective
    
[^34]: 基于权重滤波的多类可解释性卸载图像分类

    Multi-Class Explainable Unlearning for Image Classification via Weight Filtering. (arXiv:2304.02049v1 [cs.CV])

    [http://arxiv.org/abs/2304.02049](http://arxiv.org/abs/2304.02049)

    本论文提出一种基于权重滤波的多类可解释性卸载图像分类方法，可以在单个未训练轮中取消学习网络的所有类别，并且恢复可解释的类别表示。

    

    机器卸载是最近浮现的一种选择性地将训练数据点的影响从网络中删除的范式。尽管现有方法已经集中在卸载训练数据的小子集或单个类别，但在本文中，我们采取了不同的方法，设计了一个框架，可以在单个未训练轮中取消学习图像分类网络的所有类别。我们提出的方法通过内部组件的记忆矩阵来调节图像分类网络，以便在训练后，同一网络可以有选择地展示任何类别的未学习行为。通过发现每个类别特定的权重，我们的方法还通过设计可解释性机制来恢复类别的表示。我们在小规模和中规模图像分类数据集上使用CNN和Transformer-based骨架测试了提出的框架。我们的工作提供了一种多类可解释性卸载的解决方案。

    Machine Unlearning has recently been emerging as a paradigm for selectively removing the impact of training datapoints from a network. While existing approaches have focused on unlearning either a small subset of the training data or a single class, in this paper we take a different path and devise a framework that can unlearn all classes of an image classification network in a single untraining round. Our proposed technique learns to modulate the inner components of an image classification network through memory matrices so that, after training, the same network can selectively exhibit an unlearning behavior over any of the classes. By discovering weights which are specific to each of the classes, our approach also recovers a representation of the classes which is explainable by-design. We test the proposed framework, which we name Weight Filtering network (WF-Net), on small-scale and medium-scale image classification datasets, with both CNN and Transformer-based backbones. Our work p
    
[^35]: 大型语言模型在算术任务中的表现如何？

    How well do Large Language Models perform in Arithmetic tasks?. (arXiv:2304.02015v1 [cs.CL])

    [http://arxiv.org/abs/2304.02015](http://arxiv.org/abs/2304.02015)

    这项工作提出了一个算术数据集 MATH 401 以评估大型语言模型的计算能力，并具体测试了 GPT-4、ChatGPT、InstrctGPT、Galactica 和 LLaMA 等模型的表现。

    

    大型语言模型已经具备了连贯思路的能力，能够逐步解答数学问题。解决数学问题不仅需要通过思维连贯的能力分解问题，还需要针对每个步骤正确计算算术表达式。据我们所知，还没有任何研究专门评估大型语言模型的计算能力。在这项工作中，我们提出了一个算术数据集 MATH 401，用于测试最新的大型语言模型，包括 GPT-4、ChatGPT、InstrctGPT、Galactica 和 LLaMA，其中涉及各种算术表达式，并提供了大型语言模型的能力的详细分析。MATH 401 数据集和评估代码已在 \url{https://github.com/GanjinZero/math401-llm} 上发布。

    Large language models have emerged abilities including chain-of-thought to answer math word problems step by step. Solving math word problems not only requires abilities to disassemble problems via chain-of-thought but also needs to calculate arithmetic expressions correctly for each step. To the best of our knowledge, there is no work to focus on evaluating the arithmetic ability of large language models. In this work, we propose an arithmetic dataset MATH 401 to test the latest large language models including GPT-4, ChatGPT, InstrctGPT, Galactica, and LLaMA with various arithmetic expressions and provide a detailed analysis of the ability of large language models. MATH 401 and evaluation codes are released at \url{https://github.com/GanjinZero/math401-llm}.
    
[^36]: 社会文化知识在仇恨言论检测任务中对选项的选择是必要的

    Sociocultural knowledge is needed for selection of shots in hate speech detection tasks. (arXiv:2304.01890v1 [cs.CL])

    [http://arxiv.org/abs/2304.01890](http://arxiv.org/abs/2304.01890)

    HATELEXICON是一个包含巴西，德国，印度和肯尼亚仇恨言论的词汇表，利用其可以提高模型在训练中的性能表现。

    

    我们引入了HATELEXICON，这是一个包含巴西，德国，印度和肯尼亚的蔑称和仇恨言论目标的词汇表，以帮助模型的训练和可解释性。我们展示了我们的词汇表如何用于解释模型预测，表明发展用于分类极端言论的模型，在进行预测时严重依赖目标词。此外，我们提出了一种通过HATELEXICON来辅助低资源环境下训练选项的方法，选项选择在小样本学习中尤为重要。在我们的工作中，我们使用HASOC数据对德语和印地语进行了几个示范学习，并将Multilingual HateCheck（MHC）作为基准。我们展示了根据我们的词汇表选择样本，相对于随机采样的模型，能够更好地在MHC上表现。因此，当仅有少量的训练样本时，使用我们的词汇表来选择包含更多社会文化信息的样本能够更好地提高在仇恨言论检测任务中的性能。

    We introduce HATELEXICON, a lexicon of slurs and targets of hate speech for the countries of Brazil, Germany, India and Kenya, to aid training and interpretability of models. We demonstrate how our lexicon can be used to interpret model predictions, showing that models developed to classify extreme speech rely heavily on target words when making predictions. Further, we propose a method to aid shot selection for training in low-resource settings via HATELEXICON. In few-shot learning, the selection of shots is of paramount importance to model performance. In our work, we simulate a few-shot setting for German and Hindi, using HASOC data for training and the Multilingual HateCheck (MHC) as a benchmark. We show that selecting shots based on our lexicon leads to models performing better on MHC than models trained on shots sampled randomly. Thus, when given only a few training examples, using our lexicon to select shots containing more sociocultural information leads to better few-shot perf
    
[^37]: 无监督排序的单模糊图像到视频序列方法——HyperCUT

    HyperCUT: Video Sequence from a Single Blurry Image using Unsupervised Ordering. (arXiv:2304.01686v1 [cs.CV])

    [http://arxiv.org/abs/2304.01686](http://arxiv.org/abs/2304.01686)

    本文提出了一种新的无监督排序方法来解决单模糊图像到视频序列的重建问题，显式地分配每个序列的顺序，有效提高了去模糊模型的训练质量，并在合成和真实数据集上获得了最好的结果。

    

    本文研究了图像到视频去模糊的模型训练中的一个关键问题：由于前向和后向序列都可以作为模糊图像对应的清晰图像序列，因此序列顺序的模糊性极大地干扰了模型的训练效果。为解决这一问题，本文提出了一种有效的自监督排序方法，将每个视频序列映射到一个高维潜在空间中的向量，并利用超平面将其与其反向序列区分开来，显式地分配了每个序列的顺序。最后，我们在合成和真实数据集上验证了我们的方法，并与现有的图像到视频去模糊方法相比取得了最优秀的性能。

    We consider the challenging task of training models for image-to-video deblurring, which aims to recover a sequence of sharp images corresponding to a given blurry image input. A critical issue disturbing the training of an image-to-video model is the ambiguity of the frame ordering since both the forward and backward sequences are plausible solutions. This paper proposes an effective self-supervised ordering scheme that allows training high-quality image-to-video deblurring models. Unlike previous methods that rely on order-invariant losses, we assign an explicit order for each video sequence, thus avoiding the order-ambiguity issue. Specifically, we map each video sequence to a vector in a latent high-dimensional space so that there exists a hyperplane such that for every video sequence, the vectors extracted from it and its reversed sequence are on different sides of the hyperplane. The side of the vectors will be used to define the order of the corresponding sequence. Last but not 
    
[^38]: 聊天GPT，还是不聊天GPT：这是一个问题！

    To ChatGPT, or not to ChatGPT: That is the question!. (arXiv:2304.01487v1 [cs.LG])

    [http://arxiv.org/abs/2304.01487](http://arxiv.org/abs/2304.01487)

    研究评估了聊天GPT检测中的最新技术和其他AI生成文本检测工具的表现，并提出区分人工生成和AI生成文本的重要性。

    

    聊天GPT已经成为一种全球感知。随着聊天GPT和其他大型语言模型（LLM）的出现，对于他们的误用的担忧也增加了，例如传播虚假消息，抄袭，操纵公众舆论，欺骗和欺诈。因此，区分人工生成和AI生成的文本变得越来越重要。研究人员提出了各种检测方法，从基本的二元分类器到更复杂的深度学习模型。一些检测技术依赖于统计特征或句法模式，而其他一些则包含语义或上下文信息以提高准确性。本研究的主要目标是对聊天GPT检测中最新技术进行全面和现代化的评估。此外，我们还评估了其他未专门声称检测聊天GPT生成内容的AI生成文本检测工具以评估它们在检测聊天GPT生成内容方面的表现。在我们的评估中，我们使用了一个包含人工编写和聊天GPT生成的文本的大型数据集。

    ChatGPT has become a global sensation. As ChatGPT and other Large Language Models (LLMs) emerge, concerns of misusing them in various ways increase, such as disseminating fake news, plagiarism, manipulating public opinion, cheating, and fraud. Hence, distinguishing AI-generated from human-generated becomes increasingly essential. Researchers have proposed various detection methodologies, ranging from basic binary classifiers to more complex deep-learning models. Some detection techniques rely on statistical characteristics or syntactic patterns, while others incorporate semantic or contextual information to improve accuracy. The primary objective of this study is to provide a comprehensive and contemporary assessment of the most recent techniques in ChatGPT detection. Additionally, we evaluated other AI-generated text detection tools that do not specifically claim to detect ChatGPT-generated content to assess their performance in detecting ChatGPT-generated content. For our evaluation,
    
[^39]: Spam-T5：基于小样本的邮件垃圾检测的大型语言模型基准测试

    Spam-T5: Benchmarking Large Language Models for Few-Shot Email Spam Detection. (arXiv:2304.01238v1 [cs.CL])

    [http://arxiv.org/abs/2304.01238](http://arxiv.org/abs/2304.01238)

    本文通过比较不同类型的大型语言模型和传统机器学习技术在邮件垃圾检测中的表现，发现大多数情况下，大型语言模型优于传统技术，特别是在样本有限的情况下。同时，本文还介绍了经过改进和微调的Spam-T5模型，该模型具有出色的性能表现。

    

    本文通过比较三种不同类型的大型语言模型（BERT-like、Sentence Transformers和Seq2Seq）以及传统机器学习技术（如朴素贝叶斯和LightGBM）在邮件垃圾检测中的有效性，研究了大型语言模型在邮件垃圾检测中的作用。同时，我们还评估了这些模型在四个公共数据集上的表现，并使用不同数量的训练样本（完整训练集和小样本）进行了测试。 发现在大多数情况下，LLMs优于基线技术，特别是在小样本情况下。这种适应性使LLMs在邮件垃圾检测任务中具有独特的优势，因为标记样本数量有限，并且模型需要经常更新。此外，我们介绍了Spam-T5模型，该模型是专门为检测电子邮件垃圾而进行了改进和微调。我们的结果表明，Spam-T5模型具有出色的性能。

    This paper investigates the effectiveness of large language models (LLMs) in email spam detection by comparing prominent models from three distinct families: BERT-like, Sentence Transformers, and Seq2Seq. Additionally, we examine well-established machine learning techniques for spam detection, such as Na\"ive Bayes and LightGBM, as baseline methods. We assess the performance of these models across four public datasets, utilizing different numbers of training samples (full training set and few-shot settings). Our findings reveal that, in the majority of cases, LLMs surpass the performance of the popular baseline techniques, particularly in few-shot scenarios. This adaptability renders LLMs uniquely suited to spam detection tasks, where labeled samples are limited in number and models require frequent updates. Additionally, we introduce Spam-T5, a Flan-T5 model that has been specifically adapted and fine-tuned for the purpose of detecting email spam. Our results demonstrate that Spam-T5 
    
[^40]: POLAR-Express: 神经网络控制系统的高效准确形式可达性分析

    POLAR-Express: Efficient and Precise Formal Reachability Analysis of Neural-Network Controlled Systems. (arXiv:2304.01218v1 [eess.SY])

    [http://arxiv.org/abs/2304.01218](http://arxiv.org/abs/2304.01218)

    POLAR-Express 是一种高效且准确的形式可达性分析工具，用于验证神经网络控制系统的安全性。它使用 Taylor 模型算术和逐层传播技术，可以分析具有连续激活功能的前馈神经网络，并在 ReLU 激活函数上提供了一种更有效的精确传播 TM 的新方法。

    

    在挑战性的控制问题上，扮演控制器角色的神经网络 (NN) 展示出了令人印象深刻的实验性能。但神经网络控制系统 (NNCS) 在实际应用中的潜在采用也引起了日益增长的对这些 NNCS 安全性的担忧，特别是在安全关键应用中的使用。本文提出了 POLAR-Express，一种高效且准确的形式可达性分析工具，用于验证 NNCS 的安全性。POLAR-Express 使用 Taylor 模型算术，逐层横跨神经网络来传播 Taylor 模型 (TM) 以计算神经网络函数的近似值。它可以用于分析任何具有连续激活功能的前馈神经网络。我们还提出了一种在 ReLU 激活函数上更有效地精确传播 TM 的新方法。此外，POLAR-Express 为逐层传播提供了并行计算支持。

    Neural networks (NNs) playing the role of controllers have demonstrated impressive empirical performances on challenging control problems. However, the potential adoption of NN controllers in real-life applications also gives rise to a growing concern over the safety of these neural-network controlled systems (NNCSs), especially when used in safety-critical applications. In this work, we present POLAR-Express, an efficient and precise formal reachability analysis tool for verifying the safety of NNCSs. POLAR-Express uses Taylor model arithmetic to propagate Taylor models (TMs) across a neural network layer-by-layer to compute an overapproximation of the neural-network function. It can be applied to analyze any feed-forward neural network with continuous activation functions. We also present a novel approach to propagate TMs more efficiently and precisely across ReLU activation functions. In addition, POLAR-Express provides parallel computation support for the layer-by-layer propagation
    
[^41]: AUDIT：基于潜在扩散模型的指令引导音频编辑

    AUDIT: Audio Editing by Following Instructions with Latent Diffusion Models. (arXiv:2304.00830v2 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2304.00830](http://arxiv.org/abs/2304.00830)

    研究提出了基于潜在扩散模型的指令引导音频编辑模型AUDIT，实现了在各种音频编辑任务上的最先进性能，并通过自适应方案和文本到片段匹配模块解决了先前扩散-based方法存在的问题。

    

    音频编辑可用于各种目的，例如添加背景音效、替换乐器和修复损坏的音频。最近，一些基于扩散的方法通过使用以输出音频的文本说明为条件的扩散和去噪过程来实现零-shot音频编辑。然而，这些方法仍存在一些问题：1）它们并没有被训练用于编辑任务，不能保证良好的编辑效果；2）它们可能会错误地修改不需要编辑的音频片段；3）他们需要输出音频的完整描述，这在实际情况下并不总是可用或必要。在这项工作中，我们提出了AUDIT，一种基于潜在扩散模型的指令引导音频编辑模型。特别是，AUDIT具有三个主要设计特点：1）我们为不同的音频编辑任务构建三元训练数据（指令，输入音频，输出音频）并使用指令和输入（要编辑的音频）音频对训练扩散模型；2）我们引入了一种新的自适应方案，使用很少量的编辑任务音频数据对扩散模型进行微调；3）我们提出了一个文本到片段匹配模块，自动将输入指令与要编辑的相应音频片段对齐。实验结果表明，AUDIT在各种音频编辑任务上实现了最先进的性能，并且比以前的基于扩散的方法表现更优秀。

    Audio editing is applicable for various purposes, such as adding background sound effects, replacing a musical instrument, and repairing damaged audio. Recently, some diffusion-based methods achieved zero-shot audio editing by using a diffusion and denoising process conditioned on the text description of the output audio. However, these methods still have some problems: 1) they have not been trained on editing tasks and cannot ensure good editing effects; 2) they can erroneously modify audio segments that do not require editing; 3) they need a complete description of the output audio, which is not always available or necessary in practical scenarios. In this work, we propose AUDIT, an instruction-guided audio editing model based on latent diffusion models. Specifically, AUDIT has three main design features: 1) we construct triplet training data (instruction, input audio, output audio) for different audio editing tasks and train a diffusion model using instruction and input (to be edite
    
[^42]: 基于联邦学习的医疗元宇宙研究综述：概念、应用、挑战与未来方向

    A Survey on Federated Learning for the Healthcare Metaverse: Concepts, Applications, Challenges, and Future Directions. (arXiv:2304.00524v2 [cs.CY] UPDATED)

    [http://arxiv.org/abs/2304.00524](http://arxiv.org/abs/2304.00524)

    本综述介绍了联邦学习在医疗元宇宙中的应用。在介绍了联邦学习、物联网中的医疗系统和元宇宙医疗的基础上，讨论了联邦学习在元宇宙医疗中的优点和挑战。未来的研究方向和问题也被确定。

    

    近年来，技术的进步大大改善了医疗系统，提供了各种智能医疗服务，提高了生活质量。联邦学习（FL）作为人工智能（AI）的新分支，为处理医疗系统中的隐私问题并利用分布式设备上的数据和计算资源提供了机会。此外，元宇宙通过整合人工智能、云边缘计算、物联网（IoT）、区块链和语义通信等新兴技术，已经在一般和特别是在医疗行业中，改变了许多垂直领域。显然，FL展示了许多优点，并为传统的和元宇宙的医疗系统提供了新的机会，这促使我们对FL在元宇宙医疗系统中的应用进行调查。首先，我们介绍了基于物联网的医疗系统、传统医疗领域中的FL以及元宇宙医疗。然后，讨论了FL在元宇宙医疗中的应用及其优点。随后，突出了元宇宙医疗中FL所面临的挑战和问题。最后，确定了未来的方向和研究问题，以指导和启发对FL在医疗元宇宙中的研究。

    Recent technological advancements have considerately improved healthcare systems to provide various intelligent healthcare services and improve the quality of life. Federated learning (FL), a new branch of artificial intelligence (AI), opens opportunities to deal with privacy issues in healthcare systems and exploit data and computing resources available at distributed devices. Additionally, the Metaverse, through integrating emerging technologies, such as AI, cloud edge computing, Internet of Things (IoT), blockchain, and semantic communications, has transformed many vertical domains in general and the healthcare sector in particular. Obviously, FL shows many benefits and provides new opportunities for conventional and Metaverse healthcare, motivating us to provide a survey on the usage of FL for Metaverse healthcare systems. First, we present preliminaries to IoT-based healthcare systems, FL in conventional healthcare, and Metaverse healthcare. The benefits of FL in Metaverse healthc
    
[^43]: DAMO-StreamNet：自动驾驶中流式感知的优化

    DAMO-StreamNet: Optimizing Streaming Perception in Autonomous Driving. (arXiv:2303.17144v1 [cs.CV])

    [http://arxiv.org/abs/2303.17144](http://arxiv.org/abs/2303.17144)

    DAMO-StreamNet是一个优化自动驾驶中流式感知的框架，它融合了YOLO系列的最新进展，并通过颈部结构、双分支结构、蒸馏机制和实时预测机制等关键创新点，提供了尖端的解决方案。

    

    实时感知，或者说流式感知，是自动驾驶中一个至关重要但尚未得到充分研究的方面。为了填补这一空白，我们提出了DAMO-StreamNet，它将YOLO系列的最新进展与空间和时间感知机制的全面分析相结合，提供了一个尖端的解决方案。DAMO-StreamNet的关键创新点包括：(1)一个鲁棒的颈部结构，融合了可变形卷积，增强了感受野和特征对齐能力。(2)一个双分支结构，整合了短通道语义特征和长通道时序特征，提高了运动状态预测的精度。(3)一个在logits级别上进行的蒸馏机制，对齐教师网络和学生网络的语义空间。(4)一个实时预测机制，更新支持帧的特征与当前帧，确保推理过程中的无缝流式感知。

    Real-time perception, or streaming perception, is a crucial aspect of autonomous driving that has yet to be thoroughly explored in existing research. To address this gap, we present DAMO-StreamNet, an optimized framework that combines recent advances from the YOLO series with a comprehensive analysis of spatial and temporal perception mechanisms, delivering a cutting-edge solution. The key innovations of DAMO-StreamNet are: (1) A robust neck structure incorporating deformable convolution, enhancing the receptive field and feature alignment capabilities. (2) A dual-branch structure that integrates short-path semantic features and long-path temporal features, improving motion state prediction accuracy. (3) Logits-level distillation for efficient optimization, aligning the logits of teacher and student networks in semantic space. (4) A real-time forecasting mechanism that updates support frame features with the current frame, ensuring seamless streaming perception during inference. Our ex
    
[^44]: TabRet: 预训练Transformer-based表格模型，支持未知列

    TabRet: Pre-training Transformer-based Tabular Models for Unseen Columns. (arXiv:2303.15747v1 [cs.LG])

    [http://arxiv.org/abs/2303.15747](http://arxiv.org/abs/2303.15747)

    提出了一种可预训练的Transformer-based表格模型：TabRet，能够支持未知列，并在医疗保健分类任务上表现优秀。重新标记化和随机洗牌增强对性能提升有贡献。

    

    我们提出了一种名为TabRet的可预训练Transformer-based表格模型。TabRet旨在为包含未在预训练中见过的列的下游任务提供支持。与其他方法不同，TabRet在微调之前有一个额外的学习步骤，称为重新标记化，它基于遮蔽自动编码损失来校准特征嵌入。在实验中，我们使用大量的公共健康调查数据对TabRet进行预训练，并在医疗保健分类任务上进行微调，在四个数据集上实现了最佳AUC性能。此外，消融研究表明，在预训练期间进行重新标记化和随机洗牌增强对性能提升有贡献。

    We present \emph{TabRet}, a pre-trainable Transformer-based model for tabular data. TabRet is designed to work on a downstream task that contains columns not seen in pre-training. Unlike other methods, TabRet has an extra learning step before fine-tuning called \emph{retokenizing}, which calibrates feature embeddings based on the masked autoencoding loss. In experiments, we pre-trained TabRet with a large collection of public health surveys and fine-tuned it on classification tasks in healthcare, and TabRet achieved the best AUC performance on four datasets. In addition, an ablation study shows retokenizing and random shuffle augmentation of columns during pre-training contributed to performance gains.
    
[^45]: 时序和非时序数据因果发现方法综述

    A Survey on Causal Discovery Methods for Temporal and Non-Temporal Data. (arXiv:2303.15027v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2303.15027](http://arxiv.org/abs/2303.15027)

    本文综述了时序和非时序数据因果发现的方法，探讨了不同算法在不同情况下识别因果边缘的能力。同时还介绍了可用于评估因果发现方法性能的基准数据集和常见指标。

    

    因果发现（CD）是从数据中识别系统变量间因果关系的过程。多年来，已经开发了几种方法，主要基于数据的统计特性来揭示潜在的因果机制。本文对设计用于处理独立同分布（i.i.d.）数据和时间序列数据的因果发现方法进行了广泛探讨。为此，我们首先介绍了因果发现中的常用术语，然后全面讨论了在不同情况下识别因果边缘的算法。我们进一步讨论了可用于评估因果发现方法性能的基准数据集，可用于执行因果发现的工具或软件包以及评估这些方法的常见指标。我们还在不同基准数据集上测试了一些常见因果发现算法。

    Causal Discovery (CD) is the process of identifying the cause-effect relationships among the variables of a system from data. Over the years, several methods have been developed primarily based on the statistical properties of data to uncover the underlying causal mechanism. In this study, we present an extensive discussion on the methods designed to perform causal discovery from both independent and identically distributed (i.i.d.) data and time series data. For this purpose, we first introduce the common terminologies in causal discovery, and then provide a comprehensive discussion of the algorithms designed to identify the causal edges in different settings. We further discuss some of the benchmark datasets available for evaluating the performance of the causal discovery methods, available tools or software packages to perform causal discovery readily, and the common metrics used to evaluate these methods. We also test some common causal discovery algorithms on different benchmark d
    
[^46]: 基于深度学习的振动信号去噪方法

    Vibration Signal Denoising Using Deep Learning. (arXiv:2303.11413v1 [eess.SP])

    [http://arxiv.org/abs/2303.11413](http://arxiv.org/abs/2303.11413)

    本文研究了基于深度学习的去除脚步引起的振动信号的噪声的方法，该方法适用于高斯噪声和非平稳噪声。

    

    由脚步引起的结构振动信号被广泛用于人员识别、定位、人类活动推断、结构健康监测等任务。然而，由于环境噪声、电磁干扰等因素的影响，实际采集的信号通常会带有噪声。噪声的存在影响了信号处理过程，从而影响了最终任务的准确性和误差。本文主要探讨了基于深度学习的去除脚步引起的振动信号的噪声的方法。我们考虑了不同类型的噪声，包括高斯噪声和非平稳噪声等。

    Structure vibration signals induced by footsteps are widely used for tasks like occupant identification, localization, human activity inference, structure health monitoring and so on. The vibration signals are collected as time series with amplitude values. However, the collected signals are always noisy in practice due to the influence of environmental noise, electromagnetic interference and other factors. The presence of noise affects the process of signal analysis, thus affecting the accuracy and error of the final tasks. In this paper, we mainly explore the denoising methods for footstep-induced vibration signals. We have considered different kinds of noise including stationary noises such as gaussian noises and non-stationary noises such as item-dropping vibration noise and music noises.
    
[^47]: 基于稳定扩散的高度个性化文本嵌入图像操作

    Highly Personalized Text Embedding for Image Manipulation by Stable Diffusion. (arXiv:2303.08767v1 [cs.CV])

    [http://arxiv.org/abs/2303.08767](http://arxiv.org/abs/2303.08767)

    本论文提出了一种简单但高效的个性化方法——使用高度个性化文本嵌入来进行图像操作，可以运用于图像的背景、纹理和动作的编辑，不需要多个参考图像或复杂训练，能实现复杂语义图像编辑。

    

    稳定扩散模型已经展现出在图像生成和操作方面的卓越性能，但内部随机性带来的挑战在于如何保留和操作图像内容和身份。虽然之前的方法如“梦境相机”和“文本反转”提出了使用模型或潜在表示个性化来保持内容，但它们对多个参考图像和复杂训练的依赖限制了它们的实用性。本文提出了一种简单而又非常有效的个性化方法，使用高度个性化（HiPer）文本嵌入通过分解CLIP嵌入空间实现个性化和内容操作。我们的方法不需要模型微调或识别符，但仍可以仅通过单个图像和目标文本来实现背景、纹理和动作的操作。通过对多样化的目标文本的实验，我们证明了我们的方法在各种任务中都能产生高度个性化和复杂的语义图像编辑。

    Diffusion models have shown superior performance in image generation and manipulation, but the inherent stochasticity presents challenges in preserving and manipulating image content and identity. While previous approaches like DreamBooth and Textual Inversion have proposed model or latent representation personalization to maintain the content, their reliance on multiple reference images and complex training limits their practicality. In this paper, we present a simple yet highly effective approach to personalization using highly personalized (HiPer) text embedding by decomposing the CLIP embedding space for personalization and content manipulation. Our method does not require model fine-tuning or identifiers, yet still enables manipulation of background, texture, and motion with just a single image and target text. Through experiments on diverse target texts, we demonstrate that our approach produces highly personalized and complex semantic image edits across a wide range of tasks. We
    
[^48]: 对抗训练是否需要使用整个训练数据集？

    Do we need entire training data for adversarial training?. (arXiv:2303.06241v1 [cs.CV])

    [http://arxiv.org/abs/2303.06241](http://arxiv.org/abs/2303.06241)

    本文提出了一种新的对抗训练方法，通过对训练数据集进行筛选，仅使用易受对抗攻击的样本进行训练，从而减少训练时间。

    This paper proposes a new adversarial training method that reduces training time by selecting only the adversarially-prone samples from the training dataset.

    深度神经网络（DNN）被用于解决许多领域的问题，包括自动驾驶汽车和医学图像等安全关键领域。DNN对抗攻击的脆弱性已经被广泛关注。近年来，已经提出了许多方法来通过对抗训练来解决这个问题。几乎所有的方法都会为整个训练数据集生成对抗性示例，从而大大增加了训练时间。我们展示了通过仅使用训练数据的子集进行对抗训练，可以减少任何对抗训练算法的训练时间。为了选择子集，我们从训练数据中过滤出易受对抗攻击的样本。我们对所有训练样本执行简单的对抗攻击，以过滤出这个子集。在这个攻击中，我们向每个像素添加一个小扰动和几条网格线到输入图像中。我们对易受对抗攻击的子集进行对抗训练，并且...

    Deep Neural Networks (DNNs) are being used to solve a wide range of problems in many domains including safety-critical domains like self-driving cars and medical imagery. DNNs suffer from vulnerability against adversarial attacks. In the past few years, numerous approaches have been proposed to tackle this problem by training networks using adversarial training. Almost all the approaches generate adversarial examples for the entire training dataset, thus increasing the training time drastically. We show that we can decrease the training time for any adversarial training algorithm by using only a subset of training data for adversarial training. To select the subset, we filter the adversarially-prone samples from the training data. We perform a simple adversarial attack on all training examples to filter this subset. In this attack, we add a small perturbation to each pixel and a few grid lines to the input image.  We perform adversarial training on the adversarially-prone subset and mi
    
[^49]: 基于规则的OoD检测

    Rule-based Out-Of-Distribution Detection. (arXiv:2303.01860v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2303.01860](http://arxiv.org/abs/2303.01860)

    本文提出了一种基于XAI的方法，通过不同指标鉴别in-distribution和out of-distribution之间的相似性，从而实现非参数统计模型的OoD检测。该方法在多个复杂场景下表现出良好的检测准确度和评估训练和操作条件的接近程度。

    

    检测数据集中是否存在OoD是部署机器学习的最关键问题之一。本文采用XAI解释性人工智能方法，通过考虑不同的指标鉴别出in-distribution和out of-distribution之间的相似性。该方法是非参数统计模型，不需要分布假设。在复杂应用场景下（如预测性维护、车队管制、网络安全追踪等）的验证显示了该方法在检测准确度和评估训练和操作条件的接近程度方面的优良性能。结果可在以下链接中的github上获得: https://github.com/giacomo97cnr/Rule-based-ODD。

    Out-of-distribution detection is one of the most critical issue in the deployment of machine learning. The data analyst must assure that data in operation should be compliant with the training phase as well as understand if the environment has changed in a way that autonomous decisions would not be safe anymore. The method of the paper is based on eXplainable Artificial Intelligence (XAI); it takes into account different metrics to identify any resemblance between in-distribution and out of, as seen by the XAI model. The approach is non-parametric and distributional assumption free. The validation over complex scenarios (predictive maintenance, vehicle platooning, covert channels in cybersecurity) corroborates both precision in detection and evaluation of training-operation conditions proximity. Results are available via open source and open data at the following link: https://github.com/giacomo97cnr/Rule-based-ODD.
    
[^50]: KHAN：基于知识的层次化注意力网络用于准确的政治立场预测

    KHAN: Knowledge-Aware Hierarchical Attention Networks for Accurate Political Stance Prediction. (arXiv:2302.12126v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2302.12126](http://arxiv.org/abs/2302.12126)

    本文提出了一种新颖的知识感知政治立场预测方法，采用层次化注意力网络、外部知识库和知识感知损失函数，可以有效地捕捉新闻文章中的关键信息，优于现有方法。

    

    新闻文章的政治立场预测已被广泛研究，以减轻回声室效应，即人们落入其思想，强化其现有信念。以往关于政治立场问题的研究重点在于（1）识别可以反映新闻文章政治立场的政治因素和（2）有效地捕捉这些因素。尽管它们在经验上成功了，但在政治立场预测中其识别的因素的有效性没有得到充分证明。在此基础上，本文通过用户研究调查政治立场预测中的重要因素，并观察到新闻文章的环境和语调（隐含）以及文章中涉及的现实实体的外部知识（显式）在确定其政治立场方面是重要的。基于这一观察结果，我们提出了一种新颖的知识感知政治立场预测方法（KHAN），采用（1）层次化注意力网络有效地捕捉新闻文章中的关键信息，（2）外部知识库提供关于文章中提到的实体的额外信息，以及（3）知识感知损失函数学习优先考虑重要信息。多个数据集上的实验结果表明，我们的方法显著优于现有方法。

    The political stance prediction for news articles has been widely studied to mitigate the echo chamber effect -- people fall into their thoughts and reinforce their pre-existing beliefs. The previous works for the political stance problem focus on (1) identifying political factors that could reflect the political stance of a news article and (2) capturing those factors effectively. Despite their empirical successes, they are not sufficiently justified in terms of how effective their identified factors are in the political stance prediction. Motivated by this, in this work, we conduct a user study to investigate important factors in political stance prediction, and observe that the context and tone of a news article (implicit) and external knowledge for real-world entities appearing in the article (explicit) are important in determining its political stance. Based on this observation, we propose a novel knowledge-aware approach to political stance prediction (KHAN), employing (1) hierar
    
[^51]: 基于拓扑学的特征选择方法：一种基于图论的过滤特征选择方法

    Topological Feature Selection: A Graph-Based Filter Feature Selection Approach. (arXiv:2302.09543v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.09543](http://arxiv.org/abs/2302.09543)

    本文提出了一种基于图论的特征选择方法，利用拓扑学和依赖关系，具有高度灵活性和解释性，在16个基准数据集上显示出优于或匹配于当前最先进技术的表现。

    

    本文介绍了一种新型的无监督的基于图论过滤方法进行特征选择的技术，利用了拓扑约束网络表示的威力。我们使用一系列弦图（三角最大过滤图）模拟特征之间的相互依赖关系，并通过研究特征在网络内的相对位置来最大化特征相关性的可能性。这种方法相对于其他方法有三个特点：（i）高度可调，易于适应输入数据的性质；（ii）完全可解释，同时保持了显著的简单性；（iii）计算成本比其替代方案更加便宜。我们在来自不同应用领域的16个基准数据集上测试了我们的算法，结果表明在异构评估条件下，它优于或与当前最先进技术相匹配。

    In this paper, we introduce a novel unsupervised, graph-based filter feature selection technique which exploits the power of topologically constrained network representations. We model dependency structures among features using a family of chordal graphs (the Triangulated Maximally Filtered Graph), and we maximise the likelihood of features' relevance by studying their relative position inside the network. Such an approach presents three aspects that are particularly satisfactory compared to its alternatives: (i) it is highly tunable and easily adaptable to the nature of input data; (ii) it is fully explainable, maintaining, at the same time, a remarkable level of simplicity; (iii) it is computationally cheaper compared to its alternatives. We test our algorithm on 16 benchmark datasets from different applicative domains showing that it outperforms or matches the current state-of-the-art under heterogeneous evaluation conditions.
    
[^52]: 视觉学习者遇见Web图像-文本对

    Vision Learners Meet Web Image-Text Pairs. (arXiv:2301.07088v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2301.07088](http://arxiv.org/abs/2301.07088)

    本论文提出了一种基于网络数据的新型视觉学习方法MUlti-modal Generator (MUG)。在视觉数据集的转移学习任务上取得了最先进的表现，是之前最佳结果的3.4%和2.2%的提升。

    

    大多数最新的自监督学习方法都是在维护良好的ImageNet-1K数据集上进行预训练的。在本研究中，考虑到网络数据的出色可伸缩性，我们认为自我监督预训练应该基于嘈杂的网络源图文配对数据。首先，我们在如此设置下，对大规模网络数据上的代表性自监督预训练方法进行了基准研究。我们比较了一系列方法，包括使用被屏蔽的训练目标的单模式方法和使用图像-文本对比训练的多模式方法。我们发现，现有的多模态方法在视觉转移学习任务上并不比单模态方法表现更好。我们提出了一个信息论视角来解释这些基准结果，这提供了如何设计新型视觉学习者的见解。受到这些见解的启发，我们提出了一种新的视觉表示预训练方法——多模式生成器（MUG），它从可伸缩的网络源图文数据中学习。MUG在几个视觉数据集的转移学习任务上取得了最先进的性能，在CIFAR-10上优于之前最佳的结果3.4％，在STL-10上优于之前最佳的结果2.2％。

    Most recent self-supervised learning methods are pre-trained on the well-curated ImageNet-1K dataset. In this work, given the excellent scalability of web data, we consider self-supervised pre-training on noisy web sourced image-text paired data. First, we conduct a benchmark study of representative self-supervised pre-training methods on large-scale web data in a like-for-like setting. We compare a range of methods, including single-modal ones that use masked training objectives and multi-modal ones that use image-text constrastive training. We observe that existing multi-modal methods do not outperform their single-modal counterparts on vision transfer learning tasks. We derive an information-theoretical view to explain these benchmark results, which provides insight into how to design a novel vision learner. Inspired by this insight, we present a new visual representation pre-training method, MUlti-modal Generator~(MUG), that learns from scalable web sourced image-text data. MUG ach
    
[^53]: PIVOT：基于提示的视频持续学习方法

    PIVOT: Prompting for Video Continual Learning. (arXiv:2212.04842v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2212.04842](http://arxiv.org/abs/2212.04842)

    PIVOT 是一种针对视频数据的基于提示的持续学习方法，利用预训练模型减少了可训练参数的数量以及相应的遗忘，并且是第一个能够在无域内预训练情况下有效使用提示机制的方法。

    

    现代机器学习管道由于数据可用性、存储配额、隐私规定和昂贵的注释过程而受限。这些约束使得在此类动态注释集上训练和更新大规模模型变得困难或不可能。不断学习直接解决了这个问题，其最终目标是设计出方法，在新（未见过的）类别中，深度神经网络能够有效地学习相关模式，而不会对先前学习的类别的表现产生显著影响。在本文中，我们解决了针对视频数据的持续学习问题。我们介绍了PIVOT，这是一种新颖的方法，利用来自图像领域的预训练模型的广泛知识，从而减少了可训练参数的数量以及相应的遗忘。与以前的方法不同，我们的方法是第一种在没有域内预训练的情况下有效使用提示机制进行持续学习的方法。我们的实验结果表明，PIVOT 改善了现有技术的性能。

    Modern machine learning pipelines are limited due to data availability, storage quotas, privacy regulations, and expensive annotation processes. These constraints make it difficult or impossible to train and update large-scale models on such dynamic annotated sets. Continual learning directly approaches this problem, with the ultimate goal of devising methods where a deep neural network effectively learns relevant patterns for new (unseen) classes, without significantly altering its performance on previously learned ones. In this paper, we address the problem of continual learning for video data. We introduce PIVOT, a novel method that leverages extensive knowledge in pre-trained models from the image domain, thereby reducing the number of trainable parameters and the associated forgetting. Unlike previous methods, ours is the first approach that effectively uses prompting mechanisms for continual learning without any in-domain pre-training. Our experiments show that PIVOT improves sta
    
[^54]: 用于地理空间探索的视觉主动搜索框架

    A Visual Active Search Framework for Geospatial Exploration. (arXiv:2211.15788v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.15788](http://arxiv.org/abs/2211.15788)

    本论文提出了基于视觉主动搜索框架的强化学习方法，以协助地理空间探索问题。该框架以广阔区域的图像为输入，通过有限的查询来验证目标对象示例的存在，并提供关于目标对象空间分布的信息。

    

    许多问题可以被视为利用航空影像协助的地理空间搜索的形式，例如检测盗猎活动和人口贩卖等。本论文在视觉主动搜索（VAS）框架中建立了这类问题的模型，该框架以广阔区域的图像为输入，并旨在尽可能多地识别目标对象的示例。通过一系列有限的查询，VAS会验证在给定区域内是否存在示例。VAS的一个关键特征是，每个这样的查询都会提供有关目标对象空间分布的信息，超出了可视化所捕捉的内容（例如，由于空间相关性）。我们提出了针对VAS的强化学习方法，利用完全注释的搜索任务集作为训练数据来学习搜索策略，并将输入图像的特征与主动搜索状态的自然表示相结合。此外，我们提出了域自适应技术，以改善决策时的策略。

    Many problems can be viewed as forms of geospatial search aided by aerial imagery, with examples ranging from detecting poaching activity to human trafficking. We model this class of problems in a visual active search (VAS) framework, which takes as input an image of a broad area, and aims to identify as many examples of a target object as possible. It does this through a limited sequence of queries, each of which verifies whether an example is present in a given region. A crucial feature of VAS is that each such query is informative about the spatial distribution of target objects beyond what is captured visually (for example, due to spatial correlation). We propose a reinforcement learning approach for VAS that leverages a collection of fully annotated search tasks as training data to learn a search policy, and combines features of the input image with a natural representation of active search state. Additionally, we propose domain adaptation techniques to improve the policy at decis
    
[^55]: 移动机器人的2D推动操作中的集体智能

    Collective Intelligence for 2D Push Manipulation with Mobile Robots. (arXiv:2211.15136v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2211.15136](http://arxiv.org/abs/2211.15136)

    本研究利用基于软体物理模拟器的规划器和基于注意力的神经网络，实现了移动机器人2D协作推动操作中的集体智能，比传统方法具有更好的性能并具备环境自适应能力。

    

    自然系统通常表现出能够自我组织和适应变化的集体智能，但大多数人工系统缺乏这种等效性。本文探讨使用移动机器人进行2D协作推动操作的集体智能系统的可能性。我们展示了将从软体物理模拟派生的规划器提炼为基于注意力的神经网络后，我们的多机器人推动操作系统相对于基线系统具有更好的性能，并可适应外部扰动和环境变化完成任务。

    While natural systems often present collective intelligence that allows them to self-organize and adapt to changes, the equivalent is missing in most artificial systems. We explore the possibility of such a system in the context of cooperative 2D push manipulations using mobile robots. Although conventional works demonstrate potential solutions for the problem in restricted settings, they have computational and learning difficulties. More importantly, these systems do not possess the ability to adapt when facing environmental changes. In this work, we show that by distilling a planner derived from a differentiable soft-body physics simulator into an attention-based neural network, our multi-robot push manipulation system achieves better performance than baselines. In addition, our system also generalizes to configurations not seen during training and is able to adapt toward task completions when external turbulence and environmental changes are applied. Supplementary videos can be foun
    
[^56]: 离线强化学习的策略指导模仿方法

    A Policy-Guided Imitation Approach for Offline Reinforcement Learning. (arXiv:2210.08323v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.08323](http://arxiv.org/abs/2210.08323)

    该论文提出了一种策略指导模仿方法，它将传统的奖励最大化策略分解成引导策略和执行策略。在训练期间，引导策略和执行策略在仅使用数据集中的数据的情况下进行学习。在评估期间，引导策略通过指引执行策略以最大化奖励，起到“先知”的作用，允许合理的超出分布泛化。

    

    在离线强化学习中，通常可以将方法分为两种：基于强化学习和基于模仿学习。基于强化学习的方法原则上可以享受超出分布的泛化，但会出现错误的离策略评估。基于模仿学习的方法避免了离策略评估，但过于保守，难以超越数据集。本研究提出了一种替代方法，继承了模仿式方法的训练稳定性，同时仍允许逻辑上的超出分布泛化。我们将离线强化学习中传统的奖励最大化策略分解成引导策略和执行策略。在训练期间，引导策略和执行策略仅使用数据集中的数据，以监督和解耦的方式进行学习。在评估期间，引导策略通过告诉执行策略应该去哪里以最大化奖励，来指引执行策略的走向，作为“先知”。如此，我们的算法允许“状态组合性”，从而实现了合理的超出分布泛化。

    Offline reinforcement learning (RL) methods can generally be categorized into two types: RL-based and Imitation-based. RL-based methods could in principle enjoy out-of-distribution generalization but suffer from erroneous off-policy evaluation. Imitation-based methods avoid off-policy evaluation but are too conservative to surpass the dataset. In this study, we propose an alternative approach, inheriting the training stability of imitation-style methods while still allowing logical out-of-distribution generalization. We decompose the conventional reward-maximizing policy in offline RL into a guide-policy and an execute-policy. During training, the guide-poicy and execute-policy are learned using only data from the dataset, in a supervised and decoupled manner. During evaluation, the guide-policy guides the execute-policy by telling where it should go so that the reward can be maximized, serving as the \textit{Prophet}. By doing so, our algorithm allows \textit{state-compositionality} f
    
[^57]: 无目标后门水印：朝着无害和隐蔽的数据集版权保护迈进

    Untargeted Backdoor Watermark: Towards Harmless and Stealthy Dataset Copyright Protection. (arXiv:2210.00875v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2210.00875](http://arxiv.org/abs/2210.00875)

    本文提出了一种针对数据集版权保护的无害和隐蔽的无目标后门水印方案，可以达到与最先进方案相当或更好的水印效果，并证明对模型性能无害且隐蔽。

    

    深度神经网络已在实践中展现出了其优越性。可以说，深度神经网络的快速发展在很大程度上得益于高质量（开源）数据集，研究人员和开发人员可以在此基础上轻松地评估和改进他们的学习方法。由于数据收集通常是耗时甚至昂贵的，如何保护其版权具有重要意义并值得进一步探索。本文重新审视了数据集的所有权验证。我们发现，现有的验证方法由于有目标的后门水印的特性，会在受保护的数据集上训练的深度神经网络中引入新的安全风险。为了解决这个问题，在本文中，我们探讨了无目标后门水印方案，其中异常的模型行为不是确定性的。具体而言，我们介绍了两个分散度，并证明了它们的相关性，基于此我们在受污染标签和干净标签攻击设置下设计了无目标后门水印。实验结果表明，我们提出的方法在水印提取的准确性和模型性能上都能够达到甚至超过现有最先进的方案。此外，我们提出的方法还被证明对模型性能无害且隐蔽，不会引入任何可检测的扭曲或故障。

    Deep neural networks (DNNs) have demonstrated their superiority in practice. Arguably, the rapid development of DNNs is largely benefited from high-quality (open-sourced) datasets, based on which researchers and developers can easily evaluate and improve their learning methods. Since the data collection is usually time-consuming or even expensive, how to protect their copyrights is of great significance and worth further exploration. In this paper, we revisit dataset ownership verification. We find that existing verification methods introduced new security risks in DNNs trained on the protected dataset, due to the targeted nature of poison-only backdoor watermarks. To alleviate this problem, in this work, we explore the untargeted backdoor watermarking scheme, where the abnormal model behaviors are not deterministic. Specifically, we introduce two dispersibilities and prove their correlation, based on which we design the untargeted backdoor watermark under both poisoned-label and clean
    
[^58]: 通过本地几何角度理解VAEs的对抗鲁棒性

    Adversarial robustness of VAEs through the lens of local geometry. (arXiv:2208.03923v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.03923](http://arxiv.org/abs/2208.03923)

    本文证明了对手攻击VAEs的最佳方法是利用由编码器和解码器网络引起的随机回溯度规张量的方向偏差。

    

    在对变分自编码器（VAEs）进行无监督攻击时，对手会找到一个输入样本中的小扰动，从而显着改变其潜在空间编码，从而损害了一个固定编码器的重构。这种脆弱性已知的原因是潜在后验分布的近似与先验分布之间的不匹配导致的潜在空间扭曲。因此，输入样本中的微小变化可能会将其编码移动到潜在空间中的低/零密度区域，从而产生无限制的生成。本文证明了对手攻击VAEs的最佳方法是利用由编码器和解码器网络引起的随机回溯度规张量的方向偏差。编码器的回溯度规张量测量它从输入到潜在空间的微小潜在体积的变化。因此，它可以被视为分析输入扰动导致潜在空间扭曲效果的镜头。

    In an unsupervised attack on variational autoencoders (VAEs), an adversary finds a small perturbation in an input sample that significantly changes its latent space encoding, thereby compromising the reconstruction for a fixed decoder. A known reason for such vulnerability is the distortions in the latent space resulting from a mismatch between approximated latent posterior and a prior distribution. Consequently, a slight change in an input sample can move its encoding to a low/zero density region in the latent space resulting in an unconstrained generation. This paper demonstrates that an optimal way for an adversary to attack VAEs is to exploit a directional bias of a stochastic pullback metric tensor induced by the encoder and decoder networks. The pullback metric tensor of an encoder measures the change in infinitesimal latent volume from an input to a latent space. Thus, it can be viewed as a lens to analyse the effect of input perturbations leading to latent space distortions. We
    
[^59]: DeepIPC：在真实环境中实现自动驾驶的深度感知和控制

    DeepIPC: Deeply Integrated Perception and Control for an Autonomous Vehicle in Real Environments. (arXiv:2207.09934v4 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2207.09934](http://arxiv.org/abs/2207.09934)

    本文介绍了DeepIPC，一个端到端自动驾驶模型，能够同时处理感知和控制任务。实验结果表明，DeepIPC具有最佳的可驾性和多任务性能，甚至比其他模型所需的参数更少。

    

    本文提出DeepIPC，一个处理自动驾驶中感知和控制的端到端模型。该模型由感知和控制器两个主要部分组成。感知模块使用RGBD图像执行语义分割和鸟瞰图语义映射，并提供它们的编码特征。同时，控制器模块使用GNSS定位和角速度的测量来处理这些特征，估计一系列路径点和潜在特征。然后，使用两个不同的代理将路径点和潜在特征转换为一组导航控制，以驱动车辆。该模型通过预测行车记录和在不同真实场景下进行自动驾驶来进行评估。实验结果表明，DeepIPC即使使用较少的参数也能实现最佳可驾性和多任务性能，优于其他模型。相关代码可在 https://github.com 找到。

    We propose DeepIPC, an end-to-end autonomous driving model that handles both perception and control tasks in driving a vehicle. The model consists of two main parts, perception and controller modules. The perception module takes an RGBD image to perform semantic segmentation and bird's eye view (BEV) semantic mapping along with providing their encoded features. Meanwhile, the controller module processes these features with the measurement of GNSS locations and angular speed to estimate waypoints that come with latent features. Then, two different agents are used to translate waypoints and latent features into a set of navigational controls to drive the vehicle. The model is evaluated by predicting driving records and performing automated driving under various conditions in real environments. The experimental results show that DeepIPC achieves the best drivability and multi-task performance even with fewer parameters compared to the other models. Codes are available at https://github.co
    
[^60]: PatchCensor：通过穷尽测试提高视觉Transformers的补丁鲁棒性认证

    PatchCensor: Patch Robustness Certification for Transformers via Exhaustive Testing. (arXiv:2111.10481v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2111.10481](http://arxiv.org/abs/2111.10481)

    PatchCensor是一种用于视觉Transformer的补丁鲁棒性认证方法，基于全面测试并考虑最坏的补丁攻击情境，能够提供受保证的准确性。

    

    与其他经典神经网络一样，视觉Transformer（ViT）也被认为是高度非线性的，并且容易受到自然和对抗性补丁扰动的干扰。在真实的工业环境中，这种限制可能对ViT的部署产生威胁，尤其是在安全关键场景中。本文提出了PatchCensor，旨在通过全面测试来证明ViT的补丁鲁棒性。我们试图通过考虑最坏的补丁攻击情境来提供可证明的保证。与那些可能被自适应攻击破坏的对抗性补丁的经验性防御措施不同，认证可靠的方法可以在某些条件下提供对于任意攻击的受保证的准确性。但是，现有的鲁棒性认证主要基于鲁棒性训练，这通常需要大量的训练工作，并且会在正常样本上牺牲模型的性能。为了弥合这一差距，PatchCensor旨在通过检测和剔除不合规则的区域来提高整个系统的鲁棒性。

    Vision Transformer (ViT) is known to be highly nonlinear like other classical neural networks and could be easily fooled by both natural and adversarial patch perturbations. This limitation could pose a threat to the deployment of ViT in the real industrial environment, especially in safety-critical scenarios. In this work, we propose PatchCensor, aiming to certify the patch robustness of ViT by applying exhaustive testing. We try to provide a provable guarantee by considering the worst patch attack scenarios. Unlike empirical defenses against adversarial patches that may be adaptively breached, certified robust approaches can provide a certified accuracy against arbitrary attacks under certain conditions. However, existing robustness certifications are mostly based on robust training, which often requires substantial training efforts and the sacrifice of model performance on normal samples. To bridge the gap, PatchCensor seeks to improve the robustness of the whole system by detecting
    
[^61]: 从SLAM到情境感知：挑战与综述

    From SLAM to Situational Awareness: Challenges and Survey. (arXiv:2110.00273v3 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2110.00273](http://arxiv.org/abs/2110.00273)

    本文研究旨在连接广泛的多学科现有知识，为移动机器人构建完整的情境感知（SA）系统，以提升其自主能力。

    

    移动机器人能够高效、安全地执行复杂任务的能力受限于其对环境（即情况）的了解。先进的推理、决策和执行技能使智能体能够在未知环境中自主行动。情境感知（SA）是人类的一种基本能力，已在心理学、军事、航空航天和教育等各个领域深入研究。然而，在机器人领域中尚未考虑情境感知，而是专注于单一的概念，如感知、空间感知、传感器融合、状态估计和同时定位与映射（SLAM）。因此，本研究旨在连接广泛的多学科现有知识，为我们认为对于自主性至关重要的移动机器人完整的SA系统铺平道路。为此，我们定义了构建机器人SA的主要组成部分及其所涉及的领域。因此，本文

    The capability of a mobile robot to efficiently and safely perform complex missions is limited by its knowledge of the environment, namely the situation. Advanced reasoning, decision-making, and execution skills enable an intelligent agent to act autonomously in unknown environments. Situational Awareness (SA) is a fundamental capability of humans that has been deeply studied in various fields, such as psychology, military, aerospace, and education. Nevertheless, it has yet to be considered in robotics, which has focused on single compartmentalized concepts such as sensing, spatial perception, sensor fusion, state estimation, and Simultaneous Localization and Mapping (SLAM). Hence, the present research aims to connect the broad multidisciplinary existing knowledge to pave the way for a complete SA system for mobile robotics that we deem paramount for autonomy. To this aim, we define the principal components to structure a robotic SA and their area of competence. Accordingly, this paper
    
[^62]: Federated Submodel Optimization for Hot and Cold Data Features（热点和冷门数据特征的联邦子模型优化）

    Federated Submodel Optimization for Hot and Cold Data Features. (arXiv:2109.07704v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2109.07704](http://arxiv.org/abs/2109.07704)

    本文提出了一种针对联邦学习稀疏数据特征的联邦子模型平均算法(FedSubAvg)，该算法可以有效避免数据稀疏问题导致的计算下降，并保证每个模型参数的全局更新期望等于涉及它的客户端的本地更新平均值。该算法的新度量元素梯度范数可以更好地表征在稀疏数据上的联邦优化收敛。

    

    本文研究联邦学习中的实际数据特征，其中客户端的非独立同分布数据具有稀疏特征，并且某个客户端的本地数据通常仅涉及完整模型的一小部分，称为子模型。由于数据稀疏性，传统的联邦平均（FedAvg）算法或其变体将严重减慢，因为在更新全局模型时，每个客户端除其子模型外的零更新被不准确地聚合。因此，我们提出了联邦子模型平均（FedSubAvg），确保每个模型参数的全局更新的期望等于涉及它的客户端的本地更新的平均值。我们通过推导一个称为元素梯度范数的新度量上界来理论上证明了FedSubAvg的收敛速度。特别地，这个新度量可以表征在稀疏数据上的联邦优化收敛，而传统的平方梯度度量则无法完成这一任务。

    We study practical data characteristics underlying federated learning, where non-i.i.d. data from clients have sparse features, and a certain client's local data normally involves only a small part of the full model, called a submodel. Due to data sparsity, the classical federated averaging (FedAvg) algorithm or its variants will be severely slowed down, because when updating the global model, each client's zero update of the full model excluding its submodel is inaccurately aggregated. Therefore, we propose federated submodel averaging (FedSubAvg), ensuring that the expectation of the global update of each model parameter is equal to the average of the local updates of the clients who involve it. We theoretically proved the convergence rate of FedSubAvg by deriving an upper bound under a new metric called the element-wise gradient norm. In particular, this new metric can characterize the convergence of federated optimization over sparse data, while the conventional metric of squared g
    
[^63]: 利用机器学习技术改进半导体器件建模的电子设计自动化

    Improving Semiconductor Device Modeling for Electronic Design Automation by Machine Learning Techniques. (arXiv:2105.11453v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2105.11453](http://arxiv.org/abs/2105.11453)

    本文提出一种利用机器学习技术改进半导体器件建模的方法，通过自我增强策略和变分自编码器技术，只需少量实验数据点即可实现高精度预测，有效降低平均绝对误差，具有广泛应用价值。

    

    机器学习（ML）技术在技术计算机辅助设计（TCAD）方法中的应用对半导体行业有很大的益处。然而，ML模型的性能在很大程度上依赖于训练数据集的质量和数量。由于器件制造的复杂性和成本，这些数据集在半导体行业中很难获得。本文提出一种自我增强策略，利用变分自编码器（VAE）技术改进基于ML的器件建模。这些技术只需要少量实验数据点，并且不依赖于TCAD工具。为了证明我们的方法的有效性，我们将其应用于镓氮器件中欧姆电阻值的深度神经网络预测任务。在预测实验结果时，平均绝对误差降低了70%。我们的方法具有固有的灵活性，可以轻松适应各种任务，因此具有广泛应用价值。

    The semiconductors industry benefits greatly from the integration of Machine Learning (ML)-based techniques in Technology Computer-Aided Design (TCAD) methods. The performance of ML models however relies heavily on the quality and quantity of training datasets. They can be particularly difficult to obtain in the semiconductor industry due to the complexity and expense of the device fabrication. In this paper, we propose a self-augmentation strategy for improving ML-based device modeling using variational autoencoder-based techniques. These techniques require a small number of experimental data points and does not rely on TCAD tools. To demonstrate the effectiveness of our approach, we apply it to a deep neural network-based prediction task for the Ohmic resistance value in Gallium Nitride devices. A 70% reduction in mean absolute error when predicting experimental results is achieved. The inherent flexibility of our approach allows easy adaptation to various tasks, thus making it highl
    
[^64]: 无分割关注：BERT是否需要中间层？

    Undivided Attention: Are Intermediate Layers Necessary for BERT?. (arXiv:2012.11881v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2012.11881](http://arxiv.org/abs/2012.11881)

    本论文调查了中间层对于BERT在下游任务表现的作用，表明删除中间层数量可以减少模型参数和训练时间，同时对下游任务的影响较小，学习到的表示不受影响。

    

    最近，基于BERT的模型在解决各种自然语言处理（NLP）任务方面非常成功，例如阅读理解、自然语言推理、情感分析等。所有基于BERT的架构都具有一个自注意力块，后跟一个中间层块作为基本构建组件。然而，文献中缺乏对包含这些中间层的强有力的理由。在这项工作中，我们调查了中间层对下游任务的整体网络性能的重要性。我们表明，减少中间层数量并修改BERT-BASE的架构会导致下游任务的微小损失，同时减少模型的参数和训练时间。此外，我们使用中心化内核对齐和探测线性分类器来获得对我们的架构修改的洞见，并证明删除中间层对学习到的表示没有显着影响。

    In recent times, BERT-based models have been extremely successful in solving a variety of natural language processing (NLP) tasks such as reading comprehension, natural language inference, sentiment analysis, etc. All BERT-based architectures have a self-attention block followed by a block of intermediate layers as the basic building component. However, a strong justification for the inclusion of these intermediate layers remains missing in the literature. In this work we investigate the importance of intermediate layers on the overall network performance of downstream tasks. We show that reducing the number of intermediate layers and modifying the architecture for BERT-BASE results in minimal loss in fine-tuning accuracy for downstream tasks while decreasing the number of parameters and training time of the model. Additionally, we use centered kernel alignment and probing linear classifiers to gain insight into our architectural modifications and justify that removal of intermediate l
    
[^65]: CokeBERT: 增强预训练语言模型的上下文知识选择与嵌入

    CokeBERT: Contextual Knowledge Selection and Embedding towards Enhanced Pre-Trained Language Models. (arXiv:2009.13964v5 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2009.13964](http://arxiv.org/abs/2009.13964)

    本文提出了一种名为Coke的新框架，用于将上下文知识动态选择和嵌入到预训练语言模型中，以避免对输入文本匹配效果差的冗余和模糊知识的影响，并在知识驱动的自然语言处理任务上取得了优异表现。

    

    近期有许多工作致力于利用知识图谱中的外部异构知识来增强预训练语言模型（PLMs），并在各种知识驱动的自然语言处理任务上获得了一致的改进。然而，大多数这些知识增强的PLMs仅嵌入KG中的静态子图（“知识上下文”），而不考虑PLMs可能根据特定文本（“文本上下文”）动态变化所需的知识。因此，本文提出了一种新的框架，即Coke，用于为PLMs动态选择上下文知识并根据文本上下文嵌入知识上下文，从而可以避免KG中的冗余和模糊知识对输入文本的匹配效果不佳的影响。实验证明，Coke在典型的知识驱动的自然语言处理任务上优于各种基准模型，表明了利用动态知识上下文进行语言理解的有效性。除了性能方面的改进，所选择的动态知识能够提高语言模型的可解释性，以指导深入理解PLMs所学习的知识。

    Several recent efforts have been devoted to enhancing pre-trained language models (PLMs) by utilizing extra heterogeneous knowledge in knowledge graphs (KGs) and achieved consistent improvements on various knowledge-driven NLP tasks. However, most of these knowledge-enhanced PLMs embed static sub-graphs of KGs ("knowledge context"), regardless of that the knowledge required by PLMs may change dynamically according to specific text ("textual context"). In this paper, we propose a novel framework named Coke to dynamically select contextual knowledge and embed knowledge context according to textual context for PLMs, which can avoid the effect of redundant and ambiguous knowledge in KGs that cannot match the input text. Our experimental results show that Coke outperforms various baselines on typical knowledge-driven NLP tasks, indicating the effectiveness of utilizing dynamic knowledge context for language understanding. Besides the performance improvements, the dynamically selected knowle
    

