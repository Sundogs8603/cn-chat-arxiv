# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Monitoring Algorithmic Fairness.](http://arxiv.org/abs/2305.15979) | 本论文介绍了一种对机器学习系统进行算法公正性监控的方法，该方法可以监测公正性属性，如人口平等和等同机会，为确保机器学习系统不基于敏感属性歧视个人提供有用的工具。 |
| [^2] | [Quantifying the Intrinsic Usefulness of Attributional Explanations for Graph Neural Networks with Artificial Simulatability Studies.](http://arxiv.org/abs/2305.15961) | 本文介绍了如何用人工模拟研究量化解释对图神经网络的内在有用性，以解决评估解释质量的挑战性问题。研究表明，相关解释可以显著提高样本的性能。 |
| [^3] | [Improved Algorithms for Allen's Interval Algebra by Dynamic Programming with Sublinear Partitioning.](http://arxiv.org/abs/2305.15950) | Allen的区间代数的运行时间从naive $2^{O(n^2)}$到$O^*((1.0615n)^{n})$，本文提出了一种新的动态规划方法，通过次线性划分，获得了$O^*((\frac{cn}{\log{n}})^{n})$的主要改进。 |
| [^4] | [Learning to Act through Evolution of Neural Diversity in Random Neural Networks.](http://arxiv.org/abs/2305.15945) | 本文提出一种通过演化神经元中心参数来获得一组多样化的神经元，从而执行复杂计算的方法。研究显示，演化神经参数就足以使代理在不优化任何突触权重的情况下解决强化学习任务。 |
| [^5] | [How to Turn Your Knowledge Graph Embeddings into Generative Models via Probabilistic Circuits.](http://arxiv.org/abs/2305.15944) | 本文提出了将知识图嵌入模型转化为生成模型的方法，允许精确MLE学习、有效抽样新的三元组以及保证逻辑约束，获得了比原始KGEs更具伸缩性的性能。 |
| [^6] | [Learning DAGs from Data with Few Root Causes.](http://arxiv.org/abs/2305.15936) | 该论文提出了一种新的算法，能够从仅有少量根因的数据中学习DAGs，并证明了其可识别性，并在性能上优于以前的方法。 |
| [^7] | [A Diagnosis Algorithms for a Rotary Indexing Machine.](http://arxiv.org/abs/2305.15934) | 本文提出了一种基于产品角度的诊断算法，该算法通过跟踪产品在径向索引机中的步骤来诊断故障。文章分析了RIM的特性以及这些特性如何影响这些机器中的故障诊断。 |
| [^8] | [Towards a Capability Assessment Model for the Comprehension and Adoption of AI in Organisations.](http://arxiv.org/abs/2305.15922) | 本文介绍了一个五级AI能力评估模型（AI-CAM）和相关的AI能力矩阵（AI-CM），用于帮助组织的决策者了解基于机器学习技术的AI数据分析用例、使用语义技术的知识工程和代表数据的知识表示以及旨在模仿人类推理和决策的AI解决方案的核心能力要求。 |
| [^9] | [Learning Assumption-based Argumentation Frameworks.](http://arxiv.org/abs/2305.15921) | 提出了一种基于假设的论证框架(Assumption-based Argumentation, ABA)的学习方法，与现有的方法相比，该方法能够更好地学习一般规则的例外情况，具有更好的预测精度。 |
| [^10] | [A Fast Algorithm for Consistency Checking Partially Ordered Time.](http://arxiv.org/abs/2305.15917) | 本文提出了一种快速算法来检查部分有序时间的一致性，相比已有算法时间复杂度更低，为$O^*((0.26n)^n)$，是通过对类似于全序的结构进行精细枚举，并贪婪地扩展向解决方案实现的。 |
| [^11] | [MTCue: Learning Zero-Shot Control of Extra-Textual Attributes by Leveraging Unstructured Context in Neural Machine Translation.](http://arxiv.org/abs/2305.15904) | 本研究提出了一种新颖的神经机器翻译框架MTCue，它将所有上下文解释为文本，实现了可转移性并学会了以零样本的方式利用额外的文本属性（如礼貌和对话行为等变量）的控制。在四个语言对的翻译方向上，MTCue的翻译质量显着提高，BLEU（+0.88）和Comet（+1.58）。 |
| [^12] | [Exponential Smoothing for Off-Policy Learning.](http://arxiv.org/abs/2305.15877) | 本文研究了离线学习中最小化风险的倒数倾向评分(IPS)的平滑正则化，推导出了可处理、可扩展、可解释的学习证明，并确定了在何种情况下不需要正则化IPS。 |
| [^13] | [Linguistic Properties of Truthful Response.](http://arxiv.org/abs/2305.15875) | 该论文研究了LLM的不真实回答现象，发现GPT-3模型对给定提示的回答在语言特性上很相似。同时，该论文证明了在没有评估内容本身的情况下，仅依赖模型回答的风格成分即可分类真实性。 |
| [^14] | [Jointprop: Joint Semi-supervised Learning for Entity and Relation Extraction with Heterogeneous Graph-based Propagation.](http://arxiv.org/abs/2305.15872) | 提出了Jointprop框架，用于基于异构图传播的联合半监督实体和关系提取，采用一个统一的异构图来使用未标记数据中的全局结构信息，提高实体和关系提取性能。 |
| [^15] | [Extracting Text Representations for Terms and Phrases in Technical Domains.](http://arxiv.org/abs/2305.15867) | 本文提出了一种无监督的文本编码方法，使用小型基于字符的模型重构大型预训练嵌入矩阵，其可以在技术领域内达到与句子编码器相同的质量，但大小为后者的五分之一，计算时间能快10倍。 |
| [^16] | [Sequential Integrated Gradients: a simple but effective method for explaining language models.](http://arxiv.org/abs/2305.15853) | 本文提出了一种名为顺序 Integrated Gradients（SIG）的解释语言模型的新方法，通过保持其他单词不变，仅在基线和感兴趣的单词之间创建插值来计算句子中每个单词的重要性，并用训练的令牌“mask”替换基线令牌“pad”来显着改善解释效果。 |
| [^17] | [Self-contradictory Hallucinations of Large Language Models: Evaluation, Detection and Mitigation.](http://arxiv.org/abs/2305.15852) | 本文对大型语言模型的自相矛盾幻觉进行了评估、检测和缓解，探究了这一幻觉形式的普遍存在性。通过设计框架有效触发自相矛盾，发现不同语言模型中这种现象都频繁出现。ChatGPT和GPT-4能够准确识别自相矛盾，而Vicuna-13B则有些困难。 |
| [^18] | [Improved Multi-Scale Grid Rendering of Point Clouds for Radar Object Detection Networks.](http://arxiv.org/abs/2305.15836) | 本文提出了一种新的体系结构，即多尺度 KPPillarsBEV，以缓解雷达目标检测中从点云数据转化为网格结构过程中的信息丢失问题，并提出了一种新的网格渲染方法 KPBEV。实验结果表明，该方法显著优于现有方法。 |
| [^19] | [PDE+: Enhancing Generalization via PDE with Adaptive Distributional Diffusion.](http://arxiv.org/abs/2305.15835) | 本文提出了PDE+，一种通过自适应分布扩散的偏微分方程来增强神经网络泛化的方法，实验结果表明，在几个基准数据集上，不仅能取得最先进或有竞争性的性能，还有更少的训练轮次和网络参数。 |
| [^20] | [Unifying gradient regularization for Heterogeneous Graph Neural Networks.](http://arxiv.org/abs/2305.15811) | 本研究提出了一种新的梯度正则化方法Grug，旨在统一HGNN中的图形拓扑和节点特征的正则化，并解决了过度平滑、非鲁棒性等问题，综合效果和效率优于几种现有方法。 |
| [^21] | [ChatGPT for PLC/DCS Control Logic Generation.](http://arxiv.org/abs/2305.15809) | 本文探索使用大语言模型生成PLC/DCS控制逻辑，并创造了用于测试和比较这些模型控制能力的基准测试集合。 |
| [^22] | [Lucy-SKG: Learning to Play Rocket League Efficiently Using Deep Reinforcement Learning.](http://arxiv.org/abs/2305.15801) | 本文介绍了一种基于强化学习模型的 Lucy-SKG 学习玩“火箭联盟”游戏的方法，在性能上超过了该游戏中排名最高的两个机器人，并成为了最先进的代理程序。 |
| [^23] | [Feature space reduction method for ultrahigh-dimensional, multiclass data: Random forest-based multiround screening (RFMS).](http://arxiv.org/abs/2305.15793) | 本研究提出了一种新的算法RFMS，该算法可以有效的处理数千个类别的超高维度数据，通过锦标赛排序和特征重要性选择实现特征选择，具有与行业标准的特征筛选方法相当的性能和更多的优点。 |
| [^24] | [Service Composition in the ChatGPT Era.](http://arxiv.org/abs/2305.15788) | ChatGPT-like系统能够支持自动服务组合领域，并提出了新的研究领域。 |
| [^25] | [On the Planning Abilities of Large Language Models -- A Critical Investigation.](http://arxiv.org/abs/2305.15771) | 本文研究了大型语言模型在规划能力上的表现，发现LLMs自主生成可执行计划的能力有限，但在启发式模式下表现更有前途。 |
| [^26] | [TLNets: Transformation Learning Networks for long-range time-series prediction.](http://arxiv.org/abs/2305.15770) | 提出了一种基于变换学习的网络架构设计方案，实现了在学习中增强感受野和融合不同尺度特征的优点，使用了傅里叶变换、奇异值分解、矩阵乘法和卷积块等四种不同的变换机制进行构建，模型表现优异，超过了几种最先进的解决方案。 |
| [^27] | [MERGE: Fast Private Text Generation.](http://arxiv.org/abs/2305.15769) | 该论文提出了MERGE，一个基于Transformer语言模型的快速私有文本生成框架。实验结果表明，MERGE在保护隐私的同时，实现了26.5倍的加速和80%的通信字节数减少。 |
| [^28] | [Language-Guided 3D Object Detection in Point Cloud for Autonomous Driving.](http://arxiv.org/abs/2305.15765) | 本文提出了一个基于语言指导的点云三维物体检测方法，包含了一种有效的令牌融合策略，并实现了与自然语言语义融合的激光雷达物体探测。我们的方法在跨模态实验中达到了最先进的性能，并且具有高效的优势。 |
| [^29] | [Healing Unsafe Dialogue Responses with Weak Supervision Signals.](http://arxiv.org/abs/2305.15757) | 本论文提出了一种名为TEMP的无监督伪标签采样方法，可以自动分配潜在的安全响应，解决大规模对话系统的不安全响应生成问题。 |
| [^30] | [TransWorldNG: Traffic Simulation via Foundation Model.](http://arxiv.org/abs/2305.15743) | TransWorldNG是一种基于数据驱动算法和图形计算技术的交通仿真器，能够自动学习交通模式并生成更真实的交通模式，具有更好的可扩展性。 |
| [^31] | [MPE4G: Multimodal Pretrained Encoder for Co-Speech Gesture Generation.](http://arxiv.org/abs/2305.15740) | 本文提出了一种名为MPE4G的多模态预训练编码器框架，用于共同语音手势生成。该方法通过自我监督学习来训练基于多头注意力的编码器以包含每种模态的信息，并且可以生成逼真的共同语音手势，即使存在缺失或嘈杂的输入模态。 |
| [^32] | [Analysis and tuning of a three-term DMC.](http://arxiv.org/abs/2305.15735) | 本文研究了一种使用三项QP的DMC算法，可以比传统的两项DMC实现更高的性能和鲁棒性。 |
| [^33] | [On the Impact of Knowledge Distillation for Model Interpretability.](http://arxiv.org/abs/2305.15734) | 本文研究表明，知识蒸馏可以提高模型的可解释性和准确性，并归因于从老师模型传递给学生模型的类相似信息的存在。 |
| [^34] | [Efficient Neural Music Generation.](http://arxiv.org/abs/2305.15719) | 提出了一种名为MeLoDy的高效神经音乐生成模型，它可以在实时、单CPU环境下生成各种风格和长度的多样化、高质量的音乐，并显著减少了计算成本。 |
| [^35] | [Knowledge Diffusion for Distillation.](http://arxiv.org/abs/2305.15712) | 本文提出了一种新型的知识蒸馏方法DiffKD，通过扩散模型去除学生特征中的噪声信息，提取清晰的有价值信息，实现更好的蒸馏效果。 |
| [^36] | [The Benefits of Being Distributional: Small-Loss Bounds for Reinforcement Learning.](http://arxiv.org/abs/2305.15703) | 通过小损失边界的视角，我们提供了分布式RL好处的一个解释，该边界与实例相关的最优成本成比例。如果最优成本很小，分布式方法优于非分布式方法。 |
| [^37] | [Rethink Diversity in Deep Learning Testing.](http://arxiv.org/abs/2305.15698) | 本文提出针对深度学习测试的六个具体指标来检测和缓解其漏洞，认为许多DNN测试任务应该是定向测试问题而不是通用测试任务。 |
| [^38] | [Privacy Protectability: An Information-theoretical Approach.](http://arxiv.org/abs/2305.15697) | 本文提出了一种新的度量标准——隐私可保护度，用于衡量在给定某种视频分析任务的情况下，视频流可以得到多大程度的保护，并使用信息理论工具对其进行量化。 |
| [^39] | [Deep Neural Networks in Video Human Action Recognition: A Review.](http://arxiv.org/abs/2305.15692) | 本文综述了视频行为识别中深度神经网络的应用。与传统机器学习框架相比，该方法优势显著，具有更高的准确性和更广泛的应用性。 |
| [^40] | [Zero-shot Approach to Overcome Perturbation Sensitivity of Prompts.](http://arxiv.org/abs/2305.15689) | 本研究提出了一种零样本方法，自动生成多个类似于基础提示的高质量提示，并使用新的度量方法进行排名，从而克服了提示的扰动敏感性，并在情感分类任务中具有较高的准确性。 |
| [^41] | [RewriteLM: An Instruction-Tuned Large Language Model for Text Rewriting.](http://arxiv.org/abs/2305.15685) | 本文介绍了 RewriteLM，一种指令调整的大型语言模型，用于长篇文本重写。同时，我们提出了一个名为 OpenRewriteEval 的基准测试，用于评估各种类型的开放式长篇文本重写。我们采用新的策略来促进多样的指令和偏好数据生成，从而为长篇文本重写提供更好的评估手段。 |
| [^42] | [Revisiting non-English Text Simplification: A Unified Multilingual Benchmark.](http://arxiv.org/abs/2305.15678) | 这篇论文介绍了MultiSim基准，它包含了27个资源、12种语言超过1.7百万个复杂-简单的句子对。使用该基准进行预训练的多语言语言模型可以在非英语环境中带来令人兴奋的性能提升，并且俄语在跨语言转移方面表现出强大的性能。 |
| [^43] | [PROTO: Iterative Policy Regularized Offline-to-Online Reinforcement Learning.](http://arxiv.org/abs/2305.15669) | 提出了一种名为PROTO的新型离线到在线强化学习框架，通过迭代演化的规范化项，克服了现有方法的性能次优、适应性有限和计算效率不足等问题。PROTO可以极大地适应各种方法，且仅需最小的额外计算，实现了高效和有效的离线到在线强化学习。 |
| [^44] | [How to escape sharp minima.](http://arxiv.org/abs/2305.15659) | 本文探讨了发现平坦极小值点的算法问题，在支持找到局部近似平坦的极小值点的基础上，设计了两种算法：一种基于梯度的算法，一种基于最小化锐度的算法。 |
| [^45] | [Towards Total Online Unsupervised Anomaly Detection and Localization in Industrial Vision.](http://arxiv.org/abs/2305.15652) | 本文提出一种名为LeMO的在线学习图像异常检测方法，本方法使用学习性记忆，包含了基于对比学习的异常检测损失函数，可同时优化内存和图像特征，能够适应工业在线流数据需求，具有高效性和简单性。 |
| [^46] | [Meta Adaptive Task Sampling for Few-Domain Generalization.](http://arxiv.org/abs/2305.15644) | 本文提出了少域通用化框架和元自适应任务采样（MATS）过程，旨在利用极少量的新任务域来学习可推广的模型，并在基础任务上获得知识以提高模型的域外泛化性能。 |
| [^47] | [Learning-Based Automatic Synthesis of Software Code and Configuration.](http://arxiv.org/abs/2305.15642) | 本文介绍了一种基于学习的自动软件代码和配置生成方法，其中通过遗传算法和神经网络训练的适应度函数自动合成程序，使用协方差矩阵适应进化策略完成程序合成，并采用强化学习方法实现配置生成。 |
| [^48] | [Patient Outcome Predictions Improve Operations at a Large Hospital Network.](http://arxiv.org/abs/2305.15629) | 美国一家大型医院网络与学术界和顾问合作，开发了机器学习模型，可以准确预测患者的短期和长期结果，同时将预测与医生的预测相结合可以使更多的患者出院并减少了再入院。 |
| [^49] | [Reverse Engineering Self-Supervised Learning.](http://arxiv.org/abs/2305.15614) | 本文逆向工程了自监督学习（SSL）训练表示，发现SSL训练过程中的正则化项本质上促进了样本基于语义标签的聚类。SSL训练的表示与语义类别更加接近，对齐在训练过程中增加，而且在网络深度加深时增加。 |
| [^50] | [Size Generalizability of Graph Neural Networks on Biological Data: Insights and Practices from the Spectral Perspective.](http://arxiv.org/abs/2305.15611) | 本文通过谱角度的方法，研究了GNNs的尺寸可泛化性问题，并在真实生物数据集上进行了实验，发现GNNs在度分布和谱分布偏移时均表现敏感，在同一数据集的大图上的性能仍然下降，揭示了 GNNs的尺寸可泛化性问题。 |
| [^51] | [Control invariant set enhanced safe reinforcement learning: improved sampling efficiency, guaranteed stability and robustness.](http://arxiv.org/abs/2305.15602) | 本文提出了一种控制不变集增强强化学习的方法，该方法包括两个学习阶段以提高稳定性保证和采样效率。作者研究了该方法的健壮性，并将CIS纳入到奖励设计、初始状态采样和状态重置程序中。 |
| [^52] | [Text-Augmented Open Knowledge Graph Completion via Pre-Trained Language Models.](http://arxiv.org/abs/2305.15597) | TAGREAL是一种可自动生成高质量查询提示信息，从大型文本语料库中检索支持信息以从PLM中探测知识的方法，用于开放知识图谱补全中，在两个基准数据集上取得了最先进的表现，并且即使在有限的训练数据情况下，仍然具有突出的性能。 |
| [^53] | [Distributed Online Rollout for Multivehicle Routing in Unmapped Environments.](http://arxiv.org/abs/2305.15596) | 本文提出了一个在线策略，通过分布式的协调策略解决了未建图环境下多车辆路径规划问题。 |
| [^54] | [How do humans perceive adversarial text? A reality check on the validity and naturalness of word-based adversarial attacks.](http://arxiv.org/abs/2305.15587) | 本研究通过调查人们对抗性文本样本的可感知性，得出现有文本攻击在人类参与的现实世界场景中是不切实际的，提供了更为现实的对NLP模型鲁棒性的评估。 |
| [^55] | [Inverse square Levy walk emerging universally in goal-oriented tasks.](http://arxiv.org/abs/2305.15559) | 本研究证明了反平方Levy步态（称为Cauchy步态）在目标导向任务中普遍出现。 |
| [^56] | [From Interactive to Co-Constructive Task Learning.](http://arxiv.org/abs/2305.15535) | 本文提出将机器人交互学习的研究重点放在协同构建过程上，以使机器人能够在日常情况下从非专家用户那里学习。 |
| [^57] | [The Larger They Are, the Harder They Fail: Language Models do not Recognize Identifier Swaps in Python.](http://arxiv.org/abs/2305.15507) | 该论文揭示了大型语言模型对Python标识符交换的识别问题，尤其是在逆比例缩放现象影响下表现更为显著。这表明LLM缺乏深刻、抽象的理解，无法胜任与训练偏差的任务。 |
| [^58] | [Large Language Models for User Interest Journeys.](http://arxiv.org/abs/2305.15498) | 该论文提出了使用大型语言模型(LLMs)对用户兴趣进行建模的方法，并通过定义兴趣旅程，提出了一种模型旨在提高推荐的质量，并提供了可解释性和新颖性。 |
| [^59] | [SPRING: GPT-4 Out-performs RL Algorithms by Studying Papers and Reasoning.](http://arxiv.org/abs/2305.15486) | SPRING是一个新的方法，能够在开放世界游戏中表现出色，它通过阅读游戏的原始学术论文并使用所学知识进行推理并玩游戏。。 |
| [^60] | [Trends and Challenges Towards an Effective Data-Driven Decision Making in UK SMEs: Case Studies and Lessons Learnt from the Analysis of 85 SMEs.](http://arxiv.org/abs/2305.15454) | 本文以英国西米德兰地区85家中小企业为例，探讨了中小企业在进行有效数据驱动决策方面面临的趋势和挑战，尤其是由于有限的资源和融资渠道受限等原因，如何在技能和IT投资方面寻找平衡。 |
| [^61] | [PromptNER: Prompting For Named Entity Recognition.](http://arxiv.org/abs/2305.15444) | PromptNER是一种基于提示的命名实体识别算法，利用LLM生成潜在实体列表并提供解释，在少样本NER和跨领域NER方面实现了最先进性能。 |
| [^62] | [PulseNet: Deep Learning ECG-signal classification using random augmentation policy and continous wavelet transform for canines.](http://arxiv.org/abs/2305.15424) | 该论文提出了一种利用深度学习对犬的ECG信号进行自动分类的方法，通过使用随机数据增强策略和连续小波变换，分类精度得到了提高。 |
| [^63] | [A Hybrid Semantic-Geometric Approach for Clutter-Resistant Floorplan Generation from Building Point Clouds.](http://arxiv.org/abs/2305.15420) | 本研究提出了一种基于混合语义几何方法的建筑点云无杂乱物平面生成方法，通过预处理输入点云、使用PointNet++网络标记点、最后通过几何方法生成无杂乱物的平面。 |
| [^64] | [Machine learning-assisted close-set X-ray diffraction phase identification of transition metals.](http://arxiv.org/abs/2305.15410) | 本文探究了如何利用机器学习从过渡金属及其氧化物的X射线衍射数据中预测晶体结构相，提出的机器学习框架实现了竞争性能，表明了机器学习对X射线衍射和晶体结构确定领域的潜在影响。 |
| [^65] | [Knowledge-Design: Pushing the Limit of Protein Deign via Knowledge Refinement.](http://arxiv.org/abs/2305.15151) | 本文提出了一种知识感知模块来提炼低质量残基，引入记忆检索机制实现了超过50%的训练时间节省，并取得了较好的性能表现，是蛋白质设计领域的一次创新。 |
| [^66] | [Contrastive Training of Complex-Valued Autoencoders for Object Discovery.](http://arxiv.org/abs/2305.15001) | 介绍了一种架构修改和新颖对比学习方法，大大改进了同步性基模型，首次获得了一类同步性基模型，在多物体彩色数据集中无监督地发现物体。 |
| [^67] | [CMOT: Cross-modal Mixup via Optimal Transport for Speech Translation.](http://arxiv.org/abs/2305.14635) | CMOT是一种用于跨模态语音翻译的方法，通过最优传输找到语音和文本序列之间的对齐，并在标记级别上混合不同模态的序列，实现了在有限数据下更好的性能表现。 |
| [^68] | [Unsupervised Discovery of Continuous Skills on a Sphere.](http://arxiv.org/abs/2305.14377) | 本文提出了一种名为DISCS的方法，可以无限学习不同的连续技能，在MuJoCo Ant机器人控制环境中展示了其比其他方法更具多样性。 |
| [^69] | [Weakly Supervised AUC Optimization: A Unified Partial AUC Approach.](http://arxiv.org/abs/2305.14258) | 本文提出了WSAUC，一种解决弱监督下AUC优化问题的统一框架，它包括噪声标签学习、正-无标签学习、多实例学习和半监督学习场景，并提出了一种新型的部分AUC——反转部分AUC（rpAUC），作为鲁棒的AUC最大化训练目标，为各种弱监督场景下的AUC优化提供了一种通用解决方案。 |
| [^70] | [Algorithmic Unfairness through the Lens of EU Non-Discrimination Law: Or Why the Law is not a Decision Tree.](http://arxiv.org/abs/2305.13938) | 本文分析了算法不公平性的示例，并从欧盟非歧视法的角度讲述了其中涉及的不公正。同时，本文建立了框架以帮助决策者确定算法公平性指标以符合欧盟非歧视法理。 |
| [^71] | [Trend-Based SAC Beam Control Method with Zero-Shot in Superconducting Linear Accelerator.](http://arxiv.org/abs/2305.13869) | 本文提出了一种基于趋势的零-shot束流控制方法，在 CAFe II 和 LPI 中取得了良好的效果，可以将校正时间缩短到人类专家所需时间的十分之一。 |
| [^72] | [EXACT: Extensive Attack for Split Learning.](http://arxiv.org/abs/2305.12997) | 本文提出了一种名为EXACT的方法，可以安全地在分布式学习中进行梯度交换，同时保护隐私、保持准确性和效率性。 |
| [^73] | [FIT: Far-reaching Interleaved Transformers.](http://arxiv.org/abs/2305.12689) | FIT是一种基于Transformer的架构，将数据标记分组，使用局部层和全局层进行操作。通过交错使用这些层并使用交叉注意力促进信息交换，FIT在一系列任务中均实现最先进的性能。 |
| [^74] | [Markov $\alpha$-Potential Games: Equilibrium Approximation and Regret Analysis.](http://arxiv.org/abs/2305.12553) | 本文提出了一种新的框架来研究马尔可夫博弈，即马尔可夫 $\alpha$-势博弈。介绍了两种算法来计算其中的纳什均衡，且表明这些算法能够找到近似均衡。 |
| [^75] | [Deep Radar Inverse Sensor Models for Dynamic Occupancy Grid Maps.](http://arxiv.org/abs/2305.12409) | 该研究提出了一种基于深度学习的雷达逆向传感器模型，用于将稀疏雷达检测映射到极坐标测量网格，并生成动态网格地图，实验结果表明该方法优于手工制作的几何ISM。与最先进的深度学习方法相比，该方法为从有限视场的雷达中学习极坐标方案的单帧测量网格的第一个方法。 |
| [^76] | [EEG and EMG dataset for the detection of errors introduced by an active orthosis device.](http://arxiv.org/abs/2305.11996) | 该论文介绍了一个包含EEG和EMG记录的数据集，这些记录来自被协助使用主动矫形器移动右臂的受试者。研究者故意引入了一些错误，以检测新的错误检测方法。 |
| [^77] | [Comparing Software Developers with ChatGPT: An Empirical Investigation.](http://arxiv.org/abs/2305.11837) | 这篇论文调查了ChatGPT与软件开发人员的比较，认为全面的实证研究可以帮助回答基于AI的计算是否可以提高生产力甚至取代软件工程师进行软件开发的争议。 |
| [^78] | [StereoVAE: A lightweight stereo matching system through embedded GPUs.](http://arxiv.org/abs/2305.11566) | 本论文提出了通过嵌入式GPU实现的轻量级立体匹配系统-StereoVAE，该系统采用基于VAE的小型神经网络对传统匹配方法生成的小尺寸粗糙视差图进行上采样与细化，达到了提高匹配精度和保证实时处理的目的。 |
| [^79] | [Zero-Shot Text Classification via Self-Supervised Tuning.](http://arxiv.org/abs/2305.11442) | 本文提出了一种基于自监督调整的零样本文本分类算法，通过使用无标签数据来调整语言模型，通过学习预测段落中的第一句话，实现了对未见过任务的零样本推断，模型不需要注释数据进行元调整，对模板的选择不敏感，并在实验中取得不错的结果。 |
| [^80] | [UniControl: A Unified Diffusion Model for Controllable Visual Generation In the Wild.](http://arxiv.org/abs/2305.11147) | 本文提出了UniControl，一种新的生成基础模型，能够整合广泛的可控条件与图像任务，并仍然允许任意语言提示，UniControl能够进行像素级精确的图像生成，其中视觉条件主要影响生成的结构，语言提示则引导样式和上下文。我们提出了一种统一的扩散模型，它将扩散过程和变分自动编码器的优点结合起来，实验结果表明UniControl在可控性和图像质量方面均优于现有的最先进模型。 |
| [^81] | [Integrating Item Relevance in Training Loss for Sequential Recommender Systems.](http://arxiv.org/abs/2305.10824) | 本文提出了一种融合项目相关性的新型训练损失函数，用于提高序列推荐系统对噪声的鲁棒性和性能。 |
| [^82] | [Assessing Hidden Risks of LLMs: An Empirical Study on Robustness, Consistency, and Credibility.](http://arxiv.org/abs/2305.10235) | 本研究是一项关于大型语言模型方面的实证研究，对主流语言模型进行了大量查询和分析，结果发现这些模型存在着鲁棒性、一致性和可信性方面的潜在风险。 |
| [^83] | [AAAI 2022 Fall Symposium: System-1 and System-2 realized within the Common Model of Cognition.](http://arxiv.org/abs/2305.09091) | 该论文提出了一个基于认知通用模型的 System-1 和 System-2 实现方法，解决了其区分不清的问题。通用模型提供了对涉及 System-1 和 System-2 的计算单元、基础机制和其对学习、元认知和情感的影响的全面视野。 |
| [^84] | [sustain.AI: a Recommender System to analyze Sustainability Reports.](http://arxiv.org/abs/2305.08711) | sustain.AI是一个智能的、上下文感知的推荐系统，可以帮助审计师、金融投资者以及广大公众高效地分析公司的可持续性报告，并通过与GRI标准匹配来提供更好的推荐精度。 |
| [^85] | [Using LLM-assisted Annotation for Corpus Linguistics: A Case Study of Local Grammar Analysis.](http://arxiv.org/abs/2305.08339) | 本文研究了使用基于大语言模型的聊天机器人自动标注文本的潜力，重点考察了从本地语法角度观察道歉言语行为构成的功能元素的程度，并比较了不同模型在注释任务中的表现，结果表明Bing聊天机器人在任务中表现优于ChatGPT和人类标注员。 |
| [^86] | [TinyStories: How Small Can Language Models Be and Still Speak Coherent English?.](http://arxiv.org/abs/2305.07759) | 本文针对小型语言模型生成连贯的英文文本难题，引入了一个合成故事数据集 TinyStories，并探索小型模型规模、结构复杂度和训练数据规模对于语言模型表现的影响，证明了仅含 200 万参数的简单语言模型也能产生连贯的短故事。 |
| [^87] | [SemEval-2023 Task 2: Fine-grained Multilingual Named Entity Recognition (MultiCoNER 2).](http://arxiv.org/abs/2305.06586) | 该论文介绍了SemEval-2023 Task 2的研究发现，任务旨在通过使用MultiCoNER V2数据集，识别12种语言中复杂的细粒度命名实体。最优方法是将外部知识融入transformer模型，最具挑战性的是媒体标题和产品名称等实体类型。 |
| [^88] | [Multi-Prompt with Depth Partitioned Cross-Modal Learning.](http://arxiv.org/abs/2305.06221) | 本研究提出了划分的多模态提示（PMPO）方法，将软提示从一个扩展到多个，通过连接多个提示到视觉编码器的不同深度上，能够更好地捕捉视觉表示的上下文深度，与传统单提示方法相比，在下游视觉语言任务中具有更好的表现。 |
| [^89] | [How Do In-Context Examples Affect Compositional Generalization?.](http://arxiv.org/abs/2305.04835) | 本文提出了CoFe测试套件来调查上下文组合泛化。实验结果表明，上下文示例应该在结构上与测试用例类似，相互之间应该不同，而且单独地简单。 |
| [^90] | [GPT for Semi-Automated Data Science: Introducing CAAFE for Context-Aware Automated Feature Engineering.](http://arxiv.org/abs/2305.03403) | 介绍了一种名为CAAFE的上下文感知自动特征工程方法，它利用大型语言模型根据数据集描述生成更多具有语义意义的特征，能够提高大多数数据集的性能，平均ROC AUC表现提高至0.822。 |
| [^91] | [Few-shot Event Detection: An Empirical Study and a Unified View.](http://arxiv.org/abs/2305.01901) | 本文从两个实用的设置出发，分析比较了十种代表性的小样本事件检测方法，归纳总结出了原型方法的性能优越性，并在此基础上提出了一种简单且有效的方法。 |
| [^92] | [Complex Logical Reasoning over Knowledge Graphs using Large Language Models.](http://arxiv.org/abs/2305.01157) | 本文提出了一种使用大型语言模型的解耦方法，将复杂的知识图谱推理形式化为上下文知识图搜索和抽象逻辑查询推理的组合，与现有方法相比，它在多个逻辑查询结构的标准基准数据集上都表现出更好的性能，并且在更高复杂性的查询中获得了显着的性能提升。 |
| [^93] | [Game-based Platforms for Artificial Intelligence Research.](http://arxiv.org/abs/2304.13269) | 本文回顾了基于游戏的人工智能研究平台，讨论了不同研究领域和创意设计在其中的应用和发展，并探讨了其未来趋势。 |
| [^94] | [Bandit Algorithm Driven by a Classical Random Walk and a Quantum Walk.](http://arxiv.org/abs/2304.10118) | 本文提出了一个基于量子游走的算法解决多臂赌博问题，将探索和利用与量子游走行为联系起来，相比于经典随机游走策略实现了高性能。 |
| [^95] | [Bandit-Based Policy Invariant Explicit Shaping for Incorporating External Advice in Reinforcement Learning.](http://arxiv.org/abs/2304.07163) | 本文研究了如何基于Bandit方法将外部建议融入到强化学习中，并提出了三种不同的塑形算法：UCB-PIES（UPIES）， Racing-PIES（RPIES）和Lazy PIES（LPIES）。实验结果表明这些算法在样本复杂度、学习速度和形状质量方面都取得了良好的效果。 |
| [^96] | [RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment.](http://arxiv.org/abs/2304.06767) | RAFT框架引入了奖励排名微调方法，用于对齐生成型基础模型，以解决强化学习带来的低效和不稳定性问题。 |
| [^97] | [LLMMaps -- A Visual Metaphor for Stratified Evaluation of Large Language Models.](http://arxiv.org/abs/2304.00457) | LLMMaps是一种分层评估大型语言模型性能的可视化技术，能够揭示取得高准确度和产生幻觉的子领域，并指导模型的进一步发展。 |
| [^98] | [HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace.](http://arxiv.org/abs/2303.17580) | 用ChatGPT作为任务规划工具，利用大型语言模型（LLM）作为控制器来整合现有的AI模型，解决复杂的AI任务。 |
| [^99] | [ISimDL: Importance Sampling-Driven Acceleration of Fault Injection Simulations for Evaluating the Robustness of Deep Learning.](http://arxiv.org/abs/2303.08035) | 本论文提出了一种新方法ISimDL，利用神经元灵敏度生成重要性采样，加速故障注入模拟，有效评估了先进的DL系统对硬件故障的韧性，同时显著减少了所需的模拟数量。 |
| [^100] | [Hybrid Approach for Solving Real-World Bin Packing Problem Instances Using Quantum Annealers.](http://arxiv.org/abs/2303.01977) | 该研究提出了一个量子-经典混合框架，名为Q4RealBPP，可以考虑真实世界的限制特征，解决三维装箱问题，支持工业和物流行业的需求。 |
| [^101] | [Approximating Energy Market Clearing and Bidding With Model-Based Reinforcement Learning.](http://arxiv.org/abs/2303.01772) | 本文研究了基于模型的强化学习用于能源市场清算和出价的应用方法。通过用学习的OPF代理模型以及明确的市场规则替代传统计算方法，本方法极大地减少了训练时间并适用于市场设计和更现实地建模市场参与者。 |
| [^102] | [One Fits All:Power General Time Series Analysis by Pretrained LM.](http://arxiv.org/abs/2302.11939) | 本论文提出了一种称为 Frozen Pretrained Transformer (FPT) 的预训练模型，利用从数十亿标记训练出来的语言或 CV 模型进行时间序列分析的所有主要类型任务的微调，进而使其在所有任务中都具备着最先进的性能和泛化能力。 |
| [^103] | [Project and Probe: Sample-Efficient Domain Adaptation by Interpolating Orthogonal Features.](http://arxiv.org/abs/2302.05441) | 本文提出了一种名为Projec and Probe（Pro$^2$）的方法，该方法能够通过插值学习多样化的特征来适应目标分布，极大地提高了使用少量目标数据进行迁移学习的效果。 |
| [^104] | [Memory-Based Meta-Learning on Non-Stationary Distributions.](http://arxiv.org/abs/2302.03067) | 本文研究基于记忆的元学习在非平稳分布上的应用，重点关注在部分可观察环境中的自然语言和动作-观察序列，研究表明各种类型的基于记忆的神经模型可以准确地逼近贝叶斯最优算法，并执行潜在参数的贝叶斯推断。 |
| [^105] | [A Toy Model of Universality: Reverse Engineering How Networks Learn Group Operations.](http://arxiv.org/abs/2302.03025) | 本文通过研究小型神经网络如何学习实现群组合来研究普适性假设。它提出了一种新的算法，使神经网络能够通过任意有限群来实现组合，从而完全描述网络在此任务上学习的电路和特征族。 |
| [^106] | [Minimizing Trajectory Curvature of ODE-based Generative Models.](http://arxiv.org/abs/2301.12003) | 本论文提出通过训练正向过程以最小化生成轨迹的曲率，来优化ODE/SDE-based生成模型的采样速度，实验表明此方法有效。 |
| [^107] | [Selective Explanations: Leveraging Human Input to Align Explainable AI.](http://arxiv.org/abs/2301.09656) | 本研究提出一种通过利用人类输入生成选择性解释的通用框架，以弥合可解释人工智能（XAI）与人类解释的差距，并且在决策支持任务中进行了实验证明其有效性。 |
| [^108] | [State of the Art and Potentialities of Graph-level Learning.](http://arxiv.org/abs/2301.05860) | 本文主要介绍了图嵌入学习，它将一组图作为输入，自动提取特征并将图编码为低维表示。深度学习的发展使得图嵌入学习适应了日益增长的图规模，从而获得了很多成功，但是传统学习方法通常会受到计算瓶颈的影响。 |
| [^109] | [Continual Contrastive Finetuning Improves Low-Resource Relation Extraction.](http://arxiv.org/abs/2212.10823) | 本文提出了一种使用连续对比微调的方法来改进低资源关系提取，通过使用一致的对比学习目标预训练和微调RE模型，以及多中心对比损失来允许一个关系形成多个聚类。实验结果表明该方法可以显着提高低资源情况和领域中的关系提取性能。 |
| [^110] | [Do language models have coherent mental models of everyday things?.](http://arxiv.org/abs/2212.10029) | 语言模型缺乏对日常物品的一致性心理模型，会因此出现荒谬的解决方法。虽然最先进的预训练语言模型具有这些实体的知识碎片，但它们无法为所有实体产生一致且正确的心理模型。语言模型训练可以改善这种情况。 |
| [^111] | [BLOOM+1: Adding Language Support to BLOOM for Zero-Shot Prompting.](http://arxiv.org/abs/2212.09535) | 本文在BLOOM模型中应用语言适应策略，将其适应到新语言上，并在八种新语言的零样本提示表现中提升了性能。适配器微调比大模型的持续预训练更有效，提示性能主要由语言适应数据的大小确定。 |
| [^112] | [CALIME: Causality-Aware Local Interpretable Model-Agnostic Explanations.](http://arxiv.org/abs/2212.05256) | 本文提出CALIME方法，将因果知识融入可解释性人工智能方法中，以解决特征独立性的缺陷，并取得了优于初始方法的黑盒模型模拟保真度和解释稳定性的表现。 |
| [^113] | [Lattice-Free Sequence Discriminative Training for Phoneme-Based Neural Transducers.](http://arxiv.org/abs/2212.04325) | 本文提出了三种无格栅训练目标，用于基于音素的神经传递器的最终后验输出，与使用N-best列表的方法相比，无格栅方法在训练期间消除了假设生成的步骤，从而导致更高效的训练，在单词错误率上表现也有6.5％的相对改进。 |
| [^114] | [MEGAN: Multi-Explanation Graph Attention Network.](http://arxiv.org/abs/2211.13236) | MEGAN是一种可以在多个通道中产生节点和边的说明性解释的图注意力网络，对于改进图回归预测的可解释性至关重要。此外，它是完全可微的，可以在解释监督方式下主动地训练说明。 |
| [^115] | [Multi-Level Knowledge Distillation for Out-of-Distribution Detection in Text.](http://arxiv.org/abs/2211.11300) | 本文提出了一种多层知识蒸馏方法，融合了语言模型的训练和微调方法来进行文本中的离群检测，实验结果表明其有效性。 |
| [^116] | [Scalar Invariant Networks with Zero Bias.](http://arxiv.org/abs/2211.08486) | 本文证明了在解决许多图像任务(例如图像分类)时可以忽略偏置，并且零偏置神经网络在实际图像分类任务中表现良好，同时具有标量 (乘法) 不变性，从而在改变对比度时仍能保持预测不变。 |
| [^117] | [Using Persuasive Writing Strategies to Explain and Detect Health Misinformation.](http://arxiv.org/abs/2211.05985) | 本研究旨在通过使用说服性写作技巧的文本段落进行分类来增加自动化虚假信息检测的新层次，以产生可解释的理由。我们提出了一个包含常见说服性写作策略的注释方案和数据集，并使用 RoBERTa 文本分类模型进行实验。 |
| [^118] | [Impact of Adversarial Training on Robustness and Generalizability of Language Models.](http://arxiv.org/abs/2211.05523) | 本研究比较了在变压器语言模型中不同的对抗训练方法，并发现预训练数据增强或训练时间输入扰动可以实现更好的鲁棒性，而训练中使用的嵌入空间扰动可以显著提高泛化性。神经元的语言相关性分析表明，这些改进是由于存在“更专业”的神经元。这是首个对语言模型对抗训练不同方法进行全面比较的工作。 |
| [^119] | [Mastering the Unsupervised Reinforcement Learning Benchmark from Pixels.](http://arxiv.org/abs/2209.12016) | 本研究提出了一种新的无监督模型驱动的强化学习方法，结合混合规划器进行任务感知的微调策略，在无监督强化学习基准测试中获得了93.59%的整体标准化性能，超过之前的基线，是解决无监督强化学习中泛化能力提升的一项重要进展。 |
| [^120] | [DICE: Data-Efficient Clinical Event Extraction with Generative Models.](http://arxiv.org/abs/2208.07989) | 介绍了一种稳健高效的基于生成模型的临床事件抽取方法DICE，引入了对比性学习目标和特殊标记，共同训练实体提及和事件抽取等辅助任务，所提出的MACCROBAT-EE数据集为临床事件抽取提供了基准测试。 |
| [^121] | [Claim-Dissector: An Interpretable Fact-Checking System with Joint Re-ranking and Veracity Prediction.](http://arxiv.org/abs/2207.14116) | Claim-Dissector是一款联合重排和真实性预测的可解释的事实核查系统，可以识别与声明相关的证据，并确定声明的真实性。该系统的个人贡献以及证据所支持或反驳声明的贡献都可以被识别。 |
| [^122] | [Multitrack Music Transformer.](http://arxiv.org/abs/2207.06983) | 这篇论文提出了一种新的多轨音乐表示方法，可以支持各种不同的乐器，在短的序列长度下实现了性能上的显著提升，同时提出了一种新方法用于分析音乐自我关注，并验证了模型更关注与当前音符形成和谐跨度和位于同一八度的音符。 |
| [^123] | [DGraph: A Large-Scale Financial Dataset for Graph Anomaly Detection.](http://arxiv.org/abs/2207.03579) | 本文介绍了一个名为DGraph的大规模金融数据集，提供了全面观察和广泛实验，揭示了异常节点和正常节点不同的特征和结构，标记和未标记节点都是检测欺诈者的必不可少的信息。 |
| [^124] | [When are Post-hoc Conceptual Explanations Identifiable?.](http://arxiv.org/abs/2206.13872) | 本论文提出了可识别的概念发现方法，可以恢复出多个已知的概念，以确保解释的可靠性。对于具有依赖关系的概念，提出了两种新的方法，利用图像生成过程的功能组合性质。该方法明显优于现有方法。 |
| [^125] | [Off-Policy Evaluation with Online Adaptation for Robot Exploration in Challenging Environments.](http://arxiv.org/abs/2204.03140) | 本文提出了一种在线自适应离策略评估方法，通过学习状态价值函数来为机器人在真实挑战性环境中探索提供指导，并设计了一种内在奖励函数，使机器人能够在稀疏外部奖励的情况下获得更多信息。 |
| [^126] | [Hierarchical Path-planning from Speech Instructions with Spatial Concept-based Topometric Semantic Mapping.](http://arxiv.org/abs/2203.10820) | 该论文提出了一种基于拓扑语义映射的分层路径规划方法，可以通过语音指令和waypoint灵活地规划路径，包括地点连通性，提供了一种快速近似推理方法。 |
| [^127] | [Contrastive Learning of Sociopragmatic Meaning in Social Media.](http://arxiv.org/abs/2203.07648) | 提出了一种社交媒体中社会语用意义的对比学习框架，该框架能够学习可迁移的任务不可知表示学习，并在各种对比学习框架中表现最佳。 |
| [^128] | [HyperMixer: An MLP-based Low Cost Alternative to Transformers.](http://arxiv.org/abs/2203.03691) | HyperMixer是一种低成本的基于MLP的Transformer替代方案，通过动态形成标记混合MLP来实现自然语言理解，其性能比替代方案好，并可与Transformer媲美，成本更低。 |
| [^129] | [pNLP-Mixer: an Efficient all-MLP Architecture for Language.](http://arxiv.org/abs/2202.04350) | pNLP-Mixer是一种新型的MLP-Mixer模型，不需要嵌入层，用于设备上高效的自然语言处理，可以达到基于transformer架构的大型预训练语言模型相近的性能，却只需要很少的资源。 |

# 详细

[^1]: 监控算法公正性

    Monitoring Algorithmic Fairness. (arXiv:2305.15979v1 [cs.CY])

    [http://arxiv.org/abs/2305.15979](http://arxiv.org/abs/2305.15979)

    本论文介绍了一种对机器学习系统进行算法公正性监控的方法，该方法可以监测公正性属性，如人口平等和等同机会，为确保机器学习系统不基于敏感属性歧视个人提供有用的工具。

    

    机器学习系统被广泛应用于对人类做出决策，因此确保它们是公正的（即不会基于敏感属性歧视个人）非常重要。本文介绍了针对模型未知但假定具有马尔可夫链结构的系统的算法公正性的运行时验证。我们引入了一种规范语言，可以建模许多常见的算法公正性属性，例如人口平等，等同机会和社会负担。我们构建了监视器，观察给定系统生成的一系列事件，并在每次观察之后输出该系统在到目前为止的运行中有多公正或有多倾向于偏见的定量估计。该估计值在给定的置信水平和变量误差范围下被证明是正确的，其中误差范围随着观察序列的长度而变得更紧。我们的监视器有两种类型，分别使用频率派和贝叶斯统计推断技术。

    Machine-learned systems are in widespread use for making decisions about humans, and it is important that they are fair, i.e., not biased against individuals based on sensitive attributes. We present runtime verification of algorithmic fairness for systems whose models are unknown, but are assumed to have a Markov chain structure. We introduce a specification language that can model many common algorithmic fairness properties, such as demographic parity, equal opportunity, and social burden. We build monitors that observe a long sequence of events as generated by a given system, and output, after each observation, a quantitative estimate of how fair or biased the system was on that run until that point in time. The estimate is proven to be correct modulo a variable error bound and a given confidence level, where the error bound gets tighter as the observed sequence gets longer. Our monitors are of two types, and use, respectively, frequentist and Bayesian statistical inference techniqu
    
[^2]: 用人工模拟研究量化解释对图神经网络的内在有用性

    Quantifying the Intrinsic Usefulness of Attributional Explanations for Graph Neural Networks with Artificial Simulatability Studies. (arXiv:2305.15961v1 [cs.LG])

    [http://arxiv.org/abs/2305.15961](http://arxiv.org/abs/2305.15961)

    本文介绍了如何用人工模拟研究量化解释对图神经网络的内在有用性，以解决评估解释质量的挑战性问题。研究表明，相关解释可以显著提高样本的性能。

    

    尽管可解释的人工智能越来越重要，但评估解释质量仍然是一个具有挑战性的问题。因为与人类实验相关的高成本，通常使用各种代理度量来近似量化解释质量。在本研究中，我们将人工模拟研究扩展到图神经网络领域。我们使用支持解释的图神经网络进行模拟性研究，量化归因图解释的内在有用性，而不是昂贵的人类试验。我们进行了广泛的消融研究，以调查所提出的分析在哪些条件下最有意义。我们还在实际的图分类和回归数据集上验证了我们的方法适用性。我们发现相关解释可以显著提高样本的性能。

    Despite the increasing relevance of explainable AI, assessing the quality of explanations remains a challenging issue. Due to the high costs associated with human-subject experiments, various proxy metrics are often used to approximately quantify explanation quality. Generally, one possible interpretation of the quality of an explanation is its inherent value for teaching a related concept to a student. In this work, we extend artificial simulatability studies to the domain of graph neural networks. Instead of costly human trials, we use explanation-supervisable graph neural networks to perform simulatability studies to quantify the inherent usefulness of attributional graph explanations. We perform an extensive ablation study to investigate the conditions under which the proposed analyses are most meaningful. We additionally validate our methods applicability on real-world graph classification and regression datasets. We find that relevant explanations can significantly boost the samp
    
[^3]: 通过次线性划分的动态规划改进了Allen的区间代数算法。

    Improved Algorithms for Allen's Interval Algebra by Dynamic Programming with Sublinear Partitioning. (arXiv:2305.15950v1 [cs.CC])

    [http://arxiv.org/abs/2305.15950](http://arxiv.org/abs/2305.15950)

    Allen的区间代数的运行时间从naive $2^{O(n^2)}$到$O^*((1.0615n)^{n})$，本文提出了一种新的动态规划方法，通过次线性划分，获得了$O^*((\frac{cn}{\log{n}})^{n})$的主要改进。

    

    Allen的区间代数是定性时间推理中最知名的演算法之一，在人工智能领域有许多应用。最近，在NP困难推理任务的细化复杂度上出现了激增，将运行时间从naive $2^{O(n^2)}$提高到$O^*((1.0615n)^{n})$，对于单位区间和有限数量重叠区间的算法甚至更快（$O^*(\cdot)$符号抑制了多项式因子）。尽管这些改进，目前已知的最佳下界仍然只有$2^{o(n)}$（在指数时间假设下），而重大跃进无论是向前还是向后似乎都需要基本的计算复杂性进展。本文提出了一种解决NP困难定性推理问题的新框架，我们称之为次线性划分动态规划。使用这种技术，我们获得了Allen的区间代数的主要改进$O^*((\frac{cn}{\log{n}})^{n})$。

    Allen's interval algebra is one of the most well-known calculi in qualitative temporal reasoning with numerous applications in artificial intelligence. Recently, there has been a surge of improvements in the fine-grained complexity of NP-hard reasoning tasks, improving the running time from the naive $2^{O(n^2)}$ to $O^*((1.0615n)^{n})$, with even faster algorithms for unit intervals a bounded number of overlapping intervals (the $O^*(\cdot)$ notation suppresses polynomial factors). Despite these improvements the best known lower bound is still only $2^{o(n)}$ (under the exponential-time hypothesis) and major improvements in either direction seemingly require fundamental advances in computational complexity. In this paper we propose a novel framework for solving NP-hard qualitative reasoning problems which we refer to as dynamic programming with sublinear partitioning. Using this technique we obtain a major improvement of $O^*((\frac{cn}{\log{n}})^{n})$ for Allen's interval algebra. To
    
[^4]: 随机神经网络中通过神经元多样性的演化学习行为

    Learning to Act through Evolution of Neural Diversity in Random Neural Networks. (arXiv:2305.15945v1 [cs.NE])

    [http://arxiv.org/abs/2305.15945](http://arxiv.org/abs/2305.15945)

    本文提出一种通过演化神经元中心参数来获得一组多样化的神经元，从而执行复杂计算的方法。研究显示，演化神经参数就足以使代理在不优化任何突触权重的情况下解决强化学习任务。

    

    生物神经系统由不同类别神经元组成的网络构成，它们是多样化、复杂的信息处理器。在大多数人工神经网络中，神经计算被抽象为一个激活函数，通常是在整个网络或同一层中的所有神经元之间共享的；神经网络的训练主要关注突触权重的优化。本文推荐通过优化神经元中心参数来获得一组多样化的神经元，从而执行复杂计算。我们展示了这种方法的潜力，证明仅通过演化神经参数即可使代理在不优化任何突触权重的情况下解决各种强化学习任务。虽然不致力于精确的生物模型，但通过比当前常见的做法更大程度地对神经元参数化，我们能够探讨随机神经网络中神经多样性提供的计算能力。这些结果引起了人们的极大兴趣。

    Biological nervous systems consist of networks of diverse, sophisticated information processors in the form of neurons of different classes. In most artificial neural networks (ANNs), neural computation is abstracted to an activation function that is usually shared between all neurons within a layer or even the whole network; training of ANNs focuses on synaptic optimization. In this paper, we propose the optimization of neuro-centric parameters to attain a set of diverse neurons that can perform complex computations. Demonstrating the promise of the approach, we show that evolving neural parameters alone allows agents to solve various reinforcement learning tasks without optimizing any synaptic weights. While not aiming to be an accurate biological model, parameterizing neurons to a larger degree than the current common practice, allows us to ask questions about the computational abilities afforded by neural diversity in random neural networks. The presented results open up interestin
    
[^5]: 如何通过概率电路将您的知识图嵌入转化为生成模型

    How to Turn Your Knowledge Graph Embeddings into Generative Models via Probabilistic Circuits. (arXiv:2305.15944v1 [cs.LG])

    [http://arxiv.org/abs/2305.15944](http://arxiv.org/abs/2305.15944)

    本文提出了将知识图嵌入模型转化为生成模型的方法，允许精确MLE学习、有效抽样新的三元组以及保证逻辑约束，获得了比原始KGEs更具伸缩性的性能。

    

    一些成功的知识图嵌入模型可用作基于能量的模型，而这篇论文解释了这些模型的得分函数，将其重新解释成为电路形式--这是一种允许有效边际化的约束计算图。然后，我们设计了两个方法来获得有效的生成电路模型，其中一个方法是将其激活限制为非负数，另一个方法是将其输出平方。我们的解释不会影响到预测节点连边模型的性能，但电路框架使得MLE的精确学习、新三元组的有效抽样以及保证逻辑约束得以满足成为可能。此外，我们的模型在拥有数百万个实体的图上比原始的KGEs更具伸缩性。

    Some of the most successful knowledge graph embedding (KGE) models for link prediction -- CP, RESCAL, TuckER, ComplEx -- can be interpreted as energy-based models. Under this perspective they are not amenable for exact maximum-likelihood estimation (MLE), sampling and struggle to integrate logical constraints. This work re-interprets the score functions of these KGEs as circuits -- constrained computational graphs allowing efficient marginalisation. Then, we design two recipes to obtain efficient generative circuit models by either restricting their activations to be non-negative or squaring their outputs. Our interpretation comes with little or no loss of performance for link prediction, while the circuits framework unlocks exact learning by MLE, efficient sampling of new triples, and guarantee that logical constraints are satisfied by design. Furthermore, our models scale more gracefully than the original KGEs on graphs with millions of entities.
    
[^6]: 从少量根因的数据中学习DAGs

    Learning DAGs from Data with Few Root Causes. (arXiv:2305.15936v1 [cs.LG])

    [http://arxiv.org/abs/2305.15936](http://arxiv.org/abs/2305.15936)

    该论文提出了一种新的算法，能够从仅有少量根因的数据中学习DAGs，并证明了其可识别性，并在性能上优于以前的方法。

    

    我们提出了一种新的角度和算法，用于从线性结构方程模型(SEM)生成的数据中学习有向无环图(DAGs)。我们首先展示线性SEM可以被视为一种线性变换，先前的工作使用一个由与节点关联的随机值根因(我们将其称为)的稠密输入向量计算数据。我们考虑仅存在几个根因(近似)的情况，并在数据的测量中引入噪声。从直觉上讲，这意味着DAG数据是由少数数据生成事件产生的，其效果通过DAG传播。我们在这种新设置中证明可识别性，并表明真正的DAG是根因向量L0-范数的全局最小化者。对于具有少量根因的数据，有和没有噪音的情况下，我们表现出比以前的DAG学习方法更优异的性能。

    We present a novel perspective and algorithm for learning directed acyclic graphs (DAGs) from data generated by a linear structural equation model (SEM). First, we show that a linear SEM can be viewed as a linear transform that, in prior work, computes the data from a dense input vector of random valued root causes (as we will call them) associated with the nodes. Instead, we consider the case of (approximately) few root causes and also introduce noise in the measurement of the data. Intuitively, this means that the DAG data is produced by few data-generating events whose effect percolates through the DAG. We prove identifiability in this new setting and show that the true DAG is the global minimizer of the $L^0$-norm of the vector of root causes. For data with few root causes, with and without noise, we show superior performance compared to prior DAG learning methods.
    
[^7]: 径向索引机的诊断算法

    A Diagnosis Algorithms for a Rotary Indexing Machine. (arXiv:2305.15934v1 [cs.AI])

    [http://arxiv.org/abs/2305.15934](http://arxiv.org/abs/2305.15934)

    本文提出了一种基于产品角度的诊断算法，该算法通过跟踪产品在径向索引机中的步骤来诊断故障。文章分析了RIM的特性以及这些特性如何影响这些机器中的故障诊断。

    

    径向索引机（RIM）因能够在无需手动重新定位的情况下执行多个生产步骤，从而缩短生产时间、提高精度和一致性，广泛用于制造业。尽管具有诸多优势，但在RIM中诊断故障的研究较少，尤其是从这些机器进行的实际生产步骤角度考虑。故障造成的长时间停机对于雇用这些机器的较小公司尤其是有问题的。为了弥补这一空白，我们基于产品角度提出了一种诊断算法，重点关注由 RIM 执行的产品。该算法跟踪产品在机器中经过的步骤，并能够诊断故障的可能原因。我们还分析了RIM的特性以及这些特性如何影响这些机器中的故障诊断。我们的贡献有三个方面。首先，我们提供了RIM特性的分析，

    Rotary Indexing Machines (RIMs) are widely used in manufacturing due to their ability to perform multiple production steps on a single product without manual repositioning, reducing production time and improving accuracy and consistency. Despite their advantages, little research has been done on diagnosing faults in RIMs, especially from the perspective of the actual production steps carried out on these machines. Long downtimes due to failures are problematic, especially for smaller companies employing these machines. To address this gap, we propose a diagnosis algorithm based on the product perspective, which focuses on the product being processed by RIMs. The algorithm traces the steps that a product takes through the machine and is able to diagnose possible causes in case of failure. We also analyze the properties of RIMs and how these influence the diagnosis of faults in these machines. Our contributions are three-fold. Firstly, we provide an analysis of the properties of RIMs and
    
[^8]: 面向组织中对人工智能理解和采用的能力评估模型

    Towards a Capability Assessment Model for the Comprehension and Adoption of AI in Organisations. (arXiv:2305.15922v1 [cs.CY])

    [http://arxiv.org/abs/2305.15922](http://arxiv.org/abs/2305.15922)

    本文介绍了一个五级AI能力评估模型（AI-CAM）和相关的AI能力矩阵（AI-CM），用于帮助组织的决策者了解基于机器学习技术的AI数据分析用例、使用语义技术的知识工程和代表数据的知识表示以及旨在模仿人类推理和决策的AI解决方案的核心能力要求。

    

    人工智能（AI）的理解和采用面临实际和伦理问题。本文介绍了一个五级AI能力评估模型（AI-CAM）和相关的AI能力矩阵（AI-CM），以帮助从业人员理解和采用AI。这些实用工具是为商业高管、技术人员和其他组织利益相关者设计的。它们基于对AI的全面概念，相对于其他AI采用模型，也是开放源代码。因此，AI-CAM和AI-CM提供了一个可以获取的资源，以帮助组织的决策者了解基于机器学习技术的基于AI的数据分析用例、使用语义技术来工程化和代表数据、信息和知识的知识表示以及旨在模仿人类推理和决策的AI解决方案的核心能力要求。

    The comprehension and adoption of Artificial Intelligence (AI) are beset with practical and ethical problems. This article presents a 5-level AI Capability Assessment Model (AI-CAM) and a related AI Capabilities Matrix (AI-CM) to assist practitioners in AI comprehension and adoption. These practical tools were developed with business executives, technologists, and other organisational stakeholders in mind. They are founded on a comprehensive conception of AI compared to those in other AI adoption models and are also open-source artefacts. Thus, the AI-CAM and AI-CM present an accessible resource to help inform organisational decision-makers on the capability requirements for (1) AI-based data analytics use cases based on machine learning technologies; (2) Knowledge representation to engineer and represent data, information and knowledge using semantic technologies; and (3) AI-based solutions that seek to emulate human reasoning and decision-making. The AI-CAM covers the core capability
    
[^9]: 学习基于假设的论证框架

    Learning Assumption-based Argumentation Frameworks. (arXiv:2305.15921v1 [cs.AI])

    [http://arxiv.org/abs/2305.15921](http://arxiv.org/abs/2305.15921)

    提出了一种基于假设的论证框架(Assumption-based Argumentation, ABA)的学习方法，与现有的方法相比，该方法能够更好地学习一般规则的例外情况，具有更好的预测精度。

    

    我们提出了一种新颖的基于逻辑的学习方法，从正负例中，使用给定的背景知识生成假设为基础的论证框架(Assumption-based Argumentation, ABA)，这些ABA框架可以映射到具有失败否定的逻辑程序，这些程序也可以是非流纹理的。现有的基于论证的方法通过将例外事项解释为反驳攻击来学习一般规则的例外情况，而我们的方法将其解释为扰动攻击。我们的学习技术基于使用转换规则，包括一些改编自逻辑程序转换规则（特别是折叠）以及其他规则，例如逐字学习和假设引入。我们提出了一个通用策略，按照适当的顺序应用转换规则来学习流纹理框架，我们也提出了一种处理非流纹理情况的变体。我们通过许多例子说明我们方法的优点，这些例子表明，我们既能够学习与提供的正负例相对应的ABA框架，又能够比现有方法产生的框架具有更好的预测精度。

    We propose a novel approach to logic-based learning which generates assumption-based argumentation (ABA) frameworks from positive and negative examples, using a given background knowledge. These ABA frameworks can be mapped onto logic programs with negation as failure that may be non-stratified. Whereas existing argumentation-based methods learn exceptions to general rules by interpreting the exceptions as rebuttal attacks, our approach interprets them as undercutting attacks. Our learning technique is based on the use of transformation rules, including some adapted from logic program transformation rules (notably folding) as well as others, such as rote learning and assumption introduction. We present a general strategy that applies the transformation rules in a suitable order to learn stratified frameworks, and we also propose a variant that handles the non-stratified case. We illustrate the benefits of our approach with a number of examples, which show that, on one hand, we are able
    
[^10]: 一种检查部分有序时间一致性的快速算法

    A Fast Algorithm for Consistency Checking Partially Ordered Time. (arXiv:2305.15917v1 [cs.CC])

    [http://arxiv.org/abs/2305.15917](http://arxiv.org/abs/2305.15917)

    本文提出了一种快速算法来检查部分有序时间的一致性，相比已有算法时间复杂度更低，为$O^*((0.26n)^n)$，是通过对类似于全序的结构进行精细枚举，并贪婪地扩展向解决方案实现的。

    

    时间的部分有序模型在应用中自然而然地出现，这些应用中代理或进程不能完美地彼此通信，可以追溯到Lamport的开创性工作。本文研究了判断是否一个（可能不完整的）事件系统的描述是一致的问题，即部分有序时间（POT）点代数的网络一致性问题。虽然这个问题的经典复杂性已经完全解决，但是关于POT的细粒度复杂性的了解相对较少，除了可以通过枚举有序分区以$O^*((0.368n)^n)$时间解决。我们构造了一种更快的算法，其运行时间仅为$O^*((0.26n)^n)$。这是通过对类似于全序的结构进行精细枚举，然后向解决方案贪婪地扩展来实现的。虽然早期的类似想法已经在相关问题中探索过，但实际上POT的分析是非平凡的，需要重大的新成果。

    Partially ordered models of time occur naturally in applications where agents or processes cannot perfectly communicate with each other, and can be traced back to the seminal work of Lamport. In this paper we consider the problem of deciding if a (likely incomplete) description of a system of events is consistent, the network consistency problem for the point algebra of partially ordered time (POT). While the classical complexity of this problem has been fully settled, comparably little is known of the fine-grained complexity of POT except that it can be solved in $O^*((0.368n)^n)$ time by enumerating ordered partitions. We construct a much faster algorithm with a run-time bounded by $O^*((0.26n)^n)$. This is achieved by a sophisticated enumeration of structures similar to total orders, which are then greedily expanded toward a solution. While similar ideas have been explored earlier for related problems it turns out that the analysis for POT is non-trivial and requires significant new
    
[^11]: MTCue：利用神经机器翻译中未结构化上下文学习零样本控制额外文本属性

    MTCue: Learning Zero-Shot Control of Extra-Textual Attributes by Leveraging Unstructured Context in Neural Machine Translation. (arXiv:2305.15904v1 [cs.CL])

    [http://arxiv.org/abs/2305.15904](http://arxiv.org/abs/2305.15904)

    本研究提出了一种新颖的神经机器翻译框架MTCue，它将所有上下文解释为文本，实现了可转移性并学会了以零样本的方式利用额外的文本属性（如礼貌和对话行为等变量）的控制。在四个语言对的翻译方向上，MTCue的翻译质量显着提高，BLEU（+0.88）和Comet（+1.58）。

    

    高效地利用文本内和文本外的上下文仍是机器和人类翻译之间的关键差距之一。现有的研究主要集中在在翻译中提供个别定义良好类型的上下文，如周围的文本或离散的外部变量（如说话者的性别）。本文介绍了MTCue，这是一个新颖的神经机器翻译（NMT）框架，它将所有上下文（包括离散变量）解释为文本。 MTCue学习上下文的抽象表达，即使在不同的数据设置和低资源场景中，也能实现可转移性并利用类似属性。我们不断评估MTCue在四个语言对的翻译方向上，重点关注具有文档和元数据上下文访问权限的对话领域。与参数匹配的非上下文基线相比，我们的框架在翻译质量方面表现出显着的提高，BLEU（+0.88）和Comet（+1.58）。此外，MTCue成功地学会了以零样本的方式利用额外的文本属性，实现了诸如礼貌和对话行为等变量的控制。

    Efficient utilisation of both intra- and extra-textual context remains one of the critical gaps between machine and human translation. Existing research has primarily focused on providing individual, well-defined types of context in translation, such as the surrounding text or discrete external variables like the speaker's gender. This work introduces MTCue, a novel neural machine translation (NMT) framework that interprets all context (including discrete variables) as text. MTCue learns an abstract representation of context, enabling transferability across different data settings and leveraging similar attributes in low-resource scenarios. With a focus on a dialogue domain with access to document and metadata context, we extensively evaluate MTCue in four language pairs in both translation directions. Our framework demonstrates significant improvements in translation quality over a parameter-matched non-contextual baseline, as measured by BLEU (+0.88) and Comet (+1.58). Moreover, MTCu
    
[^12]: 指数平滑用于离线策略学习

    Exponential Smoothing for Off-Policy Learning. (arXiv:2305.15877v1 [cs.LG])

    [http://arxiv.org/abs/2305.15877](http://arxiv.org/abs/2305.15877)

    本文研究了离线学习中最小化风险的倒数倾向评分(IPS)的平滑正则化，推导出了可处理、可扩展、可解释的学习证明，并确定了在何种情况下不需要正则化IPS。

    

    离线策略学习旨在通过最小化风险的倒数倾向评分（IPS）来寻找改进的策略，通常使用记录的赌博数据。在本文中，我们研究了IPS的平滑正则化，推导出了一个双向PAC-Bayes泛化界限。该界限是可处理的、可扩展的、可解释的并提供了学习证明。我们通过一系列学习任务展示了我们方法的相关性和有利的性能。由于我们的界限适用于标准IPS，因此我们能够提供关于何时正则化IPS有用的见解。即，我们确定了不需要正则化的情况。这与在实践中，剪辑IPS常常比OPL中的标准IPS表现更好的信念相反。

    Off-policy learning (OPL) aims at finding improved policies from logged bandit data, often by minimizing the inverse propensity scoring (IPS) estimator of the risk. In this work, we investigate a smooth regularization for IPS, for which we derive a two-sided PAC-Bayes generalization bound. The bound is tractable, scalable, interpretable and provides learning certificates. In particular, it is also valid for standard IPS without making the assumption that the importance weights are bounded. We demonstrate the relevance of our approach and its favorable performance through a set of learning tasks. Since our bound holds for standard IPS, we are able to provide insight into when regularizing IPS is useful. Namely, we identify cases where regularization might not be needed. This goes against the belief that, in practice, clipped IPS often enjoys favorable performance than standard IPS in OPL.
    
[^13]: 真实回答的语言特性研究

    Linguistic Properties of Truthful Response. (arXiv:2305.15875v1 [cs.CL])

    [http://arxiv.org/abs/2305.15875](http://arxiv.org/abs/2305.15875)

    该论文研究了LLM的不真实回答现象，发现GPT-3模型对给定提示的回答在语言特性上很相似。同时，该论文证明了在没有评估内容本身的情况下，仅依赖模型回答的风格成分即可分类真实性。

    

    我们使用220个手工制作的语言特性对LLM不真实回答的现象进行了研究。我们专注于GPT-3模型，并发现不同大小的LLM对给定提示的回答在语言特性上很相似。我们通过训练只依赖模型回答的风格成分来分类陈述真实性的支持向量机扩展了这一发现。虽然数据集大小限制了我们的当前研究成果，但我们提供了有希望的证据，表明可以在不评估内容本身的情况下检测真实性。

    We investigate the phenomenon of an LLM's untruthful response using a large set of 220 handcrafted linguistic features. We focus on GPT-3 models and find that the linguistic profiles of responses are similar across model sizes. That is, how varying-sized LLMs respond to given prompts stays similar on the linguistic properties level. We expand upon this finding by training support vector machines that rely only upon the stylistic components of model responses to classify the truthfulness of statements. Though the dataset size limits our current findings, we present promising evidence that truthfulness detection is possible without evaluating the content itself.
    
[^14]: Jointprop：基于异构图传播的实体和关系联合半监督学习

    Jointprop: Joint Semi-supervised Learning for Entity and Relation Extraction with Heterogeneous Graph-based Propagation. (arXiv:2305.15872v1 [cs.CL])

    [http://arxiv.org/abs/2305.15872](http://arxiv.org/abs/2305.15872)

    提出了Jointprop框架，用于基于异构图传播的联合半监督实体和关系提取，采用一个统一的异构图来使用未标记数据中的全局结构信息，提高实体和关系提取性能。

    

    半监督学习是从有限数据中提取实体和关系的重要方法。然而，当前的半监督方法分别处理命名实体识别和关系抽取两个任务，并忽略实体和关系实例之间的交叉相关性以及未标记数据中相似实例的存在。为了解决这些问题，我们提出了Jointprop，一种基于异构图传播的联合半监督实体和关系提取框架，其捕获各个任务之间的全局结构信息，并利用未标记数据中的交互。具体地，我们从实体和关系候选构建了一个统一的基于标记的异构图，并根据置信度得分传播类标签。然后，我们采用传播学习方案来利用标记和未标记样本之间的关联。在基准数据集上的实验表明，我们的框架表现出令人满意的实体和关系提取性能。

    Semi-supervised learning has been an important approach to address challenges in extracting entities and relations from limited data. However, current semi-supervised works handle the two tasks (i.e., Named Entity Recognition and Relation Extraction) separately and ignore the cross-correlation of entity and relation instances as well as the existence of similar instances across unlabeled data. To alleviate the issues, we propose Jointprop, a Heterogeneous Graph-based Propagation framework for joint semi-supervised entity and relation extraction, which captures the global structure information between individual tasks and exploits interactions within unlabeled data. Specifically, we construct a unified span-based heterogeneous graph from entity and relation candidates and propagate class labels based on confidence scores. We then employ a propagation learning scheme to leverage the affinities between labelled and unlabeled samples. Experiments on benchmark datasets show that our framewo
    
[^15]: 技术领域术语和短语的文本表征提取

    Extracting Text Representations for Terms and Phrases in Technical Domains. (arXiv:2305.15867v1 [cs.CL])

    [http://arxiv.org/abs/2305.15867](http://arxiv.org/abs/2305.15867)

    本文提出了一种无监督的文本编码方法，使用小型基于字符的模型重构大型预训练嵌入矩阵，其可以在技术领域内达到与句子编码器相同的质量，但大小为后者的五分之一，计算时间能快10倍。

    

    获取术语和短语的密集表示是面向高度技术领域的知识发现平台的重要任务。常用的方法包括使用自监督设置训练领域特定的嵌入或使用训练过相似性任务的句子编码器模型。本文提出了一种完全无监督的文本编码方法，其中包括使用小型基于字符的模型来重构大型预训练嵌入矩阵。与静态嵌入相比，句子编码器不会受到词汇外问题的影响，但会带来显著的计算成本。

    Extracting dense representations for terms and phrases is a task of great importance for knowledge discovery platforms targeting highly-technical fields. Dense representations are used as features for downstream components and have multiple applications ranging from ranking results in search to summarization. Common approaches to create dense representations include training domain-specific embeddings with self-supervised setups or using sentence encoder models trained over similarity tasks. In contrast to static embeddings, sentence encoders do not suffer from the out-of-vocabulary (OOV) problem, but impose significant computational costs. In this paper, we propose a fully unsupervised approach to text encoding that consists of training small character-based models with the objective of reconstructing large pre-trained embedding matrices. Models trained with this approach can not only match the quality of sentence encoders in technical domains, but are 5 times smaller and up to 10 tim
    
[^16]: 顺序Integrated Gradients：一种解释语言模型的简单而有效的方法。

    Sequential Integrated Gradients: a simple but effective method for explaining language models. (arXiv:2305.15853v1 [cs.CL])

    [http://arxiv.org/abs/2305.15853](http://arxiv.org/abs/2305.15853)

    本文提出了一种名为顺序 Integrated Gradients（SIG）的解释语言模型的新方法，通过保持其他单词不变，仅在基线和感兴趣的单词之间创建插值来计算句子中每个单词的重要性，并用训练的令牌“mask”替换基线令牌“pad”来显着改善解释效果。

    

    几种解释方法（例如Integrated Gradients（IG））可以被描述为基于路径的方法，因为它们依赖于数据和无信息基线之间的直线。然而，当应用于语言模型时，这些方法同时为每个句子单词量产生路径，这可能会导致从插值词生成的句子没有明确的含义，或者与原始句子相比有显着不同的含义。为了使这些句子的含义尽可能接近原始句子，我们提出了顺序Integrated Gradients（SIG），它通过保持其他单词不变，仅在基线和感兴趣的单词之间创建插值来计算句子中每个单词的重要性。此外，受到几个语言模型的训练过程的启发，我们还建议用训练的令牌“mask”替换基线令牌“pad”。虽然这只是对原始IG方法的简单改进，但效果显著。

    Several explanation methods such as Integrated Gradients (IG) can be characterised as path-based methods, as they rely on a straight line between the data and an uninformative baseline. However, when applied to language models, these methods produce a path for each word of a sentence simultaneously, which could lead to creating sentences from interpolated words either having no clear meaning, or having a significantly different meaning compared to the original sentence. In order to keep the meaning of these sentences as close as possible to the original one, we propose Sequential Integrated Gradients (SIG), which computes the importance of each word in a sentence by keeping fixed every other words, only creating interpolations between the baseline and the word of interest. Moreover, inspired by the training procedure of several language models, we also propose to replace the baseline token "pad" with the trained token "mask". While being a simple improvement over the original IG method
    
[^17]: 大型语言模型的自相矛盾幻觉：评估、检测和缓解

    Self-contradictory Hallucinations of Large Language Models: Evaluation, Detection and Mitigation. (arXiv:2305.15852v1 [cs.CL])

    [http://arxiv.org/abs/2305.15852](http://arxiv.org/abs/2305.15852)

    本文对大型语言模型的自相矛盾幻觉进行了评估、检测和缓解，探究了这一幻觉形式的普遍存在性。通过设计框架有效触发自相矛盾，发现不同语言模型中这种现象都频繁出现。ChatGPT和GPT-4能够准确识别自相矛盾，而Vicuna-13B则有些困难。

    

    大型语言模型容易产生幻想的文本。自相矛盾是一种重要的幻觉形式，指的是语言模型在同一语境中生成两个矛盾的句子。本文针对最先进、经过指导的语言模型，对自相矛盾进行了全面的分析、评估、检测和缓解。我们设计了一个框架来有效地触发自相矛盾，评估结果表明，无论是对于著名的还是不太出名的话题，不同的语言模型中自相矛盾都经常发生。

    Large language models (large LMs) are susceptible to producing text with hallucinated content. Self-contradiction, where the LM generates two contradictory sentences within the same context, is an important form of hallucination. In this work, we present a comprehensive analysis on self-contradiction for state-of-the-art, instruction-tuned LMs, including evaluation, detection, and mitigation. To effectively trigger self-contradictions, we design a framework that constrains LMs to generate appropriate sentence pairs. Our evaluation on these sentence pairs reveals that self-contradictions occur frequently across different LMs for both famous and lesser-known topics. Next, we prompt the LMs to detect self-contradictions. Our results indicate that ChatGPT and GPT-4 are able to accurately identify self-contradictions, while Vicuna-13B struggles to do so. For example, with our best prompting method, ChatGPT achieves 91.0% precision and 80.5% recall on the sentence pairs generated by itself. 
    
[^18]: 用于雷达目标检测的点云多尺度网格渲染的改进

    Improved Multi-Scale Grid Rendering of Point Clouds for Radar Object Detection Networks. (arXiv:2305.15836v1 [cs.CV])

    [http://arxiv.org/abs/2305.15836](http://arxiv.org/abs/2305.15836)

    本文提出了一种新的体系结构，即多尺度 KPPillarsBEV，以缓解雷达目标检测中从点云数据转化为网格结构过程中的信息丢失问题，并提出了一种新的网格渲染方法 KPBEV。实验结果表明，该方法显著优于现有方法。

    

    将点云转换为网格表征，然后应用卷积神经网络的结构可用于雷达目标检测，但从不规则的点云数据转换为密集的网格结构常常会导致信息的丢失，这是由于点的离散化和聚合造成的。本文提出了一种新的体系结构，即多尺度 KPPillarsBEV，以缓解网格渲染的负面影响。具体而言，我们提出了一种新的网格渲染方法 KPBEV，它利用核点卷积的描述能力来改进网格渲染过程中局部点云上下文的编码。此外，我们还提出了一种通用的多尺度网格渲染公式，以任意网格渲染方法将多尺度特征映射融合到检测网络的卷积骨干中。我们在 nuScenes 数据集上进行了大量实验，并评估了检测汽车、卡车和公交车方法的性能。结果表明，我们提出的多尺度 KPPillarsBEV 结构和 KPBEV 网格渲染方法在性能上优于现有的方法。

    Architectures that first convert point clouds to a grid representation and then apply convolutional neural networks achieve good performance for radar-based object detection. However, the transfer from irregular point cloud data to a dense grid structure is often associated with a loss of information, due to the discretization and aggregation of points. In this paper, we propose a novel architecture, multi-scale KPPillarsBEV, that aims to mitigate the negative effects of grid rendering. Specifically, we propose a novel grid rendering method, KPBEV, which leverages the descriptive power of kernel point convolutions to improve the encoding of local point cloud contexts during grid rendering. In addition, we propose a general multi-scale grid rendering formulation to incorporate multi-scale feature maps into convolutional backbones of detection networks with arbitrary grid rendering methods. We perform extensive experiments on the nuScenes dataset and evaluate the methods in terms of dete
    
[^19]: PDE+：通过自适应分布扩散的偏微分方程增强泛化能力

    PDE+: Enhancing Generalization via PDE with Adaptive Distributional Diffusion. (arXiv:2305.15835v1 [cs.LG])

    [http://arxiv.org/abs/2305.15835](http://arxiv.org/abs/2305.15835)

    本文提出了PDE+，一种通过自适应分布扩散的偏微分方程来增强神经网络泛化的方法，实验结果表明，在几个基准数据集上，不仅能取得最先进或有竞争性的性能，还有更少的训练轮次和网络参数。

    

    神经网络的泛化是机器学习中的一个核心挑战，特别是在面对与训练数据不同的分布时的性能。目前主要采用基于数据驱动范式的方式，如数据增强，对抗性训练和噪声注入等，这些方法由于模型的非平滑性可能会导致泛化能力受限。因此，本文提出了从偏微分方程（PDE）的角度来研究泛化问题，旨在直接通过神经网络的基础函数来增强它，而不是聚焦于调整输入数据。具体而言，我们首先建立了神经网络泛化与特定PDE解的平滑度之间的联系，即“输运方程”。这样建立后，我们提出了一个总体框架，将自适应分布扩散引入输运方程中，以增强其解的平滑度，从而改善泛化能力。在神经网络训练的上下文中，我们的方法可以作为插件模块使用，与任何现有的训练算法无缝集成。我们在几个基准数据集上进行了实验，实验结果表明，我们提出的方法具有更少的网络参数和训练轮次，并取得了最先进或具有竞争性的性能。

    The generalization of neural networks is a central challenge in machine learning, especially concerning the performance under distributions that differ from training ones. Current methods, mainly based on the data-driven paradigm such as data augmentation, adversarial training, and noise injection, may encounter limited generalization due to model non-smoothness. In this paper, we propose to investigate generalization from a Partial Differential Equation (PDE) perspective, aiming to enhance it directly through the underlying function of neural networks, rather than focusing on adjusting input data. Specifically, we first establish the connection between neural network generalization and the smoothness of the solution to a specific PDE, namely ``transport equation''. Building upon this, we propose a general framework that introduces adaptive distributional diffusion into transport equation to enhance the smoothness of its solution, thereby improving generalization. In the context of neu
    
[^20]: 异构图神经网络梯度正则化统一方法

    Unifying gradient regularization for Heterogeneous Graph Neural Networks. (arXiv:2305.15811v1 [cs.LG])

    [http://arxiv.org/abs/2305.15811](http://arxiv.org/abs/2305.15811)

    本研究提出了一种新的梯度正则化方法Grug，旨在统一HGNN中的图形拓扑和节点特征的正则化，并解决了过度平滑、非鲁棒性等问题，综合效果和效率优于几种现有方法。

    

    异构图神经网络是一种强大的深度学习方法，用于学习异构图的表征。尽管HGNN迅速发展，但仍面临过度平滑和非鲁棒性等挑战。先前的研究表明，使用梯度正则化方法可以缓解这些问题，但现有的梯度正则化方法专注于图形拓扑或节点特征，缺乏统一方法。本文提出了一种新的梯度正则化方法Grug，旨在统一HGNN中的图形拓扑和节点特征的正则化，并解决了过度平滑、非鲁棒性等问题。实验证明，Grug在几个基准数据集上优于几种现有方法。

    Heterogeneous Graph Neural Networks (HGNNs) are a class of powerful deep learning methods widely used to learn representations of heterogeneous graphs. Despite the fast development of HGNNs, they still face some challenges such as over-smoothing, and non-robustness. Previous studies have shown that these problems can be reduced by using gradient regularization methods. However, the existing gradient regularization methods focus on either graph topology or node features. There is no universal approach to integrate these features, which severely affects the efficiency of regularization. In addition, the inclusion of gradient regularization into HGNNs sometimes leads to some problems, such as an unstable training process, increased complexity and insufficient coverage regularized information. Furthermore, there is still short of a complete theoretical analysis of the effects of gradient regularization on HGNNs. In this paper, we propose a novel gradient regularization method called Grug, 
    
[^21]: ChatGPT用于PLC/DCS控制逻辑生成

    ChatGPT for PLC/DCS Control Logic Generation. (arXiv:2305.15809v1 [cs.SE])

    [http://arxiv.org/abs/2305.15809](http://arxiv.org/abs/2305.15809)

    本文探索使用大语言模型生成PLC/DCS控制逻辑，并创造了用于测试和比较这些模型控制能力的基准测试集合。

    

    大语言模型(LLMs)提供生成型AI已经流行起来，以支持软件工程师创建、汇总、优化和记录源代码。但是LLMs如何支持控制工程师使用典型的控制编程语言进行编程任务仍然未知。研究人员已经探讨了GitHub CoPilot或DeepMind AlphaCode进行源代码生成，但尚未攻克控制逻辑编程。本文的贡献是一项探索性研究，我们创建了100个LLM提示在10个代表性类别中分析PLCs和DCS的控制逻辑生成的自然语言。我们使用ChatGPT和GPT-4 LLM生成答案来测试这些提示。它在许多情况下生成符合语法规范的IEC 61131-3结构化文本代码，并展示了有用的推理技巧，这可能会提高控制工程师的生产力。我们的提示集是一个更正式的LLM基准测试的基础，以测试和比较此类模型的控制能力。

    Large language models (LLMs) providing generative AI have become popular to support software engineers in creating, summarizing, optimizing, and documenting source code. It is still unknown how LLMs can support control engineers using typical control programming languages in programming tasks. Researchers have explored GitHub CoPilot or DeepMind AlphaCode for source code generation but did not yet tackle control logic programming. The contribution of this paper is an exploratory study, for which we created 100 LLM prompts in 10 representative categories to analyze control logic generation for of PLCs and DCS from natural language. We tested the prompts by generating answers with ChatGPT using the GPT-4 LLM. It generated syntactically correct IEC 61131-3 Structured Text code in many cases and demonstrated useful reasoning skills that could boost control engineer productivity. Our prompt collection is the basis for a more formal LLM benchmark to test and compare such models for control l
    
[^22]: Lucy-SKG：使用深度强化学习高效学习玩“火箭联盟”游戏

    Lucy-SKG: Learning to Play Rocket League Efficiently Using Deep Reinforcement Learning. (arXiv:2305.15801v1 [cs.LG])

    [http://arxiv.org/abs/2305.15801](http://arxiv.org/abs/2305.15801)

    本文介绍了一种基于强化学习模型的 Lucy-SKG 学习玩“火箭联盟”游戏的方法，在性能上超过了该游戏中排名最高的两个机器人，并成为了最先进的代理程序。

    

    在AI领域取得进展的一种成功策略是将游戏视为问题，这已被证明会导致各种重大突破。本文中，我们采用这种策略来研究“火箭联盟”游戏，这是一款广受欢迎但相对未被深入探索的3D多人在线游戏，具有独特的物理引擎和复杂的动力学，对于开发高效和高性能的游戏代理程序来说是一个巨大的挑战。我们提出了一种基于强化学习模型的 Lucy-SKG，以高效的方式学习如何玩火箭联盟游戏，在性能上明显优于本游戏中排名最高的两个机器人 Necto（2022年机器人冠军）和其后继 Nexto，从而成为最先进的代理程序。我们的贡献包括：a）开发了奖励分析和可视化库；b）通过我们提出的肌肉反馈组合，捕捉复杂奖励类型的效用的新型参数化奖励形状函数。

    A successful tactic that is followed by the scientific community for advancing AI is to treat games as problems, which has been proven to lead to various breakthroughs. We adapt this strategy in order to study Rocket League, a widely popular but rather under-explored 3D multiplayer video game with a distinct physics engine and complex dynamics that pose a significant challenge in developing efficient and high-performance game-playing agents. In this paper, we present Lucy-SKG, a Reinforcement Learning-based model that learned how to play Rocket League in a sample-efficient manner, outperforming by a notable margin the two highest-ranking bots in this game, namely Necto (2022 bot champion) and its successor Nexto, thus becoming a state-of-the-art agent. Our contributions include: a) the development of a reward analysis and visualization library, b) novel parameterizable reward shape functions that capture the utility of complex reward types via our proposed Kinesthetic Reward Combinatio
    
[^23]: 特征空间降维方法对于超高维度多类数据的研究：基于随机森林的多轮筛选(RFMS)

    Feature space reduction method for ultrahigh-dimensional, multiclass data: Random forest-based multiround screening (RFMS). (arXiv:2305.15793v1 [cs.LG])

    [http://arxiv.org/abs/2305.15793](http://arxiv.org/abs/2305.15793)

    本研究提出了一种新的算法RFMS，该算法可以有效的处理数千个类别的超高维度数据，通过锦标赛排序和特征重要性选择实现特征选择，具有与行业标准的特征筛选方法相当的性能和更多的优点。

    

    近年来，已有大量针对拥有数十万特征的超高维度数据的筛选方法被发布，然而，大多数方法无法处理数千个类别的数据。基于多通道生物特征数据验证用户的预测模型也面临着这个问题。本研究提出了一种新的方法，即基于随机森林的多轮筛选(RFMS)，可以在这种情况下有效地应用。该算法将特征空间划分为小的子集，并执行一系列部分模型构建。这些部分模型用于实现基于锦标赛排序和基于特征重要性的特征选择。使用合成生物特征空间生成器BiometricBlender来对RFMS进行基准测试。根据结果，RFMS与行业标准的特征筛选方法相当，并且同时具有许多优点。

    In recent years, numerous screening methods have been published for ultrahigh-dimensional data that contain hundreds of thousands of features; however, most of these features cannot handle data with thousands of classes. Prediction models built to authenticate users based on multichannel biometric data result in this type of problem. In this study, we present a novel method known as random forest-based multiround screening (RFMS) that can be effectively applied under such circumstances. The proposed algorithm divides the feature space into small subsets and executes a series of partial model builds. These partial models are used to implement tournament-based sorting and the selection of features based on their importance. To benchmark RFMS, a synthetic biometric feature space generator known as BiometricBlender is employed. Based on the results, the RFMS is on par with industry-standard feature screening methods while simultaneously possessing many advantages over these methods.
    
[^24]: 在ChatGPT时代的服务组合

    Service Composition in the ChatGPT Era. (arXiv:2305.15788v1 [cs.SE])

    [http://arxiv.org/abs/2305.15788](http://arxiv.org/abs/2305.15788)

    ChatGPT-like系统能够支持自动服务组合领域，并提出了新的研究领域。

    

    本文探讨了ChatGPT等系统如何支持自动服务组合领域，并确定了新的研究领域，以便充分利用这样的工具在面向服务的组合领域中。

    The paper speculates about how ChatGPT-like systems can support the field of automated service composition and identifies new research areas to explore in order to take advantage of such tools in the field of service-oriented composition.
    
[^25]: 关于大型语言模型的规划能力——一项关键调查

    On the Planning Abilities of Large Language Models -- A Critical Investigation. (arXiv:2305.15771v1 [cs.AI])

    [http://arxiv.org/abs/2305.15771](http://arxiv.org/abs/2305.15771)

    本文研究了大型语言模型在规划能力上的表现，发现LLMs自主生成可执行计划的能力有限，但在启发式模式下表现更有前途。

    

    本文受到语言模型在常识规划任务中具有新兴推理能力的声称的启发，旨在研究其规划能力。我们的目标是评估LLMs在自主生成常识规划任务中的计划效果和LLMs作为其他智能体（AI规划者）在其规划任务中启发性指导的潜力。我们通过生成一套与国际计划竞赛中使用的相似领域的实例进行系统研究，并在两种不同的模式下评估LLMs：自主和启发式。我们的研究结果显示，LLMs自主生成可执行计划的能力相当有限，最佳模型（GPT-4）在领域中的平均成功率约为12%。然而，启发式模式下的结果显示了更有前途的迹象。在启发式模式下，我们证明LLM生成的计划可以改善基础规划器的搜索过程和结果。

    Intrigued by the claims of emergent reasoning capabilities in LLMs trained on general web corpora, in this paper, we set out to investigate their planning capabilities. We aim to evaluate (1) the effectiveness of LLMs in generating plans autonomously in commonsense planning tasks and (2) the potential of LLMs as a source of heuristic guidance for other agents (AI planners) in their planning tasks. We conduct a systematic study by generating a suite of instances on domains similar to the ones employed in the International Planning Competition and evaluate LLMs in two distinct modes: autonomous and heuristic. Our findings reveal that LLMs' ability to generate executable plans autonomously is rather limited, with the best model (GPT-4) having an average success rate of ~12% across the domains. However, the results in the heuristic mode show more promise. In the heuristic mode, we demonstrate that LLM-generated plans can improve the search process for underlying sound planners and addition
    
[^26]: TLNets: 基于变换学习的长期时间序列预测网络

    TLNets: Transformation Learning Networks for long-range time-series prediction. (arXiv:2305.15770v1 [cs.LG])

    [http://arxiv.org/abs/2305.15770](http://arxiv.org/abs/2305.15770)

    提出了一种基于变换学习的网络架构设计方案，实现了在学习中增强感受野和融合不同尺度特征的优点，使用了傅里叶变换、奇异值分解、矩阵乘法和卷积块等四种不同的变换机制进行构建，模型表现优异，超过了几种最先进的解决方案。

    

    时间序列预测是许多学科领域里的一个广泛问题，如气象学、交通监测、投资、能源生产和消费等。许多统计和机器学习策略已经被开发来解决这个问题，但是这些方法要么缺乏可解释性，要么当预测时间范围增加时表现不佳。为此，我们提出了一种基于变换的网络架构设计方案，具有潜力在学习中实现增强的感受野，并在不同尺度上融合特征的好处。在这种情况下，我们引入了四种不同的变换机制作为构建学习模型的基础，包括傅里叶变换（FT）、奇异值分解（SVD）、矩阵乘法和卷积块。因此，我们基于以上构建块开发了四个学习模型，分别为FT-Matrix、FT-SVD、FT-Conv和Conv-SVD。需要注意的是，FT和SVD模型具有提取低维表示以增强学习性能的能力。我们在各种合成和实际长时间序列数据集上进行了经验评估，结果显示相对于几种最先进的解决方案，我们的提出的模型具有更优异的长期预测准确性。

    Time series prediction is a prevalent issue across various disciplines, such as meteorology, traffic surveillance, investment, and energy production and consumption. Many statistical and machine-learning strategies have been developed to tackle this problem. However, these approaches either lack explainability or exhibit less satisfactory performance when the prediction horizon increases. To this end, we propose a novel plan for the designing of networks' architecture based on transformations, possessing the potential to achieve an enhanced receptive field in learning which brings benefits to fuse features across scales. In this context, we introduce four different transformation mechanisms as bases to construct the learning model including Fourier Transform (FT), Singular Value Decomposition (SVD), matrix multiplication and Conv block. Hence, we develop four learning models based on the above building blocks, namely, FT-Matrix, FT-SVD, FT-Conv, and Conv-SVD. Note that the FT and SVD b
    
[^27]: MERGE: 快速的私有文本生成

    MERGE: Fast Private Text Generation. (arXiv:2305.15769v1 [cs.CL])

    [http://arxiv.org/abs/2305.15769](http://arxiv.org/abs/2305.15769)

    该论文提出了MERGE，一个基于Transformer语言模型的快速私有文本生成框架。实验结果表明，MERGE在保护隐私的同时，实现了26.5倍的加速和80%的通信字节数减少。

    

    近年来，人们越来越关注NLP服务和Transformer模型的私有推理。然而，现有的两方隐私保护方法仅考虑NLU场景，而文本生成的私有推理，如翻译、对话和代码补全，仍未解决。此外，将现有的隐私保护方法迁移到NLG模型时，性能表现差，而在训练阶段受到收敛问题的困扰。为了解决这些问题，我们提出了MERGE，这是一个基于Transformer语言模型的快速私有文本生成框架。具体而言，MERGE重用输出隐藏状态作为单词嵌入，以跳过嵌入计算，并重新组织Transformer模块中的线性操作以加速向前过程。基于这两个优化，大量的实验表明，在序列长度为512时，MERGE可实现26.5倍的加速，并减少80\%的通信字节数。

    Recent years have seen increasing concerns about the private inference of NLP services and Transformer models. However, existing two-party privacy-preserving methods solely consider NLU scenarios, while the private inference of text generation such as translation, dialogue, and code completion remains unsolved. Besides, while migrated to NLG models, existing privacy-preserving methods perform poorly in terms of inference speed, and suffer from the convergence problem during the training stage. To address these issues, we propose MERGE, a fast private text generation framework for Transformer-based language models. Specifically, MERGE reuse the output hidden state as the word embedding to bypass the embedding computation, and reorganize the linear operations in the Transformer module to accelerate the forward procedure. Based on these two optimizations, extensive experiments show that MERGE can achieve a 26.5x speedup under the sequence length 512, and reduce 80\% communication bytes, w
    
[^28]: 自动驾驶中基于语言指导的点云三维物体检测

    Language-Guided 3D Object Detection in Point Cloud for Autonomous Driving. (arXiv:2305.15765v1 [cs.CV])

    [http://arxiv.org/abs/2305.15765](http://arxiv.org/abs/2305.15765)

    本文提出了一个基于语言指导的点云三维物体检测方法，包含了一种有效的令牌融合策略，并实现了与自然语言语义融合的激光雷达物体探测。我们的方法在跨模态实验中达到了最先进的性能，并且具有高效的优势。

    

    本文研究了在自动驾驶场景下接受自然语言指令的点云中的三维物体检测问题。以往的方法通常聚焦于二维或者三维室内场景，难以准确地预测在自动驾驶场景中所查询的三维区域。本文提出了一个新的多模态视觉定位任务，称为激光雷达视觉定位。并且，我们提出了一种多模态单端定位方法，该方法包含了一种有效的令牌融合策略，实现了与自然语言语义融合的激光雷达物体探测。我们的方法可以灵活地对图像特征进行集成，以提供更丰富的纹理和颜色信息。跨模态的实验结果证明了我们的方法具有高效的性能。

    This paper addresses the problem of 3D referring expression comprehension (REC) in autonomous driving scenario, which aims to ground a natural language to the targeted region in LiDAR point clouds. Previous approaches for REC usually focus on the 2D or 3D-indoor domain, which is not suitable for accurately predicting the location of the queried 3D region in an autonomous driving scene. In addition, the upper-bound limitation and the heavy computation cost motivate us to explore a better solution. In this work, we propose a new multi-modal visual grounding task, termed LiDAR Grounding. Then we devise a Multi-modal Single Shot Grounding (MSSG) approach with an effective token fusion strategy. It jointly learns the LiDAR-based object detector with the language features and predicts the targeted region directly from the detector without any post-processing. Moreover, the image feature can be flexibly integrated into our approach to provide rich texture and color information. The cross-moda
    
[^29]: 利用弱监督信号治愈不安全的对话回复

    Healing Unsafe Dialogue Responses with Weak Supervision Signals. (arXiv:2305.15757v1 [cs.CL])

    [http://arxiv.org/abs/2305.15757](http://arxiv.org/abs/2305.15757)

    本论文提出了一种名为TEMP的无监督伪标签采样方法，可以自动分配潜在的安全响应，解决大规模对话系统的不安全响应生成问题。

    

    近年来，人们对大规模对话系统的不安全响应生成越来越担忧，代理会从现实世界语料库中学习攻击性或有偏见的行为。提出了一些方法通过检测和替换不安全的训练样本来解决上述问题，虽然有效，但它们的注释成本高，并且在未见过的场景以及对抗性攻击方面适应性差。此外，忽略提供安全的响应（例如简单地替换模板）将导致对话信息缺失的问题。为了解决这些问题，我们提出了一种无监督的伪标签采样方法TEMP，可以自动分配潜在的安全响应。具体而言，我们的TEMP方法将响应分成几个簇，并使用一种自适应锐化采样策略进行多标签采样，灵感来自于不安全样本通常少且分布在尾部的观察。

    Recent years have seen increasing concerns about the unsafe response generation of large-scale dialogue systems, where agents will learn offensive or biased behaviors from the real-world corpus. Some methods are proposed to address the above issue by detecting and replacing unsafe training examples in a pipeline style. Though effective, they suffer from a high annotation cost and adapt poorly to unseen scenarios as well as adversarial attacks. Besides, the neglect of providing safe responses (e.g. simply replacing with templates) will cause the information-missing problem of dialogues. To address these issues, we propose an unsupervised pseudo-label sampling method, TEMP, that can automatically assign potential safe responses. Specifically, our TEMP method groups responses into several clusters and samples multiple labels with an adaptively sharpened sampling strategy, inspired by the observation that unsafe samples in the clusters are usually few and distribute in the tail. Extensive 
    
[^30]: TransWorldNG: 通过基础模型实现交通仿真

    TransWorldNG: Traffic Simulation via Foundation Model. (arXiv:2305.15743v1 [cs.AI])

    [http://arxiv.org/abs/2305.15743](http://arxiv.org/abs/2305.15743)

    TransWorldNG是一种基于数据驱动算法和图形计算技术的交通仿真器，能够自动学习交通模式并生成更真实的交通模式，具有更好的可扩展性。

    

    交通仿真是交通决策和政策开发的关键工具。然而，面对交通环境的高维度和异质性，实现逼真的仿真一直是一个长期的挑战。本文介绍了使用数据驱动算法和图形计算技术从真实数据中学习交通动态的交通仿真器TransWordNG。介绍了TransWorldNG的功能和结构，它利用基础模型进行交通管理和控制。结果表明，与传统仿真器相比，TransWorldNG能够生成更真实的交通模式。此外，TransWorldNG表现出更好的可扩展性，随着场景规模的增大，计算时间呈线性增长。据我们所知，这是第一个能够自动从实际数据中学习交通模式并高效生成准确逼真的交通仿真器。

    Traffic simulation is a crucial tool for transportation decision-making and policy development. However, achieving realistic simulations in the face of the high dimensionality and heterogeneity of traffic environments is a longstanding challenge. In this paper, we present TransWordNG, a traffic simulator that uses Data-driven algorithms and Graph Computing techniques to learn traffic dynamics from real data. The functionality and structure of TransWorldNG are introduced, which utilize a foundation model for transportation management and control. The results demonstrate that TransWorldNG can generate more realistic traffic patterns compared to traditional simulators. Additionally, TransWorldNG exhibits better scalability, as it shows linear growth in computation time as the scenario scale increases. To the best of our knowledge, this is the first traffic simulator that can automatically learn traffic patterns from real-world data and efficiently generate accurate and realistic traffic e
    
[^31]: MPE4G: 多模态预训练编码器用于同时语音手势生成

    MPE4G: Multimodal Pretrained Encoder for Co-Speech Gesture Generation. (arXiv:2305.15740v1 [cs.CV])

    [http://arxiv.org/abs/2305.15740](http://arxiv.org/abs/2305.15740)

    本文提出了一种名为MPE4G的多模态预训练编码器框架，用于共同语音手势生成。该方法通过自我监督学习来训练基于多头注意力的编码器以包含每种模态的信息，并且可以生成逼真的共同语音手势，即使存在缺失或嘈杂的输入模态。

    

    当虚拟代理人与人类进行交互时，手势对于通过语音传递其意图至关重要。先前的多模态同时语音手势生成模型需要对所有模态的编码特征进行编码，才能生成手势。如果某些输入模态被移除或包含噪音，则模型可能无法正确生成手势。为了获得稳健和广义的编码，我们提出了一种新的框架，使用多模态预先训练的编码器进行共同语音手势生成。在所提出的方法中，基于多头注意力的编码器使用自我监督学习进行训练，以包含每种模态的信息。此外，我们收集由3D关节旋转组成的全身手势以改善可视化，并将手势应用于可扩展身体模型。通过一系列的实验和人工评估，所提出的方法在给定所有输入模态以及缺少或包含嘈杂模态时，都可以生成逼真的共同语音手势。

    When virtual agents interact with humans, gestures are crucial to delivering their intentions with speech. Previous multimodal co-speech gesture generation models required encoded features of all modalities to generate gestures. If some input modalities are removed or contain noise, the model may not generate the gestures properly. To acquire robust and generalized encodings, we propose a novel framework with a multimodal pre-trained encoder for co-speech gesture generation. In the proposed method, the multi-head-attention-based encoder is trained with self-supervised learning to contain the information on each modality. Moreover, we collect full-body gestures that consist of 3D joint rotations to improve visualization and apply gestures to the extensible body model. Through the series of experiments and human evaluation, the proposed method renders realistic co-speech gestures not only when all input modalities are given but also when the input modalities are missing or noisy.
    
[^32]: 三项DMC的分析与调优

    Analysis and tuning of a three-term DMC. (arXiv:2305.15735v1 [eess.SY])

    [http://arxiv.org/abs/2305.15735](http://arxiv.org/abs/2305.15735)

    本文研究了一种使用三项QP的DMC算法，可以比传统的两项DMC实现更高的性能和鲁棒性。

    

    许多在工业中使用和在控制学术中研究的MPC（模型预测控制）算法使用两项QP（二次规划），其中第一项是输出误差加权范数，第二项是输入增量加权范数。本文研究了一种使用三项QP的DMC（动态矩阵控制）算法，其中第三项是输出增量的加权范数。在分析中，建立了三项DMC和两项DMC之间的关系，并基于此导出了闭环响应曲线。根据分析，为三项DMC开发了两种控制器调节程序，一种用于闭环阶跃响应，一种用于扰动降低。最后，证明三项DMC可以比两项DMC实现更高的性能和鲁棒性。模拟研究用于展示研究结果和调优方法。

    Most MPC (Model Predictive Control) algorithms used in industries and studied in the control academia use a two-term QP (quadratic programming), where the first term is the weighted norm of the output errors, and the second term is that of the input increments. In this work, a DMC (Dynamic Matrix Control) algorithm that uses three-term QP is studied, where the third term is the weighted norm of the output increments. In the analysis, a relationship between the three-term DMC and the two-term DMC is established; based on that, the closed-loop response curves are derived. Based on the analysis, two controller tuning procedures are developed for the three-term DMC, one for closed-loop step response and one for disturbance reduction. Finally, it will be proven that the three-term DMC can achieve a higher performance and robustness than the two-term DMC can. Simulation studies are used to demonstrate the findings and the tuning methods.
    
[^33]: 关于知识蒸馏在模型可解释性上的影响

    On the Impact of Knowledge Distillation for Model Interpretability. (arXiv:2305.15734v1 [cs.LG])

    [http://arxiv.org/abs/2305.15734](http://arxiv.org/abs/2305.15734)

    本文研究表明，知识蒸馏可以提高模型的可解释性和准确性，并归因于从老师模型传递给学生模型的类相似信息的存在。

    

    近期的研究表明，知识蒸馏(KD)为何可以提高模型性能已被阐明，但是很少有人研究除提高模型性能外KD的其他优点。本研究旨在证明KD不仅可以提高模型准确性，还可以提高模型的可解释性。我们通过网络分解算法中检测的概念检测器数量对模型的可解释性进行了定量比较。我们将可解释性的提高归因于从老师模型传递给学生模型的类相似信息。我们首先通过逻辑回归蒸馏确认了类相似信息的传递。然后，我们分析了类相似信息在模型可解释性方面的作用，包括其存在或缺失以及相似信息的程度。我们进行了各种定量和定性实验，并在不同数据集、不同的KD方法和不同的模型结构上检查了结果。

    Several recent studies have elucidated why knowledge distillation (KD) improves model performance. However, few have researched the other advantages of KD in addition to its improving model performance. In this study, we have attempted to show that KD enhances the interpretability as well as the accuracy of models. We measured the number of concept detectors identified in network dissection for a quantitative comparison of model interpretability. We attributed the improvement in interpretability to the class-similarity information transferred from the teacher to student models. First, we confirmed the transfer of class-similarity information from the teacher to student model via logit distillation. Then, we analyzed how class-similarity information affects model interpretability in terms of its presence or absence and degree of similarity information. We conducted various quantitative and qualitative experiments and examined the results on different datasets, different KD methods, and 
    
[^34]: 高效神经音乐生成

    Efficient Neural Music Generation. (arXiv:2305.15719v1 [cs.SD])

    [http://arxiv.org/abs/2305.15719](http://arxiv.org/abs/2305.15719)

    提出了一种名为MeLoDy的高效神经音乐生成模型，它可以在实时、单CPU环境下生成各种风格和长度的多样化、高质量的音乐，并显著减少了计算成本。

    

    近年来，基于先进的MusicLM，音乐生成取得了显著进展。MusicLM由三个LM的层次结构组成，分别用于语义、粗略声学和细节声学建模。然而，使用MusicLM进行采样需要逐个处理这些LM以获得细粒度的声学标记，这使得计算成本高，不适用于实时生成。在本文中，我们提出了MeLoDy（M代表音乐；L代表LM；D代表扩散），这是一种LM引导的扩散模型，可以生成音乐音频，同时在采样10秒或30秒音乐时，分别减少了MusicLM中的95.7%或99.6%的前向传递。MeLoDy继承了MusicLM的语义建模的最高级别的LM，并应用了新颖的双路径扩散(DPD)模型和音频VAE-GAN来高效地将条件语义标记解码为波形。DPD是一种不对称的扩散过程，沿着确定性路径传播高级别语义信息，沿着随机路径传播低级声学细节。经验上，MeLoDy可以在单个CPU上实时生成各种风格和长度的多样化、高质量的音乐。

    Recent progress in music generation has been remarkably advanced by the state-of-the-art MusicLM, which comprises a hierarchy of three LMs, respectively, for semantic, coarse acoustic, and fine acoustic modelings. Yet, sampling with the MusicLM requires processing through these LMs one by one to obtain the fine-grained acoustic tokens, making it computationally expensive and prohibitive for a real-time generation. Efficient music generation with a quality on par with MusicLM remains a significant challenge. In this paper, we present MeLoDy (M for music; L for LM; D for diffusion), an LM-guided diffusion model that generates music audios of state-of-the-art quality meanwhile reducing 95.7% or 99.6% forward passes in MusicLM, respectively, for sampling 10s or 30s music. MeLoDy inherits the highest-level LM from MusicLM for semantic modeling, and applies a novel dual-path diffusion (DPD) model and an audio VAE-GAN to efficiently decode the conditioning semantic tokens into waveform. DPD i
    
[^35]: 知识蒸馏中的知识扩散

    Knowledge Diffusion for Distillation. (arXiv:2305.15712v1 [cs.CV])

    [http://arxiv.org/abs/2305.15712](http://arxiv.org/abs/2305.15712)

    本文提出了一种新型的知识蒸馏方法DiffKD，通过扩散模型去除学生特征中的噪声信息，提取清晰的有价值信息，实现更好的蒸馏效果。

    

    在知识蒸馏中，教师和学生之间的表征差距是一个新兴的话题。为了减少这种差距并提高表现，当前的方法通常采用复杂的训练方案、损失函数和特征对齐，这些都是任务特定和特征特定的。本文提出了一种名为DiffKD的新型KD方法，通过扩散模型明确去噪和匹配特征，来摆脱噪声信息，提取有价值的信息，从而达到更好的蒸馏效果。我们的方法基于这样一个观察：学生的特征通常比教师的特征更多噪声，因为学生模型的容量更小。因此我们提出了使用教师特征训练的扩散模型对学生特征进行去噪的方法。这使我们能够在细化的清洁特征和教师特征之间进行更好的蒸馏。此外，我们引入了一种轻量级的线性扩散模型。

    The representation gap between teacher and student is an emerging topic in knowledge distillation (KD). To reduce the gap and improve the performance, current methods often resort to complicated training schemes, loss functions, and feature alignments, which are task-specific and feature-specific. In this paper, we state that the essence of these methods is to discard the noisy information and distill the valuable information in the feature, and propose a novel KD method dubbed DiffKD, to explicitly denoise and match features using diffusion models. Our approach is based on the observation that student features typically contain more noises than teacher features due to the smaller capacity of student model. To address this, we propose to denoise student features using a diffusion model trained by teacher features. This allows us to perform better distillation between the refined clean feature and teacher feature. Additionally, we introduce a light-weight diffusion model with a linear a
    
[^36]: 分布式强化学习的好处：小损失边界

    The Benefits of Being Distributional: Small-Loss Bounds for Reinforcement Learning. (arXiv:2305.15703v1 [cs.LG])

    [http://arxiv.org/abs/2305.15703](http://arxiv.org/abs/2305.15703)

    通过小损失边界的视角，我们提供了分布式RL好处的一个解释，该边界与实例相关的最优成本成比例。如果最优成本很小，分布式方法优于非分布式方法。

    

    虽然分布式强化学习已经取得了实证成果，但其何时何地有益的问题尚未得到回答。在这项工作中，通过小损失边界的视角，我们提供了分布式RL好处的一个解释，该边界与实例相关的最优成本成比例。如果最优成本很小，我们的边界会比非分布式方法更强。作为热身，我们展示了学习成本分布会在情境展开（CB）中导致小损失后悔边界，我们发现分布式CB在三个具有挑战性的任务上比最先进的技术在实证上表现更好。对于在线RL，我们提出了一个分布式版本空间算法，该算法使用最大似然估计构建置信区间，并证明了它在表格MDP中实现了小损失后悔，同时在潜变量模型中享有小损失PAC边界。以类似的见解为基础，我们提出了一个分布式离线RL算法

    While distributional reinforcement learning (RL) has demonstrated empirical success, the question of when and why it is beneficial has remained unanswered. In this work, we provide one explanation for the benefits of distributional RL through the lens of small-loss bounds, which scale with the instance-dependent optimal cost. If the optimal cost is small, our bounds are stronger than those from non-distributional approaches. As warmup, we show that learning the cost distribution leads to small-loss regret bounds in contextual bandits (CB), and we find that distributional CB empirically outperforms the state-of-the-art on three challenging tasks. For online RL, we propose a distributional version-space algorithm that constructs confidence sets using maximum likelihood estimation, and we prove that it achieves small-loss regret in the tabular MDPs and enjoys small-loss PAC bounds in latent variable models. Building on similar insights, we propose a distributional offline RL algorithm bas
    
[^37]: 重新思考深度学习测试中的多样性

    Rethink Diversity in Deep Learning Testing. (arXiv:2305.15698v1 [cs.SE])

    [http://arxiv.org/abs/2305.15698](http://arxiv.org/abs/2305.15698)

    本文提出针对深度学习测试的六个具体指标来检测和缓解其漏洞，认为许多DNN测试任务应该是定向测试问题而不是通用测试任务。

    

    深度神经网络（DNNs）展现了非凡的能力，是现代软件系统的重要组成部分。然而，它们也面临着各种漏洞，如对抗攻击和不公平性。因此，测试深度学习（DL）系统是一项重要任务，以检测和减轻这些漏洞。受传统软件测试成功的启发，提出了各种DNN的多样性度量，以帮助有效地暴露DNNs的buggy行为。在本文中，我们认为许多DNN测试任务应该被视为定向测试问题而不是通用测试任务，因为这些任务是具体和明确定义的。因此，基于多样性的方法效果较差。遵循我们的测试目标和DNN语义的论证，我们推导出6个指标，可以用于DNN测试，并仔细分析它们的应用范围。我们通过经验性结果显示它们的应用性。

    Deep neural networks (DNNs) have demonstrated extraordinary capabilities and are an integral part of modern software systems. However, they also suffer from various vulnerabilities such as adversarial attacks and unfairness. Testing deep learning (DL) systems is therefore an important task, to detect and mitigate those vulnerabilities. Motivated by the success of traditional software testing, which often employs diversity heuristics, various diversity measures on DNNs have been proposed to help efficiently expose the buggy behavior of DNNs. In this work, we argue that many DNN testing tasks should be treated as directed testing problems rather than general-purpose testing tasks, because these tasks are specific and well-defined. Hence, the diversity-based approach is less effective.  Following our argument based on the semantics of DNNs and the testing goal, we derive $6$ metrics that can be used for DNN testing and carefully analyze their application scopes. We empirically show their 
    
[^38]: 隐私保护：一种信息理论方法

    Privacy Protectability: An Information-theoretical Approach. (arXiv:2305.15697v1 [cs.CR])

    [http://arxiv.org/abs/2305.15697](http://arxiv.org/abs/2305.15697)

    本文提出了一种新的度量标准——隐私可保护度，用于衡量在给定某种视频分析任务的情况下，视频流可以得到多大程度的保护，并使用信息理论工具对其进行量化。

    

    最近，推理隐私引起越来越多的关注。推理隐私问题最明显地出现在广泛部署的边缘-云视频分析系统中，其中云需要从边缘捕获视频。视频数据可能包含敏感信息，并且在传输到云进行推理时会遭受攻击。已经提出了许多隐私保护方案。然而，方案的性能需要通过实验确定或通过分析特定案例进行推断。在本文中，我们提出了一种新的度量标准，称为“隐私可保护度”，用于表征在给定某种视频分析任务的情况下，视频流可以得到多大程度的保护。这种度量标准具有强的操作意义。例如，低可保护性意味着可能需要建立一个整体安全的环境。我们还可以评估隐私保护方案，例如，假设它混淆了视频数据，则在混淆数据后该方案实现了多大程度的保护。我们使用信息理论工具引入了定义并量化了隐私保护性。我们通过检查一些典型的视频分析任务来说明所提出的度量标准的实际效用。

    Recently, inference privacy has attracted increasing attention. The inference privacy concern arises most notably in the widely deployed edge-cloud video analytics systems, where the cloud needs the videos captured from the edge. The video data can contain sensitive information and subject to attack when they are transmitted to the cloud for inference. Many privacy protection schemes have been proposed. Yet, the performance of a scheme needs to be determined by experiments or inferred by analyzing the specific case. In this paper, we propose a new metric, \textit{privacy protectability}, to characterize to what degree a video stream can be protected given a certain video analytics task. Such a metric has strong operational meaning. For example, low protectability means that it may be necessary to set up an overall secure environment. We can also evaluate a privacy protection scheme, e.g., assume it obfuscates the video data, what level of protection this scheme has achieved after obfus
    
[^39]: 视频人体动作识别中的深度神经网络：一项综述

    Deep Neural Networks in Video Human Action Recognition: A Review. (arXiv:2305.15692v1 [cs.CV])

    [http://arxiv.org/abs/2305.15692](http://arxiv.org/abs/2305.15692)

    本文综述了视频行为识别中深度神经网络的应用。与传统机器学习框架相比，该方法优势显著，具有更高的准确性和更广泛的应用性。

    

    目前，视频行为识别是计算机视觉中最基础的任务之一。深度学习的二维神经网络用于识别像RGB、RGB-D或光流格式这样的像素级信息的图像，随着监控视频的日益普及和更多与人体动作识别相关的任务的出现，需要考虑帧间信息分析。比起像素级信息，研究人员更多地研究基于视频的识别，从几何任务中提取更多信息。我们提出了多个新颖的研究工作，对这些派生的深度学习框架进行了比较，而不是与机器学习框架进行比较。比较发生在现有框架和仅为视频格式的数据集之间，特别是由于人体动作具有特殊性质，其使用越来越广泛。

    Currently, video behavior recognition is one of the most foundational tasks of computer vision. The 2D neural networks of deep learning are built for recognizing pixel-level information such as images with RGB, RGB-D, or optical flow formats, with the current increasingly wide usage of surveillance video and more tasks related to human action recognition. There are increasing tasks requiring temporal information for frames dependency analysis. The researchers have widely studied video-based recognition rather than image-based(pixel-based) only to extract more informative elements from geometry tasks. Our current related research addresses multiple novel proposed research works and compares their advantages and disadvantages between the derived deep learning frameworks rather than machine learning frameworks. The comparison happened between existing frameworks and datasets, which are video format data only. Due to the specific properties of human actions and the increasingly wide usage 
    
[^40]: 克服提示扰动敏感性的零样本方法

    Zero-shot Approach to Overcome Perturbation Sensitivity of Prompts. (arXiv:2305.15689v1 [cs.CL])

    [http://arxiv.org/abs/2305.15689](http://arxiv.org/abs/2305.15689)

    本研究提出了一种零样本方法，自动生成多个类似于基础提示的高质量提示，并使用新的度量方法进行排名，从而克服了提示的扰动敏感性，并在情感分类任务中具有较高的准确性。

    

    最近的研究表明，自然语言提示可以帮助利用预训练语言模型学习的知识进行二元句级情感分类任务。具体来说，这些方法利用少量样本学习设置，使用手动或自动生成的提示来微调情感分类模型。然而，这些方法的性能对所使用提示的扰动敏感。此外，这些方法依赖于少量带标签实例进行自动提示生成和提示排序。本研究旨在在零样本设置中为所给定的任务找到高质量的提示。我们的提议方法给定一个基础提示，采用位置、推理和释义技术自动生成多个类似于基础提示的提示，然后使用一种新的度量方法对提示进行排名。我们从实验上证明，排名靠前的提示具有很高的质量，并在提示扰动鲁棒性和整体准确性方面显著优于基础提示和其他现有的提示生成方法。

    Recent studies have demonstrated that natural-language prompts can help to leverage the knowledge learned by pre-trained language models for the binary sentence-level sentiment classification task. Specifically, these methods utilize few-shot learning settings to fine-tune the sentiment classification model using manual or automatically generated prompts. However, the performance of these methods is sensitive to the perturbations of the utilized prompts. Furthermore, these methods depend on a few labeled instances for automatic prompt generation and prompt ranking. This study aims to find high-quality prompts for the given task in a zero-shot setting. Given a base prompt, our proposed approach automatically generates multiple prompts similar to the base prompt employing positional, reasoning, and paraphrasing techniques and then ranks the prompts using a novel metric. We empirically demonstrate that the top-ranked prompts are high-quality and significantly outperform the base prompt an
    
[^41]: RewriteLM：一种面向文本重写的指令调整大型语言模型。

    RewriteLM: An Instruction-Tuned Large Language Model for Text Rewriting. (arXiv:2305.15685v1 [cs.CL])

    [http://arxiv.org/abs/2305.15685](http://arxiv.org/abs/2305.15685)

    本文介绍了 RewriteLM，一种指令调整的大型语言模型，用于长篇文本重写。同时，我们提出了一个名为 OpenRewriteEval 的基准测试，用于评估各种类型的开放式长篇文本重写。我们采用新的策略来促进多样的指令和偏好数据生成，从而为长篇文本重写提供更好的评估手段。

    

    大型语言模型已经展现出在长篇文本生成任务中通过自然语言指令表达来的惊人的零-shot能力，然而用户对于长篇文本重写的期望值很高，模型产生的意外重写（“幻觉”）会对其整体性能产生负面影响。现有的评估基准主要关注有限的重写风格和句子级重写，而不是长篇开放式重写。我们引入了一个新的基准测试OpenRewriteEval，它涵盖了通过自然语言指令表达的各种重写类型。它特别设计用于促进长篇文本开放式重写的评估。此外，我们提出了一种强大的基线模型RewriteLM，一个用于长篇文本重写的指令调整大型语言模型。我们开发了一些新策略，以最小人工干预促进生成多样的指令和偏好数据。

    Large Language Models (LLMs) have demonstrated impressive zero-shot capabilities in long-form text generation tasks expressed through natural language instructions. However, user expectations for long-form text rewriting is high, and unintended rewrites (''hallucinations'') produced by the model can negatively impact its overall performance. Existing evaluation benchmarks primarily focus on limited rewriting styles and sentence-level rewriting rather than long-form open-ended rewriting.We introduce OpenRewriteEval, a novel benchmark that covers a wide variety of rewriting types expressed through natural language instructions. It is specifically designed to facilitate the evaluation of open-ended rewriting of long-form texts. In addition, we propose a strong baseline model, RewriteLM, an instruction-tuned large language model for long-form text rewriting. We develop new strategies that facilitate the generation of diverse instructions and preference data with minimal human intervention.
    
[^42]: 重新审视非英语文本简化：一个统一的多语言基准

    Revisiting non-English Text Simplification: A Unified Multilingual Benchmark. (arXiv:2305.15678v1 [cs.CL])

    [http://arxiv.org/abs/2305.15678](http://arxiv.org/abs/2305.15678)

    这篇论文介绍了MultiSim基准，它包含了27个资源、12种语言超过1.7百万个复杂-简单的句子对。使用该基准进行预训练的多语言语言模型可以在非英语环境中带来令人兴奋的性能提升，并且俄语在跨语言转移方面表现出强大的性能。

    

    最近英语自动文本简化（ATS）研究中高质量、大规模的英语资源的进展将英语ATS研究的前沿推向了更高的水平。然而，由于缺乏一个覆盖多种语言中的复杂-简洁句子对的多样化评估基准，对多语言文本简化的研究工作较少。本文介绍了MultiSim基准，这是一个收集了12种不同语言中超过1.7百万个复杂-简单句子对的27个资源的集合。这个基准将鼓励研究开发更有效的多语言文本简化模型和评估指标。我们使用MultiSim与预训练的多语言语言模型进行的实验显示，在非英语环境中进行多语言训练可以带来令人兴奋的性能提升。我们观察到俄语在零-shot跨语言转移到低资源语言的强大表现。我们进一步展示使用BLOOM-176b的少量提示可以达到可比的参考简化质量。

    Recent advancements in high-quality, large-scale English resources have pushed the frontier of English Automatic Text Simplification (ATS) research. However, less work has been done on multilingual text simplification due to the lack of a diverse evaluation benchmark that covers complex-simple sentence pairs in many languages. This paper introduces the MultiSim benchmark, a collection of 27 resources in 12 distinct languages containing over 1.7 million complex-simple sentence pairs. This benchmark will encourage research in developing more effective multilingual text simplification models and evaluation metrics. Our experiments using MultiSim with pre-trained multilingual language models reveal exciting performance improvements from multilingual training in non-English settings. We observe strong performance from Russian in zero-shot cross-lingual transfer to low-resource languages. We further show that few-shot prompting with BLOOM-176b achieves comparable quality to reference simplif
    
[^43]: PROTO: 迭代策略规范化离线到在线强化学习

    PROTO: Iterative Policy Regularized Offline-to-Online Reinforcement Learning. (arXiv:2305.15669v1 [cs.LG])

    [http://arxiv.org/abs/2305.15669](http://arxiv.org/abs/2305.15669)

    提出了一种名为PROTO的新型离线到在线强化学习框架，通过迭代演化的规范化项，克服了现有方法的性能次优、适应性有限和计算效率不足等问题。PROTO可以极大地适应各种方法，且仅需最小的额外计算，实现了高效和有效的离线到在线强化学习。

    

    离线到在线强化学习通过结合离线预训练和在线微调的优点，承诺提高样本效率和策略性能。然而，现有方法虽然有效，但存在性能次优、适应性有限和计算效率不足等问题。我们提出了一种新的框架PROTO，通过将标准强化学习目标与迭代演化的规范化项相结合，克服了上述限制。PROTO通过执行信任区域样式更新，在渐进放松约束强度的同时，使初始微调稳定、最终性能最优。通过调整只有几行代码，PROTO可以将任何离线策略预训练和标准离线强化学习微调桥接起来，从而产生对各种方法的极强适应性。PROTO简单而优雅，仅强加最小的额外计算，实现了高效和有效的离线到在线强化学习。

    Offline-to-online reinforcement learning (RL), by combining the benefits of offline pretraining and online finetuning, promises enhanced sample efficiency and policy performance. However, existing methods, effective as they are, suffer from suboptimal performance, limited adaptability, and unsatisfactory computational efficiency. We propose a novel framework, PROTO, which overcomes the aforementioned limitations by augmenting the standard RL objective with an iteratively evolving regularization term. Performing a trust-region-style update, PROTO yields stable initial finetuning and optimal final performance by gradually evolving the regularization term to relax the constraint strength. By adjusting only a few lines of code, PROTO can bridge any offline policy pretraining and standard off-policy RL finetuning to form a powerful offline-to-online RL pathway, birthing great adaptability to diverse methods. Simple yet elegant, PROTO imposes minimal additional computation and enables highly
    
[^44]: 如何逃离锐化的极小值点

    How to escape sharp minima. (arXiv:2305.15659v1 [cs.LG])

    [http://arxiv.org/abs/2305.15659](http://arxiv.org/abs/2305.15659)

    本文探讨了发现平坦极小值点的算法问题，在支持找到局部近似平坦的极小值点的基础上，设计了两种算法：一种基于梯度的算法，一种基于最小化锐度的算法。

    

    现代机器学习应用程序中的优化算法已经取得了显著的成功，这些算法被设计用来发现平坦的极小值点。为了解决这个算法问题，本文采用损失函数海森矩阵的迹来度量它的平坦程度，并形式化定义了近似平坦极小值点的概念。在此概念下，我们设计了有效地找到近似平坦极小值点的算法。针对一般的损失函数，我们提出了一种基于梯度的算法，可以有效地找到近似平坦的局部极小值点。算法的主要组件是使用从随机扰动迭代中计算的梯度来估计导致更平坦极小值点的方向。对于成本函数是训练数据上的经验风险的设置，我们提出了一种更快速的算法，受最近提出的实用算法——锐度感知最小化的启发。

    Modern machine learning applications have seen a remarkable success of optimization algorithms that are designed to find flat minima. Motivated by this paradigm, this work formulates and studies the algorithmic question of how to find flat minima. As an initial effort, this work adopts the trace of hessian of the cost function as the measure of flatness, and formally defines the notion of approximate flat minima. Under this notion, we then design algorithms that find approximate flat minima efficiently. For general cost functions, we present a gradient-based algorithm that finds an approximate flat local minimum efficiently. The main component of the algorithm is to use gradients computed from randomly perturbed iterates to estimate a direction that leads to flatter minima. For the setting where the cost function is an empirical risk over training data, we present a faster algorithm that is inspired by a recently proposed practical algorithm called sharpness-aware minimization, support
    
[^45]: 到达工业视觉中的总体在线无监督异常检测与定位

    Towards Total Online Unsupervised Anomaly Detection and Localization in Industrial Vision. (arXiv:2305.15652v1 [cs.CV])

    [http://arxiv.org/abs/2305.15652](http://arxiv.org/abs/2305.15652)

    本文提出一种名为LeMO的在线学习图像异常检测方法，本方法使用学习性记忆，包含了基于对比学习的异常检测损失函数，可同时优化内存和图像特征，能够适应工业在线流数据需求，具有高效性和简单性。

    

    虽然现有的图像异常检测方法取得了令人印象深刻的结果，但它们大多是需要过多预先收集数据的离线学习范式，在需要在线流数据的工业场景中限制其适应性。基于在线学习的图像异常检测方法更适用于工业在线流数据，但鲜为人知。本文首次提出了一种完全在线学习的图像异常检测方法，名为LeMO，即为在线图像异常检测学习记忆，其利用的是学习性记忆，经初始化为正交随机噪声，消除了内存初始化的过度数据需求和离线数据收集的低效性。此外，为了实现在线内存和图像目标导向特征的联合优化，还设计了基于对比学习的异常检测损失函数。所提出的方法简单而高效。大量实验证明了其有效性。

    Although existing image anomaly detection methods yield impressive results, they are mostly an offline learning paradigm that requires excessive data pre-collection, limiting their adaptability in industrial scenarios with online streaming data. Online learning-based image anomaly detection methods are more compatible with industrial online streaming data but are rarely noticed. For the first time, this paper presents a fully online learning image anomaly detection method, namely LeMO, learning memory for online image anomaly detection. LeMO leverages learnable memory initialized with orthogonal random noise, eliminating the need for excessive data in memory initialization and circumventing the inefficiencies of offline data collection. Moreover, a contrastive learning-based loss function for anomaly detection is designed to enable online joint optimization of memory and image target-oriented features. The presented method is simple and highly effective. Extensive experiments demonstra
    
[^46]: 元自适应任务采样实现少域通用化

    Meta Adaptive Task Sampling for Few-Domain Generalization. (arXiv:2305.15644v1 [cs.LG])

    [http://arxiv.org/abs/2305.15644](http://arxiv.org/abs/2305.15644)

    本文提出了少域通用化框架和元自适应任务采样（MATS）过程，旨在利用极少量的新任务域来学习可推广的模型，并在基础任务上获得知识以提高模型的域外泛化性能。

    

    为确保模型具有足够的域外泛化性能，传统的领域通用化方法常常需要在不同底层分布的多个源数据上进行训练。然而，这些方法的成功很大程度上取决于有多样化的训练分布。但由于高昂的费用、隐私问题或数据的稀缺性等原因，通常需要付出巨大的努力才能获得足够的异构数据。因此，在感知到的异质性受限时，如何提高模型的域外泛化性能是一个有趣但鲜为人知的问题。在本文中，我们提出了一个名为少域通用化的新框架，旨在从极少量的新任务域中学习可推广的模型，并利用先前在基础任务上获得的知识。此外，我们提出了一种元自适应任务采样（MATS）过程，根据它们的语义和领域转移相异性来区分基础任务。

    To ensure the out-of-distribution (OOD) generalization performance, traditional domain generalization (DG) methods resort to training on data from multiple sources with different underlying distributions. And the success of those DG methods largely depends on the fact that there are diverse training distributions. However, it usually needs great efforts to obtain enough heterogeneous data due to the high expenses, privacy issues or the scarcity of data. Thus an interesting yet seldom investigated problem arises: how to improve the OOD generalization performance when the perceived heterogeneity is limited. In this paper, we instantiate a new framework called few-domain generalization (FDG), which aims to learn a generalizable model from very few domains of novel tasks with the knowledge acquired from previous learning experiences on base tasks. Moreover, we propose a Meta Adaptive Task Sampling (MATS) procedure to differentiate base tasks according to their semantic and domain-shift sim
    
[^47]: 基于学习的软件代码和配置自动生成

    Learning-Based Automatic Synthesis of Software Code and Configuration. (arXiv:2305.15642v1 [cs.SE])

    [http://arxiv.org/abs/2305.15642](http://arxiv.org/abs/2305.15642)

    本文介绍了一种基于学习的自动软件代码和配置生成方法，其中通过遗传算法和神经网络训练的适应度函数自动合成程序，使用协方差矩阵适应进化策略完成程序合成，并采用强化学习方法实现配置生成。

    

    软件行业的需求增加和软件工程师的稀缺性推动研究人员和从业者自动化软件生成和配置的过程。大规模的自动软件代码生成和配置是一个非常复杂和具有挑战性的任务。本研究将自动软件生成和配置拆分为两个不同的任务进行研究。在第一个任务中，我们提出了使用输入输出规范自动合成软件的方法。这个任务被进一步分为两个子任务。第一个子任务是使用遗传算法合成程序，其驱动神经网络基于程序跟踪和规范训练的适应度函数。对于第二个子任务，我们将程序合成形式化为连续优化问题，并使用协方差矩阵适应进化策略（一种最先进的连续优化方法）来合成程序。最后，对于配置生成，我们采用基于强化学习的方法来实现。

    Increasing demands in software industry and scarcity of software engineers motivates researchers and practitioners to automate the process of software generation and configuration. Large scale automatic software generation and configuration is a very complex and challenging task. In this proposal, we set out to investigate this problem by breaking down automatic software generation and configuration into two different tasks. In first task, we propose to synthesize software automatically with input output specifications. This task is further broken down into two sub-tasks. The first sub-task is about synthesizing programs with a genetic algorithm which is driven by a neural network based fitness function trained with program traces and specifications. For the second sub-task, we formulate program synthesis as a continuous optimization problem and synthesize programs with covariance matrix adaption evolutionary strategy (a state-of-the-art continuous optimization method). Finally, for th
    
[^48]: 患者结果预测改善了大型医院网络的运营

    Patient Outcome Predictions Improve Operations at a Large Hospital Network. (arXiv:2305.15629v1 [cs.LG])

    [http://arxiv.org/abs/2305.15629](http://arxiv.org/abs/2305.15629)

    美国一家大型医院网络与学术界和顾问合作，开发了机器学习模型，可以准确预测患者的短期和长期结果，同时将预测与医生的预测相结合可以使更多的患者出院并减少了再入院。

    

    问题定义：获得准确的患者结果预测可以增强医务人员的决策能力，最终使医院的所有利益相关者受益。美国一家大型医院网络一直与学术界和顾问合作，预测其七家医院所有住院患者的短期和长期结果。方法/结果：我们开发了机器学习模型，预测下一个24小时/48小时出院和重症监护室转移，出院死亡率和出院安排的概率。所有模型都实现了高的外部样本AUC（75.7％-92.5％），并且很好地校准。此外，将48小时出院预测与医生同时预测相结合，可以使更多的患者出院（10％-28.7％），并减少了7天/ 30天的再入院（p值小于0.001）。我们实现了一个自动化管道，每天早上提取数据并更新预测，以及用户友好型软件和彩色警报系统。

    Problem definition: Access to accurate predictions of patients' outcomes can enhance medical staff's decision-making, which ultimately benefits all stakeholders in the hospitals. A large hospital network in the US has been collaborating with academics and consultants to predict short-term and long-term outcomes for all inpatients across their seven hospitals. Methodology/results: We develop machine learning models that predict the probabilities of next 24-hr/48-hr discharge and intensive care unit transfers, end-of-stay mortality and discharge dispositions. All models achieve high out-of-sample AUC (75.7%-92.5%) and are well calibrated. In addition, combining 48-hr discharge predictions with doctors' predictions simultaneously enables more patient discharges (10%-28.7%) and fewer 7-day/30-day readmissions ($p$-value $<0.001$). We implement an automated pipeline that extracts data and updates predictions every morning, as well as user-friendly software and a color-coded alert system to 
    
[^49]: 自监督学习的逆向工程

    Reverse Engineering Self-Supervised Learning. (arXiv:2305.15614v1 [cs.LG])

    [http://arxiv.org/abs/2305.15614](http://arxiv.org/abs/2305.15614)

    本文逆向工程了自监督学习（SSL）训练表示，发现SSL训练过程中的正则化项本质上促进了样本基于语义标签的聚类。SSL训练的表示与语义类别更加接近，对齐在训练过程中增加，而且在网络深度加深时增加。

    

    自监督学习（SSL）是机器学习中有力的工具，但理解学习表示及其基础机制仍然是一个挑战。本文对SSL训练表示进行了深入的实证分析，包括多种模型、架构和超参数。我们的研究揭示了SSL训练过程的一个有趣方面：它本质上促进了样本基于语义标签的聚类，这令人惊讶的是，这是由SSL目标的正则化项驱动的。这种聚类过程不仅增强了下游分类，而且压缩了数据信息。此外，我们发现SSL训练的表示与语义类别更加接近，而不是随机类别。值得注意的是，我们展示了学习表示与各种层次的语义类别对齐，并且这种对齐在训练过程中增加，而且在网络深度加深时增加。

    Self-supervised learning (SSL) is a powerful tool in machine learning, but understanding the learned representations and their underlying mechanisms remains a challenge. This paper presents an in-depth empirical analysis of SSL-trained representations, encompassing diverse models, architectures, and hyperparameters. Our study reveals an intriguing aspect of the SSL training process: it inherently facilitates the clustering of samples with respect to semantic labels, which is surprisingly driven by the SSL objective's regularization term. This clustering process not only enhances downstream classification but also compresses the data information. Furthermore, we establish that SSL-trained representations align more closely with semantic classes rather than random classes. Remarkably, we show that learned representations align with semantic classes across various hierarchical levels, and this alignment increases during training and when moving deeper into the network. Our findings provid
    
[^50]: 基于谱角度剖析生物数据中图神经网络的尺寸可泛化性：观点和实践

    Size Generalizability of Graph Neural Networks on Biological Data: Insights and Practices from the Spectral Perspective. (arXiv:2305.15611v1 [cs.LG])

    [http://arxiv.org/abs/2305.15611](http://arxiv.org/abs/2305.15611)

    本文通过谱角度的方法，研究了GNNs的尺寸可泛化性问题，并在真实生物数据集上进行了实验，发现GNNs在度分布和谱分布偏移时均表现敏感，在同一数据集的大图上的性能仍然下降，揭示了 GNNs的尺寸可泛化性问题。

    

    本文探讨了图神经网络 (GNNs) 是否具有从小图中学习的知识可推广到同一领域的大图中。之前的研究表明，不同大小的图之间的分布偏移，尤其是度分布，可能会导致图分类任务的性能下降。然而，在生物数据集中，度数是有界的，因此度分布的偏移很小。即使度分布偏移很小，我们观察到GNNs在同一数据集的大图上的性能仍然下降，暗示有其他原因。事实上，以往对于真实数据集中各种图尺寸引起的分布偏移类型和属性的探索不足。此外，以前的尺寸可泛化性分析大多集中在空间领域。为填补这些空白，我们采用谱角度去研究GNNs在生物图数据上的尺寸可泛化性。我们首先提出一个新框架来模拟各种类型的度分布偏移，并利用它来测试GNNs 在真实生物数据集上的尺寸可泛化性。我们的实验表明，除了度分布偏移外，GNNs 还对图大小变化引起的谱分布偏移很敏感。我们进一步分析了不同的GNN模型的影响，并表明，一些模型比其他模型更具有尺寸泛化性。本文展示了关于GNNs尺寸可泛化性问题的新观点和实践，并为该领域的未来研究提供了有益的洞察和建议。

    We investigate the question of whether the knowledge learned by graph neural networks (GNNs) from small graphs is generalizable to large graphs in the same domain. Prior works suggest that the distribution shift, particularly in the degree distribution, between graphs of different sizes can lead to performance degradation in the graph classification task. However, this may not be the case for biological datasets where the degrees are bounded and the distribution shift of degrees is small. Even with little degree distribution shift, our observations show that GNNs' performance on larger graphs from the same datasets still degrades, suggesting other causes. In fact, there has been a lack of exploration in real datasets to understand the types and properties of distribution shifts caused by various graph sizes. Furthermore, previous analyses of size generalizability mostly focus on the spatial domain.  To fill these gaps, we take the spectral perspective and study the size generalizabilit
    
[^51]: 控制不变集增强的安全强化学习：改进的采样效率、保证的稳定性和健壮性

    Control invariant set enhanced safe reinforcement learning: improved sampling efficiency, guaranteed stability and robustness. (arXiv:2305.15602v1 [eess.SY])

    [http://arxiv.org/abs/2305.15602](http://arxiv.org/abs/2305.15602)

    本文提出了一种控制不变集增强强化学习的方法，该方法包括两个学习阶段以提高稳定性保证和采样效率。作者研究了该方法的健壮性，并将CIS纳入到奖励设计、初始状态采样和状态重置程序中。

    

    强化学习是一个备受关注的研究领域，特别是安全强化学习因其能够处理真实应用中至关重要的安全约束而备受瞩目。本文提出了一种名为控制不变集增强强化学习（CIS）的新型学习方法，该方法利用使用CIS的显式形式来提高稳定性保证和采样效率。此外，本研究还在存在不确定性的情况下研究了所提方法的健壮性。该方法包括两个学习阶段：离线和在线。在离线阶段，CIS被纳入到奖励设计、初始状态采样和状态重置程序中。这种纳入CIS的方式有助于提高离线训练过程中的采样效率。在在线阶段，当预测的下一步状态在CIS之外时，即引入安全准则时，RL将会重新进行训练。

    Reinforcement learning (RL) is an area of significant research interest, and safe RL in particular is attracting attention due to its ability to handle safety-driven constraints that are crucial for real-world applications. This work proposes a novel approach to RL training, called control invariant set (CIS) enhanced RL, which leverages the advantages of utilizing the explicit form of CIS to improve stability guarantees and sampling efficiency. Furthermore, the robustness of the proposed approach is investigated in the presence of uncertainty. The approach consists of two learning stages: offline and online. In the offline stage, CIS is incorporated into the reward design, initial state sampling, and state reset procedures. This incorporation of CIS facilitates improved sampling efficiency during the offline training process. In the online stage, RL is retrained whenever the predicted next step state is outside of the CIS, which serves as a stability criterion, by introducing a Safety
    
[^52]: 基于预训练语言模型的文本增强开放知识图谱补全

    Text-Augmented Open Knowledge Graph Completion via Pre-Trained Language Models. (arXiv:2305.15597v1 [cs.CL])

    [http://arxiv.org/abs/2305.15597](http://arxiv.org/abs/2305.15597)

    TAGREAL是一种可自动生成高质量查询提示信息，从大型文本语料库中检索支持信息以从PLM中探测知识的方法，用于开放知识图谱补全中，在两个基准数据集上取得了最先进的表现，并且即使在有限的训练数据情况下，仍然具有突出的性能。

    

    开放知识图谱补全的任务是从已知事实中提取新的发现。现有的增强知识图谱补全的方法要么需要事实三元组以扩大图推理空间，要么需要手动设计提示信息以从预训练语言模型中提取知识，这些方法性能有限，需要专家昂贵的工作。为此，我们提出了TAGREAL，它自动生成高质量的查询提示信息，并从大型文本语料库中检索支持信息以从PLM中探测知识以完成知识图谱补全。结果表明，TAGREAL在两个基准数据集上实现了最先进的性能。我们发现，即使是在有限的训练数据情况下，TAGREAL的性能仍然非常突出，超过了现有的基于嵌入、基于图和基于PLM的方法。

    The mission of open knowledge graph (KG) completion is to draw new findings from known facts. Existing works that augment KG completion require either (1) factual triples to enlarge the graph reasoning space or (2) manually designed prompts to extract knowledge from a pre-trained language model (PLM), exhibiting limited performance and requiring expensive efforts from experts. To this end, we propose TAGREAL that automatically generates quality query prompts and retrieves support information from large text corpora to probe knowledge from PLM for KG completion. The results show that TAGREAL achieves state-of-the-art performance on two benchmark datasets. We find that TAGREAL has superb performance even with limited training data, outperforming existing embedding-based, graph-based, and PLM-based methods.
    
[^53]: 未建图环境下多车辆路径规划的分布式在线策略

    Distributed Online Rollout for Multivehicle Routing in Unmapped Environments. (arXiv:2305.15596v1 [cs.DC])

    [http://arxiv.org/abs/2305.15596](http://arxiv.org/abs/2305.15596)

    本文提出了一个在线策略，通过分布式的协调策略解决了未建图环境下多车辆路径规划问题。

    

    本文考虑了一个广泛化的多车辆路径规划问题：给定一个网络、一组占据网络节点子集的智能体和一组任务, 我们寻求一个最小成本的移动序列，以满足每个任务至少被一个智能体访问的约束条件。与经典问题不同的是，我们假定没有中央服务器，每个智能体都是一个个体处理器，没有关于基础网络（包括任务和智能体位置）的先验知识。此外，我们的智能体具有严格的本地通信和感知能力（限制在它们各自位置周围的半径范围内），更接近多实际应用。我们提出了一种在线策略方法，其中智能体根据集中式模拟器训练的学习型策略局部规划其轨迹。我们在模拟环境中展示了我们的方法，并表明它在网络拓扑和任务分布的变化下仍然能够实现近最优性能。

    In this work we consider a generalization of the well-known multivehicle routing problem: given a network, a set of agents occupying a subset of its nodes, and a set of tasks, we seek a minimum cost sequence of movements subject to the constraint that each task is visited by some agent at least once. The classical version of this problem assumes a central computational server that observes the entire state of the system perfectly and directs individual agents according to a centralized control scheme. In contrast, we assume that there is no centralized server and that each agent is an individual processor with no a priori knowledge of the underlying network (including task and agent locations). Moreover, our agents possess strictly local communication and sensing capabilities (restricted to a fixed radius around their respective locations), aligning more closely with several real-world multiagent applications. These restrictions introduce many challenges that are overcome through local
    
[^54]: 人类如何感知对抗文本？对基于词语对抗攻击的有效性和自然性进行现实检验。

    How do humans perceive adversarial text? A reality check on the validity and naturalness of word-based adversarial attacks. (arXiv:2305.15587v1 [cs.CL])

    [http://arxiv.org/abs/2305.15587](http://arxiv.org/abs/2305.15587)

    本研究通过调查人们对抗性文本样本的可感知性，得出现有文本攻击在人类参与的现实世界场景中是不切实际的，提供了更为现实的对NLP模型鲁棒性的评估。

    

    基于机器学习的自然语言处理(NLP)模型容易受到对抗攻击——恶意算法会微小地修改输入文本，导致模型做出错误预测。然而，这些攻击的评估忽略了不可察觉性质或者在有限的情况下进行研究。这意味着对抗扰动不会通过任何人类质量测试，也不会对通过人工检查的NLP系统构成真正的威胁。为了绕过这个限制并实现NLP模型鲁棒性的适当评估（以及后来的改进），我们对378名人类参与者就当前最先进的攻击方法生产的文本对抗样本的可感知性进行了调查。我们的结果表明，现有的文本攻击在人类参与的现实世界场景中是不切实际的。这与先前规模较小的人类研究相矛盾，后者报道了攻击成功的过于乐观结论。通过我们的工作，我们希望为当前对NLP模型的对抗攻击的研究做出积极贡献，提供对其潜在影响的更现实的看法。

    Natural Language Processing (NLP) models based on Machine Learning (ML) are susceptible to adversarial attacks -- malicious algorithms that imperceptibly modify input text to force models into making incorrect predictions. However, evaluations of these attacks ignore the property of imperceptibility or study it under limited settings. This entails that adversarial perturbations would not pass any human quality gate and do not represent real threats to human-checked NLP systems. To bypass this limitation and enable proper assessment (and later, improvement) of NLP model robustness, we have surveyed 378 human participants about the perceptibility of text adversarial examples produced by state-of-the-art methods. Our results underline that existing text attacks are impractical in real-world scenarios where humans are involved. This contrasts with previous smaller-scale human studies, which reported overly optimistic conclusions regarding attack success. Through our work, we hope to positi
    
[^55]: 目标导向任务中的反平方Levy步态普遍出现。

    Inverse square Levy walk emerging universally in goal-oriented tasks. (arXiv:2305.15559v1 [cond-mat.stat-mech])

    [http://arxiv.org/abs/2305.15559](http://arxiv.org/abs/2305.15559)

    本研究证明了反平方Levy步态（称为Cauchy步态）在目标导向任务中普遍出现。

    

    Levy步态中，步长出现频率遵循幂律分布，可以在各种生物的迁移行为中观察到。观察到了接近于2的幂指数的Levy步态，但其原因尚不清楚。本研究旨在提出一种普遍产生反平方Levy步态（称为Cauchy步态）的模型，并确定出Cauchy步态出现条件。我们证明了，在目标导向的任务中，Cauchy步态普遍出现。我们使用术语“目标导向”，当目标明确时，但可以通过不同的方式实现，而无法确定唯一的方式。我们进行了模拟，一个代理观察到在二维空间中从概率分布生成的数据，并连续估计该概率分布的中心坐标。代理有一个概率分布模型作为数据生成分布的假设，并可以修改该模型，以使其更符合实际情况。

    The Levy walk in which the frequency of occurrence of step lengths follows a power-law distribution, can be observed in the migratory behavior of organisms at various levels. Levy walks with power exponents close to 2 are observed, and the reasons are unclear. This study aims to propose a model that universally generates inverse square Levy walks (called Cauchy walks) and to identify the conditions under which Cauchy walks appear. We demonstrate that Cauchy walks emerge universally in goal-oriented tasks. We use the term "goal-oriented" when the goal is clear, but this can be achieved in different ways, which cannot be uniquely determined. We performed a simulation in which an agent observed the data generated from a probability distribution in a two-dimensional space and successively estimated the central coordinates of that probability distribution. The agent has a model of probability distribution as a hypothesis for data-generating distribution and can modify the model such that ea
    
[^56]: 从交互式到协同构建任务学习

    From Interactive to Co-Constructive Task Learning. (arXiv:2305.15535v1 [cs.RO])

    [http://arxiv.org/abs/2305.15535](http://arxiv.org/abs/2305.15535)

    本文提出将机器人交互学习的研究重点放在协同构建过程上，以使机器人能够在日常情况下从非专家用户那里学习。

    

    人类通过利用支架策略，以很少的任务演示次数，将新任务或适应任务的相关方面教给社交伙伴，并利用先前的知识和重要的先前联合体验产生共同理解和共同执行所需的步骤。这一过程已经在父母幼儿互动中被发现和分析，并构成“共同建构”，因为它允许教师和学习者共同为任务做出贡献。我们建议将机器人交互学习的研究重点放在这种共同建构过程上，以使机器人能够在日常情况下从非专家用户那里学习。接下来，我们将审查当前的交互式任务学习提案，并讨论它们在涉及互动方面的主要贡献。然后，我们将讨论我们关于协同构建的概念，并总结来自成年人-孩子和人机交互的研究洞见，以阐明它。

    Humans have developed the capability to teach relevant aspects of new or adapted tasks to a social peer with very few task demonstrations by making use of scaffolding strategies that leverage prior knowledge and importantly prior joint experience to yield a joint understanding and a joint execution of the required steps to solve the task. This process has been discovered and analyzed in parent-infant interaction and constitutes a ``co-construction'' as it allows both, the teacher and the learner, to jointly contribute to the task. We propose to focus research in robot interactive learning on this co-construction process to enable robots to learn from non-expert users in everyday situations. In the following, we will review current proposals for interactive task learning and discuss their main contributions with respect to the entailing interaction. We then discuss our notion of co-construction and summarize research insights from adult-child and human-robot interactions to elucidate it
    
[^57]: 越大的语言模型错误越难以捉摸：Python中的标识符交换不被语言模型识别

    The Larger They Are, the Harder They Fail: Language Models do not Recognize Identifier Swaps in Python. (arXiv:2305.15507v1 [cs.CL])

    [http://arxiv.org/abs/2305.15507](http://arxiv.org/abs/2305.15507)

    该论文揭示了大型语言模型对Python标识符交换的识别问题，尤其是在逆比例缩放现象影响下表现更为显著。这表明LLM缺乏深刻、抽象的理解，无法胜任与训练偏差的任务。

    

    大型语言模型已成功应用于代码生成任务，这引发了一个问题，即这些模型对编程的理解程度如何。传统的编程语言具有不变性和等变性，人类程序员能直观地理解和利用这些性质，如标识符重命名（近似）不变性。我们发现，LLM在默认函数名称交换时不仅无法正确生成Python代码，有些模型在模型大小增加时甚至变得更加自信地进行错误预测，这是最近发现的逆比例缩放现象的实例，与通常观察到的模型大小增加会提高预测质量的趋势相反。我们的发现表明，尽管LLM的典型情况表现出色，但它们仍然缺乏一个深刻的、抽象的理解据以操纵内容，使它们无法胜任与训练偏差的任务。

    Large Language Models (LLMs) have successfully been applied to code generation tasks, raising the question of how well these models understand programming. Typical programming languages have invariances and equivariances in their semantics that human programmers intuitively understand and exploit, such as the (near) invariance to the renaming of identifiers. We show that LLMs not only fail to properly generate correct Python code when default function names are swapped, but some of them even become more confident in their incorrect predictions as the model size increases, an instance of the recently discovered phenomenon of Inverse Scaling, which runs contrary to the commonly observed trend of increasing prediction quality with increasing model size. Our findings indicate that, despite their astonishing typical-case performance, LLMs still lack a deep, abstract understanding of the content they manipulate, making them unsuitable for tasks that statistically deviate from their training 
    
[^58]: 用户兴趣旅程的大型语言模型

    Large Language Models for User Interest Journeys. (arXiv:2305.15498v1 [cs.CL])

    [http://arxiv.org/abs/2305.15498](http://arxiv.org/abs/2305.15498)

    该论文提出了使用大型语言模型(LLMs)对用户兴趣进行建模的方法，并通过定义兴趣旅程，提出了一种模型旨在提高推荐的质量，并提供了可解释性和新颖性。

    

    大型语言模型（LLMs）已经展示出在自然语言理解和生成方面的令人瞩目能力。然而，它们在更深入地理解用户和改善个性化推荐平台体验方面的潜力还远未被发挥。本文旨在填补这一空白。我们提出了使用LLMs对用户兴趣进行建模的方法，并定义了兴趣旅程作为用户基于他们的活动而遍历过的兴趣状态序列。我们的实验证明，相对于传统的用户表示方法，我们提出的方法可以提高推荐的质量，并且生成的兴趣旅程为推荐过程提供了可解释性和新颖性。

    Large language models (LLMs) have shown impressive capabilities in natural language understanding and generation. Their potential for deeper user understanding and improved personalized user experience on recommendation platforms is, however, largely untapped. This paper aims to address this gap. Recommender systems today capture users' interests through encoding their historical activities on the platforms. The generated user representations are hard to examine or interpret. On the other hand, if we were to ask people about interests they pursue in their life, they might talk about their hobbies, like I just started learning the ukulele, or their relaxation routines, e.g., I like to watch Saturday Night Live, or I want to plant a vertical garden. We argue, and demonstrate through extensive experiments, that LLMs as foundation models can reason through user activities, and describe their interests in nuanced and interesting ways, similar to how a human would.  We define interest journe
    
[^59]: SPRING: GPT-4通过学习论文和推理在游戏中表现超过RL算法

    SPRING: GPT-4 Out-performs RL Algorithms by Studying Papers and Reasoning. (arXiv:2305.15486v1 [cs.AI])

    [http://arxiv.org/abs/2305.15486](http://arxiv.org/abs/2305.15486)

    SPRING是一个新的方法，能够在开放世界游戏中表现出色，它通过阅读游戏的原始学术论文并使用所学知识进行推理并玩游戏。。

    

    开放世界游戏由于其多任务、深度探索和目标优先级要求，对AI算法提出了重大挑战。尽管强化学习（RL）在解决游戏方面很受欢迎，但其高样本复杂性限制了它在像Crafter或Minecraft这样复杂的开放世界游戏中的有效性。我们提出了一种新颖的方法SPRING，通过阅读游戏的原始学术论文并使用所学知识进行推理并玩游戏，来解决这个问题。在给定LaTeX源作为游戏语境和代理当前观察的描述的情况下，我们的SPRING框架利用具有游戏相关问题的定向无环图（DAG）作为节点和依赖关系作为边。通过按拓扑顺序遍历DAG并计算每个节点的LLM响应来确定在环境中采取的最优行动，LLM对最终节点的答案直接转化为环境行动。在我们的实验中，我们研究了

    Open-world survival games pose significant challenges for AI algorithms due to their multi-tasking, deep exploration, and goal prioritization requirements. Despite reinforcement learning (RL) being popular for solving games, its high sample complexity limits its effectiveness in complex open-world games like Crafter or Minecraft. We propose a novel approach, SPRING, to read the game's original academic paper and use the knowledge learned to reason and play the game through a large language model (LLM). Prompted with the LaTeX source as game context and a description of the agent's current observation, our SPRING framework employs a directed acyclic graph (DAG) with game-related questions as nodes and dependencies as edges. We identify the optimal action to take in the environment by traversing the DAG and calculating LLM responses for each node in topological order, with the LLM's answer to final node directly translating to environment actions. In our experiments, we study the quality
    
[^60]: 英国中小企业有效数据驱动决策的趋势与挑战：基于对85家中小企业案例研究和经验教训分析

    Trends and Challenges Towards an Effective Data-Driven Decision Making in UK SMEs: Case Studies and Lessons Learnt from the Analysis of 85 SMEs. (arXiv:2305.15454v1 [cs.CY])

    [http://arxiv.org/abs/2305.15454](http://arxiv.org/abs/2305.15454)

    本文以英国西米德兰地区85家中小企业为例，探讨了中小企业在进行有效数据驱动决策方面面临的趋势和挑战，尤其是由于有限的资源和融资渠道受限等原因，如何在技能和IT投资方面寻找平衡。

    

    数据科学的采用为中小型企业带来了巨大的好处，包括提高业务生产力、促进经济增长、创新和创造就业机会。数据科学可以支持中小企业优化生产流程，预测客户需求，预测机器故障并提供高效的智能服务。企业还可以利用人工智能和大数据的力量以及智能使用数字技术来提高生产力和业务表现，为创新铺平道路。然而，将数据科学决策整合到中小企业中需要技能和IT投资。在大多数情况下，由于资源有限和融资渠道受限，这些费用超出了中小企业的负担范围。本文以英国西米德兰地区为主要案例研究85家中小企业，提出了针对组织有效数据驱动决策的趋势和挑战。该工作是作为3年ERDF（欧洲区域发展基金）计划的一部分资助的。

    The adoption of data science brings vast benefits to Small and Medium-sized Enterprises (SMEs) including business productivity, economic growth, innovation and jobs creation. Data Science can support SMEs to optimise production processes, anticipate customers' needs, predict machinery failures and deliver efficient smart services. Businesses can also harness the power of Artificial Intelligence (AI) and Big Data and the smart use of digital technologies to enhance productivity and performance, paving the way for innovation. However, integrating data science decisions into an SME requires both skills and IT investments. In most cases, such expenses are beyond the means of SMEs due to limited resources and restricted access to financing. This paper presents trends and challenges towards an effective data-driven decision making for organisations based on a case study of 85 SMEs, mostly from the West Midlands region of England. The work is supported as part of a 3 years ERDF (European Regi
    
[^61]: PromptNER: 基于提示的命名实体识别

    PromptNER: Prompting For Named Entity Recognition. (arXiv:2305.15444v1 [cs.CL])

    [http://arxiv.org/abs/2305.15444](http://arxiv.org/abs/2305.15444)

    PromptNER是一种基于提示的命名实体识别算法，利用LLM生成潜在实体列表并提供解释，在少样本NER和跨领域NER方面实现了最先进性能。

    

    令人惊讶的是，大型语言模型（LLMs）和越来越多的基于提示的启发式方法现在提供了强大的现成方法，为各种经典的NLP问题提供了少量样本的解决方案。然而，尽管有着令人期待的初步结果，但这些基于LLM的少样本方法在命名实体识别（NER）方面仍远未达到最先进水平，现有的方法包括通过端到端结构理解学习表示，并在标准标记语料库上进行微调。本文介绍了PromptNER，一种新的用于少样本和跨领域NER的最先进算法。为了适应任何新的NER任务，PromptNER需要提供一组实体定义，除基本的少样本样例以外。给定输入句子，PromptNER提示LLM生成一个潜在实体列表，并提供相应的解释，证明它们与提供的实体类型定义的兼容性。值得注意的是，PromptNER在少样本NER任务方面实现了最先进的性能，并在具有挑战性的WikiAnn数据集上为跨领域NER设定了新的SOTA。

    In a surprising turn, Large Language Models (LLMs) together with a growing arsenal of prompt-based heuristics now offer powerful off-the-shelf approaches providing few-shot solutions to myriad classic NLP problems. However, despite promising early results, these LLM-based few-shot methods remain far from the state of the art in Named Entity Recognition (NER), where prevailing methods include learning representations via end-to-end structural understanding and fine-tuning on standard labeled corpora. In this paper, we introduce PromptNER, a new state-of-the-art algorithm for few-Shot and cross-domain NER. To adapt to any new NER task PromptNER requires a set of entity definitions in addition to the standard few-shot examples. Given a sentence, PromptNER prompts an LLM to produce a list of potential entities along with corresponding explanations justifying their compatibility with the provided entity type definitions. Remarkably, PromptNER achieves state-of-the-art performance on few-sho
    
[^62]: PulseNet: 使用随机数据增强策略和连续小波变换进行犬ECG信号分类的深度学习模型。

    PulseNet: Deep Learning ECG-signal classification using random augmentation policy and continous wavelet transform for canines. (arXiv:2305.15424v1 [eess.SP])

    [http://arxiv.org/abs/2305.15424](http://arxiv.org/abs/2305.15424)

    该论文提出了一种利用深度学习对犬的ECG信号进行自动分类的方法，通过使用随机数据增强策略和连续小波变换，分类精度得到了提高。

    

    评估犬的心电图(ECG)需要熟练的兽医，但目前可用的兽医心脏病专家用于ECG解读和诊断支持的数量有限。开发自动评估ECG序列的工具可以通过提供临床医生实时结果和决策支持工具来改善兽医护理。我们实现了一个深度卷积神经网络(CNN)来将犬的心电图序列分类为正常或异常。将ECG记录转换为8秒的第二导联序列，根据是否存在一种或多种心脏异常将其分类为正常或异常。训练ECG序列使用RandomAugmentECG进行随机数据增强，这是一个专门为该项目实现的新增强库。然后，每个块使用连续小波变换转换成2D scalogram。2D scalogram使用二元CNN分类器分类成正常或异常。

    Evaluating canine electrocardiograms (ECG) require skilled veterinarians, but current availability of veterinary cardiologists for ECG interpretation and diagnostic support is limited. Developing tools for automated assessment of ECG sequences can improve veterinary care by providing clinicians real-time results and decision support tools. We implement a deep convolutional neural network (CNN) approach for classifying canine electrocardiogram sequences as either normal or abnormal. ECG records are converted into 8 second Lead II sequences and classified as either normal (no evidence of cardiac abnormalities) or abnormal (presence of one or more cardiac abnormalities). For training ECG sequences are randomly augmented using RandomAugmentECG, a new augmentation library implemented specifically for this project. Each chunk is then is converted using a continuous wavelet transform into a 2D scalogram. The 2D scalogram are then classified as either normal or abnormal by a binary CNN classif
    
[^63]: 基于混合语义几何方法的建筑点云无杂乱物平面生成

    A Hybrid Semantic-Geometric Approach for Clutter-Resistant Floorplan Generation from Building Point Clouds. (arXiv:2305.15420v1 [cs.CV])

    [http://arxiv.org/abs/2305.15420](http://arxiv.org/abs/2305.15420)

    本研究提出了一种基于混合语义几何方法的建筑点云无杂乱物平面生成方法，通过预处理输入点云、使用PointNet++网络标记点、最后通过几何方法生成无杂乱物的平面。

    

    建筑信息模拟（BIM）技术是现代建筑工程和项目管理工作流程的关键组成部分。展示项目现场空间实际情况的现状BIM模型可以为利益相关者提供关键信息，用于建设进度监控、错误检查和建筑维护。自动将扫描原始数据转换为BIM模型（扫描到BIM）的几何方法通常无法利用数据中更高级的语义信息，而语义分割方法仅在点级别上输出标签，而不创建对象级模型，这是BIM所必需的。为解决这些问题，本研究提出了一种基于混合语义几何方法的建筑点云无杂乱物平面生成方法。首先通过归一化坐标系和去除异常值来预处理输入点云。然后，基于PointNet++的语义分割网络用于对点进行标记，最后通过几何方法生成无杂乱物的平面。

    Building Information Modeling (BIM) technology is a key component of modern construction engineering and project management workflows. As-is BIM models that represent the spatial reality of a project site can offer crucial information to stakeholders for construction progress monitoring, error checking, and building maintenance purposes. Geometric methods for automatically converting raw scan data into BIM models (Scan-to-BIM) often fail to make use of higher-level semantic information in the data. Whereas, semantic segmentation methods only output labels at the point level without creating object level models that is necessary for BIM. To address these issues, this research proposes a hybrid semantic-geometric approach for clutter-resistant floorplan generation from laser-scanned building point clouds. The input point clouds are first pre-processed by normalizing the coordinate system and removing outliers. Then, a semantic segmentation network based on PointNet++ is used to label eac
    
[^64]: 机器学习辅助的过渡金属X射线衍射相识别

    Machine learning-assisted close-set X-ray diffraction phase identification of transition metals. (arXiv:2305.15410v1 [cond-mat.mtrl-sci])

    [http://arxiv.org/abs/2305.15410](http://arxiv.org/abs/2305.15410)

    本文探究了如何利用机器学习从过渡金属及其氧化物的X射线衍射数据中预测晶体结构相，提出的机器学习框架实现了竞争性能，表明了机器学习对X射线衍射和晶体结构确定领域的潜在影响。

    

    机器学习被应用于X射线衍射相预测问题，并取得了良好的结果。在本文中，我们描述了一种使用机器学习从过渡金属及其氧化物的X射线衍射数据中预测晶体结构相的方法。我们评估了我们的方法的性能，并比较了其多种设置下的性能。结果表明，所提出的机器学习框架实现了竞争性能。这证明了机器学习对X射线衍射和晶体结构确定领域的潜在影响。开源实现：https://github.com/maxnygma/NeuralXRD.

    Machine learning has been applied to the problem of X-ray diffraction phase prediction with promising results. In this paper, we describe a method for using machine learning to predict crystal structure phases from X-ray diffraction data of transition metals and their oxides. We evaluate the performance of our method and compare the variety of its settings. Our results demonstrate that the proposed machine learning framework achieves competitive performance. This demonstrates the potential for machine learning to significantly impact the field of X-ray diffraction and crystal structure determination. Open-source implementation: https://github.com/maxnygma/NeuralXRD.
    
[^65]: 知识设计：通过知识提炼推动蛋白质设计的极限

    Knowledge-Design: Pushing the Limit of Protein Deign via Knowledge Refinement. (arXiv:2305.15151v2 [q-bio.BM] UPDATED)

    [http://arxiv.org/abs/2305.15151](http://arxiv.org/abs/2305.15151)

    本文提出了一种知识感知模块来提炼低质量残基，引入记忆检索机制实现了超过50%的训练时间节省，并取得了较好的性能表现，是蛋白质设计领域的一次创新。

    

    最近的研究表明，在蛋白质设计中，寻找折叠为所期望结构的氨基酸序列已经取得了竞争优势。然而，大多数研究忽略了预测置信度的重要性，未能覆盖广泛的蛋白质空间，并且没有融入常见的蛋白质知识。本文提出了一种知识感知模块来提炼低质量残基，并引入了一种记忆检索机制来节省超过50%的训练时间。我们在CATH、TS50和TS500数据集上对所提出的方法进行了广泛评估，结果显示我们的知识设计方法在CATH数据集上的性能超过了先前的PiFold方法约9％。具体来说，知识设计是第一个实现了...

    Recent studies have shown competitive performance in protein design that aims to find the amino acid sequence folding into the desired structure. However, most of them disregard the importance of predictive confidence, fail to cover the vast protein space, and do not incorporate common protein knowledge. After witnessing the great success of pretrained models on diverse protein-related tasks and the fact that recovery is highly correlated with confidence, we wonder whether this knowledge can push the limits of protein design further. As a solution, we propose a knowledge-aware module that refines low-quality residues. We also introduce a memory-retrieval mechanism to save more than 50\% of the training time. We extensively evaluate our proposed method on the CATH, TS50, and TS500 datasets and our results show that our Knowledge-Design method outperforms the previous PiFold method by approximately 9\% on the CATH dataset. Specifically, Knowledge-Design is the first method that achieves 
    
[^66]: 复数值自编码器对物体发现的对比训练

    Contrastive Training of Complex-Valued Autoencoders for Object Discovery. (arXiv:2305.15001v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.15001](http://arxiv.org/abs/2305.15001)

    介绍了一种架构修改和新颖对比学习方法，大大改进了同步性基模型，首次获得了一类同步性基模型，在多物体彩色数据集中无监督地发现物体。

    

    当前最先进的物体中心模型使用插槽和注意力路由进行绑定。然而，这类模型有几个概念性的局限性：插槽的数量是硬编码的；所有插槽的容量相等；训练成本高昂；插槽内没有目标级别的关系因素。原则上，基于同步性的模型可以通过使用复数值激活在其相位分量中存储绑定信息来解决这些限制。然而，这类基于同步性的模型的工作示例只是最近才有，而且实际上仍然限于玩具灰度数据集的同时存储不到三个物体。在这里，我们介绍了架构修改和一种新颖的对比学习方法，极大地改进了最先进的同步性基模型。我们首次获得了一类同步性基模型，能够在多物体彩色数据集中以无监督的方式发现物体。

    Current state-of-the-art object-centric models use slots and attention-based routing for binding. However, this class of models has several conceptual limitations: the number of slots is hardwired; all slots have equal capacity; training has high computational cost; there are no object-level relational factors within slots. Synchrony-based models in principle can address these limitations by using complex-valued activations which store binding information in their phase components. However, working examples of such synchrony-based models have been developed only very recently, and are still limited to toy grayscale datasets and simultaneous storage of less than three objects in practice. Here we introduce architectural modifications and a novel contrastive learning method that greatly improve the state-of-the-art synchrony-based model. For the first time, we obtain a class of synchrony-based models capable of discovering objects in an unsupervised manner in multi-object color datasets 
    
[^67]: CMOT: 通过最优传输进行跨模态Mixup，用于语音翻译

    CMOT: Cross-modal Mixup via Optimal Transport for Speech Translation. (arXiv:2305.14635v1 [cs.CL])

    [http://arxiv.org/abs/2305.14635](http://arxiv.org/abs/2305.14635)

    CMOT是一种用于跨模态语音翻译的方法，通过最优传输找到语音和文本序列之间的对齐，并在标记级别上混合不同模态的序列，实现了在有限数据下更好的性能表现。

    

    端到端语音翻译（ST）是将源语言中的语音信号翻译成目标语言文本的任务。作为一项跨模态任务，端到端ST在有限数据下进行训练非常困难。现有的方法通常尝试从机器翻译（MT）中转移知识，但其性能由于语音和文本之间的模态差距受到限制。本文提出了Cross-modal Mixup via Optimal Transport（CMOT）来克服模态差距。我们通过最优传输找到语音和文本序列之间的对齐，然后使用对齐在标记级别上混合来自不同模态的序列。在MuST-C ST基准测试上的实验表明，CMOT在8个翻译方向上实现了平均BLEU值为30.0，超过了以前的方法。进一步的分析表明，CMOT可以自适应地找到模态之间的对齐，有助于缓解语音和文本之间的模态差距。代码公开可用于 https://github.com/ic

    End-to-end speech translation (ST) is the task of translating speech signals in the source language into text in the target language. As a cross-modal task, end-to-end ST is difficult to train with limited data. Existing methods often try to transfer knowledge from machine translation (MT), but their performances are restricted by the modality gap between speech and text. In this paper, we propose Cross-modal Mixup via Optimal Transport CMOT to overcome the modality gap. We find the alignment between speech and text sequences via optimal transport and then mix up the sequences from different modalities at a token level using the alignment. Experiments on the MuST-C ST benchmark demonstrate that CMOT achieves an average BLEU of 30.0 in 8 translation directions, outperforming previous methods. Further analysis shows CMOT can adaptively find the alignment between modalities, which helps alleviate the modality gap between speech and text. Code is publicly available at https://github.com/ic
    
[^68]: 无监督地在球面上学习连续技能

    Unsupervised Discovery of Continuous Skills on a Sphere. (arXiv:2305.14377v1 [cs.LG])

    [http://arxiv.org/abs/2305.14377](http://arxiv.org/abs/2305.14377)

    本文提出了一种名为DISCS的方法，可以无限学习不同的连续技能，在MuJoCo Ant机器人控制环境中展示了其比其他方法更具多样性。

    

    最近，研究了许多种无监督强化学习方法，用于学习不同的技能帮助机器人产生多样化的行为，而不需要外部奖励。但是，现有的大多数方法学习的是有限数量的离散技能，因此它们展现出的行为多样性也很有限。本文提出了一种新的方法，名为“在球面上学习连续技能的发现”，可以学习无限种不同的技能。在这种方法中，技能是通过最大化技能和状态之间的互信息来学习的，而每种技能都对应于球面上的一个连续值。由于技能在DISCS中的表示是连续的，所以可以学习无限多种不同的技能。我们将现有的方法和DISCS应用于MuJoCo Ant机器人控制环境中，并展示了DISCS可以比其他方法学习到更多的多样化技能。

    Recently, methods for learning diverse skills to generate various behaviors without external rewards have been actively studied as a form of unsupervised reinforcement learning. However, most of the existing methods learn a finite number of discrete skills, and thus the variety of behaviors that can be exhibited with the learned skills is limited. In this paper, we propose a novel method for learning potentially an infinite number of different skills, which is named discovery of continuous skills on a sphere (DISCS). In DISCS, skills are learned by maximizing mutual information between skills and states, and each skill corresponds to a continuous value on a sphere. Because the representations of skills in DISCS are continuous, infinitely diverse skills could be learned. We examine existing methods and DISCS in the MuJoCo Ant robot control environments and show that DISCS can learn much more diverse skills than the other methods.
    
[^69]: 弱监督下AUC优化：统一的部分AUC方法

    Weakly Supervised AUC Optimization: A Unified Partial AUC Approach. (arXiv:2305.14258v1 [cs.LG] CROSS LISTED)

    [http://arxiv.org/abs/2305.14258](http://arxiv.org/abs/2305.14258)

    本文提出了WSAUC，一种解决弱监督下AUC优化问题的统一框架，它包括噪声标签学习、正-无标签学习、多实例学习和半监督学习场景，并提出了一种新型的部分AUC——反转部分AUC（rpAUC），作为鲁棒的AUC最大化训练目标，为各种弱监督场景下的AUC优化提供了一种通用解决方案。

    

    由于获取完美的监督通常很困难，现实中的机器学习任务通常面临不准确、不完整或不精确的监督，统称为弱监督。在本文中，我们提出了WSAUC，一种用于解决弱监督下AUC优化问题的统一框架，它涵盖了噪声标签学习、正-无标签学习、多实例学习和半监督学习场景。在WSAUC框架内，我们首先将各种弱监督场景下的AUC优化问题框架化为最小化受污染集合上AUC风险的常见形式，并证明经验风险最小化问题与真实AUC一致。然后，我们介绍一种新型的部分AUC，即反转部分AUC（rpAUC），它作为鲁棒的AUC最大化训练目标，在存在污染标签的情况下发挥作用。WSAUC为各种弱监督场景下的AUC优化提供了一种通用解决方案。

    Since acquiring perfect supervision is usually difficult, real-world machine learning tasks often confront inaccurate, incomplete, or inexact supervision, collectively referred to as weak supervision. In this work, we present WSAUC, a unified framework for weakly supervised AUC optimization problems, which covers noisy label learning, positive-unlabeled learning, multi-instance learning, and semi-supervised learning scenarios. Within the WSAUC framework, we first frame the AUC optimization problems in various weakly supervised scenarios as a common formulation of minimizing the AUC risk on contaminated sets, and demonstrate that the empirical risk minimization problems are consistent with the true AUC. Then, we introduce a new type of partial AUC, specifically, the reversed partial AUC (rpAUC), which serves as a robust training objective for AUC maximization in the presence of contaminated labels. WSAUC offers a universal solution for AUC optimization in various weakly supervised scena
    
[^70]: 通过欧盟非歧视法的视角讲述算法不公平性：或谓法律非决策树。

    Algorithmic Unfairness through the Lens of EU Non-Discrimination Law: Or Why the Law is not a Decision Tree. (arXiv:2305.13938v1 [cs.CY])

    [http://arxiv.org/abs/2305.13938](http://arxiv.org/abs/2305.13938)

    本文分析了算法不公平性的示例，并从欧盟非歧视法的角度讲述了其中涉及的不公正。同时，本文建立了框架以帮助决策者确定算法公平性指标以符合欧盟非歧视法理。

    

    人工智能系统中美感到不公平和歧视的问题最近引起了法律和计算机科学学者的越来越多的关注。然而，算法偏见和公平性以及法律上的歧视和平等概念之间的重叠程度通常不清楚，导致计算机科学和法律之间的误解。本文旨在阐明欧盟非歧视法与计算机科学文献中提出的算法公平性概念在多大程度上重合以及它们的区别。本文的贡献如下：首先，我们通过欧盟案例法的角度来分析算法不公平的典型例子，找出与欧盟案例法的类比之处。其次，我们建立了一个框架，以帮助决策者确定算法和AI系统的公平性指标，以确保它们符合欧盟的非歧视法理。

    Concerns regarding unfairness and discrimination in the context of artificial intelligence (AI) systems have recently received increased attention from both legal and computer science scholars. Yet, the degree of overlap between notions of algorithmic bias and fairness on the one hand, and legal notions of discrimination and equality on the other, is often unclear, leading to misunderstandings between computer science and law. What types of bias and unfairness does the law address when it prohibits discrimination? What role can fairness metrics play in establishing legal compliance? In this paper, we aim to illustrate to what extent European Union (EU) non-discrimination law coincides with notions of algorithmic fairness proposed in computer science literature and where they differ. The contributions of this paper are as follows. First, we analyse seminal examples of algorithmic unfairness through the lens of EU non-discrimination law, drawing parallels with EU case law. Second, we set
    
[^71]: 一种基于趋势的超导线性加速器零-shot束流控制方法

    Trend-Based SAC Beam Control Method with Zero-Shot in Superconducting Linear Accelerator. (arXiv:2305.13869v1 [physics.acc-ph])

    [http://arxiv.org/abs/2305.13869](http://arxiv.org/abs/2305.13869)

    本文提出了一种基于趋势的零-shot束流控制方法，在 CAFe II 和 LPI 中取得了良好的效果，可以将校正时间缩短到人类专家所需时间的十分之一。

    

    超导线性加速器是现代科学研究的高度灵活的设施，需要每周重新配置和调整。因此，最小化设置时间对于提供充足的实验时间至关重要。我们提出了一种基于趋势的软 actor-critic(TBSAC)束流控制方法，具有强大的鲁棒性，允许代理在模拟环境中进行训练，并直接应用于真正的加速器中，实现了零-shot控制。为了验证我们的方法的有效性，分别在中国超重元素加速器设施(CAFe II)和一个轻质粒子注入器(LPI)中执行了两个不同的典型束流控制任务。在CAFe II的三个低温模块中分别执行了轨道校正任务，调谐所需时间已经减少到人类专家所需时间的十分之一，校正后的RMS值都小于1毫米。另一个传输效率优化任务在CAFe II的加速器段LPI中进行了

    The superconducting linear accelerator is a highly flexiable facility for modern scientific discoveries, necessitating weekly reconfiguration and tuning. Accordingly, minimizing setup time proves essential in affording users with ample experimental time. We propose a trend-based soft actor-critic(TBSAC) beam control method with strong robustness, allowing the agents to be trained in a simulated environment and applied to the real accelerator directly with zero-shot. To validate the effectiveness of our method, two different typical beam control tasks were performed on China Accelerator Facility for Superheavy Elements (CAFe II) and a light particle injector(LPI) respectively. The orbit correction tasks were performed in three cryomodules in CAFe II seperately, the time required for tuning has been reduced to one-tenth of that needed by human experts, and the RMS values of the corrected orbit were all less than 1mm. The other transmission efficiency optimization task was conducted in th
    
[^72]: EXACT：用于分布式学习的全面攻击方法

    EXACT: Extensive Attack for Split Learning. (arXiv:2305.12997v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.12997](http://arxiv.org/abs/2305.12997)

    本文提出了一种名为EXACT的方法，可以安全地在分布式学习中进行梯度交换，同时保护隐私、保持准确性和效率性。

    

    隐私保护机器学习可以帮助我们训练和部署利用私人信息的模型。在设备上进行机器学习可以使我们在推断期间完全避免与第三方服务器共享信息。然而，与服务器端相比，设备上的模型通常较不准确，因为它们通常只依赖于一小组设备特征且需要足够小才能在终端用户设备上高效运行。分布式学习是一种有前途的方法，可以克服这些限制。在分布式学习中，将一个大型的机器学习模型分成两部分，大部分位于服务器端，小部分在设备上执行，旨在整合私有特征。然而，这种模型的端到端训练需要在分界处交换梯度，这可能编码私有特征或标签。在本文中，我们提出了一种名为 EXACT（Extensive Attack for Split Learning）的新颖隐私保护方法，通过引入广泛的噪声实现安全的梯度交换，同时保持模型的准确性和效率。我们在三个基准数据集上评估了我们的方法，结果显示它在准确性和隐私保护方面优于现有隐私保护方法。

    Privacy-Preserving machine learning (PPML) can help us train and deploy models that utilize private information. In particular, on-device Machine Learning allows us to completely avoid sharing information with a third-party server during inference. However, on-device models are typically less accurate when compared to the server counterparts due to the fact that (1) they typically only rely on a small set of on-device features and (2) they need to be small enough to run efficiently on end-user devices. Split Learning (SL) is a promising approach that can overcome these limitations. In SL, a large machine learning model is divided into two parts, with the bigger part residing on the server-side and a smaller part executing on-device, aiming to incorporate the private features. However, end-to-end training of such models requires exchanging gradients at the cut layer, which might encode private features or labels. In this paper, we provide insights into potential privacy risks associated
    
[^73]: FIT：远程交错Transformer

    FIT: Far-reaching Interleaved Transformers. (arXiv:2305.12689v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.12689](http://arxiv.org/abs/2305.12689)

    FIT是一种基于Transformer的架构，将数据标记分组，使用局部层和全局层进行操作。通过交错使用这些层并使用交叉注意力促进信息交换，FIT在一系列任务中均实现最先进的性能。

    

    我们提出了FIT：一种基于Transformer的架构，具有高效的自我注意力和自适应计算能力。与原始Transformer不同的是，我们将数据标记分成组，每个组是一个较短的标记序列。我们使用两种类型的Transformer层：局部层在每个组内操作数据标记，而全局层在一个更小的引入的潜在标记集合上操作。这些层包括与标准Transformer相同的自我注意力和前馈层，被交错使用，交叉注意力用于在同一组内数据和潜在标记之间促进信息交换。每个大小为n的组内的注意力复杂度为$O(n^2)$，但对于长度为L的序列，可以在全局范围内达到$O(L^{{4}/{3}})$。通过更多地依赖执行使用更小潜在标记集合的全局层，可以进一步提高效率。FIT是一种多用途的架构，可应用于广泛的任务，并在几个基准数据集上实现了最先进的性能，包括语言建模和图像字幕生成。

    We present FIT: a transformer-based architecture with efficient self-attention and adaptive computation. Unlike original transformers, which operate on a single sequence of data tokens, we divide the data tokens into groups, with each group being a shorter sequence of tokens. We employ two types of transformer layers: local layers operate on data tokens within each group, while global layers operate on a smaller set of introduced latent tokens. These layers, comprising the same set of self-attention and feed-forward layers as standard transformers, are interleaved, and cross-attention is used to facilitate information exchange between data and latent tokens within the same group. The attention complexity is $O(n^2)$ locally within each group of size $n$, but can reach $O(L^{{4}/{3}})$ globally for sequence length of $L$. The efficiency can be further enhanced by relying more on global layers that perform adaptive computation using a smaller set of latent tokens. FIT is a versatile arch
    
[^74]: 马尔可夫$\alpha$-势博弈:均衡近似与遗憾分析

    Markov $\alpha$-Potential Games: Equilibrium Approximation and Regret Analysis. (arXiv:2305.12553v2 [cs.GT] UPDATED)

    [http://arxiv.org/abs/2305.12553](http://arxiv.org/abs/2305.12553)

    本文提出了一种新的框架来研究马尔可夫博弈，即马尔可夫 $\alpha$-势博弈。介绍了两种算法来计算其中的纳什均衡，且表明这些算法能够找到近似均衡。

    

    本文提出了一种研究马尔可夫博弈中多代理交互的新框架:马尔可夫$\alpha$-势博弈。马尔可夫势博弈是马尔可夫 $\alpha$-势博弈的特殊情况，马尔可夫拥堵博弈和扰动马尔可夫团队博弈是两个重要且实际意义重大的博弈类。本文提供了两个博弈的$\alpha$-势函数，并针对博弈参数表征了差距 $\alpha$。引入了两种算法——投影梯度上升算法和顺序最大改进平滑最佳反应动态——来近似计算马尔可夫$\alpha$-势博弈中的稳态纳什均衡。每个算法的纳什遗憾都显示为时间跨度的亚线性缩放。我们的分析和数值实验表明，简单的算法能够找到马尔可夫$\alpha$-势博弈的近似均衡。

    This paper proposes a new framework to study multi-agent interaction in Markov games: Markov $\alpha$-potential games. Markov potential games are special cases of Markov $\alpha$-potential games, so are two important and practically significant classes of games: Markov congestion games and perturbed Markov team games. In this paper, {$\alpha$-potential} functions for both games are provided and the gap $\alpha$ is characterized with respect to game parameters. Two algorithms -- the projected gradient-ascent algorithm and the sequential maximum improvement smoothed best response dynamics -- are introduced for approximating the stationary Nash equilibrium in Markov $\alpha$-potential games. The Nash-regret for each algorithm is shown to scale sub-linearly in time horizon. Our analysis and numerical experiments demonstrates that simple algorithms are capable of finding approximate equilibrium in Markov $\alpha$-potential games.
    
[^75]: 动态占据网格地图的深度雷达逆向传感器模型

    Deep Radar Inverse Sensor Models for Dynamic Occupancy Grid Maps. (arXiv:2305.12409v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2305.12409](http://arxiv.org/abs/2305.12409)

    该研究提出了一种基于深度学习的雷达逆向传感器模型，用于将稀疏雷达检测映射到极坐标测量网格，并生成动态网格地图，实验结果表明该方法优于手工制作的几何ISM。与最先进的深度学习方法相比，该方法为从有限视场的雷达中学习极坐标方案的单帧测量网格的第一个方法。

    

    实现自动驾驶的一个重要步骤是基于传感器输入对车辆环境进行建模。由于其众所周知的优势，雷达成为推断围绕车辆的网格单元占用状态的流行选择。为了解决雷达检测数据稀疏性和噪声问题，我们提出了一种基于深度学习的逆向传感器模型（ISM），用于学习从稀疏雷达检测到极坐标测量网格的映射。改进的基于激光雷达测量的网格用作参考。学习到的雷达测量网格与雷达多普勒速度测量相结合，进一步用于生成动态网格地图（DGM）。在实际的高速公路情景实验中表明，我们的方法优于手工制作的几何ISM。与最先进的深度学习方法相比，我们的方法是第一个从有限视场（FOV）的雷达中学习极坐标方案的单帧测量网格的方法。学习框架使学习到的ISM可以直接嵌入到现有的贝叶斯状态估计方案中，以提高环境建模的准确性。

    To implement autonomous driving, one essential step is to model the vehicle environment based on the sensor inputs. Radars, with their well-known advantages, became a popular option to infer the occupancy state of grid cells surrounding the vehicle. To tackle data sparsity and noise of radar detections, we propose a deep learning-based Inverse Sensor Model (ISM) to learn the mapping from sparse radar detections to polar measurement grids. Improved lidar-based measurement grids are used as reference. The learned radar measurement grids, combined with radar Doppler velocity measurements, are further used to generate a Dynamic Grid Map (DGM). Experiments in real-world highway scenarios show that our approach outperforms the hand-crafted geometric ISMs. In comparison to state-of-the-art deep learning methods, our approach is the first one to learn a single-frame measurement grid in the polar scheme from radars with a limited Field Of View (FOV). The learning framework makes the learned ISM
    
[^76]: EEG和EMG数据集，用于检测主动矫形器引入的错误

    EEG and EMG dataset for the detection of errors introduced by an active orthosis device. (arXiv:2305.11996v1 [cs.HC])

    [http://arxiv.org/abs/2305.11996](http://arxiv.org/abs/2305.11996)

    该论文介绍了一个包含EEG和EMG记录的数据集，这些记录来自被协助使用主动矫形器移动右臂的受试者。研究者故意引入了一些错误，以检测新的错误检测方法。

    

    本文介绍了包含电脑图谱（EEG）和肌电图（EMG）记录的数据集，这些记录来自8名被协助使用主动矫形器移动右臂的受试者。该设备支持肘关节运动，即右臂的屈曲和伸展。当矫形器主动移动被试者的手臂时，故意引入了一些错误，持续时间很短。在此期间，矫形器向相反的方向移动。本文解释了实验设置，并展示了跨所有受试者的一些行为分析。此外，我们还为一个受试者提供了平均事件相关电位分析，以提供关于数据质量和错误引入所引起的EEG活动的见解。在此介绍的数据集是公开可访问的。本研究的目的是为研究社区提供数据集，特别是针对异步检测错误的新方法的开发。

    This paper presents a dataset containing recordings of the electroencephalogram (EEG) and the electromyogram (EMG) from eight subjects who were assisted in moving their right arm by an active orthosis device. The supported movements were elbow joint movements, i.e., flexion and extension of the right arm. While the orthosis was actively moving the subject's arm, some errors were deliberately introduced for a short duration of time. During this time, the orthosis moved in the opposite direction. In this paper, we explain the experimental setup and present some behavioral analyses across all subjects. Additionally, we present an average event-related potential analysis for one subject to offer insights into the data quality and the EEG activity caused by the error introduction. The dataset described herein is openly accessible. The aim of this study was to provide a dataset to the research community, particularly for the development of new methods in the asynchronous detection of erroneo
    
[^77]: 使用ChatGPT比较软件开发人员：一项实证调查

    Comparing Software Developers with ChatGPT: An Empirical Investigation. (arXiv:2305.11837v1 [cs.SE])

    [http://arxiv.org/abs/2305.11837](http://arxiv.org/abs/2305.11837)

    这篇论文调查了ChatGPT与软件开发人员的比较，认为全面的实证研究可以帮助回答基于AI的计算是否可以提高生产力甚至取代软件工程师进行软件开发的争议。

    

    自动化在软件工程（SE）中的应用已从理论到现实中转变。许多学术文章记录了人工智能成功应用于项目管理、建模、测试和开发等领域的案例。最近的创新是引入了ChatGPT，这是一个被吹捧为能够生成编程代码和为开发人员提供软件测试策略的机器学习聊天机器人。虽然有猜测认为基于AI的计算可以提高生产力甚至取代软件工程师进行软件开发，但目前缺乏实证证据来验证这一点。此外，尽管AI系统的主要重点在于增强准确性，但非功能性要求包括能源效率、漏洞、公平性（即人为偏见）和安全性经常受到不足的关注。本文认为，全面的实证研究可以帮助回答这些问题。

    The advent of automation in particular Software Engineering (SE) tasks has transitioned from theory to reality. Numerous scholarly articles have documented the successful application of Artificial Intelligence to address issues in areas such as project management, modeling, testing, and development. A recent innovation is the introduction of ChatGPT, an ML-infused chatbot, touted as a resource proficient in generating programming codes and formulating software testing strategies for developers and testers respectively. Although there is speculation that AI-based computation can increase productivity and even substitute software engineers in software development, there is currently a lack of empirical evidence to verify this. Moreover, despite the primary focus on enhancing the accuracy of AI systems, non-functional requirements including energy efficiency, vulnerability, fairness (i.e., human bias), and safety frequently receive insufficient attention. This paper posits that a comprehe
    
[^78]: 通过嵌入式GPU实现的轻量级立体匹配系统-StereoVAE

    StereoVAE: A lightweight stereo matching system through embedded GPUs. (arXiv:2305.11566v1 [cs.CV])

    [http://arxiv.org/abs/2305.11566](http://arxiv.org/abs/2305.11566)

    本论文提出了通过嵌入式GPU实现的轻量级立体匹配系统-StereoVAE，该系统采用基于VAE的小型神经网络对传统匹配方法生成的小尺寸粗糙视差图进行上采样与细化，达到了提高匹配精度和保证实时处理的目的。

    

    本论文提出了通过嵌入式GPU实现的轻量级立体匹配系统-StereoVAE，它打破了立体匹配中精度和处理速度之间的平衡，使得我们的嵌入式系统能够在保证实时处理的同时进一步提高匹配精度。我们的方法的主要思想是构建一个基于变分自编码器（VAE）的小型神经网络，对传统匹配方法生成的小尺寸粗糙视差图进行上采样与细化。这种混合结构不仅可以带来传统方法的计算复杂度优势，还可以保证神经网络的影响下的匹配精度。对KITTI 2015基准测试的广泛实验表明，我们的轻量级立体匹配系统在提高由不同算法生成的粗糙视差图的准确性方面表现出高鲁棒性，同时在嵌入式GPU上实时运行。

    We present a lightweight system for stereo matching through embedded GPUs. It breaks the trade-off between accuracy and processing speed in stereo matching, enabling our embedded system to further improve the matching accuracy while ensuring real-time processing. The main idea of our method is to construct a tiny neural network based on variational auto-encoder (VAE) to upsample and refinement a small size of coarse disparity map, which is first generated by a traditional matching method. The proposed hybrid structure cannot only bring the advantage of traditional methods in terms of computational complexity, but also ensure the matching accuracy under the impact of neural network. Extensive experiments on the KITTI 2015 benchmark demonstrate that our tiny system exhibits high robustness in improving the accuracy of the coarse disparity maps generated by different algorithms, while also running in real-time on embedded GPUs.
    
[^79]: 基于自监督调整的零样本文本分类算法

    Zero-Shot Text Classification via Self-Supervised Tuning. (arXiv:2305.11442v1 [cs.CL])

    [http://arxiv.org/abs/2305.11442](http://arxiv.org/abs/2305.11442)

    本文提出了一种基于自监督调整的零样本文本分类算法，通过使用无标签数据来调整语言模型，通过学习预测段落中的第一句话，实现了对未见过任务的零样本推断，模型不需要注释数据进行元调整，对模板的选择不敏感，并在实验中取得不错的结果。

    

    现有的零样本文本分类方法要么使用预训练语言模型进行提示，但这种方法对模板的选择非常敏感；要么依赖于大量相关任务的注释数据进行元调整。本文提出了一种基于自监督学习的新范式，通过使用无标签数据来调整语言模型，称为自监督调整。通过探索自由文本的内在结构，我们提出了一种新的学习目标，称为首句预测，以弥合无标签数据和文本分类任务之间的差距。调整模型以学习根据剩余文本来预测段落中的第一句话后，该模型能够推断出未见过的任务，如主题分类和情感分析。实验结果表明，我们的模型在10个任务中的7个任务上优于现有基准线。此外，分析表明，我们的模型对模板的选择不敏感，并且不需要注释数据进行元调整。

    Existing solutions to zero-shot text classification either conduct prompting with pre-trained language models, which is sensitive to the choices of templates, or rely on large-scale annotated data of relevant tasks for meta-tuning. In this work, we propose a new paradigm based on self-supervised learning to solve zero-shot text classification tasks by tuning the language models with unlabeled data, called self-supervised tuning. By exploring the inherent structure of free texts, we propose a new learning objective called first sentence prediction to bridge the gap between unlabeled data and text classification tasks. After tuning the model to learn to predict the first sentence in a paragraph based on the rest, the model is able to conduct zero-shot inference on unseen tasks such as topic classification and sentiment analysis. Experimental results show that our model outperforms the state-of-the-art baselines on 7 out of 10 tasks. Moreover, the analysis reveals that our model is less s
    
[^80]: UniControl：统一的扩散模型用于可控的生动图像生成

    UniControl: A Unified Diffusion Model for Controllable Visual Generation In the Wild. (arXiv:2305.11147v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2305.11147](http://arxiv.org/abs/2305.11147)

    本文提出了UniControl，一种新的生成基础模型，能够整合广泛的可控条件与图像任务，并仍然允许任意语言提示，UniControl能够进行像素级精确的图像生成，其中视觉条件主要影响生成的结构，语言提示则引导样式和上下文。我们提出了一种统一的扩散模型，它将扩散过程和变分自动编码器的优点结合起来，实验结果表明UniControl在可控性和图像质量方面均优于现有的最先进模型。

    

    实现机器自治和人类控制往往代表着相互矛盾的目标，特别是在交互式人工智能系统的设计中。现实中的可控视觉生成模型（如稳定扩散）在使用任意语言时具有良好的表现，但在生成具有空间、结构或几何控制的图像方面往往表现不佳。本文提出了UniControl，这是一种新的生成基础模型，它将广泛的可控条件与图像（C2I）任务整合到一个单一的框架中，同时仍允许任意语言提示。UniControl能够进行像素级精确的图像生成，其中视觉条件主要影响生成的结构，语言提示则引导样式和上下文。为了使UniControl具备处理各种视觉条件的能力，我们提出了一种统一的扩散模型，它将扩散过程和变分自动编码器的优点结合起来。实验结果表明，我们提出的UniControl在可控性和图像质量方面均优于现有的最先进模型。

    Achieving machine autonomy and human control often represent divergent objectives in the design of interactive AI systems. Visual generative foundation models such as Stable Diffusion show promise in navigating these goals, especially when prompted with arbitrary languages. However, they often fall short in generating images with spatial, structural, or geometric controls. The integration of such controls, which can accommodate various visual conditions in a single unified model, remains an unaddressed challenge. In response, we introduce UniControl, a new generative foundation model that consolidates a wide array of controllable condition-to-image (C2I) tasks within a singular framework, while still allowing for arbitrary language prompts. UniControl enables pixel-level-precise image generation, where visual conditions primarily influence the generated structures and language prompts guide the style and context. To equip UniControl with the capacity to handle diverse visual conditions
    
[^81]: 融合项目相关性的序列推荐系统训练损失函数

    Integrating Item Relevance in Training Loss for Sequential Recommender Systems. (arXiv:2305.10824v1 [cs.IR])

    [http://arxiv.org/abs/2305.10824](http://arxiv.org/abs/2305.10824)

    本文提出了一种融合项目相关性的新型训练损失函数，用于提高序列推荐系统对噪声的鲁棒性和性能。

    

    序列推荐系统是一种受欢迎的推荐系统，它通过学习用户的历史数据来预测用户下一个可能与之交互的项目。然而，用户的交互可能会受到来自帐户共享、不一致的偏好或意外点击等噪声的影响。为了解决这个问题，我们（i）提出了一个考虑多个未来项目的新的评估协议，（ii）引入了一种新的关注相关性的损失函数，用于训练具有多个未来项目的序列推荐系统，以使其对噪声更加鲁棒。我们的关注相关性模型在传统评估协议中提高了NDCG@10约1.2%和HR约0.88%，而在新评估协议中，改进的NDCG@10约1.63%和HR约1.5%。

    Sequential Recommender Systems (SRSs) are a popular type of recommender system that learns from a user's history to predict the next item they are likely to interact with. However, user interactions can be affected by noise stemming from account sharing, inconsistent preferences, or accidental clicks. To address this issue, we (i) propose a new evaluation protocol that takes multiple future items into account and (ii) introduce a novel relevance-aware loss function to train a SRS with multiple future items to make it more robust to noise. Our relevance-aware models obtain an improvement of ~1.2% of NDCG@10 and 0.88% in the traditional evaluation protocol, while in the new evaluation protocol, the improvement is ~1.63% of NDCG@10 and ~1.5% of HR w.r.t the best performing models.
    
[^82]: 评估LLM的隐藏风险：关于鲁棒性、一致性和可信性的实证研究

    Assessing Hidden Risks of LLMs: An Empirical Study on Robustness, Consistency, and Credibility. (arXiv:2305.10235v1 [cs.LG])

    [http://arxiv.org/abs/2305.10235](http://arxiv.org/abs/2305.10235)

    本研究是一项关于大型语言模型方面的实证研究，对主流语言模型进行了大量查询和分析，结果发现这些模型存在着鲁棒性、一致性和可信性方面的潜在风险。

    

    大型语言模型（LLMs）的普及对于许多领域产生了重大影响，特别是在其开放式环境（如API、开源模型和插件）中。然而，随着LLMs的广泛部署，缺乏全面讨论和分析潜在风险的研究。因此，我们进行了一项初步但开创性的研究，涵盖了LLMs系统的鲁棒性、一致性和可信性。我们提出了一个自动化工作流程来处理大量查询/响应。总体而言，我们对包括ChatGPT、LLaMA和OPT在内的主流LLMs进行了100多万个查询。我们的工作流核心包括数据原语，随后是自动解释器，评估这些LLMs在不同的对抗性度量系统下的表现。结果，我们得出了几个、也许是不幸的结论，这些结论相当不同

    The recent popularity of large language models (LLMs) has brought a significant impact to boundless fields, particularly through their open-ended ecosystem such as the APIs, open-sourced models, and plugins. However, with their widespread deployment, there is a general lack of research that thoroughly discusses and analyzes the potential risks concealed. In that case, we intend to conduct a preliminary but pioneering study covering the robustness, consistency, and credibility of LLMs systems. With most of the related literature in the era of LLM uncharted, we propose an automated workflow that copes with an upscaled number of queries/responses. Overall, we conduct over a million queries to the mainstream LLMs including ChatGPT, LLaMA, and OPT. Core to our workflow consists of a data primitive, followed by an automated interpreter that evaluates these LLMs under different adversarial metrical systems. As a result, we draw several, and perhaps unfortunate, conclusions that are quite unco
    
[^83]: AAAI 2022 秋季研讨会：基于认知通用模型实现的 System-1 和 System-2

    AAAI 2022 Fall Symposium: System-1 and System-2 realized within the Common Model of Cognition. (arXiv:2305.09091v1 [cs.AI])

    [http://arxiv.org/abs/2305.09091](http://arxiv.org/abs/2305.09091)

    该论文提出了一个基于认知通用模型的 System-1 和 System-2 实现方法，解决了其区分不清的问题。通用模型提供了对涉及 System-1 和 System-2 的计算单元、基础机制和其对学习、元认知和情感的影响的全面视野。

    

    AI 领域中对 System-1 和 System-2 二元模型的引入一直受到其区分不清的阻碍。我们通过将 System-1 和 System-2 置于认知通用模型中来解决此问题和其他问题。结果显示，被认为是 System-1 和 2 特征的不同之处实际上形成了一种认知属性的范围。通用模型提供了有关涉及 System-1 和 System-2 的计算单元、其基础机制以及对学习、元认知和情感的影响的全面视野。

    Attempts to import dual-system descriptions of System-1 and System-2 into AI have been hindered by a lack of clarity over their distinction. We address this and other issues by situating System-1 and System-2 within the Common Model of Cognition. Results show that what are thought to be distinctive characteristics of System-1 and 2 instead form a spectrum of cognitive properties. The Common Model provides a comprehensive vision of the computational units involved in System-1 and System-2, their underlying mechanisms, and the implications for learning, metacognition, and emotion.
    
[^84]: sustain.AI: 一种分析可持续性报告的推荐系统

    sustain.AI: a Recommender System to analyze Sustainability Reports. (arXiv:2305.08711v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.08711](http://arxiv.org/abs/2305.08711)

    sustain.AI是一个智能的、上下文感知的推荐系统，可以帮助审计师、金融投资者以及广大公众高效地分析公司的可持续性报告，并通过与GRI标准匹配来提供更好的推荐精度。

    

    本文介绍了sustain.AI，这是一个智能的、上下文感知的推荐系统，可帮助审计师、金融投资者以及广大公众高效地分析公司的可持续性报告。该工具利用了端到端可训练的架构，将基于BERT的编码模块与多标签分类头相结合，将可持续性报告中的相关文本段落与全球报告倡议（GRI）标准中的相应法律法规匹配。我们在两个新颖的德国可持续性报告数据集上评估了我们的模型，并始终实现了与多个强基线模型相比更高的推荐性能。此外，sustain.AI已经公开在https://sustain.ki.nrw/上提供给所有人使用。

    We present $\text{sustain.AI}$, an intelligent, context-aware recommender system that assists auditors and financial investors as well as the general public to efficiently analyze companies' sustainability reports. The tool leverages an end-to-end trainable architecture that couples a BERT-based encoding module with a multi-label classification head to match relevant text passages from sustainability reports to their respective law regulations from the Global Reporting Initiative (GRI) standards. We evaluate our model on two novel German sustainability reporting data sets and consistently achieve a significantly higher recommendation performance compared to multiple strong baselines. Furthermore, $\text{sustain.AI}$ is publicly available for everyone at https://sustain.ki.nrw/.
    
[^85]: 使用LLM辅助注释进行语料库语言学研究：本地语法分析案例研究

    Using LLM-assisted Annotation for Corpus Linguistics: A Case Study of Local Grammar Analysis. (arXiv:2305.08339v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.08339](http://arxiv.org/abs/2305.08339)

    本文研究了使用基于大语言模型的聊天机器人自动标注文本的潜力，重点考察了从本地语法角度观察道歉言语行为构成的功能元素的程度，并比较了不同模型在注释任务中的表现，结果表明Bing聊天机器人在任务中表现优于ChatGPT和人类标注员。

    

    基于大语言模型（LLMs）的聊天机器人在语言理解方面表现出很强的能力。本研究探索LLMs在协助基于语料库的语言学研究方面的潜力，通过将文本自动标注为特定语言信息类别。具体而言，我们研究了从本地语法的角度观察道歉言语行为构成的功能元素的程度，通过比较基于GPT-3.5的ChatGPT、基于GPT-4的Bing聊天机器人和人类编码器在注释任务中的表现。结果表明，Bing聊天机器人在任务中表现显着优于ChatGPT。与人类标注员相比，Bing聊天机器人的整体表现略低于人类标注员的表现，但已经取得了较高的F1得分:道歉标记99.95％，原因标记91.91％，道歉者标记95.35％，被道歉者标记89.74％和加强标记96.47％。这表明，在语言类别清晰且可以轻松识别的情况下，使用LLM辅助注释进行语料库语言学研究是可行的。

    Chatbots based on Large Language Models (LLMs) have shown strong capabilities in language understanding. In this study, we explore the potential of LLMs in assisting corpus-based linguistic studies through automatic annotation of texts with specific categories of linguistic information. Specifically, we examined to what extent LLMs understand the functional elements constituting the speech act of apology from a local grammar perspective, by comparing the performance of ChatGPT (powered by GPT-3.5), the Bing chatbot (powered by GPT-4), and a human coder in the annotation task. The results demonstrate that the Bing chatbot significantly outperformed ChatGPT in the task. Compared to human annotator, the overall performance of the Bing chatbot was slightly less satisfactory. However, it already achieved high F1 scores: 99.95% for the tag of APOLOGISING, 91.91% for REASON, 95.35% for APOLOGISER, 89.74% for APOLOGISEE, and 96.47% for INTENSIFIER. This suggests that it is feasible to use LLM-
    
[^86]: TinyStories: 语言模型能简小到什么程度却依然能够讲述连贯的英文故事？

    TinyStories: How Small Can Language Models Be and Still Speak Coherent English?. (arXiv:2305.07759v1 [cs.CL])

    [http://arxiv.org/abs/2305.07759](http://arxiv.org/abs/2305.07759)

    本文针对小型语言模型生成连贯的英文文本难题，引入了一个合成故事数据集 TinyStories，并探索小型模型规模、结构复杂度和训练数据规模对于语言模型表现的影响，证明了仅含 200 万参数的简单语言模型也能产生连贯的短故事。

    

    语言模型是自然语言处理中强大的工具，但在小型化时经常难以产生连贯和流畅的文本。本文引入了一个名为 TinyStories 的合成故事数据集，用于训练和评估规模小、复杂度低的语言模型对于短故事的生成能力。

    Language models (LMs) are powerful tools for natural language processing, but they often struggle to produce coherent and fluent text when they are small. Models with around 125M parameters such as GPT-Neo (small) or GPT-2 (small) can rarely generate coherent and consistent English text beyond a few words even after extensive training. This raises the question of whether the emergence of the ability to produce coherent English text only occurs at larger scales (with hundreds of millions of parameters or more) and complex architectures (with many layers of global attention).  In this work, we introduce TinyStories, a synthetic dataset of short stories that only contain words that a typical 3 to 4-year-olds usually understand, generated by GPT-3.5 and GPT-4. We show that TinyStories can be used to train and evaluate LMs that are much smaller than the state-of-the-art models (below 10 million total parameters), or have much simpler architectures (with only one transformer block), yet stil
    
[^87]: SemEval-2023任务2：细粒度多语言命名实体识别（MultiCoNER 2）

    SemEval-2023 Task 2: Fine-grained Multilingual Named Entity Recognition (MultiCoNER 2). (arXiv:2305.06586v1 [cs.CL])

    [http://arxiv.org/abs/2305.06586](http://arxiv.org/abs/2305.06586)

    该论文介绍了SemEval-2023 Task 2的研究发现，任务旨在通过使用MultiCoNER V2数据集，识别12种语言中复杂的细粒度命名实体。最优方法是将外部知识融入transformer模型，最具挑战性的是媒体标题和产品名称等实体类型。

    

    我们介绍了SemEval-2023任务2关于细粒度多语言命名实体识别（MultiCoNER 2）的研究发现。该任务分为13个轨道，重点关注12种语言和单语、多语和嘈杂环境下识别复杂的细粒度命名实体（如WRITTENWORK、VEHICLE、MUSICALGRP）的方法。任务使用MultiCoNER V2数据集，该数据集由Bangla、Chinese、English、Farsi、French、German、Hindi、Italian、Portuguese、Spanish、Swedish和Ukrainian组成，共有220万个实例。MultiCoNER 2是SemEval-2023中最受欢迎的任务之一，吸引了47个队伍提交842个结果，其中34个队伍提交了系统论文。结果表明，媒体标题和产品名称等复杂实体类型是最具挑战性的，将外部知识融入transformer模型的方法取得了最佳性能，并且在Creative Work和Group类别上获得了最大增益，即使使用外部知识仍然具有挑战性。

    We present the findings of SemEval-2023 Task 2 on Fine-grained Multilingual Named Entity Recognition (MultiCoNER 2). Divided into 13 tracks, the task focused on methods to identify complex fine-grained named entities (like WRITTENWORK, VEHICLE, MUSICALGRP) across 12 languages, in both monolingual and multilingual scenarios, as well as noisy settings. The task used the MultiCoNER V2 dataset, composed of 2.2 million instances in Bangla, Chinese, English, Farsi, French, German, Hindi, Italian., Portuguese, Spanish, Swedish, and Ukrainian. MultiCoNER 2 was one of the most popular tasks of SemEval-2023. It attracted 842 submissions from 47 teams, and 34 teams submitted system papers. Results showed that complex entity types such as media titles and product names were the most challenging. Methods fusing external knowledge into transformer models achieved the best performance, and the largest gains were on the Creative Work and Group classes, which are still challenging even with external kn
    
[^88]: 带深度划分的多提示模态交叉学习

    Multi-Prompt with Depth Partitioned Cross-Modal Learning. (arXiv:2305.06221v1 [cs.CV])

    [http://arxiv.org/abs/2305.06221](http://arxiv.org/abs/2305.06221)

    本研究提出了划分的多模态提示（PMPO）方法，将软提示从一个扩展到多个，通过连接多个提示到视觉编码器的不同深度上，能够更好地捕捉视觉表示的上下文深度，与传统单提示方法相比，在下游视觉语言任务中具有更好的表现。

    

    近年来，软提示学习方法被提出来微调大型视觉-语言预训练模型以完成各种下游任务。这些方法通常将可学习的文本标记与类别标记组合作为模型的输入，其中模型的参数被冻结。然而，它们经常使用单一提示来描述类别上下文，而不能充分捕捉类别的多样属性。本研究介绍了划分的多模态提示（PMPO），这是一种多模态提示技术，将软提示从一个可学习提示扩展到多个提示。我们的方法将视觉编码器深度进行分割，并将可学习提示连接到分离的视觉深度上，使不同提示能够捕捉视觉表示的层次上下文深度。此外，为了最大限度地利用多提示学习的优势，我们将来自手动设计的模板和可学习的多提示的先验信息结合在一起，从而提高了模型的泛化能力。

    In recent years, soft prompt learning methods have been proposed to fine-tune large-scale vision-language pre-trained models for various downstream tasks. These methods typically combine learnable textual tokens with class tokens as input for models with frozen parameters. However, they often employ a single prompt to describe class contexts, failing to capture categories' diverse attributes adequately. This study introduces the Partitioned Multi-modal Prompt (PMPO), a multi-modal prompting technique that extends the soft prompt from a single learnable prompt to multiple prompts. Our method divides the visual encoder depths and connects learnable prompts to the separated visual depths, enabling different prompts to capture the hierarchical contextual depths of visual representations. Furthermore, to maximize the advantages of multi-prompt learning, we incorporate prior information from manually designed templates and learnable multi-prompts, thus improving the generalization capabiliti
    
[^89]: 如何影响上下文范例在组合通用性中的作用？

    How Do In-Context Examples Affect Compositional Generalization?. (arXiv:2305.04835v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.04835](http://arxiv.org/abs/2305.04835)

    本文提出了CoFe测试套件来调查上下文组合泛化。实验结果表明，上下文示例应该在结构上与测试用例类似，相互之间应该不同，而且单独地简单。

    

    组合泛化——理解看不见的已知原始组合——是人类智能中的一个重要推理能力。AI社区主要通过在许多训练样本上微调神经网络来研究这种能力，然而还不清楚上下文学习——基于大型语言模型的主要少样本范式——是否展示组合泛化。在本文中，我们提出了CoFe，一个测试套件来调查上下文组合泛化。我们发现，组合泛化性能很容易受到上下文示例选择的影响，因此提出了研究问题：什么是在组合泛化中制作好的上下文示例的关键因素。我们研究了三个潜在因素：相似性、多样性和复杂性。我们的系统实验表明，在组合通用性中，上下文示例应该在结构上与测试用例类似，相互之间应该不同，而且单独地简单。

    Compositional generalization--understanding unseen combinations of seen primitives--is an essential reasoning capability in human intelligence. The AI community mainly studies this capability by fine-tuning neural networks on lots of training samples, while it is still unclear whether and how in-context learning--the prevailing few-shot paradigm based on large language models--exhibits compositional generalization. In this paper, we present CoFe, a test suite to investigate in-context compositional generalization. We find that the compositional generalization performance can be easily affected by the selection of in-context examples, thus raising the research question what the key factors are to make good in-context examples for compositional generalization. We study three potential factors: similarity, diversity and complexity. Our systematic experiments indicate that in-context examples should be structurally similar to the test case, diverse from each other, and individually simple.
    
[^90]: GPT用于半自动化数据科学：引入CAAFE实现上下文感知自动特征工程

    GPT for Semi-Automated Data Science: Introducing CAAFE for Context-Aware Automated Feature Engineering. (arXiv:2305.03403v1 [cs.AI])

    [http://arxiv.org/abs/2305.03403](http://arxiv.org/abs/2305.03403)

    介绍了一种名为CAAFE的上下文感知自动特征工程方法，它利用大型语言模型根据数据集描述生成更多具有语义意义的特征，能够提高大多数数据集的性能，平均ROC AUC表现提高至0.822。

    

    随着自动化机器学习（AutoML）领域的发展，将领域知识纳入这些系统中变得越来越重要。我们利用大型语言模型（LLMs）的强大功能提出了一种方法来实现这一目标。具体地，我们介绍了一种用于表格数据的特征工程方法，名为上下文感知自动特征工程（CAAFE），它利用LLM根据数据集的描述生成更多具有语义意义的特征。该方法产生用于创建新特征的Python代码，并提供生成特征的效用说明。尽管方法论上很简单，但CAAFE提高了14个数据集中11个数据集的性能，与2个数据集并列，只有1个数据集性能下降，从而使所有数据集的平均ROC AUC表现从0.798提升至0.822。对于所评估的数据集，这一改进与使用随机森林（AUC 0.782）代替逻辑回归（AUC 0.754）所获得的平均改进相似。此外，

    As the field of automated machine learning (AutoML) advances, it becomes increasingly important to include domain knowledge within these systems. We present an approach for doing so by harnessing the power of large language models (LLMs). Specifically, we introduce Context-Aware Automated Feature Engineering (CAAFE), a feature engineering method for tabular datasets that utilizes an LLM to generate additional semantically meaningful features for tabular datasets based on their descriptions. The method produces both Python code for creating new features and explanations for the utility of the generated features.  Despite being methodologically simple, CAAFE enhances performance on 11 out of 14 datasets, ties on 2 and looses on 1 - boosting mean ROC AUC performance from 0.798 to 0.822 across all datasets. On the evaluated datasets, this improvement is similar to the average improvement achieved by using a random forest (AUC 0.782) instead of logistic regression (AUC 0.754).  Furthermore,
    
[^91]: 小样本事件检测：经验研究和统一视角

    Few-shot Event Detection: An Empirical Study and a Unified View. (arXiv:2305.01901v1 [cs.CL])

    [http://arxiv.org/abs/2305.01901](http://arxiv.org/abs/2305.01901)

    本文从两个实用的设置出发，分析比较了十种代表性的小样本事件检测方法，归纳总结出了原型方法的性能优越性，并在此基础上提出了一种简单且有效的方法。

    

    小样本事件检测 (ED) 已经被广泛研究，然而这也带来了明显的差异，例如各种动机、任务和实验设置，这些差异妨碍了模型的理解和未来进展。本文提出了一项彻底的经验研究、一个ED模型的统一视角和一个更好的统一基准线。为了公平评估，我们选择了两个实用的设置：低资源设置来评估泛化能力和类转移设置来评估可转移性。我们比较了三个数据集上的十种代表性方法，这些方法大致被分为基于提示和基于原型的模型进行详细分析。为了调查基于原型方法的优越性能，我们分解了设计并建立了一个统一框架。基于此，我们不仅提出了一种简单而有效的方法（例如在低资源设置下获得2.7％F1收益），而且为未来的研究提供了许多有价值的研究见解。

    Few-shot event detection (ED) has been widely studied, while this brings noticeable discrepancies, e.g., various motivations, tasks, and experimental settings, that hinder the understanding of models for future progress. This paper presents a thorough empirical study, a unified view of ED models, and a better unified baseline. For fair evaluation, we choose two practical settings: low-resource setting to assess generalization ability and class-transfer setting for transferability. We compare ten representative methods on three datasets, which are roughly grouped into prompt-based and prototype-based models for detailed analysis. To investigate the superior performance of prototype-based methods, we break down the design and build a unified framework. Based on that, we not only propose a simple yet effective method (e.g., 2.7% F1 gains under low-resource setting) but also offer many valuable research insights for future research.
    
[^92]: 使用大型语言模型在知识图谱上进行复杂逻辑推理

    Complex Logical Reasoning over Knowledge Graphs using Large Language Models. (arXiv:2305.01157v1 [cs.LO])

    [http://arxiv.org/abs/2305.01157](http://arxiv.org/abs/2305.01157)

    本文提出了一种使用大型语言模型的解耦方法，将复杂的知识图谱推理形式化为上下文知识图搜索和抽象逻辑查询推理的组合，与现有方法相比，它在多个逻辑查询结构的标准基准数据集上都表现出更好的性能，并且在更高复杂性的查询中获得了显着的性能提升。

    

    在知识图谱上进行推理是一项具有挑战性的任务，它需要对实体之间的复杂关系以及它们之间的基础逻辑进行深入理解。当前的方法依赖于学习几何来嵌入实体的向量空间进行逻辑查询操作，但是它们在复杂查询和特定数据集表示方面表现不佳。本文提出了一种新颖的解耦方法，称为基于语言引导的知识图谱抽象推理（LARK），将复杂的知识图谱推理形式化为上下文知识图搜索和抽象逻辑查询推理的组合，以分别利用图形提取算法和大型语言模型的优势。我们的实验表明，所提出的方法在多个逻辑查询结构的标准基准数据集上优于现有的知识图谱推理方法，在更高复杂性的查询中获得了显着的性能提升。

    Reasoning over knowledge graphs (KGs) is a challenging task that requires a deep understanding of the complex relationships between entities and the underlying logic of their relations. Current approaches rely on learning geometries to embed entities in vector space for logical query operations, but they suffer from subpar performance on complex queries and dataset-specific representations. In this paper, we propose a novel decoupled approach, Language-guided Abstract Reasoning over Knowledge graphs (LARK), that formulates complex KG reasoning as a combination of contextual KG search and abstract logical query reasoning, to leverage the strengths of graph extraction algorithms and large language models (LLM), respectively. Our experiments demonstrate that the proposed approach outperforms state-of-the-art KG reasoning methods on standard benchmark datasets across several logical query constructs, with significant performance gain for queries of higher complexity. Furthermore, we show t
    
[^93]: 基于游戏的人工智能研究平台

    Game-based Platforms for Artificial Intelligence Research. (arXiv:2304.13269v1 [cs.AI])

    [http://arxiv.org/abs/2304.13269](http://arxiv.org/abs/2304.13269)

    本文回顾了基于游戏的人工智能研究平台，讨论了不同研究领域和创意设计在其中的应用和发展，并探讨了其未来趋势。

    

    游戏具有现实世界场景的广泛特征，成为了人工智能研究的理想测试基地，共同的研究领域包括学习和优化、动态和不确定环境下的决策制定、博弈论、计划与排程、设计和教育等。已实施了许多开源游戏或基于游戏的环境用于研究人工智能。除了单人或多人、合作或对抗性游戏外，在创意设计方面也越来越受到关注。这些平台为探索和比较人工智能的思想和技术提供了理想基准。本文回顾了基于游戏的人工智能研究平台，讨论了由这些平台演变引起的研究趋势，并展望了未来。

    Games have been the perfect test-beds for artificial intelligence research for the characteristics that widely exist in real-world scenarios. Learning and optimisation, decision making in dynamic and uncertain environments, game theory, planning and scheduling, design and education are common research areas shared between games and real-world problems. Numerous open-sourced games or game-based environments have been implemented for studying artificial intelligence. In addition to single- or multi-player, collaborative or adversarial games, there has also been growing interest in implementing platforms for creative design in recent years. Those platforms provide ideal benchmarks for exploring and comparing artificial intelligence ideas and techniques. This paper reviews the game-based platforms for artificial intelligence research, discusses the research trend induced by the evolution of those platforms, and gives an outlook.
    
[^94]: 一个由经典随机游走和量子游走驱动的赌博算法

    Bandit Algorithm Driven by a Classical Random Walk and a Quantum Walk. (arXiv:2304.10118v1 [quant-ph])

    [http://arxiv.org/abs/2304.10118](http://arxiv.org/abs/2304.10118)

    本文提出了一个基于量子游走的算法解决多臂赌博问题，将探索和利用与量子游走行为联系起来，相比于经典随机游走策略实现了高性能。

    

    量子游走具有经典随机游走所不具备的属性——线性传播和局部化共存——并且这种属性被用于实现各种应用。本文提出了一种基于量子游走的多臂赌博问题算法，将使赌博问题困难的两种操作——探索和利用——与这两种量子游走行为联系起来。我们展示了这种基于量子游走的新策略相比于相应的随机游走策略实现了高性能。

    Quantum walks (QWs) have the property that classical random walks (RWs) do not possess -- coexistence of linear spreading and localization -- and this property is utilized to implement various kinds of applications. This paper proposes a quantum-walk-based algorithm for multi-armed-bandit (MAB) problems by associating the two operations that make MAB problems difficult -exploration and exploitation -- with these two behaviors of QWs. We show that this new policy based on the QWs realizes high performance compared with the corresponding RW-based one.
    
[^95]: 强化学习中基于Bandit方法的显式塑形外部建议算法的研究

    Bandit-Based Policy Invariant Explicit Shaping for Incorporating External Advice in Reinforcement Learning. (arXiv:2304.07163v1 [cs.AI])

    [http://arxiv.org/abs/2304.07163](http://arxiv.org/abs/2304.07163)

    本文研究了如何基于Bandit方法将外部建议融入到强化学习中，并提出了三种不同的塑形算法：UCB-PIES（UPIES）， Racing-PIES（RPIES）和Lazy PIES（LPIES）。实验结果表明这些算法在样本复杂度、学习速度和形状质量方面都取得了良好的效果。

    

    强化学习（RL）算法中的一个关键问题是如何将外部或专家的建议融入到学习当中。本文将将将此问题表述为一种多臂赌博机称为塑形赌博机（shaping-bandits）。我们提出了三种不同的塑形算法：UCB-PIES（UPIES）， Racing-PIES（RPIES）和Lazy PIES（LPIES）。通过在模拟环境和LQR和Atari环境中的实验，我们证明了这三种算法在样本复杂度、学习速度和形状质量方面的有效性。

    A key challenge for a reinforcement learning (RL) agent is to incorporate external/expert1 advice in its learning. The desired goals of an algorithm that can shape the learning of an RL agent with external advice include (a) maintaining policy invariance; (b) accelerating the learning of the agent; and (c) learning from arbitrary advice [3]. To address this challenge this paper formulates the problem of incorporating external advice in RL as a multi-armed bandit called shaping-bandits. The reward of each arm of shaping bandits corresponds to the return obtained by following the expert or by following a default RL algorithm learning on the true environment reward.We show that directly applying existing bandit and shaping algorithms that do not reason about the non-stationary nature of the underlying returns can lead to poor results. Thus we propose UCB-PIES (UPIES), Racing-PIES (RPIES), and Lazy PIES (LPIES) three different shaping algorithms built on different assumptions that reason a
    
[^96]: RAFT: 奖励排名微调用于生成型基础模型对齐

    RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment. (arXiv:2304.06767v1 [cs.LG])

    [http://arxiv.org/abs/2304.06767](http://arxiv.org/abs/2304.06767)

    RAFT框架引入了奖励排名微调方法，用于对齐生成型基础模型，以解决强化学习带来的低效和不稳定性问题。

    

    生成型基础模型容易受到广泛的无监督训练数据带来的隐式偏见的影响。这些偏见可能导致子优样本、扭曲的结果和不公平，可能产生重大影响。因此，将这些模型与人的伦理和偏好对齐是确保它们在真实应用中负责任和有效的部署的关键步骤。以往的研究主要采用人类反馈的强化学习（ RLHF）作为解决这个问题的手段。在 RL 算法的指导下，用人类反馈指导的奖励模型对生成模型进行微调。然而， RL 算法的低效性和不稳定性常常会对生成模型的成功对齐产生重大障碍，因此需要开发一种更为强大和简化的方法。为此，我们引入了一个新的框架，即奖励排名微调（ RAFT ），旨在对齐生成基础模型。

    Generative foundation models are susceptible to implicit biases that can arise from extensive unsupervised training data. Such biases can produce suboptimal samples, skewed outcomes, and unfairness, with potentially significant repercussions. Consequently, aligning these models with human ethics and preferences is an essential step toward ensuring their responsible and effective deployment in real-world applications. Prior research has primarily employed Reinforcement Learning from Human Feedback (RLHF) as a means of addressing this problem, wherein generative models are fine-tuned using RL algorithms guided by a human-feedback-informed reward model. However, the inefficiencies and instabilities associated with RL algorithms frequently present substantial obstacles to the successful alignment of generative models, necessitating the development of a more robust and streamlined approach. To this end, we introduce a new framework, Reward rAnked FineTuning (RAFT), designed to align generat
    
[^97]: LLMMaps——大型语言模型分层评价的可视化隐喻

    LLMMaps -- A Visual Metaphor for Stratified Evaluation of Large Language Models. (arXiv:2304.00457v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2304.00457](http://arxiv.org/abs/2304.00457)

    LLMMaps是一种分层评估大型语言模型性能的可视化技术，能够揭示取得高准确度和产生幻觉的子领域，并指导模型的进一步发展。

    

    大型语言模型(LLMs)在自然语言处理中取得了革命性的进展，并在各种任务中展示了惊人的能力。然而，它们容易产生幻觉，即模型在响应中暴露出不正确或错误的信息，这使得必须采用勤奋的评估方法。虽然LLM在特定知识领域中的表现通常是基于问答(Q&A)数据集进行评估，但这些评估通常仅报告整个领域的单个准确度数字，这一程序在透明度和模型改进方面存在问题。分层评估可以揭示可能更容易发生幻觉的子领域，从而有助于更好地评估LLMs的风险并指导它们的进一步发展。为支持这样的分层评估，我们提出了LLMMaps作为一种新的可视化技术，使用户能够根据Q&A数据集评估LLMs的性能。LLMMaps提供了对LLMs在不同子领域中的知识分布的详细洞察，允许用户放大领域的特定部分并探索模型性能上的差异。我们的实验证明，LLMMaps有助于识别出更容易出现LLM幻觉的子领域，并可以指导模型的发展，以改善这些领域的准确性。

    Large Language Models (LLMs) have revolutionized natural language processing and demonstrated impressive capabilities in various tasks. Unfortunately, they are prone to hallucinations, where the model exposes incorrect or false information in its responses, which renders diligent evaluation approaches mandatory. While LLM performance in specific knowledge fields is often evaluated based on question and answer (Q&A) datasets, such evaluations usually report only a single accuracy number for the entire field, a procedure which is problematic with respect to transparency and model improvement. A stratified evaluation could instead reveal subfields, where hallucinations are more likely to occur and thus help to better assess LLMs' risks and guide their further development. To support such stratified evaluations, we propose LLMMaps as a novel visualization technique that enables users to evaluate LLMs' performance with respect to Q&A datasets. LLMMaps provide detailed insights into LLMs' kn
    
[^98]: HuggingGPT: 在HugingFace中使用ChatGPT及其伙伴解决AI任务

    HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace. (arXiv:2303.17580v1 [cs.CL])

    [http://arxiv.org/abs/2303.17580](http://arxiv.org/abs/2303.17580)

    用ChatGPT作为任务规划工具，利用大型语言模型（LLM）作为控制器来整合现有的AI模型，解决复杂的AI任务。

    

    解决不同领域和模态的复杂AI任务是通向人工智能的关键步骤。本文提出了一个系统，利用大型语言模型（LLMs）作为控制器来管理现有的AI模型以解决AI任务，语言成为通用接口来赋能它。具体来说，我们使用ChatGPT作为任务规划工具，根据HuggingFace中可用的模型功能描述来选择模型，在选定AI模型的情况下执行每个子任务，并总结响应。

    Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence (AGI). While there are abundant AI models available for different domains and modalities, they cannot handle complicated AI tasks. Considering large language models (LLMs) have exhibited exceptional ability in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks and language could be a generic interface to empower this. Based on this philosophy, we present HuggingGPT, a system that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., HuggingFace) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in HuggingFace, execute each subtask with the selected AI model, and summarize the response acco
    
[^99]: ISimDL: 通过重要性采样驱动的故障注入模拟，加速深度学习强健性评估

    ISimDL: Importance Sampling-Driven Acceleration of Fault Injection Simulations for Evaluating the Robustness of Deep Learning. (arXiv:2303.08035v1 [cs.LG])

    [http://arxiv.org/abs/2303.08035](http://arxiv.org/abs/2303.08035)

    本论文提出了一种新方法ISimDL，利用神经元灵敏度生成重要性采样，加速故障注入模拟，有效评估了先进的DL系统对硬件故障的韧性，同时显著减少了所需的模拟数量。

    

    深度学习(DL)系统已在许多应用中广泛应用，需要专用的硬件加速器和芯片。在纳米时代，设备越来越容易受到永久性和瞬变故障的影响。因此，我们需要一种有效的方法来分析先进的DL系统对此类故障的韧性，并了解神经加速器芯片中的故障如何在DL应用级别上表现为错误，其中故障可能导致无法检测和恢复的错误。使用故障注入，我们可以通过在软件级别修改神经元权重和输出来执行DL系统的韧性研究，就好像硬件受到瞬变故障的影响一样。现有的故障模型减少了搜索空间，使分析更快，但需要该模型的先验知识，并且不允许进一步分析筛选出的搜索空间。因此，我们提出了ISimDL，一种新的方法，它利用神经元灵敏度生成重要性采样，并加速故障注入模拟。ISimDL可以有效评估先进的DL系统对硬件故障的韧性，而不会影响分析的准确性。所提出的方法显着减少了故障注入分析所需的模拟数量，同时仍确保足够覆盖搜索空间。我们将ISimDL应用于代表性的卷积神经网络，使用CIFAR-10和ImageNet数据集，并展示它提供显著的加速，同时仍保持与现有最先进故障注入方法相同的准确性水平。

    Deep Learning (DL) systems have proliferated in many applications, requiring specialized hardware accelerators and chips. In the nano-era, devices have become increasingly more susceptible to permanent and transient faults. Therefore, we need an efficient methodology for analyzing the resilience of advanced DL systems against such faults, and understand how the faults in neural accelerator chips manifest as errors at the DL application level, where faults can lead to undetectable and unrecoverable errors. Using fault injection, we can perform resilience investigations of the DL system by modifying neuron weights and outputs at the software-level, as if the hardware had been affected by a transient fault. Existing fault models reduce the search space, allowing faster analysis, but requiring a-priori knowledge on the model, and not allowing further analysis of the filtered-out search space. Therefore, we propose ISimDL, a novel methodology that employs neuron sensitivity to generate impo
    
[^100]: 用量子退火器解决现实世界中的装箱问题的混合方法

    Hybrid Approach for Solving Real-World Bin Packing Problem Instances Using Quantum Annealers. (arXiv:2303.01977v2 [cs.ET] UPDATED)

    [http://arxiv.org/abs/2303.01977](http://arxiv.org/abs/2303.01977)

    该研究提出了一个量子-经典混合框架，名为Q4RealBPP，可以考虑真实世界的限制特征，解决三维装箱问题，支持工业和物流行业的需求。

    

    高效地将物品装入箱子是一项常见的日常任务，也被称为“装箱问题”。由于行业和物流领域的广泛关注，它在人工智能领域得到了广泛研究。自几十年以来，许多变种已被提出，其中三维装箱问题是最接近实际用例的一个。我们介绍了一个混合的量子-经典框架，用于解决考虑不同现实特征的三维装箱问题（Q4RealBPP），例如：（i）包装和箱子尺寸，（ii）超重限制，（iii）物品类别之间的亲和力和（iv）物品排序的偏好。Q4RealBPP允许解决考虑到工业和物流部门广泛评价的限制的3dBPP的现实导向的实例。

    Efficient packing of items into bins is a common daily task. Known as Bin Packing Problem, it has been intensively studied in the field of artificial intelligence, thanks to the wide interest from industry and logistics. Since decades, many variants have been proposed, with the three-dimensional Bin Packing Problem as the closest one to real-world use cases. We introduce a hybrid quantum-classical framework for solving real-world three-dimensional Bin Packing Problems (Q4RealBPP), considering different realistic characteristics, such as: i) package and bin dimensions, ii) overweight restrictions, iii) affinities among item categories and iv) preferences for item ordering. Q4RealBPP permits the solving of real-world oriented instances of 3dBPP, contemplating restrictions well appreciated by industrial and logistics sectors.
    
[^101]: 基于模型的强化学习在能源市场清算和出价中的应用研究

    Approximating Energy Market Clearing and Bidding With Model-Based Reinforcement Learning. (arXiv:2303.01772v2 [eess.SY] UPDATED)

    [http://arxiv.org/abs/2303.01772](http://arxiv.org/abs/2303.01772)

    本文研究了基于模型的强化学习用于能源市场清算和出价的应用方法。通过用学习的OPF代理模型以及明确的市场规则替代传统计算方法，本方法极大地减少了训练时间并适用于市场设计和更现实地建模市场参与者。

    

    能源市场可能会为市场参与者的不良行为提供激励。多智能体强化学习是预测能源市场参与者预期行为的有前途的新方法。然而，强化学习需要许多与系统的交互才能收敛，而电力系统环境通常包括广泛的计算，例如用于市场清算的最优功率流量（OPF）计算。为了解决这个复杂性，我们提供了一个能源市场的模型给基本的MARL算法，这个模型采用了学习的OPF近似值和明确的市场规则。学习的OPF代理模型使得OPF的明确解决变得不必要。我们的实验表明，该模型还将训练时间降低了约一个数量级，但代价是略微更差的纳什均衡近似值。我们方法的潜在应用是市场设计，更现实地对市场参与者进行建模以及对市场动态的改进理解。

    Energy markets can provide incentives for undesired behavior of market participants. Multi-agent Reinforcement learning (MARL) is a promising new approach to predicting the expected behavior of energy market participants. However, reinforcement learning requires many interactions with the system to converge, and the power system environment often consists of extensive computations, e.g., optimal power flow (OPF) calculation for market clearing. To tackle this complexity, we provide a model of the energy market to a basic MARL algorithm in the form of a learned OPF approximation and explicit market rules. The learned OPF surrogate model makes an explicit solving of the OPF completely unnecessary. Our experiments demonstrate that the model additionally reduces training time by about one order of magnitude but at the cost of a slightly worse approximation of the Nash equilibrium. Potential applications of our method are market design, more realistic modeling of market participants, and an
    
[^102]: 一站式解决方案：利用预训练 LM 进行强大的时间序列分析

    One Fits All:Power General Time Series Analysis by Pretrained LM. (arXiv:2302.11939v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.11939](http://arxiv.org/abs/2302.11939)

    本论文提出了一种称为 Frozen Pretrained Transformer (FPT) 的预训练模型，利用从数十亿标记训练出来的语言或 CV 模型进行时间序列分析的所有主要类型任务的微调，进而使其在所有任务中都具备着最先进的性能和泛化能力。

    

    尽管预训练模型在自然语言处理 (NLP) 和计算机视觉 (CV) 领域取得了巨大成功，但在通用时间序列分析领域取得的进展有限。与 NLP 和 CV 不同的是，这些领域采用统一模型即可执行不同的任务，而在每个时间序列分析任务中，专门设计的方法仍然占据主导地位，如分类、异常检测、预测和少样本学习。阻碍预训练模型发展的主要挑战是缺乏大量用于训练的数据。在本文中，我们通过利用从数十亿标记训练出来的语言或 CV 模型，来解决这一挑战，用于时间序列分析。具体而言，我们避免改变预训练语言或图像模型中残差块中的自注意力和前向传递层。这种模型被称为冻结的预训练变压器 (FPT)，通过对涉及时间序列分析的所有主要类型的任务进行微调进行评估，包括分类、异常检测、预测和少样本学习等。实验结果证明，FPT 在所有任务中都具有最先进的性能和泛化能力。

    Although we have witnessed great success of pre-trained models in natural language processing (NLP) and computer vision (CV), limited progress has been made for general time series analysis. Unlike NLP and CV where a unified model can be used to perform different tasks, specially designed approach still dominates in each time series analysis task such as classification, anomaly detection, forecasting, and few-shot learning. The main challenge that blocks the development of pre-trained model for time series analysis is the lack of a large amount of data for training. In this work, we address this challenge by leveraging language or CV models, pre-trained from billions of tokens, for time series analysis. Specifically, we refrain from altering the self-attention and feedforward layers of the residual blocks in the pre-trained language or image model. This model, known as the Frozen Pretrained Transformer (FPT), is evaluated through fine-tuning on all major types of tasks involving time s
    
[^103]: 通过插值正交特征实现高效域自适应的Projec and Probe方法

    Project and Probe: Sample-Efficient Domain Adaptation by Interpolating Orthogonal Features. (arXiv:2302.05441v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.05441](http://arxiv.org/abs/2302.05441)

    本文提出了一种名为Projec and Probe（Pro$^2$）的方法，该方法能够通过插值学习多样化的特征来适应目标分布，极大地提高了使用少量目标数据进行迁移学习的效果。

    

    通过少量目标数据进行迁移学习是适应预训练模型到分布变化的一个有效且常见的方法。在某些情况下，目标数据标签可能很难获得，因此我们只能访问有限数量的目标数据点。为了最大化利用极小的目标数据集，本文提出了一种轻量、高效的方法，通过插值学习多样化的特征来适应目标分布。我们的方法，Projec and Probe（Pro$^2$），首先学习一个线性投影，将预训练嵌入映射到正交方向上，同时可预测源数据集中的标签。这一步的目标是学习多样化的预测特征，以便在分布变化后仍有一些特征是有用的。接着，Pro$^2$利用少量目标数据在这些投影特征之上学习一个线性分类器。理论上，我们发现Pro$^2$能够产生更高效的泛化效果。

    Transfer learning with a small amount of target data is an effective and common approach to adapting a pre-trained model to distribution shifts. In some situations, target data labels may be expensive to obtain, so we may only have access to a limited number of target data points. To make the most of a very small target dataset, we propose a lightweight, sample-efficient approach that learns a diverse set of features and adapts to a target distribution by interpolating these features. Our approach, Project and Probe (Pro$^2$), first learns a linear projection that maps a pre-trained embedding onto orthogonal directions while being predictive of labels in the source dataset. The goal of this step is to learn a variety of predictive features, so that at least some of them remain useful after distribution shift. Pro$^2$ then learns a linear classifier on top of these projected features using a small target dataset. Theoretically, we find that Pro$^2$ results in more sample-efficient gener
    
[^104]: 基于记忆的元学习在非平稳分布上的应用

    Memory-Based Meta-Learning on Non-Stationary Distributions. (arXiv:2302.03067v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.03067](http://arxiv.org/abs/2302.03067)

    本文研究基于记忆的元学习在非平稳分布上的应用，重点关注在部分可观察环境中的自然语言和动作-观察序列，研究表明各种类型的基于记忆的神经模型可以准确地逼近贝叶斯最优算法，并执行潜在参数的贝叶斯推断。

    

    基于记忆的元学习是一种逼近贝叶斯最优预测器的技术。在相当一般的条件下，通过最小化顺序预测误差（由对数损失度量）会导致隐式元学习。本文的目标是调查当前序列预测模型和训练方案能否实现这种解释的深度。重点关注具有未观察到的切换点的分段平稳源，很可能捕捉到部分可观察环境中的自然语言和动作-观察序列的一个重要特征。我们展示了各种类型的基于记忆的神经模型（包括变形金刚模型、LSTM和RNN）可以学习准确地逼近已知的贝叶斯最优算法，并表现得好像在每个段内对潜在切换点和控制数据分布的潜在参数执行贝叶斯推断。

    Memory-based meta-learning is a technique for approximating Bayes-optimal predictors. Under fairly general conditions, minimizing sequential prediction error, measured by the log loss, leads to implicit meta-learning. The goal of this work is to investigate how far this interpretation can be realized by current sequence prediction models and training regimes. The focus is on piecewise stationary sources with unobserved switching-points, which arguably capture an important characteristic of natural language and action-observation sequences in partially observable environments. We show that various types of memory-based neural models, including Transformers, LSTMs, and RNNs can learn to accurately approximate known Bayes-optimal algorithms and behave as if performing Bayesian inference over the latent switching-points and the latent parameters governing the data distribution within each segment.
    
[^105]: 一种普适性的玩具模型：逆向工程网络如何学习群操作

    A Toy Model of Universality: Reverse Engineering How Networks Learn Group Operations. (arXiv:2302.03025v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.03025](http://arxiv.org/abs/2302.03025)

    本文通过研究小型神经网络如何学习实现群组合来研究普适性假设。它提出了一种新的算法，使神经网络能够通过任意有限群来实现组合，从而完全描述网络在此任务上学习的电路和特征族。

    

    普适性是机械解释性中的一个关键假设--不同的模型在类似任务上训练时学习相似的特征和电路。本文通过研究小型神经网络如何学习实现群组合来研究普适性假设。我们通过数学表示论提出了一种新的算法，使神经网络能够通过任意有限群来实现组合。然后，我们通过逆向工程模型的逻辑和权重来展示网络始终学习此算法，并使用消融实验证实了我们的理解。通过研究训练在不同群上的不同架构的网络，我们发现普适性的证据不一：使用我们的算法，我们可以完全描述网络在此任务上学习的电路和特征族，但对于给定的网络，学习的精确电路以及它们的发展顺序是任意的。

    Universality is a key hypothesis in mechanistic interpretability -- that different models learn similar features and circuits when trained on similar tasks. In this work, we study the universality hypothesis by examining how small neural networks learn to implement group composition. We present a novel algorithm by which neural networks may implement composition for any finite group via mathematical representation theory. We then show that networks consistently learn this algorithm by reverse engineering model logits and weights, and confirm our understanding using ablations. By studying networks of differing architectures trained on various groups, we find mixed evidence for universality: using our algorithm, we can completely characterize the family of circuits and features that networks learn on this task, but for a given network the precise circuits learned -- as well as the order they develop -- are arbitrary.
    
[^106]: 最小化ODE生成模型轨迹的曲率

    Minimizing Trajectory Curvature of ODE-based Generative Models. (arXiv:2301.12003v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.12003](http://arxiv.org/abs/2301.12003)

    本论文提出通过训练正向过程以最小化生成轨迹的曲率，来优化ODE/SDE-based生成模型的采样速度，实验表明此方法有效。

    

    最近，基于ODE / SDE的生成模型，如扩散模型，矩形流和流匹配，将生成过程定义为固定正向过程的时间反演。尽管这些模型在大规模数据集上表现出令人印象深刻的性能，但数值模拟需要多次评估神经网络，导致采样速度缓慢。我们认为原因在于学习到的生成轨迹具有很高的曲率，因为它与数值求解器的截断误差直接相关。基于正向过程和曲率之间的关系，本文提出一种有效的方法，通过训练正向过程以最小化生成轨迹的曲率，而无需进行任何ODE / SDE模拟。实验表明，我们的方法实现了比先前模型更低的曲率，并因此降低了采样成本，同时保持了竞争性能。

    Recent ODE/SDE-based generative models, such as diffusion models, rectified flows, and flow matching, define a generative process as a time reversal of a fixed forward process. Even though these models show impressive performance on large-scale datasets, numerical simulation requires multiple evaluations of a neural network, leading to a slow sampling speed. We attribute the reason to the high curvature of the learned generative trajectories, as it is directly related to the truncation error of a numerical solver. Based on the relationship between the forward process and the curvature, here we present an efficient method of training the forward process to minimize the curvature of generative trajectories without any ODE/SDE simulation. Experiments show that our method achieves a lower curvature than previous models and, therefore, decreased sampling costs while maintaining competitive performance. Code is available at https://github.com/sangyun884/fast-ode.
    
[^107]: 选择性解释：利用人类输入对齐可解释人工智能

    Selective Explanations: Leveraging Human Input to Align Explainable AI. (arXiv:2301.09656v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2301.09656](http://arxiv.org/abs/2301.09656)

    本研究提出一种通过利用人类输入生成选择性解释的通用框架，以弥合可解释人工智能（XAI）与人类解释的差距，并且在决策支持任务中进行了实验证明其有效性。

    

    近年来，出现了大量的可解释人工智能（XAI）算法，但它们经常因与人类解释的生产和消费方式存在显著差距而受到批评。因此，目前的XAI技术往往难以使用并缺乏有效性。在本文中，我们尝试通过使AI解释具有选择性（这是人类解释的基本属性之一）来弥合这些差距，通过根据接收方的偏好有选择性地呈现大量模型原因的子集来实现。我们提出了一个通用的框架，通过利用小样本上的人类输入来生成选择性解释。该框架开辟了一个丰富的设计空间，涵盖了不同的选择性目标、输入类型等。作为一个展示，我们使用决策支持任务来探索基于决策者认为相关的选择性解释。我们进行了两项实验研究，以检查从大一组模型原因中选择的三个子集与未选择的子集相比，选择性解释的效果。

    While a vast collection of explainable AI (XAI) algorithms have been developed in recent years, they are often criticized for significant gaps with how humans produce and consume explanations. As a result, current XAI techniques are often found to be hard to use and lack effectiveness. In this work, we attempt to close these gaps by making AI explanations selective -- a fundamental property of human explanations -- by selectively presenting a subset from a large set of model reasons based on what aligns with the recipient's preferences. We propose a general framework for generating selective explanations by leveraging human input on a small sample. This framework opens up a rich design space that accounts for different selectivity goals, types of input, and more. As a showcase, we use a decision-support task to explore selective explanations based on what the decision-maker would consider relevant to the decision task. We conducted two experimental studies to examine three out of a bro
    
[^108]: 图嵌入学习的现状和潜力

    State of the Art and Potentialities of Graph-level Learning. (arXiv:2301.05860v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.05860](http://arxiv.org/abs/2301.05860)

    本文主要介绍了图嵌入学习，它将一组图作为输入，自动提取特征并将图编码为低维表示。深度学习的发展使得图嵌入学习适应了日益增长的图规模，从而获得了很多成功，但是传统学习方法通常会受到计算瓶颈的影响。

    

    图表现了关系数据的一种优越能力，如化合物、蛋白质和社交网络等。因此，图嵌入学习，将一组图作为输入，已应用于许多任务，包括比较、回归、分类等。传统的学习一组图的方法严重依赖于手工制作的特征，如亚结构。但这些方法虽然受益于良好的可解释性，但常常因无法避免图同构问题而受到计算瓶颈的影响。相反，深度学习通过自动提取特征和将图编码为低维表示，帮助图嵌入学习适应日益增长的图规模。因此，这些深度图学习方法已经造成了许多成功。然而，目前没有全面的综述来回顾从传统学习到深度学习方法的图嵌入学习。

    Graphs have a superior ability to represent relational data, like chemical compounds, proteins, and social networks. Hence, graph-level learning, which takes a set of graphs as input, has been applied to many tasks including comparison, regression, classification, and more. Traditional approaches to learning a set of graphs heavily rely on hand-crafted features, such as substructures. But while these methods benefit from good interpretability, they often suffer from computational bottlenecks as they cannot skirt the graph isomorphism problem. Conversely, deep learning has helped graph-level learning adapt to the growing scale of graphs by extracting features automatically and encoding graphs into low-dimensional representations. As a result, these deep graph learning methods have been responsible for many successes. Yet, there is no comprehensive survey that reviews graph-level learning starting with traditional learning and moving through to the deep learning approaches. This article 
    
[^109]: 连续对比微调改进低资源关系提取

    Continual Contrastive Finetuning Improves Low-Resource Relation Extraction. (arXiv:2212.10823v1 [cs.CL] CROSS LISTED)

    [http://arxiv.org/abs/2212.10823](http://arxiv.org/abs/2212.10823)

    本文提出了一种使用连续对比微调的方法来改进低资源关系提取，通过使用一致的对比学习目标预训练和微调RE模型，以及多中心对比损失来允许一个关系形成多个聚类。实验结果表明该方法可以显着提高低资源情况和领域中的关系提取性能。

    

    关系提取（RE）依赖结构化注释语料库进行模型训练，尤其在低资源情况和领域中，该任务具有挑战性。近期研究通过自监督学习来解决低资源的RE，其中解决方案包括通过RE目标预训练关系嵌入，并通过分类为基础的目标对有标签数据进行微调。然而，这种方法的一个关键挑战是目标之间的差距，它阻止RE模型充分利用预训练表示中的知识。本文旨在弥合差距，并提出使用一致的对比学习目标预训练和微调RE模型。由于在这种表示学习范式中，一个关系可能在表示空间中轻松形成多个聚类，因此我们进一步提出了多中心对比损失，允许一个关系形成多个聚类以更好地对齐预训练。在两个文档中的实验表明，所提出的方法可以在低资源情况和领域中显着提高关系提取性能。

    Relation extraction (RE), which has relied on structurally annotated corpora for model training, has been particularly challenging in low-resource scenarios and domains. Recent literature has tackled low-resource RE by self-supervised learning, where the solution involves pretraining the relation embedding by RE-based objective and finetuning on labeled data by classification-based objective. However, a critical challenge to this approach is the gap in objectives, which prevents the RE model from fully utilizing the knowledge in pretrained representations. In this paper, we aim at bridging the gap and propose to pretrain and finetune the RE model using consistent objectives of contrastive learning. Since in this kind of representation learning paradigm, one relation may easily form multiple clusters in the representation space, we further propose a multi-center contrastive loss that allows one relation to form multiple clusters to better align with pretraining. Experiments on two docum
    
[^110]: 语言模型是否具有日常物品的一致性心理模型？

    Do language models have coherent mental models of everyday things?. (arXiv:2212.10029v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.10029](http://arxiv.org/abs/2212.10029)

    语言模型缺乏对日常物品的一致性心理模型，会因此出现荒谬的解决方法。虽然最先进的预训练语言模型具有这些实体的知识碎片，但它们无法为所有实体产生一致且正确的心理模型。语言模型训练可以改善这种情况。

    

    当人们想到像“鸡蛋”这样的日常用品时，通常会有一个与之相关联的心理图像。这种常识性知识有助于我们理解这些日常用品的工作原理以及如何与它们交互。然而，如果系统对这样的日常用品没有一致的图像，比如认为鸡蛋黄包围着壳，那么它可能不得不采取荒谬的方法，比如试图把鸡蛋黄刮下壳放入平底锅中煎煮。语言模型是否具有这种日常用品的一致性心理模型？为了调查这个问题，我们提出了一个基准数据集，包括100种日常用品、它们的部件以及这些部件之间的关系。我们观察到，像GPT-3和Macaw这样的最先进的预训练语言模型具有这些实体的知识碎片，但它们无法为所有实体产生一致且正确的心理模型。我们还发现，对这个基准数据集进行语言模型训练可以提高它们在某些方面的性能。

    When people think of everyday things like an "egg," they typically have a mental image associated with it. This commonsense knowledge helps us understand how these everyday things work and how to interact with them. For example, when someone tries to make a fried egg, they know that it has a shell and that it can be cracked open to reveal the egg white and yolk inside. However, if a system does not have a coherent picture of such everyday things, thinking that the egg yolk surrounds the shell, then it might have to resort to ridiculous approaches such as trying to scrape the egg yolk off the shell into the pan. Do language models have a coherent picture of such everyday things? To investigate this, we propose a benchmark dataset consisting of 100 everyday things, their parts, and the relationships between these parts. We observe that state-of-the-art pre-trained language models (LMs) like GPT-3 and Macaw have fragments of knowledge about these entities, but they fail to produce consist
    
[^111]: BLOOM+1：为零样本提示添加语言支持

    BLOOM+1: Adding Language Support to BLOOM for Zero-Shot Prompting. (arXiv:2212.09535v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.09535](http://arxiv.org/abs/2212.09535)

    本文在BLOOM模型中应用语言适应策略，将其适应到新语言上，并在八种新语言的零样本提示表现中提升了性能。适配器微调比大模型的持续预训练更有效，提示性能主要由语言适应数据的大小确定。

    

    BLOOM模型是一个大型公开的多语言语言模型，但其预训练仅限于46种语言。为了将BLOOM的好处扩展到其他语言，而不会产生过高的成本，有必要将BLOOM适应到新的语言上。本文将现有的语言适应策略应用于BLOOM，并在资源受限的情况下对其在八种新语言的零样本提示表现进行基准测试。我们发现，语言适应对于提高新语言的零样本性能是有效的。令人惊讶的是，我们发现适配器微调比大模型的持续预训练更有效。此外，我们发现提示性能不会受到语言特定性的显着影响，如书写系统。它主要由语言适应数据的大小确定。我们还向BLOOMZ添加了新语言，这是BLOOM的多任务微调版本，能够跟随提示。

    The BLOOM model is a large publicly available multilingual language model, but its pretraining was limited to 46 languages. To extend the benefits of BLOOM to other languages without incurring prohibitively large costs, it is desirable to adapt BLOOM to new languages not seen during pretraining. In this work, we apply existing language adaptation strategies to BLOOM and benchmark its zero-shot prompting performance on eight new languages in a resource-constrained setting. We find language adaptation to be effective at improving zero-shot performance in new languages. Surprisingly, we find that adapter-based finetuning is more effective than continued pretraining for large models. In addition, we discover that prompting performance is not significantly affected by language specifics, such as the writing system. It is primarily determined by the size of the language adaptation data. We also add new languages to BLOOMZ, which is a multitask finetuned version of BLOOM capable of following 
    
[^112]: CALIME: 因果感知的局部可解释性模型-无关解释

    CALIME: Causality-Aware Local Interpretable Model-Agnostic Explanations. (arXiv:2212.05256v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2212.05256](http://arxiv.org/abs/2212.05256)

    本文提出CALIME方法，将因果知识融入可解释性人工智能方法中，以解决特征独立性的缺陷，并取得了优于初始方法的黑盒模型模拟保真度和解释稳定性的表现。

    

    可解释性人工智能方法的一个重要缺点是假设特征独立性。本文着眼于将因果知识融入可解释性人工智能方法中，以增加信任并帮助用户评估解释的质量。我们提出一种新颖的扩展方法，明确地在围绕输入实例生成的数据中编码因果关系，以解释模型。大量实验证明，我们的方法在模仿黑盒子的保真度和解释的稳定性方面均比初始方法表现优异。

    A significant drawback of eXplainable Artificial Intelligence (XAI) approaches is the assumption of feature independence. This paper focuses on integrating causal knowledge in XAI methods to increase trust and help users assess explanations' quality. We propose a novel extension to a widely used local and model-agnostic explainer that explicitly encodes causal relationships in the data generated around the input instance to explain. Extensive experiments show that our method achieves superior performance comparing the initial one for both the fidelity in mimicking the black-box and the stability of the explanations.
    
[^113]: 无格栅序列鉴别训练用于基于音素的神经传递器

    Lattice-Free Sequence Discriminative Training for Phoneme-Based Neural Transducers. (arXiv:2212.04325v3 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2212.04325](http://arxiv.org/abs/2212.04325)

    本文提出了三种无格栅训练目标，用于基于音素的神经传递器的最终后验输出，与使用N-best列表的方法相比，无格栅方法在训练期间消除了假设生成的步骤，从而导致更高效的训练，在单词错误率上表现也有6.5％的相对改进。

    

    最近，RNN-Transducer已经在各种自动语音识别任务中取得了显着成果。然而，无格栅序列鉴别训练方法，在混合模型中获得了优异的性能，但在RNN-Transducer中很少被研究。在本文中，我们提出了三个无格栅训练目标，即无格栅最大互信息、无格栅段级最小贝叶斯风险和无格栅最小贝叶斯风险，用于具有有限上下文依赖性的基于音素的神经传递器的最终后验输出。与使用N-best列表的方法相比，无格栅方法在训练期间消除了假设生成的解码步骤，从而导致更高效的训练。实验结果表明，与序列级交叉熵训练模型相比，无格栅方法在单词错误率上获得了高达6.5％的相对改进。与基于N-best列表的最小贝叶斯风险目标相比，无格栅方法具有更高的灵活性和可行性，尤其是在N-best列表中具有一些噪声和错误的情况下。

    Recently, RNN-Transducers have achieved remarkable results on various automatic speech recognition tasks. However, lattice-free sequence discriminative training methods, which obtain superior performance in hybrid models, are rarely investigated in RNN-Transducers. In this work, we propose three lattice-free training objectives, namely lattice-free maximum mutual information, lattice-free segment-level minimum Bayes risk, and lattice-free minimum Bayes risk, which are used for the final posterior output of the phoneme-based neural transducer with a limited context dependency. Compared to criteria using N-best lists, lattice-free methods eliminate the decoding step for hypotheses generation during training, which leads to more efficient training. Experimental results show that lattice-free methods gain up to 6.5% relative improvement in word error rate compared to a sequence-level cross-entropy trained model. Compared to the N-best-list based minimum Bayes risk objectives, lattice-free 
    
[^114]: MEGAN: 多解释图注意力网络

    MEGAN: Multi-Explanation Graph Attention Network. (arXiv:2211.13236v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.13236](http://arxiv.org/abs/2211.13236)

    MEGAN是一种可以在多个通道中产生节点和边的说明性解释的图注意力网络，对于改进图回归预测的可解释性至关重要。此外，它是完全可微的，可以在解释监督方式下主动地训练说明。

    

    我们提出了一种多解释图注意力网络（MEGAN）。与现有的图可解释方法不同，我们的网络可以沿多个通道产生节点和边的说明性解释，其数量独立于任务规格说明。这对于改进图回归预测的可解释性至关重要，因为可以将解释分为相对于参考值的正面和负面证据。此外，我们的基于注意力的网络是完全可微的，可以在解释监督方式下主动地训练说明。我们首先在已知地面真相说明的合成图回归数据集上验证了我们的模型。我们的网络在单一和多解释情况下均优于现有的基准可解释方法，在解释监督期间实现了接近完美的解释准确性。最后，我们展示了我们的模型在多个真实世界数据集上的能力。

    We propose a multi-explanation graph attention network (MEGAN). Unlike existing graph explainability methods, our network can produce node and edge attributional explanations along multiple channels, the number of which is independent of task specifications. This proves crucial to improve the interpretability of graph regression predictions, as explanations can be split into positive and negative evidence w.r.t to a reference value. Additionally, our attention-based network is fully differentiable and explanations can actively be trained in an explanation-supervised manner. We first validate our model on a synthetic graph regression dataset with known ground-truth explanations. Our network outperforms existing baseline explainability methods for the single- as well as the multi-explanation case, achieving near-perfect explanation accuracy during explanation supervision. Finally, we demonstrate our model's capabilities on multiple real-world datasets. We find that our model produces spa
    
[^115]: 文本中的离群检测的多层知识蒸馏

    Multi-Level Knowledge Distillation for Out-of-Distribution Detection in Text. (arXiv:2211.11300v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2211.11300](http://arxiv.org/abs/2211.11300)

    本文提出了一种多层知识蒸馏方法，融合了语言模型的训练和微调方法来进行文本中的离群检测，实验结果表明其有效性。

    

    自监督表示学习已经证明是只使用内分布(ID)样例文本进行离群检测的宝贵组成部分。这些方法要么从头开始训练语言模型，要么通过使用ID样例微调预训练语言模型，然后将语言模型输出的困惑度作为离群得分。本文分析了两种离群检测方法的互补特性，并提出了一种融合它们优势并减轻它们的局限性的多层知识蒸馏方法。具体而言，我们使用微调模型作为老师，在ID示例上教授一个随机初始化的学生模型。除了预测层蒸馏外，我们还提出了一种基于相似性的中间层蒸馏方法，以全面探索老师模型的表示空间。通过这种方式，学习的学生可以更好地表示ID数据流形，同时获得更强的将OoD示例映射到流形之外的能力。基准数据集上的实验结果证明了我们所提出的方法与竞争基线相比的有效性。

    Self-supervised representation learning has proved to be a valuable component for out-of-distribution (OoD) detection with only the texts of in-distribution (ID) examples. These approaches either train a language model from scratch or fine-tune a pre-trained language model using ID examples, and then take the perplexity output by the language model as OoD scores. In this paper, we analyze the complementary characteristics of both OoD detection methods and propose a multi-level knowledge distillation approach that integrates their strengths while mitigating their limitations. Specifically, we use a fine-tuned model as the teacher to teach a randomly initialized student model on the ID examples. Besides the prediction layer distillation, we present a similarity-based intermediate layer distillation method to thoroughly explore the representation space of the teacher model. In this way, the learned student can better represent the ID data manifold while gaining a stronger ability to map O
    
[^116]: 零偏置标量不变网络

    Scalar Invariant Networks with Zero Bias. (arXiv:2211.08486v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.08486](http://arxiv.org/abs/2211.08486)

    本文证明了在解决许多图像任务(例如图像分类)时可以忽略偏置，并且零偏置神经网络在实际图像分类任务中表现良好，同时具有标量 (乘法) 不变性，从而在改变对比度时仍能保持预测不变。

    

    与权重一样，偏置项也是许多流行的机器学习模型(包括神经网络)可学习的参数。人们认为偏差能有效地增加神经网络表示能力来解决计算机视觉中的各种任务。然而，我们认为，如果我们从第一原理考虑图像在输入空间中的内在分布以及模型应具有的一些期望特性，则偏差可以完全忽略，以解决许多与图像相关的任务，例如图像分类任务。我们的观察结果表明，零偏置神经网络在实际图像分类任务上可能与带偏置的神经网络表现相当。此外，我们证明零偏置神经网络具有称为标量(乘法)不变性的良好属性，这使得当改变输入图像的对比度时，神经网络的预测保持不变。然后，我们将标量不变性扩展到更一般的情况…

    Just like weights, bias terms are the learnable parameters of many popular machine learning models, including neural networks. Biases are believed to effectively increase the representational power of neural networks to solve a wide range of tasks in computer vision. However, we argue that if we consider the intrinsic distribution of images in the input space as well as some desired properties a model should have from the first principles, biases can be completely ignored in addressing many image-related tasks, such as image classification. Our observation indicates that zero-bias neural networks could perform comparably to neural networks with bias at least on practical image classification tasks. In addition, we prove that zero-bias neural networks possess a nice property called scalar (multiplication) invariance, which allows the prediction of neural networks remains the same when altering the contrast of the input image. We then extend scalar invariance to more general cases that a
    
[^117]: 使用说服性写作策略来解释和检测健康错误信息

    Using Persuasive Writing Strategies to Explain and Detect Health Misinformation. (arXiv:2211.05985v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2211.05985](http://arxiv.org/abs/2211.05985)

    本研究旨在通过使用说服性写作技巧的文本段落进行分类来增加自动化虚假信息检测的新层次，以产生可解释的理由。我们提出了一个包含常见说服性写作策略的注释方案和数据集，并使用 RoBERTa 文本分类模型进行实验。

    

    虚假信息的传播是当今社会的一大问题，许多学术界和工业界的研究人员正在努力解决这个问题。由于每天创造的虚假信息数量巨大，将此任务留给人工事实检查员是不切实际的。数据科学家和研究人员多年来一直致力于自动化虚假信息检测，但今天仍然是一个具有挑战性的问题。我们的研究目标是为自动化虚假信息检测添加一个新层次；使用具有说服性写作技巧的文本段落进行分类，以产生可解释的理由，说明为什么这篇文章可以标记为虚假信息。为此，我们提出了一个包含许多常见说服性写作策略的新注释方案，以及相应的人工注释数据集。我们使用 RoBERTa 文本分类模型来完成此任务，因为它在自然语言处理方面具有高性能。我们开发了几种基于语言模型的基线模型，并提供了结果分析。

    The spread of misinformation is a prominent problem in today's society, and many researchers in academia and industry are trying to combat it. Due to the vast amount of misinformation that is created every day, it is unrealistic to leave this task to human fact-checkers. Data scientists and researchers have been working on automated misinformation detection for years, and it is still a challenging problem today. The goal of our research is to add a new level to automated misinformation detection; classifying segments of text with persuasive writing techniques in order to produce interpretable reasoning for why an article can be marked as misinformation. To accomplish this, we present a novel annotation scheme containing many common persuasive writing tactics, along with a dataset with human annotations accordingly. For this task, we make use of a RoBERTa model for text classification, due to its high performance in NLP. We develop several language model-based baselines and present the 
    
[^118]: 对抗训练对语言模型的鲁棒性和泛化性的影响

    Impact of Adversarial Training on Robustness and Generalizability of Language Models. (arXiv:2211.05523v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2211.05523](http://arxiv.org/abs/2211.05523)

    本研究比较了在变压器语言模型中不同的对抗训练方法，并发现预训练数据增强或训练时间输入扰动可以实现更好的鲁棒性，而训练中使用的嵌入空间扰动可以显著提高泛化性。神经元的语言相关性分析表明，这些改进是由于存在“更专业”的神经元。这是首个对语言模型对抗训练不同方法进行全面比较的工作。

    

    对抗训练被广泛认为是防御对抗攻击的最有效手段。但是，已经确认对抗训练模型同时实现鲁棒性和泛化性需要进行权衡。本研究旨在深入比较语言模型中不同的对抗训练方法。具体而言，我们研究了在变压器语言模型中预训练数据增强、训练时间输入扰动和嵌入空间扰动对鲁棒性和泛化性的影响。我们的研究结果表明，通过预训练数据增强或训练时间输入扰动可以实现更好的鲁棒性。然而，通过嵌入空间扰动进行训练可以显著地提高泛化性。学习模型神经元的语言相关性分析表明，改善泛化性是由于存在“更专业”的神经元。据我们所知，这是首个对语言模型对抗训练不同方法进行全面比较的工作。

    Adversarial training is widely acknowledged as the most effective defense against adversarial attacks. However, it is also well established that achieving both robustness and generalization in adversarially trained models involves a trade-off. The goal of this work is to provide an in depth comparison of different approaches for adversarial training in language models. Specifically, we study the effect of pre-training data augmentation as well as training time input perturbations vs. embedding space perturbations on the robustness and generalization of transformer-based language models. Our findings suggest that better robustness can be achieved by pre-training data augmentation or by training with input space perturbation. However, training with embedding space perturbation significantly improves generalization. A linguistic correlation analysis of neurons of the learned models reveals that the improved generalization is due to 'more specialized' neurons. To the best of our knowledge,
    
[^119]: 从像素开始掌握无监督强化学习基准测试

    Mastering the Unsupervised Reinforcement Learning Benchmark from Pixels. (arXiv:2209.12016v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2209.12016](http://arxiv.org/abs/2209.12016)

    本研究提出了一种新的无监督模型驱动的强化学习方法，结合混合规划器进行任务感知的微调策略，在无监督强化学习基准测试中获得了93.59%的整体标准化性能，超过之前的基线，是解决无监督强化学习中泛化能力提升的一项重要进展。

    

    从视觉感知数据中控制人工智能智能体是一项艰巨的任务。强化学习算法可以成功，但需要智能体与环境之间大量的交互。为了缓解这个问题，无监督强化学习提出了使用自我监督交互和学习的策略，以更快地适应未来的任务。然而，正如无监督强化学习基准测试所显示的那样，当前的无监督策略是否能够提高泛化能力在视觉控制环境中尤其不清楚。在这项工作中，我们研究了无监督强化学习基准测试，并提出了一种新的方法来解决它，使用无监督模型驱动的强化学习来预训练智能体，并结合新提出的混合规划器Dyna-MPC来进行任务感知的微调策略，以适应下游任务。在无监督强化学习基准测试中，我们的方法获得了93.59%的整体标准化性能，超过了以前的基线。我们通过大规模消融研究对该方法进行了经验评估，并表明预训练、微调和Dyna-MPC是成功的关键因素。我们的发现为了解无监督强化学习在视觉控制中的潜力和局限性提供了经验的基础。

    Controlling artificial agents from visual sensory data is an arduous task. Reinforcement learning (RL) algorithms can succeed but require large amounts of interactions between the agent and the environment. To alleviate the issue, unsupervised RL proposes to employ self-supervised interaction and learning, for adapting faster to future tasks. Yet, as shown in the Unsupervised RL Benchmark (URLB; Laskin et al. 2021), whether current unsupervised strategies can improve generalization capabilities is still unclear, especially in visual control settings. In this work, we study the URLB and propose a new method to solve it, using unsupervised model-based RL, for pre-training the agent, and a task-aware fine-tuning strategy combined with a new proposed hybrid planner, Dyna-MPC, to adapt the agent for downstream tasks. On URLB, our method obtains 93.59% overall normalized performance, surpassing previous baselines by a staggering margin. The approach is empirically evaluated through a large-s
    
[^120]: DICE：基于生成模型的高效临床事件抽取

    DICE: Data-Efficient Clinical Event Extraction with Generative Models. (arXiv:2208.07989v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2208.07989](http://arxiv.org/abs/2208.07989)

    介绍了一种稳健高效的基于生成模型的临床事件抽取方法DICE，引入了对比性学习目标和特殊标记，共同训练实体提及和事件抽取等辅助任务，所提出的MACCROBAT-EE数据集为临床事件抽取提供了基准测试。

    

    临床领域的事件抽取是一个未被充分研究的研究领域。缺乏训练数据，以及领域特定术语的数量众多和实体界限模糊，使得这项任务尤其具有挑战性。本文介绍了DICE，一种稳健、高效的临床事件抽取生成模型。DICE将事件抽取作为条件生成问题，并引入对比性学习目标，以准确确定生物医学提及的边界。DICE还联合训练辅助提及标识任务和事件抽取任务，以更好地确定实体提及边界，并进一步引入特殊的标记来作为触发器和参数候选项，以包含其各自的任务中的确定实体问题。为了对临床事件抽取进行基准测试，我们根据现有的临床信息提取数据集MACCRO，构建了第一个带有参数批注的临床事件抽取数据集MACCROBAT-EE。

    Event extraction for the clinical domain is an under-explored research area. The lack of training data along with the high volume of domain-specific terminologies with vague entity boundaries makes the task especially challenging. In this paper, we introduce DICE, a robust and data-efficient generative model for clinical event extraction. DICE frames event extraction as a conditional generation problem and introduces a contrastive learning objective to accurately decide the boundaries of biomedical mentions. DICE also trains an auxiliary mention identification task jointly with event extraction tasks to better identify entity mention boundaries, and further introduces special markers to incorporate identified entity mentions as trigger and argument candidates for their respective tasks. To benchmark clinical event extraction, we compose MACCROBAT-EE, the first clinical event extraction dataset with argument annotation, based on an existing clinical information extraction dataset MACCRO
    
[^121]: Claim-Dissector: 一款联合重排和真实性预测的可解释的事实核查系统

    Claim-Dissector: An Interpretable Fact-Checking System with Joint Re-ranking and Veracity Prediction. (arXiv:2207.14116v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2207.14116](http://arxiv.org/abs/2207.14116)

    Claim-Dissector是一款联合重排和真实性预测的可解释的事实核查系统，可以识别与声明相关的证据，并确定声明的真实性。该系统的个人贡献以及证据所支持或反驳声明的贡献都可以被识别。

    

    我们提出了Claim-Dissector，一种针对事实核查和分析的新型潜变量模型，给出一个声明和一组检索到的证据，联合学习识别：（i）与给定声明相关的证据，（ii）声明的真实性。我们建议以可解释的方式解开每个证据的相关性概率及其对最终真实性概率的影响-最终真实性概率与每个证据相关性概率的线性整合成比例。通过这种方式，可以识别出每个证据对最终预测概率的个人贡献。在每个证据的相关性概率中，我们的模型还可以进一步区分每个相关证据是支持（S）还是反驳（R）声明。这样可以量化S/R概率对最终结论的贡献或检测有异议的证据。尽管我们的系统具有可解释性，但在FEVER竞赛中，其结果与最先进的结果相当。

    We present Claim-Dissector: a novel latent variable model for fact-checking and analysis, which given a claim and a set of retrieved evidences jointly learns to identify: (i) the relevant evidences to the given claim, (ii) the veracity of the claim. We propose to disentangle the per-evidence relevance probability and its contribution to the final veracity probability in an interpretable way -- the final veracity probability is proportional to a linear ensemble of per-evidence relevance probabilities. In this way, the individual contributions of evidences towards the final predicted probability can be identified. In per-evidence relevance probability, our model can further distinguish whether each relevant evidence is supporting (S) or refuting (R) the claim. This allows to quantify how much the S/R probability contributes to the final verdict or to detect disagreeing evidence.  Despite its interpretable nature, our system achieves results competitive with state-of-the-art on the FEVER 
    
[^122]: 多轨音乐 Transformer

    Multitrack Music Transformer. (arXiv:2207.06983v3 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2207.06983](http://arxiv.org/abs/2207.06983)

    这篇论文提出了一种新的多轨音乐表示方法，可以支持各种不同的乐器，在短的序列长度下实现了性能上的显著提升，同时提出了一种新方法用于分析音乐自我关注，并验证了模型更关注与当前音符形成和谐跨度和位于同一八度的音符。

    

    目前使用 Transformer 模型生成多轨音乐的方法在乐器数量、音乐片段长度和推理速度方面有限制，部分原因在于已有表示方式需要长度较长的输入序列，从而需要更多的内存。在本文中，我们提出了一种新的多轨音乐表示方法，可以支持各种不同的乐器，同时使输入序列长度更短。我们提出的 Multitrack Music Transformer（MMT）与最先进的系统相比具有可比性，在主观听测试中排在两个最近提出的模型之间，同时在速度和内存占用上都实现了显著的提升，使得该方法在实时即兴创作或接近实时的创意应用中更为实用。此外，我们提出了一种分析音乐自我关注的新方法，并展示了训练模型更关注与当前音符形成和谐跨度和位于同一八度的音符。

    Existing approaches for generating multitrack music with transformer models have been limited in terms of the number of instruments, the length of the music segments and slow inference. This is partly due to the memory requirements of the lengthy input sequences necessitated by existing representations. In this work, we propose a new multitrack music representation that allows a diverse set of instruments while keeping a short sequence length. Our proposed Multitrack Music Transformer (MMT) achieves comparable performance with state-of-the-art systems, landing in between two recently proposed models in a subjective listening test, while achieving substantial speedups and memory reductions over both, making the method attractive for real time improvisation or near real time creative applications. Further, we propose a new measure for analyzing musical self-attention and show that the trained model attends more to notes that form a consonant interval with the current note and to notes th
    
[^123]: DGraph: 用于图异常检测的大规模金融数据集

    DGraph: A Large-Scale Financial Dataset for Graph Anomaly Detection. (arXiv:2207.03579v3 [cs.SI] UPDATED)

    [http://arxiv.org/abs/2207.03579](http://arxiv.org/abs/2207.03579)

    本文介绍了一个名为DGraph的大规模金融数据集，提供了全面观察和广泛实验，揭示了异常节点和正常节点不同的特征和结构，标记和未标记节点都是检测欺诈者的必不可少的信息。

    

    图异常检测（GAD）由于其实用性和理论价值而成为近期热门研究领域。由于GAD强调应用和异常样本的罕见性，丰富其数据集的变化是基础性的工作。因此，本文提出了DGraph，它是金融领域中的实际动态图。DGraph克服了当前GAD数据集的许多限制。它包含约3百万个节点、4百万个动态边缘和1百万个地面真实节点。我们提供了对DGraph的全面观察，揭示了异常节点和正常节点通常具有不同的结构、邻居分布和时间动态。此外，它表明标记节点对于检测欺诈者也是必不可少的。此外，我们在DGraph上进行了广泛的实验。观察和实验表明，DGraph是推动GAD研究的有力工具，可以深入探索异常节点。

    Graph Anomaly Detection (GAD) has recently become a hot research spot due to its practicability and theoretical value. Since GAD emphasizes the application and the rarity of anomalous samples, enriching the varieties of its datasets is fundamental work. Thus, this paper present DGraph, a real-world dynamic graph in the finance domain. DGraph overcomes many limitations of current GAD datasets. It contains about 3M nodes, 4M dynamic edges, and 1M ground-truth nodes. We provide a comprehensive observation of DGraph, revealing that anomalous nodes and normal nodes generally have different structures, neighbor distribution, and temporal dynamics. Moreover, it suggests that unlabeled nodes are also essential for detecting fraudsters. Furthermore, we conduct extensive experiments on DGraph. Observation and experiments demonstrate that DGraph is propulsive to advance GAD research and enable in-depth exploration of anomalous nodes.
    
[^124]: 后验概念解释何时可识别？

    When are Post-hoc Conceptual Explanations Identifiable?. (arXiv:2206.13872v4 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2206.13872](http://arxiv.org/abs/2206.13872)

    本论文提出了可识别的概念发现方法，可以恢复出多个已知的概念，以确保解释的可靠性。对于具有依赖关系的概念，提出了两种新的方法，利用图像生成过程的功能组合性质。该方法明显优于现有方法。

    

    学习嵌入通常需要通过概念解释来理解和分解，这种需求在解释中不包含有效概念标签的情况下尤为显著。为了提供后验解释，概念发现方法会在已训练的嵌入空间中搜索解释性强的概念，例如物体形状或颜色。与之前的工作不同，我们认为概念发现应该是可识别的，这意味着可以被证明地恢复出多个已知的概念，以确保解释的可靠性。为了作为一个起点，我们明确地将概念发现与传统方法（例如主成分分析和独立成分分析）联系起来，并通过表明它们可以恢复具有非高斯分布的独立概念来阐明这一点。对于具有依赖关系的概念，我们提出了两种新的方法，利用图像生成过程的功能组合性质。我们的可证明可识别的概念发现方法明显优于现有方法。

    Interest in understanding and factorizing learned embedding spaces through conceptual explanations is steadily growing. When no human concept labels are available, concept discovery methods search trained embedding spaces for interpretable concepts like object shape or color that can be used to provide post-hoc explanations for decisions. Unlike previous work, we argue that concept discovery should be identifiable, meaning that a number of known concepts can be provably recovered to guarantee reliability of the explanations. As a starting point, we explicitly make the connection between concept discovery and classical methods like Principal Component Analysis and Independent Component Analysis by showing that they can recover independent concepts with non-Gaussian distributions. For dependent concepts, we propose two novel approaches that exploit functional compositionality properties of image-generating processes. Our provably identifiable concept discovery methods substantially outpe
    
[^125]: 在挑战性环境中进行机器人探索的在线自适应离策略评估方法

    Off-Policy Evaluation with Online Adaptation for Robot Exploration in Challenging Environments. (arXiv:2204.03140v3 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2204.03140](http://arxiv.org/abs/2204.03140)

    本文提出了一种在线自适应离策略评估方法，通过学习状态价值函数来为机器人在真实挑战性环境中探索提供指导，并设计了一种内在奖励函数，使机器人能够在稀疏外部奖励的情况下获得更多信息。

    

    自主探索具有许多重要应用。但是，基于信息增益或基于前沿的传统探索仅依赖于机器人当前状态来确定即时探索目标，缺乏预测未来状态价值的能力，因此导致探索决策低效。本文提出了一种方法，学习如何衡量“好”状态（以状态价值函数衡量），为机器人在真实挑战性环境中探索提供指导。我们将我们的工作制定为机器人探索的离策略评估（OPERE）问题。它包括对真实世界数据的离线蒙特卡罗训练，并执行时间差分（TD）在线自适应来优化经过训练的价值估计器。我们还设计了一种基于传感器信息覆盖率的内在奖励函数，以使机器人在稀疏外部奖励的情况下获得更多信息。结果表明，我们的方法使得机器人能够预测未来状态的价值，并在挑战性环境中做出高效的探索决策。

    Autonomous exploration has many important applications. However, classic information gain-based or frontier-based exploration only relies on the robot current state to determine the immediate exploration goal, which lacks the capability of predicting the value of future states and thus leads to inefficient exploration decisions. This paper presents a method to learn how "good" states are, measured by the state value function, to provide a guidance for robot exploration in real-world challenging environments. We formulate our work as an off-policy evaluation (OPE) problem for robot exploration (OPERE). It consists of offline Monte-Carlo training on real-world data and performs Temporal Difference (TD) online adaptation to optimize the trained value estimator. We also design an intrinsic reward function based on sensor information coverage to enable the robot to gain more information with sparse extrinsic rewards. Results show that our method enables the robot to predict the value of fut
    
[^126]: 利用基于空间概念的拓扑语义映射进行从语音指令的分层路径规划

    Hierarchical Path-planning from Speech Instructions with Spatial Concept-based Topometric Semantic Mapping. (arXiv:2203.10820v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2203.10820](http://arxiv.org/abs/2203.10820)

    该论文提出了一种基于拓扑语义映射的分层路径规划方法，可以通过语音指令和waypoint灵活地规划路径，包括地点连通性，提供了一种快速近似推理方法。

    

    在现实世界中，使用人类语音指令进行机器人导航至目的地是自主移动机器人至关重要的。然而，机器人可以采用不同的路径到达同一目标，而最短路径不一定是最优的。因此需要一种方法来灵活地接受waypoint的指标，规划更好的替代路径，即使需要绕路。此外，机器人需要实时推理能力。本研究旨在通过拓扑语义映射实现一个分层的空间表示，并结合语音指令和waypoint进行路径规划。我们提出了SpCoTMHP，一种利用多模式空间概念的层次路径规划方法，包括地点连通性。这种方法提供了一种新颖的集成概率生成模型和快速近似推理方法，层次结构中的各个级别之间可以相互交互。

    Navigating to destinations using human speech instructions is essential for autonomous mobile robots operating in the real world. Although robots can take different paths toward the same goal, the shortest path is not always optimal. A desired approach is to flexibly accommodate waypoint specifications, planning a better alternative path, even with detours. Furthermore, robots require real-time inference capabilities. Spatial representations include semantic, topological, and metric levels, each capturing different aspects of the environment. This study aims to realize a hierarchical spatial representation by a topometric semantic map and path planning with speech instructions, including waypoints. We propose SpCoTMHP, a hierarchical path-planning method that utilizes multimodal spatial concepts, incorporating place connectivity. This approach provides a novel integrated probabilistic generative model and fast approximate inference, with interaction among the hierarchy levels. A formul
    
[^127]: 社交媒体中社会语用意义的对比学习

    Contrastive Learning of Sociopragmatic Meaning in Social Media. (arXiv:2203.07648v4 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2203.07648](http://arxiv.org/abs/2203.07648)

    提出了一种社交媒体中社会语用意义的对比学习框架，该框架能够学习可迁移的任务不可知表示学习，并在各种对比学习框架中表现最佳。

    

    最近自然语言处理中的表示学习和对比学习等研究进展尚未广泛考虑社会语用意义这一类别（即不同语言社区内的交流意义）。为了弥补这一空白，我们提出了一种新的框架，用于学习可迁移至各种社会语用任务（如情感、仇恨言论、幽默、讽刺）的任务不可知表示学习。我们的框架在领域内和领域外数据以及一般和少样本情况下的各种对比学习框架中表现最佳。例如，与两个流行的预训练语言模型相比，我们的方法在每个数据集仅用20个训练样本微调时，平均F1值在16个数据集上提高了11.66个百分点。

    Recent progress in representation and contrastive learning in NLP has not widely considered the class of \textit{sociopragmatic meaning} (i.e., meaning in interaction within different language communities). To bridge this gap, we propose a novel framework for learning task-agnostic representations transferable to a wide range of sociopragmatic tasks (e.g., emotion, hate speech, humor, sarcasm). Our framework outperforms other contrastive learning frameworks for both in-domain and out-of-domain data, across both the general and few-shot settings. For example, compared to two popular pre-trained language models, our method obtains an improvement of $11.66$ average $F_1$ on $16$ datasets when fine-tuned on only $20$ training samples per dataset.
    
[^128]: HyperMixer：一种基于MLP的低成本Transformer替代方案

    HyperMixer: An MLP-based Low Cost Alternative to Transformers. (arXiv:2203.03691v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2203.03691](http://arxiv.org/abs/2203.03691)

    HyperMixer是一种低成本的基于MLP的Transformer替代方案，通过动态形成标记混合MLP来实现自然语言理解，其性能比替代方案好，并可与Transformer媲美，成本更低。

    

    Transformer架构是自然语言理解的首选模型，但它们的成本相当高，因为它们在输入长度方面具有二次复杂度，需要大量的训练数据，并且可能难以调整。为了降低成本，我们研究了简单的基于MLP的架构。我们发现现有的架构（例如MLPMixer）通过静态的MLP独立地应用于每个特征，而过于脱离自然语言理解所需的归纳偏差。在本文中，我们提出了一种简单的改进，即HyperMixer，它使用超网络动态地形成标记混合MLP。实验上，我们证明了我们的模型表现优于替代的基于MLP的模型，并与Transformer媲美。与Transformer不同，HyperMixer在处理时间、训练数据和超参数调整方面具有大大降低的成本。

    Transformer-based architectures are the model of choice for natural language understanding, but they come at a significant cost, as they have quadratic complexity in the input length, require a lot of training data, and can be difficult to tune. In the pursuit of lower costs, we investigate simple MLP-based architectures. We find that existing architectures such as MLPMixer, which achieves token mixing through a static MLP applied to each feature independently, are too detached from the inductive biases required for natural language understanding. In this paper, we propose a simple variant, HyperMixer, which forms the token mixing MLP dynamically using hypernetworks. Empirically, we demonstrate that our model performs better than alternative MLP-based models, and on par with Transformers. In contrast to Transformers, HyperMixer achieves these results at substantially lower costs in terms of processing time, training data, and hyperparameter tuning.
    
[^129]: pNLP-Mixer：一种高效的全MLP架构用于自然语言处理

    pNLP-Mixer: an Efficient all-MLP Architecture for Language. (arXiv:2202.04350v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2202.04350](http://arxiv.org/abs/2202.04350)

    pNLP-Mixer是一种新型的MLP-Mixer模型，不需要嵌入层，用于设备上高效的自然语言处理，可以达到基于transformer架构的大型预训练语言模型相近的性能，却只需要很少的资源。

    

    基于Transformer架构的大型预训练语言模型已经彻底改变了自然语言处理(NLP)领域的格局。然而，在智能手表等受限设备上部署这些模型完全不可行，因为它们的大小和推理成本。作为Transformer架构的替代方案，最近关于高效NLP的工作表明，权重高效的模型可以在兆字节级的模型大小中获得简单任务(如槽填充和意图分类)的竞争性能。这项工作介绍了pNLP-Mixer架构，一种用于设备上NLP的无嵌入MLP-Mixer模型，由于采用了新颖的投影层，因此实现了高效的权重。我们在两个多语义解析数据集MTOP和multiATIS上评估了一个大小仅为1兆字节的pNLP-Mixer模型。我们的量化模型在MTOP和multi-ATIS上实现了mBERT的99.4％和97.8％的性能，同时使用的资源仅为mBERT的170倍。

    Large pre-trained language models based on transformer architecture have drastically changed the natural language processing (NLP) landscape. However, deploying those models for on-device applications in constrained devices such as smart watches is completely impractical due to their size and inference cost. As an alternative to transformer-based architectures, recent work on efficient NLP has shown that weight-efficient models can attain competitive performance for simple tasks, such as slot filling and intent classification, with model sizes in the order of the megabyte. This work introduces the pNLP-Mixer architecture, an embedding-free MLP-Mixer model for on-device NLP that achieves high weight-efficiency thanks to a novel projection layer. We evaluate a pNLP-Mixer model of only one megabyte in size on two multi-lingual semantic parsing datasets, MTOP and multiATIS. Our quantized model achieves 99.4% and 97.8% the performance of mBERT on MTOP and multi-ATIS, while using 170x fewer 
    

