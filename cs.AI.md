# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [LMFlow: An Extensible Toolkit for Finetuning and Inference of Large Foundation Models.](http://arxiv.org/abs/2306.12420) | LMFlow是一个可扩展和轻量级的工具包，为大型基础模型提供完整的微调工作流程，以支持在有限的计算资源下进行个性化训练，并支持连续预训练和指令微调以适应不同的专业任务。 |
| [^2] | [Improving Software Requirements Prioritization through the Lens of Constraint Solving.](http://arxiv.org/abs/2306.12391) | 本文提出了一种使用约束求解器和配对比较进行需求优先级排序的交互式方法，该方法可以在需求分析师的交互作用下积累知识，可以显著地优于现有技术的效果。 |
| [^3] | [Probing the limit of hydrologic predictability with the Transformer network.](http://arxiv.org/abs/2306.12384) | 本文探究了采用Transformer网络对水文预测问题的建模效果，发现相比LSTM模型缺乏优势，可能是由于水文预测问题的马尔可夫特性所导致。 |
| [^4] | [SIFTER: A Task-specific Alignment Strategy for Enhancing Sentence Embeddings.](http://arxiv.org/abs/2306.12280) | SIFTER 是一种适应不同任务需求的任务特定对齐策略，用于增强句子嵌入。 |
| [^5] | [Knowledge-based Multimodal Music Similarity.](http://arxiv.org/abs/2306.12249) | 本文研究了基于知识的多模态音乐相似性，旨在开发一个完全可解释和可解释的系统，为最终用户提供更多对音乐相似性和分类系统的控制和理解。 |
| [^6] | [Fantastic Weights and How to Find Them: Where to Prune in Dynamic Sparse Training.](http://arxiv.org/abs/2306.12230) | 该论文对动态稀疏训练中的剪枝位置进行了实证分析，发现在低密度范围内，最简单的大小方法提供了最佳性能。 |
| [^7] | [Automated Machine Learning for Remaining Useful Life Predictions.](http://arxiv.org/abs/2306.12215) | 本文介绍了一种自动化的机器学习方法，名为AutoRUL，用于自动预测工程系统的剩余使用寿命（RUL）。该方法将微调的标准回归方法与高预测能力的集成相结合，并通过八个真实世界的和合成数据集的评估，证明AutoML提供了一种可行的选择。 |
| [^8] | [Investigating Pre-trained Language Models on Cross-Domain Datasets, a Step Closer to General AI.](http://arxiv.org/abs/2306.12205) | 本文研究了预训练语言模型在跨领域任务中的表现，结果表明预训练模型可以是实现通用人工智能的重要一步。 |
| [^9] | [Opening the Black Box: Analyzing Attention Weights and Hidden States in Pre-trained Language Models for Non-language Tasks.](http://arxiv.org/abs/2306.12198) | 本论文揭示了预训练语言模型在非语言任务中的内部运作，具体使用约束算术问题探索模型的注意力权重分数和隐藏状态，并发现了有前途的结果，该模型以略微结构化的方式解决分层问题，类似于人类解决问题的策略。 |
| [^10] | [Facial Expression Re-targeting from a Single Character.](http://arxiv.org/abs/2306.12188) | 本研究使用只有一个角色的合成数据集解决了面部表情数据集稀少、标注困难等问题。开发了一种新颖的深度学习架构，能够将面部器官的标志点分组组织，并将它们连接到相关的表情权重上。 |
| [^11] | [Adversarial Attacks Neutralization via Data Set Randomization.](http://arxiv.org/abs/2306.12161) | 本文提出一种新的防御机制，将原始数据集伪随机投影到一个新的数据集中，在多个有不同决策边界的训练分类器中随机选择一个来测试输入，能够中和对抗攻击，提高了分类器的安全性和可靠性。 |
| [^12] | [Fast Segment Anything.](http://arxiv.org/abs/2306.12156) | 本文提出了一种速度更快的方法来完成Segment Anything模型的任务，将任务重新定义为分段生成和提示，使用实例分割方法来训练现有的检测器，以此来达到与SAM方法相当的性能。 |
| [^13] | [Benchmark data to study the influence of pre-training on explanation performance in MR image classification.](http://arxiv.org/abs/2306.12150) | 本研究提出了一个MRI分类任务的基准数据集，用于评估不同模型的解释性能。实验结果表明，XAI方法并不一定比简单模型提供更好的解释，且CNN的解释能力取决于底层数据的复杂性和标签的质量。 |
| [^14] | [Synaptic metaplasticity with multi-level memristive devices.](http://arxiv.org/abs/2306.12142) | 本研究提出了一种基于忆阻器的硬件解决方案，用于在推理和训练期间实现元可塑性，从而解决神经网络的灾难性遗忘问题。 |
| [^15] | [Lightweight wood panel defect detection method incorporating attention mechanism and feature fusion network.](http://arxiv.org/abs/2306.12113) | 提出了一种轻量级木板缺陷检测方法，包括特征融合网络、ShuffleNetv2主干网络、干扰块和位移块的优化以及关注机制，相比其他方法具有更快的检测速度和更好的性能。 |
| [^16] | [Learning Latent Dynamics via Invariant Decomposition and (Spatio-)Temporal Transformers.](http://arxiv.org/abs/2306.12077) | 该文提出了一种从高维经验数据中学习动力系统的方法，基于不变分解和（空间-）时间变换器，能够自动分离出一种实例特定的编码和一种潜在动态模型，使用经验数据作为模型的输入，通过在任意连续时间推断系统行为，与显式的神经ODE公式不同，高效可扩展。 |
| [^17] | [EquiformerV2: Improved Equivariant Transformer for Scaling to Higher-Degree Representations.](http://arxiv.org/abs/2306.12059) | 本文提出了EquiformerV2，通过使用新的卷积类型和架构改进，扩展了等变Transformer到更高的等变表示，在处理大型数据集时表现更好，能量和力的表现也得到了提高，计算效率也得到了提升。 |
| [^18] | [Sample Attackability in Natural Language Adversarial Attacks.](http://arxiv.org/abs/2306.12043) | 本文研究了自然语言处理领域中，哪些样本最易受到对抗攻击或最具鲁棒性的问题，并扩展了该领域中的“样本攻击能力/鲁棒性”的定义。实验发现，基于深度学习的检测器可以更好地识别出一个看不见的目标模型中最易受攻击和最具鲁棒性的样本。 |
| [^19] | [Quantifying Node-based Core Resilience.](http://arxiv.org/abs/2306.12038) | 本文提出了一种名为依赖核韧性的新度量，用于衡量节点在插入和删除边的情况下的核韧性，以捕捉邻居节点和未来邻居节点对节点的影响，并在真实网络上展示了其优越性。 |
| [^20] | [A Semi-Autoregressive Graph Generative Model for Dependency Graph Parsing.](http://arxiv.org/abs/2306.12018) | 提出了一种半自回归的依存解析器，成功捕获了节点和边缘之间的显式依赖关系，取得了最先进的结果。 |
| [^21] | [An Overview of Catastrophic AI Risks.](http://arxiv.org/abs/2306.12001) | 本文综述了人工智能灾难性风险的四个主要来源，包括恶意使用、人工智能竞赛、组织风险和流氓人工智能。 |
| [^22] | [TauPETGen: Text-Conditional Tau PET Image Synthesis Based on Latent Diffusion Models.](http://arxiv.org/abs/2306.11984) | 本文提出了一种基于潜在扩散模型的新颖文本有条件Tau PET图像合成方法，可根据文本描述和被试的MR图像生成逼真的Tau PET图像，有助于研究不同测量之间的关系，并增加Tau PET数据集的公共可用性。 |
| [^23] | [AdCraft: An Advanced Reinforcement Learning Benchmark Environment for Search Engine Marketing Optimization.](http://arxiv.org/abs/2306.11971) | AdCraft是一种高级强化学习基准环境，用于模拟出价和预算变化的搜索引擎营销(SEM)活动，可用于评估和提高SEM出价和预算管理相关的RL算法的鲁棒性。 |
| [^24] | [On the Optimal Bounds for Noisy Computing.](http://arxiv.org/abs/2306.11951) | 本文改进了有噪音计算问题的自适应采样和非自适应采样模型下所有四个函数的下界，并且大多数下界与上界符合，差异不超过一个常数因子。 |
| [^25] | [Adaptive Ensemble Q-learning: Minimizing Estimation Bias via Error Feedback.](http://arxiv.org/abs/2306.11918) | 本研究提出了自适应集成 Q 学习算法，通过误差反馈来减小集成方法中估计偏差的影响，并结合模型识别自适应控制（MIAC）来实现集成大小自适应。 |
| [^26] | [Structure-Aware Robustness Certificates for Graph Classification.](http://arxiv.org/abs/2306.11915) | 该论文提出了一种新的随机平滑方法，可以根据图的不同预定义结构生成对应的鲁棒性证书，从而在多个基准图分类任务中取得领先的对抗性攻击下鲁棒性结果。 |
| [^27] | [Unexplainable Explanations: Towards Interpreting tSNE and UMAP Embeddings.](http://arxiv.org/abs/2306.11898) | 本文研究旨在回答ARDR算法的收敛性问题，我们将ARDR方法与传统的降维技术联系起来，可以将PCA嵌入完全恢复，通过稍加修改可以用LLE复现ARDR嵌入，并形式化了一系列猜想，如果成立，可以将2D嵌入中的结构归因于输入分布。 |
| [^28] | [Reinforcement Learning-based Virtual Fixtures for Teleoperation of Hydraulic Construction Machine.](http://arxiv.org/abs/2306.11897) | 本文提出了一种基于强化学习的方法来优化施工机器人任务性能。实验表明该方法在典型施工任务中的表现良好。 |
| [^29] | [Out of Distribution Generalization via Interventional Style Transfer in Single-Cell Microscopy.](http://arxiv.org/abs/2306.11890) | 本文提出了一种新的方法干预风格转移（IST），通过生成干预训练分布，从而显着改善了单细胞显微镜下外域泛化的问题。 |
| [^30] | [SPRINT: Scalable Policy Pre-Training via Language Instruction Relabeling.](http://arxiv.org/abs/2306.11886) | SPRINT 提出了一种离线策略预训练方法，通过指令重标记及离线强化学习实现可扩展的预训练任务，大大减少了预训练所需的人力，同时使机器人能够获取更丰富的技能库，相较于之前的预训练方法，能够更快地学习新的长时间跨度任务。 |
| [^31] | [Reward Shaping via Diffusion Process in Reinforcement Learning.](http://arxiv.org/abs/2306.11885) | 本研究使用扩散过程来进行奖励塑形，提供了解决探索 - 利用权衡的优雅框架。同时，阐明了信息熵、随机系统动力学以及它们对熵产生的影响之间的关系。 |
| [^32] | [Proactive Human-Robot Co-Assembly: Leveraging Human Intention Prediction and Robust Safe Control.](http://arxiv.org/abs/2306.11862) | 本文提出了一个用于积极人机协作的集成框架，通过鲁棒的意图预测和安全控制来提高协作效率和避免交互安全风险。 |
| [^33] | [Discovering Causality for Efficient Cooperation in Multi-Agent Environments.](http://arxiv.org/abs/2306.11846) | 本文研究了因果关系在多智能体强化学习中的应用，通过对懒惰智能体进行惩罚以帮助团队取得更好的效果。此外，研究了如何利用因果关系估计改善对智能体的信用分配，以及如何使用Amortized Causal Discovery自动检测多智能体环境中的因果关系。 |
| [^34] | [Retrieval-Based Transformer for Table Augmentation.](http://arxiv.org/abs/2306.11843) | 本文提出了一种自动数据处理的新方法，其中使用基于检索的Transformer模型来解决表格增强任务，并采用自学习策略来训练模型以重构原始值或标题，以便减轻数据分析师在数据处理中的工作量。 |
| [^35] | [Any Deep ReLU Network is Shallow.](http://arxiv.org/abs/2306.11827) | 该论文证明了任何深度的ReLU网络都可以被重写为一个具有透明性的浅层网络。这一结论有助于解释模型行为。 |
| [^36] | [Learning to Generate Better Than Your LLM.](http://arxiv.org/abs/2306.11816) | 本论文研究了基于强化学习算法 RLGF，用于在 GPT-3 等动态黑匣子指导下微调大型语言模型 LLM 的条件文本生成，相比通用 RL 算法，该算法在 IMDB 和 CommonGen 任务中表现更好。 |
| [^37] | [Designing Explainable Predictive Machine Learning Artifacts: Methodology and Practical Demonstration.](http://arxiv.org/abs/2306.11771) | 本文提出了一个解释性预测机器学习工件的设计方法学，以解决决策者不愿使用现代机器学习算法应用程序的问题，该方法学包括五个阶段。通过设计并实施一个工件，预测诊断为新冠肺炎的患者是否将来会出现花粉热症状来证明其适用性。 |
| [^38] | [A Graphical Modeling Language for Artificial Intelligence Applications in Automation Systems.](http://arxiv.org/abs/2306.11767) | 本论文介绍了一种图形建模语言，使得在自动化系统中对人工智能应用进行一致和可理解的建模，并将各个子领域分成特定领域的子系统，从而减少现有工作量。 |
| [^39] | [Learning and evolution: factors influencing an effective combination.](http://arxiv.org/abs/2306.11761) | 本文探讨了学习和进化相结合是否能够找到比单独进化发现更好的解决方案，而研究结果表明，当在学习和选择过程中引入噪声等特定条件时，这种组合才能取得成功。 |
| [^40] | [Deep Learning Accelerator in Loop Reliability Evaluation for Autonomous Driving.](http://arxiv.org/abs/2306.11759) | 本论文提出了一个DLA-in-loop可靠性评估平台，用于在早期DLA设计阶段评估自主驾驶系统中使用的DLA的总故障前行驶距离等高级可靠性指标，从而避免高昂的设计迭代成本。 |
| [^41] | [MRFI: An Open Source Multi-Resolution Fault Injection Framework for Neural Network Processing.](http://arxiv.org/abs/2306.11758) | MRFI是一个高度可配置的神经网络故障注入工具，用户可以修改独立的故障配置文件进行注入和漏洞分析。 |
| [^42] | [On Identifiability of Conditional Causal Effects.](http://arxiv.org/abs/2306.11755) | 本文研究了任意条件因果效应的可识别性问题，并通过提供完备算法证明了 Pearl 的 do-演算法对于这个问题是完备的。该工作对现有结果进行了扩展，并提供了对条件因果效应可识别性的更完整的理解。 |
| [^43] | [The Manipulation Problem: Conversational AI as a Threat to Epistemic Agency.](http://arxiv.org/abs/2306.11748) | AI对话系统的发展可能带来操纵威胁，例如影响购买选择、错误信仰或泄露个人数据。本文探讨其操纵策略，提出一套认知规定以保护个人免受不知不觉操纵。 |
| [^44] | [Multi-view 3D Object Reconstruction and Uncertainty Modelling with Neural Shape Prior.](http://arxiv.org/abs/2306.11739) | 本文提出了一种基于神经形状先验的三维物体重建方法，通过在表示中模拟不确定性，并提供可靠的不确定性估计，这种方法在单视角和多视角3D重建基准测试上均取得了最先进的性能。 |
| [^45] | [RM-PRT: Realistic Robotic Manipulation Simulator and Benchmark with Progressive Reasoning Tasks.](http://arxiv.org/abs/2306.11335) | 该论文介绍了一个基于逼真机器人操作模拟器的基准评估RM-PRT，并提出了通用的评估流程，旨在通过大规模自然语言指令和多模态提示对机器人操作进行详细评估。 |
| [^46] | [Transforming Graphs for Enhanced Attribute-Based Clustering: An Innovative Graph Transformer Method.](http://arxiv.org/abs/2306.11307) | 本文提出了一种称为GTAGC的图形自编码器图形变换自编码器方法，通过融合图自编码器和图形变换器，GTAGC能够捕获全局依赖关系，从而有助于提高图聚类的性能。 |
| [^47] | [BayLing: Bridging Cross-lingual Alignment and Instruction Following through Interactive Translation for Large Language Models.](http://arxiv.org/abs/2306.10968) | BayLing是一个指令遵循的LLM，通过交互式翻译将语言生成和指令遵循的能力从英语转移到其他语言，使得在非英语语言上也能够实现与最先进的LLMs相同的性能，而且不需要语言特定数据或指令。 |
| [^48] | [Deep Reinforcement Learning with Multitask Episodic Memory Based on Task-Conditioned Hypernetwork.](http://arxiv.org/abs/2306.10698) | 人工智能领域，一个新算法利用基于任务条件化超网络的检索网络，根据任务调整网络参数，以解决深度强化学习中选择最相关的过去经验并将其融合到既有决策网络中的问题。 |
| [^49] | [MARBLE: Music Audio Representation Benchmark for Universal Evaluation.](http://arxiv.org/abs/2306.10548) | 本论文介绍了MARBLE，一个音乐音频表征通用评估基准，它为音乐理解领域的研究和发展提供了一个全面和可持续性的基础，并提供各种音乐信息检索（MIR）任务的基准。 |
| [^50] | [Advancing Biomedicine with Graph Representation Learning: Recent Progress, Challenges, and Future Directions.](http://arxiv.org/abs/2306.10456) | 图表示学习（GRL）已经成为一个重要领域，在生物医学领域有着广泛的应用，未来研究方向是解决GRL当前面临的挑战。 |
| [^51] | [Co-Certificate Learning with SAT Modulo Symmetries.](http://arxiv.org/abs/2306.10427) | 该论文提出了一种基于SAT模质组合的新方法，称为合证书学习(co-certificate learning)，用于解决满足给定co-NP属性的同构图问题。这种方法比最近提出的基于SAT的方法快几个数量级，并具有更好的可扩展性。 |
| [^52] | [Enlighten-anything:When Segment Anything Model Meets Low-light Image Enhancement.](http://arxiv.org/abs/2306.10286) | 本文提出了Enlighten-anything，在低光图像增强中将分段模型与SAM融合，实现了良好视觉感知的融合图像。 |
| [^53] | [Matrix Diagonalization as a Board Game: Teaching an Eigensolver the Fastest Path to Solution.](http://arxiv.org/abs/2306.10075) | 本文展示了如何使用强化学习加速矩阵对角化，将选择最快的解决方案的过程视为棋盘游戏，为数值线性代数的性能提供了一种有前途的工具。 |
| [^54] | [Structured Cooperative Learning with Graphical Model Priors.](http://arxiv.org/abs/2306.09595) | 本文提出了结构化协作学习算法，在不同设备之间通过协作完成分散任务。通过图模型先验生成的协作图，算法可以自动捕捉设备之间的跨任务相关性。 |
| [^55] | [Recognizing Unseen Objects via Multimodal Intensive Knowledge Graph Propagation.](http://arxiv.org/abs/2306.08487) | 本文提出了一个多模态知识图谱传播框架，利用文本信息和视觉内容对未见过的物体进行零样本学习和识别。 |
| [^56] | [TopP\&R: Robust Support Estimation Approach for Evaluating Fidelity and Diversity in Generative Models.](http://arxiv.org/abs/2306.08013) | 本文提出了一种鲁棒可靠的生成模型评估指标TopP\&R，通过引入拓扑和统计处理进行严格的支持估计。TopP\&R仅保留具有一定置信水平的具有拓扑和统计上重要性的特征，对于噪声特征具有强大的鲁棒性，并提供了统计一致性。 |
| [^57] | [Don't trust your eyes: on the (un)reliability of feature visualizations.](http://arxiv.org/abs/2306.04719) | 本文探讨了神经网络如何从像素中提取模式的问题，并研究了特征可视化的可靠性。实验证据表明，由于优化过程中固有的限制，特征可视化能够可靠理解的功能集非常有限，对于解释神经网络如何处理自然图像的解释能力产生怀疑。 |
| [^58] | [$\textit{WHAT}$, $\textit{WHEN}$, and $\textit{HOW}$ to Ground: Designing User Persona-Aware Conversational Agents for Engaging Dialogue.](http://arxiv.org/abs/2306.03361) | 本文提出了一种针对商业环境的、能够平衡对话流畅性和趋向于理解对话系统的个性化开放领域对话系统方法，通过加权数据集混合、负角色信息增强方法，以及设计个性化对话数据集，解决了 $\textit{WHAT}$、$\textit{WHEN}$和$\textit{HOW}$ 等问题，同时提高了对话系统响应的可控性和解释性。 |
| [^59] | [SGEM: Test-Time Adaptation for Automatic Speech Recognition via Sequential-Level Generalized Entropy Minimization.](http://arxiv.org/abs/2306.01981) | SGEM提出了一种新的测试时自适应框架，利用波束搜索和广义熵最小化调整预训练ASR模型，取得了三个主流ASR模型在不同领域转变时的最新性能。 |
| [^60] | [PDT: Pretrained Dual Transformers for Time-aware Bipartite Graphs.](http://arxiv.org/abs/2306.01913) | 该论文提出了一种预训练模型，用于学习二分图中用户和内容之间的上下文知识，并采用对比学习任务以提高性能。 |
| [^61] | [FigGen: Text to Scientific Figure Generation.](http://arxiv.org/abs/2306.00800) | 本文介绍了 FigGen，一种通过将文本描述转换为科学图形的基于扩散的方法。该技术探索了文本到图形生成的领域，并解决了该任务的主要挑战。 |
| [^62] | [RL + Model-based Control: Using On-demand Optimal Control to Learn Versatile Legged Locomotion.](http://arxiv.org/abs/2305.17842) | 本文提出了一种 RL+模型控制框架以开发出可以有效可靠地学习的健壮控制策略，通过整合有限时间最优控制生成的按需参考运动分散 RL 过程，同时克服了建模简化的固有局限性，在足式 locomotion 上实现了多功能和强健，能泛化参考运动并处理更复杂的运动任务。 |
| [^63] | [Breast Cancer Segmentation using Attention-based Convolutional Network and Explainable AI.](http://arxiv.org/abs/2305.14389) | 本研究提出了一种基于注意力机制的卷积神经网络，结合可解释人工智能和红外热像技术进行乳腺癌分割，提高了检测和分类的速度和精度，并在UNet结构中使用Grad-CAM分析了潜在偏差和弱点区域，比现有模型表现更优。 |
| [^64] | [GUARD: A Safe Reinforcement Learning Benchmark.](http://arxiv.org/abs/2305.13681) | GUARD是一个广义统一安全强化学习开发基准测试平台，是目前广泛遍布且包含各种RL代理、任务和安全约束规范的一站式基准测试，能够全面涵盖最先进的安全RL算法，并具有高度的可自定义性。 |
| [^65] | [ConvXAI: Delivering Heterogeneous AI Explanations via Conversations to Support Human-AI Scientific Writing.](http://arxiv.org/abs/2305.09770) | ConvXAI是一个基于对话的XAI系统，它集成了多种XAI类型，并将实际用户需求嵌入设计中，以提高实用性。 |
| [^66] | [Adaptive Learning Path Navigation Based on Knowledge Tracing and Reinforcement Learning.](http://arxiv.org/abs/2305.04475) | 本文介绍了一种名为自适应学习路径导航（ALPN）系统，它通过将关注知识追踪（AKT）模型与增强熵近端策略优化（EPPO）算法相结合，来为学生提供高度自适应的学习路径，从而显著提高了学习效果。 |
| [^67] | [Input Augmentation with SAM: Boosting Medical Image Segmentation with Segmentation Foundation Model.](http://arxiv.org/abs/2304.11332) | 本文介绍如何使用大型的通用分割模型SAM来提升医学图像分割，展示了如何通过使用SAM生成的掩模、特征和稳定性分数来构建和训练更好的医学图像分割模型，并在两个数据集上进行了验证。 |
| [^68] | [Handling Wikidata Qualifiers in Reasoning.](http://arxiv.org/abs/2304.03375) | 本文提出了处理Wikidata限定词的方法，包括对限定词进行分类和使用多排序逻辑语言形式化Wikidata模型，以应用于推理。 |
| [^69] | [Exploring Vision-Language Models for Imbalanced Learning.](http://arxiv.org/abs/2304.01457) | 本文探索了如何通过向视觉-语言模型添加轻量级解码器和利用不平衡算法来改进性能，实验表明改进后的VLM在iNaturalist18、CIFAR-100和Visual Genome数据集上分类准确度显著提高，特别是对于少数类，性能提升很大。 |
| [^70] | [Natural scene reconstruction from fMRI signals using generative latent diffusion.](http://arxiv.org/abs/2303.05334) | 本研究提出了一个基于生成性潜在扩散的两阶段场景重建框架，称作“Brain-Diffuser”，能够从fMRI信号重建复杂场景图像。 |
| [^71] | [Contrastive Hierarchical Clustering.](http://arxiv.org/abs/2303.03389) | 本文提出了 CoHiClust 模型，一种基于深度神经网络的对比层次聚类模型，通过自监督学习方法，生成与我们直觉和图像语义相符的合理聚类结构，且在大部分图像数据集上的聚类准确性超过了最先进的平面聚类模型。 |
| [^72] | [Constrained Decision Transformer for Offline Safe Reinforcement Learning.](http://arxiv.org/abs/2302.07351) | 本文研究了离线安全强化学习问题，提出了约束决策Transformer方法。该方法可以在部署过程中动态调整权衡，具有学习自适应、安全、鲁棒且高报酬的优势表现。 |
| [^73] | [Sources of Richness and Ineffability for Phenomenally Conscious States.](http://arxiv.org/abs/2302.06403) | 本文提供了一个信息论动力系统的视角，来解释意识的丰富性和难以言说性。在我们的框架中，意识体验的丰富性对应于意识状态中的信息量，而难以言说性则对应于不同处理阶段丢失的信息量。 |
| [^74] | [PLay: Parametrically Conditioned Layout Generation using Latent Diffusion.](http://arxiv.org/abs/2301.11529) | 本文提出了一种有条件的潜在扩散模型 PLay，能够根据用户指定的指南在矢量图空间中生成参数化条件布局。相比以前的工作，在三个数据集和用户研究中表现更好，并为专业布局设计带来了新颖和互动的体验。 |
| [^75] | [VRDU: A Benchmark for Visually-rich Document Understanding.](http://arxiv.org/abs/2211.15421) | 本研究提出了一个名为VRDU的基准测试，以更全面地反映实际文档的复杂性，其中包含具有挑战性的丰富模式、复杂模板和多样的布局。该基准测试可用于评估文档中提取结构化数据的模型。 |
| [^76] | [DeepIPC: Deeply Integrated Perception and Control for an Autonomous Vehicle in Real Environments.](http://arxiv.org/abs/2207.09934) | 本文介绍了DeepIPC，一个端到端自动驾驶模型，能够同时处理感知和控制任务。实验结果表明，DeepIPC具有最佳的可驾性和多任务性能，甚至比其他模型所需的参数更少。 |
| [^77] | [Stability and Generalization of Stochastic Optimization with Nonconvex and Nonsmooth Problems.](http://arxiv.org/abs/2206.07082) | 本文首次启动了对非凸非光滑问题上随机优化的系统稳定性和泛化分析，引入了新颖的算法稳定性度量，并建立了它们与种群梯度和经验梯度之间的定量连接。 |
| [^78] | [AIGenC: AI generalisation via creativity.](http://arxiv.org/abs/2205.09738) | AIGenC 提出了一种创造性的计算模型，通过引入概念空间和分层结构来提高人工智能代理的通用性和创新能力，实验表明其在通用任务中的表现优于最先进方法。 |
| [^79] | [Efficient Off-Policy Reinforcement Learning via Brain-Inspired Computing.](http://arxiv.org/abs/2205.06978) | 本文提出了一种QHD离线策略，它基于超维强化学习与脑启发计算，实现稳健和实时学习。QHD相比于DQN具有更高效率且适用于高效的强化学习，具有在线和实时学习潜力。 |
| [^80] | [Value Gradient weighted Model-Based Reinforcement Learning.](http://arxiv.org/abs/2204.01464) | 本文提出了一种基于价值梯度加权的模型驱动强化学习方法，用于提高模型学习的性能。 |
| [^81] | [Missing Value Imputation on Multidimensional Time Series.](http://arxiv.org/abs/2103.01600) | 本文提出了DeepMVI，一种用于填充多维时间序列数据中缺失值的深度学习方法。该方法结合时间序列中的精细和粗粒度模式，以及跨分类维度的相关系列趋势。经过改进，DeepMVI表现出比其他算法更优秀的结果。 |

# 详细

[^1]: LMFlow：用于大型基础模型微调和推理的可扩展工具包

    LMFlow: An Extensible Toolkit for Finetuning and Inference of Large Foundation Models. (arXiv:2306.12420v1 [cs.CL])

    [http://arxiv.org/abs/2306.12420](http://arxiv.org/abs/2306.12420)

    LMFlow是一个可扩展和轻量级的工具包，为大型基础模型提供完整的微调工作流程，以支持在有限的计算资源下进行个性化训练，并支持连续预训练和指令微调以适应不同的专业任务。

    

    大型基础模型展现出比传统方法更接近人类智能的能力，已经引起了人工智能界的广泛关注。然而，大部分模型在专业任务应用中仍然表现出明显的缺陷，需要微调才能获得令人满意的性能。随着可用模型和专业任务数量的增加，通用微调的工作变得非常棘手。在本文中，我们采取了第一步解决这个问题。我们介绍了一个可扩展和轻量级的工具包LMFlow，旨在简化大型基础模型的微调和推理。LMFlow为大型基础模型提供了完整的微调工作流程，支持在有限的计算资源下进行个性化训练。此外，它还支持连续预训练、指令微调等功能，以更好地适应不同的专业任务。

    Large foundation models have demonstrated a great ability to achieve general human-level intelligence far beyond traditional approaches. As the technique keeps attracting attention from the AI community, more and more large foundation models have become publically available. However, most of those models exhibit a major deficiency in specialized-task applications, where the step of finetuning is still required for obtaining satisfactory performance. As the number of available models and specialized tasks keeps growing, the job of general finetuning becomes highly nontrivial. In this paper, we take the first step to address this issue. We introduce an extensible and lightweight toolkit, LMFlow, which aims to simplify the finetuning and inference of general large foundation models. LMFlow offers a complete finetuning workflow for a large foundation model to support personalized training with limited computing resources. Furthermore, it supports continuous pretraining, instruction tuning,
    
[^2]: 通过约束求解改进软件需求优先级排序

    Improving Software Requirements Prioritization through the Lens of Constraint Solving. (arXiv:2306.12391v1 [cs.SE])

    [http://arxiv.org/abs/2306.12391](http://arxiv.org/abs/2306.12391)

    本文提出了一种使用约束求解器和配对比较进行需求优先级排序的交互式方法，该方法可以在需求分析师的交互作用下积累知识，可以显著地优于现有技术的效果。

    

    需求优先级排序是早期软件开发过程中关键的活动之一，它产生了一组要实施的重点需求。该排序过程基于多个特征（包括最终用户的偏好、实现成本和技术依赖关系）为需求提供了平等性。本文提出了一种使用配对比较和约束求解器进行需求优先级排序的交互式方法。当相对优先级不能基于需求文档中存在的知识确定时，我们的方法通过需求分析师的交互式知识积累来决定需求的相对优先级。需求的最终排序通过约束求解器和交互式配对比较来完成。我们使用真实的医疗保健项目需求进行了方法评估。依赖于约束求解器的所提出的优先级排序方法优于现有的状态-of-the-art

    Requirements prioritization is a critical activity during the early software development process, which produces a set of key requirements to implement. The prioritization process offers a parity among the requirements based on multiple characteristics, including end-users' preferences, cost to implement, and technical dependencies. This paper presents an interactive method to requirements prioritization that leverages the pairwise comparisons and a constraint solver. Our method employs an interactive accumulation of knowledge from the requirements analyst when the relative priority among the requirements cannot be determined based on the existing knowledge from the requirements documents. The final ranking of the requirements is produced via the constraint solver and interactive pairwise comparisons. We evaluate the proposed method using the requirements from a real healthcare project. The proposed prioritization method relying on a constraint solver outperforms state-of-the-art inter
    
[^3]: 用Transformer网络探究水文预测的极限（arXiv:2306.12384v1 [cs.LG]）

    Probing the limit of hydrologic predictability with the Transformer network. (arXiv:2306.12384v1 [cs.LG])

    [http://arxiv.org/abs/2306.12384](http://arxiv.org/abs/2306.12384)

    本文探究了采用Transformer网络对水文预测问题的建模效果，发现相比LSTM模型缺乏优势，可能是由于水文预测问题的马尔可夫特性所导致。

    

    自其引入水文学领域以来，循环神经网络（如LSTM）在已知的可比基准上的日水文图表指标方面一直证明难以超越。除水文学外，Transformer现在已成为连续预测任务的首选模型，这使得它成为一个有趣的架构进行研究。在本文中，我们首先展示了香草Transformer架构在广泛使用的CAMELS数据集上与LSTM相比毫无竞争力，由于短期过程而特别滞后于高流量指标。然而，不需要递归的变体Transformer可以获得与LSTM相混合的比较，产生相同的Kling-Gupta效率系数（KGE）以及其他指标。Transformer没有优势与水文预测问题的马尔可夫特性有关。与LSTM类似，Transformer也可以合并多个强制数据集来改善模型性能。然而，虽然将水文预测问题的关键特征转换为更描述性的空间，Transformer模型似乎无法超越LSTM模型的预测性能。

    For a number of years since its introduction to hydrology, recurrent neural networks like long short-term memory (LSTM) have proven remarkably difficult to surpass in terms of daily hydrograph metrics on known, comparable benchmarks. Outside of hydrology, Transformers have now become the model of choice for sequential prediction tasks, making it a curious architecture to investigate. Here, we first show that a vanilla Transformer architecture is not competitive against LSTM on the widely benchmarked CAMELS dataset, and lagged especially for the high-flow metrics due to short-term processes. However, a recurrence-free variant of Transformer can obtain mixed comparisons with LSTM, producing the same Kling-Gupta efficiency coefficient (KGE), along with other metrics. The lack of advantages for the Transformer is linked to the Markovian nature of the hydrologic prediction problem. Similar to LSTM, the Transformer can also merge multiple forcing dataset to improve model performance. While t
    
[^4]: SIFTER: 一种用于增强句子嵌入的任务特定对齐策略

    SIFTER: A Task-specific Alignment Strategy for Enhancing Sentence Embeddings. (arXiv:2306.12280v1 [cs.CL])

    [http://arxiv.org/abs/2306.12280](http://arxiv.org/abs/2306.12280)

    SIFTER 是一种适应不同任务需求的任务特定对齐策略，用于增强句子嵌入。

    

    预训练模型后进行微调已成为自然语言处理任务中的主流方法。虽然预训练模型具有泛化的优势，但它们的表现在不同领域任务中仍可能存在显著差异。这是因为不同领域的数据分布不同，并且不同的下游任务对句子的敏感程度也不同。该论文提出了一种用于增强句子嵌入的任务特定对齐策略 SIFTER，以适应不同任务需求。

    The paradigm of pre-training followed by fine-tuning on downstream tasks has become the mainstream method in natural language processing tasks. Although pre-trained models have the advantage of generalization, their performance may still vary significantly across different domain tasks. This is because the data distribution in different domains varies. For example, the different parts of the sentence 'He married Smt. Dipali Ghosh in 1947 and led a very happy married life' may have different impact for downstream tasks. For similarity calculations, words such as 'led' and 'life' are more important. On the other hand, for sentiment analysis, the word 'happy' is crucial. This indicates that different downstream tasks have different levels of sensitivity to sentence components. Our starting point is to scale information of the model and data according to the specifics of downstream tasks, enhancing domain information of relevant parts for these tasks and reducing irrelevant elements for di
    
[^5]: 基于知识的多模态音乐相似性研究

    Knowledge-based Multimodal Music Similarity. (arXiv:2306.12249v1 [cs.SD])

    [http://arxiv.org/abs/2306.12249](http://arxiv.org/abs/2306.12249)

    本文研究了基于知识的多模态音乐相似性，旨在开发一个完全可解释和可解释的系统，为最终用户提供更多对音乐相似性和分类系统的控制和理解。

    

    音乐相似性是音乐检索、推荐系统和音乐分析的重要方面。而且，对于音乐专家来说，相似性可以研究作曲家和历史时期之间的类比和影响。目前，针对音乐相似性的方法主要依赖符号内容，这可能成本高昂且不总是易于获得。相比之下，使用音频信号的方法通常无法提供有关观察到的相似性背后原因的任何见解。本研究通过使用符号和音频内容来研究音乐相似性，解决了当前方法的局限性。本研究的目的是开发一个完全可解释和可解释的系统，可以为最终用户提供更多对音乐相似性和分类系统的控制和理解。

    Music similarity is an essential aspect of music retrieval, recommendation systems, and music analysis. Moreover, similarity is of vital interest for music experts, as it allows studying analogies and influences among composers and historical periods. Current approaches to musical similarity rely mainly on symbolic content, which can be expensive to produce and is not always readily available. Conversely, approaches using audio signals typically fail to provide any insight about the reasons behind the observed similarity. This research addresses the limitations of current approaches by focusing on the study of musical similarity using both symbolic and audio content. The aim of this research is to develop a fully explainable and interpretable system that can provide end-users with more control and understanding of music similarity and classification systems.
    
[^6]: 奇妙的权重及其查找方法：动态稀疏训练中的剪枝位置

    Fantastic Weights and How to Find Them: Where to Prune in Dynamic Sparse Training. (arXiv:2306.12230v1 [cs.LG])

    [http://arxiv.org/abs/2306.12230](http://arxiv.org/abs/2306.12230)

    该论文对动态稀疏训练中的剪枝位置进行了实证分析，发现在低密度范围内，最简单的大小方法提供了最佳性能。

    

    动态稀疏训练（DST）是一个快速发展的研究领域，旨在通过在训练过程中调整神经网络的拓扑结构来优化其稀疏初始化。已经证明，在特定条件下，DST能够胜过密集模型。该框架的关键组成部分是剪枝和生长标准，这些标准在训练过程中被反复应用以调整网络的稀疏连接。虽然生长标准对DST性能的影响相对较好地研究了，但剪枝标准的影响仍然被忽视。为解决这个问题，我们设计并进行了对各种剪枝标准的广泛实证分析，以更好地了解它们对 DST 解决方案动态的影响。令人惊讶的是，我们发现大多数研究方法都产生类似的结果。在低密度范围内，最简单的技术——基于大小的方法，提供了最佳性能。

    Dynamic Sparse Training (DST) is a rapidly evolving area of research that seeks to optimize the sparse initialization of a neural network by adapting its topology during training. It has been shown that under specific conditions, DST is able to outperform dense models. The key components of this framework are the pruning and growing criteria, which are repeatedly applied during the training process to adjust the network's sparse connectivity. While the growing criterion's impact on DST performance is relatively well studied, the influence of the pruning criterion remains overlooked. To address this issue, we design and perform an extensive empirical analysis of various pruning criteria to better understand their effect on the dynamics of DST solutions. Surprisingly, we find that most of the studied methods yield similar results. The differences become more significant in the low-density regime, where the best performance is predominantly given by the simplest technique: magnitude-based
    
[^7]: 面向剩余使用寿命预测的自动化机器学习

    Automated Machine Learning for Remaining Useful Life Predictions. (arXiv:2306.12215v1 [cs.LG])

    [http://arxiv.org/abs/2306.12215](http://arxiv.org/abs/2306.12215)

    本文介绍了一种自动化的机器学习方法，名为AutoRUL，用于自动预测工程系统的剩余使用寿命（RUL）。该方法将微调的标准回归方法与高预测能力的集成相结合，并通过八个真实世界的和合成数据集的评估，证明AutoML提供了一种可行的选择。

    

    预测工程系统的剩余使用寿命（RUL）是预测与健康管理中的重要任务。最近，数据驱动的方法在RUL预测中普及，相比模型驱动的方法不需要工程系统的物理知识。但是，这只是将需要的物理专业知识替换成机器学习（ML）专业知识，而这种专业知识通常也不可得。自动化机器学习（AutoML）承诺自动构建端到端的ML管道，使领域专家而非ML专家能够创建自己的模型。本文介绍了AutoRUL，一种AutoML驱动的端到端方法，用于自动RUL预测。AutoRUL将微调的标准回归方法与高预测能力的集成相结合。通过将所提出的方法用于八个真实世界的和合成数据集，与最先进的手工模型进行比较，我们表明AutoML提供了一种可行的选择。

    Being able to predict the remaining useful life (RUL) of an engineering system is an important task in prognostics and health management. Recently, data-driven approaches to RUL predictions are becoming prevalent over model-based approaches since no underlying physical knowledge of the engineering system is required. Yet, this just replaces required expertise of the underlying physics with machine learning (ML) expertise, which is often also not available. Automated machine learning (AutoML) promises to build end-to-end ML pipelines automatically enabling domain experts without ML expertise to create their own models. This paper introduces AutoRUL, an AutoML-driven end-to-end approach for automatic RUL predictions. AutoRUL combines fine-tuned standard regression methods to an ensemble with high predictive power. By evaluating the proposed method on eight real-world and synthetic datasets against state-of-the-art hand-crafted models, we show that AutoML provides a viable alternative to 
    
[^8]: 探究预训练语言模型在跨领域数据集上的表现，迈向通用人工智能的一步。

    Investigating Pre-trained Language Models on Cross-Domain Datasets, a Step Closer to General AI. (arXiv:2306.12205v1 [cs.CL])

    [http://arxiv.org/abs/2306.12205](http://arxiv.org/abs/2306.12205)

    本文研究了预训练语言模型在跨领域任务中的表现，结果表明预训练模型可以是实现通用人工智能的重要一步。

    

    最近，预训练语言模型已成为微调各种语言任务的强有力工具。理想情况下，当模型在大量数据上进行预训练时，希望其能获得隐式知识。本文研究了预训练语言模型在泛化到不同非语言任务的能力。特别是，我们测试了来自不同领域的任务，如计算机视觉、分层数据推理和蛋白质折叠预测。我们使用的四个预训练模型，T5、BART、BERT 和 GPT-2 都取得了优异的成绩。它们的表现很相似，且它们的表现比从头开始训练的 transformers 要好得多。例如，在 Listops 数据集上，预训练语言模型的平均准确率为 58.7％，而从头开始训练的 transformers 的平均准确率为 29.0％。跨三种类型的数据集所展示的显著改进表明，预训练语言模型可能是实现通用人工智能的有前途的一步。

    Pre-trained language models have recently emerged as a powerful tool for fine-tuning a variety of language tasks. Ideally, when models are pre-trained on large amount of data, they are expected to gain implicit knowledge. In this paper, we investigate the ability of pre-trained language models to generalize to different non-language tasks. In particular, we test them on tasks from different domains such as computer vision, reasoning on hierarchical data, and protein fold prediction. The four pre-trained models that we used, T5, BART, BERT, and GPT-2 achieve outstanding results. They all have similar performance and they outperform transformers that are trained from scratch by a large margin. For instance, pre-trained language models perform better on the Listops dataset, with an average accuracy of 58.7\%, compared to transformers trained from scratch, which have an average accuracy of 29.0\%. The significant improvement demonstrated across three types of datasets suggests that pre-tra
    
[^9]: 揭开黑匣子：分析预训练语言模型在非语言任务中的注意力权重和隐藏状态

    Opening the Black Box: Analyzing Attention Weights and Hidden States in Pre-trained Language Models for Non-language Tasks. (arXiv:2306.12198v1 [cs.CL])

    [http://arxiv.org/abs/2306.12198](http://arxiv.org/abs/2306.12198)

    本论文揭示了预训练语言模型在非语言任务中的内部运作，具体使用约束算术问题探索模型的注意力权重分数和隐藏状态，并发现了有前途的结果，该模型以略微结构化的方式解决分层问题，类似于人类解决问题的策略。

    

    由于大多数先进模型的“黑匣子”特性，研究深度学习语言模型一直是一个重要的研究领域。随着基于transformers的预训练语言模型的最近进展及其在日常生活中的不断集成，解决这个问题变得更加紧迫。为了实现可解释的AI模型，必须理解涉及的过程步骤，并将其与人类思维过程进行比较。因此，在本文中，我们使用简单易懂的非语言任务来探索这些模型的内部运作。具体来说，我们将预训练语言模型应用于具有分层结构的约束算术问题，以分析其注意力权重分数和隐藏状态。调查结果显示出令人鼓舞的结果，模型以略微结构化的方式解决分层问题，类似于人类解决问题的策略。

    Investigating deep learning language models has always been a significant research area due to the ``black box" nature of most advanced models. With the recent advancements in pre-trained language models based on transformers and their increasing integration into daily life, addressing this issue has become more pressing. In order to achieve an explainable AI model, it is essential to comprehend the procedural steps involved and compare them with human thought processes. Thus, in this paper, we use simple, well-understood non-language tasks to explore these models' inner workings. Specifically, we apply a pre-trained language model to constrained arithmetic problems with hierarchical structure, to analyze their attention weight scores and hidden states. The investigation reveals promising results, with the model addressing hierarchical problems in a moderately structured manner, similar to human problem-solving strategies. Additionally, by inspecting the attention weights layer by laye
    
[^10]: 单个角色的面部表情重定向研究

    Facial Expression Re-targeting from a Single Character. (arXiv:2306.12188v1 [cs.GR])

    [http://arxiv.org/abs/2306.12188](http://arxiv.org/abs/2306.12188)

    本研究使用只有一个角色的合成数据集解决了面部表情数据集稀少、标注困难等问题。开发了一种新颖的深度学习架构，能够将面部器官的标志点分组组织，并将它们连接到相关的表情权重上。

    

    数字化面部动画中的视频重定向是虚拟现实、社交媒体、游戏、电影和视频会议中常用的技术，旨在根据人脸视频激活角色的面部表情。本文中提出的方法解决了面部表情数据集稀少、标注困难等问题。我们使用只有一个角色的合成数据集，将每帧图像重新表示为面部标志点，并开发出一种新颖的深度学习架构。该方法分组组织每个面部器官的标志点，并将它们连接到相关的表情权重上。此外，我们还使用了其他方法增强面部表情的表达效果。

    Video retargeting for digital face animation is used in virtual reality, social media, gaming, movies, and video conference, aiming to animate avatars' facial expressions based on videos of human faces. The standard method to represent facial expressions for 3D characters is by blendshapes, a vector of weights representing the avatar's neutral shape and its variations under facial expressions, e.g., smile, puff, blinking. Datasets of paired frames with blendshape vectors are rare, and labeling can be laborious, time-consuming, and subjective. In this work, we developed an approach that handles the lack of appropriate datasets. Instead, we used a synthetic dataset of only one character. To generalize various characters, we re-represented each frame to face landmarks. We developed a unique deep-learning architecture that groups landmarks for each facial organ and connects them to relevant blendshape weights. Additionally, we incorporated complementary methods for facial expressions that 
    
[^11]: 数据集随机化的对抗攻击中和方法

    Adversarial Attacks Neutralization via Data Set Randomization. (arXiv:2306.12161v1 [cs.LG])

    [http://arxiv.org/abs/2306.12161](http://arxiv.org/abs/2306.12161)

    本文提出一种新的防御机制，将原始数据集伪随机投影到一个新的数据集中，在多个有不同决策边界的训练分类器中随机选择一个来测试输入，能够中和对抗攻击，提高了分类器的安全性和可靠性。

    

    对深度学习模型的对抗攻击对它们的可靠性和安全性构成了严重的威胁。现有的防御机制要么只针对某一特定类型的攻击，要么容易受到复杂攻击的影响。我们提出了一种新的防御机制，虽然着眼于基于图像的分类器，但与引用类别相关的特征是一般的。它植根于超空间投影，提供了原始数据集的伪随机投影到一个新的数据集。在训练过程中，我们使用创建了各种投影数据集的一套生成器，每个生成器训练一个特定的分类器，从而得到具有不同决策边界的不同训练分类器。在测试过程中，选择一个分类器来测试输入。我们的方法不会损害对于合法输入的准确性。除了详细阐述和提供我们的防御机制的全面特征化外，我们还提供了四个实验的概念证明。

    Adversarial attacks on deep-learning models pose a serious threat to their reliability and security. Existing defense mechanisms are narrow addressing a specific type of attack or being vulnerable to sophisticated attacks. We propose a new defense mechanism that, while being focused on image-based classifiers, is general with respect to the cited category. It is rooted on hyperspace projection. In particular, our solution provides a pseudo-random projection of the original dataset into a new dataset. The proposed defense mechanism creates a set of diverse projected datasets, where each projected dataset is used to train a specific classifier, resulting in different trained classifiers with different decision boundaries. During testing, it randomly selects a classifier to test the input. Our approach does not sacrifice accuracy over legitimate input. Other than detailing and providing a thorough characterization of our defense mechanism, we also provide a proof of concept of using four 
    
[^12]: 快速的区分任何东西

    Fast Segment Anything. (arXiv:2306.12156v1 [cs.CV])

    [http://arxiv.org/abs/2306.12156](http://arxiv.org/abs/2306.12156)

    本文提出了一种速度更快的方法来完成Segment Anything模型的任务，将任务重新定义为分段生成和提示，使用实例分割方法来训练现有的检测器，以此来达到与SAM方法相当的性能。

    

    最近提出的“segment anything model”（SAM）在许多计算机视觉任务中产生了重大影响。它正在成为许多高级任务的基础步骤，如图像分割、图像说明和图像编辑。然而，它巨大的计算成本阻碍了它在更广泛的工业场景中的应用。主要的计算成本来自于高分辨率输入的Transformer架构。在本文中，我们提出了一种具有可比较性能的加速替代方法。通过将任务重新定义为分段生成和提示，我们发现具有实例分割分支的常规CNN检测器也可以很好地完成这项任务。具体而言，我们将此任务转换为深入研究的实例分割任务，并直接使用SAM作者发布的1/50 SA-1B数据集训练现有的实例分割方法。通过我们的方法，我们在50倍更高的速度下实现了与SAM方法相当的性能。

    The recently proposed segment anything model (SAM) has made a significant influence in many computer vision tasks. It is becoming a foundation step for many high-level tasks, like image segmentation, image caption, and image editing. However, its huge computation costs prevent it from wider applications in industry scenarios. The computation mainly comes from the Transformer architecture at high-resolution inputs. In this paper, we propose a speed-up alternative method for this fundamental task with comparable performance. By reformulating the task as segments-generation and prompting, we find that a regular CNN detector with an instance segmentation branch can also accomplish this task well. Specifically, we convert this task to the well-studied instance segmentation task and directly train the existing instance segmentation method using only 1/50 of the SA-1B dataset published by SAM authors. With our method, we achieve a comparable performance with the SAM method at 50 times higher 
    
[^13]: 基于预训练的影响因素研究医学图像分类解释性能的基准数据

    Benchmark data to study the influence of pre-training on explanation performance in MR image classification. (arXiv:2306.12150v1 [cs.CV])

    [http://arxiv.org/abs/2306.12150](http://arxiv.org/abs/2306.12150)

    本研究提出了一个MRI分类任务的基准数据集，用于评估不同模型的解释性能。实验结果表明，XAI方法并不一定比简单模型提供更好的解释，且CNN的解释能力取决于底层数据的复杂性和标签的质量。

    

    卷积神经网络（CNN）常常在医学预测任务中被成功地应用，通常与迁移学习相结合，在训练数据不足时能够提高性能。然而，由于CNN产生的模型高度复杂且通常不提供任何有关其预测机制的信息，这促使了“可解释性”人工智能（XAI）领域的研究。本文提出了一个基准数据集，用于在MRI分类任务中定量评估解释性能。通过这个基准数据集，我们可以了解迁移学习对解释质量的影响。实验结果表明，应用于基于迁移学习的CNN的流行XAI方法并不一定比简单模型提供更好的解释，并且CNN提供有意义解释的能力严重依赖于底层数据的复杂性和标签的质量。

    Convolutional Neural Networks (CNNs) are frequently and successfully used in medical prediction tasks. They are often used in combination with transfer learning, leading to improved performance when training data for the task are scarce. The resulting models are highly complex and typically do not provide any insight into their predictive mechanisms, motivating the field of 'explainable' artificial intelligence (XAI). However, previous studies have rarely quantitatively evaluated the 'explanation performance' of XAI methods against ground-truth data, and transfer learning and its influence on objective measures of explanation performance has not been investigated. Here, we propose a benchmark dataset that allows for quantifying explanation performance in a realistic magnetic resonance imaging (MRI) classification task. We employ this benchmark to understand the influence of transfer learning on the quality of explanations. Experimental results show that popular XAI methods applied to t
    
[^14]: 多级忆阻器的突触元可塑性

    Synaptic metaplasticity with multi-level memristive devices. (arXiv:2306.12142v1 [cs.NE])

    [http://arxiv.org/abs/2306.12142](http://arxiv.org/abs/2306.12142)

    本研究提出了一种基于忆阻器的硬件解决方案，用于在推理和训练期间实现元可塑性，从而解决神经网络的灾难性遗忘问题。

    

    深度学习在各种任务上取得了显著进展，有时在某些情况下超越了人类表现。然而，神经网络的一个缺点是灾难性遗忘，即在学习新任务时，曾经训练过的网络会忘记原有的解决方案。为了解决这个问题，最近的一些研究提出了基于元可塑性的二值神经网络（BNN）的解决方案。在本研究中，我们将这种解决方案扩展到量化神经网络（QNN），并提出了一种基于忆阻器的硬件解决方案，用于在推理和训练期间实现元可塑性。我们提出了一种硬件架构，将经过量化的权重与以模拟多级方式编程的忆阻器器件集成在一起，并与数字处理单元结合使用，实现高精度的元可塑存储。我们使用一个组合软件框架和基于忆阻器交叉阵列的内存计算，在130纳米CMOS技术下进行了验证。实验结果表明，一个两层感知器可以使用我们的方法正确地学习两个不同的任务。

    Deep learning has made remarkable progress in various tasks, surpassing human performance in some cases. However, one drawback of neural networks is catastrophic forgetting, where a network trained on one task forgets the solution when learning a new one. To address this issue, recent works have proposed solutions based on Binarized Neural Networks (BNNs) incorporating metaplasticity. In this work, we extend this solution to quantized neural networks (QNNs) and present a memristor-based hardware solution for implementing metaplasticity during both inference and training. We propose a hardware architecture that integrates quantized weights in memristor devices programmed in an analog multi-level fashion with a digital processing unit for high-precision metaplastic storage. We validated our approach using a combined software framework and memristor based crossbar array for in-memory computing fabricated in 130 nm CMOS technology. Our experimental results show that a two-layer perceptron 
    
[^15]: 融合关注机制和特征融合网络的轻量级木板缺陷检测方法

    Lightweight wood panel defect detection method incorporating attention mechanism and feature fusion network. (arXiv:2306.12113v1 [cs.CV])

    [http://arxiv.org/abs/2306.12113](http://arxiv.org/abs/2306.12113)

    提出了一种轻量级木板缺陷检测方法，包括特征融合网络、ShuffleNetv2主干网络、干扰块和位移块的优化以及关注机制，相比其他方法具有更快的检测速度和更好的性能。

    

    近年来，深度学习在木板缺陷检测方面取得了重大进展，但是仍存在检测率不高、检测速度慢以及难以在木板表面部署嵌入式设备等挑战。为了克服这些问题，我们提出了一种称为YOLOv5-LW的轻量级木板缺陷检测方法，它集成了注意力机制和特征融合网络。该方法通过引入多尺度双向特征金字塔网络（MBiFPN）作为特征融合网络来增强可接受缺陷的检测能力，同时采用ShuffleNetv2网络模型作为主干网络来实现轻量化设计，引入干扰块和位移块来优化模型，最后引入注意力机制来提高敏感度并减少错误检测。与其他最先进的方法相比，我们的方法在木板缺陷检测任务上实现了更快的检测和更好的性能。

    In recent years, deep learning has made significant progress in wood panel defect detection. However, there are still challenges such as low detection , slow detection speed, and difficulties in deploying embedded devices on wood panel surfaces. To overcome these issues, we propose a lightweight wood panel defect detection method called YOLOv5-LW, which incorporates attention mechanisms and a feature fusion network.Firstly, to enhance the detection capability of acceptable defects, we introduce the Multi-scale Bi-directional Feature Pyramid Network (MBiFPN) as a feature fusion network. The MBiFPN reduces feature loss, enriches local and detailed features, and improves the model's detection capability for acceptable defects.Secondly, to achieve a lightweight design, we reconstruct the ShuffleNetv2 network model as the backbone network. This reconstruction reduces the number of parameters and computational requirements while maintaining performance. We also introduce the Stem Block and S
    
[^16]: 通过不变分解和（空间-）时间变换器学习潜在动态

    Learning Latent Dynamics via Invariant Decomposition and (Spatio-)Temporal Transformers. (arXiv:2306.12077v1 [cs.LG])

    [http://arxiv.org/abs/2306.12077](http://arxiv.org/abs/2306.12077)

    该文提出了一种从高维经验数据中学习动力系统的方法，基于不变分解和（空间-）时间变换器，能够自动分离出一种实例特定的编码和一种潜在动态模型，使用经验数据作为模型的输入，通过在任意连续时间推断系统行为，与显式的神经ODE公式不同，高效可扩展。

    

    我们提出了一种利用变分自编码器和（空间-）时间注意力的方法，通过一个设计旨在强制实施某些科学动机的不变性的框架，从高维经验数据中学习动力学系统。我们专注于这样一种情况，即来自一个系统的多个不同实例的数据可用，其潜在动力学模型在开始时完全不知道。该方法基于一个分离，即一种实例特定的编码（捕获初始条件、常数等）和一种潜在动态模型，该模型本身在系统的所有实例/实现中都是通用的。这种分离是以自动化、数据驱动的方式实现的，只需要经验数据作为模型的输入。该方法允许在任何连续时间有效地推断系统行为，但不需要显式的神经ODE公式，这使得它高效且高度可扩展。我们通过简单的理论分析和实验结果研究了这种行为。

    We propose a method for learning dynamical systems from high-dimensional empirical data that combines variational autoencoders and (spatio-)temporal attention within a framework designed to enforce certain scientifically-motivated invariances. We focus on the setting in which data are available from multiple different instances of a system whose underlying dynamical model is entirely unknown at the outset. The approach rests on a separation into an instance-specific encoding (capturing initial conditions, constants etc.) and a latent dynamics model that is itself universal across all instances/realizations of the system. The separation is achieved in an automated, data-driven manner and only empirical data are required as inputs to the model. The approach allows effective inference of system behaviour at any continuous time but does not require an explicit neural ODE formulation, which makes it efficient and highly scalable. We study behaviour through simple theoretical analyses and ex
    
[^17]: EquiformerV2: 改进的等变Transformer，用于扩展到更高次表示

    EquiformerV2: Improved Equivariant Transformer for Scaling to Higher-Degree Representations. (arXiv:2306.12059v1 [cs.LG])

    [http://arxiv.org/abs/2306.12059](http://arxiv.org/abs/2306.12059)

    本文提出了EquiformerV2，通过使用新的卷积类型和架构改进，扩展了等变Transformer到更高的等变表示，在处理大型数据集时表现更好，能量和力的表现也得到了提高，计算效率也得到了提升。

    

    等变Transformer（例如Equiformer）已经证明了将Transformer应用于3D原子系统领域的功效。但是，由于计算复杂性，它们仍然局限于小数次等变表示。在本文中，我们调查了这些架构是否能够很好地扩展到更高的次数。从Equiformer开始，我们首先用eSCN卷积替换了$SO(3)$卷积，以有效地合并更高次的张量。然后，为了更好地利用更高次的能力，我们提出了三个架构改进——注意力重标准化、可分离的$S^2$激活和可分离层归一化。将这一切放在一起，我们提出了EquiformerV2，在大型OC20数据集上的表现优于以前的最先进方法，在力上提高了最多$12\%$，能量上提高了$4\%$，提供更好的速度-准确性权衡，并且在计算吸附能所需的DFT计算量方面缩减了2倍。

    Equivariant Transformers such as Equiformer have demonstrated the efficacy of applying Transformers to the domain of 3D atomistic systems. However, they are still limited to small degrees of equivariant representations due to their computational complexity. In this paper, we investigate whether these architectures can scale well to higher degrees. Starting from Equiformer, we first replace $SO(3)$ convolutions with eSCN convolutions to efficiently incorporate higher-degree tensors. Then, to better leverage the power of higher degrees, we propose three architectural improvements -- attention re-normalization, separable $S^2$ activation and separable layer normalization. Putting this all together, we propose EquiformerV2, which outperforms previous state-of-the-art methods on the large-scale OC20 dataset by up to $12\%$ on forces, $4\%$ on energies, offers better speed-accuracy trade-offs, and $2\times$ reduction in DFT calculations needed for computing adsorption energies.
    
[^18]: 自然语言对抗攻击中的样本攻击能力研究

    Sample Attackability in Natural Language Adversarial Attacks. (arXiv:2306.12043v1 [cs.CL])

    [http://arxiv.org/abs/2306.12043](http://arxiv.org/abs/2306.12043)

    本文研究了自然语言处理领域中，哪些样本最易受到对抗攻击或最具鲁棒性的问题，并扩展了该领域中的“样本攻击能力/鲁棒性”的定义。实验发现，基于深度学习的检测器可以更好地识别出一个看不见的目标模型中最易受攻击和最具鲁棒性的样本。

    

    自然语言处理领域的对抗攻击研究已经在设计强大的攻击方法和防御方法方面取得了重大进展。然而，很少有研究努力去确定哪些源样本是最易受攻击或最具鲁棒性，即在一个看不见的目标模型上，我们是否可以确定哪些样本最容易受到对抗攻击。本文正式扩展了自然语言处理攻击中样本攻击能力/鲁棒性的定义。在两个流行的自然语言处理数据集、四种最先进的模型和四种不同的自然语言对抗攻击方法上的实验表明，样本不确定性不能充分描述攻击性/鲁棒性样本的特征，因此基于深度学习的检测器可以更好地识别出一个看不见的目标模型中最易受攻击和最具鲁棒性的样本。然而，进一步的分析发现，在不同的自然语言处理攻击方法中，对于哪些样本被认为是最易受攻击/最具鲁棒性，不存在明显的一致性。

    Adversarial attack research in natural language processing (NLP) has made significant progress in designing powerful attack methods and defence approaches. However, few efforts have sought to identify which source samples are the most attackable or robust, i.e. can we determine for an unseen target model, which samples are the most vulnerable to an adversarial attack. This work formally extends the definition of sample attackability/robustness for NLP attacks. Experiments on two popular NLP datasets, four state of the art models and four different NLP adversarial attack methods, demonstrate that sample uncertainty is insufficient for describing characteristics of attackable/robust samples and hence a deep learning based detector can perform much better at identifying the most attackable and robust samples for an unseen target model. Nevertheless, further analysis finds that there is little agreement in which samples are considered the most attackable/robust across different NLP attack 
    
[^19]: 定量节点基核韧性

    Quantifying Node-based Core Resilience. (arXiv:2306.12038v1 [cs.SI])

    [http://arxiv.org/abs/2306.12038](http://arxiv.org/abs/2306.12038)

    本文提出了一种名为依赖核韧性的新度量，用于衡量节点在插入和删除边的情况下的核韧性，以捕捉邻居节点和未来邻居节点对节点的影响，并在真实网络上展示了其优越性。

    

    核分解是各种图分析任务（如密集子图的发现和识别有影响力的节点）的高效构建块。其关键弱点是对图的变化非常敏感：插入或删除几条边就可能极大地改变图的核结构。因此，需要在全局和局部水平上表征、量化和，如果可能的话，提高给定图的核结构韧性。以前的研究大多考虑了整个图或其中重要子图的核韧性。本文研究了基于节点的核韧性，通过插入和删除边来度量。我们首先证明了以前提出的度量指标Core Strength不能正确地捕捉到节点在删除边的情况下的核韧性。接下来，我们引入依赖图的概念，以捕捉邻居节点（对于边删除）和可能的未来邻居节点（对于边插入）对节点核韧性的影响。基于依赖图，我们提出了一种新的基于节点的韧性度量，称为依赖核韧性。我们在各种真实网络上评估了新度量，并展示了它在捕捉节点真正核韧性方面优于Core Strength。

    Core decomposition is an efficient building block for various graph analysis tasks such as dense subgraph discovery and identifying influential nodes. One crucial weakness of the core decomposition is its sensitivity to changes in the graph: inserting or removing a few edges can drastically change the core structure of a graph. Hence, it is essential to characterize, quantify, and, if possible, improve the resilience of the core structure of a given graph in global and local levels. Previous works mostly considered the core resilience of the entire graph or important subgraphs in it. In this work, we study node-based core resilience measures upon edge removals and insertions. We first show that a previously proposed measure, Core Strength, does not correctly capture the core resilience of a node upon edge removals. Next, we introduce the concept of dependency graph to capture the impact of neighbor nodes (for edge removal) and probable future neighbor nodes (for edge insertion) on the 
    
[^20]: 一种半自回归图生成模型用于依存关系图解析

    A Semi-Autoregressive Graph Generative Model for Dependency Graph Parsing. (arXiv:2306.12018v1 [cs.CL])

    [http://arxiv.org/abs/2306.12018](http://arxiv.org/abs/2306.12018)

    提出了一种半自回归的依存解析器，成功捕获了节点和边缘之间的显式依赖关系，取得了最先进的结果。

    

    近年来，神经依存解析取得了令人瞩目的进展。根据对图联合概率的不同分解方法，现有解析器可以大致分为自回归和非自回归模式。然而，当将有向边视为显式依赖关系时，我们发现依存图中存在独立和相互依赖的组件混合，标志着先前的模型无法准确捕捉节点和边缘之间的显式依赖关系。基于这一特性，我们设计了一个半自回归的依存解析器，通过加入节点组和边组自回归地并将节点嵌入一次性的方式来生成依存图。我们的模型成功捕获了节点和边缘之间的显式依赖关系，并在多个基准数据集上实现了最先进的结果。

    Recent years have witnessed the impressive progress in Neural Dependency Parsing. According to the different factorization approaches to the graph joint probabilities, existing parsers can be roughly divided into autoregressive and non-autoregressive patterns. The former means that the graph should be factorized into multiple sequentially dependent components, then it can be built up component by component. And the latter assumes these components to be independent so that they can be outputted in a one-shot manner. However, when treating the directed edge as an explicit dependency relationship, we discover that there is a mixture of independent and interdependent components in the dependency graph, signifying that both aforementioned models fail to precisely capture the explicit dependencies among nodes and edges. Based on this property, we design a Semi-Autoregressive Dependency Parser to generate dependency graphs via adding node groups and edge groups autoregressively while pouring 
    
[^21]: 人工智能灾难性风险综述

    An Overview of Catastrophic AI Risks. (arXiv:2306.12001v1 [cs.CY])

    [http://arxiv.org/abs/2306.12001](http://arxiv.org/abs/2306.12001)

    本文综述了人工智能灾难性风险的四个主要来源，包括恶意使用、人工智能竞赛、组织风险和流氓人工智能。

    

    人工智能的快速发展引起了专家、政策制定者和世界各国领导人对越来越先进的人工智能系统可能带来灾难性风险的担忧。虽然已经有很多风险被单独详细介绍过，但迫切需要系统地讨论和说明潜在危险，以更好地支持减轻这些风险的努力。本文概述了人工智能灾难性风险的主要来源，我们将其分为四个类别：恶意使用，即个人或团体有意使用人工智能造成伤害；人工智能竞赛，即竞争环境促使行动者部署不安全的人工智能或放弃控制权交给人工智能；组织风险，突出人为和复杂系统如何增加灾难性事故发生的可能性；以及流氓人工智能，描述了控制比人类智能更高的代理程序困难的固有难题。对于每个风险类别，我们描述了具体的危害。

    Rapid advancements in artificial intelligence (AI) have sparked growing concerns among experts, policymakers, and world leaders regarding the potential for increasingly advanced AI systems to pose catastrophic risks. Although numerous risks have been detailed separately, there is a pressing need for a systematic discussion and illustration of the potential dangers to better inform efforts to mitigate them. This paper provides an overview of the main sources of catastrophic AI risks, which we organize into four categories: malicious use, in which individuals or groups intentionally use AIs to cause harm; AI race, in which competitive environments compel actors to deploy unsafe AIs or cede control to AIs; organizational risks, highlighting how human factors and complex systems can increase the chances of catastrophic accidents; and rogue AIs, describing the inherent difficulty in controlling agents far more intelligent than humans. For each category of risk, we describe specific hazards,
    
[^22]: TauPETGen：基于潜在扩散模型的文本有条件Tau PET图像合成

    TauPETGen: Text-Conditional Tau PET Image Synthesis Based on Latent Diffusion Models. (arXiv:2306.11984v1 [eess.IV])

    [http://arxiv.org/abs/2306.11984](http://arxiv.org/abs/2306.11984)

    本文提出了一种基于潜在扩散模型的新颖文本有条件Tau PET图像合成方法，可根据文本描述和被试的MR图像生成逼真的Tau PET图像，有助于研究不同测量之间的关系，并增加Tau PET数据集的公共可用性。

    

    本文提出了一种新颖的文本引导图像合成技术，可以根据文本描述和被试的MR图像生成逼真的Tau PET图像。生成的Tau PET图像有助于研究不同测量之间的关系，并增加Tau PET数据集的公共可用性。该方法基于潜在扩散模型，使用文本描述和被试的MR图像为条件进行图像生成。被试的MR图像可以提供解剖细节，而文本描述（如性别、扫描时间、认知测试分数和淀粉样蛋白状态）则可提供有关Tau神经纤维缠结可能沉积的位置的进一步指导。基于临床[18F]MK-6240数据集的初步实验结果表明，所提出的方法在生成不同临床阶段的逼真Tau PET图像方面具有可行性。

    In this work, we developed a novel text-guided image synthesis technique which could generate realistic tau PET images from textual descriptions and the subject's MR image. The generated tau PET images have the potential to be used in examining relations between different measures and also increasing the public availability of tau PET datasets. The method was based on latent diffusion models. Both textual descriptions and the subject's MR prior image were utilized as conditions during image generation. The subject's MR image can provide anatomical details, while the text descriptions, such as gender, scan time, cognitive test scores, and amyloid status, can provide further guidance regarding where the tau neurofibrillary tangles might be deposited. Preliminary experimental results based on clinical [18F]MK-6240 datasets demonstrate the feasibility of the proposed method in generating realistic tau PET images at different clinical stages.
    
[^23]: AdCraft：一种用于搜索引擎营销优化的高级强化学习基准环境

    AdCraft: An Advanced Reinforcement Learning Benchmark Environment for Search Engine Marketing Optimization. (arXiv:2306.11971v1 [cs.LG])

    [http://arxiv.org/abs/2306.11971](http://arxiv.org/abs/2306.11971)

    AdCraft是一种高级强化学习基准环境，用于模拟出价和预算变化的搜索引擎营销(SEM)活动，可用于评估和提高SEM出价和预算管理相关的RL算法的鲁棒性。

    

    本文介绍了一种新的强化学习基准环境—— AdCraft，其具有随机和非静态特性。该环境模拟了搜索引擎营销中出价和预算的动态变化。SEM是一种利用付费广告来增加网站在搜索引擎结果页面上的可见性的数字营销技术。SEM广告活动的表现取决于多个因素，包括关键字选择、广告设计、出价管理、预算调整和表现监控。深度强化学习最近被认为是一种优化SEM广告投放活动的潜在策略，但需要大量数据，在实践中可能成本高昂或不可行。我们的可定制环境使从业者能够评估和提高与SEM出价和预算管理相关的RL算法的鲁棒性，而无需付出这些成本。通过在AdCraft环境下进行一系列实验，

    We introduce \env{}, a novel benchmark environment for the Reinforcement Learning (RL) community distinguished by its stochastic and non-stationary properties. The environment simulates bidding and budgeting dynamics within Search Engine Marketing (SEM), a digital marketing technique utilizing paid advertising to enhance the visibility of websites on search engine results pages (SERPs). The performance of SEM advertisement campaigns depends on several factors, including keyword selection, ad design, bid management, budget adjustments, and performance monitoring. Deep RL recently emerged as a potential strategy to optimize campaign profitability within the complex and dynamic landscape of SEM but it requires substantial data, which may be costly or infeasible to acquire in practice. Our customizable environment enables practitioners to assess and enhance the robustness of RL algorithms pertinent to SEM bid and budget management without such costs. Through a series of experiments within 
    
[^24]: 论如何处理有噪音的计算问题的最优界限

    On the Optimal Bounds for Noisy Computing. (arXiv:2306.11951v1 [cs.DS])

    [http://arxiv.org/abs/2306.11951](http://arxiv.org/abs/2306.11951)

    本文改进了有噪音计算问题的自适应采样和非自适应采样模型下所有四个函数的下界，并且大多数下界与上界符合，差异不超过一个常数因子。

    

    我们重新考虑了Feige等人1994年考虑的使用有噪声的信息进行计算的问题，其中包括从有噪声的查询中计算广义 OR 函数，以及从有噪声的成对比较中计算 MAX、SEARCH 和 SORT 函数。对于给定的 $K$ 个元素，目标是在每个查询的结果以概率 $p$ 翻转时，以至少 $1-\delta$ 的概率正确地恢复所需函数。我们考虑了自适应采样设置和非自适应采样设置。先前的工作提供了关于最坏情况下查询复杂度的紧密边界，这些边界是以 $K$ 依赖性为基础的。然而，在依赖于 $\delta$ 和 $p$ 方面，上下界并不匹配。我们提高了自适应和非自适应查询模型下所有四个函数的下界。我们的大多数下界与上界符合，差异不超过一个常数因子。

    We revisit the problem of computing with noisy information considered in Feige et al. 1994, which includes computing the OR function from noisy queries, and computing the MAX, SEARCH and SORT functions from noisy pairwise comparisons. For $K$ given elements, the goal is to correctly recover the desired function with probability at least $1-\delta$ when the outcome of each query is flipped with probability $p$. We consider both the adaptive sampling setting where each query can be adaptively designed based on past outcomes, and the non-adaptive sampling setting where the query cannot depend on past outcomes. The prior work provides tight bounds on the worst-case query complexity in terms of the dependence on $K$. However, the upper and lower bounds do not match in terms of the dependence on $\delta$ and $p$. We improve the lower bounds for all the four functions under both adaptive and non-adaptive query models. Most of our lower bounds match the upper bounds up to constant factors when
    
[^25]: 自适应集成 Q 学习：通过误差反馈来减小估计偏差

    Adaptive Ensemble Q-learning: Minimizing Estimation Bias via Error Feedback. (arXiv:2306.11918v1 [cs.LG])

    [http://arxiv.org/abs/2306.11918](http://arxiv.org/abs/2306.11918)

    本研究提出了自适应集成 Q 学习算法，通过误差反馈来减小集成方法中估计偏差的影响，并结合模型识别自适应控制（MIAC）来实现集成大小自适应。

    

    集成方法是缓解 Q-learning 中过度估计问题的一种有效方法，其中使用多个函数逼近器来估计动作值。估计偏误严重依赖于集成大小（即目标中使用的 Q 函数逼近器数量），因此决定“正确”的集成大小非常困难，因为在学习过程中函数逼近误差会发生变化。针对这一挑战，我们首先根据估计偏差导出了上限和下限，并根据这些界对集成大小进行自适应，从而将偏差驱动到接近零，因此可以相应地处理时间变化逼近误差的影响。我们将集成方法与模型识别自适应控制（MIAC）相结合，提出了自适应集成 Q 学习（AdaEQ），是一种广义的 Q 学习算法，它可以有效地自适应集成大小。

    The ensemble method is a promising way to mitigate the overestimation issue in Q-learning, where multiple function approximators are used to estimate the action values. It is known that the estimation bias hinges heavily on the ensemble size (i.e., the number of Q-function approximators used in the target), and that determining the `right' ensemble size is highly nontrivial, because of the time-varying nature of the function approximation errors during the learning process. To tackle this challenge, we first derive an upper bound and a lower bound on the estimation bias, based on which the ensemble size is adapted to drive the bias to be nearly zero, thereby coping with the impact of the time-varying approximation errors accordingly. Motivated by the theoretic findings, we advocate that the ensemble method can be combined with Model Identification Adaptive Control (MIAC) for effective ensemble size adaptation. Specifically, we devise Adaptive Ensemble Q-learning (AdaEQ), a generalized 
    
[^26]: 图分类问题中结构感知的鲁棒性认证

    Structure-Aware Robustness Certificates for Graph Classification. (arXiv:2306.11915v1 [cs.LG])

    [http://arxiv.org/abs/2306.11915](http://arxiv.org/abs/2306.11915)

    该论文提出了一种新的随机平滑方法，可以根据图的不同预定义结构生成对应的鲁棒性证书，从而在多个基准图分类任务中取得领先的对抗性攻击下鲁棒性结果。

    

    对于基于图的机器学习模型进行鲁棒性认证是保证安全性的一个至关重要的挑战。目前用于图分类器的鲁棒性证明保证与节点对翻转（添加或删除边缘）的总数有关，这相当于以邻接矩阵为中心的l0球。尽管从理论上看很有吸引力，但这种各向同性的结构噪声在实际场景中可能过于严格，因为有些节点对于确定分类器的输出更为关键。在这种情况下，证书给出了对图模型鲁棒性的悲观描述。为了解决这个问题，我们开发了一种基于随机平滑的方法，将非各向同性的噪声分布添加到输入图结构中。我们展示了我们的过程为分类器生成了结构感知的证书，因此鲁棒性证书的大小可以在图的不同预定义结构之间变化。我们在几个基准图分类任务上展示了我们方法的优势，在对抗性攻击的鲁棒性方面取得了最先进的结果。

    Certifying the robustness of a graph-based machine learning model poses a critical challenge for safety. Current robustness certificates for graph classifiers guarantee output invariance with respect to the total number of node pair flips (edge addition or edge deletion), which amounts to an $l_{0}$ ball centred on the adjacency matrix. Although theoretically attractive, this type of isotropic structural noise can be too restrictive in practical scenarios where some node pairs are more critical than others in determining the classifier's output. The certificate, in this case, gives a pessimistic depiction of the robustness of the graph model. To tackle this issue, we develop a randomised smoothing method based on adding an anisotropic noise distribution to the input graph structure. We show that our process generates structural-aware certificates for our classifiers, whereby the magnitude of robustness certificates can vary across different pre-defined structures of the graph. We demon
    
[^27]: 无法解释的解释：解释tSNE和UMAP嵌入的方法

    Unexplainable Explanations: Towards Interpreting tSNE and UMAP Embeddings. (arXiv:2306.11898v1 [cs.LG])

    [http://arxiv.org/abs/2306.11898](http://arxiv.org/abs/2306.11898)

    本文研究旨在回答ARDR算法的收敛性问题，我们将ARDR方法与传统的降维技术联系起来，可以将PCA嵌入完全恢复，通过稍加修改可以用LLE复现ARDR嵌入，并形式化了一系列猜想，如果成立，可以将2D嵌入中的结构归因于输入分布。

    

    将神经网络的潜在空间解释为吸引/排斥降维（ARDR）方法（如tSNE和UMAP）已成为标准。这取决于2D表示中的结构与模型潜在空间中的结构一致的先决条件。然而，这是一个未经证明的假设，我们不知道ARDR算法是否有任何收敛保证。本文旨在通过将ARDR方法与传统的降维技术联系起来，来回答这个问题。具体来说，我们展示了可以通过在随机初始化的数据集上应用吸引和排斥来完全恢复PCA嵌入。我们还展示了如果稍加修改，局部线性嵌入（LLE）可以复现ARDR嵌入的方法。最后，我们系统化了一系列猜想，如果这些猜想成立，就可以将2D嵌入中的结构归因于输入分布。

    It has become standard to explain neural network latent spaces with attraction/repulsion dimensionality reduction (ARDR) methods like tSNE and UMAP. This relies on the premise that structure in the 2D representation is consistent with the structure in the model's latent space. However, this is an unproven assumption -- we are unaware of any convergence guarantees for ARDR algorithms. We work on closing this question by relating ARDR methods to classical dimensionality reduction techniques. Specifically, we show that one can fully recover a PCA embedding by applying attractions and repulsions onto a randomly initialized dataset. We also show that, with a small change, Locally Linear Embeddings (LLE) can reproduce ARDR embeddings. Finally, we formalize a series of conjectures that, if true, would allow one to attribute structure in the 2D embedding back to the input distribution.
    
[^28]: 基于强化学习的液压建筑机器人远程操作虚拟装置

    Reinforcement Learning-based Virtual Fixtures for Teleoperation of Hydraulic Construction Machine. (arXiv:2306.11897v1 [cs.RO])

    [http://arxiv.org/abs/2306.11897](http://arxiv.org/abs/2306.11897)

    本文提出了一种基于强化学习的方法来优化施工机器人任务性能。实验表明该方法在典型施工任务中的表现良好。

    

    远程操纵施工设备是建筑行业的重要方面，能够实现远距离安全操纵机器人。然而，使用手柄进行关节级别的远程操作需要经过长时间的培训才能熟练掌握，同时，由于机器的多自由度特性，机器的运动只能在执行后进行验证，使得优化控制具有挑战性。针对这个问题，本研究提出了一种基于强化学习的方法来优化任务性能。通过学习获得的控制策略用于提供关节的高效控制和协调的任务指令。为了评估所提出框架的有效性，使用Brokk 170施工机进行用户研究，评估其在典型施工任务（将凿子插入钻孔中）中的表现。实验结果表明，所提出的框架具有实用性。

    The utilization of teleoperation is a crucial aspect of the construction industry, as it enables operators to control machines safely from a distance. However, remote operation of these machines at a joint level using individual joysticks necessitates extensive training for operators to achieve proficiency due to their multiple degrees of freedom. Additionally, verifying the machine resulting motion is only possible after execution, making optimal control challenging. In addressing this issue, this study proposes a reinforcement learning-based approach to optimize task performance. The control policy acquired through learning is used to provide instructions on efficiently controlling and coordinating multiple joints. To evaluate the effectiveness of the proposed framework, a user study is conducted with a Brokk 170 construction machine by assessing its performance in a typical construction task involving inserting a chisel into a borehole. The effectiveness of the proposed framework is
    
[^29]: 单细胞显微镜下，干预风格转移实现外域泛化

    Out of Distribution Generalization via Interventional Style Transfer in Single-Cell Microscopy. (arXiv:2306.11890v1 [cs.CV])

    [http://arxiv.org/abs/2306.11890](http://arxiv.org/abs/2306.11890)

    本文提出了一种新的方法干预风格转移（IST），通过生成干预训练分布，从而显着改善了单细胞显微镜下外域泛化的问题。

    

    计算机视觉技术的真实世界应用，包括生物医学研究中的发现过程，需要对上下文干扰具有不变性和对新数据具有泛化性的因果表征。利用两个新的单细胞荧光显微镜数据集的内在复制结构，我们提出了一般适用的测试方法，以评估模型在越来越具挑战性的OOD泛化水平上学习因果表征的程度。我们发现，尽管根据其他已建立的度量衡量，既有的天真基线设计用于防止混淆，但这些基线在这些测试中都会失效。我们引入了一种新的方法，干预风格转移（IST），通过生成干预训练分布，在其中减轻生物原因和干扰因素之间的虚假相关性，从而显着改善了OOD泛化性能。我们发布了我们的代码和数据集。

    Real-world deployment of computer vision systems, including in the discovery processes of biomedical research, requires causal representations that are invariant to contextual nuisances and generalize to new data. Leveraging the internal replicate structure of two novel single-cell fluorescent microscopy datasets, we propose generally applicable tests to assess the extent to which models learn causal representations across increasingly challenging levels of OOD-generalization. We show that despite seemingly strong performance, as assessed by other established metrics, both naive and contemporary baselines designed to ward against confounding, collapse on these tests. We introduce a new method, Interventional Style Transfer (IST), that substantially improves OOD generalization by generating interventional training distributions in which spurious correlations between biological causes and nuisances are mitigated. We publish our code and datasets.
    
[^30]: SPRINT：通过语言指令 relabeling 实现可扩展的策略预训练

    SPRINT: Scalable Policy Pre-Training via Language Instruction Relabeling. (arXiv:2306.11886v1 [cs.RO])

    [http://arxiv.org/abs/2306.11886](http://arxiv.org/abs/2306.11886)

    SPRINT 提出了一种离线策略预训练方法，通过指令重标记及离线强化学习实现可扩展的预训练任务，大大减少了预训练所需的人力，同时使机器人能够获取更丰富的技能库，相较于之前的预训练方法，能够更快地学习新的长时间跨度任务。

    

    预训练机器人策略并赋予丰富的技能集合可以大大加速下游任务的学习。先前的研究通过自然语言指令定义预训练任务，但这需要人为地注释数十万个指令。因此，我们提出了 SPRINT，这是一种可扩展的离线策略预训练方法，可大大减少预训练多样的技能所需的人力。我们的方法使用两个核心想法来自动扩展基础预训练任务：通过大型语言模型来进行指令重标记和通过离线强化学习进行交叉轨迹技能链接。因此，SPRINT 预训练可以为机器人装备更丰富的技能库。在家庭模拟器和真实机器人厨房操作任务中的实验结果表明，SPRINT 相对于之前的预训练方法能够更快地学习新的长时间跨度任务。

    Pre-training robot policies with a rich set of skills can substantially accelerate the learning of downstream tasks. Prior works have defined pre-training tasks via natural language instructions, but doing so requires tedious human annotation of hundreds of thousands of instructions. Thus, we propose SPRINT, a scalable offline policy pre-training approach which substantially reduces the human effort needed for pre-training a diverse set of skills. Our method uses two core ideas to automatically expand a base set of pre-training tasks: instruction relabeling via large language models and cross-trajectory skill chaining through offline reinforcement learning. As a result, SPRINT pre-training equips robots with a much richer repertoire of skills. Experimental results in a household simulator and on a real robot kitchen manipulation task show that SPRINT leads to substantially faster learning of new long-horizon tasks than previous pre-training approaches. Website at https://clvrai.com/spr
    
[^31]: 强化学习中的扩散过程奖励塑形

    Reward Shaping via Diffusion Process in Reinforcement Learning. (arXiv:2306.11885v1 [cs.LG])

    [http://arxiv.org/abs/2306.11885](http://arxiv.org/abs/2306.11885)

    本研究使用扩散过程来进行奖励塑形，提供了解决探索 - 利用权衡的优雅框架。同时，阐明了信息熵、随机系统动力学以及它们对熵产生的影响之间的关系。

    

    强化学习（RL）模型不断发展，以在不确定的马尔可夫决策过程（MDP）中平衡探索和利用。本研究利用随机热力学和系统动力学原理，通过扩散过程探索奖励塑形。这提供了一个优雅的框架来思考探索 - 利用权衡。本文阐明了信息熵，随机系统动力学及其对熵产生的影响之间的关系。此探索使我们可以构建一个双重框架，可以解释为派生有效策略的最大熵程序，或者是计算信息成本和收益的修正成本优化程序。这项工作提出了信息的物理本质及其对MDP中的在线学习的影响的新视角，从而更好地理解RL中的信息取向公式。

    Reinforcement Learning (RL) models have continually evolved to navigate the exploration - exploitation trade-off in uncertain Markov Decision Processes (MDPs). In this study, I leverage the principles of stochastic thermodynamics and system dynamics to explore reward shaping via diffusion processes. This provides an elegant framework as a way to think about exploration-exploitation trade-off. This article sheds light on relationships between information entropy, stochastic system dynamics, and their influences on entropy production. This exploration allows us to construct a dual-pronged framework that can be interpreted as either a maximum entropy program for deriving efficient policies or a modified cost optimization program accounting for informational costs and benefits. This work presents a novel perspective on the physical nature of information and its implications for online learning in MDPs, consequently providing a better understanding of information-oriented formulations in RL
    
[^32]: 积极协作的人机装配：利用人类意图预测和鲁棒安全控制

    Proactive Human-Robot Co-Assembly: Leveraging Human Intention Prediction and Robust Safe Control. (arXiv:2306.11862v1 [cs.RO])

    [http://arxiv.org/abs/2306.11862](http://arxiv.org/abs/2306.11862)

    本文提出了一个用于积极人机协作的集成框架，通过鲁棒的意图预测和安全控制来提高协作效率和避免交互安全风险。

    

    人机协作是实现灵活制造以满足不同客户需求的关键组成部分。然而，由于多种挑战，构建能够在安全高效地协助人类的智能机器人很困难。本文提出了一个用于积极人机协作的集成框架。该框架包括了一个鲁棒的意图预测模块和一个鲁棒的安全控制模块，分别用于提高协作效率和避免交互安全风险。通过在一个Kinova Gen3机器人上执行协作任务，实验结果表明了该解决方案对于环境变化和不同人类行为具有鲁棒性。

    Human-robot collaboration (HRC) is one key component to achieving flexible manufacturing to meet the different needs of customers. However, it is difficult to build intelligent robots that can proactively assist humans in a safe and efficient way due to several challenges.First, it is challenging to achieve efficient collaboration due to diverse human behaviors and data scarcity. Second, it is difficult to ensure interactive safety due to uncertainty in human behaviors. This paper presents an integrated framework for proactive HRC. A robust intention prediction module, which leverages prior task information and human-in-the-loop training, is learned to guide the robot for efficient collaboration. The proposed framework also uses robust safe control to ensure interactive safety under uncertainty. The developed framework is applied to a co-assembly task using a Kinova Gen3 robot. The experiment demonstrates that our solution is robust to environmental changes as well as different human p
    
[^33]: 在多智能体环境下发现因果关系以实现高效协作

    Discovering Causality for Efficient Cooperation in Multi-Agent Environments. (arXiv:2306.11846v1 [cs.AI])

    [http://arxiv.org/abs/2306.11846](http://arxiv.org/abs/2306.11846)

    本文研究了因果关系在多智能体强化学习中的应用，通过对懒惰智能体进行惩罚以帮助团队取得更好的效果。此外，研究了如何利用因果关系估计改善对智能体的信用分配，以及如何使用Amortized Causal Discovery自动检测多智能体环境中的因果关系。

    

    在合作的多智能体强化学习中，智能体需要作为一个团队学习行为以实现共同的目标。然而，在学习任务的过程中，一些智能体可能会学习到次优策略，从而未能对团队目标做出贡献。这些智能体被称为“懒惰智能体”，因为它们的非合作性行为可能来自于未能理解是否导致回报。本文中，我们调查了因果关系在合作学习中的应用，以及如何应用它来惩罚这些懒惰的智能体。我们观察到，因果关系估计可用于改进对智能体的信用分配，并展示了如何利用它来改进多智能体独立学习。此外，我们还研究了如何使用分摊因果发现来自动检测多智能体环境中的因果关系。

    In cooperative Multi-Agent Reinforcement Learning (MARL) agents are required to learn behaviours as a team to achieve a common goal. However, while learning a task, some agents may end up learning sub-optimal policies, not contributing to the objective of the team. Such agents are called lazy agents due to their non-cooperative behaviours that may arise from failing to understand whether they caused the rewards. As a consequence, we observe that the emergence of cooperative behaviours is not necessarily a byproduct of being able to solve a task as a team. In this paper, we investigate the applications of causality in MARL and how it can be applied in MARL to penalise these lazy agents. We observe that causality estimations can be used to improve the credit assignment to the agents and show how it can be leveraged to improve independent learning in MARL. Furthermore, we investigate how Amortized Causal Discovery can be used to automate causality detection within MARL environments. The r
    
[^34]: 基于检索的Transformer模型用于表格增强

    Retrieval-Based Transformer for Table Augmentation. (arXiv:2306.11843v1 [cs.CL])

    [http://arxiv.org/abs/2306.11843](http://arxiv.org/abs/2306.11843)

    本文提出了一种自动数据处理的新方法，其中使用基于检索的Transformer模型来解决表格增强任务，并采用自学习策略来训练模型以重构原始值或标题，以便减轻数据分析师在数据处理中的工作量。

    

    数据准备（也称数据整理）通常被认为是进行分析或构建机器学习模型时最耗费时间和精力的步骤之一。本文引入了一种自动数据处理的新方法，以试图减轻最终用户（例如数据分析师）在从数据湖中构建动态表格数据的过程中的工作量。我们旨在解决表格增强任务，包括行/列填充和数据插补。给定一组表格，我们提出了一种检索增强的自学习Transformer模型。我们的自学习策略是从语料库中随机去除表格，并训练检索模型以在给定部分表格作为输入的情况下重构原始值或标题。我们采用这种策略来首先训练密集的神经检索模型

    Data preparation, also called data wrangling, is considered one of the most expensive and time-consuming steps when performing analytics or building machine learning models. Preparing data typically involves collecting and merging data from complex heterogeneous, and often large-scale data sources, such as data lakes. In this paper, we introduce a novel approach toward automatic data wrangling in an attempt to alleviate the effort of end-users, e.g. data analysts, in structuring dynamic views from data lakes in the form of tabular data. We aim to address table augmentation tasks, including row/column population and data imputation. Given a corpus of tables, we propose a retrieval augmented self-trained transformer model. Our self-learning strategy consists in randomly ablating tables from the corpus and training the retrieval-based model to reconstruct the original values or headers given the partial tables as input. We adopt this strategy to first train the dense neural retrieval mode
    
[^35]: 任何深度ReLU网络都是浅层网络

    Any Deep ReLU Network is Shallow. (arXiv:2306.11827v1 [cs.LG])

    [http://arxiv.org/abs/2306.11827](http://arxiv.org/abs/2306.11827)

    该论文证明了任何深度的ReLU网络都可以被重写为一个具有透明性的浅层网络。这一结论有助于解释模型行为。

    

    我们构造性地证明了每个深度的ReLU网络可以被重写为一个函数上等价的三层网络，其中权重值为延迟实数。基于此证明，我们提供了一个算法，可以给出一个深度ReLU网络对应的显式权重。由此得到的浅层网络是透明的，并用于生成模型行为的解释。

    We constructively prove that every deep ReLU network can be rewritten as a functionally identical three-layer network with weights valued in the extended reals. Based on this proof, we provide an algorithm that, given a deep ReLU network, finds the explicit weights of the corresponding shallow network. The resulting shallow network is transparent and used to generate explanations of the model s behaviour.
    
[^36]: 学会生成比你的LMM更好的文本

    Learning to Generate Better Than Your LLM. (arXiv:2306.11816v1 [cs.LG])

    [http://arxiv.org/abs/2306.11816](http://arxiv.org/abs/2306.11816)

    本论文研究了基于强化学习算法 RLGF，用于在 GPT-3 等动态黑匣子指导下微调大型语言模型 LLM 的条件文本生成，相比通用 RL 算法，该算法在 IMDB 和 CommonGen 任务中表现更好。

    

    强化学习(RL)已经成为一种强大的范例，用于优化大型语言模型 (LLM) 条件文本生成。特别地，最近的LLM，如ChatGPT和GPT - 4能够与用户进行流畅的对话，并融合了RL和人类反馈。本研究受到学习搜索算法的启发，并利用文本生成的关键特性，探索了超出通用RL算法如PPO之外的强化学习算法。特别地，我们扩展了RL算法，使其能够与动态黑匣子的指导LLM如GPT-3进行交互，并提出了具有引导反馈的RL(RLGF)，这是一套用于LLM微调的RL算法。我们在GRUE基准测试的IMDB正向评论和CommonGen文本生成任务上进行了实验。我们展示了我们的RL算法比监督学习(SL)和默认PPO基线表现更高，证明了与指导LLM互动的好处。

    Reinforcement learning (RL) has emerged as a powerful paradigm for fine-tuning Large Language Models (LLMs) for conditional text generation. In particular, recent LLMs such as ChatGPT and GPT-4 can engage in fluent conversations with users by incorporating RL and feedback from humans. Inspired by learning-to-search algorithms and capitalizing on key properties of text generation, we seek to investigate reinforcement learning algorithms beyond general purpose algorithms such as Proximal policy optimization (PPO). In particular, we extend RL algorithms to allow them to interact with a dynamic black-box guide LLM such as GPT-3 and propose RL with guided feedback (RLGF), a suite of RL algorithms for LLM fine-tuning. We experiment on the IMDB positive review and CommonGen text generation task from the GRUE benchmark. We show that our RL algorithms achieve higher performance than supervised learning (SL) and default PPO baselines, demonstrating the benefit of interaction with the guide LLM. 
    
[^37]: 解释性预测机器学习工件的设计：方法和实用演示

    Designing Explainable Predictive Machine Learning Artifacts: Methodology and Practical Demonstration. (arXiv:2306.11771v1 [cs.SE])

    [http://arxiv.org/abs/2306.11771](http://arxiv.org/abs/2306.11771)

    本文提出了一个解释性预测机器学习工件的设计方法学，以解决决策者不愿使用现代机器学习算法应用程序的问题，该方法学包括五个阶段。通过设计并实施一个工件，预测诊断为新冠肺炎的患者是否将来会出现花粉热症状来证明其适用性。

    

    面向预测的机器学习在组织中越来越有价值，因为它可以在重要的业务领域推动应用。然而，来自各行各业公司的决策者仍然极为不愿意雇用基于现代机器学习算法的应用程序。我们把这个问题归因于对于先进的机器学习算法普遍持有的观点，即它们是“黑盒子”，其复杂性不允许揭示驱动相应系统输出的因素。为了有助于克服这种采用障碍，我们认为信息系统研究应更多地关注设计能向人类决策者解释其预测的原型预测导向机器学习应用程序（即工件）。然而，尽管最近出现了许多工具来促进这种工件的开发，但迄今为止对其开发的研究还很少。我们认为这种研究空白是由于缺乏已建立的设计方法学，因此在本文中提出这样的方法学。我们的方法学包括五个阶段，引导工程师完成工件的调查、规范、设计、验证和实施。我们通过设计并实施一个工件来证明其适用性，该工件预测诊断为新冠肺炎的患者是否将来会出现花粉热症状。

    Prediction-oriented machine learning is becoming increasingly valuable to organizations, as it may drive applications in crucial business areas. However, decision-makers from companies across various industries are still largely reluctant to employ applications based on modern machine learning algorithms. We ascribe this issue to the widely held view on advanced machine learning algorithms as "black boxes" whose complexity does not allow for uncovering the factors that drive the output of a corresponding system. To contribute to overcome this adoption barrier, we argue that research in information systems should devote more attention to the design of prototypical prediction-oriented machine learning applications (i.e., artifacts) whose predictions can be explained to human decision-makers. However, despite the recent emergence of a variety of tools that facilitate the development of such artifacts, there has so far been little research on their development. We attribute this research g
    
[^38]: 一种用于自动化系统中人工智能应用的图形建模语言

    A Graphical Modeling Language for Artificial Intelligence Applications in Automation Systems. (arXiv:2306.11767v1 [cs.AI])

    [http://arxiv.org/abs/2306.11767](http://arxiv.org/abs/2306.11767)

    本论文介绍了一种图形建模语言，使得在自动化系统中对人工智能应用进行一致和可理解的建模，并将各个子领域分成特定领域的子系统，从而减少现有工作量。

    

    自动化系统中的人工智能应用通常是分布式系统，其开发和集成涉及多个专家。每个专家使用自己的领域特定建模语言和工具来模拟系统元素。目前尚不存在一种跨学科的图形建模语言，能够将人工智能应用作为整体系统进行建模，使所有学科都能够理解。因此，本论文提出了一种图形建模语言，能够在系统级别上对自动化系统中的人工智能应用进行一致和可理解的建模。这使得可以将各个子领域分成特定领域的子系统，从而减少现有的工作量。

    Artificial Intelligence (AI) applications in automation systems are usually distributed systems whose development and integration involve several experts. Each expert uses its own domain-specific modeling language and tools to model the system elements. An interdisciplinary graphical modeling language that enables the modeling of an AI application as an overall system comprehensible to all disciplines does not yet exist. As a result, there is often a lack of interdisciplinary system understanding, leading to increased development, integration, and maintenance efforts. This paper therefore presents a graphical modeling language that enables consistent and understandable modeling of AI applications in automation systems at system level. This makes it possible to subdivide individual subareas into domain specific subsystems and thus reduce the existing efforts.
    
[^39]: 学习与进化：影响有效结合的因素。

    Learning and evolution: factors influencing an effective combination. (arXiv:2306.11761v1 [cs.NE])

    [http://arxiv.org/abs/2306.11761](http://arxiv.org/abs/2306.11761)

    本文探讨了学习和进化相结合是否能够找到比单独进化发现更好的解决方案，而研究结果表明，当在学习和选择过程中引入噪声等特定条件时，这种组合才能取得成功。

    

    在人工智能和神经进化社区中，进化和学习之间的相互关系是一个有争议的论点。本文作者探讨了将学习和进化相结合是否能够找到比单独进化发现更好的解决方案的问题，并呈现了一系列实证研究，突出了一些特定条件，如在学习和选择过程中引入噪声等，这些条件决定了这种组合的成功。结果在两个定性不同的领域中获得，其中代理/环境交互是最小或不存在的。

    The mutual relationship between evolution and learning is a controversial argument among the artificial intelligence and neuro-evolution communities. After more than three decades, there is still no common agreement on the matter. In this paper the author investigates whether combining learning and evolution permits to find better solutions than those discovered by evolution alone. More specifically, the author presents a series of empirical studies that highlight some specific conditions determining the success of such a combination, like the introduction of noise during the learning and selection processes. Results are obtained in two qualitatively different domains, where agent/environment interactions are minimal or absent.
    
[^40]: 自主驾驶中的深度学习加速器循环可靠性评估

    Deep Learning Accelerator in Loop Reliability Evaluation for Autonomous Driving. (arXiv:2306.11759v1 [cs.AI])

    [http://arxiv.org/abs/2306.11759](http://arxiv.org/abs/2306.11759)

    本论文提出了一个DLA-in-loop可靠性评估平台，用于在早期DLA设计阶段评估自主驾驶系统中使用的DLA的总故障前行驶距离等高级可靠性指标，从而避免高昂的设计迭代成本。

    

    自主驾驶系统中使用的深度学习加速器(DLA)的可靠性对系统安全有着重大影响。然而，DLA的可靠性通常通过低级指标（如输出的均方误差）进行评估，这些指标与自主驾驶中的总故障前行驶距离等高级指标仍有很大差异。因此，在后硅阶段评估的高级可靠性指标可能仍会导致DLA设计修订，进而导致针对自主驾驶的昂贵可靠的DLA设计迭代。为解决这一问题，我们提出了一个DLA循环可靠性评估平台，以便在早期DLA设计阶段评估系统可靠性。

    The reliability of deep learning accelerators (DLAs) used in autonomous driving systems has significant impact on the system safety. However, the DLA reliability is usually evaluated with low-level metrics like mean square errors of the output which remains rather different from the high-level metrics like total distance traveled before failure in autonomous driving. As a result, the high-level reliability metrics evaluated at the post-silicon stage may still lead to DLA design revision and result in expensive reliable DLA design iterations targeting at autonomous driving. To address the problem, we proposed a DLA-in-loop reliability evaluation platform to enable system reliability evaluation at the early DLA design stage.
    
[^41]: MRFI：神经网络处理的开源多分辨率故障注入框架

    MRFI: An Open Source Multi-Resolution Fault Injection Framework for Neural Network Processing. (arXiv:2306.11758v1 [cs.LG])

    [http://arxiv.org/abs/2306.11758](http://arxiv.org/abs/2306.11758)

    MRFI是一个高度可配置的神经网络故障注入工具，用户可以修改独立的故障配置文件进行注入和漏洞分析。

    

    为了确保即使在不可靠的硬件上也能进行有弹性的神经网络处理，通常需要在深度神经网络模型部署之前进行各种硬件故障的全面可靠性分析，并且需要高效的错误注入工具。然而，大多数现有的故障注入工具仍然局限于对神经元的基本故障注入，并未提供细粒度漏洞分析能力。此外，许多故障注入工具仍需要更改神经网络模型并使故障注入与正常神经网络处理紧密耦合，这进一步增加了故障注入工具的使用难度并减慢了故障模拟。在这项工作中，我们提出了一个高度可配置的深度神经网络多分辨率故障注入工具MRFI。它使用户能够修改独立的故障配置文件，而不是修改神经网络模型进行故障注入和漏洞分析。

    To ensure resilient neural network processing on even unreliable hardware, comprehensive reliability analysis against various hardware faults is generally required before the deep neural network models are deployed, and efficient error injection tools are highly demanded. However, most existing fault injection tools remain rather limited to basic fault injection to neurons and fail to provide fine-grained vulnerability analysis capability. In addition, many of the fault injection tools still need to change the neural network models and make the fault injection closely coupled with normal neural network processing, which further complicates the use of the fault injection tools and slows down the fault simulation. In this work, we propose MRFI, a highly configurable multi-resolution fault injection tool for deep neural networks. It enables users to modify an independent fault configuration file rather than neural network models for the fault injection and vulnerability analysis. Particul
    
[^42]: 关于条件因果效应的可识别性

    On Identifiability of Conditional Causal Effects. (arXiv:2306.11755v1 [cs.AI])

    [http://arxiv.org/abs/2306.11755](http://arxiv.org/abs/2306.11755)

    本文研究了任意条件因果效应的可识别性问题，并通过提供完备算法证明了 Pearl 的 do-演算法对于这个问题是完备的。该工作对现有结果进行了扩展，并提供了对条件因果效应可识别性的更完整的理解。

    

    本文研究了在给定因果图和某些形式为 $Q[S]:=P(S|do(V\setminus S))$ 的观测和/或干预分布时，任意条件因果效应的可识别性问题。我们称此问题为条件广义可识别性问题（c-gID），并通过提供 c-gID 问题的完备算法证明了 Pearl 的 do-演算法对于 c-gID 问题的完备性。本文通过明确加入了可识别性所必需的正性假定，对 Lee 等人（2020）、Correa 等人（2021）中的 c-gID 问题进行了重新讨论。本文扩展了 Lee 等人（2019）、Kivva 等人（2022）的关于一般可识别性（gID）的研究，后者研究了无条件因果效应的问题，并扩展了 Shpitser 和 Pearl（2006b）的研究，后者仅给出观测分布 $P(\mathbf{V})$ 的条件下条件因果效应的可识别性。

    We address the problem of identifiability of an arbitrary conditional causal effect given both the causal graph and a set of any observational and/or interventional distributions of the form $Q[S]:=P(S|do(V\setminus S))$, where $V$ denotes the set of all observed variables and $S\subseteq V$. We call this problem conditional generalized identifiability (c-gID in short) and prove the completeness of Pearl's $do$-calculus for the c-gID problem by providing sound and complete algorithm for the c-gID problem. This work revisited the c-gID problem in Lee et al. [2020], Correa et al. [2021] by adding explicitly the positivity assumption which is crucial for identifiability. It extends the results of [Lee et al., 2019, Kivva et al., 2022] on general identifiability (gID) which studied the problem for unconditional causal effects and Shpitser and Pearl [2006b] on identifiability of conditional causal effects given merely the observational distribution $P(\mathbf{V})$ as our algorithm generaliz
    
[^43]: 操纵问题：对于认知代理的AI对话系统构成的威胁

    The Manipulation Problem: Conversational AI as a Threat to Epistemic Agency. (arXiv:2306.11748v1 [cs.HC])

    [http://arxiv.org/abs/2306.11748](http://arxiv.org/abs/2306.11748)

    AI对话系统的发展可能带来操纵威胁，例如影响购买选择、错误信仰或泄露个人数据。本文探讨其操纵策略，提出一套认知规定以保护个人免受不知不觉操纵。

    

    与话术AI技术在过去18个月内已经取得显著进展。因此，可能会在不久的将来部署用于追求针对性影响目标的对话代理。有时被称为“AI操纵问题”，这种新兴的风险是，消费者将不知不觉地与有技巧地说服他们购买特定产品、相信特定错误信息或愚弄他们透露敏感个人数据的有害AI代理进行实时对话。对于许多用户来说，像ChatGPT和LaMDA这样的现有系统感觉很安全，因为它们主要是基于文字的，但是该行业已经开始向实时语音和具有逼真数字形象的虚拟人物转变，这将使得可以部署有议程驱动的虚拟代言人（VSPs），它们将通过实时自适应影响变得高度具有说服力。本文探讨了VSP可能会使用的操纵策略以及用户对此类策略的不充分防御。最后提出了一套认知规定，旨在保护个人免受VSP的不知不觉操纵。

    The technology of Conversational AI has made significant advancements over the last eighteen months. As a consequence, conversational agents are likely to be deployed in the near future that are designed to pursue targeted influence objectives. Sometimes referred to as the "AI Manipulation Problem," the emerging risk is that consumers will unwittingly engage in real-time dialog with predatory AI agents that can skillfully persuade them to buy particular products, believe particular pieces of misinformation, or fool them into revealing sensitive personal data. For many users, current systems like ChatGPT and LaMDA feel safe because they are primarily text-based, but the industry is already shifting towards real-time voice and photorealistic digital personas that look, move, and express like real people. This will enable the deployment of agenda-driven Virtual Spokespeople (VSPs) that will be highly persuasive through real-time adaptive influence. This paper explores the manipulative tac
    
[^44]: 基于神经形状先验的多视角三维物体重建与不确定性建模

    Multi-view 3D Object Reconstruction and Uncertainty Modelling with Neural Shape Prior. (arXiv:2306.11739v1 [cs.CV])

    [http://arxiv.org/abs/2306.11739](http://arxiv.org/abs/2306.11739)

    本文提出了一种基于神经形状先验的三维物体重建方法，通过在表示中模拟不确定性，并提供可靠的不确定性估计，这种方法在单视角和多视角3D重建基准测试上均取得了最先进的性能。

    

    三维物体重建在语义场景理解中起着重要作用。由于缺乏深度信息、遮挡和噪声等问题，从单ocular图像直接重建详细的三维形状是具有挑战性的。当前大多数方法都是生成确定性的物体模型，并没有意识到重建的不确定性。我们通过利用神经对象表示解决了这个问题，该表示从大型3D对象模型数据集中学习物体形状分布并将其映射到潜在空间中。我们提出了一种方法在表示中建模不确定性，并定义了一种感知不确定性的编码器，直接从单个输入图像生成具有不确定性的潜在代码。此外，我们提出了一种方法将潜在代码中的不确定性传播到SDF值中，并为每个网格组件生成带有局部不确定性的三维对象网格。最后，我们提出了一种基于贝叶斯框架的渐进融合方法，将多角度对象的潜在代码融合成具有不确定性建模的完整3D模型。实验结果表明，我们的方法在单视角和多视角3D重建基准测试上均取得了最先进的性能，同时提供了可靠且定量的不确定性估计。

    3D object reconstruction is important for semantic scene understanding. It is challenging to reconstruct detailed 3D shapes from monocular images directly due to a lack of depth information, occlusion and noise. Most current methods generate deterministic object models without any awareness of the uncertainty of the reconstruction. We tackle this problem by leveraging a neural object representation which learns an object shape distribution from large dataset of 3d object models and maps it into a latent space. We propose a method to model uncertainty as part of the representation and define an uncertainty-aware encoder which generates latent codes with uncertainty directly from individual input images. Further, we propose a method to propagate the uncertainty in the latent code to SDF values and generate a 3d object mesh with local uncertainty for each mesh component. Finally, we propose an incremental fusion method under a Bayesian framework to fuse the latent codes from multi-view ob
    
[^45]: RM-PRT: 真实的机器人操作模拟器和基于渐进推理任务的基准评估

    RM-PRT: Realistic Robotic Manipulation Simulator and Benchmark with Progressive Reasoning Tasks. (arXiv:2306.11335v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2306.11335](http://arxiv.org/abs/2306.11335)

    该论文介绍了一个基于逼真机器人操作模拟器的基准评估RM-PRT，并提出了通用的评估流程，旨在通过大规模自然语言指令和多模态提示对机器人操作进行详细评估。

    

    最近，预训练的大规模语言模型（LLM），如ChatGPT和GPT-4的出现，显着推进了机器的自然语言理解能力。这一突破使我们能够将这些开源LLM无缝地集成到统一的机器人模拟器环境中，以帮助机器人准确理解和执行人类自然语言指令。为此，我们引入了一个逼真的机器人操作模拟器，并在此基础上构建了一个基于渐进推理任务的机器人操作基准（RM-PRT）。具体而言，RM-PRT基准评估基于Unreal Engine 5构建了一个新的高保真数字双胞胎场景，其中包括782个类别，2023个物体，并使用ChatGPT生成了15,000个自然语言指令，以详细评估机器人操作。我们提出了一个通用的RM-PRT基准评估流程，该流程接受包含自然语言指令的多模态提示作为输入，并自动输出行动。

    Recently, the advent of pre-trained large-scale language models (LLMs) like ChatGPT and GPT-4 have significantly advanced the machine's natural language understanding capabilities. This breakthrough has allowed us to seamlessly integrate these open-source LLMs into a unified robot simulator environment to help robots accurately understand and execute human natural language instructions. To this end, in this work, we introduce a realistic robotic manipulation simulator and build a Robotic Manipulation with Progressive Reasoning Tasks (RM-PRT) benchmark on this basis. Specifically, the RM-PRT benchmark builds a new high-fidelity digital twin scene based on Unreal Engine 5, which includes 782 categories, 2023 objects, and 15K natural language instructions generated by ChatGPT for a detailed evaluation of robot manipulation. We propose a general pipeline for the RM-PRT benchmark that takes as input multimodal prompts containing natural language instructions and automatically outputs action
    
[^46]: 增强属性聚类的图形变换方法：一种创新的图形转换器方法

    Transforming Graphs for Enhanced Attribute-Based Clustering: An Innovative Graph Transformer Method. (arXiv:2306.11307v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.11307](http://arxiv.org/abs/2306.11307)

    本文提出了一种称为GTAGC的图形自编码器图形变换自编码器方法，通过融合图自编码器和图形变换器，GTAGC能够捕获全局依赖关系，从而有助于提高图聚类的性能。

    

    图表示学习是一种有影响力的方法，它使得我们更深入地理解图结构化数据，并有助于图聚类，这是各个领域的一个关键任务。注意力机制最近已经进入了图学习的领域，这从根本上改变了研究趋势。因此，图注意力网络和图注意力自编码器已成为图聚类任务优选的工具。然而，这些方法主要采用局部注意机制，从而限制了它们理解图中节点之间复杂全局依赖性的能力。为了解决这些障碍，本研究介绍了一种称为图自编码器的图形变换自编码器的图形聚类（GTAGC）的创新方法。通过将图自编码器与图形变换器融合，GTAGC能够捕获节点之间的全局依赖关系。这种融合提高了性能，使得图形聚类任务有了显着的提升。

    Graph Representation Learning (GRL) is an influential methodology, enabling a more profound understanding of graph-structured data and aiding graph clustering, a critical task across various domains. The recent incursion of attention mechanisms, originally an artifact of Natural Language Processing (NLP), into the realm of graph learning has spearheaded a notable shift in research trends. Consequently, Graph Attention Networks (GATs) and Graph Attention Auto-Encoders have emerged as preferred tools for graph clustering tasks. Yet, these methods primarily employ a local attention mechanism, thereby curbing their capacity to apprehend the intricate global dependencies between nodes within graphs. Addressing these impediments, this study introduces an innovative method known as the Graph Transformer Auto-Encoder for Graph Clustering (GTAGC). By melding the Graph Auto-Encoder with the Graph Transformer, GTAGC is adept at capturing global dependencies between nodes. This integration amplifi
    
[^47]: BayLing：通过交互式翻译连接跨语言对齐和指令追踪，为大型语言模型提供支持。

    BayLing: Bridging Cross-lingual Alignment and Instruction Following through Interactive Translation for Large Language Models. (arXiv:2306.10968v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2306.10968](http://arxiv.org/abs/2306.10968)

    BayLing是一个指令遵循的LLM，通过交互式翻译将语言生成和指令遵循的能力从英语转移到其他语言，使得在非英语语言上也能够实现与最先进的LLMs相同的性能，而且不需要语言特定数据或指令。

    

    大型语言模型（LLMs）展示了惊人的语言理解和生成能力。从基础LLMs发展为指令追踪LLMs，指令调优在将LLMs与人类偏好对齐方面起着至关重要的作用。然而，现有的LLMs通常专注于英语，导致非英语语言表现较差。为了提高非英语语言的性能，需要收集基础LLMs的语言特定的训练数据，并构建语言特定的指令以进行指令调优，这两者都是沉重的负担。为了最小化人类工作量，我们提出通过交互式翻译任务将语言生成和指令遵循的能力从英语转移到其他语言。我们开发了BayLing，一种指令遵循LLM，利用LLaMA作为基础LLM，自动构建交互式翻译指令以进行指令调优。对英语、中文和西班牙语三种语言的广泛评估表明，BayLing在不需要语言特定数据或指令的情况下实现了与最先进的LLMs相当的性能。

    Large language models (LLMs) have demonstrated remarkable prowess in language understanding and generation. Advancing from foundation LLMs to instructionfollowing LLMs, instruction tuning plays a vital role in aligning LLMs to human preferences. However, the existing LLMs are usually focused on English, leading to inferior performance in non-English languages. In order to improve the performance for non-English languages, it is necessary to collect language-specific training data for foundation LLMs and construct language-specific instructions for instruction tuning, both of which are heavy loads. To minimize human workload, we propose to transfer the capabilities of language generation and instruction following from English to other languages through an interactive translation task. We have developed BayLing, an instruction-following LLM by utilizing LLaMA as the foundation LLM and automatically constructing interactive translation instructions for instructing tuning. Extensive assess
    
[^48]: 基于任务条件化超网络的多任务记忆深度强化学习

    Deep Reinforcement Learning with Multitask Episodic Memory Based on Task-Conditioned Hypernetwork. (arXiv:2306.10698v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.10698](http://arxiv.org/abs/2306.10698)

    人工智能领域，一个新算法利用基于任务条件化超网络的检索网络，根据任务调整网络参数，以解决深度强化学习中选择最相关的过去经验并将其融合到既有决策网络中的问题。

    

    深度强化学习算法通常受到采样效率低下的限制，严重依赖与环境的多次交互才能获得准确的决策能力。相比之下，人类似乎依赖海马体从过去有关任务的经历中检索相关信息，在学习新任务时指导其决策，而不是仅仅依赖于环境交互。然而，为代理设计类似海马体的模块以将过去的经历融入既有的强化学习算法面临两个挑战。第一个挑战涉及选择当前任务最相关的过去经验，第二个是将这些经验与决策网络相结合。为了解决这些问题，我们提出了一种新算法，利用基于任务条件化超网络的检索网络，根据任务调整检索网络的参数。

    Deep reinforcement learning algorithms are usually impeded by sampling inefficiency, heavily depending on multiple interactions with the environment to acquire accurate decision-making capabilities. In contrast, humans seem to rely on their hippocampus to retrieve relevant information from past experiences of relevant tasks, which guides their decision-making when learning a new task, rather than exclusively depending on environmental interactions. Nevertheless, designing a hippocampus-like module for an agent to incorporate past experiences into established reinforcement learning algorithms presents two challenges. The first challenge involves selecting the most relevant past experiences for the current task, and the second is integrating such experiences into the decision network. To address these challenges, we propose a novel algorithm that utilizes a retrieval network based on a task-conditioned hypernetwork, which adapts the retrieval network's parameters depending on the task. A
    
[^49]: MARBLE：音乐音频表征通用评估基准

    MARBLE: Music Audio Representation Benchmark for Universal Evaluation. (arXiv:2306.10548v2 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2306.10548](http://arxiv.org/abs/2306.10548)

    本论文介绍了MARBLE，一个音乐音频表征通用评估基准，它为音乐理解领域的研究和发展提供了一个全面和可持续性的基础，并提供各种音乐信息检索（MIR）任务的基准。

    

    在艺术与人工智能（AI）之间交叉的广泛时代中，例如图像生成和虚构共创，音乐的AI仍然相对初步，特别是在音乐理解方面。针对这个问题，本论文介绍了一个通用的音乐音频表征评估基准MARBLE，旨在提供各种音乐信息检索（MIR）任务的基准，通过定义包括声学，演奏，乐谱和高级描述在内的四个层次的综合分类法。然后，我们基于8个公共可用数据集上的14项任务建立了一个统一的协议，提供了所有基于音乐录音开发的开放源代码的预训练模型的表征的公平和标准的评估。此外，MARBLE提供了一个易于使用、可扩展和可重用的工具库，以支持社区驱动的客观基准评估。

    In the era of extensive intersection between art and Artificial Intelligence (AI), such as image generation and fiction co-creation, AI for music remains relatively nascent, particularly in music understanding. This is evident in the limited work on deep music representations, the scarcity of large-scale datasets, and the absence of a universal and community-driven benchmark. To address this issue, we introduce the Music Audio Representation Benchmark for universaL Evaluation, termed MARBLE. It aims to provide a benchmark for various Music Information Retrieval (MIR) tasks by defining a comprehensive taxonomy with four hierarchy levels, including acoustic, performance, score, and high-level description. We then establish a unified protocol based on 14 tasks on 8 public-available datasets, providing a fair and standard assessment of representations of all open-sourced pre-trained models developed on music recordings as baselines. Besides, MARBLE offers an easy-to-use, extendable, and re
    
[^50]: 用图表示学习推进生物医学：最新进展，挑战和未来方向综述

    Advancing Biomedicine with Graph Representation Learning: Recent Progress, Challenges, and Future Directions. (arXiv:2306.10456v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.10456](http://arxiv.org/abs/2306.10456)

    图表示学习（GRL）已经成为一个重要领域，在生物医学领域有着广泛的应用，未来研究方向是解决GRL当前面临的挑战。

    

    图表示学习（GRL）已经成为一个重要领域，在包括生物医学在内的多个领域都做出了重大贡献。本综述的目的是回顾GRL方法在生物医学领域的最新进展和应用。我们还强调了GRL当前面临的主要挑战，以及未来研究的潜在方向。

    Graph representation learning (GRL) has emerged as a pivotal field that has contributed significantly to breakthroughs in various fields, including biomedicine. The objective of this survey is to review the latest advancements in GRL methods and their applications in the biomedical field. We also highlight key challenges currently faced by GRL and outline potential directions for future research.
    
[^51]: 基于 SAT 模质的合证书学习

    Co-Certificate Learning with SAT Modulo Symmetries. (arXiv:2306.10427v2 [quant-ph] UPDATED)

    [http://arxiv.org/abs/2306.10427](http://arxiv.org/abs/2306.10427)

    该论文提出了一种基于SAT模质组合的新方法，称为合证书学习(co-certificate learning)，用于解决满足给定co-NP属性的同构图问题。这种方法比最近提出的基于SAT的方法快几个数量级，并具有更好的可扩展性。

    

    我们提出了一种新的 SAT-based 方法，用于生成满足给定 co-NP 属性的所有同构图。我们的方法扩展了 SAT Modulo Symmetry (SMS) 框架，并使用一种称为 co-certificate learning 的技术。如果 SMS 生成违反给定 co-NP 属性的候选图形，则我们获得这种违反的证书，即 co-NP 属性的“合证书”。 合证书导致 SAT 求解器作为 SMS 的后端学习其 CDCL 程序的子句。我们证明了 SMS 加上 co-certificate learning 是一种强大的方法，可以改善 Kochen-Specker 矢量系统大小下限的最佳已知界限，这是量子力学基础的中心问题，已经研究了半个多世纪。我们的方法比最近提出的基于 SAT 的方法快几个数量级，并具有更好的可扩展性。

    We present a new SAT-based method for generating all graphs up to isomorphism that satisfy a given co-NP property. Our method extends the SAT Modulo Symmetry (SMS) framework with a technique that we call co-certificate learning. If SMS generates a candidate graph that violates the given co-NP property, we obtain a certificate for this violation, i.e., `co-certificate' for the co-NP property. The co-certificate gives rise to a clause that the SAT solver, serving as SMS's backend, learns as part of its CDCL procedure. We demonstrate that SMS plus co-certificate learning is a powerful method that allows us to improve the best-known lower bound on the size of Kochen-Specker vector systems, a problem that is central to the foundations of quantum mechanics and has been studied for over half a century. Our approach is orders of magnitude faster and scales significantly better than a recently proposed SAT-based method.
    
[^52]: Enlighten-anything: 当分段模型遇见低光图像增强

    Enlighten-anything:When Segment Anything Model Meets Low-light Image Enhancement. (arXiv:2306.10286v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2306.10286](http://arxiv.org/abs/2306.10286)

    本文提出了Enlighten-anything，在低光图像增强中将分段模型与SAM融合，实现了良好视觉感知的融合图像。

    

    图像恢复是一项低级别视觉任务，大多数CNN方法都是作为黑盒子设计的，缺乏透明度和固有美学。许多无监督方法忽略了低光场景中可见信息的退化，这会严重影响补充信息的聚合，并使融合算法无法在极端情况下产生令人满意的融合结果。本文提出了Enlighten-anything，能够将SAM分段的语义意图与低光图像增强相结合，获得具有良好视觉感知的融合图像。无监督学习的泛化能力得到了极大提高，对LOL数据集的实验表明，我们的方法在PSNR上比基线提高了3dB，在SSIM上提高了8。 SAM的零样本学习为无监督低光增强提供了有力的帮助。Enlighten-anything的源代码可以从 https://github.com/zhangbaijin/enlighten-any 获得。

    Image restoration is a low-level visual task, and most CNN methods are designed as black boxes, lacking transparency and intrinsic aesthetics. Many unsupervised approaches ignore the degradation of visible information in low-light scenes, which will seriously affect the aggregation of complementary information and also make the fusion algorithm unable to produce satisfactory fusion results under extreme conditions. In this paper, we propose Enlighten-anything, which is able to enhance and fuse the semantic intent of SAM segmentation with low-light images to obtain fused images with good visual perception. The generalization ability of unsupervised learning is greatly improved, and experiments on LOL dataset are conducted to show that our method improves 3db in PSNR over baseline and 8 in SSIM. zero-shot learning of SAM introduces a powerful aid for unsupervised low-light enhancement. The source code of Enlighten-anything can be obtained from https://github.com/zhangbaijin/enlighten-any
    
[^53]: 将矩阵对角化作为一种棋盘游戏: 教授本征值求解器最快的解决路径

    Matrix Diagonalization as a Board Game: Teaching an Eigensolver the Fastest Path to Solution. (arXiv:2306.10075v2 [math.NA] UPDATED)

    [http://arxiv.org/abs/2306.10075](http://arxiv.org/abs/2306.10075)

    本文展示了如何使用强化学习加速矩阵对角化，将选择最快的解决方案的过程视为棋盘游戏，为数值线性代数的性能提供了一种有前途的工具。

    

    矩阵对角化是科学计算中许多领域的基础。对角化矩阵以解决特征值问题需要一系列迭代，最终达到所有特征值和特征向量的充分收敛和精确解决。这通常会导致高计算成本。在这里，我们展示了如何使用 AlphaZero 框架的强化学习，将选择最快的解决方案的过程视为棋盘游戏，以加速 Jacobi 对角化。为了证明我们方法的可行性，我们将 Jacobi 对角化算法应用于出现在量子化学计算中的对称哈密顿矩阵。我们发现通常可以实现显着加速。我们的发现突显了使用机器学习作为改进数值线性代数性能的一种有前途的工具。

    Matrix diagonalization is at the cornerstone of numerous fields of scientific computing. Diagonalizing a matrix to solve an eigenvalue problem requires a sequential path of iterations that eventually reaches a sufficiently converged and accurate solution for all the eigenvalues and eigenvectors. This typically translates into a high computational cost. Here we demonstrate how reinforcement learning, using the AlphaZero framework, can accelerate Jacobi matrix diagonalizations by viewing the selection of the fastest path to solution as a board game. To demonstrate the viability of our approach we apply the Jacobi diagonalization algorithm to symmetric Hamiltonian matrices that appear in quantum chemistry calculations. We find that a significant acceleration can often be achieved. Our findings highlight the opportunity to use machine learning as a promising tool to improve the performance of numerical linear algebra.
    
[^54]: 结构化图模型先验下的协作学习

    Structured Cooperative Learning with Graphical Model Priors. (arXiv:2306.09595v1 [cs.LG])

    [http://arxiv.org/abs/2306.09595](http://arxiv.org/abs/2306.09595)

    本文提出了结构化协作学习算法，在不同设备之间通过协作完成分散任务。通过图模型先验生成的协作图，算法可以自动捕捉设备之间的跨任务相关性。

    

    我们研究了如何在分散设备上对不同任务进行个性化建模，这些设备的局部数据受限。我们提出了“结构化协作学习（SCooL）”，其中一个跨设备的协作图由图模型先验生成，以自动协调设备之间的相互学习。通过选择施加不同结构的图模型，我们可以通过变分推断推导出一类丰富的现有和新型去中心化学习算法。特别地，我们展示了三种 SCooL 的示例，在其中以 Dirac 分布、随机块模型（SBM）和注意力作为生成协作图的先验。这些 EM 类型的算法通过更新协作图和协同本地模型学习之间进行交替，仅通过监视模型更新来优化协作图，从而可以自动捕捉设备之间的跨任务相关性。

    We study how to train personalized models for different tasks on decentralized devices with limited local data. We propose "Structured Cooperative Learning (SCooL)", in which a cooperation graph across devices is generated by a graphical model prior to automatically coordinate mutual learning between devices. By choosing graphical models enforcing different structures, we can derive a rich class of existing and novel decentralized learning algorithms via variational inference. In particular, we show three instantiations of SCooL that adopt Dirac distribution, stochastic block model (SBM), and attention as the prior generating cooperation graphs. These EM-type algorithms alternate between updating the cooperation graph and cooperative learning of local models. They can automatically capture the cross-task correlations among devices by only monitoring their model updating in order to optimize the cooperation graph. We evaluate SCooL and compare it with existing decentralized learning met
    
[^55]: 通过多模态强化知识图谱传播识别未见过的物体

    Recognizing Unseen Objects via Multimodal Intensive Knowledge Graph Propagation. (arXiv:2306.08487v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2306.08487](http://arxiv.org/abs/2306.08487)

    本文提出了一个多模态知识图谱传播框架，利用文本信息和视觉内容对未见过的物体进行零样本学习和识别。

    

    零样本学习（Zero-Shot Learning，ZSL）旨在自动识别未见过的物体，是机器持续理解新的现实世界知识的一种有前途的学习范式。知识图谱（Knowledge Graph, KG）被证明是一种有效的处理具有大规模和非属性数据的零样本任务的方案。先前的研究通常将已经知道和未知的对象之间的关系嵌入到已知知识图谱的视觉信息中，以提高未知数据的认知能力。实际上，现实世界的知识是由多模态事实自然形成的。与仅从图形角度考虑的普通结构知识相比，多模态 KG 可以为认知系统提供细粒度知识。例如，文本描述和视觉内容可以比仅依赖于知识三元组描述更多的关键细节。不幸的是，由于不同模态之间的特征对准的瓶颈，这种多模态细粒度的知识主要未被开发利用。

    Zero-Shot Learning (ZSL), which aims at automatically recognizing unseen objects, is a promising learning paradigm to understand new real-world knowledge for machines continuously. Recently, the Knowledge Graph (KG) has been proven as an effective scheme for handling the zero-shot task with large-scale and non-attribute data. Prior studies always embed relationships of seen and unseen objects into visual information from existing knowledge graphs to promote the cognitive ability of the unseen data. Actually, real-world knowledge is naturally formed by multimodal facts. Compared with ordinary structural knowledge from a graph perspective, multimodal KG can provide cognitive systems with fine-grained knowledge. For example, the text description and visual content can depict more critical details of a fact than only depending on knowledge triplets. Unfortunately, this multimodal fine-grained knowledge is largely unexploited due to the bottleneck of feature alignment between different moda
    
[^56]: TopP\&R: 具有鲁棒性的支持估计方法，用于评估生成模型中的保真度和多样性

    TopP\&R: Robust Support Estimation Approach for Evaluating Fidelity and Diversity in Generative Models. (arXiv:2306.08013v1 [cs.LG])

    [http://arxiv.org/abs/2306.08013](http://arxiv.org/abs/2306.08013)

    本文提出了一种鲁棒可靠的生成模型评估指标TopP\&R，通过引入拓扑和统计处理进行严格的支持估计。TopP\&R仅保留具有一定置信水平的具有拓扑和统计上重要性的特征，对于噪声特征具有强大的鲁棒性，并提供了统计一致性。

    

    本文提出了一种鲁棒可靠的生成模型评估指标，通过引入拓扑和统计处理进行严格的支持估计。现有的度量标准，如Inception Score（IS），Fr\'echet Inception Distance（FID）以及Precision and Recall（P\&R）的变体，严重依赖于从样本特征估计的支持。然而，尽管评估的质量完全取决于其可靠性，但其估计的可靠性并没有得到严肃的讨论（并被忽视）。本文提出了拓扑精度和召回率（TopP\&R，发音为“topper”），它提供了一种系统的方法来估计支持，仅保留具有一定置信水平的具有拓扑和统计上重要性的特征。这不仅使TopP\&R对于噪声特征具有强大的鲁棒性，而且还提供了统计一致性。我们的理论和实验结果表明，TopP\&R对于离群值和非独立同分布具有鲁棒性。

    We propose a robust and reliable evaluation metric for generative models by introducing topological and statistical treatments for rigorous support estimation. Existing metrics, such as Inception Score (IS), Fr\'echet Inception Distance (FID), and the variants of Precision and Recall (P\&R), heavily rely on supports that are estimated from sample features. However, the reliability of their estimation has not been seriously discussed (and overlooked) even though the quality of the evaluation entirely depends on it. In this paper, we propose Topological Precision and Recall (TopP\&R, pronounced 'topper'), which provides a systematic approach to estimating supports, retaining only topologically and statistically important features with a certain level of confidence. This not only makes TopP\&R strong for noisy features, but also provides statistical consistency. Our theoretical and experimental results show that TopP\&R is robust to outliers and non-independent and identically distributed
    
[^57]: 不要相信你的眼睛：关于特征可视化的（不）可靠性。

    Don't trust your eyes: on the (un)reliability of feature visualizations. (arXiv:2306.04719v1 [cs.CV])

    [http://arxiv.org/abs/2306.04719](http://arxiv.org/abs/2306.04719)

    本文探讨了神经网络如何从像素中提取模式的问题，并研究了特征可视化的可靠性。实验证据表明，由于优化过程中固有的限制，特征可视化能够可靠理解的功能集非常有限，对于解释神经网络如何处理自然图像的解释能力产生怀疑。

    

    神经网络是如何从像素中提取模式的？特征可视化通过优化来可视化高激活的模式，试图回答这个重要问题。如今，可视化方法构成了我们对神经网络内部工作的了解的基础，作为一种机械式的可解释性。在这里，我们问：特征可视化有多可靠？我们通过开发网络电路来诈骗特征可视化，使其显示完全与自然输入的正常网络行为毫无联系的任意模式。然后，我们提供证据表明在标准，未操纵网络中发生了类似的现象：特征可视化与标准输入处理非常不同，对神经网络如何处理自然图像的解释能力产生怀疑。我们通过理论证明支撑这一经验发现，由于优化过程中固有的限制，可以通过特征可视化可靠理解的功能集极其有限。

    How do neural networks extract patterns from pixels? Feature visualizations attempt to answer this important question by visualizing highly activating patterns through optimization. Today, visualization methods form the foundation of our knowledge about the internal workings of neural networks, as a type of mechanistic interpretability. Here we ask: How reliable are feature visualizations? We start our investigation by developing network circuits that trick feature visualizations into showing arbitrary patterns that are completely disconnected from normal network behavior on natural input. We then provide evidence for a similar phenomenon occurring in standard, unmanipulated networks: feature visualizations are processed very differently from standard input, casting doubt on their ability to "explain" how neural networks process natural images. We underpin this empirical finding by theory proving that the set of functions that can be reliably understood by feature visualization is extr
    
[^58]: 设计用户角色感知的对话代理进行有趣的对话：$\textit{WHAT}$, $\textit{WHEN}$, and $\textit{HOW}$ to Ground

    $\textit{WHAT}$, $\textit{WHEN}$, and $\textit{HOW}$ to Ground: Designing User Persona-Aware Conversational Agents for Engaging Dialogue. (arXiv:2306.03361v1 [cs.CL])

    [http://arxiv.org/abs/2306.03361](http://arxiv.org/abs/2306.03361)

    本文提出了一种针对商业环境的、能够平衡对话流畅性和趋向于理解对话系统的个性化开放领域对话系统方法，通过加权数据集混合、负角色信息增强方法，以及设计个性化对话数据集，解决了 $\textit{WHAT}$、$\textit{WHEN}$和$\textit{HOW}$ 等问题，同时提高了对话系统响应的可控性和解释性。

    

    本文提出了一种建立个性化开放领域对话系统以解决商业设置中涉及个性化对话响应与非正式响应交替的$\textit{WWH}$（$\textit{WHAT}$、$\textit{WHEN}$和$\textit{HOW}$）问题的方法。所提出的方法涉及加权数据集混合、负角色信息增强方法以及设计个性化对话数据集，以应对个性化、开放领域对话系统中$\textit{WWH}$的挑战。本文有效地平衡了对话流畅性和趋向于理解对话系统，同时还引入了响应类型标签来提高可控性和解释性。这些方法的组合导致了更加流畅的对话，证明了基于主观人类评估和客观评估的实验结果。

    This paper presents a method for building a personalized open-domain dialogue system to address the $\textit{WWH}$ ($\textit{WHAT}$, $\textit{WHEN}$, and $\textit{HOW}$) problem for natural response generation in a commercial setting, where personalized dialogue responses are heavily interleaved with casual response turns. The proposed approach involves weighted dataset blending, negative persona information augmentation methods, and the design of personalized conversation datasets to address the challenges of $\textit{WWH}$ in personalized, open-domain dialogue systems. Our work effectively balances dialogue fluency and tendency to ground, while also introducing a response-type label to improve the controllability and explainability of the grounded responses. The combination of these methods leads to more fluent conversations, as evidenced by subjective human evaluations as well as objective evaluations.
    
[^59]: SGEM：通过序列级广义熵最小化实现自动语音识别的测试时自适应

    SGEM: Test-Time Adaptation for Automatic Speech Recognition via Sequential-Level Generalized Entropy Minimization. (arXiv:2306.01981v2 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2306.01981](http://arxiv.org/abs/2306.01981)

    SGEM提出了一种新的测试时自适应框架，利用波束搜索和广义熵最小化调整预训练ASR模型，取得了三个主流ASR模型在不同领域转变时的最新性能。

    

    在许多实际情况下，自动语音识别（ASR）模型经常暴露于数据分布的变化，导致错误的预测。为了解决这个问题，最近提出了一种现有的测试时自适应（TTA）方法，可以在没有源数据的情况下调整预训练的ASR模型以适应未标记的测试实例。尽管有了不错的性能提升，但这项工作仅依赖于简单的贪心解码，并在帧级别上跨越时间步长进行调整，这在模型输出的序列性质下可能不是最优的。出于这个动机，我们提出了一个新的TTA框架，称为SGEM，用于一般ASR模型。为了处理序列输出，SGEM首先利用波束搜索来探索候选输出标志，并选择最可信的标志。然后，它利用广义熵最小化和负抽样作为无监督目标来适应模型。在各种领域的转变下，SGEM实现了三种主流ASR模型的最新性能。

    Automatic speech recognition (ASR) models are frequently exposed to data distribution shifts in many real-world scenarios, leading to erroneous predictions. To tackle this issue, an existing test-time adaptation (TTA) method has recently been proposed to adapt the pre-trained ASR model on unlabeled test instances without source data. Despite decent performance gain, this work relies solely on naive greedy decoding and performs adaptation across timesteps at a frame level, which may not be optimal given the sequential nature of the model output. Motivated by this, we propose a novel TTA framework, dubbed SGEM, for general ASR models. To treat the sequential output, SGEM first exploits beam search to explore candidate output logits and selects the most plausible one. Then, it utilizes generalized entropy minimization and negative sampling as unsupervised objectives to adapt the model. SGEM achieves state-of-the-art performance for three mainstream ASR models under various domain shifts.
    
[^60]: PDT: 面向时间感知的双向预训练变压器模型用于二分图

    PDT: Pretrained Dual Transformers for Time-aware Bipartite Graphs. (arXiv:2306.01913v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2306.01913](http://arxiv.org/abs/2306.01913)

    该论文提出了一种预训练模型，用于学习二分图中用户和内容之间的上下文知识，并采用对比学习任务以提高性能。

    

    在许多机器学习应用中，预先训练大型模型正在普及并涌现，随着不断增长的用户生成内容。已经认识到，从描绘用户内容交互的数据集中学习上下文知识对下游任务至关重要。尽管有几项研究尝试通过预训练方法学习上下文知识，但为这种任务找到最佳的训练目标和策略仍然是一个具有挑战性的问题。在这项工作中，我们认为在表示用户-内容交互的数据集中，有两个不同的上下文知识方面，即用户方面和内容方面。为了学习上下文知识，我们提出了一种预训练方法，该方法学习用户方面和内容方面之间的双向映射，我们将训练目标制定为对比学习任务，并提出了双重Transformer架构来编码上下文知识。

    Pre-training on large models is prevalent and emerging with the ever-growing user-generated content in many machine learning application categories. It has been recognized that learning contextual knowledge from the datasets depicting user-content interaction plays a vital role in downstream tasks. Despite several studies attempting to learn contextual knowledge via pre-training methods, finding an optimal training objective and strategy for this type of task remains a challenging problem. In this work, we contend that there are two distinct aspects of contextual knowledge, namely the user-side and the content-side, for datasets where user-content interaction can be represented as a bipartite graph. To learn contextual knowledge, we propose a pre-training method that learns a bi-directional mapping between the spaces of the user-side and the content-side. We formulate the training goal as a contrastive learning task and propose a dual-Transformer architecture to encode the contextual k
    
[^61]: FigGen: 文本到科学图形生成

    FigGen: Text to Scientific Figure Generation. (arXiv:2306.00800v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2306.00800](http://arxiv.org/abs/2306.00800)

    本文介绍了 FigGen，一种通过将文本描述转换为科学图形的基于扩散的方法。该技术探索了文本到图形生成的领域，并解决了该任务的主要挑战。

    

    近年来，生成建模领域取得了巨大的发展，特别是在生成自然图像和艺术方面。最近的技术在创造复杂的视觉组合并展现出令人印象深刻的逼真度和质量方面显示出了令人瞩目的潜力。然而，最先进的方法一直专注于自然图像的狭窄领域，而其他分布则尚未被探索。在本文中，我们引入了文本到图形生成的问题，即从文本描述中创造论文的科学图形。我们提出了 FigGen，这是一种基于扩散的文本到图形技术，也介绍了所提出任务的主要挑战。代码和模型可在 https://github.com/joanrod/figure-diffusion 获取。

    The generative modeling landscape has experienced tremendous growth in recent years, particularly in generating natural images and art. Recent techniques have shown impressive potential in creating complex visual compositions while delivering impressive realism and quality. However, state-of-the-art methods have been focusing on the narrow domain of natural images, while other distributions remain unexplored. In this paper, we introduce the problem of text-to-figure generation, that is creating scientific figures of papers from text descriptions. We present FigGen, a diffusion-based approach for text-to-figure as well as the main challenges of the proposed task. Code and models are available at https://github.com/joanrod/figure-diffusion
    
[^62]: RL+模型控制：使用按需最优控制学习多功能足式 locomotion

    RL + Model-based Control: Using On-demand Optimal Control to Learn Versatile Legged Locomotion. (arXiv:2305.17842v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2305.17842](http://arxiv.org/abs/2305.17842)

    本文提出了一种 RL+模型控制框架以开发出可以有效可靠地学习的健壮控制策略，通过整合有限时间最优控制生成的按需参考运动分散 RL 过程，同时克服了建模简化的固有局限性，在足式 locomotion 上实现了多功能和强健，能泛化参考运动并处理更复杂的运动任务。

    

    本文提出了一种控制框架，将基于模型的最优控制和强化学习（RL）相结合，实现了多功能和强健的足式 locomotion。我们的方法通过整合有限时间最优控制生成的按需参考运动来增强 RL 训练过程，覆盖了广泛的速度和步态。这些参考运动作为 RL 策略模仿的目标，导致开发出可有效可靠地学习的健壮控制策略。此外，通过考虑全身动力学，RL 克服了建模简化的固有局限性。通过仿真和硬件实验，我们展示了 RL 训练过程在我们的框架内的强健性和可控性。此外，我们的方法展示了泛化参考运动和处理可能对简化模型构成挑战的更复杂的运动任务的能力，利用了 RL 的灵活性。

    This letter presents a control framework that combines model-based optimal control and reinforcement learning (RL) to achieve versatile and robust legged locomotion. Our approach enhances the RL training process by incorporating on-demand reference motions generated through finite-horizon optimal control, covering a broad range of velocities and gaits. These reference motions serve as targets for the RL policy to imitate, resulting in the development of robust control policies that can be learned efficiently and reliably. Moreover, by considering whole-body dynamics, RL overcomes the inherent limitations of modelling simplifications. Through simulation and hardware experiments, we demonstrate the robustness and controllability of the RL training process within our framework. Furthermore, our method demonstrates the ability to generalize reference motions and handle more complex locomotion tasks that may pose challenges for the simplified model, leveraging the flexibility of RL.
    
[^63]: 基于注意力机制的卷积神经网络在乳腺癌分割中的应用及可解释人工智能

    Breast Cancer Segmentation using Attention-based Convolutional Network and Explainable AI. (arXiv:2305.14389v1 [cs.CV])

    [http://arxiv.org/abs/2305.14389](http://arxiv.org/abs/2305.14389)

    本研究提出了一种基于注意力机制的卷积神经网络，结合可解释人工智能和红外热像技术进行乳腺癌分割，提高了检测和分类的速度和精度，并在UNet结构中使用Grad-CAM分析了潜在偏差和弱点区域，比现有模型表现更优。

    

    乳腺癌是一种严重的健康威胁，目前尚无长期治愈方法。早期检测至关重要，但乳腺X线摄影的解释受到大量假阳性和假阴性的影响。随着乳腺癌例数有望超过肺癌，改进早期检测方法至关重要。红外热像技术结合人工智能提供了希望。本研究提出了一种基于注意力机制的卷积神经网络进行分割的方法，提高了乳腺癌检测和分类的速度和精度。该系统使用可解释人工智能对图像进行增强和进行癌症分割。我们提出了一个基于变压器注意力机制的卷积结构（UNet）用于故障识别，并使用梯度加权类激活映射（Grad-CAM）来分析在IRT图像中UNet结构的偏差和弱点区域。与现有最先进模型相比，我们提出的框架的优越性得到了证实。

    Breast cancer (BC) remains a significant health threat, with no long-term cure currently available. Early detection is crucial, yet mammography interpretation is hindered by high false positives and negatives. With BC incidence projected to surpass lung cancer, improving early detection methods is vital. Thermography, using high-resolution infrared cameras, offers promise, especially when combined with artificial intelligence (AI). This work presents an attention-based convolutional neural network for segmentation, providing increased speed and precision in BC detection and classification. The system enhances images and performs cancer segmentation with explainable AI. We propose a transformer-attention-based convolutional architecture (UNet) for fault identification and employ Gradient-weighted Class Activation Mapping (Grad-CAM) to analyze areas of bias and weakness in the UNet architecture with IRT images. The superiority of our proposed framework is confirmed when compared with exi
    
[^64]: GUARD: 一个安全强化学习基准测试平台

    GUARD: A Safe Reinforcement Learning Benchmark. (arXiv:2305.13681v1 [cs.LG])

    [http://arxiv.org/abs/2305.13681](http://arxiv.org/abs/2305.13681)

    GUARD是一个广义统一安全强化学习开发基准测试平台，是目前广泛遍布且包含各种RL代理、任务和安全约束规范的一站式基准测试，能够全面涵盖最先进的安全RL算法，并具有高度的可自定义性。

    

    由于试错的性质，将RL算法应用于安全关键的现实应用（例如自动驾驶、人机交互、机器人操作等）通常是具有挑战性的，因为这些错误是不可容忍的。最近，安全RL（即约束RL）已经在文献中迅速出现，其中代理在满足约束条件的同时，探索环境。由于算法和任务的多样性，比较现有的安全RL算法仍然很困难。为了填补这一空白，我们介绍了GUARD，一个广义统一安全强化学习开发基准测试平台。与现有基准相比，GUARD具有几个优点。首先，GUARD是一个广义基准测试平台，具有各种RL代理、任务和安全约束规范。其次，GUARD全面涵盖了最先进的安全RL算法，并具有自包含的实现。第三，GUARD在任务和算法方面具有高度的可自定义性。我们提供了状态下现有方法在GUARD上的基准测试结果。

    Due to the trial-and-error nature, it is typically challenging to apply RL algorithms to safety-critical real-world applications, such as autonomous driving, human-robot interaction, robot manipulation, etc, where such errors are not tolerable. Recently, safe RL (i.e. constrained RL) has emerged rapidly in the literature, in which the agents explore the environment while satisfying constraints. Due to the diversity of algorithms and tasks, it remains difficult to compare existing safe RL algorithms. To fill that gap, we introduce GUARD, a Generalized Unified SAfe Reinforcement Learning Development Benchmark. GUARD has several advantages compared to existing benchmarks. First, GUARD is a generalized benchmark with a wide variety of RL agents, tasks, and safety constraint specifications. Second, GUARD comprehensively covers state-of-the-art safe RL algorithms with self-contained implementations. Third, GUARD is highly customizable in tasks and algorithms. We present a comparison of state
    
[^65]: ConvXAI：通过对话提供异构的AI解释，支持人机科技写作

    ConvXAI: Delivering Heterogeneous AI Explanations via Conversations to Support Human-AI Scientific Writing. (arXiv:2305.09770v1 [cs.HC])

    [http://arxiv.org/abs/2305.09770](http://arxiv.org/abs/2305.09770)

    ConvXAI是一个基于对话的XAI系统，它集成了多种XAI类型，并将实际用户需求嵌入设计中，以提高实用性。

    

    尽管已经提出了各种各样的人工智能解释（XAI）方法来解释AI系统，但目前的方法是否对人类实用仍存在不一致的发现。为了改善XAI方法的实用性，一系列研究确定了现实世界中多样化和动态的用户需求与现有XAI方法之间的差距。虽然之前的研究设想将多种XAI方法集成到通用XAI界面（例如，基于对话或GUI的XAI系统）中以减轻这些差距，但缺少针对这些系统如何设计以满足实际用户需求的研究。在本研究中，我们提出了ConvXAI，这是一个基于对话的XAI系统，它结合了多种XAI类型，并赋予用户通过通用的XAI对话界面提出各种XAI问题的能力。特别地，我们创新地将实际用户需求（即，基于格式研究的四个原则）嵌入ConvXAI设计中，以提高实用性。

    While various AI explanation (XAI) methods have been proposed to interpret AI systems, whether the state-of-the-art XAI methods are practically useful for humans remains inconsistent findings. To improve the usefulness of XAI methods, a line of studies identifies the gaps between the diverse and dynamic real-world user needs with the status quo of XAI methods. Although prior studies envision mitigating these gaps by integrating multiple XAI methods into the universal XAI interfaces (e.g., conversational or GUI-based XAI systems), there is a lack of work investigating how these systems should be designed to meet practical user needs. In this study, we present ConvXAI, a conversational XAI system that incorporates multiple XAI types, and empowers users to request a variety of XAI questions via a universal XAI dialogue interface. Particularly, we innovatively embed practical user needs (i.e., four principles grounding on the formative study) into ConvXAI design to improve practical useful
    
[^66]: 基于知识追踪和强化学习的自适应学习路径导航

    Adaptive Learning Path Navigation Based on Knowledge Tracing and Reinforcement Learning. (arXiv:2305.04475v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2305.04475](http://arxiv.org/abs/2305.04475)

    本文介绍了一种名为自适应学习路径导航（ALPN）系统，它通过将关注知识追踪（AKT）模型与增强熵近端策略优化（EPPO）算法相结合，来为学生提供高度自适应的学习路径，从而显著提高了学习效果。

    

    本文介绍了一种名为自适应学习路径导航（ALPN）系统的新方法，它通过为学生提供高度自适应的学习路径来增强E-learning平台。ALPN系统将关注知识追踪（AKT）模型，该模型评估学生的知识状态，与提出的增强熵近端策略优化（EPPO）算法相结合。这种新算法可以优化学习材料的推荐。通过将这些模型协调一致，ALPN系统将学习路径量身定制为学生的需求，显著提高了学习效果。实验结果表明，ALPN系统在最大化学习结果方面比以前的研究提高了8.2％，而在生成学习路径方面提供了10.5％更高的多样性。该系统在自适应E-learning方面标志着重大进展，可能会改变数字化时代的教育格局。

    This paper introduces the Adaptive Learning Path Navigation (ALPN) system, a novel approach for enhancing E-learning platforms by providing highly adaptive learning paths for students. The ALPN system integrates the Attentive Knowledge Tracing (AKT) model, which assesses students' knowledge states, with the proposed Entropy-enhanced Proximal Policy Optimization (EPPO) algorithm. This new algorithm optimizes the recommendation of learning materials. By harmonizing these models, the ALPN system tailors the learning path to students' needs, significantly increasing learning effectiveness. Experimental results demonstrate that the ALPN system outperforms previous research by 8.2% in maximizing learning outcomes and provides a 10.5% higher diversity in generating learning paths. The proposed system marks a significant advancement in adaptive E-learning, potentially transforming the educational landscape in the digital era.
    
[^67]: 利用SAM的输入增强技术: 以分割基础模型为基础提升医学图像分割

    Input Augmentation with SAM: Boosting Medical Image Segmentation with Segmentation Foundation Model. (arXiv:2304.11332v1 [cs.CV])

    [http://arxiv.org/abs/2304.11332](http://arxiv.org/abs/2304.11332)

    本文介绍如何使用大型的通用分割模型SAM来提升医学图像分割，展示了如何通过使用SAM生成的掩模、特征和稳定性分数来构建和训练更好的医学图像分割模型，并在两个数据集上进行了验证。

    

    Segment Anything Model (SAM)是一个最近发展的通用分割模型，用于计算机视觉任务. SAM使用了超过1亿个掩模的1100万图像进行训练，可以为自然场景图像中的广泛对象生成分割结果。本研究展示了如何利用这样一个大型基础模型来进行医学图像分割，尽管SAM并没有立即为医学图像提供高质量的分割，但其生成的掩模、特征和稳定性分数对于构建和训练更好的医学图像分割模型非常有用。特别地，我们演示了如何使用SAM来增强经典的医学图像分割模型（如U-Net）的图像输入。对两个数据集的实验表明了我们所提出的方法的有效性。

    The Segment Anything Model (SAM) is a recently developed large model for general-purpose segmentation for computer vision tasks. SAM was trained using 11 million images with over 1 billion masks and can produce segmentation results for a wide range of objects in natural scene images. SAM can be viewed as a general perception model for segmentation (partitioning images into semantically meaningful regions). Thus, how to utilize such a large foundation model for medical image segmentation is an emerging research target. This paper shows that although SAM does not immediately give high-quality segmentation for medical images, its generated masks, features, and stability scores are useful for building and training better medical image segmentation models. In particular, we demonstrate how to use SAM to augment image inputs for a commonly-used medical image segmentation model (e.g., U-Net). Experiments on two datasets show the effectiveness of our proposed method.
    
[^68]: 推理中处理Wikidata限定词的问题

    Handling Wikidata Qualifiers in Reasoning. (arXiv:2304.03375v1 [cs.AI])

    [http://arxiv.org/abs/2304.03375](http://arxiv.org/abs/2304.03375)

    本文提出了处理Wikidata限定词的方法，包括对限定词进行分类和使用多排序逻辑语言形式化Wikidata模型，以应用于推理。

    

    Wikidata是一个被许多社区广泛应用于各种应用程序的知识图谱。Wikidata语句带有限定词值对，用于描述信息，例如语句的有效上下文，其因果关系，来源等。在推理中处理限定词是一个具有挑战性的问题。本文提出了一个解决方法，即通过对限定符进行分类和将Wikidata模型形式化为一种多排序的逻辑语言来解决此问题，通过将该逻辑与语义Web规则语言（SWRL）相结合，演示了我们的方法的实用性。

    Wikidata is a knowledge graph increasingly adopted by many communities for diverse applications. Wikidata statements are annotated with qualifier-value pairs that are used to depict information, such as the validity context of the statement, its causality, provenances, etc. Handling the qualifiers in reasoning is a challenging problem. When defining inference rules (in particular, rules on ontological properties (x subclass of y, z instance of x, etc.)), one must consider the qualifiers, as most of them participate in the semantics of the statements. This poses a complex problem because a) there is a massive number of qualifiers, and b) the qualifiers of the inferred statement are often a combination of the qualifiers in the rule condition. In this work, we propose to address this problem by a) defining a categorization of the qualifiers b) formalizing the Wikidata model with a many-sorted logical language; the sorts of this language are the qualifier categories. We couple this logic w
    
[^69]: 探索视觉-语言模型在不平衡学习中的应用

    Exploring Vision-Language Models for Imbalanced Learning. (arXiv:2304.01457v1 [cs.AI])

    [http://arxiv.org/abs/2304.01457](http://arxiv.org/abs/2304.01457)

    本文探索了如何通过向视觉-语言模型添加轻量级解码器和利用不平衡算法来改进性能，实验表明改进后的VLM在iNaturalist18、CIFAR-100和Visual Genome数据集上分类准确度显著提高，特别是对于少数类，性能提升很大。

    

    使用对比语言-图像预训练的视觉-语言模型（VLMs）已经显示出有希望的零样本分类表现。然而，在不平衡数据集上，它们的性能相对较差，在训练数据集中类的分布倾斜，导致在预测少数类方面性能不佳。我们提出向VLM添加轻量级解码器，以避免由于大量类别导致的内存不足问题，并捕捉尾部类别的微妙特征。然后，我们探索了利用提示调整、微调以及加入不平衡算法（例如Focal Loss、Balanced SoftMax和Distribution Alignment）来改进VLM。实验表明，在使用解码器和不平衡方法时，VLM的性能可以进一步提高。具体而言，我们改进的VLM在iNaturalist18、CIFAR-100和Visual Genome数据集上的分类准确度平均提高了6.58%、69.82%和10.43%。

    Vision-Language models (VLMs) that use contrastive language-image pre-training have shown promising zero-shot classification performance. However, their performance on imbalanced dataset is relatively poor, where the distribution of classes in the training dataset is skewed, leading to poor performance in predicting minority classes. For instance, CLIP achieved only 5% accuracy on the iNaturalist18 dataset. We propose to add a lightweight decoder to VLMs to avoid OOM (out of memory) problem caused by large number of classes and capture nuanced features for tail classes. Then, we explore improvements of VLMs using prompt tuning, fine-tuning, and incorporating imbalanced algorithms such as Focal Loss, Balanced SoftMax and Distribution Alignment. Experiments demonstrate that the performance of VLMs can be further boosted when used with decoder and imbalanced methods. Specifically, our improved VLMs significantly outperforms zero-shot classification by an average accuracy of 6.58%, 69.82%,
    
[^70]: 基于生成性潜在扩散的fMRI信号自然场景重建

    Natural scene reconstruction from fMRI signals using generative latent diffusion. (arXiv:2303.05334v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2303.05334](http://arxiv.org/abs/2303.05334)

    本研究提出了一个基于生成性潜在扩散的两阶段场景重建框架，称作“Brain-Diffuser”，能够从fMRI信号重建复杂场景图像。

    

    在神经解码研究中，最引人注目的话题之一是基于fMRI信号重建感知到的自然图像。先前的研究已经成功地重新创建了视觉的不同方面，例如低水平属性（形状、纹理、布局）或高水平特征（对象类别、场景细节语义），但通常无法同时为复杂的场景图像重建这些属性。生成AI最近通过能够生成高复杂度图像的潜在扩散模型迈出了重要一步。在这里，我们探讨如何利用这种创新技术进行脑解码。我们提出了一个名为“Brain-Diffuser”的两阶段场景重建框架。在第一阶段，从fMRI信号开始，我们使用VDVAE（非常深的变分自编码器）模型重建图像，捕捉低水平属性和总体布局。在第二阶段中，我们使用潜在扩散模型的图像到图像框架来提取更高级别的信息，从而重建复杂的场景图像。

    In neural decoding research, one of the most intriguing topics is the reconstruction of perceived natural images based on fMRI signals. Previous studies have succeeded in re-creating different aspects of the visuals, such as low-level properties (shape, texture, layout) or high-level features (category of objects, descriptive semantics of scenes) but have typically failed to reconstruct these properties together for complex scene images. Generative AI has recently made a leap forward with latent diffusion models capable of generating high-complexity images. Here, we investigate how to take advantage of this innovative technology for brain decoding. We present a two-stage scene reconstruction framework called ``Brain-Diffuser''. In the first stage, starting from fMRI signals, we reconstruct images that capture low-level properties and overall layout using a VDVAE (Very Deep Variational Autoencoder) model. In the second stage, we use the image-to-image framework of a latent diffusion mod
    
[^71]: 对比层次聚类

    Contrastive Hierarchical Clustering. (arXiv:2303.03389v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.03389](http://arxiv.org/abs/2303.03389)

    本文提出了 CoHiClust 模型，一种基于深度神经网络的对比层次聚类模型，通过自监督学习方法，生成与我们直觉和图像语义相符的合理聚类结构，且在大部分图像数据集上的聚类准确性超过了最先进的平面聚类模型。

    

    深度聚类一直被平面模型所占主导地位，这些模型将数据集分成预定义数量的组。尽管最近的方法在流行的基准测试中与基本事实的相似度非常高，但平面分区提供的信息有限。本文介绍了一种基于深度神经网络的对比层次聚类模型 CoHiClust，可应用于典型图像数据。通过采用自监督学习方法，CoHiClust 可以将基础网络蒸馏为二叉树，而不需要访问任何标记的数据。层次聚类结构可以用于分析聚类之间的关系以及衡量数据点之间的相似度。实验证明，CoHiClust 生成了一个合理的聚类结构，符合我们的直觉和图像语义。此外，它与最先进的平面聚类相比，在大多数图像数据集上获得了更高的聚类准确性。

    Deep clustering has been dominated by flat models, which split a dataset into a predefined number of groups. Although recent methods achieve an extremely high similarity with the ground truth on popular benchmarks, the information contained in the flat partition is limited. In this paper, we introduce CoHiClust, a Contrastive Hierarchical Clustering model based on deep neural networks, which can be applied to typical image data. By employing a self-supervised learning approach, CoHiClust distills the base network into a binary tree without access to any labeled data. The hierarchical clustering structure can be used to analyze the relationship between clusters, as well as to measure the similarity between data points. Experiments demonstrate that CoHiClust generates a reasonable structure of clusters, which is consistent with our intuition and image semantics. Moreover, it obtains superior clustering accuracy on most of the image datasets compared to the state-of-the-art flat clusterin
    
[^72]: 离线安全强化学习中的约束决策Transformer

    Constrained Decision Transformer for Offline Safe Reinforcement Learning. (arXiv:2302.07351v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.07351](http://arxiv.org/abs/2302.07351)

    本文研究了离线安全强化学习问题，提出了约束决策Transformer方法。该方法可以在部署过程中动态调整权衡，具有学习自适应、安全、鲁棒且高报酬的优势表现。

    

    安全强化学习通过与环境交互训练约束满足策略。本文致力于解决更具挑战性的问题：从离线数据集中学习安全策略。我们从多目标优化的角度研究了离线安全强化学习问题，并提出了可ε-可减概念来表征问题难度。安全和任务绩效之间的固有权衡激发了我们提出约束决策Transformer（CDT）方法，该方法可以在部署过程中动态调整权衡。大量实验证明了所提出的方法在学习自适应、安全、鲁棒且高报酬的策略方面的优势。与所有任务相同的超参数下，CDT在保持零-shot适应能力的同时，以大幅超过其变体和强离线安全RL基线的性能表现，使我们的方法更适用于约束条件下的现实RL。

    Safe reinforcement learning (RL) trains a constraint satisfaction policy by interacting with the environment. We aim to tackle a more challenging problem: learning a safe policy from an offline dataset. We study the offline safe RL problem from a novel multi-objective optimization perspective and propose the $\epsilon$-reducible concept to characterize problem difficulties. The inherent trade-offs between safety and task performance inspire us to propose the constrained decision transformer (CDT) approach, which can dynamically adjust the trade-offs during deployment. Extensive experiments show the advantages of the proposed method in learning an adaptive, safe, robust, and high-reward policy. CDT outperforms its variants and strong offline safe RL baselines by a large margin with the same hyperparameters across all tasks, while keeping the zero-shot adaptation capability to different constraint thresholds, making our approach more suitable for real-world RL under constraints. The code
    
[^73]: 现象意识状态的丰富性和难以言说性的来源

    Sources of Richness and Ineffability for Phenomenally Conscious States. (arXiv:2302.06403v3 [q-bio.NC] UPDATED)

    [http://arxiv.org/abs/2302.06403](http://arxiv.org/abs/2302.06403)

    本文提供了一个信息论动力系统的视角，来解释意识的丰富性和难以言说性。在我们的框架中，意识体验的丰富性对应于意识状态中的信息量，而难以言说性则对应于不同处理阶段丢失的信息量。

    This paper provides an information theoretic dynamical systems perspective on the richness and ineffability of consciousness. In their framework, the richness of conscious experience corresponds to the amount of information in a conscious state and ineffability corresponds to the amount of information lost at different stages of processing.

    意识状态（即有某种感受的状态）似乎既丰富又充满细节，又难以完全描述或回忆。特别是难以言说性的问题是哲学上长期存在的问题，部分激发了解释鸿沟的信念：意识不能归结为基础物理过程。在这里，我们提供了一个信息论动力系统的视角，来解释意识的丰富性和难以言说性。在我们的框架中，意识体验的丰富性对应于意识状态中的信息量，而难以言说性则对应于不同处理阶段丢失的信息量。我们描述了工作记忆中的吸引子动力学如何导致我们原始体验的贫乏回忆，语言的离散符号性质不足以描述体验的丰富和高维结构，以及认知功能相似性如何影响体验的共享和交流。

    Conscious states (states that there is something it is like to be in) seem both rich or full of detail, and ineffable or hard to fully describe or recall. The problem of ineffability, in particular, is a longstanding issue in philosophy that partly motivates the explanatory gap: the belief that consciousness cannot be reduced to underlying physical processes. Here, we provide an information theoretic dynamical systems perspective on the richness and ineffability of consciousness. In our framework, the richness of conscious experience corresponds to the amount of information in a conscious state and ineffability corresponds to the amount of information lost at different stages of processing. We describe how attractor dynamics in working memory would induce impoverished recollections of our original experiences, how the discrete symbolic nature of language is insufficient for describing the rich and high-dimensional structure of experiences, and how similarity in the cognitive function o
    
[^74]: PLay：使用潜在扩散生成参数条件化的布局设计

    PLay: Parametrically Conditioned Layout Generation using Latent Diffusion. (arXiv:2301.11529v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.11529](http://arxiv.org/abs/2301.11529)

    本文提出了一种有条件的潜在扩散模型 PLay，能够根据用户指定的指南在矢量图空间中生成参数化条件布局。相比以前的工作，在三个数据集和用户研究中表现更好，并为专业布局设计带来了新颖和互动的体验。

    

    布局设计是各种设计领域（包括用户界面、文档和图形设计）中的重要任务。由于此任务需要设计师付出繁琐的手动努力，先前的工作尝试使用生成模型自动化此过程，但通常无法提供直观的用户控制和实现设计目标。在本文中，我们构建了一个有条件的潜在扩散模型 PLay，它从用户指定的指南中生成矢量图空间中的参数化条件布局。这些指南通常由设计师在当前实践中用于表示他们的设计意图。我们的方法在包括 FID 和 FD-VG 的三个数据集和用户研究中优于以前的工作。此外，它为专业布局设计流程带来了一种新颖和互动的体验。

    Layout design is an important task in various design fields, including user interface, document, and graphic design. As this task requires tedious manual effort by designers, prior works have attempted to automate this process using generative models, but commonly fell short of providing intuitive user controls and achieving design objectives. In this paper, we build a conditional latent diffusion model, PLay, that generates parametrically conditioned layouts in vector graphic space from user-specified guidelines, which are commonly used by designers for representing their design intents in current practices. Our method outperforms prior works across three datasets on metrics including FID and FD-VG, and in user study. Moreover, it brings a novel and interactive experience to professional layout design processes.
    
[^75]: VRDU：面向视觉丰富的文档理解的基准测试

    VRDU: A Benchmark for Visually-rich Document Understanding. (arXiv:2211.15421v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2211.15421](http://arxiv.org/abs/2211.15421)

    本研究提出了一个名为VRDU的基准测试，以更全面地反映实际文档的复杂性，其中包含具有挑战性的丰富模式、复杂模板和多样的布局。该基准测试可用于评估文档中提取结构化数据的模型。

    

    理解丰富视觉化业务文档以提取结构化数据和自动化业务工作流程在学术界和工业界都受到关注。虽然最近的多模式语言模型取得了令人印象深刻的成果，但我们发现现有的基准测试不反映工业中实际文档的复杂性。在这项工作中，我们确定了更全面的基准测试的必要条件，并提出了一个称为Visually Rich Document Understanding (VRDU)的基准测试。VRDU包含两个数据集，代表了多种挑战：丰富的模式，包括各种数据类型以及分层实体; 复杂的模板，包括表格和多列布局; 以及单个文档类型中不同布局（模板）的多样性。我们设计了少样本和常规实验设置，以及一个精心设计的匹配算法来评估提取结果。我们报告了强基线的性能，并提供了三个观察结果：(1)通用n的推广。

    Understanding visually-rich business documents to extract structured data and automate business workflows has been receiving attention both in academia and industry. Although recent multi-modal language models have achieved impressive results, we find that existing benchmarks do not reflect the complexity of real documents seen in industry. In this work, we identify the desiderata for a more comprehensive benchmark and propose one we call Visually Rich Document Understanding (VRDU). VRDU contains two datasets that represent several challenges: rich schema including diverse data types as well as hierarchical entities, complex templates including tables and multi-column layouts, and diversity of different layouts (templates) within a single document type. We design few-shot and conventional experiment settings along with a carefully designed matching algorithm to evaluate extraction results. We report the performance of strong baselines and offer three observations: (1) generalizing to n
    
[^76]: DeepIPC：在真实环境中实现自动驾驶的深度感知和控制

    DeepIPC: Deeply Integrated Perception and Control for an Autonomous Vehicle in Real Environments. (arXiv:2207.09934v4 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2207.09934](http://arxiv.org/abs/2207.09934)

    本文介绍了DeepIPC，一个端到端自动驾驶模型，能够同时处理感知和控制任务。实验结果表明，DeepIPC具有最佳的可驾性和多任务性能，甚至比其他模型所需的参数更少。

    

    本文提出DeepIPC，一个处理自动驾驶中感知和控制的端到端模型。该模型由感知和控制器两个主要部分组成。感知模块使用RGBD图像执行语义分割和鸟瞰图语义映射，并提供它们的编码特征。同时，控制器模块使用GNSS定位和角速度的测量来处理这些特征，估计一系列路径点和潜在特征。然后，使用两个不同的代理将路径点和潜在特征转换为一组导航控制，以驱动车辆。该模型通过预测行车记录和在不同真实场景下进行自动驾驶来进行评估。实验结果表明，DeepIPC即使使用较少的参数也能实现最佳可驾性和多任务性能，优于其他模型。相关代码可在 https://github.com 找到。

    We propose DeepIPC, an end-to-end autonomous driving model that handles both perception and control tasks in driving a vehicle. The model consists of two main parts, perception and controller modules. The perception module takes an RGBD image to perform semantic segmentation and bird's eye view (BEV) semantic mapping along with providing their encoded features. Meanwhile, the controller module processes these features with the measurement of GNSS locations and angular speed to estimate waypoints that come with latent features. Then, two different agents are used to translate waypoints and latent features into a set of navigational controls to drive the vehicle. The model is evaluated by predicting driving records and performing automated driving under various conditions in real environments. The experimental results show that DeepIPC achieves the best drivability and multi-task performance even with fewer parameters compared to the other models. Codes are available at https://github.co
    
[^77]: 非凸非光滑问题中随机优化的稳定性和泛化能力

    Stability and Generalization of Stochastic Optimization with Nonconvex and Nonsmooth Problems. (arXiv:2206.07082v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2206.07082](http://arxiv.org/abs/2206.07082)

    本文首次启动了对非凸非光滑问题上随机优化的系统稳定性和泛化分析，引入了新颖的算法稳定性度量，并建立了它们与种群梯度和经验梯度之间的定量连接。

    

    随机优化在机器学习中的应用非常广泛，这促使了许多理论研究，以理解其实际成功。大多数现有的研究都集中在优化误差的收敛性上，而随机优化的泛化分析远远滞后。这尤其适用于实践中经常遇到的非凸非光滑问题。在本文中，我们首次启动了对非凸非光滑问题上随机优化的系统稳定性和泛化分析。我们引入了新颖的算法稳定性度量，并建立了它们与种群梯度和经验梯度之间的定量连接，然后进一步将其扩展到研究经验风险和群体风险的Moreau包络之间的差距。据我们所知，这些稳定性和泛化性的定量联系，无论是在梯度还是Moreau包络方面，都是文献中首次出现的。我们进一步将我们的泛化理论应用于分析深度线性神经网络(DLNN)，在其中我们展示了所提出的理论可以比现有的工作提供更紧的泛化误差界。我们的理论结果通过对合成和现实世界数据集的数值实验进行了验证。

    Stochastic optimization has found wide applications in minimizing objective functions in machine learning, which motivates a lot of theoretical studies to understand its practical success. Most of existing studies focus on the convergence of optimization errors, while the generalization analysis of stochastic optimization is much lagging behind. This is especially the case for nonconvex and nonsmooth problems often encountered in practice. In this paper, we initialize a systematic stability and generalization analysis of stochastic optimization on nonconvex and nonsmooth problems. We introduce novel algorithmic stability measures and establish their quantitative connection on the gap between population gradients and empirical gradients, which is then further extended to study the gap between the Moreau envelope of the empirical risk and that of the population risk. To our knowledge, these quantitative connection between stability and generalization in terms of either gradients or Morea
    
[^78]: AIGenC：通过创造力进行 AI 通用化

    AIGenC: AI generalisation via creativity. (arXiv:2205.09738v4 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2205.09738](http://arxiv.org/abs/2205.09738)

    AIGenC 提出了一种创造性的计算模型，通过引入概念空间和分层结构来提高人工智能代理的通用性和创新能力，实验表明其在通用任务中的表现优于最先进方法。

    

    本文受到创造力的认知理论的启发，引入了一个计算模型（AIGenC），旨在提供必要的组成部分，使人工智能代理能够学习、使用和生成可转移的表征。与机器表征学习不同的是，生物表征包括关系和联想信息，嵌入了丰富和结构化的概念空间。AIGenC 模型采用分层的图形架构，具有不同组件获取的各种级别和类型的表征。第一个组件部分——概念处理，从感官输入中提取对象和支配因素，并将它们编码成概念空间。生成的表征存储在双重记忆系统中，并通过强化学习获得目标导向和时间信息的丰富度，从而创建了更高层次的抽象。另外两个组件并行工作，检测和恢复存储表征中的相关概念和创造连接，使系统能够生成新型解决方案并在新场景中应用它们。实验结果表明，AIGenC 在各种通用任务中优于现有的最先进方法。

    Inspired by cognitive theories of creativity, this paper introduces a computational model (AIGenC) that lays down the necessary components to enable artificial agents to learn, use and generate transferable representations. Unlike machine representation learning, which relies exclusively on raw sensory data, biological representations incorporate relational and associative information that embeds rich and structured concept spaces. The AIGenC model poses a hierarchical graph architecture with various levels and types of representations procured by different components. The first component, Concept Processing, extracts objects and affordances from sensory input and encodes them into a concept space. The resulting representations are stored in a dual memory system and enriched with goal-directed and temporal information acquired through reinforcement learning, creating a higher-level of abstraction. Two additional components work in parallel to detect and recover relevant concepts and cr
    
[^79]: 基于脑启发计算的高效离线强化学习

    Efficient Off-Policy Reinforcement Learning via Brain-Inspired Computing. (arXiv:2205.06978v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.06978](http://arxiv.org/abs/2205.06978)

    本文提出了一种QHD离线策略，它基于超维强化学习与脑启发计算，实现稳健和实时学习。QHD相比于DQN具有更高效率且适用于高效的强化学习，具有在线和实时学习潜力。

    

    强化学习（RL）已经开创了增强现有智能系统的新机会，这些系统通常包括复杂的决策过程。然而，现代RL算法，如Deep Q-Networks（DQN），基于深度神经网络，导致计算成本高昂。本文提出了QHD，一种基于超维强化学习的离线策略，模仿大脑的属性，实现稳健和实时学习。QHD依靠轻量级的脑启发模型，在未知环境中学习最佳策略。在桌面和功率限制的嵌入式平台上，QHD的整体效率显著优于DQN，同时提供更高或可比较的回报。QHD也适用于高度有效的强化学习，具有极大的在线和实时学习潜力。我们的解决方案支持小的经验重放批量，与DQN相比提供12.3倍的加速，并确保最小的质量损失。我们的评估显示，QHD与先进的RL算法相比，具有更少的计算成本和更高的效率。

    Reinforcement Learning (RL) has opened up new opportunities to enhance existing smart systems that generally include a complex decision-making process. However, modern RL algorithms, e.g., Deep Q-Networks (DQN), are based on deep neural networks, resulting in high computational costs. In this paper, we propose QHD, an off-policy value-based Hyperdimensional Reinforcement Learning, that mimics brain properties toward robust and real-time learning. QHD relies on a lightweight brain-inspired model to learn an optimal policy in an unknown environment. On both desktop and power-limited embedded platforms, QHD achieves significantly better overall efficiency than DQN while providing higher or comparable rewards. QHD is also suitable for highly-efficient reinforcement learning with great potential for online and real-time learning. Our solution supports a small experience replay batch size that provides 12.3 times speedup compared to DQN while ensuring minimal quality loss. Our evaluation sho
    
[^80]: 基于价值梯度加权的模型驱动强化学习

    Value Gradient weighted Model-Based Reinforcement Learning. (arXiv:2204.01464v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2204.01464](http://arxiv.org/abs/2204.01464)

    本文提出了一种基于价值梯度加权的模型驱动强化学习方法，用于提高模型学习的性能。

    

    模型驱动强化学习（MBRL）是一种高效获取控制策略的技术，但模型误差往往会导致性能下降。MBRL中的模型通常仅用于重建动态，特别是状态观察值，而模型误差对策略的影响不会被训练目标捕捉到。这导致MBRL的目标与实际使用的损失函数的目标不匹配，从而影响策略和价值的学习效果。本文提出了一种基于价值梯度加权的模型驱动强化学习方法（VaGraM），用于解决这个问题。

    Model-based reinforcement learning (MBRL) is a sample efficient technique to obtain control policies, yet unavoidable modeling errors often lead performance deterioration. The model in MBRL is often solely fitted to reconstruct dynamics, state observations in particular, while the impact of model error on the policy is not captured by the training objective. This leads to a mismatch between the intended goal of MBRL, enabling good policy and value learning, and the target of the loss function employed in practice, future state prediction. Naive intuition would suggest that value-aware model learning would fix this problem and, indeed, several solutions to this objective mismatch problem have been proposed based on theoretical analysis. However, they tend to be inferior in practice to commonly used maximum likelihood (MLE) based approaches. In this paper we propose the Value-gradient weighted Model Learning (VaGraM), a novel method for value-aware model learning which improves the perfo
    
[^81]: 多维时间序列的缺失值填充方法DeepMVI研究

    Missing Value Imputation on Multidimensional Time Series. (arXiv:2103.01600v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2103.01600](http://arxiv.org/abs/2103.01600)

    本文提出了DeepMVI，一种用于填充多维时间序列数据中缺失值的深度学习方法。该方法结合时间序列中的精细和粗粒度模式，以及跨分类维度的相关系列趋势。经过改进，DeepMVI表现出比其他算法更优秀的结果。

    

    本文提出了DeepMVI，一种用于填充多维时间序列数据中缺失值的深度学习方法。由于来自不同来源的长时间数据聚合会产生大量缺失数据，因此对于可靠的数据分析，需要仔细处理缺失数据。我们的方法使用神经网络来结合时间序列中的精细和粗粒度模式，以及跨分类维度的相关系列趋势。在经过多次不成功的尝试之后，我们设计了自己的网络，其中包括具有新型卷积窗口特征和核回归的时间变形器。在各种真实数据集上的填充设置中，我们展示了比几种最先进的算法更好的结果。

    We present DeepMVI, a deep learning method for missing value imputation in multidimensional time-series datasets. Missing values are commonplace in decision support platforms that aggregate data over long time stretches from disparate sources, and reliable data analytics calls for careful handling of missing data. One strategy is imputing the missing values, and a wide variety of algorithms exist spanning simple interpolation, matrix factorization methods like SVD, statistical models like Kalman filters, and recent deep learning methods. We show that often these provide worse results on aggregate analytics compared to just excluding the missing data. DeepMVI uses a neural network to combine fine-grained and coarse-grained patterns along a time series, and trends from related series across categorical dimensions. After failing with off-the-shelf neural architectures, we design our own network that includes a temporal transformer with a novel convolutional window feature, and kernel regr
    

