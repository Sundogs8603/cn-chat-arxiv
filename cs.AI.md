# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Towards Trustworthy Explanation: On Causal Rationalization.](http://arxiv.org/abs/2306.14115) | 该论文介绍了一种新的因果关系解释方法，通过在解释中引入非虚假性和效率，从因果推断的角度定义了因果概率，从而建立了必要和充分解释的主要组成部分，相比现有的基于关联的解释方法，这种方法有更加优越的性能表现。 |
| [^2] | [TNPAR: Topological Neural Poisson Auto-Regressive Model for Learning Granger Causal Structure from Event Sequences.](http://arxiv.org/abs/2306.14114) | 该论文提出了一种基于拓扑神经泊松自回归模型的方法，同时考虑先验拓扑网络和潜在的Granger因果结构来学习事件序列中的Granger因果关系，该方法通过推断因果关系来解决序列数据中的非 i.i.d. 问题 |
| [^3] | [Is RLHF More Difficult than Standard RL?.](http://arxiv.org/abs/2306.14111) | 本文证明了对于广泛的偏好模型，我们可以使用现有的算法和技术直接解决基于偏好的RL问题，而几乎不需要额外的成本。 |
| [^4] | [Semi-supervised Object Detection: A Survey on Recent Research and Progress.](http://arxiv.org/abs/2306.14106) | 本文综述并介绍了半监督物体检测 (SSOD) 的最新研究进展，主要包括数据增强方法，伪标签，一致性正则化，图形和迁移学习等半监督策略，并讨论了未来的研究方向和常用的评估指标。 |
| [^5] | [Language models are weak learners.](http://arxiv.org/abs/2306.14101) | 本文证明了，基于提示的大型语言模型可以作为弱学习器应用于表格数据的boosting算法中，并且在某些情况下可以优于传统的树形模型。 |
| [^6] | [Chinese Fine-Grained Financial Sentiment Analysis with Large Language Models.](http://arxiv.org/abs/2306.14096) | 本文提出了一个用于企业预警的新型、广泛的中文细粒度金融情感分析数据集FinChina SA，并使用现有开源大语言模型对其进行评估和实验。该数据集将成为推进真实金融情感分析任务探索的宝贵资源。 |
| [^7] | [Fighting Uncertainty with Gradients: Offline Reinforcement Learning via Diffusion Score Matching.](http://arxiv.org/abs/2306.14079) | 本文提出了平滑的距离数据度量标准，并将其与离线强化学习相结合，以对抗不确定性和分布偏移的挑战。该方法不仅在最小化梯度不确定性时稳定收敛到数据，而且不易低估真实不确定性，是一种有前途的策略搜索方法。 |
| [^8] | [Full Automation of Goal-driven LLM Dialog Threads with And-Or Recursors and Refiner Oracles.](http://arxiv.org/abs/2306.14077) | 本文提出一种全自动化的LLM对话线程算法，通过递归的探索和展开，实现对任务的深度逐步推理，并应用于推论预测、因果解释、建议等方案的实现。 |
| [^9] | [Intensity-free Convolutional Temporal Point Process: Incorporating Local and Global Event Contexts.](http://arxiv.org/abs/2306.14072) | 本文提出了一种TPP建模方法，将连续时间卷积事件编码器与RNN集成，以融合局部和全局上下文。实验结果表明，该模型具有较好的性能表现。 |
| [^10] | [Offline Policy Evaluation for Reinforcement Learning with Adaptively Collected Data.](http://arxiv.org/abs/2306.14063) | 本论文提出了一种自适应采集数据的离线强化学习策略评估方法，为表格MDPs推导出高概率、实例相关的误差边界，并实现了自适应设置下的极小值最优离线学习。 |
| [^11] | [On the Uses of Large Language Models to Interpret Ambiguous Cyberattack Descriptions.](http://arxiv.org/abs/2306.14062) | 本文研究了大型语言模型在解释模糊的网络攻击描述中的应用。实验表明，以相关文本数据训练的BaseLLM可以大大改善攻击技术的解释，并且超越了领域专家的解释能力。该研究还讨论了使用LLMs进行网络威胁情报分析的影响和局限性。 |
| [^12] | [Transforming a Quadruped into a Guide Robot for the Visually Impaired: Formalizing Wayfinding, Interaction Modeling, and Safety Mechanism.](http://arxiv.org/abs/2306.14055) | 本论文探讨了将四足机器人转化为导航机器人的三个关键主题，并介绍了一种名为“延迟挽具”的交互模型以及一种行动屏蔽机制来提高用户的安全性。 |
| [^13] | [Towards Optimal Pricing of Demand Response -- A Nonparametric Constrained Policy Optimization Approach.](http://arxiv.org/abs/2306.14047) | 本研究提出了一种非参数约束策略优化方法，通过消除对策略表示的限制性假设，提高优化度并确保策略更新的稳定性，从而实现电力负荷高峰和低峰之间的合理转移。 |
| [^14] | [Machine Learning needs its own Randomness Standard: Randomised Smoothing and PRNG-based attacks.](http://arxiv.org/abs/2306.14043) | 本文研究了机器学习中随机性可被攻击者利用的问题，主要关注流行的随机平滑方法。该方法用于训练可证明鲁棒性的模型和量化不确定性，但其随机性可能导致系统被攻击。 |
| [^15] | [Partitioning-Guided K-Means: Extreme Empty Cluster Resolution for Extreme Model Compression.](http://arxiv.org/abs/2306.14031) | 本研究提出了一种名为PG k-means的算法，基于划分来解决空簇，提高了iPQ与量化噪声的精确性。 |
| [^16] | [LLM-assisted Generation of Hardware Assertions.](http://arxiv.org/abs/2306.14027) | 本论文研究使用LLMs来生成硬件的安全断言。通过使用自然语言提示生成SystemVerilog断言来替代编写具有挑战的安全断言。 |
| [^17] | [A clustering and graph deep learning-based framework for COVID-19 drug repurposing.](http://arxiv.org/abs/2306.13995) | 本文提出了一种基于聚类和图深度学习的方法，在 COVID-19 药物再利用中发现了具有见地的药物相互作用，并确定了潜在有效的药物再利用候选物。 |
| [^18] | [Robust Classification of High-Dimensional Data using Data-Adaptive Energy Distance.](http://arxiv.org/abs/2306.13985) | 该论文提出了一种用于高维低样本量数据分类的稳健的数据自适应能量距离分类器，该分类器无需调参且在一定条件下可以实现完美分类，已在模拟研究和实际数据分析中得到证明比其他方法表现更优。 |
| [^19] | [Fusing Multimodal Signals on Hyper-complex Space for Extreme Abstractive Text Summarization (TL;DR) of Scientific Contents.](http://arxiv.org/abs/2306.13968) | 本文介绍了mTLDR数据集，这是一个多模态数据集，利用超复空间上融合多模态信号进行极端抽象文本摘要的任务。mTLDRgen模型成功实现了TL;DR生成的多模态系统。 |
| [^20] | [Boosting Model Inversion Attacks with Adversarial Examples.](http://arxiv.org/abs/2306.13965) | 本文提出了一种新的基于学习的模型反演攻击训练方法，通过添加语义损失函数和注入对抗样本，可以在黑盒设置下提高攻击精度和成功率。 |
| [^21] | [Categorical Approach to Conflict Resolution: Integrating Category Theory into the Graph Model for Conflict Resolution.](http://arxiv.org/abs/2306.13961) | 本论文介绍了一种新型的冲突解决框架，称为C-GMCR，它将范畴论整合到传统的图模型中，能够提供更抽象和通用的分析冲突解决的方式。通过应用到囚徒困境和其他案例中，发现分类方法提供了新的视角和可能导致更有效的冲突解决策略的发展。 |
| [^22] | [Regular SE(3) Group Convolutions for Volumetric Medical Image Analysis.](http://arxiv.org/abs/2306.13960) | 本文提出了基于正则化SE(3)群卷积的体积医学图像分析方法，通过分解连续SO(3)核和空间核以实现旋转平移等变性，并在医学分类任务中获得了显著性能提升。 |
| [^23] | [Emotion Flip Reasoning in Multiparty Conversations.](http://arxiv.org/abs/2306.13959) | 本文旨在识别说话者情感翻转背后的推动者，提出了一个数据集和神经架构c进行支持。 |
| [^24] | [Pointwise-in-Time Explanation for Linear Temporal Logic Rules.](http://arxiv.org/abs/2306.13956) | 本文提出了一个可以评估给定路径规划中特定时间点上的单个线性时间逻辑(LTL)约束的相关性和状态的框架，可以用于在离散时间、离散空间中执行有限计划的代理任务中，为用户提供时间点解释和规则参数状态的洞察力。 |
| [^25] | [Unleashing Realistic Air Quality Forecasting: Introducing the Ready-to-Use PurpleAirSF Dataset.](http://arxiv.org/abs/2306.13948) | 该论文介绍了PurpleAirSF数据集，这是一个易于获取的数据集，具有高时间分辨率、多种空气质量测量指标和广泛的地理范围，可用于研究人员的空气质量预测建模和空气污染模式研究，同时此数据集可用于未来开发新型的应用模型。 |
| [^26] | [Large Sequence Models for Sequential Decision-Making: A Survey.](http://arxiv.org/abs/2306.13945) | 本综述全面概述了使用Transformer等序列模型解决顺序决策问题的最近研究进展，并按照处理样本效率、信用分配和部分可观察性的方式对其进行分类。 |
| [^27] | [Safe Reinforcement Learning with Dead-Ends Avoidance and Recovery.](http://arxiv.org/abs/2306.13944) | 本文提出了一种利用死局的边界来辨别安全和不安全状态，以确保安全性同时减少对探索的限制的方法。采用了分离的强化学习框架，训练了两个策略：一个任务策略，专注于任务表现，以及一个恢复策略，最大化安全性。 |
| [^28] | [Are Good Explainers Secretly Human-in-the-Loop Active Learners?.](http://arxiv.org/abs/2306.13935) | 本文提出了一种可解释的AI技术，用于获取额外的训练数据，同时考虑到人类的介入，这可以通过模拟来评估其效用，同时具有与标准主动学习算法的可比性。 |
| [^29] | [Comparative Study of Predicting Stock Index Using Deep Learning Models.](http://arxiv.org/abs/2306.13931) | 本文通过对传统预测方法和深度学习模型进行比较研究，证明Deep AR在股票指数预测方面表现最佳，具有较高的鲁棒性。 |
| [^30] | [Evaluating the Utility of GAN Generated Synthetic Tabular Data for Class Balancing and Low Resource Settings.](http://arxiv.org/abs/2306.13929) | 本研究评估了GAN生成的合成数据在解决分类任务中不平衡数据和提高模型在低资源环境中的性能方面的实用性，并发现在GAN合成数据上进行训练的模型性能表现更佳。 |
| [^31] | [Active Data Acquisition in Autonomous Driving Simulation.](http://arxiv.org/abs/2306.13923) | 本文提出了一种主动数据采集策略来解决自动驾驶模拟中大量冗余数据集的问题，并通过实验验证，该策略可以显著降低标注成本和数据集大小，同时提高数据集的整体质量，从而提高自动驾驶系统的性能。 |
| [^32] | [Spatio-temporal Storytelling? Leveraging Generative Models for Semantic Trajectory Analysis.](http://arxiv.org/abs/2306.13905) | 本文通过生成语言模型对语义轨迹进行分析和生成合成语义轨迹数据，为从城市规划到个性化推荐引擎和商业策略等一系列应用做出贡献。 |
| [^33] | [Math Word Problem Solving by Generating Linguistic Variants of Problem Statements.](http://arxiv.org/abs/2306.13899) | 该论文提出了一个通过生成问题文本语言变体的方法来解决数学问题，该方法利用DeBERTa作为编码器，同时引入了一个挑战性的数据集用于评估模型性能。结果表明，该框架在两个基准数据集以及作者提出的数据集上优于现有技术水平的模型，证明了其推导正确的解决方案表达式的能力。 |
| [^34] | [ICN: Interactive Convolutional Network for Forecasting Travel Demand of Shared Micromobility.](http://arxiv.org/abs/2306.13897) | 本文提出了一种名为交互式卷积网络（ICN）的深度学习模型，用于预测共享微移动的时空出行需求。ICN模型采用新颖的通道扩张方法和卷积操作同时捕捉时间和空间依赖关系，提取不同时间分辨率下的特征，实现高精度时空共享微移动需求的预测。 |
| [^35] | [Differentially Private Decentralized Deep Learning with Consensus Algorithms.](http://arxiv.org/abs/2306.13892) | 本论文提出了一种具有差分隐私保护的分散学习算法，可用于合作分散深度学习，防止共享模型参数时泄露私有数据集的信息。 |
| [^36] | [Manipulation Risks in Explainable AI: The Implications of the Disagreement Problem.](http://arxiv.org/abs/2306.13885) | 本文讨论可解释人工智能中的操纵风险，即同一决策或预测可能有多种解释带来的挑战。本文分析了攻击机器学习模型或底层数据以影响解释与直接利用解释阶段的策略，并探讨了解释提供者可追求的几个目标和具体场景。 |
| [^37] | [Learning-to-Rank Meets Language: Boosting Language-Driven Ordering Alignment for Ordinal Classification.](http://arxiv.org/abs/2306.13856) | 本文提出了一种利用语言驱动的高效序数分类方法，即L2RCLIP，它通过视觉-语言对齐任务充分利用语言中的序数先验，利用补充提示调整技术RankFormer增强原始排序提示的排序关系，并使用跨模态排序约束损失(CMOCL)进一步将语言先验融入模型中。在多个标准数据集中，L2RCLIP都比现有最先进方法具有更好的性能表现。 |
| [^38] | [Similarity Preserving Adversarial Graph Contrastive Learning.](http://arxiv.org/abs/2306.13854) | 本文提出了一种相似性保持的对抗图对比学习（SP-AGCL）框架，可以实现对抗攻击的对抗鲁棒性，同时保持节点特征相似性。 |
| [^39] | [Is Pre-training Truly Better Than Meta-Learning?.](http://arxiv.org/abs/2306.13841) | 在少样本学习中，当数据集的正式多样性较低时，预训练模型（PT）胜过模型无关元学习（MAML）。当正式多样性较高时，MAML更好。 |
| [^40] | [Beyond Scale: the Diversity Coefficient as a Data Quality Metric Demonstrates LLMs are Pre-trained on Formally Diverse Data.](http://arxiv.org/abs/2306.13840) | 本论文提出使用多样性系数作为LLM预训练数据质量的指标，研究表明公开可用的LLM数据集的多样性系数很高。 |
| [^41] | [DEKGCI: A double-sided recommendation model for integrating knowledge graph and user-item interaction graph.](http://arxiv.org/abs/2306.13837) | 本文提出了DEKGCI，一种双面推荐模型，在用户-物品交互图和知识图谱中同时丰富用户和物品表示，以有效捕捉用户和物品之间的联合交互。 |
| [^42] | [The Double Helix inside the NLP Transformer.](http://arxiv.org/abs/2306.13817) | 本文介绍了一个NLP Transformer中分析位置、句法、语义和上下文信息的框架，揭示了位置信息通过螺旋路径在深层中自我分离，并在编码器和解码器侧生成词性聚类。提出了替代在语义嵌入中添加位置信息的方法。 |
| [^43] | [Potential Benefits of Employing Large Language Models in Research in Moral Education and Development.](http://arxiv.org/abs/2306.13805) | 本文探讨如何使用大型语言模型（LLM）在道德教育和发展研究领域做出贡献。最近的LLM具有新兴的上下文学习和思维链功能，可以通过推理和修订来解决困境。 |
| [^44] | [Elephants and Algorithms: A Review of the Current and Future Role of AI in Elephant Monitoring.](http://arxiv.org/abs/2306.13803) | 本文探讨了AI和ML在保护非洲保护区中至关重要的大象方面的作用。新的AI和ML技术提供了解决方案以处理并提取重要信息。在AI专家和生态研究人员之间的协作下，这些创新技术可以帮助增强野生动物保护，为其他物种设定先例。 |
| [^45] | [Achieving Diversity in Objective Space for Sample-efficient Search of Multiobjective Optimization Problems.](http://arxiv.org/abs/2306.13780) | 本文提出一种新的多目标优化搜索方法，通过搜索满足用户指定性能标准的多样化结果集来找到一组有前途的设计解决方案，具有更好地展示出优秀解空间和提供决策者优秀设计决策的优势。 |
| [^46] | [CeBed: A Benchmark for Deep Data-Driven OFDM Channel Estimation.](http://arxiv.org/abs/2306.13761) | 本文提出了一个称为CeBed的测试平台，用于评估和比较不同的数据驱动OFDM信道估计方法，解决了领域内实验条件不一致和缺乏可重复性的问题。 |
| [^47] | [Task-Driven Graph Attention for Hierarchical Relational Object Navigation.](http://arxiv.org/abs/2306.13760) | 本文研究了分层关系物体导航任务，使用场景图作为环境表示，提出了基于任务驱动图注意力模型的解决方案，可有效地探索具有长时间跨度和部分可观察性的大场景。 |
| [^48] | [Improving Panoptic Segmentation for Nighttime or Low-Illumination Urban Driving Scenes.](http://arxiv.org/abs/2306.13725) | 本文提出了两种新的方法，通过域翻译来提高全景分割在夜间或低照度城市驾驶场景中的性能和鲁棒性。 |
| [^49] | [Social AI and the Challenges of the Human-AI Ecosystem.](http://arxiv.org/abs/2306.13723) | 本文介绍了人工智能生态系统研究中出现的挑战，并探讨了社交AI提高集体问题解决能力方面的潜力，同时也需要解决新的技术和伦理问题。 |
| [^50] | [Use case cards: a use case reporting framework inspired by the European AI Act.](http://arxiv.org/abs/2306.13701) | 本文提出了一种新的用例文档化框架，称为"用例卡"，旨在隐式评估AI系统的风险水平并定义相关要求。这是一种基于UML的模板和支持UML的图的组合。 |
| [^51] | [Broadening the perspective for sustainable AI: Comprehensive sustainability criteria and indicators for AI systems.](http://arxiv.org/abs/2306.13686) | 本文提出了SCAIS框架，包含一组19个可持续性标准和67个指标，旨在促进和结构化关于可持续人工智能的讨论。这种跨学科方法为实现人工智能系统的可持续发展提供了基础。 |
| [^52] | [Model Families for Multi-Criteria Decision Support: A COVID-19 Case Study.](http://arxiv.org/abs/2306.13683) | 本文提出了一种模型家族的思维方式，将研究任务分解为相互作用的较小模型以适应不断变化的建模目的、系统范围和映射因果关系。以COVID-19为例，这种策略带来许多优势。 |
| [^53] | [Evaluating the overall sensitivity of saliency-based explanation methods.](http://arxiv.org/abs/2306.13682) | 本文研究了如何生成对“黑匣子”深度学习模型的忠实解释，提出了一个扩展的测试方法来确定解释方法的总体敏感性，并通过例子展示了如何使用这个方法比较卷积神经网络的多个解释方法。 |
| [^54] | [GPT-Based Models Meet Simulation: How to Efficiently Use Large-Scale Pre-Trained Language Models Across Simulation Tasks.](http://arxiv.org/abs/2306.13679) | 本文是关于大规模预训练语言模型在科学仿真中的研究。研究关注四个建模和仿真任务的LLMs的预期益处和限制，并提供实际指导。 |
| [^55] | [What drives the acceptance of AI technology?: the role of expectations and experiences.](http://arxiv.org/abs/2306.13670) | 人们接受人工智能的意愿很大程度上受当前人工智能产品和服务的经验、对人工智能的期望和过去的ICT技术经验的影响。人工智能经验和ICT经验以两种方式影响人工智能接受意愿，以直接途径和间接途径，而对人工智能的期望值则可以进一步提高人工智能的接受意愿。 |
| [^56] | [Statistical relational learning and neuro-symbolic AI: what does first-order logic offer?.](http://arxiv.org/abs/2306.13660) | 本文旨在探讨使用一阶逻辑来表示知识的方式，对机器学习和逻辑专家都有指导作用，同时也对统计关系学习和神经符号AI领域有重要的理论意义。 |
| [^57] | [Toward A Logical Theory Of Fairness and Bias.](http://arxiv.org/abs/2306.13659) | 本文通过对公平性定义进行形式化重构，将其应用基于认知环境模型，提出了实现公平和偏见理论，包括三个概念：通过无意识来实现公平、人口学平等和反事实公正。 |
| [^58] | [On Computational Mechanisms for Shared Intentionality, and Speculation on Rationality and Consciousness.](http://arxiv.org/abs/2306.13657) | 本文提出了一种共享意图优先的理论，探讨了支持计算机代理人之间共享意图的基本机制所必须具备的特征，并探索了这些机制如何适用于人类以提供对人类理性和意识的解释。 |
| [^59] | [Pruning for Better Domain Generalizability.](http://arxiv.org/abs/2306.13237) | 本文研究了基于剪枝的域泛化方法，提出了一种新的剪枝评分方法DSS，该方法不是为了保持源准确性，而是直接增强模型的鲁棒性。实验证明该方法可以与最先进的泛化方法结合使用，即便只引入少量稀疏也能显著提高模型性能。 |
| [^60] | [Can Differentiable Decision Trees Learn Interpretable Reward Functions?.](http://arxiv.org/abs/2306.13004) | 本文提出了使用可微分决策树从人类偏好中学习具有表达能力和可解释性的奖励函数，通过在多个环境上的评估，发现该方法能够学习到可解释的奖励函数，但在强化学习测试时表现受到树的离散性的影响。 |
| [^61] | [An Overview of Catastrophic AI Risks.](http://arxiv.org/abs/2306.12001) | 本文综述了人工智能灾难性风险的四个主要来源，包括恶意使用、人工智能竞赛、组织风险和流氓人工智能。 |
| [^62] | [Structure-Aware Robustness Certificates for Graph Classification.](http://arxiv.org/abs/2306.11915) | 该论文提出了一种新的随机平滑方法，可以根据图的不同预定义结构生成对应的鲁棒性证书，从而在多个基准图分类任务中取得领先的对抗性攻击下鲁棒性结果。 |
| [^63] | [Learn to Accumulate Evidence from All Training Samples: Theory and Practice.](http://arxiv.org/abs/2306.11113) | 本文提出了一种新的激活函数，All-Positive (AP)激活，避免了现有证据激活函数中的零证据区域，同时能够更好地表达负证据量。实验证明，该方法在多个基准数据集上优于现有方法。 |
| [^64] | [Neural Priming for Sample-Efficient Adaptation.](http://arxiv.org/abs/2306.10191) | 本文提出神经启动技术，用于使大型预训练模型适应于分布变化和下游任务，无需过多标记样本。通过回忆数据进行轻量更新可以显著提高准确性。 |
| [^65] | [Energy-Based Cross Attention for Bayesian Context Update in Text-to-Image Diffusion Models.](http://arxiv.org/abs/2306.09869) | 本论文提出了能量交叉注意力的EBM框架，通过更新和转移上下文向量，隐式最小化能量函数的嵌套层次，优化文本到图像扩散模型的语义对齐问题，实现零样本组合生成。 |
| [^66] | [TSMixer: Lightweight MLP-Mixer Model for Multivariate Time Series Forecasting.](http://arxiv.org/abs/2306.09364) | TSMixer是一种用于多元时间序列预测的轻量级MLP-Mixer模型，可以有效地捕捉时间序列属性并在准确性方面超越了Transformers的方法。 |
| [^67] | [Exploring the MIT Mathematics and EECS Curriculum Using Large Language Models.](http://arxiv.org/abs/2306.08997) | 通过使用大型语言模型，翻译了MIT数学和EECS课程中的4550个题目，开发出一个可以自动评分的模型，并探索了课程、问题和答案之间的关系。 |
| [^68] | [NeuroGraph: Benchmarks for Graph Machine Learning in Brain Connectomics.](http://arxiv.org/abs/2306.06202) | 本文介绍了神经成像领域的图机器学习基准测试NeuroGraph，并探讨了数据集生成的搜索空间。 |
| [^69] | [Pave the Way to Grasp Anything: Transferring Foundation Models for Universal Pick-Place Robots.](http://arxiv.org/abs/2306.05716) | 本研究提出了一种基于语言分割掩模的新方法，用于解决通用型机器人的泛化能力问题，提高了在开放域场景中新对象的抓取操作的学习效率和推广效果。 |
| [^70] | [FheFL: Fully Homomorphic Encryption Friendly Privacy-Preserving Federated Learning with Byzantine Users.](http://arxiv.org/abs/2306.05112) | 本论文介绍了一种新的联邦学习算法，采用FHE加密技术，既可以保护模型更新的隐私，又可以防止恶意用户破坏全局模型。 |
| [^71] | [ChatGPT Informed Graph Neural Network for Stock Movement Prediction.](http://arxiv.org/abs/2306.03763) | 该研究介绍了一种新的框架，利用ChatGPT技术增强图神经网络，能够从财经新闻中提取出不断变化的网络结构，并用于股票价格预测，获得了超过基于深度学习的最新基准的表现，提示了ChatGPT在文本推断和金融预测方面的潜力。 |
| [^72] | [SQL-PaLM: Improved Large Language Model Adaptation for Text-to-SQL.](http://arxiv.org/abs/2306.00739) | 本文提出了一种基于大语言模型的Text-to-SQL模型SQL-PaLM，使用了面向Text-to-SQL的基于执行的自一致提示方法，在Spider上实现了77.3%的测试套件准确度，并显着超越以前的最新技术的方法。 |
| [^73] | [Task-Equivariant Graph Few-shot Learning.](http://arxiv.org/abs/2305.18758) | 本文提出了一种任务等变图Few-shot学习（TEG）框架，利用图神经网络的等变性质来使模型学习可转移的任务适应策略。该方法在各种Few-shot分类基准上展示了最先进的性能。 |
| [^74] | [Generating Behaviorally Diverse Policies with Latent Diffusion Models.](http://arxiv.org/abs/2305.18738) | 本文使用扩散模型将档案压缩为单个对于策略参数的生成模型，实现了13倍的压缩比率，同时保留了98%的原始回报和89%的原始覆盖率，并允许灵活选择和排序行为。 |
| [^75] | [Networked Time Series Imputation via Position-aware Graph Enhanced Variational Autoencoders.](http://arxiv.org/abs/2305.18612) | 本文提出了一种新颖的基于位置感知图增强变分自编码器方法来处理网络时间序列插补问题。该方法能够处理NTS数据中的图动态和缺失边，从而在合成和真实数据集上实现了优异的表现。 |
| [^76] | [Inverse square Levy walk emerging universally in goal-oriented tasks.](http://arxiv.org/abs/2305.15559) | 本研究证明了反平方Levy步态（称为Cauchy步态）在目标导向任务中普遍出现。 |
| [^77] | [Multi-agent Reinforcement Learning: Asynchronous Communication and Linear Function Approximation.](http://arxiv.org/abs/2305.06446) | 该论文探讨了多智能体强化学习在情节式马尔可夫决策过程中的协作问题，提出了一种基于值迭代的算法，可以实现异步通信，在保证合作优势的同时降低通信开销。通过提供和证明的算法和复杂度界限，为多智能体强化学习在实际应用中提供理论依据。 |
| [^78] | [A Framework for Designing Foundation Model based Systems.](http://arxiv.org/abs/2305.05352) | 本文提出了一个基于基础模型的系统分类体系，分类和比较了基础模型和基于基础模型的系统的特点。它为设计基于基础模型的系统时做出主要的设计决策提供了具体的指导，并突出了相关的权衡。 |
| [^79] | [Learngene: Inheriting Condensed Knowledge from the Ancestry Model to Descendant Models.](http://arxiv.org/abs/2305.02279) | 本文提出了一种机器学习范式 Learngene，将积累的知识压缩成更为紧凑的信息片段并继承给后代模型，以便于适应新的环境 |
| [^80] | [Beyond Classification: Financial Reasoning in State-of-the-Art Language Models.](http://arxiv.org/abs/2305.01505) | 本研究探讨了大语言模型在财务推理领域的潜在应用，对任务制定、数据生成、提示方法和评估能力等方面进行了详细研究，最终在各种数据集大小上对参数规模为2.8B至13B的各种GPT变体进行基准测试。 |
| [^81] | [Online Platt Scaling with Calibeating.](http://arxiv.org/abs/2305.00070) | 本文提出了一种在线Platt缩放及其校准方法，其理论基础强大，可以处理分布漂移和对抗性结果序列，无需超参数调整，在一系列合成和真实数据集上表现出卓越的性能。 |
| [^82] | [TorchBench: Benchmarking PyTorch with High API Surface Coverage.](http://arxiv.org/abs/2304.14226) | TorchBench是一款新型基准测试套件，可全面表征PyTorch软件栈的性能，指导模型、PyTorch框架和GPU库的性能优化。 |
| [^83] | [Hint-Aug: Drawing Hints from Foundation Vision Transformers Towards Boosted Few-Shot Parameter-Efficient Tuning.](http://arxiv.org/abs/2304.12520) | 我们提出了一种名为Hint-Aug的框架，利用先前预训练的FViTs学到的高度代表性特征来增强调参数据，解决了FViTs在少样本数据的情况下的“饥饿”特性，并成功地提高了FViT训练的鲁棒性和调参表现。 |
| [^84] | [Improving Autoregressive NLP Tasks via Modular Linearized Attention.](http://arxiv.org/abs/2304.08453) | 本文提出模块化线性化注意力机制（MLA）以最大化推理质量并实现速度提升，并在多个自回归自然语言处理任务上验证了该方法，包括语音到文本神经机器翻译（S2T NMT）、语音到文本同声传译（SimulST）和自回归文本到频谱图任务，在TTS上具有高效率收益，在NMT和SimulST的训练和推理过程中表现出竞争性能。 |
| [^85] | [Emergence of Symbols in Neural Networks for Semantic Understanding and Communication.](http://arxiv.org/abs/2304.06377) | 本文介绍了一种名为SEA-net的神经网络解决方案，可以生成符号，实现语义理解和交流。这些符号可以捕捉到组成性语义信息，并呈现类似自然语言的内在结构。 |
| [^86] | [Efficiently Aligned Cross-Lingual Transfer Learning for Conversational Tasks using Prompt-Tuning.](http://arxiv.org/abs/2304.01295) | 本文提出了一个平行大规模多语种会话数据集XSGD，开发了一种有效的基于提示调整的方法来学习对齐提示，同时研究了跨语言任务的NLI-based和vanilla分类器，并在插槽填充和意图分类任务上评估了模型的跨语言泛化能力。 |
| [^87] | [From Wide to Deep: Dimension Lifting Network for Parameter-efficient Knowledge Graph Embedding.](http://arxiv.org/abs/2303.12816) | 本文提出了一个用于实现参数高效的知识图谱嵌入的深度网络，通过增加深度克服因采用低维实体表示而导致的模型精度下降和模型参数减少有限的问题。 |
| [^88] | [Matryoshka Policy Gradient for Entropy-Regularized RL: Convergence and Global Optimality.](http://arxiv.org/abs/2303.12785) | 本文介绍了一种新的策略梯度算法——莫特里卡多策略梯度(MPG)，该算法尤其在最大熵强化学习中表现突出，能够实现一系列策略的训练和学习以达到任务的最优化，具有极高的收敛性和全局最优性。 |
| [^89] | [Multi-agent Reinforcement Learning for Regional Signal control in Large-scale Grid Traffic network.](http://arxiv.org/abs/2303.11899) | 本文提出了一种新的训练框架 RegionLight，基于交叉口之间的邻接关系将智能体分配到每个区域中。同时，研究人员扩展了BDQ方法为DBDQ，以限制联合动作空间大小的增长并缓解智能体训练问题。 |
| [^90] | [Sequential Query Encoding For Complex Query Answering on Knowledge Graphs.](http://arxiv.org/abs/2302.13114) | 本文提出一种名为SQE的序列查询编码方法，将复杂查询答案编码为一个序列，从而实现快速和强大的知识图谱推理。 |
| [^91] | [K-SHAP: Policy Clustering Algorithm for Anonymous State-Action Pairs.](http://arxiv.org/abs/2302.11996) | 本文提出了一种名为K-SHAP的算法，来解决多个智能体保持匿名且仅有状态-动作对的情况下学习智能体决策的问题。 |
| [^92] | [Approximately Bayes-Optimal Pseudo Label Selection.](http://arxiv.org/abs/2302.08883) | 本文介绍了BPLS，一种用于PLS的贝叶斯框架，通过解析逼近选择标签实例的标准，以避免由过度自信但错误预测的实例选择而导致的确认偏差问题。 |
| [^93] | [A Systematic Literature Review of Human-Centered, Ethical, and Responsible AI.](http://arxiv.org/abs/2302.05284) | 本文通过对164篇主题性回顾，总结了人性化、道德化和负责任AI领域的底层映射，发现当前的HCER-AI研究重点是治理、公正和可解释性，而AIES、FAccT、CHI和CSCW等会议专注于特定主题，缺乏对隐私、安全和人类繁荣的关注。 |
| [^94] | [QR-CLIP: Introducing Explicit Open-World Knowledge for Location and Time Reasoning.](http://arxiv.org/abs/2302.00952) | 本文提出QR-CLIP模型，通过引入开放世界知识进行位置和时间推理，在此任务上取得了约10%和130%的相对提升。 |
| [^95] | [Identifying Adversarially Attackable and Robust Samples.](http://arxiv.org/abs/2301.12896) | 本文提出了一种深度学习方法，用于检测哪些样本最容易受到对抗性攻击，从而确定哪些样本最不容易受到攻击。实验结果表明，这种检测器在不同的模型结构中具有较好的可移植性和检测性能。 |
| [^96] | [Separate And Diffuse: Using a Pretrained Diffusion Model for Improving Source Separation.](http://arxiv.org/abs/2301.10752) | 该论文使用预训练的扩散模型和确定性模型的输出进行线性组合，取得了在多个基准测试中对于2、3、5、10和20个说话者的最先进结果。 |
| [^97] | [Do I have the Knowledge to Answer? Investigating Answerability of Knowledge Base Questions.](http://arxiv.org/abs/2212.10189) | 本文探究了在知识库问答中的可回答性问题，使用新的GrailQAbility基准KBQA数据集，发现现有的KBQA模型在处理无法回答问题时性能下降，并对无法回答的检测存在问题，需要进一步研究来使KBQA系统对无法回答具有鲁棒性。 |
| [^98] | [Real-Time Neural Light Field on Mobile Devices.](http://arxiv.org/abs/2212.08057) | 本文提出了一种在移动设备上实时运行的高效网络，用于神经渲染。 |
| [^99] | [Variable Decision-Frequency Option Critic.](http://arxiv.org/abs/2212.04407) | 这篇论文提出了一个名为CTCO的框架，其中代理选择选项作为可变持续时间的子策略。这个框架可以以任何所需频率与系统交互，从而提供平滑的动作变化，相比传统RL和时间抽象RL方法，其性能更好。 |
| [^100] | [Probing neural language models for understanding of words of estimative probability.](http://arxiv.org/abs/2211.03358) | 本研究探究了神经语言处理模型对于估计概率词语的理解能力，使用UNLI数据集和构建WEP数据集进行实验，发现语言模型在预测WEP的存在时很有效，但没有完全捕捉到与每个词相关联的共识概率水平。 |
| [^101] | [Heterogeneous Trajectory Forecasting via Risk and Scene Graph Learning.](http://arxiv.org/abs/2211.00848) | 本文提出了一种基于风险和场景图学习的异构道路代理轨迹预测方法，通过异构风险图和分层场景图进行交互建模，取得了优于其他方法的效果。 |
| [^102] | [GFlowOut: Dropout with Generative Flow Networks.](http://arxiv.org/abs/2210.12928) | GFlowOut是一种使用生成流网络的Dropout方法，可以更好地估计复杂的后验分布和样本相关性，并在几个基准数据集上实现了最先进的性能和良好的校准不确定性估计。 |
| [^103] | [Diffusion Models for Causal Discovery via Topological Ordering.](http://arxiv.org/abs/2210.06201) | 本文提出基于扩散模型的DiffAN拓扑排序算法，用于解决因果发现中的搜索空间优化问题。 |
| [^104] | [Training Debiased Subnetworks with Contrastive Weight Pruning.](http://arxiv.org/abs/2210.05247) | 本文探讨了在存在强假相关的偏置网络中提取最优无偏子网络的问题，并提出了使用对比剪枝权重训练实现去偏置子网络的算法 DCWP，在多个应用中都有良好的效果。 |
| [^105] | [KSAT: Knowledge-infused Self Attention Transformer -- Integrating Multiple Domain-Specific Contexts.](http://arxiv.org/abs/2210.04307) | KSAT使用外部知识源引入知识引导偏见来整合多个领域特定上下文的自我关注结构。 |
| [^106] | [Towards Out-of-Distribution Adversarial Robustness.](http://arxiv.org/abs/2210.03150) | 该论文介绍了一种面向OOD的对抗鲁棒性方法，通过将每个攻击类型视为一个领域，应用风险外推方法实现对各攻击的相似鲁棒性水平，实现了在训练和测试时的更高性能，是对抗鲁棒性研究中的创新贡献。 |
| [^107] | [RankMe: Assessing the downstream performance of pretrained self-supervised representations by their rank.](http://arxiv.org/abs/2210.02885) | 本文提出了一个简单的无监督准则 RankMe，通过评估有效排名，可以指示学习JE-SSL表示的质量，而无需任何标签。 |
| [^108] | [Taking a Respite from Representation Learning for Molecular Property Prediction.](http://arxiv.org/abs/2209.13492) | 本研究对一系列分子表征模型进行了系统评估，发现基于固定表征的模型在分子属性预测中具有一定优势，同时也揭示了活性断崖问题。 |
| [^109] | [Simplifying Model-based RL: Learning Representations, Latent-space Models, and Policies with One Objective.](http://arxiv.org/abs/2209.08466) | 提出了一个单一的、具有自洽性的目标，它共同优化了隐空间模型和策略，以实现高回报，从而简化模型为基础的强化学习方法。 |
| [^110] | [R\'{e}nyi Divergence Deep Mutual Learning.](http://arxiv.org/abs/2209.05732) | 本文提出在深度互相学习中使用R\'{e}nyi散度，它能够在不引入大量复杂度的情况下持续提高性能，获得了广泛的实证结果的支持。 |
| [^111] | [A Cognitive Study on Semantic Similarity Analysis of Large Corpora: A Transformer-based Approach.](http://arxiv.org/abs/2207.11716) | 本文通过使用Transformer在U.S Patent Phrase to Phrase Matching Dataset上进行语义相似度分析，提高了算法效率，达到了令人满意的结果。 |
| [^112] | [Language Models as Knowledge Embeddings.](http://arxiv.org/abs/2206.12617) | 该论文提出了一种使用语言模型来推导知识嵌入的方法LMKE，它旨在提高对丰富的长尾实体的表示能力并解决基于描述的先前方法的问题，实验结果表明该方法在多个基准数据集上实现了最先进的性能。 |
| [^113] | [DegreEmbed: incorporating entity embedding into logic rule learning for knowledge graph reasoning.](http://arxiv.org/abs/2112.09933) | 本文介绍了一种将实体嵌入和逻辑规则挖掘相结合的新模型DegreEmbed，用于知识图谱推理。实验结果表明，DegreEmbed在链接预测和规则提取方面优于现有方法。 |
| [^114] | [Unified Instance and Knowledge Alignment Pretraining for Aspect-based Sentiment Analysis.](http://arxiv.org/abs/2110.13398) | 本论文提出了一种统一实例和知识对齐预训练框架，能够有效解决预训练和下游ABSA数据集之间的领域偏移问题，提高了基于方面的情感分析的性能，达到了最先进水平。 |
| [^115] | [Counterfactual Samples Synthesizing and Training for Robust Visual Question Answering.](http://arxiv.org/abs/2110.01013) | 本文提出了一种新的模型无关的对抗样本合成和训练（CSST）策略，可以有效解决当前视觉问答模型的语言偏差问题，显著改善模型的性能并具备理想的可视化解释和问题敏感性。 |
| [^116] | [The Proximal ID Algorithm.](http://arxiv.org/abs/2108.06818) | 近端ID算法是一种将仪器变量和代理相结合的识别因果关系的算法，可以在利用幸运的外部辅助手段的同时，调整未观测因素，对多元系统进行非参数识别。 |
| [^117] | [The Difficulty of Novelty Detection in Open-World Physical Domains: An Application to Angry Birds.](http://arxiv.org/abs/2106.08670) | 本文提出了一种定性物理方法来量化开放世界物理领域中新颖性检测的难度，并在愤怒的小鸟游戏中进行了验证，结果显示我们计算的检测困难度与人类用户相符。 |
| [^118] | [Benchmarking Robustness of Deep Learning Classifiers Using Two-Factor Perturbation.](http://arxiv.org/abs/2103.03102) | 本文提出了一种新的基准测试方法和工具，通过双因素扰动来评估深度学习分类器的鲁棒性。使用该方法和工具，作者比较了不同的两因素扰动条件下DL分类器的鲁棒性，并为开发更鲁棒的DL分类器提供了见解。 |
| [^119] | [Representation Ensembling for Synergistic Lifelong Learning with Quasilinear Complexity.](http://arxiv.org/abs/2004.12908) | 本文提出了一种名为RELL的方法，利用知识蒸馏和知识保持正则化方法，以协同集成在不同任务上独立学习的表示，在准线性复杂度下实现了前向和后向传递。实验结果表明，在各种基准数据集上，RELL的表现优于现有的最先进方法，尤其是在存在灾难性遗忘的情况下，能够显着改善反向传递。 |

# 详细

[^1]: 朝着可信的解释：因果关系解释论文研究

    Towards Trustworthy Explanation: On Causal Rationalization. (arXiv:2306.14115v1 [cs.LG])

    [http://arxiv.org/abs/2306.14115](http://arxiv.org/abs/2306.14115)

    该论文介绍了一种新的因果关系解释方法，通过在解释中引入非虚假性和效率，从因果推断的角度定义了因果概率，从而建立了必要和充分解释的主要组成部分，相比现有的基于关联的解释方法，这种方法有更加优越的性能表现。

    

    随着自然语言处理的最新进展，解释成为了通过选择输入文本的子集来解释黑盒模型中主要变化的一个基本的自我解释图。然而，现有的基于关联的解释方法在两个或多个片段高度互相关联时无法识别真正的解释，因此对预测准确性提供类似的贡献，所谓的虚假性。为了解决这一限制，我们从因果推断的角度新颖地将两个因果期望值（非虚假性和效率）引入了解释中。我们根据一种新提出的解释结构因果模型定义了一系列的因果概率，通过其理论鉴定，建立了必要和充分解释的主要组成部分。我们在真实世界的评论和医疗数据集上证明了所提出的因果关系解释的优越性能。

    With recent advances in natural language processing, rationalization becomes an essential self-explaining diagram to disentangle the black box by selecting a subset of input texts to account for the major variation in prediction. Yet, existing association-based approaches on rationalization cannot identify true rationales when two or more snippets are highly inter-correlated and thus provide a similar contribution to prediction accuracy, so-called spuriousness. To address this limitation, we novelly leverage two causal desiderata, non-spuriousness and efficiency, into rationalization from the causal inference perspective. We formally define a series of probabilities of causation based on a newly proposed structural causal model of rationalization, with its theoretical identification established as the main component of learning necessary and sufficient rationales. The superior performance of the proposed causal rationalization is demonstrated on real-world review and medical datasets w
    
[^2]: TNPAR: 基于拓扑神经泊松自回归模型的事件序列Granger因果结构学习

    TNPAR: Topological Neural Poisson Auto-Regressive Model for Learning Granger Causal Structure from Event Sequences. (arXiv:2306.14114v1 [cs.LG])

    [http://arxiv.org/abs/2306.14114](http://arxiv.org/abs/2306.14114)

    该论文提出了一种基于拓扑神经泊松自回归模型的方法，同时考虑先验拓扑网络和潜在的Granger因果结构来学习事件序列中的Granger因果关系，该方法通过推断因果关系来解决序列数据中的非 i.i.d. 问题

    

    从事件序列中学习Granger因果关系是各种应用中具有挑战性但又至关重要的任务。大多数现有方法都依赖于事件序列独立同分布 (i.i.d.) 的假设。然而，由于事件序列之间的固有依赖关系，这一 i.i.d. 假设经常被违反。幸运的是，在实践中，我们发现这些依赖关系可以被建模成一个拓扑网络，因此可以通过将先验拓扑网络引入Granger因果发现来解决非 i.i.d. 问题。这一发现促使我们解决两个问题：1) 如何在模型事件序列时同时考虑先验拓扑网络和潜在的Granger因果结构；2) 如何学习Granger因果结构。为此，我们设计了一个两阶段的统一拓扑神经泊松自回归模型。在生成阶段，我们采用神经泊松过程的一种变体来建模事件发生的时刻，并通过拓扑关系和现有事件序列推断因果关系。

    Learning Granger causality from event sequences is a challenging but essential task across various applications. Most existing methods rely on the assumption that event sequences are independent and identically distributed (i.i.d.). However, this i.i.d. assumption is often violated due to the inherent dependencies among the event sequences. Fortunately, in practice, we find these dependencies can be modeled by a topological network, suggesting a potential solution to the non-i.i.d. problem by introducing the prior topological network into Granger causal discovery. This observation prompts us to tackle two ensuing challenges: 1) how to model the event sequences while incorporating both the prior topological network and the latent Granger causal structure, and 2) how to learn the Granger causal structure. To this end, we devise a two-stage unified topological neural Poisson auto-regressive model. During the generation stage, we employ a variant of the neural Poisson process to model the 
    
[^3]: RLHF是否比标准RL更困难？

    Is RLHF More Difficult than Standard RL?. (arXiv:2306.14111v1 [cs.LG])

    [http://arxiv.org/abs/2306.14111](http://arxiv.org/abs/2306.14111)

    本文证明了对于广泛的偏好模型，我们可以使用现有的算法和技术直接解决基于偏好的RL问题，而几乎不需要额外的成本。

    

    从人类反馈学习的强化学习（RLHF）是从偏好信号学习，而标准强化学习（RL）则直接从奖励信号学习。偏好信号可能包含的信息比奖励信号少，这使得基于偏好的RL似乎更加困难。本文理论上证明，对于广泛的偏好模型，我们可以使用现有的算法和技术直接解决基于偏好的RL问题，而几乎不需要额外的成本。具体而言，我们将问题分为两类：（1）基于奖励概率模型的偏好，此时可以将问题简化为容忍奖励小误差的鲁棒奖励RL问题；（2）对于一般的任意偏好且目标是找到von Neumann获胜者的情况，我们将问题简化为多智能体奖励RL问题，该问题可以在一组受限制的策略下找到马尔可夫博弈的因子纳什平衡解。后一种情况可以进一步降低成对关系的MDP。

    Reinforcement learning from Human Feedback (RLHF) learns from preference signals, while standard Reinforcement Learning (RL) directly learns from reward signals. Preferences arguably contain less information than rewards, which makes preference-based RL seemingly more difficult. This paper theoretically proves that, for a wide range of preference models, we can solve preference-based RL directly using existing algorithms and techniques for reward-based RL, with small or no extra costs. Specifically, (1) for preferences that are drawn from reward-based probabilistic models, we reduce the problem to robust reward-based RL that can tolerate small errors in rewards; (2) for general arbitrary preferences where the objective is to find the von Neumann winner, we reduce the problem to multiagent reward-based RL which finds Nash equilibria for factored Markov games under a restricted set of policies. The latter case can be further reduce to adversarial MDP when preferences only depend on the f
    
[^4]: 半监督物体检测：最新研究和进展综述

    Semi-supervised Object Detection: A Survey on Recent Research and Progress. (arXiv:2306.14106v1 [cs.CV])

    [http://arxiv.org/abs/2306.14106](http://arxiv.org/abs/2306.14106)

    本文综述并介绍了半监督物体检测 (SSOD) 的最新研究进展，主要包括数据增强方法，伪标签，一致性正则化，图形和迁移学习等半监督策略，并讨论了未来的研究方向和常用的评估指标。

    

    近年来，深度学习技术已成熟应用于物体检测领域，大多数算法趋向于监督学习。然而，大量标记数据需要高昂的人力成本，导致低效率和限制。半监督物体检测 (SSOD) 因其高研究价值和实用性而越来越受到关注。它旨在通过使用少量标记数据和大量未标记数据学习信息。本文从五个方面全面而最新地介绍了 SSOD 方法。我们首先简要介绍了几种数据增强的方法。然后，我们将主流的半监督策略分为伪标签，一致性正则化，基于图形和基于迁移学习的方法，并介绍了一些在有挑战的情况下的方法。我们进一步介绍了广泛使用的损失函数，然后概述了常见的基准数据集和评估指标。最后，我们回顾了最近在 SSOD 上的进展并讨论了未来的潜在方向。

    In recent years, deep learning technology has been maturely applied in the field of object detection, and most algorithms tend to be supervised learning. However, a large amount of labeled data requires high costs of human resources, which brings about low efficiency and limitations. Semi-supervised object detection (SSOD) has been paid more and more attentions due to its high research value and practicability. It is designed to learn information by using small amounts of labeled data and large amounts of unlabeled data. In this paper, we present a comprehensive and up-to-date survey on the SSOD approaches from five aspects. We first briefly introduce several ways of data augmentation. Then, we dive the mainstream semi-supervised strategies into pseudo labels, consistent regularization, graph based and transfer learning based methods, and introduce some methods in challenging settings. We further present widely-used loss functions, and then we outline the common benchmark datasets and 
    
[^5]: 语言模型是弱学习器

    Language models are weak learners. (arXiv:2306.14101v1 [cs.LG])

    [http://arxiv.org/abs/2306.14101](http://arxiv.org/abs/2306.14101)

    本文证明了，基于提示的大型语言模型可以作为弱学习器应用于表格数据的boosting算法中，并且在某些情况下可以优于传统的树形模型。

    

    实践和理论机器学习中的一个中心概念是弱学习器，即在任何给定的数据分布上都能取得比随机更好的性能的分类器，即使只是略微好一点。这样的弱学习器构成了经典机器学习方法（如boosting）的实用基础。在这项工作中，我们展示了基于提示的大型语言模型可以有效地作为上述弱学习器进行操作。具体而言，我们演示了在表格数据中使用大型语言模型（LLM）作为boosting算法中的弱学习器。我们展示了通过提供（根据感兴趣的分布进行适当采样的）表格数据样本的文本描述，LLM可以产生样本的汇总，作为分类的模板，并在这个任务中实现作为弱学习器的目的。我们将这些模型纳入boosting方法中，在某些环境中，可以利用LLM中的知识，优于传统的树形模型。

    A central notion in practical and theoretical machine learning is that of a $\textit{weak learner}$, classifiers that achieve better-than-random performance (on any given distribution over data), even by a small margin. Such weak learners form the practical basis for canonical machine learning methods such as boosting. In this work, we illustrate that prompt-based large language models can operate effectively as said weak learners. Specifically, we illustrate the use of a large language model (LLM) as a weak learner in a boosting algorithm applied to tabular data. We show that by providing (properly sampled according to the distribution of interest) text descriptions of tabular data samples, LLMs can produce a summary of the samples that serves as a template for classification and achieves the aim of acting as a weak learner on this task. We incorporate these models into a boosting approach, which in some settings can leverage the knowledge within the LLM to outperform traditional tree
    
[^6]: 基于大语言模型的中文细粒度金融情感分析

    Chinese Fine-Grained Financial Sentiment Analysis with Large Language Models. (arXiv:2306.14096v1 [cs.CL])

    [http://arxiv.org/abs/2306.14096](http://arxiv.org/abs/2306.14096)

    本文提出了一个用于企业预警的新型、广泛的中文细粒度金融情感分析数据集FinChina SA，并使用现有开源大语言模型对其进行评估和实验。该数据集将成为推进真实金融情感分析任务探索的宝贵资源。

    

    金融领域实体级别的细粒度情感分析是情感分析的重要子任务，目前面临着众多挑战。其中主要挑战之一来自于缺乏专门设计用于金融文本情感分析的高质量大规模标注语料库，这限制了开发有效文本处理技术所需的数据的可用性。大语言模型（LLMs）的最新进展在自然语言处理任务中取得了显著的性能，主要集中在语言模式匹配方面。在本文中，我们提出了一个新颖的、广泛的中文细粒度金融情感分析数据集FinChina SA，用于企业预警。我们对流行的现有开源LLMs使用我们的数据集进行了全面的评估和实验。我们坚信，我们的数据集将成为推动真实世界金融情感分析任务探索的宝贵资源。

    Entity-level fine-grained sentiment analysis in the financial domain is a crucial subtask of sentiment analysis and currently faces numerous challenges. The primary challenge stems from the lack of high-quality and large-scale annotated corpora specifically designed for financial text sentiment analysis, which in turn limits the availability of data necessary for developing effective text processing techniques. Recent advancements in large language models (LLMs) have yielded remarkable performance in natural language processing tasks, primarily centered around language pattern matching. In this paper, we propose a novel and extensive Chinese fine-grained financial sentiment analysis dataset, FinChina SA, for enterprise early warning. We thoroughly evaluate and experiment with well-known existing open-source LLMs using our dataset. We firmly believe that our dataset will serve as a valuable resource to advance the exploration of real-world financial sentiment analysis tasks, which shoul
    
[^7]: 利用梯度对抗不确定性：通过扩散分数匹配实现离线强化学习

    Fighting Uncertainty with Gradients: Offline Reinforcement Learning via Diffusion Score Matching. (arXiv:2306.14079v1 [cs.LG])

    [http://arxiv.org/abs/2306.14079](http://arxiv.org/abs/2306.14079)

    本文提出了平滑的距离数据度量标准，并将其与离线强化学习相结合，以对抗不确定性和分布偏移的挑战。该方法不仅在最小化梯度不确定性时稳定收敛到数据，而且不易低估真实不确定性，是一种有前途的策略搜索方法。

    

    离线优化范式，例如离线强化学习（RL）或模仿学习（IL），允许策略搜索算法利用离线数据，但需要仔细处理不确定性以避免分布偏移的挑战。由于其在高维度中的有效性，基于梯度的策略搜索方法是一种有前途的方向；然而，我们需要更仔细地考虑这些方法如何与不确定性估计相互影响。我们声称，为了让不确定性度量适用于基于梯度的优化，它必须在最小化梯度不确定性时稳定地收敛到数据，并且不易低估真实不确定性。我们研究了平滑的数据距离作为度量标准，并展示了它不仅稳定地收敛到数据，而且还允许我们通过Lipschitz常数来分析模型偏差。此外，我们建立了平滑的数据距离和数据似然之间的等价性。

    Offline optimization paradigms such as offline Reinforcement Learning (RL) or Imitation Learning (IL) allow policy search algorithms to make use of offline data, but require careful incorporation of uncertainty in order to circumvent the challenges of distribution shift. Gradient-based policy search methods are a promising direction due to their effectiveness in high dimensions; however, we require a more careful consideration of how these methods interplay with uncertainty estimation. We claim that in order for an uncertainty metric to be amenable for gradient-based optimization, it must be (i) stably convergent to data when uncertainty is minimized with gradients, and (ii) not prone to underestimation of true uncertainty. We investigate smoothed distance to data as a metric, and show that it not only stably converges to data, but also allows us to analyze model bias with Lipschitz constants. Moreover, we establish an equivalence between smoothed distance to data and data likelihood, 
    
[^8]: 带And-Or Recursors和Refiner Oracles的目标驱动LLM对话线程的全自动化

    Full Automation of Goal-driven LLM Dialog Threads with And-Or Recursors and Refiner Oracles. (arXiv:2306.14077v1 [cs.AI])

    [http://arxiv.org/abs/2306.14077](http://arxiv.org/abs/2306.14077)

    本文提出一种全自动化的LLM对话线程算法，通过递归的探索和展开，实现对任务的深度逐步推理，并应用于推论预测、因果解释、建议等方案的实现。

    

    我们通过递归地探索备选项(OR节点)和展开细节(AND节点)来自动化LLM对话线程中的深度逐步推理，以达到给定深度。我们从一个简洁的任务特定启动器开始，通过合成一个总结迄今为止深度优先步骤的提示，来引导自动对话线程专注于任务。我们的算法源自一个Horn Clause解释器的简单递归下降实现，但我们将逻辑引擎调整得适应LLMs训练过程中所使用的自然语言推理模式。语义相似性与基本事实或来自另一个LLM实例的oracle建议用于限制搜索空间并验证作为答案返回的结果的合理化步骤。最后，生成的Horn Clause程序的唯一最小模型收集推理过程的结果。作为应用，我们概述了推论预测、因果解释、建议等方案的实现。

    We automate deep step-by step reasoning in an LLM dialog thread by recursively exploring alternatives (OR-nodes) and expanding details (AND-nodes) up to a given depth. Starting from a single succinct task-specific initiator we steer the automated dialog thread to stay focussed on the task by synthesizing a prompt that summarizes the depth-first steps taken so far.  Our algorithm is derived from a simple recursive descent implementation of a Horn Clause interpreter, except that we accommodate our logic engine to fit the natural language reasoning patterns LLMs have been trained on. Semantic similarity to ground-truth facts or oracle advice from another LLM instance is used to restrict the search space and validate the traces of justification steps returned as answers. At the end, the unique minimal model of a generated Horn Clause program collects the results of the reasoning process.  As applications, we sketch implementations of consequence predictions, causal explanations, recommenda
    
[^9]: 无强度卷积时空点过程: 融合局部与全局事件语境

    Intensity-free Convolutional Temporal Point Process: Incorporating Local and Global Event Contexts. (arXiv:2306.14072v1 [cs.LG])

    [http://arxiv.org/abs/2306.14072](http://arxiv.org/abs/2306.14072)

    本文提出了一种TPP建模方法，将连续时间卷积事件编码器与RNN集成，以融合局部和全局上下文。实验结果表明，该模型具有较好的性能表现。

    

    连续时间领域的事件预测是一项至关重要但相当困难的任务。时间点过程(TPP)学习模型在这个领域中表现出了巨大的优势。现有的模型主要集中于使用像循环神经网络(RNN)或自我注意机制之类的技术来编码事件的全局上下文。但是，局部事件上下文对事件的发生也起着重要作用，但这方面却很少被关注。流行的卷积神经网络专为捕获局部上下文而设计，但由于无法在连续时间模型化，因此从未应用于TPP建模。本研究提出了一种将局部和全局上下文相结合的新型TPP建模方法，即将连续时间卷积事件编码器与RNN集成。所提出的框架具有灵活性和可扩展性，可以处理具有长序列和复杂潜在模式的大型数据集。实验结果表明，所提出的模型改善了性能。

    Event prediction in the continuous-time domain is a crucial but rather difficult task. Temporal point process (TPP) learning models have shown great advantages in this area. Existing models mainly focus on encoding global contexts of events using techniques like recurrent neural networks (RNNs) or self-attention mechanisms. However, local event contexts also play an important role in the occurrences of events, which has been largely ignored. Popular convolutional neural networks, which are designated for local context capturing, have never been applied to TPP modelling due to their incapability of modelling in continuous time. In this work, we propose a novel TPP modelling approach that combines local and global contexts by integrating a continuous-time convolutional event encoder with an RNN. The presented framework is flexible and scalable to handle large datasets with long sequences and complex latent patterns. The experimental result shows that the proposed model improves the perfo
    
[^10]: 自适应采集数据的离线强化学习策略评估

    Offline Policy Evaluation for Reinforcement Learning with Adaptively Collected Data. (arXiv:2306.14063v1 [cs.LG])

    [http://arxiv.org/abs/2306.14063](http://arxiv.org/abs/2306.14063)

    本论文提出了一种自适应采集数据的离线强化学习策略评估方法，为表格MDPs推导出高概率、实例相关的误差边界，并实现了自适应设置下的极小值最优离线学习。

    

    发展离线RL方法样本复杂度的理论保证是实现数据需求量较大的RL算法实际可行的重要步骤。目前，大多数结果依赖于关于数据分布的不现实的假设，即包括一个由单一记录策略收集的i.i.d.轨迹集。我们考虑一个更一般的设置，即数据集可以是自适应收集的。我们为表格MDPs中的TMIS离线策略评估（OPE）估计器在这个广义设置中开发理论，推导其估计误差的高概率、实例相关边界。我们还回收了自适应设置下的极小值最优离线学习。最后，我们进行模拟，以经验分析这些估计器在自适应和非自适应模式下的行为。

    Developing theoretical guarantees on the sample complexity of offline RL methods is an important step towards making data-hungry RL algorithms practically viable. Currently, most results hinge on unrealistic assumptions about the data distribution -- namely that it comprises a set of i.i.d. trajectories collected by a single logging policy. We consider a more general setting where the dataset may have been gathered adaptively. We develop theory for the TMIS Offline Policy Evaluation (OPE) estimator in this generalized setting for tabular MDPs, deriving high-probability, instance-dependent bounds on its estimation error. We also recover minimax-optimal offline learning in the adaptive setting. Finally, we conduct simulations to empirically analyze the behavior of these estimators under adaptive and non-adaptive regimes.
    
[^11]: 大型语言模型在解释模糊的网络攻击描述中的应用研究

    On the Uses of Large Language Models to Interpret Ambiguous Cyberattack Descriptions. (arXiv:2306.14062v1 [cs.AI])

    [http://arxiv.org/abs/2306.14062](http://arxiv.org/abs/2306.14062)

    本文研究了大型语言模型在解释模糊的网络攻击描述中的应用。实验表明，以相关文本数据训练的BaseLLM可以大大改善攻击技术的解释，并且超越了领域专家的解释能力。该研究还讨论了使用LLMs进行网络威胁情报分析的影响和局限性。

    

    漏洞和攻击的数量、种类和速度的变化使得人类专家和经验在事件威胁分析中变得困难。MITRE AT＆CK框架使用战术、技术和程序（TTP）描述攻击者如何和为何利用漏洞。但是，一个安全专业人士撰写的TTP描述可能被另一个人解释得非常不同，这会导致网络安全操作甚至商业、政策和法律决策的混淆。与此同时，人工智能的进步已经导致自然语言处理（NLP）算法在网络操作中的各种任务中的增加使用。随着大型语言模型（LLM）的兴起，由于LLM的语义理解和可扩展性，NLP任务得到了显着的改善。这让我们质疑LLM在如何解释TTP或一般网络攻击描述方面的表现。我们提出并分析了直接使用LLMs以及训练BaseLLMs以描述TTP并评估结果。我们的实验证明了以相关文本数据训练的BaseLLM大大改善了TTP的解释，并可以胜过领域专家的解释能力。我们还讨论了使用LLMs进行网络威胁情报分析的影响和局限性。

    The volume, variety, and velocity of change in vulnerabilities and exploits have made incident threat analysis challenging with human expertise and experience along. The MITRE AT&CK framework employs Tactics, Techniques, and Procedures (TTPs) to describe how and why attackers exploit vulnerabilities. However, a TTP description written by one security professional can be interpreted very differently by another, leading to confusion in cybersecurity operations or even business, policy, and legal decisions. Meanwhile, advancements in AI have led to the increasing use of Natural Language Processing (NLP) algorithms to assist the various tasks in cyber operations. With the rise of Large Language Models (LLMs), NLP tasks have significantly improved because of the LLM's semantic understanding and scalability. This leads us to question how well LLMs can interpret TTP or general cyberattack descriptions. We propose and analyze the direct use of LLMs as well as training BaseLLMs with ATT&CK desc
    
[^12]: 将四足机器人转化为视障人士导航机器人：形式化导航、交互建模和安全机制

    Transforming a Quadruped into a Guide Robot for the Visually Impaired: Formalizing Wayfinding, Interaction Modeling, and Safety Mechanism. (arXiv:2306.14055v1 [cs.RO])

    [http://arxiv.org/abs/2306.14055](http://arxiv.org/abs/2306.14055)

    本论文探讨了将四足机器人转化为导航机器人的三个关键主题，并介绍了一种名为“延迟挽具”的交互模型以及一种行动屏蔽机制来提高用户的安全性。

    

    本论文探讨了将四足机器人转化为服务于视障人士的导航机器人的原则。导航机器人有很大的潜力解决仅有 2% 到 3% 的潜在盲或视力障碍用户可以使用的导盲动物的有限状况。为了建造一台成功的导航机器人，我们探讨了三个关键主题：(1) 形式化导盲犬和人的导航机制，(2) 开发一种基于数据的模型来描述他们的交互，(3) 提高用户的安全性。

    This paper explores the principles for transforming a quadrupedal robot into a guide robot for individuals with visual impairments. A guide robot has great potential to resolve the limited availability of guide animals that are accessible to only two to three percent of the potential blind or visually impaired (BVI) users. To build a successful guide robot, our paper explores three key topics: (1) formalizing the navigation mechanism of a guide dog and a human, (2) developing a data-driven model of their interaction, and (3) improving user safety. First, we formalize the wayfinding task of the human-guide robot team using Markov Decision Processes based on the literature and interviews. Then we collect real human-robot interaction data from three visually impaired and six sighted people and develop an interaction model called the ``Delayed Harness'' to effectively simulate the navigation behaviors of the team. Additionally, we introduce an action shielding mechanism to enhance user saf
    
[^13]: 面向需求响应的最优定价--一种非参数约束策略优化方法

    Towards Optimal Pricing of Demand Response -- A Nonparametric Constrained Policy Optimization Approach. (arXiv:2306.14047v1 [cs.LG])

    [http://arxiv.org/abs/2306.14047](http://arxiv.org/abs/2306.14047)

    本研究提出了一种非参数约束策略优化方法，通过消除对策略表示的限制性假设，提高优化度并确保策略更新的稳定性，从而实现电力负荷高峰和低峰之间的合理转移。

    

    需求响应（DR）已被证明是降低电力市场供需两侧不确定性和峰值负荷的有效方法。然而，对于DR研究而言，一个关键问题是如何适当地调整电力价格，以将电力负荷从高峰转移到低峰时段。本文提出了一种创新的非参数约束策略优化方法，旨在提高优化度并确保策略更新的稳定性。

    Demand response (DR) has been demonstrated to be an effective method for reducing peak load and mitigating uncertainties on both the supply and demand sides of the electricity market. One critical question for DR research is how to appropriately adjust electricity prices in order to shift electrical load from peak to off-peak hours. In recent years, reinforcement learning (RL) has been used to address the price-based DR problem because it is a model-free technique that does not necessitate the identification of models for end-use customers. However, the majority of RL methods cannot guarantee the stability and optimality of the learned pricing policy, which is undesirable in safety-critical power systems and may result in high customer bills. In this paper, we propose an innovative nonparametric constrained policy optimization approach that improves optimality while ensuring stability of the policy update, by removing the restrictive assumption on policy representation that the majorit
    
[^14]: 机器学习需要自己的随机标准：随机平滑和基于PRNG的攻击。

    Machine Learning needs its own Randomness Standard: Randomised Smoothing and PRNG-based attacks. (arXiv:2306.14043v1 [cs.LG])

    [http://arxiv.org/abs/2306.14043](http://arxiv.org/abs/2306.14043)

    本文研究了机器学习中随机性可被攻击者利用的问题，主要关注流行的随机平滑方法。该方法用于训练可证明鲁棒性的模型和量化不确定性，但其随机性可能导致系统被攻击。

    

    随机性支持机器学习中的许多关键功能，包括优化、数据选择、隐私和安全。机器学习系统将生成或收集随机性的任务外包给了编译器、云服务提供商或工具链中的其他地方。但是，攻击者利用不良随机性甚至创建随机性的历史悠久，就像NSA放置后门在随机数生成器中以破解加密一样。本文考虑是否能够仅利用攻击者通常依赖的随机性来危害机器学习系统。我们将重点放在随机平滑上，这是一种流行的方法，用于训练可证明鲁棒性的模型，并为任意模型的特定输入数据点提供认证。我们选择随机平滑是因为它用于安全和安全（用于对抗对抗性示例和量化不确定性，分别）。在幕后，它依赖于采样高斯噪声来探索围绕数据点的体积。

    Randomness supports many critical functions in the field of machine learning (ML) including optimisation, data selection, privacy, and security. ML systems outsource the task of generating or harvesting randomness to the compiler, the cloud service provider or elsewhere in the toolchain. Yet there is a long history of attackers exploiting poor randomness, or even creating it -- as when the NSA put backdoors in random number generators to break cryptography. In this paper we consider whether attackers can compromise an ML system using only the randomness on which they commonly rely. We focus our effort on Randomised Smoothing, a popular approach to train certifiably robust models, and to certify specific input datapoints of an arbitrary model. We choose Randomised Smoothing since it is used for both security and safety -- to counteract adversarial examples and quantify uncertainty respectively. Under the hood, it relies on sampling Gaussian noise to explore the volume around a data poin
    
[^15]: 基于划分指导的k-means算法：极端模型压缩下的极端空簇解决方法

    Partitioning-Guided K-Means: Extreme Empty Cluster Resolution for Extreme Model Compression. (arXiv:2306.14031v1 [cs.LG])

    [http://arxiv.org/abs/2306.14031](http://arxiv.org/abs/2306.14031)

    本研究提出了一种名为PG k-means的算法，基于划分来解决空簇，提高了iPQ与量化噪声的精确性。

    

    在深度学习中，紧凑性对于模型在低资源应用中的可用性至关重要，极端模型压缩的常见方法是量化。本文考虑迭代乘积量化（iPQ）与量化噪声是这一领域中的最新技术，但是这种量化框架由于存在空簇而导致推理质量下降，我们提出了几种新的增强方法，旨在通过解决空簇来提高iPQ与量化噪声的精确性。我们的贡献被称为基于划分指导的k-means算法（PG k-means），是由三个主要组成部分组成的高度增强的k-means实现。首先，我们提出了一种基于划分的预分配策略，确保没有初始空簇并鼓励均匀的权重分布。其次，我们提出了一种经验优越的空簇解决启发式算法，通过谨慎地分割大簇来执行。最后，我们限制了PG k-means的迭代总次数。

    Compactness in deep learning can be critical to a model's viability in low-resource applications, and a common approach to extreme model compression is quantization. We consider Iterative Product Quantization (iPQ) with Quant-Noise to be state-of-the-art in this area, but this quantization framework suffers from preventable inference quality degradation due to prevalent empty clusters. In this paper, we propose several novel enhancements aiming to improve the accuracy of iPQ with Quant-Noise by focusing on resolving empty clusters. Our contribution, which we call Partitioning-Guided k-means (PG k-means), is a heavily augmented k-means implementation composed of three main components. First, we propose a partitioning-based pre-assignment strategy that ensures no initial empty clusters and encourages an even weight-to-cluster distribution. Second, we propose an empirically superior empty cluster resolution heuristic executed via cautious partitioning of large clusters. Finally, we constr
    
[^16]: 基于LLM的硬件断言生成辅助

    LLM-assisted Generation of Hardware Assertions. (arXiv:2306.14027v1 [cs.CR])

    [http://arxiv.org/abs/2306.14027](http://arxiv.org/abs/2306.14027)

    本论文研究使用LLMs来生成硬件的安全断言。通过使用自然语言提示生成SystemVerilog断言来替代编写具有挑战的安全断言。

    

    计算机系统的安全性通常依赖于硬件的安全性。硬件漏洞对系统有严重影响，因此需要技术支持安全验证活动。断言验证是一种流行的验证技术，它涉及在一组断言中捕捉设计意图，这些断言可用于形式验证或基于测试的检查。然而，编写以安全为中心的断言是一项具有挑战性的任务。在本研究中，我们探讨使用新型大型语言模型（LLMs）进行硬件断言生成的代码生成技术，其中主要使用自然语言提示（例如在断言文件中看到的代码注释）生成SystemVerilog断言。我们关注一种流行的LLM，并对其在给定不同详细级别的提示的情况下编写断言的能力进行了表征。我们设计了一个评估框架，生成各种LLM辅助下产生的系统断言形式进行评估。

    The security of computer systems typically relies on a hardware root of trust. As vulnerabilities in hardware can have severe implications on a system, there is a need for techniques to support security verification activities. Assertion-based verification is a popular verification technique that involves capturing design intent in a set of assertions that can be used in formal verification or testing-based checking. However, writing security-centric assertions is a challenging task. In this work, we investigate the use of emerging large language models (LLMs) for code generation in hardware assertion generation for security, where primarily natural language prompts, such as those one would see as code comments in assertion files, are used to produce SystemVerilog assertions. We focus our attention on a popular LLM and characterize its ability to write assertions out of the box, given varying levels of detail in the prompt. We design an evaluation framework that generates a variety of 
    
[^17]: 基于聚类和图深度学习的 COVID-19 药物再利用框架

    A clustering and graph deep learning-based framework for COVID-19 drug repurposing. (arXiv:2306.13995v1 [cs.AI])

    [http://arxiv.org/abs/2306.13995](http://arxiv.org/abs/2306.13995)

    本文提出了一种基于聚类和图深度学习的方法，在 COVID-19 药物再利用中发现了具有见地的药物相互作用，并确定了潜在有效的药物再利用候选物。

    

    药物再利用 (或重新定位) 是寻找已经获得药物监管机构 (例如美国食品药品监督管理局和治疗商品管理局) 批准用于其他疾病的药物的新治疗用途的过程。这包括分析不同生物实体之间的相互作用，例如药物靶点 (基因/蛋白质和生物通路) 和药物属性，以发现新的药物靶点或药物-疾病关系。机器学习和深度学习等人工智能方法已经成功地分析了生物医学领域中复杂的异质数据，并且已被用于药物再利用。本研究提出了一种新的无监督机器学习框架，其利用基于图的自编码器在异质药物数据上进行多特征类型聚类。数据集包括 438 种药物，其中 224 种正在接受 COVID-19 临床试验 (类别 A)。其余药物经过系统过滤以确保再利用潜在药物候选物的安全性 (类别 B)。所提出的框架在 COVID-19 特定药物再利用任务上进行评估，结果表明它能够揭示有见地的药物相互作用并确定潜在有效的药物再利用候选物。

    Drug repurposing (or repositioning) is the process of finding new therapeutic uses for drugs already approved by drug regulatory authorities (e.g., the Food and Drug Administration (FDA) and Therapeutic Goods Administration (TGA)) for other diseases. This involves analyzing the interactions between different biological entities, such as drug targets (genes/proteins and biological pathways) and drug properties, to discover novel drug-target or drug-disease relations. Artificial intelligence methods such as machine learning and deep learning have successfully analyzed complex heterogeneous data in the biomedical domain and have also been used for drug repurposing. This study presents a novel unsupervised machine learning framework that utilizes a graph-based autoencoder for multi-feature type clustering on heterogeneous drug data. The dataset consists of 438 drugs, of which 224 are under clinical trials for COVID-19 (category A). The rest are systematically filtered to ensure the safety 
    
[^18]: 使用数据自适应能量距离的高维数据稳健分类

    Robust Classification of High-Dimensional Data using Data-Adaptive Energy Distance. (arXiv:2306.13985v1 [stat.ML])

    [http://arxiv.org/abs/2306.13985](http://arxiv.org/abs/2306.13985)

    该论文提出了一种用于高维低样本量数据分类的稳健的数据自适应能量距离分类器，该分类器无需调参且在一定条件下可以实现完美分类，已在模拟研究和实际数据分析中得到证明比其他方法表现更优。

    

    在真实世界中，高维低样本量（HDLSS）数据的分类面临挑战，例如基因表达研究、癌症研究和医学成像等领域。本文提出了一些专门为HDLSS数据设计的分类器的开发和分析。这些分类器没有调节参数，并且是稳健的，因为它们不受底层数据分布的任何矩条件的影响。研究表明，在一些相当普遍的条件下，它们在HDLSS渐近区域内可以实现完美分类。还比较了所提出分类器的性能。我们的理论结果得到了广泛的模拟研究和实际数据分析的支持，证明了所提出分类技术优于几种广泛认可的方法的有希望优势。

    Classification of high-dimensional low sample size (HDLSS) data poses a challenge in a variety of real-world situations, such as gene expression studies, cancer research, and medical imaging. This article presents the development and analysis of some classifiers that are specifically designed for HDLSS data. These classifiers are free of tuning parameters and are robust, in the sense that they are devoid of any moment conditions of the underlying data distributions. It is shown that they yield perfect classification in the HDLSS asymptotic regime, under some fairly general conditions. The comparative performance of the proposed classifiers is also investigated. Our theoretical results are supported by extensive simulation studies and real data analysis, which demonstrate promising advantages of the proposed classification techniques over several widely recognized methods.
    
[^19]: 利用超复空间上融合多模态信号进行极端抽象文本摘要

    Fusing Multimodal Signals on Hyper-complex Space for Extreme Abstractive Text Summarization (TL;DR) of Scientific Contents. (arXiv:2306.13968v1 [cs.CL])

    [http://arxiv.org/abs/2306.13968](http://arxiv.org/abs/2306.13968)

    本文介绍了mTLDR数据集，这是一个多模态数据集，利用超复空间上融合多模态信号进行极端抽象文本摘要的任务。mTLDRgen模型成功实现了TL;DR生成的多模态系统。

    

    在已有注释摘要和丰富数据的基础上，科学文本摘要领域已经取得了显著的进展。然而，多个输入模态（例如视频和音频）的利用尚未得到彻底探索。目前，科学多模态输入的文本摘要系统往往采用较长的目标摘要，如摘要，在文本摘要任务中表现不佳。在本文中，我们通过利用多个输入模态来处理极端抽象文本摘要（即TL;DR生成）的新任务。为此，我们介绍了mTLDR，这是一个首创的数据集，包括视频、音频和文本，以及作者撰写的摘要和专家注释的摘要。mTLDR数据集搜集了来自不同学术会议记录总共4182个实例，如ICLR、ACL和CVPR。随后，我们提出了mTLDRgen，这是一种适用于超复杂空间上极端抽象摘要的编码解码深度神经网络架构。我们在mTLDR数据集上获得了显著的结果，并报告了首个成功实施TL;DR生成的多模态系统的结果。

    The realm of scientific text summarization has experienced remarkable progress due to the availability of annotated brief summaries and ample data. However, the utilization of multiple input modalities, such as videos and audio, has yet to be thoroughly explored. At present, scientific multimodal-input-based text summarization systems tend to employ longer target summaries like abstracts, leading to an underwhelming performance in the task of text summarization.  In this paper, we deal with a novel task of extreme abstractive text summarization (aka TL;DR generation) by leveraging multiple input modalities. To this end, we introduce mTLDR, a first-of-its-kind dataset for the aforementioned task, comprising videos, audio, and text, along with both author-composed summaries and expert-annotated summaries. The mTLDR dataset accompanies a total of 4,182 instances collected from various academic conference proceedings, such as ICLR, ACL, and CVPR. Subsequently, we present mTLDRgen, an encod
    
[^20]: 用对抗样本提升模型反演攻击能力

    Boosting Model Inversion Attacks with Adversarial Examples. (arXiv:2306.13965v1 [cs.CR])

    [http://arxiv.org/abs/2306.13965](http://arxiv.org/abs/2306.13965)

    本文提出了一种新的基于学习的模型反演攻击训练方法，通过添加语义损失函数和注入对抗样本，可以在黑盒设置下提高攻击精度和成功率。

    

    模型反演攻击指恢复目标模型的训练数据，这对机器学习模型的隐私构成严重威胁。然而，这些攻击尤其是基于学习的方法，通常会面临低攻击精度的问题，即机器学习分类器对这些恢复数据的分类精度较低。对此，最近的研究表明，基于 GAN 的模型反演攻击可以有效提高攻击精度。然而，这些基于 GAN 的攻击只重构每类的类代表性训练数据，而基于学习的攻击可以重构不同类别中多样化的训练数据。因此，在本文中，我们提出了一种新的基于学习的模型反演攻击训练范式，可以在黑盒设置下实现更高的攻击精度。具体而言，我们通过添加语义损失函数来规范化攻击模型的训练过程，其次，我们在攻击模型训练数据中注入对抗样本，使其对抗性攻击更加鲁棒。实验结果表明，我们的方法在攻击精度和成功率方面优于现有方法。

    Model inversion attacks involve reconstructing the training data of a target model, which raises serious privacy concerns for machine learning models. However, these attacks, especially learning-based methods, are likely to suffer from low attack accuracy, i.e., low classification accuracy of these reconstructed data by machine learning classifiers. Recent studies showed an alternative strategy of model inversion attacks, GAN-based optimization, can improve the attack accuracy effectively. However, these series of GAN-based attacks reconstruct only class-representative training data for a class, whereas learning-based attacks can reconstruct diverse data for different training data in each class. Hence, in this paper, we propose a new training paradigm for a learning-based model inversion attack that can achieve higher attack accuracy in a black-box setting. First, we regularize the training process of the attack model with an added semantic loss function and, second, we inject adversa
    
[^21]: 利用范畴论整合到冲突解决中的图模型的分类方法

    Categorical Approach to Conflict Resolution: Integrating Category Theory into the Graph Model for Conflict Resolution. (arXiv:2306.13961v1 [cs.AI])

    [http://arxiv.org/abs/2306.13961](http://arxiv.org/abs/2306.13961)

    本论文介绍了一种新型的冲突解决框架，称为C-GMCR，它将范畴论整合到传统的图模型中，能够提供更抽象和通用的分析冲突解决的方式。通过应用到囚徒困境和其他案例中，发现分类方法提供了新的视角和可能导致更有效的冲突解决策略的发展。

    

    本文介绍了冲突解决中的范畴图模型（C-GMCR），这是一种将范畴论整合到传统的图模型中的新型框架。C-GMCR框架提供了更抽象和通用的方式来建模和分析冲突解决，使研究人员能够发现更深层次的见解和联系。本文介绍了C-GMCR框架的基本概念、方法和应用到著名的囚徒困境和其他代表性案例中。结果表明，分类方法为稳定概念提供了新的视角，并有可能导致更有效的冲突解决策略的发展。

    This paper introduces the Categorical Graph Model for Conflict Resolution (C-GMCR), a novel framework that integrates category theory into the traditional Graph Model for Conflict Resolution (GMCR). The C-GMCR framework provides a more abstract and general way to model and analyze conflict resolution, enabling researchers to uncover deeper insights and connections. We present the basic concepts, methods, and application of the C-GMCR framework to the well-known Prisoner's Dilemma and other representative cases. The findings suggest that the categorical approach offers new perspectives on stability concepts and can potentially lead to the development of more effective conflict resolution strategies.
    
[^22]: 基于正则化SE(3)群卷积的体积医学图像分析

    Regular SE(3) Group Convolutions for Volumetric Medical Image Analysis. (arXiv:2306.13960v1 [cs.CV])

    [http://arxiv.org/abs/2306.13960](http://arxiv.org/abs/2306.13960)

    本文提出了基于正则化SE(3)群卷积的体积医学图像分析方法，通过分解连续SO(3)核和空间核以实现旋转平移等变性，并在医学分类任务中获得了显著性能提升。

    

    研究表明，正则组卷积神经网络(G-CNN)可以提高模型性能并提高对不同几何对称性的等变性。本文解决了SE(3)问题，即旋转平移等变性在体积数据上的问题。体积图像数据在许多医疗设置中普遍存在。受可分离组卷积的最新工作的启发，我们设计了一个SE(3)群卷积核，将其分解为连续的SO(3)（旋转）核和空间核。我们通过采样均匀的SO(3)网格来近似连续设定下的对称性。我们的连续SO(3)核是通过类似均匀网格的RBF插值参数化的。我们展示了我们的方法在体积医学图像分析中的优势。我们的SE(3)等变模型在具有挑战性的医学分类任务上始终优于CNN和常规离散G-CNN，并显示出显着改进的泛化能力。我们的方法在噪声数据下的性能提高了达到16.5%。

    Regular group convolutional neural networks (G-CNNs) have been shown to increase model performance and improve equivariance to different geometrical symmetries. This work addresses the problem of SE(3), i.e., roto-translation equivariance, on volumetric data. Volumetric image data is prevalent in many medical settings. Motivated by the recent work on separable group convolutions, we devise a SE(3) group convolution kernel separated into a continuous SO(3) (rotation) kernel and a spatial kernel. We approximate equivariance to the continuous setting by sampling uniform SO(3) grids. Our continuous SO(3) kernel is parameterized via RBF interpolation on similarly uniform grids. We demonstrate the advantages of our approach in volumetric medical image analysis. Our SE(3) equivariant models consistently outperform CNNs and regular discrete G-CNNs on challenging medical classification tasks and show significantly improved generalization capabilities. Our approach achieves up to a 16.5% gain in
    
[^23]: 多方会话中的情感反转推理

    Emotion Flip Reasoning in Multiparty Conversations. (arXiv:2306.13959v1 [cs.CL])

    [http://arxiv.org/abs/2306.13959](http://arxiv.org/abs/2306.13959)

    本文旨在识别说话者情感翻转背后的推动者，提出了一个数据集和神经架构c进行支持。

    

    在对话中，说话者可能有不同的情感状态，它们的动态在理解情感话语中起着重要作用。然而，仅仅检测情感并不足以完全理解会话过程中发生的说话者特定情感变化。为了高效地理解说话者的情感动态，有必要确定导致说话者情感翻转的原因或推动者。在本文中，我们探讨了名为Instigator based Emotion Flip Reasoning (EFR) 的任务，旨在识别会话中说话者情感翻转背后的推动者。例如，从快乐到愤怒的情感翻转可能是由威胁这样的推动者引起的。为了实现这个任务，我们提出了一个包括符合情感心理学标准的EFR推动者标签的数据集MELD-I。为了评估数据集，我们提出了一个新颖的神经架构c。

    In a conversational dialogue, speakers may have different emotional states and their dynamics play an important role in understanding dialogue's emotional discourse. However, simply detecting emotions is not sufficient to entirely comprehend the speaker-specific changes in emotion that occur during a conversation. To understand the emotional dynamics of speakers in an efficient manner, it is imperative to identify the rationale or instigator behind any changes or flips in emotion expressed by the speaker. In this paper, we explore the task called Instigator based Emotion Flip Reasoning (EFR), which aims to identify the instigator behind a speaker's emotion flip within a conversation. For example, an emotion flip from joy to anger could be caused by an instigator like threat. To facilitate this task, we present MELD-I, a dataset that includes ground-truth EFR instigator labels, which are in line with emotional psychology. To evaluate the dataset, we propose a novel neural architecture c
    
[^24]: 线性时间逻辑规则的时间点解释框架

    Pointwise-in-Time Explanation for Linear Temporal Logic Rules. (arXiv:2306.13956v1 [cs.AI])

    [http://arxiv.org/abs/2306.13956](http://arxiv.org/abs/2306.13956)

    本文提出了一个可以评估给定路径规划中特定时间点上的单个线性时间逻辑(LTL)约束的相关性和状态的框架，可以用于在离散时间、离散空间中执行有限计划的代理任务中，为用户提供时间点解释和规则参数状态的洞察力。

    

    本文介绍了一个框架来评估给定路径规划中特定时间点上的单个线性时间逻辑(LTL)约束的相关性，这个任务被我们称为“时间点解释”。我们开发了一个包含状态评估算法的框架，适用于在Kripke结构可表达的离散时间、离散空间中执行有限计划的代理。在给定的结构上和已知约束代理的一组LTL规则的计划中，该算法针对两种类型的用户查询响应地生成解释。对于所选的查询时间，解释识别哪些规则是活动的，哪些规则刚刚被满足，哪些规则是不活动的，其中框架状态标准是正式和直观地定义的。解释还可以包括单个规则参数的状态，以提供进一步的洞察力。在本文中，我们系统地介绍了这个新颖的框架，并提供了其实现的示例。

    This work introduces a framework to assess the relevance of individual linear temporal logic (LTL) constraints at specific times in a given path plan, a task we refer to as "pointwise-in-time" explanation. We develop this framework, featuring a status assessment algorithm, for agents which execute finite plans in a discrete-time, discrete-space setting expressible via a Kripke structure. Given a plan on this structure and a set of LTL rules which are known to constrain the agent, the algorithm responds to two types of user queries to produce explanation. For the selected query time, explanations identify which rules are active, which have just been satisfied, and which are inactive, where the framework status criteria are formally and intuitively defined. Explanations may also include the status of individual rule arguments to provide further insight. In this paper, we systematically present this novel framework and provide an example of its implementation.
    
[^25]: 实现真实空气质量预测：介绍易于使用的PurpleAirSF数据集

    Unleashing Realistic Air Quality Forecasting: Introducing the Ready-to-Use PurpleAirSF Dataset. (arXiv:2306.13948v1 [cs.LG])

    [http://arxiv.org/abs/2306.13948](http://arxiv.org/abs/2306.13948)

    该论文介绍了PurpleAirSF数据集，这是一个易于获取的数据集，具有高时间分辨率、多种空气质量测量指标和广泛的地理范围，可用于研究人员的空气质量预测建模和空气污染模式研究，同时此数据集可用于未来开发新型的应用模型。

    

    空气质量预测由于机器学习和深度学习模型的进步引起了人们的极大关注。然而，复杂的数据采集和开放数据集的缺乏给研究人员造成了挑战，从而阻碍了有效的模型验证。本文介绍了PurpleAirSF，这是一个综合全面且易于获取的数据集，从PurpleAir网络中收集而来。该数据集具有高时间分辨率、各种空气质量测量指标和广泛的地理覆盖范围，可作为研究人员开发新型预测模型、研究空气污染模式以及调查其对健康和环境的影响的有用工具。我们介绍了构建PurpleAirSF所采用的数据采集和处理方法。此外，我们还使用经典和现代时空预测模型进行了初步实验，从而为未来空气质量预测模型的制定建立了基准。

    Air quality forecasting has garnered significant attention recently, with data-driven models taking center stage due to advancements in machine learning and deep learning models. However, researchers face challenges with complex data acquisition and the lack of open-sourced datasets, hindering efficient model validation. This paper introduces PurpleAirSF, a comprehensive and easily accessible dataset collected from the PurpleAir network. With its high temporal resolution, various air quality measures, and diverse geographical coverage, this dataset serves as a useful tool for researchers aiming to develop novel forecasting models, study air pollution patterns, and investigate their impacts on health and the environment. We present a detailed account of the data collection and processing methods employed to build PurpleAirSF. Furthermore, we conduct preliminary experiments using both classic and modern spatio-temporal forecasting models, thereby establishing a benchmark for future air q
    
[^26]: 大型序列模型用于顺序决策：综述

    Large Sequence Models for Sequential Decision-Making: A Survey. (arXiv:2306.13945v1 [cs.LG])

    [http://arxiv.org/abs/2306.13945](http://arxiv.org/abs/2306.13945)

    本综述全面概述了使用Transformer等序列模型解决顺序决策问题的最近研究进展，并按照处理样本效率、信用分配和部分可观察性的方式对其进行分类。

    

    Transformer结构促进了自然语言处理和计算机视觉中预测任务的大规模通用序列模型的发展，例如GPT-3和Swin Transformer。虽然最初设计用于预测问题，但自然而然地会询问它们是否适用于通常存在样本效率、信用分配和部分可观察性问题的顺序决策和强化学习问题。近年来，序列模型，特别是Transformer，吸引了RL社区越来越多的关注，产生了许多具有显着有效性和通用性的方法。该综述全面概述了最近的工作，旨在通过讨论顺序决策和序列建模之间的联系，并基于它们处理前述问题的方式对它们进行分类，解决使用序列模型（例如Transformer）解决顺序决策任务的问题。

    Transformer architectures have facilitated the development of large-scale and general-purpose sequence models for prediction tasks in natural language processing and computer vision, e.g., GPT-3 and Swin Transformer. Although originally designed for prediction problems, it is natural to inquire about their suitability for sequential decision-making and reinforcement learning problems, which are typically beset by long-standing issues involving sample efficiency, credit assignment, and partial observability. In recent years, sequence models, especially the Transformer, have attracted increasing interest in the RL communities, spawning numerous approaches with notable effectiveness and generalizability. This survey presents a comprehensive overview of recent works aimed at solving sequential decision-making tasks with sequence models such as the Transformer, by discussing the connection between sequential decision-making and sequence modeling, and categorizing them based on the way they 
    
[^27]: 具有避开死局和恢复能力的安全强化学习

    Safe Reinforcement Learning with Dead-Ends Avoidance and Recovery. (arXiv:2306.13944v1 [cs.LG])

    [http://arxiv.org/abs/2306.13944](http://arxiv.org/abs/2306.13944)

    本文提出了一种利用死局的边界来辨别安全和不安全状态，以确保安全性同时减少对探索的限制的方法。采用了分离的强化学习框架，训练了两个策略：一个任务策略，专注于任务表现，以及一个恢复策略，最大化安全性。

    

    安全性是将强化学习应用于现实环境任务时面临的主要挑战之一。为了确保在训练过程中和之后的安全性，现有的方法往往采用过于保守的策略以避免不安全的情况。但是，过于保守的策略严重阻碍了探索，使算法的回报大大降低。在本文中，我们提出了一种方法来构建一个边界，区分安全和不安全的状态。我们构建的边界等价于区分死局状态，表明安全探索的最大程度，因此在探索方面的限制最小。类似于恢复强化学习，我们利用一个分离的强化学习框架来学习两个策略，(1) 只考虑改善任务表现的任务策略，以及 (2) 最大化安全性的恢复策略。恢复策略和相应的安全性批判家在离线数据集上进行预训练，其中安全批判家会区分安全和不安全的状态，而恢复策略会采取措施以从不安全状态中恢复。

    Safety is one of the main challenges in applying reinforcement learning to realistic environmental tasks. To ensure safety during and after training process, existing methods tend to adopt overly conservative policy to avoid unsafe situations. However, overly conservative policy severely hinders the exploration, and makes the algorithms substantially less rewarding. In this paper, we propose a method to construct a boundary that discriminates safe and unsafe states. The boundary we construct is equivalent to distinguishing dead-end states, indicating the maximum extent to which safe exploration is guaranteed, and thus has minimum limitation on exploration. Similar to Recovery Reinforcement Learning, we utilize a decoupled RL framework to learn two policies, (1) a task policy that only considers improving the task performance, and (2) a recovery policy that maximizes safety. The recovery policy and a corresponding safety critic are pretrained on an offline dataset, in which the safety c
    
[^28]: 良好的解释者暗地里是人类-主动学习者吗？

    Are Good Explainers Secretly Human-in-the-Loop Active Learners?. (arXiv:2306.13935v1 [cs.AI])

    [http://arxiv.org/abs/2306.13935](http://arxiv.org/abs/2306.13935)

    本文提出了一种可解释的AI技术，用于获取额外的训练数据，同时考虑到人类的介入，这可以通过模拟来评估其效用，同时具有与标准主动学习算法的可比性。

    

    可解释的人工智能（XAI）技术近年来在多个用例中变得流行。在这里，我们考虑了它在研究模型预测以收集额外训练数据方面的应用。我们认为这相当于主动学习，其中查询策略涉及人类的介入。我们提供了一个人类角色的数学近似，并提出了一个端到端工作流的通用形式化。这使我们能够严格比较此用法与标准主动学习算法，同时允许扩展工作流。一个额外的好处是，它们的效用可以通过模拟来评估，而不是进行昂贵的用户研究。我们还提出了一些初步的有前途的结果。

    Explainable AI (XAI) techniques have become popular for multiple use-cases in the past few years. Here we consider its use in studying model predictions to gather additional training data. We argue that this is equivalent to Active Learning, where the query strategy involves a human-in-the-loop. We provide a mathematical approximation for the role of the human, and present a general formalization of the end-to-end workflow. This enables us to rigorously compare this use with standard Active Learning algorithms, while allowing for extensions to the workflow. An added benefit is that their utility can be assessed via simulation instead of conducting expensive user-studies. We also present some initial promising results.
    
[^29]: 深度学习模型预测股票指数的比较研究

    Comparative Study of Predicting Stock Index Using Deep Learning Models. (arXiv:2306.13931v1 [cs.LG])

    [http://arxiv.org/abs/2306.13931](http://arxiv.org/abs/2306.13931)

    本文通过对传统预测方法和深度学习模型进行比较研究，证明Deep AR在股票指数预测方面表现最佳，具有较高的鲁棒性。

    

    过去几十年中现已尝试了许多时间序列预测方法，包括传统的技术分析、算法统计模型和最近的机器学习和人工智能方法。近年来，神经网络已经被纳入到预测场景中，如LSTM和常规RNN方法，利用了短期和长期的依赖关系。本研究评估了传统的预测方法，如ARIMA、SARIMA和SARIMAX，以及使用RNN构建的新型神经网络方法，如DF-RNN、DSSM和Deep AR。使用来自Kaggle的标准NIFTY-50数据集使用MSE、RMSE、MAPE、POCID和Theil's U等指标来评估这些模型。结果表明，Deep AR在所有其他常规深度学习和传统方法中表现最佳，其MAPE为0.01，RMSE为189。此外，当减少训练数据量时，Deep AR和GRU的性能不会降低，这表明它们的鲁棒性很高。

    Time series forecasting has seen many methods attempted over the past few decades, including traditional technical analysis, algorithmic statistical models, and more recent machine learning and artificial intelligence approaches. Recently, neural networks have been incorporated into the forecasting scenario, such as the LSTM and conventional RNN approaches, which utilize short-term and long-term dependencies. This study evaluates traditional forecasting methods, such as ARIMA, SARIMA, and SARIMAX, and newer neural network approaches, such as DF-RNN, DSSM, and Deep AR, built using RNNs. The standard NIFTY-50 dataset from Kaggle is used to assess these models using metrics such as MSE, RMSE, MAPE, POCID, and Theil's U. Results show that Deep AR outperformed all other conventional deep learning and traditional approaches, with the lowest MAPE of 0.01 and RMSE of 189. Additionally, the performance of Deep AR and GRU did not degrade when the amount of training data was reduced, suggesting t
    
[^30]: 评估GAN生成的合成表格数据用于类别平衡和低资源环境中的实用性

    Evaluating the Utility of GAN Generated Synthetic Tabular Data for Class Balancing and Low Resource Settings. (arXiv:2306.13929v1 [cs.LG])

    [http://arxiv.org/abs/2306.13929](http://arxiv.org/abs/2306.13929)

    本研究评估了GAN生成的合成数据在解决分类任务中不平衡数据和提高模型在低资源环境中的性能方面的实用性，并发现在GAN合成数据上进行训练的模型性能表现更佳。

    

    本研究旨在解决分类任务中不平衡数据的问题，并评估了SMOTE、ADASYN和GAN技术在生成合成数据以解决类别不平衡并改善在低资源环境中分类模型性能方面的适用性。该研究采用广义线性模型算法进行类别平衡实验，采用随机森林算法进行低资源设置实验，以评估在不同训练数据下的模型性能。所有分类模型的主要评估指标是召回率。类别平衡实验的结果表明，在GAN平衡数据上训练的GLM模型实现了最高的召回率。同样在低资源实验中，训练在GAN合成数据上的模型展现出比原始数据更好的召回率。这些发现展示了GAN生成的合成数据在解决分类任务中不平衡数据的挑战和改善低资源环境中模型性能的潜力。

    The present study aimed to address the issue of imbalanced data in classification tasks and evaluated the suitability of SMOTE, ADASYN, and GAN techniques in generating synthetic data to address the class imbalance and improve the performance of classification models in low-resource settings. The study employed the Generalised Linear Model (GLM) algorithm for class balancing experiments and the Random Forest (RF) algorithm for low-resource setting experiments to assess model performance under varying training data. The recall metric was the primary evaluation metric for all classification models. The results of the class balancing experiments showed that the GLM model trained on GAN-balanced data achieved the highest recall value. Similarly, in low-resource experiments, models trained on data enhanced with GAN-synthesized data exhibited better recall values than original data. These findings demonstrate the potential of GAN-generated synthetic data for addressing the challenge of imbal
    
[^31]: 自动驾驶模拟中的主动数据采集

    Active Data Acquisition in Autonomous Driving Simulation. (arXiv:2306.13923v1 [cs.LG])

    [http://arxiv.org/abs/2306.13923](http://arxiv.org/abs/2306.13923)

    本文提出了一种主动数据采集策略来解决自动驾驶模拟中大量冗余数据集的问题，并通过实验验证，该策略可以显著降低标注成本和数据集大小，同时提高数据集的整体质量，从而提高自动驾驶系统的性能。

    

    自动驾驶算法高度依赖基于学习的模型，这些模型需要大量的数据集进行训练。然而，这些数据集通常包含大量冗余信息，而且收集和处理这些数据集可能非常耗时和昂贵。为了解决这个问题，本文提出了主动数据采集策略的概念。对于高质量的数据，增加采集密度可以提高数据集的整体质量，最终实现与原始数据集类似甚至更好的结果，同时降低标注成本和数据集大小。本文设计了实验来验证收集的数据集的质量，并演示该策略可以显著降低标注成本和数据集大小，同时提高数据集的整体质量，从而提高自动驾驶系统的性能。实现所提出方法的源代码可在https://github.com/Th1nkMore公开获取。

    Autonomous driving algorithms rely heavily on learning-based models, which require large datasets for training. However, there is often a large amount of redundant information in these datasets, while collecting and processing these datasets can be time-consuming and expensive. To address this issue, this paper proposes the concept of an active data-collecting strategy. For high-quality data, increasing the collection density can improve the overall quality of the dataset, ultimately achieving similar or even better results than the original dataset with lower labeling costs and smaller dataset sizes. In this paper, we design experiments to verify the quality of the collected dataset and to demonstrate this strategy can significantly reduce labeling costs and dataset size while improving the overall quality of the dataset, leading to better performance of autonomous driving systems. The source code implementing the proposed approach is publicly available on https://github.com/Th1nkMore
    
[^32]: 利用生成模型进行语义轨迹分析的时空叙事？

    Spatio-temporal Storytelling? Leveraging Generative Models for Semantic Trajectory Analysis. (arXiv:2306.13905v1 [cs.CL])

    [http://arxiv.org/abs/2306.13905](http://arxiv.org/abs/2306.13905)

    本文通过生成语言模型对语义轨迹进行分析和生成合成语义轨迹数据，为从城市规划到个性化推荐引擎和商业策略等一系列应用做出贡献。

    

    本文提出了一种使用生成语言模型对语义轨迹跟踪进行分析和生成合成语义轨迹数据（SST）的愿景。利用深度学习的进步，如自然语言处理（NLP）、计算机视觉等领域的进展，我们旨在创建智能模型，可以研究不同上下文中的语义轨迹，预测未来趋势，增强机器对动物、人类、货物等移动情况的理解，提高人机交互，并为从城市规划到个性化推荐引擎和商业策略等一系列应用做出贡献。

    In this paper, we lay out a vision for analysing semantic trajectory traces and generating synthetic semantic trajectory data (SSTs) using generative language model. Leveraging the advancements in deep learning, as evident by progress in the field of natural language processing (NLP), computer vision, etc. we intend to create intelligent models that can study the semantic trajectories in various contexts, predicting future trends, increasing machine understanding of the movement of animals, humans, goods, etc. enhancing human-computer interactions, and contributing to an array of applications ranging from urban-planning to personalized recommendation engines and business strategy.
    
[^33]: 通过生成问题陈述的语言变体解决数学问题

    Math Word Problem Solving by Generating Linguistic Variants of Problem Statements. (arXiv:2306.13899v1 [cs.CL])

    [http://arxiv.org/abs/2306.13899](http://arxiv.org/abs/2306.13899)

    该论文提出了一个通过生成问题文本语言变体的方法来解决数学问题，该方法利用DeBERTa作为编码器，同时引入了一个挑战性的数据集用于评估模型性能。结果表明，该框架在两个基准数据集以及作者提出的数据集上优于现有技术水平的模型，证明了其推导正确的解决方案表达式的能力。

    

    数学推理艺术是智力进展的基本支柱，是培养人类独创性的核心催化剂。最近，研究人员已发表了大量围绕解决数学语言问题（MWP）的作品，这是迈向通用AI的重要步骤。这些现有模型容易依赖于肤浅的启发式和虚假的相关性来推导解决方案表达式。为了改善这一问题，在本文中，我们提出了一个基于生成问题文本语言变体的MWP求解器框架。该方法涉及解决每个不同变体的问题并选择得票最多的预测表达式。我们使用DeBERTa（具有解码增强的BERT和分离注意力）作为编码器，以利用其丰富的文本表示和增强的遮罩解码器来构造解决方案表达式。此外，我们引入了一个具有挑战性的数据集$\mathrm{P\small{ARA}\normalsize}_\mathrm{gen}$-MWP，以评估模型在生成和解析变体问题方面的性能。我们的结果显示，所提出的框架在两个基准数据集和我们提出的$\mathrm{P\small{ARA}\normalsize}_\mathrm{gen}$-MWP上都优于现有技术水平的模型，证明了其通过理解和推理问题文本的语义细微差别来推导正确的解决方案表达式的能力。

    The art of mathematical reasoning stands as a fundamental pillar of intellectual progress and is a central catalyst in cultivating human ingenuity. Researchers have recently published a plethora of works centered around the task of solving Math Word Problems (MWP) $-$ a crucial stride towards general AI. These existing models are susceptible to dependency on shallow heuristics and spurious correlations to derive the solution expressions. In order to ameliorate this issue, in this paper, we propose a framework for MWP solvers based on the generation of linguistic variants of the problem text. The approach involves solving each of the variant problems and electing the predicted expression with the majority of the votes. We use DeBERTa (Decoding-enhanced BERT with disentangled attention) as the encoder to leverage its rich textual representations and enhanced mask decoder to construct the solution expressions. Furthermore, we introduce a challenging dataset, $\mathrm{P\small{ARA}\normalsi
    
[^34]: ICN：交互式卷积网络用于共享微移动出行需求预测

    ICN: Interactive Convolutional Network for Forecasting Travel Demand of Shared Micromobility. (arXiv:2306.13897v1 [cs.CY])

    [http://arxiv.org/abs/2306.13897](http://arxiv.org/abs/2306.13897)

    本文提出了一种名为交互式卷积网络（ICN）的深度学习模型，用于预测共享微移动的时空出行需求。ICN模型采用新颖的通道扩张方法和卷积操作同时捕捉时间和空间依赖关系，提取不同时间分辨率下的特征，实现高精度时空共享微移动需求的预测。

    

    准确的共享微移动需求预测对于交通规划和管理至关重要。尽管深度学习模型为处理需求预测问题提供了强大的工具，但是对于高精度时空共享微移动需求的预测研究仍然不足。本文提出了一种名为交互式卷积网络（ICN）的深度学习模型，用于预测共享微移动的时空出行需求。所提出的模型利用旅行行为知识，基于多维空间信息（即人口统计学，功能和交通供给）开发了一种新颖的通道扩张方法来构建深度学习模型。我们使用卷积运算来处理扩张的张量，从而同时捕捉时间和空间依赖关系。基于二叉树结构的架构和交互式卷积，ICN模型提取不同时间分辨率下的特征，然后生成预测。

    Accurate shared micromobility demand predictions are essential for transportation planning and management. Although deep learning models provide powerful tools to deal with demand prediction problems, studies on forecasting highly-accurate spatiotemporal shared micromobility demand are still lacking. This paper proposes a deep learning model named Interactive Convolutional Network (ICN) to forecast spatiotemporal travel demand for shared micromobility. The proposed model develops a novel channel dilation method by utilizing multi-dimensional spatial information (i.e., demographics, functionality, and transportation supply) based on travel behavior knowledge for building the deep learning model. We use the convolution operation to process the dilated tensor to simultaneously capture temporal and spatial dependencies. Based on a binary-tree-structured architecture and interactive convolution, the ICN model extracts features at different temporal resolutions, and then generates prediction
    
[^35]: 具有共识算法的差分隐私分散深度学习

    Differentially Private Decentralized Deep Learning with Consensus Algorithms. (arXiv:2306.13892v1 [cs.LG])

    [http://arxiv.org/abs/2306.13892](http://arxiv.org/abs/2306.13892)

    本论文提出了一种具有差分隐私保护的分散学习算法，可用于合作分散深度学习，防止共享模型参数时泄露私有数据集的信息。

    

    合作分散深度学习依赖于通信代理之间的直接信息交换，每个代理都可以访问应该保持私有的本地数据集。目标是在训练后使得所有代理在模型参数上达成共识。然而，与不可信的邻居代理共享参数可能会泄露有关本地数据集的可利用信息。为了解决这个问题，我们介绍了一种差分隐私分散学习方法，以在合作训练期间和之后保护每个代理的本地数据集。在我们的方法中，我们将常用于集中式深度学习的差分隐私随机梯度下降（DP-SGD）泛化到实用的基于子梯度和ADMM的分散学习方法中。我们的算法的差分隐私保证适用于任意深度学习目标函数，并分析了强凸目标函数的收敛性质。我们将我们的算法与其他差分隐私算法进行比较。

    Cooperative decentralized deep learning relies on direct information exchange between communicating agents, each with access to a local dataset which should be kept private. The goal is for all agents to achieve consensus on model parameters after training. However, sharing parameters with untrustworthy neighboring agents could leak exploitable information about local datasets. To combat this, we introduce differentially private decentralized learning that secures each agent's local dataset during and after cooperative training. In our approach, we generalize Differentially Private Stochastic Gradient Descent (DP-SGD) -- a popular differentially private training method for centralized deep learning -- to practical subgradient- and ADMM-based decentralized learning methods. Our algorithms' differential privacy guarantee holds for arbitrary deep learning objective functions, and we analyze the convergence properties for strongly convex objective functions. We compare our algorithms again
    
[^36]: 可解释人工智能中的操纵风险: 不一致问题的影响

    Manipulation Risks in Explainable AI: The Implications of the Disagreement Problem. (arXiv:2306.13885v1 [cs.AI])

    [http://arxiv.org/abs/2306.13885](http://arxiv.org/abs/2306.13885)

    本文讨论可解释人工智能中的操纵风险，即同一决策或预测可能有多种解释带来的挑战。本文分析了攻击机器学习模型或底层数据以影响解释与直接利用解释阶段的策略，并探讨了解释提供者可追求的几个目标和具体场景。

    

    人工智能系统在我们生活的高风险领域中越来越广泛地应用，这增加了解释这些决策并确保它们与我们想要的决策一致的需求。因此，可解释人工智能（XAI）领域显现出来。然而，它面临一项重大挑战，即不一致问题，即同一人工智能的决策或预测可能有多种解释。虽然已经认识到了不一致问题的存在，但与此问题相关的潜在影响尚未被广泛研究。在本文中，我们首先概述了解释提供者可以采用的不同策略，以使返回的解释符合他们的利益。我们区分了攻击机器学习模型或底层数据以影响解释的策略和直接利用解释阶段的策略。接下来，我们分析了解释提供者可追求的几个目标和具体场景。

    Artificial Intelligence (AI) systems are increasingly used in high-stakes domains of our life, increasing the need to explain these decisions and to make sure that they are aligned with how we want the decision to be made. The field of Explainable AI (XAI) has emerged in response. However, it faces a significant challenge known as the disagreement problem, where multiple explanations are possible for the same AI decision or prediction. While the existence of the disagreement problem is acknowledged, the potential implications associated with this problem have not yet been widely studied. First, we provide an overview of the different strategies explanation providers could deploy to adapt the returned explanation to their benefit. We make a distinction between strategies that attack the machine learning model or underlying data to influence the explanations, and strategies that leverage the explanation phase directly. Next, we analyse several objectives and concrete scenarios the provid
    
[^37]: 学习排序遇见语言：增强基于语言驱动的排序对齐以支持序数分类

    Learning-to-Rank Meets Language: Boosting Language-Driven Ordering Alignment for Ordinal Classification. (arXiv:2306.13856v1 [cs.CV])

    [http://arxiv.org/abs/2306.13856](http://arxiv.org/abs/2306.13856)

    本文提出了一种利用语言驱动的高效序数分类方法，即L2RCLIP，它通过视觉-语言对齐任务充分利用语言中的序数先验，利用补充提示调整技术RankFormer增强原始排序提示的排序关系，并使用跨模态排序约束损失(CMOCL)进一步将语言先验融入模型中。在多个标准数据集中，L2RCLIP都比现有最先进方法具有更好的性能表现。

    

    我们提出了一种新颖的基于语言驱动的排序对准方法，用于序数分类。在序数分类中，标签包含额外的排序关系，如果仅依赖于训练数据，很容易出现过拟合现象。最近预训练的视觉-语言模型的发展启发我们通过将原始任务转化为视觉-语言对齐任务来利用人类语言中丰富的序数先验。因此，我们提出了L2RCLIP，它从两个方面充分利用了语言先验：首先，我们引入了一种补充提示调整技术RankFormer，旨在增强原始排序提示的排序关系。它在单词嵌入空间中使用标记级别的注意力和残差风格提示混合。其次，为了进一步融入语言先验，我们重新考虑了香草交叉熵损失的近似绑定优化，并在跨模态嵌入空间内进行了重构。因此，我们提出了一种跨模态排序约束损失（CMOCL），用于规范从语言中导出的序数约束。实验结果表明，我们提出的方法在多个流行的序数分类基准数据集上均显著优于现有最先进方法。

    We present a novel language-driven ordering alignment method for ordinal classification. The labels in ordinal classification contain additional ordering relations, making them prone to overfitting when relying solely on training data. Recent developments in pre-trained vision-language models inspire us to leverage the rich ordinal priors in human language by converting the original task into a vision-language alignment task. Consequently, we propose L2RCLIP, which fully utilizes the language priors from two perspectives. First, we introduce a complementary prompt tuning technique called RankFormer, designed to enhance the ordering relation of original rank prompts. It employs token-level attention with residual-style prompt blending in the word embedding space. Second, to further incorporate language priors, we revisit the approximate bound optimization of vanilla cross-entropy loss and restructure it within the cross-modal embedding space. Consequently, we propose a cross-modal ordin
    
[^38]: 相似性保持对抗图对比学习

    Similarity Preserving Adversarial Graph Contrastive Learning. (arXiv:2306.13854v1 [cs.LG])

    [http://arxiv.org/abs/2306.13854](http://arxiv.org/abs/2306.13854)

    本文提出了一种相似性保持的对抗图对比学习（SP-AGCL）框架，可以实现对抗攻击的对抗鲁棒性，同时保持节点特征相似性。

    

    最近的研究表明，图神经网络模型易受到对抗攻击，即对图结构和节点特征进行微小扰动。在各种图神经网络模型中，基于图对比学习（GCL）的方法特别容易受到对抗攻击，因为它们的固有设计高度依赖于从原始图派生出的自监督信号，然而当图受到攻击时，原始图中已经包含了噪声。为了实现对这种攻击的对抗鲁棒性，现有方法将对抗训练（AT）应用于GCL框架，将攻击的图作为GCL框架下的增强。然而，我们发现现有的经过对抗训练的GCL方法在保持节点特征相似性方面付出了代价。在本文中，我们提出了一种相似性保持的对抗图对比学习（SP-AGCL）框架，将干净的图与两个不同视图的辅助图进行对比。

    Recent works demonstrate that GNN models are vulnerable to adversarial attacks, which refer to imperceptible perturbation on the graph structure and node features. Among various GNN models, graph contrastive learning (GCL) based methods specifically suffer from adversarial attacks due to their inherent design that highly depends on the self-supervision signals derived from the original graph, which however already contains noise when the graph is attacked. To achieve adversarial robustness against such attacks, existing methods adopt adversarial training (AT) to the GCL framework, which considers the attacked graph as an augmentation under the GCL framework. However, we find that existing adversarially trained GCL methods achieve robustness at the expense of not being able to preserve the node feature similarity. In this paper, we propose a similarity-preserving adversarial graph contrastive learning (SP-AGCL) framework that contrasts the clean graph with two auxiliary views of differe
    
[^39]: 预训练真的比元学习更好吗？

    Is Pre-training Truly Better Than Meta-Learning?. (arXiv:2306.13841v1 [cs.LG])

    [http://arxiv.org/abs/2306.13841](http://arxiv.org/abs/2306.13841)

    在少样本学习中，当数据集的正式多样性较低时，预训练模型（PT）胜过模型无关元学习（MAML）。当正式多样性较高时，MAML更好。

    

    在少样本学习的背景下，目前普遍认为固定的预训练模型（PT）加上在评价时微调最后一层，胜过标准的元学习算法。我们通过深入的实证研究和广泛的数据集比较PT和模型无关元学习（MAML）这些说法。与以前的工作不同，我们强调使用相同的体系结构、相同的优化器，以及所有模型都训练到收敛。关键地，我们使用一个更严格的统计工具——效应量（Cohen's d）——来确定使用PT与使用MAML之间的模型差异的实际意义。然后使用一个预先提出的度量——多样性系数——来计算数据集的平均正式多样性。使用这种分析，我们证明了以下事实：1. 当数据集的正式多样性较低时，PT在平均意义上胜过MAML；2. 当正式多样性较高时，MAML胜过PT。

    In the context of few-shot learning, it is currently believed that a fixed pre-trained (PT) model, along with fine-tuning the final layer during evaluation, outperforms standard meta-learning algorithms. We re-evaluate these claims under an in-depth empirical examination of an extensive set of formally diverse datasets and compare PT to Model Agnostic Meta-Learning (MAML). Unlike previous work, we emphasize a fair comparison by using: the same architecture, the same optimizer, and all models trained to convergence. Crucially, we use a more rigorous statistical tool -- the effect size (Cohen's d) -- to determine the practical significance of the difference between a model trained with PT vs. a MAML. We then use a previously proposed metric -- the diversity coefficient -- to compute the average formal diversity of a dataset. Using this analysis, we demonstrate the following: 1. when the formal diversity of a data set is low, PT beats MAML on average and 2. when the formal diversity is hi
    
[^40]: 超越规模：多样性系数作为数据质量指标证明了LLMs是在形式多样的数据上预先训练的

    Beyond Scale: the Diversity Coefficient as a Data Quality Metric Demonstrates LLMs are Pre-trained on Formally Diverse Data. (arXiv:2306.13840v1 [cs.CL])

    [http://arxiv.org/abs/2306.13840](http://arxiv.org/abs/2306.13840)

    本论文提出使用多样性系数作为LLM预训练数据质量的指标，研究表明公开可用的LLM数据集的多样性系数很高。

    

    当前，预先训练强大的大语言模型(LLMs)的趋势主要集中在模型和数据集规模的扩大。然而，预先训练数据的质量对于训练强大的LLMs来说是一个重要因素，但它是一个模糊的概念，尚未完全表征。因此，我们使用最近提出的Task2Vec多样性系数来基于数据质量的形式方面，超越规模本身。具体而言，我们测量公开可用的预先训练数据集的多样性系数，以证明它们的形式多样性高于理论的下限和上限。此外，为了建立对多样性系数的信心，我们进行可解释性实验，并发现该系数与多样性的直观属性相吻合，例如，随着潜在概念数量的增加，它增加。我们得出结论，多样性系数是可靠的，表明公开可用的LLM数据集的多样性系数很高，并推测它可以作为预训练LLMs模型的数据质量指标。

    Current trends to pre-train capable Large Language Models (LLMs) mostly focus on scaling of model and dataset size. However, the quality of pre-training data is an important factor for training powerful LLMs, yet it is a nebulous concept that has not been fully characterized. Therefore, we use the recently proposed Task2Vec diversity coefficient to ground and understand formal aspects of data quality, to go beyond scale alone. Specifically, we measure the diversity coefficient of publicly available pre-training datasets to demonstrate that their formal diversity is high when compared to theoretical lower and upper bounds. In addition, to build confidence in the diversity coefficient, we conduct interpretability experiments and find that the coefficient aligns with intuitive properties of diversity, e.g., it increases as the number of latent concepts increases. We conclude the diversity coefficient is reliable, show it's high for publicly available LLM datasets, and conjecture it can be
    
[^41]: DEKGCI：一种用于整合知识图谱与用户-物品交互图的双面推荐模型

    DEKGCI: A double-sided recommendation model for integrating knowledge graph and user-item interaction graph. (arXiv:2306.13837v1 [cs.IR])

    [http://arxiv.org/abs/2306.13837](http://arxiv.org/abs/2306.13837)

    本文提出了DEKGCI，一种双面推荐模型，在用户-物品交互图和知识图谱中同时丰富用户和物品表示，以有效捕捉用户和物品之间的联合交互。

    

    由于能够提供丰富的信息，知识图谱和用户-物品交互图在推荐系统中被频繁使用来建模用户和物品。然而，现有的研究通常只关注其中一种信息源（即知识图谱或用户-物品交互图），导致未充分利用整合两种信息源所带来的好处。本文提出了一种新颖的双面推荐模型DEKGCI。在DEKGCI中，我们使用来自用户-物品交互图的高阶协作信号来丰富用户表示，同时利用来自知识图谱的高阶结构和语义信息来丰富物品表示。DEKGCI同时学习用户和物品表示，以有效捕捉用户和物品之间的联合交互。实验采用了三个真实世界的数据集来评估DEKGCI。

    Both knowledge graphs and user-item interaction graphs are frequently used in recommender systems due to their ability to provide rich information for modeling users and items. However, existing studies often focused on one of these sources (either the knowledge graph or the user-item interaction graph), resulting in underutilization of the benefits that can be obtained by integrating both sources of information. In this paper, we propose DEKGCI, a novel double-sided recommendation model. In DEKGCI, we use the high-order collaborative signals from the user-item interaction graph to enrich the user representations on the user side. Additionally, we utilize the high-order structural and semantic information from the knowledge graph to enrich the item representations on the item side. DEKGCI simultaneously learns the user and item representations to effectively capture the joint interactions between users and items. Three real-world datasets are adopted in the experiments to evaluate DEKG
    
[^42]: NLP Transformer中的双螺旋结构

    The Double Helix inside the NLP Transformer. (arXiv:2306.13817v1 [cs.AI])

    [http://arxiv.org/abs/2306.13817](http://arxiv.org/abs/2306.13817)

    本文介绍了一个NLP Transformer中分析位置、句法、语义和上下文信息的框架，揭示了位置信息通过螺旋路径在深层中自我分离，并在编码器和解码器侧生成词性聚类。提出了替代在语义嵌入中添加位置信息的方法。

    

    我们介绍了一个NLP Transformer中分析不同信息类型的框架。在这个方法中，我们区分了四层信息：位置、句法、语义和上下文。我们还提出，常见的在语义嵌入中添加位置信息的做法是次优的，提议使用线性加法(Linar-and-Add)的方法。我们的分析揭示了位置信息在深层中的自我分离。我们展示了嵌入向量的位置组成部分遵循螺旋的路径，在编码器侧和解码器侧都是如此。我们另外还展示，编码器侧的概念维度生成了词性(PoS)的聚类。在解码器侧，我们展示使用二元语法的方法有助于揭示下一个标记的词性聚类。我们的方法为阐明通过NLP Transformer的深层信息处理铺平了道路。

    We introduce a framework for analyzing various types of information in an NLP Transformer. In this approach, we distinguish four layers of information: positional, syntactic, semantic, and contextual. We also argue that the common practice of adding positional information to semantic embedding is sub-optimal and propose instead a Linear-and-Add approach. Our analysis reveals an autogenetic separation of positional information through the deep layers. We show that the distilled positional components of the embedding vectors follow the path of a helix, both on the encoder side and on the decoder side. We additionally show that on the encoder side, the conceptual dimensions generate Part-of-Speech (PoS) clusters. On the decoder side, we show that a di-gram approach helps to reveal the PoS clusters of the next token. Our approach paves a way to elucidate the processing of information through the deep layers of an NLP Transformer.
    
[^43]: 使用大型语言模型在道德教育和发展研究中的潜在优势

    Potential Benefits of Employing Large Language Models in Research in Moral Education and Development. (arXiv:2306.13805v1 [cs.CY])

    [http://arxiv.org/abs/2306.13805](http://arxiv.org/abs/2306.13805)

    本文探讨如何使用大型语言模型（LLM）在道德教育和发展研究领域做出贡献。最近的LLM具有新兴的上下文学习和思维链功能，可以通过推理和修订来解决困境。

    

    最近，计算机科学家通过使用大规模语料库和人工强化训练预测模型，开发了大型语言模型（LLM）。 LLM已成为实现人工智能在各个领域精确性的一种有前途的方式。有趣的是，最近的LLM具有模拟复杂人类认知的新兴功能特性，特别是上下文学习和思维链，这些特性在以前的预测模型中不可用。本文将探讨LLM如何可能为道德教育和发展研究做出贡献。为了实现这个目标，我将回顾最近发表的会议论文和ArXiv预印本，概述LLM中实现的新颖功能特性。我还打算使用ChatGPT进行简短实验，以研究LLM处理道德困境和外部反馈时的行为。结果表明，LLM可能能够基于推理和修订来解决困境。

    Recently, computer scientists have developed large language models (LLMs) by training prediction models with large-scale language corpora and human reinforcements. The LLMs have become one promising way to implement artificial intelligence with accuracy in various fields. Interestingly, recent LLMs possess emergent functional features that emulate sophisticated human cognition, especially in-context learning and the chain of thought, which were unavailable in previous prediction models. In this paper, I will examine how LLMs might contribute to moral education and development research. To achieve this goal, I will review the most recently published conference papers and ArXiv preprints to overview the novel functional features implemented in LLMs. I also intend to conduct brief experiments with ChatGPT to investigate how LLMs behave while addressing ethical dilemmas and external feedback. The results suggest that LLMs might be capable of solving dilemmas based on reasoning and revising
    
[^44]: 大象与算法：人工智能在大象监测中的当前和未来作用综述

    Elephants and Algorithms: A Review of the Current and Future Role of AI in Elephant Monitoring. (arXiv:2306.13803v1 [cs.AI])

    [http://arxiv.org/abs/2306.13803](http://arxiv.org/abs/2306.13803)

    本文探讨了AI和ML在保护非洲保护区中至关重要的大象方面的作用。新的AI和ML技术提供了解决方案以处理并提取重要信息。在AI专家和生态研究人员之间的协作下，这些创新技术可以帮助增强野生动物保护，为其他物种设定先例。

    

    人工智能（AI）和机器学习（ML）为增进对动物行为和保护策略的理解提供了革命性机会。以非洲保护区中至关重要的大象为焦点，本文探讨了AI和ML在它们保护中的作用。给定从各种传感器（如摄像头、麦克风、地震仪、无人机和卫星）收集到的越来越多的数据，挑战在于管理和解读这些庞大的数据。新的AI和ML技术提供了简化这一过程的解决方案，帮助我们提取重要信息，否则可能会被忽视。本文重点介绍了不同的AI驱动监测方法及其在改善大象保护方面的潜力。AI专家和生态研究人员之间的协作是利用这些创新技术以增强野生动物保护的关键所在，为许多其他物种设定了先例。

    Artificial intelligence (AI) and machine learning (ML) present revolutionary opportunities to enhance our understanding of animal behavior and conservation strategies. Using elephants, a crucial species in Africa's protected areas, as our focal point, we delve into the role of AI and ML in their conservation. Given the increasing amounts of data gathered from a variety of sensors like cameras, microphones, geophones, drones, and satellites, the challenge lies in managing and interpreting this vast data. New AI and ML techniques offer solutions to streamline this process, helping us extract vital information that might otherwise be overlooked. This paper focuses on the different AI-driven monitoring methods and their potential for improving elephant conservation. Collaborative efforts between AI experts and ecological researchers are essential in leveraging these innovative technologies for enhanced wildlife conservation, setting a precedent for numerous other species.
    
[^45]: 实现多目标优化问题的样本有效搜索中的目标空间多样性

    Achieving Diversity in Objective Space for Sample-efficient Search of Multiobjective Optimization Problems. (arXiv:2306.13780v1 [cs.AI])

    [http://arxiv.org/abs/2306.13780](http://arxiv.org/abs/2306.13780)

    本文提出一种新的多目标优化搜索方法，通过搜索满足用户指定性能标准的多样化结果集来找到一组有前途的设计解决方案，具有更好地展示出优秀解空间和提供决策者优秀设计决策的优势。

    

    高效地解决多目标优化问题，如材料设计等重要科学和工程应用的模拟优化，正变得越来越重要。这主要是由于这些应用所涉及的费用昂贵，需要针对样本有效的、能够有效探索Pareto前沿的多目标优化方法，以找到一组有前途的设计解决方案。本文提出了一种方法，不使用显式优化来识别Pareto前沿，而是建议搜索满足用户指定性能标准的多样化结果集。这种方法提供了一个有前途的设计决策的强大池，帮助决策者更好地了解优秀解空间。为了实现这个结果，我们引入了满足度量期望 （LMS）收益函数，分析了其行为和特性，并在各种问题上展示了其可行性。

    Efficiently solving multi-objective optimization problems for simulation optimization of important scientific and engineering applications such as materials design is becoming an increasingly important research topic. This is due largely to the expensive costs associated with said applications, and the resulting need for sample-efficient, multiobjective optimization methods that efficiently explore the Pareto frontier to expose a promising set of design solutions. We propose moving away from using explicit optimization to identify the Pareto frontier and instead suggest searching for a diverse set of outcomes that satisfy user-specified performance criteria. This method presents decision makers with a robust pool of promising design decisions and helps them better understand the space of good solutions. To achieve this outcome, we introduce the Likelihood of Metric Satisfaction (LMS) acquisition function, analyze its behavior and properties, and demonstrate its viability on various pro
    
[^46]: CeBed: 一个基于深度数据驱动的OFDM信道估计的基准测试

    CeBed: A Benchmark for Deep Data-Driven OFDM Channel Estimation. (arXiv:2306.13761v1 [cs.AI])

    [http://arxiv.org/abs/2306.13761](http://arxiv.org/abs/2306.13761)

    本文提出了一个称为CeBed的测试平台，用于评估和比较不同的数据驱动OFDM信道估计方法，解决了领域内实验条件不一致和缺乏可重复性的问题。

    

    深度学习广泛应用于无线通信问题中，包括信道估计。尽管存在许多数据驱动方法，但由于实验条件不一致和缺乏标准化的实验设计，对它们进行公正和现实的比较是困难的。此外，数据驱动方法的性能通常基于经验分析进行比较。缺乏可重复性和标准化评估工具（例如数据集、代码库）阻碍了数据驱动方法在信道估计和无线通信等领域的发展和进步。在这项工作中，我们介绍了一个建立基准测试的倡议，统一了几种数据驱动的OFDM信道估计方法。具体而言，我们提出了CeBed（信道估计测试平台），包括涵盖各种系统模型和传播条件的不同数据集，以及十个深度和传统的基线实现。本文旨在为评估和比较不同的数据驱动OFDM信道估计方法提供标准化的基准，解决领域内实验条件不一致和缺乏可重复性的问题。

    Deep learning has been extensively used in wireless communication problems, including channel estimation. Although several data-driven approaches exist, a fair and realistic comparison between them is difficult due to inconsistencies in the experimental conditions and the lack of a standardized experimental design. In addition, the performance of data-driven approaches is often compared based on empirical analysis. The lack of reproducibility and availability of standardized evaluation tools (e.g., datasets, codebases) hinder the development and progress of data-driven methods for channel estimation and wireless communication in general. In this work, we introduce an initiative to build benchmarks that unify several data-driven OFDM channel estimation approaches. Specifically, we present CeBed (a testbed for channel estimation) including different datasets covering various systems models and propagation conditions along with the implementation of ten deep and traditional baselines. Thi
    
[^47]: 基于任务驱动的图注意力在分层关系物体导航中的应用

    Task-Driven Graph Attention for Hierarchical Relational Object Navigation. (arXiv:2306.13760v1 [cs.AI])

    [http://arxiv.org/abs/2306.13760](http://arxiv.org/abs/2306.13760)

    本文研究了分层关系物体导航任务，使用场景图作为环境表示，提出了基于任务驱动图注意力模型的解决方案，可有效地探索具有长时间跨度和部分可观察性的大场景。

    

    大场景中的Embodied AI agents经常需要导航以寻找物体。本文研究了一种自然出现的物体导航任务变体，即分层关系物体导航（HRON），其目标是寻找由逻辑谓词指定的物体，这些谓词以分层结构组织，其中关于家具和房间的对象相关，例如在厨房的桌子上找到一个苹果。解决这样的任务需要一种有效的表示方法来推理对象关系并将环境和任务目标中的关系相关联。HRON在大场景（例如家庭）中特别具有挑战性，因为它具有部分可观察性和长时间跨度，这需要可以紧凑地存储过去的信息并有效地探索场景的解决方案。我们实验证明，与图像或2D地图等传统表示方法相比，场景图是最适合的表示形式。我们提出了一种解决方案，它使用场景图作为表示并使用任务驱动图注意力模型来进行有效的探索。

    Embodied AI agents in large scenes often need to navigate to find objects. In this work, we study a naturally emerging variant of the object navigation task, hierarchical relational object navigation (HRON), where the goal is to find objects specified by logical predicates organized in a hierarchical structure objects related to furniture and then to rooms - such as finding an apple on top of a table in the kitchen. Solving such a task requires an efficient representation to reason about object relations and correlate the relations in the environment and in the task goal. HRON in large scenes (e.g. homes) is particularly challenging due to its partial observability and long horizon, which invites solutions that can compactly store the past information while effectively exploring the scene. We demonstrate experimentally that scene graphs are the best-suited representation compared to conventional representations such as images or 2D maps. We propose a solution that uses scene graphs a
    
[^48]: 改善夜间或低照度城市驾驶场景的全景分割

    Improving Panoptic Segmentation for Nighttime or Low-Illumination Urban Driving Scenes. (arXiv:2306.13725v1 [cs.CV])

    [http://arxiv.org/abs/2306.13725](http://arxiv.org/abs/2306.13725)

    本文提出了两种新的方法，通过域翻译来提高全景分割在夜间或低照度城市驾驶场景中的性能和鲁棒性。

    

    自主驾驶汽车和驾驶系统使用场景解析作为了解环境的基本工具。全景分割是一种最先进的技术，在这种情况下被证明是至关重要的。近年来，基于深度学习的架构已被用于全景分割的有效和高效实现。然而，当面对黑暗、光照不足或夜间图像等恶劣条件时，现有方法的表现比白天图像的表现差。造成结果不佳的主要因素之一是缺乏足够和准确标注的城市驾驶夜间图像。在这项工作中，我们提出了两种新方法，一种是通过域翻译方法，提高全景分割在夜间或低照度城市驾驶场景中的性能和鲁棒性。所提出的方法利用CycleGAN（Zhu et al.，2017）将有现有全景注释的白天图像转换为其夜间版本。然后使用修改后的图像训练全景分割模型，该模型能够准确地在低照度场景下分割物体。

    Autonomous vehicles and driving systems use scene parsing as an essential tool to understand the surrounding environment. Panoptic segmentation is a state-of-the-art technique which proves to be pivotal in this use case. Deep learning-based architectures have been utilized for effective and efficient Panoptic Segmentation in recent times. However, when it comes to adverse conditions like dark scenes with poor illumination or nighttime images, existing methods perform poorly in comparison to daytime images. One of the main factors for poor results is the lack of sufficient and accurately annotated nighttime images for urban driving scenes. In this work, we propose two new methods, first to improve the performance, and second to improve the robustness of panoptic segmentation in nighttime or poor illumination urban driving scenes using a domain translation approach. The proposed approach makes use of CycleGAN (Zhu et al., 2017) to translate daytime images with existing panoptic annotatio
    
[^49]: 社交AI与人工智能生态系统的挑战。

    Social AI and the Challenges of the Human-AI Ecosystem. (arXiv:2306.13723v1 [cs.AI])

    [http://arxiv.org/abs/2306.13723](http://arxiv.org/abs/2306.13723)

    本文介绍了人工智能生态系统研究中出现的挑战，并探讨了社交AI提高集体问题解决能力方面的潜力，同时也需要解决新的技术和伦理问题。

    

    大规模的社会技术系统中，人与人工智能（AI）系统相互作用的崛起（包括助手和推荐系统）增加了出现集体现象和临界点的机会，并可能带来意想不到的、可能是意外的后果。例如，导航系统的建议可能会在太多的司机被引导到同一路线时造成混乱，社交媒体上的个性化推荐可能会放大极化、过滤气泡和激进化。另一方面，我们可以学习如何培育“群体智慧”和集体行动效应，以应对社会和环境挑战。为了理解人工智能对社会技术系统的影响并设计与人类合作以帮助克服社会问题的下一代人工智能，我们建议在复杂系统、网络科学和人工智能的交叉点上建立社交AI的基础。在这个角度上，我们提出了从研究人工智能生态系统中出现的挑战开始讨论社交AI提高集体问题解决能力的潜力。我们认为，社交AI可以为社会带来显著的利益，但也引发了需要解决的新的技术和伦理挑战。

    The rise of large-scale socio-technical systems in which humans interact with artificial intelligence (AI) systems (including assistants and recommenders, in short AIs) multiplies the opportunity for the emergence of collective phenomena and tipping points, with unexpected, possibly unintended, consequences. For example, navigation systems' suggestions may create chaos if too many drivers are directed on the same route, and personalised recommendations on social media may amplify polarisation, filter bubbles, and radicalisation. On the other hand, we may learn how to foster the "wisdom of crowds" and collective action effects to face social and environmental challenges. In order to understand the impact of AI on socio-technical systems and design next-generation AIs that team with humans to help overcome societal problems rather than exacerbate them, we propose to build the foundations of Social AI at the intersection of Complex Systems, Network Science and AI. In this perspective pape
    
[^50]: 用例卡：一种受欧洲AI法案启发的用例报告框架

    Use case cards: a use case reporting framework inspired by the European AI Act. (arXiv:2306.13701v1 [cs.CY])

    [http://arxiv.org/abs/2306.13701](http://arxiv.org/abs/2306.13701)

    本文提出了一种新的用例文档化框架，称为"用例卡"，旨在隐式评估AI系统的风险水平并定义相关要求。这是一种基于UML的模板和支持UML的图的组合。

    

    尽管AI社区最近努力朝着标准化程序的方向发展，以记录模型、方法、系统或数据集，但目前尚无方法专注于与欧洲AI法案（AI法案）的基于风险的方法对齐的用例。在本文中，我们基于统一标记语言（UML）标准中包括的用例建模，提出了一种新的用例文档化框架，称为"用例卡"。与其他文档方法不同，我们专注于AI系统的预期目的和操作用途。它由两个主要部分组成。第一部分是基于UML的模板，旨在隐式评估AI系统的风险水平并定义相关要求。第二个部分是一个支持UML的图，旨在提供有关系统用户交互和关系的信息。所提出的框架是涉及欧盟政策专家和学者的协同设计过程的结果。

    Despite recent efforts by the Artificial Intelligence (AI) community to move towards standardised procedures for documenting models, methods, systems or datasets, there is currently no methodology focused on use cases aligned with the risk-based approach of the European AI Act (AI Act). In this paper, we propose a new framework for the documentation of use cases, that we call "use case cards", based on the use case modelling included in the Unified Markup Language (UML) standard. Unlike other documentation methodologies, we focus on the intended purpose and operational use of an AI system. It consists of two main parts. Firstly, a UML-based template, tailored to allow implicitly assessing the risk level of the AI system and defining relevant requirements. Secondly, a supporting UML diagram designed to provide information about the system-user interactions and relationships. The proposed framework is the result of a co-design process involving a relevant team of EU policy experts and sc
    
[^51]: 拓展可持续人工智能的视角： 人工智能系统的综合可持续性标准和指标

    Broadening the perspective for sustainable AI: Comprehensive sustainability criteria and indicators for AI systems. (arXiv:2306.13686v1 [cs.CY])

    [http://arxiv.org/abs/2306.13686](http://arxiv.org/abs/2306.13686)

    本文提出了SCAIS框架，包含一组19个可持续性标准和67个指标，旨在促进和结构化关于可持续人工智能的讨论。这种跨学科方法为实现人工智能系统的可持续发展提供了基础。

    

    人工智能系统的增加使用导致了多方面的社会、环境和经济后果，包括非透明的决策过程、歧视、不平等加剧、人工智能模型的能量消耗和温室气体排放，以及经济实力的集中。本文通过考虑可持续发展的多方面性，为“可持续人工智能”的理念提供了实质性的支持。提出了SCAIS框架（人工智能系统的可持续性标准和指标），其中包含一组19个可持续性标准和67个指标，这些标准和指标基于批判性审查和专家研讨的结果。这种跨学科方法为促进和结构化关于可持续人工智能的讨论提供了独特的整体性视角。此外，它提供了一个具体框架，为AI系统的后续发展和评估打下了基础。

    The increased use of AI systems is associated with multi-faceted societal, environmental, and economic consequences. These include non-transparent decision-making processes, discrimination, increasing inequalities, rising energy consumption and greenhouse gas emissions in AI model development and application, and an increasing concentration of economic power. By considering the multi-dimensionality of sustainability, this paper takes steps towards substantiating the call for an overarching perspective on "sustainable AI". It presents the SCAIS Framework (Sustainability Criteria and Indicators for Artificial Intelligence Systems) which contains a set 19 sustainability criteria for sustainable AI and 67 indicators that is based on the results of a critical review and expert workshops. This interdisciplinary approach contributes a unique holistic perspective to facilitate and structure the discourse on sustainable AI. Further, it provides a concrete framework that lays the foundation for 
    
[^52]: 多准则决策支持的模型家族：以COVID-19为例的研究

    Model Families for Multi-Criteria Decision Support: A COVID-19 Case Study. (arXiv:2306.13683v1 [cs.AI])

    [http://arxiv.org/abs/2306.13683](http://arxiv.org/abs/2306.13683)

    本文提出了一种模型家族的思维方式，将研究任务分解为相互作用的较小模型以适应不断变化的建模目的、系统范围和映射因果关系。以COVID-19为例，这种策略带来许多优势。

    

    基于模型的决策支持在长期项目中存在诸多挑战，需要定期重新评估、建模和实现模型，以适应不断变化的建模目的、系统范围和映射因果关系。本文重新评估了20世纪90年代提出的模型家族概念，并将其作为一种创造大型研究项目决策支持框架的思维方式。我们的策略是将研究任务分解为相互作用的较小模型，这些模型专门对应于研究问题。我们通过COVID-19决策支持模型家族的示例来说明这种策略带来的许多优势。

    Continued model-based decision support is associated with particular challenges, especially in long-term projects. Due to the regularly changing questions and the often changing understanding of the underlying system, the models used must be regularly re-evaluated, -modelled and -implemented with respect to changing modelling purpose, system boundaries and mapped causalities. Usually, this leads to models with continuously growing complexity and volume. In this work we aim to reevaluate the idea of the model family, dating back to the 1990s, and use it to promote this as a mindset in the creation of decision support frameworks in large research projects. The idea is to generally not develop and enhance a single standalone model, but to divide the research tasks into interacting smaller models which specifically correspond to the research question. This strategy comes with many advantages, which we explain using the example of a family of models for decision support in the COVID-19 cris
    
[^53]: 评估基于显著性的解释方法的总体敏感性

    Evaluating the overall sensitivity of saliency-based explanation methods. (arXiv:2306.13682v1 [cs.LG])

    [http://arxiv.org/abs/2306.13682](http://arxiv.org/abs/2306.13682)

    本文研究了如何生成对“黑匣子”深度学习模型的忠实解释，提出了一个扩展的测试方法来确定解释方法的总体敏感性，并通过例子展示了如何使用这个方法比较卷积神经网络的多个解释方法。

    

    我们着眼于生成对"黑匣子"深度学习模型的忠实解释的需求。已经提出了几种测试来确定解释方法的忠实程度，但它们缺乏跨领域的适用性和严谨的方法论。因此，我们选择了一个现有的不依赖特定模型，并且非常适合比较多个解释方法的忠实程度的测试，并通过指定正式的阈值和建立标准来扩展它，以确定解释方法的总体敏感性。我们通过例子展示了如何使用这个扩展方法来比较卷积神经网络的多个解释方法。最后，我们讨论了敏感性和忠实程度之间的关系，并考虑如何调整测试来评估其他领域的不同解释方法。

    We address the need to generate faithful explanations of "black box" Deep Learning models. Several tests have been proposed to determine aspects of faithfulness of explanation methods, but they lack cross-domain applicability and a rigorous methodology. Hence, we select an existing test that is model agnostic and is well-suited for comparing one aspect of faithfulness (i.e., sensitivity) of multiple explanation methods, and extend it by specifying formal thresh-olds and building criteria to determine the over-all sensitivity of the explanation method. We present examples of how multiple explanation methods for Convolutional Neural Networks can be compared using this extended methodology. Finally, we discuss the relationship between sensitivity and faithfulness and consider how the test can be adapted to assess different explanation methods in other domains.
    
[^54]: 基于GPT的模型遇见仿真：如何有效地应用大规模预训练语言模型于仿真任务

    GPT-Based Models Meet Simulation: How to Efficiently Use Large-Scale Pre-Trained Language Models Across Simulation Tasks. (arXiv:2306.13679v1 [cs.HC])

    [http://arxiv.org/abs/2306.13679](http://arxiv.org/abs/2306.13679)

    本文是关于大规模预训练语言模型在科学仿真中的研究。研究关注四个建模和仿真任务的LLMs的预期益处和限制，并提供实际指导。

    

    大规模预训练语言模型（LLMs），如ChatGPT或GPT-4所提供的颠覆性技术，在多个应用领域引起了广泛关注，通常强调高水平的机会和担忧。本文是关于LLMs在科学仿真中应用的第一篇研究。我们关注四个建模和仿真任务，每次评估LLMs的预期益处和限制，同时为模型构建者提供实际指导。第一个任务旨在解释概念模型的结构，以促进参与者在建模过程中的参与。第二个任务专注于汇总仿真输出，以便模型用户能够识别出优选场景。第三个任务旨在通过文本传达对仿真可视化的见解，以扩大仿真平台的可访问性。最后，最后一个任务引出了使用LLMs解释仿真错误和问题的可能性，以便模型开发者更有效地解决这些问题。

    The disruptive technology provided by large-scale pre-trained language models (LLMs) such as ChatGPT or GPT-4 has received significant attention in several application domains, often with an emphasis on high-level opportunities and concerns. This paper is the first examination regarding the use of LLMs for scientific simulations. We focus on four modeling and simulation tasks, each time assessing the expected benefits and limitations of LLMs while providing practical guidance for modelers regarding the steps involved. The first task is devoted to explaining the structure of a conceptual model to promote the engagement of participants in the modeling process. The second task focuses on summarizing simulation outputs, so that model users can identify a preferred scenario. The third task seeks to broaden accessibility to simulation platforms by conveying the insights of simulation visualizations via text. Finally, the last task evokes the possibility of explaining simulation errors and pr
    
[^55]: 什么推动了人工智能技术的接受？期望和经验的作用。

    What drives the acceptance of AI technology?: the role of expectations and experiences. (arXiv:2306.13670v1 [cs.CY])

    [http://arxiv.org/abs/2306.13670](http://arxiv.org/abs/2306.13670)

    人们接受人工智能的意愿很大程度上受当前人工智能产品和服务的经验、对人工智能的期望和过去的ICT技术经验的影响。人工智能经验和ICT经验以两种方式影响人工智能接受意愿，以直接途径和间接途径，而对人工智能的期望值则可以进一步提高人工智能的接受意愿。

    

    近年来，人工智能产品和服务已作为试点向潜在用户提供。当前人工智能产品和服务的经验、对人工智能的期望、以及过去的ICT技术经验都在很大程度上影响着人们接受人工智能的意愿。该研究旨在探究影响人工智能接受意愿的因素，并理解其形成过程。研究结果表明，人工智能经验和过去的ICT经验以两种方式影响人工智能接受意愿。通过直接途径，更高的人工智能经验和ICT经验与更大的接受人工智能的意愿有关。此外，还存在一种间接途径，即人工智能经验和ICT经验有助于增加对人工智能的期望值，而这些期望值反过来提高了接受人工智能的意愿。根据研究结果，为计划实施人工智能的公司和公共组织提出了几项建议。

    In recent years, Artificial intelligence products and services have been offered potential users as pilots. The acceptance intention towards artificial intelligence is greatly influenced by the experience with current AI products and services, expectations for AI, and past experiences with ICT technology. This study aims to explore the factors that impact AI acceptance intention and understand the process of its formation. The analysis results of this study reveal that AI experience and past ICT experience affect AI acceptance intention in two ways. Through the direct path, higher AI experience and ICT experience are associated with a greater intention to accept AI. Additionally, there is an indirect path where AI experience and ICT experience contribute to increased expectations for AI, and these expectations, in turn, elevate acceptance intention. Based on the findings, several recommendations are suggested for companies and public organizations planning to implement artificial intel
    
[^56]: 统计关系学习和神经符号人工智能：一阶逻辑提供了什么？

    Statistical relational learning and neuro-symbolic AI: what does first-order logic offer?. (arXiv:2306.13660v1 [cs.AI])

    [http://arxiv.org/abs/2306.13660](http://arxiv.org/abs/2306.13660)

    本文旨在探讨使用一阶逻辑来表示知识的方式，对机器学习和逻辑专家都有指导作用，同时也对统计关系学习和神经符号AI领域有重要的理论意义。

    

    本文的目的是以非技术的方式简要概述和阐述使用（一阶）逻辑来表示（概率）知识的逻辑和哲学基础。我们的动机有三个。首先，对于机器学习研究人员，了解为什么研究社区关注关系表示的文章可以作为一个温和的介绍。其次，对于逻辑专家是新来的学习领域，这样一篇文章可以帮助导航有限与无限之间的差异，主观概率与随机世界语义。最后，对于统计关系学习和神经符号AI的研究人员，他们通常嵌入在具有主观概率的有限世界中，欣赏无限领域和随机世界语义所带来的理论价值至关重要。

    In this paper, our aim is to briefly survey and articulate the logical and philosophical foundations of using (first-order) logic to represent (probabilistic) knowledge in a non-technical fashion. Our motivation is three fold. First, for machine learning researchers unaware of why the research community cares about relational representations, this article can serve as a gentle introduction. Second, for logical experts who are newcomers to the learning area, such an article can help in navigating the differences between finite vs infinite, and subjective probabilities vs random-world semantics. Finally, for researchers from statistical relational learning and neuro-symbolic AI, who are usually embedded in finite worlds with subjective probabilities, appreciating what infinite domains and random-world semantics brings to the table is of utmost theoretical import.
    
[^57]: 实现公平和偏见的逻辑理论。

    Toward A Logical Theory Of Fairness and Bias. (arXiv:2306.13659v1 [cs.AI])

    [http://arxiv.org/abs/2306.13659](http://arxiv.org/abs/2306.13659)

    本文通过对公平性定义进行形式化重构，将其应用基于认知环境模型，提出了实现公平和偏见理论，包括三个概念：通过无意识来实现公平、人口学平等和反事实公正。

    

    机器学习中的公平性因算法在历史数据上的训练而放大和延续历史偏差，近年来备受关注。本文提出了对公平性定义的形式重构，不是为了取代现有的定义，而是为了将它们的应用基于认知环境模型，从而允许进行丰富的环境建模。因此，我们探讨了三个概念：通过无意识来实现公平、人口学平等和反事实公正，并将它们在认知情况演算中进行了形式化。

    Fairness in machine learning is of considerable interest in recent years owing to the propensity of algorithms trained on historical data to amplify and perpetuate historical biases. In this paper, we argue for a formal reconstruction of fairness definitions, not so much to replace existing definitions but to ground their application in an epistemic setting and allow for rich environmental modelling. Consequently we look into three notions: fairness through unawareness, demographic parity and counterfactual fairness, and formalise these in the epistemic situation calculus.
    
[^58]: 论共享意图的计算机制以及有关理性和意识的推测

    On Computational Mechanisms for Shared Intentionality, and Speculation on Rationality and Consciousness. (arXiv:2306.13657v1 [cs.AI])

    [http://arxiv.org/abs/2306.13657](http://arxiv.org/abs/2306.13657)

    本文提出了一种共享意图优先的理论，探讨了支持计算机代理人之间共享意图的基本机制所必须具备的特征，并探索了这些机制如何适用于人类以提供对人类理性和意识的解释。

    

    人类独特的特征之一是我们能够进行新颖的、合作的行为或团队合作。这需要我们能够在个体之间传达目标、计划和思想，以创建共享意图。本文利用David Marr的信息处理模型，推导出支持计算机代理人之间共享意图的基本机制所必须具备的特征，并指出这些特征如何在现有的基于人工智能的机器人中实现。此外，本文提出这个思维实验所得到的机制也适用于人类，并进一步提供了与观察相符的关于人类理性、意向和感知意识的解释。这样就形成了作者所称的共享意图优先理论（SIFT）。

    A singular attribute of humankind is our ability to undertake novel, cooperative behavior, or teamwork. This requires that we can communicate goals, plans, and ideas between the brains of individuals to create shared intentionality. Using the information processing model of David Marr, I derive necessary characteristics of basic mechanisms to enable shared intentionality between computational agents and indicate how these could be implemented in present-day AI-based robots.  More speculatively, I suggest the mechanisms derived by this thought experiment apply to humans and extend to provide explanations for human rationality and aspects of intentional and phenomenal consciousness that accord with observation. This yields what I call the Shared Intentionality First Theory (SIFT) for rationality and consciousness.
    
[^59]: 基于剪枝的域泛化方法研究

    Pruning for Better Domain Generalizability. (arXiv:2306.13237v1 [cs.LG])

    [http://arxiv.org/abs/2306.13237](http://arxiv.org/abs/2306.13237)

    本文研究了基于剪枝的域泛化方法，提出了一种新的剪枝评分方法DSS，该方法不是为了保持源准确性，而是直接增强模型的鲁棒性。实验证明该方法可以与最先进的泛化方法结合使用，即便只引入少量稀疏也能显著提高模型性能。

    

    本文探讨了使用剪枝作为一种可靠的方法来提高模型的泛化能力。我们发现现有的剪枝方法，如L2已经可以在目标域性能上提供小幅度的改善。我们进一步提出了一种新的剪枝评分方法，称为DSS，设计不是为了保持源准确性而是直接增强模型的鲁棒性。我们进行了实证实验来验证我们的方法，并证明它甚至可以与MIRO(Cha等人，2022年)等最先进的泛化方法结合使用，进一步提高性能。在MNIST到MNIST-M上，通过将60%通道稀疏引入模型，我们可以将基线性能提高5个百分点以上。在DomainBed基准和最先进的MIRO上，仅通过将10%稀疏引入模型，我们就可以进一步提高其性能。代码可在以下链接找到: https://github.com/AlexSunNik/Pruning-for-Better-Domain-Generaliza

    In this paper, we investigate whether we could use pruning as a reliable method to boost the generalization ability of the model. We found that existing pruning method like L2 can already offer small improvement on the target domain performance. We further propose a novel pruning scoring method, called DSS, designed not to maintain source accuracy as typical pruning work, but to directly enhance the robustness of the model. We conduct empirical experiments to validate our method and demonstrate that it can be even combined with state-of-the-art generalization work like MIRO(Cha et al., 2022) to further boost the performance. On MNIST to MNIST-M, we could improve the baseline performance by over 5 points by introducing 60% channel sparsity into the model. On DomainBed benchmark and state-of-the-art MIRO, we can further boost its performance by 1 point only by introducing 10% sparsity into the model. Code can be found at: https://github.com/AlexSunNik/Pruning-for-Better-Domain-Generaliza
    
[^60]: 可微分决策树是否能够学习可解释的奖励函数?

    Can Differentiable Decision Trees Learn Interpretable Reward Functions?. (arXiv:2306.13004v1 [cs.LG])

    [http://arxiv.org/abs/2306.13004](http://arxiv.org/abs/2306.13004)

    本文提出了使用可微分决策树从人类偏好中学习具有表达能力和可解释性的奖励函数，通过在多个环境上的评估，发现该方法能够学习到可解释的奖励函数，但在强化学习测试时表现受到树的离散性的影响。

    

    学习人的意图和偏好的奖励函数越来越受到关注，但许多框架使用黑盒学习方法，难以解释。本文提出并评估了一种新颖方法，使用可微分决策树（DDT）从偏好中学习具有表达能力和可解释性的奖励函数，适用于低维和高维状态输入。我们在Cartpole、视觉网格世界环境和Atari游戏上评估了我们的算法，探讨了使用DDT学习可解释奖励函数的可行性。我们提供证据表明，学习到的奖励函数的树形结构有助于确定奖励函数与人类偏好的一致程度。我们可视化了学习到的奖励DDT，发现它们能够学习可解释的奖励函数，但树的离散性会影响强化学习在测试时的表现。

    There is an increasing interest in learning reward functions that model human intent and human preferences. However, many frameworks use blackbox learning methods that, while expressive, are difficult to interpret. We propose and evaluate a novel approach for learning expressive and interpretable reward functions from preferences using Differentiable Decision Trees (DDTs) for both low- and high-dimensional state inputs. We explore and discuss the viability of learning interpretable reward functions using DDTs by evaluating our algorithm on Cartpole, Visual Gridworld environments, and Atari games. We provide evidence that that the tree structure of our learned reward function is useful in determining the extent to which a reward function is aligned with human preferences. We visualize the learned reward DDTs and find that they are capable of learning interpretable reward functions but that the discrete nature of the trees hurts the performance of reinforcement learning at test time. How
    
[^61]: 人工智能灾难性风险综述

    An Overview of Catastrophic AI Risks. (arXiv:2306.12001v1 [cs.CY])

    [http://arxiv.org/abs/2306.12001](http://arxiv.org/abs/2306.12001)

    本文综述了人工智能灾难性风险的四个主要来源，包括恶意使用、人工智能竞赛、组织风险和流氓人工智能。

    

    人工智能的快速发展引起了专家、政策制定者和世界各国领导人对越来越先进的人工智能系统可能带来灾难性风险的担忧。虽然已经有很多风险被单独详细介绍过，但迫切需要系统地讨论和说明潜在危险，以更好地支持减轻这些风险的努力。本文概述了人工智能灾难性风险的主要来源，我们将其分为四个类别：恶意使用，即个人或团体有意使用人工智能造成伤害；人工智能竞赛，即竞争环境促使行动者部署不安全的人工智能或放弃控制权交给人工智能；组织风险，突出人为和复杂系统如何增加灾难性事故发生的可能性；以及流氓人工智能，描述了控制比人类智能更高的代理程序困难的固有难题。对于每个风险类别，我们描述了具体的危害。

    Rapid advancements in artificial intelligence (AI) have sparked growing concerns among experts, policymakers, and world leaders regarding the potential for increasingly advanced AI systems to pose catastrophic risks. Although numerous risks have been detailed separately, there is a pressing need for a systematic discussion and illustration of the potential dangers to better inform efforts to mitigate them. This paper provides an overview of the main sources of catastrophic AI risks, which we organize into four categories: malicious use, in which individuals or groups intentionally use AIs to cause harm; AI race, in which competitive environments compel actors to deploy unsafe AIs or cede control to AIs; organizational risks, highlighting how human factors and complex systems can increase the chances of catastrophic accidents; and rogue AIs, describing the inherent difficulty in controlling agents far more intelligent than humans. For each category of risk, we describe specific hazards,
    
[^62]: 图分类问题中结构感知的鲁棒性认证

    Structure-Aware Robustness Certificates for Graph Classification. (arXiv:2306.11915v1 [cs.LG])

    [http://arxiv.org/abs/2306.11915](http://arxiv.org/abs/2306.11915)

    该论文提出了一种新的随机平滑方法，可以根据图的不同预定义结构生成对应的鲁棒性证书，从而在多个基准图分类任务中取得领先的对抗性攻击下鲁棒性结果。

    

    对于基于图的机器学习模型进行鲁棒性认证是保证安全性的一个至关重要的挑战。目前用于图分类器的鲁棒性证明保证与节点对翻转（添加或删除边缘）的总数有关，这相当于以邻接矩阵为中心的l0球。尽管从理论上看很有吸引力，但这种各向同性的结构噪声在实际场景中可能过于严格，因为有些节点对于确定分类器的输出更为关键。在这种情况下，证书给出了对图模型鲁棒性的悲观描述。为了解决这个问题，我们开发了一种基于随机平滑的方法，将非各向同性的噪声分布添加到输入图结构中。我们展示了我们的过程为分类器生成了结构感知的证书，因此鲁棒性证书的大小可以在图的不同预定义结构之间变化。我们在几个基准图分类任务上展示了我们方法的优势，在对抗性攻击的鲁棒性方面取得了最先进的结果。

    Certifying the robustness of a graph-based machine learning model poses a critical challenge for safety. Current robustness certificates for graph classifiers guarantee output invariance with respect to the total number of node pair flips (edge addition or edge deletion), which amounts to an $l_{0}$ ball centred on the adjacency matrix. Although theoretically attractive, this type of isotropic structural noise can be too restrictive in practical scenarios where some node pairs are more critical than others in determining the classifier's output. The certificate, in this case, gives a pessimistic depiction of the robustness of the graph model. To tackle this issue, we develop a randomised smoothing method based on adding an anisotropic noise distribution to the input graph structure. We show that our process generates structural-aware certificates for our classifiers, whereby the magnitude of robustness certificates can vary across different pre-defined structures of the graph. We demon
    
[^63]: 学习从所有训练样本中累积证据：理论与实践

    Learn to Accumulate Evidence from All Training Samples: Theory and Practice. (arXiv:2306.11113v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.11113](http://arxiv.org/abs/2306.11113)

    本文提出了一种新的激活函数，All-Positive (AP)激活，避免了现有证据激活函数中的零证据区域，同时能够更好地表达负证据量。实验证明，该方法在多个基准数据集上优于现有方法。

    

    基于置信度理论和主观逻辑的证据深度学习提供了一种原则性和计算效率高的方法，用于将确定性神经网络变为不确定性感知的模型。结果产生的证据模型可以使用学习到的证据量化细粒度不确定性。为了确保理论上合理的证据模型，证据需要是非负的，这需要使用特殊的激活函数进行模型训练和推断。这个限制通常导致预测性能不如标准的softmax模型，使得将它们扩展到许多大规模数据集具有挑战性。为了揭示这种不良行为的真正原因，我们在理论上研究证据模型，并确定了一个根本限制，它解释了不良性能：现有的证明激活函数创建了零证据区域，这阻止了模型从落入这些区域的训练样本中学习。对证据激活函数的深入分析揭示了它们可以被视为将原始输入转换为证据量，然后使用信念更新规则进行聚合的转换器。受这一观察的启发，我们提出了一种新的激活函数，称为全正激活（All-Positive，AP），它通过构造避免零证据区域，优于现有的证据激活函数在许多基准数据集上。此外，我们表明AP激活减少了ReLU激活，从而恢复了ReLU的良好正定性质，并且它的泛化允许负证据量，因此比现有的证据激活更具表现力。实验证明，我们提出的方法在几个具有挑战性的基准数据集上实现了有竞争力的性能，并优于现有方法。

    Evidential deep learning, built upon belief theory and subjective logic, offers a principled and computationally efficient way to turn a deterministic neural network uncertainty-aware. The resultant evidential models can quantify fine-grained uncertainty using the learned evidence. To ensure theoretically sound evidential models, the evidence needs to be non-negative, which requires special activation functions for model training and inference. This constraint often leads to inferior predictive performance compared to standard softmax models, making it challenging to extend them to many large-scale datasets. To unveil the real cause of this undesired behavior, we theoretically investigate evidential models and identify a fundamental limitation that explains the inferior performance: existing evidential activation functions create zero evidence regions, which prevent the model to learn from training samples falling into such regions. A deeper analysis of evidential activation functions 
    
[^64]: 神经启动技术用于小样本自适应

    Neural Priming for Sample-Efficient Adaptation. (arXiv:2306.10191v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.10191](http://arxiv.org/abs/2306.10191)

    本文提出神经启动技术，用于使大型预训练模型适应于分布变化和下游任务，无需过多标记样本。通过回忆数据进行轻量更新可以显著提高准确性。

    

    本文提出神经启动技术，用于在未经过大量标记样本的情况下，使大型预训练模型适应于分布变化和下游任务。在给定类名或无标签测试样本时，神经启动可以使模型回忆起预训练期间看到的相关数据并以此为基础条件化其参数，从而使其针对测试分布做好准备。神经启动还可以在测试时进行，即使是针对如LAION-2B这样大型预训练数据集。在各种分布变化和迁移学习基准测试中，对回忆数据进行轻量更新可以显著提高准确性。具体而言，在零样本设置下，我们看到ImageNet的准确性提高了2.45％，在标准的迁移学习基准测试中平均准确性提高了3.81％。此外，在推理时使用神经启动来适应分布变化，我们看到ImageNetV2的准确性提高了1.41％。这些结果证明了神经启动在处理小样本自适应挑战中的有效性。

    We propose Neural Priming, a technique for adapting large pretrained models to distribution shifts and downstream tasks given few or no labeled examples. Presented with class names or unlabeled test samples, Neural Priming enables the model to recall and conditions its parameters on relevant data seen throughout pretraining, thereby priming it for the test distribution. Neural Priming can be performed at test time, even for pretraining datasets as large as LAION-2B. Performing lightweight updates on the recalled data significantly improves accuracy across a variety of distribution shift and transfer learning benchmarks. Concretely, in the zero-shot setting, we see a 2.45% improvement in accuracy on ImageNet and 3.81% accuracy improvement on average across standard transfer learning benchmarks. Further, using Neural Priming at inference to adapt to distribution shift, we see a 1.41% accuracy improvement on ImageNetV2. These results demonstrate the effectiveness of Neural Priming in addr
    
[^65]: 文本到图像扩散模型中的能量交叉注意力用于贝叶斯上下文更新

    Energy-Based Cross Attention for Bayesian Context Update in Text-to-Image Diffusion Models. (arXiv:2306.09869v1 [cs.CV])

    [http://arxiv.org/abs/2306.09869](http://arxiv.org/abs/2306.09869)

    本论文提出了能量交叉注意力的EBM框架，通过更新和转移上下文向量，隐式最小化能量函数的嵌套层次，优化文本到图像扩散模型的语义对齐问题，实现零样本组合生成。

    

    尽管文本到图像扩散模型在图像生成任务中表现出色，但最近的研究提出了一个问题，即生成的图像有时无法捕捉到文本提示的预期语义内容，这种现象通常被称为语义错位。为了解决这个问题，我们提出了一种新颖的基于能量的模型（EBM）框架。具体而言，我们首先在去噪自编码器的每个交叉注意力层中制定潜在图像表示和文本嵌入的EBM。然后，我们获得上下文向量的对数后验梯度，可以更新和转移到后续的交叉注意力层，从而隐式地最小化嵌套层次的能量函数。我们的潜在EBMs还允许零样本组合生成，即通过不同上下文的交叉注意力输出的线性组合。通过大量实验，我们证明了所提出的方法在处理各种图像生成任务方面非常有效，并可以显著降低文本提示和生成图像之间的语义错位现象。

    Despite the remarkable performance of text-to-image diffusion models in image generation tasks, recent studies have raised the issue that generated images sometimes cannot capture the intended semantic contents of the text prompts, which phenomenon is often called semantic misalignment. To address this, here we present a novel energy-based model (EBM) framework. Specifically, we first formulate EBMs of latent image representations and text embeddings in each cross-attention layer of the denoising autoencoder. Then, we obtain the gradient of the log posterior of context vectors, which can be updated and transferred to the subsequent cross-attention layer, thereby implicitly minimizing a nested hierarchy of energy functions. Our latent EBMs further allow zero-shot compositional generation as a linear combination of cross-attention outputs from different contexts. Using extensive experiments, we demonstrate that the proposed method is highly effective in handling various image generation 
    
[^66]: TSMixer: 用于多元时间序列预测的轻量级MLP-Mixer模型

    TSMixer: Lightweight MLP-Mixer Model for Multivariate Time Series Forecasting. (arXiv:2306.09364v1 [cs.LG])

    [http://arxiv.org/abs/2306.09364](http://arxiv.org/abs/2306.09364)

    TSMixer是一种用于多元时间序列预测的轻量级MLP-Mixer模型，可以有效地捕捉时间序列属性并在准确性方面超越了Transformers的方法。

    

    Transformers因其能够捕捉长序列交互而在时间序列预测中备受青睐。然而，其内存和计算要求高的问题对长期预测构成了严重瓶颈。为了解决这一问题，我们提出了TSMixer，这是一种轻量级神经架构，专为多元预测和补丁时间序列表示学习而设计，是Transformers的有效替代。我们的模型借鉴了MLP-Mixer模型在计算机视觉中的成功经验。我们展示了将视觉MLP-Mixer适应于时间序列的挑战，并引入了经过实验证实的组件以提高准确性。这包括一种新的设计范式，即将在线协调头附加到MLP-Mixer骨干上，以显式地建模时间序列的属性，如层次结构和通道相关性。我们还提出了一种混合通道建模方法，平衡了编码多个时间序列通道和保留单个通道信息之间的权衡。我们的实验表明，TSMixer在一元和多元时间序列预测任务中均实现了最先进的性能，同时需要比基于Transformers的方法少得多的参数。

    Transformers have gained popularity in time series forecasting for their ability to capture long-sequence interactions. However, their high memory and computing requirements pose a critical bottleneck for long-term forecasting. To address this, we propose TSMixer, a lightweight neural architecture exclusively composed of multi-layer perceptron (MLP) modules. TSMixer is designed for multivariate forecasting and representation learning on patched time series, providing an efficient alternative to Transformers. Our model draws inspiration from the success of MLP-Mixer models in computer vision. We demonstrate the challenges involved in adapting Vision MLP-Mixer for time series and introduce empirically validated components to enhance accuracy. This includes a novel design paradigm of attaching online reconciliation heads to the MLP-Mixer backbone, for explicitly modeling the time-series properties such as hierarchy and channel-correlations. We also propose a Hybrid channel modeling approa
    
[^67]: 使用大型语言模型探索MIT数学和EECS课程

    Exploring the MIT Mathematics and EECS Curriculum Using Large Language Models. (arXiv:2306.08997v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2306.08997](http://arxiv.org/abs/2306.08997)

    通过使用大型语言模型，翻译了MIT数学和EECS课程中的4550个题目，开发出一个可以自动评分的模型，并探索了课程、问题和答案之间的关系。

    

    我们整理了一个综合性数据集，包括了获取学位所需的所有MIT数学和电气工程及计算机科学（EECS）课程的题目集、期中考试和期末考试中的4550个问题和解决方案。我们评估了大型语言模型实现任何MIT数学和EECS专业毕业要求的能力。我们的结果表明，GPT-3.5成功解决了整个MIT课程的三分之一，而GPT-4在题目中不包含图像的测试集上经过提示工程后达到了完美的解决率。我们在此数据集上对开源大型语言模型进行了微调。我们采用GPT-4自动评分，提供了课程、问题和答案类型的详细性能分析。通过将问题嵌入低维空间，我们探索了问题、主题和课程之间的关系，并发现哪些问题和课程是解决其他问题和课程所必需的。

    We curate a comprehensive dataset of 4,550 questions and solutions from problem sets, midterm exams, and final exams across all MIT Mathematics and Electrical Engineering and Computer Science (EECS) courses required for obtaining a degree. We evaluate the ability of large language models to fulfill the graduation requirements for any MIT major in Mathematics and EECS. Our results demonstrate that GPT-3.5 successfully solves a third of the entire MIT curriculum, while GPT-4, with prompt engineering, achieves a perfect solve rate on a test set excluding questions based on images. We fine-tune an open-source large language model on this dataset. We employ GPT-4 to automatically grade model responses, providing a detailed performance breakdown by course, question, and answer type. By embedding questions in a low-dimensional space, we explore the relationships between questions, topics, and classes and discover which questions and classes are required for solving other questions and classes
    
[^68]: NeuroGraph:面向脑连接组学的图机器学习基准测试

    NeuroGraph: Benchmarks for Graph Machine Learning in Brain Connectomics. (arXiv:2306.06202v1 [cs.LG])

    [http://arxiv.org/abs/2306.06202](http://arxiv.org/abs/2306.06202)

    本文介绍了神经成像领域的图机器学习基准测试NeuroGraph，并探讨了数据集生成的搜索空间。

    

    机器学习为分析高维功能性神经成像数据提供了有价值的工具，已被证明对预测各种神经疾病、精神障碍和认知模式有效。在功能磁共振成像研究中，大脑区域之间的相互作用通常使用基于图的表示进行建模。图机器学习方法的有效性已在多个领域得到证实，标志着数据解释和预测建模中的一个转变步骤。然而，尽管有前景，但由于图形数据集构建的广泛预处理流水线和大参数搜索空间，在神经成像领域中应用这些技术的转换仍然受到意外的限制。本文介绍了NeuroGraph(一个基于图的神经成像数据集)，它涵盖了多个行为和认知特征类别。我们深入探讨了数据集生成搜索空间

    Machine learning provides a valuable tool for analyzing high-dimensional functional neuroimaging data, and is proving effective in predicting various neurological conditions, psychiatric disorders, and cognitive patterns. In functional Magnetic Resonance Imaging (MRI) research, interactions between brain regions are commonly modeled using graph-based representations. The potency of graph machine learning methods has been established across myriad domains, marking a transformative step in data interpretation and predictive modeling. Yet, despite their promise, the transposition of these techniques to the neuroimaging domain remains surprisingly under-explored due to the expansive preprocessing pipeline and large parameter search space for graph-based datasets construction. In this paper, we introduce NeuroGraph, a collection of graph-based neuroimaging datasets that span multiple categories of behavioral and cognitive traits. We delve deeply into the dataset generation search space by c
    
[^69]: 为抓住任何物品铺平道路：基于迁移学习的通用抓取放置机器人模型

    Pave the Way to Grasp Anything: Transferring Foundation Models for Universal Pick-Place Robots. (arXiv:2306.05716v1 [cs.RO])

    [http://arxiv.org/abs/2306.05716](http://arxiv.org/abs/2306.05716)

    本研究提出了一种基于语言分割掩模的新方法，用于解决通用型机器人的泛化能力问题，提高了在开放域场景中新对象的抓取操作的学习效率和推广效果。

    

    提高通用型机器人的泛化能力一直是研究社区长期追求的重要挑战。现有的方法通常依赖于收集大规模现实世界机器人数据，如 RT-1 数据集。然而，这些方法通常效率低下，限制了它们在具有新对象和多样背景的开放域场景中的能力。本文提出了一种新的范例，有效地利用最先进的基础模型生成的基于语言的分割掩模，以解决日常场景中广泛的拾放机器人操作任务。通过将掩模传达的精确语义和几何形状集成到我们的多视角策略模型中，我们的方法可以感知准确的物体姿态并实现高效学习，同时也有助于有效的新对象的推广。我们的方法同时可以实现在训练时观察到相似形状的新物体的抓取操作。

    Improving the generalization capabilities of general-purpose robotic agents has long been a significant challenge actively pursued by research communities. Existing approaches often rely on collecting large-scale real-world robotic data, such as the RT-1 dataset. However, these approaches typically suffer from low efficiency, limiting their capability in open-domain scenarios with new objects, and diverse backgrounds. In this paper, we propose a novel paradigm that effectively leverages language-grounded segmentation masks generated by state-of-the-art foundation models, to address a wide range of pick-and-place robot manipulation tasks in everyday scenarios. By integrating precise semantics and geometries conveyed from masks into our multi-view policy model, our approach can perceive accurate object poses and enable sample-efficient learning. Besides, such design facilitates effective generalization for grasping new objects with similar shapes observed during training. Our approach co
    
[^70]: FheFL：支持完全同态加密的隐私保护联邦学习与拜占庭用户

    FheFL: Fully Homomorphic Encryption Friendly Privacy-Preserving Federated Learning with Byzantine Users. (arXiv:2306.05112v1 [cs.AI])

    [http://arxiv.org/abs/2306.05112](http://arxiv.org/abs/2306.05112)

    本论文介绍了一种新的联邦学习算法，采用FHE加密技术，既可以保护模型更新的隐私，又可以防止恶意用户破坏全局模型。

    

    联邦学习（FL）技术最初是为了缓解传统机器学习范式中可能出现的数据隐私问题而开发的。尽管FL确保用户的数据始终保留在用户手中，但局部训练模型的梯度必须与集中式服务器通信以构建全局模型。这导致隐私泄露，使得服务器可以从共享的梯度中推断出用户数据的私密信息。为了缓解这一缺陷，下一代FL架构提出了加密和匿名化技术，以保护模型更新免受服务器的攻击。然而，这种方法会带来其他挑战，例如恶意用户可能通过共享虚假梯度来破坏全局模型。由于梯度被加密，服务器无法识别和排除不良用户以保护全局模型。因此，为了缓解这两种攻击，本文提出了一种基于完全同态加密（FHE）的新方案。

    The federated learning (FL) technique was initially developed to mitigate data privacy issues that can arise in the traditional machine learning paradigm. While FL ensures that a user's data always remain with the user, the gradients of the locally trained models must be communicated with the centralized server to build the global model. This results in privacy leakage, where the server can infer private information of the users' data from the shared gradients. To mitigate this flaw, the next-generation FL architectures proposed encryption and anonymization techniques to protect the model updates from the server. However, this approach creates other challenges, such as a malicious user might sabotage the global model by sharing false gradients. Since the gradients are encrypted, the server is unable to identify and eliminate rogue users which would protect the global model. Therefore, to mitigate both attacks, this paper proposes a novel fully homomorphic encryption (FHE) based scheme 
    
[^71]: ChatGPT信息的图神经网络用于股票价格预测

    ChatGPT Informed Graph Neural Network for Stock Movement Prediction. (arXiv:2306.03763v1 [q-fin.ST])

    [http://arxiv.org/abs/2306.03763](http://arxiv.org/abs/2306.03763)

    该研究介绍了一种新的框架，利用ChatGPT技术增强图神经网络，能够从财经新闻中提取出不断变化的网络结构，并用于股票价格预测，获得了超过基于深度学习的最新基准的表现，提示了ChatGPT在文本推断和金融预测方面的潜力。

    

    ChatGPT已在各种自然语言处理（NLP）任务中展示了出色的能力。然而，它从时间文本数据（尤其是财经新闻）推断动态网络结构的潜力仍是一个未开发的领域。在这项研究中，我们介绍了一个新的框架，利用ChatGPT的图推断能力来增强图神经网络（GNN）。我们的框架巧妙地从文本数据中提取出不断变化的网络结构，并将这些网络结构融合到图神经网络中，进行后续的预测任务。股票价格预测的实验结果表明，我们的模型始终优于基于深度学习的最新基准。此外，基于我们模型的产出构建的组合展示出更高的年化累计回报、更低的波动性和最大回撤。这种卓越表现突显了ChatGPT用于基于文本的网络推断和金融预测应用的潜力。

    ChatGPT has demonstrated remarkable capabilities across various natural language processing (NLP) tasks. However, its potential for inferring dynamic network structures from temporal textual data, specifically financial news, remains an unexplored frontier. In this research, we introduce a novel framework that leverages ChatGPT's graph inference capabilities to enhance Graph Neural Networks (GNN). Our framework adeptly extracts evolving network structures from textual data, and incorporates these networks into graph neural networks for subsequent predictive tasks. The experimental results from stock movement forecasting indicate our model has consistently outperformed the state-of-the-art Deep Learning-based benchmarks. Furthermore, the portfolios constructed based on our model's outputs demonstrate higher annualized cumulative returns, alongside reduced volatility and maximum drawdown. This superior performance highlights the potential of ChatGPT for text-based network inferences and 
    
[^72]: SQL-PaLM：针对Text-to-SQL的改进大语言模型适应性

    SQL-PaLM: Improved Large Language Model Adaptation for Text-to-SQL. (arXiv:2306.00739v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2306.00739](http://arxiv.org/abs/2306.00739)

    本文提出了一种基于大语言模型的Text-to-SQL模型SQL-PaLM，使用了面向Text-to-SQL的基于执行的自一致提示方法，在Spider上实现了77.3%的测试套件准确度，并显着超越以前的最新技术的方法。

    

    大语言模型（LLMs）的一个令人印象深刻的新兴功能是生成代码，包括用于数据库的结构化查询语言（SQL）。对于将自然语言文本转换为SQL查询的任务，即Text-to-SQL，LLMs的适应性至关重要，具体取决于使用的适应性数据量。本文提出了一种基于LLM的Text-to-SQL模型SQL-PaLM，利用了PaLM-2，推动了两种设置的最新进展。Few-shot SQL-PaLM基于面向Text-to-SQL的基于执行的自一致提示方法，可在Spider上实现77.3%的测试套件准确度，据我们所知，这是第一个通过显着较大的微调超越以前的最新技术的方法。此外，我们证明经过精细调整的SQL-PALM可进一步提高1%的性能。为了将SQL-PaLM应用于实际场景，我们进一步评估了其对其他挑战的稳健性。

    One impressive emergent capability of large language models (LLMs) is generation of code, including Structured Query Language (SQL) for databases. For the task of converting natural language text to SQL queries, Text-to-SQL, adaptation of LLMs is of paramount importance, both in in-context learning and fine-tuning settings, depending on the amount of adaptation data used. In this paper, we propose an LLM-based Text-to-SQL model SQL-PaLM, leveraging on PaLM-2, that pushes the state-of-the-art in both settings. Few-shot SQL-PaLM is based on an execution-based self-consistency prompting approach designed for Text-to-SQL, and achieves 77.3% in test-suite accuracy on Spider, which to our best knowledge is the first to outperform previous state-of-the-art with fine-tuning by a significant margin, 4%. Furthermore, we demonstrate that the fine-tuned SQL-PALM outperforms it further by another 1%. Towards applying SQL-PaLM to real-world scenarios we further evaluate its robustness on other chall
    
[^73]: 任务等变图Few-shot学习

    Task-Equivariant Graph Few-shot Learning. (arXiv:2305.18758v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.18758](http://arxiv.org/abs/2305.18758)

    本文提出了一种任务等变图Few-shot学习（TEG）框架，利用图神经网络的等变性质来使模型学习可转移的任务适应策略。该方法在各种Few-shot分类基准上展示了最先进的性能。

    

    虽然图神经网络（GNN）在节点分类任务中取得了成功，但其性能严重依赖于每类具有足够标记节点的可用性。在现实情况下，不是所有类都有许多标记节点，模型可能需要分类新类别，使得手动标记变得困难。为了解决这个问题，GNN需要能够用有限数量的标记节点对节点进行分类，称为Few-shot节点分类。先前的基于剧集元学习的方法在Few-shot节点分类中取得了成功，但我们的发现表明仅有多样的训练元任务才能实现最佳性能。为了应对基于元学习的Few-shot学习的挑战，我们提出了一种新的方法，即任务等变图Few-shot学习（TEG）框架。我们的TEG框架通过利用图神经网络的等变性质来使模型学习可转移的任务适应策略。我们在各种Few-shot分类基准上展示了我们提出的方法的有效性，实现了最先进的性能。

    Although Graph Neural Networks (GNNs) have been successful in node classification tasks, their performance heavily relies on the availability of a sufficient number of labeled nodes per class. In real-world situations, not all classes have many labeled nodes and there may be instances where the model needs to classify new classes, making manual labeling difficult. To solve this problem, it is important for GNNs to be able to classify nodes with a limited number of labeled nodes, known as few-shot node classification. Previous episodic meta-learning based methods have demonstrated success in few-shot node classification, but our findings suggest that optimal performance can only be achieved with a substantial amount of diverse training meta-tasks. To address this challenge of meta-learning based few-shot learning (FSL), we propose a new approach, the Task-Equivariant Graph few-shot learning (TEG) framework. Our TEG framework enables the model to learn transferable task-adaptation strate
    
[^74]: 使用潜在扩散模型生成行为多样化的策略

    Generating Behaviorally Diverse Policies with Latent Diffusion Models. (arXiv:2305.18738v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.18738](http://arxiv.org/abs/2305.18738)

    本文使用扩散模型将档案压缩为单个对于策略参数的生成模型，实现了13倍的压缩比率，同时保留了98%的原始回报和89%的原始覆盖率，并允许灵活选择和排序行为。

    

    最近在品质多样化强化学习（QD-RL）领域取得了进展，使得学习行为多样化、高性能的策略成为可能。然而，这些方法通常涉及存储数千个策略，导致空间复杂度高且难以适应更多行为的扩展。将档案压缩为单个模型，同时保留原始策略集的性能和覆盖率，已被证明是具有挑战性的。在本文中，我们提出使用扩散模型将归档压缩为单个对于策略参数的生成模型。我们展示了我们的方法实现了13倍的压缩比率，同时恢复了98%的原始回报和89%的原始覆盖率。扩散模型的调节机制还允许灵活选择和排序行为，包括使用语言。

    Recent progress in Quality Diversity Reinforcement Learning (QD-RL) has enabled learning a collection of behaviorally diverse, high performing policies. However, these methods typically involve storing thousands of policies, which results in high space-complexity and poor scaling to additional behaviors. Condensing the archive into a single model while retaining the performance and coverage of the original collection of policies has proved challenging. In this work, we propose using diffusion models to distill the archive into a single generative model over policy parameters. We show that our method achieves a compression ratio of 13x while recovering 98% of the original rewards and 89% of the original coverage. Further, the conditioning mechanism of diffusion models allows for flexibly selecting and sequencing behaviors, including using language. Project website: https://sites.google.com/view/policydiffusion/home
    
[^75]: 基于位置感知图增强变分自编码器的网络时间序列插补

    Networked Time Series Imputation via Position-aware Graph Enhanced Variational Autoencoders. (arXiv:2305.18612v1 [cs.LG])

    [http://arxiv.org/abs/2305.18612](http://arxiv.org/abs/2305.18612)

    本文提出了一种新颖的基于位置感知图增强变分自编码器方法来处理网络时间序列插补问题。该方法能够处理NTS数据中的图动态和缺失边，从而在合成和真实数据集上实现了优异的表现。

    

    多元时间序列插补是近年来广泛研究的问题。现有方法可以分为两大类，包括（1）主要关注时间序列特征的深度递归或生成模型，以及（2）基于图神经网络（GNN）的模型，利用MTS固有图结构的拓扑信息作为插补的关系归纳偏差。然而，这些方法要么忽略了拓扑信息，要么假定图结构固定且准确已知。因此，在更具挑战性的网络时间序列（NTS）数据中，它们无法充分利用图动态进行精确的插补，其中底层图不断变化并可能存在缺失边。本文提出了一种新方法来克服这些限制。首先，我们定义了包含节点时间序列特征和图结构中缺失值的NTS插补问题。然后，我们设计了一种名为PGE-VAE的新模型，它利用定位编码技术将时间序列信息合并到图神经网络中。具体而言，我们建议使用自我注意机制来捕捉图中不同时间步骤和不同节点之间的依赖关系。此外，我们引入了一个动态图生成网络来学习图结构的演化，可以处理缺失的边并适应图动态。在合成和真实数据集上的广泛实验表明，我们提出的方法优于现有最先进的方法。

    Multivariate time series (MTS) imputation is a widely studied problem in recent years. Existing methods can be divided into two main groups, including (1) deep recurrent or generative models that primarily focus on time series features, and (2) graph neural networks (GNNs) based models that utilize the topological information from the inherent graph structure of MTS as relational inductive bias for imputation. Nevertheless, these methods either neglect topological information or assume the graph structure is fixed and accurately known. Thus, they fail to fully utilize the graph dynamics for precise imputation in more challenging MTS data such as networked time series (NTS), where the underlying graph is constantly changing and might have missing edges. In this paper, we propose a novel approach to overcome these limitations. First, we define the problem of imputation over NTS which contains missing values in both node time series features and graph structures. Then, we design a new mod
    
[^76]: 目标导向任务中的反平方Levy步态普遍出现。

    Inverse square Levy walk emerging universally in goal-oriented tasks. (arXiv:2305.15559v1 [cond-mat.stat-mech])

    [http://arxiv.org/abs/2305.15559](http://arxiv.org/abs/2305.15559)

    本研究证明了反平方Levy步态（称为Cauchy步态）在目标导向任务中普遍出现。

    

    Levy步态中，步长出现频率遵循幂律分布，可以在各种生物的迁移行为中观察到。观察到了接近于2的幂指数的Levy步态，但其原因尚不清楚。本研究旨在提出一种普遍产生反平方Levy步态（称为Cauchy步态）的模型，并确定出Cauchy步态出现条件。我们证明了，在目标导向的任务中，Cauchy步态普遍出现。我们使用术语“目标导向”，当目标明确时，但可以通过不同的方式实现，而无法确定唯一的方式。我们进行了模拟，一个代理观察到在二维空间中从概率分布生成的数据，并连续估计该概率分布的中心坐标。代理有一个概率分布模型作为数据生成分布的假设，并可以修改该模型，以使其更符合实际情况。

    The Levy walk in which the frequency of occurrence of step lengths follows a power-law distribution, can be observed in the migratory behavior of organisms at various levels. Levy walks with power exponents close to 2 are observed, and the reasons are unclear. This study aims to propose a model that universally generates inverse square Levy walks (called Cauchy walks) and to identify the conditions under which Cauchy walks appear. We demonstrate that Cauchy walks emerge universally in goal-oriented tasks. We use the term "goal-oriented" when the goal is clear, but this can be achieved in different ways, which cannot be uniquely determined. We performed a simulation in which an agent observed the data generated from a probability distribution in a two-dimensional space and successively estimated the central coordinates of that probability distribution. The agent has a model of probability distribution as a hypothesis for data-generating distribution and can modify the model such that ea
    
[^77]: 多智能体强化学习: 异步通信和线性函数逼近

    Multi-agent Reinforcement Learning: Asynchronous Communication and Linear Function Approximation. (arXiv:2305.06446v1 [cs.LG])

    [http://arxiv.org/abs/2305.06446](http://arxiv.org/abs/2305.06446)

    该论文探讨了多智能体强化学习在情节式马尔可夫决策过程中的协作问题，提出了一种基于值迭代的算法，可以实现异步通信，在保证合作优势的同时降低通信开销。通过提供和证明的算法和复杂度界限，为多智能体强化学习在实际应用中提供理论依据。

    

    我们研究了多智能体强化学习在情节式马尔科夫决策过程中的设置，多个智能体通过中央服务器进行通信以合作。我们提出了一种基于值迭代的可证明有效的算法，可以实现异步通信，同时确保合作优势且通信开销低。我们证明了在使用线性函数逼近的情况下，我们的算法具有 $\tilde{\mathcal{O}}(d^{3/2}H^2\sqrt{K})$ 的遗憾值和 $\tilde{\mathcal{O}}(dHM^2)$ 的通信复杂度，其中 $d$ 是特征维数，$H$ 是时间跨度，$M$ 是智能体总数，$K$ 是总情节数。我们还提供了一个下限证明，表明通过协作至少需要 $\Omega(dM)$ 的通信复杂度才能改善性能。

    We study multi-agent reinforcement learning in the setting of episodic Markov decision processes, where multiple agents cooperate via communication through a central server. We propose a provably efficient algorithm based on value iteration that enable asynchronous communication while ensuring the advantage of cooperation with low communication overhead. With linear function approximation, we prove that our algorithm enjoys an $\tilde{\mathcal{O}}(d^{3/2}H^2\sqrt{K})$ regret with $\tilde{\mathcal{O}}(dHM^2)$ communication complexity, where $d$ is the feature dimension, $H$ is the horizon length, $M$ is the total number of agents, and $K$ is the total number of episodes. We also provide a lower bound showing that a minimal $\Omega(dM)$ communication complexity is required to improve the performance through collaboration.
    
[^78]: 基于基础模型的系统设计框架

    A Framework for Designing Foundation Model based Systems. (arXiv:2305.05352v1 [cs.SE])

    [http://arxiv.org/abs/2305.05352](http://arxiv.org/abs/2305.05352)

    本文提出了一个基于基础模型的系统分类体系，分类和比较了基础模型和基于基础模型的系统的特点。它为设计基于基础模型的系统时做出主要的设计决策提供了具体的指导，并突出了相关的权衡。

    

    最近推出了大型语言模型(LLM)的聊天机器人，如ChatGPT，这引起了人们对基础模型的广泛关注。基础模型被广泛认为将成为未来人工智能系统的基石。由于基础模型处于早期阶段，基于基础模型的系统设计尚未得到系统地探索。人们对在软件架构中引入基础模型的影响知之甚少。因此，在本文中，我们提出了一个基于基础模型的系统分类法，对基础模型和基于基础模型的系统的特点进行了分类和比较。我们的分类法包括三个类别：基础模型预训练和微调、基于基础模型的系统架构设计和负责任的AI设计。这个分类法为设计基于基础模型的系统时做出主要的设计决策提供了具体的指导，并突出了相关的权衡。

    The recent release of large language model (LLM) based chatbots, such as ChatGPT, has attracted significant attention on foundations models. It is widely believed that foundation models will serve as the fundamental building blocks for future AI systems. As foundation models are in their early stages, the design of foundation model based systems has not yet been systematically explored. There is little understanding about the impact of introducing foundation models in software architecture. Therefore, in this paper, we propose a taxonomy of foundation model based systems, which classifies and compares the characteristics of foundation models and foundation model based systems. Our taxonomy comprises three categories: foundation model pretraining and fine-tuning, architecture design of foundation model based systems, and responsible-AI-by-design. This taxonomy provides concrete guidance for making major design decisions when designing foundation model based systems and highlights trade-
    
[^79]: Learngene: 从祖先模型中继承压缩知识到后代模型

    Learngene: Inheriting Condensed Knowledge from the Ancestry Model to Descendant Models. (arXiv:2305.02279v1 [cs.LG])

    [http://arxiv.org/abs/2305.02279](http://arxiv.org/abs/2305.02279)

    本文提出了一种机器学习范式 Learngene，将积累的知识压缩成更为紧凑的信息片段并继承给后代模型，以便于适应新的环境

    

    在一个生物的连续进化过程中，它的基因积累了广泛的经验和知识，使新生后代能够快速适应其特定环境。受到这一观察的启发，我们提出了一种新的机器学习范 paradigm，即 Learngene，使学习模型能够融合基因的三个关键特征。 (i) 积累：知识在祖先模型的连续学习过程中积累。 (ii) 压缩：将积累的详尽知识压缩成更为紧凑的信息片段，即 Learngene。 (iii) 继承：将压缩的 Learngene 继承给后代模型，以便于适应新的环境。由于积累已在一些成熟的范式中得到研究，如大规模预训练和终身学习，因此我们专注于压缩和继承，这引发了三个关键问题，并为这些问题提供了初步的解决方案。

    During the continuous evolution of one organism's ancestry, its genes accumulate extensive experiences and knowledge, enabling newborn descendants to rapidly adapt to their specific environments. Motivated by this observation, we propose a novel machine learning paradigm \textit{Learngene} to enable learning models to incorporate three key characteristics of genes. (i) Accumulating: the knowledge is accumulated during the continuous learning of an \textbf{ancestry model}. (ii) Condensing: the exhaustive accumulated knowledge is condensed into a much more compact information piece, \ie \textbf{learngene}. (iii): Inheriting: the condensed \textbf{learngene} is inherited to make it easier for \textbf{descendant models} to adapt to new environments. Since accumulating has been studied in some well-developed paradigms like large-scale pre-training and lifelong learning, we focus on condensing and inheriting, which induces three key issues and we provide the preliminary solutions to these is
    
[^80]: 超越分类：最先进的语言模型中的财务推理

    Beyond Classification: Financial Reasoning in State-of-the-Art Language Models. (arXiv:2305.01505v1 [cs.CL])

    [http://arxiv.org/abs/2305.01505](http://arxiv.org/abs/2305.01505)

    本研究探讨了大语言模型在财务推理领域的潜在应用，对任务制定、数据生成、提示方法和评估能力等方面进行了详细研究，最终在各种数据集大小上对参数规模为2.8B至13B的各种GPT变体进行基准测试。

    

    大语言模型(LLMs)由1000亿及以上的参数组成，在复杂的多步推理任务中表现出了非凡的能力。然而，这种通用的进展应用在很少领域中，例如临床或法律领域，而财务推理领域基本上未被探索。据我们所知，LLMs解决财务推理问题的能力从未被研究过，并且它是否可以在任何规模上完成仍未知。为了填补这一知识空白，本研究对LLMs在财务领域的潜在应用进行了全面调查。调查包括对一系列主题的详细探讨，包括任务制定，合成数据生成，提示方法和评估能力。此外，本研究在不同的数据集大小上对参数规模为2.8B至13B的各种GPT变体进行基准测试，包括有无指导调整。

    Large Language Models (LLMs), consisting of 100 billion or more parameters, have demonstrated remarkable ability in complex multi-step reasoning tasks. However, the application of such generic advancements has been limited to a few fields, such as clinical or legal, with the field of financial reasoning remaining largely unexplored. To the best of our knowledge, the ability of LLMs to solve financial reasoning problems has never been dealt with, and whether it can be performed at any scale remains unknown. To address this knowledge gap, this research presents a comprehensive investigation into the potential application of LLMs in the financial domain. The investigation includes a detailed exploration of a range of subjects, including task formulation, synthetic data generation, prompting methods, and evaluation capability. Furthermore, the study benchmarks various GPT variants with parameter scales ranging from 2.8B to 13B, with and without instruction tuning, on diverse dataset sizes.
    
[^81]: 在线Platt缩放及其校准方法

    Online Platt Scaling with Calibeating. (arXiv:2305.00070v1 [cs.LG])

    [http://arxiv.org/abs/2305.00070](http://arxiv.org/abs/2305.00070)

    本文提出了一种在线Platt缩放及其校准方法，其理论基础强大，可以处理分布漂移和对抗性结果序列，无需超参数调整，在一系列合成和真实数据集上表现出卓越的性能。

    

    我们提出了一种在线后校准方法，称为在线Platt缩放(OPS)，它将Platt缩放技术与在线逻辑回归相结合。我们展示了OPS如何在分布漂移的i.i.d.和非i.i.d.情况下平稳适应。此外，当最佳的Platt缩放模型本身被错误校准时，我们使用一种最近开发的称为calibeating的技术来增强OPS，使其更加鲁棒。理论上，我们得到的OPS+calibeating方法对于对抗性结果序列是保证校准的。在实验上，它在一系列合成和真实数据集上均表现出卓越的性能，无需超参数调整。最后，我们将所有OPS思想扩展到beta缩放方法。

    We present an online post-hoc calibration method, called Online Platt Scaling (OPS), which combines the Platt scaling technique with online logistic regression. We demonstrate that OPS smoothly adapts between i.i.d. and non-i.i.d. settings with distribution drift. Further, in scenarios where the best Platt scaling model is itself miscalibrated, we enhance OPS by incorporating a recently developed technique called calibeating to make it more robust. Theoretically, our resulting OPS+calibeating method is guaranteed to be calibrated for adversarial outcome sequences. Empirically, it is effective on a range of synthetic and real-world datasets, with and without distribution drifts, achieving superior performance without hyperparameter tuning. Finally, we extend all OPS ideas to the beta scaling method.
    
[^82]: TorchBench: 用高API表面覆盖率评估PyTorch性能的基准套件

    TorchBench: Benchmarking PyTorch with High API Surface Coverage. (arXiv:2304.14226v1 [cs.LG])

    [http://arxiv.org/abs/2304.14226](http://arxiv.org/abs/2304.14226)

    TorchBench是一款新型基准测试套件，可全面表征PyTorch软件栈的性能，指导模型、PyTorch框架和GPU库的性能优化。

    

    深度学习是多个领域中的革命性技术。为了方便模型的开发和部署，提出了许多深度学习框架，其中PyTorch是最流行的解决方案之一。PyTorch软件栈的生态性能至关重要，可节省模型训练成本并减少模型推理的响应时间。本文提出了TorchBench，一款新型基准测试套件，用于研究PyTorch软件栈的性能。与现有基准测试套件不同，TorchBench包含了许多代表性模型，覆盖了大量PyTorch API表面。TorchBench能够全面地表征PyTorch软件栈的性能，指导模型、PyTorch框架和GPU库的性能优化。我们展示了TorchBench的两个实际用例。第一，我们对TorchBench进行性能剖析，以识别PyTorch的GPU性能效率问题。我们能够优化许多性能故障并向上游提交贡献。

    Deep learning (DL) has been a revolutionary technique in various domains. To facilitate the model development and deployment, many deep learning frameworks are proposed, among which PyTorch is one of the most popular solutions. The performance of ecosystem around PyTorch is critically important, which saves the costs of training models and reduces the response time of model inferences. In this paper, we propose TorchBench, a novel benchmark suite to study the performance of PyTorch software stack. Unlike existing benchmark suites, TorchBench encloses many representative models, covering a large PyTorch API surface. TorchBench is able to comprehensively characterize the performance of the PyTorch software stack, guiding the performance optimization across models, PyTorch framework, and GPU libraries. We show two practical use cases of TorchBench. (1) We profile TorchBench to identify GPU performance inefficiencies in PyTorch. We are able to optimize many performance bugs and upstream pa
    
[^83]: Hint-Aug: 从基础视觉变换器中获取提示，实现增强的少样本参数高效调优

    Hint-Aug: Drawing Hints from Foundation Vision Transformers Towards Boosted Few-Shot Parameter-Efficient Tuning. (arXiv:2304.12520v1 [cs.CV])

    [http://arxiv.org/abs/2304.12520](http://arxiv.org/abs/2304.12520)

    我们提出了一种名为Hint-Aug的框架，利用先前预训练的FViTs学到的高度代表性特征来增强调参数据，解决了FViTs在少样本数据的情况下的“饥饿”特性，并成功地提高了FViT训练的鲁棒性和调参表现。

    

    尽管越来越需要调优基础视觉变换器（FViT）用于下游任务，但在数据受限的情况下（例如，少样本调优），充分发挥FViTs的潜力仍然是一个挑战，因为FViTs的数据特性是饥饿的。由于少示例调参数据包含有限的特征，因此常见的数据增强技术在此情况下无法发挥作用。因此，我们首先确定FViTs在少样本调优方面的机会：预先训练的FViTs已经从大规模预训练数据中学到了高度代表性的特征，并且这些特征在广泛使用的参数高效调优过程中完全保留。我们因此假设利用这些已学习的特征来增强调参数据可以提高少样本FViT调优的效果。为此，我们提出了一个名为Hint-based Data Augmentation (Hint-Aug) 的框架，旨在通过将调整样本的过度拟合部分与预先训练的FViTs的学习特征相结合来增强FViT的少样本调优。结果表明，Hint-Aug显着提高了FViT训练的鲁棒性，从而实现了在仅使用少量数据集的情况下进行调参，使FViTs的性能优于当前基准。

    Despite the growing demand for tuning foundation vision transformers (FViTs) on downstream tasks, fully unleashing FViTs' potential under data-limited scenarios (e.g., few-shot tuning) remains a challenge due to FViTs' data-hungry nature. Common data augmentation techniques fall short in this context due to the limited features contained in the few-shot tuning data. To tackle this challenge, we first identify an opportunity for FViTs in few-shot tuning: pretrained FViTs themselves have already learned highly representative features from large-scale pretraining data, which are fully preserved during widely used parameter-efficient tuning. We thus hypothesize that leveraging those learned features to augment the tuning data can boost the effectiveness of few-shot FViT tuning. To this end, we propose a framework called Hint-based Data Augmentation (Hint-Aug), which aims to boost FViT in few-shot tuning by augmenting the over-fitted parts of tuning samples with the learned features of pret
    
[^84]: 通过模块化线性化注意力机制改进自回归自然语言处理任务

    Improving Autoregressive NLP Tasks via Modular Linearized Attention. (arXiv:2304.08453v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2304.08453](http://arxiv.org/abs/2304.08453)

    本文提出模块化线性化注意力机制（MLA）以最大化推理质量并实现速度提升，并在多个自回归自然语言处理任务上验证了该方法，包括语音到文本神经机器翻译（S2T NMT）、语音到文本同声传译（SimulST）和自回归文本到频谱图任务，在TTS上具有高效率收益，在NMT和SimulST的训练和推理过程中表现出竞争性能。

    

    多种自然语言处理任务需要的模型必须在最终应用于边缘或其他资源受限制的环境中高效且小型。尽管先前的研究已将这些模型的大小减小，但在不影响性能的前提下提高计算效率仍然很困难，特别是对于自回归任务而言。本文提出了一种模块化线性化注意力机制（MLA），它结合了多个有效的注意力机制，包括cosFormer，以最大化推理质量并实现显着的速度提升。我们在几个自回归自然语言处理任务上验证了这种方法，包括语音到文本神经机器翻译（S2T NMT）、语音到文本同声传译（SimulST）和自回归文本到频谱图任务，在TTS上具有高效率收益，在NMT和SimulST的训练和推理过程中表现出竞争性能。

    Various natural language processing (NLP) tasks necessitate models that are efficient and small based on their ultimate application at the edge or in other resource-constrained environments. While prior research has reduced the size of these models, increasing computational efficiency without considerable performance impacts remains difficult, especially for autoregressive tasks. This paper proposes {modular linearized attention (MLA), which combines multiple efficient attention mechanisms, including cosFormer, to maximize inference quality while achieving notable speedups. We validate this approach on several autoregressive NLP tasks, including speech-to-text neural machine translation (S2T NMT), speech-to-text simultaneous translation (SimulST), and autoregressive text-to-spectrogram, noting efficiency gains on TTS and competitive performance for NMT and SimulST during training and inference.
    
[^85]: 神经网络中符号的出现与语义理解和交流

    Emergence of Symbols in Neural Networks for Semantic Understanding and Communication. (arXiv:2304.06377v1 [cs.AI])

    [http://arxiv.org/abs/2304.06377](http://arxiv.org/abs/2304.06377)

    本文介绍了一种名为SEA-net的神经网络解决方案，可以生成符号，实现语义理解和交流。这些符号可以捕捉到组成性语义信息，并呈现类似自然语言的内在结构。

    

    能够创造有意义的符号，并熟练地将它们用于更高的认知功能，如交流、推理、规划等，是人类智能的重要和独特之处。 目前，深度神经网络仍远远落后于人类创造符号进行这些高级认知功能的能力。本文提出了一种名为SEA-net的解决方案，使神经网络具有符号创造、语义理解和交流能力。SEA-net生成动态配置网络以执行特定任务的符号。这些符号捕捉了组成性语义信息，使系统能够通过纯符号操作或交流获得新功能。此外，我们发现这些自动生成的符号呈现出类似自然语言的内在结构，表明在人类大脑和人工神经网络中生成和理解符号的共同框架。我们希望这将成为将来发展人工智能的助推器。

    Being able to create meaningful symbols and proficiently use them for higher cognitive functions such as communication, reasoning, planning, etc., is essential and unique for human intelligence. Current deep neural networks are still far behind human's ability to create symbols for such higher cognitive functions. Here we propose a solution, named SEA-net, to endow neural networks with ability of symbol creation, semantic understanding and communication. SEA-net generates symbols that dynamically configure the network to perform specific tasks. These symbols capture compositional semantic information that enables the system to acquire new functions purely by symbolic manipulation or communication. In addition, we found that these self-generated symbols exhibit an intrinsic structure resembling that of natural language, suggesting a common framework underlying the generation and understanding of symbols in both human brains and artificial neural networks. We hope that it will be instrum
    
[^86]: 有效地对齐跨语言会话任务的提示调整跨语言转移学习

    Efficiently Aligned Cross-Lingual Transfer Learning for Conversational Tasks using Prompt-Tuning. (arXiv:2304.01295v1 [cs.CL])

    [http://arxiv.org/abs/2304.01295](http://arxiv.org/abs/2304.01295)

    本文提出了一个平行大规模多语种会话数据集XSGD，开发了一种有效的基于提示调整的方法来学习对齐提示，同时研究了跨语言任务的NLI-based和vanilla分类器，并在插槽填充和意图分类任务上评估了模型的跨语言泛化能力。

    

    针对自然语言处理任务，跨语言转移的语言模型已被广泛研究，但是对于会话任务的研究相对较少。本文提出了XSGD，这是一个由Schema-Guided Dialogue（SGD）翻译成105种其他语言的平行大规模多语种会话数据集。为了实现对齐的跨语言表示方法，我们开发了一种有效的基于提示调整的方法来学习对齐提示。我们还研究了两种不同的分类器：NLI-based和vanilla分类器，并测试了对齐提示所实现的跨语言能力。我们在两个对话任务（插槽填充和意图分类）上评估了我们模型的跨语言泛化能力。

    Cross-lingual transfer of language models trained on high-resource languages like English has been widely studied for many NLP tasks, but focus on conversational tasks has been rather limited. This is partly due to the high cost of obtaining non-English conversational data, which results in limited coverage. In this work, we introduce XSGD, a parallel and large-scale multilingual conversation dataset that we created by translating the English-only Schema-Guided Dialogue (SGD) dataset (Rastogi et al., 2020) into 105 other languages. XSGD contains approximately 330k utterances per language. To facilitate aligned cross-lingual representations, we develop an efficient prompt-tuning-based method for learning alignment prompts. We also investigate two different classifiers: NLI-based and vanilla classifiers, and test cross-lingual capability enabled by the aligned prompts. We evaluate our model's cross-lingual generalization capabilities on two conversation tasks: slot-filling and intent cla
    
[^87]: 从宽到深：维度提升网络用于参数高效的知识图谱嵌入

    From Wide to Deep: Dimension Lifting Network for Parameter-efficient Knowledge Graph Embedding. (arXiv:2303.12816v1 [cs.LG])

    [http://arxiv.org/abs/2303.12816](http://arxiv.org/abs/2303.12816)

    本文提出了一个用于实现参数高效的知识图谱嵌入的深度网络，通过增加深度克服因采用低维实体表示而导致的模型精度下降和模型参数减少有限的问题。

    

    知识图谱嵌入（KGE）将实体和关系映射到向量表示对于下游任务非常重要。传统的KGE方法需要相对高维的实体表示来保留知识图谱的结构信息，但会导致庞大的模型参数。最近的方法通过采用低维实体表示来降低模型参数，同时开发技术（例如知识蒸馏）来补偿降维。然而，这样的操作会导致模型精度下降和模型参数减少有限。具体来说，我们将所有实体表示的级联视为嵌入层，那么采用高维实体表示的传统KGE方法等同于扩展嵌入层的宽度以获得表现力。为了在不牺牲准确度的情况下实现参数效率，我们相反地增加深度，并提出一个更深的实体嵌入网络。

    Knowledge graph embedding (KGE) that maps entities and relations into vector representations is essential for downstream tasks. Conventional KGE methods require relatively high-dimensional entity representations to preserve the structural information of knowledge graph, but lead to oversized model parameters. Recent methods reduce model parameters by adopting low-dimensional entity representations, while developing techniques (e.g., knowledge distillation) to compensate for the reduced dimension. However, such operations produce degraded model accuracy and limited reduction of model parameters. Specifically, we view the concatenation of all entity representations as an embedding layer, and then conventional KGE methods that adopt high-dimensional entity representations equal to enlarging the width of the embedding layer to gain expressiveness. To achieve parameter efficiency without sacrificing accuracy, we instead increase the depth and propose a deeper embedding network for entity re
    
[^88]: 熵正则化强化学习的莫特里卡多策略梯度：收敛性与全局最优性

    Matryoshka Policy Gradient for Entropy-Regularized RL: Convergence and Global Optimality. (arXiv:2303.12785v1 [cs.LG])

    [http://arxiv.org/abs/2303.12785](http://arxiv.org/abs/2303.12785)

    本文介绍了一种新的策略梯度算法——莫特里卡多策略梯度(MPG)，该算法尤其在最大熵强化学习中表现突出，能够实现一系列策略的训练和学习以达到任务的最优化，具有极高的收敛性和全局最优性。

    

    本文介绍并研究了一种新的策略梯度算法——莫特里卡多策略梯度(MPG)，在最大熵强化学习的背景下，代理目标是最大化除了累计奖励外的熵奖励。MPG与标准PG的不同之处在于它训练一系列策略同时学习有限的任务，而不是针对单一的标准目标训练一个单一的策略。对于softmax策略，我们证明了MPG的收敛性和极限的全局最优性，通过证明MPG目标的唯一临界点是最优策略；即使在连续紧致状态空间的情况下，这些结果仍然成立。MPG直观、理论上Sound，我们进一步展示了标准最大熵目标的最优策略可以通过MPG框架的最优策略进行任意精度的逼近。最后，我们证明了在策略用神经网络参数化的情况下，MPG非常适合。

    A novel Policy Gradient (PG) algorithm, called Matryoshka Policy Gradient (MPG), is introduced and studied, in the context of max-entropy reinforcement learning, where an agent aims at maximising entropy bonuses additional to its cumulative rewards. MPG differs from standard PG in that it trains a sequence of policies to learn finite horizon tasks simultaneously, instead of a single policy for the single standard objective. For softmax policies, we prove convergence of MPG and global optimality of the limit by showing that the only critical point of the MPG objective is the optimal policy; these results hold true even in the case of continuous compact state space. MPG is intuitive, theoretically sound and we furthermore show that the optimal policy of the standard max-entropy objective can be approximated arbitrarily well by the optimal policy of the MPG framework. Finally, we justify that MPG is well suited when the policies are parametrized with neural networks and we provide an simp
    
[^89]: 多智能体强化学习用于大规模格网交通网络区域信号控制

    Multi-agent Reinforcement Learning for Regional Signal control in Large-scale Grid Traffic network. (arXiv:2303.11899v1 [cs.AI])

    [http://arxiv.org/abs/2303.11899](http://arxiv.org/abs/2303.11899)

    本文提出了一种新的训练框架 RegionLight，基于交叉口之间的邻接关系将智能体分配到每个区域中。同时，研究人员扩展了BDQ方法为DBDQ，以限制联合动作空间大小的增长并缓解智能体训练问题。

    

    多智能体强化学习（MARL）的自适应交通信号控制是当前非常流行的研究领域。大多数现有方法中，一个智能体控制单个路口，这些方法侧重于路口之间的协作。然而，MARL的非稳态性质随着交通网络规模的增长，仍然限制着上述方法的性能。一种妥协的策略是将一名智能体分配到一组路口中，以减少智能体数量。这种策略存在两个挑战，一个是如何将交通网络划分成小区域，另一个是如何搜索区域内的最优联合动作。本文提出了一种新的训练框架RegionLight，其中我们的区域划分规则基于交叉口之间的邻接关系，并扩展了Branching Dueling Q-Network(BDQ)。该方法将BDQ进一步优化为Dynamic Branching Dueling Q-Network(DBDQ)，以限制联合动作空间大小的增长并缓解智能体训练问题。

    Adaptive traffic signal control with Multi-agent Reinforcement Learning(MARL) is a very popular topic nowadays. In most existing novel methods, one agent controls single intersections and these methods focus on the cooperation between intersections. However, the non-stationary property of MARL still limits the performance of the above methods as the size of traffic networks grows. One compromised strategy is to assign one agent with a region of intersections to reduce the number of agents. There are two challenges in this strategy, one is how to partition a traffic network into small regions and the other is how to search for the optimal joint actions for a region of intersections. In this paper, we propose a novel training framework RegionLight where our region partition rule is based on the adjacency between the intersection and extended Branching Dueling Q-Network(BDQ) to Dynamic Branching Dueling Q-Network(DBDQ) to bound the growth of the size of joint action space and alleviate th
    
[^90]: 知识图谱上复杂查询答案的序列查询编码

    Sequential Query Encoding For Complex Query Answering on Knowledge Graphs. (arXiv:2302.13114v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2302.13114](http://arxiv.org/abs/2302.13114)

    本文提出一种名为SQE的序列查询编码方法，将复杂查询答案编码为一个序列，从而实现快速和强大的知识图谱推理。

    

    复杂查询答案是知识图谱推理的重要和基础任务。查询编码被提出作为复杂查询答案的快速而强大的解决方案。在编码过程中，大多数现有的QE方法首先将逻辑查询解析为可执行的计算有向无环图(DAG)，然后使用神经网络对操作符进行参数化，最后递归执行这些神经网络化的操作符。然而，参数化和执行范式可能会潜在地过于复杂，它可以通过单一的神经网络编码器进行结构简化。与此同时，像LSTM和Transformer这样的序列编码器被证明对于编码相关任务的语义图非常有效。受此启发，我们提出了序列查询编码(SQE)作为编码CQA查询的替代方案。SQE首先使用基于搜索的算法将计算图线性化为一系列标记，然后使用序列编码器将这些标记编码为向量。

    Complex Query Answering (CQA) is an important and fundamental task for knowledge graph (KG) reasoning. Query encoding (QE) is proposed as a fast and robust solution to CQA. In the encoding process, most existing QE methods first parse the logical query into an executable computational direct-acyclic graph (DAG), then use neural networks to parameterize the operators, and finally, recursively execute these neuralized operators. However, the parameterization-and-execution paradigm may be potentially over-complicated, as it can be structurally simplified by a single neural network encoder. Meanwhile, sequence encoders, like LSTM and Transformer, proved to be effective for encoding semantic graphs in related tasks. Motivated by this, we propose sequential query encoding (SQE) as an alternative to encode queries for CQA. Instead of parameterizing and executing the computational graph, SQE first uses a search-based algorithm to linearize the computational graph to a sequence of tokens and th
    
[^91]: K-SHAP: 一种用于匿名状态-动作对的策略聚类算法

    K-SHAP: Policy Clustering Algorithm for Anonymous State-Action Pairs. (arXiv:2302.11996v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.11996](http://arxiv.org/abs/2302.11996)

    本文提出了一种名为K-SHAP的算法，来解决多个智能体保持匿名且仅有状态-动作对的情况下学习智能体决策的问题。

    

    从观测数据中学习智能体行为已经被证明可以提高我们对它们决策过程的理解，从而增强我们解释它们与环境和其他智能体之间交互的能力。尽管文献中已经提出了多种学习技术，但还有一种特定的情况尚未被探索，那就是智能体身份保持匿名的多智能体系统。例如，在金融市场中，标记数据通常是专有的，仅公开多个市场参与者交互而产生的匿名状态-动作对。因此，智能体行动序列不可观测，限制了现有工作的适用性。本文提出了一种策略聚类算法K-SHAP，它学习根据智能体策略对匿名状态-动作对进行分组。我们将该问题作为模仿学习(IL)任务，学习一个w...

    Learning agent behaviors from observational data has shown to improve our understanding of their decision-making processes, advancing our ability to explain their interactions with the environment and other agents. While multiple learning techniques have been proposed in the literature, there is one particular setting that has not been explored yet: multi agent systems where agent identities remain anonymous. For instance, in financial markets labeled data that identifies market participant strategies is typically proprietary, and only the anonymous state-action pairs that result from the interaction of multiple market participants are publicly available. As a result, sequences of agent actions are not observable, restricting the applicability of existing work. In this paper, we propose a Policy Clustering algorithm, called K-SHAP, that learns to group anonymous state-action pairs according to the agent policies. We frame the problem as an Imitation Learning (IL) task, and we learn a w
    
[^92]: 近乎贝叶斯最优的伪标签选择

    Approximately Bayes-Optimal Pseudo Label Selection. (arXiv:2302.08883v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.08883](http://arxiv.org/abs/2302.08883)

    本文介绍了BPLS，一种用于PLS的贝叶斯框架，通过解析逼近选择标签实例的标准，以避免由过度自信但错误预测的实例选择而导致的确认偏差问题。

    

    自训练的半监督学习严重依赖于伪标签选择（PLS）。选择通常取决于初始模型拟合标记数据的程度。过早的过拟合可能通过选择具有过度自信但错误的预测的实例（通常称为确认偏差）而传播到最终模型。本文介绍了BPLS，这是一种用于PLS的贝叶斯框架，旨在减轻这个问题。其核心是选择标签实例的标准：伪样本的后验预测的分析近似。我们通过证明伪样本的后验预测的贝叶斯最优性获得了这种选择标准。我们进一步通过解析逼近克服计算难题。它与边际似然的关系使我们能够提出基于拉普拉斯方法和高斯积分的逼近。我们针对参数广义线性和非参数广义加性模型对BPLS进行了实证评估。

    Semi-supervised learning by self-training heavily relies on pseudo-label selection (PLS). The selection often depends on the initial model fit on labeled data. Early overfitting might thus be propagated to the final model by selecting instances with overconfident but erroneous predictions, often referred to as confirmation bias. This paper introduces BPLS, a Bayesian framework for PLS that aims to mitigate this issue. At its core lies a criterion for selecting instances to label: an analytical approximation of the posterior predictive of pseudo-samples. We derive this selection criterion by proving Bayes optimality of the posterior predictive of pseudo-samples. We further overcome computational hurdles by approximating the criterion analytically. Its relation to the marginal likelihood allows us to come up with an approximation based on Laplace's method and the Gaussian integral. We empirically assess BPLS for parametric generalized linear and non-parametric generalized additive models
    
[^93]: 人性化、道德化和负责任AI的系统文献综述

    A Systematic Literature Review of Human-Centered, Ethical, and Responsible AI. (arXiv:2302.05284v3 [cs.HC] UPDATED)

    [http://arxiv.org/abs/2302.05284](http://arxiv.org/abs/2302.05284)

    本文通过对164篇主题性回顾，总结了人性化、道德化和负责任AI领域的底层映射，发现当前的HCER-AI研究重点是治理、公正和可解释性，而AIES、FAccT、CHI和CSCW等会议专注于特定主题，缺乏对隐私、安全和人类繁荣的关注。

    

    随着人工智能的快速发展，考虑AI的道德和社会影响变得越来越重要。本文通过对AIES、CHI、CSCW和FAccT等主要会议中164篇论文的主题性回顾和分析，提出了目前人性化AI、道德化AI和负责任AI（HCER-AI）交叉研究领域的底层映射。HCER-AI领域的研究强调治理、公正和可解释性。这些会议专注于特定主题，而AIES在HCER-AI方面论文较少，强调治理，很少发表有关隐私、安全和人类繁荣的论文。FAccT在治理方面发表的论文更多，但缺乏关于隐私、安全和人类繁荣的论文。CHI和CSCW作为更成熟的会议，拥有更广泛的研究领域。

    As Artificial Intelligence (AI) continues to advance rapidly, it becomes increasingly important to consider AI's ethical and societal implications. In this paper, we present a bottom-up mapping of the current state of research at the intersection of Human-Centered AI, Ethical, and Responsible AI (HCER-AI) by thematically reviewing and analyzing 164 research papers from leading conferences in ethical, social, and human factors of AI: AIES, CHI, CSCW, and FAccT. The ongoing research in HCER-AI places emphasis on governance, fairness, and explainability. These conferences, however, concentrate on specific themes rather than encompassing all aspects. While AIES has fewer papers on HCER-AI, it emphasizes governance and rarely publishes papers about privacy, security, and human flourishing. FAccT publishes more on governance and lacks papers on privacy, security, and human flourishing. CHI and CSCW, as more established conferences, have a broader research portfolio. We find that the current 
    
[^94]: QR-CLIP: 引入显式开放世界知识进行位置和时间推理

    QR-CLIP: Introducing Explicit Open-World Knowledge for Location and Time Reasoning. (arXiv:2302.00952v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.00952](http://arxiv.org/abs/2302.00952)

    本文提出QR-CLIP模型，通过引入开放世界知识进行位置和时间推理，在此任务上取得了约10%和130%的相对提升。

    

    每天的图像可能传达需要我们从中记忆和推断出深刻信息的抽象含义。在本文中，我们教会机器预测图片拍摄的地点和时间，而不是执行传统的分割或分类任务，以鼓励人类类似的推理。受到Horn的QR理论的启发，我们设计了一个由两个部分组成的新型QR-CLIP模型: 1) 数量模块首先回顾更多的开放世界知识作为候选的语言输入; 2) 相关性模块仔细估计视觉和语言线索，并推断出位置和时间。实验显示，我们的QR-CLIP十分有效，并且在每个任务上都比之前的最高水平表现平均提升了约10%和130%的相对提升。本研究为位置和时间推理奠定了技术基础，并表明有效引入开放世界知识是完成这些任务的方法之一。

    Daily images may convey abstract meanings that require us to memorize and infer profound information from them. To encourage such human-like reasoning, in this work, we teach machines to predict where and when it was taken rather than performing basic tasks like traditional segmentation or classification. Inspired by Horn's QR theory, we designed a novel QR-CLIP model consisting of two components: 1) the Quantity module first retrospects more open-world knowledge as the candidate language inputs; 2) the Relevance module carefully estimates vision and language cues and infers the location and time. Experiments show our QR-CLIP's effectiveness, and it outperforms the previous SOTA on each task by an average of about 10% and 130% relative lift in terms of location and time reasoning. This study lays a technical foundation for location and time reasoning and suggests that effectively introducing open-world knowledge is one of the panaceas for the tasks.
    
[^95]: 鉴定容易受到对抗性攻击的样本和强韧样本

    Identifying Adversarially Attackable and Robust Samples. (arXiv:2301.12896v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.12896](http://arxiv.org/abs/2301.12896)

    本文提出了一种深度学习方法，用于检测哪些样本最容易受到对抗性攻击，从而确定哪些样本最不容易受到攻击。实验结果表明，这种检测器在不同的模型结构中具有较好的可移植性和检测性能。

    

    对抗性攻击将微小的，难以感知的扰动插入输入样本，导致深度学习模型的输出发生大量不期望的变化。虽然对抗性攻击的生成和防御已经得到广泛研究，但对从输入数据角度理解对抗性攻击的研究仍然很有限。本文引入了样本攻击性的概念，旨在确定最容易受到对抗性攻击的样本（攻击性样本），从而反过来确定最不容易受到攻击的样本（强韧样本）。我们提出了一种基于深度学习的方法，用于检测针对未知目标模型的未见数据集中，容易受到对抗性攻击和强韧性样本。标准图像分类数据集上的实验证实了深度攻击性检测器在不同体系结构中的可移植性。我们发现，与基于简单模型不确定性的措施相比，深度攻击性检测器表现更好。

    Adversarial attacks insert small, imperceptible perturbations to input samples that cause large, undesired changes to the output of deep learning models. Despite extensive research on generating adversarial attacks and building defense systems, there has been limited research on understanding adversarial attacks from an input-data perspective. This work introduces the notion of sample attackability, where we aim to identify samples that are most susceptible to adversarial attacks (attackable samples) and conversely also identify the least susceptible samples (robust samples). We propose a deep-learning-based method to detect the adversarially attackable and robust samples in an unseen dataset for an unseen target model. Experiments on standard image classification datasets enables us to assess the portability of the deep attackability detector across a range of architectures. We find that the deep attackability detector performs better than simple model uncertainty-based measures for i
    
[^96]: 分离与扩散：使用预训练的扩散模型提高源分离

    Separate And Diffuse: Using a Pretrained Diffusion Model for Improving Source Separation. (arXiv:2301.10752v2 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2301.10752](http://arxiv.org/abs/2301.10752)

    该论文使用预训练的扩散模型和确定性模型的输出进行线性组合，取得了在多个基准测试中对于2、3、5、10和20个说话者的最先进结果。

    

    语音分离问题，也被称为鸡尾酒会问题，指的是从多个混合的语音信号中隔离出单个语音信号的任务。先前的源分离工作在人类语音领域推导出了源分离任务的上限，该上限是针对确定性模型而推导的。最近生成模型的进展挑战了该限制。我们展示了如何将上限推广到随机生成模型的情况。将预先训练对单一说话者声音进行建模的扩散模型 Vocoder 应用于确定性分离模型的输出，可实现最先进的分离结果。我们的方法需要将分离模型的输出与扩散模型的输出进行组合，使用一个学习模型推断出的权重，在频率域内进行线性组合。我们在多个基准测试中展示了对于2、3、5、10和20个说话者的最先进结果。

    The problem of speech separation, also known as the cocktail party problem, refers to the task of isolating a single speech signal from a mixture of speech signals. Previous work on source separation derived an upper bound for the source separation task in the domain of human speech. This bound is derived for deterministic models. Recent advancements in generative models challenge this bound. We show how the upper bound can be generalized to the case of random generative models. Applying a diffusion model Vocoder that was pretrained to model single-speaker voices on the output of a deterministic separation model leads to state-of-the-art separation results. It is shown that this requires one to combine the output of the separation model with that of the diffusion model. In our method, a linear combination is performed, in the frequency domain, using weights that are inferred by a learned model. We show state-of-the-art results on 2, 3, 5, 10, and 20 speakers on multiple benchmarks. In 
    
[^97]: 我有足够的知识回答吗？探究知识库问答的可回答性。

    Do I have the Knowledge to Answer? Investigating Answerability of Knowledge Base Questions. (arXiv:2212.10189v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.10189](http://arxiv.org/abs/2212.10189)

    本文探究了在知识库问答中的可回答性问题，使用新的GrailQAbility基准KBQA数据集，发现现有的KBQA模型在处理无法回答问题时性能下降，并对无法回答的检测存在问题，需要进一步研究来使KBQA系统对无法回答具有鲁棒性。

    

    在对知识库进行自然语言问答时，缺失的事实、不完整的模式和有限的范围自然地导致许多问题无法回答。虽然在其他问答环境中已经探讨了可回答性，但对于知识库问答（KBQA）尚未进行研究。我们首先识别了各种形式的知识库不完整性，使得问题无法回答，并通过有系统地调整GrailQA（一个仅包含可回答问题的流行KBQA数据集）来创建具有无法回答问题的新的GrailQAbility基准KBQA数据集。在三个最先进的KBQA模型的实验中，我们发现即使在适当调整无法回答的问题后，所有三个模型的性能也会下降。此外，这些模型常常因错误的原因检测出无法回答，并发现特定形式的无法回答尤其难以处理。这强调了进一步研究使KBQA系统对无法回答具有鲁棒性的必要性。

    When answering natural language questions over knowledge bases, missing facts, incomplete schema and limited scope naturally lead to many questions being unanswerable. While answerability has been explored in other QA settings, it has not been studied for QA over knowledge bases (KBQA). We create GrailQAbility, a new benchmark KBQA dataset with unanswerability, by first identifying various forms of KB incompleteness that make questions unanswerable, and then systematically adapting GrailQA (a popular KBQA dataset with only answerable questions). Experimenting with three state-of-the-art KBQA models, we find that all three models suffer a drop in performance even after suitable adaptation for unanswerable questions. In addition, these often detect unanswerability for wrong reasons and find specific forms of unanswerability particularly difficult to handle. This underscores the need for further research in making KBQA systems robust to unanswerability
    
[^98]: 移动设备上的实时神经光场渲染

    Real-Time Neural Light Field on Mobile Devices. (arXiv:2212.08057v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2212.08057](http://arxiv.org/abs/2212.08057)

    本文提出了一种在移动设备上实时运行的高效网络，用于神经渲染。

    

    近期，神经渲染场（NeRF）在利用隐式神经表示法来表示3D场景，并实现新视角合成方面取得了令人印象深刻的成果。由于体积渲染的过程，NeRF的推理速度非常缓慢，限制了在移动设备等资源受限的硬件上利用NeRF的应用场景。有许多工作致力于减少运行NeRF模型的延迟。但是，他们大多仍需要高端GPU进行加速或额外的存储内存，这在移动设备上都不可用。另一个新出现的方向则利用神经光场（NeLF）来进行加速，因为一条射线上只需进行一次向前传递即可预测像素颜色。然而，为了达到与NeRF类似的渲染质量，NeLF中的网络设计需要大量的计算，这对移动设备并不友好。在本文中，我们提出了一种在移动设备上实时运行的高效网络，用于神经渲染。

    Recent efforts in Neural Rendering Fields (NeRF) have shown impressive results on novel view synthesis by utilizing implicit neural representation to represent 3D scenes. Due to the process of volumetric rendering, the inference speed for NeRF is extremely slow, limiting the application scenarios of utilizing NeRF on resource-constrained hardware, such as mobile devices. Many works have been conducted to reduce the latency of running NeRF models. However, most of them still require high-end GPU for acceleration or extra storage memory, which is all unavailable on mobile devices. Another emerging direction utilizes the neural light field (NeLF) for speedup, as only one forward pass is performed on a ray to predict the pixel color. Nevertheless, to reach a similar rendering quality as NeRF, the network in NeLF is designed with intensive computation, which is not mobile-friendly. In this work, we propose an efficient network that runs in real-time on mobile devices for neural rendering. W
    
[^99]: 可变化决策频率的选项评论者

    Variable Decision-Frequency Option Critic. (arXiv:2212.04407v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.04407](http://arxiv.org/abs/2212.04407)

    这篇论文提出了一个名为CTCO的框架，其中代理选择选项作为可变持续时间的子策略。这个框架可以以任何所需频率与系统交互，从而提供平滑的动作变化，相比传统RL和时间抽象RL方法，其性能更好。

    

    在传统的强化学习算法中，代理在离散和固定的时间间隔内做出决策。决策之间的持续时间变成了一个关键的超参数，因为设置得太短可能会增加问题的难度，需要代理进行多次决策才能实现其目标，而设置得太长会导致代理失去对系统的控制。然而，物理系统不一定需要恒定的控制频率，对于学习代理来说，一般情况下，当需要时以高频率运行，而在可能时以低频率运行更好。我们提出了一个名为连续时间连续选项 (CTCO) 的框架，其中代理选择选项作为可变持续时间的子策略。这些选项是时间连续的，可以以任何所需频率与系统交互，从而提供平滑的动作变化。我们通过将其性能与传统 RL 和时间抽象 RL 方法进行比较，展示了 CTCO 的有效性。

    In classic reinforcement learning algorithms, agents make decisions at discrete and fixed time intervals. The duration between decisions becomes a crucial hyperparameter, as setting it too short may increase the difficulty of the problem by requiring the agent to make numerous decisions to achieve its goal, while setting it too long can result in the agent losing control over the system. However, physical systems do not necessarily require a constant control frequency, and for learning agents, it is often preferable to operate with a low frequency when possible and a high frequency when necessary. We propose a framework called Continuous-Time Continuous-Options (CTCO), where the agent chooses options as sub-policies of variable durations. These options are time-continuous and can interact with the system at any desired frequency providing a smooth change of actions. We demonstrate the effectiveness of CTCO by comparing its performance to classical RL and temporal-abstraction RL methods
    
[^100]: 探究神经语言模型对估计概率词语的理解

    Probing neural language models for understanding of words of estimative probability. (arXiv:2211.03358v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2211.03358](http://arxiv.org/abs/2211.03358)

    本研究探究了神经语言处理模型对于估计概率词语的理解能力，使用UNLI数据集和构建WEP数据集进行实验，发现语言模型在预测WEP的存在时很有效，但没有完全捕捉到与每个词相关联的共识概率水平。

    

    估计概率词语(WEP)是陈述的可信度表达（例如，可能，或许，很有可能，怀疑，不可能等）。多项调查表明，人类评估者在为WEP分配数值概率水平时存在一致性。例如， Fagen-Ulmschneider(2015)的调查中，“很有可能”对应着中位数0.90+-0.08的几率。本研究测量了神经语言处理模型捕捉与每个WEP相关联的共识概率水平的能力。首先，我们使用UNLI数据集(陈等人，2020)，将前提和假设与其感知的联合概率p相联系，构建提示，例如“[前提]。[WEP]，[假设]。”并评估语言模型是否可以预测WEP共识概率水平是否接近于p。其次，我们构建了一个基于WEP的概率推理数据集，测试语言模型是否能够使用WEP组合进行推理。在提示“[事件A]很有可能。[事件B]不可能。[结果]会发生的概率是多少？ "时，人们可以根据WEP词汇的可信度估算概率水平。我们对两个现有的神经语言处理模型进行了实验，并观察到虽然它们在预测WEP的存在时很有效，但它们没有完全捕捉到与每个词相关联的共识概率水平。

    Words of estimative probability (WEP) are expressions of a statement's plausibility (probably, maybe, likely, doubt, likely, unlikely, impossible...). Multiple surveys demonstrate the agreement of human evaluators when assigning numerical probability levels to WEP. For example, highly likely corresponds to a median chance of 0.90+-0.08 in Fagen-Ulmschneider (2015)'s survey. In this work, we measure the ability of neural language processing models to capture the consensual probability level associated to each WEP. Firstly, we use the UNLI dataset (Chen et al., 2020) which associates premises and hypotheses with their perceived joint probability p, to construct prompts, e.g. "[PREMISE]. [WEP], [HYPOTHESIS]." and assess whether language models can predict whether the WEP consensual probability level is close to p. Secondly, we construct a dataset of WEP-based probabilistic reasoning, to test whether language models can reason with WEP compositions. When prompted "[EVENTA] is likely. [EVEN
    
[^101]: 基于风险和场景图学习的异构轨迹预测

    Heterogeneous Trajectory Forecasting via Risk and Scene Graph Learning. (arXiv:2211.00848v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.00848](http://arxiv.org/abs/2211.00848)

    本文提出了一种基于风险和场景图学习的异构道路代理轨迹预测方法，通过异构风险图和分层场景图进行交互建模，取得了优于其他方法的效果。

    

    异构轨迹预测对智能交通系统至关重要，但由于建模异构道路代理之间的复杂交互关系及其代理-环境约束的难度而具有挑战性。在本文中，我们提出了一种基于风险和场景图学习的异构道路代理轨迹预测方法，它由异构风险图和分层场景图组成，从代理类别和它们可移动的语义区域两个方面来考虑。异构风险图将每种道路代理分组，并基于有效的碰撞风险度量计算它们的交互邻接矩阵。驾驶场景的分层场景图通过推断道路代理和由道路场景语法对齐的道路语义布局之间的关系来建模。基于这种形式化，我们可以在驾驶情况下获得有效的轨迹预测，并证明其优于其他最先进的方法。

    Heterogeneous trajectory forecasting is critical for intelligent transportation systems, but it is challenging because of the difficulty of modeling the complex interaction relations among the heterogeneous road agents as well as their agent-environment constraints. In this work, we propose a risk and scene graph learning method for trajectory forecasting of heterogeneous road agents, which consists of a Heterogeneous Risk Graph (HRG) and a Hierarchical Scene Graph (HSG) from the aspects of agent category and their movable semantic regions. HRG groups each kind of road agent and calculates their interaction adjacency matrix based on an effective collision risk metric. HSG of the driving scene is modeled by inferring the relationship between road agents and road semantic layout aligned by the road scene grammar. Based on this formulation, we can obtain effective trajectory forecasting in driving situations, and superior performance to other state-of-the-art approaches is demonstrated by
    
[^102]: GFlowOut：使用生成流网络的Dropout

    GFlowOut: Dropout with Generative Flow Networks. (arXiv:2210.12928v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.12928](http://arxiv.org/abs/2210.12928)

    GFlowOut是一种使用生成流网络的Dropout方法，可以更好地估计复杂的后验分布和样本相关性，并在几个基准数据集上实现了最先进的性能和良好的校准不确定性估计。

    

    贝叶斯推理为解决现代神经网络的许多关键问题（例如不良校准和泛化，以及数据效率）提供了原则性的工具。然而，将贝叶斯推理扩展到大型架构是具有挑战性的并且需要严格的近似。Monte Carlo Dropout已被广泛用作近似推理的相对便宜的方法，并使用深度神经网络估计不确定性。传统上，dropout掩码是从固定分布中独立采样的。最近的研究表明，dropout掩码可以被视为潜在变量，并且可以使用变分推理进行推断。这些方法面临两个重要的挑战：（a）掩码的后验分布可能高度多模态，很难用标准变分推理进行近似；（b）充分利用dropout掩码之间的样本相关信息和相关性以改善后验估计并不是微不足道的。在本研究中，我们提出了GFlowOut，一种使用生成流网络模拟dropout掩码分布的新方法。我们的方法提供了一种灵活且可处理的方法来模拟复杂的掩码后验分布，并可以更好地捕获掩码之间的样本相关性。我们展示了GFlowOut在几个基准数据集上实现了最先进的性能，并提供了良好校准的不确定性估计。

    Bayesian Inference offers principled tools to tackle many critical problems with modern neural networks such as poor calibration and generalization, and data inefficiency. However, scaling Bayesian inference to large architectures is challenging and requires restrictive approximations. Monte Carlo Dropout has been widely used as a relatively cheap way for approximate Inference and to estimate uncertainty with deep neural networks. Traditionally, the dropout mask is sampled independently from a fixed distribution. Recent works show that the dropout mask can be viewed as a latent variable, which can be inferred with variational inference. These methods face two important challenges: (a) the posterior distribution over masks can be highly multi-modal which can be difficult to approximate with standard variational inference and (b) it is not trivial to fully utilize sample-dependent information and correlation among dropout masks to improve posterior estimation. In this work, we propose GF
    
[^103]: 基于拓扑排序的因果发现扩散模型

    Diffusion Models for Causal Discovery via Topological Ordering. (arXiv:2210.06201v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.06201](http://arxiv.org/abs/2210.06201)

    本文提出基于扩散模型的DiffAN拓扑排序算法，用于解决因果发现中的搜索空间优化问题。

    

    在考虑到将功能关系约束为非线性带有加性噪声（ANM）的情况下，从观测数据中发现因果关系成为可能。即使带有强大的假设，因果发现也涉及到在有向无环图（DAGs）空间中进行昂贵的搜索问题。拓扑排序方法通过在排列空间中搜索而不是图形空间中搜索，从而减少了因果发现优化空间。对于ANMs，可以使用数据对数似然的Hessian来找到因果图中的叶节点，从而允许它的拓扑排序。然而，现有的用于获取Hessian的计算方法仍然无法扩展为变量数量和样本数量增加的情况。因此，受到扩散概率模型（DPMs）最近创新的启发，我们提出了一种名为DiffAN的拓扑排序算法。

    Discovering causal relations from observational data becomes possible with additional assumptions such as considering the functional relations to be constrained as nonlinear with additive noise (ANM). Even with strong assumptions, causal discovery involves an expensive search problem over the space of directed acyclic graphs (DAGs). \emph{Topological ordering} approaches reduce the optimisation space of causal discovery by searching over a permutation rather than graph space. For ANMs, the \emph{Hessian} of the data log-likelihood can be used for finding leaf nodes in a causal graph, allowing its topological ordering. However, existing computational methods for obtaining the Hessian still do not scale as the number of variables and the number of samples increase. Therefore, inspired by recent innovations in diffusion probabilistic models (DPMs), we propose \emph{DiffAN}\footnote{Implementation is available at \url{https://github.com/vios-s/DiffAN} .}, a topological ordering algorithm t
    
[^104]: 使用对比剪枝权重训练去偏置子网络

    Training Debiased Subnetworks with Contrastive Weight Pruning. (arXiv:2210.05247v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.05247](http://arxiv.org/abs/2210.05247)

    本文探讨了在存在强假相关的偏置网络中提取最优无偏子网络的问题，并提出了使用对比剪枝权重训练实现去偏置子网络的算法 DCWP，在多个应用中都有良好的效果。

    

    神经网络通常存在偏置性，导致提供具有误导性的统计证据，不能很好地推广。因此，提出了在偏置网络中提取最优无偏功能子网络的问题。本文首先提出了现有算法在探索具有强假相关性的无偏子网络存在限制的理论洞见，然后进一步阐明了偏差冲突样本对结构学习的重要性，并基于学习的（伪）无偏样本和选择性偏差冲突样本，提出了去偏置对比剪枝（DCWP）算法。在图像分类、语言模型和强化学习等各种应用中验证了 DCWP 的有效性。

    Neural networks are often biased to spuriously correlated features that provide misleading statistical evidence that does not generalize. This raises an interesting question: ``Does an optimal unbiased functional subnetwork exist in a severely biased network? If so, how to extract such subnetwork?" While empirical evidence has been accumulated about the existence of such unbiased subnetworks, these observations are mainly based on the guidance of ground-truth unbiased samples. Thus, it is unexplored how to discover the optimal subnetworks with biased training datasets in practice. To address this, here we first present our theoretical insight that alerts potential limitations of existing algorithms in exploring unbiased subnetworks in the presence of strong spurious correlations. We then further elucidate the importance of bias-conflicting samples on structure learning. Motivated by these observations, we propose a Debiased Contrastive Weight Pruning (DCWP) algorithm, which probes unbi
    
[^105]: KSAT：知识注入的自我关注变压器——整合多个领域特定的上下文

    KSAT: Knowledge-infused Self Attention Transformer -- Integrating Multiple Domain-Specific Contexts. (arXiv:2210.04307v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2210.04307](http://arxiv.org/abs/2210.04307)

    KSAT使用外部知识源引入知识引导偏见来整合多个领域特定上下文的自我关注结构。

    

    领域特定的语言理解需要整合多个相关的上下文信息。例如，“我拿着一把枪，对我的生活感到很糟糕，如果明天我不醒来，这可能不是最糟糕的事情”，其中包含自杀和抑郁症相关的行为（多个上下文）。自注意力结构中的领域特定性通过在相关领域特定资源的摘录上进行微调（数据集和外部知识-与自杀和抑郁症相关的心理健康诊断的医学教科书章节）来处理。我们提出了一种修改后的自我关注结构KSAT，通过使用外部知识源实现了多个领域特定上下文的整合。KSAT在每个知识源的专门自我关注层中引入知识引导偏见来完成这一点。此外，KSAT提供了控制学习和知识利用之间权衡的机制。

    Domain-specific language understanding requires integrating multiple pieces of relevant contextual information. For example, we see both suicide and depression-related behavior (multiple contexts) in the text ``I have a gun and feel pretty bad about my life, and it wouldn't be the worst thing if I didn't wake up tomorrow''. Domain specificity in self-attention architectures is handled by fine-tuning on excerpts from relevant domain specific resources (datasets and external knowledge - medical textbook chapters on mental health diagnosis related to suicide and depression). We propose a modified self-attention architecture Knowledge-infused Self Attention Transformer (KSAT) that achieves the integration of multiple domain-specific contexts through the use of external knowledge sources. KSAT introduces knowledge-guided biases in dedicated self-attention layers for each knowledge source to accomplish this. In addition, KSAT provides mechanics for controlling the trade-off between learning 
    
[^106]: 迈向面向OOD的对抗鲁棒性

    Towards Out-of-Distribution Adversarial Robustness. (arXiv:2210.03150v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.03150](http://arxiv.org/abs/2210.03150)

    该论文介绍了一种面向OOD的对抗鲁棒性方法，通过将每个攻击类型视为一个领域，应用风险外推方法实现对各攻击的相似鲁棒性水平，实现了在训练和测试时的更高性能，是对抗鲁棒性研究中的创新贡献。

    

    对抗鲁棒性仍然是深度学习的一个主要挑战。一个核心问题是对一种攻击的鲁棒性往往不能转移到其他攻击。我们展示了通过采用领域泛化方法，可以在许多常用攻击中改善鲁棒性。具体来说，我们将每种攻击视为一个领域，并应用风险外推方法（REx），促进对所有训练攻击的相似鲁棒水平。与现有方法相比，在训练期间看到的攻击上，我们获得类似或更高级别的最坏情况下的对抗鲁棒性。此外，在家族或测试时只遇到的攻击的调整中，我们实现了更高的性能。在攻击集合上，我们的方法将MNIST的最佳现有基线的准确性从3.4%提高到25.9％，在CIFAR10上从16.9％提高到23.5％。

    Adversarial robustness continues to be a major challenge for deep learning. A core issue is that robustness to one type of attack often fails to transfer to other attacks. While prior work establishes a theoretical trade-off in robustness against different $L_p$ norms, we show that there is potential for improvement against many commonly used attacks by adopting a domain generalisation approach. Concretely, we treat each type of attack as a domain, and apply the Risk Extrapolation method (REx), which promotes similar levels of robustness against all training attacks. Compared to existing methods, we obtain similar or superior worst-case adversarial robustness on attacks seen during training. Moreover, we achieve superior performance on families or tunings of attacks only encountered at test time. On ensembles of attacks, our approach improves the accuracy from 3.4% with the best existing baseline to 25.9% on MNIST, and from 16.9% to 23.5% on CIFAR10.
    
[^107]: 通过排名评估预训练自监督表示的下游性能：RankMe

    RankMe: Assessing the downstream performance of pretrained self-supervised representations by their rank. (arXiv:2210.02885v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.02885](http://arxiv.org/abs/2210.02885)

    本文提出了一个简单的无监督准则 RankMe，通过评估有效排名，可以指示学习JE-SSL表示的质量，而无需任何标签。

    

    无监督自监督学习（JE-SSL）的快速发展，使得出现了许多方法变化，但只有少数原则性指导方针，能够帮助从业人员成功地部署它们。本文开发了一个简单的无监督准则，即效果排名，可以指示学习JE-SSL表示的质量。这种方法简单而且计算友好，甚至可以在不需要任何标签的情况下评估JE-SSL表示的性能，即使在不同的下游数据集上也是如此。

    Joint-Embedding Self Supervised Learning (JE-SSL) has seen a rapid development, with the emergence of many method variations but only few principled guidelines that would help practitioners to successfully deploy them. The main reason for that pitfall comes from JE-SSL's core principle of not employing any input reconstruction therefore lacking visual cues of unsuccessful training. Adding non informative loss values to that, it becomes difficult to deploy SSL on a new dataset for which no labels can help to judge the quality of the learned representation. In this study, we develop a simple unsupervised criterion that is indicative of the quality of the learned JE-SSL representations: their effective rank. Albeit simple and computationally friendly, this method -- coined RankMe -- allows one to assess the performance of JE-SSL representations, even on different downstream datasets, without requiring any labels. A further benefit of RankMe is that it does not have any training or hyper-p
    
[^108]: 暂停分子表征学习：用于分子属性预测的新方法

    Taking a Respite from Representation Learning for Molecular Property Prediction. (arXiv:2209.13492v3 [q-bio.QM] UPDATED)

    [http://arxiv.org/abs/2209.13492](http://arxiv.org/abs/2209.13492)

    本研究对一系列分子表征模型进行了系统评估，发现基于固定表征的模型在分子属性预测中具有一定优势，同时也揭示了活性断崖问题。

    

    人工智能在药物发现中的应用越来越广泛，其中重要的任务之一就是分子属性预测。虽然分子表征学习的技术如此发达，但其背后的基础问题却未被认真探究。在本研究中，我们使用多种分子表征对一系列代表性模型进行了系统评估。除了常用的MoleculeNet基准数据集外，我们还从ChEMBL数据库和文献中收集了一套与阿片类物质相关的数据集以及两个额外的活性数据集。同时，我们也组装了一系列具有不同规模的描述符数据集来评估模型的性能。总共，我们训练了62,820个模型，其中包括50,220个使用固定表征的模型、4,200个使用SMILES序列的模型和8,400个使用分子图的模型。我们首先进行了数据集分析，并强调了阿片类物质中的活性断崖问题。

    Artificial intelligence (AI) has been widely applied in drug discovery with a major task as molecular property prediction. Despite booming techniques in molecular representation learning, fundamentals underlying molecular property prediction haven't been carefully examined yet. In this study, we conducted a systematic evaluation on a collection of representative models using various molecular representations. In addition to the commonly used MoleculeNet benchmark datasets, we also assembled a suite of opioids-related datasets from ChEMBL and two additional activity datasets from literature. To interrogate the basic predictive power, we also assembled a series of descriptors datasets with varying sizes to evaluate the models' performance. In total, we trained 62,820 models, including 50,220 models on fixed representations, 4,200 models on SMILES sequences and 8,400 models on molecular graphs. We first conducted dataset profiling and highlighted the activity-cliffs issue in the opioids-r
    
[^109]: 简化模型为基础的强化学习：通过一个目标学习表示、隐空间模型和策略

    Simplifying Model-based RL: Learning Representations, Latent-space Models, and Policies with One Objective. (arXiv:2209.08466v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.08466](http://arxiv.org/abs/2209.08466)

    提出了一个单一的、具有自洽性的目标，它共同优化了隐空间模型和策略，以实现高回报，从而简化模型为基础的强化学习方法。

    

    虽然学习环境内部模型的强化学习（RL）方法可能比其模型无关的对手更具样本效率，但学习从高维传感器中模拟原始观察结果的模型可能具有挑战性。

    While reinforcement learning (RL) methods that learn an internal model of the environment have the potential to be more sample efficient than their model-free counterparts, learning to model raw observations from high dimensional sensors can be challenging. Prior work has addressed this challenge by learning low-dimensional representation of observations through auxiliary objectives, such as reconstruction or value prediction. However, the alignment between these auxiliary objectives and the RL objective is often unclear. In this work, we propose a single objective which jointly optimizes a latent-space model and policy to achieve high returns while remaining self-consistent. This objective is a lower bound on expected returns. Unlike prior bounds for model-based RL on policy exploration or model guarantees, our bound is directly on the overall RL objective. We demonstrate that the resulting algorithm matches or improves the sample-efficiency of the best prior model-based and model-fre
    
[^110]: R\'{e}nyi散度深度互相学习

    R\'{e}nyi Divergence Deep Mutual Learning. (arXiv:2209.05732v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.05732](http://arxiv.org/abs/2209.05732)

    本文提出在深度互相学习中使用R\'{e}nyi散度，它能够在不引入大量复杂度的情况下持续提高性能，获得了广泛的实证结果的支持。

    

    本文重审了一种简单而有效的计算范式——深度互相学习（DML）。我们提出使用R\'{e}nyi散度而不是KL散度，这种做法更加灵活、可调，以改善vanilla DML。这种修改能够在有限的附加复杂性下不断提高性能。该范例的收敛性进行了理论分析，并且表明具有恒定学习率的随机梯度下降在非凸优化任务的最坏情况下收敛的偏差为$\mathcal{O}(1)$。

    This paper revisits Deep Mutual Learning (DML), a simple yet effective computing paradigm. We propose using R\'{e}nyi divergence instead of the KL divergence, which is more flexible and tunable, to improve vanilla DML. This modification is able to consistently improve performance over vanilla DML with limited additional complexity. The convergence properties of the proposed paradigm are analyzed theoretically, and Stochastic Gradient Descent with a constant learning rate is shown to converge with $\mathcal{O}(1)$-bias in the worst case scenario for nonconvex optimization tasks. That is, learning will reach nearby local optima but continue searching within a bounded scope, which may help mitigate overfitting. Finally, our extensive empirical results demonstrate the advantage of combining DML and R\'{e}nyi divergence, which further improves generalized models.
    
[^111]: 基于Transformer的大语料库语义相似度分析的认知研究

    A Cognitive Study on Semantic Similarity Analysis of Large Corpora: A Transformer-based Approach. (arXiv:2207.11716v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2207.11716](http://arxiv.org/abs/2207.11716)

    本文通过使用Transformer在U.S Patent Phrase to Phrase Matching Dataset上进行语义相似度分析，提高了算法效率，达到了令人满意的结果。

    

    语义相似度分析和建模是当今自然语言处理许多先驱应用中基本认可的任务。由于顺序模式识别的感知，许多神经网络（如RNN和LSTM）在语义相似度建模方面取得了令人满意的结果。但是，由于它们无法以非顺序方式处理信息，因此这些解决方案被认为效率低下，从而导致上下文提取不当。Transformer因其非顺序数据处理和自我关注等优势而成为最先进的架构。本文使用传统和基于transformer的技术对美国专利短语进行语义相似度分析和建模。我们对四种不同版本的解码增强BERT-DeBERTa进行实验，并通过K折交叉验证来提高其性能。实验结果证明了我们的方法的有效性。

    Semantic similarity analysis and modeling is a fundamentally acclaimed task in many pioneering applications of natural language processing today. Owing to the sensation of sequential pattern recognition, many neural networks like RNNs and LSTMs have achieved satisfactory results in semantic similarity modeling. However, these solutions are considered inefficient due to their inability to process information in a non-sequential manner, thus leading to the improper extraction of context. Transformers function as the state-of-the-art architecture due to their advantages like non-sequential data processing and self-attention. In this paper, we perform semantic similarity analysis and modeling on the U.S Patent Phrase to Phrase Matching Dataset using both traditional and transformer-based techniques. We experiment upon four different variants of the Decoding Enhanced BERT - DeBERTa and enhance its performance by performing K-Fold Cross-Validation. The experimental results demonstrate our me
    
[^112]: 语言模型作为知识嵌入

    Language Models as Knowledge Embeddings. (arXiv:2206.12617v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2206.12617](http://arxiv.org/abs/2206.12617)

    该论文提出了一种使用语言模型来推导知识嵌入的方法LMKE，它旨在提高对丰富的长尾实体的表示能力并解决基于描述的先前方法的问题，实验结果表明该方法在多个基准数据集上实现了最先进的性能。

    

    知识嵌入是通过将实体和关系嵌入到连续向量空间中来表示知识图谱的一种方法。现有的方法主要是基于结构或基于描述。基于结构的方法学习表示，以保留知识图谱的内在结构。它们不能很好地表示现实世界知识图谱中有限结构信息下丰富的长尾实体。基于描述的方法利用文本信息和语言模型。在这个方向上的先前方法几乎无法超越基于结构的方法，并且存在昂贵的负采样和限制性描述需求等问题。在本文中，我们提出了LMKE，采用语言模型来推导知识嵌入，旨在丰富长尾实体的表示并解决基于描述的先前方法的问题。我们用对比学习框架来表述基于描述的知识嵌入学习，以提高训练和评价的效率。实验结果表明，LMKE在多个基准数据集上实现了最先进的性能，超越了基于结构和基于先前描述的方法。

    Knowledge embeddings (KE) represent a knowledge graph (KG) by embedding entities and relations into continuous vector spaces. Existing methods are mainly structure-based or description-based. Structure-based methods learn representations that preserve the inherent structure of KGs. They cannot well represent abundant long-tail entities in real-world KGs with limited structural information. Description-based methods leverage textual information and language models. Prior approaches in this direction barely outperform structure-based ones, and suffer from problems like expensive negative sampling and restrictive description demand. In this paper, we propose LMKE, which adopts Language Models to derive Knowledge Embeddings, aiming at both enriching representations of long-tail entities and solving problems of prior description-based methods. We formulate description-based KE learning with a contrastive learning framework to improve efficiency in training and evaluation. Experimental resul
    
[^113]: DegreEmbed: 将实体嵌入融入逻辑规则学习，用于知识图谱推理

    DegreEmbed: incorporating entity embedding into logic rule learning for knowledge graph reasoning. (arXiv:2112.09933v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2112.09933](http://arxiv.org/abs/2112.09933)

    本文介绍了一种将实体嵌入和逻辑规则挖掘相结合的新模型DegreEmbed，用于知识图谱推理。实验结果表明，DegreEmbed在链接预测和规则提取方面优于现有方法。

    

    知识图谱是一种结构化的现实世界事实表示，它们是智能数据库，将人类知识融入其中，帮助机器模仿人类的问题解决方式。然而，实际的知识图谱通常非常庞大，而且不可避免地存在缺失的事实，从而损害了基于知识图谱推理的问答和推荐系统等应用。知识图谱中的链接预测是通过基于现有知识推理来完成缺失事实的任务。主要有两种研究流派：一种学习实体和关系的低维度嵌入，可以探索潜在模式；另一种通过挖掘逻辑规则获得良好的可解释性。不幸的是，涉及各种类型的实体和关系的现代知识图谱的异质性在先前的研究中没有得到很好的考虑。本文提出了DegreEmbed，一种模型，它将基于嵌入的学习和逻辑规则挖掘相结合，用于知识图谱推理。DegreEmbed将实体嵌入融入逻辑规则学习中，使模型能够同时处理实体属性和关系，并解决知识图谱的异质性问题。基准数据集上的实验结果表明，DegreEmbed在链接预测和规则提取方面优于现有方法。

    Knowledge graphs (KGs), as structured representations of real world facts, are intelligent databases incorporating human knowledge that can help machine imitate the way of human problem solving. However, KGs are usually huge and there are inevitably missing facts in KGs, thus undermining applications such as question answering and recommender systems that are based on knowledge graph reasoning. Link prediction for knowledge graphs is the task aiming to complete missing facts by reasoning based on the existing knowledge. Two main streams of research are widely studied: one learns low-dimensional embeddings for entities and relations that can explore latent patterns, and the other gains good interpretability by mining logical rules. Unfortunately, the heterogeneity of modern KGs that involve entities and relations of various types is not well considered in the previous studies. In this paper, we propose DegreEmbed, a model that combines embedding-based learning and logic rule mining for 
    
[^114]: 统一实例和知识对齐预训练用于基于方面的情感分析

    Unified Instance and Knowledge Alignment Pretraining for Aspect-based Sentiment Analysis. (arXiv:2110.13398v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2110.13398](http://arxiv.org/abs/2110.13398)

    本论文提出了一种统一实例和知识对齐预训练框架，能够有效解决预训练和下游ABSA数据集之间的领域偏移问题，提高了基于方面的情感分析的性能，达到了最先进水平。

    

    基于方面的情感分析（ABSA）旨在确定对某个方面的情感倾向。由于昂贵且有限的标记数据，预训练策略已成为ABSA的事实标准。然而，预训练和下游ABSA数据集之间总是存在严重的领域偏移，直接微调时的知识转移效果不佳，导致下游任务表现亚优化。为了缓解这种领域偏移，我们引入了一个统一的对齐预训练框架，包括实例和知识层面的对齐，将其融入到预训练和微调流程中。具体而言，我们首先设计了一种新颖的分阶段检索采样方法，从大规模的预训练数据集中选择与目标领域相关的实例，从而实现预训练和目标领域实例的对齐（第一阶段）。然后，我们引入了基于知识指导的策略，进一步桥接知识层面的领域差异。我们在基准ABSA数据集上评估了我们的方法，并展示了其超越强基线的卓越性能。我们的方法在多个ABSA数据集上实现了最先进的结果，包括SemEval 2014任务4、SemEval 2015任务12和SemEval 2016任务5。

    Aspect-based Sentiment Analysis (ABSA) aims to determine the sentiment polarity towards an aspect. Because of the expensive and limited labelled data, the pretraining strategy has become the de-facto standard for ABSA. However, there always exists severe domain shift between the pretraining and downstream ABSA datasets, hindering the effective knowledge transfer when directly finetuning and making the downstream task performs sub-optimal. To mitigate such domain shift, we introduce a unified alignment pretraining framework into the vanilla pretrain-finetune pipeline with both instance- and knowledge-level alignments. Specifically, we first devise a novel coarse-to-fine retrieval sampling approach to select target domain-related instances from the large-scale pretraining dataset, thus aligning the instances between pretraining and target domains (First Stage). Then, we introduce a knowledge guidance-based strategy to further bridge the domain gap at the knowledge level. In practice, we 
    
[^115]: 强健的视觉问答需要合成对抗样本来训练

    Counterfactual Samples Synthesizing and Training for Robust Visual Question Answering. (arXiv:2110.01013v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2110.01013](http://arxiv.org/abs/2110.01013)

    本文提出了一种新的模型无关的对抗样本合成和训练（CSST）策略，可以有效解决当前视觉问答模型的语言偏差问题，显著改善模型的性能并具备理想的可视化解释和问题敏感性。

    

    当前的视觉问答模型往往只捕捉训练数据集中的表面语言相关性，并且不能很好地推广到具有不同问答分布的测试集中。为了减少这些语言偏差，最近的视觉问答研究引入了一个辅助的仅问题模型来规范有针对性的VQA模型的训练，并在诊断基准测试中取得了主导地位。但是，由于复杂的模型设计，这些基于集成的方法无法具备理想的VQA模型的两个不可或缺的特征：1）可视化解释：模型在做决策时应依赖于正确的视觉区域。2）问题敏感：模型对问题中的语言变化应具有敏感性。为此，我们提出了一种新的模型无关的对抗样本合成和训练（CSST）策略。经过CSST训练后，VQA模型被迫关注所有关键对象和单词，从而显著改善了视觉问答的性能。

    Today's VQA models still tend to capture superficial linguistic correlations in the training set and fail to generalize to the test set with different QA distributions. To reduce these language biases, recent VQA works introduce an auxiliary question-only model to regularize the training of targeted VQA model, and achieve dominating performance on diagnostic benchmarks for out-of-distribution testing. However, due to complex model design, these ensemble-based methods are unable to equip themselves with two indispensable characteristics of an ideal VQA model: 1) Visual-explainable: The model should rely on the right visual regions when making decisions. 2) Question-sensitive: The model should be sensitive to the linguistic variations in questions. To this end, we propose a novel model-agnostic Counterfactual Samples Synthesizing and Training (CSST) strategy. After training with CSST, VQA models are forced to focus on all critical objects and words, which significantly improves both visu
    
[^116]: 近端ID算法

    The Proximal ID Algorithm. (arXiv:2108.06818v2 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2108.06818](http://arxiv.org/abs/2108.06818)

    近端ID算法是一种将仪器变量和代理相结合的识别因果关系的算法，可以在利用幸运的外部辅助手段的同时，调整未观测因素，对多元系统进行非参数识别。

    

    未观测到的混杂是从观测数据中建立有效因果结论的基本障碍。为解决这一障碍，发展了两种互补的方法：通过幸运的外部辅助手段（例如仪器变量或代理），获得识别；或者通过ID算法，利用图形因果模型中编码的完整数据分布上的马尔可夫限制来进行识别。本文旨在开发前两者的综合方法，以产生多元系统中当前已知的最普适识别算法——近端ID算法。除了能够在ID算法成功的所有情况下获得非参数识别，我们的方法还允许我们系统地利用代理来调整未观测到的混杂因素的存在，否则这些因素会阻止识别。此外，我们还概述了一类因果推断估计策略。

    Unobserved confounding is a fundamental obstacle to establishing valid causal conclusions from observational data. Two complementary types of approaches have been developed to address this obstacle: obtaining identification using fortuitous external aids, such as instrumental variables or proxies, or by means of the ID algorithm, using Markov restrictions on the full data distribution encoded in graphical causal models. In this paper we aim to develop a synthesis of the former and latter approaches to identification in causal inference to yield the most general identification algorithm in multivariate systems currently known -- the proximal ID algorithm. In addition to being able to obtain nonparametric identification in all cases where the ID algorithm succeeds, our approach allows us to systematically exploit proxies to adjust for the presence of unobserved confounders that would have otherwise prevented identification. In addition, we outline a class of estimation strategies for cau
    
[^117]: 开放世界物理领域中新奇性检测的困难性：以愤怒的小鸟为例

    The Difficulty of Novelty Detection in Open-World Physical Domains: An Application to Angry Birds. (arXiv:2106.08670v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2106.08670](http://arxiv.org/abs/2106.08670)

    本文提出了一种定性物理方法来量化开放世界物理领域中新颖性检测的难度，并在愤怒的小鸟游戏中进行了验证，结果显示我们计算的检测困难度与人类用户相符。

    

    在开放世界环境中识别和响应新奇情况是人类认知的一个关键能力，也是人工智能系统面临的持久性问题。在开放世界中，新奇性可以呈现多种形式，也可能很容易或很难被检测到。因此，为了准确评估人工智能系统的新奇性检测能力，有必要研究不同类型新奇性的检测难度。本文提出了一种基于定性物理方法来量化开放世界物理领域中新颖性检测困难度的方法。我们将此方法应用于流行的物理模拟游戏愤怒的小鸟，并进行了针对不同新奇性的用户研究以验证我们的方法。结果表明，我们计算的检测困难度与人类用户的相符合。

    Detecting and responding to novel situations in open-world environments is a key capability of human cognition and is a persistent problem for AI systems. In an open-world, novelties can appear in many different forms and may be easy or hard to detect. Therefore, to accurately evaluate the novelty detection capability of AI systems, it is necessary to investigate how difficult it may be to detect different types of novelty. In this paper, we propose a qualitative physics-based method to quantify the difficulty of novelty detection focusing on open-world physical domains. We apply our method in the popular physics simulation game Angry Birds, and conduct a user study across different novelties to validate our method. Results indicate that our calculated detection difficulties are in line with those of human users.
    
[^118]: 使用双因素扰动评估深度学习分类器的鲁棒性

    Benchmarking Robustness of Deep Learning Classifiers Using Two-Factor Perturbation. (arXiv:2103.03102v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2103.03102](http://arxiv.org/abs/2103.03102)

    本文提出了一种新的基准测试方法和工具，通过双因素扰动来评估深度学习分类器的鲁棒性。使用该方法和工具，作者比较了不同的两因素扰动条件下DL分类器的鲁棒性，并为开发更鲁棒的DL分类器提供了见解。

    

    本文旨在对深度学习（DL）分类器的鲁棒性进行基准测试。我们创新地提出了一种新的基准测试方法来评估DL分类器的鲁棒性，并引入了一种新的四象限统计可视化工具，包括最小准确性、最大准确性、平均准确性和变异系数，用于评估DL分类器的鲁棒性。我们创建了一个全面的69个基准测试图像集，包括一个干净的集合、单因素扰动的集合和双因素扰动条件的集合，以衡量鲁棒的DL分类器。通过收集实验结果，我们首先报告使用双因素扰动图像可以提高DL分类器的鲁棒性和准确性。双因素扰动包括（1）在两个序列中都应用的两个数字扰动（椒盐噪声和高斯噪声），以及（2）一个数字扰动（椒盐噪声）和一个几何扰动（旋转）。除此之外，通过我们的基准测试工具，我们发现不同的DL模型对两因素扰动有不同的鲁棒性行为，暴露了分类器行为的不均匀性和基准测试的必要性。使用我们的基准测试方法和工具，我们比较了不同的两因素扰动条件下DL分类器的鲁棒性，并为开发更鲁棒的DL分类器提供了见解。

    This paper adds to the fundamental body of work on benchmarking the robustness of deep learning (DL) classifiers. We innovate a new benchmarking methodology to evaluate robustness of DL classifiers. Also, we introduce a new four-quadrant statistical visualization tool, including minimum accuracy, maximum accuracy, mean accuracy, and coefficient of variation, for benchmarking robustness of DL classifiers. To measure robust DL classifiers, we created a comprehensive 69 benchmarking image set, including a clean set, sets with single factor perturbations, and sets with two-factor perturbation conditions. After collecting experimental results, we first report that using two-factor perturbed images improves both robustness and accuracy of DL classifiers. The two-factor perturbation includes (1) two digital perturbations (salt & pepper noise and Gaussian noise) applied in both sequences, and (2) one digital perturbation (salt & pepper noise) and a geometric perturbation (rotation) applied in 
    
[^119]: 代表性集成在准线性复杂度下实现协同生命周期学习

    Representation Ensembling for Synergistic Lifelong Learning with Quasilinear Complexity. (arXiv:2004.12908v16 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2004.12908](http://arxiv.org/abs/2004.12908)

    本文提出了一种名为RELL的方法，利用知识蒸馏和知识保持正则化方法，以协同集成在不同任务上独立学习的表示，在准线性复杂度下实现了前向和后向传递。实验结果表明，在各种基准数据集上，RELL的表现优于现有的最先进方法，尤其是在存在灾难性遗忘的情况下，能够显着改善反向传递。

    

    在终身学习中，数据不仅可以用于改进当前任务的性能，还可以用于之前和尚未遇到的任务。传统的机器学习则从空白状态开始，仅针对单个任务使用数据。虽然传统迁移学习算法可以提高未来任务的性能，但在学习新任务后对旧任务的性能下降（称为遗忘）。近期针对连续或终身学习的许多方法都试图在给定新任务的情况下保持对旧任务的性能。但是，仅努力避免忘记将目标定得过低。终身学习的目标不仅应该是提高未来任务（前向传递）的性能，而且还应该是用任何新数据提高过去任务（反向传递）的性能。我们的关键见解是，我们可以协同集成分别在不同任务上独立学习的表示，以实现准线性复杂度下的前向和后向传递。本文提出了一种新方法，称为“终身学习中的表示集成（RELL）”，它集成了知识蒸馏和知识保持正则化方法，以利用不同表示中包含的互补信息。我们的实验表明，RELL在各种基准数据集上都优于现有最先进方法，尤其是在存在灾难性遗忘的情况下实现了显着更好的反向传递。

    In lifelong learning, data are used to improve performance not only on the current task, but also on previously encountered, and as yet unencountered tasks. In contrast, classical machine learning, which we define as, starts from a blank slate, or tabula rasa and uses data only for the single task at hand. While typical transfer learning algorithms can improve performance on future tasks, their performance on prior tasks degrades upon learning new tasks (called forgetting). Many recent approaches for continual or lifelong learning have attempted to maintain performance on old tasks given new tasks. But striving to avoid forgetting sets the goal unnecessarily low. The goal of lifelong learning should be not only to improve performance on future tasks (forward transfer) but also on past tasks (backward transfer) with any new data. Our key insight is that we can synergistically ensemble representations -- that were learned independently on disparate tasks -- to enable both forward and bac
    

