# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [ChatGPT as Data Augmentation for Compositional Generalization: A Case Study in Open Intent Detection.](http://arxiv.org/abs/2308.13517) | 本文通过使用ChatGPT作为数据增强技术，提高了开放意图检测任务中的合成泛化能力，有效改善了模型性能，并在多个基准测试中超过了现有技术。 |
| [^2] | [Does Asking Clarifying Questions Increases Confidence in Generated Code? On the Communication Skills of Large Language Models.](http://arxiv.org/abs/2308.13507) | 通过在生成代码之前提问澄清问题，大型语言模型的代码生成能力可以得到提升，增加了对生成代码的信心。 |
| [^3] | [Attending Generalizability in Course of Deep Fake Detection by Exploring Multi-task Learning.](http://arxiv.org/abs/2308.13503) | 本研究通过使用多任务学习技术，在深度伪造检测中解决了泛化性问题。在经过广泛的实验后，研究发现所提出的检测模型能够准确检测训练中未遇到的篡改方法。 |
| [^4] | [Escaping the Sample Trap: Fast and Accurate Epistemic Uncertainty Estimation with Pairwise-Distance Estimators.](http://arxiv.org/abs/2308.13498) | 本文介绍了使用配对距离估计器对集成模型进行认识不确定性估计的新方法，相比于常用的深度学习方法，该方法能够更快速、更准确地在更大的空间和更高维度上估计认识不确定性。 |
| [^5] | [Open Gaze: An Open-Source Implementation Replicating Google's Eye Tracking Paper.](http://arxiv.org/abs/2308.13495) | 本论文提出了一个仿制谷歌眼动论文的开源实现，重点是通过整合机器学习技术，在智能手机上实现与谷歌论文相当的准确眼动追踪解决方案。 |
| [^6] | [Ultrafast-and-Ultralight ConvNet-Based Intelligent Monitoring System for Diagnosing Early-Stage Mpox Anytime and Anywhere.](http://arxiv.org/abs/2308.13492) | 提出了一种超快超轻的卷积神经网络Fast-MpoxNet，用于早期猴痘诊断。它具有较小的参数量和较快的处理速度，并通过特征融合和辅助损失增强策略提高了诊断性能。 |
| [^7] | [Towards Optimal Head-to-head Autonomous Racing with Curriculum Reinforcement Learning.](http://arxiv.org/abs/2308.13491) | 本论文提出了一种基于课程学习和控制屏障函数的框架，用于优化一对一自主赛车策略。该框架通过逐步过渡到更复杂的真实环境，同时确保智能体的安全性和最优策略。 |
| [^8] | [Temporal Uncertainty Localization to Enable Human-in-the-loop Analysis of Dynamic Contrast-enhanced Cardiac MRI Datasets.](http://arxiv.org/abs/2308.13488) | 本文提出了一种新的时空不确定性度量作为动态质量控制（dQC）工具，用于DNN分割自由呼吸DCE-CMRI数据集，并建立了人在回路框架来改善分割结果。 |
| [^9] | [OCTAL: Graph Representation Learning for LTL Model Checking.](http://arxiv.org/abs/2308.13474) | 本文提出了一个基于图形表示学习的框架，用于解决LTL模型检测问题。实证实验结果表明，该框架相比传统方法具有较高的准确性和速度优势。 |
| [^10] | [Leveraging Knowledge and Reinforcement Learning for Enhanced Reliability of Language Models.](http://arxiv.org/abs/2308.13467) | 本研究通过利用知识和强化学习的方法，实现了一个知识引导的语言模型集成，通过整合外部知识来弥补现有数据集中的信息缺失，从而提高了语言模型的可靠性和准确性。 |
| [^11] | [Learning to Intervene on Concept Bottlenecks.](http://arxiv.org/abs/2308.13453) | 该论文提出了一种扩展了概念瓶颈模型的概念瓶颈记忆模型（CB2M），通过学习将干预推广到不同情境并重新应用先前干预来自动改善模型性能。当没有先前的人类干预信息时，CB2M能够检测错误并请求有针对性的干预。 |
| [^12] | [Representing Timed Automata and Timing Anomalies of Cyber-Physical Production Systems in Knowledge Graphs.](http://arxiv.org/abs/2308.13433) | 本文旨在通过将学习到的时间自动机与关于系统的形式化知识图谱相结合，改进基于模型的网络物理生产系统的异常检测。通过在知识图谱中描述模型和异常，以便操作员更容易解释。 |
| [^13] | [QKSAN: A Quantum Kernel Self-Attention Network.](http://arxiv.org/abs/2308.13422) | QKSAN是一个量子核自注意力网络，通过结合量子核方法和自注意力机制，提高了量子机器学习模型在大规模高维量子数据上的有效性。 |
| [^14] | [SoTaNa: The Open-Source Software Development Assistant.](http://arxiv.org/abs/2308.13416) | SoTaNa是一种开源软件开发助手，利用ChatGPT生成高质量数据并通过参数高效微调方法增强LLaMA模型。它在回答Stack Overflow问题和代码摘要和生成方面展示了出色的能力。 |
| [^15] | [TFDNet: Time-Frequency Enhanced Decomposed Network for Long-term Time Series Forecasting.](http://arxiv.org/abs/2308.13386) | 这篇论文提出了一种称为TFDNet的时频增强分解网络，用于从时频域捕获长期时间序列的基本模式和时间周期性。 |
| [^16] | [On the Impact of Language Selection for Training and Evaluating Programming Language Models.](http://arxiv.org/abs/2308.13354) | 这项研究根据使用CodeBERT模型分析编程语言的表示，发现编程语言之间在标记表示方面存在差异，建议使用这种相似度度量方法来选择跨多种语言的模型。 |
| [^17] | [A Generic Machine Learning Framework for Fully-Unsupervised Anomaly Detection with Contaminated Data.](http://arxiv.org/abs/2308.13352) | 这篇论文介绍了一个完全无监督的机器学习框架，用于处理在训练数据中含有异常样本的异常检测任务。 |
| [^18] | [Squeeze aggregated excitation network.](http://arxiv.org/abs/2308.13343) | 本论文提出了一种名为Squeeze聚合激励网络的新方法，用于在卷积神经网络中学习全局通道表示。该方法通过挤压和聚合激励的方式，在通道级别上连接空间表示，提高了模型性能。 |
| [^19] | [Transforming the Output of Generative Pre-trained Transformer: The Influence of the PGI Framework on Attention Dynamics.](http://arxiv.org/abs/2308.13317) | 本文介绍了一种名为PGI的新方法，在实际商业问题中应用于GPT模型。该方法利用GPT模型的能力来理解复杂的语言结构，并生成上下文相关的回应。实验证实了PGI策略的有效性，并帮助解决了人类智能低度利用的问题。 |
| [^20] | [Asch Meets HRI: Human Conformity to Robot Groups.](http://arxiv.org/abs/2308.13307) | 本文旨在研究在工业机器人背景下的群体动力学和同侪压力。通过将经典的Asch实验引入HRI中，我们将测试参与者在群体机器人同伴给出错误回应时对机器人回应的遵循程度，并关注群体规模、机器人可信度、心理压力和同侪压力的影响。 |
| [^21] | [Dynamic Residual Classifier for Class Incremental Learning.](http://arxiv.org/abs/2308.13305) | 本文提出了一种动态剩余分类器（DRC）来解决类增量学习中的数据不平衡问题。DRC通过分支层合并和模型适应和融合（MAF）流程的结合，在传统CI任务中取得了最先进的结果。 |
| [^22] | [Learning Compact Neural Networks with Deep Overparameterised Multitask Learning.](http://arxiv.org/abs/2308.13300) | 本文提出了一种使用深度超参数化多任务学习来学习紧凑的神经网络的方法，通过在训练中超参数化模型架构并更有效地共享跨任务的超参数化模型参数，以实现更好的优化和泛化。 |
| [^23] | [JAX-LOB: A GPU-Accelerated limit order book simulator to unlock large scale reinforcement learning for trading.](http://arxiv.org/abs/2308.13289) | JAX-LOB是第一个GPU加速的LOB模拟器，可以并行处理数千个订单簿，以较低的处理时间实现大规模强化学习交易，为金融交易研究提供了重要工具。 |
| [^24] | [AtmoRep: A stochastic model of atmosphere dynamics using large scale representation learning.](http://arxiv.org/abs/2308.13280) | 提出了一种称为AtmoRep的大气动力学随机模型，它利用大规模表示学习和人工智能技术来确定复杂的大气动力学的通用描述，从而为各种应用提供技能结果。 |
| [^25] | [Hyperbolic Random Forests.](http://arxiv.org/abs/2308.13279) | 该论文提出了一种在非欧几里得空间中将随机森林推广的方法，并使用水平球重新定义了分割的概念。为了处理多类数据和不平衡实验，论文还提出了一种新的类组合方法。 |
| [^26] | [Integrating LLMs and Decision Transformers for Language Grounded Generative Quality-Diversity.](http://arxiv.org/abs/2308.13278) | 本文提出了一种将LLMs和Decision Transformers集成到语言驱动的生成质量多样性问题中的方法，通过利用大型语言模型增加具有轨迹的自然语言描述的库，并训练一个依赖于这些描述的策略来解决问题。 |
| [^27] | [Knowledge-Driven CoT: Exploring Faithful Reasoning in LLMs for Knowledge-intensive Question Answering.](http://arxiv.org/abs/2308.13259) | 本文提出了一个名为知识驱动的思维链（KD-CoT）的框架，用于验证和修改LLMs中的推理过程，通过与外部知识的交互来解决幻觉和错误传播的问题。 |
| [^28] | [Decoding Natural Images from EEG for Object Recognition.](http://arxiv.org/abs/2308.13234) | 本文提出了一种自我监督的框架，从EEG信号中学习图像表示，并采用对比学习来对齐这两种模态。通过在最广泛的EEG图像数据集上的实验证明了该方法的优越性能和生物合理性。 |
| [^29] | [MultiCapCLIP: Auto-Encoding Prompts for Zero-Shot Multilingual Visual Captioning.](http://arxiv.org/abs/2308.13218) | MultiCapCLIP是一种零样本的方法，可以生成不同场景和语言的视觉字幕，无需下游数据集中的任何标记。在训练阶段，它通过检索概念提示并自编码学习写作风格来生成字幕。在测试阶段，它直接利用视觉数据进行检索。 |
| [^30] | [Physics-Inspired Neural Graph ODE for Long-term Dynamical Simulation.](http://arxiv.org/abs/2308.13212) | 提出一种受物理启发的神经图ODE算法（PINGO），用于模拟和建模多对象物理系统的长期动态，并解决了当前模型泛化能力差的问题。 |
| [^31] | [Formalising Natural Language Quantifiers for Human-Robot Interactions.](http://arxiv.org/abs/2308.13192) | 本文提出了一种在人机交互中形式化自然语言量词的方法，并设计了一个端到端系统以实现自然语言输入到逻辑表示的转换和评估，从而实现对机器人的控制。 |
| [^32] | [Chunk, Align, Select: A Simple Long-sequence Processing Method for Transformers.](http://arxiv.org/abs/2308.13191) | 这种方法提出了一种简单的框架，使得transformer能够处理更长的序列，同时计算和内存成本与输入序列长度线性增长。 |
| [^33] | [Falcon: Accelerating Homomorphically Encrypted Convolutions for Efficient Private Mobile Network Inference.](http://arxiv.org/abs/2308.13189) | Falcon是一种用于加速基于同态加密的卷积的密集打包算法，通过优化打包密度和降低通信开销，实现了在运算器级别上超过15.6倍的延迟降低，并且在网络级别上提供了更高的加密推断速度。 |
| [^34] | [Structural Cycle GAN for Virtual Immunohistochemistry Staining of Gland Markers in the Colon.](http://arxiv.org/abs/2308.13182) | 本研究提出了一种新的生成模型SC-GAN，用于合成H&E图像和IHC染色之间的转换。该模型利用边缘结构信息和注意力模块，增强了特征定位并保留了上下文信息。 |
| [^35] | [DAG-ACFL: Asynchronous Clustered Federated Learning based on DAG-DLT.](http://arxiv.org/abs/2308.13158) | DAG-ACFL是一种基于DAG-DLT的异步聚类联邦学习框架，通过选择相似分布的客户端模型来聚合全局模型，同时采用自适应的tip选择算法降低通信和存储成本。 |
| [^36] | [Diverse, Top-k, and Top-Quality Planning Over Simulators.](http://arxiv.org/abs/2308.13147) | 本文介绍了一种使用蒙特卡洛树搜索（MCTS）的新颖方法，用于多样化、前k个和高质量的规划模拟器上的规划问题。通过从预生成的搜索树中提取有界规划集，并评估路径相对质量的度量标准，该方法在只有黑箱模拟模型的问题中表现出色，并提出了修改MCTS算法以增加多样性的建议。 |
| [^37] | [A Survey of Diffusion Based Image Generation Models: Issues and Their Solutions.](http://arxiv.org/abs/2308.13142) | 这项研究对基于扩散的图像生成模型进行了综述，总结了存在的问题以及当前的解决方案。 |
| [^38] | [Hybrid Genetic Algorithm and Hill Climbing Optimization for the Neural Network.](http://arxiv.org/abs/2308.13099) | 本文提出了一种混合遗传算法和爬山算法的模型，用于在CIFAR-100数据集上优化卷积神经网络。该模型利用遗传算法进行全局搜索和探索搜索空间，在此基础上使用爬山算法进行局部优化，并引入变异操作以避免局部最优解。评估结果表明该模型相较于标准的遗传算法和爬山算法在性能上有所提升。 |
| [^39] | [Racing Towards Reinforcement Learning based control of an Autonomous Formula SAE Car.](http://arxiv.org/abs/2308.13088) | 本文介绍了使用深度强化学习控制自主Formula SAE赛车的研究。实验结果表明，这种方法可以在模拟环境中成功学习赛车驾驶，并成功应用于真实赛道上的物理平台。该研究提供对方法的局限性和未来研究方向的指导。 |
| [^40] | [Multivariate Time Series Anomaly Detection: Fancy Algorithms and Flawed Evaluation Methodology.](http://arxiv.org/abs/2308.13068) | 多元时间序列异常检测是一个研究领域，但目前存在组织不够有序和评估协议有缺陷的问题。文章评估了许多最近算法的性能，并指出了针对多元时间序列异常检测的评估协议存在的问题及如何缓解这些问题的方法。 |
| [^41] | [Causal Parrots: Large Language Models May Talk Causality But Are Not Causal.](http://arxiv.org/abs/2308.13067) | 大型语言模型（LLM）不能具备因果性，它们只是重复嵌入在数据中的因果知识。 |
| [^42] | [Multi-BERT for Embeddings for Recommendation System.](http://arxiv.org/abs/2308.13050) | 本文提出了一种使用多种最先进的自然语言处理模型来生成文档嵌入的方法，并在图书推荐任务中取得了比单一模型更好的性能。 |
| [^43] | [Federated Learning of Causal Effects from Incomplete Observational Data.](http://arxiv.org/abs/2308.13047) | 我们提出了一种联邦学习的方法，可以从多个分布式和不完整的数据源中进行因果推断，估计因果效应并解决因为缺失值引入的偏差问题。 |
| [^44] | [Financial News Analytics Using Fine-Tuned Llama 2 GPT Model.](http://arxiv.org/abs/2308.13032) | 本研究通过精细调整的Llama 2模型实现了金融新闻的多任务分析，包括文本分析、摘要和情感提取等。实验结果显示，提取的命名实体情感可以作为有监督机器学习模型的预测特征。 |
| [^45] | [Generalizable Zero-Shot Speaker Adaptive Speech Synthesis with Disentangled Representations.](http://arxiv.org/abs/2308.13007) | 本研究提出了一种通用的零样本说话者自适应语音合成模型，通过解耦表示学习来改善模型在未知说话者方面的泛化能力。 |
| [^46] | [Spherical Vision Transformer for 360-degree Video Saliency Prediction.](http://arxiv.org/abs/2308.13004) | 本论文提出了一种用于360度视频显著性预测的球形视觉Transformer模型，通过引入切线图像表示和球形几何感知的机制，以及一种无监督正则化项来减少伪像，实现了有效的全景视频理解和显著性预测。 |
| [^47] | [Perimeter Control with Heterogeneous Cordon Signal Behaviors: A Semi-Model Dependent Reinforcement Learning Approach.](http://arxiv.org/abs/2308.12985) | 本文引入了一个基于半模型依赖的多智能体强化学习（MARL）框架来实现具有异构警戒信号行为的周界控制。 |
| [^48] | [A Survey of AI Music Generation Tools and Models.](http://arxiv.org/abs/2308.12982) | 这篇论文提供了一个全面的AI音乐生成工具综述，分类了基于参数、文本和图像的音乐生成方法，并详细介绍了这些工具的优点、局限性以及选择过程中需要考虑的因素。 |
| [^49] | [An approach based on Open Research Knowledge Graph for Knowledge Acquisition from scientific papers.](http://arxiv.org/abs/2308.12981) | 这项研究利用开放研究知识图谱作为计算机辅助工具，从科学论文中组织关键洞察，以加速知识获取的过程。 |
| [^50] | [Can Linguistic Knowledge Improve Multimodal Alignment in Vision-Language Pretraining?.](http://arxiv.org/abs/2308.12898) | 本论文研究了语言知识在多模态对齐中的作用，设计并发布了一个多模态对齐探测基准来检测关键的语言组成部分。 |
| [^51] | [Diffusion Language Models Can Perform Many Tasks with Scaling and Instruction-Finetuning.](http://arxiv.org/abs/2308.12219) | 本文研究表明，通过扩展扩散语言模型的数据、规模和任务，可以有效使其成为强大的语言学习者。实验证明，扩展扩散语言模型在解决通用语言任务方面能够持续提高性能。 |
| [^52] | [Exploring the Optimization Objective of One-Class Classification for Anomaly Detection.](http://arxiv.org/abs/2308.11898) | 本研究探索了一类分类（OCC）的优化目标，发现在适当的范数空间中，任何空间都可以作为超球心的等效替代，而不依赖于训练样本的分布假设。 |
| [^53] | [Self-Deception: Reverse Penetrating the Semantic Firewall of Large Language Models.](http://arxiv.org/abs/2308.11521) | 这篇论文研究了大型语言模型的越狱问题，并提出了一种自动越狱方法，介绍了语义防火墙的概念和三种技术实现方法。 |
| [^54] | [Bayesian polynomial neural networks and polynomial neural ordinary differential equations.](http://arxiv.org/abs/2308.10892) | 本研究提出了贝叶斯推断方法来改善多项式神经网络和多项式神经常微分方程在方程恢复问题中的表现，其中拉普拉斯近似是最佳的方法。这种方法可以推广到更广泛的符号神经网络类别。 |
| [^55] | [Deep Reinforcement Learning for Artificial Upwelling Energy Management.](http://arxiv.org/abs/2308.10199) | 通过使用深度强化学习算法，我们提出了一种新颖的能源管理方法来优化操作人工上涌系统。通过将问题建模为马尔可夫决策过程，并结合分位网络和深度竞争网络的思想，我们的方法在提高能源效率方面取得了显著成效。 |
| [^56] | [Beyond Sharing: Conflict-Aware Multivariate Time Series Anomaly Detection.](http://arxiv.org/abs/2308.08915) | 这篇论文提出了一种冲突感知的多变量时间序列异常检测算法，该算法通过为每个指标提供独特的结构来缓解指标回归目标之间的冲突。 |
| [^57] | [How to Mask in Error Correction Code Transformer: Systematic and Double Masking.](http://arxiv.org/abs/2308.08128) | 该论文介绍了在纠错码变压器中使用系统化编码和双重遮蔽的方法，以提高性能和减少计算复杂性。 |
| [^58] | [Face Encryption via Frequency-Restricted Identity-Agnostic Attacks.](http://arxiv.org/abs/2308.05983) | 通过受限频率的身份不可知攻击进行人脸加密，解决了使用外部扰动加密人脸图像的隐私保护问题，并提出了一种弱黑盒场景下可行的解决方案。 |
| [^59] | [Cumulative Reasoning With Large Language Models.](http://arxiv.org/abs/2308.04371) | 本文提出了一种名为累积推理（CR）的新方法，利用语言模型以累积和迭代的方式模拟人类思维过程，通过将任务分解为较小的组件，简化问题解决过程，取得了优于现有方法的性能，并在逻辑推理和24点游戏中实现了显著提升。 |
| [^60] | [Replace Scoring with Arrangement: A Contextual Set-to-Arrangement Framework for Learning-to-Rank.](http://arxiv.org/abs/2308.02860) | 这篇论文提出了一种新的学习排序框架，名为STARank，它通过直接生成候选项目排列来替代个别评分和排序操作，并且是端到端可微分的。 |
| [^61] | [ChatMOF: An Autonomous AI System for Predicting and Generating Metal-Organic Frameworks.](http://arxiv.org/abs/2308.01423) | ChatMOF是一种自主AI系统，用于预测和生成金属-有机骨架。通过利用大规模语言模型，它能够从文本输入中提取关键细节，并提供适当的回应。该系统通过组合代理、工具包和评估器的核心组件，实现了数据检索、性质预测和结构生成等多个任务。研究进一步展示了在材料科学中使用大型语言模型的优势和潜力。 |
| [^62] | [BubbleML: A Multi-Physics Dataset and Benchmarks for Machine Learning.](http://arxiv.org/abs/2307.14623) | BubbleML是一个用于机器学习的多物理数据集，通过物理驱动模拟获得准确的地面真实信息，并在各种沸腾场景中验证了其可靠性和潜力。 |
| [^63] | [TMR-RD: Training-based Model Refinement and Representation Disagreement for Semi-Supervised Object Detection.](http://arxiv.org/abs/2307.13755) | 在半监督目标检测中，本文提出了基于训练的模型精化(TMR)阶段和表示分歧(RD)策略，用来解决伪标签噪声和教师-学生模型的一致性问题。TMR阶段通过轻量级缩放操作优化模型权重，防止过度拟合或遗忘学到的模式；RD策略帮助保持模型的差异，鼓励学生模型探索互补的表示。 |
| [^64] | [ACTI at EVALITA 2023: Overview of the Conspiracy Theory Identification Task.](http://arxiv.org/abs/2307.06954) | ACTI在EVALITA 2023中的阴谋论辨识任务共有15支团队参与，通过使用大型语言模型判断阴谋内容和分类，得出了关于利用这些模型抵制在在线平台传播错误信息的结论。 |
| [^65] | [IntelliGraphs: Datasets for Benchmarking Knowledge Graph Generation.](http://arxiv.org/abs/2307.06698) | IntelliGraphs是一组新的知识图谱数据集，用于评估知识图谱生成。其中包含具有逻辑规则表达的语义的子图，用于评估子图推断的模型。 |
| [^66] | [To Spike or Not To Spike: A Digital Hardware Perspective on Deep Learning Acceleration.](http://arxiv.org/abs/2306.15749) | 神经形态计算旨在通过仿真脑部操作来提高深度学习模型的效率，但是在SNNs的高效硬件后端设计上仍需进一步研究。 |
| [^67] | [Modern Constraint Programming Education: Lessons for the Future.](http://arxiv.org/abs/2306.13676) | 本文探讨了现代约束编程教育的前景，重点关注在线和虚拟课程，并总结了乔治亚理工学院的约束编程课程的重要收获，提出了一些教学和推广方法以及组织变革的想法，以促进约束编程教育的长期发展。 |
| [^68] | [Domain-specific ChatBots for Science using Embeddings.](http://arxiv.org/abs/2306.10067) | 本论文演示如何利用现有方法和软件工具结合嵌入技术设计面向科学领域的聊天机器人，该机器人能够处理科学文献，提供特定领域的上下文信息，并在初步研究辅助知识方面为物理科学家提供帮助。 |
| [^69] | [SDR-GAIN: A High Real-Time Occluded Pedestrian Pose Completion Method for Autonomous Driving.](http://arxiv.org/abs/2306.03538) | SDR-GAIN是一种用于解决行人姿态中部分遮挡问题的关键点补全方法，它通过对不完整的关键点进行降维，统一特征分布，并使用GAN框架的两种生成模型来完成姿态的补全。该方法的实验表明性能优于基本的GAIN框架。 |
| [^70] | [Risk-Aware Reward Shaping of Reinforcement Learning Agents for Autonomous Driving.](http://arxiv.org/abs/2306.03220) | 本研究针对自动驾驶中RL代理的安全性问题，提出了一种增加风险感知的奖励形成方法来提高其训练和测试性能。该方法通过额外的重塑奖励项来鼓励探索并惩罚风险驾驶行为，证明其在各种RL代理中具有优势。 |
| [^71] | [SpeechGen: Unlocking the Generative Power of Speech Language Models with Prompts.](http://arxiv.org/abs/2306.02207) | 本文探索了一个名为SpeechGen的统一框架，通过提示调节，解锁了语音语言模型的生成能力，成功地实现了直接适应连续语音到离散标记的任务，使得语音生成成为可能。 |
| [^72] | [In Defense of Pure 16-bit Floating-Point Neural Networks.](http://arxiv.org/abs/2305.10947) | 本文探讨了纯16位浮点神经网络的被忽视的效率，提供了理论分析来探讨16位和32位模型的差异，并可以定量解释16位模型与其32位对应物之间的条件。 |
| [^73] | [Region-Aware Pretraining for Open-Vocabulary Object Detection with Vision Transformers.](http://arxiv.org/abs/2305.07011) | 本文提出了一种基于视觉变压器的对比图像-文本预训练方法，针对开放词汇的物体检测任务，采用区域感知预训练、聚焦损失和新颖物体提案等技术，在LVIS上取得了32.1$AP_r$的最佳效果。 |
| [^74] | [CryCeleb: A Speaker Verification Dataset Based on Infant Cry Sounds.](http://arxiv.org/abs/2305.00969) | CryCeleb是一个基于婴儿哭声的说话人认证数据集，包括超过6小时的手动分割哭声，可用于研究婴儿哭声分析。 |
| [^75] | [Q-learning Based System for Path Planning with UAV Swarms in Obstacle Environments.](http://arxiv.org/abs/2303.17655) | 本文提出了一种基于强化学习的Q学习算法，能够在有障碍物的环境中通过人工神经网络进行路径规划优化，从而减少能量消耗和人力成本。 |
| [^76] | [ViewRefer: Grasp the Multi-view Knowledge for 3D Visual Grounding with GPT and Prototype Guidance.](http://arxiv.org/abs/2303.16894) | 本文提出了ViewRefer，这是一个多视角的三维视觉定位框架，利用大规模语言模型和多视角原型，从文本和3D模态中获取视角知识并增强框架的表现。 |
| [^77] | [Learning Causal Attributions in Neural Networks: Beyond Direct Effects.](http://arxiv.org/abs/2303.13850) | 本文提出了一种方法来估计和维护神经网络模型中输入-输出属性的因果关系，不仅考虑直接影响，还能考虑间接影响。此方法能够在训练神经网络模型的同时有效地量化因果归因，实验结果表明该方法能够有效地学习因果归因。 |
| [^78] | [PDSketch: Integrated Planning Domain Programming and Learning.](http://arxiv.org/abs/2303.05501) | 本文通过PDSketch语言和可训练的神经网络，实现了模型的学习和在线规划，加速了机器人的灵活性和通用性。 |
| [^79] | [Internally Rewarded Reinforcement Learning.](http://arxiv.org/abs/2302.00270) | 这项研究探讨了一类强化学习问题，其中策略的奖励信号由与之相关且同时优化的判别器生成，导致学习过程不稳定。实验结果表明，修剪线性奖励函数可以稳定训练过程。 |
| [^80] | [SceneRF: Self-Supervised Monocular 3D Scene Reconstruction with Radiance Fields.](http://arxiv.org/abs/2212.02501) | SceneRF利用自监督学习结合NeRF的辐射场技术，无需深度监督，只需使用图像序列训练，可以高效处理大场景，能够生成新的深度视图并进行3D场景重建，性能在室内外场景中优于所有基线方法。 |
| [^81] | [WSSL: Weighted Self-supervised Learning Framework For Image-inpainting.](http://arxiv.org/abs/2211.13856) | 我们提出了一种加权自监督学习框架用于图像修复，通过学习多个权重的先验任务特征，并利用重建损失和感知损失函数来生成更具吸引力的图像。 |
| [^82] | [Distinctive Self-Similar Object Detection.](http://arxiv.org/abs/2211.10995) | 本文提出了一种独特的自相似物体检测方法，通过使用火和烟的自相似特征来解决其形状多样性的问题，并设计了一种半监督方法来评估和提高物体检测精度。 |
| [^83] | [Distributed Graph Neural Network Training: A Survey.](http://arxiv.org/abs/2211.00216) | 这项调查研究了分布式图神经网络训练中的挑战，并提出了解决方案来优化特征通信、模型精度和分布式同步。 |
| [^84] | [Augmenting Reinforcement Learning with Transformer-based Scene Representation Learning for Decision-making of Autonomous Driving.](http://arxiv.org/abs/2208.12263) | 本研究提出了Scene-Rep Transformer来提升强化学习决策能力，通过改进场景表示编码和顺序预测潜在蒸馏。采用多阶段Transformer编码器建模交互意识和意图意识，并使用顺序潜在Transformer进行自监督学习，加速训练和减少探索空间。 |
| [^85] | [A Perceptually Optimized and Self-Calibrated Tone Mapping Operator.](http://arxiv.org/abs/2206.09146) | 本文开发了一种两步神经网络TMO，具有自校准和感知优化功能，可以将HDR图像压缩到LDR图像，同时通过感知度量实现了灵敏的质量优化。 |
| [^86] | [Grammar-Based Grounded Lexicon Learning.](http://arxiv.org/abs/2202.08806) | 基于语法的基础词汇学习（G2L2）是一种从基础数据中学习语言含义表示的方法，通过将单词映射到语法类型和神经符号语义程序，利用基于语法的组合推导句子的含义，最终可以在基础输入上执行。 |
| [^87] | [A Simplified Variant of G\"odel's Ontological Argument.](http://arxiv.org/abs/2202.06264) | 本论文提出了G\"odel的本体论证的简化变体，该变体在基本模态逻辑K或KT中已经是有效的，避免了复杂的谓词，并且展示了人机交互在计算形而上学中的应用。 |
| [^88] | [Graph-Based Recommendation System Enhanced with Community Detection.](http://arxiv.org/abs/2201.03622) | 本文提出了一个基于图的推荐系统，利用数学和统计方法确定标签的相似性，包括词汇相似性和共现解决方案，并考虑了标签分配的时间，以提高推荐的准确性。 |
| [^89] | [Fact Check: Analyzing Financial Events from Multilingual News Sources.](http://arxiv.org/abs/2106.15221) | FactCheck in finance是一个基于深度学习模型的网络新闻聚合器，能够从多语言新闻源中提取重要的金融事件，并通过无监督聚类方法对其进行聚类。通过使用Transformer-based事实核查器来检查新闻文章的可信度，该系统显示出优于几个强基准模型的性能。 |
| [^90] | [Symbolic Relational Deep Reinforcement Learning based on Graph Neural Networks and Autoregressive Policy Decomposition.](http://arxiv.org/abs/2009.12462) | 这篇论文介绍了一种基于图神经网络和自回归策略分解的深度强化学习框架，能够处理符号关系问题的可变状态和动作空间，并在多个领域展现了广泛的适用性和令人印象深刻的零-shot泛化能力。 |

# 详细

[^1]: ChatGPT作为合成泛化的数据增强方法在开放意图检测中的案例研究

    ChatGPT as Data Augmentation for Compositional Generalization: A Case Study in Open Intent Detection. (arXiv:2308.13517v1 [cs.CL])

    [http://arxiv.org/abs/2308.13517](http://arxiv.org/abs/2308.13517)

    本文通过使用ChatGPT作为数据增强技术，提高了开放意图检测任务中的合成泛化能力，有效改善了模型性能，并在多个基准测试中超过了现有技术。

    

    开放意图检测是自然语言理解的关键方面，涉及对用户生成的文本中以前未见过的意图进行识别。虽然在这个领域取得了一些进展，但是在处理语言组成成分的新组合方面仍面临挑战，这对于合成泛化至关重要。本文提出了一个案例研究，探讨了在开放意图检测任务中将ChatGPT作为数据增强技术来提高合成泛化能力。我们首先讨论了现有基准在评估这个问题时的局限性，强调了构建数据集来解决开放意图检测任务中的合成泛化问题的需求。通过将ChatGPT生成的合成数据纳入训练过程中，我们证明了我们的方法可以有效提高模型性能。对多个基准进行了严格评估，结果显示我们的方法优于现有技术，并显著提高了开放意图检测的性能。

    Open intent detection, a crucial aspect of natural language understanding, involves the identification of previously unseen intents in user-generated text. Despite the progress made in this field, challenges persist in handling new combinations of language components, which is essential for compositional generalization. In this paper, we present a case study exploring the use of ChatGPT as a data augmentation technique to enhance compositional generalization in open intent detection tasks. We begin by discussing the limitations of existing benchmarks in evaluating this problem, highlighting the need for constructing datasets for addressing compositional generalization in open intent detection tasks. By incorporating synthetic data generated by ChatGPT into the training process, we demonstrate that our approach can effectively improve model performance. Rigorous evaluation of multiple benchmarks reveals that our method outperforms existing techniques and significantly enhances open inte
    
[^2]: 提问澄清问题是否增加了生成代码的信心？关于大型语言模型的沟通能力的研究

    Does Asking Clarifying Questions Increases Confidence in Generated Code? On the Communication Skills of Large Language Models. (arXiv:2308.13507v1 [cs.SE])

    [http://arxiv.org/abs/2308.13507](http://arxiv.org/abs/2308.13507)

    通过在生成代码之前提问澄清问题，大型语言模型的代码生成能力可以得到提升，增加了对生成代码的信心。

    

    大型语言模型(LLMs)显著提高了代码生成任务的能力。然而，LLMs在成为顶级软件工程师方面仍存在差距。基于观察到顶级软件工程师通常会提出澄清问题以减少需求和编码解决方案的不确定性，我们认为在代码生成任务中，LLMs也应该采用同样的方法。通过在生成最终代码之前提出深入的问题，可以减轻使用LLMs进行编程所面临的挑战，如意图规范不明确、计算思维不足和代码质量不理想。这反过来增加了对生成代码的自信。在这项工作中，我们探讨如何利用更好的沟通技巧来增加对生成代码的信心。我们提出了一个以沟通为中心的过程，利用LLM生成的沟通器来识别高度不确定或信心低的问题。

    Large language models (LLMs) have significantly improved the ability to perform tasks in the field of code generation. However, there is still a gap between LLMs being capable coders and being top-tier software engineers. Based on the observation that top-level software engineers often ask clarifying questions to reduce ambiguity in both requirements and coding solutions, we argue that the same should be applied to LLMs for code generation tasks. By asking probing questions in various topics before generating the final code, the challenges of programming with LLMs, such as unclear intent specification, lack of computational thinking, and undesired code quality, may be alleviated. This, in turn, increases confidence in the generated code. In this work, we explore how to leverage better communication skills to achieve greater confidence in generated code. We propose a communication-centered process that uses an LLM-generated communicator to identify issues with high ambiguity or low conf
    
[^3]: 通过探究多任务学习，解决深度伪造检测中的泛化性问题

    Attending Generalizability in Course of Deep Fake Detection by Exploring Multi-task Learning. (arXiv:2308.13503v1 [cs.CV])

    [http://arxiv.org/abs/2308.13503](http://arxiv.org/abs/2308.13503)

    本研究通过使用多任务学习技术，在深度伪造检测中解决了泛化性问题。在经过广泛的实验后，研究发现所提出的检测模型能够准确检测训练中未遇到的篡改方法。

    

    本研究探索了多种多任务学习技术的方式，旨在在跨操控场景中将视频分类为原始或被篡改，以解决深度伪造场景中的泛化性问题。我们在FaceForensics++数据集上进行了广泛的实验，该数据集包含1000个原始视频，通过四种不同的技术进行了篡改，共计5000个视频。我们对多任务学习和对比学习技术进行了广泛的实验，这些技术在文献中已经被广泛研究，具有泛化性的好处。可以得出结论：所提出的检测模型非常泛化，即与最先进的方法相比，能够准确检测在训练中未遇到的篡改方法。

    This work explores various ways of exploring multi-task learning (MTL) techniques aimed at classifying videos as original or manipulated in cross-manipulation scenario to attend generalizability in deep fake scenario. The dataset used in our evaluation is FaceForensics++, which features 1000 original videos manipulated by four different techniques, with a total of 5000 videos. We conduct extensive experiments on multi-task learning and contrastive techniques, which are well studied in literature for their generalization benefits. It can be concluded that the proposed detection model is quite generalized, i.e., accurately detects manipulation methods not encountered during training as compared to the state-of-the-art.
    
[^4]: 逃离样本陷阱：使用配对距离估计器快速准确地估计认识不确定性

    Escaping the Sample Trap: Fast and Accurate Epistemic Uncertainty Estimation with Pairwise-Distance Estimators. (arXiv:2308.13498v1 [cs.LG])

    [http://arxiv.org/abs/2308.13498](http://arxiv.org/abs/2308.13498)

    本文介绍了使用配对距离估计器对集成模型进行认识不确定性估计的新方法，相比于常用的深度学习方法，该方法能够更快速、更准确地在更大的空间和更高维度上估计认识不确定性。

    

    本文介绍了一种使用配对距离估计器（PaiDEs）对集成模型进行认识不确定性估计的新方法。这些估计器利用模型组件之间的配对距离来建立熵的边界，并将这些边界作为基于信息准则的估计值。与最近基于样本的蒙特卡洛估计器用于认识不确定性估计的深度学习方法不同，PaiDEs能够在更大的空间（最多100倍）上以更快的速度（最多100倍）估计认识不确定性，并在更高维度上具有更准确的性能。为了验证我们的方法，我们进行了一系列用于评估认识不确定性估计的实验：一维正弦数据，摆动物体（Pendulum-v0），跳跃机器人（Hopper-v2），蚂蚁机器人（Ant-v2）和人形机器人（Humanoid-v2）。对于每个实验设置，我们应用了主动学习框架来展示PaiDEs在认识不确定性估计中的优势。

    This work introduces a novel approach for epistemic uncertainty estimation for ensemble models using pairwise-distance estimators (PaiDEs). These estimators utilize the pairwise-distance between model components to establish bounds on entropy and uses said bounds as estimates for information-based criterion. Unlike recent deep learning methods for epistemic uncertainty estimation, which rely on sample-based Monte Carlo estimators, PaiDEs are able to estimate epistemic uncertainty up to 100$\times$ faster, over a larger space (up to 100$\times$) and perform more accurately in higher dimensions. To validate our approach, we conducted a series of experiments commonly used to evaluate epistemic uncertainty estimation: 1D sinusoidal data, Pendulum-v0, Hopper-v2, Ant-v2 and Humanoid-v2. For each experimental setting, an Active Learning framework was applied to demonstrate the advantages of PaiDEs for epistemic uncertainty estimation.
    
[^5]: 开放注视：一个仿制谷歌眼动论文的开源实现

    Open Gaze: An Open-Source Implementation Replicating Google's Eye Tracking Paper. (arXiv:2308.13495v1 [cs.CV])

    [http://arxiv.org/abs/2308.13495](http://arxiv.org/abs/2308.13495)

    本论文提出了一个仿制谷歌眼动论文的开源实现，重点是通过整合机器学习技术，在智能手机上实现与谷歌论文相当的准确眼动追踪解决方案。

    

    眼动已经成为视觉研究、语言分析和可用性评估等不同领域的重要工具。然而，大多数先前的研究集中在使用专门的、昂贵的眼动追踪硬件的扩展式桌面显示器上。尽管智能手机的普及率和使用频率很高，但对于智能手机上的眼球移动模式却鲜有见解。在本文中，我们提出了一个基于智能手机的开源注视追踪实现，模拟了谷歌论文提出的方法论（其源代码仍然是专有的）。我们的重点是在不需要额外硬件的情况下达到与谷歌论文方法相当的准确度。通过整合机器学习技术，我们揭示了一种本地于智能手机的准确眼动追踪解决方案。我们的方法展示了与最先进的移动眼动追踪器相当的精度。

    Eye tracking has been a pivotal tool in diverse fields such as vision research, language analysis, and usability assessment. The majority of prior investigations, however, have concentrated on expansive desktop displays employing specialized, costly eye tracking hardware that lacks scalability. Remarkably little insight exists into ocular movement patterns on smartphones, despite their widespread adoption and significant usage. In this manuscript, we present an open-source implementation of a smartphone-based gaze tracker that emulates the methodology proposed by a GooglePaper (whose source code remains proprietary). Our focus is on attaining accuracy comparable to that attained through the GooglePaper's methodology, without the necessity for supplementary hardware. Through the integration of machine learning techniques, we unveil an accurate eye tracking solution that is native to smartphones. Our approach demonstrates precision akin to the state-of-the-art mobile eye trackers, which 
    
[^6]: 超快超轻卷积神经网络智能监测系统用于在任何时间和地点诊断早期猴痘

    Ultrafast-and-Ultralight ConvNet-Based Intelligent Monitoring System for Diagnosing Early-Stage Mpox Anytime and Anywhere. (arXiv:2308.13492v1 [cs.CV])

    [http://arxiv.org/abs/2308.13492](http://arxiv.org/abs/2308.13492)

    提出了一种超快超轻的卷积神经网络Fast-MpoxNet，用于早期猴痘诊断。它具有较小的参数量和较快的处理速度，并通过特征融合和辅助损失增强策略提高了诊断性能。

    

    由于缺乏更高效的猴痘诊断工具，其传播仍然未受控制，给全球健康带来了巨大的挑战。虽然相关研究已经证明了深度学习模型在猴痘诊断方面的高效性，但是对于早期猴痘的推理速度、参数大小和诊断性能的忽视使得这些模型无法在现实世界中应用。为了解决这些问题，我们提出了一种超快超轻的网络，名为Fast-MpoxNet。Fast-MpoxNet只有0.27M个参数，并且可以在CPU上以每秒68帧的速度处理输入图像。为了克服小模型容量带来的诊断性能限制，它集成了基于注意力的特征融合模块和多个辅助损失增强策略，以更好地检测微小的图像变化和优化权重。通过转移学习和五折交叉验证，Fast-MpoxNet实现了94.26%的准确率。

    Due to the lack of more efficient diagnostic tools for monkeypox, its spread remains unchecked, presenting a formidable challenge to global health. While the high efficacy of deep learning models for monkeypox diagnosis has been demonstrated in related studies, the overlook of inference speed, the parameter size and diagnosis performance for early-stage monkeypox renders the models inapplicable in real-world settings. To address these challenges, we proposed an ultrafast and ultralight network named Fast-MpoxNet. Fast-MpoxNet possesses only 0.27M parameters and can process input images at 68 frames per second (FPS) on the CPU. To counteract the diagnostic performance limitation brought about by the small model capacity, it integrates the attention-based feature fusion module and the multiple auxiliary losses enhancement strategy for better detecting subtle image changes and optimizing weights. Using transfer learning and five-fold cross-validation, Fast-MpoxNet achieves 94.26% Accuracy
    
[^7]: 实现优化的一对一自主赛车策略的课程强化学习的方式

    Towards Optimal Head-to-head Autonomous Racing with Curriculum Reinforcement Learning. (arXiv:2308.13491v1 [cs.RO])

    [http://arxiv.org/abs/2308.13491](http://arxiv.org/abs/2308.13491)

    本论文提出了一种基于课程学习和控制屏障函数的框架，用于优化一对一自主赛车策略。该框架通过逐步过渡到更复杂的真实环境，同时确保智能体的安全性和最优策略。

    

    一对一自主赛车是一个具有挑战性的问题，车辆需要在摩擦或操控极限下运行，以实现最短的圈速，同时还要积极寻找超车/保持领先的策略。在本研究中，我们提出了一个用于强化学习的一对一赛车环境，能够准确地模拟车辆动力学。一些先前的研究尝试直接在复杂的车辆动力学环境中学习策略，但未能学到最优策略。在本研究中，我们提出了一个基于课程学习的框架，通过从较简单的车辆模型过渡到更复杂的真实环境，将强化学习智能体的策略训练得更接近最优策略。同时，我们还提出了一个基于控制屏障函数的安全强化学习算法，以更有效地保证智能体的安全性而不损害最优性。

    Head-to-head autonomous racing is a challenging problem, as the vehicle needs to operate at the friction or handling limits in order to achieve minimum lap times while also actively looking for strategies to overtake/stay ahead of the opponent. In this work we propose a head-to-head racing environment for reinforcement learning which accurately models vehicle dynamics. Some previous works have tried learning a policy directly in the complex vehicle dynamics environment but have failed to learn an optimal policy. In this work, we propose a curriculum learning-based framework by transitioning from a simpler vehicle model to a more complex real environment to teach the reinforcement learning agent a policy closer to the optimal policy. We also propose a control barrier function-based safe reinforcement learning algorithm to enforce the safety of the agent in a more effective way while not compromising on optimality.
    
[^8]: 临时不确定性定位以实现人在回路分析动态增强心脏磁共振成像数据集

    Temporal Uncertainty Localization to Enable Human-in-the-loop Analysis of Dynamic Contrast-enhanced Cardiac MRI Datasets. (arXiv:2308.13488v1 [eess.IV])

    [http://arxiv.org/abs/2308.13488](http://arxiv.org/abs/2308.13488)

    本文提出了一种新的时空不确定性度量作为动态质量控制（dQC）工具，用于DNN分割自由呼吸DCE-CMRI数据集，并建立了人在回路框架来改善分割结果。

    

    动态增强心脏磁共振成像（DCE-CMRI）是一种广泛用于诊断心肌血流（灌注）异常的模式。在典型的自由呼吸DCE-CMRI扫描中，会在各种对比度“进入/离开”阶段获取接近300幅心肌灌注的时序图像。对于每一帧DCE图像序列中心肌轮廓的手动分割可能是繁琐和耗时的，特别是当非刚性运动校正失败或不可用时。虽然深度神经网络（DNN）显示出对DCE-CMRI数据集进行分析的潜力，但缺乏一种可靠检测失败分割的“动态质量控制”（dQC）技术。在本文中，我们提出了一种新的时空不确定性度量作为基于DNN的自由呼吸DCE-CMRI数据集分割的dQC工具，并通过在外部数据集上验证所提出的度量，并建立一个人在回路框架来改善分割结果。

    Dynamic contrast-enhanced (DCE) cardiac magnetic resonance imaging (CMRI) is a widely used modality for diagnosing myocardial blood flow (perfusion) abnormalities. During a typical free-breathing DCE-CMRI scan, close to 300 time-resolved images of myocardial perfusion are acquired at various contrast "wash in/out" phases. Manual segmentation of myocardial contours in each time-frame of a DCE image series can be tedious and time-consuming, particularly when non-rigid motion correction has failed or is unavailable. While deep neural networks (DNNs) have shown promise for analyzing DCE-CMRI datasets, a "dynamic quality control" (dQC) technique for reliably detecting failed segmentations is lacking. Here we propose a new space-time uncertainty metric as a dQC tool for DNN-based segmentation of free-breathing DCE-CMRI datasets by validating the proposed metric on an external dataset and establishing a human-in-the-loop framework to improve the segmentation results. In the proposed approach,
    
[^9]: OCTAL: 用于LTL模型检测的图形表示学习

    OCTAL: Graph Representation Learning for LTL Model Checking. (arXiv:2308.13474v1 [cs.LO])

    [http://arxiv.org/abs/2308.13474](http://arxiv.org/abs/2308.13474)

    本文提出了一个基于图形表示学习的框架，用于解决LTL模型检测问题。实证实验结果表明，该框架相比传统方法具有较高的准确性和速度优势。

    

    模型检测广泛应用于验证复杂和并发系统是否符合规范。纯符号方法虽然流行，但由于需要进行交叉乘积操作而导致状态空间膨胀问题，对于大规模系统和/或规范来说成本太高。本文提出使用图形表示学习（GRL）来解决线性时态逻辑（LTL）模型检测问题，系统和规范分别用Büchi自动机和LTL公式表示。设计了一种基于GRL的新框架\model，用于学习图形结构化系统和规范的表示，从而将模型检测问题转化为二元分类。在两个模型检测场景上的实证实验表明，\model取得了很有希望的准确性，与规范典型SOTA模型检查器相比，整体加速了高达11倍，仅仅进行可满足性检查的加速率达到了31倍。

    Model Checking is widely applied in verifying the correctness of complex and concurrent systems against a specification. Pure symbolic approaches while popular, suffer from the state space explosion problem due to cross product operations required that make them prohibitively expensive for large-scale systems and/or specifications. In this paper, we propose to use graph representation learning (GRL) for solving linear temporal logic (LTL) model checking, where the system and the specification are expressed by a B{\"u}chi automaton and an LTL formula, respectively. A novel GRL-based framework \model, is designed to learn the representation of the graph-structured system and specification, which reduces the model checking problem to binary classification. Empirical experiments on two model checking scenarios show that \model achieves promising accuracy, with up to $11\times$ overall speedup against canonical SOTA model checkers and $31\times$ for satisfiability checking alone.
    
[^10]: 利用知识和强化学习提高语言模型的可靠性

    Leveraging Knowledge and Reinforcement Learning for Enhanced Reliability of Language Models. (arXiv:2308.13467v1 [cs.CL])

    [http://arxiv.org/abs/2308.13467](http://arxiv.org/abs/2308.13467)

    本研究通过利用知识和强化学习的方法，实现了一个知识引导的语言模型集成，通过整合外部知识来弥补现有数据集中的信息缺失，从而提高了语言模型的可靠性和准确性。

    

    自然语言处理(NLP)社区一直在使用众包技术，创建用于训练现代语言模型如BERT的基准数据集，例如General Language Understanding and Evaluation(GLUE)。GLUE任务使用互评计量方法（如Cohens Kappa）来衡量可靠性分数。然而，语言模型的可靠性方面常常被忽视。为解决这个问题，我们探索了一种知识引导的语言模型集成方法，利用强化学习将ConceptNet和维基百科的知识作为知识图嵌入进行整合。这种方法模仿了人类注释者使用外部知识来弥补数据集中的信息缺失。通过在九个GLUE数据集上的研究表明，语言模型集成可以增强可靠性和准确性得分，超过现有最先进方法。

    The Natural Language Processing(NLP) community has been using crowd sourcing techniques to create benchmark datasets such as General Language Understanding and Evaluation(GLUE) for training modern Language Models such as BERT. GLUE tasks measure the reliability scores using inter annotator metrics i.e. Cohens Kappa. However, the reliability aspect of LMs has often been overlooked. To counter this problem, we explore a knowledge-guided LM ensembling approach that leverages reinforcement learning to integrate knowledge from ConceptNet and Wikipedia as knowledge graph embeddings. This approach mimics human annotators resorting to external knowledge to compensate for information deficits in the datasets. Across nine GLUE datasets, our research shows that ensembling strengthens reliability and accuracy scores, outperforming state of the art.
    
[^11]: 学习干预概念瓶颈

    Learning to Intervene on Concept Bottlenecks. (arXiv:2308.13453v1 [cs.LG])

    [http://arxiv.org/abs/2308.13453](http://arxiv.org/abs/2308.13453)

    该论文提出了一种扩展了概念瓶颈模型的概念瓶颈记忆模型（CB2M），通过学习将干预推广到不同情境并重新应用先前干预来自动改善模型性能。当没有先前的人类干预信息时，CB2M能够检测错误并请求有针对性的干预。

    

    传统的深度学习模型缺乏解释性，而概念瓶颈模型（CBM）通过其概念表示提供固有的解释。具体而言，它们允许用户通过更新概念值并纠正模型的预测输出来进行干预交互。然而，传统方法中这些干预仅应用于模型一次后即被丢弃。为了纠正这一问题，我们提出了概念瓶颈记忆模型（CB2M），这是CBM的一个扩展。具体而言，CB2M通过双折叠记忆学习将干预的推广到适当的新情境中，从而能够学习检测错误并重新应用先前的干预。通过这种方式，CB2M能够从最初获得的少量干预中自动提高模型的性能。如果没有先前的人类干预信息，CB2M可以检测到CBM瓶颈的潜在错误并请求有针对性的干预。

    While traditional deep learning models often lack interpretability, concept bottleneck models (CBMs) provide inherent explanations via their concept representations. Specifically, they allow users to perform interventional interactions on these concepts by updating the concept values and thus correcting the predictive output of the model. Traditionally, however, these interventions are applied to the model only once and discarded afterward. To rectify this, we present concept bottleneck memory models (CB2M), an extension to CBMs. Specifically, a CB2M learns to generalize interventions to appropriate novel situations via a two-fold memory with which it can learn to detect mistakes and to reapply previous interventions. In this way, a CB2M learns to automatically improve model performance from a few initially obtained interventions. If no prior human interventions are available, a CB2M can detect potential mistakes of the CBM bottleneck and request targeted interventions. In our experime
    
[^12]: 在知识图谱中表示定时自动机及其在网络物理生产系统中的时间异常

    Representing Timed Automata and Timing Anomalies of Cyber-Physical Production Systems in Knowledge Graphs. (arXiv:2308.13433v1 [cs.AI])

    [http://arxiv.org/abs/2308.13433](http://arxiv.org/abs/2308.13433)

    本文旨在通过将学习到的时间自动机与关于系统的形式化知识图谱相结合，改进基于模型的网络物理生产系统的异常检测。通过在知识图谱中描述模型和异常，以便操作员更容易解释。

    

    基于模型的异常检测是识别网络物理生产系统预期行为偏差的成功方法。由于手动创建这些模型是一项耗时的过程，从数据中学习并以时间自动机等通用形式进行表示是有优势的。然而，由于对系统的额外信息缺乏，这些模型（以及由此检测到的异常）可能难以解释。本文旨在通过将学习到的时间自动机与关于系统的形式化知识图谱相结合来改进基于模型的网络物理生产系统的异常检测。模型和检测到的异常都在知识图谱中描述，以便操作员更容易解释模型和检测到的异常。作者还提出了必要概念的本体论。该方法在一个五罐混合网络物理生产系统上进行了验证，能够形式化地定义自动机模型和时间特性。

    Model-Based Anomaly Detection has been a successful approach to identify deviations from the expected behavior of Cyber-Physical Production Systems. Since manual creation of these models is a time-consuming process, it is advantageous to learn them from data and represent them in a generic formalism like timed automata. However, these models - and by extension, the detected anomalies - can be challenging to interpret due to a lack of additional information about the system. This paper aims to improve model-based anomaly detection in CPPS by combining the learned timed automaton with a formal knowledge graph about the system. Both the model and the detected anomalies are described in the knowledge graph in order to allow operators an easier interpretation of the model and the detected anomalies. The authors additionally propose an ontology of the necessary concepts. The approach was validated on a five-tank mixing CPPS and was able to formally define both automata model as well as timin
    
[^13]: QKSAN: 量子核自注意力网络

    QKSAN: A Quantum Kernel Self-Attention Network. (arXiv:2308.13422v1 [quant-ph])

    [http://arxiv.org/abs/2308.13422](http://arxiv.org/abs/2308.13422)

    QKSAN是一个量子核自注意力网络，通过结合量子核方法和自注意力机制，提高了量子机器学习模型在大规模高维量子数据上的有效性。

    

    自注意力机制（SAM）擅长从数据内部提取重要信息，以提高模型的计算效率。然而，许多量子机器学习（QML）模型缺乏像SAM这样区分信息的固有连接的能力，这限制了它们在大规模高维量子数据上的有效性。为了解决这个问题，引入了量子核自注意力机制（QKSAM），它将量子核方法（QKM）的数据表示优势与SAM的有效信息提取能力相结合。基于QKSAM构建了一个量子核自注意力网络（QKSAN）框架，利用延迟测量原理（DMP）和条件测量技术，在计算过程中以概率测量的方式释放了一半的量子资源。量子核自注意力分数（QKSAS）确定测量条件并反映了量子系统的概率本质。

    Self-Attention Mechanism (SAM) is skilled at extracting important information from the interior of data to improve the computational efficiency of models. Nevertheless, many Quantum Machine Learning (QML) models lack the ability to distinguish the intrinsic connections of information like SAM, which limits their effectiveness on massive high-dimensional quantum data. To address this issue, a Quantum Kernel Self-Attention Mechanism (QKSAM) is introduced, which combines the data representation benefit of Quantum Kernel Methods (QKM) with the efficient information extraction capability of SAM. A Quantum Kernel Self-Attention Network (QKSAN) framework is built based on QKSAM, with Deferred Measurement Principle (DMP) and conditional measurement techniques, which releases half of the quantum resources with probabilistic measurements during computation. The Quantum Kernel Self-Attention Score (QKSAS) determines the measurement conditions and reflects the probabilistic nature of quantum syste
    
[^14]: SoTaNa: 开源软件开发助手

    SoTaNa: The Open-Source Software Development Assistant. (arXiv:2308.13416v1 [cs.SE])

    [http://arxiv.org/abs/2308.13416](http://arxiv.org/abs/2308.13416)

    SoTaNa是一种开源软件开发助手，利用ChatGPT生成高质量数据并通过参数高效微调方法增强LLaMA模型。它在回答Stack Overflow问题和代码摘要和生成方面展示了出色的能力。

    

    软件开发在推动现代社会的创新和效率方面起着关键作用。为满足这一动态领域的需求，迫切需要一种有效的软件开发助手。然而，现有的大型语言模型如ChatGPT存在训练数据和模型权重等可及性有限的问题。虽然像LLaMA这样的其他大型开源模型已显示出潜力，但它们仍然难以理解人类意图。在本文中，我们介绍了SoTaNa，一种开源软件开发助手。SoTaNa利用ChatGPT生成基于指导的高质量软件工程领域的数据，并采用参数高效微调方法来增强开源基础模型LLaMA。我们通过评估\our{}在回答Stack Overflow问题方面的效果，并展示了其能力。此外，我们还讨论了它在代码摘要和生成方面的能力，以及其影响。

    Software development plays a crucial role in driving innovation and efficiency across modern societies. To meet the demands of this dynamic field, there is a growing need for an effective software development assistant. However, existing large language models represented by ChatGPT suffer from limited accessibility, including training data and model weights. Although other large open-source models like LLaMA have shown promise, they still struggle with understanding human intent. In this paper, we present SoTaNa, an open-source software development assistant. SoTaNa utilizes ChatGPT to generate high-quality instruction-based data for the domain of software engineering and employs a parameter-efficient fine-tuning approach to enhance the open-source foundation model, LLaMA. We evaluate the effectiveness of \our{} in answering Stack Overflow questions and demonstrate its capabilities. Additionally, we discuss its capabilities in code summarization and generation, as well as the impact of
    
[^15]: TFDNet：增强时频分解网络用于长期时间序列预测

    TFDNet: Time-Frequency Enhanced Decomposed Network for Long-term Time Series Forecasting. (arXiv:2308.13386v1 [cs.LG])

    [http://arxiv.org/abs/2308.13386](http://arxiv.org/abs/2308.13386)

    这篇论文提出了一种称为TFDNet的时频增强分解网络，用于从时频域捕获长期时间序列的基本模式和时间周期性。

    

    长期时间序列预测是一项重要任务，并且具有广泛的实际应用。最近的方法专注于捕获来自单一域（例如时间域或频率域）的基本模式，并且没有从时频域综合处理长期时间序列。在本文中，我们提出了一种称为时频增强分解网络（TFDNet）的方法，可以在时频域捕获长期的基本模式和时间周期性。在TFDNet中，我们设计了一个多尺度时频增强编码器主干，并开发两个分别用于捕获多分辨率中分解趋势和季节分量中的不同模式的趋势和季节时频块。通过研究和整合多变量时间序列的潜在不同通道相关模式，我们探索了时频块中核操作的多样化内核学习策略。

    Long-term time series forecasting is a vital task and has a wide range of real applications. Recent methods focus on capturing the underlying patterns from one single domain (e.g. the time domain or the frequency domain), and have not taken a holistic view to process long-term time series from the time-frequency domains. In this paper, we propose a Time-Frequency Enhanced Decomposed Network (TFDNet) to capture both the long-term underlying patterns and temporal periodicity from the time-frequency domain. In TFDNet, we devise a multi-scale time-frequency enhanced encoder backbone and develop two separate trend and seasonal time-frequency blocks to capture the distinct patterns within the decomposed trend and seasonal components in multi-resolutions. Diverse kernel learning strategies of the kernel operations in time-frequency blocks have been explored, by investigating and incorporating the potential different channel-wise correlation patterns of multivariate time series. Experimental e
    
[^16]: 关于语言选择对训练和评估编程语言模型的影响

    On the Impact of Language Selection for Training and Evaluating Programming Language Models. (arXiv:2308.13354v1 [cs.SE])

    [http://arxiv.org/abs/2308.13354](http://arxiv.org/abs/2308.13354)

    这项研究根据使用CodeBERT模型分析编程语言的表示，发现编程语言之间在标记表示方面存在差异，建议使用这种相似度度量方法来选择跨多种语言的模型。

    

    基于Transformer的语言模型的最新进展显示出在增强这些模型的多语言能力方面具有显著潜力。在自然语言任务中取得的显著进展不仅适用于编程语言领域，而且还扩展到编程语言领域。尽管这些模型具备从多种语言中学习的能力，但评估通常只关注同一种语言的特定组合。在本研究中，我们使用基于CodeBERT模型的编程语言表示分析来评估编程语言的相似性。我们的实验揭示了像C++、Python和Java这样的语言中的标记表示之间存在相近性，而像Mathematica和R这样的语言中的相同标记显示出显著的不相似性。我们的研究结果表明，当处理多种语言时，这种现象可能导致性能挑战。因此，我们建议使用我们的相似度度量来选择一个可以平衡多种语言的模型。

    The recent advancements in Transformer-based Language Models have demonstrated significant potential in enhancing the multilingual capabilities of these models. The remarkable progress made in this domain not only applies to natural language tasks but also extends to the domain of programming languages. Despite the ability of these models to learn from multiple languages, evaluations typically focus on particular combinations of the same languages. In this study, we evaluate the similarity of programming languages by analyzing their representations using a CodeBERT-based model. Our experiments reveal that token representation in languages such as C++, Python, and Java exhibit proximity to one another, whereas the same tokens in languages such as Mathematica and R display significant dissimilarity. Our findings suggest that this phenomenon can potentially result in performance challenges when dealing with diverse languages. Thus, we recommend using our similarity measure to select a div
    
[^17]: 一个用于完全无监督异常检测的通用机器学习框架，适用于污染数据

    A Generic Machine Learning Framework for Fully-Unsupervised Anomaly Detection with Contaminated Data. (arXiv:2308.13352v1 [cs.LG])

    [http://arxiv.org/abs/2308.13352](http://arxiv.org/abs/2308.13352)

    这篇论文介绍了一个完全无监督的机器学习框架，用于处理在训练数据中含有异常样本的异常检测任务。

    

    在各个领域和应用中，机器学习算法已经解决了异常检测（AD）任务。这些算法中大多数使用正常数据对一个基于残差的模型进行训练，并根据未见样本与学习到的正常范围的不相似性来分配异常分数。这些方法的基本假设是可以用无异常的数据进行训练。然而，在真实世界中的操作环境中，训练数据通常会与一定比例的异常样本混合。而利用污染数据进行训练必然会导致基于残差的算法的AD性能下降。本文介绍了一个完全无监督的用于AD任务的污染训练数据的改进框架。该框架是通用的，可应用于任何基于残差的机器学习模型。我们展示了该框架在两个多元时间数据集上的应用。

    Anomaly detection (AD) tasks have been solved using machine learning algorithms in various domains and applications. The great majority of these algorithms use normal data to train a residual-based model, and assign anomaly scores to unseen samples based on their dissimilarity with the learned normal regime. The underlying assumption of these approaches is that anomaly-free data is available for training. This is, however, often not the case in real-world operational settings, where the training data may be contaminated with a certain fraction of abnormal samples. Training with contaminated data, in turn, inevitably leads to a deteriorated AD performance of the residual-based algorithms.  In this paper we introduce a framework for a fully unsupervised refinement of contaminated training data for AD tasks. The framework is generic and can be applied to any residual-based machine learning model. We demonstrate the application of the framework to two public datasets of multivariate time s
    
[^18]: Squeeze聚合激励网络

    Squeeze aggregated excitation network. (arXiv:2308.13343v1 [cs.CV])

    [http://arxiv.org/abs/2308.13343](http://arxiv.org/abs/2308.13343)

    本论文提出了一种名为Squeeze聚合激励网络的新方法，用于在卷积神经网络中学习全局通道表示。该方法通过挤压和聚合激励的方式，在通道级别上连接空间表示，提高了模型性能。

    

    卷积神经网络在视觉任务中具有空间表示能力，通过在通道水平上显式建模，Squeeze and excitation将通道级别的表示连接起来。多层感知器学习全局表示，在大多数模型中，它通常在所有卷积层之后在最后使用，以收集分类之前学到的所有信息。我们提出了一种在通道内引发全局表示以提高模型性能的方法。我们提出了SaEnet，即Squeeze聚合激励网络，用于学习层之间的全局通道表示。所提出的模块利用挤压后通过聚合激励恢复其形状之前传递重要信息的优势。我们还引入了在网络中具有多分支线性(密集)层的新思想。这个层从压缩的信息中学习全局表示，增强了网络的表征能力。

    Convolutional neural networks have spatial representations which read patterns in the vision tasks. Squeeze and excitation links the channel wise representations by explicitly modeling on channel level. Multi layer perceptrons learn global representations and in most of the models it is used often at the end after all convolutional layers to gather all the information learned before classification. We propose a method of inducing the global representations within channels to have better performance of the model. We propose SaEnet, Squeeze aggregated excitation network, for learning global channelwise representation in between layers. The proposed module takes advantage of passing important information after squeeze by having aggregated excitation before regaining its shape. We also introduce a new idea of having a multibranch linear(dense) layer in the network. This learns global representations from the condensed information which enhances the representational power of the network. Th
    
[^19]: 改造生成预训练变换器的输出: PGI框架对注意力动态的影响

    Transforming the Output of Generative Pre-trained Transformer: The Influence of the PGI Framework on Attention Dynamics. (arXiv:2308.13317v1 [cs.AI])

    [http://arxiv.org/abs/2308.13317](http://arxiv.org/abs/2308.13317)

    本文介绍了一种名为PGI的新方法，在实际商业问题中应用于GPT模型。该方法利用GPT模型的能力来理解复杂的语言结构，并生成上下文相关的回应。实验证实了PGI策略的有效性，并帮助解决了人类智能低度利用的问题。

    

    本文提出了一种名为Persona-Grouping-Intelligence (PGI)的新方法，旨在解决GPT模型在实际商业问题中的应用所带来的挑战。PGI利用GPT模型的内在能力来理解复杂的语言结构，并生成与上下文相关的回应。实验在一个商业场景中进行，该场景存在人类智能被低效的商业流程低度利用的问题。该方法的主要目标是利用GPT模型来减轻人类在广泛、单调和重复的任务中的工作负荷，将重点转向决策活动。该实验生成的4,000个回应的验证准确率为93.81%，突出了PGI策略的有效性。这种范式转变有效地解决了人类智能低度利用的问题，使企业环境与决策活动相一致。

    This paper presents a novel approach named Persona-Grouping-Intelligence (PGI), which has been crafted to tackle the challenges posed by GPT models when applied to real-world business issues. PGI leverages the inherent capabilities of the GPT model to comprehend intricate language structures and generate responses that are contextually relevant. The experiment occurred in a business scenario where human intelligence was being underutilized due to less optimized business processes. The primary objective of this approach is to leverage GPT models to reduce the workload on humans in tasks that are extensive, monotonous, and repetitive. Instead, the focus is redirected toward decision-making activities. Remarkably, the experiment yielded an accuracy rate of 93.81% in validating 4,000 responses generated by the model, underscoring the effectiveness of the PGI strategies. Effectively addressing the issue of underutilized human intelligence, this paradigm shift aligns business environments wi
    
[^20]: Asch遇上HRI：人类对机器人群体的遵从行为

    Asch Meets HRI: Human Conformity to Robot Groups. (arXiv:2308.13307v1 [cs.RO])

    [http://arxiv.org/abs/2308.13307](http://arxiv.org/abs/2308.13307)

    本文旨在研究在工业机器人背景下的群体动力学和同侪压力。通过将经典的Asch实验引入HRI中，我们将测试参与者在群体机器人同伴给出错误回应时对机器人回应的遵循程度，并关注群体规模、机器人可信度、心理压力和同侪压力的影响。

    

    我们提出了一个研究大纲，旨在探究在工业机器人的背景下的群体动力学和同侪压力。我们的研究计划的动机是工业机器人已经成为人机共同工作的一个不可或缺的部分。然而，工业机器人在机器人可信度、群体动力学以及潜在用户跟随机器人指示的倾向方面的研究中还很少被整合。因此，我们的目标是将经典的Asch实验（参见\cite{Asch_51}）将其转化为在工业机器人中进行的HRI实验。具体来说，我们将测试参与者在面对一个群体（相对于个体）的工业机器人手臂（相对于人类）同伴给出错误回应时，对机器人回应的遵循程度。我们对工业机器人背景下的群体规模、被感知的机器人可信度、心理压力和同侪压力的影响感兴趣。通过这项研究的结果，我们希望揭示可能支撑HRI的群体动力学。

    We present a research outline that aims at investigating group dynamics and peer pressure in the context of industrial robots. Our research plan was motivated by the fact that industrial robots became already an integral part of human-robot co-working. However, industrial robots have been sparsely integrated into research on robot credibility, group dynamics, and potential users' tendency to follow a robot's indication. Therefore, we aim to transfer the classic Asch experiment (see \cite{Asch_51}) into HRI with industrial robots. More precisely, we will test to what extent participants follow a robot's response when confronted with a group (vs. individual) industrial robot arms (vs. human) peers who give a false response. We are interested in highlighting the effects of group size, perceived robot credibility, psychological stress, and peer pressure in the context of industrial robots. With the results of this research, we hope to highlight group dynamics that might underlie HRI in ind
    
[^21]: 动态剩余分类器用于类增量学习

    Dynamic Residual Classifier for Class Incremental Learning. (arXiv:2308.13305v1 [cs.CV])

    [http://arxiv.org/abs/2308.13305](http://arxiv.org/abs/2308.13305)

    本文提出了一种动态剩余分类器（DRC）来解决类增量学习中的数据不平衡问题。DRC通过分支层合并和模型适应和融合（MAF）流程的结合，在传统CI任务中取得了最先进的结果。

    

    在类增量学习中，演练策略被广泛用于减轻灾难性遗忘问题，通过保留先前任务的有限范例。由于旧类和新类之间样本数量不平衡，分类器学习可能会产生偏差。现有的类增量学习方法利用长尾识别技术来处理增量任务中的数据不平衡问题，例如调整的损失和数据重新取样方法。本文展示了类增量学习中数据不平衡的动态性，并提出了一种新颖的动态剩余分类器（DRC）来应对这种具有挑战性的情景。具体而言，DRC建立在最近的先进剩余分类器之上，通过分支层合并来处理模型增长问题。此外，DRC与不同的类增量学习流程兼容，并且显著改进了它们。将DRC与模型适应和融合（MAF）流程相结合，该方法在传统CI方面取得了最先进的结果。

    The rehearsal strategy is widely used to alleviate the catastrophic forgetting problem in class incremental learning (CIL) by preserving limited exemplars from previous tasks. With imbalanced sample numbers between old and new classes, the classifier learning can be biased. Existing CIL methods exploit the long-tailed (LT) recognition techniques, e.g., the adjusted losses and the data re-sampling methods, to handle the data imbalance issue within each increment task. In this work, the dynamic nature of data imbalance in CIL is shown and a novel Dynamic Residual Classifier (DRC) is proposed to handle this challenging scenario. Specifically, DRC is built upon a recent advance residual classifier with the branch layer merging to handle the model-growing problem. Moreover, DRC is compatible with different CIL pipelines and substantially improves them. Combining DRC with the model adaptation and fusion (MAF) pipeline, this method achieves state-of-the-art results on both the conventional CI
    
[^22]: 使用深度超参数化多任务学习来学习紧凑的神经网络

    Learning Compact Neural Networks with Deep Overparameterised Multitask Learning. (arXiv:2308.13300v1 [cs.LG])

    [http://arxiv.org/abs/2308.13300](http://arxiv.org/abs/2308.13300)

    本文提出了一种使用深度超参数化多任务学习来学习紧凑的神经网络的方法，通过在训练中超参数化模型架构并更有效地共享跨任务的超参数化模型参数，以实现更好的优化和泛化。

    

    紧凑的神经网络在实际应用中具有许多优点。然而，用小参数大小和低计算成本来训练紧凑的神经网络以达到与更复杂、更强大的体系结构相同或更好的模型性能通常是具有挑战性的。这在多任务学习中尤其如此，因为不同的任务竞争资源。我们提出了一种简单、高效、有效的多任务学习超参数化神经网络设计，通过在训练中超参数化模型架构并更有效地共享跨任务的超参数化模型参数，以实现更好的优化和泛化。在两个具有挑战性的多任务数据集（NYUv2和COCO）上的实验证明了所提方法在各种卷积网络和参数大小上的有效性。

    Compact neural network offers many benefits for real-world applications. However, it is usually challenging to train the compact neural networks with small parameter sizes and low computational costs to achieve the same or better model performance compared to more complex and powerful architecture. This is particularly true for multitask learning, with different tasks competing for resources. We present a simple, efficient and effective multitask learning overparameterisation neural network design by overparameterising the model architecture in training and sharing the overparameterised model parameters more effectively across tasks, for better optimisation and generalisation. Experiments on two challenging multitask datasets (NYUv2 and COCO) demonstrate the effectiveness of the proposed method across various convolutional networks and parameter sizes.
    
[^23]: JAX-LOB：一种基于GPU加速的限价单簿模拟器，为大规模强化学习交易解锁

    JAX-LOB: A GPU-Accelerated limit order book simulator to unlock large scale reinforcement learning for trading. (arXiv:2308.13289v1 [q-fin.TR])

    [http://arxiv.org/abs/2308.13289](http://arxiv.org/abs/2308.13289)

    JAX-LOB是第一个GPU加速的LOB模拟器，可以并行处理数千个订单簿，以较低的处理时间实现大规模强化学习交易，为金融交易研究提供了重要工具。

    

    全球金融交易所使用限价单簿（LOB）来处理订单和匹配交易。为了研究目的，需要具有大规模高效的LOB动态模拟器。以前曾在基于代理模型（ABMs），强化学习（RL）环境和生成模型的上下文中实现了LOB模拟器，处理来自历史数据集和手工代理的订单流。对于许多应用程序，需要处理多个订单簿，无论是用于ABM的校准还是RL代理的训练。我们展示了首个能够并行处理数千本订单簿且每个消息处理时间显著减少的GPU-enabled LOB模拟器-JAX-LOB的实现。我们的模拟器JAX-LOB的实现基于设计选择，旨在充分利用JAX的功能，同时不损害与LOB相关的机制的真实性。我们将JAX-LOB与其他JAX包集成，以提供如何适用的示例。

    Financial exchanges across the world use limit order books (LOBs) to process orders and match trades. For research purposes it is important to have large scale efficient simulators of LOB dynamics. LOB simulators have previously been implemented in the context of agent-based models (ABMs), reinforcement learning (RL) environments, and generative models, processing order flows from historical data sets and hand-crafted agents alike. For many applications, there is a requirement for processing multiple books, either for the calibration of ABMs or for the training of RL agents. We showcase the first GPU-enabled LOB simulator designed to process thousands of books in parallel, with a notably reduced per-message processing time. The implementation of our simulator - JAX-LOB - is based on design choices that aim to best exploit the powers of JAX without compromising on the realism of LOB-related mechanisms. We integrate JAX-LOB with other JAX packages, to provide an example of how one may ad
    
[^24]: AtmoRep:一种利用大规模表示学习的大气动力学随机模型

    AtmoRep: A stochastic model of atmosphere dynamics using large scale representation learning. (arXiv:2308.13280v1 [physics.ao-ph])

    [http://arxiv.org/abs/2308.13280](http://arxiv.org/abs/2308.13280)

    提出了一种称为AtmoRep的大气动力学随机模型，它利用大规模表示学习和人工智能技术来确定复杂的大气动力学的通用描述，从而为各种应用提供技能结果。

    

    大气对人类有多种影响，从因天气不良而丧生的损失到对社会的长期社会和经济影响。因此，对大气动力学进行计算机模拟对我们和未来的世代的福祉非常重要。在这里，我们提出了AtmoRep，一种新颖的、与任务无关的大气动力学随机计算机模型，可以为广泛的应用提供技能结果。AtmoRep利用人工智能的大规模表示学习来确定大气高度复杂、随机动力学的通用描述，该描述基于历史轨迹的最佳可用估计，这些历史轨迹受观测约束。这是通过一种新颖的自监督学习目标和一个独特的集合实现的，该集合从随机模型中采样，其可变性受历史记录中的可变性启发。AtmoRep的任务无关性使其能够为各种应用提供灵活的结果。

    The atmosphere affects humans in a multitude of ways, from loss of life due to adverse weather effects to long-term social and economic impacts on societies. Computer simulations of atmospheric dynamics are, therefore, of great importance for the well-being of our and future generations. Here, we propose AtmoRep, a novel, task-independent stochastic computer model of atmospheric dynamics that can provide skillful results for a wide range of applications. AtmoRep uses large-scale representation learning from artificial intelligence to determine a general description of the highly complex, stochastic dynamics of the atmosphere from the best available estimate of the system's historical trajectory as constrained by observations. This is enabled by a novel self-supervised learning objective and a unique ensemble that samples from the stochastic model with a variability informed by the one in the historical record. The task-independent nature of AtmoRep enables skillful results for a divers
    
[^25]: 非欧几里得随机森林

    Hyperbolic Random Forests. (arXiv:2308.13279v1 [cs.LG])

    [http://arxiv.org/abs/2308.13279](http://arxiv.org/abs/2308.13279)

    该论文提出了一种在非欧几里得空间中将随机森林推广的方法，并使用水平球重新定义了分割的概念。为了处理多类数据和不平衡实验，论文还提出了一种新的类组合方法。

    

    非欧几里得空间由于许多现实世界数据集的分层结构（无论是隐式还是显式）而成为表示数据的流行选择。随之而来的是需要能够在非欧几里得空间中解决分类等基本任务的算法。最近，有多篇论文研究了非欧几里得空间中基于超平面的分类器（如逻辑回归和支持向量机）的替代方法。虽然有效，但这些方法在处理更复杂的分层数据时存在困难。因此，我们提出将众所周知的随机森林推广到非欧几里得空间。我们通过使用水平球重新定义了分割的概念来实现这一点。由于找到全局最优分割是计算上难以处理的，我们通过一个大边界分类器找到候选的水平球。为了使非欧几里得随机森林适用于多类数据和不平衡实验，我们还概述了一种基于它们的最低公共祖先和类平衡的类组合方法。

    Hyperbolic space is becoming a popular choice for representing data due to the hierarchical structure - whether implicit or explicit - of many real-world datasets. Along with it comes a need for algorithms capable of solving fundamental tasks, such as classification, in hyperbolic space. Recently, multiple papers have investigated hyperbolic alternatives to hyperplane-based classifiers, such as logistic regression and SVMs. While effective, these approaches struggle with more complex hierarchical data. We, therefore, propose to generalize the well-known random forests to hyperbolic space. We do this by redefining the notion of a split using horospheres. Since finding the globally optimal split is computationally intractable, we find candidate horospheres through a large-margin classifier. To make hyperbolic random forests work on multi-class data and imbalanced experiments, we furthermore outline a new method for combining classes based on their lowest common ancestor and a class-balan
    
[^26]: 将LLMs和Decision Transformers集成到语言驱动的生成质量多样性中

    Integrating LLMs and Decision Transformers for Language Grounded Generative Quality-Diversity. (arXiv:2308.13278v1 [cs.LG])

    [http://arxiv.org/abs/2308.13278](http://arxiv.org/abs/2308.13278)

    本文提出了一种将LLMs和Decision Transformers集成到语言驱动的生成质量多样性问题中的方法，通过利用大型语言模型增加具有轨迹的自然语言描述的库，并训练一个依赖于这些描述的策略来解决问题。

    

    质量多样性是一种用于解决强化学习和控制领域问题的随机优化分支，其目的是构建表现良好且在行为空间上具有多样性的政策/技能库。这样的存档通常由有限数量的反应代理组成，每个代理都与唯一的行为描述符相关联，而在粗略离散化的空间之外实例化行为描述符并不直观。虽然最近有一些工作提出了解决这个问题的方法，但生成的轨迹在目标行为描述符规定之外很难进行定制。我们提出在具有静态场景元素的环境中，通过利用大型语言模型增加具有轨迹的自然语言描述的库，并训练一个依赖于这些描述的策略来共同解决这些问题。

    Quality-Diversity is a branch of stochastic optimization that is often applied to problems from the Reinforcement Learning and control domains in order to construct repertoires of well-performing policies/skills that exhibit diversity with respect to a behavior space. Such archives are usually composed of a finite number of reactive agents which are each associated to a unique behavior descriptor, and instantiating behavior descriptors outside of that coarsely discretized space is not straight-forward. While a few recent works suggest solutions to that issue, the trajectory that is generated is not easily customizable beyond the specification of a target behavior descriptor. We propose to jointly solve those problems in environments where semantic information about static scene elements is available by leveraging a Large Language Model to augment the repertoire with natural language descriptions of trajectories, and training a policy conditioned on those descriptions. Thus, our method 
    
[^27]: 基于知识驱动的CoT：探索LLMs中对知识密集型问答进行忠实推理

    Knowledge-Driven CoT: Exploring Faithful Reasoning in LLMs for Knowledge-intensive Question Answering. (arXiv:2308.13259v1 [cs.CL])

    [http://arxiv.org/abs/2308.13259](http://arxiv.org/abs/2308.13259)

    本文提出了一个名为知识驱动的思维链（KD-CoT）的框架，用于验证和修改LLMs中的推理过程，通过与外部知识的交互来解决幻觉和错误传播的问题。

    

    大型语言模型（LLMs）配备了思维链（CoT），在各种下游任务中展现出了令人印象深刻的推理能力。但是，由于幻觉和无法访问外部知识，LLMs在对知识密集型任务（如知识库问答）进行推理时常常会产生不正确或不忠实的中间推理步骤。为了缓解这个问题，我们提出了一个名为知识驱动的思维链（KD-CoT）的框架，通过与外部知识的交互来验证和修改CoT中的推理过程，从而克服幻觉和错误传播。具体地，我们将LLMs的CoT推理过程规范化为结构化的多轮问答格式。在每一轮中，LLMs与一个问答系统进行交互，该系统检索外部知识并基于检索到的准确答案产生忠实的推理过程。我们开发的KBQA CoT集合促进了LLMs的结构化CoT推理，它作为上下文学习的一部分。

    Equipped with Chain-of-Thought (CoT), Large language models (LLMs) have shown impressive reasoning ability in various downstream tasks. Even so, suffering from hallucinations and the inability to access external knowledge, LLMs often come with incorrect or unfaithful intermediate reasoning steps, especially in the context of answering knowledge-intensive tasks such as KBQA. To alleviate this issue, we propose a framework called Knowledge-Driven Chain-of-Thought (KD-CoT) to verify and modify reasoning traces in CoT via interaction with external knowledge, and thus overcome the hallucinations and error propagation. Concretely, we formulate the CoT rationale process of LLMs into a structured multi-round QA format. In each round, LLMs interact with a QA system that retrieves external knowledge and produce faithful reasoning traces based on retrieved precise answers. The structured CoT reasoning of LLMs is facilitated by our developed KBQA CoT collection, which serves as in-context learning
    
[^28]: 从脑电图解码自然图像进行物体识别

    Decoding Natural Images from EEG for Object Recognition. (arXiv:2308.13234v1 [cs.HC])

    [http://arxiv.org/abs/2308.13234](http://arxiv.org/abs/2308.13234)

    本文提出了一种自我监督的框架，从EEG信号中学习图像表示，并采用对比学习来对齐这两种模态。通过在最广泛的EEG图像数据集上的实验证明了该方法的优越性能和生物合理性。

    

    电脑脑图（EEG）以其高时间分辨率和适度的信噪比而闻名。最近，能否从EEG中解码自然图像成为热门问题。在本文中，我们提出了一个自我监督的框架，从EEG信号中学习图像表示。具体而言，我们首先使用图像和EEG编码器从配对的图像刺激和EEG响应中提取特征。然后，我们采用对比学习来通过约束它们的相似性来对齐这两种模态。此外，在EEG编码器之前，我们引入了两个即插即用的模块，用于捕捉空间相关性。我们的方法在最广泛的EEG图像数据集上取得了最先进的结果，在200种零样本任务中，top-1准确度达到15.6%，top-5准确度达到42.8%。更重要的是，对EEG信号的时间、空间、频谱和语义方面进行的大量实验证明了其良好的生物合理性。这些结果提供了有价值的见解。

    Electroencephalogram (EEG) is a brain signal known for its high time resolution and moderate signal-to-noise ratio. Whether natural images can be decoded from EEG has been a hot issue recently. In this paper, we propose a self-supervised framework to learn image representations from EEG signals. Specifically, image and EEG encoders are first used to extract features from paired image stimuli and EEG responses. Then we employ contrastive learning to align these two modalities by constraining their similarity. Additionally, we introduce two plug-in-play modules that capture spatial correlations before the EEG encoder. Our approach achieves state-of-the-art results on the most extensive EEG-image dataset, with a top-1 accuracy of 15.6% and a top-5 accuracy of 42.8% in 200-way zero-shot tasks. More importantly, extensive experiments analyzing the temporal, spatial, spectral, and semantic aspects of EEG signals demonstrate good biological plausibility. These results offer valuable insights 
    
[^29]: MultiCapCLIP: 零样本多语言视觉字幕的自编码提示

    MultiCapCLIP: Auto-Encoding Prompts for Zero-Shot Multilingual Visual Captioning. (arXiv:2308.13218v1 [cs.CV])

    [http://arxiv.org/abs/2308.13218](http://arxiv.org/abs/2308.13218)

    MultiCapCLIP是一种零样本的方法，可以生成不同场景和语言的视觉字幕，无需下游数据集中的任何标记。在训练阶段，它通过检索概念提示并自编码学习写作风格来生成字幕。在测试阶段，它直接利用视觉数据进行检索。

    

    监督式视觉字幕模型通常需要大量的图像或视频与特定语言的描述进行配对（即视觉-字幕对）进行训练。然而，对于许多场景和语言来说，收集和标注大规模数据集是耗时且昂贵的。因此，通常无法提供足够的标记对。为了解决标签不足的问题，我们提出了一种简单而有效的零样本方法MultiCapCLIP，它可以在没有下游数据集的任何标记的情况下为不同场景和语言生成视觉字幕。在训练阶段，MultiCapCLIP仅需要文本数据作为输入。然后它进行两个主要步骤：1）检索保留新场景相关领域知识的概念提示；2）自编码提示以学习输出所需语言的字幕的写作风格。在测试阶段，MultiCapCLIP直接将视觉数据作为输入来检索...

    Supervised visual captioning models typically require a large scale of images or videos paired with descriptions in a specific language (i.e., the vision-caption pairs) for training. However, collecting and labeling large-scale datasets is time-consuming and expensive for many scenarios and languages. Therefore, sufficient labeled pairs are usually not available. To deal with the label shortage problem, we present a simple yet effective zero-shot approach MultiCapCLIP that can generate visual captions for different scenarios and languages without any labeled vision-caption pairs of downstream datasets. In the training stage, MultiCapCLIP only requires text data for input. Then it conducts two main steps: 1) retrieving concept prompts that preserve the corresponding domain knowledge of new scenarios; 2) auto-encoding the prompts to learn writing styles to output captions in a desired language. In the testing stage, MultiCapCLIP instead takes visual data as input directly to retrieve the
    
[^30]: 受物理启发的神经图ODE用于长期动力学模拟

    Physics-Inspired Neural Graph ODE for Long-term Dynamical Simulation. (arXiv:2308.13212v1 [cs.LG])

    [http://arxiv.org/abs/2308.13212](http://arxiv.org/abs/2308.13212)

    提出一种受物理启发的神经图ODE算法（PINGO），用于模拟和建模多对象物理系统的长期动态，并解决了当前模型泛化能力差的问题。

    

    模拟和建模多对象物理系统的长期动态是一项重要且具有挑战性的任务。目前的研究利用具有等变性质的图神经网络(GNNs)对物理系统进行建模。具体而言，他们将动力学建模为一系列具有固定时间间隔的离散状态，并学习所有相邻状态之间的直接映射。然而，这种直接映射忽略了两个状态之间的连续性。换句话说，我们已经验证了在当前基于GNN的直接映射模型中，两个离散动态状态之间存在无数可能的轨迹。这个问题极大地阻碍了模型的泛化能力，导致长期模拟的性能较差。在本文中，为了更好地通过离散监督信号建模潜在轨迹，我们提出了一个受物理启发的神经图ODE(PINGO)算法。

    Simulating and modeling the long-term dynamics of multi-object physical systems is an essential and challenging task. Current studies model the physical systems utilizing Graph Neural Networks (GNNs) with equivariant properties. Specifically, they model the dynamics as a sequence of discrete states with a fixed time interval and learn a direct mapping for all the two adjacent states. However, this direct mapping overlooks the continuous nature between the two states. Namely, we have verified that there are countless possible trajectories between two discrete dynamic states in current GNN-based direct mapping models. This issue greatly hinders the model generalization ability, leading to poor performance of the long-term simulation. In this paper, to better model the latent trajectory through discrete supervision signals, we propose a Physics-Inspired Neural Graph ODE (PINGO) algorithm. In PINGO, to ensure the uniqueness of the trajectory, we construct a Physics-Inspired Neural ODE fram
    
[^31]: 人机交互中自然语言量词的形式化

    Formalising Natural Language Quantifiers for Human-Robot Interactions. (arXiv:2308.13192v1 [cs.AI])

    [http://arxiv.org/abs/2308.13192](http://arxiv.org/abs/2308.13192)

    本文提出了一种在人机交互中形式化自然语言量词的方法，并设计了一个端到端系统以实现自然语言输入到逻辑表示的转换和评估，从而实现对机器人的控制。

    

    我们提出了一种在人机交互背景下形式化自然语言量词的方法。该解决方案基于一阶逻辑并扩展了表示变量基数的能力，类似于广义量词。为了展示这种方法，我们设计了一个端到端的系统，能够接收自然语言输入，将其转换为形式化的逻辑表示，评估它，并返回结果或向模拟机器人发送命令。

    We present a method for formalising quantifiers in natural language in the context of human-robot interactions. The solution is based on first-order logic extended with capabilities to represent the cardinality of variables, operating similarly to generalised quantifiers. To demonstrate the method, we designed an end-to-end system able to receive input as natural language, convert it into a formal logical representation, evaluate it, and return a result or send a command to a simulated robot.
    
[^32]: Chunk, Align, Select: 一种简单的用于transformer的长序列处理方法

    Chunk, Align, Select: A Simple Long-sequence Processing Method for Transformers. (arXiv:2308.13191v1 [cs.CL])

    [http://arxiv.org/abs/2308.13191](http://arxiv.org/abs/2308.13191)

    这种方法提出了一种简单的框架，使得transformer能够处理更长的序列，同时计算和内存成本与输入序列长度线性增长。

    

    尽管在自然语言处理中占据主导地位，基于transformer的模型仍然面临着长序列处理的挑战，因为transformer中自注意操作的计算成本随着输入序列长度的增加呈二次增长。为了减轻长序列处理的复杂性，我们提出了一个简单的框架，使得现有的预训练transformer能够处理更长的序列，同时计算和内存成本与输入序列长度线性增长。具体来说，我们的方法将每个长序列输入划分为一批chunk，然后在编码过程中对chunk之间的信息进行对齐，最后从编码器中选择最具代表性的隐藏状态进行解码。为了提取chunk之间的语义信息，我们在每个编码transformer块中对chunk之间的起始和结束token进行对齐。为了学习一个有效的隐藏状态选择策略，我们设计了一个双重更新机制。

    Although dominant in natural language processing, transformer-based models remain challenged by the task of long-sequence processing, because the computational cost of self-attention operations in transformers swells quadratically with the input sequence length. To alleviate the complexity of long-sequence processing, we propose a simple framework to enable the offthe-shelf pre-trained transformers to process much longer sequences, while the computation and memory costs remain growing linearly with the input sequence lengths. More specifically, our method divides each long-sequence input into a batch of chunks, then aligns the interchunk information during the encoding steps, and finally selects the most representative hidden states from the encoder for the decoding process. To extract inter-chunk semantic information, we align the start and end token embeddings among chunks in each encoding transformer block. To learn an effective hidden selection policy, we design a dual updating sch
    
[^33]: Falcon: 加速基于同态加密的卷积以实现高效的私密移动网络推断

    Falcon: Accelerating Homomorphically Encrypted Convolutions for Efficient Private Mobile Network Inference. (arXiv:2308.13189v1 [cs.CR])

    [http://arxiv.org/abs/2308.13189](http://arxiv.org/abs/2308.13189)

    Falcon是一种用于加速基于同态加密的卷积的密集打包算法，通过优化打包密度和降低通信开销，实现了在运算器级别上超过15.6倍的延迟降低，并且在网络级别上提供了更高的加密推断速度。

    

    高效网络，如MobileNetV2、EfficientNet等，在轻量级计算下实现了最先进的准确率。然而，现有的基于同态加密的两方计算框架对于这些网络并没有进行优化，并且在推断过程中存在高开销。我们观察到低效主要来自于打包算法，该算法忽略了计算特性和同态加密深度卷积的通信瓶颈。因此，在本文中，我们提出了Falcon，一种用于基于同态加密的两方计算框架的有效密集打包算法。Falcon采用了一种零感知的贪心打包算法和一种通信感知的操作器平铺策略，以提高深度卷积的打包密度。与SOTA的基于同态加密的两方计算框架（如CrypTFlow2、Iron和Cheetah）相比，Falcon在运算器级别分别实现了超过15.6倍、5.1倍和1.8倍的延迟降低。同时，在网络级别上，Falcon可以实现更高的加密推断速度。

    Efficient networks, e.g., MobileNetV2, EfficientNet, etc, achieves state-of-the-art (SOTA) accuracy with lightweight computation. However, existing homomorphic encryption (HE)-based two-party computation (2PC) frameworks are not optimized for these networks and suffer from a high inference overhead. We observe the inefficiency mainly comes from the packing algorithm, which ignores the computation characteristics and the communication bottleneck of homomorphically encrypted depthwise convolutions. Therefore, in this paper, we propose Falcon, an effective dense packing algorithm for HE-based 2PC frameworks. Falcon features a zero-aware greedy packing algorithm and a communication-aware operator tiling strategy to improve the packing density for depthwise convolutions. Compared to SOTA HE-based 2PC frameworks, e.g., CrypTFlow2, Iron and Cheetah, Falcon achieves more than 15.6x, 5.1x and 1.8x latency reduction, respectively, at operator level. Meanwhile, at network level, Falcon allows for
    
[^34]: 结构Cycle GAN用于在结肠中对腺体标记进行虚拟免疫组织化学染色

    Structural Cycle GAN for Virtual Immunohistochemistry Staining of Gland Markers in the Colon. (arXiv:2308.13182v1 [cs.CV])

    [http://arxiv.org/abs/2308.13182](http://arxiv.org/abs/2308.13182)

    本研究提出了一种新的生成模型SC-GAN，用于合成H&E图像和IHC染色之间的转换。该模型利用边缘结构信息和注意力模块，增强了特征定位并保留了上下文信息。

    

    随着数字扫描仪和深度学习的出现，诊断操作可能从显微镜移到桌面上。血红素和伊红染色（H&E）是最常用于疾病分析、诊断和分级的染色方法之一，但病理学家确实需要不同的免疫组织化学（IHC）染色来分析特定的结构或细胞。在单个标本上获得所有这些染色（H&E和不同的IHC）是一项繁琐耗时的任务。因此，虚拟染色已成为一个重要的研究方向。在这里，我们提出了一种新颖的生成模型，结构Cycle GAN（SC-GAN），用于从H&E图像合成IHC染色反之亦然。我们的方法明确地将边缘结构信息（除了颜色数据）纳入到所提出的生成模型的解码器中，并且在生成过程中仅使用注意力模块。这种集成增强了特征定位并在生成过程中保留了上下文信息。

    With the advent of digital scanners and deep learning, diagnostic operations may move from a microscope to a desktop. Hematoxylin and Eosin (H&E) staining is one of the most frequently used stains for disease analysis, diagnosis, and grading, but pathologists do need different immunohistochemical (IHC) stains to analyze specific structures or cells. Obtaining all of these stains (H&E and different IHCs) on a single specimen is a tedious and time-consuming task. Consequently, virtual staining has emerged as an essential research direction. Here, we propose a novel generative model, Structural Cycle-GAN (SC-GAN), for synthesizing IHC stains from H&E images, and vice versa. Our method expressly incorporates structural information in the form of edges (in addition to color data) and employs attention modules exclusively in the decoder of the proposed generator model. This integration enhances feature localization and preserves contextual information during the generation process. In additi
    
[^35]: DAG-ACFL：基于DAG-DLT的异步聚类联邦学习

    DAG-ACFL: Asynchronous Clustered Federated Learning based on DAG-DLT. (arXiv:2308.13158v1 [cs.LG])

    [http://arxiv.org/abs/2308.13158](http://arxiv.org/abs/2308.13158)

    DAG-ACFL是一种基于DAG-DLT的异步聚类联邦学习框架，通过选择相似分布的客户端模型来聚合全局模型，同时采用自适应的tip选择算法降低通信和存储成本。

    

    联邦学习旨在在保护客户数据隐私的同时协同训练全局模型。然而，由于客户数据的非独立同分布，联邦学习面临挑战。聚类联邦学习(CFL)作为一种有前景的解决方案已经出现，但大多数现有的CFL框架都采用同步框架缺乏异步性。提出了一种基于有向无环图分布式账本技术(DAG-DLT)的异步CFL框架SDAGFL，但其完全去中心化导致了高通信和存储成本。我们提出了DAG-ACFL，这是一种基于DAG-DLT的异步聚类FL框架。首先详细介绍了DAG-ACFL的组成部分。然后设计了一种基于模型参数的余弦相似度的tip选择算法，以聚合具有相似分布的客户端的模型。采用自适应tip选择算法，利用变点检测动态确定所选tip的数量。我们评估了DAG-ACFL的性能并与其他方法进行了比较。

    Federated learning (FL) aims to collaboratively train a global model while ensuring client data privacy. However, FL faces challenges from the non-IID data distribution among clients. Clustered FL (CFL) has emerged as a promising solution, but most existing CFL frameworks adopt synchronous frameworks lacking asynchrony. An asynchronous CFL framework called SDAGFL based on directed acyclic graph distributed ledger techniques (DAG-DLT) was proposed, but its complete decentralization leads to high communication and storage costs. We propose DAG-ACFL, an asynchronous clustered FL framework based on directed acyclic graph distributed ledger techniques (DAG-DLT). We first detail the components of DAG-ACFL. A tip selection algorithm based on the cosine similarity of model parameters is then designed to aggregate models from clients with similar distributions. An adaptive tip selection algorithm leveraging change-point detection dynamically determines the number of selected tips. We evaluate t
    
[^36]: 多样化、前k个和高质量的规划模拟器上的规划

    Diverse, Top-k, and Top-Quality Planning Over Simulators. (arXiv:2308.13147v1 [cs.AI])

    [http://arxiv.org/abs/2308.13147](http://arxiv.org/abs/2308.13147)

    本文介绍了一种使用蒙特卡洛树搜索（MCTS）的新颖方法，用于多样化、前k个和高质量的规划模拟器上的规划问题。通过从预生成的搜索树中提取有界规划集，并评估路径相对质量的度量标准，该方法在只有黑箱模拟模型的问题中表现出色，并提出了修改MCTS算法以增加多样性的建议。

    

    多样化、前k个和高质量的规划关注的是生成顺序决策问题的解集。以往这个领域是传统规划器的领域，需要一个问题实例的符号模型。本文提出了一种新颖的替代方法，使用蒙特卡洛树搜索（MCTS），使其适用于只有黑箱模拟模型的问题。我们提出了从预生成的最优搜索树中提取有界规划集的过程，以及评估通过搜索树的路径相对质量的度量标准。我们在具有隐藏信息的路径规划问题上展示了这种方法，并建议修改MCTS算法以增加生成规划的多样性。我们的结果表明，我们的方法可以在传统规划器不适用的领域生成多样化和高质量的规划集。

    Diverse, top-k, and top-quality planning are concerned with the generation of sets of solutions to sequential decision problems. Previously this area has been the domain of classical planners that require a symbolic model of the problem instance. This paper proposes a novel alternative approach that uses Monte Carlo Tree Search (MCTS), enabling application to problems for which only a black-box simulation model is available. We present a procedure for extracting bounded sets of plans from pre-generated search trees in best-first order, and a metric for evaluating the relative quality of paths through a search tree. We demonstrate this approach on a path-planning problem with hidden information, and suggest adaptations to the MCTS algorithm to increase the diversity of generated plans. Our results show that our method can generate diverse and high-quality plan sets in domains where classical planners are not applicable.
    
[^37]: 基于扩散的图像生成模型综述：问题及其解决方案调查

    A Survey of Diffusion Based Image Generation Models: Issues and Their Solutions. (arXiv:2308.13142v1 [cs.CV])

    [http://arxiv.org/abs/2308.13142](http://arxiv.org/abs/2308.13142)

    这项研究对基于扩散的图像生成模型进行了综述，总结了存在的问题以及当前的解决方案。

    

    最近，在大型模型的发展方面取得了显著进展。在ChatGPT的成功之后，引入了许多语言模型，展示出了令人瞩目的性能。类似的进展也在图像生成模型中观察到，如谷歌的Imagen模型、OpenAI的DALL-E 2和稳定的扩散模型，在生成图像方面表现出了令人印象深刻的能力。然而，类似于大型语言模型，这些模型仍然面临未解决的挑战。幸运的是，稳定的扩散模型和其基本数学原理的开源可用性使学术界能够广泛分析当前图像生成模型的性能，并基于这个稳定的扩散框架进行改进。本调查旨在研究与图像生成模型相关的现有问题和当前的解决方案。

    Recently, there has been significant progress in the development of large models. Following the success of ChatGPT, numerous language models have been introduced, demonstrating remarkable performance. Similar advancements have also been observed in image generation models, such as Google's Imagen model, OpenAI's DALL-E 2, and stable diffusion models, which have exhibited impressive capabilities in generating images. However, similar to large language models, these models still encounter unresolved challenges. Fortunately, the availability of open-source stable diffusion models and their underlying mathematical principles has enabled the academic community to extensively analyze the performance of current image generation models and make improvements based on this stable diffusion framework. This survey aims to examine the existing issues and the current solutions pertaining to image generation models.
    
[^38]: 混合遗传算法和爬山算法优化神经网络

    Hybrid Genetic Algorithm and Hill Climbing Optimization for the Neural Network. (arXiv:2308.13099v1 [cs.NE])

    [http://arxiv.org/abs/2308.13099](http://arxiv.org/abs/2308.13099)

    本文提出了一种混合遗传算法和爬山算法的模型，用于在CIFAR-100数据集上优化卷积神经网络。该模型利用遗传算法进行全局搜索和探索搜索空间，在此基础上使用爬山算法进行局部优化，并引入变异操作以避免局部最优解。评估结果表明该模型相较于标准的遗传算法和爬山算法在性能上有所提升。

    

    本文提出了一种混合模型，将遗传算法和爬山算法结合起来，用于在CIFAR-100数据集上优化卷积神经网络（CNN）。该模型利用一个染色体种群来表示CNN模型的超参数。遗传算法用于选择和繁殖最适应的染色体生成新的后代。然后，爬山算法被应用于后代上，进一步优化它们的超参数。引入变异操作以使种群多样化，并防止算法陷入局部最优解。遗传算法用于全局搜索和探索搜索空间，而爬山算法用于局部优化有前景的解决方案。目标函数是在CIFAR-100测试集上训练的神经网络的准确率。通过与标准的遗传算法和爬山算法进行比较，评估了混合模型的性能。

    In this paper, we propose a hybrid model combining genetic algorithm and hill climbing algorithm for optimizing Convolutional Neural Networks (CNNs) on the CIFAR-100 dataset. The proposed model utilizes a population of chromosomes that represent the hyperparameters of the CNN model. The genetic algorithm is used for selecting and breeding the fittest chromosomes to generate new offspring. The hill climbing algorithm is then applied to the offspring to further optimize their hyperparameters. The mutation operation is introduced to diversify the population and to prevent the algorithm from getting stuck in local optima. The Genetic Algorithm is used for global search and exploration of the search space, while Hill Climbing is used for local optimization of promising solutions. The objective function is the accuracy of the trained neural network on the CIFAR-100 test set. The performance of the hybrid model is evaluated by comparing it with the standard genetic algorithm and hill-climbing
    
[^39]: 对一辆自主Formula SAE赛车的基于强化学习的控制进行竞赛

    Racing Towards Reinforcement Learning based control of an Autonomous Formula SAE Car. (arXiv:2308.13088v1 [cs.RO])

    [http://arxiv.org/abs/2308.13088](http://arxiv.org/abs/2308.13088)

    本文介绍了使用深度强化学习控制自主Formula SAE赛车的研究。实验结果表明，这种方法可以在模拟环境中成功学习赛车驾驶，并成功应用于真实赛道上的物理平台。该研究提供对方法的局限性和未来研究方向的指导。

    

    随着自主导航研究的日益流行，Formula Student（FS）赛事在其比赛列表中引入了自动驾驶车辆（DV）类别。本文介绍了对这些比赛中的自主FS赛车利用深度强化学习（RL）进行端到端控制的初步研究。我们在类似于实际设计的赛道上，通过在Turtlebot2平台上进行模拟进行训练，使用两种最先进的RL算法。结果表明，我们的方法可以成功地在模拟环境中学习赛车驾驶，并将其转移到真实世界的赛道上的物理平台。最后，我们对所提出方法的限制进行了探讨，并提出了将强化学习应用于全尺度自主FS赛车的未来方向的指导。

    With the rising popularity of autonomous navigation research, Formula Student (FS) events are introducing a Driverless Vehicle (DV) category to their event list. This paper presents the initial investigation into utilising Deep Reinforcement Learning (RL) for end-to-end control of an autonomous FS race car for these competitions. We train two state-of-the-art RL algorithms in simulation on tracks analogous to the full-scale design on a Turtlebot2 platform. The results demonstrate that our approach can successfully learn to race in simulation and then transfer to a real-world racetrack on the physical platform. Finally, we provide insights into the limitations of the presented approach and guidance into the future directions for applying RL toward full-scale autonomous FS racing.
    
[^40]: 多元时间序列异常检测: 炫酷算法和有缺陷的评估方法

    Multivariate Time Series Anomaly Detection: Fancy Algorithms and Flawed Evaluation Methodology. (arXiv:2308.13068v1 [cs.LG])

    [http://arxiv.org/abs/2308.13068](http://arxiv.org/abs/2308.13068)

    多元时间序列异常检测是一个研究领域，但目前存在组织不够有序和评估协议有缺陷的问题。文章评估了许多最近算法的性能，并指出了针对多元时间序列异常检测的评估协议存在的问题及如何缓解这些问题的方法。

    

    多元时间序列（MVTS）的异常检测是一个长期存在且具有挑战性的研究课题，近年来吸引了工业界和学术界的大量研究努力。然而，对文献的仔细研究让我们意识到：1）该领域的社区活跃，但并不像计算机视觉（CV）和自然语言处理（NLP）等其他机器学习领域那样组织有序；2）大多数提出的解决方案使用不合适或存在明显缺陷的评估协议进行评估，缺乏科学基础。其中一个非常流行的协议，即所谓的 \pa 协议，是如此有缺陷，以至于随机猜测可以显示系统地优于迄今为止开发的\emph{所有}算法。在本文中，我们使用更健壮的协议对许多最近的算法进行回顾和评估，并讨论在MVTS异常检测的背景下，一个本来很好的协议可能存在的问题以及如何减轻这些问题。我们还对基准数据集表达了关切。

    Multivariate Time Series (MVTS) anomaly detection is a long-standing and challenging research topic that has attracted tremendous research effort from both industry and academia recently. However, a careful study of the literature makes us realize that 1) the community is active but not as organized as other sibling machine learning communities such as Computer Vision (CV) and Natural Language Processing (NLP), and 2) most proposed solutions are evaluated using either inappropriate or highly flawed protocols, with an apparent lack of scientific foundation. So flawed is one very popular protocol, the so-called \pa protocol, that a random guess can be shown to systematically outperform \emph{all} algorithms developed so far. In this paper, we review and evaluate many recent algorithms using more robust protocols and discuss how a normally good protocol may have weaknesses in the context of MVTS anomaly detection and how to mitigate them. We also share our concerns about benchmark dataset
    
[^41]: 因果鹦鹉：大型语言模型可能谈论因果性，但它们并不具备因果性

    Causal Parrots: Large Language Models May Talk Causality But Are Not Causal. (arXiv:2308.13067v1 [cs.AI])

    [http://arxiv.org/abs/2308.13067](http://arxiv.org/abs/2308.13067)

    大型语言模型（LLM）不能具备因果性，它们只是重复嵌入在数据中的因果知识。

    

    有人认为规模是实现人工智能的全部所需，甚至可以涵盖因果模型。我们明确指出，大型语言模型（LLM）不能具备因果性，并解释为什么有时我们可能有这种感觉。为此，我们定义并举例了一种新的结构因果模型（SCM）的子群，称之为元SCM，它在其变量中编码关于其他SCM的因果事实。我们猜测，在LLM成功进行因果推理的情况下，背后可能存在一个相应的元SCM，在其数据中展示了自然语言中因果事实之间的相关性，而LLM最终是在这些数据上进行训练的。如果我们的假设成立，那么这将意味着LLM就像鹦鹉一样，它们只是重复嵌入在数据中的因果知识。我们的实证分析提供了支持证据，表明当前的LLM甚至是弱“因果鹦鹉”。

    Some argue scale is all what is needed to achieve AI, covering even causal models. We make it clear that large language models (LLMs) cannot be causal and give reason onto why sometimes we might feel otherwise. To this end, we define and exemplify a new subgroup of Structural Causal Model (SCM) that we call meta SCM which encode causal facts about other SCM within their variables. We conjecture that in the cases where LLM succeed in doing causal inference, underlying was a respective meta SCM that exposed correlations between causal facts in natural language on whose data the LLM was ultimately trained. If our hypothesis holds true, then this would imply that LLMs are like parrots in that they simply recite the causal knowledge embedded in the data. Our empirical analysis provides favoring evidence that current LLMs are even weak `causal parrots.'
    
[^42]: 多重BERT用于推荐系统中的嵌入

    Multi-BERT for Embeddings for Recommendation System. (arXiv:2308.13050v1 [cs.IR])

    [http://arxiv.org/abs/2308.13050](http://arxiv.org/abs/2308.13050)

    本文提出了一种使用多种最先进的自然语言处理模型来生成文档嵌入的方法，并在图书推荐任务中取得了比单一模型更好的性能。

    

    本文提出了一种新颖的方法，使用句子BERT（SBERT）和RoBERTa两种最先进的自然语言处理模型来生成文档嵌入。我们的方法将句子视为标记，并为它们生成嵌入，使模型能够捕捉文档内的句子内部和句子间关系。我们在图书推荐任务上评估了我们的模型，并展示了它在生成语义丰富和准确的文档嵌入方面的有效性。为了评估我们的方法的性能，我们在Goodreads数据集上进行了图书推荐任务的实验。我们将使用我们的MULTI-BERT模型生成的文档嵌入与仅使用SBERT生成的嵌入进行比较。我们使用精确度作为评估指标来比较生成嵌入的质量。结果表明，我们的模型在生成嵌入的质量方面始终优于SBERT。此外，我们发现...

    In this paper, we propose a novel approach for generating document embeddings using a combination of Sentence-BERT (SBERT) and RoBERTa, two state-of-the-art natural language processing models. Our approach treats sentences as tokens and generates embeddings for them, allowing the model to capture both intra-sentence and inter-sentence relations within a document. We evaluate our model on a book recommendation task and demonstrate its effectiveness in generating more semantically rich and accurate document embeddings. To assess the performance of our approach, we conducted experiments on a book recommendation task using the Goodreads dataset. We compared the document embeddings generated using our MULTI-BERT model to those generated using SBERT alone. We used precision as our evaluation metric to compare the quality of the generated embeddings. Our results showed that our model consistently outperformed SBERT in terms of the quality of the generated embeddings. Furthermore, we found tha
    
[^43]: 不完整观测数据的联邦因果效应学习

    Federated Learning of Causal Effects from Incomplete Observational Data. (arXiv:2308.13047v1 [cs.LG])

    [http://arxiv.org/abs/2308.13047](http://arxiv.org/abs/2308.13047)

    我们提出了一种联邦学习的方法，可以从多个分布式和不完整的数据源中进行因果推断，估计因果效应并解决因为缺失值引入的偏差问题。

    

    分布式和不完整的数据源在实际应用中很常见，对因果推断提出了巨大挑战。由于隐私限制，这些数据源无法合并为一个实体，而其中的缺失值可能会引入偏差到因果估计中。我们提出了一种新的方法，可以从多个分布式和不完整的数据源中进行联邦因果推断，从而估计因果效应。我们的方法将损失函数拆分为多个组件，每个组件对应于具有缺失值的特定数据源。我们的方法在缺失随机假设下考虑了缺失数据，并估计了因果估计的高阶统计量。我们的方法从分散的数据源中恢复观察到的混淆变量的条件分布，以识别因果效应。我们的框架估计了异质的条件分布以应对不完整的数据源。

    Decentralized and incomplete data sources are prevalent in real-world applications, posing a formidable challenge for causal inference. These sources cannot be consolidated into a single entity owing to privacy constraints, and the presence of missing values within them can potentially introduce bias to the causal estimands. We introduce a new approach for federated causal inference from incomplete data, enabling the estimation of causal effects from multiple decentralized and incomplete data sources. Our approach disentangles the loss function into multiple components, each corresponding to a specific data source with missing values. Our approach accounts for the missing data under the missing at random assumption, while also estimating higher-order statistics of the causal estimands. Our method recovers the conditional distribution of missing confounders given the observed confounders from the decentralized data sources to identify causal effects. Our framework estimates heterogeneou
    
[^44]: 使用精细调整的Llama 2 GPT模型进行金融新闻分析

    Financial News Analytics Using Fine-Tuned Llama 2 GPT Model. (arXiv:2308.13032v1 [cs.CL])

    [http://arxiv.org/abs/2308.13032](http://arxiv.org/abs/2308.13032)

    本研究通过精细调整的Llama 2模型实现了金融新闻的多任务分析，包括文本分析、摘要和情感提取等。实验结果显示，提取的命名实体情感可以作为有监督机器学习模型的预测特征。

    

    本文考虑了使用精细调整的Llama 2 Large Language Model (LLM) 对金融新闻进行多任务分析的可能性。通过PEFT/LoRA方法对模型进行精细调整，主要包括从金融市场角度分析文本、突出文本的主要观点、对文本进行摘要和提取具有适当情感的命名实体等任务。实验结果表明，经过精细调整的Llama 2模型能够进行多任务的金融新闻分析，其响应的结构可以部分为结构化文本，另一部分数据可以采用JSON格式进一步处理。提取的命名实体情感可以被视为具有定量目标变量的监督机器学习模型的预测特征。

    The paper considers the possibility to fine-tune Llama 2 Large Language Model (LLM) for the multitask analysis of financial news. For fine-tuning, the PEFT/LoRA based approach was used. In the study, the model was fine-tuned for the following tasks: analysing a text from financial market perspectives, highlighting main points of a text, summarizing a text and extracting named entities with appropriate sentiments. The obtained results show that the fine-tuned Llama 2 model can perform a multitask financial news analysis with a specified structure of response, part of response can be a structured text and another part of data can have JSON format for further processing. Extracted sentiments for named entities can be considered as predictive features in supervised machine learning models with quantitative target variables.
    
[^45]: 具有解耦表示的通用零样本说话者自适应语音合成

    Generalizable Zero-Shot Speaker Adaptive Speech Synthesis with Disentangled Representations. (arXiv:2308.13007v1 [cs.SD])

    [http://arxiv.org/abs/2308.13007](http://arxiv.org/abs/2308.13007)

    本研究提出了一种通用的零样本说话者自适应语音合成模型，通过解耦表示学习来改善模型在未知说话者方面的泛化能力。

    

    虽然大多数关于语音合成的研究都集中在为数据集内的说话者合成高质量的语音上，但同样重要且未解决的问题是为没有参考数据的数据集外的未知说话者合成语音，即说话者自适应语音合成。许多研究提出了针对此任务的零样本说话者自适应文本到语音和声音转换方法。然而，由于模型在分布外数据中的泛化能力差，当前大多数方法在为未知说话者（即不在训练数据集中的说话者）合成语音时，会导致自然度和说话者相似性的降低。为了解决这个问题，我们提出了GZS-TV，一种通用的零样本说话者自适应文本到语音和声音转换模型。GZS-TV引入了解耦表示学习，以提高模型的泛化能力，并利用表示学习技术进行说话者嵌入提取和音色转换。

    While most research into speech synthesis has focused on synthesizing high-quality speech for in-dataset speakers, an equally essential yet unsolved problem is synthesizing speech for unseen speakers who are out-of-dataset with limited reference data, i.e., speaker adaptive speech synthesis. Many studies have proposed zero-shot speaker adaptive text-to-speech and voice conversion approaches aimed at this task. However, most current approaches suffer from the degradation of naturalness and speaker similarity when synthesizing speech for unseen speakers (i.e., speakers not in the training dataset) due to the poor generalizability of the model in out-of-distribution data. To address this problem, we propose GZS-TV, a generalizable zero-shot speaker adaptive text-to-speech and voice conversion model. GZS-TV introduces disentangled representation learning for both speaker embedding extraction and timbre transformation to improve model generalization and leverages the representation learning
    
[^46]: 用于360度视频显著性预测的球形视觉Transformer

    Spherical Vision Transformer for 360-degree Video Saliency Prediction. (arXiv:2308.13004v1 [cs.CV])

    [http://arxiv.org/abs/2308.13004](http://arxiv.org/abs/2308.13004)

    本论文提出了一种用于360度视频显著性预测的球形视觉Transformer模型，通过引入切线图像表示和球形几何感知的机制，以及一种无监督正则化项来减少伪像，实现了有效的全景视频理解和显著性预测。

    

    对全景视频（ODVs）的兴趣日益增长，该视频捕捉了整个视野（FOV），因此在计算机视觉中，360度显著性预测变得重要起来。然而，预测人类在360度场景中看向何处面临独特的挑战，包括球形失真、高分辨率和有限的标记数据。我们提出了一种名为SalViT360的基于视觉Transformer的全景视频模型，该模型利用切线图像表示。我们引入了一种球形几何感知的时空自注意机制，能够有效理解全景视频。此外，我们还提出了一种基于一致性的无监督正则化项，用于投影-based的360度密集预测模型，以减少投影反演后预测中出现的伪像。我们的方法是第一个采用切线图像进行全景显著性预测的方法，我们在三个全景视频显著性数据集上的实验结果证明了其效果。

    The growing interest in omnidirectional videos (ODVs) that capture the full field-of-view (FOV) has gained 360-degree saliency prediction importance in computer vision. However, predicting where humans look in 360-degree scenes presents unique challenges, including spherical distortion, high resolution, and limited labelled data. We propose a novel vision-transformer-based model for omnidirectional videos named SalViT360 that leverages tangent image representations. We introduce a spherical geometry-aware spatiotemporal self-attention mechanism that is capable of effective omnidirectional video understanding. Furthermore, we present a consistency-based unsupervised regularization term for projection-based 360-degree dense-prediction models to reduce artefacts in the predictions that occur after inverse projection. Our approach is the first to employ tangent images for omnidirectional saliency prediction, and our experimental results on three ODV saliency datasets demonstrate its effect
    
[^47]: 异构警戒信号行为下的周界控制：基于半模型依赖的强化学习方法

    Perimeter Control with Heterogeneous Cordon Signal Behaviors: A Semi-Model Dependent Reinforcement Learning Approach. (arXiv:2308.12985v1 [cs.AI])

    [http://arxiv.org/abs/2308.12985](http://arxiv.org/abs/2308.12985)

    本文引入了一个基于半模型依赖的多智能体强化学习（MARL）框架来实现具有异构警戒信号行为的周界控制。

    

    周界控制（PC）策略旨在通过监测受保护网络（PN）的转移流量来解决城市道路网络在过饱和情况下的控制问题。现有研究中对警戒信号的均匀测量率忽视了交叉口级别的交通状态的多样性，这可能导致严重的局部交通拥堵和破坏网络稳定性。本文引入了一个基于半模型依赖的多智能体强化学习（MARL）框架来实现具有异构警戒信号行为的周界控制。所提出的策略将基于MARL的信号控制方法与集中式反馈PC策略相结合，并应用于PN的警戒信号。它是一个两阶段的系统，反馈PC策略检测PN的整体交通状态，然后将本地指令分发给由MARL框架中的智能体控制的警戒信号。每个警戒信号都独立而不同，创建了弹性和分布

    Perimeter Control (PC) strategies have been proposed to address urban road network control in oversaturated situations by monitoring transfer flows of the Protected Network (PN). The uniform metering rate for cordon signals in existing studies ignores the variety of local traffic states at the intersection level, which may cause severe local traffic congestion and ruin the network stability. This paper introduces a semi-model dependent Multi-Agent Reinforcement Learning (MARL) framework to conduct PC with heterogeneous cordon signal behaviors. The proposed strategy integrates the MARL-based signal control method with centralized feedback PC policy and is applied to cordon signals of the PN. It operates as a two-stage system, with the feedback PC strategy detecting the overall traffic state within the PN and then distributing local instructions to cordon signals controlled by agents in the MARL framework. Each cordon signal acts independently and differently, creating a slack and distri
    
[^48]: AI音乐生成工具和模型综述

    A Survey of AI Music Generation Tools and Models. (arXiv:2308.12982v1 [cs.SD])

    [http://arxiv.org/abs/2308.12982](http://arxiv.org/abs/2308.12982)

    这篇论文提供了一个全面的AI音乐生成工具综述，分类了基于参数、文本和图像的音乐生成方法，并详细介绍了这些工具的优点、局限性以及选择过程中需要考虑的因素。

    

    在这项工作中，我们提供了一个全面的AI音乐生成工具的综述，包括研究项目和商业应用。为了进行我们的分析，我们将音乐生成方法分为三类：基于参数、基于文本和基于图像。我们的综述突出了这些工具的多样性和功能特点，适用于从普通听众到专业音乐人的各种用户。我们观察到每个工具都有自己的优点和局限性。因此，我们编制了一个全面的列表，列出了在选择工具过程中应考虑的这些因素。此外，我们的综述提供了关于AI音乐生成的底层机制和挑战的重要见解。

    In this work, we provide a comprehensive survey of AI music generation tools, including both research projects and commercialized applications. To conduct our analysis, we classified music generation approaches into three categories: parameter-based, text-based, and visual-based classes. Our survey highlights the diverse possibilities and functional features of these tools, which cater to a wide range of users, from regular listeners to professional musicians. We observed that each tool has its own set of advantages and limitations. As a result, we have compiled a comprehensive list of these factors that should be considered during the tool selection process. Moreover, our survey offers critical insights into the underlying mechanisms and challenges of AI music generation.
    
[^49]: 基于开放研究知识图谱的科学论文知识获取方法

    An approach based on Open Research Knowledge Graph for Knowledge Acquisition from scientific papers. (arXiv:2308.12981v1 [cs.DL])

    [http://arxiv.org/abs/2308.12981](http://arxiv.org/abs/2308.12981)

    这项研究利用开放研究知识图谱作为计算机辅助工具，从科学论文中组织关键洞察，以加速知识获取的过程。

    

    科学论文可以分为两个主要部分，即元数据和全文。元数据提供了论文的简要概述，而全文包含了对其他研究人员有价值的关键洞察。为了从科学论文中获取元数据和关键洞察，知识获取是一项核心活动。它包括收集、分析和组织嵌入在科学论文中的知识，以便在需要时可以使用和重复使用。鉴于丰富的科学文献，手动知识获取是一项繁琐的任务。因此，通常采用计算机辅助和（半）自动策略。我们在本研究中的目的是：利用与本体学习相关的论文策划开放研究知识图谱（ORKG），并使用ORKG作为计算机辅助工具，组织从研究论文中提取的关键洞察。该方法被用于记录“流行病学监测系统设计”。

    A scientific paper can be divided into two major constructs which are Metadata and Full-body text. Metadata provides a brief overview of the paper while the Full-body text contains key-insights that can be valuable to fellow researchers. To retrieve metadata and key-insights from scientific papers, knowledge acquisition is a central activity. It consists of gathering, analyzing and organizing knowledge embedded in scientific papers in such a way that it can be used and reused whenever needed. Given the wealth of scientific literature, manual knowledge acquisition is a cumbersome task. Thus, computer-assisted and (semi-)automatic strategies are generally adopted. Our purpose in this research was two fold: curate Open Research Knowledge Graph (ORKG) with papers related to ontology learning and define an approach using ORKG as a computer-assisted tool to organize key-insights extracted from research papers. This approach was used to document the "epidemiological surveillance systems desig
    
[^50]: 语言知识能改进视觉-语言预训练中的多模态对齐吗？

    Can Linguistic Knowledge Improve Multimodal Alignment in Vision-Language Pretraining?. (arXiv:2308.12898v1 [cs.MM])

    [http://arxiv.org/abs/2308.12898](http://arxiv.org/abs/2308.12898)

    本论文研究了语言知识在多模态对齐中的作用，设计并发布了一个多模态对齐探测基准来检测关键的语言组成部分。

    

    多媒体领域对通过多模态预训练神经网络模型感知和表达物理世界展现出了强烈的兴趣，其中视觉-语言相关的研究是当前最吸引人的话题之一。然而，目前对以下两个问题的探索非常有限：1）在视觉-语言预训练中是否可以提取关键的语言知识（如语义和句法），2）这种语言知识如何影响或增强多模态对齐。因此，本文旨在阐明全面的语言知识，包括语义表达和句法结构，对多模态对齐的影响。具体而言，我们设计并发布了SNARE，第一个大规模的多模态对齐探测基准，来检测关键的语言组成部分，如词汇、语义和句法知识，包含了四个任务：语义结构、否定逻辑、属性归属和关系组合。基于我们的实验结果，我们证明了.....

    The multimedia community has shown a significant interest in perceiving and representing the physical world with multimodal pretrained neural network models, and among them, the visual-language pertaining (VLP) is, currently, the most captivating topic. However, there have been few endeavors dedicated to the exploration of 1) whether essential linguistic knowledge (e.g., semantics and syntax) can be extracted during VLP, and 2) how such linguistic knowledge impact or enhance the multimodal alignment. In response, here we aim to elucidate the impact of comprehensive linguistic knowledge, including semantic expression and syntactic structure, on multimodal alignment. Specifically, we design and release the SNARE, the first large-scale multimodal alignment probing benchmark, to detect the vital linguistic components, e.g., lexical, semantic, and syntax knowledge, containing four tasks: Semantic structure, Negation logic, Attribute ownership, and Relationship composition. Based on our prop
    
[^51]: 扩展性和指导调优的扩散语言模型能够完成多种任务

    Diffusion Language Models Can Perform Many Tasks with Scaling and Instruction-Finetuning. (arXiv:2308.12219v1 [cs.CL])

    [http://arxiv.org/abs/2308.12219](http://arxiv.org/abs/2308.12219)

    本文研究表明，通过扩展扩散语言模型的数据、规模和任务，可以有效使其成为强大的语言学习者。实验证明，扩展扩散语言模型在解决通用语言任务方面能够持续提高性能。

    

    最近生成式人工智能的兴起得益于扩散概率模型的生成能力和大规模语言模型的可扩展性。尽管具有潜力，但扩散语言模型是否能够解决与自回归模型相媲美的通用语言任务仍然不明确。本文证明了在数据、规模和任务方面扩展扩散模型能够有效使其成为强大的语言学习者。我们通过先通过掩码语言建模预训练从大规模数据中获取知识，再通过扩散适应将预训练的掩码语言模型改进为扩散语言模型，通过任务特定的微调和指导调优来发掘其在解决通用语言任务方面的多样性。实验证明，扩展扩散语言模型能够在下游语言任务中持续提高性能。

    The recent surge of generative AI has been fueled by the generative power of diffusion probabilistic models and the scalable capabilities of large language models. Despite their potential, it remains elusive whether diffusion language models can solve general language tasks comparable to their autoregressive counterparts. This paper demonstrates that scaling diffusion models w.r.t. data, sizes, and tasks can effectively make them strong language learners. We build competent diffusion language models at scale by first acquiring knowledge from massive data via masked language modeling pretraining thanks to their intrinsic connections. We then reprogram pretrained masked language models into diffusion language models via diffusive adaptation, wherein task-specific finetuning and instruction finetuning are explored to unlock their versatility in solving general language tasks. Experiments show that scaling diffusion language models consistently improves performance across downstream langua
    
[^52]: 探索用于异常检测的一类分类的优化目标

    Exploring the Optimization Objective of One-Class Classification for Anomaly Detection. (arXiv:2308.11898v1 [cs.CV])

    [http://arxiv.org/abs/2308.11898](http://arxiv.org/abs/2308.11898)

    本研究探索了一类分类（OCC）的优化目标，发现在适当的范数空间中，任何空间都可以作为超球心的等效替代，而不依赖于训练样本的分布假设。

    

    一类分类（OCC）是一种长期以来用于异常检测的方法。借助预训练骨干网络的强大表示能力，OCC方法在性能上有了显著的提升。通常，大多数这些OCC方法采用迁移学习来增强预训练骨干网络特征的区分性，从而实现了卓越的效果。虽然当前大多数方法强调特征迁移策略，但我们认为OCC方法中的优化目标空间也可能是影响性能的一个关键因素。在本工作中，我们对OCC的优化目标进行了全面调查。通过严格的理论分析和推导，我们揭示了一个关键的洞见：在适当的范数空间中，任何空间都可以作为超球心的等效替代，无需依赖于训练样本的分布假设。此外，我们提供了确定可行域的指南。

    One-class classification (OCC) is a longstanding method for anomaly detection. With the powerful representation capability of the pre-trained backbone, OCC methods have witnessed significant performance improvements. Typically, most of these OCC methods employ transfer learning to enhance the discriminative nature of the pre-trained backbone's features, thus achieving remarkable efficacy. While most current approaches emphasize feature transfer strategies, we argue that the optimization objective space within OCC methods could also be an underlying critical factor influencing performance. In this work, we conducted a thorough investigation into the optimization objective of OCC. Through rigorous theoretical analysis and derivation, we unveil a key insights: any space with the suitable norm can serve as an equivalent substitute for the hypersphere center, without relying on the distribution assumption of training samples. Further, we provide guidelines for determining the feasible domai
    
[^53]: 自我欺骗：逆向破解大型语言模型的语义防火墙

    Self-Deception: Reverse Penetrating the Semantic Firewall of Large Language Models. (arXiv:2308.11521v1 [cs.CL])

    [http://arxiv.org/abs/2308.11521](http://arxiv.org/abs/2308.11521)

    这篇论文研究了大型语言模型的越狱问题，并提出了一种自动越狱方法，介绍了语义防火墙的概念和三种技术实现方法。

    

    大型语言模型（LLM），如ChatGPT，具有接近人工通用智能的惊人能力。虽然为各种社会需求提供了便利，但LLM也降低了生成有害内容的成本。因此，LLM开发人员已经部署了语义级的防御机制，用于识别和拒绝可能导致不适当内容的提示。不幸的是，这些防御机制并不完全可靠，一些攻击者已经设计出了“越狱”提示，临时使LLM忘记内容防御规则并回答任何不适当的问题。迄今为止，业界和学术界尚无关于这些语义级攻击和防御原则的明确解释。本文研究了LLM越狱问题，并首次提出了一种自动越狱方法。我们提出了语义防火墙的概念，并提供了三种技术实现方法。

    Large language models (LLMs), such as ChatGPT, have emerged with astonishing capabilities approaching artificial general intelligence. While providing convenience for various societal needs, LLMs have also lowered the cost of generating harmful content. Consequently, LLM developers have deployed semantic-level defenses to recognize and reject prompts that may lead to inappropriate content. Unfortunately, these defenses are not foolproof, and some attackers have crafted "jailbreak" prompts that temporarily hypnotize the LLM into forgetting content defense rules and answering any improper questions. To date, there is no clear explanation of the principles behind these semantic-level attacks and defenses in both industry and academia.  This paper investigates the LLM jailbreak problem and proposes an automatic jailbreak method for the first time. We propose the concept of a semantic firewall and provide three technical implementation approaches. Inspired by the attack that penetrates trad
    
[^54]: 贝叶斯多项式神经网络和多项式神经常微分方程

    Bayesian polynomial neural networks and polynomial neural ordinary differential equations. (arXiv:2308.10892v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2308.10892](http://arxiv.org/abs/2308.10892)

    本研究提出了贝叶斯推断方法来改善多项式神经网络和多项式神经常微分方程在方程恢复问题中的表现，其中拉普拉斯近似是最佳的方法。这种方法可以推广到更广泛的符号神经网络类别。

    

    多项式神经网络和多项式神经常微分方程是最近用于科学和工程问题方程恢复的两种强大方法。然而，这些方法只能提供模型参数的点估计，并且目前不能适应噪声数据。我们通过开发和验证以下贝叶斯推断方法来解决这个挑战: 拉普拉斯近似、马尔可夫链蒙特卡洛(MCMC)采样方法和变分推断。我们发现拉普拉斯近似是这类问题的最佳方法。我们的工作可以轻松扩展到多项式神经网络所属的更广泛的符号神经网络类别。

    Symbolic regression with polynomial neural networks and polynomial neural ordinary differential equations (ODEs) are two recent and powerful approaches for equation recovery of many science and engineering problems. However, these methods provide point estimates for the model parameters and are currently unable to accommodate noisy data. We address this challenge by developing and validating the following Bayesian inference methods: the Laplace approximation, Markov Chain Monte Carlo (MCMC) sampling methods, and variational inference. We have found the Laplace approximation to be the best method for this class of problems. Our work can be easily extended to the broader class of symbolic neural networks to which the polynomial neural network belongs.
    
[^55]: 人工上涌能源管理的深度强化学习

    Deep Reinforcement Learning for Artificial Upwelling Energy Management. (arXiv:2308.10199v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2308.10199](http://arxiv.org/abs/2308.10199)

    通过使用深度强化学习算法，我们提出了一种新颖的能源管理方法来优化操作人工上涌系统。通过将问题建模为马尔可夫决策过程，并结合分位网络和深度竞争网络的思想，我们的方法在提高能源效率方面取得了显著成效。

    

    近年来，人工上涌（AU）作为一种将富含营养的底层水提升到海面、刺激海藻生长并增加海洋碳封存的方法，受到了越来越多的关注。这导致在中国开发了第一个太阳能供电和空气增压的AU系统（AUS）。然而，在复杂的海洋环境中高效调度气体喷射系统仍然是操作AUS的关键挑战，因为它有潜力显著提高能源效率。为了解决这一挑战，我们提出了一种利用深度强化学习（DRL）算法开发AUS运行的高效策略的新能源管理方法。具体而言，我们将最大化AUS能源效率的问题建模为马尔可夫决策过程，并将分布式强化学习（QR-DQN）中的分位网络与深度竞争网络相结合以解决这个问题。通过大量的仿真实验，我们证明了我们的方法可以显著提高AUS的能源效率。

    The potential of artificial upwelling (AU) as a means of lifting nutrient-rich bottom water to the surface, stimulating seaweed growth, and consequently enhancing ocean carbon sequestration, has been gaining increasing attention in recent years. This has led to the development of the first solar-powered and air-lifted AU system (AUS) in China. However, efficient scheduling of air injection systems in complex marine environments remains a crucial challenge in operating AUS, as it holds the potential to significantly improve energy efficiency. To tackle this challenge, we propose a novel energy management approach that utilizes deep reinforcement learning (DRL) algorithm to develop efficient strategies for operating AUS. Specifically, we formulate the problem of maximizing the energy efficiency of AUS as a Markov decision process and integrate the quantile network in distributional reinforcement learning (QR-DQN) with the deep dueling network to solve it. Through extensive simulations, w
    
[^56]: 超越共享：冲突感知的多变量时间序列异常检测

    Beyond Sharing: Conflict-Aware Multivariate Time Series Anomaly Detection. (arXiv:2308.08915v1 [cs.LG])

    [http://arxiv.org/abs/2308.08915](http://arxiv.org/abs/2308.08915)

    这篇论文提出了一种冲突感知的多变量时间序列异常检测算法，该算法通过为每个指标提供独特的结构来缓解指标回归目标之间的冲突。

    

    大量的关键绩效指标(KPI)以多变量时间序列数据(MTS)的形式进行监测，以确保软件应用程序和服务系统的可靠性。准确检测MTS的异常对于后续的故障排除非常关键。异常的稀缺性和手动标记导致了各种自监督的MTS异常检测方法的发展，这些方法优化了一个涵盖所有指标回归目标/损失的整体目标/损失。然而，我们的实证研究发现了指标回归目标之间冲突的普遍存在，导致MTS模型在不同的损失中挣扎。这一关键方面显著影响检测性能，但在现有方法中被忽视了。为了解决这个问题，通过模仿多门专家混合模型(MMoE)的设计，我们引入了CAD，一种冲突感知的多变量KPI异常检测算法。CAD为每个指标提供了一个独特的结构，以缓解冲突。

    Massive key performance indicators (KPIs) are monitored as multivariate time series data (MTS) to ensure the reliability of the software applications and service system. Accurately detecting the abnormality of MTS is very critical for subsequent fault elimination. The scarcity of anomalies and manual labeling has led to the development of various self-supervised MTS anomaly detection (AD) methods, which optimize an overall objective/loss encompassing all metrics' regression objectives/losses. However, our empirical study uncovers the prevalence of conflicts among metrics' regression objectives, causing MTS models to grapple with different losses. This critical aspect significantly impacts detection performance but has been overlooked in existing approaches. To address this problem, by mimicking the design of multi-gate mixture-of-experts (MMoE), we introduce CAD, a Conflict-aware multivariate KPI Anomaly Detection algorithm. CAD offers an exclusive structure for each metric to mitigate
    
[^57]: 如何在纠错码变压器中进行遮蔽：系统化与双重遮蔽

    How to Mask in Error Correction Code Transformer: Systematic and Double Masking. (arXiv:2308.08128v1 [cs.LG])

    [http://arxiv.org/abs/2308.08128](http://arxiv.org/abs/2308.08128)

    该论文介绍了在纠错码变压器中使用系统化编码和双重遮蔽的方法，以提高性能和减少计算复杂性。

    

    在通信和存储系统中，纠错码（ECC）对于确保数据可靠性至关重要。随着深度学习在不同领域的应用广泛扩展，神经网络解码器已成为研究的焦点，超越传统解码算法。在这些神经解码器中，纠错码变压器（ECCT）已经实现了最先进的性能，大幅超过其他方法。为了进一步提高ECCT的性能，我们提出了两种新方法。首先，利用ECC的系统编码技术，我们引入了一个新的遮蔽矩阵来改善ECCT的性能并减少计算复杂性。其次，我们提出了一种新的ECCT变压器架构，称为双重遮蔽的ECCT。该架构以并行方式使用两个不同的遮蔽矩阵，以学习遮蔽自注意力块中编码字位之间更多样的特征关系。大量实验证明了我们方法的有效性。

    In communication and storage systems, error correction codes (ECCs) are pivotal in ensuring data reliability. As deep learning's applicability has broadened across diverse domains, there is a growing research focus on neural network-based decoders that outperform traditional decoding algorithms. Among these neural decoders, Error Correction Code Transformer (ECCT) has achieved the state-of-the-art performance, outperforming other methods by large margins. To further enhance the performance of ECCT, we propose two novel methods. First, leveraging the systematic encoding technique of ECCs, we introduce a new masking matrix for ECCT, aiming to improve the performance and reduce the computational complexity. Second, we propose a novel transformer architecture of ECCT called a double-masked ECCT. This architecture employs two different mask matrices in a parallel manner to learn more diverse features of the relationship between codeword bits in the masked self-attention blocks. Extensive si
    
[^58]: 通过受限频率的身份不可知攻击进行人脸加密

    Face Encryption via Frequency-Restricted Identity-Agnostic Attacks. (arXiv:2308.05983v1 [cs.CV])

    [http://arxiv.org/abs/2308.05983](http://arxiv.org/abs/2308.05983)

    通过受限频率的身份不可知攻击进行人脸加密，解决了使用外部扰动加密人脸图像的隐私保护问题，并提出了一种弱黑盒场景下可行的解决方案。

    

    每天有数十亿人在社交媒体上分享他们的日常照片。然而，恶意采集者利用深度人脸识别系统轻松地从这些图片中窃取他们的生物特征信息（例如人脸）。一些研究正在进行中，通过引入难以察觉的扰动来生成加密人脸照片，以减少人脸信息泄漏。然而，现有的研究需要更强的黑盒场景可行性和更自然的视觉外观，这对隐私保护的可行性构成了挑战。为了解决这些问题，我们提出了一种受限频率的身份不可知（FRIA）框架，以从未经授权的人脸识别中加密人脸图像，而无需访问个人信息。对于弱黑盒场景的可行性，我们观察到多个人脸识别模型中的平均特征表示相似，因此我们提出利用通过互联网爬取的数据集中的平均特征作为t

    Billions of people are sharing their daily live images on social media everyday. However, malicious collectors use deep face recognition systems to easily steal their biometric information (e.g., faces) from these images. Some studies are being conducted to generate encrypted face photos using adversarial attacks by introducing imperceptible perturbations to reduce face information leakage. However, existing studies need stronger black-box scenario feasibility and more natural visual appearances, which challenge the feasibility of privacy protection. To address these problems, we propose a frequency-restricted identity-agnostic (FRIA) framework to encrypt face images from unauthorized face recognition without access to personal information. As for the weak black-box scenario feasibility, we obverse that representations of the average feature in multiple face recognition models are similar, thus we propose to utilize the average feature via the crawled dataset from the Internet as the t
    
[^59]: 用大型语言模型进行累积推理的论文

    Cumulative Reasoning With Large Language Models. (arXiv:2308.04371v1 [cs.AI])

    [http://arxiv.org/abs/2308.04371](http://arxiv.org/abs/2308.04371)

    本文提出了一种名为累积推理（CR）的新方法，利用语言模型以累积和迭代的方式模拟人类思维过程，通过将任务分解为较小的组件，简化问题解决过程，取得了优于现有方法的性能，并在逻辑推理和24点游戏中实现了显著提升。

    

    虽然语言模型强大且多功能，但它们通常无法解决高度复杂的问题。这是因为解决复杂问题需要深思熟虑，而在训练过程中对此只有最小程度的指导。在本文中，我们提出了一种新方法，称为累积推理（CR），它以累积和迭代的方式利用语言模型来模拟人类的思维过程。通过将任务分解为较小的组件，我们的方法简化了问题解决过程，使其更易管理和更有效。对于逻辑推理任务，CR在性能上始终超过现有方法，提高了多达9.3％，并在经过策划的FOLIO维基数据集上实现了惊人的98.04％的准确率。在24点游戏的背景下，CR实现了94％的准确率，相比先前最先进的方法，提升了20％。

    While language models are powerful and versatile, they often fail to address highly complex problems. This is because solving complex problems requires deliberate thinking, which has been only minimally guided during training. In this paper, we propose a new method called Cumulative Reasoning (CR), which employs language models in a cumulative and iterative manner to emulate human thought processes. By decomposing tasks into smaller components, \ournameb streamlines the problem-solving process, rendering it both more manageable and effective. For logical inference tasks, CR consistently outperforms existing methods with an improvement up to 9.3\%, and achieves the astonishing accuracy of 98.04\% on the curated FOLIO wiki dataset. In the context of the Game of 24, CR achieves an accuracy of 94\%, which signifies a substantial enhancement of 20\% over the previous state-of-the-art method.
    
[^60]: 用安排取代评分：一种用于学习排序的上下文集合到排列框架

    Replace Scoring with Arrangement: A Contextual Set-to-Arrangement Framework for Learning-to-Rank. (arXiv:2308.02860v1 [cs.IR])

    [http://arxiv.org/abs/2308.02860](http://arxiv.org/abs/2308.02860)

    这篇论文提出了一种新的学习排序框架，名为STARank，它通过直接生成候选项目排列来替代个别评分和排序操作，并且是端到端可微分的。

    

    学习排序是top-N推荐任务中的核心技术，理想的排名器应该是一个从项目集合到排列（即排列）的映射。现有的大多数解决方案属于概率排序原则（PRP）范式，即首先对候选集中的每个项目进行评分，然后执行排序操作以生成排名列表。然而，这些方法忽视了个体评分过程中候选项目之间的上下文依赖性，并且排序操作是不可微分的。为了解决上述问题，我们提出了一种名为STARank的集合到排列排序框架，它直接生成候选项目的排列，而不需要进行个别评分和排序操作，并且是端到端可微分的。因此，STARank可以在只有真实排列可访问但没有项目的真实相关度分数的情况下运行。为此，STARank首先阅读候选项目...

    Learning-to-rank is a core technique in the top-N recommendation task, where an ideal ranker would be a mapping from an item set to an arrangement (a.k.a. permutation). Most existing solutions fall in the paradigm of probabilistic ranking principle (PRP), i.e., first score each item in the candidate set and then perform a sort operation to generate the top ranking list. However, these approaches neglect the contextual dependence among candidate items during individual scoring, and the sort operation is non-differentiable. To bypass the above issues, we propose Set-To-Arrangement Ranking (STARank), a new framework directly generates the permutations of the candidate items without the need for individually scoring and sort operations; and is end-to-end differentiable. As a result, STARank can operate when only the ground-truth permutations are accessible without requiring access to the ground-truth relevance scores for items. For this purpose, STARank first reads the candidate items in t
    
[^61]: ChatMOF: 一种自主人工智能系统用于预测和生成金属-有机骨架

    ChatMOF: An Autonomous AI System for Predicting and Generating Metal-Organic Frameworks. (arXiv:2308.01423v1 [cs.CL])

    [http://arxiv.org/abs/2308.01423](http://arxiv.org/abs/2308.01423)

    ChatMOF是一种自主AI系统，用于预测和生成金属-有机骨架。通过利用大规模语言模型，它能够从文本输入中提取关键细节，并提供适当的回应。该系统通过组合代理、工具包和评估器的核心组件，实现了数据检索、性质预测和结构生成等多个任务。研究进一步展示了在材料科学中使用大型语言模型的优势和潜力。

    

    ChatMOF是一个自主人工智能系统，用于预测和生成金属-有机骨架（MOFs）。通过利用大规模语言模型（gpt-3.5-turbo），ChatMOF从文本输入中提取关键细节并提供适当的回应，从而消除了对刚性结构化查询的需求。该系统由三个核心组件（即代理、工具包和评估器）组成，形成一个强大的流水线，管理多种任务，包括数据检索、性质预测和结构生成。该研究进一步探讨了在材料科学中使用大型语言模型（LLMs）人工智能系统的优点和限制，并展示了其对未来发展的变革潜力。

    ChatMOF is an autonomous Artificial Intelligence (AI) system that is built to predict and generate of metal-organic frameworks (MOFs). By leveraging a large-scale language model (gpt-3.5-turbo), ChatMOF extracts key details from textual inputs and delivers appropriate responses, thus eliminating the necessity for rigid structured queries. The system is comprised of three core components (i.e. an agent, a toolkit, and an evaluator) and it forms a robust pipeline that manages a variety of tasks, including data retrieval, property prediction, and structure generation. The study further explores the merits and constraints of using large language models (LLMs) AI system in material sciences using and showcases its transformative potential for future advancements.
    
[^62]: BubbleML: 用于机器学习的多物理数据集和基准

    BubbleML: A Multi-Physics Dataset and Benchmarks for Machine Learning. (arXiv:2307.14623v1 [cs.LG])

    [http://arxiv.org/abs/2307.14623](http://arxiv.org/abs/2307.14623)

    BubbleML是一个用于机器学习的多物理数据集，通过物理驱动模拟获得准确的地面真实信息，并在各种沸腾场景中验证了其可靠性和潜力。

    

    在相变现象领域，缺乏适用于机器学习训练的可访问和多样化的数据集是一个重要挑战。现有的实验数据集通常受限，可用性有限且地面真实数据稀缺，阻碍了我们对这种复杂多物理现象的理解。为了弥补这一差距，我们提出了BubbleML数据集（https://github.com/HPCForge/BubbleML），它利用物理驱动的模拟为各种沸腾场景提供准确的地面真实信息，包括核泡池沸腾、流动沸腾和亚冷沸腾。这个广泛的数据集涵盖了各种参数，包括不同的重力条件、流量、亚冷水平和壁面过热，总共有51个模拟。BubbleML已经通过实验观察和趋势进行了验证，被确认为机器学习研究的宝贵资源。此外，我们展示了它促进多样化降低温度沸腾研究的潜力。

    In the field of phase change phenomena, the lack of accessible and diverse datasets suitable for machine learning (ML) training poses a significant challenge. Existing experimental datasets are often restricted, with limited availability and sparse ground truth data, impeding our understanding of this complex multi-physics phenomena. To bridge this gap, we present the BubbleML Dataset(https://github.com/HPCForge/BubbleML) which leverages physics-driven simulations to provide accurate ground truth information for various boiling scenarios, encompassing nucleate pool boiling, flow boiling, and sub-cooled boiling. This extensive dataset covers a wide range of parameters, including varying gravity conditions, flow rates, sub-cooling levels, and wall superheat, comprising 51 simulations. BubbleML is validated against experimental observations and trends, establishing it as an invaluable resource for ML research. Furthermore, we showcase its potential to facilitate exploration of diverse dow
    
[^63]: TMR-RD: 用于半监督目标检测的基于训练的模型精化和表示分歧方法

    TMR-RD: Training-based Model Refinement and Representation Disagreement for Semi-Supervised Object Detection. (arXiv:2307.13755v1 [cs.CV])

    [http://arxiv.org/abs/2307.13755](http://arxiv.org/abs/2307.13755)

    在半监督目标检测中，本文提出了基于训练的模型精化(TMR)阶段和表示分歧(RD)策略，用来解决伪标签噪声和教师-学生模型的一致性问题。TMR阶段通过轻量级缩放操作优化模型权重，防止过度拟合或遗忘学到的模式；RD策略帮助保持模型的差异，鼓励学生模型探索互补的表示。

    

    半监督目标检测(SSOD)可以将有限的标记数据和大量的未标记数据结合起来，提高现有目标检测器的性能和泛化能力。尽管取得了许多进展，但是最近的SSOD方法仍然面临着伪标签噪声/误导、经典指数移动平均(EMA)策略和后期训练中教师-学生模型的一致性等挑战。本文提出了一种新颖的基于训练的模型精化(TMR)阶段和简单而有效的表示分歧(RD)策略，以解决经典EMA的局限性和一致性问题。教师-学生模型的TMR阶段优化了轻量级缩放操作，以精化模型的权重，并防止过度拟合或遗忘从未标记数据中学到的模式。同时，RD策略帮助保持这些模型的差异，鼓励学生模型探索互补的表示。此外，我们使用级连回归来生成... (摘要未完整提供)

    Semi-supervised object detection (SSOD) can incorporate limited labeled data and large amounts of unlabeled data to improve the performance and generalization of existing object detectors. Despite many advances, recent SSOD methods are still challenged by noisy/misleading pseudo-labels, classical exponential moving average (EMA) strategy, and the consensus of Teacher-Student models in the latter stages of training. This paper proposes a novel training-based model refinement (TMR) stage and a simple yet effective representation disagreement (RD) strategy to address the limitations of classical EMA and the consensus problem. The TMR stage of Teacher-Student models optimizes the lightweight scaling operation to refine the model's weights and prevent overfitting or forgetting learned patterns from unlabeled data. Meanwhile, the RD strategy helps keep these models diverged to encourage the student model to explore complementary representations. In addition, we use cascade regression to gene
    
[^64]: ACTI在EVALITA 2023中的综述：阴谋论辨识任务概述

    ACTI at EVALITA 2023: Overview of the Conspiracy Theory Identification Task. (arXiv:2307.06954v1 [cs.CL])

    [http://arxiv.org/abs/2307.06954](http://arxiv.org/abs/2307.06954)

    ACTI在EVALITA 2023中的阴谋论辨识任务共有15支团队参与，通过使用大型语言模型判断阴谋内容和分类，得出了关于利用这些模型抵制在在线平台传播错误信息的结论。

    

    阴谋论辨识任务是Evalita 2023首次提出的新共享任务。ACTI挑战仅基于Telegram上的阴谋频道评论，分为两个子任务：(i) 阴谋内容分类：辨识阴谋内容和(ii) 阴谋类别分类：针对特定阴谋理论分类。共有15支团队参与了该任务，总共提交了81个结果。我们说明了基于大型语言模型的最佳方法。最后，我们得出了关于利用这些模型来抵制在在线平台上传播错误信息的结论。

    Conspiracy Theory Identication task is a new shared task proposed for the first time at the Evalita 2023. The ACTI challenge, based exclusively on comments published on conspiratorial channels of telegram, is divided into two subtasks: (i) Conspiratorial Content Classification: identifying conspiratorial content and (ii) Conspiratorial Category Classification about specific conspiracy theory classification. A total of fifteen teams participated in the task for a total of 81 submissions. We illustrate the best performing approaches were based on the utilization of large language models. We finally draw conclusions about the utilization of these models for counteracting the spreading of misinformation in online platforms.
    
[^65]: IntelliGraphs: 用于评估知识图谱生成的数据集

    IntelliGraphs: Datasets for Benchmarking Knowledge Graph Generation. (arXiv:2307.06698v1 [cs.AI])

    [http://arxiv.org/abs/2307.06698](http://arxiv.org/abs/2307.06698)

    IntelliGraphs是一组新的知识图谱数据集，用于评估知识图谱生成。其中包含具有逻辑规则表达的语义的子图，用于评估子图推断的模型。

    

    知识图谱嵌入（KGE）模型用于学习实体和关系的连续表示。文献中一个关键的任务是预测实体之间的缺失链接。然而，知识图谱不仅仅是链接的集合，还具有其结构中的语义。语义在多个下游任务中至关重要，例如查询回答或推理。我们引入了子图推断任务，其中一个模型必须生成可能的并且语义上有效的子图。我们提出了IntelliGraphs，一个包含五个新的知识图谱数据集的集合。IntelliGraphs数据集包含具有逻辑规则表达的语义的子图，用于评估子图推断。我们还设计了产生合成数据集的数据集生成器。我们设计了四个新的基准模型，其中包括基于传统KGE的三个模型。我们评估了它们的表达能力，并展示了这些模型无法捕捉到语义。我们相信这一基准将促进该领域的发展。

    Knowledge Graph Embedding (KGE) models are used to learn continuous representations of entities and relations. A key task in the literature is predicting missing links between entities. However, Knowledge Graphs are not just sets of links but also have semantics underlying their structure. Semantics is crucial in several downstream tasks, such as query answering or reasoning. We introduce the subgraph inference task, where a model has to generate likely and semantically valid subgraphs. We propose IntelliGraphs, a set of five new Knowledge Graph datasets. The IntelliGraphs datasets contain subgraphs with semantics expressed in logical rules for evaluating subgraph inference. We also present the dataset generator that produced the synthetic datasets. We designed four novel baseline models, which include three models based on traditional KGEs. We evaluate their expressiveness and show that these models cannot capture the semantics. We believe this benchmark will encourage the development
    
[^66]: 何去何从：深度学习加速的数字硬件视角

    To Spike or Not To Spike: A Digital Hardware Perspective on Deep Learning Acceleration. (arXiv:2306.15749v1 [cs.NE])

    [http://arxiv.org/abs/2306.15749](http://arxiv.org/abs/2306.15749)

    神经形态计算旨在通过仿真脑部操作来提高深度学习模型的效率，但是在SNNs的高效硬件后端设计上仍需进一步研究。

    

    随着深度学习模型规模的增加，它们在涵盖计算机视觉到自然语言处理等领域变得越来越有竞争力；然而，这是以效率为代价的，因为它们需要越来越多的内存和计算能力。生物脑的功耗效率超过任何大规模深度学习（DL）模型；因此，神经形态计算试图模仿脑部操作，例如基于脉冲的信息处理，以提高DL模型的效率。尽管脑部有诸如高效的信息传输、密集的神经元连接和计算与存储的共同位置等优势，但可用的生物基底严重限制了生物大脑的进化。电子硬件没有相同的约束；因此，虽然建模脉冲神经网络（SNNs）可能揭示了一个谜题的一部分，但对于SNNs的高效硬件后端设计需要进一步研究。

    As deep learning models scale, they become increasingly competitive from domains spanning computer vision to natural language processing; however, this happens at the expense of efficiency since they require increasingly more memory and computing power. The power efficiency of the biological brain outperforms the one of any large-scale deep learning (DL) model; thus, neuromorphic computing tries to mimic the brain operations, such as spike-based information processing, to improve the efficiency of DL models. Despite the benefits of the brain, such as efficient information transmission, dense neuronal interconnects, and the co-location of computation and memory, the available biological substrate has severely constrained the evolution of biological brains. Electronic hardware does not have the same constraints; therefore, while modeling spiking neural networks (SNNs) might uncover one piece of the puzzle, the design of efficient hardware backends for SNNs needs further investigation, po
    
[^67]: 现代约束编程教育：未来的教训

    Modern Constraint Programming Education: Lessons for the Future. (arXiv:2306.13676v1 [cs.CY])

    [http://arxiv.org/abs/2306.13676](http://arxiv.org/abs/2306.13676)

    本文探讨了现代约束编程教育的前景，重点关注在线和虚拟课程，并总结了乔治亚理工学院的约束编程课程的重要收获，提出了一些教学和推广方法以及组织变革的想法，以促进约束编程教育的长期发展。

    

    本文通过一位约束编程教师的视角，详述了现代约束编程教育的前景。文章概述了当前约束编程课程和教学方法，重点关注在线和虚拟课程。随后讨论了乔治亚理工学院在美国亚特兰大所采取的面向工程学生的创新约束编程教育方法。本文总结了乔治亚理工学院的约束编程课程的重要收获，并探讨了约束编程教育的未来。本文提出了一些教学和推广方法以及组织变革的想法，以促进约束编程教育的长期发展。

    This paper details an outlook on modern constraint programming (CP) education through the lens of a CP instructor. A general overview of current CP courses and instructional methods is presented, with a focus on online and virtually-delivered courses. This is followed by a discussion of the novel approach taken to introductory CP education for engineering students at large scale at the Georgia Institute of Technology (Georgia Tech) in Atlanta, GA, USA. The paper summarizes important takeaways from the Georgia Tech CP course and ends with a discussion on the future of CP education. Some ideas for instructional methods, promotional methods, and organizational changes are proposed to aid in the long-term growth of CP education.
    
[^68]: 利用嵌入技术设计面向科学领域的聊天机器人

    Domain-specific ChatBots for Science using Embeddings. (arXiv:2306.10067v1 [cs.CL])

    [http://arxiv.org/abs/2306.10067](http://arxiv.org/abs/2306.10067)

    本论文演示如何利用现有方法和软件工具结合嵌入技术设计面向科学领域的聊天机器人，该机器人能够处理科学文献，提供特定领域的上下文信息，并在初步研究辅助知识方面为物理科学家提供帮助。

    

    大语言模型(LLM)已成为强大的机器学习系统，能处理多种任务。经调整的这些系统已被转化为聊天机器人，能回答用户对广泛话题的查询，提供丰富的信息和创意回答。然而，由于它们在自然科学领域的知识仍不完整，并且面临严格需求和来源标准，因此其在物理科学研究中的应用仍受到限制。本文演示了如何轻松地将现有方法和软件工具结合起来，实现面向特定领域的聊天机器人。该系统能接受现有格式的科学文献，并使用文本嵌入查找来为LLM提供特定领域的上下文信息，以便在撰写回答时使用。我们同样证明了现有的图像嵌入方法可以用于跨出版物图片的搜索和检索。这些结果表明，在提供初步研究辅助知识方面，LLM已经适用于物理科学家的使用，并且进一步的开发可以扩展这些应用。

    Large language models (LLMs) have emerged as powerful machine-learning systems capable of handling a myriad of tasks. Tuned versions of these systems have been turned into chatbots that can respond to user queries on a vast diversity of topics, providing informative and creative replies. However, their application to physical science research remains limited owing to their incomplete knowledge in these areas, contrasted with the needs of rigor and sourcing in science domains. Here, we demonstrate how existing methods and software tools can be easily combined to yield a domain-specific chatbot. The system ingests scientific documents in existing formats, and uses text embedding lookup to provide the LLM with domain-specific contextual information when composing its reply. We similarly demonstrate that existing image embedding methods can be used for search and retrieval across publication figures. These results confirm that LLMs are already suitable for use by physical scientists in acc
    
[^69]: SDR-GAIN：一种用于自动驾驶的高实时遮挡行人姿态完成方法

    SDR-GAIN: A High Real-Time Occluded Pedestrian Pose Completion Method for Autonomous Driving. (arXiv:2306.03538v1 [cs.CV])

    [http://arxiv.org/abs/2306.03538](http://arxiv.org/abs/2306.03538)

    SDR-GAIN是一种用于解决行人姿态中部分遮挡问题的关键点补全方法，它通过对不完整的关键点进行降维，统一特征分布，并使用GAN框架的两种生成模型来完成姿态的补全。该方法的实验表明性能优于基本的GAIN框架。

    

    为了缓解基于人体姿态关键点的行人检测算法中部分遮挡带来的挑战，我们提出了一种称为分离和降维基于生成对抗性补全网络(SDR-GAIN)的新型行人姿势关键点补全方法。首先，我们利用OpenPose在图像中估计行人的姿态。然后，我们对由于遮挡或其他因素而不完整的行人头部和躯干关键点进行维度缩减，以增强特征并进一步统一特征分布。最后，我们引入了基于生成对抗网络(GAN)框架的两种生成模型，这些模型融合了Huber损失、残差结构和L1正则化来生成部分遮挡行人不完整头部和躯干姿态关键点的缺失部分，从而实现了姿态补全。我们在MS COCO和JAAD数据集上的实验表明，SDR-GAIN的性能优于基本的GAIN框架。

    To mitigate the challenges arising from partial occlusion in human pose keypoint based pedestrian detection methods , we present a novel pedestrian pose keypoint completion method called the separation and dimensionality reduction-based generative adversarial imputation networks (SDR-GAIN) . Firstly, we utilize OpenPose to estimate pedestrian poses in images. Then, we isolate the head and torso keypoints of pedestrians with incomplete keypoints due to occlusion or other factors and perform dimensionality reduction to enhance features and further unify feature distribution. Finally, we introduce two generative models based on the generative adversarial networks (GAN) framework, which incorporate Huber loss, residual structure, and L1 regularization to generate missing parts of the incomplete head and torso pose keypoints of partially occluded pedestrians, resulting in pose completion. Our experiments on MS COCO and JAAD datasets demonstrate that SDR-GAIN outperforms basic GAIN framework
    
[^70]: 面向自动驾驶的风险感知奖励形成的强化学习代理

    Risk-Aware Reward Shaping of Reinforcement Learning Agents for Autonomous Driving. (arXiv:2306.03220v1 [cs.RO])

    [http://arxiv.org/abs/2306.03220](http://arxiv.org/abs/2306.03220)

    本研究针对自动驾驶中RL代理的安全性问题，提出了一种增加风险感知的奖励形成方法来提高其训练和测试性能。该方法通过额外的重塑奖励项来鼓励探索并惩罚风险驾驶行为，证明其在各种RL代理中具有优势。

    

    强化学习是自动驾驶中有效的运动规划方法，可以通过与环境的交互数据自动学习最优驾驶策略。然而，对于RL代理的奖励函数，其对于性能的影响很大，但是很难确定。传统的研究主要关注安全驾驶状态的奖励，但并未纳入车辆风险驾驶行为的感知。本文研究如何使用风险感知的奖励形成来提高自动驾驶中RL代理的训练和测试性能。根据实践中规定的一般自动驾驶的安全要求，我们提出了额外的重塑奖励项，以鼓励探索并惩罚风险驾驶行为。 OpenAI Gym中的模拟研究表明了风险感知奖励形成在各种RL代理中的优势。同时，我们指出代理转移的方式对风险感知奖励形成影响的现实潜力。

    Reinforcement learning (RL) is an effective approach to motion planning in autonomous driving, where an optimal driving policy can be automatically learned using the interaction data with the environment. Nevertheless, the reward function for an RL agent, which is significant to its performance, is challenging to be determined. The conventional work mainly focuses on rewarding safe driving states but does not incorporate the awareness of risky driving behaviors of the vehicles. In this paper, we investigate how to use risk-aware reward shaping to leverage the training and test performance of RL agents in autonomous driving. Based on the essential requirements that prescribe the safety specifications for general autonomous driving in practice, we propose additional reshaped reward terms that encourage exploration and penalize risky driving behaviors. A simulation study in OpenAI Gym indicates the advantage of risk-aware reward shaping for various RL agents. Also, we point out that proxi
    
[^71]: SpeechGen: 利用提示解锁语音语言模型的生成能力

    SpeechGen: Unlocking the Generative Power of Speech Language Models with Prompts. (arXiv:2306.02207v2 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2306.02207](http://arxiv.org/abs/2306.02207)

    本文探索了一个名为SpeechGen的统一框架，通过提示调节，解锁了语音语言模型的生成能力，成功地实现了直接适应连续语音到离散标记的任务，使得语音生成成为可能。

    

    大型语言模型（LLM）在人工智能生成内容（AIGC）中引起了相当大的关注，特别是随着ChatGPT的出现。然而，将连续语音直接适应于处理离散标记的LLM仍然是一个未解决的挑战，这妨碍了LLM在语音生成方面的应用。高级语音LM们无法充分利用语音信号所包含的丰富信息，包括说话者和情感等，这些信息仅通过文本数据无法获取。在一些语音分类任务中，简单的提示调整已经表现出明显的参数效率和竞争性能的提高。但在多大程度上提示能够有效地激发语音LM的生成任务仍然是一个未知的问题。本文提出了一项先驱性研究，该研究在称为SpeechGen的统一框架中使用提示调节来刺激语音LM进行各种生成任务，并具有约10M可训练参数。

    Large language models (LLMs) have gained considerable attention for Artificial Intelligence Generated Content (AIGC), particularly with the emergence of ChatGPT. However, the direct adaptation of continuous speech to LLMs that process discrete tokens remains an unsolved challenge, hindering the application of LLMs for speech generation. The advanced speech LMs are in the corner, as that speech signals encapsulate a wealth of information, including speaker and emotion, beyond textual data alone. Prompt tuning has demonstrated notable gains in parameter efficiency and competitive performance on some speech classification tasks. However, the extent to which prompts can effectively elicit generation tasks from speech LMs remains an open question. In this paper, we present pioneering research that explores the application of prompt tuning to stimulate speech LMs for various generation tasks, within a unified framework called SpeechGen, with around 10M trainable parameters. The proposed unif
    
[^72]: 关于纯16位浮点神经网络的辩护

    In Defense of Pure 16-bit Floating-Point Neural Networks. (arXiv:2305.10947v1 [cs.LG])

    [http://arxiv.org/abs/2305.10947](http://arxiv.org/abs/2305.10947)

    本文探讨了纯16位浮点神经网络的被忽视的效率，提供了理论分析来探讨16位和32位模型的差异，并可以定量解释16位模型与其32位对应物之间的条件。

    

    减少编码神经网络权重和激活所需的位数是非常可取的，因为它可以加快神经网络的训练和推理时间，同时减少内存消耗。因此，这一领域的研究引起了广泛关注，以开发利用更低精度计算的神经网络，比如混合精度训练。有趣的是，目前不存在纯16位浮点设置的方法。本文揭示了纯16位浮点神经网络被忽视的效率。我们通过提供全面的理论分析来探讨造成16位和32位模型的差异的因素。我们规范化了浮点误差和容忍度的概念，从而可以定量解释16位模型与其32位对应物之间密切逼近结果的条件。这种理论探索提供了新的视角。

    Reducing the number of bits needed to encode the weights and activations of neural networks is highly desirable as it speeds up their training and inference time while reducing memory consumption. For these reasons, research in this area has attracted significant attention toward developing neural networks that leverage lower-precision computing, such as mixed-precision training. Interestingly, none of the existing approaches has investigated pure 16-bit floating-point settings. In this paper, we shed light on the overlooked efficiency of pure 16-bit floating-point neural networks. As such, we provide a comprehensive theoretical analysis to investigate the factors contributing to the differences observed between 16-bit and 32-bit models. We formalize the concepts of floating-point error and tolerance, enabling us to quantitatively explain the conditions under which a 16-bit model can closely approximate the results of its 32-bit counterpart. This theoretical exploration offers perspect
    
[^73]: 区域感知预训练：视觉变压器下的开放词汇物体检测

    Region-Aware Pretraining for Open-Vocabulary Object Detection with Vision Transformers. (arXiv:2305.07011v1 [cs.CV])

    [http://arxiv.org/abs/2305.07011](http://arxiv.org/abs/2305.07011)

    本文提出了一种基于视觉变压器的对比图像-文本预训练方法，针对开放词汇的物体检测任务，采用区域感知预训练、聚焦损失和新颖物体提案等技术，在LVIS上取得了32.1$AP_r$的最佳效果。

    

    本文提出了区域感知开放词汇视觉变压器（RO-ViT），一种对比图像-文本预训练方法，旨在填补图像级预训练和开放词汇物体检测之间的差距。在预训练阶段，我们建议随机裁剪并调整位置嵌入的区域，而不是使用整个图像位置嵌入。这更好地匹配了检测微调阶段中区域级别上使用位置嵌入的方式。此外，我们用聚焦损失替换了对比学习中常用的softmax交叉熵损失，以更好地学习那些有信息量但难以捕捉的例子。最后，我们利用了最近在新颖物体提案方面的进展，以改进开放词汇检测的微调。我们在LVIS和COCO开放词汇检测基准上评估了完整模型和零-shot转移性能。RO-ViT在LVIS上实现了32.1$AP_r$的最佳效果，超过现有最佳方法5.8个百分点，同时还具有竞争性的零-shot转移检测结果。

    We present Region-aware Open-vocabulary Vision Transformers (RO-ViT) - a contrastive image-text pretraining recipe to bridge the gap between image-level pretraining and open-vocabulary object detection. At the pretraining phase, we propose to randomly crop and resize regions of positional embeddings instead of using the whole image positional embeddings. This better matches the use of positional embeddings at region-level in the detection finetuning phase. In addition, we replace the common softmax cross entropy loss in contrastive learning with focal loss to better learn the informative yet difficult examples. Finally, we leverage recent advances in novel object proposals to improve open-vocabulary detection finetuning. We evaluate our full model on the LVIS and COCO open-vocabulary detection benchmarks and zero-shot transfer. RO-ViT achieves a state-of-the-art 32.1 $AP_r$ on LVIS, surpassing the best existing approach by +5.8 points in addition to competitive zero-shot transfer detec
    
[^74]: CryCeleb: 基于婴儿哭声的说话人认证数据集

    CryCeleb: A Speaker Verification Dataset Based on Infant Cry Sounds. (arXiv:2305.00969v2 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2305.00969](http://arxiv.org/abs/2305.00969)

    CryCeleb是一个基于婴儿哭声的说话人认证数据集，包括超过6小时的手动分割哭声，可用于研究婴儿哭声分析。

    

    本文描述了Ubenwa CryCeleb数据集——一个标记的婴儿哭声收集，以及附带的CryCeleb 2023任务——一个基于婴儿哭声的公共说话人验证挑战。我们释放出786名新生儿超过6小时的手动分割哭声，以鼓励婴儿哭声分析方面的研究。

    This paper describes the Ubenwa CryCeleb dataset - a labeled collection of infant cries, and the accompanying CryCeleb 2023 task - a public speaker verification challenge based on infant cry sounds. We release for academic usage more than 6 hours of manually segmented cry sounds from 786 newborns to encourage research in infant cry analysis.
    
[^75]: 基于Q学习的无人机集群障碍物路径规划系统

    Q-learning Based System for Path Planning with UAV Swarms in Obstacle Environments. (arXiv:2303.17655v1 [cs.AI])

    [http://arxiv.org/abs/2303.17655](http://arxiv.org/abs/2303.17655)

    本文提出了一种基于强化学习的Q学习算法，能够在有障碍物的环境中通过人工神经网络进行路径规划优化，从而减少能量消耗和人力成本。

    

    随着无人机集群的自主控制需求不断增加，面对复杂环境中的障碍物，路径规划对于优化能量消耗和减少人力成本具有重要作用。本篇论文提出了一种基于强化学习的Q学习算法，通过人工神经网络不断调整学习，实现在有障碍物的环境中进行路径规划。

    Path Planning methods for autonomous control of Unmanned Aerial Vehicle (UAV) swarms are on the rise because of all the advantages they bring. There are more and more scenarios where autonomous control of multiple UAVs is required. Most of these scenarios present a large number of obstacles, such as power lines or trees. If all UAVs can be operated autonomously, personnel expenses can be decreased. In addition, if their flight paths are optimal, energy consumption is reduced. This ensures that more battery time is left for other operations. In this paper, a Reinforcement Learning based system is proposed for solving this problem in environments with obstacles by making use of Q-Learning. This method allows a model, in this particular case an Artificial Neural Network, to self-adjust by learning from its mistakes and achievements. Regardless of the size of the map or the number of UAVs in the swarm, the goal of these paths is to ensure complete coverage of an area with fixed obstacles f
    
[^76]: ViewRefer: 基于GPT和样例引导的多视角知识处理的三维视觉定位

    ViewRefer: Grasp the Multi-view Knowledge for 3D Visual Grounding with GPT and Prototype Guidance. (arXiv:2303.16894v1 [cs.CV])

    [http://arxiv.org/abs/2303.16894](http://arxiv.org/abs/2303.16894)

    本文提出了ViewRefer，这是一个多视角的三维视觉定位框架，利用大规模语言模型和多视角原型，从文本和3D模态中获取视角知识并增强框架的表现。

    

    通过利用多视角输入的3D场景，可以缓解3D视觉定位中的视角差异问题。然而，现有方法通常忽略了嵌入在文本模态中的视角线索，并且未能权衡不同视图的相对重要性。本文提出了ViewRefer，这是一个多视角的三维视觉定位框架，探索如何从文本和3D模态中获取视角知识。其中，ViewRefer利用大规模语言模型（例如GPT）的多样化语言知识，将单一的定位文本扩展为多个几何一致的描述；同时，在3D模态中，引入了基于Transformer的融合模块和视图间注意力，以增强视图之间物体的交互。此外，还提出了一组可学习的多视角原型，用于记忆不同视角下的场景无关知识，从两个方面增强了框架。

    Understanding 3D scenes from multi-view inputs has been proven to alleviate the view discrepancy issue in 3D visual grounding. However, existing methods normally neglect the view cues embedded in the text modality and fail to weigh the relative importance of different views. In this paper, we propose ViewRefer, a multi-view framework for 3D visual grounding exploring how to grasp the view knowledge from both text and 3D modalities. For the text branch, ViewRefer leverages the diverse linguistic knowledge of large-scale language models, e.g., GPT, to expand a single grounding text to multiple geometry-consistent descriptions. Meanwhile, in the 3D modality, a transformer fusion module with inter-view attention is introduced to boost the interaction of objects across views. On top of that, we further present a set of learnable multi-view prototypes, which memorize scene-agnostic knowledge for different views, and enhance the framework from two perspectives: a view-guided attention module 
    
[^77]: 神经网络中的因果归因学习：超越直接影响

    Learning Causal Attributions in Neural Networks: Beyond Direct Effects. (arXiv:2303.13850v1 [cs.LG])

    [http://arxiv.org/abs/2303.13850](http://arxiv.org/abs/2303.13850)

    本文提出了一种方法来估计和维护神经网络模型中输入-输出属性的因果关系，不仅考虑直接影响，还能考虑间接影响。此方法能够在训练神经网络模型的同时有效地量化因果归因，实验结果表明该方法能够有效地学习因果归因。

    

    近年来，捕捉和维护神经网络模型中的因果关系引起了越来越多的关注。本文研究了用因果方法估计和维护神经网络模型中的输入-输出属性。特别地，现有的研究仅假设输入变量独立（由于神经网络架构），因此仅研究直接影响。我们将神经网络视为结构性因果模型，并提出在输入特征中引入边缘以捕捉和维护直接和间接因果效应的简单而有效的方法，同时训练神经网络模型。我们还提出有效的近似策略来量化高维数据的因果归因。我们在合成和真实数据集上进行了各种实验，结果表明，所提出的方法学习了接近基本事实效果的直接和间接因果归因。

    There has been a growing interest in capturing and maintaining causal relationships in Neural Network (NN) models in recent years. We study causal approaches to estimate and maintain input-output attributions in NN models in this work. In particular, existing efforts in this direction assume independence among input variables (by virtue of the NN architecture), and hence study only direct causal effects. Viewing an NN as a structural causal model (SCM), we instead focus on going beyond direct effects, introduce edges among input features, and provide a simple yet effective methodology to capture and maintain direct and indirect causal effects while training an NN model. We also propose effective approximation strategies to quantify causal attributions in high dimensional data. Our wide range of experiments on synthetic and real-world datasets show that the proposed ante-hoc method learns causal attributions for both direct and indirect causal effects close to the ground truth effects.
    
[^78]: PDSketch: 集成规划领域编程和学习

    PDSketch: Integrated Planning Domain Programming and Learning. (arXiv:2303.05501v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2303.05501](http://arxiv.org/abs/2303.05501)

    本文通过PDSketch语言和可训练的神经网络，实现了模型的学习和在线规划，加速了机器人的灵活性和通用性。

    

    本文研究了一种模型学习和在线规划方法，以构建灵活和通用的机器人。具体而言，我们研究了如何利用底层环境转换模型中的局部性和稀疏结构，以提高模型泛化能力、数据效率和运行效率。我们提出了一种新的域定义语言，名为PDSketch。它允许用户灵活地定义转换模型中的高级结构，例如对象和特征的依赖关系，类似于程序员使用TensorFlow或PyTorch指定卷积神经网络的核大小和隐藏维度的方式。转换模型的细节将由可训练的神经网络填充。基于定义的结构和学习参数，PDSketch自动生成与域无关的规划启发式算法，无需额外的训练。衍生的启发式算法加速了对新目标的规划性能。

    This paper studies a model learning and online planning approach towards building flexible and general robots. Specifically, we investigate how to exploit the locality and sparsity structures in the underlying environmental transition model to improve model generalization, data-efficiency, and runtime-efficiency. We present a new domain definition language, named PDSketch. It allows users to flexibly define high-level structures in the transition models, such as object and feature dependencies, in a way similar to how programmers use TensorFlow or PyTorch to specify kernel sizes and hidden dimensions of a convolutional neural network. The details of the transition model will be filled in by trainable neural networks. Based on the defined structures and learned parameters, PDSketch automatically generates domain-independent planning heuristics without additional training. The derived heuristics accelerate the performance-time planning for novel goals.
    
[^79]: 内部奖励的强化学习

    Internally Rewarded Reinforcement Learning. (arXiv:2302.00270v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.00270](http://arxiv.org/abs/2302.00270)

    这项研究探讨了一类强化学习问题，其中策略的奖励信号由与之相关且同时优化的判别器生成，导致学习过程不稳定。实验结果表明，修剪线性奖励函数可以稳定训练过程。

    

    我们研究了一类强化学习问题，其中用于策略学习的奖励信号由一个与策略相关且与策略同时优化的判别器生成。策略和判别器之间的相互依赖导致了不稳定的学习过程，因为来自不成熟判别器的奖励信号是嘈杂的，阻碍了策略的学习；反过来，未经优化的策略也会阻碍判别器的学习。我们将这种学习设置称为“内部奖励的强化学习”（IRRL），因为奖励不是直接来自环境，而是由判别器“内部”提供的。在本文中，我们正式地表述了IRRL，并提出了一类属于IRRL的问题。我们从理论上推导并经验性地分析了IRRL中奖励函数的影响，并基于这些分析提出了修剪线性奖励函数。实验结果表明，所提出的奖励函数可以持续稳定训练过程。

    We study a class of reinforcement learning problems where the reward signals for policy learning are generated by a discriminator that is dependent on and jointly optimized with the policy. This interdependence between the policy and the discriminator leads to an unstable learning process because reward signals from an immature discriminator are noisy and impede policy learning, and conversely, an under-optimized policy impedes discriminator learning. We call this learning setting \textit{Internally Rewarded Reinforcement Learning} (IRRL) as the reward is not provided directly by the environment but \textit{internally} by the discriminator. In this paper, we formally formulate IRRL and present a class of problems that belong to IRRL. We theoretically derive and empirically analyze the effect of the reward function in IRRL and based on these analyses propose the clipped linear reward function. Experimental results show that the proposed reward function can consistently stabilize the tra
    
[^80]: SceneRF: 利用辐射场的自监督单目3D场景重建

    SceneRF: Self-Supervised Monocular 3D Scene Reconstruction with Radiance Fields. (arXiv:2212.02501v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2212.02501](http://arxiv.org/abs/2212.02501)

    SceneRF利用自监督学习结合NeRF的辐射场技术，无需深度监督，只需使用图像序列训练，可以高效处理大场景，能够生成新的深度视图并进行3D场景重建，性能在室内外场景中优于所有基线方法。

    

    二维图像三维重建的研究已经得到广泛关注，通常需要使用深度监督来进行训练。为了减少对昂贵数据集的依赖，我们提出了SceneRF方法，这是一种完全基于图像序列进行训练的自监督单目场景重建方法。基于最新的神经辐射场技术(NeRF)，我们优化了一个辐射场，并采用了显式深度优化和新颖的概率采样策略来有效处理大场景。在推理阶段，只需输入单个图像即可生成新的深度视图，并将其融合在一起以获得3D场景重建。全面的实验结果表明，我们在室内BundleFusion和室外SemanticKITTI场景下，性能优于最近的所有基线，能够更好地进行新视角深度合成和场景重建。我们的代码可在https://astra-vision.github.io/SceneRF上获得。

    3D reconstruction from 2D image was extensively studied, training with depth supervision. To relax the dependence to costly-acquired datasets, we propose SceneRF, a self-supervised monocular scene reconstruction method using only posed image sequences for training. Fueled by the recent progress in neural radiance fields (NeRF) we optimize a radiance field though with explicit depth optimization and a novel probabilistic sampling strategy to efficiently handle large scenes. At inference, a single input image suffices to hallucinate novel depth views which are fused together to obtain 3D scene reconstruction. Thorough experiments demonstrate that we outperform all recent baselines for novel depth views synthesis and scene reconstruction, on indoor BundleFusion and outdoor SemanticKITTI. Our code is available at https://astra-vision.github.io/SceneRF.
    
[^81]: WSSL：加权自监督学习框架用于图像修复

    WSSL: Weighted Self-supervised Learning Framework For Image-inpainting. (arXiv:2211.13856v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.13856](http://arxiv.org/abs/2211.13856)

    我们提出了一种加权自监督学习框架用于图像修复，通过学习多个权重的先验任务特征，并利用重建损失和感知损失函数来生成更具吸引力的图像。

    

    图像修复是重新生成图像丢失部分的过程。基于监督算法的方法表现出色，但存在两个重要的缺点。它们在使用未见数据进行测试时表现不佳。它们无法捕捉到图像的全局上下文，导致结果视觉上不吸引人。我们提出了一种新颖的自监督学习框架用于图像修复：加权自监督学习（WSSL），以解决这些问题。我们设计了WSSL来从多个加权的先验任务中学习特征。然后利用这些特征进行下游任务，即图像修复。为了改善我们的框架的性能并产生更具视觉吸引力的图像，我们还提出了一种新颖的图像修复损失函数。该损失函数利用重建损失和感知损失函数来重新生成图像。我们的实验证明，WSSL优于先前的方法，并且我们的损失函数有助于产生更好的结果。

    Image inpainting is the process of regenerating lost parts of the image. Supervised algorithm-based methods have shown excellent results but have two significant drawbacks. They do not perform well when tested with unseen data. They fail to capture the global context of the image, resulting in a visually unappealing result. We propose a novel self-supervised learning framework for image-inpainting: Weighted Self-Supervised Learning (WSSL) to tackle these problems. We designed WSSL to learn features from multiple weighted pretext tasks. These features are then utilized for the downstream task, image-inpainting. To improve the performance of our framework and produce more visually appealing images, we also present a novel loss function for image inpainting. The loss function takes advantage of both reconstruction loss and perceptual loss functions to regenerate the image. Our experimentation shows WSSL outperforms previous methods, and our loss function helps produce better results.
    
[^82]: 独特的自相似物体检测

    Distinctive Self-Similar Object Detection. (arXiv:2211.10995v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.10995](http://arxiv.org/abs/2211.10995)

    本文提出了一种独特的自相似物体检测方法，通过使用火和烟的自相似特征来解决其形状多样性的问题，并设计了一种半监督方法来评估和提高物体检测精度。

    

    基于深度学习的物体检测在人工智能的实际应用中展现出了重要的存在。然而，像火和烟这样的物体由于其非固态和各种各样的形状，对物体检测提出了挑战，因此在实际的火灾预防和控制中难以真正满足要求。在本文中，我们提出了火和烟中的独特的自相似特征可以解决各种形状的困扰。据我们所知，我们是第一个讨论这个问题的。为了评估火和烟的自相似性并提高物体检测的精度，我们设计了一种使用Hausdorff距离描述实例之间相似度的半监督方法。此外，基于自相似的概念，我们还设计了一种评估这个特定任务的新方法，以更公平的方式进行评估。我们精心设计了我们的网络架构。

    Deep learning-based object detection has demonstrated a significant presence in the practical applications of artificial intelligence. However, objects such as fire and smoke, pose challenges to object detection because of their non-solid and various shapes, and consequently difficult to truly meet requirements in practical fire prevention and control. In this paper, we propose that the distinctive fractal feature of self-similar in fire and smoke can relieve us from struggling with their various shapes. To our best knowledge, we are the first to discuss this problem. In order to evaluate the self-similarity of the fire and smoke and improve the precision of object detection, we design a semi-supervised method that use Hausdorff distance to describe the resemblance between instances. Besides, based on the concept of self-similar, we have devised a novel methodology for evaluating this particular task in a more equitable manner. We have meticulously designed our network architecture bas
    
[^83]: 分布式图神经网络训练：一项调查

    Distributed Graph Neural Network Training: A Survey. (arXiv:2211.00216v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.00216](http://arxiv.org/abs/2211.00216)

    这项调查研究了分布式图神经网络训练中的挑战，并提出了解决方案来优化特征通信、模型精度和分布式同步。

    

    图神经网络（GNNs）是一种在图上进行训练的深度学习模型，在多个领域取得了成功应用。尽管GNNs的有效性，但是将其扩展到大规模图依然具有挑战性。分布式计算成为训练大规模GNNs的有希望的解决方案，因为它能提供丰富的计算资源。然而，图结构的依赖性使得实现高效的分布式GNN训练变得困难，存在大量的通信和负载不平衡。近年来，人们对分布式GNN训练进行了许多努力，并提出了一系列训练算法和系统。然而，在分布式执行GNN训练的优化技术方面缺乏系统性的综述。本调查分析了分布式GNN训练中的三个主要挑战：大规模特征通信、模型精度损失和分布式同步。

    Graph neural networks (GNNs) are a type of deep learning models that are trained on graphs and have been successfully applied in various domains. Despite the effectiveness of GNNs, it is still challenging for GNNs to efficiently scale to large graphs. As a remedy, distributed computing becomes a promising solution of training large-scale GNNs, since it is able to provide abundant computing resources. However, the dependency of graph structure increases the difficulty of achieving high-efficiency distributed GNN training, which suffers from the massive communication and workload imbalance. In recent years, many efforts have been made on distributed GNN training, and an array of training algorithms and systems have been proposed. Yet, there is a lack of systematic review on the optimization techniques for the distributed execution of GNN training. In this survey, we analyze three major challenges in distributed GNN training that are massive feature communication, the loss of model accura
    
[^84]: 使用基于Transformer的场景表示学习增强强化学习用于自动驾驶决策

    Augmenting Reinforcement Learning with Transformer-based Scene Representation Learning for Decision-making of Autonomous Driving. (arXiv:2208.12263v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.12263](http://arxiv.org/abs/2208.12263)

    本研究提出了Scene-Rep Transformer来提升强化学习决策能力，通过改进场景表示编码和顺序预测潜在蒸馏。采用多阶段Transformer编码器建模交互意识和意图意识，并使用顺序潜在Transformer进行自监督学习，加速训练和减少探索空间。

    

    城市自动驾驶的决策是具有挑战性的，由于交通参与者的随机性和道路结构的复杂性。尽管基于强化学习（RL）的决策方案在处理城市驾驶场景方面很有前景，但它的采样效率低且适应性差。在本文中，我们提出了Scene-Rep Transformer来改善RL决策能力，通过更好的场景表示编码和顺序预测潜在蒸馏。具体而言，我们构建了一个多阶段Transformer（MST）编码器，用于建模自车与其邻居之间的交互意识以及代理者与候选路径之间的意图意识。我们采用了一个具有自监督学习目标的顺序潜在Transformer（SLT），将未来的预测信息蒸馏到潜在的场景表示中，以减少探索空间并加快训练。

    Decision-making for urban autonomous driving is challenging due to the stochastic nature of interactive traffic participants and the complexity of road structures. Although reinforcement learning (RL)-based decision-making scheme is promising to handle urban driving scenarios, it suffers from low sample efficiency and poor adaptability. In this paper, we propose Scene-Rep Transformer to improve the RL decision-making capabilities with better scene representation encoding and sequential predictive latent distillation. Specifically, a multi-stage Transformer (MST) encoder is constructed to model not only the interaction awareness between the ego vehicle and its neighbors but also intention awareness between the agents and their candidate routes. A sequential latent Transformer (SLT) with self-supervised learning objectives is employed to distill the future predictive information into the latent scene representation, in order to reduce the exploration space and speed up training. The fina
    
[^85]: 一种感知优化且自校准的色调映射运算符

    A Perceptually Optimized and Self-Calibrated Tone Mapping Operator. (arXiv:2206.09146v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2206.09146](http://arxiv.org/abs/2206.09146)

    本文开发了一种两步神经网络TMO，具有自校准和感知优化功能，可以将HDR图像压缩到LDR图像，同时通过感知度量实现了灵敏的质量优化。

    

    随着高动态范围（HDR）摄影的普及和可访问性增加，对动态范围压缩的色调映射运算符（TMO）需求日益增加。本文开发了一个两步神经网络TMO，具有自校准和感知优化功能。第一阶段，我们首先将HDR图像分解成规范化拉普拉斯金字塔，然后使用两个轻量级深度神经网络（DNN），以规范化表示作为输入来估计相应LDR图像的拉普拉斯金字塔。我们通过最小化规范化拉普拉斯金字塔距离（NLPD）来优化色调映射网络，这是与人类对色调映射图像质量的判断相一致的感知度量。在第二阶段中，输入的HDR图像是自校准的，以计算出最终的LDR图像。我们将同一张HDR图像输入经过不同最大亮度重新调整比例后的学习色调映射网络中，来完成这一自校准过程。

    With the increasing popularity and accessibility of high dynamic range (HDR) photography, tone mapping operators (TMOs) for dynamic range compression are practically demanding. In this paper, we develop a two-stage neural network-based TMO that is self-calibrated and perceptually optimized. In Stage one, motivated by the physiology of the early stages of the human visual system, we first decompose an HDR image into a normalized Laplacian pyramid. We then use two lightweight deep neural networks (DNNs), taking the normalized representation as input and estimating the Laplacian pyramid of the corresponding LDR image. We optimize the tone mapping network by minimizing the normalized Laplacian pyramid distance (NLPD), a perceptual metric aligning with human judgments of tone-mapped image quality. In Stage two, the input HDR image is self-calibrated to compute the final LDR image. We feed the same HDR image but rescaled with different maximum luminances to the learned tone mapping network, 
    
[^86]: 基于语法的基础词汇学习

    Grammar-Based Grounded Lexicon Learning. (arXiv:2202.08806v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2202.08806](http://arxiv.org/abs/2202.08806)

    基于语法的基础词汇学习（G2L2）是一种从基础数据中学习语言含义表示的方法，通过将单词映射到语法类型和神经符号语义程序，利用基于语法的组合推导句子的含义，最终可以在基础输入上执行。

    

    我们提出了一种基于语法的基础词汇学习方法（G2L2），用于从基础数据（如图像和文本的配对）中学习语言的组合和基于基础的含义表示。G2L2的核心是一组词汇条目，将每个单词映射到一个语法类型和神经符号语义程序的元组。给定一个输入句子，G2L2首先查找与每个标记相关联的词汇条目。然后通过基于语法的组合词汇含义来推导句子的含义作为可执行的神经符号程序。恢复的含义程序可以在基础输入上执行。为了在指数级增长的组合空间中促进学习，我们引入了一种基于逻辑回归的channel pruning方法。

    We present Grammar-Based Grounded Lexicon Learning (G2L2), a lexicalist approach toward learning a compositional and grounded meaning representation of language from grounded data, such as paired images and texts. At the core of G2L2 is a collection of lexicon entries, which map each word to a tuple of a syntactic type and a neuro-symbolic semantic program. For example, the word shiny has a syntactic type of adjective; its neuro-symbolic semantic program has the symbolic form {\lambda}x. filter(x, SHINY), where the concept SHINY is associated with a neural network embedding, which will be used to classify shiny objects. Given an input sentence, G2L2 first looks up the lexicon entries associated with each token. It then derives the meaning of the sentence as an executable neuro-symbolic program by composing lexical meanings based on syntax. The recovered meaning programs can be executed on grounded inputs. To facilitate learning in an exponentially-growing compositional space, we introd
    
[^87]: G\"odel的本体论证的简化变体

    A Simplified Variant of G\"odel's Ontological Argument. (arXiv:2202.06264v3 [cs.LO] UPDATED)

    [http://arxiv.org/abs/2202.06264](http://arxiv.org/abs/2202.06264)

    本论文提出了G\"odel的本体论证的简化变体，该变体在基本模态逻辑K或KT中已经是有效的，避免了复杂的谓词，并且展示了人机交互在计算形而上学中的应用。

    

    提出了G\"odel的本体论证的简化变体。这个简化的论证在基本模态逻辑K或KT中已经是有效的，它不会遭受模态崩溃，并且避免了G\"odel所使用的相当复杂的本质（Ess.）和必然存在（NE）的谓词。所提出的变体是通过与现代证明助理系统交互进行一系列理论简化实验的副产物。这些实验的起点是G\"odel论证的计算机编码，然后系统地应用自动推理技术来得到所展示的简化变体。所呈现的工作因此展示了计算形而上学中富有成果的人机交互。这个展示结果是否增加或减少了本体论证的吸引力和说服力，是一个我想交给哲学和神学讨论的问题。

    A simplified variant of G\"odel's ontological argument is presented. The simplified argument is valid already in basic modal logics K or KT, it does not suffer from modal collapse, and it avoids the rather complex predicates of essence (Ess.) and necessary existence (NE) as used by G\"odel. The variant presented has been obtained as a side result of a series of theory simplification experiments conducted in interaction with a modern proof assistant system. The starting point for these experiments was the computer encoding of G\"odel's argument, and then automated reasoning techniques were systematically applied to arrive at the simplified variant presented. The presented work thus exemplifies a fruitful human-computer interaction in computational metaphysics. Whether the presented result increases or decreases the attractiveness and persuasiveness of the ontological argument is a question I would like to pass on to philosophy and theology.
    
[^88]: 基于图的推荐系统在社区检测中的增强

    Graph-Based Recommendation System Enhanced with Community Detection. (arXiv:2201.03622v3 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2201.03622](http://arxiv.org/abs/2201.03622)

    本文提出了一个基于图的推荐系统，利用数学和统计方法确定标签的相似性，包括词汇相似性和共现解决方案，并考虑了标签分配的时间，以提高推荐的准确性。

    

    许多研究者已经利用标签信息来改善推荐系统中推荐技术的性能。通过研究用户的标签，可以了解他们的兴趣，从而提高推荐的准确性。然而，由于用户自定义标签的任意性和缺乏限制，确定其确切含义和标签之间的相似性存在问题。本文利用数学和统计方法确定标签的词汇相似性和共现解决方案，以分配语义相似性。另外，考虑到用户兴趣随时间变化，本文还在共现标签中考虑了标签分配的时间以确定标签的相似性。然后，基于标签的相似性创建图形模型来建模用户的兴趣。

    Many researchers have used tag information to improve the performance of recommendation techniques in recommender systems. Examining the tags of users will help to get their interests and leads to more accuracy in the recommendations. Since user-defined tags are chosen freely and without any restrictions, problems arise in determining their exact meaning and the similarity of tags. However, using thesaurus and ontologies to find the meaning of tags is not very efficient due to their free definition by users and the use of different languages in many data sets. Therefore, this article uses mathematical and statistical methods to determine lexical similarity and co-occurrence tags solution to assign semantic similarity. On the other hand, due to the change of users' interests over time this article has considered the time of tag assignments in co-occurrence tags for determining similarity of tags. Then the graph is created based on similarity of tags. For modeling the interests of the us
    
[^89]: 事实核查: 分析多语言新闻源中的金融事件

    Fact Check: Analyzing Financial Events from Multilingual News Sources. (arXiv:2106.15221v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2106.15221](http://arxiv.org/abs/2106.15221)

    FactCheck in finance是一个基于深度学习模型的网络新闻聚合器，能够从多语言新闻源中提取重要的金融事件，并通过无监督聚类方法对其进行聚类。通过使用Transformer-based事实核查器来检查新闻文章的可信度，该系统显示出优于几个强基准模型的性能。

    

    近年来，金融新闻数据的规模和复杂性急剧增加，使得投资分析师越来越难以获取有价值的见解和进行分析。我们提出了一种名为FactCheck in finance的基于深度学习模型的网络新闻聚合器，以从多语言新闻源中提供投资分析师整体视角下的重要金融事件，并使用无监督聚类方法提取事件。我们提供了一个网络界面，使用基于Transformer的事实核查器来检查新闻文章的可信度。该事实核查器的性能使用与并购事件相关的数据集进行评估，并显示优于几个强基准模型。

    The explosion in the sheer magnitude and complexity of financial news data in recent years makes it increasingly challenging for investment analysts to extract valuable insights and perform analysis. We propose FactCheck in finance, a web-based news aggregator with deep learning models, to provide analysts with a holistic view of important financial events from multilingual news sources and extract events using an unsupervised clustering method. A web interface is provided to examine the credibility of news articles using a transformer-based fact-checker. The performance of the fact checker is evaluated using a dataset related to merger and acquisition (M\&A) events and is shown to outperform several strong baselines.
    
[^90]: 基于图神经网络和自回归策略分解的符号关系深度强化学习

    Symbolic Relational Deep Reinforcement Learning based on Graph Neural Networks and Autoregressive Policy Decomposition. (arXiv:2009.12462v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2009.12462](http://arxiv.org/abs/2009.12462)

    这篇论文介绍了一种基于图神经网络和自回归策略分解的深度强化学习框架，能够处理符号关系问题的可变状态和动作空间，并在多个领域展现了广泛的适用性和令人印象深刻的零-shot泛化能力。

    

    我们关注于以对象、它们之间的关系和以对象为中心的动作来自然定义的符号关系问题中的强化学习。这些问题具有可变的状态和动作空间，对于大多数现有的强化学习方法而言，找到一个固定长度的表示是困难的，甚至不可能的。我们提出了一个基于图神经网络和自回归策略分解的深度强化学习框架，可以自然地应用于这些问题，并且完全是领域无关的。我们在三个不同的领域展示了该框架的广泛适用性，并展示了在不同问题大小上引人注目的零-shot泛化效果。

    We focus on reinforcement learning (RL) in relational problems that are naturally defined in terms of objects, their relations, and object-centric actions. These problems are characterized by variable state and action spaces, and finding a fixed-length representation, required by most existing RL methods, is difficult, if not impossible. We present a deep RL framework based on graph neural networks and auto-regressive policy decomposition that naturally works with these problems and is completely domain-independent. We demonstrate the framework's broad applicability in three distinct domains and show impressive zero-shot generalization over different problem sizes.
    

